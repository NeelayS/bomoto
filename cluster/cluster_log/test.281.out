Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=281, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15736-15791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477537
Iteration 2/25 | Loss: 0.00084594
Iteration 3/25 | Loss: 0.00071796
Iteration 4/25 | Loss: 0.00068483
Iteration 5/25 | Loss: 0.00067224
Iteration 6/25 | Loss: 0.00067018
Iteration 7/25 | Loss: 0.00066944
Iteration 8/25 | Loss: 0.00066937
Iteration 9/25 | Loss: 0.00066937
Iteration 10/25 | Loss: 0.00066937
Iteration 11/25 | Loss: 0.00066937
Iteration 12/25 | Loss: 0.00066937
Iteration 13/25 | Loss: 0.00066937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006693733739666641, 0.0006693733739666641, 0.0006693733739666641, 0.0006693733739666641, 0.0006693733739666641]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006693733739666641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.59519196
Iteration 2/25 | Loss: 0.00026245
Iteration 3/25 | Loss: 0.00026245
Iteration 4/25 | Loss: 0.00026245
Iteration 5/25 | Loss: 0.00026245
Iteration 6/25 | Loss: 0.00026245
Iteration 7/25 | Loss: 0.00026245
Iteration 8/25 | Loss: 0.00026245
Iteration 9/25 | Loss: 0.00026245
Iteration 10/25 | Loss: 0.00026245
Iteration 11/25 | Loss: 0.00026245
Iteration 12/25 | Loss: 0.00026245
Iteration 13/25 | Loss: 0.00026245
Iteration 14/25 | Loss: 0.00026245
Iteration 15/25 | Loss: 0.00026245
Iteration 16/25 | Loss: 0.00026245
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00026244716718792915, 0.00026244716718792915, 0.00026244716718792915, 0.00026244716718792915, 0.00026244716718792915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026244716718792915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026245
Iteration 2/1000 | Loss: 0.00003420
Iteration 3/1000 | Loss: 0.00002668
Iteration 4/1000 | Loss: 0.00002506
Iteration 5/1000 | Loss: 0.00002393
Iteration 6/1000 | Loss: 0.00002331
Iteration 7/1000 | Loss: 0.00002272
Iteration 8/1000 | Loss: 0.00002235
Iteration 9/1000 | Loss: 0.00002206
Iteration 10/1000 | Loss: 0.00002180
Iteration 11/1000 | Loss: 0.00002166
Iteration 12/1000 | Loss: 0.00002165
Iteration 13/1000 | Loss: 0.00002164
Iteration 14/1000 | Loss: 0.00002159
Iteration 15/1000 | Loss: 0.00002150
Iteration 16/1000 | Loss: 0.00002149
Iteration 17/1000 | Loss: 0.00002148
Iteration 18/1000 | Loss: 0.00002148
Iteration 19/1000 | Loss: 0.00002148
Iteration 20/1000 | Loss: 0.00002147
Iteration 21/1000 | Loss: 0.00002147
Iteration 22/1000 | Loss: 0.00002147
Iteration 23/1000 | Loss: 0.00002146
Iteration 24/1000 | Loss: 0.00002146
Iteration 25/1000 | Loss: 0.00002146
Iteration 26/1000 | Loss: 0.00002146
Iteration 27/1000 | Loss: 0.00002145
Iteration 28/1000 | Loss: 0.00002145
Iteration 29/1000 | Loss: 0.00002145
Iteration 30/1000 | Loss: 0.00002144
Iteration 31/1000 | Loss: 0.00002144
Iteration 32/1000 | Loss: 0.00002144
Iteration 33/1000 | Loss: 0.00002144
Iteration 34/1000 | Loss: 0.00002144
Iteration 35/1000 | Loss: 0.00002144
Iteration 36/1000 | Loss: 0.00002143
Iteration 37/1000 | Loss: 0.00002143
Iteration 38/1000 | Loss: 0.00002143
Iteration 39/1000 | Loss: 0.00002142
Iteration 40/1000 | Loss: 0.00002142
Iteration 41/1000 | Loss: 0.00002142
Iteration 42/1000 | Loss: 0.00002141
Iteration 43/1000 | Loss: 0.00002141
Iteration 44/1000 | Loss: 0.00002141
Iteration 45/1000 | Loss: 0.00002140
Iteration 46/1000 | Loss: 0.00002140
Iteration 47/1000 | Loss: 0.00002140
Iteration 48/1000 | Loss: 0.00002140
Iteration 49/1000 | Loss: 0.00002139
Iteration 50/1000 | Loss: 0.00002139
Iteration 51/1000 | Loss: 0.00002138
Iteration 52/1000 | Loss: 0.00002138
Iteration 53/1000 | Loss: 0.00002138
Iteration 54/1000 | Loss: 0.00002138
Iteration 55/1000 | Loss: 0.00002137
Iteration 56/1000 | Loss: 0.00002137
Iteration 57/1000 | Loss: 0.00002137
Iteration 58/1000 | Loss: 0.00002136
Iteration 59/1000 | Loss: 0.00002135
Iteration 60/1000 | Loss: 0.00002135
Iteration 61/1000 | Loss: 0.00002135
Iteration 62/1000 | Loss: 0.00002134
Iteration 63/1000 | Loss: 0.00002134
Iteration 64/1000 | Loss: 0.00002134
Iteration 65/1000 | Loss: 0.00002134
Iteration 66/1000 | Loss: 0.00002133
Iteration 67/1000 | Loss: 0.00002132
Iteration 68/1000 | Loss: 0.00002131
Iteration 69/1000 | Loss: 0.00002131
Iteration 70/1000 | Loss: 0.00002130
Iteration 71/1000 | Loss: 0.00002130
Iteration 72/1000 | Loss: 0.00002130
Iteration 73/1000 | Loss: 0.00002129
Iteration 74/1000 | Loss: 0.00002129
Iteration 75/1000 | Loss: 0.00002129
Iteration 76/1000 | Loss: 0.00002128
Iteration 77/1000 | Loss: 0.00002128
Iteration 78/1000 | Loss: 0.00002128
Iteration 79/1000 | Loss: 0.00002128
Iteration 80/1000 | Loss: 0.00002128
Iteration 81/1000 | Loss: 0.00002128
Iteration 82/1000 | Loss: 0.00002128
Iteration 83/1000 | Loss: 0.00002127
Iteration 84/1000 | Loss: 0.00002127
Iteration 85/1000 | Loss: 0.00002127
Iteration 86/1000 | Loss: 0.00002127
Iteration 87/1000 | Loss: 0.00002127
Iteration 88/1000 | Loss: 0.00002127
Iteration 89/1000 | Loss: 0.00002126
Iteration 90/1000 | Loss: 0.00002126
Iteration 91/1000 | Loss: 0.00002126
Iteration 92/1000 | Loss: 0.00002125
Iteration 93/1000 | Loss: 0.00002125
Iteration 94/1000 | Loss: 0.00002125
Iteration 95/1000 | Loss: 0.00002125
Iteration 96/1000 | Loss: 0.00002125
Iteration 97/1000 | Loss: 0.00002125
Iteration 98/1000 | Loss: 0.00002125
Iteration 99/1000 | Loss: 0.00002125
Iteration 100/1000 | Loss: 0.00002125
Iteration 101/1000 | Loss: 0.00002125
Iteration 102/1000 | Loss: 0.00002125
Iteration 103/1000 | Loss: 0.00002124
Iteration 104/1000 | Loss: 0.00002124
Iteration 105/1000 | Loss: 0.00002124
Iteration 106/1000 | Loss: 0.00002124
Iteration 107/1000 | Loss: 0.00002123
Iteration 108/1000 | Loss: 0.00002123
Iteration 109/1000 | Loss: 0.00002123
Iteration 110/1000 | Loss: 0.00002123
Iteration 111/1000 | Loss: 0.00002122
Iteration 112/1000 | Loss: 0.00002122
Iteration 113/1000 | Loss: 0.00002122
Iteration 114/1000 | Loss: 0.00002122
Iteration 115/1000 | Loss: 0.00002122
Iteration 116/1000 | Loss: 0.00002122
Iteration 117/1000 | Loss: 0.00002122
Iteration 118/1000 | Loss: 0.00002122
Iteration 119/1000 | Loss: 0.00002122
Iteration 120/1000 | Loss: 0.00002122
Iteration 121/1000 | Loss: 0.00002122
Iteration 122/1000 | Loss: 0.00002121
Iteration 123/1000 | Loss: 0.00002121
Iteration 124/1000 | Loss: 0.00002121
Iteration 125/1000 | Loss: 0.00002121
Iteration 126/1000 | Loss: 0.00002121
Iteration 127/1000 | Loss: 0.00002121
Iteration 128/1000 | Loss: 0.00002121
Iteration 129/1000 | Loss: 0.00002121
Iteration 130/1000 | Loss: 0.00002120
Iteration 131/1000 | Loss: 0.00002120
Iteration 132/1000 | Loss: 0.00002120
Iteration 133/1000 | Loss: 0.00002120
Iteration 134/1000 | Loss: 0.00002120
Iteration 135/1000 | Loss: 0.00002120
Iteration 136/1000 | Loss: 0.00002120
Iteration 137/1000 | Loss: 0.00002120
Iteration 138/1000 | Loss: 0.00002120
Iteration 139/1000 | Loss: 0.00002120
Iteration 140/1000 | Loss: 0.00002120
Iteration 141/1000 | Loss: 0.00002120
Iteration 142/1000 | Loss: 0.00002120
Iteration 143/1000 | Loss: 0.00002120
Iteration 144/1000 | Loss: 0.00002120
Iteration 145/1000 | Loss: 0.00002120
Iteration 146/1000 | Loss: 0.00002120
Iteration 147/1000 | Loss: 0.00002120
Iteration 148/1000 | Loss: 0.00002120
Iteration 149/1000 | Loss: 0.00002120
Iteration 150/1000 | Loss: 0.00002120
Iteration 151/1000 | Loss: 0.00002120
Iteration 152/1000 | Loss: 0.00002119
Iteration 153/1000 | Loss: 0.00002119
Iteration 154/1000 | Loss: 0.00002119
Iteration 155/1000 | Loss: 0.00002119
Iteration 156/1000 | Loss: 0.00002119
Iteration 157/1000 | Loss: 0.00002119
Iteration 158/1000 | Loss: 0.00002119
Iteration 159/1000 | Loss: 0.00002119
Iteration 160/1000 | Loss: 0.00002119
Iteration 161/1000 | Loss: 0.00002119
Iteration 162/1000 | Loss: 0.00002119
Iteration 163/1000 | Loss: 0.00002119
Iteration 164/1000 | Loss: 0.00002119
Iteration 165/1000 | Loss: 0.00002119
Iteration 166/1000 | Loss: 0.00002119
Iteration 167/1000 | Loss: 0.00002118
Iteration 168/1000 | Loss: 0.00002118
Iteration 169/1000 | Loss: 0.00002118
Iteration 170/1000 | Loss: 0.00002118
Iteration 171/1000 | Loss: 0.00002118
Iteration 172/1000 | Loss: 0.00002118
Iteration 173/1000 | Loss: 0.00002118
Iteration 174/1000 | Loss: 0.00002118
Iteration 175/1000 | Loss: 0.00002118
Iteration 176/1000 | Loss: 0.00002118
Iteration 177/1000 | Loss: 0.00002118
Iteration 178/1000 | Loss: 0.00002118
Iteration 179/1000 | Loss: 0.00002118
Iteration 180/1000 | Loss: 0.00002118
Iteration 181/1000 | Loss: 0.00002118
Iteration 182/1000 | Loss: 0.00002118
Iteration 183/1000 | Loss: 0.00002118
Iteration 184/1000 | Loss: 0.00002118
Iteration 185/1000 | Loss: 0.00002118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.1180187104619108e-05, 2.1180187104619108e-05, 2.1180187104619108e-05, 2.1180187104619108e-05, 2.1180187104619108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1180187104619108e-05

Optimization complete. Final v2v error: 3.8409626483917236 mm

Highest mean error: 4.249777317047119 mm for frame 20

Lowest mean error: 3.2897348403930664 mm for frame 45

Saving results

Total time: 41.740649938583374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058592
Iteration 2/25 | Loss: 0.00186666
Iteration 3/25 | Loss: 0.00130301
Iteration 4/25 | Loss: 0.00098340
Iteration 5/25 | Loss: 0.00097379
Iteration 6/25 | Loss: 0.00105538
Iteration 7/25 | Loss: 0.00091147
Iteration 8/25 | Loss: 0.00077875
Iteration 9/25 | Loss: 0.00072716
Iteration 10/25 | Loss: 0.00071568
Iteration 11/25 | Loss: 0.00068935
Iteration 12/25 | Loss: 0.00068855
Iteration 13/25 | Loss: 0.00068569
Iteration 14/25 | Loss: 0.00067504
Iteration 15/25 | Loss: 0.00066795
Iteration 16/25 | Loss: 0.00066950
Iteration 17/25 | Loss: 0.00066641
Iteration 18/25 | Loss: 0.00066381
Iteration 19/25 | Loss: 0.00066273
Iteration 20/25 | Loss: 0.00066641
Iteration 21/25 | Loss: 0.00066701
Iteration 22/25 | Loss: 0.00066208
Iteration 23/25 | Loss: 0.00066130
Iteration 24/25 | Loss: 0.00065629
Iteration 25/25 | Loss: 0.00065376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51559246
Iteration 2/25 | Loss: 0.00056575
Iteration 3/25 | Loss: 0.00056575
Iteration 4/25 | Loss: 0.00056575
Iteration 5/25 | Loss: 0.00056575
Iteration 6/25 | Loss: 0.00056575
Iteration 7/25 | Loss: 0.00056575
Iteration 8/25 | Loss: 0.00056575
Iteration 9/25 | Loss: 0.00056575
Iteration 10/25 | Loss: 0.00056575
Iteration 11/25 | Loss: 0.00056575
Iteration 12/25 | Loss: 0.00056575
Iteration 13/25 | Loss: 0.00056575
Iteration 14/25 | Loss: 0.00056575
Iteration 15/25 | Loss: 0.00056575
Iteration 16/25 | Loss: 0.00056575
Iteration 17/25 | Loss: 0.00056575
Iteration 18/25 | Loss: 0.00056575
Iteration 19/25 | Loss: 0.00056575
Iteration 20/25 | Loss: 0.00056575
Iteration 21/25 | Loss: 0.00056575
Iteration 22/25 | Loss: 0.00056575
Iteration 23/25 | Loss: 0.00056575
Iteration 24/25 | Loss: 0.00056575
Iteration 25/25 | Loss: 0.00056575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056575
Iteration 2/1000 | Loss: 0.00007681
Iteration 3/1000 | Loss: 0.00005076
Iteration 4/1000 | Loss: 0.00002684
Iteration 5/1000 | Loss: 0.00004354
Iteration 6/1000 | Loss: 0.00035320
Iteration 7/1000 | Loss: 0.00020079
Iteration 8/1000 | Loss: 0.00003356
Iteration 9/1000 | Loss: 0.00005533
Iteration 10/1000 | Loss: 0.00005279
Iteration 11/1000 | Loss: 0.00002293
Iteration 12/1000 | Loss: 0.00036439
Iteration 13/1000 | Loss: 0.00016903
Iteration 14/1000 | Loss: 0.00004703
Iteration 15/1000 | Loss: 0.00035400
Iteration 16/1000 | Loss: 0.00010552
Iteration 17/1000 | Loss: 0.00013649
Iteration 18/1000 | Loss: 0.00071670
Iteration 19/1000 | Loss: 0.00019996
Iteration 20/1000 | Loss: 0.00027879
Iteration 21/1000 | Loss: 0.00003191
Iteration 22/1000 | Loss: 0.00002236
Iteration 23/1000 | Loss: 0.00001911
Iteration 24/1000 | Loss: 0.00001717
Iteration 25/1000 | Loss: 0.00001611
Iteration 26/1000 | Loss: 0.00001548
Iteration 27/1000 | Loss: 0.00001501
Iteration 28/1000 | Loss: 0.00001464
Iteration 29/1000 | Loss: 0.00001434
Iteration 30/1000 | Loss: 0.00001414
Iteration 31/1000 | Loss: 0.00001414
Iteration 32/1000 | Loss: 0.00001405
Iteration 33/1000 | Loss: 0.00001399
Iteration 34/1000 | Loss: 0.00001395
Iteration 35/1000 | Loss: 0.00001394
Iteration 36/1000 | Loss: 0.00001393
Iteration 37/1000 | Loss: 0.00001392
Iteration 38/1000 | Loss: 0.00001389
Iteration 39/1000 | Loss: 0.00001388
Iteration 40/1000 | Loss: 0.00001388
Iteration 41/1000 | Loss: 0.00001387
Iteration 42/1000 | Loss: 0.00001385
Iteration 43/1000 | Loss: 0.00001385
Iteration 44/1000 | Loss: 0.00001385
Iteration 45/1000 | Loss: 0.00001385
Iteration 46/1000 | Loss: 0.00001385
Iteration 47/1000 | Loss: 0.00001385
Iteration 48/1000 | Loss: 0.00001385
Iteration 49/1000 | Loss: 0.00001385
Iteration 50/1000 | Loss: 0.00001384
Iteration 51/1000 | Loss: 0.00001384
Iteration 52/1000 | Loss: 0.00001384
Iteration 53/1000 | Loss: 0.00001384
Iteration 54/1000 | Loss: 0.00001384
Iteration 55/1000 | Loss: 0.00001384
Iteration 56/1000 | Loss: 0.00001384
Iteration 57/1000 | Loss: 0.00001381
Iteration 58/1000 | Loss: 0.00001380
Iteration 59/1000 | Loss: 0.00001380
Iteration 60/1000 | Loss: 0.00001380
Iteration 61/1000 | Loss: 0.00001379
Iteration 62/1000 | Loss: 0.00001379
Iteration 63/1000 | Loss: 0.00001378
Iteration 64/1000 | Loss: 0.00001377
Iteration 65/1000 | Loss: 0.00001377
Iteration 66/1000 | Loss: 0.00001377
Iteration 67/1000 | Loss: 0.00001377
Iteration 68/1000 | Loss: 0.00001377
Iteration 69/1000 | Loss: 0.00001377
Iteration 70/1000 | Loss: 0.00001377
Iteration 71/1000 | Loss: 0.00001376
Iteration 72/1000 | Loss: 0.00001376
Iteration 73/1000 | Loss: 0.00001376
Iteration 74/1000 | Loss: 0.00001376
Iteration 75/1000 | Loss: 0.00001376
Iteration 76/1000 | Loss: 0.00001376
Iteration 77/1000 | Loss: 0.00001375
Iteration 78/1000 | Loss: 0.00001375
Iteration 79/1000 | Loss: 0.00001375
Iteration 80/1000 | Loss: 0.00001374
Iteration 81/1000 | Loss: 0.00001374
Iteration 82/1000 | Loss: 0.00001373
Iteration 83/1000 | Loss: 0.00001373
Iteration 84/1000 | Loss: 0.00001372
Iteration 85/1000 | Loss: 0.00001372
Iteration 86/1000 | Loss: 0.00001369
Iteration 87/1000 | Loss: 0.00001369
Iteration 88/1000 | Loss: 0.00001369
Iteration 89/1000 | Loss: 0.00001368
Iteration 90/1000 | Loss: 0.00001368
Iteration 91/1000 | Loss: 0.00001367
Iteration 92/1000 | Loss: 0.00001367
Iteration 93/1000 | Loss: 0.00001366
Iteration 94/1000 | Loss: 0.00001366
Iteration 95/1000 | Loss: 0.00001365
Iteration 96/1000 | Loss: 0.00001365
Iteration 97/1000 | Loss: 0.00001365
Iteration 98/1000 | Loss: 0.00001365
Iteration 99/1000 | Loss: 0.00001364
Iteration 100/1000 | Loss: 0.00001364
Iteration 101/1000 | Loss: 0.00001364
Iteration 102/1000 | Loss: 0.00001359
Iteration 103/1000 | Loss: 0.00001351
Iteration 104/1000 | Loss: 0.00001351
Iteration 105/1000 | Loss: 0.00001349
Iteration 106/1000 | Loss: 0.00001348
Iteration 107/1000 | Loss: 0.00001347
Iteration 108/1000 | Loss: 0.00001347
Iteration 109/1000 | Loss: 0.00001345
Iteration 110/1000 | Loss: 0.00001344
Iteration 111/1000 | Loss: 0.00001343
Iteration 112/1000 | Loss: 0.00001342
Iteration 113/1000 | Loss: 0.00001342
Iteration 114/1000 | Loss: 0.00001341
Iteration 115/1000 | Loss: 0.00001341
Iteration 116/1000 | Loss: 0.00001340
Iteration 117/1000 | Loss: 0.00001336
Iteration 118/1000 | Loss: 0.00001332
Iteration 119/1000 | Loss: 0.00001332
Iteration 120/1000 | Loss: 0.00001332
Iteration 121/1000 | Loss: 0.00001332
Iteration 122/1000 | Loss: 0.00001332
Iteration 123/1000 | Loss: 0.00001332
Iteration 124/1000 | Loss: 0.00001332
Iteration 125/1000 | Loss: 0.00001331
Iteration 126/1000 | Loss: 0.00001331
Iteration 127/1000 | Loss: 0.00001331
Iteration 128/1000 | Loss: 0.00001331
Iteration 129/1000 | Loss: 0.00001331
Iteration 130/1000 | Loss: 0.00001331
Iteration 131/1000 | Loss: 0.00001331
Iteration 132/1000 | Loss: 0.00001330
Iteration 133/1000 | Loss: 0.00001330
Iteration 134/1000 | Loss: 0.00001330
Iteration 135/1000 | Loss: 0.00001330
Iteration 136/1000 | Loss: 0.00001329
Iteration 137/1000 | Loss: 0.00001329
Iteration 138/1000 | Loss: 0.00001329
Iteration 139/1000 | Loss: 0.00001328
Iteration 140/1000 | Loss: 0.00001328
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001327
Iteration 143/1000 | Loss: 0.00001327
Iteration 144/1000 | Loss: 0.00001326
Iteration 145/1000 | Loss: 0.00001326
Iteration 146/1000 | Loss: 0.00001326
Iteration 147/1000 | Loss: 0.00001325
Iteration 148/1000 | Loss: 0.00001325
Iteration 149/1000 | Loss: 0.00001325
Iteration 150/1000 | Loss: 0.00001325
Iteration 151/1000 | Loss: 0.00001325
Iteration 152/1000 | Loss: 0.00001324
Iteration 153/1000 | Loss: 0.00001324
Iteration 154/1000 | Loss: 0.00001324
Iteration 155/1000 | Loss: 0.00001324
Iteration 156/1000 | Loss: 0.00001324
Iteration 157/1000 | Loss: 0.00001324
Iteration 158/1000 | Loss: 0.00001324
Iteration 159/1000 | Loss: 0.00001323
Iteration 160/1000 | Loss: 0.00001323
Iteration 161/1000 | Loss: 0.00001323
Iteration 162/1000 | Loss: 0.00001323
Iteration 163/1000 | Loss: 0.00001323
Iteration 164/1000 | Loss: 0.00001323
Iteration 165/1000 | Loss: 0.00001322
Iteration 166/1000 | Loss: 0.00001322
Iteration 167/1000 | Loss: 0.00001322
Iteration 168/1000 | Loss: 0.00001322
Iteration 169/1000 | Loss: 0.00001322
Iteration 170/1000 | Loss: 0.00001322
Iteration 171/1000 | Loss: 0.00001321
Iteration 172/1000 | Loss: 0.00001320
Iteration 173/1000 | Loss: 0.00001320
Iteration 174/1000 | Loss: 0.00001320
Iteration 175/1000 | Loss: 0.00001320
Iteration 176/1000 | Loss: 0.00001319
Iteration 177/1000 | Loss: 0.00001319
Iteration 178/1000 | Loss: 0.00001319
Iteration 179/1000 | Loss: 0.00001319
Iteration 180/1000 | Loss: 0.00001319
Iteration 181/1000 | Loss: 0.00001318
Iteration 182/1000 | Loss: 0.00001318
Iteration 183/1000 | Loss: 0.00001318
Iteration 184/1000 | Loss: 0.00001318
Iteration 185/1000 | Loss: 0.00001318
Iteration 186/1000 | Loss: 0.00001318
Iteration 187/1000 | Loss: 0.00001318
Iteration 188/1000 | Loss: 0.00001317
Iteration 189/1000 | Loss: 0.00001317
Iteration 190/1000 | Loss: 0.00001317
Iteration 191/1000 | Loss: 0.00001317
Iteration 192/1000 | Loss: 0.00001316
Iteration 193/1000 | Loss: 0.00001316
Iteration 194/1000 | Loss: 0.00001316
Iteration 195/1000 | Loss: 0.00001316
Iteration 196/1000 | Loss: 0.00001316
Iteration 197/1000 | Loss: 0.00001316
Iteration 198/1000 | Loss: 0.00001315
Iteration 199/1000 | Loss: 0.00001315
Iteration 200/1000 | Loss: 0.00001314
Iteration 201/1000 | Loss: 0.00001314
Iteration 202/1000 | Loss: 0.00001314
Iteration 203/1000 | Loss: 0.00001314
Iteration 204/1000 | Loss: 0.00001314
Iteration 205/1000 | Loss: 0.00001313
Iteration 206/1000 | Loss: 0.00001313
Iteration 207/1000 | Loss: 0.00001313
Iteration 208/1000 | Loss: 0.00001313
Iteration 209/1000 | Loss: 0.00001313
Iteration 210/1000 | Loss: 0.00001313
Iteration 211/1000 | Loss: 0.00001313
Iteration 212/1000 | Loss: 0.00001313
Iteration 213/1000 | Loss: 0.00001313
Iteration 214/1000 | Loss: 0.00001312
Iteration 215/1000 | Loss: 0.00001312
Iteration 216/1000 | Loss: 0.00001312
Iteration 217/1000 | Loss: 0.00001311
Iteration 218/1000 | Loss: 0.00001311
Iteration 219/1000 | Loss: 0.00001311
Iteration 220/1000 | Loss: 0.00001311
Iteration 221/1000 | Loss: 0.00001311
Iteration 222/1000 | Loss: 0.00001311
Iteration 223/1000 | Loss: 0.00001311
Iteration 224/1000 | Loss: 0.00001311
Iteration 225/1000 | Loss: 0.00001311
Iteration 226/1000 | Loss: 0.00001311
Iteration 227/1000 | Loss: 0.00001310
Iteration 228/1000 | Loss: 0.00001310
Iteration 229/1000 | Loss: 0.00001310
Iteration 230/1000 | Loss: 0.00001310
Iteration 231/1000 | Loss: 0.00001310
Iteration 232/1000 | Loss: 0.00001310
Iteration 233/1000 | Loss: 0.00001310
Iteration 234/1000 | Loss: 0.00001310
Iteration 235/1000 | Loss: 0.00001310
Iteration 236/1000 | Loss: 0.00001309
Iteration 237/1000 | Loss: 0.00001309
Iteration 238/1000 | Loss: 0.00001309
Iteration 239/1000 | Loss: 0.00001309
Iteration 240/1000 | Loss: 0.00001309
Iteration 241/1000 | Loss: 0.00001309
Iteration 242/1000 | Loss: 0.00001309
Iteration 243/1000 | Loss: 0.00001309
Iteration 244/1000 | Loss: 0.00001309
Iteration 245/1000 | Loss: 0.00001309
Iteration 246/1000 | Loss: 0.00001308
Iteration 247/1000 | Loss: 0.00001308
Iteration 248/1000 | Loss: 0.00001308
Iteration 249/1000 | Loss: 0.00001308
Iteration 250/1000 | Loss: 0.00001308
Iteration 251/1000 | Loss: 0.00001308
Iteration 252/1000 | Loss: 0.00001308
Iteration 253/1000 | Loss: 0.00001308
Iteration 254/1000 | Loss: 0.00001308
Iteration 255/1000 | Loss: 0.00001307
Iteration 256/1000 | Loss: 0.00001307
Iteration 257/1000 | Loss: 0.00001307
Iteration 258/1000 | Loss: 0.00001307
Iteration 259/1000 | Loss: 0.00001307
Iteration 260/1000 | Loss: 0.00001306
Iteration 261/1000 | Loss: 0.00001306
Iteration 262/1000 | Loss: 0.00001306
Iteration 263/1000 | Loss: 0.00001306
Iteration 264/1000 | Loss: 0.00001306
Iteration 265/1000 | Loss: 0.00001306
Iteration 266/1000 | Loss: 0.00001306
Iteration 267/1000 | Loss: 0.00001306
Iteration 268/1000 | Loss: 0.00001306
Iteration 269/1000 | Loss: 0.00001306
Iteration 270/1000 | Loss: 0.00001306
Iteration 271/1000 | Loss: 0.00001306
Iteration 272/1000 | Loss: 0.00001305
Iteration 273/1000 | Loss: 0.00001305
Iteration 274/1000 | Loss: 0.00001305
Iteration 275/1000 | Loss: 0.00001305
Iteration 276/1000 | Loss: 0.00001305
Iteration 277/1000 | Loss: 0.00001305
Iteration 278/1000 | Loss: 0.00001305
Iteration 279/1000 | Loss: 0.00001305
Iteration 280/1000 | Loss: 0.00001305
Iteration 281/1000 | Loss: 0.00001305
Iteration 282/1000 | Loss: 0.00001304
Iteration 283/1000 | Loss: 0.00001304
Iteration 284/1000 | Loss: 0.00001304
Iteration 285/1000 | Loss: 0.00001304
Iteration 286/1000 | Loss: 0.00001304
Iteration 287/1000 | Loss: 0.00001304
Iteration 288/1000 | Loss: 0.00001304
Iteration 289/1000 | Loss: 0.00001304
Iteration 290/1000 | Loss: 0.00001304
Iteration 291/1000 | Loss: 0.00001304
Iteration 292/1000 | Loss: 0.00001304
Iteration 293/1000 | Loss: 0.00001304
Iteration 294/1000 | Loss: 0.00001304
Iteration 295/1000 | Loss: 0.00001304
Iteration 296/1000 | Loss: 0.00001303
Iteration 297/1000 | Loss: 0.00001303
Iteration 298/1000 | Loss: 0.00001303
Iteration 299/1000 | Loss: 0.00001303
Iteration 300/1000 | Loss: 0.00001303
Iteration 301/1000 | Loss: 0.00001303
Iteration 302/1000 | Loss: 0.00001303
Iteration 303/1000 | Loss: 0.00001303
Iteration 304/1000 | Loss: 0.00001303
Iteration 305/1000 | Loss: 0.00001303
Iteration 306/1000 | Loss: 0.00001302
Iteration 307/1000 | Loss: 0.00001302
Iteration 308/1000 | Loss: 0.00001302
Iteration 309/1000 | Loss: 0.00001302
Iteration 310/1000 | Loss: 0.00001302
Iteration 311/1000 | Loss: 0.00001302
Iteration 312/1000 | Loss: 0.00001302
Iteration 313/1000 | Loss: 0.00001302
Iteration 314/1000 | Loss: 0.00001302
Iteration 315/1000 | Loss: 0.00001302
Iteration 316/1000 | Loss: 0.00001302
Iteration 317/1000 | Loss: 0.00001302
Iteration 318/1000 | Loss: 0.00001302
Iteration 319/1000 | Loss: 0.00001302
Iteration 320/1000 | Loss: 0.00001302
Iteration 321/1000 | Loss: 0.00001302
Iteration 322/1000 | Loss: 0.00001302
Iteration 323/1000 | Loss: 0.00001302
Iteration 324/1000 | Loss: 0.00001302
Iteration 325/1000 | Loss: 0.00001302
Iteration 326/1000 | Loss: 0.00001302
Iteration 327/1000 | Loss: 0.00001302
Iteration 328/1000 | Loss: 0.00001302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 328. Stopping optimization.
Last 5 losses: [1.301975134992972e-05, 1.301975134992972e-05, 1.301975134992972e-05, 1.301975134992972e-05, 1.301975134992972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.301975134992972e-05

Optimization complete. Final v2v error: 3.0283191204071045 mm

Highest mean error: 5.937158584594727 mm for frame 31

Lowest mean error: 2.7485384941101074 mm for frame 85

Saving results

Total time: 116.3477292060852
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930593
Iteration 2/25 | Loss: 0.00140017
Iteration 3/25 | Loss: 0.00087931
Iteration 4/25 | Loss: 0.00076181
Iteration 5/25 | Loss: 0.00075550
Iteration 6/25 | Loss: 0.00075453
Iteration 7/25 | Loss: 0.00075453
Iteration 8/25 | Loss: 0.00075453
Iteration 9/25 | Loss: 0.00075453
Iteration 10/25 | Loss: 0.00075453
Iteration 11/25 | Loss: 0.00075453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000754532462451607, 0.000754532462451607, 0.000754532462451607, 0.000754532462451607, 0.000754532462451607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000754532462451607

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45168662
Iteration 2/25 | Loss: 0.00033890
Iteration 3/25 | Loss: 0.00033889
Iteration 4/25 | Loss: 0.00033889
Iteration 5/25 | Loss: 0.00033889
Iteration 6/25 | Loss: 0.00033889
Iteration 7/25 | Loss: 0.00033889
Iteration 8/25 | Loss: 0.00033889
Iteration 9/25 | Loss: 0.00033889
Iteration 10/25 | Loss: 0.00033889
Iteration 11/25 | Loss: 0.00033889
Iteration 12/25 | Loss: 0.00033889
Iteration 13/25 | Loss: 0.00033889
Iteration 14/25 | Loss: 0.00033889
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00033889253973029554, 0.00033889253973029554, 0.00033889253973029554, 0.00033889253973029554, 0.00033889253973029554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033889253973029554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033889
Iteration 2/1000 | Loss: 0.00004123
Iteration 3/1000 | Loss: 0.00003148
Iteration 4/1000 | Loss: 0.00002516
Iteration 5/1000 | Loss: 0.00002343
Iteration 6/1000 | Loss: 0.00002244
Iteration 7/1000 | Loss: 0.00002193
Iteration 8/1000 | Loss: 0.00002151
Iteration 9/1000 | Loss: 0.00002108
Iteration 10/1000 | Loss: 0.00002070
Iteration 11/1000 | Loss: 0.00002033
Iteration 12/1000 | Loss: 0.00002006
Iteration 13/1000 | Loss: 0.00001981
Iteration 14/1000 | Loss: 0.00001958
Iteration 15/1000 | Loss: 0.00001955
Iteration 16/1000 | Loss: 0.00001953
Iteration 17/1000 | Loss: 0.00001948
Iteration 18/1000 | Loss: 0.00001947
Iteration 19/1000 | Loss: 0.00001940
Iteration 20/1000 | Loss: 0.00001938
Iteration 21/1000 | Loss: 0.00001935
Iteration 22/1000 | Loss: 0.00001931
Iteration 23/1000 | Loss: 0.00001930
Iteration 24/1000 | Loss: 0.00001928
Iteration 25/1000 | Loss: 0.00001928
Iteration 26/1000 | Loss: 0.00001927
Iteration 27/1000 | Loss: 0.00001927
Iteration 28/1000 | Loss: 0.00001927
Iteration 29/1000 | Loss: 0.00001927
Iteration 30/1000 | Loss: 0.00001926
Iteration 31/1000 | Loss: 0.00001924
Iteration 32/1000 | Loss: 0.00001924
Iteration 33/1000 | Loss: 0.00001924
Iteration 34/1000 | Loss: 0.00001923
Iteration 35/1000 | Loss: 0.00001923
Iteration 36/1000 | Loss: 0.00001923
Iteration 37/1000 | Loss: 0.00001923
Iteration 38/1000 | Loss: 0.00001923
Iteration 39/1000 | Loss: 0.00001922
Iteration 40/1000 | Loss: 0.00001922
Iteration 41/1000 | Loss: 0.00001922
Iteration 42/1000 | Loss: 0.00001922
Iteration 43/1000 | Loss: 0.00001921
Iteration 44/1000 | Loss: 0.00001921
Iteration 45/1000 | Loss: 0.00001921
Iteration 46/1000 | Loss: 0.00001921
Iteration 47/1000 | Loss: 0.00001921
Iteration 48/1000 | Loss: 0.00001921
Iteration 49/1000 | Loss: 0.00001921
Iteration 50/1000 | Loss: 0.00001921
Iteration 51/1000 | Loss: 0.00001920
Iteration 52/1000 | Loss: 0.00001919
Iteration 53/1000 | Loss: 0.00001919
Iteration 54/1000 | Loss: 0.00001919
Iteration 55/1000 | Loss: 0.00001919
Iteration 56/1000 | Loss: 0.00001919
Iteration 57/1000 | Loss: 0.00001919
Iteration 58/1000 | Loss: 0.00001919
Iteration 59/1000 | Loss: 0.00001919
Iteration 60/1000 | Loss: 0.00001919
Iteration 61/1000 | Loss: 0.00001919
Iteration 62/1000 | Loss: 0.00001919
Iteration 63/1000 | Loss: 0.00001919
Iteration 64/1000 | Loss: 0.00001919
Iteration 65/1000 | Loss: 0.00001919
Iteration 66/1000 | Loss: 0.00001919
Iteration 67/1000 | Loss: 0.00001918
Iteration 68/1000 | Loss: 0.00001918
Iteration 69/1000 | Loss: 0.00001918
Iteration 70/1000 | Loss: 0.00001918
Iteration 71/1000 | Loss: 0.00001918
Iteration 72/1000 | Loss: 0.00001918
Iteration 73/1000 | Loss: 0.00001918
Iteration 74/1000 | Loss: 0.00001918
Iteration 75/1000 | Loss: 0.00001918
Iteration 76/1000 | Loss: 0.00001918
Iteration 77/1000 | Loss: 0.00001918
Iteration 78/1000 | Loss: 0.00001918
Iteration 79/1000 | Loss: 0.00001918
Iteration 80/1000 | Loss: 0.00001918
Iteration 81/1000 | Loss: 0.00001917
Iteration 82/1000 | Loss: 0.00001917
Iteration 83/1000 | Loss: 0.00001917
Iteration 84/1000 | Loss: 0.00001917
Iteration 85/1000 | Loss: 0.00001917
Iteration 86/1000 | Loss: 0.00001917
Iteration 87/1000 | Loss: 0.00001917
Iteration 88/1000 | Loss: 0.00001917
Iteration 89/1000 | Loss: 0.00001917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.917468398460187e-05, 1.917468398460187e-05, 1.917468398460187e-05, 1.917468398460187e-05, 1.917468398460187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.917468398460187e-05

Optimization complete. Final v2v error: 3.669013500213623 mm

Highest mean error: 3.8444113731384277 mm for frame 112

Lowest mean error: 3.518904685974121 mm for frame 69

Saving results

Total time: 36.456281900405884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377218
Iteration 2/25 | Loss: 0.00074517
Iteration 3/25 | Loss: 0.00061295
Iteration 4/25 | Loss: 0.00059221
Iteration 5/25 | Loss: 0.00058520
Iteration 6/25 | Loss: 0.00058295
Iteration 7/25 | Loss: 0.00058247
Iteration 8/25 | Loss: 0.00058247
Iteration 9/25 | Loss: 0.00058247
Iteration 10/25 | Loss: 0.00058247
Iteration 11/25 | Loss: 0.00058247
Iteration 12/25 | Loss: 0.00058247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005824749823659658, 0.0005824749823659658, 0.0005824749823659658, 0.0005824749823659658, 0.0005824749823659658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005824749823659658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45253158
Iteration 2/25 | Loss: 0.00024904
Iteration 3/25 | Loss: 0.00024903
Iteration 4/25 | Loss: 0.00024903
Iteration 5/25 | Loss: 0.00024903
Iteration 6/25 | Loss: 0.00024903
Iteration 7/25 | Loss: 0.00024903
Iteration 8/25 | Loss: 0.00024903
Iteration 9/25 | Loss: 0.00024903
Iteration 10/25 | Loss: 0.00024903
Iteration 11/25 | Loss: 0.00024903
Iteration 12/25 | Loss: 0.00024903
Iteration 13/25 | Loss: 0.00024903
Iteration 14/25 | Loss: 0.00024903
Iteration 15/25 | Loss: 0.00024903
Iteration 16/25 | Loss: 0.00024903
Iteration 17/25 | Loss: 0.00024903
Iteration 18/25 | Loss: 0.00024903
Iteration 19/25 | Loss: 0.00024903
Iteration 20/25 | Loss: 0.00024903
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00024902919540181756, 0.00024902919540181756, 0.00024902919540181756, 0.00024902919540181756, 0.00024902919540181756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024902919540181756

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024903
Iteration 2/1000 | Loss: 0.00002227
Iteration 3/1000 | Loss: 0.00001650
Iteration 4/1000 | Loss: 0.00001517
Iteration 5/1000 | Loss: 0.00001419
Iteration 6/1000 | Loss: 0.00001371
Iteration 7/1000 | Loss: 0.00001360
Iteration 8/1000 | Loss: 0.00001327
Iteration 9/1000 | Loss: 0.00001312
Iteration 10/1000 | Loss: 0.00001290
Iteration 11/1000 | Loss: 0.00001290
Iteration 12/1000 | Loss: 0.00001285
Iteration 13/1000 | Loss: 0.00001278
Iteration 14/1000 | Loss: 0.00001270
Iteration 15/1000 | Loss: 0.00001263
Iteration 16/1000 | Loss: 0.00001262
Iteration 17/1000 | Loss: 0.00001257
Iteration 18/1000 | Loss: 0.00001254
Iteration 19/1000 | Loss: 0.00001252
Iteration 20/1000 | Loss: 0.00001251
Iteration 21/1000 | Loss: 0.00001249
Iteration 22/1000 | Loss: 0.00001249
Iteration 23/1000 | Loss: 0.00001249
Iteration 24/1000 | Loss: 0.00001249
Iteration 25/1000 | Loss: 0.00001248
Iteration 26/1000 | Loss: 0.00001248
Iteration 27/1000 | Loss: 0.00001248
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001248
Iteration 30/1000 | Loss: 0.00001246
Iteration 31/1000 | Loss: 0.00001246
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001245
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001244
Iteration 39/1000 | Loss: 0.00001244
Iteration 40/1000 | Loss: 0.00001244
Iteration 41/1000 | Loss: 0.00001244
Iteration 42/1000 | Loss: 0.00001243
Iteration 43/1000 | Loss: 0.00001243
Iteration 44/1000 | Loss: 0.00001243
Iteration 45/1000 | Loss: 0.00001242
Iteration 46/1000 | Loss: 0.00001242
Iteration 47/1000 | Loss: 0.00001242
Iteration 48/1000 | Loss: 0.00001241
Iteration 49/1000 | Loss: 0.00001241
Iteration 50/1000 | Loss: 0.00001241
Iteration 51/1000 | Loss: 0.00001240
Iteration 52/1000 | Loss: 0.00001240
Iteration 53/1000 | Loss: 0.00001239
Iteration 54/1000 | Loss: 0.00001239
Iteration 55/1000 | Loss: 0.00001238
Iteration 56/1000 | Loss: 0.00001238
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001237
Iteration 61/1000 | Loss: 0.00001237
Iteration 62/1000 | Loss: 0.00001237
Iteration 63/1000 | Loss: 0.00001236
Iteration 64/1000 | Loss: 0.00001236
Iteration 65/1000 | Loss: 0.00001236
Iteration 66/1000 | Loss: 0.00001236
Iteration 67/1000 | Loss: 0.00001236
Iteration 68/1000 | Loss: 0.00001235
Iteration 69/1000 | Loss: 0.00001235
Iteration 70/1000 | Loss: 0.00001235
Iteration 71/1000 | Loss: 0.00001235
Iteration 72/1000 | Loss: 0.00001235
Iteration 73/1000 | Loss: 0.00001234
Iteration 74/1000 | Loss: 0.00001234
Iteration 75/1000 | Loss: 0.00001234
Iteration 76/1000 | Loss: 0.00001234
Iteration 77/1000 | Loss: 0.00001234
Iteration 78/1000 | Loss: 0.00001233
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001232
Iteration 81/1000 | Loss: 0.00001232
Iteration 82/1000 | Loss: 0.00001232
Iteration 83/1000 | Loss: 0.00001232
Iteration 84/1000 | Loss: 0.00001231
Iteration 85/1000 | Loss: 0.00001231
Iteration 86/1000 | Loss: 0.00001231
Iteration 87/1000 | Loss: 0.00001231
Iteration 88/1000 | Loss: 0.00001231
Iteration 89/1000 | Loss: 0.00001231
Iteration 90/1000 | Loss: 0.00001231
Iteration 91/1000 | Loss: 0.00001231
Iteration 92/1000 | Loss: 0.00001231
Iteration 93/1000 | Loss: 0.00001231
Iteration 94/1000 | Loss: 0.00001230
Iteration 95/1000 | Loss: 0.00001230
Iteration 96/1000 | Loss: 0.00001229
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001228
Iteration 100/1000 | Loss: 0.00001228
Iteration 101/1000 | Loss: 0.00001228
Iteration 102/1000 | Loss: 0.00001228
Iteration 103/1000 | Loss: 0.00001228
Iteration 104/1000 | Loss: 0.00001228
Iteration 105/1000 | Loss: 0.00001228
Iteration 106/1000 | Loss: 0.00001228
Iteration 107/1000 | Loss: 0.00001228
Iteration 108/1000 | Loss: 0.00001228
Iteration 109/1000 | Loss: 0.00001228
Iteration 110/1000 | Loss: 0.00001228
Iteration 111/1000 | Loss: 0.00001227
Iteration 112/1000 | Loss: 0.00001227
Iteration 113/1000 | Loss: 0.00001227
Iteration 114/1000 | Loss: 0.00001227
Iteration 115/1000 | Loss: 0.00001227
Iteration 116/1000 | Loss: 0.00001227
Iteration 117/1000 | Loss: 0.00001227
Iteration 118/1000 | Loss: 0.00001227
Iteration 119/1000 | Loss: 0.00001226
Iteration 120/1000 | Loss: 0.00001226
Iteration 121/1000 | Loss: 0.00001226
Iteration 122/1000 | Loss: 0.00001226
Iteration 123/1000 | Loss: 0.00001225
Iteration 124/1000 | Loss: 0.00001225
Iteration 125/1000 | Loss: 0.00001225
Iteration 126/1000 | Loss: 0.00001225
Iteration 127/1000 | Loss: 0.00001225
Iteration 128/1000 | Loss: 0.00001225
Iteration 129/1000 | Loss: 0.00001225
Iteration 130/1000 | Loss: 0.00001225
Iteration 131/1000 | Loss: 0.00001225
Iteration 132/1000 | Loss: 0.00001225
Iteration 133/1000 | Loss: 0.00001225
Iteration 134/1000 | Loss: 0.00001225
Iteration 135/1000 | Loss: 0.00001225
Iteration 136/1000 | Loss: 0.00001225
Iteration 137/1000 | Loss: 0.00001225
Iteration 138/1000 | Loss: 0.00001225
Iteration 139/1000 | Loss: 0.00001225
Iteration 140/1000 | Loss: 0.00001225
Iteration 141/1000 | Loss: 0.00001225
Iteration 142/1000 | Loss: 0.00001225
Iteration 143/1000 | Loss: 0.00001225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.2251990483491682e-05, 1.2251990483491682e-05, 1.2251990483491682e-05, 1.2251990483491682e-05, 1.2251990483491682e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2251990483491682e-05

Optimization complete. Final v2v error: 2.9452617168426514 mm

Highest mean error: 3.142833948135376 mm for frame 150

Lowest mean error: 2.8105976581573486 mm for frame 257

Saving results

Total time: 42.19812297821045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839933
Iteration 2/25 | Loss: 0.00114881
Iteration 3/25 | Loss: 0.00080573
Iteration 4/25 | Loss: 0.00073404
Iteration 5/25 | Loss: 0.00071090
Iteration 6/25 | Loss: 0.00070580
Iteration 7/25 | Loss: 0.00070465
Iteration 8/25 | Loss: 0.00070458
Iteration 9/25 | Loss: 0.00070458
Iteration 10/25 | Loss: 0.00070458
Iteration 11/25 | Loss: 0.00070458
Iteration 12/25 | Loss: 0.00070458
Iteration 13/25 | Loss: 0.00070458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007045783568173647, 0.0007045783568173647, 0.0007045783568173647, 0.0007045783568173647, 0.0007045783568173647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007045783568173647

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41801751
Iteration 2/25 | Loss: 0.00034553
Iteration 3/25 | Loss: 0.00034552
Iteration 4/25 | Loss: 0.00034552
Iteration 5/25 | Loss: 0.00034552
Iteration 6/25 | Loss: 0.00034552
Iteration 7/25 | Loss: 0.00034552
Iteration 8/25 | Loss: 0.00034552
Iteration 9/25 | Loss: 0.00034552
Iteration 10/25 | Loss: 0.00034552
Iteration 11/25 | Loss: 0.00034552
Iteration 12/25 | Loss: 0.00034552
Iteration 13/25 | Loss: 0.00034552
Iteration 14/25 | Loss: 0.00034552
Iteration 15/25 | Loss: 0.00034552
Iteration 16/25 | Loss: 0.00034552
Iteration 17/25 | Loss: 0.00034552
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00034552018041722476, 0.00034552018041722476, 0.00034552018041722476, 0.00034552018041722476, 0.00034552018041722476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034552018041722476

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034552
Iteration 2/1000 | Loss: 0.00004091
Iteration 3/1000 | Loss: 0.00002955
Iteration 4/1000 | Loss: 0.00002566
Iteration 5/1000 | Loss: 0.00002435
Iteration 6/1000 | Loss: 0.00002337
Iteration 7/1000 | Loss: 0.00002253
Iteration 8/1000 | Loss: 0.00002206
Iteration 9/1000 | Loss: 0.00002154
Iteration 10/1000 | Loss: 0.00002120
Iteration 11/1000 | Loss: 0.00002098
Iteration 12/1000 | Loss: 0.00002098
Iteration 13/1000 | Loss: 0.00002095
Iteration 14/1000 | Loss: 0.00002086
Iteration 15/1000 | Loss: 0.00002070
Iteration 16/1000 | Loss: 0.00002060
Iteration 17/1000 | Loss: 0.00002056
Iteration 18/1000 | Loss: 0.00002055
Iteration 19/1000 | Loss: 0.00002055
Iteration 20/1000 | Loss: 0.00002054
Iteration 21/1000 | Loss: 0.00002054
Iteration 22/1000 | Loss: 0.00002053
Iteration 23/1000 | Loss: 0.00002052
Iteration 24/1000 | Loss: 0.00002051
Iteration 25/1000 | Loss: 0.00002051
Iteration 26/1000 | Loss: 0.00002051
Iteration 27/1000 | Loss: 0.00002051
Iteration 28/1000 | Loss: 0.00002050
Iteration 29/1000 | Loss: 0.00002050
Iteration 30/1000 | Loss: 0.00002050
Iteration 31/1000 | Loss: 0.00002049
Iteration 32/1000 | Loss: 0.00002049
Iteration 33/1000 | Loss: 0.00002048
Iteration 34/1000 | Loss: 0.00002048
Iteration 35/1000 | Loss: 0.00002047
Iteration 36/1000 | Loss: 0.00002047
Iteration 37/1000 | Loss: 0.00002047
Iteration 38/1000 | Loss: 0.00002045
Iteration 39/1000 | Loss: 0.00002045
Iteration 40/1000 | Loss: 0.00002044
Iteration 41/1000 | Loss: 0.00002044
Iteration 42/1000 | Loss: 0.00002042
Iteration 43/1000 | Loss: 0.00002041
Iteration 44/1000 | Loss: 0.00002041
Iteration 45/1000 | Loss: 0.00002041
Iteration 46/1000 | Loss: 0.00002041
Iteration 47/1000 | Loss: 0.00002041
Iteration 48/1000 | Loss: 0.00002041
Iteration 49/1000 | Loss: 0.00002041
Iteration 50/1000 | Loss: 0.00002041
Iteration 51/1000 | Loss: 0.00002041
Iteration 52/1000 | Loss: 0.00002040
Iteration 53/1000 | Loss: 0.00002039
Iteration 54/1000 | Loss: 0.00002039
Iteration 55/1000 | Loss: 0.00002039
Iteration 56/1000 | Loss: 0.00002038
Iteration 57/1000 | Loss: 0.00002038
Iteration 58/1000 | Loss: 0.00002038
Iteration 59/1000 | Loss: 0.00002038
Iteration 60/1000 | Loss: 0.00002037
Iteration 61/1000 | Loss: 0.00002037
Iteration 62/1000 | Loss: 0.00002037
Iteration 63/1000 | Loss: 0.00002037
Iteration 64/1000 | Loss: 0.00002037
Iteration 65/1000 | Loss: 0.00002037
Iteration 66/1000 | Loss: 0.00002036
Iteration 67/1000 | Loss: 0.00002036
Iteration 68/1000 | Loss: 0.00002036
Iteration 69/1000 | Loss: 0.00002036
Iteration 70/1000 | Loss: 0.00002036
Iteration 71/1000 | Loss: 0.00002036
Iteration 72/1000 | Loss: 0.00002036
Iteration 73/1000 | Loss: 0.00002036
Iteration 74/1000 | Loss: 0.00002036
Iteration 75/1000 | Loss: 0.00002036
Iteration 76/1000 | Loss: 0.00002035
Iteration 77/1000 | Loss: 0.00002035
Iteration 78/1000 | Loss: 0.00002035
Iteration 79/1000 | Loss: 0.00002035
Iteration 80/1000 | Loss: 0.00002035
Iteration 81/1000 | Loss: 0.00002034
Iteration 82/1000 | Loss: 0.00002034
Iteration 83/1000 | Loss: 0.00002034
Iteration 84/1000 | Loss: 0.00002034
Iteration 85/1000 | Loss: 0.00002034
Iteration 86/1000 | Loss: 0.00002034
Iteration 87/1000 | Loss: 0.00002034
Iteration 88/1000 | Loss: 0.00002033
Iteration 89/1000 | Loss: 0.00002033
Iteration 90/1000 | Loss: 0.00002033
Iteration 91/1000 | Loss: 0.00002033
Iteration 92/1000 | Loss: 0.00002033
Iteration 93/1000 | Loss: 0.00002032
Iteration 94/1000 | Loss: 0.00002032
Iteration 95/1000 | Loss: 0.00002032
Iteration 96/1000 | Loss: 0.00002032
Iteration 97/1000 | Loss: 0.00002032
Iteration 98/1000 | Loss: 0.00002032
Iteration 99/1000 | Loss: 0.00002032
Iteration 100/1000 | Loss: 0.00002032
Iteration 101/1000 | Loss: 0.00002031
Iteration 102/1000 | Loss: 0.00002031
Iteration 103/1000 | Loss: 0.00002031
Iteration 104/1000 | Loss: 0.00002031
Iteration 105/1000 | Loss: 0.00002031
Iteration 106/1000 | Loss: 0.00002031
Iteration 107/1000 | Loss: 0.00002031
Iteration 108/1000 | Loss: 0.00002031
Iteration 109/1000 | Loss: 0.00002031
Iteration 110/1000 | Loss: 0.00002031
Iteration 111/1000 | Loss: 0.00002030
Iteration 112/1000 | Loss: 0.00002030
Iteration 113/1000 | Loss: 0.00002030
Iteration 114/1000 | Loss: 0.00002030
Iteration 115/1000 | Loss: 0.00002030
Iteration 116/1000 | Loss: 0.00002030
Iteration 117/1000 | Loss: 0.00002030
Iteration 118/1000 | Loss: 0.00002030
Iteration 119/1000 | Loss: 0.00002030
Iteration 120/1000 | Loss: 0.00002030
Iteration 121/1000 | Loss: 0.00002030
Iteration 122/1000 | Loss: 0.00002029
Iteration 123/1000 | Loss: 0.00002029
Iteration 124/1000 | Loss: 0.00002029
Iteration 125/1000 | Loss: 0.00002029
Iteration 126/1000 | Loss: 0.00002029
Iteration 127/1000 | Loss: 0.00002029
Iteration 128/1000 | Loss: 0.00002029
Iteration 129/1000 | Loss: 0.00002029
Iteration 130/1000 | Loss: 0.00002029
Iteration 131/1000 | Loss: 0.00002028
Iteration 132/1000 | Loss: 0.00002028
Iteration 133/1000 | Loss: 0.00002028
Iteration 134/1000 | Loss: 0.00002028
Iteration 135/1000 | Loss: 0.00002028
Iteration 136/1000 | Loss: 0.00002028
Iteration 137/1000 | Loss: 0.00002028
Iteration 138/1000 | Loss: 0.00002028
Iteration 139/1000 | Loss: 0.00002028
Iteration 140/1000 | Loss: 0.00002028
Iteration 141/1000 | Loss: 0.00002028
Iteration 142/1000 | Loss: 0.00002028
Iteration 143/1000 | Loss: 0.00002028
Iteration 144/1000 | Loss: 0.00002028
Iteration 145/1000 | Loss: 0.00002028
Iteration 146/1000 | Loss: 0.00002028
Iteration 147/1000 | Loss: 0.00002028
Iteration 148/1000 | Loss: 0.00002028
Iteration 149/1000 | Loss: 0.00002028
Iteration 150/1000 | Loss: 0.00002028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.0275530914659612e-05, 2.0275530914659612e-05, 2.0275530914659612e-05, 2.0275530914659612e-05, 2.0275530914659612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0275530914659612e-05

Optimization complete. Final v2v error: 3.7908518314361572 mm

Highest mean error: 4.994045257568359 mm for frame 151

Lowest mean error: 3.260667324066162 mm for frame 34

Saving results

Total time: 46.37570667266846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808856
Iteration 2/25 | Loss: 0.00115425
Iteration 3/25 | Loss: 0.00077380
Iteration 4/25 | Loss: 0.00070723
Iteration 5/25 | Loss: 0.00069229
Iteration 6/25 | Loss: 0.00068128
Iteration 7/25 | Loss: 0.00068709
Iteration 8/25 | Loss: 0.00067732
Iteration 9/25 | Loss: 0.00066781
Iteration 10/25 | Loss: 0.00066343
Iteration 11/25 | Loss: 0.00066124
Iteration 12/25 | Loss: 0.00065998
Iteration 13/25 | Loss: 0.00066292
Iteration 14/25 | Loss: 0.00066227
Iteration 15/25 | Loss: 0.00066000
Iteration 16/25 | Loss: 0.00065889
Iteration 17/25 | Loss: 0.00065801
Iteration 18/25 | Loss: 0.00065740
Iteration 19/25 | Loss: 0.00065691
Iteration 20/25 | Loss: 0.00065672
Iteration 21/25 | Loss: 0.00065659
Iteration 22/25 | Loss: 0.00065657
Iteration 23/25 | Loss: 0.00065655
Iteration 24/25 | Loss: 0.00065654
Iteration 25/25 | Loss: 0.00065654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72999763
Iteration 2/25 | Loss: 0.00030972
Iteration 3/25 | Loss: 0.00030972
Iteration 4/25 | Loss: 0.00030972
Iteration 5/25 | Loss: 0.00030972
Iteration 6/25 | Loss: 0.00030972
Iteration 7/25 | Loss: 0.00030972
Iteration 8/25 | Loss: 0.00030972
Iteration 9/25 | Loss: 0.00030972
Iteration 10/25 | Loss: 0.00030972
Iteration 11/25 | Loss: 0.00030972
Iteration 12/25 | Loss: 0.00030972
Iteration 13/25 | Loss: 0.00030972
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00030971853993833065, 0.00030971853993833065, 0.00030971853993833065, 0.00030971853993833065, 0.00030971853993833065]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030971853993833065

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030972
Iteration 2/1000 | Loss: 0.00002573
Iteration 3/1000 | Loss: 0.00004215
Iteration 4/1000 | Loss: 0.00001941
Iteration 5/1000 | Loss: 0.00001855
Iteration 6/1000 | Loss: 0.00001816
Iteration 7/1000 | Loss: 0.00001780
Iteration 8/1000 | Loss: 0.00001775
Iteration 9/1000 | Loss: 0.00001762
Iteration 10/1000 | Loss: 0.00001752
Iteration 11/1000 | Loss: 0.00001748
Iteration 12/1000 | Loss: 0.00001738
Iteration 13/1000 | Loss: 0.00001736
Iteration 14/1000 | Loss: 0.00001736
Iteration 15/1000 | Loss: 0.00001735
Iteration 16/1000 | Loss: 0.00001734
Iteration 17/1000 | Loss: 0.00001734
Iteration 18/1000 | Loss: 0.00001730
Iteration 19/1000 | Loss: 0.00001726
Iteration 20/1000 | Loss: 0.00001726
Iteration 21/1000 | Loss: 0.00001726
Iteration 22/1000 | Loss: 0.00001722
Iteration 23/1000 | Loss: 0.00001719
Iteration 24/1000 | Loss: 0.00001716
Iteration 25/1000 | Loss: 0.00001714
Iteration 26/1000 | Loss: 0.00001714
Iteration 27/1000 | Loss: 0.00001713
Iteration 28/1000 | Loss: 0.00001713
Iteration 29/1000 | Loss: 0.00001712
Iteration 30/1000 | Loss: 0.00004376
Iteration 31/1000 | Loss: 0.00002066
Iteration 32/1000 | Loss: 0.00001706
Iteration 33/1000 | Loss: 0.00001706
Iteration 34/1000 | Loss: 0.00001705
Iteration 35/1000 | Loss: 0.00001705
Iteration 36/1000 | Loss: 0.00001705
Iteration 37/1000 | Loss: 0.00001705
Iteration 38/1000 | Loss: 0.00001705
Iteration 39/1000 | Loss: 0.00001705
Iteration 40/1000 | Loss: 0.00001705
Iteration 41/1000 | Loss: 0.00001705
Iteration 42/1000 | Loss: 0.00001705
Iteration 43/1000 | Loss: 0.00001705
Iteration 44/1000 | Loss: 0.00001705
Iteration 45/1000 | Loss: 0.00001704
Iteration 46/1000 | Loss: 0.00001704
Iteration 47/1000 | Loss: 0.00001704
Iteration 48/1000 | Loss: 0.00001704
Iteration 49/1000 | Loss: 0.00001704
Iteration 50/1000 | Loss: 0.00001704
Iteration 51/1000 | Loss: 0.00001704
Iteration 52/1000 | Loss: 0.00001704
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001704
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001703
Iteration 57/1000 | Loss: 0.00001703
Iteration 58/1000 | Loss: 0.00001703
Iteration 59/1000 | Loss: 0.00001703
Iteration 60/1000 | Loss: 0.00001703
Iteration 61/1000 | Loss: 0.00001702
Iteration 62/1000 | Loss: 0.00001702
Iteration 63/1000 | Loss: 0.00001701
Iteration 64/1000 | Loss: 0.00001701
Iteration 65/1000 | Loss: 0.00001700
Iteration 66/1000 | Loss: 0.00001700
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001699
Iteration 69/1000 | Loss: 0.00001699
Iteration 70/1000 | Loss: 0.00001699
Iteration 71/1000 | Loss: 0.00001698
Iteration 72/1000 | Loss: 0.00001698
Iteration 73/1000 | Loss: 0.00001698
Iteration 74/1000 | Loss: 0.00001698
Iteration 75/1000 | Loss: 0.00001697
Iteration 76/1000 | Loss: 0.00001697
Iteration 77/1000 | Loss: 0.00001697
Iteration 78/1000 | Loss: 0.00001697
Iteration 79/1000 | Loss: 0.00001697
Iteration 80/1000 | Loss: 0.00001696
Iteration 81/1000 | Loss: 0.00001696
Iteration 82/1000 | Loss: 0.00001696
Iteration 83/1000 | Loss: 0.00001696
Iteration 84/1000 | Loss: 0.00001696
Iteration 85/1000 | Loss: 0.00001695
Iteration 86/1000 | Loss: 0.00001695
Iteration 87/1000 | Loss: 0.00001695
Iteration 88/1000 | Loss: 0.00001695
Iteration 89/1000 | Loss: 0.00001694
Iteration 90/1000 | Loss: 0.00001694
Iteration 91/1000 | Loss: 0.00001694
Iteration 92/1000 | Loss: 0.00001693
Iteration 93/1000 | Loss: 0.00001693
Iteration 94/1000 | Loss: 0.00001693
Iteration 95/1000 | Loss: 0.00001693
Iteration 96/1000 | Loss: 0.00001693
Iteration 97/1000 | Loss: 0.00001692
Iteration 98/1000 | Loss: 0.00001692
Iteration 99/1000 | Loss: 0.00001692
Iteration 100/1000 | Loss: 0.00001692
Iteration 101/1000 | Loss: 0.00001692
Iteration 102/1000 | Loss: 0.00001692
Iteration 103/1000 | Loss: 0.00001692
Iteration 104/1000 | Loss: 0.00001692
Iteration 105/1000 | Loss: 0.00001692
Iteration 106/1000 | Loss: 0.00001692
Iteration 107/1000 | Loss: 0.00001692
Iteration 108/1000 | Loss: 0.00001692
Iteration 109/1000 | Loss: 0.00001692
Iteration 110/1000 | Loss: 0.00001692
Iteration 111/1000 | Loss: 0.00001692
Iteration 112/1000 | Loss: 0.00001692
Iteration 113/1000 | Loss: 0.00001692
Iteration 114/1000 | Loss: 0.00001692
Iteration 115/1000 | Loss: 0.00001692
Iteration 116/1000 | Loss: 0.00001692
Iteration 117/1000 | Loss: 0.00001692
Iteration 118/1000 | Loss: 0.00001692
Iteration 119/1000 | Loss: 0.00001692
Iteration 120/1000 | Loss: 0.00001692
Iteration 121/1000 | Loss: 0.00001692
Iteration 122/1000 | Loss: 0.00001692
Iteration 123/1000 | Loss: 0.00001692
Iteration 124/1000 | Loss: 0.00001692
Iteration 125/1000 | Loss: 0.00001692
Iteration 126/1000 | Loss: 0.00001692
Iteration 127/1000 | Loss: 0.00001692
Iteration 128/1000 | Loss: 0.00001692
Iteration 129/1000 | Loss: 0.00001692
Iteration 130/1000 | Loss: 0.00001692
Iteration 131/1000 | Loss: 0.00001692
Iteration 132/1000 | Loss: 0.00001692
Iteration 133/1000 | Loss: 0.00001692
Iteration 134/1000 | Loss: 0.00001692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.691593752184417e-05, 1.691593752184417e-05, 1.691593752184417e-05, 1.691593752184417e-05, 1.691593752184417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.691593752184417e-05

Optimization complete. Final v2v error: 3.4363980293273926 mm

Highest mean error: 4.279043197631836 mm for frame 118

Lowest mean error: 2.948899984359741 mm for frame 78

Saving results

Total time: 64.43523979187012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081629
Iteration 2/25 | Loss: 0.00157932
Iteration 3/25 | Loss: 0.00115081
Iteration 4/25 | Loss: 0.00076377
Iteration 5/25 | Loss: 0.00067561
Iteration 6/25 | Loss: 0.00069834
Iteration 7/25 | Loss: 0.00067005
Iteration 8/25 | Loss: 0.00065240
Iteration 9/25 | Loss: 0.00063710
Iteration 10/25 | Loss: 0.00063060
Iteration 11/25 | Loss: 0.00063759
Iteration 12/25 | Loss: 0.00062128
Iteration 13/25 | Loss: 0.00062032
Iteration 14/25 | Loss: 0.00062008
Iteration 15/25 | Loss: 0.00062003
Iteration 16/25 | Loss: 0.00062003
Iteration 17/25 | Loss: 0.00062003
Iteration 18/25 | Loss: 0.00062003
Iteration 19/25 | Loss: 0.00062003
Iteration 20/25 | Loss: 0.00062002
Iteration 21/25 | Loss: 0.00062002
Iteration 22/25 | Loss: 0.00062002
Iteration 23/25 | Loss: 0.00062002
Iteration 24/25 | Loss: 0.00062002
Iteration 25/25 | Loss: 0.00062002

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66423035
Iteration 2/25 | Loss: 0.00034972
Iteration 3/25 | Loss: 0.00027754
Iteration 4/25 | Loss: 0.00027754
Iteration 5/25 | Loss: 0.00027754
Iteration 6/25 | Loss: 0.00027754
Iteration 7/25 | Loss: 0.00027754
Iteration 8/25 | Loss: 0.00027754
Iteration 9/25 | Loss: 0.00027753
Iteration 10/25 | Loss: 0.00027753
Iteration 11/25 | Loss: 0.00027753
Iteration 12/25 | Loss: 0.00027753
Iteration 13/25 | Loss: 0.00027753
Iteration 14/25 | Loss: 0.00027753
Iteration 15/25 | Loss: 0.00027753
Iteration 16/25 | Loss: 0.00027753
Iteration 17/25 | Loss: 0.00027753
Iteration 18/25 | Loss: 0.00027753
Iteration 19/25 | Loss: 0.00027753
Iteration 20/25 | Loss: 0.00027753
Iteration 21/25 | Loss: 0.00027753
Iteration 22/25 | Loss: 0.00027753
Iteration 23/25 | Loss: 0.00027753
Iteration 24/25 | Loss: 0.00027753
Iteration 25/25 | Loss: 0.00027753

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027753
Iteration 2/1000 | Loss: 0.00002447
Iteration 3/1000 | Loss: 0.00001790
Iteration 4/1000 | Loss: 0.00001668
Iteration 5/1000 | Loss: 0.00001594
Iteration 6/1000 | Loss: 0.00001547
Iteration 7/1000 | Loss: 0.00001510
Iteration 8/1000 | Loss: 0.00001484
Iteration 9/1000 | Loss: 0.00001480
Iteration 10/1000 | Loss: 0.00001463
Iteration 11/1000 | Loss: 0.00001461
Iteration 12/1000 | Loss: 0.00001458
Iteration 13/1000 | Loss: 0.00001458
Iteration 14/1000 | Loss: 0.00001452
Iteration 15/1000 | Loss: 0.00001451
Iteration 16/1000 | Loss: 0.00001450
Iteration 17/1000 | Loss: 0.00001445
Iteration 18/1000 | Loss: 0.00001444
Iteration 19/1000 | Loss: 0.00001438
Iteration 20/1000 | Loss: 0.00001437
Iteration 21/1000 | Loss: 0.00001436
Iteration 22/1000 | Loss: 0.00001436
Iteration 23/1000 | Loss: 0.00001436
Iteration 24/1000 | Loss: 0.00001435
Iteration 25/1000 | Loss: 0.00001435
Iteration 26/1000 | Loss: 0.00001434
Iteration 27/1000 | Loss: 0.00001434
Iteration 28/1000 | Loss: 0.00001433
Iteration 29/1000 | Loss: 0.00001433
Iteration 30/1000 | Loss: 0.00001433
Iteration 31/1000 | Loss: 0.00001432
Iteration 32/1000 | Loss: 0.00001432
Iteration 33/1000 | Loss: 0.00001432
Iteration 34/1000 | Loss: 0.00001431
Iteration 35/1000 | Loss: 0.00001430
Iteration 36/1000 | Loss: 0.00001430
Iteration 37/1000 | Loss: 0.00001430
Iteration 38/1000 | Loss: 0.00001430
Iteration 39/1000 | Loss: 0.00001429
Iteration 40/1000 | Loss: 0.00001427
Iteration 41/1000 | Loss: 0.00001427
Iteration 42/1000 | Loss: 0.00001426
Iteration 43/1000 | Loss: 0.00001426
Iteration 44/1000 | Loss: 0.00001426
Iteration 45/1000 | Loss: 0.00001426
Iteration 46/1000 | Loss: 0.00001426
Iteration 47/1000 | Loss: 0.00001425
Iteration 48/1000 | Loss: 0.00001425
Iteration 49/1000 | Loss: 0.00001425
Iteration 50/1000 | Loss: 0.00001421
Iteration 51/1000 | Loss: 0.00001420
Iteration 52/1000 | Loss: 0.00001419
Iteration 53/1000 | Loss: 0.00001419
Iteration 54/1000 | Loss: 0.00001419
Iteration 55/1000 | Loss: 0.00001418
Iteration 56/1000 | Loss: 0.00001416
Iteration 57/1000 | Loss: 0.00001416
Iteration 58/1000 | Loss: 0.00001416
Iteration 59/1000 | Loss: 0.00001416
Iteration 60/1000 | Loss: 0.00001416
Iteration 61/1000 | Loss: 0.00001415
Iteration 62/1000 | Loss: 0.00001415
Iteration 63/1000 | Loss: 0.00001415
Iteration 64/1000 | Loss: 0.00001415
Iteration 65/1000 | Loss: 0.00001415
Iteration 66/1000 | Loss: 0.00001415
Iteration 67/1000 | Loss: 0.00001415
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00001415
Iteration 70/1000 | Loss: 0.00001414
Iteration 71/1000 | Loss: 0.00001410
Iteration 72/1000 | Loss: 0.00001410
Iteration 73/1000 | Loss: 0.00001409
Iteration 74/1000 | Loss: 0.00001409
Iteration 75/1000 | Loss: 0.00001409
Iteration 76/1000 | Loss: 0.00001409
Iteration 77/1000 | Loss: 0.00001409
Iteration 78/1000 | Loss: 0.00001409
Iteration 79/1000 | Loss: 0.00001409
Iteration 80/1000 | Loss: 0.00001409
Iteration 81/1000 | Loss: 0.00001408
Iteration 82/1000 | Loss: 0.00001406
Iteration 83/1000 | Loss: 0.00001406
Iteration 84/1000 | Loss: 0.00001406
Iteration 85/1000 | Loss: 0.00001406
Iteration 86/1000 | Loss: 0.00001406
Iteration 87/1000 | Loss: 0.00001406
Iteration 88/1000 | Loss: 0.00001406
Iteration 89/1000 | Loss: 0.00001406
Iteration 90/1000 | Loss: 0.00001406
Iteration 91/1000 | Loss: 0.00001406
Iteration 92/1000 | Loss: 0.00001406
Iteration 93/1000 | Loss: 0.00001406
Iteration 94/1000 | Loss: 0.00001406
Iteration 95/1000 | Loss: 0.00001405
Iteration 96/1000 | Loss: 0.00001405
Iteration 97/1000 | Loss: 0.00001404
Iteration 98/1000 | Loss: 0.00001404
Iteration 99/1000 | Loss: 0.00001404
Iteration 100/1000 | Loss: 0.00001403
Iteration 101/1000 | Loss: 0.00001403
Iteration 102/1000 | Loss: 0.00001403
Iteration 103/1000 | Loss: 0.00001403
Iteration 104/1000 | Loss: 0.00001402
Iteration 105/1000 | Loss: 0.00001402
Iteration 106/1000 | Loss: 0.00001402
Iteration 107/1000 | Loss: 0.00001402
Iteration 108/1000 | Loss: 0.00001401
Iteration 109/1000 | Loss: 0.00001401
Iteration 110/1000 | Loss: 0.00001401
Iteration 111/1000 | Loss: 0.00001401
Iteration 112/1000 | Loss: 0.00001401
Iteration 113/1000 | Loss: 0.00001401
Iteration 114/1000 | Loss: 0.00001401
Iteration 115/1000 | Loss: 0.00001400
Iteration 116/1000 | Loss: 0.00001400
Iteration 117/1000 | Loss: 0.00001400
Iteration 118/1000 | Loss: 0.00001400
Iteration 119/1000 | Loss: 0.00001400
Iteration 120/1000 | Loss: 0.00001400
Iteration 121/1000 | Loss: 0.00001400
Iteration 122/1000 | Loss: 0.00001400
Iteration 123/1000 | Loss: 0.00001400
Iteration 124/1000 | Loss: 0.00001399
Iteration 125/1000 | Loss: 0.00001399
Iteration 126/1000 | Loss: 0.00001399
Iteration 127/1000 | Loss: 0.00001399
Iteration 128/1000 | Loss: 0.00001399
Iteration 129/1000 | Loss: 0.00001399
Iteration 130/1000 | Loss: 0.00001399
Iteration 131/1000 | Loss: 0.00001399
Iteration 132/1000 | Loss: 0.00001398
Iteration 133/1000 | Loss: 0.00001398
Iteration 134/1000 | Loss: 0.00001398
Iteration 135/1000 | Loss: 0.00001398
Iteration 136/1000 | Loss: 0.00001398
Iteration 137/1000 | Loss: 0.00001398
Iteration 138/1000 | Loss: 0.00001398
Iteration 139/1000 | Loss: 0.00001398
Iteration 140/1000 | Loss: 0.00001398
Iteration 141/1000 | Loss: 0.00001398
Iteration 142/1000 | Loss: 0.00001398
Iteration 143/1000 | Loss: 0.00001398
Iteration 144/1000 | Loss: 0.00001398
Iteration 145/1000 | Loss: 0.00001398
Iteration 146/1000 | Loss: 0.00001397
Iteration 147/1000 | Loss: 0.00001397
Iteration 148/1000 | Loss: 0.00001397
Iteration 149/1000 | Loss: 0.00001397
Iteration 150/1000 | Loss: 0.00001397
Iteration 151/1000 | Loss: 0.00001397
Iteration 152/1000 | Loss: 0.00001397
Iteration 153/1000 | Loss: 0.00001397
Iteration 154/1000 | Loss: 0.00001397
Iteration 155/1000 | Loss: 0.00001397
Iteration 156/1000 | Loss: 0.00001397
Iteration 157/1000 | Loss: 0.00001396
Iteration 158/1000 | Loss: 0.00001396
Iteration 159/1000 | Loss: 0.00001396
Iteration 160/1000 | Loss: 0.00001396
Iteration 161/1000 | Loss: 0.00001396
Iteration 162/1000 | Loss: 0.00001396
Iteration 163/1000 | Loss: 0.00001396
Iteration 164/1000 | Loss: 0.00001396
Iteration 165/1000 | Loss: 0.00001396
Iteration 166/1000 | Loss: 0.00001396
Iteration 167/1000 | Loss: 0.00001396
Iteration 168/1000 | Loss: 0.00001396
Iteration 169/1000 | Loss: 0.00001396
Iteration 170/1000 | Loss: 0.00001396
Iteration 171/1000 | Loss: 0.00001396
Iteration 172/1000 | Loss: 0.00001396
Iteration 173/1000 | Loss: 0.00001396
Iteration 174/1000 | Loss: 0.00001396
Iteration 175/1000 | Loss: 0.00001396
Iteration 176/1000 | Loss: 0.00001396
Iteration 177/1000 | Loss: 0.00001396
Iteration 178/1000 | Loss: 0.00001396
Iteration 179/1000 | Loss: 0.00001396
Iteration 180/1000 | Loss: 0.00001396
Iteration 181/1000 | Loss: 0.00001396
Iteration 182/1000 | Loss: 0.00001396
Iteration 183/1000 | Loss: 0.00001396
Iteration 184/1000 | Loss: 0.00001396
Iteration 185/1000 | Loss: 0.00001396
Iteration 186/1000 | Loss: 0.00001396
Iteration 187/1000 | Loss: 0.00001396
Iteration 188/1000 | Loss: 0.00001396
Iteration 189/1000 | Loss: 0.00001396
Iteration 190/1000 | Loss: 0.00001396
Iteration 191/1000 | Loss: 0.00001396
Iteration 192/1000 | Loss: 0.00001396
Iteration 193/1000 | Loss: 0.00001396
Iteration 194/1000 | Loss: 0.00001396
Iteration 195/1000 | Loss: 0.00001396
Iteration 196/1000 | Loss: 0.00001396
Iteration 197/1000 | Loss: 0.00001396
Iteration 198/1000 | Loss: 0.00001396
Iteration 199/1000 | Loss: 0.00001396
Iteration 200/1000 | Loss: 0.00001396
Iteration 201/1000 | Loss: 0.00001396
Iteration 202/1000 | Loss: 0.00001396
Iteration 203/1000 | Loss: 0.00001396
Iteration 204/1000 | Loss: 0.00001396
Iteration 205/1000 | Loss: 0.00001396
Iteration 206/1000 | Loss: 0.00001396
Iteration 207/1000 | Loss: 0.00001396
Iteration 208/1000 | Loss: 0.00001396
Iteration 209/1000 | Loss: 0.00001396
Iteration 210/1000 | Loss: 0.00001396
Iteration 211/1000 | Loss: 0.00001396
Iteration 212/1000 | Loss: 0.00001396
Iteration 213/1000 | Loss: 0.00001396
Iteration 214/1000 | Loss: 0.00001396
Iteration 215/1000 | Loss: 0.00001396
Iteration 216/1000 | Loss: 0.00001396
Iteration 217/1000 | Loss: 0.00001396
Iteration 218/1000 | Loss: 0.00001396
Iteration 219/1000 | Loss: 0.00001396
Iteration 220/1000 | Loss: 0.00001396
Iteration 221/1000 | Loss: 0.00001396
Iteration 222/1000 | Loss: 0.00001396
Iteration 223/1000 | Loss: 0.00001396
Iteration 224/1000 | Loss: 0.00001396
Iteration 225/1000 | Loss: 0.00001396
Iteration 226/1000 | Loss: 0.00001396
Iteration 227/1000 | Loss: 0.00001396
Iteration 228/1000 | Loss: 0.00001396
Iteration 229/1000 | Loss: 0.00001396
Iteration 230/1000 | Loss: 0.00001396
Iteration 231/1000 | Loss: 0.00001396
Iteration 232/1000 | Loss: 0.00001396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.3955758731754031e-05, 1.3955758731754031e-05, 1.3955758731754031e-05, 1.3955758731754031e-05, 1.3955758731754031e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3955758731754031e-05

Optimization complete. Final v2v error: 3.1625776290893555 mm

Highest mean error: 3.3321139812469482 mm for frame 4

Lowest mean error: 2.960096836090088 mm for frame 52

Saving results

Total time: 55.55975914001465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405493
Iteration 2/25 | Loss: 0.00088490
Iteration 3/25 | Loss: 0.00068834
Iteration 4/25 | Loss: 0.00064832
Iteration 5/25 | Loss: 0.00064032
Iteration 6/25 | Loss: 0.00063809
Iteration 7/25 | Loss: 0.00063752
Iteration 8/25 | Loss: 0.00063734
Iteration 9/25 | Loss: 0.00063734
Iteration 10/25 | Loss: 0.00063734
Iteration 11/25 | Loss: 0.00063734
Iteration 12/25 | Loss: 0.00063734
Iteration 13/25 | Loss: 0.00063734
Iteration 14/25 | Loss: 0.00063734
Iteration 15/25 | Loss: 0.00063734
Iteration 16/25 | Loss: 0.00063734
Iteration 17/25 | Loss: 0.00063734
Iteration 18/25 | Loss: 0.00063734
Iteration 19/25 | Loss: 0.00063734
Iteration 20/25 | Loss: 0.00063734
Iteration 21/25 | Loss: 0.00063734
Iteration 22/25 | Loss: 0.00063734
Iteration 23/25 | Loss: 0.00063734
Iteration 24/25 | Loss: 0.00063734
Iteration 25/25 | Loss: 0.00063734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006373352953232825, 0.0006373352953232825, 0.0006373352953232825, 0.0006373352953232825, 0.0006373352953232825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006373352953232825

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50636053
Iteration 2/25 | Loss: 0.00031695
Iteration 3/25 | Loss: 0.00031694
Iteration 4/25 | Loss: 0.00031694
Iteration 5/25 | Loss: 0.00031694
Iteration 6/25 | Loss: 0.00031694
Iteration 7/25 | Loss: 0.00031694
Iteration 8/25 | Loss: 0.00031694
Iteration 9/25 | Loss: 0.00031694
Iteration 10/25 | Loss: 0.00031694
Iteration 11/25 | Loss: 0.00031694
Iteration 12/25 | Loss: 0.00031694
Iteration 13/25 | Loss: 0.00031694
Iteration 14/25 | Loss: 0.00031694
Iteration 15/25 | Loss: 0.00031694
Iteration 16/25 | Loss: 0.00031694
Iteration 17/25 | Loss: 0.00031693
Iteration 18/25 | Loss: 0.00031694
Iteration 19/25 | Loss: 0.00031694
Iteration 20/25 | Loss: 0.00031694
Iteration 21/25 | Loss: 0.00031694
Iteration 22/25 | Loss: 0.00031694
Iteration 23/25 | Loss: 0.00031694
Iteration 24/25 | Loss: 0.00031694
Iteration 25/25 | Loss: 0.00031694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031694
Iteration 2/1000 | Loss: 0.00003515
Iteration 3/1000 | Loss: 0.00002358
Iteration 4/1000 | Loss: 0.00001984
Iteration 5/1000 | Loss: 0.00001900
Iteration 6/1000 | Loss: 0.00001819
Iteration 7/1000 | Loss: 0.00001758
Iteration 8/1000 | Loss: 0.00001713
Iteration 9/1000 | Loss: 0.00001679
Iteration 10/1000 | Loss: 0.00001663
Iteration 11/1000 | Loss: 0.00001652
Iteration 12/1000 | Loss: 0.00001645
Iteration 13/1000 | Loss: 0.00001643
Iteration 14/1000 | Loss: 0.00001641
Iteration 15/1000 | Loss: 0.00001641
Iteration 16/1000 | Loss: 0.00001640
Iteration 17/1000 | Loss: 0.00001640
Iteration 18/1000 | Loss: 0.00001639
Iteration 19/1000 | Loss: 0.00001638
Iteration 20/1000 | Loss: 0.00001638
Iteration 21/1000 | Loss: 0.00001637
Iteration 22/1000 | Loss: 0.00001637
Iteration 23/1000 | Loss: 0.00001636
Iteration 24/1000 | Loss: 0.00001636
Iteration 25/1000 | Loss: 0.00001635
Iteration 26/1000 | Loss: 0.00001635
Iteration 27/1000 | Loss: 0.00001634
Iteration 28/1000 | Loss: 0.00001634
Iteration 29/1000 | Loss: 0.00001633
Iteration 30/1000 | Loss: 0.00001633
Iteration 31/1000 | Loss: 0.00001632
Iteration 32/1000 | Loss: 0.00001630
Iteration 33/1000 | Loss: 0.00001629
Iteration 34/1000 | Loss: 0.00001629
Iteration 35/1000 | Loss: 0.00001629
Iteration 36/1000 | Loss: 0.00001628
Iteration 37/1000 | Loss: 0.00001628
Iteration 38/1000 | Loss: 0.00001627
Iteration 39/1000 | Loss: 0.00001627
Iteration 40/1000 | Loss: 0.00001626
Iteration 41/1000 | Loss: 0.00001626
Iteration 42/1000 | Loss: 0.00001626
Iteration 43/1000 | Loss: 0.00001626
Iteration 44/1000 | Loss: 0.00001625
Iteration 45/1000 | Loss: 0.00001625
Iteration 46/1000 | Loss: 0.00001624
Iteration 47/1000 | Loss: 0.00001624
Iteration 48/1000 | Loss: 0.00001623
Iteration 49/1000 | Loss: 0.00001623
Iteration 50/1000 | Loss: 0.00001623
Iteration 51/1000 | Loss: 0.00001623
Iteration 52/1000 | Loss: 0.00001623
Iteration 53/1000 | Loss: 0.00001623
Iteration 54/1000 | Loss: 0.00001623
Iteration 55/1000 | Loss: 0.00001623
Iteration 56/1000 | Loss: 0.00001623
Iteration 57/1000 | Loss: 0.00001623
Iteration 58/1000 | Loss: 0.00001623
Iteration 59/1000 | Loss: 0.00001623
Iteration 60/1000 | Loss: 0.00001623
Iteration 61/1000 | Loss: 0.00001622
Iteration 62/1000 | Loss: 0.00001622
Iteration 63/1000 | Loss: 0.00001621
Iteration 64/1000 | Loss: 0.00001621
Iteration 65/1000 | Loss: 0.00001621
Iteration 66/1000 | Loss: 0.00001621
Iteration 67/1000 | Loss: 0.00001620
Iteration 68/1000 | Loss: 0.00001620
Iteration 69/1000 | Loss: 0.00001620
Iteration 70/1000 | Loss: 0.00001620
Iteration 71/1000 | Loss: 0.00001619
Iteration 72/1000 | Loss: 0.00001619
Iteration 73/1000 | Loss: 0.00001618
Iteration 74/1000 | Loss: 0.00001618
Iteration 75/1000 | Loss: 0.00001618
Iteration 76/1000 | Loss: 0.00001617
Iteration 77/1000 | Loss: 0.00001617
Iteration 78/1000 | Loss: 0.00001617
Iteration 79/1000 | Loss: 0.00001617
Iteration 80/1000 | Loss: 0.00001616
Iteration 81/1000 | Loss: 0.00001616
Iteration 82/1000 | Loss: 0.00001615
Iteration 83/1000 | Loss: 0.00001615
Iteration 84/1000 | Loss: 0.00001614
Iteration 85/1000 | Loss: 0.00001614
Iteration 86/1000 | Loss: 0.00001614
Iteration 87/1000 | Loss: 0.00001614
Iteration 88/1000 | Loss: 0.00001614
Iteration 89/1000 | Loss: 0.00001614
Iteration 90/1000 | Loss: 0.00001613
Iteration 91/1000 | Loss: 0.00001613
Iteration 92/1000 | Loss: 0.00001613
Iteration 93/1000 | Loss: 0.00001612
Iteration 94/1000 | Loss: 0.00001612
Iteration 95/1000 | Loss: 0.00001612
Iteration 96/1000 | Loss: 0.00001612
Iteration 97/1000 | Loss: 0.00001611
Iteration 98/1000 | Loss: 0.00001611
Iteration 99/1000 | Loss: 0.00001611
Iteration 100/1000 | Loss: 0.00001611
Iteration 101/1000 | Loss: 0.00001611
Iteration 102/1000 | Loss: 0.00001611
Iteration 103/1000 | Loss: 0.00001610
Iteration 104/1000 | Loss: 0.00001610
Iteration 105/1000 | Loss: 0.00001610
Iteration 106/1000 | Loss: 0.00001610
Iteration 107/1000 | Loss: 0.00001609
Iteration 108/1000 | Loss: 0.00001609
Iteration 109/1000 | Loss: 0.00001609
Iteration 110/1000 | Loss: 0.00001609
Iteration 111/1000 | Loss: 0.00001609
Iteration 112/1000 | Loss: 0.00001608
Iteration 113/1000 | Loss: 0.00001608
Iteration 114/1000 | Loss: 0.00001608
Iteration 115/1000 | Loss: 0.00001608
Iteration 116/1000 | Loss: 0.00001607
Iteration 117/1000 | Loss: 0.00001607
Iteration 118/1000 | Loss: 0.00001607
Iteration 119/1000 | Loss: 0.00001607
Iteration 120/1000 | Loss: 0.00001607
Iteration 121/1000 | Loss: 0.00001607
Iteration 122/1000 | Loss: 0.00001606
Iteration 123/1000 | Loss: 0.00001606
Iteration 124/1000 | Loss: 0.00001606
Iteration 125/1000 | Loss: 0.00001606
Iteration 126/1000 | Loss: 0.00001606
Iteration 127/1000 | Loss: 0.00001605
Iteration 128/1000 | Loss: 0.00001605
Iteration 129/1000 | Loss: 0.00001605
Iteration 130/1000 | Loss: 0.00001605
Iteration 131/1000 | Loss: 0.00001604
Iteration 132/1000 | Loss: 0.00001604
Iteration 133/1000 | Loss: 0.00001604
Iteration 134/1000 | Loss: 0.00001604
Iteration 135/1000 | Loss: 0.00001604
Iteration 136/1000 | Loss: 0.00001604
Iteration 137/1000 | Loss: 0.00001603
Iteration 138/1000 | Loss: 0.00001603
Iteration 139/1000 | Loss: 0.00001603
Iteration 140/1000 | Loss: 0.00001603
Iteration 141/1000 | Loss: 0.00001603
Iteration 142/1000 | Loss: 0.00001603
Iteration 143/1000 | Loss: 0.00001603
Iteration 144/1000 | Loss: 0.00001603
Iteration 145/1000 | Loss: 0.00001603
Iteration 146/1000 | Loss: 0.00001603
Iteration 147/1000 | Loss: 0.00001603
Iteration 148/1000 | Loss: 0.00001603
Iteration 149/1000 | Loss: 0.00001603
Iteration 150/1000 | Loss: 0.00001602
Iteration 151/1000 | Loss: 0.00001602
Iteration 152/1000 | Loss: 0.00001602
Iteration 153/1000 | Loss: 0.00001602
Iteration 154/1000 | Loss: 0.00001602
Iteration 155/1000 | Loss: 0.00001602
Iteration 156/1000 | Loss: 0.00001602
Iteration 157/1000 | Loss: 0.00001602
Iteration 158/1000 | Loss: 0.00001602
Iteration 159/1000 | Loss: 0.00001602
Iteration 160/1000 | Loss: 0.00001602
Iteration 161/1000 | Loss: 0.00001602
Iteration 162/1000 | Loss: 0.00001602
Iteration 163/1000 | Loss: 0.00001602
Iteration 164/1000 | Loss: 0.00001602
Iteration 165/1000 | Loss: 0.00001602
Iteration 166/1000 | Loss: 0.00001602
Iteration 167/1000 | Loss: 0.00001601
Iteration 168/1000 | Loss: 0.00001601
Iteration 169/1000 | Loss: 0.00001601
Iteration 170/1000 | Loss: 0.00001601
Iteration 171/1000 | Loss: 0.00001601
Iteration 172/1000 | Loss: 0.00001601
Iteration 173/1000 | Loss: 0.00001601
Iteration 174/1000 | Loss: 0.00001601
Iteration 175/1000 | Loss: 0.00001601
Iteration 176/1000 | Loss: 0.00001600
Iteration 177/1000 | Loss: 0.00001600
Iteration 178/1000 | Loss: 0.00001600
Iteration 179/1000 | Loss: 0.00001600
Iteration 180/1000 | Loss: 0.00001600
Iteration 181/1000 | Loss: 0.00001600
Iteration 182/1000 | Loss: 0.00001600
Iteration 183/1000 | Loss: 0.00001600
Iteration 184/1000 | Loss: 0.00001600
Iteration 185/1000 | Loss: 0.00001600
Iteration 186/1000 | Loss: 0.00001600
Iteration 187/1000 | Loss: 0.00001600
Iteration 188/1000 | Loss: 0.00001600
Iteration 189/1000 | Loss: 0.00001600
Iteration 190/1000 | Loss: 0.00001600
Iteration 191/1000 | Loss: 0.00001600
Iteration 192/1000 | Loss: 0.00001600
Iteration 193/1000 | Loss: 0.00001600
Iteration 194/1000 | Loss: 0.00001600
Iteration 195/1000 | Loss: 0.00001600
Iteration 196/1000 | Loss: 0.00001600
Iteration 197/1000 | Loss: 0.00001600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.600305040483363e-05, 1.600305040483363e-05, 1.600305040483363e-05, 1.600305040483363e-05, 1.600305040483363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.600305040483363e-05

Optimization complete. Final v2v error: 3.3774867057800293 mm

Highest mean error: 4.331173896789551 mm for frame 46

Lowest mean error: 2.955230236053467 mm for frame 92

Saving results

Total time: 41.18178963661194
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382940
Iteration 2/25 | Loss: 0.00076363
Iteration 3/25 | Loss: 0.00066279
Iteration 4/25 | Loss: 0.00063841
Iteration 5/25 | Loss: 0.00063196
Iteration 6/25 | Loss: 0.00063072
Iteration 7/25 | Loss: 0.00063067
Iteration 8/25 | Loss: 0.00063067
Iteration 9/25 | Loss: 0.00063067
Iteration 10/25 | Loss: 0.00063067
Iteration 11/25 | Loss: 0.00063067
Iteration 12/25 | Loss: 0.00063067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006306666764430702, 0.0006306666764430702, 0.0006306666764430702, 0.0006306666764430702, 0.0006306666764430702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006306666764430702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58041620
Iteration 2/25 | Loss: 0.00027333
Iteration 3/25 | Loss: 0.00027333
Iteration 4/25 | Loss: 0.00027333
Iteration 5/25 | Loss: 0.00027333
Iteration 6/25 | Loss: 0.00027333
Iteration 7/25 | Loss: 0.00027333
Iteration 8/25 | Loss: 0.00027333
Iteration 9/25 | Loss: 0.00027333
Iteration 10/25 | Loss: 0.00027333
Iteration 11/25 | Loss: 0.00027333
Iteration 12/25 | Loss: 0.00027333
Iteration 13/25 | Loss: 0.00027333
Iteration 14/25 | Loss: 0.00027333
Iteration 15/25 | Loss: 0.00027333
Iteration 16/25 | Loss: 0.00027333
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00027332574245519936, 0.00027332574245519936, 0.00027332574245519936, 0.00027332574245519936, 0.00027332574245519936]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027332574245519936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027333
Iteration 2/1000 | Loss: 0.00002659
Iteration 3/1000 | Loss: 0.00002003
Iteration 4/1000 | Loss: 0.00001832
Iteration 5/1000 | Loss: 0.00001760
Iteration 6/1000 | Loss: 0.00001698
Iteration 7/1000 | Loss: 0.00001663
Iteration 8/1000 | Loss: 0.00001645
Iteration 9/1000 | Loss: 0.00001628
Iteration 10/1000 | Loss: 0.00001628
Iteration 11/1000 | Loss: 0.00001628
Iteration 12/1000 | Loss: 0.00001625
Iteration 13/1000 | Loss: 0.00001621
Iteration 14/1000 | Loss: 0.00001619
Iteration 15/1000 | Loss: 0.00001619
Iteration 16/1000 | Loss: 0.00001619
Iteration 17/1000 | Loss: 0.00001618
Iteration 18/1000 | Loss: 0.00001615
Iteration 19/1000 | Loss: 0.00001615
Iteration 20/1000 | Loss: 0.00001614
Iteration 21/1000 | Loss: 0.00001610
Iteration 22/1000 | Loss: 0.00001610
Iteration 23/1000 | Loss: 0.00001609
Iteration 24/1000 | Loss: 0.00001609
Iteration 25/1000 | Loss: 0.00001608
Iteration 26/1000 | Loss: 0.00001608
Iteration 27/1000 | Loss: 0.00001608
Iteration 28/1000 | Loss: 0.00001607
Iteration 29/1000 | Loss: 0.00001607
Iteration 30/1000 | Loss: 0.00001607
Iteration 31/1000 | Loss: 0.00001607
Iteration 32/1000 | Loss: 0.00001607
Iteration 33/1000 | Loss: 0.00001607
Iteration 34/1000 | Loss: 0.00001606
Iteration 35/1000 | Loss: 0.00001606
Iteration 36/1000 | Loss: 0.00001606
Iteration 37/1000 | Loss: 0.00001606
Iteration 38/1000 | Loss: 0.00001606
Iteration 39/1000 | Loss: 0.00001606
Iteration 40/1000 | Loss: 0.00001606
Iteration 41/1000 | Loss: 0.00001606
Iteration 42/1000 | Loss: 0.00001606
Iteration 43/1000 | Loss: 0.00001606
Iteration 44/1000 | Loss: 0.00001606
Iteration 45/1000 | Loss: 0.00001606
Iteration 46/1000 | Loss: 0.00001606
Iteration 47/1000 | Loss: 0.00001605
Iteration 48/1000 | Loss: 0.00001605
Iteration 49/1000 | Loss: 0.00001605
Iteration 50/1000 | Loss: 0.00001605
Iteration 51/1000 | Loss: 0.00001605
Iteration 52/1000 | Loss: 0.00001605
Iteration 53/1000 | Loss: 0.00001604
Iteration 54/1000 | Loss: 0.00001604
Iteration 55/1000 | Loss: 0.00001603
Iteration 56/1000 | Loss: 0.00001603
Iteration 57/1000 | Loss: 0.00001603
Iteration 58/1000 | Loss: 0.00001603
Iteration 59/1000 | Loss: 0.00001603
Iteration 60/1000 | Loss: 0.00001603
Iteration 61/1000 | Loss: 0.00001602
Iteration 62/1000 | Loss: 0.00001602
Iteration 63/1000 | Loss: 0.00001602
Iteration 64/1000 | Loss: 0.00001601
Iteration 65/1000 | Loss: 0.00001601
Iteration 66/1000 | Loss: 0.00001600
Iteration 67/1000 | Loss: 0.00001600
Iteration 68/1000 | Loss: 0.00001600
Iteration 69/1000 | Loss: 0.00001599
Iteration 70/1000 | Loss: 0.00001599
Iteration 71/1000 | Loss: 0.00001599
Iteration 72/1000 | Loss: 0.00001599
Iteration 73/1000 | Loss: 0.00001598
Iteration 74/1000 | Loss: 0.00001597
Iteration 75/1000 | Loss: 0.00001597
Iteration 76/1000 | Loss: 0.00001597
Iteration 77/1000 | Loss: 0.00001596
Iteration 78/1000 | Loss: 0.00001596
Iteration 79/1000 | Loss: 0.00001596
Iteration 80/1000 | Loss: 0.00001596
Iteration 81/1000 | Loss: 0.00001595
Iteration 82/1000 | Loss: 0.00001595
Iteration 83/1000 | Loss: 0.00001595
Iteration 84/1000 | Loss: 0.00001594
Iteration 85/1000 | Loss: 0.00001594
Iteration 86/1000 | Loss: 0.00001593
Iteration 87/1000 | Loss: 0.00001593
Iteration 88/1000 | Loss: 0.00001592
Iteration 89/1000 | Loss: 0.00001592
Iteration 90/1000 | Loss: 0.00001591
Iteration 91/1000 | Loss: 0.00001590
Iteration 92/1000 | Loss: 0.00001590
Iteration 93/1000 | Loss: 0.00001590
Iteration 94/1000 | Loss: 0.00001589
Iteration 95/1000 | Loss: 0.00001589
Iteration 96/1000 | Loss: 0.00001589
Iteration 97/1000 | Loss: 0.00001589
Iteration 98/1000 | Loss: 0.00001588
Iteration 99/1000 | Loss: 0.00001588
Iteration 100/1000 | Loss: 0.00001587
Iteration 101/1000 | Loss: 0.00001587
Iteration 102/1000 | Loss: 0.00001587
Iteration 103/1000 | Loss: 0.00001587
Iteration 104/1000 | Loss: 0.00001587
Iteration 105/1000 | Loss: 0.00001587
Iteration 106/1000 | Loss: 0.00001586
Iteration 107/1000 | Loss: 0.00001586
Iteration 108/1000 | Loss: 0.00001586
Iteration 109/1000 | Loss: 0.00001586
Iteration 110/1000 | Loss: 0.00001586
Iteration 111/1000 | Loss: 0.00001585
Iteration 112/1000 | Loss: 0.00001585
Iteration 113/1000 | Loss: 0.00001585
Iteration 114/1000 | Loss: 0.00001585
Iteration 115/1000 | Loss: 0.00001585
Iteration 116/1000 | Loss: 0.00001585
Iteration 117/1000 | Loss: 0.00001585
Iteration 118/1000 | Loss: 0.00001585
Iteration 119/1000 | Loss: 0.00001584
Iteration 120/1000 | Loss: 0.00001584
Iteration 121/1000 | Loss: 0.00001584
Iteration 122/1000 | Loss: 0.00001584
Iteration 123/1000 | Loss: 0.00001584
Iteration 124/1000 | Loss: 0.00001584
Iteration 125/1000 | Loss: 0.00001584
Iteration 126/1000 | Loss: 0.00001584
Iteration 127/1000 | Loss: 0.00001584
Iteration 128/1000 | Loss: 0.00001584
Iteration 129/1000 | Loss: 0.00001584
Iteration 130/1000 | Loss: 0.00001584
Iteration 131/1000 | Loss: 0.00001584
Iteration 132/1000 | Loss: 0.00001584
Iteration 133/1000 | Loss: 0.00001584
Iteration 134/1000 | Loss: 0.00001584
Iteration 135/1000 | Loss: 0.00001584
Iteration 136/1000 | Loss: 0.00001584
Iteration 137/1000 | Loss: 0.00001583
Iteration 138/1000 | Loss: 0.00001583
Iteration 139/1000 | Loss: 0.00001583
Iteration 140/1000 | Loss: 0.00001583
Iteration 141/1000 | Loss: 0.00001583
Iteration 142/1000 | Loss: 0.00001583
Iteration 143/1000 | Loss: 0.00001583
Iteration 144/1000 | Loss: 0.00001583
Iteration 145/1000 | Loss: 0.00001583
Iteration 146/1000 | Loss: 0.00001583
Iteration 147/1000 | Loss: 0.00001583
Iteration 148/1000 | Loss: 0.00001582
Iteration 149/1000 | Loss: 0.00001582
Iteration 150/1000 | Loss: 0.00001582
Iteration 151/1000 | Loss: 0.00001582
Iteration 152/1000 | Loss: 0.00001582
Iteration 153/1000 | Loss: 0.00001582
Iteration 154/1000 | Loss: 0.00001582
Iteration 155/1000 | Loss: 0.00001582
Iteration 156/1000 | Loss: 0.00001582
Iteration 157/1000 | Loss: 0.00001582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.5824061847524717e-05, 1.5824061847524717e-05, 1.5824061847524717e-05, 1.5824061847524717e-05, 1.5824061847524717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5824061847524717e-05

Optimization complete. Final v2v error: 3.3469152450561523 mm

Highest mean error: 3.6382548809051514 mm for frame 168

Lowest mean error: 3.1573452949523926 mm for frame 210

Saving results

Total time: 38.54589033126831
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421074
Iteration 2/25 | Loss: 0.00075461
Iteration 3/25 | Loss: 0.00064009
Iteration 4/25 | Loss: 0.00062046
Iteration 5/25 | Loss: 0.00061602
Iteration 6/25 | Loss: 0.00061516
Iteration 7/25 | Loss: 0.00061516
Iteration 8/25 | Loss: 0.00061516
Iteration 9/25 | Loss: 0.00061516
Iteration 10/25 | Loss: 0.00061516
Iteration 11/25 | Loss: 0.00061516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006151580600999296, 0.0006151580600999296, 0.0006151580600999296, 0.0006151580600999296, 0.0006151580600999296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006151580600999296

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.40776277
Iteration 2/25 | Loss: 0.00027671
Iteration 3/25 | Loss: 0.00027671
Iteration 4/25 | Loss: 0.00027671
Iteration 5/25 | Loss: 0.00027671
Iteration 6/25 | Loss: 0.00027671
Iteration 7/25 | Loss: 0.00027671
Iteration 8/25 | Loss: 0.00027671
Iteration 9/25 | Loss: 0.00027671
Iteration 10/25 | Loss: 0.00027671
Iteration 11/25 | Loss: 0.00027671
Iteration 12/25 | Loss: 0.00027671
Iteration 13/25 | Loss: 0.00027671
Iteration 14/25 | Loss: 0.00027671
Iteration 15/25 | Loss: 0.00027671
Iteration 16/25 | Loss: 0.00027671
Iteration 17/25 | Loss: 0.00027671
Iteration 18/25 | Loss: 0.00027671
Iteration 19/25 | Loss: 0.00027671
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00027670973213389516, 0.00027670973213389516, 0.00027670973213389516, 0.00027670973213389516, 0.00027670973213389516]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027670973213389516

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027671
Iteration 2/1000 | Loss: 0.00002478
Iteration 3/1000 | Loss: 0.00001727
Iteration 4/1000 | Loss: 0.00001633
Iteration 5/1000 | Loss: 0.00001538
Iteration 6/1000 | Loss: 0.00001497
Iteration 7/1000 | Loss: 0.00001457
Iteration 8/1000 | Loss: 0.00001434
Iteration 9/1000 | Loss: 0.00001420
Iteration 10/1000 | Loss: 0.00001419
Iteration 11/1000 | Loss: 0.00001418
Iteration 12/1000 | Loss: 0.00001417
Iteration 13/1000 | Loss: 0.00001406
Iteration 14/1000 | Loss: 0.00001406
Iteration 15/1000 | Loss: 0.00001403
Iteration 16/1000 | Loss: 0.00001403
Iteration 17/1000 | Loss: 0.00001402
Iteration 18/1000 | Loss: 0.00001400
Iteration 19/1000 | Loss: 0.00001398
Iteration 20/1000 | Loss: 0.00001397
Iteration 21/1000 | Loss: 0.00001397
Iteration 22/1000 | Loss: 0.00001397
Iteration 23/1000 | Loss: 0.00001388
Iteration 24/1000 | Loss: 0.00001388
Iteration 25/1000 | Loss: 0.00001387
Iteration 26/1000 | Loss: 0.00001386
Iteration 27/1000 | Loss: 0.00001383
Iteration 28/1000 | Loss: 0.00001383
Iteration 29/1000 | Loss: 0.00001383
Iteration 30/1000 | Loss: 0.00001383
Iteration 31/1000 | Loss: 0.00001383
Iteration 32/1000 | Loss: 0.00001382
Iteration 33/1000 | Loss: 0.00001382
Iteration 34/1000 | Loss: 0.00001381
Iteration 35/1000 | Loss: 0.00001381
Iteration 36/1000 | Loss: 0.00001381
Iteration 37/1000 | Loss: 0.00001380
Iteration 38/1000 | Loss: 0.00001380
Iteration 39/1000 | Loss: 0.00001380
Iteration 40/1000 | Loss: 0.00001379
Iteration 41/1000 | Loss: 0.00001379
Iteration 42/1000 | Loss: 0.00001378
Iteration 43/1000 | Loss: 0.00001378
Iteration 44/1000 | Loss: 0.00001377
Iteration 45/1000 | Loss: 0.00001377
Iteration 46/1000 | Loss: 0.00001377
Iteration 47/1000 | Loss: 0.00001377
Iteration 48/1000 | Loss: 0.00001377
Iteration 49/1000 | Loss: 0.00001377
Iteration 50/1000 | Loss: 0.00001376
Iteration 51/1000 | Loss: 0.00001376
Iteration 52/1000 | Loss: 0.00001376
Iteration 53/1000 | Loss: 0.00001376
Iteration 54/1000 | Loss: 0.00001375
Iteration 55/1000 | Loss: 0.00001375
Iteration 56/1000 | Loss: 0.00001374
Iteration 57/1000 | Loss: 0.00001373
Iteration 58/1000 | Loss: 0.00001373
Iteration 59/1000 | Loss: 0.00001373
Iteration 60/1000 | Loss: 0.00001372
Iteration 61/1000 | Loss: 0.00001372
Iteration 62/1000 | Loss: 0.00001372
Iteration 63/1000 | Loss: 0.00001371
Iteration 64/1000 | Loss: 0.00001369
Iteration 65/1000 | Loss: 0.00001369
Iteration 66/1000 | Loss: 0.00001369
Iteration 67/1000 | Loss: 0.00001369
Iteration 68/1000 | Loss: 0.00001369
Iteration 69/1000 | Loss: 0.00001368
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001367
Iteration 72/1000 | Loss: 0.00001367
Iteration 73/1000 | Loss: 0.00001367
Iteration 74/1000 | Loss: 0.00001366
Iteration 75/1000 | Loss: 0.00001366
Iteration 76/1000 | Loss: 0.00001366
Iteration 77/1000 | Loss: 0.00001366
Iteration 78/1000 | Loss: 0.00001366
Iteration 79/1000 | Loss: 0.00001365
Iteration 80/1000 | Loss: 0.00001365
Iteration 81/1000 | Loss: 0.00001365
Iteration 82/1000 | Loss: 0.00001365
Iteration 83/1000 | Loss: 0.00001365
Iteration 84/1000 | Loss: 0.00001365
Iteration 85/1000 | Loss: 0.00001365
Iteration 86/1000 | Loss: 0.00001365
Iteration 87/1000 | Loss: 0.00001365
Iteration 88/1000 | Loss: 0.00001365
Iteration 89/1000 | Loss: 0.00001364
Iteration 90/1000 | Loss: 0.00001364
Iteration 91/1000 | Loss: 0.00001364
Iteration 92/1000 | Loss: 0.00001364
Iteration 93/1000 | Loss: 0.00001364
Iteration 94/1000 | Loss: 0.00001364
Iteration 95/1000 | Loss: 0.00001364
Iteration 96/1000 | Loss: 0.00001364
Iteration 97/1000 | Loss: 0.00001364
Iteration 98/1000 | Loss: 0.00001363
Iteration 99/1000 | Loss: 0.00001363
Iteration 100/1000 | Loss: 0.00001363
Iteration 101/1000 | Loss: 0.00001363
Iteration 102/1000 | Loss: 0.00001363
Iteration 103/1000 | Loss: 0.00001363
Iteration 104/1000 | Loss: 0.00001363
Iteration 105/1000 | Loss: 0.00001363
Iteration 106/1000 | Loss: 0.00001363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.3633707567350939e-05, 1.3633707567350939e-05, 1.3633707567350939e-05, 1.3633707567350939e-05, 1.3633707567350939e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3633707567350939e-05

Optimization complete. Final v2v error: 3.1463778018951416 mm

Highest mean error: 3.411524772644043 mm for frame 110

Lowest mean error: 2.956041097640991 mm for frame 6

Saving results

Total time: 33.477978467941284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00730215
Iteration 2/25 | Loss: 0.00155205
Iteration 3/25 | Loss: 0.00108962
Iteration 4/25 | Loss: 0.00093422
Iteration 5/25 | Loss: 0.00083268
Iteration 6/25 | Loss: 0.00083019
Iteration 7/25 | Loss: 0.00083625
Iteration 8/25 | Loss: 0.00076887
Iteration 9/25 | Loss: 0.00075422
Iteration 10/25 | Loss: 0.00076809
Iteration 11/25 | Loss: 0.00075022
Iteration 12/25 | Loss: 0.00074871
Iteration 13/25 | Loss: 0.00074774
Iteration 14/25 | Loss: 0.00074685
Iteration 15/25 | Loss: 0.00074593
Iteration 16/25 | Loss: 0.00074478
Iteration 17/25 | Loss: 0.00076778
Iteration 18/25 | Loss: 0.00077136
Iteration 19/25 | Loss: 0.00070701
Iteration 20/25 | Loss: 0.00067134
Iteration 21/25 | Loss: 0.00066173
Iteration 22/25 | Loss: 0.00065973
Iteration 23/25 | Loss: 0.00065920
Iteration 24/25 | Loss: 0.00065913
Iteration 25/25 | Loss: 0.00065913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.85399342
Iteration 2/25 | Loss: 0.00072542
Iteration 3/25 | Loss: 0.00072542
Iteration 4/25 | Loss: 0.00072542
Iteration 5/25 | Loss: 0.00072542
Iteration 6/25 | Loss: 0.00072542
Iteration 7/25 | Loss: 0.00072542
Iteration 8/25 | Loss: 0.00072542
Iteration 9/25 | Loss: 0.00072542
Iteration 10/25 | Loss: 0.00072542
Iteration 11/25 | Loss: 0.00072542
Iteration 12/25 | Loss: 0.00072542
Iteration 13/25 | Loss: 0.00072542
Iteration 14/25 | Loss: 0.00072542
Iteration 15/25 | Loss: 0.00072542
Iteration 16/25 | Loss: 0.00072542
Iteration 17/25 | Loss: 0.00072542
Iteration 18/25 | Loss: 0.00072542
Iteration 19/25 | Loss: 0.00072542
Iteration 20/25 | Loss: 0.00072542
Iteration 21/25 | Loss: 0.00072542
Iteration 22/25 | Loss: 0.00072542
Iteration 23/25 | Loss: 0.00072542
Iteration 24/25 | Loss: 0.00072542
Iteration 25/25 | Loss: 0.00072542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072542
Iteration 2/1000 | Loss: 0.00009885
Iteration 3/1000 | Loss: 0.00006136
Iteration 4/1000 | Loss: 0.00025346
Iteration 5/1000 | Loss: 0.00016613
Iteration 6/1000 | Loss: 0.00055491
Iteration 7/1000 | Loss: 0.00057869
Iteration 8/1000 | Loss: 0.00005890
Iteration 9/1000 | Loss: 0.00030289
Iteration 10/1000 | Loss: 0.00003671
Iteration 11/1000 | Loss: 0.00003418
Iteration 12/1000 | Loss: 0.00003228
Iteration 13/1000 | Loss: 0.00003127
Iteration 14/1000 | Loss: 0.00003067
Iteration 15/1000 | Loss: 0.00003018
Iteration 16/1000 | Loss: 0.00002980
Iteration 17/1000 | Loss: 0.00002931
Iteration 18/1000 | Loss: 0.00002892
Iteration 19/1000 | Loss: 0.00002848
Iteration 20/1000 | Loss: 0.00002820
Iteration 21/1000 | Loss: 0.00119355
Iteration 22/1000 | Loss: 0.00130204
Iteration 23/1000 | Loss: 0.00004420
Iteration 24/1000 | Loss: 0.00003185
Iteration 25/1000 | Loss: 0.00002462
Iteration 26/1000 | Loss: 0.00002278
Iteration 27/1000 | Loss: 0.00002141
Iteration 28/1000 | Loss: 0.00002015
Iteration 29/1000 | Loss: 0.00001952
Iteration 30/1000 | Loss: 0.00001907
Iteration 31/1000 | Loss: 0.00001874
Iteration 32/1000 | Loss: 0.00001846
Iteration 33/1000 | Loss: 0.00001845
Iteration 34/1000 | Loss: 0.00001827
Iteration 35/1000 | Loss: 0.00001821
Iteration 36/1000 | Loss: 0.00001816
Iteration 37/1000 | Loss: 0.00001816
Iteration 38/1000 | Loss: 0.00001815
Iteration 39/1000 | Loss: 0.00001815
Iteration 40/1000 | Loss: 0.00001814
Iteration 41/1000 | Loss: 0.00001813
Iteration 42/1000 | Loss: 0.00001813
Iteration 43/1000 | Loss: 0.00001808
Iteration 44/1000 | Loss: 0.00001806
Iteration 45/1000 | Loss: 0.00001805
Iteration 46/1000 | Loss: 0.00001805
Iteration 47/1000 | Loss: 0.00001804
Iteration 48/1000 | Loss: 0.00001803
Iteration 49/1000 | Loss: 0.00001802
Iteration 50/1000 | Loss: 0.00001802
Iteration 51/1000 | Loss: 0.00001801
Iteration 52/1000 | Loss: 0.00001799
Iteration 53/1000 | Loss: 0.00001799
Iteration 54/1000 | Loss: 0.00001799
Iteration 55/1000 | Loss: 0.00001799
Iteration 56/1000 | Loss: 0.00001799
Iteration 57/1000 | Loss: 0.00001799
Iteration 58/1000 | Loss: 0.00001799
Iteration 59/1000 | Loss: 0.00001798
Iteration 60/1000 | Loss: 0.00001797
Iteration 61/1000 | Loss: 0.00001797
Iteration 62/1000 | Loss: 0.00001796
Iteration 63/1000 | Loss: 0.00001796
Iteration 64/1000 | Loss: 0.00001796
Iteration 65/1000 | Loss: 0.00001796
Iteration 66/1000 | Loss: 0.00001796
Iteration 67/1000 | Loss: 0.00001795
Iteration 68/1000 | Loss: 0.00001795
Iteration 69/1000 | Loss: 0.00001795
Iteration 70/1000 | Loss: 0.00001795
Iteration 71/1000 | Loss: 0.00001795
Iteration 72/1000 | Loss: 0.00001795
Iteration 73/1000 | Loss: 0.00001794
Iteration 74/1000 | Loss: 0.00001794
Iteration 75/1000 | Loss: 0.00001794
Iteration 76/1000 | Loss: 0.00001794
Iteration 77/1000 | Loss: 0.00001794
Iteration 78/1000 | Loss: 0.00001794
Iteration 79/1000 | Loss: 0.00001794
Iteration 80/1000 | Loss: 0.00001794
Iteration 81/1000 | Loss: 0.00001794
Iteration 82/1000 | Loss: 0.00001794
Iteration 83/1000 | Loss: 0.00001794
Iteration 84/1000 | Loss: 0.00051340
Iteration 85/1000 | Loss: 0.00001962
Iteration 86/1000 | Loss: 0.00001788
Iteration 87/1000 | Loss: 0.00001706
Iteration 88/1000 | Loss: 0.00001646
Iteration 89/1000 | Loss: 0.00001604
Iteration 90/1000 | Loss: 0.00001586
Iteration 91/1000 | Loss: 0.00001585
Iteration 92/1000 | Loss: 0.00001582
Iteration 93/1000 | Loss: 0.00001580
Iteration 94/1000 | Loss: 0.00001575
Iteration 95/1000 | Loss: 0.00001568
Iteration 96/1000 | Loss: 0.00001561
Iteration 97/1000 | Loss: 0.00001561
Iteration 98/1000 | Loss: 0.00001560
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001559
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001558
Iteration 103/1000 | Loss: 0.00001557
Iteration 104/1000 | Loss: 0.00001557
Iteration 105/1000 | Loss: 0.00001557
Iteration 106/1000 | Loss: 0.00001556
Iteration 107/1000 | Loss: 0.00001556
Iteration 108/1000 | Loss: 0.00001556
Iteration 109/1000 | Loss: 0.00001556
Iteration 110/1000 | Loss: 0.00001556
Iteration 111/1000 | Loss: 0.00001555
Iteration 112/1000 | Loss: 0.00001555
Iteration 113/1000 | Loss: 0.00001555
Iteration 114/1000 | Loss: 0.00001555
Iteration 115/1000 | Loss: 0.00001555
Iteration 116/1000 | Loss: 0.00001555
Iteration 117/1000 | Loss: 0.00001555
Iteration 118/1000 | Loss: 0.00001555
Iteration 119/1000 | Loss: 0.00001554
Iteration 120/1000 | Loss: 0.00001554
Iteration 121/1000 | Loss: 0.00001554
Iteration 122/1000 | Loss: 0.00001554
Iteration 123/1000 | Loss: 0.00001554
Iteration 124/1000 | Loss: 0.00001554
Iteration 125/1000 | Loss: 0.00001554
Iteration 126/1000 | Loss: 0.00001554
Iteration 127/1000 | Loss: 0.00001554
Iteration 128/1000 | Loss: 0.00001554
Iteration 129/1000 | Loss: 0.00001554
Iteration 130/1000 | Loss: 0.00001553
Iteration 131/1000 | Loss: 0.00001553
Iteration 132/1000 | Loss: 0.00001553
Iteration 133/1000 | Loss: 0.00001553
Iteration 134/1000 | Loss: 0.00001553
Iteration 135/1000 | Loss: 0.00001552
Iteration 136/1000 | Loss: 0.00001552
Iteration 137/1000 | Loss: 0.00001552
Iteration 138/1000 | Loss: 0.00001552
Iteration 139/1000 | Loss: 0.00001552
Iteration 140/1000 | Loss: 0.00001552
Iteration 141/1000 | Loss: 0.00001551
Iteration 142/1000 | Loss: 0.00001551
Iteration 143/1000 | Loss: 0.00001551
Iteration 144/1000 | Loss: 0.00001551
Iteration 145/1000 | Loss: 0.00001551
Iteration 146/1000 | Loss: 0.00001551
Iteration 147/1000 | Loss: 0.00001550
Iteration 148/1000 | Loss: 0.00001550
Iteration 149/1000 | Loss: 0.00001550
Iteration 150/1000 | Loss: 0.00001550
Iteration 151/1000 | Loss: 0.00001550
Iteration 152/1000 | Loss: 0.00001550
Iteration 153/1000 | Loss: 0.00001550
Iteration 154/1000 | Loss: 0.00001550
Iteration 155/1000 | Loss: 0.00001550
Iteration 156/1000 | Loss: 0.00001550
Iteration 157/1000 | Loss: 0.00001550
Iteration 158/1000 | Loss: 0.00001550
Iteration 159/1000 | Loss: 0.00001550
Iteration 160/1000 | Loss: 0.00001550
Iteration 161/1000 | Loss: 0.00001550
Iteration 162/1000 | Loss: 0.00001549
Iteration 163/1000 | Loss: 0.00001549
Iteration 164/1000 | Loss: 0.00001549
Iteration 165/1000 | Loss: 0.00001549
Iteration 166/1000 | Loss: 0.00001549
Iteration 167/1000 | Loss: 0.00001549
Iteration 168/1000 | Loss: 0.00001549
Iteration 169/1000 | Loss: 0.00001549
Iteration 170/1000 | Loss: 0.00001548
Iteration 171/1000 | Loss: 0.00001548
Iteration 172/1000 | Loss: 0.00001548
Iteration 173/1000 | Loss: 0.00001548
Iteration 174/1000 | Loss: 0.00001548
Iteration 175/1000 | Loss: 0.00001548
Iteration 176/1000 | Loss: 0.00001548
Iteration 177/1000 | Loss: 0.00001548
Iteration 178/1000 | Loss: 0.00001548
Iteration 179/1000 | Loss: 0.00001548
Iteration 180/1000 | Loss: 0.00001548
Iteration 181/1000 | Loss: 0.00001547
Iteration 182/1000 | Loss: 0.00001547
Iteration 183/1000 | Loss: 0.00001547
Iteration 184/1000 | Loss: 0.00001547
Iteration 185/1000 | Loss: 0.00001547
Iteration 186/1000 | Loss: 0.00001547
Iteration 187/1000 | Loss: 0.00001547
Iteration 188/1000 | Loss: 0.00001547
Iteration 189/1000 | Loss: 0.00001547
Iteration 190/1000 | Loss: 0.00001547
Iteration 191/1000 | Loss: 0.00001547
Iteration 192/1000 | Loss: 0.00001547
Iteration 193/1000 | Loss: 0.00001547
Iteration 194/1000 | Loss: 0.00001547
Iteration 195/1000 | Loss: 0.00001547
Iteration 196/1000 | Loss: 0.00001547
Iteration 197/1000 | Loss: 0.00001547
Iteration 198/1000 | Loss: 0.00001547
Iteration 199/1000 | Loss: 0.00001547
Iteration 200/1000 | Loss: 0.00001546
Iteration 201/1000 | Loss: 0.00001546
Iteration 202/1000 | Loss: 0.00001546
Iteration 203/1000 | Loss: 0.00001546
Iteration 204/1000 | Loss: 0.00001546
Iteration 205/1000 | Loss: 0.00001546
Iteration 206/1000 | Loss: 0.00001546
Iteration 207/1000 | Loss: 0.00001546
Iteration 208/1000 | Loss: 0.00001546
Iteration 209/1000 | Loss: 0.00001546
Iteration 210/1000 | Loss: 0.00001546
Iteration 211/1000 | Loss: 0.00001546
Iteration 212/1000 | Loss: 0.00001546
Iteration 213/1000 | Loss: 0.00001546
Iteration 214/1000 | Loss: 0.00001546
Iteration 215/1000 | Loss: 0.00001546
Iteration 216/1000 | Loss: 0.00001546
Iteration 217/1000 | Loss: 0.00001546
Iteration 218/1000 | Loss: 0.00001546
Iteration 219/1000 | Loss: 0.00001546
Iteration 220/1000 | Loss: 0.00001546
Iteration 221/1000 | Loss: 0.00001546
Iteration 222/1000 | Loss: 0.00001546
Iteration 223/1000 | Loss: 0.00001546
Iteration 224/1000 | Loss: 0.00001546
Iteration 225/1000 | Loss: 0.00001546
Iteration 226/1000 | Loss: 0.00001546
Iteration 227/1000 | Loss: 0.00001546
Iteration 228/1000 | Loss: 0.00001546
Iteration 229/1000 | Loss: 0.00001546
Iteration 230/1000 | Loss: 0.00001546
Iteration 231/1000 | Loss: 0.00001546
Iteration 232/1000 | Loss: 0.00001546
Iteration 233/1000 | Loss: 0.00001546
Iteration 234/1000 | Loss: 0.00001546
Iteration 235/1000 | Loss: 0.00001546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.5456569599336945e-05, 1.5456569599336945e-05, 1.5456569599336945e-05, 1.5456569599336945e-05, 1.5456569599336945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5456569599336945e-05

Optimization complete. Final v2v error: 3.3872501850128174 mm

Highest mean error: 4.42289400100708 mm for frame 3

Lowest mean error: 3.139779806137085 mm for frame 145

Saving results

Total time: 130.0997176170349
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052244
Iteration 2/25 | Loss: 0.00238996
Iteration 3/25 | Loss: 0.00154529
Iteration 4/25 | Loss: 0.00131871
Iteration 5/25 | Loss: 0.00124994
Iteration 6/25 | Loss: 0.00117681
Iteration 7/25 | Loss: 0.00113433
Iteration 8/25 | Loss: 0.00108179
Iteration 9/25 | Loss: 0.00104968
Iteration 10/25 | Loss: 0.00100915
Iteration 11/25 | Loss: 0.00096820
Iteration 12/25 | Loss: 0.00094106
Iteration 13/25 | Loss: 0.00093255
Iteration 14/25 | Loss: 0.00092512
Iteration 15/25 | Loss: 0.00092507
Iteration 16/25 | Loss: 0.00092286
Iteration 17/25 | Loss: 0.00091968
Iteration 18/25 | Loss: 0.00091763
Iteration 19/25 | Loss: 0.00091704
Iteration 20/25 | Loss: 0.00091692
Iteration 21/25 | Loss: 0.00091688
Iteration 22/25 | Loss: 0.00091688
Iteration 23/25 | Loss: 0.00091688
Iteration 24/25 | Loss: 0.00091688
Iteration 25/25 | Loss: 0.00091688

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44735241
Iteration 2/25 | Loss: 0.00210086
Iteration 3/25 | Loss: 0.00210086
Iteration 4/25 | Loss: 0.00210086
Iteration 5/25 | Loss: 0.00210086
Iteration 6/25 | Loss: 0.00210086
Iteration 7/25 | Loss: 0.00210086
Iteration 8/25 | Loss: 0.00210085
Iteration 9/25 | Loss: 0.00210085
Iteration 10/25 | Loss: 0.00210085
Iteration 11/25 | Loss: 0.00210085
Iteration 12/25 | Loss: 0.00210085
Iteration 13/25 | Loss: 0.00210085
Iteration 14/25 | Loss: 0.00210085
Iteration 15/25 | Loss: 0.00210085
Iteration 16/25 | Loss: 0.00210085
Iteration 17/25 | Loss: 0.00210085
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0021008539479225874, 0.0021008539479225874, 0.0021008539479225874, 0.0021008539479225874, 0.0021008539479225874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021008539479225874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210085
Iteration 2/1000 | Loss: 0.00091650
Iteration 3/1000 | Loss: 0.00078578
Iteration 4/1000 | Loss: 0.00106044
Iteration 5/1000 | Loss: 0.00026014
Iteration 6/1000 | Loss: 0.00019388
Iteration 7/1000 | Loss: 0.00027786
Iteration 8/1000 | Loss: 0.00016423
Iteration 9/1000 | Loss: 0.00011738
Iteration 10/1000 | Loss: 0.00010546
Iteration 11/1000 | Loss: 0.00009709
Iteration 12/1000 | Loss: 0.00008898
Iteration 13/1000 | Loss: 0.00042810
Iteration 14/1000 | Loss: 0.00009433
Iteration 15/1000 | Loss: 0.00008458
Iteration 16/1000 | Loss: 0.00008157
Iteration 17/1000 | Loss: 0.00007966
Iteration 18/1000 | Loss: 0.00007702
Iteration 19/1000 | Loss: 0.00007575
Iteration 20/1000 | Loss: 0.00012825
Iteration 21/1000 | Loss: 0.00028694
Iteration 22/1000 | Loss: 0.00161834
Iteration 23/1000 | Loss: 0.00299803
Iteration 24/1000 | Loss: 0.00294460
Iteration 25/1000 | Loss: 0.00024961
Iteration 26/1000 | Loss: 0.00016746
Iteration 27/1000 | Loss: 0.00025536
Iteration 28/1000 | Loss: 0.00008930
Iteration 29/1000 | Loss: 0.00006657
Iteration 30/1000 | Loss: 0.00004650
Iteration 31/1000 | Loss: 0.00003568
Iteration 32/1000 | Loss: 0.00021623
Iteration 33/1000 | Loss: 0.00003384
Iteration 34/1000 | Loss: 0.00002664
Iteration 35/1000 | Loss: 0.00002358
Iteration 36/1000 | Loss: 0.00002087
Iteration 37/1000 | Loss: 0.00023161
Iteration 38/1000 | Loss: 0.00002079
Iteration 39/1000 | Loss: 0.00001635
Iteration 40/1000 | Loss: 0.00001532
Iteration 41/1000 | Loss: 0.00001462
Iteration 42/1000 | Loss: 0.00001380
Iteration 43/1000 | Loss: 0.00001316
Iteration 44/1000 | Loss: 0.00001264
Iteration 45/1000 | Loss: 0.00001240
Iteration 46/1000 | Loss: 0.00001230
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001222
Iteration 49/1000 | Loss: 0.00001215
Iteration 50/1000 | Loss: 0.00001211
Iteration 51/1000 | Loss: 0.00001208
Iteration 52/1000 | Loss: 0.00001208
Iteration 53/1000 | Loss: 0.00001206
Iteration 54/1000 | Loss: 0.00001205
Iteration 55/1000 | Loss: 0.00001204
Iteration 56/1000 | Loss: 0.00001203
Iteration 57/1000 | Loss: 0.00001201
Iteration 58/1000 | Loss: 0.00001201
Iteration 59/1000 | Loss: 0.00001200
Iteration 60/1000 | Loss: 0.00001200
Iteration 61/1000 | Loss: 0.00001199
Iteration 62/1000 | Loss: 0.00001199
Iteration 63/1000 | Loss: 0.00001198
Iteration 64/1000 | Loss: 0.00001198
Iteration 65/1000 | Loss: 0.00001198
Iteration 66/1000 | Loss: 0.00001197
Iteration 67/1000 | Loss: 0.00001197
Iteration 68/1000 | Loss: 0.00001197
Iteration 69/1000 | Loss: 0.00001196
Iteration 70/1000 | Loss: 0.00001196
Iteration 71/1000 | Loss: 0.00001196
Iteration 72/1000 | Loss: 0.00001195
Iteration 73/1000 | Loss: 0.00001195
Iteration 74/1000 | Loss: 0.00001195
Iteration 75/1000 | Loss: 0.00001195
Iteration 76/1000 | Loss: 0.00001194
Iteration 77/1000 | Loss: 0.00001194
Iteration 78/1000 | Loss: 0.00001194
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001193
Iteration 81/1000 | Loss: 0.00001193
Iteration 82/1000 | Loss: 0.00001192
Iteration 83/1000 | Loss: 0.00001192
Iteration 84/1000 | Loss: 0.00001192
Iteration 85/1000 | Loss: 0.00001192
Iteration 86/1000 | Loss: 0.00001192
Iteration 87/1000 | Loss: 0.00001192
Iteration 88/1000 | Loss: 0.00001192
Iteration 89/1000 | Loss: 0.00001191
Iteration 90/1000 | Loss: 0.00001191
Iteration 91/1000 | Loss: 0.00001191
Iteration 92/1000 | Loss: 0.00001191
Iteration 93/1000 | Loss: 0.00001191
Iteration 94/1000 | Loss: 0.00001191
Iteration 95/1000 | Loss: 0.00001191
Iteration 96/1000 | Loss: 0.00001191
Iteration 97/1000 | Loss: 0.00001191
Iteration 98/1000 | Loss: 0.00001190
Iteration 99/1000 | Loss: 0.00001190
Iteration 100/1000 | Loss: 0.00001190
Iteration 101/1000 | Loss: 0.00001190
Iteration 102/1000 | Loss: 0.00001190
Iteration 103/1000 | Loss: 0.00001190
Iteration 104/1000 | Loss: 0.00001190
Iteration 105/1000 | Loss: 0.00001190
Iteration 106/1000 | Loss: 0.00001190
Iteration 107/1000 | Loss: 0.00001190
Iteration 108/1000 | Loss: 0.00001190
Iteration 109/1000 | Loss: 0.00001190
Iteration 110/1000 | Loss: 0.00001189
Iteration 111/1000 | Loss: 0.00001189
Iteration 112/1000 | Loss: 0.00001189
Iteration 113/1000 | Loss: 0.00001189
Iteration 114/1000 | Loss: 0.00001189
Iteration 115/1000 | Loss: 0.00001189
Iteration 116/1000 | Loss: 0.00001189
Iteration 117/1000 | Loss: 0.00001189
Iteration 118/1000 | Loss: 0.00001189
Iteration 119/1000 | Loss: 0.00001189
Iteration 120/1000 | Loss: 0.00001189
Iteration 121/1000 | Loss: 0.00001189
Iteration 122/1000 | Loss: 0.00001189
Iteration 123/1000 | Loss: 0.00001189
Iteration 124/1000 | Loss: 0.00001188
Iteration 125/1000 | Loss: 0.00001188
Iteration 126/1000 | Loss: 0.00001188
Iteration 127/1000 | Loss: 0.00001188
Iteration 128/1000 | Loss: 0.00001188
Iteration 129/1000 | Loss: 0.00001188
Iteration 130/1000 | Loss: 0.00001188
Iteration 131/1000 | Loss: 0.00001188
Iteration 132/1000 | Loss: 0.00001188
Iteration 133/1000 | Loss: 0.00001188
Iteration 134/1000 | Loss: 0.00001187
Iteration 135/1000 | Loss: 0.00001187
Iteration 136/1000 | Loss: 0.00001187
Iteration 137/1000 | Loss: 0.00001187
Iteration 138/1000 | Loss: 0.00001187
Iteration 139/1000 | Loss: 0.00001187
Iteration 140/1000 | Loss: 0.00001187
Iteration 141/1000 | Loss: 0.00001187
Iteration 142/1000 | Loss: 0.00001187
Iteration 143/1000 | Loss: 0.00001187
Iteration 144/1000 | Loss: 0.00001187
Iteration 145/1000 | Loss: 0.00001187
Iteration 146/1000 | Loss: 0.00001186
Iteration 147/1000 | Loss: 0.00001186
Iteration 148/1000 | Loss: 0.00001186
Iteration 149/1000 | Loss: 0.00001186
Iteration 150/1000 | Loss: 0.00001186
Iteration 151/1000 | Loss: 0.00001186
Iteration 152/1000 | Loss: 0.00001186
Iteration 153/1000 | Loss: 0.00001186
Iteration 154/1000 | Loss: 0.00001186
Iteration 155/1000 | Loss: 0.00001186
Iteration 156/1000 | Loss: 0.00001186
Iteration 157/1000 | Loss: 0.00001186
Iteration 158/1000 | Loss: 0.00001186
Iteration 159/1000 | Loss: 0.00001185
Iteration 160/1000 | Loss: 0.00001185
Iteration 161/1000 | Loss: 0.00001185
Iteration 162/1000 | Loss: 0.00001185
Iteration 163/1000 | Loss: 0.00001185
Iteration 164/1000 | Loss: 0.00001185
Iteration 165/1000 | Loss: 0.00001185
Iteration 166/1000 | Loss: 0.00001185
Iteration 167/1000 | Loss: 0.00001185
Iteration 168/1000 | Loss: 0.00001185
Iteration 169/1000 | Loss: 0.00001185
Iteration 170/1000 | Loss: 0.00001185
Iteration 171/1000 | Loss: 0.00001185
Iteration 172/1000 | Loss: 0.00001185
Iteration 173/1000 | Loss: 0.00001185
Iteration 174/1000 | Loss: 0.00001185
Iteration 175/1000 | Loss: 0.00001185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.184752272820333e-05, 1.184752272820333e-05, 1.184752272820333e-05, 1.184752272820333e-05, 1.184752272820333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.184752272820333e-05

Optimization complete. Final v2v error: 2.952089786529541 mm

Highest mean error: 4.061079025268555 mm for frame 231

Lowest mean error: 2.7564446926116943 mm for frame 29

Saving results

Total time: 129.58945536613464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874628
Iteration 2/25 | Loss: 0.00082978
Iteration 3/25 | Loss: 0.00063984
Iteration 4/25 | Loss: 0.00061068
Iteration 5/25 | Loss: 0.00060322
Iteration 6/25 | Loss: 0.00060099
Iteration 7/25 | Loss: 0.00060066
Iteration 8/25 | Loss: 0.00060066
Iteration 9/25 | Loss: 0.00060066
Iteration 10/25 | Loss: 0.00060066
Iteration 11/25 | Loss: 0.00060066
Iteration 12/25 | Loss: 0.00060066
Iteration 13/25 | Loss: 0.00060066
Iteration 14/25 | Loss: 0.00060066
Iteration 15/25 | Loss: 0.00060066
Iteration 16/25 | Loss: 0.00060066
Iteration 17/25 | Loss: 0.00060066
Iteration 18/25 | Loss: 0.00060066
Iteration 19/25 | Loss: 0.00060066
Iteration 20/25 | Loss: 0.00060066
Iteration 21/25 | Loss: 0.00060066
Iteration 22/25 | Loss: 0.00060066
Iteration 23/25 | Loss: 0.00060066
Iteration 24/25 | Loss: 0.00060066
Iteration 25/25 | Loss: 0.00060066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44922578
Iteration 2/25 | Loss: 0.00023638
Iteration 3/25 | Loss: 0.00023636
Iteration 4/25 | Loss: 0.00023636
Iteration 5/25 | Loss: 0.00023635
Iteration 6/25 | Loss: 0.00023635
Iteration 7/25 | Loss: 0.00023635
Iteration 8/25 | Loss: 0.00023635
Iteration 9/25 | Loss: 0.00023635
Iteration 10/25 | Loss: 0.00023635
Iteration 11/25 | Loss: 0.00023635
Iteration 12/25 | Loss: 0.00023635
Iteration 13/25 | Loss: 0.00023635
Iteration 14/25 | Loss: 0.00023635
Iteration 15/25 | Loss: 0.00023635
Iteration 16/25 | Loss: 0.00023635
Iteration 17/25 | Loss: 0.00023635
Iteration 18/25 | Loss: 0.00023635
Iteration 19/25 | Loss: 0.00023635
Iteration 20/25 | Loss: 0.00023635
Iteration 21/25 | Loss: 0.00023635
Iteration 22/25 | Loss: 0.00023635
Iteration 23/25 | Loss: 0.00023635
Iteration 24/25 | Loss: 0.00023635
Iteration 25/25 | Loss: 0.00023635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023635
Iteration 2/1000 | Loss: 0.00002066
Iteration 3/1000 | Loss: 0.00001515
Iteration 4/1000 | Loss: 0.00001352
Iteration 5/1000 | Loss: 0.00001270
Iteration 6/1000 | Loss: 0.00001208
Iteration 7/1000 | Loss: 0.00001167
Iteration 8/1000 | Loss: 0.00001145
Iteration 9/1000 | Loss: 0.00001142
Iteration 10/1000 | Loss: 0.00001142
Iteration 11/1000 | Loss: 0.00001140
Iteration 12/1000 | Loss: 0.00001137
Iteration 13/1000 | Loss: 0.00001136
Iteration 14/1000 | Loss: 0.00001136
Iteration 15/1000 | Loss: 0.00001135
Iteration 16/1000 | Loss: 0.00001128
Iteration 17/1000 | Loss: 0.00001124
Iteration 18/1000 | Loss: 0.00001124
Iteration 19/1000 | Loss: 0.00001122
Iteration 20/1000 | Loss: 0.00001119
Iteration 21/1000 | Loss: 0.00001119
Iteration 22/1000 | Loss: 0.00001119
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001117
Iteration 25/1000 | Loss: 0.00001116
Iteration 26/1000 | Loss: 0.00001113
Iteration 27/1000 | Loss: 0.00001113
Iteration 28/1000 | Loss: 0.00001111
Iteration 29/1000 | Loss: 0.00001110
Iteration 30/1000 | Loss: 0.00001110
Iteration 31/1000 | Loss: 0.00001109
Iteration 32/1000 | Loss: 0.00001109
Iteration 33/1000 | Loss: 0.00001108
Iteration 34/1000 | Loss: 0.00001108
Iteration 35/1000 | Loss: 0.00001108
Iteration 36/1000 | Loss: 0.00001108
Iteration 37/1000 | Loss: 0.00001107
Iteration 38/1000 | Loss: 0.00001106
Iteration 39/1000 | Loss: 0.00001105
Iteration 40/1000 | Loss: 0.00001104
Iteration 41/1000 | Loss: 0.00001104
Iteration 42/1000 | Loss: 0.00001104
Iteration 43/1000 | Loss: 0.00001103
Iteration 44/1000 | Loss: 0.00001103
Iteration 45/1000 | Loss: 0.00001103
Iteration 46/1000 | Loss: 0.00001103
Iteration 47/1000 | Loss: 0.00001103
Iteration 48/1000 | Loss: 0.00001102
Iteration 49/1000 | Loss: 0.00001102
Iteration 50/1000 | Loss: 0.00001102
Iteration 51/1000 | Loss: 0.00001102
Iteration 52/1000 | Loss: 0.00001102
Iteration 53/1000 | Loss: 0.00001102
Iteration 54/1000 | Loss: 0.00001102
Iteration 55/1000 | Loss: 0.00001102
Iteration 56/1000 | Loss: 0.00001101
Iteration 57/1000 | Loss: 0.00001101
Iteration 58/1000 | Loss: 0.00001101
Iteration 59/1000 | Loss: 0.00001101
Iteration 60/1000 | Loss: 0.00001101
Iteration 61/1000 | Loss: 0.00001101
Iteration 62/1000 | Loss: 0.00001101
Iteration 63/1000 | Loss: 0.00001101
Iteration 64/1000 | Loss: 0.00001100
Iteration 65/1000 | Loss: 0.00001100
Iteration 66/1000 | Loss: 0.00001100
Iteration 67/1000 | Loss: 0.00001100
Iteration 68/1000 | Loss: 0.00001100
Iteration 69/1000 | Loss: 0.00001099
Iteration 70/1000 | Loss: 0.00001099
Iteration 71/1000 | Loss: 0.00001099
Iteration 72/1000 | Loss: 0.00001099
Iteration 73/1000 | Loss: 0.00001099
Iteration 74/1000 | Loss: 0.00001099
Iteration 75/1000 | Loss: 0.00001099
Iteration 76/1000 | Loss: 0.00001099
Iteration 77/1000 | Loss: 0.00001099
Iteration 78/1000 | Loss: 0.00001099
Iteration 79/1000 | Loss: 0.00001099
Iteration 80/1000 | Loss: 0.00001099
Iteration 81/1000 | Loss: 0.00001099
Iteration 82/1000 | Loss: 0.00001099
Iteration 83/1000 | Loss: 0.00001099
Iteration 84/1000 | Loss: 0.00001099
Iteration 85/1000 | Loss: 0.00001099
Iteration 86/1000 | Loss: 0.00001099
Iteration 87/1000 | Loss: 0.00001099
Iteration 88/1000 | Loss: 0.00001099
Iteration 89/1000 | Loss: 0.00001099
Iteration 90/1000 | Loss: 0.00001099
Iteration 91/1000 | Loss: 0.00001099
Iteration 92/1000 | Loss: 0.00001099
Iteration 93/1000 | Loss: 0.00001099
Iteration 94/1000 | Loss: 0.00001099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.0985245353367645e-05, 1.0985245353367645e-05, 1.0985245353367645e-05, 1.0985245353367645e-05, 1.0985245353367645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0985245353367645e-05

Optimization complete. Final v2v error: 2.83786940574646 mm

Highest mean error: 2.9948813915252686 mm for frame 4

Lowest mean error: 2.7378060817718506 mm for frame 126

Saving results

Total time: 31.229372262954712
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494339
Iteration 2/25 | Loss: 0.00120285
Iteration 3/25 | Loss: 0.00078198
Iteration 4/25 | Loss: 0.00067047
Iteration 5/25 | Loss: 0.00064374
Iteration 6/25 | Loss: 0.00063686
Iteration 7/25 | Loss: 0.00063550
Iteration 8/25 | Loss: 0.00063541
Iteration 9/25 | Loss: 0.00063541
Iteration 10/25 | Loss: 0.00063541
Iteration 11/25 | Loss: 0.00063541
Iteration 12/25 | Loss: 0.00063541
Iteration 13/25 | Loss: 0.00063541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006354130455292761, 0.0006354130455292761, 0.0006354130455292761, 0.0006354130455292761, 0.0006354130455292761]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006354130455292761

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47123206
Iteration 2/25 | Loss: 0.00026354
Iteration 3/25 | Loss: 0.00026354
Iteration 4/25 | Loss: 0.00026354
Iteration 5/25 | Loss: 0.00026354
Iteration 6/25 | Loss: 0.00026354
Iteration 7/25 | Loss: 0.00026354
Iteration 8/25 | Loss: 0.00026354
Iteration 9/25 | Loss: 0.00026354
Iteration 10/25 | Loss: 0.00026354
Iteration 11/25 | Loss: 0.00026354
Iteration 12/25 | Loss: 0.00026354
Iteration 13/25 | Loss: 0.00026354
Iteration 14/25 | Loss: 0.00026354
Iteration 15/25 | Loss: 0.00026354
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00026354147121310234, 0.00026354147121310234, 0.00026354147121310234, 0.00026354147121310234, 0.00026354147121310234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026354147121310234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026354
Iteration 2/1000 | Loss: 0.00002621
Iteration 3/1000 | Loss: 0.00001877
Iteration 4/1000 | Loss: 0.00001719
Iteration 5/1000 | Loss: 0.00001611
Iteration 6/1000 | Loss: 0.00001537
Iteration 7/1000 | Loss: 0.00001506
Iteration 8/1000 | Loss: 0.00001466
Iteration 9/1000 | Loss: 0.00001459
Iteration 10/1000 | Loss: 0.00001443
Iteration 11/1000 | Loss: 0.00001439
Iteration 12/1000 | Loss: 0.00001435
Iteration 13/1000 | Loss: 0.00001435
Iteration 14/1000 | Loss: 0.00001434
Iteration 15/1000 | Loss: 0.00001433
Iteration 16/1000 | Loss: 0.00001427
Iteration 17/1000 | Loss: 0.00001420
Iteration 18/1000 | Loss: 0.00001420
Iteration 19/1000 | Loss: 0.00001415
Iteration 20/1000 | Loss: 0.00001415
Iteration 21/1000 | Loss: 0.00001414
Iteration 22/1000 | Loss: 0.00001414
Iteration 23/1000 | Loss: 0.00001413
Iteration 24/1000 | Loss: 0.00001408
Iteration 25/1000 | Loss: 0.00001405
Iteration 26/1000 | Loss: 0.00001404
Iteration 27/1000 | Loss: 0.00001403
Iteration 28/1000 | Loss: 0.00001403
Iteration 29/1000 | Loss: 0.00001402
Iteration 30/1000 | Loss: 0.00001402
Iteration 31/1000 | Loss: 0.00001401
Iteration 32/1000 | Loss: 0.00001401
Iteration 33/1000 | Loss: 0.00001399
Iteration 34/1000 | Loss: 0.00001399
Iteration 35/1000 | Loss: 0.00001399
Iteration 36/1000 | Loss: 0.00001399
Iteration 37/1000 | Loss: 0.00001399
Iteration 38/1000 | Loss: 0.00001399
Iteration 39/1000 | Loss: 0.00001399
Iteration 40/1000 | Loss: 0.00001399
Iteration 41/1000 | Loss: 0.00001399
Iteration 42/1000 | Loss: 0.00001399
Iteration 43/1000 | Loss: 0.00001398
Iteration 44/1000 | Loss: 0.00001398
Iteration 45/1000 | Loss: 0.00001396
Iteration 46/1000 | Loss: 0.00001396
Iteration 47/1000 | Loss: 0.00001396
Iteration 48/1000 | Loss: 0.00001396
Iteration 49/1000 | Loss: 0.00001396
Iteration 50/1000 | Loss: 0.00001396
Iteration 51/1000 | Loss: 0.00001396
Iteration 52/1000 | Loss: 0.00001395
Iteration 53/1000 | Loss: 0.00001395
Iteration 54/1000 | Loss: 0.00001395
Iteration 55/1000 | Loss: 0.00001395
Iteration 56/1000 | Loss: 0.00001394
Iteration 57/1000 | Loss: 0.00001394
Iteration 58/1000 | Loss: 0.00001394
Iteration 59/1000 | Loss: 0.00001394
Iteration 60/1000 | Loss: 0.00001393
Iteration 61/1000 | Loss: 0.00001393
Iteration 62/1000 | Loss: 0.00001393
Iteration 63/1000 | Loss: 0.00001393
Iteration 64/1000 | Loss: 0.00001392
Iteration 65/1000 | Loss: 0.00001392
Iteration 66/1000 | Loss: 0.00001392
Iteration 67/1000 | Loss: 0.00001391
Iteration 68/1000 | Loss: 0.00001391
Iteration 69/1000 | Loss: 0.00001391
Iteration 70/1000 | Loss: 0.00001391
Iteration 71/1000 | Loss: 0.00001390
Iteration 72/1000 | Loss: 0.00001390
Iteration 73/1000 | Loss: 0.00001390
Iteration 74/1000 | Loss: 0.00001390
Iteration 75/1000 | Loss: 0.00001390
Iteration 76/1000 | Loss: 0.00001390
Iteration 77/1000 | Loss: 0.00001390
Iteration 78/1000 | Loss: 0.00001390
Iteration 79/1000 | Loss: 0.00001389
Iteration 80/1000 | Loss: 0.00001389
Iteration 81/1000 | Loss: 0.00001389
Iteration 82/1000 | Loss: 0.00001388
Iteration 83/1000 | Loss: 0.00001388
Iteration 84/1000 | Loss: 0.00001388
Iteration 85/1000 | Loss: 0.00001388
Iteration 86/1000 | Loss: 0.00001388
Iteration 87/1000 | Loss: 0.00001388
Iteration 88/1000 | Loss: 0.00001388
Iteration 89/1000 | Loss: 0.00001388
Iteration 90/1000 | Loss: 0.00001387
Iteration 91/1000 | Loss: 0.00001387
Iteration 92/1000 | Loss: 0.00001387
Iteration 93/1000 | Loss: 0.00001387
Iteration 94/1000 | Loss: 0.00001387
Iteration 95/1000 | Loss: 0.00001386
Iteration 96/1000 | Loss: 0.00001386
Iteration 97/1000 | Loss: 0.00001386
Iteration 98/1000 | Loss: 0.00001386
Iteration 99/1000 | Loss: 0.00001386
Iteration 100/1000 | Loss: 0.00001385
Iteration 101/1000 | Loss: 0.00001385
Iteration 102/1000 | Loss: 0.00001385
Iteration 103/1000 | Loss: 0.00001385
Iteration 104/1000 | Loss: 0.00001384
Iteration 105/1000 | Loss: 0.00001384
Iteration 106/1000 | Loss: 0.00001384
Iteration 107/1000 | Loss: 0.00001384
Iteration 108/1000 | Loss: 0.00001384
Iteration 109/1000 | Loss: 0.00001384
Iteration 110/1000 | Loss: 0.00001384
Iteration 111/1000 | Loss: 0.00001384
Iteration 112/1000 | Loss: 0.00001383
Iteration 113/1000 | Loss: 0.00001383
Iteration 114/1000 | Loss: 0.00001383
Iteration 115/1000 | Loss: 0.00001383
Iteration 116/1000 | Loss: 0.00001383
Iteration 117/1000 | Loss: 0.00001383
Iteration 118/1000 | Loss: 0.00001383
Iteration 119/1000 | Loss: 0.00001383
Iteration 120/1000 | Loss: 0.00001383
Iteration 121/1000 | Loss: 0.00001383
Iteration 122/1000 | Loss: 0.00001383
Iteration 123/1000 | Loss: 0.00001383
Iteration 124/1000 | Loss: 0.00001383
Iteration 125/1000 | Loss: 0.00001382
Iteration 126/1000 | Loss: 0.00001382
Iteration 127/1000 | Loss: 0.00001382
Iteration 128/1000 | Loss: 0.00001382
Iteration 129/1000 | Loss: 0.00001381
Iteration 130/1000 | Loss: 0.00001381
Iteration 131/1000 | Loss: 0.00001381
Iteration 132/1000 | Loss: 0.00001381
Iteration 133/1000 | Loss: 0.00001381
Iteration 134/1000 | Loss: 0.00001381
Iteration 135/1000 | Loss: 0.00001381
Iteration 136/1000 | Loss: 0.00001381
Iteration 137/1000 | Loss: 0.00001381
Iteration 138/1000 | Loss: 0.00001381
Iteration 139/1000 | Loss: 0.00001381
Iteration 140/1000 | Loss: 0.00001381
Iteration 141/1000 | Loss: 0.00001381
Iteration 142/1000 | Loss: 0.00001381
Iteration 143/1000 | Loss: 0.00001381
Iteration 144/1000 | Loss: 0.00001381
Iteration 145/1000 | Loss: 0.00001381
Iteration 146/1000 | Loss: 0.00001381
Iteration 147/1000 | Loss: 0.00001381
Iteration 148/1000 | Loss: 0.00001381
Iteration 149/1000 | Loss: 0.00001381
Iteration 150/1000 | Loss: 0.00001381
Iteration 151/1000 | Loss: 0.00001381
Iteration 152/1000 | Loss: 0.00001381
Iteration 153/1000 | Loss: 0.00001381
Iteration 154/1000 | Loss: 0.00001381
Iteration 155/1000 | Loss: 0.00001381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.3809268239128869e-05, 1.3809268239128869e-05, 1.3809268239128869e-05, 1.3809268239128869e-05, 1.3809268239128869e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3809268239128869e-05

Optimization complete. Final v2v error: 3.0981061458587646 mm

Highest mean error: 4.324028968811035 mm for frame 90

Lowest mean error: 2.855987787246704 mm for frame 45

Saving results

Total time: 40.416945934295654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898754
Iteration 2/25 | Loss: 0.00231051
Iteration 3/25 | Loss: 0.00150264
Iteration 4/25 | Loss: 0.00123926
Iteration 5/25 | Loss: 0.00102707
Iteration 6/25 | Loss: 0.00097045
Iteration 7/25 | Loss: 0.00093089
Iteration 8/25 | Loss: 0.00089262
Iteration 9/25 | Loss: 0.00082171
Iteration 10/25 | Loss: 0.00080329
Iteration 11/25 | Loss: 0.00079015
Iteration 12/25 | Loss: 0.00077558
Iteration 13/25 | Loss: 0.00077327
Iteration 14/25 | Loss: 0.00076068
Iteration 15/25 | Loss: 0.00074822
Iteration 16/25 | Loss: 0.00074272
Iteration 17/25 | Loss: 0.00074170
Iteration 18/25 | Loss: 0.00074104
Iteration 19/25 | Loss: 0.00074157
Iteration 20/25 | Loss: 0.00074140
Iteration 21/25 | Loss: 0.00073963
Iteration 22/25 | Loss: 0.00073769
Iteration 23/25 | Loss: 0.00074092
Iteration 24/25 | Loss: 0.00073566
Iteration 25/25 | Loss: 0.00073275

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55657482
Iteration 2/25 | Loss: 0.00099241
Iteration 3/25 | Loss: 0.00099241
Iteration 4/25 | Loss: 0.00099241
Iteration 5/25 | Loss: 0.00099241
Iteration 6/25 | Loss: 0.00099241
Iteration 7/25 | Loss: 0.00099241
Iteration 8/25 | Loss: 0.00099241
Iteration 9/25 | Loss: 0.00099241
Iteration 10/25 | Loss: 0.00099241
Iteration 11/25 | Loss: 0.00099240
Iteration 12/25 | Loss: 0.00099240
Iteration 13/25 | Loss: 0.00099240
Iteration 14/25 | Loss: 0.00099240
Iteration 15/25 | Loss: 0.00099240
Iteration 16/25 | Loss: 0.00099240
Iteration 17/25 | Loss: 0.00099240
Iteration 18/25 | Loss: 0.00099240
Iteration 19/25 | Loss: 0.00099240
Iteration 20/25 | Loss: 0.00099240
Iteration 21/25 | Loss: 0.00099240
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009924048790708184, 0.0009924048790708184, 0.0009924048790708184, 0.0009924048790708184, 0.0009924048790708184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009924048790708184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099240
Iteration 2/1000 | Loss: 0.00012435
Iteration 3/1000 | Loss: 0.00010466
Iteration 4/1000 | Loss: 0.00008217
Iteration 5/1000 | Loss: 0.00008663
Iteration 6/1000 | Loss: 0.00008235
Iteration 7/1000 | Loss: 0.00006952
Iteration 8/1000 | Loss: 0.00006524
Iteration 9/1000 | Loss: 0.00049147
Iteration 10/1000 | Loss: 0.00014010
Iteration 11/1000 | Loss: 0.00039817
Iteration 12/1000 | Loss: 0.00009902
Iteration 13/1000 | Loss: 0.00006590
Iteration 14/1000 | Loss: 0.00007602
Iteration 15/1000 | Loss: 0.00005468
Iteration 16/1000 | Loss: 0.00006531
Iteration 17/1000 | Loss: 0.00005657
Iteration 18/1000 | Loss: 0.00006601
Iteration 19/1000 | Loss: 0.00005319
Iteration 20/1000 | Loss: 0.00005809
Iteration 21/1000 | Loss: 0.00005893
Iteration 22/1000 | Loss: 0.00005901
Iteration 23/1000 | Loss: 0.00035884
Iteration 24/1000 | Loss: 0.00024687
Iteration 25/1000 | Loss: 0.00031881
Iteration 26/1000 | Loss: 0.00006445
Iteration 27/1000 | Loss: 0.00005511
Iteration 28/1000 | Loss: 0.00007231
Iteration 29/1000 | Loss: 0.00004678
Iteration 30/1000 | Loss: 0.00006404
Iteration 31/1000 | Loss: 0.00005730
Iteration 32/1000 | Loss: 0.00004754
Iteration 33/1000 | Loss: 0.00004503
Iteration 34/1000 | Loss: 0.00004242
Iteration 35/1000 | Loss: 0.00004160
Iteration 36/1000 | Loss: 0.00005972
Iteration 37/1000 | Loss: 0.00006803
Iteration 38/1000 | Loss: 0.00004761
Iteration 39/1000 | Loss: 0.00004240
Iteration 40/1000 | Loss: 0.00004029
Iteration 41/1000 | Loss: 0.00003910
Iteration 42/1000 | Loss: 0.00003855
Iteration 43/1000 | Loss: 0.00003817
Iteration 44/1000 | Loss: 0.00003782
Iteration 45/1000 | Loss: 0.00003756
Iteration 46/1000 | Loss: 0.00003743
Iteration 47/1000 | Loss: 0.00003735
Iteration 48/1000 | Loss: 0.00003718
Iteration 49/1000 | Loss: 0.00003714
Iteration 50/1000 | Loss: 0.00003702
Iteration 51/1000 | Loss: 0.00003702
Iteration 52/1000 | Loss: 0.00003693
Iteration 53/1000 | Loss: 0.00003693
Iteration 54/1000 | Loss: 0.00003692
Iteration 55/1000 | Loss: 0.00003683
Iteration 56/1000 | Loss: 0.00003680
Iteration 57/1000 | Loss: 0.00003662
Iteration 58/1000 | Loss: 0.00003637
Iteration 59/1000 | Loss: 0.00003618
Iteration 60/1000 | Loss: 0.00003610
Iteration 61/1000 | Loss: 0.00003592
Iteration 62/1000 | Loss: 0.00003566
Iteration 63/1000 | Loss: 0.00015756
Iteration 64/1000 | Loss: 0.00003886
Iteration 65/1000 | Loss: 0.00003554
Iteration 66/1000 | Loss: 0.00003434
Iteration 67/1000 | Loss: 0.00003333
Iteration 68/1000 | Loss: 0.00003233
Iteration 69/1000 | Loss: 0.00019335
Iteration 70/1000 | Loss: 0.00003656
Iteration 71/1000 | Loss: 0.00003278
Iteration 72/1000 | Loss: 0.00003150
Iteration 73/1000 | Loss: 0.00003011
Iteration 74/1000 | Loss: 0.00002922
Iteration 75/1000 | Loss: 0.00016164
Iteration 76/1000 | Loss: 0.00003410
Iteration 77/1000 | Loss: 0.00003008
Iteration 78/1000 | Loss: 0.00002905
Iteration 79/1000 | Loss: 0.00002771
Iteration 80/1000 | Loss: 0.00002693
Iteration 81/1000 | Loss: 0.00002651
Iteration 82/1000 | Loss: 0.00002640
Iteration 83/1000 | Loss: 0.00002637
Iteration 84/1000 | Loss: 0.00002636
Iteration 85/1000 | Loss: 0.00002618
Iteration 86/1000 | Loss: 0.00002614
Iteration 87/1000 | Loss: 0.00002611
Iteration 88/1000 | Loss: 0.00002609
Iteration 89/1000 | Loss: 0.00002608
Iteration 90/1000 | Loss: 0.00002607
Iteration 91/1000 | Loss: 0.00002606
Iteration 92/1000 | Loss: 0.00014476
Iteration 93/1000 | Loss: 0.00002979
Iteration 94/1000 | Loss: 0.00002741
Iteration 95/1000 | Loss: 0.00002620
Iteration 96/1000 | Loss: 0.00002524
Iteration 97/1000 | Loss: 0.00002466
Iteration 98/1000 | Loss: 0.00002442
Iteration 99/1000 | Loss: 0.00002442
Iteration 100/1000 | Loss: 0.00002439
Iteration 101/1000 | Loss: 0.00002430
Iteration 102/1000 | Loss: 0.00002423
Iteration 103/1000 | Loss: 0.00002421
Iteration 104/1000 | Loss: 0.00002420
Iteration 105/1000 | Loss: 0.00002420
Iteration 106/1000 | Loss: 0.00002420
Iteration 107/1000 | Loss: 0.00002419
Iteration 108/1000 | Loss: 0.00002418
Iteration 109/1000 | Loss: 0.00002418
Iteration 110/1000 | Loss: 0.00002417
Iteration 111/1000 | Loss: 0.00002417
Iteration 112/1000 | Loss: 0.00002416
Iteration 113/1000 | Loss: 0.00002416
Iteration 114/1000 | Loss: 0.00002416
Iteration 115/1000 | Loss: 0.00002416
Iteration 116/1000 | Loss: 0.00002416
Iteration 117/1000 | Loss: 0.00002416
Iteration 118/1000 | Loss: 0.00002416
Iteration 119/1000 | Loss: 0.00002415
Iteration 120/1000 | Loss: 0.00002415
Iteration 121/1000 | Loss: 0.00002415
Iteration 122/1000 | Loss: 0.00002415
Iteration 123/1000 | Loss: 0.00002415
Iteration 124/1000 | Loss: 0.00002415
Iteration 125/1000 | Loss: 0.00002415
Iteration 126/1000 | Loss: 0.00002415
Iteration 127/1000 | Loss: 0.00002414
Iteration 128/1000 | Loss: 0.00002414
Iteration 129/1000 | Loss: 0.00002414
Iteration 130/1000 | Loss: 0.00002413
Iteration 131/1000 | Loss: 0.00002413
Iteration 132/1000 | Loss: 0.00002413
Iteration 133/1000 | Loss: 0.00002413
Iteration 134/1000 | Loss: 0.00002413
Iteration 135/1000 | Loss: 0.00002413
Iteration 136/1000 | Loss: 0.00002413
Iteration 137/1000 | Loss: 0.00002413
Iteration 138/1000 | Loss: 0.00002413
Iteration 139/1000 | Loss: 0.00002413
Iteration 140/1000 | Loss: 0.00002413
Iteration 141/1000 | Loss: 0.00002412
Iteration 142/1000 | Loss: 0.00002412
Iteration 143/1000 | Loss: 0.00002412
Iteration 144/1000 | Loss: 0.00002411
Iteration 145/1000 | Loss: 0.00002411
Iteration 146/1000 | Loss: 0.00002411
Iteration 147/1000 | Loss: 0.00002411
Iteration 148/1000 | Loss: 0.00002410
Iteration 149/1000 | Loss: 0.00002410
Iteration 150/1000 | Loss: 0.00002410
Iteration 151/1000 | Loss: 0.00002409
Iteration 152/1000 | Loss: 0.00002409
Iteration 153/1000 | Loss: 0.00002409
Iteration 154/1000 | Loss: 0.00002409
Iteration 155/1000 | Loss: 0.00002409
Iteration 156/1000 | Loss: 0.00002409
Iteration 157/1000 | Loss: 0.00002408
Iteration 158/1000 | Loss: 0.00002408
Iteration 159/1000 | Loss: 0.00002408
Iteration 160/1000 | Loss: 0.00002408
Iteration 161/1000 | Loss: 0.00002407
Iteration 162/1000 | Loss: 0.00002407
Iteration 163/1000 | Loss: 0.00002407
Iteration 164/1000 | Loss: 0.00002407
Iteration 165/1000 | Loss: 0.00002407
Iteration 166/1000 | Loss: 0.00002406
Iteration 167/1000 | Loss: 0.00002406
Iteration 168/1000 | Loss: 0.00002406
Iteration 169/1000 | Loss: 0.00002406
Iteration 170/1000 | Loss: 0.00002406
Iteration 171/1000 | Loss: 0.00002406
Iteration 172/1000 | Loss: 0.00002406
Iteration 173/1000 | Loss: 0.00002406
Iteration 174/1000 | Loss: 0.00002406
Iteration 175/1000 | Loss: 0.00002406
Iteration 176/1000 | Loss: 0.00002405
Iteration 177/1000 | Loss: 0.00002405
Iteration 178/1000 | Loss: 0.00002405
Iteration 179/1000 | Loss: 0.00002405
Iteration 180/1000 | Loss: 0.00002404
Iteration 181/1000 | Loss: 0.00002404
Iteration 182/1000 | Loss: 0.00002404
Iteration 183/1000 | Loss: 0.00002404
Iteration 184/1000 | Loss: 0.00002404
Iteration 185/1000 | Loss: 0.00002404
Iteration 186/1000 | Loss: 0.00002404
Iteration 187/1000 | Loss: 0.00002404
Iteration 188/1000 | Loss: 0.00002404
Iteration 189/1000 | Loss: 0.00002404
Iteration 190/1000 | Loss: 0.00002404
Iteration 191/1000 | Loss: 0.00002404
Iteration 192/1000 | Loss: 0.00002404
Iteration 193/1000 | Loss: 0.00002404
Iteration 194/1000 | Loss: 0.00002404
Iteration 195/1000 | Loss: 0.00002404
Iteration 196/1000 | Loss: 0.00002404
Iteration 197/1000 | Loss: 0.00002404
Iteration 198/1000 | Loss: 0.00002404
Iteration 199/1000 | Loss: 0.00002404
Iteration 200/1000 | Loss: 0.00002404
Iteration 201/1000 | Loss: 0.00002404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.403904181846883e-05, 2.403904181846883e-05, 2.403904181846883e-05, 2.403904181846883e-05, 2.403904181846883e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.403904181846883e-05

Optimization complete. Final v2v error: 3.3885982036590576 mm

Highest mean error: 12.045894622802734 mm for frame 57

Lowest mean error: 2.8516879081726074 mm for frame 215

Saving results

Total time: 201.5243399143219
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850360
Iteration 2/25 | Loss: 0.00107557
Iteration 3/25 | Loss: 0.00068202
Iteration 4/25 | Loss: 0.00064377
Iteration 5/25 | Loss: 0.00061918
Iteration 6/25 | Loss: 0.00061560
Iteration 7/25 | Loss: 0.00061662
Iteration 8/25 | Loss: 0.00061573
Iteration 9/25 | Loss: 0.00061417
Iteration 10/25 | Loss: 0.00061353
Iteration 11/25 | Loss: 0.00061304
Iteration 12/25 | Loss: 0.00061276
Iteration 13/25 | Loss: 0.00061264
Iteration 14/25 | Loss: 0.00061253
Iteration 15/25 | Loss: 0.00061247
Iteration 16/25 | Loss: 0.00061247
Iteration 17/25 | Loss: 0.00061247
Iteration 18/25 | Loss: 0.00061247
Iteration 19/25 | Loss: 0.00061247
Iteration 20/25 | Loss: 0.00061246
Iteration 21/25 | Loss: 0.00061246
Iteration 22/25 | Loss: 0.00061246
Iteration 23/25 | Loss: 0.00061246
Iteration 24/25 | Loss: 0.00061246
Iteration 25/25 | Loss: 0.00061246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.63782167
Iteration 2/25 | Loss: 0.00030186
Iteration 3/25 | Loss: 0.00030185
Iteration 4/25 | Loss: 0.00030185
Iteration 5/25 | Loss: 0.00030185
Iteration 6/25 | Loss: 0.00030184
Iteration 7/25 | Loss: 0.00030184
Iteration 8/25 | Loss: 0.00030184
Iteration 9/25 | Loss: 0.00030184
Iteration 10/25 | Loss: 0.00030184
Iteration 11/25 | Loss: 0.00030184
Iteration 12/25 | Loss: 0.00030184
Iteration 13/25 | Loss: 0.00030184
Iteration 14/25 | Loss: 0.00030184
Iteration 15/25 | Loss: 0.00030184
Iteration 16/25 | Loss: 0.00030184
Iteration 17/25 | Loss: 0.00030184
Iteration 18/25 | Loss: 0.00030184
Iteration 19/25 | Loss: 0.00030184
Iteration 20/25 | Loss: 0.00030184
Iteration 21/25 | Loss: 0.00030184
Iteration 22/25 | Loss: 0.00030184
Iteration 23/25 | Loss: 0.00030184
Iteration 24/25 | Loss: 0.00030184
Iteration 25/25 | Loss: 0.00030184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030184
Iteration 2/1000 | Loss: 0.00002061
Iteration 3/1000 | Loss: 0.00001643
Iteration 4/1000 | Loss: 0.00001556
Iteration 5/1000 | Loss: 0.00001475
Iteration 6/1000 | Loss: 0.00001438
Iteration 7/1000 | Loss: 0.00001411
Iteration 8/1000 | Loss: 0.00001390
Iteration 9/1000 | Loss: 0.00001381
Iteration 10/1000 | Loss: 0.00001374
Iteration 11/1000 | Loss: 0.00001364
Iteration 12/1000 | Loss: 0.00001360
Iteration 13/1000 | Loss: 0.00001359
Iteration 14/1000 | Loss: 0.00001359
Iteration 15/1000 | Loss: 0.00001358
Iteration 16/1000 | Loss: 0.00001357
Iteration 17/1000 | Loss: 0.00001357
Iteration 18/1000 | Loss: 0.00001357
Iteration 19/1000 | Loss: 0.00001357
Iteration 20/1000 | Loss: 0.00001356
Iteration 21/1000 | Loss: 0.00001356
Iteration 22/1000 | Loss: 0.00001355
Iteration 23/1000 | Loss: 0.00001355
Iteration 24/1000 | Loss: 0.00001354
Iteration 25/1000 | Loss: 0.00001354
Iteration 26/1000 | Loss: 0.00001354
Iteration 27/1000 | Loss: 0.00001353
Iteration 28/1000 | Loss: 0.00001353
Iteration 29/1000 | Loss: 0.00001353
Iteration 30/1000 | Loss: 0.00001352
Iteration 31/1000 | Loss: 0.00001352
Iteration 32/1000 | Loss: 0.00001352
Iteration 33/1000 | Loss: 0.00001351
Iteration 34/1000 | Loss: 0.00001351
Iteration 35/1000 | Loss: 0.00001351
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001350
Iteration 38/1000 | Loss: 0.00001350
Iteration 39/1000 | Loss: 0.00001350
Iteration 40/1000 | Loss: 0.00001350
Iteration 41/1000 | Loss: 0.00001349
Iteration 42/1000 | Loss: 0.00001349
Iteration 43/1000 | Loss: 0.00001349
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001348
Iteration 46/1000 | Loss: 0.00001348
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00001347
Iteration 49/1000 | Loss: 0.00001347
Iteration 50/1000 | Loss: 0.00001347
Iteration 51/1000 | Loss: 0.00001347
Iteration 52/1000 | Loss: 0.00001347
Iteration 53/1000 | Loss: 0.00001347
Iteration 54/1000 | Loss: 0.00001347
Iteration 55/1000 | Loss: 0.00001346
Iteration 56/1000 | Loss: 0.00001346
Iteration 57/1000 | Loss: 0.00001346
Iteration 58/1000 | Loss: 0.00001345
Iteration 59/1000 | Loss: 0.00001345
Iteration 60/1000 | Loss: 0.00001345
Iteration 61/1000 | Loss: 0.00001345
Iteration 62/1000 | Loss: 0.00001345
Iteration 63/1000 | Loss: 0.00001345
Iteration 64/1000 | Loss: 0.00001344
Iteration 65/1000 | Loss: 0.00001344
Iteration 66/1000 | Loss: 0.00001344
Iteration 67/1000 | Loss: 0.00001343
Iteration 68/1000 | Loss: 0.00001343
Iteration 69/1000 | Loss: 0.00001343
Iteration 70/1000 | Loss: 0.00001343
Iteration 71/1000 | Loss: 0.00001342
Iteration 72/1000 | Loss: 0.00001342
Iteration 73/1000 | Loss: 0.00001342
Iteration 74/1000 | Loss: 0.00001342
Iteration 75/1000 | Loss: 0.00001342
Iteration 76/1000 | Loss: 0.00001341
Iteration 77/1000 | Loss: 0.00001341
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001340
Iteration 80/1000 | Loss: 0.00001340
Iteration 81/1000 | Loss: 0.00001340
Iteration 82/1000 | Loss: 0.00001340
Iteration 83/1000 | Loss: 0.00001339
Iteration 84/1000 | Loss: 0.00001339
Iteration 85/1000 | Loss: 0.00001339
Iteration 86/1000 | Loss: 0.00001339
Iteration 87/1000 | Loss: 0.00001339
Iteration 88/1000 | Loss: 0.00001339
Iteration 89/1000 | Loss: 0.00001339
Iteration 90/1000 | Loss: 0.00001338
Iteration 91/1000 | Loss: 0.00001338
Iteration 92/1000 | Loss: 0.00001337
Iteration 93/1000 | Loss: 0.00001337
Iteration 94/1000 | Loss: 0.00001337
Iteration 95/1000 | Loss: 0.00001337
Iteration 96/1000 | Loss: 0.00001337
Iteration 97/1000 | Loss: 0.00001336
Iteration 98/1000 | Loss: 0.00001336
Iteration 99/1000 | Loss: 0.00001336
Iteration 100/1000 | Loss: 0.00001336
Iteration 101/1000 | Loss: 0.00001336
Iteration 102/1000 | Loss: 0.00001336
Iteration 103/1000 | Loss: 0.00001336
Iteration 104/1000 | Loss: 0.00001336
Iteration 105/1000 | Loss: 0.00001335
Iteration 106/1000 | Loss: 0.00001335
Iteration 107/1000 | Loss: 0.00001334
Iteration 108/1000 | Loss: 0.00001334
Iteration 109/1000 | Loss: 0.00001334
Iteration 110/1000 | Loss: 0.00001334
Iteration 111/1000 | Loss: 0.00001334
Iteration 112/1000 | Loss: 0.00001334
Iteration 113/1000 | Loss: 0.00001334
Iteration 114/1000 | Loss: 0.00001334
Iteration 115/1000 | Loss: 0.00001334
Iteration 116/1000 | Loss: 0.00001334
Iteration 117/1000 | Loss: 0.00001334
Iteration 118/1000 | Loss: 0.00001334
Iteration 119/1000 | Loss: 0.00001334
Iteration 120/1000 | Loss: 0.00001334
Iteration 121/1000 | Loss: 0.00001334
Iteration 122/1000 | Loss: 0.00001334
Iteration 123/1000 | Loss: 0.00001334
Iteration 124/1000 | Loss: 0.00001334
Iteration 125/1000 | Loss: 0.00001334
Iteration 126/1000 | Loss: 0.00001334
Iteration 127/1000 | Loss: 0.00001334
Iteration 128/1000 | Loss: 0.00001334
Iteration 129/1000 | Loss: 0.00001334
Iteration 130/1000 | Loss: 0.00001334
Iteration 131/1000 | Loss: 0.00001334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.3342014426598325e-05, 1.3342014426598325e-05, 1.3342014426598325e-05, 1.3342014426598325e-05, 1.3342014426598325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3342014426598325e-05

Optimization complete. Final v2v error: 3.117758274078369 mm

Highest mean error: 3.444373607635498 mm for frame 220

Lowest mean error: 2.7560250759124756 mm for frame 180

Saving results

Total time: 56.54336953163147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040406
Iteration 2/25 | Loss: 0.00271219
Iteration 3/25 | Loss: 0.00172419
Iteration 4/25 | Loss: 0.00150671
Iteration 5/25 | Loss: 0.00159781
Iteration 6/25 | Loss: 0.00134253
Iteration 7/25 | Loss: 0.00114196
Iteration 8/25 | Loss: 0.00096378
Iteration 9/25 | Loss: 0.00091745
Iteration 10/25 | Loss: 0.00089771
Iteration 11/25 | Loss: 0.00087564
Iteration 12/25 | Loss: 0.00086637
Iteration 13/25 | Loss: 0.00085929
Iteration 14/25 | Loss: 0.00085429
Iteration 15/25 | Loss: 0.00085766
Iteration 16/25 | Loss: 0.00084722
Iteration 17/25 | Loss: 0.00084075
Iteration 18/25 | Loss: 0.00083876
Iteration 19/25 | Loss: 0.00084108
Iteration 20/25 | Loss: 0.00084759
Iteration 21/25 | Loss: 0.00083632
Iteration 22/25 | Loss: 0.00083916
Iteration 23/25 | Loss: 0.00084325
Iteration 24/25 | Loss: 0.00083490
Iteration 25/25 | Loss: 0.00083248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45880294
Iteration 2/25 | Loss: 0.00253956
Iteration 3/25 | Loss: 0.00243066
Iteration 4/25 | Loss: 0.00243066
Iteration 5/25 | Loss: 0.00243066
Iteration 6/25 | Loss: 0.00243066
Iteration 7/25 | Loss: 0.00243066
Iteration 8/25 | Loss: 0.00243066
Iteration 9/25 | Loss: 0.00243066
Iteration 10/25 | Loss: 0.00243066
Iteration 11/25 | Loss: 0.00243066
Iteration 12/25 | Loss: 0.00243066
Iteration 13/25 | Loss: 0.00243066
Iteration 14/25 | Loss: 0.00243066
Iteration 15/25 | Loss: 0.00243066
Iteration 16/25 | Loss: 0.00243066
Iteration 17/25 | Loss: 0.00243066
Iteration 18/25 | Loss: 0.00243066
Iteration 19/25 | Loss: 0.00243066
Iteration 20/25 | Loss: 0.00243066
Iteration 21/25 | Loss: 0.00243066
Iteration 22/25 | Loss: 0.00243066
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002430659718811512, 0.002430659718811512, 0.002430659718811512, 0.002430659718811512, 0.002430659718811512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002430659718811512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243066
Iteration 2/1000 | Loss: 0.00684208
Iteration 3/1000 | Loss: 0.00065980
Iteration 4/1000 | Loss: 0.00057510
Iteration 5/1000 | Loss: 0.00033011
Iteration 6/1000 | Loss: 0.00041505
Iteration 7/1000 | Loss: 0.00025690
Iteration 8/1000 | Loss: 0.00030748
Iteration 9/1000 | Loss: 0.00094043
Iteration 10/1000 | Loss: 0.00053064
Iteration 11/1000 | Loss: 0.00076039
Iteration 12/1000 | Loss: 0.00233161
Iteration 13/1000 | Loss: 0.00175391
Iteration 14/1000 | Loss: 0.00168917
Iteration 15/1000 | Loss: 0.00100298
Iteration 16/1000 | Loss: 0.00144867
Iteration 17/1000 | Loss: 0.00237465
Iteration 18/1000 | Loss: 0.00166940
Iteration 19/1000 | Loss: 0.00038064
Iteration 20/1000 | Loss: 0.00181556
Iteration 21/1000 | Loss: 0.00185333
Iteration 22/1000 | Loss: 0.00092953
Iteration 23/1000 | Loss: 0.00113687
Iteration 24/1000 | Loss: 0.00126707
Iteration 25/1000 | Loss: 0.00067922
Iteration 26/1000 | Loss: 0.00053733
Iteration 27/1000 | Loss: 0.00069812
Iteration 28/1000 | Loss: 0.00038820
Iteration 29/1000 | Loss: 0.00050911
Iteration 30/1000 | Loss: 0.00086178
Iteration 31/1000 | Loss: 0.00093952
Iteration 32/1000 | Loss: 0.00029624
Iteration 33/1000 | Loss: 0.00013209
Iteration 34/1000 | Loss: 0.00031314
Iteration 35/1000 | Loss: 0.00054904
Iteration 36/1000 | Loss: 0.00072205
Iteration 37/1000 | Loss: 0.00062212
Iteration 38/1000 | Loss: 0.00117913
Iteration 39/1000 | Loss: 0.00118875
Iteration 40/1000 | Loss: 0.00142702
Iteration 41/1000 | Loss: 0.00109364
Iteration 42/1000 | Loss: 0.00088041
Iteration 43/1000 | Loss: 0.00110952
Iteration 44/1000 | Loss: 0.00077383
Iteration 45/1000 | Loss: 0.00054677
Iteration 46/1000 | Loss: 0.00024263
Iteration 47/1000 | Loss: 0.00050548
Iteration 48/1000 | Loss: 0.00015091
Iteration 49/1000 | Loss: 0.00009824
Iteration 50/1000 | Loss: 0.00038905
Iteration 51/1000 | Loss: 0.00015172
Iteration 52/1000 | Loss: 0.00035392
Iteration 53/1000 | Loss: 0.00034327
Iteration 54/1000 | Loss: 0.00038504
Iteration 55/1000 | Loss: 0.00040931
Iteration 56/1000 | Loss: 0.00034565
Iteration 57/1000 | Loss: 0.00011889
Iteration 58/1000 | Loss: 0.00033440
Iteration 59/1000 | Loss: 0.00024373
Iteration 60/1000 | Loss: 0.00013849
Iteration 61/1000 | Loss: 0.00007917
Iteration 62/1000 | Loss: 0.00004086
Iteration 63/1000 | Loss: 0.00003748
Iteration 64/1000 | Loss: 0.00005220
Iteration 65/1000 | Loss: 0.00049919
Iteration 66/1000 | Loss: 0.00030834
Iteration 67/1000 | Loss: 0.00005328
Iteration 68/1000 | Loss: 0.00004184
Iteration 69/1000 | Loss: 0.00003549
Iteration 70/1000 | Loss: 0.00003762
Iteration 71/1000 | Loss: 0.00017205
Iteration 72/1000 | Loss: 0.00010061
Iteration 73/1000 | Loss: 0.00003453
Iteration 74/1000 | Loss: 0.00004911
Iteration 75/1000 | Loss: 0.00013349
Iteration 76/1000 | Loss: 0.00011773
Iteration 77/1000 | Loss: 0.00006837
Iteration 78/1000 | Loss: 0.00011423
Iteration 79/1000 | Loss: 0.00023398
Iteration 80/1000 | Loss: 0.00036677
Iteration 81/1000 | Loss: 0.00039737
Iteration 82/1000 | Loss: 0.00014498
Iteration 83/1000 | Loss: 0.00026463
Iteration 84/1000 | Loss: 0.00075615
Iteration 85/1000 | Loss: 0.00018383
Iteration 86/1000 | Loss: 0.00003412
Iteration 87/1000 | Loss: 0.00005674
Iteration 88/1000 | Loss: 0.00002491
Iteration 89/1000 | Loss: 0.00003785
Iteration 90/1000 | Loss: 0.00003492
Iteration 91/1000 | Loss: 0.00005504
Iteration 92/1000 | Loss: 0.00004399
Iteration 93/1000 | Loss: 0.00025889
Iteration 94/1000 | Loss: 0.00023786
Iteration 95/1000 | Loss: 0.00026647
Iteration 96/1000 | Loss: 0.00005458
Iteration 97/1000 | Loss: 0.00002463
Iteration 98/1000 | Loss: 0.00005249
Iteration 99/1000 | Loss: 0.00004035
Iteration 100/1000 | Loss: 0.00002889
Iteration 101/1000 | Loss: 0.00003187
Iteration 102/1000 | Loss: 0.00004220
Iteration 103/1000 | Loss: 0.00003824
Iteration 104/1000 | Loss: 0.00003274
Iteration 105/1000 | Loss: 0.00003411
Iteration 106/1000 | Loss: 0.00003371
Iteration 107/1000 | Loss: 0.00003159
Iteration 108/1000 | Loss: 0.00002831
Iteration 109/1000 | Loss: 0.00002635
Iteration 110/1000 | Loss: 0.00003123
Iteration 111/1000 | Loss: 0.00017223
Iteration 112/1000 | Loss: 0.00006421
Iteration 113/1000 | Loss: 0.00006974
Iteration 114/1000 | Loss: 0.00003781
Iteration 115/1000 | Loss: 0.00003244
Iteration 116/1000 | Loss: 0.00002849
Iteration 117/1000 | Loss: 0.00002597
Iteration 118/1000 | Loss: 0.00002574
Iteration 119/1000 | Loss: 0.00002618
Iteration 120/1000 | Loss: 0.00003115
Iteration 121/1000 | Loss: 0.00003221
Iteration 122/1000 | Loss: 0.00002927
Iteration 123/1000 | Loss: 0.00003206
Iteration 124/1000 | Loss: 0.00002890
Iteration 125/1000 | Loss: 0.00003105
Iteration 126/1000 | Loss: 0.00002621
Iteration 127/1000 | Loss: 0.00003354
Iteration 128/1000 | Loss: 0.00002135
Iteration 129/1000 | Loss: 0.00002835
Iteration 130/1000 | Loss: 0.00003561
Iteration 131/1000 | Loss: 0.00003137
Iteration 132/1000 | Loss: 0.00002026
Iteration 133/1000 | Loss: 0.00003316
Iteration 134/1000 | Loss: 0.00003526
Iteration 135/1000 | Loss: 0.00002564
Iteration 136/1000 | Loss: 0.00003223
Iteration 137/1000 | Loss: 0.00003452
Iteration 138/1000 | Loss: 0.00003475
Iteration 139/1000 | Loss: 0.00003002
Iteration 140/1000 | Loss: 0.00002755
Iteration 141/1000 | Loss: 0.00002778
Iteration 142/1000 | Loss: 0.00002840
Iteration 143/1000 | Loss: 0.00002771
Iteration 144/1000 | Loss: 0.00002779
Iteration 145/1000 | Loss: 0.00002435
Iteration 146/1000 | Loss: 0.00003452
Iteration 147/1000 | Loss: 0.00002794
Iteration 148/1000 | Loss: 0.00003426
Iteration 149/1000 | Loss: 0.00003177
Iteration 150/1000 | Loss: 0.00002796
Iteration 151/1000 | Loss: 0.00002579
Iteration 152/1000 | Loss: 0.00002794
Iteration 153/1000 | Loss: 0.00003072
Iteration 154/1000 | Loss: 0.00002867
Iteration 155/1000 | Loss: 0.00003357
Iteration 156/1000 | Loss: 0.00004134
Iteration 157/1000 | Loss: 0.00003572
Iteration 158/1000 | Loss: 0.00002813
Iteration 159/1000 | Loss: 0.00003367
Iteration 160/1000 | Loss: 0.00003113
Iteration 161/1000 | Loss: 0.00003076
Iteration 162/1000 | Loss: 0.00001938
Iteration 163/1000 | Loss: 0.00003008
Iteration 164/1000 | Loss: 0.00003826
Iteration 165/1000 | Loss: 0.00003320
Iteration 166/1000 | Loss: 0.00003194
Iteration 167/1000 | Loss: 0.00004193
Iteration 168/1000 | Loss: 0.00003065
Iteration 169/1000 | Loss: 0.00004351
Iteration 170/1000 | Loss: 0.00004416
Iteration 171/1000 | Loss: 0.00003533
Iteration 172/1000 | Loss: 0.00003447
Iteration 173/1000 | Loss: 0.00002071
Iteration 174/1000 | Loss: 0.00003095
Iteration 175/1000 | Loss: 0.00003575
Iteration 176/1000 | Loss: 0.00003168
Iteration 177/1000 | Loss: 0.00003589
Iteration 178/1000 | Loss: 0.00004043
Iteration 179/1000 | Loss: 0.00004043
Iteration 180/1000 | Loss: 0.00003771
Iteration 181/1000 | Loss: 0.00003950
Iteration 182/1000 | Loss: 0.00003058
Iteration 183/1000 | Loss: 0.00006216
Iteration 184/1000 | Loss: 0.00003159
Iteration 185/1000 | Loss: 0.00003640
Iteration 186/1000 | Loss: 0.00003379
Iteration 187/1000 | Loss: 0.00002603
Iteration 188/1000 | Loss: 0.00003797
Iteration 189/1000 | Loss: 0.00003177
Iteration 190/1000 | Loss: 0.00002909
Iteration 191/1000 | Loss: 0.00003462
Iteration 192/1000 | Loss: 0.00003014
Iteration 193/1000 | Loss: 0.00003652
Iteration 194/1000 | Loss: 0.00003443
Iteration 195/1000 | Loss: 0.00003859
Iteration 196/1000 | Loss: 0.00003121
Iteration 197/1000 | Loss: 0.00003345
Iteration 198/1000 | Loss: 0.00003192
Iteration 199/1000 | Loss: 0.00003735
Iteration 200/1000 | Loss: 0.00002955
Iteration 201/1000 | Loss: 0.00002709
Iteration 202/1000 | Loss: 0.00003391
Iteration 203/1000 | Loss: 0.00003221
Iteration 204/1000 | Loss: 0.00003682
Iteration 205/1000 | Loss: 0.00003353
Iteration 206/1000 | Loss: 0.00003300
Iteration 207/1000 | Loss: 0.00003236
Iteration 208/1000 | Loss: 0.00003167
Iteration 209/1000 | Loss: 0.00003176
Iteration 210/1000 | Loss: 0.00002891
Iteration 211/1000 | Loss: 0.00002837
Iteration 212/1000 | Loss: 0.00003256
Iteration 213/1000 | Loss: 0.00003358
Iteration 214/1000 | Loss: 0.00003471
Iteration 215/1000 | Loss: 0.00003366
Iteration 216/1000 | Loss: 0.00003109
Iteration 217/1000 | Loss: 0.00003416
Iteration 218/1000 | Loss: 0.00003401
Iteration 219/1000 | Loss: 0.00003279
Iteration 220/1000 | Loss: 0.00003309
Iteration 221/1000 | Loss: 0.00004086
Iteration 222/1000 | Loss: 0.00002529
Iteration 223/1000 | Loss: 0.00001990
Iteration 224/1000 | Loss: 0.00001780
Iteration 225/1000 | Loss: 0.00001677
Iteration 226/1000 | Loss: 0.00001608
Iteration 227/1000 | Loss: 0.00001569
Iteration 228/1000 | Loss: 0.00001547
Iteration 229/1000 | Loss: 0.00001546
Iteration 230/1000 | Loss: 0.00001546
Iteration 231/1000 | Loss: 0.00001522
Iteration 232/1000 | Loss: 0.00001503
Iteration 233/1000 | Loss: 0.00022137
Iteration 234/1000 | Loss: 0.00021940
Iteration 235/1000 | Loss: 0.00101481
Iteration 236/1000 | Loss: 0.00022140
Iteration 237/1000 | Loss: 0.00011878
Iteration 238/1000 | Loss: 0.00001571
Iteration 239/1000 | Loss: 0.00019051
Iteration 240/1000 | Loss: 0.00026909
Iteration 241/1000 | Loss: 0.00018002
Iteration 242/1000 | Loss: 0.00008491
Iteration 243/1000 | Loss: 0.00010097
Iteration 244/1000 | Loss: 0.00004193
Iteration 245/1000 | Loss: 0.00002200
Iteration 246/1000 | Loss: 0.00004487
Iteration 247/1000 | Loss: 0.00001693
Iteration 248/1000 | Loss: 0.00003947
Iteration 249/1000 | Loss: 0.00001515
Iteration 250/1000 | Loss: 0.00001432
Iteration 251/1000 | Loss: 0.00001639
Iteration 252/1000 | Loss: 0.00001446
Iteration 253/1000 | Loss: 0.00001573
Iteration 254/1000 | Loss: 0.00001330
Iteration 255/1000 | Loss: 0.00001309
Iteration 256/1000 | Loss: 0.00001308
Iteration 257/1000 | Loss: 0.00001293
Iteration 258/1000 | Loss: 0.00001287
Iteration 259/1000 | Loss: 0.00001286
Iteration 260/1000 | Loss: 0.00001286
Iteration 261/1000 | Loss: 0.00001285
Iteration 262/1000 | Loss: 0.00001285
Iteration 263/1000 | Loss: 0.00001284
Iteration 264/1000 | Loss: 0.00021099
Iteration 265/1000 | Loss: 0.00019978
Iteration 266/1000 | Loss: 0.00001580
Iteration 267/1000 | Loss: 0.00001344
Iteration 268/1000 | Loss: 0.00021745
Iteration 269/1000 | Loss: 0.00008910
Iteration 270/1000 | Loss: 0.00001915
Iteration 271/1000 | Loss: 0.00001563
Iteration 272/1000 | Loss: 0.00012151
Iteration 273/1000 | Loss: 0.00022913
Iteration 274/1000 | Loss: 0.00003983
Iteration 275/1000 | Loss: 0.00007053
Iteration 276/1000 | Loss: 0.00001845
Iteration 277/1000 | Loss: 0.00001579
Iteration 278/1000 | Loss: 0.00001769
Iteration 279/1000 | Loss: 0.00001485
Iteration 280/1000 | Loss: 0.00001548
Iteration 281/1000 | Loss: 0.00001364
Iteration 282/1000 | Loss: 0.00002595
Iteration 283/1000 | Loss: 0.00003131
Iteration 284/1000 | Loss: 0.00002169
Iteration 285/1000 | Loss: 0.00001309
Iteration 286/1000 | Loss: 0.00001434
Iteration 287/1000 | Loss: 0.00001251
Iteration 288/1000 | Loss: 0.00001250
Iteration 289/1000 | Loss: 0.00001249
Iteration 290/1000 | Loss: 0.00001653
Iteration 291/1000 | Loss: 0.00001214
Iteration 292/1000 | Loss: 0.00001211
Iteration 293/1000 | Loss: 0.00001209
Iteration 294/1000 | Loss: 0.00001209
Iteration 295/1000 | Loss: 0.00001516
Iteration 296/1000 | Loss: 0.00001244
Iteration 297/1000 | Loss: 0.00001235
Iteration 298/1000 | Loss: 0.00001189
Iteration 299/1000 | Loss: 0.00001188
Iteration 300/1000 | Loss: 0.00001187
Iteration 301/1000 | Loss: 0.00001187
Iteration 302/1000 | Loss: 0.00001187
Iteration 303/1000 | Loss: 0.00002007
Iteration 304/1000 | Loss: 0.00001187
Iteration 305/1000 | Loss: 0.00001187
Iteration 306/1000 | Loss: 0.00001185
Iteration 307/1000 | Loss: 0.00001184
Iteration 308/1000 | Loss: 0.00001182
Iteration 309/1000 | Loss: 0.00001181
Iteration 310/1000 | Loss: 0.00001180
Iteration 311/1000 | Loss: 0.00001180
Iteration 312/1000 | Loss: 0.00001180
Iteration 313/1000 | Loss: 0.00001179
Iteration 314/1000 | Loss: 0.00001179
Iteration 315/1000 | Loss: 0.00001179
Iteration 316/1000 | Loss: 0.00001178
Iteration 317/1000 | Loss: 0.00001178
Iteration 318/1000 | Loss: 0.00001178
Iteration 319/1000 | Loss: 0.00001178
Iteration 320/1000 | Loss: 0.00001177
Iteration 321/1000 | Loss: 0.00001177
Iteration 322/1000 | Loss: 0.00001177
Iteration 323/1000 | Loss: 0.00001177
Iteration 324/1000 | Loss: 0.00001177
Iteration 325/1000 | Loss: 0.00001177
Iteration 326/1000 | Loss: 0.00001176
Iteration 327/1000 | Loss: 0.00001176
Iteration 328/1000 | Loss: 0.00001176
Iteration 329/1000 | Loss: 0.00001176
Iteration 330/1000 | Loss: 0.00001176
Iteration 331/1000 | Loss: 0.00001176
Iteration 332/1000 | Loss: 0.00001176
Iteration 333/1000 | Loss: 0.00001176
Iteration 334/1000 | Loss: 0.00001175
Iteration 335/1000 | Loss: 0.00001175
Iteration 336/1000 | Loss: 0.00001175
Iteration 337/1000 | Loss: 0.00001174
Iteration 338/1000 | Loss: 0.00001174
Iteration 339/1000 | Loss: 0.00001174
Iteration 340/1000 | Loss: 0.00001174
Iteration 341/1000 | Loss: 0.00001174
Iteration 342/1000 | Loss: 0.00001174
Iteration 343/1000 | Loss: 0.00001174
Iteration 344/1000 | Loss: 0.00001174
Iteration 345/1000 | Loss: 0.00001174
Iteration 346/1000 | Loss: 0.00001174
Iteration 347/1000 | Loss: 0.00001174
Iteration 348/1000 | Loss: 0.00001174
Iteration 349/1000 | Loss: 0.00001174
Iteration 350/1000 | Loss: 0.00001174
Iteration 351/1000 | Loss: 0.00001174
Iteration 352/1000 | Loss: 0.00001173
Iteration 353/1000 | Loss: 0.00001173
Iteration 354/1000 | Loss: 0.00001173
Iteration 355/1000 | Loss: 0.00001173
Iteration 356/1000 | Loss: 0.00001173
Iteration 357/1000 | Loss: 0.00001173
Iteration 358/1000 | Loss: 0.00001173
Iteration 359/1000 | Loss: 0.00001173
Iteration 360/1000 | Loss: 0.00001173
Iteration 361/1000 | Loss: 0.00001173
Iteration 362/1000 | Loss: 0.00001173
Iteration 363/1000 | Loss: 0.00001172
Iteration 364/1000 | Loss: 0.00001172
Iteration 365/1000 | Loss: 0.00001172
Iteration 366/1000 | Loss: 0.00001172
Iteration 367/1000 | Loss: 0.00001172
Iteration 368/1000 | Loss: 0.00001172
Iteration 369/1000 | Loss: 0.00001172
Iteration 370/1000 | Loss: 0.00001172
Iteration 371/1000 | Loss: 0.00001172
Iteration 372/1000 | Loss: 0.00001172
Iteration 373/1000 | Loss: 0.00001172
Iteration 374/1000 | Loss: 0.00001172
Iteration 375/1000 | Loss: 0.00001172
Iteration 376/1000 | Loss: 0.00001172
Iteration 377/1000 | Loss: 0.00001172
Iteration 378/1000 | Loss: 0.00001172
Iteration 379/1000 | Loss: 0.00001172
Iteration 380/1000 | Loss: 0.00001171
Iteration 381/1000 | Loss: 0.00001171
Iteration 382/1000 | Loss: 0.00001171
Iteration 383/1000 | Loss: 0.00001171
Iteration 384/1000 | Loss: 0.00001171
Iteration 385/1000 | Loss: 0.00001171
Iteration 386/1000 | Loss: 0.00001171
Iteration 387/1000 | Loss: 0.00001171
Iteration 388/1000 | Loss: 0.00001171
Iteration 389/1000 | Loss: 0.00001171
Iteration 390/1000 | Loss: 0.00001171
Iteration 391/1000 | Loss: 0.00001171
Iteration 392/1000 | Loss: 0.00001171
Iteration 393/1000 | Loss: 0.00001171
Iteration 394/1000 | Loss: 0.00001171
Iteration 395/1000 | Loss: 0.00001171
Iteration 396/1000 | Loss: 0.00001171
Iteration 397/1000 | Loss: 0.00001171
Iteration 398/1000 | Loss: 0.00001171
Iteration 399/1000 | Loss: 0.00001171
Iteration 400/1000 | Loss: 0.00001171
Iteration 401/1000 | Loss: 0.00001171
Iteration 402/1000 | Loss: 0.00001171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 402. Stopping optimization.
Last 5 losses: [1.1713213098119013e-05, 1.1713213098119013e-05, 1.1713213098119013e-05, 1.1713213098119013e-05, 1.1713213098119013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1713213098119013e-05

Optimization complete. Final v2v error: 2.8929028511047363 mm

Highest mean error: 3.9551331996917725 mm for frame 73

Lowest mean error: 2.4779038429260254 mm for frame 110

Saving results

Total time: 515.5172302722931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842972
Iteration 2/25 | Loss: 0.00099992
Iteration 3/25 | Loss: 0.00071638
Iteration 4/25 | Loss: 0.00068674
Iteration 5/25 | Loss: 0.00067712
Iteration 6/25 | Loss: 0.00067508
Iteration 7/25 | Loss: 0.00067451
Iteration 8/25 | Loss: 0.00067440
Iteration 9/25 | Loss: 0.00067440
Iteration 10/25 | Loss: 0.00067440
Iteration 11/25 | Loss: 0.00067440
Iteration 12/25 | Loss: 0.00067440
Iteration 13/25 | Loss: 0.00067440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00067439756821841, 0.00067439756821841, 0.00067439756821841, 0.00067439756821841, 0.00067439756821841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00067439756821841

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.56356812
Iteration 2/25 | Loss: 0.00031662
Iteration 3/25 | Loss: 0.00031651
Iteration 4/25 | Loss: 0.00031651
Iteration 5/25 | Loss: 0.00031651
Iteration 6/25 | Loss: 0.00031651
Iteration 7/25 | Loss: 0.00031651
Iteration 8/25 | Loss: 0.00031651
Iteration 9/25 | Loss: 0.00031651
Iteration 10/25 | Loss: 0.00031651
Iteration 11/25 | Loss: 0.00031651
Iteration 12/25 | Loss: 0.00031651
Iteration 13/25 | Loss: 0.00031651
Iteration 14/25 | Loss: 0.00031651
Iteration 15/25 | Loss: 0.00031651
Iteration 16/25 | Loss: 0.00031651
Iteration 17/25 | Loss: 0.00031651
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00031650529126636684, 0.00031650529126636684, 0.00031650529126636684, 0.00031650529126636684, 0.00031650529126636684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031650529126636684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031651
Iteration 2/1000 | Loss: 0.00003083
Iteration 3/1000 | Loss: 0.00002414
Iteration 4/1000 | Loss: 0.00002289
Iteration 5/1000 | Loss: 0.00002174
Iteration 6/1000 | Loss: 0.00002109
Iteration 7/1000 | Loss: 0.00002058
Iteration 8/1000 | Loss: 0.00002012
Iteration 9/1000 | Loss: 0.00001985
Iteration 10/1000 | Loss: 0.00001965
Iteration 11/1000 | Loss: 0.00001951
Iteration 12/1000 | Loss: 0.00001950
Iteration 13/1000 | Loss: 0.00001949
Iteration 14/1000 | Loss: 0.00001948
Iteration 15/1000 | Loss: 0.00001947
Iteration 16/1000 | Loss: 0.00001945
Iteration 17/1000 | Loss: 0.00001944
Iteration 18/1000 | Loss: 0.00001943
Iteration 19/1000 | Loss: 0.00001943
Iteration 20/1000 | Loss: 0.00001942
Iteration 21/1000 | Loss: 0.00001942
Iteration 22/1000 | Loss: 0.00001941
Iteration 23/1000 | Loss: 0.00001941
Iteration 24/1000 | Loss: 0.00001938
Iteration 25/1000 | Loss: 0.00001936
Iteration 26/1000 | Loss: 0.00001934
Iteration 27/1000 | Loss: 0.00001933
Iteration 28/1000 | Loss: 0.00001933
Iteration 29/1000 | Loss: 0.00001932
Iteration 30/1000 | Loss: 0.00001932
Iteration 31/1000 | Loss: 0.00001932
Iteration 32/1000 | Loss: 0.00001932
Iteration 33/1000 | Loss: 0.00001932
Iteration 34/1000 | Loss: 0.00001932
Iteration 35/1000 | Loss: 0.00001932
Iteration 36/1000 | Loss: 0.00001932
Iteration 37/1000 | Loss: 0.00001932
Iteration 38/1000 | Loss: 0.00001932
Iteration 39/1000 | Loss: 0.00001932
Iteration 40/1000 | Loss: 0.00001931
Iteration 41/1000 | Loss: 0.00001931
Iteration 42/1000 | Loss: 0.00001930
Iteration 43/1000 | Loss: 0.00001930
Iteration 44/1000 | Loss: 0.00001930
Iteration 45/1000 | Loss: 0.00001930
Iteration 46/1000 | Loss: 0.00001930
Iteration 47/1000 | Loss: 0.00001930
Iteration 48/1000 | Loss: 0.00001930
Iteration 49/1000 | Loss: 0.00001930
Iteration 50/1000 | Loss: 0.00001930
Iteration 51/1000 | Loss: 0.00001930
Iteration 52/1000 | Loss: 0.00001930
Iteration 53/1000 | Loss: 0.00001929
Iteration 54/1000 | Loss: 0.00001929
Iteration 55/1000 | Loss: 0.00001929
Iteration 56/1000 | Loss: 0.00001928
Iteration 57/1000 | Loss: 0.00001928
Iteration 58/1000 | Loss: 0.00001928
Iteration 59/1000 | Loss: 0.00001927
Iteration 60/1000 | Loss: 0.00001927
Iteration 61/1000 | Loss: 0.00001927
Iteration 62/1000 | Loss: 0.00001926
Iteration 63/1000 | Loss: 0.00001926
Iteration 64/1000 | Loss: 0.00001926
Iteration 65/1000 | Loss: 0.00001925
Iteration 66/1000 | Loss: 0.00001925
Iteration 67/1000 | Loss: 0.00001925
Iteration 68/1000 | Loss: 0.00001924
Iteration 69/1000 | Loss: 0.00001924
Iteration 70/1000 | Loss: 0.00001924
Iteration 71/1000 | Loss: 0.00001923
Iteration 72/1000 | Loss: 0.00001923
Iteration 73/1000 | Loss: 0.00001922
Iteration 74/1000 | Loss: 0.00001922
Iteration 75/1000 | Loss: 0.00001922
Iteration 76/1000 | Loss: 0.00001922
Iteration 77/1000 | Loss: 0.00001921
Iteration 78/1000 | Loss: 0.00001921
Iteration 79/1000 | Loss: 0.00001921
Iteration 80/1000 | Loss: 0.00001920
Iteration 81/1000 | Loss: 0.00001920
Iteration 82/1000 | Loss: 0.00001920
Iteration 83/1000 | Loss: 0.00001919
Iteration 84/1000 | Loss: 0.00001919
Iteration 85/1000 | Loss: 0.00001919
Iteration 86/1000 | Loss: 0.00001919
Iteration 87/1000 | Loss: 0.00001918
Iteration 88/1000 | Loss: 0.00001918
Iteration 89/1000 | Loss: 0.00001918
Iteration 90/1000 | Loss: 0.00001918
Iteration 91/1000 | Loss: 0.00001918
Iteration 92/1000 | Loss: 0.00001917
Iteration 93/1000 | Loss: 0.00001917
Iteration 94/1000 | Loss: 0.00001917
Iteration 95/1000 | Loss: 0.00001916
Iteration 96/1000 | Loss: 0.00001916
Iteration 97/1000 | Loss: 0.00001916
Iteration 98/1000 | Loss: 0.00001915
Iteration 99/1000 | Loss: 0.00001915
Iteration 100/1000 | Loss: 0.00001915
Iteration 101/1000 | Loss: 0.00001914
Iteration 102/1000 | Loss: 0.00001914
Iteration 103/1000 | Loss: 0.00001914
Iteration 104/1000 | Loss: 0.00001914
Iteration 105/1000 | Loss: 0.00001914
Iteration 106/1000 | Loss: 0.00001914
Iteration 107/1000 | Loss: 0.00001914
Iteration 108/1000 | Loss: 0.00001913
Iteration 109/1000 | Loss: 0.00001913
Iteration 110/1000 | Loss: 0.00001912
Iteration 111/1000 | Loss: 0.00001912
Iteration 112/1000 | Loss: 0.00001912
Iteration 113/1000 | Loss: 0.00001912
Iteration 114/1000 | Loss: 0.00001912
Iteration 115/1000 | Loss: 0.00001912
Iteration 116/1000 | Loss: 0.00001912
Iteration 117/1000 | Loss: 0.00001911
Iteration 118/1000 | Loss: 0.00001911
Iteration 119/1000 | Loss: 0.00001911
Iteration 120/1000 | Loss: 0.00001911
Iteration 121/1000 | Loss: 0.00001911
Iteration 122/1000 | Loss: 0.00001911
Iteration 123/1000 | Loss: 0.00001910
Iteration 124/1000 | Loss: 0.00001910
Iteration 125/1000 | Loss: 0.00001910
Iteration 126/1000 | Loss: 0.00001910
Iteration 127/1000 | Loss: 0.00001910
Iteration 128/1000 | Loss: 0.00001910
Iteration 129/1000 | Loss: 0.00001910
Iteration 130/1000 | Loss: 0.00001910
Iteration 131/1000 | Loss: 0.00001910
Iteration 132/1000 | Loss: 0.00001910
Iteration 133/1000 | Loss: 0.00001910
Iteration 134/1000 | Loss: 0.00001910
Iteration 135/1000 | Loss: 0.00001910
Iteration 136/1000 | Loss: 0.00001910
Iteration 137/1000 | Loss: 0.00001910
Iteration 138/1000 | Loss: 0.00001910
Iteration 139/1000 | Loss: 0.00001910
Iteration 140/1000 | Loss: 0.00001910
Iteration 141/1000 | Loss: 0.00001910
Iteration 142/1000 | Loss: 0.00001910
Iteration 143/1000 | Loss: 0.00001910
Iteration 144/1000 | Loss: 0.00001910
Iteration 145/1000 | Loss: 0.00001910
Iteration 146/1000 | Loss: 0.00001910
Iteration 147/1000 | Loss: 0.00001910
Iteration 148/1000 | Loss: 0.00001910
Iteration 149/1000 | Loss: 0.00001910
Iteration 150/1000 | Loss: 0.00001910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.9096112737315707e-05, 1.9096112737315707e-05, 1.9096112737315707e-05, 1.9096112737315707e-05, 1.9096112737315707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9096112737315707e-05

Optimization complete. Final v2v error: 3.6836650371551514 mm

Highest mean error: 4.4896392822265625 mm for frame 109

Lowest mean error: 3.2686455249786377 mm for frame 167

Saving results

Total time: 44.080601930618286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805898
Iteration 2/25 | Loss: 0.00118029
Iteration 3/25 | Loss: 0.00080542
Iteration 4/25 | Loss: 0.00073854
Iteration 5/25 | Loss: 0.00072863
Iteration 6/25 | Loss: 0.00072676
Iteration 7/25 | Loss: 0.00072662
Iteration 8/25 | Loss: 0.00072662
Iteration 9/25 | Loss: 0.00072662
Iteration 10/25 | Loss: 0.00072662
Iteration 11/25 | Loss: 0.00072662
Iteration 12/25 | Loss: 0.00072662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007266239845193923, 0.0007266239845193923, 0.0007266239845193923, 0.0007266239845193923, 0.0007266239845193923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007266239845193923

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45235670
Iteration 2/25 | Loss: 0.00037871
Iteration 3/25 | Loss: 0.00037870
Iteration 4/25 | Loss: 0.00037870
Iteration 5/25 | Loss: 0.00037870
Iteration 6/25 | Loss: 0.00037870
Iteration 7/25 | Loss: 0.00037870
Iteration 8/25 | Loss: 0.00037870
Iteration 9/25 | Loss: 0.00037869
Iteration 10/25 | Loss: 0.00037869
Iteration 11/25 | Loss: 0.00037869
Iteration 12/25 | Loss: 0.00037869
Iteration 13/25 | Loss: 0.00037869
Iteration 14/25 | Loss: 0.00037869
Iteration 15/25 | Loss: 0.00037869
Iteration 16/25 | Loss: 0.00037869
Iteration 17/25 | Loss: 0.00037869
Iteration 18/25 | Loss: 0.00037869
Iteration 19/25 | Loss: 0.00037869
Iteration 20/25 | Loss: 0.00037869
Iteration 21/25 | Loss: 0.00037869
Iteration 22/25 | Loss: 0.00037869
Iteration 23/25 | Loss: 0.00037869
Iteration 24/25 | Loss: 0.00037869
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003786943561863154, 0.0003786943561863154, 0.0003786943561863154, 0.0003786943561863154, 0.0003786943561863154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003786943561863154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037869
Iteration 2/1000 | Loss: 0.00002970
Iteration 3/1000 | Loss: 0.00002336
Iteration 4/1000 | Loss: 0.00002197
Iteration 5/1000 | Loss: 0.00002114
Iteration 6/1000 | Loss: 0.00002065
Iteration 7/1000 | Loss: 0.00002026
Iteration 8/1000 | Loss: 0.00001991
Iteration 9/1000 | Loss: 0.00001983
Iteration 10/1000 | Loss: 0.00001982
Iteration 11/1000 | Loss: 0.00001981
Iteration 12/1000 | Loss: 0.00001980
Iteration 13/1000 | Loss: 0.00001980
Iteration 14/1000 | Loss: 0.00001970
Iteration 15/1000 | Loss: 0.00001970
Iteration 16/1000 | Loss: 0.00001969
Iteration 17/1000 | Loss: 0.00001968
Iteration 18/1000 | Loss: 0.00001966
Iteration 19/1000 | Loss: 0.00001960
Iteration 20/1000 | Loss: 0.00001960
Iteration 21/1000 | Loss: 0.00001959
Iteration 22/1000 | Loss: 0.00001958
Iteration 23/1000 | Loss: 0.00001957
Iteration 24/1000 | Loss: 0.00001957
Iteration 25/1000 | Loss: 0.00001957
Iteration 26/1000 | Loss: 0.00001957
Iteration 27/1000 | Loss: 0.00001957
Iteration 28/1000 | Loss: 0.00001957
Iteration 29/1000 | Loss: 0.00001957
Iteration 30/1000 | Loss: 0.00001957
Iteration 31/1000 | Loss: 0.00001957
Iteration 32/1000 | Loss: 0.00001957
Iteration 33/1000 | Loss: 0.00001956
Iteration 34/1000 | Loss: 0.00001956
Iteration 35/1000 | Loss: 0.00001956
Iteration 36/1000 | Loss: 0.00001956
Iteration 37/1000 | Loss: 0.00001956
Iteration 38/1000 | Loss: 0.00001956
Iteration 39/1000 | Loss: 0.00001955
Iteration 40/1000 | Loss: 0.00001955
Iteration 41/1000 | Loss: 0.00001954
Iteration 42/1000 | Loss: 0.00001954
Iteration 43/1000 | Loss: 0.00001954
Iteration 44/1000 | Loss: 0.00001954
Iteration 45/1000 | Loss: 0.00001954
Iteration 46/1000 | Loss: 0.00001953
Iteration 47/1000 | Loss: 0.00001953
Iteration 48/1000 | Loss: 0.00001953
Iteration 49/1000 | Loss: 0.00001953
Iteration 50/1000 | Loss: 0.00001953
Iteration 51/1000 | Loss: 0.00001953
Iteration 52/1000 | Loss: 0.00001953
Iteration 53/1000 | Loss: 0.00001953
Iteration 54/1000 | Loss: 0.00001953
Iteration 55/1000 | Loss: 0.00001953
Iteration 56/1000 | Loss: 0.00001952
Iteration 57/1000 | Loss: 0.00001952
Iteration 58/1000 | Loss: 0.00001952
Iteration 59/1000 | Loss: 0.00001952
Iteration 60/1000 | Loss: 0.00001952
Iteration 61/1000 | Loss: 0.00001951
Iteration 62/1000 | Loss: 0.00001951
Iteration 63/1000 | Loss: 0.00001951
Iteration 64/1000 | Loss: 0.00001951
Iteration 65/1000 | Loss: 0.00001951
Iteration 66/1000 | Loss: 0.00001950
Iteration 67/1000 | Loss: 0.00001950
Iteration 68/1000 | Loss: 0.00001950
Iteration 69/1000 | Loss: 0.00001950
Iteration 70/1000 | Loss: 0.00001950
Iteration 71/1000 | Loss: 0.00001950
Iteration 72/1000 | Loss: 0.00001950
Iteration 73/1000 | Loss: 0.00001950
Iteration 74/1000 | Loss: 0.00001950
Iteration 75/1000 | Loss: 0.00001950
Iteration 76/1000 | Loss: 0.00001950
Iteration 77/1000 | Loss: 0.00001950
Iteration 78/1000 | Loss: 0.00001949
Iteration 79/1000 | Loss: 0.00001949
Iteration 80/1000 | Loss: 0.00001949
Iteration 81/1000 | Loss: 0.00001949
Iteration 82/1000 | Loss: 0.00001949
Iteration 83/1000 | Loss: 0.00001949
Iteration 84/1000 | Loss: 0.00001949
Iteration 85/1000 | Loss: 0.00001949
Iteration 86/1000 | Loss: 0.00001949
Iteration 87/1000 | Loss: 0.00001949
Iteration 88/1000 | Loss: 0.00001949
Iteration 89/1000 | Loss: 0.00001949
Iteration 90/1000 | Loss: 0.00001949
Iteration 91/1000 | Loss: 0.00001949
Iteration 92/1000 | Loss: 0.00001949
Iteration 93/1000 | Loss: 0.00001949
Iteration 94/1000 | Loss: 0.00001949
Iteration 95/1000 | Loss: 0.00001949
Iteration 96/1000 | Loss: 0.00001949
Iteration 97/1000 | Loss: 0.00001949
Iteration 98/1000 | Loss: 0.00001949
Iteration 99/1000 | Loss: 0.00001949
Iteration 100/1000 | Loss: 0.00001949
Iteration 101/1000 | Loss: 0.00001949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.9489181795506738e-05, 1.9489181795506738e-05, 1.9489181795506738e-05, 1.9489181795506738e-05, 1.9489181795506738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9489181795506738e-05

Optimization complete. Final v2v error: 3.6718850135803223 mm

Highest mean error: 3.831451177597046 mm for frame 63

Lowest mean error: 3.3875012397766113 mm for frame 8

Saving results

Total time: 34.45973324775696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894075
Iteration 2/25 | Loss: 0.00116977
Iteration 3/25 | Loss: 0.00090903
Iteration 4/25 | Loss: 0.00082839
Iteration 5/25 | Loss: 0.00081242
Iteration 6/25 | Loss: 0.00080953
Iteration 7/25 | Loss: 0.00080837
Iteration 8/25 | Loss: 0.00080830
Iteration 9/25 | Loss: 0.00080830
Iteration 10/25 | Loss: 0.00080830
Iteration 11/25 | Loss: 0.00080830
Iteration 12/25 | Loss: 0.00080830
Iteration 13/25 | Loss: 0.00080830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008082985295914114, 0.0008082985295914114, 0.0008082985295914114, 0.0008082985295914114, 0.0008082985295914114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008082985295914114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47406507
Iteration 2/25 | Loss: 0.00069128
Iteration 3/25 | Loss: 0.00069128
Iteration 4/25 | Loss: 0.00069128
Iteration 5/25 | Loss: 0.00069128
Iteration 6/25 | Loss: 0.00069128
Iteration 7/25 | Loss: 0.00069128
Iteration 8/25 | Loss: 0.00069128
Iteration 9/25 | Loss: 0.00069128
Iteration 10/25 | Loss: 0.00069128
Iteration 11/25 | Loss: 0.00069128
Iteration 12/25 | Loss: 0.00069128
Iteration 13/25 | Loss: 0.00069128
Iteration 14/25 | Loss: 0.00069128
Iteration 15/25 | Loss: 0.00069128
Iteration 16/25 | Loss: 0.00069128
Iteration 17/25 | Loss: 0.00069128
Iteration 18/25 | Loss: 0.00069128
Iteration 19/25 | Loss: 0.00069128
Iteration 20/25 | Loss: 0.00069128
Iteration 21/25 | Loss: 0.00069128
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006912783719599247, 0.0006912783719599247, 0.0006912783719599247, 0.0006912783719599247, 0.0006912783719599247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006912783719599247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069128
Iteration 2/1000 | Loss: 0.00006331
Iteration 3/1000 | Loss: 0.00004568
Iteration 4/1000 | Loss: 0.00003759
Iteration 5/1000 | Loss: 0.00003185
Iteration 6/1000 | Loss: 0.00003019
Iteration 7/1000 | Loss: 0.00002909
Iteration 8/1000 | Loss: 0.00002832
Iteration 9/1000 | Loss: 0.00002784
Iteration 10/1000 | Loss: 0.00002749
Iteration 11/1000 | Loss: 0.00002722
Iteration 12/1000 | Loss: 0.00002699
Iteration 13/1000 | Loss: 0.00002692
Iteration 14/1000 | Loss: 0.00002680
Iteration 15/1000 | Loss: 0.00002674
Iteration 16/1000 | Loss: 0.00002674
Iteration 17/1000 | Loss: 0.00002674
Iteration 18/1000 | Loss: 0.00002674
Iteration 19/1000 | Loss: 0.00002674
Iteration 20/1000 | Loss: 0.00002673
Iteration 21/1000 | Loss: 0.00002673
Iteration 22/1000 | Loss: 0.00002672
Iteration 23/1000 | Loss: 0.00002667
Iteration 24/1000 | Loss: 0.00002664
Iteration 25/1000 | Loss: 0.00002663
Iteration 26/1000 | Loss: 0.00002663
Iteration 27/1000 | Loss: 0.00002663
Iteration 28/1000 | Loss: 0.00002663
Iteration 29/1000 | Loss: 0.00002662
Iteration 30/1000 | Loss: 0.00002662
Iteration 31/1000 | Loss: 0.00002662
Iteration 32/1000 | Loss: 0.00002661
Iteration 33/1000 | Loss: 0.00002661
Iteration 34/1000 | Loss: 0.00002661
Iteration 35/1000 | Loss: 0.00002660
Iteration 36/1000 | Loss: 0.00002660
Iteration 37/1000 | Loss: 0.00002660
Iteration 38/1000 | Loss: 0.00002660
Iteration 39/1000 | Loss: 0.00002659
Iteration 40/1000 | Loss: 0.00002659
Iteration 41/1000 | Loss: 0.00002657
Iteration 42/1000 | Loss: 0.00002657
Iteration 43/1000 | Loss: 0.00002657
Iteration 44/1000 | Loss: 0.00002656
Iteration 45/1000 | Loss: 0.00002656
Iteration 46/1000 | Loss: 0.00002656
Iteration 47/1000 | Loss: 0.00002656
Iteration 48/1000 | Loss: 0.00002656
Iteration 49/1000 | Loss: 0.00002656
Iteration 50/1000 | Loss: 0.00002656
Iteration 51/1000 | Loss: 0.00002656
Iteration 52/1000 | Loss: 0.00002656
Iteration 53/1000 | Loss: 0.00002656
Iteration 54/1000 | Loss: 0.00002656
Iteration 55/1000 | Loss: 0.00002656
Iteration 56/1000 | Loss: 0.00002656
Iteration 57/1000 | Loss: 0.00002656
Iteration 58/1000 | Loss: 0.00002656
Iteration 59/1000 | Loss: 0.00002656
Iteration 60/1000 | Loss: 0.00002656
Iteration 61/1000 | Loss: 0.00002656
Iteration 62/1000 | Loss: 0.00002656
Iteration 63/1000 | Loss: 0.00002656
Iteration 64/1000 | Loss: 0.00002656
Iteration 65/1000 | Loss: 0.00002656
Iteration 66/1000 | Loss: 0.00002656
Iteration 67/1000 | Loss: 0.00002656
Iteration 68/1000 | Loss: 0.00002656
Iteration 69/1000 | Loss: 0.00002656
Iteration 70/1000 | Loss: 0.00002656
Iteration 71/1000 | Loss: 0.00002656
Iteration 72/1000 | Loss: 0.00002656
Iteration 73/1000 | Loss: 0.00002656
Iteration 74/1000 | Loss: 0.00002656
Iteration 75/1000 | Loss: 0.00002656
Iteration 76/1000 | Loss: 0.00002656
Iteration 77/1000 | Loss: 0.00002656
Iteration 78/1000 | Loss: 0.00002656
Iteration 79/1000 | Loss: 0.00002656
Iteration 80/1000 | Loss: 0.00002656
Iteration 81/1000 | Loss: 0.00002656
Iteration 82/1000 | Loss: 0.00002656
Iteration 83/1000 | Loss: 0.00002656
Iteration 84/1000 | Loss: 0.00002656
Iteration 85/1000 | Loss: 0.00002656
Iteration 86/1000 | Loss: 0.00002656
Iteration 87/1000 | Loss: 0.00002656
Iteration 88/1000 | Loss: 0.00002656
Iteration 89/1000 | Loss: 0.00002656
Iteration 90/1000 | Loss: 0.00002656
Iteration 91/1000 | Loss: 0.00002656
Iteration 92/1000 | Loss: 0.00002656
Iteration 93/1000 | Loss: 0.00002656
Iteration 94/1000 | Loss: 0.00002656
Iteration 95/1000 | Loss: 0.00002656
Iteration 96/1000 | Loss: 0.00002656
Iteration 97/1000 | Loss: 0.00002656
Iteration 98/1000 | Loss: 0.00002656
Iteration 99/1000 | Loss: 0.00002656
Iteration 100/1000 | Loss: 0.00002656
Iteration 101/1000 | Loss: 0.00002656
Iteration 102/1000 | Loss: 0.00002656
Iteration 103/1000 | Loss: 0.00002656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.6557547244010493e-05, 2.6557547244010493e-05, 2.6557547244010493e-05, 2.6557547244010493e-05, 2.6557547244010493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6557547244010493e-05

Optimization complete. Final v2v error: 4.211511611938477 mm

Highest mean error: 4.560668468475342 mm for frame 77

Lowest mean error: 3.5029256343841553 mm for frame 12

Saving results

Total time: 37.774513244628906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01070044
Iteration 2/25 | Loss: 0.00277433
Iteration 3/25 | Loss: 0.00171791
Iteration 4/25 | Loss: 0.00151844
Iteration 5/25 | Loss: 0.00135680
Iteration 6/25 | Loss: 0.00119842
Iteration 7/25 | Loss: 0.00114401
Iteration 8/25 | Loss: 0.00109094
Iteration 9/25 | Loss: 0.00104048
Iteration 10/25 | Loss: 0.00100656
Iteration 11/25 | Loss: 0.00100846
Iteration 12/25 | Loss: 0.00099871
Iteration 13/25 | Loss: 0.00099781
Iteration 14/25 | Loss: 0.00097994
Iteration 15/25 | Loss: 0.00096894
Iteration 16/25 | Loss: 0.00096267
Iteration 17/25 | Loss: 0.00096598
Iteration 18/25 | Loss: 0.00096193
Iteration 19/25 | Loss: 0.00095595
Iteration 20/25 | Loss: 0.00095315
Iteration 21/25 | Loss: 0.00095955
Iteration 22/25 | Loss: 0.00095939
Iteration 23/25 | Loss: 0.00095001
Iteration 24/25 | Loss: 0.00094737
Iteration 25/25 | Loss: 0.00094560

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.89663076
Iteration 2/25 | Loss: 0.00235074
Iteration 3/25 | Loss: 0.00235073
Iteration 4/25 | Loss: 0.00235073
Iteration 5/25 | Loss: 0.00235073
Iteration 6/25 | Loss: 0.00235073
Iteration 7/25 | Loss: 0.00235073
Iteration 8/25 | Loss: 0.00235073
Iteration 9/25 | Loss: 0.00235073
Iteration 10/25 | Loss: 0.00235073
Iteration 11/25 | Loss: 0.00235073
Iteration 12/25 | Loss: 0.00235073
Iteration 13/25 | Loss: 0.00235073
Iteration 14/25 | Loss: 0.00235073
Iteration 15/25 | Loss: 0.00235073
Iteration 16/25 | Loss: 0.00235073
Iteration 17/25 | Loss: 0.00235073
Iteration 18/25 | Loss: 0.00235073
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0023507310543209314, 0.0023507310543209314, 0.0023507310543209314, 0.0023507310543209314, 0.0023507310543209314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023507310543209314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00235073
Iteration 2/1000 | Loss: 0.00066942
Iteration 3/1000 | Loss: 0.00045975
Iteration 4/1000 | Loss: 0.00092933
Iteration 5/1000 | Loss: 0.00070100
Iteration 6/1000 | Loss: 0.00027908
Iteration 7/1000 | Loss: 0.00019754
Iteration 8/1000 | Loss: 0.00073251
Iteration 9/1000 | Loss: 0.00035968
Iteration 10/1000 | Loss: 0.00016006
Iteration 11/1000 | Loss: 0.00014608
Iteration 12/1000 | Loss: 0.00022177
Iteration 13/1000 | Loss: 0.00014107
Iteration 14/1000 | Loss: 0.00013326
Iteration 15/1000 | Loss: 0.00012920
Iteration 16/1000 | Loss: 0.00012403
Iteration 17/1000 | Loss: 0.00073490
Iteration 18/1000 | Loss: 0.00118164
Iteration 19/1000 | Loss: 0.00063634
Iteration 20/1000 | Loss: 0.00024986
Iteration 21/1000 | Loss: 0.00012006
Iteration 22/1000 | Loss: 0.00011545
Iteration 23/1000 | Loss: 0.00092416
Iteration 24/1000 | Loss: 0.00018318
Iteration 25/1000 | Loss: 0.00014273
Iteration 26/1000 | Loss: 0.00022209
Iteration 27/1000 | Loss: 0.00026085
Iteration 28/1000 | Loss: 0.00016969
Iteration 29/1000 | Loss: 0.00012404
Iteration 30/1000 | Loss: 0.00010937
Iteration 31/1000 | Loss: 0.00010288
Iteration 32/1000 | Loss: 0.00009811
Iteration 33/1000 | Loss: 0.00009389
Iteration 34/1000 | Loss: 0.00009103
Iteration 35/1000 | Loss: 0.00008938
Iteration 36/1000 | Loss: 0.00008835
Iteration 37/1000 | Loss: 0.00008742
Iteration 38/1000 | Loss: 0.00008668
Iteration 39/1000 | Loss: 0.00008605
Iteration 40/1000 | Loss: 0.00008546
Iteration 41/1000 | Loss: 0.00008514
Iteration 42/1000 | Loss: 0.00008467
Iteration 43/1000 | Loss: 0.00008433
Iteration 44/1000 | Loss: 0.00095254
Iteration 45/1000 | Loss: 0.00254283
Iteration 46/1000 | Loss: 0.00251274
Iteration 47/1000 | Loss: 0.00057185
Iteration 48/1000 | Loss: 0.00079947
Iteration 49/1000 | Loss: 0.00042791
Iteration 50/1000 | Loss: 0.00069169
Iteration 51/1000 | Loss: 0.00089224
Iteration 52/1000 | Loss: 0.00017931
Iteration 53/1000 | Loss: 0.00082458
Iteration 54/1000 | Loss: 0.00055798
Iteration 55/1000 | Loss: 0.00049650
Iteration 56/1000 | Loss: 0.00060759
Iteration 57/1000 | Loss: 0.00058770
Iteration 58/1000 | Loss: 0.00059040
Iteration 59/1000 | Loss: 0.00009048
Iteration 60/1000 | Loss: 0.00061569
Iteration 61/1000 | Loss: 0.00048922
Iteration 62/1000 | Loss: 0.00052916
Iteration 63/1000 | Loss: 0.00011510
Iteration 64/1000 | Loss: 0.00009995
Iteration 65/1000 | Loss: 0.00020734
Iteration 66/1000 | Loss: 0.00144087
Iteration 67/1000 | Loss: 0.00209090
Iteration 68/1000 | Loss: 0.00407461
Iteration 69/1000 | Loss: 0.00166514
Iteration 70/1000 | Loss: 0.00026063
Iteration 71/1000 | Loss: 0.00011808
Iteration 72/1000 | Loss: 0.00073841
Iteration 73/1000 | Loss: 0.00067489
Iteration 74/1000 | Loss: 0.00090947
Iteration 75/1000 | Loss: 0.00155040
Iteration 76/1000 | Loss: 0.00078795
Iteration 77/1000 | Loss: 0.00052043
Iteration 78/1000 | Loss: 0.00050068
Iteration 79/1000 | Loss: 0.00041433
Iteration 80/1000 | Loss: 0.00013252
Iteration 81/1000 | Loss: 0.00037042
Iteration 82/1000 | Loss: 0.00059247
Iteration 83/1000 | Loss: 0.00037734
Iteration 84/1000 | Loss: 0.00048151
Iteration 85/1000 | Loss: 0.00033880
Iteration 86/1000 | Loss: 0.00043116
Iteration 87/1000 | Loss: 0.00033485
Iteration 88/1000 | Loss: 0.00134979
Iteration 89/1000 | Loss: 0.00024402
Iteration 90/1000 | Loss: 0.00067415
Iteration 91/1000 | Loss: 0.00068895
Iteration 92/1000 | Loss: 0.00062036
Iteration 93/1000 | Loss: 0.00098295
Iteration 94/1000 | Loss: 0.00010861
Iteration 95/1000 | Loss: 0.00035525
Iteration 96/1000 | Loss: 0.00032497
Iteration 97/1000 | Loss: 0.00029139
Iteration 98/1000 | Loss: 0.00061987
Iteration 99/1000 | Loss: 0.00044919
Iteration 100/1000 | Loss: 0.00035828
Iteration 101/1000 | Loss: 0.00092072
Iteration 102/1000 | Loss: 0.00056230
Iteration 103/1000 | Loss: 0.00033014
Iteration 104/1000 | Loss: 0.00014129
Iteration 105/1000 | Loss: 0.00008226
Iteration 106/1000 | Loss: 0.00039199
Iteration 107/1000 | Loss: 0.00075437
Iteration 108/1000 | Loss: 0.00035340
Iteration 109/1000 | Loss: 0.00011096
Iteration 110/1000 | Loss: 0.00008686
Iteration 111/1000 | Loss: 0.00008002
Iteration 112/1000 | Loss: 0.00007787
Iteration 113/1000 | Loss: 0.00007626
Iteration 114/1000 | Loss: 0.00007546
Iteration 115/1000 | Loss: 0.00007513
Iteration 116/1000 | Loss: 0.00007495
Iteration 117/1000 | Loss: 0.00007483
Iteration 118/1000 | Loss: 0.00007482
Iteration 119/1000 | Loss: 0.00007481
Iteration 120/1000 | Loss: 0.00007480
Iteration 121/1000 | Loss: 0.00007460
Iteration 122/1000 | Loss: 0.00007432
Iteration 123/1000 | Loss: 0.00007406
Iteration 124/1000 | Loss: 0.00007385
Iteration 125/1000 | Loss: 0.00007381
Iteration 126/1000 | Loss: 0.00007380
Iteration 127/1000 | Loss: 0.00007379
Iteration 128/1000 | Loss: 0.00007379
Iteration 129/1000 | Loss: 0.00007378
Iteration 130/1000 | Loss: 0.00007378
Iteration 131/1000 | Loss: 0.00007377
Iteration 132/1000 | Loss: 0.00007377
Iteration 133/1000 | Loss: 0.00007376
Iteration 134/1000 | Loss: 0.00007376
Iteration 135/1000 | Loss: 0.00007375
Iteration 136/1000 | Loss: 0.00007375
Iteration 137/1000 | Loss: 0.00007374
Iteration 138/1000 | Loss: 0.00007373
Iteration 139/1000 | Loss: 0.00007368
Iteration 140/1000 | Loss: 0.00007364
Iteration 141/1000 | Loss: 0.00007363
Iteration 142/1000 | Loss: 0.00007362
Iteration 143/1000 | Loss: 0.00007362
Iteration 144/1000 | Loss: 0.00007361
Iteration 145/1000 | Loss: 0.00007352
Iteration 146/1000 | Loss: 0.00007349
Iteration 147/1000 | Loss: 0.00007348
Iteration 148/1000 | Loss: 0.00007348
Iteration 149/1000 | Loss: 0.00007348
Iteration 150/1000 | Loss: 0.00007348
Iteration 151/1000 | Loss: 0.00007347
Iteration 152/1000 | Loss: 0.00007347
Iteration 153/1000 | Loss: 0.00007347
Iteration 154/1000 | Loss: 0.00007347
Iteration 155/1000 | Loss: 0.00007346
Iteration 156/1000 | Loss: 0.00007346
Iteration 157/1000 | Loss: 0.00007345
Iteration 158/1000 | Loss: 0.00007345
Iteration 159/1000 | Loss: 0.00007345
Iteration 160/1000 | Loss: 0.00007345
Iteration 161/1000 | Loss: 0.00007344
Iteration 162/1000 | Loss: 0.00007344
Iteration 163/1000 | Loss: 0.00007344
Iteration 164/1000 | Loss: 0.00007344
Iteration 165/1000 | Loss: 0.00007344
Iteration 166/1000 | Loss: 0.00007344
Iteration 167/1000 | Loss: 0.00007343
Iteration 168/1000 | Loss: 0.00007343
Iteration 169/1000 | Loss: 0.00007343
Iteration 170/1000 | Loss: 0.00007343
Iteration 171/1000 | Loss: 0.00007342
Iteration 172/1000 | Loss: 0.00007342
Iteration 173/1000 | Loss: 0.00007342
Iteration 174/1000 | Loss: 0.00007342
Iteration 175/1000 | Loss: 0.00007342
Iteration 176/1000 | Loss: 0.00007342
Iteration 177/1000 | Loss: 0.00007342
Iteration 178/1000 | Loss: 0.00007341
Iteration 179/1000 | Loss: 0.00007341
Iteration 180/1000 | Loss: 0.00007341
Iteration 181/1000 | Loss: 0.00007341
Iteration 182/1000 | Loss: 0.00007341
Iteration 183/1000 | Loss: 0.00007341
Iteration 184/1000 | Loss: 0.00007340
Iteration 185/1000 | Loss: 0.00007340
Iteration 186/1000 | Loss: 0.00007340
Iteration 187/1000 | Loss: 0.00007340
Iteration 188/1000 | Loss: 0.00007340
Iteration 189/1000 | Loss: 0.00007339
Iteration 190/1000 | Loss: 0.00007339
Iteration 191/1000 | Loss: 0.00007339
Iteration 192/1000 | Loss: 0.00007338
Iteration 193/1000 | Loss: 0.00007338
Iteration 194/1000 | Loss: 0.00007338
Iteration 195/1000 | Loss: 0.00007338
Iteration 196/1000 | Loss: 0.00007337
Iteration 197/1000 | Loss: 0.00007337
Iteration 198/1000 | Loss: 0.00007337
Iteration 199/1000 | Loss: 0.00007337
Iteration 200/1000 | Loss: 0.00007337
Iteration 201/1000 | Loss: 0.00007337
Iteration 202/1000 | Loss: 0.00007337
Iteration 203/1000 | Loss: 0.00007337
Iteration 204/1000 | Loss: 0.00007337
Iteration 205/1000 | Loss: 0.00007337
Iteration 206/1000 | Loss: 0.00007337
Iteration 207/1000 | Loss: 0.00007337
Iteration 208/1000 | Loss: 0.00007336
Iteration 209/1000 | Loss: 0.00007336
Iteration 210/1000 | Loss: 0.00007336
Iteration 211/1000 | Loss: 0.00007336
Iteration 212/1000 | Loss: 0.00007336
Iteration 213/1000 | Loss: 0.00007336
Iteration 214/1000 | Loss: 0.00007336
Iteration 215/1000 | Loss: 0.00007336
Iteration 216/1000 | Loss: 0.00007336
Iteration 217/1000 | Loss: 0.00007336
Iteration 218/1000 | Loss: 0.00007336
Iteration 219/1000 | Loss: 0.00007336
Iteration 220/1000 | Loss: 0.00007336
Iteration 221/1000 | Loss: 0.00007336
Iteration 222/1000 | Loss: 0.00007336
Iteration 223/1000 | Loss: 0.00007336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [7.336230191867799e-05, 7.336230191867799e-05, 7.336230191867799e-05, 7.336230191867799e-05, 7.336230191867799e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.336230191867799e-05

Optimization complete. Final v2v error: 4.744833469390869 mm

Highest mean error: 12.021544456481934 mm for frame 53

Lowest mean error: 3.198427677154541 mm for frame 5

Saving results

Total time: 229.01229071617126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796164
Iteration 2/25 | Loss: 0.00101765
Iteration 3/25 | Loss: 0.00067845
Iteration 4/25 | Loss: 0.00064118
Iteration 5/25 | Loss: 0.00062162
Iteration 6/25 | Loss: 0.00061901
Iteration 7/25 | Loss: 0.00062158
Iteration 8/25 | Loss: 0.00062075
Iteration 9/25 | Loss: 0.00061840
Iteration 10/25 | Loss: 0.00061764
Iteration 11/25 | Loss: 0.00061695
Iteration 12/25 | Loss: 0.00061634
Iteration 13/25 | Loss: 0.00061589
Iteration 14/25 | Loss: 0.00061574
Iteration 15/25 | Loss: 0.00061564
Iteration 16/25 | Loss: 0.00061560
Iteration 17/25 | Loss: 0.00061560
Iteration 18/25 | Loss: 0.00061560
Iteration 19/25 | Loss: 0.00061560
Iteration 20/25 | Loss: 0.00061560
Iteration 21/25 | Loss: 0.00061560
Iteration 22/25 | Loss: 0.00061560
Iteration 23/25 | Loss: 0.00061560
Iteration 24/25 | Loss: 0.00061560
Iteration 25/25 | Loss: 0.00061560

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.52570271
Iteration 2/25 | Loss: 0.00030889
Iteration 3/25 | Loss: 0.00030889
Iteration 4/25 | Loss: 0.00030889
Iteration 5/25 | Loss: 0.00030889
Iteration 6/25 | Loss: 0.00030889
Iteration 7/25 | Loss: 0.00030889
Iteration 8/25 | Loss: 0.00030889
Iteration 9/25 | Loss: 0.00030889
Iteration 10/25 | Loss: 0.00030889
Iteration 11/25 | Loss: 0.00030888
Iteration 12/25 | Loss: 0.00030888
Iteration 13/25 | Loss: 0.00030888
Iteration 14/25 | Loss: 0.00030888
Iteration 15/25 | Loss: 0.00030888
Iteration 16/25 | Loss: 0.00030888
Iteration 17/25 | Loss: 0.00030888
Iteration 18/25 | Loss: 0.00030888
Iteration 19/25 | Loss: 0.00030888
Iteration 20/25 | Loss: 0.00030888
Iteration 21/25 | Loss: 0.00030888
Iteration 22/25 | Loss: 0.00030888
Iteration 23/25 | Loss: 0.00030888
Iteration 24/25 | Loss: 0.00030888
Iteration 25/25 | Loss: 0.00030888

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030888
Iteration 2/1000 | Loss: 0.00002469
Iteration 3/1000 | Loss: 0.00002714
Iteration 4/1000 | Loss: 0.00001620
Iteration 5/1000 | Loss: 0.00001538
Iteration 6/1000 | Loss: 0.00001473
Iteration 7/1000 | Loss: 0.00001460
Iteration 8/1000 | Loss: 0.00001423
Iteration 9/1000 | Loss: 0.00001399
Iteration 10/1000 | Loss: 0.00001382
Iteration 11/1000 | Loss: 0.00001368
Iteration 12/1000 | Loss: 0.00001361
Iteration 13/1000 | Loss: 0.00001360
Iteration 14/1000 | Loss: 0.00001360
Iteration 15/1000 | Loss: 0.00001359
Iteration 16/1000 | Loss: 0.00001358
Iteration 17/1000 | Loss: 0.00001353
Iteration 18/1000 | Loss: 0.00001352
Iteration 19/1000 | Loss: 0.00001351
Iteration 20/1000 | Loss: 0.00001351
Iteration 21/1000 | Loss: 0.00001350
Iteration 22/1000 | Loss: 0.00001348
Iteration 23/1000 | Loss: 0.00001345
Iteration 24/1000 | Loss: 0.00001344
Iteration 25/1000 | Loss: 0.00001344
Iteration 26/1000 | Loss: 0.00001343
Iteration 27/1000 | Loss: 0.00001339
Iteration 28/1000 | Loss: 0.00003123
Iteration 29/1000 | Loss: 0.00001537
Iteration 30/1000 | Loss: 0.00001336
Iteration 31/1000 | Loss: 0.00001336
Iteration 32/1000 | Loss: 0.00001336
Iteration 33/1000 | Loss: 0.00001336
Iteration 34/1000 | Loss: 0.00001336
Iteration 35/1000 | Loss: 0.00001335
Iteration 36/1000 | Loss: 0.00001335
Iteration 37/1000 | Loss: 0.00001335
Iteration 38/1000 | Loss: 0.00001335
Iteration 39/1000 | Loss: 0.00001334
Iteration 40/1000 | Loss: 0.00001334
Iteration 41/1000 | Loss: 0.00001361
Iteration 42/1000 | Loss: 0.00001331
Iteration 43/1000 | Loss: 0.00001331
Iteration 44/1000 | Loss: 0.00001330
Iteration 45/1000 | Loss: 0.00001330
Iteration 46/1000 | Loss: 0.00001330
Iteration 47/1000 | Loss: 0.00001330
Iteration 48/1000 | Loss: 0.00001330
Iteration 49/1000 | Loss: 0.00001330
Iteration 50/1000 | Loss: 0.00001330
Iteration 51/1000 | Loss: 0.00001330
Iteration 52/1000 | Loss: 0.00001330
Iteration 53/1000 | Loss: 0.00001329
Iteration 54/1000 | Loss: 0.00001329
Iteration 55/1000 | Loss: 0.00001329
Iteration 56/1000 | Loss: 0.00001329
Iteration 57/1000 | Loss: 0.00001328
Iteration 58/1000 | Loss: 0.00001328
Iteration 59/1000 | Loss: 0.00001327
Iteration 60/1000 | Loss: 0.00001327
Iteration 61/1000 | Loss: 0.00001327
Iteration 62/1000 | Loss: 0.00001327
Iteration 63/1000 | Loss: 0.00001327
Iteration 64/1000 | Loss: 0.00001326
Iteration 65/1000 | Loss: 0.00001326
Iteration 66/1000 | Loss: 0.00001325
Iteration 67/1000 | Loss: 0.00001322
Iteration 68/1000 | Loss: 0.00001322
Iteration 69/1000 | Loss: 0.00001322
Iteration 70/1000 | Loss: 0.00001321
Iteration 71/1000 | Loss: 0.00001319
Iteration 72/1000 | Loss: 0.00001318
Iteration 73/1000 | Loss: 0.00001318
Iteration 74/1000 | Loss: 0.00001317
Iteration 75/1000 | Loss: 0.00001317
Iteration 76/1000 | Loss: 0.00001317
Iteration 77/1000 | Loss: 0.00001316
Iteration 78/1000 | Loss: 0.00001316
Iteration 79/1000 | Loss: 0.00001315
Iteration 80/1000 | Loss: 0.00001314
Iteration 81/1000 | Loss: 0.00001314
Iteration 82/1000 | Loss: 0.00001314
Iteration 83/1000 | Loss: 0.00001313
Iteration 84/1000 | Loss: 0.00001313
Iteration 85/1000 | Loss: 0.00001313
Iteration 86/1000 | Loss: 0.00001313
Iteration 87/1000 | Loss: 0.00001313
Iteration 88/1000 | Loss: 0.00001313
Iteration 89/1000 | Loss: 0.00001312
Iteration 90/1000 | Loss: 0.00001312
Iteration 91/1000 | Loss: 0.00001312
Iteration 92/1000 | Loss: 0.00001312
Iteration 93/1000 | Loss: 0.00001312
Iteration 94/1000 | Loss: 0.00001312
Iteration 95/1000 | Loss: 0.00001312
Iteration 96/1000 | Loss: 0.00001312
Iteration 97/1000 | Loss: 0.00001312
Iteration 98/1000 | Loss: 0.00001312
Iteration 99/1000 | Loss: 0.00001311
Iteration 100/1000 | Loss: 0.00001311
Iteration 101/1000 | Loss: 0.00001311
Iteration 102/1000 | Loss: 0.00001311
Iteration 103/1000 | Loss: 0.00001311
Iteration 104/1000 | Loss: 0.00001311
Iteration 105/1000 | Loss: 0.00001311
Iteration 106/1000 | Loss: 0.00001311
Iteration 107/1000 | Loss: 0.00001311
Iteration 108/1000 | Loss: 0.00001311
Iteration 109/1000 | Loss: 0.00001311
Iteration 110/1000 | Loss: 0.00001311
Iteration 111/1000 | Loss: 0.00001311
Iteration 112/1000 | Loss: 0.00001311
Iteration 113/1000 | Loss: 0.00001311
Iteration 114/1000 | Loss: 0.00001311
Iteration 115/1000 | Loss: 0.00001311
Iteration 116/1000 | Loss: 0.00001311
Iteration 117/1000 | Loss: 0.00001310
Iteration 118/1000 | Loss: 0.00001310
Iteration 119/1000 | Loss: 0.00001310
Iteration 120/1000 | Loss: 0.00001310
Iteration 121/1000 | Loss: 0.00001310
Iteration 122/1000 | Loss: 0.00001310
Iteration 123/1000 | Loss: 0.00001310
Iteration 124/1000 | Loss: 0.00001309
Iteration 125/1000 | Loss: 0.00001309
Iteration 126/1000 | Loss: 0.00001309
Iteration 127/1000 | Loss: 0.00001309
Iteration 128/1000 | Loss: 0.00001309
Iteration 129/1000 | Loss: 0.00001309
Iteration 130/1000 | Loss: 0.00001309
Iteration 131/1000 | Loss: 0.00001309
Iteration 132/1000 | Loss: 0.00001309
Iteration 133/1000 | Loss: 0.00001309
Iteration 134/1000 | Loss: 0.00001309
Iteration 135/1000 | Loss: 0.00001309
Iteration 136/1000 | Loss: 0.00001309
Iteration 137/1000 | Loss: 0.00001309
Iteration 138/1000 | Loss: 0.00001309
Iteration 139/1000 | Loss: 0.00001308
Iteration 140/1000 | Loss: 0.00001308
Iteration 141/1000 | Loss: 0.00001308
Iteration 142/1000 | Loss: 0.00001308
Iteration 143/1000 | Loss: 0.00001308
Iteration 144/1000 | Loss: 0.00001307
Iteration 145/1000 | Loss: 0.00001307
Iteration 146/1000 | Loss: 0.00001307
Iteration 147/1000 | Loss: 0.00001307
Iteration 148/1000 | Loss: 0.00001307
Iteration 149/1000 | Loss: 0.00001307
Iteration 150/1000 | Loss: 0.00001307
Iteration 151/1000 | Loss: 0.00001307
Iteration 152/1000 | Loss: 0.00001307
Iteration 153/1000 | Loss: 0.00003122
Iteration 154/1000 | Loss: 0.00003121
Iteration 155/1000 | Loss: 0.00001734
Iteration 156/1000 | Loss: 0.00001426
Iteration 157/1000 | Loss: 0.00001306
Iteration 158/1000 | Loss: 0.00001306
Iteration 159/1000 | Loss: 0.00001305
Iteration 160/1000 | Loss: 0.00001305
Iteration 161/1000 | Loss: 0.00001305
Iteration 162/1000 | Loss: 0.00001305
Iteration 163/1000 | Loss: 0.00001305
Iteration 164/1000 | Loss: 0.00001305
Iteration 165/1000 | Loss: 0.00001305
Iteration 166/1000 | Loss: 0.00001305
Iteration 167/1000 | Loss: 0.00001305
Iteration 168/1000 | Loss: 0.00001305
Iteration 169/1000 | Loss: 0.00001305
Iteration 170/1000 | Loss: 0.00001305
Iteration 171/1000 | Loss: 0.00001305
Iteration 172/1000 | Loss: 0.00001305
Iteration 173/1000 | Loss: 0.00001305
Iteration 174/1000 | Loss: 0.00001305
Iteration 175/1000 | Loss: 0.00001305
Iteration 176/1000 | Loss: 0.00001305
Iteration 177/1000 | Loss: 0.00001305
Iteration 178/1000 | Loss: 0.00001305
Iteration 179/1000 | Loss: 0.00001305
Iteration 180/1000 | Loss: 0.00001305
Iteration 181/1000 | Loss: 0.00001305
Iteration 182/1000 | Loss: 0.00001305
Iteration 183/1000 | Loss: 0.00001305
Iteration 184/1000 | Loss: 0.00001305
Iteration 185/1000 | Loss: 0.00001305
Iteration 186/1000 | Loss: 0.00001305
Iteration 187/1000 | Loss: 0.00001305
Iteration 188/1000 | Loss: 0.00001305
Iteration 189/1000 | Loss: 0.00001305
Iteration 190/1000 | Loss: 0.00001305
Iteration 191/1000 | Loss: 0.00001305
Iteration 192/1000 | Loss: 0.00001305
Iteration 193/1000 | Loss: 0.00001305
Iteration 194/1000 | Loss: 0.00001305
Iteration 195/1000 | Loss: 0.00001305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.305048408539733e-05, 1.305048408539733e-05, 1.305048408539733e-05, 1.305048408539733e-05, 1.305048408539733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.305048408539733e-05

Optimization complete. Final v2v error: 3.0785787105560303 mm

Highest mean error: 3.5002968311309814 mm for frame 44

Lowest mean error: 2.7711448669433594 mm for frame 22

Saving results

Total time: 68.72953796386719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428550
Iteration 2/25 | Loss: 0.00103666
Iteration 3/25 | Loss: 0.00071205
Iteration 4/25 | Loss: 0.00066929
Iteration 5/25 | Loss: 0.00065735
Iteration 6/25 | Loss: 0.00065456
Iteration 7/25 | Loss: 0.00065389
Iteration 8/25 | Loss: 0.00065389
Iteration 9/25 | Loss: 0.00065389
Iteration 10/25 | Loss: 0.00065389
Iteration 11/25 | Loss: 0.00065389
Iteration 12/25 | Loss: 0.00065389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006538945599459112, 0.0006538945599459112, 0.0006538945599459112, 0.0006538945599459112, 0.0006538945599459112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006538945599459112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45769680
Iteration 2/25 | Loss: 0.00032487
Iteration 3/25 | Loss: 0.00032487
Iteration 4/25 | Loss: 0.00032487
Iteration 5/25 | Loss: 0.00032487
Iteration 6/25 | Loss: 0.00032487
Iteration 7/25 | Loss: 0.00032487
Iteration 8/25 | Loss: 0.00032487
Iteration 9/25 | Loss: 0.00032487
Iteration 10/25 | Loss: 0.00032487
Iteration 11/25 | Loss: 0.00032487
Iteration 12/25 | Loss: 0.00032487
Iteration 13/25 | Loss: 0.00032487
Iteration 14/25 | Loss: 0.00032487
Iteration 15/25 | Loss: 0.00032487
Iteration 16/25 | Loss: 0.00032487
Iteration 17/25 | Loss: 0.00032487
Iteration 18/25 | Loss: 0.00032487
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00032486708369106054, 0.00032486708369106054, 0.00032486708369106054, 0.00032486708369106054, 0.00032486708369106054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032486708369106054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032487
Iteration 2/1000 | Loss: 0.00002242
Iteration 3/1000 | Loss: 0.00001726
Iteration 4/1000 | Loss: 0.00001527
Iteration 5/1000 | Loss: 0.00001451
Iteration 6/1000 | Loss: 0.00001398
Iteration 7/1000 | Loss: 0.00001363
Iteration 8/1000 | Loss: 0.00001338
Iteration 9/1000 | Loss: 0.00001321
Iteration 10/1000 | Loss: 0.00001316
Iteration 11/1000 | Loss: 0.00001312
Iteration 12/1000 | Loss: 0.00001298
Iteration 13/1000 | Loss: 0.00001291
Iteration 14/1000 | Loss: 0.00001285
Iteration 15/1000 | Loss: 0.00001285
Iteration 16/1000 | Loss: 0.00001280
Iteration 17/1000 | Loss: 0.00001275
Iteration 18/1000 | Loss: 0.00001275
Iteration 19/1000 | Loss: 0.00001274
Iteration 20/1000 | Loss: 0.00001268
Iteration 21/1000 | Loss: 0.00001267
Iteration 22/1000 | Loss: 0.00001267
Iteration 23/1000 | Loss: 0.00001266
Iteration 24/1000 | Loss: 0.00001266
Iteration 25/1000 | Loss: 0.00001265
Iteration 26/1000 | Loss: 0.00001264
Iteration 27/1000 | Loss: 0.00001263
Iteration 28/1000 | Loss: 0.00001263
Iteration 29/1000 | Loss: 0.00001262
Iteration 30/1000 | Loss: 0.00001262
Iteration 31/1000 | Loss: 0.00001262
Iteration 32/1000 | Loss: 0.00001261
Iteration 33/1000 | Loss: 0.00001261
Iteration 34/1000 | Loss: 0.00001261
Iteration 35/1000 | Loss: 0.00001261
Iteration 36/1000 | Loss: 0.00001261
Iteration 37/1000 | Loss: 0.00001261
Iteration 38/1000 | Loss: 0.00001261
Iteration 39/1000 | Loss: 0.00001260
Iteration 40/1000 | Loss: 0.00001260
Iteration 41/1000 | Loss: 0.00001259
Iteration 42/1000 | Loss: 0.00001259
Iteration 43/1000 | Loss: 0.00001258
Iteration 44/1000 | Loss: 0.00001258
Iteration 45/1000 | Loss: 0.00001258
Iteration 46/1000 | Loss: 0.00001258
Iteration 47/1000 | Loss: 0.00001257
Iteration 48/1000 | Loss: 0.00001257
Iteration 49/1000 | Loss: 0.00001257
Iteration 50/1000 | Loss: 0.00001257
Iteration 51/1000 | Loss: 0.00001257
Iteration 52/1000 | Loss: 0.00001256
Iteration 53/1000 | Loss: 0.00001255
Iteration 54/1000 | Loss: 0.00001255
Iteration 55/1000 | Loss: 0.00001255
Iteration 56/1000 | Loss: 0.00001255
Iteration 57/1000 | Loss: 0.00001254
Iteration 58/1000 | Loss: 0.00001254
Iteration 59/1000 | Loss: 0.00001254
Iteration 60/1000 | Loss: 0.00001253
Iteration 61/1000 | Loss: 0.00001253
Iteration 62/1000 | Loss: 0.00001253
Iteration 63/1000 | Loss: 0.00001252
Iteration 64/1000 | Loss: 0.00001252
Iteration 65/1000 | Loss: 0.00001252
Iteration 66/1000 | Loss: 0.00001252
Iteration 67/1000 | Loss: 0.00001252
Iteration 68/1000 | Loss: 0.00001252
Iteration 69/1000 | Loss: 0.00001252
Iteration 70/1000 | Loss: 0.00001252
Iteration 71/1000 | Loss: 0.00001251
Iteration 72/1000 | Loss: 0.00001251
Iteration 73/1000 | Loss: 0.00001251
Iteration 74/1000 | Loss: 0.00001251
Iteration 75/1000 | Loss: 0.00001251
Iteration 76/1000 | Loss: 0.00001251
Iteration 77/1000 | Loss: 0.00001251
Iteration 78/1000 | Loss: 0.00001250
Iteration 79/1000 | Loss: 0.00001250
Iteration 80/1000 | Loss: 0.00001250
Iteration 81/1000 | Loss: 0.00001250
Iteration 82/1000 | Loss: 0.00001249
Iteration 83/1000 | Loss: 0.00001249
Iteration 84/1000 | Loss: 0.00001249
Iteration 85/1000 | Loss: 0.00001248
Iteration 86/1000 | Loss: 0.00001248
Iteration 87/1000 | Loss: 0.00001248
Iteration 88/1000 | Loss: 0.00001247
Iteration 89/1000 | Loss: 0.00001247
Iteration 90/1000 | Loss: 0.00001247
Iteration 91/1000 | Loss: 0.00001247
Iteration 92/1000 | Loss: 0.00001246
Iteration 93/1000 | Loss: 0.00001246
Iteration 94/1000 | Loss: 0.00001246
Iteration 95/1000 | Loss: 0.00001246
Iteration 96/1000 | Loss: 0.00001246
Iteration 97/1000 | Loss: 0.00001246
Iteration 98/1000 | Loss: 0.00001246
Iteration 99/1000 | Loss: 0.00001246
Iteration 100/1000 | Loss: 0.00001246
Iteration 101/1000 | Loss: 0.00001245
Iteration 102/1000 | Loss: 0.00001245
Iteration 103/1000 | Loss: 0.00001245
Iteration 104/1000 | Loss: 0.00001245
Iteration 105/1000 | Loss: 0.00001245
Iteration 106/1000 | Loss: 0.00001245
Iteration 107/1000 | Loss: 0.00001245
Iteration 108/1000 | Loss: 0.00001245
Iteration 109/1000 | Loss: 0.00001245
Iteration 110/1000 | Loss: 0.00001244
Iteration 111/1000 | Loss: 0.00001244
Iteration 112/1000 | Loss: 0.00001244
Iteration 113/1000 | Loss: 0.00001244
Iteration 114/1000 | Loss: 0.00001244
Iteration 115/1000 | Loss: 0.00001244
Iteration 116/1000 | Loss: 0.00001244
Iteration 117/1000 | Loss: 0.00001244
Iteration 118/1000 | Loss: 0.00001244
Iteration 119/1000 | Loss: 0.00001244
Iteration 120/1000 | Loss: 0.00001244
Iteration 121/1000 | Loss: 0.00001244
Iteration 122/1000 | Loss: 0.00001244
Iteration 123/1000 | Loss: 0.00001244
Iteration 124/1000 | Loss: 0.00001244
Iteration 125/1000 | Loss: 0.00001244
Iteration 126/1000 | Loss: 0.00001243
Iteration 127/1000 | Loss: 0.00001243
Iteration 128/1000 | Loss: 0.00001243
Iteration 129/1000 | Loss: 0.00001243
Iteration 130/1000 | Loss: 0.00001243
Iteration 131/1000 | Loss: 0.00001243
Iteration 132/1000 | Loss: 0.00001243
Iteration 133/1000 | Loss: 0.00001243
Iteration 134/1000 | Loss: 0.00001243
Iteration 135/1000 | Loss: 0.00001243
Iteration 136/1000 | Loss: 0.00001243
Iteration 137/1000 | Loss: 0.00001243
Iteration 138/1000 | Loss: 0.00001243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.2434699783625547e-05, 1.2434699783625547e-05, 1.2434699783625547e-05, 1.2434699783625547e-05, 1.2434699783625547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2434699783625547e-05

Optimization complete. Final v2v error: 2.8972771167755127 mm

Highest mean error: 4.526625156402588 mm for frame 239

Lowest mean error: 2.612638235092163 mm for frame 68

Saving results

Total time: 41.89789152145386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071075
Iteration 2/25 | Loss: 0.00173712
Iteration 3/25 | Loss: 0.00109124
Iteration 4/25 | Loss: 0.00094460
Iteration 5/25 | Loss: 0.00087021
Iteration 6/25 | Loss: 0.00083267
Iteration 7/25 | Loss: 0.00078765
Iteration 8/25 | Loss: 0.00078749
Iteration 9/25 | Loss: 0.00076681
Iteration 10/25 | Loss: 0.00074526
Iteration 11/25 | Loss: 0.00074045
Iteration 12/25 | Loss: 0.00073418
Iteration 13/25 | Loss: 0.00072591
Iteration 14/25 | Loss: 0.00072210
Iteration 15/25 | Loss: 0.00072063
Iteration 16/25 | Loss: 0.00072013
Iteration 17/25 | Loss: 0.00071999
Iteration 18/25 | Loss: 0.00071998
Iteration 19/25 | Loss: 0.00071998
Iteration 20/25 | Loss: 0.00071998
Iteration 21/25 | Loss: 0.00071997
Iteration 22/25 | Loss: 0.00071997
Iteration 23/25 | Loss: 0.00071997
Iteration 24/25 | Loss: 0.00071997
Iteration 25/25 | Loss: 0.00071997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47584176
Iteration 2/25 | Loss: 0.00037581
Iteration 3/25 | Loss: 0.00037581
Iteration 4/25 | Loss: 0.00037581
Iteration 5/25 | Loss: 0.00037581
Iteration 6/25 | Loss: 0.00037581
Iteration 7/25 | Loss: 0.00037581
Iteration 8/25 | Loss: 0.00037581
Iteration 9/25 | Loss: 0.00037581
Iteration 10/25 | Loss: 0.00037581
Iteration 11/25 | Loss: 0.00037581
Iteration 12/25 | Loss: 0.00037581
Iteration 13/25 | Loss: 0.00037581
Iteration 14/25 | Loss: 0.00037581
Iteration 15/25 | Loss: 0.00037581
Iteration 16/25 | Loss: 0.00037581
Iteration 17/25 | Loss: 0.00037581
Iteration 18/25 | Loss: 0.00037581
Iteration 19/25 | Loss: 0.00037581
Iteration 20/25 | Loss: 0.00037581
Iteration 21/25 | Loss: 0.00037581
Iteration 22/25 | Loss: 0.00037581
Iteration 23/25 | Loss: 0.00037581
Iteration 24/25 | Loss: 0.00037581
Iteration 25/25 | Loss: 0.00037581

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037581
Iteration 2/1000 | Loss: 0.00005268
Iteration 3/1000 | Loss: 0.00003646
Iteration 4/1000 | Loss: 0.00003088
Iteration 5/1000 | Loss: 0.00002813
Iteration 6/1000 | Loss: 0.00002607
Iteration 7/1000 | Loss: 0.00002524
Iteration 8/1000 | Loss: 0.00002441
Iteration 9/1000 | Loss: 0.00002383
Iteration 10/1000 | Loss: 0.00042067
Iteration 11/1000 | Loss: 0.00023824
Iteration 12/1000 | Loss: 0.00046454
Iteration 13/1000 | Loss: 0.00005702
Iteration 14/1000 | Loss: 0.00002352
Iteration 15/1000 | Loss: 0.00002168
Iteration 16/1000 | Loss: 0.00002027
Iteration 17/1000 | Loss: 0.00001964
Iteration 18/1000 | Loss: 0.00001915
Iteration 19/1000 | Loss: 0.00001894
Iteration 20/1000 | Loss: 0.00001878
Iteration 21/1000 | Loss: 0.00001877
Iteration 22/1000 | Loss: 0.00001873
Iteration 23/1000 | Loss: 0.00001867
Iteration 24/1000 | Loss: 0.00001864
Iteration 25/1000 | Loss: 0.00001862
Iteration 26/1000 | Loss: 0.00001861
Iteration 27/1000 | Loss: 0.00001860
Iteration 28/1000 | Loss: 0.00001859
Iteration 29/1000 | Loss: 0.00001859
Iteration 30/1000 | Loss: 0.00001858
Iteration 31/1000 | Loss: 0.00001858
Iteration 32/1000 | Loss: 0.00001856
Iteration 33/1000 | Loss: 0.00001855
Iteration 34/1000 | Loss: 0.00001855
Iteration 35/1000 | Loss: 0.00001854
Iteration 36/1000 | Loss: 0.00001853
Iteration 37/1000 | Loss: 0.00001853
Iteration 38/1000 | Loss: 0.00001851
Iteration 39/1000 | Loss: 0.00001851
Iteration 40/1000 | Loss: 0.00001850
Iteration 41/1000 | Loss: 0.00001849
Iteration 42/1000 | Loss: 0.00001848
Iteration 43/1000 | Loss: 0.00001848
Iteration 44/1000 | Loss: 0.00001847
Iteration 45/1000 | Loss: 0.00001847
Iteration 46/1000 | Loss: 0.00001846
Iteration 47/1000 | Loss: 0.00001846
Iteration 48/1000 | Loss: 0.00001846
Iteration 49/1000 | Loss: 0.00001846
Iteration 50/1000 | Loss: 0.00001846
Iteration 51/1000 | Loss: 0.00001846
Iteration 52/1000 | Loss: 0.00001846
Iteration 53/1000 | Loss: 0.00001846
Iteration 54/1000 | Loss: 0.00001846
Iteration 55/1000 | Loss: 0.00001846
Iteration 56/1000 | Loss: 0.00001845
Iteration 57/1000 | Loss: 0.00001845
Iteration 58/1000 | Loss: 0.00001845
Iteration 59/1000 | Loss: 0.00001845
Iteration 60/1000 | Loss: 0.00001845
Iteration 61/1000 | Loss: 0.00001843
Iteration 62/1000 | Loss: 0.00001843
Iteration 63/1000 | Loss: 0.00001843
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001842
Iteration 66/1000 | Loss: 0.00001842
Iteration 67/1000 | Loss: 0.00001841
Iteration 68/1000 | Loss: 0.00001841
Iteration 69/1000 | Loss: 0.00001841
Iteration 70/1000 | Loss: 0.00001841
Iteration 71/1000 | Loss: 0.00001841
Iteration 72/1000 | Loss: 0.00001841
Iteration 73/1000 | Loss: 0.00001840
Iteration 74/1000 | Loss: 0.00001840
Iteration 75/1000 | Loss: 0.00001840
Iteration 76/1000 | Loss: 0.00001840
Iteration 77/1000 | Loss: 0.00001840
Iteration 78/1000 | Loss: 0.00001839
Iteration 79/1000 | Loss: 0.00001839
Iteration 80/1000 | Loss: 0.00001839
Iteration 81/1000 | Loss: 0.00001839
Iteration 82/1000 | Loss: 0.00001839
Iteration 83/1000 | Loss: 0.00001839
Iteration 84/1000 | Loss: 0.00001838
Iteration 85/1000 | Loss: 0.00001838
Iteration 86/1000 | Loss: 0.00001838
Iteration 87/1000 | Loss: 0.00001838
Iteration 88/1000 | Loss: 0.00001838
Iteration 89/1000 | Loss: 0.00001837
Iteration 90/1000 | Loss: 0.00001837
Iteration 91/1000 | Loss: 0.00001837
Iteration 92/1000 | Loss: 0.00001837
Iteration 93/1000 | Loss: 0.00001837
Iteration 94/1000 | Loss: 0.00001837
Iteration 95/1000 | Loss: 0.00001836
Iteration 96/1000 | Loss: 0.00001836
Iteration 97/1000 | Loss: 0.00001836
Iteration 98/1000 | Loss: 0.00001836
Iteration 99/1000 | Loss: 0.00001835
Iteration 100/1000 | Loss: 0.00001835
Iteration 101/1000 | Loss: 0.00001835
Iteration 102/1000 | Loss: 0.00001835
Iteration 103/1000 | Loss: 0.00001834
Iteration 104/1000 | Loss: 0.00001834
Iteration 105/1000 | Loss: 0.00001834
Iteration 106/1000 | Loss: 0.00001834
Iteration 107/1000 | Loss: 0.00001834
Iteration 108/1000 | Loss: 0.00001834
Iteration 109/1000 | Loss: 0.00001834
Iteration 110/1000 | Loss: 0.00001834
Iteration 111/1000 | Loss: 0.00001834
Iteration 112/1000 | Loss: 0.00001834
Iteration 113/1000 | Loss: 0.00001833
Iteration 114/1000 | Loss: 0.00001833
Iteration 115/1000 | Loss: 0.00001833
Iteration 116/1000 | Loss: 0.00001833
Iteration 117/1000 | Loss: 0.00001833
Iteration 118/1000 | Loss: 0.00001833
Iteration 119/1000 | Loss: 0.00001833
Iteration 120/1000 | Loss: 0.00001833
Iteration 121/1000 | Loss: 0.00001833
Iteration 122/1000 | Loss: 0.00001833
Iteration 123/1000 | Loss: 0.00001832
Iteration 124/1000 | Loss: 0.00001832
Iteration 125/1000 | Loss: 0.00001832
Iteration 126/1000 | Loss: 0.00001832
Iteration 127/1000 | Loss: 0.00001832
Iteration 128/1000 | Loss: 0.00001832
Iteration 129/1000 | Loss: 0.00001832
Iteration 130/1000 | Loss: 0.00001832
Iteration 131/1000 | Loss: 0.00001831
Iteration 132/1000 | Loss: 0.00001831
Iteration 133/1000 | Loss: 0.00001831
Iteration 134/1000 | Loss: 0.00001831
Iteration 135/1000 | Loss: 0.00001831
Iteration 136/1000 | Loss: 0.00001831
Iteration 137/1000 | Loss: 0.00001831
Iteration 138/1000 | Loss: 0.00001831
Iteration 139/1000 | Loss: 0.00001831
Iteration 140/1000 | Loss: 0.00001831
Iteration 141/1000 | Loss: 0.00001831
Iteration 142/1000 | Loss: 0.00001831
Iteration 143/1000 | Loss: 0.00001831
Iteration 144/1000 | Loss: 0.00001831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.8305901903659105e-05, 1.8305901903659105e-05, 1.8305901903659105e-05, 1.8305901903659105e-05, 1.8305901903659105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8305901903659105e-05

Optimization complete. Final v2v error: 3.590172052383423 mm

Highest mean error: 4.0964460372924805 mm for frame 184

Lowest mean error: 3.257615804672241 mm for frame 37

Saving results

Total time: 71.66578507423401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00362942
Iteration 2/25 | Loss: 0.00069434
Iteration 3/25 | Loss: 0.00058511
Iteration 4/25 | Loss: 0.00057052
Iteration 5/25 | Loss: 0.00056552
Iteration 6/25 | Loss: 0.00056401
Iteration 7/25 | Loss: 0.00056369
Iteration 8/25 | Loss: 0.00056369
Iteration 9/25 | Loss: 0.00056369
Iteration 10/25 | Loss: 0.00056369
Iteration 11/25 | Loss: 0.00056369
Iteration 12/25 | Loss: 0.00056369
Iteration 13/25 | Loss: 0.00056369
Iteration 14/25 | Loss: 0.00056369
Iteration 15/25 | Loss: 0.00056369
Iteration 16/25 | Loss: 0.00056369
Iteration 17/25 | Loss: 0.00056369
Iteration 18/25 | Loss: 0.00056369
Iteration 19/25 | Loss: 0.00056369
Iteration 20/25 | Loss: 0.00056369
Iteration 21/25 | Loss: 0.00056369
Iteration 22/25 | Loss: 0.00056369
Iteration 23/25 | Loss: 0.00056369
Iteration 24/25 | Loss: 0.00056369
Iteration 25/25 | Loss: 0.00056369

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46253288
Iteration 2/25 | Loss: 0.00024805
Iteration 3/25 | Loss: 0.00024805
Iteration 4/25 | Loss: 0.00024805
Iteration 5/25 | Loss: 0.00024805
Iteration 6/25 | Loss: 0.00024805
Iteration 7/25 | Loss: 0.00024805
Iteration 8/25 | Loss: 0.00024805
Iteration 9/25 | Loss: 0.00024805
Iteration 10/25 | Loss: 0.00024804
Iteration 11/25 | Loss: 0.00024804
Iteration 12/25 | Loss: 0.00024804
Iteration 13/25 | Loss: 0.00024804
Iteration 14/25 | Loss: 0.00024804
Iteration 15/25 | Loss: 0.00024804
Iteration 16/25 | Loss: 0.00024804
Iteration 17/25 | Loss: 0.00024804
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00024804461281746626, 0.00024804461281746626, 0.00024804461281746626, 0.00024804461281746626, 0.00024804461281746626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024804461281746626

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024804
Iteration 2/1000 | Loss: 0.00002005
Iteration 3/1000 | Loss: 0.00001287
Iteration 4/1000 | Loss: 0.00001058
Iteration 5/1000 | Loss: 0.00000984
Iteration 6/1000 | Loss: 0.00000941
Iteration 7/1000 | Loss: 0.00000916
Iteration 8/1000 | Loss: 0.00000907
Iteration 9/1000 | Loss: 0.00000906
Iteration 10/1000 | Loss: 0.00000904
Iteration 11/1000 | Loss: 0.00000904
Iteration 12/1000 | Loss: 0.00000901
Iteration 13/1000 | Loss: 0.00000901
Iteration 14/1000 | Loss: 0.00000901
Iteration 15/1000 | Loss: 0.00000899
Iteration 16/1000 | Loss: 0.00000897
Iteration 17/1000 | Loss: 0.00000895
Iteration 18/1000 | Loss: 0.00000895
Iteration 19/1000 | Loss: 0.00000895
Iteration 20/1000 | Loss: 0.00000895
Iteration 21/1000 | Loss: 0.00000895
Iteration 22/1000 | Loss: 0.00000895
Iteration 23/1000 | Loss: 0.00000895
Iteration 24/1000 | Loss: 0.00000895
Iteration 25/1000 | Loss: 0.00000895
Iteration 26/1000 | Loss: 0.00000894
Iteration 27/1000 | Loss: 0.00000894
Iteration 28/1000 | Loss: 0.00000894
Iteration 29/1000 | Loss: 0.00000892
Iteration 30/1000 | Loss: 0.00000891
Iteration 31/1000 | Loss: 0.00000891
Iteration 32/1000 | Loss: 0.00000891
Iteration 33/1000 | Loss: 0.00000891
Iteration 34/1000 | Loss: 0.00000891
Iteration 35/1000 | Loss: 0.00000890
Iteration 36/1000 | Loss: 0.00000890
Iteration 37/1000 | Loss: 0.00000889
Iteration 38/1000 | Loss: 0.00000889
Iteration 39/1000 | Loss: 0.00000888
Iteration 40/1000 | Loss: 0.00000887
Iteration 41/1000 | Loss: 0.00000887
Iteration 42/1000 | Loss: 0.00000887
Iteration 43/1000 | Loss: 0.00000887
Iteration 44/1000 | Loss: 0.00000886
Iteration 45/1000 | Loss: 0.00000886
Iteration 46/1000 | Loss: 0.00000886
Iteration 47/1000 | Loss: 0.00000885
Iteration 48/1000 | Loss: 0.00000885
Iteration 49/1000 | Loss: 0.00000884
Iteration 50/1000 | Loss: 0.00000883
Iteration 51/1000 | Loss: 0.00000881
Iteration 52/1000 | Loss: 0.00000881
Iteration 53/1000 | Loss: 0.00000881
Iteration 54/1000 | Loss: 0.00000881
Iteration 55/1000 | Loss: 0.00000881
Iteration 56/1000 | Loss: 0.00000881
Iteration 57/1000 | Loss: 0.00000881
Iteration 58/1000 | Loss: 0.00000881
Iteration 59/1000 | Loss: 0.00000881
Iteration 60/1000 | Loss: 0.00000880
Iteration 61/1000 | Loss: 0.00000880
Iteration 62/1000 | Loss: 0.00000880
Iteration 63/1000 | Loss: 0.00000880
Iteration 64/1000 | Loss: 0.00000880
Iteration 65/1000 | Loss: 0.00000880
Iteration 66/1000 | Loss: 0.00000880
Iteration 67/1000 | Loss: 0.00000880
Iteration 68/1000 | Loss: 0.00000880
Iteration 69/1000 | Loss: 0.00000879
Iteration 70/1000 | Loss: 0.00000879
Iteration 71/1000 | Loss: 0.00000879
Iteration 72/1000 | Loss: 0.00000879
Iteration 73/1000 | Loss: 0.00000879
Iteration 74/1000 | Loss: 0.00000879
Iteration 75/1000 | Loss: 0.00000878
Iteration 76/1000 | Loss: 0.00000878
Iteration 77/1000 | Loss: 0.00000878
Iteration 78/1000 | Loss: 0.00000878
Iteration 79/1000 | Loss: 0.00000877
Iteration 80/1000 | Loss: 0.00000877
Iteration 81/1000 | Loss: 0.00000876
Iteration 82/1000 | Loss: 0.00000876
Iteration 83/1000 | Loss: 0.00000876
Iteration 84/1000 | Loss: 0.00000875
Iteration 85/1000 | Loss: 0.00000875
Iteration 86/1000 | Loss: 0.00000874
Iteration 87/1000 | Loss: 0.00000874
Iteration 88/1000 | Loss: 0.00000874
Iteration 89/1000 | Loss: 0.00000874
Iteration 90/1000 | Loss: 0.00000874
Iteration 91/1000 | Loss: 0.00000873
Iteration 92/1000 | Loss: 0.00000873
Iteration 93/1000 | Loss: 0.00000873
Iteration 94/1000 | Loss: 0.00000873
Iteration 95/1000 | Loss: 0.00000873
Iteration 96/1000 | Loss: 0.00000873
Iteration 97/1000 | Loss: 0.00000872
Iteration 98/1000 | Loss: 0.00000871
Iteration 99/1000 | Loss: 0.00000871
Iteration 100/1000 | Loss: 0.00000870
Iteration 101/1000 | Loss: 0.00000870
Iteration 102/1000 | Loss: 0.00000870
Iteration 103/1000 | Loss: 0.00000869
Iteration 104/1000 | Loss: 0.00000869
Iteration 105/1000 | Loss: 0.00000869
Iteration 106/1000 | Loss: 0.00000868
Iteration 107/1000 | Loss: 0.00000868
Iteration 108/1000 | Loss: 0.00000867
Iteration 109/1000 | Loss: 0.00000867
Iteration 110/1000 | Loss: 0.00000867
Iteration 111/1000 | Loss: 0.00000867
Iteration 112/1000 | Loss: 0.00000866
Iteration 113/1000 | Loss: 0.00000866
Iteration 114/1000 | Loss: 0.00000866
Iteration 115/1000 | Loss: 0.00000866
Iteration 116/1000 | Loss: 0.00000866
Iteration 117/1000 | Loss: 0.00000866
Iteration 118/1000 | Loss: 0.00000866
Iteration 119/1000 | Loss: 0.00000866
Iteration 120/1000 | Loss: 0.00000865
Iteration 121/1000 | Loss: 0.00000865
Iteration 122/1000 | Loss: 0.00000865
Iteration 123/1000 | Loss: 0.00000865
Iteration 124/1000 | Loss: 0.00000865
Iteration 125/1000 | Loss: 0.00000865
Iteration 126/1000 | Loss: 0.00000865
Iteration 127/1000 | Loss: 0.00000865
Iteration 128/1000 | Loss: 0.00000865
Iteration 129/1000 | Loss: 0.00000865
Iteration 130/1000 | Loss: 0.00000864
Iteration 131/1000 | Loss: 0.00000864
Iteration 132/1000 | Loss: 0.00000864
Iteration 133/1000 | Loss: 0.00000864
Iteration 134/1000 | Loss: 0.00000864
Iteration 135/1000 | Loss: 0.00000864
Iteration 136/1000 | Loss: 0.00000864
Iteration 137/1000 | Loss: 0.00000864
Iteration 138/1000 | Loss: 0.00000864
Iteration 139/1000 | Loss: 0.00000864
Iteration 140/1000 | Loss: 0.00000864
Iteration 141/1000 | Loss: 0.00000864
Iteration 142/1000 | Loss: 0.00000864
Iteration 143/1000 | Loss: 0.00000864
Iteration 144/1000 | Loss: 0.00000863
Iteration 145/1000 | Loss: 0.00000863
Iteration 146/1000 | Loss: 0.00000863
Iteration 147/1000 | Loss: 0.00000863
Iteration 148/1000 | Loss: 0.00000863
Iteration 149/1000 | Loss: 0.00000863
Iteration 150/1000 | Loss: 0.00000863
Iteration 151/1000 | Loss: 0.00000863
Iteration 152/1000 | Loss: 0.00000862
Iteration 153/1000 | Loss: 0.00000862
Iteration 154/1000 | Loss: 0.00000862
Iteration 155/1000 | Loss: 0.00000862
Iteration 156/1000 | Loss: 0.00000862
Iteration 157/1000 | Loss: 0.00000862
Iteration 158/1000 | Loss: 0.00000862
Iteration 159/1000 | Loss: 0.00000862
Iteration 160/1000 | Loss: 0.00000862
Iteration 161/1000 | Loss: 0.00000862
Iteration 162/1000 | Loss: 0.00000862
Iteration 163/1000 | Loss: 0.00000862
Iteration 164/1000 | Loss: 0.00000862
Iteration 165/1000 | Loss: 0.00000862
Iteration 166/1000 | Loss: 0.00000862
Iteration 167/1000 | Loss: 0.00000862
Iteration 168/1000 | Loss: 0.00000862
Iteration 169/1000 | Loss: 0.00000862
Iteration 170/1000 | Loss: 0.00000861
Iteration 171/1000 | Loss: 0.00000861
Iteration 172/1000 | Loss: 0.00000861
Iteration 173/1000 | Loss: 0.00000861
Iteration 174/1000 | Loss: 0.00000861
Iteration 175/1000 | Loss: 0.00000860
Iteration 176/1000 | Loss: 0.00000860
Iteration 177/1000 | Loss: 0.00000860
Iteration 178/1000 | Loss: 0.00000859
Iteration 179/1000 | Loss: 0.00000859
Iteration 180/1000 | Loss: 0.00000859
Iteration 181/1000 | Loss: 0.00000859
Iteration 182/1000 | Loss: 0.00000859
Iteration 183/1000 | Loss: 0.00000859
Iteration 184/1000 | Loss: 0.00000859
Iteration 185/1000 | Loss: 0.00000859
Iteration 186/1000 | Loss: 0.00000859
Iteration 187/1000 | Loss: 0.00000859
Iteration 188/1000 | Loss: 0.00000859
Iteration 189/1000 | Loss: 0.00000859
Iteration 190/1000 | Loss: 0.00000858
Iteration 191/1000 | Loss: 0.00000858
Iteration 192/1000 | Loss: 0.00000858
Iteration 193/1000 | Loss: 0.00000858
Iteration 194/1000 | Loss: 0.00000858
Iteration 195/1000 | Loss: 0.00000858
Iteration 196/1000 | Loss: 0.00000858
Iteration 197/1000 | Loss: 0.00000858
Iteration 198/1000 | Loss: 0.00000858
Iteration 199/1000 | Loss: 0.00000858
Iteration 200/1000 | Loss: 0.00000858
Iteration 201/1000 | Loss: 0.00000858
Iteration 202/1000 | Loss: 0.00000858
Iteration 203/1000 | Loss: 0.00000858
Iteration 204/1000 | Loss: 0.00000857
Iteration 205/1000 | Loss: 0.00000857
Iteration 206/1000 | Loss: 0.00000857
Iteration 207/1000 | Loss: 0.00000857
Iteration 208/1000 | Loss: 0.00000857
Iteration 209/1000 | Loss: 0.00000857
Iteration 210/1000 | Loss: 0.00000857
Iteration 211/1000 | Loss: 0.00000857
Iteration 212/1000 | Loss: 0.00000857
Iteration 213/1000 | Loss: 0.00000857
Iteration 214/1000 | Loss: 0.00000857
Iteration 215/1000 | Loss: 0.00000856
Iteration 216/1000 | Loss: 0.00000856
Iteration 217/1000 | Loss: 0.00000856
Iteration 218/1000 | Loss: 0.00000856
Iteration 219/1000 | Loss: 0.00000856
Iteration 220/1000 | Loss: 0.00000856
Iteration 221/1000 | Loss: 0.00000856
Iteration 222/1000 | Loss: 0.00000856
Iteration 223/1000 | Loss: 0.00000856
Iteration 224/1000 | Loss: 0.00000856
Iteration 225/1000 | Loss: 0.00000856
Iteration 226/1000 | Loss: 0.00000856
Iteration 227/1000 | Loss: 0.00000855
Iteration 228/1000 | Loss: 0.00000855
Iteration 229/1000 | Loss: 0.00000855
Iteration 230/1000 | Loss: 0.00000855
Iteration 231/1000 | Loss: 0.00000855
Iteration 232/1000 | Loss: 0.00000855
Iteration 233/1000 | Loss: 0.00000855
Iteration 234/1000 | Loss: 0.00000855
Iteration 235/1000 | Loss: 0.00000855
Iteration 236/1000 | Loss: 0.00000855
Iteration 237/1000 | Loss: 0.00000855
Iteration 238/1000 | Loss: 0.00000855
Iteration 239/1000 | Loss: 0.00000855
Iteration 240/1000 | Loss: 0.00000855
Iteration 241/1000 | Loss: 0.00000855
Iteration 242/1000 | Loss: 0.00000855
Iteration 243/1000 | Loss: 0.00000855
Iteration 244/1000 | Loss: 0.00000855
Iteration 245/1000 | Loss: 0.00000855
Iteration 246/1000 | Loss: 0.00000855
Iteration 247/1000 | Loss: 0.00000855
Iteration 248/1000 | Loss: 0.00000855
Iteration 249/1000 | Loss: 0.00000855
Iteration 250/1000 | Loss: 0.00000855
Iteration 251/1000 | Loss: 0.00000855
Iteration 252/1000 | Loss: 0.00000855
Iteration 253/1000 | Loss: 0.00000855
Iteration 254/1000 | Loss: 0.00000855
Iteration 255/1000 | Loss: 0.00000855
Iteration 256/1000 | Loss: 0.00000855
Iteration 257/1000 | Loss: 0.00000855
Iteration 258/1000 | Loss: 0.00000855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [8.54623067425564e-06, 8.54623067425564e-06, 8.54623067425564e-06, 8.54623067425564e-06, 8.54623067425564e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.54623067425564e-06

Optimization complete. Final v2v error: 2.502063274383545 mm

Highest mean error: 2.703638792037964 mm for frame 73

Lowest mean error: 2.4353203773498535 mm for frame 112

Saving results

Total time: 39.27001070976257
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066584
Iteration 2/25 | Loss: 0.00234655
Iteration 3/25 | Loss: 0.00105528
Iteration 4/25 | Loss: 0.00088942
Iteration 5/25 | Loss: 0.00080753
Iteration 6/25 | Loss: 0.00079193
Iteration 7/25 | Loss: 0.00083985
Iteration 8/25 | Loss: 0.00073973
Iteration 9/25 | Loss: 0.00069411
Iteration 10/25 | Loss: 0.00065926
Iteration 11/25 | Loss: 0.00072647
Iteration 12/25 | Loss: 0.00061706
Iteration 13/25 | Loss: 0.00060899
Iteration 14/25 | Loss: 0.00060693
Iteration 15/25 | Loss: 0.00060387
Iteration 16/25 | Loss: 0.00060339
Iteration 17/25 | Loss: 0.00060256
Iteration 18/25 | Loss: 0.00060114
Iteration 19/25 | Loss: 0.00060114
Iteration 20/25 | Loss: 0.00060114
Iteration 21/25 | Loss: 0.00060114
Iteration 22/25 | Loss: 0.00060114
Iteration 23/25 | Loss: 0.00060114
Iteration 24/25 | Loss: 0.00060114
Iteration 25/25 | Loss: 0.00060114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42286527
Iteration 2/25 | Loss: 0.00037452
Iteration 3/25 | Loss: 0.00037451
Iteration 4/25 | Loss: 0.00026027
Iteration 5/25 | Loss: 0.00026027
Iteration 6/25 | Loss: 0.00026027
Iteration 7/25 | Loss: 0.00026027
Iteration 8/25 | Loss: 0.00026027
Iteration 9/25 | Loss: 0.00026026
Iteration 10/25 | Loss: 0.00026026
Iteration 11/25 | Loss: 0.00026026
Iteration 12/25 | Loss: 0.00026026
Iteration 13/25 | Loss: 0.00026026
Iteration 14/25 | Loss: 0.00026026
Iteration 15/25 | Loss: 0.00026026
Iteration 16/25 | Loss: 0.00026026
Iteration 17/25 | Loss: 0.00026026
Iteration 18/25 | Loss: 0.00026026
Iteration 19/25 | Loss: 0.00026026
Iteration 20/25 | Loss: 0.00026026
Iteration 21/25 | Loss: 0.00026026
Iteration 22/25 | Loss: 0.00026026
Iteration 23/25 | Loss: 0.00026026
Iteration 24/25 | Loss: 0.00026026
Iteration 25/25 | Loss: 0.00026026
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00026026382693089545, 0.00026026382693089545, 0.00026026382693089545, 0.00026026382693089545, 0.00026026382693089545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026026382693089545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026026
Iteration 2/1000 | Loss: 0.00011863
Iteration 3/1000 | Loss: 0.00065253
Iteration 4/1000 | Loss: 0.00010707
Iteration 5/1000 | Loss: 0.00005972
Iteration 6/1000 | Loss: 0.00002064
Iteration 7/1000 | Loss: 0.00003031
Iteration 8/1000 | Loss: 0.00003007
Iteration 9/1000 | Loss: 0.00015363
Iteration 10/1000 | Loss: 0.00012695
Iteration 11/1000 | Loss: 0.00002804
Iteration 12/1000 | Loss: 0.00001582
Iteration 13/1000 | Loss: 0.00001992
Iteration 14/1000 | Loss: 0.00004568
Iteration 15/1000 | Loss: 0.00002430
Iteration 16/1000 | Loss: 0.00005155
Iteration 17/1000 | Loss: 0.00005227
Iteration 18/1000 | Loss: 0.00001574
Iteration 19/1000 | Loss: 0.00003898
Iteration 20/1000 | Loss: 0.00003915
Iteration 21/1000 | Loss: 0.00047333
Iteration 22/1000 | Loss: 0.00005638
Iteration 23/1000 | Loss: 0.00002768
Iteration 24/1000 | Loss: 0.00002940
Iteration 25/1000 | Loss: 0.00018818
Iteration 26/1000 | Loss: 0.00004460
Iteration 27/1000 | Loss: 0.00004334
Iteration 28/1000 | Loss: 0.00002629
Iteration 29/1000 | Loss: 0.00002261
Iteration 30/1000 | Loss: 0.00001544
Iteration 31/1000 | Loss: 0.00001539
Iteration 32/1000 | Loss: 0.00001538
Iteration 33/1000 | Loss: 0.00001538
Iteration 34/1000 | Loss: 0.00001538
Iteration 35/1000 | Loss: 0.00001538
Iteration 36/1000 | Loss: 0.00001575
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001555
Iteration 39/1000 | Loss: 0.00003438
Iteration 40/1000 | Loss: 0.00002909
Iteration 41/1000 | Loss: 0.00005707
Iteration 42/1000 | Loss: 0.00001546
Iteration 43/1000 | Loss: 0.00003109
Iteration 44/1000 | Loss: 0.00002689
Iteration 45/1000 | Loss: 0.00005147
Iteration 46/1000 | Loss: 0.00002211
Iteration 47/1000 | Loss: 0.00027899
Iteration 48/1000 | Loss: 0.00009609
Iteration 49/1000 | Loss: 0.00003926
Iteration 50/1000 | Loss: 0.00003501
Iteration 51/1000 | Loss: 0.00001546
Iteration 52/1000 | Loss: 0.00003537
Iteration 53/1000 | Loss: 0.00003560
Iteration 54/1000 | Loss: 0.00001531
Iteration 55/1000 | Loss: 0.00001524
Iteration 56/1000 | Loss: 0.00001524
Iteration 57/1000 | Loss: 0.00001930
Iteration 58/1000 | Loss: 0.00001523
Iteration 59/1000 | Loss: 0.00001523
Iteration 60/1000 | Loss: 0.00001523
Iteration 61/1000 | Loss: 0.00001523
Iteration 62/1000 | Loss: 0.00001523
Iteration 63/1000 | Loss: 0.00001522
Iteration 64/1000 | Loss: 0.00001522
Iteration 65/1000 | Loss: 0.00001522
Iteration 66/1000 | Loss: 0.00001522
Iteration 67/1000 | Loss: 0.00001522
Iteration 68/1000 | Loss: 0.00001522
Iteration 69/1000 | Loss: 0.00001522
Iteration 70/1000 | Loss: 0.00001522
Iteration 71/1000 | Loss: 0.00001522
Iteration 72/1000 | Loss: 0.00001522
Iteration 73/1000 | Loss: 0.00001522
Iteration 74/1000 | Loss: 0.00001522
Iteration 75/1000 | Loss: 0.00001522
Iteration 76/1000 | Loss: 0.00001522
Iteration 77/1000 | Loss: 0.00001522
Iteration 78/1000 | Loss: 0.00001522
Iteration 79/1000 | Loss: 0.00001522
Iteration 80/1000 | Loss: 0.00001522
Iteration 81/1000 | Loss: 0.00001522
Iteration 82/1000 | Loss: 0.00001522
Iteration 83/1000 | Loss: 0.00001522
Iteration 84/1000 | Loss: 0.00001522
Iteration 85/1000 | Loss: 0.00001522
Iteration 86/1000 | Loss: 0.00001522
Iteration 87/1000 | Loss: 0.00001522
Iteration 88/1000 | Loss: 0.00001522
Iteration 89/1000 | Loss: 0.00001522
Iteration 90/1000 | Loss: 0.00001522
Iteration 91/1000 | Loss: 0.00001522
Iteration 92/1000 | Loss: 0.00001522
Iteration 93/1000 | Loss: 0.00001522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.5223817172227427e-05, 1.5223817172227427e-05, 1.5223817172227427e-05, 1.5223817172227427e-05, 1.5223817172227427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5223817172227427e-05

Optimization complete. Final v2v error: 3.2563858032226562 mm

Highest mean error: 3.4890542030334473 mm for frame 152

Lowest mean error: 3.0174624919891357 mm for frame 107

Saving results

Total time: 99.41598272323608
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00638234
Iteration 2/25 | Loss: 0.00100094
Iteration 3/25 | Loss: 0.00071593
Iteration 4/25 | Loss: 0.00065748
Iteration 5/25 | Loss: 0.00064786
Iteration 6/25 | Loss: 0.00064512
Iteration 7/25 | Loss: 0.00064486
Iteration 8/25 | Loss: 0.00064486
Iteration 9/25 | Loss: 0.00064486
Iteration 10/25 | Loss: 0.00064486
Iteration 11/25 | Loss: 0.00064486
Iteration 12/25 | Loss: 0.00064486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006448578205890954, 0.0006448578205890954, 0.0006448578205890954, 0.0006448578205890954, 0.0006448578205890954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006448578205890954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45284843
Iteration 2/25 | Loss: 0.00018055
Iteration 3/25 | Loss: 0.00018051
Iteration 4/25 | Loss: 0.00018051
Iteration 5/25 | Loss: 0.00018051
Iteration 6/25 | Loss: 0.00018051
Iteration 7/25 | Loss: 0.00018051
Iteration 8/25 | Loss: 0.00018051
Iteration 9/25 | Loss: 0.00018051
Iteration 10/25 | Loss: 0.00018051
Iteration 11/25 | Loss: 0.00018051
Iteration 12/25 | Loss: 0.00018051
Iteration 13/25 | Loss: 0.00018051
Iteration 14/25 | Loss: 0.00018051
Iteration 15/25 | Loss: 0.00018051
Iteration 16/25 | Loss: 0.00018051
Iteration 17/25 | Loss: 0.00018051
Iteration 18/25 | Loss: 0.00018051
Iteration 19/25 | Loss: 0.00018051
Iteration 20/25 | Loss: 0.00018051
Iteration 21/25 | Loss: 0.00018051
Iteration 22/25 | Loss: 0.00018051
Iteration 23/25 | Loss: 0.00018051
Iteration 24/25 | Loss: 0.00018051
Iteration 25/25 | Loss: 0.00018051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018051
Iteration 2/1000 | Loss: 0.00002310
Iteration 3/1000 | Loss: 0.00001685
Iteration 4/1000 | Loss: 0.00001514
Iteration 5/1000 | Loss: 0.00001415
Iteration 6/1000 | Loss: 0.00001364
Iteration 7/1000 | Loss: 0.00001326
Iteration 8/1000 | Loss: 0.00001304
Iteration 9/1000 | Loss: 0.00001285
Iteration 10/1000 | Loss: 0.00001282
Iteration 11/1000 | Loss: 0.00001281
Iteration 12/1000 | Loss: 0.00001273
Iteration 13/1000 | Loss: 0.00001263
Iteration 14/1000 | Loss: 0.00001260
Iteration 15/1000 | Loss: 0.00001257
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001248
Iteration 18/1000 | Loss: 0.00001246
Iteration 19/1000 | Loss: 0.00001244
Iteration 20/1000 | Loss: 0.00001244
Iteration 21/1000 | Loss: 0.00001241
Iteration 22/1000 | Loss: 0.00001241
Iteration 23/1000 | Loss: 0.00001241
Iteration 24/1000 | Loss: 0.00001241
Iteration 25/1000 | Loss: 0.00001241
Iteration 26/1000 | Loss: 0.00001241
Iteration 27/1000 | Loss: 0.00001241
Iteration 28/1000 | Loss: 0.00001241
Iteration 29/1000 | Loss: 0.00001240
Iteration 30/1000 | Loss: 0.00001240
Iteration 31/1000 | Loss: 0.00001239
Iteration 32/1000 | Loss: 0.00001239
Iteration 33/1000 | Loss: 0.00001239
Iteration 34/1000 | Loss: 0.00001238
Iteration 35/1000 | Loss: 0.00001238
Iteration 36/1000 | Loss: 0.00001238
Iteration 37/1000 | Loss: 0.00001237
Iteration 38/1000 | Loss: 0.00001237
Iteration 39/1000 | Loss: 0.00001237
Iteration 40/1000 | Loss: 0.00001237
Iteration 41/1000 | Loss: 0.00001237
Iteration 42/1000 | Loss: 0.00001237
Iteration 43/1000 | Loss: 0.00001237
Iteration 44/1000 | Loss: 0.00001236
Iteration 45/1000 | Loss: 0.00001236
Iteration 46/1000 | Loss: 0.00001235
Iteration 47/1000 | Loss: 0.00001235
Iteration 48/1000 | Loss: 0.00001235
Iteration 49/1000 | Loss: 0.00001235
Iteration 50/1000 | Loss: 0.00001235
Iteration 51/1000 | Loss: 0.00001235
Iteration 52/1000 | Loss: 0.00001235
Iteration 53/1000 | Loss: 0.00001235
Iteration 54/1000 | Loss: 0.00001234
Iteration 55/1000 | Loss: 0.00001234
Iteration 56/1000 | Loss: 0.00001234
Iteration 57/1000 | Loss: 0.00001234
Iteration 58/1000 | Loss: 0.00001234
Iteration 59/1000 | Loss: 0.00001234
Iteration 60/1000 | Loss: 0.00001234
Iteration 61/1000 | Loss: 0.00001233
Iteration 62/1000 | Loss: 0.00001233
Iteration 63/1000 | Loss: 0.00001233
Iteration 64/1000 | Loss: 0.00001233
Iteration 65/1000 | Loss: 0.00001233
Iteration 66/1000 | Loss: 0.00001233
Iteration 67/1000 | Loss: 0.00001233
Iteration 68/1000 | Loss: 0.00001232
Iteration 69/1000 | Loss: 0.00001232
Iteration 70/1000 | Loss: 0.00001232
Iteration 71/1000 | Loss: 0.00001232
Iteration 72/1000 | Loss: 0.00001232
Iteration 73/1000 | Loss: 0.00001232
Iteration 74/1000 | Loss: 0.00001232
Iteration 75/1000 | Loss: 0.00001231
Iteration 76/1000 | Loss: 0.00001231
Iteration 77/1000 | Loss: 0.00001231
Iteration 78/1000 | Loss: 0.00001231
Iteration 79/1000 | Loss: 0.00001231
Iteration 80/1000 | Loss: 0.00001230
Iteration 81/1000 | Loss: 0.00001230
Iteration 82/1000 | Loss: 0.00001230
Iteration 83/1000 | Loss: 0.00001230
Iteration 84/1000 | Loss: 0.00001230
Iteration 85/1000 | Loss: 0.00001230
Iteration 86/1000 | Loss: 0.00001230
Iteration 87/1000 | Loss: 0.00001230
Iteration 88/1000 | Loss: 0.00001229
Iteration 89/1000 | Loss: 0.00001229
Iteration 90/1000 | Loss: 0.00001229
Iteration 91/1000 | Loss: 0.00001229
Iteration 92/1000 | Loss: 0.00001229
Iteration 93/1000 | Loss: 0.00001229
Iteration 94/1000 | Loss: 0.00001229
Iteration 95/1000 | Loss: 0.00001229
Iteration 96/1000 | Loss: 0.00001229
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001229
Iteration 100/1000 | Loss: 0.00001229
Iteration 101/1000 | Loss: 0.00001229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.2288026482565328e-05, 1.2288026482565328e-05, 1.2288026482565328e-05, 1.2288026482565328e-05, 1.2288026482565328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2288026482565328e-05

Optimization complete. Final v2v error: 3.0256919860839844 mm

Highest mean error: 3.3998360633850098 mm for frame 189

Lowest mean error: 2.808366537094116 mm for frame 171

Saving results

Total time: 35.34492015838623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00683605
Iteration 2/25 | Loss: 0.00101155
Iteration 3/25 | Loss: 0.00074669
Iteration 4/25 | Loss: 0.00072282
Iteration 5/25 | Loss: 0.00071615
Iteration 6/25 | Loss: 0.00071443
Iteration 7/25 | Loss: 0.00071420
Iteration 8/25 | Loss: 0.00071420
Iteration 9/25 | Loss: 0.00071420
Iteration 10/25 | Loss: 0.00071420
Iteration 11/25 | Loss: 0.00071420
Iteration 12/25 | Loss: 0.00071420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007142003742046654, 0.0007142003742046654, 0.0007142003742046654, 0.0007142003742046654, 0.0007142003742046654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007142003742046654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.77671933
Iteration 2/25 | Loss: 0.00035496
Iteration 3/25 | Loss: 0.00035493
Iteration 4/25 | Loss: 0.00035493
Iteration 5/25 | Loss: 0.00035492
Iteration 6/25 | Loss: 0.00035492
Iteration 7/25 | Loss: 0.00035492
Iteration 8/25 | Loss: 0.00035492
Iteration 9/25 | Loss: 0.00035492
Iteration 10/25 | Loss: 0.00035492
Iteration 11/25 | Loss: 0.00035492
Iteration 12/25 | Loss: 0.00035492
Iteration 13/25 | Loss: 0.00035492
Iteration 14/25 | Loss: 0.00035492
Iteration 15/25 | Loss: 0.00035492
Iteration 16/25 | Loss: 0.00035492
Iteration 17/25 | Loss: 0.00035492
Iteration 18/25 | Loss: 0.00035492
Iteration 19/25 | Loss: 0.00035492
Iteration 20/25 | Loss: 0.00035492
Iteration 21/25 | Loss: 0.00035492
Iteration 22/25 | Loss: 0.00035492
Iteration 23/25 | Loss: 0.00035492
Iteration 24/25 | Loss: 0.00035492
Iteration 25/25 | Loss: 0.00035492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035492
Iteration 2/1000 | Loss: 0.00002801
Iteration 3/1000 | Loss: 0.00002164
Iteration 4/1000 | Loss: 0.00002060
Iteration 5/1000 | Loss: 0.00001957
Iteration 6/1000 | Loss: 0.00001905
Iteration 7/1000 | Loss: 0.00001868
Iteration 8/1000 | Loss: 0.00001837
Iteration 9/1000 | Loss: 0.00001810
Iteration 10/1000 | Loss: 0.00001798
Iteration 11/1000 | Loss: 0.00001795
Iteration 12/1000 | Loss: 0.00001788
Iteration 13/1000 | Loss: 0.00001785
Iteration 14/1000 | Loss: 0.00001785
Iteration 15/1000 | Loss: 0.00001784
Iteration 16/1000 | Loss: 0.00001784
Iteration 17/1000 | Loss: 0.00001783
Iteration 18/1000 | Loss: 0.00001783
Iteration 19/1000 | Loss: 0.00001782
Iteration 20/1000 | Loss: 0.00001782
Iteration 21/1000 | Loss: 0.00001781
Iteration 22/1000 | Loss: 0.00001781
Iteration 23/1000 | Loss: 0.00001780
Iteration 24/1000 | Loss: 0.00001780
Iteration 25/1000 | Loss: 0.00001779
Iteration 26/1000 | Loss: 0.00001779
Iteration 27/1000 | Loss: 0.00001779
Iteration 28/1000 | Loss: 0.00001779
Iteration 29/1000 | Loss: 0.00001778
Iteration 30/1000 | Loss: 0.00001777
Iteration 31/1000 | Loss: 0.00001777
Iteration 32/1000 | Loss: 0.00001777
Iteration 33/1000 | Loss: 0.00001776
Iteration 34/1000 | Loss: 0.00001773
Iteration 35/1000 | Loss: 0.00001772
Iteration 36/1000 | Loss: 0.00001770
Iteration 37/1000 | Loss: 0.00001770
Iteration 38/1000 | Loss: 0.00001769
Iteration 39/1000 | Loss: 0.00001769
Iteration 40/1000 | Loss: 0.00001766
Iteration 41/1000 | Loss: 0.00001766
Iteration 42/1000 | Loss: 0.00001766
Iteration 43/1000 | Loss: 0.00001766
Iteration 44/1000 | Loss: 0.00001766
Iteration 45/1000 | Loss: 0.00001766
Iteration 46/1000 | Loss: 0.00001766
Iteration 47/1000 | Loss: 0.00001766
Iteration 48/1000 | Loss: 0.00001765
Iteration 49/1000 | Loss: 0.00001765
Iteration 50/1000 | Loss: 0.00001764
Iteration 51/1000 | Loss: 0.00001764
Iteration 52/1000 | Loss: 0.00001763
Iteration 53/1000 | Loss: 0.00001763
Iteration 54/1000 | Loss: 0.00001763
Iteration 55/1000 | Loss: 0.00001763
Iteration 56/1000 | Loss: 0.00001762
Iteration 57/1000 | Loss: 0.00001762
Iteration 58/1000 | Loss: 0.00001762
Iteration 59/1000 | Loss: 0.00001762
Iteration 60/1000 | Loss: 0.00001761
Iteration 61/1000 | Loss: 0.00001761
Iteration 62/1000 | Loss: 0.00001761
Iteration 63/1000 | Loss: 0.00001761
Iteration 64/1000 | Loss: 0.00001761
Iteration 65/1000 | Loss: 0.00001761
Iteration 66/1000 | Loss: 0.00001760
Iteration 67/1000 | Loss: 0.00001760
Iteration 68/1000 | Loss: 0.00001759
Iteration 69/1000 | Loss: 0.00001759
Iteration 70/1000 | Loss: 0.00001759
Iteration 71/1000 | Loss: 0.00001759
Iteration 72/1000 | Loss: 0.00001759
Iteration 73/1000 | Loss: 0.00001759
Iteration 74/1000 | Loss: 0.00001759
Iteration 75/1000 | Loss: 0.00001759
Iteration 76/1000 | Loss: 0.00001759
Iteration 77/1000 | Loss: 0.00001759
Iteration 78/1000 | Loss: 0.00001758
Iteration 79/1000 | Loss: 0.00001758
Iteration 80/1000 | Loss: 0.00001758
Iteration 81/1000 | Loss: 0.00001758
Iteration 82/1000 | Loss: 0.00001758
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00001758
Iteration 85/1000 | Loss: 0.00001758
Iteration 86/1000 | Loss: 0.00001758
Iteration 87/1000 | Loss: 0.00001757
Iteration 88/1000 | Loss: 0.00001757
Iteration 89/1000 | Loss: 0.00001757
Iteration 90/1000 | Loss: 0.00001757
Iteration 91/1000 | Loss: 0.00001757
Iteration 92/1000 | Loss: 0.00001757
Iteration 93/1000 | Loss: 0.00001757
Iteration 94/1000 | Loss: 0.00001757
Iteration 95/1000 | Loss: 0.00001757
Iteration 96/1000 | Loss: 0.00001757
Iteration 97/1000 | Loss: 0.00001757
Iteration 98/1000 | Loss: 0.00001757
Iteration 99/1000 | Loss: 0.00001756
Iteration 100/1000 | Loss: 0.00001756
Iteration 101/1000 | Loss: 0.00001756
Iteration 102/1000 | Loss: 0.00001756
Iteration 103/1000 | Loss: 0.00001756
Iteration 104/1000 | Loss: 0.00001756
Iteration 105/1000 | Loss: 0.00001756
Iteration 106/1000 | Loss: 0.00001756
Iteration 107/1000 | Loss: 0.00001756
Iteration 108/1000 | Loss: 0.00001756
Iteration 109/1000 | Loss: 0.00001756
Iteration 110/1000 | Loss: 0.00001756
Iteration 111/1000 | Loss: 0.00001756
Iteration 112/1000 | Loss: 0.00001756
Iteration 113/1000 | Loss: 0.00001755
Iteration 114/1000 | Loss: 0.00001755
Iteration 115/1000 | Loss: 0.00001755
Iteration 116/1000 | Loss: 0.00001755
Iteration 117/1000 | Loss: 0.00001755
Iteration 118/1000 | Loss: 0.00001755
Iteration 119/1000 | Loss: 0.00001755
Iteration 120/1000 | Loss: 0.00001755
Iteration 121/1000 | Loss: 0.00001755
Iteration 122/1000 | Loss: 0.00001755
Iteration 123/1000 | Loss: 0.00001755
Iteration 124/1000 | Loss: 0.00001755
Iteration 125/1000 | Loss: 0.00001755
Iteration 126/1000 | Loss: 0.00001755
Iteration 127/1000 | Loss: 0.00001755
Iteration 128/1000 | Loss: 0.00001754
Iteration 129/1000 | Loss: 0.00001754
Iteration 130/1000 | Loss: 0.00001754
Iteration 131/1000 | Loss: 0.00001754
Iteration 132/1000 | Loss: 0.00001754
Iteration 133/1000 | Loss: 0.00001754
Iteration 134/1000 | Loss: 0.00001754
Iteration 135/1000 | Loss: 0.00001754
Iteration 136/1000 | Loss: 0.00001754
Iteration 137/1000 | Loss: 0.00001754
Iteration 138/1000 | Loss: 0.00001754
Iteration 139/1000 | Loss: 0.00001754
Iteration 140/1000 | Loss: 0.00001754
Iteration 141/1000 | Loss: 0.00001754
Iteration 142/1000 | Loss: 0.00001753
Iteration 143/1000 | Loss: 0.00001753
Iteration 144/1000 | Loss: 0.00001753
Iteration 145/1000 | Loss: 0.00001753
Iteration 146/1000 | Loss: 0.00001753
Iteration 147/1000 | Loss: 0.00001753
Iteration 148/1000 | Loss: 0.00001753
Iteration 149/1000 | Loss: 0.00001753
Iteration 150/1000 | Loss: 0.00001752
Iteration 151/1000 | Loss: 0.00001752
Iteration 152/1000 | Loss: 0.00001752
Iteration 153/1000 | Loss: 0.00001752
Iteration 154/1000 | Loss: 0.00001752
Iteration 155/1000 | Loss: 0.00001752
Iteration 156/1000 | Loss: 0.00001752
Iteration 157/1000 | Loss: 0.00001752
Iteration 158/1000 | Loss: 0.00001752
Iteration 159/1000 | Loss: 0.00001752
Iteration 160/1000 | Loss: 0.00001752
Iteration 161/1000 | Loss: 0.00001752
Iteration 162/1000 | Loss: 0.00001752
Iteration 163/1000 | Loss: 0.00001751
Iteration 164/1000 | Loss: 0.00001751
Iteration 165/1000 | Loss: 0.00001751
Iteration 166/1000 | Loss: 0.00001751
Iteration 167/1000 | Loss: 0.00001751
Iteration 168/1000 | Loss: 0.00001751
Iteration 169/1000 | Loss: 0.00001751
Iteration 170/1000 | Loss: 0.00001751
Iteration 171/1000 | Loss: 0.00001751
Iteration 172/1000 | Loss: 0.00001751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.7514208593638614e-05, 1.7514208593638614e-05, 1.7514208593638614e-05, 1.7514208593638614e-05, 1.7514208593638614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7514208593638614e-05

Optimization complete. Final v2v error: 3.5311367511749268 mm

Highest mean error: 4.267606735229492 mm for frame 176

Lowest mean error: 3.0194852352142334 mm for frame 189

Saving results

Total time: 39.26724314689636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00371246
Iteration 2/25 | Loss: 0.00098202
Iteration 3/25 | Loss: 0.00067853
Iteration 4/25 | Loss: 0.00064071
Iteration 5/25 | Loss: 0.00063209
Iteration 6/25 | Loss: 0.00063050
Iteration 7/25 | Loss: 0.00062985
Iteration 8/25 | Loss: 0.00062985
Iteration 9/25 | Loss: 0.00062985
Iteration 10/25 | Loss: 0.00062985
Iteration 11/25 | Loss: 0.00062985
Iteration 12/25 | Loss: 0.00062985
Iteration 13/25 | Loss: 0.00062985
Iteration 14/25 | Loss: 0.00062985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000629851536359638, 0.000629851536359638, 0.000629851536359638, 0.000629851536359638, 0.000629851536359638]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000629851536359638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55608153
Iteration 2/25 | Loss: 0.00028047
Iteration 3/25 | Loss: 0.00028046
Iteration 4/25 | Loss: 0.00028046
Iteration 5/25 | Loss: 0.00028046
Iteration 6/25 | Loss: 0.00028046
Iteration 7/25 | Loss: 0.00028046
Iteration 8/25 | Loss: 0.00028046
Iteration 9/25 | Loss: 0.00028046
Iteration 10/25 | Loss: 0.00028046
Iteration 11/25 | Loss: 0.00028046
Iteration 12/25 | Loss: 0.00028046
Iteration 13/25 | Loss: 0.00028046
Iteration 14/25 | Loss: 0.00028046
Iteration 15/25 | Loss: 0.00028046
Iteration 16/25 | Loss: 0.00028046
Iteration 17/25 | Loss: 0.00028046
Iteration 18/25 | Loss: 0.00028046
Iteration 19/25 | Loss: 0.00028046
Iteration 20/25 | Loss: 0.00028046
Iteration 21/25 | Loss: 0.00028046
Iteration 22/25 | Loss: 0.00028046
Iteration 23/25 | Loss: 0.00028046
Iteration 24/25 | Loss: 0.00028046
Iteration 25/25 | Loss: 0.00028046
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0002804602263495326, 0.0002804602263495326, 0.0002804602263495326, 0.0002804602263495326, 0.0002804602263495326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002804602263495326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028046
Iteration 2/1000 | Loss: 0.00002852
Iteration 3/1000 | Loss: 0.00001918
Iteration 4/1000 | Loss: 0.00001742
Iteration 5/1000 | Loss: 0.00001673
Iteration 6/1000 | Loss: 0.00001626
Iteration 7/1000 | Loss: 0.00001594
Iteration 8/1000 | Loss: 0.00001558
Iteration 9/1000 | Loss: 0.00001540
Iteration 10/1000 | Loss: 0.00001533
Iteration 11/1000 | Loss: 0.00001531
Iteration 12/1000 | Loss: 0.00001530
Iteration 13/1000 | Loss: 0.00001522
Iteration 14/1000 | Loss: 0.00001521
Iteration 15/1000 | Loss: 0.00001521
Iteration 16/1000 | Loss: 0.00001520
Iteration 17/1000 | Loss: 0.00001520
Iteration 18/1000 | Loss: 0.00001519
Iteration 19/1000 | Loss: 0.00001519
Iteration 20/1000 | Loss: 0.00001519
Iteration 21/1000 | Loss: 0.00001519
Iteration 22/1000 | Loss: 0.00001519
Iteration 23/1000 | Loss: 0.00001519
Iteration 24/1000 | Loss: 0.00001518
Iteration 25/1000 | Loss: 0.00001518
Iteration 26/1000 | Loss: 0.00001518
Iteration 27/1000 | Loss: 0.00001518
Iteration 28/1000 | Loss: 0.00001517
Iteration 29/1000 | Loss: 0.00001517
Iteration 30/1000 | Loss: 0.00001517
Iteration 31/1000 | Loss: 0.00001516
Iteration 32/1000 | Loss: 0.00001516
Iteration 33/1000 | Loss: 0.00001516
Iteration 34/1000 | Loss: 0.00001516
Iteration 35/1000 | Loss: 0.00001516
Iteration 36/1000 | Loss: 0.00001516
Iteration 37/1000 | Loss: 0.00001515
Iteration 38/1000 | Loss: 0.00001515
Iteration 39/1000 | Loss: 0.00001515
Iteration 40/1000 | Loss: 0.00001515
Iteration 41/1000 | Loss: 0.00001515
Iteration 42/1000 | Loss: 0.00001514
Iteration 43/1000 | Loss: 0.00001514
Iteration 44/1000 | Loss: 0.00001513
Iteration 45/1000 | Loss: 0.00001513
Iteration 46/1000 | Loss: 0.00001513
Iteration 47/1000 | Loss: 0.00001513
Iteration 48/1000 | Loss: 0.00001513
Iteration 49/1000 | Loss: 0.00001512
Iteration 50/1000 | Loss: 0.00001512
Iteration 51/1000 | Loss: 0.00001512
Iteration 52/1000 | Loss: 0.00001512
Iteration 53/1000 | Loss: 0.00001512
Iteration 54/1000 | Loss: 0.00001511
Iteration 55/1000 | Loss: 0.00001511
Iteration 56/1000 | Loss: 0.00001511
Iteration 57/1000 | Loss: 0.00001511
Iteration 58/1000 | Loss: 0.00001511
Iteration 59/1000 | Loss: 0.00001511
Iteration 60/1000 | Loss: 0.00001510
Iteration 61/1000 | Loss: 0.00001510
Iteration 62/1000 | Loss: 0.00001510
Iteration 63/1000 | Loss: 0.00001510
Iteration 64/1000 | Loss: 0.00001510
Iteration 65/1000 | Loss: 0.00001510
Iteration 66/1000 | Loss: 0.00001510
Iteration 67/1000 | Loss: 0.00001510
Iteration 68/1000 | Loss: 0.00001510
Iteration 69/1000 | Loss: 0.00001510
Iteration 70/1000 | Loss: 0.00001510
Iteration 71/1000 | Loss: 0.00001510
Iteration 72/1000 | Loss: 0.00001510
Iteration 73/1000 | Loss: 0.00001510
Iteration 74/1000 | Loss: 0.00001510
Iteration 75/1000 | Loss: 0.00001510
Iteration 76/1000 | Loss: 0.00001510
Iteration 77/1000 | Loss: 0.00001510
Iteration 78/1000 | Loss: 0.00001510
Iteration 79/1000 | Loss: 0.00001510
Iteration 80/1000 | Loss: 0.00001510
Iteration 81/1000 | Loss: 0.00001510
Iteration 82/1000 | Loss: 0.00001510
Iteration 83/1000 | Loss: 0.00001510
Iteration 84/1000 | Loss: 0.00001510
Iteration 85/1000 | Loss: 0.00001510
Iteration 86/1000 | Loss: 0.00001510
Iteration 87/1000 | Loss: 0.00001510
Iteration 88/1000 | Loss: 0.00001510
Iteration 89/1000 | Loss: 0.00001510
Iteration 90/1000 | Loss: 0.00001510
Iteration 91/1000 | Loss: 0.00001510
Iteration 92/1000 | Loss: 0.00001510
Iteration 93/1000 | Loss: 0.00001510
Iteration 94/1000 | Loss: 0.00001510
Iteration 95/1000 | Loss: 0.00001510
Iteration 96/1000 | Loss: 0.00001510
Iteration 97/1000 | Loss: 0.00001510
Iteration 98/1000 | Loss: 0.00001510
Iteration 99/1000 | Loss: 0.00001510
Iteration 100/1000 | Loss: 0.00001510
Iteration 101/1000 | Loss: 0.00001510
Iteration 102/1000 | Loss: 0.00001510
Iteration 103/1000 | Loss: 0.00001510
Iteration 104/1000 | Loss: 0.00001510
Iteration 105/1000 | Loss: 0.00001510
Iteration 106/1000 | Loss: 0.00001510
Iteration 107/1000 | Loss: 0.00001510
Iteration 108/1000 | Loss: 0.00001510
Iteration 109/1000 | Loss: 0.00001510
Iteration 110/1000 | Loss: 0.00001510
Iteration 111/1000 | Loss: 0.00001510
Iteration 112/1000 | Loss: 0.00001510
Iteration 113/1000 | Loss: 0.00001510
Iteration 114/1000 | Loss: 0.00001510
Iteration 115/1000 | Loss: 0.00001510
Iteration 116/1000 | Loss: 0.00001510
Iteration 117/1000 | Loss: 0.00001510
Iteration 118/1000 | Loss: 0.00001510
Iteration 119/1000 | Loss: 0.00001510
Iteration 120/1000 | Loss: 0.00001510
Iteration 121/1000 | Loss: 0.00001510
Iteration 122/1000 | Loss: 0.00001510
Iteration 123/1000 | Loss: 0.00001510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.5100533346412703e-05, 1.5100533346412703e-05, 1.5100533346412703e-05, 1.5100533346412703e-05, 1.5100533346412703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5100533346412703e-05

Optimization complete. Final v2v error: 3.3054885864257812 mm

Highest mean error: 3.572916030883789 mm for frame 22

Lowest mean error: 2.7802581787109375 mm for frame 5

Saving results

Total time: 31.28707456588745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821134
Iteration 2/25 | Loss: 0.00090848
Iteration 3/25 | Loss: 0.00069192
Iteration 4/25 | Loss: 0.00066426
Iteration 5/25 | Loss: 0.00065449
Iteration 6/25 | Loss: 0.00065321
Iteration 7/25 | Loss: 0.00065321
Iteration 8/25 | Loss: 0.00065321
Iteration 9/25 | Loss: 0.00065321
Iteration 10/25 | Loss: 0.00065321
Iteration 11/25 | Loss: 0.00065321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006532076513394713, 0.0006532076513394713, 0.0006532076513394713, 0.0006532076513394713, 0.0006532076513394713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006532076513394713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46158957
Iteration 2/25 | Loss: 0.00029638
Iteration 3/25 | Loss: 0.00029636
Iteration 4/25 | Loss: 0.00029636
Iteration 5/25 | Loss: 0.00029636
Iteration 6/25 | Loss: 0.00029636
Iteration 7/25 | Loss: 0.00029636
Iteration 8/25 | Loss: 0.00029636
Iteration 9/25 | Loss: 0.00029636
Iteration 10/25 | Loss: 0.00029636
Iteration 11/25 | Loss: 0.00029636
Iteration 12/25 | Loss: 0.00029636
Iteration 13/25 | Loss: 0.00029636
Iteration 14/25 | Loss: 0.00029636
Iteration 15/25 | Loss: 0.00029636
Iteration 16/25 | Loss: 0.00029636
Iteration 17/25 | Loss: 0.00029636
Iteration 18/25 | Loss: 0.00029636
Iteration 19/25 | Loss: 0.00029636
Iteration 20/25 | Loss: 0.00029636
Iteration 21/25 | Loss: 0.00029636
Iteration 22/25 | Loss: 0.00029636
Iteration 23/25 | Loss: 0.00029636
Iteration 24/25 | Loss: 0.00029636
Iteration 25/25 | Loss: 0.00029636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029636
Iteration 2/1000 | Loss: 0.00003232
Iteration 3/1000 | Loss: 0.00002548
Iteration 4/1000 | Loss: 0.00002127
Iteration 5/1000 | Loss: 0.00001984
Iteration 6/1000 | Loss: 0.00001901
Iteration 7/1000 | Loss: 0.00001839
Iteration 8/1000 | Loss: 0.00001800
Iteration 9/1000 | Loss: 0.00001768
Iteration 10/1000 | Loss: 0.00001747
Iteration 11/1000 | Loss: 0.00001738
Iteration 12/1000 | Loss: 0.00001731
Iteration 13/1000 | Loss: 0.00001729
Iteration 14/1000 | Loss: 0.00001726
Iteration 15/1000 | Loss: 0.00001725
Iteration 16/1000 | Loss: 0.00001724
Iteration 17/1000 | Loss: 0.00001723
Iteration 18/1000 | Loss: 0.00001722
Iteration 19/1000 | Loss: 0.00001712
Iteration 20/1000 | Loss: 0.00001709
Iteration 21/1000 | Loss: 0.00001708
Iteration 22/1000 | Loss: 0.00001705
Iteration 23/1000 | Loss: 0.00001704
Iteration 24/1000 | Loss: 0.00001702
Iteration 25/1000 | Loss: 0.00001702
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001701
Iteration 28/1000 | Loss: 0.00001700
Iteration 29/1000 | Loss: 0.00001699
Iteration 30/1000 | Loss: 0.00001699
Iteration 31/1000 | Loss: 0.00001698
Iteration 32/1000 | Loss: 0.00001698
Iteration 33/1000 | Loss: 0.00001698
Iteration 34/1000 | Loss: 0.00001697
Iteration 35/1000 | Loss: 0.00001697
Iteration 36/1000 | Loss: 0.00001696
Iteration 37/1000 | Loss: 0.00001696
Iteration 38/1000 | Loss: 0.00001696
Iteration 39/1000 | Loss: 0.00001695
Iteration 40/1000 | Loss: 0.00001695
Iteration 41/1000 | Loss: 0.00001695
Iteration 42/1000 | Loss: 0.00001695
Iteration 43/1000 | Loss: 0.00001695
Iteration 44/1000 | Loss: 0.00001694
Iteration 45/1000 | Loss: 0.00001694
Iteration 46/1000 | Loss: 0.00001694
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001691
Iteration 49/1000 | Loss: 0.00001691
Iteration 50/1000 | Loss: 0.00001691
Iteration 51/1000 | Loss: 0.00001691
Iteration 52/1000 | Loss: 0.00001690
Iteration 53/1000 | Loss: 0.00001690
Iteration 54/1000 | Loss: 0.00001690
Iteration 55/1000 | Loss: 0.00001689
Iteration 56/1000 | Loss: 0.00001689
Iteration 57/1000 | Loss: 0.00001689
Iteration 58/1000 | Loss: 0.00001688
Iteration 59/1000 | Loss: 0.00001688
Iteration 60/1000 | Loss: 0.00001688
Iteration 61/1000 | Loss: 0.00001688
Iteration 62/1000 | Loss: 0.00001688
Iteration 63/1000 | Loss: 0.00001688
Iteration 64/1000 | Loss: 0.00001688
Iteration 65/1000 | Loss: 0.00001688
Iteration 66/1000 | Loss: 0.00001688
Iteration 67/1000 | Loss: 0.00001688
Iteration 68/1000 | Loss: 0.00001688
Iteration 69/1000 | Loss: 0.00001688
Iteration 70/1000 | Loss: 0.00001687
Iteration 71/1000 | Loss: 0.00001687
Iteration 72/1000 | Loss: 0.00001687
Iteration 73/1000 | Loss: 0.00001687
Iteration 74/1000 | Loss: 0.00001686
Iteration 75/1000 | Loss: 0.00001686
Iteration 76/1000 | Loss: 0.00001686
Iteration 77/1000 | Loss: 0.00001685
Iteration 78/1000 | Loss: 0.00001685
Iteration 79/1000 | Loss: 0.00001685
Iteration 80/1000 | Loss: 0.00001685
Iteration 81/1000 | Loss: 0.00001685
Iteration 82/1000 | Loss: 0.00001684
Iteration 83/1000 | Loss: 0.00001683
Iteration 84/1000 | Loss: 0.00001683
Iteration 85/1000 | Loss: 0.00001682
Iteration 86/1000 | Loss: 0.00001682
Iteration 87/1000 | Loss: 0.00001682
Iteration 88/1000 | Loss: 0.00001682
Iteration 89/1000 | Loss: 0.00001682
Iteration 90/1000 | Loss: 0.00001681
Iteration 91/1000 | Loss: 0.00001681
Iteration 92/1000 | Loss: 0.00001681
Iteration 93/1000 | Loss: 0.00001681
Iteration 94/1000 | Loss: 0.00001681
Iteration 95/1000 | Loss: 0.00001680
Iteration 96/1000 | Loss: 0.00001680
Iteration 97/1000 | Loss: 0.00001680
Iteration 98/1000 | Loss: 0.00001680
Iteration 99/1000 | Loss: 0.00001680
Iteration 100/1000 | Loss: 0.00001680
Iteration 101/1000 | Loss: 0.00001680
Iteration 102/1000 | Loss: 0.00001680
Iteration 103/1000 | Loss: 0.00001680
Iteration 104/1000 | Loss: 0.00001680
Iteration 105/1000 | Loss: 0.00001680
Iteration 106/1000 | Loss: 0.00001679
Iteration 107/1000 | Loss: 0.00001679
Iteration 108/1000 | Loss: 0.00001679
Iteration 109/1000 | Loss: 0.00001679
Iteration 110/1000 | Loss: 0.00001679
Iteration 111/1000 | Loss: 0.00001679
Iteration 112/1000 | Loss: 0.00001679
Iteration 113/1000 | Loss: 0.00001679
Iteration 114/1000 | Loss: 0.00001679
Iteration 115/1000 | Loss: 0.00001679
Iteration 116/1000 | Loss: 0.00001679
Iteration 117/1000 | Loss: 0.00001679
Iteration 118/1000 | Loss: 0.00001679
Iteration 119/1000 | Loss: 0.00001679
Iteration 120/1000 | Loss: 0.00001679
Iteration 121/1000 | Loss: 0.00001679
Iteration 122/1000 | Loss: 0.00001679
Iteration 123/1000 | Loss: 0.00001679
Iteration 124/1000 | Loss: 0.00001679
Iteration 125/1000 | Loss: 0.00001679
Iteration 126/1000 | Loss: 0.00001679
Iteration 127/1000 | Loss: 0.00001679
Iteration 128/1000 | Loss: 0.00001679
Iteration 129/1000 | Loss: 0.00001679
Iteration 130/1000 | Loss: 0.00001679
Iteration 131/1000 | Loss: 0.00001679
Iteration 132/1000 | Loss: 0.00001679
Iteration 133/1000 | Loss: 0.00001679
Iteration 134/1000 | Loss: 0.00001679
Iteration 135/1000 | Loss: 0.00001679
Iteration 136/1000 | Loss: 0.00001679
Iteration 137/1000 | Loss: 0.00001679
Iteration 138/1000 | Loss: 0.00001679
Iteration 139/1000 | Loss: 0.00001679
Iteration 140/1000 | Loss: 0.00001679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.678986882325262e-05, 1.678986882325262e-05, 1.678986882325262e-05, 1.678986882325262e-05, 1.678986882325262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.678986882325262e-05

Optimization complete. Final v2v error: 3.4147632122039795 mm

Highest mean error: 3.887669086456299 mm for frame 62

Lowest mean error: 3.0438997745513916 mm for frame 28

Saving results

Total time: 36.95837759971619
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00570105
Iteration 2/25 | Loss: 0.00093893
Iteration 3/25 | Loss: 0.00073670
Iteration 4/25 | Loss: 0.00069938
Iteration 5/25 | Loss: 0.00069180
Iteration 6/25 | Loss: 0.00069073
Iteration 7/25 | Loss: 0.00069073
Iteration 8/25 | Loss: 0.00069073
Iteration 9/25 | Loss: 0.00069073
Iteration 10/25 | Loss: 0.00069073
Iteration 11/25 | Loss: 0.00069073
Iteration 12/25 | Loss: 0.00069073
Iteration 13/25 | Loss: 0.00069073
Iteration 14/25 | Loss: 0.00069073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006907272036187351, 0.0006907272036187351, 0.0006907272036187351, 0.0006907272036187351, 0.0006907272036187351]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006907272036187351

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 14.23937798
Iteration 2/25 | Loss: 0.00023397
Iteration 3/25 | Loss: 0.00023396
Iteration 4/25 | Loss: 0.00023396
Iteration 5/25 | Loss: 0.00023396
Iteration 6/25 | Loss: 0.00023396
Iteration 7/25 | Loss: 0.00023396
Iteration 8/25 | Loss: 0.00023396
Iteration 9/25 | Loss: 0.00023396
Iteration 10/25 | Loss: 0.00023396
Iteration 11/25 | Loss: 0.00023396
Iteration 12/25 | Loss: 0.00023396
Iteration 13/25 | Loss: 0.00023396
Iteration 14/25 | Loss: 0.00023396
Iteration 15/25 | Loss: 0.00023396
Iteration 16/25 | Loss: 0.00023396
Iteration 17/25 | Loss: 0.00023396
Iteration 18/25 | Loss: 0.00023396
Iteration 19/25 | Loss: 0.00023396
Iteration 20/25 | Loss: 0.00023396
Iteration 21/25 | Loss: 0.00023396
Iteration 22/25 | Loss: 0.00023396
Iteration 23/25 | Loss: 0.00023396
Iteration 24/25 | Loss: 0.00023396
Iteration 25/25 | Loss: 0.00023396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023396
Iteration 2/1000 | Loss: 0.00003106
Iteration 3/1000 | Loss: 0.00001890
Iteration 4/1000 | Loss: 0.00001745
Iteration 5/1000 | Loss: 0.00001671
Iteration 6/1000 | Loss: 0.00001636
Iteration 7/1000 | Loss: 0.00001611
Iteration 8/1000 | Loss: 0.00001585
Iteration 9/1000 | Loss: 0.00001577
Iteration 10/1000 | Loss: 0.00001574
Iteration 11/1000 | Loss: 0.00001572
Iteration 12/1000 | Loss: 0.00001570
Iteration 13/1000 | Loss: 0.00001570
Iteration 14/1000 | Loss: 0.00001570
Iteration 15/1000 | Loss: 0.00001570
Iteration 16/1000 | Loss: 0.00001570
Iteration 17/1000 | Loss: 0.00001570
Iteration 18/1000 | Loss: 0.00001570
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001570
Iteration 21/1000 | Loss: 0.00001570
Iteration 22/1000 | Loss: 0.00001570
Iteration 23/1000 | Loss: 0.00001569
Iteration 24/1000 | Loss: 0.00001569
Iteration 25/1000 | Loss: 0.00001568
Iteration 26/1000 | Loss: 0.00001567
Iteration 27/1000 | Loss: 0.00001567
Iteration 28/1000 | Loss: 0.00001567
Iteration 29/1000 | Loss: 0.00001567
Iteration 30/1000 | Loss: 0.00001567
Iteration 31/1000 | Loss: 0.00001567
Iteration 32/1000 | Loss: 0.00001567
Iteration 33/1000 | Loss: 0.00001567
Iteration 34/1000 | Loss: 0.00001566
Iteration 35/1000 | Loss: 0.00001566
Iteration 36/1000 | Loss: 0.00001566
Iteration 37/1000 | Loss: 0.00001566
Iteration 38/1000 | Loss: 0.00001566
Iteration 39/1000 | Loss: 0.00001566
Iteration 40/1000 | Loss: 0.00001565
Iteration 41/1000 | Loss: 0.00001565
Iteration 42/1000 | Loss: 0.00001565
Iteration 43/1000 | Loss: 0.00001564
Iteration 44/1000 | Loss: 0.00001564
Iteration 45/1000 | Loss: 0.00001563
Iteration 46/1000 | Loss: 0.00001563
Iteration 47/1000 | Loss: 0.00001562
Iteration 48/1000 | Loss: 0.00001562
Iteration 49/1000 | Loss: 0.00001561
Iteration 50/1000 | Loss: 0.00001561
Iteration 51/1000 | Loss: 0.00001560
Iteration 52/1000 | Loss: 0.00001560
Iteration 53/1000 | Loss: 0.00001560
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001560
Iteration 56/1000 | Loss: 0.00001560
Iteration 57/1000 | Loss: 0.00001560
Iteration 58/1000 | Loss: 0.00001560
Iteration 59/1000 | Loss: 0.00001560
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001558
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001558
Iteration 66/1000 | Loss: 0.00001557
Iteration 67/1000 | Loss: 0.00001557
Iteration 68/1000 | Loss: 0.00001557
Iteration 69/1000 | Loss: 0.00001557
Iteration 70/1000 | Loss: 0.00001557
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00001556
Iteration 75/1000 | Loss: 0.00001556
Iteration 76/1000 | Loss: 0.00001556
Iteration 77/1000 | Loss: 0.00001555
Iteration 78/1000 | Loss: 0.00001555
Iteration 79/1000 | Loss: 0.00001555
Iteration 80/1000 | Loss: 0.00001555
Iteration 81/1000 | Loss: 0.00001555
Iteration 82/1000 | Loss: 0.00001555
Iteration 83/1000 | Loss: 0.00001555
Iteration 84/1000 | Loss: 0.00001555
Iteration 85/1000 | Loss: 0.00001555
Iteration 86/1000 | Loss: 0.00001554
Iteration 87/1000 | Loss: 0.00001554
Iteration 88/1000 | Loss: 0.00001554
Iteration 89/1000 | Loss: 0.00001554
Iteration 90/1000 | Loss: 0.00001554
Iteration 91/1000 | Loss: 0.00001554
Iteration 92/1000 | Loss: 0.00001554
Iteration 93/1000 | Loss: 0.00001554
Iteration 94/1000 | Loss: 0.00001554
Iteration 95/1000 | Loss: 0.00001554
Iteration 96/1000 | Loss: 0.00001554
Iteration 97/1000 | Loss: 0.00001554
Iteration 98/1000 | Loss: 0.00001554
Iteration 99/1000 | Loss: 0.00001554
Iteration 100/1000 | Loss: 0.00001554
Iteration 101/1000 | Loss: 0.00001554
Iteration 102/1000 | Loss: 0.00001554
Iteration 103/1000 | Loss: 0.00001553
Iteration 104/1000 | Loss: 0.00001553
Iteration 105/1000 | Loss: 0.00001553
Iteration 106/1000 | Loss: 0.00001553
Iteration 107/1000 | Loss: 0.00001553
Iteration 108/1000 | Loss: 0.00001553
Iteration 109/1000 | Loss: 0.00001553
Iteration 110/1000 | Loss: 0.00001553
Iteration 111/1000 | Loss: 0.00001553
Iteration 112/1000 | Loss: 0.00001553
Iteration 113/1000 | Loss: 0.00001553
Iteration 114/1000 | Loss: 0.00001553
Iteration 115/1000 | Loss: 0.00001553
Iteration 116/1000 | Loss: 0.00001553
Iteration 117/1000 | Loss: 0.00001553
Iteration 118/1000 | Loss: 0.00001553
Iteration 119/1000 | Loss: 0.00001553
Iteration 120/1000 | Loss: 0.00001553
Iteration 121/1000 | Loss: 0.00001553
Iteration 122/1000 | Loss: 0.00001553
Iteration 123/1000 | Loss: 0.00001553
Iteration 124/1000 | Loss: 0.00001553
Iteration 125/1000 | Loss: 0.00001553
Iteration 126/1000 | Loss: 0.00001553
Iteration 127/1000 | Loss: 0.00001553
Iteration 128/1000 | Loss: 0.00001553
Iteration 129/1000 | Loss: 0.00001553
Iteration 130/1000 | Loss: 0.00001553
Iteration 131/1000 | Loss: 0.00001553
Iteration 132/1000 | Loss: 0.00001553
Iteration 133/1000 | Loss: 0.00001553
Iteration 134/1000 | Loss: 0.00001553
Iteration 135/1000 | Loss: 0.00001553
Iteration 136/1000 | Loss: 0.00001553
Iteration 137/1000 | Loss: 0.00001553
Iteration 138/1000 | Loss: 0.00001553
Iteration 139/1000 | Loss: 0.00001553
Iteration 140/1000 | Loss: 0.00001553
Iteration 141/1000 | Loss: 0.00001553
Iteration 142/1000 | Loss: 0.00001553
Iteration 143/1000 | Loss: 0.00001553
Iteration 144/1000 | Loss: 0.00001553
Iteration 145/1000 | Loss: 0.00001553
Iteration 146/1000 | Loss: 0.00001553
Iteration 147/1000 | Loss: 0.00001553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.5526948118349537e-05, 1.5526948118349537e-05, 1.5526948118349537e-05, 1.5526948118349537e-05, 1.5526948118349537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5526948118349537e-05

Optimization complete. Final v2v error: 3.406158208847046 mm

Highest mean error: 3.5589170455932617 mm for frame 128

Lowest mean error: 3.2214486598968506 mm for frame 75

Saving results

Total time: 34.57228922843933
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382640
Iteration 2/25 | Loss: 0.00069125
Iteration 3/25 | Loss: 0.00059428
Iteration 4/25 | Loss: 0.00058039
Iteration 5/25 | Loss: 0.00057583
Iteration 6/25 | Loss: 0.00057472
Iteration 7/25 | Loss: 0.00057467
Iteration 8/25 | Loss: 0.00057467
Iteration 9/25 | Loss: 0.00057467
Iteration 10/25 | Loss: 0.00057467
Iteration 11/25 | Loss: 0.00057467
Iteration 12/25 | Loss: 0.00057467
Iteration 13/25 | Loss: 0.00057467
Iteration 14/25 | Loss: 0.00057467
Iteration 15/25 | Loss: 0.00057467
Iteration 16/25 | Loss: 0.00057467
Iteration 17/25 | Loss: 0.00057467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005746679962612689, 0.0005746679962612689, 0.0005746679962612689, 0.0005746679962612689, 0.0005746679962612689]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005746679962612689

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.14290142
Iteration 2/25 | Loss: 0.00028312
Iteration 3/25 | Loss: 0.00028311
Iteration 4/25 | Loss: 0.00028311
Iteration 5/25 | Loss: 0.00028311
Iteration 6/25 | Loss: 0.00028311
Iteration 7/25 | Loss: 0.00028311
Iteration 8/25 | Loss: 0.00028311
Iteration 9/25 | Loss: 0.00028311
Iteration 10/25 | Loss: 0.00028311
Iteration 11/25 | Loss: 0.00028311
Iteration 12/25 | Loss: 0.00028311
Iteration 13/25 | Loss: 0.00028311
Iteration 14/25 | Loss: 0.00028311
Iteration 15/25 | Loss: 0.00028311
Iteration 16/25 | Loss: 0.00028311
Iteration 17/25 | Loss: 0.00028311
Iteration 18/25 | Loss: 0.00028311
Iteration 19/25 | Loss: 0.00028311
Iteration 20/25 | Loss: 0.00028311
Iteration 21/25 | Loss: 0.00028311
Iteration 22/25 | Loss: 0.00028311
Iteration 23/25 | Loss: 0.00028311
Iteration 24/25 | Loss: 0.00028311
Iteration 25/25 | Loss: 0.00028311

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028311
Iteration 2/1000 | Loss: 0.00001648
Iteration 3/1000 | Loss: 0.00001201
Iteration 4/1000 | Loss: 0.00001130
Iteration 5/1000 | Loss: 0.00001077
Iteration 6/1000 | Loss: 0.00001061
Iteration 7/1000 | Loss: 0.00001033
Iteration 8/1000 | Loss: 0.00001027
Iteration 9/1000 | Loss: 0.00001021
Iteration 10/1000 | Loss: 0.00001016
Iteration 11/1000 | Loss: 0.00001015
Iteration 12/1000 | Loss: 0.00001009
Iteration 13/1000 | Loss: 0.00001007
Iteration 14/1000 | Loss: 0.00001003
Iteration 15/1000 | Loss: 0.00001003
Iteration 16/1000 | Loss: 0.00001002
Iteration 17/1000 | Loss: 0.00001002
Iteration 18/1000 | Loss: 0.00001002
Iteration 19/1000 | Loss: 0.00001000
Iteration 20/1000 | Loss: 0.00000999
Iteration 21/1000 | Loss: 0.00000999
Iteration 22/1000 | Loss: 0.00000999
Iteration 23/1000 | Loss: 0.00000998
Iteration 24/1000 | Loss: 0.00000998
Iteration 25/1000 | Loss: 0.00000998
Iteration 26/1000 | Loss: 0.00000995
Iteration 27/1000 | Loss: 0.00000995
Iteration 28/1000 | Loss: 0.00000994
Iteration 29/1000 | Loss: 0.00000994
Iteration 30/1000 | Loss: 0.00000994
Iteration 31/1000 | Loss: 0.00000994
Iteration 32/1000 | Loss: 0.00000994
Iteration 33/1000 | Loss: 0.00000994
Iteration 34/1000 | Loss: 0.00000994
Iteration 35/1000 | Loss: 0.00000994
Iteration 36/1000 | Loss: 0.00000993
Iteration 37/1000 | Loss: 0.00000992
Iteration 38/1000 | Loss: 0.00000991
Iteration 39/1000 | Loss: 0.00000991
Iteration 40/1000 | Loss: 0.00000991
Iteration 41/1000 | Loss: 0.00000991
Iteration 42/1000 | Loss: 0.00000991
Iteration 43/1000 | Loss: 0.00000991
Iteration 44/1000 | Loss: 0.00000990
Iteration 45/1000 | Loss: 0.00000989
Iteration 46/1000 | Loss: 0.00000989
Iteration 47/1000 | Loss: 0.00000988
Iteration 48/1000 | Loss: 0.00000988
Iteration 49/1000 | Loss: 0.00000988
Iteration 50/1000 | Loss: 0.00000988
Iteration 51/1000 | Loss: 0.00000987
Iteration 52/1000 | Loss: 0.00000987
Iteration 53/1000 | Loss: 0.00000987
Iteration 54/1000 | Loss: 0.00000986
Iteration 55/1000 | Loss: 0.00000985
Iteration 56/1000 | Loss: 0.00000985
Iteration 57/1000 | Loss: 0.00000985
Iteration 58/1000 | Loss: 0.00000984
Iteration 59/1000 | Loss: 0.00000984
Iteration 60/1000 | Loss: 0.00000984
Iteration 61/1000 | Loss: 0.00000984
Iteration 62/1000 | Loss: 0.00000984
Iteration 63/1000 | Loss: 0.00000984
Iteration 64/1000 | Loss: 0.00000984
Iteration 65/1000 | Loss: 0.00000984
Iteration 66/1000 | Loss: 0.00000983
Iteration 67/1000 | Loss: 0.00000983
Iteration 68/1000 | Loss: 0.00000983
Iteration 69/1000 | Loss: 0.00000983
Iteration 70/1000 | Loss: 0.00000983
Iteration 71/1000 | Loss: 0.00000983
Iteration 72/1000 | Loss: 0.00000983
Iteration 73/1000 | Loss: 0.00000982
Iteration 74/1000 | Loss: 0.00000982
Iteration 75/1000 | Loss: 0.00000982
Iteration 76/1000 | Loss: 0.00000982
Iteration 77/1000 | Loss: 0.00000982
Iteration 78/1000 | Loss: 0.00000982
Iteration 79/1000 | Loss: 0.00000982
Iteration 80/1000 | Loss: 0.00000982
Iteration 81/1000 | Loss: 0.00000982
Iteration 82/1000 | Loss: 0.00000982
Iteration 83/1000 | Loss: 0.00000982
Iteration 84/1000 | Loss: 0.00000982
Iteration 85/1000 | Loss: 0.00000982
Iteration 86/1000 | Loss: 0.00000982
Iteration 87/1000 | Loss: 0.00000982
Iteration 88/1000 | Loss: 0.00000982
Iteration 89/1000 | Loss: 0.00000982
Iteration 90/1000 | Loss: 0.00000982
Iteration 91/1000 | Loss: 0.00000982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [9.821330422710162e-06, 9.821330422710162e-06, 9.821330422710162e-06, 9.821330422710162e-06, 9.821330422710162e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.821330422710162e-06

Optimization complete. Final v2v error: 2.6685791015625 mm

Highest mean error: 2.8272316455841064 mm for frame 107

Lowest mean error: 2.590344190597534 mm for frame 44

Saving results

Total time: 27.89043426513672
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820306
Iteration 2/25 | Loss: 0.00131346
Iteration 3/25 | Loss: 0.00085757
Iteration 4/25 | Loss: 0.00080187
Iteration 5/25 | Loss: 0.00078494
Iteration 6/25 | Loss: 0.00078051
Iteration 7/25 | Loss: 0.00077913
Iteration 8/25 | Loss: 0.00077882
Iteration 9/25 | Loss: 0.00077882
Iteration 10/25 | Loss: 0.00077882
Iteration 11/25 | Loss: 0.00077882
Iteration 12/25 | Loss: 0.00077882
Iteration 13/25 | Loss: 0.00077882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007788201910443604, 0.0007788201910443604, 0.0007788201910443604, 0.0007788201910443604, 0.0007788201910443604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007788201910443604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17154896
Iteration 2/25 | Loss: 0.00034296
Iteration 3/25 | Loss: 0.00034296
Iteration 4/25 | Loss: 0.00034296
Iteration 5/25 | Loss: 0.00034296
Iteration 6/25 | Loss: 0.00034296
Iteration 7/25 | Loss: 0.00034296
Iteration 8/25 | Loss: 0.00034296
Iteration 9/25 | Loss: 0.00034296
Iteration 10/25 | Loss: 0.00034296
Iteration 11/25 | Loss: 0.00034296
Iteration 12/25 | Loss: 0.00034296
Iteration 13/25 | Loss: 0.00034296
Iteration 14/25 | Loss: 0.00034296
Iteration 15/25 | Loss: 0.00034296
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00034295645309612155, 0.00034295645309612155, 0.00034295645309612155, 0.00034295645309612155, 0.00034295645309612155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034295645309612155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034296
Iteration 2/1000 | Loss: 0.00005329
Iteration 3/1000 | Loss: 0.00003995
Iteration 4/1000 | Loss: 0.00003564
Iteration 5/1000 | Loss: 0.00003392
Iteration 6/1000 | Loss: 0.00003255
Iteration 7/1000 | Loss: 0.00003173
Iteration 8/1000 | Loss: 0.00003091
Iteration 9/1000 | Loss: 0.00003030
Iteration 10/1000 | Loss: 0.00002987
Iteration 11/1000 | Loss: 0.00002947
Iteration 12/1000 | Loss: 0.00002906
Iteration 13/1000 | Loss: 0.00002871
Iteration 14/1000 | Loss: 0.00002845
Iteration 15/1000 | Loss: 0.00002823
Iteration 16/1000 | Loss: 0.00002805
Iteration 17/1000 | Loss: 0.00002804
Iteration 18/1000 | Loss: 0.00002800
Iteration 19/1000 | Loss: 0.00002796
Iteration 20/1000 | Loss: 0.00002796
Iteration 21/1000 | Loss: 0.00002794
Iteration 22/1000 | Loss: 0.00002792
Iteration 23/1000 | Loss: 0.00002788
Iteration 24/1000 | Loss: 0.00002787
Iteration 25/1000 | Loss: 0.00002777
Iteration 26/1000 | Loss: 0.00002776
Iteration 27/1000 | Loss: 0.00002772
Iteration 28/1000 | Loss: 0.00002772
Iteration 29/1000 | Loss: 0.00002770
Iteration 30/1000 | Loss: 0.00002768
Iteration 31/1000 | Loss: 0.00002768
Iteration 32/1000 | Loss: 0.00002768
Iteration 33/1000 | Loss: 0.00002768
Iteration 34/1000 | Loss: 0.00002768
Iteration 35/1000 | Loss: 0.00002768
Iteration 36/1000 | Loss: 0.00002768
Iteration 37/1000 | Loss: 0.00002768
Iteration 38/1000 | Loss: 0.00002767
Iteration 39/1000 | Loss: 0.00002767
Iteration 40/1000 | Loss: 0.00002767
Iteration 41/1000 | Loss: 0.00002767
Iteration 42/1000 | Loss: 0.00002767
Iteration 43/1000 | Loss: 0.00002767
Iteration 44/1000 | Loss: 0.00002767
Iteration 45/1000 | Loss: 0.00002767
Iteration 46/1000 | Loss: 0.00002767
Iteration 47/1000 | Loss: 0.00002767
Iteration 48/1000 | Loss: 0.00002767
Iteration 49/1000 | Loss: 0.00002767
Iteration 50/1000 | Loss: 0.00002767
Iteration 51/1000 | Loss: 0.00002767
Iteration 52/1000 | Loss: 0.00002767
Iteration 53/1000 | Loss: 0.00002767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 53. Stopping optimization.
Last 5 losses: [2.7672169380821288e-05, 2.7672169380821288e-05, 2.7672169380821288e-05, 2.7672169380821288e-05, 2.7672169380821288e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7672169380821288e-05

Optimization complete. Final v2v error: 4.23624849319458 mm

Highest mean error: 5.615320682525635 mm for frame 82

Lowest mean error: 3.508635997772217 mm for frame 192

Saving results

Total time: 41.17449069023132
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853321
Iteration 2/25 | Loss: 0.00157218
Iteration 3/25 | Loss: 0.00101590
Iteration 4/25 | Loss: 0.00098881
Iteration 5/25 | Loss: 0.00098054
Iteration 6/25 | Loss: 0.00097903
Iteration 7/25 | Loss: 0.00097880
Iteration 8/25 | Loss: 0.00097880
Iteration 9/25 | Loss: 0.00097880
Iteration 10/25 | Loss: 0.00097880
Iteration 11/25 | Loss: 0.00097880
Iteration 12/25 | Loss: 0.00097880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000978800468146801, 0.000978800468146801, 0.000978800468146801, 0.000978800468146801, 0.000978800468146801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000978800468146801

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.59554636
Iteration 2/25 | Loss: 0.00025970
Iteration 3/25 | Loss: 0.00025970
Iteration 4/25 | Loss: 0.00025970
Iteration 5/25 | Loss: 0.00025970
Iteration 6/25 | Loss: 0.00025970
Iteration 7/25 | Loss: 0.00025970
Iteration 8/25 | Loss: 0.00025970
Iteration 9/25 | Loss: 0.00025969
Iteration 10/25 | Loss: 0.00025969
Iteration 11/25 | Loss: 0.00025969
Iteration 12/25 | Loss: 0.00025969
Iteration 13/25 | Loss: 0.00025969
Iteration 14/25 | Loss: 0.00025969
Iteration 15/25 | Loss: 0.00025969
Iteration 16/25 | Loss: 0.00025969
Iteration 17/25 | Loss: 0.00025969
Iteration 18/25 | Loss: 0.00025969
Iteration 19/25 | Loss: 0.00025969
Iteration 20/25 | Loss: 0.00025969
Iteration 21/25 | Loss: 0.00025969
Iteration 22/25 | Loss: 0.00025969
Iteration 23/25 | Loss: 0.00025969
Iteration 24/25 | Loss: 0.00025969
Iteration 25/25 | Loss: 0.00025969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025969
Iteration 2/1000 | Loss: 0.00005815
Iteration 3/1000 | Loss: 0.00004596
Iteration 4/1000 | Loss: 0.00004077
Iteration 5/1000 | Loss: 0.00003871
Iteration 6/1000 | Loss: 0.00003781
Iteration 7/1000 | Loss: 0.00003714
Iteration 8/1000 | Loss: 0.00003646
Iteration 9/1000 | Loss: 0.00003599
Iteration 10/1000 | Loss: 0.00003566
Iteration 11/1000 | Loss: 0.00003543
Iteration 12/1000 | Loss: 0.00003516
Iteration 13/1000 | Loss: 0.00003494
Iteration 14/1000 | Loss: 0.00003466
Iteration 15/1000 | Loss: 0.00003443
Iteration 16/1000 | Loss: 0.00003425
Iteration 17/1000 | Loss: 0.00003408
Iteration 18/1000 | Loss: 0.00003393
Iteration 19/1000 | Loss: 0.00003389
Iteration 20/1000 | Loss: 0.00003385
Iteration 21/1000 | Loss: 0.00003381
Iteration 22/1000 | Loss: 0.00003379
Iteration 23/1000 | Loss: 0.00003374
Iteration 24/1000 | Loss: 0.00003374
Iteration 25/1000 | Loss: 0.00003372
Iteration 26/1000 | Loss: 0.00003372
Iteration 27/1000 | Loss: 0.00003372
Iteration 28/1000 | Loss: 0.00003372
Iteration 29/1000 | Loss: 0.00003372
Iteration 30/1000 | Loss: 0.00003372
Iteration 31/1000 | Loss: 0.00003371
Iteration 32/1000 | Loss: 0.00003371
Iteration 33/1000 | Loss: 0.00003371
Iteration 34/1000 | Loss: 0.00003371
Iteration 35/1000 | Loss: 0.00003371
Iteration 36/1000 | Loss: 0.00003370
Iteration 37/1000 | Loss: 0.00003369
Iteration 38/1000 | Loss: 0.00003368
Iteration 39/1000 | Loss: 0.00003368
Iteration 40/1000 | Loss: 0.00003368
Iteration 41/1000 | Loss: 0.00003367
Iteration 42/1000 | Loss: 0.00003367
Iteration 43/1000 | Loss: 0.00003366
Iteration 44/1000 | Loss: 0.00003366
Iteration 45/1000 | Loss: 0.00003365
Iteration 46/1000 | Loss: 0.00003365
Iteration 47/1000 | Loss: 0.00003365
Iteration 48/1000 | Loss: 0.00003365
Iteration 49/1000 | Loss: 0.00003365
Iteration 50/1000 | Loss: 0.00003364
Iteration 51/1000 | Loss: 0.00003364
Iteration 52/1000 | Loss: 0.00003363
Iteration 53/1000 | Loss: 0.00003363
Iteration 54/1000 | Loss: 0.00003363
Iteration 55/1000 | Loss: 0.00003362
Iteration 56/1000 | Loss: 0.00003362
Iteration 57/1000 | Loss: 0.00003362
Iteration 58/1000 | Loss: 0.00003362
Iteration 59/1000 | Loss: 0.00003362
Iteration 60/1000 | Loss: 0.00003361
Iteration 61/1000 | Loss: 0.00003361
Iteration 62/1000 | Loss: 0.00003361
Iteration 63/1000 | Loss: 0.00003360
Iteration 64/1000 | Loss: 0.00003360
Iteration 65/1000 | Loss: 0.00003360
Iteration 66/1000 | Loss: 0.00003360
Iteration 67/1000 | Loss: 0.00003360
Iteration 68/1000 | Loss: 0.00003360
Iteration 69/1000 | Loss: 0.00003360
Iteration 70/1000 | Loss: 0.00003359
Iteration 71/1000 | Loss: 0.00003359
Iteration 72/1000 | Loss: 0.00003359
Iteration 73/1000 | Loss: 0.00003359
Iteration 74/1000 | Loss: 0.00003359
Iteration 75/1000 | Loss: 0.00003359
Iteration 76/1000 | Loss: 0.00003359
Iteration 77/1000 | Loss: 0.00003359
Iteration 78/1000 | Loss: 0.00003358
Iteration 79/1000 | Loss: 0.00003358
Iteration 80/1000 | Loss: 0.00003358
Iteration 81/1000 | Loss: 0.00003358
Iteration 82/1000 | Loss: 0.00003358
Iteration 83/1000 | Loss: 0.00003358
Iteration 84/1000 | Loss: 0.00003358
Iteration 85/1000 | Loss: 0.00003358
Iteration 86/1000 | Loss: 0.00003357
Iteration 87/1000 | Loss: 0.00003357
Iteration 88/1000 | Loss: 0.00003357
Iteration 89/1000 | Loss: 0.00003356
Iteration 90/1000 | Loss: 0.00003356
Iteration 91/1000 | Loss: 0.00003356
Iteration 92/1000 | Loss: 0.00003356
Iteration 93/1000 | Loss: 0.00003356
Iteration 94/1000 | Loss: 0.00003355
Iteration 95/1000 | Loss: 0.00003355
Iteration 96/1000 | Loss: 0.00003355
Iteration 97/1000 | Loss: 0.00003355
Iteration 98/1000 | Loss: 0.00003355
Iteration 99/1000 | Loss: 0.00003355
Iteration 100/1000 | Loss: 0.00003355
Iteration 101/1000 | Loss: 0.00003355
Iteration 102/1000 | Loss: 0.00003355
Iteration 103/1000 | Loss: 0.00003355
Iteration 104/1000 | Loss: 0.00003354
Iteration 105/1000 | Loss: 0.00003354
Iteration 106/1000 | Loss: 0.00003354
Iteration 107/1000 | Loss: 0.00003353
Iteration 108/1000 | Loss: 0.00003353
Iteration 109/1000 | Loss: 0.00003353
Iteration 110/1000 | Loss: 0.00003353
Iteration 111/1000 | Loss: 0.00003353
Iteration 112/1000 | Loss: 0.00003353
Iteration 113/1000 | Loss: 0.00003352
Iteration 114/1000 | Loss: 0.00003352
Iteration 115/1000 | Loss: 0.00003352
Iteration 116/1000 | Loss: 0.00003352
Iteration 117/1000 | Loss: 0.00003352
Iteration 118/1000 | Loss: 0.00003352
Iteration 119/1000 | Loss: 0.00003352
Iteration 120/1000 | Loss: 0.00003352
Iteration 121/1000 | Loss: 0.00003351
Iteration 122/1000 | Loss: 0.00003351
Iteration 123/1000 | Loss: 0.00003351
Iteration 124/1000 | Loss: 0.00003351
Iteration 125/1000 | Loss: 0.00003351
Iteration 126/1000 | Loss: 0.00003351
Iteration 127/1000 | Loss: 0.00003350
Iteration 128/1000 | Loss: 0.00003350
Iteration 129/1000 | Loss: 0.00003350
Iteration 130/1000 | Loss: 0.00003350
Iteration 131/1000 | Loss: 0.00003350
Iteration 132/1000 | Loss: 0.00003350
Iteration 133/1000 | Loss: 0.00003350
Iteration 134/1000 | Loss: 0.00003350
Iteration 135/1000 | Loss: 0.00003349
Iteration 136/1000 | Loss: 0.00003349
Iteration 137/1000 | Loss: 0.00003349
Iteration 138/1000 | Loss: 0.00003349
Iteration 139/1000 | Loss: 0.00003349
Iteration 140/1000 | Loss: 0.00003349
Iteration 141/1000 | Loss: 0.00003349
Iteration 142/1000 | Loss: 0.00003349
Iteration 143/1000 | Loss: 0.00003349
Iteration 144/1000 | Loss: 0.00003349
Iteration 145/1000 | Loss: 0.00003348
Iteration 146/1000 | Loss: 0.00003348
Iteration 147/1000 | Loss: 0.00003348
Iteration 148/1000 | Loss: 0.00003348
Iteration 149/1000 | Loss: 0.00003348
Iteration 150/1000 | Loss: 0.00003348
Iteration 151/1000 | Loss: 0.00003348
Iteration 152/1000 | Loss: 0.00003348
Iteration 153/1000 | Loss: 0.00003348
Iteration 154/1000 | Loss: 0.00003348
Iteration 155/1000 | Loss: 0.00003347
Iteration 156/1000 | Loss: 0.00003347
Iteration 157/1000 | Loss: 0.00003347
Iteration 158/1000 | Loss: 0.00003346
Iteration 159/1000 | Loss: 0.00003346
Iteration 160/1000 | Loss: 0.00003346
Iteration 161/1000 | Loss: 0.00003346
Iteration 162/1000 | Loss: 0.00003346
Iteration 163/1000 | Loss: 0.00003346
Iteration 164/1000 | Loss: 0.00003346
Iteration 165/1000 | Loss: 0.00003346
Iteration 166/1000 | Loss: 0.00003346
Iteration 167/1000 | Loss: 0.00003346
Iteration 168/1000 | Loss: 0.00003345
Iteration 169/1000 | Loss: 0.00003345
Iteration 170/1000 | Loss: 0.00003345
Iteration 171/1000 | Loss: 0.00003345
Iteration 172/1000 | Loss: 0.00003345
Iteration 173/1000 | Loss: 0.00003345
Iteration 174/1000 | Loss: 0.00003345
Iteration 175/1000 | Loss: 0.00003345
Iteration 176/1000 | Loss: 0.00003345
Iteration 177/1000 | Loss: 0.00003344
Iteration 178/1000 | Loss: 0.00003344
Iteration 179/1000 | Loss: 0.00003344
Iteration 180/1000 | Loss: 0.00003344
Iteration 181/1000 | Loss: 0.00003344
Iteration 182/1000 | Loss: 0.00003344
Iteration 183/1000 | Loss: 0.00003344
Iteration 184/1000 | Loss: 0.00003344
Iteration 185/1000 | Loss: 0.00003343
Iteration 186/1000 | Loss: 0.00003343
Iteration 187/1000 | Loss: 0.00003343
Iteration 188/1000 | Loss: 0.00003343
Iteration 189/1000 | Loss: 0.00003343
Iteration 190/1000 | Loss: 0.00003343
Iteration 191/1000 | Loss: 0.00003343
Iteration 192/1000 | Loss: 0.00003342
Iteration 193/1000 | Loss: 0.00003342
Iteration 194/1000 | Loss: 0.00003342
Iteration 195/1000 | Loss: 0.00003342
Iteration 196/1000 | Loss: 0.00003342
Iteration 197/1000 | Loss: 0.00003342
Iteration 198/1000 | Loss: 0.00003342
Iteration 199/1000 | Loss: 0.00003342
Iteration 200/1000 | Loss: 0.00003342
Iteration 201/1000 | Loss: 0.00003342
Iteration 202/1000 | Loss: 0.00003342
Iteration 203/1000 | Loss: 0.00003342
Iteration 204/1000 | Loss: 0.00003342
Iteration 205/1000 | Loss: 0.00003342
Iteration 206/1000 | Loss: 0.00003342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [3.3422602427890524e-05, 3.3422602427890524e-05, 3.3422602427890524e-05, 3.3422602427890524e-05, 3.3422602427890524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3422602427890524e-05

Optimization complete. Final v2v error: 4.639333724975586 mm

Highest mean error: 5.743243217468262 mm for frame 164

Lowest mean error: 3.666869640350342 mm for frame 10

Saving results

Total time: 52.63553738594055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00365214
Iteration 2/25 | Loss: 0.00070870
Iteration 3/25 | Loss: 0.00060699
Iteration 4/25 | Loss: 0.00059161
Iteration 5/25 | Loss: 0.00058561
Iteration 6/25 | Loss: 0.00058415
Iteration 7/25 | Loss: 0.00058386
Iteration 8/25 | Loss: 0.00058386
Iteration 9/25 | Loss: 0.00058386
Iteration 10/25 | Loss: 0.00058386
Iteration 11/25 | Loss: 0.00058386
Iteration 12/25 | Loss: 0.00058386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005838556098751724, 0.0005838556098751724, 0.0005838556098751724, 0.0005838556098751724, 0.0005838556098751724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005838556098751724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.90527844
Iteration 2/25 | Loss: 0.00029322
Iteration 3/25 | Loss: 0.00029321
Iteration 4/25 | Loss: 0.00029321
Iteration 5/25 | Loss: 0.00029321
Iteration 6/25 | Loss: 0.00029321
Iteration 7/25 | Loss: 0.00029321
Iteration 8/25 | Loss: 0.00029321
Iteration 9/25 | Loss: 0.00029321
Iteration 10/25 | Loss: 0.00029321
Iteration 11/25 | Loss: 0.00029321
Iteration 12/25 | Loss: 0.00029321
Iteration 13/25 | Loss: 0.00029321
Iteration 14/25 | Loss: 0.00029321
Iteration 15/25 | Loss: 0.00029321
Iteration 16/25 | Loss: 0.00029321
Iteration 17/25 | Loss: 0.00029321
Iteration 18/25 | Loss: 0.00029321
Iteration 19/25 | Loss: 0.00029321
Iteration 20/25 | Loss: 0.00029321
Iteration 21/25 | Loss: 0.00029321
Iteration 22/25 | Loss: 0.00029321
Iteration 23/25 | Loss: 0.00029321
Iteration 24/25 | Loss: 0.00029321
Iteration 25/25 | Loss: 0.00029321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029321
Iteration 2/1000 | Loss: 0.00001830
Iteration 3/1000 | Loss: 0.00001309
Iteration 4/1000 | Loss: 0.00001221
Iteration 5/1000 | Loss: 0.00001186
Iteration 6/1000 | Loss: 0.00001171
Iteration 7/1000 | Loss: 0.00001144
Iteration 8/1000 | Loss: 0.00001137
Iteration 9/1000 | Loss: 0.00001133
Iteration 10/1000 | Loss: 0.00001125
Iteration 11/1000 | Loss: 0.00001122
Iteration 12/1000 | Loss: 0.00001119
Iteration 13/1000 | Loss: 0.00001119
Iteration 14/1000 | Loss: 0.00001118
Iteration 15/1000 | Loss: 0.00001112
Iteration 16/1000 | Loss: 0.00001111
Iteration 17/1000 | Loss: 0.00001111
Iteration 18/1000 | Loss: 0.00001110
Iteration 19/1000 | Loss: 0.00001110
Iteration 20/1000 | Loss: 0.00001110
Iteration 21/1000 | Loss: 0.00001109
Iteration 22/1000 | Loss: 0.00001109
Iteration 23/1000 | Loss: 0.00001108
Iteration 24/1000 | Loss: 0.00001108
Iteration 25/1000 | Loss: 0.00001107
Iteration 26/1000 | Loss: 0.00001102
Iteration 27/1000 | Loss: 0.00001101
Iteration 28/1000 | Loss: 0.00001100
Iteration 29/1000 | Loss: 0.00001100
Iteration 30/1000 | Loss: 0.00001098
Iteration 31/1000 | Loss: 0.00001098
Iteration 32/1000 | Loss: 0.00001097
Iteration 33/1000 | Loss: 0.00001096
Iteration 34/1000 | Loss: 0.00001096
Iteration 35/1000 | Loss: 0.00001095
Iteration 36/1000 | Loss: 0.00001095
Iteration 37/1000 | Loss: 0.00001094
Iteration 38/1000 | Loss: 0.00001089
Iteration 39/1000 | Loss: 0.00001088
Iteration 40/1000 | Loss: 0.00001088
Iteration 41/1000 | Loss: 0.00001088
Iteration 42/1000 | Loss: 0.00001087
Iteration 43/1000 | Loss: 0.00001087
Iteration 44/1000 | Loss: 0.00001087
Iteration 45/1000 | Loss: 0.00001086
Iteration 46/1000 | Loss: 0.00001086
Iteration 47/1000 | Loss: 0.00001085
Iteration 48/1000 | Loss: 0.00001085
Iteration 49/1000 | Loss: 0.00001085
Iteration 50/1000 | Loss: 0.00001085
Iteration 51/1000 | Loss: 0.00001084
Iteration 52/1000 | Loss: 0.00001084
Iteration 53/1000 | Loss: 0.00001084
Iteration 54/1000 | Loss: 0.00001084
Iteration 55/1000 | Loss: 0.00001084
Iteration 56/1000 | Loss: 0.00001084
Iteration 57/1000 | Loss: 0.00001083
Iteration 58/1000 | Loss: 0.00001083
Iteration 59/1000 | Loss: 0.00001083
Iteration 60/1000 | Loss: 0.00001083
Iteration 61/1000 | Loss: 0.00001083
Iteration 62/1000 | Loss: 0.00001083
Iteration 63/1000 | Loss: 0.00001083
Iteration 64/1000 | Loss: 0.00001083
Iteration 65/1000 | Loss: 0.00001082
Iteration 66/1000 | Loss: 0.00001082
Iteration 67/1000 | Loss: 0.00001082
Iteration 68/1000 | Loss: 0.00001082
Iteration 69/1000 | Loss: 0.00001082
Iteration 70/1000 | Loss: 0.00001082
Iteration 71/1000 | Loss: 0.00001082
Iteration 72/1000 | Loss: 0.00001082
Iteration 73/1000 | Loss: 0.00001081
Iteration 74/1000 | Loss: 0.00001081
Iteration 75/1000 | Loss: 0.00001081
Iteration 76/1000 | Loss: 0.00001080
Iteration 77/1000 | Loss: 0.00001080
Iteration 78/1000 | Loss: 0.00001080
Iteration 79/1000 | Loss: 0.00001079
Iteration 80/1000 | Loss: 0.00001078
Iteration 81/1000 | Loss: 0.00001078
Iteration 82/1000 | Loss: 0.00001077
Iteration 83/1000 | Loss: 0.00001076
Iteration 84/1000 | Loss: 0.00001076
Iteration 85/1000 | Loss: 0.00001075
Iteration 86/1000 | Loss: 0.00001075
Iteration 87/1000 | Loss: 0.00001075
Iteration 88/1000 | Loss: 0.00001075
Iteration 89/1000 | Loss: 0.00001075
Iteration 90/1000 | Loss: 0.00001075
Iteration 91/1000 | Loss: 0.00001074
Iteration 92/1000 | Loss: 0.00001074
Iteration 93/1000 | Loss: 0.00001074
Iteration 94/1000 | Loss: 0.00001074
Iteration 95/1000 | Loss: 0.00001074
Iteration 96/1000 | Loss: 0.00001074
Iteration 97/1000 | Loss: 0.00001074
Iteration 98/1000 | Loss: 0.00001073
Iteration 99/1000 | Loss: 0.00001073
Iteration 100/1000 | Loss: 0.00001073
Iteration 101/1000 | Loss: 0.00001072
Iteration 102/1000 | Loss: 0.00001072
Iteration 103/1000 | Loss: 0.00001072
Iteration 104/1000 | Loss: 0.00001072
Iteration 105/1000 | Loss: 0.00001071
Iteration 106/1000 | Loss: 0.00001071
Iteration 107/1000 | Loss: 0.00001071
Iteration 108/1000 | Loss: 0.00001071
Iteration 109/1000 | Loss: 0.00001071
Iteration 110/1000 | Loss: 0.00001071
Iteration 111/1000 | Loss: 0.00001071
Iteration 112/1000 | Loss: 0.00001071
Iteration 113/1000 | Loss: 0.00001071
Iteration 114/1000 | Loss: 0.00001071
Iteration 115/1000 | Loss: 0.00001071
Iteration 116/1000 | Loss: 0.00001071
Iteration 117/1000 | Loss: 0.00001071
Iteration 118/1000 | Loss: 0.00001071
Iteration 119/1000 | Loss: 0.00001071
Iteration 120/1000 | Loss: 0.00001071
Iteration 121/1000 | Loss: 0.00001071
Iteration 122/1000 | Loss: 0.00001070
Iteration 123/1000 | Loss: 0.00001070
Iteration 124/1000 | Loss: 0.00001070
Iteration 125/1000 | Loss: 0.00001070
Iteration 126/1000 | Loss: 0.00001070
Iteration 127/1000 | Loss: 0.00001070
Iteration 128/1000 | Loss: 0.00001070
Iteration 129/1000 | Loss: 0.00001070
Iteration 130/1000 | Loss: 0.00001070
Iteration 131/1000 | Loss: 0.00001070
Iteration 132/1000 | Loss: 0.00001070
Iteration 133/1000 | Loss: 0.00001070
Iteration 134/1000 | Loss: 0.00001070
Iteration 135/1000 | Loss: 0.00001070
Iteration 136/1000 | Loss: 0.00001070
Iteration 137/1000 | Loss: 0.00001069
Iteration 138/1000 | Loss: 0.00001069
Iteration 139/1000 | Loss: 0.00001069
Iteration 140/1000 | Loss: 0.00001069
Iteration 141/1000 | Loss: 0.00001069
Iteration 142/1000 | Loss: 0.00001069
Iteration 143/1000 | Loss: 0.00001069
Iteration 144/1000 | Loss: 0.00001069
Iteration 145/1000 | Loss: 0.00001069
Iteration 146/1000 | Loss: 0.00001069
Iteration 147/1000 | Loss: 0.00001069
Iteration 148/1000 | Loss: 0.00001069
Iteration 149/1000 | Loss: 0.00001069
Iteration 150/1000 | Loss: 0.00001069
Iteration 151/1000 | Loss: 0.00001069
Iteration 152/1000 | Loss: 0.00001069
Iteration 153/1000 | Loss: 0.00001069
Iteration 154/1000 | Loss: 0.00001069
Iteration 155/1000 | Loss: 0.00001069
Iteration 156/1000 | Loss: 0.00001069
Iteration 157/1000 | Loss: 0.00001069
Iteration 158/1000 | Loss: 0.00001069
Iteration 159/1000 | Loss: 0.00001069
Iteration 160/1000 | Loss: 0.00001069
Iteration 161/1000 | Loss: 0.00001069
Iteration 162/1000 | Loss: 0.00001069
Iteration 163/1000 | Loss: 0.00001069
Iteration 164/1000 | Loss: 0.00001069
Iteration 165/1000 | Loss: 0.00001069
Iteration 166/1000 | Loss: 0.00001069
Iteration 167/1000 | Loss: 0.00001069
Iteration 168/1000 | Loss: 0.00001069
Iteration 169/1000 | Loss: 0.00001069
Iteration 170/1000 | Loss: 0.00001069
Iteration 171/1000 | Loss: 0.00001069
Iteration 172/1000 | Loss: 0.00001069
Iteration 173/1000 | Loss: 0.00001069
Iteration 174/1000 | Loss: 0.00001069
Iteration 175/1000 | Loss: 0.00001069
Iteration 176/1000 | Loss: 0.00001069
Iteration 177/1000 | Loss: 0.00001069
Iteration 178/1000 | Loss: 0.00001069
Iteration 179/1000 | Loss: 0.00001069
Iteration 180/1000 | Loss: 0.00001069
Iteration 181/1000 | Loss: 0.00001069
Iteration 182/1000 | Loss: 0.00001069
Iteration 183/1000 | Loss: 0.00001069
Iteration 184/1000 | Loss: 0.00001069
Iteration 185/1000 | Loss: 0.00001069
Iteration 186/1000 | Loss: 0.00001069
Iteration 187/1000 | Loss: 0.00001069
Iteration 188/1000 | Loss: 0.00001069
Iteration 189/1000 | Loss: 0.00001069
Iteration 190/1000 | Loss: 0.00001069
Iteration 191/1000 | Loss: 0.00001069
Iteration 192/1000 | Loss: 0.00001069
Iteration 193/1000 | Loss: 0.00001069
Iteration 194/1000 | Loss: 0.00001069
Iteration 195/1000 | Loss: 0.00001069
Iteration 196/1000 | Loss: 0.00001069
Iteration 197/1000 | Loss: 0.00001069
Iteration 198/1000 | Loss: 0.00001069
Iteration 199/1000 | Loss: 0.00001069
Iteration 200/1000 | Loss: 0.00001069
Iteration 201/1000 | Loss: 0.00001069
Iteration 202/1000 | Loss: 0.00001069
Iteration 203/1000 | Loss: 0.00001069
Iteration 204/1000 | Loss: 0.00001069
Iteration 205/1000 | Loss: 0.00001069
Iteration 206/1000 | Loss: 0.00001069
Iteration 207/1000 | Loss: 0.00001069
Iteration 208/1000 | Loss: 0.00001069
Iteration 209/1000 | Loss: 0.00001069
Iteration 210/1000 | Loss: 0.00001069
Iteration 211/1000 | Loss: 0.00001069
Iteration 212/1000 | Loss: 0.00001069
Iteration 213/1000 | Loss: 0.00001069
Iteration 214/1000 | Loss: 0.00001069
Iteration 215/1000 | Loss: 0.00001069
Iteration 216/1000 | Loss: 0.00001069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.0691465831769165e-05, 1.0691465831769165e-05, 1.0691465831769165e-05, 1.0691465831769165e-05, 1.0691465831769165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0691465831769165e-05

Optimization complete. Final v2v error: 2.7869863510131836 mm

Highest mean error: 2.9274954795837402 mm for frame 130

Lowest mean error: 2.696525812149048 mm for frame 109

Saving results

Total time: 35.719353437423706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039762
Iteration 2/25 | Loss: 0.00206857
Iteration 3/25 | Loss: 0.00143493
Iteration 4/25 | Loss: 0.00130839
Iteration 5/25 | Loss: 0.00127058
Iteration 6/25 | Loss: 0.00121549
Iteration 7/25 | Loss: 0.00120145
Iteration 8/25 | Loss: 0.00115786
Iteration 9/25 | Loss: 0.00113724
Iteration 10/25 | Loss: 0.00113683
Iteration 11/25 | Loss: 0.00113109
Iteration 12/25 | Loss: 0.00112378
Iteration 13/25 | Loss: 0.00111557
Iteration 14/25 | Loss: 0.00110498
Iteration 15/25 | Loss: 0.00110100
Iteration 16/25 | Loss: 0.00109969
Iteration 17/25 | Loss: 0.00110292
Iteration 18/25 | Loss: 0.00109787
Iteration 19/25 | Loss: 0.00109668
Iteration 20/25 | Loss: 0.00109624
Iteration 21/25 | Loss: 0.00109566
Iteration 22/25 | Loss: 0.00109935
Iteration 23/25 | Loss: 0.00109462
Iteration 24/25 | Loss: 0.00109320
Iteration 25/25 | Loss: 0.00109293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43115747
Iteration 2/25 | Loss: 0.00334396
Iteration 3/25 | Loss: 0.00334396
Iteration 4/25 | Loss: 0.00334396
Iteration 5/25 | Loss: 0.00334396
Iteration 6/25 | Loss: 0.00334396
Iteration 7/25 | Loss: 0.00334396
Iteration 8/25 | Loss: 0.00334396
Iteration 9/25 | Loss: 0.00334396
Iteration 10/25 | Loss: 0.00334396
Iteration 11/25 | Loss: 0.00334396
Iteration 12/25 | Loss: 0.00334396
Iteration 13/25 | Loss: 0.00334396
Iteration 14/25 | Loss: 0.00334396
Iteration 15/25 | Loss: 0.00334396
Iteration 16/25 | Loss: 0.00334396
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003343955148011446, 0.003343955148011446, 0.003343955148011446, 0.003343955148011446, 0.003343955148011446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003343955148011446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00334396
Iteration 2/1000 | Loss: 0.00139434
Iteration 3/1000 | Loss: 0.00133673
Iteration 4/1000 | Loss: 0.00057642
Iteration 5/1000 | Loss: 0.00053813
Iteration 6/1000 | Loss: 0.00035567
Iteration 7/1000 | Loss: 0.00054006
Iteration 8/1000 | Loss: 0.00023959
Iteration 9/1000 | Loss: 0.00019199
Iteration 10/1000 | Loss: 0.00016874
Iteration 11/1000 | Loss: 0.00014726
Iteration 12/1000 | Loss: 0.00016184
Iteration 13/1000 | Loss: 0.00012870
Iteration 14/1000 | Loss: 0.00012142
Iteration 15/1000 | Loss: 0.00012586
Iteration 16/1000 | Loss: 0.00018955
Iteration 17/1000 | Loss: 0.00048221
Iteration 18/1000 | Loss: 0.00700734
Iteration 19/1000 | Loss: 0.00238739
Iteration 20/1000 | Loss: 0.00193893
Iteration 21/1000 | Loss: 0.00022760
Iteration 22/1000 | Loss: 0.00054836
Iteration 23/1000 | Loss: 0.00012144
Iteration 24/1000 | Loss: 0.00008484
Iteration 25/1000 | Loss: 0.00005423
Iteration 26/1000 | Loss: 0.00006968
Iteration 27/1000 | Loss: 0.00005592
Iteration 28/1000 | Loss: 0.00002918
Iteration 29/1000 | Loss: 0.00002595
Iteration 30/1000 | Loss: 0.00002218
Iteration 31/1000 | Loss: 0.00003379
Iteration 32/1000 | Loss: 0.00003583
Iteration 33/1000 | Loss: 0.00001762
Iteration 34/1000 | Loss: 0.00001663
Iteration 35/1000 | Loss: 0.00001571
Iteration 36/1000 | Loss: 0.00001502
Iteration 37/1000 | Loss: 0.00001445
Iteration 38/1000 | Loss: 0.00001408
Iteration 39/1000 | Loss: 0.00001383
Iteration 40/1000 | Loss: 0.00001375
Iteration 41/1000 | Loss: 0.00001369
Iteration 42/1000 | Loss: 0.00001369
Iteration 43/1000 | Loss: 0.00001358
Iteration 44/1000 | Loss: 0.00001358
Iteration 45/1000 | Loss: 0.00001358
Iteration 46/1000 | Loss: 0.00001358
Iteration 47/1000 | Loss: 0.00001358
Iteration 48/1000 | Loss: 0.00001358
Iteration 49/1000 | Loss: 0.00001358
Iteration 50/1000 | Loss: 0.00001358
Iteration 51/1000 | Loss: 0.00001358
Iteration 52/1000 | Loss: 0.00001355
Iteration 53/1000 | Loss: 0.00001355
Iteration 54/1000 | Loss: 0.00001354
Iteration 55/1000 | Loss: 0.00001354
Iteration 56/1000 | Loss: 0.00001354
Iteration 57/1000 | Loss: 0.00001353
Iteration 58/1000 | Loss: 0.00001353
Iteration 59/1000 | Loss: 0.00001353
Iteration 60/1000 | Loss: 0.00001352
Iteration 61/1000 | Loss: 0.00001352
Iteration 62/1000 | Loss: 0.00001352
Iteration 63/1000 | Loss: 0.00001352
Iteration 64/1000 | Loss: 0.00001352
Iteration 65/1000 | Loss: 0.00001352
Iteration 66/1000 | Loss: 0.00001351
Iteration 67/1000 | Loss: 0.00001351
Iteration 68/1000 | Loss: 0.00001351
Iteration 69/1000 | Loss: 0.00001351
Iteration 70/1000 | Loss: 0.00001351
Iteration 71/1000 | Loss: 0.00001350
Iteration 72/1000 | Loss: 0.00001350
Iteration 73/1000 | Loss: 0.00001350
Iteration 74/1000 | Loss: 0.00001350
Iteration 75/1000 | Loss: 0.00001350
Iteration 76/1000 | Loss: 0.00001350
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00001349
Iteration 80/1000 | Loss: 0.00001349
Iteration 81/1000 | Loss: 0.00001349
Iteration 82/1000 | Loss: 0.00001349
Iteration 83/1000 | Loss: 0.00001349
Iteration 84/1000 | Loss: 0.00001349
Iteration 85/1000 | Loss: 0.00001348
Iteration 86/1000 | Loss: 0.00001348
Iteration 87/1000 | Loss: 0.00001348
Iteration 88/1000 | Loss: 0.00001348
Iteration 89/1000 | Loss: 0.00001348
Iteration 90/1000 | Loss: 0.00001348
Iteration 91/1000 | Loss: 0.00001348
Iteration 92/1000 | Loss: 0.00001348
Iteration 93/1000 | Loss: 0.00001348
Iteration 94/1000 | Loss: 0.00001348
Iteration 95/1000 | Loss: 0.00001348
Iteration 96/1000 | Loss: 0.00001347
Iteration 97/1000 | Loss: 0.00001347
Iteration 98/1000 | Loss: 0.00001347
Iteration 99/1000 | Loss: 0.00001347
Iteration 100/1000 | Loss: 0.00001347
Iteration 101/1000 | Loss: 0.00001347
Iteration 102/1000 | Loss: 0.00001346
Iteration 103/1000 | Loss: 0.00001346
Iteration 104/1000 | Loss: 0.00001346
Iteration 105/1000 | Loss: 0.00001346
Iteration 106/1000 | Loss: 0.00001345
Iteration 107/1000 | Loss: 0.00001345
Iteration 108/1000 | Loss: 0.00001345
Iteration 109/1000 | Loss: 0.00001345
Iteration 110/1000 | Loss: 0.00001344
Iteration 111/1000 | Loss: 0.00001343
Iteration 112/1000 | Loss: 0.00001343
Iteration 113/1000 | Loss: 0.00001343
Iteration 114/1000 | Loss: 0.00001342
Iteration 115/1000 | Loss: 0.00001342
Iteration 116/1000 | Loss: 0.00001342
Iteration 117/1000 | Loss: 0.00001342
Iteration 118/1000 | Loss: 0.00001342
Iteration 119/1000 | Loss: 0.00001342
Iteration 120/1000 | Loss: 0.00001342
Iteration 121/1000 | Loss: 0.00001342
Iteration 122/1000 | Loss: 0.00001341
Iteration 123/1000 | Loss: 0.00001341
Iteration 124/1000 | Loss: 0.00001341
Iteration 125/1000 | Loss: 0.00001340
Iteration 126/1000 | Loss: 0.00001340
Iteration 127/1000 | Loss: 0.00001340
Iteration 128/1000 | Loss: 0.00001339
Iteration 129/1000 | Loss: 0.00001339
Iteration 130/1000 | Loss: 0.00001339
Iteration 131/1000 | Loss: 0.00001339
Iteration 132/1000 | Loss: 0.00001339
Iteration 133/1000 | Loss: 0.00001339
Iteration 134/1000 | Loss: 0.00001339
Iteration 135/1000 | Loss: 0.00001339
Iteration 136/1000 | Loss: 0.00001339
Iteration 137/1000 | Loss: 0.00001339
Iteration 138/1000 | Loss: 0.00001339
Iteration 139/1000 | Loss: 0.00001339
Iteration 140/1000 | Loss: 0.00001339
Iteration 141/1000 | Loss: 0.00001338
Iteration 142/1000 | Loss: 0.00001338
Iteration 143/1000 | Loss: 0.00001338
Iteration 144/1000 | Loss: 0.00001338
Iteration 145/1000 | Loss: 0.00001338
Iteration 146/1000 | Loss: 0.00001338
Iteration 147/1000 | Loss: 0.00001338
Iteration 148/1000 | Loss: 0.00001338
Iteration 149/1000 | Loss: 0.00001338
Iteration 150/1000 | Loss: 0.00001338
Iteration 151/1000 | Loss: 0.00001338
Iteration 152/1000 | Loss: 0.00001338
Iteration 153/1000 | Loss: 0.00001338
Iteration 154/1000 | Loss: 0.00001338
Iteration 155/1000 | Loss: 0.00001338
Iteration 156/1000 | Loss: 0.00001338
Iteration 157/1000 | Loss: 0.00001338
Iteration 158/1000 | Loss: 0.00001338
Iteration 159/1000 | Loss: 0.00001338
Iteration 160/1000 | Loss: 0.00001338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.3376197784964461e-05, 1.3376197784964461e-05, 1.3376197784964461e-05, 1.3376197784964461e-05, 1.3376197784964461e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3376197784964461e-05

Optimization complete. Final v2v error: 3.10837459564209 mm

Highest mean error: 3.4445977210998535 mm for frame 11

Lowest mean error: 2.900071144104004 mm for frame 47

Saving results

Total time: 124.38451099395752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812137
Iteration 2/25 | Loss: 0.00101469
Iteration 3/25 | Loss: 0.00075001
Iteration 4/25 | Loss: 0.00068269
Iteration 5/25 | Loss: 0.00067585
Iteration 6/25 | Loss: 0.00066972
Iteration 7/25 | Loss: 0.00066797
Iteration 8/25 | Loss: 0.00067117
Iteration 9/25 | Loss: 0.00068620
Iteration 10/25 | Loss: 0.00066638
Iteration 11/25 | Loss: 0.00065863
Iteration 12/25 | Loss: 0.00065672
Iteration 13/25 | Loss: 0.00065609
Iteration 14/25 | Loss: 0.00065518
Iteration 15/25 | Loss: 0.00065479
Iteration 16/25 | Loss: 0.00065451
Iteration 17/25 | Loss: 0.00065440
Iteration 18/25 | Loss: 0.00065433
Iteration 19/25 | Loss: 0.00065433
Iteration 20/25 | Loss: 0.00065433
Iteration 21/25 | Loss: 0.00065432
Iteration 22/25 | Loss: 0.00065432
Iteration 23/25 | Loss: 0.00065432
Iteration 24/25 | Loss: 0.00065432
Iteration 25/25 | Loss: 0.00065432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.98397923
Iteration 2/25 | Loss: 0.00032118
Iteration 3/25 | Loss: 0.00032110
Iteration 4/25 | Loss: 0.00032110
Iteration 5/25 | Loss: 0.00032109
Iteration 6/25 | Loss: 0.00032109
Iteration 7/25 | Loss: 0.00032109
Iteration 8/25 | Loss: 0.00032109
Iteration 9/25 | Loss: 0.00032109
Iteration 10/25 | Loss: 0.00032109
Iteration 11/25 | Loss: 0.00032109
Iteration 12/25 | Loss: 0.00032109
Iteration 13/25 | Loss: 0.00032109
Iteration 14/25 | Loss: 0.00032109
Iteration 15/25 | Loss: 0.00032109
Iteration 16/25 | Loss: 0.00032109
Iteration 17/25 | Loss: 0.00032109
Iteration 18/25 | Loss: 0.00032109
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003210925788152963, 0.0003210925788152963, 0.0003210925788152963, 0.0003210925788152963, 0.0003210925788152963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003210925788152963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032109
Iteration 2/1000 | Loss: 0.00002478
Iteration 3/1000 | Loss: 0.00001760
Iteration 4/1000 | Loss: 0.00001668
Iteration 5/1000 | Loss: 0.00001602
Iteration 6/1000 | Loss: 0.00001565
Iteration 7/1000 | Loss: 0.00001535
Iteration 8/1000 | Loss: 0.00001511
Iteration 9/1000 | Loss: 0.00001501
Iteration 10/1000 | Loss: 0.00001498
Iteration 11/1000 | Loss: 0.00001497
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001494
Iteration 14/1000 | Loss: 0.00001488
Iteration 15/1000 | Loss: 0.00001487
Iteration 16/1000 | Loss: 0.00001475
Iteration 17/1000 | Loss: 0.00001475
Iteration 18/1000 | Loss: 0.00001474
Iteration 19/1000 | Loss: 0.00001474
Iteration 20/1000 | Loss: 0.00001473
Iteration 21/1000 | Loss: 0.00001473
Iteration 22/1000 | Loss: 0.00001469
Iteration 23/1000 | Loss: 0.00001468
Iteration 24/1000 | Loss: 0.00001468
Iteration 25/1000 | Loss: 0.00001466
Iteration 26/1000 | Loss: 0.00001465
Iteration 27/1000 | Loss: 0.00001464
Iteration 28/1000 | Loss: 0.00001463
Iteration 29/1000 | Loss: 0.00001463
Iteration 30/1000 | Loss: 0.00001463
Iteration 31/1000 | Loss: 0.00001463
Iteration 32/1000 | Loss: 0.00001463
Iteration 33/1000 | Loss: 0.00001463
Iteration 34/1000 | Loss: 0.00001463
Iteration 35/1000 | Loss: 0.00001463
Iteration 36/1000 | Loss: 0.00001463
Iteration 37/1000 | Loss: 0.00001463
Iteration 38/1000 | Loss: 0.00001462
Iteration 39/1000 | Loss: 0.00001461
Iteration 40/1000 | Loss: 0.00001461
Iteration 41/1000 | Loss: 0.00001461
Iteration 42/1000 | Loss: 0.00001460
Iteration 43/1000 | Loss: 0.00001460
Iteration 44/1000 | Loss: 0.00001460
Iteration 45/1000 | Loss: 0.00001459
Iteration 46/1000 | Loss: 0.00001459
Iteration 47/1000 | Loss: 0.00001459
Iteration 48/1000 | Loss: 0.00001458
Iteration 49/1000 | Loss: 0.00001458
Iteration 50/1000 | Loss: 0.00001457
Iteration 51/1000 | Loss: 0.00001457
Iteration 52/1000 | Loss: 0.00001457
Iteration 53/1000 | Loss: 0.00001457
Iteration 54/1000 | Loss: 0.00001457
Iteration 55/1000 | Loss: 0.00001457
Iteration 56/1000 | Loss: 0.00001456
Iteration 57/1000 | Loss: 0.00001456
Iteration 58/1000 | Loss: 0.00001456
Iteration 59/1000 | Loss: 0.00001455
Iteration 60/1000 | Loss: 0.00001455
Iteration 61/1000 | Loss: 0.00001454
Iteration 62/1000 | Loss: 0.00001454
Iteration 63/1000 | Loss: 0.00001454
Iteration 64/1000 | Loss: 0.00001454
Iteration 65/1000 | Loss: 0.00001454
Iteration 66/1000 | Loss: 0.00001454
Iteration 67/1000 | Loss: 0.00001454
Iteration 68/1000 | Loss: 0.00001454
Iteration 69/1000 | Loss: 0.00001454
Iteration 70/1000 | Loss: 0.00001453
Iteration 71/1000 | Loss: 0.00001453
Iteration 72/1000 | Loss: 0.00001452
Iteration 73/1000 | Loss: 0.00001452
Iteration 74/1000 | Loss: 0.00001452
Iteration 75/1000 | Loss: 0.00001452
Iteration 76/1000 | Loss: 0.00001451
Iteration 77/1000 | Loss: 0.00001451
Iteration 78/1000 | Loss: 0.00001451
Iteration 79/1000 | Loss: 0.00001451
Iteration 80/1000 | Loss: 0.00001451
Iteration 81/1000 | Loss: 0.00001451
Iteration 82/1000 | Loss: 0.00001451
Iteration 83/1000 | Loss: 0.00001451
Iteration 84/1000 | Loss: 0.00001451
Iteration 85/1000 | Loss: 0.00001450
Iteration 86/1000 | Loss: 0.00001450
Iteration 87/1000 | Loss: 0.00001450
Iteration 88/1000 | Loss: 0.00001449
Iteration 89/1000 | Loss: 0.00001449
Iteration 90/1000 | Loss: 0.00001449
Iteration 91/1000 | Loss: 0.00001449
Iteration 92/1000 | Loss: 0.00001449
Iteration 93/1000 | Loss: 0.00001449
Iteration 94/1000 | Loss: 0.00001448
Iteration 95/1000 | Loss: 0.00001448
Iteration 96/1000 | Loss: 0.00001448
Iteration 97/1000 | Loss: 0.00001447
Iteration 98/1000 | Loss: 0.00001447
Iteration 99/1000 | Loss: 0.00001447
Iteration 100/1000 | Loss: 0.00001446
Iteration 101/1000 | Loss: 0.00001446
Iteration 102/1000 | Loss: 0.00001446
Iteration 103/1000 | Loss: 0.00001446
Iteration 104/1000 | Loss: 0.00001446
Iteration 105/1000 | Loss: 0.00001446
Iteration 106/1000 | Loss: 0.00001446
Iteration 107/1000 | Loss: 0.00001446
Iteration 108/1000 | Loss: 0.00001446
Iteration 109/1000 | Loss: 0.00001445
Iteration 110/1000 | Loss: 0.00001445
Iteration 111/1000 | Loss: 0.00001444
Iteration 112/1000 | Loss: 0.00001444
Iteration 113/1000 | Loss: 0.00001444
Iteration 114/1000 | Loss: 0.00001443
Iteration 115/1000 | Loss: 0.00001443
Iteration 116/1000 | Loss: 0.00001443
Iteration 117/1000 | Loss: 0.00001443
Iteration 118/1000 | Loss: 0.00001443
Iteration 119/1000 | Loss: 0.00001443
Iteration 120/1000 | Loss: 0.00001443
Iteration 121/1000 | Loss: 0.00001443
Iteration 122/1000 | Loss: 0.00001443
Iteration 123/1000 | Loss: 0.00001443
Iteration 124/1000 | Loss: 0.00001443
Iteration 125/1000 | Loss: 0.00001442
Iteration 126/1000 | Loss: 0.00001442
Iteration 127/1000 | Loss: 0.00001442
Iteration 128/1000 | Loss: 0.00001442
Iteration 129/1000 | Loss: 0.00001442
Iteration 130/1000 | Loss: 0.00001442
Iteration 131/1000 | Loss: 0.00001441
Iteration 132/1000 | Loss: 0.00001441
Iteration 133/1000 | Loss: 0.00001441
Iteration 134/1000 | Loss: 0.00001441
Iteration 135/1000 | Loss: 0.00001441
Iteration 136/1000 | Loss: 0.00001441
Iteration 137/1000 | Loss: 0.00001441
Iteration 138/1000 | Loss: 0.00001441
Iteration 139/1000 | Loss: 0.00001441
Iteration 140/1000 | Loss: 0.00001441
Iteration 141/1000 | Loss: 0.00001441
Iteration 142/1000 | Loss: 0.00001441
Iteration 143/1000 | Loss: 0.00001441
Iteration 144/1000 | Loss: 0.00001441
Iteration 145/1000 | Loss: 0.00001441
Iteration 146/1000 | Loss: 0.00001441
Iteration 147/1000 | Loss: 0.00001441
Iteration 148/1000 | Loss: 0.00001441
Iteration 149/1000 | Loss: 0.00001441
Iteration 150/1000 | Loss: 0.00001441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.4408194147108588e-05, 1.4408194147108588e-05, 1.4408194147108588e-05, 1.4408194147108588e-05, 1.4408194147108588e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4408194147108588e-05

Optimization complete. Final v2v error: 3.224759578704834 mm

Highest mean error: 3.6955010890960693 mm for frame 165

Lowest mean error: 2.786638021469116 mm for frame 120

Saving results

Total time: 60.06488037109375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878906
Iteration 2/25 | Loss: 0.00124972
Iteration 3/25 | Loss: 0.00090465
Iteration 4/25 | Loss: 0.00083223
Iteration 5/25 | Loss: 0.00081475
Iteration 6/25 | Loss: 0.00081572
Iteration 7/25 | Loss: 0.00080250
Iteration 8/25 | Loss: 0.00079157
Iteration 9/25 | Loss: 0.00074542
Iteration 10/25 | Loss: 0.00073429
Iteration 11/25 | Loss: 0.00073378
Iteration 12/25 | Loss: 0.00073374
Iteration 13/25 | Loss: 0.00073374
Iteration 14/25 | Loss: 0.00073374
Iteration 15/25 | Loss: 0.00073374
Iteration 16/25 | Loss: 0.00073374
Iteration 17/25 | Loss: 0.00073374
Iteration 18/25 | Loss: 0.00073374
Iteration 19/25 | Loss: 0.00073374
Iteration 20/25 | Loss: 0.00073373
Iteration 21/25 | Loss: 0.00073373
Iteration 22/25 | Loss: 0.00073373
Iteration 23/25 | Loss: 0.00073373
Iteration 24/25 | Loss: 0.00073373
Iteration 25/25 | Loss: 0.00073373

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99858278
Iteration 2/25 | Loss: 0.00025936
Iteration 3/25 | Loss: 0.00025935
Iteration 4/25 | Loss: 0.00025935
Iteration 5/25 | Loss: 0.00025935
Iteration 6/25 | Loss: 0.00025935
Iteration 7/25 | Loss: 0.00025935
Iteration 8/25 | Loss: 0.00025935
Iteration 9/25 | Loss: 0.00025935
Iteration 10/25 | Loss: 0.00025935
Iteration 11/25 | Loss: 0.00025935
Iteration 12/25 | Loss: 0.00025935
Iteration 13/25 | Loss: 0.00025935
Iteration 14/25 | Loss: 0.00025935
Iteration 15/25 | Loss: 0.00025935
Iteration 16/25 | Loss: 0.00025935
Iteration 17/25 | Loss: 0.00025935
Iteration 18/25 | Loss: 0.00025935
Iteration 19/25 | Loss: 0.00025935
Iteration 20/25 | Loss: 0.00025935
Iteration 21/25 | Loss: 0.00025935
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00025934932637028396, 0.00025934932637028396, 0.00025934932637028396, 0.00025934932637028396, 0.00025934932637028396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025934932637028396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025935
Iteration 2/1000 | Loss: 0.00003789
Iteration 3/1000 | Loss: 0.00003013
Iteration 4/1000 | Loss: 0.00002726
Iteration 5/1000 | Loss: 0.00002579
Iteration 6/1000 | Loss: 0.00002477
Iteration 7/1000 | Loss: 0.00002386
Iteration 8/1000 | Loss: 0.00002321
Iteration 9/1000 | Loss: 0.00002284
Iteration 10/1000 | Loss: 0.00002268
Iteration 11/1000 | Loss: 0.00002258
Iteration 12/1000 | Loss: 0.00002255
Iteration 13/1000 | Loss: 0.00002255
Iteration 14/1000 | Loss: 0.00002254
Iteration 15/1000 | Loss: 0.00002248
Iteration 16/1000 | Loss: 0.00002248
Iteration 17/1000 | Loss: 0.00002247
Iteration 18/1000 | Loss: 0.00002246
Iteration 19/1000 | Loss: 0.00002246
Iteration 20/1000 | Loss: 0.00002245
Iteration 21/1000 | Loss: 0.00002245
Iteration 22/1000 | Loss: 0.00002245
Iteration 23/1000 | Loss: 0.00002244
Iteration 24/1000 | Loss: 0.00002244
Iteration 25/1000 | Loss: 0.00002244
Iteration 26/1000 | Loss: 0.00002244
Iteration 27/1000 | Loss: 0.00002243
Iteration 28/1000 | Loss: 0.00002243
Iteration 29/1000 | Loss: 0.00002243
Iteration 30/1000 | Loss: 0.00002243
Iteration 31/1000 | Loss: 0.00002243
Iteration 32/1000 | Loss: 0.00002243
Iteration 33/1000 | Loss: 0.00002242
Iteration 34/1000 | Loss: 0.00002242
Iteration 35/1000 | Loss: 0.00002242
Iteration 36/1000 | Loss: 0.00002242
Iteration 37/1000 | Loss: 0.00002241
Iteration 38/1000 | Loss: 0.00002241
Iteration 39/1000 | Loss: 0.00002241
Iteration 40/1000 | Loss: 0.00002241
Iteration 41/1000 | Loss: 0.00002240
Iteration 42/1000 | Loss: 0.00002240
Iteration 43/1000 | Loss: 0.00002240
Iteration 44/1000 | Loss: 0.00002240
Iteration 45/1000 | Loss: 0.00002239
Iteration 46/1000 | Loss: 0.00002239
Iteration 47/1000 | Loss: 0.00002239
Iteration 48/1000 | Loss: 0.00002239
Iteration 49/1000 | Loss: 0.00002239
Iteration 50/1000 | Loss: 0.00002239
Iteration 51/1000 | Loss: 0.00002238
Iteration 52/1000 | Loss: 0.00002238
Iteration 53/1000 | Loss: 0.00002238
Iteration 54/1000 | Loss: 0.00002237
Iteration 55/1000 | Loss: 0.00002237
Iteration 56/1000 | Loss: 0.00002237
Iteration 57/1000 | Loss: 0.00002237
Iteration 58/1000 | Loss: 0.00002236
Iteration 59/1000 | Loss: 0.00002236
Iteration 60/1000 | Loss: 0.00002236
Iteration 61/1000 | Loss: 0.00002236
Iteration 62/1000 | Loss: 0.00002235
Iteration 63/1000 | Loss: 0.00002235
Iteration 64/1000 | Loss: 0.00002234
Iteration 65/1000 | Loss: 0.00002234
Iteration 66/1000 | Loss: 0.00002234
Iteration 67/1000 | Loss: 0.00002234
Iteration 68/1000 | Loss: 0.00002234
Iteration 69/1000 | Loss: 0.00002234
Iteration 70/1000 | Loss: 0.00002234
Iteration 71/1000 | Loss: 0.00002234
Iteration 72/1000 | Loss: 0.00002234
Iteration 73/1000 | Loss: 0.00002234
Iteration 74/1000 | Loss: 0.00002233
Iteration 75/1000 | Loss: 0.00002233
Iteration 76/1000 | Loss: 0.00002233
Iteration 77/1000 | Loss: 0.00002233
Iteration 78/1000 | Loss: 0.00002233
Iteration 79/1000 | Loss: 0.00002233
Iteration 80/1000 | Loss: 0.00002233
Iteration 81/1000 | Loss: 0.00002233
Iteration 82/1000 | Loss: 0.00002233
Iteration 83/1000 | Loss: 0.00002233
Iteration 84/1000 | Loss: 0.00002233
Iteration 85/1000 | Loss: 0.00002233
Iteration 86/1000 | Loss: 0.00002233
Iteration 87/1000 | Loss: 0.00002233
Iteration 88/1000 | Loss: 0.00002233
Iteration 89/1000 | Loss: 0.00002233
Iteration 90/1000 | Loss: 0.00002233
Iteration 91/1000 | Loss: 0.00002233
Iteration 92/1000 | Loss: 0.00002233
Iteration 93/1000 | Loss: 0.00002233
Iteration 94/1000 | Loss: 0.00002233
Iteration 95/1000 | Loss: 0.00002233
Iteration 96/1000 | Loss: 0.00002233
Iteration 97/1000 | Loss: 0.00002233
Iteration 98/1000 | Loss: 0.00002233
Iteration 99/1000 | Loss: 0.00002233
Iteration 100/1000 | Loss: 0.00002233
Iteration 101/1000 | Loss: 0.00002233
Iteration 102/1000 | Loss: 0.00002233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.232558290415909e-05, 2.232558290415909e-05, 2.232558290415909e-05, 2.232558290415909e-05, 2.232558290415909e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.232558290415909e-05

Optimization complete. Final v2v error: 4.009330749511719 mm

Highest mean error: 4.450255393981934 mm for frame 80

Lowest mean error: 3.6286439895629883 mm for frame 44

Saving results

Total time: 41.598772048950195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01126657
Iteration 2/25 | Loss: 0.00314604
Iteration 3/25 | Loss: 0.00172677
Iteration 4/25 | Loss: 0.00147604
Iteration 5/25 | Loss: 0.00115469
Iteration 6/25 | Loss: 0.00102843
Iteration 7/25 | Loss: 0.00096327
Iteration 8/25 | Loss: 0.00089706
Iteration 9/25 | Loss: 0.00085894
Iteration 10/25 | Loss: 0.00082673
Iteration 11/25 | Loss: 0.00077222
Iteration 12/25 | Loss: 0.00073393
Iteration 13/25 | Loss: 0.00071192
Iteration 14/25 | Loss: 0.00069240
Iteration 15/25 | Loss: 0.00068402
Iteration 16/25 | Loss: 0.00068141
Iteration 17/25 | Loss: 0.00067972
Iteration 18/25 | Loss: 0.00067935
Iteration 19/25 | Loss: 0.00067930
Iteration 20/25 | Loss: 0.00067930
Iteration 21/25 | Loss: 0.00067930
Iteration 22/25 | Loss: 0.00067929
Iteration 23/25 | Loss: 0.00067929
Iteration 24/25 | Loss: 0.00067929
Iteration 25/25 | Loss: 0.00067929

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53522635
Iteration 2/25 | Loss: 0.00024096
Iteration 3/25 | Loss: 0.00024095
Iteration 4/25 | Loss: 0.00024095
Iteration 5/25 | Loss: 0.00024095
Iteration 6/25 | Loss: 0.00024095
Iteration 7/25 | Loss: 0.00024095
Iteration 8/25 | Loss: 0.00024095
Iteration 9/25 | Loss: 0.00024095
Iteration 10/25 | Loss: 0.00024095
Iteration 11/25 | Loss: 0.00024095
Iteration 12/25 | Loss: 0.00024095
Iteration 13/25 | Loss: 0.00024095
Iteration 14/25 | Loss: 0.00024095
Iteration 15/25 | Loss: 0.00024095
Iteration 16/25 | Loss: 0.00024095
Iteration 17/25 | Loss: 0.00024095
Iteration 18/25 | Loss: 0.00024095
Iteration 19/25 | Loss: 0.00024095
Iteration 20/25 | Loss: 0.00024095
Iteration 21/25 | Loss: 0.00024095
Iteration 22/25 | Loss: 0.00024095
Iteration 23/25 | Loss: 0.00024095
Iteration 24/25 | Loss: 0.00024095
Iteration 25/25 | Loss: 0.00024095
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00024095024855341762, 0.00024095024855341762, 0.00024095024855341762, 0.00024095024855341762, 0.00024095024855341762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024095024855341762

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024095
Iteration 2/1000 | Loss: 0.00003841
Iteration 3/1000 | Loss: 0.00002648
Iteration 4/1000 | Loss: 0.00002387
Iteration 5/1000 | Loss: 0.00002303
Iteration 6/1000 | Loss: 0.00002219
Iteration 7/1000 | Loss: 0.00002173
Iteration 8/1000 | Loss: 0.00002115
Iteration 9/1000 | Loss: 0.00002083
Iteration 10/1000 | Loss: 0.00002064
Iteration 11/1000 | Loss: 0.00002054
Iteration 12/1000 | Loss: 0.00002053
Iteration 13/1000 | Loss: 0.00002047
Iteration 14/1000 | Loss: 0.00002037
Iteration 15/1000 | Loss: 0.00002028
Iteration 16/1000 | Loss: 0.00002025
Iteration 17/1000 | Loss: 0.00002024
Iteration 18/1000 | Loss: 0.00002024
Iteration 19/1000 | Loss: 0.00002024
Iteration 20/1000 | Loss: 0.00002024
Iteration 21/1000 | Loss: 0.00002023
Iteration 22/1000 | Loss: 0.00002023
Iteration 23/1000 | Loss: 0.00002019
Iteration 24/1000 | Loss: 0.00002019
Iteration 25/1000 | Loss: 0.00002018
Iteration 26/1000 | Loss: 0.00002018
Iteration 27/1000 | Loss: 0.00002016
Iteration 28/1000 | Loss: 0.00002016
Iteration 29/1000 | Loss: 0.00002014
Iteration 30/1000 | Loss: 0.00002014
Iteration 31/1000 | Loss: 0.00002013
Iteration 32/1000 | Loss: 0.00002013
Iteration 33/1000 | Loss: 0.00002013
Iteration 34/1000 | Loss: 0.00002012
Iteration 35/1000 | Loss: 0.00002012
Iteration 36/1000 | Loss: 0.00002012
Iteration 37/1000 | Loss: 0.00002011
Iteration 38/1000 | Loss: 0.00002011
Iteration 39/1000 | Loss: 0.00002010
Iteration 40/1000 | Loss: 0.00002010
Iteration 41/1000 | Loss: 0.00002009
Iteration 42/1000 | Loss: 0.00002009
Iteration 43/1000 | Loss: 0.00002009
Iteration 44/1000 | Loss: 0.00002008
Iteration 45/1000 | Loss: 0.00002008
Iteration 46/1000 | Loss: 0.00002008
Iteration 47/1000 | Loss: 0.00002008
Iteration 48/1000 | Loss: 0.00002008
Iteration 49/1000 | Loss: 0.00002008
Iteration 50/1000 | Loss: 0.00002008
Iteration 51/1000 | Loss: 0.00002007
Iteration 52/1000 | Loss: 0.00002006
Iteration 53/1000 | Loss: 0.00002006
Iteration 54/1000 | Loss: 0.00002006
Iteration 55/1000 | Loss: 0.00002006
Iteration 56/1000 | Loss: 0.00002005
Iteration 57/1000 | Loss: 0.00002005
Iteration 58/1000 | Loss: 0.00002005
Iteration 59/1000 | Loss: 0.00002005
Iteration 60/1000 | Loss: 0.00002005
Iteration 61/1000 | Loss: 0.00002005
Iteration 62/1000 | Loss: 0.00002005
Iteration 63/1000 | Loss: 0.00002005
Iteration 64/1000 | Loss: 0.00002005
Iteration 65/1000 | Loss: 0.00002005
Iteration 66/1000 | Loss: 0.00002005
Iteration 67/1000 | Loss: 0.00002004
Iteration 68/1000 | Loss: 0.00002004
Iteration 69/1000 | Loss: 0.00002004
Iteration 70/1000 | Loss: 0.00002004
Iteration 71/1000 | Loss: 0.00002004
Iteration 72/1000 | Loss: 0.00002003
Iteration 73/1000 | Loss: 0.00002003
Iteration 74/1000 | Loss: 0.00002003
Iteration 75/1000 | Loss: 0.00002003
Iteration 76/1000 | Loss: 0.00002003
Iteration 77/1000 | Loss: 0.00002003
Iteration 78/1000 | Loss: 0.00002003
Iteration 79/1000 | Loss: 0.00002003
Iteration 80/1000 | Loss: 0.00002003
Iteration 81/1000 | Loss: 0.00002003
Iteration 82/1000 | Loss: 0.00002002
Iteration 83/1000 | Loss: 0.00002002
Iteration 84/1000 | Loss: 0.00002001
Iteration 85/1000 | Loss: 0.00002001
Iteration 86/1000 | Loss: 0.00002001
Iteration 87/1000 | Loss: 0.00002001
Iteration 88/1000 | Loss: 0.00002000
Iteration 89/1000 | Loss: 0.00002000
Iteration 90/1000 | Loss: 0.00001999
Iteration 91/1000 | Loss: 0.00001999
Iteration 92/1000 | Loss: 0.00001999
Iteration 93/1000 | Loss: 0.00001998
Iteration 94/1000 | Loss: 0.00001996
Iteration 95/1000 | Loss: 0.00001996
Iteration 96/1000 | Loss: 0.00001996
Iteration 97/1000 | Loss: 0.00001996
Iteration 98/1000 | Loss: 0.00001995
Iteration 99/1000 | Loss: 0.00001995
Iteration 100/1000 | Loss: 0.00001995
Iteration 101/1000 | Loss: 0.00001994
Iteration 102/1000 | Loss: 0.00001994
Iteration 103/1000 | Loss: 0.00001993
Iteration 104/1000 | Loss: 0.00001993
Iteration 105/1000 | Loss: 0.00001993
Iteration 106/1000 | Loss: 0.00001992
Iteration 107/1000 | Loss: 0.00001992
Iteration 108/1000 | Loss: 0.00001992
Iteration 109/1000 | Loss: 0.00001991
Iteration 110/1000 | Loss: 0.00001991
Iteration 111/1000 | Loss: 0.00001991
Iteration 112/1000 | Loss: 0.00001991
Iteration 113/1000 | Loss: 0.00001991
Iteration 114/1000 | Loss: 0.00001990
Iteration 115/1000 | Loss: 0.00001990
Iteration 116/1000 | Loss: 0.00001989
Iteration 117/1000 | Loss: 0.00001989
Iteration 118/1000 | Loss: 0.00001989
Iteration 119/1000 | Loss: 0.00001988
Iteration 120/1000 | Loss: 0.00001988
Iteration 121/1000 | Loss: 0.00001988
Iteration 122/1000 | Loss: 0.00001988
Iteration 123/1000 | Loss: 0.00001988
Iteration 124/1000 | Loss: 0.00001987
Iteration 125/1000 | Loss: 0.00001987
Iteration 126/1000 | Loss: 0.00001987
Iteration 127/1000 | Loss: 0.00001987
Iteration 128/1000 | Loss: 0.00001987
Iteration 129/1000 | Loss: 0.00001987
Iteration 130/1000 | Loss: 0.00001987
Iteration 131/1000 | Loss: 0.00001987
Iteration 132/1000 | Loss: 0.00001987
Iteration 133/1000 | Loss: 0.00001987
Iteration 134/1000 | Loss: 0.00001987
Iteration 135/1000 | Loss: 0.00001987
Iteration 136/1000 | Loss: 0.00001986
Iteration 137/1000 | Loss: 0.00001986
Iteration 138/1000 | Loss: 0.00001986
Iteration 139/1000 | Loss: 0.00001986
Iteration 140/1000 | Loss: 0.00001986
Iteration 141/1000 | Loss: 0.00001986
Iteration 142/1000 | Loss: 0.00001986
Iteration 143/1000 | Loss: 0.00001986
Iteration 144/1000 | Loss: 0.00001986
Iteration 145/1000 | Loss: 0.00001986
Iteration 146/1000 | Loss: 0.00001986
Iteration 147/1000 | Loss: 0.00001986
Iteration 148/1000 | Loss: 0.00001985
Iteration 149/1000 | Loss: 0.00001985
Iteration 150/1000 | Loss: 0.00001985
Iteration 151/1000 | Loss: 0.00001985
Iteration 152/1000 | Loss: 0.00001985
Iteration 153/1000 | Loss: 0.00001985
Iteration 154/1000 | Loss: 0.00001985
Iteration 155/1000 | Loss: 0.00001985
Iteration 156/1000 | Loss: 0.00001985
Iteration 157/1000 | Loss: 0.00001984
Iteration 158/1000 | Loss: 0.00001984
Iteration 159/1000 | Loss: 0.00001984
Iteration 160/1000 | Loss: 0.00001984
Iteration 161/1000 | Loss: 0.00001984
Iteration 162/1000 | Loss: 0.00001984
Iteration 163/1000 | Loss: 0.00001984
Iteration 164/1000 | Loss: 0.00001984
Iteration 165/1000 | Loss: 0.00001984
Iteration 166/1000 | Loss: 0.00001984
Iteration 167/1000 | Loss: 0.00001984
Iteration 168/1000 | Loss: 0.00001983
Iteration 169/1000 | Loss: 0.00001983
Iteration 170/1000 | Loss: 0.00001983
Iteration 171/1000 | Loss: 0.00001983
Iteration 172/1000 | Loss: 0.00001983
Iteration 173/1000 | Loss: 0.00001983
Iteration 174/1000 | Loss: 0.00001983
Iteration 175/1000 | Loss: 0.00001983
Iteration 176/1000 | Loss: 0.00001983
Iteration 177/1000 | Loss: 0.00001983
Iteration 178/1000 | Loss: 0.00001983
Iteration 179/1000 | Loss: 0.00001983
Iteration 180/1000 | Loss: 0.00001983
Iteration 181/1000 | Loss: 0.00001983
Iteration 182/1000 | Loss: 0.00001982
Iteration 183/1000 | Loss: 0.00001982
Iteration 184/1000 | Loss: 0.00001982
Iteration 185/1000 | Loss: 0.00001982
Iteration 186/1000 | Loss: 0.00001982
Iteration 187/1000 | Loss: 0.00001982
Iteration 188/1000 | Loss: 0.00001982
Iteration 189/1000 | Loss: 0.00001982
Iteration 190/1000 | Loss: 0.00001982
Iteration 191/1000 | Loss: 0.00001982
Iteration 192/1000 | Loss: 0.00001982
Iteration 193/1000 | Loss: 0.00001982
Iteration 194/1000 | Loss: 0.00001982
Iteration 195/1000 | Loss: 0.00001982
Iteration 196/1000 | Loss: 0.00001982
Iteration 197/1000 | Loss: 0.00001982
Iteration 198/1000 | Loss: 0.00001982
Iteration 199/1000 | Loss: 0.00001982
Iteration 200/1000 | Loss: 0.00001982
Iteration 201/1000 | Loss: 0.00001982
Iteration 202/1000 | Loss: 0.00001982
Iteration 203/1000 | Loss: 0.00001981
Iteration 204/1000 | Loss: 0.00001981
Iteration 205/1000 | Loss: 0.00001981
Iteration 206/1000 | Loss: 0.00001981
Iteration 207/1000 | Loss: 0.00001981
Iteration 208/1000 | Loss: 0.00001981
Iteration 209/1000 | Loss: 0.00001980
Iteration 210/1000 | Loss: 0.00001980
Iteration 211/1000 | Loss: 0.00001980
Iteration 212/1000 | Loss: 0.00001980
Iteration 213/1000 | Loss: 0.00001980
Iteration 214/1000 | Loss: 0.00001980
Iteration 215/1000 | Loss: 0.00001980
Iteration 216/1000 | Loss: 0.00001980
Iteration 217/1000 | Loss: 0.00001980
Iteration 218/1000 | Loss: 0.00001980
Iteration 219/1000 | Loss: 0.00001980
Iteration 220/1000 | Loss: 0.00001980
Iteration 221/1000 | Loss: 0.00001980
Iteration 222/1000 | Loss: 0.00001980
Iteration 223/1000 | Loss: 0.00001979
Iteration 224/1000 | Loss: 0.00001979
Iteration 225/1000 | Loss: 0.00001979
Iteration 226/1000 | Loss: 0.00001979
Iteration 227/1000 | Loss: 0.00001979
Iteration 228/1000 | Loss: 0.00001979
Iteration 229/1000 | Loss: 0.00001979
Iteration 230/1000 | Loss: 0.00001979
Iteration 231/1000 | Loss: 0.00001979
Iteration 232/1000 | Loss: 0.00001979
Iteration 233/1000 | Loss: 0.00001979
Iteration 234/1000 | Loss: 0.00001979
Iteration 235/1000 | Loss: 0.00001979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.9788929421338253e-05, 1.9788929421338253e-05, 1.9788929421338253e-05, 1.9788929421338253e-05, 1.9788929421338253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9788929421338253e-05

Optimization complete. Final v2v error: 3.727672815322876 mm

Highest mean error: 4.786049842834473 mm for frame 100

Lowest mean error: 3.1395890712738037 mm for frame 158

Saving results

Total time: 67.9225869178772
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00629503
Iteration 2/25 | Loss: 0.00077952
Iteration 3/25 | Loss: 0.00065945
Iteration 4/25 | Loss: 0.00063502
Iteration 5/25 | Loss: 0.00062371
Iteration 6/25 | Loss: 0.00062199
Iteration 7/25 | Loss: 0.00062167
Iteration 8/25 | Loss: 0.00062167
Iteration 9/25 | Loss: 0.00062167
Iteration 10/25 | Loss: 0.00062167
Iteration 11/25 | Loss: 0.00062167
Iteration 12/25 | Loss: 0.00062167
Iteration 13/25 | Loss: 0.00062167
Iteration 14/25 | Loss: 0.00062167
Iteration 15/25 | Loss: 0.00062167
Iteration 16/25 | Loss: 0.00062167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006216703332029283, 0.0006216703332029283, 0.0006216703332029283, 0.0006216703332029283, 0.0006216703332029283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006216703332029283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75489473
Iteration 2/25 | Loss: 0.00028693
Iteration 3/25 | Loss: 0.00028693
Iteration 4/25 | Loss: 0.00028693
Iteration 5/25 | Loss: 0.00028693
Iteration 6/25 | Loss: 0.00028693
Iteration 7/25 | Loss: 0.00028693
Iteration 8/25 | Loss: 0.00028693
Iteration 9/25 | Loss: 0.00028693
Iteration 10/25 | Loss: 0.00028693
Iteration 11/25 | Loss: 0.00028693
Iteration 12/25 | Loss: 0.00028693
Iteration 13/25 | Loss: 0.00028693
Iteration 14/25 | Loss: 0.00028693
Iteration 15/25 | Loss: 0.00028693
Iteration 16/25 | Loss: 0.00028693
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0002869306190405041, 0.0002869306190405041, 0.0002869306190405041, 0.0002869306190405041, 0.0002869306190405041]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002869306190405041

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028693
Iteration 2/1000 | Loss: 0.00002370
Iteration 3/1000 | Loss: 0.00001684
Iteration 4/1000 | Loss: 0.00001578
Iteration 5/1000 | Loss: 0.00001487
Iteration 6/1000 | Loss: 0.00001450
Iteration 7/1000 | Loss: 0.00001426
Iteration 8/1000 | Loss: 0.00001404
Iteration 9/1000 | Loss: 0.00001388
Iteration 10/1000 | Loss: 0.00001381
Iteration 11/1000 | Loss: 0.00001381
Iteration 12/1000 | Loss: 0.00001375
Iteration 13/1000 | Loss: 0.00001373
Iteration 14/1000 | Loss: 0.00001373
Iteration 15/1000 | Loss: 0.00001373
Iteration 16/1000 | Loss: 0.00001373
Iteration 17/1000 | Loss: 0.00001372
Iteration 18/1000 | Loss: 0.00001368
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001366
Iteration 21/1000 | Loss: 0.00001361
Iteration 22/1000 | Loss: 0.00001360
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001356
Iteration 25/1000 | Loss: 0.00001355
Iteration 26/1000 | Loss: 0.00001355
Iteration 27/1000 | Loss: 0.00001355
Iteration 28/1000 | Loss: 0.00001354
Iteration 29/1000 | Loss: 0.00001354
Iteration 30/1000 | Loss: 0.00001354
Iteration 31/1000 | Loss: 0.00001353
Iteration 32/1000 | Loss: 0.00001353
Iteration 33/1000 | Loss: 0.00001352
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001351
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001351
Iteration 38/1000 | Loss: 0.00001351
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001351
Iteration 41/1000 | Loss: 0.00001351
Iteration 42/1000 | Loss: 0.00001351
Iteration 43/1000 | Loss: 0.00001350
Iteration 44/1000 | Loss: 0.00001350
Iteration 45/1000 | Loss: 0.00001350
Iteration 46/1000 | Loss: 0.00001350
Iteration 47/1000 | Loss: 0.00001349
Iteration 48/1000 | Loss: 0.00001348
Iteration 49/1000 | Loss: 0.00001345
Iteration 50/1000 | Loss: 0.00001343
Iteration 51/1000 | Loss: 0.00001343
Iteration 52/1000 | Loss: 0.00001343
Iteration 53/1000 | Loss: 0.00001342
Iteration 54/1000 | Loss: 0.00001342
Iteration 55/1000 | Loss: 0.00001339
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001338
Iteration 58/1000 | Loss: 0.00001338
Iteration 59/1000 | Loss: 0.00001338
Iteration 60/1000 | Loss: 0.00001338
Iteration 61/1000 | Loss: 0.00001337
Iteration 62/1000 | Loss: 0.00001337
Iteration 63/1000 | Loss: 0.00001336
Iteration 64/1000 | Loss: 0.00001336
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001333
Iteration 69/1000 | Loss: 0.00001333
Iteration 70/1000 | Loss: 0.00001332
Iteration 71/1000 | Loss: 0.00001332
Iteration 72/1000 | Loss: 0.00001332
Iteration 73/1000 | Loss: 0.00001332
Iteration 74/1000 | Loss: 0.00001332
Iteration 75/1000 | Loss: 0.00001331
Iteration 76/1000 | Loss: 0.00001330
Iteration 77/1000 | Loss: 0.00001330
Iteration 78/1000 | Loss: 0.00001330
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001330
Iteration 82/1000 | Loss: 0.00001330
Iteration 83/1000 | Loss: 0.00001330
Iteration 84/1000 | Loss: 0.00001330
Iteration 85/1000 | Loss: 0.00001330
Iteration 86/1000 | Loss: 0.00001330
Iteration 87/1000 | Loss: 0.00001330
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001329
Iteration 91/1000 | Loss: 0.00001328
Iteration 92/1000 | Loss: 0.00001328
Iteration 93/1000 | Loss: 0.00001328
Iteration 94/1000 | Loss: 0.00001328
Iteration 95/1000 | Loss: 0.00001327
Iteration 96/1000 | Loss: 0.00001327
Iteration 97/1000 | Loss: 0.00001327
Iteration 98/1000 | Loss: 0.00001327
Iteration 99/1000 | Loss: 0.00001327
Iteration 100/1000 | Loss: 0.00001327
Iteration 101/1000 | Loss: 0.00001327
Iteration 102/1000 | Loss: 0.00001327
Iteration 103/1000 | Loss: 0.00001327
Iteration 104/1000 | Loss: 0.00001327
Iteration 105/1000 | Loss: 0.00001327
Iteration 106/1000 | Loss: 0.00001326
Iteration 107/1000 | Loss: 0.00001326
Iteration 108/1000 | Loss: 0.00001326
Iteration 109/1000 | Loss: 0.00001326
Iteration 110/1000 | Loss: 0.00001326
Iteration 111/1000 | Loss: 0.00001326
Iteration 112/1000 | Loss: 0.00001326
Iteration 113/1000 | Loss: 0.00001326
Iteration 114/1000 | Loss: 0.00001325
Iteration 115/1000 | Loss: 0.00001325
Iteration 116/1000 | Loss: 0.00001325
Iteration 117/1000 | Loss: 0.00001325
Iteration 118/1000 | Loss: 0.00001325
Iteration 119/1000 | Loss: 0.00001325
Iteration 120/1000 | Loss: 0.00001325
Iteration 121/1000 | Loss: 0.00001325
Iteration 122/1000 | Loss: 0.00001325
Iteration 123/1000 | Loss: 0.00001325
Iteration 124/1000 | Loss: 0.00001325
Iteration 125/1000 | Loss: 0.00001325
Iteration 126/1000 | Loss: 0.00001325
Iteration 127/1000 | Loss: 0.00001325
Iteration 128/1000 | Loss: 0.00001325
Iteration 129/1000 | Loss: 0.00001325
Iteration 130/1000 | Loss: 0.00001325
Iteration 131/1000 | Loss: 0.00001325
Iteration 132/1000 | Loss: 0.00001325
Iteration 133/1000 | Loss: 0.00001324
Iteration 134/1000 | Loss: 0.00001324
Iteration 135/1000 | Loss: 0.00001324
Iteration 136/1000 | Loss: 0.00001324
Iteration 137/1000 | Loss: 0.00001324
Iteration 138/1000 | Loss: 0.00001324
Iteration 139/1000 | Loss: 0.00001324
Iteration 140/1000 | Loss: 0.00001324
Iteration 141/1000 | Loss: 0.00001324
Iteration 142/1000 | Loss: 0.00001324
Iteration 143/1000 | Loss: 0.00001323
Iteration 144/1000 | Loss: 0.00001323
Iteration 145/1000 | Loss: 0.00001323
Iteration 146/1000 | Loss: 0.00001323
Iteration 147/1000 | Loss: 0.00001323
Iteration 148/1000 | Loss: 0.00001323
Iteration 149/1000 | Loss: 0.00001323
Iteration 150/1000 | Loss: 0.00001323
Iteration 151/1000 | Loss: 0.00001323
Iteration 152/1000 | Loss: 0.00001323
Iteration 153/1000 | Loss: 0.00001323
Iteration 154/1000 | Loss: 0.00001323
Iteration 155/1000 | Loss: 0.00001323
Iteration 156/1000 | Loss: 0.00001323
Iteration 157/1000 | Loss: 0.00001323
Iteration 158/1000 | Loss: 0.00001323
Iteration 159/1000 | Loss: 0.00001323
Iteration 160/1000 | Loss: 0.00001323
Iteration 161/1000 | Loss: 0.00001322
Iteration 162/1000 | Loss: 0.00001322
Iteration 163/1000 | Loss: 0.00001322
Iteration 164/1000 | Loss: 0.00001322
Iteration 165/1000 | Loss: 0.00001322
Iteration 166/1000 | Loss: 0.00001322
Iteration 167/1000 | Loss: 0.00001322
Iteration 168/1000 | Loss: 0.00001322
Iteration 169/1000 | Loss: 0.00001322
Iteration 170/1000 | Loss: 0.00001322
Iteration 171/1000 | Loss: 0.00001322
Iteration 172/1000 | Loss: 0.00001322
Iteration 173/1000 | Loss: 0.00001322
Iteration 174/1000 | Loss: 0.00001322
Iteration 175/1000 | Loss: 0.00001322
Iteration 176/1000 | Loss: 0.00001322
Iteration 177/1000 | Loss: 0.00001322
Iteration 178/1000 | Loss: 0.00001322
Iteration 179/1000 | Loss: 0.00001322
Iteration 180/1000 | Loss: 0.00001322
Iteration 181/1000 | Loss: 0.00001322
Iteration 182/1000 | Loss: 0.00001322
Iteration 183/1000 | Loss: 0.00001322
Iteration 184/1000 | Loss: 0.00001322
Iteration 185/1000 | Loss: 0.00001322
Iteration 186/1000 | Loss: 0.00001322
Iteration 187/1000 | Loss: 0.00001322
Iteration 188/1000 | Loss: 0.00001322
Iteration 189/1000 | Loss: 0.00001322
Iteration 190/1000 | Loss: 0.00001322
Iteration 191/1000 | Loss: 0.00001322
Iteration 192/1000 | Loss: 0.00001322
Iteration 193/1000 | Loss: 0.00001322
Iteration 194/1000 | Loss: 0.00001322
Iteration 195/1000 | Loss: 0.00001322
Iteration 196/1000 | Loss: 0.00001322
Iteration 197/1000 | Loss: 0.00001322
Iteration 198/1000 | Loss: 0.00001322
Iteration 199/1000 | Loss: 0.00001322
Iteration 200/1000 | Loss: 0.00001322
Iteration 201/1000 | Loss: 0.00001322
Iteration 202/1000 | Loss: 0.00001322
Iteration 203/1000 | Loss: 0.00001322
Iteration 204/1000 | Loss: 0.00001322
Iteration 205/1000 | Loss: 0.00001322
Iteration 206/1000 | Loss: 0.00001322
Iteration 207/1000 | Loss: 0.00001322
Iteration 208/1000 | Loss: 0.00001322
Iteration 209/1000 | Loss: 0.00001322
Iteration 210/1000 | Loss: 0.00001322
Iteration 211/1000 | Loss: 0.00001322
Iteration 212/1000 | Loss: 0.00001322
Iteration 213/1000 | Loss: 0.00001322
Iteration 214/1000 | Loss: 0.00001322
Iteration 215/1000 | Loss: 0.00001322
Iteration 216/1000 | Loss: 0.00001322
Iteration 217/1000 | Loss: 0.00001322
Iteration 218/1000 | Loss: 0.00001322
Iteration 219/1000 | Loss: 0.00001322
Iteration 220/1000 | Loss: 0.00001322
Iteration 221/1000 | Loss: 0.00001322
Iteration 222/1000 | Loss: 0.00001322
Iteration 223/1000 | Loss: 0.00001322
Iteration 224/1000 | Loss: 0.00001322
Iteration 225/1000 | Loss: 0.00001322
Iteration 226/1000 | Loss: 0.00001322
Iteration 227/1000 | Loss: 0.00001322
Iteration 228/1000 | Loss: 0.00001322
Iteration 229/1000 | Loss: 0.00001322
Iteration 230/1000 | Loss: 0.00001322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.3217605555837508e-05, 1.3217605555837508e-05, 1.3217605555837508e-05, 1.3217605555837508e-05, 1.3217605555837508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3217605555837508e-05

Optimization complete. Final v2v error: 3.0985419750213623 mm

Highest mean error: 3.414656639099121 mm for frame 78

Lowest mean error: 2.965226173400879 mm for frame 11

Saving results

Total time: 39.67422008514404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816104
Iteration 2/25 | Loss: 0.00098011
Iteration 3/25 | Loss: 0.00071177
Iteration 4/25 | Loss: 0.00067144
Iteration 5/25 | Loss: 0.00066372
Iteration 6/25 | Loss: 0.00066153
Iteration 7/25 | Loss: 0.00066116
Iteration 8/25 | Loss: 0.00066116
Iteration 9/25 | Loss: 0.00066116
Iteration 10/25 | Loss: 0.00066116
Iteration 11/25 | Loss: 0.00066116
Iteration 12/25 | Loss: 0.00066116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006611648132093251, 0.0006611648132093251, 0.0006611648132093251, 0.0006611648132093251, 0.0006611648132093251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006611648132093251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39620352
Iteration 2/25 | Loss: 0.00026040
Iteration 3/25 | Loss: 0.00026039
Iteration 4/25 | Loss: 0.00026039
Iteration 5/25 | Loss: 0.00026039
Iteration 6/25 | Loss: 0.00026039
Iteration 7/25 | Loss: 0.00026039
Iteration 8/25 | Loss: 0.00026039
Iteration 9/25 | Loss: 0.00026039
Iteration 10/25 | Loss: 0.00026039
Iteration 11/25 | Loss: 0.00026039
Iteration 12/25 | Loss: 0.00026039
Iteration 13/25 | Loss: 0.00026039
Iteration 14/25 | Loss: 0.00026039
Iteration 15/25 | Loss: 0.00026039
Iteration 16/25 | Loss: 0.00026039
Iteration 17/25 | Loss: 0.00026039
Iteration 18/25 | Loss: 0.00026039
Iteration 19/25 | Loss: 0.00026039
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00026039083604700863, 0.00026039083604700863, 0.00026039083604700863, 0.00026039083604700863, 0.00026039083604700863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026039083604700863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026039
Iteration 2/1000 | Loss: 0.00002991
Iteration 3/1000 | Loss: 0.00002327
Iteration 4/1000 | Loss: 0.00002195
Iteration 5/1000 | Loss: 0.00002094
Iteration 6/1000 | Loss: 0.00002016
Iteration 7/1000 | Loss: 0.00001963
Iteration 8/1000 | Loss: 0.00001917
Iteration 9/1000 | Loss: 0.00001891
Iteration 10/1000 | Loss: 0.00001868
Iteration 11/1000 | Loss: 0.00001853
Iteration 12/1000 | Loss: 0.00001851
Iteration 13/1000 | Loss: 0.00001848
Iteration 14/1000 | Loss: 0.00001848
Iteration 15/1000 | Loss: 0.00001846
Iteration 16/1000 | Loss: 0.00001845
Iteration 17/1000 | Loss: 0.00001843
Iteration 18/1000 | Loss: 0.00001842
Iteration 19/1000 | Loss: 0.00001842
Iteration 20/1000 | Loss: 0.00001842
Iteration 21/1000 | Loss: 0.00001841
Iteration 22/1000 | Loss: 0.00001841
Iteration 23/1000 | Loss: 0.00001840
Iteration 24/1000 | Loss: 0.00001838
Iteration 25/1000 | Loss: 0.00001838
Iteration 26/1000 | Loss: 0.00001838
Iteration 27/1000 | Loss: 0.00001836
Iteration 28/1000 | Loss: 0.00001836
Iteration 29/1000 | Loss: 0.00001836
Iteration 30/1000 | Loss: 0.00001835
Iteration 31/1000 | Loss: 0.00001835
Iteration 32/1000 | Loss: 0.00001835
Iteration 33/1000 | Loss: 0.00001834
Iteration 34/1000 | Loss: 0.00001834
Iteration 35/1000 | Loss: 0.00001834
Iteration 36/1000 | Loss: 0.00001834
Iteration 37/1000 | Loss: 0.00001833
Iteration 38/1000 | Loss: 0.00001833
Iteration 39/1000 | Loss: 0.00001833
Iteration 40/1000 | Loss: 0.00001833
Iteration 41/1000 | Loss: 0.00001832
Iteration 42/1000 | Loss: 0.00001832
Iteration 43/1000 | Loss: 0.00001832
Iteration 44/1000 | Loss: 0.00001831
Iteration 45/1000 | Loss: 0.00001831
Iteration 46/1000 | Loss: 0.00001831
Iteration 47/1000 | Loss: 0.00001831
Iteration 48/1000 | Loss: 0.00001831
Iteration 49/1000 | Loss: 0.00001831
Iteration 50/1000 | Loss: 0.00001831
Iteration 51/1000 | Loss: 0.00001830
Iteration 52/1000 | Loss: 0.00001830
Iteration 53/1000 | Loss: 0.00001830
Iteration 54/1000 | Loss: 0.00001830
Iteration 55/1000 | Loss: 0.00001830
Iteration 56/1000 | Loss: 0.00001830
Iteration 57/1000 | Loss: 0.00001830
Iteration 58/1000 | Loss: 0.00001830
Iteration 59/1000 | Loss: 0.00001830
Iteration 60/1000 | Loss: 0.00001830
Iteration 61/1000 | Loss: 0.00001829
Iteration 62/1000 | Loss: 0.00001829
Iteration 63/1000 | Loss: 0.00001829
Iteration 64/1000 | Loss: 0.00001829
Iteration 65/1000 | Loss: 0.00001829
Iteration 66/1000 | Loss: 0.00001829
Iteration 67/1000 | Loss: 0.00001829
Iteration 68/1000 | Loss: 0.00001829
Iteration 69/1000 | Loss: 0.00001829
Iteration 70/1000 | Loss: 0.00001829
Iteration 71/1000 | Loss: 0.00001829
Iteration 72/1000 | Loss: 0.00001829
Iteration 73/1000 | Loss: 0.00001829
Iteration 74/1000 | Loss: 0.00001829
Iteration 75/1000 | Loss: 0.00001829
Iteration 76/1000 | Loss: 0.00001828
Iteration 77/1000 | Loss: 0.00001828
Iteration 78/1000 | Loss: 0.00001828
Iteration 79/1000 | Loss: 0.00001828
Iteration 80/1000 | Loss: 0.00001828
Iteration 81/1000 | Loss: 0.00001828
Iteration 82/1000 | Loss: 0.00001828
Iteration 83/1000 | Loss: 0.00001828
Iteration 84/1000 | Loss: 0.00001828
Iteration 85/1000 | Loss: 0.00001828
Iteration 86/1000 | Loss: 0.00001828
Iteration 87/1000 | Loss: 0.00001828
Iteration 88/1000 | Loss: 0.00001828
Iteration 89/1000 | Loss: 0.00001828
Iteration 90/1000 | Loss: 0.00001828
Iteration 91/1000 | Loss: 0.00001828
Iteration 92/1000 | Loss: 0.00001828
Iteration 93/1000 | Loss: 0.00001827
Iteration 94/1000 | Loss: 0.00001827
Iteration 95/1000 | Loss: 0.00001827
Iteration 96/1000 | Loss: 0.00001827
Iteration 97/1000 | Loss: 0.00001827
Iteration 98/1000 | Loss: 0.00001827
Iteration 99/1000 | Loss: 0.00001827
Iteration 100/1000 | Loss: 0.00001827
Iteration 101/1000 | Loss: 0.00001827
Iteration 102/1000 | Loss: 0.00001827
Iteration 103/1000 | Loss: 0.00001827
Iteration 104/1000 | Loss: 0.00001827
Iteration 105/1000 | Loss: 0.00001827
Iteration 106/1000 | Loss: 0.00001827
Iteration 107/1000 | Loss: 0.00001826
Iteration 108/1000 | Loss: 0.00001826
Iteration 109/1000 | Loss: 0.00001826
Iteration 110/1000 | Loss: 0.00001826
Iteration 111/1000 | Loss: 0.00001826
Iteration 112/1000 | Loss: 0.00001826
Iteration 113/1000 | Loss: 0.00001826
Iteration 114/1000 | Loss: 0.00001825
Iteration 115/1000 | Loss: 0.00001825
Iteration 116/1000 | Loss: 0.00001825
Iteration 117/1000 | Loss: 0.00001825
Iteration 118/1000 | Loss: 0.00001825
Iteration 119/1000 | Loss: 0.00001825
Iteration 120/1000 | Loss: 0.00001824
Iteration 121/1000 | Loss: 0.00001824
Iteration 122/1000 | Loss: 0.00001824
Iteration 123/1000 | Loss: 0.00001824
Iteration 124/1000 | Loss: 0.00001824
Iteration 125/1000 | Loss: 0.00001824
Iteration 126/1000 | Loss: 0.00001823
Iteration 127/1000 | Loss: 0.00001823
Iteration 128/1000 | Loss: 0.00001823
Iteration 129/1000 | Loss: 0.00001823
Iteration 130/1000 | Loss: 0.00001823
Iteration 131/1000 | Loss: 0.00001823
Iteration 132/1000 | Loss: 0.00001823
Iteration 133/1000 | Loss: 0.00001823
Iteration 134/1000 | Loss: 0.00001823
Iteration 135/1000 | Loss: 0.00001823
Iteration 136/1000 | Loss: 0.00001823
Iteration 137/1000 | Loss: 0.00001823
Iteration 138/1000 | Loss: 0.00001823
Iteration 139/1000 | Loss: 0.00001823
Iteration 140/1000 | Loss: 0.00001823
Iteration 141/1000 | Loss: 0.00001823
Iteration 142/1000 | Loss: 0.00001822
Iteration 143/1000 | Loss: 0.00001823
Iteration 144/1000 | Loss: 0.00001822
Iteration 145/1000 | Loss: 0.00001822
Iteration 146/1000 | Loss: 0.00001822
Iteration 147/1000 | Loss: 0.00001823
Iteration 148/1000 | Loss: 0.00001823
Iteration 149/1000 | Loss: 0.00001823
Iteration 150/1000 | Loss: 0.00001822
Iteration 151/1000 | Loss: 0.00001822
Iteration 152/1000 | Loss: 0.00001822
Iteration 153/1000 | Loss: 0.00001823
Iteration 154/1000 | Loss: 0.00001822
Iteration 155/1000 | Loss: 0.00001823
Iteration 156/1000 | Loss: 0.00001823
Iteration 157/1000 | Loss: 0.00001823
Iteration 158/1000 | Loss: 0.00001823
Iteration 159/1000 | Loss: 0.00001823
Iteration 160/1000 | Loss: 0.00001823
Iteration 161/1000 | Loss: 0.00001823
Iteration 162/1000 | Loss: 0.00001823
Iteration 163/1000 | Loss: 0.00001822
Iteration 164/1000 | Loss: 0.00001822
Iteration 165/1000 | Loss: 0.00001823
Iteration 166/1000 | Loss: 0.00001823
Iteration 167/1000 | Loss: 0.00001823
Iteration 168/1000 | Loss: 0.00001823
Iteration 169/1000 | Loss: 0.00001822
Iteration 170/1000 | Loss: 0.00001823
Iteration 171/1000 | Loss: 0.00001823
Iteration 172/1000 | Loss: 0.00001823
Iteration 173/1000 | Loss: 0.00001823
Iteration 174/1000 | Loss: 0.00001823
Iteration 175/1000 | Loss: 0.00001822
Iteration 176/1000 | Loss: 0.00001822
Iteration 177/1000 | Loss: 0.00001822
Iteration 178/1000 | Loss: 0.00001822
Iteration 179/1000 | Loss: 0.00001822
Iteration 180/1000 | Loss: 0.00001822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.8224998711957596e-05, 1.8224998711957596e-05, 1.8224998711957596e-05, 1.8224998711957596e-05, 1.8224998711957596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8224998711957596e-05

Optimization complete. Final v2v error: 3.597625732421875 mm

Highest mean error: 4.695096015930176 mm for frame 201

Lowest mean error: 3.051295518875122 mm for frame 159

Saving results

Total time: 41.64979791641235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544325
Iteration 2/25 | Loss: 0.00099235
Iteration 3/25 | Loss: 0.00068446
Iteration 4/25 | Loss: 0.00064012
Iteration 5/25 | Loss: 0.00062841
Iteration 6/25 | Loss: 0.00061618
Iteration 7/25 | Loss: 0.00061461
Iteration 8/25 | Loss: 0.00061441
Iteration 9/25 | Loss: 0.00061438
Iteration 10/25 | Loss: 0.00061438
Iteration 11/25 | Loss: 0.00061437
Iteration 12/25 | Loss: 0.00061437
Iteration 13/25 | Loss: 0.00061437
Iteration 14/25 | Loss: 0.00061437
Iteration 15/25 | Loss: 0.00061437
Iteration 16/25 | Loss: 0.00061437
Iteration 17/25 | Loss: 0.00061437
Iteration 18/25 | Loss: 0.00061437
Iteration 19/25 | Loss: 0.00061437
Iteration 20/25 | Loss: 0.00061437
Iteration 21/25 | Loss: 0.00061437
Iteration 22/25 | Loss: 0.00061437
Iteration 23/25 | Loss: 0.00061437
Iteration 24/25 | Loss: 0.00061437
Iteration 25/25 | Loss: 0.00061436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.00673294
Iteration 2/25 | Loss: 0.00028704
Iteration 3/25 | Loss: 0.00028704
Iteration 4/25 | Loss: 0.00028704
Iteration 5/25 | Loss: 0.00028704
Iteration 6/25 | Loss: 0.00028704
Iteration 7/25 | Loss: 0.00028704
Iteration 8/25 | Loss: 0.00028704
Iteration 9/25 | Loss: 0.00028704
Iteration 10/25 | Loss: 0.00028704
Iteration 11/25 | Loss: 0.00028704
Iteration 12/25 | Loss: 0.00028704
Iteration 13/25 | Loss: 0.00028704
Iteration 14/25 | Loss: 0.00028704
Iteration 15/25 | Loss: 0.00028704
Iteration 16/25 | Loss: 0.00028704
Iteration 17/25 | Loss: 0.00028704
Iteration 18/25 | Loss: 0.00028704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002870365569833666, 0.0002870365569833666, 0.0002870365569833666, 0.0002870365569833666, 0.0002870365569833666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002870365569833666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028704
Iteration 2/1000 | Loss: 0.00001942
Iteration 3/1000 | Loss: 0.00003507
Iteration 4/1000 | Loss: 0.00001386
Iteration 5/1000 | Loss: 0.00001271
Iteration 6/1000 | Loss: 0.00001236
Iteration 7/1000 | Loss: 0.00004039
Iteration 8/1000 | Loss: 0.00001209
Iteration 9/1000 | Loss: 0.00001192
Iteration 10/1000 | Loss: 0.00004077
Iteration 11/1000 | Loss: 0.00001362
Iteration 12/1000 | Loss: 0.00001647
Iteration 13/1000 | Loss: 0.00001172
Iteration 14/1000 | Loss: 0.00001171
Iteration 15/1000 | Loss: 0.00001171
Iteration 16/1000 | Loss: 0.00001171
Iteration 17/1000 | Loss: 0.00001170
Iteration 18/1000 | Loss: 0.00001163
Iteration 19/1000 | Loss: 0.00001163
Iteration 20/1000 | Loss: 0.00001161
Iteration 21/1000 | Loss: 0.00001161
Iteration 22/1000 | Loss: 0.00001160
Iteration 23/1000 | Loss: 0.00001160
Iteration 24/1000 | Loss: 0.00001160
Iteration 25/1000 | Loss: 0.00001160
Iteration 26/1000 | Loss: 0.00001159
Iteration 27/1000 | Loss: 0.00001159
Iteration 28/1000 | Loss: 0.00001158
Iteration 29/1000 | Loss: 0.00001158
Iteration 30/1000 | Loss: 0.00001158
Iteration 31/1000 | Loss: 0.00001158
Iteration 32/1000 | Loss: 0.00001158
Iteration 33/1000 | Loss: 0.00001157
Iteration 34/1000 | Loss: 0.00001157
Iteration 35/1000 | Loss: 0.00001157
Iteration 36/1000 | Loss: 0.00001156
Iteration 37/1000 | Loss: 0.00001156
Iteration 38/1000 | Loss: 0.00001156
Iteration 39/1000 | Loss: 0.00001156
Iteration 40/1000 | Loss: 0.00001156
Iteration 41/1000 | Loss: 0.00001156
Iteration 42/1000 | Loss: 0.00001156
Iteration 43/1000 | Loss: 0.00001155
Iteration 44/1000 | Loss: 0.00001155
Iteration 45/1000 | Loss: 0.00001155
Iteration 46/1000 | Loss: 0.00001155
Iteration 47/1000 | Loss: 0.00001154
Iteration 48/1000 | Loss: 0.00001154
Iteration 49/1000 | Loss: 0.00001154
Iteration 50/1000 | Loss: 0.00001154
Iteration 51/1000 | Loss: 0.00001154
Iteration 52/1000 | Loss: 0.00001154
Iteration 53/1000 | Loss: 0.00001153
Iteration 54/1000 | Loss: 0.00001153
Iteration 55/1000 | Loss: 0.00001153
Iteration 56/1000 | Loss: 0.00001153
Iteration 57/1000 | Loss: 0.00001153
Iteration 58/1000 | Loss: 0.00001153
Iteration 59/1000 | Loss: 0.00001152
Iteration 60/1000 | Loss: 0.00001152
Iteration 61/1000 | Loss: 0.00001152
Iteration 62/1000 | Loss: 0.00001152
Iteration 63/1000 | Loss: 0.00001152
Iteration 64/1000 | Loss: 0.00001152
Iteration 65/1000 | Loss: 0.00001152
Iteration 66/1000 | Loss: 0.00001152
Iteration 67/1000 | Loss: 0.00001152
Iteration 68/1000 | Loss: 0.00001151
Iteration 69/1000 | Loss: 0.00001151
Iteration 70/1000 | Loss: 0.00001151
Iteration 71/1000 | Loss: 0.00001151
Iteration 72/1000 | Loss: 0.00001151
Iteration 73/1000 | Loss: 0.00001151
Iteration 74/1000 | Loss: 0.00001151
Iteration 75/1000 | Loss: 0.00001151
Iteration 76/1000 | Loss: 0.00001151
Iteration 77/1000 | Loss: 0.00001151
Iteration 78/1000 | Loss: 0.00001151
Iteration 79/1000 | Loss: 0.00001151
Iteration 80/1000 | Loss: 0.00001150
Iteration 81/1000 | Loss: 0.00001150
Iteration 82/1000 | Loss: 0.00001150
Iteration 83/1000 | Loss: 0.00001150
Iteration 84/1000 | Loss: 0.00001150
Iteration 85/1000 | Loss: 0.00001149
Iteration 86/1000 | Loss: 0.00001149
Iteration 87/1000 | Loss: 0.00001149
Iteration 88/1000 | Loss: 0.00001149
Iteration 89/1000 | Loss: 0.00001149
Iteration 90/1000 | Loss: 0.00001149
Iteration 91/1000 | Loss: 0.00001148
Iteration 92/1000 | Loss: 0.00001148
Iteration 93/1000 | Loss: 0.00001148
Iteration 94/1000 | Loss: 0.00001148
Iteration 95/1000 | Loss: 0.00001148
Iteration 96/1000 | Loss: 0.00001148
Iteration 97/1000 | Loss: 0.00001148
Iteration 98/1000 | Loss: 0.00001148
Iteration 99/1000 | Loss: 0.00001148
Iteration 100/1000 | Loss: 0.00001148
Iteration 101/1000 | Loss: 0.00001147
Iteration 102/1000 | Loss: 0.00001147
Iteration 103/1000 | Loss: 0.00001147
Iteration 104/1000 | Loss: 0.00001147
Iteration 105/1000 | Loss: 0.00001147
Iteration 106/1000 | Loss: 0.00001147
Iteration 107/1000 | Loss: 0.00001147
Iteration 108/1000 | Loss: 0.00001147
Iteration 109/1000 | Loss: 0.00001147
Iteration 110/1000 | Loss: 0.00001147
Iteration 111/1000 | Loss: 0.00001147
Iteration 112/1000 | Loss: 0.00001147
Iteration 113/1000 | Loss: 0.00001146
Iteration 114/1000 | Loss: 0.00001146
Iteration 115/1000 | Loss: 0.00001146
Iteration 116/1000 | Loss: 0.00001146
Iteration 117/1000 | Loss: 0.00001145
Iteration 118/1000 | Loss: 0.00001144
Iteration 119/1000 | Loss: 0.00001144
Iteration 120/1000 | Loss: 0.00001144
Iteration 121/1000 | Loss: 0.00001143
Iteration 122/1000 | Loss: 0.00001143
Iteration 123/1000 | Loss: 0.00001143
Iteration 124/1000 | Loss: 0.00001142
Iteration 125/1000 | Loss: 0.00001142
Iteration 126/1000 | Loss: 0.00001142
Iteration 127/1000 | Loss: 0.00001142
Iteration 128/1000 | Loss: 0.00001141
Iteration 129/1000 | Loss: 0.00001141
Iteration 130/1000 | Loss: 0.00001141
Iteration 131/1000 | Loss: 0.00001141
Iteration 132/1000 | Loss: 0.00001141
Iteration 133/1000 | Loss: 0.00001141
Iteration 134/1000 | Loss: 0.00001141
Iteration 135/1000 | Loss: 0.00001141
Iteration 136/1000 | Loss: 0.00001141
Iteration 137/1000 | Loss: 0.00001141
Iteration 138/1000 | Loss: 0.00001141
Iteration 139/1000 | Loss: 0.00001140
Iteration 140/1000 | Loss: 0.00001140
Iteration 141/1000 | Loss: 0.00001140
Iteration 142/1000 | Loss: 0.00001140
Iteration 143/1000 | Loss: 0.00001140
Iteration 144/1000 | Loss: 0.00001140
Iteration 145/1000 | Loss: 0.00001139
Iteration 146/1000 | Loss: 0.00001139
Iteration 147/1000 | Loss: 0.00001139
Iteration 148/1000 | Loss: 0.00001139
Iteration 149/1000 | Loss: 0.00001139
Iteration 150/1000 | Loss: 0.00001139
Iteration 151/1000 | Loss: 0.00001139
Iteration 152/1000 | Loss: 0.00001139
Iteration 153/1000 | Loss: 0.00001139
Iteration 154/1000 | Loss: 0.00001139
Iteration 155/1000 | Loss: 0.00001139
Iteration 156/1000 | Loss: 0.00001138
Iteration 157/1000 | Loss: 0.00001138
Iteration 158/1000 | Loss: 0.00001138
Iteration 159/1000 | Loss: 0.00001138
Iteration 160/1000 | Loss: 0.00001138
Iteration 161/1000 | Loss: 0.00001138
Iteration 162/1000 | Loss: 0.00001138
Iteration 163/1000 | Loss: 0.00001138
Iteration 164/1000 | Loss: 0.00001138
Iteration 165/1000 | Loss: 0.00001138
Iteration 166/1000 | Loss: 0.00001138
Iteration 167/1000 | Loss: 0.00001138
Iteration 168/1000 | Loss: 0.00001138
Iteration 169/1000 | Loss: 0.00001138
Iteration 170/1000 | Loss: 0.00001138
Iteration 171/1000 | Loss: 0.00001138
Iteration 172/1000 | Loss: 0.00001138
Iteration 173/1000 | Loss: 0.00001138
Iteration 174/1000 | Loss: 0.00001138
Iteration 175/1000 | Loss: 0.00001138
Iteration 176/1000 | Loss: 0.00001138
Iteration 177/1000 | Loss: 0.00001138
Iteration 178/1000 | Loss: 0.00001138
Iteration 179/1000 | Loss: 0.00001138
Iteration 180/1000 | Loss: 0.00001138
Iteration 181/1000 | Loss: 0.00001138
Iteration 182/1000 | Loss: 0.00001138
Iteration 183/1000 | Loss: 0.00001138
Iteration 184/1000 | Loss: 0.00001138
Iteration 185/1000 | Loss: 0.00001138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.1380524483683985e-05, 1.1380524483683985e-05, 1.1380524483683985e-05, 1.1380524483683985e-05, 1.1380524483683985e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1380524483683985e-05

Optimization complete. Final v2v error: 2.896970510482788 mm

Highest mean error: 3.4625065326690674 mm for frame 198

Lowest mean error: 2.590942859649658 mm for frame 140

Saving results

Total time: 51.45374155044556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803306
Iteration 2/25 | Loss: 0.00126034
Iteration 3/25 | Loss: 0.00096703
Iteration 4/25 | Loss: 0.00088645
Iteration 5/25 | Loss: 0.00086412
Iteration 6/25 | Loss: 0.00083097
Iteration 7/25 | Loss: 0.00083564
Iteration 8/25 | Loss: 0.00081235
Iteration 9/25 | Loss: 0.00079539
Iteration 10/25 | Loss: 0.00079410
Iteration 11/25 | Loss: 0.00079136
Iteration 12/25 | Loss: 0.00079061
Iteration 13/25 | Loss: 0.00079018
Iteration 14/25 | Loss: 0.00078948
Iteration 15/25 | Loss: 0.00078893
Iteration 16/25 | Loss: 0.00079079
Iteration 17/25 | Loss: 0.00078961
Iteration 18/25 | Loss: 0.00078782
Iteration 19/25 | Loss: 0.00078780
Iteration 20/25 | Loss: 0.00079010
Iteration 21/25 | Loss: 0.00079236
Iteration 22/25 | Loss: 0.00079009
Iteration 23/25 | Loss: 0.00079119
Iteration 24/25 | Loss: 0.00078881
Iteration 25/25 | Loss: 0.00078915

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38444853
Iteration 2/25 | Loss: 0.00096389
Iteration 3/25 | Loss: 0.00090332
Iteration 4/25 | Loss: 0.00090332
Iteration 5/25 | Loss: 0.00090332
Iteration 6/25 | Loss: 0.00090331
Iteration 7/25 | Loss: 0.00090331
Iteration 8/25 | Loss: 0.00090331
Iteration 9/25 | Loss: 0.00090331
Iteration 10/25 | Loss: 0.00090331
Iteration 11/25 | Loss: 0.00090331
Iteration 12/25 | Loss: 0.00090331
Iteration 13/25 | Loss: 0.00090331
Iteration 14/25 | Loss: 0.00090331
Iteration 15/25 | Loss: 0.00090331
Iteration 16/25 | Loss: 0.00090331
Iteration 17/25 | Loss: 0.00090331
Iteration 18/25 | Loss: 0.00090331
Iteration 19/25 | Loss: 0.00090331
Iteration 20/25 | Loss: 0.00090331
Iteration 21/25 | Loss: 0.00090331
Iteration 22/25 | Loss: 0.00090331
Iteration 23/25 | Loss: 0.00090331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009033141541294754, 0.0009033141541294754, 0.0009033141541294754, 0.0009033141541294754, 0.0009033141541294754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009033141541294754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090331
Iteration 2/1000 | Loss: 0.00046573
Iteration 3/1000 | Loss: 0.00065848
Iteration 4/1000 | Loss: 0.00058602
Iteration 5/1000 | Loss: 0.00060085
Iteration 6/1000 | Loss: 0.00013357
Iteration 7/1000 | Loss: 0.00012791
Iteration 8/1000 | Loss: 0.00013722
Iteration 9/1000 | Loss: 0.00012860
Iteration 10/1000 | Loss: 0.00065116
Iteration 11/1000 | Loss: 0.00134595
Iteration 12/1000 | Loss: 0.00127367
Iteration 13/1000 | Loss: 0.00055855
Iteration 14/1000 | Loss: 0.00056827
Iteration 15/1000 | Loss: 0.00131122
Iteration 16/1000 | Loss: 0.00039370
Iteration 17/1000 | Loss: 0.00057490
Iteration 18/1000 | Loss: 0.00049640
Iteration 19/1000 | Loss: 0.00047851
Iteration 20/1000 | Loss: 0.00093997
Iteration 21/1000 | Loss: 0.00039184
Iteration 22/1000 | Loss: 0.00017960
Iteration 23/1000 | Loss: 0.00035656
Iteration 24/1000 | Loss: 0.00017459
Iteration 25/1000 | Loss: 0.00063996
Iteration 26/1000 | Loss: 0.00031889
Iteration 27/1000 | Loss: 0.00017416
Iteration 28/1000 | Loss: 0.00015090
Iteration 29/1000 | Loss: 0.00012442
Iteration 30/1000 | Loss: 0.00070482
Iteration 31/1000 | Loss: 0.00033214
Iteration 32/1000 | Loss: 0.00015452
Iteration 33/1000 | Loss: 0.00020661
Iteration 34/1000 | Loss: 0.00008553
Iteration 35/1000 | Loss: 0.00012406
Iteration 36/1000 | Loss: 0.00011212
Iteration 37/1000 | Loss: 0.00008965
Iteration 38/1000 | Loss: 0.00007743
Iteration 39/1000 | Loss: 0.00019617
Iteration 40/1000 | Loss: 0.00023545
Iteration 41/1000 | Loss: 0.00018581
Iteration 42/1000 | Loss: 0.00008948
Iteration 43/1000 | Loss: 0.00003903
Iteration 44/1000 | Loss: 0.00003715
Iteration 45/1000 | Loss: 0.00016873
Iteration 46/1000 | Loss: 0.00015768
Iteration 47/1000 | Loss: 0.00012801
Iteration 48/1000 | Loss: 0.00020644
Iteration 49/1000 | Loss: 0.00026753
Iteration 50/1000 | Loss: 0.00016060
Iteration 51/1000 | Loss: 0.00046835
Iteration 52/1000 | Loss: 0.00017963
Iteration 53/1000 | Loss: 0.00008435
Iteration 54/1000 | Loss: 0.00007707
Iteration 55/1000 | Loss: 0.00005615
Iteration 56/1000 | Loss: 0.00003534
Iteration 57/1000 | Loss: 0.00022579
Iteration 58/1000 | Loss: 0.00010387
Iteration 59/1000 | Loss: 0.00039208
Iteration 60/1000 | Loss: 0.00016321
Iteration 61/1000 | Loss: 0.00023236
Iteration 62/1000 | Loss: 0.00031206
Iteration 63/1000 | Loss: 0.00029269
Iteration 64/1000 | Loss: 0.00042429
Iteration 65/1000 | Loss: 0.00024409
Iteration 66/1000 | Loss: 0.00033653
Iteration 67/1000 | Loss: 0.00011347
Iteration 68/1000 | Loss: 0.00021403
Iteration 69/1000 | Loss: 0.00019714
Iteration 70/1000 | Loss: 0.00026753
Iteration 71/1000 | Loss: 0.00005063
Iteration 72/1000 | Loss: 0.00023801
Iteration 73/1000 | Loss: 0.00005043
Iteration 74/1000 | Loss: 0.00029014
Iteration 75/1000 | Loss: 0.00007979
Iteration 76/1000 | Loss: 0.00030857
Iteration 77/1000 | Loss: 0.00040099
Iteration 78/1000 | Loss: 0.00040309
Iteration 79/1000 | Loss: 0.00060980
Iteration 80/1000 | Loss: 0.00045432
Iteration 81/1000 | Loss: 0.00037982
Iteration 82/1000 | Loss: 0.00025868
Iteration 83/1000 | Loss: 0.00066758
Iteration 84/1000 | Loss: 0.00006057
Iteration 85/1000 | Loss: 0.00008708
Iteration 86/1000 | Loss: 0.00013990
Iteration 87/1000 | Loss: 0.00046267
Iteration 88/1000 | Loss: 0.00014529
Iteration 89/1000 | Loss: 0.00003395
Iteration 90/1000 | Loss: 0.00003299
Iteration 91/1000 | Loss: 0.00015525
Iteration 92/1000 | Loss: 0.00006935
Iteration 93/1000 | Loss: 0.00018024
Iteration 94/1000 | Loss: 0.00052259
Iteration 95/1000 | Loss: 0.00040036
Iteration 96/1000 | Loss: 0.00037603
Iteration 97/1000 | Loss: 0.00011792
Iteration 98/1000 | Loss: 0.00019889
Iteration 99/1000 | Loss: 0.00003395
Iteration 100/1000 | Loss: 0.00003311
Iteration 101/1000 | Loss: 0.00021649
Iteration 102/1000 | Loss: 0.00034912
Iteration 103/1000 | Loss: 0.00037812
Iteration 104/1000 | Loss: 0.00036011
Iteration 105/1000 | Loss: 0.00033793
Iteration 106/1000 | Loss: 0.00018509
Iteration 107/1000 | Loss: 0.00022280
Iteration 108/1000 | Loss: 0.00039367
Iteration 109/1000 | Loss: 0.00020324
Iteration 110/1000 | Loss: 0.00012652
Iteration 111/1000 | Loss: 0.00004567
Iteration 112/1000 | Loss: 0.00003214
Iteration 113/1000 | Loss: 0.00019123
Iteration 114/1000 | Loss: 0.00036463
Iteration 115/1000 | Loss: 0.00040823
Iteration 116/1000 | Loss: 0.00029130
Iteration 117/1000 | Loss: 0.00033297
Iteration 118/1000 | Loss: 0.00004291
Iteration 119/1000 | Loss: 0.00003555
Iteration 120/1000 | Loss: 0.00019862
Iteration 121/1000 | Loss: 0.00031220
Iteration 122/1000 | Loss: 0.00025123
Iteration 123/1000 | Loss: 0.00004167
Iteration 124/1000 | Loss: 0.00003174
Iteration 125/1000 | Loss: 0.00002924
Iteration 126/1000 | Loss: 0.00002835
Iteration 127/1000 | Loss: 0.00002764
Iteration 128/1000 | Loss: 0.00002730
Iteration 129/1000 | Loss: 0.00002704
Iteration 130/1000 | Loss: 0.00002685
Iteration 131/1000 | Loss: 0.00002667
Iteration 132/1000 | Loss: 0.00002655
Iteration 133/1000 | Loss: 0.00002654
Iteration 134/1000 | Loss: 0.00002654
Iteration 135/1000 | Loss: 0.00002645
Iteration 136/1000 | Loss: 0.00002641
Iteration 137/1000 | Loss: 0.00002638
Iteration 138/1000 | Loss: 0.00002637
Iteration 139/1000 | Loss: 0.00002637
Iteration 140/1000 | Loss: 0.00002636
Iteration 141/1000 | Loss: 0.00002636
Iteration 142/1000 | Loss: 0.00002636
Iteration 143/1000 | Loss: 0.00002635
Iteration 144/1000 | Loss: 0.00002635
Iteration 145/1000 | Loss: 0.00002635
Iteration 146/1000 | Loss: 0.00002634
Iteration 147/1000 | Loss: 0.00002633
Iteration 148/1000 | Loss: 0.00002631
Iteration 149/1000 | Loss: 0.00002628
Iteration 150/1000 | Loss: 0.00002628
Iteration 151/1000 | Loss: 0.00002627
Iteration 152/1000 | Loss: 0.00002617
Iteration 153/1000 | Loss: 0.00002615
Iteration 154/1000 | Loss: 0.00002614
Iteration 155/1000 | Loss: 0.00002613
Iteration 156/1000 | Loss: 0.00002612
Iteration 157/1000 | Loss: 0.00002607
Iteration 158/1000 | Loss: 0.00002606
Iteration 159/1000 | Loss: 0.00002606
Iteration 160/1000 | Loss: 0.00002606
Iteration 161/1000 | Loss: 0.00002606
Iteration 162/1000 | Loss: 0.00002606
Iteration 163/1000 | Loss: 0.00002606
Iteration 164/1000 | Loss: 0.00002606
Iteration 165/1000 | Loss: 0.00002606
Iteration 166/1000 | Loss: 0.00002606
Iteration 167/1000 | Loss: 0.00002606
Iteration 168/1000 | Loss: 0.00002605
Iteration 169/1000 | Loss: 0.00002605
Iteration 170/1000 | Loss: 0.00002605
Iteration 171/1000 | Loss: 0.00002604
Iteration 172/1000 | Loss: 0.00002604
Iteration 173/1000 | Loss: 0.00002604
Iteration 174/1000 | Loss: 0.00002604
Iteration 175/1000 | Loss: 0.00002604
Iteration 176/1000 | Loss: 0.00002604
Iteration 177/1000 | Loss: 0.00002604
Iteration 178/1000 | Loss: 0.00002604
Iteration 179/1000 | Loss: 0.00002604
Iteration 180/1000 | Loss: 0.00002604
Iteration 181/1000 | Loss: 0.00002604
Iteration 182/1000 | Loss: 0.00002604
Iteration 183/1000 | Loss: 0.00002604
Iteration 184/1000 | Loss: 0.00002604
Iteration 185/1000 | Loss: 0.00002603
Iteration 186/1000 | Loss: 0.00002603
Iteration 187/1000 | Loss: 0.00002603
Iteration 188/1000 | Loss: 0.00002603
Iteration 189/1000 | Loss: 0.00002603
Iteration 190/1000 | Loss: 0.00002603
Iteration 191/1000 | Loss: 0.00002603
Iteration 192/1000 | Loss: 0.00002603
Iteration 193/1000 | Loss: 0.00002603
Iteration 194/1000 | Loss: 0.00002603
Iteration 195/1000 | Loss: 0.00002603
Iteration 196/1000 | Loss: 0.00002603
Iteration 197/1000 | Loss: 0.00002603
Iteration 198/1000 | Loss: 0.00002603
Iteration 199/1000 | Loss: 0.00002603
Iteration 200/1000 | Loss: 0.00002603
Iteration 201/1000 | Loss: 0.00002603
Iteration 202/1000 | Loss: 0.00002603
Iteration 203/1000 | Loss: 0.00002603
Iteration 204/1000 | Loss: 0.00002603
Iteration 205/1000 | Loss: 0.00002603
Iteration 206/1000 | Loss: 0.00002603
Iteration 207/1000 | Loss: 0.00002603
Iteration 208/1000 | Loss: 0.00002603
Iteration 209/1000 | Loss: 0.00002603
Iteration 210/1000 | Loss: 0.00002603
Iteration 211/1000 | Loss: 0.00002603
Iteration 212/1000 | Loss: 0.00002603
Iteration 213/1000 | Loss: 0.00002603
Iteration 214/1000 | Loss: 0.00002603
Iteration 215/1000 | Loss: 0.00002603
Iteration 216/1000 | Loss: 0.00002603
Iteration 217/1000 | Loss: 0.00002603
Iteration 218/1000 | Loss: 0.00002603
Iteration 219/1000 | Loss: 0.00002603
Iteration 220/1000 | Loss: 0.00002603
Iteration 221/1000 | Loss: 0.00002603
Iteration 222/1000 | Loss: 0.00002603
Iteration 223/1000 | Loss: 0.00002603
Iteration 224/1000 | Loss: 0.00002603
Iteration 225/1000 | Loss: 0.00002603
Iteration 226/1000 | Loss: 0.00002603
Iteration 227/1000 | Loss: 0.00002603
Iteration 228/1000 | Loss: 0.00002603
Iteration 229/1000 | Loss: 0.00002603
Iteration 230/1000 | Loss: 0.00002603
Iteration 231/1000 | Loss: 0.00002603
Iteration 232/1000 | Loss: 0.00002603
Iteration 233/1000 | Loss: 0.00002603
Iteration 234/1000 | Loss: 0.00002603
Iteration 235/1000 | Loss: 0.00002603
Iteration 236/1000 | Loss: 0.00002603
Iteration 237/1000 | Loss: 0.00002603
Iteration 238/1000 | Loss: 0.00002603
Iteration 239/1000 | Loss: 0.00002603
Iteration 240/1000 | Loss: 0.00002603
Iteration 241/1000 | Loss: 0.00002603
Iteration 242/1000 | Loss: 0.00002603
Iteration 243/1000 | Loss: 0.00002603
Iteration 244/1000 | Loss: 0.00002603
Iteration 245/1000 | Loss: 0.00002603
Iteration 246/1000 | Loss: 0.00002603
Iteration 247/1000 | Loss: 0.00002603
Iteration 248/1000 | Loss: 0.00002603
Iteration 249/1000 | Loss: 0.00002603
Iteration 250/1000 | Loss: 0.00002603
Iteration 251/1000 | Loss: 0.00002603
Iteration 252/1000 | Loss: 0.00002603
Iteration 253/1000 | Loss: 0.00002603
Iteration 254/1000 | Loss: 0.00002603
Iteration 255/1000 | Loss: 0.00002603
Iteration 256/1000 | Loss: 0.00002603
Iteration 257/1000 | Loss: 0.00002603
Iteration 258/1000 | Loss: 0.00002603
Iteration 259/1000 | Loss: 0.00002603
Iteration 260/1000 | Loss: 0.00002603
Iteration 261/1000 | Loss: 0.00002603
Iteration 262/1000 | Loss: 0.00002603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 262. Stopping optimization.
Last 5 losses: [2.6031035304185934e-05, 2.6031035304185934e-05, 2.6031035304185934e-05, 2.6031035304185934e-05, 2.6031035304185934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6031035304185934e-05

Optimization complete. Final v2v error: 4.226919174194336 mm

Highest mean error: 5.555064678192139 mm for frame 184

Lowest mean error: 3.8342857360839844 mm for frame 17

Saving results

Total time: 277.3520815372467
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824962
Iteration 2/25 | Loss: 0.00088581
Iteration 3/25 | Loss: 0.00064580
Iteration 4/25 | Loss: 0.00062217
Iteration 5/25 | Loss: 0.00061610
Iteration 6/25 | Loss: 0.00061351
Iteration 7/25 | Loss: 0.00061271
Iteration 8/25 | Loss: 0.00061271
Iteration 9/25 | Loss: 0.00061271
Iteration 10/25 | Loss: 0.00061271
Iteration 11/25 | Loss: 0.00061271
Iteration 12/25 | Loss: 0.00061271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006127067608758807, 0.0006127067608758807, 0.0006127067608758807, 0.0006127067608758807, 0.0006127067608758807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006127067608758807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46385777
Iteration 2/25 | Loss: 0.00031772
Iteration 3/25 | Loss: 0.00031772
Iteration 4/25 | Loss: 0.00031772
Iteration 5/25 | Loss: 0.00031772
Iteration 6/25 | Loss: 0.00031772
Iteration 7/25 | Loss: 0.00031772
Iteration 8/25 | Loss: 0.00031772
Iteration 9/25 | Loss: 0.00031772
Iteration 10/25 | Loss: 0.00031772
Iteration 11/25 | Loss: 0.00031772
Iteration 12/25 | Loss: 0.00031772
Iteration 13/25 | Loss: 0.00031772
Iteration 14/25 | Loss: 0.00031772
Iteration 15/25 | Loss: 0.00031772
Iteration 16/25 | Loss: 0.00031772
Iteration 17/25 | Loss: 0.00031772
Iteration 18/25 | Loss: 0.00031772
Iteration 19/25 | Loss: 0.00031772
Iteration 20/25 | Loss: 0.00031772
Iteration 21/25 | Loss: 0.00031772
Iteration 22/25 | Loss: 0.00031772
Iteration 23/25 | Loss: 0.00031772
Iteration 24/25 | Loss: 0.00031772
Iteration 25/25 | Loss: 0.00031772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031772
Iteration 2/1000 | Loss: 0.00002321
Iteration 3/1000 | Loss: 0.00001516
Iteration 4/1000 | Loss: 0.00001374
Iteration 5/1000 | Loss: 0.00001299
Iteration 6/1000 | Loss: 0.00001250
Iteration 7/1000 | Loss: 0.00001217
Iteration 8/1000 | Loss: 0.00001197
Iteration 9/1000 | Loss: 0.00001197
Iteration 10/1000 | Loss: 0.00001188
Iteration 11/1000 | Loss: 0.00001170
Iteration 12/1000 | Loss: 0.00001170
Iteration 13/1000 | Loss: 0.00001161
Iteration 14/1000 | Loss: 0.00001155
Iteration 15/1000 | Loss: 0.00001154
Iteration 16/1000 | Loss: 0.00001154
Iteration 17/1000 | Loss: 0.00001153
Iteration 18/1000 | Loss: 0.00001152
Iteration 19/1000 | Loss: 0.00001148
Iteration 20/1000 | Loss: 0.00001148
Iteration 21/1000 | Loss: 0.00001147
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001144
Iteration 24/1000 | Loss: 0.00001143
Iteration 25/1000 | Loss: 0.00001143
Iteration 26/1000 | Loss: 0.00001142
Iteration 27/1000 | Loss: 0.00001142
Iteration 28/1000 | Loss: 0.00001142
Iteration 29/1000 | Loss: 0.00001142
Iteration 30/1000 | Loss: 0.00001142
Iteration 31/1000 | Loss: 0.00001141
Iteration 32/1000 | Loss: 0.00001141
Iteration 33/1000 | Loss: 0.00001141
Iteration 34/1000 | Loss: 0.00001141
Iteration 35/1000 | Loss: 0.00001141
Iteration 36/1000 | Loss: 0.00001141
Iteration 37/1000 | Loss: 0.00001141
Iteration 38/1000 | Loss: 0.00001141
Iteration 39/1000 | Loss: 0.00001141
Iteration 40/1000 | Loss: 0.00001141
Iteration 41/1000 | Loss: 0.00001141
Iteration 42/1000 | Loss: 0.00001141
Iteration 43/1000 | Loss: 0.00001139
Iteration 44/1000 | Loss: 0.00001139
Iteration 45/1000 | Loss: 0.00001138
Iteration 46/1000 | Loss: 0.00001137
Iteration 47/1000 | Loss: 0.00001137
Iteration 48/1000 | Loss: 0.00001136
Iteration 49/1000 | Loss: 0.00001135
Iteration 50/1000 | Loss: 0.00001135
Iteration 51/1000 | Loss: 0.00001134
Iteration 52/1000 | Loss: 0.00001134
Iteration 53/1000 | Loss: 0.00001134
Iteration 54/1000 | Loss: 0.00001134
Iteration 55/1000 | Loss: 0.00001134
Iteration 56/1000 | Loss: 0.00001134
Iteration 57/1000 | Loss: 0.00001133
Iteration 58/1000 | Loss: 0.00001133
Iteration 59/1000 | Loss: 0.00001133
Iteration 60/1000 | Loss: 0.00001132
Iteration 61/1000 | Loss: 0.00001132
Iteration 62/1000 | Loss: 0.00001132
Iteration 63/1000 | Loss: 0.00001132
Iteration 64/1000 | Loss: 0.00001132
Iteration 65/1000 | Loss: 0.00001132
Iteration 66/1000 | Loss: 0.00001132
Iteration 67/1000 | Loss: 0.00001132
Iteration 68/1000 | Loss: 0.00001132
Iteration 69/1000 | Loss: 0.00001132
Iteration 70/1000 | Loss: 0.00001132
Iteration 71/1000 | Loss: 0.00001132
Iteration 72/1000 | Loss: 0.00001132
Iteration 73/1000 | Loss: 0.00001131
Iteration 74/1000 | Loss: 0.00001131
Iteration 75/1000 | Loss: 0.00001131
Iteration 76/1000 | Loss: 0.00001131
Iteration 77/1000 | Loss: 0.00001131
Iteration 78/1000 | Loss: 0.00001131
Iteration 79/1000 | Loss: 0.00001131
Iteration 80/1000 | Loss: 0.00001131
Iteration 81/1000 | Loss: 0.00001131
Iteration 82/1000 | Loss: 0.00001131
Iteration 83/1000 | Loss: 0.00001131
Iteration 84/1000 | Loss: 0.00001131
Iteration 85/1000 | Loss: 0.00001131
Iteration 86/1000 | Loss: 0.00001131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.131017597799655e-05, 1.131017597799655e-05, 1.131017597799655e-05, 1.131017597799655e-05, 1.131017597799655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.131017597799655e-05

Optimization complete. Final v2v error: 2.8195958137512207 mm

Highest mean error: 3.2359063625335693 mm for frame 91

Lowest mean error: 2.667842149734497 mm for frame 32

Saving results

Total time: 33.459635496139526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108785
Iteration 2/25 | Loss: 0.00200789
Iteration 3/25 | Loss: 0.00110671
Iteration 4/25 | Loss: 0.00092705
Iteration 5/25 | Loss: 0.00088990
Iteration 6/25 | Loss: 0.00086775
Iteration 7/25 | Loss: 0.00079766
Iteration 8/25 | Loss: 0.00075908
Iteration 9/25 | Loss: 0.00072432
Iteration 10/25 | Loss: 0.00071268
Iteration 11/25 | Loss: 0.00071242
Iteration 12/25 | Loss: 0.00070561
Iteration 13/25 | Loss: 0.00070202
Iteration 14/25 | Loss: 0.00070088
Iteration 15/25 | Loss: 0.00070057
Iteration 16/25 | Loss: 0.00070045
Iteration 17/25 | Loss: 0.00070038
Iteration 18/25 | Loss: 0.00070037
Iteration 19/25 | Loss: 0.00070037
Iteration 20/25 | Loss: 0.00070037
Iteration 21/25 | Loss: 0.00070037
Iteration 22/25 | Loss: 0.00070037
Iteration 23/25 | Loss: 0.00070037
Iteration 24/25 | Loss: 0.00070037
Iteration 25/25 | Loss: 0.00070037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28368008
Iteration 2/25 | Loss: 0.00024483
Iteration 3/25 | Loss: 0.00024483
Iteration 4/25 | Loss: 0.00024483
Iteration 5/25 | Loss: 0.00024483
Iteration 6/25 | Loss: 0.00024483
Iteration 7/25 | Loss: 0.00024483
Iteration 8/25 | Loss: 0.00024483
Iteration 9/25 | Loss: 0.00024483
Iteration 10/25 | Loss: 0.00024483
Iteration 11/25 | Loss: 0.00024483
Iteration 12/25 | Loss: 0.00024483
Iteration 13/25 | Loss: 0.00024483
Iteration 14/25 | Loss: 0.00024483
Iteration 15/25 | Loss: 0.00024483
Iteration 16/25 | Loss: 0.00024483
Iteration 17/25 | Loss: 0.00024483
Iteration 18/25 | Loss: 0.00024483
Iteration 19/25 | Loss: 0.00024483
Iteration 20/25 | Loss: 0.00024483
Iteration 21/25 | Loss: 0.00024483
Iteration 22/25 | Loss: 0.00024483
Iteration 23/25 | Loss: 0.00024483
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00024483154993504286, 0.00024483154993504286, 0.00024483154993504286, 0.00024483154993504286, 0.00024483154993504286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024483154993504286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024483
Iteration 2/1000 | Loss: 0.00004033
Iteration 3/1000 | Loss: 0.00002990
Iteration 4/1000 | Loss: 0.00005799
Iteration 5/1000 | Loss: 0.00002543
Iteration 6/1000 | Loss: 0.00006713
Iteration 7/1000 | Loss: 0.00002400
Iteration 8/1000 | Loss: 0.00005146
Iteration 9/1000 | Loss: 0.00002357
Iteration 10/1000 | Loss: 0.00002317
Iteration 11/1000 | Loss: 0.00002287
Iteration 12/1000 | Loss: 0.00004076
Iteration 13/1000 | Loss: 0.00005278
Iteration 14/1000 | Loss: 0.00002261
Iteration 15/1000 | Loss: 0.00003266
Iteration 16/1000 | Loss: 0.00002245
Iteration 17/1000 | Loss: 0.00003799
Iteration 18/1000 | Loss: 0.00002429
Iteration 19/1000 | Loss: 0.00002221
Iteration 20/1000 | Loss: 0.00002221
Iteration 21/1000 | Loss: 0.00002221
Iteration 22/1000 | Loss: 0.00002220
Iteration 23/1000 | Loss: 0.00002220
Iteration 24/1000 | Loss: 0.00002218
Iteration 25/1000 | Loss: 0.00003009
Iteration 26/1000 | Loss: 0.00002209
Iteration 27/1000 | Loss: 0.00002206
Iteration 28/1000 | Loss: 0.00002206
Iteration 29/1000 | Loss: 0.00002206
Iteration 30/1000 | Loss: 0.00002206
Iteration 31/1000 | Loss: 0.00002206
Iteration 32/1000 | Loss: 0.00002206
Iteration 33/1000 | Loss: 0.00002206
Iteration 34/1000 | Loss: 0.00002206
Iteration 35/1000 | Loss: 0.00002206
Iteration 36/1000 | Loss: 0.00002206
Iteration 37/1000 | Loss: 0.00002206
Iteration 38/1000 | Loss: 0.00002206
Iteration 39/1000 | Loss: 0.00002206
Iteration 40/1000 | Loss: 0.00002206
Iteration 41/1000 | Loss: 0.00002206
Iteration 42/1000 | Loss: 0.00002206
Iteration 43/1000 | Loss: 0.00002206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 43. Stopping optimization.
Last 5 losses: [2.2056146917748265e-05, 2.2056146917748265e-05, 2.2056146917748265e-05, 2.2056146917748265e-05, 2.2056146917748265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2056146917748265e-05

Optimization complete. Final v2v error: 3.6903491020202637 mm

Highest mean error: 6.114515781402588 mm for frame 76

Lowest mean error: 2.778560161590576 mm for frame 145

Saving results

Total time: 59.48832821846008
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431966
Iteration 2/25 | Loss: 0.00093215
Iteration 3/25 | Loss: 0.00066190
Iteration 4/25 | Loss: 0.00063636
Iteration 5/25 | Loss: 0.00062810
Iteration 6/25 | Loss: 0.00062519
Iteration 7/25 | Loss: 0.00062444
Iteration 8/25 | Loss: 0.00062433
Iteration 9/25 | Loss: 0.00062433
Iteration 10/25 | Loss: 0.00062433
Iteration 11/25 | Loss: 0.00062433
Iteration 12/25 | Loss: 0.00062433
Iteration 13/25 | Loss: 0.00062433
Iteration 14/25 | Loss: 0.00062433
Iteration 15/25 | Loss: 0.00062433
Iteration 16/25 | Loss: 0.00062433
Iteration 17/25 | Loss: 0.00062433
Iteration 18/25 | Loss: 0.00062433
Iteration 19/25 | Loss: 0.00062433
Iteration 20/25 | Loss: 0.00062433
Iteration 21/25 | Loss: 0.00062433
Iteration 22/25 | Loss: 0.00062433
Iteration 23/25 | Loss: 0.00062433
Iteration 24/25 | Loss: 0.00062433
Iteration 25/25 | Loss: 0.00062433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49447930
Iteration 2/25 | Loss: 0.00028944
Iteration 3/25 | Loss: 0.00028942
Iteration 4/25 | Loss: 0.00028942
Iteration 5/25 | Loss: 0.00028942
Iteration 6/25 | Loss: 0.00028942
Iteration 7/25 | Loss: 0.00028942
Iteration 8/25 | Loss: 0.00028941
Iteration 9/25 | Loss: 0.00028941
Iteration 10/25 | Loss: 0.00028941
Iteration 11/25 | Loss: 0.00028941
Iteration 12/25 | Loss: 0.00028941
Iteration 13/25 | Loss: 0.00028941
Iteration 14/25 | Loss: 0.00028941
Iteration 15/25 | Loss: 0.00028941
Iteration 16/25 | Loss: 0.00028941
Iteration 17/25 | Loss: 0.00028941
Iteration 18/25 | Loss: 0.00028941
Iteration 19/25 | Loss: 0.00028941
Iteration 20/25 | Loss: 0.00028941
Iteration 21/25 | Loss: 0.00028941
Iteration 22/25 | Loss: 0.00028941
Iteration 23/25 | Loss: 0.00028941
Iteration 24/25 | Loss: 0.00028941
Iteration 25/25 | Loss: 0.00028941

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028941
Iteration 2/1000 | Loss: 0.00003129
Iteration 3/1000 | Loss: 0.00001771
Iteration 4/1000 | Loss: 0.00001532
Iteration 5/1000 | Loss: 0.00001444
Iteration 6/1000 | Loss: 0.00001391
Iteration 7/1000 | Loss: 0.00001360
Iteration 8/1000 | Loss: 0.00001335
Iteration 9/1000 | Loss: 0.00001330
Iteration 10/1000 | Loss: 0.00001318
Iteration 11/1000 | Loss: 0.00001306
Iteration 12/1000 | Loss: 0.00001306
Iteration 13/1000 | Loss: 0.00001305
Iteration 14/1000 | Loss: 0.00001304
Iteration 15/1000 | Loss: 0.00001303
Iteration 16/1000 | Loss: 0.00001299
Iteration 17/1000 | Loss: 0.00001299
Iteration 18/1000 | Loss: 0.00001294
Iteration 19/1000 | Loss: 0.00001294
Iteration 20/1000 | Loss: 0.00001294
Iteration 21/1000 | Loss: 0.00001293
Iteration 22/1000 | Loss: 0.00001293
Iteration 23/1000 | Loss: 0.00001292
Iteration 24/1000 | Loss: 0.00001292
Iteration 25/1000 | Loss: 0.00001292
Iteration 26/1000 | Loss: 0.00001292
Iteration 27/1000 | Loss: 0.00001291
Iteration 28/1000 | Loss: 0.00001290
Iteration 29/1000 | Loss: 0.00001290
Iteration 30/1000 | Loss: 0.00001290
Iteration 31/1000 | Loss: 0.00001290
Iteration 32/1000 | Loss: 0.00001290
Iteration 33/1000 | Loss: 0.00001290
Iteration 34/1000 | Loss: 0.00001290
Iteration 35/1000 | Loss: 0.00001290
Iteration 36/1000 | Loss: 0.00001289
Iteration 37/1000 | Loss: 0.00001289
Iteration 38/1000 | Loss: 0.00001289
Iteration 39/1000 | Loss: 0.00001289
Iteration 40/1000 | Loss: 0.00001288
Iteration 41/1000 | Loss: 0.00001287
Iteration 42/1000 | Loss: 0.00001287
Iteration 43/1000 | Loss: 0.00001287
Iteration 44/1000 | Loss: 0.00001286
Iteration 45/1000 | Loss: 0.00001286
Iteration 46/1000 | Loss: 0.00001286
Iteration 47/1000 | Loss: 0.00001286
Iteration 48/1000 | Loss: 0.00001286
Iteration 49/1000 | Loss: 0.00001285
Iteration 50/1000 | Loss: 0.00001285
Iteration 51/1000 | Loss: 0.00001285
Iteration 52/1000 | Loss: 0.00001284
Iteration 53/1000 | Loss: 0.00001284
Iteration 54/1000 | Loss: 0.00001284
Iteration 55/1000 | Loss: 0.00001283
Iteration 56/1000 | Loss: 0.00001283
Iteration 57/1000 | Loss: 0.00001283
Iteration 58/1000 | Loss: 0.00001283
Iteration 59/1000 | Loss: 0.00001283
Iteration 60/1000 | Loss: 0.00001283
Iteration 61/1000 | Loss: 0.00001283
Iteration 62/1000 | Loss: 0.00001282
Iteration 63/1000 | Loss: 0.00001282
Iteration 64/1000 | Loss: 0.00001282
Iteration 65/1000 | Loss: 0.00001281
Iteration 66/1000 | Loss: 0.00001281
Iteration 67/1000 | Loss: 0.00001281
Iteration 68/1000 | Loss: 0.00001280
Iteration 69/1000 | Loss: 0.00001280
Iteration 70/1000 | Loss: 0.00001280
Iteration 71/1000 | Loss: 0.00001280
Iteration 72/1000 | Loss: 0.00001280
Iteration 73/1000 | Loss: 0.00001280
Iteration 74/1000 | Loss: 0.00001280
Iteration 75/1000 | Loss: 0.00001279
Iteration 76/1000 | Loss: 0.00001279
Iteration 77/1000 | Loss: 0.00001279
Iteration 78/1000 | Loss: 0.00001279
Iteration 79/1000 | Loss: 0.00001279
Iteration 80/1000 | Loss: 0.00001279
Iteration 81/1000 | Loss: 0.00001279
Iteration 82/1000 | Loss: 0.00001279
Iteration 83/1000 | Loss: 0.00001278
Iteration 84/1000 | Loss: 0.00001278
Iteration 85/1000 | Loss: 0.00001278
Iteration 86/1000 | Loss: 0.00001278
Iteration 87/1000 | Loss: 0.00001278
Iteration 88/1000 | Loss: 0.00001277
Iteration 89/1000 | Loss: 0.00001277
Iteration 90/1000 | Loss: 0.00001277
Iteration 91/1000 | Loss: 0.00001276
Iteration 92/1000 | Loss: 0.00001276
Iteration 93/1000 | Loss: 0.00001276
Iteration 94/1000 | Loss: 0.00001276
Iteration 95/1000 | Loss: 0.00001276
Iteration 96/1000 | Loss: 0.00001276
Iteration 97/1000 | Loss: 0.00001275
Iteration 98/1000 | Loss: 0.00001275
Iteration 99/1000 | Loss: 0.00001275
Iteration 100/1000 | Loss: 0.00001275
Iteration 101/1000 | Loss: 0.00001275
Iteration 102/1000 | Loss: 0.00001275
Iteration 103/1000 | Loss: 0.00001275
Iteration 104/1000 | Loss: 0.00001275
Iteration 105/1000 | Loss: 0.00001275
Iteration 106/1000 | Loss: 0.00001275
Iteration 107/1000 | Loss: 0.00001274
Iteration 108/1000 | Loss: 0.00001274
Iteration 109/1000 | Loss: 0.00001274
Iteration 110/1000 | Loss: 0.00001274
Iteration 111/1000 | Loss: 0.00001274
Iteration 112/1000 | Loss: 0.00001274
Iteration 113/1000 | Loss: 0.00001274
Iteration 114/1000 | Loss: 0.00001274
Iteration 115/1000 | Loss: 0.00001274
Iteration 116/1000 | Loss: 0.00001274
Iteration 117/1000 | Loss: 0.00001274
Iteration 118/1000 | Loss: 0.00001273
Iteration 119/1000 | Loss: 0.00001273
Iteration 120/1000 | Loss: 0.00001273
Iteration 121/1000 | Loss: 0.00001273
Iteration 122/1000 | Loss: 0.00001273
Iteration 123/1000 | Loss: 0.00001273
Iteration 124/1000 | Loss: 0.00001272
Iteration 125/1000 | Loss: 0.00001272
Iteration 126/1000 | Loss: 0.00001272
Iteration 127/1000 | Loss: 0.00001272
Iteration 128/1000 | Loss: 0.00001272
Iteration 129/1000 | Loss: 0.00001272
Iteration 130/1000 | Loss: 0.00001272
Iteration 131/1000 | Loss: 0.00001272
Iteration 132/1000 | Loss: 0.00001272
Iteration 133/1000 | Loss: 0.00001272
Iteration 134/1000 | Loss: 0.00001272
Iteration 135/1000 | Loss: 0.00001272
Iteration 136/1000 | Loss: 0.00001272
Iteration 137/1000 | Loss: 0.00001272
Iteration 138/1000 | Loss: 0.00001272
Iteration 139/1000 | Loss: 0.00001272
Iteration 140/1000 | Loss: 0.00001272
Iteration 141/1000 | Loss: 0.00001272
Iteration 142/1000 | Loss: 0.00001272
Iteration 143/1000 | Loss: 0.00001272
Iteration 144/1000 | Loss: 0.00001272
Iteration 145/1000 | Loss: 0.00001272
Iteration 146/1000 | Loss: 0.00001272
Iteration 147/1000 | Loss: 0.00001272
Iteration 148/1000 | Loss: 0.00001272
Iteration 149/1000 | Loss: 0.00001272
Iteration 150/1000 | Loss: 0.00001272
Iteration 151/1000 | Loss: 0.00001272
Iteration 152/1000 | Loss: 0.00001272
Iteration 153/1000 | Loss: 0.00001272
Iteration 154/1000 | Loss: 0.00001272
Iteration 155/1000 | Loss: 0.00001272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.2722890460281633e-05, 1.2722890460281633e-05, 1.2722890460281633e-05, 1.2722890460281633e-05, 1.2722890460281633e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2722890460281633e-05

Optimization complete. Final v2v error: 2.924189805984497 mm

Highest mean error: 3.9401190280914307 mm for frame 56

Lowest mean error: 2.2632644176483154 mm for frame 119

Saving results

Total time: 36.147557735443115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390320
Iteration 2/25 | Loss: 0.00083877
Iteration 3/25 | Loss: 0.00067323
Iteration 4/25 | Loss: 0.00064020
Iteration 5/25 | Loss: 0.00063437
Iteration 6/25 | Loss: 0.00063235
Iteration 7/25 | Loss: 0.00063156
Iteration 8/25 | Loss: 0.00063138
Iteration 9/25 | Loss: 0.00063138
Iteration 10/25 | Loss: 0.00063138
Iteration 11/25 | Loss: 0.00063138
Iteration 12/25 | Loss: 0.00063138
Iteration 13/25 | Loss: 0.00063138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006313762278296053, 0.0006313762278296053, 0.0006313762278296053, 0.0006313762278296053, 0.0006313762278296053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006313762278296053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48749352
Iteration 2/25 | Loss: 0.00034336
Iteration 3/25 | Loss: 0.00034336
Iteration 4/25 | Loss: 0.00034336
Iteration 5/25 | Loss: 0.00034336
Iteration 6/25 | Loss: 0.00034336
Iteration 7/25 | Loss: 0.00034336
Iteration 8/25 | Loss: 0.00034336
Iteration 9/25 | Loss: 0.00034336
Iteration 10/25 | Loss: 0.00034336
Iteration 11/25 | Loss: 0.00034336
Iteration 12/25 | Loss: 0.00034336
Iteration 13/25 | Loss: 0.00034336
Iteration 14/25 | Loss: 0.00034336
Iteration 15/25 | Loss: 0.00034336
Iteration 16/25 | Loss: 0.00034336
Iteration 17/25 | Loss: 0.00034336
Iteration 18/25 | Loss: 0.00034336
Iteration 19/25 | Loss: 0.00034336
Iteration 20/25 | Loss: 0.00034336
Iteration 21/25 | Loss: 0.00034336
Iteration 22/25 | Loss: 0.00034336
Iteration 23/25 | Loss: 0.00034336
Iteration 24/25 | Loss: 0.00034336
Iteration 25/25 | Loss: 0.00034336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034336
Iteration 2/1000 | Loss: 0.00003211
Iteration 3/1000 | Loss: 0.00001997
Iteration 4/1000 | Loss: 0.00001508
Iteration 5/1000 | Loss: 0.00001415
Iteration 6/1000 | Loss: 0.00001362
Iteration 7/1000 | Loss: 0.00001323
Iteration 8/1000 | Loss: 0.00001283
Iteration 9/1000 | Loss: 0.00001269
Iteration 10/1000 | Loss: 0.00001265
Iteration 11/1000 | Loss: 0.00001264
Iteration 12/1000 | Loss: 0.00001257
Iteration 13/1000 | Loss: 0.00001253
Iteration 14/1000 | Loss: 0.00001250
Iteration 15/1000 | Loss: 0.00001249
Iteration 16/1000 | Loss: 0.00001248
Iteration 17/1000 | Loss: 0.00001247
Iteration 18/1000 | Loss: 0.00001246
Iteration 19/1000 | Loss: 0.00001246
Iteration 20/1000 | Loss: 0.00001244
Iteration 21/1000 | Loss: 0.00001243
Iteration 22/1000 | Loss: 0.00001242
Iteration 23/1000 | Loss: 0.00001242
Iteration 24/1000 | Loss: 0.00001241
Iteration 25/1000 | Loss: 0.00001241
Iteration 26/1000 | Loss: 0.00001241
Iteration 27/1000 | Loss: 0.00001240
Iteration 28/1000 | Loss: 0.00001238
Iteration 29/1000 | Loss: 0.00001238
Iteration 30/1000 | Loss: 0.00001237
Iteration 31/1000 | Loss: 0.00001237
Iteration 32/1000 | Loss: 0.00001237
Iteration 33/1000 | Loss: 0.00001237
Iteration 34/1000 | Loss: 0.00001237
Iteration 35/1000 | Loss: 0.00001237
Iteration 36/1000 | Loss: 0.00001237
Iteration 37/1000 | Loss: 0.00001237
Iteration 38/1000 | Loss: 0.00001237
Iteration 39/1000 | Loss: 0.00001236
Iteration 40/1000 | Loss: 0.00001236
Iteration 41/1000 | Loss: 0.00001236
Iteration 42/1000 | Loss: 0.00001236
Iteration 43/1000 | Loss: 0.00001236
Iteration 44/1000 | Loss: 0.00001236
Iteration 45/1000 | Loss: 0.00001234
Iteration 46/1000 | Loss: 0.00001233
Iteration 47/1000 | Loss: 0.00001233
Iteration 48/1000 | Loss: 0.00001233
Iteration 49/1000 | Loss: 0.00001233
Iteration 50/1000 | Loss: 0.00001232
Iteration 51/1000 | Loss: 0.00001232
Iteration 52/1000 | Loss: 0.00001231
Iteration 53/1000 | Loss: 0.00001231
Iteration 54/1000 | Loss: 0.00001230
Iteration 55/1000 | Loss: 0.00001230
Iteration 56/1000 | Loss: 0.00001227
Iteration 57/1000 | Loss: 0.00001227
Iteration 58/1000 | Loss: 0.00001226
Iteration 59/1000 | Loss: 0.00001226
Iteration 60/1000 | Loss: 0.00001226
Iteration 61/1000 | Loss: 0.00001226
Iteration 62/1000 | Loss: 0.00001226
Iteration 63/1000 | Loss: 0.00001226
Iteration 64/1000 | Loss: 0.00001225
Iteration 65/1000 | Loss: 0.00001224
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001222
Iteration 70/1000 | Loss: 0.00001222
Iteration 71/1000 | Loss: 0.00001222
Iteration 72/1000 | Loss: 0.00001221
Iteration 73/1000 | Loss: 0.00001221
Iteration 74/1000 | Loss: 0.00001221
Iteration 75/1000 | Loss: 0.00001220
Iteration 76/1000 | Loss: 0.00001220
Iteration 77/1000 | Loss: 0.00001219
Iteration 78/1000 | Loss: 0.00001219
Iteration 79/1000 | Loss: 0.00001219
Iteration 80/1000 | Loss: 0.00001219
Iteration 81/1000 | Loss: 0.00001219
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001218
Iteration 85/1000 | Loss: 0.00001218
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001217
Iteration 88/1000 | Loss: 0.00001217
Iteration 89/1000 | Loss: 0.00001217
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001216
Iteration 93/1000 | Loss: 0.00001216
Iteration 94/1000 | Loss: 0.00001216
Iteration 95/1000 | Loss: 0.00001216
Iteration 96/1000 | Loss: 0.00001216
Iteration 97/1000 | Loss: 0.00001216
Iteration 98/1000 | Loss: 0.00001215
Iteration 99/1000 | Loss: 0.00001215
Iteration 100/1000 | Loss: 0.00001215
Iteration 101/1000 | Loss: 0.00001215
Iteration 102/1000 | Loss: 0.00001215
Iteration 103/1000 | Loss: 0.00001215
Iteration 104/1000 | Loss: 0.00001215
Iteration 105/1000 | Loss: 0.00001215
Iteration 106/1000 | Loss: 0.00001215
Iteration 107/1000 | Loss: 0.00001214
Iteration 108/1000 | Loss: 0.00001214
Iteration 109/1000 | Loss: 0.00001214
Iteration 110/1000 | Loss: 0.00001214
Iteration 111/1000 | Loss: 0.00001214
Iteration 112/1000 | Loss: 0.00001213
Iteration 113/1000 | Loss: 0.00001213
Iteration 114/1000 | Loss: 0.00001213
Iteration 115/1000 | Loss: 0.00001213
Iteration 116/1000 | Loss: 0.00001213
Iteration 117/1000 | Loss: 0.00001213
Iteration 118/1000 | Loss: 0.00001213
Iteration 119/1000 | Loss: 0.00001213
Iteration 120/1000 | Loss: 0.00001213
Iteration 121/1000 | Loss: 0.00001213
Iteration 122/1000 | Loss: 0.00001213
Iteration 123/1000 | Loss: 0.00001212
Iteration 124/1000 | Loss: 0.00001212
Iteration 125/1000 | Loss: 0.00001212
Iteration 126/1000 | Loss: 0.00001212
Iteration 127/1000 | Loss: 0.00001211
Iteration 128/1000 | Loss: 0.00001211
Iteration 129/1000 | Loss: 0.00001211
Iteration 130/1000 | Loss: 0.00001211
Iteration 131/1000 | Loss: 0.00001211
Iteration 132/1000 | Loss: 0.00001211
Iteration 133/1000 | Loss: 0.00001210
Iteration 134/1000 | Loss: 0.00001210
Iteration 135/1000 | Loss: 0.00001210
Iteration 136/1000 | Loss: 0.00001210
Iteration 137/1000 | Loss: 0.00001210
Iteration 138/1000 | Loss: 0.00001210
Iteration 139/1000 | Loss: 0.00001210
Iteration 140/1000 | Loss: 0.00001210
Iteration 141/1000 | Loss: 0.00001210
Iteration 142/1000 | Loss: 0.00001210
Iteration 143/1000 | Loss: 0.00001210
Iteration 144/1000 | Loss: 0.00001210
Iteration 145/1000 | Loss: 0.00001209
Iteration 146/1000 | Loss: 0.00001209
Iteration 147/1000 | Loss: 0.00001209
Iteration 148/1000 | Loss: 0.00001209
Iteration 149/1000 | Loss: 0.00001209
Iteration 150/1000 | Loss: 0.00001209
Iteration 151/1000 | Loss: 0.00001209
Iteration 152/1000 | Loss: 0.00001209
Iteration 153/1000 | Loss: 0.00001209
Iteration 154/1000 | Loss: 0.00001209
Iteration 155/1000 | Loss: 0.00001209
Iteration 156/1000 | Loss: 0.00001209
Iteration 157/1000 | Loss: 0.00001208
Iteration 158/1000 | Loss: 0.00001208
Iteration 159/1000 | Loss: 0.00001208
Iteration 160/1000 | Loss: 0.00001208
Iteration 161/1000 | Loss: 0.00001208
Iteration 162/1000 | Loss: 0.00001207
Iteration 163/1000 | Loss: 0.00001207
Iteration 164/1000 | Loss: 0.00001207
Iteration 165/1000 | Loss: 0.00001207
Iteration 166/1000 | Loss: 0.00001207
Iteration 167/1000 | Loss: 0.00001207
Iteration 168/1000 | Loss: 0.00001207
Iteration 169/1000 | Loss: 0.00001207
Iteration 170/1000 | Loss: 0.00001207
Iteration 171/1000 | Loss: 0.00001207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.2073016478097998e-05, 1.2073016478097998e-05, 1.2073016478097998e-05, 1.2073016478097998e-05, 1.2073016478097998e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2073016478097998e-05

Optimization complete. Final v2v error: 2.8558592796325684 mm

Highest mean error: 3.2465522289276123 mm for frame 2

Lowest mean error: 2.6025259494781494 mm for frame 96

Saving results

Total time: 37.37664556503296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867384
Iteration 2/25 | Loss: 0.00127049
Iteration 3/25 | Loss: 0.00086071
Iteration 4/25 | Loss: 0.00078212
Iteration 5/25 | Loss: 0.00076739
Iteration 6/25 | Loss: 0.00078235
Iteration 7/25 | Loss: 0.00077224
Iteration 8/25 | Loss: 0.00075792
Iteration 9/25 | Loss: 0.00075474
Iteration 10/25 | Loss: 0.00075401
Iteration 11/25 | Loss: 0.00075394
Iteration 12/25 | Loss: 0.00075394
Iteration 13/25 | Loss: 0.00075394
Iteration 14/25 | Loss: 0.00075394
Iteration 15/25 | Loss: 0.00075394
Iteration 16/25 | Loss: 0.00075394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000753935775719583, 0.000753935775719583, 0.000753935775719583, 0.000753935775719583, 0.000753935775719583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000753935775719583

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45132375
Iteration 2/25 | Loss: 0.00026160
Iteration 3/25 | Loss: 0.00026157
Iteration 4/25 | Loss: 0.00026156
Iteration 5/25 | Loss: 0.00026156
Iteration 6/25 | Loss: 0.00026156
Iteration 7/25 | Loss: 0.00026156
Iteration 8/25 | Loss: 0.00026156
Iteration 9/25 | Loss: 0.00026156
Iteration 10/25 | Loss: 0.00026156
Iteration 11/25 | Loss: 0.00026156
Iteration 12/25 | Loss: 0.00026156
Iteration 13/25 | Loss: 0.00026156
Iteration 14/25 | Loss: 0.00026156
Iteration 15/25 | Loss: 0.00026156
Iteration 16/25 | Loss: 0.00026156
Iteration 17/25 | Loss: 0.00026156
Iteration 18/25 | Loss: 0.00026156
Iteration 19/25 | Loss: 0.00026156
Iteration 20/25 | Loss: 0.00026156
Iteration 21/25 | Loss: 0.00026156
Iteration 22/25 | Loss: 0.00026156
Iteration 23/25 | Loss: 0.00026156
Iteration 24/25 | Loss: 0.00026156
Iteration 25/25 | Loss: 0.00026156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026156
Iteration 2/1000 | Loss: 0.00004486
Iteration 3/1000 | Loss: 0.00003458
Iteration 4/1000 | Loss: 0.00003153
Iteration 5/1000 | Loss: 0.00002983
Iteration 6/1000 | Loss: 0.00002867
Iteration 7/1000 | Loss: 0.00002796
Iteration 8/1000 | Loss: 0.00002730
Iteration 9/1000 | Loss: 0.00002678
Iteration 10/1000 | Loss: 0.00002648
Iteration 11/1000 | Loss: 0.00002628
Iteration 12/1000 | Loss: 0.00002610
Iteration 13/1000 | Loss: 0.00002606
Iteration 14/1000 | Loss: 0.00002599
Iteration 15/1000 | Loss: 0.00002596
Iteration 16/1000 | Loss: 0.00002595
Iteration 17/1000 | Loss: 0.00002592
Iteration 18/1000 | Loss: 0.00002587
Iteration 19/1000 | Loss: 0.00002586
Iteration 20/1000 | Loss: 0.00002585
Iteration 21/1000 | Loss: 0.00002585
Iteration 22/1000 | Loss: 0.00002584
Iteration 23/1000 | Loss: 0.00002584
Iteration 24/1000 | Loss: 0.00002583
Iteration 25/1000 | Loss: 0.00002582
Iteration 26/1000 | Loss: 0.00002582
Iteration 27/1000 | Loss: 0.00002581
Iteration 28/1000 | Loss: 0.00002581
Iteration 29/1000 | Loss: 0.00002580
Iteration 30/1000 | Loss: 0.00002580
Iteration 31/1000 | Loss: 0.00002580
Iteration 32/1000 | Loss: 0.00002580
Iteration 33/1000 | Loss: 0.00002579
Iteration 34/1000 | Loss: 0.00002579
Iteration 35/1000 | Loss: 0.00002579
Iteration 36/1000 | Loss: 0.00002579
Iteration 37/1000 | Loss: 0.00002579
Iteration 38/1000 | Loss: 0.00002579
Iteration 39/1000 | Loss: 0.00002579
Iteration 40/1000 | Loss: 0.00002579
Iteration 41/1000 | Loss: 0.00002579
Iteration 42/1000 | Loss: 0.00002579
Iteration 43/1000 | Loss: 0.00002579
Iteration 44/1000 | Loss: 0.00002578
Iteration 45/1000 | Loss: 0.00002578
Iteration 46/1000 | Loss: 0.00002578
Iteration 47/1000 | Loss: 0.00002578
Iteration 48/1000 | Loss: 0.00002578
Iteration 49/1000 | Loss: 0.00002577
Iteration 50/1000 | Loss: 0.00002577
Iteration 51/1000 | Loss: 0.00002577
Iteration 52/1000 | Loss: 0.00002576
Iteration 53/1000 | Loss: 0.00002576
Iteration 54/1000 | Loss: 0.00002576
Iteration 55/1000 | Loss: 0.00002575
Iteration 56/1000 | Loss: 0.00002575
Iteration 57/1000 | Loss: 0.00002575
Iteration 58/1000 | Loss: 0.00002575
Iteration 59/1000 | Loss: 0.00002575
Iteration 60/1000 | Loss: 0.00002574
Iteration 61/1000 | Loss: 0.00002574
Iteration 62/1000 | Loss: 0.00002574
Iteration 63/1000 | Loss: 0.00002574
Iteration 64/1000 | Loss: 0.00002574
Iteration 65/1000 | Loss: 0.00002574
Iteration 66/1000 | Loss: 0.00002573
Iteration 67/1000 | Loss: 0.00002573
Iteration 68/1000 | Loss: 0.00002573
Iteration 69/1000 | Loss: 0.00002573
Iteration 70/1000 | Loss: 0.00002573
Iteration 71/1000 | Loss: 0.00002572
Iteration 72/1000 | Loss: 0.00002572
Iteration 73/1000 | Loss: 0.00002572
Iteration 74/1000 | Loss: 0.00002572
Iteration 75/1000 | Loss: 0.00002572
Iteration 76/1000 | Loss: 0.00002572
Iteration 77/1000 | Loss: 0.00002572
Iteration 78/1000 | Loss: 0.00002572
Iteration 79/1000 | Loss: 0.00002572
Iteration 80/1000 | Loss: 0.00002571
Iteration 81/1000 | Loss: 0.00002571
Iteration 82/1000 | Loss: 0.00002571
Iteration 83/1000 | Loss: 0.00002571
Iteration 84/1000 | Loss: 0.00002571
Iteration 85/1000 | Loss: 0.00002571
Iteration 86/1000 | Loss: 0.00002571
Iteration 87/1000 | Loss: 0.00002570
Iteration 88/1000 | Loss: 0.00002570
Iteration 89/1000 | Loss: 0.00002570
Iteration 90/1000 | Loss: 0.00002570
Iteration 91/1000 | Loss: 0.00002570
Iteration 92/1000 | Loss: 0.00002570
Iteration 93/1000 | Loss: 0.00002570
Iteration 94/1000 | Loss: 0.00002569
Iteration 95/1000 | Loss: 0.00002569
Iteration 96/1000 | Loss: 0.00002569
Iteration 97/1000 | Loss: 0.00002569
Iteration 98/1000 | Loss: 0.00002569
Iteration 99/1000 | Loss: 0.00002568
Iteration 100/1000 | Loss: 0.00002568
Iteration 101/1000 | Loss: 0.00002568
Iteration 102/1000 | Loss: 0.00002568
Iteration 103/1000 | Loss: 0.00002568
Iteration 104/1000 | Loss: 0.00002568
Iteration 105/1000 | Loss: 0.00002567
Iteration 106/1000 | Loss: 0.00002567
Iteration 107/1000 | Loss: 0.00002567
Iteration 108/1000 | Loss: 0.00002567
Iteration 109/1000 | Loss: 0.00002566
Iteration 110/1000 | Loss: 0.00002566
Iteration 111/1000 | Loss: 0.00002566
Iteration 112/1000 | Loss: 0.00002565
Iteration 113/1000 | Loss: 0.00002565
Iteration 114/1000 | Loss: 0.00002565
Iteration 115/1000 | Loss: 0.00002565
Iteration 116/1000 | Loss: 0.00002565
Iteration 117/1000 | Loss: 0.00002565
Iteration 118/1000 | Loss: 0.00002565
Iteration 119/1000 | Loss: 0.00002565
Iteration 120/1000 | Loss: 0.00002565
Iteration 121/1000 | Loss: 0.00002565
Iteration 122/1000 | Loss: 0.00002565
Iteration 123/1000 | Loss: 0.00002564
Iteration 124/1000 | Loss: 0.00002564
Iteration 125/1000 | Loss: 0.00002564
Iteration 126/1000 | Loss: 0.00002564
Iteration 127/1000 | Loss: 0.00002564
Iteration 128/1000 | Loss: 0.00002564
Iteration 129/1000 | Loss: 0.00002564
Iteration 130/1000 | Loss: 0.00002564
Iteration 131/1000 | Loss: 0.00002564
Iteration 132/1000 | Loss: 0.00002564
Iteration 133/1000 | Loss: 0.00002563
Iteration 134/1000 | Loss: 0.00002563
Iteration 135/1000 | Loss: 0.00002563
Iteration 136/1000 | Loss: 0.00002563
Iteration 137/1000 | Loss: 0.00002563
Iteration 138/1000 | Loss: 0.00002563
Iteration 139/1000 | Loss: 0.00002563
Iteration 140/1000 | Loss: 0.00002563
Iteration 141/1000 | Loss: 0.00002563
Iteration 142/1000 | Loss: 0.00002563
Iteration 143/1000 | Loss: 0.00002562
Iteration 144/1000 | Loss: 0.00002562
Iteration 145/1000 | Loss: 0.00002562
Iteration 146/1000 | Loss: 0.00002562
Iteration 147/1000 | Loss: 0.00002562
Iteration 148/1000 | Loss: 0.00002562
Iteration 149/1000 | Loss: 0.00002562
Iteration 150/1000 | Loss: 0.00002562
Iteration 151/1000 | Loss: 0.00002562
Iteration 152/1000 | Loss: 0.00002562
Iteration 153/1000 | Loss: 0.00002562
Iteration 154/1000 | Loss: 0.00002562
Iteration 155/1000 | Loss: 0.00002562
Iteration 156/1000 | Loss: 0.00002562
Iteration 157/1000 | Loss: 0.00002562
Iteration 158/1000 | Loss: 0.00002562
Iteration 159/1000 | Loss: 0.00002562
Iteration 160/1000 | Loss: 0.00002562
Iteration 161/1000 | Loss: 0.00002562
Iteration 162/1000 | Loss: 0.00002562
Iteration 163/1000 | Loss: 0.00002562
Iteration 164/1000 | Loss: 0.00002562
Iteration 165/1000 | Loss: 0.00002562
Iteration 166/1000 | Loss: 0.00002562
Iteration 167/1000 | Loss: 0.00002562
Iteration 168/1000 | Loss: 0.00002562
Iteration 169/1000 | Loss: 0.00002562
Iteration 170/1000 | Loss: 0.00002562
Iteration 171/1000 | Loss: 0.00002562
Iteration 172/1000 | Loss: 0.00002562
Iteration 173/1000 | Loss: 0.00002562
Iteration 174/1000 | Loss: 0.00002562
Iteration 175/1000 | Loss: 0.00002562
Iteration 176/1000 | Loss: 0.00002562
Iteration 177/1000 | Loss: 0.00002562
Iteration 178/1000 | Loss: 0.00002562
Iteration 179/1000 | Loss: 0.00002562
Iteration 180/1000 | Loss: 0.00002562
Iteration 181/1000 | Loss: 0.00002562
Iteration 182/1000 | Loss: 0.00002562
Iteration 183/1000 | Loss: 0.00002562
Iteration 184/1000 | Loss: 0.00002562
Iteration 185/1000 | Loss: 0.00002562
Iteration 186/1000 | Loss: 0.00002562
Iteration 187/1000 | Loss: 0.00002562
Iteration 188/1000 | Loss: 0.00002562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.5620613087085076e-05, 2.5620613087085076e-05, 2.5620613087085076e-05, 2.5620613087085076e-05, 2.5620613087085076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5620613087085076e-05

Optimization complete. Final v2v error: 4.173388957977295 mm

Highest mean error: 4.862131118774414 mm for frame 32

Lowest mean error: 3.5418481826782227 mm for frame 155

Saving results

Total time: 54.1389536857605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01097618
Iteration 2/25 | Loss: 0.00287205
Iteration 3/25 | Loss: 0.00162726
Iteration 4/25 | Loss: 0.00148320
Iteration 5/25 | Loss: 0.00130073
Iteration 6/25 | Loss: 0.00133831
Iteration 7/25 | Loss: 0.00126702
Iteration 8/25 | Loss: 0.00097096
Iteration 9/25 | Loss: 0.00090397
Iteration 10/25 | Loss: 0.00083333
Iteration 11/25 | Loss: 0.00080593
Iteration 12/25 | Loss: 0.00078150
Iteration 13/25 | Loss: 0.00074405
Iteration 14/25 | Loss: 0.00073349
Iteration 15/25 | Loss: 0.00071340
Iteration 16/25 | Loss: 0.00070998
Iteration 17/25 | Loss: 0.00071888
Iteration 18/25 | Loss: 0.00071516
Iteration 19/25 | Loss: 0.00071270
Iteration 20/25 | Loss: 0.00070989
Iteration 21/25 | Loss: 0.00070639
Iteration 22/25 | Loss: 0.00070306
Iteration 23/25 | Loss: 0.00070447
Iteration 24/25 | Loss: 0.00070246
Iteration 25/25 | Loss: 0.00070248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39235306
Iteration 2/25 | Loss: 0.00088441
Iteration 3/25 | Loss: 0.00079441
Iteration 4/25 | Loss: 0.00079440
Iteration 5/25 | Loss: 0.00079440
Iteration 6/25 | Loss: 0.00079440
Iteration 7/25 | Loss: 0.00079440
Iteration 8/25 | Loss: 0.00079440
Iteration 9/25 | Loss: 0.00079440
Iteration 10/25 | Loss: 0.00079440
Iteration 11/25 | Loss: 0.00079440
Iteration 12/25 | Loss: 0.00079440
Iteration 13/25 | Loss: 0.00079440
Iteration 14/25 | Loss: 0.00079440
Iteration 15/25 | Loss: 0.00079440
Iteration 16/25 | Loss: 0.00079440
Iteration 17/25 | Loss: 0.00079440
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007944018580019474, 0.0007944018580019474, 0.0007944018580019474, 0.0007944018580019474, 0.0007944018580019474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007944018580019474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079440
Iteration 2/1000 | Loss: 0.00020516
Iteration 3/1000 | Loss: 0.00017921
Iteration 4/1000 | Loss: 0.00008852
Iteration 5/1000 | Loss: 0.00007345
Iteration 6/1000 | Loss: 0.00011035
Iteration 7/1000 | Loss: 0.00006535
Iteration 8/1000 | Loss: 0.00006298
Iteration 9/1000 | Loss: 0.00013477
Iteration 10/1000 | Loss: 0.00005923
Iteration 11/1000 | Loss: 0.00080745
Iteration 12/1000 | Loss: 0.00867970
Iteration 13/1000 | Loss: 0.01743230
Iteration 14/1000 | Loss: 0.01159487
Iteration 15/1000 | Loss: 0.00101148
Iteration 16/1000 | Loss: 0.00076112
Iteration 17/1000 | Loss: 0.00016550
Iteration 18/1000 | Loss: 0.00157334
Iteration 19/1000 | Loss: 0.00187759
Iteration 20/1000 | Loss: 0.00220566
Iteration 21/1000 | Loss: 0.00133243
Iteration 22/1000 | Loss: 0.00140834
Iteration 23/1000 | Loss: 0.00186340
Iteration 24/1000 | Loss: 0.00228832
Iteration 25/1000 | Loss: 0.00086288
Iteration 26/1000 | Loss: 0.00108152
Iteration 27/1000 | Loss: 0.00121530
Iteration 28/1000 | Loss: 0.00157356
Iteration 29/1000 | Loss: 0.00077350
Iteration 30/1000 | Loss: 0.00185942
Iteration 31/1000 | Loss: 0.00131644
Iteration 32/1000 | Loss: 0.00159409
Iteration 33/1000 | Loss: 0.00210629
Iteration 34/1000 | Loss: 0.00200401
Iteration 35/1000 | Loss: 0.00161507
Iteration 36/1000 | Loss: 0.00202508
Iteration 37/1000 | Loss: 0.00203757
Iteration 38/1000 | Loss: 0.00242268
Iteration 39/1000 | Loss: 0.00266384
Iteration 40/1000 | Loss: 0.00115401
Iteration 41/1000 | Loss: 0.00116475
Iteration 42/1000 | Loss: 0.00243997
Iteration 43/1000 | Loss: 0.00100956
Iteration 44/1000 | Loss: 0.00111994
Iteration 45/1000 | Loss: 0.00308325
Iteration 46/1000 | Loss: 0.00277153
Iteration 47/1000 | Loss: 0.00280412
Iteration 48/1000 | Loss: 0.00321554
Iteration 49/1000 | Loss: 0.00353125
Iteration 50/1000 | Loss: 0.00277339
Iteration 51/1000 | Loss: 0.00235095
Iteration 52/1000 | Loss: 0.00083991
Iteration 53/1000 | Loss: 0.00134037
Iteration 54/1000 | Loss: 0.00131106
Iteration 55/1000 | Loss: 0.00135703
Iteration 56/1000 | Loss: 0.00164692
Iteration 57/1000 | Loss: 0.00169205
Iteration 58/1000 | Loss: 0.00171502
Iteration 59/1000 | Loss: 0.00289950
Iteration 60/1000 | Loss: 0.00131197
Iteration 61/1000 | Loss: 0.00096444
Iteration 62/1000 | Loss: 0.00228918
Iteration 63/1000 | Loss: 0.00095483
Iteration 64/1000 | Loss: 0.00009701
Iteration 65/1000 | Loss: 0.00070613
Iteration 66/1000 | Loss: 0.00049680
Iteration 67/1000 | Loss: 0.00047516
Iteration 68/1000 | Loss: 0.00005046
Iteration 69/1000 | Loss: 0.00057827
Iteration 70/1000 | Loss: 0.00059048
Iteration 71/1000 | Loss: 0.00061279
Iteration 72/1000 | Loss: 0.00059083
Iteration 73/1000 | Loss: 0.00008095
Iteration 74/1000 | Loss: 0.00009392
Iteration 75/1000 | Loss: 0.00040565
Iteration 76/1000 | Loss: 0.00008843
Iteration 77/1000 | Loss: 0.00016161
Iteration 78/1000 | Loss: 0.00008175
Iteration 79/1000 | Loss: 0.00023122
Iteration 80/1000 | Loss: 0.00005786
Iteration 81/1000 | Loss: 0.00067751
Iteration 82/1000 | Loss: 0.00146853
Iteration 83/1000 | Loss: 0.00009538
Iteration 84/1000 | Loss: 0.00003799
Iteration 85/1000 | Loss: 0.00048916
Iteration 86/1000 | Loss: 0.00038669
Iteration 87/1000 | Loss: 0.00003964
Iteration 88/1000 | Loss: 0.00004821
Iteration 89/1000 | Loss: 0.00004982
Iteration 90/1000 | Loss: 0.00002583
Iteration 91/1000 | Loss: 0.00002523
Iteration 92/1000 | Loss: 0.00004964
Iteration 93/1000 | Loss: 0.00055324
Iteration 94/1000 | Loss: 0.00027968
Iteration 95/1000 | Loss: 0.00006779
Iteration 96/1000 | Loss: 0.00006022
Iteration 97/1000 | Loss: 0.00002473
Iteration 98/1000 | Loss: 0.00009986
Iteration 99/1000 | Loss: 0.00002845
Iteration 100/1000 | Loss: 0.00002690
Iteration 101/1000 | Loss: 0.00003694
Iteration 102/1000 | Loss: 0.00002428
Iteration 103/1000 | Loss: 0.00004717
Iteration 104/1000 | Loss: 0.00002404
Iteration 105/1000 | Loss: 0.00061409
Iteration 106/1000 | Loss: 0.00020846
Iteration 107/1000 | Loss: 0.00004718
Iteration 108/1000 | Loss: 0.00004921
Iteration 109/1000 | Loss: 0.00003838
Iteration 110/1000 | Loss: 0.00002688
Iteration 111/1000 | Loss: 0.00002763
Iteration 112/1000 | Loss: 0.00050485
Iteration 113/1000 | Loss: 0.00020547
Iteration 114/1000 | Loss: 0.00003435
Iteration 115/1000 | Loss: 0.00003408
Iteration 116/1000 | Loss: 0.00004216
Iteration 117/1000 | Loss: 0.00002405
Iteration 118/1000 | Loss: 0.00003501
Iteration 119/1000 | Loss: 0.00002467
Iteration 120/1000 | Loss: 0.00003625
Iteration 121/1000 | Loss: 0.00002604
Iteration 122/1000 | Loss: 0.00053346
Iteration 123/1000 | Loss: 0.00030526
Iteration 124/1000 | Loss: 0.00002515
Iteration 125/1000 | Loss: 0.00004085
Iteration 126/1000 | Loss: 0.00002391
Iteration 127/1000 | Loss: 0.00102694
Iteration 128/1000 | Loss: 0.00065570
Iteration 129/1000 | Loss: 0.00096528
Iteration 130/1000 | Loss: 0.00075312
Iteration 131/1000 | Loss: 0.00031769
Iteration 132/1000 | Loss: 0.00074294
Iteration 133/1000 | Loss: 0.00044760
Iteration 134/1000 | Loss: 0.00058192
Iteration 135/1000 | Loss: 0.00031142
Iteration 136/1000 | Loss: 0.00062092
Iteration 137/1000 | Loss: 0.00067284
Iteration 138/1000 | Loss: 0.00012949
Iteration 139/1000 | Loss: 0.00012031
Iteration 140/1000 | Loss: 0.00068771
Iteration 141/1000 | Loss: 0.00032986
Iteration 142/1000 | Loss: 0.00100659
Iteration 143/1000 | Loss: 0.00118481
Iteration 144/1000 | Loss: 0.00031486
Iteration 145/1000 | Loss: 0.00030259
Iteration 146/1000 | Loss: 0.00026240
Iteration 147/1000 | Loss: 0.00003744
Iteration 148/1000 | Loss: 0.00002634
Iteration 149/1000 | Loss: 0.00002205
Iteration 150/1000 | Loss: 0.00017752
Iteration 151/1000 | Loss: 0.00029513
Iteration 152/1000 | Loss: 0.00014786
Iteration 153/1000 | Loss: 0.00034958
Iteration 154/1000 | Loss: 0.00002894
Iteration 155/1000 | Loss: 0.00003024
Iteration 156/1000 | Loss: 0.00002307
Iteration 157/1000 | Loss: 0.00001874
Iteration 158/1000 | Loss: 0.00007382
Iteration 159/1000 | Loss: 0.00001763
Iteration 160/1000 | Loss: 0.00001696
Iteration 161/1000 | Loss: 0.00004420
Iteration 162/1000 | Loss: 0.00008899
Iteration 163/1000 | Loss: 0.00001679
Iteration 164/1000 | Loss: 0.00001636
Iteration 165/1000 | Loss: 0.00001624
Iteration 166/1000 | Loss: 0.00001624
Iteration 167/1000 | Loss: 0.00001624
Iteration 168/1000 | Loss: 0.00001623
Iteration 169/1000 | Loss: 0.00001622
Iteration 170/1000 | Loss: 0.00001619
Iteration 171/1000 | Loss: 0.00001619
Iteration 172/1000 | Loss: 0.00001618
Iteration 173/1000 | Loss: 0.00001615
Iteration 174/1000 | Loss: 0.00003837
Iteration 175/1000 | Loss: 0.00001618
Iteration 176/1000 | Loss: 0.00001606
Iteration 177/1000 | Loss: 0.00001606
Iteration 178/1000 | Loss: 0.00001606
Iteration 179/1000 | Loss: 0.00001606
Iteration 180/1000 | Loss: 0.00001606
Iteration 181/1000 | Loss: 0.00001606
Iteration 182/1000 | Loss: 0.00001605
Iteration 183/1000 | Loss: 0.00001605
Iteration 184/1000 | Loss: 0.00001605
Iteration 185/1000 | Loss: 0.00001604
Iteration 186/1000 | Loss: 0.00001604
Iteration 187/1000 | Loss: 0.00001604
Iteration 188/1000 | Loss: 0.00001604
Iteration 189/1000 | Loss: 0.00001604
Iteration 190/1000 | Loss: 0.00001604
Iteration 191/1000 | Loss: 0.00001603
Iteration 192/1000 | Loss: 0.00001603
Iteration 193/1000 | Loss: 0.00001603
Iteration 194/1000 | Loss: 0.00001602
Iteration 195/1000 | Loss: 0.00001602
Iteration 196/1000 | Loss: 0.00001601
Iteration 197/1000 | Loss: 0.00001601
Iteration 198/1000 | Loss: 0.00001601
Iteration 199/1000 | Loss: 0.00001601
Iteration 200/1000 | Loss: 0.00001601
Iteration 201/1000 | Loss: 0.00001601
Iteration 202/1000 | Loss: 0.00001601
Iteration 203/1000 | Loss: 0.00001601
Iteration 204/1000 | Loss: 0.00001601
Iteration 205/1000 | Loss: 0.00001601
Iteration 206/1000 | Loss: 0.00001601
Iteration 207/1000 | Loss: 0.00001601
Iteration 208/1000 | Loss: 0.00001601
Iteration 209/1000 | Loss: 0.00001601
Iteration 210/1000 | Loss: 0.00001601
Iteration 211/1000 | Loss: 0.00001601
Iteration 212/1000 | Loss: 0.00001601
Iteration 213/1000 | Loss: 0.00001601
Iteration 214/1000 | Loss: 0.00001601
Iteration 215/1000 | Loss: 0.00001601
Iteration 216/1000 | Loss: 0.00001601
Iteration 217/1000 | Loss: 0.00001601
Iteration 218/1000 | Loss: 0.00001601
Iteration 219/1000 | Loss: 0.00001601
Iteration 220/1000 | Loss: 0.00001601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.6006742953322828e-05, 1.6006742953322828e-05, 1.6006742953322828e-05, 1.6006742953322828e-05, 1.6006742953322828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6006742953322828e-05

Optimization complete. Final v2v error: 3.0711283683776855 mm

Highest mean error: 11.86572551727295 mm for frame 4

Lowest mean error: 2.861788511276245 mm for frame 13

Saving results

Total time: 284.5066523551941
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00582967
Iteration 2/25 | Loss: 0.00118840
Iteration 3/25 | Loss: 0.00082166
Iteration 4/25 | Loss: 0.00074831
Iteration 5/25 | Loss: 0.00073011
Iteration 6/25 | Loss: 0.00072452
Iteration 7/25 | Loss: 0.00072305
Iteration 8/25 | Loss: 0.00072255
Iteration 9/25 | Loss: 0.00072231
Iteration 10/25 | Loss: 0.00072228
Iteration 11/25 | Loss: 0.00072228
Iteration 12/25 | Loss: 0.00072228
Iteration 13/25 | Loss: 0.00072228
Iteration 14/25 | Loss: 0.00072228
Iteration 15/25 | Loss: 0.00072228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007222750573419034, 0.0007222750573419034, 0.0007222750573419034, 0.0007222750573419034, 0.0007222750573419034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007222750573419034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56962872
Iteration 2/25 | Loss: 0.00035960
Iteration 3/25 | Loss: 0.00035960
Iteration 4/25 | Loss: 0.00035959
Iteration 5/25 | Loss: 0.00035959
Iteration 6/25 | Loss: 0.00035959
Iteration 7/25 | Loss: 0.00035959
Iteration 8/25 | Loss: 0.00035959
Iteration 9/25 | Loss: 0.00035959
Iteration 10/25 | Loss: 0.00035959
Iteration 11/25 | Loss: 0.00035959
Iteration 12/25 | Loss: 0.00035959
Iteration 13/25 | Loss: 0.00035959
Iteration 14/25 | Loss: 0.00035959
Iteration 15/25 | Loss: 0.00035959
Iteration 16/25 | Loss: 0.00035959
Iteration 17/25 | Loss: 0.00035959
Iteration 18/25 | Loss: 0.00035959
Iteration 19/25 | Loss: 0.00035959
Iteration 20/25 | Loss: 0.00035959
Iteration 21/25 | Loss: 0.00035959
Iteration 22/25 | Loss: 0.00035959
Iteration 23/25 | Loss: 0.00035959
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00035959199885837734, 0.00035959199885837734, 0.00035959199885837734, 0.00035959199885837734, 0.00035959199885837734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00035959199885837734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035959
Iteration 2/1000 | Loss: 0.00003949
Iteration 3/1000 | Loss: 0.00003948
Iteration 4/1000 | Loss: 0.00002867
Iteration 5/1000 | Loss: 0.00002695
Iteration 6/1000 | Loss: 0.00002543
Iteration 7/1000 | Loss: 0.00002472
Iteration 8/1000 | Loss: 0.00002413
Iteration 9/1000 | Loss: 0.00002355
Iteration 10/1000 | Loss: 0.00002299
Iteration 11/1000 | Loss: 0.00053879
Iteration 12/1000 | Loss: 0.00030132
Iteration 13/1000 | Loss: 0.00005154
Iteration 14/1000 | Loss: 0.00004068
Iteration 15/1000 | Loss: 0.00003510
Iteration 16/1000 | Loss: 0.00002963
Iteration 17/1000 | Loss: 0.00002747
Iteration 18/1000 | Loss: 0.00002552
Iteration 19/1000 | Loss: 0.00002432
Iteration 20/1000 | Loss: 0.00002671
Iteration 21/1000 | Loss: 0.00002332
Iteration 22/1000 | Loss: 0.00021789
Iteration 23/1000 | Loss: 0.00010434
Iteration 24/1000 | Loss: 0.00004343
Iteration 25/1000 | Loss: 0.00003514
Iteration 26/1000 | Loss: 0.00015591
Iteration 27/1000 | Loss: 0.00008066
Iteration 28/1000 | Loss: 0.00016882
Iteration 29/1000 | Loss: 0.00002783
Iteration 30/1000 | Loss: 0.00002586
Iteration 31/1000 | Loss: 0.00002491
Iteration 32/1000 | Loss: 0.00029624
Iteration 33/1000 | Loss: 0.00023259
Iteration 34/1000 | Loss: 0.00003449
Iteration 35/1000 | Loss: 0.00002677
Iteration 36/1000 | Loss: 0.00002957
Iteration 37/1000 | Loss: 0.00002399
Iteration 38/1000 | Loss: 0.00028508
Iteration 39/1000 | Loss: 0.00004357
Iteration 40/1000 | Loss: 0.00003346
Iteration 41/1000 | Loss: 0.00003159
Iteration 42/1000 | Loss: 0.00002863
Iteration 43/1000 | Loss: 0.00002698
Iteration 44/1000 | Loss: 0.00002588
Iteration 45/1000 | Loss: 0.00002747
Iteration 46/1000 | Loss: 0.00002499
Iteration 47/1000 | Loss: 0.00002616
Iteration 48/1000 | Loss: 0.00002414
Iteration 49/1000 | Loss: 0.00026381
Iteration 50/1000 | Loss: 0.00006913
Iteration 51/1000 | Loss: 0.00011026
Iteration 52/1000 | Loss: 0.00002598
Iteration 53/1000 | Loss: 0.00011556
Iteration 54/1000 | Loss: 0.00009994
Iteration 55/1000 | Loss: 0.00010920
Iteration 56/1000 | Loss: 0.00009541
Iteration 57/1000 | Loss: 0.00010033
Iteration 58/1000 | Loss: 0.00008373
Iteration 59/1000 | Loss: 0.00009523
Iteration 60/1000 | Loss: 0.00018424
Iteration 61/1000 | Loss: 0.00002903
Iteration 62/1000 | Loss: 0.00002396
Iteration 63/1000 | Loss: 0.00002278
Iteration 64/1000 | Loss: 0.00002229
Iteration 65/1000 | Loss: 0.00002192
Iteration 66/1000 | Loss: 0.00002167
Iteration 67/1000 | Loss: 0.00002141
Iteration 68/1000 | Loss: 0.00002115
Iteration 69/1000 | Loss: 0.00002096
Iteration 70/1000 | Loss: 0.00002095
Iteration 71/1000 | Loss: 0.00002094
Iteration 72/1000 | Loss: 0.00002093
Iteration 73/1000 | Loss: 0.00002092
Iteration 74/1000 | Loss: 0.00002092
Iteration 75/1000 | Loss: 0.00002088
Iteration 76/1000 | Loss: 0.00002087
Iteration 77/1000 | Loss: 0.00002078
Iteration 78/1000 | Loss: 0.00002075
Iteration 79/1000 | Loss: 0.00002074
Iteration 80/1000 | Loss: 0.00002074
Iteration 81/1000 | Loss: 0.00002073
Iteration 82/1000 | Loss: 0.00002073
Iteration 83/1000 | Loss: 0.00002072
Iteration 84/1000 | Loss: 0.00002071
Iteration 85/1000 | Loss: 0.00002070
Iteration 86/1000 | Loss: 0.00002069
Iteration 87/1000 | Loss: 0.00002067
Iteration 88/1000 | Loss: 0.00002067
Iteration 89/1000 | Loss: 0.00002066
Iteration 90/1000 | Loss: 0.00002065
Iteration 91/1000 | Loss: 0.00002064
Iteration 92/1000 | Loss: 0.00002064
Iteration 93/1000 | Loss: 0.00002064
Iteration 94/1000 | Loss: 0.00002063
Iteration 95/1000 | Loss: 0.00002063
Iteration 96/1000 | Loss: 0.00002062
Iteration 97/1000 | Loss: 0.00002062
Iteration 98/1000 | Loss: 0.00002061
Iteration 99/1000 | Loss: 0.00002061
Iteration 100/1000 | Loss: 0.00002059
Iteration 101/1000 | Loss: 0.00002056
Iteration 102/1000 | Loss: 0.00002056
Iteration 103/1000 | Loss: 0.00002055
Iteration 104/1000 | Loss: 0.00002054
Iteration 105/1000 | Loss: 0.00002054
Iteration 106/1000 | Loss: 0.00002053
Iteration 107/1000 | Loss: 0.00002053
Iteration 108/1000 | Loss: 0.00002051
Iteration 109/1000 | Loss: 0.00002048
Iteration 110/1000 | Loss: 0.00002047
Iteration 111/1000 | Loss: 0.00002047
Iteration 112/1000 | Loss: 0.00002046
Iteration 113/1000 | Loss: 0.00002041
Iteration 114/1000 | Loss: 0.00002033
Iteration 115/1000 | Loss: 0.00002033
Iteration 116/1000 | Loss: 0.00002030
Iteration 117/1000 | Loss: 0.00002029
Iteration 118/1000 | Loss: 0.00002028
Iteration 119/1000 | Loss: 0.00002028
Iteration 120/1000 | Loss: 0.00002027
Iteration 121/1000 | Loss: 0.00002024
Iteration 122/1000 | Loss: 0.00002024
Iteration 123/1000 | Loss: 0.00002024
Iteration 124/1000 | Loss: 0.00002022
Iteration 125/1000 | Loss: 0.00002021
Iteration 126/1000 | Loss: 0.00002020
Iteration 127/1000 | Loss: 0.00002020
Iteration 128/1000 | Loss: 0.00002020
Iteration 129/1000 | Loss: 0.00002019
Iteration 130/1000 | Loss: 0.00002019
Iteration 131/1000 | Loss: 0.00002019
Iteration 132/1000 | Loss: 0.00002018
Iteration 133/1000 | Loss: 0.00002018
Iteration 134/1000 | Loss: 0.00002018
Iteration 135/1000 | Loss: 0.00002017
Iteration 136/1000 | Loss: 0.00002017
Iteration 137/1000 | Loss: 0.00002017
Iteration 138/1000 | Loss: 0.00002017
Iteration 139/1000 | Loss: 0.00002017
Iteration 140/1000 | Loss: 0.00002016
Iteration 141/1000 | Loss: 0.00002016
Iteration 142/1000 | Loss: 0.00002016
Iteration 143/1000 | Loss: 0.00002016
Iteration 144/1000 | Loss: 0.00002016
Iteration 145/1000 | Loss: 0.00002015
Iteration 146/1000 | Loss: 0.00002015
Iteration 147/1000 | Loss: 0.00002015
Iteration 148/1000 | Loss: 0.00002015
Iteration 149/1000 | Loss: 0.00002015
Iteration 150/1000 | Loss: 0.00002015
Iteration 151/1000 | Loss: 0.00002015
Iteration 152/1000 | Loss: 0.00002015
Iteration 153/1000 | Loss: 0.00002015
Iteration 154/1000 | Loss: 0.00002014
Iteration 155/1000 | Loss: 0.00002014
Iteration 156/1000 | Loss: 0.00002014
Iteration 157/1000 | Loss: 0.00002014
Iteration 158/1000 | Loss: 0.00002014
Iteration 159/1000 | Loss: 0.00002014
Iteration 160/1000 | Loss: 0.00002014
Iteration 161/1000 | Loss: 0.00002013
Iteration 162/1000 | Loss: 0.00002013
Iteration 163/1000 | Loss: 0.00002013
Iteration 164/1000 | Loss: 0.00002013
Iteration 165/1000 | Loss: 0.00002012
Iteration 166/1000 | Loss: 0.00002012
Iteration 167/1000 | Loss: 0.00002012
Iteration 168/1000 | Loss: 0.00002012
Iteration 169/1000 | Loss: 0.00002011
Iteration 170/1000 | Loss: 0.00002009
Iteration 171/1000 | Loss: 0.00002009
Iteration 172/1000 | Loss: 0.00002009
Iteration 173/1000 | Loss: 0.00002008
Iteration 174/1000 | Loss: 0.00002008
Iteration 175/1000 | Loss: 0.00002008
Iteration 176/1000 | Loss: 0.00002008
Iteration 177/1000 | Loss: 0.00002007
Iteration 178/1000 | Loss: 0.00002007
Iteration 179/1000 | Loss: 0.00002007
Iteration 180/1000 | Loss: 0.00002007
Iteration 181/1000 | Loss: 0.00002006
Iteration 182/1000 | Loss: 0.00002006
Iteration 183/1000 | Loss: 0.00002006
Iteration 184/1000 | Loss: 0.00002006
Iteration 185/1000 | Loss: 0.00002005
Iteration 186/1000 | Loss: 0.00002005
Iteration 187/1000 | Loss: 0.00002005
Iteration 188/1000 | Loss: 0.00002005
Iteration 189/1000 | Loss: 0.00002005
Iteration 190/1000 | Loss: 0.00002004
Iteration 191/1000 | Loss: 0.00002004
Iteration 192/1000 | Loss: 0.00002004
Iteration 193/1000 | Loss: 0.00002004
Iteration 194/1000 | Loss: 0.00002004
Iteration 195/1000 | Loss: 0.00002003
Iteration 196/1000 | Loss: 0.00002003
Iteration 197/1000 | Loss: 0.00002003
Iteration 198/1000 | Loss: 0.00002003
Iteration 199/1000 | Loss: 0.00002003
Iteration 200/1000 | Loss: 0.00002003
Iteration 201/1000 | Loss: 0.00002003
Iteration 202/1000 | Loss: 0.00002003
Iteration 203/1000 | Loss: 0.00002003
Iteration 204/1000 | Loss: 0.00002003
Iteration 205/1000 | Loss: 0.00002003
Iteration 206/1000 | Loss: 0.00002003
Iteration 207/1000 | Loss: 0.00002003
Iteration 208/1000 | Loss: 0.00002002
Iteration 209/1000 | Loss: 0.00002002
Iteration 210/1000 | Loss: 0.00002002
Iteration 211/1000 | Loss: 0.00002002
Iteration 212/1000 | Loss: 0.00002002
Iteration 213/1000 | Loss: 0.00002002
Iteration 214/1000 | Loss: 0.00002002
Iteration 215/1000 | Loss: 0.00002002
Iteration 216/1000 | Loss: 0.00002002
Iteration 217/1000 | Loss: 0.00002002
Iteration 218/1000 | Loss: 0.00002002
Iteration 219/1000 | Loss: 0.00002002
Iteration 220/1000 | Loss: 0.00002001
Iteration 221/1000 | Loss: 0.00002001
Iteration 222/1000 | Loss: 0.00002001
Iteration 223/1000 | Loss: 0.00002001
Iteration 224/1000 | Loss: 0.00002001
Iteration 225/1000 | Loss: 0.00002001
Iteration 226/1000 | Loss: 0.00002001
Iteration 227/1000 | Loss: 0.00002001
Iteration 228/1000 | Loss: 0.00002001
Iteration 229/1000 | Loss: 0.00002001
Iteration 230/1000 | Loss: 0.00002001
Iteration 231/1000 | Loss: 0.00002001
Iteration 232/1000 | Loss: 0.00002000
Iteration 233/1000 | Loss: 0.00002000
Iteration 234/1000 | Loss: 0.00002000
Iteration 235/1000 | Loss: 0.00002000
Iteration 236/1000 | Loss: 0.00002000
Iteration 237/1000 | Loss: 0.00002000
Iteration 238/1000 | Loss: 0.00002000
Iteration 239/1000 | Loss: 0.00002000
Iteration 240/1000 | Loss: 0.00002000
Iteration 241/1000 | Loss: 0.00002000
Iteration 242/1000 | Loss: 0.00002000
Iteration 243/1000 | Loss: 0.00002000
Iteration 244/1000 | Loss: 0.00002000
Iteration 245/1000 | Loss: 0.00002000
Iteration 246/1000 | Loss: 0.00002000
Iteration 247/1000 | Loss: 0.00002000
Iteration 248/1000 | Loss: 0.00002000
Iteration 249/1000 | Loss: 0.00002000
Iteration 250/1000 | Loss: 0.00002000
Iteration 251/1000 | Loss: 0.00002000
Iteration 252/1000 | Loss: 0.00002000
Iteration 253/1000 | Loss: 0.00002000
Iteration 254/1000 | Loss: 0.00002000
Iteration 255/1000 | Loss: 0.00002000
Iteration 256/1000 | Loss: 0.00002000
Iteration 257/1000 | Loss: 0.00002000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [2.0004843463539146e-05, 2.0004843463539146e-05, 2.0004843463539146e-05, 2.0004843463539146e-05, 2.0004843463539146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0004843463539146e-05

Optimization complete. Final v2v error: 3.5784716606140137 mm

Highest mean error: 13.237311363220215 mm for frame 147

Lowest mean error: 2.8391733169555664 mm for frame 75

Saving results

Total time: 149.61548614501953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460107
Iteration 2/25 | Loss: 0.00084392
Iteration 3/25 | Loss: 0.00072417
Iteration 4/25 | Loss: 0.00069300
Iteration 5/25 | Loss: 0.00068006
Iteration 6/25 | Loss: 0.00067757
Iteration 7/25 | Loss: 0.00067744
Iteration 8/25 | Loss: 0.00067744
Iteration 9/25 | Loss: 0.00067744
Iteration 10/25 | Loss: 0.00067744
Iteration 11/25 | Loss: 0.00067744
Iteration 12/25 | Loss: 0.00067744
Iteration 13/25 | Loss: 0.00067744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006774363573640585, 0.0006774363573640585, 0.0006774363573640585, 0.0006774363573640585, 0.0006774363573640585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006774363573640585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45212710
Iteration 2/25 | Loss: 0.00032578
Iteration 3/25 | Loss: 0.00032576
Iteration 4/25 | Loss: 0.00032576
Iteration 5/25 | Loss: 0.00032576
Iteration 6/25 | Loss: 0.00032576
Iteration 7/25 | Loss: 0.00032576
Iteration 8/25 | Loss: 0.00032576
Iteration 9/25 | Loss: 0.00032576
Iteration 10/25 | Loss: 0.00032576
Iteration 11/25 | Loss: 0.00032576
Iteration 12/25 | Loss: 0.00032576
Iteration 13/25 | Loss: 0.00032576
Iteration 14/25 | Loss: 0.00032576
Iteration 15/25 | Loss: 0.00032576
Iteration 16/25 | Loss: 0.00032576
Iteration 17/25 | Loss: 0.00032576
Iteration 18/25 | Loss: 0.00032576
Iteration 19/25 | Loss: 0.00032576
Iteration 20/25 | Loss: 0.00032576
Iteration 21/25 | Loss: 0.00032576
Iteration 22/25 | Loss: 0.00032576
Iteration 23/25 | Loss: 0.00032576
Iteration 24/25 | Loss: 0.00032576
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003257575735915452, 0.0003257575735915452, 0.0003257575735915452, 0.0003257575735915452, 0.0003257575735915452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003257575735915452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032576
Iteration 2/1000 | Loss: 0.00003359
Iteration 3/1000 | Loss: 0.00002110
Iteration 4/1000 | Loss: 0.00001924
Iteration 5/1000 | Loss: 0.00001826
Iteration 6/1000 | Loss: 0.00001788
Iteration 7/1000 | Loss: 0.00001738
Iteration 8/1000 | Loss: 0.00001714
Iteration 9/1000 | Loss: 0.00001711
Iteration 10/1000 | Loss: 0.00001705
Iteration 11/1000 | Loss: 0.00001696
Iteration 12/1000 | Loss: 0.00001696
Iteration 13/1000 | Loss: 0.00001694
Iteration 14/1000 | Loss: 0.00001684
Iteration 15/1000 | Loss: 0.00001680
Iteration 16/1000 | Loss: 0.00001679
Iteration 17/1000 | Loss: 0.00001672
Iteration 18/1000 | Loss: 0.00001672
Iteration 19/1000 | Loss: 0.00001672
Iteration 20/1000 | Loss: 0.00001670
Iteration 21/1000 | Loss: 0.00001670
Iteration 22/1000 | Loss: 0.00001669
Iteration 23/1000 | Loss: 0.00001669
Iteration 24/1000 | Loss: 0.00001669
Iteration 25/1000 | Loss: 0.00001669
Iteration 26/1000 | Loss: 0.00001668
Iteration 27/1000 | Loss: 0.00001668
Iteration 28/1000 | Loss: 0.00001667
Iteration 29/1000 | Loss: 0.00001667
Iteration 30/1000 | Loss: 0.00001667
Iteration 31/1000 | Loss: 0.00001667
Iteration 32/1000 | Loss: 0.00001666
Iteration 33/1000 | Loss: 0.00001666
Iteration 34/1000 | Loss: 0.00001665
Iteration 35/1000 | Loss: 0.00001665
Iteration 36/1000 | Loss: 0.00001664
Iteration 37/1000 | Loss: 0.00001664
Iteration 38/1000 | Loss: 0.00001664
Iteration 39/1000 | Loss: 0.00001664
Iteration 40/1000 | Loss: 0.00001664
Iteration 41/1000 | Loss: 0.00001663
Iteration 42/1000 | Loss: 0.00001663
Iteration 43/1000 | Loss: 0.00001663
Iteration 44/1000 | Loss: 0.00001663
Iteration 45/1000 | Loss: 0.00001663
Iteration 46/1000 | Loss: 0.00001663
Iteration 47/1000 | Loss: 0.00001663
Iteration 48/1000 | Loss: 0.00001663
Iteration 49/1000 | Loss: 0.00001663
Iteration 50/1000 | Loss: 0.00001663
Iteration 51/1000 | Loss: 0.00001662
Iteration 52/1000 | Loss: 0.00001662
Iteration 53/1000 | Loss: 0.00001662
Iteration 54/1000 | Loss: 0.00001662
Iteration 55/1000 | Loss: 0.00001662
Iteration 56/1000 | Loss: 0.00001662
Iteration 57/1000 | Loss: 0.00001662
Iteration 58/1000 | Loss: 0.00001662
Iteration 59/1000 | Loss: 0.00001661
Iteration 60/1000 | Loss: 0.00001661
Iteration 61/1000 | Loss: 0.00001661
Iteration 62/1000 | Loss: 0.00001661
Iteration 63/1000 | Loss: 0.00001661
Iteration 64/1000 | Loss: 0.00001661
Iteration 65/1000 | Loss: 0.00001660
Iteration 66/1000 | Loss: 0.00001660
Iteration 67/1000 | Loss: 0.00001660
Iteration 68/1000 | Loss: 0.00001660
Iteration 69/1000 | Loss: 0.00001660
Iteration 70/1000 | Loss: 0.00001660
Iteration 71/1000 | Loss: 0.00001659
Iteration 72/1000 | Loss: 0.00001659
Iteration 73/1000 | Loss: 0.00001659
Iteration 74/1000 | Loss: 0.00001659
Iteration 75/1000 | Loss: 0.00001659
Iteration 76/1000 | Loss: 0.00001659
Iteration 77/1000 | Loss: 0.00001659
Iteration 78/1000 | Loss: 0.00001659
Iteration 79/1000 | Loss: 0.00001659
Iteration 80/1000 | Loss: 0.00001659
Iteration 81/1000 | Loss: 0.00001659
Iteration 82/1000 | Loss: 0.00001659
Iteration 83/1000 | Loss: 0.00001659
Iteration 84/1000 | Loss: 0.00001658
Iteration 85/1000 | Loss: 0.00001658
Iteration 86/1000 | Loss: 0.00001658
Iteration 87/1000 | Loss: 0.00001658
Iteration 88/1000 | Loss: 0.00001658
Iteration 89/1000 | Loss: 0.00001658
Iteration 90/1000 | Loss: 0.00001658
Iteration 91/1000 | Loss: 0.00001658
Iteration 92/1000 | Loss: 0.00001658
Iteration 93/1000 | Loss: 0.00001658
Iteration 94/1000 | Loss: 0.00001658
Iteration 95/1000 | Loss: 0.00001658
Iteration 96/1000 | Loss: 0.00001658
Iteration 97/1000 | Loss: 0.00001658
Iteration 98/1000 | Loss: 0.00001658
Iteration 99/1000 | Loss: 0.00001658
Iteration 100/1000 | Loss: 0.00001658
Iteration 101/1000 | Loss: 0.00001658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.658196015341673e-05, 1.658196015341673e-05, 1.658196015341673e-05, 1.658196015341673e-05, 1.658196015341673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.658196015341673e-05

Optimization complete. Final v2v error: 3.402630567550659 mm

Highest mean error: 3.8805227279663086 mm for frame 194

Lowest mean error: 3.127669334411621 mm for frame 100

Saving results

Total time: 35.348830699920654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00712225
Iteration 2/25 | Loss: 0.00159343
Iteration 3/25 | Loss: 0.00082269
Iteration 4/25 | Loss: 0.00073859
Iteration 5/25 | Loss: 0.00070969
Iteration 6/25 | Loss: 0.00072316
Iteration 7/25 | Loss: 0.00071335
Iteration 8/25 | Loss: 0.00070005
Iteration 9/25 | Loss: 0.00068143
Iteration 10/25 | Loss: 0.00067201
Iteration 11/25 | Loss: 0.00066804
Iteration 12/25 | Loss: 0.00066738
Iteration 13/25 | Loss: 0.00066733
Iteration 14/25 | Loss: 0.00066733
Iteration 15/25 | Loss: 0.00066733
Iteration 16/25 | Loss: 0.00066733
Iteration 17/25 | Loss: 0.00066733
Iteration 18/25 | Loss: 0.00066733
Iteration 19/25 | Loss: 0.00066733
Iteration 20/25 | Loss: 0.00066733
Iteration 21/25 | Loss: 0.00066732
Iteration 22/25 | Loss: 0.00066732
Iteration 23/25 | Loss: 0.00066732
Iteration 24/25 | Loss: 0.00066732
Iteration 25/25 | Loss: 0.00066732

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.72642231
Iteration 2/25 | Loss: 0.00035617
Iteration 3/25 | Loss: 0.00035617
Iteration 4/25 | Loss: 0.00035617
Iteration 5/25 | Loss: 0.00035617
Iteration 6/25 | Loss: 0.00035617
Iteration 7/25 | Loss: 0.00035617
Iteration 8/25 | Loss: 0.00035617
Iteration 9/25 | Loss: 0.00035617
Iteration 10/25 | Loss: 0.00035617
Iteration 11/25 | Loss: 0.00035617
Iteration 12/25 | Loss: 0.00035617
Iteration 13/25 | Loss: 0.00035617
Iteration 14/25 | Loss: 0.00035617
Iteration 15/25 | Loss: 0.00035617
Iteration 16/25 | Loss: 0.00035617
Iteration 17/25 | Loss: 0.00035617
Iteration 18/25 | Loss: 0.00035617
Iteration 19/25 | Loss: 0.00035617
Iteration 20/25 | Loss: 0.00035617
Iteration 21/25 | Loss: 0.00035617
Iteration 22/25 | Loss: 0.00035617
Iteration 23/25 | Loss: 0.00035617
Iteration 24/25 | Loss: 0.00035617
Iteration 25/25 | Loss: 0.00035617
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000356165663106367, 0.000356165663106367, 0.000356165663106367, 0.000356165663106367, 0.000356165663106367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000356165663106367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035617
Iteration 2/1000 | Loss: 0.00002397
Iteration 3/1000 | Loss: 0.00001883
Iteration 4/1000 | Loss: 0.00001660
Iteration 5/1000 | Loss: 0.00032683
Iteration 6/1000 | Loss: 0.00002261
Iteration 7/1000 | Loss: 0.00002022
Iteration 8/1000 | Loss: 0.00001754
Iteration 9/1000 | Loss: 0.00001678
Iteration 10/1000 | Loss: 0.00001611
Iteration 11/1000 | Loss: 0.00001564
Iteration 12/1000 | Loss: 0.00001518
Iteration 13/1000 | Loss: 0.00001506
Iteration 14/1000 | Loss: 0.00001504
Iteration 15/1000 | Loss: 0.00001495
Iteration 16/1000 | Loss: 0.00001491
Iteration 17/1000 | Loss: 0.00001490
Iteration 18/1000 | Loss: 0.00001475
Iteration 19/1000 | Loss: 0.00001464
Iteration 20/1000 | Loss: 0.00001748
Iteration 21/1000 | Loss: 0.00001445
Iteration 22/1000 | Loss: 0.00001401
Iteration 23/1000 | Loss: 0.00001381
Iteration 24/1000 | Loss: 0.00001380
Iteration 25/1000 | Loss: 0.00001373
Iteration 26/1000 | Loss: 0.00001371
Iteration 27/1000 | Loss: 0.00001369
Iteration 28/1000 | Loss: 0.00001368
Iteration 29/1000 | Loss: 0.00001367
Iteration 30/1000 | Loss: 0.00001367
Iteration 31/1000 | Loss: 0.00001366
Iteration 32/1000 | Loss: 0.00001366
Iteration 33/1000 | Loss: 0.00001361
Iteration 34/1000 | Loss: 0.00001349
Iteration 35/1000 | Loss: 0.00001347
Iteration 36/1000 | Loss: 0.00001347
Iteration 37/1000 | Loss: 0.00001346
Iteration 38/1000 | Loss: 0.00001346
Iteration 39/1000 | Loss: 0.00001345
Iteration 40/1000 | Loss: 0.00001345
Iteration 41/1000 | Loss: 0.00001345
Iteration 42/1000 | Loss: 0.00001345
Iteration 43/1000 | Loss: 0.00001344
Iteration 44/1000 | Loss: 0.00001343
Iteration 45/1000 | Loss: 0.00001343
Iteration 46/1000 | Loss: 0.00001343
Iteration 47/1000 | Loss: 0.00001343
Iteration 48/1000 | Loss: 0.00001343
Iteration 49/1000 | Loss: 0.00001343
Iteration 50/1000 | Loss: 0.00001343
Iteration 51/1000 | Loss: 0.00001343
Iteration 52/1000 | Loss: 0.00001343
Iteration 53/1000 | Loss: 0.00001343
Iteration 54/1000 | Loss: 0.00001343
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001343
Iteration 57/1000 | Loss: 0.00001343
Iteration 58/1000 | Loss: 0.00001343
Iteration 59/1000 | Loss: 0.00001342
Iteration 60/1000 | Loss: 0.00001342
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001341
Iteration 63/1000 | Loss: 0.00001341
Iteration 64/1000 | Loss: 0.00001341
Iteration 65/1000 | Loss: 0.00001341
Iteration 66/1000 | Loss: 0.00001341
Iteration 67/1000 | Loss: 0.00001341
Iteration 68/1000 | Loss: 0.00001340
Iteration 69/1000 | Loss: 0.00001340
Iteration 70/1000 | Loss: 0.00001340
Iteration 71/1000 | Loss: 0.00001340
Iteration 72/1000 | Loss: 0.00001340
Iteration 73/1000 | Loss: 0.00001340
Iteration 74/1000 | Loss: 0.00001340
Iteration 75/1000 | Loss: 0.00001340
Iteration 76/1000 | Loss: 0.00001340
Iteration 77/1000 | Loss: 0.00001340
Iteration 78/1000 | Loss: 0.00001339
Iteration 79/1000 | Loss: 0.00001339
Iteration 80/1000 | Loss: 0.00001339
Iteration 81/1000 | Loss: 0.00001339
Iteration 82/1000 | Loss: 0.00001339
Iteration 83/1000 | Loss: 0.00001339
Iteration 84/1000 | Loss: 0.00001339
Iteration 85/1000 | Loss: 0.00001339
Iteration 86/1000 | Loss: 0.00001339
Iteration 87/1000 | Loss: 0.00001339
Iteration 88/1000 | Loss: 0.00001339
Iteration 89/1000 | Loss: 0.00001339
Iteration 90/1000 | Loss: 0.00001339
Iteration 91/1000 | Loss: 0.00001339
Iteration 92/1000 | Loss: 0.00001339
Iteration 93/1000 | Loss: 0.00001339
Iteration 94/1000 | Loss: 0.00001339
Iteration 95/1000 | Loss: 0.00001339
Iteration 96/1000 | Loss: 0.00001338
Iteration 97/1000 | Loss: 0.00001338
Iteration 98/1000 | Loss: 0.00001338
Iteration 99/1000 | Loss: 0.00001338
Iteration 100/1000 | Loss: 0.00001338
Iteration 101/1000 | Loss: 0.00001338
Iteration 102/1000 | Loss: 0.00001338
Iteration 103/1000 | Loss: 0.00001338
Iteration 104/1000 | Loss: 0.00001338
Iteration 105/1000 | Loss: 0.00001338
Iteration 106/1000 | Loss: 0.00001338
Iteration 107/1000 | Loss: 0.00001338
Iteration 108/1000 | Loss: 0.00001338
Iteration 109/1000 | Loss: 0.00001338
Iteration 110/1000 | Loss: 0.00001338
Iteration 111/1000 | Loss: 0.00001338
Iteration 112/1000 | Loss: 0.00001338
Iteration 113/1000 | Loss: 0.00001338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.3383839359448757e-05, 1.3383839359448757e-05, 1.3383839359448757e-05, 1.3383839359448757e-05, 1.3383839359448757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3383839359448757e-05

Optimization complete. Final v2v error: 3.1246492862701416 mm

Highest mean error: 4.004209041595459 mm for frame 122

Lowest mean error: 2.8366360664367676 mm for frame 94

Saving results

Total time: 57.22191596031189
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397508
Iteration 2/25 | Loss: 0.00076618
Iteration 3/25 | Loss: 0.00061158
Iteration 4/25 | Loss: 0.00058877
Iteration 5/25 | Loss: 0.00058297
Iteration 6/25 | Loss: 0.00058129
Iteration 7/25 | Loss: 0.00058094
Iteration 8/25 | Loss: 0.00058094
Iteration 9/25 | Loss: 0.00058094
Iteration 10/25 | Loss: 0.00058094
Iteration 11/25 | Loss: 0.00058094
Iteration 12/25 | Loss: 0.00058094
Iteration 13/25 | Loss: 0.00058094
Iteration 14/25 | Loss: 0.00058094
Iteration 15/25 | Loss: 0.00058094
Iteration 16/25 | Loss: 0.00058094
Iteration 17/25 | Loss: 0.00058094
Iteration 18/25 | Loss: 0.00058094
Iteration 19/25 | Loss: 0.00058094
Iteration 20/25 | Loss: 0.00058094
Iteration 21/25 | Loss: 0.00058094
Iteration 22/25 | Loss: 0.00058094
Iteration 23/25 | Loss: 0.00058094
Iteration 24/25 | Loss: 0.00058094
Iteration 25/25 | Loss: 0.00058094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46033764
Iteration 2/25 | Loss: 0.00023373
Iteration 3/25 | Loss: 0.00023373
Iteration 4/25 | Loss: 0.00023372
Iteration 5/25 | Loss: 0.00023372
Iteration 6/25 | Loss: 0.00023372
Iteration 7/25 | Loss: 0.00023372
Iteration 8/25 | Loss: 0.00023372
Iteration 9/25 | Loss: 0.00023372
Iteration 10/25 | Loss: 0.00023372
Iteration 11/25 | Loss: 0.00023372
Iteration 12/25 | Loss: 0.00023372
Iteration 13/25 | Loss: 0.00023372
Iteration 14/25 | Loss: 0.00023372
Iteration 15/25 | Loss: 0.00023372
Iteration 16/25 | Loss: 0.00023372
Iteration 17/25 | Loss: 0.00023372
Iteration 18/25 | Loss: 0.00023372
Iteration 19/25 | Loss: 0.00023372
Iteration 20/25 | Loss: 0.00023372
Iteration 21/25 | Loss: 0.00023372
Iteration 22/25 | Loss: 0.00023372
Iteration 23/25 | Loss: 0.00023372
Iteration 24/25 | Loss: 0.00023372
Iteration 25/25 | Loss: 0.00023372

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023372
Iteration 2/1000 | Loss: 0.00002205
Iteration 3/1000 | Loss: 0.00001454
Iteration 4/1000 | Loss: 0.00001150
Iteration 5/1000 | Loss: 0.00001082
Iteration 6/1000 | Loss: 0.00001034
Iteration 7/1000 | Loss: 0.00001010
Iteration 8/1000 | Loss: 0.00000993
Iteration 9/1000 | Loss: 0.00000990
Iteration 10/1000 | Loss: 0.00000985
Iteration 11/1000 | Loss: 0.00000985
Iteration 12/1000 | Loss: 0.00000982
Iteration 13/1000 | Loss: 0.00000982
Iteration 14/1000 | Loss: 0.00000981
Iteration 15/1000 | Loss: 0.00000981
Iteration 16/1000 | Loss: 0.00000980
Iteration 17/1000 | Loss: 0.00000980
Iteration 18/1000 | Loss: 0.00000980
Iteration 19/1000 | Loss: 0.00000979
Iteration 20/1000 | Loss: 0.00000979
Iteration 21/1000 | Loss: 0.00000974
Iteration 22/1000 | Loss: 0.00000973
Iteration 23/1000 | Loss: 0.00000972
Iteration 24/1000 | Loss: 0.00000972
Iteration 25/1000 | Loss: 0.00000972
Iteration 26/1000 | Loss: 0.00000972
Iteration 27/1000 | Loss: 0.00000971
Iteration 28/1000 | Loss: 0.00000971
Iteration 29/1000 | Loss: 0.00000970
Iteration 30/1000 | Loss: 0.00000970
Iteration 31/1000 | Loss: 0.00000969
Iteration 32/1000 | Loss: 0.00000969
Iteration 33/1000 | Loss: 0.00000969
Iteration 34/1000 | Loss: 0.00000969
Iteration 35/1000 | Loss: 0.00000968
Iteration 36/1000 | Loss: 0.00000967
Iteration 37/1000 | Loss: 0.00000967
Iteration 38/1000 | Loss: 0.00000967
Iteration 39/1000 | Loss: 0.00000967
Iteration 40/1000 | Loss: 0.00000966
Iteration 41/1000 | Loss: 0.00000966
Iteration 42/1000 | Loss: 0.00000966
Iteration 43/1000 | Loss: 0.00000966
Iteration 44/1000 | Loss: 0.00000966
Iteration 45/1000 | Loss: 0.00000965
Iteration 46/1000 | Loss: 0.00000965
Iteration 47/1000 | Loss: 0.00000965
Iteration 48/1000 | Loss: 0.00000965
Iteration 49/1000 | Loss: 0.00000964
Iteration 50/1000 | Loss: 0.00000964
Iteration 51/1000 | Loss: 0.00000964
Iteration 52/1000 | Loss: 0.00000964
Iteration 53/1000 | Loss: 0.00000963
Iteration 54/1000 | Loss: 0.00000963
Iteration 55/1000 | Loss: 0.00000963
Iteration 56/1000 | Loss: 0.00000962
Iteration 57/1000 | Loss: 0.00000962
Iteration 58/1000 | Loss: 0.00000961
Iteration 59/1000 | Loss: 0.00000961
Iteration 60/1000 | Loss: 0.00000960
Iteration 61/1000 | Loss: 0.00000960
Iteration 62/1000 | Loss: 0.00000960
Iteration 63/1000 | Loss: 0.00000959
Iteration 64/1000 | Loss: 0.00000959
Iteration 65/1000 | Loss: 0.00000958
Iteration 66/1000 | Loss: 0.00000958
Iteration 67/1000 | Loss: 0.00000957
Iteration 68/1000 | Loss: 0.00000957
Iteration 69/1000 | Loss: 0.00000957
Iteration 70/1000 | Loss: 0.00000956
Iteration 71/1000 | Loss: 0.00000956
Iteration 72/1000 | Loss: 0.00000956
Iteration 73/1000 | Loss: 0.00000956
Iteration 74/1000 | Loss: 0.00000955
Iteration 75/1000 | Loss: 0.00000954
Iteration 76/1000 | Loss: 0.00000954
Iteration 77/1000 | Loss: 0.00000954
Iteration 78/1000 | Loss: 0.00000953
Iteration 79/1000 | Loss: 0.00000953
Iteration 80/1000 | Loss: 0.00000953
Iteration 81/1000 | Loss: 0.00000953
Iteration 82/1000 | Loss: 0.00000953
Iteration 83/1000 | Loss: 0.00000953
Iteration 84/1000 | Loss: 0.00000953
Iteration 85/1000 | Loss: 0.00000953
Iteration 86/1000 | Loss: 0.00000953
Iteration 87/1000 | Loss: 0.00000952
Iteration 88/1000 | Loss: 0.00000952
Iteration 89/1000 | Loss: 0.00000952
Iteration 90/1000 | Loss: 0.00000952
Iteration 91/1000 | Loss: 0.00000952
Iteration 92/1000 | Loss: 0.00000952
Iteration 93/1000 | Loss: 0.00000952
Iteration 94/1000 | Loss: 0.00000951
Iteration 95/1000 | Loss: 0.00000951
Iteration 96/1000 | Loss: 0.00000951
Iteration 97/1000 | Loss: 0.00000951
Iteration 98/1000 | Loss: 0.00000951
Iteration 99/1000 | Loss: 0.00000951
Iteration 100/1000 | Loss: 0.00000951
Iteration 101/1000 | Loss: 0.00000951
Iteration 102/1000 | Loss: 0.00000951
Iteration 103/1000 | Loss: 0.00000951
Iteration 104/1000 | Loss: 0.00000951
Iteration 105/1000 | Loss: 0.00000951
Iteration 106/1000 | Loss: 0.00000951
Iteration 107/1000 | Loss: 0.00000950
Iteration 108/1000 | Loss: 0.00000950
Iteration 109/1000 | Loss: 0.00000950
Iteration 110/1000 | Loss: 0.00000950
Iteration 111/1000 | Loss: 0.00000950
Iteration 112/1000 | Loss: 0.00000950
Iteration 113/1000 | Loss: 0.00000950
Iteration 114/1000 | Loss: 0.00000950
Iteration 115/1000 | Loss: 0.00000950
Iteration 116/1000 | Loss: 0.00000950
Iteration 117/1000 | Loss: 0.00000950
Iteration 118/1000 | Loss: 0.00000949
Iteration 119/1000 | Loss: 0.00000949
Iteration 120/1000 | Loss: 0.00000949
Iteration 121/1000 | Loss: 0.00000949
Iteration 122/1000 | Loss: 0.00000949
Iteration 123/1000 | Loss: 0.00000949
Iteration 124/1000 | Loss: 0.00000949
Iteration 125/1000 | Loss: 0.00000949
Iteration 126/1000 | Loss: 0.00000948
Iteration 127/1000 | Loss: 0.00000948
Iteration 128/1000 | Loss: 0.00000948
Iteration 129/1000 | Loss: 0.00000948
Iteration 130/1000 | Loss: 0.00000948
Iteration 131/1000 | Loss: 0.00000948
Iteration 132/1000 | Loss: 0.00000947
Iteration 133/1000 | Loss: 0.00000947
Iteration 134/1000 | Loss: 0.00000947
Iteration 135/1000 | Loss: 0.00000947
Iteration 136/1000 | Loss: 0.00000947
Iteration 137/1000 | Loss: 0.00000947
Iteration 138/1000 | Loss: 0.00000947
Iteration 139/1000 | Loss: 0.00000946
Iteration 140/1000 | Loss: 0.00000946
Iteration 141/1000 | Loss: 0.00000946
Iteration 142/1000 | Loss: 0.00000946
Iteration 143/1000 | Loss: 0.00000946
Iteration 144/1000 | Loss: 0.00000946
Iteration 145/1000 | Loss: 0.00000945
Iteration 146/1000 | Loss: 0.00000945
Iteration 147/1000 | Loss: 0.00000945
Iteration 148/1000 | Loss: 0.00000945
Iteration 149/1000 | Loss: 0.00000945
Iteration 150/1000 | Loss: 0.00000944
Iteration 151/1000 | Loss: 0.00000944
Iteration 152/1000 | Loss: 0.00000944
Iteration 153/1000 | Loss: 0.00000944
Iteration 154/1000 | Loss: 0.00000944
Iteration 155/1000 | Loss: 0.00000944
Iteration 156/1000 | Loss: 0.00000944
Iteration 157/1000 | Loss: 0.00000944
Iteration 158/1000 | Loss: 0.00000944
Iteration 159/1000 | Loss: 0.00000944
Iteration 160/1000 | Loss: 0.00000944
Iteration 161/1000 | Loss: 0.00000943
Iteration 162/1000 | Loss: 0.00000943
Iteration 163/1000 | Loss: 0.00000943
Iteration 164/1000 | Loss: 0.00000943
Iteration 165/1000 | Loss: 0.00000943
Iteration 166/1000 | Loss: 0.00000943
Iteration 167/1000 | Loss: 0.00000943
Iteration 168/1000 | Loss: 0.00000943
Iteration 169/1000 | Loss: 0.00000943
Iteration 170/1000 | Loss: 0.00000943
Iteration 171/1000 | Loss: 0.00000942
Iteration 172/1000 | Loss: 0.00000942
Iteration 173/1000 | Loss: 0.00000942
Iteration 174/1000 | Loss: 0.00000942
Iteration 175/1000 | Loss: 0.00000942
Iteration 176/1000 | Loss: 0.00000942
Iteration 177/1000 | Loss: 0.00000942
Iteration 178/1000 | Loss: 0.00000942
Iteration 179/1000 | Loss: 0.00000942
Iteration 180/1000 | Loss: 0.00000942
Iteration 181/1000 | Loss: 0.00000942
Iteration 182/1000 | Loss: 0.00000942
Iteration 183/1000 | Loss: 0.00000942
Iteration 184/1000 | Loss: 0.00000942
Iteration 185/1000 | Loss: 0.00000942
Iteration 186/1000 | Loss: 0.00000942
Iteration 187/1000 | Loss: 0.00000942
Iteration 188/1000 | Loss: 0.00000942
Iteration 189/1000 | Loss: 0.00000942
Iteration 190/1000 | Loss: 0.00000942
Iteration 191/1000 | Loss: 0.00000942
Iteration 192/1000 | Loss: 0.00000941
Iteration 193/1000 | Loss: 0.00000941
Iteration 194/1000 | Loss: 0.00000941
Iteration 195/1000 | Loss: 0.00000941
Iteration 196/1000 | Loss: 0.00000941
Iteration 197/1000 | Loss: 0.00000941
Iteration 198/1000 | Loss: 0.00000941
Iteration 199/1000 | Loss: 0.00000941
Iteration 200/1000 | Loss: 0.00000941
Iteration 201/1000 | Loss: 0.00000941
Iteration 202/1000 | Loss: 0.00000941
Iteration 203/1000 | Loss: 0.00000941
Iteration 204/1000 | Loss: 0.00000941
Iteration 205/1000 | Loss: 0.00000941
Iteration 206/1000 | Loss: 0.00000941
Iteration 207/1000 | Loss: 0.00000941
Iteration 208/1000 | Loss: 0.00000941
Iteration 209/1000 | Loss: 0.00000941
Iteration 210/1000 | Loss: 0.00000940
Iteration 211/1000 | Loss: 0.00000940
Iteration 212/1000 | Loss: 0.00000940
Iteration 213/1000 | Loss: 0.00000940
Iteration 214/1000 | Loss: 0.00000940
Iteration 215/1000 | Loss: 0.00000940
Iteration 216/1000 | Loss: 0.00000940
Iteration 217/1000 | Loss: 0.00000940
Iteration 218/1000 | Loss: 0.00000940
Iteration 219/1000 | Loss: 0.00000940
Iteration 220/1000 | Loss: 0.00000940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [9.40278641792247e-06, 9.40278641792247e-06, 9.40278641792247e-06, 9.40278641792247e-06, 9.40278641792247e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.40278641792247e-06

Optimization complete. Final v2v error: 2.6020164489746094 mm

Highest mean error: 3.3407180309295654 mm for frame 54

Lowest mean error: 2.473128318786621 mm for frame 147

Saving results

Total time: 39.533820152282715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847462
Iteration 2/25 | Loss: 0.00113185
Iteration 3/25 | Loss: 0.00083266
Iteration 4/25 | Loss: 0.00071930
Iteration 5/25 | Loss: 0.00070347
Iteration 6/25 | Loss: 0.00069515
Iteration 7/25 | Loss: 0.00069259
Iteration 8/25 | Loss: 0.00069174
Iteration 9/25 | Loss: 0.00069156
Iteration 10/25 | Loss: 0.00069155
Iteration 11/25 | Loss: 0.00069155
Iteration 12/25 | Loss: 0.00069154
Iteration 13/25 | Loss: 0.00069154
Iteration 14/25 | Loss: 0.00069154
Iteration 15/25 | Loss: 0.00069154
Iteration 16/25 | Loss: 0.00069154
Iteration 17/25 | Loss: 0.00069154
Iteration 18/25 | Loss: 0.00069154
Iteration 19/25 | Loss: 0.00069154
Iteration 20/25 | Loss: 0.00069154
Iteration 21/25 | Loss: 0.00069154
Iteration 22/25 | Loss: 0.00069153
Iteration 23/25 | Loss: 0.00069153
Iteration 24/25 | Loss: 0.00069153
Iteration 25/25 | Loss: 0.00069153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.03028011
Iteration 2/25 | Loss: 0.00036727
Iteration 3/25 | Loss: 0.00036721
Iteration 4/25 | Loss: 0.00036721
Iteration 5/25 | Loss: 0.00036721
Iteration 6/25 | Loss: 0.00036721
Iteration 7/25 | Loss: 0.00036721
Iteration 8/25 | Loss: 0.00036721
Iteration 9/25 | Loss: 0.00036721
Iteration 10/25 | Loss: 0.00036721
Iteration 11/25 | Loss: 0.00036721
Iteration 12/25 | Loss: 0.00036721
Iteration 13/25 | Loss: 0.00036721
Iteration 14/25 | Loss: 0.00036721
Iteration 15/25 | Loss: 0.00036721
Iteration 16/25 | Loss: 0.00036721
Iteration 17/25 | Loss: 0.00036721
Iteration 18/25 | Loss: 0.00036721
Iteration 19/25 | Loss: 0.00036721
Iteration 20/25 | Loss: 0.00036721
Iteration 21/25 | Loss: 0.00036721
Iteration 22/25 | Loss: 0.00036721
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00036721143987961113, 0.00036721143987961113, 0.00036721143987961113, 0.00036721143987961113, 0.00036721143987961113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036721143987961113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036721
Iteration 2/1000 | Loss: 0.00003253
Iteration 3/1000 | Loss: 0.00002353
Iteration 4/1000 | Loss: 0.00002067
Iteration 5/1000 | Loss: 0.00001984
Iteration 6/1000 | Loss: 0.00001927
Iteration 7/1000 | Loss: 0.00001887
Iteration 8/1000 | Loss: 0.00001859
Iteration 9/1000 | Loss: 0.00015142
Iteration 10/1000 | Loss: 0.00057441
Iteration 11/1000 | Loss: 0.00016145
Iteration 12/1000 | Loss: 0.00065131
Iteration 13/1000 | Loss: 0.00016955
Iteration 14/1000 | Loss: 0.00010478
Iteration 15/1000 | Loss: 0.00004987
Iteration 16/1000 | Loss: 0.00015757
Iteration 17/1000 | Loss: 0.00004748
Iteration 18/1000 | Loss: 0.00001852
Iteration 19/1000 | Loss: 0.00001821
Iteration 20/1000 | Loss: 0.00001811
Iteration 21/1000 | Loss: 0.00001809
Iteration 22/1000 | Loss: 0.00001809
Iteration 23/1000 | Loss: 0.00001807
Iteration 24/1000 | Loss: 0.00011950
Iteration 25/1000 | Loss: 0.00002000
Iteration 26/1000 | Loss: 0.00008609
Iteration 27/1000 | Loss: 0.00001803
Iteration 28/1000 | Loss: 0.00005907
Iteration 29/1000 | Loss: 0.00002657
Iteration 30/1000 | Loss: 0.00001796
Iteration 31/1000 | Loss: 0.00001784
Iteration 32/1000 | Loss: 0.00001782
Iteration 33/1000 | Loss: 0.00001779
Iteration 34/1000 | Loss: 0.00001779
Iteration 35/1000 | Loss: 0.00001778
Iteration 36/1000 | Loss: 0.00001777
Iteration 37/1000 | Loss: 0.00001776
Iteration 38/1000 | Loss: 0.00001775
Iteration 39/1000 | Loss: 0.00001772
Iteration 40/1000 | Loss: 0.00001769
Iteration 41/1000 | Loss: 0.00001767
Iteration 42/1000 | Loss: 0.00014694
Iteration 43/1000 | Loss: 0.00002033
Iteration 44/1000 | Loss: 0.00012528
Iteration 45/1000 | Loss: 0.00001775
Iteration 46/1000 | Loss: 0.00001773
Iteration 47/1000 | Loss: 0.00001761
Iteration 48/1000 | Loss: 0.00001761
Iteration 49/1000 | Loss: 0.00001761
Iteration 50/1000 | Loss: 0.00001761
Iteration 51/1000 | Loss: 0.00001761
Iteration 52/1000 | Loss: 0.00001760
Iteration 53/1000 | Loss: 0.00001760
Iteration 54/1000 | Loss: 0.00001760
Iteration 55/1000 | Loss: 0.00001760
Iteration 56/1000 | Loss: 0.00001760
Iteration 57/1000 | Loss: 0.00001760
Iteration 58/1000 | Loss: 0.00001760
Iteration 59/1000 | Loss: 0.00001759
Iteration 60/1000 | Loss: 0.00001759
Iteration 61/1000 | Loss: 0.00001759
Iteration 62/1000 | Loss: 0.00001759
Iteration 63/1000 | Loss: 0.00001758
Iteration 64/1000 | Loss: 0.00001758
Iteration 65/1000 | Loss: 0.00001758
Iteration 66/1000 | Loss: 0.00001758
Iteration 67/1000 | Loss: 0.00001758
Iteration 68/1000 | Loss: 0.00001758
Iteration 69/1000 | Loss: 0.00001758
Iteration 70/1000 | Loss: 0.00001758
Iteration 71/1000 | Loss: 0.00001757
Iteration 72/1000 | Loss: 0.00001757
Iteration 73/1000 | Loss: 0.00001757
Iteration 74/1000 | Loss: 0.00001757
Iteration 75/1000 | Loss: 0.00001757
Iteration 76/1000 | Loss: 0.00001757
Iteration 77/1000 | Loss: 0.00001757
Iteration 78/1000 | Loss: 0.00001757
Iteration 79/1000 | Loss: 0.00001757
Iteration 80/1000 | Loss: 0.00001757
Iteration 81/1000 | Loss: 0.00001757
Iteration 82/1000 | Loss: 0.00001757
Iteration 83/1000 | Loss: 0.00001757
Iteration 84/1000 | Loss: 0.00001757
Iteration 85/1000 | Loss: 0.00001757
Iteration 86/1000 | Loss: 0.00001757
Iteration 87/1000 | Loss: 0.00001757
Iteration 88/1000 | Loss: 0.00001757
Iteration 89/1000 | Loss: 0.00001757
Iteration 90/1000 | Loss: 0.00001757
Iteration 91/1000 | Loss: 0.00001757
Iteration 92/1000 | Loss: 0.00001757
Iteration 93/1000 | Loss: 0.00001757
Iteration 94/1000 | Loss: 0.00001757
Iteration 95/1000 | Loss: 0.00001757
Iteration 96/1000 | Loss: 0.00001757
Iteration 97/1000 | Loss: 0.00001757
Iteration 98/1000 | Loss: 0.00001757
Iteration 99/1000 | Loss: 0.00001757
Iteration 100/1000 | Loss: 0.00001757
Iteration 101/1000 | Loss: 0.00001757
Iteration 102/1000 | Loss: 0.00001757
Iteration 103/1000 | Loss: 0.00001757
Iteration 104/1000 | Loss: 0.00001757
Iteration 105/1000 | Loss: 0.00001757
Iteration 106/1000 | Loss: 0.00001757
Iteration 107/1000 | Loss: 0.00001757
Iteration 108/1000 | Loss: 0.00001757
Iteration 109/1000 | Loss: 0.00001757
Iteration 110/1000 | Loss: 0.00001757
Iteration 111/1000 | Loss: 0.00001757
Iteration 112/1000 | Loss: 0.00001757
Iteration 113/1000 | Loss: 0.00001757
Iteration 114/1000 | Loss: 0.00001757
Iteration 115/1000 | Loss: 0.00001757
Iteration 116/1000 | Loss: 0.00001757
Iteration 117/1000 | Loss: 0.00001757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.75682707777014e-05, 1.75682707777014e-05, 1.75682707777014e-05, 1.75682707777014e-05, 1.75682707777014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.75682707777014e-05

Optimization complete. Final v2v error: 3.4963510036468506 mm

Highest mean error: 4.183091640472412 mm for frame 56

Lowest mean error: 2.922178030014038 mm for frame 112

Saving results

Total time: 64.04442071914673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739356
Iteration 2/25 | Loss: 0.00094045
Iteration 3/25 | Loss: 0.00074353
Iteration 4/25 | Loss: 0.00071778
Iteration 5/25 | Loss: 0.00070988
Iteration 6/25 | Loss: 0.00070740
Iteration 7/25 | Loss: 0.00070700
Iteration 8/25 | Loss: 0.00070700
Iteration 9/25 | Loss: 0.00070700
Iteration 10/25 | Loss: 0.00070700
Iteration 11/25 | Loss: 0.00070700
Iteration 12/25 | Loss: 0.00070700
Iteration 13/25 | Loss: 0.00070700
Iteration 14/25 | Loss: 0.00070700
Iteration 15/25 | Loss: 0.00070700
Iteration 16/25 | Loss: 0.00070700
Iteration 17/25 | Loss: 0.00070700
Iteration 18/25 | Loss: 0.00070700
Iteration 19/25 | Loss: 0.00070700
Iteration 20/25 | Loss: 0.00070700
Iteration 21/25 | Loss: 0.00070700
Iteration 22/25 | Loss: 0.00070700
Iteration 23/25 | Loss: 0.00070700
Iteration 24/25 | Loss: 0.00070700
Iteration 25/25 | Loss: 0.00070700

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46416152
Iteration 2/25 | Loss: 0.00033091
Iteration 3/25 | Loss: 0.00033091
Iteration 4/25 | Loss: 0.00033091
Iteration 5/25 | Loss: 0.00033091
Iteration 6/25 | Loss: 0.00033091
Iteration 7/25 | Loss: 0.00033091
Iteration 8/25 | Loss: 0.00033091
Iteration 9/25 | Loss: 0.00033091
Iteration 10/25 | Loss: 0.00033091
Iteration 11/25 | Loss: 0.00033091
Iteration 12/25 | Loss: 0.00033091
Iteration 13/25 | Loss: 0.00033091
Iteration 14/25 | Loss: 0.00033091
Iteration 15/25 | Loss: 0.00033091
Iteration 16/25 | Loss: 0.00033091
Iteration 17/25 | Loss: 0.00033091
Iteration 18/25 | Loss: 0.00033091
Iteration 19/25 | Loss: 0.00033091
Iteration 20/25 | Loss: 0.00033091
Iteration 21/25 | Loss: 0.00033091
Iteration 22/25 | Loss: 0.00033091
Iteration 23/25 | Loss: 0.00033091
Iteration 24/25 | Loss: 0.00033091
Iteration 25/25 | Loss: 0.00033091

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033091
Iteration 2/1000 | Loss: 0.00003260
Iteration 3/1000 | Loss: 0.00002423
Iteration 4/1000 | Loss: 0.00002275
Iteration 5/1000 | Loss: 0.00002184
Iteration 6/1000 | Loss: 0.00002107
Iteration 7/1000 | Loss: 0.00002062
Iteration 8/1000 | Loss: 0.00002025
Iteration 9/1000 | Loss: 0.00002000
Iteration 10/1000 | Loss: 0.00001976
Iteration 11/1000 | Loss: 0.00001956
Iteration 12/1000 | Loss: 0.00001944
Iteration 13/1000 | Loss: 0.00001943
Iteration 14/1000 | Loss: 0.00001939
Iteration 15/1000 | Loss: 0.00001935
Iteration 16/1000 | Loss: 0.00001935
Iteration 17/1000 | Loss: 0.00001933
Iteration 18/1000 | Loss: 0.00001932
Iteration 19/1000 | Loss: 0.00001932
Iteration 20/1000 | Loss: 0.00001931
Iteration 21/1000 | Loss: 0.00001931
Iteration 22/1000 | Loss: 0.00001930
Iteration 23/1000 | Loss: 0.00001930
Iteration 24/1000 | Loss: 0.00001929
Iteration 25/1000 | Loss: 0.00001929
Iteration 26/1000 | Loss: 0.00001929
Iteration 27/1000 | Loss: 0.00001928
Iteration 28/1000 | Loss: 0.00001928
Iteration 29/1000 | Loss: 0.00001927
Iteration 30/1000 | Loss: 0.00001927
Iteration 31/1000 | Loss: 0.00001927
Iteration 32/1000 | Loss: 0.00001927
Iteration 33/1000 | Loss: 0.00001926
Iteration 34/1000 | Loss: 0.00001926
Iteration 35/1000 | Loss: 0.00001926
Iteration 36/1000 | Loss: 0.00001926
Iteration 37/1000 | Loss: 0.00001926
Iteration 38/1000 | Loss: 0.00001926
Iteration 39/1000 | Loss: 0.00001925
Iteration 40/1000 | Loss: 0.00001924
Iteration 41/1000 | Loss: 0.00001924
Iteration 42/1000 | Loss: 0.00001924
Iteration 43/1000 | Loss: 0.00001923
Iteration 44/1000 | Loss: 0.00001923
Iteration 45/1000 | Loss: 0.00001922
Iteration 46/1000 | Loss: 0.00001922
Iteration 47/1000 | Loss: 0.00001922
Iteration 48/1000 | Loss: 0.00001922
Iteration 49/1000 | Loss: 0.00001921
Iteration 50/1000 | Loss: 0.00001921
Iteration 51/1000 | Loss: 0.00001921
Iteration 52/1000 | Loss: 0.00001920
Iteration 53/1000 | Loss: 0.00001920
Iteration 54/1000 | Loss: 0.00001920
Iteration 55/1000 | Loss: 0.00001920
Iteration 56/1000 | Loss: 0.00001920
Iteration 57/1000 | Loss: 0.00001920
Iteration 58/1000 | Loss: 0.00001920
Iteration 59/1000 | Loss: 0.00001919
Iteration 60/1000 | Loss: 0.00001919
Iteration 61/1000 | Loss: 0.00001919
Iteration 62/1000 | Loss: 0.00001919
Iteration 63/1000 | Loss: 0.00001919
Iteration 64/1000 | Loss: 0.00001919
Iteration 65/1000 | Loss: 0.00001919
Iteration 66/1000 | Loss: 0.00001919
Iteration 67/1000 | Loss: 0.00001919
Iteration 68/1000 | Loss: 0.00001918
Iteration 69/1000 | Loss: 0.00001918
Iteration 70/1000 | Loss: 0.00001918
Iteration 71/1000 | Loss: 0.00001918
Iteration 72/1000 | Loss: 0.00001918
Iteration 73/1000 | Loss: 0.00001918
Iteration 74/1000 | Loss: 0.00001918
Iteration 75/1000 | Loss: 0.00001917
Iteration 76/1000 | Loss: 0.00001917
Iteration 77/1000 | Loss: 0.00001917
Iteration 78/1000 | Loss: 0.00001917
Iteration 79/1000 | Loss: 0.00001917
Iteration 80/1000 | Loss: 0.00001917
Iteration 81/1000 | Loss: 0.00001917
Iteration 82/1000 | Loss: 0.00001916
Iteration 83/1000 | Loss: 0.00001916
Iteration 84/1000 | Loss: 0.00001916
Iteration 85/1000 | Loss: 0.00001916
Iteration 86/1000 | Loss: 0.00001916
Iteration 87/1000 | Loss: 0.00001916
Iteration 88/1000 | Loss: 0.00001916
Iteration 89/1000 | Loss: 0.00001916
Iteration 90/1000 | Loss: 0.00001916
Iteration 91/1000 | Loss: 0.00001916
Iteration 92/1000 | Loss: 0.00001916
Iteration 93/1000 | Loss: 0.00001916
Iteration 94/1000 | Loss: 0.00001916
Iteration 95/1000 | Loss: 0.00001916
Iteration 96/1000 | Loss: 0.00001916
Iteration 97/1000 | Loss: 0.00001916
Iteration 98/1000 | Loss: 0.00001916
Iteration 99/1000 | Loss: 0.00001916
Iteration 100/1000 | Loss: 0.00001916
Iteration 101/1000 | Loss: 0.00001915
Iteration 102/1000 | Loss: 0.00001915
Iteration 103/1000 | Loss: 0.00001915
Iteration 104/1000 | Loss: 0.00001915
Iteration 105/1000 | Loss: 0.00001915
Iteration 106/1000 | Loss: 0.00001915
Iteration 107/1000 | Loss: 0.00001914
Iteration 108/1000 | Loss: 0.00001914
Iteration 109/1000 | Loss: 0.00001914
Iteration 110/1000 | Loss: 0.00001914
Iteration 111/1000 | Loss: 0.00001914
Iteration 112/1000 | Loss: 0.00001914
Iteration 113/1000 | Loss: 0.00001914
Iteration 114/1000 | Loss: 0.00001914
Iteration 115/1000 | Loss: 0.00001914
Iteration 116/1000 | Loss: 0.00001914
Iteration 117/1000 | Loss: 0.00001913
Iteration 118/1000 | Loss: 0.00001913
Iteration 119/1000 | Loss: 0.00001913
Iteration 120/1000 | Loss: 0.00001913
Iteration 121/1000 | Loss: 0.00001913
Iteration 122/1000 | Loss: 0.00001912
Iteration 123/1000 | Loss: 0.00001912
Iteration 124/1000 | Loss: 0.00001912
Iteration 125/1000 | Loss: 0.00001912
Iteration 126/1000 | Loss: 0.00001912
Iteration 127/1000 | Loss: 0.00001912
Iteration 128/1000 | Loss: 0.00001912
Iteration 129/1000 | Loss: 0.00001912
Iteration 130/1000 | Loss: 0.00001912
Iteration 131/1000 | Loss: 0.00001912
Iteration 132/1000 | Loss: 0.00001912
Iteration 133/1000 | Loss: 0.00001912
Iteration 134/1000 | Loss: 0.00001911
Iteration 135/1000 | Loss: 0.00001911
Iteration 136/1000 | Loss: 0.00001911
Iteration 137/1000 | Loss: 0.00001911
Iteration 138/1000 | Loss: 0.00001911
Iteration 139/1000 | Loss: 0.00001911
Iteration 140/1000 | Loss: 0.00001911
Iteration 141/1000 | Loss: 0.00001911
Iteration 142/1000 | Loss: 0.00001911
Iteration 143/1000 | Loss: 0.00001911
Iteration 144/1000 | Loss: 0.00001911
Iteration 145/1000 | Loss: 0.00001911
Iteration 146/1000 | Loss: 0.00001911
Iteration 147/1000 | Loss: 0.00001911
Iteration 148/1000 | Loss: 0.00001910
Iteration 149/1000 | Loss: 0.00001910
Iteration 150/1000 | Loss: 0.00001910
Iteration 151/1000 | Loss: 0.00001910
Iteration 152/1000 | Loss: 0.00001910
Iteration 153/1000 | Loss: 0.00001910
Iteration 154/1000 | Loss: 0.00001910
Iteration 155/1000 | Loss: 0.00001910
Iteration 156/1000 | Loss: 0.00001910
Iteration 157/1000 | Loss: 0.00001910
Iteration 158/1000 | Loss: 0.00001910
Iteration 159/1000 | Loss: 0.00001910
Iteration 160/1000 | Loss: 0.00001910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.9102166334050708e-05, 1.9102166334050708e-05, 1.9102166334050708e-05, 1.9102166334050708e-05, 1.9102166334050708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9102166334050708e-05

Optimization complete. Final v2v error: 3.6718616485595703 mm

Highest mean error: 5.358287811279297 mm for frame 239

Lowest mean error: 3.07749080657959 mm for frame 18

Saving results

Total time: 43.34528040885925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00372168
Iteration 2/25 | Loss: 0.00071864
Iteration 3/25 | Loss: 0.00061648
Iteration 4/25 | Loss: 0.00060127
Iteration 5/25 | Loss: 0.00059830
Iteration 6/25 | Loss: 0.00059779
Iteration 7/25 | Loss: 0.00059779
Iteration 8/25 | Loss: 0.00059779
Iteration 9/25 | Loss: 0.00059779
Iteration 10/25 | Loss: 0.00059779
Iteration 11/25 | Loss: 0.00059779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005977940163575113, 0.0005977940163575113, 0.0005977940163575113, 0.0005977940163575113, 0.0005977940163575113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005977940163575113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46202719
Iteration 2/25 | Loss: 0.00027043
Iteration 3/25 | Loss: 0.00027043
Iteration 4/25 | Loss: 0.00027043
Iteration 5/25 | Loss: 0.00027043
Iteration 6/25 | Loss: 0.00027043
Iteration 7/25 | Loss: 0.00027043
Iteration 8/25 | Loss: 0.00027043
Iteration 9/25 | Loss: 0.00027043
Iteration 10/25 | Loss: 0.00027043
Iteration 11/25 | Loss: 0.00027043
Iteration 12/25 | Loss: 0.00027043
Iteration 13/25 | Loss: 0.00027043
Iteration 14/25 | Loss: 0.00027043
Iteration 15/25 | Loss: 0.00027043
Iteration 16/25 | Loss: 0.00027043
Iteration 17/25 | Loss: 0.00027043
Iteration 18/25 | Loss: 0.00027043
Iteration 19/25 | Loss: 0.00027043
Iteration 20/25 | Loss: 0.00027043
Iteration 21/25 | Loss: 0.00027043
Iteration 22/25 | Loss: 0.00027043
Iteration 23/25 | Loss: 0.00027043
Iteration 24/25 | Loss: 0.00027043
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0002704310172703117, 0.0002704310172703117, 0.0002704310172703117, 0.0002704310172703117, 0.0002704310172703117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002704310172703117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027043
Iteration 2/1000 | Loss: 0.00002168
Iteration 3/1000 | Loss: 0.00001505
Iteration 4/1000 | Loss: 0.00001399
Iteration 5/1000 | Loss: 0.00001337
Iteration 6/1000 | Loss: 0.00001289
Iteration 7/1000 | Loss: 0.00001263
Iteration 8/1000 | Loss: 0.00001257
Iteration 9/1000 | Loss: 0.00001240
Iteration 10/1000 | Loss: 0.00001231
Iteration 11/1000 | Loss: 0.00001230
Iteration 12/1000 | Loss: 0.00001230
Iteration 13/1000 | Loss: 0.00001225
Iteration 14/1000 | Loss: 0.00001224
Iteration 15/1000 | Loss: 0.00001217
Iteration 16/1000 | Loss: 0.00001217
Iteration 17/1000 | Loss: 0.00001217
Iteration 18/1000 | Loss: 0.00001217
Iteration 19/1000 | Loss: 0.00001217
Iteration 20/1000 | Loss: 0.00001217
Iteration 21/1000 | Loss: 0.00001217
Iteration 22/1000 | Loss: 0.00001215
Iteration 23/1000 | Loss: 0.00001212
Iteration 24/1000 | Loss: 0.00001212
Iteration 25/1000 | Loss: 0.00001212
Iteration 26/1000 | Loss: 0.00001211
Iteration 27/1000 | Loss: 0.00001211
Iteration 28/1000 | Loss: 0.00001211
Iteration 29/1000 | Loss: 0.00001211
Iteration 30/1000 | Loss: 0.00001210
Iteration 31/1000 | Loss: 0.00001210
Iteration 32/1000 | Loss: 0.00001210
Iteration 33/1000 | Loss: 0.00001209
Iteration 34/1000 | Loss: 0.00001209
Iteration 35/1000 | Loss: 0.00001208
Iteration 36/1000 | Loss: 0.00001206
Iteration 37/1000 | Loss: 0.00001206
Iteration 38/1000 | Loss: 0.00001206
Iteration 39/1000 | Loss: 0.00001206
Iteration 40/1000 | Loss: 0.00001206
Iteration 41/1000 | Loss: 0.00001206
Iteration 42/1000 | Loss: 0.00001206
Iteration 43/1000 | Loss: 0.00001206
Iteration 44/1000 | Loss: 0.00001205
Iteration 45/1000 | Loss: 0.00001205
Iteration 46/1000 | Loss: 0.00001203
Iteration 47/1000 | Loss: 0.00001203
Iteration 48/1000 | Loss: 0.00001202
Iteration 49/1000 | Loss: 0.00001201
Iteration 50/1000 | Loss: 0.00001201
Iteration 51/1000 | Loss: 0.00001199
Iteration 52/1000 | Loss: 0.00001199
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001198
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001197
Iteration 57/1000 | Loss: 0.00001197
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001195
Iteration 61/1000 | Loss: 0.00001195
Iteration 62/1000 | Loss: 0.00001194
Iteration 63/1000 | Loss: 0.00001193
Iteration 64/1000 | Loss: 0.00001193
Iteration 65/1000 | Loss: 0.00001192
Iteration 66/1000 | Loss: 0.00001192
Iteration 67/1000 | Loss: 0.00001192
Iteration 68/1000 | Loss: 0.00001192
Iteration 69/1000 | Loss: 0.00001192
Iteration 70/1000 | Loss: 0.00001192
Iteration 71/1000 | Loss: 0.00001192
Iteration 72/1000 | Loss: 0.00001192
Iteration 73/1000 | Loss: 0.00001192
Iteration 74/1000 | Loss: 0.00001192
Iteration 75/1000 | Loss: 0.00001192
Iteration 76/1000 | Loss: 0.00001191
Iteration 77/1000 | Loss: 0.00001191
Iteration 78/1000 | Loss: 0.00001191
Iteration 79/1000 | Loss: 0.00001191
Iteration 80/1000 | Loss: 0.00001191
Iteration 81/1000 | Loss: 0.00001191
Iteration 82/1000 | Loss: 0.00001190
Iteration 83/1000 | Loss: 0.00001190
Iteration 84/1000 | Loss: 0.00001190
Iteration 85/1000 | Loss: 0.00001190
Iteration 86/1000 | Loss: 0.00001190
Iteration 87/1000 | Loss: 0.00001190
Iteration 88/1000 | Loss: 0.00001190
Iteration 89/1000 | Loss: 0.00001190
Iteration 90/1000 | Loss: 0.00001190
Iteration 91/1000 | Loss: 0.00001190
Iteration 92/1000 | Loss: 0.00001190
Iteration 93/1000 | Loss: 0.00001190
Iteration 94/1000 | Loss: 0.00001190
Iteration 95/1000 | Loss: 0.00001190
Iteration 96/1000 | Loss: 0.00001189
Iteration 97/1000 | Loss: 0.00001189
Iteration 98/1000 | Loss: 0.00001189
Iteration 99/1000 | Loss: 0.00001189
Iteration 100/1000 | Loss: 0.00001189
Iteration 101/1000 | Loss: 0.00001189
Iteration 102/1000 | Loss: 0.00001189
Iteration 103/1000 | Loss: 0.00001189
Iteration 104/1000 | Loss: 0.00001189
Iteration 105/1000 | Loss: 0.00001189
Iteration 106/1000 | Loss: 0.00001189
Iteration 107/1000 | Loss: 0.00001189
Iteration 108/1000 | Loss: 0.00001189
Iteration 109/1000 | Loss: 0.00001188
Iteration 110/1000 | Loss: 0.00001188
Iteration 111/1000 | Loss: 0.00001188
Iteration 112/1000 | Loss: 0.00001188
Iteration 113/1000 | Loss: 0.00001188
Iteration 114/1000 | Loss: 0.00001188
Iteration 115/1000 | Loss: 0.00001188
Iteration 116/1000 | Loss: 0.00001188
Iteration 117/1000 | Loss: 0.00001188
Iteration 118/1000 | Loss: 0.00001188
Iteration 119/1000 | Loss: 0.00001188
Iteration 120/1000 | Loss: 0.00001188
Iteration 121/1000 | Loss: 0.00001188
Iteration 122/1000 | Loss: 0.00001188
Iteration 123/1000 | Loss: 0.00001188
Iteration 124/1000 | Loss: 0.00001188
Iteration 125/1000 | Loss: 0.00001188
Iteration 126/1000 | Loss: 0.00001188
Iteration 127/1000 | Loss: 0.00001187
Iteration 128/1000 | Loss: 0.00001187
Iteration 129/1000 | Loss: 0.00001187
Iteration 130/1000 | Loss: 0.00001187
Iteration 131/1000 | Loss: 0.00001187
Iteration 132/1000 | Loss: 0.00001187
Iteration 133/1000 | Loss: 0.00001187
Iteration 134/1000 | Loss: 0.00001187
Iteration 135/1000 | Loss: 0.00001187
Iteration 136/1000 | Loss: 0.00001187
Iteration 137/1000 | Loss: 0.00001187
Iteration 138/1000 | Loss: 0.00001187
Iteration 139/1000 | Loss: 0.00001187
Iteration 140/1000 | Loss: 0.00001187
Iteration 141/1000 | Loss: 0.00001187
Iteration 142/1000 | Loss: 0.00001187
Iteration 143/1000 | Loss: 0.00001187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.1867296962009277e-05, 1.1867296962009277e-05, 1.1867296962009277e-05, 1.1867296962009277e-05, 1.1867296962009277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1867296962009277e-05

Optimization complete. Final v2v error: 2.9338431358337402 mm

Highest mean error: 2.993311882019043 mm for frame 159

Lowest mean error: 2.854677200317383 mm for frame 191

Saving results

Total time: 37.55888915061951
