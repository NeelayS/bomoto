Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=239, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13384-13439
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00350137
Iteration 2/25 | Loss: 0.00082031
Iteration 3/25 | Loss: 0.00069309
Iteration 4/25 | Loss: 0.00068135
Iteration 5/25 | Loss: 0.00067705
Iteration 6/25 | Loss: 0.00067598
Iteration 7/25 | Loss: 0.00067598
Iteration 8/25 | Loss: 0.00067598
Iteration 9/25 | Loss: 0.00067598
Iteration 10/25 | Loss: 0.00067598
Iteration 11/25 | Loss: 0.00067598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00067598168971017, 0.00067598168971017, 0.00067598168971017, 0.00067598168971017, 0.00067598168971017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00067598168971017

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68600595
Iteration 2/25 | Loss: 0.00026992
Iteration 3/25 | Loss: 0.00026991
Iteration 4/25 | Loss: 0.00026991
Iteration 5/25 | Loss: 0.00026991
Iteration 6/25 | Loss: 0.00026991
Iteration 7/25 | Loss: 0.00026991
Iteration 8/25 | Loss: 0.00026991
Iteration 9/25 | Loss: 0.00026991
Iteration 10/25 | Loss: 0.00026991
Iteration 11/25 | Loss: 0.00026991
Iteration 12/25 | Loss: 0.00026991
Iteration 13/25 | Loss: 0.00026991
Iteration 14/25 | Loss: 0.00026991
Iteration 15/25 | Loss: 0.00026991
Iteration 16/25 | Loss: 0.00026991
Iteration 17/25 | Loss: 0.00026991
Iteration 18/25 | Loss: 0.00026991
Iteration 19/25 | Loss: 0.00026991
Iteration 20/25 | Loss: 0.00026991
Iteration 21/25 | Loss: 0.00026991
Iteration 22/25 | Loss: 0.00026991
Iteration 23/25 | Loss: 0.00026991
Iteration 24/25 | Loss: 0.00026991
Iteration 25/25 | Loss: 0.00026991
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00026991302729584277, 0.00026991302729584277, 0.00026991302729584277, 0.00026991302729584277, 0.00026991302729584277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026991302729584277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026991
Iteration 2/1000 | Loss: 0.00001982
Iteration 3/1000 | Loss: 0.00001634
Iteration 4/1000 | Loss: 0.00001500
Iteration 5/1000 | Loss: 0.00001446
Iteration 6/1000 | Loss: 0.00001434
Iteration 7/1000 | Loss: 0.00001394
Iteration 8/1000 | Loss: 0.00001375
Iteration 9/1000 | Loss: 0.00001371
Iteration 10/1000 | Loss: 0.00001370
Iteration 11/1000 | Loss: 0.00001363
Iteration 12/1000 | Loss: 0.00001360
Iteration 13/1000 | Loss: 0.00001359
Iteration 14/1000 | Loss: 0.00001359
Iteration 15/1000 | Loss: 0.00001357
Iteration 16/1000 | Loss: 0.00001357
Iteration 17/1000 | Loss: 0.00001356
Iteration 18/1000 | Loss: 0.00001356
Iteration 19/1000 | Loss: 0.00001356
Iteration 20/1000 | Loss: 0.00001355
Iteration 21/1000 | Loss: 0.00001355
Iteration 22/1000 | Loss: 0.00001355
Iteration 23/1000 | Loss: 0.00001354
Iteration 24/1000 | Loss: 0.00001354
Iteration 25/1000 | Loss: 0.00001354
Iteration 26/1000 | Loss: 0.00001353
Iteration 27/1000 | Loss: 0.00001353
Iteration 28/1000 | Loss: 0.00001353
Iteration 29/1000 | Loss: 0.00001353
Iteration 30/1000 | Loss: 0.00001353
Iteration 31/1000 | Loss: 0.00001353
Iteration 32/1000 | Loss: 0.00001352
Iteration 33/1000 | Loss: 0.00001352
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001352
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001351
Iteration 41/1000 | Loss: 0.00001351
Iteration 42/1000 | Loss: 0.00001351
Iteration 43/1000 | Loss: 0.00001351
Iteration 44/1000 | Loss: 0.00001351
Iteration 45/1000 | Loss: 0.00001351
Iteration 46/1000 | Loss: 0.00001351
Iteration 47/1000 | Loss: 0.00001350
Iteration 48/1000 | Loss: 0.00001350
Iteration 49/1000 | Loss: 0.00001350
Iteration 50/1000 | Loss: 0.00001350
Iteration 51/1000 | Loss: 0.00001350
Iteration 52/1000 | Loss: 0.00001350
Iteration 53/1000 | Loss: 0.00001350
Iteration 54/1000 | Loss: 0.00001350
Iteration 55/1000 | Loss: 0.00001349
Iteration 56/1000 | Loss: 0.00001349
Iteration 57/1000 | Loss: 0.00001349
Iteration 58/1000 | Loss: 0.00001349
Iteration 59/1000 | Loss: 0.00001349
Iteration 60/1000 | Loss: 0.00001349
Iteration 61/1000 | Loss: 0.00001349
Iteration 62/1000 | Loss: 0.00001349
Iteration 63/1000 | Loss: 0.00001349
Iteration 64/1000 | Loss: 0.00001349
Iteration 65/1000 | Loss: 0.00001348
Iteration 66/1000 | Loss: 0.00001348
Iteration 67/1000 | Loss: 0.00001348
Iteration 68/1000 | Loss: 0.00001348
Iteration 69/1000 | Loss: 0.00001348
Iteration 70/1000 | Loss: 0.00001348
Iteration 71/1000 | Loss: 0.00001348
Iteration 72/1000 | Loss: 0.00001348
Iteration 73/1000 | Loss: 0.00001348
Iteration 74/1000 | Loss: 0.00001348
Iteration 75/1000 | Loss: 0.00001348
Iteration 76/1000 | Loss: 0.00001348
Iteration 77/1000 | Loss: 0.00001348
Iteration 78/1000 | Loss: 0.00001348
Iteration 79/1000 | Loss: 0.00001348
Iteration 80/1000 | Loss: 0.00001348
Iteration 81/1000 | Loss: 0.00001348
Iteration 82/1000 | Loss: 0.00001347
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001347
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001347
Iteration 88/1000 | Loss: 0.00001347
Iteration 89/1000 | Loss: 0.00001347
Iteration 90/1000 | Loss: 0.00001347
Iteration 91/1000 | Loss: 0.00001347
Iteration 92/1000 | Loss: 0.00001347
Iteration 93/1000 | Loss: 0.00001347
Iteration 94/1000 | Loss: 0.00001347
Iteration 95/1000 | Loss: 0.00001347
Iteration 96/1000 | Loss: 0.00001347
Iteration 97/1000 | Loss: 0.00001346
Iteration 98/1000 | Loss: 0.00001346
Iteration 99/1000 | Loss: 0.00001346
Iteration 100/1000 | Loss: 0.00001345
Iteration 101/1000 | Loss: 0.00001345
Iteration 102/1000 | Loss: 0.00001345
Iteration 103/1000 | Loss: 0.00001345
Iteration 104/1000 | Loss: 0.00001345
Iteration 105/1000 | Loss: 0.00001345
Iteration 106/1000 | Loss: 0.00001344
Iteration 107/1000 | Loss: 0.00001344
Iteration 108/1000 | Loss: 0.00001344
Iteration 109/1000 | Loss: 0.00001344
Iteration 110/1000 | Loss: 0.00001344
Iteration 111/1000 | Loss: 0.00001344
Iteration 112/1000 | Loss: 0.00001344
Iteration 113/1000 | Loss: 0.00001344
Iteration 114/1000 | Loss: 0.00001344
Iteration 115/1000 | Loss: 0.00001344
Iteration 116/1000 | Loss: 0.00001344
Iteration 117/1000 | Loss: 0.00001344
Iteration 118/1000 | Loss: 0.00001343
Iteration 119/1000 | Loss: 0.00001343
Iteration 120/1000 | Loss: 0.00001343
Iteration 121/1000 | Loss: 0.00001342
Iteration 122/1000 | Loss: 0.00001342
Iteration 123/1000 | Loss: 0.00001342
Iteration 124/1000 | Loss: 0.00001342
Iteration 125/1000 | Loss: 0.00001342
Iteration 126/1000 | Loss: 0.00001342
Iteration 127/1000 | Loss: 0.00001342
Iteration 128/1000 | Loss: 0.00001342
Iteration 129/1000 | Loss: 0.00001342
Iteration 130/1000 | Loss: 0.00001342
Iteration 131/1000 | Loss: 0.00001341
Iteration 132/1000 | Loss: 0.00001341
Iteration 133/1000 | Loss: 0.00001341
Iteration 134/1000 | Loss: 0.00001341
Iteration 135/1000 | Loss: 0.00001341
Iteration 136/1000 | Loss: 0.00001341
Iteration 137/1000 | Loss: 0.00001341
Iteration 138/1000 | Loss: 0.00001341
Iteration 139/1000 | Loss: 0.00001341
Iteration 140/1000 | Loss: 0.00001341
Iteration 141/1000 | Loss: 0.00001341
Iteration 142/1000 | Loss: 0.00001341
Iteration 143/1000 | Loss: 0.00001341
Iteration 144/1000 | Loss: 0.00001340
Iteration 145/1000 | Loss: 0.00001340
Iteration 146/1000 | Loss: 0.00001340
Iteration 147/1000 | Loss: 0.00001340
Iteration 148/1000 | Loss: 0.00001340
Iteration 149/1000 | Loss: 0.00001340
Iteration 150/1000 | Loss: 0.00001340
Iteration 151/1000 | Loss: 0.00001340
Iteration 152/1000 | Loss: 0.00001340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.340387825621292e-05, 1.340387825621292e-05, 1.340387825621292e-05, 1.340387825621292e-05, 1.340387825621292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.340387825621292e-05

Optimization complete. Final v2v error: 3.1444108486175537 mm

Highest mean error: 3.366982936859131 mm for frame 138

Lowest mean error: 2.701127290725708 mm for frame 1

Saving results

Total time: 36.93250298500061
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973514
Iteration 2/25 | Loss: 0.00154688
Iteration 3/25 | Loss: 0.00103753
Iteration 4/25 | Loss: 0.00098105
Iteration 5/25 | Loss: 0.00095572
Iteration 6/25 | Loss: 0.00094975
Iteration 7/25 | Loss: 0.00094843
Iteration 8/25 | Loss: 0.00094843
Iteration 9/25 | Loss: 0.00094843
Iteration 10/25 | Loss: 0.00094843
Iteration 11/25 | Loss: 0.00094843
Iteration 12/25 | Loss: 0.00094843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000948430213611573, 0.000948430213611573, 0.000948430213611573, 0.000948430213611573, 0.000948430213611573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000948430213611573

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16713393
Iteration 2/25 | Loss: 0.00029055
Iteration 3/25 | Loss: 0.00029055
Iteration 4/25 | Loss: 0.00029055
Iteration 5/25 | Loss: 0.00029054
Iteration 6/25 | Loss: 0.00029054
Iteration 7/25 | Loss: 0.00029054
Iteration 8/25 | Loss: 0.00029054
Iteration 9/25 | Loss: 0.00029054
Iteration 10/25 | Loss: 0.00029054
Iteration 11/25 | Loss: 0.00029054
Iteration 12/25 | Loss: 0.00029054
Iteration 13/25 | Loss: 0.00029054
Iteration 14/25 | Loss: 0.00029054
Iteration 15/25 | Loss: 0.00029054
Iteration 16/25 | Loss: 0.00029054
Iteration 17/25 | Loss: 0.00029054
Iteration 18/25 | Loss: 0.00029054
Iteration 19/25 | Loss: 0.00029054
Iteration 20/25 | Loss: 0.00029054
Iteration 21/25 | Loss: 0.00029054
Iteration 22/25 | Loss: 0.00029054
Iteration 23/25 | Loss: 0.00029054
Iteration 24/25 | Loss: 0.00029054
Iteration 25/25 | Loss: 0.00029054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029054
Iteration 2/1000 | Loss: 0.00007365
Iteration 3/1000 | Loss: 0.00005600
Iteration 4/1000 | Loss: 0.00005163
Iteration 5/1000 | Loss: 0.00004890
Iteration 6/1000 | Loss: 0.00004715
Iteration 7/1000 | Loss: 0.00004635
Iteration 8/1000 | Loss: 0.00004552
Iteration 9/1000 | Loss: 0.00004490
Iteration 10/1000 | Loss: 0.00004445
Iteration 11/1000 | Loss: 0.00004407
Iteration 12/1000 | Loss: 0.00004382
Iteration 13/1000 | Loss: 0.00004368
Iteration 14/1000 | Loss: 0.00004359
Iteration 15/1000 | Loss: 0.00004357
Iteration 16/1000 | Loss: 0.00004351
Iteration 17/1000 | Loss: 0.00004348
Iteration 18/1000 | Loss: 0.00004347
Iteration 19/1000 | Loss: 0.00004346
Iteration 20/1000 | Loss: 0.00004346
Iteration 21/1000 | Loss: 0.00004341
Iteration 22/1000 | Loss: 0.00004338
Iteration 23/1000 | Loss: 0.00004338
Iteration 24/1000 | Loss: 0.00004337
Iteration 25/1000 | Loss: 0.00004336
Iteration 26/1000 | Loss: 0.00004336
Iteration 27/1000 | Loss: 0.00004336
Iteration 28/1000 | Loss: 0.00004333
Iteration 29/1000 | Loss: 0.00004332
Iteration 30/1000 | Loss: 0.00004331
Iteration 31/1000 | Loss: 0.00004331
Iteration 32/1000 | Loss: 0.00004330
Iteration 33/1000 | Loss: 0.00004329
Iteration 34/1000 | Loss: 0.00004329
Iteration 35/1000 | Loss: 0.00004329
Iteration 36/1000 | Loss: 0.00004328
Iteration 37/1000 | Loss: 0.00004328
Iteration 38/1000 | Loss: 0.00004328
Iteration 39/1000 | Loss: 0.00004328
Iteration 40/1000 | Loss: 0.00004327
Iteration 41/1000 | Loss: 0.00004326
Iteration 42/1000 | Loss: 0.00004326
Iteration 43/1000 | Loss: 0.00004326
Iteration 44/1000 | Loss: 0.00004325
Iteration 45/1000 | Loss: 0.00004325
Iteration 46/1000 | Loss: 0.00004324
Iteration 47/1000 | Loss: 0.00004324
Iteration 48/1000 | Loss: 0.00004324
Iteration 49/1000 | Loss: 0.00004323
Iteration 50/1000 | Loss: 0.00004323
Iteration 51/1000 | Loss: 0.00004323
Iteration 52/1000 | Loss: 0.00004323
Iteration 53/1000 | Loss: 0.00004323
Iteration 54/1000 | Loss: 0.00004323
Iteration 55/1000 | Loss: 0.00004323
Iteration 56/1000 | Loss: 0.00004322
Iteration 57/1000 | Loss: 0.00004322
Iteration 58/1000 | Loss: 0.00004322
Iteration 59/1000 | Loss: 0.00004321
Iteration 60/1000 | Loss: 0.00004321
Iteration 61/1000 | Loss: 0.00004321
Iteration 62/1000 | Loss: 0.00004320
Iteration 63/1000 | Loss: 0.00004320
Iteration 64/1000 | Loss: 0.00004320
Iteration 65/1000 | Loss: 0.00004319
Iteration 66/1000 | Loss: 0.00004319
Iteration 67/1000 | Loss: 0.00004319
Iteration 68/1000 | Loss: 0.00004318
Iteration 69/1000 | Loss: 0.00004318
Iteration 70/1000 | Loss: 0.00004318
Iteration 71/1000 | Loss: 0.00004317
Iteration 72/1000 | Loss: 0.00004317
Iteration 73/1000 | Loss: 0.00004317
Iteration 74/1000 | Loss: 0.00004317
Iteration 75/1000 | Loss: 0.00004316
Iteration 76/1000 | Loss: 0.00004316
Iteration 77/1000 | Loss: 0.00004316
Iteration 78/1000 | Loss: 0.00004316
Iteration 79/1000 | Loss: 0.00004316
Iteration 80/1000 | Loss: 0.00004315
Iteration 81/1000 | Loss: 0.00004315
Iteration 82/1000 | Loss: 0.00004315
Iteration 83/1000 | Loss: 0.00004315
Iteration 84/1000 | Loss: 0.00004315
Iteration 85/1000 | Loss: 0.00004315
Iteration 86/1000 | Loss: 0.00004315
Iteration 87/1000 | Loss: 0.00004315
Iteration 88/1000 | Loss: 0.00004315
Iteration 89/1000 | Loss: 0.00004315
Iteration 90/1000 | Loss: 0.00004314
Iteration 91/1000 | Loss: 0.00004314
Iteration 92/1000 | Loss: 0.00004313
Iteration 93/1000 | Loss: 0.00004313
Iteration 94/1000 | Loss: 0.00004313
Iteration 95/1000 | Loss: 0.00004312
Iteration 96/1000 | Loss: 0.00004312
Iteration 97/1000 | Loss: 0.00004312
Iteration 98/1000 | Loss: 0.00004312
Iteration 99/1000 | Loss: 0.00004311
Iteration 100/1000 | Loss: 0.00004311
Iteration 101/1000 | Loss: 0.00004310
Iteration 102/1000 | Loss: 0.00004310
Iteration 103/1000 | Loss: 0.00004310
Iteration 104/1000 | Loss: 0.00004310
Iteration 105/1000 | Loss: 0.00004309
Iteration 106/1000 | Loss: 0.00004309
Iteration 107/1000 | Loss: 0.00004309
Iteration 108/1000 | Loss: 0.00004308
Iteration 109/1000 | Loss: 0.00004308
Iteration 110/1000 | Loss: 0.00004308
Iteration 111/1000 | Loss: 0.00004307
Iteration 112/1000 | Loss: 0.00004307
Iteration 113/1000 | Loss: 0.00004307
Iteration 114/1000 | Loss: 0.00004307
Iteration 115/1000 | Loss: 0.00004306
Iteration 116/1000 | Loss: 0.00004306
Iteration 117/1000 | Loss: 0.00004306
Iteration 118/1000 | Loss: 0.00004305
Iteration 119/1000 | Loss: 0.00004305
Iteration 120/1000 | Loss: 0.00004305
Iteration 121/1000 | Loss: 0.00004305
Iteration 122/1000 | Loss: 0.00004304
Iteration 123/1000 | Loss: 0.00004304
Iteration 124/1000 | Loss: 0.00004304
Iteration 125/1000 | Loss: 0.00004303
Iteration 126/1000 | Loss: 0.00004303
Iteration 127/1000 | Loss: 0.00004303
Iteration 128/1000 | Loss: 0.00004302
Iteration 129/1000 | Loss: 0.00004302
Iteration 130/1000 | Loss: 0.00004302
Iteration 131/1000 | Loss: 0.00004302
Iteration 132/1000 | Loss: 0.00004301
Iteration 133/1000 | Loss: 0.00004301
Iteration 134/1000 | Loss: 0.00004301
Iteration 135/1000 | Loss: 0.00004301
Iteration 136/1000 | Loss: 0.00004300
Iteration 137/1000 | Loss: 0.00004300
Iteration 138/1000 | Loss: 0.00004300
Iteration 139/1000 | Loss: 0.00004300
Iteration 140/1000 | Loss: 0.00004299
Iteration 141/1000 | Loss: 0.00004299
Iteration 142/1000 | Loss: 0.00004299
Iteration 143/1000 | Loss: 0.00004299
Iteration 144/1000 | Loss: 0.00004299
Iteration 145/1000 | Loss: 0.00004298
Iteration 146/1000 | Loss: 0.00004298
Iteration 147/1000 | Loss: 0.00004298
Iteration 148/1000 | Loss: 0.00004298
Iteration 149/1000 | Loss: 0.00004298
Iteration 150/1000 | Loss: 0.00004298
Iteration 151/1000 | Loss: 0.00004298
Iteration 152/1000 | Loss: 0.00004298
Iteration 153/1000 | Loss: 0.00004298
Iteration 154/1000 | Loss: 0.00004298
Iteration 155/1000 | Loss: 0.00004297
Iteration 156/1000 | Loss: 0.00004297
Iteration 157/1000 | Loss: 0.00004297
Iteration 158/1000 | Loss: 0.00004296
Iteration 159/1000 | Loss: 0.00004296
Iteration 160/1000 | Loss: 0.00004296
Iteration 161/1000 | Loss: 0.00004296
Iteration 162/1000 | Loss: 0.00004296
Iteration 163/1000 | Loss: 0.00004296
Iteration 164/1000 | Loss: 0.00004296
Iteration 165/1000 | Loss: 0.00004295
Iteration 166/1000 | Loss: 0.00004295
Iteration 167/1000 | Loss: 0.00004295
Iteration 168/1000 | Loss: 0.00004295
Iteration 169/1000 | Loss: 0.00004294
Iteration 170/1000 | Loss: 0.00004294
Iteration 171/1000 | Loss: 0.00004294
Iteration 172/1000 | Loss: 0.00004294
Iteration 173/1000 | Loss: 0.00004294
Iteration 174/1000 | Loss: 0.00004294
Iteration 175/1000 | Loss: 0.00004293
Iteration 176/1000 | Loss: 0.00004293
Iteration 177/1000 | Loss: 0.00004293
Iteration 178/1000 | Loss: 0.00004293
Iteration 179/1000 | Loss: 0.00004293
Iteration 180/1000 | Loss: 0.00004293
Iteration 181/1000 | Loss: 0.00004293
Iteration 182/1000 | Loss: 0.00004293
Iteration 183/1000 | Loss: 0.00004293
Iteration 184/1000 | Loss: 0.00004293
Iteration 185/1000 | Loss: 0.00004293
Iteration 186/1000 | Loss: 0.00004293
Iteration 187/1000 | Loss: 0.00004292
Iteration 188/1000 | Loss: 0.00004292
Iteration 189/1000 | Loss: 0.00004292
Iteration 190/1000 | Loss: 0.00004292
Iteration 191/1000 | Loss: 0.00004292
Iteration 192/1000 | Loss: 0.00004292
Iteration 193/1000 | Loss: 0.00004292
Iteration 194/1000 | Loss: 0.00004292
Iteration 195/1000 | Loss: 0.00004292
Iteration 196/1000 | Loss: 0.00004292
Iteration 197/1000 | Loss: 0.00004292
Iteration 198/1000 | Loss: 0.00004292
Iteration 199/1000 | Loss: 0.00004291
Iteration 200/1000 | Loss: 0.00004291
Iteration 201/1000 | Loss: 0.00004291
Iteration 202/1000 | Loss: 0.00004291
Iteration 203/1000 | Loss: 0.00004291
Iteration 204/1000 | Loss: 0.00004291
Iteration 205/1000 | Loss: 0.00004291
Iteration 206/1000 | Loss: 0.00004291
Iteration 207/1000 | Loss: 0.00004291
Iteration 208/1000 | Loss: 0.00004291
Iteration 209/1000 | Loss: 0.00004291
Iteration 210/1000 | Loss: 0.00004291
Iteration 211/1000 | Loss: 0.00004291
Iteration 212/1000 | Loss: 0.00004291
Iteration 213/1000 | Loss: 0.00004291
Iteration 214/1000 | Loss: 0.00004291
Iteration 215/1000 | Loss: 0.00004291
Iteration 216/1000 | Loss: 0.00004291
Iteration 217/1000 | Loss: 0.00004291
Iteration 218/1000 | Loss: 0.00004291
Iteration 219/1000 | Loss: 0.00004291
Iteration 220/1000 | Loss: 0.00004291
Iteration 221/1000 | Loss: 0.00004291
Iteration 222/1000 | Loss: 0.00004291
Iteration 223/1000 | Loss: 0.00004291
Iteration 224/1000 | Loss: 0.00004291
Iteration 225/1000 | Loss: 0.00004291
Iteration 226/1000 | Loss: 0.00004291
Iteration 227/1000 | Loss: 0.00004291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [4.291165168979205e-05, 4.291165168979205e-05, 4.291165168979205e-05, 4.291165168979205e-05, 4.291165168979205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.291165168979205e-05

Optimization complete. Final v2v error: 5.268856525421143 mm

Highest mean error: 7.137966632843018 mm for frame 149

Lowest mean error: 4.261646270751953 mm for frame 19

Saving results

Total time: 57.1510374546051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01172472
Iteration 2/25 | Loss: 0.00398807
Iteration 3/25 | Loss: 0.00317606
Iteration 4/25 | Loss: 0.00234229
Iteration 5/25 | Loss: 0.00186382
Iteration 6/25 | Loss: 0.00171127
Iteration 7/25 | Loss: 0.00155478
Iteration 8/25 | Loss: 0.00141451
Iteration 9/25 | Loss: 0.00142900
Iteration 10/25 | Loss: 0.00137659
Iteration 11/25 | Loss: 0.00137533
Iteration 12/25 | Loss: 0.00138331
Iteration 13/25 | Loss: 0.00135208
Iteration 14/25 | Loss: 0.00137713
Iteration 15/25 | Loss: 0.00131434
Iteration 16/25 | Loss: 0.00130693
Iteration 17/25 | Loss: 0.00130592
Iteration 18/25 | Loss: 0.00130556
Iteration 19/25 | Loss: 0.00130539
Iteration 20/25 | Loss: 0.00130529
Iteration 21/25 | Loss: 0.00130529
Iteration 22/25 | Loss: 0.00130528
Iteration 23/25 | Loss: 0.00130528
Iteration 24/25 | Loss: 0.00130528
Iteration 25/25 | Loss: 0.00130528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.51075888
Iteration 2/25 | Loss: 0.00052912
Iteration 3/25 | Loss: 0.00052911
Iteration 4/25 | Loss: 0.00052911
Iteration 5/25 | Loss: 0.00052911
Iteration 6/25 | Loss: 0.00052910
Iteration 7/25 | Loss: 0.00052910
Iteration 8/25 | Loss: 0.00052910
Iteration 9/25 | Loss: 0.00052910
Iteration 10/25 | Loss: 0.00052910
Iteration 11/25 | Loss: 0.00052910
Iteration 12/25 | Loss: 0.00052910
Iteration 13/25 | Loss: 0.00052910
Iteration 14/25 | Loss: 0.00052910
Iteration 15/25 | Loss: 0.00052910
Iteration 16/25 | Loss: 0.00052910
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005291039124131203, 0.0005291039124131203, 0.0005291039124131203, 0.0005291039124131203, 0.0005291039124131203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005291039124131203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052910
Iteration 2/1000 | Loss: 0.00301865
Iteration 3/1000 | Loss: 0.00346687
Iteration 4/1000 | Loss: 0.00296281
Iteration 5/1000 | Loss: 0.00181753
Iteration 6/1000 | Loss: 0.00232353
Iteration 7/1000 | Loss: 0.00165764
Iteration 8/1000 | Loss: 0.00206956
Iteration 9/1000 | Loss: 0.00130339
Iteration 10/1000 | Loss: 0.00158706
Iteration 11/1000 | Loss: 0.00087921
Iteration 12/1000 | Loss: 0.00139747
Iteration 13/1000 | Loss: 0.00066989
Iteration 14/1000 | Loss: 0.00104611
Iteration 15/1000 | Loss: 0.00073624
Iteration 16/1000 | Loss: 0.00031701
Iteration 17/1000 | Loss: 0.00024716
Iteration 18/1000 | Loss: 0.00015192
Iteration 19/1000 | Loss: 0.00054346
Iteration 20/1000 | Loss: 0.00024001
Iteration 21/1000 | Loss: 0.00011770
Iteration 22/1000 | Loss: 0.00010388
Iteration 23/1000 | Loss: 0.00009416
Iteration 24/1000 | Loss: 0.00008839
Iteration 25/1000 | Loss: 0.00008342
Iteration 26/1000 | Loss: 0.00007899
Iteration 27/1000 | Loss: 0.00020344
Iteration 28/1000 | Loss: 0.00008921
Iteration 29/1000 | Loss: 0.00008333
Iteration 30/1000 | Loss: 0.00019251
Iteration 31/1000 | Loss: 0.00058337
Iteration 32/1000 | Loss: 0.00049375
Iteration 33/1000 | Loss: 0.00011666
Iteration 34/1000 | Loss: 0.00010052
Iteration 35/1000 | Loss: 0.00009225
Iteration 36/1000 | Loss: 0.00008689
Iteration 37/1000 | Loss: 0.00008262
Iteration 38/1000 | Loss: 0.00007771
Iteration 39/1000 | Loss: 0.00007600
Iteration 40/1000 | Loss: 0.00022905
Iteration 41/1000 | Loss: 0.00009341
Iteration 42/1000 | Loss: 0.00008043
Iteration 43/1000 | Loss: 0.00007597
Iteration 44/1000 | Loss: 0.00007415
Iteration 45/1000 | Loss: 0.00024062
Iteration 46/1000 | Loss: 0.00007894
Iteration 47/1000 | Loss: 0.00007379
Iteration 48/1000 | Loss: 0.00007128
Iteration 49/1000 | Loss: 0.00006998
Iteration 50/1000 | Loss: 0.00006907
Iteration 51/1000 | Loss: 0.00006829
Iteration 52/1000 | Loss: 0.00006762
Iteration 53/1000 | Loss: 0.00008455
Iteration 54/1000 | Loss: 0.00007186
Iteration 55/1000 | Loss: 0.00006779
Iteration 56/1000 | Loss: 0.00006639
Iteration 57/1000 | Loss: 0.00006549
Iteration 58/1000 | Loss: 0.00006496
Iteration 59/1000 | Loss: 0.00006467
Iteration 60/1000 | Loss: 0.00022980
Iteration 61/1000 | Loss: 0.00007644
Iteration 62/1000 | Loss: 0.00007272
Iteration 63/1000 | Loss: 0.00006952
Iteration 64/1000 | Loss: 0.00006848
Iteration 65/1000 | Loss: 0.00008010
Iteration 66/1000 | Loss: 0.00017705
Iteration 67/1000 | Loss: 0.00006959
Iteration 68/1000 | Loss: 0.00006730
Iteration 69/1000 | Loss: 0.00006516
Iteration 70/1000 | Loss: 0.00006395
Iteration 71/1000 | Loss: 0.00006311
Iteration 72/1000 | Loss: 0.00009433
Iteration 73/1000 | Loss: 0.00008249
Iteration 74/1000 | Loss: 0.00006772
Iteration 75/1000 | Loss: 0.00006386
Iteration 76/1000 | Loss: 0.00007234
Iteration 77/1000 | Loss: 0.00006378
Iteration 78/1000 | Loss: 0.00007198
Iteration 79/1000 | Loss: 0.00006195
Iteration 80/1000 | Loss: 0.00006134
Iteration 81/1000 | Loss: 0.00007540
Iteration 82/1000 | Loss: 0.00006169
Iteration 83/1000 | Loss: 0.00006029
Iteration 84/1000 | Loss: 0.00005955
Iteration 85/1000 | Loss: 0.00005928
Iteration 86/1000 | Loss: 0.00005898
Iteration 87/1000 | Loss: 0.00005879
Iteration 88/1000 | Loss: 0.00005876
Iteration 89/1000 | Loss: 0.00005861
Iteration 90/1000 | Loss: 0.00005858
Iteration 91/1000 | Loss: 0.00005856
Iteration 92/1000 | Loss: 0.00005856
Iteration 93/1000 | Loss: 0.00005856
Iteration 94/1000 | Loss: 0.00005856
Iteration 95/1000 | Loss: 0.00005856
Iteration 96/1000 | Loss: 0.00005856
Iteration 97/1000 | Loss: 0.00005856
Iteration 98/1000 | Loss: 0.00005856
Iteration 99/1000 | Loss: 0.00005856
Iteration 100/1000 | Loss: 0.00005856
Iteration 101/1000 | Loss: 0.00005852
Iteration 102/1000 | Loss: 0.00005850
Iteration 103/1000 | Loss: 0.00005850
Iteration 104/1000 | Loss: 0.00005850
Iteration 105/1000 | Loss: 0.00005850
Iteration 106/1000 | Loss: 0.00005850
Iteration 107/1000 | Loss: 0.00005850
Iteration 108/1000 | Loss: 0.00005850
Iteration 109/1000 | Loss: 0.00005850
Iteration 110/1000 | Loss: 0.00005850
Iteration 111/1000 | Loss: 0.00005850
Iteration 112/1000 | Loss: 0.00005849
Iteration 113/1000 | Loss: 0.00005849
Iteration 114/1000 | Loss: 0.00005849
Iteration 115/1000 | Loss: 0.00005848
Iteration 116/1000 | Loss: 0.00005848
Iteration 117/1000 | Loss: 0.00005847
Iteration 118/1000 | Loss: 0.00005847
Iteration 119/1000 | Loss: 0.00005847
Iteration 120/1000 | Loss: 0.00005847
Iteration 121/1000 | Loss: 0.00005847
Iteration 122/1000 | Loss: 0.00005846
Iteration 123/1000 | Loss: 0.00005846
Iteration 124/1000 | Loss: 0.00005846
Iteration 125/1000 | Loss: 0.00005846
Iteration 126/1000 | Loss: 0.00005846
Iteration 127/1000 | Loss: 0.00005846
Iteration 128/1000 | Loss: 0.00005845
Iteration 129/1000 | Loss: 0.00005845
Iteration 130/1000 | Loss: 0.00005845
Iteration 131/1000 | Loss: 0.00005845
Iteration 132/1000 | Loss: 0.00005845
Iteration 133/1000 | Loss: 0.00005845
Iteration 134/1000 | Loss: 0.00005845
Iteration 135/1000 | Loss: 0.00005845
Iteration 136/1000 | Loss: 0.00005845
Iteration 137/1000 | Loss: 0.00005845
Iteration 138/1000 | Loss: 0.00005845
Iteration 139/1000 | Loss: 0.00005845
Iteration 140/1000 | Loss: 0.00005845
Iteration 141/1000 | Loss: 0.00005845
Iteration 142/1000 | Loss: 0.00005845
Iteration 143/1000 | Loss: 0.00005844
Iteration 144/1000 | Loss: 0.00005844
Iteration 145/1000 | Loss: 0.00005844
Iteration 146/1000 | Loss: 0.00005844
Iteration 147/1000 | Loss: 0.00005843
Iteration 148/1000 | Loss: 0.00005843
Iteration 149/1000 | Loss: 0.00005843
Iteration 150/1000 | Loss: 0.00005843
Iteration 151/1000 | Loss: 0.00005843
Iteration 152/1000 | Loss: 0.00005843
Iteration 153/1000 | Loss: 0.00005843
Iteration 154/1000 | Loss: 0.00005842
Iteration 155/1000 | Loss: 0.00005842
Iteration 156/1000 | Loss: 0.00005841
Iteration 157/1000 | Loss: 0.00005840
Iteration 158/1000 | Loss: 0.00005840
Iteration 159/1000 | Loss: 0.00005840
Iteration 160/1000 | Loss: 0.00005840
Iteration 161/1000 | Loss: 0.00005840
Iteration 162/1000 | Loss: 0.00005840
Iteration 163/1000 | Loss: 0.00005840
Iteration 164/1000 | Loss: 0.00005840
Iteration 165/1000 | Loss: 0.00005840
Iteration 166/1000 | Loss: 0.00005839
Iteration 167/1000 | Loss: 0.00005839
Iteration 168/1000 | Loss: 0.00005839
Iteration 169/1000 | Loss: 0.00005839
Iteration 170/1000 | Loss: 0.00005838
Iteration 171/1000 | Loss: 0.00005838
Iteration 172/1000 | Loss: 0.00005838
Iteration 173/1000 | Loss: 0.00005838
Iteration 174/1000 | Loss: 0.00005838
Iteration 175/1000 | Loss: 0.00005837
Iteration 176/1000 | Loss: 0.00005837
Iteration 177/1000 | Loss: 0.00005837
Iteration 178/1000 | Loss: 0.00005837
Iteration 179/1000 | Loss: 0.00005836
Iteration 180/1000 | Loss: 0.00005836
Iteration 181/1000 | Loss: 0.00005836
Iteration 182/1000 | Loss: 0.00005836
Iteration 183/1000 | Loss: 0.00005836
Iteration 184/1000 | Loss: 0.00005836
Iteration 185/1000 | Loss: 0.00005836
Iteration 186/1000 | Loss: 0.00005836
Iteration 187/1000 | Loss: 0.00005836
Iteration 188/1000 | Loss: 0.00005836
Iteration 189/1000 | Loss: 0.00005836
Iteration 190/1000 | Loss: 0.00005836
Iteration 191/1000 | Loss: 0.00005836
Iteration 192/1000 | Loss: 0.00005836
Iteration 193/1000 | Loss: 0.00005836
Iteration 194/1000 | Loss: 0.00005836
Iteration 195/1000 | Loss: 0.00005836
Iteration 196/1000 | Loss: 0.00005836
Iteration 197/1000 | Loss: 0.00005836
Iteration 198/1000 | Loss: 0.00005835
Iteration 199/1000 | Loss: 0.00005835
Iteration 200/1000 | Loss: 0.00005835
Iteration 201/1000 | Loss: 0.00005835
Iteration 202/1000 | Loss: 0.00005835
Iteration 203/1000 | Loss: 0.00005834
Iteration 204/1000 | Loss: 0.00005834
Iteration 205/1000 | Loss: 0.00005834
Iteration 206/1000 | Loss: 0.00005834
Iteration 207/1000 | Loss: 0.00005834
Iteration 208/1000 | Loss: 0.00005834
Iteration 209/1000 | Loss: 0.00005834
Iteration 210/1000 | Loss: 0.00005834
Iteration 211/1000 | Loss: 0.00005834
Iteration 212/1000 | Loss: 0.00005834
Iteration 213/1000 | Loss: 0.00005834
Iteration 214/1000 | Loss: 0.00005834
Iteration 215/1000 | Loss: 0.00005833
Iteration 216/1000 | Loss: 0.00005833
Iteration 217/1000 | Loss: 0.00005833
Iteration 218/1000 | Loss: 0.00005833
Iteration 219/1000 | Loss: 0.00005832
Iteration 220/1000 | Loss: 0.00005831
Iteration 221/1000 | Loss: 0.00005831
Iteration 222/1000 | Loss: 0.00005831
Iteration 223/1000 | Loss: 0.00005830
Iteration 224/1000 | Loss: 0.00005830
Iteration 225/1000 | Loss: 0.00005830
Iteration 226/1000 | Loss: 0.00005830
Iteration 227/1000 | Loss: 0.00005830
Iteration 228/1000 | Loss: 0.00005829
Iteration 229/1000 | Loss: 0.00005829
Iteration 230/1000 | Loss: 0.00005829
Iteration 231/1000 | Loss: 0.00005829
Iteration 232/1000 | Loss: 0.00005829
Iteration 233/1000 | Loss: 0.00005829
Iteration 234/1000 | Loss: 0.00005828
Iteration 235/1000 | Loss: 0.00005828
Iteration 236/1000 | Loss: 0.00005828
Iteration 237/1000 | Loss: 0.00005828
Iteration 238/1000 | Loss: 0.00005828
Iteration 239/1000 | Loss: 0.00005828
Iteration 240/1000 | Loss: 0.00005828
Iteration 241/1000 | Loss: 0.00005828
Iteration 242/1000 | Loss: 0.00005828
Iteration 243/1000 | Loss: 0.00005828
Iteration 244/1000 | Loss: 0.00005828
Iteration 245/1000 | Loss: 0.00005828
Iteration 246/1000 | Loss: 0.00005828
Iteration 247/1000 | Loss: 0.00005828
Iteration 248/1000 | Loss: 0.00005828
Iteration 249/1000 | Loss: 0.00005828
Iteration 250/1000 | Loss: 0.00005828
Iteration 251/1000 | Loss: 0.00005828
Iteration 252/1000 | Loss: 0.00005828
Iteration 253/1000 | Loss: 0.00005828
Iteration 254/1000 | Loss: 0.00005828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [5.8277390053262934e-05, 5.8277390053262934e-05, 5.8277390053262934e-05, 5.8277390053262934e-05, 5.8277390053262934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.8277390053262934e-05

Optimization complete. Final v2v error: 5.9613237380981445 mm

Highest mean error: 6.95236873626709 mm for frame 60

Lowest mean error: 4.8920087814331055 mm for frame 121

Saving results

Total time: 174.32221484184265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904042
Iteration 2/25 | Loss: 0.00094488
Iteration 3/25 | Loss: 0.00077272
Iteration 4/25 | Loss: 0.00075014
Iteration 5/25 | Loss: 0.00074428
Iteration 6/25 | Loss: 0.00074255
Iteration 7/25 | Loss: 0.00074224
Iteration 8/25 | Loss: 0.00074224
Iteration 9/25 | Loss: 0.00074224
Iteration 10/25 | Loss: 0.00074224
Iteration 11/25 | Loss: 0.00074224
Iteration 12/25 | Loss: 0.00074224
Iteration 13/25 | Loss: 0.00074224
Iteration 14/25 | Loss: 0.00074224
Iteration 15/25 | Loss: 0.00074224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007422394119203091, 0.0007422394119203091, 0.0007422394119203091, 0.0007422394119203091, 0.0007422394119203091]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007422394119203091

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46113038
Iteration 2/25 | Loss: 0.00033937
Iteration 3/25 | Loss: 0.00033935
Iteration 4/25 | Loss: 0.00033935
Iteration 5/25 | Loss: 0.00033935
Iteration 6/25 | Loss: 0.00033935
Iteration 7/25 | Loss: 0.00033935
Iteration 8/25 | Loss: 0.00033935
Iteration 9/25 | Loss: 0.00033935
Iteration 10/25 | Loss: 0.00033935
Iteration 11/25 | Loss: 0.00033935
Iteration 12/25 | Loss: 0.00033935
Iteration 13/25 | Loss: 0.00033935
Iteration 14/25 | Loss: 0.00033935
Iteration 15/25 | Loss: 0.00033935
Iteration 16/25 | Loss: 0.00033935
Iteration 17/25 | Loss: 0.00033935
Iteration 18/25 | Loss: 0.00033935
Iteration 19/25 | Loss: 0.00033935
Iteration 20/25 | Loss: 0.00033935
Iteration 21/25 | Loss: 0.00033935
Iteration 22/25 | Loss: 0.00033935
Iteration 23/25 | Loss: 0.00033935
Iteration 24/25 | Loss: 0.00033935
Iteration 25/25 | Loss: 0.00033935
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00033934632665477693, 0.00033934632665477693, 0.00033934632665477693, 0.00033934632665477693, 0.00033934632665477693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033934632665477693

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033935
Iteration 2/1000 | Loss: 0.00002956
Iteration 3/1000 | Loss: 0.00002288
Iteration 4/1000 | Loss: 0.00002004
Iteration 5/1000 | Loss: 0.00001864
Iteration 6/1000 | Loss: 0.00001797
Iteration 7/1000 | Loss: 0.00001737
Iteration 8/1000 | Loss: 0.00001698
Iteration 9/1000 | Loss: 0.00001669
Iteration 10/1000 | Loss: 0.00001650
Iteration 11/1000 | Loss: 0.00001646
Iteration 12/1000 | Loss: 0.00001642
Iteration 13/1000 | Loss: 0.00001642
Iteration 14/1000 | Loss: 0.00001642
Iteration 15/1000 | Loss: 0.00001642
Iteration 16/1000 | Loss: 0.00001641
Iteration 17/1000 | Loss: 0.00001640
Iteration 18/1000 | Loss: 0.00001639
Iteration 19/1000 | Loss: 0.00001638
Iteration 20/1000 | Loss: 0.00001638
Iteration 21/1000 | Loss: 0.00001638
Iteration 22/1000 | Loss: 0.00001638
Iteration 23/1000 | Loss: 0.00001638
Iteration 24/1000 | Loss: 0.00001638
Iteration 25/1000 | Loss: 0.00001638
Iteration 26/1000 | Loss: 0.00001638
Iteration 27/1000 | Loss: 0.00001638
Iteration 28/1000 | Loss: 0.00001638
Iteration 29/1000 | Loss: 0.00001638
Iteration 30/1000 | Loss: 0.00001638
Iteration 31/1000 | Loss: 0.00001637
Iteration 32/1000 | Loss: 0.00001634
Iteration 33/1000 | Loss: 0.00001634
Iteration 34/1000 | Loss: 0.00001632
Iteration 35/1000 | Loss: 0.00001632
Iteration 36/1000 | Loss: 0.00001631
Iteration 37/1000 | Loss: 0.00001629
Iteration 38/1000 | Loss: 0.00001629
Iteration 39/1000 | Loss: 0.00001629
Iteration 40/1000 | Loss: 0.00001629
Iteration 41/1000 | Loss: 0.00001629
Iteration 42/1000 | Loss: 0.00001629
Iteration 43/1000 | Loss: 0.00001629
Iteration 44/1000 | Loss: 0.00001628
Iteration 45/1000 | Loss: 0.00001628
Iteration 46/1000 | Loss: 0.00001628
Iteration 47/1000 | Loss: 0.00001628
Iteration 48/1000 | Loss: 0.00001627
Iteration 49/1000 | Loss: 0.00001627
Iteration 50/1000 | Loss: 0.00001627
Iteration 51/1000 | Loss: 0.00001627
Iteration 52/1000 | Loss: 0.00001627
Iteration 53/1000 | Loss: 0.00001627
Iteration 54/1000 | Loss: 0.00001627
Iteration 55/1000 | Loss: 0.00001627
Iteration 56/1000 | Loss: 0.00001627
Iteration 57/1000 | Loss: 0.00001627
Iteration 58/1000 | Loss: 0.00001627
Iteration 59/1000 | Loss: 0.00001627
Iteration 60/1000 | Loss: 0.00001627
Iteration 61/1000 | Loss: 0.00001627
Iteration 62/1000 | Loss: 0.00001627
Iteration 63/1000 | Loss: 0.00001627
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001627
Iteration 67/1000 | Loss: 0.00001627
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [1.6272315406240523e-05, 1.6272315406240523e-05, 1.6272315406240523e-05, 1.6272315406240523e-05, 1.6272315406240523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6272315406240523e-05

Optimization complete. Final v2v error: 3.5398502349853516 mm

Highest mean error: 3.750781297683716 mm for frame 1

Lowest mean error: 3.136286973953247 mm for frame 131

Saving results

Total time: 29.00602650642395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00213958
Iteration 2/25 | Loss: 0.00096117
Iteration 3/25 | Loss: 0.00082794
Iteration 4/25 | Loss: 0.00079416
Iteration 5/25 | Loss: 0.00077907
Iteration 6/25 | Loss: 0.00077631
Iteration 7/25 | Loss: 0.00077540
Iteration 8/25 | Loss: 0.00077528
Iteration 9/25 | Loss: 0.00077528
Iteration 10/25 | Loss: 0.00077528
Iteration 11/25 | Loss: 0.00077528
Iteration 12/25 | Loss: 0.00077528
Iteration 13/25 | Loss: 0.00077528
Iteration 14/25 | Loss: 0.00077528
Iteration 15/25 | Loss: 0.00077528
Iteration 16/25 | Loss: 0.00077528
Iteration 17/25 | Loss: 0.00077528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007752782548777759, 0.0007752782548777759, 0.0007752782548777759, 0.0007752782548777759, 0.0007752782548777759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007752782548777759

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46292186
Iteration 2/25 | Loss: 0.00038771
Iteration 3/25 | Loss: 0.00038771
Iteration 4/25 | Loss: 0.00038771
Iteration 5/25 | Loss: 0.00038771
Iteration 6/25 | Loss: 0.00038771
Iteration 7/25 | Loss: 0.00038771
Iteration 8/25 | Loss: 0.00038771
Iteration 9/25 | Loss: 0.00038771
Iteration 10/25 | Loss: 0.00038771
Iteration 11/25 | Loss: 0.00038771
Iteration 12/25 | Loss: 0.00038771
Iteration 13/25 | Loss: 0.00038771
Iteration 14/25 | Loss: 0.00038771
Iteration 15/25 | Loss: 0.00038771
Iteration 16/25 | Loss: 0.00038771
Iteration 17/25 | Loss: 0.00038771
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003877096460200846, 0.0003877096460200846, 0.0003877096460200846, 0.0003877096460200846, 0.0003877096460200846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003877096460200846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038771
Iteration 2/1000 | Loss: 0.00005846
Iteration 3/1000 | Loss: 0.00003142
Iteration 4/1000 | Loss: 0.00002173
Iteration 5/1000 | Loss: 0.00001898
Iteration 6/1000 | Loss: 0.00001787
Iteration 7/1000 | Loss: 0.00001707
Iteration 8/1000 | Loss: 0.00001645
Iteration 9/1000 | Loss: 0.00001590
Iteration 10/1000 | Loss: 0.00001551
Iteration 11/1000 | Loss: 0.00001528
Iteration 12/1000 | Loss: 0.00001517
Iteration 13/1000 | Loss: 0.00001516
Iteration 14/1000 | Loss: 0.00001515
Iteration 15/1000 | Loss: 0.00001505
Iteration 16/1000 | Loss: 0.00001505
Iteration 17/1000 | Loss: 0.00001505
Iteration 18/1000 | Loss: 0.00001498
Iteration 19/1000 | Loss: 0.00001497
Iteration 20/1000 | Loss: 0.00001490
Iteration 21/1000 | Loss: 0.00001487
Iteration 22/1000 | Loss: 0.00001486
Iteration 23/1000 | Loss: 0.00001482
Iteration 24/1000 | Loss: 0.00001482
Iteration 25/1000 | Loss: 0.00001482
Iteration 26/1000 | Loss: 0.00001482
Iteration 27/1000 | Loss: 0.00001482
Iteration 28/1000 | Loss: 0.00001476
Iteration 29/1000 | Loss: 0.00001475
Iteration 30/1000 | Loss: 0.00001474
Iteration 31/1000 | Loss: 0.00001473
Iteration 32/1000 | Loss: 0.00001473
Iteration 33/1000 | Loss: 0.00001472
Iteration 34/1000 | Loss: 0.00001472
Iteration 35/1000 | Loss: 0.00001471
Iteration 36/1000 | Loss: 0.00001471
Iteration 37/1000 | Loss: 0.00001469
Iteration 38/1000 | Loss: 0.00001469
Iteration 39/1000 | Loss: 0.00001469
Iteration 40/1000 | Loss: 0.00001468
Iteration 41/1000 | Loss: 0.00001468
Iteration 42/1000 | Loss: 0.00001468
Iteration 43/1000 | Loss: 0.00001467
Iteration 44/1000 | Loss: 0.00001467
Iteration 45/1000 | Loss: 0.00001466
Iteration 46/1000 | Loss: 0.00001465
Iteration 47/1000 | Loss: 0.00001465
Iteration 48/1000 | Loss: 0.00001465
Iteration 49/1000 | Loss: 0.00001464
Iteration 50/1000 | Loss: 0.00001464
Iteration 51/1000 | Loss: 0.00001464
Iteration 52/1000 | Loss: 0.00001464
Iteration 53/1000 | Loss: 0.00001464
Iteration 54/1000 | Loss: 0.00001464
Iteration 55/1000 | Loss: 0.00001464
Iteration 56/1000 | Loss: 0.00001464
Iteration 57/1000 | Loss: 0.00001464
Iteration 58/1000 | Loss: 0.00001464
Iteration 59/1000 | Loss: 0.00001463
Iteration 60/1000 | Loss: 0.00001463
Iteration 61/1000 | Loss: 0.00001463
Iteration 62/1000 | Loss: 0.00001463
Iteration 63/1000 | Loss: 0.00001463
Iteration 64/1000 | Loss: 0.00001463
Iteration 65/1000 | Loss: 0.00001463
Iteration 66/1000 | Loss: 0.00001463
Iteration 67/1000 | Loss: 0.00001463
Iteration 68/1000 | Loss: 0.00001463
Iteration 69/1000 | Loss: 0.00001463
Iteration 70/1000 | Loss: 0.00001462
Iteration 71/1000 | Loss: 0.00001462
Iteration 72/1000 | Loss: 0.00001462
Iteration 73/1000 | Loss: 0.00001462
Iteration 74/1000 | Loss: 0.00001462
Iteration 75/1000 | Loss: 0.00001461
Iteration 76/1000 | Loss: 0.00001461
Iteration 77/1000 | Loss: 0.00001461
Iteration 78/1000 | Loss: 0.00001461
Iteration 79/1000 | Loss: 0.00001461
Iteration 80/1000 | Loss: 0.00001461
Iteration 81/1000 | Loss: 0.00001461
Iteration 82/1000 | Loss: 0.00001461
Iteration 83/1000 | Loss: 0.00001461
Iteration 84/1000 | Loss: 0.00001460
Iteration 85/1000 | Loss: 0.00001460
Iteration 86/1000 | Loss: 0.00001460
Iteration 87/1000 | Loss: 0.00001460
Iteration 88/1000 | Loss: 0.00001459
Iteration 89/1000 | Loss: 0.00001459
Iteration 90/1000 | Loss: 0.00001459
Iteration 91/1000 | Loss: 0.00001459
Iteration 92/1000 | Loss: 0.00001459
Iteration 93/1000 | Loss: 0.00001459
Iteration 94/1000 | Loss: 0.00001458
Iteration 95/1000 | Loss: 0.00001458
Iteration 96/1000 | Loss: 0.00001458
Iteration 97/1000 | Loss: 0.00001458
Iteration 98/1000 | Loss: 0.00001458
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001458
Iteration 101/1000 | Loss: 0.00001458
Iteration 102/1000 | Loss: 0.00001458
Iteration 103/1000 | Loss: 0.00001458
Iteration 104/1000 | Loss: 0.00001458
Iteration 105/1000 | Loss: 0.00001457
Iteration 106/1000 | Loss: 0.00001457
Iteration 107/1000 | Loss: 0.00001457
Iteration 108/1000 | Loss: 0.00001456
Iteration 109/1000 | Loss: 0.00001456
Iteration 110/1000 | Loss: 0.00001456
Iteration 111/1000 | Loss: 0.00001455
Iteration 112/1000 | Loss: 0.00001455
Iteration 113/1000 | Loss: 0.00001455
Iteration 114/1000 | Loss: 0.00001455
Iteration 115/1000 | Loss: 0.00001455
Iteration 116/1000 | Loss: 0.00001455
Iteration 117/1000 | Loss: 0.00001455
Iteration 118/1000 | Loss: 0.00001455
Iteration 119/1000 | Loss: 0.00001454
Iteration 120/1000 | Loss: 0.00001454
Iteration 121/1000 | Loss: 0.00001454
Iteration 122/1000 | Loss: 0.00001454
Iteration 123/1000 | Loss: 0.00001454
Iteration 124/1000 | Loss: 0.00001454
Iteration 125/1000 | Loss: 0.00001454
Iteration 126/1000 | Loss: 0.00001454
Iteration 127/1000 | Loss: 0.00001454
Iteration 128/1000 | Loss: 0.00001454
Iteration 129/1000 | Loss: 0.00001453
Iteration 130/1000 | Loss: 0.00001453
Iteration 131/1000 | Loss: 0.00001453
Iteration 132/1000 | Loss: 0.00001453
Iteration 133/1000 | Loss: 0.00001453
Iteration 134/1000 | Loss: 0.00001453
Iteration 135/1000 | Loss: 0.00001453
Iteration 136/1000 | Loss: 0.00001453
Iteration 137/1000 | Loss: 0.00001453
Iteration 138/1000 | Loss: 0.00001453
Iteration 139/1000 | Loss: 0.00001453
Iteration 140/1000 | Loss: 0.00001453
Iteration 141/1000 | Loss: 0.00001453
Iteration 142/1000 | Loss: 0.00001452
Iteration 143/1000 | Loss: 0.00001452
Iteration 144/1000 | Loss: 0.00001452
Iteration 145/1000 | Loss: 0.00001452
Iteration 146/1000 | Loss: 0.00001452
Iteration 147/1000 | Loss: 0.00001452
Iteration 148/1000 | Loss: 0.00001452
Iteration 149/1000 | Loss: 0.00001452
Iteration 150/1000 | Loss: 0.00001451
Iteration 151/1000 | Loss: 0.00001451
Iteration 152/1000 | Loss: 0.00001451
Iteration 153/1000 | Loss: 0.00001451
Iteration 154/1000 | Loss: 0.00001451
Iteration 155/1000 | Loss: 0.00001451
Iteration 156/1000 | Loss: 0.00001451
Iteration 157/1000 | Loss: 0.00001451
Iteration 158/1000 | Loss: 0.00001451
Iteration 159/1000 | Loss: 0.00001451
Iteration 160/1000 | Loss: 0.00001451
Iteration 161/1000 | Loss: 0.00001451
Iteration 162/1000 | Loss: 0.00001451
Iteration 163/1000 | Loss: 0.00001451
Iteration 164/1000 | Loss: 0.00001451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.4512587767967489e-05, 1.4512587767967489e-05, 1.4512587767967489e-05, 1.4512587767967489e-05, 1.4512587767967489e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4512587767967489e-05

Optimization complete. Final v2v error: 3.3005568981170654 mm

Highest mean error: 3.730775833129883 mm for frame 45

Lowest mean error: 2.7504570484161377 mm for frame 96

Saving results

Total time: 39.792481422424316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493912
Iteration 2/25 | Loss: 0.00090993
Iteration 3/25 | Loss: 0.00076748
Iteration 4/25 | Loss: 0.00073963
Iteration 5/25 | Loss: 0.00073286
Iteration 6/25 | Loss: 0.00073149
Iteration 7/25 | Loss: 0.00073121
Iteration 8/25 | Loss: 0.00073121
Iteration 9/25 | Loss: 0.00073121
Iteration 10/25 | Loss: 0.00073121
Iteration 11/25 | Loss: 0.00073121
Iteration 12/25 | Loss: 0.00073121
Iteration 13/25 | Loss: 0.00073121
Iteration 14/25 | Loss: 0.00073121
Iteration 15/25 | Loss: 0.00073121
Iteration 16/25 | Loss: 0.00073121
Iteration 17/25 | Loss: 0.00073121
Iteration 18/25 | Loss: 0.00073121
Iteration 19/25 | Loss: 0.00073121
Iteration 20/25 | Loss: 0.00073121
Iteration 21/25 | Loss: 0.00073121
Iteration 22/25 | Loss: 0.00073121
Iteration 23/25 | Loss: 0.00073121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007312082452699542, 0.0007312082452699542, 0.0007312082452699542, 0.0007312082452699542, 0.0007312082452699542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007312082452699542

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.90869474
Iteration 2/25 | Loss: 0.00038252
Iteration 3/25 | Loss: 0.00038252
Iteration 4/25 | Loss: 0.00038252
Iteration 5/25 | Loss: 0.00038251
Iteration 6/25 | Loss: 0.00038251
Iteration 7/25 | Loss: 0.00038251
Iteration 8/25 | Loss: 0.00038251
Iteration 9/25 | Loss: 0.00038251
Iteration 10/25 | Loss: 0.00038251
Iteration 11/25 | Loss: 0.00038251
Iteration 12/25 | Loss: 0.00038251
Iteration 13/25 | Loss: 0.00038251
Iteration 14/25 | Loss: 0.00038251
Iteration 15/25 | Loss: 0.00038251
Iteration 16/25 | Loss: 0.00038251
Iteration 17/25 | Loss: 0.00038251
Iteration 18/25 | Loss: 0.00038251
Iteration 19/25 | Loss: 0.00038251
Iteration 20/25 | Loss: 0.00038251
Iteration 21/25 | Loss: 0.00038251
Iteration 22/25 | Loss: 0.00038251
Iteration 23/25 | Loss: 0.00038251
Iteration 24/25 | Loss: 0.00038251
Iteration 25/25 | Loss: 0.00038251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038251
Iteration 2/1000 | Loss: 0.00002879
Iteration 3/1000 | Loss: 0.00001901
Iteration 4/1000 | Loss: 0.00001629
Iteration 5/1000 | Loss: 0.00001517
Iteration 6/1000 | Loss: 0.00001452
Iteration 7/1000 | Loss: 0.00001406
Iteration 8/1000 | Loss: 0.00001384
Iteration 9/1000 | Loss: 0.00001384
Iteration 10/1000 | Loss: 0.00001383
Iteration 11/1000 | Loss: 0.00001374
Iteration 12/1000 | Loss: 0.00001368
Iteration 13/1000 | Loss: 0.00001367
Iteration 14/1000 | Loss: 0.00001365
Iteration 15/1000 | Loss: 0.00001365
Iteration 16/1000 | Loss: 0.00001363
Iteration 17/1000 | Loss: 0.00001363
Iteration 18/1000 | Loss: 0.00001363
Iteration 19/1000 | Loss: 0.00001363
Iteration 20/1000 | Loss: 0.00001362
Iteration 21/1000 | Loss: 0.00001362
Iteration 22/1000 | Loss: 0.00001362
Iteration 23/1000 | Loss: 0.00001362
Iteration 24/1000 | Loss: 0.00001362
Iteration 25/1000 | Loss: 0.00001362
Iteration 26/1000 | Loss: 0.00001362
Iteration 27/1000 | Loss: 0.00001362
Iteration 28/1000 | Loss: 0.00001362
Iteration 29/1000 | Loss: 0.00001361
Iteration 30/1000 | Loss: 0.00001361
Iteration 31/1000 | Loss: 0.00001361
Iteration 32/1000 | Loss: 0.00001360
Iteration 33/1000 | Loss: 0.00001358
Iteration 34/1000 | Loss: 0.00001358
Iteration 35/1000 | Loss: 0.00001357
Iteration 36/1000 | Loss: 0.00001357
Iteration 37/1000 | Loss: 0.00001357
Iteration 38/1000 | Loss: 0.00001357
Iteration 39/1000 | Loss: 0.00001357
Iteration 40/1000 | Loss: 0.00001356
Iteration 41/1000 | Loss: 0.00001356
Iteration 42/1000 | Loss: 0.00001355
Iteration 43/1000 | Loss: 0.00001355
Iteration 44/1000 | Loss: 0.00001355
Iteration 45/1000 | Loss: 0.00001355
Iteration 46/1000 | Loss: 0.00001355
Iteration 47/1000 | Loss: 0.00001355
Iteration 48/1000 | Loss: 0.00001354
Iteration 49/1000 | Loss: 0.00001354
Iteration 50/1000 | Loss: 0.00001354
Iteration 51/1000 | Loss: 0.00001354
Iteration 52/1000 | Loss: 0.00001354
Iteration 53/1000 | Loss: 0.00001354
Iteration 54/1000 | Loss: 0.00001354
Iteration 55/1000 | Loss: 0.00001354
Iteration 56/1000 | Loss: 0.00001354
Iteration 57/1000 | Loss: 0.00001354
Iteration 58/1000 | Loss: 0.00001354
Iteration 59/1000 | Loss: 0.00001354
Iteration 60/1000 | Loss: 0.00001354
Iteration 61/1000 | Loss: 0.00001354
Iteration 62/1000 | Loss: 0.00001354
Iteration 63/1000 | Loss: 0.00001354
Iteration 64/1000 | Loss: 0.00001354
Iteration 65/1000 | Loss: 0.00001354
Iteration 66/1000 | Loss: 0.00001354
Iteration 67/1000 | Loss: 0.00001354
Iteration 68/1000 | Loss: 0.00001354
Iteration 69/1000 | Loss: 0.00001354
Iteration 70/1000 | Loss: 0.00001354
Iteration 71/1000 | Loss: 0.00001354
Iteration 72/1000 | Loss: 0.00001354
Iteration 73/1000 | Loss: 0.00001354
Iteration 74/1000 | Loss: 0.00001354
Iteration 75/1000 | Loss: 0.00001354
Iteration 76/1000 | Loss: 0.00001354
Iteration 77/1000 | Loss: 0.00001354
Iteration 78/1000 | Loss: 0.00001354
Iteration 79/1000 | Loss: 0.00001354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.3540777217713185e-05, 1.3540777217713185e-05, 1.3540777217713185e-05, 1.3540777217713185e-05, 1.3540777217713185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3540777217713185e-05

Optimization complete. Final v2v error: 3.1502485275268555 mm

Highest mean error: 3.8177003860473633 mm for frame 115

Lowest mean error: 2.6827504634857178 mm for frame 90

Saving results

Total time: 27.197509765625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01087868
Iteration 2/25 | Loss: 0.00398744
Iteration 3/25 | Loss: 0.00232951
Iteration 4/25 | Loss: 0.00199233
Iteration 5/25 | Loss: 0.00188695
Iteration 6/25 | Loss: 0.00179322
Iteration 7/25 | Loss: 0.00160355
Iteration 8/25 | Loss: 0.00128350
Iteration 9/25 | Loss: 0.00114794
Iteration 10/25 | Loss: 0.00101295
Iteration 11/25 | Loss: 0.00105430
Iteration 12/25 | Loss: 0.00096262
Iteration 13/25 | Loss: 0.00093080
Iteration 14/25 | Loss: 0.00094193
Iteration 15/25 | Loss: 0.00092297
Iteration 16/25 | Loss: 0.00092251
Iteration 17/25 | Loss: 0.00090326
Iteration 18/25 | Loss: 0.00086546
Iteration 19/25 | Loss: 0.00084046
Iteration 20/25 | Loss: 0.00084121
Iteration 21/25 | Loss: 0.00082310
Iteration 22/25 | Loss: 0.00082755
Iteration 23/25 | Loss: 0.00082293
Iteration 24/25 | Loss: 0.00082375
Iteration 25/25 | Loss: 0.00082348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45374691
Iteration 2/25 | Loss: 0.00120192
Iteration 3/25 | Loss: 0.00100212
Iteration 4/25 | Loss: 0.00100212
Iteration 5/25 | Loss: 0.00100212
Iteration 6/25 | Loss: 0.00100212
Iteration 7/25 | Loss: 0.00100212
Iteration 8/25 | Loss: 0.00100212
Iteration 9/25 | Loss: 0.00100212
Iteration 10/25 | Loss: 0.00100212
Iteration 11/25 | Loss: 0.00100212
Iteration 12/25 | Loss: 0.00100212
Iteration 13/25 | Loss: 0.00100212
Iteration 14/25 | Loss: 0.00100212
Iteration 15/25 | Loss: 0.00100212
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010021175257861614, 0.0010021175257861614, 0.0010021175257861614, 0.0010021175257861614, 0.0010021175257861614]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010021175257861614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100212
Iteration 2/1000 | Loss: 0.00118988
Iteration 3/1000 | Loss: 0.00049311
Iteration 4/1000 | Loss: 0.00026997
Iteration 5/1000 | Loss: 0.00017849
Iteration 6/1000 | Loss: 0.00012057
Iteration 7/1000 | Loss: 0.00033752
Iteration 8/1000 | Loss: 0.00015109
Iteration 9/1000 | Loss: 0.00010236
Iteration 10/1000 | Loss: 0.00010826
Iteration 11/1000 | Loss: 0.00011495
Iteration 12/1000 | Loss: 0.00015908
Iteration 13/1000 | Loss: 0.00010127
Iteration 14/1000 | Loss: 0.00013915
Iteration 15/1000 | Loss: 0.00022872
Iteration 16/1000 | Loss: 0.00008234
Iteration 17/1000 | Loss: 0.00082054
Iteration 18/1000 | Loss: 0.00057093
Iteration 19/1000 | Loss: 0.00079537
Iteration 20/1000 | Loss: 0.00052366
Iteration 21/1000 | Loss: 0.00912596
Iteration 22/1000 | Loss: 0.00670334
Iteration 23/1000 | Loss: 0.00220097
Iteration 24/1000 | Loss: 0.00066951
Iteration 25/1000 | Loss: 0.00011488
Iteration 26/1000 | Loss: 0.00015968
Iteration 27/1000 | Loss: 0.00016410
Iteration 28/1000 | Loss: 0.00017798
Iteration 29/1000 | Loss: 0.00026783
Iteration 30/1000 | Loss: 0.00008623
Iteration 31/1000 | Loss: 0.00016511
Iteration 32/1000 | Loss: 0.00004722
Iteration 33/1000 | Loss: 0.00007286
Iteration 34/1000 | Loss: 0.00010010
Iteration 35/1000 | Loss: 0.00004136
Iteration 36/1000 | Loss: 0.00005518
Iteration 37/1000 | Loss: 0.00016600
Iteration 38/1000 | Loss: 0.00035060
Iteration 39/1000 | Loss: 0.00002293
Iteration 40/1000 | Loss: 0.00001958
Iteration 41/1000 | Loss: 0.00006570
Iteration 42/1000 | Loss: 0.00001863
Iteration 43/1000 | Loss: 0.00004092
Iteration 44/1000 | Loss: 0.00002757
Iteration 45/1000 | Loss: 0.00007216
Iteration 46/1000 | Loss: 0.00004366
Iteration 47/1000 | Loss: 0.00005801
Iteration 48/1000 | Loss: 0.00006302
Iteration 49/1000 | Loss: 0.00011164
Iteration 50/1000 | Loss: 0.00002472
Iteration 51/1000 | Loss: 0.00004833
Iteration 52/1000 | Loss: 0.00002132
Iteration 53/1000 | Loss: 0.00002981
Iteration 54/1000 | Loss: 0.00003503
Iteration 55/1000 | Loss: 0.00009716
Iteration 56/1000 | Loss: 0.00002822
Iteration 57/1000 | Loss: 0.00001451
Iteration 58/1000 | Loss: 0.00003306
Iteration 59/1000 | Loss: 0.00001520
Iteration 60/1000 | Loss: 0.00003218
Iteration 61/1000 | Loss: 0.00003218
Iteration 62/1000 | Loss: 0.00031388
Iteration 63/1000 | Loss: 0.00002877
Iteration 64/1000 | Loss: 0.00004592
Iteration 65/1000 | Loss: 0.00004610
Iteration 66/1000 | Loss: 0.00004729
Iteration 67/1000 | Loss: 0.00002258
Iteration 68/1000 | Loss: 0.00002821
Iteration 69/1000 | Loss: 0.00001553
Iteration 70/1000 | Loss: 0.00001718
Iteration 71/1000 | Loss: 0.00003400
Iteration 72/1000 | Loss: 0.00003194
Iteration 73/1000 | Loss: 0.00001585
Iteration 74/1000 | Loss: 0.00001801
Iteration 75/1000 | Loss: 0.00001407
Iteration 76/1000 | Loss: 0.00003240
Iteration 77/1000 | Loss: 0.00001615
Iteration 78/1000 | Loss: 0.00001492
Iteration 79/1000 | Loss: 0.00001402
Iteration 80/1000 | Loss: 0.00001402
Iteration 81/1000 | Loss: 0.00001402
Iteration 82/1000 | Loss: 0.00001401
Iteration 83/1000 | Loss: 0.00001401
Iteration 84/1000 | Loss: 0.00001401
Iteration 85/1000 | Loss: 0.00001401
Iteration 86/1000 | Loss: 0.00001401
Iteration 87/1000 | Loss: 0.00001401
Iteration 88/1000 | Loss: 0.00001401
Iteration 89/1000 | Loss: 0.00001400
Iteration 90/1000 | Loss: 0.00001399
Iteration 91/1000 | Loss: 0.00001594
Iteration 92/1000 | Loss: 0.00001594
Iteration 93/1000 | Loss: 0.00005316
Iteration 94/1000 | Loss: 0.00002349
Iteration 95/1000 | Loss: 0.00001450
Iteration 96/1000 | Loss: 0.00002311
Iteration 97/1000 | Loss: 0.00001644
Iteration 98/1000 | Loss: 0.00001483
Iteration 99/1000 | Loss: 0.00001419
Iteration 100/1000 | Loss: 0.00001396
Iteration 101/1000 | Loss: 0.00001396
Iteration 102/1000 | Loss: 0.00001396
Iteration 103/1000 | Loss: 0.00001396
Iteration 104/1000 | Loss: 0.00001396
Iteration 105/1000 | Loss: 0.00001396
Iteration 106/1000 | Loss: 0.00001396
Iteration 107/1000 | Loss: 0.00001396
Iteration 108/1000 | Loss: 0.00001411
Iteration 109/1000 | Loss: 0.00001396
Iteration 110/1000 | Loss: 0.00001396
Iteration 111/1000 | Loss: 0.00001396
Iteration 112/1000 | Loss: 0.00001396
Iteration 113/1000 | Loss: 0.00001396
Iteration 114/1000 | Loss: 0.00001396
Iteration 115/1000 | Loss: 0.00001396
Iteration 116/1000 | Loss: 0.00001396
Iteration 117/1000 | Loss: 0.00001396
Iteration 118/1000 | Loss: 0.00001396
Iteration 119/1000 | Loss: 0.00001396
Iteration 120/1000 | Loss: 0.00001396
Iteration 121/1000 | Loss: 0.00001396
Iteration 122/1000 | Loss: 0.00001396
Iteration 123/1000 | Loss: 0.00001396
Iteration 124/1000 | Loss: 0.00001396
Iteration 125/1000 | Loss: 0.00001396
Iteration 126/1000 | Loss: 0.00001396
Iteration 127/1000 | Loss: 0.00001396
Iteration 128/1000 | Loss: 0.00001396
Iteration 129/1000 | Loss: 0.00001396
Iteration 130/1000 | Loss: 0.00001396
Iteration 131/1000 | Loss: 0.00001396
Iteration 132/1000 | Loss: 0.00001396
Iteration 133/1000 | Loss: 0.00001396
Iteration 134/1000 | Loss: 0.00001396
Iteration 135/1000 | Loss: 0.00001396
Iteration 136/1000 | Loss: 0.00001396
Iteration 137/1000 | Loss: 0.00001396
Iteration 138/1000 | Loss: 0.00001396
Iteration 139/1000 | Loss: 0.00001396
Iteration 140/1000 | Loss: 0.00001396
Iteration 141/1000 | Loss: 0.00001396
Iteration 142/1000 | Loss: 0.00001396
Iteration 143/1000 | Loss: 0.00001396
Iteration 144/1000 | Loss: 0.00001396
Iteration 145/1000 | Loss: 0.00001396
Iteration 146/1000 | Loss: 0.00001396
Iteration 147/1000 | Loss: 0.00001396
Iteration 148/1000 | Loss: 0.00001396
Iteration 149/1000 | Loss: 0.00001396
Iteration 150/1000 | Loss: 0.00001396
Iteration 151/1000 | Loss: 0.00001396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.3956257134850603e-05, 1.3956257134850603e-05, 1.3956257134850603e-05, 1.3956257134850603e-05, 1.3956257134850603e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3956257134850603e-05

Optimization complete. Final v2v error: 2.874113082885742 mm

Highest mean error: 10.530521392822266 mm for frame 74

Lowest mean error: 2.3838729858398438 mm for frame 101

Saving results

Total time: 160.68319964408875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435112
Iteration 2/25 | Loss: 0.00087965
Iteration 3/25 | Loss: 0.00077135
Iteration 4/25 | Loss: 0.00074651
Iteration 5/25 | Loss: 0.00073687
Iteration 6/25 | Loss: 0.00073495
Iteration 7/25 | Loss: 0.00073452
Iteration 8/25 | Loss: 0.00073452
Iteration 9/25 | Loss: 0.00073452
Iteration 10/25 | Loss: 0.00073452
Iteration 11/25 | Loss: 0.00073452
Iteration 12/25 | Loss: 0.00073452
Iteration 13/25 | Loss: 0.00073452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007345167105086148, 0.0007345167105086148, 0.0007345167105086148, 0.0007345167105086148, 0.0007345167105086148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007345167105086148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59124839
Iteration 2/25 | Loss: 0.00028031
Iteration 3/25 | Loss: 0.00028031
Iteration 4/25 | Loss: 0.00028031
Iteration 5/25 | Loss: 0.00028031
Iteration 6/25 | Loss: 0.00028031
Iteration 7/25 | Loss: 0.00028030
Iteration 8/25 | Loss: 0.00028030
Iteration 9/25 | Loss: 0.00028030
Iteration 10/25 | Loss: 0.00028030
Iteration 11/25 | Loss: 0.00028030
Iteration 12/25 | Loss: 0.00028030
Iteration 13/25 | Loss: 0.00028030
Iteration 14/25 | Loss: 0.00028030
Iteration 15/25 | Loss: 0.00028030
Iteration 16/25 | Loss: 0.00028030
Iteration 17/25 | Loss: 0.00028030
Iteration 18/25 | Loss: 0.00028030
Iteration 19/25 | Loss: 0.00028030
Iteration 20/25 | Loss: 0.00028030
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002803035022225231, 0.0002803035022225231, 0.0002803035022225231, 0.0002803035022225231, 0.0002803035022225231]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002803035022225231

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028030
Iteration 2/1000 | Loss: 0.00003316
Iteration 3/1000 | Loss: 0.00002160
Iteration 4/1000 | Loss: 0.00001907
Iteration 5/1000 | Loss: 0.00001785
Iteration 6/1000 | Loss: 0.00001698
Iteration 7/1000 | Loss: 0.00001650
Iteration 8/1000 | Loss: 0.00001617
Iteration 9/1000 | Loss: 0.00001611
Iteration 10/1000 | Loss: 0.00001608
Iteration 11/1000 | Loss: 0.00001601
Iteration 12/1000 | Loss: 0.00001601
Iteration 13/1000 | Loss: 0.00001600
Iteration 14/1000 | Loss: 0.00001598
Iteration 15/1000 | Loss: 0.00001595
Iteration 16/1000 | Loss: 0.00001594
Iteration 17/1000 | Loss: 0.00001594
Iteration 18/1000 | Loss: 0.00001592
Iteration 19/1000 | Loss: 0.00001591
Iteration 20/1000 | Loss: 0.00001590
Iteration 21/1000 | Loss: 0.00001589
Iteration 22/1000 | Loss: 0.00001588
Iteration 23/1000 | Loss: 0.00001588
Iteration 24/1000 | Loss: 0.00001588
Iteration 25/1000 | Loss: 0.00001586
Iteration 26/1000 | Loss: 0.00001585
Iteration 27/1000 | Loss: 0.00001584
Iteration 28/1000 | Loss: 0.00001584
Iteration 29/1000 | Loss: 0.00001583
Iteration 30/1000 | Loss: 0.00001583
Iteration 31/1000 | Loss: 0.00001583
Iteration 32/1000 | Loss: 0.00001583
Iteration 33/1000 | Loss: 0.00001582
Iteration 34/1000 | Loss: 0.00001582
Iteration 35/1000 | Loss: 0.00001582
Iteration 36/1000 | Loss: 0.00001582
Iteration 37/1000 | Loss: 0.00001582
Iteration 38/1000 | Loss: 0.00001582
Iteration 39/1000 | Loss: 0.00001582
Iteration 40/1000 | Loss: 0.00001581
Iteration 41/1000 | Loss: 0.00001581
Iteration 42/1000 | Loss: 0.00001581
Iteration 43/1000 | Loss: 0.00001580
Iteration 44/1000 | Loss: 0.00001580
Iteration 45/1000 | Loss: 0.00001580
Iteration 46/1000 | Loss: 0.00001580
Iteration 47/1000 | Loss: 0.00001580
Iteration 48/1000 | Loss: 0.00001580
Iteration 49/1000 | Loss: 0.00001580
Iteration 50/1000 | Loss: 0.00001579
Iteration 51/1000 | Loss: 0.00001579
Iteration 52/1000 | Loss: 0.00001579
Iteration 53/1000 | Loss: 0.00001579
Iteration 54/1000 | Loss: 0.00001579
Iteration 55/1000 | Loss: 0.00001579
Iteration 56/1000 | Loss: 0.00001579
Iteration 57/1000 | Loss: 0.00001579
Iteration 58/1000 | Loss: 0.00001579
Iteration 59/1000 | Loss: 0.00001579
Iteration 60/1000 | Loss: 0.00001579
Iteration 61/1000 | Loss: 0.00001579
Iteration 62/1000 | Loss: 0.00001579
Iteration 63/1000 | Loss: 0.00001578
Iteration 64/1000 | Loss: 0.00001578
Iteration 65/1000 | Loss: 0.00001578
Iteration 66/1000 | Loss: 0.00001578
Iteration 67/1000 | Loss: 0.00001578
Iteration 68/1000 | Loss: 0.00001578
Iteration 69/1000 | Loss: 0.00001577
Iteration 70/1000 | Loss: 0.00001577
Iteration 71/1000 | Loss: 0.00001577
Iteration 72/1000 | Loss: 0.00001577
Iteration 73/1000 | Loss: 0.00001577
Iteration 74/1000 | Loss: 0.00001577
Iteration 75/1000 | Loss: 0.00001577
Iteration 76/1000 | Loss: 0.00001577
Iteration 77/1000 | Loss: 0.00001577
Iteration 78/1000 | Loss: 0.00001577
Iteration 79/1000 | Loss: 0.00001577
Iteration 80/1000 | Loss: 0.00001577
Iteration 81/1000 | Loss: 0.00001577
Iteration 82/1000 | Loss: 0.00001577
Iteration 83/1000 | Loss: 0.00001577
Iteration 84/1000 | Loss: 0.00001577
Iteration 85/1000 | Loss: 0.00001577
Iteration 86/1000 | Loss: 0.00001576
Iteration 87/1000 | Loss: 0.00001576
Iteration 88/1000 | Loss: 0.00001576
Iteration 89/1000 | Loss: 0.00001576
Iteration 90/1000 | Loss: 0.00001576
Iteration 91/1000 | Loss: 0.00001576
Iteration 92/1000 | Loss: 0.00001576
Iteration 93/1000 | Loss: 0.00001576
Iteration 94/1000 | Loss: 0.00001576
Iteration 95/1000 | Loss: 0.00001576
Iteration 96/1000 | Loss: 0.00001576
Iteration 97/1000 | Loss: 0.00001576
Iteration 98/1000 | Loss: 0.00001576
Iteration 99/1000 | Loss: 0.00001576
Iteration 100/1000 | Loss: 0.00001576
Iteration 101/1000 | Loss: 0.00001576
Iteration 102/1000 | Loss: 0.00001576
Iteration 103/1000 | Loss: 0.00001576
Iteration 104/1000 | Loss: 0.00001576
Iteration 105/1000 | Loss: 0.00001576
Iteration 106/1000 | Loss: 0.00001576
Iteration 107/1000 | Loss: 0.00001576
Iteration 108/1000 | Loss: 0.00001576
Iteration 109/1000 | Loss: 0.00001576
Iteration 110/1000 | Loss: 0.00001576
Iteration 111/1000 | Loss: 0.00001576
Iteration 112/1000 | Loss: 0.00001576
Iteration 113/1000 | Loss: 0.00001576
Iteration 114/1000 | Loss: 0.00001576
Iteration 115/1000 | Loss: 0.00001576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.5758239896968007e-05, 1.5758239896968007e-05, 1.5758239896968007e-05, 1.5758239896968007e-05, 1.5758239896968007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5758239896968007e-05

Optimization complete. Final v2v error: 3.358612298965454 mm

Highest mean error: 3.8055801391601562 mm for frame 153

Lowest mean error: 2.999295711517334 mm for frame 108

Saving results

Total time: 30.287185192108154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438926
Iteration 2/25 | Loss: 0.00111654
Iteration 3/25 | Loss: 0.00081815
Iteration 4/25 | Loss: 0.00077977
Iteration 5/25 | Loss: 0.00077039
Iteration 6/25 | Loss: 0.00076783
Iteration 7/25 | Loss: 0.00076713
Iteration 8/25 | Loss: 0.00076688
Iteration 9/25 | Loss: 0.00076688
Iteration 10/25 | Loss: 0.00076688
Iteration 11/25 | Loss: 0.00076688
Iteration 12/25 | Loss: 0.00076688
Iteration 13/25 | Loss: 0.00076688
Iteration 14/25 | Loss: 0.00076688
Iteration 15/25 | Loss: 0.00076688
Iteration 16/25 | Loss: 0.00076688
Iteration 17/25 | Loss: 0.00076688
Iteration 18/25 | Loss: 0.00076688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007668823236599565, 0.0007668823236599565, 0.0007668823236599565, 0.0007668823236599565, 0.0007668823236599565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007668823236599565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23032451
Iteration 2/25 | Loss: 0.00031370
Iteration 3/25 | Loss: 0.00031369
Iteration 4/25 | Loss: 0.00031369
Iteration 5/25 | Loss: 0.00031369
Iteration 6/25 | Loss: 0.00031369
Iteration 7/25 | Loss: 0.00031369
Iteration 8/25 | Loss: 0.00031369
Iteration 9/25 | Loss: 0.00031369
Iteration 10/25 | Loss: 0.00031369
Iteration 11/25 | Loss: 0.00031369
Iteration 12/25 | Loss: 0.00031369
Iteration 13/25 | Loss: 0.00031369
Iteration 14/25 | Loss: 0.00031369
Iteration 15/25 | Loss: 0.00031369
Iteration 16/25 | Loss: 0.00031369
Iteration 17/25 | Loss: 0.00031369
Iteration 18/25 | Loss: 0.00031369
Iteration 19/25 | Loss: 0.00031369
Iteration 20/25 | Loss: 0.00031369
Iteration 21/25 | Loss: 0.00031369
Iteration 22/25 | Loss: 0.00031369
Iteration 23/25 | Loss: 0.00031369
Iteration 24/25 | Loss: 0.00031369
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003136926388833672, 0.0003136926388833672, 0.0003136926388833672, 0.0003136926388833672, 0.0003136926388833672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003136926388833672

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031369
Iteration 2/1000 | Loss: 0.00002842
Iteration 3/1000 | Loss: 0.00001959
Iteration 4/1000 | Loss: 0.00001705
Iteration 5/1000 | Loss: 0.00001624
Iteration 6/1000 | Loss: 0.00001574
Iteration 7/1000 | Loss: 0.00001536
Iteration 8/1000 | Loss: 0.00001530
Iteration 9/1000 | Loss: 0.00001523
Iteration 10/1000 | Loss: 0.00001522
Iteration 11/1000 | Loss: 0.00001522
Iteration 12/1000 | Loss: 0.00001519
Iteration 13/1000 | Loss: 0.00001515
Iteration 14/1000 | Loss: 0.00001514
Iteration 15/1000 | Loss: 0.00001514
Iteration 16/1000 | Loss: 0.00001514
Iteration 17/1000 | Loss: 0.00001513
Iteration 18/1000 | Loss: 0.00001513
Iteration 19/1000 | Loss: 0.00001512
Iteration 20/1000 | Loss: 0.00001509
Iteration 21/1000 | Loss: 0.00001504
Iteration 22/1000 | Loss: 0.00001504
Iteration 23/1000 | Loss: 0.00001504
Iteration 24/1000 | Loss: 0.00001503
Iteration 25/1000 | Loss: 0.00001503
Iteration 26/1000 | Loss: 0.00001501
Iteration 27/1000 | Loss: 0.00001501
Iteration 28/1000 | Loss: 0.00001500
Iteration 29/1000 | Loss: 0.00001500
Iteration 30/1000 | Loss: 0.00001500
Iteration 31/1000 | Loss: 0.00001500
Iteration 32/1000 | Loss: 0.00001500
Iteration 33/1000 | Loss: 0.00001500
Iteration 34/1000 | Loss: 0.00001500
Iteration 35/1000 | Loss: 0.00001500
Iteration 36/1000 | Loss: 0.00001500
Iteration 37/1000 | Loss: 0.00001499
Iteration 38/1000 | Loss: 0.00001498
Iteration 39/1000 | Loss: 0.00001497
Iteration 40/1000 | Loss: 0.00001497
Iteration 41/1000 | Loss: 0.00001496
Iteration 42/1000 | Loss: 0.00001496
Iteration 43/1000 | Loss: 0.00001494
Iteration 44/1000 | Loss: 0.00001494
Iteration 45/1000 | Loss: 0.00001494
Iteration 46/1000 | Loss: 0.00001493
Iteration 47/1000 | Loss: 0.00001493
Iteration 48/1000 | Loss: 0.00001492
Iteration 49/1000 | Loss: 0.00001492
Iteration 50/1000 | Loss: 0.00001489
Iteration 51/1000 | Loss: 0.00001488
Iteration 52/1000 | Loss: 0.00001488
Iteration 53/1000 | Loss: 0.00001488
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001488
Iteration 56/1000 | Loss: 0.00001488
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001488
Iteration 59/1000 | Loss: 0.00001488
Iteration 60/1000 | Loss: 0.00001488
Iteration 61/1000 | Loss: 0.00001488
Iteration 62/1000 | Loss: 0.00001488
Iteration 63/1000 | Loss: 0.00001488
Iteration 64/1000 | Loss: 0.00001487
Iteration 65/1000 | Loss: 0.00001487
Iteration 66/1000 | Loss: 0.00001487
Iteration 67/1000 | Loss: 0.00001487
Iteration 68/1000 | Loss: 0.00001487
Iteration 69/1000 | Loss: 0.00001487
Iteration 70/1000 | Loss: 0.00001487
Iteration 71/1000 | Loss: 0.00001487
Iteration 72/1000 | Loss: 0.00001487
Iteration 73/1000 | Loss: 0.00001487
Iteration 74/1000 | Loss: 0.00001487
Iteration 75/1000 | Loss: 0.00001487
Iteration 76/1000 | Loss: 0.00001487
Iteration 77/1000 | Loss: 0.00001487
Iteration 78/1000 | Loss: 0.00001487
Iteration 79/1000 | Loss: 0.00001487
Iteration 80/1000 | Loss: 0.00001487
Iteration 81/1000 | Loss: 0.00001487
Iteration 82/1000 | Loss: 0.00001487
Iteration 83/1000 | Loss: 0.00001487
Iteration 84/1000 | Loss: 0.00001487
Iteration 85/1000 | Loss: 0.00001487
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001487
Iteration 88/1000 | Loss: 0.00001487
Iteration 89/1000 | Loss: 0.00001487
Iteration 90/1000 | Loss: 0.00001487
Iteration 91/1000 | Loss: 0.00001487
Iteration 92/1000 | Loss: 0.00001487
Iteration 93/1000 | Loss: 0.00001487
Iteration 94/1000 | Loss: 0.00001487
Iteration 95/1000 | Loss: 0.00001487
Iteration 96/1000 | Loss: 0.00001487
Iteration 97/1000 | Loss: 0.00001487
Iteration 98/1000 | Loss: 0.00001487
Iteration 99/1000 | Loss: 0.00001487
Iteration 100/1000 | Loss: 0.00001487
Iteration 101/1000 | Loss: 0.00001487
Iteration 102/1000 | Loss: 0.00001487
Iteration 103/1000 | Loss: 0.00001487
Iteration 104/1000 | Loss: 0.00001487
Iteration 105/1000 | Loss: 0.00001487
Iteration 106/1000 | Loss: 0.00001487
Iteration 107/1000 | Loss: 0.00001487
Iteration 108/1000 | Loss: 0.00001487
Iteration 109/1000 | Loss: 0.00001487
Iteration 110/1000 | Loss: 0.00001487
Iteration 111/1000 | Loss: 0.00001487
Iteration 112/1000 | Loss: 0.00001487
Iteration 113/1000 | Loss: 0.00001487
Iteration 114/1000 | Loss: 0.00001487
Iteration 115/1000 | Loss: 0.00001487
Iteration 116/1000 | Loss: 0.00001487
Iteration 117/1000 | Loss: 0.00001487
Iteration 118/1000 | Loss: 0.00001487
Iteration 119/1000 | Loss: 0.00001487
Iteration 120/1000 | Loss: 0.00001487
Iteration 121/1000 | Loss: 0.00001487
Iteration 122/1000 | Loss: 0.00001487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.4865108823869377e-05, 1.4865108823869377e-05, 1.4865108823869377e-05, 1.4865108823869377e-05, 1.4865108823869377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4865108823869377e-05

Optimization complete. Final v2v error: 3.2782199382781982 mm

Highest mean error: 3.3738033771514893 mm for frame 8

Lowest mean error: 3.063011884689331 mm for frame 88

Saving results

Total time: 29.326215505599976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874182
Iteration 2/25 | Loss: 0.00111961
Iteration 3/25 | Loss: 0.00081849
Iteration 4/25 | Loss: 0.00079570
Iteration 5/25 | Loss: 0.00078774
Iteration 6/25 | Loss: 0.00078605
Iteration 7/25 | Loss: 0.00078593
Iteration 8/25 | Loss: 0.00078593
Iteration 9/25 | Loss: 0.00078593
Iteration 10/25 | Loss: 0.00078593
Iteration 11/25 | Loss: 0.00078593
Iteration 12/25 | Loss: 0.00078593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000785928568802774, 0.000785928568802774, 0.000785928568802774, 0.000785928568802774, 0.000785928568802774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000785928568802774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.87180090
Iteration 2/25 | Loss: 0.00033882
Iteration 3/25 | Loss: 0.00033879
Iteration 4/25 | Loss: 0.00033879
Iteration 5/25 | Loss: 0.00033879
Iteration 6/25 | Loss: 0.00033879
Iteration 7/25 | Loss: 0.00033879
Iteration 8/25 | Loss: 0.00033879
Iteration 9/25 | Loss: 0.00033879
Iteration 10/25 | Loss: 0.00033879
Iteration 11/25 | Loss: 0.00033879
Iteration 12/25 | Loss: 0.00033879
Iteration 13/25 | Loss: 0.00033879
Iteration 14/25 | Loss: 0.00033879
Iteration 15/25 | Loss: 0.00033879
Iteration 16/25 | Loss: 0.00033879
Iteration 17/25 | Loss: 0.00033879
Iteration 18/25 | Loss: 0.00033879
Iteration 19/25 | Loss: 0.00033879
Iteration 20/25 | Loss: 0.00033879
Iteration 21/25 | Loss: 0.00033879
Iteration 22/25 | Loss: 0.00033879
Iteration 23/25 | Loss: 0.00033879
Iteration 24/25 | Loss: 0.00033879
Iteration 25/25 | Loss: 0.00033879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033879
Iteration 2/1000 | Loss: 0.00003477
Iteration 3/1000 | Loss: 0.00002982
Iteration 4/1000 | Loss: 0.00002737
Iteration 5/1000 | Loss: 0.00002619
Iteration 6/1000 | Loss: 0.00002527
Iteration 7/1000 | Loss: 0.00002463
Iteration 8/1000 | Loss: 0.00002410
Iteration 9/1000 | Loss: 0.00002376
Iteration 10/1000 | Loss: 0.00002357
Iteration 11/1000 | Loss: 0.00002349
Iteration 12/1000 | Loss: 0.00002339
Iteration 13/1000 | Loss: 0.00002337
Iteration 14/1000 | Loss: 0.00002336
Iteration 15/1000 | Loss: 0.00002336
Iteration 16/1000 | Loss: 0.00002336
Iteration 17/1000 | Loss: 0.00002335
Iteration 18/1000 | Loss: 0.00002335
Iteration 19/1000 | Loss: 0.00002335
Iteration 20/1000 | Loss: 0.00002335
Iteration 21/1000 | Loss: 0.00002335
Iteration 22/1000 | Loss: 0.00002335
Iteration 23/1000 | Loss: 0.00002335
Iteration 24/1000 | Loss: 0.00002335
Iteration 25/1000 | Loss: 0.00002334
Iteration 26/1000 | Loss: 0.00002333
Iteration 27/1000 | Loss: 0.00002333
Iteration 28/1000 | Loss: 0.00002333
Iteration 29/1000 | Loss: 0.00002333
Iteration 30/1000 | Loss: 0.00002332
Iteration 31/1000 | Loss: 0.00002332
Iteration 32/1000 | Loss: 0.00002332
Iteration 33/1000 | Loss: 0.00002332
Iteration 34/1000 | Loss: 0.00002332
Iteration 35/1000 | Loss: 0.00002332
Iteration 36/1000 | Loss: 0.00002332
Iteration 37/1000 | Loss: 0.00002332
Iteration 38/1000 | Loss: 0.00002332
Iteration 39/1000 | Loss: 0.00002332
Iteration 40/1000 | Loss: 0.00002332
Iteration 41/1000 | Loss: 0.00002331
Iteration 42/1000 | Loss: 0.00002330
Iteration 43/1000 | Loss: 0.00002330
Iteration 44/1000 | Loss: 0.00002330
Iteration 45/1000 | Loss: 0.00002330
Iteration 46/1000 | Loss: 0.00002330
Iteration 47/1000 | Loss: 0.00002330
Iteration 48/1000 | Loss: 0.00002330
Iteration 49/1000 | Loss: 0.00002330
Iteration 50/1000 | Loss: 0.00002330
Iteration 51/1000 | Loss: 0.00002330
Iteration 52/1000 | Loss: 0.00002330
Iteration 53/1000 | Loss: 0.00002330
Iteration 54/1000 | Loss: 0.00002330
Iteration 55/1000 | Loss: 0.00002330
Iteration 56/1000 | Loss: 0.00002330
Iteration 57/1000 | Loss: 0.00002330
Iteration 58/1000 | Loss: 0.00002330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [2.3297665393329225e-05, 2.3297665393329225e-05, 2.3297665393329225e-05, 2.3297665393329225e-05, 2.3297665393329225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3297665393329225e-05

Optimization complete. Final v2v error: 4.125180721282959 mm

Highest mean error: 4.87568473815918 mm for frame 233

Lowest mean error: 3.5835936069488525 mm for frame 132

Saving results

Total time: 32.436296701431274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852846
Iteration 2/25 | Loss: 0.00119644
Iteration 3/25 | Loss: 0.00082261
Iteration 4/25 | Loss: 0.00075397
Iteration 5/25 | Loss: 0.00073812
Iteration 6/25 | Loss: 0.00073373
Iteration 7/25 | Loss: 0.00073197
Iteration 8/25 | Loss: 0.00073106
Iteration 9/25 | Loss: 0.00073078
Iteration 10/25 | Loss: 0.00073116
Iteration 11/25 | Loss: 0.00073083
Iteration 12/25 | Loss: 0.00073028
Iteration 13/25 | Loss: 0.00072996
Iteration 14/25 | Loss: 0.00073055
Iteration 15/25 | Loss: 0.00072988
Iteration 16/25 | Loss: 0.00073041
Iteration 17/25 | Loss: 0.00073015
Iteration 18/25 | Loss: 0.00073047
Iteration 19/25 | Loss: 0.00073046
Iteration 20/25 | Loss: 0.00072993
Iteration 21/25 | Loss: 0.00072987
Iteration 22/25 | Loss: 0.00072987
Iteration 23/25 | Loss: 0.00072987
Iteration 24/25 | Loss: 0.00072986
Iteration 25/25 | Loss: 0.00072986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.22545767
Iteration 2/25 | Loss: 0.00031764
Iteration 3/25 | Loss: 0.00031761
Iteration 4/25 | Loss: 0.00031761
Iteration 5/25 | Loss: 0.00031761
Iteration 6/25 | Loss: 0.00031761
Iteration 7/25 | Loss: 0.00031761
Iteration 8/25 | Loss: 0.00031761
Iteration 9/25 | Loss: 0.00031761
Iteration 10/25 | Loss: 0.00031761
Iteration 11/25 | Loss: 0.00031761
Iteration 12/25 | Loss: 0.00031761
Iteration 13/25 | Loss: 0.00031761
Iteration 14/25 | Loss: 0.00031761
Iteration 15/25 | Loss: 0.00031761
Iteration 16/25 | Loss: 0.00031761
Iteration 17/25 | Loss: 0.00031761
Iteration 18/25 | Loss: 0.00031761
Iteration 19/25 | Loss: 0.00031761
Iteration 20/25 | Loss: 0.00031761
Iteration 21/25 | Loss: 0.00031761
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00031760765705257654, 0.00031760765705257654, 0.00031760765705257654, 0.00031760765705257654, 0.00031760765705257654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031760765705257654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031761
Iteration 2/1000 | Loss: 0.00002951
Iteration 3/1000 | Loss: 0.00002372
Iteration 4/1000 | Loss: 0.00002150
Iteration 5/1000 | Loss: 0.00002055
Iteration 6/1000 | Loss: 0.00002000
Iteration 7/1000 | Loss: 0.00001965
Iteration 8/1000 | Loss: 0.00002067
Iteration 9/1000 | Loss: 0.00001954
Iteration 10/1000 | Loss: 0.00001909
Iteration 11/1000 | Loss: 0.00001907
Iteration 12/1000 | Loss: 0.00001905
Iteration 13/1000 | Loss: 0.00001903
Iteration 14/1000 | Loss: 0.00001903
Iteration 15/1000 | Loss: 0.00001902
Iteration 16/1000 | Loss: 0.00001898
Iteration 17/1000 | Loss: 0.00001897
Iteration 18/1000 | Loss: 0.00001897
Iteration 19/1000 | Loss: 0.00001896
Iteration 20/1000 | Loss: 0.00001896
Iteration 21/1000 | Loss: 0.00001895
Iteration 22/1000 | Loss: 0.00001894
Iteration 23/1000 | Loss: 0.00001892
Iteration 24/1000 | Loss: 0.00001942
Iteration 25/1000 | Loss: 0.00001895
Iteration 26/1000 | Loss: 0.00001885
Iteration 27/1000 | Loss: 0.00001884
Iteration 28/1000 | Loss: 0.00001884
Iteration 29/1000 | Loss: 0.00001914
Iteration 30/1000 | Loss: 0.00001901
Iteration 31/1000 | Loss: 0.00001884
Iteration 32/1000 | Loss: 0.00001884
Iteration 33/1000 | Loss: 0.00001884
Iteration 34/1000 | Loss: 0.00001884
Iteration 35/1000 | Loss: 0.00001883
Iteration 36/1000 | Loss: 0.00001883
Iteration 37/1000 | Loss: 0.00001883
Iteration 38/1000 | Loss: 0.00001883
Iteration 39/1000 | Loss: 0.00001883
Iteration 40/1000 | Loss: 0.00001883
Iteration 41/1000 | Loss: 0.00001883
Iteration 42/1000 | Loss: 0.00001883
Iteration 43/1000 | Loss: 0.00001883
Iteration 44/1000 | Loss: 0.00001883
Iteration 45/1000 | Loss: 0.00001882
Iteration 46/1000 | Loss: 0.00001882
Iteration 47/1000 | Loss: 0.00001881
Iteration 48/1000 | Loss: 0.00001881
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001923
Iteration 52/1000 | Loss: 0.00001900
Iteration 53/1000 | Loss: 0.00001883
Iteration 54/1000 | Loss: 0.00001881
Iteration 55/1000 | Loss: 0.00001881
Iteration 56/1000 | Loss: 0.00001880
Iteration 57/1000 | Loss: 0.00001879
Iteration 58/1000 | Loss: 0.00001879
Iteration 59/1000 | Loss: 0.00001916
Iteration 60/1000 | Loss: 0.00001902
Iteration 61/1000 | Loss: 0.00001879
Iteration 62/1000 | Loss: 0.00001917
Iteration 63/1000 | Loss: 0.00001908
Iteration 64/1000 | Loss: 0.00001918
Iteration 65/1000 | Loss: 0.00001909
Iteration 66/1000 | Loss: 0.00001913
Iteration 67/1000 | Loss: 0.00001912
Iteration 68/1000 | Loss: 0.00001917
Iteration 69/1000 | Loss: 0.00001906
Iteration 70/1000 | Loss: 0.00001910
Iteration 71/1000 | Loss: 0.00001910
Iteration 72/1000 | Loss: 0.00001911
Iteration 73/1000 | Loss: 0.00001904
Iteration 74/1000 | Loss: 0.00001914
Iteration 75/1000 | Loss: 0.00001916
Iteration 76/1000 | Loss: 0.00001920
Iteration 77/1000 | Loss: 0.00001916
Iteration 78/1000 | Loss: 0.00001922
Iteration 79/1000 | Loss: 0.00001912
Iteration 80/1000 | Loss: 0.00001917
Iteration 81/1000 | Loss: 0.00001918
Iteration 82/1000 | Loss: 0.00001919
Iteration 83/1000 | Loss: 0.00001918
Iteration 84/1000 | Loss: 0.00001920
Iteration 85/1000 | Loss: 0.00001916
Iteration 86/1000 | Loss: 0.00001919
Iteration 87/1000 | Loss: 0.00001917
Iteration 88/1000 | Loss: 0.00001912
Iteration 89/1000 | Loss: 0.00001914
Iteration 90/1000 | Loss: 0.00001921
Iteration 91/1000 | Loss: 0.00001915
Iteration 92/1000 | Loss: 0.00001921
Iteration 93/1000 | Loss: 0.00001912
Iteration 94/1000 | Loss: 0.00001919
Iteration 95/1000 | Loss: 0.00001923
Iteration 96/1000 | Loss: 0.00001924
Iteration 97/1000 | Loss: 0.00001894
Iteration 98/1000 | Loss: 0.00001871
Iteration 99/1000 | Loss: 0.00001871
Iteration 100/1000 | Loss: 0.00001871
Iteration 101/1000 | Loss: 0.00001871
Iteration 102/1000 | Loss: 0.00001871
Iteration 103/1000 | Loss: 0.00001871
Iteration 104/1000 | Loss: 0.00001906
Iteration 105/1000 | Loss: 0.00001911
Iteration 106/1000 | Loss: 0.00001910
Iteration 107/1000 | Loss: 0.00001913
Iteration 108/1000 | Loss: 0.00001901
Iteration 109/1000 | Loss: 0.00001901
Iteration 110/1000 | Loss: 0.00001915
Iteration 111/1000 | Loss: 0.00001915
Iteration 112/1000 | Loss: 0.00001901
Iteration 113/1000 | Loss: 0.00001902
Iteration 114/1000 | Loss: 0.00001895
Iteration 115/1000 | Loss: 0.00001899
Iteration 116/1000 | Loss: 0.00001875
Iteration 117/1000 | Loss: 0.00001901
Iteration 118/1000 | Loss: 0.00001892
Iteration 119/1000 | Loss: 0.00001909
Iteration 120/1000 | Loss: 0.00001913
Iteration 121/1000 | Loss: 0.00001907
Iteration 122/1000 | Loss: 0.00001915
Iteration 123/1000 | Loss: 0.00001902
Iteration 124/1000 | Loss: 0.00001889
Iteration 125/1000 | Loss: 0.00001871
Iteration 126/1000 | Loss: 0.00001871
Iteration 127/1000 | Loss: 0.00001871
Iteration 128/1000 | Loss: 0.00001871
Iteration 129/1000 | Loss: 0.00001871
Iteration 130/1000 | Loss: 0.00001871
Iteration 131/1000 | Loss: 0.00001871
Iteration 132/1000 | Loss: 0.00001870
Iteration 133/1000 | Loss: 0.00001870
Iteration 134/1000 | Loss: 0.00001870
Iteration 135/1000 | Loss: 0.00001870
Iteration 136/1000 | Loss: 0.00001870
Iteration 137/1000 | Loss: 0.00001870
Iteration 138/1000 | Loss: 0.00001870
Iteration 139/1000 | Loss: 0.00001870
Iteration 140/1000 | Loss: 0.00001870
Iteration 141/1000 | Loss: 0.00001870
Iteration 142/1000 | Loss: 0.00001870
Iteration 143/1000 | Loss: 0.00001870
Iteration 144/1000 | Loss: 0.00001870
Iteration 145/1000 | Loss: 0.00001870
Iteration 146/1000 | Loss: 0.00001870
Iteration 147/1000 | Loss: 0.00001870
Iteration 148/1000 | Loss: 0.00001870
Iteration 149/1000 | Loss: 0.00001870
Iteration 150/1000 | Loss: 0.00001870
Iteration 151/1000 | Loss: 0.00001870
Iteration 152/1000 | Loss: 0.00001870
Iteration 153/1000 | Loss: 0.00001870
Iteration 154/1000 | Loss: 0.00001870
Iteration 155/1000 | Loss: 0.00001870
Iteration 156/1000 | Loss: 0.00001870
Iteration 157/1000 | Loss: 0.00001870
Iteration 158/1000 | Loss: 0.00001870
Iteration 159/1000 | Loss: 0.00001870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.8696593542699702e-05, 1.8696593542699702e-05, 1.8696593542699702e-05, 1.8696593542699702e-05, 1.8696593542699702e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8696593542699702e-05

Optimization complete. Final v2v error: 3.6703948974609375 mm

Highest mean error: 9.819289207458496 mm for frame 183

Lowest mean error: 3.2343525886535645 mm for frame 191

Saving results

Total time: 135.109384059906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01110770
Iteration 2/25 | Loss: 0.00253863
Iteration 3/25 | Loss: 0.00144036
Iteration 4/25 | Loss: 0.00129191
Iteration 5/25 | Loss: 0.00117765
Iteration 6/25 | Loss: 0.00114078
Iteration 7/25 | Loss: 0.00113242
Iteration 8/25 | Loss: 0.00107500
Iteration 9/25 | Loss: 0.00103979
Iteration 10/25 | Loss: 0.00099259
Iteration 11/25 | Loss: 0.00097546
Iteration 12/25 | Loss: 0.00096772
Iteration 13/25 | Loss: 0.00095744
Iteration 14/25 | Loss: 0.00094599
Iteration 15/25 | Loss: 0.00094513
Iteration 16/25 | Loss: 0.00093096
Iteration 17/25 | Loss: 0.00092357
Iteration 18/25 | Loss: 0.00091325
Iteration 19/25 | Loss: 0.00090861
Iteration 20/25 | Loss: 0.00091251
Iteration 21/25 | Loss: 0.00090890
Iteration 22/25 | Loss: 0.00090225
Iteration 23/25 | Loss: 0.00090051
Iteration 24/25 | Loss: 0.00089951
Iteration 25/25 | Loss: 0.00090641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39036822
Iteration 2/25 | Loss: 0.00249730
Iteration 3/25 | Loss: 0.00249729
Iteration 4/25 | Loss: 0.00249729
Iteration 5/25 | Loss: 0.00201067
Iteration 6/25 | Loss: 0.00201034
Iteration 7/25 | Loss: 0.00201034
Iteration 8/25 | Loss: 0.00201034
Iteration 9/25 | Loss: 0.00201034
Iteration 10/25 | Loss: 0.00201034
Iteration 11/25 | Loss: 0.00201034
Iteration 12/25 | Loss: 0.00201034
Iteration 13/25 | Loss: 0.00201034
Iteration 14/25 | Loss: 0.00201034
Iteration 15/25 | Loss: 0.00201034
Iteration 16/25 | Loss: 0.00201034
Iteration 17/25 | Loss: 0.00201034
Iteration 18/25 | Loss: 0.00201034
Iteration 19/25 | Loss: 0.00201034
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020103384740650654, 0.0020103384740650654, 0.0020103384740650654, 0.0020103384740650654, 0.0020103384740650654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020103384740650654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201034
Iteration 2/1000 | Loss: 0.00879053
Iteration 3/1000 | Loss: 0.00198584
Iteration 4/1000 | Loss: 0.00076314
Iteration 5/1000 | Loss: 0.00050881
Iteration 6/1000 | Loss: 0.00067106
Iteration 7/1000 | Loss: 0.00052022
Iteration 8/1000 | Loss: 0.00015952
Iteration 9/1000 | Loss: 0.00065614
Iteration 10/1000 | Loss: 0.00024268
Iteration 11/1000 | Loss: 0.00094132
Iteration 12/1000 | Loss: 0.00010346
Iteration 13/1000 | Loss: 0.00010950
Iteration 14/1000 | Loss: 0.00013747
Iteration 15/1000 | Loss: 0.00011258
Iteration 16/1000 | Loss: 0.00011192
Iteration 17/1000 | Loss: 0.00094770
Iteration 18/1000 | Loss: 0.00054174
Iteration 19/1000 | Loss: 0.00117565
Iteration 20/1000 | Loss: 0.00092229
Iteration 21/1000 | Loss: 0.00072598
Iteration 22/1000 | Loss: 0.00108851
Iteration 23/1000 | Loss: 0.00116244
Iteration 24/1000 | Loss: 0.00647492
Iteration 25/1000 | Loss: 0.00364184
Iteration 26/1000 | Loss: 0.00102703
Iteration 27/1000 | Loss: 0.00045589
Iteration 28/1000 | Loss: 0.00030400
Iteration 29/1000 | Loss: 0.00012601
Iteration 30/1000 | Loss: 0.00075479
Iteration 31/1000 | Loss: 0.00097307
Iteration 32/1000 | Loss: 0.00053219
Iteration 33/1000 | Loss: 0.00074307
Iteration 34/1000 | Loss: 0.00060699
Iteration 35/1000 | Loss: 0.00066632
Iteration 36/1000 | Loss: 0.00088183
Iteration 37/1000 | Loss: 0.00439849
Iteration 38/1000 | Loss: 0.00337469
Iteration 39/1000 | Loss: 0.00405136
Iteration 40/1000 | Loss: 0.00170675
Iteration 41/1000 | Loss: 0.00437662
Iteration 42/1000 | Loss: 0.00190071
Iteration 43/1000 | Loss: 0.00040300
Iteration 44/1000 | Loss: 0.00212108
Iteration 45/1000 | Loss: 0.00034161
Iteration 46/1000 | Loss: 0.00075133
Iteration 47/1000 | Loss: 0.00028646
Iteration 48/1000 | Loss: 0.00033577
Iteration 49/1000 | Loss: 0.00064188
Iteration 50/1000 | Loss: 0.00075158
Iteration 51/1000 | Loss: 0.00068033
Iteration 52/1000 | Loss: 0.00035303
Iteration 53/1000 | Loss: 0.00065014
Iteration 54/1000 | Loss: 0.00064783
Iteration 55/1000 | Loss: 0.00039737
Iteration 56/1000 | Loss: 0.00005662
Iteration 57/1000 | Loss: 0.00053476
Iteration 58/1000 | Loss: 0.00010990
Iteration 59/1000 | Loss: 0.00004801
Iteration 60/1000 | Loss: 0.00088836
Iteration 61/1000 | Loss: 0.00033918
Iteration 62/1000 | Loss: 0.00052366
Iteration 63/1000 | Loss: 0.00040578
Iteration 64/1000 | Loss: 0.00079225
Iteration 65/1000 | Loss: 0.00074461
Iteration 66/1000 | Loss: 0.00030793
Iteration 67/1000 | Loss: 0.00008869
Iteration 68/1000 | Loss: 0.00056523
Iteration 69/1000 | Loss: 0.00030529
Iteration 70/1000 | Loss: 0.00029211
Iteration 71/1000 | Loss: 0.00022033
Iteration 72/1000 | Loss: 0.00014472
Iteration 73/1000 | Loss: 0.00023162
Iteration 74/1000 | Loss: 0.00019099
Iteration 75/1000 | Loss: 0.00011832
Iteration 76/1000 | Loss: 0.00040201
Iteration 77/1000 | Loss: 0.00045018
Iteration 78/1000 | Loss: 0.00031581
Iteration 79/1000 | Loss: 0.00005300
Iteration 80/1000 | Loss: 0.00070534
Iteration 81/1000 | Loss: 0.00194151
Iteration 82/1000 | Loss: 0.00028966
Iteration 83/1000 | Loss: 0.00034320
Iteration 84/1000 | Loss: 0.00004611
Iteration 85/1000 | Loss: 0.00004367
Iteration 86/1000 | Loss: 0.00041931
Iteration 87/1000 | Loss: 0.00058827
Iteration 88/1000 | Loss: 0.00178610
Iteration 89/1000 | Loss: 0.00015997
Iteration 90/1000 | Loss: 0.00004434
Iteration 91/1000 | Loss: 0.00039436
Iteration 92/1000 | Loss: 0.00021375
Iteration 93/1000 | Loss: 0.00030080
Iteration 94/1000 | Loss: 0.00044448
Iteration 95/1000 | Loss: 0.00023202
Iteration 96/1000 | Loss: 0.00153029
Iteration 97/1000 | Loss: 0.00047499
Iteration 98/1000 | Loss: 0.00050676
Iteration 99/1000 | Loss: 0.00047596
Iteration 100/1000 | Loss: 0.00004805
Iteration 101/1000 | Loss: 0.00003691
Iteration 102/1000 | Loss: 0.00024529
Iteration 103/1000 | Loss: 0.00003552
Iteration 104/1000 | Loss: 0.00003267
Iteration 105/1000 | Loss: 0.00003130
Iteration 106/1000 | Loss: 0.00003063
Iteration 107/1000 | Loss: 0.00002988
Iteration 108/1000 | Loss: 0.00027804
Iteration 109/1000 | Loss: 0.00053005
Iteration 110/1000 | Loss: 0.00105416
Iteration 111/1000 | Loss: 0.00041772
Iteration 112/1000 | Loss: 0.00020360
Iteration 113/1000 | Loss: 0.00004075
Iteration 114/1000 | Loss: 0.00003126
Iteration 115/1000 | Loss: 0.00002885
Iteration 116/1000 | Loss: 0.00040447
Iteration 117/1000 | Loss: 0.00025369
Iteration 118/1000 | Loss: 0.00033938
Iteration 119/1000 | Loss: 0.00002973
Iteration 120/1000 | Loss: 0.00002663
Iteration 121/1000 | Loss: 0.00002541
Iteration 122/1000 | Loss: 0.00002487
Iteration 123/1000 | Loss: 0.00002438
Iteration 124/1000 | Loss: 0.00002410
Iteration 125/1000 | Loss: 0.00002408
Iteration 126/1000 | Loss: 0.00002403
Iteration 127/1000 | Loss: 0.00002398
Iteration 128/1000 | Loss: 0.00002388
Iteration 129/1000 | Loss: 0.00002386
Iteration 130/1000 | Loss: 0.00002380
Iteration 131/1000 | Loss: 0.00002380
Iteration 132/1000 | Loss: 0.00002378
Iteration 133/1000 | Loss: 0.00002378
Iteration 134/1000 | Loss: 0.00002377
Iteration 135/1000 | Loss: 0.00002375
Iteration 136/1000 | Loss: 0.00002375
Iteration 137/1000 | Loss: 0.00002370
Iteration 138/1000 | Loss: 0.00002370
Iteration 139/1000 | Loss: 0.00002369
Iteration 140/1000 | Loss: 0.00002363
Iteration 141/1000 | Loss: 0.00002362
Iteration 142/1000 | Loss: 0.00002358
Iteration 143/1000 | Loss: 0.00002355
Iteration 144/1000 | Loss: 0.00002355
Iteration 145/1000 | Loss: 0.00002355
Iteration 146/1000 | Loss: 0.00002354
Iteration 147/1000 | Loss: 0.00002353
Iteration 148/1000 | Loss: 0.00002353
Iteration 149/1000 | Loss: 0.00002353
Iteration 150/1000 | Loss: 0.00002352
Iteration 151/1000 | Loss: 0.00002352
Iteration 152/1000 | Loss: 0.00002351
Iteration 153/1000 | Loss: 0.00002351
Iteration 154/1000 | Loss: 0.00002351
Iteration 155/1000 | Loss: 0.00002351
Iteration 156/1000 | Loss: 0.00002351
Iteration 157/1000 | Loss: 0.00002351
Iteration 158/1000 | Loss: 0.00002350
Iteration 159/1000 | Loss: 0.00002350
Iteration 160/1000 | Loss: 0.00002350
Iteration 161/1000 | Loss: 0.00002349
Iteration 162/1000 | Loss: 0.00002348
Iteration 163/1000 | Loss: 0.00002348
Iteration 164/1000 | Loss: 0.00002347
Iteration 165/1000 | Loss: 0.00002347
Iteration 166/1000 | Loss: 0.00002347
Iteration 167/1000 | Loss: 0.00002347
Iteration 168/1000 | Loss: 0.00002346
Iteration 169/1000 | Loss: 0.00002346
Iteration 170/1000 | Loss: 0.00002346
Iteration 171/1000 | Loss: 0.00002346
Iteration 172/1000 | Loss: 0.00002346
Iteration 173/1000 | Loss: 0.00002346
Iteration 174/1000 | Loss: 0.00002346
Iteration 175/1000 | Loss: 0.00002346
Iteration 176/1000 | Loss: 0.00002346
Iteration 177/1000 | Loss: 0.00002345
Iteration 178/1000 | Loss: 0.00002345
Iteration 179/1000 | Loss: 0.00002345
Iteration 180/1000 | Loss: 0.00002345
Iteration 181/1000 | Loss: 0.00002345
Iteration 182/1000 | Loss: 0.00002345
Iteration 183/1000 | Loss: 0.00002345
Iteration 184/1000 | Loss: 0.00002345
Iteration 185/1000 | Loss: 0.00002345
Iteration 186/1000 | Loss: 0.00002344
Iteration 187/1000 | Loss: 0.00002344
Iteration 188/1000 | Loss: 0.00002344
Iteration 189/1000 | Loss: 0.00002344
Iteration 190/1000 | Loss: 0.00002343
Iteration 191/1000 | Loss: 0.00002343
Iteration 192/1000 | Loss: 0.00002343
Iteration 193/1000 | Loss: 0.00002343
Iteration 194/1000 | Loss: 0.00002342
Iteration 195/1000 | Loss: 0.00002342
Iteration 196/1000 | Loss: 0.00002342
Iteration 197/1000 | Loss: 0.00002341
Iteration 198/1000 | Loss: 0.00002341
Iteration 199/1000 | Loss: 0.00002341
Iteration 200/1000 | Loss: 0.00002341
Iteration 201/1000 | Loss: 0.00002340
Iteration 202/1000 | Loss: 0.00002340
Iteration 203/1000 | Loss: 0.00002340
Iteration 204/1000 | Loss: 0.00002340
Iteration 205/1000 | Loss: 0.00002340
Iteration 206/1000 | Loss: 0.00002339
Iteration 207/1000 | Loss: 0.00002339
Iteration 208/1000 | Loss: 0.00002339
Iteration 209/1000 | Loss: 0.00002339
Iteration 210/1000 | Loss: 0.00002339
Iteration 211/1000 | Loss: 0.00002339
Iteration 212/1000 | Loss: 0.00002339
Iteration 213/1000 | Loss: 0.00002339
Iteration 214/1000 | Loss: 0.00002338
Iteration 215/1000 | Loss: 0.00002338
Iteration 216/1000 | Loss: 0.00002338
Iteration 217/1000 | Loss: 0.00002338
Iteration 218/1000 | Loss: 0.00002338
Iteration 219/1000 | Loss: 0.00002337
Iteration 220/1000 | Loss: 0.00002337
Iteration 221/1000 | Loss: 0.00002337
Iteration 222/1000 | Loss: 0.00002337
Iteration 223/1000 | Loss: 0.00002337
Iteration 224/1000 | Loss: 0.00002337
Iteration 225/1000 | Loss: 0.00002337
Iteration 226/1000 | Loss: 0.00002337
Iteration 227/1000 | Loss: 0.00002337
Iteration 228/1000 | Loss: 0.00002337
Iteration 229/1000 | Loss: 0.00002337
Iteration 230/1000 | Loss: 0.00002337
Iteration 231/1000 | Loss: 0.00002337
Iteration 232/1000 | Loss: 0.00002337
Iteration 233/1000 | Loss: 0.00002337
Iteration 234/1000 | Loss: 0.00002337
Iteration 235/1000 | Loss: 0.00002337
Iteration 236/1000 | Loss: 0.00002337
Iteration 237/1000 | Loss: 0.00002337
Iteration 238/1000 | Loss: 0.00002337
Iteration 239/1000 | Loss: 0.00002337
Iteration 240/1000 | Loss: 0.00002337
Iteration 241/1000 | Loss: 0.00002337
Iteration 242/1000 | Loss: 0.00002337
Iteration 243/1000 | Loss: 0.00002337
Iteration 244/1000 | Loss: 0.00002337
Iteration 245/1000 | Loss: 0.00002337
Iteration 246/1000 | Loss: 0.00002337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [2.336512625333853e-05, 2.336512625333853e-05, 2.336512625333853e-05, 2.336512625333853e-05, 2.336512625333853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.336512625333853e-05

Optimization complete. Final v2v error: 3.8664965629577637 mm

Highest mean error: 10.746552467346191 mm for frame 99

Lowest mean error: 3.0178356170654297 mm for frame 83

Saving results

Total time: 268.85467743873596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855845
Iteration 2/25 | Loss: 0.00176778
Iteration 3/25 | Loss: 0.00119091
Iteration 4/25 | Loss: 0.00100706
Iteration 5/25 | Loss: 0.00098435
Iteration 6/25 | Loss: 0.00102914
Iteration 7/25 | Loss: 0.00091434
Iteration 8/25 | Loss: 0.00089724
Iteration 9/25 | Loss: 0.00101865
Iteration 10/25 | Loss: 0.00089388
Iteration 11/25 | Loss: 0.00083763
Iteration 12/25 | Loss: 0.00082984
Iteration 13/25 | Loss: 0.00082813
Iteration 14/25 | Loss: 0.00082792
Iteration 15/25 | Loss: 0.00082788
Iteration 16/25 | Loss: 0.00082787
Iteration 17/25 | Loss: 0.00082787
Iteration 18/25 | Loss: 0.00082787
Iteration 19/25 | Loss: 0.00082787
Iteration 20/25 | Loss: 0.00082787
Iteration 21/25 | Loss: 0.00082787
Iteration 22/25 | Loss: 0.00082786
Iteration 23/25 | Loss: 0.00082786
Iteration 24/25 | Loss: 0.00082786
Iteration 25/25 | Loss: 0.00082786

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60179424
Iteration 2/25 | Loss: 0.00042012
Iteration 3/25 | Loss: 0.00042011
Iteration 4/25 | Loss: 0.00042011
Iteration 5/25 | Loss: 0.00042011
Iteration 6/25 | Loss: 0.00042011
Iteration 7/25 | Loss: 0.00042011
Iteration 8/25 | Loss: 0.00042011
Iteration 9/25 | Loss: 0.00042011
Iteration 10/25 | Loss: 0.00042011
Iteration 11/25 | Loss: 0.00042011
Iteration 12/25 | Loss: 0.00042011
Iteration 13/25 | Loss: 0.00042011
Iteration 14/25 | Loss: 0.00042011
Iteration 15/25 | Loss: 0.00042011
Iteration 16/25 | Loss: 0.00042011
Iteration 17/25 | Loss: 0.00042011
Iteration 18/25 | Loss: 0.00042011
Iteration 19/25 | Loss: 0.00042011
Iteration 20/25 | Loss: 0.00042011
Iteration 21/25 | Loss: 0.00042011
Iteration 22/25 | Loss: 0.00042011
Iteration 23/25 | Loss: 0.00042011
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00042011160985566676, 0.00042011160985566676, 0.00042011160985566676, 0.00042011160985566676, 0.00042011160985566676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00042011160985566676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042011
Iteration 2/1000 | Loss: 0.00004229
Iteration 3/1000 | Loss: 0.00003051
Iteration 4/1000 | Loss: 0.00002651
Iteration 5/1000 | Loss: 0.00002430
Iteration 6/1000 | Loss: 0.00002332
Iteration 7/1000 | Loss: 0.00002267
Iteration 8/1000 | Loss: 0.00002229
Iteration 9/1000 | Loss: 0.00002201
Iteration 10/1000 | Loss: 0.00002196
Iteration 11/1000 | Loss: 0.00002192
Iteration 12/1000 | Loss: 0.00002180
Iteration 13/1000 | Loss: 0.00002175
Iteration 14/1000 | Loss: 0.00002173
Iteration 15/1000 | Loss: 0.00002173
Iteration 16/1000 | Loss: 0.00002172
Iteration 17/1000 | Loss: 0.00002169
Iteration 18/1000 | Loss: 0.00002165
Iteration 19/1000 | Loss: 0.00002164
Iteration 20/1000 | Loss: 0.00002162
Iteration 21/1000 | Loss: 0.00002161
Iteration 22/1000 | Loss: 0.00002161
Iteration 23/1000 | Loss: 0.00002158
Iteration 24/1000 | Loss: 0.00002158
Iteration 25/1000 | Loss: 0.00002157
Iteration 26/1000 | Loss: 0.00002156
Iteration 27/1000 | Loss: 0.00002156
Iteration 28/1000 | Loss: 0.00002153
Iteration 29/1000 | Loss: 0.00002153
Iteration 30/1000 | Loss: 0.00002152
Iteration 31/1000 | Loss: 0.00002152
Iteration 32/1000 | Loss: 0.00002152
Iteration 33/1000 | Loss: 0.00002151
Iteration 34/1000 | Loss: 0.00002150
Iteration 35/1000 | Loss: 0.00002150
Iteration 36/1000 | Loss: 0.00002149
Iteration 37/1000 | Loss: 0.00002148
Iteration 38/1000 | Loss: 0.00002148
Iteration 39/1000 | Loss: 0.00002147
Iteration 40/1000 | Loss: 0.00002146
Iteration 41/1000 | Loss: 0.00002146
Iteration 42/1000 | Loss: 0.00002146
Iteration 43/1000 | Loss: 0.00002145
Iteration 44/1000 | Loss: 0.00002145
Iteration 45/1000 | Loss: 0.00002145
Iteration 46/1000 | Loss: 0.00002145
Iteration 47/1000 | Loss: 0.00002144
Iteration 48/1000 | Loss: 0.00002144
Iteration 49/1000 | Loss: 0.00002144
Iteration 50/1000 | Loss: 0.00002143
Iteration 51/1000 | Loss: 0.00002143
Iteration 52/1000 | Loss: 0.00002143
Iteration 53/1000 | Loss: 0.00002142
Iteration 54/1000 | Loss: 0.00002142
Iteration 55/1000 | Loss: 0.00002142
Iteration 56/1000 | Loss: 0.00002142
Iteration 57/1000 | Loss: 0.00002141
Iteration 58/1000 | Loss: 0.00002141
Iteration 59/1000 | Loss: 0.00002141
Iteration 60/1000 | Loss: 0.00002141
Iteration 61/1000 | Loss: 0.00002141
Iteration 62/1000 | Loss: 0.00002141
Iteration 63/1000 | Loss: 0.00002141
Iteration 64/1000 | Loss: 0.00002140
Iteration 65/1000 | Loss: 0.00002140
Iteration 66/1000 | Loss: 0.00002140
Iteration 67/1000 | Loss: 0.00002139
Iteration 68/1000 | Loss: 0.00002139
Iteration 69/1000 | Loss: 0.00002139
Iteration 70/1000 | Loss: 0.00002139
Iteration 71/1000 | Loss: 0.00002139
Iteration 72/1000 | Loss: 0.00002139
Iteration 73/1000 | Loss: 0.00002139
Iteration 74/1000 | Loss: 0.00002138
Iteration 75/1000 | Loss: 0.00002138
Iteration 76/1000 | Loss: 0.00002138
Iteration 77/1000 | Loss: 0.00002138
Iteration 78/1000 | Loss: 0.00002138
Iteration 79/1000 | Loss: 0.00002138
Iteration 80/1000 | Loss: 0.00002138
Iteration 81/1000 | Loss: 0.00002137
Iteration 82/1000 | Loss: 0.00002137
Iteration 83/1000 | Loss: 0.00002137
Iteration 84/1000 | Loss: 0.00002137
Iteration 85/1000 | Loss: 0.00002136
Iteration 86/1000 | Loss: 0.00002136
Iteration 87/1000 | Loss: 0.00002136
Iteration 88/1000 | Loss: 0.00002136
Iteration 89/1000 | Loss: 0.00002136
Iteration 90/1000 | Loss: 0.00002136
Iteration 91/1000 | Loss: 0.00002136
Iteration 92/1000 | Loss: 0.00002136
Iteration 93/1000 | Loss: 0.00002136
Iteration 94/1000 | Loss: 0.00002136
Iteration 95/1000 | Loss: 0.00002136
Iteration 96/1000 | Loss: 0.00002136
Iteration 97/1000 | Loss: 0.00002136
Iteration 98/1000 | Loss: 0.00002136
Iteration 99/1000 | Loss: 0.00002136
Iteration 100/1000 | Loss: 0.00002136
Iteration 101/1000 | Loss: 0.00002135
Iteration 102/1000 | Loss: 0.00002135
Iteration 103/1000 | Loss: 0.00002135
Iteration 104/1000 | Loss: 0.00002135
Iteration 105/1000 | Loss: 0.00002135
Iteration 106/1000 | Loss: 0.00002135
Iteration 107/1000 | Loss: 0.00002135
Iteration 108/1000 | Loss: 0.00002134
Iteration 109/1000 | Loss: 0.00002134
Iteration 110/1000 | Loss: 0.00002134
Iteration 111/1000 | Loss: 0.00002134
Iteration 112/1000 | Loss: 0.00002134
Iteration 113/1000 | Loss: 0.00002134
Iteration 114/1000 | Loss: 0.00002134
Iteration 115/1000 | Loss: 0.00002134
Iteration 116/1000 | Loss: 0.00002134
Iteration 117/1000 | Loss: 0.00002134
Iteration 118/1000 | Loss: 0.00002134
Iteration 119/1000 | Loss: 0.00002134
Iteration 120/1000 | Loss: 0.00002134
Iteration 121/1000 | Loss: 0.00002134
Iteration 122/1000 | Loss: 0.00002134
Iteration 123/1000 | Loss: 0.00002134
Iteration 124/1000 | Loss: 0.00002134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.1336123609216884e-05, 2.1336123609216884e-05, 2.1336123609216884e-05, 2.1336123609216884e-05, 2.1336123609216884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1336123609216884e-05

Optimization complete. Final v2v error: 3.856701135635376 mm

Highest mean error: 4.809671401977539 mm for frame 88

Lowest mean error: 3.1913721561431885 mm for frame 101

Saving results

Total time: 50.45977330207825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872605
Iteration 2/25 | Loss: 0.00088224
Iteration 3/25 | Loss: 0.00070170
Iteration 4/25 | Loss: 0.00067821
Iteration 5/25 | Loss: 0.00067364
Iteration 6/25 | Loss: 0.00067226
Iteration 7/25 | Loss: 0.00067185
Iteration 8/25 | Loss: 0.00067185
Iteration 9/25 | Loss: 0.00067185
Iteration 10/25 | Loss: 0.00067185
Iteration 11/25 | Loss: 0.00067185
Iteration 12/25 | Loss: 0.00067185
Iteration 13/25 | Loss: 0.00067185
Iteration 14/25 | Loss: 0.00067185
Iteration 15/25 | Loss: 0.00067185
Iteration 16/25 | Loss: 0.00067185
Iteration 17/25 | Loss: 0.00067185
Iteration 18/25 | Loss: 0.00067185
Iteration 19/25 | Loss: 0.00067185
Iteration 20/25 | Loss: 0.00067185
Iteration 21/25 | Loss: 0.00067185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006718490039929748, 0.0006718490039929748, 0.0006718490039929748, 0.0006718490039929748, 0.0006718490039929748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006718490039929748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46164501
Iteration 2/25 | Loss: 0.00031668
Iteration 3/25 | Loss: 0.00031668
Iteration 4/25 | Loss: 0.00031667
Iteration 5/25 | Loss: 0.00031667
Iteration 6/25 | Loss: 0.00031667
Iteration 7/25 | Loss: 0.00031667
Iteration 8/25 | Loss: 0.00031667
Iteration 9/25 | Loss: 0.00031667
Iteration 10/25 | Loss: 0.00031667
Iteration 11/25 | Loss: 0.00031667
Iteration 12/25 | Loss: 0.00031667
Iteration 13/25 | Loss: 0.00031667
Iteration 14/25 | Loss: 0.00031667
Iteration 15/25 | Loss: 0.00031667
Iteration 16/25 | Loss: 0.00031667
Iteration 17/25 | Loss: 0.00031667
Iteration 18/25 | Loss: 0.00031667
Iteration 19/25 | Loss: 0.00031667
Iteration 20/25 | Loss: 0.00031667
Iteration 21/25 | Loss: 0.00031667
Iteration 22/25 | Loss: 0.00031667
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00031667272560298443, 0.00031667272560298443, 0.00031667272560298443, 0.00031667272560298443, 0.00031667272560298443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031667272560298443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031667
Iteration 2/1000 | Loss: 0.00002293
Iteration 3/1000 | Loss: 0.00001485
Iteration 4/1000 | Loss: 0.00001312
Iteration 5/1000 | Loss: 0.00001246
Iteration 6/1000 | Loss: 0.00001206
Iteration 7/1000 | Loss: 0.00001177
Iteration 8/1000 | Loss: 0.00001162
Iteration 9/1000 | Loss: 0.00001157
Iteration 10/1000 | Loss: 0.00001156
Iteration 11/1000 | Loss: 0.00001156
Iteration 12/1000 | Loss: 0.00001152
Iteration 13/1000 | Loss: 0.00001152
Iteration 14/1000 | Loss: 0.00001151
Iteration 15/1000 | Loss: 0.00001151
Iteration 16/1000 | Loss: 0.00001151
Iteration 17/1000 | Loss: 0.00001150
Iteration 18/1000 | Loss: 0.00001150
Iteration 19/1000 | Loss: 0.00001149
Iteration 20/1000 | Loss: 0.00001148
Iteration 21/1000 | Loss: 0.00001148
Iteration 22/1000 | Loss: 0.00001148
Iteration 23/1000 | Loss: 0.00001148
Iteration 24/1000 | Loss: 0.00001148
Iteration 25/1000 | Loss: 0.00001148
Iteration 26/1000 | Loss: 0.00001148
Iteration 27/1000 | Loss: 0.00001148
Iteration 28/1000 | Loss: 0.00001148
Iteration 29/1000 | Loss: 0.00001147
Iteration 30/1000 | Loss: 0.00001147
Iteration 31/1000 | Loss: 0.00001147
Iteration 32/1000 | Loss: 0.00001146
Iteration 33/1000 | Loss: 0.00001146
Iteration 34/1000 | Loss: 0.00001145
Iteration 35/1000 | Loss: 0.00001145
Iteration 36/1000 | Loss: 0.00001145
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001145
Iteration 39/1000 | Loss: 0.00001144
Iteration 40/1000 | Loss: 0.00001144
Iteration 41/1000 | Loss: 0.00001144
Iteration 42/1000 | Loss: 0.00001144
Iteration 43/1000 | Loss: 0.00001144
Iteration 44/1000 | Loss: 0.00001143
Iteration 45/1000 | Loss: 0.00001143
Iteration 46/1000 | Loss: 0.00001143
Iteration 47/1000 | Loss: 0.00001142
Iteration 48/1000 | Loss: 0.00001142
Iteration 49/1000 | Loss: 0.00001142
Iteration 50/1000 | Loss: 0.00001142
Iteration 51/1000 | Loss: 0.00001142
Iteration 52/1000 | Loss: 0.00001142
Iteration 53/1000 | Loss: 0.00001141
Iteration 54/1000 | Loss: 0.00001141
Iteration 55/1000 | Loss: 0.00001141
Iteration 56/1000 | Loss: 0.00001141
Iteration 57/1000 | Loss: 0.00001140
Iteration 58/1000 | Loss: 0.00001140
Iteration 59/1000 | Loss: 0.00001140
Iteration 60/1000 | Loss: 0.00001140
Iteration 61/1000 | Loss: 0.00001139
Iteration 62/1000 | Loss: 0.00001139
Iteration 63/1000 | Loss: 0.00001139
Iteration 64/1000 | Loss: 0.00001139
Iteration 65/1000 | Loss: 0.00001139
Iteration 66/1000 | Loss: 0.00001138
Iteration 67/1000 | Loss: 0.00001138
Iteration 68/1000 | Loss: 0.00001138
Iteration 69/1000 | Loss: 0.00001137
Iteration 70/1000 | Loss: 0.00001137
Iteration 71/1000 | Loss: 0.00001137
Iteration 72/1000 | Loss: 0.00001136
Iteration 73/1000 | Loss: 0.00001136
Iteration 74/1000 | Loss: 0.00001136
Iteration 75/1000 | Loss: 0.00001136
Iteration 76/1000 | Loss: 0.00001136
Iteration 77/1000 | Loss: 0.00001136
Iteration 78/1000 | Loss: 0.00001136
Iteration 79/1000 | Loss: 0.00001136
Iteration 80/1000 | Loss: 0.00001135
Iteration 81/1000 | Loss: 0.00001135
Iteration 82/1000 | Loss: 0.00001135
Iteration 83/1000 | Loss: 0.00001135
Iteration 84/1000 | Loss: 0.00001135
Iteration 85/1000 | Loss: 0.00001135
Iteration 86/1000 | Loss: 0.00001135
Iteration 87/1000 | Loss: 0.00001135
Iteration 88/1000 | Loss: 0.00001135
Iteration 89/1000 | Loss: 0.00001135
Iteration 90/1000 | Loss: 0.00001135
Iteration 91/1000 | Loss: 0.00001135
Iteration 92/1000 | Loss: 0.00001134
Iteration 93/1000 | Loss: 0.00001134
Iteration 94/1000 | Loss: 0.00001134
Iteration 95/1000 | Loss: 0.00001134
Iteration 96/1000 | Loss: 0.00001134
Iteration 97/1000 | Loss: 0.00001134
Iteration 98/1000 | Loss: 0.00001134
Iteration 99/1000 | Loss: 0.00001134
Iteration 100/1000 | Loss: 0.00001134
Iteration 101/1000 | Loss: 0.00001134
Iteration 102/1000 | Loss: 0.00001134
Iteration 103/1000 | Loss: 0.00001133
Iteration 104/1000 | Loss: 0.00001133
Iteration 105/1000 | Loss: 0.00001133
Iteration 106/1000 | Loss: 0.00001133
Iteration 107/1000 | Loss: 0.00001133
Iteration 108/1000 | Loss: 0.00001133
Iteration 109/1000 | Loss: 0.00001133
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001133
Iteration 113/1000 | Loss: 0.00001133
Iteration 114/1000 | Loss: 0.00001133
Iteration 115/1000 | Loss: 0.00001132
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001132
Iteration 118/1000 | Loss: 0.00001132
Iteration 119/1000 | Loss: 0.00001132
Iteration 120/1000 | Loss: 0.00001132
Iteration 121/1000 | Loss: 0.00001132
Iteration 122/1000 | Loss: 0.00001132
Iteration 123/1000 | Loss: 0.00001132
Iteration 124/1000 | Loss: 0.00001132
Iteration 125/1000 | Loss: 0.00001132
Iteration 126/1000 | Loss: 0.00001132
Iteration 127/1000 | Loss: 0.00001132
Iteration 128/1000 | Loss: 0.00001132
Iteration 129/1000 | Loss: 0.00001132
Iteration 130/1000 | Loss: 0.00001132
Iteration 131/1000 | Loss: 0.00001132
Iteration 132/1000 | Loss: 0.00001132
Iteration 133/1000 | Loss: 0.00001132
Iteration 134/1000 | Loss: 0.00001132
Iteration 135/1000 | Loss: 0.00001132
Iteration 136/1000 | Loss: 0.00001132
Iteration 137/1000 | Loss: 0.00001132
Iteration 138/1000 | Loss: 0.00001132
Iteration 139/1000 | Loss: 0.00001132
Iteration 140/1000 | Loss: 0.00001132
Iteration 141/1000 | Loss: 0.00001132
Iteration 142/1000 | Loss: 0.00001132
Iteration 143/1000 | Loss: 0.00001132
Iteration 144/1000 | Loss: 0.00001132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.1324871593387797e-05, 1.1324871593387797e-05, 1.1324871593387797e-05, 1.1324871593387797e-05, 1.1324871593387797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1324871593387797e-05

Optimization complete. Final v2v error: 2.8885955810546875 mm

Highest mean error: 3.142512083053589 mm for frame 92

Lowest mean error: 2.60156512260437 mm for frame 0

Saving results

Total time: 31.497973918914795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046456
Iteration 2/25 | Loss: 0.00267140
Iteration 3/25 | Loss: 0.00187776
Iteration 4/25 | Loss: 0.00167469
Iteration 5/25 | Loss: 0.00148143
Iteration 6/25 | Loss: 0.00144257
Iteration 7/25 | Loss: 0.00128348
Iteration 8/25 | Loss: 0.00122730
Iteration 9/25 | Loss: 0.00119568
Iteration 10/25 | Loss: 0.00117428
Iteration 11/25 | Loss: 0.00115750
Iteration 12/25 | Loss: 0.00115542
Iteration 13/25 | Loss: 0.00113690
Iteration 14/25 | Loss: 0.00113197
Iteration 15/25 | Loss: 0.00112239
Iteration 16/25 | Loss: 0.00112297
Iteration 17/25 | Loss: 0.00111036
Iteration 18/25 | Loss: 0.00109902
Iteration 19/25 | Loss: 0.00109563
Iteration 20/25 | Loss: 0.00109469
Iteration 21/25 | Loss: 0.00109419
Iteration 22/25 | Loss: 0.00109359
Iteration 23/25 | Loss: 0.00109286
Iteration 24/25 | Loss: 0.00109156
Iteration 25/25 | Loss: 0.00109050

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42851257
Iteration 2/25 | Loss: 0.00710262
Iteration 3/25 | Loss: 0.00343079
Iteration 4/25 | Loss: 0.00343078
Iteration 5/25 | Loss: 0.00343077
Iteration 6/25 | Loss: 0.00343077
Iteration 7/25 | Loss: 0.00343077
Iteration 8/25 | Loss: 0.00343077
Iteration 9/25 | Loss: 0.00343077
Iteration 10/25 | Loss: 0.00343077
Iteration 11/25 | Loss: 0.00343077
Iteration 12/25 | Loss: 0.00343077
Iteration 13/25 | Loss: 0.00343077
Iteration 14/25 | Loss: 0.00343077
Iteration 15/25 | Loss: 0.00343077
Iteration 16/25 | Loss: 0.00343077
Iteration 17/25 | Loss: 0.00343077
Iteration 18/25 | Loss: 0.00343077
Iteration 19/25 | Loss: 0.00343077
Iteration 20/25 | Loss: 0.00343077
Iteration 21/25 | Loss: 0.00343077
Iteration 22/25 | Loss: 0.00343077
Iteration 23/25 | Loss: 0.00343077
Iteration 24/25 | Loss: 0.00343077
Iteration 25/25 | Loss: 0.00343077

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00343077
Iteration 2/1000 | Loss: 0.00459928
Iteration 3/1000 | Loss: 0.00097803
Iteration 4/1000 | Loss: 0.00087910
Iteration 5/1000 | Loss: 0.00032219
Iteration 6/1000 | Loss: 0.00022707
Iteration 7/1000 | Loss: 0.00019426
Iteration 8/1000 | Loss: 0.00042970
Iteration 9/1000 | Loss: 0.00042817
Iteration 10/1000 | Loss: 0.00046302
Iteration 11/1000 | Loss: 0.00015755
Iteration 12/1000 | Loss: 0.00014096
Iteration 13/1000 | Loss: 0.00030648
Iteration 14/1000 | Loss: 0.00013406
Iteration 15/1000 | Loss: 0.00012509
Iteration 16/1000 | Loss: 0.00031999
Iteration 17/1000 | Loss: 0.00038528
Iteration 18/1000 | Loss: 0.00048739
Iteration 19/1000 | Loss: 0.00012425
Iteration 20/1000 | Loss: 0.00011509
Iteration 21/1000 | Loss: 0.00011071
Iteration 22/1000 | Loss: 0.00010833
Iteration 23/1000 | Loss: 0.00010523
Iteration 24/1000 | Loss: 0.00010368
Iteration 25/1000 | Loss: 0.00010272
Iteration 26/1000 | Loss: 0.00010187
Iteration 27/1000 | Loss: 0.00010229
Iteration 28/1000 | Loss: 0.00020941
Iteration 29/1000 | Loss: 0.00053016
Iteration 30/1000 | Loss: 0.00014266
Iteration 31/1000 | Loss: 0.00010951
Iteration 32/1000 | Loss: 0.00010396
Iteration 33/1000 | Loss: 0.00009956
Iteration 34/1000 | Loss: 0.00009609
Iteration 35/1000 | Loss: 0.00009314
Iteration 36/1000 | Loss: 0.00009217
Iteration 37/1000 | Loss: 0.00009149
Iteration 38/1000 | Loss: 0.00009134
Iteration 39/1000 | Loss: 0.00009077
Iteration 40/1000 | Loss: 0.00009090
Iteration 41/1000 | Loss: 0.00009066
Iteration 42/1000 | Loss: 0.00009035
Iteration 43/1000 | Loss: 0.00020364
Iteration 44/1000 | Loss: 0.00009543
Iteration 45/1000 | Loss: 0.00009187
Iteration 46/1000 | Loss: 0.00009064
Iteration 47/1000 | Loss: 0.00008963
Iteration 48/1000 | Loss: 0.00008930
Iteration 49/1000 | Loss: 0.00008899
Iteration 50/1000 | Loss: 0.00008852
Iteration 51/1000 | Loss: 0.00008824
Iteration 52/1000 | Loss: 0.00008815
Iteration 53/1000 | Loss: 0.00008811
Iteration 54/1000 | Loss: 0.00008810
Iteration 55/1000 | Loss: 0.00008809
Iteration 56/1000 | Loss: 0.00008808
Iteration 57/1000 | Loss: 0.00008808
Iteration 58/1000 | Loss: 0.00008807
Iteration 59/1000 | Loss: 0.00008807
Iteration 60/1000 | Loss: 0.00008806
Iteration 61/1000 | Loss: 0.00008805
Iteration 62/1000 | Loss: 0.00008805
Iteration 63/1000 | Loss: 0.00008805
Iteration 64/1000 | Loss: 0.00008804
Iteration 65/1000 | Loss: 0.00008838
Iteration 66/1000 | Loss: 0.00008837
Iteration 67/1000 | Loss: 0.00008836
Iteration 68/1000 | Loss: 0.00008837
Iteration 69/1000 | Loss: 0.00008836
Iteration 70/1000 | Loss: 0.00008804
Iteration 71/1000 | Loss: 0.00008795
Iteration 72/1000 | Loss: 0.00008795
Iteration 73/1000 | Loss: 0.00008795
Iteration 74/1000 | Loss: 0.00008795
Iteration 75/1000 | Loss: 0.00008795
Iteration 76/1000 | Loss: 0.00008795
Iteration 77/1000 | Loss: 0.00008825
Iteration 78/1000 | Loss: 0.00008801
Iteration 79/1000 | Loss: 0.00008794
Iteration 80/1000 | Loss: 0.00008794
Iteration 81/1000 | Loss: 0.00008794
Iteration 82/1000 | Loss: 0.00008794
Iteration 83/1000 | Loss: 0.00008794
Iteration 84/1000 | Loss: 0.00008794
Iteration 85/1000 | Loss: 0.00008824
Iteration 86/1000 | Loss: 0.00008824
Iteration 87/1000 | Loss: 0.00008798
Iteration 88/1000 | Loss: 0.00008796
Iteration 89/1000 | Loss: 0.00008795
Iteration 90/1000 | Loss: 0.00008795
Iteration 91/1000 | Loss: 0.00008794
Iteration 92/1000 | Loss: 0.00008794
Iteration 93/1000 | Loss: 0.00008794
Iteration 94/1000 | Loss: 0.00008792
Iteration 95/1000 | Loss: 0.00008792
Iteration 96/1000 | Loss: 0.00008792
Iteration 97/1000 | Loss: 0.00008792
Iteration 98/1000 | Loss: 0.00008792
Iteration 99/1000 | Loss: 0.00008792
Iteration 100/1000 | Loss: 0.00008792
Iteration 101/1000 | Loss: 0.00008792
Iteration 102/1000 | Loss: 0.00008792
Iteration 103/1000 | Loss: 0.00008792
Iteration 104/1000 | Loss: 0.00008792
Iteration 105/1000 | Loss: 0.00008791
Iteration 106/1000 | Loss: 0.00008791
Iteration 107/1000 | Loss: 0.00008791
Iteration 108/1000 | Loss: 0.00008791
Iteration 109/1000 | Loss: 0.00008791
Iteration 110/1000 | Loss: 0.00008791
Iteration 111/1000 | Loss: 0.00008791
Iteration 112/1000 | Loss: 0.00008791
Iteration 113/1000 | Loss: 0.00008791
Iteration 114/1000 | Loss: 0.00008791
Iteration 115/1000 | Loss: 0.00008791
Iteration 116/1000 | Loss: 0.00008791
Iteration 117/1000 | Loss: 0.00008791
Iteration 118/1000 | Loss: 0.00008790
Iteration 119/1000 | Loss: 0.00008790
Iteration 120/1000 | Loss: 0.00008790
Iteration 121/1000 | Loss: 0.00008790
Iteration 122/1000 | Loss: 0.00008790
Iteration 123/1000 | Loss: 0.00008790
Iteration 124/1000 | Loss: 0.00008790
Iteration 125/1000 | Loss: 0.00008790
Iteration 126/1000 | Loss: 0.00008790
Iteration 127/1000 | Loss: 0.00008790
Iteration 128/1000 | Loss: 0.00008790
Iteration 129/1000 | Loss: 0.00008790
Iteration 130/1000 | Loss: 0.00008790
Iteration 131/1000 | Loss: 0.00008790
Iteration 132/1000 | Loss: 0.00008819
Iteration 133/1000 | Loss: 0.00008819
Iteration 134/1000 | Loss: 0.00008819
Iteration 135/1000 | Loss: 0.00008818
Iteration 136/1000 | Loss: 0.00008817
Iteration 137/1000 | Loss: 0.00008817
Iteration 138/1000 | Loss: 0.00008815
Iteration 139/1000 | Loss: 0.00008792
Iteration 140/1000 | Loss: 0.00008792
Iteration 141/1000 | Loss: 0.00008791
Iteration 142/1000 | Loss: 0.00008791
Iteration 143/1000 | Loss: 0.00008790
Iteration 144/1000 | Loss: 0.00008790
Iteration 145/1000 | Loss: 0.00008790
Iteration 146/1000 | Loss: 0.00008789
Iteration 147/1000 | Loss: 0.00008789
Iteration 148/1000 | Loss: 0.00008789
Iteration 149/1000 | Loss: 0.00008789
Iteration 150/1000 | Loss: 0.00008789
Iteration 151/1000 | Loss: 0.00008789
Iteration 152/1000 | Loss: 0.00008789
Iteration 153/1000 | Loss: 0.00008789
Iteration 154/1000 | Loss: 0.00008789
Iteration 155/1000 | Loss: 0.00008816
Iteration 156/1000 | Loss: 0.00008816
Iteration 157/1000 | Loss: 0.00008816
Iteration 158/1000 | Loss: 0.00008816
Iteration 159/1000 | Loss: 0.00008789
Iteration 160/1000 | Loss: 0.00008789
Iteration 161/1000 | Loss: 0.00008789
Iteration 162/1000 | Loss: 0.00008789
Iteration 163/1000 | Loss: 0.00008789
Iteration 164/1000 | Loss: 0.00008789
Iteration 165/1000 | Loss: 0.00008788
Iteration 166/1000 | Loss: 0.00008788
Iteration 167/1000 | Loss: 0.00008788
Iteration 168/1000 | Loss: 0.00008814
Iteration 169/1000 | Loss: 0.00008848
Iteration 170/1000 | Loss: 0.00008847
Iteration 171/1000 | Loss: 0.00008807
Iteration 172/1000 | Loss: 0.00008790
Iteration 173/1000 | Loss: 0.00008790
Iteration 174/1000 | Loss: 0.00008790
Iteration 175/1000 | Loss: 0.00008789
Iteration 176/1000 | Loss: 0.00008789
Iteration 177/1000 | Loss: 0.00008789
Iteration 178/1000 | Loss: 0.00008789
Iteration 179/1000 | Loss: 0.00008788
Iteration 180/1000 | Loss: 0.00008788
Iteration 181/1000 | Loss: 0.00008788
Iteration 182/1000 | Loss: 0.00008788
Iteration 183/1000 | Loss: 0.00008788
Iteration 184/1000 | Loss: 0.00008788
Iteration 185/1000 | Loss: 0.00008788
Iteration 186/1000 | Loss: 0.00008788
Iteration 187/1000 | Loss: 0.00008788
Iteration 188/1000 | Loss: 0.00008788
Iteration 189/1000 | Loss: 0.00008787
Iteration 190/1000 | Loss: 0.00008787
Iteration 191/1000 | Loss: 0.00008787
Iteration 192/1000 | Loss: 0.00008787
Iteration 193/1000 | Loss: 0.00008787
Iteration 194/1000 | Loss: 0.00008787
Iteration 195/1000 | Loss: 0.00008787
Iteration 196/1000 | Loss: 0.00008787
Iteration 197/1000 | Loss: 0.00008787
Iteration 198/1000 | Loss: 0.00008787
Iteration 199/1000 | Loss: 0.00008787
Iteration 200/1000 | Loss: 0.00008787
Iteration 201/1000 | Loss: 0.00008787
Iteration 202/1000 | Loss: 0.00008787
Iteration 203/1000 | Loss: 0.00008787
Iteration 204/1000 | Loss: 0.00008787
Iteration 205/1000 | Loss: 0.00008787
Iteration 206/1000 | Loss: 0.00008787
Iteration 207/1000 | Loss: 0.00008787
Iteration 208/1000 | Loss: 0.00008787
Iteration 209/1000 | Loss: 0.00008787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [8.786829857854173e-05, 8.786829857854173e-05, 8.786829857854173e-05, 8.786829857854173e-05, 8.786829857854173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.786829857854173e-05

Optimization complete. Final v2v error: 5.102873802185059 mm

Highest mean error: 21.26028060913086 mm for frame 222

Lowest mean error: 3.907355546951294 mm for frame 48

Saving results

Total time: 156.42670488357544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1190/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1190/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392525
Iteration 2/25 | Loss: 0.00099032
Iteration 3/25 | Loss: 0.00085286
Iteration 4/25 | Loss: 0.00082457
Iteration 5/25 | Loss: 0.00081326
Iteration 6/25 | Loss: 0.00080988
Iteration 7/25 | Loss: 0.00080889
Iteration 8/25 | Loss: 0.00080848
Iteration 9/25 | Loss: 0.00080847
Iteration 10/25 | Loss: 0.00080847
Iteration 11/25 | Loss: 0.00080847
Iteration 12/25 | Loss: 0.00080847
Iteration 13/25 | Loss: 0.00080847
Iteration 14/25 | Loss: 0.00080847
Iteration 15/25 | Loss: 0.00080847
Iteration 16/25 | Loss: 0.00080847
Iteration 17/25 | Loss: 0.00080847
Iteration 18/25 | Loss: 0.00080847
Iteration 19/25 | Loss: 0.00080847
Iteration 20/25 | Loss: 0.00080847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008084748405963182, 0.0008084748405963182, 0.0008084748405963182, 0.0008084748405963182, 0.0008084748405963182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008084748405963182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53564787
Iteration 2/25 | Loss: 0.00051977
Iteration 3/25 | Loss: 0.00051977
Iteration 4/25 | Loss: 0.00051977
Iteration 5/25 | Loss: 0.00051977
Iteration 6/25 | Loss: 0.00051977
Iteration 7/25 | Loss: 0.00051977
Iteration 8/25 | Loss: 0.00051977
Iteration 9/25 | Loss: 0.00051977
Iteration 10/25 | Loss: 0.00051977
Iteration 11/25 | Loss: 0.00051977
Iteration 12/25 | Loss: 0.00051977
Iteration 13/25 | Loss: 0.00051977
Iteration 14/25 | Loss: 0.00051977
Iteration 15/25 | Loss: 0.00051977
Iteration 16/25 | Loss: 0.00051977
Iteration 17/25 | Loss: 0.00051977
Iteration 18/25 | Loss: 0.00051977
Iteration 19/25 | Loss: 0.00051977
Iteration 20/25 | Loss: 0.00051977
Iteration 21/25 | Loss: 0.00051977
Iteration 22/25 | Loss: 0.00051977
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005197678110562265, 0.0005197678110562265, 0.0005197678110562265, 0.0005197678110562265, 0.0005197678110562265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005197678110562265

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051977
Iteration 2/1000 | Loss: 0.00004781
Iteration 3/1000 | Loss: 0.00003370
Iteration 4/1000 | Loss: 0.00002778
Iteration 5/1000 | Loss: 0.00002529
Iteration 6/1000 | Loss: 0.00002395
Iteration 7/1000 | Loss: 0.00002322
Iteration 8/1000 | Loss: 0.00002276
Iteration 9/1000 | Loss: 0.00002226
Iteration 10/1000 | Loss: 0.00002180
Iteration 11/1000 | Loss: 0.00002152
Iteration 12/1000 | Loss: 0.00002133
Iteration 13/1000 | Loss: 0.00002111
Iteration 14/1000 | Loss: 0.00002101
Iteration 15/1000 | Loss: 0.00002100
Iteration 16/1000 | Loss: 0.00002093
Iteration 17/1000 | Loss: 0.00002093
Iteration 18/1000 | Loss: 0.00002090
Iteration 19/1000 | Loss: 0.00002089
Iteration 20/1000 | Loss: 0.00002086
Iteration 21/1000 | Loss: 0.00002085
Iteration 22/1000 | Loss: 0.00002085
Iteration 23/1000 | Loss: 0.00002084
Iteration 24/1000 | Loss: 0.00002084
Iteration 25/1000 | Loss: 0.00002083
Iteration 26/1000 | Loss: 0.00002082
Iteration 27/1000 | Loss: 0.00002082
Iteration 28/1000 | Loss: 0.00002082
Iteration 29/1000 | Loss: 0.00002082
Iteration 30/1000 | Loss: 0.00002081
Iteration 31/1000 | Loss: 0.00002077
Iteration 32/1000 | Loss: 0.00002077
Iteration 33/1000 | Loss: 0.00002076
Iteration 34/1000 | Loss: 0.00002076
Iteration 35/1000 | Loss: 0.00002075
Iteration 36/1000 | Loss: 0.00002075
Iteration 37/1000 | Loss: 0.00002075
Iteration 38/1000 | Loss: 0.00002074
Iteration 39/1000 | Loss: 0.00002074
Iteration 40/1000 | Loss: 0.00002074
Iteration 41/1000 | Loss: 0.00002074
Iteration 42/1000 | Loss: 0.00002074
Iteration 43/1000 | Loss: 0.00002073
Iteration 44/1000 | Loss: 0.00002073
Iteration 45/1000 | Loss: 0.00002073
Iteration 46/1000 | Loss: 0.00002072
Iteration 47/1000 | Loss: 0.00002072
Iteration 48/1000 | Loss: 0.00002071
Iteration 49/1000 | Loss: 0.00002071
Iteration 50/1000 | Loss: 0.00002071
Iteration 51/1000 | Loss: 0.00002070
Iteration 52/1000 | Loss: 0.00002070
Iteration 53/1000 | Loss: 0.00002070
Iteration 54/1000 | Loss: 0.00002070
Iteration 55/1000 | Loss: 0.00002069
Iteration 56/1000 | Loss: 0.00002069
Iteration 57/1000 | Loss: 0.00002069
Iteration 58/1000 | Loss: 0.00002069
Iteration 59/1000 | Loss: 0.00002069
Iteration 60/1000 | Loss: 0.00002068
Iteration 61/1000 | Loss: 0.00002068
Iteration 62/1000 | Loss: 0.00002068
Iteration 63/1000 | Loss: 0.00002068
Iteration 64/1000 | Loss: 0.00002068
Iteration 65/1000 | Loss: 0.00002067
Iteration 66/1000 | Loss: 0.00002067
Iteration 67/1000 | Loss: 0.00002067
Iteration 68/1000 | Loss: 0.00002067
Iteration 69/1000 | Loss: 0.00002067
Iteration 70/1000 | Loss: 0.00002067
Iteration 71/1000 | Loss: 0.00002067
Iteration 72/1000 | Loss: 0.00002067
Iteration 73/1000 | Loss: 0.00002067
Iteration 74/1000 | Loss: 0.00002066
Iteration 75/1000 | Loss: 0.00002066
Iteration 76/1000 | Loss: 0.00002066
Iteration 77/1000 | Loss: 0.00002066
Iteration 78/1000 | Loss: 0.00002066
Iteration 79/1000 | Loss: 0.00002066
Iteration 80/1000 | Loss: 0.00002066
Iteration 81/1000 | Loss: 0.00002066
Iteration 82/1000 | Loss: 0.00002065
Iteration 83/1000 | Loss: 0.00002065
Iteration 84/1000 | Loss: 0.00002065
Iteration 85/1000 | Loss: 0.00002065
Iteration 86/1000 | Loss: 0.00002064
Iteration 87/1000 | Loss: 0.00002064
Iteration 88/1000 | Loss: 0.00002064
Iteration 89/1000 | Loss: 0.00002064
Iteration 90/1000 | Loss: 0.00002064
Iteration 91/1000 | Loss: 0.00002064
Iteration 92/1000 | Loss: 0.00002063
Iteration 93/1000 | Loss: 0.00002063
Iteration 94/1000 | Loss: 0.00002063
Iteration 95/1000 | Loss: 0.00002063
Iteration 96/1000 | Loss: 0.00002063
Iteration 97/1000 | Loss: 0.00002062
Iteration 98/1000 | Loss: 0.00002062
Iteration 99/1000 | Loss: 0.00002062
Iteration 100/1000 | Loss: 0.00002062
Iteration 101/1000 | Loss: 0.00002062
Iteration 102/1000 | Loss: 0.00002062
Iteration 103/1000 | Loss: 0.00002062
Iteration 104/1000 | Loss: 0.00002062
Iteration 105/1000 | Loss: 0.00002062
Iteration 106/1000 | Loss: 0.00002062
Iteration 107/1000 | Loss: 0.00002062
Iteration 108/1000 | Loss: 0.00002062
Iteration 109/1000 | Loss: 0.00002062
Iteration 110/1000 | Loss: 0.00002062
Iteration 111/1000 | Loss: 0.00002062
Iteration 112/1000 | Loss: 0.00002062
Iteration 113/1000 | Loss: 0.00002062
Iteration 114/1000 | Loss: 0.00002062
Iteration 115/1000 | Loss: 0.00002062
Iteration 116/1000 | Loss: 0.00002062
Iteration 117/1000 | Loss: 0.00002062
Iteration 118/1000 | Loss: 0.00002062
Iteration 119/1000 | Loss: 0.00002062
Iteration 120/1000 | Loss: 0.00002062
Iteration 121/1000 | Loss: 0.00002062
Iteration 122/1000 | Loss: 0.00002062
Iteration 123/1000 | Loss: 0.00002062
Iteration 124/1000 | Loss: 0.00002062
Iteration 125/1000 | Loss: 0.00002062
Iteration 126/1000 | Loss: 0.00002062
Iteration 127/1000 | Loss: 0.00002062
Iteration 128/1000 | Loss: 0.00002062
Iteration 129/1000 | Loss: 0.00002062
Iteration 130/1000 | Loss: 0.00002062
Iteration 131/1000 | Loss: 0.00002062
Iteration 132/1000 | Loss: 0.00002062
Iteration 133/1000 | Loss: 0.00002062
Iteration 134/1000 | Loss: 0.00002062
Iteration 135/1000 | Loss: 0.00002062
Iteration 136/1000 | Loss: 0.00002062
Iteration 137/1000 | Loss: 0.00002062
Iteration 138/1000 | Loss: 0.00002062
Iteration 139/1000 | Loss: 0.00002062
Iteration 140/1000 | Loss: 0.00002062
Iteration 141/1000 | Loss: 0.00002062
Iteration 142/1000 | Loss: 0.00002062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.0616951587726362e-05, 2.0616951587726362e-05, 2.0616951587726362e-05, 2.0616951587726362e-05, 2.0616951587726362e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0616951587726362e-05

Optimization complete. Final v2v error: 3.7274320125579834 mm

Highest mean error: 4.967222690582275 mm for frame 106

Lowest mean error: 2.736684799194336 mm for frame 95

Saving results

Total time: 41.18401622772217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00515329
Iteration 2/25 | Loss: 0.00086805
Iteration 3/25 | Loss: 0.00070627
Iteration 4/25 | Loss: 0.00067406
Iteration 5/25 | Loss: 0.00066648
Iteration 6/25 | Loss: 0.00066498
Iteration 7/25 | Loss: 0.00066434
Iteration 8/25 | Loss: 0.00066412
Iteration 9/25 | Loss: 0.00066408
Iteration 10/25 | Loss: 0.00066408
Iteration 11/25 | Loss: 0.00066408
Iteration 12/25 | Loss: 0.00066408
Iteration 13/25 | Loss: 0.00066408
Iteration 14/25 | Loss: 0.00066408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006640806095674634, 0.0006640806095674634, 0.0006640806095674634, 0.0006640806095674634, 0.0006640806095674634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006640806095674634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44478655
Iteration 2/25 | Loss: 0.00009956
Iteration 3/25 | Loss: 0.00009955
Iteration 4/25 | Loss: 0.00009955
Iteration 5/25 | Loss: 0.00009955
Iteration 6/25 | Loss: 0.00009955
Iteration 7/25 | Loss: 0.00009955
Iteration 8/25 | Loss: 0.00009955
Iteration 9/25 | Loss: 0.00009955
Iteration 10/25 | Loss: 0.00009955
Iteration 11/25 | Loss: 0.00009955
Iteration 12/25 | Loss: 0.00009955
Iteration 13/25 | Loss: 0.00009955
Iteration 14/25 | Loss: 0.00009955
Iteration 15/25 | Loss: 0.00009955
Iteration 16/25 | Loss: 0.00009955
Iteration 17/25 | Loss: 0.00009955
Iteration 18/25 | Loss: 0.00009955
Iteration 19/25 | Loss: 0.00009955
Iteration 20/25 | Loss: 0.00009955
Iteration 21/25 | Loss: 0.00009955
Iteration 22/25 | Loss: 0.00009955
Iteration 23/25 | Loss: 0.00009955
Iteration 24/25 | Loss: 0.00009955
Iteration 25/25 | Loss: 0.00009955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00009955
Iteration 2/1000 | Loss: 0.00003020
Iteration 3/1000 | Loss: 0.00002591
Iteration 4/1000 | Loss: 0.00002373
Iteration 5/1000 | Loss: 0.00002270
Iteration 6/1000 | Loss: 0.00002193
Iteration 7/1000 | Loss: 0.00002148
Iteration 8/1000 | Loss: 0.00002114
Iteration 9/1000 | Loss: 0.00002094
Iteration 10/1000 | Loss: 0.00002092
Iteration 11/1000 | Loss: 0.00002083
Iteration 12/1000 | Loss: 0.00002077
Iteration 13/1000 | Loss: 0.00002075
Iteration 14/1000 | Loss: 0.00002074
Iteration 15/1000 | Loss: 0.00002071
Iteration 16/1000 | Loss: 0.00002068
Iteration 17/1000 | Loss: 0.00002067
Iteration 18/1000 | Loss: 0.00002065
Iteration 19/1000 | Loss: 0.00002064
Iteration 20/1000 | Loss: 0.00002064
Iteration 21/1000 | Loss: 0.00002063
Iteration 22/1000 | Loss: 0.00002063
Iteration 23/1000 | Loss: 0.00002062
Iteration 24/1000 | Loss: 0.00002061
Iteration 25/1000 | Loss: 0.00002061
Iteration 26/1000 | Loss: 0.00002060
Iteration 27/1000 | Loss: 0.00002060
Iteration 28/1000 | Loss: 0.00002059
Iteration 29/1000 | Loss: 0.00002059
Iteration 30/1000 | Loss: 0.00002058
Iteration 31/1000 | Loss: 0.00002058
Iteration 32/1000 | Loss: 0.00002058
Iteration 33/1000 | Loss: 0.00002057
Iteration 34/1000 | Loss: 0.00002057
Iteration 35/1000 | Loss: 0.00002056
Iteration 36/1000 | Loss: 0.00002056
Iteration 37/1000 | Loss: 0.00002055
Iteration 38/1000 | Loss: 0.00002055
Iteration 39/1000 | Loss: 0.00002055
Iteration 40/1000 | Loss: 0.00002054
Iteration 41/1000 | Loss: 0.00002053
Iteration 42/1000 | Loss: 0.00002053
Iteration 43/1000 | Loss: 0.00002053
Iteration 44/1000 | Loss: 0.00002053
Iteration 45/1000 | Loss: 0.00002052
Iteration 46/1000 | Loss: 0.00002052
Iteration 47/1000 | Loss: 0.00002052
Iteration 48/1000 | Loss: 0.00002052
Iteration 49/1000 | Loss: 0.00002052
Iteration 50/1000 | Loss: 0.00002052
Iteration 51/1000 | Loss: 0.00002051
Iteration 52/1000 | Loss: 0.00002051
Iteration 53/1000 | Loss: 0.00002050
Iteration 54/1000 | Loss: 0.00002050
Iteration 55/1000 | Loss: 0.00002050
Iteration 56/1000 | Loss: 0.00002049
Iteration 57/1000 | Loss: 0.00002049
Iteration 58/1000 | Loss: 0.00002049
Iteration 59/1000 | Loss: 0.00002048
Iteration 60/1000 | Loss: 0.00002048
Iteration 61/1000 | Loss: 0.00002048
Iteration 62/1000 | Loss: 0.00002048
Iteration 63/1000 | Loss: 0.00002048
Iteration 64/1000 | Loss: 0.00002048
Iteration 65/1000 | Loss: 0.00002048
Iteration 66/1000 | Loss: 0.00002047
Iteration 67/1000 | Loss: 0.00002047
Iteration 68/1000 | Loss: 0.00002047
Iteration 69/1000 | Loss: 0.00002047
Iteration 70/1000 | Loss: 0.00002047
Iteration 71/1000 | Loss: 0.00002047
Iteration 72/1000 | Loss: 0.00002047
Iteration 73/1000 | Loss: 0.00002047
Iteration 74/1000 | Loss: 0.00002047
Iteration 75/1000 | Loss: 0.00002047
Iteration 76/1000 | Loss: 0.00002046
Iteration 77/1000 | Loss: 0.00002046
Iteration 78/1000 | Loss: 0.00002046
Iteration 79/1000 | Loss: 0.00002046
Iteration 80/1000 | Loss: 0.00002046
Iteration 81/1000 | Loss: 0.00002046
Iteration 82/1000 | Loss: 0.00002046
Iteration 83/1000 | Loss: 0.00002046
Iteration 84/1000 | Loss: 0.00002046
Iteration 85/1000 | Loss: 0.00002046
Iteration 86/1000 | Loss: 0.00002046
Iteration 87/1000 | Loss: 0.00002046
Iteration 88/1000 | Loss: 0.00002046
Iteration 89/1000 | Loss: 0.00002045
Iteration 90/1000 | Loss: 0.00002045
Iteration 91/1000 | Loss: 0.00002045
Iteration 92/1000 | Loss: 0.00002045
Iteration 93/1000 | Loss: 0.00002045
Iteration 94/1000 | Loss: 0.00002045
Iteration 95/1000 | Loss: 0.00002045
Iteration 96/1000 | Loss: 0.00002045
Iteration 97/1000 | Loss: 0.00002045
Iteration 98/1000 | Loss: 0.00002045
Iteration 99/1000 | Loss: 0.00002045
Iteration 100/1000 | Loss: 0.00002045
Iteration 101/1000 | Loss: 0.00002045
Iteration 102/1000 | Loss: 0.00002045
Iteration 103/1000 | Loss: 0.00002044
Iteration 104/1000 | Loss: 0.00002044
Iteration 105/1000 | Loss: 0.00002044
Iteration 106/1000 | Loss: 0.00002044
Iteration 107/1000 | Loss: 0.00002044
Iteration 108/1000 | Loss: 0.00002044
Iteration 109/1000 | Loss: 0.00002044
Iteration 110/1000 | Loss: 0.00002044
Iteration 111/1000 | Loss: 0.00002044
Iteration 112/1000 | Loss: 0.00002044
Iteration 113/1000 | Loss: 0.00002044
Iteration 114/1000 | Loss: 0.00002044
Iteration 115/1000 | Loss: 0.00002044
Iteration 116/1000 | Loss: 0.00002043
Iteration 117/1000 | Loss: 0.00002043
Iteration 118/1000 | Loss: 0.00002043
Iteration 119/1000 | Loss: 0.00002043
Iteration 120/1000 | Loss: 0.00002043
Iteration 121/1000 | Loss: 0.00002043
Iteration 122/1000 | Loss: 0.00002043
Iteration 123/1000 | Loss: 0.00002043
Iteration 124/1000 | Loss: 0.00002043
Iteration 125/1000 | Loss: 0.00002043
Iteration 126/1000 | Loss: 0.00002043
Iteration 127/1000 | Loss: 0.00002043
Iteration 128/1000 | Loss: 0.00002042
Iteration 129/1000 | Loss: 0.00002042
Iteration 130/1000 | Loss: 0.00002042
Iteration 131/1000 | Loss: 0.00002042
Iteration 132/1000 | Loss: 0.00002042
Iteration 133/1000 | Loss: 0.00002042
Iteration 134/1000 | Loss: 0.00002042
Iteration 135/1000 | Loss: 0.00002042
Iteration 136/1000 | Loss: 0.00002042
Iteration 137/1000 | Loss: 0.00002042
Iteration 138/1000 | Loss: 0.00002042
Iteration 139/1000 | Loss: 0.00002042
Iteration 140/1000 | Loss: 0.00002042
Iteration 141/1000 | Loss: 0.00002042
Iteration 142/1000 | Loss: 0.00002041
Iteration 143/1000 | Loss: 0.00002041
Iteration 144/1000 | Loss: 0.00002041
Iteration 145/1000 | Loss: 0.00002041
Iteration 146/1000 | Loss: 0.00002041
Iteration 147/1000 | Loss: 0.00002041
Iteration 148/1000 | Loss: 0.00002041
Iteration 149/1000 | Loss: 0.00002041
Iteration 150/1000 | Loss: 0.00002041
Iteration 151/1000 | Loss: 0.00002041
Iteration 152/1000 | Loss: 0.00002041
Iteration 153/1000 | Loss: 0.00002040
Iteration 154/1000 | Loss: 0.00002040
Iteration 155/1000 | Loss: 0.00002040
Iteration 156/1000 | Loss: 0.00002040
Iteration 157/1000 | Loss: 0.00002040
Iteration 158/1000 | Loss: 0.00002040
Iteration 159/1000 | Loss: 0.00002040
Iteration 160/1000 | Loss: 0.00002040
Iteration 161/1000 | Loss: 0.00002040
Iteration 162/1000 | Loss: 0.00002040
Iteration 163/1000 | Loss: 0.00002040
Iteration 164/1000 | Loss: 0.00002040
Iteration 165/1000 | Loss: 0.00002040
Iteration 166/1000 | Loss: 0.00002040
Iteration 167/1000 | Loss: 0.00002040
Iteration 168/1000 | Loss: 0.00002040
Iteration 169/1000 | Loss: 0.00002040
Iteration 170/1000 | Loss: 0.00002040
Iteration 171/1000 | Loss: 0.00002040
Iteration 172/1000 | Loss: 0.00002040
Iteration 173/1000 | Loss: 0.00002039
Iteration 174/1000 | Loss: 0.00002039
Iteration 175/1000 | Loss: 0.00002039
Iteration 176/1000 | Loss: 0.00002039
Iteration 177/1000 | Loss: 0.00002039
Iteration 178/1000 | Loss: 0.00002039
Iteration 179/1000 | Loss: 0.00002039
Iteration 180/1000 | Loss: 0.00002039
Iteration 181/1000 | Loss: 0.00002039
Iteration 182/1000 | Loss: 0.00002039
Iteration 183/1000 | Loss: 0.00002039
Iteration 184/1000 | Loss: 0.00002039
Iteration 185/1000 | Loss: 0.00002039
Iteration 186/1000 | Loss: 0.00002039
Iteration 187/1000 | Loss: 0.00002039
Iteration 188/1000 | Loss: 0.00002039
Iteration 189/1000 | Loss: 0.00002039
Iteration 190/1000 | Loss: 0.00002039
Iteration 191/1000 | Loss: 0.00002039
Iteration 192/1000 | Loss: 0.00002039
Iteration 193/1000 | Loss: 0.00002039
Iteration 194/1000 | Loss: 0.00002039
Iteration 195/1000 | Loss: 0.00002039
Iteration 196/1000 | Loss: 0.00002039
Iteration 197/1000 | Loss: 0.00002039
Iteration 198/1000 | Loss: 0.00002039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.0389461496961303e-05, 2.0389461496961303e-05, 2.0389461496961303e-05, 2.0389461496961303e-05, 2.0389461496961303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0389461496961303e-05

Optimization complete. Final v2v error: 3.814969778060913 mm

Highest mean error: 4.493742942810059 mm for frame 93

Lowest mean error: 3.180284261703491 mm for frame 142

Saving results

Total time: 40.04531121253967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01097541
Iteration 2/25 | Loss: 0.00266806
Iteration 3/25 | Loss: 0.00208814
Iteration 4/25 | Loss: 0.00196548
Iteration 5/25 | Loss: 0.00200524
Iteration 6/25 | Loss: 0.00188083
Iteration 7/25 | Loss: 0.00181288
Iteration 8/25 | Loss: 0.00145805
Iteration 9/25 | Loss: 0.00125636
Iteration 10/25 | Loss: 0.00108891
Iteration 11/25 | Loss: 0.00104971
Iteration 12/25 | Loss: 0.00102317
Iteration 13/25 | Loss: 0.00100470
Iteration 14/25 | Loss: 0.00098267
Iteration 15/25 | Loss: 0.00097857
Iteration 16/25 | Loss: 0.00097624
Iteration 17/25 | Loss: 0.00097189
Iteration 18/25 | Loss: 0.00096756
Iteration 19/25 | Loss: 0.00096677
Iteration 20/25 | Loss: 0.00096659
Iteration 21/25 | Loss: 0.00096647
Iteration 22/25 | Loss: 0.00096639
Iteration 23/25 | Loss: 0.00096639
Iteration 24/25 | Loss: 0.00096639
Iteration 25/25 | Loss: 0.00096638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27421451
Iteration 2/25 | Loss: 0.00179544
Iteration 3/25 | Loss: 0.00175795
Iteration 4/25 | Loss: 0.00175795
Iteration 5/25 | Loss: 0.00175795
Iteration 6/25 | Loss: 0.00175795
Iteration 7/25 | Loss: 0.00175794
Iteration 8/25 | Loss: 0.00175794
Iteration 9/25 | Loss: 0.00175794
Iteration 10/25 | Loss: 0.00175794
Iteration 11/25 | Loss: 0.00175794
Iteration 12/25 | Loss: 0.00175794
Iteration 13/25 | Loss: 0.00175794
Iteration 14/25 | Loss: 0.00175794
Iteration 15/25 | Loss: 0.00175794
Iteration 16/25 | Loss: 0.00175794
Iteration 17/25 | Loss: 0.00175794
Iteration 18/25 | Loss: 0.00175794
Iteration 19/25 | Loss: 0.00175794
Iteration 20/25 | Loss: 0.00175794
Iteration 21/25 | Loss: 0.00175794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0017579440027475357, 0.0017579440027475357, 0.0017579440027475357, 0.0017579440027475357, 0.0017579440027475357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017579440027475357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175794
Iteration 2/1000 | Loss: 0.00059554
Iteration 3/1000 | Loss: 0.00056621
Iteration 4/1000 | Loss: 0.00079501
Iteration 5/1000 | Loss: 0.00060965
Iteration 6/1000 | Loss: 0.00061123
Iteration 7/1000 | Loss: 0.00044304
Iteration 8/1000 | Loss: 0.00037831
Iteration 9/1000 | Loss: 0.00012704
Iteration 10/1000 | Loss: 0.00013535
Iteration 11/1000 | Loss: 0.00045201
Iteration 12/1000 | Loss: 0.00064544
Iteration 13/1000 | Loss: 0.00027262
Iteration 14/1000 | Loss: 0.00102971
Iteration 15/1000 | Loss: 0.00068497
Iteration 16/1000 | Loss: 0.00033407
Iteration 17/1000 | Loss: 0.00011219
Iteration 18/1000 | Loss: 0.00032726
Iteration 19/1000 | Loss: 0.00158480
Iteration 20/1000 | Loss: 0.00048704
Iteration 21/1000 | Loss: 0.00007631
Iteration 22/1000 | Loss: 0.00006662
Iteration 23/1000 | Loss: 0.00006062
Iteration 24/1000 | Loss: 0.00062192
Iteration 25/1000 | Loss: 0.00131237
Iteration 26/1000 | Loss: 0.00028581
Iteration 27/1000 | Loss: 0.00007820
Iteration 28/1000 | Loss: 0.00011357
Iteration 29/1000 | Loss: 0.00007222
Iteration 30/1000 | Loss: 0.00021250
Iteration 31/1000 | Loss: 0.00005389
Iteration 32/1000 | Loss: 0.00036470
Iteration 33/1000 | Loss: 0.00012325
Iteration 34/1000 | Loss: 0.00006459
Iteration 35/1000 | Loss: 0.00005224
Iteration 36/1000 | Loss: 0.00004969
Iteration 37/1000 | Loss: 0.00031106
Iteration 38/1000 | Loss: 0.00008158
Iteration 39/1000 | Loss: 0.00031725
Iteration 40/1000 | Loss: 0.00032670
Iteration 41/1000 | Loss: 0.00028211
Iteration 42/1000 | Loss: 0.00020464
Iteration 43/1000 | Loss: 0.00013755
Iteration 44/1000 | Loss: 0.00012549
Iteration 45/1000 | Loss: 0.00004663
Iteration 46/1000 | Loss: 0.00007412
Iteration 47/1000 | Loss: 0.00004028
Iteration 48/1000 | Loss: 0.00010781
Iteration 49/1000 | Loss: 0.00003712
Iteration 50/1000 | Loss: 0.00008866
Iteration 51/1000 | Loss: 0.00003618
Iteration 52/1000 | Loss: 0.00003576
Iteration 53/1000 | Loss: 0.00036147
Iteration 54/1000 | Loss: 0.00023418
Iteration 55/1000 | Loss: 0.00006340
Iteration 56/1000 | Loss: 0.00017301
Iteration 57/1000 | Loss: 0.00086707
Iteration 58/1000 | Loss: 0.00003591
Iteration 59/1000 | Loss: 0.00008893
Iteration 60/1000 | Loss: 0.00003117
Iteration 61/1000 | Loss: 0.00002975
Iteration 62/1000 | Loss: 0.00007153
Iteration 63/1000 | Loss: 0.00002843
Iteration 64/1000 | Loss: 0.00002798
Iteration 65/1000 | Loss: 0.00002783
Iteration 66/1000 | Loss: 0.00002777
Iteration 67/1000 | Loss: 0.00002776
Iteration 68/1000 | Loss: 0.00002776
Iteration 69/1000 | Loss: 0.00009053
Iteration 70/1000 | Loss: 0.00002772
Iteration 71/1000 | Loss: 0.00002760
Iteration 72/1000 | Loss: 0.00002760
Iteration 73/1000 | Loss: 0.00002760
Iteration 74/1000 | Loss: 0.00002759
Iteration 75/1000 | Loss: 0.00002759
Iteration 76/1000 | Loss: 0.00002758
Iteration 77/1000 | Loss: 0.00002758
Iteration 78/1000 | Loss: 0.00002758
Iteration 79/1000 | Loss: 0.00002758
Iteration 80/1000 | Loss: 0.00002754
Iteration 81/1000 | Loss: 0.00002753
Iteration 82/1000 | Loss: 0.00002751
Iteration 83/1000 | Loss: 0.00002751
Iteration 84/1000 | Loss: 0.00002750
Iteration 85/1000 | Loss: 0.00002749
Iteration 86/1000 | Loss: 0.00002749
Iteration 87/1000 | Loss: 0.00002748
Iteration 88/1000 | Loss: 0.00002748
Iteration 89/1000 | Loss: 0.00002748
Iteration 90/1000 | Loss: 0.00002747
Iteration 91/1000 | Loss: 0.00002747
Iteration 92/1000 | Loss: 0.00002747
Iteration 93/1000 | Loss: 0.00002747
Iteration 94/1000 | Loss: 0.00002746
Iteration 95/1000 | Loss: 0.00002746
Iteration 96/1000 | Loss: 0.00002745
Iteration 97/1000 | Loss: 0.00002745
Iteration 98/1000 | Loss: 0.00002744
Iteration 99/1000 | Loss: 0.00002743
Iteration 100/1000 | Loss: 0.00002743
Iteration 101/1000 | Loss: 0.00002742
Iteration 102/1000 | Loss: 0.00002742
Iteration 103/1000 | Loss: 0.00002741
Iteration 104/1000 | Loss: 0.00002741
Iteration 105/1000 | Loss: 0.00002740
Iteration 106/1000 | Loss: 0.00002739
Iteration 107/1000 | Loss: 0.00002738
Iteration 108/1000 | Loss: 0.00002738
Iteration 109/1000 | Loss: 0.00002738
Iteration 110/1000 | Loss: 0.00002738
Iteration 111/1000 | Loss: 0.00002738
Iteration 112/1000 | Loss: 0.00002738
Iteration 113/1000 | Loss: 0.00002738
Iteration 114/1000 | Loss: 0.00002738
Iteration 115/1000 | Loss: 0.00002738
Iteration 116/1000 | Loss: 0.00002738
Iteration 117/1000 | Loss: 0.00002738
Iteration 118/1000 | Loss: 0.00002738
Iteration 119/1000 | Loss: 0.00002738
Iteration 120/1000 | Loss: 0.00002737
Iteration 121/1000 | Loss: 0.00002737
Iteration 122/1000 | Loss: 0.00002737
Iteration 123/1000 | Loss: 0.00002737
Iteration 124/1000 | Loss: 0.00002736
Iteration 125/1000 | Loss: 0.00002736
Iteration 126/1000 | Loss: 0.00002736
Iteration 127/1000 | Loss: 0.00002736
Iteration 128/1000 | Loss: 0.00002735
Iteration 129/1000 | Loss: 0.00002735
Iteration 130/1000 | Loss: 0.00002735
Iteration 131/1000 | Loss: 0.00002735
Iteration 132/1000 | Loss: 0.00002735
Iteration 133/1000 | Loss: 0.00002735
Iteration 134/1000 | Loss: 0.00002735
Iteration 135/1000 | Loss: 0.00002735
Iteration 136/1000 | Loss: 0.00002735
Iteration 137/1000 | Loss: 0.00002735
Iteration 138/1000 | Loss: 0.00002735
Iteration 139/1000 | Loss: 0.00002735
Iteration 140/1000 | Loss: 0.00002735
Iteration 141/1000 | Loss: 0.00002735
Iteration 142/1000 | Loss: 0.00002735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.7351399694452994e-05, 2.7351399694452994e-05, 2.7351399694452994e-05, 2.7351399694452994e-05, 2.7351399694452994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7351399694452994e-05

Optimization complete. Final v2v error: 4.586902141571045 mm

Highest mean error: 4.970963478088379 mm for frame 114

Lowest mean error: 4.432663917541504 mm for frame 17

Saving results

Total time: 137.77914595603943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496807
Iteration 2/25 | Loss: 0.00097270
Iteration 3/25 | Loss: 0.00070114
Iteration 4/25 | Loss: 0.00065171
Iteration 5/25 | Loss: 0.00064150
Iteration 6/25 | Loss: 0.00063956
Iteration 7/25 | Loss: 0.00063917
Iteration 8/25 | Loss: 0.00063917
Iteration 9/25 | Loss: 0.00063917
Iteration 10/25 | Loss: 0.00063917
Iteration 11/25 | Loss: 0.00063917
Iteration 12/25 | Loss: 0.00063917
Iteration 13/25 | Loss: 0.00063917
Iteration 14/25 | Loss: 0.00063917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006391739589162171, 0.0006391739589162171, 0.0006391739589162171, 0.0006391739589162171, 0.0006391739589162171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006391739589162171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80452025
Iteration 2/25 | Loss: 0.00016340
Iteration 3/25 | Loss: 0.00016340
Iteration 4/25 | Loss: 0.00016339
Iteration 5/25 | Loss: 0.00016339
Iteration 6/25 | Loss: 0.00016339
Iteration 7/25 | Loss: 0.00016339
Iteration 8/25 | Loss: 0.00016339
Iteration 9/25 | Loss: 0.00016339
Iteration 10/25 | Loss: 0.00016339
Iteration 11/25 | Loss: 0.00016339
Iteration 12/25 | Loss: 0.00016339
Iteration 13/25 | Loss: 0.00016339
Iteration 14/25 | Loss: 0.00016339
Iteration 15/25 | Loss: 0.00016339
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0001633937208680436, 0.0001633937208680436, 0.0001633937208680436, 0.0001633937208680436, 0.0001633937208680436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001633937208680436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016339
Iteration 2/1000 | Loss: 0.00002514
Iteration 3/1000 | Loss: 0.00002028
Iteration 4/1000 | Loss: 0.00001863
Iteration 5/1000 | Loss: 0.00001776
Iteration 6/1000 | Loss: 0.00001727
Iteration 7/1000 | Loss: 0.00001676
Iteration 8/1000 | Loss: 0.00001637
Iteration 9/1000 | Loss: 0.00001609
Iteration 10/1000 | Loss: 0.00001604
Iteration 11/1000 | Loss: 0.00001593
Iteration 12/1000 | Loss: 0.00001593
Iteration 13/1000 | Loss: 0.00001585
Iteration 14/1000 | Loss: 0.00001585
Iteration 15/1000 | Loss: 0.00001585
Iteration 16/1000 | Loss: 0.00001585
Iteration 17/1000 | Loss: 0.00001585
Iteration 18/1000 | Loss: 0.00001585
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001583
Iteration 21/1000 | Loss: 0.00001583
Iteration 22/1000 | Loss: 0.00001583
Iteration 23/1000 | Loss: 0.00001582
Iteration 24/1000 | Loss: 0.00001582
Iteration 25/1000 | Loss: 0.00001582
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00001581
Iteration 28/1000 | Loss: 0.00001581
Iteration 29/1000 | Loss: 0.00001581
Iteration 30/1000 | Loss: 0.00001581
Iteration 31/1000 | Loss: 0.00001581
Iteration 32/1000 | Loss: 0.00001581
Iteration 33/1000 | Loss: 0.00001580
Iteration 34/1000 | Loss: 0.00001580
Iteration 35/1000 | Loss: 0.00001580
Iteration 36/1000 | Loss: 0.00001580
Iteration 37/1000 | Loss: 0.00001580
Iteration 38/1000 | Loss: 0.00001580
Iteration 39/1000 | Loss: 0.00001579
Iteration 40/1000 | Loss: 0.00001579
Iteration 41/1000 | Loss: 0.00001578
Iteration 42/1000 | Loss: 0.00001578
Iteration 43/1000 | Loss: 0.00001578
Iteration 44/1000 | Loss: 0.00001578
Iteration 45/1000 | Loss: 0.00001578
Iteration 46/1000 | Loss: 0.00001578
Iteration 47/1000 | Loss: 0.00001578
Iteration 48/1000 | Loss: 0.00001578
Iteration 49/1000 | Loss: 0.00001578
Iteration 50/1000 | Loss: 0.00001578
Iteration 51/1000 | Loss: 0.00001577
Iteration 52/1000 | Loss: 0.00001577
Iteration 53/1000 | Loss: 0.00001577
Iteration 54/1000 | Loss: 0.00001577
Iteration 55/1000 | Loss: 0.00001577
Iteration 56/1000 | Loss: 0.00001577
Iteration 57/1000 | Loss: 0.00001577
Iteration 58/1000 | Loss: 0.00001576
Iteration 59/1000 | Loss: 0.00001576
Iteration 60/1000 | Loss: 0.00001576
Iteration 61/1000 | Loss: 0.00001576
Iteration 62/1000 | Loss: 0.00001576
Iteration 63/1000 | Loss: 0.00001576
Iteration 64/1000 | Loss: 0.00001576
Iteration 65/1000 | Loss: 0.00001576
Iteration 66/1000 | Loss: 0.00001575
Iteration 67/1000 | Loss: 0.00001575
Iteration 68/1000 | Loss: 0.00001575
Iteration 69/1000 | Loss: 0.00001575
Iteration 70/1000 | Loss: 0.00001575
Iteration 71/1000 | Loss: 0.00001575
Iteration 72/1000 | Loss: 0.00001575
Iteration 73/1000 | Loss: 0.00001575
Iteration 74/1000 | Loss: 0.00001575
Iteration 75/1000 | Loss: 0.00001575
Iteration 76/1000 | Loss: 0.00001574
Iteration 77/1000 | Loss: 0.00001574
Iteration 78/1000 | Loss: 0.00001573
Iteration 79/1000 | Loss: 0.00001573
Iteration 80/1000 | Loss: 0.00001572
Iteration 81/1000 | Loss: 0.00001571
Iteration 82/1000 | Loss: 0.00001571
Iteration 83/1000 | Loss: 0.00001571
Iteration 84/1000 | Loss: 0.00001571
Iteration 85/1000 | Loss: 0.00001571
Iteration 86/1000 | Loss: 0.00001571
Iteration 87/1000 | Loss: 0.00001571
Iteration 88/1000 | Loss: 0.00001571
Iteration 89/1000 | Loss: 0.00001571
Iteration 90/1000 | Loss: 0.00001571
Iteration 91/1000 | Loss: 0.00001570
Iteration 92/1000 | Loss: 0.00001570
Iteration 93/1000 | Loss: 0.00001570
Iteration 94/1000 | Loss: 0.00001569
Iteration 95/1000 | Loss: 0.00001569
Iteration 96/1000 | Loss: 0.00001569
Iteration 97/1000 | Loss: 0.00001569
Iteration 98/1000 | Loss: 0.00001569
Iteration 99/1000 | Loss: 0.00001568
Iteration 100/1000 | Loss: 0.00001568
Iteration 101/1000 | Loss: 0.00001568
Iteration 102/1000 | Loss: 0.00001568
Iteration 103/1000 | Loss: 0.00001568
Iteration 104/1000 | Loss: 0.00001567
Iteration 105/1000 | Loss: 0.00001567
Iteration 106/1000 | Loss: 0.00001567
Iteration 107/1000 | Loss: 0.00001567
Iteration 108/1000 | Loss: 0.00001567
Iteration 109/1000 | Loss: 0.00001566
Iteration 110/1000 | Loss: 0.00001566
Iteration 111/1000 | Loss: 0.00001566
Iteration 112/1000 | Loss: 0.00001566
Iteration 113/1000 | Loss: 0.00001566
Iteration 114/1000 | Loss: 0.00001566
Iteration 115/1000 | Loss: 0.00001566
Iteration 116/1000 | Loss: 0.00001565
Iteration 117/1000 | Loss: 0.00001565
Iteration 118/1000 | Loss: 0.00001565
Iteration 119/1000 | Loss: 0.00001565
Iteration 120/1000 | Loss: 0.00001565
Iteration 121/1000 | Loss: 0.00001565
Iteration 122/1000 | Loss: 0.00001565
Iteration 123/1000 | Loss: 0.00001565
Iteration 124/1000 | Loss: 0.00001565
Iteration 125/1000 | Loss: 0.00001565
Iteration 126/1000 | Loss: 0.00001565
Iteration 127/1000 | Loss: 0.00001565
Iteration 128/1000 | Loss: 0.00001564
Iteration 129/1000 | Loss: 0.00001564
Iteration 130/1000 | Loss: 0.00001564
Iteration 131/1000 | Loss: 0.00001563
Iteration 132/1000 | Loss: 0.00001563
Iteration 133/1000 | Loss: 0.00001563
Iteration 134/1000 | Loss: 0.00001563
Iteration 135/1000 | Loss: 0.00001563
Iteration 136/1000 | Loss: 0.00001563
Iteration 137/1000 | Loss: 0.00001563
Iteration 138/1000 | Loss: 0.00001563
Iteration 139/1000 | Loss: 0.00001563
Iteration 140/1000 | Loss: 0.00001563
Iteration 141/1000 | Loss: 0.00001563
Iteration 142/1000 | Loss: 0.00001563
Iteration 143/1000 | Loss: 0.00001562
Iteration 144/1000 | Loss: 0.00001562
Iteration 145/1000 | Loss: 0.00001562
Iteration 146/1000 | Loss: 0.00001562
Iteration 147/1000 | Loss: 0.00001562
Iteration 148/1000 | Loss: 0.00001562
Iteration 149/1000 | Loss: 0.00001562
Iteration 150/1000 | Loss: 0.00001561
Iteration 151/1000 | Loss: 0.00001561
Iteration 152/1000 | Loss: 0.00001561
Iteration 153/1000 | Loss: 0.00001561
Iteration 154/1000 | Loss: 0.00001561
Iteration 155/1000 | Loss: 0.00001561
Iteration 156/1000 | Loss: 0.00001561
Iteration 157/1000 | Loss: 0.00001561
Iteration 158/1000 | Loss: 0.00001561
Iteration 159/1000 | Loss: 0.00001561
Iteration 160/1000 | Loss: 0.00001561
Iteration 161/1000 | Loss: 0.00001561
Iteration 162/1000 | Loss: 0.00001561
Iteration 163/1000 | Loss: 0.00001561
Iteration 164/1000 | Loss: 0.00001561
Iteration 165/1000 | Loss: 0.00001560
Iteration 166/1000 | Loss: 0.00001560
Iteration 167/1000 | Loss: 0.00001560
Iteration 168/1000 | Loss: 0.00001560
Iteration 169/1000 | Loss: 0.00001560
Iteration 170/1000 | Loss: 0.00001560
Iteration 171/1000 | Loss: 0.00001560
Iteration 172/1000 | Loss: 0.00001560
Iteration 173/1000 | Loss: 0.00001560
Iteration 174/1000 | Loss: 0.00001560
Iteration 175/1000 | Loss: 0.00001560
Iteration 176/1000 | Loss: 0.00001560
Iteration 177/1000 | Loss: 0.00001559
Iteration 178/1000 | Loss: 0.00001559
Iteration 179/1000 | Loss: 0.00001559
Iteration 180/1000 | Loss: 0.00001559
Iteration 181/1000 | Loss: 0.00001559
Iteration 182/1000 | Loss: 0.00001559
Iteration 183/1000 | Loss: 0.00001559
Iteration 184/1000 | Loss: 0.00001559
Iteration 185/1000 | Loss: 0.00001559
Iteration 186/1000 | Loss: 0.00001559
Iteration 187/1000 | Loss: 0.00001558
Iteration 188/1000 | Loss: 0.00001558
Iteration 189/1000 | Loss: 0.00001558
Iteration 190/1000 | Loss: 0.00001558
Iteration 191/1000 | Loss: 0.00001558
Iteration 192/1000 | Loss: 0.00001558
Iteration 193/1000 | Loss: 0.00001558
Iteration 194/1000 | Loss: 0.00001558
Iteration 195/1000 | Loss: 0.00001558
Iteration 196/1000 | Loss: 0.00001558
Iteration 197/1000 | Loss: 0.00001558
Iteration 198/1000 | Loss: 0.00001558
Iteration 199/1000 | Loss: 0.00001558
Iteration 200/1000 | Loss: 0.00001558
Iteration 201/1000 | Loss: 0.00001558
Iteration 202/1000 | Loss: 0.00001558
Iteration 203/1000 | Loss: 0.00001558
Iteration 204/1000 | Loss: 0.00001558
Iteration 205/1000 | Loss: 0.00001557
Iteration 206/1000 | Loss: 0.00001557
Iteration 207/1000 | Loss: 0.00001557
Iteration 208/1000 | Loss: 0.00001557
Iteration 209/1000 | Loss: 0.00001557
Iteration 210/1000 | Loss: 0.00001557
Iteration 211/1000 | Loss: 0.00001557
Iteration 212/1000 | Loss: 0.00001557
Iteration 213/1000 | Loss: 0.00001557
Iteration 214/1000 | Loss: 0.00001557
Iteration 215/1000 | Loss: 0.00001557
Iteration 216/1000 | Loss: 0.00001557
Iteration 217/1000 | Loss: 0.00001557
Iteration 218/1000 | Loss: 0.00001557
Iteration 219/1000 | Loss: 0.00001557
Iteration 220/1000 | Loss: 0.00001557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.5567564332741313e-05, 1.5567564332741313e-05, 1.5567564332741313e-05, 1.5567564332741313e-05, 1.5567564332741313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5567564332741313e-05

Optimization complete. Final v2v error: 3.381803512573242 mm

Highest mean error: 3.978332042694092 mm for frame 1

Lowest mean error: 3.019334554672241 mm for frame 252

Saving results

Total time: 45.042885541915894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01131315
Iteration 2/25 | Loss: 0.00172753
Iteration 3/25 | Loss: 0.00099518
Iteration 4/25 | Loss: 0.00088400
Iteration 5/25 | Loss: 0.00087398
Iteration 6/25 | Loss: 0.00084833
Iteration 7/25 | Loss: 0.00080943
Iteration 8/25 | Loss: 0.00078775
Iteration 9/25 | Loss: 0.00078791
Iteration 10/25 | Loss: 0.00077563
Iteration 11/25 | Loss: 0.00077169
Iteration 12/25 | Loss: 0.00077065
Iteration 13/25 | Loss: 0.00076581
Iteration 14/25 | Loss: 0.00076104
Iteration 15/25 | Loss: 0.00076233
Iteration 16/25 | Loss: 0.00076277
Iteration 17/25 | Loss: 0.00075927
Iteration 18/25 | Loss: 0.00076141
Iteration 19/25 | Loss: 0.00075876
Iteration 20/25 | Loss: 0.00075528
Iteration 21/25 | Loss: 0.00075781
Iteration 22/25 | Loss: 0.00075765
Iteration 23/25 | Loss: 0.00075396
Iteration 24/25 | Loss: 0.00075854
Iteration 25/25 | Loss: 0.00075969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.14163399
Iteration 2/25 | Loss: 0.00039263
Iteration 3/25 | Loss: 0.00029178
Iteration 4/25 | Loss: 0.00029178
Iteration 5/25 | Loss: 0.00029178
Iteration 6/25 | Loss: 0.00029178
Iteration 7/25 | Loss: 0.00029177
Iteration 8/25 | Loss: 0.00029177
Iteration 9/25 | Loss: 0.00029177
Iteration 10/25 | Loss: 0.00029177
Iteration 11/25 | Loss: 0.00029177
Iteration 12/25 | Loss: 0.00029177
Iteration 13/25 | Loss: 0.00029177
Iteration 14/25 | Loss: 0.00029177
Iteration 15/25 | Loss: 0.00029177
Iteration 16/25 | Loss: 0.00029177
Iteration 17/25 | Loss: 0.00029177
Iteration 18/25 | Loss: 0.00029177
Iteration 19/25 | Loss: 0.00029177
Iteration 20/25 | Loss: 0.00029177
Iteration 21/25 | Loss: 0.00029177
Iteration 22/25 | Loss: 0.00029177
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002917738747783005, 0.0002917738747783005, 0.0002917738747783005, 0.0002917738747783005, 0.0002917738747783005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002917738747783005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029177
Iteration 2/1000 | Loss: 0.00013634
Iteration 3/1000 | Loss: 0.00015777
Iteration 4/1000 | Loss: 0.00017562
Iteration 5/1000 | Loss: 0.00012730
Iteration 6/1000 | Loss: 0.00008720
Iteration 7/1000 | Loss: 0.00016687
Iteration 8/1000 | Loss: 0.00021453
Iteration 9/1000 | Loss: 0.00020350
Iteration 10/1000 | Loss: 0.00006906
Iteration 11/1000 | Loss: 0.00009458
Iteration 12/1000 | Loss: 0.00028605
Iteration 13/1000 | Loss: 0.00022653
Iteration 14/1000 | Loss: 0.00029036
Iteration 15/1000 | Loss: 0.00025292
Iteration 16/1000 | Loss: 0.00024146
Iteration 17/1000 | Loss: 0.00025330
Iteration 18/1000 | Loss: 0.00013587
Iteration 19/1000 | Loss: 0.00011463
Iteration 20/1000 | Loss: 0.00025550
Iteration 21/1000 | Loss: 0.00012497
Iteration 22/1000 | Loss: 0.00006071
Iteration 23/1000 | Loss: 0.00017058
Iteration 24/1000 | Loss: 0.00009787
Iteration 25/1000 | Loss: 0.00017206
Iteration 26/1000 | Loss: 0.00011669
Iteration 27/1000 | Loss: 0.00011702
Iteration 28/1000 | Loss: 0.00014341
Iteration 29/1000 | Loss: 0.00011889
Iteration 30/1000 | Loss: 0.00028312
Iteration 31/1000 | Loss: 0.00021112
Iteration 32/1000 | Loss: 0.00027387
Iteration 33/1000 | Loss: 0.00032343
Iteration 34/1000 | Loss: 0.00027146
Iteration 35/1000 | Loss: 0.00023034
Iteration 36/1000 | Loss: 0.00023742
Iteration 37/1000 | Loss: 0.00013288
Iteration 38/1000 | Loss: 0.00013946
Iteration 39/1000 | Loss: 0.00010938
Iteration 40/1000 | Loss: 0.00015456
Iteration 41/1000 | Loss: 0.00011280
Iteration 42/1000 | Loss: 0.00008617
Iteration 43/1000 | Loss: 0.00008221
Iteration 44/1000 | Loss: 0.00005360
Iteration 45/1000 | Loss: 0.00002605
Iteration 46/1000 | Loss: 0.00015153
Iteration 47/1000 | Loss: 0.00015478
Iteration 48/1000 | Loss: 0.00016369
Iteration 49/1000 | Loss: 0.00012936
Iteration 50/1000 | Loss: 0.00014139
Iteration 51/1000 | Loss: 0.00009963
Iteration 52/1000 | Loss: 0.00007673
Iteration 53/1000 | Loss: 0.00009836
Iteration 54/1000 | Loss: 0.00016073
Iteration 55/1000 | Loss: 0.00005544
Iteration 56/1000 | Loss: 0.00010578
Iteration 57/1000 | Loss: 0.00026574
Iteration 58/1000 | Loss: 0.00041458
Iteration 59/1000 | Loss: 0.00007836
Iteration 60/1000 | Loss: 0.00002591
Iteration 61/1000 | Loss: 0.00002713
Iteration 62/1000 | Loss: 0.00012795
Iteration 63/1000 | Loss: 0.00010469
Iteration 64/1000 | Loss: 0.00019622
Iteration 65/1000 | Loss: 0.00014227
Iteration 66/1000 | Loss: 0.00003190
Iteration 67/1000 | Loss: 0.00002626
Iteration 68/1000 | Loss: 0.00002479
Iteration 69/1000 | Loss: 0.00015770
Iteration 70/1000 | Loss: 0.00014626
Iteration 71/1000 | Loss: 0.00002612
Iteration 72/1000 | Loss: 0.00002841
Iteration 73/1000 | Loss: 0.00007318
Iteration 74/1000 | Loss: 0.00002562
Iteration 75/1000 | Loss: 0.00002498
Iteration 76/1000 | Loss: 0.00002442
Iteration 77/1000 | Loss: 0.00016449
Iteration 78/1000 | Loss: 0.00016254
Iteration 79/1000 | Loss: 0.00004988
Iteration 80/1000 | Loss: 0.00004711
Iteration 81/1000 | Loss: 0.00027609
Iteration 82/1000 | Loss: 0.00009673
Iteration 83/1000 | Loss: 0.00008610
Iteration 84/1000 | Loss: 0.00003072
Iteration 85/1000 | Loss: 0.00003638
Iteration 86/1000 | Loss: 0.00007260
Iteration 87/1000 | Loss: 0.00002462
Iteration 88/1000 | Loss: 0.00017663
Iteration 89/1000 | Loss: 0.00022902
Iteration 90/1000 | Loss: 0.00033764
Iteration 91/1000 | Loss: 0.00021414
Iteration 92/1000 | Loss: 0.00029756
Iteration 93/1000 | Loss: 0.00019068
Iteration 94/1000 | Loss: 0.00025359
Iteration 95/1000 | Loss: 0.00029935
Iteration 96/1000 | Loss: 0.00022399
Iteration 97/1000 | Loss: 0.00018306
Iteration 98/1000 | Loss: 0.00019948
Iteration 99/1000 | Loss: 0.00017293
Iteration 100/1000 | Loss: 0.00017967
Iteration 101/1000 | Loss: 0.00032067
Iteration 102/1000 | Loss: 0.00018788
Iteration 103/1000 | Loss: 0.00029909
Iteration 104/1000 | Loss: 0.00012378
Iteration 105/1000 | Loss: 0.00023395
Iteration 106/1000 | Loss: 0.00014316
Iteration 107/1000 | Loss: 0.00017540
Iteration 108/1000 | Loss: 0.00018192
Iteration 109/1000 | Loss: 0.00017089
Iteration 110/1000 | Loss: 0.00035025
Iteration 111/1000 | Loss: 0.00014040
Iteration 112/1000 | Loss: 0.00019728
Iteration 113/1000 | Loss: 0.00012262
Iteration 114/1000 | Loss: 0.00007911
Iteration 115/1000 | Loss: 0.00008112
Iteration 116/1000 | Loss: 0.00008255
Iteration 117/1000 | Loss: 0.00006186
Iteration 118/1000 | Loss: 0.00007536
Iteration 119/1000 | Loss: 0.00003600
Iteration 120/1000 | Loss: 0.00007863
Iteration 121/1000 | Loss: 0.00007778
Iteration 122/1000 | Loss: 0.00008888
Iteration 123/1000 | Loss: 0.00008252
Iteration 124/1000 | Loss: 0.00010464
Iteration 125/1000 | Loss: 0.00005416
Iteration 126/1000 | Loss: 0.00007233
Iteration 127/1000 | Loss: 0.00009077
Iteration 128/1000 | Loss: 0.00019957
Iteration 129/1000 | Loss: 0.00010955
Iteration 130/1000 | Loss: 0.00008575
Iteration 131/1000 | Loss: 0.00013510
Iteration 132/1000 | Loss: 0.00009658
Iteration 133/1000 | Loss: 0.00004617
Iteration 134/1000 | Loss: 0.00003083
Iteration 135/1000 | Loss: 0.00005231
Iteration 136/1000 | Loss: 0.00022718
Iteration 137/1000 | Loss: 0.00020013
Iteration 138/1000 | Loss: 0.00004128
Iteration 139/1000 | Loss: 0.00025004
Iteration 140/1000 | Loss: 0.00003516
Iteration 141/1000 | Loss: 0.00006022
Iteration 142/1000 | Loss: 0.00002862
Iteration 143/1000 | Loss: 0.00002823
Iteration 144/1000 | Loss: 0.00002909
Iteration 145/1000 | Loss: 0.00002599
Iteration 146/1000 | Loss: 0.00003468
Iteration 147/1000 | Loss: 0.00008137
Iteration 148/1000 | Loss: 0.00002575
Iteration 149/1000 | Loss: 0.00002610
Iteration 150/1000 | Loss: 0.00002692
Iteration 151/1000 | Loss: 0.00002384
Iteration 152/1000 | Loss: 0.00002375
Iteration 153/1000 | Loss: 0.00003351
Iteration 154/1000 | Loss: 0.00002472
Iteration 155/1000 | Loss: 0.00002383
Iteration 156/1000 | Loss: 0.00002373
Iteration 157/1000 | Loss: 0.00002351
Iteration 158/1000 | Loss: 0.00002351
Iteration 159/1000 | Loss: 0.00002351
Iteration 160/1000 | Loss: 0.00002351
Iteration 161/1000 | Loss: 0.00002351
Iteration 162/1000 | Loss: 0.00002351
Iteration 163/1000 | Loss: 0.00002351
Iteration 164/1000 | Loss: 0.00002351
Iteration 165/1000 | Loss: 0.00002351
Iteration 166/1000 | Loss: 0.00002351
Iteration 167/1000 | Loss: 0.00002350
Iteration 168/1000 | Loss: 0.00002350
Iteration 169/1000 | Loss: 0.00002350
Iteration 170/1000 | Loss: 0.00002350
Iteration 171/1000 | Loss: 0.00002350
Iteration 172/1000 | Loss: 0.00002349
Iteration 173/1000 | Loss: 0.00002349
Iteration 174/1000 | Loss: 0.00002348
Iteration 175/1000 | Loss: 0.00002347
Iteration 176/1000 | Loss: 0.00002347
Iteration 177/1000 | Loss: 0.00002347
Iteration 178/1000 | Loss: 0.00002345
Iteration 179/1000 | Loss: 0.00002345
Iteration 180/1000 | Loss: 0.00002344
Iteration 181/1000 | Loss: 0.00002344
Iteration 182/1000 | Loss: 0.00002343
Iteration 183/1000 | Loss: 0.00002342
Iteration 184/1000 | Loss: 0.00002341
Iteration 185/1000 | Loss: 0.00002340
Iteration 186/1000 | Loss: 0.00004261
Iteration 187/1000 | Loss: 0.00002346
Iteration 188/1000 | Loss: 0.00002338
Iteration 189/1000 | Loss: 0.00002337
Iteration 190/1000 | Loss: 0.00002337
Iteration 191/1000 | Loss: 0.00002337
Iteration 192/1000 | Loss: 0.00002336
Iteration 193/1000 | Loss: 0.00002336
Iteration 194/1000 | Loss: 0.00002336
Iteration 195/1000 | Loss: 0.00002335
Iteration 196/1000 | Loss: 0.00002335
Iteration 197/1000 | Loss: 0.00002335
Iteration 198/1000 | Loss: 0.00002335
Iteration 199/1000 | Loss: 0.00002335
Iteration 200/1000 | Loss: 0.00002335
Iteration 201/1000 | Loss: 0.00002334
Iteration 202/1000 | Loss: 0.00002334
Iteration 203/1000 | Loss: 0.00002334
Iteration 204/1000 | Loss: 0.00002334
Iteration 205/1000 | Loss: 0.00002334
Iteration 206/1000 | Loss: 0.00002334
Iteration 207/1000 | Loss: 0.00002333
Iteration 208/1000 | Loss: 0.00002333
Iteration 209/1000 | Loss: 0.00002333
Iteration 210/1000 | Loss: 0.00002333
Iteration 211/1000 | Loss: 0.00002333
Iteration 212/1000 | Loss: 0.00002333
Iteration 213/1000 | Loss: 0.00002333
Iteration 214/1000 | Loss: 0.00002333
Iteration 215/1000 | Loss: 0.00002333
Iteration 216/1000 | Loss: 0.00002333
Iteration 217/1000 | Loss: 0.00002333
Iteration 218/1000 | Loss: 0.00002333
Iteration 219/1000 | Loss: 0.00002333
Iteration 220/1000 | Loss: 0.00002333
Iteration 221/1000 | Loss: 0.00002333
Iteration 222/1000 | Loss: 0.00002333
Iteration 223/1000 | Loss: 0.00002333
Iteration 224/1000 | Loss: 0.00002333
Iteration 225/1000 | Loss: 0.00002333
Iteration 226/1000 | Loss: 0.00002333
Iteration 227/1000 | Loss: 0.00002332
Iteration 228/1000 | Loss: 0.00002332
Iteration 229/1000 | Loss: 0.00002332
Iteration 230/1000 | Loss: 0.00002332
Iteration 231/1000 | Loss: 0.00002332
Iteration 232/1000 | Loss: 0.00002332
Iteration 233/1000 | Loss: 0.00002332
Iteration 234/1000 | Loss: 0.00002332
Iteration 235/1000 | Loss: 0.00002332
Iteration 236/1000 | Loss: 0.00002332
Iteration 237/1000 | Loss: 0.00002331
Iteration 238/1000 | Loss: 0.00002331
Iteration 239/1000 | Loss: 0.00002331
Iteration 240/1000 | Loss: 0.00002331
Iteration 241/1000 | Loss: 0.00002331
Iteration 242/1000 | Loss: 0.00002331
Iteration 243/1000 | Loss: 0.00002331
Iteration 244/1000 | Loss: 0.00002331
Iteration 245/1000 | Loss: 0.00002331
Iteration 246/1000 | Loss: 0.00002331
Iteration 247/1000 | Loss: 0.00002331
Iteration 248/1000 | Loss: 0.00002331
Iteration 249/1000 | Loss: 0.00002331
Iteration 250/1000 | Loss: 0.00002331
Iteration 251/1000 | Loss: 0.00002331
Iteration 252/1000 | Loss: 0.00002331
Iteration 253/1000 | Loss: 0.00002331
Iteration 254/1000 | Loss: 0.00002331
Iteration 255/1000 | Loss: 0.00002331
Iteration 256/1000 | Loss: 0.00002331
Iteration 257/1000 | Loss: 0.00002331
Iteration 258/1000 | Loss: 0.00002331
Iteration 259/1000 | Loss: 0.00002331
Iteration 260/1000 | Loss: 0.00002331
Iteration 261/1000 | Loss: 0.00002331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [2.331355062779039e-05, 2.331355062779039e-05, 2.331355062779039e-05, 2.331355062779039e-05, 2.331355062779039e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.331355062779039e-05

Optimization complete. Final v2v error: 4.105600357055664 mm

Highest mean error: 5.907118797302246 mm for frame 182

Lowest mean error: 3.68440318107605 mm for frame 113

Saving results

Total time: 312.004474401474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00670353
Iteration 2/25 | Loss: 0.00128256
Iteration 3/25 | Loss: 0.00079352
Iteration 4/25 | Loss: 0.00075342
Iteration 5/25 | Loss: 0.00074525
Iteration 6/25 | Loss: 0.00074419
Iteration 7/25 | Loss: 0.00074413
Iteration 8/25 | Loss: 0.00074413
Iteration 9/25 | Loss: 0.00074413
Iteration 10/25 | Loss: 0.00074413
Iteration 11/25 | Loss: 0.00074413
Iteration 12/25 | Loss: 0.00074413
Iteration 13/25 | Loss: 0.00074413
Iteration 14/25 | Loss: 0.00074413
Iteration 15/25 | Loss: 0.00074413
Iteration 16/25 | Loss: 0.00074413
Iteration 17/25 | Loss: 0.00074413
Iteration 18/25 | Loss: 0.00074413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007441345951519907, 0.0007441345951519907, 0.0007441345951519907, 0.0007441345951519907, 0.0007441345951519907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007441345951519907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46900666
Iteration 2/25 | Loss: 0.00010146
Iteration 3/25 | Loss: 0.00010143
Iteration 4/25 | Loss: 0.00010143
Iteration 5/25 | Loss: 0.00010143
Iteration 6/25 | Loss: 0.00010143
Iteration 7/25 | Loss: 0.00010143
Iteration 8/25 | Loss: 0.00010143
Iteration 9/25 | Loss: 0.00010143
Iteration 10/25 | Loss: 0.00010143
Iteration 11/25 | Loss: 0.00010143
Iteration 12/25 | Loss: 0.00010143
Iteration 13/25 | Loss: 0.00010143
Iteration 14/25 | Loss: 0.00010143
Iteration 15/25 | Loss: 0.00010143
Iteration 16/25 | Loss: 0.00010143
Iteration 17/25 | Loss: 0.00010143
Iteration 18/25 | Loss: 0.00010143
Iteration 19/25 | Loss: 0.00010143
Iteration 20/25 | Loss: 0.00010143
Iteration 21/25 | Loss: 0.00010143
Iteration 22/25 | Loss: 0.00010143
Iteration 23/25 | Loss: 0.00010143
Iteration 24/25 | Loss: 0.00010143
Iteration 25/25 | Loss: 0.00010143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00010143
Iteration 2/1000 | Loss: 0.00002598
Iteration 3/1000 | Loss: 0.00002305
Iteration 4/1000 | Loss: 0.00002212
Iteration 5/1000 | Loss: 0.00002162
Iteration 6/1000 | Loss: 0.00002134
Iteration 7/1000 | Loss: 0.00002097
Iteration 8/1000 | Loss: 0.00002076
Iteration 9/1000 | Loss: 0.00002074
Iteration 10/1000 | Loss: 0.00002064
Iteration 11/1000 | Loss: 0.00002063
Iteration 12/1000 | Loss: 0.00002062
Iteration 13/1000 | Loss: 0.00002061
Iteration 14/1000 | Loss: 0.00002058
Iteration 15/1000 | Loss: 0.00002057
Iteration 16/1000 | Loss: 0.00002056
Iteration 17/1000 | Loss: 0.00002056
Iteration 18/1000 | Loss: 0.00002056
Iteration 19/1000 | Loss: 0.00002055
Iteration 20/1000 | Loss: 0.00002055
Iteration 21/1000 | Loss: 0.00002055
Iteration 22/1000 | Loss: 0.00002055
Iteration 23/1000 | Loss: 0.00002055
Iteration 24/1000 | Loss: 0.00002055
Iteration 25/1000 | Loss: 0.00002055
Iteration 26/1000 | Loss: 0.00002055
Iteration 27/1000 | Loss: 0.00002055
Iteration 28/1000 | Loss: 0.00002055
Iteration 29/1000 | Loss: 0.00002055
Iteration 30/1000 | Loss: 0.00002055
Iteration 31/1000 | Loss: 0.00002055
Iteration 32/1000 | Loss: 0.00002055
Iteration 33/1000 | Loss: 0.00002055
Iteration 34/1000 | Loss: 0.00002055
Iteration 35/1000 | Loss: 0.00002055
Iteration 36/1000 | Loss: 0.00002055
Iteration 37/1000 | Loss: 0.00002055
Iteration 38/1000 | Loss: 0.00002055
Iteration 39/1000 | Loss: 0.00002055
Iteration 40/1000 | Loss: 0.00002055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 40. Stopping optimization.
Last 5 losses: [2.0545436200336553e-05, 2.0545436200336553e-05, 2.0545436200336553e-05, 2.0545436200336553e-05, 2.0545436200336553e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0545436200336553e-05

Optimization complete. Final v2v error: 3.7430107593536377 mm

Highest mean error: 4.04814338684082 mm for frame 176

Lowest mean error: 3.4175102710723877 mm for frame 109

Saving results

Total time: 25.18801212310791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01151444
Iteration 2/25 | Loss: 0.00328405
Iteration 3/25 | Loss: 0.00208347
Iteration 4/25 | Loss: 0.00190363
Iteration 5/25 | Loss: 0.00246007
Iteration 6/25 | Loss: 0.00217892
Iteration 7/25 | Loss: 0.00139095
Iteration 8/25 | Loss: 0.00109177
Iteration 9/25 | Loss: 0.00102110
Iteration 10/25 | Loss: 0.00099503
Iteration 11/25 | Loss: 0.00098203
Iteration 12/25 | Loss: 0.00097757
Iteration 13/25 | Loss: 0.00097589
Iteration 14/25 | Loss: 0.00097457
Iteration 15/25 | Loss: 0.00097402
Iteration 16/25 | Loss: 0.00097387
Iteration 17/25 | Loss: 0.00097385
Iteration 18/25 | Loss: 0.00097385
Iteration 19/25 | Loss: 0.00097384
Iteration 20/25 | Loss: 0.00097384
Iteration 21/25 | Loss: 0.00097384
Iteration 22/25 | Loss: 0.00097384
Iteration 23/25 | Loss: 0.00097384
Iteration 24/25 | Loss: 0.00097384
Iteration 25/25 | Loss: 0.00097384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67847079
Iteration 2/25 | Loss: 0.00038594
Iteration 3/25 | Loss: 0.00038594
Iteration 4/25 | Loss: 0.00038594
Iteration 5/25 | Loss: 0.00038594
Iteration 6/25 | Loss: 0.00038594
Iteration 7/25 | Loss: 0.00038594
Iteration 8/25 | Loss: 0.00038594
Iteration 9/25 | Loss: 0.00038594
Iteration 10/25 | Loss: 0.00038594
Iteration 11/25 | Loss: 0.00038594
Iteration 12/25 | Loss: 0.00038594
Iteration 13/25 | Loss: 0.00038594
Iteration 14/25 | Loss: 0.00038594
Iteration 15/25 | Loss: 0.00038594
Iteration 16/25 | Loss: 0.00038594
Iteration 17/25 | Loss: 0.00038594
Iteration 18/25 | Loss: 0.00038594
Iteration 19/25 | Loss: 0.00038594
Iteration 20/25 | Loss: 0.00038594
Iteration 21/25 | Loss: 0.00038594
Iteration 22/25 | Loss: 0.00038594
Iteration 23/25 | Loss: 0.00038594
Iteration 24/25 | Loss: 0.00038594
Iteration 25/25 | Loss: 0.00038594

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038594
Iteration 2/1000 | Loss: 0.00008133
Iteration 3/1000 | Loss: 0.00006416
Iteration 4/1000 | Loss: 0.00005883
Iteration 5/1000 | Loss: 0.00005653
Iteration 6/1000 | Loss: 0.00005504
Iteration 7/1000 | Loss: 0.00005374
Iteration 8/1000 | Loss: 0.00005276
Iteration 9/1000 | Loss: 0.00005205
Iteration 10/1000 | Loss: 0.00005136
Iteration 11/1000 | Loss: 0.00005081
Iteration 12/1000 | Loss: 0.00005044
Iteration 13/1000 | Loss: 0.00005018
Iteration 14/1000 | Loss: 0.00005000
Iteration 15/1000 | Loss: 0.00004998
Iteration 16/1000 | Loss: 0.00004976
Iteration 17/1000 | Loss: 0.00004964
Iteration 18/1000 | Loss: 0.00004960
Iteration 19/1000 | Loss: 0.00004960
Iteration 20/1000 | Loss: 0.00004947
Iteration 21/1000 | Loss: 0.00004946
Iteration 22/1000 | Loss: 0.00004943
Iteration 23/1000 | Loss: 0.00004937
Iteration 24/1000 | Loss: 0.00004937
Iteration 25/1000 | Loss: 0.00004935
Iteration 26/1000 | Loss: 0.00004935
Iteration 27/1000 | Loss: 0.00004935
Iteration 28/1000 | Loss: 0.00004934
Iteration 29/1000 | Loss: 0.00004934
Iteration 30/1000 | Loss: 0.00004934
Iteration 31/1000 | Loss: 0.00004934
Iteration 32/1000 | Loss: 0.00004934
Iteration 33/1000 | Loss: 0.00004933
Iteration 34/1000 | Loss: 0.00004929
Iteration 35/1000 | Loss: 0.00004929
Iteration 36/1000 | Loss: 0.00004929
Iteration 37/1000 | Loss: 0.00004929
Iteration 38/1000 | Loss: 0.00004928
Iteration 39/1000 | Loss: 0.00004928
Iteration 40/1000 | Loss: 0.00004928
Iteration 41/1000 | Loss: 0.00004928
Iteration 42/1000 | Loss: 0.00004928
Iteration 43/1000 | Loss: 0.00004927
Iteration 44/1000 | Loss: 0.00004927
Iteration 45/1000 | Loss: 0.00004927
Iteration 46/1000 | Loss: 0.00004927
Iteration 47/1000 | Loss: 0.00004927
Iteration 48/1000 | Loss: 0.00004927
Iteration 49/1000 | Loss: 0.00004926
Iteration 50/1000 | Loss: 0.00004926
Iteration 51/1000 | Loss: 0.00004926
Iteration 52/1000 | Loss: 0.00004926
Iteration 53/1000 | Loss: 0.00004926
Iteration 54/1000 | Loss: 0.00004926
Iteration 55/1000 | Loss: 0.00004926
Iteration 56/1000 | Loss: 0.00004926
Iteration 57/1000 | Loss: 0.00004925
Iteration 58/1000 | Loss: 0.00004925
Iteration 59/1000 | Loss: 0.00004925
Iteration 60/1000 | Loss: 0.00004925
Iteration 61/1000 | Loss: 0.00004925
Iteration 62/1000 | Loss: 0.00004925
Iteration 63/1000 | Loss: 0.00004925
Iteration 64/1000 | Loss: 0.00004925
Iteration 65/1000 | Loss: 0.00004925
Iteration 66/1000 | Loss: 0.00004925
Iteration 67/1000 | Loss: 0.00004925
Iteration 68/1000 | Loss: 0.00004924
Iteration 69/1000 | Loss: 0.00004924
Iteration 70/1000 | Loss: 0.00004923
Iteration 71/1000 | Loss: 0.00004923
Iteration 72/1000 | Loss: 0.00004923
Iteration 73/1000 | Loss: 0.00004922
Iteration 74/1000 | Loss: 0.00004921
Iteration 75/1000 | Loss: 0.00004921
Iteration 76/1000 | Loss: 0.00004921
Iteration 77/1000 | Loss: 0.00004921
Iteration 78/1000 | Loss: 0.00004920
Iteration 79/1000 | Loss: 0.00004920
Iteration 80/1000 | Loss: 0.00004920
Iteration 81/1000 | Loss: 0.00004920
Iteration 82/1000 | Loss: 0.00004920
Iteration 83/1000 | Loss: 0.00004919
Iteration 84/1000 | Loss: 0.00004919
Iteration 85/1000 | Loss: 0.00004919
Iteration 86/1000 | Loss: 0.00004919
Iteration 87/1000 | Loss: 0.00004919
Iteration 88/1000 | Loss: 0.00004919
Iteration 89/1000 | Loss: 0.00004919
Iteration 90/1000 | Loss: 0.00004919
Iteration 91/1000 | Loss: 0.00004919
Iteration 92/1000 | Loss: 0.00004919
Iteration 93/1000 | Loss: 0.00004919
Iteration 94/1000 | Loss: 0.00004919
Iteration 95/1000 | Loss: 0.00004918
Iteration 96/1000 | Loss: 0.00004918
Iteration 97/1000 | Loss: 0.00004918
Iteration 98/1000 | Loss: 0.00004918
Iteration 99/1000 | Loss: 0.00004918
Iteration 100/1000 | Loss: 0.00004918
Iteration 101/1000 | Loss: 0.00004918
Iteration 102/1000 | Loss: 0.00004918
Iteration 103/1000 | Loss: 0.00004918
Iteration 104/1000 | Loss: 0.00004918
Iteration 105/1000 | Loss: 0.00004918
Iteration 106/1000 | Loss: 0.00004917
Iteration 107/1000 | Loss: 0.00004917
Iteration 108/1000 | Loss: 0.00004917
Iteration 109/1000 | Loss: 0.00004917
Iteration 110/1000 | Loss: 0.00004917
Iteration 111/1000 | Loss: 0.00004917
Iteration 112/1000 | Loss: 0.00004917
Iteration 113/1000 | Loss: 0.00004917
Iteration 114/1000 | Loss: 0.00004916
Iteration 115/1000 | Loss: 0.00004916
Iteration 116/1000 | Loss: 0.00004916
Iteration 117/1000 | Loss: 0.00004916
Iteration 118/1000 | Loss: 0.00004916
Iteration 119/1000 | Loss: 0.00004916
Iteration 120/1000 | Loss: 0.00004916
Iteration 121/1000 | Loss: 0.00004916
Iteration 122/1000 | Loss: 0.00004915
Iteration 123/1000 | Loss: 0.00004915
Iteration 124/1000 | Loss: 0.00004915
Iteration 125/1000 | Loss: 0.00004915
Iteration 126/1000 | Loss: 0.00004915
Iteration 127/1000 | Loss: 0.00004915
Iteration 128/1000 | Loss: 0.00004915
Iteration 129/1000 | Loss: 0.00004915
Iteration 130/1000 | Loss: 0.00004915
Iteration 131/1000 | Loss: 0.00004915
Iteration 132/1000 | Loss: 0.00004915
Iteration 133/1000 | Loss: 0.00004915
Iteration 134/1000 | Loss: 0.00004915
Iteration 135/1000 | Loss: 0.00004915
Iteration 136/1000 | Loss: 0.00004915
Iteration 137/1000 | Loss: 0.00004915
Iteration 138/1000 | Loss: 0.00004915
Iteration 139/1000 | Loss: 0.00004915
Iteration 140/1000 | Loss: 0.00004915
Iteration 141/1000 | Loss: 0.00004915
Iteration 142/1000 | Loss: 0.00004915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [4.914839519187808e-05, 4.914839519187808e-05, 4.914839519187808e-05, 4.914839519187808e-05, 4.914839519187808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.914839519187808e-05

Optimization complete. Final v2v error: 5.7126312255859375 mm

Highest mean error: 5.935338020324707 mm for frame 1

Lowest mean error: 5.482456207275391 mm for frame 243

Saving results

Total time: 72.34100842475891
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834073
Iteration 2/25 | Loss: 0.00089542
Iteration 3/25 | Loss: 0.00068032
Iteration 4/25 | Loss: 0.00064533
Iteration 5/25 | Loss: 0.00062893
Iteration 6/25 | Loss: 0.00062498
Iteration 7/25 | Loss: 0.00062387
Iteration 8/25 | Loss: 0.00062354
Iteration 9/25 | Loss: 0.00062354
Iteration 10/25 | Loss: 0.00062354
Iteration 11/25 | Loss: 0.00062354
Iteration 12/25 | Loss: 0.00062354
Iteration 13/25 | Loss: 0.00062354
Iteration 14/25 | Loss: 0.00062354
Iteration 15/25 | Loss: 0.00062354
Iteration 16/25 | Loss: 0.00062354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006235427572391927, 0.0006235427572391927, 0.0006235427572391927, 0.0006235427572391927, 0.0006235427572391927]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006235427572391927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.26271677
Iteration 2/25 | Loss: 0.00008073
Iteration 3/25 | Loss: 0.00008073
Iteration 4/25 | Loss: 0.00008073
Iteration 5/25 | Loss: 0.00008073
Iteration 6/25 | Loss: 0.00008073
Iteration 7/25 | Loss: 0.00008073
Iteration 8/25 | Loss: 0.00008073
Iteration 9/25 | Loss: 0.00008073
Iteration 10/25 | Loss: 0.00008073
Iteration 11/25 | Loss: 0.00008073
Iteration 12/25 | Loss: 0.00008073
Iteration 13/25 | Loss: 0.00008073
Iteration 14/25 | Loss: 0.00008073
Iteration 15/25 | Loss: 0.00008073
Iteration 16/25 | Loss: 0.00008073
Iteration 17/25 | Loss: 0.00008073
Iteration 18/25 | Loss: 0.00008073
Iteration 19/25 | Loss: 0.00008073
Iteration 20/25 | Loss: 0.00008073
Iteration 21/25 | Loss: 0.00008073
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [8.07286414783448e-05, 8.07286414783448e-05, 8.07286414783448e-05, 8.07286414783448e-05, 8.07286414783448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.07286414783448e-05

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00008073
Iteration 2/1000 | Loss: 0.00002060
Iteration 3/1000 | Loss: 0.00001864
Iteration 4/1000 | Loss: 0.00001771
Iteration 5/1000 | Loss: 0.00001742
Iteration 6/1000 | Loss: 0.00001707
Iteration 7/1000 | Loss: 0.00001702
Iteration 8/1000 | Loss: 0.00001671
Iteration 9/1000 | Loss: 0.00001661
Iteration 10/1000 | Loss: 0.00001656
Iteration 11/1000 | Loss: 0.00001656
Iteration 12/1000 | Loss: 0.00001655
Iteration 13/1000 | Loss: 0.00001655
Iteration 14/1000 | Loss: 0.00001655
Iteration 15/1000 | Loss: 0.00001655
Iteration 16/1000 | Loss: 0.00001655
Iteration 17/1000 | Loss: 0.00001655
Iteration 18/1000 | Loss: 0.00001655
Iteration 19/1000 | Loss: 0.00001655
Iteration 20/1000 | Loss: 0.00001654
Iteration 21/1000 | Loss: 0.00001654
Iteration 22/1000 | Loss: 0.00001653
Iteration 23/1000 | Loss: 0.00001653
Iteration 24/1000 | Loss: 0.00001646
Iteration 25/1000 | Loss: 0.00001646
Iteration 26/1000 | Loss: 0.00001646
Iteration 27/1000 | Loss: 0.00001646
Iteration 28/1000 | Loss: 0.00001646
Iteration 29/1000 | Loss: 0.00001645
Iteration 30/1000 | Loss: 0.00001645
Iteration 31/1000 | Loss: 0.00001645
Iteration 32/1000 | Loss: 0.00001645
Iteration 33/1000 | Loss: 0.00001641
Iteration 34/1000 | Loss: 0.00001641
Iteration 35/1000 | Loss: 0.00001641
Iteration 36/1000 | Loss: 0.00001640
Iteration 37/1000 | Loss: 0.00001640
Iteration 38/1000 | Loss: 0.00001640
Iteration 39/1000 | Loss: 0.00001640
Iteration 40/1000 | Loss: 0.00001638
Iteration 41/1000 | Loss: 0.00001637
Iteration 42/1000 | Loss: 0.00001637
Iteration 43/1000 | Loss: 0.00001637
Iteration 44/1000 | Loss: 0.00001637
Iteration 45/1000 | Loss: 0.00001637
Iteration 46/1000 | Loss: 0.00001636
Iteration 47/1000 | Loss: 0.00001636
Iteration 48/1000 | Loss: 0.00001636
Iteration 49/1000 | Loss: 0.00001636
Iteration 50/1000 | Loss: 0.00001635
Iteration 51/1000 | Loss: 0.00001635
Iteration 52/1000 | Loss: 0.00001635
Iteration 53/1000 | Loss: 0.00001634
Iteration 54/1000 | Loss: 0.00001634
Iteration 55/1000 | Loss: 0.00001634
Iteration 56/1000 | Loss: 0.00001634
Iteration 57/1000 | Loss: 0.00001634
Iteration 58/1000 | Loss: 0.00001634
Iteration 59/1000 | Loss: 0.00001634
Iteration 60/1000 | Loss: 0.00001634
Iteration 61/1000 | Loss: 0.00001633
Iteration 62/1000 | Loss: 0.00001633
Iteration 63/1000 | Loss: 0.00001633
Iteration 64/1000 | Loss: 0.00001633
Iteration 65/1000 | Loss: 0.00001633
Iteration 66/1000 | Loss: 0.00001633
Iteration 67/1000 | Loss: 0.00001633
Iteration 68/1000 | Loss: 0.00001633
Iteration 69/1000 | Loss: 0.00001632
Iteration 70/1000 | Loss: 0.00001632
Iteration 71/1000 | Loss: 0.00001632
Iteration 72/1000 | Loss: 0.00001632
Iteration 73/1000 | Loss: 0.00001632
Iteration 74/1000 | Loss: 0.00001632
Iteration 75/1000 | Loss: 0.00001632
Iteration 76/1000 | Loss: 0.00001631
Iteration 77/1000 | Loss: 0.00001631
Iteration 78/1000 | Loss: 0.00001631
Iteration 79/1000 | Loss: 0.00001631
Iteration 80/1000 | Loss: 0.00001631
Iteration 81/1000 | Loss: 0.00001631
Iteration 82/1000 | Loss: 0.00001630
Iteration 83/1000 | Loss: 0.00001630
Iteration 84/1000 | Loss: 0.00001630
Iteration 85/1000 | Loss: 0.00001630
Iteration 86/1000 | Loss: 0.00001630
Iteration 87/1000 | Loss: 0.00001630
Iteration 88/1000 | Loss: 0.00001629
Iteration 89/1000 | Loss: 0.00001629
Iteration 90/1000 | Loss: 0.00001629
Iteration 91/1000 | Loss: 0.00001629
Iteration 92/1000 | Loss: 0.00001629
Iteration 93/1000 | Loss: 0.00001629
Iteration 94/1000 | Loss: 0.00001629
Iteration 95/1000 | Loss: 0.00001629
Iteration 96/1000 | Loss: 0.00001629
Iteration 97/1000 | Loss: 0.00001629
Iteration 98/1000 | Loss: 0.00001629
Iteration 99/1000 | Loss: 0.00001629
Iteration 100/1000 | Loss: 0.00001629
Iteration 101/1000 | Loss: 0.00001629
Iteration 102/1000 | Loss: 0.00001629
Iteration 103/1000 | Loss: 0.00001629
Iteration 104/1000 | Loss: 0.00001628
Iteration 105/1000 | Loss: 0.00001628
Iteration 106/1000 | Loss: 0.00001628
Iteration 107/1000 | Loss: 0.00001628
Iteration 108/1000 | Loss: 0.00001628
Iteration 109/1000 | Loss: 0.00001628
Iteration 110/1000 | Loss: 0.00001628
Iteration 111/1000 | Loss: 0.00001628
Iteration 112/1000 | Loss: 0.00001627
Iteration 113/1000 | Loss: 0.00001627
Iteration 114/1000 | Loss: 0.00001627
Iteration 115/1000 | Loss: 0.00001627
Iteration 116/1000 | Loss: 0.00001627
Iteration 117/1000 | Loss: 0.00001627
Iteration 118/1000 | Loss: 0.00001627
Iteration 119/1000 | Loss: 0.00001627
Iteration 120/1000 | Loss: 0.00001627
Iteration 121/1000 | Loss: 0.00001627
Iteration 122/1000 | Loss: 0.00001627
Iteration 123/1000 | Loss: 0.00001627
Iteration 124/1000 | Loss: 0.00001627
Iteration 125/1000 | Loss: 0.00001626
Iteration 126/1000 | Loss: 0.00001626
Iteration 127/1000 | Loss: 0.00001626
Iteration 128/1000 | Loss: 0.00001626
Iteration 129/1000 | Loss: 0.00001626
Iteration 130/1000 | Loss: 0.00001626
Iteration 131/1000 | Loss: 0.00001626
Iteration 132/1000 | Loss: 0.00001626
Iteration 133/1000 | Loss: 0.00001626
Iteration 134/1000 | Loss: 0.00001626
Iteration 135/1000 | Loss: 0.00001625
Iteration 136/1000 | Loss: 0.00001625
Iteration 137/1000 | Loss: 0.00001625
Iteration 138/1000 | Loss: 0.00001625
Iteration 139/1000 | Loss: 0.00001625
Iteration 140/1000 | Loss: 0.00001625
Iteration 141/1000 | Loss: 0.00001625
Iteration 142/1000 | Loss: 0.00001625
Iteration 143/1000 | Loss: 0.00001625
Iteration 144/1000 | Loss: 0.00001625
Iteration 145/1000 | Loss: 0.00001625
Iteration 146/1000 | Loss: 0.00001625
Iteration 147/1000 | Loss: 0.00001625
Iteration 148/1000 | Loss: 0.00001625
Iteration 149/1000 | Loss: 0.00001624
Iteration 150/1000 | Loss: 0.00001624
Iteration 151/1000 | Loss: 0.00001624
Iteration 152/1000 | Loss: 0.00001624
Iteration 153/1000 | Loss: 0.00001624
Iteration 154/1000 | Loss: 0.00001624
Iteration 155/1000 | Loss: 0.00001624
Iteration 156/1000 | Loss: 0.00001624
Iteration 157/1000 | Loss: 0.00001624
Iteration 158/1000 | Loss: 0.00001624
Iteration 159/1000 | Loss: 0.00001624
Iteration 160/1000 | Loss: 0.00001624
Iteration 161/1000 | Loss: 0.00001624
Iteration 162/1000 | Loss: 0.00001624
Iteration 163/1000 | Loss: 0.00001624
Iteration 164/1000 | Loss: 0.00001624
Iteration 165/1000 | Loss: 0.00001624
Iteration 166/1000 | Loss: 0.00001624
Iteration 167/1000 | Loss: 0.00001624
Iteration 168/1000 | Loss: 0.00001624
Iteration 169/1000 | Loss: 0.00001624
Iteration 170/1000 | Loss: 0.00001624
Iteration 171/1000 | Loss: 0.00001624
Iteration 172/1000 | Loss: 0.00001624
Iteration 173/1000 | Loss: 0.00001624
Iteration 174/1000 | Loss: 0.00001624
Iteration 175/1000 | Loss: 0.00001624
Iteration 176/1000 | Loss: 0.00001624
Iteration 177/1000 | Loss: 0.00001624
Iteration 178/1000 | Loss: 0.00001624
Iteration 179/1000 | Loss: 0.00001624
Iteration 180/1000 | Loss: 0.00001624
Iteration 181/1000 | Loss: 0.00001624
Iteration 182/1000 | Loss: 0.00001624
Iteration 183/1000 | Loss: 0.00001624
Iteration 184/1000 | Loss: 0.00001624
Iteration 185/1000 | Loss: 0.00001624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.6236790543189272e-05, 1.6236790543189272e-05, 1.6236790543189272e-05, 1.6236790543189272e-05, 1.6236790543189272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6236790543189272e-05

Optimization complete. Final v2v error: 3.4582414627075195 mm

Highest mean error: 4.083217620849609 mm for frame 65

Lowest mean error: 3.192380905151367 mm for frame 8

Saving results

Total time: 33.751506328582764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454085
Iteration 2/25 | Loss: 0.00165684
Iteration 3/25 | Loss: 0.00112880
Iteration 4/25 | Loss: 0.00095895
Iteration 5/25 | Loss: 0.00099512
Iteration 6/25 | Loss: 0.00110394
Iteration 7/25 | Loss: 0.00109918
Iteration 8/25 | Loss: 0.00090034
Iteration 9/25 | Loss: 0.00082629
Iteration 10/25 | Loss: 0.00081644
Iteration 11/25 | Loss: 0.00081433
Iteration 12/25 | Loss: 0.00081354
Iteration 13/25 | Loss: 0.00081911
Iteration 14/25 | Loss: 0.00083938
Iteration 15/25 | Loss: 0.00082040
Iteration 16/25 | Loss: 0.00080864
Iteration 17/25 | Loss: 0.00080547
Iteration 18/25 | Loss: 0.00080462
Iteration 19/25 | Loss: 0.00080439
Iteration 20/25 | Loss: 0.00080428
Iteration 21/25 | Loss: 0.00080427
Iteration 22/25 | Loss: 0.00080427
Iteration 23/25 | Loss: 0.00080427
Iteration 24/25 | Loss: 0.00080426
Iteration 25/25 | Loss: 0.00080426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39744544
Iteration 2/25 | Loss: 0.00034083
Iteration 3/25 | Loss: 0.00034083
Iteration 4/25 | Loss: 0.00034083
Iteration 5/25 | Loss: 0.00034083
Iteration 6/25 | Loss: 0.00034083
Iteration 7/25 | Loss: 0.00034083
Iteration 8/25 | Loss: 0.00034083
Iteration 9/25 | Loss: 0.00034083
Iteration 10/25 | Loss: 0.00034083
Iteration 11/25 | Loss: 0.00034083
Iteration 12/25 | Loss: 0.00034083
Iteration 13/25 | Loss: 0.00034083
Iteration 14/25 | Loss: 0.00034083
Iteration 15/25 | Loss: 0.00034083
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0003408264892641455, 0.0003408264892641455, 0.0003408264892641455, 0.0003408264892641455, 0.0003408264892641455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003408264892641455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034083
Iteration 2/1000 | Loss: 0.00173352
Iteration 3/1000 | Loss: 0.00093087
Iteration 4/1000 | Loss: 0.00053311
Iteration 5/1000 | Loss: 0.00041270
Iteration 6/1000 | Loss: 0.00015014
Iteration 7/1000 | Loss: 0.00005002
Iteration 8/1000 | Loss: 0.00049694
Iteration 9/1000 | Loss: 0.00016616
Iteration 10/1000 | Loss: 0.00030168
Iteration 11/1000 | Loss: 0.00018517
Iteration 12/1000 | Loss: 0.00004818
Iteration 13/1000 | Loss: 0.00004593
Iteration 14/1000 | Loss: 0.00044035
Iteration 15/1000 | Loss: 0.00019005
Iteration 16/1000 | Loss: 0.00036608
Iteration 17/1000 | Loss: 0.00008121
Iteration 18/1000 | Loss: 0.00005432
Iteration 19/1000 | Loss: 0.00021100
Iteration 20/1000 | Loss: 0.00013131
Iteration 21/1000 | Loss: 0.00004574
Iteration 22/1000 | Loss: 0.00004436
Iteration 23/1000 | Loss: 0.00043041
Iteration 24/1000 | Loss: 0.00020236
Iteration 25/1000 | Loss: 0.00127439
Iteration 26/1000 | Loss: 0.00043610
Iteration 27/1000 | Loss: 0.00004784
Iteration 28/1000 | Loss: 0.00004371
Iteration 29/1000 | Loss: 0.00004309
Iteration 30/1000 | Loss: 0.00004280
Iteration 31/1000 | Loss: 0.00004250
Iteration 32/1000 | Loss: 0.00004233
Iteration 33/1000 | Loss: 0.00004219
Iteration 34/1000 | Loss: 0.00004215
Iteration 35/1000 | Loss: 0.00004207
Iteration 36/1000 | Loss: 0.00004207
Iteration 37/1000 | Loss: 0.00004204
Iteration 38/1000 | Loss: 0.00004204
Iteration 39/1000 | Loss: 0.00004204
Iteration 40/1000 | Loss: 0.00004204
Iteration 41/1000 | Loss: 0.00004203
Iteration 42/1000 | Loss: 0.00004203
Iteration 43/1000 | Loss: 0.00004203
Iteration 44/1000 | Loss: 0.00004202
Iteration 45/1000 | Loss: 0.00004201
Iteration 46/1000 | Loss: 0.00004201
Iteration 47/1000 | Loss: 0.00004200
Iteration 48/1000 | Loss: 0.00004199
Iteration 49/1000 | Loss: 0.00004198
Iteration 50/1000 | Loss: 0.00004196
Iteration 51/1000 | Loss: 0.00004187
Iteration 52/1000 | Loss: 0.00004184
Iteration 53/1000 | Loss: 0.00004183
Iteration 54/1000 | Loss: 0.00004182
Iteration 55/1000 | Loss: 0.00004182
Iteration 56/1000 | Loss: 0.00004181
Iteration 57/1000 | Loss: 0.00004174
Iteration 58/1000 | Loss: 0.00004174
Iteration 59/1000 | Loss: 0.00004173
Iteration 60/1000 | Loss: 0.00004173
Iteration 61/1000 | Loss: 0.00004173
Iteration 62/1000 | Loss: 0.00004172
Iteration 63/1000 | Loss: 0.00004169
Iteration 64/1000 | Loss: 0.00004169
Iteration 65/1000 | Loss: 0.00004168
Iteration 66/1000 | Loss: 0.00004167
Iteration 67/1000 | Loss: 0.00004167
Iteration 68/1000 | Loss: 0.00004165
Iteration 69/1000 | Loss: 0.00004165
Iteration 70/1000 | Loss: 0.00004165
Iteration 71/1000 | Loss: 0.00004165
Iteration 72/1000 | Loss: 0.00004165
Iteration 73/1000 | Loss: 0.00004165
Iteration 74/1000 | Loss: 0.00004165
Iteration 75/1000 | Loss: 0.00004165
Iteration 76/1000 | Loss: 0.00004165
Iteration 77/1000 | Loss: 0.00004164
Iteration 78/1000 | Loss: 0.00004164
Iteration 79/1000 | Loss: 0.00004164
Iteration 80/1000 | Loss: 0.00004164
Iteration 81/1000 | Loss: 0.00004164
Iteration 82/1000 | Loss: 0.00004163
Iteration 83/1000 | Loss: 0.00004163
Iteration 84/1000 | Loss: 0.00004163
Iteration 85/1000 | Loss: 0.00004163
Iteration 86/1000 | Loss: 0.00004163
Iteration 87/1000 | Loss: 0.00004163
Iteration 88/1000 | Loss: 0.00004162
Iteration 89/1000 | Loss: 0.00004162
Iteration 90/1000 | Loss: 0.00004162
Iteration 91/1000 | Loss: 0.00004162
Iteration 92/1000 | Loss: 0.00004162
Iteration 93/1000 | Loss: 0.00004162
Iteration 94/1000 | Loss: 0.00004162
Iteration 95/1000 | Loss: 0.00004162
Iteration 96/1000 | Loss: 0.00004162
Iteration 97/1000 | Loss: 0.00004162
Iteration 98/1000 | Loss: 0.00004161
Iteration 99/1000 | Loss: 0.00004161
Iteration 100/1000 | Loss: 0.00004161
Iteration 101/1000 | Loss: 0.00004161
Iteration 102/1000 | Loss: 0.00004161
Iteration 103/1000 | Loss: 0.00004161
Iteration 104/1000 | Loss: 0.00004160
Iteration 105/1000 | Loss: 0.00004160
Iteration 106/1000 | Loss: 0.00004160
Iteration 107/1000 | Loss: 0.00004159
Iteration 108/1000 | Loss: 0.00004159
Iteration 109/1000 | Loss: 0.00004159
Iteration 110/1000 | Loss: 0.00004159
Iteration 111/1000 | Loss: 0.00004159
Iteration 112/1000 | Loss: 0.00004159
Iteration 113/1000 | Loss: 0.00004159
Iteration 114/1000 | Loss: 0.00004158
Iteration 115/1000 | Loss: 0.00004158
Iteration 116/1000 | Loss: 0.00004158
Iteration 117/1000 | Loss: 0.00004158
Iteration 118/1000 | Loss: 0.00004158
Iteration 119/1000 | Loss: 0.00004158
Iteration 120/1000 | Loss: 0.00004158
Iteration 121/1000 | Loss: 0.00004158
Iteration 122/1000 | Loss: 0.00004158
Iteration 123/1000 | Loss: 0.00004158
Iteration 124/1000 | Loss: 0.00004157
Iteration 125/1000 | Loss: 0.00004157
Iteration 126/1000 | Loss: 0.00004157
Iteration 127/1000 | Loss: 0.00004157
Iteration 128/1000 | Loss: 0.00004157
Iteration 129/1000 | Loss: 0.00004157
Iteration 130/1000 | Loss: 0.00004157
Iteration 131/1000 | Loss: 0.00004157
Iteration 132/1000 | Loss: 0.00004157
Iteration 133/1000 | Loss: 0.00004157
Iteration 134/1000 | Loss: 0.00004157
Iteration 135/1000 | Loss: 0.00004157
Iteration 136/1000 | Loss: 0.00004157
Iteration 137/1000 | Loss: 0.00004157
Iteration 138/1000 | Loss: 0.00004156
Iteration 139/1000 | Loss: 0.00004156
Iteration 140/1000 | Loss: 0.00004156
Iteration 141/1000 | Loss: 0.00004156
Iteration 142/1000 | Loss: 0.00004156
Iteration 143/1000 | Loss: 0.00004156
Iteration 144/1000 | Loss: 0.00004156
Iteration 145/1000 | Loss: 0.00004156
Iteration 146/1000 | Loss: 0.00004156
Iteration 147/1000 | Loss: 0.00004156
Iteration 148/1000 | Loss: 0.00004156
Iteration 149/1000 | Loss: 0.00004156
Iteration 150/1000 | Loss: 0.00004156
Iteration 151/1000 | Loss: 0.00004156
Iteration 152/1000 | Loss: 0.00004156
Iteration 153/1000 | Loss: 0.00004156
Iteration 154/1000 | Loss: 0.00004156
Iteration 155/1000 | Loss: 0.00004156
Iteration 156/1000 | Loss: 0.00004155
Iteration 157/1000 | Loss: 0.00004155
Iteration 158/1000 | Loss: 0.00004155
Iteration 159/1000 | Loss: 0.00004155
Iteration 160/1000 | Loss: 0.00004155
Iteration 161/1000 | Loss: 0.00004155
Iteration 162/1000 | Loss: 0.00004155
Iteration 163/1000 | Loss: 0.00004154
Iteration 164/1000 | Loss: 0.00004154
Iteration 165/1000 | Loss: 0.00004154
Iteration 166/1000 | Loss: 0.00004154
Iteration 167/1000 | Loss: 0.00004154
Iteration 168/1000 | Loss: 0.00004154
Iteration 169/1000 | Loss: 0.00004154
Iteration 170/1000 | Loss: 0.00004154
Iteration 171/1000 | Loss: 0.00004154
Iteration 172/1000 | Loss: 0.00004154
Iteration 173/1000 | Loss: 0.00004154
Iteration 174/1000 | Loss: 0.00004154
Iteration 175/1000 | Loss: 0.00004154
Iteration 176/1000 | Loss: 0.00004154
Iteration 177/1000 | Loss: 0.00004154
Iteration 178/1000 | Loss: 0.00004154
Iteration 179/1000 | Loss: 0.00004154
Iteration 180/1000 | Loss: 0.00004154
Iteration 181/1000 | Loss: 0.00004154
Iteration 182/1000 | Loss: 0.00004154
Iteration 183/1000 | Loss: 0.00004154
Iteration 184/1000 | Loss: 0.00004154
Iteration 185/1000 | Loss: 0.00004154
Iteration 186/1000 | Loss: 0.00004154
Iteration 187/1000 | Loss: 0.00004154
Iteration 188/1000 | Loss: 0.00004154
Iteration 189/1000 | Loss: 0.00004154
Iteration 190/1000 | Loss: 0.00004154
Iteration 191/1000 | Loss: 0.00004154
Iteration 192/1000 | Loss: 0.00004154
Iteration 193/1000 | Loss: 0.00004154
Iteration 194/1000 | Loss: 0.00004154
Iteration 195/1000 | Loss: 0.00004154
Iteration 196/1000 | Loss: 0.00004154
Iteration 197/1000 | Loss: 0.00004154
Iteration 198/1000 | Loss: 0.00004154
Iteration 199/1000 | Loss: 0.00004154
Iteration 200/1000 | Loss: 0.00004154
Iteration 201/1000 | Loss: 0.00004154
Iteration 202/1000 | Loss: 0.00004154
Iteration 203/1000 | Loss: 0.00004154
Iteration 204/1000 | Loss: 0.00004154
Iteration 205/1000 | Loss: 0.00004154
Iteration 206/1000 | Loss: 0.00004154
Iteration 207/1000 | Loss: 0.00004154
Iteration 208/1000 | Loss: 0.00004154
Iteration 209/1000 | Loss: 0.00004154
Iteration 210/1000 | Loss: 0.00004154
Iteration 211/1000 | Loss: 0.00004154
Iteration 212/1000 | Loss: 0.00004154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [4.153820555075072e-05, 4.153820555075072e-05, 4.153820555075072e-05, 4.153820555075072e-05, 4.153820555075072e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.153820555075072e-05

Optimization complete. Final v2v error: 4.5363335609436035 mm

Highest mean error: 6.047266483306885 mm for frame 208

Lowest mean error: 3.6173014640808105 mm for frame 76

Saving results

Total time: 109.80961418151855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105628
Iteration 2/25 | Loss: 0.00368085
Iteration 3/25 | Loss: 0.00267354
Iteration 4/25 | Loss: 0.00232053
Iteration 5/25 | Loss: 0.00205028
Iteration 6/25 | Loss: 0.00192584
Iteration 7/25 | Loss: 0.00178604
Iteration 8/25 | Loss: 0.00188528
Iteration 9/25 | Loss: 0.00159856
Iteration 10/25 | Loss: 0.00149099
Iteration 11/25 | Loss: 0.00141031
Iteration 12/25 | Loss: 0.00137122
Iteration 13/25 | Loss: 0.00137449
Iteration 14/25 | Loss: 0.00135721
Iteration 15/25 | Loss: 0.00133999
Iteration 16/25 | Loss: 0.00134165
Iteration 17/25 | Loss: 0.00132506
Iteration 18/25 | Loss: 0.00132226
Iteration 19/25 | Loss: 0.00131753
Iteration 20/25 | Loss: 0.00131516
Iteration 21/25 | Loss: 0.00131672
Iteration 22/25 | Loss: 0.00131243
Iteration 23/25 | Loss: 0.00130949
Iteration 24/25 | Loss: 0.00131009
Iteration 25/25 | Loss: 0.00130938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35370564
Iteration 2/25 | Loss: 0.00815465
Iteration 3/25 | Loss: 0.00658680
Iteration 4/25 | Loss: 0.00658680
Iteration 5/25 | Loss: 0.00658680
Iteration 6/25 | Loss: 0.00658680
Iteration 7/25 | Loss: 0.00658680
Iteration 8/25 | Loss: 0.00658680
Iteration 9/25 | Loss: 0.00658680
Iteration 10/25 | Loss: 0.00658680
Iteration 11/25 | Loss: 0.00658680
Iteration 12/25 | Loss: 0.00658680
Iteration 13/25 | Loss: 0.00658680
Iteration 14/25 | Loss: 0.00658680
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.006586798932403326, 0.006586798932403326, 0.006586798932403326, 0.006586798932403326, 0.006586798932403326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006586798932403326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00658680
Iteration 2/1000 | Loss: 0.00209007
Iteration 3/1000 | Loss: 0.00319627
Iteration 4/1000 | Loss: 0.00054391
Iteration 5/1000 | Loss: 0.00171591
Iteration 6/1000 | Loss: 0.00170575
Iteration 7/1000 | Loss: 0.00232449
Iteration 8/1000 | Loss: 0.00490345
Iteration 9/1000 | Loss: 0.00071858
Iteration 10/1000 | Loss: 0.00117836
Iteration 11/1000 | Loss: 0.00071087
Iteration 12/1000 | Loss: 0.00057429
Iteration 13/1000 | Loss: 0.00139380
Iteration 14/1000 | Loss: 0.00076150
Iteration 15/1000 | Loss: 0.00125853
Iteration 16/1000 | Loss: 0.00080355
Iteration 17/1000 | Loss: 0.00033489
Iteration 18/1000 | Loss: 0.00065784
Iteration 19/1000 | Loss: 0.00036641
Iteration 20/1000 | Loss: 0.00029670
Iteration 21/1000 | Loss: 0.00102858
Iteration 22/1000 | Loss: 0.00052298
Iteration 23/1000 | Loss: 0.00193190
Iteration 24/1000 | Loss: 0.00100535
Iteration 25/1000 | Loss: 0.00063577
Iteration 26/1000 | Loss: 0.00037082
Iteration 27/1000 | Loss: 0.00044033
Iteration 28/1000 | Loss: 0.00033045
Iteration 29/1000 | Loss: 0.00045454
Iteration 30/1000 | Loss: 0.00027110
Iteration 31/1000 | Loss: 0.00143249
Iteration 32/1000 | Loss: 0.00091506
Iteration 33/1000 | Loss: 0.00040748
Iteration 34/1000 | Loss: 0.00033975
Iteration 35/1000 | Loss: 0.00041301
Iteration 36/1000 | Loss: 0.00040082
Iteration 37/1000 | Loss: 0.00034892
Iteration 38/1000 | Loss: 0.00068811
Iteration 39/1000 | Loss: 0.00024553
Iteration 40/1000 | Loss: 0.00024331
Iteration 41/1000 | Loss: 0.00024170
Iteration 42/1000 | Loss: 0.00024048
Iteration 43/1000 | Loss: 0.00023939
Iteration 44/1000 | Loss: 0.00023863
Iteration 45/1000 | Loss: 0.00042972
Iteration 46/1000 | Loss: 0.00023939
Iteration 47/1000 | Loss: 0.00023748
Iteration 48/1000 | Loss: 0.00023709
Iteration 49/1000 | Loss: 0.00034548
Iteration 50/1000 | Loss: 0.00034547
Iteration 51/1000 | Loss: 0.00080506
Iteration 52/1000 | Loss: 0.00037539
Iteration 53/1000 | Loss: 0.00032562
Iteration 54/1000 | Loss: 0.00023763
Iteration 55/1000 | Loss: 0.00023698
Iteration 56/1000 | Loss: 0.00023674
Iteration 57/1000 | Loss: 0.00042718
Iteration 58/1000 | Loss: 0.00024547
Iteration 59/1000 | Loss: 0.00024643
Iteration 60/1000 | Loss: 0.00023653
Iteration 61/1000 | Loss: 0.00031170
Iteration 62/1000 | Loss: 0.00041636
Iteration 63/1000 | Loss: 0.00057693
Iteration 64/1000 | Loss: 0.00024271
Iteration 65/1000 | Loss: 0.00025165
Iteration 66/1000 | Loss: 0.00023648
Iteration 67/1000 | Loss: 0.00023631
Iteration 68/1000 | Loss: 0.00023621
Iteration 69/1000 | Loss: 0.00023618
Iteration 70/1000 | Loss: 0.00023617
Iteration 71/1000 | Loss: 0.00023617
Iteration 72/1000 | Loss: 0.00023616
Iteration 73/1000 | Loss: 0.00023613
Iteration 74/1000 | Loss: 0.00023612
Iteration 75/1000 | Loss: 0.00023612
Iteration 76/1000 | Loss: 0.00023612
Iteration 77/1000 | Loss: 0.00023612
Iteration 78/1000 | Loss: 0.00023612
Iteration 79/1000 | Loss: 0.00023612
Iteration 80/1000 | Loss: 0.00023612
Iteration 81/1000 | Loss: 0.00023612
Iteration 82/1000 | Loss: 0.00023612
Iteration 83/1000 | Loss: 0.00023612
Iteration 84/1000 | Loss: 0.00023611
Iteration 85/1000 | Loss: 0.00023609
Iteration 86/1000 | Loss: 0.00023609
Iteration 87/1000 | Loss: 0.00023608
Iteration 88/1000 | Loss: 0.00023608
Iteration 89/1000 | Loss: 0.00023607
Iteration 90/1000 | Loss: 0.00023607
Iteration 91/1000 | Loss: 0.00023606
Iteration 92/1000 | Loss: 0.00023605
Iteration 93/1000 | Loss: 0.00023605
Iteration 94/1000 | Loss: 0.00023604
Iteration 95/1000 | Loss: 0.00023604
Iteration 96/1000 | Loss: 0.00023604
Iteration 97/1000 | Loss: 0.00023604
Iteration 98/1000 | Loss: 0.00023604
Iteration 99/1000 | Loss: 0.00023604
Iteration 100/1000 | Loss: 0.00032399
Iteration 101/1000 | Loss: 0.00087047
Iteration 102/1000 | Loss: 0.00024504
Iteration 103/1000 | Loss: 0.00034579
Iteration 104/1000 | Loss: 0.00080124
Iteration 105/1000 | Loss: 0.00023754
Iteration 106/1000 | Loss: 0.00024608
Iteration 107/1000 | Loss: 0.00023633
Iteration 108/1000 | Loss: 0.00023605
Iteration 109/1000 | Loss: 0.00023596
Iteration 110/1000 | Loss: 0.00023596
Iteration 111/1000 | Loss: 0.00023596
Iteration 112/1000 | Loss: 0.00023596
Iteration 113/1000 | Loss: 0.00023596
Iteration 114/1000 | Loss: 0.00023596
Iteration 115/1000 | Loss: 0.00023596
Iteration 116/1000 | Loss: 0.00023596
Iteration 117/1000 | Loss: 0.00023596
Iteration 118/1000 | Loss: 0.00023596
Iteration 119/1000 | Loss: 0.00023595
Iteration 120/1000 | Loss: 0.00023594
Iteration 121/1000 | Loss: 0.00023593
Iteration 122/1000 | Loss: 0.00023593
Iteration 123/1000 | Loss: 0.00023593
Iteration 124/1000 | Loss: 0.00023593
Iteration 125/1000 | Loss: 0.00023593
Iteration 126/1000 | Loss: 0.00023592
Iteration 127/1000 | Loss: 0.00033074
Iteration 128/1000 | Loss: 0.00023599
Iteration 129/1000 | Loss: 0.00023590
Iteration 130/1000 | Loss: 0.00023590
Iteration 131/1000 | Loss: 0.00023589
Iteration 132/1000 | Loss: 0.00023589
Iteration 133/1000 | Loss: 0.00023589
Iteration 134/1000 | Loss: 0.00023589
Iteration 135/1000 | Loss: 0.00023589
Iteration 136/1000 | Loss: 0.00023589
Iteration 137/1000 | Loss: 0.00023589
Iteration 138/1000 | Loss: 0.00023589
Iteration 139/1000 | Loss: 0.00023589
Iteration 140/1000 | Loss: 0.00023589
Iteration 141/1000 | Loss: 0.00023589
Iteration 142/1000 | Loss: 0.00023589
Iteration 143/1000 | Loss: 0.00023589
Iteration 144/1000 | Loss: 0.00023588
Iteration 145/1000 | Loss: 0.00023588
Iteration 146/1000 | Loss: 0.00023588
Iteration 147/1000 | Loss: 0.00023588
Iteration 148/1000 | Loss: 0.00023588
Iteration 149/1000 | Loss: 0.00023588
Iteration 150/1000 | Loss: 0.00023588
Iteration 151/1000 | Loss: 0.00023588
Iteration 152/1000 | Loss: 0.00023588
Iteration 153/1000 | Loss: 0.00023588
Iteration 154/1000 | Loss: 0.00023588
Iteration 155/1000 | Loss: 0.00023588
Iteration 156/1000 | Loss: 0.00023588
Iteration 157/1000 | Loss: 0.00023588
Iteration 158/1000 | Loss: 0.00023588
Iteration 159/1000 | Loss: 0.00023588
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [0.00023588324256706983, 0.00023588324256706983, 0.00023588324256706983, 0.00023588324256706983, 0.00023588324256706983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023588324256706983

Optimization complete. Final v2v error: 8.489978790283203 mm

Highest mean error: 11.210383415222168 mm for frame 93

Lowest mean error: 4.953980445861816 mm for frame 179

Saving results

Total time: 181.87866401672363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881413
Iteration 2/25 | Loss: 0.00121860
Iteration 3/25 | Loss: 0.00082422
Iteration 4/25 | Loss: 0.00077171
Iteration 5/25 | Loss: 0.00076159
Iteration 6/25 | Loss: 0.00075959
Iteration 7/25 | Loss: 0.00075913
Iteration 8/25 | Loss: 0.00075913
Iteration 9/25 | Loss: 0.00075913
Iteration 10/25 | Loss: 0.00075913
Iteration 11/25 | Loss: 0.00075913
Iteration 12/25 | Loss: 0.00075913
Iteration 13/25 | Loss: 0.00075913
Iteration 14/25 | Loss: 0.00075913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007591345347464085, 0.0007591345347464085, 0.0007591345347464085, 0.0007591345347464085, 0.0007591345347464085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007591345347464085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97517341
Iteration 2/25 | Loss: 0.00021831
Iteration 3/25 | Loss: 0.00021830
Iteration 4/25 | Loss: 0.00021830
Iteration 5/25 | Loss: 0.00021830
Iteration 6/25 | Loss: 0.00021830
Iteration 7/25 | Loss: 0.00021830
Iteration 8/25 | Loss: 0.00021830
Iteration 9/25 | Loss: 0.00021830
Iteration 10/25 | Loss: 0.00021830
Iteration 11/25 | Loss: 0.00021830
Iteration 12/25 | Loss: 0.00021830
Iteration 13/25 | Loss: 0.00021830
Iteration 14/25 | Loss: 0.00021830
Iteration 15/25 | Loss: 0.00021830
Iteration 16/25 | Loss: 0.00021830
Iteration 17/25 | Loss: 0.00021830
Iteration 18/25 | Loss: 0.00021830
Iteration 19/25 | Loss: 0.00021830
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00021829511388204992, 0.00021829511388204992, 0.00021829511388204992, 0.00021829511388204992, 0.00021829511388204992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021829511388204992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021830
Iteration 2/1000 | Loss: 0.00004139
Iteration 3/1000 | Loss: 0.00003140
Iteration 4/1000 | Loss: 0.00002890
Iteration 5/1000 | Loss: 0.00002705
Iteration 6/1000 | Loss: 0.00002586
Iteration 7/1000 | Loss: 0.00002508
Iteration 8/1000 | Loss: 0.00002457
Iteration 9/1000 | Loss: 0.00002444
Iteration 10/1000 | Loss: 0.00002422
Iteration 11/1000 | Loss: 0.00002400
Iteration 12/1000 | Loss: 0.00002390
Iteration 13/1000 | Loss: 0.00002390
Iteration 14/1000 | Loss: 0.00002390
Iteration 15/1000 | Loss: 0.00002390
Iteration 16/1000 | Loss: 0.00002390
Iteration 17/1000 | Loss: 0.00002390
Iteration 18/1000 | Loss: 0.00002390
Iteration 19/1000 | Loss: 0.00002390
Iteration 20/1000 | Loss: 0.00002390
Iteration 21/1000 | Loss: 0.00002390
Iteration 22/1000 | Loss: 0.00002389
Iteration 23/1000 | Loss: 0.00002388
Iteration 24/1000 | Loss: 0.00002388
Iteration 25/1000 | Loss: 0.00002388
Iteration 26/1000 | Loss: 0.00002387
Iteration 27/1000 | Loss: 0.00002387
Iteration 28/1000 | Loss: 0.00002387
Iteration 29/1000 | Loss: 0.00002387
Iteration 30/1000 | Loss: 0.00002387
Iteration 31/1000 | Loss: 0.00002387
Iteration 32/1000 | Loss: 0.00002386
Iteration 33/1000 | Loss: 0.00002386
Iteration 34/1000 | Loss: 0.00002386
Iteration 35/1000 | Loss: 0.00002385
Iteration 36/1000 | Loss: 0.00002385
Iteration 37/1000 | Loss: 0.00002384
Iteration 38/1000 | Loss: 0.00002382
Iteration 39/1000 | Loss: 0.00002381
Iteration 40/1000 | Loss: 0.00002381
Iteration 41/1000 | Loss: 0.00002381
Iteration 42/1000 | Loss: 0.00002381
Iteration 43/1000 | Loss: 0.00002381
Iteration 44/1000 | Loss: 0.00002381
Iteration 45/1000 | Loss: 0.00002381
Iteration 46/1000 | Loss: 0.00002380
Iteration 47/1000 | Loss: 0.00002380
Iteration 48/1000 | Loss: 0.00002380
Iteration 49/1000 | Loss: 0.00002380
Iteration 50/1000 | Loss: 0.00002380
Iteration 51/1000 | Loss: 0.00002380
Iteration 52/1000 | Loss: 0.00002380
Iteration 53/1000 | Loss: 0.00002380
Iteration 54/1000 | Loss: 0.00002379
Iteration 55/1000 | Loss: 0.00002379
Iteration 56/1000 | Loss: 0.00002379
Iteration 57/1000 | Loss: 0.00002379
Iteration 58/1000 | Loss: 0.00002379
Iteration 59/1000 | Loss: 0.00002379
Iteration 60/1000 | Loss: 0.00002379
Iteration 61/1000 | Loss: 0.00002379
Iteration 62/1000 | Loss: 0.00002379
Iteration 63/1000 | Loss: 0.00002379
Iteration 64/1000 | Loss: 0.00002379
Iteration 65/1000 | Loss: 0.00002379
Iteration 66/1000 | Loss: 0.00002379
Iteration 67/1000 | Loss: 0.00002379
Iteration 68/1000 | Loss: 0.00002379
Iteration 69/1000 | Loss: 0.00002379
Iteration 70/1000 | Loss: 0.00002379
Iteration 71/1000 | Loss: 0.00002379
Iteration 72/1000 | Loss: 0.00002379
Iteration 73/1000 | Loss: 0.00002379
Iteration 74/1000 | Loss: 0.00002379
Iteration 75/1000 | Loss: 0.00002379
Iteration 76/1000 | Loss: 0.00002379
Iteration 77/1000 | Loss: 0.00002379
Iteration 78/1000 | Loss: 0.00002379
Iteration 79/1000 | Loss: 0.00002379
Iteration 80/1000 | Loss: 0.00002379
Iteration 81/1000 | Loss: 0.00002379
Iteration 82/1000 | Loss: 0.00002379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [2.3790464183548465e-05, 2.3790464183548465e-05, 2.3790464183548465e-05, 2.3790464183548465e-05, 2.3790464183548465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3790464183548465e-05

Optimization complete. Final v2v error: 4.215196132659912 mm

Highest mean error: 4.710418224334717 mm for frame 1

Lowest mean error: 3.8877573013305664 mm for frame 46

Saving results

Total time: 29.801794052124023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00508265
Iteration 2/25 | Loss: 0.00090697
Iteration 3/25 | Loss: 0.00071885
Iteration 4/25 | Loss: 0.00069135
Iteration 5/25 | Loss: 0.00068506
Iteration 6/25 | Loss: 0.00068336
Iteration 7/25 | Loss: 0.00068294
Iteration 8/25 | Loss: 0.00068294
Iteration 9/25 | Loss: 0.00068294
Iteration 10/25 | Loss: 0.00068294
Iteration 11/25 | Loss: 0.00068294
Iteration 12/25 | Loss: 0.00068294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006829368649050593, 0.0006829368649050593, 0.0006829368649050593, 0.0006829368649050593, 0.0006829368649050593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006829368649050593

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37607574
Iteration 2/25 | Loss: 0.00010674
Iteration 3/25 | Loss: 0.00010673
Iteration 4/25 | Loss: 0.00010673
Iteration 5/25 | Loss: 0.00010672
Iteration 6/25 | Loss: 0.00010672
Iteration 7/25 | Loss: 0.00010672
Iteration 8/25 | Loss: 0.00010672
Iteration 9/25 | Loss: 0.00010672
Iteration 10/25 | Loss: 0.00010672
Iteration 11/25 | Loss: 0.00010672
Iteration 12/25 | Loss: 0.00010672
Iteration 13/25 | Loss: 0.00010672
Iteration 14/25 | Loss: 0.00010672
Iteration 15/25 | Loss: 0.00010672
Iteration 16/25 | Loss: 0.00010672
Iteration 17/25 | Loss: 0.00010672
Iteration 18/25 | Loss: 0.00010672
Iteration 19/25 | Loss: 0.00010672
Iteration 20/25 | Loss: 0.00010672
Iteration 21/25 | Loss: 0.00010672
Iteration 22/25 | Loss: 0.00010672
Iteration 23/25 | Loss: 0.00010672
Iteration 24/25 | Loss: 0.00010672
Iteration 25/25 | Loss: 0.00010672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00010672309144865721, 0.00010672309144865721, 0.00010672309144865721, 0.00010672309144865721, 0.00010672309144865721]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00010672309144865721

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00010672
Iteration 2/1000 | Loss: 0.00003298
Iteration 3/1000 | Loss: 0.00002575
Iteration 4/1000 | Loss: 0.00002389
Iteration 5/1000 | Loss: 0.00002292
Iteration 6/1000 | Loss: 0.00002229
Iteration 7/1000 | Loss: 0.00002189
Iteration 8/1000 | Loss: 0.00002166
Iteration 9/1000 | Loss: 0.00002159
Iteration 10/1000 | Loss: 0.00002156
Iteration 11/1000 | Loss: 0.00002156
Iteration 12/1000 | Loss: 0.00002148
Iteration 13/1000 | Loss: 0.00002148
Iteration 14/1000 | Loss: 0.00002142
Iteration 15/1000 | Loss: 0.00002142
Iteration 16/1000 | Loss: 0.00002141
Iteration 17/1000 | Loss: 0.00002138
Iteration 18/1000 | Loss: 0.00002136
Iteration 19/1000 | Loss: 0.00002135
Iteration 20/1000 | Loss: 0.00002135
Iteration 21/1000 | Loss: 0.00002134
Iteration 22/1000 | Loss: 0.00002133
Iteration 23/1000 | Loss: 0.00002132
Iteration 24/1000 | Loss: 0.00002128
Iteration 25/1000 | Loss: 0.00002128
Iteration 26/1000 | Loss: 0.00002127
Iteration 27/1000 | Loss: 0.00002127
Iteration 28/1000 | Loss: 0.00002127
Iteration 29/1000 | Loss: 0.00002126
Iteration 30/1000 | Loss: 0.00002126
Iteration 31/1000 | Loss: 0.00002126
Iteration 32/1000 | Loss: 0.00002126
Iteration 33/1000 | Loss: 0.00002126
Iteration 34/1000 | Loss: 0.00002126
Iteration 35/1000 | Loss: 0.00002126
Iteration 36/1000 | Loss: 0.00002124
Iteration 37/1000 | Loss: 0.00002124
Iteration 38/1000 | Loss: 0.00002124
Iteration 39/1000 | Loss: 0.00002124
Iteration 40/1000 | Loss: 0.00002124
Iteration 41/1000 | Loss: 0.00002124
Iteration 42/1000 | Loss: 0.00002124
Iteration 43/1000 | Loss: 0.00002123
Iteration 44/1000 | Loss: 0.00002123
Iteration 45/1000 | Loss: 0.00002123
Iteration 46/1000 | Loss: 0.00002123
Iteration 47/1000 | Loss: 0.00002122
Iteration 48/1000 | Loss: 0.00002122
Iteration 49/1000 | Loss: 0.00002121
Iteration 50/1000 | Loss: 0.00002121
Iteration 51/1000 | Loss: 0.00002121
Iteration 52/1000 | Loss: 0.00002121
Iteration 53/1000 | Loss: 0.00002121
Iteration 54/1000 | Loss: 0.00002121
Iteration 55/1000 | Loss: 0.00002121
Iteration 56/1000 | Loss: 0.00002120
Iteration 57/1000 | Loss: 0.00002120
Iteration 58/1000 | Loss: 0.00002120
Iteration 59/1000 | Loss: 0.00002120
Iteration 60/1000 | Loss: 0.00002120
Iteration 61/1000 | Loss: 0.00002120
Iteration 62/1000 | Loss: 0.00002120
Iteration 63/1000 | Loss: 0.00002120
Iteration 64/1000 | Loss: 0.00002120
Iteration 65/1000 | Loss: 0.00002119
Iteration 66/1000 | Loss: 0.00002119
Iteration 67/1000 | Loss: 0.00002119
Iteration 68/1000 | Loss: 0.00002119
Iteration 69/1000 | Loss: 0.00002119
Iteration 70/1000 | Loss: 0.00002119
Iteration 71/1000 | Loss: 0.00002119
Iteration 72/1000 | Loss: 0.00002119
Iteration 73/1000 | Loss: 0.00002119
Iteration 74/1000 | Loss: 0.00002119
Iteration 75/1000 | Loss: 0.00002118
Iteration 76/1000 | Loss: 0.00002118
Iteration 77/1000 | Loss: 0.00002118
Iteration 78/1000 | Loss: 0.00002118
Iteration 79/1000 | Loss: 0.00002117
Iteration 80/1000 | Loss: 0.00002117
Iteration 81/1000 | Loss: 0.00002117
Iteration 82/1000 | Loss: 0.00002116
Iteration 83/1000 | Loss: 0.00002116
Iteration 84/1000 | Loss: 0.00002116
Iteration 85/1000 | Loss: 0.00002116
Iteration 86/1000 | Loss: 0.00002116
Iteration 87/1000 | Loss: 0.00002115
Iteration 88/1000 | Loss: 0.00002115
Iteration 89/1000 | Loss: 0.00002115
Iteration 90/1000 | Loss: 0.00002115
Iteration 91/1000 | Loss: 0.00002115
Iteration 92/1000 | Loss: 0.00002115
Iteration 93/1000 | Loss: 0.00002115
Iteration 94/1000 | Loss: 0.00002115
Iteration 95/1000 | Loss: 0.00002115
Iteration 96/1000 | Loss: 0.00002115
Iteration 97/1000 | Loss: 0.00002115
Iteration 98/1000 | Loss: 0.00002115
Iteration 99/1000 | Loss: 0.00002115
Iteration 100/1000 | Loss: 0.00002114
Iteration 101/1000 | Loss: 0.00002114
Iteration 102/1000 | Loss: 0.00002114
Iteration 103/1000 | Loss: 0.00002114
Iteration 104/1000 | Loss: 0.00002114
Iteration 105/1000 | Loss: 0.00002114
Iteration 106/1000 | Loss: 0.00002114
Iteration 107/1000 | Loss: 0.00002114
Iteration 108/1000 | Loss: 0.00002114
Iteration 109/1000 | Loss: 0.00002114
Iteration 110/1000 | Loss: 0.00002114
Iteration 111/1000 | Loss: 0.00002114
Iteration 112/1000 | Loss: 0.00002114
Iteration 113/1000 | Loss: 0.00002114
Iteration 114/1000 | Loss: 0.00002114
Iteration 115/1000 | Loss: 0.00002114
Iteration 116/1000 | Loss: 0.00002113
Iteration 117/1000 | Loss: 0.00002113
Iteration 118/1000 | Loss: 0.00002113
Iteration 119/1000 | Loss: 0.00002113
Iteration 120/1000 | Loss: 0.00002113
Iteration 121/1000 | Loss: 0.00002113
Iteration 122/1000 | Loss: 0.00002113
Iteration 123/1000 | Loss: 0.00002113
Iteration 124/1000 | Loss: 0.00002113
Iteration 125/1000 | Loss: 0.00002113
Iteration 126/1000 | Loss: 0.00002113
Iteration 127/1000 | Loss: 0.00002113
Iteration 128/1000 | Loss: 0.00002113
Iteration 129/1000 | Loss: 0.00002113
Iteration 130/1000 | Loss: 0.00002113
Iteration 131/1000 | Loss: 0.00002113
Iteration 132/1000 | Loss: 0.00002113
Iteration 133/1000 | Loss: 0.00002113
Iteration 134/1000 | Loss: 0.00002113
Iteration 135/1000 | Loss: 0.00002113
Iteration 136/1000 | Loss: 0.00002113
Iteration 137/1000 | Loss: 0.00002113
Iteration 138/1000 | Loss: 0.00002113
Iteration 139/1000 | Loss: 0.00002113
Iteration 140/1000 | Loss: 0.00002113
Iteration 141/1000 | Loss: 0.00002113
Iteration 142/1000 | Loss: 0.00002113
Iteration 143/1000 | Loss: 0.00002113
Iteration 144/1000 | Loss: 0.00002113
Iteration 145/1000 | Loss: 0.00002113
Iteration 146/1000 | Loss: 0.00002113
Iteration 147/1000 | Loss: 0.00002113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.1133097106940113e-05, 2.1133097106940113e-05, 2.1133097106940113e-05, 2.1133097106940113e-05, 2.1133097106940113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1133097106940113e-05

Optimization complete. Final v2v error: 3.779611587524414 mm

Highest mean error: 4.915460586547852 mm for frame 130

Lowest mean error: 2.882896661758423 mm for frame 5

Saving results

Total time: 33.0701117515564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864469
Iteration 2/25 | Loss: 0.00077316
Iteration 3/25 | Loss: 0.00061021
Iteration 4/25 | Loss: 0.00059014
Iteration 5/25 | Loss: 0.00058535
Iteration 6/25 | Loss: 0.00058393
Iteration 7/25 | Loss: 0.00058360
Iteration 8/25 | Loss: 0.00058360
Iteration 9/25 | Loss: 0.00058360
Iteration 10/25 | Loss: 0.00058360
Iteration 11/25 | Loss: 0.00058360
Iteration 12/25 | Loss: 0.00058360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005836037453263998, 0.0005836037453263998, 0.0005836037453263998, 0.0005836037453263998, 0.0005836037453263998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005836037453263998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38079059
Iteration 2/25 | Loss: 0.00008564
Iteration 3/25 | Loss: 0.00008564
Iteration 4/25 | Loss: 0.00008564
Iteration 5/25 | Loss: 0.00008564
Iteration 6/25 | Loss: 0.00008564
Iteration 7/25 | Loss: 0.00008564
Iteration 8/25 | Loss: 0.00008564
Iteration 9/25 | Loss: 0.00008564
Iteration 10/25 | Loss: 0.00008564
Iteration 11/25 | Loss: 0.00008564
Iteration 12/25 | Loss: 0.00008564
Iteration 13/25 | Loss: 0.00008564
Iteration 14/25 | Loss: 0.00008564
Iteration 15/25 | Loss: 0.00008564
Iteration 16/25 | Loss: 0.00008564
Iteration 17/25 | Loss: 0.00008564
Iteration 18/25 | Loss: 0.00008564
Iteration 19/25 | Loss: 0.00008564
Iteration 20/25 | Loss: 0.00008564
Iteration 21/25 | Loss: 0.00008564
Iteration 22/25 | Loss: 0.00008564
Iteration 23/25 | Loss: 0.00008564
Iteration 24/25 | Loss: 0.00008564
Iteration 25/25 | Loss: 0.00008564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00008564
Iteration 2/1000 | Loss: 0.00002276
Iteration 3/1000 | Loss: 0.00001890
Iteration 4/1000 | Loss: 0.00001762
Iteration 5/1000 | Loss: 0.00001681
Iteration 6/1000 | Loss: 0.00001618
Iteration 7/1000 | Loss: 0.00001578
Iteration 8/1000 | Loss: 0.00001570
Iteration 9/1000 | Loss: 0.00001565
Iteration 10/1000 | Loss: 0.00001564
Iteration 11/1000 | Loss: 0.00001554
Iteration 12/1000 | Loss: 0.00001551
Iteration 13/1000 | Loss: 0.00001551
Iteration 14/1000 | Loss: 0.00001550
Iteration 15/1000 | Loss: 0.00001550
Iteration 16/1000 | Loss: 0.00001550
Iteration 17/1000 | Loss: 0.00001550
Iteration 18/1000 | Loss: 0.00001546
Iteration 19/1000 | Loss: 0.00001541
Iteration 20/1000 | Loss: 0.00001539
Iteration 21/1000 | Loss: 0.00001539
Iteration 22/1000 | Loss: 0.00001538
Iteration 23/1000 | Loss: 0.00001538
Iteration 24/1000 | Loss: 0.00001536
Iteration 25/1000 | Loss: 0.00001535
Iteration 26/1000 | Loss: 0.00001535
Iteration 27/1000 | Loss: 0.00001534
Iteration 28/1000 | Loss: 0.00001534
Iteration 29/1000 | Loss: 0.00001533
Iteration 30/1000 | Loss: 0.00001533
Iteration 31/1000 | Loss: 0.00001531
Iteration 32/1000 | Loss: 0.00001531
Iteration 33/1000 | Loss: 0.00001530
Iteration 34/1000 | Loss: 0.00001530
Iteration 35/1000 | Loss: 0.00001530
Iteration 36/1000 | Loss: 0.00001529
Iteration 37/1000 | Loss: 0.00001529
Iteration 38/1000 | Loss: 0.00001529
Iteration 39/1000 | Loss: 0.00001529
Iteration 40/1000 | Loss: 0.00001529
Iteration 41/1000 | Loss: 0.00001528
Iteration 42/1000 | Loss: 0.00001528
Iteration 43/1000 | Loss: 0.00001528
Iteration 44/1000 | Loss: 0.00001528
Iteration 45/1000 | Loss: 0.00001527
Iteration 46/1000 | Loss: 0.00001526
Iteration 47/1000 | Loss: 0.00001526
Iteration 48/1000 | Loss: 0.00001525
Iteration 49/1000 | Loss: 0.00001525
Iteration 50/1000 | Loss: 0.00001525
Iteration 51/1000 | Loss: 0.00001525
Iteration 52/1000 | Loss: 0.00001525
Iteration 53/1000 | Loss: 0.00001525
Iteration 54/1000 | Loss: 0.00001525
Iteration 55/1000 | Loss: 0.00001524
Iteration 56/1000 | Loss: 0.00001524
Iteration 57/1000 | Loss: 0.00001524
Iteration 58/1000 | Loss: 0.00001524
Iteration 59/1000 | Loss: 0.00001524
Iteration 60/1000 | Loss: 0.00001524
Iteration 61/1000 | Loss: 0.00001524
Iteration 62/1000 | Loss: 0.00001524
Iteration 63/1000 | Loss: 0.00001524
Iteration 64/1000 | Loss: 0.00001523
Iteration 65/1000 | Loss: 0.00001523
Iteration 66/1000 | Loss: 0.00001523
Iteration 67/1000 | Loss: 0.00001523
Iteration 68/1000 | Loss: 0.00001523
Iteration 69/1000 | Loss: 0.00001523
Iteration 70/1000 | Loss: 0.00001523
Iteration 71/1000 | Loss: 0.00001522
Iteration 72/1000 | Loss: 0.00001522
Iteration 73/1000 | Loss: 0.00001522
Iteration 74/1000 | Loss: 0.00001522
Iteration 75/1000 | Loss: 0.00001522
Iteration 76/1000 | Loss: 0.00001521
Iteration 77/1000 | Loss: 0.00001521
Iteration 78/1000 | Loss: 0.00001521
Iteration 79/1000 | Loss: 0.00001521
Iteration 80/1000 | Loss: 0.00001521
Iteration 81/1000 | Loss: 0.00001521
Iteration 82/1000 | Loss: 0.00001521
Iteration 83/1000 | Loss: 0.00001521
Iteration 84/1000 | Loss: 0.00001521
Iteration 85/1000 | Loss: 0.00001521
Iteration 86/1000 | Loss: 0.00001521
Iteration 87/1000 | Loss: 0.00001521
Iteration 88/1000 | Loss: 0.00001520
Iteration 89/1000 | Loss: 0.00001520
Iteration 90/1000 | Loss: 0.00001519
Iteration 91/1000 | Loss: 0.00001519
Iteration 92/1000 | Loss: 0.00001518
Iteration 93/1000 | Loss: 0.00001518
Iteration 94/1000 | Loss: 0.00001518
Iteration 95/1000 | Loss: 0.00001518
Iteration 96/1000 | Loss: 0.00001518
Iteration 97/1000 | Loss: 0.00001518
Iteration 98/1000 | Loss: 0.00001517
Iteration 99/1000 | Loss: 0.00001517
Iteration 100/1000 | Loss: 0.00001517
Iteration 101/1000 | Loss: 0.00001516
Iteration 102/1000 | Loss: 0.00001516
Iteration 103/1000 | Loss: 0.00001516
Iteration 104/1000 | Loss: 0.00001516
Iteration 105/1000 | Loss: 0.00001516
Iteration 106/1000 | Loss: 0.00001515
Iteration 107/1000 | Loss: 0.00001515
Iteration 108/1000 | Loss: 0.00001515
Iteration 109/1000 | Loss: 0.00001515
Iteration 110/1000 | Loss: 0.00001515
Iteration 111/1000 | Loss: 0.00001515
Iteration 112/1000 | Loss: 0.00001515
Iteration 113/1000 | Loss: 0.00001515
Iteration 114/1000 | Loss: 0.00001515
Iteration 115/1000 | Loss: 0.00001515
Iteration 116/1000 | Loss: 0.00001515
Iteration 117/1000 | Loss: 0.00001514
Iteration 118/1000 | Loss: 0.00001514
Iteration 119/1000 | Loss: 0.00001514
Iteration 120/1000 | Loss: 0.00001514
Iteration 121/1000 | Loss: 0.00001514
Iteration 122/1000 | Loss: 0.00001514
Iteration 123/1000 | Loss: 0.00001513
Iteration 124/1000 | Loss: 0.00001513
Iteration 125/1000 | Loss: 0.00001513
Iteration 126/1000 | Loss: 0.00001513
Iteration 127/1000 | Loss: 0.00001513
Iteration 128/1000 | Loss: 0.00001513
Iteration 129/1000 | Loss: 0.00001512
Iteration 130/1000 | Loss: 0.00001512
Iteration 131/1000 | Loss: 0.00001511
Iteration 132/1000 | Loss: 0.00001510
Iteration 133/1000 | Loss: 0.00001510
Iteration 134/1000 | Loss: 0.00001510
Iteration 135/1000 | Loss: 0.00001510
Iteration 136/1000 | Loss: 0.00001510
Iteration 137/1000 | Loss: 0.00001510
Iteration 138/1000 | Loss: 0.00001510
Iteration 139/1000 | Loss: 0.00001509
Iteration 140/1000 | Loss: 0.00001509
Iteration 141/1000 | Loss: 0.00001509
Iteration 142/1000 | Loss: 0.00001509
Iteration 143/1000 | Loss: 0.00001508
Iteration 144/1000 | Loss: 0.00001508
Iteration 145/1000 | Loss: 0.00001508
Iteration 146/1000 | Loss: 0.00001508
Iteration 147/1000 | Loss: 0.00001508
Iteration 148/1000 | Loss: 0.00001507
Iteration 149/1000 | Loss: 0.00001507
Iteration 150/1000 | Loss: 0.00001507
Iteration 151/1000 | Loss: 0.00001507
Iteration 152/1000 | Loss: 0.00001507
Iteration 153/1000 | Loss: 0.00001507
Iteration 154/1000 | Loss: 0.00001506
Iteration 155/1000 | Loss: 0.00001506
Iteration 156/1000 | Loss: 0.00001506
Iteration 157/1000 | Loss: 0.00001506
Iteration 158/1000 | Loss: 0.00001505
Iteration 159/1000 | Loss: 0.00001505
Iteration 160/1000 | Loss: 0.00001505
Iteration 161/1000 | Loss: 0.00001505
Iteration 162/1000 | Loss: 0.00001505
Iteration 163/1000 | Loss: 0.00001505
Iteration 164/1000 | Loss: 0.00001505
Iteration 165/1000 | Loss: 0.00001504
Iteration 166/1000 | Loss: 0.00001504
Iteration 167/1000 | Loss: 0.00001504
Iteration 168/1000 | Loss: 0.00001504
Iteration 169/1000 | Loss: 0.00001504
Iteration 170/1000 | Loss: 0.00001503
Iteration 171/1000 | Loss: 0.00001503
Iteration 172/1000 | Loss: 0.00001503
Iteration 173/1000 | Loss: 0.00001503
Iteration 174/1000 | Loss: 0.00001503
Iteration 175/1000 | Loss: 0.00001503
Iteration 176/1000 | Loss: 0.00001503
Iteration 177/1000 | Loss: 0.00001503
Iteration 178/1000 | Loss: 0.00001503
Iteration 179/1000 | Loss: 0.00001503
Iteration 180/1000 | Loss: 0.00001503
Iteration 181/1000 | Loss: 0.00001502
Iteration 182/1000 | Loss: 0.00001502
Iteration 183/1000 | Loss: 0.00001502
Iteration 184/1000 | Loss: 0.00001502
Iteration 185/1000 | Loss: 0.00001502
Iteration 186/1000 | Loss: 0.00001502
Iteration 187/1000 | Loss: 0.00001502
Iteration 188/1000 | Loss: 0.00001502
Iteration 189/1000 | Loss: 0.00001502
Iteration 190/1000 | Loss: 0.00001502
Iteration 191/1000 | Loss: 0.00001502
Iteration 192/1000 | Loss: 0.00001502
Iteration 193/1000 | Loss: 0.00001502
Iteration 194/1000 | Loss: 0.00001502
Iteration 195/1000 | Loss: 0.00001502
Iteration 196/1000 | Loss: 0.00001502
Iteration 197/1000 | Loss: 0.00001502
Iteration 198/1000 | Loss: 0.00001502
Iteration 199/1000 | Loss: 0.00001502
Iteration 200/1000 | Loss: 0.00001502
Iteration 201/1000 | Loss: 0.00001502
Iteration 202/1000 | Loss: 0.00001502
Iteration 203/1000 | Loss: 0.00001502
Iteration 204/1000 | Loss: 0.00001502
Iteration 205/1000 | Loss: 0.00001502
Iteration 206/1000 | Loss: 0.00001502
Iteration 207/1000 | Loss: 0.00001502
Iteration 208/1000 | Loss: 0.00001502
Iteration 209/1000 | Loss: 0.00001502
Iteration 210/1000 | Loss: 0.00001502
Iteration 211/1000 | Loss: 0.00001502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.5019413694972172e-05, 1.5019413694972172e-05, 1.5019413694972172e-05, 1.5019413694972172e-05, 1.5019413694972172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5019413694972172e-05

Optimization complete. Final v2v error: 3.3072850704193115 mm

Highest mean error: 3.828735828399658 mm for frame 91

Lowest mean error: 3.0086476802825928 mm for frame 30

Saving results

Total time: 38.08895921707153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103001
Iteration 2/25 | Loss: 0.01103000
Iteration 3/25 | Loss: 0.01103000
Iteration 4/25 | Loss: 0.01103000
Iteration 5/25 | Loss: 0.01103000
Iteration 6/25 | Loss: 0.01103000
Iteration 7/25 | Loss: 0.01102999
Iteration 8/25 | Loss: 0.01102999
Iteration 9/25 | Loss: 0.01102999
Iteration 10/25 | Loss: 0.01102999
Iteration 11/25 | Loss: 0.01102998
Iteration 12/25 | Loss: 0.01102998
Iteration 13/25 | Loss: 0.01102998
Iteration 14/25 | Loss: 0.01102998
Iteration 15/25 | Loss: 0.01102998
Iteration 16/25 | Loss: 0.01102998
Iteration 17/25 | Loss: 0.01102998
Iteration 18/25 | Loss: 0.01102998
Iteration 19/25 | Loss: 0.01102997
Iteration 20/25 | Loss: 0.01102997
Iteration 21/25 | Loss: 0.01102997
Iteration 22/25 | Loss: 0.01102997
Iteration 23/25 | Loss: 0.01102997
Iteration 24/25 | Loss: 0.01102996
Iteration 25/25 | Loss: 0.01102996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11286426
Iteration 2/25 | Loss: 0.10439162
Iteration 3/25 | Loss: 0.10226271
Iteration 4/25 | Loss: 0.10113655
Iteration 5/25 | Loss: 0.10111181
Iteration 6/25 | Loss: 0.10111179
Iteration 7/25 | Loss: 0.10111179
Iteration 8/25 | Loss: 0.10111177
Iteration 9/25 | Loss: 0.10111177
Iteration 10/25 | Loss: 0.10111177
Iteration 11/25 | Loss: 0.10111177
Iteration 12/25 | Loss: 0.10111177
Iteration 13/25 | Loss: 0.10111177
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.1011117696762085, 0.1011117696762085, 0.1011117696762085, 0.1011117696762085, 0.1011117696762085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1011117696762085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10111177
Iteration 2/1000 | Loss: 0.00245632
Iteration 3/1000 | Loss: 0.00160611
Iteration 4/1000 | Loss: 0.00050680
Iteration 5/1000 | Loss: 0.00059235
Iteration 6/1000 | Loss: 0.00025429
Iteration 7/1000 | Loss: 0.00019345
Iteration 8/1000 | Loss: 0.00024287
Iteration 9/1000 | Loss: 0.00008738
Iteration 10/1000 | Loss: 0.00020814
Iteration 11/1000 | Loss: 0.00022170
Iteration 12/1000 | Loss: 0.00006648
Iteration 13/1000 | Loss: 0.00006825
Iteration 14/1000 | Loss: 0.00014130
Iteration 15/1000 | Loss: 0.00007568
Iteration 16/1000 | Loss: 0.00016576
Iteration 17/1000 | Loss: 0.00014577
Iteration 18/1000 | Loss: 0.00006305
Iteration 19/1000 | Loss: 0.00013023
Iteration 20/1000 | Loss: 0.00011462
Iteration 21/1000 | Loss: 0.00021480
Iteration 22/1000 | Loss: 0.00019580
Iteration 23/1000 | Loss: 0.00005523
Iteration 24/1000 | Loss: 0.00006659
Iteration 25/1000 | Loss: 0.00007117
Iteration 26/1000 | Loss: 0.00007210
Iteration 27/1000 | Loss: 0.00039179
Iteration 28/1000 | Loss: 0.00013424
Iteration 29/1000 | Loss: 0.00005239
Iteration 30/1000 | Loss: 0.00011587
Iteration 31/1000 | Loss: 0.00003924
Iteration 32/1000 | Loss: 0.00003669
Iteration 33/1000 | Loss: 0.00003997
Iteration 34/1000 | Loss: 0.00007370
Iteration 35/1000 | Loss: 0.00003829
Iteration 36/1000 | Loss: 0.00003454
Iteration 37/1000 | Loss: 0.00029356
Iteration 38/1000 | Loss: 0.00022791
Iteration 39/1000 | Loss: 0.00007686
Iteration 40/1000 | Loss: 0.00003429
Iteration 41/1000 | Loss: 0.00008485
Iteration 42/1000 | Loss: 0.00005058
Iteration 43/1000 | Loss: 0.00003223
Iteration 44/1000 | Loss: 0.00004819
Iteration 45/1000 | Loss: 0.00004287
Iteration 46/1000 | Loss: 0.00003428
Iteration 47/1000 | Loss: 0.00004849
Iteration 48/1000 | Loss: 0.00008426
Iteration 49/1000 | Loss: 0.00003066
Iteration 50/1000 | Loss: 0.00004418
Iteration 51/1000 | Loss: 0.00007341
Iteration 52/1000 | Loss: 0.00016819
Iteration 53/1000 | Loss: 0.00004277
Iteration 54/1000 | Loss: 0.00007101
Iteration 55/1000 | Loss: 0.00006151
Iteration 56/1000 | Loss: 0.00004231
Iteration 57/1000 | Loss: 0.00003032
Iteration 58/1000 | Loss: 0.00004745
Iteration 59/1000 | Loss: 0.00003048
Iteration 60/1000 | Loss: 0.00009037
Iteration 61/1000 | Loss: 0.00002988
Iteration 62/1000 | Loss: 0.00003321
Iteration 63/1000 | Loss: 0.00003058
Iteration 64/1000 | Loss: 0.00002997
Iteration 65/1000 | Loss: 0.00006123
Iteration 66/1000 | Loss: 0.00012030
Iteration 67/1000 | Loss: 0.00003838
Iteration 68/1000 | Loss: 0.00006080
Iteration 69/1000 | Loss: 0.00004166
Iteration 70/1000 | Loss: 0.00003277
Iteration 71/1000 | Loss: 0.00003799
Iteration 72/1000 | Loss: 0.00004646
Iteration 73/1000 | Loss: 0.00003792
Iteration 74/1000 | Loss: 0.00003661
Iteration 75/1000 | Loss: 0.00003288
Iteration 76/1000 | Loss: 0.00002969
Iteration 77/1000 | Loss: 0.00002995
Iteration 78/1000 | Loss: 0.00004358
Iteration 79/1000 | Loss: 0.00003080
Iteration 80/1000 | Loss: 0.00002985
Iteration 81/1000 | Loss: 0.00003577
Iteration 82/1000 | Loss: 0.00002981
Iteration 83/1000 | Loss: 0.00003438
Iteration 84/1000 | Loss: 0.00007066
Iteration 85/1000 | Loss: 0.00003151
Iteration 86/1000 | Loss: 0.00003263
Iteration 87/1000 | Loss: 0.00005563
Iteration 88/1000 | Loss: 0.00003037
Iteration 89/1000 | Loss: 0.00008059
Iteration 90/1000 | Loss: 0.00003011
Iteration 91/1000 | Loss: 0.00003451
Iteration 92/1000 | Loss: 0.00015680
Iteration 93/1000 | Loss: 0.00008870
Iteration 94/1000 | Loss: 0.00004015
Iteration 95/1000 | Loss: 0.00004041
Iteration 96/1000 | Loss: 0.00002910
Iteration 97/1000 | Loss: 0.00002910
Iteration 98/1000 | Loss: 0.00002910
Iteration 99/1000 | Loss: 0.00002910
Iteration 100/1000 | Loss: 0.00002910
Iteration 101/1000 | Loss: 0.00002910
Iteration 102/1000 | Loss: 0.00002910
Iteration 103/1000 | Loss: 0.00002909
Iteration 104/1000 | Loss: 0.00002909
Iteration 105/1000 | Loss: 0.00002909
Iteration 106/1000 | Loss: 0.00002909
Iteration 107/1000 | Loss: 0.00002909
Iteration 108/1000 | Loss: 0.00002909
Iteration 109/1000 | Loss: 0.00002909
Iteration 110/1000 | Loss: 0.00002909
Iteration 111/1000 | Loss: 0.00002909
Iteration 112/1000 | Loss: 0.00002908
Iteration 113/1000 | Loss: 0.00002908
Iteration 114/1000 | Loss: 0.00003009
Iteration 115/1000 | Loss: 0.00002949
Iteration 116/1000 | Loss: 0.00002906
Iteration 117/1000 | Loss: 0.00002906
Iteration 118/1000 | Loss: 0.00002906
Iteration 119/1000 | Loss: 0.00002905
Iteration 120/1000 | Loss: 0.00002905
Iteration 121/1000 | Loss: 0.00002905
Iteration 122/1000 | Loss: 0.00002905
Iteration 123/1000 | Loss: 0.00002905
Iteration 124/1000 | Loss: 0.00002904
Iteration 125/1000 | Loss: 0.00002904
Iteration 126/1000 | Loss: 0.00002904
Iteration 127/1000 | Loss: 0.00002903
Iteration 128/1000 | Loss: 0.00002903
Iteration 129/1000 | Loss: 0.00002903
Iteration 130/1000 | Loss: 0.00002903
Iteration 131/1000 | Loss: 0.00002903
Iteration 132/1000 | Loss: 0.00002903
Iteration 133/1000 | Loss: 0.00002903
Iteration 134/1000 | Loss: 0.00002903
Iteration 135/1000 | Loss: 0.00002903
Iteration 136/1000 | Loss: 0.00002903
Iteration 137/1000 | Loss: 0.00009446
Iteration 138/1000 | Loss: 0.00006160
Iteration 139/1000 | Loss: 0.00003212
Iteration 140/1000 | Loss: 0.00003021
Iteration 141/1000 | Loss: 0.00002975
Iteration 142/1000 | Loss: 0.00002900
Iteration 143/1000 | Loss: 0.00002900
Iteration 144/1000 | Loss: 0.00002900
Iteration 145/1000 | Loss: 0.00002974
Iteration 146/1000 | Loss: 0.00002974
Iteration 147/1000 | Loss: 0.00004323
Iteration 148/1000 | Loss: 0.00002896
Iteration 149/1000 | Loss: 0.00002894
Iteration 150/1000 | Loss: 0.00002891
Iteration 151/1000 | Loss: 0.00002891
Iteration 152/1000 | Loss: 0.00002891
Iteration 153/1000 | Loss: 0.00002891
Iteration 154/1000 | Loss: 0.00002891
Iteration 155/1000 | Loss: 0.00002891
Iteration 156/1000 | Loss: 0.00002891
Iteration 157/1000 | Loss: 0.00002891
Iteration 158/1000 | Loss: 0.00002891
Iteration 159/1000 | Loss: 0.00002891
Iteration 160/1000 | Loss: 0.00002891
Iteration 161/1000 | Loss: 0.00002890
Iteration 162/1000 | Loss: 0.00002890
Iteration 163/1000 | Loss: 0.00002890
Iteration 164/1000 | Loss: 0.00002890
Iteration 165/1000 | Loss: 0.00002890
Iteration 166/1000 | Loss: 0.00002890
Iteration 167/1000 | Loss: 0.00002890
Iteration 168/1000 | Loss: 0.00002890
Iteration 169/1000 | Loss: 0.00002890
Iteration 170/1000 | Loss: 0.00002890
Iteration 171/1000 | Loss: 0.00002890
Iteration 172/1000 | Loss: 0.00002890
Iteration 173/1000 | Loss: 0.00002889
Iteration 174/1000 | Loss: 0.00002889
Iteration 175/1000 | Loss: 0.00002889
Iteration 176/1000 | Loss: 0.00002889
Iteration 177/1000 | Loss: 0.00002889
Iteration 178/1000 | Loss: 0.00002889
Iteration 179/1000 | Loss: 0.00002889
Iteration 180/1000 | Loss: 0.00002889
Iteration 181/1000 | Loss: 0.00002889
Iteration 182/1000 | Loss: 0.00002889
Iteration 183/1000 | Loss: 0.00007811
Iteration 184/1000 | Loss: 0.00005466
Iteration 185/1000 | Loss: 0.00002944
Iteration 186/1000 | Loss: 0.00005785
Iteration 187/1000 | Loss: 0.00002892
Iteration 188/1000 | Loss: 0.00002896
Iteration 189/1000 | Loss: 0.00002895
Iteration 190/1000 | Loss: 0.00002873
Iteration 191/1000 | Loss: 0.00002865
Iteration 192/1000 | Loss: 0.00002865
Iteration 193/1000 | Loss: 0.00002863
Iteration 194/1000 | Loss: 0.00002863
Iteration 195/1000 | Loss: 0.00002853
Iteration 196/1000 | Loss: 0.00002851
Iteration 197/1000 | Loss: 0.00006894
Iteration 198/1000 | Loss: 0.00002846
Iteration 199/1000 | Loss: 0.00016526
Iteration 200/1000 | Loss: 0.00003232
Iteration 201/1000 | Loss: 0.00004375
Iteration 202/1000 | Loss: 0.00003261
Iteration 203/1000 | Loss: 0.00007892
Iteration 204/1000 | Loss: 0.00004244
Iteration 205/1000 | Loss: 0.00006679
Iteration 206/1000 | Loss: 0.00003357
Iteration 207/1000 | Loss: 0.00007728
Iteration 208/1000 | Loss: 0.00004729
Iteration 209/1000 | Loss: 0.00005650
Iteration 210/1000 | Loss: 0.00003622
Iteration 211/1000 | Loss: 0.00004867
Iteration 212/1000 | Loss: 0.00011039
Iteration 213/1000 | Loss: 0.00004684
Iteration 214/1000 | Loss: 0.00015481
Iteration 215/1000 | Loss: 0.00004900
Iteration 216/1000 | Loss: 0.00007998
Iteration 217/1000 | Loss: 0.00005048
Iteration 218/1000 | Loss: 0.00010878
Iteration 219/1000 | Loss: 0.00005034
Iteration 220/1000 | Loss: 0.00004125
Iteration 221/1000 | Loss: 0.00003568
Iteration 222/1000 | Loss: 0.00004502
Iteration 223/1000 | Loss: 0.00004980
Iteration 224/1000 | Loss: 0.00003256
Iteration 225/1000 | Loss: 0.00003197
Iteration 226/1000 | Loss: 0.00003998
Iteration 227/1000 | Loss: 0.00003030
Iteration 228/1000 | Loss: 0.00003211
Iteration 229/1000 | Loss: 0.00002861
Iteration 230/1000 | Loss: 0.00002819
Iteration 231/1000 | Loss: 0.00002818
Iteration 232/1000 | Loss: 0.00002818
Iteration 233/1000 | Loss: 0.00002817
Iteration 234/1000 | Loss: 0.00003041
Iteration 235/1000 | Loss: 0.00005078
Iteration 236/1000 | Loss: 0.00002816
Iteration 237/1000 | Loss: 0.00002802
Iteration 238/1000 | Loss: 0.00002797
Iteration 239/1000 | Loss: 0.00002797
Iteration 240/1000 | Loss: 0.00002797
Iteration 241/1000 | Loss: 0.00002797
Iteration 242/1000 | Loss: 0.00002797
Iteration 243/1000 | Loss: 0.00002797
Iteration 244/1000 | Loss: 0.00002797
Iteration 245/1000 | Loss: 0.00002797
Iteration 246/1000 | Loss: 0.00002797
Iteration 247/1000 | Loss: 0.00002797
Iteration 248/1000 | Loss: 0.00002797
Iteration 249/1000 | Loss: 0.00002797
Iteration 250/1000 | Loss: 0.00002797
Iteration 251/1000 | Loss: 0.00002797
Iteration 252/1000 | Loss: 0.00002797
Iteration 253/1000 | Loss: 0.00002797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [2.7968369977315888e-05, 2.7968369977315888e-05, 2.7968369977315888e-05, 2.7968369977315888e-05, 2.7968369977315888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7968369977315888e-05

Optimization complete. Final v2v error: 4.219375133514404 mm

Highest mean error: 15.749542236328125 mm for frame 59

Lowest mean error: 3.4124159812927246 mm for frame 149

Saving results

Total time: 249.45165586471558
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997058
Iteration 2/25 | Loss: 0.00312263
Iteration 3/25 | Loss: 0.00191886
Iteration 4/25 | Loss: 0.00158826
Iteration 5/25 | Loss: 0.00131260
Iteration 6/25 | Loss: 0.00112973
Iteration 7/25 | Loss: 0.00097875
Iteration 8/25 | Loss: 0.00089401
Iteration 9/25 | Loss: 0.00084206
Iteration 10/25 | Loss: 0.00081234
Iteration 11/25 | Loss: 0.00079442
Iteration 12/25 | Loss: 0.00078907
Iteration 13/25 | Loss: 0.00078337
Iteration 14/25 | Loss: 0.00077922
Iteration 15/25 | Loss: 0.00077829
Iteration 16/25 | Loss: 0.00077662
Iteration 17/25 | Loss: 0.00077622
Iteration 18/25 | Loss: 0.00077309
Iteration 19/25 | Loss: 0.00077268
Iteration 20/25 | Loss: 0.00077148
Iteration 21/25 | Loss: 0.00077309
Iteration 22/25 | Loss: 0.00076873
Iteration 23/25 | Loss: 0.00076812
Iteration 24/25 | Loss: 0.00076800
Iteration 25/25 | Loss: 0.00076786

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42022932
Iteration 2/25 | Loss: 0.00084760
Iteration 3/25 | Loss: 0.00071200
Iteration 4/25 | Loss: 0.00071200
Iteration 5/25 | Loss: 0.00071200
Iteration 6/25 | Loss: 0.00071200
Iteration 7/25 | Loss: 0.00071200
Iteration 8/25 | Loss: 0.00071200
Iteration 9/25 | Loss: 0.00071200
Iteration 10/25 | Loss: 0.00071200
Iteration 11/25 | Loss: 0.00071200
Iteration 12/25 | Loss: 0.00071200
Iteration 13/25 | Loss: 0.00071200
Iteration 14/25 | Loss: 0.00071200
Iteration 15/25 | Loss: 0.00071200
Iteration 16/25 | Loss: 0.00071200
Iteration 17/25 | Loss: 0.00071200
Iteration 18/25 | Loss: 0.00071200
Iteration 19/25 | Loss: 0.00071200
Iteration 20/25 | Loss: 0.00071200
Iteration 21/25 | Loss: 0.00071200
Iteration 22/25 | Loss: 0.00071200
Iteration 23/25 | Loss: 0.00071200
Iteration 24/25 | Loss: 0.00071200
Iteration 25/25 | Loss: 0.00071200

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071200
Iteration 2/1000 | Loss: 0.00024235
Iteration 3/1000 | Loss: 0.00120375
Iteration 4/1000 | Loss: 0.00058893
Iteration 5/1000 | Loss: 0.00059894
Iteration 6/1000 | Loss: 0.00019434
Iteration 7/1000 | Loss: 0.00076539
Iteration 8/1000 | Loss: 0.00009816
Iteration 9/1000 | Loss: 0.00023423
Iteration 10/1000 | Loss: 0.00009431
Iteration 11/1000 | Loss: 0.00014126
Iteration 12/1000 | Loss: 0.00017291
Iteration 13/1000 | Loss: 0.00017053
Iteration 14/1000 | Loss: 0.00013750
Iteration 15/1000 | Loss: 0.00018722
Iteration 16/1000 | Loss: 0.00012674
Iteration 17/1000 | Loss: 0.00028917
Iteration 18/1000 | Loss: 0.00231785
Iteration 19/1000 | Loss: 0.00257967
Iteration 20/1000 | Loss: 0.00066174
Iteration 21/1000 | Loss: 0.00382950
Iteration 22/1000 | Loss: 0.00177162
Iteration 23/1000 | Loss: 0.00148120
Iteration 24/1000 | Loss: 0.00097248
Iteration 25/1000 | Loss: 0.00121127
Iteration 26/1000 | Loss: 0.00079929
Iteration 27/1000 | Loss: 0.00098426
Iteration 28/1000 | Loss: 0.00073823
Iteration 29/1000 | Loss: 0.00046659
Iteration 30/1000 | Loss: 0.00075238
Iteration 31/1000 | Loss: 0.00022700
Iteration 32/1000 | Loss: 0.00031487
Iteration 33/1000 | Loss: 0.00022876
Iteration 34/1000 | Loss: 0.00027456
Iteration 35/1000 | Loss: 0.00017219
Iteration 36/1000 | Loss: 0.00032792
Iteration 37/1000 | Loss: 0.00062305
Iteration 38/1000 | Loss: 0.00045931
Iteration 39/1000 | Loss: 0.00067848
Iteration 40/1000 | Loss: 0.00040446
Iteration 41/1000 | Loss: 0.00040666
Iteration 42/1000 | Loss: 0.00025032
Iteration 43/1000 | Loss: 0.00033594
Iteration 44/1000 | Loss: 0.00006850
Iteration 45/1000 | Loss: 0.00012174
Iteration 46/1000 | Loss: 0.00007463
Iteration 47/1000 | Loss: 0.00012313
Iteration 48/1000 | Loss: 0.00008105
Iteration 49/1000 | Loss: 0.00005407
Iteration 50/1000 | Loss: 0.00040746
Iteration 51/1000 | Loss: 0.00019039
Iteration 52/1000 | Loss: 0.00047193
Iteration 53/1000 | Loss: 0.00026621
Iteration 54/1000 | Loss: 0.00037450
Iteration 55/1000 | Loss: 0.00045712
Iteration 56/1000 | Loss: 0.00025872
Iteration 57/1000 | Loss: 0.00072732
Iteration 58/1000 | Loss: 0.00023391
Iteration 59/1000 | Loss: 0.00048243
Iteration 60/1000 | Loss: 0.00031666
Iteration 61/1000 | Loss: 0.00047695
Iteration 62/1000 | Loss: 0.00015348
Iteration 63/1000 | Loss: 0.00017234
Iteration 64/1000 | Loss: 0.00013323
Iteration 65/1000 | Loss: 0.00041645
Iteration 66/1000 | Loss: 0.00031350
Iteration 67/1000 | Loss: 0.00063236
Iteration 68/1000 | Loss: 0.00073833
Iteration 69/1000 | Loss: 0.00080151
Iteration 70/1000 | Loss: 0.00268729
Iteration 71/1000 | Loss: 0.00298850
Iteration 72/1000 | Loss: 0.00320233
Iteration 73/1000 | Loss: 0.00207832
Iteration 74/1000 | Loss: 0.00107509
Iteration 75/1000 | Loss: 0.00032578
Iteration 76/1000 | Loss: 0.00031302
Iteration 77/1000 | Loss: 0.00023149
Iteration 78/1000 | Loss: 0.00032439
Iteration 79/1000 | Loss: 0.00024592
Iteration 80/1000 | Loss: 0.00028233
Iteration 81/1000 | Loss: 0.00030459
Iteration 82/1000 | Loss: 0.00033681
Iteration 83/1000 | Loss: 0.00027458
Iteration 84/1000 | Loss: 0.00031542
Iteration 85/1000 | Loss: 0.00033354
Iteration 86/1000 | Loss: 0.00020408
Iteration 87/1000 | Loss: 0.00021398
Iteration 88/1000 | Loss: 0.00010068
Iteration 89/1000 | Loss: 0.00018943
Iteration 90/1000 | Loss: 0.00010919
Iteration 91/1000 | Loss: 0.00031239
Iteration 92/1000 | Loss: 0.00022703
Iteration 93/1000 | Loss: 0.00012285
Iteration 94/1000 | Loss: 0.00033229
Iteration 95/1000 | Loss: 0.00225138
Iteration 96/1000 | Loss: 0.00037305
Iteration 97/1000 | Loss: 0.00022462
Iteration 98/1000 | Loss: 0.00014015
Iteration 99/1000 | Loss: 0.00014375
Iteration 100/1000 | Loss: 0.00026007
Iteration 101/1000 | Loss: 0.00024411
Iteration 102/1000 | Loss: 0.00042317
Iteration 103/1000 | Loss: 0.00014459
Iteration 104/1000 | Loss: 0.00032756
Iteration 105/1000 | Loss: 0.00011427
Iteration 106/1000 | Loss: 0.00020309
Iteration 107/1000 | Loss: 0.00018615
Iteration 108/1000 | Loss: 0.00014793
Iteration 109/1000 | Loss: 0.00032938
Iteration 110/1000 | Loss: 0.00022950
Iteration 111/1000 | Loss: 0.00055117
Iteration 112/1000 | Loss: 0.00029905
Iteration 113/1000 | Loss: 0.00021351
Iteration 114/1000 | Loss: 0.00015164
Iteration 115/1000 | Loss: 0.00023135
Iteration 116/1000 | Loss: 0.00018770
Iteration 117/1000 | Loss: 0.00018294
Iteration 118/1000 | Loss: 0.00014823
Iteration 119/1000 | Loss: 0.00016456
Iteration 120/1000 | Loss: 0.00069383
Iteration 121/1000 | Loss: 0.00005741
Iteration 122/1000 | Loss: 0.00010086
Iteration 123/1000 | Loss: 0.00004825
Iteration 124/1000 | Loss: 0.00007653
Iteration 125/1000 | Loss: 0.00009112
Iteration 126/1000 | Loss: 0.00010175
Iteration 127/1000 | Loss: 0.00014564
Iteration 128/1000 | Loss: 0.00009478
Iteration 129/1000 | Loss: 0.00015204
Iteration 130/1000 | Loss: 0.00009377
Iteration 131/1000 | Loss: 0.00017010
Iteration 132/1000 | Loss: 0.00024884
Iteration 133/1000 | Loss: 0.00026656
Iteration 134/1000 | Loss: 0.00010719
Iteration 135/1000 | Loss: 0.00020464
Iteration 136/1000 | Loss: 0.00036936
Iteration 137/1000 | Loss: 0.00019987
Iteration 138/1000 | Loss: 0.00011471
Iteration 139/1000 | Loss: 0.00023495
Iteration 140/1000 | Loss: 0.00025981
Iteration 141/1000 | Loss: 0.00017212
Iteration 142/1000 | Loss: 0.00021772
Iteration 143/1000 | Loss: 0.00138255
Iteration 144/1000 | Loss: 0.00006182
Iteration 145/1000 | Loss: 0.00025200
Iteration 146/1000 | Loss: 0.00016611
Iteration 147/1000 | Loss: 0.00014978
Iteration 148/1000 | Loss: 0.00011672
Iteration 149/1000 | Loss: 0.00017189
Iteration 150/1000 | Loss: 0.00011482
Iteration 151/1000 | Loss: 0.00016496
Iteration 152/1000 | Loss: 0.00013989
Iteration 153/1000 | Loss: 0.00014222
Iteration 154/1000 | Loss: 0.00012410
Iteration 155/1000 | Loss: 0.00010342
Iteration 156/1000 | Loss: 0.00009226
Iteration 157/1000 | Loss: 0.00011940
Iteration 158/1000 | Loss: 0.00008769
Iteration 159/1000 | Loss: 0.00017297
Iteration 160/1000 | Loss: 0.00013459
Iteration 161/1000 | Loss: 0.00009473
Iteration 162/1000 | Loss: 0.00031016
Iteration 163/1000 | Loss: 0.00053449
Iteration 164/1000 | Loss: 0.00014279
Iteration 165/1000 | Loss: 0.00028744
Iteration 166/1000 | Loss: 0.00011066
Iteration 167/1000 | Loss: 0.00017066
Iteration 168/1000 | Loss: 0.00007106
Iteration 169/1000 | Loss: 0.00009211
Iteration 170/1000 | Loss: 0.00010436
Iteration 171/1000 | Loss: 0.00008966
Iteration 172/1000 | Loss: 0.00015022
Iteration 173/1000 | Loss: 0.00009121
Iteration 174/1000 | Loss: 0.00020825
Iteration 175/1000 | Loss: 0.00009583
Iteration 176/1000 | Loss: 0.00010987
Iteration 177/1000 | Loss: 0.00010326
Iteration 178/1000 | Loss: 0.00038609
Iteration 179/1000 | Loss: 0.00014540
Iteration 180/1000 | Loss: 0.00034388
Iteration 181/1000 | Loss: 0.00019332
Iteration 182/1000 | Loss: 0.00036169
Iteration 183/1000 | Loss: 0.00010541
Iteration 184/1000 | Loss: 0.00006746
Iteration 185/1000 | Loss: 0.00009546
Iteration 186/1000 | Loss: 0.00008296
Iteration 187/1000 | Loss: 0.00008769
Iteration 188/1000 | Loss: 0.00009633
Iteration 189/1000 | Loss: 0.00013721
Iteration 190/1000 | Loss: 0.00005992
Iteration 191/1000 | Loss: 0.00005558
Iteration 192/1000 | Loss: 0.00004146
Iteration 193/1000 | Loss: 0.00005210
Iteration 194/1000 | Loss: 0.00003906
Iteration 195/1000 | Loss: 0.00104156
Iteration 196/1000 | Loss: 0.00092640
Iteration 197/1000 | Loss: 0.00020199
Iteration 198/1000 | Loss: 0.00028949
Iteration 199/1000 | Loss: 0.00047404
Iteration 200/1000 | Loss: 0.00009098
Iteration 201/1000 | Loss: 0.00008182
Iteration 202/1000 | Loss: 0.00003564
Iteration 203/1000 | Loss: 0.00012654
Iteration 204/1000 | Loss: 0.00007032
Iteration 205/1000 | Loss: 0.00011149
Iteration 206/1000 | Loss: 0.00004475
Iteration 207/1000 | Loss: 0.00015307
Iteration 208/1000 | Loss: 0.00002707
Iteration 209/1000 | Loss: 0.00003115
Iteration 210/1000 | Loss: 0.00002379
Iteration 211/1000 | Loss: 0.00003956
Iteration 212/1000 | Loss: 0.00002148
Iteration 213/1000 | Loss: 0.00002227
Iteration 214/1000 | Loss: 0.00002092
Iteration 215/1000 | Loss: 0.00002065
Iteration 216/1000 | Loss: 0.00002559
Iteration 217/1000 | Loss: 0.00002040
Iteration 218/1000 | Loss: 0.00002028
Iteration 219/1000 | Loss: 0.00002027
Iteration 220/1000 | Loss: 0.00002025
Iteration 221/1000 | Loss: 0.00002024
Iteration 222/1000 | Loss: 0.00002020
Iteration 223/1000 | Loss: 0.00002016
Iteration 224/1000 | Loss: 0.00002015
Iteration 225/1000 | Loss: 0.00002014
Iteration 226/1000 | Loss: 0.00002014
Iteration 227/1000 | Loss: 0.00002013
Iteration 228/1000 | Loss: 0.00002012
Iteration 229/1000 | Loss: 0.00002012
Iteration 230/1000 | Loss: 0.00002011
Iteration 231/1000 | Loss: 0.00002011
Iteration 232/1000 | Loss: 0.00002010
Iteration 233/1000 | Loss: 0.00002008
Iteration 234/1000 | Loss: 0.00002008
Iteration 235/1000 | Loss: 0.00002007
Iteration 236/1000 | Loss: 0.00002007
Iteration 237/1000 | Loss: 0.00002007
Iteration 238/1000 | Loss: 0.00002007
Iteration 239/1000 | Loss: 0.00002007
Iteration 240/1000 | Loss: 0.00002007
Iteration 241/1000 | Loss: 0.00002007
Iteration 242/1000 | Loss: 0.00002007
Iteration 243/1000 | Loss: 0.00002007
Iteration 244/1000 | Loss: 0.00002007
Iteration 245/1000 | Loss: 0.00002007
Iteration 246/1000 | Loss: 0.00002007
Iteration 247/1000 | Loss: 0.00002007
Iteration 248/1000 | Loss: 0.00002006
Iteration 249/1000 | Loss: 0.00002006
Iteration 250/1000 | Loss: 0.00002006
Iteration 251/1000 | Loss: 0.00002006
Iteration 252/1000 | Loss: 0.00002006
Iteration 253/1000 | Loss: 0.00002006
Iteration 254/1000 | Loss: 0.00002006
Iteration 255/1000 | Loss: 0.00002006
Iteration 256/1000 | Loss: 0.00002006
Iteration 257/1000 | Loss: 0.00002005
Iteration 258/1000 | Loss: 0.00002005
Iteration 259/1000 | Loss: 0.00002005
Iteration 260/1000 | Loss: 0.00002005
Iteration 261/1000 | Loss: 0.00002005
Iteration 262/1000 | Loss: 0.00002005
Iteration 263/1000 | Loss: 0.00002005
Iteration 264/1000 | Loss: 0.00002005
Iteration 265/1000 | Loss: 0.00002005
Iteration 266/1000 | Loss: 0.00002005
Iteration 267/1000 | Loss: 0.00002005
Iteration 268/1000 | Loss: 0.00002005
Iteration 269/1000 | Loss: 0.00002005
Iteration 270/1000 | Loss: 0.00002005
Iteration 271/1000 | Loss: 0.00002005
Iteration 272/1000 | Loss: 0.00002005
Iteration 273/1000 | Loss: 0.00002005
Iteration 274/1000 | Loss: 0.00002004
Iteration 275/1000 | Loss: 0.00002004
Iteration 276/1000 | Loss: 0.00002004
Iteration 277/1000 | Loss: 0.00002004
Iteration 278/1000 | Loss: 0.00002004
Iteration 279/1000 | Loss: 0.00002004
Iteration 280/1000 | Loss: 0.00002004
Iteration 281/1000 | Loss: 0.00002003
Iteration 282/1000 | Loss: 0.00002003
Iteration 283/1000 | Loss: 0.00002003
Iteration 284/1000 | Loss: 0.00002002
Iteration 285/1000 | Loss: 0.00002002
Iteration 286/1000 | Loss: 0.00002002
Iteration 287/1000 | Loss: 0.00002001
Iteration 288/1000 | Loss: 0.00002001
Iteration 289/1000 | Loss: 0.00002001
Iteration 290/1000 | Loss: 0.00002001
Iteration 291/1000 | Loss: 0.00002001
Iteration 292/1000 | Loss: 0.00002001
Iteration 293/1000 | Loss: 0.00002001
Iteration 294/1000 | Loss: 0.00002001
Iteration 295/1000 | Loss: 0.00002001
Iteration 296/1000 | Loss: 0.00002001
Iteration 297/1000 | Loss: 0.00002001
Iteration 298/1000 | Loss: 0.00002001
Iteration 299/1000 | Loss: 0.00002001
Iteration 300/1000 | Loss: 0.00002000
Iteration 301/1000 | Loss: 0.00002000
Iteration 302/1000 | Loss: 0.00002000
Iteration 303/1000 | Loss: 0.00002000
Iteration 304/1000 | Loss: 0.00002000
Iteration 305/1000 | Loss: 0.00002000
Iteration 306/1000 | Loss: 0.00002000
Iteration 307/1000 | Loss: 0.00002000
Iteration 308/1000 | Loss: 0.00002000
Iteration 309/1000 | Loss: 0.00002000
Iteration 310/1000 | Loss: 0.00002000
Iteration 311/1000 | Loss: 0.00002000
Iteration 312/1000 | Loss: 0.00002000
Iteration 313/1000 | Loss: 0.00001999
Iteration 314/1000 | Loss: 0.00001999
Iteration 315/1000 | Loss: 0.00001999
Iteration 316/1000 | Loss: 0.00001999
Iteration 317/1000 | Loss: 0.00001999
Iteration 318/1000 | Loss: 0.00001999
Iteration 319/1000 | Loss: 0.00001999
Iteration 320/1000 | Loss: 0.00001999
Iteration 321/1000 | Loss: 0.00001999
Iteration 322/1000 | Loss: 0.00001999
Iteration 323/1000 | Loss: 0.00001999
Iteration 324/1000 | Loss: 0.00001999
Iteration 325/1000 | Loss: 0.00001999
Iteration 326/1000 | Loss: 0.00001999
Iteration 327/1000 | Loss: 0.00001999
Iteration 328/1000 | Loss: 0.00001999
Iteration 329/1000 | Loss: 0.00001999
Iteration 330/1000 | Loss: 0.00001998
Iteration 331/1000 | Loss: 0.00001998
Iteration 332/1000 | Loss: 0.00001998
Iteration 333/1000 | Loss: 0.00001998
Iteration 334/1000 | Loss: 0.00001998
Iteration 335/1000 | Loss: 0.00001998
Iteration 336/1000 | Loss: 0.00001998
Iteration 337/1000 | Loss: 0.00001998
Iteration 338/1000 | Loss: 0.00001998
Iteration 339/1000 | Loss: 0.00001998
Iteration 340/1000 | Loss: 0.00001998
Iteration 341/1000 | Loss: 0.00001998
Iteration 342/1000 | Loss: 0.00001998
Iteration 343/1000 | Loss: 0.00001998
Iteration 344/1000 | Loss: 0.00001998
Iteration 345/1000 | Loss: 0.00001998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 345. Stopping optimization.
Last 5 losses: [1.9981333025498316e-05, 1.9981333025498316e-05, 1.9981333025498316e-05, 1.9981333025498316e-05, 1.9981333025498316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9981333025498316e-05

Optimization complete. Final v2v error: 3.5977587699890137 mm

Highest mean error: 13.015369415283203 mm for frame 78

Lowest mean error: 3.241497039794922 mm for frame 10

Saving results

Total time: 415.7020626068115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499004
Iteration 2/25 | Loss: 0.00093594
Iteration 3/25 | Loss: 0.00079752
Iteration 4/25 | Loss: 0.00076565
Iteration 5/25 | Loss: 0.00075202
Iteration 6/25 | Loss: 0.00074949
Iteration 7/25 | Loss: 0.00074871
Iteration 8/25 | Loss: 0.00074867
Iteration 9/25 | Loss: 0.00074867
Iteration 10/25 | Loss: 0.00074867
Iteration 11/25 | Loss: 0.00074867
Iteration 12/25 | Loss: 0.00074867
Iteration 13/25 | Loss: 0.00074867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007486724643968046, 0.0007486724643968046, 0.0007486724643968046, 0.0007486724643968046, 0.0007486724643968046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007486724643968046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34886026
Iteration 2/25 | Loss: 0.00024753
Iteration 3/25 | Loss: 0.00024753
Iteration 4/25 | Loss: 0.00024752
Iteration 5/25 | Loss: 0.00024752
Iteration 6/25 | Loss: 0.00024752
Iteration 7/25 | Loss: 0.00024752
Iteration 8/25 | Loss: 0.00024752
Iteration 9/25 | Loss: 0.00024752
Iteration 10/25 | Loss: 0.00024752
Iteration 11/25 | Loss: 0.00024752
Iteration 12/25 | Loss: 0.00024752
Iteration 13/25 | Loss: 0.00024752
Iteration 14/25 | Loss: 0.00024752
Iteration 15/25 | Loss: 0.00024752
Iteration 16/25 | Loss: 0.00024752
Iteration 17/25 | Loss: 0.00024752
Iteration 18/25 | Loss: 0.00024752
Iteration 19/25 | Loss: 0.00024752
Iteration 20/25 | Loss: 0.00024752
Iteration 21/25 | Loss: 0.00024752
Iteration 22/25 | Loss: 0.00024752
Iteration 23/25 | Loss: 0.00024752
Iteration 24/25 | Loss: 0.00024752
Iteration 25/25 | Loss: 0.00024752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024752
Iteration 2/1000 | Loss: 0.00005674
Iteration 3/1000 | Loss: 0.00004600
Iteration 4/1000 | Loss: 0.00004225
Iteration 5/1000 | Loss: 0.00003999
Iteration 6/1000 | Loss: 0.00003844
Iteration 7/1000 | Loss: 0.00003751
Iteration 8/1000 | Loss: 0.00003698
Iteration 9/1000 | Loss: 0.00003663
Iteration 10/1000 | Loss: 0.00003646
Iteration 11/1000 | Loss: 0.00003639
Iteration 12/1000 | Loss: 0.00003634
Iteration 13/1000 | Loss: 0.00003632
Iteration 14/1000 | Loss: 0.00003632
Iteration 15/1000 | Loss: 0.00003630
Iteration 16/1000 | Loss: 0.00003625
Iteration 17/1000 | Loss: 0.00003625
Iteration 18/1000 | Loss: 0.00003624
Iteration 19/1000 | Loss: 0.00003620
Iteration 20/1000 | Loss: 0.00003617
Iteration 21/1000 | Loss: 0.00003616
Iteration 22/1000 | Loss: 0.00003615
Iteration 23/1000 | Loss: 0.00003615
Iteration 24/1000 | Loss: 0.00003614
Iteration 25/1000 | Loss: 0.00003614
Iteration 26/1000 | Loss: 0.00003614
Iteration 27/1000 | Loss: 0.00003613
Iteration 28/1000 | Loss: 0.00003613
Iteration 29/1000 | Loss: 0.00003613
Iteration 30/1000 | Loss: 0.00003613
Iteration 31/1000 | Loss: 0.00003612
Iteration 32/1000 | Loss: 0.00003612
Iteration 33/1000 | Loss: 0.00003611
Iteration 34/1000 | Loss: 0.00003609
Iteration 35/1000 | Loss: 0.00003609
Iteration 36/1000 | Loss: 0.00003609
Iteration 37/1000 | Loss: 0.00003609
Iteration 38/1000 | Loss: 0.00003609
Iteration 39/1000 | Loss: 0.00003609
Iteration 40/1000 | Loss: 0.00003609
Iteration 41/1000 | Loss: 0.00003609
Iteration 42/1000 | Loss: 0.00003608
Iteration 43/1000 | Loss: 0.00003608
Iteration 44/1000 | Loss: 0.00003608
Iteration 45/1000 | Loss: 0.00003608
Iteration 46/1000 | Loss: 0.00003607
Iteration 47/1000 | Loss: 0.00003607
Iteration 48/1000 | Loss: 0.00003607
Iteration 49/1000 | Loss: 0.00003606
Iteration 50/1000 | Loss: 0.00003606
Iteration 51/1000 | Loss: 0.00003606
Iteration 52/1000 | Loss: 0.00003605
Iteration 53/1000 | Loss: 0.00003605
Iteration 54/1000 | Loss: 0.00003605
Iteration 55/1000 | Loss: 0.00003605
Iteration 56/1000 | Loss: 0.00003604
Iteration 57/1000 | Loss: 0.00003604
Iteration 58/1000 | Loss: 0.00003604
Iteration 59/1000 | Loss: 0.00003604
Iteration 60/1000 | Loss: 0.00003604
Iteration 61/1000 | Loss: 0.00003604
Iteration 62/1000 | Loss: 0.00003604
Iteration 63/1000 | Loss: 0.00003603
Iteration 64/1000 | Loss: 0.00003603
Iteration 65/1000 | Loss: 0.00003603
Iteration 66/1000 | Loss: 0.00003603
Iteration 67/1000 | Loss: 0.00003603
Iteration 68/1000 | Loss: 0.00003602
Iteration 69/1000 | Loss: 0.00003602
Iteration 70/1000 | Loss: 0.00003602
Iteration 71/1000 | Loss: 0.00003601
Iteration 72/1000 | Loss: 0.00003601
Iteration 73/1000 | Loss: 0.00003600
Iteration 74/1000 | Loss: 0.00003600
Iteration 75/1000 | Loss: 0.00003599
Iteration 76/1000 | Loss: 0.00003599
Iteration 77/1000 | Loss: 0.00003599
Iteration 78/1000 | Loss: 0.00003598
Iteration 79/1000 | Loss: 0.00003598
Iteration 80/1000 | Loss: 0.00003598
Iteration 81/1000 | Loss: 0.00003598
Iteration 82/1000 | Loss: 0.00003597
Iteration 83/1000 | Loss: 0.00003597
Iteration 84/1000 | Loss: 0.00003597
Iteration 85/1000 | Loss: 0.00003597
Iteration 86/1000 | Loss: 0.00003597
Iteration 87/1000 | Loss: 0.00003597
Iteration 88/1000 | Loss: 0.00003597
Iteration 89/1000 | Loss: 0.00003596
Iteration 90/1000 | Loss: 0.00003596
Iteration 91/1000 | Loss: 0.00003596
Iteration 92/1000 | Loss: 0.00003596
Iteration 93/1000 | Loss: 0.00003596
Iteration 94/1000 | Loss: 0.00003596
Iteration 95/1000 | Loss: 0.00003596
Iteration 96/1000 | Loss: 0.00003595
Iteration 97/1000 | Loss: 0.00003595
Iteration 98/1000 | Loss: 0.00003595
Iteration 99/1000 | Loss: 0.00003595
Iteration 100/1000 | Loss: 0.00003595
Iteration 101/1000 | Loss: 0.00003595
Iteration 102/1000 | Loss: 0.00003595
Iteration 103/1000 | Loss: 0.00003595
Iteration 104/1000 | Loss: 0.00003595
Iteration 105/1000 | Loss: 0.00003594
Iteration 106/1000 | Loss: 0.00003594
Iteration 107/1000 | Loss: 0.00003594
Iteration 108/1000 | Loss: 0.00003594
Iteration 109/1000 | Loss: 0.00003594
Iteration 110/1000 | Loss: 0.00003594
Iteration 111/1000 | Loss: 0.00003594
Iteration 112/1000 | Loss: 0.00003594
Iteration 113/1000 | Loss: 0.00003594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [3.594367444748059e-05, 3.594367444748059e-05, 3.594367444748059e-05, 3.594367444748059e-05, 3.594367444748059e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.594367444748059e-05

Optimization complete. Final v2v error: 4.642954349517822 mm

Highest mean error: 5.7002739906311035 mm for frame 92

Lowest mean error: 3.6146154403686523 mm for frame 144

Saving results

Total time: 37.381284952163696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388783
Iteration 2/25 | Loss: 0.00072237
Iteration 3/25 | Loss: 0.00062165
Iteration 4/25 | Loss: 0.00060700
Iteration 5/25 | Loss: 0.00060279
Iteration 6/25 | Loss: 0.00060179
Iteration 7/25 | Loss: 0.00060155
Iteration 8/25 | Loss: 0.00060155
Iteration 9/25 | Loss: 0.00060155
Iteration 10/25 | Loss: 0.00060155
Iteration 11/25 | Loss: 0.00060155
Iteration 12/25 | Loss: 0.00060155
Iteration 13/25 | Loss: 0.00060155
Iteration 14/25 | Loss: 0.00060155
Iteration 15/25 | Loss: 0.00060155
Iteration 16/25 | Loss: 0.00060155
Iteration 17/25 | Loss: 0.00060155
Iteration 18/25 | Loss: 0.00060155
Iteration 19/25 | Loss: 0.00060155
Iteration 20/25 | Loss: 0.00060155
Iteration 21/25 | Loss: 0.00060155
Iteration 22/25 | Loss: 0.00060155
Iteration 23/25 | Loss: 0.00060155
Iteration 24/25 | Loss: 0.00060155
Iteration 25/25 | Loss: 0.00060155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.60828519
Iteration 2/25 | Loss: 0.00009266
Iteration 3/25 | Loss: 0.00009265
Iteration 4/25 | Loss: 0.00009265
Iteration 5/25 | Loss: 0.00009265
Iteration 6/25 | Loss: 0.00009265
Iteration 7/25 | Loss: 0.00009265
Iteration 8/25 | Loss: 0.00009265
Iteration 9/25 | Loss: 0.00009265
Iteration 10/25 | Loss: 0.00009265
Iteration 11/25 | Loss: 0.00009265
Iteration 12/25 | Loss: 0.00009265
Iteration 13/25 | Loss: 0.00009265
Iteration 14/25 | Loss: 0.00009265
Iteration 15/25 | Loss: 0.00009265
Iteration 16/25 | Loss: 0.00009265
Iteration 17/25 | Loss: 0.00009265
Iteration 18/25 | Loss: 0.00009265
Iteration 19/25 | Loss: 0.00009265
Iteration 20/25 | Loss: 0.00009265
Iteration 21/25 | Loss: 0.00009265
Iteration 22/25 | Loss: 0.00009265
Iteration 23/25 | Loss: 0.00009265
Iteration 24/25 | Loss: 0.00009265
Iteration 25/25 | Loss: 0.00009265

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00009265
Iteration 2/1000 | Loss: 0.00002483
Iteration 3/1000 | Loss: 0.00002082
Iteration 4/1000 | Loss: 0.00001908
Iteration 5/1000 | Loss: 0.00001795
Iteration 6/1000 | Loss: 0.00001734
Iteration 7/1000 | Loss: 0.00001705
Iteration 8/1000 | Loss: 0.00001699
Iteration 9/1000 | Loss: 0.00001695
Iteration 10/1000 | Loss: 0.00001695
Iteration 11/1000 | Loss: 0.00001694
Iteration 12/1000 | Loss: 0.00001693
Iteration 13/1000 | Loss: 0.00001688
Iteration 14/1000 | Loss: 0.00001686
Iteration 15/1000 | Loss: 0.00001686
Iteration 16/1000 | Loss: 0.00001686
Iteration 17/1000 | Loss: 0.00001681
Iteration 18/1000 | Loss: 0.00001681
Iteration 19/1000 | Loss: 0.00001679
Iteration 20/1000 | Loss: 0.00001679
Iteration 21/1000 | Loss: 0.00001677
Iteration 22/1000 | Loss: 0.00001673
Iteration 23/1000 | Loss: 0.00001673
Iteration 24/1000 | Loss: 0.00001673
Iteration 25/1000 | Loss: 0.00001673
Iteration 26/1000 | Loss: 0.00001673
Iteration 27/1000 | Loss: 0.00001672
Iteration 28/1000 | Loss: 0.00001672
Iteration 29/1000 | Loss: 0.00001672
Iteration 30/1000 | Loss: 0.00001668
Iteration 31/1000 | Loss: 0.00001668
Iteration 32/1000 | Loss: 0.00001668
Iteration 33/1000 | Loss: 0.00001667
Iteration 34/1000 | Loss: 0.00001667
Iteration 35/1000 | Loss: 0.00001667
Iteration 36/1000 | Loss: 0.00001667
Iteration 37/1000 | Loss: 0.00001667
Iteration 38/1000 | Loss: 0.00001665
Iteration 39/1000 | Loss: 0.00001665
Iteration 40/1000 | Loss: 0.00001665
Iteration 41/1000 | Loss: 0.00001664
Iteration 42/1000 | Loss: 0.00001664
Iteration 43/1000 | Loss: 0.00001664
Iteration 44/1000 | Loss: 0.00001664
Iteration 45/1000 | Loss: 0.00001664
Iteration 46/1000 | Loss: 0.00001664
Iteration 47/1000 | Loss: 0.00001664
Iteration 48/1000 | Loss: 0.00001664
Iteration 49/1000 | Loss: 0.00001664
Iteration 50/1000 | Loss: 0.00001663
Iteration 51/1000 | Loss: 0.00001663
Iteration 52/1000 | Loss: 0.00001663
Iteration 53/1000 | Loss: 0.00001662
Iteration 54/1000 | Loss: 0.00001662
Iteration 55/1000 | Loss: 0.00001662
Iteration 56/1000 | Loss: 0.00001661
Iteration 57/1000 | Loss: 0.00001661
Iteration 58/1000 | Loss: 0.00001660
Iteration 59/1000 | Loss: 0.00001660
Iteration 60/1000 | Loss: 0.00001660
Iteration 61/1000 | Loss: 0.00001660
Iteration 62/1000 | Loss: 0.00001659
Iteration 63/1000 | Loss: 0.00001659
Iteration 64/1000 | Loss: 0.00001658
Iteration 65/1000 | Loss: 0.00001658
Iteration 66/1000 | Loss: 0.00001658
Iteration 67/1000 | Loss: 0.00001658
Iteration 68/1000 | Loss: 0.00001658
Iteration 69/1000 | Loss: 0.00001657
Iteration 70/1000 | Loss: 0.00001657
Iteration 71/1000 | Loss: 0.00001656
Iteration 72/1000 | Loss: 0.00001655
Iteration 73/1000 | Loss: 0.00001655
Iteration 74/1000 | Loss: 0.00001654
Iteration 75/1000 | Loss: 0.00001654
Iteration 76/1000 | Loss: 0.00001654
Iteration 77/1000 | Loss: 0.00001654
Iteration 78/1000 | Loss: 0.00001654
Iteration 79/1000 | Loss: 0.00001654
Iteration 80/1000 | Loss: 0.00001654
Iteration 81/1000 | Loss: 0.00001653
Iteration 82/1000 | Loss: 0.00001653
Iteration 83/1000 | Loss: 0.00001652
Iteration 84/1000 | Loss: 0.00001652
Iteration 85/1000 | Loss: 0.00001652
Iteration 86/1000 | Loss: 0.00001651
Iteration 87/1000 | Loss: 0.00001651
Iteration 88/1000 | Loss: 0.00001651
Iteration 89/1000 | Loss: 0.00001651
Iteration 90/1000 | Loss: 0.00001651
Iteration 91/1000 | Loss: 0.00001650
Iteration 92/1000 | Loss: 0.00001650
Iteration 93/1000 | Loss: 0.00001650
Iteration 94/1000 | Loss: 0.00001650
Iteration 95/1000 | Loss: 0.00001649
Iteration 96/1000 | Loss: 0.00001648
Iteration 97/1000 | Loss: 0.00001648
Iteration 98/1000 | Loss: 0.00001648
Iteration 99/1000 | Loss: 0.00001647
Iteration 100/1000 | Loss: 0.00001647
Iteration 101/1000 | Loss: 0.00001647
Iteration 102/1000 | Loss: 0.00001647
Iteration 103/1000 | Loss: 0.00001647
Iteration 104/1000 | Loss: 0.00001647
Iteration 105/1000 | Loss: 0.00001646
Iteration 106/1000 | Loss: 0.00001646
Iteration 107/1000 | Loss: 0.00001646
Iteration 108/1000 | Loss: 0.00001646
Iteration 109/1000 | Loss: 0.00001645
Iteration 110/1000 | Loss: 0.00001645
Iteration 111/1000 | Loss: 0.00001644
Iteration 112/1000 | Loss: 0.00001644
Iteration 113/1000 | Loss: 0.00001644
Iteration 114/1000 | Loss: 0.00001644
Iteration 115/1000 | Loss: 0.00001644
Iteration 116/1000 | Loss: 0.00001644
Iteration 117/1000 | Loss: 0.00001644
Iteration 118/1000 | Loss: 0.00001644
Iteration 119/1000 | Loss: 0.00001643
Iteration 120/1000 | Loss: 0.00001643
Iteration 121/1000 | Loss: 0.00001643
Iteration 122/1000 | Loss: 0.00001643
Iteration 123/1000 | Loss: 0.00001643
Iteration 124/1000 | Loss: 0.00001643
Iteration 125/1000 | Loss: 0.00001643
Iteration 126/1000 | Loss: 0.00001643
Iteration 127/1000 | Loss: 0.00001642
Iteration 128/1000 | Loss: 0.00001642
Iteration 129/1000 | Loss: 0.00001642
Iteration 130/1000 | Loss: 0.00001642
Iteration 131/1000 | Loss: 0.00001642
Iteration 132/1000 | Loss: 0.00001642
Iteration 133/1000 | Loss: 0.00001642
Iteration 134/1000 | Loss: 0.00001641
Iteration 135/1000 | Loss: 0.00001641
Iteration 136/1000 | Loss: 0.00001640
Iteration 137/1000 | Loss: 0.00001640
Iteration 138/1000 | Loss: 0.00001640
Iteration 139/1000 | Loss: 0.00001640
Iteration 140/1000 | Loss: 0.00001639
Iteration 141/1000 | Loss: 0.00001639
Iteration 142/1000 | Loss: 0.00001639
Iteration 143/1000 | Loss: 0.00001639
Iteration 144/1000 | Loss: 0.00001638
Iteration 145/1000 | Loss: 0.00001638
Iteration 146/1000 | Loss: 0.00001638
Iteration 147/1000 | Loss: 0.00001638
Iteration 148/1000 | Loss: 0.00001638
Iteration 149/1000 | Loss: 0.00001638
Iteration 150/1000 | Loss: 0.00001638
Iteration 151/1000 | Loss: 0.00001638
Iteration 152/1000 | Loss: 0.00001638
Iteration 153/1000 | Loss: 0.00001638
Iteration 154/1000 | Loss: 0.00001637
Iteration 155/1000 | Loss: 0.00001637
Iteration 156/1000 | Loss: 0.00001637
Iteration 157/1000 | Loss: 0.00001637
Iteration 158/1000 | Loss: 0.00001637
Iteration 159/1000 | Loss: 0.00001637
Iteration 160/1000 | Loss: 0.00001637
Iteration 161/1000 | Loss: 0.00001637
Iteration 162/1000 | Loss: 0.00001637
Iteration 163/1000 | Loss: 0.00001637
Iteration 164/1000 | Loss: 0.00001637
Iteration 165/1000 | Loss: 0.00001637
Iteration 166/1000 | Loss: 0.00001637
Iteration 167/1000 | Loss: 0.00001637
Iteration 168/1000 | Loss: 0.00001637
Iteration 169/1000 | Loss: 0.00001637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.636631350265816e-05, 1.636631350265816e-05, 1.636631350265816e-05, 1.636631350265816e-05, 1.636631350265816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.636631350265816e-05

Optimization complete. Final v2v error: 3.468844413757324 mm

Highest mean error: 3.6296439170837402 mm for frame 103

Lowest mean error: 3.262579917907715 mm for frame 116

Saving results

Total time: 35.80692005157471
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863255
Iteration 2/25 | Loss: 0.00073926
Iteration 3/25 | Loss: 0.00062966
Iteration 4/25 | Loss: 0.00061668
Iteration 5/25 | Loss: 0.00061280
Iteration 6/25 | Loss: 0.00061201
Iteration 7/25 | Loss: 0.00061186
Iteration 8/25 | Loss: 0.00061186
Iteration 9/25 | Loss: 0.00061186
Iteration 10/25 | Loss: 0.00061186
Iteration 11/25 | Loss: 0.00061186
Iteration 12/25 | Loss: 0.00061186
Iteration 13/25 | Loss: 0.00061186
Iteration 14/25 | Loss: 0.00061186
Iteration 15/25 | Loss: 0.00061186
Iteration 16/25 | Loss: 0.00061186
Iteration 17/25 | Loss: 0.00061186
Iteration 18/25 | Loss: 0.00061186
Iteration 19/25 | Loss: 0.00061186
Iteration 20/25 | Loss: 0.00061186
Iteration 21/25 | Loss: 0.00061186
Iteration 22/25 | Loss: 0.00061186
Iteration 23/25 | Loss: 0.00061186
Iteration 24/25 | Loss: 0.00061186
Iteration 25/25 | Loss: 0.00061186

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.07962418
Iteration 2/25 | Loss: 0.00011755
Iteration 3/25 | Loss: 0.00011752
Iteration 4/25 | Loss: 0.00011752
Iteration 5/25 | Loss: 0.00011752
Iteration 6/25 | Loss: 0.00011752
Iteration 7/25 | Loss: 0.00011752
Iteration 8/25 | Loss: 0.00011752
Iteration 9/25 | Loss: 0.00011752
Iteration 10/25 | Loss: 0.00011752
Iteration 11/25 | Loss: 0.00011752
Iteration 12/25 | Loss: 0.00011752
Iteration 13/25 | Loss: 0.00011752
Iteration 14/25 | Loss: 0.00011752
Iteration 15/25 | Loss: 0.00011752
Iteration 16/25 | Loss: 0.00011752
Iteration 17/25 | Loss: 0.00011752
Iteration 18/25 | Loss: 0.00011752
Iteration 19/25 | Loss: 0.00011752
Iteration 20/25 | Loss: 0.00011752
Iteration 21/25 | Loss: 0.00011752
Iteration 22/25 | Loss: 0.00011752
Iteration 23/25 | Loss: 0.00011752
Iteration 24/25 | Loss: 0.00011752
Iteration 25/25 | Loss: 0.00011752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00011752
Iteration 2/1000 | Loss: 0.00002549
Iteration 3/1000 | Loss: 0.00002011
Iteration 4/1000 | Loss: 0.00001850
Iteration 5/1000 | Loss: 0.00001728
Iteration 6/1000 | Loss: 0.00001658
Iteration 7/1000 | Loss: 0.00001610
Iteration 8/1000 | Loss: 0.00001594
Iteration 9/1000 | Loss: 0.00001588
Iteration 10/1000 | Loss: 0.00001584
Iteration 11/1000 | Loss: 0.00001582
Iteration 12/1000 | Loss: 0.00001581
Iteration 13/1000 | Loss: 0.00001580
Iteration 14/1000 | Loss: 0.00001580
Iteration 15/1000 | Loss: 0.00001578
Iteration 16/1000 | Loss: 0.00001575
Iteration 17/1000 | Loss: 0.00001574
Iteration 18/1000 | Loss: 0.00001568
Iteration 19/1000 | Loss: 0.00001566
Iteration 20/1000 | Loss: 0.00001566
Iteration 21/1000 | Loss: 0.00001566
Iteration 22/1000 | Loss: 0.00001565
Iteration 23/1000 | Loss: 0.00001565
Iteration 24/1000 | Loss: 0.00001565
Iteration 25/1000 | Loss: 0.00001564
Iteration 26/1000 | Loss: 0.00001564
Iteration 27/1000 | Loss: 0.00001564
Iteration 28/1000 | Loss: 0.00001563
Iteration 29/1000 | Loss: 0.00001563
Iteration 30/1000 | Loss: 0.00001563
Iteration 31/1000 | Loss: 0.00001562
Iteration 32/1000 | Loss: 0.00001562
Iteration 33/1000 | Loss: 0.00001562
Iteration 34/1000 | Loss: 0.00001561
Iteration 35/1000 | Loss: 0.00001561
Iteration 36/1000 | Loss: 0.00001561
Iteration 37/1000 | Loss: 0.00001561
Iteration 38/1000 | Loss: 0.00001561
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001561
Iteration 41/1000 | Loss: 0.00001561
Iteration 42/1000 | Loss: 0.00001560
Iteration 43/1000 | Loss: 0.00001560
Iteration 44/1000 | Loss: 0.00001560
Iteration 45/1000 | Loss: 0.00001560
Iteration 46/1000 | Loss: 0.00001559
Iteration 47/1000 | Loss: 0.00001559
Iteration 48/1000 | Loss: 0.00001559
Iteration 49/1000 | Loss: 0.00001559
Iteration 50/1000 | Loss: 0.00001558
Iteration 51/1000 | Loss: 0.00001558
Iteration 52/1000 | Loss: 0.00001558
Iteration 53/1000 | Loss: 0.00001558
Iteration 54/1000 | Loss: 0.00001558
Iteration 55/1000 | Loss: 0.00001557
Iteration 56/1000 | Loss: 0.00001557
Iteration 57/1000 | Loss: 0.00001557
Iteration 58/1000 | Loss: 0.00001557
Iteration 59/1000 | Loss: 0.00001557
Iteration 60/1000 | Loss: 0.00001557
Iteration 61/1000 | Loss: 0.00001557
Iteration 62/1000 | Loss: 0.00001557
Iteration 63/1000 | Loss: 0.00001557
Iteration 64/1000 | Loss: 0.00001557
Iteration 65/1000 | Loss: 0.00001557
Iteration 66/1000 | Loss: 0.00001556
Iteration 67/1000 | Loss: 0.00001556
Iteration 68/1000 | Loss: 0.00001556
Iteration 69/1000 | Loss: 0.00001556
Iteration 70/1000 | Loss: 0.00001556
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00001556
Iteration 75/1000 | Loss: 0.00001556
Iteration 76/1000 | Loss: 0.00001556
Iteration 77/1000 | Loss: 0.00001556
Iteration 78/1000 | Loss: 0.00001556
Iteration 79/1000 | Loss: 0.00001556
Iteration 80/1000 | Loss: 0.00001555
Iteration 81/1000 | Loss: 0.00001555
Iteration 82/1000 | Loss: 0.00001555
Iteration 83/1000 | Loss: 0.00001555
Iteration 84/1000 | Loss: 0.00001555
Iteration 85/1000 | Loss: 0.00001555
Iteration 86/1000 | Loss: 0.00001555
Iteration 87/1000 | Loss: 0.00001555
Iteration 88/1000 | Loss: 0.00001555
Iteration 89/1000 | Loss: 0.00001555
Iteration 90/1000 | Loss: 0.00001555
Iteration 91/1000 | Loss: 0.00001555
Iteration 92/1000 | Loss: 0.00001555
Iteration 93/1000 | Loss: 0.00001555
Iteration 94/1000 | Loss: 0.00001555
Iteration 95/1000 | Loss: 0.00001554
Iteration 96/1000 | Loss: 0.00001554
Iteration 97/1000 | Loss: 0.00001554
Iteration 98/1000 | Loss: 0.00001554
Iteration 99/1000 | Loss: 0.00001554
Iteration 100/1000 | Loss: 0.00001554
Iteration 101/1000 | Loss: 0.00001554
Iteration 102/1000 | Loss: 0.00001554
Iteration 103/1000 | Loss: 0.00001554
Iteration 104/1000 | Loss: 0.00001554
Iteration 105/1000 | Loss: 0.00001554
Iteration 106/1000 | Loss: 0.00001554
Iteration 107/1000 | Loss: 0.00001554
Iteration 108/1000 | Loss: 0.00001554
Iteration 109/1000 | Loss: 0.00001554
Iteration 110/1000 | Loss: 0.00001554
Iteration 111/1000 | Loss: 0.00001554
Iteration 112/1000 | Loss: 0.00001554
Iteration 113/1000 | Loss: 0.00001554
Iteration 114/1000 | Loss: 0.00001554
Iteration 115/1000 | Loss: 0.00001554
Iteration 116/1000 | Loss: 0.00001554
Iteration 117/1000 | Loss: 0.00001554
Iteration 118/1000 | Loss: 0.00001554
Iteration 119/1000 | Loss: 0.00001554
Iteration 120/1000 | Loss: 0.00001554
Iteration 121/1000 | Loss: 0.00001554
Iteration 122/1000 | Loss: 0.00001554
Iteration 123/1000 | Loss: 0.00001554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.5535451893811114e-05, 1.5535451893811114e-05, 1.5535451893811114e-05, 1.5535451893811114e-05, 1.5535451893811114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5535451893811114e-05

Optimization complete. Final v2v error: 3.3697590827941895 mm

Highest mean error: 3.6947710514068604 mm for frame 21

Lowest mean error: 3.007899284362793 mm for frame 38

Saving results

Total time: 30.42280101776123
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433696
Iteration 2/25 | Loss: 0.00092147
Iteration 3/25 | Loss: 0.00065793
Iteration 4/25 | Loss: 0.00063562
Iteration 5/25 | Loss: 0.00063274
Iteration 6/25 | Loss: 0.00063213
Iteration 7/25 | Loss: 0.00063213
Iteration 8/25 | Loss: 0.00063213
Iteration 9/25 | Loss: 0.00063213
Iteration 10/25 | Loss: 0.00063213
Iteration 11/25 | Loss: 0.00063213
Iteration 12/25 | Loss: 0.00063213
Iteration 13/25 | Loss: 0.00063213
Iteration 14/25 | Loss: 0.00063213
Iteration 15/25 | Loss: 0.00063213
Iteration 16/25 | Loss: 0.00063213
Iteration 17/25 | Loss: 0.00063213
Iteration 18/25 | Loss: 0.00063213
Iteration 19/25 | Loss: 0.00063213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006321308319456875, 0.0006321308319456875, 0.0006321308319456875, 0.0006321308319456875, 0.0006321308319456875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006321308319456875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.82297039
Iteration 2/25 | Loss: 0.00007354
Iteration 3/25 | Loss: 0.00007353
Iteration 4/25 | Loss: 0.00007353
Iteration 5/25 | Loss: 0.00007353
Iteration 6/25 | Loss: 0.00007353
Iteration 7/25 | Loss: 0.00007352
Iteration 8/25 | Loss: 0.00007352
Iteration 9/25 | Loss: 0.00007352
Iteration 10/25 | Loss: 0.00007352
Iteration 11/25 | Loss: 0.00007352
Iteration 12/25 | Loss: 0.00007352
Iteration 13/25 | Loss: 0.00007352
Iteration 14/25 | Loss: 0.00007352
Iteration 15/25 | Loss: 0.00007352
Iteration 16/25 | Loss: 0.00007352
Iteration 17/25 | Loss: 0.00007352
Iteration 18/25 | Loss: 0.00007352
Iteration 19/25 | Loss: 0.00007352
Iteration 20/25 | Loss: 0.00007352
Iteration 21/25 | Loss: 0.00007352
Iteration 22/25 | Loss: 0.00007352
Iteration 23/25 | Loss: 0.00007352
Iteration 24/25 | Loss: 0.00007352
Iteration 25/25 | Loss: 0.00007352

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00007352
Iteration 2/1000 | Loss: 0.00002525
Iteration 3/1000 | Loss: 0.00002140
Iteration 4/1000 | Loss: 0.00001980
Iteration 5/1000 | Loss: 0.00001870
Iteration 6/1000 | Loss: 0.00001804
Iteration 7/1000 | Loss: 0.00001769
Iteration 8/1000 | Loss: 0.00001747
Iteration 9/1000 | Loss: 0.00001739
Iteration 10/1000 | Loss: 0.00001735
Iteration 11/1000 | Loss: 0.00001735
Iteration 12/1000 | Loss: 0.00001727
Iteration 13/1000 | Loss: 0.00001727
Iteration 14/1000 | Loss: 0.00001726
Iteration 15/1000 | Loss: 0.00001722
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001722
Iteration 18/1000 | Loss: 0.00001722
Iteration 19/1000 | Loss: 0.00001722
Iteration 20/1000 | Loss: 0.00001722
Iteration 21/1000 | Loss: 0.00001722
Iteration 22/1000 | Loss: 0.00001719
Iteration 23/1000 | Loss: 0.00001718
Iteration 24/1000 | Loss: 0.00001718
Iteration 25/1000 | Loss: 0.00001718
Iteration 26/1000 | Loss: 0.00001718
Iteration 27/1000 | Loss: 0.00001717
Iteration 28/1000 | Loss: 0.00001717
Iteration 29/1000 | Loss: 0.00001717
Iteration 30/1000 | Loss: 0.00001717
Iteration 31/1000 | Loss: 0.00001716
Iteration 32/1000 | Loss: 0.00001716
Iteration 33/1000 | Loss: 0.00001716
Iteration 34/1000 | Loss: 0.00001716
Iteration 35/1000 | Loss: 0.00001715
Iteration 36/1000 | Loss: 0.00001715
Iteration 37/1000 | Loss: 0.00001715
Iteration 38/1000 | Loss: 0.00001715
Iteration 39/1000 | Loss: 0.00001715
Iteration 40/1000 | Loss: 0.00001715
Iteration 41/1000 | Loss: 0.00001715
Iteration 42/1000 | Loss: 0.00001715
Iteration 43/1000 | Loss: 0.00001714
Iteration 44/1000 | Loss: 0.00001714
Iteration 45/1000 | Loss: 0.00001713
Iteration 46/1000 | Loss: 0.00001713
Iteration 47/1000 | Loss: 0.00001713
Iteration 48/1000 | Loss: 0.00001713
Iteration 49/1000 | Loss: 0.00001713
Iteration 50/1000 | Loss: 0.00001713
Iteration 51/1000 | Loss: 0.00001713
Iteration 52/1000 | Loss: 0.00001713
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001712
Iteration 55/1000 | Loss: 0.00001712
Iteration 56/1000 | Loss: 0.00001712
Iteration 57/1000 | Loss: 0.00001712
Iteration 58/1000 | Loss: 0.00001712
Iteration 59/1000 | Loss: 0.00001712
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001711
Iteration 62/1000 | Loss: 0.00001711
Iteration 63/1000 | Loss: 0.00001711
Iteration 64/1000 | Loss: 0.00001711
Iteration 65/1000 | Loss: 0.00001711
Iteration 66/1000 | Loss: 0.00001711
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001710
Iteration 71/1000 | Loss: 0.00001710
Iteration 72/1000 | Loss: 0.00001710
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001710
Iteration 75/1000 | Loss: 0.00001710
Iteration 76/1000 | Loss: 0.00001710
Iteration 77/1000 | Loss: 0.00001710
Iteration 78/1000 | Loss: 0.00001709
Iteration 79/1000 | Loss: 0.00001709
Iteration 80/1000 | Loss: 0.00001709
Iteration 81/1000 | Loss: 0.00001709
Iteration 82/1000 | Loss: 0.00001709
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001709
Iteration 88/1000 | Loss: 0.00001708
Iteration 89/1000 | Loss: 0.00001708
Iteration 90/1000 | Loss: 0.00001708
Iteration 91/1000 | Loss: 0.00001708
Iteration 92/1000 | Loss: 0.00001708
Iteration 93/1000 | Loss: 0.00001708
Iteration 94/1000 | Loss: 0.00001708
Iteration 95/1000 | Loss: 0.00001708
Iteration 96/1000 | Loss: 0.00001708
Iteration 97/1000 | Loss: 0.00001708
Iteration 98/1000 | Loss: 0.00001707
Iteration 99/1000 | Loss: 0.00001707
Iteration 100/1000 | Loss: 0.00001707
Iteration 101/1000 | Loss: 0.00001707
Iteration 102/1000 | Loss: 0.00001707
Iteration 103/1000 | Loss: 0.00001707
Iteration 104/1000 | Loss: 0.00001707
Iteration 105/1000 | Loss: 0.00001707
Iteration 106/1000 | Loss: 0.00001707
Iteration 107/1000 | Loss: 0.00001707
Iteration 108/1000 | Loss: 0.00001707
Iteration 109/1000 | Loss: 0.00001706
Iteration 110/1000 | Loss: 0.00001706
Iteration 111/1000 | Loss: 0.00001706
Iteration 112/1000 | Loss: 0.00001706
Iteration 113/1000 | Loss: 0.00001706
Iteration 114/1000 | Loss: 0.00001706
Iteration 115/1000 | Loss: 0.00001706
Iteration 116/1000 | Loss: 0.00001706
Iteration 117/1000 | Loss: 0.00001705
Iteration 118/1000 | Loss: 0.00001705
Iteration 119/1000 | Loss: 0.00001705
Iteration 120/1000 | Loss: 0.00001705
Iteration 121/1000 | Loss: 0.00001705
Iteration 122/1000 | Loss: 0.00001705
Iteration 123/1000 | Loss: 0.00001705
Iteration 124/1000 | Loss: 0.00001705
Iteration 125/1000 | Loss: 0.00001704
Iteration 126/1000 | Loss: 0.00001704
Iteration 127/1000 | Loss: 0.00001704
Iteration 128/1000 | Loss: 0.00001704
Iteration 129/1000 | Loss: 0.00001704
Iteration 130/1000 | Loss: 0.00001703
Iteration 131/1000 | Loss: 0.00001703
Iteration 132/1000 | Loss: 0.00001703
Iteration 133/1000 | Loss: 0.00001702
Iteration 134/1000 | Loss: 0.00001702
Iteration 135/1000 | Loss: 0.00001702
Iteration 136/1000 | Loss: 0.00001702
Iteration 137/1000 | Loss: 0.00001701
Iteration 138/1000 | Loss: 0.00001701
Iteration 139/1000 | Loss: 0.00001701
Iteration 140/1000 | Loss: 0.00001701
Iteration 141/1000 | Loss: 0.00001701
Iteration 142/1000 | Loss: 0.00001701
Iteration 143/1000 | Loss: 0.00001701
Iteration 144/1000 | Loss: 0.00001701
Iteration 145/1000 | Loss: 0.00001701
Iteration 146/1000 | Loss: 0.00001700
Iteration 147/1000 | Loss: 0.00001700
Iteration 148/1000 | Loss: 0.00001700
Iteration 149/1000 | Loss: 0.00001700
Iteration 150/1000 | Loss: 0.00001700
Iteration 151/1000 | Loss: 0.00001700
Iteration 152/1000 | Loss: 0.00001700
Iteration 153/1000 | Loss: 0.00001700
Iteration 154/1000 | Loss: 0.00001700
Iteration 155/1000 | Loss: 0.00001700
Iteration 156/1000 | Loss: 0.00001700
Iteration 157/1000 | Loss: 0.00001700
Iteration 158/1000 | Loss: 0.00001700
Iteration 159/1000 | Loss: 0.00001700
Iteration 160/1000 | Loss: 0.00001700
Iteration 161/1000 | Loss: 0.00001700
Iteration 162/1000 | Loss: 0.00001700
Iteration 163/1000 | Loss: 0.00001700
Iteration 164/1000 | Loss: 0.00001700
Iteration 165/1000 | Loss: 0.00001700
Iteration 166/1000 | Loss: 0.00001700
Iteration 167/1000 | Loss: 0.00001700
Iteration 168/1000 | Loss: 0.00001700
Iteration 169/1000 | Loss: 0.00001700
Iteration 170/1000 | Loss: 0.00001700
Iteration 171/1000 | Loss: 0.00001700
Iteration 172/1000 | Loss: 0.00001700
Iteration 173/1000 | Loss: 0.00001700
Iteration 174/1000 | Loss: 0.00001700
Iteration 175/1000 | Loss: 0.00001700
Iteration 176/1000 | Loss: 0.00001700
Iteration 177/1000 | Loss: 0.00001700
Iteration 178/1000 | Loss: 0.00001700
Iteration 179/1000 | Loss: 0.00001700
Iteration 180/1000 | Loss: 0.00001700
Iteration 181/1000 | Loss: 0.00001700
Iteration 182/1000 | Loss: 0.00001700
Iteration 183/1000 | Loss: 0.00001700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.700030770734884e-05, 1.700030770734884e-05, 1.700030770734884e-05, 1.700030770734884e-05, 1.700030770734884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.700030770734884e-05

Optimization complete. Final v2v error: 3.550474166870117 mm

Highest mean error: 3.876688003540039 mm for frame 109

Lowest mean error: 3.280428647994995 mm for frame 54

Saving results

Total time: 33.907965898513794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393647
Iteration 2/25 | Loss: 0.00087332
Iteration 3/25 | Loss: 0.00068025
Iteration 4/25 | Loss: 0.00064946
Iteration 5/25 | Loss: 0.00064318
Iteration 6/25 | Loss: 0.00064148
Iteration 7/25 | Loss: 0.00064112
Iteration 8/25 | Loss: 0.00064112
Iteration 9/25 | Loss: 0.00064112
Iteration 10/25 | Loss: 0.00064112
Iteration 11/25 | Loss: 0.00064112
Iteration 12/25 | Loss: 0.00064112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006411217036657035, 0.0006411217036657035, 0.0006411217036657035, 0.0006411217036657035, 0.0006411217036657035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006411217036657035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37363172
Iteration 2/25 | Loss: 0.00012083
Iteration 3/25 | Loss: 0.00012082
Iteration 4/25 | Loss: 0.00012082
Iteration 5/25 | Loss: 0.00012082
Iteration 6/25 | Loss: 0.00012082
Iteration 7/25 | Loss: 0.00012082
Iteration 8/25 | Loss: 0.00012082
Iteration 9/25 | Loss: 0.00012082
Iteration 10/25 | Loss: 0.00012082
Iteration 11/25 | Loss: 0.00012082
Iteration 12/25 | Loss: 0.00012082
Iteration 13/25 | Loss: 0.00012082
Iteration 14/25 | Loss: 0.00012082
Iteration 15/25 | Loss: 0.00012082
Iteration 16/25 | Loss: 0.00012082
Iteration 17/25 | Loss: 0.00012082
Iteration 18/25 | Loss: 0.00012082
Iteration 19/25 | Loss: 0.00012082
Iteration 20/25 | Loss: 0.00012082
Iteration 21/25 | Loss: 0.00012082
Iteration 22/25 | Loss: 0.00012082
Iteration 23/25 | Loss: 0.00012082
Iteration 24/25 | Loss: 0.00012082
Iteration 25/25 | Loss: 0.00012082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00012082
Iteration 2/1000 | Loss: 0.00003111
Iteration 3/1000 | Loss: 0.00002452
Iteration 4/1000 | Loss: 0.00002230
Iteration 5/1000 | Loss: 0.00002086
Iteration 6/1000 | Loss: 0.00001977
Iteration 7/1000 | Loss: 0.00001916
Iteration 8/1000 | Loss: 0.00001876
Iteration 9/1000 | Loss: 0.00001862
Iteration 10/1000 | Loss: 0.00001841
Iteration 11/1000 | Loss: 0.00001837
Iteration 12/1000 | Loss: 0.00001821
Iteration 13/1000 | Loss: 0.00001811
Iteration 14/1000 | Loss: 0.00001809
Iteration 15/1000 | Loss: 0.00001808
Iteration 16/1000 | Loss: 0.00001807
Iteration 17/1000 | Loss: 0.00001807
Iteration 18/1000 | Loss: 0.00001804
Iteration 19/1000 | Loss: 0.00001803
Iteration 20/1000 | Loss: 0.00001803
Iteration 21/1000 | Loss: 0.00001802
Iteration 22/1000 | Loss: 0.00001802
Iteration 23/1000 | Loss: 0.00001802
Iteration 24/1000 | Loss: 0.00001802
Iteration 25/1000 | Loss: 0.00001801
Iteration 26/1000 | Loss: 0.00001801
Iteration 27/1000 | Loss: 0.00001799
Iteration 28/1000 | Loss: 0.00001799
Iteration 29/1000 | Loss: 0.00001798
Iteration 30/1000 | Loss: 0.00001798
Iteration 31/1000 | Loss: 0.00001798
Iteration 32/1000 | Loss: 0.00001797
Iteration 33/1000 | Loss: 0.00001797
Iteration 34/1000 | Loss: 0.00001797
Iteration 35/1000 | Loss: 0.00001796
Iteration 36/1000 | Loss: 0.00001796
Iteration 37/1000 | Loss: 0.00001796
Iteration 38/1000 | Loss: 0.00001795
Iteration 39/1000 | Loss: 0.00001795
Iteration 40/1000 | Loss: 0.00001795
Iteration 41/1000 | Loss: 0.00001795
Iteration 42/1000 | Loss: 0.00001795
Iteration 43/1000 | Loss: 0.00001794
Iteration 44/1000 | Loss: 0.00001794
Iteration 45/1000 | Loss: 0.00001794
Iteration 46/1000 | Loss: 0.00001794
Iteration 47/1000 | Loss: 0.00001794
Iteration 48/1000 | Loss: 0.00001793
Iteration 49/1000 | Loss: 0.00001793
Iteration 50/1000 | Loss: 0.00001793
Iteration 51/1000 | Loss: 0.00001793
Iteration 52/1000 | Loss: 0.00001793
Iteration 53/1000 | Loss: 0.00001793
Iteration 54/1000 | Loss: 0.00001792
Iteration 55/1000 | Loss: 0.00001792
Iteration 56/1000 | Loss: 0.00001792
Iteration 57/1000 | Loss: 0.00001790
Iteration 58/1000 | Loss: 0.00001790
Iteration 59/1000 | Loss: 0.00001789
Iteration 60/1000 | Loss: 0.00001789
Iteration 61/1000 | Loss: 0.00001788
Iteration 62/1000 | Loss: 0.00001788
Iteration 63/1000 | Loss: 0.00001788
Iteration 64/1000 | Loss: 0.00001787
Iteration 65/1000 | Loss: 0.00001786
Iteration 66/1000 | Loss: 0.00001786
Iteration 67/1000 | Loss: 0.00001785
Iteration 68/1000 | Loss: 0.00001785
Iteration 69/1000 | Loss: 0.00001784
Iteration 70/1000 | Loss: 0.00001784
Iteration 71/1000 | Loss: 0.00001783
Iteration 72/1000 | Loss: 0.00001782
Iteration 73/1000 | Loss: 0.00001782
Iteration 74/1000 | Loss: 0.00001782
Iteration 75/1000 | Loss: 0.00001782
Iteration 76/1000 | Loss: 0.00001782
Iteration 77/1000 | Loss: 0.00001781
Iteration 78/1000 | Loss: 0.00001781
Iteration 79/1000 | Loss: 0.00001781
Iteration 80/1000 | Loss: 0.00001781
Iteration 81/1000 | Loss: 0.00001781
Iteration 82/1000 | Loss: 0.00001781
Iteration 83/1000 | Loss: 0.00001781
Iteration 84/1000 | Loss: 0.00001781
Iteration 85/1000 | Loss: 0.00001781
Iteration 86/1000 | Loss: 0.00001781
Iteration 87/1000 | Loss: 0.00001781
Iteration 88/1000 | Loss: 0.00001781
Iteration 89/1000 | Loss: 0.00001781
Iteration 90/1000 | Loss: 0.00001781
Iteration 91/1000 | Loss: 0.00001781
Iteration 92/1000 | Loss: 0.00001780
Iteration 93/1000 | Loss: 0.00001780
Iteration 94/1000 | Loss: 0.00001780
Iteration 95/1000 | Loss: 0.00001779
Iteration 96/1000 | Loss: 0.00001779
Iteration 97/1000 | Loss: 0.00001779
Iteration 98/1000 | Loss: 0.00001779
Iteration 99/1000 | Loss: 0.00001779
Iteration 100/1000 | Loss: 0.00001779
Iteration 101/1000 | Loss: 0.00001779
Iteration 102/1000 | Loss: 0.00001779
Iteration 103/1000 | Loss: 0.00001779
Iteration 104/1000 | Loss: 0.00001778
Iteration 105/1000 | Loss: 0.00001778
Iteration 106/1000 | Loss: 0.00001778
Iteration 107/1000 | Loss: 0.00001778
Iteration 108/1000 | Loss: 0.00001778
Iteration 109/1000 | Loss: 0.00001778
Iteration 110/1000 | Loss: 0.00001778
Iteration 111/1000 | Loss: 0.00001778
Iteration 112/1000 | Loss: 0.00001778
Iteration 113/1000 | Loss: 0.00001778
Iteration 114/1000 | Loss: 0.00001778
Iteration 115/1000 | Loss: 0.00001778
Iteration 116/1000 | Loss: 0.00001778
Iteration 117/1000 | Loss: 0.00001778
Iteration 118/1000 | Loss: 0.00001777
Iteration 119/1000 | Loss: 0.00001777
Iteration 120/1000 | Loss: 0.00001777
Iteration 121/1000 | Loss: 0.00001777
Iteration 122/1000 | Loss: 0.00001776
Iteration 123/1000 | Loss: 0.00001776
Iteration 124/1000 | Loss: 0.00001776
Iteration 125/1000 | Loss: 0.00001776
Iteration 126/1000 | Loss: 0.00001775
Iteration 127/1000 | Loss: 0.00001775
Iteration 128/1000 | Loss: 0.00001775
Iteration 129/1000 | Loss: 0.00001775
Iteration 130/1000 | Loss: 0.00001775
Iteration 131/1000 | Loss: 0.00001774
Iteration 132/1000 | Loss: 0.00001774
Iteration 133/1000 | Loss: 0.00001774
Iteration 134/1000 | Loss: 0.00001774
Iteration 135/1000 | Loss: 0.00001774
Iteration 136/1000 | Loss: 0.00001774
Iteration 137/1000 | Loss: 0.00001774
Iteration 138/1000 | Loss: 0.00001774
Iteration 139/1000 | Loss: 0.00001774
Iteration 140/1000 | Loss: 0.00001774
Iteration 141/1000 | Loss: 0.00001774
Iteration 142/1000 | Loss: 0.00001774
Iteration 143/1000 | Loss: 0.00001773
Iteration 144/1000 | Loss: 0.00001773
Iteration 145/1000 | Loss: 0.00001773
Iteration 146/1000 | Loss: 0.00001773
Iteration 147/1000 | Loss: 0.00001773
Iteration 148/1000 | Loss: 0.00001773
Iteration 149/1000 | Loss: 0.00001773
Iteration 150/1000 | Loss: 0.00001773
Iteration 151/1000 | Loss: 0.00001773
Iteration 152/1000 | Loss: 0.00001773
Iteration 153/1000 | Loss: 0.00001773
Iteration 154/1000 | Loss: 0.00001773
Iteration 155/1000 | Loss: 0.00001773
Iteration 156/1000 | Loss: 0.00001773
Iteration 157/1000 | Loss: 0.00001773
Iteration 158/1000 | Loss: 0.00001773
Iteration 159/1000 | Loss: 0.00001773
Iteration 160/1000 | Loss: 0.00001773
Iteration 161/1000 | Loss: 0.00001773
Iteration 162/1000 | Loss: 0.00001773
Iteration 163/1000 | Loss: 0.00001773
Iteration 164/1000 | Loss: 0.00001773
Iteration 165/1000 | Loss: 0.00001772
Iteration 166/1000 | Loss: 0.00001772
Iteration 167/1000 | Loss: 0.00001772
Iteration 168/1000 | Loss: 0.00001772
Iteration 169/1000 | Loss: 0.00001772
Iteration 170/1000 | Loss: 0.00001772
Iteration 171/1000 | Loss: 0.00001772
Iteration 172/1000 | Loss: 0.00001772
Iteration 173/1000 | Loss: 0.00001772
Iteration 174/1000 | Loss: 0.00001772
Iteration 175/1000 | Loss: 0.00001772
Iteration 176/1000 | Loss: 0.00001772
Iteration 177/1000 | Loss: 0.00001772
Iteration 178/1000 | Loss: 0.00001772
Iteration 179/1000 | Loss: 0.00001772
Iteration 180/1000 | Loss: 0.00001772
Iteration 181/1000 | Loss: 0.00001772
Iteration 182/1000 | Loss: 0.00001772
Iteration 183/1000 | Loss: 0.00001772
Iteration 184/1000 | Loss: 0.00001772
Iteration 185/1000 | Loss: 0.00001772
Iteration 186/1000 | Loss: 0.00001772
Iteration 187/1000 | Loss: 0.00001772
Iteration 188/1000 | Loss: 0.00001772
Iteration 189/1000 | Loss: 0.00001772
Iteration 190/1000 | Loss: 0.00001772
Iteration 191/1000 | Loss: 0.00001772
Iteration 192/1000 | Loss: 0.00001772
Iteration 193/1000 | Loss: 0.00001772
Iteration 194/1000 | Loss: 0.00001772
Iteration 195/1000 | Loss: 0.00001772
Iteration 196/1000 | Loss: 0.00001772
Iteration 197/1000 | Loss: 0.00001772
Iteration 198/1000 | Loss: 0.00001772
Iteration 199/1000 | Loss: 0.00001772
Iteration 200/1000 | Loss: 0.00001772
Iteration 201/1000 | Loss: 0.00001772
Iteration 202/1000 | Loss: 0.00001772
Iteration 203/1000 | Loss: 0.00001772
Iteration 204/1000 | Loss: 0.00001772
Iteration 205/1000 | Loss: 0.00001772
Iteration 206/1000 | Loss: 0.00001772
Iteration 207/1000 | Loss: 0.00001772
Iteration 208/1000 | Loss: 0.00001772
Iteration 209/1000 | Loss: 0.00001772
Iteration 210/1000 | Loss: 0.00001772
Iteration 211/1000 | Loss: 0.00001772
Iteration 212/1000 | Loss: 0.00001772
Iteration 213/1000 | Loss: 0.00001772
Iteration 214/1000 | Loss: 0.00001772
Iteration 215/1000 | Loss: 0.00001772
Iteration 216/1000 | Loss: 0.00001772
Iteration 217/1000 | Loss: 0.00001772
Iteration 218/1000 | Loss: 0.00001772
Iteration 219/1000 | Loss: 0.00001772
Iteration 220/1000 | Loss: 0.00001772
Iteration 221/1000 | Loss: 0.00001772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.7720140021992847e-05, 1.7720140021992847e-05, 1.7720140021992847e-05, 1.7720140021992847e-05, 1.7720140021992847e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7720140021992847e-05

Optimization complete. Final v2v error: 3.4952847957611084 mm

Highest mean error: 3.943464756011963 mm for frame 64

Lowest mean error: 3.0117223262786865 mm for frame 21

Saving results

Total time: 41.93302369117737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919378
Iteration 2/25 | Loss: 0.00127029
Iteration 3/25 | Loss: 0.00092046
Iteration 4/25 | Loss: 0.00083271
Iteration 5/25 | Loss: 0.00081819
Iteration 6/25 | Loss: 0.00081614
Iteration 7/25 | Loss: 0.00081581
Iteration 8/25 | Loss: 0.00081581
Iteration 9/25 | Loss: 0.00081581
Iteration 10/25 | Loss: 0.00081581
Iteration 11/25 | Loss: 0.00081581
Iteration 12/25 | Loss: 0.00081581
Iteration 13/25 | Loss: 0.00081581
Iteration 14/25 | Loss: 0.00081581
Iteration 15/25 | Loss: 0.00081581
Iteration 16/25 | Loss: 0.00081581
Iteration 17/25 | Loss: 0.00081581
Iteration 18/25 | Loss: 0.00081581
Iteration 19/25 | Loss: 0.00081581
Iteration 20/25 | Loss: 0.00081581
Iteration 21/25 | Loss: 0.00081581
Iteration 22/25 | Loss: 0.00081581
Iteration 23/25 | Loss: 0.00081581
Iteration 24/25 | Loss: 0.00081581
Iteration 25/25 | Loss: 0.00081581

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95553505
Iteration 2/25 | Loss: 0.00031765
Iteration 3/25 | Loss: 0.00031765
Iteration 4/25 | Loss: 0.00031765
Iteration 5/25 | Loss: 0.00031765
Iteration 6/25 | Loss: 0.00031765
Iteration 7/25 | Loss: 0.00031765
Iteration 8/25 | Loss: 0.00031765
Iteration 9/25 | Loss: 0.00031765
Iteration 10/25 | Loss: 0.00031765
Iteration 11/25 | Loss: 0.00031765
Iteration 12/25 | Loss: 0.00031765
Iteration 13/25 | Loss: 0.00031765
Iteration 14/25 | Loss: 0.00031765
Iteration 15/25 | Loss: 0.00031765
Iteration 16/25 | Loss: 0.00031765
Iteration 17/25 | Loss: 0.00031765
Iteration 18/25 | Loss: 0.00031765
Iteration 19/25 | Loss: 0.00031765
Iteration 20/25 | Loss: 0.00031765
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00031764720915816724, 0.00031764720915816724, 0.00031764720915816724, 0.00031764720915816724, 0.00031764720915816724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031764720915816724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031765
Iteration 2/1000 | Loss: 0.00006612
Iteration 3/1000 | Loss: 0.00004677
Iteration 4/1000 | Loss: 0.00003989
Iteration 5/1000 | Loss: 0.00003743
Iteration 6/1000 | Loss: 0.00003537
Iteration 7/1000 | Loss: 0.00003369
Iteration 8/1000 | Loss: 0.00003251
Iteration 9/1000 | Loss: 0.00003194
Iteration 10/1000 | Loss: 0.00003146
Iteration 11/1000 | Loss: 0.00003121
Iteration 12/1000 | Loss: 0.00003090
Iteration 13/1000 | Loss: 0.00003075
Iteration 14/1000 | Loss: 0.00003072
Iteration 15/1000 | Loss: 0.00003072
Iteration 16/1000 | Loss: 0.00003072
Iteration 17/1000 | Loss: 0.00003072
Iteration 18/1000 | Loss: 0.00003072
Iteration 19/1000 | Loss: 0.00003072
Iteration 20/1000 | Loss: 0.00003072
Iteration 21/1000 | Loss: 0.00003071
Iteration 22/1000 | Loss: 0.00003071
Iteration 23/1000 | Loss: 0.00003071
Iteration 24/1000 | Loss: 0.00003071
Iteration 25/1000 | Loss: 0.00003069
Iteration 26/1000 | Loss: 0.00003065
Iteration 27/1000 | Loss: 0.00003065
Iteration 28/1000 | Loss: 0.00003064
Iteration 29/1000 | Loss: 0.00003064
Iteration 30/1000 | Loss: 0.00003063
Iteration 31/1000 | Loss: 0.00003063
Iteration 32/1000 | Loss: 0.00003062
Iteration 33/1000 | Loss: 0.00003061
Iteration 34/1000 | Loss: 0.00003061
Iteration 35/1000 | Loss: 0.00003061
Iteration 36/1000 | Loss: 0.00003061
Iteration 37/1000 | Loss: 0.00003061
Iteration 38/1000 | Loss: 0.00003060
Iteration 39/1000 | Loss: 0.00003060
Iteration 40/1000 | Loss: 0.00003059
Iteration 41/1000 | Loss: 0.00003059
Iteration 42/1000 | Loss: 0.00003058
Iteration 43/1000 | Loss: 0.00003058
Iteration 44/1000 | Loss: 0.00003058
Iteration 45/1000 | Loss: 0.00003058
Iteration 46/1000 | Loss: 0.00003058
Iteration 47/1000 | Loss: 0.00003058
Iteration 48/1000 | Loss: 0.00003058
Iteration 49/1000 | Loss: 0.00003057
Iteration 50/1000 | Loss: 0.00003057
Iteration 51/1000 | Loss: 0.00003057
Iteration 52/1000 | Loss: 0.00003057
Iteration 53/1000 | Loss: 0.00003057
Iteration 54/1000 | Loss: 0.00003057
Iteration 55/1000 | Loss: 0.00003057
Iteration 56/1000 | Loss: 0.00003057
Iteration 57/1000 | Loss: 0.00003057
Iteration 58/1000 | Loss: 0.00003057
Iteration 59/1000 | Loss: 0.00003057
Iteration 60/1000 | Loss: 0.00003057
Iteration 61/1000 | Loss: 0.00003057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [3.0573661206290126e-05, 3.0573661206290126e-05, 3.0573661206290126e-05, 3.0573661206290126e-05, 3.0573661206290126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0573661206290126e-05

Optimization complete. Final v2v error: 4.642788887023926 mm

Highest mean error: 5.203253746032715 mm for frame 29

Lowest mean error: 4.133111476898193 mm for frame 141

Saving results

Total time: 31.891916751861572
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917105
Iteration 2/25 | Loss: 0.00159468
Iteration 3/25 | Loss: 0.00095182
Iteration 4/25 | Loss: 0.00084492
Iteration 5/25 | Loss: 0.00078282
Iteration 6/25 | Loss: 0.00078096
Iteration 7/25 | Loss: 0.00076175
Iteration 8/25 | Loss: 0.00076045
Iteration 9/25 | Loss: 0.00074834
Iteration 10/25 | Loss: 0.00073895
Iteration 11/25 | Loss: 0.00071570
Iteration 12/25 | Loss: 0.00070042
Iteration 13/25 | Loss: 0.00069630
Iteration 14/25 | Loss: 0.00068228
Iteration 15/25 | Loss: 0.00067558
Iteration 16/25 | Loss: 0.00067746
Iteration 17/25 | Loss: 0.00067433
Iteration 18/25 | Loss: 0.00067244
Iteration 19/25 | Loss: 0.00067250
Iteration 20/25 | Loss: 0.00067087
Iteration 21/25 | Loss: 0.00066990
Iteration 22/25 | Loss: 0.00068055
Iteration 23/25 | Loss: 0.00066687
Iteration 24/25 | Loss: 0.00066703
Iteration 25/25 | Loss: 0.00066659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.52912903
Iteration 2/25 | Loss: 0.00030527
Iteration 3/25 | Loss: 0.00030527
Iteration 4/25 | Loss: 0.00030527
Iteration 5/25 | Loss: 0.00030527
Iteration 6/25 | Loss: 0.00030527
Iteration 7/25 | Loss: 0.00030527
Iteration 8/25 | Loss: 0.00030527
Iteration 9/25 | Loss: 0.00030527
Iteration 10/25 | Loss: 0.00030527
Iteration 11/25 | Loss: 0.00030527
Iteration 12/25 | Loss: 0.00030527
Iteration 13/25 | Loss: 0.00030527
Iteration 14/25 | Loss: 0.00030527
Iteration 15/25 | Loss: 0.00030527
Iteration 16/25 | Loss: 0.00030527
Iteration 17/25 | Loss: 0.00030527
Iteration 18/25 | Loss: 0.00030527
Iteration 19/25 | Loss: 0.00030527
Iteration 20/25 | Loss: 0.00030527
Iteration 21/25 | Loss: 0.00030527
Iteration 22/25 | Loss: 0.00030527
Iteration 23/25 | Loss: 0.00030527
Iteration 24/25 | Loss: 0.00030527
Iteration 25/25 | Loss: 0.00030527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030527
Iteration 2/1000 | Loss: 0.00011595
Iteration 3/1000 | Loss: 0.00022074
Iteration 4/1000 | Loss: 0.00018361
Iteration 5/1000 | Loss: 0.00021830
Iteration 6/1000 | Loss: 0.00031776
Iteration 7/1000 | Loss: 0.00026965
Iteration 8/1000 | Loss: 0.00015304
Iteration 9/1000 | Loss: 0.00022054
Iteration 10/1000 | Loss: 0.00027234
Iteration 11/1000 | Loss: 0.00031157
Iteration 12/1000 | Loss: 0.00016329
Iteration 13/1000 | Loss: 0.00012902
Iteration 14/1000 | Loss: 0.00011317
Iteration 15/1000 | Loss: 0.00009818
Iteration 16/1000 | Loss: 0.00013586
Iteration 17/1000 | Loss: 0.00010952
Iteration 18/1000 | Loss: 0.00017810
Iteration 19/1000 | Loss: 0.00013155
Iteration 20/1000 | Loss: 0.00013794
Iteration 21/1000 | Loss: 0.00015653
Iteration 22/1000 | Loss: 0.00007905
Iteration 23/1000 | Loss: 0.00006287
Iteration 24/1000 | Loss: 0.00010097
Iteration 25/1000 | Loss: 0.00005234
Iteration 26/1000 | Loss: 0.00007336
Iteration 27/1000 | Loss: 0.00006333
Iteration 28/1000 | Loss: 0.00008574
Iteration 29/1000 | Loss: 0.00006015
Iteration 30/1000 | Loss: 0.00005260
Iteration 31/1000 | Loss: 0.00006063
Iteration 32/1000 | Loss: 0.00006605
Iteration 33/1000 | Loss: 0.00007977
Iteration 34/1000 | Loss: 0.00006806
Iteration 35/1000 | Loss: 0.00007279
Iteration 36/1000 | Loss: 0.00006398
Iteration 37/1000 | Loss: 0.00006289
Iteration 38/1000 | Loss: 0.00006632
Iteration 39/1000 | Loss: 0.00005276
Iteration 40/1000 | Loss: 0.00021289
Iteration 41/1000 | Loss: 0.00005643
Iteration 42/1000 | Loss: 0.00003668
Iteration 43/1000 | Loss: 0.00003763
Iteration 44/1000 | Loss: 0.00008402
Iteration 45/1000 | Loss: 0.00001914
Iteration 46/1000 | Loss: 0.00001812
Iteration 47/1000 | Loss: 0.00001772
Iteration 48/1000 | Loss: 0.00001760
Iteration 49/1000 | Loss: 0.00001732
Iteration 50/1000 | Loss: 0.00001727
Iteration 51/1000 | Loss: 0.00001706
Iteration 52/1000 | Loss: 0.00006634
Iteration 53/1000 | Loss: 0.00002163
Iteration 54/1000 | Loss: 0.00001814
Iteration 55/1000 | Loss: 0.00001690
Iteration 56/1000 | Loss: 0.00001688
Iteration 57/1000 | Loss: 0.00001681
Iteration 58/1000 | Loss: 0.00001675
Iteration 59/1000 | Loss: 0.00006718
Iteration 60/1000 | Loss: 0.00001710
Iteration 61/1000 | Loss: 0.00001674
Iteration 62/1000 | Loss: 0.00001669
Iteration 63/1000 | Loss: 0.00001668
Iteration 64/1000 | Loss: 0.00001668
Iteration 65/1000 | Loss: 0.00001665
Iteration 66/1000 | Loss: 0.00001665
Iteration 67/1000 | Loss: 0.00001665
Iteration 68/1000 | Loss: 0.00001665
Iteration 69/1000 | Loss: 0.00001665
Iteration 70/1000 | Loss: 0.00001664
Iteration 71/1000 | Loss: 0.00001664
Iteration 72/1000 | Loss: 0.00001664
Iteration 73/1000 | Loss: 0.00001659
Iteration 74/1000 | Loss: 0.00001659
Iteration 75/1000 | Loss: 0.00001659
Iteration 76/1000 | Loss: 0.00001658
Iteration 77/1000 | Loss: 0.00001658
Iteration 78/1000 | Loss: 0.00001658
Iteration 79/1000 | Loss: 0.00001657
Iteration 80/1000 | Loss: 0.00001657
Iteration 81/1000 | Loss: 0.00001657
Iteration 82/1000 | Loss: 0.00001657
Iteration 83/1000 | Loss: 0.00001657
Iteration 84/1000 | Loss: 0.00001657
Iteration 85/1000 | Loss: 0.00001657
Iteration 86/1000 | Loss: 0.00001657
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001657
Iteration 90/1000 | Loss: 0.00001657
Iteration 91/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.657271604926791e-05, 1.657271604926791e-05, 1.657271604926791e-05, 1.657271604926791e-05, 1.657271604926791e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.657271604926791e-05

Optimization complete. Final v2v error: 3.432955741882324 mm

Highest mean error: 5.131570816040039 mm for frame 95

Lowest mean error: 2.837185859680176 mm for frame 124

Saving results

Total time: 135.0839695930481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836100
Iteration 2/25 | Loss: 0.00089738
Iteration 3/25 | Loss: 0.00068856
Iteration 4/25 | Loss: 0.00063696
Iteration 5/25 | Loss: 0.00062699
Iteration 6/25 | Loss: 0.00062422
Iteration 7/25 | Loss: 0.00062385
Iteration 8/25 | Loss: 0.00062385
Iteration 9/25 | Loss: 0.00062385
Iteration 10/25 | Loss: 0.00062385
Iteration 11/25 | Loss: 0.00062385
Iteration 12/25 | Loss: 0.00062385
Iteration 13/25 | Loss: 0.00062385
Iteration 14/25 | Loss: 0.00062385
Iteration 15/25 | Loss: 0.00062385
Iteration 16/25 | Loss: 0.00062385
Iteration 17/25 | Loss: 0.00062385
Iteration 18/25 | Loss: 0.00062385
Iteration 19/25 | Loss: 0.00062385
Iteration 20/25 | Loss: 0.00062385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006238471833057702, 0.0006238471833057702, 0.0006238471833057702, 0.0006238471833057702, 0.0006238471833057702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006238471833057702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38019872
Iteration 2/25 | Loss: 0.00009539
Iteration 3/25 | Loss: 0.00009539
Iteration 4/25 | Loss: 0.00009539
Iteration 5/25 | Loss: 0.00009539
Iteration 6/25 | Loss: 0.00009539
Iteration 7/25 | Loss: 0.00009539
Iteration 8/25 | Loss: 0.00009539
Iteration 9/25 | Loss: 0.00009539
Iteration 10/25 | Loss: 0.00009539
Iteration 11/25 | Loss: 0.00009539
Iteration 12/25 | Loss: 0.00009539
Iteration 13/25 | Loss: 0.00009539
Iteration 14/25 | Loss: 0.00009539
Iteration 15/25 | Loss: 0.00009539
Iteration 16/25 | Loss: 0.00009539
Iteration 17/25 | Loss: 0.00009539
Iteration 18/25 | Loss: 0.00009539
Iteration 19/25 | Loss: 0.00009539
Iteration 20/25 | Loss: 0.00009539
Iteration 21/25 | Loss: 0.00009539
Iteration 22/25 | Loss: 0.00009539
Iteration 23/25 | Loss: 0.00009539
Iteration 24/25 | Loss: 0.00009539
Iteration 25/25 | Loss: 0.00009539

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00009539
Iteration 2/1000 | Loss: 0.00002196
Iteration 3/1000 | Loss: 0.00001663
Iteration 4/1000 | Loss: 0.00001553
Iteration 5/1000 | Loss: 0.00001491
Iteration 6/1000 | Loss: 0.00001447
Iteration 7/1000 | Loss: 0.00001419
Iteration 8/1000 | Loss: 0.00001401
Iteration 9/1000 | Loss: 0.00001395
Iteration 10/1000 | Loss: 0.00001392
Iteration 11/1000 | Loss: 0.00001391
Iteration 12/1000 | Loss: 0.00001389
Iteration 13/1000 | Loss: 0.00001389
Iteration 14/1000 | Loss: 0.00001388
Iteration 15/1000 | Loss: 0.00001388
Iteration 16/1000 | Loss: 0.00001387
Iteration 17/1000 | Loss: 0.00001386
Iteration 18/1000 | Loss: 0.00001386
Iteration 19/1000 | Loss: 0.00001385
Iteration 20/1000 | Loss: 0.00001385
Iteration 21/1000 | Loss: 0.00001385
Iteration 22/1000 | Loss: 0.00001382
Iteration 23/1000 | Loss: 0.00001382
Iteration 24/1000 | Loss: 0.00001381
Iteration 25/1000 | Loss: 0.00001381
Iteration 26/1000 | Loss: 0.00001379
Iteration 27/1000 | Loss: 0.00001379
Iteration 28/1000 | Loss: 0.00001379
Iteration 29/1000 | Loss: 0.00001379
Iteration 30/1000 | Loss: 0.00001379
Iteration 31/1000 | Loss: 0.00001379
Iteration 32/1000 | Loss: 0.00001379
Iteration 33/1000 | Loss: 0.00001379
Iteration 34/1000 | Loss: 0.00001379
Iteration 35/1000 | Loss: 0.00001379
Iteration 36/1000 | Loss: 0.00001379
Iteration 37/1000 | Loss: 0.00001379
Iteration 38/1000 | Loss: 0.00001378
Iteration 39/1000 | Loss: 0.00001378
Iteration 40/1000 | Loss: 0.00001378
Iteration 41/1000 | Loss: 0.00001378
Iteration 42/1000 | Loss: 0.00001377
Iteration 43/1000 | Loss: 0.00001377
Iteration 44/1000 | Loss: 0.00001377
Iteration 45/1000 | Loss: 0.00001377
Iteration 46/1000 | Loss: 0.00001376
Iteration 47/1000 | Loss: 0.00001376
Iteration 48/1000 | Loss: 0.00001376
Iteration 49/1000 | Loss: 0.00001376
Iteration 50/1000 | Loss: 0.00001375
Iteration 51/1000 | Loss: 0.00001375
Iteration 52/1000 | Loss: 0.00001375
Iteration 53/1000 | Loss: 0.00001375
Iteration 54/1000 | Loss: 0.00001375
Iteration 55/1000 | Loss: 0.00001375
Iteration 56/1000 | Loss: 0.00001374
Iteration 57/1000 | Loss: 0.00001374
Iteration 58/1000 | Loss: 0.00001374
Iteration 59/1000 | Loss: 0.00001374
Iteration 60/1000 | Loss: 0.00001374
Iteration 61/1000 | Loss: 0.00001374
Iteration 62/1000 | Loss: 0.00001374
Iteration 63/1000 | Loss: 0.00001374
Iteration 64/1000 | Loss: 0.00001374
Iteration 65/1000 | Loss: 0.00001374
Iteration 66/1000 | Loss: 0.00001374
Iteration 67/1000 | Loss: 0.00001374
Iteration 68/1000 | Loss: 0.00001374
Iteration 69/1000 | Loss: 0.00001374
Iteration 70/1000 | Loss: 0.00001374
Iteration 71/1000 | Loss: 0.00001374
Iteration 72/1000 | Loss: 0.00001374
Iteration 73/1000 | Loss: 0.00001374
Iteration 74/1000 | Loss: 0.00001374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.3736043001699727e-05, 1.3736043001699727e-05, 1.3736043001699727e-05, 1.3736043001699727e-05, 1.3736043001699727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3736043001699727e-05

Optimization complete. Final v2v error: 3.113358736038208 mm

Highest mean error: 3.3829879760742188 mm for frame 130

Lowest mean error: 2.5195019245147705 mm for frame 1

Saving results

Total time: 31.290237426757812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00609055
Iteration 2/25 | Loss: 0.00101421
Iteration 3/25 | Loss: 0.00079254
Iteration 4/25 | Loss: 0.00073647
Iteration 5/25 | Loss: 0.00072293
Iteration 6/25 | Loss: 0.00071839
Iteration 7/25 | Loss: 0.00071708
Iteration 8/25 | Loss: 0.00071682
Iteration 9/25 | Loss: 0.00071682
Iteration 10/25 | Loss: 0.00071682
Iteration 11/25 | Loss: 0.00071682
Iteration 12/25 | Loss: 0.00071682
Iteration 13/25 | Loss: 0.00071682
Iteration 14/25 | Loss: 0.00071682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007168164011090994, 0.0007168164011090994, 0.0007168164011090994, 0.0007168164011090994, 0.0007168164011090994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007168164011090994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08386469
Iteration 2/25 | Loss: 0.00014653
Iteration 3/25 | Loss: 0.00014644
Iteration 4/25 | Loss: 0.00014644
Iteration 5/25 | Loss: 0.00014644
Iteration 6/25 | Loss: 0.00014644
Iteration 7/25 | Loss: 0.00014644
Iteration 8/25 | Loss: 0.00014644
Iteration 9/25 | Loss: 0.00014644
Iteration 10/25 | Loss: 0.00014644
Iteration 11/25 | Loss: 0.00014644
Iteration 12/25 | Loss: 0.00014644
Iteration 13/25 | Loss: 0.00014644
Iteration 14/25 | Loss: 0.00014644
Iteration 15/25 | Loss: 0.00014644
Iteration 16/25 | Loss: 0.00014644
Iteration 17/25 | Loss: 0.00014644
Iteration 18/25 | Loss: 0.00014644
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00014643854228779674, 0.00014643854228779674, 0.00014643854228779674, 0.00014643854228779674, 0.00014643854228779674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014643854228779674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00014644
Iteration 2/1000 | Loss: 0.00003835
Iteration 3/1000 | Loss: 0.00003116
Iteration 4/1000 | Loss: 0.00002736
Iteration 5/1000 | Loss: 0.00002631
Iteration 6/1000 | Loss: 0.00002572
Iteration 7/1000 | Loss: 0.00002519
Iteration 8/1000 | Loss: 0.00002463
Iteration 9/1000 | Loss: 0.00002424
Iteration 10/1000 | Loss: 0.00002393
Iteration 11/1000 | Loss: 0.00002373
Iteration 12/1000 | Loss: 0.00002368
Iteration 13/1000 | Loss: 0.00002348
Iteration 14/1000 | Loss: 0.00002347
Iteration 15/1000 | Loss: 0.00002337
Iteration 16/1000 | Loss: 0.00002331
Iteration 17/1000 | Loss: 0.00002319
Iteration 18/1000 | Loss: 0.00002305
Iteration 19/1000 | Loss: 0.00002303
Iteration 20/1000 | Loss: 0.00002303
Iteration 21/1000 | Loss: 0.00002302
Iteration 22/1000 | Loss: 0.00002302
Iteration 23/1000 | Loss: 0.00002302
Iteration 24/1000 | Loss: 0.00002302
Iteration 25/1000 | Loss: 0.00002302
Iteration 26/1000 | Loss: 0.00002302
Iteration 27/1000 | Loss: 0.00002302
Iteration 28/1000 | Loss: 0.00002302
Iteration 29/1000 | Loss: 0.00002298
Iteration 30/1000 | Loss: 0.00002297
Iteration 31/1000 | Loss: 0.00002297
Iteration 32/1000 | Loss: 0.00002296
Iteration 33/1000 | Loss: 0.00002296
Iteration 34/1000 | Loss: 0.00002296
Iteration 35/1000 | Loss: 0.00002296
Iteration 36/1000 | Loss: 0.00002295
Iteration 37/1000 | Loss: 0.00002295
Iteration 38/1000 | Loss: 0.00002293
Iteration 39/1000 | Loss: 0.00002292
Iteration 40/1000 | Loss: 0.00002292
Iteration 41/1000 | Loss: 0.00002292
Iteration 42/1000 | Loss: 0.00002291
Iteration 43/1000 | Loss: 0.00002289
Iteration 44/1000 | Loss: 0.00002286
Iteration 45/1000 | Loss: 0.00002285
Iteration 46/1000 | Loss: 0.00002284
Iteration 47/1000 | Loss: 0.00002281
Iteration 48/1000 | Loss: 0.00002278
Iteration 49/1000 | Loss: 0.00002277
Iteration 50/1000 | Loss: 0.00002277
Iteration 51/1000 | Loss: 0.00002277
Iteration 52/1000 | Loss: 0.00002277
Iteration 53/1000 | Loss: 0.00002276
Iteration 54/1000 | Loss: 0.00002276
Iteration 55/1000 | Loss: 0.00002276
Iteration 56/1000 | Loss: 0.00002276
Iteration 57/1000 | Loss: 0.00002276
Iteration 58/1000 | Loss: 0.00002275
Iteration 59/1000 | Loss: 0.00002273
Iteration 60/1000 | Loss: 0.00002273
Iteration 61/1000 | Loss: 0.00002273
Iteration 62/1000 | Loss: 0.00002273
Iteration 63/1000 | Loss: 0.00002273
Iteration 64/1000 | Loss: 0.00002273
Iteration 65/1000 | Loss: 0.00002273
Iteration 66/1000 | Loss: 0.00002273
Iteration 67/1000 | Loss: 0.00002273
Iteration 68/1000 | Loss: 0.00002272
Iteration 69/1000 | Loss: 0.00002272
Iteration 70/1000 | Loss: 0.00002272
Iteration 71/1000 | Loss: 0.00002272
Iteration 72/1000 | Loss: 0.00002272
Iteration 73/1000 | Loss: 0.00002272
Iteration 74/1000 | Loss: 0.00002272
Iteration 75/1000 | Loss: 0.00002272
Iteration 76/1000 | Loss: 0.00002271
Iteration 77/1000 | Loss: 0.00002271
Iteration 78/1000 | Loss: 0.00002271
Iteration 79/1000 | Loss: 0.00002270
Iteration 80/1000 | Loss: 0.00002270
Iteration 81/1000 | Loss: 0.00002269
Iteration 82/1000 | Loss: 0.00002269
Iteration 83/1000 | Loss: 0.00002269
Iteration 84/1000 | Loss: 0.00002269
Iteration 85/1000 | Loss: 0.00002269
Iteration 86/1000 | Loss: 0.00002269
Iteration 87/1000 | Loss: 0.00002269
Iteration 88/1000 | Loss: 0.00002268
Iteration 89/1000 | Loss: 0.00002268
Iteration 90/1000 | Loss: 0.00002268
Iteration 91/1000 | Loss: 0.00002268
Iteration 92/1000 | Loss: 0.00002267
Iteration 93/1000 | Loss: 0.00002267
Iteration 94/1000 | Loss: 0.00002266
Iteration 95/1000 | Loss: 0.00002265
Iteration 96/1000 | Loss: 0.00002265
Iteration 97/1000 | Loss: 0.00002265
Iteration 98/1000 | Loss: 0.00002264
Iteration 99/1000 | Loss: 0.00002264
Iteration 100/1000 | Loss: 0.00002264
Iteration 101/1000 | Loss: 0.00002264
Iteration 102/1000 | Loss: 0.00002264
Iteration 103/1000 | Loss: 0.00002263
Iteration 104/1000 | Loss: 0.00002263
Iteration 105/1000 | Loss: 0.00002263
Iteration 106/1000 | Loss: 0.00002262
Iteration 107/1000 | Loss: 0.00002262
Iteration 108/1000 | Loss: 0.00002262
Iteration 109/1000 | Loss: 0.00002261
Iteration 110/1000 | Loss: 0.00002260
Iteration 111/1000 | Loss: 0.00002260
Iteration 112/1000 | Loss: 0.00002260
Iteration 113/1000 | Loss: 0.00002260
Iteration 114/1000 | Loss: 0.00002259
Iteration 115/1000 | Loss: 0.00002259
Iteration 116/1000 | Loss: 0.00002259
Iteration 117/1000 | Loss: 0.00002259
Iteration 118/1000 | Loss: 0.00002259
Iteration 119/1000 | Loss: 0.00002259
Iteration 120/1000 | Loss: 0.00002259
Iteration 121/1000 | Loss: 0.00002259
Iteration 122/1000 | Loss: 0.00002259
Iteration 123/1000 | Loss: 0.00002259
Iteration 124/1000 | Loss: 0.00002259
Iteration 125/1000 | Loss: 0.00002259
Iteration 126/1000 | Loss: 0.00002259
Iteration 127/1000 | Loss: 0.00002259
Iteration 128/1000 | Loss: 0.00002259
Iteration 129/1000 | Loss: 0.00002259
Iteration 130/1000 | Loss: 0.00002259
Iteration 131/1000 | Loss: 0.00002259
Iteration 132/1000 | Loss: 0.00002259
Iteration 133/1000 | Loss: 0.00002259
Iteration 134/1000 | Loss: 0.00002259
Iteration 135/1000 | Loss: 0.00002259
Iteration 136/1000 | Loss: 0.00002259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.2588314095628448e-05, 2.2588314095628448e-05, 2.2588314095628448e-05, 2.2588314095628448e-05, 2.2588314095628448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2588314095628448e-05

Optimization complete. Final v2v error: 4.032418251037598 mm

Highest mean error: 4.590610027313232 mm for frame 65

Lowest mean error: 3.406484365463257 mm for frame 0

Saving results

Total time: 44.30596923828125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809214
Iteration 2/25 | Loss: 0.00113787
Iteration 3/25 | Loss: 0.00087455
Iteration 4/25 | Loss: 0.00080276
Iteration 5/25 | Loss: 0.00077940
Iteration 6/25 | Loss: 0.00077428
Iteration 7/25 | Loss: 0.00077227
Iteration 8/25 | Loss: 0.00077183
Iteration 9/25 | Loss: 0.00077167
Iteration 10/25 | Loss: 0.00077154
Iteration 11/25 | Loss: 0.00077140
Iteration 12/25 | Loss: 0.00077119
Iteration 13/25 | Loss: 0.00077100
Iteration 14/25 | Loss: 0.00077074
Iteration 15/25 | Loss: 0.00077013
Iteration 16/25 | Loss: 0.00077211
Iteration 17/25 | Loss: 0.00077324
Iteration 18/25 | Loss: 0.00077025
Iteration 19/25 | Loss: 0.00077188
Iteration 20/25 | Loss: 0.00077300
Iteration 21/25 | Loss: 0.00077329
Iteration 22/25 | Loss: 0.00077279
Iteration 23/25 | Loss: 0.00077264
Iteration 24/25 | Loss: 0.00077278
Iteration 25/25 | Loss: 0.00077225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.89680147
Iteration 2/25 | Loss: 0.00029797
Iteration 3/25 | Loss: 0.00029780
Iteration 4/25 | Loss: 0.00029780
Iteration 5/25 | Loss: 0.00029780
Iteration 6/25 | Loss: 0.00029780
Iteration 7/25 | Loss: 0.00029780
Iteration 8/25 | Loss: 0.00029780
Iteration 9/25 | Loss: 0.00029780
Iteration 10/25 | Loss: 0.00029780
Iteration 11/25 | Loss: 0.00029780
Iteration 12/25 | Loss: 0.00029780
Iteration 13/25 | Loss: 0.00029780
Iteration 14/25 | Loss: 0.00029780
Iteration 15/25 | Loss: 0.00029780
Iteration 16/25 | Loss: 0.00029780
Iteration 17/25 | Loss: 0.00029780
Iteration 18/25 | Loss: 0.00029780
Iteration 19/25 | Loss: 0.00029780
Iteration 20/25 | Loss: 0.00029780
Iteration 21/25 | Loss: 0.00029780
Iteration 22/25 | Loss: 0.00029780
Iteration 23/25 | Loss: 0.00029780
Iteration 24/25 | Loss: 0.00029780
Iteration 25/25 | Loss: 0.00029780

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029780
Iteration 2/1000 | Loss: 0.00008370
Iteration 3/1000 | Loss: 0.00013900
Iteration 4/1000 | Loss: 0.00005706
Iteration 5/1000 | Loss: 0.00014992
Iteration 6/1000 | Loss: 0.00013905
Iteration 7/1000 | Loss: 0.00009207
Iteration 8/1000 | Loss: 0.00011079
Iteration 9/1000 | Loss: 0.00016444
Iteration 10/1000 | Loss: 0.00016705
Iteration 11/1000 | Loss: 0.00009291
Iteration 12/1000 | Loss: 0.00010620
Iteration 13/1000 | Loss: 0.00004773
Iteration 14/1000 | Loss: 0.00012423
Iteration 15/1000 | Loss: 0.00012713
Iteration 16/1000 | Loss: 0.00013266
Iteration 17/1000 | Loss: 0.00018648
Iteration 18/1000 | Loss: 0.00015193
Iteration 19/1000 | Loss: 0.00013388
Iteration 20/1000 | Loss: 0.00017097
Iteration 21/1000 | Loss: 0.00008377
Iteration 22/1000 | Loss: 0.00009496
Iteration 23/1000 | Loss: 0.00014791
Iteration 24/1000 | Loss: 0.00014709
Iteration 25/1000 | Loss: 0.00016729
Iteration 26/1000 | Loss: 0.00022060
Iteration 27/1000 | Loss: 0.00016507
Iteration 28/1000 | Loss: 0.00014448
Iteration 29/1000 | Loss: 0.00005407
Iteration 30/1000 | Loss: 0.00004243
Iteration 31/1000 | Loss: 0.00004461
Iteration 32/1000 | Loss: 0.00015663
Iteration 33/1000 | Loss: 0.00013166
Iteration 34/1000 | Loss: 0.00010528
Iteration 35/1000 | Loss: 0.00013154
Iteration 36/1000 | Loss: 0.00007131
Iteration 37/1000 | Loss: 0.00011457
Iteration 38/1000 | Loss: 0.00008894
Iteration 39/1000 | Loss: 0.00008194
Iteration 40/1000 | Loss: 0.00012544
Iteration 41/1000 | Loss: 0.00012420
Iteration 42/1000 | Loss: 0.00015171
Iteration 43/1000 | Loss: 0.00012756
Iteration 44/1000 | Loss: 0.00017819
Iteration 45/1000 | Loss: 0.00013755
Iteration 46/1000 | Loss: 0.00009457
Iteration 47/1000 | Loss: 0.00014978
Iteration 48/1000 | Loss: 0.00014156
Iteration 49/1000 | Loss: 0.00016598
Iteration 50/1000 | Loss: 0.00004723
Iteration 51/1000 | Loss: 0.00007360
Iteration 52/1000 | Loss: 0.00019016
Iteration 53/1000 | Loss: 0.00008633
Iteration 54/1000 | Loss: 0.00010950
Iteration 55/1000 | Loss: 0.00005144
Iteration 56/1000 | Loss: 0.00005826
Iteration 57/1000 | Loss: 0.00005378
Iteration 58/1000 | Loss: 0.00004977
Iteration 59/1000 | Loss: 0.00006315
Iteration 60/1000 | Loss: 0.00003479
Iteration 61/1000 | Loss: 0.00012191
Iteration 62/1000 | Loss: 0.00009857
Iteration 63/1000 | Loss: 0.00011284
Iteration 64/1000 | Loss: 0.00012272
Iteration 65/1000 | Loss: 0.00005180
Iteration 66/1000 | Loss: 0.00015002
Iteration 67/1000 | Loss: 0.00006358
Iteration 68/1000 | Loss: 0.00009834
Iteration 69/1000 | Loss: 0.00014235
Iteration 70/1000 | Loss: 0.00010799
Iteration 71/1000 | Loss: 0.00014949
Iteration 72/1000 | Loss: 0.00026038
Iteration 73/1000 | Loss: 0.00004075
Iteration 74/1000 | Loss: 0.00025888
Iteration 75/1000 | Loss: 0.00008041
Iteration 76/1000 | Loss: 0.00003604
Iteration 77/1000 | Loss: 0.00003359
Iteration 78/1000 | Loss: 0.00003251
Iteration 79/1000 | Loss: 0.00003195
Iteration 80/1000 | Loss: 0.00003130
Iteration 81/1000 | Loss: 0.00003087
Iteration 82/1000 | Loss: 0.00003057
Iteration 83/1000 | Loss: 0.00003035
Iteration 84/1000 | Loss: 0.00003034
Iteration 85/1000 | Loss: 0.00003033
Iteration 86/1000 | Loss: 0.00003016
Iteration 87/1000 | Loss: 0.00003011
Iteration 88/1000 | Loss: 0.00002996
Iteration 89/1000 | Loss: 0.00002992
Iteration 90/1000 | Loss: 0.00002992
Iteration 91/1000 | Loss: 0.00002989
Iteration 92/1000 | Loss: 0.00002989
Iteration 93/1000 | Loss: 0.00002988
Iteration 94/1000 | Loss: 0.00002988
Iteration 95/1000 | Loss: 0.00002988
Iteration 96/1000 | Loss: 0.00002988
Iteration 97/1000 | Loss: 0.00002988
Iteration 98/1000 | Loss: 0.00002988
Iteration 99/1000 | Loss: 0.00002988
Iteration 100/1000 | Loss: 0.00002988
Iteration 101/1000 | Loss: 0.00002988
Iteration 102/1000 | Loss: 0.00002987
Iteration 103/1000 | Loss: 0.00002987
Iteration 104/1000 | Loss: 0.00002985
Iteration 105/1000 | Loss: 0.00002985
Iteration 106/1000 | Loss: 0.00002985
Iteration 107/1000 | Loss: 0.00002984
Iteration 108/1000 | Loss: 0.00002984
Iteration 109/1000 | Loss: 0.00002984
Iteration 110/1000 | Loss: 0.00002983
Iteration 111/1000 | Loss: 0.00002983
Iteration 112/1000 | Loss: 0.00002982
Iteration 113/1000 | Loss: 0.00002982
Iteration 114/1000 | Loss: 0.00002982
Iteration 115/1000 | Loss: 0.00002982
Iteration 116/1000 | Loss: 0.00002981
Iteration 117/1000 | Loss: 0.00002981
Iteration 118/1000 | Loss: 0.00002981
Iteration 119/1000 | Loss: 0.00002981
Iteration 120/1000 | Loss: 0.00002981
Iteration 121/1000 | Loss: 0.00002981
Iteration 122/1000 | Loss: 0.00002981
Iteration 123/1000 | Loss: 0.00002980
Iteration 124/1000 | Loss: 0.00002980
Iteration 125/1000 | Loss: 0.00002980
Iteration 126/1000 | Loss: 0.00002979
Iteration 127/1000 | Loss: 0.00002979
Iteration 128/1000 | Loss: 0.00002978
Iteration 129/1000 | Loss: 0.00002977
Iteration 130/1000 | Loss: 0.00002977
Iteration 131/1000 | Loss: 0.00002976
Iteration 132/1000 | Loss: 0.00002976
Iteration 133/1000 | Loss: 0.00002976
Iteration 134/1000 | Loss: 0.00002975
Iteration 135/1000 | Loss: 0.00002975
Iteration 136/1000 | Loss: 0.00002975
Iteration 137/1000 | Loss: 0.00002974
Iteration 138/1000 | Loss: 0.00002974
Iteration 139/1000 | Loss: 0.00002974
Iteration 140/1000 | Loss: 0.00002974
Iteration 141/1000 | Loss: 0.00002974
Iteration 142/1000 | Loss: 0.00002974
Iteration 143/1000 | Loss: 0.00002974
Iteration 144/1000 | Loss: 0.00002973
Iteration 145/1000 | Loss: 0.00002973
Iteration 146/1000 | Loss: 0.00002973
Iteration 147/1000 | Loss: 0.00002973
Iteration 148/1000 | Loss: 0.00002973
Iteration 149/1000 | Loss: 0.00002973
Iteration 150/1000 | Loss: 0.00002973
Iteration 151/1000 | Loss: 0.00002973
Iteration 152/1000 | Loss: 0.00002973
Iteration 153/1000 | Loss: 0.00002972
Iteration 154/1000 | Loss: 0.00002972
Iteration 155/1000 | Loss: 0.00002972
Iteration 156/1000 | Loss: 0.00002972
Iteration 157/1000 | Loss: 0.00002971
Iteration 158/1000 | Loss: 0.00002971
Iteration 159/1000 | Loss: 0.00002971
Iteration 160/1000 | Loss: 0.00002971
Iteration 161/1000 | Loss: 0.00002970
Iteration 162/1000 | Loss: 0.00002970
Iteration 163/1000 | Loss: 0.00002970
Iteration 164/1000 | Loss: 0.00002970
Iteration 165/1000 | Loss: 0.00002970
Iteration 166/1000 | Loss: 0.00002969
Iteration 167/1000 | Loss: 0.00002969
Iteration 168/1000 | Loss: 0.00002969
Iteration 169/1000 | Loss: 0.00002969
Iteration 170/1000 | Loss: 0.00002969
Iteration 171/1000 | Loss: 0.00002969
Iteration 172/1000 | Loss: 0.00002969
Iteration 173/1000 | Loss: 0.00002969
Iteration 174/1000 | Loss: 0.00002969
Iteration 175/1000 | Loss: 0.00002969
Iteration 176/1000 | Loss: 0.00002969
Iteration 177/1000 | Loss: 0.00002969
Iteration 178/1000 | Loss: 0.00002968
Iteration 179/1000 | Loss: 0.00002968
Iteration 180/1000 | Loss: 0.00002968
Iteration 181/1000 | Loss: 0.00002968
Iteration 182/1000 | Loss: 0.00002968
Iteration 183/1000 | Loss: 0.00002968
Iteration 184/1000 | Loss: 0.00002968
Iteration 185/1000 | Loss: 0.00002968
Iteration 186/1000 | Loss: 0.00002968
Iteration 187/1000 | Loss: 0.00002968
Iteration 188/1000 | Loss: 0.00002968
Iteration 189/1000 | Loss: 0.00002968
Iteration 190/1000 | Loss: 0.00002968
Iteration 191/1000 | Loss: 0.00002968
Iteration 192/1000 | Loss: 0.00002968
Iteration 193/1000 | Loss: 0.00002968
Iteration 194/1000 | Loss: 0.00002968
Iteration 195/1000 | Loss: 0.00002968
Iteration 196/1000 | Loss: 0.00002968
Iteration 197/1000 | Loss: 0.00002968
Iteration 198/1000 | Loss: 0.00002968
Iteration 199/1000 | Loss: 0.00002968
Iteration 200/1000 | Loss: 0.00002968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.9683038519578986e-05, 2.9683038519578986e-05, 2.9683038519578986e-05, 2.9683038519578986e-05, 2.9683038519578986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9683038519578986e-05

Optimization complete. Final v2v error: 4.624777317047119 mm

Highest mean error: 6.606389045715332 mm for frame 107

Lowest mean error: 3.4173495769500732 mm for frame 231

Saving results

Total time: 197.95643162727356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0169/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0169/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01287393
Iteration 2/25 | Loss: 0.00398545
Iteration 3/25 | Loss: 0.00258737
Iteration 4/25 | Loss: 0.00221279
Iteration 5/25 | Loss: 0.00178414
Iteration 6/25 | Loss: 0.00155769
Iteration 7/25 | Loss: 0.00145297
Iteration 8/25 | Loss: 0.00140507
Iteration 9/25 | Loss: 0.00134882
Iteration 10/25 | Loss: 0.00133827
Iteration 11/25 | Loss: 0.00131783
Iteration 12/25 | Loss: 0.00130956
Iteration 13/25 | Loss: 0.00129512
Iteration 14/25 | Loss: 0.00129328
Iteration 15/25 | Loss: 0.00129048
Iteration 16/25 | Loss: 0.00128493
Iteration 17/25 | Loss: 0.00129350
Iteration 18/25 | Loss: 0.00128641
Iteration 19/25 | Loss: 0.00127463
Iteration 20/25 | Loss: 0.00126829
Iteration 21/25 | Loss: 0.00127199
Iteration 22/25 | Loss: 0.00127146
Iteration 23/25 | Loss: 0.00127523
Iteration 24/25 | Loss: 0.00127272
Iteration 25/25 | Loss: 0.00126295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.61479729
Iteration 2/25 | Loss: 0.00427886
Iteration 3/25 | Loss: 0.00427886
Iteration 4/25 | Loss: 0.00427886
Iteration 5/25 | Loss: 0.00427886
Iteration 6/25 | Loss: 0.00427886
Iteration 7/25 | Loss: 0.00427886
Iteration 8/25 | Loss: 0.00427886
Iteration 9/25 | Loss: 0.00427886
Iteration 10/25 | Loss: 0.00427886
Iteration 11/25 | Loss: 0.00427886
Iteration 12/25 | Loss: 0.00427886
Iteration 13/25 | Loss: 0.00427886
Iteration 14/25 | Loss: 0.00427886
Iteration 15/25 | Loss: 0.00427886
Iteration 16/25 | Loss: 0.00427886
Iteration 17/25 | Loss: 0.00427886
Iteration 18/25 | Loss: 0.00427886
Iteration 19/25 | Loss: 0.00427886
Iteration 20/25 | Loss: 0.00427886
Iteration 21/25 | Loss: 0.00427886
Iteration 22/25 | Loss: 0.00427886
Iteration 23/25 | Loss: 0.00427886
Iteration 24/25 | Loss: 0.00427886
Iteration 25/25 | Loss: 0.00427886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00427886
Iteration 2/1000 | Loss: 0.00147902
Iteration 3/1000 | Loss: 0.00475211
Iteration 4/1000 | Loss: 0.00541532
Iteration 5/1000 | Loss: 0.00444539
Iteration 6/1000 | Loss: 0.00161095
Iteration 7/1000 | Loss: 0.00215688
Iteration 8/1000 | Loss: 0.00085557
Iteration 9/1000 | Loss: 0.00151684
Iteration 10/1000 | Loss: 0.00080760
Iteration 11/1000 | Loss: 0.00116994
Iteration 12/1000 | Loss: 0.00104963
Iteration 13/1000 | Loss: 0.00198038
Iteration 14/1000 | Loss: 0.00063928
Iteration 15/1000 | Loss: 0.00150924
Iteration 16/1000 | Loss: 0.00114508
Iteration 17/1000 | Loss: 0.00069560
Iteration 18/1000 | Loss: 0.00150557
Iteration 19/1000 | Loss: 0.00115067
Iteration 20/1000 | Loss: 0.00270541
Iteration 21/1000 | Loss: 0.00160751
Iteration 22/1000 | Loss: 0.00135725
Iteration 23/1000 | Loss: 0.00231843
Iteration 24/1000 | Loss: 0.00204136
Iteration 25/1000 | Loss: 0.00192585
Iteration 26/1000 | Loss: 0.00108741
Iteration 27/1000 | Loss: 0.00154215
Iteration 28/1000 | Loss: 0.00147890
Iteration 29/1000 | Loss: 0.00203438
Iteration 30/1000 | Loss: 0.00174572
Iteration 31/1000 | Loss: 0.00197302
Iteration 32/1000 | Loss: 0.00083105
Iteration 33/1000 | Loss: 0.00160952
Iteration 34/1000 | Loss: 0.00226509
Iteration 35/1000 | Loss: 0.00104234
Iteration 36/1000 | Loss: 0.00098633
Iteration 37/1000 | Loss: 0.00134118
Iteration 38/1000 | Loss: 0.00101887
Iteration 39/1000 | Loss: 0.00128161
Iteration 40/1000 | Loss: 0.00118174
Iteration 41/1000 | Loss: 0.00090456
Iteration 42/1000 | Loss: 0.00081948
Iteration 43/1000 | Loss: 0.00088443
Iteration 44/1000 | Loss: 0.00074455
Iteration 45/1000 | Loss: 0.00093170
Iteration 46/1000 | Loss: 0.00087446
Iteration 47/1000 | Loss: 0.00317594
Iteration 48/1000 | Loss: 0.00107604
Iteration 49/1000 | Loss: 0.00130477
Iteration 50/1000 | Loss: 0.00087131
Iteration 51/1000 | Loss: 0.00083155
Iteration 52/1000 | Loss: 0.00081534
Iteration 53/1000 | Loss: 0.00136173
Iteration 54/1000 | Loss: 0.00076809
Iteration 55/1000 | Loss: 0.00153400
Iteration 56/1000 | Loss: 0.00167913
Iteration 57/1000 | Loss: 0.00083151
Iteration 58/1000 | Loss: 0.00124233
Iteration 59/1000 | Loss: 0.00076169
Iteration 60/1000 | Loss: 0.00119714
Iteration 61/1000 | Loss: 0.00100349
Iteration 62/1000 | Loss: 0.00084335
Iteration 63/1000 | Loss: 0.00104387
Iteration 64/1000 | Loss: 0.00114749
Iteration 65/1000 | Loss: 0.00093553
Iteration 66/1000 | Loss: 0.00172581
Iteration 67/1000 | Loss: 0.00094307
Iteration 68/1000 | Loss: 0.00070685
Iteration 69/1000 | Loss: 0.00164916
Iteration 70/1000 | Loss: 0.00166750
Iteration 71/1000 | Loss: 0.00080422
Iteration 72/1000 | Loss: 0.00077827
Iteration 73/1000 | Loss: 0.00097409
Iteration 74/1000 | Loss: 0.00135848
Iteration 75/1000 | Loss: 0.00043026
Iteration 76/1000 | Loss: 0.00055680
Iteration 77/1000 | Loss: 0.00069056
Iteration 78/1000 | Loss: 0.00038365
Iteration 79/1000 | Loss: 0.00031516
Iteration 80/1000 | Loss: 0.00193072
Iteration 81/1000 | Loss: 0.00090968
Iteration 82/1000 | Loss: 0.00051468
Iteration 83/1000 | Loss: 0.00086951
Iteration 84/1000 | Loss: 0.00047586
Iteration 85/1000 | Loss: 0.00050328
Iteration 86/1000 | Loss: 0.00038189
Iteration 87/1000 | Loss: 0.00034811
Iteration 88/1000 | Loss: 0.00041207
Iteration 89/1000 | Loss: 0.00042387
Iteration 90/1000 | Loss: 0.00061690
Iteration 91/1000 | Loss: 0.00052361
Iteration 92/1000 | Loss: 0.00040631
Iteration 93/1000 | Loss: 0.00041285
Iteration 94/1000 | Loss: 0.00017269
Iteration 95/1000 | Loss: 0.00012100
Iteration 96/1000 | Loss: 0.00021183
Iteration 97/1000 | Loss: 0.00052000
Iteration 98/1000 | Loss: 0.00044806
Iteration 99/1000 | Loss: 0.00028749
Iteration 100/1000 | Loss: 0.00033918
Iteration 101/1000 | Loss: 0.00033354
Iteration 102/1000 | Loss: 0.00032728
Iteration 103/1000 | Loss: 0.00040710
Iteration 104/1000 | Loss: 0.00111517
Iteration 105/1000 | Loss: 0.00061290
Iteration 106/1000 | Loss: 0.00041565
Iteration 107/1000 | Loss: 0.00101263
Iteration 108/1000 | Loss: 0.00056817
Iteration 109/1000 | Loss: 0.00012459
Iteration 110/1000 | Loss: 0.00013283
Iteration 111/1000 | Loss: 0.00011127
Iteration 112/1000 | Loss: 0.00018828
Iteration 113/1000 | Loss: 0.00014789
Iteration 114/1000 | Loss: 0.00039573
Iteration 115/1000 | Loss: 0.00038730
Iteration 116/1000 | Loss: 0.00070907
Iteration 117/1000 | Loss: 0.00055663
Iteration 118/1000 | Loss: 0.00035459
Iteration 119/1000 | Loss: 0.00048685
Iteration 120/1000 | Loss: 0.00044484
Iteration 121/1000 | Loss: 0.00041419
Iteration 122/1000 | Loss: 0.00063150
Iteration 123/1000 | Loss: 0.00072429
Iteration 124/1000 | Loss: 0.00035839
Iteration 125/1000 | Loss: 0.00047535
Iteration 126/1000 | Loss: 0.00030293
Iteration 127/1000 | Loss: 0.00021590
Iteration 128/1000 | Loss: 0.00013074
Iteration 129/1000 | Loss: 0.00023813
Iteration 130/1000 | Loss: 0.00032842
Iteration 131/1000 | Loss: 0.00032705
Iteration 132/1000 | Loss: 0.00038328
Iteration 133/1000 | Loss: 0.00048617
Iteration 134/1000 | Loss: 0.00030517
Iteration 135/1000 | Loss: 0.00024984
Iteration 136/1000 | Loss: 0.00066389
Iteration 137/1000 | Loss: 0.00082071
Iteration 138/1000 | Loss: 0.00223910
Iteration 139/1000 | Loss: 0.00062177
Iteration 140/1000 | Loss: 0.00181301
Iteration 141/1000 | Loss: 0.00095063
Iteration 142/1000 | Loss: 0.00055075
Iteration 143/1000 | Loss: 0.00041534
Iteration 144/1000 | Loss: 0.00054154
Iteration 145/1000 | Loss: 0.00018485
Iteration 146/1000 | Loss: 0.00021967
Iteration 147/1000 | Loss: 0.00022035
Iteration 148/1000 | Loss: 0.00051415
Iteration 149/1000 | Loss: 0.00038327
Iteration 150/1000 | Loss: 0.00029342
Iteration 151/1000 | Loss: 0.00021034
Iteration 152/1000 | Loss: 0.00028155
Iteration 153/1000 | Loss: 0.00039011
Iteration 154/1000 | Loss: 0.00091846
Iteration 155/1000 | Loss: 0.00011179
Iteration 156/1000 | Loss: 0.00096956
Iteration 157/1000 | Loss: 0.00232787
Iteration 158/1000 | Loss: 0.00154556
Iteration 159/1000 | Loss: 0.00019866
Iteration 160/1000 | Loss: 0.00079861
Iteration 161/1000 | Loss: 0.00087662
Iteration 162/1000 | Loss: 0.00104075
Iteration 163/1000 | Loss: 0.00101412
Iteration 164/1000 | Loss: 0.00047223
Iteration 165/1000 | Loss: 0.00011055
Iteration 166/1000 | Loss: 0.00092453
Iteration 167/1000 | Loss: 0.00039586
Iteration 168/1000 | Loss: 0.00089562
Iteration 169/1000 | Loss: 0.00035845
Iteration 170/1000 | Loss: 0.00009374
Iteration 171/1000 | Loss: 0.00008736
Iteration 172/1000 | Loss: 0.00190901
Iteration 173/1000 | Loss: 0.00033433
Iteration 174/1000 | Loss: 0.00021732
Iteration 175/1000 | Loss: 0.00008954
Iteration 176/1000 | Loss: 0.00008498
Iteration 177/1000 | Loss: 0.00008119
Iteration 178/1000 | Loss: 0.00096546
Iteration 179/1000 | Loss: 0.00046449
Iteration 180/1000 | Loss: 0.00022507
Iteration 181/1000 | Loss: 0.00008653
Iteration 182/1000 | Loss: 0.00029082
Iteration 183/1000 | Loss: 0.00008940
Iteration 184/1000 | Loss: 0.00008357
Iteration 185/1000 | Loss: 0.00096538
Iteration 186/1000 | Loss: 0.00011195
Iteration 187/1000 | Loss: 0.00008363
Iteration 188/1000 | Loss: 0.00007863
Iteration 189/1000 | Loss: 0.00028556
Iteration 190/1000 | Loss: 0.00060923
Iteration 191/1000 | Loss: 0.00023435
Iteration 192/1000 | Loss: 0.00008930
Iteration 193/1000 | Loss: 0.00007925
Iteration 194/1000 | Loss: 0.00095568
Iteration 195/1000 | Loss: 0.00007789
Iteration 196/1000 | Loss: 0.00007182
Iteration 197/1000 | Loss: 0.00006888
Iteration 198/1000 | Loss: 0.00006608
Iteration 199/1000 | Loss: 0.00093467
Iteration 200/1000 | Loss: 0.00006925
Iteration 201/1000 | Loss: 0.00006469
Iteration 202/1000 | Loss: 0.00006289
Iteration 203/1000 | Loss: 0.00006030
Iteration 204/1000 | Loss: 0.00005911
Iteration 205/1000 | Loss: 0.00005854
Iteration 206/1000 | Loss: 0.00091301
Iteration 207/1000 | Loss: 0.00079874
Iteration 208/1000 | Loss: 0.00010029
Iteration 209/1000 | Loss: 0.00007044
Iteration 210/1000 | Loss: 0.00006195
Iteration 211/1000 | Loss: 0.00008622
Iteration 212/1000 | Loss: 0.00006182
Iteration 213/1000 | Loss: 0.00007215
Iteration 214/1000 | Loss: 0.00005526
Iteration 215/1000 | Loss: 0.00088899
Iteration 216/1000 | Loss: 0.00006817
Iteration 217/1000 | Loss: 0.00005786
Iteration 218/1000 | Loss: 0.00005535
Iteration 219/1000 | Loss: 0.00005198
Iteration 220/1000 | Loss: 0.00005036
Iteration 221/1000 | Loss: 0.00004927
Iteration 222/1000 | Loss: 0.00086403
Iteration 223/1000 | Loss: 0.00005383
Iteration 224/1000 | Loss: 0.00004994
Iteration 225/1000 | Loss: 0.00004843
Iteration 226/1000 | Loss: 0.00004660
Iteration 227/1000 | Loss: 0.00237576
Iteration 228/1000 | Loss: 0.00005722
Iteration 229/1000 | Loss: 0.00004899
Iteration 230/1000 | Loss: 0.00004384
Iteration 231/1000 | Loss: 0.00003972
Iteration 232/1000 | Loss: 0.00003782
Iteration 233/1000 | Loss: 0.00003688
Iteration 234/1000 | Loss: 0.00003650
Iteration 235/1000 | Loss: 0.00003609
Iteration 236/1000 | Loss: 0.00003579
Iteration 237/1000 | Loss: 0.00003556
Iteration 238/1000 | Loss: 0.00003552
Iteration 239/1000 | Loss: 0.00003549
Iteration 240/1000 | Loss: 0.00003549
Iteration 241/1000 | Loss: 0.00003548
Iteration 242/1000 | Loss: 0.00003548
Iteration 243/1000 | Loss: 0.00003548
Iteration 244/1000 | Loss: 0.00003548
Iteration 245/1000 | Loss: 0.00003548
Iteration 246/1000 | Loss: 0.00003548
Iteration 247/1000 | Loss: 0.00003548
Iteration 248/1000 | Loss: 0.00003548
Iteration 249/1000 | Loss: 0.00003548
Iteration 250/1000 | Loss: 0.00003548
Iteration 251/1000 | Loss: 0.00003548
Iteration 252/1000 | Loss: 0.00003547
Iteration 253/1000 | Loss: 0.00003547
Iteration 254/1000 | Loss: 0.00003547
Iteration 255/1000 | Loss: 0.00003547
Iteration 256/1000 | Loss: 0.00003547
Iteration 257/1000 | Loss: 0.00003547
Iteration 258/1000 | Loss: 0.00003546
Iteration 259/1000 | Loss: 0.00003546
Iteration 260/1000 | Loss: 0.00003546
Iteration 261/1000 | Loss: 0.00003546
Iteration 262/1000 | Loss: 0.00003545
Iteration 263/1000 | Loss: 0.00003545
Iteration 264/1000 | Loss: 0.00003545
Iteration 265/1000 | Loss: 0.00003545
Iteration 266/1000 | Loss: 0.00003545
Iteration 267/1000 | Loss: 0.00003545
Iteration 268/1000 | Loss: 0.00003545
Iteration 269/1000 | Loss: 0.00003545
Iteration 270/1000 | Loss: 0.00003545
Iteration 271/1000 | Loss: 0.00003544
Iteration 272/1000 | Loss: 0.00003544
Iteration 273/1000 | Loss: 0.00003544
Iteration 274/1000 | Loss: 0.00003544
Iteration 275/1000 | Loss: 0.00003542
Iteration 276/1000 | Loss: 0.00003542
Iteration 277/1000 | Loss: 0.00003541
Iteration 278/1000 | Loss: 0.00003541
Iteration 279/1000 | Loss: 0.00003540
Iteration 280/1000 | Loss: 0.00003540
Iteration 281/1000 | Loss: 0.00003537
Iteration 282/1000 | Loss: 0.00003537
Iteration 283/1000 | Loss: 0.00003536
Iteration 284/1000 | Loss: 0.00003536
Iteration 285/1000 | Loss: 0.00003535
Iteration 286/1000 | Loss: 0.00003535
Iteration 287/1000 | Loss: 0.00003534
Iteration 288/1000 | Loss: 0.00003534
Iteration 289/1000 | Loss: 0.00003534
Iteration 290/1000 | Loss: 0.00003533
Iteration 291/1000 | Loss: 0.00003533
Iteration 292/1000 | Loss: 0.00003533
Iteration 293/1000 | Loss: 0.00003532
Iteration 294/1000 | Loss: 0.00003532
Iteration 295/1000 | Loss: 0.00003529
Iteration 296/1000 | Loss: 0.00003526
Iteration 297/1000 | Loss: 0.00003525
Iteration 298/1000 | Loss: 0.00003525
Iteration 299/1000 | Loss: 0.00003525
Iteration 300/1000 | Loss: 0.00003525
Iteration 301/1000 | Loss: 0.00003525
Iteration 302/1000 | Loss: 0.00003525
Iteration 303/1000 | Loss: 0.00003524
Iteration 304/1000 | Loss: 0.00003524
Iteration 305/1000 | Loss: 0.00003524
Iteration 306/1000 | Loss: 0.00003523
Iteration 307/1000 | Loss: 0.00003523
Iteration 308/1000 | Loss: 0.00003523
Iteration 309/1000 | Loss: 0.00003522
Iteration 310/1000 | Loss: 0.00003522
Iteration 311/1000 | Loss: 0.00003522
Iteration 312/1000 | Loss: 0.00003522
Iteration 313/1000 | Loss: 0.00003522
Iteration 314/1000 | Loss: 0.00003522
Iteration 315/1000 | Loss: 0.00003522
Iteration 316/1000 | Loss: 0.00003522
Iteration 317/1000 | Loss: 0.00003521
Iteration 318/1000 | Loss: 0.00003521
Iteration 319/1000 | Loss: 0.00003521
Iteration 320/1000 | Loss: 0.00003521
Iteration 321/1000 | Loss: 0.00003521
Iteration 322/1000 | Loss: 0.00003521
Iteration 323/1000 | Loss: 0.00003520
Iteration 324/1000 | Loss: 0.00003520
Iteration 325/1000 | Loss: 0.00003520
Iteration 326/1000 | Loss: 0.00003520
Iteration 327/1000 | Loss: 0.00003519
Iteration 328/1000 | Loss: 0.00003519
Iteration 329/1000 | Loss: 0.00003519
Iteration 330/1000 | Loss: 0.00003519
Iteration 331/1000 | Loss: 0.00003518
Iteration 332/1000 | Loss: 0.00003518
Iteration 333/1000 | Loss: 0.00003517
Iteration 334/1000 | Loss: 0.00003517
Iteration 335/1000 | Loss: 0.00003516
Iteration 336/1000 | Loss: 0.00003516
Iteration 337/1000 | Loss: 0.00003516
Iteration 338/1000 | Loss: 0.00003515
Iteration 339/1000 | Loss: 0.00003515
Iteration 340/1000 | Loss: 0.00003515
Iteration 341/1000 | Loss: 0.00003514
Iteration 342/1000 | Loss: 0.00003514
Iteration 343/1000 | Loss: 0.00003512
Iteration 344/1000 | Loss: 0.00003512
Iteration 345/1000 | Loss: 0.00003512
Iteration 346/1000 | Loss: 0.00003511
Iteration 347/1000 | Loss: 0.00003511
Iteration 348/1000 | Loss: 0.00003511
Iteration 349/1000 | Loss: 0.00003511
Iteration 350/1000 | Loss: 0.00003511
Iteration 351/1000 | Loss: 0.00003510
Iteration 352/1000 | Loss: 0.00003510
Iteration 353/1000 | Loss: 0.00003510
Iteration 354/1000 | Loss: 0.00003510
Iteration 355/1000 | Loss: 0.00003510
Iteration 356/1000 | Loss: 0.00003510
Iteration 357/1000 | Loss: 0.00003510
Iteration 358/1000 | Loss: 0.00003510
Iteration 359/1000 | Loss: 0.00003509
Iteration 360/1000 | Loss: 0.00003509
Iteration 361/1000 | Loss: 0.00003509
Iteration 362/1000 | Loss: 0.00003508
Iteration 363/1000 | Loss: 0.00003508
Iteration 364/1000 | Loss: 0.00003508
Iteration 365/1000 | Loss: 0.00003508
Iteration 366/1000 | Loss: 0.00003508
Iteration 367/1000 | Loss: 0.00003507
Iteration 368/1000 | Loss: 0.00003507
Iteration 369/1000 | Loss: 0.00003507
Iteration 370/1000 | Loss: 0.00003506
Iteration 371/1000 | Loss: 0.00003506
Iteration 372/1000 | Loss: 0.00003506
Iteration 373/1000 | Loss: 0.00003506
Iteration 374/1000 | Loss: 0.00003505
Iteration 375/1000 | Loss: 0.00003505
Iteration 376/1000 | Loss: 0.00003505
Iteration 377/1000 | Loss: 0.00003505
Iteration 378/1000 | Loss: 0.00003505
Iteration 379/1000 | Loss: 0.00003505
Iteration 380/1000 | Loss: 0.00003505
Iteration 381/1000 | Loss: 0.00003505
Iteration 382/1000 | Loss: 0.00003505
Iteration 383/1000 | Loss: 0.00003505
Iteration 384/1000 | Loss: 0.00003504
Iteration 385/1000 | Loss: 0.00003504
Iteration 386/1000 | Loss: 0.00003504
Iteration 387/1000 | Loss: 0.00003504
Iteration 388/1000 | Loss: 0.00003504
Iteration 389/1000 | Loss: 0.00003504
Iteration 390/1000 | Loss: 0.00003504
Iteration 391/1000 | Loss: 0.00003503
Iteration 392/1000 | Loss: 0.00003503
Iteration 393/1000 | Loss: 0.00003503
Iteration 394/1000 | Loss: 0.00003503
Iteration 395/1000 | Loss: 0.00003502
Iteration 396/1000 | Loss: 0.00003502
Iteration 397/1000 | Loss: 0.00003502
Iteration 398/1000 | Loss: 0.00003502
Iteration 399/1000 | Loss: 0.00003502
Iteration 400/1000 | Loss: 0.00003502
Iteration 401/1000 | Loss: 0.00003502
Iteration 402/1000 | Loss: 0.00003502
Iteration 403/1000 | Loss: 0.00003502
Iteration 404/1000 | Loss: 0.00003501
Iteration 405/1000 | Loss: 0.00003501
Iteration 406/1000 | Loss: 0.00003501
Iteration 407/1000 | Loss: 0.00003501
Iteration 408/1000 | Loss: 0.00003501
Iteration 409/1000 | Loss: 0.00003501
Iteration 410/1000 | Loss: 0.00003501
Iteration 411/1000 | Loss: 0.00003501
Iteration 412/1000 | Loss: 0.00003501
Iteration 413/1000 | Loss: 0.00003501
Iteration 414/1000 | Loss: 0.00003501
Iteration 415/1000 | Loss: 0.00003501
Iteration 416/1000 | Loss: 0.00003501
Iteration 417/1000 | Loss: 0.00003500
Iteration 418/1000 | Loss: 0.00003500
Iteration 419/1000 | Loss: 0.00003500
Iteration 420/1000 | Loss: 0.00003500
Iteration 421/1000 | Loss: 0.00003500
Iteration 422/1000 | Loss: 0.00003500
Iteration 423/1000 | Loss: 0.00003500
Iteration 424/1000 | Loss: 0.00003500
Iteration 425/1000 | Loss: 0.00003500
Iteration 426/1000 | Loss: 0.00003500
Iteration 427/1000 | Loss: 0.00003500
Iteration 428/1000 | Loss: 0.00003500
Iteration 429/1000 | Loss: 0.00003500
Iteration 430/1000 | Loss: 0.00003500
Iteration 431/1000 | Loss: 0.00003500
Iteration 432/1000 | Loss: 0.00003500
Iteration 433/1000 | Loss: 0.00003500
Iteration 434/1000 | Loss: 0.00003500
Iteration 435/1000 | Loss: 0.00003500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 435. Stopping optimization.
Last 5 losses: [3.500122329569422e-05, 3.500122329569422e-05, 3.500122329569422e-05, 3.500122329569422e-05, 3.500122329569422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.500122329569422e-05

Optimization complete. Final v2v error: 4.787172794342041 mm

Highest mean error: 5.522902488708496 mm for frame 152

Lowest mean error: 4.047194004058838 mm for frame 63

Saving results

Total time: 402.5372579097748
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876837
Iteration 2/25 | Loss: 0.00139885
Iteration 3/25 | Loss: 0.00090527
Iteration 4/25 | Loss: 0.00078813
Iteration 5/25 | Loss: 0.00077049
Iteration 6/25 | Loss: 0.00076754
Iteration 7/25 | Loss: 0.00076679
Iteration 8/25 | Loss: 0.00076663
Iteration 9/25 | Loss: 0.00076663
Iteration 10/25 | Loss: 0.00076663
Iteration 11/25 | Loss: 0.00076663
Iteration 12/25 | Loss: 0.00076663
Iteration 13/25 | Loss: 0.00076663
Iteration 14/25 | Loss: 0.00076663
Iteration 15/25 | Loss: 0.00076663
Iteration 16/25 | Loss: 0.00076663
Iteration 17/25 | Loss: 0.00076663
Iteration 18/25 | Loss: 0.00076663
Iteration 19/25 | Loss: 0.00076663
Iteration 20/25 | Loss: 0.00076663
Iteration 21/25 | Loss: 0.00076663
Iteration 22/25 | Loss: 0.00076663
Iteration 23/25 | Loss: 0.00076663
Iteration 24/25 | Loss: 0.00076663
Iteration 25/25 | Loss: 0.00076663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34381020
Iteration 2/25 | Loss: 0.00029083
Iteration 3/25 | Loss: 0.00029081
Iteration 4/25 | Loss: 0.00029081
Iteration 5/25 | Loss: 0.00029081
Iteration 6/25 | Loss: 0.00029081
Iteration 7/25 | Loss: 0.00029081
Iteration 8/25 | Loss: 0.00029081
Iteration 9/25 | Loss: 0.00029081
Iteration 10/25 | Loss: 0.00029081
Iteration 11/25 | Loss: 0.00029081
Iteration 12/25 | Loss: 0.00029081
Iteration 13/25 | Loss: 0.00029081
Iteration 14/25 | Loss: 0.00029081
Iteration 15/25 | Loss: 0.00029081
Iteration 16/25 | Loss: 0.00029081
Iteration 17/25 | Loss: 0.00029081
Iteration 18/25 | Loss: 0.00029081
Iteration 19/25 | Loss: 0.00029081
Iteration 20/25 | Loss: 0.00029081
Iteration 21/25 | Loss: 0.00029081
Iteration 22/25 | Loss: 0.00029081
Iteration 23/25 | Loss: 0.00029081
Iteration 24/25 | Loss: 0.00029081
Iteration 25/25 | Loss: 0.00029081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029081
Iteration 2/1000 | Loss: 0.00003675
Iteration 3/1000 | Loss: 0.00002416
Iteration 4/1000 | Loss: 0.00001983
Iteration 5/1000 | Loss: 0.00001846
Iteration 6/1000 | Loss: 0.00001729
Iteration 7/1000 | Loss: 0.00001675
Iteration 8/1000 | Loss: 0.00001650
Iteration 9/1000 | Loss: 0.00001628
Iteration 10/1000 | Loss: 0.00001626
Iteration 11/1000 | Loss: 0.00001617
Iteration 12/1000 | Loss: 0.00001614
Iteration 13/1000 | Loss: 0.00001609
Iteration 14/1000 | Loss: 0.00001609
Iteration 15/1000 | Loss: 0.00001608
Iteration 16/1000 | Loss: 0.00001607
Iteration 17/1000 | Loss: 0.00001607
Iteration 18/1000 | Loss: 0.00001604
Iteration 19/1000 | Loss: 0.00001604
Iteration 20/1000 | Loss: 0.00001598
Iteration 21/1000 | Loss: 0.00001598
Iteration 22/1000 | Loss: 0.00001598
Iteration 23/1000 | Loss: 0.00001597
Iteration 24/1000 | Loss: 0.00001597
Iteration 25/1000 | Loss: 0.00001597
Iteration 26/1000 | Loss: 0.00001597
Iteration 27/1000 | Loss: 0.00001597
Iteration 28/1000 | Loss: 0.00001597
Iteration 29/1000 | Loss: 0.00001597
Iteration 30/1000 | Loss: 0.00001597
Iteration 31/1000 | Loss: 0.00001596
Iteration 32/1000 | Loss: 0.00001596
Iteration 33/1000 | Loss: 0.00001595
Iteration 34/1000 | Loss: 0.00001595
Iteration 35/1000 | Loss: 0.00001595
Iteration 36/1000 | Loss: 0.00001595
Iteration 37/1000 | Loss: 0.00001595
Iteration 38/1000 | Loss: 0.00001595
Iteration 39/1000 | Loss: 0.00001595
Iteration 40/1000 | Loss: 0.00001594
Iteration 41/1000 | Loss: 0.00001594
Iteration 42/1000 | Loss: 0.00001593
Iteration 43/1000 | Loss: 0.00001593
Iteration 44/1000 | Loss: 0.00001593
Iteration 45/1000 | Loss: 0.00001593
Iteration 46/1000 | Loss: 0.00001593
Iteration 47/1000 | Loss: 0.00001593
Iteration 48/1000 | Loss: 0.00001593
Iteration 49/1000 | Loss: 0.00001592
Iteration 50/1000 | Loss: 0.00001592
Iteration 51/1000 | Loss: 0.00001592
Iteration 52/1000 | Loss: 0.00001592
Iteration 53/1000 | Loss: 0.00001592
Iteration 54/1000 | Loss: 0.00001591
Iteration 55/1000 | Loss: 0.00001591
Iteration 56/1000 | Loss: 0.00001591
Iteration 57/1000 | Loss: 0.00001590
Iteration 58/1000 | Loss: 0.00001590
Iteration 59/1000 | Loss: 0.00001590
Iteration 60/1000 | Loss: 0.00001590
Iteration 61/1000 | Loss: 0.00001589
Iteration 62/1000 | Loss: 0.00001589
Iteration 63/1000 | Loss: 0.00001589
Iteration 64/1000 | Loss: 0.00001589
Iteration 65/1000 | Loss: 0.00001589
Iteration 66/1000 | Loss: 0.00001589
Iteration 67/1000 | Loss: 0.00001589
Iteration 68/1000 | Loss: 0.00001589
Iteration 69/1000 | Loss: 0.00001589
Iteration 70/1000 | Loss: 0.00001589
Iteration 71/1000 | Loss: 0.00001589
Iteration 72/1000 | Loss: 0.00001589
Iteration 73/1000 | Loss: 0.00001588
Iteration 74/1000 | Loss: 0.00001588
Iteration 75/1000 | Loss: 0.00001588
Iteration 76/1000 | Loss: 0.00001588
Iteration 77/1000 | Loss: 0.00001588
Iteration 78/1000 | Loss: 0.00001588
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001588
Iteration 81/1000 | Loss: 0.00001588
Iteration 82/1000 | Loss: 0.00001588
Iteration 83/1000 | Loss: 0.00001588
Iteration 84/1000 | Loss: 0.00001588
Iteration 85/1000 | Loss: 0.00001588
Iteration 86/1000 | Loss: 0.00001588
Iteration 87/1000 | Loss: 0.00001588
Iteration 88/1000 | Loss: 0.00001588
Iteration 89/1000 | Loss: 0.00001588
Iteration 90/1000 | Loss: 0.00001588
Iteration 91/1000 | Loss: 0.00001588
Iteration 92/1000 | Loss: 0.00001587
Iteration 93/1000 | Loss: 0.00001587
Iteration 94/1000 | Loss: 0.00001587
Iteration 95/1000 | Loss: 0.00001587
Iteration 96/1000 | Loss: 0.00001587
Iteration 97/1000 | Loss: 0.00001587
Iteration 98/1000 | Loss: 0.00001587
Iteration 99/1000 | Loss: 0.00001587
Iteration 100/1000 | Loss: 0.00001587
Iteration 101/1000 | Loss: 0.00001586
Iteration 102/1000 | Loss: 0.00001586
Iteration 103/1000 | Loss: 0.00001586
Iteration 104/1000 | Loss: 0.00001586
Iteration 105/1000 | Loss: 0.00001586
Iteration 106/1000 | Loss: 0.00001586
Iteration 107/1000 | Loss: 0.00001586
Iteration 108/1000 | Loss: 0.00001586
Iteration 109/1000 | Loss: 0.00001586
Iteration 110/1000 | Loss: 0.00001586
Iteration 111/1000 | Loss: 0.00001586
Iteration 112/1000 | Loss: 0.00001586
Iteration 113/1000 | Loss: 0.00001586
Iteration 114/1000 | Loss: 0.00001586
Iteration 115/1000 | Loss: 0.00001586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.5855208403081633e-05, 1.5855208403081633e-05, 1.5855208403081633e-05, 1.5855208403081633e-05, 1.5855208403081633e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5855208403081633e-05

Optimization complete. Final v2v error: 3.2937703132629395 mm

Highest mean error: 3.5694868564605713 mm for frame 116

Lowest mean error: 2.6709792613983154 mm for frame 36

Saving results

Total time: 33.77931189537048
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393893
Iteration 2/25 | Loss: 0.00090836
Iteration 3/25 | Loss: 0.00072642
Iteration 4/25 | Loss: 0.00070115
Iteration 5/25 | Loss: 0.00069313
Iteration 6/25 | Loss: 0.00069045
Iteration 7/25 | Loss: 0.00068977
Iteration 8/25 | Loss: 0.00068977
Iteration 9/25 | Loss: 0.00068977
Iteration 10/25 | Loss: 0.00068977
Iteration 11/25 | Loss: 0.00068977
Iteration 12/25 | Loss: 0.00068977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006897723069414496, 0.0006897723069414496, 0.0006897723069414496, 0.0006897723069414496, 0.0006897723069414496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006897723069414496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44600761
Iteration 2/25 | Loss: 0.00018850
Iteration 3/25 | Loss: 0.00018850
Iteration 4/25 | Loss: 0.00018850
Iteration 5/25 | Loss: 0.00018850
Iteration 6/25 | Loss: 0.00018850
Iteration 7/25 | Loss: 0.00018850
Iteration 8/25 | Loss: 0.00018850
Iteration 9/25 | Loss: 0.00018850
Iteration 10/25 | Loss: 0.00018850
Iteration 11/25 | Loss: 0.00018849
Iteration 12/25 | Loss: 0.00018849
Iteration 13/25 | Loss: 0.00018849
Iteration 14/25 | Loss: 0.00018849
Iteration 15/25 | Loss: 0.00018849
Iteration 16/25 | Loss: 0.00018849
Iteration 17/25 | Loss: 0.00018849
Iteration 18/25 | Loss: 0.00018849
Iteration 19/25 | Loss: 0.00018849
Iteration 20/25 | Loss: 0.00018849
Iteration 21/25 | Loss: 0.00018849
Iteration 22/25 | Loss: 0.00018849
Iteration 23/25 | Loss: 0.00018849
Iteration 24/25 | Loss: 0.00018849
Iteration 25/25 | Loss: 0.00018849

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018849
Iteration 2/1000 | Loss: 0.00002310
Iteration 3/1000 | Loss: 0.00001908
Iteration 4/1000 | Loss: 0.00001741
Iteration 5/1000 | Loss: 0.00001614
Iteration 6/1000 | Loss: 0.00001557
Iteration 7/1000 | Loss: 0.00001521
Iteration 8/1000 | Loss: 0.00001507
Iteration 9/1000 | Loss: 0.00001492
Iteration 10/1000 | Loss: 0.00001476
Iteration 11/1000 | Loss: 0.00001461
Iteration 12/1000 | Loss: 0.00001461
Iteration 13/1000 | Loss: 0.00001461
Iteration 14/1000 | Loss: 0.00001461
Iteration 15/1000 | Loss: 0.00001458
Iteration 16/1000 | Loss: 0.00001454
Iteration 17/1000 | Loss: 0.00001454
Iteration 18/1000 | Loss: 0.00001450
Iteration 19/1000 | Loss: 0.00001450
Iteration 20/1000 | Loss: 0.00001448
Iteration 21/1000 | Loss: 0.00001445
Iteration 22/1000 | Loss: 0.00001444
Iteration 23/1000 | Loss: 0.00001444
Iteration 24/1000 | Loss: 0.00001443
Iteration 25/1000 | Loss: 0.00001443
Iteration 26/1000 | Loss: 0.00001443
Iteration 27/1000 | Loss: 0.00001442
Iteration 28/1000 | Loss: 0.00001442
Iteration 29/1000 | Loss: 0.00001442
Iteration 30/1000 | Loss: 0.00001442
Iteration 31/1000 | Loss: 0.00001441
Iteration 32/1000 | Loss: 0.00001441
Iteration 33/1000 | Loss: 0.00001441
Iteration 34/1000 | Loss: 0.00001440
Iteration 35/1000 | Loss: 0.00001440
Iteration 36/1000 | Loss: 0.00001440
Iteration 37/1000 | Loss: 0.00001439
Iteration 38/1000 | Loss: 0.00001439
Iteration 39/1000 | Loss: 0.00001438
Iteration 40/1000 | Loss: 0.00001438
Iteration 41/1000 | Loss: 0.00001437
Iteration 42/1000 | Loss: 0.00001437
Iteration 43/1000 | Loss: 0.00001437
Iteration 44/1000 | Loss: 0.00001436
Iteration 45/1000 | Loss: 0.00001436
Iteration 46/1000 | Loss: 0.00001435
Iteration 47/1000 | Loss: 0.00001435
Iteration 48/1000 | Loss: 0.00001434
Iteration 49/1000 | Loss: 0.00001433
Iteration 50/1000 | Loss: 0.00001432
Iteration 51/1000 | Loss: 0.00001432
Iteration 52/1000 | Loss: 0.00001430
Iteration 53/1000 | Loss: 0.00001430
Iteration 54/1000 | Loss: 0.00001430
Iteration 55/1000 | Loss: 0.00001429
Iteration 56/1000 | Loss: 0.00001428
Iteration 57/1000 | Loss: 0.00001427
Iteration 58/1000 | Loss: 0.00001426
Iteration 59/1000 | Loss: 0.00001425
Iteration 60/1000 | Loss: 0.00001425
Iteration 61/1000 | Loss: 0.00001424
Iteration 62/1000 | Loss: 0.00001424
Iteration 63/1000 | Loss: 0.00001424
Iteration 64/1000 | Loss: 0.00001423
Iteration 65/1000 | Loss: 0.00001423
Iteration 66/1000 | Loss: 0.00001423
Iteration 67/1000 | Loss: 0.00001423
Iteration 68/1000 | Loss: 0.00001423
Iteration 69/1000 | Loss: 0.00001423
Iteration 70/1000 | Loss: 0.00001422
Iteration 71/1000 | Loss: 0.00001422
Iteration 72/1000 | Loss: 0.00001422
Iteration 73/1000 | Loss: 0.00001421
Iteration 74/1000 | Loss: 0.00001421
Iteration 75/1000 | Loss: 0.00001420
Iteration 76/1000 | Loss: 0.00001420
Iteration 77/1000 | Loss: 0.00001420
Iteration 78/1000 | Loss: 0.00001420
Iteration 79/1000 | Loss: 0.00001420
Iteration 80/1000 | Loss: 0.00001419
Iteration 81/1000 | Loss: 0.00001419
Iteration 82/1000 | Loss: 0.00001419
Iteration 83/1000 | Loss: 0.00001419
Iteration 84/1000 | Loss: 0.00001419
Iteration 85/1000 | Loss: 0.00001419
Iteration 86/1000 | Loss: 0.00001419
Iteration 87/1000 | Loss: 0.00001419
Iteration 88/1000 | Loss: 0.00001419
Iteration 89/1000 | Loss: 0.00001418
Iteration 90/1000 | Loss: 0.00001418
Iteration 91/1000 | Loss: 0.00001417
Iteration 92/1000 | Loss: 0.00001417
Iteration 93/1000 | Loss: 0.00001417
Iteration 94/1000 | Loss: 0.00001417
Iteration 95/1000 | Loss: 0.00001416
Iteration 96/1000 | Loss: 0.00001416
Iteration 97/1000 | Loss: 0.00001416
Iteration 98/1000 | Loss: 0.00001416
Iteration 99/1000 | Loss: 0.00001416
Iteration 100/1000 | Loss: 0.00001416
Iteration 101/1000 | Loss: 0.00001416
Iteration 102/1000 | Loss: 0.00001416
Iteration 103/1000 | Loss: 0.00001415
Iteration 104/1000 | Loss: 0.00001415
Iteration 105/1000 | Loss: 0.00001415
Iteration 106/1000 | Loss: 0.00001415
Iteration 107/1000 | Loss: 0.00001415
Iteration 108/1000 | Loss: 0.00001415
Iteration 109/1000 | Loss: 0.00001415
Iteration 110/1000 | Loss: 0.00001415
Iteration 111/1000 | Loss: 0.00001415
Iteration 112/1000 | Loss: 0.00001415
Iteration 113/1000 | Loss: 0.00001415
Iteration 114/1000 | Loss: 0.00001415
Iteration 115/1000 | Loss: 0.00001415
Iteration 116/1000 | Loss: 0.00001415
Iteration 117/1000 | Loss: 0.00001415
Iteration 118/1000 | Loss: 0.00001415
Iteration 119/1000 | Loss: 0.00001414
Iteration 120/1000 | Loss: 0.00001414
Iteration 121/1000 | Loss: 0.00001414
Iteration 122/1000 | Loss: 0.00001414
Iteration 123/1000 | Loss: 0.00001414
Iteration 124/1000 | Loss: 0.00001414
Iteration 125/1000 | Loss: 0.00001414
Iteration 126/1000 | Loss: 0.00001413
Iteration 127/1000 | Loss: 0.00001413
Iteration 128/1000 | Loss: 0.00001413
Iteration 129/1000 | Loss: 0.00001413
Iteration 130/1000 | Loss: 0.00001413
Iteration 131/1000 | Loss: 0.00001413
Iteration 132/1000 | Loss: 0.00001413
Iteration 133/1000 | Loss: 0.00001413
Iteration 134/1000 | Loss: 0.00001413
Iteration 135/1000 | Loss: 0.00001413
Iteration 136/1000 | Loss: 0.00001413
Iteration 137/1000 | Loss: 0.00001413
Iteration 138/1000 | Loss: 0.00001413
Iteration 139/1000 | Loss: 0.00001413
Iteration 140/1000 | Loss: 0.00001413
Iteration 141/1000 | Loss: 0.00001413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.4130580893834122e-05, 1.4130580893834122e-05, 1.4130580893834122e-05, 1.4130580893834122e-05, 1.4130580893834122e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4130580893834122e-05

Optimization complete. Final v2v error: 3.172645330429077 mm

Highest mean error: 3.480623245239258 mm for frame 110

Lowest mean error: 2.9025251865386963 mm for frame 1

Saving results

Total time: 37.34786510467529
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439980
Iteration 2/25 | Loss: 0.00087354
Iteration 3/25 | Loss: 0.00075358
Iteration 4/25 | Loss: 0.00073647
Iteration 5/25 | Loss: 0.00073377
Iteration 6/25 | Loss: 0.00073340
Iteration 7/25 | Loss: 0.00073340
Iteration 8/25 | Loss: 0.00073340
Iteration 9/25 | Loss: 0.00073340
Iteration 10/25 | Loss: 0.00073340
Iteration 11/25 | Loss: 0.00073340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000733397901058197, 0.000733397901058197, 0.000733397901058197, 0.000733397901058197, 0.000733397901058197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000733397901058197

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05039585
Iteration 2/25 | Loss: 0.00024523
Iteration 3/25 | Loss: 0.00024523
Iteration 4/25 | Loss: 0.00024523
Iteration 5/25 | Loss: 0.00024523
Iteration 6/25 | Loss: 0.00024523
Iteration 7/25 | Loss: 0.00024523
Iteration 8/25 | Loss: 0.00024523
Iteration 9/25 | Loss: 0.00024523
Iteration 10/25 | Loss: 0.00024523
Iteration 11/25 | Loss: 0.00024523
Iteration 12/25 | Loss: 0.00024523
Iteration 13/25 | Loss: 0.00024523
Iteration 14/25 | Loss: 0.00024523
Iteration 15/25 | Loss: 0.00024523
Iteration 16/25 | Loss: 0.00024523
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0002452283806633204, 0.0002452283806633204, 0.0002452283806633204, 0.0002452283806633204, 0.0002452283806633204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002452283806633204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024523
Iteration 2/1000 | Loss: 0.00003164
Iteration 3/1000 | Loss: 0.00002347
Iteration 4/1000 | Loss: 0.00002067
Iteration 5/1000 | Loss: 0.00001980
Iteration 6/1000 | Loss: 0.00001917
Iteration 7/1000 | Loss: 0.00001846
Iteration 8/1000 | Loss: 0.00001822
Iteration 9/1000 | Loss: 0.00001796
Iteration 10/1000 | Loss: 0.00001791
Iteration 11/1000 | Loss: 0.00001784
Iteration 12/1000 | Loss: 0.00001783
Iteration 13/1000 | Loss: 0.00001769
Iteration 14/1000 | Loss: 0.00001769
Iteration 15/1000 | Loss: 0.00001768
Iteration 16/1000 | Loss: 0.00001763
Iteration 17/1000 | Loss: 0.00001762
Iteration 18/1000 | Loss: 0.00001758
Iteration 19/1000 | Loss: 0.00001756
Iteration 20/1000 | Loss: 0.00001749
Iteration 21/1000 | Loss: 0.00001749
Iteration 22/1000 | Loss: 0.00001749
Iteration 23/1000 | Loss: 0.00001749
Iteration 24/1000 | Loss: 0.00001749
Iteration 25/1000 | Loss: 0.00001749
Iteration 26/1000 | Loss: 0.00001747
Iteration 27/1000 | Loss: 0.00001747
Iteration 28/1000 | Loss: 0.00001747
Iteration 29/1000 | Loss: 0.00001746
Iteration 30/1000 | Loss: 0.00001746
Iteration 31/1000 | Loss: 0.00001746
Iteration 32/1000 | Loss: 0.00001746
Iteration 33/1000 | Loss: 0.00001746
Iteration 34/1000 | Loss: 0.00001746
Iteration 35/1000 | Loss: 0.00001746
Iteration 36/1000 | Loss: 0.00001745
Iteration 37/1000 | Loss: 0.00001745
Iteration 38/1000 | Loss: 0.00001745
Iteration 39/1000 | Loss: 0.00001745
Iteration 40/1000 | Loss: 0.00001742
Iteration 41/1000 | Loss: 0.00001742
Iteration 42/1000 | Loss: 0.00001742
Iteration 43/1000 | Loss: 0.00001741
Iteration 44/1000 | Loss: 0.00001741
Iteration 45/1000 | Loss: 0.00001741
Iteration 46/1000 | Loss: 0.00001741
Iteration 47/1000 | Loss: 0.00001741
Iteration 48/1000 | Loss: 0.00001740
Iteration 49/1000 | Loss: 0.00001740
Iteration 50/1000 | Loss: 0.00001740
Iteration 51/1000 | Loss: 0.00001740
Iteration 52/1000 | Loss: 0.00001740
Iteration 53/1000 | Loss: 0.00001739
Iteration 54/1000 | Loss: 0.00001739
Iteration 55/1000 | Loss: 0.00001739
Iteration 56/1000 | Loss: 0.00001739
Iteration 57/1000 | Loss: 0.00001739
Iteration 58/1000 | Loss: 0.00001739
Iteration 59/1000 | Loss: 0.00001738
Iteration 60/1000 | Loss: 0.00001738
Iteration 61/1000 | Loss: 0.00001738
Iteration 62/1000 | Loss: 0.00001738
Iteration 63/1000 | Loss: 0.00001738
Iteration 64/1000 | Loss: 0.00001738
Iteration 65/1000 | Loss: 0.00001738
Iteration 66/1000 | Loss: 0.00001738
Iteration 67/1000 | Loss: 0.00001738
Iteration 68/1000 | Loss: 0.00001738
Iteration 69/1000 | Loss: 0.00001738
Iteration 70/1000 | Loss: 0.00001738
Iteration 71/1000 | Loss: 0.00001738
Iteration 72/1000 | Loss: 0.00001738
Iteration 73/1000 | Loss: 0.00001738
Iteration 74/1000 | Loss: 0.00001738
Iteration 75/1000 | Loss: 0.00001738
Iteration 76/1000 | Loss: 0.00001738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.7379092241753824e-05, 1.7379092241753824e-05, 1.7379092241753824e-05, 1.7379092241753824e-05, 1.7379092241753824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7379092241753824e-05

Optimization complete. Final v2v error: 3.5158305168151855 mm

Highest mean error: 3.5396668910980225 mm for frame 55

Lowest mean error: 3.4744114875793457 mm for frame 115

Saving results

Total time: 27.184484243392944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780784
Iteration 2/25 | Loss: 0.00100025
Iteration 3/25 | Loss: 0.00085527
Iteration 4/25 | Loss: 0.00081932
Iteration 5/25 | Loss: 0.00081175
Iteration 6/25 | Loss: 0.00081025
Iteration 7/25 | Loss: 0.00080996
Iteration 8/25 | Loss: 0.00080996
Iteration 9/25 | Loss: 0.00080996
Iteration 10/25 | Loss: 0.00080996
Iteration 11/25 | Loss: 0.00080996
Iteration 12/25 | Loss: 0.00080992
Iteration 13/25 | Loss: 0.00080992
Iteration 14/25 | Loss: 0.00080992
Iteration 15/25 | Loss: 0.00080992
Iteration 16/25 | Loss: 0.00080992
Iteration 17/25 | Loss: 0.00080992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008099217666313052, 0.0008099217666313052, 0.0008099217666313052, 0.0008099217666313052, 0.0008099217666313052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008099217666313052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47271466
Iteration 2/25 | Loss: 0.00031565
Iteration 3/25 | Loss: 0.00031565
Iteration 4/25 | Loss: 0.00031565
Iteration 5/25 | Loss: 0.00031565
Iteration 6/25 | Loss: 0.00031565
Iteration 7/25 | Loss: 0.00031565
Iteration 8/25 | Loss: 0.00031565
Iteration 9/25 | Loss: 0.00031565
Iteration 10/25 | Loss: 0.00031565
Iteration 11/25 | Loss: 0.00031565
Iteration 12/25 | Loss: 0.00031565
Iteration 13/25 | Loss: 0.00031565
Iteration 14/25 | Loss: 0.00031565
Iteration 15/25 | Loss: 0.00031565
Iteration 16/25 | Loss: 0.00031565
Iteration 17/25 | Loss: 0.00031565
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003156457096338272, 0.0003156457096338272, 0.0003156457096338272, 0.0003156457096338272, 0.0003156457096338272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003156457096338272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031565
Iteration 2/1000 | Loss: 0.00004819
Iteration 3/1000 | Loss: 0.00003767
Iteration 4/1000 | Loss: 0.00003384
Iteration 5/1000 | Loss: 0.00003193
Iteration 6/1000 | Loss: 0.00003050
Iteration 7/1000 | Loss: 0.00002972
Iteration 8/1000 | Loss: 0.00002905
Iteration 9/1000 | Loss: 0.00002859
Iteration 10/1000 | Loss: 0.00002831
Iteration 11/1000 | Loss: 0.00002804
Iteration 12/1000 | Loss: 0.00002802
Iteration 13/1000 | Loss: 0.00002791
Iteration 14/1000 | Loss: 0.00002787
Iteration 15/1000 | Loss: 0.00002771
Iteration 16/1000 | Loss: 0.00002766
Iteration 17/1000 | Loss: 0.00002764
Iteration 18/1000 | Loss: 0.00002755
Iteration 19/1000 | Loss: 0.00002755
Iteration 20/1000 | Loss: 0.00002751
Iteration 21/1000 | Loss: 0.00002751
Iteration 22/1000 | Loss: 0.00002751
Iteration 23/1000 | Loss: 0.00002750
Iteration 24/1000 | Loss: 0.00002750
Iteration 25/1000 | Loss: 0.00002749
Iteration 26/1000 | Loss: 0.00002749
Iteration 27/1000 | Loss: 0.00002749
Iteration 28/1000 | Loss: 0.00002748
Iteration 29/1000 | Loss: 0.00002748
Iteration 30/1000 | Loss: 0.00002747
Iteration 31/1000 | Loss: 0.00002747
Iteration 32/1000 | Loss: 0.00002746
Iteration 33/1000 | Loss: 0.00002746
Iteration 34/1000 | Loss: 0.00002745
Iteration 35/1000 | Loss: 0.00002745
Iteration 36/1000 | Loss: 0.00002744
Iteration 37/1000 | Loss: 0.00002744
Iteration 38/1000 | Loss: 0.00002744
Iteration 39/1000 | Loss: 0.00002743
Iteration 40/1000 | Loss: 0.00002743
Iteration 41/1000 | Loss: 0.00002742
Iteration 42/1000 | Loss: 0.00002741
Iteration 43/1000 | Loss: 0.00002741
Iteration 44/1000 | Loss: 0.00002741
Iteration 45/1000 | Loss: 0.00002740
Iteration 46/1000 | Loss: 0.00002740
Iteration 47/1000 | Loss: 0.00002739
Iteration 48/1000 | Loss: 0.00002739
Iteration 49/1000 | Loss: 0.00002739
Iteration 50/1000 | Loss: 0.00002739
Iteration 51/1000 | Loss: 0.00002739
Iteration 52/1000 | Loss: 0.00002739
Iteration 53/1000 | Loss: 0.00002739
Iteration 54/1000 | Loss: 0.00002738
Iteration 55/1000 | Loss: 0.00002738
Iteration 56/1000 | Loss: 0.00002738
Iteration 57/1000 | Loss: 0.00002738
Iteration 58/1000 | Loss: 0.00002738
Iteration 59/1000 | Loss: 0.00002737
Iteration 60/1000 | Loss: 0.00002737
Iteration 61/1000 | Loss: 0.00002737
Iteration 62/1000 | Loss: 0.00002737
Iteration 63/1000 | Loss: 0.00002737
Iteration 64/1000 | Loss: 0.00002737
Iteration 65/1000 | Loss: 0.00002737
Iteration 66/1000 | Loss: 0.00002737
Iteration 67/1000 | Loss: 0.00002737
Iteration 68/1000 | Loss: 0.00002736
Iteration 69/1000 | Loss: 0.00002736
Iteration 70/1000 | Loss: 0.00002736
Iteration 71/1000 | Loss: 0.00002736
Iteration 72/1000 | Loss: 0.00002736
Iteration 73/1000 | Loss: 0.00002736
Iteration 74/1000 | Loss: 0.00002736
Iteration 75/1000 | Loss: 0.00002736
Iteration 76/1000 | Loss: 0.00002736
Iteration 77/1000 | Loss: 0.00002736
Iteration 78/1000 | Loss: 0.00002736
Iteration 79/1000 | Loss: 0.00002736
Iteration 80/1000 | Loss: 0.00002736
Iteration 81/1000 | Loss: 0.00002735
Iteration 82/1000 | Loss: 0.00002735
Iteration 83/1000 | Loss: 0.00002735
Iteration 84/1000 | Loss: 0.00002735
Iteration 85/1000 | Loss: 0.00002735
Iteration 86/1000 | Loss: 0.00002735
Iteration 87/1000 | Loss: 0.00002735
Iteration 88/1000 | Loss: 0.00002735
Iteration 89/1000 | Loss: 0.00002735
Iteration 90/1000 | Loss: 0.00002735
Iteration 91/1000 | Loss: 0.00002734
Iteration 92/1000 | Loss: 0.00002734
Iteration 93/1000 | Loss: 0.00002734
Iteration 94/1000 | Loss: 0.00002734
Iteration 95/1000 | Loss: 0.00002734
Iteration 96/1000 | Loss: 0.00002734
Iteration 97/1000 | Loss: 0.00002734
Iteration 98/1000 | Loss: 0.00002733
Iteration 99/1000 | Loss: 0.00002733
Iteration 100/1000 | Loss: 0.00002733
Iteration 101/1000 | Loss: 0.00002733
Iteration 102/1000 | Loss: 0.00002733
Iteration 103/1000 | Loss: 0.00002733
Iteration 104/1000 | Loss: 0.00002733
Iteration 105/1000 | Loss: 0.00002733
Iteration 106/1000 | Loss: 0.00002733
Iteration 107/1000 | Loss: 0.00002733
Iteration 108/1000 | Loss: 0.00002733
Iteration 109/1000 | Loss: 0.00002733
Iteration 110/1000 | Loss: 0.00002732
Iteration 111/1000 | Loss: 0.00002732
Iteration 112/1000 | Loss: 0.00002732
Iteration 113/1000 | Loss: 0.00002732
Iteration 114/1000 | Loss: 0.00002732
Iteration 115/1000 | Loss: 0.00002732
Iteration 116/1000 | Loss: 0.00002732
Iteration 117/1000 | Loss: 0.00002732
Iteration 118/1000 | Loss: 0.00002732
Iteration 119/1000 | Loss: 0.00002731
Iteration 120/1000 | Loss: 0.00002731
Iteration 121/1000 | Loss: 0.00002731
Iteration 122/1000 | Loss: 0.00002731
Iteration 123/1000 | Loss: 0.00002731
Iteration 124/1000 | Loss: 0.00002731
Iteration 125/1000 | Loss: 0.00002731
Iteration 126/1000 | Loss: 0.00002731
Iteration 127/1000 | Loss: 0.00002731
Iteration 128/1000 | Loss: 0.00002731
Iteration 129/1000 | Loss: 0.00002731
Iteration 130/1000 | Loss: 0.00002730
Iteration 131/1000 | Loss: 0.00002730
Iteration 132/1000 | Loss: 0.00002730
Iteration 133/1000 | Loss: 0.00002730
Iteration 134/1000 | Loss: 0.00002730
Iteration 135/1000 | Loss: 0.00002730
Iteration 136/1000 | Loss: 0.00002730
Iteration 137/1000 | Loss: 0.00002730
Iteration 138/1000 | Loss: 0.00002730
Iteration 139/1000 | Loss: 0.00002730
Iteration 140/1000 | Loss: 0.00002730
Iteration 141/1000 | Loss: 0.00002730
Iteration 142/1000 | Loss: 0.00002730
Iteration 143/1000 | Loss: 0.00002729
Iteration 144/1000 | Loss: 0.00002729
Iteration 145/1000 | Loss: 0.00002729
Iteration 146/1000 | Loss: 0.00002729
Iteration 147/1000 | Loss: 0.00002729
Iteration 148/1000 | Loss: 0.00002729
Iteration 149/1000 | Loss: 0.00002729
Iteration 150/1000 | Loss: 0.00002729
Iteration 151/1000 | Loss: 0.00002729
Iteration 152/1000 | Loss: 0.00002729
Iteration 153/1000 | Loss: 0.00002728
Iteration 154/1000 | Loss: 0.00002728
Iteration 155/1000 | Loss: 0.00002728
Iteration 156/1000 | Loss: 0.00002728
Iteration 157/1000 | Loss: 0.00002728
Iteration 158/1000 | Loss: 0.00002728
Iteration 159/1000 | Loss: 0.00002728
Iteration 160/1000 | Loss: 0.00002728
Iteration 161/1000 | Loss: 0.00002728
Iteration 162/1000 | Loss: 0.00002728
Iteration 163/1000 | Loss: 0.00002728
Iteration 164/1000 | Loss: 0.00002727
Iteration 165/1000 | Loss: 0.00002727
Iteration 166/1000 | Loss: 0.00002727
Iteration 167/1000 | Loss: 0.00002727
Iteration 168/1000 | Loss: 0.00002727
Iteration 169/1000 | Loss: 0.00002727
Iteration 170/1000 | Loss: 0.00002727
Iteration 171/1000 | Loss: 0.00002727
Iteration 172/1000 | Loss: 0.00002727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.727478204178624e-05, 2.727478204178624e-05, 2.727478204178624e-05, 2.727478204178624e-05, 2.727478204178624e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.727478204178624e-05

Optimization complete. Final v2v error: 4.2769951820373535 mm

Highest mean error: 5.038293361663818 mm for frame 40

Lowest mean error: 3.750594139099121 mm for frame 153

Saving results

Total time: 40.757933378219604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01089989
Iteration 2/25 | Loss: 0.00245528
Iteration 3/25 | Loss: 0.00163320
Iteration 4/25 | Loss: 0.00123874
Iteration 5/25 | Loss: 0.00119032
Iteration 6/25 | Loss: 0.00118007
Iteration 7/25 | Loss: 0.00108368
Iteration 8/25 | Loss: 0.00102574
Iteration 9/25 | Loss: 0.00100849
Iteration 10/25 | Loss: 0.00097819
Iteration 11/25 | Loss: 0.00095686
Iteration 12/25 | Loss: 0.00091723
Iteration 13/25 | Loss: 0.00088493
Iteration 14/25 | Loss: 0.00087268
Iteration 15/25 | Loss: 0.00087111
Iteration 16/25 | Loss: 0.00087077
Iteration 17/25 | Loss: 0.00087161
Iteration 18/25 | Loss: 0.00087045
Iteration 19/25 | Loss: 0.00086965
Iteration 20/25 | Loss: 0.00086833
Iteration 21/25 | Loss: 0.00086809
Iteration 22/25 | Loss: 0.00086646
Iteration 23/25 | Loss: 0.00086619
Iteration 24/25 | Loss: 0.00086615
Iteration 25/25 | Loss: 0.00086615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.99484873
Iteration 2/25 | Loss: 0.00047696
Iteration 3/25 | Loss: 0.00046775
Iteration 4/25 | Loss: 0.00046781
Iteration 5/25 | Loss: 0.00046282
Iteration 6/25 | Loss: 0.00046282
Iteration 7/25 | Loss: 0.00046282
Iteration 8/25 | Loss: 0.00046282
Iteration 9/25 | Loss: 0.00046282
Iteration 10/25 | Loss: 0.00046282
Iteration 11/25 | Loss: 0.00046282
Iteration 12/25 | Loss: 0.00046282
Iteration 13/25 | Loss: 0.00046282
Iteration 14/25 | Loss: 0.00046282
Iteration 15/25 | Loss: 0.00046282
Iteration 16/25 | Loss: 0.00046282
Iteration 17/25 | Loss: 0.00046282
Iteration 18/25 | Loss: 0.00046282
Iteration 19/25 | Loss: 0.00046282
Iteration 20/25 | Loss: 0.00046282
Iteration 21/25 | Loss: 0.00046282
Iteration 22/25 | Loss: 0.00046282
Iteration 23/25 | Loss: 0.00046282
Iteration 24/25 | Loss: 0.00046282
Iteration 25/25 | Loss: 0.00046282

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046282
Iteration 2/1000 | Loss: 0.00534854
Iteration 3/1000 | Loss: 0.00030509
Iteration 4/1000 | Loss: 0.00009743
Iteration 5/1000 | Loss: 0.00016057
Iteration 6/1000 | Loss: 0.00007440
Iteration 7/1000 | Loss: 0.00013241
Iteration 8/1000 | Loss: 0.00011903
Iteration 9/1000 | Loss: 0.00002228
Iteration 10/1000 | Loss: 0.00004898
Iteration 11/1000 | Loss: 0.00002069
Iteration 12/1000 | Loss: 0.00002395
Iteration 13/1000 | Loss: 0.00011089
Iteration 14/1000 | Loss: 0.00001852
Iteration 15/1000 | Loss: 0.00002056
Iteration 16/1000 | Loss: 0.00008172
Iteration 17/1000 | Loss: 0.00002390
Iteration 18/1000 | Loss: 0.00002559
Iteration 19/1000 | Loss: 0.00001766
Iteration 20/1000 | Loss: 0.00001748
Iteration 21/1000 | Loss: 0.00001735
Iteration 22/1000 | Loss: 0.00001734
Iteration 23/1000 | Loss: 0.00001730
Iteration 24/1000 | Loss: 0.00001730
Iteration 25/1000 | Loss: 0.00001729
Iteration 26/1000 | Loss: 0.00001729
Iteration 27/1000 | Loss: 0.00041597
Iteration 28/1000 | Loss: 0.00002238
Iteration 29/1000 | Loss: 0.00009037
Iteration 30/1000 | Loss: 0.00001753
Iteration 31/1000 | Loss: 0.00001650
Iteration 32/1000 | Loss: 0.00001569
Iteration 33/1000 | Loss: 0.00006412
Iteration 34/1000 | Loss: 0.00001543
Iteration 35/1000 | Loss: 0.00002767
Iteration 36/1000 | Loss: 0.00001504
Iteration 37/1000 | Loss: 0.00005973
Iteration 38/1000 | Loss: 0.00004387
Iteration 39/1000 | Loss: 0.00006293
Iteration 40/1000 | Loss: 0.00003264
Iteration 41/1000 | Loss: 0.00003264
Iteration 42/1000 | Loss: 0.00022949
Iteration 43/1000 | Loss: 0.00004584
Iteration 44/1000 | Loss: 0.00001701
Iteration 45/1000 | Loss: 0.00001537
Iteration 46/1000 | Loss: 0.00002734
Iteration 47/1000 | Loss: 0.00002003
Iteration 48/1000 | Loss: 0.00001480
Iteration 49/1000 | Loss: 0.00001480
Iteration 50/1000 | Loss: 0.00001480
Iteration 51/1000 | Loss: 0.00001480
Iteration 52/1000 | Loss: 0.00001480
Iteration 53/1000 | Loss: 0.00001480
Iteration 54/1000 | Loss: 0.00001480
Iteration 55/1000 | Loss: 0.00001480
Iteration 56/1000 | Loss: 0.00001480
Iteration 57/1000 | Loss: 0.00001480
Iteration 58/1000 | Loss: 0.00001480
Iteration 59/1000 | Loss: 0.00001480
Iteration 60/1000 | Loss: 0.00001480
Iteration 61/1000 | Loss: 0.00001480
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001479
Iteration 64/1000 | Loss: 0.00001479
Iteration 65/1000 | Loss: 0.00001479
Iteration 66/1000 | Loss: 0.00001479
Iteration 67/1000 | Loss: 0.00001479
Iteration 68/1000 | Loss: 0.00001479
Iteration 69/1000 | Loss: 0.00001479
Iteration 70/1000 | Loss: 0.00001479
Iteration 71/1000 | Loss: 0.00001479
Iteration 72/1000 | Loss: 0.00001479
Iteration 73/1000 | Loss: 0.00001479
Iteration 74/1000 | Loss: 0.00001478
Iteration 75/1000 | Loss: 0.00001477
Iteration 76/1000 | Loss: 0.00001477
Iteration 77/1000 | Loss: 0.00001477
Iteration 78/1000 | Loss: 0.00001476
Iteration 79/1000 | Loss: 0.00001476
Iteration 80/1000 | Loss: 0.00001475
Iteration 81/1000 | Loss: 0.00003015
Iteration 82/1000 | Loss: 0.00001473
Iteration 83/1000 | Loss: 0.00001469
Iteration 84/1000 | Loss: 0.00001469
Iteration 85/1000 | Loss: 0.00001468
Iteration 86/1000 | Loss: 0.00001467
Iteration 87/1000 | Loss: 0.00001467
Iteration 88/1000 | Loss: 0.00001467
Iteration 89/1000 | Loss: 0.00001467
Iteration 90/1000 | Loss: 0.00001467
Iteration 91/1000 | Loss: 0.00001467
Iteration 92/1000 | Loss: 0.00001467
Iteration 93/1000 | Loss: 0.00001466
Iteration 94/1000 | Loss: 0.00001466
Iteration 95/1000 | Loss: 0.00001466
Iteration 96/1000 | Loss: 0.00001466
Iteration 97/1000 | Loss: 0.00001466
Iteration 98/1000 | Loss: 0.00001466
Iteration 99/1000 | Loss: 0.00001466
Iteration 100/1000 | Loss: 0.00001466
Iteration 101/1000 | Loss: 0.00001466
Iteration 102/1000 | Loss: 0.00001466
Iteration 103/1000 | Loss: 0.00001466
Iteration 104/1000 | Loss: 0.00001466
Iteration 105/1000 | Loss: 0.00001465
Iteration 106/1000 | Loss: 0.00003034
Iteration 107/1000 | Loss: 0.00001466
Iteration 108/1000 | Loss: 0.00001465
Iteration 109/1000 | Loss: 0.00001465
Iteration 110/1000 | Loss: 0.00001465
Iteration 111/1000 | Loss: 0.00001465
Iteration 112/1000 | Loss: 0.00001465
Iteration 113/1000 | Loss: 0.00001465
Iteration 114/1000 | Loss: 0.00001464
Iteration 115/1000 | Loss: 0.00001464
Iteration 116/1000 | Loss: 0.00001464
Iteration 117/1000 | Loss: 0.00001464
Iteration 118/1000 | Loss: 0.00001464
Iteration 119/1000 | Loss: 0.00001464
Iteration 120/1000 | Loss: 0.00001464
Iteration 121/1000 | Loss: 0.00001464
Iteration 122/1000 | Loss: 0.00001464
Iteration 123/1000 | Loss: 0.00001464
Iteration 124/1000 | Loss: 0.00001464
Iteration 125/1000 | Loss: 0.00001464
Iteration 126/1000 | Loss: 0.00001464
Iteration 127/1000 | Loss: 0.00001464
Iteration 128/1000 | Loss: 0.00001463
Iteration 129/1000 | Loss: 0.00001463
Iteration 130/1000 | Loss: 0.00001463
Iteration 131/1000 | Loss: 0.00001463
Iteration 132/1000 | Loss: 0.00001463
Iteration 133/1000 | Loss: 0.00001463
Iteration 134/1000 | Loss: 0.00001463
Iteration 135/1000 | Loss: 0.00001463
Iteration 136/1000 | Loss: 0.00001463
Iteration 137/1000 | Loss: 0.00001463
Iteration 138/1000 | Loss: 0.00001463
Iteration 139/1000 | Loss: 0.00001463
Iteration 140/1000 | Loss: 0.00001463
Iteration 141/1000 | Loss: 0.00001463
Iteration 142/1000 | Loss: 0.00001463
Iteration 143/1000 | Loss: 0.00001463
Iteration 144/1000 | Loss: 0.00001463
Iteration 145/1000 | Loss: 0.00001463
Iteration 146/1000 | Loss: 0.00001463
Iteration 147/1000 | Loss: 0.00001463
Iteration 148/1000 | Loss: 0.00001463
Iteration 149/1000 | Loss: 0.00001463
Iteration 150/1000 | Loss: 0.00001463
Iteration 151/1000 | Loss: 0.00001463
Iteration 152/1000 | Loss: 0.00001463
Iteration 153/1000 | Loss: 0.00001463
Iteration 154/1000 | Loss: 0.00001463
Iteration 155/1000 | Loss: 0.00001463
Iteration 156/1000 | Loss: 0.00001463
Iteration 157/1000 | Loss: 0.00001463
Iteration 158/1000 | Loss: 0.00001463
Iteration 159/1000 | Loss: 0.00001463
Iteration 160/1000 | Loss: 0.00001463
Iteration 161/1000 | Loss: 0.00001463
Iteration 162/1000 | Loss: 0.00001463
Iteration 163/1000 | Loss: 0.00001463
Iteration 164/1000 | Loss: 0.00001463
Iteration 165/1000 | Loss: 0.00001463
Iteration 166/1000 | Loss: 0.00001463
Iteration 167/1000 | Loss: 0.00001463
Iteration 168/1000 | Loss: 0.00001463
Iteration 169/1000 | Loss: 0.00001463
Iteration 170/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.4625825315306429e-05, 1.4625825315306429e-05, 1.4625825315306429e-05, 1.4625825315306429e-05, 1.4625825315306429e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4625825315306429e-05

Optimization complete. Final v2v error: 3.0895559787750244 mm

Highest mean error: 8.835387229919434 mm for frame 60

Lowest mean error: 2.760230779647827 mm for frame 48

Saving results

Total time: 111.35981035232544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037706
Iteration 2/25 | Loss: 0.01037706
Iteration 3/25 | Loss: 0.00266185
Iteration 4/25 | Loss: 0.00204182
Iteration 5/25 | Loss: 0.00185685
Iteration 6/25 | Loss: 0.00180684
Iteration 7/25 | Loss: 0.00175460
Iteration 8/25 | Loss: 0.00169881
Iteration 9/25 | Loss: 0.00166855
Iteration 10/25 | Loss: 0.00165943
Iteration 11/25 | Loss: 0.00165610
Iteration 12/25 | Loss: 0.00164580
Iteration 13/25 | Loss: 0.00161428
Iteration 14/25 | Loss: 0.00159625
Iteration 15/25 | Loss: 0.00158888
Iteration 16/25 | Loss: 0.00157671
Iteration 17/25 | Loss: 0.00158189
Iteration 18/25 | Loss: 0.00156261
Iteration 19/25 | Loss: 0.00155658
Iteration 20/25 | Loss: 0.00155189
Iteration 21/25 | Loss: 0.00154390
Iteration 22/25 | Loss: 0.00153322
Iteration 23/25 | Loss: 0.00152652
Iteration 24/25 | Loss: 0.00152724
Iteration 25/25 | Loss: 0.00152392

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46813583
Iteration 2/25 | Loss: 0.01131998
Iteration 3/25 | Loss: 0.00883407
Iteration 4/25 | Loss: 0.00883407
Iteration 5/25 | Loss: 0.00883406
Iteration 6/25 | Loss: 0.00883406
Iteration 7/25 | Loss: 0.00883406
Iteration 8/25 | Loss: 0.00883406
Iteration 9/25 | Loss: 0.00883406
Iteration 10/25 | Loss: 0.00883406
Iteration 11/25 | Loss: 0.00883406
Iteration 12/25 | Loss: 0.00883406
Iteration 13/25 | Loss: 0.00883406
Iteration 14/25 | Loss: 0.00883406
Iteration 15/25 | Loss: 0.00883406
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.008834063075482845, 0.008834063075482845, 0.008834063075482845, 0.008834063075482845, 0.008834063075482845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008834063075482845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00883406
Iteration 2/1000 | Loss: 0.00690344
Iteration 3/1000 | Loss: 0.01690468
Iteration 4/1000 | Loss: 0.00404870
Iteration 5/1000 | Loss: 0.00140949
Iteration 6/1000 | Loss: 0.00135579
Iteration 7/1000 | Loss: 0.00303057
Iteration 8/1000 | Loss: 0.00424536
Iteration 9/1000 | Loss: 0.00117242
Iteration 10/1000 | Loss: 0.00209554
Iteration 11/1000 | Loss: 0.00229332
Iteration 12/1000 | Loss: 0.00251712
Iteration 13/1000 | Loss: 0.00250190
Iteration 14/1000 | Loss: 0.00241900
Iteration 15/1000 | Loss: 0.00181247
Iteration 16/1000 | Loss: 0.00112347
Iteration 17/1000 | Loss: 0.00322611
Iteration 18/1000 | Loss: 0.00225787
Iteration 19/1000 | Loss: 0.00113278
Iteration 20/1000 | Loss: 0.00284090
Iteration 21/1000 | Loss: 0.00073453
Iteration 22/1000 | Loss: 0.00223487
Iteration 23/1000 | Loss: 0.00261186
Iteration 24/1000 | Loss: 0.00183497
Iteration 25/1000 | Loss: 0.00170872
Iteration 26/1000 | Loss: 0.00214137
Iteration 27/1000 | Loss: 0.00538537
Iteration 28/1000 | Loss: 0.00293571
Iteration 29/1000 | Loss: 0.00308838
Iteration 30/1000 | Loss: 0.00108790
Iteration 31/1000 | Loss: 0.00156247
Iteration 32/1000 | Loss: 0.00195436
Iteration 33/1000 | Loss: 0.00233251
Iteration 34/1000 | Loss: 0.00171366
Iteration 35/1000 | Loss: 0.00240502
Iteration 36/1000 | Loss: 0.00151175
Iteration 37/1000 | Loss: 0.00301764
Iteration 38/1000 | Loss: 0.00268975
Iteration 39/1000 | Loss: 0.00182779
Iteration 40/1000 | Loss: 0.00129362
Iteration 41/1000 | Loss: 0.00141315
Iteration 42/1000 | Loss: 0.00223316
Iteration 43/1000 | Loss: 0.00481046
Iteration 44/1000 | Loss: 0.00399302
Iteration 45/1000 | Loss: 0.00512026
Iteration 46/1000 | Loss: 0.00326974
Iteration 47/1000 | Loss: 0.00168212
Iteration 48/1000 | Loss: 0.00116770
Iteration 49/1000 | Loss: 0.00115654
Iteration 50/1000 | Loss: 0.00137843
Iteration 51/1000 | Loss: 0.00078248
Iteration 52/1000 | Loss: 0.00254749
Iteration 53/1000 | Loss: 0.00080484
Iteration 54/1000 | Loss: 0.00129239
Iteration 55/1000 | Loss: 0.00136937
Iteration 56/1000 | Loss: 0.00124085
Iteration 57/1000 | Loss: 0.00098492
Iteration 58/1000 | Loss: 0.00237242
Iteration 59/1000 | Loss: 0.00296744
Iteration 60/1000 | Loss: 0.00060247
Iteration 61/1000 | Loss: 0.00180743
Iteration 62/1000 | Loss: 0.00149215
Iteration 63/1000 | Loss: 0.00057099
Iteration 64/1000 | Loss: 0.00174242
Iteration 65/1000 | Loss: 0.00139717
Iteration 66/1000 | Loss: 0.00231637
Iteration 67/1000 | Loss: 0.00048342
Iteration 68/1000 | Loss: 0.00054559
Iteration 69/1000 | Loss: 0.00222937
Iteration 70/1000 | Loss: 0.00338457
Iteration 71/1000 | Loss: 0.00478797
Iteration 72/1000 | Loss: 0.00369556
Iteration 73/1000 | Loss: 0.00319622
Iteration 74/1000 | Loss: 0.00162759
Iteration 75/1000 | Loss: 0.00092064
Iteration 76/1000 | Loss: 0.00111688
Iteration 77/1000 | Loss: 0.00135223
Iteration 78/1000 | Loss: 0.00104088
Iteration 79/1000 | Loss: 0.00079949
Iteration 80/1000 | Loss: 0.00123651
Iteration 81/1000 | Loss: 0.00100999
Iteration 82/1000 | Loss: 0.00131270
Iteration 83/1000 | Loss: 0.00101718
Iteration 84/1000 | Loss: 0.00052273
Iteration 85/1000 | Loss: 0.00182568
Iteration 86/1000 | Loss: 0.00090863
Iteration 87/1000 | Loss: 0.00131194
Iteration 88/1000 | Loss: 0.00092248
Iteration 89/1000 | Loss: 0.00111078
Iteration 90/1000 | Loss: 0.00046557
Iteration 91/1000 | Loss: 0.00097600
Iteration 92/1000 | Loss: 0.00072865
Iteration 93/1000 | Loss: 0.00138581
Iteration 94/1000 | Loss: 0.00073918
Iteration 95/1000 | Loss: 0.00044518
Iteration 96/1000 | Loss: 0.00074922
Iteration 97/1000 | Loss: 0.00067004
Iteration 98/1000 | Loss: 0.00139959
Iteration 99/1000 | Loss: 0.00090778
Iteration 100/1000 | Loss: 0.00105434
Iteration 101/1000 | Loss: 0.00144098
Iteration 102/1000 | Loss: 0.00047749
Iteration 103/1000 | Loss: 0.00035699
Iteration 104/1000 | Loss: 0.00022452
Iteration 105/1000 | Loss: 0.00036761
Iteration 106/1000 | Loss: 0.00045514
Iteration 107/1000 | Loss: 0.00061282
Iteration 108/1000 | Loss: 0.00019376
Iteration 109/1000 | Loss: 0.00038137
Iteration 110/1000 | Loss: 0.00012971
Iteration 111/1000 | Loss: 0.00076535
Iteration 112/1000 | Loss: 0.00056489
Iteration 113/1000 | Loss: 0.00167771
Iteration 114/1000 | Loss: 0.00104020
Iteration 115/1000 | Loss: 0.00052935
Iteration 116/1000 | Loss: 0.00056003
Iteration 117/1000 | Loss: 0.00182966
Iteration 118/1000 | Loss: 0.00097764
Iteration 119/1000 | Loss: 0.00060517
Iteration 120/1000 | Loss: 0.00099683
Iteration 121/1000 | Loss: 0.00041210
Iteration 122/1000 | Loss: 0.00035633
Iteration 123/1000 | Loss: 0.00046135
Iteration 124/1000 | Loss: 0.00067647
Iteration 125/1000 | Loss: 0.00100973
Iteration 126/1000 | Loss: 0.00110833
Iteration 127/1000 | Loss: 0.00033436
Iteration 128/1000 | Loss: 0.00024785
Iteration 129/1000 | Loss: 0.00051203
Iteration 130/1000 | Loss: 0.00021719
Iteration 131/1000 | Loss: 0.00014845
Iteration 132/1000 | Loss: 0.00032498
Iteration 133/1000 | Loss: 0.00051726
Iteration 134/1000 | Loss: 0.00039305
Iteration 135/1000 | Loss: 0.00092428
Iteration 136/1000 | Loss: 0.00024283
Iteration 137/1000 | Loss: 0.00035532
Iteration 138/1000 | Loss: 0.00010851
Iteration 139/1000 | Loss: 0.00009318
Iteration 140/1000 | Loss: 0.00012488
Iteration 141/1000 | Loss: 0.00033943
Iteration 142/1000 | Loss: 0.00009897
Iteration 143/1000 | Loss: 0.00051696
Iteration 144/1000 | Loss: 0.00072424
Iteration 145/1000 | Loss: 0.00054436
Iteration 146/1000 | Loss: 0.00046095
Iteration 147/1000 | Loss: 0.00025410
Iteration 148/1000 | Loss: 0.00012778
Iteration 149/1000 | Loss: 0.00018969
Iteration 150/1000 | Loss: 0.00049268
Iteration 151/1000 | Loss: 0.00036937
Iteration 152/1000 | Loss: 0.00024690
Iteration 153/1000 | Loss: 0.00049770
Iteration 154/1000 | Loss: 0.00040666
Iteration 155/1000 | Loss: 0.00040988
Iteration 156/1000 | Loss: 0.00035675
Iteration 157/1000 | Loss: 0.00036823
Iteration 158/1000 | Loss: 0.00045550
Iteration 159/1000 | Loss: 0.00028380
Iteration 160/1000 | Loss: 0.00055874
Iteration 161/1000 | Loss: 0.00020267
Iteration 162/1000 | Loss: 0.00030622
Iteration 163/1000 | Loss: 0.00033509
Iteration 164/1000 | Loss: 0.00035491
Iteration 165/1000 | Loss: 0.00083906
Iteration 166/1000 | Loss: 0.00017712
Iteration 167/1000 | Loss: 0.00023455
Iteration 168/1000 | Loss: 0.00051676
Iteration 169/1000 | Loss: 0.00034085
Iteration 170/1000 | Loss: 0.00023125
Iteration 171/1000 | Loss: 0.00049044
Iteration 172/1000 | Loss: 0.00009670
Iteration 173/1000 | Loss: 0.00018772
Iteration 174/1000 | Loss: 0.00053766
Iteration 175/1000 | Loss: 0.00020446
Iteration 176/1000 | Loss: 0.00011380
Iteration 177/1000 | Loss: 0.00025521
Iteration 178/1000 | Loss: 0.00026305
Iteration 179/1000 | Loss: 0.00005434
Iteration 180/1000 | Loss: 0.00044128
Iteration 181/1000 | Loss: 0.00031135
Iteration 182/1000 | Loss: 0.00005218
Iteration 183/1000 | Loss: 0.00017239
Iteration 184/1000 | Loss: 0.00031239
Iteration 185/1000 | Loss: 0.00054697
Iteration 186/1000 | Loss: 0.00017240
Iteration 187/1000 | Loss: 0.00034767
Iteration 188/1000 | Loss: 0.00023022
Iteration 189/1000 | Loss: 0.00041645
Iteration 190/1000 | Loss: 0.00005600
Iteration 191/1000 | Loss: 0.00005189
Iteration 192/1000 | Loss: 0.00007013
Iteration 193/1000 | Loss: 0.00005188
Iteration 194/1000 | Loss: 0.00007076
Iteration 195/1000 | Loss: 0.00043597
Iteration 196/1000 | Loss: 0.00103961
Iteration 197/1000 | Loss: 0.00007956
Iteration 198/1000 | Loss: 0.00007122
Iteration 199/1000 | Loss: 0.00018097
Iteration 200/1000 | Loss: 0.00043970
Iteration 201/1000 | Loss: 0.00020470
Iteration 202/1000 | Loss: 0.00017989
Iteration 203/1000 | Loss: 0.00014016
Iteration 204/1000 | Loss: 0.00005810
Iteration 205/1000 | Loss: 0.00007647
Iteration 206/1000 | Loss: 0.00006101
Iteration 207/1000 | Loss: 0.00014275
Iteration 208/1000 | Loss: 0.00030515
Iteration 209/1000 | Loss: 0.00005500
Iteration 210/1000 | Loss: 0.00005309
Iteration 211/1000 | Loss: 0.00004812
Iteration 212/1000 | Loss: 0.00005730
Iteration 213/1000 | Loss: 0.00004448
Iteration 214/1000 | Loss: 0.00005293
Iteration 215/1000 | Loss: 0.00004005
Iteration 216/1000 | Loss: 0.00005552
Iteration 217/1000 | Loss: 0.00017768
Iteration 218/1000 | Loss: 0.00005708
Iteration 219/1000 | Loss: 0.00004503
Iteration 220/1000 | Loss: 0.00004204
Iteration 221/1000 | Loss: 0.00004084
Iteration 222/1000 | Loss: 0.00004009
Iteration 223/1000 | Loss: 0.00003963
Iteration 224/1000 | Loss: 0.00019448
Iteration 225/1000 | Loss: 0.00016912
Iteration 226/1000 | Loss: 0.00005866
Iteration 227/1000 | Loss: 0.00010881
Iteration 228/1000 | Loss: 0.00005553
Iteration 229/1000 | Loss: 0.00004187
Iteration 230/1000 | Loss: 0.00005913
Iteration 231/1000 | Loss: 0.00005376
Iteration 232/1000 | Loss: 0.00005073
Iteration 233/1000 | Loss: 0.00004824
Iteration 234/1000 | Loss: 0.00004478
Iteration 235/1000 | Loss: 0.00005123
Iteration 236/1000 | Loss: 0.00022022
Iteration 237/1000 | Loss: 0.00026193
Iteration 238/1000 | Loss: 0.00003949
Iteration 239/1000 | Loss: 0.00003791
Iteration 240/1000 | Loss: 0.00003712
Iteration 241/1000 | Loss: 0.00013083
Iteration 242/1000 | Loss: 0.00004730
Iteration 243/1000 | Loss: 0.00004885
Iteration 244/1000 | Loss: 0.00003639
Iteration 245/1000 | Loss: 0.00003626
Iteration 246/1000 | Loss: 0.00003624
Iteration 247/1000 | Loss: 0.00003622
Iteration 248/1000 | Loss: 0.00003616
Iteration 249/1000 | Loss: 0.00003608
Iteration 250/1000 | Loss: 0.00003606
Iteration 251/1000 | Loss: 0.00003597
Iteration 252/1000 | Loss: 0.00003587
Iteration 253/1000 | Loss: 0.00003581
Iteration 254/1000 | Loss: 0.00003579
Iteration 255/1000 | Loss: 0.00003576
Iteration 256/1000 | Loss: 0.00003576
Iteration 257/1000 | Loss: 0.00003575
Iteration 258/1000 | Loss: 0.00003575
Iteration 259/1000 | Loss: 0.00003575
Iteration 260/1000 | Loss: 0.00003573
Iteration 261/1000 | Loss: 0.00003570
Iteration 262/1000 | Loss: 0.00003566
Iteration 263/1000 | Loss: 0.00003563
Iteration 264/1000 | Loss: 0.00003562
Iteration 265/1000 | Loss: 0.00003560
Iteration 266/1000 | Loss: 0.00003560
Iteration 267/1000 | Loss: 0.00003559
Iteration 268/1000 | Loss: 0.00003558
Iteration 269/1000 | Loss: 0.00004720
Iteration 270/1000 | Loss: 0.00003881
Iteration 271/1000 | Loss: 0.00003684
Iteration 272/1000 | Loss: 0.00004080
Iteration 273/1000 | Loss: 0.00004547
Iteration 274/1000 | Loss: 0.00003653
Iteration 275/1000 | Loss: 0.00003604
Iteration 276/1000 | Loss: 0.00003567
Iteration 277/1000 | Loss: 0.00003553
Iteration 278/1000 | Loss: 0.00003553
Iteration 279/1000 | Loss: 0.00003548
Iteration 280/1000 | Loss: 0.00003545
Iteration 281/1000 | Loss: 0.00003545
Iteration 282/1000 | Loss: 0.00003545
Iteration 283/1000 | Loss: 0.00003545
Iteration 284/1000 | Loss: 0.00003545
Iteration 285/1000 | Loss: 0.00003545
Iteration 286/1000 | Loss: 0.00003545
Iteration 287/1000 | Loss: 0.00003544
Iteration 288/1000 | Loss: 0.00003544
Iteration 289/1000 | Loss: 0.00003544
Iteration 290/1000 | Loss: 0.00003544
Iteration 291/1000 | Loss: 0.00003544
Iteration 292/1000 | Loss: 0.00003544
Iteration 293/1000 | Loss: 0.00003544
Iteration 294/1000 | Loss: 0.00003544
Iteration 295/1000 | Loss: 0.00003544
Iteration 296/1000 | Loss: 0.00003544
Iteration 297/1000 | Loss: 0.00003544
Iteration 298/1000 | Loss: 0.00003544
Iteration 299/1000 | Loss: 0.00003544
Iteration 300/1000 | Loss: 0.00003543
Iteration 301/1000 | Loss: 0.00003543
Iteration 302/1000 | Loss: 0.00003543
Iteration 303/1000 | Loss: 0.00003542
Iteration 304/1000 | Loss: 0.00003541
Iteration 305/1000 | Loss: 0.00003541
Iteration 306/1000 | Loss: 0.00003541
Iteration 307/1000 | Loss: 0.00003541
Iteration 308/1000 | Loss: 0.00003540
Iteration 309/1000 | Loss: 0.00003539
Iteration 310/1000 | Loss: 0.00003539
Iteration 311/1000 | Loss: 0.00003539
Iteration 312/1000 | Loss: 0.00003539
Iteration 313/1000 | Loss: 0.00003539
Iteration 314/1000 | Loss: 0.00003539
Iteration 315/1000 | Loss: 0.00003538
Iteration 316/1000 | Loss: 0.00003538
Iteration 317/1000 | Loss: 0.00003538
Iteration 318/1000 | Loss: 0.00003538
Iteration 319/1000 | Loss: 0.00003538
Iteration 320/1000 | Loss: 0.00003538
Iteration 321/1000 | Loss: 0.00003538
Iteration 322/1000 | Loss: 0.00003538
Iteration 323/1000 | Loss: 0.00003538
Iteration 324/1000 | Loss: 0.00003538
Iteration 325/1000 | Loss: 0.00003538
Iteration 326/1000 | Loss: 0.00003538
Iteration 327/1000 | Loss: 0.00003538
Iteration 328/1000 | Loss: 0.00003537
Iteration 329/1000 | Loss: 0.00003537
Iteration 330/1000 | Loss: 0.00003537
Iteration 331/1000 | Loss: 0.00003537
Iteration 332/1000 | Loss: 0.00003537
Iteration 333/1000 | Loss: 0.00003537
Iteration 334/1000 | Loss: 0.00003537
Iteration 335/1000 | Loss: 0.00003537
Iteration 336/1000 | Loss: 0.00003537
Iteration 337/1000 | Loss: 0.00003537
Iteration 338/1000 | Loss: 0.00003537
Iteration 339/1000 | Loss: 0.00003537
Iteration 340/1000 | Loss: 0.00003537
Iteration 341/1000 | Loss: 0.00003537
Iteration 342/1000 | Loss: 0.00003537
Iteration 343/1000 | Loss: 0.00003537
Iteration 344/1000 | Loss: 0.00003537
Iteration 345/1000 | Loss: 0.00003537
Iteration 346/1000 | Loss: 0.00003537
Iteration 347/1000 | Loss: 0.00003537
Iteration 348/1000 | Loss: 0.00003537
Iteration 349/1000 | Loss: 0.00003537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 349. Stopping optimization.
Last 5 losses: [3.536735675879754e-05, 3.536735675879754e-05, 3.536735675879754e-05, 3.536735675879754e-05, 3.536735675879754e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.536735675879754e-05

Optimization complete. Final v2v error: 4.352495193481445 mm

Highest mean error: 13.549128532409668 mm for frame 73

Lowest mean error: 3.759554147720337 mm for frame 27

Saving results

Total time: 470.23159742355347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778913
Iteration 2/25 | Loss: 0.00137643
Iteration 3/25 | Loss: 0.00099807
Iteration 4/25 | Loss: 0.00092068
Iteration 5/25 | Loss: 0.00088738
Iteration 6/25 | Loss: 0.00086201
Iteration 7/25 | Loss: 0.00085029
Iteration 8/25 | Loss: 0.00083849
Iteration 9/25 | Loss: 0.00083656
Iteration 10/25 | Loss: 0.00083606
Iteration 11/25 | Loss: 0.00083638
Iteration 12/25 | Loss: 0.00083569
Iteration 13/25 | Loss: 0.00083557
Iteration 14/25 | Loss: 0.00083495
Iteration 15/25 | Loss: 0.00083469
Iteration 16/25 | Loss: 0.00083495
Iteration 17/25 | Loss: 0.00083402
Iteration 18/25 | Loss: 0.00083300
Iteration 19/25 | Loss: 0.00083671
Iteration 20/25 | Loss: 0.00083666
Iteration 21/25 | Loss: 0.00083603
Iteration 22/25 | Loss: 0.00083541
Iteration 23/25 | Loss: 0.00083457
Iteration 24/25 | Loss: 0.00083651
Iteration 25/25 | Loss: 0.00083461

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02775240
Iteration 2/25 | Loss: 0.00071594
Iteration 3/25 | Loss: 0.00071588
Iteration 4/25 | Loss: 0.00071588
Iteration 5/25 | Loss: 0.00071588
Iteration 6/25 | Loss: 0.00071588
Iteration 7/25 | Loss: 0.00071588
Iteration 8/25 | Loss: 0.00071588
Iteration 9/25 | Loss: 0.00071588
Iteration 10/25 | Loss: 0.00071588
Iteration 11/25 | Loss: 0.00071588
Iteration 12/25 | Loss: 0.00071587
Iteration 13/25 | Loss: 0.00071587
Iteration 14/25 | Loss: 0.00071587
Iteration 15/25 | Loss: 0.00071587
Iteration 16/25 | Loss: 0.00071587
Iteration 17/25 | Loss: 0.00071587
Iteration 18/25 | Loss: 0.00071587
Iteration 19/25 | Loss: 0.00071587
Iteration 20/25 | Loss: 0.00071587
Iteration 21/25 | Loss: 0.00071587
Iteration 22/25 | Loss: 0.00071587
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000715874950401485, 0.000715874950401485, 0.000715874950401485, 0.000715874950401485, 0.000715874950401485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000715874950401485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071588
Iteration 2/1000 | Loss: 0.00046829
Iteration 3/1000 | Loss: 0.00107188
Iteration 4/1000 | Loss: 0.00048589
Iteration 5/1000 | Loss: 0.00012239
Iteration 6/1000 | Loss: 0.00033923
Iteration 7/1000 | Loss: 0.00006150
Iteration 8/1000 | Loss: 0.00051921
Iteration 9/1000 | Loss: 0.00006304
Iteration 10/1000 | Loss: 0.00042246
Iteration 11/1000 | Loss: 0.00006057
Iteration 12/1000 | Loss: 0.00061331
Iteration 13/1000 | Loss: 0.00012355
Iteration 14/1000 | Loss: 0.00058165
Iteration 15/1000 | Loss: 0.00021576
Iteration 16/1000 | Loss: 0.00057294
Iteration 17/1000 | Loss: 0.00045815
Iteration 18/1000 | Loss: 0.00004833
Iteration 19/1000 | Loss: 0.00004021
Iteration 20/1000 | Loss: 0.00003658
Iteration 21/1000 | Loss: 0.00003463
Iteration 22/1000 | Loss: 0.00003288
Iteration 23/1000 | Loss: 0.00003128
Iteration 24/1000 | Loss: 0.00003024
Iteration 25/1000 | Loss: 0.00002955
Iteration 26/1000 | Loss: 0.00059877
Iteration 27/1000 | Loss: 0.00003314
Iteration 28/1000 | Loss: 0.00002971
Iteration 29/1000 | Loss: 0.00002832
Iteration 30/1000 | Loss: 0.00002743
Iteration 31/1000 | Loss: 0.00002658
Iteration 32/1000 | Loss: 0.00002597
Iteration 33/1000 | Loss: 0.00002568
Iteration 34/1000 | Loss: 0.00002547
Iteration 35/1000 | Loss: 0.00002527
Iteration 36/1000 | Loss: 0.00002516
Iteration 37/1000 | Loss: 0.00002515
Iteration 38/1000 | Loss: 0.00002510
Iteration 39/1000 | Loss: 0.00002507
Iteration 40/1000 | Loss: 0.00002506
Iteration 41/1000 | Loss: 0.00002504
Iteration 42/1000 | Loss: 0.00002503
Iteration 43/1000 | Loss: 0.00002503
Iteration 44/1000 | Loss: 0.00002503
Iteration 45/1000 | Loss: 0.00002502
Iteration 46/1000 | Loss: 0.00002502
Iteration 47/1000 | Loss: 0.00002502
Iteration 48/1000 | Loss: 0.00002501
Iteration 49/1000 | Loss: 0.00002501
Iteration 50/1000 | Loss: 0.00002501
Iteration 51/1000 | Loss: 0.00002501
Iteration 52/1000 | Loss: 0.00002501
Iteration 53/1000 | Loss: 0.00002501
Iteration 54/1000 | Loss: 0.00002501
Iteration 55/1000 | Loss: 0.00002500
Iteration 56/1000 | Loss: 0.00002500
Iteration 57/1000 | Loss: 0.00002500
Iteration 58/1000 | Loss: 0.00002500
Iteration 59/1000 | Loss: 0.00002499
Iteration 60/1000 | Loss: 0.00002498
Iteration 61/1000 | Loss: 0.00002497
Iteration 62/1000 | Loss: 0.00002496
Iteration 63/1000 | Loss: 0.00002496
Iteration 64/1000 | Loss: 0.00002493
Iteration 65/1000 | Loss: 0.00002489
Iteration 66/1000 | Loss: 0.00002489
Iteration 67/1000 | Loss: 0.00002489
Iteration 68/1000 | Loss: 0.00002489
Iteration 69/1000 | Loss: 0.00002489
Iteration 70/1000 | Loss: 0.00002488
Iteration 71/1000 | Loss: 0.00002488
Iteration 72/1000 | Loss: 0.00002488
Iteration 73/1000 | Loss: 0.00002487
Iteration 74/1000 | Loss: 0.00002487
Iteration 75/1000 | Loss: 0.00002484
Iteration 76/1000 | Loss: 0.00002484
Iteration 77/1000 | Loss: 0.00002484
Iteration 78/1000 | Loss: 0.00002484
Iteration 79/1000 | Loss: 0.00002484
Iteration 80/1000 | Loss: 0.00002484
Iteration 81/1000 | Loss: 0.00002484
Iteration 82/1000 | Loss: 0.00002483
Iteration 83/1000 | Loss: 0.00002483
Iteration 84/1000 | Loss: 0.00002483
Iteration 85/1000 | Loss: 0.00002483
Iteration 86/1000 | Loss: 0.00002483
Iteration 87/1000 | Loss: 0.00002483
Iteration 88/1000 | Loss: 0.00002483
Iteration 89/1000 | Loss: 0.00002483
Iteration 90/1000 | Loss: 0.00002483
Iteration 91/1000 | Loss: 0.00002482
Iteration 92/1000 | Loss: 0.00002482
Iteration 93/1000 | Loss: 0.00002482
Iteration 94/1000 | Loss: 0.00002482
Iteration 95/1000 | Loss: 0.00002482
Iteration 96/1000 | Loss: 0.00002482
Iteration 97/1000 | Loss: 0.00002481
Iteration 98/1000 | Loss: 0.00002481
Iteration 99/1000 | Loss: 0.00002481
Iteration 100/1000 | Loss: 0.00002481
Iteration 101/1000 | Loss: 0.00002481
Iteration 102/1000 | Loss: 0.00002481
Iteration 103/1000 | Loss: 0.00002480
Iteration 104/1000 | Loss: 0.00002480
Iteration 105/1000 | Loss: 0.00002480
Iteration 106/1000 | Loss: 0.00002480
Iteration 107/1000 | Loss: 0.00002480
Iteration 108/1000 | Loss: 0.00002480
Iteration 109/1000 | Loss: 0.00002480
Iteration 110/1000 | Loss: 0.00002480
Iteration 111/1000 | Loss: 0.00002480
Iteration 112/1000 | Loss: 0.00002480
Iteration 113/1000 | Loss: 0.00002480
Iteration 114/1000 | Loss: 0.00002479
Iteration 115/1000 | Loss: 0.00002479
Iteration 116/1000 | Loss: 0.00002479
Iteration 117/1000 | Loss: 0.00002479
Iteration 118/1000 | Loss: 0.00002478
Iteration 119/1000 | Loss: 0.00002478
Iteration 120/1000 | Loss: 0.00002478
Iteration 121/1000 | Loss: 0.00002478
Iteration 122/1000 | Loss: 0.00002477
Iteration 123/1000 | Loss: 0.00002477
Iteration 124/1000 | Loss: 0.00002477
Iteration 125/1000 | Loss: 0.00002477
Iteration 126/1000 | Loss: 0.00002477
Iteration 127/1000 | Loss: 0.00002477
Iteration 128/1000 | Loss: 0.00002477
Iteration 129/1000 | Loss: 0.00002477
Iteration 130/1000 | Loss: 0.00002477
Iteration 131/1000 | Loss: 0.00002477
Iteration 132/1000 | Loss: 0.00002477
Iteration 133/1000 | Loss: 0.00002477
Iteration 134/1000 | Loss: 0.00002477
Iteration 135/1000 | Loss: 0.00002476
Iteration 136/1000 | Loss: 0.00002476
Iteration 137/1000 | Loss: 0.00002476
Iteration 138/1000 | Loss: 0.00002476
Iteration 139/1000 | Loss: 0.00002476
Iteration 140/1000 | Loss: 0.00002475
Iteration 141/1000 | Loss: 0.00002475
Iteration 142/1000 | Loss: 0.00002475
Iteration 143/1000 | Loss: 0.00002475
Iteration 144/1000 | Loss: 0.00002475
Iteration 145/1000 | Loss: 0.00002475
Iteration 146/1000 | Loss: 0.00002475
Iteration 147/1000 | Loss: 0.00002475
Iteration 148/1000 | Loss: 0.00002475
Iteration 149/1000 | Loss: 0.00002475
Iteration 150/1000 | Loss: 0.00002475
Iteration 151/1000 | Loss: 0.00002475
Iteration 152/1000 | Loss: 0.00002475
Iteration 153/1000 | Loss: 0.00002475
Iteration 154/1000 | Loss: 0.00002475
Iteration 155/1000 | Loss: 0.00002475
Iteration 156/1000 | Loss: 0.00002475
Iteration 157/1000 | Loss: 0.00002475
Iteration 158/1000 | Loss: 0.00002474
Iteration 159/1000 | Loss: 0.00002474
Iteration 160/1000 | Loss: 0.00002474
Iteration 161/1000 | Loss: 0.00002474
Iteration 162/1000 | Loss: 0.00002474
Iteration 163/1000 | Loss: 0.00002474
Iteration 164/1000 | Loss: 0.00002474
Iteration 165/1000 | Loss: 0.00002474
Iteration 166/1000 | Loss: 0.00002474
Iteration 167/1000 | Loss: 0.00002474
Iteration 168/1000 | Loss: 0.00002474
Iteration 169/1000 | Loss: 0.00002474
Iteration 170/1000 | Loss: 0.00002474
Iteration 171/1000 | Loss: 0.00002474
Iteration 172/1000 | Loss: 0.00002474
Iteration 173/1000 | Loss: 0.00002474
Iteration 174/1000 | Loss: 0.00002474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.4739820219110698e-05, 2.4739820219110698e-05, 2.4739820219110698e-05, 2.4739820219110698e-05, 2.4739820219110698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4739820219110698e-05

Optimization complete. Final v2v error: 4.134067058563232 mm

Highest mean error: 4.863641262054443 mm for frame 233

Lowest mean error: 3.7304415702819824 mm for frame 211

Saving results

Total time: 123.5832359790802
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01112206
Iteration 2/25 | Loss: 0.00238507
Iteration 3/25 | Loss: 0.00210027
Iteration 4/25 | Loss: 0.00148188
Iteration 5/25 | Loss: 0.00111342
Iteration 6/25 | Loss: 0.00101476
Iteration 7/25 | Loss: 0.00091900
Iteration 8/25 | Loss: 0.00085544
Iteration 9/25 | Loss: 0.00092147
Iteration 10/25 | Loss: 0.00085721
Iteration 11/25 | Loss: 0.00087034
Iteration 12/25 | Loss: 0.00083315
Iteration 13/25 | Loss: 0.00082677
Iteration 14/25 | Loss: 0.00083517
Iteration 15/25 | Loss: 0.00085870
Iteration 16/25 | Loss: 0.00086914
Iteration 17/25 | Loss: 0.00079602
Iteration 18/25 | Loss: 0.00080407
Iteration 19/25 | Loss: 0.00080329
Iteration 20/25 | Loss: 0.00076979
Iteration 21/25 | Loss: 0.00076169
Iteration 22/25 | Loss: 0.00075822
Iteration 23/25 | Loss: 0.00075601
Iteration 24/25 | Loss: 0.00075456
Iteration 25/25 | Loss: 0.00078470

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45157003
Iteration 2/25 | Loss: 0.00170917
Iteration 3/25 | Loss: 0.00170917
Iteration 4/25 | Loss: 0.00170916
Iteration 5/25 | Loss: 0.00170916
Iteration 6/25 | Loss: 0.00170916
Iteration 7/25 | Loss: 0.00170916
Iteration 8/25 | Loss: 0.00170916
Iteration 9/25 | Loss: 0.00170916
Iteration 10/25 | Loss: 0.00170916
Iteration 11/25 | Loss: 0.00170916
Iteration 12/25 | Loss: 0.00170916
Iteration 13/25 | Loss: 0.00170916
Iteration 14/25 | Loss: 0.00170916
Iteration 15/25 | Loss: 0.00170916
Iteration 16/25 | Loss: 0.00170916
Iteration 17/25 | Loss: 0.00170916
Iteration 18/25 | Loss: 0.00170916
Iteration 19/25 | Loss: 0.00170916
Iteration 20/25 | Loss: 0.00170916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0017091614427044988, 0.0017091614427044988, 0.0017091614427044988, 0.0017091614427044988, 0.0017091614427044988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017091614427044988

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170916
Iteration 2/1000 | Loss: 0.00010094
Iteration 3/1000 | Loss: 0.00005554
Iteration 4/1000 | Loss: 0.00004413
Iteration 5/1000 | Loss: 0.00071724
Iteration 6/1000 | Loss: 0.00207550
Iteration 7/1000 | Loss: 0.00113832
Iteration 8/1000 | Loss: 0.00055351
Iteration 9/1000 | Loss: 0.00077272
Iteration 10/1000 | Loss: 0.00096290
Iteration 11/1000 | Loss: 0.00123016
Iteration 12/1000 | Loss: 0.00104065
Iteration 13/1000 | Loss: 0.00132867
Iteration 14/1000 | Loss: 0.00016544
Iteration 15/1000 | Loss: 0.00035719
Iteration 16/1000 | Loss: 0.00041839
Iteration 17/1000 | Loss: 0.00011297
Iteration 18/1000 | Loss: 0.00043599
Iteration 19/1000 | Loss: 0.00011602
Iteration 20/1000 | Loss: 0.00002940
Iteration 21/1000 | Loss: 0.00002514
Iteration 22/1000 | Loss: 0.00002279
Iteration 23/1000 | Loss: 0.00002162
Iteration 24/1000 | Loss: 0.00002091
Iteration 25/1000 | Loss: 0.00002031
Iteration 26/1000 | Loss: 0.00001986
Iteration 27/1000 | Loss: 0.00001949
Iteration 28/1000 | Loss: 0.00001919
Iteration 29/1000 | Loss: 0.00001893
Iteration 30/1000 | Loss: 0.00001888
Iteration 31/1000 | Loss: 0.00001875
Iteration 32/1000 | Loss: 0.00001868
Iteration 33/1000 | Loss: 0.00001859
Iteration 34/1000 | Loss: 0.00001859
Iteration 35/1000 | Loss: 0.00001856
Iteration 36/1000 | Loss: 0.00001852
Iteration 37/1000 | Loss: 0.00001849
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001848
Iteration 40/1000 | Loss: 0.00001848
Iteration 41/1000 | Loss: 0.00001848
Iteration 42/1000 | Loss: 0.00001847
Iteration 43/1000 | Loss: 0.00001847
Iteration 44/1000 | Loss: 0.00001847
Iteration 45/1000 | Loss: 0.00001847
Iteration 46/1000 | Loss: 0.00001847
Iteration 47/1000 | Loss: 0.00001847
Iteration 48/1000 | Loss: 0.00001847
Iteration 49/1000 | Loss: 0.00001846
Iteration 50/1000 | Loss: 0.00001846
Iteration 51/1000 | Loss: 0.00001846
Iteration 52/1000 | Loss: 0.00001846
Iteration 53/1000 | Loss: 0.00001846
Iteration 54/1000 | Loss: 0.00001846
Iteration 55/1000 | Loss: 0.00001846
Iteration 56/1000 | Loss: 0.00001845
Iteration 57/1000 | Loss: 0.00001845
Iteration 58/1000 | Loss: 0.00001845
Iteration 59/1000 | Loss: 0.00001845
Iteration 60/1000 | Loss: 0.00001844
Iteration 61/1000 | Loss: 0.00001844
Iteration 62/1000 | Loss: 0.00001844
Iteration 63/1000 | Loss: 0.00001844
Iteration 64/1000 | Loss: 0.00001844
Iteration 65/1000 | Loss: 0.00001844
Iteration 66/1000 | Loss: 0.00001844
Iteration 67/1000 | Loss: 0.00001844
Iteration 68/1000 | Loss: 0.00001844
Iteration 69/1000 | Loss: 0.00001844
Iteration 70/1000 | Loss: 0.00001843
Iteration 71/1000 | Loss: 0.00001843
Iteration 72/1000 | Loss: 0.00001843
Iteration 73/1000 | Loss: 0.00001843
Iteration 74/1000 | Loss: 0.00001842
Iteration 75/1000 | Loss: 0.00001842
Iteration 76/1000 | Loss: 0.00001842
Iteration 77/1000 | Loss: 0.00001842
Iteration 78/1000 | Loss: 0.00001842
Iteration 79/1000 | Loss: 0.00001842
Iteration 80/1000 | Loss: 0.00001842
Iteration 81/1000 | Loss: 0.00001842
Iteration 82/1000 | Loss: 0.00001842
Iteration 83/1000 | Loss: 0.00001842
Iteration 84/1000 | Loss: 0.00001842
Iteration 85/1000 | Loss: 0.00001842
Iteration 86/1000 | Loss: 0.00001841
Iteration 87/1000 | Loss: 0.00001841
Iteration 88/1000 | Loss: 0.00001841
Iteration 89/1000 | Loss: 0.00001841
Iteration 90/1000 | Loss: 0.00001841
Iteration 91/1000 | Loss: 0.00001840
Iteration 92/1000 | Loss: 0.00001840
Iteration 93/1000 | Loss: 0.00001840
Iteration 94/1000 | Loss: 0.00001840
Iteration 95/1000 | Loss: 0.00001840
Iteration 96/1000 | Loss: 0.00001839
Iteration 97/1000 | Loss: 0.00001839
Iteration 98/1000 | Loss: 0.00001839
Iteration 99/1000 | Loss: 0.00001839
Iteration 100/1000 | Loss: 0.00001839
Iteration 101/1000 | Loss: 0.00001839
Iteration 102/1000 | Loss: 0.00001839
Iteration 103/1000 | Loss: 0.00001839
Iteration 104/1000 | Loss: 0.00001838
Iteration 105/1000 | Loss: 0.00001838
Iteration 106/1000 | Loss: 0.00001838
Iteration 107/1000 | Loss: 0.00001838
Iteration 108/1000 | Loss: 0.00001837
Iteration 109/1000 | Loss: 0.00001837
Iteration 110/1000 | Loss: 0.00001836
Iteration 111/1000 | Loss: 0.00001836
Iteration 112/1000 | Loss: 0.00001836
Iteration 113/1000 | Loss: 0.00001836
Iteration 114/1000 | Loss: 0.00001836
Iteration 115/1000 | Loss: 0.00001836
Iteration 116/1000 | Loss: 0.00001836
Iteration 117/1000 | Loss: 0.00001836
Iteration 118/1000 | Loss: 0.00001835
Iteration 119/1000 | Loss: 0.00001835
Iteration 120/1000 | Loss: 0.00001835
Iteration 121/1000 | Loss: 0.00001835
Iteration 122/1000 | Loss: 0.00001835
Iteration 123/1000 | Loss: 0.00001835
Iteration 124/1000 | Loss: 0.00001835
Iteration 125/1000 | Loss: 0.00001835
Iteration 126/1000 | Loss: 0.00001835
Iteration 127/1000 | Loss: 0.00001835
Iteration 128/1000 | Loss: 0.00001835
Iteration 129/1000 | Loss: 0.00001834
Iteration 130/1000 | Loss: 0.00001834
Iteration 131/1000 | Loss: 0.00001834
Iteration 132/1000 | Loss: 0.00001834
Iteration 133/1000 | Loss: 0.00001834
Iteration 134/1000 | Loss: 0.00001834
Iteration 135/1000 | Loss: 0.00001833
Iteration 136/1000 | Loss: 0.00001833
Iteration 137/1000 | Loss: 0.00001833
Iteration 138/1000 | Loss: 0.00001833
Iteration 139/1000 | Loss: 0.00001833
Iteration 140/1000 | Loss: 0.00001833
Iteration 141/1000 | Loss: 0.00001833
Iteration 142/1000 | Loss: 0.00001833
Iteration 143/1000 | Loss: 0.00001833
Iteration 144/1000 | Loss: 0.00001833
Iteration 145/1000 | Loss: 0.00001833
Iteration 146/1000 | Loss: 0.00001833
Iteration 147/1000 | Loss: 0.00001833
Iteration 148/1000 | Loss: 0.00001833
Iteration 149/1000 | Loss: 0.00001833
Iteration 150/1000 | Loss: 0.00001833
Iteration 151/1000 | Loss: 0.00001833
Iteration 152/1000 | Loss: 0.00001832
Iteration 153/1000 | Loss: 0.00001832
Iteration 154/1000 | Loss: 0.00001832
Iteration 155/1000 | Loss: 0.00001832
Iteration 156/1000 | Loss: 0.00001832
Iteration 157/1000 | Loss: 0.00001832
Iteration 158/1000 | Loss: 0.00001832
Iteration 159/1000 | Loss: 0.00001832
Iteration 160/1000 | Loss: 0.00001832
Iteration 161/1000 | Loss: 0.00001832
Iteration 162/1000 | Loss: 0.00001832
Iteration 163/1000 | Loss: 0.00001832
Iteration 164/1000 | Loss: 0.00001832
Iteration 165/1000 | Loss: 0.00001832
Iteration 166/1000 | Loss: 0.00001832
Iteration 167/1000 | Loss: 0.00001832
Iteration 168/1000 | Loss: 0.00001832
Iteration 169/1000 | Loss: 0.00001832
Iteration 170/1000 | Loss: 0.00001832
Iteration 171/1000 | Loss: 0.00001832
Iteration 172/1000 | Loss: 0.00001832
Iteration 173/1000 | Loss: 0.00001831
Iteration 174/1000 | Loss: 0.00001831
Iteration 175/1000 | Loss: 0.00001831
Iteration 176/1000 | Loss: 0.00001831
Iteration 177/1000 | Loss: 0.00001831
Iteration 178/1000 | Loss: 0.00001831
Iteration 179/1000 | Loss: 0.00001831
Iteration 180/1000 | Loss: 0.00001831
Iteration 181/1000 | Loss: 0.00001831
Iteration 182/1000 | Loss: 0.00001831
Iteration 183/1000 | Loss: 0.00001831
Iteration 184/1000 | Loss: 0.00001831
Iteration 185/1000 | Loss: 0.00001831
Iteration 186/1000 | Loss: 0.00001831
Iteration 187/1000 | Loss: 0.00001831
Iteration 188/1000 | Loss: 0.00001831
Iteration 189/1000 | Loss: 0.00001831
Iteration 190/1000 | Loss: 0.00001831
Iteration 191/1000 | Loss: 0.00001831
Iteration 192/1000 | Loss: 0.00001831
Iteration 193/1000 | Loss: 0.00001831
Iteration 194/1000 | Loss: 0.00001831
Iteration 195/1000 | Loss: 0.00001831
Iteration 196/1000 | Loss: 0.00001831
Iteration 197/1000 | Loss: 0.00001831
Iteration 198/1000 | Loss: 0.00001831
Iteration 199/1000 | Loss: 0.00001831
Iteration 200/1000 | Loss: 0.00001831
Iteration 201/1000 | Loss: 0.00001831
Iteration 202/1000 | Loss: 0.00001831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.8305148842046037e-05, 1.8305148842046037e-05, 1.8305148842046037e-05, 1.8305148842046037e-05, 1.8305148842046037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8305148842046037e-05

Optimization complete. Final v2v error: 3.5884692668914795 mm

Highest mean error: 4.980894088745117 mm for frame 66

Lowest mean error: 3.108950614929199 mm for frame 154

Saving results

Total time: 105.54713845252991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00610414
Iteration 2/25 | Loss: 0.00105304
Iteration 3/25 | Loss: 0.00084332
Iteration 4/25 | Loss: 0.00080523
Iteration 5/25 | Loss: 0.00079271
Iteration 6/25 | Loss: 0.00079012
Iteration 7/25 | Loss: 0.00078930
Iteration 8/25 | Loss: 0.00078920
Iteration 9/25 | Loss: 0.00078920
Iteration 10/25 | Loss: 0.00078920
Iteration 11/25 | Loss: 0.00078920
Iteration 12/25 | Loss: 0.00078920
Iteration 13/25 | Loss: 0.00078920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007891983259469271, 0.0007891983259469271, 0.0007891983259469271, 0.0007891983259469271, 0.0007891983259469271]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007891983259469271

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57106340
Iteration 2/25 | Loss: 0.00028309
Iteration 3/25 | Loss: 0.00028308
Iteration 4/25 | Loss: 0.00028308
Iteration 5/25 | Loss: 0.00028308
Iteration 6/25 | Loss: 0.00028308
Iteration 7/25 | Loss: 0.00028308
Iteration 8/25 | Loss: 0.00028308
Iteration 9/25 | Loss: 0.00028308
Iteration 10/25 | Loss: 0.00028308
Iteration 11/25 | Loss: 0.00028308
Iteration 12/25 | Loss: 0.00028308
Iteration 13/25 | Loss: 0.00028308
Iteration 14/25 | Loss: 0.00028308
Iteration 15/25 | Loss: 0.00028308
Iteration 16/25 | Loss: 0.00028308
Iteration 17/25 | Loss: 0.00028308
Iteration 18/25 | Loss: 0.00028308
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00028307928005233407, 0.00028307928005233407, 0.00028307928005233407, 0.00028307928005233407, 0.00028307928005233407]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028307928005233407

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028308
Iteration 2/1000 | Loss: 0.00004261
Iteration 3/1000 | Loss: 0.00003020
Iteration 4/1000 | Loss: 0.00002668
Iteration 5/1000 | Loss: 0.00002530
Iteration 6/1000 | Loss: 0.00002411
Iteration 7/1000 | Loss: 0.00002346
Iteration 8/1000 | Loss: 0.00002298
Iteration 9/1000 | Loss: 0.00002258
Iteration 10/1000 | Loss: 0.00002229
Iteration 11/1000 | Loss: 0.00002202
Iteration 12/1000 | Loss: 0.00002185
Iteration 13/1000 | Loss: 0.00002176
Iteration 14/1000 | Loss: 0.00002170
Iteration 15/1000 | Loss: 0.00002168
Iteration 16/1000 | Loss: 0.00002168
Iteration 17/1000 | Loss: 0.00002167
Iteration 18/1000 | Loss: 0.00002165
Iteration 19/1000 | Loss: 0.00002160
Iteration 20/1000 | Loss: 0.00002159
Iteration 21/1000 | Loss: 0.00002158
Iteration 22/1000 | Loss: 0.00002157
Iteration 23/1000 | Loss: 0.00002157
Iteration 24/1000 | Loss: 0.00002150
Iteration 25/1000 | Loss: 0.00002149
Iteration 26/1000 | Loss: 0.00002148
Iteration 27/1000 | Loss: 0.00002147
Iteration 28/1000 | Loss: 0.00002146
Iteration 29/1000 | Loss: 0.00002143
Iteration 30/1000 | Loss: 0.00002143
Iteration 31/1000 | Loss: 0.00002143
Iteration 32/1000 | Loss: 0.00002142
Iteration 33/1000 | Loss: 0.00002142
Iteration 34/1000 | Loss: 0.00002141
Iteration 35/1000 | Loss: 0.00002141
Iteration 36/1000 | Loss: 0.00002141
Iteration 37/1000 | Loss: 0.00002140
Iteration 38/1000 | Loss: 0.00002139
Iteration 39/1000 | Loss: 0.00002139
Iteration 40/1000 | Loss: 0.00002139
Iteration 41/1000 | Loss: 0.00002138
Iteration 42/1000 | Loss: 0.00002138
Iteration 43/1000 | Loss: 0.00002138
Iteration 44/1000 | Loss: 0.00002138
Iteration 45/1000 | Loss: 0.00002137
Iteration 46/1000 | Loss: 0.00002137
Iteration 47/1000 | Loss: 0.00002137
Iteration 48/1000 | Loss: 0.00002137
Iteration 49/1000 | Loss: 0.00002137
Iteration 50/1000 | Loss: 0.00002137
Iteration 51/1000 | Loss: 0.00002137
Iteration 52/1000 | Loss: 0.00002137
Iteration 53/1000 | Loss: 0.00002136
Iteration 54/1000 | Loss: 0.00002136
Iteration 55/1000 | Loss: 0.00002135
Iteration 56/1000 | Loss: 0.00002135
Iteration 57/1000 | Loss: 0.00002135
Iteration 58/1000 | Loss: 0.00002134
Iteration 59/1000 | Loss: 0.00002134
Iteration 60/1000 | Loss: 0.00002134
Iteration 61/1000 | Loss: 0.00002133
Iteration 62/1000 | Loss: 0.00002133
Iteration 63/1000 | Loss: 0.00002133
Iteration 64/1000 | Loss: 0.00002133
Iteration 65/1000 | Loss: 0.00002133
Iteration 66/1000 | Loss: 0.00002133
Iteration 67/1000 | Loss: 0.00002133
Iteration 68/1000 | Loss: 0.00002133
Iteration 69/1000 | Loss: 0.00002133
Iteration 70/1000 | Loss: 0.00002133
Iteration 71/1000 | Loss: 0.00002132
Iteration 72/1000 | Loss: 0.00002132
Iteration 73/1000 | Loss: 0.00002132
Iteration 74/1000 | Loss: 0.00002131
Iteration 75/1000 | Loss: 0.00002131
Iteration 76/1000 | Loss: 0.00002131
Iteration 77/1000 | Loss: 0.00002131
Iteration 78/1000 | Loss: 0.00002131
Iteration 79/1000 | Loss: 0.00002131
Iteration 80/1000 | Loss: 0.00002131
Iteration 81/1000 | Loss: 0.00002131
Iteration 82/1000 | Loss: 0.00002131
Iteration 83/1000 | Loss: 0.00002130
Iteration 84/1000 | Loss: 0.00002130
Iteration 85/1000 | Loss: 0.00002130
Iteration 86/1000 | Loss: 0.00002130
Iteration 87/1000 | Loss: 0.00002130
Iteration 88/1000 | Loss: 0.00002129
Iteration 89/1000 | Loss: 0.00002129
Iteration 90/1000 | Loss: 0.00002129
Iteration 91/1000 | Loss: 0.00002128
Iteration 92/1000 | Loss: 0.00002128
Iteration 93/1000 | Loss: 0.00002128
Iteration 94/1000 | Loss: 0.00002128
Iteration 95/1000 | Loss: 0.00002127
Iteration 96/1000 | Loss: 0.00002127
Iteration 97/1000 | Loss: 0.00002127
Iteration 98/1000 | Loss: 0.00002126
Iteration 99/1000 | Loss: 0.00002126
Iteration 100/1000 | Loss: 0.00002126
Iteration 101/1000 | Loss: 0.00002126
Iteration 102/1000 | Loss: 0.00002126
Iteration 103/1000 | Loss: 0.00002126
Iteration 104/1000 | Loss: 0.00002126
Iteration 105/1000 | Loss: 0.00002126
Iteration 106/1000 | Loss: 0.00002125
Iteration 107/1000 | Loss: 0.00002125
Iteration 108/1000 | Loss: 0.00002125
Iteration 109/1000 | Loss: 0.00002125
Iteration 110/1000 | Loss: 0.00002125
Iteration 111/1000 | Loss: 0.00002125
Iteration 112/1000 | Loss: 0.00002125
Iteration 113/1000 | Loss: 0.00002125
Iteration 114/1000 | Loss: 0.00002124
Iteration 115/1000 | Loss: 0.00002124
Iteration 116/1000 | Loss: 0.00002124
Iteration 117/1000 | Loss: 0.00002124
Iteration 118/1000 | Loss: 0.00002124
Iteration 119/1000 | Loss: 0.00002124
Iteration 120/1000 | Loss: 0.00002124
Iteration 121/1000 | Loss: 0.00002124
Iteration 122/1000 | Loss: 0.00002124
Iteration 123/1000 | Loss: 0.00002124
Iteration 124/1000 | Loss: 0.00002123
Iteration 125/1000 | Loss: 0.00002123
Iteration 126/1000 | Loss: 0.00002123
Iteration 127/1000 | Loss: 0.00002123
Iteration 128/1000 | Loss: 0.00002123
Iteration 129/1000 | Loss: 0.00002123
Iteration 130/1000 | Loss: 0.00002123
Iteration 131/1000 | Loss: 0.00002123
Iteration 132/1000 | Loss: 0.00002123
Iteration 133/1000 | Loss: 0.00002123
Iteration 134/1000 | Loss: 0.00002123
Iteration 135/1000 | Loss: 0.00002123
Iteration 136/1000 | Loss: 0.00002123
Iteration 137/1000 | Loss: 0.00002122
Iteration 138/1000 | Loss: 0.00002122
Iteration 139/1000 | Loss: 0.00002122
Iteration 140/1000 | Loss: 0.00002122
Iteration 141/1000 | Loss: 0.00002122
Iteration 142/1000 | Loss: 0.00002122
Iteration 143/1000 | Loss: 0.00002122
Iteration 144/1000 | Loss: 0.00002122
Iteration 145/1000 | Loss: 0.00002122
Iteration 146/1000 | Loss: 0.00002122
Iteration 147/1000 | Loss: 0.00002122
Iteration 148/1000 | Loss: 0.00002122
Iteration 149/1000 | Loss: 0.00002122
Iteration 150/1000 | Loss: 0.00002121
Iteration 151/1000 | Loss: 0.00002121
Iteration 152/1000 | Loss: 0.00002121
Iteration 153/1000 | Loss: 0.00002121
Iteration 154/1000 | Loss: 0.00002121
Iteration 155/1000 | Loss: 0.00002121
Iteration 156/1000 | Loss: 0.00002121
Iteration 157/1000 | Loss: 0.00002121
Iteration 158/1000 | Loss: 0.00002121
Iteration 159/1000 | Loss: 0.00002121
Iteration 160/1000 | Loss: 0.00002121
Iteration 161/1000 | Loss: 0.00002121
Iteration 162/1000 | Loss: 0.00002121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.1208881662460044e-05, 2.1208881662460044e-05, 2.1208881662460044e-05, 2.1208881662460044e-05, 2.1208881662460044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1208881662460044e-05

Optimization complete. Final v2v error: 3.769623041152954 mm

Highest mean error: 4.950982093811035 mm for frame 37

Lowest mean error: 2.8693315982818604 mm for frame 221

Saving results

Total time: 47.98077201843262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382282
Iteration 2/25 | Loss: 0.00088879
Iteration 3/25 | Loss: 0.00074898
Iteration 4/25 | Loss: 0.00072837
Iteration 5/25 | Loss: 0.00072326
Iteration 6/25 | Loss: 0.00072168
Iteration 7/25 | Loss: 0.00072134
Iteration 8/25 | Loss: 0.00072134
Iteration 9/25 | Loss: 0.00072134
Iteration 10/25 | Loss: 0.00072134
Iteration 11/25 | Loss: 0.00072134
Iteration 12/25 | Loss: 0.00072134
Iteration 13/25 | Loss: 0.00072134
Iteration 14/25 | Loss: 0.00072134
Iteration 15/25 | Loss: 0.00072134
Iteration 16/25 | Loss: 0.00072134
Iteration 17/25 | Loss: 0.00072134
Iteration 18/25 | Loss: 0.00072134
Iteration 19/25 | Loss: 0.00072134
Iteration 20/25 | Loss: 0.00072134
Iteration 21/25 | Loss: 0.00072134
Iteration 22/25 | Loss: 0.00072134
Iteration 23/25 | Loss: 0.00072134
Iteration 24/25 | Loss: 0.00072134
Iteration 25/25 | Loss: 0.00072134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46849573
Iteration 2/25 | Loss: 0.00027708
Iteration 3/25 | Loss: 0.00027708
Iteration 4/25 | Loss: 0.00027708
Iteration 5/25 | Loss: 0.00027708
Iteration 6/25 | Loss: 0.00027708
Iteration 7/25 | Loss: 0.00027708
Iteration 8/25 | Loss: 0.00027708
Iteration 9/25 | Loss: 0.00027708
Iteration 10/25 | Loss: 0.00027708
Iteration 11/25 | Loss: 0.00027708
Iteration 12/25 | Loss: 0.00027708
Iteration 13/25 | Loss: 0.00027708
Iteration 14/25 | Loss: 0.00027708
Iteration 15/25 | Loss: 0.00027708
Iteration 16/25 | Loss: 0.00027708
Iteration 17/25 | Loss: 0.00027708
Iteration 18/25 | Loss: 0.00027708
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002770754508674145, 0.0002770754508674145, 0.0002770754508674145, 0.0002770754508674145, 0.0002770754508674145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002770754508674145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027708
Iteration 2/1000 | Loss: 0.00002464
Iteration 3/1000 | Loss: 0.00001652
Iteration 4/1000 | Loss: 0.00001491
Iteration 5/1000 | Loss: 0.00001383
Iteration 6/1000 | Loss: 0.00001334
Iteration 7/1000 | Loss: 0.00001310
Iteration 8/1000 | Loss: 0.00001296
Iteration 9/1000 | Loss: 0.00001294
Iteration 10/1000 | Loss: 0.00001289
Iteration 11/1000 | Loss: 0.00001279
Iteration 12/1000 | Loss: 0.00001274
Iteration 13/1000 | Loss: 0.00001274
Iteration 14/1000 | Loss: 0.00001273
Iteration 15/1000 | Loss: 0.00001272
Iteration 16/1000 | Loss: 0.00001271
Iteration 17/1000 | Loss: 0.00001270
Iteration 18/1000 | Loss: 0.00001269
Iteration 19/1000 | Loss: 0.00001267
Iteration 20/1000 | Loss: 0.00001263
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001261
Iteration 24/1000 | Loss: 0.00001260
Iteration 25/1000 | Loss: 0.00001259
Iteration 26/1000 | Loss: 0.00001258
Iteration 27/1000 | Loss: 0.00001258
Iteration 28/1000 | Loss: 0.00001257
Iteration 29/1000 | Loss: 0.00001257
Iteration 30/1000 | Loss: 0.00001256
Iteration 31/1000 | Loss: 0.00001256
Iteration 32/1000 | Loss: 0.00001256
Iteration 33/1000 | Loss: 0.00001255
Iteration 34/1000 | Loss: 0.00001255
Iteration 35/1000 | Loss: 0.00001255
Iteration 36/1000 | Loss: 0.00001255
Iteration 37/1000 | Loss: 0.00001254
Iteration 38/1000 | Loss: 0.00001254
Iteration 39/1000 | Loss: 0.00001254
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001253
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001252
Iteration 44/1000 | Loss: 0.00001252
Iteration 45/1000 | Loss: 0.00001251
Iteration 46/1000 | Loss: 0.00001251
Iteration 47/1000 | Loss: 0.00001251
Iteration 48/1000 | Loss: 0.00001250
Iteration 49/1000 | Loss: 0.00001250
Iteration 50/1000 | Loss: 0.00001250
Iteration 51/1000 | Loss: 0.00001249
Iteration 52/1000 | Loss: 0.00001249
Iteration 53/1000 | Loss: 0.00001249
Iteration 54/1000 | Loss: 0.00001249
Iteration 55/1000 | Loss: 0.00001249
Iteration 56/1000 | Loss: 0.00001249
Iteration 57/1000 | Loss: 0.00001249
Iteration 58/1000 | Loss: 0.00001248
Iteration 59/1000 | Loss: 0.00001248
Iteration 60/1000 | Loss: 0.00001248
Iteration 61/1000 | Loss: 0.00001248
Iteration 62/1000 | Loss: 0.00001248
Iteration 63/1000 | Loss: 0.00001248
Iteration 64/1000 | Loss: 0.00001248
Iteration 65/1000 | Loss: 0.00001248
Iteration 66/1000 | Loss: 0.00001247
Iteration 67/1000 | Loss: 0.00001247
Iteration 68/1000 | Loss: 0.00001247
Iteration 69/1000 | Loss: 0.00001246
Iteration 70/1000 | Loss: 0.00001246
Iteration 71/1000 | Loss: 0.00001246
Iteration 72/1000 | Loss: 0.00001246
Iteration 73/1000 | Loss: 0.00001246
Iteration 74/1000 | Loss: 0.00001245
Iteration 75/1000 | Loss: 0.00001245
Iteration 76/1000 | Loss: 0.00001245
Iteration 77/1000 | Loss: 0.00001245
Iteration 78/1000 | Loss: 0.00001245
Iteration 79/1000 | Loss: 0.00001245
Iteration 80/1000 | Loss: 0.00001245
Iteration 81/1000 | Loss: 0.00001245
Iteration 82/1000 | Loss: 0.00001245
Iteration 83/1000 | Loss: 0.00001245
Iteration 84/1000 | Loss: 0.00001244
Iteration 85/1000 | Loss: 0.00001244
Iteration 86/1000 | Loss: 0.00001244
Iteration 87/1000 | Loss: 0.00001244
Iteration 88/1000 | Loss: 0.00001244
Iteration 89/1000 | Loss: 0.00001244
Iteration 90/1000 | Loss: 0.00001244
Iteration 91/1000 | Loss: 0.00001244
Iteration 92/1000 | Loss: 0.00001244
Iteration 93/1000 | Loss: 0.00001244
Iteration 94/1000 | Loss: 0.00001243
Iteration 95/1000 | Loss: 0.00001243
Iteration 96/1000 | Loss: 0.00001243
Iteration 97/1000 | Loss: 0.00001243
Iteration 98/1000 | Loss: 0.00001243
Iteration 99/1000 | Loss: 0.00001243
Iteration 100/1000 | Loss: 0.00001243
Iteration 101/1000 | Loss: 0.00001243
Iteration 102/1000 | Loss: 0.00001243
Iteration 103/1000 | Loss: 0.00001242
Iteration 104/1000 | Loss: 0.00001242
Iteration 105/1000 | Loss: 0.00001242
Iteration 106/1000 | Loss: 0.00001242
Iteration 107/1000 | Loss: 0.00001242
Iteration 108/1000 | Loss: 0.00001242
Iteration 109/1000 | Loss: 0.00001242
Iteration 110/1000 | Loss: 0.00001242
Iteration 111/1000 | Loss: 0.00001242
Iteration 112/1000 | Loss: 0.00001242
Iteration 113/1000 | Loss: 0.00001242
Iteration 114/1000 | Loss: 0.00001241
Iteration 115/1000 | Loss: 0.00001241
Iteration 116/1000 | Loss: 0.00001241
Iteration 117/1000 | Loss: 0.00001241
Iteration 118/1000 | Loss: 0.00001241
Iteration 119/1000 | Loss: 0.00001241
Iteration 120/1000 | Loss: 0.00001241
Iteration 121/1000 | Loss: 0.00001241
Iteration 122/1000 | Loss: 0.00001241
Iteration 123/1000 | Loss: 0.00001241
Iteration 124/1000 | Loss: 0.00001241
Iteration 125/1000 | Loss: 0.00001241
Iteration 126/1000 | Loss: 0.00001241
Iteration 127/1000 | Loss: 0.00001241
Iteration 128/1000 | Loss: 0.00001241
Iteration 129/1000 | Loss: 0.00001241
Iteration 130/1000 | Loss: 0.00001240
Iteration 131/1000 | Loss: 0.00001240
Iteration 132/1000 | Loss: 0.00001240
Iteration 133/1000 | Loss: 0.00001240
Iteration 134/1000 | Loss: 0.00001240
Iteration 135/1000 | Loss: 0.00001240
Iteration 136/1000 | Loss: 0.00001240
Iteration 137/1000 | Loss: 0.00001240
Iteration 138/1000 | Loss: 0.00001240
Iteration 139/1000 | Loss: 0.00001240
Iteration 140/1000 | Loss: 0.00001239
Iteration 141/1000 | Loss: 0.00001239
Iteration 142/1000 | Loss: 0.00001239
Iteration 143/1000 | Loss: 0.00001239
Iteration 144/1000 | Loss: 0.00001239
Iteration 145/1000 | Loss: 0.00001239
Iteration 146/1000 | Loss: 0.00001239
Iteration 147/1000 | Loss: 0.00001239
Iteration 148/1000 | Loss: 0.00001239
Iteration 149/1000 | Loss: 0.00001239
Iteration 150/1000 | Loss: 0.00001239
Iteration 151/1000 | Loss: 0.00001239
Iteration 152/1000 | Loss: 0.00001238
Iteration 153/1000 | Loss: 0.00001238
Iteration 154/1000 | Loss: 0.00001238
Iteration 155/1000 | Loss: 0.00001238
Iteration 156/1000 | Loss: 0.00001238
Iteration 157/1000 | Loss: 0.00001238
Iteration 158/1000 | Loss: 0.00001238
Iteration 159/1000 | Loss: 0.00001238
Iteration 160/1000 | Loss: 0.00001238
Iteration 161/1000 | Loss: 0.00001238
Iteration 162/1000 | Loss: 0.00001238
Iteration 163/1000 | Loss: 0.00001238
Iteration 164/1000 | Loss: 0.00001238
Iteration 165/1000 | Loss: 0.00001238
Iteration 166/1000 | Loss: 0.00001238
Iteration 167/1000 | Loss: 0.00001238
Iteration 168/1000 | Loss: 0.00001238
Iteration 169/1000 | Loss: 0.00001237
Iteration 170/1000 | Loss: 0.00001237
Iteration 171/1000 | Loss: 0.00001237
Iteration 172/1000 | Loss: 0.00001237
Iteration 173/1000 | Loss: 0.00001237
Iteration 174/1000 | Loss: 0.00001237
Iteration 175/1000 | Loss: 0.00001237
Iteration 176/1000 | Loss: 0.00001237
Iteration 177/1000 | Loss: 0.00001237
Iteration 178/1000 | Loss: 0.00001237
Iteration 179/1000 | Loss: 0.00001237
Iteration 180/1000 | Loss: 0.00001237
Iteration 181/1000 | Loss: 0.00001237
Iteration 182/1000 | Loss: 0.00001237
Iteration 183/1000 | Loss: 0.00001237
Iteration 184/1000 | Loss: 0.00001237
Iteration 185/1000 | Loss: 0.00001237
Iteration 186/1000 | Loss: 0.00001237
Iteration 187/1000 | Loss: 0.00001237
Iteration 188/1000 | Loss: 0.00001236
Iteration 189/1000 | Loss: 0.00001236
Iteration 190/1000 | Loss: 0.00001236
Iteration 191/1000 | Loss: 0.00001236
Iteration 192/1000 | Loss: 0.00001236
Iteration 193/1000 | Loss: 0.00001236
Iteration 194/1000 | Loss: 0.00001236
Iteration 195/1000 | Loss: 0.00001236
Iteration 196/1000 | Loss: 0.00001236
Iteration 197/1000 | Loss: 0.00001236
Iteration 198/1000 | Loss: 0.00001236
Iteration 199/1000 | Loss: 0.00001236
Iteration 200/1000 | Loss: 0.00001236
Iteration 201/1000 | Loss: 0.00001236
Iteration 202/1000 | Loss: 0.00001236
Iteration 203/1000 | Loss: 0.00001236
Iteration 204/1000 | Loss: 0.00001236
Iteration 205/1000 | Loss: 0.00001236
Iteration 206/1000 | Loss: 0.00001236
Iteration 207/1000 | Loss: 0.00001236
Iteration 208/1000 | Loss: 0.00001236
Iteration 209/1000 | Loss: 0.00001236
Iteration 210/1000 | Loss: 0.00001236
Iteration 211/1000 | Loss: 0.00001236
Iteration 212/1000 | Loss: 0.00001236
Iteration 213/1000 | Loss: 0.00001236
Iteration 214/1000 | Loss: 0.00001236
Iteration 215/1000 | Loss: 0.00001236
Iteration 216/1000 | Loss: 0.00001236
Iteration 217/1000 | Loss: 0.00001236
Iteration 218/1000 | Loss: 0.00001236
Iteration 219/1000 | Loss: 0.00001236
Iteration 220/1000 | Loss: 0.00001236
Iteration 221/1000 | Loss: 0.00001236
Iteration 222/1000 | Loss: 0.00001236
Iteration 223/1000 | Loss: 0.00001236
Iteration 224/1000 | Loss: 0.00001236
Iteration 225/1000 | Loss: 0.00001236
Iteration 226/1000 | Loss: 0.00001236
Iteration 227/1000 | Loss: 0.00001236
Iteration 228/1000 | Loss: 0.00001236
Iteration 229/1000 | Loss: 0.00001236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.2359220818325412e-05, 1.2359220818325412e-05, 1.2359220818325412e-05, 1.2359220818325412e-05, 1.2359220818325412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2359220818325412e-05

Optimization complete. Final v2v error: 2.8962037563323975 mm

Highest mean error: 3.4774153232574463 mm for frame 73

Lowest mean error: 2.423881769180298 mm for frame 53

Saving results

Total time: 38.04923057556152
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855089
Iteration 2/25 | Loss: 0.00087105
Iteration 3/25 | Loss: 0.00074754
Iteration 4/25 | Loss: 0.00071762
Iteration 5/25 | Loss: 0.00070566
Iteration 6/25 | Loss: 0.00070403
Iteration 7/25 | Loss: 0.00070403
Iteration 8/25 | Loss: 0.00070403
Iteration 9/25 | Loss: 0.00070403
Iteration 10/25 | Loss: 0.00070403
Iteration 11/25 | Loss: 0.00070403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007040266063995659, 0.0007040266063995659, 0.0007040266063995659, 0.0007040266063995659, 0.0007040266063995659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007040266063995659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.10464764
Iteration 2/25 | Loss: 0.00023425
Iteration 3/25 | Loss: 0.00023422
Iteration 4/25 | Loss: 0.00023422
Iteration 5/25 | Loss: 0.00023422
Iteration 6/25 | Loss: 0.00023422
Iteration 7/25 | Loss: 0.00023422
Iteration 8/25 | Loss: 0.00023422
Iteration 9/25 | Loss: 0.00023422
Iteration 10/25 | Loss: 0.00023422
Iteration 11/25 | Loss: 0.00023422
Iteration 12/25 | Loss: 0.00023421
Iteration 13/25 | Loss: 0.00023421
Iteration 14/25 | Loss: 0.00023421
Iteration 15/25 | Loss: 0.00023421
Iteration 16/25 | Loss: 0.00023421
Iteration 17/25 | Loss: 0.00023421
Iteration 18/25 | Loss: 0.00023421
Iteration 19/25 | Loss: 0.00023421
Iteration 20/25 | Loss: 0.00023421
Iteration 21/25 | Loss: 0.00023421
Iteration 22/25 | Loss: 0.00023421
Iteration 23/25 | Loss: 0.00023421
Iteration 24/25 | Loss: 0.00023421
Iteration 25/25 | Loss: 0.00023421

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023421
Iteration 2/1000 | Loss: 0.00002928
Iteration 3/1000 | Loss: 0.00002056
Iteration 4/1000 | Loss: 0.00001940
Iteration 5/1000 | Loss: 0.00001831
Iteration 6/1000 | Loss: 0.00001785
Iteration 7/1000 | Loss: 0.00001722
Iteration 8/1000 | Loss: 0.00001689
Iteration 9/1000 | Loss: 0.00001659
Iteration 10/1000 | Loss: 0.00001642
Iteration 11/1000 | Loss: 0.00001634
Iteration 12/1000 | Loss: 0.00001632
Iteration 13/1000 | Loss: 0.00001618
Iteration 14/1000 | Loss: 0.00001616
Iteration 15/1000 | Loss: 0.00001610
Iteration 16/1000 | Loss: 0.00001608
Iteration 17/1000 | Loss: 0.00001608
Iteration 18/1000 | Loss: 0.00001608
Iteration 19/1000 | Loss: 0.00001606
Iteration 20/1000 | Loss: 0.00001606
Iteration 21/1000 | Loss: 0.00001605
Iteration 22/1000 | Loss: 0.00001605
Iteration 23/1000 | Loss: 0.00001604
Iteration 24/1000 | Loss: 0.00001604
Iteration 25/1000 | Loss: 0.00001603
Iteration 26/1000 | Loss: 0.00001602
Iteration 27/1000 | Loss: 0.00001602
Iteration 28/1000 | Loss: 0.00001602
Iteration 29/1000 | Loss: 0.00001600
Iteration 30/1000 | Loss: 0.00001599
Iteration 31/1000 | Loss: 0.00001599
Iteration 32/1000 | Loss: 0.00001597
Iteration 33/1000 | Loss: 0.00001597
Iteration 34/1000 | Loss: 0.00001597
Iteration 35/1000 | Loss: 0.00001596
Iteration 36/1000 | Loss: 0.00001594
Iteration 37/1000 | Loss: 0.00001591
Iteration 38/1000 | Loss: 0.00001585
Iteration 39/1000 | Loss: 0.00001585
Iteration 40/1000 | Loss: 0.00001583
Iteration 41/1000 | Loss: 0.00001579
Iteration 42/1000 | Loss: 0.00001579
Iteration 43/1000 | Loss: 0.00001579
Iteration 44/1000 | Loss: 0.00001579
Iteration 45/1000 | Loss: 0.00001579
Iteration 46/1000 | Loss: 0.00001579
Iteration 47/1000 | Loss: 0.00001579
Iteration 48/1000 | Loss: 0.00001579
Iteration 49/1000 | Loss: 0.00001579
Iteration 50/1000 | Loss: 0.00001578
Iteration 51/1000 | Loss: 0.00001578
Iteration 52/1000 | Loss: 0.00001577
Iteration 53/1000 | Loss: 0.00001577
Iteration 54/1000 | Loss: 0.00001576
Iteration 55/1000 | Loss: 0.00001576
Iteration 56/1000 | Loss: 0.00001576
Iteration 57/1000 | Loss: 0.00001576
Iteration 58/1000 | Loss: 0.00001576
Iteration 59/1000 | Loss: 0.00001575
Iteration 60/1000 | Loss: 0.00001575
Iteration 61/1000 | Loss: 0.00001575
Iteration 62/1000 | Loss: 0.00001575
Iteration 63/1000 | Loss: 0.00001575
Iteration 64/1000 | Loss: 0.00001575
Iteration 65/1000 | Loss: 0.00001575
Iteration 66/1000 | Loss: 0.00001574
Iteration 67/1000 | Loss: 0.00001574
Iteration 68/1000 | Loss: 0.00001573
Iteration 69/1000 | Loss: 0.00001573
Iteration 70/1000 | Loss: 0.00001573
Iteration 71/1000 | Loss: 0.00001573
Iteration 72/1000 | Loss: 0.00001573
Iteration 73/1000 | Loss: 0.00001573
Iteration 74/1000 | Loss: 0.00001573
Iteration 75/1000 | Loss: 0.00001573
Iteration 76/1000 | Loss: 0.00001573
Iteration 77/1000 | Loss: 0.00001573
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001572
Iteration 80/1000 | Loss: 0.00001572
Iteration 81/1000 | Loss: 0.00001571
Iteration 82/1000 | Loss: 0.00001571
Iteration 83/1000 | Loss: 0.00001571
Iteration 84/1000 | Loss: 0.00001571
Iteration 85/1000 | Loss: 0.00001571
Iteration 86/1000 | Loss: 0.00001571
Iteration 87/1000 | Loss: 0.00001570
Iteration 88/1000 | Loss: 0.00001570
Iteration 89/1000 | Loss: 0.00001570
Iteration 90/1000 | Loss: 0.00001570
Iteration 91/1000 | Loss: 0.00001570
Iteration 92/1000 | Loss: 0.00001570
Iteration 93/1000 | Loss: 0.00001570
Iteration 94/1000 | Loss: 0.00001570
Iteration 95/1000 | Loss: 0.00001570
Iteration 96/1000 | Loss: 0.00001570
Iteration 97/1000 | Loss: 0.00001570
Iteration 98/1000 | Loss: 0.00001570
Iteration 99/1000 | Loss: 0.00001570
Iteration 100/1000 | Loss: 0.00001569
Iteration 101/1000 | Loss: 0.00001569
Iteration 102/1000 | Loss: 0.00001569
Iteration 103/1000 | Loss: 0.00001569
Iteration 104/1000 | Loss: 0.00001569
Iteration 105/1000 | Loss: 0.00001569
Iteration 106/1000 | Loss: 0.00001569
Iteration 107/1000 | Loss: 0.00001569
Iteration 108/1000 | Loss: 0.00001569
Iteration 109/1000 | Loss: 0.00001568
Iteration 110/1000 | Loss: 0.00001568
Iteration 111/1000 | Loss: 0.00001568
Iteration 112/1000 | Loss: 0.00001568
Iteration 113/1000 | Loss: 0.00001568
Iteration 114/1000 | Loss: 0.00001568
Iteration 115/1000 | Loss: 0.00001568
Iteration 116/1000 | Loss: 0.00001568
Iteration 117/1000 | Loss: 0.00001568
Iteration 118/1000 | Loss: 0.00001568
Iteration 119/1000 | Loss: 0.00001568
Iteration 120/1000 | Loss: 0.00001568
Iteration 121/1000 | Loss: 0.00001568
Iteration 122/1000 | Loss: 0.00001567
Iteration 123/1000 | Loss: 0.00001567
Iteration 124/1000 | Loss: 0.00001567
Iteration 125/1000 | Loss: 0.00001567
Iteration 126/1000 | Loss: 0.00001567
Iteration 127/1000 | Loss: 0.00001567
Iteration 128/1000 | Loss: 0.00001567
Iteration 129/1000 | Loss: 0.00001567
Iteration 130/1000 | Loss: 0.00001567
Iteration 131/1000 | Loss: 0.00001567
Iteration 132/1000 | Loss: 0.00001567
Iteration 133/1000 | Loss: 0.00001567
Iteration 134/1000 | Loss: 0.00001567
Iteration 135/1000 | Loss: 0.00001567
Iteration 136/1000 | Loss: 0.00001567
Iteration 137/1000 | Loss: 0.00001566
Iteration 138/1000 | Loss: 0.00001566
Iteration 139/1000 | Loss: 0.00001566
Iteration 140/1000 | Loss: 0.00001566
Iteration 141/1000 | Loss: 0.00001566
Iteration 142/1000 | Loss: 0.00001566
Iteration 143/1000 | Loss: 0.00001566
Iteration 144/1000 | Loss: 0.00001566
Iteration 145/1000 | Loss: 0.00001566
Iteration 146/1000 | Loss: 0.00001566
Iteration 147/1000 | Loss: 0.00001566
Iteration 148/1000 | Loss: 0.00001566
Iteration 149/1000 | Loss: 0.00001566
Iteration 150/1000 | Loss: 0.00001566
Iteration 151/1000 | Loss: 0.00001566
Iteration 152/1000 | Loss: 0.00001566
Iteration 153/1000 | Loss: 0.00001566
Iteration 154/1000 | Loss: 0.00001566
Iteration 155/1000 | Loss: 0.00001566
Iteration 156/1000 | Loss: 0.00001566
Iteration 157/1000 | Loss: 0.00001566
Iteration 158/1000 | Loss: 0.00001566
Iteration 159/1000 | Loss: 0.00001566
Iteration 160/1000 | Loss: 0.00001566
Iteration 161/1000 | Loss: 0.00001566
Iteration 162/1000 | Loss: 0.00001566
Iteration 163/1000 | Loss: 0.00001566
Iteration 164/1000 | Loss: 0.00001566
Iteration 165/1000 | Loss: 0.00001566
Iteration 166/1000 | Loss: 0.00001566
Iteration 167/1000 | Loss: 0.00001566
Iteration 168/1000 | Loss: 0.00001566
Iteration 169/1000 | Loss: 0.00001566
Iteration 170/1000 | Loss: 0.00001566
Iteration 171/1000 | Loss: 0.00001566
Iteration 172/1000 | Loss: 0.00001566
Iteration 173/1000 | Loss: 0.00001566
Iteration 174/1000 | Loss: 0.00001566
Iteration 175/1000 | Loss: 0.00001566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.5655721654184163e-05, 1.5655721654184163e-05, 1.5655721654184163e-05, 1.5655721654184163e-05, 1.5655721654184163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5655721654184163e-05

Optimization complete. Final v2v error: 3.386866807937622 mm

Highest mean error: 3.8606622219085693 mm for frame 24

Lowest mean error: 3.165745973587036 mm for frame 181

Saving results

Total time: 40.8009135723114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540501
Iteration 2/25 | Loss: 0.00108787
Iteration 3/25 | Loss: 0.00078784
Iteration 4/25 | Loss: 0.00076068
Iteration 5/25 | Loss: 0.00075252
Iteration 6/25 | Loss: 0.00074989
Iteration 7/25 | Loss: 0.00074932
Iteration 8/25 | Loss: 0.00074928
Iteration 9/25 | Loss: 0.00074928
Iteration 10/25 | Loss: 0.00074928
Iteration 11/25 | Loss: 0.00074928
Iteration 12/25 | Loss: 0.00074928
Iteration 13/25 | Loss: 0.00074928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007492828299291432, 0.0007492828299291432, 0.0007492828299291432, 0.0007492828299291432, 0.0007492828299291432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007492828299291432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77190351
Iteration 2/25 | Loss: 0.00023945
Iteration 3/25 | Loss: 0.00023945
Iteration 4/25 | Loss: 0.00023945
Iteration 5/25 | Loss: 0.00023945
Iteration 6/25 | Loss: 0.00023945
Iteration 7/25 | Loss: 0.00023945
Iteration 8/25 | Loss: 0.00023945
Iteration 9/25 | Loss: 0.00023945
Iteration 10/25 | Loss: 0.00023945
Iteration 11/25 | Loss: 0.00023945
Iteration 12/25 | Loss: 0.00023945
Iteration 13/25 | Loss: 0.00023945
Iteration 14/25 | Loss: 0.00023945
Iteration 15/25 | Loss: 0.00023945
Iteration 16/25 | Loss: 0.00023945
Iteration 17/25 | Loss: 0.00023945
Iteration 18/25 | Loss: 0.00023945
Iteration 19/25 | Loss: 0.00023945
Iteration 20/25 | Loss: 0.00023945
Iteration 21/25 | Loss: 0.00023945
Iteration 22/25 | Loss: 0.00023945
Iteration 23/25 | Loss: 0.00023945
Iteration 24/25 | Loss: 0.00023945
Iteration 25/25 | Loss: 0.00023945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023945
Iteration 2/1000 | Loss: 0.00003269
Iteration 3/1000 | Loss: 0.00002058
Iteration 4/1000 | Loss: 0.00001816
Iteration 5/1000 | Loss: 0.00001715
Iteration 6/1000 | Loss: 0.00001637
Iteration 7/1000 | Loss: 0.00001606
Iteration 8/1000 | Loss: 0.00001572
Iteration 9/1000 | Loss: 0.00001550
Iteration 10/1000 | Loss: 0.00001541
Iteration 11/1000 | Loss: 0.00001527
Iteration 12/1000 | Loss: 0.00001524
Iteration 13/1000 | Loss: 0.00001508
Iteration 14/1000 | Loss: 0.00001497
Iteration 15/1000 | Loss: 0.00001481
Iteration 16/1000 | Loss: 0.00001466
Iteration 17/1000 | Loss: 0.00001465
Iteration 18/1000 | Loss: 0.00001463
Iteration 19/1000 | Loss: 0.00001461
Iteration 20/1000 | Loss: 0.00001461
Iteration 21/1000 | Loss: 0.00001461
Iteration 22/1000 | Loss: 0.00001460
Iteration 23/1000 | Loss: 0.00001460
Iteration 24/1000 | Loss: 0.00001460
Iteration 25/1000 | Loss: 0.00001460
Iteration 26/1000 | Loss: 0.00001458
Iteration 27/1000 | Loss: 0.00001457
Iteration 28/1000 | Loss: 0.00001457
Iteration 29/1000 | Loss: 0.00001455
Iteration 30/1000 | Loss: 0.00001455
Iteration 31/1000 | Loss: 0.00001454
Iteration 32/1000 | Loss: 0.00001453
Iteration 33/1000 | Loss: 0.00001449
Iteration 34/1000 | Loss: 0.00001449
Iteration 35/1000 | Loss: 0.00001447
Iteration 36/1000 | Loss: 0.00001447
Iteration 37/1000 | Loss: 0.00001447
Iteration 38/1000 | Loss: 0.00001446
Iteration 39/1000 | Loss: 0.00001446
Iteration 40/1000 | Loss: 0.00001445
Iteration 41/1000 | Loss: 0.00001445
Iteration 42/1000 | Loss: 0.00001445
Iteration 43/1000 | Loss: 0.00001445
Iteration 44/1000 | Loss: 0.00001445
Iteration 45/1000 | Loss: 0.00001445
Iteration 46/1000 | Loss: 0.00001445
Iteration 47/1000 | Loss: 0.00001445
Iteration 48/1000 | Loss: 0.00001445
Iteration 49/1000 | Loss: 0.00001445
Iteration 50/1000 | Loss: 0.00001444
Iteration 51/1000 | Loss: 0.00001444
Iteration 52/1000 | Loss: 0.00001444
Iteration 53/1000 | Loss: 0.00001443
Iteration 54/1000 | Loss: 0.00001442
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001435
Iteration 60/1000 | Loss: 0.00001435
Iteration 61/1000 | Loss: 0.00001434
Iteration 62/1000 | Loss: 0.00001433
Iteration 63/1000 | Loss: 0.00001433
Iteration 64/1000 | Loss: 0.00001433
Iteration 65/1000 | Loss: 0.00001433
Iteration 66/1000 | Loss: 0.00001433
Iteration 67/1000 | Loss: 0.00001433
Iteration 68/1000 | Loss: 0.00001432
Iteration 69/1000 | Loss: 0.00001432
Iteration 70/1000 | Loss: 0.00001432
Iteration 71/1000 | Loss: 0.00001432
Iteration 72/1000 | Loss: 0.00001432
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001432
Iteration 77/1000 | Loss: 0.00001432
Iteration 78/1000 | Loss: 0.00001432
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001431
Iteration 82/1000 | Loss: 0.00001431
Iteration 83/1000 | Loss: 0.00001431
Iteration 84/1000 | Loss: 0.00001431
Iteration 85/1000 | Loss: 0.00001431
Iteration 86/1000 | Loss: 0.00001431
Iteration 87/1000 | Loss: 0.00001430
Iteration 88/1000 | Loss: 0.00001430
Iteration 89/1000 | Loss: 0.00001429
Iteration 90/1000 | Loss: 0.00001429
Iteration 91/1000 | Loss: 0.00001429
Iteration 92/1000 | Loss: 0.00001429
Iteration 93/1000 | Loss: 0.00001429
Iteration 94/1000 | Loss: 0.00001428
Iteration 95/1000 | Loss: 0.00001428
Iteration 96/1000 | Loss: 0.00001427
Iteration 97/1000 | Loss: 0.00001427
Iteration 98/1000 | Loss: 0.00001427
Iteration 99/1000 | Loss: 0.00001426
Iteration 100/1000 | Loss: 0.00001426
Iteration 101/1000 | Loss: 0.00001426
Iteration 102/1000 | Loss: 0.00001425
Iteration 103/1000 | Loss: 0.00001425
Iteration 104/1000 | Loss: 0.00001425
Iteration 105/1000 | Loss: 0.00001425
Iteration 106/1000 | Loss: 0.00001425
Iteration 107/1000 | Loss: 0.00001425
Iteration 108/1000 | Loss: 0.00001424
Iteration 109/1000 | Loss: 0.00001424
Iteration 110/1000 | Loss: 0.00001424
Iteration 111/1000 | Loss: 0.00001424
Iteration 112/1000 | Loss: 0.00001423
Iteration 113/1000 | Loss: 0.00001423
Iteration 114/1000 | Loss: 0.00001423
Iteration 115/1000 | Loss: 0.00001423
Iteration 116/1000 | Loss: 0.00001423
Iteration 117/1000 | Loss: 0.00001423
Iteration 118/1000 | Loss: 0.00001423
Iteration 119/1000 | Loss: 0.00001422
Iteration 120/1000 | Loss: 0.00001422
Iteration 121/1000 | Loss: 0.00001422
Iteration 122/1000 | Loss: 0.00001422
Iteration 123/1000 | Loss: 0.00001422
Iteration 124/1000 | Loss: 0.00001421
Iteration 125/1000 | Loss: 0.00001421
Iteration 126/1000 | Loss: 0.00001421
Iteration 127/1000 | Loss: 0.00001421
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001420
Iteration 130/1000 | Loss: 0.00001420
Iteration 131/1000 | Loss: 0.00001420
Iteration 132/1000 | Loss: 0.00001419
Iteration 133/1000 | Loss: 0.00001419
Iteration 134/1000 | Loss: 0.00001419
Iteration 135/1000 | Loss: 0.00001419
Iteration 136/1000 | Loss: 0.00001419
Iteration 137/1000 | Loss: 0.00001419
Iteration 138/1000 | Loss: 0.00001419
Iteration 139/1000 | Loss: 0.00001419
Iteration 140/1000 | Loss: 0.00001419
Iteration 141/1000 | Loss: 0.00001419
Iteration 142/1000 | Loss: 0.00001419
Iteration 143/1000 | Loss: 0.00001418
Iteration 144/1000 | Loss: 0.00001418
Iteration 145/1000 | Loss: 0.00001418
Iteration 146/1000 | Loss: 0.00001418
Iteration 147/1000 | Loss: 0.00001418
Iteration 148/1000 | Loss: 0.00001418
Iteration 149/1000 | Loss: 0.00001418
Iteration 150/1000 | Loss: 0.00001418
Iteration 151/1000 | Loss: 0.00001417
Iteration 152/1000 | Loss: 0.00001417
Iteration 153/1000 | Loss: 0.00001417
Iteration 154/1000 | Loss: 0.00001417
Iteration 155/1000 | Loss: 0.00001417
Iteration 156/1000 | Loss: 0.00001417
Iteration 157/1000 | Loss: 0.00001417
Iteration 158/1000 | Loss: 0.00001417
Iteration 159/1000 | Loss: 0.00001417
Iteration 160/1000 | Loss: 0.00001417
Iteration 161/1000 | Loss: 0.00001417
Iteration 162/1000 | Loss: 0.00001417
Iteration 163/1000 | Loss: 0.00001416
Iteration 164/1000 | Loss: 0.00001416
Iteration 165/1000 | Loss: 0.00001416
Iteration 166/1000 | Loss: 0.00001416
Iteration 167/1000 | Loss: 0.00001416
Iteration 168/1000 | Loss: 0.00001416
Iteration 169/1000 | Loss: 0.00001416
Iteration 170/1000 | Loss: 0.00001416
Iteration 171/1000 | Loss: 0.00001416
Iteration 172/1000 | Loss: 0.00001416
Iteration 173/1000 | Loss: 0.00001415
Iteration 174/1000 | Loss: 0.00001415
Iteration 175/1000 | Loss: 0.00001415
Iteration 176/1000 | Loss: 0.00001415
Iteration 177/1000 | Loss: 0.00001415
Iteration 178/1000 | Loss: 0.00001415
Iteration 179/1000 | Loss: 0.00001415
Iteration 180/1000 | Loss: 0.00001415
Iteration 181/1000 | Loss: 0.00001415
Iteration 182/1000 | Loss: 0.00001415
Iteration 183/1000 | Loss: 0.00001415
Iteration 184/1000 | Loss: 0.00001414
Iteration 185/1000 | Loss: 0.00001414
Iteration 186/1000 | Loss: 0.00001414
Iteration 187/1000 | Loss: 0.00001414
Iteration 188/1000 | Loss: 0.00001414
Iteration 189/1000 | Loss: 0.00001414
Iteration 190/1000 | Loss: 0.00001414
Iteration 191/1000 | Loss: 0.00001414
Iteration 192/1000 | Loss: 0.00001414
Iteration 193/1000 | Loss: 0.00001414
Iteration 194/1000 | Loss: 0.00001414
Iteration 195/1000 | Loss: 0.00001414
Iteration 196/1000 | Loss: 0.00001414
Iteration 197/1000 | Loss: 0.00001414
Iteration 198/1000 | Loss: 0.00001414
Iteration 199/1000 | Loss: 0.00001414
Iteration 200/1000 | Loss: 0.00001414
Iteration 201/1000 | Loss: 0.00001414
Iteration 202/1000 | Loss: 0.00001414
Iteration 203/1000 | Loss: 0.00001414
Iteration 204/1000 | Loss: 0.00001414
Iteration 205/1000 | Loss: 0.00001414
Iteration 206/1000 | Loss: 0.00001414
Iteration 207/1000 | Loss: 0.00001414
Iteration 208/1000 | Loss: 0.00001414
Iteration 209/1000 | Loss: 0.00001414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.4140864550427068e-05, 1.4140864550427068e-05, 1.4140864550427068e-05, 1.4140864550427068e-05, 1.4140864550427068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4140864550427068e-05

Optimization complete. Final v2v error: 3.1427195072174072 mm

Highest mean error: 4.014441013336182 mm for frame 8

Lowest mean error: 2.8602941036224365 mm for frame 45

Saving results

Total time: 46.74558448791504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467089
Iteration 2/25 | Loss: 0.00088504
Iteration 3/25 | Loss: 0.00073870
Iteration 4/25 | Loss: 0.00072138
Iteration 5/25 | Loss: 0.00071806
Iteration 6/25 | Loss: 0.00071787
Iteration 7/25 | Loss: 0.00071787
Iteration 8/25 | Loss: 0.00071787
Iteration 9/25 | Loss: 0.00071787
Iteration 10/25 | Loss: 0.00071787
Iteration 11/25 | Loss: 0.00071787
Iteration 12/25 | Loss: 0.00071787
Iteration 13/25 | Loss: 0.00071787
Iteration 14/25 | Loss: 0.00071787
Iteration 15/25 | Loss: 0.00071787
Iteration 16/25 | Loss: 0.00071787
Iteration 17/25 | Loss: 0.00071787
Iteration 18/25 | Loss: 0.00071787
Iteration 19/25 | Loss: 0.00071787
Iteration 20/25 | Loss: 0.00071787
Iteration 21/25 | Loss: 0.00071787
Iteration 22/25 | Loss: 0.00071787
Iteration 23/25 | Loss: 0.00071787
Iteration 24/25 | Loss: 0.00071787
Iteration 25/25 | Loss: 0.00071787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.92656898
Iteration 2/25 | Loss: 0.00023279
Iteration 3/25 | Loss: 0.00023279
Iteration 4/25 | Loss: 0.00023279
Iteration 5/25 | Loss: 0.00023279
Iteration 6/25 | Loss: 0.00023279
Iteration 7/25 | Loss: 0.00023279
Iteration 8/25 | Loss: 0.00023278
Iteration 9/25 | Loss: 0.00023278
Iteration 10/25 | Loss: 0.00023278
Iteration 11/25 | Loss: 0.00023278
Iteration 12/25 | Loss: 0.00023278
Iteration 13/25 | Loss: 0.00023278
Iteration 14/25 | Loss: 0.00023278
Iteration 15/25 | Loss: 0.00023278
Iteration 16/25 | Loss: 0.00023278
Iteration 17/25 | Loss: 0.00023278
Iteration 18/25 | Loss: 0.00023278
Iteration 19/25 | Loss: 0.00023278
Iteration 20/25 | Loss: 0.00023278
Iteration 21/25 | Loss: 0.00023278
Iteration 22/25 | Loss: 0.00023278
Iteration 23/25 | Loss: 0.00023278
Iteration 24/25 | Loss: 0.00023278
Iteration 25/25 | Loss: 0.00023278

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023278
Iteration 2/1000 | Loss: 0.00002048
Iteration 3/1000 | Loss: 0.00001735
Iteration 4/1000 | Loss: 0.00001646
Iteration 5/1000 | Loss: 0.00001535
Iteration 6/1000 | Loss: 0.00001495
Iteration 7/1000 | Loss: 0.00001455
Iteration 8/1000 | Loss: 0.00001429
Iteration 9/1000 | Loss: 0.00001407
Iteration 10/1000 | Loss: 0.00001407
Iteration 11/1000 | Loss: 0.00001406
Iteration 12/1000 | Loss: 0.00001401
Iteration 13/1000 | Loss: 0.00001400
Iteration 14/1000 | Loss: 0.00001399
Iteration 15/1000 | Loss: 0.00001399
Iteration 16/1000 | Loss: 0.00001398
Iteration 17/1000 | Loss: 0.00001398
Iteration 18/1000 | Loss: 0.00001385
Iteration 19/1000 | Loss: 0.00001383
Iteration 20/1000 | Loss: 0.00001382
Iteration 21/1000 | Loss: 0.00001379
Iteration 22/1000 | Loss: 0.00001378
Iteration 23/1000 | Loss: 0.00001378
Iteration 24/1000 | Loss: 0.00001378
Iteration 25/1000 | Loss: 0.00001377
Iteration 26/1000 | Loss: 0.00001377
Iteration 27/1000 | Loss: 0.00001365
Iteration 28/1000 | Loss: 0.00001362
Iteration 29/1000 | Loss: 0.00001362
Iteration 30/1000 | Loss: 0.00001361
Iteration 31/1000 | Loss: 0.00001360
Iteration 32/1000 | Loss: 0.00001358
Iteration 33/1000 | Loss: 0.00001358
Iteration 34/1000 | Loss: 0.00001357
Iteration 35/1000 | Loss: 0.00001357
Iteration 36/1000 | Loss: 0.00001357
Iteration 37/1000 | Loss: 0.00001355
Iteration 38/1000 | Loss: 0.00001353
Iteration 39/1000 | Loss: 0.00001350
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001347
Iteration 42/1000 | Loss: 0.00001347
Iteration 43/1000 | Loss: 0.00001346
Iteration 44/1000 | Loss: 0.00001345
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001341
Iteration 48/1000 | Loss: 0.00001341
Iteration 49/1000 | Loss: 0.00001341
Iteration 50/1000 | Loss: 0.00001340
Iteration 51/1000 | Loss: 0.00001338
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001338
Iteration 58/1000 | Loss: 0.00001338
Iteration 59/1000 | Loss: 0.00001338
Iteration 60/1000 | Loss: 0.00001338
Iteration 61/1000 | Loss: 0.00001337
Iteration 62/1000 | Loss: 0.00001337
Iteration 63/1000 | Loss: 0.00001337
Iteration 64/1000 | Loss: 0.00001337
Iteration 65/1000 | Loss: 0.00001337
Iteration 66/1000 | Loss: 0.00001336
Iteration 67/1000 | Loss: 0.00001336
Iteration 68/1000 | Loss: 0.00001335
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001335
Iteration 71/1000 | Loss: 0.00001335
Iteration 72/1000 | Loss: 0.00001335
Iteration 73/1000 | Loss: 0.00001335
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001334
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001333
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001333
Iteration 83/1000 | Loss: 0.00001333
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001332
Iteration 86/1000 | Loss: 0.00001332
Iteration 87/1000 | Loss: 0.00001332
Iteration 88/1000 | Loss: 0.00001332
Iteration 89/1000 | Loss: 0.00001332
Iteration 90/1000 | Loss: 0.00001332
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001331
Iteration 93/1000 | Loss: 0.00001331
Iteration 94/1000 | Loss: 0.00001330
Iteration 95/1000 | Loss: 0.00001330
Iteration 96/1000 | Loss: 0.00001330
Iteration 97/1000 | Loss: 0.00001330
Iteration 98/1000 | Loss: 0.00001330
Iteration 99/1000 | Loss: 0.00001329
Iteration 100/1000 | Loss: 0.00001329
Iteration 101/1000 | Loss: 0.00001329
Iteration 102/1000 | Loss: 0.00001329
Iteration 103/1000 | Loss: 0.00001329
Iteration 104/1000 | Loss: 0.00001329
Iteration 105/1000 | Loss: 0.00001329
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001329
Iteration 110/1000 | Loss: 0.00001329
Iteration 111/1000 | Loss: 0.00001329
Iteration 112/1000 | Loss: 0.00001329
Iteration 113/1000 | Loss: 0.00001329
Iteration 114/1000 | Loss: 0.00001329
Iteration 115/1000 | Loss: 0.00001329
Iteration 116/1000 | Loss: 0.00001329
Iteration 117/1000 | Loss: 0.00001329
Iteration 118/1000 | Loss: 0.00001328
Iteration 119/1000 | Loss: 0.00001328
Iteration 120/1000 | Loss: 0.00001328
Iteration 121/1000 | Loss: 0.00001328
Iteration 122/1000 | Loss: 0.00001328
Iteration 123/1000 | Loss: 0.00001327
Iteration 124/1000 | Loss: 0.00001327
Iteration 125/1000 | Loss: 0.00001327
Iteration 126/1000 | Loss: 0.00001327
Iteration 127/1000 | Loss: 0.00001327
Iteration 128/1000 | Loss: 0.00001327
Iteration 129/1000 | Loss: 0.00001327
Iteration 130/1000 | Loss: 0.00001327
Iteration 131/1000 | Loss: 0.00001327
Iteration 132/1000 | Loss: 0.00001327
Iteration 133/1000 | Loss: 0.00001327
Iteration 134/1000 | Loss: 0.00001327
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001327
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001327
Iteration 143/1000 | Loss: 0.00001327
Iteration 144/1000 | Loss: 0.00001327
Iteration 145/1000 | Loss: 0.00001327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.3271415809867904e-05, 1.3271415809867904e-05, 1.3271415809867904e-05, 1.3271415809867904e-05, 1.3271415809867904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3271415809867904e-05

Optimization complete. Final v2v error: 3.104794502258301 mm

Highest mean error: 3.338094711303711 mm for frame 92

Lowest mean error: 2.958289623260498 mm for frame 1

Saving results

Total time: 40.960227966308594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00338208
Iteration 2/25 | Loss: 0.00115938
Iteration 3/25 | Loss: 0.00077286
Iteration 4/25 | Loss: 0.00072434
Iteration 5/25 | Loss: 0.00071304
Iteration 6/25 | Loss: 0.00071065
Iteration 7/25 | Loss: 0.00070991
Iteration 8/25 | Loss: 0.00070972
Iteration 9/25 | Loss: 0.00070972
Iteration 10/25 | Loss: 0.00070972
Iteration 11/25 | Loss: 0.00070972
Iteration 12/25 | Loss: 0.00070972
Iteration 13/25 | Loss: 0.00070972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000709720712620765, 0.000709720712620765, 0.000709720712620765, 0.000709720712620765, 0.000709720712620765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000709720712620765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50747454
Iteration 2/25 | Loss: 0.00024715
Iteration 3/25 | Loss: 0.00024715
Iteration 4/25 | Loss: 0.00024715
Iteration 5/25 | Loss: 0.00024715
Iteration 6/25 | Loss: 0.00024715
Iteration 7/25 | Loss: 0.00024715
Iteration 8/25 | Loss: 0.00024715
Iteration 9/25 | Loss: 0.00024715
Iteration 10/25 | Loss: 0.00024715
Iteration 11/25 | Loss: 0.00024715
Iteration 12/25 | Loss: 0.00024715
Iteration 13/25 | Loss: 0.00024715
Iteration 14/25 | Loss: 0.00024715
Iteration 15/25 | Loss: 0.00024715
Iteration 16/25 | Loss: 0.00024715
Iteration 17/25 | Loss: 0.00024715
Iteration 18/25 | Loss: 0.00024715
Iteration 19/25 | Loss: 0.00024715
Iteration 20/25 | Loss: 0.00024715
Iteration 21/25 | Loss: 0.00024715
Iteration 22/25 | Loss: 0.00024715
Iteration 23/25 | Loss: 0.00024715
Iteration 24/25 | Loss: 0.00024715
Iteration 25/25 | Loss: 0.00024715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024715
Iteration 2/1000 | Loss: 0.00002444
Iteration 3/1000 | Loss: 0.00001767
Iteration 4/1000 | Loss: 0.00001525
Iteration 5/1000 | Loss: 0.00001434
Iteration 6/1000 | Loss: 0.00001358
Iteration 7/1000 | Loss: 0.00001317
Iteration 8/1000 | Loss: 0.00001285
Iteration 9/1000 | Loss: 0.00001262
Iteration 10/1000 | Loss: 0.00001240
Iteration 11/1000 | Loss: 0.00001234
Iteration 12/1000 | Loss: 0.00001220
Iteration 13/1000 | Loss: 0.00001218
Iteration 14/1000 | Loss: 0.00001214
Iteration 15/1000 | Loss: 0.00001212
Iteration 16/1000 | Loss: 0.00001203
Iteration 17/1000 | Loss: 0.00001198
Iteration 18/1000 | Loss: 0.00001196
Iteration 19/1000 | Loss: 0.00001192
Iteration 20/1000 | Loss: 0.00001192
Iteration 21/1000 | Loss: 0.00001190
Iteration 22/1000 | Loss: 0.00001190
Iteration 23/1000 | Loss: 0.00001189
Iteration 24/1000 | Loss: 0.00001189
Iteration 25/1000 | Loss: 0.00001189
Iteration 26/1000 | Loss: 0.00001189
Iteration 27/1000 | Loss: 0.00001188
Iteration 28/1000 | Loss: 0.00001188
Iteration 29/1000 | Loss: 0.00001188
Iteration 30/1000 | Loss: 0.00001188
Iteration 31/1000 | Loss: 0.00001188
Iteration 32/1000 | Loss: 0.00001188
Iteration 33/1000 | Loss: 0.00001188
Iteration 34/1000 | Loss: 0.00001188
Iteration 35/1000 | Loss: 0.00001187
Iteration 36/1000 | Loss: 0.00001186
Iteration 37/1000 | Loss: 0.00001186
Iteration 38/1000 | Loss: 0.00001185
Iteration 39/1000 | Loss: 0.00001185
Iteration 40/1000 | Loss: 0.00001185
Iteration 41/1000 | Loss: 0.00001185
Iteration 42/1000 | Loss: 0.00001185
Iteration 43/1000 | Loss: 0.00001185
Iteration 44/1000 | Loss: 0.00001184
Iteration 45/1000 | Loss: 0.00001184
Iteration 46/1000 | Loss: 0.00001184
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001184
Iteration 49/1000 | Loss: 0.00001184
Iteration 50/1000 | Loss: 0.00001184
Iteration 51/1000 | Loss: 0.00001184
Iteration 52/1000 | Loss: 0.00001184
Iteration 53/1000 | Loss: 0.00001183
Iteration 54/1000 | Loss: 0.00001182
Iteration 55/1000 | Loss: 0.00001182
Iteration 56/1000 | Loss: 0.00001182
Iteration 57/1000 | Loss: 0.00001182
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001181
Iteration 60/1000 | Loss: 0.00001181
Iteration 61/1000 | Loss: 0.00001181
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001179
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001179
Iteration 71/1000 | Loss: 0.00001179
Iteration 72/1000 | Loss: 0.00001178
Iteration 73/1000 | Loss: 0.00001178
Iteration 74/1000 | Loss: 0.00001178
Iteration 75/1000 | Loss: 0.00001178
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001178
Iteration 78/1000 | Loss: 0.00001177
Iteration 79/1000 | Loss: 0.00001177
Iteration 80/1000 | Loss: 0.00001177
Iteration 81/1000 | Loss: 0.00001177
Iteration 82/1000 | Loss: 0.00001176
Iteration 83/1000 | Loss: 0.00001176
Iteration 84/1000 | Loss: 0.00001176
Iteration 85/1000 | Loss: 0.00001176
Iteration 86/1000 | Loss: 0.00001176
Iteration 87/1000 | Loss: 0.00001176
Iteration 88/1000 | Loss: 0.00001176
Iteration 89/1000 | Loss: 0.00001176
Iteration 90/1000 | Loss: 0.00001175
Iteration 91/1000 | Loss: 0.00001175
Iteration 92/1000 | Loss: 0.00001175
Iteration 93/1000 | Loss: 0.00001175
Iteration 94/1000 | Loss: 0.00001175
Iteration 95/1000 | Loss: 0.00001175
Iteration 96/1000 | Loss: 0.00001175
Iteration 97/1000 | Loss: 0.00001175
Iteration 98/1000 | Loss: 0.00001175
Iteration 99/1000 | Loss: 0.00001174
Iteration 100/1000 | Loss: 0.00001174
Iteration 101/1000 | Loss: 0.00001174
Iteration 102/1000 | Loss: 0.00001173
Iteration 103/1000 | Loss: 0.00001173
Iteration 104/1000 | Loss: 0.00001173
Iteration 105/1000 | Loss: 0.00001173
Iteration 106/1000 | Loss: 0.00001173
Iteration 107/1000 | Loss: 0.00001173
Iteration 108/1000 | Loss: 0.00001173
Iteration 109/1000 | Loss: 0.00001172
Iteration 110/1000 | Loss: 0.00001172
Iteration 111/1000 | Loss: 0.00001172
Iteration 112/1000 | Loss: 0.00001172
Iteration 113/1000 | Loss: 0.00001172
Iteration 114/1000 | Loss: 0.00001172
Iteration 115/1000 | Loss: 0.00001171
Iteration 116/1000 | Loss: 0.00001171
Iteration 117/1000 | Loss: 0.00001171
Iteration 118/1000 | Loss: 0.00001171
Iteration 119/1000 | Loss: 0.00001171
Iteration 120/1000 | Loss: 0.00001171
Iteration 121/1000 | Loss: 0.00001171
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Iteration 124/1000 | Loss: 0.00001170
Iteration 125/1000 | Loss: 0.00001170
Iteration 126/1000 | Loss: 0.00001170
Iteration 127/1000 | Loss: 0.00001170
Iteration 128/1000 | Loss: 0.00001170
Iteration 129/1000 | Loss: 0.00001170
Iteration 130/1000 | Loss: 0.00001170
Iteration 131/1000 | Loss: 0.00001170
Iteration 132/1000 | Loss: 0.00001170
Iteration 133/1000 | Loss: 0.00001170
Iteration 134/1000 | Loss: 0.00001170
Iteration 135/1000 | Loss: 0.00001170
Iteration 136/1000 | Loss: 0.00001169
Iteration 137/1000 | Loss: 0.00001169
Iteration 138/1000 | Loss: 0.00001169
Iteration 139/1000 | Loss: 0.00001169
Iteration 140/1000 | Loss: 0.00001169
Iteration 141/1000 | Loss: 0.00001169
Iteration 142/1000 | Loss: 0.00001169
Iteration 143/1000 | Loss: 0.00001169
Iteration 144/1000 | Loss: 0.00001169
Iteration 145/1000 | Loss: 0.00001169
Iteration 146/1000 | Loss: 0.00001169
Iteration 147/1000 | Loss: 0.00001169
Iteration 148/1000 | Loss: 0.00001169
Iteration 149/1000 | Loss: 0.00001169
Iteration 150/1000 | Loss: 0.00001169
Iteration 151/1000 | Loss: 0.00001169
Iteration 152/1000 | Loss: 0.00001169
Iteration 153/1000 | Loss: 0.00001169
Iteration 154/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.1694373824866489e-05, 1.1694373824866489e-05, 1.1694373824866489e-05, 1.1694373824866489e-05, 1.1694373824866489e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1694373824866489e-05

Optimization complete. Final v2v error: 2.9320669174194336 mm

Highest mean error: 3.1736397743225098 mm for frame 27

Lowest mean error: 2.8040881156921387 mm for frame 61

Saving results

Total time: 38.6101348400116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00670608
Iteration 2/25 | Loss: 0.00083952
Iteration 3/25 | Loss: 0.00069551
Iteration 4/25 | Loss: 0.00067610
Iteration 5/25 | Loss: 0.00067018
Iteration 6/25 | Loss: 0.00066847
Iteration 7/25 | Loss: 0.00066817
Iteration 8/25 | Loss: 0.00066817
Iteration 9/25 | Loss: 0.00066817
Iteration 10/25 | Loss: 0.00066817
Iteration 11/25 | Loss: 0.00066817
Iteration 12/25 | Loss: 0.00066817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006681690574623644, 0.0006681690574623644, 0.0006681690574623644, 0.0006681690574623644, 0.0006681690574623644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006681690574623644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.68433571
Iteration 2/25 | Loss: 0.00027476
Iteration 3/25 | Loss: 0.00027476
Iteration 4/25 | Loss: 0.00027476
Iteration 5/25 | Loss: 0.00027475
Iteration 6/25 | Loss: 0.00027475
Iteration 7/25 | Loss: 0.00027475
Iteration 8/25 | Loss: 0.00027475
Iteration 9/25 | Loss: 0.00027475
Iteration 10/25 | Loss: 0.00027475
Iteration 11/25 | Loss: 0.00027475
Iteration 12/25 | Loss: 0.00027475
Iteration 13/25 | Loss: 0.00027475
Iteration 14/25 | Loss: 0.00027475
Iteration 15/25 | Loss: 0.00027475
Iteration 16/25 | Loss: 0.00027475
Iteration 17/25 | Loss: 0.00027475
Iteration 18/25 | Loss: 0.00027475
Iteration 19/25 | Loss: 0.00027475
Iteration 20/25 | Loss: 0.00027475
Iteration 21/25 | Loss: 0.00027475
Iteration 22/25 | Loss: 0.00027475
Iteration 23/25 | Loss: 0.00027475
Iteration 24/25 | Loss: 0.00027475
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0002747530525084585, 0.0002747530525084585, 0.0002747530525084585, 0.0002747530525084585, 0.0002747530525084585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002747530525084585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027475
Iteration 2/1000 | Loss: 0.00001805
Iteration 3/1000 | Loss: 0.00001176
Iteration 4/1000 | Loss: 0.00001071
Iteration 5/1000 | Loss: 0.00001000
Iteration 6/1000 | Loss: 0.00000982
Iteration 7/1000 | Loss: 0.00000968
Iteration 8/1000 | Loss: 0.00000961
Iteration 9/1000 | Loss: 0.00000960
Iteration 10/1000 | Loss: 0.00000959
Iteration 11/1000 | Loss: 0.00000956
Iteration 12/1000 | Loss: 0.00000955
Iteration 13/1000 | Loss: 0.00000953
Iteration 14/1000 | Loss: 0.00000953
Iteration 15/1000 | Loss: 0.00000952
Iteration 16/1000 | Loss: 0.00000946
Iteration 17/1000 | Loss: 0.00000946
Iteration 18/1000 | Loss: 0.00000945
Iteration 19/1000 | Loss: 0.00000944
Iteration 20/1000 | Loss: 0.00000941
Iteration 21/1000 | Loss: 0.00000940
Iteration 22/1000 | Loss: 0.00000940
Iteration 23/1000 | Loss: 0.00000939
Iteration 24/1000 | Loss: 0.00000938
Iteration 25/1000 | Loss: 0.00000936
Iteration 26/1000 | Loss: 0.00000936
Iteration 27/1000 | Loss: 0.00000935
Iteration 28/1000 | Loss: 0.00000933
Iteration 29/1000 | Loss: 0.00000933
Iteration 30/1000 | Loss: 0.00000932
Iteration 31/1000 | Loss: 0.00000932
Iteration 32/1000 | Loss: 0.00000932
Iteration 33/1000 | Loss: 0.00000931
Iteration 34/1000 | Loss: 0.00000931
Iteration 35/1000 | Loss: 0.00000930
Iteration 36/1000 | Loss: 0.00000930
Iteration 37/1000 | Loss: 0.00000929
Iteration 38/1000 | Loss: 0.00000929
Iteration 39/1000 | Loss: 0.00000929
Iteration 40/1000 | Loss: 0.00000929
Iteration 41/1000 | Loss: 0.00000929
Iteration 42/1000 | Loss: 0.00000929
Iteration 43/1000 | Loss: 0.00000928
Iteration 44/1000 | Loss: 0.00000928
Iteration 45/1000 | Loss: 0.00000928
Iteration 46/1000 | Loss: 0.00000928
Iteration 47/1000 | Loss: 0.00000927
Iteration 48/1000 | Loss: 0.00000927
Iteration 49/1000 | Loss: 0.00000926
Iteration 50/1000 | Loss: 0.00000926
Iteration 51/1000 | Loss: 0.00000926
Iteration 52/1000 | Loss: 0.00000925
Iteration 53/1000 | Loss: 0.00000925
Iteration 54/1000 | Loss: 0.00000925
Iteration 55/1000 | Loss: 0.00000924
Iteration 56/1000 | Loss: 0.00000924
Iteration 57/1000 | Loss: 0.00000924
Iteration 58/1000 | Loss: 0.00000923
Iteration 59/1000 | Loss: 0.00000923
Iteration 60/1000 | Loss: 0.00000923
Iteration 61/1000 | Loss: 0.00000923
Iteration 62/1000 | Loss: 0.00000923
Iteration 63/1000 | Loss: 0.00000922
Iteration 64/1000 | Loss: 0.00000922
Iteration 65/1000 | Loss: 0.00000921
Iteration 66/1000 | Loss: 0.00000920
Iteration 67/1000 | Loss: 0.00000920
Iteration 68/1000 | Loss: 0.00000920
Iteration 69/1000 | Loss: 0.00000920
Iteration 70/1000 | Loss: 0.00000919
Iteration 71/1000 | Loss: 0.00000919
Iteration 72/1000 | Loss: 0.00000919
Iteration 73/1000 | Loss: 0.00000918
Iteration 74/1000 | Loss: 0.00000918
Iteration 75/1000 | Loss: 0.00000918
Iteration 76/1000 | Loss: 0.00000917
Iteration 77/1000 | Loss: 0.00000917
Iteration 78/1000 | Loss: 0.00000917
Iteration 79/1000 | Loss: 0.00000917
Iteration 80/1000 | Loss: 0.00000917
Iteration 81/1000 | Loss: 0.00000917
Iteration 82/1000 | Loss: 0.00000917
Iteration 83/1000 | Loss: 0.00000916
Iteration 84/1000 | Loss: 0.00000916
Iteration 85/1000 | Loss: 0.00000916
Iteration 86/1000 | Loss: 0.00000916
Iteration 87/1000 | Loss: 0.00000916
Iteration 88/1000 | Loss: 0.00000916
Iteration 89/1000 | Loss: 0.00000916
Iteration 90/1000 | Loss: 0.00000916
Iteration 91/1000 | Loss: 0.00000916
Iteration 92/1000 | Loss: 0.00000915
Iteration 93/1000 | Loss: 0.00000915
Iteration 94/1000 | Loss: 0.00000915
Iteration 95/1000 | Loss: 0.00000915
Iteration 96/1000 | Loss: 0.00000915
Iteration 97/1000 | Loss: 0.00000915
Iteration 98/1000 | Loss: 0.00000914
Iteration 99/1000 | Loss: 0.00000914
Iteration 100/1000 | Loss: 0.00000914
Iteration 101/1000 | Loss: 0.00000914
Iteration 102/1000 | Loss: 0.00000914
Iteration 103/1000 | Loss: 0.00000914
Iteration 104/1000 | Loss: 0.00000914
Iteration 105/1000 | Loss: 0.00000914
Iteration 106/1000 | Loss: 0.00000914
Iteration 107/1000 | Loss: 0.00000914
Iteration 108/1000 | Loss: 0.00000914
Iteration 109/1000 | Loss: 0.00000914
Iteration 110/1000 | Loss: 0.00000913
Iteration 111/1000 | Loss: 0.00000913
Iteration 112/1000 | Loss: 0.00000913
Iteration 113/1000 | Loss: 0.00000913
Iteration 114/1000 | Loss: 0.00000913
Iteration 115/1000 | Loss: 0.00000913
Iteration 116/1000 | Loss: 0.00000912
Iteration 117/1000 | Loss: 0.00000912
Iteration 118/1000 | Loss: 0.00000912
Iteration 119/1000 | Loss: 0.00000912
Iteration 120/1000 | Loss: 0.00000912
Iteration 121/1000 | Loss: 0.00000912
Iteration 122/1000 | Loss: 0.00000912
Iteration 123/1000 | Loss: 0.00000912
Iteration 124/1000 | Loss: 0.00000912
Iteration 125/1000 | Loss: 0.00000911
Iteration 126/1000 | Loss: 0.00000911
Iteration 127/1000 | Loss: 0.00000911
Iteration 128/1000 | Loss: 0.00000911
Iteration 129/1000 | Loss: 0.00000910
Iteration 130/1000 | Loss: 0.00000910
Iteration 131/1000 | Loss: 0.00000910
Iteration 132/1000 | Loss: 0.00000910
Iteration 133/1000 | Loss: 0.00000910
Iteration 134/1000 | Loss: 0.00000910
Iteration 135/1000 | Loss: 0.00000910
Iteration 136/1000 | Loss: 0.00000910
Iteration 137/1000 | Loss: 0.00000910
Iteration 138/1000 | Loss: 0.00000910
Iteration 139/1000 | Loss: 0.00000909
Iteration 140/1000 | Loss: 0.00000909
Iteration 141/1000 | Loss: 0.00000909
Iteration 142/1000 | Loss: 0.00000909
Iteration 143/1000 | Loss: 0.00000909
Iteration 144/1000 | Loss: 0.00000909
Iteration 145/1000 | Loss: 0.00000909
Iteration 146/1000 | Loss: 0.00000909
Iteration 147/1000 | Loss: 0.00000909
Iteration 148/1000 | Loss: 0.00000909
Iteration 149/1000 | Loss: 0.00000909
Iteration 150/1000 | Loss: 0.00000909
Iteration 151/1000 | Loss: 0.00000908
Iteration 152/1000 | Loss: 0.00000908
Iteration 153/1000 | Loss: 0.00000908
Iteration 154/1000 | Loss: 0.00000908
Iteration 155/1000 | Loss: 0.00000908
Iteration 156/1000 | Loss: 0.00000908
Iteration 157/1000 | Loss: 0.00000908
Iteration 158/1000 | Loss: 0.00000908
Iteration 159/1000 | Loss: 0.00000908
Iteration 160/1000 | Loss: 0.00000908
Iteration 161/1000 | Loss: 0.00000908
Iteration 162/1000 | Loss: 0.00000908
Iteration 163/1000 | Loss: 0.00000907
Iteration 164/1000 | Loss: 0.00000907
Iteration 165/1000 | Loss: 0.00000907
Iteration 166/1000 | Loss: 0.00000907
Iteration 167/1000 | Loss: 0.00000907
Iteration 168/1000 | Loss: 0.00000907
Iteration 169/1000 | Loss: 0.00000907
Iteration 170/1000 | Loss: 0.00000907
Iteration 171/1000 | Loss: 0.00000907
Iteration 172/1000 | Loss: 0.00000907
Iteration 173/1000 | Loss: 0.00000907
Iteration 174/1000 | Loss: 0.00000907
Iteration 175/1000 | Loss: 0.00000907
Iteration 176/1000 | Loss: 0.00000907
Iteration 177/1000 | Loss: 0.00000907
Iteration 178/1000 | Loss: 0.00000907
Iteration 179/1000 | Loss: 0.00000906
Iteration 180/1000 | Loss: 0.00000906
Iteration 181/1000 | Loss: 0.00000906
Iteration 182/1000 | Loss: 0.00000906
Iteration 183/1000 | Loss: 0.00000906
Iteration 184/1000 | Loss: 0.00000906
Iteration 185/1000 | Loss: 0.00000906
Iteration 186/1000 | Loss: 0.00000906
Iteration 187/1000 | Loss: 0.00000906
Iteration 188/1000 | Loss: 0.00000906
Iteration 189/1000 | Loss: 0.00000906
Iteration 190/1000 | Loss: 0.00000906
Iteration 191/1000 | Loss: 0.00000906
Iteration 192/1000 | Loss: 0.00000906
Iteration 193/1000 | Loss: 0.00000906
Iteration 194/1000 | Loss: 0.00000906
Iteration 195/1000 | Loss: 0.00000906
Iteration 196/1000 | Loss: 0.00000906
Iteration 197/1000 | Loss: 0.00000906
Iteration 198/1000 | Loss: 0.00000906
Iteration 199/1000 | Loss: 0.00000906
Iteration 200/1000 | Loss: 0.00000906
Iteration 201/1000 | Loss: 0.00000906
Iteration 202/1000 | Loss: 0.00000906
Iteration 203/1000 | Loss: 0.00000906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [9.05529122974258e-06, 9.05529122974258e-06, 9.05529122974258e-06, 9.05529122974258e-06, 9.05529122974258e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.05529122974258e-06

Optimization complete. Final v2v error: 2.6056015491485596 mm

Highest mean error: 2.826054334640503 mm for frame 99

Lowest mean error: 2.427274703979492 mm for frame 131

Saving results

Total time: 34.93861222267151
