Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=75, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4200-4255
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565877
Iteration 2/25 | Loss: 0.00125067
Iteration 3/25 | Loss: 0.00119128
Iteration 4/25 | Loss: 0.00118256
Iteration 5/25 | Loss: 0.00117935
Iteration 6/25 | Loss: 0.00117920
Iteration 7/25 | Loss: 0.00117920
Iteration 8/25 | Loss: 0.00117920
Iteration 9/25 | Loss: 0.00117920
Iteration 10/25 | Loss: 0.00117920
Iteration 11/25 | Loss: 0.00117920
Iteration 12/25 | Loss: 0.00117920
Iteration 13/25 | Loss: 0.00117920
Iteration 14/25 | Loss: 0.00117920
Iteration 15/25 | Loss: 0.00117920
Iteration 16/25 | Loss: 0.00117920
Iteration 17/25 | Loss: 0.00117920
Iteration 18/25 | Loss: 0.00117920
Iteration 19/25 | Loss: 0.00117920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011792047880589962, 0.0011792047880589962, 0.0011792047880589962, 0.0011792047880589962, 0.0011792047880589962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011792047880589962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.81369948
Iteration 2/25 | Loss: 0.00067423
Iteration 3/25 | Loss: 0.00067423
Iteration 4/25 | Loss: 0.00067423
Iteration 5/25 | Loss: 0.00067423
Iteration 6/25 | Loss: 0.00067423
Iteration 7/25 | Loss: 0.00067423
Iteration 8/25 | Loss: 0.00067423
Iteration 9/25 | Loss: 0.00067423
Iteration 10/25 | Loss: 0.00067423
Iteration 11/25 | Loss: 0.00067423
Iteration 12/25 | Loss: 0.00067423
Iteration 13/25 | Loss: 0.00067423
Iteration 14/25 | Loss: 0.00067423
Iteration 15/25 | Loss: 0.00067423
Iteration 16/25 | Loss: 0.00067423
Iteration 17/25 | Loss: 0.00067423
Iteration 18/25 | Loss: 0.00067423
Iteration 19/25 | Loss: 0.00067423
Iteration 20/25 | Loss: 0.00067423
Iteration 21/25 | Loss: 0.00067423
Iteration 22/25 | Loss: 0.00067423
Iteration 23/25 | Loss: 0.00067423
Iteration 24/25 | Loss: 0.00067423
Iteration 25/25 | Loss: 0.00067423

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067423
Iteration 2/1000 | Loss: 0.00002206
Iteration 3/1000 | Loss: 0.00001554
Iteration 4/1000 | Loss: 0.00001415
Iteration 5/1000 | Loss: 0.00001347
Iteration 6/1000 | Loss: 0.00001308
Iteration 7/1000 | Loss: 0.00001275
Iteration 8/1000 | Loss: 0.00001258
Iteration 9/1000 | Loss: 0.00001249
Iteration 10/1000 | Loss: 0.00001234
Iteration 11/1000 | Loss: 0.00001229
Iteration 12/1000 | Loss: 0.00001224
Iteration 13/1000 | Loss: 0.00001223
Iteration 14/1000 | Loss: 0.00001223
Iteration 15/1000 | Loss: 0.00001220
Iteration 16/1000 | Loss: 0.00001219
Iteration 17/1000 | Loss: 0.00001219
Iteration 18/1000 | Loss: 0.00001218
Iteration 19/1000 | Loss: 0.00001214
Iteration 20/1000 | Loss: 0.00001205
Iteration 21/1000 | Loss: 0.00001203
Iteration 22/1000 | Loss: 0.00001202
Iteration 23/1000 | Loss: 0.00001202
Iteration 24/1000 | Loss: 0.00001201
Iteration 25/1000 | Loss: 0.00001200
Iteration 26/1000 | Loss: 0.00001199
Iteration 27/1000 | Loss: 0.00001199
Iteration 28/1000 | Loss: 0.00001199
Iteration 29/1000 | Loss: 0.00001199
Iteration 30/1000 | Loss: 0.00001198
Iteration 31/1000 | Loss: 0.00001198
Iteration 32/1000 | Loss: 0.00001197
Iteration 33/1000 | Loss: 0.00001197
Iteration 34/1000 | Loss: 0.00001196
Iteration 35/1000 | Loss: 0.00001196
Iteration 36/1000 | Loss: 0.00001195
Iteration 37/1000 | Loss: 0.00001193
Iteration 38/1000 | Loss: 0.00001191
Iteration 39/1000 | Loss: 0.00001191
Iteration 40/1000 | Loss: 0.00001190
Iteration 41/1000 | Loss: 0.00001190
Iteration 42/1000 | Loss: 0.00001190
Iteration 43/1000 | Loss: 0.00001189
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001189
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001188
Iteration 49/1000 | Loss: 0.00001188
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001187
Iteration 52/1000 | Loss: 0.00001187
Iteration 53/1000 | Loss: 0.00001187
Iteration 54/1000 | Loss: 0.00001187
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001186
Iteration 57/1000 | Loss: 0.00001186
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001185
Iteration 60/1000 | Loss: 0.00001185
Iteration 61/1000 | Loss: 0.00001184
Iteration 62/1000 | Loss: 0.00001184
Iteration 63/1000 | Loss: 0.00001183
Iteration 64/1000 | Loss: 0.00001183
Iteration 65/1000 | Loss: 0.00001183
Iteration 66/1000 | Loss: 0.00001183
Iteration 67/1000 | Loss: 0.00001183
Iteration 68/1000 | Loss: 0.00001183
Iteration 69/1000 | Loss: 0.00001182
Iteration 70/1000 | Loss: 0.00001182
Iteration 71/1000 | Loss: 0.00001182
Iteration 72/1000 | Loss: 0.00001182
Iteration 73/1000 | Loss: 0.00001181
Iteration 74/1000 | Loss: 0.00001181
Iteration 75/1000 | Loss: 0.00001181
Iteration 76/1000 | Loss: 0.00001180
Iteration 77/1000 | Loss: 0.00001179
Iteration 78/1000 | Loss: 0.00001177
Iteration 79/1000 | Loss: 0.00001176
Iteration 80/1000 | Loss: 0.00001176
Iteration 81/1000 | Loss: 0.00001176
Iteration 82/1000 | Loss: 0.00001175
Iteration 83/1000 | Loss: 0.00001175
Iteration 84/1000 | Loss: 0.00001175
Iteration 85/1000 | Loss: 0.00001174
Iteration 86/1000 | Loss: 0.00001174
Iteration 87/1000 | Loss: 0.00001174
Iteration 88/1000 | Loss: 0.00001173
Iteration 89/1000 | Loss: 0.00001173
Iteration 90/1000 | Loss: 0.00001173
Iteration 91/1000 | Loss: 0.00001173
Iteration 92/1000 | Loss: 0.00001173
Iteration 93/1000 | Loss: 0.00001173
Iteration 94/1000 | Loss: 0.00001173
Iteration 95/1000 | Loss: 0.00001173
Iteration 96/1000 | Loss: 0.00001172
Iteration 97/1000 | Loss: 0.00001172
Iteration 98/1000 | Loss: 0.00001172
Iteration 99/1000 | Loss: 0.00001172
Iteration 100/1000 | Loss: 0.00001172
Iteration 101/1000 | Loss: 0.00001171
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001171
Iteration 104/1000 | Loss: 0.00001171
Iteration 105/1000 | Loss: 0.00001171
Iteration 106/1000 | Loss: 0.00001171
Iteration 107/1000 | Loss: 0.00001171
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001170
Iteration 110/1000 | Loss: 0.00001170
Iteration 111/1000 | Loss: 0.00001170
Iteration 112/1000 | Loss: 0.00001170
Iteration 113/1000 | Loss: 0.00001170
Iteration 114/1000 | Loss: 0.00001170
Iteration 115/1000 | Loss: 0.00001170
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001170
Iteration 119/1000 | Loss: 0.00001170
Iteration 120/1000 | Loss: 0.00001170
Iteration 121/1000 | Loss: 0.00001170
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Iteration 124/1000 | Loss: 0.00001169
Iteration 125/1000 | Loss: 0.00001169
Iteration 126/1000 | Loss: 0.00001169
Iteration 127/1000 | Loss: 0.00001169
Iteration 128/1000 | Loss: 0.00001169
Iteration 129/1000 | Loss: 0.00001169
Iteration 130/1000 | Loss: 0.00001169
Iteration 131/1000 | Loss: 0.00001169
Iteration 132/1000 | Loss: 0.00001169
Iteration 133/1000 | Loss: 0.00001169
Iteration 134/1000 | Loss: 0.00001168
Iteration 135/1000 | Loss: 0.00001168
Iteration 136/1000 | Loss: 0.00001168
Iteration 137/1000 | Loss: 0.00001168
Iteration 138/1000 | Loss: 0.00001168
Iteration 139/1000 | Loss: 0.00001167
Iteration 140/1000 | Loss: 0.00001166
Iteration 141/1000 | Loss: 0.00001166
Iteration 142/1000 | Loss: 0.00001166
Iteration 143/1000 | Loss: 0.00001166
Iteration 144/1000 | Loss: 0.00001166
Iteration 145/1000 | Loss: 0.00001165
Iteration 146/1000 | Loss: 0.00001165
Iteration 147/1000 | Loss: 0.00001165
Iteration 148/1000 | Loss: 0.00001165
Iteration 149/1000 | Loss: 0.00001165
Iteration 150/1000 | Loss: 0.00001164
Iteration 151/1000 | Loss: 0.00001164
Iteration 152/1000 | Loss: 0.00001164
Iteration 153/1000 | Loss: 0.00001164
Iteration 154/1000 | Loss: 0.00001164
Iteration 155/1000 | Loss: 0.00001164
Iteration 156/1000 | Loss: 0.00001164
Iteration 157/1000 | Loss: 0.00001163
Iteration 158/1000 | Loss: 0.00001163
Iteration 159/1000 | Loss: 0.00001163
Iteration 160/1000 | Loss: 0.00001163
Iteration 161/1000 | Loss: 0.00001163
Iteration 162/1000 | Loss: 0.00001163
Iteration 163/1000 | Loss: 0.00001162
Iteration 164/1000 | Loss: 0.00001162
Iteration 165/1000 | Loss: 0.00001162
Iteration 166/1000 | Loss: 0.00001162
Iteration 167/1000 | Loss: 0.00001162
Iteration 168/1000 | Loss: 0.00001162
Iteration 169/1000 | Loss: 0.00001162
Iteration 170/1000 | Loss: 0.00001162
Iteration 171/1000 | Loss: 0.00001162
Iteration 172/1000 | Loss: 0.00001162
Iteration 173/1000 | Loss: 0.00001162
Iteration 174/1000 | Loss: 0.00001162
Iteration 175/1000 | Loss: 0.00001162
Iteration 176/1000 | Loss: 0.00001162
Iteration 177/1000 | Loss: 0.00001162
Iteration 178/1000 | Loss: 0.00001161
Iteration 179/1000 | Loss: 0.00001161
Iteration 180/1000 | Loss: 0.00001161
Iteration 181/1000 | Loss: 0.00001161
Iteration 182/1000 | Loss: 0.00001161
Iteration 183/1000 | Loss: 0.00001161
Iteration 184/1000 | Loss: 0.00001161
Iteration 185/1000 | Loss: 0.00001161
Iteration 186/1000 | Loss: 0.00001161
Iteration 187/1000 | Loss: 0.00001161
Iteration 188/1000 | Loss: 0.00001161
Iteration 189/1000 | Loss: 0.00001161
Iteration 190/1000 | Loss: 0.00001160
Iteration 191/1000 | Loss: 0.00001160
Iteration 192/1000 | Loss: 0.00001160
Iteration 193/1000 | Loss: 0.00001160
Iteration 194/1000 | Loss: 0.00001160
Iteration 195/1000 | Loss: 0.00001160
Iteration 196/1000 | Loss: 0.00001160
Iteration 197/1000 | Loss: 0.00001160
Iteration 198/1000 | Loss: 0.00001160
Iteration 199/1000 | Loss: 0.00001160
Iteration 200/1000 | Loss: 0.00001160
Iteration 201/1000 | Loss: 0.00001160
Iteration 202/1000 | Loss: 0.00001160
Iteration 203/1000 | Loss: 0.00001160
Iteration 204/1000 | Loss: 0.00001160
Iteration 205/1000 | Loss: 0.00001160
Iteration 206/1000 | Loss: 0.00001160
Iteration 207/1000 | Loss: 0.00001160
Iteration 208/1000 | Loss: 0.00001160
Iteration 209/1000 | Loss: 0.00001160
Iteration 210/1000 | Loss: 0.00001160
Iteration 211/1000 | Loss: 0.00001160
Iteration 212/1000 | Loss: 0.00001159
Iteration 213/1000 | Loss: 0.00001159
Iteration 214/1000 | Loss: 0.00001159
Iteration 215/1000 | Loss: 0.00001159
Iteration 216/1000 | Loss: 0.00001159
Iteration 217/1000 | Loss: 0.00001159
Iteration 218/1000 | Loss: 0.00001159
Iteration 219/1000 | Loss: 0.00001159
Iteration 220/1000 | Loss: 0.00001159
Iteration 221/1000 | Loss: 0.00001159
Iteration 222/1000 | Loss: 0.00001159
Iteration 223/1000 | Loss: 0.00001159
Iteration 224/1000 | Loss: 0.00001159
Iteration 225/1000 | Loss: 0.00001159
Iteration 226/1000 | Loss: 0.00001159
Iteration 227/1000 | Loss: 0.00001159
Iteration 228/1000 | Loss: 0.00001159
Iteration 229/1000 | Loss: 0.00001158
Iteration 230/1000 | Loss: 0.00001158
Iteration 231/1000 | Loss: 0.00001158
Iteration 232/1000 | Loss: 0.00001158
Iteration 233/1000 | Loss: 0.00001158
Iteration 234/1000 | Loss: 0.00001158
Iteration 235/1000 | Loss: 0.00001158
Iteration 236/1000 | Loss: 0.00001158
Iteration 237/1000 | Loss: 0.00001158
Iteration 238/1000 | Loss: 0.00001158
Iteration 239/1000 | Loss: 0.00001158
Iteration 240/1000 | Loss: 0.00001158
Iteration 241/1000 | Loss: 0.00001158
Iteration 242/1000 | Loss: 0.00001158
Iteration 243/1000 | Loss: 0.00001158
Iteration 244/1000 | Loss: 0.00001158
Iteration 245/1000 | Loss: 0.00001158
Iteration 246/1000 | Loss: 0.00001158
Iteration 247/1000 | Loss: 0.00001158
Iteration 248/1000 | Loss: 0.00001158
Iteration 249/1000 | Loss: 0.00001158
Iteration 250/1000 | Loss: 0.00001158
Iteration 251/1000 | Loss: 0.00001158
Iteration 252/1000 | Loss: 0.00001158
Iteration 253/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.1583205377974082e-05, 1.1583205377974082e-05, 1.1583205377974082e-05, 1.1583205377974082e-05, 1.1583205377974082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1583205377974082e-05

Optimization complete. Final v2v error: 2.913816452026367 mm

Highest mean error: 3.3462929725646973 mm for frame 110

Lowest mean error: 2.763406753540039 mm for frame 40

Saving results

Total time: 41.03591823577881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859718
Iteration 2/25 | Loss: 0.00149313
Iteration 3/25 | Loss: 0.00131620
Iteration 4/25 | Loss: 0.00128550
Iteration 5/25 | Loss: 0.00128258
Iteration 6/25 | Loss: 0.00128161
Iteration 7/25 | Loss: 0.00128150
Iteration 8/25 | Loss: 0.00128150
Iteration 9/25 | Loss: 0.00128150
Iteration 10/25 | Loss: 0.00128150
Iteration 11/25 | Loss: 0.00128150
Iteration 12/25 | Loss: 0.00128150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012815030058845878, 0.0012815030058845878, 0.0012815030058845878, 0.0012815030058845878, 0.0012815030058845878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012815030058845878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44335389
Iteration 2/25 | Loss: 0.00060752
Iteration 3/25 | Loss: 0.00060752
Iteration 4/25 | Loss: 0.00060752
Iteration 5/25 | Loss: 0.00060752
Iteration 6/25 | Loss: 0.00060752
Iteration 7/25 | Loss: 0.00060752
Iteration 8/25 | Loss: 0.00060752
Iteration 9/25 | Loss: 0.00060752
Iteration 10/25 | Loss: 0.00060752
Iteration 11/25 | Loss: 0.00060752
Iteration 12/25 | Loss: 0.00060752
Iteration 13/25 | Loss: 0.00060752
Iteration 14/25 | Loss: 0.00060752
Iteration 15/25 | Loss: 0.00060752
Iteration 16/25 | Loss: 0.00060752
Iteration 17/25 | Loss: 0.00060752
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006075190613046288, 0.0006075190613046288, 0.0006075190613046288, 0.0006075190613046288, 0.0006075190613046288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006075190613046288

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060752
Iteration 2/1000 | Loss: 0.00009665
Iteration 3/1000 | Loss: 0.00005597
Iteration 4/1000 | Loss: 0.00004017
Iteration 5/1000 | Loss: 0.00003620
Iteration 6/1000 | Loss: 0.00003427
Iteration 7/1000 | Loss: 0.00003319
Iteration 8/1000 | Loss: 0.00003252
Iteration 9/1000 | Loss: 0.00003200
Iteration 10/1000 | Loss: 0.00003174
Iteration 11/1000 | Loss: 0.00003168
Iteration 12/1000 | Loss: 0.00003142
Iteration 13/1000 | Loss: 0.00003121
Iteration 14/1000 | Loss: 0.00003121
Iteration 15/1000 | Loss: 0.00003114
Iteration 16/1000 | Loss: 0.00003104
Iteration 17/1000 | Loss: 0.00003099
Iteration 18/1000 | Loss: 0.00003095
Iteration 19/1000 | Loss: 0.00003085
Iteration 20/1000 | Loss: 0.00003081
Iteration 21/1000 | Loss: 0.00003079
Iteration 22/1000 | Loss: 0.00003077
Iteration 23/1000 | Loss: 0.00003071
Iteration 24/1000 | Loss: 0.00003071
Iteration 25/1000 | Loss: 0.00003067
Iteration 26/1000 | Loss: 0.00003067
Iteration 27/1000 | Loss: 0.00003066
Iteration 28/1000 | Loss: 0.00003066
Iteration 29/1000 | Loss: 0.00003066
Iteration 30/1000 | Loss: 0.00003065
Iteration 31/1000 | Loss: 0.00003065
Iteration 32/1000 | Loss: 0.00003064
Iteration 33/1000 | Loss: 0.00003064
Iteration 34/1000 | Loss: 0.00003064
Iteration 35/1000 | Loss: 0.00003063
Iteration 36/1000 | Loss: 0.00003063
Iteration 37/1000 | Loss: 0.00003063
Iteration 38/1000 | Loss: 0.00003062
Iteration 39/1000 | Loss: 0.00003062
Iteration 40/1000 | Loss: 0.00003062
Iteration 41/1000 | Loss: 0.00003061
Iteration 42/1000 | Loss: 0.00003061
Iteration 43/1000 | Loss: 0.00003060
Iteration 44/1000 | Loss: 0.00003060
Iteration 45/1000 | Loss: 0.00003059
Iteration 46/1000 | Loss: 0.00003059
Iteration 47/1000 | Loss: 0.00003059
Iteration 48/1000 | Loss: 0.00003058
Iteration 49/1000 | Loss: 0.00003056
Iteration 50/1000 | Loss: 0.00003056
Iteration 51/1000 | Loss: 0.00003055
Iteration 52/1000 | Loss: 0.00003055
Iteration 53/1000 | Loss: 0.00003054
Iteration 54/1000 | Loss: 0.00003054
Iteration 55/1000 | Loss: 0.00003054
Iteration 56/1000 | Loss: 0.00003054
Iteration 57/1000 | Loss: 0.00003053
Iteration 58/1000 | Loss: 0.00003053
Iteration 59/1000 | Loss: 0.00003053
Iteration 60/1000 | Loss: 0.00003053
Iteration 61/1000 | Loss: 0.00003052
Iteration 62/1000 | Loss: 0.00003052
Iteration 63/1000 | Loss: 0.00003052
Iteration 64/1000 | Loss: 0.00003051
Iteration 65/1000 | Loss: 0.00003051
Iteration 66/1000 | Loss: 0.00003050
Iteration 67/1000 | Loss: 0.00003050
Iteration 68/1000 | Loss: 0.00003050
Iteration 69/1000 | Loss: 0.00003049
Iteration 70/1000 | Loss: 0.00003049
Iteration 71/1000 | Loss: 0.00003049
Iteration 72/1000 | Loss: 0.00003049
Iteration 73/1000 | Loss: 0.00003049
Iteration 74/1000 | Loss: 0.00003049
Iteration 75/1000 | Loss: 0.00003048
Iteration 76/1000 | Loss: 0.00003048
Iteration 77/1000 | Loss: 0.00003048
Iteration 78/1000 | Loss: 0.00003047
Iteration 79/1000 | Loss: 0.00003047
Iteration 80/1000 | Loss: 0.00003047
Iteration 81/1000 | Loss: 0.00003047
Iteration 82/1000 | Loss: 0.00003047
Iteration 83/1000 | Loss: 0.00003046
Iteration 84/1000 | Loss: 0.00003046
Iteration 85/1000 | Loss: 0.00003046
Iteration 86/1000 | Loss: 0.00003046
Iteration 87/1000 | Loss: 0.00003045
Iteration 88/1000 | Loss: 0.00003045
Iteration 89/1000 | Loss: 0.00003045
Iteration 90/1000 | Loss: 0.00003045
Iteration 91/1000 | Loss: 0.00003045
Iteration 92/1000 | Loss: 0.00003045
Iteration 93/1000 | Loss: 0.00003045
Iteration 94/1000 | Loss: 0.00003045
Iteration 95/1000 | Loss: 0.00003045
Iteration 96/1000 | Loss: 0.00003045
Iteration 97/1000 | Loss: 0.00003044
Iteration 98/1000 | Loss: 0.00003044
Iteration 99/1000 | Loss: 0.00003044
Iteration 100/1000 | Loss: 0.00003044
Iteration 101/1000 | Loss: 0.00003043
Iteration 102/1000 | Loss: 0.00003043
Iteration 103/1000 | Loss: 0.00003043
Iteration 104/1000 | Loss: 0.00003043
Iteration 105/1000 | Loss: 0.00003042
Iteration 106/1000 | Loss: 0.00003042
Iteration 107/1000 | Loss: 0.00003042
Iteration 108/1000 | Loss: 0.00003042
Iteration 109/1000 | Loss: 0.00003041
Iteration 110/1000 | Loss: 0.00003041
Iteration 111/1000 | Loss: 0.00003041
Iteration 112/1000 | Loss: 0.00003041
Iteration 113/1000 | Loss: 0.00003041
Iteration 114/1000 | Loss: 0.00003041
Iteration 115/1000 | Loss: 0.00003041
Iteration 116/1000 | Loss: 0.00003041
Iteration 117/1000 | Loss: 0.00003041
Iteration 118/1000 | Loss: 0.00003041
Iteration 119/1000 | Loss: 0.00003040
Iteration 120/1000 | Loss: 0.00003040
Iteration 121/1000 | Loss: 0.00003040
Iteration 122/1000 | Loss: 0.00003040
Iteration 123/1000 | Loss: 0.00003040
Iteration 124/1000 | Loss: 0.00003040
Iteration 125/1000 | Loss: 0.00003040
Iteration 126/1000 | Loss: 0.00003040
Iteration 127/1000 | Loss: 0.00003040
Iteration 128/1000 | Loss: 0.00003040
Iteration 129/1000 | Loss: 0.00003040
Iteration 130/1000 | Loss: 0.00003040
Iteration 131/1000 | Loss: 0.00003040
Iteration 132/1000 | Loss: 0.00003040
Iteration 133/1000 | Loss: 0.00003039
Iteration 134/1000 | Loss: 0.00003039
Iteration 135/1000 | Loss: 0.00003039
Iteration 136/1000 | Loss: 0.00003039
Iteration 137/1000 | Loss: 0.00003039
Iteration 138/1000 | Loss: 0.00003039
Iteration 139/1000 | Loss: 0.00003039
Iteration 140/1000 | Loss: 0.00003039
Iteration 141/1000 | Loss: 0.00003039
Iteration 142/1000 | Loss: 0.00003039
Iteration 143/1000 | Loss: 0.00003039
Iteration 144/1000 | Loss: 0.00003039
Iteration 145/1000 | Loss: 0.00003038
Iteration 146/1000 | Loss: 0.00003038
Iteration 147/1000 | Loss: 0.00003038
Iteration 148/1000 | Loss: 0.00003038
Iteration 149/1000 | Loss: 0.00003038
Iteration 150/1000 | Loss: 0.00003038
Iteration 151/1000 | Loss: 0.00003038
Iteration 152/1000 | Loss: 0.00003038
Iteration 153/1000 | Loss: 0.00003038
Iteration 154/1000 | Loss: 0.00003038
Iteration 155/1000 | Loss: 0.00003038
Iteration 156/1000 | Loss: 0.00003037
Iteration 157/1000 | Loss: 0.00003037
Iteration 158/1000 | Loss: 0.00003037
Iteration 159/1000 | Loss: 0.00003037
Iteration 160/1000 | Loss: 0.00003037
Iteration 161/1000 | Loss: 0.00003037
Iteration 162/1000 | Loss: 0.00003037
Iteration 163/1000 | Loss: 0.00003037
Iteration 164/1000 | Loss: 0.00003037
Iteration 165/1000 | Loss: 0.00003037
Iteration 166/1000 | Loss: 0.00003037
Iteration 167/1000 | Loss: 0.00003037
Iteration 168/1000 | Loss: 0.00003037
Iteration 169/1000 | Loss: 0.00003037
Iteration 170/1000 | Loss: 0.00003037
Iteration 171/1000 | Loss: 0.00003037
Iteration 172/1000 | Loss: 0.00003037
Iteration 173/1000 | Loss: 0.00003037
Iteration 174/1000 | Loss: 0.00003037
Iteration 175/1000 | Loss: 0.00003037
Iteration 176/1000 | Loss: 0.00003037
Iteration 177/1000 | Loss: 0.00003036
Iteration 178/1000 | Loss: 0.00003036
Iteration 179/1000 | Loss: 0.00003036
Iteration 180/1000 | Loss: 0.00003036
Iteration 181/1000 | Loss: 0.00003036
Iteration 182/1000 | Loss: 0.00003036
Iteration 183/1000 | Loss: 0.00003036
Iteration 184/1000 | Loss: 0.00003036
Iteration 185/1000 | Loss: 0.00003036
Iteration 186/1000 | Loss: 0.00003036
Iteration 187/1000 | Loss: 0.00003036
Iteration 188/1000 | Loss: 0.00003036
Iteration 189/1000 | Loss: 0.00003036
Iteration 190/1000 | Loss: 0.00003036
Iteration 191/1000 | Loss: 0.00003036
Iteration 192/1000 | Loss: 0.00003036
Iteration 193/1000 | Loss: 0.00003036
Iteration 194/1000 | Loss: 0.00003036
Iteration 195/1000 | Loss: 0.00003036
Iteration 196/1000 | Loss: 0.00003036
Iteration 197/1000 | Loss: 0.00003036
Iteration 198/1000 | Loss: 0.00003036
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [3.0363329642568715e-05, 3.0363329642568715e-05, 3.0363329642568715e-05, 3.0363329642568715e-05, 3.0363329642568715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0363329642568715e-05

Optimization complete. Final v2v error: 4.580076217651367 mm

Highest mean error: 5.0249810218811035 mm for frame 76

Lowest mean error: 3.627105474472046 mm for frame 7

Saving results

Total time: 43.278427839279175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491040
Iteration 2/25 | Loss: 0.00142280
Iteration 3/25 | Loss: 0.00130759
Iteration 4/25 | Loss: 0.00128810
Iteration 5/25 | Loss: 0.00128100
Iteration 6/25 | Loss: 0.00128034
Iteration 7/25 | Loss: 0.00128034
Iteration 8/25 | Loss: 0.00128034
Iteration 9/25 | Loss: 0.00128034
Iteration 10/25 | Loss: 0.00128034
Iteration 11/25 | Loss: 0.00128034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012803444406017661, 0.0012803444406017661, 0.0012803444406017661, 0.0012803444406017661, 0.0012803444406017661]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012803444406017661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46693337
Iteration 2/25 | Loss: 0.00069447
Iteration 3/25 | Loss: 0.00069444
Iteration 4/25 | Loss: 0.00069444
Iteration 5/25 | Loss: 0.00069444
Iteration 6/25 | Loss: 0.00069444
Iteration 7/25 | Loss: 0.00069444
Iteration 8/25 | Loss: 0.00069444
Iteration 9/25 | Loss: 0.00069444
Iteration 10/25 | Loss: 0.00069444
Iteration 11/25 | Loss: 0.00069444
Iteration 12/25 | Loss: 0.00069444
Iteration 13/25 | Loss: 0.00069444
Iteration 14/25 | Loss: 0.00069444
Iteration 15/25 | Loss: 0.00069444
Iteration 16/25 | Loss: 0.00069444
Iteration 17/25 | Loss: 0.00069444
Iteration 18/25 | Loss: 0.00069444
Iteration 19/25 | Loss: 0.00069444
Iteration 20/25 | Loss: 0.00069444
Iteration 21/25 | Loss: 0.00069444
Iteration 22/25 | Loss: 0.00069444
Iteration 23/25 | Loss: 0.00069444
Iteration 24/25 | Loss: 0.00069444
Iteration 25/25 | Loss: 0.00069444

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069444
Iteration 2/1000 | Loss: 0.00003888
Iteration 3/1000 | Loss: 0.00002752
Iteration 4/1000 | Loss: 0.00002490
Iteration 5/1000 | Loss: 0.00002387
Iteration 6/1000 | Loss: 0.00002288
Iteration 7/1000 | Loss: 0.00002228
Iteration 8/1000 | Loss: 0.00002185
Iteration 9/1000 | Loss: 0.00002146
Iteration 10/1000 | Loss: 0.00002117
Iteration 11/1000 | Loss: 0.00002104
Iteration 12/1000 | Loss: 0.00002101
Iteration 13/1000 | Loss: 0.00002100
Iteration 14/1000 | Loss: 0.00002097
Iteration 15/1000 | Loss: 0.00002096
Iteration 16/1000 | Loss: 0.00002096
Iteration 17/1000 | Loss: 0.00002092
Iteration 18/1000 | Loss: 0.00002089
Iteration 19/1000 | Loss: 0.00002088
Iteration 20/1000 | Loss: 0.00002086
Iteration 21/1000 | Loss: 0.00002085
Iteration 22/1000 | Loss: 0.00002085
Iteration 23/1000 | Loss: 0.00002084
Iteration 24/1000 | Loss: 0.00002083
Iteration 25/1000 | Loss: 0.00002083
Iteration 26/1000 | Loss: 0.00002082
Iteration 27/1000 | Loss: 0.00002081
Iteration 28/1000 | Loss: 0.00002081
Iteration 29/1000 | Loss: 0.00002080
Iteration 30/1000 | Loss: 0.00002080
Iteration 31/1000 | Loss: 0.00002080
Iteration 32/1000 | Loss: 0.00002079
Iteration 33/1000 | Loss: 0.00002078
Iteration 34/1000 | Loss: 0.00002078
Iteration 35/1000 | Loss: 0.00002076
Iteration 36/1000 | Loss: 0.00002075
Iteration 37/1000 | Loss: 0.00002074
Iteration 38/1000 | Loss: 0.00002074
Iteration 39/1000 | Loss: 0.00002073
Iteration 40/1000 | Loss: 0.00002073
Iteration 41/1000 | Loss: 0.00002073
Iteration 42/1000 | Loss: 0.00002071
Iteration 43/1000 | Loss: 0.00002070
Iteration 44/1000 | Loss: 0.00002069
Iteration 45/1000 | Loss: 0.00002069
Iteration 46/1000 | Loss: 0.00002068
Iteration 47/1000 | Loss: 0.00002064
Iteration 48/1000 | Loss: 0.00002064
Iteration 49/1000 | Loss: 0.00002064
Iteration 50/1000 | Loss: 0.00002063
Iteration 51/1000 | Loss: 0.00002063
Iteration 52/1000 | Loss: 0.00002063
Iteration 53/1000 | Loss: 0.00002060
Iteration 54/1000 | Loss: 0.00002060
Iteration 55/1000 | Loss: 0.00002059
Iteration 56/1000 | Loss: 0.00002058
Iteration 57/1000 | Loss: 0.00002058
Iteration 58/1000 | Loss: 0.00002058
Iteration 59/1000 | Loss: 0.00002058
Iteration 60/1000 | Loss: 0.00002058
Iteration 61/1000 | Loss: 0.00002058
Iteration 62/1000 | Loss: 0.00002058
Iteration 63/1000 | Loss: 0.00002057
Iteration 64/1000 | Loss: 0.00002057
Iteration 65/1000 | Loss: 0.00002056
Iteration 66/1000 | Loss: 0.00002055
Iteration 67/1000 | Loss: 0.00002054
Iteration 68/1000 | Loss: 0.00002054
Iteration 69/1000 | Loss: 0.00002054
Iteration 70/1000 | Loss: 0.00002054
Iteration 71/1000 | Loss: 0.00002054
Iteration 72/1000 | Loss: 0.00002054
Iteration 73/1000 | Loss: 0.00002054
Iteration 74/1000 | Loss: 0.00002054
Iteration 75/1000 | Loss: 0.00002054
Iteration 76/1000 | Loss: 0.00002053
Iteration 77/1000 | Loss: 0.00002053
Iteration 78/1000 | Loss: 0.00002053
Iteration 79/1000 | Loss: 0.00002052
Iteration 80/1000 | Loss: 0.00002051
Iteration 81/1000 | Loss: 0.00002051
Iteration 82/1000 | Loss: 0.00002051
Iteration 83/1000 | Loss: 0.00002050
Iteration 84/1000 | Loss: 0.00002050
Iteration 85/1000 | Loss: 0.00002050
Iteration 86/1000 | Loss: 0.00002050
Iteration 87/1000 | Loss: 0.00002050
Iteration 88/1000 | Loss: 0.00002050
Iteration 89/1000 | Loss: 0.00002050
Iteration 90/1000 | Loss: 0.00002050
Iteration 91/1000 | Loss: 0.00002050
Iteration 92/1000 | Loss: 0.00002050
Iteration 93/1000 | Loss: 0.00002050
Iteration 94/1000 | Loss: 0.00002050
Iteration 95/1000 | Loss: 0.00002050
Iteration 96/1000 | Loss: 0.00002050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.050117836915888e-05, 2.050117836915888e-05, 2.050117836915888e-05, 2.050117836915888e-05, 2.050117836915888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.050117836915888e-05

Optimization complete. Final v2v error: 3.7274956703186035 mm

Highest mean error: 5.210144519805908 mm for frame 215

Lowest mean error: 3.1134231090545654 mm for frame 42

Saving results

Total time: 37.51734733581543
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773906
Iteration 2/25 | Loss: 0.00167250
Iteration 3/25 | Loss: 0.00146198
Iteration 4/25 | Loss: 0.00143001
Iteration 5/25 | Loss: 0.00141969
Iteration 6/25 | Loss: 0.00141728
Iteration 7/25 | Loss: 0.00141680
Iteration 8/25 | Loss: 0.00141680
Iteration 9/25 | Loss: 0.00141680
Iteration 10/25 | Loss: 0.00141680
Iteration 11/25 | Loss: 0.00141680
Iteration 12/25 | Loss: 0.00141680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014167979825288057, 0.0014167979825288057, 0.0014167979825288057, 0.0014167979825288057, 0.0014167979825288057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014167979825288057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.93222713
Iteration 2/25 | Loss: 0.00099272
Iteration 3/25 | Loss: 0.00099245
Iteration 4/25 | Loss: 0.00099245
Iteration 5/25 | Loss: 0.00099245
Iteration 6/25 | Loss: 0.00099245
Iteration 7/25 | Loss: 0.00099245
Iteration 8/25 | Loss: 0.00099245
Iteration 9/25 | Loss: 0.00099245
Iteration 10/25 | Loss: 0.00099245
Iteration 11/25 | Loss: 0.00099245
Iteration 12/25 | Loss: 0.00099245
Iteration 13/25 | Loss: 0.00099245
Iteration 14/25 | Loss: 0.00099245
Iteration 15/25 | Loss: 0.00099245
Iteration 16/25 | Loss: 0.00099245
Iteration 17/25 | Loss: 0.00099245
Iteration 18/25 | Loss: 0.00099245
Iteration 19/25 | Loss: 0.00099245
Iteration 20/25 | Loss: 0.00099245
Iteration 21/25 | Loss: 0.00099245
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009924467885866761, 0.0009924467885866761, 0.0009924467885866761, 0.0009924467885866761, 0.0009924467885866761]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009924467885866761

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099245
Iteration 2/1000 | Loss: 0.00007877
Iteration 3/1000 | Loss: 0.00005286
Iteration 4/1000 | Loss: 0.00004628
Iteration 5/1000 | Loss: 0.00004405
Iteration 6/1000 | Loss: 0.00004254
Iteration 7/1000 | Loss: 0.00004166
Iteration 8/1000 | Loss: 0.00004072
Iteration 9/1000 | Loss: 0.00004011
Iteration 10/1000 | Loss: 0.00003965
Iteration 11/1000 | Loss: 0.00003921
Iteration 12/1000 | Loss: 0.00003891
Iteration 13/1000 | Loss: 0.00003866
Iteration 14/1000 | Loss: 0.00003842
Iteration 15/1000 | Loss: 0.00003824
Iteration 16/1000 | Loss: 0.00003813
Iteration 17/1000 | Loss: 0.00003810
Iteration 18/1000 | Loss: 0.00003801
Iteration 19/1000 | Loss: 0.00003793
Iteration 20/1000 | Loss: 0.00003786
Iteration 21/1000 | Loss: 0.00003786
Iteration 22/1000 | Loss: 0.00003784
Iteration 23/1000 | Loss: 0.00003783
Iteration 24/1000 | Loss: 0.00003782
Iteration 25/1000 | Loss: 0.00003782
Iteration 26/1000 | Loss: 0.00003780
Iteration 27/1000 | Loss: 0.00003777
Iteration 28/1000 | Loss: 0.00003776
Iteration 29/1000 | Loss: 0.00003772
Iteration 30/1000 | Loss: 0.00003772
Iteration 31/1000 | Loss: 0.00003769
Iteration 32/1000 | Loss: 0.00003769
Iteration 33/1000 | Loss: 0.00003768
Iteration 34/1000 | Loss: 0.00003768
Iteration 35/1000 | Loss: 0.00003767
Iteration 36/1000 | Loss: 0.00003767
Iteration 37/1000 | Loss: 0.00003767
Iteration 38/1000 | Loss: 0.00003767
Iteration 39/1000 | Loss: 0.00003766
Iteration 40/1000 | Loss: 0.00003766
Iteration 41/1000 | Loss: 0.00003766
Iteration 42/1000 | Loss: 0.00003765
Iteration 43/1000 | Loss: 0.00003765
Iteration 44/1000 | Loss: 0.00003763
Iteration 45/1000 | Loss: 0.00003762
Iteration 46/1000 | Loss: 0.00003762
Iteration 47/1000 | Loss: 0.00003761
Iteration 48/1000 | Loss: 0.00003761
Iteration 49/1000 | Loss: 0.00003761
Iteration 50/1000 | Loss: 0.00003761
Iteration 51/1000 | Loss: 0.00003760
Iteration 52/1000 | Loss: 0.00003760
Iteration 53/1000 | Loss: 0.00003760
Iteration 54/1000 | Loss: 0.00003759
Iteration 55/1000 | Loss: 0.00003759
Iteration 56/1000 | Loss: 0.00003759
Iteration 57/1000 | Loss: 0.00003759
Iteration 58/1000 | Loss: 0.00003758
Iteration 59/1000 | Loss: 0.00003758
Iteration 60/1000 | Loss: 0.00003758
Iteration 61/1000 | Loss: 0.00003757
Iteration 62/1000 | Loss: 0.00003757
Iteration 63/1000 | Loss: 0.00003757
Iteration 64/1000 | Loss: 0.00003757
Iteration 65/1000 | Loss: 0.00003756
Iteration 66/1000 | Loss: 0.00003756
Iteration 67/1000 | Loss: 0.00003756
Iteration 68/1000 | Loss: 0.00003755
Iteration 69/1000 | Loss: 0.00003755
Iteration 70/1000 | Loss: 0.00003755
Iteration 71/1000 | Loss: 0.00003755
Iteration 72/1000 | Loss: 0.00003755
Iteration 73/1000 | Loss: 0.00003755
Iteration 74/1000 | Loss: 0.00003755
Iteration 75/1000 | Loss: 0.00003755
Iteration 76/1000 | Loss: 0.00003755
Iteration 77/1000 | Loss: 0.00003754
Iteration 78/1000 | Loss: 0.00003754
Iteration 79/1000 | Loss: 0.00003754
Iteration 80/1000 | Loss: 0.00003753
Iteration 81/1000 | Loss: 0.00003753
Iteration 82/1000 | Loss: 0.00003753
Iteration 83/1000 | Loss: 0.00003753
Iteration 84/1000 | Loss: 0.00003753
Iteration 85/1000 | Loss: 0.00003753
Iteration 86/1000 | Loss: 0.00003753
Iteration 87/1000 | Loss: 0.00003753
Iteration 88/1000 | Loss: 0.00003752
Iteration 89/1000 | Loss: 0.00003752
Iteration 90/1000 | Loss: 0.00003752
Iteration 91/1000 | Loss: 0.00003752
Iteration 92/1000 | Loss: 0.00003752
Iteration 93/1000 | Loss: 0.00003752
Iteration 94/1000 | Loss: 0.00003752
Iteration 95/1000 | Loss: 0.00003752
Iteration 96/1000 | Loss: 0.00003752
Iteration 97/1000 | Loss: 0.00003752
Iteration 98/1000 | Loss: 0.00003752
Iteration 99/1000 | Loss: 0.00003752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [3.752232078113593e-05, 3.752232078113593e-05, 3.752232078113593e-05, 3.752232078113593e-05, 3.752232078113593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.752232078113593e-05

Optimization complete. Final v2v error: 4.897989273071289 mm

Highest mean error: 7.593451976776123 mm for frame 145

Lowest mean error: 3.8520395755767822 mm for frame 86

Saving results

Total time: 49.853293895721436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00545496
Iteration 2/25 | Loss: 0.00126850
Iteration 3/25 | Loss: 0.00121127
Iteration 4/25 | Loss: 0.00120242
Iteration 5/25 | Loss: 0.00119940
Iteration 6/25 | Loss: 0.00119914
Iteration 7/25 | Loss: 0.00119914
Iteration 8/25 | Loss: 0.00119914
Iteration 9/25 | Loss: 0.00119914
Iteration 10/25 | Loss: 0.00119914
Iteration 11/25 | Loss: 0.00119914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011991390492767096, 0.0011991390492767096, 0.0011991390492767096, 0.0011991390492767096, 0.0011991390492767096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011991390492767096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.51325893
Iteration 2/25 | Loss: 0.00061485
Iteration 3/25 | Loss: 0.00061484
Iteration 4/25 | Loss: 0.00061484
Iteration 5/25 | Loss: 0.00061484
Iteration 6/25 | Loss: 0.00061484
Iteration 7/25 | Loss: 0.00061483
Iteration 8/25 | Loss: 0.00061483
Iteration 9/25 | Loss: 0.00061483
Iteration 10/25 | Loss: 0.00061483
Iteration 11/25 | Loss: 0.00061483
Iteration 12/25 | Loss: 0.00061483
Iteration 13/25 | Loss: 0.00061483
Iteration 14/25 | Loss: 0.00061483
Iteration 15/25 | Loss: 0.00061483
Iteration 16/25 | Loss: 0.00061483
Iteration 17/25 | Loss: 0.00061483
Iteration 18/25 | Loss: 0.00061483
Iteration 19/25 | Loss: 0.00061483
Iteration 20/25 | Loss: 0.00061483
Iteration 21/25 | Loss: 0.00061483
Iteration 22/25 | Loss: 0.00061483
Iteration 23/25 | Loss: 0.00061483
Iteration 24/25 | Loss: 0.00061483
Iteration 25/25 | Loss: 0.00061483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061483
Iteration 2/1000 | Loss: 0.00002567
Iteration 3/1000 | Loss: 0.00001876
Iteration 4/1000 | Loss: 0.00001734
Iteration 5/1000 | Loss: 0.00001650
Iteration 6/1000 | Loss: 0.00001601
Iteration 7/1000 | Loss: 0.00001554
Iteration 8/1000 | Loss: 0.00001544
Iteration 9/1000 | Loss: 0.00001522
Iteration 10/1000 | Loss: 0.00001497
Iteration 11/1000 | Loss: 0.00001479
Iteration 12/1000 | Loss: 0.00001479
Iteration 13/1000 | Loss: 0.00001475
Iteration 14/1000 | Loss: 0.00001466
Iteration 15/1000 | Loss: 0.00001461
Iteration 16/1000 | Loss: 0.00001460
Iteration 17/1000 | Loss: 0.00001459
Iteration 18/1000 | Loss: 0.00001455
Iteration 19/1000 | Loss: 0.00001447
Iteration 20/1000 | Loss: 0.00001446
Iteration 21/1000 | Loss: 0.00001445
Iteration 22/1000 | Loss: 0.00001443
Iteration 23/1000 | Loss: 0.00001438
Iteration 24/1000 | Loss: 0.00001438
Iteration 25/1000 | Loss: 0.00001438
Iteration 26/1000 | Loss: 0.00001437
Iteration 27/1000 | Loss: 0.00001436
Iteration 28/1000 | Loss: 0.00001436
Iteration 29/1000 | Loss: 0.00001434
Iteration 30/1000 | Loss: 0.00001433
Iteration 31/1000 | Loss: 0.00001433
Iteration 32/1000 | Loss: 0.00001433
Iteration 33/1000 | Loss: 0.00001432
Iteration 34/1000 | Loss: 0.00001431
Iteration 35/1000 | Loss: 0.00001431
Iteration 36/1000 | Loss: 0.00001430
Iteration 37/1000 | Loss: 0.00001430
Iteration 38/1000 | Loss: 0.00001426
Iteration 39/1000 | Loss: 0.00001422
Iteration 40/1000 | Loss: 0.00001422
Iteration 41/1000 | Loss: 0.00001421
Iteration 42/1000 | Loss: 0.00001418
Iteration 43/1000 | Loss: 0.00001417
Iteration 44/1000 | Loss: 0.00001416
Iteration 45/1000 | Loss: 0.00001415
Iteration 46/1000 | Loss: 0.00001414
Iteration 47/1000 | Loss: 0.00001414
Iteration 48/1000 | Loss: 0.00001413
Iteration 49/1000 | Loss: 0.00001413
Iteration 50/1000 | Loss: 0.00001410
Iteration 51/1000 | Loss: 0.00001410
Iteration 52/1000 | Loss: 0.00001408
Iteration 53/1000 | Loss: 0.00001407
Iteration 54/1000 | Loss: 0.00001407
Iteration 55/1000 | Loss: 0.00001406
Iteration 56/1000 | Loss: 0.00001406
Iteration 57/1000 | Loss: 0.00001405
Iteration 58/1000 | Loss: 0.00001405
Iteration 59/1000 | Loss: 0.00001405
Iteration 60/1000 | Loss: 0.00001404
Iteration 61/1000 | Loss: 0.00001404
Iteration 62/1000 | Loss: 0.00001404
Iteration 63/1000 | Loss: 0.00001403
Iteration 64/1000 | Loss: 0.00001403
Iteration 65/1000 | Loss: 0.00001402
Iteration 66/1000 | Loss: 0.00001402
Iteration 67/1000 | Loss: 0.00001402
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001401
Iteration 71/1000 | Loss: 0.00001401
Iteration 72/1000 | Loss: 0.00001400
Iteration 73/1000 | Loss: 0.00001400
Iteration 74/1000 | Loss: 0.00001400
Iteration 75/1000 | Loss: 0.00001399
Iteration 76/1000 | Loss: 0.00001399
Iteration 77/1000 | Loss: 0.00001399
Iteration 78/1000 | Loss: 0.00001399
Iteration 79/1000 | Loss: 0.00001399
Iteration 80/1000 | Loss: 0.00001399
Iteration 81/1000 | Loss: 0.00001399
Iteration 82/1000 | Loss: 0.00001399
Iteration 83/1000 | Loss: 0.00001399
Iteration 84/1000 | Loss: 0.00001399
Iteration 85/1000 | Loss: 0.00001398
Iteration 86/1000 | Loss: 0.00001398
Iteration 87/1000 | Loss: 0.00001398
Iteration 88/1000 | Loss: 0.00001398
Iteration 89/1000 | Loss: 0.00001398
Iteration 90/1000 | Loss: 0.00001398
Iteration 91/1000 | Loss: 0.00001397
Iteration 92/1000 | Loss: 0.00001395
Iteration 93/1000 | Loss: 0.00001395
Iteration 94/1000 | Loss: 0.00001395
Iteration 95/1000 | Loss: 0.00001395
Iteration 96/1000 | Loss: 0.00001395
Iteration 97/1000 | Loss: 0.00001395
Iteration 98/1000 | Loss: 0.00001394
Iteration 99/1000 | Loss: 0.00001394
Iteration 100/1000 | Loss: 0.00001394
Iteration 101/1000 | Loss: 0.00001394
Iteration 102/1000 | Loss: 0.00001394
Iteration 103/1000 | Loss: 0.00001393
Iteration 104/1000 | Loss: 0.00001393
Iteration 105/1000 | Loss: 0.00001393
Iteration 106/1000 | Loss: 0.00001393
Iteration 107/1000 | Loss: 0.00001393
Iteration 108/1000 | Loss: 0.00001393
Iteration 109/1000 | Loss: 0.00001393
Iteration 110/1000 | Loss: 0.00001393
Iteration 111/1000 | Loss: 0.00001393
Iteration 112/1000 | Loss: 0.00001393
Iteration 113/1000 | Loss: 0.00001393
Iteration 114/1000 | Loss: 0.00001393
Iteration 115/1000 | Loss: 0.00001393
Iteration 116/1000 | Loss: 0.00001393
Iteration 117/1000 | Loss: 0.00001393
Iteration 118/1000 | Loss: 0.00001393
Iteration 119/1000 | Loss: 0.00001393
Iteration 120/1000 | Loss: 0.00001393
Iteration 121/1000 | Loss: 0.00001393
Iteration 122/1000 | Loss: 0.00001393
Iteration 123/1000 | Loss: 0.00001393
Iteration 124/1000 | Loss: 0.00001393
Iteration 125/1000 | Loss: 0.00001393
Iteration 126/1000 | Loss: 0.00001393
Iteration 127/1000 | Loss: 0.00001393
Iteration 128/1000 | Loss: 0.00001393
Iteration 129/1000 | Loss: 0.00001393
Iteration 130/1000 | Loss: 0.00001393
Iteration 131/1000 | Loss: 0.00001393
Iteration 132/1000 | Loss: 0.00001393
Iteration 133/1000 | Loss: 0.00001393
Iteration 134/1000 | Loss: 0.00001393
Iteration 135/1000 | Loss: 0.00001393
Iteration 136/1000 | Loss: 0.00001393
Iteration 137/1000 | Loss: 0.00001393
Iteration 138/1000 | Loss: 0.00001393
Iteration 139/1000 | Loss: 0.00001393
Iteration 140/1000 | Loss: 0.00001393
Iteration 141/1000 | Loss: 0.00001393
Iteration 142/1000 | Loss: 0.00001393
Iteration 143/1000 | Loss: 0.00001393
Iteration 144/1000 | Loss: 0.00001393
Iteration 145/1000 | Loss: 0.00001393
Iteration 146/1000 | Loss: 0.00001393
Iteration 147/1000 | Loss: 0.00001393
Iteration 148/1000 | Loss: 0.00001393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.3933969057688955e-05, 1.3933969057688955e-05, 1.3933969057688955e-05, 1.3933969057688955e-05, 1.3933969057688955e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3933969057688955e-05

Optimization complete. Final v2v error: 3.1545052528381348 mm

Highest mean error: 3.844961643218994 mm for frame 93

Lowest mean error: 2.831414222717285 mm for frame 35

Saving results

Total time: 37.617923974990845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852095
Iteration 2/25 | Loss: 0.00166074
Iteration 3/25 | Loss: 0.00141085
Iteration 4/25 | Loss: 0.00136562
Iteration 5/25 | Loss: 0.00135682
Iteration 6/25 | Loss: 0.00135161
Iteration 7/25 | Loss: 0.00134991
Iteration 8/25 | Loss: 0.00134917
Iteration 9/25 | Loss: 0.00134865
Iteration 10/25 | Loss: 0.00134846
Iteration 11/25 | Loss: 0.00134832
Iteration 12/25 | Loss: 0.00134828
Iteration 13/25 | Loss: 0.00134828
Iteration 14/25 | Loss: 0.00134827
Iteration 15/25 | Loss: 0.00134827
Iteration 16/25 | Loss: 0.00134827
Iteration 17/25 | Loss: 0.00134827
Iteration 18/25 | Loss: 0.00134827
Iteration 19/25 | Loss: 0.00134827
Iteration 20/25 | Loss: 0.00134827
Iteration 21/25 | Loss: 0.00134827
Iteration 22/25 | Loss: 0.00134827
Iteration 23/25 | Loss: 0.00134826
Iteration 24/25 | Loss: 0.00134826
Iteration 25/25 | Loss: 0.00134826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39355719
Iteration 2/25 | Loss: 0.00121753
Iteration 3/25 | Loss: 0.00121752
Iteration 4/25 | Loss: 0.00121752
Iteration 5/25 | Loss: 0.00121751
Iteration 6/25 | Loss: 0.00121751
Iteration 7/25 | Loss: 0.00121751
Iteration 8/25 | Loss: 0.00121751
Iteration 9/25 | Loss: 0.00121751
Iteration 10/25 | Loss: 0.00121751
Iteration 11/25 | Loss: 0.00121751
Iteration 12/25 | Loss: 0.00121751
Iteration 13/25 | Loss: 0.00121751
Iteration 14/25 | Loss: 0.00121751
Iteration 15/25 | Loss: 0.00121751
Iteration 16/25 | Loss: 0.00121751
Iteration 17/25 | Loss: 0.00121751
Iteration 18/25 | Loss: 0.00121751
Iteration 19/25 | Loss: 0.00121751
Iteration 20/25 | Loss: 0.00121751
Iteration 21/25 | Loss: 0.00121751
Iteration 22/25 | Loss: 0.00121751
Iteration 23/25 | Loss: 0.00121751
Iteration 24/25 | Loss: 0.00121751
Iteration 25/25 | Loss: 0.00121751

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121751
Iteration 2/1000 | Loss: 0.00018618
Iteration 3/1000 | Loss: 0.00012613
Iteration 4/1000 | Loss: 0.00010655
Iteration 5/1000 | Loss: 0.00009671
Iteration 6/1000 | Loss: 0.00041475
Iteration 7/1000 | Loss: 0.00018656
Iteration 8/1000 | Loss: 0.00013772
Iteration 9/1000 | Loss: 0.00013760
Iteration 10/1000 | Loss: 0.00006947
Iteration 11/1000 | Loss: 0.00013700
Iteration 12/1000 | Loss: 0.00005947
Iteration 13/1000 | Loss: 0.00008125
Iteration 14/1000 | Loss: 0.00005147
Iteration 15/1000 | Loss: 0.00007362
Iteration 16/1000 | Loss: 0.00004674
Iteration 17/1000 | Loss: 0.00004395
Iteration 18/1000 | Loss: 0.00004184
Iteration 19/1000 | Loss: 0.00004010
Iteration 20/1000 | Loss: 0.00055547
Iteration 21/1000 | Loss: 0.00006083
Iteration 22/1000 | Loss: 0.00009511
Iteration 23/1000 | Loss: 0.00004256
Iteration 24/1000 | Loss: 0.00003862
Iteration 25/1000 | Loss: 0.00003780
Iteration 26/1000 | Loss: 0.00005166
Iteration 27/1000 | Loss: 0.00003721
Iteration 28/1000 | Loss: 0.00003578
Iteration 29/1000 | Loss: 0.00003543
Iteration 30/1000 | Loss: 0.00003502
Iteration 31/1000 | Loss: 0.00003472
Iteration 32/1000 | Loss: 0.00003463
Iteration 33/1000 | Loss: 0.00003459
Iteration 34/1000 | Loss: 0.00003458
Iteration 35/1000 | Loss: 0.00003455
Iteration 36/1000 | Loss: 0.00003445
Iteration 37/1000 | Loss: 0.00003444
Iteration 38/1000 | Loss: 0.00003443
Iteration 39/1000 | Loss: 0.00003440
Iteration 40/1000 | Loss: 0.00003440
Iteration 41/1000 | Loss: 0.00003439
Iteration 42/1000 | Loss: 0.00003439
Iteration 43/1000 | Loss: 0.00003438
Iteration 44/1000 | Loss: 0.00003438
Iteration 45/1000 | Loss: 0.00003437
Iteration 46/1000 | Loss: 0.00003436
Iteration 47/1000 | Loss: 0.00003436
Iteration 48/1000 | Loss: 0.00003435
Iteration 49/1000 | Loss: 0.00003434
Iteration 50/1000 | Loss: 0.00003431
Iteration 51/1000 | Loss: 0.00003428
Iteration 52/1000 | Loss: 0.00003426
Iteration 53/1000 | Loss: 0.00003426
Iteration 54/1000 | Loss: 0.00003426
Iteration 55/1000 | Loss: 0.00003426
Iteration 56/1000 | Loss: 0.00003426
Iteration 57/1000 | Loss: 0.00003425
Iteration 58/1000 | Loss: 0.00003425
Iteration 59/1000 | Loss: 0.00003423
Iteration 60/1000 | Loss: 0.00003423
Iteration 61/1000 | Loss: 0.00003422
Iteration 62/1000 | Loss: 0.00003422
Iteration 63/1000 | Loss: 0.00003421
Iteration 64/1000 | Loss: 0.00003421
Iteration 65/1000 | Loss: 0.00003421
Iteration 66/1000 | Loss: 0.00003420
Iteration 67/1000 | Loss: 0.00003420
Iteration 68/1000 | Loss: 0.00003420
Iteration 69/1000 | Loss: 0.00003420
Iteration 70/1000 | Loss: 0.00003420
Iteration 71/1000 | Loss: 0.00003420
Iteration 72/1000 | Loss: 0.00003420
Iteration 73/1000 | Loss: 0.00003419
Iteration 74/1000 | Loss: 0.00003419
Iteration 75/1000 | Loss: 0.00003419
Iteration 76/1000 | Loss: 0.00003419
Iteration 77/1000 | Loss: 0.00003419
Iteration 78/1000 | Loss: 0.00003419
Iteration 79/1000 | Loss: 0.00003419
Iteration 80/1000 | Loss: 0.00003419
Iteration 81/1000 | Loss: 0.00003419
Iteration 82/1000 | Loss: 0.00003419
Iteration 83/1000 | Loss: 0.00003418
Iteration 84/1000 | Loss: 0.00003418
Iteration 85/1000 | Loss: 0.00003418
Iteration 86/1000 | Loss: 0.00003418
Iteration 87/1000 | Loss: 0.00003418
Iteration 88/1000 | Loss: 0.00003418
Iteration 89/1000 | Loss: 0.00003418
Iteration 90/1000 | Loss: 0.00003418
Iteration 91/1000 | Loss: 0.00003418
Iteration 92/1000 | Loss: 0.00003417
Iteration 93/1000 | Loss: 0.00003417
Iteration 94/1000 | Loss: 0.00003417
Iteration 95/1000 | Loss: 0.00003417
Iteration 96/1000 | Loss: 0.00003417
Iteration 97/1000 | Loss: 0.00003416
Iteration 98/1000 | Loss: 0.00003416
Iteration 99/1000 | Loss: 0.00003416
Iteration 100/1000 | Loss: 0.00003416
Iteration 101/1000 | Loss: 0.00003416
Iteration 102/1000 | Loss: 0.00003416
Iteration 103/1000 | Loss: 0.00003416
Iteration 104/1000 | Loss: 0.00003415
Iteration 105/1000 | Loss: 0.00003415
Iteration 106/1000 | Loss: 0.00003415
Iteration 107/1000 | Loss: 0.00003415
Iteration 108/1000 | Loss: 0.00003415
Iteration 109/1000 | Loss: 0.00003415
Iteration 110/1000 | Loss: 0.00003414
Iteration 111/1000 | Loss: 0.00003414
Iteration 112/1000 | Loss: 0.00003414
Iteration 113/1000 | Loss: 0.00003414
Iteration 114/1000 | Loss: 0.00003413
Iteration 115/1000 | Loss: 0.00003413
Iteration 116/1000 | Loss: 0.00003413
Iteration 117/1000 | Loss: 0.00003412
Iteration 118/1000 | Loss: 0.00003412
Iteration 119/1000 | Loss: 0.00003412
Iteration 120/1000 | Loss: 0.00003412
Iteration 121/1000 | Loss: 0.00003411
Iteration 122/1000 | Loss: 0.00003411
Iteration 123/1000 | Loss: 0.00003411
Iteration 124/1000 | Loss: 0.00003411
Iteration 125/1000 | Loss: 0.00003410
Iteration 126/1000 | Loss: 0.00003410
Iteration 127/1000 | Loss: 0.00003410
Iteration 128/1000 | Loss: 0.00003410
Iteration 129/1000 | Loss: 0.00003409
Iteration 130/1000 | Loss: 0.00003409
Iteration 131/1000 | Loss: 0.00003409
Iteration 132/1000 | Loss: 0.00003409
Iteration 133/1000 | Loss: 0.00003409
Iteration 134/1000 | Loss: 0.00003409
Iteration 135/1000 | Loss: 0.00003408
Iteration 136/1000 | Loss: 0.00003408
Iteration 137/1000 | Loss: 0.00003408
Iteration 138/1000 | Loss: 0.00003408
Iteration 139/1000 | Loss: 0.00003408
Iteration 140/1000 | Loss: 0.00003408
Iteration 141/1000 | Loss: 0.00003408
Iteration 142/1000 | Loss: 0.00003407
Iteration 143/1000 | Loss: 0.00003407
Iteration 144/1000 | Loss: 0.00003407
Iteration 145/1000 | Loss: 0.00003407
Iteration 146/1000 | Loss: 0.00003406
Iteration 147/1000 | Loss: 0.00003406
Iteration 148/1000 | Loss: 0.00003406
Iteration 149/1000 | Loss: 0.00003406
Iteration 150/1000 | Loss: 0.00003406
Iteration 151/1000 | Loss: 0.00003406
Iteration 152/1000 | Loss: 0.00003405
Iteration 153/1000 | Loss: 0.00003405
Iteration 154/1000 | Loss: 0.00003405
Iteration 155/1000 | Loss: 0.00003405
Iteration 156/1000 | Loss: 0.00003405
Iteration 157/1000 | Loss: 0.00003404
Iteration 158/1000 | Loss: 0.00003404
Iteration 159/1000 | Loss: 0.00003404
Iteration 160/1000 | Loss: 0.00003404
Iteration 161/1000 | Loss: 0.00003404
Iteration 162/1000 | Loss: 0.00003404
Iteration 163/1000 | Loss: 0.00003404
Iteration 164/1000 | Loss: 0.00003404
Iteration 165/1000 | Loss: 0.00003404
Iteration 166/1000 | Loss: 0.00003404
Iteration 167/1000 | Loss: 0.00003404
Iteration 168/1000 | Loss: 0.00003404
Iteration 169/1000 | Loss: 0.00003404
Iteration 170/1000 | Loss: 0.00003404
Iteration 171/1000 | Loss: 0.00003404
Iteration 172/1000 | Loss: 0.00003404
Iteration 173/1000 | Loss: 0.00003404
Iteration 174/1000 | Loss: 0.00003404
Iteration 175/1000 | Loss: 0.00003404
Iteration 176/1000 | Loss: 0.00003404
Iteration 177/1000 | Loss: 0.00003403
Iteration 178/1000 | Loss: 0.00003403
Iteration 179/1000 | Loss: 0.00003403
Iteration 180/1000 | Loss: 0.00003403
Iteration 181/1000 | Loss: 0.00003403
Iteration 182/1000 | Loss: 0.00003402
Iteration 183/1000 | Loss: 0.00003402
Iteration 184/1000 | Loss: 0.00003402
Iteration 185/1000 | Loss: 0.00003402
Iteration 186/1000 | Loss: 0.00003402
Iteration 187/1000 | Loss: 0.00003402
Iteration 188/1000 | Loss: 0.00003402
Iteration 189/1000 | Loss: 0.00003402
Iteration 190/1000 | Loss: 0.00003402
Iteration 191/1000 | Loss: 0.00003402
Iteration 192/1000 | Loss: 0.00003402
Iteration 193/1000 | Loss: 0.00003402
Iteration 194/1000 | Loss: 0.00003402
Iteration 195/1000 | Loss: 0.00003402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [3.401963476790115e-05, 3.401963476790115e-05, 3.401963476790115e-05, 3.401963476790115e-05, 3.401963476790115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.401963476790115e-05

Optimization complete. Final v2v error: 4.6170172691345215 mm

Highest mean error: 6.995386123657227 mm for frame 58

Lowest mean error: 4.031834125518799 mm for frame 115

Saving results

Total time: 90.87309718132019
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778617
Iteration 2/25 | Loss: 0.00166958
Iteration 3/25 | Loss: 0.00134299
Iteration 4/25 | Loss: 0.00129146
Iteration 5/25 | Loss: 0.00128083
Iteration 6/25 | Loss: 0.00127865
Iteration 7/25 | Loss: 0.00127817
Iteration 8/25 | Loss: 0.00127799
Iteration 9/25 | Loss: 0.00127797
Iteration 10/25 | Loss: 0.00127797
Iteration 11/25 | Loss: 0.00127797
Iteration 12/25 | Loss: 0.00127797
Iteration 13/25 | Loss: 0.00127797
Iteration 14/25 | Loss: 0.00127797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00127796723973006, 0.00127796723973006, 0.00127796723973006, 0.00127796723973006, 0.00127796723973006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00127796723973006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39715815
Iteration 2/25 | Loss: 0.00069974
Iteration 3/25 | Loss: 0.00069974
Iteration 4/25 | Loss: 0.00069974
Iteration 5/25 | Loss: 0.00069974
Iteration 6/25 | Loss: 0.00069974
Iteration 7/25 | Loss: 0.00069974
Iteration 8/25 | Loss: 0.00069974
Iteration 9/25 | Loss: 0.00069974
Iteration 10/25 | Loss: 0.00069974
Iteration 11/25 | Loss: 0.00069974
Iteration 12/25 | Loss: 0.00069974
Iteration 13/25 | Loss: 0.00069974
Iteration 14/25 | Loss: 0.00069974
Iteration 15/25 | Loss: 0.00069974
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006997407181188464, 0.0006997407181188464, 0.0006997407181188464, 0.0006997407181188464, 0.0006997407181188464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006997407181188464

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069974
Iteration 2/1000 | Loss: 0.00005519
Iteration 3/1000 | Loss: 0.00003616
Iteration 4/1000 | Loss: 0.00003229
Iteration 5/1000 | Loss: 0.00003071
Iteration 6/1000 | Loss: 0.00004282
Iteration 7/1000 | Loss: 0.00003034
Iteration 8/1000 | Loss: 0.00002861
Iteration 9/1000 | Loss: 0.00002769
Iteration 10/1000 | Loss: 0.00002722
Iteration 11/1000 | Loss: 0.00002694
Iteration 12/1000 | Loss: 0.00002666
Iteration 13/1000 | Loss: 0.00002625
Iteration 14/1000 | Loss: 0.00002596
Iteration 15/1000 | Loss: 0.00002574
Iteration 16/1000 | Loss: 0.00002555
Iteration 17/1000 | Loss: 0.00002539
Iteration 18/1000 | Loss: 0.00002536
Iteration 19/1000 | Loss: 0.00002528
Iteration 20/1000 | Loss: 0.00002516
Iteration 21/1000 | Loss: 0.00002512
Iteration 22/1000 | Loss: 0.00002504
Iteration 23/1000 | Loss: 0.00002504
Iteration 24/1000 | Loss: 0.00002503
Iteration 25/1000 | Loss: 0.00002499
Iteration 26/1000 | Loss: 0.00002498
Iteration 27/1000 | Loss: 0.00002493
Iteration 28/1000 | Loss: 0.00002490
Iteration 29/1000 | Loss: 0.00002489
Iteration 30/1000 | Loss: 0.00002489
Iteration 31/1000 | Loss: 0.00002489
Iteration 32/1000 | Loss: 0.00002489
Iteration 33/1000 | Loss: 0.00002489
Iteration 34/1000 | Loss: 0.00002489
Iteration 35/1000 | Loss: 0.00002489
Iteration 36/1000 | Loss: 0.00002488
Iteration 37/1000 | Loss: 0.00002488
Iteration 38/1000 | Loss: 0.00002487
Iteration 39/1000 | Loss: 0.00002487
Iteration 40/1000 | Loss: 0.00002485
Iteration 41/1000 | Loss: 0.00002484
Iteration 42/1000 | Loss: 0.00002484
Iteration 43/1000 | Loss: 0.00002484
Iteration 44/1000 | Loss: 0.00002484
Iteration 45/1000 | Loss: 0.00002484
Iteration 46/1000 | Loss: 0.00002484
Iteration 47/1000 | Loss: 0.00002483
Iteration 48/1000 | Loss: 0.00002483
Iteration 49/1000 | Loss: 0.00002483
Iteration 50/1000 | Loss: 0.00002483
Iteration 51/1000 | Loss: 0.00002483
Iteration 52/1000 | Loss: 0.00002482
Iteration 53/1000 | Loss: 0.00002482
Iteration 54/1000 | Loss: 0.00002481
Iteration 55/1000 | Loss: 0.00021345
Iteration 56/1000 | Loss: 0.00004627
Iteration 57/1000 | Loss: 0.00002811
Iteration 58/1000 | Loss: 0.00002626
Iteration 59/1000 | Loss: 0.00002522
Iteration 60/1000 | Loss: 0.00002464
Iteration 61/1000 | Loss: 0.00002441
Iteration 62/1000 | Loss: 0.00002436
Iteration 63/1000 | Loss: 0.00002432
Iteration 64/1000 | Loss: 0.00002432
Iteration 65/1000 | Loss: 0.00002431
Iteration 66/1000 | Loss: 0.00002430
Iteration 67/1000 | Loss: 0.00002430
Iteration 68/1000 | Loss: 0.00002430
Iteration 69/1000 | Loss: 0.00002429
Iteration 70/1000 | Loss: 0.00002429
Iteration 71/1000 | Loss: 0.00002429
Iteration 72/1000 | Loss: 0.00002429
Iteration 73/1000 | Loss: 0.00002429
Iteration 74/1000 | Loss: 0.00002428
Iteration 75/1000 | Loss: 0.00002428
Iteration 76/1000 | Loss: 0.00002428
Iteration 77/1000 | Loss: 0.00002427
Iteration 78/1000 | Loss: 0.00002427
Iteration 79/1000 | Loss: 0.00002427
Iteration 80/1000 | Loss: 0.00002426
Iteration 81/1000 | Loss: 0.00002426
Iteration 82/1000 | Loss: 0.00002426
Iteration 83/1000 | Loss: 0.00002426
Iteration 84/1000 | Loss: 0.00002426
Iteration 85/1000 | Loss: 0.00002426
Iteration 86/1000 | Loss: 0.00002425
Iteration 87/1000 | Loss: 0.00002425
Iteration 88/1000 | Loss: 0.00002424
Iteration 89/1000 | Loss: 0.00002424
Iteration 90/1000 | Loss: 0.00002423
Iteration 91/1000 | Loss: 0.00002423
Iteration 92/1000 | Loss: 0.00002423
Iteration 93/1000 | Loss: 0.00002423
Iteration 94/1000 | Loss: 0.00002423
Iteration 95/1000 | Loss: 0.00002422
Iteration 96/1000 | Loss: 0.00002422
Iteration 97/1000 | Loss: 0.00002422
Iteration 98/1000 | Loss: 0.00002421
Iteration 99/1000 | Loss: 0.00002421
Iteration 100/1000 | Loss: 0.00002421
Iteration 101/1000 | Loss: 0.00002421
Iteration 102/1000 | Loss: 0.00002421
Iteration 103/1000 | Loss: 0.00002420
Iteration 104/1000 | Loss: 0.00002420
Iteration 105/1000 | Loss: 0.00002420
Iteration 106/1000 | Loss: 0.00002420
Iteration 107/1000 | Loss: 0.00002420
Iteration 108/1000 | Loss: 0.00002420
Iteration 109/1000 | Loss: 0.00002420
Iteration 110/1000 | Loss: 0.00002420
Iteration 111/1000 | Loss: 0.00002420
Iteration 112/1000 | Loss: 0.00002419
Iteration 113/1000 | Loss: 0.00002419
Iteration 114/1000 | Loss: 0.00002419
Iteration 115/1000 | Loss: 0.00002419
Iteration 116/1000 | Loss: 0.00002419
Iteration 117/1000 | Loss: 0.00002419
Iteration 118/1000 | Loss: 0.00002419
Iteration 119/1000 | Loss: 0.00002419
Iteration 120/1000 | Loss: 0.00002418
Iteration 121/1000 | Loss: 0.00002418
Iteration 122/1000 | Loss: 0.00002418
Iteration 123/1000 | Loss: 0.00002418
Iteration 124/1000 | Loss: 0.00002418
Iteration 125/1000 | Loss: 0.00002417
Iteration 126/1000 | Loss: 0.00002417
Iteration 127/1000 | Loss: 0.00002417
Iteration 128/1000 | Loss: 0.00002417
Iteration 129/1000 | Loss: 0.00002417
Iteration 130/1000 | Loss: 0.00002417
Iteration 131/1000 | Loss: 0.00002417
Iteration 132/1000 | Loss: 0.00002417
Iteration 133/1000 | Loss: 0.00002417
Iteration 134/1000 | Loss: 0.00002417
Iteration 135/1000 | Loss: 0.00002417
Iteration 136/1000 | Loss: 0.00002417
Iteration 137/1000 | Loss: 0.00002416
Iteration 138/1000 | Loss: 0.00002416
Iteration 139/1000 | Loss: 0.00002416
Iteration 140/1000 | Loss: 0.00002416
Iteration 141/1000 | Loss: 0.00002416
Iteration 142/1000 | Loss: 0.00002416
Iteration 143/1000 | Loss: 0.00002416
Iteration 144/1000 | Loss: 0.00002416
Iteration 145/1000 | Loss: 0.00002416
Iteration 146/1000 | Loss: 0.00002416
Iteration 147/1000 | Loss: 0.00002416
Iteration 148/1000 | Loss: 0.00002416
Iteration 149/1000 | Loss: 0.00002416
Iteration 150/1000 | Loss: 0.00002416
Iteration 151/1000 | Loss: 0.00002416
Iteration 152/1000 | Loss: 0.00002416
Iteration 153/1000 | Loss: 0.00002416
Iteration 154/1000 | Loss: 0.00002416
Iteration 155/1000 | Loss: 0.00002416
Iteration 156/1000 | Loss: 0.00002416
Iteration 157/1000 | Loss: 0.00002416
Iteration 158/1000 | Loss: 0.00002416
Iteration 159/1000 | Loss: 0.00002416
Iteration 160/1000 | Loss: 0.00002416
Iteration 161/1000 | Loss: 0.00002416
Iteration 162/1000 | Loss: 0.00002416
Iteration 163/1000 | Loss: 0.00002416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.4160915927495807e-05, 2.4160915927495807e-05, 2.4160915927495807e-05, 2.4160915927495807e-05, 2.4160915927495807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4160915927495807e-05

Optimization complete. Final v2v error: 4.087153911590576 mm

Highest mean error: 5.290176868438721 mm for frame 65

Lowest mean error: 3.189873695373535 mm for frame 124

Saving results

Total time: 71.15779685974121
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00571164
Iteration 2/25 | Loss: 0.00127880
Iteration 3/25 | Loss: 0.00120388
Iteration 4/25 | Loss: 0.00119017
Iteration 5/25 | Loss: 0.00118443
Iteration 6/25 | Loss: 0.00118358
Iteration 7/25 | Loss: 0.00118358
Iteration 8/25 | Loss: 0.00118358
Iteration 9/25 | Loss: 0.00118358
Iteration 10/25 | Loss: 0.00118358
Iteration 11/25 | Loss: 0.00118358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001183577231131494, 0.001183577231131494, 0.001183577231131494, 0.001183577231131494, 0.001183577231131494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001183577231131494

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.79221296
Iteration 2/25 | Loss: 0.00064951
Iteration 3/25 | Loss: 0.00064951
Iteration 4/25 | Loss: 0.00064951
Iteration 5/25 | Loss: 0.00064951
Iteration 6/25 | Loss: 0.00064951
Iteration 7/25 | Loss: 0.00064951
Iteration 8/25 | Loss: 0.00064951
Iteration 9/25 | Loss: 0.00064951
Iteration 10/25 | Loss: 0.00064951
Iteration 11/25 | Loss: 0.00064951
Iteration 12/25 | Loss: 0.00064951
Iteration 13/25 | Loss: 0.00064951
Iteration 14/25 | Loss: 0.00064950
Iteration 15/25 | Loss: 0.00064950
Iteration 16/25 | Loss: 0.00064950
Iteration 17/25 | Loss: 0.00064950
Iteration 18/25 | Loss: 0.00064950
Iteration 19/25 | Loss: 0.00064950
Iteration 20/25 | Loss: 0.00064950
Iteration 21/25 | Loss: 0.00064950
Iteration 22/25 | Loss: 0.00064950
Iteration 23/25 | Loss: 0.00064950
Iteration 24/25 | Loss: 0.00064950
Iteration 25/25 | Loss: 0.00064950

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064950
Iteration 2/1000 | Loss: 0.00002600
Iteration 3/1000 | Loss: 0.00001771
Iteration 4/1000 | Loss: 0.00001529
Iteration 5/1000 | Loss: 0.00001444
Iteration 6/1000 | Loss: 0.00001390
Iteration 7/1000 | Loss: 0.00001354
Iteration 8/1000 | Loss: 0.00001342
Iteration 9/1000 | Loss: 0.00001334
Iteration 10/1000 | Loss: 0.00001333
Iteration 11/1000 | Loss: 0.00001330
Iteration 12/1000 | Loss: 0.00001309
Iteration 13/1000 | Loss: 0.00001293
Iteration 14/1000 | Loss: 0.00001286
Iteration 15/1000 | Loss: 0.00001285
Iteration 16/1000 | Loss: 0.00001284
Iteration 17/1000 | Loss: 0.00001269
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001267
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001259
Iteration 22/1000 | Loss: 0.00001258
Iteration 23/1000 | Loss: 0.00001258
Iteration 24/1000 | Loss: 0.00001256
Iteration 25/1000 | Loss: 0.00001254
Iteration 26/1000 | Loss: 0.00001254
Iteration 27/1000 | Loss: 0.00001251
Iteration 28/1000 | Loss: 0.00001251
Iteration 29/1000 | Loss: 0.00001251
Iteration 30/1000 | Loss: 0.00001251
Iteration 31/1000 | Loss: 0.00001251
Iteration 32/1000 | Loss: 0.00001250
Iteration 33/1000 | Loss: 0.00001250
Iteration 34/1000 | Loss: 0.00001250
Iteration 35/1000 | Loss: 0.00001250
Iteration 36/1000 | Loss: 0.00001248
Iteration 37/1000 | Loss: 0.00001248
Iteration 38/1000 | Loss: 0.00001248
Iteration 39/1000 | Loss: 0.00001248
Iteration 40/1000 | Loss: 0.00001248
Iteration 41/1000 | Loss: 0.00001247
Iteration 42/1000 | Loss: 0.00001246
Iteration 43/1000 | Loss: 0.00001246
Iteration 44/1000 | Loss: 0.00001246
Iteration 45/1000 | Loss: 0.00001245
Iteration 46/1000 | Loss: 0.00001245
Iteration 47/1000 | Loss: 0.00001245
Iteration 48/1000 | Loss: 0.00001245
Iteration 49/1000 | Loss: 0.00001245
Iteration 50/1000 | Loss: 0.00001245
Iteration 51/1000 | Loss: 0.00001244
Iteration 52/1000 | Loss: 0.00001244
Iteration 53/1000 | Loss: 0.00001244
Iteration 54/1000 | Loss: 0.00001244
Iteration 55/1000 | Loss: 0.00001244
Iteration 56/1000 | Loss: 0.00001244
Iteration 57/1000 | Loss: 0.00001240
Iteration 58/1000 | Loss: 0.00001240
Iteration 59/1000 | Loss: 0.00001240
Iteration 60/1000 | Loss: 0.00001240
Iteration 61/1000 | Loss: 0.00001240
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001238
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001235
Iteration 67/1000 | Loss: 0.00001235
Iteration 68/1000 | Loss: 0.00001234
Iteration 69/1000 | Loss: 0.00001234
Iteration 70/1000 | Loss: 0.00001234
Iteration 71/1000 | Loss: 0.00001233
Iteration 72/1000 | Loss: 0.00001233
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001233
Iteration 75/1000 | Loss: 0.00001233
Iteration 76/1000 | Loss: 0.00001232
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001231
Iteration 81/1000 | Loss: 0.00001231
Iteration 82/1000 | Loss: 0.00001231
Iteration 83/1000 | Loss: 0.00001230
Iteration 84/1000 | Loss: 0.00001230
Iteration 85/1000 | Loss: 0.00001230
Iteration 86/1000 | Loss: 0.00001230
Iteration 87/1000 | Loss: 0.00001229
Iteration 88/1000 | Loss: 0.00001229
Iteration 89/1000 | Loss: 0.00001229
Iteration 90/1000 | Loss: 0.00001228
Iteration 91/1000 | Loss: 0.00001228
Iteration 92/1000 | Loss: 0.00001227
Iteration 93/1000 | Loss: 0.00001227
Iteration 94/1000 | Loss: 0.00001227
Iteration 95/1000 | Loss: 0.00001227
Iteration 96/1000 | Loss: 0.00001227
Iteration 97/1000 | Loss: 0.00001227
Iteration 98/1000 | Loss: 0.00001226
Iteration 99/1000 | Loss: 0.00001226
Iteration 100/1000 | Loss: 0.00001226
Iteration 101/1000 | Loss: 0.00001226
Iteration 102/1000 | Loss: 0.00001226
Iteration 103/1000 | Loss: 0.00001226
Iteration 104/1000 | Loss: 0.00001226
Iteration 105/1000 | Loss: 0.00001226
Iteration 106/1000 | Loss: 0.00001226
Iteration 107/1000 | Loss: 0.00001226
Iteration 108/1000 | Loss: 0.00001226
Iteration 109/1000 | Loss: 0.00001225
Iteration 110/1000 | Loss: 0.00001225
Iteration 111/1000 | Loss: 0.00001225
Iteration 112/1000 | Loss: 0.00001225
Iteration 113/1000 | Loss: 0.00001224
Iteration 114/1000 | Loss: 0.00001224
Iteration 115/1000 | Loss: 0.00001224
Iteration 116/1000 | Loss: 0.00001224
Iteration 117/1000 | Loss: 0.00001224
Iteration 118/1000 | Loss: 0.00001224
Iteration 119/1000 | Loss: 0.00001224
Iteration 120/1000 | Loss: 0.00001224
Iteration 121/1000 | Loss: 0.00001224
Iteration 122/1000 | Loss: 0.00001224
Iteration 123/1000 | Loss: 0.00001223
Iteration 124/1000 | Loss: 0.00001223
Iteration 125/1000 | Loss: 0.00001223
Iteration 126/1000 | Loss: 0.00001223
Iteration 127/1000 | Loss: 0.00001222
Iteration 128/1000 | Loss: 0.00001222
Iteration 129/1000 | Loss: 0.00001221
Iteration 130/1000 | Loss: 0.00001221
Iteration 131/1000 | Loss: 0.00001221
Iteration 132/1000 | Loss: 0.00001221
Iteration 133/1000 | Loss: 0.00001221
Iteration 134/1000 | Loss: 0.00001221
Iteration 135/1000 | Loss: 0.00001220
Iteration 136/1000 | Loss: 0.00001220
Iteration 137/1000 | Loss: 0.00001220
Iteration 138/1000 | Loss: 0.00001220
Iteration 139/1000 | Loss: 0.00001220
Iteration 140/1000 | Loss: 0.00001220
Iteration 141/1000 | Loss: 0.00001218
Iteration 142/1000 | Loss: 0.00001218
Iteration 143/1000 | Loss: 0.00001218
Iteration 144/1000 | Loss: 0.00001218
Iteration 145/1000 | Loss: 0.00001218
Iteration 146/1000 | Loss: 0.00001218
Iteration 147/1000 | Loss: 0.00001218
Iteration 148/1000 | Loss: 0.00001218
Iteration 149/1000 | Loss: 0.00001218
Iteration 150/1000 | Loss: 0.00001218
Iteration 151/1000 | Loss: 0.00001218
Iteration 152/1000 | Loss: 0.00001218
Iteration 153/1000 | Loss: 0.00001218
Iteration 154/1000 | Loss: 0.00001218
Iteration 155/1000 | Loss: 0.00001217
Iteration 156/1000 | Loss: 0.00001217
Iteration 157/1000 | Loss: 0.00001217
Iteration 158/1000 | Loss: 0.00001217
Iteration 159/1000 | Loss: 0.00001217
Iteration 160/1000 | Loss: 0.00001217
Iteration 161/1000 | Loss: 0.00001217
Iteration 162/1000 | Loss: 0.00001217
Iteration 163/1000 | Loss: 0.00001217
Iteration 164/1000 | Loss: 0.00001216
Iteration 165/1000 | Loss: 0.00001216
Iteration 166/1000 | Loss: 0.00001216
Iteration 167/1000 | Loss: 0.00001216
Iteration 168/1000 | Loss: 0.00001216
Iteration 169/1000 | Loss: 0.00001216
Iteration 170/1000 | Loss: 0.00001216
Iteration 171/1000 | Loss: 0.00001216
Iteration 172/1000 | Loss: 0.00001216
Iteration 173/1000 | Loss: 0.00001216
Iteration 174/1000 | Loss: 0.00001216
Iteration 175/1000 | Loss: 0.00001216
Iteration 176/1000 | Loss: 0.00001216
Iteration 177/1000 | Loss: 0.00001216
Iteration 178/1000 | Loss: 0.00001216
Iteration 179/1000 | Loss: 0.00001216
Iteration 180/1000 | Loss: 0.00001216
Iteration 181/1000 | Loss: 0.00001216
Iteration 182/1000 | Loss: 0.00001216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.2160817277617753e-05, 1.2160817277617753e-05, 1.2160817277617753e-05, 1.2160817277617753e-05, 1.2160817277617753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2160817277617753e-05

Optimization complete. Final v2v error: 2.965280532836914 mm

Highest mean error: 3.2472422122955322 mm for frame 60

Lowest mean error: 2.795353889465332 mm for frame 122

Saving results

Total time: 39.3878960609436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059152
Iteration 2/25 | Loss: 0.01059152
Iteration 3/25 | Loss: 0.01059152
Iteration 4/25 | Loss: 0.01059151
Iteration 5/25 | Loss: 0.01059151
Iteration 6/25 | Loss: 0.01059151
Iteration 7/25 | Loss: 0.01059151
Iteration 8/25 | Loss: 0.01059151
Iteration 9/25 | Loss: 0.01059151
Iteration 10/25 | Loss: 0.01059150
Iteration 11/25 | Loss: 0.01059150
Iteration 12/25 | Loss: 0.01059150
Iteration 13/25 | Loss: 0.01059150
Iteration 14/25 | Loss: 0.01059150
Iteration 15/25 | Loss: 0.01059149
Iteration 16/25 | Loss: 0.01059149
Iteration 17/25 | Loss: 0.01059149
Iteration 18/25 | Loss: 0.01059149
Iteration 19/25 | Loss: 0.01059149
Iteration 20/25 | Loss: 0.01059148
Iteration 21/25 | Loss: 0.01059148
Iteration 22/25 | Loss: 0.01059148
Iteration 23/25 | Loss: 0.01059148
Iteration 24/25 | Loss: 0.01059148
Iteration 25/25 | Loss: 0.01059147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80334747
Iteration 2/25 | Loss: 0.07898527
Iteration 3/25 | Loss: 0.07897482
Iteration 4/25 | Loss: 0.07894500
Iteration 5/25 | Loss: 0.07894495
Iteration 6/25 | Loss: 0.07894538
Iteration 7/25 | Loss: 0.07894536
Iteration 8/25 | Loss: 0.07894479
Iteration 9/25 | Loss: 0.07894506
Iteration 10/25 | Loss: 0.07894341
Iteration 11/25 | Loss: 0.07894339
Iteration 12/25 | Loss: 0.07894338
Iteration 13/25 | Loss: 0.07894338
Iteration 14/25 | Loss: 0.07894337
Iteration 15/25 | Loss: 0.07894337
Iteration 16/25 | Loss: 0.07894337
Iteration 17/25 | Loss: 0.07894337
Iteration 18/25 | Loss: 0.07894337
Iteration 19/25 | Loss: 0.07894337
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.07894337177276611, 0.07894337177276611, 0.07894337177276611, 0.07894337177276611, 0.07894337177276611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07894337177276611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07894337
Iteration 2/1000 | Loss: 0.00061384
Iteration 3/1000 | Loss: 0.00018592
Iteration 4/1000 | Loss: 0.00012468
Iteration 5/1000 | Loss: 0.00013395
Iteration 6/1000 | Loss: 0.00003569
Iteration 7/1000 | Loss: 0.00022809
Iteration 8/1000 | Loss: 0.00130421
Iteration 9/1000 | Loss: 0.00084023
Iteration 10/1000 | Loss: 0.00007349
Iteration 11/1000 | Loss: 0.00009797
Iteration 12/1000 | Loss: 0.00012812
Iteration 13/1000 | Loss: 0.00005622
Iteration 14/1000 | Loss: 0.00003417
Iteration 15/1000 | Loss: 0.00003200
Iteration 16/1000 | Loss: 0.00030945
Iteration 17/1000 | Loss: 0.00010794
Iteration 18/1000 | Loss: 0.00016033
Iteration 19/1000 | Loss: 0.00002489
Iteration 20/1000 | Loss: 0.00002394
Iteration 21/1000 | Loss: 0.00002513
Iteration 22/1000 | Loss: 0.00010208
Iteration 23/1000 | Loss: 0.00037209
Iteration 24/1000 | Loss: 0.00017329
Iteration 25/1000 | Loss: 0.00050782
Iteration 26/1000 | Loss: 0.00005189
Iteration 27/1000 | Loss: 0.00003169
Iteration 28/1000 | Loss: 0.00002304
Iteration 29/1000 | Loss: 0.00002069
Iteration 30/1000 | Loss: 0.00012075
Iteration 31/1000 | Loss: 0.00002483
Iteration 32/1000 | Loss: 0.00006547
Iteration 33/1000 | Loss: 0.00002031
Iteration 34/1000 | Loss: 0.00002802
Iteration 35/1000 | Loss: 0.00011519
Iteration 36/1000 | Loss: 0.00001890
Iteration 37/1000 | Loss: 0.00009084
Iteration 38/1000 | Loss: 0.00002260
Iteration 39/1000 | Loss: 0.00002254
Iteration 40/1000 | Loss: 0.00002210
Iteration 41/1000 | Loss: 0.00014099
Iteration 42/1000 | Loss: 0.00003203
Iteration 43/1000 | Loss: 0.00008930
Iteration 44/1000 | Loss: 0.00062801
Iteration 45/1000 | Loss: 0.00035458
Iteration 46/1000 | Loss: 0.00003317
Iteration 47/1000 | Loss: 0.00019012
Iteration 48/1000 | Loss: 0.00001884
Iteration 49/1000 | Loss: 0.00001711
Iteration 50/1000 | Loss: 0.00001695
Iteration 51/1000 | Loss: 0.00001681
Iteration 52/1000 | Loss: 0.00007677
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00007949
Iteration 55/1000 | Loss: 0.00031282
Iteration 56/1000 | Loss: 0.00002587
Iteration 57/1000 | Loss: 0.00003109
Iteration 58/1000 | Loss: 0.00001641
Iteration 59/1000 | Loss: 0.00001637
Iteration 60/1000 | Loss: 0.00001636
Iteration 61/1000 | Loss: 0.00001636
Iteration 62/1000 | Loss: 0.00001636
Iteration 63/1000 | Loss: 0.00001635
Iteration 64/1000 | Loss: 0.00001634
Iteration 65/1000 | Loss: 0.00001634
Iteration 66/1000 | Loss: 0.00001634
Iteration 67/1000 | Loss: 0.00001633
Iteration 68/1000 | Loss: 0.00001632
Iteration 69/1000 | Loss: 0.00001632
Iteration 70/1000 | Loss: 0.00001632
Iteration 71/1000 | Loss: 0.00001632
Iteration 72/1000 | Loss: 0.00001631
Iteration 73/1000 | Loss: 0.00001631
Iteration 74/1000 | Loss: 0.00001631
Iteration 75/1000 | Loss: 0.00001631
Iteration 76/1000 | Loss: 0.00001631
Iteration 77/1000 | Loss: 0.00001630
Iteration 78/1000 | Loss: 0.00001629
Iteration 79/1000 | Loss: 0.00001628
Iteration 80/1000 | Loss: 0.00001628
Iteration 81/1000 | Loss: 0.00001627
Iteration 82/1000 | Loss: 0.00001626
Iteration 83/1000 | Loss: 0.00001626
Iteration 84/1000 | Loss: 0.00001626
Iteration 85/1000 | Loss: 0.00001626
Iteration 86/1000 | Loss: 0.00001626
Iteration 87/1000 | Loss: 0.00001626
Iteration 88/1000 | Loss: 0.00001626
Iteration 89/1000 | Loss: 0.00001626
Iteration 90/1000 | Loss: 0.00001626
Iteration 91/1000 | Loss: 0.00001625
Iteration 92/1000 | Loss: 0.00001625
Iteration 93/1000 | Loss: 0.00001624
Iteration 94/1000 | Loss: 0.00001624
Iteration 95/1000 | Loss: 0.00001623
Iteration 96/1000 | Loss: 0.00001623
Iteration 97/1000 | Loss: 0.00001623
Iteration 98/1000 | Loss: 0.00001623
Iteration 99/1000 | Loss: 0.00001623
Iteration 100/1000 | Loss: 0.00002144
Iteration 101/1000 | Loss: 0.00001657
Iteration 102/1000 | Loss: 0.00001622
Iteration 103/1000 | Loss: 0.00001622
Iteration 104/1000 | Loss: 0.00001622
Iteration 105/1000 | Loss: 0.00001622
Iteration 106/1000 | Loss: 0.00001621
Iteration 107/1000 | Loss: 0.00001621
Iteration 108/1000 | Loss: 0.00001621
Iteration 109/1000 | Loss: 0.00001620
Iteration 110/1000 | Loss: 0.00001619
Iteration 111/1000 | Loss: 0.00001619
Iteration 112/1000 | Loss: 0.00001619
Iteration 113/1000 | Loss: 0.00001618
Iteration 114/1000 | Loss: 0.00001618
Iteration 115/1000 | Loss: 0.00001618
Iteration 116/1000 | Loss: 0.00001618
Iteration 117/1000 | Loss: 0.00001618
Iteration 118/1000 | Loss: 0.00001618
Iteration 119/1000 | Loss: 0.00001618
Iteration 120/1000 | Loss: 0.00001617
Iteration 121/1000 | Loss: 0.00001617
Iteration 122/1000 | Loss: 0.00001617
Iteration 123/1000 | Loss: 0.00001616
Iteration 124/1000 | Loss: 0.00001616
Iteration 125/1000 | Loss: 0.00007233
Iteration 126/1000 | Loss: 0.00015292
Iteration 127/1000 | Loss: 0.00001683
Iteration 128/1000 | Loss: 0.00001613
Iteration 129/1000 | Loss: 0.00001612
Iteration 130/1000 | Loss: 0.00001612
Iteration 131/1000 | Loss: 0.00001611
Iteration 132/1000 | Loss: 0.00001611
Iteration 133/1000 | Loss: 0.00001611
Iteration 134/1000 | Loss: 0.00001611
Iteration 135/1000 | Loss: 0.00001611
Iteration 136/1000 | Loss: 0.00001611
Iteration 137/1000 | Loss: 0.00001611
Iteration 138/1000 | Loss: 0.00001611
Iteration 139/1000 | Loss: 0.00007318
Iteration 140/1000 | Loss: 0.00023484
Iteration 141/1000 | Loss: 0.00001672
Iteration 142/1000 | Loss: 0.00002616
Iteration 143/1000 | Loss: 0.00001616
Iteration 144/1000 | Loss: 0.00001607
Iteration 145/1000 | Loss: 0.00001607
Iteration 146/1000 | Loss: 0.00001606
Iteration 147/1000 | Loss: 0.00001606
Iteration 148/1000 | Loss: 0.00006276
Iteration 149/1000 | Loss: 0.00003789
Iteration 150/1000 | Loss: 0.00035925
Iteration 151/1000 | Loss: 0.00007198
Iteration 152/1000 | Loss: 0.00001999
Iteration 153/1000 | Loss: 0.00001703
Iteration 154/1000 | Loss: 0.00002649
Iteration 155/1000 | Loss: 0.00001648
Iteration 156/1000 | Loss: 0.00005769
Iteration 157/1000 | Loss: 0.00002473
Iteration 158/1000 | Loss: 0.00001608
Iteration 159/1000 | Loss: 0.00006140
Iteration 160/1000 | Loss: 0.00001617
Iteration 161/1000 | Loss: 0.00001602
Iteration 162/1000 | Loss: 0.00001602
Iteration 163/1000 | Loss: 0.00001601
Iteration 164/1000 | Loss: 0.00001601
Iteration 165/1000 | Loss: 0.00001600
Iteration 166/1000 | Loss: 0.00001600
Iteration 167/1000 | Loss: 0.00001599
Iteration 168/1000 | Loss: 0.00001598
Iteration 169/1000 | Loss: 0.00001598
Iteration 170/1000 | Loss: 0.00001597
Iteration 171/1000 | Loss: 0.00001597
Iteration 172/1000 | Loss: 0.00001597
Iteration 173/1000 | Loss: 0.00001597
Iteration 174/1000 | Loss: 0.00001597
Iteration 175/1000 | Loss: 0.00001597
Iteration 176/1000 | Loss: 0.00001597
Iteration 177/1000 | Loss: 0.00001596
Iteration 178/1000 | Loss: 0.00001596
Iteration 179/1000 | Loss: 0.00001596
Iteration 180/1000 | Loss: 0.00001596
Iteration 181/1000 | Loss: 0.00001596
Iteration 182/1000 | Loss: 0.00001596
Iteration 183/1000 | Loss: 0.00001596
Iteration 184/1000 | Loss: 0.00001596
Iteration 185/1000 | Loss: 0.00001596
Iteration 186/1000 | Loss: 0.00001596
Iteration 187/1000 | Loss: 0.00001596
Iteration 188/1000 | Loss: 0.00001596
Iteration 189/1000 | Loss: 0.00001596
Iteration 190/1000 | Loss: 0.00001596
Iteration 191/1000 | Loss: 0.00001596
Iteration 192/1000 | Loss: 0.00001596
Iteration 193/1000 | Loss: 0.00001596
Iteration 194/1000 | Loss: 0.00001596
Iteration 195/1000 | Loss: 0.00001596
Iteration 196/1000 | Loss: 0.00001596
Iteration 197/1000 | Loss: 0.00001596
Iteration 198/1000 | Loss: 0.00001596
Iteration 199/1000 | Loss: 0.00001596
Iteration 200/1000 | Loss: 0.00001596
Iteration 201/1000 | Loss: 0.00001596
Iteration 202/1000 | Loss: 0.00001596
Iteration 203/1000 | Loss: 0.00001596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.5960136806825176e-05, 1.5960136806825176e-05, 1.5960136806825176e-05, 1.5960136806825176e-05, 1.5960136806825176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5960136806825176e-05

Optimization complete. Final v2v error: 3.3455183506011963 mm

Highest mean error: 3.7654542922973633 mm for frame 134

Lowest mean error: 3.043224573135376 mm for frame 23

Saving results

Total time: 128.21286034584045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397992
Iteration 2/25 | Loss: 0.00131813
Iteration 3/25 | Loss: 0.00120993
Iteration 4/25 | Loss: 0.00119327
Iteration 5/25 | Loss: 0.00118822
Iteration 6/25 | Loss: 0.00118712
Iteration 7/25 | Loss: 0.00118707
Iteration 8/25 | Loss: 0.00118707
Iteration 9/25 | Loss: 0.00118707
Iteration 10/25 | Loss: 0.00118707
Iteration 11/25 | Loss: 0.00118707
Iteration 12/25 | Loss: 0.00118707
Iteration 13/25 | Loss: 0.00118707
Iteration 14/25 | Loss: 0.00118707
Iteration 15/25 | Loss: 0.00118707
Iteration 16/25 | Loss: 0.00118707
Iteration 17/25 | Loss: 0.00118707
Iteration 18/25 | Loss: 0.00118707
Iteration 19/25 | Loss: 0.00118707
Iteration 20/25 | Loss: 0.00118707
Iteration 21/25 | Loss: 0.00118707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011870701564475894, 0.0011870701564475894, 0.0011870701564475894, 0.0011870701564475894, 0.0011870701564475894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011870701564475894

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45007288
Iteration 2/25 | Loss: 0.00055098
Iteration 3/25 | Loss: 0.00055097
Iteration 4/25 | Loss: 0.00055097
Iteration 5/25 | Loss: 0.00055097
Iteration 6/25 | Loss: 0.00055097
Iteration 7/25 | Loss: 0.00055097
Iteration 8/25 | Loss: 0.00055097
Iteration 9/25 | Loss: 0.00055097
Iteration 10/25 | Loss: 0.00055097
Iteration 11/25 | Loss: 0.00055097
Iteration 12/25 | Loss: 0.00055097
Iteration 13/25 | Loss: 0.00055097
Iteration 14/25 | Loss: 0.00055097
Iteration 15/25 | Loss: 0.00055097
Iteration 16/25 | Loss: 0.00055097
Iteration 17/25 | Loss: 0.00055097
Iteration 18/25 | Loss: 0.00055097
Iteration 19/25 | Loss: 0.00055097
Iteration 20/25 | Loss: 0.00055097
Iteration 21/25 | Loss: 0.00055097
Iteration 22/25 | Loss: 0.00055097
Iteration 23/25 | Loss: 0.00055097
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005509690381586552, 0.0005509690381586552, 0.0005509690381586552, 0.0005509690381586552, 0.0005509690381586552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005509690381586552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055097
Iteration 2/1000 | Loss: 0.00004131
Iteration 3/1000 | Loss: 0.00003028
Iteration 4/1000 | Loss: 0.00002469
Iteration 5/1000 | Loss: 0.00002245
Iteration 6/1000 | Loss: 0.00002102
Iteration 7/1000 | Loss: 0.00001985
Iteration 8/1000 | Loss: 0.00001921
Iteration 9/1000 | Loss: 0.00001863
Iteration 10/1000 | Loss: 0.00001834
Iteration 11/1000 | Loss: 0.00001814
Iteration 12/1000 | Loss: 0.00001796
Iteration 13/1000 | Loss: 0.00001776
Iteration 14/1000 | Loss: 0.00001759
Iteration 15/1000 | Loss: 0.00001755
Iteration 16/1000 | Loss: 0.00001750
Iteration 17/1000 | Loss: 0.00001745
Iteration 18/1000 | Loss: 0.00001742
Iteration 19/1000 | Loss: 0.00001741
Iteration 20/1000 | Loss: 0.00001738
Iteration 21/1000 | Loss: 0.00001733
Iteration 22/1000 | Loss: 0.00001732
Iteration 23/1000 | Loss: 0.00001731
Iteration 24/1000 | Loss: 0.00001729
Iteration 25/1000 | Loss: 0.00001725
Iteration 26/1000 | Loss: 0.00001725
Iteration 27/1000 | Loss: 0.00001723
Iteration 28/1000 | Loss: 0.00001723
Iteration 29/1000 | Loss: 0.00001722
Iteration 30/1000 | Loss: 0.00001722
Iteration 31/1000 | Loss: 0.00001721
Iteration 32/1000 | Loss: 0.00001721
Iteration 33/1000 | Loss: 0.00001721
Iteration 34/1000 | Loss: 0.00001721
Iteration 35/1000 | Loss: 0.00001721
Iteration 36/1000 | Loss: 0.00001720
Iteration 37/1000 | Loss: 0.00001720
Iteration 38/1000 | Loss: 0.00001720
Iteration 39/1000 | Loss: 0.00001719
Iteration 40/1000 | Loss: 0.00001719
Iteration 41/1000 | Loss: 0.00001719
Iteration 42/1000 | Loss: 0.00001718
Iteration 43/1000 | Loss: 0.00001718
Iteration 44/1000 | Loss: 0.00001718
Iteration 45/1000 | Loss: 0.00001718
Iteration 46/1000 | Loss: 0.00001718
Iteration 47/1000 | Loss: 0.00001717
Iteration 48/1000 | Loss: 0.00001717
Iteration 49/1000 | Loss: 0.00001717
Iteration 50/1000 | Loss: 0.00001717
Iteration 51/1000 | Loss: 0.00001717
Iteration 52/1000 | Loss: 0.00001717
Iteration 53/1000 | Loss: 0.00001716
Iteration 54/1000 | Loss: 0.00001716
Iteration 55/1000 | Loss: 0.00001716
Iteration 56/1000 | Loss: 0.00001716
Iteration 57/1000 | Loss: 0.00001715
Iteration 58/1000 | Loss: 0.00001715
Iteration 59/1000 | Loss: 0.00001715
Iteration 60/1000 | Loss: 0.00001714
Iteration 61/1000 | Loss: 0.00001714
Iteration 62/1000 | Loss: 0.00001714
Iteration 63/1000 | Loss: 0.00001713
Iteration 64/1000 | Loss: 0.00001713
Iteration 65/1000 | Loss: 0.00001713
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001711
Iteration 72/1000 | Loss: 0.00001711
Iteration 73/1000 | Loss: 0.00001711
Iteration 74/1000 | Loss: 0.00001710
Iteration 75/1000 | Loss: 0.00001710
Iteration 76/1000 | Loss: 0.00001709
Iteration 77/1000 | Loss: 0.00001709
Iteration 78/1000 | Loss: 0.00001709
Iteration 79/1000 | Loss: 0.00001709
Iteration 80/1000 | Loss: 0.00001708
Iteration 81/1000 | Loss: 0.00001708
Iteration 82/1000 | Loss: 0.00001708
Iteration 83/1000 | Loss: 0.00001707
Iteration 84/1000 | Loss: 0.00001707
Iteration 85/1000 | Loss: 0.00001707
Iteration 86/1000 | Loss: 0.00001707
Iteration 87/1000 | Loss: 0.00001707
Iteration 88/1000 | Loss: 0.00001707
Iteration 89/1000 | Loss: 0.00001706
Iteration 90/1000 | Loss: 0.00001706
Iteration 91/1000 | Loss: 0.00001706
Iteration 92/1000 | Loss: 0.00001705
Iteration 93/1000 | Loss: 0.00001705
Iteration 94/1000 | Loss: 0.00001705
Iteration 95/1000 | Loss: 0.00001705
Iteration 96/1000 | Loss: 0.00001705
Iteration 97/1000 | Loss: 0.00001705
Iteration 98/1000 | Loss: 0.00001705
Iteration 99/1000 | Loss: 0.00001704
Iteration 100/1000 | Loss: 0.00001704
Iteration 101/1000 | Loss: 0.00001704
Iteration 102/1000 | Loss: 0.00001704
Iteration 103/1000 | Loss: 0.00001704
Iteration 104/1000 | Loss: 0.00001704
Iteration 105/1000 | Loss: 0.00001704
Iteration 106/1000 | Loss: 0.00001704
Iteration 107/1000 | Loss: 0.00001704
Iteration 108/1000 | Loss: 0.00001704
Iteration 109/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.704195892671123e-05, 1.704195892671123e-05, 1.704195892671123e-05, 1.704195892671123e-05, 1.704195892671123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.704195892671123e-05

Optimization complete. Final v2v error: 3.4728469848632812 mm

Highest mean error: 4.291446208953857 mm for frame 23

Lowest mean error: 2.9843809604644775 mm for frame 10

Saving results

Total time: 38.662211656570435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871574
Iteration 2/25 | Loss: 0.00179715
Iteration 3/25 | Loss: 0.00146422
Iteration 4/25 | Loss: 0.00141374
Iteration 5/25 | Loss: 0.00141430
Iteration 6/25 | Loss: 0.00139679
Iteration 7/25 | Loss: 0.00133444
Iteration 8/25 | Loss: 0.00132154
Iteration 9/25 | Loss: 0.00131519
Iteration 10/25 | Loss: 0.00129535
Iteration 11/25 | Loss: 0.00129112
Iteration 12/25 | Loss: 0.00128509
Iteration 13/25 | Loss: 0.00128334
Iteration 14/25 | Loss: 0.00128292
Iteration 15/25 | Loss: 0.00128281
Iteration 16/25 | Loss: 0.00128281
Iteration 17/25 | Loss: 0.00128280
Iteration 18/25 | Loss: 0.00128280
Iteration 19/25 | Loss: 0.00128280
Iteration 20/25 | Loss: 0.00128280
Iteration 21/25 | Loss: 0.00128280
Iteration 22/25 | Loss: 0.00128280
Iteration 23/25 | Loss: 0.00128280
Iteration 24/25 | Loss: 0.00128280
Iteration 25/25 | Loss: 0.00128280

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98465925
Iteration 2/25 | Loss: 0.00055210
Iteration 3/25 | Loss: 0.00055209
Iteration 4/25 | Loss: 0.00055209
Iteration 5/25 | Loss: 0.00055209
Iteration 6/25 | Loss: 0.00055209
Iteration 7/25 | Loss: 0.00055209
Iteration 8/25 | Loss: 0.00055209
Iteration 9/25 | Loss: 0.00055209
Iteration 10/25 | Loss: 0.00055209
Iteration 11/25 | Loss: 0.00055209
Iteration 12/25 | Loss: 0.00055209
Iteration 13/25 | Loss: 0.00055209
Iteration 14/25 | Loss: 0.00055208
Iteration 15/25 | Loss: 0.00055208
Iteration 16/25 | Loss: 0.00055208
Iteration 17/25 | Loss: 0.00055208
Iteration 18/25 | Loss: 0.00055208
Iteration 19/25 | Loss: 0.00055208
Iteration 20/25 | Loss: 0.00055208
Iteration 21/25 | Loss: 0.00055208
Iteration 22/25 | Loss: 0.00055208
Iteration 23/25 | Loss: 0.00055208
Iteration 24/25 | Loss: 0.00055208
Iteration 25/25 | Loss: 0.00055208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055208
Iteration 2/1000 | Loss: 0.00004652
Iteration 3/1000 | Loss: 0.00003773
Iteration 4/1000 | Loss: 0.00003523
Iteration 5/1000 | Loss: 0.00003406
Iteration 6/1000 | Loss: 0.00003289
Iteration 7/1000 | Loss: 0.00003195
Iteration 8/1000 | Loss: 0.00003152
Iteration 9/1000 | Loss: 0.00003093
Iteration 10/1000 | Loss: 0.00003058
Iteration 11/1000 | Loss: 0.00003025
Iteration 12/1000 | Loss: 0.00003021
Iteration 13/1000 | Loss: 0.00003197
Iteration 14/1000 | Loss: 0.00003006
Iteration 15/1000 | Loss: 0.00003003
Iteration 16/1000 | Loss: 0.00002955
Iteration 17/1000 | Loss: 0.00002927
Iteration 18/1000 | Loss: 0.00002906
Iteration 19/1000 | Loss: 0.00002881
Iteration 20/1000 | Loss: 0.00002864
Iteration 21/1000 | Loss: 0.00002860
Iteration 22/1000 | Loss: 0.00002860
Iteration 23/1000 | Loss: 0.00002859
Iteration 24/1000 | Loss: 0.00002858
Iteration 25/1000 | Loss: 0.00002858
Iteration 26/1000 | Loss: 0.00002858
Iteration 27/1000 | Loss: 0.00002857
Iteration 28/1000 | Loss: 0.00002857
Iteration 29/1000 | Loss: 0.00002857
Iteration 30/1000 | Loss: 0.00002857
Iteration 31/1000 | Loss: 0.00002857
Iteration 32/1000 | Loss: 0.00002857
Iteration 33/1000 | Loss: 0.00002857
Iteration 34/1000 | Loss: 0.00002857
Iteration 35/1000 | Loss: 0.00002857
Iteration 36/1000 | Loss: 0.00002857
Iteration 37/1000 | Loss: 0.00002856
Iteration 38/1000 | Loss: 0.00002856
Iteration 39/1000 | Loss: 0.00002855
Iteration 40/1000 | Loss: 0.00002855
Iteration 41/1000 | Loss: 0.00002855
Iteration 42/1000 | Loss: 0.00002854
Iteration 43/1000 | Loss: 0.00002854
Iteration 44/1000 | Loss: 0.00002853
Iteration 45/1000 | Loss: 0.00002853
Iteration 46/1000 | Loss: 0.00002852
Iteration 47/1000 | Loss: 0.00002852
Iteration 48/1000 | Loss: 0.00002852
Iteration 49/1000 | Loss: 0.00002852
Iteration 50/1000 | Loss: 0.00002852
Iteration 51/1000 | Loss: 0.00002851
Iteration 52/1000 | Loss: 0.00002851
Iteration 53/1000 | Loss: 0.00002851
Iteration 54/1000 | Loss: 0.00002851
Iteration 55/1000 | Loss: 0.00002851
Iteration 56/1000 | Loss: 0.00002850
Iteration 57/1000 | Loss: 0.00002850
Iteration 58/1000 | Loss: 0.00002850
Iteration 59/1000 | Loss: 0.00002850
Iteration 60/1000 | Loss: 0.00002850
Iteration 61/1000 | Loss: 0.00002850
Iteration 62/1000 | Loss: 0.00002850
Iteration 63/1000 | Loss: 0.00002850
Iteration 64/1000 | Loss: 0.00002849
Iteration 65/1000 | Loss: 0.00002849
Iteration 66/1000 | Loss: 0.00002849
Iteration 67/1000 | Loss: 0.00002849
Iteration 68/1000 | Loss: 0.00002849
Iteration 69/1000 | Loss: 0.00002849
Iteration 70/1000 | Loss: 0.00002849
Iteration 71/1000 | Loss: 0.00002849
Iteration 72/1000 | Loss: 0.00002849
Iteration 73/1000 | Loss: 0.00002848
Iteration 74/1000 | Loss: 0.00002848
Iteration 75/1000 | Loss: 0.00002848
Iteration 76/1000 | Loss: 0.00002848
Iteration 77/1000 | Loss: 0.00002848
Iteration 78/1000 | Loss: 0.00002848
Iteration 79/1000 | Loss: 0.00002848
Iteration 80/1000 | Loss: 0.00002847
Iteration 81/1000 | Loss: 0.00002847
Iteration 82/1000 | Loss: 0.00002847
Iteration 83/1000 | Loss: 0.00002846
Iteration 84/1000 | Loss: 0.00002846
Iteration 85/1000 | Loss: 0.00002846
Iteration 86/1000 | Loss: 0.00002846
Iteration 87/1000 | Loss: 0.00002846
Iteration 88/1000 | Loss: 0.00002846
Iteration 89/1000 | Loss: 0.00002846
Iteration 90/1000 | Loss: 0.00002845
Iteration 91/1000 | Loss: 0.00002845
Iteration 92/1000 | Loss: 0.00002845
Iteration 93/1000 | Loss: 0.00002845
Iteration 94/1000 | Loss: 0.00002845
Iteration 95/1000 | Loss: 0.00002845
Iteration 96/1000 | Loss: 0.00002845
Iteration 97/1000 | Loss: 0.00002845
Iteration 98/1000 | Loss: 0.00002845
Iteration 99/1000 | Loss: 0.00002844
Iteration 100/1000 | Loss: 0.00002844
Iteration 101/1000 | Loss: 0.00002844
Iteration 102/1000 | Loss: 0.00002844
Iteration 103/1000 | Loss: 0.00002844
Iteration 104/1000 | Loss: 0.00002844
Iteration 105/1000 | Loss: 0.00002844
Iteration 106/1000 | Loss: 0.00002844
Iteration 107/1000 | Loss: 0.00002844
Iteration 108/1000 | Loss: 0.00002844
Iteration 109/1000 | Loss: 0.00002844
Iteration 110/1000 | Loss: 0.00002844
Iteration 111/1000 | Loss: 0.00002843
Iteration 112/1000 | Loss: 0.00002843
Iteration 113/1000 | Loss: 0.00002843
Iteration 114/1000 | Loss: 0.00002843
Iteration 115/1000 | Loss: 0.00002843
Iteration 116/1000 | Loss: 0.00002842
Iteration 117/1000 | Loss: 0.00002842
Iteration 118/1000 | Loss: 0.00002842
Iteration 119/1000 | Loss: 0.00002842
Iteration 120/1000 | Loss: 0.00002842
Iteration 121/1000 | Loss: 0.00002842
Iteration 122/1000 | Loss: 0.00002842
Iteration 123/1000 | Loss: 0.00002842
Iteration 124/1000 | Loss: 0.00002842
Iteration 125/1000 | Loss: 0.00002842
Iteration 126/1000 | Loss: 0.00002842
Iteration 127/1000 | Loss: 0.00002842
Iteration 128/1000 | Loss: 0.00002842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.8424636184354313e-05, 2.8424636184354313e-05, 2.8424636184354313e-05, 2.8424636184354313e-05, 2.8424636184354313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8424636184354313e-05

Optimization complete. Final v2v error: 4.410648822784424 mm

Highest mean error: 5.098148345947266 mm for frame 46

Lowest mean error: 4.346930503845215 mm for frame 87

Saving results

Total time: 58.88296699523926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00762344
Iteration 2/25 | Loss: 0.00214424
Iteration 3/25 | Loss: 0.00162130
Iteration 4/25 | Loss: 0.00159804
Iteration 5/25 | Loss: 0.00145726
Iteration 6/25 | Loss: 0.00141721
Iteration 7/25 | Loss: 0.00138504
Iteration 8/25 | Loss: 0.00137323
Iteration 9/25 | Loss: 0.00135135
Iteration 10/25 | Loss: 0.00134936
Iteration 11/25 | Loss: 0.00133669
Iteration 12/25 | Loss: 0.00133347
Iteration 13/25 | Loss: 0.00133236
Iteration 14/25 | Loss: 0.00133410
Iteration 15/25 | Loss: 0.00133112
Iteration 16/25 | Loss: 0.00132481
Iteration 17/25 | Loss: 0.00132704
Iteration 18/25 | Loss: 0.00132884
Iteration 19/25 | Loss: 0.00131911
Iteration 20/25 | Loss: 0.00131776
Iteration 21/25 | Loss: 0.00131761
Iteration 22/25 | Loss: 0.00131759
Iteration 23/25 | Loss: 0.00131759
Iteration 24/25 | Loss: 0.00131758
Iteration 25/25 | Loss: 0.00131758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35984528
Iteration 2/25 | Loss: 0.00072531
Iteration 3/25 | Loss: 0.00072531
Iteration 4/25 | Loss: 0.00072531
Iteration 5/25 | Loss: 0.00072531
Iteration 6/25 | Loss: 0.00072531
Iteration 7/25 | Loss: 0.00072531
Iteration 8/25 | Loss: 0.00072531
Iteration 9/25 | Loss: 0.00072531
Iteration 10/25 | Loss: 0.00072531
Iteration 11/25 | Loss: 0.00072531
Iteration 12/25 | Loss: 0.00072531
Iteration 13/25 | Loss: 0.00072531
Iteration 14/25 | Loss: 0.00072531
Iteration 15/25 | Loss: 0.00072531
Iteration 16/25 | Loss: 0.00072531
Iteration 17/25 | Loss: 0.00072531
Iteration 18/25 | Loss: 0.00072531
Iteration 19/25 | Loss: 0.00072531
Iteration 20/25 | Loss: 0.00072531
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007253123330883682, 0.0007253123330883682, 0.0007253123330883682, 0.0007253123330883682, 0.0007253123330883682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007253123330883682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072531
Iteration 2/1000 | Loss: 0.00028695
Iteration 3/1000 | Loss: 0.00006179
Iteration 4/1000 | Loss: 0.00004478
Iteration 5/1000 | Loss: 0.00003985
Iteration 6/1000 | Loss: 0.00003733
Iteration 7/1000 | Loss: 0.00003557
Iteration 8/1000 | Loss: 0.00003446
Iteration 9/1000 | Loss: 0.00070259
Iteration 10/1000 | Loss: 0.00004851
Iteration 11/1000 | Loss: 0.00003713
Iteration 12/1000 | Loss: 0.00003307
Iteration 13/1000 | Loss: 0.00002927
Iteration 14/1000 | Loss: 0.00002686
Iteration 15/1000 | Loss: 0.00002551
Iteration 16/1000 | Loss: 0.00002471
Iteration 17/1000 | Loss: 0.00002405
Iteration 18/1000 | Loss: 0.00002346
Iteration 19/1000 | Loss: 0.00002305
Iteration 20/1000 | Loss: 0.00002267
Iteration 21/1000 | Loss: 0.00002240
Iteration 22/1000 | Loss: 0.00002221
Iteration 23/1000 | Loss: 0.00002202
Iteration 24/1000 | Loss: 0.00002202
Iteration 25/1000 | Loss: 0.00002202
Iteration 26/1000 | Loss: 0.00002201
Iteration 27/1000 | Loss: 0.00002201
Iteration 28/1000 | Loss: 0.00002199
Iteration 29/1000 | Loss: 0.00002193
Iteration 30/1000 | Loss: 0.00002193
Iteration 31/1000 | Loss: 0.00002189
Iteration 32/1000 | Loss: 0.00002188
Iteration 33/1000 | Loss: 0.00002188
Iteration 34/1000 | Loss: 0.00002188
Iteration 35/1000 | Loss: 0.00002188
Iteration 36/1000 | Loss: 0.00002188
Iteration 37/1000 | Loss: 0.00002188
Iteration 38/1000 | Loss: 0.00002186
Iteration 39/1000 | Loss: 0.00002185
Iteration 40/1000 | Loss: 0.00002184
Iteration 41/1000 | Loss: 0.00002183
Iteration 42/1000 | Loss: 0.00002182
Iteration 43/1000 | Loss: 0.00002182
Iteration 44/1000 | Loss: 0.00002181
Iteration 45/1000 | Loss: 0.00002181
Iteration 46/1000 | Loss: 0.00002181
Iteration 47/1000 | Loss: 0.00002181
Iteration 48/1000 | Loss: 0.00002181
Iteration 49/1000 | Loss: 0.00002181
Iteration 50/1000 | Loss: 0.00002180
Iteration 51/1000 | Loss: 0.00002180
Iteration 52/1000 | Loss: 0.00002180
Iteration 53/1000 | Loss: 0.00002180
Iteration 54/1000 | Loss: 0.00002180
Iteration 55/1000 | Loss: 0.00002180
Iteration 56/1000 | Loss: 0.00002179
Iteration 57/1000 | Loss: 0.00002179
Iteration 58/1000 | Loss: 0.00002179
Iteration 59/1000 | Loss: 0.00002178
Iteration 60/1000 | Loss: 0.00002178
Iteration 61/1000 | Loss: 0.00002178
Iteration 62/1000 | Loss: 0.00002178
Iteration 63/1000 | Loss: 0.00002178
Iteration 64/1000 | Loss: 0.00002178
Iteration 65/1000 | Loss: 0.00002177
Iteration 66/1000 | Loss: 0.00002177
Iteration 67/1000 | Loss: 0.00002177
Iteration 68/1000 | Loss: 0.00002177
Iteration 69/1000 | Loss: 0.00002177
Iteration 70/1000 | Loss: 0.00002177
Iteration 71/1000 | Loss: 0.00002177
Iteration 72/1000 | Loss: 0.00002177
Iteration 73/1000 | Loss: 0.00002177
Iteration 74/1000 | Loss: 0.00002176
Iteration 75/1000 | Loss: 0.00002176
Iteration 76/1000 | Loss: 0.00002176
Iteration 77/1000 | Loss: 0.00002176
Iteration 78/1000 | Loss: 0.00002176
Iteration 79/1000 | Loss: 0.00002176
Iteration 80/1000 | Loss: 0.00002176
Iteration 81/1000 | Loss: 0.00002176
Iteration 82/1000 | Loss: 0.00002176
Iteration 83/1000 | Loss: 0.00002176
Iteration 84/1000 | Loss: 0.00002175
Iteration 85/1000 | Loss: 0.00002175
Iteration 86/1000 | Loss: 0.00002175
Iteration 87/1000 | Loss: 0.00002175
Iteration 88/1000 | Loss: 0.00002175
Iteration 89/1000 | Loss: 0.00002175
Iteration 90/1000 | Loss: 0.00002175
Iteration 91/1000 | Loss: 0.00002175
Iteration 92/1000 | Loss: 0.00002175
Iteration 93/1000 | Loss: 0.00002175
Iteration 94/1000 | Loss: 0.00002175
Iteration 95/1000 | Loss: 0.00002175
Iteration 96/1000 | Loss: 0.00002175
Iteration 97/1000 | Loss: 0.00002175
Iteration 98/1000 | Loss: 0.00002175
Iteration 99/1000 | Loss: 0.00002175
Iteration 100/1000 | Loss: 0.00002175
Iteration 101/1000 | Loss: 0.00002174
Iteration 102/1000 | Loss: 0.00002174
Iteration 103/1000 | Loss: 0.00002174
Iteration 104/1000 | Loss: 0.00002174
Iteration 105/1000 | Loss: 0.00002174
Iteration 106/1000 | Loss: 0.00002174
Iteration 107/1000 | Loss: 0.00002174
Iteration 108/1000 | Loss: 0.00002174
Iteration 109/1000 | Loss: 0.00002174
Iteration 110/1000 | Loss: 0.00002174
Iteration 111/1000 | Loss: 0.00002174
Iteration 112/1000 | Loss: 0.00002174
Iteration 113/1000 | Loss: 0.00002174
Iteration 114/1000 | Loss: 0.00002174
Iteration 115/1000 | Loss: 0.00002174
Iteration 116/1000 | Loss: 0.00002174
Iteration 117/1000 | Loss: 0.00002174
Iteration 118/1000 | Loss: 0.00002174
Iteration 119/1000 | Loss: 0.00002173
Iteration 120/1000 | Loss: 0.00002173
Iteration 121/1000 | Loss: 0.00002173
Iteration 122/1000 | Loss: 0.00002173
Iteration 123/1000 | Loss: 0.00002173
Iteration 124/1000 | Loss: 0.00002173
Iteration 125/1000 | Loss: 0.00002173
Iteration 126/1000 | Loss: 0.00002173
Iteration 127/1000 | Loss: 0.00002173
Iteration 128/1000 | Loss: 0.00002173
Iteration 129/1000 | Loss: 0.00002173
Iteration 130/1000 | Loss: 0.00002173
Iteration 131/1000 | Loss: 0.00002173
Iteration 132/1000 | Loss: 0.00002172
Iteration 133/1000 | Loss: 0.00002172
Iteration 134/1000 | Loss: 0.00002172
Iteration 135/1000 | Loss: 0.00002172
Iteration 136/1000 | Loss: 0.00002172
Iteration 137/1000 | Loss: 0.00002172
Iteration 138/1000 | Loss: 0.00002172
Iteration 139/1000 | Loss: 0.00002172
Iteration 140/1000 | Loss: 0.00002172
Iteration 141/1000 | Loss: 0.00002172
Iteration 142/1000 | Loss: 0.00002172
Iteration 143/1000 | Loss: 0.00002172
Iteration 144/1000 | Loss: 0.00002172
Iteration 145/1000 | Loss: 0.00002172
Iteration 146/1000 | Loss: 0.00002172
Iteration 147/1000 | Loss: 0.00002172
Iteration 148/1000 | Loss: 0.00002172
Iteration 149/1000 | Loss: 0.00002171
Iteration 150/1000 | Loss: 0.00002171
Iteration 151/1000 | Loss: 0.00002171
Iteration 152/1000 | Loss: 0.00002171
Iteration 153/1000 | Loss: 0.00002171
Iteration 154/1000 | Loss: 0.00002171
Iteration 155/1000 | Loss: 0.00002171
Iteration 156/1000 | Loss: 0.00002171
Iteration 157/1000 | Loss: 0.00002171
Iteration 158/1000 | Loss: 0.00002171
Iteration 159/1000 | Loss: 0.00002171
Iteration 160/1000 | Loss: 0.00002171
Iteration 161/1000 | Loss: 0.00002171
Iteration 162/1000 | Loss: 0.00002171
Iteration 163/1000 | Loss: 0.00002171
Iteration 164/1000 | Loss: 0.00002171
Iteration 165/1000 | Loss: 0.00002171
Iteration 166/1000 | Loss: 0.00002171
Iteration 167/1000 | Loss: 0.00002171
Iteration 168/1000 | Loss: 0.00002171
Iteration 169/1000 | Loss: 0.00002171
Iteration 170/1000 | Loss: 0.00002171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.1705924154957756e-05, 2.1705924154957756e-05, 2.1705924154957756e-05, 2.1705924154957756e-05, 2.1705924154957756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1705924154957756e-05

Optimization complete. Final v2v error: 3.8684518337249756 mm

Highest mean error: 4.223531246185303 mm for frame 5

Lowest mean error: 3.6550750732421875 mm for frame 151

Saving results

Total time: 90.26769018173218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784513
Iteration 2/25 | Loss: 0.00153891
Iteration 3/25 | Loss: 0.00125508
Iteration 4/25 | Loss: 0.00123249
Iteration 5/25 | Loss: 0.00122899
Iteration 6/25 | Loss: 0.00122886
Iteration 7/25 | Loss: 0.00122886
Iteration 8/25 | Loss: 0.00122886
Iteration 9/25 | Loss: 0.00122886
Iteration 10/25 | Loss: 0.00122886
Iteration 11/25 | Loss: 0.00122886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012288629077374935, 0.0012288629077374935, 0.0012288629077374935, 0.0012288629077374935, 0.0012288629077374935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012288629077374935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45482278
Iteration 2/25 | Loss: 0.00069162
Iteration 3/25 | Loss: 0.00069162
Iteration 4/25 | Loss: 0.00069162
Iteration 5/25 | Loss: 0.00069162
Iteration 6/25 | Loss: 0.00069162
Iteration 7/25 | Loss: 0.00069161
Iteration 8/25 | Loss: 0.00069161
Iteration 9/25 | Loss: 0.00069161
Iteration 10/25 | Loss: 0.00069161
Iteration 11/25 | Loss: 0.00069161
Iteration 12/25 | Loss: 0.00069161
Iteration 13/25 | Loss: 0.00069161
Iteration 14/25 | Loss: 0.00069161
Iteration 15/25 | Loss: 0.00069161
Iteration 16/25 | Loss: 0.00069161
Iteration 17/25 | Loss: 0.00069161
Iteration 18/25 | Loss: 0.00069161
Iteration 19/25 | Loss: 0.00069161
Iteration 20/25 | Loss: 0.00069161
Iteration 21/25 | Loss: 0.00069161
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000691613822709769, 0.000691613822709769, 0.000691613822709769, 0.000691613822709769, 0.000691613822709769]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000691613822709769

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069161
Iteration 2/1000 | Loss: 0.00003273
Iteration 3/1000 | Loss: 0.00002455
Iteration 4/1000 | Loss: 0.00002028
Iteration 5/1000 | Loss: 0.00001890
Iteration 6/1000 | Loss: 0.00001767
Iteration 7/1000 | Loss: 0.00001699
Iteration 8/1000 | Loss: 0.00001651
Iteration 9/1000 | Loss: 0.00001623
Iteration 10/1000 | Loss: 0.00001593
Iteration 11/1000 | Loss: 0.00001577
Iteration 12/1000 | Loss: 0.00001575
Iteration 13/1000 | Loss: 0.00001559
Iteration 14/1000 | Loss: 0.00001558
Iteration 15/1000 | Loss: 0.00001558
Iteration 16/1000 | Loss: 0.00001552
Iteration 17/1000 | Loss: 0.00001551
Iteration 18/1000 | Loss: 0.00001549
Iteration 19/1000 | Loss: 0.00001549
Iteration 20/1000 | Loss: 0.00001545
Iteration 21/1000 | Loss: 0.00001539
Iteration 22/1000 | Loss: 0.00001536
Iteration 23/1000 | Loss: 0.00001536
Iteration 24/1000 | Loss: 0.00001536
Iteration 25/1000 | Loss: 0.00001536
Iteration 26/1000 | Loss: 0.00001536
Iteration 27/1000 | Loss: 0.00001536
Iteration 28/1000 | Loss: 0.00001536
Iteration 29/1000 | Loss: 0.00001536
Iteration 30/1000 | Loss: 0.00001535
Iteration 31/1000 | Loss: 0.00001535
Iteration 32/1000 | Loss: 0.00001535
Iteration 33/1000 | Loss: 0.00001535
Iteration 34/1000 | Loss: 0.00001535
Iteration 35/1000 | Loss: 0.00001535
Iteration 36/1000 | Loss: 0.00001535
Iteration 37/1000 | Loss: 0.00001535
Iteration 38/1000 | Loss: 0.00001535
Iteration 39/1000 | Loss: 0.00001535
Iteration 40/1000 | Loss: 0.00001535
Iteration 41/1000 | Loss: 0.00001535
Iteration 42/1000 | Loss: 0.00001534
Iteration 43/1000 | Loss: 0.00001534
Iteration 44/1000 | Loss: 0.00001534
Iteration 45/1000 | Loss: 0.00001534
Iteration 46/1000 | Loss: 0.00001534
Iteration 47/1000 | Loss: 0.00001534
Iteration 48/1000 | Loss: 0.00001534
Iteration 49/1000 | Loss: 0.00001533
Iteration 50/1000 | Loss: 0.00001532
Iteration 51/1000 | Loss: 0.00001532
Iteration 52/1000 | Loss: 0.00001531
Iteration 53/1000 | Loss: 0.00001531
Iteration 54/1000 | Loss: 0.00001530
Iteration 55/1000 | Loss: 0.00001529
Iteration 56/1000 | Loss: 0.00001528
Iteration 57/1000 | Loss: 0.00001528
Iteration 58/1000 | Loss: 0.00001528
Iteration 59/1000 | Loss: 0.00001528
Iteration 60/1000 | Loss: 0.00001528
Iteration 61/1000 | Loss: 0.00001528
Iteration 62/1000 | Loss: 0.00001527
Iteration 63/1000 | Loss: 0.00001527
Iteration 64/1000 | Loss: 0.00001527
Iteration 65/1000 | Loss: 0.00001527
Iteration 66/1000 | Loss: 0.00001526
Iteration 67/1000 | Loss: 0.00001526
Iteration 68/1000 | Loss: 0.00001525
Iteration 69/1000 | Loss: 0.00001524
Iteration 70/1000 | Loss: 0.00001524
Iteration 71/1000 | Loss: 0.00001524
Iteration 72/1000 | Loss: 0.00001524
Iteration 73/1000 | Loss: 0.00001523
Iteration 74/1000 | Loss: 0.00001523
Iteration 75/1000 | Loss: 0.00001523
Iteration 76/1000 | Loss: 0.00001523
Iteration 77/1000 | Loss: 0.00001523
Iteration 78/1000 | Loss: 0.00001523
Iteration 79/1000 | Loss: 0.00001523
Iteration 80/1000 | Loss: 0.00001523
Iteration 81/1000 | Loss: 0.00001523
Iteration 82/1000 | Loss: 0.00001522
Iteration 83/1000 | Loss: 0.00001522
Iteration 84/1000 | Loss: 0.00001522
Iteration 85/1000 | Loss: 0.00001522
Iteration 86/1000 | Loss: 0.00001522
Iteration 87/1000 | Loss: 0.00001522
Iteration 88/1000 | Loss: 0.00001522
Iteration 89/1000 | Loss: 0.00001521
Iteration 90/1000 | Loss: 0.00001521
Iteration 91/1000 | Loss: 0.00001521
Iteration 92/1000 | Loss: 0.00001521
Iteration 93/1000 | Loss: 0.00001521
Iteration 94/1000 | Loss: 0.00001521
Iteration 95/1000 | Loss: 0.00001520
Iteration 96/1000 | Loss: 0.00001520
Iteration 97/1000 | Loss: 0.00001520
Iteration 98/1000 | Loss: 0.00001520
Iteration 99/1000 | Loss: 0.00001520
Iteration 100/1000 | Loss: 0.00001520
Iteration 101/1000 | Loss: 0.00001519
Iteration 102/1000 | Loss: 0.00001519
Iteration 103/1000 | Loss: 0.00001519
Iteration 104/1000 | Loss: 0.00001519
Iteration 105/1000 | Loss: 0.00001519
Iteration 106/1000 | Loss: 0.00001519
Iteration 107/1000 | Loss: 0.00001519
Iteration 108/1000 | Loss: 0.00001519
Iteration 109/1000 | Loss: 0.00001519
Iteration 110/1000 | Loss: 0.00001519
Iteration 111/1000 | Loss: 0.00001519
Iteration 112/1000 | Loss: 0.00001519
Iteration 113/1000 | Loss: 0.00001519
Iteration 114/1000 | Loss: 0.00001519
Iteration 115/1000 | Loss: 0.00001519
Iteration 116/1000 | Loss: 0.00001519
Iteration 117/1000 | Loss: 0.00001519
Iteration 118/1000 | Loss: 0.00001519
Iteration 119/1000 | Loss: 0.00001519
Iteration 120/1000 | Loss: 0.00001519
Iteration 121/1000 | Loss: 0.00001519
Iteration 122/1000 | Loss: 0.00001519
Iteration 123/1000 | Loss: 0.00001519
Iteration 124/1000 | Loss: 0.00001519
Iteration 125/1000 | Loss: 0.00001519
Iteration 126/1000 | Loss: 0.00001519
Iteration 127/1000 | Loss: 0.00001519
Iteration 128/1000 | Loss: 0.00001519
Iteration 129/1000 | Loss: 0.00001519
Iteration 130/1000 | Loss: 0.00001519
Iteration 131/1000 | Loss: 0.00001519
Iteration 132/1000 | Loss: 0.00001519
Iteration 133/1000 | Loss: 0.00001519
Iteration 134/1000 | Loss: 0.00001519
Iteration 135/1000 | Loss: 0.00001519
Iteration 136/1000 | Loss: 0.00001519
Iteration 137/1000 | Loss: 0.00001519
Iteration 138/1000 | Loss: 0.00001519
Iteration 139/1000 | Loss: 0.00001519
Iteration 140/1000 | Loss: 0.00001519
Iteration 141/1000 | Loss: 0.00001519
Iteration 142/1000 | Loss: 0.00001519
Iteration 143/1000 | Loss: 0.00001519
Iteration 144/1000 | Loss: 0.00001519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.5189065379672684e-05, 1.5189065379672684e-05, 1.5189065379672684e-05, 1.5189065379672684e-05, 1.5189065379672684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5189065379672684e-05

Optimization complete. Final v2v error: 3.276550054550171 mm

Highest mean error: 3.601746082305908 mm for frame 95

Lowest mean error: 2.9887726306915283 mm for frame 78

Saving results

Total time: 34.47527718544006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01161053
Iteration 2/25 | Loss: 0.00452621
Iteration 3/25 | Loss: 0.00309026
Iteration 4/25 | Loss: 0.00324357
Iteration 5/25 | Loss: 0.00255940
Iteration 6/25 | Loss: 0.00246583
Iteration 7/25 | Loss: 0.00236670
Iteration 8/25 | Loss: 0.00227386
Iteration 9/25 | Loss: 0.00221905
Iteration 10/25 | Loss: 0.00219591
Iteration 11/25 | Loss: 0.00210706
Iteration 12/25 | Loss: 0.00211486
Iteration 13/25 | Loss: 0.00211990
Iteration 14/25 | Loss: 0.00211603
Iteration 15/25 | Loss: 0.00209423
Iteration 16/25 | Loss: 0.00207762
Iteration 17/25 | Loss: 0.00207608
Iteration 18/25 | Loss: 0.00207513
Iteration 19/25 | Loss: 0.00207467
Iteration 20/25 | Loss: 0.00207443
Iteration 21/25 | Loss: 0.00207433
Iteration 22/25 | Loss: 0.00207432
Iteration 23/25 | Loss: 0.00207459
Iteration 24/25 | Loss: 0.00207453
Iteration 25/25 | Loss: 0.00207418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54029101
Iteration 2/25 | Loss: 0.03222459
Iteration 3/25 | Loss: 0.00657505
Iteration 4/25 | Loss: 0.00657505
Iteration 5/25 | Loss: 0.00657505
Iteration 6/25 | Loss: 0.00657505
Iteration 7/25 | Loss: 0.00657505
Iteration 8/25 | Loss: 0.00657505
Iteration 9/25 | Loss: 0.00657505
Iteration 10/25 | Loss: 0.00657505
Iteration 11/25 | Loss: 0.00657505
Iteration 12/25 | Loss: 0.00657505
Iteration 13/25 | Loss: 0.00657505
Iteration 14/25 | Loss: 0.00657505
Iteration 15/25 | Loss: 0.00657505
Iteration 16/25 | Loss: 0.00657505
Iteration 17/25 | Loss: 0.00657505
Iteration 18/25 | Loss: 0.00657505
Iteration 19/25 | Loss: 0.00657505
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.006575045175850391, 0.006575045175850391, 0.006575045175850391, 0.006575045175850391, 0.006575045175850391]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006575045175850391

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00657505
Iteration 2/1000 | Loss: 0.00080610
Iteration 3/1000 | Loss: 0.00107875
Iteration 4/1000 | Loss: 0.00120134
Iteration 5/1000 | Loss: 0.00046491
Iteration 6/1000 | Loss: 0.00040424
Iteration 7/1000 | Loss: 0.00036391
Iteration 8/1000 | Loss: 0.00089916
Iteration 9/1000 | Loss: 0.00752119
Iteration 10/1000 | Loss: 0.02356797
Iteration 11/1000 | Loss: 0.01883875
Iteration 12/1000 | Loss: 0.00499362
Iteration 13/1000 | Loss: 0.00188055
Iteration 14/1000 | Loss: 0.00053367
Iteration 15/1000 | Loss: 0.00078443
Iteration 16/1000 | Loss: 0.00029635
Iteration 17/1000 | Loss: 0.00080173
Iteration 18/1000 | Loss: 0.00164512
Iteration 19/1000 | Loss: 0.00057678
Iteration 20/1000 | Loss: 0.00133535
Iteration 21/1000 | Loss: 0.00051013
Iteration 22/1000 | Loss: 0.00016710
Iteration 23/1000 | Loss: 0.00026892
Iteration 24/1000 | Loss: 0.00027853
Iteration 25/1000 | Loss: 0.00012382
Iteration 26/1000 | Loss: 0.00012484
Iteration 27/1000 | Loss: 0.00010617
Iteration 28/1000 | Loss: 0.00105809
Iteration 29/1000 | Loss: 0.00032530
Iteration 30/1000 | Loss: 0.00010865
Iteration 31/1000 | Loss: 0.00009473
Iteration 32/1000 | Loss: 0.00054679
Iteration 33/1000 | Loss: 0.00061285
Iteration 34/1000 | Loss: 0.00031749
Iteration 35/1000 | Loss: 0.00022951
Iteration 36/1000 | Loss: 0.00024808
Iteration 37/1000 | Loss: 0.00007984
Iteration 38/1000 | Loss: 0.00040436
Iteration 39/1000 | Loss: 0.00049576
Iteration 40/1000 | Loss: 0.00063743
Iteration 41/1000 | Loss: 0.00017139
Iteration 42/1000 | Loss: 0.00038057
Iteration 43/1000 | Loss: 0.00012702
Iteration 44/1000 | Loss: 0.00007521
Iteration 45/1000 | Loss: 0.00062282
Iteration 46/1000 | Loss: 0.00037717
Iteration 47/1000 | Loss: 0.00021456
Iteration 48/1000 | Loss: 0.00042486
Iteration 49/1000 | Loss: 0.00007863
Iteration 50/1000 | Loss: 0.00007383
Iteration 51/1000 | Loss: 0.00024479
Iteration 52/1000 | Loss: 0.00038294
Iteration 53/1000 | Loss: 0.00027797
Iteration 54/1000 | Loss: 0.00031948
Iteration 55/1000 | Loss: 0.00054748
Iteration 56/1000 | Loss: 0.00007944
Iteration 57/1000 | Loss: 0.00014782
Iteration 58/1000 | Loss: 0.00012743
Iteration 59/1000 | Loss: 0.00006824
Iteration 60/1000 | Loss: 0.00006617
Iteration 61/1000 | Loss: 0.00006446
Iteration 62/1000 | Loss: 0.00006309
Iteration 63/1000 | Loss: 0.00006189
Iteration 64/1000 | Loss: 0.00005966
Iteration 65/1000 | Loss: 0.00016855
Iteration 66/1000 | Loss: 0.00057629
Iteration 67/1000 | Loss: 0.00014991
Iteration 68/1000 | Loss: 0.00012439
Iteration 69/1000 | Loss: 0.00007803
Iteration 70/1000 | Loss: 0.00005671
Iteration 71/1000 | Loss: 0.00005550
Iteration 72/1000 | Loss: 0.00005459
Iteration 73/1000 | Loss: 0.00005382
Iteration 74/1000 | Loss: 0.00005347
Iteration 75/1000 | Loss: 0.00005316
Iteration 76/1000 | Loss: 0.00005290
Iteration 77/1000 | Loss: 0.00005274
Iteration 78/1000 | Loss: 0.00005255
Iteration 79/1000 | Loss: 0.00005241
Iteration 80/1000 | Loss: 0.00005221
Iteration 81/1000 | Loss: 0.00005200
Iteration 82/1000 | Loss: 0.00005188
Iteration 83/1000 | Loss: 0.00005184
Iteration 84/1000 | Loss: 0.00005181
Iteration 85/1000 | Loss: 0.00005168
Iteration 86/1000 | Loss: 0.00005160
Iteration 87/1000 | Loss: 0.00005154
Iteration 88/1000 | Loss: 0.00005150
Iteration 89/1000 | Loss: 0.00005150
Iteration 90/1000 | Loss: 0.00005150
Iteration 91/1000 | Loss: 0.00005150
Iteration 92/1000 | Loss: 0.00005150
Iteration 93/1000 | Loss: 0.00005150
Iteration 94/1000 | Loss: 0.00005150
Iteration 95/1000 | Loss: 0.00005150
Iteration 96/1000 | Loss: 0.00005150
Iteration 97/1000 | Loss: 0.00005149
Iteration 98/1000 | Loss: 0.00005149
Iteration 99/1000 | Loss: 0.00005149
Iteration 100/1000 | Loss: 0.00005145
Iteration 101/1000 | Loss: 0.00020444
Iteration 102/1000 | Loss: 0.00005236
Iteration 103/1000 | Loss: 0.00005166
Iteration 104/1000 | Loss: 0.00005081
Iteration 105/1000 | Loss: 0.00005036
Iteration 106/1000 | Loss: 0.00005017
Iteration 107/1000 | Loss: 0.00005002
Iteration 108/1000 | Loss: 0.00004999
Iteration 109/1000 | Loss: 0.00004996
Iteration 110/1000 | Loss: 0.00004996
Iteration 111/1000 | Loss: 0.00004992
Iteration 112/1000 | Loss: 0.00004989
Iteration 113/1000 | Loss: 0.00004989
Iteration 114/1000 | Loss: 0.00004989
Iteration 115/1000 | Loss: 0.00004989
Iteration 116/1000 | Loss: 0.00004988
Iteration 117/1000 | Loss: 0.00004988
Iteration 118/1000 | Loss: 0.00004988
Iteration 119/1000 | Loss: 0.00004988
Iteration 120/1000 | Loss: 0.00004988
Iteration 121/1000 | Loss: 0.00004988
Iteration 122/1000 | Loss: 0.00004988
Iteration 123/1000 | Loss: 0.00004987
Iteration 124/1000 | Loss: 0.00004987
Iteration 125/1000 | Loss: 0.00004987
Iteration 126/1000 | Loss: 0.00004987
Iteration 127/1000 | Loss: 0.00004987
Iteration 128/1000 | Loss: 0.00004987
Iteration 129/1000 | Loss: 0.00004987
Iteration 130/1000 | Loss: 0.00004987
Iteration 131/1000 | Loss: 0.00004987
Iteration 132/1000 | Loss: 0.00004987
Iteration 133/1000 | Loss: 0.00004986
Iteration 134/1000 | Loss: 0.00004986
Iteration 135/1000 | Loss: 0.00004986
Iteration 136/1000 | Loss: 0.00004986
Iteration 137/1000 | Loss: 0.00004986
Iteration 138/1000 | Loss: 0.00004986
Iteration 139/1000 | Loss: 0.00004986
Iteration 140/1000 | Loss: 0.00004986
Iteration 141/1000 | Loss: 0.00004986
Iteration 142/1000 | Loss: 0.00004986
Iteration 143/1000 | Loss: 0.00004986
Iteration 144/1000 | Loss: 0.00004986
Iteration 145/1000 | Loss: 0.00004986
Iteration 146/1000 | Loss: 0.00004986
Iteration 147/1000 | Loss: 0.00004986
Iteration 148/1000 | Loss: 0.00004986
Iteration 149/1000 | Loss: 0.00004986
Iteration 150/1000 | Loss: 0.00004986
Iteration 151/1000 | Loss: 0.00004985
Iteration 152/1000 | Loss: 0.00004985
Iteration 153/1000 | Loss: 0.00004985
Iteration 154/1000 | Loss: 0.00004985
Iteration 155/1000 | Loss: 0.00004985
Iteration 156/1000 | Loss: 0.00004985
Iteration 157/1000 | Loss: 0.00004985
Iteration 158/1000 | Loss: 0.00004985
Iteration 159/1000 | Loss: 0.00004985
Iteration 160/1000 | Loss: 0.00004985
Iteration 161/1000 | Loss: 0.00004984
Iteration 162/1000 | Loss: 0.00004984
Iteration 163/1000 | Loss: 0.00004984
Iteration 164/1000 | Loss: 0.00004984
Iteration 165/1000 | Loss: 0.00004984
Iteration 166/1000 | Loss: 0.00004984
Iteration 167/1000 | Loss: 0.00004984
Iteration 168/1000 | Loss: 0.00004984
Iteration 169/1000 | Loss: 0.00004984
Iteration 170/1000 | Loss: 0.00004984
Iteration 171/1000 | Loss: 0.00004984
Iteration 172/1000 | Loss: 0.00004984
Iteration 173/1000 | Loss: 0.00004984
Iteration 174/1000 | Loss: 0.00004984
Iteration 175/1000 | Loss: 0.00004984
Iteration 176/1000 | Loss: 0.00004984
Iteration 177/1000 | Loss: 0.00004983
Iteration 178/1000 | Loss: 0.00004983
Iteration 179/1000 | Loss: 0.00004983
Iteration 180/1000 | Loss: 0.00004983
Iteration 181/1000 | Loss: 0.00004983
Iteration 182/1000 | Loss: 0.00004983
Iteration 183/1000 | Loss: 0.00004983
Iteration 184/1000 | Loss: 0.00004983
Iteration 185/1000 | Loss: 0.00004983
Iteration 186/1000 | Loss: 0.00004983
Iteration 187/1000 | Loss: 0.00004983
Iteration 188/1000 | Loss: 0.00004983
Iteration 189/1000 | Loss: 0.00004983
Iteration 190/1000 | Loss: 0.00004983
Iteration 191/1000 | Loss: 0.00004983
Iteration 192/1000 | Loss: 0.00004983
Iteration 193/1000 | Loss: 0.00004983
Iteration 194/1000 | Loss: 0.00004982
Iteration 195/1000 | Loss: 0.00004982
Iteration 196/1000 | Loss: 0.00004982
Iteration 197/1000 | Loss: 0.00004982
Iteration 198/1000 | Loss: 0.00004982
Iteration 199/1000 | Loss: 0.00004982
Iteration 200/1000 | Loss: 0.00004982
Iteration 201/1000 | Loss: 0.00004982
Iteration 202/1000 | Loss: 0.00004982
Iteration 203/1000 | Loss: 0.00004982
Iteration 204/1000 | Loss: 0.00004982
Iteration 205/1000 | Loss: 0.00004982
Iteration 206/1000 | Loss: 0.00004982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [4.982236714567989e-05, 4.982236714567989e-05, 4.982236714567989e-05, 4.982236714567989e-05, 4.982236714567989e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.982236714567989e-05

Optimization complete. Final v2v error: 5.5602922439575195 mm

Highest mean error: 6.301789283752441 mm for frame 33

Lowest mean error: 4.044480323791504 mm for frame 1

Saving results

Total time: 208.14224433898926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976908
Iteration 2/25 | Loss: 0.00156003
Iteration 3/25 | Loss: 0.00134093
Iteration 4/25 | Loss: 0.00131276
Iteration 5/25 | Loss: 0.00131077
Iteration 6/25 | Loss: 0.00129686
Iteration 7/25 | Loss: 0.00129516
Iteration 8/25 | Loss: 0.00129348
Iteration 9/25 | Loss: 0.00128720
Iteration 10/25 | Loss: 0.00128555
Iteration 11/25 | Loss: 0.00128482
Iteration 12/25 | Loss: 0.00128592
Iteration 13/25 | Loss: 0.00128584
Iteration 14/25 | Loss: 0.00128081
Iteration 15/25 | Loss: 0.00127957
Iteration 16/25 | Loss: 0.00127942
Iteration 17/25 | Loss: 0.00127942
Iteration 18/25 | Loss: 0.00127941
Iteration 19/25 | Loss: 0.00127941
Iteration 20/25 | Loss: 0.00127941
Iteration 21/25 | Loss: 0.00127941
Iteration 22/25 | Loss: 0.00127940
Iteration 23/25 | Loss: 0.00127940
Iteration 24/25 | Loss: 0.00127940
Iteration 25/25 | Loss: 0.00127940

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 26.62662315
Iteration 2/25 | Loss: 0.00090297
Iteration 3/25 | Loss: 0.00090291
Iteration 4/25 | Loss: 0.00090291
Iteration 5/25 | Loss: 0.00090291
Iteration 6/25 | Loss: 0.00090291
Iteration 7/25 | Loss: 0.00090291
Iteration 8/25 | Loss: 0.00090291
Iteration 9/25 | Loss: 0.00090291
Iteration 10/25 | Loss: 0.00090291
Iteration 11/25 | Loss: 0.00090291
Iteration 12/25 | Loss: 0.00090291
Iteration 13/25 | Loss: 0.00090291
Iteration 14/25 | Loss: 0.00090291
Iteration 15/25 | Loss: 0.00090291
Iteration 16/25 | Loss: 0.00090291
Iteration 17/25 | Loss: 0.00090291
Iteration 18/25 | Loss: 0.00090291
Iteration 19/25 | Loss: 0.00090291
Iteration 20/25 | Loss: 0.00090291
Iteration 21/25 | Loss: 0.00090291
Iteration 22/25 | Loss: 0.00090290
Iteration 23/25 | Loss: 0.00090290
Iteration 24/25 | Loss: 0.00090290
Iteration 25/25 | Loss: 0.00090290

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090290
Iteration 2/1000 | Loss: 0.00006574
Iteration 3/1000 | Loss: 0.00003357
Iteration 4/1000 | Loss: 0.00002817
Iteration 5/1000 | Loss: 0.00002527
Iteration 6/1000 | Loss: 0.00002416
Iteration 7/1000 | Loss: 0.00003825
Iteration 8/1000 | Loss: 0.00005005
Iteration 9/1000 | Loss: 0.00003481
Iteration 10/1000 | Loss: 0.00005127
Iteration 11/1000 | Loss: 0.00004366
Iteration 12/1000 | Loss: 0.00004572
Iteration 13/1000 | Loss: 0.00004093
Iteration 14/1000 | Loss: 0.00004725
Iteration 15/1000 | Loss: 0.00004883
Iteration 16/1000 | Loss: 0.00004598
Iteration 17/1000 | Loss: 0.00005766
Iteration 18/1000 | Loss: 0.00004678
Iteration 19/1000 | Loss: 0.00006114
Iteration 20/1000 | Loss: 0.00005967
Iteration 21/1000 | Loss: 0.00002617
Iteration 22/1000 | Loss: 0.00004137
Iteration 23/1000 | Loss: 0.00002564
Iteration 24/1000 | Loss: 0.00002196
Iteration 25/1000 | Loss: 0.00002116
Iteration 26/1000 | Loss: 0.00002079
Iteration 27/1000 | Loss: 0.00002048
Iteration 28/1000 | Loss: 0.00002028
Iteration 29/1000 | Loss: 0.00025352
Iteration 30/1000 | Loss: 0.00002541
Iteration 31/1000 | Loss: 0.00002316
Iteration 32/1000 | Loss: 0.00002234
Iteration 33/1000 | Loss: 0.00002680
Iteration 34/1000 | Loss: 0.00002105
Iteration 35/1000 | Loss: 0.00002022
Iteration 36/1000 | Loss: 0.00001939
Iteration 37/1000 | Loss: 0.00001910
Iteration 38/1000 | Loss: 0.00001906
Iteration 39/1000 | Loss: 0.00001905
Iteration 40/1000 | Loss: 0.00001897
Iteration 41/1000 | Loss: 0.00001881
Iteration 42/1000 | Loss: 0.00001864
Iteration 43/1000 | Loss: 0.00001856
Iteration 44/1000 | Loss: 0.00001855
Iteration 45/1000 | Loss: 0.00001854
Iteration 46/1000 | Loss: 0.00001854
Iteration 47/1000 | Loss: 0.00001850
Iteration 48/1000 | Loss: 0.00001849
Iteration 49/1000 | Loss: 0.00001847
Iteration 50/1000 | Loss: 0.00001847
Iteration 51/1000 | Loss: 0.00001847
Iteration 52/1000 | Loss: 0.00001847
Iteration 53/1000 | Loss: 0.00001847
Iteration 54/1000 | Loss: 0.00001847
Iteration 55/1000 | Loss: 0.00001846
Iteration 56/1000 | Loss: 0.00001846
Iteration 57/1000 | Loss: 0.00001845
Iteration 58/1000 | Loss: 0.00001844
Iteration 59/1000 | Loss: 0.00001843
Iteration 60/1000 | Loss: 0.00001843
Iteration 61/1000 | Loss: 0.00001843
Iteration 62/1000 | Loss: 0.00001843
Iteration 63/1000 | Loss: 0.00001842
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001840
Iteration 66/1000 | Loss: 0.00001840
Iteration 67/1000 | Loss: 0.00001840
Iteration 68/1000 | Loss: 0.00001840
Iteration 69/1000 | Loss: 0.00001840
Iteration 70/1000 | Loss: 0.00001840
Iteration 71/1000 | Loss: 0.00001840
Iteration 72/1000 | Loss: 0.00001840
Iteration 73/1000 | Loss: 0.00001840
Iteration 74/1000 | Loss: 0.00001840
Iteration 75/1000 | Loss: 0.00001839
Iteration 76/1000 | Loss: 0.00001839
Iteration 77/1000 | Loss: 0.00001839
Iteration 78/1000 | Loss: 0.00001839
Iteration 79/1000 | Loss: 0.00001839
Iteration 80/1000 | Loss: 0.00001839
Iteration 81/1000 | Loss: 0.00001838
Iteration 82/1000 | Loss: 0.00001838
Iteration 83/1000 | Loss: 0.00001838
Iteration 84/1000 | Loss: 0.00001838
Iteration 85/1000 | Loss: 0.00001838
Iteration 86/1000 | Loss: 0.00001837
Iteration 87/1000 | Loss: 0.00001837
Iteration 88/1000 | Loss: 0.00001837
Iteration 89/1000 | Loss: 0.00001836
Iteration 90/1000 | Loss: 0.00001836
Iteration 91/1000 | Loss: 0.00001836
Iteration 92/1000 | Loss: 0.00001835
Iteration 93/1000 | Loss: 0.00001835
Iteration 94/1000 | Loss: 0.00001834
Iteration 95/1000 | Loss: 0.00001834
Iteration 96/1000 | Loss: 0.00001834
Iteration 97/1000 | Loss: 0.00001833
Iteration 98/1000 | Loss: 0.00001833
Iteration 99/1000 | Loss: 0.00001832
Iteration 100/1000 | Loss: 0.00001832
Iteration 101/1000 | Loss: 0.00001831
Iteration 102/1000 | Loss: 0.00001831
Iteration 103/1000 | Loss: 0.00001831
Iteration 104/1000 | Loss: 0.00001830
Iteration 105/1000 | Loss: 0.00001829
Iteration 106/1000 | Loss: 0.00001829
Iteration 107/1000 | Loss: 0.00001829
Iteration 108/1000 | Loss: 0.00001828
Iteration 109/1000 | Loss: 0.00001828
Iteration 110/1000 | Loss: 0.00001828
Iteration 111/1000 | Loss: 0.00001827
Iteration 112/1000 | Loss: 0.00001827
Iteration 113/1000 | Loss: 0.00001827
Iteration 114/1000 | Loss: 0.00001826
Iteration 115/1000 | Loss: 0.00001826
Iteration 116/1000 | Loss: 0.00001826
Iteration 117/1000 | Loss: 0.00001826
Iteration 118/1000 | Loss: 0.00001825
Iteration 119/1000 | Loss: 0.00001825
Iteration 120/1000 | Loss: 0.00001825
Iteration 121/1000 | Loss: 0.00001824
Iteration 122/1000 | Loss: 0.00001824
Iteration 123/1000 | Loss: 0.00001824
Iteration 124/1000 | Loss: 0.00001824
Iteration 125/1000 | Loss: 0.00001824
Iteration 126/1000 | Loss: 0.00001824
Iteration 127/1000 | Loss: 0.00001823
Iteration 128/1000 | Loss: 0.00001823
Iteration 129/1000 | Loss: 0.00001823
Iteration 130/1000 | Loss: 0.00001823
Iteration 131/1000 | Loss: 0.00001823
Iteration 132/1000 | Loss: 0.00001823
Iteration 133/1000 | Loss: 0.00001823
Iteration 134/1000 | Loss: 0.00001823
Iteration 135/1000 | Loss: 0.00001823
Iteration 136/1000 | Loss: 0.00001823
Iteration 137/1000 | Loss: 0.00001823
Iteration 138/1000 | Loss: 0.00001823
Iteration 139/1000 | Loss: 0.00001823
Iteration 140/1000 | Loss: 0.00001823
Iteration 141/1000 | Loss: 0.00001823
Iteration 142/1000 | Loss: 0.00001822
Iteration 143/1000 | Loss: 0.00001822
Iteration 144/1000 | Loss: 0.00001822
Iteration 145/1000 | Loss: 0.00001822
Iteration 146/1000 | Loss: 0.00001822
Iteration 147/1000 | Loss: 0.00001822
Iteration 148/1000 | Loss: 0.00001822
Iteration 149/1000 | Loss: 0.00001822
Iteration 150/1000 | Loss: 0.00001822
Iteration 151/1000 | Loss: 0.00001822
Iteration 152/1000 | Loss: 0.00001822
Iteration 153/1000 | Loss: 0.00001822
Iteration 154/1000 | Loss: 0.00001822
Iteration 155/1000 | Loss: 0.00001822
Iteration 156/1000 | Loss: 0.00001822
Iteration 157/1000 | Loss: 0.00001822
Iteration 158/1000 | Loss: 0.00001822
Iteration 159/1000 | Loss: 0.00001822
Iteration 160/1000 | Loss: 0.00001822
Iteration 161/1000 | Loss: 0.00001822
Iteration 162/1000 | Loss: 0.00001822
Iteration 163/1000 | Loss: 0.00001821
Iteration 164/1000 | Loss: 0.00001821
Iteration 165/1000 | Loss: 0.00001821
Iteration 166/1000 | Loss: 0.00001821
Iteration 167/1000 | Loss: 0.00001821
Iteration 168/1000 | Loss: 0.00001821
Iteration 169/1000 | Loss: 0.00001821
Iteration 170/1000 | Loss: 0.00001821
Iteration 171/1000 | Loss: 0.00001821
Iteration 172/1000 | Loss: 0.00001821
Iteration 173/1000 | Loss: 0.00001821
Iteration 174/1000 | Loss: 0.00001821
Iteration 175/1000 | Loss: 0.00001821
Iteration 176/1000 | Loss: 0.00001821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.821316618588753e-05, 1.821316618588753e-05, 1.821316618588753e-05, 1.821316618588753e-05, 1.821316618588753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.821316618588753e-05

Optimization complete. Final v2v error: 3.5152018070220947 mm

Highest mean error: 5.238327503204346 mm for frame 34

Lowest mean error: 2.955422878265381 mm for frame 80

Saving results

Total time: 93.30450010299683
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885385
Iteration 2/25 | Loss: 0.00171039
Iteration 3/25 | Loss: 0.00142049
Iteration 4/25 | Loss: 0.00136845
Iteration 5/25 | Loss: 0.00137061
Iteration 6/25 | Loss: 0.00134588
Iteration 7/25 | Loss: 0.00133797
Iteration 8/25 | Loss: 0.00131624
Iteration 9/25 | Loss: 0.00130721
Iteration 10/25 | Loss: 0.00130741
Iteration 11/25 | Loss: 0.00130114
Iteration 12/25 | Loss: 0.00130076
Iteration 13/25 | Loss: 0.00130849
Iteration 14/25 | Loss: 0.00130896
Iteration 15/25 | Loss: 0.00130166
Iteration 16/25 | Loss: 0.00129488
Iteration 17/25 | Loss: 0.00129112
Iteration 18/25 | Loss: 0.00129018
Iteration 19/25 | Loss: 0.00128994
Iteration 20/25 | Loss: 0.00128988
Iteration 21/25 | Loss: 0.00128988
Iteration 22/25 | Loss: 0.00128987
Iteration 23/25 | Loss: 0.00128987
Iteration 24/25 | Loss: 0.00128987
Iteration 25/25 | Loss: 0.00128987

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.29248190
Iteration 2/25 | Loss: 0.00106839
Iteration 3/25 | Loss: 0.00106839
Iteration 4/25 | Loss: 0.00106838
Iteration 5/25 | Loss: 0.00106838
Iteration 6/25 | Loss: 0.00106838
Iteration 7/25 | Loss: 0.00106838
Iteration 8/25 | Loss: 0.00106838
Iteration 9/25 | Loss: 0.00106838
Iteration 10/25 | Loss: 0.00106838
Iteration 11/25 | Loss: 0.00106838
Iteration 12/25 | Loss: 0.00106838
Iteration 13/25 | Loss: 0.00106838
Iteration 14/25 | Loss: 0.00106838
Iteration 15/25 | Loss: 0.00106838
Iteration 16/25 | Loss: 0.00106838
Iteration 17/25 | Loss: 0.00106838
Iteration 18/25 | Loss: 0.00106838
Iteration 19/25 | Loss: 0.00106838
Iteration 20/25 | Loss: 0.00106838
Iteration 21/25 | Loss: 0.00106838
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010683813598006964, 0.0010683813598006964, 0.0010683813598006964, 0.0010683813598006964, 0.0010683813598006964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010683813598006964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106838
Iteration 2/1000 | Loss: 0.00011793
Iteration 3/1000 | Loss: 0.00034302
Iteration 4/1000 | Loss: 0.00004656
Iteration 5/1000 | Loss: 0.00003466
Iteration 6/1000 | Loss: 0.00003046
Iteration 7/1000 | Loss: 0.00002910
Iteration 8/1000 | Loss: 0.00002823
Iteration 9/1000 | Loss: 0.00002724
Iteration 10/1000 | Loss: 0.00002657
Iteration 11/1000 | Loss: 0.00002606
Iteration 12/1000 | Loss: 0.00002582
Iteration 13/1000 | Loss: 0.00002550
Iteration 14/1000 | Loss: 0.00002527
Iteration 15/1000 | Loss: 0.00002510
Iteration 16/1000 | Loss: 0.00002487
Iteration 17/1000 | Loss: 0.00002469
Iteration 18/1000 | Loss: 0.00002461
Iteration 19/1000 | Loss: 0.00002460
Iteration 20/1000 | Loss: 0.00002459
Iteration 21/1000 | Loss: 0.00002457
Iteration 22/1000 | Loss: 0.00002454
Iteration 23/1000 | Loss: 0.00002454
Iteration 24/1000 | Loss: 0.00002453
Iteration 25/1000 | Loss: 0.00002452
Iteration 26/1000 | Loss: 0.00002448
Iteration 27/1000 | Loss: 0.00002442
Iteration 28/1000 | Loss: 0.00002440
Iteration 29/1000 | Loss: 0.00002439
Iteration 30/1000 | Loss: 0.00002437
Iteration 31/1000 | Loss: 0.00002437
Iteration 32/1000 | Loss: 0.00002435
Iteration 33/1000 | Loss: 0.00002434
Iteration 34/1000 | Loss: 0.00002434
Iteration 35/1000 | Loss: 0.00002433
Iteration 36/1000 | Loss: 0.00002433
Iteration 37/1000 | Loss: 0.00002432
Iteration 38/1000 | Loss: 0.00002431
Iteration 39/1000 | Loss: 0.00002431
Iteration 40/1000 | Loss: 0.00002430
Iteration 41/1000 | Loss: 0.00002430
Iteration 42/1000 | Loss: 0.00002430
Iteration 43/1000 | Loss: 0.00002429
Iteration 44/1000 | Loss: 0.00002429
Iteration 45/1000 | Loss: 0.00002429
Iteration 46/1000 | Loss: 0.00002429
Iteration 47/1000 | Loss: 0.00002428
Iteration 48/1000 | Loss: 0.00002427
Iteration 49/1000 | Loss: 0.00002427
Iteration 50/1000 | Loss: 0.00002427
Iteration 51/1000 | Loss: 0.00002427
Iteration 52/1000 | Loss: 0.00002427
Iteration 53/1000 | Loss: 0.00002427
Iteration 54/1000 | Loss: 0.00002427
Iteration 55/1000 | Loss: 0.00002427
Iteration 56/1000 | Loss: 0.00002427
Iteration 57/1000 | Loss: 0.00002427
Iteration 58/1000 | Loss: 0.00002427
Iteration 59/1000 | Loss: 0.00002427
Iteration 60/1000 | Loss: 0.00002427
Iteration 61/1000 | Loss: 0.00002426
Iteration 62/1000 | Loss: 0.00002426
Iteration 63/1000 | Loss: 0.00002425
Iteration 64/1000 | Loss: 0.00002425
Iteration 65/1000 | Loss: 0.00002425
Iteration 66/1000 | Loss: 0.00002425
Iteration 67/1000 | Loss: 0.00002425
Iteration 68/1000 | Loss: 0.00002425
Iteration 69/1000 | Loss: 0.00002424
Iteration 70/1000 | Loss: 0.00002424
Iteration 71/1000 | Loss: 0.00002424
Iteration 72/1000 | Loss: 0.00002424
Iteration 73/1000 | Loss: 0.00002424
Iteration 74/1000 | Loss: 0.00002424
Iteration 75/1000 | Loss: 0.00002424
Iteration 76/1000 | Loss: 0.00002424
Iteration 77/1000 | Loss: 0.00002423
Iteration 78/1000 | Loss: 0.00002423
Iteration 79/1000 | Loss: 0.00002423
Iteration 80/1000 | Loss: 0.00002423
Iteration 81/1000 | Loss: 0.00002422
Iteration 82/1000 | Loss: 0.00002421
Iteration 83/1000 | Loss: 0.00002421
Iteration 84/1000 | Loss: 0.00002421
Iteration 85/1000 | Loss: 0.00002421
Iteration 86/1000 | Loss: 0.00002421
Iteration 87/1000 | Loss: 0.00002421
Iteration 88/1000 | Loss: 0.00002420
Iteration 89/1000 | Loss: 0.00002420
Iteration 90/1000 | Loss: 0.00002420
Iteration 91/1000 | Loss: 0.00002420
Iteration 92/1000 | Loss: 0.00002420
Iteration 93/1000 | Loss: 0.00002420
Iteration 94/1000 | Loss: 0.00002420
Iteration 95/1000 | Loss: 0.00002420
Iteration 96/1000 | Loss: 0.00002420
Iteration 97/1000 | Loss: 0.00002420
Iteration 98/1000 | Loss: 0.00002419
Iteration 99/1000 | Loss: 0.00002419
Iteration 100/1000 | Loss: 0.00002419
Iteration 101/1000 | Loss: 0.00002418
Iteration 102/1000 | Loss: 0.00002418
Iteration 103/1000 | Loss: 0.00002418
Iteration 104/1000 | Loss: 0.00002417
Iteration 105/1000 | Loss: 0.00002417
Iteration 106/1000 | Loss: 0.00002417
Iteration 107/1000 | Loss: 0.00002417
Iteration 108/1000 | Loss: 0.00002417
Iteration 109/1000 | Loss: 0.00002416
Iteration 110/1000 | Loss: 0.00002416
Iteration 111/1000 | Loss: 0.00002416
Iteration 112/1000 | Loss: 0.00002416
Iteration 113/1000 | Loss: 0.00002416
Iteration 114/1000 | Loss: 0.00002416
Iteration 115/1000 | Loss: 0.00002416
Iteration 116/1000 | Loss: 0.00002415
Iteration 117/1000 | Loss: 0.00002415
Iteration 118/1000 | Loss: 0.00002415
Iteration 119/1000 | Loss: 0.00002415
Iteration 120/1000 | Loss: 0.00002415
Iteration 121/1000 | Loss: 0.00002415
Iteration 122/1000 | Loss: 0.00002415
Iteration 123/1000 | Loss: 0.00002415
Iteration 124/1000 | Loss: 0.00002415
Iteration 125/1000 | Loss: 0.00002415
Iteration 126/1000 | Loss: 0.00002415
Iteration 127/1000 | Loss: 0.00002414
Iteration 128/1000 | Loss: 0.00002414
Iteration 129/1000 | Loss: 0.00002414
Iteration 130/1000 | Loss: 0.00002413
Iteration 131/1000 | Loss: 0.00002413
Iteration 132/1000 | Loss: 0.00002413
Iteration 133/1000 | Loss: 0.00002413
Iteration 134/1000 | Loss: 0.00002413
Iteration 135/1000 | Loss: 0.00002413
Iteration 136/1000 | Loss: 0.00002412
Iteration 137/1000 | Loss: 0.00002412
Iteration 138/1000 | Loss: 0.00002412
Iteration 139/1000 | Loss: 0.00002412
Iteration 140/1000 | Loss: 0.00002412
Iteration 141/1000 | Loss: 0.00002412
Iteration 142/1000 | Loss: 0.00002411
Iteration 143/1000 | Loss: 0.00002411
Iteration 144/1000 | Loss: 0.00002411
Iteration 145/1000 | Loss: 0.00002411
Iteration 146/1000 | Loss: 0.00002411
Iteration 147/1000 | Loss: 0.00002411
Iteration 148/1000 | Loss: 0.00002411
Iteration 149/1000 | Loss: 0.00002411
Iteration 150/1000 | Loss: 0.00002410
Iteration 151/1000 | Loss: 0.00002410
Iteration 152/1000 | Loss: 0.00002410
Iteration 153/1000 | Loss: 0.00002410
Iteration 154/1000 | Loss: 0.00002410
Iteration 155/1000 | Loss: 0.00002410
Iteration 156/1000 | Loss: 0.00002410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.4104614567477256e-05, 2.4104614567477256e-05, 2.4104614567477256e-05, 2.4104614567477256e-05, 2.4104614567477256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4104614567477256e-05

Optimization complete. Final v2v error: 4.085601806640625 mm

Highest mean error: 5.999989032745361 mm for frame 106

Lowest mean error: 3.4976863861083984 mm for frame 69

Saving results

Total time: 74.7100989818573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813501
Iteration 2/25 | Loss: 0.00129072
Iteration 3/25 | Loss: 0.00119823
Iteration 4/25 | Loss: 0.00118843
Iteration 5/25 | Loss: 0.00118591
Iteration 6/25 | Loss: 0.00118579
Iteration 7/25 | Loss: 0.00118579
Iteration 8/25 | Loss: 0.00118579
Iteration 9/25 | Loss: 0.00118579
Iteration 10/25 | Loss: 0.00118579
Iteration 11/25 | Loss: 0.00118579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011857886565849185, 0.0011857886565849185, 0.0011857886565849185, 0.0011857886565849185, 0.0011857886565849185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011857886565849185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45595825
Iteration 2/25 | Loss: 0.00059860
Iteration 3/25 | Loss: 0.00059859
Iteration 4/25 | Loss: 0.00059859
Iteration 5/25 | Loss: 0.00059859
Iteration 6/25 | Loss: 0.00059859
Iteration 7/25 | Loss: 0.00059859
Iteration 8/25 | Loss: 0.00059859
Iteration 9/25 | Loss: 0.00059859
Iteration 10/25 | Loss: 0.00059859
Iteration 11/25 | Loss: 0.00059859
Iteration 12/25 | Loss: 0.00059859
Iteration 13/25 | Loss: 0.00059859
Iteration 14/25 | Loss: 0.00059859
Iteration 15/25 | Loss: 0.00059859
Iteration 16/25 | Loss: 0.00059859
Iteration 17/25 | Loss: 0.00059859
Iteration 18/25 | Loss: 0.00059859
Iteration 19/25 | Loss: 0.00059859
Iteration 20/25 | Loss: 0.00059859
Iteration 21/25 | Loss: 0.00059859
Iteration 22/25 | Loss: 0.00059859
Iteration 23/25 | Loss: 0.00059859
Iteration 24/25 | Loss: 0.00059859
Iteration 25/25 | Loss: 0.00059859

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059859
Iteration 2/1000 | Loss: 0.00002842
Iteration 3/1000 | Loss: 0.00001876
Iteration 4/1000 | Loss: 0.00001599
Iteration 5/1000 | Loss: 0.00001495
Iteration 6/1000 | Loss: 0.00001422
Iteration 7/1000 | Loss: 0.00001365
Iteration 8/1000 | Loss: 0.00001333
Iteration 9/1000 | Loss: 0.00001318
Iteration 10/1000 | Loss: 0.00001296
Iteration 11/1000 | Loss: 0.00001292
Iteration 12/1000 | Loss: 0.00001287
Iteration 13/1000 | Loss: 0.00001287
Iteration 14/1000 | Loss: 0.00001284
Iteration 15/1000 | Loss: 0.00001284
Iteration 16/1000 | Loss: 0.00001284
Iteration 17/1000 | Loss: 0.00001283
Iteration 18/1000 | Loss: 0.00001283
Iteration 19/1000 | Loss: 0.00001282
Iteration 20/1000 | Loss: 0.00001282
Iteration 21/1000 | Loss: 0.00001281
Iteration 22/1000 | Loss: 0.00001279
Iteration 23/1000 | Loss: 0.00001275
Iteration 24/1000 | Loss: 0.00001275
Iteration 25/1000 | Loss: 0.00001274
Iteration 26/1000 | Loss: 0.00001274
Iteration 27/1000 | Loss: 0.00001269
Iteration 28/1000 | Loss: 0.00001269
Iteration 29/1000 | Loss: 0.00001266
Iteration 30/1000 | Loss: 0.00001266
Iteration 31/1000 | Loss: 0.00001264
Iteration 32/1000 | Loss: 0.00001263
Iteration 33/1000 | Loss: 0.00001263
Iteration 34/1000 | Loss: 0.00001263
Iteration 35/1000 | Loss: 0.00001262
Iteration 36/1000 | Loss: 0.00001258
Iteration 37/1000 | Loss: 0.00001258
Iteration 38/1000 | Loss: 0.00001257
Iteration 39/1000 | Loss: 0.00001257
Iteration 40/1000 | Loss: 0.00001257
Iteration 41/1000 | Loss: 0.00001257
Iteration 42/1000 | Loss: 0.00001257
Iteration 43/1000 | Loss: 0.00001257
Iteration 44/1000 | Loss: 0.00001257
Iteration 45/1000 | Loss: 0.00001257
Iteration 46/1000 | Loss: 0.00001257
Iteration 47/1000 | Loss: 0.00001256
Iteration 48/1000 | Loss: 0.00001256
Iteration 49/1000 | Loss: 0.00001256
Iteration 50/1000 | Loss: 0.00001254
Iteration 51/1000 | Loss: 0.00001254
Iteration 52/1000 | Loss: 0.00001254
Iteration 53/1000 | Loss: 0.00001254
Iteration 54/1000 | Loss: 0.00001254
Iteration 55/1000 | Loss: 0.00001254
Iteration 56/1000 | Loss: 0.00001254
Iteration 57/1000 | Loss: 0.00001254
Iteration 58/1000 | Loss: 0.00001253
Iteration 59/1000 | Loss: 0.00001253
Iteration 60/1000 | Loss: 0.00001253
Iteration 61/1000 | Loss: 0.00001253
Iteration 62/1000 | Loss: 0.00001253
Iteration 63/1000 | Loss: 0.00001253
Iteration 64/1000 | Loss: 0.00001253
Iteration 65/1000 | Loss: 0.00001253
Iteration 66/1000 | Loss: 0.00001252
Iteration 67/1000 | Loss: 0.00001252
Iteration 68/1000 | Loss: 0.00001251
Iteration 69/1000 | Loss: 0.00001251
Iteration 70/1000 | Loss: 0.00001250
Iteration 71/1000 | Loss: 0.00001250
Iteration 72/1000 | Loss: 0.00001250
Iteration 73/1000 | Loss: 0.00001250
Iteration 74/1000 | Loss: 0.00001250
Iteration 75/1000 | Loss: 0.00001249
Iteration 76/1000 | Loss: 0.00001249
Iteration 77/1000 | Loss: 0.00001249
Iteration 78/1000 | Loss: 0.00001249
Iteration 79/1000 | Loss: 0.00001249
Iteration 80/1000 | Loss: 0.00001249
Iteration 81/1000 | Loss: 0.00001248
Iteration 82/1000 | Loss: 0.00001248
Iteration 83/1000 | Loss: 0.00001248
Iteration 84/1000 | Loss: 0.00001248
Iteration 85/1000 | Loss: 0.00001247
Iteration 86/1000 | Loss: 0.00001246
Iteration 87/1000 | Loss: 0.00001245
Iteration 88/1000 | Loss: 0.00001245
Iteration 89/1000 | Loss: 0.00001245
Iteration 90/1000 | Loss: 0.00001245
Iteration 91/1000 | Loss: 0.00001244
Iteration 92/1000 | Loss: 0.00001244
Iteration 93/1000 | Loss: 0.00001243
Iteration 94/1000 | Loss: 0.00001243
Iteration 95/1000 | Loss: 0.00001243
Iteration 96/1000 | Loss: 0.00001243
Iteration 97/1000 | Loss: 0.00001242
Iteration 98/1000 | Loss: 0.00001241
Iteration 99/1000 | Loss: 0.00001240
Iteration 100/1000 | Loss: 0.00001240
Iteration 101/1000 | Loss: 0.00001240
Iteration 102/1000 | Loss: 0.00001240
Iteration 103/1000 | Loss: 0.00001240
Iteration 104/1000 | Loss: 0.00001239
Iteration 105/1000 | Loss: 0.00001239
Iteration 106/1000 | Loss: 0.00001239
Iteration 107/1000 | Loss: 0.00001239
Iteration 108/1000 | Loss: 0.00001239
Iteration 109/1000 | Loss: 0.00001239
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001237
Iteration 112/1000 | Loss: 0.00001236
Iteration 113/1000 | Loss: 0.00001236
Iteration 114/1000 | Loss: 0.00001235
Iteration 115/1000 | Loss: 0.00001235
Iteration 116/1000 | Loss: 0.00001235
Iteration 117/1000 | Loss: 0.00001234
Iteration 118/1000 | Loss: 0.00001234
Iteration 119/1000 | Loss: 0.00001234
Iteration 120/1000 | Loss: 0.00001233
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Iteration 123/1000 | Loss: 0.00001232
Iteration 124/1000 | Loss: 0.00001231
Iteration 125/1000 | Loss: 0.00001231
Iteration 126/1000 | Loss: 0.00001231
Iteration 127/1000 | Loss: 0.00001231
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001229
Iteration 131/1000 | Loss: 0.00001229
Iteration 132/1000 | Loss: 0.00001228
Iteration 133/1000 | Loss: 0.00001228
Iteration 134/1000 | Loss: 0.00001228
Iteration 135/1000 | Loss: 0.00001228
Iteration 136/1000 | Loss: 0.00001227
Iteration 137/1000 | Loss: 0.00001227
Iteration 138/1000 | Loss: 0.00001227
Iteration 139/1000 | Loss: 0.00001226
Iteration 140/1000 | Loss: 0.00001226
Iteration 141/1000 | Loss: 0.00001226
Iteration 142/1000 | Loss: 0.00001226
Iteration 143/1000 | Loss: 0.00001226
Iteration 144/1000 | Loss: 0.00001225
Iteration 145/1000 | Loss: 0.00001225
Iteration 146/1000 | Loss: 0.00001225
Iteration 147/1000 | Loss: 0.00001224
Iteration 148/1000 | Loss: 0.00001224
Iteration 149/1000 | Loss: 0.00001224
Iteration 150/1000 | Loss: 0.00001224
Iteration 151/1000 | Loss: 0.00001224
Iteration 152/1000 | Loss: 0.00001224
Iteration 153/1000 | Loss: 0.00001223
Iteration 154/1000 | Loss: 0.00001223
Iteration 155/1000 | Loss: 0.00001223
Iteration 156/1000 | Loss: 0.00001223
Iteration 157/1000 | Loss: 0.00001223
Iteration 158/1000 | Loss: 0.00001222
Iteration 159/1000 | Loss: 0.00001222
Iteration 160/1000 | Loss: 0.00001222
Iteration 161/1000 | Loss: 0.00001222
Iteration 162/1000 | Loss: 0.00001222
Iteration 163/1000 | Loss: 0.00001221
Iteration 164/1000 | Loss: 0.00001221
Iteration 165/1000 | Loss: 0.00001221
Iteration 166/1000 | Loss: 0.00001221
Iteration 167/1000 | Loss: 0.00001220
Iteration 168/1000 | Loss: 0.00001220
Iteration 169/1000 | Loss: 0.00001220
Iteration 170/1000 | Loss: 0.00001220
Iteration 171/1000 | Loss: 0.00001219
Iteration 172/1000 | Loss: 0.00001219
Iteration 173/1000 | Loss: 0.00001219
Iteration 174/1000 | Loss: 0.00001219
Iteration 175/1000 | Loss: 0.00001219
Iteration 176/1000 | Loss: 0.00001219
Iteration 177/1000 | Loss: 0.00001219
Iteration 178/1000 | Loss: 0.00001219
Iteration 179/1000 | Loss: 0.00001218
Iteration 180/1000 | Loss: 0.00001218
Iteration 181/1000 | Loss: 0.00001218
Iteration 182/1000 | Loss: 0.00001218
Iteration 183/1000 | Loss: 0.00001218
Iteration 184/1000 | Loss: 0.00001218
Iteration 185/1000 | Loss: 0.00001218
Iteration 186/1000 | Loss: 0.00001218
Iteration 187/1000 | Loss: 0.00001218
Iteration 188/1000 | Loss: 0.00001217
Iteration 189/1000 | Loss: 0.00001217
Iteration 190/1000 | Loss: 0.00001217
Iteration 191/1000 | Loss: 0.00001217
Iteration 192/1000 | Loss: 0.00001217
Iteration 193/1000 | Loss: 0.00001217
Iteration 194/1000 | Loss: 0.00001217
Iteration 195/1000 | Loss: 0.00001217
Iteration 196/1000 | Loss: 0.00001216
Iteration 197/1000 | Loss: 0.00001216
Iteration 198/1000 | Loss: 0.00001216
Iteration 199/1000 | Loss: 0.00001216
Iteration 200/1000 | Loss: 0.00001216
Iteration 201/1000 | Loss: 0.00001216
Iteration 202/1000 | Loss: 0.00001216
Iteration 203/1000 | Loss: 0.00001216
Iteration 204/1000 | Loss: 0.00001216
Iteration 205/1000 | Loss: 0.00001216
Iteration 206/1000 | Loss: 0.00001216
Iteration 207/1000 | Loss: 0.00001216
Iteration 208/1000 | Loss: 0.00001216
Iteration 209/1000 | Loss: 0.00001216
Iteration 210/1000 | Loss: 0.00001216
Iteration 211/1000 | Loss: 0.00001216
Iteration 212/1000 | Loss: 0.00001216
Iteration 213/1000 | Loss: 0.00001216
Iteration 214/1000 | Loss: 0.00001216
Iteration 215/1000 | Loss: 0.00001216
Iteration 216/1000 | Loss: 0.00001216
Iteration 217/1000 | Loss: 0.00001216
Iteration 218/1000 | Loss: 0.00001216
Iteration 219/1000 | Loss: 0.00001216
Iteration 220/1000 | Loss: 0.00001216
Iteration 221/1000 | Loss: 0.00001216
Iteration 222/1000 | Loss: 0.00001216
Iteration 223/1000 | Loss: 0.00001216
Iteration 224/1000 | Loss: 0.00001216
Iteration 225/1000 | Loss: 0.00001216
Iteration 226/1000 | Loss: 0.00001216
Iteration 227/1000 | Loss: 0.00001216
Iteration 228/1000 | Loss: 0.00001216
Iteration 229/1000 | Loss: 0.00001216
Iteration 230/1000 | Loss: 0.00001216
Iteration 231/1000 | Loss: 0.00001216
Iteration 232/1000 | Loss: 0.00001216
Iteration 233/1000 | Loss: 0.00001216
Iteration 234/1000 | Loss: 0.00001216
Iteration 235/1000 | Loss: 0.00001216
Iteration 236/1000 | Loss: 0.00001216
Iteration 237/1000 | Loss: 0.00001216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.2156443517596927e-05, 1.2156443517596927e-05, 1.2156443517596927e-05, 1.2156443517596927e-05, 1.2156443517596927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2156443517596927e-05

Optimization complete. Final v2v error: 2.959278106689453 mm

Highest mean error: 3.144900321960449 mm for frame 52

Lowest mean error: 2.784684181213379 mm for frame 164

Saving results

Total time: 40.608651876449585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832329
Iteration 2/25 | Loss: 0.00127017
Iteration 3/25 | Loss: 0.00119021
Iteration 4/25 | Loss: 0.00117866
Iteration 5/25 | Loss: 0.00117436
Iteration 6/25 | Loss: 0.00117387
Iteration 7/25 | Loss: 0.00117387
Iteration 8/25 | Loss: 0.00117387
Iteration 9/25 | Loss: 0.00117387
Iteration 10/25 | Loss: 0.00117387
Iteration 11/25 | Loss: 0.00117387
Iteration 12/25 | Loss: 0.00117387
Iteration 13/25 | Loss: 0.00117387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001173867261968553, 0.001173867261968553, 0.001173867261968553, 0.001173867261968553, 0.001173867261968553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001173867261968553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66498995
Iteration 2/25 | Loss: 0.00065033
Iteration 3/25 | Loss: 0.00065033
Iteration 4/25 | Loss: 0.00065033
Iteration 5/25 | Loss: 0.00065033
Iteration 6/25 | Loss: 0.00065033
Iteration 7/25 | Loss: 0.00065033
Iteration 8/25 | Loss: 0.00065033
Iteration 9/25 | Loss: 0.00065033
Iteration 10/25 | Loss: 0.00065033
Iteration 11/25 | Loss: 0.00065033
Iteration 12/25 | Loss: 0.00065033
Iteration 13/25 | Loss: 0.00065033
Iteration 14/25 | Loss: 0.00065033
Iteration 15/25 | Loss: 0.00065033
Iteration 16/25 | Loss: 0.00065033
Iteration 17/25 | Loss: 0.00065033
Iteration 18/25 | Loss: 0.00065033
Iteration 19/25 | Loss: 0.00065033
Iteration 20/25 | Loss: 0.00065033
Iteration 21/25 | Loss: 0.00065033
Iteration 22/25 | Loss: 0.00065033
Iteration 23/25 | Loss: 0.00065033
Iteration 24/25 | Loss: 0.00065033
Iteration 25/25 | Loss: 0.00065033
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006503258482553065, 0.0006503258482553065, 0.0006503258482553065, 0.0006503258482553065, 0.0006503258482553065]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006503258482553065

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065033
Iteration 2/1000 | Loss: 0.00002219
Iteration 3/1000 | Loss: 0.00001510
Iteration 4/1000 | Loss: 0.00001346
Iteration 5/1000 | Loss: 0.00001281
Iteration 6/1000 | Loss: 0.00001231
Iteration 7/1000 | Loss: 0.00001198
Iteration 8/1000 | Loss: 0.00001181
Iteration 9/1000 | Loss: 0.00001177
Iteration 10/1000 | Loss: 0.00001175
Iteration 11/1000 | Loss: 0.00001174
Iteration 12/1000 | Loss: 0.00001166
Iteration 13/1000 | Loss: 0.00001164
Iteration 14/1000 | Loss: 0.00001163
Iteration 15/1000 | Loss: 0.00001155
Iteration 16/1000 | Loss: 0.00001152
Iteration 17/1000 | Loss: 0.00001145
Iteration 18/1000 | Loss: 0.00001136
Iteration 19/1000 | Loss: 0.00001135
Iteration 20/1000 | Loss: 0.00001133
Iteration 21/1000 | Loss: 0.00001129
Iteration 22/1000 | Loss: 0.00001120
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001118
Iteration 25/1000 | Loss: 0.00001116
Iteration 26/1000 | Loss: 0.00001116
Iteration 27/1000 | Loss: 0.00001115
Iteration 28/1000 | Loss: 0.00001114
Iteration 29/1000 | Loss: 0.00001114
Iteration 30/1000 | Loss: 0.00001113
Iteration 31/1000 | Loss: 0.00001113
Iteration 32/1000 | Loss: 0.00001113
Iteration 33/1000 | Loss: 0.00001113
Iteration 34/1000 | Loss: 0.00001113
Iteration 35/1000 | Loss: 0.00001113
Iteration 36/1000 | Loss: 0.00001113
Iteration 37/1000 | Loss: 0.00001113
Iteration 38/1000 | Loss: 0.00001113
Iteration 39/1000 | Loss: 0.00001112
Iteration 40/1000 | Loss: 0.00001112
Iteration 41/1000 | Loss: 0.00001112
Iteration 42/1000 | Loss: 0.00001111
Iteration 43/1000 | Loss: 0.00001111
Iteration 44/1000 | Loss: 0.00001111
Iteration 45/1000 | Loss: 0.00001110
Iteration 46/1000 | Loss: 0.00001110
Iteration 47/1000 | Loss: 0.00001110
Iteration 48/1000 | Loss: 0.00001109
Iteration 49/1000 | Loss: 0.00001109
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001108
Iteration 52/1000 | Loss: 0.00001108
Iteration 53/1000 | Loss: 0.00001108
Iteration 54/1000 | Loss: 0.00001107
Iteration 55/1000 | Loss: 0.00001107
Iteration 56/1000 | Loss: 0.00001107
Iteration 57/1000 | Loss: 0.00001107
Iteration 58/1000 | Loss: 0.00001107
Iteration 59/1000 | Loss: 0.00001107
Iteration 60/1000 | Loss: 0.00001107
Iteration 61/1000 | Loss: 0.00001106
Iteration 62/1000 | Loss: 0.00001106
Iteration 63/1000 | Loss: 0.00001106
Iteration 64/1000 | Loss: 0.00001106
Iteration 65/1000 | Loss: 0.00001106
Iteration 66/1000 | Loss: 0.00001106
Iteration 67/1000 | Loss: 0.00001106
Iteration 68/1000 | Loss: 0.00001105
Iteration 69/1000 | Loss: 0.00001104
Iteration 70/1000 | Loss: 0.00001103
Iteration 71/1000 | Loss: 0.00001103
Iteration 72/1000 | Loss: 0.00001103
Iteration 73/1000 | Loss: 0.00001103
Iteration 74/1000 | Loss: 0.00001103
Iteration 75/1000 | Loss: 0.00001103
Iteration 76/1000 | Loss: 0.00001103
Iteration 77/1000 | Loss: 0.00001103
Iteration 78/1000 | Loss: 0.00001103
Iteration 79/1000 | Loss: 0.00001102
Iteration 80/1000 | Loss: 0.00001102
Iteration 81/1000 | Loss: 0.00001102
Iteration 82/1000 | Loss: 0.00001101
Iteration 83/1000 | Loss: 0.00001101
Iteration 84/1000 | Loss: 0.00001100
Iteration 85/1000 | Loss: 0.00001100
Iteration 86/1000 | Loss: 0.00001100
Iteration 87/1000 | Loss: 0.00001100
Iteration 88/1000 | Loss: 0.00001100
Iteration 89/1000 | Loss: 0.00001100
Iteration 90/1000 | Loss: 0.00001100
Iteration 91/1000 | Loss: 0.00001100
Iteration 92/1000 | Loss: 0.00001100
Iteration 93/1000 | Loss: 0.00001099
Iteration 94/1000 | Loss: 0.00001099
Iteration 95/1000 | Loss: 0.00001099
Iteration 96/1000 | Loss: 0.00001099
Iteration 97/1000 | Loss: 0.00001099
Iteration 98/1000 | Loss: 0.00001099
Iteration 99/1000 | Loss: 0.00001099
Iteration 100/1000 | Loss: 0.00001099
Iteration 101/1000 | Loss: 0.00001099
Iteration 102/1000 | Loss: 0.00001099
Iteration 103/1000 | Loss: 0.00001098
Iteration 104/1000 | Loss: 0.00001098
Iteration 105/1000 | Loss: 0.00001097
Iteration 106/1000 | Loss: 0.00001097
Iteration 107/1000 | Loss: 0.00001097
Iteration 108/1000 | Loss: 0.00001096
Iteration 109/1000 | Loss: 0.00001096
Iteration 110/1000 | Loss: 0.00001096
Iteration 111/1000 | Loss: 0.00001096
Iteration 112/1000 | Loss: 0.00001096
Iteration 113/1000 | Loss: 0.00001095
Iteration 114/1000 | Loss: 0.00001095
Iteration 115/1000 | Loss: 0.00001095
Iteration 116/1000 | Loss: 0.00001095
Iteration 117/1000 | Loss: 0.00001095
Iteration 118/1000 | Loss: 0.00001095
Iteration 119/1000 | Loss: 0.00001095
Iteration 120/1000 | Loss: 0.00001094
Iteration 121/1000 | Loss: 0.00001094
Iteration 122/1000 | Loss: 0.00001094
Iteration 123/1000 | Loss: 0.00001093
Iteration 124/1000 | Loss: 0.00001093
Iteration 125/1000 | Loss: 0.00001093
Iteration 126/1000 | Loss: 0.00001093
Iteration 127/1000 | Loss: 0.00001092
Iteration 128/1000 | Loss: 0.00001092
Iteration 129/1000 | Loss: 0.00001092
Iteration 130/1000 | Loss: 0.00001091
Iteration 131/1000 | Loss: 0.00001091
Iteration 132/1000 | Loss: 0.00001091
Iteration 133/1000 | Loss: 0.00001090
Iteration 134/1000 | Loss: 0.00001090
Iteration 135/1000 | Loss: 0.00001090
Iteration 136/1000 | Loss: 0.00001090
Iteration 137/1000 | Loss: 0.00001090
Iteration 138/1000 | Loss: 0.00001090
Iteration 139/1000 | Loss: 0.00001089
Iteration 140/1000 | Loss: 0.00001089
Iteration 141/1000 | Loss: 0.00001089
Iteration 142/1000 | Loss: 0.00001089
Iteration 143/1000 | Loss: 0.00001089
Iteration 144/1000 | Loss: 0.00001089
Iteration 145/1000 | Loss: 0.00001089
Iteration 146/1000 | Loss: 0.00001089
Iteration 147/1000 | Loss: 0.00001089
Iteration 148/1000 | Loss: 0.00001088
Iteration 149/1000 | Loss: 0.00001088
Iteration 150/1000 | Loss: 0.00001088
Iteration 151/1000 | Loss: 0.00001088
Iteration 152/1000 | Loss: 0.00001088
Iteration 153/1000 | Loss: 0.00001088
Iteration 154/1000 | Loss: 0.00001088
Iteration 155/1000 | Loss: 0.00001088
Iteration 156/1000 | Loss: 0.00001088
Iteration 157/1000 | Loss: 0.00001088
Iteration 158/1000 | Loss: 0.00001087
Iteration 159/1000 | Loss: 0.00001087
Iteration 160/1000 | Loss: 0.00001087
Iteration 161/1000 | Loss: 0.00001087
Iteration 162/1000 | Loss: 0.00001087
Iteration 163/1000 | Loss: 0.00001087
Iteration 164/1000 | Loss: 0.00001087
Iteration 165/1000 | Loss: 0.00001087
Iteration 166/1000 | Loss: 0.00001086
Iteration 167/1000 | Loss: 0.00001086
Iteration 168/1000 | Loss: 0.00001086
Iteration 169/1000 | Loss: 0.00001086
Iteration 170/1000 | Loss: 0.00001086
Iteration 171/1000 | Loss: 0.00001086
Iteration 172/1000 | Loss: 0.00001086
Iteration 173/1000 | Loss: 0.00001086
Iteration 174/1000 | Loss: 0.00001085
Iteration 175/1000 | Loss: 0.00001085
Iteration 176/1000 | Loss: 0.00001085
Iteration 177/1000 | Loss: 0.00001085
Iteration 178/1000 | Loss: 0.00001085
Iteration 179/1000 | Loss: 0.00001085
Iteration 180/1000 | Loss: 0.00001085
Iteration 181/1000 | Loss: 0.00001085
Iteration 182/1000 | Loss: 0.00001085
Iteration 183/1000 | Loss: 0.00001085
Iteration 184/1000 | Loss: 0.00001085
Iteration 185/1000 | Loss: 0.00001085
Iteration 186/1000 | Loss: 0.00001085
Iteration 187/1000 | Loss: 0.00001085
Iteration 188/1000 | Loss: 0.00001085
Iteration 189/1000 | Loss: 0.00001085
Iteration 190/1000 | Loss: 0.00001085
Iteration 191/1000 | Loss: 0.00001085
Iteration 192/1000 | Loss: 0.00001085
Iteration 193/1000 | Loss: 0.00001085
Iteration 194/1000 | Loss: 0.00001085
Iteration 195/1000 | Loss: 0.00001085
Iteration 196/1000 | Loss: 0.00001085
Iteration 197/1000 | Loss: 0.00001085
Iteration 198/1000 | Loss: 0.00001085
Iteration 199/1000 | Loss: 0.00001085
Iteration 200/1000 | Loss: 0.00001085
Iteration 201/1000 | Loss: 0.00001085
Iteration 202/1000 | Loss: 0.00001085
Iteration 203/1000 | Loss: 0.00001085
Iteration 204/1000 | Loss: 0.00001085
Iteration 205/1000 | Loss: 0.00001085
Iteration 206/1000 | Loss: 0.00001085
Iteration 207/1000 | Loss: 0.00001085
Iteration 208/1000 | Loss: 0.00001085
Iteration 209/1000 | Loss: 0.00001085
Iteration 210/1000 | Loss: 0.00001085
Iteration 211/1000 | Loss: 0.00001085
Iteration 212/1000 | Loss: 0.00001085
Iteration 213/1000 | Loss: 0.00001085
Iteration 214/1000 | Loss: 0.00001085
Iteration 215/1000 | Loss: 0.00001085
Iteration 216/1000 | Loss: 0.00001085
Iteration 217/1000 | Loss: 0.00001085
Iteration 218/1000 | Loss: 0.00001085
Iteration 219/1000 | Loss: 0.00001085
Iteration 220/1000 | Loss: 0.00001085
Iteration 221/1000 | Loss: 0.00001085
Iteration 222/1000 | Loss: 0.00001085
Iteration 223/1000 | Loss: 0.00001085
Iteration 224/1000 | Loss: 0.00001085
Iteration 225/1000 | Loss: 0.00001085
Iteration 226/1000 | Loss: 0.00001085
Iteration 227/1000 | Loss: 0.00001085
Iteration 228/1000 | Loss: 0.00001085
Iteration 229/1000 | Loss: 0.00001085
Iteration 230/1000 | Loss: 0.00001085
Iteration 231/1000 | Loss: 0.00001085
Iteration 232/1000 | Loss: 0.00001085
Iteration 233/1000 | Loss: 0.00001085
Iteration 234/1000 | Loss: 0.00001085
Iteration 235/1000 | Loss: 0.00001085
Iteration 236/1000 | Loss: 0.00001085
Iteration 237/1000 | Loss: 0.00001085
Iteration 238/1000 | Loss: 0.00001085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.0847943485714495e-05, 1.0847943485714495e-05, 1.0847943485714495e-05, 1.0847943485714495e-05, 1.0847943485714495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0847943485714495e-05

Optimization complete. Final v2v error: 2.819974660873413 mm

Highest mean error: 3.185094118118286 mm for frame 50

Lowest mean error: 2.677255868911743 mm for frame 32

Saving results

Total time: 36.6128945350647
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030851
Iteration 2/25 | Loss: 0.00252740
Iteration 3/25 | Loss: 0.00190615
Iteration 4/25 | Loss: 0.00163069
Iteration 5/25 | Loss: 0.00152638
Iteration 6/25 | Loss: 0.00150828
Iteration 7/25 | Loss: 0.00149566
Iteration 8/25 | Loss: 0.00143667
Iteration 9/25 | Loss: 0.00140532
Iteration 10/25 | Loss: 0.00137761
Iteration 11/25 | Loss: 0.00137702
Iteration 12/25 | Loss: 0.00136543
Iteration 13/25 | Loss: 0.00136768
Iteration 14/25 | Loss: 0.00136549
Iteration 15/25 | Loss: 0.00134677
Iteration 16/25 | Loss: 0.00135420
Iteration 17/25 | Loss: 0.00134902
Iteration 18/25 | Loss: 0.00134063
Iteration 19/25 | Loss: 0.00134031
Iteration 20/25 | Loss: 0.00133957
Iteration 21/25 | Loss: 0.00133725
Iteration 22/25 | Loss: 0.00133401
Iteration 23/25 | Loss: 0.00133350
Iteration 24/25 | Loss: 0.00133558
Iteration 25/25 | Loss: 0.00133331

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40635931
Iteration 2/25 | Loss: 0.00244648
Iteration 3/25 | Loss: 0.00244648
Iteration 4/25 | Loss: 0.00244648
Iteration 5/25 | Loss: 0.00244648
Iteration 6/25 | Loss: 0.00244648
Iteration 7/25 | Loss: 0.00244648
Iteration 8/25 | Loss: 0.00244648
Iteration 9/25 | Loss: 0.00244648
Iteration 10/25 | Loss: 0.00244648
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0024464784655719995, 0.0024464784655719995, 0.0024464784655719995, 0.0024464784655719995, 0.0024464784655719995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024464784655719995

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244648
Iteration 2/1000 | Loss: 0.00047100
Iteration 3/1000 | Loss: 0.00117658
Iteration 4/1000 | Loss: 0.00044059
Iteration 5/1000 | Loss: 0.00134561
Iteration 6/1000 | Loss: 0.00025573
Iteration 7/1000 | Loss: 0.00013284
Iteration 8/1000 | Loss: 0.00102458
Iteration 9/1000 | Loss: 0.00013102
Iteration 10/1000 | Loss: 0.00009591
Iteration 11/1000 | Loss: 0.00007878
Iteration 12/1000 | Loss: 0.00027887
Iteration 13/1000 | Loss: 0.00007494
Iteration 14/1000 | Loss: 0.00011315
Iteration 15/1000 | Loss: 0.00034896
Iteration 16/1000 | Loss: 0.00012680
Iteration 17/1000 | Loss: 0.00319864
Iteration 18/1000 | Loss: 0.00652458
Iteration 19/1000 | Loss: 0.00222972
Iteration 20/1000 | Loss: 0.00010872
Iteration 21/1000 | Loss: 0.00038120
Iteration 22/1000 | Loss: 0.00105766
Iteration 23/1000 | Loss: 0.00007914
Iteration 24/1000 | Loss: 0.00012296
Iteration 25/1000 | Loss: 0.00005345
Iteration 26/1000 | Loss: 0.00165229
Iteration 27/1000 | Loss: 0.00025343
Iteration 28/1000 | Loss: 0.00108723
Iteration 29/1000 | Loss: 0.00009820
Iteration 30/1000 | Loss: 0.00005539
Iteration 31/1000 | Loss: 0.00004832
Iteration 32/1000 | Loss: 0.00005647
Iteration 33/1000 | Loss: 0.00005292
Iteration 34/1000 | Loss: 0.00059171
Iteration 35/1000 | Loss: 0.00084346
Iteration 36/1000 | Loss: 0.00064019
Iteration 37/1000 | Loss: 0.00022368
Iteration 38/1000 | Loss: 0.00380084
Iteration 39/1000 | Loss: 0.00145509
Iteration 40/1000 | Loss: 0.00046010
Iteration 41/1000 | Loss: 0.00013018
Iteration 42/1000 | Loss: 0.00004787
Iteration 43/1000 | Loss: 0.00003967
Iteration 44/1000 | Loss: 0.00054207
Iteration 45/1000 | Loss: 0.00023269
Iteration 46/1000 | Loss: 0.00004655
Iteration 47/1000 | Loss: 0.00004271
Iteration 48/1000 | Loss: 0.00004041
Iteration 49/1000 | Loss: 0.00005013
Iteration 50/1000 | Loss: 0.00005152
Iteration 51/1000 | Loss: 0.00002577
Iteration 52/1000 | Loss: 0.00003527
Iteration 53/1000 | Loss: 0.00006128
Iteration 54/1000 | Loss: 0.00036249
Iteration 55/1000 | Loss: 0.00005956
Iteration 56/1000 | Loss: 0.00002685
Iteration 57/1000 | Loss: 0.00003268
Iteration 58/1000 | Loss: 0.00008617
Iteration 59/1000 | Loss: 0.00003446
Iteration 60/1000 | Loss: 0.00004099
Iteration 61/1000 | Loss: 0.00002801
Iteration 62/1000 | Loss: 0.00002198
Iteration 63/1000 | Loss: 0.00002247
Iteration 64/1000 | Loss: 0.00009872
Iteration 65/1000 | Loss: 0.00024935
Iteration 66/1000 | Loss: 0.00007199
Iteration 67/1000 | Loss: 0.00010194
Iteration 68/1000 | Loss: 0.00002335
Iteration 69/1000 | Loss: 0.00011425
Iteration 70/1000 | Loss: 0.00005789
Iteration 71/1000 | Loss: 0.00006451
Iteration 72/1000 | Loss: 0.00003517
Iteration 73/1000 | Loss: 0.00011095
Iteration 74/1000 | Loss: 0.00003572
Iteration 75/1000 | Loss: 0.00005019
Iteration 76/1000 | Loss: 0.00002154
Iteration 77/1000 | Loss: 0.00002152
Iteration 78/1000 | Loss: 0.00002150
Iteration 79/1000 | Loss: 0.00011758
Iteration 80/1000 | Loss: 0.00008461
Iteration 81/1000 | Loss: 0.00002450
Iteration 82/1000 | Loss: 0.00002177
Iteration 83/1000 | Loss: 0.00002148
Iteration 84/1000 | Loss: 0.00005816
Iteration 85/1000 | Loss: 0.00002236
Iteration 86/1000 | Loss: 0.00002172
Iteration 87/1000 | Loss: 0.00002143
Iteration 88/1000 | Loss: 0.00002142
Iteration 89/1000 | Loss: 0.00002142
Iteration 90/1000 | Loss: 0.00002142
Iteration 91/1000 | Loss: 0.00002142
Iteration 92/1000 | Loss: 0.00002142
Iteration 93/1000 | Loss: 0.00002141
Iteration 94/1000 | Loss: 0.00002141
Iteration 95/1000 | Loss: 0.00002141
Iteration 96/1000 | Loss: 0.00002141
Iteration 97/1000 | Loss: 0.00002141
Iteration 98/1000 | Loss: 0.00002141
Iteration 99/1000 | Loss: 0.00003204
Iteration 100/1000 | Loss: 0.00014849
Iteration 101/1000 | Loss: 0.00008352
Iteration 102/1000 | Loss: 0.00002583
Iteration 103/1000 | Loss: 0.00004673
Iteration 104/1000 | Loss: 0.00031480
Iteration 105/1000 | Loss: 0.00009925
Iteration 106/1000 | Loss: 0.00002261
Iteration 107/1000 | Loss: 0.00002134
Iteration 108/1000 | Loss: 0.00002134
Iteration 109/1000 | Loss: 0.00002134
Iteration 110/1000 | Loss: 0.00002134
Iteration 111/1000 | Loss: 0.00002134
Iteration 112/1000 | Loss: 0.00002134
Iteration 113/1000 | Loss: 0.00002134
Iteration 114/1000 | Loss: 0.00002134
Iteration 115/1000 | Loss: 0.00002134
Iteration 116/1000 | Loss: 0.00002134
Iteration 117/1000 | Loss: 0.00002134
Iteration 118/1000 | Loss: 0.00002133
Iteration 119/1000 | Loss: 0.00002133
Iteration 120/1000 | Loss: 0.00002133
Iteration 121/1000 | Loss: 0.00002132
Iteration 122/1000 | Loss: 0.00002131
Iteration 123/1000 | Loss: 0.00002130
Iteration 124/1000 | Loss: 0.00002130
Iteration 125/1000 | Loss: 0.00002130
Iteration 126/1000 | Loss: 0.00002130
Iteration 127/1000 | Loss: 0.00002129
Iteration 128/1000 | Loss: 0.00002129
Iteration 129/1000 | Loss: 0.00002129
Iteration 130/1000 | Loss: 0.00002128
Iteration 131/1000 | Loss: 0.00002717
Iteration 132/1000 | Loss: 0.00002146
Iteration 133/1000 | Loss: 0.00002172
Iteration 134/1000 | Loss: 0.00002128
Iteration 135/1000 | Loss: 0.00002127
Iteration 136/1000 | Loss: 0.00002127
Iteration 137/1000 | Loss: 0.00002127
Iteration 138/1000 | Loss: 0.00002127
Iteration 139/1000 | Loss: 0.00002127
Iteration 140/1000 | Loss: 0.00002127
Iteration 141/1000 | Loss: 0.00002127
Iteration 142/1000 | Loss: 0.00002127
Iteration 143/1000 | Loss: 0.00002127
Iteration 144/1000 | Loss: 0.00002127
Iteration 145/1000 | Loss: 0.00002127
Iteration 146/1000 | Loss: 0.00002127
Iteration 147/1000 | Loss: 0.00002127
Iteration 148/1000 | Loss: 0.00002127
Iteration 149/1000 | Loss: 0.00002127
Iteration 150/1000 | Loss: 0.00002127
Iteration 151/1000 | Loss: 0.00002127
Iteration 152/1000 | Loss: 0.00002127
Iteration 153/1000 | Loss: 0.00002127
Iteration 154/1000 | Loss: 0.00002127
Iteration 155/1000 | Loss: 0.00002127
Iteration 156/1000 | Loss: 0.00002127
Iteration 157/1000 | Loss: 0.00002127
Iteration 158/1000 | Loss: 0.00002127
Iteration 159/1000 | Loss: 0.00002127
Iteration 160/1000 | Loss: 0.00002127
Iteration 161/1000 | Loss: 0.00002127
Iteration 162/1000 | Loss: 0.00002127
Iteration 163/1000 | Loss: 0.00002127
Iteration 164/1000 | Loss: 0.00002127
Iteration 165/1000 | Loss: 0.00002127
Iteration 166/1000 | Loss: 0.00002127
Iteration 167/1000 | Loss: 0.00002127
Iteration 168/1000 | Loss: 0.00002127
Iteration 169/1000 | Loss: 0.00002127
Iteration 170/1000 | Loss: 0.00002127
Iteration 171/1000 | Loss: 0.00002127
Iteration 172/1000 | Loss: 0.00002127
Iteration 173/1000 | Loss: 0.00002127
Iteration 174/1000 | Loss: 0.00002127
Iteration 175/1000 | Loss: 0.00002127
Iteration 176/1000 | Loss: 0.00002127
Iteration 177/1000 | Loss: 0.00002127
Iteration 178/1000 | Loss: 0.00002127
Iteration 179/1000 | Loss: 0.00002127
Iteration 180/1000 | Loss: 0.00002127
Iteration 181/1000 | Loss: 0.00002127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.1269959688652307e-05, 2.1269959688652307e-05, 2.1269959688652307e-05, 2.1269959688652307e-05, 2.1269959688652307e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1269959688652307e-05

Optimization complete. Final v2v error: 3.8679025173187256 mm

Highest mean error: 5.029674053192139 mm for frame 223

Lowest mean error: 3.434828758239746 mm for frame 203

Saving results

Total time: 200.59015417099
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434057
Iteration 2/25 | Loss: 0.00142121
Iteration 3/25 | Loss: 0.00126984
Iteration 4/25 | Loss: 0.00124945
Iteration 5/25 | Loss: 0.00124535
Iteration 6/25 | Loss: 0.00124426
Iteration 7/25 | Loss: 0.00124410
Iteration 8/25 | Loss: 0.00124410
Iteration 9/25 | Loss: 0.00124410
Iteration 10/25 | Loss: 0.00124410
Iteration 11/25 | Loss: 0.00124410
Iteration 12/25 | Loss: 0.00124410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012440956197679043, 0.0012440956197679043, 0.0012440956197679043, 0.0012440956197679043, 0.0012440956197679043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012440956197679043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51448536
Iteration 2/25 | Loss: 0.00079744
Iteration 3/25 | Loss: 0.00079743
Iteration 4/25 | Loss: 0.00079743
Iteration 5/25 | Loss: 0.00079743
Iteration 6/25 | Loss: 0.00079743
Iteration 7/25 | Loss: 0.00079743
Iteration 8/25 | Loss: 0.00079743
Iteration 9/25 | Loss: 0.00079743
Iteration 10/25 | Loss: 0.00079743
Iteration 11/25 | Loss: 0.00079743
Iteration 12/25 | Loss: 0.00079743
Iteration 13/25 | Loss: 0.00079743
Iteration 14/25 | Loss: 0.00079743
Iteration 15/25 | Loss: 0.00079743
Iteration 16/25 | Loss: 0.00079743
Iteration 17/25 | Loss: 0.00079743
Iteration 18/25 | Loss: 0.00079743
Iteration 19/25 | Loss: 0.00079743
Iteration 20/25 | Loss: 0.00079743
Iteration 21/25 | Loss: 0.00079743
Iteration 22/25 | Loss: 0.00079743
Iteration 23/25 | Loss: 0.00079743
Iteration 24/25 | Loss: 0.00079743
Iteration 25/25 | Loss: 0.00079743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079743
Iteration 2/1000 | Loss: 0.00004140
Iteration 3/1000 | Loss: 0.00002431
Iteration 4/1000 | Loss: 0.00001937
Iteration 5/1000 | Loss: 0.00001807
Iteration 6/1000 | Loss: 0.00001721
Iteration 7/1000 | Loss: 0.00001671
Iteration 8/1000 | Loss: 0.00001633
Iteration 9/1000 | Loss: 0.00001610
Iteration 10/1000 | Loss: 0.00001603
Iteration 11/1000 | Loss: 0.00001602
Iteration 12/1000 | Loss: 0.00001597
Iteration 13/1000 | Loss: 0.00001589
Iteration 14/1000 | Loss: 0.00001582
Iteration 15/1000 | Loss: 0.00001576
Iteration 16/1000 | Loss: 0.00001570
Iteration 17/1000 | Loss: 0.00001570
Iteration 18/1000 | Loss: 0.00001565
Iteration 19/1000 | Loss: 0.00001564
Iteration 20/1000 | Loss: 0.00001559
Iteration 21/1000 | Loss: 0.00001555
Iteration 22/1000 | Loss: 0.00001555
Iteration 23/1000 | Loss: 0.00001553
Iteration 24/1000 | Loss: 0.00001552
Iteration 25/1000 | Loss: 0.00001552
Iteration 26/1000 | Loss: 0.00001551
Iteration 27/1000 | Loss: 0.00001550
Iteration 28/1000 | Loss: 0.00001549
Iteration 29/1000 | Loss: 0.00001548
Iteration 30/1000 | Loss: 0.00001547
Iteration 31/1000 | Loss: 0.00001547
Iteration 32/1000 | Loss: 0.00001546
Iteration 33/1000 | Loss: 0.00001546
Iteration 34/1000 | Loss: 0.00001545
Iteration 35/1000 | Loss: 0.00001544
Iteration 36/1000 | Loss: 0.00001543
Iteration 37/1000 | Loss: 0.00001542
Iteration 38/1000 | Loss: 0.00001542
Iteration 39/1000 | Loss: 0.00001541
Iteration 40/1000 | Loss: 0.00001541
Iteration 41/1000 | Loss: 0.00001540
Iteration 42/1000 | Loss: 0.00001540
Iteration 43/1000 | Loss: 0.00001540
Iteration 44/1000 | Loss: 0.00001539
Iteration 45/1000 | Loss: 0.00001539
Iteration 46/1000 | Loss: 0.00001539
Iteration 47/1000 | Loss: 0.00001539
Iteration 48/1000 | Loss: 0.00001539
Iteration 49/1000 | Loss: 0.00001538
Iteration 50/1000 | Loss: 0.00001537
Iteration 51/1000 | Loss: 0.00001537
Iteration 52/1000 | Loss: 0.00001536
Iteration 53/1000 | Loss: 0.00001536
Iteration 54/1000 | Loss: 0.00001536
Iteration 55/1000 | Loss: 0.00001535
Iteration 56/1000 | Loss: 0.00001535
Iteration 57/1000 | Loss: 0.00001535
Iteration 58/1000 | Loss: 0.00001535
Iteration 59/1000 | Loss: 0.00001534
Iteration 60/1000 | Loss: 0.00001534
Iteration 61/1000 | Loss: 0.00001534
Iteration 62/1000 | Loss: 0.00001534
Iteration 63/1000 | Loss: 0.00001534
Iteration 64/1000 | Loss: 0.00001534
Iteration 65/1000 | Loss: 0.00001533
Iteration 66/1000 | Loss: 0.00001533
Iteration 67/1000 | Loss: 0.00001533
Iteration 68/1000 | Loss: 0.00001533
Iteration 69/1000 | Loss: 0.00001533
Iteration 70/1000 | Loss: 0.00001532
Iteration 71/1000 | Loss: 0.00001532
Iteration 72/1000 | Loss: 0.00001532
Iteration 73/1000 | Loss: 0.00001531
Iteration 74/1000 | Loss: 0.00001531
Iteration 75/1000 | Loss: 0.00001531
Iteration 76/1000 | Loss: 0.00001530
Iteration 77/1000 | Loss: 0.00001530
Iteration 78/1000 | Loss: 0.00001530
Iteration 79/1000 | Loss: 0.00001530
Iteration 80/1000 | Loss: 0.00001530
Iteration 81/1000 | Loss: 0.00001529
Iteration 82/1000 | Loss: 0.00001529
Iteration 83/1000 | Loss: 0.00001529
Iteration 84/1000 | Loss: 0.00001529
Iteration 85/1000 | Loss: 0.00001528
Iteration 86/1000 | Loss: 0.00001528
Iteration 87/1000 | Loss: 0.00001528
Iteration 88/1000 | Loss: 0.00001528
Iteration 89/1000 | Loss: 0.00001528
Iteration 90/1000 | Loss: 0.00001528
Iteration 91/1000 | Loss: 0.00001528
Iteration 92/1000 | Loss: 0.00001527
Iteration 93/1000 | Loss: 0.00001527
Iteration 94/1000 | Loss: 0.00001527
Iteration 95/1000 | Loss: 0.00001527
Iteration 96/1000 | Loss: 0.00001527
Iteration 97/1000 | Loss: 0.00001527
Iteration 98/1000 | Loss: 0.00001527
Iteration 99/1000 | Loss: 0.00001527
Iteration 100/1000 | Loss: 0.00001527
Iteration 101/1000 | Loss: 0.00001527
Iteration 102/1000 | Loss: 0.00001526
Iteration 103/1000 | Loss: 0.00001526
Iteration 104/1000 | Loss: 0.00001526
Iteration 105/1000 | Loss: 0.00001526
Iteration 106/1000 | Loss: 0.00001526
Iteration 107/1000 | Loss: 0.00001525
Iteration 108/1000 | Loss: 0.00001525
Iteration 109/1000 | Loss: 0.00001525
Iteration 110/1000 | Loss: 0.00001525
Iteration 111/1000 | Loss: 0.00001525
Iteration 112/1000 | Loss: 0.00001525
Iteration 113/1000 | Loss: 0.00001525
Iteration 114/1000 | Loss: 0.00001525
Iteration 115/1000 | Loss: 0.00001525
Iteration 116/1000 | Loss: 0.00001524
Iteration 117/1000 | Loss: 0.00001524
Iteration 118/1000 | Loss: 0.00001524
Iteration 119/1000 | Loss: 0.00001524
Iteration 120/1000 | Loss: 0.00001524
Iteration 121/1000 | Loss: 0.00001523
Iteration 122/1000 | Loss: 0.00001523
Iteration 123/1000 | Loss: 0.00001523
Iteration 124/1000 | Loss: 0.00001523
Iteration 125/1000 | Loss: 0.00001523
Iteration 126/1000 | Loss: 0.00001522
Iteration 127/1000 | Loss: 0.00001522
Iteration 128/1000 | Loss: 0.00001522
Iteration 129/1000 | Loss: 0.00001522
Iteration 130/1000 | Loss: 0.00001521
Iteration 131/1000 | Loss: 0.00001521
Iteration 132/1000 | Loss: 0.00001521
Iteration 133/1000 | Loss: 0.00001521
Iteration 134/1000 | Loss: 0.00001521
Iteration 135/1000 | Loss: 0.00001521
Iteration 136/1000 | Loss: 0.00001520
Iteration 137/1000 | Loss: 0.00001520
Iteration 138/1000 | Loss: 0.00001520
Iteration 139/1000 | Loss: 0.00001520
Iteration 140/1000 | Loss: 0.00001520
Iteration 141/1000 | Loss: 0.00001520
Iteration 142/1000 | Loss: 0.00001520
Iteration 143/1000 | Loss: 0.00001520
Iteration 144/1000 | Loss: 0.00001520
Iteration 145/1000 | Loss: 0.00001520
Iteration 146/1000 | Loss: 0.00001520
Iteration 147/1000 | Loss: 0.00001519
Iteration 148/1000 | Loss: 0.00001519
Iteration 149/1000 | Loss: 0.00001519
Iteration 150/1000 | Loss: 0.00001519
Iteration 151/1000 | Loss: 0.00001519
Iteration 152/1000 | Loss: 0.00001518
Iteration 153/1000 | Loss: 0.00001518
Iteration 154/1000 | Loss: 0.00001518
Iteration 155/1000 | Loss: 0.00001518
Iteration 156/1000 | Loss: 0.00001518
Iteration 157/1000 | Loss: 0.00001517
Iteration 158/1000 | Loss: 0.00001517
Iteration 159/1000 | Loss: 0.00001517
Iteration 160/1000 | Loss: 0.00001517
Iteration 161/1000 | Loss: 0.00001516
Iteration 162/1000 | Loss: 0.00001516
Iteration 163/1000 | Loss: 0.00001516
Iteration 164/1000 | Loss: 0.00001516
Iteration 165/1000 | Loss: 0.00001516
Iteration 166/1000 | Loss: 0.00001516
Iteration 167/1000 | Loss: 0.00001516
Iteration 168/1000 | Loss: 0.00001516
Iteration 169/1000 | Loss: 0.00001516
Iteration 170/1000 | Loss: 0.00001515
Iteration 171/1000 | Loss: 0.00001515
Iteration 172/1000 | Loss: 0.00001515
Iteration 173/1000 | Loss: 0.00001515
Iteration 174/1000 | Loss: 0.00001515
Iteration 175/1000 | Loss: 0.00001515
Iteration 176/1000 | Loss: 0.00001515
Iteration 177/1000 | Loss: 0.00001515
Iteration 178/1000 | Loss: 0.00001514
Iteration 179/1000 | Loss: 0.00001514
Iteration 180/1000 | Loss: 0.00001514
Iteration 181/1000 | Loss: 0.00001514
Iteration 182/1000 | Loss: 0.00001514
Iteration 183/1000 | Loss: 0.00001514
Iteration 184/1000 | Loss: 0.00001514
Iteration 185/1000 | Loss: 0.00001514
Iteration 186/1000 | Loss: 0.00001514
Iteration 187/1000 | Loss: 0.00001514
Iteration 188/1000 | Loss: 0.00001514
Iteration 189/1000 | Loss: 0.00001514
Iteration 190/1000 | Loss: 0.00001514
Iteration 191/1000 | Loss: 0.00001514
Iteration 192/1000 | Loss: 0.00001514
Iteration 193/1000 | Loss: 0.00001514
Iteration 194/1000 | Loss: 0.00001514
Iteration 195/1000 | Loss: 0.00001514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.5142585652938578e-05, 1.5142585652938578e-05, 1.5142585652938578e-05, 1.5142585652938578e-05, 1.5142585652938578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5142585652938578e-05

Optimization complete. Final v2v error: 3.2714426517486572 mm

Highest mean error: 4.714479446411133 mm for frame 55

Lowest mean error: 2.972794771194458 mm for frame 36

Saving results

Total time: 40.52802324295044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00535872
Iteration 2/25 | Loss: 0.00157981
Iteration 3/25 | Loss: 0.00129374
Iteration 4/25 | Loss: 0.00126483
Iteration 5/25 | Loss: 0.00125468
Iteration 6/25 | Loss: 0.00125313
Iteration 7/25 | Loss: 0.00124820
Iteration 8/25 | Loss: 0.00125157
Iteration 9/25 | Loss: 0.00125338
Iteration 10/25 | Loss: 0.00124794
Iteration 11/25 | Loss: 0.00124462
Iteration 12/25 | Loss: 0.00124416
Iteration 13/25 | Loss: 0.00124405
Iteration 14/25 | Loss: 0.00124405
Iteration 15/25 | Loss: 0.00124405
Iteration 16/25 | Loss: 0.00124403
Iteration 17/25 | Loss: 0.00124403
Iteration 18/25 | Loss: 0.00124403
Iteration 19/25 | Loss: 0.00124403
Iteration 20/25 | Loss: 0.00124403
Iteration 21/25 | Loss: 0.00124403
Iteration 22/25 | Loss: 0.00124403
Iteration 23/25 | Loss: 0.00124402
Iteration 24/25 | Loss: 0.00124402
Iteration 25/25 | Loss: 0.00124402

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73935711
Iteration 2/25 | Loss: 0.00068044
Iteration 3/25 | Loss: 0.00068044
Iteration 4/25 | Loss: 0.00068044
Iteration 5/25 | Loss: 0.00068044
Iteration 6/25 | Loss: 0.00068044
Iteration 7/25 | Loss: 0.00068044
Iteration 8/25 | Loss: 0.00068044
Iteration 9/25 | Loss: 0.00068044
Iteration 10/25 | Loss: 0.00068044
Iteration 11/25 | Loss: 0.00068044
Iteration 12/25 | Loss: 0.00068044
Iteration 13/25 | Loss: 0.00068044
Iteration 14/25 | Loss: 0.00068044
Iteration 15/25 | Loss: 0.00068044
Iteration 16/25 | Loss: 0.00068044
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006804350996389985, 0.0006804350996389985, 0.0006804350996389985, 0.0006804350996389985, 0.0006804350996389985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006804350996389985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068044
Iteration 2/1000 | Loss: 0.00003237
Iteration 3/1000 | Loss: 0.00002514
Iteration 4/1000 | Loss: 0.00002174
Iteration 5/1000 | Loss: 0.00002069
Iteration 6/1000 | Loss: 0.00001994
Iteration 7/1000 | Loss: 0.00001953
Iteration 8/1000 | Loss: 0.00001919
Iteration 9/1000 | Loss: 0.00001881
Iteration 10/1000 | Loss: 0.00001847
Iteration 11/1000 | Loss: 0.00001838
Iteration 12/1000 | Loss: 0.00001820
Iteration 13/1000 | Loss: 0.00001817
Iteration 14/1000 | Loss: 0.00001813
Iteration 15/1000 | Loss: 0.00001811
Iteration 16/1000 | Loss: 0.00001811
Iteration 17/1000 | Loss: 0.00001811
Iteration 18/1000 | Loss: 0.00001809
Iteration 19/1000 | Loss: 0.00001809
Iteration 20/1000 | Loss: 0.00001807
Iteration 21/1000 | Loss: 0.00001806
Iteration 22/1000 | Loss: 0.00001806
Iteration 23/1000 | Loss: 0.00001806
Iteration 24/1000 | Loss: 0.00001805
Iteration 25/1000 | Loss: 0.00001804
Iteration 26/1000 | Loss: 0.00001802
Iteration 27/1000 | Loss: 0.00001802
Iteration 28/1000 | Loss: 0.00001802
Iteration 29/1000 | Loss: 0.00001801
Iteration 30/1000 | Loss: 0.00001801
Iteration 31/1000 | Loss: 0.00001800
Iteration 32/1000 | Loss: 0.00001799
Iteration 33/1000 | Loss: 0.00001799
Iteration 34/1000 | Loss: 0.00001798
Iteration 35/1000 | Loss: 0.00001798
Iteration 36/1000 | Loss: 0.00001798
Iteration 37/1000 | Loss: 0.00001798
Iteration 38/1000 | Loss: 0.00001798
Iteration 39/1000 | Loss: 0.00001797
Iteration 40/1000 | Loss: 0.00001797
Iteration 41/1000 | Loss: 0.00001796
Iteration 42/1000 | Loss: 0.00001796
Iteration 43/1000 | Loss: 0.00001795
Iteration 44/1000 | Loss: 0.00001795
Iteration 45/1000 | Loss: 0.00001795
Iteration 46/1000 | Loss: 0.00001794
Iteration 47/1000 | Loss: 0.00001793
Iteration 48/1000 | Loss: 0.00001793
Iteration 49/1000 | Loss: 0.00001792
Iteration 50/1000 | Loss: 0.00001792
Iteration 51/1000 | Loss: 0.00001791
Iteration 52/1000 | Loss: 0.00001791
Iteration 53/1000 | Loss: 0.00001791
Iteration 54/1000 | Loss: 0.00001791
Iteration 55/1000 | Loss: 0.00001790
Iteration 56/1000 | Loss: 0.00001789
Iteration 57/1000 | Loss: 0.00001789
Iteration 58/1000 | Loss: 0.00001788
Iteration 59/1000 | Loss: 0.00001788
Iteration 60/1000 | Loss: 0.00001787
Iteration 61/1000 | Loss: 0.00001787
Iteration 62/1000 | Loss: 0.00001786
Iteration 63/1000 | Loss: 0.00001786
Iteration 64/1000 | Loss: 0.00001786
Iteration 65/1000 | Loss: 0.00001786
Iteration 66/1000 | Loss: 0.00001786
Iteration 67/1000 | Loss: 0.00001786
Iteration 68/1000 | Loss: 0.00001786
Iteration 69/1000 | Loss: 0.00001786
Iteration 70/1000 | Loss: 0.00001785
Iteration 71/1000 | Loss: 0.00001785
Iteration 72/1000 | Loss: 0.00001785
Iteration 73/1000 | Loss: 0.00001784
Iteration 74/1000 | Loss: 0.00001784
Iteration 75/1000 | Loss: 0.00001784
Iteration 76/1000 | Loss: 0.00001784
Iteration 77/1000 | Loss: 0.00001784
Iteration 78/1000 | Loss: 0.00001784
Iteration 79/1000 | Loss: 0.00001783
Iteration 80/1000 | Loss: 0.00001783
Iteration 81/1000 | Loss: 0.00001783
Iteration 82/1000 | Loss: 0.00001783
Iteration 83/1000 | Loss: 0.00001783
Iteration 84/1000 | Loss: 0.00001783
Iteration 85/1000 | Loss: 0.00001783
Iteration 86/1000 | Loss: 0.00001783
Iteration 87/1000 | Loss: 0.00001783
Iteration 88/1000 | Loss: 0.00001783
Iteration 89/1000 | Loss: 0.00001782
Iteration 90/1000 | Loss: 0.00001782
Iteration 91/1000 | Loss: 0.00001782
Iteration 92/1000 | Loss: 0.00001782
Iteration 93/1000 | Loss: 0.00001782
Iteration 94/1000 | Loss: 0.00001782
Iteration 95/1000 | Loss: 0.00001782
Iteration 96/1000 | Loss: 0.00001782
Iteration 97/1000 | Loss: 0.00001782
Iteration 98/1000 | Loss: 0.00001782
Iteration 99/1000 | Loss: 0.00001782
Iteration 100/1000 | Loss: 0.00001781
Iteration 101/1000 | Loss: 0.00001781
Iteration 102/1000 | Loss: 0.00001781
Iteration 103/1000 | Loss: 0.00001780
Iteration 104/1000 | Loss: 0.00001780
Iteration 105/1000 | Loss: 0.00001780
Iteration 106/1000 | Loss: 0.00001780
Iteration 107/1000 | Loss: 0.00001780
Iteration 108/1000 | Loss: 0.00001780
Iteration 109/1000 | Loss: 0.00001779
Iteration 110/1000 | Loss: 0.00001779
Iteration 111/1000 | Loss: 0.00001778
Iteration 112/1000 | Loss: 0.00001778
Iteration 113/1000 | Loss: 0.00001778
Iteration 114/1000 | Loss: 0.00001778
Iteration 115/1000 | Loss: 0.00001778
Iteration 116/1000 | Loss: 0.00001778
Iteration 117/1000 | Loss: 0.00001778
Iteration 118/1000 | Loss: 0.00001778
Iteration 119/1000 | Loss: 0.00001778
Iteration 120/1000 | Loss: 0.00001778
Iteration 121/1000 | Loss: 0.00001777
Iteration 122/1000 | Loss: 0.00001777
Iteration 123/1000 | Loss: 0.00001777
Iteration 124/1000 | Loss: 0.00001777
Iteration 125/1000 | Loss: 0.00001777
Iteration 126/1000 | Loss: 0.00001777
Iteration 127/1000 | Loss: 0.00001777
Iteration 128/1000 | Loss: 0.00001777
Iteration 129/1000 | Loss: 0.00001777
Iteration 130/1000 | Loss: 0.00001776
Iteration 131/1000 | Loss: 0.00001776
Iteration 132/1000 | Loss: 0.00001776
Iteration 133/1000 | Loss: 0.00001776
Iteration 134/1000 | Loss: 0.00001776
Iteration 135/1000 | Loss: 0.00001776
Iteration 136/1000 | Loss: 0.00001776
Iteration 137/1000 | Loss: 0.00001776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.7761753042577766e-05, 1.7761753042577766e-05, 1.7761753042577766e-05, 1.7761753042577766e-05, 1.7761753042577766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7761753042577766e-05

Optimization complete. Final v2v error: 3.5097603797912598 mm

Highest mean error: 6.025107383728027 mm for frame 131

Lowest mean error: 3.0597691535949707 mm for frame 26

Saving results

Total time: 56.04150748252869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761915
Iteration 2/25 | Loss: 0.00169433
Iteration 3/25 | Loss: 0.00145987
Iteration 4/25 | Loss: 0.00141329
Iteration 5/25 | Loss: 0.00140094
Iteration 6/25 | Loss: 0.00139778
Iteration 7/25 | Loss: 0.00139759
Iteration 8/25 | Loss: 0.00139759
Iteration 9/25 | Loss: 0.00139759
Iteration 10/25 | Loss: 0.00139759
Iteration 11/25 | Loss: 0.00139759
Iteration 12/25 | Loss: 0.00139759
Iteration 13/25 | Loss: 0.00139759
Iteration 14/25 | Loss: 0.00139759
Iteration 15/25 | Loss: 0.00139759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013975887559354305, 0.0013975887559354305, 0.0013975887559354305, 0.0013975887559354305, 0.0013975887559354305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013975887559354305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23508608
Iteration 2/25 | Loss: 0.00084901
Iteration 3/25 | Loss: 0.00084899
Iteration 4/25 | Loss: 0.00084899
Iteration 5/25 | Loss: 0.00084899
Iteration 6/25 | Loss: 0.00084899
Iteration 7/25 | Loss: 0.00084899
Iteration 8/25 | Loss: 0.00084899
Iteration 9/25 | Loss: 0.00084899
Iteration 10/25 | Loss: 0.00084899
Iteration 11/25 | Loss: 0.00084899
Iteration 12/25 | Loss: 0.00084899
Iteration 13/25 | Loss: 0.00084899
Iteration 14/25 | Loss: 0.00084899
Iteration 15/25 | Loss: 0.00084899
Iteration 16/25 | Loss: 0.00084899
Iteration 17/25 | Loss: 0.00084899
Iteration 18/25 | Loss: 0.00084899
Iteration 19/25 | Loss: 0.00084899
Iteration 20/25 | Loss: 0.00084899
Iteration 21/25 | Loss: 0.00084899
Iteration 22/25 | Loss: 0.00084899
Iteration 23/25 | Loss: 0.00084899
Iteration 24/25 | Loss: 0.00084899
Iteration 25/25 | Loss: 0.00084899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084899
Iteration 2/1000 | Loss: 0.00007752
Iteration 3/1000 | Loss: 0.00005702
Iteration 4/1000 | Loss: 0.00005002
Iteration 5/1000 | Loss: 0.00004719
Iteration 6/1000 | Loss: 0.00004518
Iteration 7/1000 | Loss: 0.00004409
Iteration 8/1000 | Loss: 0.00004296
Iteration 9/1000 | Loss: 0.00004216
Iteration 10/1000 | Loss: 0.00004161
Iteration 11/1000 | Loss: 0.00004105
Iteration 12/1000 | Loss: 0.00004066
Iteration 13/1000 | Loss: 0.00004032
Iteration 14/1000 | Loss: 0.00004004
Iteration 15/1000 | Loss: 0.00003980
Iteration 16/1000 | Loss: 0.00003963
Iteration 17/1000 | Loss: 0.00003948
Iteration 18/1000 | Loss: 0.00003935
Iteration 19/1000 | Loss: 0.00003933
Iteration 20/1000 | Loss: 0.00003930
Iteration 21/1000 | Loss: 0.00003926
Iteration 22/1000 | Loss: 0.00003926
Iteration 23/1000 | Loss: 0.00003926
Iteration 24/1000 | Loss: 0.00003923
Iteration 25/1000 | Loss: 0.00003919
Iteration 26/1000 | Loss: 0.00003919
Iteration 27/1000 | Loss: 0.00003918
Iteration 28/1000 | Loss: 0.00003917
Iteration 29/1000 | Loss: 0.00003916
Iteration 30/1000 | Loss: 0.00003915
Iteration 31/1000 | Loss: 0.00003915
Iteration 32/1000 | Loss: 0.00003914
Iteration 33/1000 | Loss: 0.00003913
Iteration 34/1000 | Loss: 0.00003912
Iteration 35/1000 | Loss: 0.00003911
Iteration 36/1000 | Loss: 0.00003911
Iteration 37/1000 | Loss: 0.00003908
Iteration 38/1000 | Loss: 0.00003908
Iteration 39/1000 | Loss: 0.00003908
Iteration 40/1000 | Loss: 0.00003907
Iteration 41/1000 | Loss: 0.00003906
Iteration 42/1000 | Loss: 0.00003906
Iteration 43/1000 | Loss: 0.00003906
Iteration 44/1000 | Loss: 0.00003906
Iteration 45/1000 | Loss: 0.00003905
Iteration 46/1000 | Loss: 0.00003905
Iteration 47/1000 | Loss: 0.00003905
Iteration 48/1000 | Loss: 0.00003905
Iteration 49/1000 | Loss: 0.00003905
Iteration 50/1000 | Loss: 0.00003905
Iteration 51/1000 | Loss: 0.00003905
Iteration 52/1000 | Loss: 0.00003905
Iteration 53/1000 | Loss: 0.00003905
Iteration 54/1000 | Loss: 0.00003905
Iteration 55/1000 | Loss: 0.00003905
Iteration 56/1000 | Loss: 0.00003905
Iteration 57/1000 | Loss: 0.00003905
Iteration 58/1000 | Loss: 0.00003904
Iteration 59/1000 | Loss: 0.00003904
Iteration 60/1000 | Loss: 0.00003904
Iteration 61/1000 | Loss: 0.00003904
Iteration 62/1000 | Loss: 0.00003904
Iteration 63/1000 | Loss: 0.00003903
Iteration 64/1000 | Loss: 0.00003903
Iteration 65/1000 | Loss: 0.00003903
Iteration 66/1000 | Loss: 0.00003903
Iteration 67/1000 | Loss: 0.00003903
Iteration 68/1000 | Loss: 0.00003902
Iteration 69/1000 | Loss: 0.00003902
Iteration 70/1000 | Loss: 0.00003902
Iteration 71/1000 | Loss: 0.00003902
Iteration 72/1000 | Loss: 0.00003902
Iteration 73/1000 | Loss: 0.00003902
Iteration 74/1000 | Loss: 0.00003902
Iteration 75/1000 | Loss: 0.00003902
Iteration 76/1000 | Loss: 0.00003901
Iteration 77/1000 | Loss: 0.00003901
Iteration 78/1000 | Loss: 0.00003901
Iteration 79/1000 | Loss: 0.00003901
Iteration 80/1000 | Loss: 0.00003900
Iteration 81/1000 | Loss: 0.00003900
Iteration 82/1000 | Loss: 0.00003900
Iteration 83/1000 | Loss: 0.00003900
Iteration 84/1000 | Loss: 0.00003900
Iteration 85/1000 | Loss: 0.00003900
Iteration 86/1000 | Loss: 0.00003899
Iteration 87/1000 | Loss: 0.00003899
Iteration 88/1000 | Loss: 0.00003899
Iteration 89/1000 | Loss: 0.00003898
Iteration 90/1000 | Loss: 0.00003898
Iteration 91/1000 | Loss: 0.00003898
Iteration 92/1000 | Loss: 0.00003898
Iteration 93/1000 | Loss: 0.00003897
Iteration 94/1000 | Loss: 0.00003897
Iteration 95/1000 | Loss: 0.00003897
Iteration 96/1000 | Loss: 0.00003897
Iteration 97/1000 | Loss: 0.00003897
Iteration 98/1000 | Loss: 0.00003897
Iteration 99/1000 | Loss: 0.00003896
Iteration 100/1000 | Loss: 0.00003896
Iteration 101/1000 | Loss: 0.00003896
Iteration 102/1000 | Loss: 0.00003896
Iteration 103/1000 | Loss: 0.00003896
Iteration 104/1000 | Loss: 0.00003896
Iteration 105/1000 | Loss: 0.00003896
Iteration 106/1000 | Loss: 0.00003895
Iteration 107/1000 | Loss: 0.00003895
Iteration 108/1000 | Loss: 0.00003895
Iteration 109/1000 | Loss: 0.00003895
Iteration 110/1000 | Loss: 0.00003895
Iteration 111/1000 | Loss: 0.00003895
Iteration 112/1000 | Loss: 0.00003895
Iteration 113/1000 | Loss: 0.00003894
Iteration 114/1000 | Loss: 0.00003894
Iteration 115/1000 | Loss: 0.00003894
Iteration 116/1000 | Loss: 0.00003894
Iteration 117/1000 | Loss: 0.00003894
Iteration 118/1000 | Loss: 0.00003894
Iteration 119/1000 | Loss: 0.00003893
Iteration 120/1000 | Loss: 0.00003893
Iteration 121/1000 | Loss: 0.00003893
Iteration 122/1000 | Loss: 0.00003893
Iteration 123/1000 | Loss: 0.00003892
Iteration 124/1000 | Loss: 0.00003892
Iteration 125/1000 | Loss: 0.00003892
Iteration 126/1000 | Loss: 0.00003892
Iteration 127/1000 | Loss: 0.00003892
Iteration 128/1000 | Loss: 0.00003892
Iteration 129/1000 | Loss: 0.00003892
Iteration 130/1000 | Loss: 0.00003892
Iteration 131/1000 | Loss: 0.00003891
Iteration 132/1000 | Loss: 0.00003891
Iteration 133/1000 | Loss: 0.00003891
Iteration 134/1000 | Loss: 0.00003891
Iteration 135/1000 | Loss: 0.00003891
Iteration 136/1000 | Loss: 0.00003891
Iteration 137/1000 | Loss: 0.00003891
Iteration 138/1000 | Loss: 0.00003891
Iteration 139/1000 | Loss: 0.00003891
Iteration 140/1000 | Loss: 0.00003891
Iteration 141/1000 | Loss: 0.00003891
Iteration 142/1000 | Loss: 0.00003891
Iteration 143/1000 | Loss: 0.00003891
Iteration 144/1000 | Loss: 0.00003891
Iteration 145/1000 | Loss: 0.00003891
Iteration 146/1000 | Loss: 0.00003891
Iteration 147/1000 | Loss: 0.00003891
Iteration 148/1000 | Loss: 0.00003891
Iteration 149/1000 | Loss: 0.00003891
Iteration 150/1000 | Loss: 0.00003891
Iteration 151/1000 | Loss: 0.00003891
Iteration 152/1000 | Loss: 0.00003891
Iteration 153/1000 | Loss: 0.00003891
Iteration 154/1000 | Loss: 0.00003891
Iteration 155/1000 | Loss: 0.00003891
Iteration 156/1000 | Loss: 0.00003891
Iteration 157/1000 | Loss: 0.00003891
Iteration 158/1000 | Loss: 0.00003891
Iteration 159/1000 | Loss: 0.00003891
Iteration 160/1000 | Loss: 0.00003891
Iteration 161/1000 | Loss: 0.00003891
Iteration 162/1000 | Loss: 0.00003891
Iteration 163/1000 | Loss: 0.00003891
Iteration 164/1000 | Loss: 0.00003891
Iteration 165/1000 | Loss: 0.00003891
Iteration 166/1000 | Loss: 0.00003891
Iteration 167/1000 | Loss: 0.00003891
Iteration 168/1000 | Loss: 0.00003891
Iteration 169/1000 | Loss: 0.00003891
Iteration 170/1000 | Loss: 0.00003891
Iteration 171/1000 | Loss: 0.00003891
Iteration 172/1000 | Loss: 0.00003891
Iteration 173/1000 | Loss: 0.00003891
Iteration 174/1000 | Loss: 0.00003891
Iteration 175/1000 | Loss: 0.00003891
Iteration 176/1000 | Loss: 0.00003891
Iteration 177/1000 | Loss: 0.00003891
Iteration 178/1000 | Loss: 0.00003891
Iteration 179/1000 | Loss: 0.00003891
Iteration 180/1000 | Loss: 0.00003891
Iteration 181/1000 | Loss: 0.00003891
Iteration 182/1000 | Loss: 0.00003891
Iteration 183/1000 | Loss: 0.00003891
Iteration 184/1000 | Loss: 0.00003891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [3.8910053262952715e-05, 3.8910053262952715e-05, 3.8910053262952715e-05, 3.8910053262952715e-05, 3.8910053262952715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8910053262952715e-05

Optimization complete. Final v2v error: 5.104351043701172 mm

Highest mean error: 6.428450584411621 mm for frame 75

Lowest mean error: 3.9097819328308105 mm for frame 214

Saving results

Total time: 53.13041615486145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852486
Iteration 2/25 | Loss: 0.00173923
Iteration 3/25 | Loss: 0.00157184
Iteration 4/25 | Loss: 0.00151470
Iteration 5/25 | Loss: 0.00150774
Iteration 6/25 | Loss: 0.00152270
Iteration 7/25 | Loss: 0.00151338
Iteration 8/25 | Loss: 0.00149807
Iteration 9/25 | Loss: 0.00149013
Iteration 10/25 | Loss: 0.00147468
Iteration 11/25 | Loss: 0.00145410
Iteration 12/25 | Loss: 0.00144888
Iteration 13/25 | Loss: 0.00144743
Iteration 14/25 | Loss: 0.00144637
Iteration 15/25 | Loss: 0.00144313
Iteration 16/25 | Loss: 0.00144298
Iteration 17/25 | Loss: 0.00144219
Iteration 18/25 | Loss: 0.00144320
Iteration 19/25 | Loss: 0.00144183
Iteration 20/25 | Loss: 0.00143291
Iteration 21/25 | Loss: 0.00143015
Iteration 22/25 | Loss: 0.00142944
Iteration 23/25 | Loss: 0.00142855
Iteration 24/25 | Loss: 0.00142778
Iteration 25/25 | Loss: 0.00142726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42955565
Iteration 2/25 | Loss: 0.00273865
Iteration 3/25 | Loss: 0.00273863
Iteration 4/25 | Loss: 0.00273863
Iteration 5/25 | Loss: 0.00273863
Iteration 6/25 | Loss: 0.00273863
Iteration 7/25 | Loss: 0.00273863
Iteration 8/25 | Loss: 0.00273863
Iteration 9/25 | Loss: 0.00273862
Iteration 10/25 | Loss: 0.00273862
Iteration 11/25 | Loss: 0.00273862
Iteration 12/25 | Loss: 0.00273862
Iteration 13/25 | Loss: 0.00273862
Iteration 14/25 | Loss: 0.00273862
Iteration 15/25 | Loss: 0.00273862
Iteration 16/25 | Loss: 0.00273862
Iteration 17/25 | Loss: 0.00273862
Iteration 18/25 | Loss: 0.00273862
Iteration 19/25 | Loss: 0.00273862
Iteration 20/25 | Loss: 0.00273862
Iteration 21/25 | Loss: 0.00273862
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0027386238798499107, 0.0027386238798499107, 0.0027386238798499107, 0.0027386238798499107, 0.0027386238798499107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027386238798499107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273862
Iteration 2/1000 | Loss: 0.00026142
Iteration 3/1000 | Loss: 0.00019564
Iteration 4/1000 | Loss: 0.00016905
Iteration 5/1000 | Loss: 0.00015995
Iteration 6/1000 | Loss: 0.00015183
Iteration 7/1000 | Loss: 0.00014619
Iteration 8/1000 | Loss: 0.00014082
Iteration 9/1000 | Loss: 0.00021206
Iteration 10/1000 | Loss: 0.00013429
Iteration 11/1000 | Loss: 0.00013046
Iteration 12/1000 | Loss: 0.00012798
Iteration 13/1000 | Loss: 0.00012573
Iteration 14/1000 | Loss: 0.00012413
Iteration 15/1000 | Loss: 0.00012271
Iteration 16/1000 | Loss: 0.00012173
Iteration 17/1000 | Loss: 0.00012091
Iteration 18/1000 | Loss: 0.00012025
Iteration 19/1000 | Loss: 0.00011967
Iteration 20/1000 | Loss: 0.00011901
Iteration 21/1000 | Loss: 0.00070117
Iteration 22/1000 | Loss: 0.00019503
Iteration 23/1000 | Loss: 0.00011905
Iteration 24/1000 | Loss: 0.00055969
Iteration 25/1000 | Loss: 0.00020628
Iteration 26/1000 | Loss: 0.00011959
Iteration 27/1000 | Loss: 0.00011803
Iteration 28/1000 | Loss: 0.00050256
Iteration 29/1000 | Loss: 0.00019761
Iteration 30/1000 | Loss: 0.00011702
Iteration 31/1000 | Loss: 0.00120495
Iteration 32/1000 | Loss: 0.00025403
Iteration 33/1000 | Loss: 0.00011795
Iteration 34/1000 | Loss: 0.00054204
Iteration 35/1000 | Loss: 0.00025529
Iteration 36/1000 | Loss: 0.00254320
Iteration 37/1000 | Loss: 0.00429764
Iteration 38/1000 | Loss: 0.00177379
Iteration 39/1000 | Loss: 0.00021117
Iteration 40/1000 | Loss: 0.00015102
Iteration 41/1000 | Loss: 0.00013556
Iteration 42/1000 | Loss: 0.00051110
Iteration 43/1000 | Loss: 0.00025076
Iteration 44/1000 | Loss: 0.00044772
Iteration 45/1000 | Loss: 0.00013766
Iteration 46/1000 | Loss: 0.00011669
Iteration 47/1000 | Loss: 0.00011290
Iteration 48/1000 | Loss: 0.00010971
Iteration 49/1000 | Loss: 0.00010819
Iteration 50/1000 | Loss: 0.00010678
Iteration 51/1000 | Loss: 0.00010576
Iteration 52/1000 | Loss: 0.00010461
Iteration 53/1000 | Loss: 0.00010358
Iteration 54/1000 | Loss: 0.00010236
Iteration 55/1000 | Loss: 0.00063170
Iteration 56/1000 | Loss: 0.00021110
Iteration 57/1000 | Loss: 0.00010275
Iteration 58/1000 | Loss: 0.00060606
Iteration 59/1000 | Loss: 0.00407813
Iteration 60/1000 | Loss: 0.00310156
Iteration 61/1000 | Loss: 0.00327008
Iteration 62/1000 | Loss: 0.00158185
Iteration 63/1000 | Loss: 0.00042375
Iteration 64/1000 | Loss: 0.00018010
Iteration 65/1000 | Loss: 0.00014033
Iteration 66/1000 | Loss: 0.00013331
Iteration 67/1000 | Loss: 0.00009871
Iteration 68/1000 | Loss: 0.00008808
Iteration 69/1000 | Loss: 0.00010366
Iteration 70/1000 | Loss: 0.00008238
Iteration 71/1000 | Loss: 0.00045918
Iteration 72/1000 | Loss: 0.00025093
Iteration 73/1000 | Loss: 0.00029584
Iteration 74/1000 | Loss: 0.00007363
Iteration 75/1000 | Loss: 0.00006897
Iteration 76/1000 | Loss: 0.00006634
Iteration 77/1000 | Loss: 0.00006394
Iteration 78/1000 | Loss: 0.00006203
Iteration 79/1000 | Loss: 0.00006094
Iteration 80/1000 | Loss: 0.00005987
Iteration 81/1000 | Loss: 0.00005908
Iteration 82/1000 | Loss: 0.00005851
Iteration 83/1000 | Loss: 0.00005804
Iteration 84/1000 | Loss: 0.00005772
Iteration 85/1000 | Loss: 0.00005751
Iteration 86/1000 | Loss: 0.00005726
Iteration 87/1000 | Loss: 0.00005707
Iteration 88/1000 | Loss: 0.00005685
Iteration 89/1000 | Loss: 0.00005677
Iteration 90/1000 | Loss: 0.00005662
Iteration 91/1000 | Loss: 0.00005653
Iteration 92/1000 | Loss: 0.00005649
Iteration 93/1000 | Loss: 0.00005649
Iteration 94/1000 | Loss: 0.00005648
Iteration 95/1000 | Loss: 0.00005641
Iteration 96/1000 | Loss: 0.00005637
Iteration 97/1000 | Loss: 0.00005637
Iteration 98/1000 | Loss: 0.00005637
Iteration 99/1000 | Loss: 0.00005637
Iteration 100/1000 | Loss: 0.00005637
Iteration 101/1000 | Loss: 0.00005637
Iteration 102/1000 | Loss: 0.00005636
Iteration 103/1000 | Loss: 0.00005636
Iteration 104/1000 | Loss: 0.00005636
Iteration 105/1000 | Loss: 0.00005633
Iteration 106/1000 | Loss: 0.00005632
Iteration 107/1000 | Loss: 0.00005632
Iteration 108/1000 | Loss: 0.00005632
Iteration 109/1000 | Loss: 0.00005631
Iteration 110/1000 | Loss: 0.00005631
Iteration 111/1000 | Loss: 0.00005630
Iteration 112/1000 | Loss: 0.00005630
Iteration 113/1000 | Loss: 0.00005630
Iteration 114/1000 | Loss: 0.00005630
Iteration 115/1000 | Loss: 0.00005629
Iteration 116/1000 | Loss: 0.00005629
Iteration 117/1000 | Loss: 0.00005628
Iteration 118/1000 | Loss: 0.00005628
Iteration 119/1000 | Loss: 0.00005627
Iteration 120/1000 | Loss: 0.00005627
Iteration 121/1000 | Loss: 0.00005627
Iteration 122/1000 | Loss: 0.00005626
Iteration 123/1000 | Loss: 0.00005625
Iteration 124/1000 | Loss: 0.00005625
Iteration 125/1000 | Loss: 0.00005625
Iteration 126/1000 | Loss: 0.00005623
Iteration 127/1000 | Loss: 0.00005622
Iteration 128/1000 | Loss: 0.00005622
Iteration 129/1000 | Loss: 0.00005621
Iteration 130/1000 | Loss: 0.00005621
Iteration 131/1000 | Loss: 0.00005621
Iteration 132/1000 | Loss: 0.00005620
Iteration 133/1000 | Loss: 0.00005620
Iteration 134/1000 | Loss: 0.00005620
Iteration 135/1000 | Loss: 0.00005619
Iteration 136/1000 | Loss: 0.00005618
Iteration 137/1000 | Loss: 0.00005618
Iteration 138/1000 | Loss: 0.00005618
Iteration 139/1000 | Loss: 0.00005617
Iteration 140/1000 | Loss: 0.00005617
Iteration 141/1000 | Loss: 0.00005617
Iteration 142/1000 | Loss: 0.00005617
Iteration 143/1000 | Loss: 0.00005616
Iteration 144/1000 | Loss: 0.00005616
Iteration 145/1000 | Loss: 0.00005616
Iteration 146/1000 | Loss: 0.00005616
Iteration 147/1000 | Loss: 0.00005616
Iteration 148/1000 | Loss: 0.00005616
Iteration 149/1000 | Loss: 0.00005616
Iteration 150/1000 | Loss: 0.00005615
Iteration 151/1000 | Loss: 0.00005615
Iteration 152/1000 | Loss: 0.00005615
Iteration 153/1000 | Loss: 0.00005615
Iteration 154/1000 | Loss: 0.00005615
Iteration 155/1000 | Loss: 0.00005614
Iteration 156/1000 | Loss: 0.00005614
Iteration 157/1000 | Loss: 0.00005614
Iteration 158/1000 | Loss: 0.00005614
Iteration 159/1000 | Loss: 0.00005614
Iteration 160/1000 | Loss: 0.00005614
Iteration 161/1000 | Loss: 0.00005614
Iteration 162/1000 | Loss: 0.00005614
Iteration 163/1000 | Loss: 0.00005614
Iteration 164/1000 | Loss: 0.00005613
Iteration 165/1000 | Loss: 0.00005613
Iteration 166/1000 | Loss: 0.00005613
Iteration 167/1000 | Loss: 0.00005613
Iteration 168/1000 | Loss: 0.00005613
Iteration 169/1000 | Loss: 0.00005613
Iteration 170/1000 | Loss: 0.00005612
Iteration 171/1000 | Loss: 0.00005612
Iteration 172/1000 | Loss: 0.00005612
Iteration 173/1000 | Loss: 0.00005612
Iteration 174/1000 | Loss: 0.00005612
Iteration 175/1000 | Loss: 0.00005612
Iteration 176/1000 | Loss: 0.00005612
Iteration 177/1000 | Loss: 0.00005612
Iteration 178/1000 | Loss: 0.00005612
Iteration 179/1000 | Loss: 0.00005611
Iteration 180/1000 | Loss: 0.00005611
Iteration 181/1000 | Loss: 0.00005611
Iteration 182/1000 | Loss: 0.00005611
Iteration 183/1000 | Loss: 0.00005611
Iteration 184/1000 | Loss: 0.00005611
Iteration 185/1000 | Loss: 0.00005611
Iteration 186/1000 | Loss: 0.00005610
Iteration 187/1000 | Loss: 0.00005610
Iteration 188/1000 | Loss: 0.00005610
Iteration 189/1000 | Loss: 0.00005610
Iteration 190/1000 | Loss: 0.00005610
Iteration 191/1000 | Loss: 0.00005610
Iteration 192/1000 | Loss: 0.00005610
Iteration 193/1000 | Loss: 0.00005610
Iteration 194/1000 | Loss: 0.00005610
Iteration 195/1000 | Loss: 0.00005610
Iteration 196/1000 | Loss: 0.00005610
Iteration 197/1000 | Loss: 0.00005610
Iteration 198/1000 | Loss: 0.00005610
Iteration 199/1000 | Loss: 0.00005610
Iteration 200/1000 | Loss: 0.00005610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [5.609682193608023e-05, 5.609682193608023e-05, 5.609682193608023e-05, 5.609682193608023e-05, 5.609682193608023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.609682193608023e-05

Optimization complete. Final v2v error: 4.496467113494873 mm

Highest mean error: 11.96804141998291 mm for frame 62

Lowest mean error: 3.4155149459838867 mm for frame 128

Saving results

Total time: 187.69261240959167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925987
Iteration 2/25 | Loss: 0.00203447
Iteration 3/25 | Loss: 0.00147524
Iteration 4/25 | Loss: 0.00145771
Iteration 5/25 | Loss: 0.00145152
Iteration 6/25 | Loss: 0.00145030
Iteration 7/25 | Loss: 0.00145030
Iteration 8/25 | Loss: 0.00145030
Iteration 9/25 | Loss: 0.00145030
Iteration 10/25 | Loss: 0.00145030
Iteration 11/25 | Loss: 0.00145030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001450301962904632, 0.001450301962904632, 0.001450301962904632, 0.001450301962904632, 0.001450301962904632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001450301962904632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.55282146
Iteration 2/25 | Loss: 0.00067394
Iteration 3/25 | Loss: 0.00067394
Iteration 4/25 | Loss: 0.00067394
Iteration 5/25 | Loss: 0.00067394
Iteration 6/25 | Loss: 0.00067394
Iteration 7/25 | Loss: 0.00067394
Iteration 8/25 | Loss: 0.00067394
Iteration 9/25 | Loss: 0.00067394
Iteration 10/25 | Loss: 0.00067394
Iteration 11/25 | Loss: 0.00067394
Iteration 12/25 | Loss: 0.00067394
Iteration 13/25 | Loss: 0.00067394
Iteration 14/25 | Loss: 0.00067394
Iteration 15/25 | Loss: 0.00067394
Iteration 16/25 | Loss: 0.00067394
Iteration 17/25 | Loss: 0.00067394
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006739395903423429, 0.0006739395903423429, 0.0006739395903423429, 0.0006739395903423429, 0.0006739395903423429]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006739395903423429

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067394
Iteration 2/1000 | Loss: 0.00007658
Iteration 3/1000 | Loss: 0.00006061
Iteration 4/1000 | Loss: 0.00005456
Iteration 5/1000 | Loss: 0.00005253
Iteration 6/1000 | Loss: 0.00005131
Iteration 7/1000 | Loss: 0.00004978
Iteration 8/1000 | Loss: 0.00004866
Iteration 9/1000 | Loss: 0.00004762
Iteration 10/1000 | Loss: 0.00004642
Iteration 11/1000 | Loss: 0.00004574
Iteration 12/1000 | Loss: 0.00004504
Iteration 13/1000 | Loss: 0.00004424
Iteration 14/1000 | Loss: 0.00004385
Iteration 15/1000 | Loss: 0.00004340
Iteration 16/1000 | Loss: 0.00004289
Iteration 17/1000 | Loss: 0.00004247
Iteration 18/1000 | Loss: 0.00004220
Iteration 19/1000 | Loss: 0.00004198
Iteration 20/1000 | Loss: 0.00004171
Iteration 21/1000 | Loss: 0.00004142
Iteration 22/1000 | Loss: 0.00004123
Iteration 23/1000 | Loss: 0.00004106
Iteration 24/1000 | Loss: 0.00004104
Iteration 25/1000 | Loss: 0.00004092
Iteration 26/1000 | Loss: 0.00004090
Iteration 27/1000 | Loss: 0.00004084
Iteration 28/1000 | Loss: 0.00004075
Iteration 29/1000 | Loss: 0.00004070
Iteration 30/1000 | Loss: 0.00004070
Iteration 31/1000 | Loss: 0.00004062
Iteration 32/1000 | Loss: 0.00004062
Iteration 33/1000 | Loss: 0.00004060
Iteration 34/1000 | Loss: 0.00004060
Iteration 35/1000 | Loss: 0.00004060
Iteration 36/1000 | Loss: 0.00004060
Iteration 37/1000 | Loss: 0.00004059
Iteration 38/1000 | Loss: 0.00004058
Iteration 39/1000 | Loss: 0.00004058
Iteration 40/1000 | Loss: 0.00004058
Iteration 41/1000 | Loss: 0.00004056
Iteration 42/1000 | Loss: 0.00004054
Iteration 43/1000 | Loss: 0.00004053
Iteration 44/1000 | Loss: 0.00004053
Iteration 45/1000 | Loss: 0.00004052
Iteration 46/1000 | Loss: 0.00004052
Iteration 47/1000 | Loss: 0.00004051
Iteration 48/1000 | Loss: 0.00004051
Iteration 49/1000 | Loss: 0.00004050
Iteration 50/1000 | Loss: 0.00004050
Iteration 51/1000 | Loss: 0.00004050
Iteration 52/1000 | Loss: 0.00004050
Iteration 53/1000 | Loss: 0.00004050
Iteration 54/1000 | Loss: 0.00004050
Iteration 55/1000 | Loss: 0.00004050
Iteration 56/1000 | Loss: 0.00004050
Iteration 57/1000 | Loss: 0.00004050
Iteration 58/1000 | Loss: 0.00004050
Iteration 59/1000 | Loss: 0.00004050
Iteration 60/1000 | Loss: 0.00004050
Iteration 61/1000 | Loss: 0.00004049
Iteration 62/1000 | Loss: 0.00004049
Iteration 63/1000 | Loss: 0.00004049
Iteration 64/1000 | Loss: 0.00004049
Iteration 65/1000 | Loss: 0.00004048
Iteration 66/1000 | Loss: 0.00004048
Iteration 67/1000 | Loss: 0.00004048
Iteration 68/1000 | Loss: 0.00004047
Iteration 69/1000 | Loss: 0.00004047
Iteration 70/1000 | Loss: 0.00004047
Iteration 71/1000 | Loss: 0.00004046
Iteration 72/1000 | Loss: 0.00004046
Iteration 73/1000 | Loss: 0.00004045
Iteration 74/1000 | Loss: 0.00004045
Iteration 75/1000 | Loss: 0.00004045
Iteration 76/1000 | Loss: 0.00004044
Iteration 77/1000 | Loss: 0.00004044
Iteration 78/1000 | Loss: 0.00004044
Iteration 79/1000 | Loss: 0.00004044
Iteration 80/1000 | Loss: 0.00004044
Iteration 81/1000 | Loss: 0.00004044
Iteration 82/1000 | Loss: 0.00004044
Iteration 83/1000 | Loss: 0.00004044
Iteration 84/1000 | Loss: 0.00004043
Iteration 85/1000 | Loss: 0.00004043
Iteration 86/1000 | Loss: 0.00004043
Iteration 87/1000 | Loss: 0.00004043
Iteration 88/1000 | Loss: 0.00004043
Iteration 89/1000 | Loss: 0.00004043
Iteration 90/1000 | Loss: 0.00004043
Iteration 91/1000 | Loss: 0.00004042
Iteration 92/1000 | Loss: 0.00004042
Iteration 93/1000 | Loss: 0.00004042
Iteration 94/1000 | Loss: 0.00004042
Iteration 95/1000 | Loss: 0.00004042
Iteration 96/1000 | Loss: 0.00004042
Iteration 97/1000 | Loss: 0.00004042
Iteration 98/1000 | Loss: 0.00004042
Iteration 99/1000 | Loss: 0.00004042
Iteration 100/1000 | Loss: 0.00004042
Iteration 101/1000 | Loss: 0.00004042
Iteration 102/1000 | Loss: 0.00004042
Iteration 103/1000 | Loss: 0.00004042
Iteration 104/1000 | Loss: 0.00004042
Iteration 105/1000 | Loss: 0.00004041
Iteration 106/1000 | Loss: 0.00004041
Iteration 107/1000 | Loss: 0.00004041
Iteration 108/1000 | Loss: 0.00004041
Iteration 109/1000 | Loss: 0.00004040
Iteration 110/1000 | Loss: 0.00004040
Iteration 111/1000 | Loss: 0.00004040
Iteration 112/1000 | Loss: 0.00004040
Iteration 113/1000 | Loss: 0.00004040
Iteration 114/1000 | Loss: 0.00004040
Iteration 115/1000 | Loss: 0.00004040
Iteration 116/1000 | Loss: 0.00004040
Iteration 117/1000 | Loss: 0.00004039
Iteration 118/1000 | Loss: 0.00004039
Iteration 119/1000 | Loss: 0.00004039
Iteration 120/1000 | Loss: 0.00004038
Iteration 121/1000 | Loss: 0.00004038
Iteration 122/1000 | Loss: 0.00004038
Iteration 123/1000 | Loss: 0.00004038
Iteration 124/1000 | Loss: 0.00004038
Iteration 125/1000 | Loss: 0.00004038
Iteration 126/1000 | Loss: 0.00004038
Iteration 127/1000 | Loss: 0.00004037
Iteration 128/1000 | Loss: 0.00004037
Iteration 129/1000 | Loss: 0.00004037
Iteration 130/1000 | Loss: 0.00004036
Iteration 131/1000 | Loss: 0.00004036
Iteration 132/1000 | Loss: 0.00004036
Iteration 133/1000 | Loss: 0.00004036
Iteration 134/1000 | Loss: 0.00004036
Iteration 135/1000 | Loss: 0.00004036
Iteration 136/1000 | Loss: 0.00004036
Iteration 137/1000 | Loss: 0.00004036
Iteration 138/1000 | Loss: 0.00004036
Iteration 139/1000 | Loss: 0.00004036
Iteration 140/1000 | Loss: 0.00004036
Iteration 141/1000 | Loss: 0.00004036
Iteration 142/1000 | Loss: 0.00004036
Iteration 143/1000 | Loss: 0.00004036
Iteration 144/1000 | Loss: 0.00004035
Iteration 145/1000 | Loss: 0.00004035
Iteration 146/1000 | Loss: 0.00004035
Iteration 147/1000 | Loss: 0.00004035
Iteration 148/1000 | Loss: 0.00004035
Iteration 149/1000 | Loss: 0.00004035
Iteration 150/1000 | Loss: 0.00004035
Iteration 151/1000 | Loss: 0.00004035
Iteration 152/1000 | Loss: 0.00004035
Iteration 153/1000 | Loss: 0.00004035
Iteration 154/1000 | Loss: 0.00004035
Iteration 155/1000 | Loss: 0.00004035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [4.035341044072993e-05, 4.035341044072993e-05, 4.035341044072993e-05, 4.035341044072993e-05, 4.035341044072993e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.035341044072993e-05

Optimization complete. Final v2v error: 5.2363691329956055 mm

Highest mean error: 5.674039840698242 mm for frame 68

Lowest mean error: 4.8686842918396 mm for frame 107

Saving results

Total time: 63.35935020446777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024739
Iteration 2/25 | Loss: 0.01024739
Iteration 3/25 | Loss: 0.01024739
Iteration 4/25 | Loss: 0.01024739
Iteration 5/25 | Loss: 0.01024739
Iteration 6/25 | Loss: 0.01024738
Iteration 7/25 | Loss: 0.00198328
Iteration 8/25 | Loss: 0.00155689
Iteration 9/25 | Loss: 0.00145826
Iteration 10/25 | Loss: 0.00153627
Iteration 11/25 | Loss: 0.00154303
Iteration 12/25 | Loss: 0.00145900
Iteration 13/25 | Loss: 0.00136489
Iteration 14/25 | Loss: 0.00134176
Iteration 15/25 | Loss: 0.00132364
Iteration 16/25 | Loss: 0.00132503
Iteration 17/25 | Loss: 0.00132406
Iteration 18/25 | Loss: 0.00131837
Iteration 19/25 | Loss: 0.00130520
Iteration 20/25 | Loss: 0.00130459
Iteration 21/25 | Loss: 0.00128997
Iteration 22/25 | Loss: 0.00128429
Iteration 23/25 | Loss: 0.00127884
Iteration 24/25 | Loss: 0.00127891
Iteration 25/25 | Loss: 0.00127884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48406506
Iteration 2/25 | Loss: 0.00138354
Iteration 3/25 | Loss: 0.00132425
Iteration 4/25 | Loss: 0.00132425
Iteration 5/25 | Loss: 0.00132425
Iteration 6/25 | Loss: 0.00132425
Iteration 7/25 | Loss: 0.00132425
Iteration 8/25 | Loss: 0.00132425
Iteration 9/25 | Loss: 0.00132425
Iteration 10/25 | Loss: 0.00132425
Iteration 11/25 | Loss: 0.00132425
Iteration 12/25 | Loss: 0.00132425
Iteration 13/25 | Loss: 0.00132425
Iteration 14/25 | Loss: 0.00132424
Iteration 15/25 | Loss: 0.00132424
Iteration 16/25 | Loss: 0.00132424
Iteration 17/25 | Loss: 0.00132424
Iteration 18/25 | Loss: 0.00132424
Iteration 19/25 | Loss: 0.00132424
Iteration 20/25 | Loss: 0.00132424
Iteration 21/25 | Loss: 0.00132424
Iteration 22/25 | Loss: 0.00132424
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013242446584627032, 0.0013242446584627032, 0.0013242446584627032, 0.0013242446584627032, 0.0013242446584627032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013242446584627032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132424
Iteration 2/1000 | Loss: 0.00230222
Iteration 3/1000 | Loss: 0.00046400
Iteration 4/1000 | Loss: 0.00029670
Iteration 5/1000 | Loss: 0.00063917
Iteration 6/1000 | Loss: 0.00083773
Iteration 7/1000 | Loss: 0.00068140
Iteration 8/1000 | Loss: 0.00109332
Iteration 9/1000 | Loss: 0.00052916
Iteration 10/1000 | Loss: 0.00085492
Iteration 11/1000 | Loss: 0.00070884
Iteration 12/1000 | Loss: 0.00049211
Iteration 13/1000 | Loss: 0.00053266
Iteration 14/1000 | Loss: 0.00054159
Iteration 15/1000 | Loss: 0.00059053
Iteration 16/1000 | Loss: 0.00065408
Iteration 17/1000 | Loss: 0.00060462
Iteration 18/1000 | Loss: 0.00048377
Iteration 19/1000 | Loss: 0.00068910
Iteration 20/1000 | Loss: 0.00071397
Iteration 21/1000 | Loss: 0.00091317
Iteration 22/1000 | Loss: 0.00062750
Iteration 23/1000 | Loss: 0.00041758
Iteration 24/1000 | Loss: 0.00048176
Iteration 25/1000 | Loss: 0.00060487
Iteration 26/1000 | Loss: 0.00036285
Iteration 27/1000 | Loss: 0.00040626
Iteration 28/1000 | Loss: 0.00031997
Iteration 29/1000 | Loss: 0.00019240
Iteration 30/1000 | Loss: 0.00067779
Iteration 31/1000 | Loss: 0.00079938
Iteration 32/1000 | Loss: 0.00021488
Iteration 33/1000 | Loss: 0.00010663
Iteration 34/1000 | Loss: 0.00083527
Iteration 35/1000 | Loss: 0.00065707
Iteration 36/1000 | Loss: 0.00097750
Iteration 37/1000 | Loss: 0.00164738
Iteration 38/1000 | Loss: 0.00046923
Iteration 39/1000 | Loss: 0.00015866
Iteration 40/1000 | Loss: 0.00019628
Iteration 41/1000 | Loss: 0.00033232
Iteration 42/1000 | Loss: 0.00034032
Iteration 43/1000 | Loss: 0.00031607
Iteration 44/1000 | Loss: 0.00040556
Iteration 45/1000 | Loss: 0.00025855
Iteration 46/1000 | Loss: 0.00011789
Iteration 47/1000 | Loss: 0.00028619
Iteration 48/1000 | Loss: 0.00038249
Iteration 49/1000 | Loss: 0.00033528
Iteration 50/1000 | Loss: 0.00021624
Iteration 51/1000 | Loss: 0.00012388
Iteration 52/1000 | Loss: 0.00025755
Iteration 53/1000 | Loss: 0.00029395
Iteration 54/1000 | Loss: 0.00008440
Iteration 55/1000 | Loss: 0.00028954
Iteration 56/1000 | Loss: 0.00029637
Iteration 57/1000 | Loss: 0.00020648
Iteration 58/1000 | Loss: 0.00018279
Iteration 59/1000 | Loss: 0.00020535
Iteration 60/1000 | Loss: 0.00044021
Iteration 61/1000 | Loss: 0.00043630
Iteration 62/1000 | Loss: 0.00012996
Iteration 63/1000 | Loss: 0.00028875
Iteration 64/1000 | Loss: 0.00023989
Iteration 65/1000 | Loss: 0.00039278
Iteration 66/1000 | Loss: 0.00047860
Iteration 67/1000 | Loss: 0.00035908
Iteration 68/1000 | Loss: 0.00048189
Iteration 69/1000 | Loss: 0.00033881
Iteration 70/1000 | Loss: 0.00037456
Iteration 71/1000 | Loss: 0.00034967
Iteration 72/1000 | Loss: 0.00020099
Iteration 73/1000 | Loss: 0.00028806
Iteration 74/1000 | Loss: 0.00031251
Iteration 75/1000 | Loss: 0.00028140
Iteration 76/1000 | Loss: 0.00021903
Iteration 77/1000 | Loss: 0.00052980
Iteration 78/1000 | Loss: 0.00034438
Iteration 79/1000 | Loss: 0.00014337
Iteration 80/1000 | Loss: 0.00042253
Iteration 81/1000 | Loss: 0.00059463
Iteration 82/1000 | Loss: 0.00061369
Iteration 83/1000 | Loss: 0.00065229
Iteration 84/1000 | Loss: 0.00065501
Iteration 85/1000 | Loss: 0.00073865
Iteration 86/1000 | Loss: 0.00070758
Iteration 87/1000 | Loss: 0.00033998
Iteration 88/1000 | Loss: 0.00008095
Iteration 89/1000 | Loss: 0.00043066
Iteration 90/1000 | Loss: 0.00051396
Iteration 91/1000 | Loss: 0.00037358
Iteration 92/1000 | Loss: 0.00020751
Iteration 93/1000 | Loss: 0.00005549
Iteration 94/1000 | Loss: 0.00027945
Iteration 95/1000 | Loss: 0.00034027
Iteration 96/1000 | Loss: 0.00027436
Iteration 97/1000 | Loss: 0.00031018
Iteration 98/1000 | Loss: 0.00024708
Iteration 99/1000 | Loss: 0.00022008
Iteration 100/1000 | Loss: 0.00022330
Iteration 101/1000 | Loss: 0.00021917
Iteration 102/1000 | Loss: 0.00018537
Iteration 103/1000 | Loss: 0.00015894
Iteration 104/1000 | Loss: 0.00037072
Iteration 105/1000 | Loss: 0.00032294
Iteration 106/1000 | Loss: 0.00030205
Iteration 107/1000 | Loss: 0.00018697
Iteration 108/1000 | Loss: 0.00021343
Iteration 109/1000 | Loss: 0.00015944
Iteration 110/1000 | Loss: 0.00031789
Iteration 111/1000 | Loss: 0.00020632
Iteration 112/1000 | Loss: 0.00026067
Iteration 113/1000 | Loss: 0.00017099
Iteration 114/1000 | Loss: 0.00021077
Iteration 115/1000 | Loss: 0.00014672
Iteration 116/1000 | Loss: 0.00012933
Iteration 117/1000 | Loss: 0.00021328
Iteration 118/1000 | Loss: 0.00019386
Iteration 119/1000 | Loss: 0.00013844
Iteration 120/1000 | Loss: 0.00003097
Iteration 121/1000 | Loss: 0.00018853
Iteration 122/1000 | Loss: 0.00041014
Iteration 123/1000 | Loss: 0.00037330
Iteration 124/1000 | Loss: 0.00060731
Iteration 125/1000 | Loss: 0.00038605
Iteration 126/1000 | Loss: 0.00034819
Iteration 127/1000 | Loss: 0.00028848
Iteration 128/1000 | Loss: 0.00008570
Iteration 129/1000 | Loss: 0.00011367
Iteration 130/1000 | Loss: 0.00018290
Iteration 131/1000 | Loss: 0.00015543
Iteration 132/1000 | Loss: 0.00034020
Iteration 133/1000 | Loss: 0.00026417
Iteration 134/1000 | Loss: 0.00011183
Iteration 135/1000 | Loss: 0.00010643
Iteration 136/1000 | Loss: 0.00011679
Iteration 137/1000 | Loss: 0.00019019
Iteration 138/1000 | Loss: 0.00020292
Iteration 139/1000 | Loss: 0.00010025
Iteration 140/1000 | Loss: 0.00004498
Iteration 141/1000 | Loss: 0.00006537
Iteration 142/1000 | Loss: 0.00004687
Iteration 143/1000 | Loss: 0.00065096
Iteration 144/1000 | Loss: 0.00014348
Iteration 145/1000 | Loss: 0.00008256
Iteration 146/1000 | Loss: 0.00009193
Iteration 147/1000 | Loss: 0.00003674
Iteration 148/1000 | Loss: 0.00017290
Iteration 149/1000 | Loss: 0.00005625
Iteration 150/1000 | Loss: 0.00009918
Iteration 151/1000 | Loss: 0.00018714
Iteration 152/1000 | Loss: 0.00083687
Iteration 153/1000 | Loss: 0.00018901
Iteration 154/1000 | Loss: 0.00062823
Iteration 155/1000 | Loss: 0.00038141
Iteration 156/1000 | Loss: 0.00027570
Iteration 157/1000 | Loss: 0.00041043
Iteration 158/1000 | Loss: 0.00004111
Iteration 159/1000 | Loss: 0.00018638
Iteration 160/1000 | Loss: 0.00011059
Iteration 161/1000 | Loss: 0.00012851
Iteration 162/1000 | Loss: 0.00010696
Iteration 163/1000 | Loss: 0.00021265
Iteration 164/1000 | Loss: 0.00003191
Iteration 165/1000 | Loss: 0.00023234
Iteration 166/1000 | Loss: 0.00029743
Iteration 167/1000 | Loss: 0.00020699
Iteration 168/1000 | Loss: 0.00010852
Iteration 169/1000 | Loss: 0.00011677
Iteration 170/1000 | Loss: 0.00005109
Iteration 171/1000 | Loss: 0.00013366
Iteration 172/1000 | Loss: 0.00005758
Iteration 173/1000 | Loss: 0.00002717
Iteration 174/1000 | Loss: 0.00004051
Iteration 175/1000 | Loss: 0.00012994
Iteration 176/1000 | Loss: 0.00027516
Iteration 177/1000 | Loss: 0.00003012
Iteration 178/1000 | Loss: 0.00018207
Iteration 179/1000 | Loss: 0.00013194
Iteration 180/1000 | Loss: 0.00004854
Iteration 181/1000 | Loss: 0.00013099
Iteration 182/1000 | Loss: 0.00012497
Iteration 183/1000 | Loss: 0.00012819
Iteration 184/1000 | Loss: 0.00003140
Iteration 185/1000 | Loss: 0.00002757
Iteration 186/1000 | Loss: 0.00011121
Iteration 187/1000 | Loss: 0.00007114
Iteration 188/1000 | Loss: 0.00009224
Iteration 189/1000 | Loss: 0.00006630
Iteration 190/1000 | Loss: 0.00018348
Iteration 191/1000 | Loss: 0.00013705
Iteration 192/1000 | Loss: 0.00002708
Iteration 193/1000 | Loss: 0.00016182
Iteration 194/1000 | Loss: 0.00008159
Iteration 195/1000 | Loss: 0.00022291
Iteration 196/1000 | Loss: 0.00022880
Iteration 197/1000 | Loss: 0.00019606
Iteration 198/1000 | Loss: 0.00055623
Iteration 199/1000 | Loss: 0.00004501
Iteration 200/1000 | Loss: 0.00008391
Iteration 201/1000 | Loss: 0.00031498
Iteration 202/1000 | Loss: 0.00016555
Iteration 203/1000 | Loss: 0.00050370
Iteration 204/1000 | Loss: 0.00027765
Iteration 205/1000 | Loss: 0.00023303
Iteration 206/1000 | Loss: 0.00016340
Iteration 207/1000 | Loss: 0.00016620
Iteration 208/1000 | Loss: 0.00003772
Iteration 209/1000 | Loss: 0.00003017
Iteration 210/1000 | Loss: 0.00034793
Iteration 211/1000 | Loss: 0.00010487
Iteration 212/1000 | Loss: 0.00002558
Iteration 213/1000 | Loss: 0.00002428
Iteration 214/1000 | Loss: 0.00002314
Iteration 215/1000 | Loss: 0.00012802
Iteration 216/1000 | Loss: 0.00031116
Iteration 217/1000 | Loss: 0.00025041
Iteration 218/1000 | Loss: 0.00013277
Iteration 219/1000 | Loss: 0.00020300
Iteration 220/1000 | Loss: 0.00013496
Iteration 221/1000 | Loss: 0.00007015
Iteration 222/1000 | Loss: 0.00007024
Iteration 223/1000 | Loss: 0.00014241
Iteration 224/1000 | Loss: 0.00005089
Iteration 225/1000 | Loss: 0.00012362
Iteration 226/1000 | Loss: 0.00018444
Iteration 227/1000 | Loss: 0.00005673
Iteration 228/1000 | Loss: 0.00006794
Iteration 229/1000 | Loss: 0.00005188
Iteration 230/1000 | Loss: 0.00004000
Iteration 231/1000 | Loss: 0.00010717
Iteration 232/1000 | Loss: 0.00007966
Iteration 233/1000 | Loss: 0.00004014
Iteration 234/1000 | Loss: 0.00010217
Iteration 235/1000 | Loss: 0.00012592
Iteration 236/1000 | Loss: 0.00003310
Iteration 237/1000 | Loss: 0.00040799
Iteration 238/1000 | Loss: 0.00027025
Iteration 239/1000 | Loss: 0.00002385
Iteration 240/1000 | Loss: 0.00017899
Iteration 241/1000 | Loss: 0.00003360
Iteration 242/1000 | Loss: 0.00009463
Iteration 243/1000 | Loss: 0.00010307
Iteration 244/1000 | Loss: 0.00013483
Iteration 245/1000 | Loss: 0.00007828
Iteration 246/1000 | Loss: 0.00007811
Iteration 247/1000 | Loss: 0.00009520
Iteration 248/1000 | Loss: 0.00011556
Iteration 249/1000 | Loss: 0.00003365
Iteration 250/1000 | Loss: 0.00008301
Iteration 251/1000 | Loss: 0.00015008
Iteration 252/1000 | Loss: 0.00011771
Iteration 253/1000 | Loss: 0.00012005
Iteration 254/1000 | Loss: 0.00011105
Iteration 255/1000 | Loss: 0.00012468
Iteration 256/1000 | Loss: 0.00006559
Iteration 257/1000 | Loss: 0.00011157
Iteration 258/1000 | Loss: 0.00008706
Iteration 259/1000 | Loss: 0.00002397
Iteration 260/1000 | Loss: 0.00002049
Iteration 261/1000 | Loss: 0.00009927
Iteration 262/1000 | Loss: 0.00003257
Iteration 263/1000 | Loss: 0.00004838
Iteration 264/1000 | Loss: 0.00002286
Iteration 265/1000 | Loss: 0.00002172
Iteration 266/1000 | Loss: 0.00008819
Iteration 267/1000 | Loss: 0.00012084
Iteration 268/1000 | Loss: 0.00007352
Iteration 269/1000 | Loss: 0.00008836
Iteration 270/1000 | Loss: 0.00011061
Iteration 271/1000 | Loss: 0.00032561
Iteration 272/1000 | Loss: 0.00070995
Iteration 273/1000 | Loss: 0.00037176
Iteration 274/1000 | Loss: 0.00009390
Iteration 275/1000 | Loss: 0.00002515
Iteration 276/1000 | Loss: 0.00017129
Iteration 277/1000 | Loss: 0.00015912
Iteration 278/1000 | Loss: 0.00007782
Iteration 279/1000 | Loss: 0.00017904
Iteration 280/1000 | Loss: 0.00003409
Iteration 281/1000 | Loss: 0.00002368
Iteration 282/1000 | Loss: 0.00002150
Iteration 283/1000 | Loss: 0.00002022
Iteration 284/1000 | Loss: 0.00001972
Iteration 285/1000 | Loss: 0.00001947
Iteration 286/1000 | Loss: 0.00001936
Iteration 287/1000 | Loss: 0.00001914
Iteration 288/1000 | Loss: 0.00025089
Iteration 289/1000 | Loss: 0.00039565
Iteration 290/1000 | Loss: 0.00004972
Iteration 291/1000 | Loss: 0.00001905
Iteration 292/1000 | Loss: 0.00001881
Iteration 293/1000 | Loss: 0.00001880
Iteration 294/1000 | Loss: 0.00025565
Iteration 295/1000 | Loss: 0.00022750
Iteration 296/1000 | Loss: 0.00001944
Iteration 297/1000 | Loss: 0.00001885
Iteration 298/1000 | Loss: 0.00001879
Iteration 299/1000 | Loss: 0.00001871
Iteration 300/1000 | Loss: 0.00001868
Iteration 301/1000 | Loss: 0.00001867
Iteration 302/1000 | Loss: 0.00001866
Iteration 303/1000 | Loss: 0.00025387
Iteration 304/1000 | Loss: 0.00025386
Iteration 305/1000 | Loss: 0.00003572
Iteration 306/1000 | Loss: 0.00002489
Iteration 307/1000 | Loss: 0.00002337
Iteration 308/1000 | Loss: 0.00015006
Iteration 309/1000 | Loss: 0.00032034
Iteration 310/1000 | Loss: 0.00027552
Iteration 311/1000 | Loss: 0.00026088
Iteration 312/1000 | Loss: 0.00051064
Iteration 313/1000 | Loss: 0.00023864
Iteration 314/1000 | Loss: 0.00002703
Iteration 315/1000 | Loss: 0.00051335
Iteration 316/1000 | Loss: 0.00016604
Iteration 317/1000 | Loss: 0.00046518
Iteration 318/1000 | Loss: 0.00064451
Iteration 319/1000 | Loss: 0.00034542
Iteration 320/1000 | Loss: 0.00014765
Iteration 321/1000 | Loss: 0.00014000
Iteration 322/1000 | Loss: 0.00004777
Iteration 323/1000 | Loss: 0.00002472
Iteration 324/1000 | Loss: 0.00002057
Iteration 325/1000 | Loss: 0.00001794
Iteration 326/1000 | Loss: 0.00001657
Iteration 327/1000 | Loss: 0.00001616
Iteration 328/1000 | Loss: 0.00001581
Iteration 329/1000 | Loss: 0.00001571
Iteration 330/1000 | Loss: 0.00001568
Iteration 331/1000 | Loss: 0.00001553
Iteration 332/1000 | Loss: 0.00001551
Iteration 333/1000 | Loss: 0.00001547
Iteration 334/1000 | Loss: 0.00001541
Iteration 335/1000 | Loss: 0.00001539
Iteration 336/1000 | Loss: 0.00001538
Iteration 337/1000 | Loss: 0.00001533
Iteration 338/1000 | Loss: 0.00001532
Iteration 339/1000 | Loss: 0.00001531
Iteration 340/1000 | Loss: 0.00001531
Iteration 341/1000 | Loss: 0.00001531
Iteration 342/1000 | Loss: 0.00001530
Iteration 343/1000 | Loss: 0.00001530
Iteration 344/1000 | Loss: 0.00001530
Iteration 345/1000 | Loss: 0.00001530
Iteration 346/1000 | Loss: 0.00001530
Iteration 347/1000 | Loss: 0.00001530
Iteration 348/1000 | Loss: 0.00001529
Iteration 349/1000 | Loss: 0.00001529
Iteration 350/1000 | Loss: 0.00001529
Iteration 351/1000 | Loss: 0.00001529
Iteration 352/1000 | Loss: 0.00001529
Iteration 353/1000 | Loss: 0.00001529
Iteration 354/1000 | Loss: 0.00001529
Iteration 355/1000 | Loss: 0.00001529
Iteration 356/1000 | Loss: 0.00001528
Iteration 357/1000 | Loss: 0.00001528
Iteration 358/1000 | Loss: 0.00001528
Iteration 359/1000 | Loss: 0.00001528
Iteration 360/1000 | Loss: 0.00001528
Iteration 361/1000 | Loss: 0.00001528
Iteration 362/1000 | Loss: 0.00001527
Iteration 363/1000 | Loss: 0.00001527
Iteration 364/1000 | Loss: 0.00001527
Iteration 365/1000 | Loss: 0.00001526
Iteration 366/1000 | Loss: 0.00001526
Iteration 367/1000 | Loss: 0.00001525
Iteration 368/1000 | Loss: 0.00001525
Iteration 369/1000 | Loss: 0.00001525
Iteration 370/1000 | Loss: 0.00001525
Iteration 371/1000 | Loss: 0.00001525
Iteration 372/1000 | Loss: 0.00001524
Iteration 373/1000 | Loss: 0.00001524
Iteration 374/1000 | Loss: 0.00001524
Iteration 375/1000 | Loss: 0.00001524
Iteration 376/1000 | Loss: 0.00001524
Iteration 377/1000 | Loss: 0.00001524
Iteration 378/1000 | Loss: 0.00001524
Iteration 379/1000 | Loss: 0.00001524
Iteration 380/1000 | Loss: 0.00001524
Iteration 381/1000 | Loss: 0.00001524
Iteration 382/1000 | Loss: 0.00001523
Iteration 383/1000 | Loss: 0.00001523
Iteration 384/1000 | Loss: 0.00001523
Iteration 385/1000 | Loss: 0.00001523
Iteration 386/1000 | Loss: 0.00001523
Iteration 387/1000 | Loss: 0.00001523
Iteration 388/1000 | Loss: 0.00001523
Iteration 389/1000 | Loss: 0.00001523
Iteration 390/1000 | Loss: 0.00001523
Iteration 391/1000 | Loss: 0.00001523
Iteration 392/1000 | Loss: 0.00001523
Iteration 393/1000 | Loss: 0.00001523
Iteration 394/1000 | Loss: 0.00001523
Iteration 395/1000 | Loss: 0.00001523
Iteration 396/1000 | Loss: 0.00001523
Iteration 397/1000 | Loss: 0.00001523
Iteration 398/1000 | Loss: 0.00001523
Iteration 399/1000 | Loss: 0.00001523
Iteration 400/1000 | Loss: 0.00001523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 400. Stopping optimization.
Last 5 losses: [1.5229119526338764e-05, 1.5229119526338764e-05, 1.5229119526338764e-05, 1.5229119526338764e-05, 1.5229119526338764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5229119526338764e-05

Optimization complete. Final v2v error: 3.278169870376587 mm

Highest mean error: 4.6306471824646 mm for frame 217

Lowest mean error: 2.9659698009490967 mm for frame 141

Saving results

Total time: 557.4326684474945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396575
Iteration 2/25 | Loss: 0.00124577
Iteration 3/25 | Loss: 0.00118257
Iteration 4/25 | Loss: 0.00117084
Iteration 5/25 | Loss: 0.00116674
Iteration 6/25 | Loss: 0.00116589
Iteration 7/25 | Loss: 0.00116584
Iteration 8/25 | Loss: 0.00116584
Iteration 9/25 | Loss: 0.00116584
Iteration 10/25 | Loss: 0.00116584
Iteration 11/25 | Loss: 0.00116584
Iteration 12/25 | Loss: 0.00116584
Iteration 13/25 | Loss: 0.00116584
Iteration 14/25 | Loss: 0.00116584
Iteration 15/25 | Loss: 0.00116584
Iteration 16/25 | Loss: 0.00116584
Iteration 17/25 | Loss: 0.00116584
Iteration 18/25 | Loss: 0.00116584
Iteration 19/25 | Loss: 0.00116584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001165839727036655, 0.001165839727036655, 0.001165839727036655, 0.001165839727036655, 0.001165839727036655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001165839727036655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79837859
Iteration 2/25 | Loss: 0.00133638
Iteration 3/25 | Loss: 0.00133638
Iteration 4/25 | Loss: 0.00133638
Iteration 5/25 | Loss: 0.00133637
Iteration 6/25 | Loss: 0.00133637
Iteration 7/25 | Loss: 0.00133637
Iteration 8/25 | Loss: 0.00133637
Iteration 9/25 | Loss: 0.00133637
Iteration 10/25 | Loss: 0.00133637
Iteration 11/25 | Loss: 0.00133637
Iteration 12/25 | Loss: 0.00133637
Iteration 13/25 | Loss: 0.00133637
Iteration 14/25 | Loss: 0.00133637
Iteration 15/25 | Loss: 0.00133637
Iteration 16/25 | Loss: 0.00133637
Iteration 17/25 | Loss: 0.00133637
Iteration 18/25 | Loss: 0.00133637
Iteration 19/25 | Loss: 0.00133637
Iteration 20/25 | Loss: 0.00133637
Iteration 21/25 | Loss: 0.00133637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013363732723519206, 0.0013363732723519206, 0.0013363732723519206, 0.0013363732723519206, 0.0013363732723519206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013363732723519206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133637
Iteration 2/1000 | Loss: 0.00002468
Iteration 3/1000 | Loss: 0.00001788
Iteration 4/1000 | Loss: 0.00001489
Iteration 5/1000 | Loss: 0.00001400
Iteration 6/1000 | Loss: 0.00001327
Iteration 7/1000 | Loss: 0.00001283
Iteration 8/1000 | Loss: 0.00001241
Iteration 9/1000 | Loss: 0.00001210
Iteration 10/1000 | Loss: 0.00001197
Iteration 11/1000 | Loss: 0.00001174
Iteration 12/1000 | Loss: 0.00001151
Iteration 13/1000 | Loss: 0.00001147
Iteration 14/1000 | Loss: 0.00001145
Iteration 15/1000 | Loss: 0.00001134
Iteration 16/1000 | Loss: 0.00001134
Iteration 17/1000 | Loss: 0.00001133
Iteration 18/1000 | Loss: 0.00001120
Iteration 19/1000 | Loss: 0.00001118
Iteration 20/1000 | Loss: 0.00001118
Iteration 21/1000 | Loss: 0.00001114
Iteration 22/1000 | Loss: 0.00001114
Iteration 23/1000 | Loss: 0.00001113
Iteration 24/1000 | Loss: 0.00001112
Iteration 25/1000 | Loss: 0.00001112
Iteration 26/1000 | Loss: 0.00001108
Iteration 27/1000 | Loss: 0.00001106
Iteration 28/1000 | Loss: 0.00001106
Iteration 29/1000 | Loss: 0.00001105
Iteration 30/1000 | Loss: 0.00001103
Iteration 31/1000 | Loss: 0.00001102
Iteration 32/1000 | Loss: 0.00001102
Iteration 33/1000 | Loss: 0.00001101
Iteration 34/1000 | Loss: 0.00001100
Iteration 35/1000 | Loss: 0.00001099
Iteration 36/1000 | Loss: 0.00001098
Iteration 37/1000 | Loss: 0.00001098
Iteration 38/1000 | Loss: 0.00001097
Iteration 39/1000 | Loss: 0.00001097
Iteration 40/1000 | Loss: 0.00001096
Iteration 41/1000 | Loss: 0.00001096
Iteration 42/1000 | Loss: 0.00001095
Iteration 43/1000 | Loss: 0.00001095
Iteration 44/1000 | Loss: 0.00001094
Iteration 45/1000 | Loss: 0.00001093
Iteration 46/1000 | Loss: 0.00001093
Iteration 47/1000 | Loss: 0.00001091
Iteration 48/1000 | Loss: 0.00001091
Iteration 49/1000 | Loss: 0.00001091
Iteration 50/1000 | Loss: 0.00001090
Iteration 51/1000 | Loss: 0.00001088
Iteration 52/1000 | Loss: 0.00001087
Iteration 53/1000 | Loss: 0.00001086
Iteration 54/1000 | Loss: 0.00001086
Iteration 55/1000 | Loss: 0.00001085
Iteration 56/1000 | Loss: 0.00001085
Iteration 57/1000 | Loss: 0.00001085
Iteration 58/1000 | Loss: 0.00001084
Iteration 59/1000 | Loss: 0.00001084
Iteration 60/1000 | Loss: 0.00001084
Iteration 61/1000 | Loss: 0.00001083
Iteration 62/1000 | Loss: 0.00001083
Iteration 63/1000 | Loss: 0.00001082
Iteration 64/1000 | Loss: 0.00001082
Iteration 65/1000 | Loss: 0.00001081
Iteration 66/1000 | Loss: 0.00001081
Iteration 67/1000 | Loss: 0.00001081
Iteration 68/1000 | Loss: 0.00001081
Iteration 69/1000 | Loss: 0.00001081
Iteration 70/1000 | Loss: 0.00001080
Iteration 71/1000 | Loss: 0.00001079
Iteration 72/1000 | Loss: 0.00001079
Iteration 73/1000 | Loss: 0.00001078
Iteration 74/1000 | Loss: 0.00001078
Iteration 75/1000 | Loss: 0.00001077
Iteration 76/1000 | Loss: 0.00001077
Iteration 77/1000 | Loss: 0.00001077
Iteration 78/1000 | Loss: 0.00001077
Iteration 79/1000 | Loss: 0.00001077
Iteration 80/1000 | Loss: 0.00001076
Iteration 81/1000 | Loss: 0.00001076
Iteration 82/1000 | Loss: 0.00001076
Iteration 83/1000 | Loss: 0.00001076
Iteration 84/1000 | Loss: 0.00001076
Iteration 85/1000 | Loss: 0.00001075
Iteration 86/1000 | Loss: 0.00001075
Iteration 87/1000 | Loss: 0.00001075
Iteration 88/1000 | Loss: 0.00001075
Iteration 89/1000 | Loss: 0.00001075
Iteration 90/1000 | Loss: 0.00001074
Iteration 91/1000 | Loss: 0.00001074
Iteration 92/1000 | Loss: 0.00001074
Iteration 93/1000 | Loss: 0.00001074
Iteration 94/1000 | Loss: 0.00001074
Iteration 95/1000 | Loss: 0.00001074
Iteration 96/1000 | Loss: 0.00001074
Iteration 97/1000 | Loss: 0.00001073
Iteration 98/1000 | Loss: 0.00001073
Iteration 99/1000 | Loss: 0.00001073
Iteration 100/1000 | Loss: 0.00001072
Iteration 101/1000 | Loss: 0.00001072
Iteration 102/1000 | Loss: 0.00001072
Iteration 103/1000 | Loss: 0.00001072
Iteration 104/1000 | Loss: 0.00001072
Iteration 105/1000 | Loss: 0.00001071
Iteration 106/1000 | Loss: 0.00001071
Iteration 107/1000 | Loss: 0.00001071
Iteration 108/1000 | Loss: 0.00001071
Iteration 109/1000 | Loss: 0.00001071
Iteration 110/1000 | Loss: 0.00001071
Iteration 111/1000 | Loss: 0.00001071
Iteration 112/1000 | Loss: 0.00001071
Iteration 113/1000 | Loss: 0.00001071
Iteration 114/1000 | Loss: 0.00001071
Iteration 115/1000 | Loss: 0.00001071
Iteration 116/1000 | Loss: 0.00001070
Iteration 117/1000 | Loss: 0.00001070
Iteration 118/1000 | Loss: 0.00001070
Iteration 119/1000 | Loss: 0.00001070
Iteration 120/1000 | Loss: 0.00001070
Iteration 121/1000 | Loss: 0.00001070
Iteration 122/1000 | Loss: 0.00001069
Iteration 123/1000 | Loss: 0.00001069
Iteration 124/1000 | Loss: 0.00001069
Iteration 125/1000 | Loss: 0.00001069
Iteration 126/1000 | Loss: 0.00001069
Iteration 127/1000 | Loss: 0.00001069
Iteration 128/1000 | Loss: 0.00001069
Iteration 129/1000 | Loss: 0.00001069
Iteration 130/1000 | Loss: 0.00001069
Iteration 131/1000 | Loss: 0.00001069
Iteration 132/1000 | Loss: 0.00001069
Iteration 133/1000 | Loss: 0.00001069
Iteration 134/1000 | Loss: 0.00001069
Iteration 135/1000 | Loss: 0.00001069
Iteration 136/1000 | Loss: 0.00001069
Iteration 137/1000 | Loss: 0.00001069
Iteration 138/1000 | Loss: 0.00001069
Iteration 139/1000 | Loss: 0.00001068
Iteration 140/1000 | Loss: 0.00001068
Iteration 141/1000 | Loss: 0.00001068
Iteration 142/1000 | Loss: 0.00001068
Iteration 143/1000 | Loss: 0.00001068
Iteration 144/1000 | Loss: 0.00001068
Iteration 145/1000 | Loss: 0.00001068
Iteration 146/1000 | Loss: 0.00001068
Iteration 147/1000 | Loss: 0.00001068
Iteration 148/1000 | Loss: 0.00001068
Iteration 149/1000 | Loss: 0.00001068
Iteration 150/1000 | Loss: 0.00001067
Iteration 151/1000 | Loss: 0.00001067
Iteration 152/1000 | Loss: 0.00001067
Iteration 153/1000 | Loss: 0.00001067
Iteration 154/1000 | Loss: 0.00001067
Iteration 155/1000 | Loss: 0.00001067
Iteration 156/1000 | Loss: 0.00001067
Iteration 157/1000 | Loss: 0.00001067
Iteration 158/1000 | Loss: 0.00001067
Iteration 159/1000 | Loss: 0.00001067
Iteration 160/1000 | Loss: 0.00001067
Iteration 161/1000 | Loss: 0.00001067
Iteration 162/1000 | Loss: 0.00001067
Iteration 163/1000 | Loss: 0.00001067
Iteration 164/1000 | Loss: 0.00001067
Iteration 165/1000 | Loss: 0.00001067
Iteration 166/1000 | Loss: 0.00001067
Iteration 167/1000 | Loss: 0.00001067
Iteration 168/1000 | Loss: 0.00001067
Iteration 169/1000 | Loss: 0.00001067
Iteration 170/1000 | Loss: 0.00001067
Iteration 171/1000 | Loss: 0.00001067
Iteration 172/1000 | Loss: 0.00001067
Iteration 173/1000 | Loss: 0.00001067
Iteration 174/1000 | Loss: 0.00001067
Iteration 175/1000 | Loss: 0.00001067
Iteration 176/1000 | Loss: 0.00001067
Iteration 177/1000 | Loss: 0.00001067
Iteration 178/1000 | Loss: 0.00001067
Iteration 179/1000 | Loss: 0.00001067
Iteration 180/1000 | Loss: 0.00001067
Iteration 181/1000 | Loss: 0.00001067
Iteration 182/1000 | Loss: 0.00001067
Iteration 183/1000 | Loss: 0.00001067
Iteration 184/1000 | Loss: 0.00001067
Iteration 185/1000 | Loss: 0.00001067
Iteration 186/1000 | Loss: 0.00001067
Iteration 187/1000 | Loss: 0.00001067
Iteration 188/1000 | Loss: 0.00001067
Iteration 189/1000 | Loss: 0.00001067
Iteration 190/1000 | Loss: 0.00001067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.0667777132766787e-05, 1.0667777132766787e-05, 1.0667777132766787e-05, 1.0667777132766787e-05, 1.0667777132766787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0667777132766787e-05

Optimization complete. Final v2v error: 2.8230180740356445 mm

Highest mean error: 3.3720126152038574 mm for frame 74

Lowest mean error: 2.576350688934326 mm for frame 95

Saving results

Total time: 40.80509948730469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054642
Iteration 2/25 | Loss: 0.00471019
Iteration 3/25 | Loss: 0.00519965
Iteration 4/25 | Loss: 0.00247224
Iteration 5/25 | Loss: 0.00238313
Iteration 6/25 | Loss: 0.00209468
Iteration 7/25 | Loss: 0.00196291
Iteration 8/25 | Loss: 0.00188467
Iteration 9/25 | Loss: 0.00178787
Iteration 10/25 | Loss: 0.00173227
Iteration 11/25 | Loss: 0.00170604
Iteration 12/25 | Loss: 0.00159987
Iteration 13/25 | Loss: 0.00152898
Iteration 14/25 | Loss: 0.00151585
Iteration 15/25 | Loss: 0.00148381
Iteration 16/25 | Loss: 0.00148551
Iteration 17/25 | Loss: 0.00148039
Iteration 18/25 | Loss: 0.00147733
Iteration 19/25 | Loss: 0.00145973
Iteration 20/25 | Loss: 0.00144796
Iteration 21/25 | Loss: 0.00144312
Iteration 22/25 | Loss: 0.00145252
Iteration 23/25 | Loss: 0.00143657
Iteration 24/25 | Loss: 0.00142635
Iteration 25/25 | Loss: 0.00142408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54397440
Iteration 2/25 | Loss: 0.00258453
Iteration 3/25 | Loss: 0.00258453
Iteration 4/25 | Loss: 0.00258453
Iteration 5/25 | Loss: 0.00258453
Iteration 6/25 | Loss: 0.00258453
Iteration 7/25 | Loss: 0.00258453
Iteration 8/25 | Loss: 0.00258453
Iteration 9/25 | Loss: 0.00258452
Iteration 10/25 | Loss: 0.00258452
Iteration 11/25 | Loss: 0.00258452
Iteration 12/25 | Loss: 0.00258452
Iteration 13/25 | Loss: 0.00258452
Iteration 14/25 | Loss: 0.00258452
Iteration 15/25 | Loss: 0.00258452
Iteration 16/25 | Loss: 0.00258452
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002584523754194379, 0.002584523754194379, 0.002584523754194379, 0.002584523754194379, 0.002584523754194379]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002584523754194379

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00258452
Iteration 2/1000 | Loss: 0.00023934
Iteration 3/1000 | Loss: 0.00037311
Iteration 4/1000 | Loss: 0.00016843
Iteration 5/1000 | Loss: 0.00013562
Iteration 6/1000 | Loss: 0.00012279
Iteration 7/1000 | Loss: 0.00027696
Iteration 8/1000 | Loss: 0.00019795
Iteration 9/1000 | Loss: 0.00026735
Iteration 10/1000 | Loss: 0.00017674
Iteration 11/1000 | Loss: 0.00016199
Iteration 12/1000 | Loss: 0.00012957
Iteration 13/1000 | Loss: 0.00011266
Iteration 14/1000 | Loss: 0.00010371
Iteration 15/1000 | Loss: 0.00009944
Iteration 16/1000 | Loss: 0.00009669
Iteration 17/1000 | Loss: 0.00009374
Iteration 18/1000 | Loss: 0.00009185
Iteration 19/1000 | Loss: 0.00025687
Iteration 20/1000 | Loss: 0.00009519
Iteration 21/1000 | Loss: 0.00008952
Iteration 22/1000 | Loss: 0.00008809
Iteration 23/1000 | Loss: 0.00008725
Iteration 24/1000 | Loss: 0.00008662
Iteration 25/1000 | Loss: 0.00008624
Iteration 26/1000 | Loss: 0.00008584
Iteration 27/1000 | Loss: 0.00008550
Iteration 28/1000 | Loss: 0.00008518
Iteration 29/1000 | Loss: 0.00008492
Iteration 30/1000 | Loss: 0.00016603
Iteration 31/1000 | Loss: 0.00008732
Iteration 32/1000 | Loss: 0.00008515
Iteration 33/1000 | Loss: 0.00008402
Iteration 34/1000 | Loss: 0.00008265
Iteration 35/1000 | Loss: 0.00008134
Iteration 36/1000 | Loss: 0.00008080
Iteration 37/1000 | Loss: 0.00008009
Iteration 38/1000 | Loss: 0.00012320
Iteration 39/1000 | Loss: 0.00014447
Iteration 40/1000 | Loss: 0.00008092
Iteration 41/1000 | Loss: 0.00007709
Iteration 42/1000 | Loss: 0.00007511
Iteration 43/1000 | Loss: 0.00007225
Iteration 44/1000 | Loss: 0.00007068
Iteration 45/1000 | Loss: 0.00006928
Iteration 46/1000 | Loss: 0.00006822
Iteration 47/1000 | Loss: 0.00006722
Iteration 48/1000 | Loss: 0.00015369
Iteration 49/1000 | Loss: 0.00058705
Iteration 50/1000 | Loss: 0.00075066
Iteration 51/1000 | Loss: 0.00009966
Iteration 52/1000 | Loss: 0.00007749
Iteration 53/1000 | Loss: 0.00006533
Iteration 54/1000 | Loss: 0.00005131
Iteration 55/1000 | Loss: 0.00004246
Iteration 56/1000 | Loss: 0.00003958
Iteration 57/1000 | Loss: 0.00003760
Iteration 58/1000 | Loss: 0.00003565
Iteration 59/1000 | Loss: 0.00003439
Iteration 60/1000 | Loss: 0.00003361
Iteration 61/1000 | Loss: 0.00003286
Iteration 62/1000 | Loss: 0.00003239
Iteration 63/1000 | Loss: 0.00003216
Iteration 64/1000 | Loss: 0.00003201
Iteration 65/1000 | Loss: 0.00003193
Iteration 66/1000 | Loss: 0.00003191
Iteration 67/1000 | Loss: 0.00003185
Iteration 68/1000 | Loss: 0.00003181
Iteration 69/1000 | Loss: 0.00003181
Iteration 70/1000 | Loss: 0.00003179
Iteration 71/1000 | Loss: 0.00003178
Iteration 72/1000 | Loss: 0.00003178
Iteration 73/1000 | Loss: 0.00003177
Iteration 74/1000 | Loss: 0.00003176
Iteration 75/1000 | Loss: 0.00003175
Iteration 76/1000 | Loss: 0.00003169
Iteration 77/1000 | Loss: 0.00003169
Iteration 78/1000 | Loss: 0.00003169
Iteration 79/1000 | Loss: 0.00003169
Iteration 80/1000 | Loss: 0.00003169
Iteration 81/1000 | Loss: 0.00003168
Iteration 82/1000 | Loss: 0.00003168
Iteration 83/1000 | Loss: 0.00003168
Iteration 84/1000 | Loss: 0.00003168
Iteration 85/1000 | Loss: 0.00003168
Iteration 86/1000 | Loss: 0.00003168
Iteration 87/1000 | Loss: 0.00003168
Iteration 88/1000 | Loss: 0.00003168
Iteration 89/1000 | Loss: 0.00003168
Iteration 90/1000 | Loss: 0.00003168
Iteration 91/1000 | Loss: 0.00003168
Iteration 92/1000 | Loss: 0.00003168
Iteration 93/1000 | Loss: 0.00003167
Iteration 94/1000 | Loss: 0.00003167
Iteration 95/1000 | Loss: 0.00003167
Iteration 96/1000 | Loss: 0.00003167
Iteration 97/1000 | Loss: 0.00003167
Iteration 98/1000 | Loss: 0.00003167
Iteration 99/1000 | Loss: 0.00003167
Iteration 100/1000 | Loss: 0.00003167
Iteration 101/1000 | Loss: 0.00003167
Iteration 102/1000 | Loss: 0.00003166
Iteration 103/1000 | Loss: 0.00003166
Iteration 104/1000 | Loss: 0.00003166
Iteration 105/1000 | Loss: 0.00003166
Iteration 106/1000 | Loss: 0.00003166
Iteration 107/1000 | Loss: 0.00003166
Iteration 108/1000 | Loss: 0.00003166
Iteration 109/1000 | Loss: 0.00003166
Iteration 110/1000 | Loss: 0.00003166
Iteration 111/1000 | Loss: 0.00003166
Iteration 112/1000 | Loss: 0.00003166
Iteration 113/1000 | Loss: 0.00003166
Iteration 114/1000 | Loss: 0.00003166
Iteration 115/1000 | Loss: 0.00003166
Iteration 116/1000 | Loss: 0.00003166
Iteration 117/1000 | Loss: 0.00003166
Iteration 118/1000 | Loss: 0.00003165
Iteration 119/1000 | Loss: 0.00003165
Iteration 120/1000 | Loss: 0.00003165
Iteration 121/1000 | Loss: 0.00003164
Iteration 122/1000 | Loss: 0.00003164
Iteration 123/1000 | Loss: 0.00003164
Iteration 124/1000 | Loss: 0.00003164
Iteration 125/1000 | Loss: 0.00003164
Iteration 126/1000 | Loss: 0.00003164
Iteration 127/1000 | Loss: 0.00003164
Iteration 128/1000 | Loss: 0.00003164
Iteration 129/1000 | Loss: 0.00003164
Iteration 130/1000 | Loss: 0.00003164
Iteration 131/1000 | Loss: 0.00003163
Iteration 132/1000 | Loss: 0.00003163
Iteration 133/1000 | Loss: 0.00003163
Iteration 134/1000 | Loss: 0.00003163
Iteration 135/1000 | Loss: 0.00003163
Iteration 136/1000 | Loss: 0.00003162
Iteration 137/1000 | Loss: 0.00003162
Iteration 138/1000 | Loss: 0.00003162
Iteration 139/1000 | Loss: 0.00003162
Iteration 140/1000 | Loss: 0.00003162
Iteration 141/1000 | Loss: 0.00003162
Iteration 142/1000 | Loss: 0.00003162
Iteration 143/1000 | Loss: 0.00003162
Iteration 144/1000 | Loss: 0.00003162
Iteration 145/1000 | Loss: 0.00003161
Iteration 146/1000 | Loss: 0.00003161
Iteration 147/1000 | Loss: 0.00003161
Iteration 148/1000 | Loss: 0.00003161
Iteration 149/1000 | Loss: 0.00003161
Iteration 150/1000 | Loss: 0.00003161
Iteration 151/1000 | Loss: 0.00003161
Iteration 152/1000 | Loss: 0.00003161
Iteration 153/1000 | Loss: 0.00003161
Iteration 154/1000 | Loss: 0.00003161
Iteration 155/1000 | Loss: 0.00003161
Iteration 156/1000 | Loss: 0.00003161
Iteration 157/1000 | Loss: 0.00003161
Iteration 158/1000 | Loss: 0.00003160
Iteration 159/1000 | Loss: 0.00003160
Iteration 160/1000 | Loss: 0.00003160
Iteration 161/1000 | Loss: 0.00003160
Iteration 162/1000 | Loss: 0.00003160
Iteration 163/1000 | Loss: 0.00003160
Iteration 164/1000 | Loss: 0.00003159
Iteration 165/1000 | Loss: 0.00003159
Iteration 166/1000 | Loss: 0.00003159
Iteration 167/1000 | Loss: 0.00003159
Iteration 168/1000 | Loss: 0.00003159
Iteration 169/1000 | Loss: 0.00003159
Iteration 170/1000 | Loss: 0.00003159
Iteration 171/1000 | Loss: 0.00003159
Iteration 172/1000 | Loss: 0.00003159
Iteration 173/1000 | Loss: 0.00003159
Iteration 174/1000 | Loss: 0.00003159
Iteration 175/1000 | Loss: 0.00003159
Iteration 176/1000 | Loss: 0.00003159
Iteration 177/1000 | Loss: 0.00003159
Iteration 178/1000 | Loss: 0.00003159
Iteration 179/1000 | Loss: 0.00003159
Iteration 180/1000 | Loss: 0.00003159
Iteration 181/1000 | Loss: 0.00003159
Iteration 182/1000 | Loss: 0.00003158
Iteration 183/1000 | Loss: 0.00003158
Iteration 184/1000 | Loss: 0.00003158
Iteration 185/1000 | Loss: 0.00003158
Iteration 186/1000 | Loss: 0.00003158
Iteration 187/1000 | Loss: 0.00003158
Iteration 188/1000 | Loss: 0.00003158
Iteration 189/1000 | Loss: 0.00003158
Iteration 190/1000 | Loss: 0.00003158
Iteration 191/1000 | Loss: 0.00003158
Iteration 192/1000 | Loss: 0.00003158
Iteration 193/1000 | Loss: 0.00003158
Iteration 194/1000 | Loss: 0.00003158
Iteration 195/1000 | Loss: 0.00003157
Iteration 196/1000 | Loss: 0.00003157
Iteration 197/1000 | Loss: 0.00003157
Iteration 198/1000 | Loss: 0.00003157
Iteration 199/1000 | Loss: 0.00003157
Iteration 200/1000 | Loss: 0.00003157
Iteration 201/1000 | Loss: 0.00003157
Iteration 202/1000 | Loss: 0.00003157
Iteration 203/1000 | Loss: 0.00003157
Iteration 204/1000 | Loss: 0.00003157
Iteration 205/1000 | Loss: 0.00003157
Iteration 206/1000 | Loss: 0.00003157
Iteration 207/1000 | Loss: 0.00003157
Iteration 208/1000 | Loss: 0.00003157
Iteration 209/1000 | Loss: 0.00003156
Iteration 210/1000 | Loss: 0.00003156
Iteration 211/1000 | Loss: 0.00003156
Iteration 212/1000 | Loss: 0.00003156
Iteration 213/1000 | Loss: 0.00003156
Iteration 214/1000 | Loss: 0.00003156
Iteration 215/1000 | Loss: 0.00003156
Iteration 216/1000 | Loss: 0.00003156
Iteration 217/1000 | Loss: 0.00003156
Iteration 218/1000 | Loss: 0.00003155
Iteration 219/1000 | Loss: 0.00003155
Iteration 220/1000 | Loss: 0.00003155
Iteration 221/1000 | Loss: 0.00003155
Iteration 222/1000 | Loss: 0.00003155
Iteration 223/1000 | Loss: 0.00003155
Iteration 224/1000 | Loss: 0.00003155
Iteration 225/1000 | Loss: 0.00003155
Iteration 226/1000 | Loss: 0.00003155
Iteration 227/1000 | Loss: 0.00003155
Iteration 228/1000 | Loss: 0.00003155
Iteration 229/1000 | Loss: 0.00003155
Iteration 230/1000 | Loss: 0.00003155
Iteration 231/1000 | Loss: 0.00003155
Iteration 232/1000 | Loss: 0.00003155
Iteration 233/1000 | Loss: 0.00003154
Iteration 234/1000 | Loss: 0.00003154
Iteration 235/1000 | Loss: 0.00003154
Iteration 236/1000 | Loss: 0.00003154
Iteration 237/1000 | Loss: 0.00003154
Iteration 238/1000 | Loss: 0.00003154
Iteration 239/1000 | Loss: 0.00003154
Iteration 240/1000 | Loss: 0.00003154
Iteration 241/1000 | Loss: 0.00003154
Iteration 242/1000 | Loss: 0.00003154
Iteration 243/1000 | Loss: 0.00003154
Iteration 244/1000 | Loss: 0.00003154
Iteration 245/1000 | Loss: 0.00003154
Iteration 246/1000 | Loss: 0.00003154
Iteration 247/1000 | Loss: 0.00003154
Iteration 248/1000 | Loss: 0.00003154
Iteration 249/1000 | Loss: 0.00003153
Iteration 250/1000 | Loss: 0.00003153
Iteration 251/1000 | Loss: 0.00003153
Iteration 252/1000 | Loss: 0.00003153
Iteration 253/1000 | Loss: 0.00003153
Iteration 254/1000 | Loss: 0.00003153
Iteration 255/1000 | Loss: 0.00003153
Iteration 256/1000 | Loss: 0.00003153
Iteration 257/1000 | Loss: 0.00003153
Iteration 258/1000 | Loss: 0.00003153
Iteration 259/1000 | Loss: 0.00003153
Iteration 260/1000 | Loss: 0.00003153
Iteration 261/1000 | Loss: 0.00003153
Iteration 262/1000 | Loss: 0.00003153
Iteration 263/1000 | Loss: 0.00003153
Iteration 264/1000 | Loss: 0.00003153
Iteration 265/1000 | Loss: 0.00003153
Iteration 266/1000 | Loss: 0.00003153
Iteration 267/1000 | Loss: 0.00003153
Iteration 268/1000 | Loss: 0.00003153
Iteration 269/1000 | Loss: 0.00003153
Iteration 270/1000 | Loss: 0.00003153
Iteration 271/1000 | Loss: 0.00003153
Iteration 272/1000 | Loss: 0.00003153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [3.1531726563116536e-05, 3.1531726563116536e-05, 3.1531726563116536e-05, 3.1531726563116536e-05, 3.1531726563116536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1531726563116536e-05

Optimization complete. Final v2v error: 4.082164287567139 mm

Highest mean error: 11.347782135009766 mm for frame 81

Lowest mean error: 3.664801836013794 mm for frame 55

Saving results

Total time: 152.81316828727722
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960934
Iteration 2/25 | Loss: 0.00164425
Iteration 3/25 | Loss: 0.00124283
Iteration 4/25 | Loss: 0.00120639
Iteration 5/25 | Loss: 0.00119619
Iteration 6/25 | Loss: 0.00118755
Iteration 7/25 | Loss: 0.00118708
Iteration 8/25 | Loss: 0.00118505
Iteration 9/25 | Loss: 0.00118436
Iteration 10/25 | Loss: 0.00118407
Iteration 11/25 | Loss: 0.00118386
Iteration 12/25 | Loss: 0.00118471
Iteration 13/25 | Loss: 0.00118587
Iteration 14/25 | Loss: 0.00118604
Iteration 15/25 | Loss: 0.00118281
Iteration 16/25 | Loss: 0.00118272
Iteration 17/25 | Loss: 0.00118272
Iteration 18/25 | Loss: 0.00118267
Iteration 19/25 | Loss: 0.00118266
Iteration 20/25 | Loss: 0.00118266
Iteration 21/25 | Loss: 0.00118266
Iteration 22/25 | Loss: 0.00118266
Iteration 23/25 | Loss: 0.00118266
Iteration 24/25 | Loss: 0.00118265
Iteration 25/25 | Loss: 0.00118265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.79332590
Iteration 2/25 | Loss: 0.00159398
Iteration 3/25 | Loss: 0.00157418
Iteration 4/25 | Loss: 0.00157418
Iteration 5/25 | Loss: 0.00157418
Iteration 6/25 | Loss: 0.00157418
Iteration 7/25 | Loss: 0.00157418
Iteration 8/25 | Loss: 0.00157418
Iteration 9/25 | Loss: 0.00157418
Iteration 10/25 | Loss: 0.00157418
Iteration 11/25 | Loss: 0.00157418
Iteration 12/25 | Loss: 0.00157418
Iteration 13/25 | Loss: 0.00157418
Iteration 14/25 | Loss: 0.00157418
Iteration 15/25 | Loss: 0.00157418
Iteration 16/25 | Loss: 0.00157418
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015741775278002024, 0.0015741775278002024, 0.0015741775278002024, 0.0015741775278002024, 0.0015741775278002024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015741775278002024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157418
Iteration 2/1000 | Loss: 0.00003422
Iteration 3/1000 | Loss: 0.00002375
Iteration 4/1000 | Loss: 0.00001965
Iteration 5/1000 | Loss: 0.00001828
Iteration 6/1000 | Loss: 0.00001715
Iteration 7/1000 | Loss: 0.00001639
Iteration 8/1000 | Loss: 0.00001591
Iteration 9/1000 | Loss: 0.00001562
Iteration 10/1000 | Loss: 0.00001529
Iteration 11/1000 | Loss: 0.00001507
Iteration 12/1000 | Loss: 0.00001492
Iteration 13/1000 | Loss: 0.00001492
Iteration 14/1000 | Loss: 0.00001490
Iteration 15/1000 | Loss: 0.00001482
Iteration 16/1000 | Loss: 0.00001472
Iteration 17/1000 | Loss: 0.00001470
Iteration 18/1000 | Loss: 0.00001469
Iteration 19/1000 | Loss: 0.00001465
Iteration 20/1000 | Loss: 0.00001463
Iteration 21/1000 | Loss: 0.00001460
Iteration 22/1000 | Loss: 0.00001460
Iteration 23/1000 | Loss: 0.00001458
Iteration 24/1000 | Loss: 0.00001457
Iteration 25/1000 | Loss: 0.00001453
Iteration 26/1000 | Loss: 0.00001453
Iteration 27/1000 | Loss: 0.00001450
Iteration 28/1000 | Loss: 0.00001450
Iteration 29/1000 | Loss: 0.00001449
Iteration 30/1000 | Loss: 0.00001449
Iteration 31/1000 | Loss: 0.00001445
Iteration 32/1000 | Loss: 0.00001445
Iteration 33/1000 | Loss: 0.00001441
Iteration 34/1000 | Loss: 0.00001441
Iteration 35/1000 | Loss: 0.00001440
Iteration 36/1000 | Loss: 0.00001440
Iteration 37/1000 | Loss: 0.00001439
Iteration 38/1000 | Loss: 0.00001438
Iteration 39/1000 | Loss: 0.00001438
Iteration 40/1000 | Loss: 0.00001437
Iteration 41/1000 | Loss: 0.00001437
Iteration 42/1000 | Loss: 0.00001437
Iteration 43/1000 | Loss: 0.00001436
Iteration 44/1000 | Loss: 0.00001436
Iteration 45/1000 | Loss: 0.00001436
Iteration 46/1000 | Loss: 0.00001435
Iteration 47/1000 | Loss: 0.00001434
Iteration 48/1000 | Loss: 0.00001434
Iteration 49/1000 | Loss: 0.00001434
Iteration 50/1000 | Loss: 0.00001434
Iteration 51/1000 | Loss: 0.00001434
Iteration 52/1000 | Loss: 0.00001433
Iteration 53/1000 | Loss: 0.00001433
Iteration 54/1000 | Loss: 0.00001433
Iteration 55/1000 | Loss: 0.00001432
Iteration 56/1000 | Loss: 0.00001432
Iteration 57/1000 | Loss: 0.00001432
Iteration 58/1000 | Loss: 0.00001432
Iteration 59/1000 | Loss: 0.00001432
Iteration 60/1000 | Loss: 0.00001432
Iteration 61/1000 | Loss: 0.00001431
Iteration 62/1000 | Loss: 0.00001431
Iteration 63/1000 | Loss: 0.00001431
Iteration 64/1000 | Loss: 0.00001431
Iteration 65/1000 | Loss: 0.00001431
Iteration 66/1000 | Loss: 0.00001431
Iteration 67/1000 | Loss: 0.00001430
Iteration 68/1000 | Loss: 0.00001430
Iteration 69/1000 | Loss: 0.00001430
Iteration 70/1000 | Loss: 0.00001430
Iteration 71/1000 | Loss: 0.00001430
Iteration 72/1000 | Loss: 0.00001430
Iteration 73/1000 | Loss: 0.00001430
Iteration 74/1000 | Loss: 0.00001430
Iteration 75/1000 | Loss: 0.00001430
Iteration 76/1000 | Loss: 0.00001430
Iteration 77/1000 | Loss: 0.00001430
Iteration 78/1000 | Loss: 0.00001429
Iteration 79/1000 | Loss: 0.00001429
Iteration 80/1000 | Loss: 0.00001429
Iteration 81/1000 | Loss: 0.00001429
Iteration 82/1000 | Loss: 0.00001429
Iteration 83/1000 | Loss: 0.00001429
Iteration 84/1000 | Loss: 0.00001429
Iteration 85/1000 | Loss: 0.00001428
Iteration 86/1000 | Loss: 0.00001428
Iteration 87/1000 | Loss: 0.00001428
Iteration 88/1000 | Loss: 0.00001428
Iteration 89/1000 | Loss: 0.00001428
Iteration 90/1000 | Loss: 0.00001428
Iteration 91/1000 | Loss: 0.00001428
Iteration 92/1000 | Loss: 0.00001428
Iteration 93/1000 | Loss: 0.00001428
Iteration 94/1000 | Loss: 0.00001428
Iteration 95/1000 | Loss: 0.00001428
Iteration 96/1000 | Loss: 0.00001428
Iteration 97/1000 | Loss: 0.00001428
Iteration 98/1000 | Loss: 0.00001428
Iteration 99/1000 | Loss: 0.00001428
Iteration 100/1000 | Loss: 0.00001428
Iteration 101/1000 | Loss: 0.00001428
Iteration 102/1000 | Loss: 0.00001428
Iteration 103/1000 | Loss: 0.00001428
Iteration 104/1000 | Loss: 0.00001428
Iteration 105/1000 | Loss: 0.00001428
Iteration 106/1000 | Loss: 0.00001428
Iteration 107/1000 | Loss: 0.00001428
Iteration 108/1000 | Loss: 0.00001428
Iteration 109/1000 | Loss: 0.00001428
Iteration 110/1000 | Loss: 0.00001428
Iteration 111/1000 | Loss: 0.00001428
Iteration 112/1000 | Loss: 0.00001428
Iteration 113/1000 | Loss: 0.00001428
Iteration 114/1000 | Loss: 0.00001428
Iteration 115/1000 | Loss: 0.00001428
Iteration 116/1000 | Loss: 0.00001428
Iteration 117/1000 | Loss: 0.00001428
Iteration 118/1000 | Loss: 0.00001428
Iteration 119/1000 | Loss: 0.00001428
Iteration 120/1000 | Loss: 0.00001428
Iteration 121/1000 | Loss: 0.00001428
Iteration 122/1000 | Loss: 0.00001428
Iteration 123/1000 | Loss: 0.00001428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.4282207303040195e-05, 1.4282207303040195e-05, 1.4282207303040195e-05, 1.4282207303040195e-05, 1.4282207303040195e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4282207303040195e-05

Optimization complete. Final v2v error: 3.2052273750305176 mm

Highest mean error: 4.181082248687744 mm for frame 239

Lowest mean error: 2.671505928039551 mm for frame 134

Saving results

Total time: 64.3218822479248
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531697
Iteration 2/25 | Loss: 0.00125276
Iteration 3/25 | Loss: 0.00118153
Iteration 4/25 | Loss: 0.00117005
Iteration 5/25 | Loss: 0.00116600
Iteration 6/25 | Loss: 0.00116548
Iteration 7/25 | Loss: 0.00116548
Iteration 8/25 | Loss: 0.00116548
Iteration 9/25 | Loss: 0.00116548
Iteration 10/25 | Loss: 0.00116548
Iteration 11/25 | Loss: 0.00116548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001165481866337359, 0.001165481866337359, 0.001165481866337359, 0.001165481866337359, 0.001165481866337359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001165481866337359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56512070
Iteration 2/25 | Loss: 0.00134266
Iteration 3/25 | Loss: 0.00134266
Iteration 4/25 | Loss: 0.00134266
Iteration 5/25 | Loss: 0.00134265
Iteration 6/25 | Loss: 0.00134265
Iteration 7/25 | Loss: 0.00134265
Iteration 8/25 | Loss: 0.00134265
Iteration 9/25 | Loss: 0.00134265
Iteration 10/25 | Loss: 0.00134265
Iteration 11/25 | Loss: 0.00134265
Iteration 12/25 | Loss: 0.00134265
Iteration 13/25 | Loss: 0.00134265
Iteration 14/25 | Loss: 0.00134265
Iteration 15/25 | Loss: 0.00134265
Iteration 16/25 | Loss: 0.00134265
Iteration 17/25 | Loss: 0.00134265
Iteration 18/25 | Loss: 0.00134265
Iteration 19/25 | Loss: 0.00134265
Iteration 20/25 | Loss: 0.00134265
Iteration 21/25 | Loss: 0.00134265
Iteration 22/25 | Loss: 0.00134265
Iteration 23/25 | Loss: 0.00134265
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001342653064057231, 0.001342653064057231, 0.001342653064057231, 0.001342653064057231, 0.001342653064057231]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001342653064057231

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134265
Iteration 2/1000 | Loss: 0.00002215
Iteration 3/1000 | Loss: 0.00001650
Iteration 4/1000 | Loss: 0.00001463
Iteration 5/1000 | Loss: 0.00001360
Iteration 6/1000 | Loss: 0.00001302
Iteration 7/1000 | Loss: 0.00001248
Iteration 8/1000 | Loss: 0.00001203
Iteration 9/1000 | Loss: 0.00001173
Iteration 10/1000 | Loss: 0.00001134
Iteration 11/1000 | Loss: 0.00001117
Iteration 12/1000 | Loss: 0.00001115
Iteration 13/1000 | Loss: 0.00001103
Iteration 14/1000 | Loss: 0.00001086
Iteration 15/1000 | Loss: 0.00001074
Iteration 16/1000 | Loss: 0.00001067
Iteration 17/1000 | Loss: 0.00001065
Iteration 18/1000 | Loss: 0.00001063
Iteration 19/1000 | Loss: 0.00001062
Iteration 20/1000 | Loss: 0.00001061
Iteration 21/1000 | Loss: 0.00001061
Iteration 22/1000 | Loss: 0.00001060
Iteration 23/1000 | Loss: 0.00001060
Iteration 24/1000 | Loss: 0.00001059
Iteration 25/1000 | Loss: 0.00001057
Iteration 26/1000 | Loss: 0.00001057
Iteration 27/1000 | Loss: 0.00001054
Iteration 28/1000 | Loss: 0.00001051
Iteration 29/1000 | Loss: 0.00001046
Iteration 30/1000 | Loss: 0.00001039
Iteration 31/1000 | Loss: 0.00001036
Iteration 32/1000 | Loss: 0.00001036
Iteration 33/1000 | Loss: 0.00001035
Iteration 34/1000 | Loss: 0.00001035
Iteration 35/1000 | Loss: 0.00001033
Iteration 36/1000 | Loss: 0.00001032
Iteration 37/1000 | Loss: 0.00001032
Iteration 38/1000 | Loss: 0.00001031
Iteration 39/1000 | Loss: 0.00001031
Iteration 40/1000 | Loss: 0.00001030
Iteration 41/1000 | Loss: 0.00001029
Iteration 42/1000 | Loss: 0.00001029
Iteration 43/1000 | Loss: 0.00001029
Iteration 44/1000 | Loss: 0.00001029
Iteration 45/1000 | Loss: 0.00001029
Iteration 46/1000 | Loss: 0.00001029
Iteration 47/1000 | Loss: 0.00001028
Iteration 48/1000 | Loss: 0.00001028
Iteration 49/1000 | Loss: 0.00001028
Iteration 50/1000 | Loss: 0.00001025
Iteration 51/1000 | Loss: 0.00001025
Iteration 52/1000 | Loss: 0.00001025
Iteration 53/1000 | Loss: 0.00001024
Iteration 54/1000 | Loss: 0.00001024
Iteration 55/1000 | Loss: 0.00001024
Iteration 56/1000 | Loss: 0.00001023
Iteration 57/1000 | Loss: 0.00001023
Iteration 58/1000 | Loss: 0.00001023
Iteration 59/1000 | Loss: 0.00001023
Iteration 60/1000 | Loss: 0.00001022
Iteration 61/1000 | Loss: 0.00001022
Iteration 62/1000 | Loss: 0.00001022
Iteration 63/1000 | Loss: 0.00001021
Iteration 64/1000 | Loss: 0.00001021
Iteration 65/1000 | Loss: 0.00001021
Iteration 66/1000 | Loss: 0.00001021
Iteration 67/1000 | Loss: 0.00001021
Iteration 68/1000 | Loss: 0.00001021
Iteration 69/1000 | Loss: 0.00001021
Iteration 70/1000 | Loss: 0.00001020
Iteration 71/1000 | Loss: 0.00001020
Iteration 72/1000 | Loss: 0.00001020
Iteration 73/1000 | Loss: 0.00001020
Iteration 74/1000 | Loss: 0.00001019
Iteration 75/1000 | Loss: 0.00001019
Iteration 76/1000 | Loss: 0.00001019
Iteration 77/1000 | Loss: 0.00001018
Iteration 78/1000 | Loss: 0.00001018
Iteration 79/1000 | Loss: 0.00001018
Iteration 80/1000 | Loss: 0.00001018
Iteration 81/1000 | Loss: 0.00001018
Iteration 82/1000 | Loss: 0.00001017
Iteration 83/1000 | Loss: 0.00001017
Iteration 84/1000 | Loss: 0.00001017
Iteration 85/1000 | Loss: 0.00001017
Iteration 86/1000 | Loss: 0.00001017
Iteration 87/1000 | Loss: 0.00001016
Iteration 88/1000 | Loss: 0.00001016
Iteration 89/1000 | Loss: 0.00001016
Iteration 90/1000 | Loss: 0.00001016
Iteration 91/1000 | Loss: 0.00001015
Iteration 92/1000 | Loss: 0.00001015
Iteration 93/1000 | Loss: 0.00001015
Iteration 94/1000 | Loss: 0.00001015
Iteration 95/1000 | Loss: 0.00001015
Iteration 96/1000 | Loss: 0.00001015
Iteration 97/1000 | Loss: 0.00001014
Iteration 98/1000 | Loss: 0.00001014
Iteration 99/1000 | Loss: 0.00001014
Iteration 100/1000 | Loss: 0.00001014
Iteration 101/1000 | Loss: 0.00001013
Iteration 102/1000 | Loss: 0.00001013
Iteration 103/1000 | Loss: 0.00001012
Iteration 104/1000 | Loss: 0.00001012
Iteration 105/1000 | Loss: 0.00001011
Iteration 106/1000 | Loss: 0.00001011
Iteration 107/1000 | Loss: 0.00001011
Iteration 108/1000 | Loss: 0.00001011
Iteration 109/1000 | Loss: 0.00001011
Iteration 110/1000 | Loss: 0.00001010
Iteration 111/1000 | Loss: 0.00001010
Iteration 112/1000 | Loss: 0.00001010
Iteration 113/1000 | Loss: 0.00001010
Iteration 114/1000 | Loss: 0.00001010
Iteration 115/1000 | Loss: 0.00001009
Iteration 116/1000 | Loss: 0.00001009
Iteration 117/1000 | Loss: 0.00001008
Iteration 118/1000 | Loss: 0.00001008
Iteration 119/1000 | Loss: 0.00001008
Iteration 120/1000 | Loss: 0.00001008
Iteration 121/1000 | Loss: 0.00001008
Iteration 122/1000 | Loss: 0.00001008
Iteration 123/1000 | Loss: 0.00001008
Iteration 124/1000 | Loss: 0.00001008
Iteration 125/1000 | Loss: 0.00001008
Iteration 126/1000 | Loss: 0.00001007
Iteration 127/1000 | Loss: 0.00001007
Iteration 128/1000 | Loss: 0.00001007
Iteration 129/1000 | Loss: 0.00001007
Iteration 130/1000 | Loss: 0.00001007
Iteration 131/1000 | Loss: 0.00001007
Iteration 132/1000 | Loss: 0.00001007
Iteration 133/1000 | Loss: 0.00001007
Iteration 134/1000 | Loss: 0.00001006
Iteration 135/1000 | Loss: 0.00001006
Iteration 136/1000 | Loss: 0.00001006
Iteration 137/1000 | Loss: 0.00001006
Iteration 138/1000 | Loss: 0.00001006
Iteration 139/1000 | Loss: 0.00001006
Iteration 140/1000 | Loss: 0.00001005
Iteration 141/1000 | Loss: 0.00001005
Iteration 142/1000 | Loss: 0.00001005
Iteration 143/1000 | Loss: 0.00001005
Iteration 144/1000 | Loss: 0.00001005
Iteration 145/1000 | Loss: 0.00001005
Iteration 146/1000 | Loss: 0.00001004
Iteration 147/1000 | Loss: 0.00001004
Iteration 148/1000 | Loss: 0.00001004
Iteration 149/1000 | Loss: 0.00001004
Iteration 150/1000 | Loss: 0.00001004
Iteration 151/1000 | Loss: 0.00001004
Iteration 152/1000 | Loss: 0.00001004
Iteration 153/1000 | Loss: 0.00001004
Iteration 154/1000 | Loss: 0.00001003
Iteration 155/1000 | Loss: 0.00001003
Iteration 156/1000 | Loss: 0.00001003
Iteration 157/1000 | Loss: 0.00001003
Iteration 158/1000 | Loss: 0.00001003
Iteration 159/1000 | Loss: 0.00001003
Iteration 160/1000 | Loss: 0.00001003
Iteration 161/1000 | Loss: 0.00001002
Iteration 162/1000 | Loss: 0.00001002
Iteration 163/1000 | Loss: 0.00001002
Iteration 164/1000 | Loss: 0.00001002
Iteration 165/1000 | Loss: 0.00001002
Iteration 166/1000 | Loss: 0.00001002
Iteration 167/1000 | Loss: 0.00001002
Iteration 168/1000 | Loss: 0.00001001
Iteration 169/1000 | Loss: 0.00001001
Iteration 170/1000 | Loss: 0.00001001
Iteration 171/1000 | Loss: 0.00001001
Iteration 172/1000 | Loss: 0.00001001
Iteration 173/1000 | Loss: 0.00001001
Iteration 174/1000 | Loss: 0.00001001
Iteration 175/1000 | Loss: 0.00001001
Iteration 176/1000 | Loss: 0.00001001
Iteration 177/1000 | Loss: 0.00001001
Iteration 178/1000 | Loss: 0.00001001
Iteration 179/1000 | Loss: 0.00001000
Iteration 180/1000 | Loss: 0.00001000
Iteration 181/1000 | Loss: 0.00001000
Iteration 182/1000 | Loss: 0.00001000
Iteration 183/1000 | Loss: 0.00001000
Iteration 184/1000 | Loss: 0.00001000
Iteration 185/1000 | Loss: 0.00001000
Iteration 186/1000 | Loss: 0.00001000
Iteration 187/1000 | Loss: 0.00000999
Iteration 188/1000 | Loss: 0.00000999
Iteration 189/1000 | Loss: 0.00000999
Iteration 190/1000 | Loss: 0.00000999
Iteration 191/1000 | Loss: 0.00000999
Iteration 192/1000 | Loss: 0.00000999
Iteration 193/1000 | Loss: 0.00000999
Iteration 194/1000 | Loss: 0.00000999
Iteration 195/1000 | Loss: 0.00000999
Iteration 196/1000 | Loss: 0.00000999
Iteration 197/1000 | Loss: 0.00000999
Iteration 198/1000 | Loss: 0.00000999
Iteration 199/1000 | Loss: 0.00000999
Iteration 200/1000 | Loss: 0.00000999
Iteration 201/1000 | Loss: 0.00000999
Iteration 202/1000 | Loss: 0.00000999
Iteration 203/1000 | Loss: 0.00000999
Iteration 204/1000 | Loss: 0.00000999
Iteration 205/1000 | Loss: 0.00000999
Iteration 206/1000 | Loss: 0.00000999
Iteration 207/1000 | Loss: 0.00000999
Iteration 208/1000 | Loss: 0.00000999
Iteration 209/1000 | Loss: 0.00000999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [9.986842997022904e-06, 9.986842997022904e-06, 9.986842997022904e-06, 9.986842997022904e-06, 9.986842997022904e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.986842997022904e-06

Optimization complete. Final v2v error: 2.7469327449798584 mm

Highest mean error: 3.109924554824829 mm for frame 150

Lowest mean error: 2.5426995754241943 mm for frame 36

Saving results

Total time: 44.40886831283569
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817883
Iteration 2/25 | Loss: 0.00149694
Iteration 3/25 | Loss: 0.00131930
Iteration 4/25 | Loss: 0.00130094
Iteration 5/25 | Loss: 0.00129889
Iteration 6/25 | Loss: 0.00129889
Iteration 7/25 | Loss: 0.00129889
Iteration 8/25 | Loss: 0.00129889
Iteration 9/25 | Loss: 0.00129889
Iteration 10/25 | Loss: 0.00129889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012988887028768659, 0.0012988887028768659, 0.0012988887028768659, 0.0012988887028768659, 0.0012988887028768659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012988887028768659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92916393
Iteration 2/25 | Loss: 0.00090605
Iteration 3/25 | Loss: 0.00090605
Iteration 4/25 | Loss: 0.00090605
Iteration 5/25 | Loss: 0.00090605
Iteration 6/25 | Loss: 0.00090604
Iteration 7/25 | Loss: 0.00090604
Iteration 8/25 | Loss: 0.00090604
Iteration 9/25 | Loss: 0.00090604
Iteration 10/25 | Loss: 0.00090604
Iteration 11/25 | Loss: 0.00090604
Iteration 12/25 | Loss: 0.00090604
Iteration 13/25 | Loss: 0.00090604
Iteration 14/25 | Loss: 0.00090604
Iteration 15/25 | Loss: 0.00090604
Iteration 16/25 | Loss: 0.00090604
Iteration 17/25 | Loss: 0.00090604
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009060436859726906, 0.0009060436859726906, 0.0009060436859726906, 0.0009060436859726906, 0.0009060436859726906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009060436859726906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090604
Iteration 2/1000 | Loss: 0.00004287
Iteration 3/1000 | Loss: 0.00003036
Iteration 4/1000 | Loss: 0.00002738
Iteration 5/1000 | Loss: 0.00002584
Iteration 6/1000 | Loss: 0.00002487
Iteration 7/1000 | Loss: 0.00002427
Iteration 8/1000 | Loss: 0.00002386
Iteration 9/1000 | Loss: 0.00002346
Iteration 10/1000 | Loss: 0.00002316
Iteration 11/1000 | Loss: 0.00002285
Iteration 12/1000 | Loss: 0.00002241
Iteration 13/1000 | Loss: 0.00002213
Iteration 14/1000 | Loss: 0.00002206
Iteration 15/1000 | Loss: 0.00002194
Iteration 16/1000 | Loss: 0.00002177
Iteration 17/1000 | Loss: 0.00002165
Iteration 18/1000 | Loss: 0.00002160
Iteration 19/1000 | Loss: 0.00002158
Iteration 20/1000 | Loss: 0.00002156
Iteration 21/1000 | Loss: 0.00002154
Iteration 22/1000 | Loss: 0.00002153
Iteration 23/1000 | Loss: 0.00002148
Iteration 24/1000 | Loss: 0.00002148
Iteration 25/1000 | Loss: 0.00002142
Iteration 26/1000 | Loss: 0.00002141
Iteration 27/1000 | Loss: 0.00002141
Iteration 28/1000 | Loss: 0.00002134
Iteration 29/1000 | Loss: 0.00002133
Iteration 30/1000 | Loss: 0.00002132
Iteration 31/1000 | Loss: 0.00002132
Iteration 32/1000 | Loss: 0.00002130
Iteration 33/1000 | Loss: 0.00002130
Iteration 34/1000 | Loss: 0.00002129
Iteration 35/1000 | Loss: 0.00002129
Iteration 36/1000 | Loss: 0.00002128
Iteration 37/1000 | Loss: 0.00002128
Iteration 38/1000 | Loss: 0.00002128
Iteration 39/1000 | Loss: 0.00002128
Iteration 40/1000 | Loss: 0.00002128
Iteration 41/1000 | Loss: 0.00002128
Iteration 42/1000 | Loss: 0.00002128
Iteration 43/1000 | Loss: 0.00002128
Iteration 44/1000 | Loss: 0.00002128
Iteration 45/1000 | Loss: 0.00002128
Iteration 46/1000 | Loss: 0.00002128
Iteration 47/1000 | Loss: 0.00002128
Iteration 48/1000 | Loss: 0.00002128
Iteration 49/1000 | Loss: 0.00002128
Iteration 50/1000 | Loss: 0.00002128
Iteration 51/1000 | Loss: 0.00002128
Iteration 52/1000 | Loss: 0.00002127
Iteration 53/1000 | Loss: 0.00002127
Iteration 54/1000 | Loss: 0.00002127
Iteration 55/1000 | Loss: 0.00002127
Iteration 56/1000 | Loss: 0.00002127
Iteration 57/1000 | Loss: 0.00002127
Iteration 58/1000 | Loss: 0.00002127
Iteration 59/1000 | Loss: 0.00002127
Iteration 60/1000 | Loss: 0.00002127
Iteration 61/1000 | Loss: 0.00002127
Iteration 62/1000 | Loss: 0.00002127
Iteration 63/1000 | Loss: 0.00002127
Iteration 64/1000 | Loss: 0.00002126
Iteration 65/1000 | Loss: 0.00002126
Iteration 66/1000 | Loss: 0.00002126
Iteration 67/1000 | Loss: 0.00002126
Iteration 68/1000 | Loss: 0.00002126
Iteration 69/1000 | Loss: 0.00002126
Iteration 70/1000 | Loss: 0.00002126
Iteration 71/1000 | Loss: 0.00002126
Iteration 72/1000 | Loss: 0.00002126
Iteration 73/1000 | Loss: 0.00002126
Iteration 74/1000 | Loss: 0.00002126
Iteration 75/1000 | Loss: 0.00002126
Iteration 76/1000 | Loss: 0.00002126
Iteration 77/1000 | Loss: 0.00002126
Iteration 78/1000 | Loss: 0.00002126
Iteration 79/1000 | Loss: 0.00002126
Iteration 80/1000 | Loss: 0.00002126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [2.126086656062398e-05, 2.126086656062398e-05, 2.126086656062398e-05, 2.126086656062398e-05, 2.126086656062398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.126086656062398e-05

Optimization complete. Final v2v error: 3.8985440731048584 mm

Highest mean error: 3.933803081512451 mm for frame 87

Lowest mean error: 3.8652069568634033 mm for frame 61

Saving results

Total time: 34.75503993034363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045275
Iteration 2/25 | Loss: 0.00207440
Iteration 3/25 | Loss: 0.00150871
Iteration 4/25 | Loss: 0.00145336
Iteration 5/25 | Loss: 0.00143722
Iteration 6/25 | Loss: 0.00137713
Iteration 7/25 | Loss: 0.00135159
Iteration 8/25 | Loss: 0.00134042
Iteration 9/25 | Loss: 0.00133339
Iteration 10/25 | Loss: 0.00133690
Iteration 11/25 | Loss: 0.00133269
Iteration 12/25 | Loss: 0.00133303
Iteration 13/25 | Loss: 0.00133540
Iteration 14/25 | Loss: 0.00132657
Iteration 15/25 | Loss: 0.00132176
Iteration 16/25 | Loss: 0.00131900
Iteration 17/25 | Loss: 0.00131648
Iteration 18/25 | Loss: 0.00133492
Iteration 19/25 | Loss: 0.00133331
Iteration 20/25 | Loss: 0.00133269
Iteration 21/25 | Loss: 0.00130256
Iteration 22/25 | Loss: 0.00130077
Iteration 23/25 | Loss: 0.00130083
Iteration 24/25 | Loss: 0.00130038
Iteration 25/25 | Loss: 0.00130037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.08077574
Iteration 2/25 | Loss: 0.00142996
Iteration 3/25 | Loss: 0.00135270
Iteration 4/25 | Loss: 0.00135270
Iteration 5/25 | Loss: 0.00135270
Iteration 6/25 | Loss: 0.00135270
Iteration 7/25 | Loss: 0.00135270
Iteration 8/25 | Loss: 0.00135270
Iteration 9/25 | Loss: 0.00135270
Iteration 10/25 | Loss: 0.00135270
Iteration 11/25 | Loss: 0.00135270
Iteration 12/25 | Loss: 0.00135270
Iteration 13/25 | Loss: 0.00135270
Iteration 14/25 | Loss: 0.00135270
Iteration 15/25 | Loss: 0.00135270
Iteration 16/25 | Loss: 0.00135270
Iteration 17/25 | Loss: 0.00135270
Iteration 18/25 | Loss: 0.00135270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013526960974559188, 0.0013526960974559188, 0.0013526960974559188, 0.0013526960974559188, 0.0013526960974559188]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013526960974559188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135270
Iteration 2/1000 | Loss: 0.00012331
Iteration 3/1000 | Loss: 0.00002717
Iteration 4/1000 | Loss: 0.00008694
Iteration 5/1000 | Loss: 0.00002295
Iteration 6/1000 | Loss: 0.00007975
Iteration 7/1000 | Loss: 0.00002156
Iteration 8/1000 | Loss: 0.00002576
Iteration 9/1000 | Loss: 0.00002073
Iteration 10/1000 | Loss: 0.00002560
Iteration 11/1000 | Loss: 0.00002020
Iteration 12/1000 | Loss: 0.00002045
Iteration 13/1000 | Loss: 0.00001995
Iteration 14/1000 | Loss: 0.00001968
Iteration 15/1000 | Loss: 0.00001950
Iteration 16/1000 | Loss: 0.00001949
Iteration 17/1000 | Loss: 0.00001949
Iteration 18/1000 | Loss: 0.00001948
Iteration 19/1000 | Loss: 0.00001947
Iteration 20/1000 | Loss: 0.00001946
Iteration 21/1000 | Loss: 0.00001944
Iteration 22/1000 | Loss: 0.00001944
Iteration 23/1000 | Loss: 0.00001942
Iteration 24/1000 | Loss: 0.00001942
Iteration 25/1000 | Loss: 0.00001941
Iteration 26/1000 | Loss: 0.00001940
Iteration 27/1000 | Loss: 0.00001936
Iteration 28/1000 | Loss: 0.00001925
Iteration 29/1000 | Loss: 0.00001921
Iteration 30/1000 | Loss: 0.00001921
Iteration 31/1000 | Loss: 0.00002611
Iteration 32/1000 | Loss: 0.00001914
Iteration 33/1000 | Loss: 0.00001912
Iteration 34/1000 | Loss: 0.00001911
Iteration 35/1000 | Loss: 0.00001911
Iteration 36/1000 | Loss: 0.00001911
Iteration 37/1000 | Loss: 0.00001911
Iteration 38/1000 | Loss: 0.00001911
Iteration 39/1000 | Loss: 0.00001910
Iteration 40/1000 | Loss: 0.00001910
Iteration 41/1000 | Loss: 0.00001910
Iteration 42/1000 | Loss: 0.00001910
Iteration 43/1000 | Loss: 0.00001910
Iteration 44/1000 | Loss: 0.00001910
Iteration 45/1000 | Loss: 0.00001909
Iteration 46/1000 | Loss: 0.00001909
Iteration 47/1000 | Loss: 0.00001909
Iteration 48/1000 | Loss: 0.00001908
Iteration 49/1000 | Loss: 0.00001908
Iteration 50/1000 | Loss: 0.00001908
Iteration 51/1000 | Loss: 0.00001908
Iteration 52/1000 | Loss: 0.00001907
Iteration 53/1000 | Loss: 0.00001907
Iteration 54/1000 | Loss: 0.00001907
Iteration 55/1000 | Loss: 0.00001906
Iteration 56/1000 | Loss: 0.00001906
Iteration 57/1000 | Loss: 0.00001906
Iteration 58/1000 | Loss: 0.00001906
Iteration 59/1000 | Loss: 0.00001906
Iteration 60/1000 | Loss: 0.00001905
Iteration 61/1000 | Loss: 0.00001905
Iteration 62/1000 | Loss: 0.00001905
Iteration 63/1000 | Loss: 0.00001905
Iteration 64/1000 | Loss: 0.00001904
Iteration 65/1000 | Loss: 0.00001904
Iteration 66/1000 | Loss: 0.00001904
Iteration 67/1000 | Loss: 0.00002549
Iteration 68/1000 | Loss: 0.00001904
Iteration 69/1000 | Loss: 0.00001901
Iteration 70/1000 | Loss: 0.00001901
Iteration 71/1000 | Loss: 0.00001901
Iteration 72/1000 | Loss: 0.00001901
Iteration 73/1000 | Loss: 0.00001901
Iteration 74/1000 | Loss: 0.00001901
Iteration 75/1000 | Loss: 0.00001900
Iteration 76/1000 | Loss: 0.00001900
Iteration 77/1000 | Loss: 0.00001900
Iteration 78/1000 | Loss: 0.00001900
Iteration 79/1000 | Loss: 0.00001900
Iteration 80/1000 | Loss: 0.00001900
Iteration 81/1000 | Loss: 0.00001900
Iteration 82/1000 | Loss: 0.00001900
Iteration 83/1000 | Loss: 0.00001900
Iteration 84/1000 | Loss: 0.00001900
Iteration 85/1000 | Loss: 0.00001900
Iteration 86/1000 | Loss: 0.00001900
Iteration 87/1000 | Loss: 0.00001900
Iteration 88/1000 | Loss: 0.00001900
Iteration 89/1000 | Loss: 0.00001900
Iteration 90/1000 | Loss: 0.00001900
Iteration 91/1000 | Loss: 0.00001900
Iteration 92/1000 | Loss: 0.00001900
Iteration 93/1000 | Loss: 0.00001900
Iteration 94/1000 | Loss: 0.00001900
Iteration 95/1000 | Loss: 0.00001900
Iteration 96/1000 | Loss: 0.00001900
Iteration 97/1000 | Loss: 0.00001900
Iteration 98/1000 | Loss: 0.00001900
Iteration 99/1000 | Loss: 0.00001900
Iteration 100/1000 | Loss: 0.00001900
Iteration 101/1000 | Loss: 0.00001900
Iteration 102/1000 | Loss: 0.00001900
Iteration 103/1000 | Loss: 0.00001900
Iteration 104/1000 | Loss: 0.00001900
Iteration 105/1000 | Loss: 0.00001900
Iteration 106/1000 | Loss: 0.00001900
Iteration 107/1000 | Loss: 0.00001900
Iteration 108/1000 | Loss: 0.00001900
Iteration 109/1000 | Loss: 0.00001900
Iteration 110/1000 | Loss: 0.00001900
Iteration 111/1000 | Loss: 0.00001900
Iteration 112/1000 | Loss: 0.00001900
Iteration 113/1000 | Loss: 0.00001900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.8999695384991355e-05, 1.8999695384991355e-05, 1.8999695384991355e-05, 1.8999695384991355e-05, 1.8999695384991355e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8999695384991355e-05

Optimization complete. Final v2v error: 3.638341188430786 mm

Highest mean error: 4.052755355834961 mm for frame 175

Lowest mean error: 3.3164095878601074 mm for frame 116

Saving results

Total time: 84.10611414909363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849095
Iteration 2/25 | Loss: 0.00145310
Iteration 3/25 | Loss: 0.00131134
Iteration 4/25 | Loss: 0.00129302
Iteration 5/25 | Loss: 0.00128902
Iteration 6/25 | Loss: 0.00128847
Iteration 7/25 | Loss: 0.00128847
Iteration 8/25 | Loss: 0.00128847
Iteration 9/25 | Loss: 0.00128847
Iteration 10/25 | Loss: 0.00128847
Iteration 11/25 | Loss: 0.00128847
Iteration 12/25 | Loss: 0.00128847
Iteration 13/25 | Loss: 0.00128847
Iteration 14/25 | Loss: 0.00128847
Iteration 15/25 | Loss: 0.00128847
Iteration 16/25 | Loss: 0.00128847
Iteration 17/25 | Loss: 0.00128847
Iteration 18/25 | Loss: 0.00128847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012884668540209532, 0.0012884668540209532, 0.0012884668540209532, 0.0012884668540209532, 0.0012884668540209532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012884668540209532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23730123
Iteration 2/25 | Loss: 0.00156477
Iteration 3/25 | Loss: 0.00156476
Iteration 4/25 | Loss: 0.00156476
Iteration 5/25 | Loss: 0.00156476
Iteration 6/25 | Loss: 0.00156476
Iteration 7/25 | Loss: 0.00156476
Iteration 8/25 | Loss: 0.00156476
Iteration 9/25 | Loss: 0.00156476
Iteration 10/25 | Loss: 0.00156476
Iteration 11/25 | Loss: 0.00156476
Iteration 12/25 | Loss: 0.00156476
Iteration 13/25 | Loss: 0.00156476
Iteration 14/25 | Loss: 0.00156476
Iteration 15/25 | Loss: 0.00156476
Iteration 16/25 | Loss: 0.00156476
Iteration 17/25 | Loss: 0.00156476
Iteration 18/25 | Loss: 0.00156476
Iteration 19/25 | Loss: 0.00156476
Iteration 20/25 | Loss: 0.00156476
Iteration 21/25 | Loss: 0.00156476
Iteration 22/25 | Loss: 0.00156476
Iteration 23/25 | Loss: 0.00156476
Iteration 24/25 | Loss: 0.00156476
Iteration 25/25 | Loss: 0.00156476

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156476
Iteration 2/1000 | Loss: 0.00005736
Iteration 3/1000 | Loss: 0.00004618
Iteration 4/1000 | Loss: 0.00003862
Iteration 5/1000 | Loss: 0.00003654
Iteration 6/1000 | Loss: 0.00003455
Iteration 7/1000 | Loss: 0.00003343
Iteration 8/1000 | Loss: 0.00003279
Iteration 9/1000 | Loss: 0.00003229
Iteration 10/1000 | Loss: 0.00003197
Iteration 11/1000 | Loss: 0.00003164
Iteration 12/1000 | Loss: 0.00003140
Iteration 13/1000 | Loss: 0.00003114
Iteration 14/1000 | Loss: 0.00003093
Iteration 15/1000 | Loss: 0.00003078
Iteration 16/1000 | Loss: 0.00003070
Iteration 17/1000 | Loss: 0.00003061
Iteration 18/1000 | Loss: 0.00003055
Iteration 19/1000 | Loss: 0.00003052
Iteration 20/1000 | Loss: 0.00003051
Iteration 21/1000 | Loss: 0.00003051
Iteration 22/1000 | Loss: 0.00003051
Iteration 23/1000 | Loss: 0.00003049
Iteration 24/1000 | Loss: 0.00003048
Iteration 25/1000 | Loss: 0.00003048
Iteration 26/1000 | Loss: 0.00003047
Iteration 27/1000 | Loss: 0.00003047
Iteration 28/1000 | Loss: 0.00003047
Iteration 29/1000 | Loss: 0.00003047
Iteration 30/1000 | Loss: 0.00003046
Iteration 31/1000 | Loss: 0.00003046
Iteration 32/1000 | Loss: 0.00003046
Iteration 33/1000 | Loss: 0.00003045
Iteration 34/1000 | Loss: 0.00003044
Iteration 35/1000 | Loss: 0.00003044
Iteration 36/1000 | Loss: 0.00003043
Iteration 37/1000 | Loss: 0.00003043
Iteration 38/1000 | Loss: 0.00003043
Iteration 39/1000 | Loss: 0.00003042
Iteration 40/1000 | Loss: 0.00003042
Iteration 41/1000 | Loss: 0.00003042
Iteration 42/1000 | Loss: 0.00003041
Iteration 43/1000 | Loss: 0.00003038
Iteration 44/1000 | Loss: 0.00003038
Iteration 45/1000 | Loss: 0.00003038
Iteration 46/1000 | Loss: 0.00003037
Iteration 47/1000 | Loss: 0.00003036
Iteration 48/1000 | Loss: 0.00003036
Iteration 49/1000 | Loss: 0.00003036
Iteration 50/1000 | Loss: 0.00003036
Iteration 51/1000 | Loss: 0.00003036
Iteration 52/1000 | Loss: 0.00003036
Iteration 53/1000 | Loss: 0.00003036
Iteration 54/1000 | Loss: 0.00003035
Iteration 55/1000 | Loss: 0.00003035
Iteration 56/1000 | Loss: 0.00003035
Iteration 57/1000 | Loss: 0.00003035
Iteration 58/1000 | Loss: 0.00003034
Iteration 59/1000 | Loss: 0.00003034
Iteration 60/1000 | Loss: 0.00003034
Iteration 61/1000 | Loss: 0.00003034
Iteration 62/1000 | Loss: 0.00003034
Iteration 63/1000 | Loss: 0.00003033
Iteration 64/1000 | Loss: 0.00003033
Iteration 65/1000 | Loss: 0.00003032
Iteration 66/1000 | Loss: 0.00003032
Iteration 67/1000 | Loss: 0.00003032
Iteration 68/1000 | Loss: 0.00003031
Iteration 69/1000 | Loss: 0.00003031
Iteration 70/1000 | Loss: 0.00003031
Iteration 71/1000 | Loss: 0.00003031
Iteration 72/1000 | Loss: 0.00003031
Iteration 73/1000 | Loss: 0.00003030
Iteration 74/1000 | Loss: 0.00003030
Iteration 75/1000 | Loss: 0.00003030
Iteration 76/1000 | Loss: 0.00003030
Iteration 77/1000 | Loss: 0.00003030
Iteration 78/1000 | Loss: 0.00003029
Iteration 79/1000 | Loss: 0.00003029
Iteration 80/1000 | Loss: 0.00003029
Iteration 81/1000 | Loss: 0.00003028
Iteration 82/1000 | Loss: 0.00003028
Iteration 83/1000 | Loss: 0.00003028
Iteration 84/1000 | Loss: 0.00003028
Iteration 85/1000 | Loss: 0.00003027
Iteration 86/1000 | Loss: 0.00003027
Iteration 87/1000 | Loss: 0.00003027
Iteration 88/1000 | Loss: 0.00003026
Iteration 89/1000 | Loss: 0.00003026
Iteration 90/1000 | Loss: 0.00003025
Iteration 91/1000 | Loss: 0.00003025
Iteration 92/1000 | Loss: 0.00003025
Iteration 93/1000 | Loss: 0.00003024
Iteration 94/1000 | Loss: 0.00003024
Iteration 95/1000 | Loss: 0.00003024
Iteration 96/1000 | Loss: 0.00003024
Iteration 97/1000 | Loss: 0.00003023
Iteration 98/1000 | Loss: 0.00003023
Iteration 99/1000 | Loss: 0.00003023
Iteration 100/1000 | Loss: 0.00003023
Iteration 101/1000 | Loss: 0.00003023
Iteration 102/1000 | Loss: 0.00003022
Iteration 103/1000 | Loss: 0.00003022
Iteration 104/1000 | Loss: 0.00003022
Iteration 105/1000 | Loss: 0.00003022
Iteration 106/1000 | Loss: 0.00003022
Iteration 107/1000 | Loss: 0.00003022
Iteration 108/1000 | Loss: 0.00003022
Iteration 109/1000 | Loss: 0.00003022
Iteration 110/1000 | Loss: 0.00003021
Iteration 111/1000 | Loss: 0.00003021
Iteration 112/1000 | Loss: 0.00003021
Iteration 113/1000 | Loss: 0.00003021
Iteration 114/1000 | Loss: 0.00003021
Iteration 115/1000 | Loss: 0.00003021
Iteration 116/1000 | Loss: 0.00003020
Iteration 117/1000 | Loss: 0.00003020
Iteration 118/1000 | Loss: 0.00003020
Iteration 119/1000 | Loss: 0.00003020
Iteration 120/1000 | Loss: 0.00003020
Iteration 121/1000 | Loss: 0.00003020
Iteration 122/1000 | Loss: 0.00003020
Iteration 123/1000 | Loss: 0.00003019
Iteration 124/1000 | Loss: 0.00003019
Iteration 125/1000 | Loss: 0.00003019
Iteration 126/1000 | Loss: 0.00003019
Iteration 127/1000 | Loss: 0.00003019
Iteration 128/1000 | Loss: 0.00003019
Iteration 129/1000 | Loss: 0.00003019
Iteration 130/1000 | Loss: 0.00003019
Iteration 131/1000 | Loss: 0.00003019
Iteration 132/1000 | Loss: 0.00003019
Iteration 133/1000 | Loss: 0.00003019
Iteration 134/1000 | Loss: 0.00003019
Iteration 135/1000 | Loss: 0.00003019
Iteration 136/1000 | Loss: 0.00003019
Iteration 137/1000 | Loss: 0.00003019
Iteration 138/1000 | Loss: 0.00003019
Iteration 139/1000 | Loss: 0.00003019
Iteration 140/1000 | Loss: 0.00003019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [3.0191484256647527e-05, 3.0191484256647527e-05, 3.0191484256647527e-05, 3.0191484256647527e-05, 3.0191484256647527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0191484256647527e-05

Optimization complete. Final v2v error: 4.557979106903076 mm

Highest mean error: 5.112213611602783 mm for frame 22

Lowest mean error: 4.028004169464111 mm for frame 86

Saving results

Total time: 40.54750919342041
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034493
Iteration 2/25 | Loss: 0.00387638
Iteration 3/25 | Loss: 0.00257948
Iteration 4/25 | Loss: 0.00216451
Iteration 5/25 | Loss: 0.00196689
Iteration 6/25 | Loss: 0.00185177
Iteration 7/25 | Loss: 0.00171242
Iteration 8/25 | Loss: 0.00164134
Iteration 9/25 | Loss: 0.00161012
Iteration 10/25 | Loss: 0.00158816
Iteration 11/25 | Loss: 0.00158140
Iteration 12/25 | Loss: 0.00155605
Iteration 13/25 | Loss: 0.00155412
Iteration 14/25 | Loss: 0.00156245
Iteration 15/25 | Loss: 0.00154196
Iteration 16/25 | Loss: 0.00153024
Iteration 17/25 | Loss: 0.00152879
Iteration 18/25 | Loss: 0.00152704
Iteration 19/25 | Loss: 0.00152533
Iteration 20/25 | Loss: 0.00152444
Iteration 21/25 | Loss: 0.00152656
Iteration 22/25 | Loss: 0.00152543
Iteration 23/25 | Loss: 0.00152460
Iteration 24/25 | Loss: 0.00152469
Iteration 25/25 | Loss: 0.00152401

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20830715
Iteration 2/25 | Loss: 0.00430022
Iteration 3/25 | Loss: 0.00368962
Iteration 4/25 | Loss: 0.00368962
Iteration 5/25 | Loss: 0.00368962
Iteration 6/25 | Loss: 0.00368962
Iteration 7/25 | Loss: 0.00368962
Iteration 8/25 | Loss: 0.00368962
Iteration 9/25 | Loss: 0.00368962
Iteration 10/25 | Loss: 0.00368962
Iteration 11/25 | Loss: 0.00368962
Iteration 12/25 | Loss: 0.00368962
Iteration 13/25 | Loss: 0.00368962
Iteration 14/25 | Loss: 0.00368962
Iteration 15/25 | Loss: 0.00368962
Iteration 16/25 | Loss: 0.00368962
Iteration 17/25 | Loss: 0.00368962
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003689616220071912, 0.003689616220071912, 0.003689616220071912, 0.003689616220071912, 0.003689616220071912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003689616220071912

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00368962
Iteration 2/1000 | Loss: 0.00479585
Iteration 3/1000 | Loss: 0.00926774
Iteration 4/1000 | Loss: 0.00182500
Iteration 5/1000 | Loss: 0.00049793
Iteration 6/1000 | Loss: 0.00040683
Iteration 7/1000 | Loss: 0.00065596
Iteration 8/1000 | Loss: 0.00064149
Iteration 9/1000 | Loss: 0.00041787
Iteration 10/1000 | Loss: 0.00053520
Iteration 11/1000 | Loss: 0.00174534
Iteration 12/1000 | Loss: 0.00048601
Iteration 13/1000 | Loss: 0.00167736
Iteration 14/1000 | Loss: 0.00129416
Iteration 15/1000 | Loss: 0.00168390
Iteration 16/1000 | Loss: 0.00020331
Iteration 17/1000 | Loss: 0.00020670
Iteration 18/1000 | Loss: 0.00130383
Iteration 19/1000 | Loss: 0.00025013
Iteration 20/1000 | Loss: 0.00021927
Iteration 21/1000 | Loss: 0.00028418
Iteration 22/1000 | Loss: 0.00078587
Iteration 23/1000 | Loss: 0.00095419
Iteration 24/1000 | Loss: 0.00035717
Iteration 25/1000 | Loss: 0.00029935
Iteration 26/1000 | Loss: 0.00023155
Iteration 27/1000 | Loss: 0.00047408
Iteration 28/1000 | Loss: 0.00033839
Iteration 29/1000 | Loss: 0.00041546
Iteration 30/1000 | Loss: 0.00047595
Iteration 31/1000 | Loss: 0.00026441
Iteration 32/1000 | Loss: 0.00065720
Iteration 33/1000 | Loss: 0.00125418
Iteration 34/1000 | Loss: 0.00040121
Iteration 35/1000 | Loss: 0.00133001
Iteration 36/1000 | Loss: 0.00355962
Iteration 37/1000 | Loss: 0.00531883
Iteration 38/1000 | Loss: 0.00490345
Iteration 39/1000 | Loss: 0.00579229
Iteration 40/1000 | Loss: 0.00643543
Iteration 41/1000 | Loss: 0.00457741
Iteration 42/1000 | Loss: 0.00515515
Iteration 43/1000 | Loss: 0.00216437
Iteration 44/1000 | Loss: 0.00227604
Iteration 45/1000 | Loss: 0.00236154
Iteration 46/1000 | Loss: 0.00161277
Iteration 47/1000 | Loss: 0.00107269
Iteration 48/1000 | Loss: 0.00051875
Iteration 49/1000 | Loss: 0.00238838
Iteration 50/1000 | Loss: 0.00250455
Iteration 51/1000 | Loss: 0.00212781
Iteration 52/1000 | Loss: 0.00058535
Iteration 53/1000 | Loss: 0.00155550
Iteration 54/1000 | Loss: 0.00056512
Iteration 55/1000 | Loss: 0.00111477
Iteration 56/1000 | Loss: 0.00047191
Iteration 57/1000 | Loss: 0.00107721
Iteration 58/1000 | Loss: 0.00108682
Iteration 59/1000 | Loss: 0.00137565
Iteration 60/1000 | Loss: 0.00039679
Iteration 61/1000 | Loss: 0.00163218
Iteration 62/1000 | Loss: 0.00119284
Iteration 63/1000 | Loss: 0.00086961
Iteration 64/1000 | Loss: 0.00054652
Iteration 65/1000 | Loss: 0.00040601
Iteration 66/1000 | Loss: 0.00050239
Iteration 67/1000 | Loss: 0.00070090
Iteration 68/1000 | Loss: 0.00025004
Iteration 69/1000 | Loss: 0.00029034
Iteration 70/1000 | Loss: 0.00073984
Iteration 71/1000 | Loss: 0.00033854
Iteration 72/1000 | Loss: 0.00038709
Iteration 73/1000 | Loss: 0.00056630
Iteration 74/1000 | Loss: 0.00100238
Iteration 75/1000 | Loss: 0.00073543
Iteration 76/1000 | Loss: 0.00053629
Iteration 77/1000 | Loss: 0.00033106
Iteration 78/1000 | Loss: 0.00009374
Iteration 79/1000 | Loss: 0.00008369
Iteration 80/1000 | Loss: 0.00010975
Iteration 81/1000 | Loss: 0.00009095
Iteration 82/1000 | Loss: 0.00008786
Iteration 83/1000 | Loss: 0.00029586
Iteration 84/1000 | Loss: 0.00059299
Iteration 85/1000 | Loss: 0.00081114
Iteration 86/1000 | Loss: 0.00028682
Iteration 87/1000 | Loss: 0.00111285
Iteration 88/1000 | Loss: 0.00069905
Iteration 89/1000 | Loss: 0.00070288
Iteration 90/1000 | Loss: 0.00140630
Iteration 91/1000 | Loss: 0.00081462
Iteration 92/1000 | Loss: 0.00106806
Iteration 93/1000 | Loss: 0.00067943
Iteration 94/1000 | Loss: 0.00115497
Iteration 95/1000 | Loss: 0.00097374
Iteration 96/1000 | Loss: 0.00059997
Iteration 97/1000 | Loss: 0.00067389
Iteration 98/1000 | Loss: 0.00046235
Iteration 99/1000 | Loss: 0.00065534
Iteration 100/1000 | Loss: 0.00033142
Iteration 101/1000 | Loss: 0.00024578
Iteration 102/1000 | Loss: 0.00090796
Iteration 103/1000 | Loss: 0.00035290
Iteration 104/1000 | Loss: 0.00006995
Iteration 105/1000 | Loss: 0.00007768
Iteration 106/1000 | Loss: 0.00032988
Iteration 107/1000 | Loss: 0.00022804
Iteration 108/1000 | Loss: 0.00027161
Iteration 109/1000 | Loss: 0.00007295
Iteration 110/1000 | Loss: 0.00007296
Iteration 111/1000 | Loss: 0.00007156
Iteration 112/1000 | Loss: 0.00006202
Iteration 113/1000 | Loss: 0.00005590
Iteration 114/1000 | Loss: 0.00007019
Iteration 115/1000 | Loss: 0.00008508
Iteration 116/1000 | Loss: 0.00005609
Iteration 117/1000 | Loss: 0.00003938
Iteration 118/1000 | Loss: 0.00007255
Iteration 119/1000 | Loss: 0.00006720
Iteration 120/1000 | Loss: 0.00009143
Iteration 121/1000 | Loss: 0.00007695
Iteration 122/1000 | Loss: 0.00006489
Iteration 123/1000 | Loss: 0.00007092
Iteration 124/1000 | Loss: 0.00005776
Iteration 125/1000 | Loss: 0.00006283
Iteration 126/1000 | Loss: 0.00006648
Iteration 127/1000 | Loss: 0.00007132
Iteration 128/1000 | Loss: 0.00006851
Iteration 129/1000 | Loss: 0.00004429
Iteration 130/1000 | Loss: 0.00007013
Iteration 131/1000 | Loss: 0.00005519
Iteration 132/1000 | Loss: 0.00006108
Iteration 133/1000 | Loss: 0.00006545
Iteration 134/1000 | Loss: 0.00004909
Iteration 135/1000 | Loss: 0.00012133
Iteration 136/1000 | Loss: 0.00007564
Iteration 137/1000 | Loss: 0.00023318
Iteration 138/1000 | Loss: 0.00007286
Iteration 139/1000 | Loss: 0.00009812
Iteration 140/1000 | Loss: 0.00006419
Iteration 141/1000 | Loss: 0.00006599
Iteration 142/1000 | Loss: 0.00005926
Iteration 143/1000 | Loss: 0.00006556
Iteration 144/1000 | Loss: 0.00007102
Iteration 145/1000 | Loss: 0.00006805
Iteration 146/1000 | Loss: 0.00006527
Iteration 147/1000 | Loss: 0.00007041
Iteration 148/1000 | Loss: 0.00006567
Iteration 149/1000 | Loss: 0.00044041
Iteration 150/1000 | Loss: 0.00037568
Iteration 151/1000 | Loss: 0.00008459
Iteration 152/1000 | Loss: 0.00037454
Iteration 153/1000 | Loss: 0.00008594
Iteration 154/1000 | Loss: 0.00011690
Iteration 155/1000 | Loss: 0.00007854
Iteration 156/1000 | Loss: 0.00005273
Iteration 157/1000 | Loss: 0.00005586
Iteration 158/1000 | Loss: 0.00007226
Iteration 159/1000 | Loss: 0.00006886
Iteration 160/1000 | Loss: 0.00007103
Iteration 161/1000 | Loss: 0.00007296
Iteration 162/1000 | Loss: 0.00006793
Iteration 163/1000 | Loss: 0.00005368
Iteration 164/1000 | Loss: 0.00005132
Iteration 165/1000 | Loss: 0.00005398
Iteration 166/1000 | Loss: 0.00006216
Iteration 167/1000 | Loss: 0.00006594
Iteration 168/1000 | Loss: 0.00006779
Iteration 169/1000 | Loss: 0.00005406
Iteration 170/1000 | Loss: 0.00006618
Iteration 171/1000 | Loss: 0.00007589
Iteration 172/1000 | Loss: 0.00005976
Iteration 173/1000 | Loss: 0.00018526
Iteration 174/1000 | Loss: 0.00008813
Iteration 175/1000 | Loss: 0.00006977
Iteration 176/1000 | Loss: 0.00004540
Iteration 177/1000 | Loss: 0.00009102
Iteration 178/1000 | Loss: 0.00003616
Iteration 179/1000 | Loss: 0.00004215
Iteration 180/1000 | Loss: 0.00003211
Iteration 181/1000 | Loss: 0.00005162
Iteration 182/1000 | Loss: 0.00005813
Iteration 183/1000 | Loss: 0.00008666
Iteration 184/1000 | Loss: 0.00006671
Iteration 185/1000 | Loss: 0.00007587
Iteration 186/1000 | Loss: 0.00006584
Iteration 187/1000 | Loss: 0.00010106
Iteration 188/1000 | Loss: 0.00006441
Iteration 189/1000 | Loss: 0.00006780
Iteration 190/1000 | Loss: 0.00006681
Iteration 191/1000 | Loss: 0.00006805
Iteration 192/1000 | Loss: 0.00006800
Iteration 193/1000 | Loss: 0.00006621
Iteration 194/1000 | Loss: 0.00006302
Iteration 195/1000 | Loss: 0.00004860
Iteration 196/1000 | Loss: 0.00025285
Iteration 197/1000 | Loss: 0.00018731
Iteration 198/1000 | Loss: 0.00006016
Iteration 199/1000 | Loss: 0.00005155
Iteration 200/1000 | Loss: 0.00004732
Iteration 201/1000 | Loss: 0.00006274
Iteration 202/1000 | Loss: 0.00006128
Iteration 203/1000 | Loss: 0.00006093
Iteration 204/1000 | Loss: 0.00004410
Iteration 205/1000 | Loss: 0.00005524
Iteration 206/1000 | Loss: 0.00005956
Iteration 207/1000 | Loss: 0.00004905
Iteration 208/1000 | Loss: 0.00026060
Iteration 209/1000 | Loss: 0.00028924
Iteration 210/1000 | Loss: 0.00030333
Iteration 211/1000 | Loss: 0.00025609
Iteration 212/1000 | Loss: 0.00030035
Iteration 213/1000 | Loss: 0.00023329
Iteration 214/1000 | Loss: 0.00033803
Iteration 215/1000 | Loss: 0.00013315
Iteration 216/1000 | Loss: 0.00032224
Iteration 217/1000 | Loss: 0.00006647
Iteration 218/1000 | Loss: 0.00007062
Iteration 219/1000 | Loss: 0.00008800
Iteration 220/1000 | Loss: 0.00005191
Iteration 221/1000 | Loss: 0.00005177
Iteration 222/1000 | Loss: 0.00006654
Iteration 223/1000 | Loss: 0.00006343
Iteration 224/1000 | Loss: 0.00006085
Iteration 225/1000 | Loss: 0.00010319
Iteration 226/1000 | Loss: 0.00006414
Iteration 227/1000 | Loss: 0.00007925
Iteration 228/1000 | Loss: 0.00005753
Iteration 229/1000 | Loss: 0.00005044
Iteration 230/1000 | Loss: 0.00009340
Iteration 231/1000 | Loss: 0.00008240
Iteration 232/1000 | Loss: 0.00004742
Iteration 233/1000 | Loss: 0.00004230
Iteration 234/1000 | Loss: 0.00006401
Iteration 235/1000 | Loss: 0.00006641
Iteration 236/1000 | Loss: 0.00015404
Iteration 237/1000 | Loss: 0.00006165
Iteration 238/1000 | Loss: 0.00009823
Iteration 239/1000 | Loss: 0.00006820
Iteration 240/1000 | Loss: 0.00007805
Iteration 241/1000 | Loss: 0.00007307
Iteration 242/1000 | Loss: 0.00006690
Iteration 243/1000 | Loss: 0.00006524
Iteration 244/1000 | Loss: 0.00006740
Iteration 245/1000 | Loss: 0.00006485
Iteration 246/1000 | Loss: 0.00006656
Iteration 247/1000 | Loss: 0.00006715
Iteration 248/1000 | Loss: 0.00005962
Iteration 249/1000 | Loss: 0.00006751
Iteration 250/1000 | Loss: 0.00006413
Iteration 251/1000 | Loss: 0.00009625
Iteration 252/1000 | Loss: 0.00009044
Iteration 253/1000 | Loss: 0.00006605
Iteration 254/1000 | Loss: 0.00005553
Iteration 255/1000 | Loss: 0.00007399
Iteration 256/1000 | Loss: 0.00005052
Iteration 257/1000 | Loss: 0.00006318
Iteration 258/1000 | Loss: 0.00005467
Iteration 259/1000 | Loss: 0.00004811
Iteration 260/1000 | Loss: 0.00007565
Iteration 261/1000 | Loss: 0.00008886
Iteration 262/1000 | Loss: 0.00016050
Iteration 263/1000 | Loss: 0.00008367
Iteration 264/1000 | Loss: 0.00008675
Iteration 265/1000 | Loss: 0.00007505
Iteration 266/1000 | Loss: 0.00006878
Iteration 267/1000 | Loss: 0.00006289
Iteration 268/1000 | Loss: 0.00007335
Iteration 269/1000 | Loss: 0.00005910
Iteration 270/1000 | Loss: 0.00007704
Iteration 271/1000 | Loss: 0.00009257
Iteration 272/1000 | Loss: 0.00006450
Iteration 273/1000 | Loss: 0.00007242
Iteration 274/1000 | Loss: 0.00006565
Iteration 275/1000 | Loss: 0.00013648
Iteration 276/1000 | Loss: 0.00011286
Iteration 277/1000 | Loss: 0.00004662
Iteration 278/1000 | Loss: 0.00005876
Iteration 279/1000 | Loss: 0.00004551
Iteration 280/1000 | Loss: 0.00006194
Iteration 281/1000 | Loss: 0.00007409
Iteration 282/1000 | Loss: 0.00006633
Iteration 283/1000 | Loss: 0.00005944
Iteration 284/1000 | Loss: 0.00005928
Iteration 285/1000 | Loss: 0.00006273
Iteration 286/1000 | Loss: 0.00005642
Iteration 287/1000 | Loss: 0.00009668
Iteration 288/1000 | Loss: 0.00006705
Iteration 289/1000 | Loss: 0.00006812
Iteration 290/1000 | Loss: 0.00007107
Iteration 291/1000 | Loss: 0.00008367
Iteration 292/1000 | Loss: 0.00006617
Iteration 293/1000 | Loss: 0.00019669
Iteration 294/1000 | Loss: 0.00006591
Iteration 295/1000 | Loss: 0.00005883
Iteration 296/1000 | Loss: 0.00009014
Iteration 297/1000 | Loss: 0.00005691
Iteration 298/1000 | Loss: 0.00005618
Iteration 299/1000 | Loss: 0.00007622
Iteration 300/1000 | Loss: 0.00006153
Iteration 301/1000 | Loss: 0.00006409
Iteration 302/1000 | Loss: 0.00006935
Iteration 303/1000 | Loss: 0.00007209
Iteration 304/1000 | Loss: 0.00007234
Iteration 305/1000 | Loss: 0.00006327
Iteration 306/1000 | Loss: 0.00006720
Iteration 307/1000 | Loss: 0.00011885
Iteration 308/1000 | Loss: 0.00008069
Iteration 309/1000 | Loss: 0.00009975
Iteration 310/1000 | Loss: 0.00007052
Iteration 311/1000 | Loss: 0.00008804
Iteration 312/1000 | Loss: 0.00007018
Iteration 313/1000 | Loss: 0.00010257
Iteration 314/1000 | Loss: 0.00007503
Iteration 315/1000 | Loss: 0.00007504
Iteration 316/1000 | Loss: 0.00008601
Iteration 317/1000 | Loss: 0.00005694
Iteration 318/1000 | Loss: 0.00007807
Iteration 319/1000 | Loss: 0.00006476
Iteration 320/1000 | Loss: 0.00006821
Iteration 321/1000 | Loss: 0.00006557
Iteration 322/1000 | Loss: 0.00030128
Iteration 323/1000 | Loss: 0.00019815
Iteration 324/1000 | Loss: 0.00006546
Iteration 325/1000 | Loss: 0.00006489
Iteration 326/1000 | Loss: 0.00006622
Iteration 327/1000 | Loss: 0.00007120
Iteration 328/1000 | Loss: 0.00008850
Iteration 329/1000 | Loss: 0.00004945
Iteration 330/1000 | Loss: 0.00006120
Iteration 331/1000 | Loss: 0.00006694
Iteration 332/1000 | Loss: 0.00006648
Iteration 333/1000 | Loss: 0.00006299
Iteration 334/1000 | Loss: 0.00007788
Iteration 335/1000 | Loss: 0.00006540
Iteration 336/1000 | Loss: 0.00009161
Iteration 337/1000 | Loss: 0.00006438
Iteration 338/1000 | Loss: 0.00006827
Iteration 339/1000 | Loss: 0.00006787
Iteration 340/1000 | Loss: 0.00006926
Iteration 341/1000 | Loss: 0.00006879
Iteration 342/1000 | Loss: 0.00006848
Iteration 343/1000 | Loss: 0.00006770
Iteration 344/1000 | Loss: 0.00008444
Iteration 345/1000 | Loss: 0.00006855
Iteration 346/1000 | Loss: 0.00010410
Iteration 347/1000 | Loss: 0.00007746
Iteration 348/1000 | Loss: 0.00006786
Iteration 349/1000 | Loss: 0.00006531
Iteration 350/1000 | Loss: 0.00007755
Iteration 351/1000 | Loss: 0.00006344
Iteration 352/1000 | Loss: 0.00006107
Iteration 353/1000 | Loss: 0.00008482
Iteration 354/1000 | Loss: 0.00006556
Iteration 355/1000 | Loss: 0.00006283
Iteration 356/1000 | Loss: 0.00006835
Iteration 357/1000 | Loss: 0.00007051
Iteration 358/1000 | Loss: 0.00006786
Iteration 359/1000 | Loss: 0.00040321
Iteration 360/1000 | Loss: 0.00008208
Iteration 361/1000 | Loss: 0.00006961
Iteration 362/1000 | Loss: 0.00017870
Iteration 363/1000 | Loss: 0.00008117
Iteration 364/1000 | Loss: 0.00014555
Iteration 365/1000 | Loss: 0.00005003
Iteration 366/1000 | Loss: 0.00006376
Iteration 367/1000 | Loss: 0.00007152
Iteration 368/1000 | Loss: 0.00016729
Iteration 369/1000 | Loss: 0.00007948
Iteration 370/1000 | Loss: 0.00010174
Iteration 371/1000 | Loss: 0.00007799
Iteration 372/1000 | Loss: 0.00006541
Iteration 373/1000 | Loss: 0.00006419
Iteration 374/1000 | Loss: 0.00005667
Iteration 375/1000 | Loss: 0.00008634
Iteration 376/1000 | Loss: 0.00006890
Iteration 377/1000 | Loss: 0.00008711
Iteration 378/1000 | Loss: 0.00006619
Iteration 379/1000 | Loss: 0.00006668
Iteration 380/1000 | Loss: 0.00008511
Iteration 381/1000 | Loss: 0.00007272
Iteration 382/1000 | Loss: 0.00006437
Iteration 383/1000 | Loss: 0.00008211
Iteration 384/1000 | Loss: 0.00007363
Iteration 385/1000 | Loss: 0.00006357
Iteration 386/1000 | Loss: 0.00006169
Iteration 387/1000 | Loss: 0.00006032
Iteration 388/1000 | Loss: 0.00006885
Iteration 389/1000 | Loss: 0.00005962
Iteration 390/1000 | Loss: 0.00005714
Iteration 391/1000 | Loss: 0.00006686
Iteration 392/1000 | Loss: 0.00006294
Iteration 393/1000 | Loss: 0.00007077
Iteration 394/1000 | Loss: 0.00006150
Iteration 395/1000 | Loss: 0.00006764
Iteration 396/1000 | Loss: 0.00006454
Iteration 397/1000 | Loss: 0.00009371
Iteration 398/1000 | Loss: 0.00006319
Iteration 399/1000 | Loss: 0.00010604
Iteration 400/1000 | Loss: 0.00006067
Iteration 401/1000 | Loss: 0.00006179
Iteration 402/1000 | Loss: 0.00004815
Iteration 403/1000 | Loss: 0.00003570
Iteration 404/1000 | Loss: 0.00003193
Iteration 405/1000 | Loss: 0.00005270
Iteration 406/1000 | Loss: 0.00006566
Iteration 407/1000 | Loss: 0.00023186
Iteration 408/1000 | Loss: 0.00018762
Iteration 409/1000 | Loss: 0.00016350
Iteration 410/1000 | Loss: 0.00006158
Iteration 411/1000 | Loss: 0.00004991
Iteration 412/1000 | Loss: 0.00005422
Iteration 413/1000 | Loss: 0.00003950
Iteration 414/1000 | Loss: 0.00005070
Iteration 415/1000 | Loss: 0.00004955
Iteration 416/1000 | Loss: 0.00008752
Iteration 417/1000 | Loss: 0.00005198
Iteration 418/1000 | Loss: 0.00004896
Iteration 419/1000 | Loss: 0.00006783
Iteration 420/1000 | Loss: 0.00004858
Iteration 421/1000 | Loss: 0.00006305
Iteration 422/1000 | Loss: 0.00005216
Iteration 423/1000 | Loss: 0.00006075
Iteration 424/1000 | Loss: 0.00028587
Iteration 425/1000 | Loss: 0.00006903
Iteration 426/1000 | Loss: 0.00036019
Iteration 427/1000 | Loss: 0.00038755
Iteration 428/1000 | Loss: 0.00055539
Iteration 429/1000 | Loss: 0.00020682
Iteration 430/1000 | Loss: 0.00004809
Iteration 431/1000 | Loss: 0.00006292
Iteration 432/1000 | Loss: 0.00040833
Iteration 433/1000 | Loss: 0.00019306
Iteration 434/1000 | Loss: 0.00035377
Iteration 435/1000 | Loss: 0.00032863
Iteration 436/1000 | Loss: 0.00015391
Iteration 437/1000 | Loss: 0.00015102
Iteration 438/1000 | Loss: 0.00011206
Iteration 439/1000 | Loss: 0.00011369
Iteration 440/1000 | Loss: 0.00037855
Iteration 441/1000 | Loss: 0.00011672
Iteration 442/1000 | Loss: 0.00032440
Iteration 443/1000 | Loss: 0.00025265
Iteration 444/1000 | Loss: 0.00005514
Iteration 445/1000 | Loss: 0.00003309
Iteration 446/1000 | Loss: 0.00003518
Iteration 447/1000 | Loss: 0.00011188
Iteration 448/1000 | Loss: 0.00004659
Iteration 449/1000 | Loss: 0.00004611
Iteration 450/1000 | Loss: 0.00018029
Iteration 451/1000 | Loss: 0.00006642
Iteration 452/1000 | Loss: 0.00003460
Iteration 453/1000 | Loss: 0.00003625
Iteration 454/1000 | Loss: 0.00018351
Iteration 455/1000 | Loss: 0.00016867
Iteration 456/1000 | Loss: 0.00004733
Iteration 457/1000 | Loss: 0.00008746
Iteration 458/1000 | Loss: 0.00003520
Iteration 459/1000 | Loss: 0.00002489
Iteration 460/1000 | Loss: 0.00002226
Iteration 461/1000 | Loss: 0.00005092
Iteration 462/1000 | Loss: 0.00003611
Iteration 463/1000 | Loss: 0.00004554
Iteration 464/1000 | Loss: 0.00006145
Iteration 465/1000 | Loss: 0.00002649
Iteration 466/1000 | Loss: 0.00003029
Iteration 467/1000 | Loss: 0.00006526
Iteration 468/1000 | Loss: 0.00012660
Iteration 469/1000 | Loss: 0.00004503
Iteration 470/1000 | Loss: 0.00003363
Iteration 471/1000 | Loss: 0.00005773
Iteration 472/1000 | Loss: 0.00004036
Iteration 473/1000 | Loss: 0.00002369
Iteration 474/1000 | Loss: 0.00002673
Iteration 475/1000 | Loss: 0.00002624
Iteration 476/1000 | Loss: 0.00003836
Iteration 477/1000 | Loss: 0.00002187
Iteration 478/1000 | Loss: 0.00002252
Iteration 479/1000 | Loss: 0.00003053
Iteration 480/1000 | Loss: 0.00001906
Iteration 481/1000 | Loss: 0.00001898
Iteration 482/1000 | Loss: 0.00003746
Iteration 483/1000 | Loss: 0.00037088
Iteration 484/1000 | Loss: 0.00021725
Iteration 485/1000 | Loss: 0.00003437
Iteration 486/1000 | Loss: 0.00004292
Iteration 487/1000 | Loss: 0.00002152
Iteration 488/1000 | Loss: 0.00001909
Iteration 489/1000 | Loss: 0.00001804
Iteration 490/1000 | Loss: 0.00002180
Iteration 491/1000 | Loss: 0.00001940
Iteration 492/1000 | Loss: 0.00001725
Iteration 493/1000 | Loss: 0.00002165
Iteration 494/1000 | Loss: 0.00001700
Iteration 495/1000 | Loss: 0.00001699
Iteration 496/1000 | Loss: 0.00001762
Iteration 497/1000 | Loss: 0.00001688
Iteration 498/1000 | Loss: 0.00001688
Iteration 499/1000 | Loss: 0.00001688
Iteration 500/1000 | Loss: 0.00001688
Iteration 501/1000 | Loss: 0.00001688
Iteration 502/1000 | Loss: 0.00001688
Iteration 503/1000 | Loss: 0.00001688
Iteration 504/1000 | Loss: 0.00001688
Iteration 505/1000 | Loss: 0.00001688
Iteration 506/1000 | Loss: 0.00001688
Iteration 507/1000 | Loss: 0.00001688
Iteration 508/1000 | Loss: 0.00001687
Iteration 509/1000 | Loss: 0.00001687
Iteration 510/1000 | Loss: 0.00001687
Iteration 511/1000 | Loss: 0.00001687
Iteration 512/1000 | Loss: 0.00001687
Iteration 513/1000 | Loss: 0.00001687
Iteration 514/1000 | Loss: 0.00001687
Iteration 515/1000 | Loss: 0.00001686
Iteration 516/1000 | Loss: 0.00001686
Iteration 517/1000 | Loss: 0.00001686
Iteration 518/1000 | Loss: 0.00001686
Iteration 519/1000 | Loss: 0.00001685
Iteration 520/1000 | Loss: 0.00001685
Iteration 521/1000 | Loss: 0.00001684
Iteration 522/1000 | Loss: 0.00001684
Iteration 523/1000 | Loss: 0.00001683
Iteration 524/1000 | Loss: 0.00001683
Iteration 525/1000 | Loss: 0.00001683
Iteration 526/1000 | Loss: 0.00001683
Iteration 527/1000 | Loss: 0.00001682
Iteration 528/1000 | Loss: 0.00001682
Iteration 529/1000 | Loss: 0.00001682
Iteration 530/1000 | Loss: 0.00001682
Iteration 531/1000 | Loss: 0.00001682
Iteration 532/1000 | Loss: 0.00001682
Iteration 533/1000 | Loss: 0.00001682
Iteration 534/1000 | Loss: 0.00001682
Iteration 535/1000 | Loss: 0.00001682
Iteration 536/1000 | Loss: 0.00001681
Iteration 537/1000 | Loss: 0.00001681
Iteration 538/1000 | Loss: 0.00001681
Iteration 539/1000 | Loss: 0.00001681
Iteration 540/1000 | Loss: 0.00001681
Iteration 541/1000 | Loss: 0.00001681
Iteration 542/1000 | Loss: 0.00001681
Iteration 543/1000 | Loss: 0.00001681
Iteration 544/1000 | Loss: 0.00001681
Iteration 545/1000 | Loss: 0.00001681
Iteration 546/1000 | Loss: 0.00001681
Iteration 547/1000 | Loss: 0.00001681
Iteration 548/1000 | Loss: 0.00001681
Iteration 549/1000 | Loss: 0.00001681
Iteration 550/1000 | Loss: 0.00001681
Iteration 551/1000 | Loss: 0.00001681
Iteration 552/1000 | Loss: 0.00001681
Iteration 553/1000 | Loss: 0.00001681
Iteration 554/1000 | Loss: 0.00001681
Iteration 555/1000 | Loss: 0.00001681
Iteration 556/1000 | Loss: 0.00001681
Iteration 557/1000 | Loss: 0.00001681
Iteration 558/1000 | Loss: 0.00001681
Iteration 559/1000 | Loss: 0.00001681
Iteration 560/1000 | Loss: 0.00001681
Iteration 561/1000 | Loss: 0.00001681
Iteration 562/1000 | Loss: 0.00001681
Iteration 563/1000 | Loss: 0.00001681
Iteration 564/1000 | Loss: 0.00001681
Iteration 565/1000 | Loss: 0.00001681
Iteration 566/1000 | Loss: 0.00001681
Iteration 567/1000 | Loss: 0.00001681
Iteration 568/1000 | Loss: 0.00001681
Iteration 569/1000 | Loss: 0.00001681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 569. Stopping optimization.
Last 5 losses: [1.681096910033375e-05, 1.681096910033375e-05, 1.681096910033375e-05, 1.681096910033375e-05, 1.681096910033375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.681096910033375e-05

Optimization complete. Final v2v error: 3.2192864418029785 mm

Highest mean error: 10.110530853271484 mm for frame 107

Lowest mean error: 2.5835464000701904 mm for frame 93

Saving results

Total time: 769.9885749816895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803792
Iteration 2/25 | Loss: 0.00139970
Iteration 3/25 | Loss: 0.00127681
Iteration 4/25 | Loss: 0.00126337
Iteration 5/25 | Loss: 0.00126012
Iteration 6/25 | Loss: 0.00125921
Iteration 7/25 | Loss: 0.00125921
Iteration 8/25 | Loss: 0.00125921
Iteration 9/25 | Loss: 0.00125921
Iteration 10/25 | Loss: 0.00125921
Iteration 11/25 | Loss: 0.00125921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012592058628797531, 0.0012592058628797531, 0.0012592058628797531, 0.0012592058628797531, 0.0012592058628797531]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012592058628797531

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29392004
Iteration 2/25 | Loss: 0.00256450
Iteration 3/25 | Loss: 0.00256449
Iteration 4/25 | Loss: 0.00256449
Iteration 5/25 | Loss: 0.00256449
Iteration 6/25 | Loss: 0.00256449
Iteration 7/25 | Loss: 0.00256449
Iteration 8/25 | Loss: 0.00256449
Iteration 9/25 | Loss: 0.00256449
Iteration 10/25 | Loss: 0.00256449
Iteration 11/25 | Loss: 0.00256449
Iteration 12/25 | Loss: 0.00256449
Iteration 13/25 | Loss: 0.00256449
Iteration 14/25 | Loss: 0.00256449
Iteration 15/25 | Loss: 0.00256449
Iteration 16/25 | Loss: 0.00256449
Iteration 17/25 | Loss: 0.00256449
Iteration 18/25 | Loss: 0.00256449
Iteration 19/25 | Loss: 0.00256449
Iteration 20/25 | Loss: 0.00256449
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0025644893757998943, 0.0025644893757998943, 0.0025644893757998943, 0.0025644893757998943, 0.0025644893757998943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025644893757998943

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00256449
Iteration 2/1000 | Loss: 0.00015891
Iteration 3/1000 | Loss: 0.00009613
Iteration 4/1000 | Loss: 0.00007783
Iteration 5/1000 | Loss: 0.00006832
Iteration 6/1000 | Loss: 0.00006467
Iteration 7/1000 | Loss: 0.00006163
Iteration 8/1000 | Loss: 0.00005978
Iteration 9/1000 | Loss: 0.00005763
Iteration 10/1000 | Loss: 0.00005599
Iteration 11/1000 | Loss: 0.00005501
Iteration 12/1000 | Loss: 0.00005447
Iteration 13/1000 | Loss: 0.00005407
Iteration 14/1000 | Loss: 0.00005376
Iteration 15/1000 | Loss: 0.00005349
Iteration 16/1000 | Loss: 0.00005324
Iteration 17/1000 | Loss: 0.00005306
Iteration 18/1000 | Loss: 0.00005298
Iteration 19/1000 | Loss: 0.00005291
Iteration 20/1000 | Loss: 0.00005276
Iteration 21/1000 | Loss: 0.00005267
Iteration 22/1000 | Loss: 0.00005251
Iteration 23/1000 | Loss: 0.00005230
Iteration 24/1000 | Loss: 0.00005209
Iteration 25/1000 | Loss: 0.00005184
Iteration 26/1000 | Loss: 0.00005165
Iteration 27/1000 | Loss: 0.00005140
Iteration 28/1000 | Loss: 0.00005110
Iteration 29/1000 | Loss: 0.00005076
Iteration 30/1000 | Loss: 0.00020679
Iteration 31/1000 | Loss: 0.00160217
Iteration 32/1000 | Loss: 0.00265313
Iteration 33/1000 | Loss: 0.00084201
Iteration 34/1000 | Loss: 0.00075925
Iteration 35/1000 | Loss: 0.00008674
Iteration 36/1000 | Loss: 0.00010453
Iteration 37/1000 | Loss: 0.00005610
Iteration 38/1000 | Loss: 0.00011155
Iteration 39/1000 | Loss: 0.00054893
Iteration 40/1000 | Loss: 0.00004763
Iteration 41/1000 | Loss: 0.00004149
Iteration 42/1000 | Loss: 0.00003829
Iteration 43/1000 | Loss: 0.00015564
Iteration 44/1000 | Loss: 0.00015068
Iteration 45/1000 | Loss: 0.00003602
Iteration 46/1000 | Loss: 0.00014686
Iteration 47/1000 | Loss: 0.00004220
Iteration 48/1000 | Loss: 0.00003696
Iteration 49/1000 | Loss: 0.00003478
Iteration 50/1000 | Loss: 0.00004551
Iteration 51/1000 | Loss: 0.00016080
Iteration 52/1000 | Loss: 0.00004402
Iteration 53/1000 | Loss: 0.00017391
Iteration 54/1000 | Loss: 0.00023822
Iteration 55/1000 | Loss: 0.00019307
Iteration 56/1000 | Loss: 0.00005624
Iteration 57/1000 | Loss: 0.00005601
Iteration 58/1000 | Loss: 0.00003493
Iteration 59/1000 | Loss: 0.00003303
Iteration 60/1000 | Loss: 0.00003238
Iteration 61/1000 | Loss: 0.00003173
Iteration 62/1000 | Loss: 0.00003112
Iteration 63/1000 | Loss: 0.00003071
Iteration 64/1000 | Loss: 0.00003061
Iteration 65/1000 | Loss: 0.00003036
Iteration 66/1000 | Loss: 0.00003007
Iteration 67/1000 | Loss: 0.00003005
Iteration 68/1000 | Loss: 0.00002982
Iteration 69/1000 | Loss: 0.00002947
Iteration 70/1000 | Loss: 0.00002923
Iteration 71/1000 | Loss: 0.00002904
Iteration 72/1000 | Loss: 0.00002900
Iteration 73/1000 | Loss: 0.00002900
Iteration 74/1000 | Loss: 0.00002899
Iteration 75/1000 | Loss: 0.00002899
Iteration 76/1000 | Loss: 0.00002899
Iteration 77/1000 | Loss: 0.00002898
Iteration 78/1000 | Loss: 0.00002897
Iteration 79/1000 | Loss: 0.00002897
Iteration 80/1000 | Loss: 0.00002896
Iteration 81/1000 | Loss: 0.00002896
Iteration 82/1000 | Loss: 0.00002896
Iteration 83/1000 | Loss: 0.00002895
Iteration 84/1000 | Loss: 0.00002895
Iteration 85/1000 | Loss: 0.00002895
Iteration 86/1000 | Loss: 0.00002894
Iteration 87/1000 | Loss: 0.00002893
Iteration 88/1000 | Loss: 0.00002891
Iteration 89/1000 | Loss: 0.00002891
Iteration 90/1000 | Loss: 0.00002891
Iteration 91/1000 | Loss: 0.00002890
Iteration 92/1000 | Loss: 0.00002890
Iteration 93/1000 | Loss: 0.00002890
Iteration 94/1000 | Loss: 0.00002890
Iteration 95/1000 | Loss: 0.00002889
Iteration 96/1000 | Loss: 0.00002889
Iteration 97/1000 | Loss: 0.00002889
Iteration 98/1000 | Loss: 0.00002888
Iteration 99/1000 | Loss: 0.00002888
Iteration 100/1000 | Loss: 0.00002888
Iteration 101/1000 | Loss: 0.00002888
Iteration 102/1000 | Loss: 0.00002887
Iteration 103/1000 | Loss: 0.00002886
Iteration 104/1000 | Loss: 0.00002886
Iteration 105/1000 | Loss: 0.00002886
Iteration 106/1000 | Loss: 0.00002886
Iteration 107/1000 | Loss: 0.00002885
Iteration 108/1000 | Loss: 0.00002884
Iteration 109/1000 | Loss: 0.00002883
Iteration 110/1000 | Loss: 0.00002882
Iteration 111/1000 | Loss: 0.00002880
Iteration 112/1000 | Loss: 0.00002880
Iteration 113/1000 | Loss: 0.00002879
Iteration 114/1000 | Loss: 0.00002879
Iteration 115/1000 | Loss: 0.00002877
Iteration 116/1000 | Loss: 0.00002877
Iteration 117/1000 | Loss: 0.00002877
Iteration 118/1000 | Loss: 0.00002877
Iteration 119/1000 | Loss: 0.00002877
Iteration 120/1000 | Loss: 0.00002877
Iteration 121/1000 | Loss: 0.00002877
Iteration 122/1000 | Loss: 0.00002877
Iteration 123/1000 | Loss: 0.00002876
Iteration 124/1000 | Loss: 0.00002876
Iteration 125/1000 | Loss: 0.00002876
Iteration 126/1000 | Loss: 0.00002876
Iteration 127/1000 | Loss: 0.00002874
Iteration 128/1000 | Loss: 0.00002874
Iteration 129/1000 | Loss: 0.00002874
Iteration 130/1000 | Loss: 0.00002873
Iteration 131/1000 | Loss: 0.00002871
Iteration 132/1000 | Loss: 0.00002871
Iteration 133/1000 | Loss: 0.00002866
Iteration 134/1000 | Loss: 0.00002866
Iteration 135/1000 | Loss: 0.00002864
Iteration 136/1000 | Loss: 0.00002863
Iteration 137/1000 | Loss: 0.00002860
Iteration 138/1000 | Loss: 0.00002850
Iteration 139/1000 | Loss: 0.00002850
Iteration 140/1000 | Loss: 0.00002849
Iteration 141/1000 | Loss: 0.00002848
Iteration 142/1000 | Loss: 0.00002848
Iteration 143/1000 | Loss: 0.00002848
Iteration 144/1000 | Loss: 0.00002847
Iteration 145/1000 | Loss: 0.00002847
Iteration 146/1000 | Loss: 0.00002846
Iteration 147/1000 | Loss: 0.00002846
Iteration 148/1000 | Loss: 0.00002845
Iteration 149/1000 | Loss: 0.00002844
Iteration 150/1000 | Loss: 0.00002844
Iteration 151/1000 | Loss: 0.00002844
Iteration 152/1000 | Loss: 0.00002843
Iteration 153/1000 | Loss: 0.00002842
Iteration 154/1000 | Loss: 0.00002842
Iteration 155/1000 | Loss: 0.00002841
Iteration 156/1000 | Loss: 0.00002841
Iteration 157/1000 | Loss: 0.00002841
Iteration 158/1000 | Loss: 0.00002840
Iteration 159/1000 | Loss: 0.00002840
Iteration 160/1000 | Loss: 0.00002839
Iteration 161/1000 | Loss: 0.00002839
Iteration 162/1000 | Loss: 0.00002838
Iteration 163/1000 | Loss: 0.00002838
Iteration 164/1000 | Loss: 0.00002837
Iteration 165/1000 | Loss: 0.00002837
Iteration 166/1000 | Loss: 0.00002836
Iteration 167/1000 | Loss: 0.00002836
Iteration 168/1000 | Loss: 0.00002836
Iteration 169/1000 | Loss: 0.00002836
Iteration 170/1000 | Loss: 0.00002835
Iteration 171/1000 | Loss: 0.00002835
Iteration 172/1000 | Loss: 0.00002834
Iteration 173/1000 | Loss: 0.00002834
Iteration 174/1000 | Loss: 0.00002832
Iteration 175/1000 | Loss: 0.00002832
Iteration 176/1000 | Loss: 0.00002831
Iteration 177/1000 | Loss: 0.00002831
Iteration 178/1000 | Loss: 0.00002830
Iteration 179/1000 | Loss: 0.00002830
Iteration 180/1000 | Loss: 0.00002830
Iteration 181/1000 | Loss: 0.00002829
Iteration 182/1000 | Loss: 0.00002829
Iteration 183/1000 | Loss: 0.00002828
Iteration 184/1000 | Loss: 0.00002828
Iteration 185/1000 | Loss: 0.00002828
Iteration 186/1000 | Loss: 0.00002827
Iteration 187/1000 | Loss: 0.00002827
Iteration 188/1000 | Loss: 0.00002826
Iteration 189/1000 | Loss: 0.00002826
Iteration 190/1000 | Loss: 0.00002826
Iteration 191/1000 | Loss: 0.00002826
Iteration 192/1000 | Loss: 0.00002826
Iteration 193/1000 | Loss: 0.00002825
Iteration 194/1000 | Loss: 0.00002825
Iteration 195/1000 | Loss: 0.00002825
Iteration 196/1000 | Loss: 0.00002825
Iteration 197/1000 | Loss: 0.00002825
Iteration 198/1000 | Loss: 0.00002824
Iteration 199/1000 | Loss: 0.00002824
Iteration 200/1000 | Loss: 0.00002824
Iteration 201/1000 | Loss: 0.00002824
Iteration 202/1000 | Loss: 0.00002824
Iteration 203/1000 | Loss: 0.00002824
Iteration 204/1000 | Loss: 0.00002824
Iteration 205/1000 | Loss: 0.00002824
Iteration 206/1000 | Loss: 0.00002823
Iteration 207/1000 | Loss: 0.00002823
Iteration 208/1000 | Loss: 0.00002823
Iteration 209/1000 | Loss: 0.00002823
Iteration 210/1000 | Loss: 0.00002823
Iteration 211/1000 | Loss: 0.00002823
Iteration 212/1000 | Loss: 0.00002823
Iteration 213/1000 | Loss: 0.00002823
Iteration 214/1000 | Loss: 0.00002823
Iteration 215/1000 | Loss: 0.00002823
Iteration 216/1000 | Loss: 0.00002822
Iteration 217/1000 | Loss: 0.00002822
Iteration 218/1000 | Loss: 0.00002822
Iteration 219/1000 | Loss: 0.00002822
Iteration 220/1000 | Loss: 0.00002822
Iteration 221/1000 | Loss: 0.00002822
Iteration 222/1000 | Loss: 0.00002822
Iteration 223/1000 | Loss: 0.00002822
Iteration 224/1000 | Loss: 0.00002822
Iteration 225/1000 | Loss: 0.00002822
Iteration 226/1000 | Loss: 0.00002822
Iteration 227/1000 | Loss: 0.00002821
Iteration 228/1000 | Loss: 0.00002821
Iteration 229/1000 | Loss: 0.00002821
Iteration 230/1000 | Loss: 0.00002821
Iteration 231/1000 | Loss: 0.00002821
Iteration 232/1000 | Loss: 0.00002821
Iteration 233/1000 | Loss: 0.00002821
Iteration 234/1000 | Loss: 0.00002821
Iteration 235/1000 | Loss: 0.00002821
Iteration 236/1000 | Loss: 0.00002821
Iteration 237/1000 | Loss: 0.00002821
Iteration 238/1000 | Loss: 0.00002821
Iteration 239/1000 | Loss: 0.00002820
Iteration 240/1000 | Loss: 0.00002820
Iteration 241/1000 | Loss: 0.00002820
Iteration 242/1000 | Loss: 0.00002820
Iteration 243/1000 | Loss: 0.00002820
Iteration 244/1000 | Loss: 0.00002820
Iteration 245/1000 | Loss: 0.00002820
Iteration 246/1000 | Loss: 0.00002820
Iteration 247/1000 | Loss: 0.00002820
Iteration 248/1000 | Loss: 0.00002820
Iteration 249/1000 | Loss: 0.00002820
Iteration 250/1000 | Loss: 0.00002820
Iteration 251/1000 | Loss: 0.00002820
Iteration 252/1000 | Loss: 0.00002820
Iteration 253/1000 | Loss: 0.00002820
Iteration 254/1000 | Loss: 0.00002820
Iteration 255/1000 | Loss: 0.00002820
Iteration 256/1000 | Loss: 0.00002820
Iteration 257/1000 | Loss: 0.00002820
Iteration 258/1000 | Loss: 0.00002819
Iteration 259/1000 | Loss: 0.00002819
Iteration 260/1000 | Loss: 0.00002819
Iteration 261/1000 | Loss: 0.00002819
Iteration 262/1000 | Loss: 0.00002819
Iteration 263/1000 | Loss: 0.00002819
Iteration 264/1000 | Loss: 0.00002819
Iteration 265/1000 | Loss: 0.00002819
Iteration 266/1000 | Loss: 0.00002819
Iteration 267/1000 | Loss: 0.00002819
Iteration 268/1000 | Loss: 0.00002819
Iteration 269/1000 | Loss: 0.00002819
Iteration 270/1000 | Loss: 0.00002819
Iteration 271/1000 | Loss: 0.00002819
Iteration 272/1000 | Loss: 0.00002819
Iteration 273/1000 | Loss: 0.00002819
Iteration 274/1000 | Loss: 0.00002818
Iteration 275/1000 | Loss: 0.00002818
Iteration 276/1000 | Loss: 0.00002818
Iteration 277/1000 | Loss: 0.00002818
Iteration 278/1000 | Loss: 0.00002818
Iteration 279/1000 | Loss: 0.00002818
Iteration 280/1000 | Loss: 0.00002818
Iteration 281/1000 | Loss: 0.00002818
Iteration 282/1000 | Loss: 0.00002818
Iteration 283/1000 | Loss: 0.00002818
Iteration 284/1000 | Loss: 0.00002818
Iteration 285/1000 | Loss: 0.00002818
Iteration 286/1000 | Loss: 0.00002818
Iteration 287/1000 | Loss: 0.00002818
Iteration 288/1000 | Loss: 0.00002818
Iteration 289/1000 | Loss: 0.00002818
Iteration 290/1000 | Loss: 0.00002818
Iteration 291/1000 | Loss: 0.00002818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 291. Stopping optimization.
Last 5 losses: [2.817941458488349e-05, 2.817941458488349e-05, 2.817941458488349e-05, 2.817941458488349e-05, 2.817941458488349e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.817941458488349e-05

Optimization complete. Final v2v error: 2.998029947280884 mm

Highest mean error: 11.28184700012207 mm for frame 104

Lowest mean error: 2.2537660598754883 mm for frame 54

Saving results

Total time: 131.3964602947235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00551102
Iteration 2/25 | Loss: 0.00141614
Iteration 3/25 | Loss: 0.00125989
Iteration 4/25 | Loss: 0.00124686
Iteration 5/25 | Loss: 0.00124319
Iteration 6/25 | Loss: 0.00124302
Iteration 7/25 | Loss: 0.00124302
Iteration 8/25 | Loss: 0.00124302
Iteration 9/25 | Loss: 0.00124302
Iteration 10/25 | Loss: 0.00124302
Iteration 11/25 | Loss: 0.00124302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012430241331458092, 0.0012430241331458092, 0.0012430241331458092, 0.0012430241331458092, 0.0012430241331458092]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012430241331458092

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64944518
Iteration 2/25 | Loss: 0.00173594
Iteration 3/25 | Loss: 0.00173591
Iteration 4/25 | Loss: 0.00173591
Iteration 5/25 | Loss: 0.00173591
Iteration 6/25 | Loss: 0.00173591
Iteration 7/25 | Loss: 0.00173591
Iteration 8/25 | Loss: 0.00173591
Iteration 9/25 | Loss: 0.00173591
Iteration 10/25 | Loss: 0.00173591
Iteration 11/25 | Loss: 0.00173591
Iteration 12/25 | Loss: 0.00173591
Iteration 13/25 | Loss: 0.00173591
Iteration 14/25 | Loss: 0.00173591
Iteration 15/25 | Loss: 0.00173591
Iteration 16/25 | Loss: 0.00173591
Iteration 17/25 | Loss: 0.00173591
Iteration 18/25 | Loss: 0.00173591
Iteration 19/25 | Loss: 0.00173591
Iteration 20/25 | Loss: 0.00173591
Iteration 21/25 | Loss: 0.00173591
Iteration 22/25 | Loss: 0.00173591
Iteration 23/25 | Loss: 0.00173591
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001735908561386168, 0.001735908561386168, 0.001735908561386168, 0.001735908561386168, 0.001735908561386168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001735908561386168

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173591
Iteration 2/1000 | Loss: 0.00004858
Iteration 3/1000 | Loss: 0.00002919
Iteration 4/1000 | Loss: 0.00002592
Iteration 5/1000 | Loss: 0.00002430
Iteration 6/1000 | Loss: 0.00002284
Iteration 7/1000 | Loss: 0.00002189
Iteration 8/1000 | Loss: 0.00002115
Iteration 9/1000 | Loss: 0.00002069
Iteration 10/1000 | Loss: 0.00002027
Iteration 11/1000 | Loss: 0.00001989
Iteration 12/1000 | Loss: 0.00001966
Iteration 13/1000 | Loss: 0.00001954
Iteration 14/1000 | Loss: 0.00001937
Iteration 15/1000 | Loss: 0.00001927
Iteration 16/1000 | Loss: 0.00001922
Iteration 17/1000 | Loss: 0.00001922
Iteration 18/1000 | Loss: 0.00001919
Iteration 19/1000 | Loss: 0.00001916
Iteration 20/1000 | Loss: 0.00001915
Iteration 21/1000 | Loss: 0.00001913
Iteration 22/1000 | Loss: 0.00001913
Iteration 23/1000 | Loss: 0.00001912
Iteration 24/1000 | Loss: 0.00001912
Iteration 25/1000 | Loss: 0.00001908
Iteration 26/1000 | Loss: 0.00001905
Iteration 27/1000 | Loss: 0.00001904
Iteration 28/1000 | Loss: 0.00001904
Iteration 29/1000 | Loss: 0.00001904
Iteration 30/1000 | Loss: 0.00001903
Iteration 31/1000 | Loss: 0.00001902
Iteration 32/1000 | Loss: 0.00001902
Iteration 33/1000 | Loss: 0.00001901
Iteration 34/1000 | Loss: 0.00001901
Iteration 35/1000 | Loss: 0.00001900
Iteration 36/1000 | Loss: 0.00001900
Iteration 37/1000 | Loss: 0.00001899
Iteration 38/1000 | Loss: 0.00001898
Iteration 39/1000 | Loss: 0.00001897
Iteration 40/1000 | Loss: 0.00001897
Iteration 41/1000 | Loss: 0.00001897
Iteration 42/1000 | Loss: 0.00001896
Iteration 43/1000 | Loss: 0.00001896
Iteration 44/1000 | Loss: 0.00001896
Iteration 45/1000 | Loss: 0.00001895
Iteration 46/1000 | Loss: 0.00001895
Iteration 47/1000 | Loss: 0.00001894
Iteration 48/1000 | Loss: 0.00001894
Iteration 49/1000 | Loss: 0.00001893
Iteration 50/1000 | Loss: 0.00001893
Iteration 51/1000 | Loss: 0.00001892
Iteration 52/1000 | Loss: 0.00001891
Iteration 53/1000 | Loss: 0.00001891
Iteration 54/1000 | Loss: 0.00001890
Iteration 55/1000 | Loss: 0.00001890
Iteration 56/1000 | Loss: 0.00001890
Iteration 57/1000 | Loss: 0.00001889
Iteration 58/1000 | Loss: 0.00001887
Iteration 59/1000 | Loss: 0.00001887
Iteration 60/1000 | Loss: 0.00001887
Iteration 61/1000 | Loss: 0.00001887
Iteration 62/1000 | Loss: 0.00001887
Iteration 63/1000 | Loss: 0.00001887
Iteration 64/1000 | Loss: 0.00001887
Iteration 65/1000 | Loss: 0.00001887
Iteration 66/1000 | Loss: 0.00001887
Iteration 67/1000 | Loss: 0.00001887
Iteration 68/1000 | Loss: 0.00001886
Iteration 69/1000 | Loss: 0.00001886
Iteration 70/1000 | Loss: 0.00001886
Iteration 71/1000 | Loss: 0.00001885
Iteration 72/1000 | Loss: 0.00001885
Iteration 73/1000 | Loss: 0.00001884
Iteration 74/1000 | Loss: 0.00001884
Iteration 75/1000 | Loss: 0.00001884
Iteration 76/1000 | Loss: 0.00001884
Iteration 77/1000 | Loss: 0.00001883
Iteration 78/1000 | Loss: 0.00001883
Iteration 79/1000 | Loss: 0.00001883
Iteration 80/1000 | Loss: 0.00001882
Iteration 81/1000 | Loss: 0.00001882
Iteration 82/1000 | Loss: 0.00001882
Iteration 83/1000 | Loss: 0.00001882
Iteration 84/1000 | Loss: 0.00001882
Iteration 85/1000 | Loss: 0.00001882
Iteration 86/1000 | Loss: 0.00001882
Iteration 87/1000 | Loss: 0.00001881
Iteration 88/1000 | Loss: 0.00001881
Iteration 89/1000 | Loss: 0.00001881
Iteration 90/1000 | Loss: 0.00001881
Iteration 91/1000 | Loss: 0.00001881
Iteration 92/1000 | Loss: 0.00001880
Iteration 93/1000 | Loss: 0.00001880
Iteration 94/1000 | Loss: 0.00001880
Iteration 95/1000 | Loss: 0.00001879
Iteration 96/1000 | Loss: 0.00001879
Iteration 97/1000 | Loss: 0.00001878
Iteration 98/1000 | Loss: 0.00001878
Iteration 99/1000 | Loss: 0.00001878
Iteration 100/1000 | Loss: 0.00001878
Iteration 101/1000 | Loss: 0.00001878
Iteration 102/1000 | Loss: 0.00001878
Iteration 103/1000 | Loss: 0.00001878
Iteration 104/1000 | Loss: 0.00001878
Iteration 105/1000 | Loss: 0.00001877
Iteration 106/1000 | Loss: 0.00001877
Iteration 107/1000 | Loss: 0.00001877
Iteration 108/1000 | Loss: 0.00001877
Iteration 109/1000 | Loss: 0.00001877
Iteration 110/1000 | Loss: 0.00001877
Iteration 111/1000 | Loss: 0.00001877
Iteration 112/1000 | Loss: 0.00001876
Iteration 113/1000 | Loss: 0.00001876
Iteration 114/1000 | Loss: 0.00001876
Iteration 115/1000 | Loss: 0.00001876
Iteration 116/1000 | Loss: 0.00001875
Iteration 117/1000 | Loss: 0.00001875
Iteration 118/1000 | Loss: 0.00001875
Iteration 119/1000 | Loss: 0.00001875
Iteration 120/1000 | Loss: 0.00001875
Iteration 121/1000 | Loss: 0.00001874
Iteration 122/1000 | Loss: 0.00001874
Iteration 123/1000 | Loss: 0.00001874
Iteration 124/1000 | Loss: 0.00001873
Iteration 125/1000 | Loss: 0.00001873
Iteration 126/1000 | Loss: 0.00001873
Iteration 127/1000 | Loss: 0.00001872
Iteration 128/1000 | Loss: 0.00001872
Iteration 129/1000 | Loss: 0.00001872
Iteration 130/1000 | Loss: 0.00001872
Iteration 131/1000 | Loss: 0.00001871
Iteration 132/1000 | Loss: 0.00001871
Iteration 133/1000 | Loss: 0.00001871
Iteration 134/1000 | Loss: 0.00001871
Iteration 135/1000 | Loss: 0.00001871
Iteration 136/1000 | Loss: 0.00001870
Iteration 137/1000 | Loss: 0.00001870
Iteration 138/1000 | Loss: 0.00001870
Iteration 139/1000 | Loss: 0.00001869
Iteration 140/1000 | Loss: 0.00001869
Iteration 141/1000 | Loss: 0.00001869
Iteration 142/1000 | Loss: 0.00001869
Iteration 143/1000 | Loss: 0.00001868
Iteration 144/1000 | Loss: 0.00001868
Iteration 145/1000 | Loss: 0.00001868
Iteration 146/1000 | Loss: 0.00001867
Iteration 147/1000 | Loss: 0.00001867
Iteration 148/1000 | Loss: 0.00001867
Iteration 149/1000 | Loss: 0.00001867
Iteration 150/1000 | Loss: 0.00001867
Iteration 151/1000 | Loss: 0.00001866
Iteration 152/1000 | Loss: 0.00001866
Iteration 153/1000 | Loss: 0.00001866
Iteration 154/1000 | Loss: 0.00001866
Iteration 155/1000 | Loss: 0.00001866
Iteration 156/1000 | Loss: 0.00001865
Iteration 157/1000 | Loss: 0.00001865
Iteration 158/1000 | Loss: 0.00001865
Iteration 159/1000 | Loss: 0.00001865
Iteration 160/1000 | Loss: 0.00001865
Iteration 161/1000 | Loss: 0.00001865
Iteration 162/1000 | Loss: 0.00001865
Iteration 163/1000 | Loss: 0.00001864
Iteration 164/1000 | Loss: 0.00001864
Iteration 165/1000 | Loss: 0.00001864
Iteration 166/1000 | Loss: 0.00001864
Iteration 167/1000 | Loss: 0.00001863
Iteration 168/1000 | Loss: 0.00001863
Iteration 169/1000 | Loss: 0.00001863
Iteration 170/1000 | Loss: 0.00001863
Iteration 171/1000 | Loss: 0.00001863
Iteration 172/1000 | Loss: 0.00001863
Iteration 173/1000 | Loss: 0.00001863
Iteration 174/1000 | Loss: 0.00001863
Iteration 175/1000 | Loss: 0.00001863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.863011493696831e-05, 1.863011493696831e-05, 1.863011493696831e-05, 1.863011493696831e-05, 1.863011493696831e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.863011493696831e-05

Optimization complete. Final v2v error: 3.5454208850860596 mm

Highest mean error: 4.281455039978027 mm for frame 226

Lowest mean error: 3.1297988891601562 mm for frame 203

Saving results

Total time: 47.838791608810425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424653
Iteration 2/25 | Loss: 0.00127209
Iteration 3/25 | Loss: 0.00119422
Iteration 4/25 | Loss: 0.00118852
Iteration 5/25 | Loss: 0.00118681
Iteration 6/25 | Loss: 0.00118653
Iteration 7/25 | Loss: 0.00118653
Iteration 8/25 | Loss: 0.00118653
Iteration 9/25 | Loss: 0.00118653
Iteration 10/25 | Loss: 0.00118653
Iteration 11/25 | Loss: 0.00118653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011865299893543124, 0.0011865299893543124, 0.0011865299893543124, 0.0011865299893543124, 0.0011865299893543124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011865299893543124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.52726030
Iteration 2/25 | Loss: 0.00128307
Iteration 3/25 | Loss: 0.00128306
Iteration 4/25 | Loss: 0.00128306
Iteration 5/25 | Loss: 0.00128306
Iteration 6/25 | Loss: 0.00128306
Iteration 7/25 | Loss: 0.00128306
Iteration 8/25 | Loss: 0.00128306
Iteration 9/25 | Loss: 0.00128306
Iteration 10/25 | Loss: 0.00128306
Iteration 11/25 | Loss: 0.00128306
Iteration 12/25 | Loss: 0.00128306
Iteration 13/25 | Loss: 0.00128306
Iteration 14/25 | Loss: 0.00128306
Iteration 15/25 | Loss: 0.00128306
Iteration 16/25 | Loss: 0.00128306
Iteration 17/25 | Loss: 0.00128306
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012830597115680575, 0.0012830597115680575, 0.0012830597115680575, 0.0012830597115680575, 0.0012830597115680575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012830597115680575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128306
Iteration 2/1000 | Loss: 0.00002287
Iteration 3/1000 | Loss: 0.00001662
Iteration 4/1000 | Loss: 0.00001501
Iteration 5/1000 | Loss: 0.00001422
Iteration 6/1000 | Loss: 0.00001360
Iteration 7/1000 | Loss: 0.00001312
Iteration 8/1000 | Loss: 0.00001278
Iteration 9/1000 | Loss: 0.00001253
Iteration 10/1000 | Loss: 0.00001223
Iteration 11/1000 | Loss: 0.00001208
Iteration 12/1000 | Loss: 0.00001203
Iteration 13/1000 | Loss: 0.00001198
Iteration 14/1000 | Loss: 0.00001197
Iteration 15/1000 | Loss: 0.00001187
Iteration 16/1000 | Loss: 0.00001181
Iteration 17/1000 | Loss: 0.00001172
Iteration 18/1000 | Loss: 0.00001167
Iteration 19/1000 | Loss: 0.00001167
Iteration 20/1000 | Loss: 0.00001166
Iteration 21/1000 | Loss: 0.00001165
Iteration 22/1000 | Loss: 0.00001162
Iteration 23/1000 | Loss: 0.00001158
Iteration 24/1000 | Loss: 0.00001154
Iteration 25/1000 | Loss: 0.00001154
Iteration 26/1000 | Loss: 0.00001153
Iteration 27/1000 | Loss: 0.00001153
Iteration 28/1000 | Loss: 0.00001153
Iteration 29/1000 | Loss: 0.00001153
Iteration 30/1000 | Loss: 0.00001152
Iteration 31/1000 | Loss: 0.00001151
Iteration 32/1000 | Loss: 0.00001151
Iteration 33/1000 | Loss: 0.00001150
Iteration 34/1000 | Loss: 0.00001150
Iteration 35/1000 | Loss: 0.00001149
Iteration 36/1000 | Loss: 0.00001149
Iteration 37/1000 | Loss: 0.00001148
Iteration 38/1000 | Loss: 0.00001147
Iteration 39/1000 | Loss: 0.00001147
Iteration 40/1000 | Loss: 0.00001146
Iteration 41/1000 | Loss: 0.00001146
Iteration 42/1000 | Loss: 0.00001146
Iteration 43/1000 | Loss: 0.00001145
Iteration 44/1000 | Loss: 0.00001145
Iteration 45/1000 | Loss: 0.00001144
Iteration 46/1000 | Loss: 0.00001144
Iteration 47/1000 | Loss: 0.00001144
Iteration 48/1000 | Loss: 0.00001143
Iteration 49/1000 | Loss: 0.00001143
Iteration 50/1000 | Loss: 0.00001142
Iteration 51/1000 | Loss: 0.00001142
Iteration 52/1000 | Loss: 0.00001142
Iteration 53/1000 | Loss: 0.00001141
Iteration 54/1000 | Loss: 0.00001140
Iteration 55/1000 | Loss: 0.00001140
Iteration 56/1000 | Loss: 0.00001139
Iteration 57/1000 | Loss: 0.00001139
Iteration 58/1000 | Loss: 0.00001137
Iteration 59/1000 | Loss: 0.00001137
Iteration 60/1000 | Loss: 0.00001137
Iteration 61/1000 | Loss: 0.00001136
Iteration 62/1000 | Loss: 0.00001136
Iteration 63/1000 | Loss: 0.00001135
Iteration 64/1000 | Loss: 0.00001134
Iteration 65/1000 | Loss: 0.00001134
Iteration 66/1000 | Loss: 0.00001133
Iteration 67/1000 | Loss: 0.00001133
Iteration 68/1000 | Loss: 0.00001133
Iteration 69/1000 | Loss: 0.00001133
Iteration 70/1000 | Loss: 0.00001133
Iteration 71/1000 | Loss: 0.00001133
Iteration 72/1000 | Loss: 0.00001133
Iteration 73/1000 | Loss: 0.00001132
Iteration 74/1000 | Loss: 0.00001132
Iteration 75/1000 | Loss: 0.00001131
Iteration 76/1000 | Loss: 0.00001131
Iteration 77/1000 | Loss: 0.00001131
Iteration 78/1000 | Loss: 0.00001131
Iteration 79/1000 | Loss: 0.00001130
Iteration 80/1000 | Loss: 0.00001130
Iteration 81/1000 | Loss: 0.00001130
Iteration 82/1000 | Loss: 0.00001130
Iteration 83/1000 | Loss: 0.00001130
Iteration 84/1000 | Loss: 0.00001130
Iteration 85/1000 | Loss: 0.00001130
Iteration 86/1000 | Loss: 0.00001129
Iteration 87/1000 | Loss: 0.00001129
Iteration 88/1000 | Loss: 0.00001129
Iteration 89/1000 | Loss: 0.00001128
Iteration 90/1000 | Loss: 0.00001128
Iteration 91/1000 | Loss: 0.00001128
Iteration 92/1000 | Loss: 0.00001128
Iteration 93/1000 | Loss: 0.00001128
Iteration 94/1000 | Loss: 0.00001128
Iteration 95/1000 | Loss: 0.00001128
Iteration 96/1000 | Loss: 0.00001127
Iteration 97/1000 | Loss: 0.00001127
Iteration 98/1000 | Loss: 0.00001127
Iteration 99/1000 | Loss: 0.00001127
Iteration 100/1000 | Loss: 0.00001127
Iteration 101/1000 | Loss: 0.00001127
Iteration 102/1000 | Loss: 0.00001127
Iteration 103/1000 | Loss: 0.00001127
Iteration 104/1000 | Loss: 0.00001126
Iteration 105/1000 | Loss: 0.00001126
Iteration 106/1000 | Loss: 0.00001126
Iteration 107/1000 | Loss: 0.00001126
Iteration 108/1000 | Loss: 0.00001126
Iteration 109/1000 | Loss: 0.00001126
Iteration 110/1000 | Loss: 0.00001126
Iteration 111/1000 | Loss: 0.00001126
Iteration 112/1000 | Loss: 0.00001126
Iteration 113/1000 | Loss: 0.00001126
Iteration 114/1000 | Loss: 0.00001126
Iteration 115/1000 | Loss: 0.00001126
Iteration 116/1000 | Loss: 0.00001126
Iteration 117/1000 | Loss: 0.00001125
Iteration 118/1000 | Loss: 0.00001125
Iteration 119/1000 | Loss: 0.00001125
Iteration 120/1000 | Loss: 0.00001125
Iteration 121/1000 | Loss: 0.00001125
Iteration 122/1000 | Loss: 0.00001125
Iteration 123/1000 | Loss: 0.00001125
Iteration 124/1000 | Loss: 0.00001125
Iteration 125/1000 | Loss: 0.00001125
Iteration 126/1000 | Loss: 0.00001125
Iteration 127/1000 | Loss: 0.00001125
Iteration 128/1000 | Loss: 0.00001125
Iteration 129/1000 | Loss: 0.00001125
Iteration 130/1000 | Loss: 0.00001125
Iteration 131/1000 | Loss: 0.00001124
Iteration 132/1000 | Loss: 0.00001124
Iteration 133/1000 | Loss: 0.00001124
Iteration 134/1000 | Loss: 0.00001124
Iteration 135/1000 | Loss: 0.00001124
Iteration 136/1000 | Loss: 0.00001124
Iteration 137/1000 | Loss: 0.00001124
Iteration 138/1000 | Loss: 0.00001124
Iteration 139/1000 | Loss: 0.00001124
Iteration 140/1000 | Loss: 0.00001124
Iteration 141/1000 | Loss: 0.00001124
Iteration 142/1000 | Loss: 0.00001124
Iteration 143/1000 | Loss: 0.00001124
Iteration 144/1000 | Loss: 0.00001124
Iteration 145/1000 | Loss: 0.00001124
Iteration 146/1000 | Loss: 0.00001124
Iteration 147/1000 | Loss: 0.00001124
Iteration 148/1000 | Loss: 0.00001124
Iteration 149/1000 | Loss: 0.00001124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.1244162124057766e-05, 1.1244162124057766e-05, 1.1244162124057766e-05, 1.1244162124057766e-05, 1.1244162124057766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1244162124057766e-05

Optimization complete. Final v2v error: 2.882209300994873 mm

Highest mean error: 3.4215424060821533 mm for frame 82

Lowest mean error: 2.555096387863159 mm for frame 34

Saving results

Total time: 37.67026615142822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769653
Iteration 2/25 | Loss: 0.00165242
Iteration 3/25 | Loss: 0.00139511
Iteration 4/25 | Loss: 0.00137032
Iteration 5/25 | Loss: 0.00136648
Iteration 6/25 | Loss: 0.00136648
Iteration 7/25 | Loss: 0.00136648
Iteration 8/25 | Loss: 0.00136648
Iteration 9/25 | Loss: 0.00136648
Iteration 10/25 | Loss: 0.00136648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013664804864674807, 0.0013664804864674807, 0.0013664804864674807, 0.0013664804864674807, 0.0013664804864674807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013664804864674807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.11312652
Iteration 2/25 | Loss: 0.00136321
Iteration 3/25 | Loss: 0.00136318
Iteration 4/25 | Loss: 0.00136318
Iteration 5/25 | Loss: 0.00136318
Iteration 6/25 | Loss: 0.00136317
Iteration 7/25 | Loss: 0.00136317
Iteration 8/25 | Loss: 0.00136317
Iteration 9/25 | Loss: 0.00136317
Iteration 10/25 | Loss: 0.00136317
Iteration 11/25 | Loss: 0.00136317
Iteration 12/25 | Loss: 0.00136317
Iteration 13/25 | Loss: 0.00136317
Iteration 14/25 | Loss: 0.00136317
Iteration 15/25 | Loss: 0.00136317
Iteration 16/25 | Loss: 0.00136317
Iteration 17/25 | Loss: 0.00136317
Iteration 18/25 | Loss: 0.00136317
Iteration 19/25 | Loss: 0.00136317
Iteration 20/25 | Loss: 0.00136317
Iteration 21/25 | Loss: 0.00136317
Iteration 22/25 | Loss: 0.00136317
Iteration 23/25 | Loss: 0.00136317
Iteration 24/25 | Loss: 0.00136317
Iteration 25/25 | Loss: 0.00136317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136317
Iteration 2/1000 | Loss: 0.00004640
Iteration 3/1000 | Loss: 0.00003284
Iteration 4/1000 | Loss: 0.00002973
Iteration 5/1000 | Loss: 0.00002822
Iteration 6/1000 | Loss: 0.00002738
Iteration 7/1000 | Loss: 0.00002679
Iteration 8/1000 | Loss: 0.00002634
Iteration 9/1000 | Loss: 0.00002592
Iteration 10/1000 | Loss: 0.00002544
Iteration 11/1000 | Loss: 0.00002502
Iteration 12/1000 | Loss: 0.00002464
Iteration 13/1000 | Loss: 0.00002436
Iteration 14/1000 | Loss: 0.00002409
Iteration 15/1000 | Loss: 0.00002389
Iteration 16/1000 | Loss: 0.00002373
Iteration 17/1000 | Loss: 0.00002358
Iteration 18/1000 | Loss: 0.00002357
Iteration 19/1000 | Loss: 0.00002351
Iteration 20/1000 | Loss: 0.00002342
Iteration 21/1000 | Loss: 0.00002341
Iteration 22/1000 | Loss: 0.00002340
Iteration 23/1000 | Loss: 0.00002340
Iteration 24/1000 | Loss: 0.00002339
Iteration 25/1000 | Loss: 0.00002339
Iteration 26/1000 | Loss: 0.00002339
Iteration 27/1000 | Loss: 0.00002338
Iteration 28/1000 | Loss: 0.00002338
Iteration 29/1000 | Loss: 0.00002338
Iteration 30/1000 | Loss: 0.00002336
Iteration 31/1000 | Loss: 0.00002336
Iteration 32/1000 | Loss: 0.00002336
Iteration 33/1000 | Loss: 0.00002336
Iteration 34/1000 | Loss: 0.00002336
Iteration 35/1000 | Loss: 0.00002335
Iteration 36/1000 | Loss: 0.00002334
Iteration 37/1000 | Loss: 0.00002333
Iteration 38/1000 | Loss: 0.00002332
Iteration 39/1000 | Loss: 0.00002332
Iteration 40/1000 | Loss: 0.00002332
Iteration 41/1000 | Loss: 0.00002331
Iteration 42/1000 | Loss: 0.00002330
Iteration 43/1000 | Loss: 0.00002330
Iteration 44/1000 | Loss: 0.00002330
Iteration 45/1000 | Loss: 0.00002330
Iteration 46/1000 | Loss: 0.00002329
Iteration 47/1000 | Loss: 0.00002329
Iteration 48/1000 | Loss: 0.00002329
Iteration 49/1000 | Loss: 0.00002329
Iteration 50/1000 | Loss: 0.00002329
Iteration 51/1000 | Loss: 0.00002328
Iteration 52/1000 | Loss: 0.00002328
Iteration 53/1000 | Loss: 0.00002328
Iteration 54/1000 | Loss: 0.00002328
Iteration 55/1000 | Loss: 0.00002328
Iteration 56/1000 | Loss: 0.00002328
Iteration 57/1000 | Loss: 0.00002327
Iteration 58/1000 | Loss: 0.00002327
Iteration 59/1000 | Loss: 0.00002327
Iteration 60/1000 | Loss: 0.00002327
Iteration 61/1000 | Loss: 0.00002327
Iteration 62/1000 | Loss: 0.00002327
Iteration 63/1000 | Loss: 0.00002327
Iteration 64/1000 | Loss: 0.00002326
Iteration 65/1000 | Loss: 0.00002326
Iteration 66/1000 | Loss: 0.00002326
Iteration 67/1000 | Loss: 0.00002326
Iteration 68/1000 | Loss: 0.00002326
Iteration 69/1000 | Loss: 0.00002326
Iteration 70/1000 | Loss: 0.00002326
Iteration 71/1000 | Loss: 0.00002326
Iteration 72/1000 | Loss: 0.00002325
Iteration 73/1000 | Loss: 0.00002325
Iteration 74/1000 | Loss: 0.00002325
Iteration 75/1000 | Loss: 0.00002325
Iteration 76/1000 | Loss: 0.00002325
Iteration 77/1000 | Loss: 0.00002325
Iteration 78/1000 | Loss: 0.00002325
Iteration 79/1000 | Loss: 0.00002325
Iteration 80/1000 | Loss: 0.00002324
Iteration 81/1000 | Loss: 0.00002324
Iteration 82/1000 | Loss: 0.00002324
Iteration 83/1000 | Loss: 0.00002324
Iteration 84/1000 | Loss: 0.00002324
Iteration 85/1000 | Loss: 0.00002323
Iteration 86/1000 | Loss: 0.00002323
Iteration 87/1000 | Loss: 0.00002323
Iteration 88/1000 | Loss: 0.00002323
Iteration 89/1000 | Loss: 0.00002323
Iteration 90/1000 | Loss: 0.00002323
Iteration 91/1000 | Loss: 0.00002323
Iteration 92/1000 | Loss: 0.00002323
Iteration 93/1000 | Loss: 0.00002323
Iteration 94/1000 | Loss: 0.00002323
Iteration 95/1000 | Loss: 0.00002323
Iteration 96/1000 | Loss: 0.00002323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.32316815527156e-05, 2.32316815527156e-05, 2.32316815527156e-05, 2.32316815527156e-05, 2.32316815527156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.32316815527156e-05

Optimization complete. Final v2v error: 3.897219657897949 mm

Highest mean error: 5.040441989898682 mm for frame 208

Lowest mean error: 3.379502773284912 mm for frame 64

Saving results

Total time: 44.07634902000427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810985
Iteration 2/25 | Loss: 0.00127375
Iteration 3/25 | Loss: 0.00116777
Iteration 4/25 | Loss: 0.00116032
Iteration 5/25 | Loss: 0.00115953
Iteration 6/25 | Loss: 0.00115953
Iteration 7/25 | Loss: 0.00115953
Iteration 8/25 | Loss: 0.00115953
Iteration 9/25 | Loss: 0.00115953
Iteration 10/25 | Loss: 0.00115953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011595289688557386, 0.0011595289688557386, 0.0011595289688557386, 0.0011595289688557386, 0.0011595289688557386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011595289688557386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29022622
Iteration 2/25 | Loss: 0.00132806
Iteration 3/25 | Loss: 0.00132806
Iteration 4/25 | Loss: 0.00132805
Iteration 5/25 | Loss: 0.00132805
Iteration 6/25 | Loss: 0.00132805
Iteration 7/25 | Loss: 0.00132805
Iteration 8/25 | Loss: 0.00132805
Iteration 9/25 | Loss: 0.00132805
Iteration 10/25 | Loss: 0.00132805
Iteration 11/25 | Loss: 0.00132805
Iteration 12/25 | Loss: 0.00132805
Iteration 13/25 | Loss: 0.00132805
Iteration 14/25 | Loss: 0.00132805
Iteration 15/25 | Loss: 0.00132805
Iteration 16/25 | Loss: 0.00132805
Iteration 17/25 | Loss: 0.00132805
Iteration 18/25 | Loss: 0.00132805
Iteration 19/25 | Loss: 0.00132805
Iteration 20/25 | Loss: 0.00132805
Iteration 21/25 | Loss: 0.00132805
Iteration 22/25 | Loss: 0.00132805
Iteration 23/25 | Loss: 0.00132805
Iteration 24/25 | Loss: 0.00132805
Iteration 25/25 | Loss: 0.00132805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132805
Iteration 2/1000 | Loss: 0.00001844
Iteration 3/1000 | Loss: 0.00001378
Iteration 4/1000 | Loss: 0.00001266
Iteration 5/1000 | Loss: 0.00001175
Iteration 6/1000 | Loss: 0.00001108
Iteration 7/1000 | Loss: 0.00001074
Iteration 8/1000 | Loss: 0.00001046
Iteration 9/1000 | Loss: 0.00001017
Iteration 10/1000 | Loss: 0.00001002
Iteration 11/1000 | Loss: 0.00001001
Iteration 12/1000 | Loss: 0.00001001
Iteration 13/1000 | Loss: 0.00001000
Iteration 14/1000 | Loss: 0.00000984
Iteration 15/1000 | Loss: 0.00000971
Iteration 16/1000 | Loss: 0.00000969
Iteration 17/1000 | Loss: 0.00000969
Iteration 18/1000 | Loss: 0.00000967
Iteration 19/1000 | Loss: 0.00000965
Iteration 20/1000 | Loss: 0.00000965
Iteration 21/1000 | Loss: 0.00000963
Iteration 22/1000 | Loss: 0.00000961
Iteration 23/1000 | Loss: 0.00000960
Iteration 24/1000 | Loss: 0.00000959
Iteration 25/1000 | Loss: 0.00000957
Iteration 26/1000 | Loss: 0.00000956
Iteration 27/1000 | Loss: 0.00000956
Iteration 28/1000 | Loss: 0.00000951
Iteration 29/1000 | Loss: 0.00000950
Iteration 30/1000 | Loss: 0.00000949
Iteration 31/1000 | Loss: 0.00000948
Iteration 32/1000 | Loss: 0.00000943
Iteration 33/1000 | Loss: 0.00000943
Iteration 34/1000 | Loss: 0.00000942
Iteration 35/1000 | Loss: 0.00000941
Iteration 36/1000 | Loss: 0.00000941
Iteration 37/1000 | Loss: 0.00000940
Iteration 38/1000 | Loss: 0.00000934
Iteration 39/1000 | Loss: 0.00000932
Iteration 40/1000 | Loss: 0.00000931
Iteration 41/1000 | Loss: 0.00000930
Iteration 42/1000 | Loss: 0.00000922
Iteration 43/1000 | Loss: 0.00000922
Iteration 44/1000 | Loss: 0.00000921
Iteration 45/1000 | Loss: 0.00000919
Iteration 46/1000 | Loss: 0.00000918
Iteration 47/1000 | Loss: 0.00000918
Iteration 48/1000 | Loss: 0.00000916
Iteration 49/1000 | Loss: 0.00000916
Iteration 50/1000 | Loss: 0.00000915
Iteration 51/1000 | Loss: 0.00000915
Iteration 52/1000 | Loss: 0.00000914
Iteration 53/1000 | Loss: 0.00000912
Iteration 54/1000 | Loss: 0.00000912
Iteration 55/1000 | Loss: 0.00000911
Iteration 56/1000 | Loss: 0.00000910
Iteration 57/1000 | Loss: 0.00000910
Iteration 58/1000 | Loss: 0.00000909
Iteration 59/1000 | Loss: 0.00000909
Iteration 60/1000 | Loss: 0.00000908
Iteration 61/1000 | Loss: 0.00000907
Iteration 62/1000 | Loss: 0.00000907
Iteration 63/1000 | Loss: 0.00000906
Iteration 64/1000 | Loss: 0.00000906
Iteration 65/1000 | Loss: 0.00000905
Iteration 66/1000 | Loss: 0.00000903
Iteration 67/1000 | Loss: 0.00000902
Iteration 68/1000 | Loss: 0.00000902
Iteration 69/1000 | Loss: 0.00000902
Iteration 70/1000 | Loss: 0.00000901
Iteration 71/1000 | Loss: 0.00000901
Iteration 72/1000 | Loss: 0.00000900
Iteration 73/1000 | Loss: 0.00000899
Iteration 74/1000 | Loss: 0.00000899
Iteration 75/1000 | Loss: 0.00000898
Iteration 76/1000 | Loss: 0.00000898
Iteration 77/1000 | Loss: 0.00000898
Iteration 78/1000 | Loss: 0.00000898
Iteration 79/1000 | Loss: 0.00000897
Iteration 80/1000 | Loss: 0.00000897
Iteration 81/1000 | Loss: 0.00000897
Iteration 82/1000 | Loss: 0.00000896
Iteration 83/1000 | Loss: 0.00000896
Iteration 84/1000 | Loss: 0.00000896
Iteration 85/1000 | Loss: 0.00000895
Iteration 86/1000 | Loss: 0.00000895
Iteration 87/1000 | Loss: 0.00000894
Iteration 88/1000 | Loss: 0.00000894
Iteration 89/1000 | Loss: 0.00000894
Iteration 90/1000 | Loss: 0.00000894
Iteration 91/1000 | Loss: 0.00000894
Iteration 92/1000 | Loss: 0.00000894
Iteration 93/1000 | Loss: 0.00000894
Iteration 94/1000 | Loss: 0.00000893
Iteration 95/1000 | Loss: 0.00000893
Iteration 96/1000 | Loss: 0.00000893
Iteration 97/1000 | Loss: 0.00000892
Iteration 98/1000 | Loss: 0.00000892
Iteration 99/1000 | Loss: 0.00000892
Iteration 100/1000 | Loss: 0.00000891
Iteration 101/1000 | Loss: 0.00000890
Iteration 102/1000 | Loss: 0.00000890
Iteration 103/1000 | Loss: 0.00000889
Iteration 104/1000 | Loss: 0.00000889
Iteration 105/1000 | Loss: 0.00000888
Iteration 106/1000 | Loss: 0.00000888
Iteration 107/1000 | Loss: 0.00000887
Iteration 108/1000 | Loss: 0.00000887
Iteration 109/1000 | Loss: 0.00000886
Iteration 110/1000 | Loss: 0.00000886
Iteration 111/1000 | Loss: 0.00000885
Iteration 112/1000 | Loss: 0.00000885
Iteration 113/1000 | Loss: 0.00000884
Iteration 114/1000 | Loss: 0.00000884
Iteration 115/1000 | Loss: 0.00000883
Iteration 116/1000 | Loss: 0.00000883
Iteration 117/1000 | Loss: 0.00000883
Iteration 118/1000 | Loss: 0.00000883
Iteration 119/1000 | Loss: 0.00000883
Iteration 120/1000 | Loss: 0.00000882
Iteration 121/1000 | Loss: 0.00000882
Iteration 122/1000 | Loss: 0.00000882
Iteration 123/1000 | Loss: 0.00000882
Iteration 124/1000 | Loss: 0.00000882
Iteration 125/1000 | Loss: 0.00000882
Iteration 126/1000 | Loss: 0.00000882
Iteration 127/1000 | Loss: 0.00000882
Iteration 128/1000 | Loss: 0.00000881
Iteration 129/1000 | Loss: 0.00000881
Iteration 130/1000 | Loss: 0.00000881
Iteration 131/1000 | Loss: 0.00000880
Iteration 132/1000 | Loss: 0.00000880
Iteration 133/1000 | Loss: 0.00000880
Iteration 134/1000 | Loss: 0.00000880
Iteration 135/1000 | Loss: 0.00000880
Iteration 136/1000 | Loss: 0.00000880
Iteration 137/1000 | Loss: 0.00000880
Iteration 138/1000 | Loss: 0.00000879
Iteration 139/1000 | Loss: 0.00000879
Iteration 140/1000 | Loss: 0.00000879
Iteration 141/1000 | Loss: 0.00000879
Iteration 142/1000 | Loss: 0.00000878
Iteration 143/1000 | Loss: 0.00000878
Iteration 144/1000 | Loss: 0.00000878
Iteration 145/1000 | Loss: 0.00000877
Iteration 146/1000 | Loss: 0.00000877
Iteration 147/1000 | Loss: 0.00000877
Iteration 148/1000 | Loss: 0.00000877
Iteration 149/1000 | Loss: 0.00000877
Iteration 150/1000 | Loss: 0.00000877
Iteration 151/1000 | Loss: 0.00000877
Iteration 152/1000 | Loss: 0.00000876
Iteration 153/1000 | Loss: 0.00000876
Iteration 154/1000 | Loss: 0.00000876
Iteration 155/1000 | Loss: 0.00000876
Iteration 156/1000 | Loss: 0.00000876
Iteration 157/1000 | Loss: 0.00000876
Iteration 158/1000 | Loss: 0.00000876
Iteration 159/1000 | Loss: 0.00000876
Iteration 160/1000 | Loss: 0.00000876
Iteration 161/1000 | Loss: 0.00000876
Iteration 162/1000 | Loss: 0.00000876
Iteration 163/1000 | Loss: 0.00000876
Iteration 164/1000 | Loss: 0.00000876
Iteration 165/1000 | Loss: 0.00000876
Iteration 166/1000 | Loss: 0.00000876
Iteration 167/1000 | Loss: 0.00000875
Iteration 168/1000 | Loss: 0.00000875
Iteration 169/1000 | Loss: 0.00000875
Iteration 170/1000 | Loss: 0.00000875
Iteration 171/1000 | Loss: 0.00000875
Iteration 172/1000 | Loss: 0.00000875
Iteration 173/1000 | Loss: 0.00000875
Iteration 174/1000 | Loss: 0.00000875
Iteration 175/1000 | Loss: 0.00000875
Iteration 176/1000 | Loss: 0.00000875
Iteration 177/1000 | Loss: 0.00000875
Iteration 178/1000 | Loss: 0.00000875
Iteration 179/1000 | Loss: 0.00000875
Iteration 180/1000 | Loss: 0.00000875
Iteration 181/1000 | Loss: 0.00000875
Iteration 182/1000 | Loss: 0.00000875
Iteration 183/1000 | Loss: 0.00000875
Iteration 184/1000 | Loss: 0.00000875
Iteration 185/1000 | Loss: 0.00000874
Iteration 186/1000 | Loss: 0.00000874
Iteration 187/1000 | Loss: 0.00000874
Iteration 188/1000 | Loss: 0.00000874
Iteration 189/1000 | Loss: 0.00000874
Iteration 190/1000 | Loss: 0.00000874
Iteration 191/1000 | Loss: 0.00000874
Iteration 192/1000 | Loss: 0.00000874
Iteration 193/1000 | Loss: 0.00000874
Iteration 194/1000 | Loss: 0.00000874
Iteration 195/1000 | Loss: 0.00000874
Iteration 196/1000 | Loss: 0.00000874
Iteration 197/1000 | Loss: 0.00000874
Iteration 198/1000 | Loss: 0.00000874
Iteration 199/1000 | Loss: 0.00000874
Iteration 200/1000 | Loss: 0.00000874
Iteration 201/1000 | Loss: 0.00000873
Iteration 202/1000 | Loss: 0.00000873
Iteration 203/1000 | Loss: 0.00000873
Iteration 204/1000 | Loss: 0.00000873
Iteration 205/1000 | Loss: 0.00000873
Iteration 206/1000 | Loss: 0.00000873
Iteration 207/1000 | Loss: 0.00000873
Iteration 208/1000 | Loss: 0.00000873
Iteration 209/1000 | Loss: 0.00000873
Iteration 210/1000 | Loss: 0.00000873
Iteration 211/1000 | Loss: 0.00000873
Iteration 212/1000 | Loss: 0.00000873
Iteration 213/1000 | Loss: 0.00000873
Iteration 214/1000 | Loss: 0.00000873
Iteration 215/1000 | Loss: 0.00000873
Iteration 216/1000 | Loss: 0.00000873
Iteration 217/1000 | Loss: 0.00000873
Iteration 218/1000 | Loss: 0.00000873
Iteration 219/1000 | Loss: 0.00000873
Iteration 220/1000 | Loss: 0.00000873
Iteration 221/1000 | Loss: 0.00000873
Iteration 222/1000 | Loss: 0.00000873
Iteration 223/1000 | Loss: 0.00000873
Iteration 224/1000 | Loss: 0.00000873
Iteration 225/1000 | Loss: 0.00000873
Iteration 226/1000 | Loss: 0.00000873
Iteration 227/1000 | Loss: 0.00000873
Iteration 228/1000 | Loss: 0.00000873
Iteration 229/1000 | Loss: 0.00000873
Iteration 230/1000 | Loss: 0.00000873
Iteration 231/1000 | Loss: 0.00000873
Iteration 232/1000 | Loss: 0.00000873
Iteration 233/1000 | Loss: 0.00000873
Iteration 234/1000 | Loss: 0.00000873
Iteration 235/1000 | Loss: 0.00000873
Iteration 236/1000 | Loss: 0.00000873
Iteration 237/1000 | Loss: 0.00000873
Iteration 238/1000 | Loss: 0.00000873
Iteration 239/1000 | Loss: 0.00000873
Iteration 240/1000 | Loss: 0.00000873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [8.727667591301724e-06, 8.727667591301724e-06, 8.727667591301724e-06, 8.727667591301724e-06, 8.727667591301724e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.727667591301724e-06

Optimization complete. Final v2v error: 2.530888795852661 mm

Highest mean error: 2.7526512145996094 mm for frame 58

Lowest mean error: 2.33817195892334 mm for frame 224

Saving results

Total time: 49.24610233306885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00615344
Iteration 2/25 | Loss: 0.00142876
Iteration 3/25 | Loss: 0.00125139
Iteration 4/25 | Loss: 0.00123039
Iteration 5/25 | Loss: 0.00122884
Iteration 6/25 | Loss: 0.00122884
Iteration 7/25 | Loss: 0.00122884
Iteration 8/25 | Loss: 0.00122884
Iteration 9/25 | Loss: 0.00122884
Iteration 10/25 | Loss: 0.00122884
Iteration 11/25 | Loss: 0.00122884
Iteration 12/25 | Loss: 0.00122884
Iteration 13/25 | Loss: 0.00122884
Iteration 14/25 | Loss: 0.00122884
Iteration 15/25 | Loss: 0.00122884
Iteration 16/25 | Loss: 0.00122884
Iteration 17/25 | Loss: 0.00122884
Iteration 18/25 | Loss: 0.00122884
Iteration 19/25 | Loss: 0.00122884
Iteration 20/25 | Loss: 0.00122884
Iteration 21/25 | Loss: 0.00122884
Iteration 22/25 | Loss: 0.00122884
Iteration 23/25 | Loss: 0.00122884
Iteration 24/25 | Loss: 0.00122884
Iteration 25/25 | Loss: 0.00122884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012288398575037718, 0.0012288398575037718, 0.0012288398575037718, 0.0012288398575037718, 0.0012288398575037718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012288398575037718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27166069
Iteration 2/25 | Loss: 0.00111534
Iteration 3/25 | Loss: 0.00111531
Iteration 4/25 | Loss: 0.00111530
Iteration 5/25 | Loss: 0.00111530
Iteration 6/25 | Loss: 0.00111530
Iteration 7/25 | Loss: 0.00111530
Iteration 8/25 | Loss: 0.00111530
Iteration 9/25 | Loss: 0.00111530
Iteration 10/25 | Loss: 0.00111530
Iteration 11/25 | Loss: 0.00111530
Iteration 12/25 | Loss: 0.00111530
Iteration 13/25 | Loss: 0.00111530
Iteration 14/25 | Loss: 0.00111530
Iteration 15/25 | Loss: 0.00111530
Iteration 16/25 | Loss: 0.00111530
Iteration 17/25 | Loss: 0.00111530
Iteration 18/25 | Loss: 0.00111530
Iteration 19/25 | Loss: 0.00111530
Iteration 20/25 | Loss: 0.00111530
Iteration 21/25 | Loss: 0.00111530
Iteration 22/25 | Loss: 0.00111530
Iteration 23/25 | Loss: 0.00111530
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011153032537549734, 0.0011153032537549734, 0.0011153032537549734, 0.0011153032537549734, 0.0011153032537549734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011153032537549734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111530
Iteration 2/1000 | Loss: 0.00002362
Iteration 3/1000 | Loss: 0.00001720
Iteration 4/1000 | Loss: 0.00001555
Iteration 5/1000 | Loss: 0.00001471
Iteration 6/1000 | Loss: 0.00001416
Iteration 7/1000 | Loss: 0.00001370
Iteration 8/1000 | Loss: 0.00001323
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001259
Iteration 11/1000 | Loss: 0.00001251
Iteration 12/1000 | Loss: 0.00001235
Iteration 13/1000 | Loss: 0.00001228
Iteration 14/1000 | Loss: 0.00001210
Iteration 15/1000 | Loss: 0.00001206
Iteration 16/1000 | Loss: 0.00001201
Iteration 17/1000 | Loss: 0.00001199
Iteration 18/1000 | Loss: 0.00001193
Iteration 19/1000 | Loss: 0.00001186
Iteration 20/1000 | Loss: 0.00001184
Iteration 21/1000 | Loss: 0.00001182
Iteration 22/1000 | Loss: 0.00001181
Iteration 23/1000 | Loss: 0.00001181
Iteration 24/1000 | Loss: 0.00001176
Iteration 25/1000 | Loss: 0.00001174
Iteration 26/1000 | Loss: 0.00001174
Iteration 27/1000 | Loss: 0.00001171
Iteration 28/1000 | Loss: 0.00001170
Iteration 29/1000 | Loss: 0.00001168
Iteration 30/1000 | Loss: 0.00001167
Iteration 31/1000 | Loss: 0.00001166
Iteration 32/1000 | Loss: 0.00001165
Iteration 33/1000 | Loss: 0.00001162
Iteration 34/1000 | Loss: 0.00001161
Iteration 35/1000 | Loss: 0.00001160
Iteration 36/1000 | Loss: 0.00001160
Iteration 37/1000 | Loss: 0.00001159
Iteration 38/1000 | Loss: 0.00001159
Iteration 39/1000 | Loss: 0.00001157
Iteration 40/1000 | Loss: 0.00001155
Iteration 41/1000 | Loss: 0.00001155
Iteration 42/1000 | Loss: 0.00001154
Iteration 43/1000 | Loss: 0.00001149
Iteration 44/1000 | Loss: 0.00001148
Iteration 45/1000 | Loss: 0.00001148
Iteration 46/1000 | Loss: 0.00001148
Iteration 47/1000 | Loss: 0.00001148
Iteration 48/1000 | Loss: 0.00001148
Iteration 49/1000 | Loss: 0.00001148
Iteration 50/1000 | Loss: 0.00001148
Iteration 51/1000 | Loss: 0.00001148
Iteration 52/1000 | Loss: 0.00001148
Iteration 53/1000 | Loss: 0.00001148
Iteration 54/1000 | Loss: 0.00001148
Iteration 55/1000 | Loss: 0.00001148
Iteration 56/1000 | Loss: 0.00001148
Iteration 57/1000 | Loss: 0.00001148
Iteration 58/1000 | Loss: 0.00001148
Iteration 59/1000 | Loss: 0.00001148
Iteration 60/1000 | Loss: 0.00001148
Iteration 61/1000 | Loss: 0.00001148
Iteration 62/1000 | Loss: 0.00001148
Iteration 63/1000 | Loss: 0.00001148
Iteration 64/1000 | Loss: 0.00001148
Iteration 65/1000 | Loss: 0.00001148
Iteration 66/1000 | Loss: 0.00001148
Iteration 67/1000 | Loss: 0.00001148
Iteration 68/1000 | Loss: 0.00001148
Iteration 69/1000 | Loss: 0.00001148
Iteration 70/1000 | Loss: 0.00001148
Iteration 71/1000 | Loss: 0.00001148
Iteration 72/1000 | Loss: 0.00001148
Iteration 73/1000 | Loss: 0.00001148
Iteration 74/1000 | Loss: 0.00001148
Iteration 75/1000 | Loss: 0.00001148
Iteration 76/1000 | Loss: 0.00001148
Iteration 77/1000 | Loss: 0.00001148
Iteration 78/1000 | Loss: 0.00001148
Iteration 79/1000 | Loss: 0.00001148
Iteration 80/1000 | Loss: 0.00001148
Iteration 81/1000 | Loss: 0.00001148
Iteration 82/1000 | Loss: 0.00001148
Iteration 83/1000 | Loss: 0.00001148
Iteration 84/1000 | Loss: 0.00001148
Iteration 85/1000 | Loss: 0.00001148
Iteration 86/1000 | Loss: 0.00001148
Iteration 87/1000 | Loss: 0.00001148
Iteration 88/1000 | Loss: 0.00001148
Iteration 89/1000 | Loss: 0.00001148
Iteration 90/1000 | Loss: 0.00001148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.1476962754386477e-05, 1.1476962754386477e-05, 1.1476962754386477e-05, 1.1476962754386477e-05, 1.1476962754386477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1476962754386477e-05

Optimization complete. Final v2v error: 2.9171736240386963 mm

Highest mean error: 3.227102041244507 mm for frame 156

Lowest mean error: 2.635169267654419 mm for frame 32

Saving results

Total time: 33.99036478996277
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899394
Iteration 2/25 | Loss: 0.00143030
Iteration 3/25 | Loss: 0.00126859
Iteration 4/25 | Loss: 0.00124683
Iteration 5/25 | Loss: 0.00123964
Iteration 6/25 | Loss: 0.00123777
Iteration 7/25 | Loss: 0.00123777
Iteration 8/25 | Loss: 0.00123777
Iteration 9/25 | Loss: 0.00123777
Iteration 10/25 | Loss: 0.00123777
Iteration 11/25 | Loss: 0.00123777
Iteration 12/25 | Loss: 0.00123777
Iteration 13/25 | Loss: 0.00123777
Iteration 14/25 | Loss: 0.00123777
Iteration 15/25 | Loss: 0.00123777
Iteration 16/25 | Loss: 0.00123777
Iteration 17/25 | Loss: 0.00123777
Iteration 18/25 | Loss: 0.00123777
Iteration 19/25 | Loss: 0.00123777
Iteration 20/25 | Loss: 0.00123777
Iteration 21/25 | Loss: 0.00123777
Iteration 22/25 | Loss: 0.00123777
Iteration 23/25 | Loss: 0.00123777
Iteration 24/25 | Loss: 0.00123777
Iteration 25/25 | Loss: 0.00123777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31173146
Iteration 2/25 | Loss: 0.00126395
Iteration 3/25 | Loss: 0.00126393
Iteration 4/25 | Loss: 0.00126393
Iteration 5/25 | Loss: 0.00126393
Iteration 6/25 | Loss: 0.00126393
Iteration 7/25 | Loss: 0.00126393
Iteration 8/25 | Loss: 0.00126393
Iteration 9/25 | Loss: 0.00126393
Iteration 10/25 | Loss: 0.00126393
Iteration 11/25 | Loss: 0.00126393
Iteration 12/25 | Loss: 0.00126393
Iteration 13/25 | Loss: 0.00126393
Iteration 14/25 | Loss: 0.00126393
Iteration 15/25 | Loss: 0.00126393
Iteration 16/25 | Loss: 0.00126393
Iteration 17/25 | Loss: 0.00126393
Iteration 18/25 | Loss: 0.00126393
Iteration 19/25 | Loss: 0.00126393
Iteration 20/25 | Loss: 0.00126393
Iteration 21/25 | Loss: 0.00126393
Iteration 22/25 | Loss: 0.00126393
Iteration 23/25 | Loss: 0.00126393
Iteration 24/25 | Loss: 0.00126393
Iteration 25/25 | Loss: 0.00126393

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126393
Iteration 2/1000 | Loss: 0.00004342
Iteration 3/1000 | Loss: 0.00003006
Iteration 4/1000 | Loss: 0.00002520
Iteration 5/1000 | Loss: 0.00002385
Iteration 6/1000 | Loss: 0.00002274
Iteration 7/1000 | Loss: 0.00002212
Iteration 8/1000 | Loss: 0.00002150
Iteration 9/1000 | Loss: 0.00002107
Iteration 10/1000 | Loss: 0.00002077
Iteration 11/1000 | Loss: 0.00002053
Iteration 12/1000 | Loss: 0.00002052
Iteration 13/1000 | Loss: 0.00002037
Iteration 14/1000 | Loss: 0.00002020
Iteration 15/1000 | Loss: 0.00002007
Iteration 16/1000 | Loss: 0.00002006
Iteration 17/1000 | Loss: 0.00002006
Iteration 18/1000 | Loss: 0.00002001
Iteration 19/1000 | Loss: 0.00002001
Iteration 20/1000 | Loss: 0.00002000
Iteration 21/1000 | Loss: 0.00001999
Iteration 22/1000 | Loss: 0.00001999
Iteration 23/1000 | Loss: 0.00001991
Iteration 24/1000 | Loss: 0.00001986
Iteration 25/1000 | Loss: 0.00001986
Iteration 26/1000 | Loss: 0.00001985
Iteration 27/1000 | Loss: 0.00001985
Iteration 28/1000 | Loss: 0.00001984
Iteration 29/1000 | Loss: 0.00001984
Iteration 30/1000 | Loss: 0.00001983
Iteration 31/1000 | Loss: 0.00001982
Iteration 32/1000 | Loss: 0.00001982
Iteration 33/1000 | Loss: 0.00001978
Iteration 34/1000 | Loss: 0.00001976
Iteration 35/1000 | Loss: 0.00001976
Iteration 36/1000 | Loss: 0.00001975
Iteration 37/1000 | Loss: 0.00001974
Iteration 38/1000 | Loss: 0.00001974
Iteration 39/1000 | Loss: 0.00001973
Iteration 40/1000 | Loss: 0.00001973
Iteration 41/1000 | Loss: 0.00001973
Iteration 42/1000 | Loss: 0.00001972
Iteration 43/1000 | Loss: 0.00001972
Iteration 44/1000 | Loss: 0.00001972
Iteration 45/1000 | Loss: 0.00001971
Iteration 46/1000 | Loss: 0.00001971
Iteration 47/1000 | Loss: 0.00001971
Iteration 48/1000 | Loss: 0.00001971
Iteration 49/1000 | Loss: 0.00001970
Iteration 50/1000 | Loss: 0.00001970
Iteration 51/1000 | Loss: 0.00001970
Iteration 52/1000 | Loss: 0.00001970
Iteration 53/1000 | Loss: 0.00001970
Iteration 54/1000 | Loss: 0.00001969
Iteration 55/1000 | Loss: 0.00001969
Iteration 56/1000 | Loss: 0.00001969
Iteration 57/1000 | Loss: 0.00001968
Iteration 58/1000 | Loss: 0.00001967
Iteration 59/1000 | Loss: 0.00001967
Iteration 60/1000 | Loss: 0.00001966
Iteration 61/1000 | Loss: 0.00001966
Iteration 62/1000 | Loss: 0.00001965
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00001964
Iteration 65/1000 | Loss: 0.00001964
Iteration 66/1000 | Loss: 0.00001964
Iteration 67/1000 | Loss: 0.00001963
Iteration 68/1000 | Loss: 0.00001962
Iteration 69/1000 | Loss: 0.00001962
Iteration 70/1000 | Loss: 0.00001961
Iteration 71/1000 | Loss: 0.00001961
Iteration 72/1000 | Loss: 0.00001961
Iteration 73/1000 | Loss: 0.00001960
Iteration 74/1000 | Loss: 0.00001960
Iteration 75/1000 | Loss: 0.00001959
Iteration 76/1000 | Loss: 0.00001959
Iteration 77/1000 | Loss: 0.00001958
Iteration 78/1000 | Loss: 0.00001958
Iteration 79/1000 | Loss: 0.00001958
Iteration 80/1000 | Loss: 0.00001956
Iteration 81/1000 | Loss: 0.00001956
Iteration 82/1000 | Loss: 0.00001955
Iteration 83/1000 | Loss: 0.00001955
Iteration 84/1000 | Loss: 0.00001954
Iteration 85/1000 | Loss: 0.00001953
Iteration 86/1000 | Loss: 0.00001953
Iteration 87/1000 | Loss: 0.00001953
Iteration 88/1000 | Loss: 0.00001953
Iteration 89/1000 | Loss: 0.00001952
Iteration 90/1000 | Loss: 0.00001952
Iteration 91/1000 | Loss: 0.00001952
Iteration 92/1000 | Loss: 0.00001951
Iteration 93/1000 | Loss: 0.00001951
Iteration 94/1000 | Loss: 0.00001951
Iteration 95/1000 | Loss: 0.00001951
Iteration 96/1000 | Loss: 0.00001950
Iteration 97/1000 | Loss: 0.00001950
Iteration 98/1000 | Loss: 0.00001950
Iteration 99/1000 | Loss: 0.00001949
Iteration 100/1000 | Loss: 0.00001949
Iteration 101/1000 | Loss: 0.00001949
Iteration 102/1000 | Loss: 0.00001949
Iteration 103/1000 | Loss: 0.00001949
Iteration 104/1000 | Loss: 0.00001948
Iteration 105/1000 | Loss: 0.00001948
Iteration 106/1000 | Loss: 0.00001947
Iteration 107/1000 | Loss: 0.00001947
Iteration 108/1000 | Loss: 0.00001946
Iteration 109/1000 | Loss: 0.00001946
Iteration 110/1000 | Loss: 0.00001946
Iteration 111/1000 | Loss: 0.00001946
Iteration 112/1000 | Loss: 0.00001945
Iteration 113/1000 | Loss: 0.00001945
Iteration 114/1000 | Loss: 0.00001945
Iteration 115/1000 | Loss: 0.00001945
Iteration 116/1000 | Loss: 0.00001945
Iteration 117/1000 | Loss: 0.00001945
Iteration 118/1000 | Loss: 0.00001945
Iteration 119/1000 | Loss: 0.00001945
Iteration 120/1000 | Loss: 0.00001945
Iteration 121/1000 | Loss: 0.00001945
Iteration 122/1000 | Loss: 0.00001944
Iteration 123/1000 | Loss: 0.00001944
Iteration 124/1000 | Loss: 0.00001944
Iteration 125/1000 | Loss: 0.00001943
Iteration 126/1000 | Loss: 0.00001943
Iteration 127/1000 | Loss: 0.00001943
Iteration 128/1000 | Loss: 0.00001943
Iteration 129/1000 | Loss: 0.00001942
Iteration 130/1000 | Loss: 0.00001942
Iteration 131/1000 | Loss: 0.00001942
Iteration 132/1000 | Loss: 0.00001942
Iteration 133/1000 | Loss: 0.00001942
Iteration 134/1000 | Loss: 0.00001942
Iteration 135/1000 | Loss: 0.00001942
Iteration 136/1000 | Loss: 0.00001941
Iteration 137/1000 | Loss: 0.00001941
Iteration 138/1000 | Loss: 0.00001941
Iteration 139/1000 | Loss: 0.00001941
Iteration 140/1000 | Loss: 0.00001941
Iteration 141/1000 | Loss: 0.00001940
Iteration 142/1000 | Loss: 0.00001940
Iteration 143/1000 | Loss: 0.00001940
Iteration 144/1000 | Loss: 0.00001940
Iteration 145/1000 | Loss: 0.00001939
Iteration 146/1000 | Loss: 0.00001939
Iteration 147/1000 | Loss: 0.00001939
Iteration 148/1000 | Loss: 0.00001939
Iteration 149/1000 | Loss: 0.00001939
Iteration 150/1000 | Loss: 0.00001939
Iteration 151/1000 | Loss: 0.00001939
Iteration 152/1000 | Loss: 0.00001939
Iteration 153/1000 | Loss: 0.00001939
Iteration 154/1000 | Loss: 0.00001939
Iteration 155/1000 | Loss: 0.00001938
Iteration 156/1000 | Loss: 0.00001938
Iteration 157/1000 | Loss: 0.00001938
Iteration 158/1000 | Loss: 0.00001938
Iteration 159/1000 | Loss: 0.00001938
Iteration 160/1000 | Loss: 0.00001938
Iteration 161/1000 | Loss: 0.00001938
Iteration 162/1000 | Loss: 0.00001938
Iteration 163/1000 | Loss: 0.00001937
Iteration 164/1000 | Loss: 0.00001937
Iteration 165/1000 | Loss: 0.00001937
Iteration 166/1000 | Loss: 0.00001937
Iteration 167/1000 | Loss: 0.00001937
Iteration 168/1000 | Loss: 0.00001937
Iteration 169/1000 | Loss: 0.00001937
Iteration 170/1000 | Loss: 0.00001937
Iteration 171/1000 | Loss: 0.00001937
Iteration 172/1000 | Loss: 0.00001937
Iteration 173/1000 | Loss: 0.00001937
Iteration 174/1000 | Loss: 0.00001937
Iteration 175/1000 | Loss: 0.00001937
Iteration 176/1000 | Loss: 0.00001937
Iteration 177/1000 | Loss: 0.00001937
Iteration 178/1000 | Loss: 0.00001936
Iteration 179/1000 | Loss: 0.00001936
Iteration 180/1000 | Loss: 0.00001936
Iteration 181/1000 | Loss: 0.00001936
Iteration 182/1000 | Loss: 0.00001936
Iteration 183/1000 | Loss: 0.00001936
Iteration 184/1000 | Loss: 0.00001936
Iteration 185/1000 | Loss: 0.00001936
Iteration 186/1000 | Loss: 0.00001936
Iteration 187/1000 | Loss: 0.00001936
Iteration 188/1000 | Loss: 0.00001936
Iteration 189/1000 | Loss: 0.00001936
Iteration 190/1000 | Loss: 0.00001936
Iteration 191/1000 | Loss: 0.00001935
Iteration 192/1000 | Loss: 0.00001935
Iteration 193/1000 | Loss: 0.00001935
Iteration 194/1000 | Loss: 0.00001935
Iteration 195/1000 | Loss: 0.00001935
Iteration 196/1000 | Loss: 0.00001935
Iteration 197/1000 | Loss: 0.00001935
Iteration 198/1000 | Loss: 0.00001935
Iteration 199/1000 | Loss: 0.00001935
Iteration 200/1000 | Loss: 0.00001935
Iteration 201/1000 | Loss: 0.00001935
Iteration 202/1000 | Loss: 0.00001935
Iteration 203/1000 | Loss: 0.00001935
Iteration 204/1000 | Loss: 0.00001935
Iteration 205/1000 | Loss: 0.00001935
Iteration 206/1000 | Loss: 0.00001935
Iteration 207/1000 | Loss: 0.00001935
Iteration 208/1000 | Loss: 0.00001935
Iteration 209/1000 | Loss: 0.00001935
Iteration 210/1000 | Loss: 0.00001934
Iteration 211/1000 | Loss: 0.00001934
Iteration 212/1000 | Loss: 0.00001934
Iteration 213/1000 | Loss: 0.00001934
Iteration 214/1000 | Loss: 0.00001934
Iteration 215/1000 | Loss: 0.00001934
Iteration 216/1000 | Loss: 0.00001934
Iteration 217/1000 | Loss: 0.00001934
Iteration 218/1000 | Loss: 0.00001934
Iteration 219/1000 | Loss: 0.00001934
Iteration 220/1000 | Loss: 0.00001934
Iteration 221/1000 | Loss: 0.00001934
Iteration 222/1000 | Loss: 0.00001934
Iteration 223/1000 | Loss: 0.00001934
Iteration 224/1000 | Loss: 0.00001934
Iteration 225/1000 | Loss: 0.00001934
Iteration 226/1000 | Loss: 0.00001934
Iteration 227/1000 | Loss: 0.00001934
Iteration 228/1000 | Loss: 0.00001934
Iteration 229/1000 | Loss: 0.00001934
Iteration 230/1000 | Loss: 0.00001934
Iteration 231/1000 | Loss: 0.00001934
Iteration 232/1000 | Loss: 0.00001934
Iteration 233/1000 | Loss: 0.00001934
Iteration 234/1000 | Loss: 0.00001934
Iteration 235/1000 | Loss: 0.00001934
Iteration 236/1000 | Loss: 0.00001934
Iteration 237/1000 | Loss: 0.00001934
Iteration 238/1000 | Loss: 0.00001934
Iteration 239/1000 | Loss: 0.00001934
Iteration 240/1000 | Loss: 0.00001934
Iteration 241/1000 | Loss: 0.00001934
Iteration 242/1000 | Loss: 0.00001934
Iteration 243/1000 | Loss: 0.00001934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.9335617253091186e-05, 1.9335617253091186e-05, 1.9335617253091186e-05, 1.9335617253091186e-05, 1.9335617253091186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9335617253091186e-05

Optimization complete. Final v2v error: 3.6989972591400146 mm

Highest mean error: 5.247260570526123 mm for frame 67

Lowest mean error: 3.114508628845215 mm for frame 44

Saving results

Total time: 46.53673076629639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806083
Iteration 2/25 | Loss: 0.00123856
Iteration 3/25 | Loss: 0.00117287
Iteration 4/25 | Loss: 0.00115320
Iteration 5/25 | Loss: 0.00114616
Iteration 6/25 | Loss: 0.00114555
Iteration 7/25 | Loss: 0.00114555
Iteration 8/25 | Loss: 0.00114555
Iteration 9/25 | Loss: 0.00114555
Iteration 10/25 | Loss: 0.00114555
Iteration 11/25 | Loss: 0.00114555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011455495841801167, 0.0011455495841801167, 0.0011455495841801167, 0.0011455495841801167, 0.0011455495841801167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011455495841801167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.39715385
Iteration 2/25 | Loss: 0.00208599
Iteration 3/25 | Loss: 0.00208594
Iteration 4/25 | Loss: 0.00208594
Iteration 5/25 | Loss: 0.00208594
Iteration 6/25 | Loss: 0.00208594
Iteration 7/25 | Loss: 0.00208594
Iteration 8/25 | Loss: 0.00208594
Iteration 9/25 | Loss: 0.00208594
Iteration 10/25 | Loss: 0.00208594
Iteration 11/25 | Loss: 0.00208594
Iteration 12/25 | Loss: 0.00208594
Iteration 13/25 | Loss: 0.00208594
Iteration 14/25 | Loss: 0.00208594
Iteration 15/25 | Loss: 0.00208594
Iteration 16/25 | Loss: 0.00208594
Iteration 17/25 | Loss: 0.00208594
Iteration 18/25 | Loss: 0.00208594
Iteration 19/25 | Loss: 0.00208594
Iteration 20/25 | Loss: 0.00208594
Iteration 21/25 | Loss: 0.00208594
Iteration 22/25 | Loss: 0.00208594
Iteration 23/25 | Loss: 0.00208594
Iteration 24/25 | Loss: 0.00208594
Iteration 25/25 | Loss: 0.00208594

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208594
Iteration 2/1000 | Loss: 0.00003535
Iteration 3/1000 | Loss: 0.00002193
Iteration 4/1000 | Loss: 0.00001787
Iteration 5/1000 | Loss: 0.00001641
Iteration 6/1000 | Loss: 0.00001526
Iteration 7/1000 | Loss: 0.00001462
Iteration 8/1000 | Loss: 0.00001404
Iteration 9/1000 | Loss: 0.00001363
Iteration 10/1000 | Loss: 0.00001323
Iteration 11/1000 | Loss: 0.00001293
Iteration 12/1000 | Loss: 0.00001287
Iteration 13/1000 | Loss: 0.00001267
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001251
Iteration 18/1000 | Loss: 0.00001248
Iteration 19/1000 | Loss: 0.00001243
Iteration 20/1000 | Loss: 0.00001241
Iteration 21/1000 | Loss: 0.00001241
Iteration 22/1000 | Loss: 0.00001239
Iteration 23/1000 | Loss: 0.00001233
Iteration 24/1000 | Loss: 0.00001233
Iteration 25/1000 | Loss: 0.00001233
Iteration 26/1000 | Loss: 0.00001227
Iteration 27/1000 | Loss: 0.00001224
Iteration 28/1000 | Loss: 0.00001223
Iteration 29/1000 | Loss: 0.00001222
Iteration 30/1000 | Loss: 0.00001221
Iteration 31/1000 | Loss: 0.00001220
Iteration 32/1000 | Loss: 0.00001220
Iteration 33/1000 | Loss: 0.00001219
Iteration 34/1000 | Loss: 0.00001219
Iteration 35/1000 | Loss: 0.00001219
Iteration 36/1000 | Loss: 0.00001218
Iteration 37/1000 | Loss: 0.00001218
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001217
Iteration 40/1000 | Loss: 0.00001216
Iteration 41/1000 | Loss: 0.00001215
Iteration 42/1000 | Loss: 0.00001215
Iteration 43/1000 | Loss: 0.00001215
Iteration 44/1000 | Loss: 0.00001214
Iteration 45/1000 | Loss: 0.00001214
Iteration 46/1000 | Loss: 0.00001214
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001213
Iteration 49/1000 | Loss: 0.00001212
Iteration 50/1000 | Loss: 0.00001212
Iteration 51/1000 | Loss: 0.00001212
Iteration 52/1000 | Loss: 0.00001212
Iteration 53/1000 | Loss: 0.00001212
Iteration 54/1000 | Loss: 0.00001212
Iteration 55/1000 | Loss: 0.00001211
Iteration 56/1000 | Loss: 0.00001211
Iteration 57/1000 | Loss: 0.00001211
Iteration 58/1000 | Loss: 0.00001211
Iteration 59/1000 | Loss: 0.00001211
Iteration 60/1000 | Loss: 0.00001211
Iteration 61/1000 | Loss: 0.00001211
Iteration 62/1000 | Loss: 0.00001210
Iteration 63/1000 | Loss: 0.00001210
Iteration 64/1000 | Loss: 0.00001210
Iteration 65/1000 | Loss: 0.00001210
Iteration 66/1000 | Loss: 0.00001210
Iteration 67/1000 | Loss: 0.00001210
Iteration 68/1000 | Loss: 0.00001210
Iteration 69/1000 | Loss: 0.00001210
Iteration 70/1000 | Loss: 0.00001209
Iteration 71/1000 | Loss: 0.00001209
Iteration 72/1000 | Loss: 0.00001209
Iteration 73/1000 | Loss: 0.00001209
Iteration 74/1000 | Loss: 0.00001208
Iteration 75/1000 | Loss: 0.00001208
Iteration 76/1000 | Loss: 0.00001208
Iteration 77/1000 | Loss: 0.00001208
Iteration 78/1000 | Loss: 0.00001208
Iteration 79/1000 | Loss: 0.00001208
Iteration 80/1000 | Loss: 0.00001208
Iteration 81/1000 | Loss: 0.00001207
Iteration 82/1000 | Loss: 0.00001207
Iteration 83/1000 | Loss: 0.00001207
Iteration 84/1000 | Loss: 0.00001207
Iteration 85/1000 | Loss: 0.00001206
Iteration 86/1000 | Loss: 0.00001206
Iteration 87/1000 | Loss: 0.00001206
Iteration 88/1000 | Loss: 0.00001206
Iteration 89/1000 | Loss: 0.00001206
Iteration 90/1000 | Loss: 0.00001205
Iteration 91/1000 | Loss: 0.00001205
Iteration 92/1000 | Loss: 0.00001205
Iteration 93/1000 | Loss: 0.00001205
Iteration 94/1000 | Loss: 0.00001205
Iteration 95/1000 | Loss: 0.00001205
Iteration 96/1000 | Loss: 0.00001205
Iteration 97/1000 | Loss: 0.00001205
Iteration 98/1000 | Loss: 0.00001205
Iteration 99/1000 | Loss: 0.00001205
Iteration 100/1000 | Loss: 0.00001205
Iteration 101/1000 | Loss: 0.00001205
Iteration 102/1000 | Loss: 0.00001205
Iteration 103/1000 | Loss: 0.00001205
Iteration 104/1000 | Loss: 0.00001205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.2049628821841907e-05, 1.2049628821841907e-05, 1.2049628821841907e-05, 1.2049628821841907e-05, 1.2049628821841907e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2049628821841907e-05

Optimization complete. Final v2v error: 2.9841723442077637 mm

Highest mean error: 3.4471828937530518 mm for frame 48

Lowest mean error: 2.5958962440490723 mm for frame 217

Saving results

Total time: 41.18174600601196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00766423
Iteration 2/25 | Loss: 0.00156304
Iteration 3/25 | Loss: 0.00136835
Iteration 4/25 | Loss: 0.00135475
Iteration 5/25 | Loss: 0.00134794
Iteration 6/25 | Loss: 0.00131823
Iteration 7/25 | Loss: 0.00131761
Iteration 8/25 | Loss: 0.00129217
Iteration 9/25 | Loss: 0.00127748
Iteration 10/25 | Loss: 0.00127091
Iteration 11/25 | Loss: 0.00126185
Iteration 12/25 | Loss: 0.00125992
Iteration 13/25 | Loss: 0.00125976
Iteration 14/25 | Loss: 0.00125975
Iteration 15/25 | Loss: 0.00125974
Iteration 16/25 | Loss: 0.00125974
Iteration 17/25 | Loss: 0.00125974
Iteration 18/25 | Loss: 0.00125974
Iteration 19/25 | Loss: 0.00125974
Iteration 20/25 | Loss: 0.00125974
Iteration 21/25 | Loss: 0.00125974
Iteration 22/25 | Loss: 0.00125974
Iteration 23/25 | Loss: 0.00125974
Iteration 24/25 | Loss: 0.00125974
Iteration 25/25 | Loss: 0.00125974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24105120
Iteration 2/25 | Loss: 0.00132190
Iteration 3/25 | Loss: 0.00132187
Iteration 4/25 | Loss: 0.00132187
Iteration 5/25 | Loss: 0.00132187
Iteration 6/25 | Loss: 0.00132187
Iteration 7/25 | Loss: 0.00132187
Iteration 8/25 | Loss: 0.00132187
Iteration 9/25 | Loss: 0.00132187
Iteration 10/25 | Loss: 0.00132187
Iteration 11/25 | Loss: 0.00132187
Iteration 12/25 | Loss: 0.00132187
Iteration 13/25 | Loss: 0.00132187
Iteration 14/25 | Loss: 0.00132187
Iteration 15/25 | Loss: 0.00132187
Iteration 16/25 | Loss: 0.00132187
Iteration 17/25 | Loss: 0.00132187
Iteration 18/25 | Loss: 0.00132187
Iteration 19/25 | Loss: 0.00132187
Iteration 20/25 | Loss: 0.00132187
Iteration 21/25 | Loss: 0.00132187
Iteration 22/25 | Loss: 0.00132187
Iteration 23/25 | Loss: 0.00132187
Iteration 24/25 | Loss: 0.00132187
Iteration 25/25 | Loss: 0.00132187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132187
Iteration 2/1000 | Loss: 0.00005467
Iteration 3/1000 | Loss: 0.00003705
Iteration 4/1000 | Loss: 0.00003036
Iteration 5/1000 | Loss: 0.00005033
Iteration 6/1000 | Loss: 0.00003131
Iteration 7/1000 | Loss: 0.00002691
Iteration 8/1000 | Loss: 0.00002478
Iteration 9/1000 | Loss: 0.00002364
Iteration 10/1000 | Loss: 0.00002314
Iteration 11/1000 | Loss: 0.00002268
Iteration 12/1000 | Loss: 0.00002217
Iteration 13/1000 | Loss: 0.00002172
Iteration 14/1000 | Loss: 0.00002132
Iteration 15/1000 | Loss: 0.00002086
Iteration 16/1000 | Loss: 0.00002060
Iteration 17/1000 | Loss: 0.00002034
Iteration 18/1000 | Loss: 0.00002026
Iteration 19/1000 | Loss: 0.00002023
Iteration 20/1000 | Loss: 0.00002022
Iteration 21/1000 | Loss: 0.00002020
Iteration 22/1000 | Loss: 0.00002011
Iteration 23/1000 | Loss: 0.00001992
Iteration 24/1000 | Loss: 0.00001992
Iteration 25/1000 | Loss: 0.00001989
Iteration 26/1000 | Loss: 0.00001989
Iteration 27/1000 | Loss: 0.00001989
Iteration 28/1000 | Loss: 0.00001989
Iteration 29/1000 | Loss: 0.00001989
Iteration 30/1000 | Loss: 0.00001989
Iteration 31/1000 | Loss: 0.00001989
Iteration 32/1000 | Loss: 0.00001988
Iteration 33/1000 | Loss: 0.00001988
Iteration 34/1000 | Loss: 0.00001988
Iteration 35/1000 | Loss: 0.00001988
Iteration 36/1000 | Loss: 0.00001987
Iteration 37/1000 | Loss: 0.00001987
Iteration 38/1000 | Loss: 0.00001986
Iteration 39/1000 | Loss: 0.00001986
Iteration 40/1000 | Loss: 0.00001986
Iteration 41/1000 | Loss: 0.00001985
Iteration 42/1000 | Loss: 0.00001985
Iteration 43/1000 | Loss: 0.00001984
Iteration 44/1000 | Loss: 0.00001984
Iteration 45/1000 | Loss: 0.00001983
Iteration 46/1000 | Loss: 0.00001983
Iteration 47/1000 | Loss: 0.00001982
Iteration 48/1000 | Loss: 0.00001981
Iteration 49/1000 | Loss: 0.00001980
Iteration 50/1000 | Loss: 0.00001978
Iteration 51/1000 | Loss: 0.00001978
Iteration 52/1000 | Loss: 0.00001978
Iteration 53/1000 | Loss: 0.00001978
Iteration 54/1000 | Loss: 0.00001978
Iteration 55/1000 | Loss: 0.00001978
Iteration 56/1000 | Loss: 0.00001978
Iteration 57/1000 | Loss: 0.00001978
Iteration 58/1000 | Loss: 0.00001978
Iteration 59/1000 | Loss: 0.00001978
Iteration 60/1000 | Loss: 0.00001978
Iteration 61/1000 | Loss: 0.00001977
Iteration 62/1000 | Loss: 0.00001974
Iteration 63/1000 | Loss: 0.00001973
Iteration 64/1000 | Loss: 0.00001973
Iteration 65/1000 | Loss: 0.00001972
Iteration 66/1000 | Loss: 0.00001972
Iteration 67/1000 | Loss: 0.00001971
Iteration 68/1000 | Loss: 0.00001971
Iteration 69/1000 | Loss: 0.00001971
Iteration 70/1000 | Loss: 0.00001970
Iteration 71/1000 | Loss: 0.00001970
Iteration 72/1000 | Loss: 0.00001970
Iteration 73/1000 | Loss: 0.00001970
Iteration 74/1000 | Loss: 0.00001969
Iteration 75/1000 | Loss: 0.00001969
Iteration 76/1000 | Loss: 0.00001969
Iteration 77/1000 | Loss: 0.00001969
Iteration 78/1000 | Loss: 0.00001969
Iteration 79/1000 | Loss: 0.00001969
Iteration 80/1000 | Loss: 0.00001969
Iteration 81/1000 | Loss: 0.00001969
Iteration 82/1000 | Loss: 0.00001969
Iteration 83/1000 | Loss: 0.00001969
Iteration 84/1000 | Loss: 0.00001969
Iteration 85/1000 | Loss: 0.00001968
Iteration 86/1000 | Loss: 0.00001968
Iteration 87/1000 | Loss: 0.00001968
Iteration 88/1000 | Loss: 0.00001968
Iteration 89/1000 | Loss: 0.00001968
Iteration 90/1000 | Loss: 0.00001968
Iteration 91/1000 | Loss: 0.00001968
Iteration 92/1000 | Loss: 0.00001968
Iteration 93/1000 | Loss: 0.00001968
Iteration 94/1000 | Loss: 0.00001968
Iteration 95/1000 | Loss: 0.00001968
Iteration 96/1000 | Loss: 0.00001968
Iteration 97/1000 | Loss: 0.00001968
Iteration 98/1000 | Loss: 0.00001968
Iteration 99/1000 | Loss: 0.00001968
Iteration 100/1000 | Loss: 0.00001968
Iteration 101/1000 | Loss: 0.00001968
Iteration 102/1000 | Loss: 0.00001968
Iteration 103/1000 | Loss: 0.00001968
Iteration 104/1000 | Loss: 0.00001968
Iteration 105/1000 | Loss: 0.00001968
Iteration 106/1000 | Loss: 0.00001968
Iteration 107/1000 | Loss: 0.00001968
Iteration 108/1000 | Loss: 0.00001968
Iteration 109/1000 | Loss: 0.00001968
Iteration 110/1000 | Loss: 0.00001968
Iteration 111/1000 | Loss: 0.00001968
Iteration 112/1000 | Loss: 0.00001968
Iteration 113/1000 | Loss: 0.00001968
Iteration 114/1000 | Loss: 0.00001968
Iteration 115/1000 | Loss: 0.00001968
Iteration 116/1000 | Loss: 0.00001968
Iteration 117/1000 | Loss: 0.00001968
Iteration 118/1000 | Loss: 0.00001968
Iteration 119/1000 | Loss: 0.00001968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.9680150217027403e-05, 1.9680150217027403e-05, 1.9680150217027403e-05, 1.9680150217027403e-05, 1.9680150217027403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9680150217027403e-05

Optimization complete. Final v2v error: 3.623006582260132 mm

Highest mean error: 4.414156913757324 mm for frame 21

Lowest mean error: 3.005213975906372 mm for frame 12

Saving results

Total time: 54.90997362136841
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890365
Iteration 2/25 | Loss: 0.00211098
Iteration 3/25 | Loss: 0.00152270
Iteration 4/25 | Loss: 0.00150370
Iteration 5/25 | Loss: 0.00146291
Iteration 6/25 | Loss: 0.00141878
Iteration 7/25 | Loss: 0.00139827
Iteration 8/25 | Loss: 0.00138483
Iteration 9/25 | Loss: 0.00137237
Iteration 10/25 | Loss: 0.00137183
Iteration 11/25 | Loss: 0.00136537
Iteration 12/25 | Loss: 0.00136601
Iteration 13/25 | Loss: 0.00137053
Iteration 14/25 | Loss: 0.00137673
Iteration 15/25 | Loss: 0.00137670
Iteration 16/25 | Loss: 0.00136414
Iteration 17/25 | Loss: 0.00135598
Iteration 18/25 | Loss: 0.00134397
Iteration 19/25 | Loss: 0.00134849
Iteration 20/25 | Loss: 0.00134735
Iteration 21/25 | Loss: 0.00134282
Iteration 22/25 | Loss: 0.00134599
Iteration 23/25 | Loss: 0.00135639
Iteration 24/25 | Loss: 0.00134029
Iteration 25/25 | Loss: 0.00132994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60970294
Iteration 2/25 | Loss: 0.00916090
Iteration 3/25 | Loss: 0.00322078
Iteration 4/25 | Loss: 0.00322077
Iteration 5/25 | Loss: 0.00322077
Iteration 6/25 | Loss: 0.00322077
Iteration 7/25 | Loss: 0.00322076
Iteration 8/25 | Loss: 0.00322076
Iteration 9/25 | Loss: 0.00322076
Iteration 10/25 | Loss: 0.00322076
Iteration 11/25 | Loss: 0.00322076
Iteration 12/25 | Loss: 0.00322076
Iteration 13/25 | Loss: 0.00322076
Iteration 14/25 | Loss: 0.00322076
Iteration 15/25 | Loss: 0.00322076
Iteration 16/25 | Loss: 0.00322076
Iteration 17/25 | Loss: 0.00322076
Iteration 18/25 | Loss: 0.00322076
Iteration 19/25 | Loss: 0.00322076
Iteration 20/25 | Loss: 0.00322076
Iteration 21/25 | Loss: 0.00322076
Iteration 22/25 | Loss: 0.00322076
Iteration 23/25 | Loss: 0.00322076
Iteration 24/25 | Loss: 0.00322076
Iteration 25/25 | Loss: 0.00322076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00322076
Iteration 2/1000 | Loss: 0.00379947
Iteration 3/1000 | Loss: 0.00490673
Iteration 4/1000 | Loss: 0.00124771
Iteration 5/1000 | Loss: 0.00123366
Iteration 6/1000 | Loss: 0.00382604
Iteration 7/1000 | Loss: 0.00122586
Iteration 8/1000 | Loss: 0.00177505
Iteration 9/1000 | Loss: 0.00473247
Iteration 10/1000 | Loss: 0.00496233
Iteration 11/1000 | Loss: 0.00378904
Iteration 12/1000 | Loss: 0.00168709
Iteration 13/1000 | Loss: 0.00323694
Iteration 14/1000 | Loss: 0.00167056
Iteration 15/1000 | Loss: 0.00085221
Iteration 16/1000 | Loss: 0.00109758
Iteration 17/1000 | Loss: 0.00146053
Iteration 18/1000 | Loss: 0.00112189
Iteration 19/1000 | Loss: 0.00217723
Iteration 20/1000 | Loss: 0.00159554
Iteration 21/1000 | Loss: 0.00152981
Iteration 22/1000 | Loss: 0.00082123
Iteration 23/1000 | Loss: 0.00093312
Iteration 24/1000 | Loss: 0.00078358
Iteration 25/1000 | Loss: 0.00387299
Iteration 26/1000 | Loss: 0.00218954
Iteration 27/1000 | Loss: 0.00176746
Iteration 28/1000 | Loss: 0.00186960
Iteration 29/1000 | Loss: 0.00028291
Iteration 30/1000 | Loss: 0.00420347
Iteration 31/1000 | Loss: 0.00215315
Iteration 32/1000 | Loss: 0.00008060
Iteration 33/1000 | Loss: 0.00182024
Iteration 34/1000 | Loss: 0.00045749
Iteration 35/1000 | Loss: 0.00004937
Iteration 36/1000 | Loss: 0.00042569
Iteration 37/1000 | Loss: 0.00161073
Iteration 38/1000 | Loss: 0.00195482
Iteration 39/1000 | Loss: 0.00105335
Iteration 40/1000 | Loss: 0.00322411
Iteration 41/1000 | Loss: 0.00026168
Iteration 42/1000 | Loss: 0.00039783
Iteration 43/1000 | Loss: 0.00163371
Iteration 44/1000 | Loss: 0.00005728
Iteration 45/1000 | Loss: 0.00004963
Iteration 46/1000 | Loss: 0.00044757
Iteration 47/1000 | Loss: 0.00052045
Iteration 48/1000 | Loss: 0.00047688
Iteration 49/1000 | Loss: 0.00026760
Iteration 50/1000 | Loss: 0.00012938
Iteration 51/1000 | Loss: 0.00004030
Iteration 52/1000 | Loss: 0.00003522
Iteration 53/1000 | Loss: 0.00003318
Iteration 54/1000 | Loss: 0.00003165
Iteration 55/1000 | Loss: 0.00003118
Iteration 56/1000 | Loss: 0.00002986
Iteration 57/1000 | Loss: 0.00002896
Iteration 58/1000 | Loss: 0.00002784
Iteration 59/1000 | Loss: 0.00002662
Iteration 60/1000 | Loss: 0.00002594
Iteration 61/1000 | Loss: 0.00002546
Iteration 62/1000 | Loss: 0.00002521
Iteration 63/1000 | Loss: 0.00002500
Iteration 64/1000 | Loss: 0.00002488
Iteration 65/1000 | Loss: 0.00002483
Iteration 66/1000 | Loss: 0.00002483
Iteration 67/1000 | Loss: 0.00002483
Iteration 68/1000 | Loss: 0.00002481
Iteration 69/1000 | Loss: 0.00002479
Iteration 70/1000 | Loss: 0.00002474
Iteration 71/1000 | Loss: 0.00002474
Iteration 72/1000 | Loss: 0.00002471
Iteration 73/1000 | Loss: 0.00002471
Iteration 74/1000 | Loss: 0.00002471
Iteration 75/1000 | Loss: 0.00002468
Iteration 76/1000 | Loss: 0.00002467
Iteration 77/1000 | Loss: 0.00002467
Iteration 78/1000 | Loss: 0.00002466
Iteration 79/1000 | Loss: 0.00002466
Iteration 80/1000 | Loss: 0.00002466
Iteration 81/1000 | Loss: 0.00002465
Iteration 82/1000 | Loss: 0.00002465
Iteration 83/1000 | Loss: 0.00002464
Iteration 84/1000 | Loss: 0.00002464
Iteration 85/1000 | Loss: 0.00002461
Iteration 86/1000 | Loss: 0.00002456
Iteration 87/1000 | Loss: 0.00002445
Iteration 88/1000 | Loss: 0.00002444
Iteration 89/1000 | Loss: 0.00002443
Iteration 90/1000 | Loss: 0.00002442
Iteration 91/1000 | Loss: 0.00002441
Iteration 92/1000 | Loss: 0.00002440
Iteration 93/1000 | Loss: 0.00002440
Iteration 94/1000 | Loss: 0.00002440
Iteration 95/1000 | Loss: 0.00002439
Iteration 96/1000 | Loss: 0.00002439
Iteration 97/1000 | Loss: 0.00002439
Iteration 98/1000 | Loss: 0.00002439
Iteration 99/1000 | Loss: 0.00002439
Iteration 100/1000 | Loss: 0.00002439
Iteration 101/1000 | Loss: 0.00002439
Iteration 102/1000 | Loss: 0.00002439
Iteration 103/1000 | Loss: 0.00002439
Iteration 104/1000 | Loss: 0.00002439
Iteration 105/1000 | Loss: 0.00002438
Iteration 106/1000 | Loss: 0.00002437
Iteration 107/1000 | Loss: 0.00002437
Iteration 108/1000 | Loss: 0.00002436
Iteration 109/1000 | Loss: 0.00002435
Iteration 110/1000 | Loss: 0.00002435
Iteration 111/1000 | Loss: 0.00002435
Iteration 112/1000 | Loss: 0.00002435
Iteration 113/1000 | Loss: 0.00002435
Iteration 114/1000 | Loss: 0.00002434
Iteration 115/1000 | Loss: 0.00002434
Iteration 116/1000 | Loss: 0.00002433
Iteration 117/1000 | Loss: 0.00002433
Iteration 118/1000 | Loss: 0.00002433
Iteration 119/1000 | Loss: 0.00002433
Iteration 120/1000 | Loss: 0.00002433
Iteration 121/1000 | Loss: 0.00002432
Iteration 122/1000 | Loss: 0.00002432
Iteration 123/1000 | Loss: 0.00002432
Iteration 124/1000 | Loss: 0.00002432
Iteration 125/1000 | Loss: 0.00002431
Iteration 126/1000 | Loss: 0.00002431
Iteration 127/1000 | Loss: 0.00002430
Iteration 128/1000 | Loss: 0.00002430
Iteration 129/1000 | Loss: 0.00002429
Iteration 130/1000 | Loss: 0.00002429
Iteration 131/1000 | Loss: 0.00002429
Iteration 132/1000 | Loss: 0.00002429
Iteration 133/1000 | Loss: 0.00002429
Iteration 134/1000 | Loss: 0.00002428
Iteration 135/1000 | Loss: 0.00002427
Iteration 136/1000 | Loss: 0.00002418
Iteration 137/1000 | Loss: 0.00002414
Iteration 138/1000 | Loss: 0.00002414
Iteration 139/1000 | Loss: 0.00002414
Iteration 140/1000 | Loss: 0.00002414
Iteration 141/1000 | Loss: 0.00002414
Iteration 142/1000 | Loss: 0.00002414
Iteration 143/1000 | Loss: 0.00002414
Iteration 144/1000 | Loss: 0.00002413
Iteration 145/1000 | Loss: 0.00002413
Iteration 146/1000 | Loss: 0.00002413
Iteration 147/1000 | Loss: 0.00002413
Iteration 148/1000 | Loss: 0.00002413
Iteration 149/1000 | Loss: 0.00002413
Iteration 150/1000 | Loss: 0.00002413
Iteration 151/1000 | Loss: 0.00002413
Iteration 152/1000 | Loss: 0.00002413
Iteration 153/1000 | Loss: 0.00002413
Iteration 154/1000 | Loss: 0.00002413
Iteration 155/1000 | Loss: 0.00002413
Iteration 156/1000 | Loss: 0.00002412
Iteration 157/1000 | Loss: 0.00002412
Iteration 158/1000 | Loss: 0.00002412
Iteration 159/1000 | Loss: 0.00002412
Iteration 160/1000 | Loss: 0.00002411
Iteration 161/1000 | Loss: 0.00002411
Iteration 162/1000 | Loss: 0.00002411
Iteration 163/1000 | Loss: 0.00002411
Iteration 164/1000 | Loss: 0.00002411
Iteration 165/1000 | Loss: 0.00002411
Iteration 166/1000 | Loss: 0.00002411
Iteration 167/1000 | Loss: 0.00002411
Iteration 168/1000 | Loss: 0.00002411
Iteration 169/1000 | Loss: 0.00002410
Iteration 170/1000 | Loss: 0.00002410
Iteration 171/1000 | Loss: 0.00002410
Iteration 172/1000 | Loss: 0.00002410
Iteration 173/1000 | Loss: 0.00002410
Iteration 174/1000 | Loss: 0.00002410
Iteration 175/1000 | Loss: 0.00002410
Iteration 176/1000 | Loss: 0.00002410
Iteration 177/1000 | Loss: 0.00002410
Iteration 178/1000 | Loss: 0.00002409
Iteration 179/1000 | Loss: 0.00002409
Iteration 180/1000 | Loss: 0.00002409
Iteration 181/1000 | Loss: 0.00002409
Iteration 182/1000 | Loss: 0.00002409
Iteration 183/1000 | Loss: 0.00002408
Iteration 184/1000 | Loss: 0.00002408
Iteration 185/1000 | Loss: 0.00002408
Iteration 186/1000 | Loss: 0.00002408
Iteration 187/1000 | Loss: 0.00002408
Iteration 188/1000 | Loss: 0.00002408
Iteration 189/1000 | Loss: 0.00002408
Iteration 190/1000 | Loss: 0.00002408
Iteration 191/1000 | Loss: 0.00002408
Iteration 192/1000 | Loss: 0.00002408
Iteration 193/1000 | Loss: 0.00002408
Iteration 194/1000 | Loss: 0.00002407
Iteration 195/1000 | Loss: 0.00002407
Iteration 196/1000 | Loss: 0.00002407
Iteration 197/1000 | Loss: 0.00002407
Iteration 198/1000 | Loss: 0.00002407
Iteration 199/1000 | Loss: 0.00002407
Iteration 200/1000 | Loss: 0.00002407
Iteration 201/1000 | Loss: 0.00002407
Iteration 202/1000 | Loss: 0.00002407
Iteration 203/1000 | Loss: 0.00002407
Iteration 204/1000 | Loss: 0.00002406
Iteration 205/1000 | Loss: 0.00002406
Iteration 206/1000 | Loss: 0.00002406
Iteration 207/1000 | Loss: 0.00002406
Iteration 208/1000 | Loss: 0.00002406
Iteration 209/1000 | Loss: 0.00002406
Iteration 210/1000 | Loss: 0.00002405
Iteration 211/1000 | Loss: 0.00002405
Iteration 212/1000 | Loss: 0.00002405
Iteration 213/1000 | Loss: 0.00002405
Iteration 214/1000 | Loss: 0.00002405
Iteration 215/1000 | Loss: 0.00002405
Iteration 216/1000 | Loss: 0.00002405
Iteration 217/1000 | Loss: 0.00002404
Iteration 218/1000 | Loss: 0.00002404
Iteration 219/1000 | Loss: 0.00002404
Iteration 220/1000 | Loss: 0.00002404
Iteration 221/1000 | Loss: 0.00002404
Iteration 222/1000 | Loss: 0.00002404
Iteration 223/1000 | Loss: 0.00002404
Iteration 224/1000 | Loss: 0.00002404
Iteration 225/1000 | Loss: 0.00002404
Iteration 226/1000 | Loss: 0.00002404
Iteration 227/1000 | Loss: 0.00002404
Iteration 228/1000 | Loss: 0.00002404
Iteration 229/1000 | Loss: 0.00002404
Iteration 230/1000 | Loss: 0.00002404
Iteration 231/1000 | Loss: 0.00002404
Iteration 232/1000 | Loss: 0.00002403
Iteration 233/1000 | Loss: 0.00002403
Iteration 234/1000 | Loss: 0.00002403
Iteration 235/1000 | Loss: 0.00002403
Iteration 236/1000 | Loss: 0.00002403
Iteration 237/1000 | Loss: 0.00002402
Iteration 238/1000 | Loss: 0.00002402
Iteration 239/1000 | Loss: 0.00002402
Iteration 240/1000 | Loss: 0.00002402
Iteration 241/1000 | Loss: 0.00002402
Iteration 242/1000 | Loss: 0.00002402
Iteration 243/1000 | Loss: 0.00002401
Iteration 244/1000 | Loss: 0.00002401
Iteration 245/1000 | Loss: 0.00002401
Iteration 246/1000 | Loss: 0.00002401
Iteration 247/1000 | Loss: 0.00002401
Iteration 248/1000 | Loss: 0.00002400
Iteration 249/1000 | Loss: 0.00002400
Iteration 250/1000 | Loss: 0.00002400
Iteration 251/1000 | Loss: 0.00002400
Iteration 252/1000 | Loss: 0.00002400
Iteration 253/1000 | Loss: 0.00002400
Iteration 254/1000 | Loss: 0.00002399
Iteration 255/1000 | Loss: 0.00002399
Iteration 256/1000 | Loss: 0.00002399
Iteration 257/1000 | Loss: 0.00002399
Iteration 258/1000 | Loss: 0.00002399
Iteration 259/1000 | Loss: 0.00002399
Iteration 260/1000 | Loss: 0.00002399
Iteration 261/1000 | Loss: 0.00002399
Iteration 262/1000 | Loss: 0.00002398
Iteration 263/1000 | Loss: 0.00002398
Iteration 264/1000 | Loss: 0.00002398
Iteration 265/1000 | Loss: 0.00002397
Iteration 266/1000 | Loss: 0.00002397
Iteration 267/1000 | Loss: 0.00002397
Iteration 268/1000 | Loss: 0.00002397
Iteration 269/1000 | Loss: 0.00002397
Iteration 270/1000 | Loss: 0.00002397
Iteration 271/1000 | Loss: 0.00002397
Iteration 272/1000 | Loss: 0.00002397
Iteration 273/1000 | Loss: 0.00002397
Iteration 274/1000 | Loss: 0.00002397
Iteration 275/1000 | Loss: 0.00002397
Iteration 276/1000 | Loss: 0.00002397
Iteration 277/1000 | Loss: 0.00002397
Iteration 278/1000 | Loss: 0.00002397
Iteration 279/1000 | Loss: 0.00002397
Iteration 280/1000 | Loss: 0.00002397
Iteration 281/1000 | Loss: 0.00002397
Iteration 282/1000 | Loss: 0.00002397
Iteration 283/1000 | Loss: 0.00002397
Iteration 284/1000 | Loss: 0.00002397
Iteration 285/1000 | Loss: 0.00002397
Iteration 286/1000 | Loss: 0.00002397
Iteration 287/1000 | Loss: 0.00002397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [2.3965521904756315e-05, 2.3965521904756315e-05, 2.3965521904756315e-05, 2.3965521904756315e-05, 2.3965521904756315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3965521904756315e-05

Optimization complete. Final v2v error: 3.959040641784668 mm

Highest mean error: 6.614525318145752 mm for frame 138

Lowest mean error: 2.701242446899414 mm for frame 47

Saving results

Total time: 178.39183521270752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849012
Iteration 2/25 | Loss: 0.00127192
Iteration 3/25 | Loss: 0.00118706
Iteration 4/25 | Loss: 0.00117716
Iteration 5/25 | Loss: 0.00117431
Iteration 6/25 | Loss: 0.00117364
Iteration 7/25 | Loss: 0.00117364
Iteration 8/25 | Loss: 0.00117364
Iteration 9/25 | Loss: 0.00117364
Iteration 10/25 | Loss: 0.00117364
Iteration 11/25 | Loss: 0.00117364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001173639204353094, 0.001173639204353094, 0.001173639204353094, 0.001173639204353094, 0.001173639204353094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001173639204353094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94202769
Iteration 2/25 | Loss: 0.00138441
Iteration 3/25 | Loss: 0.00138441
Iteration 4/25 | Loss: 0.00138441
Iteration 5/25 | Loss: 0.00138441
Iteration 6/25 | Loss: 0.00138441
Iteration 7/25 | Loss: 0.00138441
Iteration 8/25 | Loss: 0.00138441
Iteration 9/25 | Loss: 0.00138441
Iteration 10/25 | Loss: 0.00138441
Iteration 11/25 | Loss: 0.00138441
Iteration 12/25 | Loss: 0.00138441
Iteration 13/25 | Loss: 0.00138441
Iteration 14/25 | Loss: 0.00138441
Iteration 15/25 | Loss: 0.00138441
Iteration 16/25 | Loss: 0.00138441
Iteration 17/25 | Loss: 0.00138441
Iteration 18/25 | Loss: 0.00138441
Iteration 19/25 | Loss: 0.00138441
Iteration 20/25 | Loss: 0.00138441
Iteration 21/25 | Loss: 0.00138441
Iteration 22/25 | Loss: 0.00138441
Iteration 23/25 | Loss: 0.00138441
Iteration 24/25 | Loss: 0.00138441
Iteration 25/25 | Loss: 0.00138441

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138441
Iteration 2/1000 | Loss: 0.00002081
Iteration 3/1000 | Loss: 0.00001455
Iteration 4/1000 | Loss: 0.00001304
Iteration 5/1000 | Loss: 0.00001227
Iteration 6/1000 | Loss: 0.00001178
Iteration 7/1000 | Loss: 0.00001123
Iteration 8/1000 | Loss: 0.00001102
Iteration 9/1000 | Loss: 0.00001101
Iteration 10/1000 | Loss: 0.00001076
Iteration 11/1000 | Loss: 0.00001052
Iteration 12/1000 | Loss: 0.00001049
Iteration 13/1000 | Loss: 0.00001040
Iteration 14/1000 | Loss: 0.00001038
Iteration 15/1000 | Loss: 0.00001032
Iteration 16/1000 | Loss: 0.00001031
Iteration 17/1000 | Loss: 0.00001028
Iteration 18/1000 | Loss: 0.00001024
Iteration 19/1000 | Loss: 0.00001017
Iteration 20/1000 | Loss: 0.00001015
Iteration 21/1000 | Loss: 0.00001014
Iteration 22/1000 | Loss: 0.00001013
Iteration 23/1000 | Loss: 0.00001013
Iteration 24/1000 | Loss: 0.00001013
Iteration 25/1000 | Loss: 0.00001012
Iteration 26/1000 | Loss: 0.00001012
Iteration 27/1000 | Loss: 0.00001012
Iteration 28/1000 | Loss: 0.00001009
Iteration 29/1000 | Loss: 0.00001009
Iteration 30/1000 | Loss: 0.00001007
Iteration 31/1000 | Loss: 0.00001003
Iteration 32/1000 | Loss: 0.00001003
Iteration 33/1000 | Loss: 0.00001002
Iteration 34/1000 | Loss: 0.00001002
Iteration 35/1000 | Loss: 0.00001000
Iteration 36/1000 | Loss: 0.00001000
Iteration 37/1000 | Loss: 0.00000998
Iteration 38/1000 | Loss: 0.00000998
Iteration 39/1000 | Loss: 0.00000997
Iteration 40/1000 | Loss: 0.00000997
Iteration 41/1000 | Loss: 0.00000997
Iteration 42/1000 | Loss: 0.00000997
Iteration 43/1000 | Loss: 0.00000997
Iteration 44/1000 | Loss: 0.00000997
Iteration 45/1000 | Loss: 0.00000996
Iteration 46/1000 | Loss: 0.00000996
Iteration 47/1000 | Loss: 0.00000996
Iteration 48/1000 | Loss: 0.00000996
Iteration 49/1000 | Loss: 0.00000994
Iteration 50/1000 | Loss: 0.00000994
Iteration 51/1000 | Loss: 0.00000994
Iteration 52/1000 | Loss: 0.00000994
Iteration 53/1000 | Loss: 0.00000994
Iteration 54/1000 | Loss: 0.00000994
Iteration 55/1000 | Loss: 0.00000994
Iteration 56/1000 | Loss: 0.00000993
Iteration 57/1000 | Loss: 0.00000993
Iteration 58/1000 | Loss: 0.00000993
Iteration 59/1000 | Loss: 0.00000993
Iteration 60/1000 | Loss: 0.00000993
Iteration 61/1000 | Loss: 0.00000993
Iteration 62/1000 | Loss: 0.00000993
Iteration 63/1000 | Loss: 0.00000993
Iteration 64/1000 | Loss: 0.00000992
Iteration 65/1000 | Loss: 0.00000992
Iteration 66/1000 | Loss: 0.00000992
Iteration 67/1000 | Loss: 0.00000991
Iteration 68/1000 | Loss: 0.00000991
Iteration 69/1000 | Loss: 0.00000991
Iteration 70/1000 | Loss: 0.00000990
Iteration 71/1000 | Loss: 0.00000990
Iteration 72/1000 | Loss: 0.00000990
Iteration 73/1000 | Loss: 0.00000989
Iteration 74/1000 | Loss: 0.00000989
Iteration 75/1000 | Loss: 0.00000988
Iteration 76/1000 | Loss: 0.00000988
Iteration 77/1000 | Loss: 0.00000988
Iteration 78/1000 | Loss: 0.00000987
Iteration 79/1000 | Loss: 0.00000987
Iteration 80/1000 | Loss: 0.00000986
Iteration 81/1000 | Loss: 0.00000986
Iteration 82/1000 | Loss: 0.00000986
Iteration 83/1000 | Loss: 0.00000986
Iteration 84/1000 | Loss: 0.00000986
Iteration 85/1000 | Loss: 0.00000985
Iteration 86/1000 | Loss: 0.00000985
Iteration 87/1000 | Loss: 0.00000985
Iteration 88/1000 | Loss: 0.00000985
Iteration 89/1000 | Loss: 0.00000984
Iteration 90/1000 | Loss: 0.00000984
Iteration 91/1000 | Loss: 0.00000984
Iteration 92/1000 | Loss: 0.00000984
Iteration 93/1000 | Loss: 0.00000983
Iteration 94/1000 | Loss: 0.00000983
Iteration 95/1000 | Loss: 0.00000983
Iteration 96/1000 | Loss: 0.00000983
Iteration 97/1000 | Loss: 0.00000983
Iteration 98/1000 | Loss: 0.00000983
Iteration 99/1000 | Loss: 0.00000983
Iteration 100/1000 | Loss: 0.00000982
Iteration 101/1000 | Loss: 0.00000981
Iteration 102/1000 | Loss: 0.00000981
Iteration 103/1000 | Loss: 0.00000980
Iteration 104/1000 | Loss: 0.00000980
Iteration 105/1000 | Loss: 0.00000980
Iteration 106/1000 | Loss: 0.00000979
Iteration 107/1000 | Loss: 0.00000979
Iteration 108/1000 | Loss: 0.00000979
Iteration 109/1000 | Loss: 0.00000979
Iteration 110/1000 | Loss: 0.00000978
Iteration 111/1000 | Loss: 0.00000978
Iteration 112/1000 | Loss: 0.00000978
Iteration 113/1000 | Loss: 0.00000978
Iteration 114/1000 | Loss: 0.00000977
Iteration 115/1000 | Loss: 0.00000977
Iteration 116/1000 | Loss: 0.00000977
Iteration 117/1000 | Loss: 0.00000977
Iteration 118/1000 | Loss: 0.00000977
Iteration 119/1000 | Loss: 0.00000977
Iteration 120/1000 | Loss: 0.00000976
Iteration 121/1000 | Loss: 0.00000976
Iteration 122/1000 | Loss: 0.00000976
Iteration 123/1000 | Loss: 0.00000976
Iteration 124/1000 | Loss: 0.00000976
Iteration 125/1000 | Loss: 0.00000976
Iteration 126/1000 | Loss: 0.00000976
Iteration 127/1000 | Loss: 0.00000976
Iteration 128/1000 | Loss: 0.00000975
Iteration 129/1000 | Loss: 0.00000975
Iteration 130/1000 | Loss: 0.00000975
Iteration 131/1000 | Loss: 0.00000975
Iteration 132/1000 | Loss: 0.00000975
Iteration 133/1000 | Loss: 0.00000975
Iteration 134/1000 | Loss: 0.00000975
Iteration 135/1000 | Loss: 0.00000974
Iteration 136/1000 | Loss: 0.00000974
Iteration 137/1000 | Loss: 0.00000974
Iteration 138/1000 | Loss: 0.00000974
Iteration 139/1000 | Loss: 0.00000974
Iteration 140/1000 | Loss: 0.00000974
Iteration 141/1000 | Loss: 0.00000974
Iteration 142/1000 | Loss: 0.00000974
Iteration 143/1000 | Loss: 0.00000974
Iteration 144/1000 | Loss: 0.00000973
Iteration 145/1000 | Loss: 0.00000973
Iteration 146/1000 | Loss: 0.00000973
Iteration 147/1000 | Loss: 0.00000973
Iteration 148/1000 | Loss: 0.00000973
Iteration 149/1000 | Loss: 0.00000973
Iteration 150/1000 | Loss: 0.00000973
Iteration 151/1000 | Loss: 0.00000973
Iteration 152/1000 | Loss: 0.00000973
Iteration 153/1000 | Loss: 0.00000973
Iteration 154/1000 | Loss: 0.00000973
Iteration 155/1000 | Loss: 0.00000973
Iteration 156/1000 | Loss: 0.00000973
Iteration 157/1000 | Loss: 0.00000973
Iteration 158/1000 | Loss: 0.00000972
Iteration 159/1000 | Loss: 0.00000972
Iteration 160/1000 | Loss: 0.00000972
Iteration 161/1000 | Loss: 0.00000972
Iteration 162/1000 | Loss: 0.00000972
Iteration 163/1000 | Loss: 0.00000972
Iteration 164/1000 | Loss: 0.00000972
Iteration 165/1000 | Loss: 0.00000972
Iteration 166/1000 | Loss: 0.00000972
Iteration 167/1000 | Loss: 0.00000972
Iteration 168/1000 | Loss: 0.00000972
Iteration 169/1000 | Loss: 0.00000972
Iteration 170/1000 | Loss: 0.00000972
Iteration 171/1000 | Loss: 0.00000972
Iteration 172/1000 | Loss: 0.00000972
Iteration 173/1000 | Loss: 0.00000972
Iteration 174/1000 | Loss: 0.00000972
Iteration 175/1000 | Loss: 0.00000972
Iteration 176/1000 | Loss: 0.00000972
Iteration 177/1000 | Loss: 0.00000972
Iteration 178/1000 | Loss: 0.00000972
Iteration 179/1000 | Loss: 0.00000972
Iteration 180/1000 | Loss: 0.00000972
Iteration 181/1000 | Loss: 0.00000972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [9.724868505145423e-06, 9.724868505145423e-06, 9.724868505145423e-06, 9.724868505145423e-06, 9.724868505145423e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.724868505145423e-06

Optimization complete. Final v2v error: 2.6543569564819336 mm

Highest mean error: 3.301708698272705 mm for frame 53

Lowest mean error: 2.427145004272461 mm for frame 101

Saving results

Total time: 36.10760283470154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826969
Iteration 2/25 | Loss: 0.00124168
Iteration 3/25 | Loss: 0.00117030
Iteration 4/25 | Loss: 0.00116381
Iteration 5/25 | Loss: 0.00116184
Iteration 6/25 | Loss: 0.00116157
Iteration 7/25 | Loss: 0.00116157
Iteration 8/25 | Loss: 0.00116157
Iteration 9/25 | Loss: 0.00116157
Iteration 10/25 | Loss: 0.00116157
Iteration 11/25 | Loss: 0.00116157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011615671683102846, 0.0011615671683102846, 0.0011615671683102846, 0.0011615671683102846, 0.0011615671683102846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011615671683102846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37718928
Iteration 2/25 | Loss: 0.00137060
Iteration 3/25 | Loss: 0.00137060
Iteration 4/25 | Loss: 0.00137060
Iteration 5/25 | Loss: 0.00137060
Iteration 6/25 | Loss: 0.00137060
Iteration 7/25 | Loss: 0.00137060
Iteration 8/25 | Loss: 0.00137059
Iteration 9/25 | Loss: 0.00137059
Iteration 10/25 | Loss: 0.00137059
Iteration 11/25 | Loss: 0.00137059
Iteration 12/25 | Loss: 0.00137059
Iteration 13/25 | Loss: 0.00137059
Iteration 14/25 | Loss: 0.00137059
Iteration 15/25 | Loss: 0.00137059
Iteration 16/25 | Loss: 0.00137059
Iteration 17/25 | Loss: 0.00137059
Iteration 18/25 | Loss: 0.00137059
Iteration 19/25 | Loss: 0.00137059
Iteration 20/25 | Loss: 0.00137059
Iteration 21/25 | Loss: 0.00137059
Iteration 22/25 | Loss: 0.00137059
Iteration 23/25 | Loss: 0.00137059
Iteration 24/25 | Loss: 0.00137059
Iteration 25/25 | Loss: 0.00137059

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137059
Iteration 2/1000 | Loss: 0.00001776
Iteration 3/1000 | Loss: 0.00001276
Iteration 4/1000 | Loss: 0.00001156
Iteration 5/1000 | Loss: 0.00001092
Iteration 6/1000 | Loss: 0.00001042
Iteration 7/1000 | Loss: 0.00000999
Iteration 8/1000 | Loss: 0.00000976
Iteration 9/1000 | Loss: 0.00000954
Iteration 10/1000 | Loss: 0.00000942
Iteration 11/1000 | Loss: 0.00000936
Iteration 12/1000 | Loss: 0.00000933
Iteration 13/1000 | Loss: 0.00000932
Iteration 14/1000 | Loss: 0.00000931
Iteration 15/1000 | Loss: 0.00000928
Iteration 16/1000 | Loss: 0.00000918
Iteration 17/1000 | Loss: 0.00000910
Iteration 18/1000 | Loss: 0.00000899
Iteration 19/1000 | Loss: 0.00000897
Iteration 20/1000 | Loss: 0.00000897
Iteration 21/1000 | Loss: 0.00000894
Iteration 22/1000 | Loss: 0.00000894
Iteration 23/1000 | Loss: 0.00000894
Iteration 24/1000 | Loss: 0.00000894
Iteration 25/1000 | Loss: 0.00000894
Iteration 26/1000 | Loss: 0.00000893
Iteration 27/1000 | Loss: 0.00000893
Iteration 28/1000 | Loss: 0.00000891
Iteration 29/1000 | Loss: 0.00000889
Iteration 30/1000 | Loss: 0.00000889
Iteration 31/1000 | Loss: 0.00000888
Iteration 32/1000 | Loss: 0.00000887
Iteration 33/1000 | Loss: 0.00000884
Iteration 34/1000 | Loss: 0.00000883
Iteration 35/1000 | Loss: 0.00000883
Iteration 36/1000 | Loss: 0.00000882
Iteration 37/1000 | Loss: 0.00000882
Iteration 38/1000 | Loss: 0.00000882
Iteration 39/1000 | Loss: 0.00000881
Iteration 40/1000 | Loss: 0.00000880
Iteration 41/1000 | Loss: 0.00000879
Iteration 42/1000 | Loss: 0.00000879
Iteration 43/1000 | Loss: 0.00000879
Iteration 44/1000 | Loss: 0.00000879
Iteration 45/1000 | Loss: 0.00000878
Iteration 46/1000 | Loss: 0.00000878
Iteration 47/1000 | Loss: 0.00000878
Iteration 48/1000 | Loss: 0.00000877
Iteration 49/1000 | Loss: 0.00000877
Iteration 50/1000 | Loss: 0.00000876
Iteration 51/1000 | Loss: 0.00000876
Iteration 52/1000 | Loss: 0.00000876
Iteration 53/1000 | Loss: 0.00000875
Iteration 54/1000 | Loss: 0.00000875
Iteration 55/1000 | Loss: 0.00000875
Iteration 56/1000 | Loss: 0.00000874
Iteration 57/1000 | Loss: 0.00000874
Iteration 58/1000 | Loss: 0.00000874
Iteration 59/1000 | Loss: 0.00000873
Iteration 60/1000 | Loss: 0.00000873
Iteration 61/1000 | Loss: 0.00000873
Iteration 62/1000 | Loss: 0.00000872
Iteration 63/1000 | Loss: 0.00000872
Iteration 64/1000 | Loss: 0.00000872
Iteration 65/1000 | Loss: 0.00000871
Iteration 66/1000 | Loss: 0.00000870
Iteration 67/1000 | Loss: 0.00000870
Iteration 68/1000 | Loss: 0.00000869
Iteration 69/1000 | Loss: 0.00000869
Iteration 70/1000 | Loss: 0.00000869
Iteration 71/1000 | Loss: 0.00000868
Iteration 72/1000 | Loss: 0.00000868
Iteration 73/1000 | Loss: 0.00000868
Iteration 74/1000 | Loss: 0.00000868
Iteration 75/1000 | Loss: 0.00000868
Iteration 76/1000 | Loss: 0.00000867
Iteration 77/1000 | Loss: 0.00000866
Iteration 78/1000 | Loss: 0.00000866
Iteration 79/1000 | Loss: 0.00000866
Iteration 80/1000 | Loss: 0.00000865
Iteration 81/1000 | Loss: 0.00000865
Iteration 82/1000 | Loss: 0.00000865
Iteration 83/1000 | Loss: 0.00000865
Iteration 84/1000 | Loss: 0.00000865
Iteration 85/1000 | Loss: 0.00000864
Iteration 86/1000 | Loss: 0.00000864
Iteration 87/1000 | Loss: 0.00000864
Iteration 88/1000 | Loss: 0.00000864
Iteration 89/1000 | Loss: 0.00000863
Iteration 90/1000 | Loss: 0.00000863
Iteration 91/1000 | Loss: 0.00000863
Iteration 92/1000 | Loss: 0.00000863
Iteration 93/1000 | Loss: 0.00000863
Iteration 94/1000 | Loss: 0.00000862
Iteration 95/1000 | Loss: 0.00000862
Iteration 96/1000 | Loss: 0.00000862
Iteration 97/1000 | Loss: 0.00000862
Iteration 98/1000 | Loss: 0.00000862
Iteration 99/1000 | Loss: 0.00000862
Iteration 100/1000 | Loss: 0.00000862
Iteration 101/1000 | Loss: 0.00000862
Iteration 102/1000 | Loss: 0.00000862
Iteration 103/1000 | Loss: 0.00000862
Iteration 104/1000 | Loss: 0.00000862
Iteration 105/1000 | Loss: 0.00000862
Iteration 106/1000 | Loss: 0.00000862
Iteration 107/1000 | Loss: 0.00000861
Iteration 108/1000 | Loss: 0.00000861
Iteration 109/1000 | Loss: 0.00000861
Iteration 110/1000 | Loss: 0.00000861
Iteration 111/1000 | Loss: 0.00000861
Iteration 112/1000 | Loss: 0.00000861
Iteration 113/1000 | Loss: 0.00000861
Iteration 114/1000 | Loss: 0.00000861
Iteration 115/1000 | Loss: 0.00000861
Iteration 116/1000 | Loss: 0.00000861
Iteration 117/1000 | Loss: 0.00000861
Iteration 118/1000 | Loss: 0.00000861
Iteration 119/1000 | Loss: 0.00000861
Iteration 120/1000 | Loss: 0.00000861
Iteration 121/1000 | Loss: 0.00000860
Iteration 122/1000 | Loss: 0.00000860
Iteration 123/1000 | Loss: 0.00000860
Iteration 124/1000 | Loss: 0.00000860
Iteration 125/1000 | Loss: 0.00000860
Iteration 126/1000 | Loss: 0.00000860
Iteration 127/1000 | Loss: 0.00000860
Iteration 128/1000 | Loss: 0.00000860
Iteration 129/1000 | Loss: 0.00000860
Iteration 130/1000 | Loss: 0.00000860
Iteration 131/1000 | Loss: 0.00000860
Iteration 132/1000 | Loss: 0.00000860
Iteration 133/1000 | Loss: 0.00000859
Iteration 134/1000 | Loss: 0.00000859
Iteration 135/1000 | Loss: 0.00000859
Iteration 136/1000 | Loss: 0.00000859
Iteration 137/1000 | Loss: 0.00000859
Iteration 138/1000 | Loss: 0.00000859
Iteration 139/1000 | Loss: 0.00000859
Iteration 140/1000 | Loss: 0.00000859
Iteration 141/1000 | Loss: 0.00000859
Iteration 142/1000 | Loss: 0.00000859
Iteration 143/1000 | Loss: 0.00000859
Iteration 144/1000 | Loss: 0.00000859
Iteration 145/1000 | Loss: 0.00000859
Iteration 146/1000 | Loss: 0.00000859
Iteration 147/1000 | Loss: 0.00000859
Iteration 148/1000 | Loss: 0.00000859
Iteration 149/1000 | Loss: 0.00000859
Iteration 150/1000 | Loss: 0.00000859
Iteration 151/1000 | Loss: 0.00000859
Iteration 152/1000 | Loss: 0.00000859
Iteration 153/1000 | Loss: 0.00000859
Iteration 154/1000 | Loss: 0.00000859
Iteration 155/1000 | Loss: 0.00000859
Iteration 156/1000 | Loss: 0.00000859
Iteration 157/1000 | Loss: 0.00000859
Iteration 158/1000 | Loss: 0.00000859
Iteration 159/1000 | Loss: 0.00000859
Iteration 160/1000 | Loss: 0.00000859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [8.593367056164425e-06, 8.593367056164425e-06, 8.593367056164425e-06, 8.593367056164425e-06, 8.593367056164425e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.593367056164425e-06

Optimization complete. Final v2v error: 2.5353240966796875 mm

Highest mean error: 2.986798048019409 mm for frame 89

Lowest mean error: 2.3850743770599365 mm for frame 4

Saving results

Total time: 35.51893997192383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806401
Iteration 2/25 | Loss: 0.00136048
Iteration 3/25 | Loss: 0.00120493
Iteration 4/25 | Loss: 0.00119388
Iteration 5/25 | Loss: 0.00119254
Iteration 6/25 | Loss: 0.00119254
Iteration 7/25 | Loss: 0.00119254
Iteration 8/25 | Loss: 0.00119254
Iteration 9/25 | Loss: 0.00119254
Iteration 10/25 | Loss: 0.00119254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011925362050533295, 0.0011925362050533295, 0.0011925362050533295, 0.0011925362050533295, 0.0011925362050533295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011925362050533295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29470277
Iteration 2/25 | Loss: 0.00138378
Iteration 3/25 | Loss: 0.00138377
Iteration 4/25 | Loss: 0.00138377
Iteration 5/25 | Loss: 0.00138377
Iteration 6/25 | Loss: 0.00138377
Iteration 7/25 | Loss: 0.00138377
Iteration 8/25 | Loss: 0.00138377
Iteration 9/25 | Loss: 0.00138377
Iteration 10/25 | Loss: 0.00138377
Iteration 11/25 | Loss: 0.00138377
Iteration 12/25 | Loss: 0.00138377
Iteration 13/25 | Loss: 0.00138377
Iteration 14/25 | Loss: 0.00138377
Iteration 15/25 | Loss: 0.00138377
Iteration 16/25 | Loss: 0.00138377
Iteration 17/25 | Loss: 0.00138377
Iteration 18/25 | Loss: 0.00138377
Iteration 19/25 | Loss: 0.00138377
Iteration 20/25 | Loss: 0.00138377
Iteration 21/25 | Loss: 0.00138377
Iteration 22/25 | Loss: 0.00138377
Iteration 23/25 | Loss: 0.00138377
Iteration 24/25 | Loss: 0.00138377
Iteration 25/25 | Loss: 0.00138377

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138377
Iteration 2/1000 | Loss: 0.00002359
Iteration 3/1000 | Loss: 0.00001577
Iteration 4/1000 | Loss: 0.00001308
Iteration 5/1000 | Loss: 0.00001216
Iteration 6/1000 | Loss: 0.00001144
Iteration 7/1000 | Loss: 0.00001101
Iteration 8/1000 | Loss: 0.00001069
Iteration 9/1000 | Loss: 0.00001029
Iteration 10/1000 | Loss: 0.00001011
Iteration 11/1000 | Loss: 0.00000992
Iteration 12/1000 | Loss: 0.00000978
Iteration 13/1000 | Loss: 0.00000975
Iteration 14/1000 | Loss: 0.00000971
Iteration 15/1000 | Loss: 0.00000964
Iteration 16/1000 | Loss: 0.00000963
Iteration 17/1000 | Loss: 0.00000963
Iteration 18/1000 | Loss: 0.00000958
Iteration 19/1000 | Loss: 0.00000957
Iteration 20/1000 | Loss: 0.00000957
Iteration 21/1000 | Loss: 0.00000955
Iteration 22/1000 | Loss: 0.00000954
Iteration 23/1000 | Loss: 0.00000954
Iteration 24/1000 | Loss: 0.00000952
Iteration 25/1000 | Loss: 0.00000951
Iteration 26/1000 | Loss: 0.00000950
Iteration 27/1000 | Loss: 0.00000950
Iteration 28/1000 | Loss: 0.00000949
Iteration 29/1000 | Loss: 0.00000948
Iteration 30/1000 | Loss: 0.00000948
Iteration 31/1000 | Loss: 0.00000947
Iteration 32/1000 | Loss: 0.00000946
Iteration 33/1000 | Loss: 0.00000945
Iteration 34/1000 | Loss: 0.00000944
Iteration 35/1000 | Loss: 0.00000944
Iteration 36/1000 | Loss: 0.00000940
Iteration 37/1000 | Loss: 0.00000939
Iteration 38/1000 | Loss: 0.00000938
Iteration 39/1000 | Loss: 0.00000938
Iteration 40/1000 | Loss: 0.00000937
Iteration 41/1000 | Loss: 0.00000937
Iteration 42/1000 | Loss: 0.00000936
Iteration 43/1000 | Loss: 0.00000936
Iteration 44/1000 | Loss: 0.00000935
Iteration 45/1000 | Loss: 0.00000934
Iteration 46/1000 | Loss: 0.00000934
Iteration 47/1000 | Loss: 0.00000933
Iteration 48/1000 | Loss: 0.00000933
Iteration 49/1000 | Loss: 0.00000933
Iteration 50/1000 | Loss: 0.00000932
Iteration 51/1000 | Loss: 0.00000932
Iteration 52/1000 | Loss: 0.00000931
Iteration 53/1000 | Loss: 0.00000931
Iteration 54/1000 | Loss: 0.00000931
Iteration 55/1000 | Loss: 0.00000931
Iteration 56/1000 | Loss: 0.00000931
Iteration 57/1000 | Loss: 0.00000930
Iteration 58/1000 | Loss: 0.00000930
Iteration 59/1000 | Loss: 0.00000930
Iteration 60/1000 | Loss: 0.00000930
Iteration 61/1000 | Loss: 0.00000930
Iteration 62/1000 | Loss: 0.00000930
Iteration 63/1000 | Loss: 0.00000929
Iteration 64/1000 | Loss: 0.00000929
Iteration 65/1000 | Loss: 0.00000929
Iteration 66/1000 | Loss: 0.00000929
Iteration 67/1000 | Loss: 0.00000927
Iteration 68/1000 | Loss: 0.00000927
Iteration 69/1000 | Loss: 0.00000927
Iteration 70/1000 | Loss: 0.00000926
Iteration 71/1000 | Loss: 0.00000926
Iteration 72/1000 | Loss: 0.00000926
Iteration 73/1000 | Loss: 0.00000926
Iteration 74/1000 | Loss: 0.00000925
Iteration 75/1000 | Loss: 0.00000925
Iteration 76/1000 | Loss: 0.00000924
Iteration 77/1000 | Loss: 0.00000924
Iteration 78/1000 | Loss: 0.00000924
Iteration 79/1000 | Loss: 0.00000923
Iteration 80/1000 | Loss: 0.00000923
Iteration 81/1000 | Loss: 0.00000923
Iteration 82/1000 | Loss: 0.00000923
Iteration 83/1000 | Loss: 0.00000923
Iteration 84/1000 | Loss: 0.00000923
Iteration 85/1000 | Loss: 0.00000923
Iteration 86/1000 | Loss: 0.00000923
Iteration 87/1000 | Loss: 0.00000923
Iteration 88/1000 | Loss: 0.00000923
Iteration 89/1000 | Loss: 0.00000923
Iteration 90/1000 | Loss: 0.00000923
Iteration 91/1000 | Loss: 0.00000923
Iteration 92/1000 | Loss: 0.00000923
Iteration 93/1000 | Loss: 0.00000923
Iteration 94/1000 | Loss: 0.00000923
Iteration 95/1000 | Loss: 0.00000923
Iteration 96/1000 | Loss: 0.00000923
Iteration 97/1000 | Loss: 0.00000923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [9.231262083631009e-06, 9.231262083631009e-06, 9.231262083631009e-06, 9.231262083631009e-06, 9.231262083631009e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.231262083631009e-06

Optimization complete. Final v2v error: 2.620905876159668 mm

Highest mean error: 2.7881431579589844 mm for frame 150

Lowest mean error: 2.451665163040161 mm for frame 76

Saving results

Total time: 37.17538571357727
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481242
Iteration 2/25 | Loss: 0.00128138
Iteration 3/25 | Loss: 0.00120791
Iteration 4/25 | Loss: 0.00119836
Iteration 5/25 | Loss: 0.00119584
Iteration 6/25 | Loss: 0.00119584
Iteration 7/25 | Loss: 0.00119584
Iteration 8/25 | Loss: 0.00119584
Iteration 9/25 | Loss: 0.00119584
Iteration 10/25 | Loss: 0.00119584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011958419345319271, 0.0011958419345319271, 0.0011958419345319271, 0.0011958419345319271, 0.0011958419345319271]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011958419345319271

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.59601450
Iteration 2/25 | Loss: 0.00130496
Iteration 3/25 | Loss: 0.00130496
Iteration 4/25 | Loss: 0.00130495
Iteration 5/25 | Loss: 0.00130495
Iteration 6/25 | Loss: 0.00130495
Iteration 7/25 | Loss: 0.00130495
Iteration 8/25 | Loss: 0.00130495
Iteration 9/25 | Loss: 0.00130495
Iteration 10/25 | Loss: 0.00130495
Iteration 11/25 | Loss: 0.00130495
Iteration 12/25 | Loss: 0.00130495
Iteration 13/25 | Loss: 0.00130495
Iteration 14/25 | Loss: 0.00130495
Iteration 15/25 | Loss: 0.00130495
Iteration 16/25 | Loss: 0.00130495
Iteration 17/25 | Loss: 0.00130495
Iteration 18/25 | Loss: 0.00130495
Iteration 19/25 | Loss: 0.00130495
Iteration 20/25 | Loss: 0.00130495
Iteration 21/25 | Loss: 0.00130495
Iteration 22/25 | Loss: 0.00130495
Iteration 23/25 | Loss: 0.00130495
Iteration 24/25 | Loss: 0.00130495
Iteration 25/25 | Loss: 0.00130495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130495
Iteration 2/1000 | Loss: 0.00001979
Iteration 3/1000 | Loss: 0.00001612
Iteration 4/1000 | Loss: 0.00001468
Iteration 5/1000 | Loss: 0.00001396
Iteration 6/1000 | Loss: 0.00001340
Iteration 7/1000 | Loss: 0.00001303
Iteration 8/1000 | Loss: 0.00001269
Iteration 9/1000 | Loss: 0.00001232
Iteration 10/1000 | Loss: 0.00001228
Iteration 11/1000 | Loss: 0.00001216
Iteration 12/1000 | Loss: 0.00001205
Iteration 13/1000 | Loss: 0.00001197
Iteration 14/1000 | Loss: 0.00001195
Iteration 15/1000 | Loss: 0.00001194
Iteration 16/1000 | Loss: 0.00001190
Iteration 17/1000 | Loss: 0.00001187
Iteration 18/1000 | Loss: 0.00001186
Iteration 19/1000 | Loss: 0.00001174
Iteration 20/1000 | Loss: 0.00001171
Iteration 21/1000 | Loss: 0.00001170
Iteration 22/1000 | Loss: 0.00001168
Iteration 23/1000 | Loss: 0.00001166
Iteration 24/1000 | Loss: 0.00001162
Iteration 25/1000 | Loss: 0.00001161
Iteration 26/1000 | Loss: 0.00001157
Iteration 27/1000 | Loss: 0.00001156
Iteration 28/1000 | Loss: 0.00001155
Iteration 29/1000 | Loss: 0.00001154
Iteration 30/1000 | Loss: 0.00001154
Iteration 31/1000 | Loss: 0.00001153
Iteration 32/1000 | Loss: 0.00001147
Iteration 33/1000 | Loss: 0.00001142
Iteration 34/1000 | Loss: 0.00001141
Iteration 35/1000 | Loss: 0.00001140
Iteration 36/1000 | Loss: 0.00001140
Iteration 37/1000 | Loss: 0.00001139
Iteration 38/1000 | Loss: 0.00001139
Iteration 39/1000 | Loss: 0.00001139
Iteration 40/1000 | Loss: 0.00001138
Iteration 41/1000 | Loss: 0.00001137
Iteration 42/1000 | Loss: 0.00001136
Iteration 43/1000 | Loss: 0.00001136
Iteration 44/1000 | Loss: 0.00001136
Iteration 45/1000 | Loss: 0.00001136
Iteration 46/1000 | Loss: 0.00001135
Iteration 47/1000 | Loss: 0.00001135
Iteration 48/1000 | Loss: 0.00001135
Iteration 49/1000 | Loss: 0.00001135
Iteration 50/1000 | Loss: 0.00001134
Iteration 51/1000 | Loss: 0.00001131
Iteration 52/1000 | Loss: 0.00001131
Iteration 53/1000 | Loss: 0.00001131
Iteration 54/1000 | Loss: 0.00001130
Iteration 55/1000 | Loss: 0.00001129
Iteration 56/1000 | Loss: 0.00001129
Iteration 57/1000 | Loss: 0.00001128
Iteration 58/1000 | Loss: 0.00001128
Iteration 59/1000 | Loss: 0.00001128
Iteration 60/1000 | Loss: 0.00001127
Iteration 61/1000 | Loss: 0.00001127
Iteration 62/1000 | Loss: 0.00001127
Iteration 63/1000 | Loss: 0.00001127
Iteration 64/1000 | Loss: 0.00001126
Iteration 65/1000 | Loss: 0.00001126
Iteration 66/1000 | Loss: 0.00001126
Iteration 67/1000 | Loss: 0.00001126
Iteration 68/1000 | Loss: 0.00001126
Iteration 69/1000 | Loss: 0.00001125
Iteration 70/1000 | Loss: 0.00001125
Iteration 71/1000 | Loss: 0.00001125
Iteration 72/1000 | Loss: 0.00001125
Iteration 73/1000 | Loss: 0.00001124
Iteration 74/1000 | Loss: 0.00001124
Iteration 75/1000 | Loss: 0.00001124
Iteration 76/1000 | Loss: 0.00001124
Iteration 77/1000 | Loss: 0.00001124
Iteration 78/1000 | Loss: 0.00001123
Iteration 79/1000 | Loss: 0.00001123
Iteration 80/1000 | Loss: 0.00001123
Iteration 81/1000 | Loss: 0.00001123
Iteration 82/1000 | Loss: 0.00001122
Iteration 83/1000 | Loss: 0.00001122
Iteration 84/1000 | Loss: 0.00001122
Iteration 85/1000 | Loss: 0.00001122
Iteration 86/1000 | Loss: 0.00001121
Iteration 87/1000 | Loss: 0.00001121
Iteration 88/1000 | Loss: 0.00001121
Iteration 89/1000 | Loss: 0.00001121
Iteration 90/1000 | Loss: 0.00001121
Iteration 91/1000 | Loss: 0.00001121
Iteration 92/1000 | Loss: 0.00001121
Iteration 93/1000 | Loss: 0.00001120
Iteration 94/1000 | Loss: 0.00001120
Iteration 95/1000 | Loss: 0.00001120
Iteration 96/1000 | Loss: 0.00001120
Iteration 97/1000 | Loss: 0.00001120
Iteration 98/1000 | Loss: 0.00001119
Iteration 99/1000 | Loss: 0.00001119
Iteration 100/1000 | Loss: 0.00001119
Iteration 101/1000 | Loss: 0.00001119
Iteration 102/1000 | Loss: 0.00001118
Iteration 103/1000 | Loss: 0.00001118
Iteration 104/1000 | Loss: 0.00001118
Iteration 105/1000 | Loss: 0.00001118
Iteration 106/1000 | Loss: 0.00001118
Iteration 107/1000 | Loss: 0.00001117
Iteration 108/1000 | Loss: 0.00001117
Iteration 109/1000 | Loss: 0.00001117
Iteration 110/1000 | Loss: 0.00001117
Iteration 111/1000 | Loss: 0.00001117
Iteration 112/1000 | Loss: 0.00001117
Iteration 113/1000 | Loss: 0.00001117
Iteration 114/1000 | Loss: 0.00001117
Iteration 115/1000 | Loss: 0.00001117
Iteration 116/1000 | Loss: 0.00001117
Iteration 117/1000 | Loss: 0.00001117
Iteration 118/1000 | Loss: 0.00001117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.117282772611361e-05, 1.117282772611361e-05, 1.117282772611361e-05, 1.117282772611361e-05, 1.117282772611361e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.117282772611361e-05

Optimization complete. Final v2v error: 2.8776040077209473 mm

Highest mean error: 3.151956558227539 mm for frame 213

Lowest mean error: 2.6342172622680664 mm for frame 118

Saving results

Total time: 39.97487664222717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398313
Iteration 2/25 | Loss: 0.00125357
Iteration 3/25 | Loss: 0.00118888
Iteration 4/25 | Loss: 0.00118629
Iteration 5/25 | Loss: 0.00118629
Iteration 6/25 | Loss: 0.00118629
Iteration 7/25 | Loss: 0.00118629
Iteration 8/25 | Loss: 0.00118629
Iteration 9/25 | Loss: 0.00118629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0011862918036058545, 0.0011862918036058545, 0.0011862918036058545, 0.0011862918036058545, 0.0011862918036058545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011862918036058545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46496010
Iteration 2/25 | Loss: 0.00123511
Iteration 3/25 | Loss: 0.00123511
Iteration 4/25 | Loss: 0.00123511
Iteration 5/25 | Loss: 0.00123511
Iteration 6/25 | Loss: 0.00123511
Iteration 7/25 | Loss: 0.00123511
Iteration 8/25 | Loss: 0.00123511
Iteration 9/25 | Loss: 0.00123511
Iteration 10/25 | Loss: 0.00123511
Iteration 11/25 | Loss: 0.00123511
Iteration 12/25 | Loss: 0.00123511
Iteration 13/25 | Loss: 0.00123511
Iteration 14/25 | Loss: 0.00123511
Iteration 15/25 | Loss: 0.00123511
Iteration 16/25 | Loss: 0.00123511
Iteration 17/25 | Loss: 0.00123511
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012351081240922213, 0.0012351081240922213, 0.0012351081240922213, 0.0012351081240922213, 0.0012351081240922213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012351081240922213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123511
Iteration 2/1000 | Loss: 0.00002641
Iteration 3/1000 | Loss: 0.00001717
Iteration 4/1000 | Loss: 0.00001347
Iteration 5/1000 | Loss: 0.00001257
Iteration 6/1000 | Loss: 0.00001202
Iteration 7/1000 | Loss: 0.00001172
Iteration 8/1000 | Loss: 0.00001140
Iteration 9/1000 | Loss: 0.00001131
Iteration 10/1000 | Loss: 0.00001094
Iteration 11/1000 | Loss: 0.00001061
Iteration 12/1000 | Loss: 0.00001040
Iteration 13/1000 | Loss: 0.00001038
Iteration 14/1000 | Loss: 0.00001036
Iteration 15/1000 | Loss: 0.00001031
Iteration 16/1000 | Loss: 0.00001026
Iteration 17/1000 | Loss: 0.00001022
Iteration 18/1000 | Loss: 0.00001021
Iteration 19/1000 | Loss: 0.00001019
Iteration 20/1000 | Loss: 0.00001018
Iteration 21/1000 | Loss: 0.00001013
Iteration 22/1000 | Loss: 0.00001010
Iteration 23/1000 | Loss: 0.00001010
Iteration 24/1000 | Loss: 0.00001008
Iteration 25/1000 | Loss: 0.00001004
Iteration 26/1000 | Loss: 0.00000998
Iteration 27/1000 | Loss: 0.00000995
Iteration 28/1000 | Loss: 0.00000994
Iteration 29/1000 | Loss: 0.00000990
Iteration 30/1000 | Loss: 0.00000987
Iteration 31/1000 | Loss: 0.00000986
Iteration 32/1000 | Loss: 0.00000985
Iteration 33/1000 | Loss: 0.00000984
Iteration 34/1000 | Loss: 0.00000984
Iteration 35/1000 | Loss: 0.00000981
Iteration 36/1000 | Loss: 0.00000981
Iteration 37/1000 | Loss: 0.00000980
Iteration 38/1000 | Loss: 0.00000980
Iteration 39/1000 | Loss: 0.00000979
Iteration 40/1000 | Loss: 0.00000979
Iteration 41/1000 | Loss: 0.00000978
Iteration 42/1000 | Loss: 0.00000978
Iteration 43/1000 | Loss: 0.00000977
Iteration 44/1000 | Loss: 0.00000977
Iteration 45/1000 | Loss: 0.00000971
Iteration 46/1000 | Loss: 0.00000964
Iteration 47/1000 | Loss: 0.00000964
Iteration 48/1000 | Loss: 0.00000963
Iteration 49/1000 | Loss: 0.00000962
Iteration 50/1000 | Loss: 0.00000959
Iteration 51/1000 | Loss: 0.00000959
Iteration 52/1000 | Loss: 0.00000959
Iteration 53/1000 | Loss: 0.00000959
Iteration 54/1000 | Loss: 0.00000958
Iteration 55/1000 | Loss: 0.00000956
Iteration 56/1000 | Loss: 0.00000954
Iteration 57/1000 | Loss: 0.00000954
Iteration 58/1000 | Loss: 0.00000954
Iteration 59/1000 | Loss: 0.00000954
Iteration 60/1000 | Loss: 0.00000954
Iteration 61/1000 | Loss: 0.00000953
Iteration 62/1000 | Loss: 0.00000953
Iteration 63/1000 | Loss: 0.00000953
Iteration 64/1000 | Loss: 0.00000953
Iteration 65/1000 | Loss: 0.00000953
Iteration 66/1000 | Loss: 0.00000953
Iteration 67/1000 | Loss: 0.00000952
Iteration 68/1000 | Loss: 0.00000952
Iteration 69/1000 | Loss: 0.00000951
Iteration 70/1000 | Loss: 0.00000951
Iteration 71/1000 | Loss: 0.00000951
Iteration 72/1000 | Loss: 0.00000950
Iteration 73/1000 | Loss: 0.00000950
Iteration 74/1000 | Loss: 0.00000950
Iteration 75/1000 | Loss: 0.00000949
Iteration 76/1000 | Loss: 0.00000948
Iteration 77/1000 | Loss: 0.00000948
Iteration 78/1000 | Loss: 0.00000947
Iteration 79/1000 | Loss: 0.00000947
Iteration 80/1000 | Loss: 0.00000947
Iteration 81/1000 | Loss: 0.00000946
Iteration 82/1000 | Loss: 0.00000945
Iteration 83/1000 | Loss: 0.00000945
Iteration 84/1000 | Loss: 0.00000945
Iteration 85/1000 | Loss: 0.00000945
Iteration 86/1000 | Loss: 0.00000945
Iteration 87/1000 | Loss: 0.00000944
Iteration 88/1000 | Loss: 0.00000944
Iteration 89/1000 | Loss: 0.00000944
Iteration 90/1000 | Loss: 0.00000943
Iteration 91/1000 | Loss: 0.00000943
Iteration 92/1000 | Loss: 0.00000943
Iteration 93/1000 | Loss: 0.00000943
Iteration 94/1000 | Loss: 0.00000941
Iteration 95/1000 | Loss: 0.00000941
Iteration 96/1000 | Loss: 0.00000941
Iteration 97/1000 | Loss: 0.00000941
Iteration 98/1000 | Loss: 0.00000941
Iteration 99/1000 | Loss: 0.00000941
Iteration 100/1000 | Loss: 0.00000941
Iteration 101/1000 | Loss: 0.00000941
Iteration 102/1000 | Loss: 0.00000941
Iteration 103/1000 | Loss: 0.00000941
Iteration 104/1000 | Loss: 0.00000941
Iteration 105/1000 | Loss: 0.00000940
Iteration 106/1000 | Loss: 0.00000940
Iteration 107/1000 | Loss: 0.00000940
Iteration 108/1000 | Loss: 0.00000940
Iteration 109/1000 | Loss: 0.00000939
Iteration 110/1000 | Loss: 0.00000939
Iteration 111/1000 | Loss: 0.00000938
Iteration 112/1000 | Loss: 0.00000938
Iteration 113/1000 | Loss: 0.00000938
Iteration 114/1000 | Loss: 0.00000938
Iteration 115/1000 | Loss: 0.00000938
Iteration 116/1000 | Loss: 0.00000937
Iteration 117/1000 | Loss: 0.00000937
Iteration 118/1000 | Loss: 0.00000937
Iteration 119/1000 | Loss: 0.00000937
Iteration 120/1000 | Loss: 0.00000937
Iteration 121/1000 | Loss: 0.00000937
Iteration 122/1000 | Loss: 0.00000937
Iteration 123/1000 | Loss: 0.00000937
Iteration 124/1000 | Loss: 0.00000937
Iteration 125/1000 | Loss: 0.00000936
Iteration 126/1000 | Loss: 0.00000936
Iteration 127/1000 | Loss: 0.00000936
Iteration 128/1000 | Loss: 0.00000936
Iteration 129/1000 | Loss: 0.00000936
Iteration 130/1000 | Loss: 0.00000936
Iteration 131/1000 | Loss: 0.00000936
Iteration 132/1000 | Loss: 0.00000935
Iteration 133/1000 | Loss: 0.00000935
Iteration 134/1000 | Loss: 0.00000935
Iteration 135/1000 | Loss: 0.00000934
Iteration 136/1000 | Loss: 0.00000934
Iteration 137/1000 | Loss: 0.00000934
Iteration 138/1000 | Loss: 0.00000934
Iteration 139/1000 | Loss: 0.00000934
Iteration 140/1000 | Loss: 0.00000934
Iteration 141/1000 | Loss: 0.00000934
Iteration 142/1000 | Loss: 0.00000934
Iteration 143/1000 | Loss: 0.00000934
Iteration 144/1000 | Loss: 0.00000934
Iteration 145/1000 | Loss: 0.00000934
Iteration 146/1000 | Loss: 0.00000934
Iteration 147/1000 | Loss: 0.00000934
Iteration 148/1000 | Loss: 0.00000934
Iteration 149/1000 | Loss: 0.00000934
Iteration 150/1000 | Loss: 0.00000934
Iteration 151/1000 | Loss: 0.00000934
Iteration 152/1000 | Loss: 0.00000934
Iteration 153/1000 | Loss: 0.00000934
Iteration 154/1000 | Loss: 0.00000934
Iteration 155/1000 | Loss: 0.00000934
Iteration 156/1000 | Loss: 0.00000934
Iteration 157/1000 | Loss: 0.00000934
Iteration 158/1000 | Loss: 0.00000934
Iteration 159/1000 | Loss: 0.00000934
Iteration 160/1000 | Loss: 0.00000934
Iteration 161/1000 | Loss: 0.00000934
Iteration 162/1000 | Loss: 0.00000934
Iteration 163/1000 | Loss: 0.00000934
Iteration 164/1000 | Loss: 0.00000934
Iteration 165/1000 | Loss: 0.00000934
Iteration 166/1000 | Loss: 0.00000934
Iteration 167/1000 | Loss: 0.00000934
Iteration 168/1000 | Loss: 0.00000934
Iteration 169/1000 | Loss: 0.00000934
Iteration 170/1000 | Loss: 0.00000934
Iteration 171/1000 | Loss: 0.00000934
Iteration 172/1000 | Loss: 0.00000934
Iteration 173/1000 | Loss: 0.00000934
Iteration 174/1000 | Loss: 0.00000934
Iteration 175/1000 | Loss: 0.00000934
Iteration 176/1000 | Loss: 0.00000934
Iteration 177/1000 | Loss: 0.00000934
Iteration 178/1000 | Loss: 0.00000934
Iteration 179/1000 | Loss: 0.00000934
Iteration 180/1000 | Loss: 0.00000934
Iteration 181/1000 | Loss: 0.00000934
Iteration 182/1000 | Loss: 0.00000934
Iteration 183/1000 | Loss: 0.00000934
Iteration 184/1000 | Loss: 0.00000934
Iteration 185/1000 | Loss: 0.00000934
Iteration 186/1000 | Loss: 0.00000934
Iteration 187/1000 | Loss: 0.00000934
Iteration 188/1000 | Loss: 0.00000934
Iteration 189/1000 | Loss: 0.00000934
Iteration 190/1000 | Loss: 0.00000934
Iteration 191/1000 | Loss: 0.00000934
Iteration 192/1000 | Loss: 0.00000934
Iteration 193/1000 | Loss: 0.00000934
Iteration 194/1000 | Loss: 0.00000934
Iteration 195/1000 | Loss: 0.00000934
Iteration 196/1000 | Loss: 0.00000934
Iteration 197/1000 | Loss: 0.00000934
Iteration 198/1000 | Loss: 0.00000934
Iteration 199/1000 | Loss: 0.00000934
Iteration 200/1000 | Loss: 0.00000934
Iteration 201/1000 | Loss: 0.00000934
Iteration 202/1000 | Loss: 0.00000934
Iteration 203/1000 | Loss: 0.00000934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [9.338651580037549e-06, 9.338651580037549e-06, 9.338651580037549e-06, 9.338651580037549e-06, 9.338651580037549e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.338651580037549e-06

Optimization complete. Final v2v error: 2.6334388256073 mm

Highest mean error: 2.8282296657562256 mm for frame 152

Lowest mean error: 2.4924988746643066 mm for frame 245

Saving results

Total time: 45.067466735839844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956666
Iteration 2/25 | Loss: 0.00218935
Iteration 3/25 | Loss: 0.00149203
Iteration 4/25 | Loss: 0.00145105
Iteration 5/25 | Loss: 0.00143452
Iteration 6/25 | Loss: 0.00130627
Iteration 7/25 | Loss: 0.00128268
Iteration 8/25 | Loss: 0.00126610
Iteration 9/25 | Loss: 0.00126164
Iteration 10/25 | Loss: 0.00126279
Iteration 11/25 | Loss: 0.00125624
Iteration 12/25 | Loss: 0.00125354
Iteration 13/25 | Loss: 0.00125198
Iteration 14/25 | Loss: 0.00125440
Iteration 15/25 | Loss: 0.00125168
Iteration 16/25 | Loss: 0.00124950
Iteration 17/25 | Loss: 0.00124914
Iteration 18/25 | Loss: 0.00124904
Iteration 19/25 | Loss: 0.00124903
Iteration 20/25 | Loss: 0.00124903
Iteration 21/25 | Loss: 0.00124903
Iteration 22/25 | Loss: 0.00124903
Iteration 23/25 | Loss: 0.00124903
Iteration 24/25 | Loss: 0.00124903
Iteration 25/25 | Loss: 0.00124903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28628552
Iteration 2/25 | Loss: 0.00100717
Iteration 3/25 | Loss: 0.00100717
Iteration 4/25 | Loss: 0.00100717
Iteration 5/25 | Loss: 0.00100716
Iteration 6/25 | Loss: 0.00100716
Iteration 7/25 | Loss: 0.00100716
Iteration 8/25 | Loss: 0.00100716
Iteration 9/25 | Loss: 0.00100716
Iteration 10/25 | Loss: 0.00100716
Iteration 11/25 | Loss: 0.00100716
Iteration 12/25 | Loss: 0.00100716
Iteration 13/25 | Loss: 0.00100716
Iteration 14/25 | Loss: 0.00100716
Iteration 15/25 | Loss: 0.00100716
Iteration 16/25 | Loss: 0.00100716
Iteration 17/25 | Loss: 0.00100716
Iteration 18/25 | Loss: 0.00100716
Iteration 19/25 | Loss: 0.00100716
Iteration 20/25 | Loss: 0.00100716
Iteration 21/25 | Loss: 0.00100716
Iteration 22/25 | Loss: 0.00100716
Iteration 23/25 | Loss: 0.00100716
Iteration 24/25 | Loss: 0.00100716
Iteration 25/25 | Loss: 0.00100716
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010071630822494626, 0.0010071630822494626, 0.0010071630822494626, 0.0010071630822494626, 0.0010071630822494626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010071630822494626

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100716
Iteration 2/1000 | Loss: 0.00003433
Iteration 3/1000 | Loss: 0.00008528
Iteration 4/1000 | Loss: 0.00001917
Iteration 5/1000 | Loss: 0.00006788
Iteration 6/1000 | Loss: 0.00001747
Iteration 7/1000 | Loss: 0.00001687
Iteration 8/1000 | Loss: 0.00008602
Iteration 9/1000 | Loss: 0.00001620
Iteration 10/1000 | Loss: 0.00001588
Iteration 11/1000 | Loss: 0.00001545
Iteration 12/1000 | Loss: 0.00002282
Iteration 13/1000 | Loss: 0.00001772
Iteration 14/1000 | Loss: 0.00001599
Iteration 15/1000 | Loss: 0.00001541
Iteration 16/1000 | Loss: 0.00001499
Iteration 17/1000 | Loss: 0.00001475
Iteration 18/1000 | Loss: 0.00001471
Iteration 19/1000 | Loss: 0.00001461
Iteration 20/1000 | Loss: 0.00001457
Iteration 21/1000 | Loss: 0.00001452
Iteration 22/1000 | Loss: 0.00001449
Iteration 23/1000 | Loss: 0.00001448
Iteration 24/1000 | Loss: 0.00001448
Iteration 25/1000 | Loss: 0.00001448
Iteration 26/1000 | Loss: 0.00001447
Iteration 27/1000 | Loss: 0.00001447
Iteration 28/1000 | Loss: 0.00001447
Iteration 29/1000 | Loss: 0.00001447
Iteration 30/1000 | Loss: 0.00001447
Iteration 31/1000 | Loss: 0.00001447
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001446
Iteration 34/1000 | Loss: 0.00001446
Iteration 35/1000 | Loss: 0.00001446
Iteration 36/1000 | Loss: 0.00001445
Iteration 37/1000 | Loss: 0.00001444
Iteration 38/1000 | Loss: 0.00001444
Iteration 39/1000 | Loss: 0.00001444
Iteration 40/1000 | Loss: 0.00001443
Iteration 41/1000 | Loss: 0.00001442
Iteration 42/1000 | Loss: 0.00001441
Iteration 43/1000 | Loss: 0.00001441
Iteration 44/1000 | Loss: 0.00001440
Iteration 45/1000 | Loss: 0.00001440
Iteration 46/1000 | Loss: 0.00001438
Iteration 47/1000 | Loss: 0.00001438
Iteration 48/1000 | Loss: 0.00001438
Iteration 49/1000 | Loss: 0.00001437
Iteration 50/1000 | Loss: 0.00001437
Iteration 51/1000 | Loss: 0.00001435
Iteration 52/1000 | Loss: 0.00001433
Iteration 53/1000 | Loss: 0.00001433
Iteration 54/1000 | Loss: 0.00001433
Iteration 55/1000 | Loss: 0.00001433
Iteration 56/1000 | Loss: 0.00001433
Iteration 57/1000 | Loss: 0.00001433
Iteration 58/1000 | Loss: 0.00001433
Iteration 59/1000 | Loss: 0.00001432
Iteration 60/1000 | Loss: 0.00001432
Iteration 61/1000 | Loss: 0.00001432
Iteration 62/1000 | Loss: 0.00001432
Iteration 63/1000 | Loss: 0.00001432
Iteration 64/1000 | Loss: 0.00001432
Iteration 65/1000 | Loss: 0.00001432
Iteration 66/1000 | Loss: 0.00001432
Iteration 67/1000 | Loss: 0.00001432
Iteration 68/1000 | Loss: 0.00001431
Iteration 69/1000 | Loss: 0.00001431
Iteration 70/1000 | Loss: 0.00001431
Iteration 71/1000 | Loss: 0.00001430
Iteration 72/1000 | Loss: 0.00001430
Iteration 73/1000 | Loss: 0.00001430
Iteration 74/1000 | Loss: 0.00001430
Iteration 75/1000 | Loss: 0.00001429
Iteration 76/1000 | Loss: 0.00001429
Iteration 77/1000 | Loss: 0.00001429
Iteration 78/1000 | Loss: 0.00001429
Iteration 79/1000 | Loss: 0.00001428
Iteration 80/1000 | Loss: 0.00001428
Iteration 81/1000 | Loss: 0.00001428
Iteration 82/1000 | Loss: 0.00001428
Iteration 83/1000 | Loss: 0.00001428
Iteration 84/1000 | Loss: 0.00001428
Iteration 85/1000 | Loss: 0.00001428
Iteration 86/1000 | Loss: 0.00001428
Iteration 87/1000 | Loss: 0.00001427
Iteration 88/1000 | Loss: 0.00001427
Iteration 89/1000 | Loss: 0.00001427
Iteration 90/1000 | Loss: 0.00001427
Iteration 91/1000 | Loss: 0.00001427
Iteration 92/1000 | Loss: 0.00001427
Iteration 93/1000 | Loss: 0.00001427
Iteration 94/1000 | Loss: 0.00001427
Iteration 95/1000 | Loss: 0.00001427
Iteration 96/1000 | Loss: 0.00001427
Iteration 97/1000 | Loss: 0.00001427
Iteration 98/1000 | Loss: 0.00001427
Iteration 99/1000 | Loss: 0.00001427
Iteration 100/1000 | Loss: 0.00001427
Iteration 101/1000 | Loss: 0.00001427
Iteration 102/1000 | Loss: 0.00001427
Iteration 103/1000 | Loss: 0.00001427
Iteration 104/1000 | Loss: 0.00001427
Iteration 105/1000 | Loss: 0.00001427
Iteration 106/1000 | Loss: 0.00001427
Iteration 107/1000 | Loss: 0.00001427
Iteration 108/1000 | Loss: 0.00001427
Iteration 109/1000 | Loss: 0.00001427
Iteration 110/1000 | Loss: 0.00001427
Iteration 111/1000 | Loss: 0.00001427
Iteration 112/1000 | Loss: 0.00001427
Iteration 113/1000 | Loss: 0.00001427
Iteration 114/1000 | Loss: 0.00001427
Iteration 115/1000 | Loss: 0.00001427
Iteration 116/1000 | Loss: 0.00001427
Iteration 117/1000 | Loss: 0.00001427
Iteration 118/1000 | Loss: 0.00001427
Iteration 119/1000 | Loss: 0.00001427
Iteration 120/1000 | Loss: 0.00001427
Iteration 121/1000 | Loss: 0.00001427
Iteration 122/1000 | Loss: 0.00001427
Iteration 123/1000 | Loss: 0.00001427
Iteration 124/1000 | Loss: 0.00001427
Iteration 125/1000 | Loss: 0.00001427
Iteration 126/1000 | Loss: 0.00001427
Iteration 127/1000 | Loss: 0.00001427
Iteration 128/1000 | Loss: 0.00001427
Iteration 129/1000 | Loss: 0.00001427
Iteration 130/1000 | Loss: 0.00001427
Iteration 131/1000 | Loss: 0.00001427
Iteration 132/1000 | Loss: 0.00001427
Iteration 133/1000 | Loss: 0.00001427
Iteration 134/1000 | Loss: 0.00001427
Iteration 135/1000 | Loss: 0.00001427
Iteration 136/1000 | Loss: 0.00001427
Iteration 137/1000 | Loss: 0.00001427
Iteration 138/1000 | Loss: 0.00001427
Iteration 139/1000 | Loss: 0.00001427
Iteration 140/1000 | Loss: 0.00001427
Iteration 141/1000 | Loss: 0.00001427
Iteration 142/1000 | Loss: 0.00001427
Iteration 143/1000 | Loss: 0.00001427
Iteration 144/1000 | Loss: 0.00001427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.4267380720411893e-05, 1.4267380720411893e-05, 1.4267380720411893e-05, 1.4267380720411893e-05, 1.4267380720411893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4267380720411893e-05

Optimization complete. Final v2v error: 3.183791399002075 mm

Highest mean error: 4.557131767272949 mm for frame 97

Lowest mean error: 2.678863763809204 mm for frame 146

Saving results

Total time: 73.00245523452759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826948
Iteration 2/25 | Loss: 0.00138000
Iteration 3/25 | Loss: 0.00123429
Iteration 4/25 | Loss: 0.00120463
Iteration 5/25 | Loss: 0.00119321
Iteration 6/25 | Loss: 0.00119031
Iteration 7/25 | Loss: 0.00119016
Iteration 8/25 | Loss: 0.00119016
Iteration 9/25 | Loss: 0.00119016
Iteration 10/25 | Loss: 0.00119016
Iteration 11/25 | Loss: 0.00119016
Iteration 12/25 | Loss: 0.00119016
Iteration 13/25 | Loss: 0.00119016
Iteration 14/25 | Loss: 0.00119016
Iteration 15/25 | Loss: 0.00119016
Iteration 16/25 | Loss: 0.00119016
Iteration 17/25 | Loss: 0.00119016
Iteration 18/25 | Loss: 0.00119016
Iteration 19/25 | Loss: 0.00119016
Iteration 20/25 | Loss: 0.00119016
Iteration 21/25 | Loss: 0.00119016
Iteration 22/25 | Loss: 0.00119016
Iteration 23/25 | Loss: 0.00119016
Iteration 24/25 | Loss: 0.00119016
Iteration 25/25 | Loss: 0.00119016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44566667
Iteration 2/25 | Loss: 0.00183476
Iteration 3/25 | Loss: 0.00183476
Iteration 4/25 | Loss: 0.00183476
Iteration 5/25 | Loss: 0.00183476
Iteration 6/25 | Loss: 0.00183476
Iteration 7/25 | Loss: 0.00183476
Iteration 8/25 | Loss: 0.00183476
Iteration 9/25 | Loss: 0.00183476
Iteration 10/25 | Loss: 0.00183476
Iteration 11/25 | Loss: 0.00183476
Iteration 12/25 | Loss: 0.00183476
Iteration 13/25 | Loss: 0.00183476
Iteration 14/25 | Loss: 0.00183476
Iteration 15/25 | Loss: 0.00183476
Iteration 16/25 | Loss: 0.00183476
Iteration 17/25 | Loss: 0.00183476
Iteration 18/25 | Loss: 0.00183476
Iteration 19/25 | Loss: 0.00183476
Iteration 20/25 | Loss: 0.00183476
Iteration 21/25 | Loss: 0.00183476
Iteration 22/25 | Loss: 0.00183476
Iteration 23/25 | Loss: 0.00183476
Iteration 24/25 | Loss: 0.00183476
Iteration 25/25 | Loss: 0.00183476
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001834761817008257, 0.001834761817008257, 0.001834761817008257, 0.001834761817008257, 0.001834761817008257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001834761817008257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183476
Iteration 2/1000 | Loss: 0.00005588
Iteration 3/1000 | Loss: 0.00004203
Iteration 4/1000 | Loss: 0.00003244
Iteration 5/1000 | Loss: 0.00002954
Iteration 6/1000 | Loss: 0.00002778
Iteration 7/1000 | Loss: 0.00002634
Iteration 8/1000 | Loss: 0.00002553
Iteration 9/1000 | Loss: 0.00002492
Iteration 10/1000 | Loss: 0.00002447
Iteration 11/1000 | Loss: 0.00002413
Iteration 12/1000 | Loss: 0.00002384
Iteration 13/1000 | Loss: 0.00002356
Iteration 14/1000 | Loss: 0.00002336
Iteration 15/1000 | Loss: 0.00002329
Iteration 16/1000 | Loss: 0.00002313
Iteration 17/1000 | Loss: 0.00002309
Iteration 18/1000 | Loss: 0.00002306
Iteration 19/1000 | Loss: 0.00002299
Iteration 20/1000 | Loss: 0.00002295
Iteration 21/1000 | Loss: 0.00002294
Iteration 22/1000 | Loss: 0.00002293
Iteration 23/1000 | Loss: 0.00002293
Iteration 24/1000 | Loss: 0.00002292
Iteration 25/1000 | Loss: 0.00002292
Iteration 26/1000 | Loss: 0.00002289
Iteration 27/1000 | Loss: 0.00002286
Iteration 28/1000 | Loss: 0.00002285
Iteration 29/1000 | Loss: 0.00002285
Iteration 30/1000 | Loss: 0.00002285
Iteration 31/1000 | Loss: 0.00002284
Iteration 32/1000 | Loss: 0.00002284
Iteration 33/1000 | Loss: 0.00002284
Iteration 34/1000 | Loss: 0.00002283
Iteration 35/1000 | Loss: 0.00002283
Iteration 36/1000 | Loss: 0.00002282
Iteration 37/1000 | Loss: 0.00002282
Iteration 38/1000 | Loss: 0.00002281
Iteration 39/1000 | Loss: 0.00002281
Iteration 40/1000 | Loss: 0.00002281
Iteration 41/1000 | Loss: 0.00002280
Iteration 42/1000 | Loss: 0.00002280
Iteration 43/1000 | Loss: 0.00002279
Iteration 44/1000 | Loss: 0.00002279
Iteration 45/1000 | Loss: 0.00002279
Iteration 46/1000 | Loss: 0.00002278
Iteration 47/1000 | Loss: 0.00002278
Iteration 48/1000 | Loss: 0.00002277
Iteration 49/1000 | Loss: 0.00002277
Iteration 50/1000 | Loss: 0.00002277
Iteration 51/1000 | Loss: 0.00002276
Iteration 52/1000 | Loss: 0.00002276
Iteration 53/1000 | Loss: 0.00002276
Iteration 54/1000 | Loss: 0.00002275
Iteration 55/1000 | Loss: 0.00002275
Iteration 56/1000 | Loss: 0.00002275
Iteration 57/1000 | Loss: 0.00002273
Iteration 58/1000 | Loss: 0.00002273
Iteration 59/1000 | Loss: 0.00002273
Iteration 60/1000 | Loss: 0.00002273
Iteration 61/1000 | Loss: 0.00002273
Iteration 62/1000 | Loss: 0.00002273
Iteration 63/1000 | Loss: 0.00002272
Iteration 64/1000 | Loss: 0.00002272
Iteration 65/1000 | Loss: 0.00002272
Iteration 66/1000 | Loss: 0.00002271
Iteration 67/1000 | Loss: 0.00002271
Iteration 68/1000 | Loss: 0.00002271
Iteration 69/1000 | Loss: 0.00002270
Iteration 70/1000 | Loss: 0.00002270
Iteration 71/1000 | Loss: 0.00002270
Iteration 72/1000 | Loss: 0.00002269
Iteration 73/1000 | Loss: 0.00002269
Iteration 74/1000 | Loss: 0.00002269
Iteration 75/1000 | Loss: 0.00002268
Iteration 76/1000 | Loss: 0.00002268
Iteration 77/1000 | Loss: 0.00002267
Iteration 78/1000 | Loss: 0.00002267
Iteration 79/1000 | Loss: 0.00002267
Iteration 80/1000 | Loss: 0.00002266
Iteration 81/1000 | Loss: 0.00002266
Iteration 82/1000 | Loss: 0.00002266
Iteration 83/1000 | Loss: 0.00002266
Iteration 84/1000 | Loss: 0.00002266
Iteration 85/1000 | Loss: 0.00002265
Iteration 86/1000 | Loss: 0.00002265
Iteration 87/1000 | Loss: 0.00002265
Iteration 88/1000 | Loss: 0.00002265
Iteration 89/1000 | Loss: 0.00002265
Iteration 90/1000 | Loss: 0.00002265
Iteration 91/1000 | Loss: 0.00002264
Iteration 92/1000 | Loss: 0.00002264
Iteration 93/1000 | Loss: 0.00002264
Iteration 94/1000 | Loss: 0.00002264
Iteration 95/1000 | Loss: 0.00002263
Iteration 96/1000 | Loss: 0.00002263
Iteration 97/1000 | Loss: 0.00002263
Iteration 98/1000 | Loss: 0.00002263
Iteration 99/1000 | Loss: 0.00002263
Iteration 100/1000 | Loss: 0.00002262
Iteration 101/1000 | Loss: 0.00002262
Iteration 102/1000 | Loss: 0.00002262
Iteration 103/1000 | Loss: 0.00002262
Iteration 104/1000 | Loss: 0.00002262
Iteration 105/1000 | Loss: 0.00002262
Iteration 106/1000 | Loss: 0.00002262
Iteration 107/1000 | Loss: 0.00002262
Iteration 108/1000 | Loss: 0.00002261
Iteration 109/1000 | Loss: 0.00002261
Iteration 110/1000 | Loss: 0.00002261
Iteration 111/1000 | Loss: 0.00002261
Iteration 112/1000 | Loss: 0.00002261
Iteration 113/1000 | Loss: 0.00002261
Iteration 114/1000 | Loss: 0.00002261
Iteration 115/1000 | Loss: 0.00002261
Iteration 116/1000 | Loss: 0.00002261
Iteration 117/1000 | Loss: 0.00002260
Iteration 118/1000 | Loss: 0.00002260
Iteration 119/1000 | Loss: 0.00002260
Iteration 120/1000 | Loss: 0.00002260
Iteration 121/1000 | Loss: 0.00002260
Iteration 122/1000 | Loss: 0.00002260
Iteration 123/1000 | Loss: 0.00002260
Iteration 124/1000 | Loss: 0.00002260
Iteration 125/1000 | Loss: 0.00002259
Iteration 126/1000 | Loss: 0.00002259
Iteration 127/1000 | Loss: 0.00002259
Iteration 128/1000 | Loss: 0.00002259
Iteration 129/1000 | Loss: 0.00002259
Iteration 130/1000 | Loss: 0.00002259
Iteration 131/1000 | Loss: 0.00002259
Iteration 132/1000 | Loss: 0.00002259
Iteration 133/1000 | Loss: 0.00002259
Iteration 134/1000 | Loss: 0.00002258
Iteration 135/1000 | Loss: 0.00002258
Iteration 136/1000 | Loss: 0.00002258
Iteration 137/1000 | Loss: 0.00002258
Iteration 138/1000 | Loss: 0.00002258
Iteration 139/1000 | Loss: 0.00002258
Iteration 140/1000 | Loss: 0.00002258
Iteration 141/1000 | Loss: 0.00002258
Iteration 142/1000 | Loss: 0.00002258
Iteration 143/1000 | Loss: 0.00002257
Iteration 144/1000 | Loss: 0.00002257
Iteration 145/1000 | Loss: 0.00002257
Iteration 146/1000 | Loss: 0.00002257
Iteration 147/1000 | Loss: 0.00002257
Iteration 148/1000 | Loss: 0.00002257
Iteration 149/1000 | Loss: 0.00002256
Iteration 150/1000 | Loss: 0.00002256
Iteration 151/1000 | Loss: 0.00002256
Iteration 152/1000 | Loss: 0.00002256
Iteration 153/1000 | Loss: 0.00002256
Iteration 154/1000 | Loss: 0.00002256
Iteration 155/1000 | Loss: 0.00002256
Iteration 156/1000 | Loss: 0.00002255
Iteration 157/1000 | Loss: 0.00002255
Iteration 158/1000 | Loss: 0.00002255
Iteration 159/1000 | Loss: 0.00002255
Iteration 160/1000 | Loss: 0.00002255
Iteration 161/1000 | Loss: 0.00002255
Iteration 162/1000 | Loss: 0.00002254
Iteration 163/1000 | Loss: 0.00002254
Iteration 164/1000 | Loss: 0.00002254
Iteration 165/1000 | Loss: 0.00002254
Iteration 166/1000 | Loss: 0.00002254
Iteration 167/1000 | Loss: 0.00002253
Iteration 168/1000 | Loss: 0.00002253
Iteration 169/1000 | Loss: 0.00002253
Iteration 170/1000 | Loss: 0.00002253
Iteration 171/1000 | Loss: 0.00002253
Iteration 172/1000 | Loss: 0.00002253
Iteration 173/1000 | Loss: 0.00002253
Iteration 174/1000 | Loss: 0.00002252
Iteration 175/1000 | Loss: 0.00002252
Iteration 176/1000 | Loss: 0.00002252
Iteration 177/1000 | Loss: 0.00002252
Iteration 178/1000 | Loss: 0.00002252
Iteration 179/1000 | Loss: 0.00002252
Iteration 180/1000 | Loss: 0.00002252
Iteration 181/1000 | Loss: 0.00002252
Iteration 182/1000 | Loss: 0.00002252
Iteration 183/1000 | Loss: 0.00002251
Iteration 184/1000 | Loss: 0.00002251
Iteration 185/1000 | Loss: 0.00002251
Iteration 186/1000 | Loss: 0.00002251
Iteration 187/1000 | Loss: 0.00002251
Iteration 188/1000 | Loss: 0.00002251
Iteration 189/1000 | Loss: 0.00002251
Iteration 190/1000 | Loss: 0.00002251
Iteration 191/1000 | Loss: 0.00002251
Iteration 192/1000 | Loss: 0.00002250
Iteration 193/1000 | Loss: 0.00002250
Iteration 194/1000 | Loss: 0.00002250
Iteration 195/1000 | Loss: 0.00002250
Iteration 196/1000 | Loss: 0.00002249
Iteration 197/1000 | Loss: 0.00002249
Iteration 198/1000 | Loss: 0.00002249
Iteration 199/1000 | Loss: 0.00002249
Iteration 200/1000 | Loss: 0.00002249
Iteration 201/1000 | Loss: 0.00002249
Iteration 202/1000 | Loss: 0.00002249
Iteration 203/1000 | Loss: 0.00002249
Iteration 204/1000 | Loss: 0.00002249
Iteration 205/1000 | Loss: 0.00002248
Iteration 206/1000 | Loss: 0.00002248
Iteration 207/1000 | Loss: 0.00002248
Iteration 208/1000 | Loss: 0.00002248
Iteration 209/1000 | Loss: 0.00002248
Iteration 210/1000 | Loss: 0.00002248
Iteration 211/1000 | Loss: 0.00002248
Iteration 212/1000 | Loss: 0.00002247
Iteration 213/1000 | Loss: 0.00002247
Iteration 214/1000 | Loss: 0.00002247
Iteration 215/1000 | Loss: 0.00002247
Iteration 216/1000 | Loss: 0.00002247
Iteration 217/1000 | Loss: 0.00002247
Iteration 218/1000 | Loss: 0.00002247
Iteration 219/1000 | Loss: 0.00002247
Iteration 220/1000 | Loss: 0.00002247
Iteration 221/1000 | Loss: 0.00002247
Iteration 222/1000 | Loss: 0.00002246
Iteration 223/1000 | Loss: 0.00002246
Iteration 224/1000 | Loss: 0.00002246
Iteration 225/1000 | Loss: 0.00002245
Iteration 226/1000 | Loss: 0.00002245
Iteration 227/1000 | Loss: 0.00002245
Iteration 228/1000 | Loss: 0.00002245
Iteration 229/1000 | Loss: 0.00002245
Iteration 230/1000 | Loss: 0.00002245
Iteration 231/1000 | Loss: 0.00002245
Iteration 232/1000 | Loss: 0.00002245
Iteration 233/1000 | Loss: 0.00002245
Iteration 234/1000 | Loss: 0.00002245
Iteration 235/1000 | Loss: 0.00002245
Iteration 236/1000 | Loss: 0.00002245
Iteration 237/1000 | Loss: 0.00002245
Iteration 238/1000 | Loss: 0.00002245
Iteration 239/1000 | Loss: 0.00002245
Iteration 240/1000 | Loss: 0.00002244
Iteration 241/1000 | Loss: 0.00002244
Iteration 242/1000 | Loss: 0.00002244
Iteration 243/1000 | Loss: 0.00002244
Iteration 244/1000 | Loss: 0.00002244
Iteration 245/1000 | Loss: 0.00002244
Iteration 246/1000 | Loss: 0.00002244
Iteration 247/1000 | Loss: 0.00002244
Iteration 248/1000 | Loss: 0.00002244
Iteration 249/1000 | Loss: 0.00002244
Iteration 250/1000 | Loss: 0.00002244
Iteration 251/1000 | Loss: 0.00002244
Iteration 252/1000 | Loss: 0.00002244
Iteration 253/1000 | Loss: 0.00002244
Iteration 254/1000 | Loss: 0.00002244
Iteration 255/1000 | Loss: 0.00002243
Iteration 256/1000 | Loss: 0.00002243
Iteration 257/1000 | Loss: 0.00002243
Iteration 258/1000 | Loss: 0.00002243
Iteration 259/1000 | Loss: 0.00002243
Iteration 260/1000 | Loss: 0.00002243
Iteration 261/1000 | Loss: 0.00002243
Iteration 262/1000 | Loss: 0.00002243
Iteration 263/1000 | Loss: 0.00002243
Iteration 264/1000 | Loss: 0.00002243
Iteration 265/1000 | Loss: 0.00002243
Iteration 266/1000 | Loss: 0.00002242
Iteration 267/1000 | Loss: 0.00002242
Iteration 268/1000 | Loss: 0.00002242
Iteration 269/1000 | Loss: 0.00002242
Iteration 270/1000 | Loss: 0.00002242
Iteration 271/1000 | Loss: 0.00002242
Iteration 272/1000 | Loss: 0.00002242
Iteration 273/1000 | Loss: 0.00002242
Iteration 274/1000 | Loss: 0.00002242
Iteration 275/1000 | Loss: 0.00002242
Iteration 276/1000 | Loss: 0.00002242
Iteration 277/1000 | Loss: 0.00002242
Iteration 278/1000 | Loss: 0.00002242
Iteration 279/1000 | Loss: 0.00002242
Iteration 280/1000 | Loss: 0.00002242
Iteration 281/1000 | Loss: 0.00002242
Iteration 282/1000 | Loss: 0.00002242
Iteration 283/1000 | Loss: 0.00002242
Iteration 284/1000 | Loss: 0.00002242
Iteration 285/1000 | Loss: 0.00002242
Iteration 286/1000 | Loss: 0.00002242
Iteration 287/1000 | Loss: 0.00002242
Iteration 288/1000 | Loss: 0.00002242
Iteration 289/1000 | Loss: 0.00002242
Iteration 290/1000 | Loss: 0.00002242
Iteration 291/1000 | Loss: 0.00002242
Iteration 292/1000 | Loss: 0.00002242
Iteration 293/1000 | Loss: 0.00002242
Iteration 294/1000 | Loss: 0.00002242
Iteration 295/1000 | Loss: 0.00002242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [2.241589936602395e-05, 2.241589936602395e-05, 2.241589936602395e-05, 2.241589936602395e-05, 2.241589936602395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.241589936602395e-05

Optimization complete. Final v2v error: 3.89349365234375 mm

Highest mean error: 5.503674507141113 mm for frame 100

Lowest mean error: 2.4818105697631836 mm for frame 15

Saving results

Total time: 51.24532961845398
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401694
Iteration 2/25 | Loss: 0.00139971
Iteration 3/25 | Loss: 0.00120144
Iteration 4/25 | Loss: 0.00118536
Iteration 5/25 | Loss: 0.00118312
Iteration 6/25 | Loss: 0.00118273
Iteration 7/25 | Loss: 0.00118273
Iteration 8/25 | Loss: 0.00118273
Iteration 9/25 | Loss: 0.00118273
Iteration 10/25 | Loss: 0.00118273
Iteration 11/25 | Loss: 0.00118273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011827272828668356, 0.0011827272828668356, 0.0011827272828668356, 0.0011827272828668356, 0.0011827272828668356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011827272828668356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28221679
Iteration 2/25 | Loss: 0.00111363
Iteration 3/25 | Loss: 0.00111363
Iteration 4/25 | Loss: 0.00111363
Iteration 5/25 | Loss: 0.00111363
Iteration 6/25 | Loss: 0.00111363
Iteration 7/25 | Loss: 0.00111363
Iteration 8/25 | Loss: 0.00111363
Iteration 9/25 | Loss: 0.00111363
Iteration 10/25 | Loss: 0.00111363
Iteration 11/25 | Loss: 0.00111363
Iteration 12/25 | Loss: 0.00111363
Iteration 13/25 | Loss: 0.00111363
Iteration 14/25 | Loss: 0.00111363
Iteration 15/25 | Loss: 0.00111363
Iteration 16/25 | Loss: 0.00111363
Iteration 17/25 | Loss: 0.00111363
Iteration 18/25 | Loss: 0.00111363
Iteration 19/25 | Loss: 0.00111363
Iteration 20/25 | Loss: 0.00111363
Iteration 21/25 | Loss: 0.00111363
Iteration 22/25 | Loss: 0.00111363
Iteration 23/25 | Loss: 0.00111363
Iteration 24/25 | Loss: 0.00111363
Iteration 25/25 | Loss: 0.00111363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111363
Iteration 2/1000 | Loss: 0.00002391
Iteration 3/1000 | Loss: 0.00001805
Iteration 4/1000 | Loss: 0.00001564
Iteration 5/1000 | Loss: 0.00001453
Iteration 6/1000 | Loss: 0.00001379
Iteration 7/1000 | Loss: 0.00001327
Iteration 8/1000 | Loss: 0.00001282
Iteration 9/1000 | Loss: 0.00001251
Iteration 10/1000 | Loss: 0.00001224
Iteration 11/1000 | Loss: 0.00001214
Iteration 12/1000 | Loss: 0.00001198
Iteration 13/1000 | Loss: 0.00001195
Iteration 14/1000 | Loss: 0.00001189
Iteration 15/1000 | Loss: 0.00001188
Iteration 16/1000 | Loss: 0.00001187
Iteration 17/1000 | Loss: 0.00001180
Iteration 18/1000 | Loss: 0.00001177
Iteration 19/1000 | Loss: 0.00001173
Iteration 20/1000 | Loss: 0.00001173
Iteration 21/1000 | Loss: 0.00001173
Iteration 22/1000 | Loss: 0.00001173
Iteration 23/1000 | Loss: 0.00001173
Iteration 24/1000 | Loss: 0.00001173
Iteration 25/1000 | Loss: 0.00001173
Iteration 26/1000 | Loss: 0.00001173
Iteration 27/1000 | Loss: 0.00001173
Iteration 28/1000 | Loss: 0.00001173
Iteration 29/1000 | Loss: 0.00001172
Iteration 30/1000 | Loss: 0.00001172
Iteration 31/1000 | Loss: 0.00001169
Iteration 32/1000 | Loss: 0.00001168
Iteration 33/1000 | Loss: 0.00001168
Iteration 34/1000 | Loss: 0.00001168
Iteration 35/1000 | Loss: 0.00001168
Iteration 36/1000 | Loss: 0.00001167
Iteration 37/1000 | Loss: 0.00001167
Iteration 38/1000 | Loss: 0.00001162
Iteration 39/1000 | Loss: 0.00001160
Iteration 40/1000 | Loss: 0.00001159
Iteration 41/1000 | Loss: 0.00001157
Iteration 42/1000 | Loss: 0.00001157
Iteration 43/1000 | Loss: 0.00001156
Iteration 44/1000 | Loss: 0.00001156
Iteration 45/1000 | Loss: 0.00001156
Iteration 46/1000 | Loss: 0.00001155
Iteration 47/1000 | Loss: 0.00001154
Iteration 48/1000 | Loss: 0.00001153
Iteration 49/1000 | Loss: 0.00001152
Iteration 50/1000 | Loss: 0.00001151
Iteration 51/1000 | Loss: 0.00001150
Iteration 52/1000 | Loss: 0.00001145
Iteration 53/1000 | Loss: 0.00001143
Iteration 54/1000 | Loss: 0.00001142
Iteration 55/1000 | Loss: 0.00001141
Iteration 56/1000 | Loss: 0.00001140
Iteration 57/1000 | Loss: 0.00001139
Iteration 58/1000 | Loss: 0.00001139
Iteration 59/1000 | Loss: 0.00001138
Iteration 60/1000 | Loss: 0.00001137
Iteration 61/1000 | Loss: 0.00001136
Iteration 62/1000 | Loss: 0.00001136
Iteration 63/1000 | Loss: 0.00001135
Iteration 64/1000 | Loss: 0.00001135
Iteration 65/1000 | Loss: 0.00001134
Iteration 66/1000 | Loss: 0.00001133
Iteration 67/1000 | Loss: 0.00001133
Iteration 68/1000 | Loss: 0.00001132
Iteration 69/1000 | Loss: 0.00001132
Iteration 70/1000 | Loss: 0.00001131
Iteration 71/1000 | Loss: 0.00001131
Iteration 72/1000 | Loss: 0.00001130
Iteration 73/1000 | Loss: 0.00001129
Iteration 74/1000 | Loss: 0.00001129
Iteration 75/1000 | Loss: 0.00001129
Iteration 76/1000 | Loss: 0.00001129
Iteration 77/1000 | Loss: 0.00001129
Iteration 78/1000 | Loss: 0.00001129
Iteration 79/1000 | Loss: 0.00001129
Iteration 80/1000 | Loss: 0.00001128
Iteration 81/1000 | Loss: 0.00001128
Iteration 82/1000 | Loss: 0.00001128
Iteration 83/1000 | Loss: 0.00001128
Iteration 84/1000 | Loss: 0.00001127
Iteration 85/1000 | Loss: 0.00001127
Iteration 86/1000 | Loss: 0.00001126
Iteration 87/1000 | Loss: 0.00001126
Iteration 88/1000 | Loss: 0.00001126
Iteration 89/1000 | Loss: 0.00001125
Iteration 90/1000 | Loss: 0.00001125
Iteration 91/1000 | Loss: 0.00001125
Iteration 92/1000 | Loss: 0.00001124
Iteration 93/1000 | Loss: 0.00001124
Iteration 94/1000 | Loss: 0.00001124
Iteration 95/1000 | Loss: 0.00001123
Iteration 96/1000 | Loss: 0.00001123
Iteration 97/1000 | Loss: 0.00001123
Iteration 98/1000 | Loss: 0.00001123
Iteration 99/1000 | Loss: 0.00001123
Iteration 100/1000 | Loss: 0.00001123
Iteration 101/1000 | Loss: 0.00001123
Iteration 102/1000 | Loss: 0.00001123
Iteration 103/1000 | Loss: 0.00001123
Iteration 104/1000 | Loss: 0.00001123
Iteration 105/1000 | Loss: 0.00001123
Iteration 106/1000 | Loss: 0.00001123
Iteration 107/1000 | Loss: 0.00001122
Iteration 108/1000 | Loss: 0.00001122
Iteration 109/1000 | Loss: 0.00001122
Iteration 110/1000 | Loss: 0.00001122
Iteration 111/1000 | Loss: 0.00001122
Iteration 112/1000 | Loss: 0.00001122
Iteration 113/1000 | Loss: 0.00001122
Iteration 114/1000 | Loss: 0.00001122
Iteration 115/1000 | Loss: 0.00001121
Iteration 116/1000 | Loss: 0.00001121
Iteration 117/1000 | Loss: 0.00001121
Iteration 118/1000 | Loss: 0.00001121
Iteration 119/1000 | Loss: 0.00001121
Iteration 120/1000 | Loss: 0.00001121
Iteration 121/1000 | Loss: 0.00001121
Iteration 122/1000 | Loss: 0.00001121
Iteration 123/1000 | Loss: 0.00001120
Iteration 124/1000 | Loss: 0.00001120
Iteration 125/1000 | Loss: 0.00001120
Iteration 126/1000 | Loss: 0.00001120
Iteration 127/1000 | Loss: 0.00001120
Iteration 128/1000 | Loss: 0.00001120
Iteration 129/1000 | Loss: 0.00001120
Iteration 130/1000 | Loss: 0.00001120
Iteration 131/1000 | Loss: 0.00001120
Iteration 132/1000 | Loss: 0.00001119
Iteration 133/1000 | Loss: 0.00001119
Iteration 134/1000 | Loss: 0.00001119
Iteration 135/1000 | Loss: 0.00001119
Iteration 136/1000 | Loss: 0.00001119
Iteration 137/1000 | Loss: 0.00001119
Iteration 138/1000 | Loss: 0.00001119
Iteration 139/1000 | Loss: 0.00001119
Iteration 140/1000 | Loss: 0.00001119
Iteration 141/1000 | Loss: 0.00001119
Iteration 142/1000 | Loss: 0.00001119
Iteration 143/1000 | Loss: 0.00001119
Iteration 144/1000 | Loss: 0.00001119
Iteration 145/1000 | Loss: 0.00001119
Iteration 146/1000 | Loss: 0.00001119
Iteration 147/1000 | Loss: 0.00001119
Iteration 148/1000 | Loss: 0.00001118
Iteration 149/1000 | Loss: 0.00001118
Iteration 150/1000 | Loss: 0.00001118
Iteration 151/1000 | Loss: 0.00001118
Iteration 152/1000 | Loss: 0.00001118
Iteration 153/1000 | Loss: 0.00001118
Iteration 154/1000 | Loss: 0.00001117
Iteration 155/1000 | Loss: 0.00001117
Iteration 156/1000 | Loss: 0.00001117
Iteration 157/1000 | Loss: 0.00001117
Iteration 158/1000 | Loss: 0.00001117
Iteration 159/1000 | Loss: 0.00001117
Iteration 160/1000 | Loss: 0.00001117
Iteration 161/1000 | Loss: 0.00001117
Iteration 162/1000 | Loss: 0.00001117
Iteration 163/1000 | Loss: 0.00001116
Iteration 164/1000 | Loss: 0.00001116
Iteration 165/1000 | Loss: 0.00001116
Iteration 166/1000 | Loss: 0.00001116
Iteration 167/1000 | Loss: 0.00001116
Iteration 168/1000 | Loss: 0.00001116
Iteration 169/1000 | Loss: 0.00001116
Iteration 170/1000 | Loss: 0.00001116
Iteration 171/1000 | Loss: 0.00001116
Iteration 172/1000 | Loss: 0.00001115
Iteration 173/1000 | Loss: 0.00001115
Iteration 174/1000 | Loss: 0.00001115
Iteration 175/1000 | Loss: 0.00001115
Iteration 176/1000 | Loss: 0.00001114
Iteration 177/1000 | Loss: 0.00001114
Iteration 178/1000 | Loss: 0.00001114
Iteration 179/1000 | Loss: 0.00001114
Iteration 180/1000 | Loss: 0.00001114
Iteration 181/1000 | Loss: 0.00001114
Iteration 182/1000 | Loss: 0.00001114
Iteration 183/1000 | Loss: 0.00001114
Iteration 184/1000 | Loss: 0.00001114
Iteration 185/1000 | Loss: 0.00001114
Iteration 186/1000 | Loss: 0.00001113
Iteration 187/1000 | Loss: 0.00001113
Iteration 188/1000 | Loss: 0.00001113
Iteration 189/1000 | Loss: 0.00001113
Iteration 190/1000 | Loss: 0.00001112
Iteration 191/1000 | Loss: 0.00001112
Iteration 192/1000 | Loss: 0.00001112
Iteration 193/1000 | Loss: 0.00001112
Iteration 194/1000 | Loss: 0.00001112
Iteration 195/1000 | Loss: 0.00001112
Iteration 196/1000 | Loss: 0.00001112
Iteration 197/1000 | Loss: 0.00001111
Iteration 198/1000 | Loss: 0.00001111
Iteration 199/1000 | Loss: 0.00001111
Iteration 200/1000 | Loss: 0.00001111
Iteration 201/1000 | Loss: 0.00001111
Iteration 202/1000 | Loss: 0.00001111
Iteration 203/1000 | Loss: 0.00001111
Iteration 204/1000 | Loss: 0.00001110
Iteration 205/1000 | Loss: 0.00001110
Iteration 206/1000 | Loss: 0.00001109
Iteration 207/1000 | Loss: 0.00001109
Iteration 208/1000 | Loss: 0.00001109
Iteration 209/1000 | Loss: 0.00001109
Iteration 210/1000 | Loss: 0.00001109
Iteration 211/1000 | Loss: 0.00001109
Iteration 212/1000 | Loss: 0.00001109
Iteration 213/1000 | Loss: 0.00001109
Iteration 214/1000 | Loss: 0.00001108
Iteration 215/1000 | Loss: 0.00001108
Iteration 216/1000 | Loss: 0.00001108
Iteration 217/1000 | Loss: 0.00001108
Iteration 218/1000 | Loss: 0.00001107
Iteration 219/1000 | Loss: 0.00001107
Iteration 220/1000 | Loss: 0.00001107
Iteration 221/1000 | Loss: 0.00001107
Iteration 222/1000 | Loss: 0.00001106
Iteration 223/1000 | Loss: 0.00001106
Iteration 224/1000 | Loss: 0.00001106
Iteration 225/1000 | Loss: 0.00001106
Iteration 226/1000 | Loss: 0.00001106
Iteration 227/1000 | Loss: 0.00001106
Iteration 228/1000 | Loss: 0.00001106
Iteration 229/1000 | Loss: 0.00001106
Iteration 230/1000 | Loss: 0.00001106
Iteration 231/1000 | Loss: 0.00001106
Iteration 232/1000 | Loss: 0.00001106
Iteration 233/1000 | Loss: 0.00001106
Iteration 234/1000 | Loss: 0.00001106
Iteration 235/1000 | Loss: 0.00001106
Iteration 236/1000 | Loss: 0.00001106
Iteration 237/1000 | Loss: 0.00001106
Iteration 238/1000 | Loss: 0.00001106
Iteration 239/1000 | Loss: 0.00001106
Iteration 240/1000 | Loss: 0.00001106
Iteration 241/1000 | Loss: 0.00001106
Iteration 242/1000 | Loss: 0.00001106
Iteration 243/1000 | Loss: 0.00001106
Iteration 244/1000 | Loss: 0.00001106
Iteration 245/1000 | Loss: 0.00001106
Iteration 246/1000 | Loss: 0.00001106
Iteration 247/1000 | Loss: 0.00001106
Iteration 248/1000 | Loss: 0.00001106
Iteration 249/1000 | Loss: 0.00001106
Iteration 250/1000 | Loss: 0.00001106
Iteration 251/1000 | Loss: 0.00001106
Iteration 252/1000 | Loss: 0.00001106
Iteration 253/1000 | Loss: 0.00001106
Iteration 254/1000 | Loss: 0.00001106
Iteration 255/1000 | Loss: 0.00001106
Iteration 256/1000 | Loss: 0.00001106
Iteration 257/1000 | Loss: 0.00001106
Iteration 258/1000 | Loss: 0.00001106
Iteration 259/1000 | Loss: 0.00001106
Iteration 260/1000 | Loss: 0.00001106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [1.106006220652489e-05, 1.106006220652489e-05, 1.106006220652489e-05, 1.106006220652489e-05, 1.106006220652489e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.106006220652489e-05

Optimization complete. Final v2v error: 2.868352174758911 mm

Highest mean error: 2.9708125591278076 mm for frame 83

Lowest mean error: 2.7707924842834473 mm for frame 127

Saving results

Total time: 43.9682457447052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080229
Iteration 2/25 | Loss: 0.00185245
Iteration 3/25 | Loss: 0.00139037
Iteration 4/25 | Loss: 0.00135594
Iteration 5/25 | Loss: 0.00134561
Iteration 6/25 | Loss: 0.00135141
Iteration 7/25 | Loss: 0.00134617
Iteration 8/25 | Loss: 0.00133252
Iteration 9/25 | Loss: 0.00132925
Iteration 10/25 | Loss: 0.00132840
Iteration 11/25 | Loss: 0.00132817
Iteration 12/25 | Loss: 0.00132810
Iteration 13/25 | Loss: 0.00132809
Iteration 14/25 | Loss: 0.00132809
Iteration 15/25 | Loss: 0.00132809
Iteration 16/25 | Loss: 0.00132809
Iteration 17/25 | Loss: 0.00132809
Iteration 18/25 | Loss: 0.00132809
Iteration 19/25 | Loss: 0.00132808
Iteration 20/25 | Loss: 0.00132808
Iteration 21/25 | Loss: 0.00132808
Iteration 22/25 | Loss: 0.00132808
Iteration 23/25 | Loss: 0.00132808
Iteration 24/25 | Loss: 0.00132808
Iteration 25/25 | Loss: 0.00132808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96831417
Iteration 2/25 | Loss: 0.00158040
Iteration 3/25 | Loss: 0.00158040
Iteration 4/25 | Loss: 0.00158040
Iteration 5/25 | Loss: 0.00158039
Iteration 6/25 | Loss: 0.00158039
Iteration 7/25 | Loss: 0.00158039
Iteration 8/25 | Loss: 0.00158039
Iteration 9/25 | Loss: 0.00158039
Iteration 10/25 | Loss: 0.00158039
Iteration 11/25 | Loss: 0.00158039
Iteration 12/25 | Loss: 0.00158039
Iteration 13/25 | Loss: 0.00158039
Iteration 14/25 | Loss: 0.00158039
Iteration 15/25 | Loss: 0.00158039
Iteration 16/25 | Loss: 0.00158039
Iteration 17/25 | Loss: 0.00158039
Iteration 18/25 | Loss: 0.00158039
Iteration 19/25 | Loss: 0.00158039
Iteration 20/25 | Loss: 0.00158039
Iteration 21/25 | Loss: 0.00158039
Iteration 22/25 | Loss: 0.00158039
Iteration 23/25 | Loss: 0.00158039
Iteration 24/25 | Loss: 0.00158039
Iteration 25/25 | Loss: 0.00158039
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0015803921269252896, 0.0015803921269252896, 0.0015803921269252896, 0.0015803921269252896, 0.0015803921269252896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015803921269252896

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158039
Iteration 2/1000 | Loss: 0.00006507
Iteration 3/1000 | Loss: 0.00004015
Iteration 4/1000 | Loss: 0.00003099
Iteration 5/1000 | Loss: 0.00002791
Iteration 6/1000 | Loss: 0.00002650
Iteration 7/1000 | Loss: 0.00002574
Iteration 8/1000 | Loss: 0.00002513
Iteration 9/1000 | Loss: 0.00002463
Iteration 10/1000 | Loss: 0.00002431
Iteration 11/1000 | Loss: 0.00002400
Iteration 12/1000 | Loss: 0.00002376
Iteration 13/1000 | Loss: 0.00002357
Iteration 14/1000 | Loss: 0.00002334
Iteration 15/1000 | Loss: 0.00002317
Iteration 16/1000 | Loss: 0.00002308
Iteration 17/1000 | Loss: 0.00002294
Iteration 18/1000 | Loss: 0.00002286
Iteration 19/1000 | Loss: 0.00002274
Iteration 20/1000 | Loss: 0.00002268
Iteration 21/1000 | Loss: 0.00002265
Iteration 22/1000 | Loss: 0.00002264
Iteration 23/1000 | Loss: 0.00002263
Iteration 24/1000 | Loss: 0.00002263
Iteration 25/1000 | Loss: 0.00002262
Iteration 26/1000 | Loss: 0.00002254
Iteration 27/1000 | Loss: 0.00002254
Iteration 28/1000 | Loss: 0.00002251
Iteration 29/1000 | Loss: 0.00002248
Iteration 30/1000 | Loss: 0.00002248
Iteration 31/1000 | Loss: 0.00002248
Iteration 32/1000 | Loss: 0.00002248
Iteration 33/1000 | Loss: 0.00002248
Iteration 34/1000 | Loss: 0.00002248
Iteration 35/1000 | Loss: 0.00002247
Iteration 36/1000 | Loss: 0.00002247
Iteration 37/1000 | Loss: 0.00002247
Iteration 38/1000 | Loss: 0.00002247
Iteration 39/1000 | Loss: 0.00002245
Iteration 40/1000 | Loss: 0.00002245
Iteration 41/1000 | Loss: 0.00002243
Iteration 42/1000 | Loss: 0.00002243
Iteration 43/1000 | Loss: 0.00002243
Iteration 44/1000 | Loss: 0.00002243
Iteration 45/1000 | Loss: 0.00002243
Iteration 46/1000 | Loss: 0.00002243
Iteration 47/1000 | Loss: 0.00002243
Iteration 48/1000 | Loss: 0.00002242
Iteration 49/1000 | Loss: 0.00002241
Iteration 50/1000 | Loss: 0.00002241
Iteration 51/1000 | Loss: 0.00002241
Iteration 52/1000 | Loss: 0.00002241
Iteration 53/1000 | Loss: 0.00002240
Iteration 54/1000 | Loss: 0.00002240
Iteration 55/1000 | Loss: 0.00002240
Iteration 56/1000 | Loss: 0.00002240
Iteration 57/1000 | Loss: 0.00002240
Iteration 58/1000 | Loss: 0.00002239
Iteration 59/1000 | Loss: 0.00002239
Iteration 60/1000 | Loss: 0.00002239
Iteration 61/1000 | Loss: 0.00002239
Iteration 62/1000 | Loss: 0.00002239
Iteration 63/1000 | Loss: 0.00002239
Iteration 64/1000 | Loss: 0.00002239
Iteration 65/1000 | Loss: 0.00002238
Iteration 66/1000 | Loss: 0.00002238
Iteration 67/1000 | Loss: 0.00002238
Iteration 68/1000 | Loss: 0.00002237
Iteration 69/1000 | Loss: 0.00002237
Iteration 70/1000 | Loss: 0.00002237
Iteration 71/1000 | Loss: 0.00002237
Iteration 72/1000 | Loss: 0.00002236
Iteration 73/1000 | Loss: 0.00002236
Iteration 74/1000 | Loss: 0.00002236
Iteration 75/1000 | Loss: 0.00002236
Iteration 76/1000 | Loss: 0.00002236
Iteration 77/1000 | Loss: 0.00002236
Iteration 78/1000 | Loss: 0.00002236
Iteration 79/1000 | Loss: 0.00002236
Iteration 80/1000 | Loss: 0.00002236
Iteration 81/1000 | Loss: 0.00002236
Iteration 82/1000 | Loss: 0.00002236
Iteration 83/1000 | Loss: 0.00002236
Iteration 84/1000 | Loss: 0.00002236
Iteration 85/1000 | Loss: 0.00002235
Iteration 86/1000 | Loss: 0.00002235
Iteration 87/1000 | Loss: 0.00002235
Iteration 88/1000 | Loss: 0.00002234
Iteration 89/1000 | Loss: 0.00002234
Iteration 90/1000 | Loss: 0.00002234
Iteration 91/1000 | Loss: 0.00002234
Iteration 92/1000 | Loss: 0.00002234
Iteration 93/1000 | Loss: 0.00002234
Iteration 94/1000 | Loss: 0.00002234
Iteration 95/1000 | Loss: 0.00002234
Iteration 96/1000 | Loss: 0.00002234
Iteration 97/1000 | Loss: 0.00002233
Iteration 98/1000 | Loss: 0.00002233
Iteration 99/1000 | Loss: 0.00002233
Iteration 100/1000 | Loss: 0.00002233
Iteration 101/1000 | Loss: 0.00002232
Iteration 102/1000 | Loss: 0.00002232
Iteration 103/1000 | Loss: 0.00002232
Iteration 104/1000 | Loss: 0.00002232
Iteration 105/1000 | Loss: 0.00002232
Iteration 106/1000 | Loss: 0.00002232
Iteration 107/1000 | Loss: 0.00002232
Iteration 108/1000 | Loss: 0.00002231
Iteration 109/1000 | Loss: 0.00002231
Iteration 110/1000 | Loss: 0.00002231
Iteration 111/1000 | Loss: 0.00002231
Iteration 112/1000 | Loss: 0.00002231
Iteration 113/1000 | Loss: 0.00002231
Iteration 114/1000 | Loss: 0.00002231
Iteration 115/1000 | Loss: 0.00002230
Iteration 116/1000 | Loss: 0.00002230
Iteration 117/1000 | Loss: 0.00002230
Iteration 118/1000 | Loss: 0.00002230
Iteration 119/1000 | Loss: 0.00002230
Iteration 120/1000 | Loss: 0.00002230
Iteration 121/1000 | Loss: 0.00002230
Iteration 122/1000 | Loss: 0.00002230
Iteration 123/1000 | Loss: 0.00002229
Iteration 124/1000 | Loss: 0.00002229
Iteration 125/1000 | Loss: 0.00002229
Iteration 126/1000 | Loss: 0.00002229
Iteration 127/1000 | Loss: 0.00002229
Iteration 128/1000 | Loss: 0.00002229
Iteration 129/1000 | Loss: 0.00002229
Iteration 130/1000 | Loss: 0.00002229
Iteration 131/1000 | Loss: 0.00002229
Iteration 132/1000 | Loss: 0.00002229
Iteration 133/1000 | Loss: 0.00002229
Iteration 134/1000 | Loss: 0.00002229
Iteration 135/1000 | Loss: 0.00002229
Iteration 136/1000 | Loss: 0.00002229
Iteration 137/1000 | Loss: 0.00002229
Iteration 138/1000 | Loss: 0.00002229
Iteration 139/1000 | Loss: 0.00002229
Iteration 140/1000 | Loss: 0.00002229
Iteration 141/1000 | Loss: 0.00002229
Iteration 142/1000 | Loss: 0.00002229
Iteration 143/1000 | Loss: 0.00002229
Iteration 144/1000 | Loss: 0.00002229
Iteration 145/1000 | Loss: 0.00002229
Iteration 146/1000 | Loss: 0.00002229
Iteration 147/1000 | Loss: 0.00002229
Iteration 148/1000 | Loss: 0.00002229
Iteration 149/1000 | Loss: 0.00002229
Iteration 150/1000 | Loss: 0.00002229
Iteration 151/1000 | Loss: 0.00002229
Iteration 152/1000 | Loss: 0.00002229
Iteration 153/1000 | Loss: 0.00002229
Iteration 154/1000 | Loss: 0.00002229
Iteration 155/1000 | Loss: 0.00002229
Iteration 156/1000 | Loss: 0.00002229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.2288968466455117e-05, 2.2288968466455117e-05, 2.2288968466455117e-05, 2.2288968466455117e-05, 2.2288968466455117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2288968466455117e-05

Optimization complete. Final v2v error: 3.9221014976501465 mm

Highest mean error: 4.630671977996826 mm for frame 107

Lowest mean error: 3.277222156524658 mm for frame 23

Saving results

Total time: 60.11426091194153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821400
Iteration 2/25 | Loss: 0.00130298
Iteration 3/25 | Loss: 0.00120730
Iteration 4/25 | Loss: 0.00119902
Iteration 5/25 | Loss: 0.00119654
Iteration 6/25 | Loss: 0.00119646
Iteration 7/25 | Loss: 0.00119646
Iteration 8/25 | Loss: 0.00119646
Iteration 9/25 | Loss: 0.00119646
Iteration 10/25 | Loss: 0.00119646
Iteration 11/25 | Loss: 0.00119646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011964627774432302, 0.0011964627774432302, 0.0011964627774432302, 0.0011964627774432302, 0.0011964627774432302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011964627774432302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67200553
Iteration 2/25 | Loss: 0.00138353
Iteration 3/25 | Loss: 0.00138353
Iteration 4/25 | Loss: 0.00138353
Iteration 5/25 | Loss: 0.00138353
Iteration 6/25 | Loss: 0.00138353
Iteration 7/25 | Loss: 0.00138353
Iteration 8/25 | Loss: 0.00138353
Iteration 9/25 | Loss: 0.00138353
Iteration 10/25 | Loss: 0.00138353
Iteration 11/25 | Loss: 0.00138353
Iteration 12/25 | Loss: 0.00138353
Iteration 13/25 | Loss: 0.00138353
Iteration 14/25 | Loss: 0.00138353
Iteration 15/25 | Loss: 0.00138353
Iteration 16/25 | Loss: 0.00138353
Iteration 17/25 | Loss: 0.00138353
Iteration 18/25 | Loss: 0.00138353
Iteration 19/25 | Loss: 0.00138353
Iteration 20/25 | Loss: 0.00138353
Iteration 21/25 | Loss: 0.00138353
Iteration 22/25 | Loss: 0.00138353
Iteration 23/25 | Loss: 0.00138353
Iteration 24/25 | Loss: 0.00138353
Iteration 25/25 | Loss: 0.00138353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138353
Iteration 2/1000 | Loss: 0.00002091
Iteration 3/1000 | Loss: 0.00001616
Iteration 4/1000 | Loss: 0.00001452
Iteration 5/1000 | Loss: 0.00001366
Iteration 6/1000 | Loss: 0.00001303
Iteration 7/1000 | Loss: 0.00001260
Iteration 8/1000 | Loss: 0.00001227
Iteration 9/1000 | Loss: 0.00001188
Iteration 10/1000 | Loss: 0.00001166
Iteration 11/1000 | Loss: 0.00001147
Iteration 12/1000 | Loss: 0.00001146
Iteration 13/1000 | Loss: 0.00001145
Iteration 14/1000 | Loss: 0.00001144
Iteration 15/1000 | Loss: 0.00001134
Iteration 16/1000 | Loss: 0.00001124
Iteration 17/1000 | Loss: 0.00001122
Iteration 18/1000 | Loss: 0.00001118
Iteration 19/1000 | Loss: 0.00001116
Iteration 20/1000 | Loss: 0.00001115
Iteration 21/1000 | Loss: 0.00001114
Iteration 22/1000 | Loss: 0.00001112
Iteration 23/1000 | Loss: 0.00001111
Iteration 24/1000 | Loss: 0.00001111
Iteration 25/1000 | Loss: 0.00001110
Iteration 26/1000 | Loss: 0.00001107
Iteration 27/1000 | Loss: 0.00001106
Iteration 28/1000 | Loss: 0.00001105
Iteration 29/1000 | Loss: 0.00001105
Iteration 30/1000 | Loss: 0.00001104
Iteration 31/1000 | Loss: 0.00001104
Iteration 32/1000 | Loss: 0.00001103
Iteration 33/1000 | Loss: 0.00001101
Iteration 34/1000 | Loss: 0.00001100
Iteration 35/1000 | Loss: 0.00001099
Iteration 36/1000 | Loss: 0.00001099
Iteration 37/1000 | Loss: 0.00001099
Iteration 38/1000 | Loss: 0.00001098
Iteration 39/1000 | Loss: 0.00001098
Iteration 40/1000 | Loss: 0.00001096
Iteration 41/1000 | Loss: 0.00001096
Iteration 42/1000 | Loss: 0.00001095
Iteration 43/1000 | Loss: 0.00001095
Iteration 44/1000 | Loss: 0.00001095
Iteration 45/1000 | Loss: 0.00001095
Iteration 46/1000 | Loss: 0.00001094
Iteration 47/1000 | Loss: 0.00001093
Iteration 48/1000 | Loss: 0.00001093
Iteration 49/1000 | Loss: 0.00001093
Iteration 50/1000 | Loss: 0.00001093
Iteration 51/1000 | Loss: 0.00001092
Iteration 52/1000 | Loss: 0.00001091
Iteration 53/1000 | Loss: 0.00001091
Iteration 54/1000 | Loss: 0.00001090
Iteration 55/1000 | Loss: 0.00001090
Iteration 56/1000 | Loss: 0.00001089
Iteration 57/1000 | Loss: 0.00001089
Iteration 58/1000 | Loss: 0.00001088
Iteration 59/1000 | Loss: 0.00001088
Iteration 60/1000 | Loss: 0.00001087
Iteration 61/1000 | Loss: 0.00001087
Iteration 62/1000 | Loss: 0.00001087
Iteration 63/1000 | Loss: 0.00001086
Iteration 64/1000 | Loss: 0.00001086
Iteration 65/1000 | Loss: 0.00001086
Iteration 66/1000 | Loss: 0.00001086
Iteration 67/1000 | Loss: 0.00001086
Iteration 68/1000 | Loss: 0.00001085
Iteration 69/1000 | Loss: 0.00001085
Iteration 70/1000 | Loss: 0.00001085
Iteration 71/1000 | Loss: 0.00001085
Iteration 72/1000 | Loss: 0.00001085
Iteration 73/1000 | Loss: 0.00001085
Iteration 74/1000 | Loss: 0.00001085
Iteration 75/1000 | Loss: 0.00001085
Iteration 76/1000 | Loss: 0.00001085
Iteration 77/1000 | Loss: 0.00001085
Iteration 78/1000 | Loss: 0.00001084
Iteration 79/1000 | Loss: 0.00001084
Iteration 80/1000 | Loss: 0.00001084
Iteration 81/1000 | Loss: 0.00001084
Iteration 82/1000 | Loss: 0.00001084
Iteration 83/1000 | Loss: 0.00001083
Iteration 84/1000 | Loss: 0.00001083
Iteration 85/1000 | Loss: 0.00001083
Iteration 86/1000 | Loss: 0.00001083
Iteration 87/1000 | Loss: 0.00001082
Iteration 88/1000 | Loss: 0.00001082
Iteration 89/1000 | Loss: 0.00001082
Iteration 90/1000 | Loss: 0.00001082
Iteration 91/1000 | Loss: 0.00001082
Iteration 92/1000 | Loss: 0.00001081
Iteration 93/1000 | Loss: 0.00001081
Iteration 94/1000 | Loss: 0.00001081
Iteration 95/1000 | Loss: 0.00001080
Iteration 96/1000 | Loss: 0.00001080
Iteration 97/1000 | Loss: 0.00001080
Iteration 98/1000 | Loss: 0.00001080
Iteration 99/1000 | Loss: 0.00001080
Iteration 100/1000 | Loss: 0.00001080
Iteration 101/1000 | Loss: 0.00001079
Iteration 102/1000 | Loss: 0.00001079
Iteration 103/1000 | Loss: 0.00001079
Iteration 104/1000 | Loss: 0.00001079
Iteration 105/1000 | Loss: 0.00001078
Iteration 106/1000 | Loss: 0.00001078
Iteration 107/1000 | Loss: 0.00001078
Iteration 108/1000 | Loss: 0.00001077
Iteration 109/1000 | Loss: 0.00001077
Iteration 110/1000 | Loss: 0.00001077
Iteration 111/1000 | Loss: 0.00001077
Iteration 112/1000 | Loss: 0.00001077
Iteration 113/1000 | Loss: 0.00001077
Iteration 114/1000 | Loss: 0.00001077
Iteration 115/1000 | Loss: 0.00001077
Iteration 116/1000 | Loss: 0.00001077
Iteration 117/1000 | Loss: 0.00001077
Iteration 118/1000 | Loss: 0.00001077
Iteration 119/1000 | Loss: 0.00001077
Iteration 120/1000 | Loss: 0.00001076
Iteration 121/1000 | Loss: 0.00001076
Iteration 122/1000 | Loss: 0.00001076
Iteration 123/1000 | Loss: 0.00001076
Iteration 124/1000 | Loss: 0.00001076
Iteration 125/1000 | Loss: 0.00001076
Iteration 126/1000 | Loss: 0.00001076
Iteration 127/1000 | Loss: 0.00001076
Iteration 128/1000 | Loss: 0.00001076
Iteration 129/1000 | Loss: 0.00001076
Iteration 130/1000 | Loss: 0.00001076
Iteration 131/1000 | Loss: 0.00001076
Iteration 132/1000 | Loss: 0.00001075
Iteration 133/1000 | Loss: 0.00001075
Iteration 134/1000 | Loss: 0.00001075
Iteration 135/1000 | Loss: 0.00001075
Iteration 136/1000 | Loss: 0.00001075
Iteration 137/1000 | Loss: 0.00001075
Iteration 138/1000 | Loss: 0.00001075
Iteration 139/1000 | Loss: 0.00001075
Iteration 140/1000 | Loss: 0.00001075
Iteration 141/1000 | Loss: 0.00001075
Iteration 142/1000 | Loss: 0.00001075
Iteration 143/1000 | Loss: 0.00001074
Iteration 144/1000 | Loss: 0.00001074
Iteration 145/1000 | Loss: 0.00001074
Iteration 146/1000 | Loss: 0.00001074
Iteration 147/1000 | Loss: 0.00001074
Iteration 148/1000 | Loss: 0.00001074
Iteration 149/1000 | Loss: 0.00001074
Iteration 150/1000 | Loss: 0.00001073
Iteration 151/1000 | Loss: 0.00001073
Iteration 152/1000 | Loss: 0.00001073
Iteration 153/1000 | Loss: 0.00001073
Iteration 154/1000 | Loss: 0.00001073
Iteration 155/1000 | Loss: 0.00001073
Iteration 156/1000 | Loss: 0.00001073
Iteration 157/1000 | Loss: 0.00001073
Iteration 158/1000 | Loss: 0.00001072
Iteration 159/1000 | Loss: 0.00001072
Iteration 160/1000 | Loss: 0.00001072
Iteration 161/1000 | Loss: 0.00001072
Iteration 162/1000 | Loss: 0.00001072
Iteration 163/1000 | Loss: 0.00001072
Iteration 164/1000 | Loss: 0.00001071
Iteration 165/1000 | Loss: 0.00001071
Iteration 166/1000 | Loss: 0.00001071
Iteration 167/1000 | Loss: 0.00001071
Iteration 168/1000 | Loss: 0.00001071
Iteration 169/1000 | Loss: 0.00001070
Iteration 170/1000 | Loss: 0.00001070
Iteration 171/1000 | Loss: 0.00001070
Iteration 172/1000 | Loss: 0.00001070
Iteration 173/1000 | Loss: 0.00001070
Iteration 174/1000 | Loss: 0.00001070
Iteration 175/1000 | Loss: 0.00001070
Iteration 176/1000 | Loss: 0.00001070
Iteration 177/1000 | Loss: 0.00001070
Iteration 178/1000 | Loss: 0.00001070
Iteration 179/1000 | Loss: 0.00001070
Iteration 180/1000 | Loss: 0.00001070
Iteration 181/1000 | Loss: 0.00001070
Iteration 182/1000 | Loss: 0.00001069
Iteration 183/1000 | Loss: 0.00001069
Iteration 184/1000 | Loss: 0.00001069
Iteration 185/1000 | Loss: 0.00001069
Iteration 186/1000 | Loss: 0.00001069
Iteration 187/1000 | Loss: 0.00001069
Iteration 188/1000 | Loss: 0.00001069
Iteration 189/1000 | Loss: 0.00001069
Iteration 190/1000 | Loss: 0.00001069
Iteration 191/1000 | Loss: 0.00001069
Iteration 192/1000 | Loss: 0.00001069
Iteration 193/1000 | Loss: 0.00001069
Iteration 194/1000 | Loss: 0.00001069
Iteration 195/1000 | Loss: 0.00001069
Iteration 196/1000 | Loss: 0.00001069
Iteration 197/1000 | Loss: 0.00001069
Iteration 198/1000 | Loss: 0.00001069
Iteration 199/1000 | Loss: 0.00001069
Iteration 200/1000 | Loss: 0.00001069
Iteration 201/1000 | Loss: 0.00001069
Iteration 202/1000 | Loss: 0.00001069
Iteration 203/1000 | Loss: 0.00001069
Iteration 204/1000 | Loss: 0.00001069
Iteration 205/1000 | Loss: 0.00001068
Iteration 206/1000 | Loss: 0.00001068
Iteration 207/1000 | Loss: 0.00001068
Iteration 208/1000 | Loss: 0.00001068
Iteration 209/1000 | Loss: 0.00001068
Iteration 210/1000 | Loss: 0.00001068
Iteration 211/1000 | Loss: 0.00001068
Iteration 212/1000 | Loss: 0.00001068
Iteration 213/1000 | Loss: 0.00001068
Iteration 214/1000 | Loss: 0.00001068
Iteration 215/1000 | Loss: 0.00001068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.0682594620448072e-05, 1.0682594620448072e-05, 1.0682594620448072e-05, 1.0682594620448072e-05, 1.0682594620448072e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0682594620448072e-05

Optimization complete. Final v2v error: 2.8145198822021484 mm

Highest mean error: 3.502916097640991 mm for frame 115

Lowest mean error: 2.593945026397705 mm for frame 71

Saving results

Total time: 41.66193175315857
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893267
Iteration 2/25 | Loss: 0.00125316
Iteration 3/25 | Loss: 0.00119650
Iteration 4/25 | Loss: 0.00118833
Iteration 5/25 | Loss: 0.00118763
Iteration 6/25 | Loss: 0.00118763
Iteration 7/25 | Loss: 0.00118763
Iteration 8/25 | Loss: 0.00118763
Iteration 9/25 | Loss: 0.00118763
Iteration 10/25 | Loss: 0.00118763
Iteration 11/25 | Loss: 0.00118763
Iteration 12/25 | Loss: 0.00118763
Iteration 13/25 | Loss: 0.00118763
Iteration 14/25 | Loss: 0.00118763
Iteration 15/25 | Loss: 0.00118763
Iteration 16/25 | Loss: 0.00118763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011876315111294389, 0.0011876315111294389, 0.0011876315111294389, 0.0011876315111294389, 0.0011876315111294389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011876315111294389

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37642765
Iteration 2/25 | Loss: 0.00125200
Iteration 3/25 | Loss: 0.00125200
Iteration 4/25 | Loss: 0.00125200
Iteration 5/25 | Loss: 0.00125200
Iteration 6/25 | Loss: 0.00125200
Iteration 7/25 | Loss: 0.00125200
Iteration 8/25 | Loss: 0.00125200
Iteration 9/25 | Loss: 0.00125200
Iteration 10/25 | Loss: 0.00125200
Iteration 11/25 | Loss: 0.00125200
Iteration 12/25 | Loss: 0.00125200
Iteration 13/25 | Loss: 0.00125200
Iteration 14/25 | Loss: 0.00125200
Iteration 15/25 | Loss: 0.00125200
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012519995216280222, 0.0012519995216280222, 0.0012519995216280222, 0.0012519995216280222, 0.0012519995216280222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012519995216280222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125200
Iteration 2/1000 | Loss: 0.00001880
Iteration 3/1000 | Loss: 0.00001582
Iteration 4/1000 | Loss: 0.00001484
Iteration 5/1000 | Loss: 0.00001412
Iteration 6/1000 | Loss: 0.00001359
Iteration 7/1000 | Loss: 0.00001325
Iteration 8/1000 | Loss: 0.00001282
Iteration 9/1000 | Loss: 0.00001253
Iteration 10/1000 | Loss: 0.00001249
Iteration 11/1000 | Loss: 0.00001241
Iteration 12/1000 | Loss: 0.00001228
Iteration 13/1000 | Loss: 0.00001221
Iteration 14/1000 | Loss: 0.00001210
Iteration 15/1000 | Loss: 0.00001203
Iteration 16/1000 | Loss: 0.00001195
Iteration 17/1000 | Loss: 0.00001194
Iteration 18/1000 | Loss: 0.00001194
Iteration 19/1000 | Loss: 0.00001194
Iteration 20/1000 | Loss: 0.00001193
Iteration 21/1000 | Loss: 0.00001193
Iteration 22/1000 | Loss: 0.00001185
Iteration 23/1000 | Loss: 0.00001184
Iteration 24/1000 | Loss: 0.00001180
Iteration 25/1000 | Loss: 0.00001180
Iteration 26/1000 | Loss: 0.00001179
Iteration 27/1000 | Loss: 0.00001179
Iteration 28/1000 | Loss: 0.00001179
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001177
Iteration 33/1000 | Loss: 0.00001167
Iteration 34/1000 | Loss: 0.00001166
Iteration 35/1000 | Loss: 0.00001166
Iteration 36/1000 | Loss: 0.00001164
Iteration 37/1000 | Loss: 0.00001163
Iteration 38/1000 | Loss: 0.00001163
Iteration 39/1000 | Loss: 0.00001162
Iteration 40/1000 | Loss: 0.00001162
Iteration 41/1000 | Loss: 0.00001162
Iteration 42/1000 | Loss: 0.00001161
Iteration 43/1000 | Loss: 0.00001161
Iteration 44/1000 | Loss: 0.00001157
Iteration 45/1000 | Loss: 0.00001156
Iteration 46/1000 | Loss: 0.00001152
Iteration 47/1000 | Loss: 0.00001152
Iteration 48/1000 | Loss: 0.00001152
Iteration 49/1000 | Loss: 0.00001152
Iteration 50/1000 | Loss: 0.00001152
Iteration 51/1000 | Loss: 0.00001152
Iteration 52/1000 | Loss: 0.00001152
Iteration 53/1000 | Loss: 0.00001152
Iteration 54/1000 | Loss: 0.00001152
Iteration 55/1000 | Loss: 0.00001152
Iteration 56/1000 | Loss: 0.00001151
Iteration 57/1000 | Loss: 0.00001151
Iteration 58/1000 | Loss: 0.00001148
Iteration 59/1000 | Loss: 0.00001148
Iteration 60/1000 | Loss: 0.00001147
Iteration 61/1000 | Loss: 0.00001147
Iteration 62/1000 | Loss: 0.00001147
Iteration 63/1000 | Loss: 0.00001146
Iteration 64/1000 | Loss: 0.00001146
Iteration 65/1000 | Loss: 0.00001146
Iteration 66/1000 | Loss: 0.00001146
Iteration 67/1000 | Loss: 0.00001146
Iteration 68/1000 | Loss: 0.00001145
Iteration 69/1000 | Loss: 0.00001144
Iteration 70/1000 | Loss: 0.00001143
Iteration 71/1000 | Loss: 0.00001143
Iteration 72/1000 | Loss: 0.00001143
Iteration 73/1000 | Loss: 0.00001143
Iteration 74/1000 | Loss: 0.00001142
Iteration 75/1000 | Loss: 0.00001142
Iteration 76/1000 | Loss: 0.00001142
Iteration 77/1000 | Loss: 0.00001142
Iteration 78/1000 | Loss: 0.00001142
Iteration 79/1000 | Loss: 0.00001142
Iteration 80/1000 | Loss: 0.00001142
Iteration 81/1000 | Loss: 0.00001142
Iteration 82/1000 | Loss: 0.00001142
Iteration 83/1000 | Loss: 0.00001142
Iteration 84/1000 | Loss: 0.00001141
Iteration 85/1000 | Loss: 0.00001141
Iteration 86/1000 | Loss: 0.00001140
Iteration 87/1000 | Loss: 0.00001140
Iteration 88/1000 | Loss: 0.00001140
Iteration 89/1000 | Loss: 0.00001140
Iteration 90/1000 | Loss: 0.00001140
Iteration 91/1000 | Loss: 0.00001140
Iteration 92/1000 | Loss: 0.00001140
Iteration 93/1000 | Loss: 0.00001140
Iteration 94/1000 | Loss: 0.00001139
Iteration 95/1000 | Loss: 0.00001139
Iteration 96/1000 | Loss: 0.00001139
Iteration 97/1000 | Loss: 0.00001139
Iteration 98/1000 | Loss: 0.00001139
Iteration 99/1000 | Loss: 0.00001139
Iteration 100/1000 | Loss: 0.00001139
Iteration 101/1000 | Loss: 0.00001139
Iteration 102/1000 | Loss: 0.00001139
Iteration 103/1000 | Loss: 0.00001139
Iteration 104/1000 | Loss: 0.00001139
Iteration 105/1000 | Loss: 0.00001139
Iteration 106/1000 | Loss: 0.00001139
Iteration 107/1000 | Loss: 0.00001139
Iteration 108/1000 | Loss: 0.00001138
Iteration 109/1000 | Loss: 0.00001138
Iteration 110/1000 | Loss: 0.00001138
Iteration 111/1000 | Loss: 0.00001138
Iteration 112/1000 | Loss: 0.00001138
Iteration 113/1000 | Loss: 0.00001137
Iteration 114/1000 | Loss: 0.00001137
Iteration 115/1000 | Loss: 0.00001136
Iteration 116/1000 | Loss: 0.00001136
Iteration 117/1000 | Loss: 0.00001136
Iteration 118/1000 | Loss: 0.00001136
Iteration 119/1000 | Loss: 0.00001136
Iteration 120/1000 | Loss: 0.00001136
Iteration 121/1000 | Loss: 0.00001136
Iteration 122/1000 | Loss: 0.00001135
Iteration 123/1000 | Loss: 0.00001135
Iteration 124/1000 | Loss: 0.00001135
Iteration 125/1000 | Loss: 0.00001135
Iteration 126/1000 | Loss: 0.00001135
Iteration 127/1000 | Loss: 0.00001135
Iteration 128/1000 | Loss: 0.00001135
Iteration 129/1000 | Loss: 0.00001135
Iteration 130/1000 | Loss: 0.00001135
Iteration 131/1000 | Loss: 0.00001135
Iteration 132/1000 | Loss: 0.00001135
Iteration 133/1000 | Loss: 0.00001134
Iteration 134/1000 | Loss: 0.00001134
Iteration 135/1000 | Loss: 0.00001134
Iteration 136/1000 | Loss: 0.00001134
Iteration 137/1000 | Loss: 0.00001134
Iteration 138/1000 | Loss: 0.00001134
Iteration 139/1000 | Loss: 0.00001134
Iteration 140/1000 | Loss: 0.00001134
Iteration 141/1000 | Loss: 0.00001134
Iteration 142/1000 | Loss: 0.00001134
Iteration 143/1000 | Loss: 0.00001134
Iteration 144/1000 | Loss: 0.00001133
Iteration 145/1000 | Loss: 0.00001133
Iteration 146/1000 | Loss: 0.00001133
Iteration 147/1000 | Loss: 0.00001133
Iteration 148/1000 | Loss: 0.00001133
Iteration 149/1000 | Loss: 0.00001133
Iteration 150/1000 | Loss: 0.00001133
Iteration 151/1000 | Loss: 0.00001133
Iteration 152/1000 | Loss: 0.00001133
Iteration 153/1000 | Loss: 0.00001133
Iteration 154/1000 | Loss: 0.00001133
Iteration 155/1000 | Loss: 0.00001133
Iteration 156/1000 | Loss: 0.00001133
Iteration 157/1000 | Loss: 0.00001133
Iteration 158/1000 | Loss: 0.00001133
Iteration 159/1000 | Loss: 0.00001133
Iteration 160/1000 | Loss: 0.00001133
Iteration 161/1000 | Loss: 0.00001133
Iteration 162/1000 | Loss: 0.00001133
Iteration 163/1000 | Loss: 0.00001133
Iteration 164/1000 | Loss: 0.00001133
Iteration 165/1000 | Loss: 0.00001133
Iteration 166/1000 | Loss: 0.00001133
Iteration 167/1000 | Loss: 0.00001133
Iteration 168/1000 | Loss: 0.00001133
Iteration 169/1000 | Loss: 0.00001133
Iteration 170/1000 | Loss: 0.00001133
Iteration 171/1000 | Loss: 0.00001133
Iteration 172/1000 | Loss: 0.00001133
Iteration 173/1000 | Loss: 0.00001133
Iteration 174/1000 | Loss: 0.00001133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.1327382708259393e-05, 1.1327382708259393e-05, 1.1327382708259393e-05, 1.1327382708259393e-05, 1.1327382708259393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1327382708259393e-05

Optimization complete. Final v2v error: 2.8960530757904053 mm

Highest mean error: 3.3447558879852295 mm for frame 146

Lowest mean error: 2.81486177444458 mm for frame 205

Saving results

Total time: 43.31479215621948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00608040
Iteration 2/25 | Loss: 0.00121795
Iteration 3/25 | Loss: 0.00116079
Iteration 4/25 | Loss: 0.00115355
Iteration 5/25 | Loss: 0.00115151
Iteration 6/25 | Loss: 0.00115151
Iteration 7/25 | Loss: 0.00115151
Iteration 8/25 | Loss: 0.00115151
Iteration 9/25 | Loss: 0.00115151
Iteration 10/25 | Loss: 0.00115151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011515122605487704, 0.0011515122605487704, 0.0011515122605487704, 0.0011515122605487704, 0.0011515122605487704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011515122605487704

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.57663965
Iteration 2/25 | Loss: 0.00131396
Iteration 3/25 | Loss: 0.00131395
Iteration 4/25 | Loss: 0.00131395
Iteration 5/25 | Loss: 0.00131395
Iteration 6/25 | Loss: 0.00131395
Iteration 7/25 | Loss: 0.00131395
Iteration 8/25 | Loss: 0.00131395
Iteration 9/25 | Loss: 0.00131395
Iteration 10/25 | Loss: 0.00131395
Iteration 11/25 | Loss: 0.00131395
Iteration 12/25 | Loss: 0.00131395
Iteration 13/25 | Loss: 0.00131395
Iteration 14/25 | Loss: 0.00131395
Iteration 15/25 | Loss: 0.00131395
Iteration 16/25 | Loss: 0.00131395
Iteration 17/25 | Loss: 0.00131395
Iteration 18/25 | Loss: 0.00131395
Iteration 19/25 | Loss: 0.00131395
Iteration 20/25 | Loss: 0.00131395
Iteration 21/25 | Loss: 0.00131395
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013139508664608002, 0.0013139508664608002, 0.0013139508664608002, 0.0013139508664608002, 0.0013139508664608002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013139508664608002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131395
Iteration 2/1000 | Loss: 0.00002331
Iteration 3/1000 | Loss: 0.00001734
Iteration 4/1000 | Loss: 0.00001489
Iteration 5/1000 | Loss: 0.00001402
Iteration 6/1000 | Loss: 0.00001321
Iteration 7/1000 | Loss: 0.00001261
Iteration 8/1000 | Loss: 0.00001221
Iteration 9/1000 | Loss: 0.00001182
Iteration 10/1000 | Loss: 0.00001151
Iteration 11/1000 | Loss: 0.00001133
Iteration 12/1000 | Loss: 0.00001113
Iteration 13/1000 | Loss: 0.00001097
Iteration 14/1000 | Loss: 0.00001093
Iteration 15/1000 | Loss: 0.00001087
Iteration 16/1000 | Loss: 0.00001085
Iteration 17/1000 | Loss: 0.00001082
Iteration 18/1000 | Loss: 0.00001080
Iteration 19/1000 | Loss: 0.00001080
Iteration 20/1000 | Loss: 0.00001079
Iteration 21/1000 | Loss: 0.00001078
Iteration 22/1000 | Loss: 0.00001072
Iteration 23/1000 | Loss: 0.00001066
Iteration 24/1000 | Loss: 0.00001062
Iteration 25/1000 | Loss: 0.00001058
Iteration 26/1000 | Loss: 0.00001056
Iteration 27/1000 | Loss: 0.00001055
Iteration 28/1000 | Loss: 0.00001053
Iteration 29/1000 | Loss: 0.00001052
Iteration 30/1000 | Loss: 0.00001052
Iteration 31/1000 | Loss: 0.00001050
Iteration 32/1000 | Loss: 0.00001050
Iteration 33/1000 | Loss: 0.00001050
Iteration 34/1000 | Loss: 0.00001050
Iteration 35/1000 | Loss: 0.00001050
Iteration 36/1000 | Loss: 0.00001050
Iteration 37/1000 | Loss: 0.00001050
Iteration 38/1000 | Loss: 0.00001050
Iteration 39/1000 | Loss: 0.00001049
Iteration 40/1000 | Loss: 0.00001047
Iteration 41/1000 | Loss: 0.00001046
Iteration 42/1000 | Loss: 0.00001046
Iteration 43/1000 | Loss: 0.00001046
Iteration 44/1000 | Loss: 0.00001046
Iteration 45/1000 | Loss: 0.00001045
Iteration 46/1000 | Loss: 0.00001045
Iteration 47/1000 | Loss: 0.00001044
Iteration 48/1000 | Loss: 0.00001044
Iteration 49/1000 | Loss: 0.00001043
Iteration 50/1000 | Loss: 0.00001043
Iteration 51/1000 | Loss: 0.00001042
Iteration 52/1000 | Loss: 0.00001042
Iteration 53/1000 | Loss: 0.00001042
Iteration 54/1000 | Loss: 0.00001041
Iteration 55/1000 | Loss: 0.00001041
Iteration 56/1000 | Loss: 0.00001041
Iteration 57/1000 | Loss: 0.00001041
Iteration 58/1000 | Loss: 0.00001040
Iteration 59/1000 | Loss: 0.00001040
Iteration 60/1000 | Loss: 0.00001039
Iteration 61/1000 | Loss: 0.00001039
Iteration 62/1000 | Loss: 0.00001038
Iteration 63/1000 | Loss: 0.00001038
Iteration 64/1000 | Loss: 0.00001038
Iteration 65/1000 | Loss: 0.00001038
Iteration 66/1000 | Loss: 0.00001038
Iteration 67/1000 | Loss: 0.00001037
Iteration 68/1000 | Loss: 0.00001037
Iteration 69/1000 | Loss: 0.00001037
Iteration 70/1000 | Loss: 0.00001036
Iteration 71/1000 | Loss: 0.00001036
Iteration 72/1000 | Loss: 0.00001036
Iteration 73/1000 | Loss: 0.00001035
Iteration 74/1000 | Loss: 0.00001034
Iteration 75/1000 | Loss: 0.00001034
Iteration 76/1000 | Loss: 0.00001034
Iteration 77/1000 | Loss: 0.00001033
Iteration 78/1000 | Loss: 0.00001033
Iteration 79/1000 | Loss: 0.00001033
Iteration 80/1000 | Loss: 0.00001032
Iteration 81/1000 | Loss: 0.00001032
Iteration 82/1000 | Loss: 0.00001031
Iteration 83/1000 | Loss: 0.00001031
Iteration 84/1000 | Loss: 0.00001031
Iteration 85/1000 | Loss: 0.00001031
Iteration 86/1000 | Loss: 0.00001030
Iteration 87/1000 | Loss: 0.00001030
Iteration 88/1000 | Loss: 0.00001030
Iteration 89/1000 | Loss: 0.00001030
Iteration 90/1000 | Loss: 0.00001030
Iteration 91/1000 | Loss: 0.00001029
Iteration 92/1000 | Loss: 0.00001029
Iteration 93/1000 | Loss: 0.00001029
Iteration 94/1000 | Loss: 0.00001029
Iteration 95/1000 | Loss: 0.00001029
Iteration 96/1000 | Loss: 0.00001029
Iteration 97/1000 | Loss: 0.00001029
Iteration 98/1000 | Loss: 0.00001028
Iteration 99/1000 | Loss: 0.00001028
Iteration 100/1000 | Loss: 0.00001028
Iteration 101/1000 | Loss: 0.00001028
Iteration 102/1000 | Loss: 0.00001027
Iteration 103/1000 | Loss: 0.00001027
Iteration 104/1000 | Loss: 0.00001027
Iteration 105/1000 | Loss: 0.00001027
Iteration 106/1000 | Loss: 0.00001026
Iteration 107/1000 | Loss: 0.00001026
Iteration 108/1000 | Loss: 0.00001025
Iteration 109/1000 | Loss: 0.00001025
Iteration 110/1000 | Loss: 0.00001025
Iteration 111/1000 | Loss: 0.00001025
Iteration 112/1000 | Loss: 0.00001025
Iteration 113/1000 | Loss: 0.00001024
Iteration 114/1000 | Loss: 0.00001024
Iteration 115/1000 | Loss: 0.00001024
Iteration 116/1000 | Loss: 0.00001024
Iteration 117/1000 | Loss: 0.00001024
Iteration 118/1000 | Loss: 0.00001023
Iteration 119/1000 | Loss: 0.00001023
Iteration 120/1000 | Loss: 0.00001023
Iteration 121/1000 | Loss: 0.00001023
Iteration 122/1000 | Loss: 0.00001023
Iteration 123/1000 | Loss: 0.00001022
Iteration 124/1000 | Loss: 0.00001022
Iteration 125/1000 | Loss: 0.00001022
Iteration 126/1000 | Loss: 0.00001021
Iteration 127/1000 | Loss: 0.00001021
Iteration 128/1000 | Loss: 0.00001021
Iteration 129/1000 | Loss: 0.00001021
Iteration 130/1000 | Loss: 0.00001021
Iteration 131/1000 | Loss: 0.00001020
Iteration 132/1000 | Loss: 0.00001020
Iteration 133/1000 | Loss: 0.00001020
Iteration 134/1000 | Loss: 0.00001020
Iteration 135/1000 | Loss: 0.00001020
Iteration 136/1000 | Loss: 0.00001020
Iteration 137/1000 | Loss: 0.00001020
Iteration 138/1000 | Loss: 0.00001020
Iteration 139/1000 | Loss: 0.00001020
Iteration 140/1000 | Loss: 0.00001020
Iteration 141/1000 | Loss: 0.00001020
Iteration 142/1000 | Loss: 0.00001020
Iteration 143/1000 | Loss: 0.00001020
Iteration 144/1000 | Loss: 0.00001020
Iteration 145/1000 | Loss: 0.00001019
Iteration 146/1000 | Loss: 0.00001019
Iteration 147/1000 | Loss: 0.00001019
Iteration 148/1000 | Loss: 0.00001019
Iteration 149/1000 | Loss: 0.00001019
Iteration 150/1000 | Loss: 0.00001019
Iteration 151/1000 | Loss: 0.00001019
Iteration 152/1000 | Loss: 0.00001019
Iteration 153/1000 | Loss: 0.00001019
Iteration 154/1000 | Loss: 0.00001019
Iteration 155/1000 | Loss: 0.00001019
Iteration 156/1000 | Loss: 0.00001019
Iteration 157/1000 | Loss: 0.00001019
Iteration 158/1000 | Loss: 0.00001019
Iteration 159/1000 | Loss: 0.00001019
Iteration 160/1000 | Loss: 0.00001019
Iteration 161/1000 | Loss: 0.00001019
Iteration 162/1000 | Loss: 0.00001019
Iteration 163/1000 | Loss: 0.00001019
Iteration 164/1000 | Loss: 0.00001019
Iteration 165/1000 | Loss: 0.00001019
Iteration 166/1000 | Loss: 0.00001019
Iteration 167/1000 | Loss: 0.00001019
Iteration 168/1000 | Loss: 0.00001019
Iteration 169/1000 | Loss: 0.00001018
Iteration 170/1000 | Loss: 0.00001018
Iteration 171/1000 | Loss: 0.00001018
Iteration 172/1000 | Loss: 0.00001018
Iteration 173/1000 | Loss: 0.00001018
Iteration 174/1000 | Loss: 0.00001018
Iteration 175/1000 | Loss: 0.00001018
Iteration 176/1000 | Loss: 0.00001018
Iteration 177/1000 | Loss: 0.00001018
Iteration 178/1000 | Loss: 0.00001018
Iteration 179/1000 | Loss: 0.00001018
Iteration 180/1000 | Loss: 0.00001018
Iteration 181/1000 | Loss: 0.00001018
Iteration 182/1000 | Loss: 0.00001018
Iteration 183/1000 | Loss: 0.00001017
Iteration 184/1000 | Loss: 0.00001017
Iteration 185/1000 | Loss: 0.00001017
Iteration 186/1000 | Loss: 0.00001017
Iteration 187/1000 | Loss: 0.00001017
Iteration 188/1000 | Loss: 0.00001017
Iteration 189/1000 | Loss: 0.00001017
Iteration 190/1000 | Loss: 0.00001017
Iteration 191/1000 | Loss: 0.00001017
Iteration 192/1000 | Loss: 0.00001017
Iteration 193/1000 | Loss: 0.00001017
Iteration 194/1000 | Loss: 0.00001017
Iteration 195/1000 | Loss: 0.00001017
Iteration 196/1000 | Loss: 0.00001017
Iteration 197/1000 | Loss: 0.00001017
Iteration 198/1000 | Loss: 0.00001017
Iteration 199/1000 | Loss: 0.00001017
Iteration 200/1000 | Loss: 0.00001017
Iteration 201/1000 | Loss: 0.00001017
Iteration 202/1000 | Loss: 0.00001017
Iteration 203/1000 | Loss: 0.00001017
Iteration 204/1000 | Loss: 0.00001016
Iteration 205/1000 | Loss: 0.00001016
Iteration 206/1000 | Loss: 0.00001016
Iteration 207/1000 | Loss: 0.00001016
Iteration 208/1000 | Loss: 0.00001016
Iteration 209/1000 | Loss: 0.00001016
Iteration 210/1000 | Loss: 0.00001016
Iteration 211/1000 | Loss: 0.00001016
Iteration 212/1000 | Loss: 0.00001016
Iteration 213/1000 | Loss: 0.00001016
Iteration 214/1000 | Loss: 0.00001016
Iteration 215/1000 | Loss: 0.00001016
Iteration 216/1000 | Loss: 0.00001016
Iteration 217/1000 | Loss: 0.00001016
Iteration 218/1000 | Loss: 0.00001016
Iteration 219/1000 | Loss: 0.00001016
Iteration 220/1000 | Loss: 0.00001016
Iteration 221/1000 | Loss: 0.00001016
Iteration 222/1000 | Loss: 0.00001016
Iteration 223/1000 | Loss: 0.00001016
Iteration 224/1000 | Loss: 0.00001016
Iteration 225/1000 | Loss: 0.00001016
Iteration 226/1000 | Loss: 0.00001016
Iteration 227/1000 | Loss: 0.00001016
Iteration 228/1000 | Loss: 0.00001016
Iteration 229/1000 | Loss: 0.00001016
Iteration 230/1000 | Loss: 0.00001016
Iteration 231/1000 | Loss: 0.00001016
Iteration 232/1000 | Loss: 0.00001016
Iteration 233/1000 | Loss: 0.00001016
Iteration 234/1000 | Loss: 0.00001016
Iteration 235/1000 | Loss: 0.00001016
Iteration 236/1000 | Loss: 0.00001016
Iteration 237/1000 | Loss: 0.00001016
Iteration 238/1000 | Loss: 0.00001016
Iteration 239/1000 | Loss: 0.00001016
Iteration 240/1000 | Loss: 0.00001016
Iteration 241/1000 | Loss: 0.00001016
Iteration 242/1000 | Loss: 0.00001016
Iteration 243/1000 | Loss: 0.00001016
Iteration 244/1000 | Loss: 0.00001016
Iteration 245/1000 | Loss: 0.00001016
Iteration 246/1000 | Loss: 0.00001016
Iteration 247/1000 | Loss: 0.00001016
Iteration 248/1000 | Loss: 0.00001016
Iteration 249/1000 | Loss: 0.00001016
Iteration 250/1000 | Loss: 0.00001016
Iteration 251/1000 | Loss: 0.00001016
Iteration 252/1000 | Loss: 0.00001016
Iteration 253/1000 | Loss: 0.00001016
Iteration 254/1000 | Loss: 0.00001016
Iteration 255/1000 | Loss: 0.00001016
Iteration 256/1000 | Loss: 0.00001016
Iteration 257/1000 | Loss: 0.00001016
Iteration 258/1000 | Loss: 0.00001016
Iteration 259/1000 | Loss: 0.00001016
Iteration 260/1000 | Loss: 0.00001016
Iteration 261/1000 | Loss: 0.00001016
Iteration 262/1000 | Loss: 0.00001016
Iteration 263/1000 | Loss: 0.00001016
Iteration 264/1000 | Loss: 0.00001016
Iteration 265/1000 | Loss: 0.00001016
Iteration 266/1000 | Loss: 0.00001016
Iteration 267/1000 | Loss: 0.00001016
Iteration 268/1000 | Loss: 0.00001016
Iteration 269/1000 | Loss: 0.00001016
Iteration 270/1000 | Loss: 0.00001016
Iteration 271/1000 | Loss: 0.00001016
Iteration 272/1000 | Loss: 0.00001016
Iteration 273/1000 | Loss: 0.00001016
Iteration 274/1000 | Loss: 0.00001016
Iteration 275/1000 | Loss: 0.00001016
Iteration 276/1000 | Loss: 0.00001016
Iteration 277/1000 | Loss: 0.00001016
Iteration 278/1000 | Loss: 0.00001016
Iteration 279/1000 | Loss: 0.00001016
Iteration 280/1000 | Loss: 0.00001016
Iteration 281/1000 | Loss: 0.00001016
Iteration 282/1000 | Loss: 0.00001016
Iteration 283/1000 | Loss: 0.00001016
Iteration 284/1000 | Loss: 0.00001016
Iteration 285/1000 | Loss: 0.00001016
Iteration 286/1000 | Loss: 0.00001016
Iteration 287/1000 | Loss: 0.00001016
Iteration 288/1000 | Loss: 0.00001016
Iteration 289/1000 | Loss: 0.00001016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 289. Stopping optimization.
Last 5 losses: [1.0160187230212614e-05, 1.0160187230212614e-05, 1.0160187230212614e-05, 1.0160187230212614e-05, 1.0160187230212614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0160187230212614e-05

Optimization complete. Final v2v error: 2.7702434062957764 mm

Highest mean error: 3.0726780891418457 mm for frame 55

Lowest mean error: 2.5920519828796387 mm for frame 160

Saving results

Total time: 45.68157196044922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00583893
Iteration 2/25 | Loss: 0.00124398
Iteration 3/25 | Loss: 0.00117597
Iteration 4/25 | Loss: 0.00116508
Iteration 5/25 | Loss: 0.00116147
Iteration 6/25 | Loss: 0.00116100
Iteration 7/25 | Loss: 0.00116100
Iteration 8/25 | Loss: 0.00116100
Iteration 9/25 | Loss: 0.00116100
Iteration 10/25 | Loss: 0.00116100
Iteration 11/25 | Loss: 0.00116100
Iteration 12/25 | Loss: 0.00116100
Iteration 13/25 | Loss: 0.00116100
Iteration 14/25 | Loss: 0.00116100
Iteration 15/25 | Loss: 0.00116100
Iteration 16/25 | Loss: 0.00116100
Iteration 17/25 | Loss: 0.00116100
Iteration 18/25 | Loss: 0.00116100
Iteration 19/25 | Loss: 0.00116100
Iteration 20/25 | Loss: 0.00116100
Iteration 21/25 | Loss: 0.00116100
Iteration 22/25 | Loss: 0.00116100
Iteration 23/25 | Loss: 0.00116100
Iteration 24/25 | Loss: 0.00116100
Iteration 25/25 | Loss: 0.00116100

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.65364933
Iteration 2/25 | Loss: 0.00134047
Iteration 3/25 | Loss: 0.00134046
Iteration 4/25 | Loss: 0.00134046
Iteration 5/25 | Loss: 0.00134046
Iteration 6/25 | Loss: 0.00134046
Iteration 7/25 | Loss: 0.00134046
Iteration 8/25 | Loss: 0.00134046
Iteration 9/25 | Loss: 0.00134046
Iteration 10/25 | Loss: 0.00134046
Iteration 11/25 | Loss: 0.00134046
Iteration 12/25 | Loss: 0.00134046
Iteration 13/25 | Loss: 0.00134046
Iteration 14/25 | Loss: 0.00134046
Iteration 15/25 | Loss: 0.00134046
Iteration 16/25 | Loss: 0.00134046
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013404617784544826, 0.0013404617784544826, 0.0013404617784544826, 0.0013404617784544826, 0.0013404617784544826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013404617784544826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134046
Iteration 2/1000 | Loss: 0.00002205
Iteration 3/1000 | Loss: 0.00001598
Iteration 4/1000 | Loss: 0.00001348
Iteration 5/1000 | Loss: 0.00001262
Iteration 6/1000 | Loss: 0.00001208
Iteration 7/1000 | Loss: 0.00001165
Iteration 8/1000 | Loss: 0.00001140
Iteration 9/1000 | Loss: 0.00001112
Iteration 10/1000 | Loss: 0.00001091
Iteration 11/1000 | Loss: 0.00001077
Iteration 12/1000 | Loss: 0.00001074
Iteration 13/1000 | Loss: 0.00001069
Iteration 14/1000 | Loss: 0.00001067
Iteration 15/1000 | Loss: 0.00001067
Iteration 16/1000 | Loss: 0.00001065
Iteration 17/1000 | Loss: 0.00001059
Iteration 18/1000 | Loss: 0.00001054
Iteration 19/1000 | Loss: 0.00001051
Iteration 20/1000 | Loss: 0.00001050
Iteration 21/1000 | Loss: 0.00001049
Iteration 22/1000 | Loss: 0.00001044
Iteration 23/1000 | Loss: 0.00001038
Iteration 24/1000 | Loss: 0.00001036
Iteration 25/1000 | Loss: 0.00001035
Iteration 26/1000 | Loss: 0.00001031
Iteration 27/1000 | Loss: 0.00001031
Iteration 28/1000 | Loss: 0.00001030
Iteration 29/1000 | Loss: 0.00001027
Iteration 30/1000 | Loss: 0.00001024
Iteration 31/1000 | Loss: 0.00001024
Iteration 32/1000 | Loss: 0.00001022
Iteration 33/1000 | Loss: 0.00001022
Iteration 34/1000 | Loss: 0.00001021
Iteration 35/1000 | Loss: 0.00001020
Iteration 36/1000 | Loss: 0.00001019
Iteration 37/1000 | Loss: 0.00001019
Iteration 38/1000 | Loss: 0.00001018
Iteration 39/1000 | Loss: 0.00001017
Iteration 40/1000 | Loss: 0.00001016
Iteration 41/1000 | Loss: 0.00001014
Iteration 42/1000 | Loss: 0.00001014
Iteration 43/1000 | Loss: 0.00001013
Iteration 44/1000 | Loss: 0.00001013
Iteration 45/1000 | Loss: 0.00001013
Iteration 46/1000 | Loss: 0.00001010
Iteration 47/1000 | Loss: 0.00001009
Iteration 48/1000 | Loss: 0.00001009
Iteration 49/1000 | Loss: 0.00001009
Iteration 50/1000 | Loss: 0.00001008
Iteration 51/1000 | Loss: 0.00001008
Iteration 52/1000 | Loss: 0.00001007
Iteration 53/1000 | Loss: 0.00001007
Iteration 54/1000 | Loss: 0.00001007
Iteration 55/1000 | Loss: 0.00001006
Iteration 56/1000 | Loss: 0.00001006
Iteration 57/1000 | Loss: 0.00001006
Iteration 58/1000 | Loss: 0.00001005
Iteration 59/1000 | Loss: 0.00001005
Iteration 60/1000 | Loss: 0.00001005
Iteration 61/1000 | Loss: 0.00001004
Iteration 62/1000 | Loss: 0.00001004
Iteration 63/1000 | Loss: 0.00001004
Iteration 64/1000 | Loss: 0.00001004
Iteration 65/1000 | Loss: 0.00001003
Iteration 66/1000 | Loss: 0.00001003
Iteration 67/1000 | Loss: 0.00001003
Iteration 68/1000 | Loss: 0.00001003
Iteration 69/1000 | Loss: 0.00001003
Iteration 70/1000 | Loss: 0.00001003
Iteration 71/1000 | Loss: 0.00001002
Iteration 72/1000 | Loss: 0.00001002
Iteration 73/1000 | Loss: 0.00001002
Iteration 74/1000 | Loss: 0.00001002
Iteration 75/1000 | Loss: 0.00001002
Iteration 76/1000 | Loss: 0.00001002
Iteration 77/1000 | Loss: 0.00001001
Iteration 78/1000 | Loss: 0.00001001
Iteration 79/1000 | Loss: 0.00001001
Iteration 80/1000 | Loss: 0.00001001
Iteration 81/1000 | Loss: 0.00001001
Iteration 82/1000 | Loss: 0.00001000
Iteration 83/1000 | Loss: 0.00001000
Iteration 84/1000 | Loss: 0.00001000
Iteration 85/1000 | Loss: 0.00001000
Iteration 86/1000 | Loss: 0.00001000
Iteration 87/1000 | Loss: 0.00000999
Iteration 88/1000 | Loss: 0.00000999
Iteration 89/1000 | Loss: 0.00000999
Iteration 90/1000 | Loss: 0.00000999
Iteration 91/1000 | Loss: 0.00000999
Iteration 92/1000 | Loss: 0.00000999
Iteration 93/1000 | Loss: 0.00000999
Iteration 94/1000 | Loss: 0.00000999
Iteration 95/1000 | Loss: 0.00000999
Iteration 96/1000 | Loss: 0.00000998
Iteration 97/1000 | Loss: 0.00000998
Iteration 98/1000 | Loss: 0.00000997
Iteration 99/1000 | Loss: 0.00000997
Iteration 100/1000 | Loss: 0.00000997
Iteration 101/1000 | Loss: 0.00000997
Iteration 102/1000 | Loss: 0.00000997
Iteration 103/1000 | Loss: 0.00000997
Iteration 104/1000 | Loss: 0.00000997
Iteration 105/1000 | Loss: 0.00000997
Iteration 106/1000 | Loss: 0.00000997
Iteration 107/1000 | Loss: 0.00000997
Iteration 108/1000 | Loss: 0.00000997
Iteration 109/1000 | Loss: 0.00000996
Iteration 110/1000 | Loss: 0.00000996
Iteration 111/1000 | Loss: 0.00000996
Iteration 112/1000 | Loss: 0.00000996
Iteration 113/1000 | Loss: 0.00000996
Iteration 114/1000 | Loss: 0.00000996
Iteration 115/1000 | Loss: 0.00000996
Iteration 116/1000 | Loss: 0.00000996
Iteration 117/1000 | Loss: 0.00000996
Iteration 118/1000 | Loss: 0.00000996
Iteration 119/1000 | Loss: 0.00000996
Iteration 120/1000 | Loss: 0.00000996
Iteration 121/1000 | Loss: 0.00000996
Iteration 122/1000 | Loss: 0.00000996
Iteration 123/1000 | Loss: 0.00000996
Iteration 124/1000 | Loss: 0.00000996
Iteration 125/1000 | Loss: 0.00000996
Iteration 126/1000 | Loss: 0.00000996
Iteration 127/1000 | Loss: 0.00000996
Iteration 128/1000 | Loss: 0.00000996
Iteration 129/1000 | Loss: 0.00000996
Iteration 130/1000 | Loss: 0.00000996
Iteration 131/1000 | Loss: 0.00000996
Iteration 132/1000 | Loss: 0.00000996
Iteration 133/1000 | Loss: 0.00000996
Iteration 134/1000 | Loss: 0.00000996
Iteration 135/1000 | Loss: 0.00000996
Iteration 136/1000 | Loss: 0.00000996
Iteration 137/1000 | Loss: 0.00000996
Iteration 138/1000 | Loss: 0.00000996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [9.962013791664504e-06, 9.962013791664504e-06, 9.962013791664504e-06, 9.962013791664504e-06, 9.962013791664504e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.962013791664504e-06

Optimization complete. Final v2v error: 2.7324559688568115 mm

Highest mean error: 3.1656808853149414 mm for frame 117

Lowest mean error: 2.5220983028411865 mm for frame 170

Saving results

Total time: 36.900760650634766
