Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=48, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2688-2743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00547834
Iteration 2/25 | Loss: 0.00198968
Iteration 3/25 | Loss: 0.00190728
Iteration 4/25 | Loss: 0.00190169
Iteration 5/25 | Loss: 0.00189992
Iteration 6/25 | Loss: 0.00189963
Iteration 7/25 | Loss: 0.00189963
Iteration 8/25 | Loss: 0.00189963
Iteration 9/25 | Loss: 0.00189963
Iteration 10/25 | Loss: 0.00189963
Iteration 11/25 | Loss: 0.00189963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0018996259896084666, 0.0018996259896084666, 0.0018996259896084666, 0.0018996259896084666, 0.0018996259896084666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018996259896084666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.96106243
Iteration 2/25 | Loss: 0.00255732
Iteration 3/25 | Loss: 0.00255731
Iteration 4/25 | Loss: 0.00255731
Iteration 5/25 | Loss: 0.00255731
Iteration 6/25 | Loss: 0.00255731
Iteration 7/25 | Loss: 0.00255731
Iteration 8/25 | Loss: 0.00255731
Iteration 9/25 | Loss: 0.00255731
Iteration 10/25 | Loss: 0.00255731
Iteration 11/25 | Loss: 0.00255731
Iteration 12/25 | Loss: 0.00255731
Iteration 13/25 | Loss: 0.00255731
Iteration 14/25 | Loss: 0.00255731
Iteration 15/25 | Loss: 0.00255731
Iteration 16/25 | Loss: 0.00255731
Iteration 17/25 | Loss: 0.00255731
Iteration 18/25 | Loss: 0.00255731
Iteration 19/25 | Loss: 0.00255731
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002557308180257678, 0.002557308180257678, 0.002557308180257678, 0.002557308180257678, 0.002557308180257678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002557308180257678

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00255731
Iteration 2/1000 | Loss: 0.00006743
Iteration 3/1000 | Loss: 0.00004421
Iteration 4/1000 | Loss: 0.00003709
Iteration 5/1000 | Loss: 0.00003439
Iteration 6/1000 | Loss: 0.00003314
Iteration 7/1000 | Loss: 0.00003239
Iteration 8/1000 | Loss: 0.00003182
Iteration 9/1000 | Loss: 0.00003148
Iteration 10/1000 | Loss: 0.00003126
Iteration 11/1000 | Loss: 0.00003104
Iteration 12/1000 | Loss: 0.00003089
Iteration 13/1000 | Loss: 0.00003080
Iteration 14/1000 | Loss: 0.00003072
Iteration 15/1000 | Loss: 0.00003072
Iteration 16/1000 | Loss: 0.00003070
Iteration 17/1000 | Loss: 0.00003069
Iteration 18/1000 | Loss: 0.00003068
Iteration 19/1000 | Loss: 0.00003068
Iteration 20/1000 | Loss: 0.00003068
Iteration 21/1000 | Loss: 0.00003067
Iteration 22/1000 | Loss: 0.00003067
Iteration 23/1000 | Loss: 0.00003067
Iteration 24/1000 | Loss: 0.00003067
Iteration 25/1000 | Loss: 0.00003066
Iteration 26/1000 | Loss: 0.00003066
Iteration 27/1000 | Loss: 0.00003066
Iteration 28/1000 | Loss: 0.00003066
Iteration 29/1000 | Loss: 0.00003065
Iteration 30/1000 | Loss: 0.00003065
Iteration 31/1000 | Loss: 0.00003065
Iteration 32/1000 | Loss: 0.00003064
Iteration 33/1000 | Loss: 0.00003064
Iteration 34/1000 | Loss: 0.00003064
Iteration 35/1000 | Loss: 0.00003064
Iteration 36/1000 | Loss: 0.00003064
Iteration 37/1000 | Loss: 0.00003064
Iteration 38/1000 | Loss: 0.00003063
Iteration 39/1000 | Loss: 0.00003063
Iteration 40/1000 | Loss: 0.00003063
Iteration 41/1000 | Loss: 0.00003063
Iteration 42/1000 | Loss: 0.00003062
Iteration 43/1000 | Loss: 0.00003062
Iteration 44/1000 | Loss: 0.00003062
Iteration 45/1000 | Loss: 0.00003061
Iteration 46/1000 | Loss: 0.00003061
Iteration 47/1000 | Loss: 0.00003060
Iteration 48/1000 | Loss: 0.00003060
Iteration 49/1000 | Loss: 0.00003060
Iteration 50/1000 | Loss: 0.00003060
Iteration 51/1000 | Loss: 0.00003060
Iteration 52/1000 | Loss: 0.00003060
Iteration 53/1000 | Loss: 0.00003060
Iteration 54/1000 | Loss: 0.00003060
Iteration 55/1000 | Loss: 0.00003060
Iteration 56/1000 | Loss: 0.00003060
Iteration 57/1000 | Loss: 0.00003060
Iteration 58/1000 | Loss: 0.00003060
Iteration 59/1000 | Loss: 0.00003059
Iteration 60/1000 | Loss: 0.00003059
Iteration 61/1000 | Loss: 0.00003059
Iteration 62/1000 | Loss: 0.00003058
Iteration 63/1000 | Loss: 0.00003058
Iteration 64/1000 | Loss: 0.00003058
Iteration 65/1000 | Loss: 0.00003058
Iteration 66/1000 | Loss: 0.00003058
Iteration 67/1000 | Loss: 0.00003058
Iteration 68/1000 | Loss: 0.00003057
Iteration 69/1000 | Loss: 0.00003057
Iteration 70/1000 | Loss: 0.00003057
Iteration 71/1000 | Loss: 0.00003057
Iteration 72/1000 | Loss: 0.00003057
Iteration 73/1000 | Loss: 0.00003057
Iteration 74/1000 | Loss: 0.00003057
Iteration 75/1000 | Loss: 0.00003057
Iteration 76/1000 | Loss: 0.00003057
Iteration 77/1000 | Loss: 0.00003057
Iteration 78/1000 | Loss: 0.00003056
Iteration 79/1000 | Loss: 0.00003056
Iteration 80/1000 | Loss: 0.00003056
Iteration 81/1000 | Loss: 0.00003056
Iteration 82/1000 | Loss: 0.00003056
Iteration 83/1000 | Loss: 0.00003056
Iteration 84/1000 | Loss: 0.00003056
Iteration 85/1000 | Loss: 0.00003055
Iteration 86/1000 | Loss: 0.00003055
Iteration 87/1000 | Loss: 0.00003055
Iteration 88/1000 | Loss: 0.00003055
Iteration 89/1000 | Loss: 0.00003055
Iteration 90/1000 | Loss: 0.00003055
Iteration 91/1000 | Loss: 0.00003055
Iteration 92/1000 | Loss: 0.00003055
Iteration 93/1000 | Loss: 0.00003055
Iteration 94/1000 | Loss: 0.00003054
Iteration 95/1000 | Loss: 0.00003054
Iteration 96/1000 | Loss: 0.00003054
Iteration 97/1000 | Loss: 0.00003054
Iteration 98/1000 | Loss: 0.00003054
Iteration 99/1000 | Loss: 0.00003054
Iteration 100/1000 | Loss: 0.00003054
Iteration 101/1000 | Loss: 0.00003054
Iteration 102/1000 | Loss: 0.00003054
Iteration 103/1000 | Loss: 0.00003054
Iteration 104/1000 | Loss: 0.00003054
Iteration 105/1000 | Loss: 0.00003054
Iteration 106/1000 | Loss: 0.00003053
Iteration 107/1000 | Loss: 0.00003053
Iteration 108/1000 | Loss: 0.00003053
Iteration 109/1000 | Loss: 0.00003053
Iteration 110/1000 | Loss: 0.00003053
Iteration 111/1000 | Loss: 0.00003053
Iteration 112/1000 | Loss: 0.00003053
Iteration 113/1000 | Loss: 0.00003053
Iteration 114/1000 | Loss: 0.00003053
Iteration 115/1000 | Loss: 0.00003053
Iteration 116/1000 | Loss: 0.00003053
Iteration 117/1000 | Loss: 0.00003053
Iteration 118/1000 | Loss: 0.00003053
Iteration 119/1000 | Loss: 0.00003053
Iteration 120/1000 | Loss: 0.00003053
Iteration 121/1000 | Loss: 0.00003053
Iteration 122/1000 | Loss: 0.00003053
Iteration 123/1000 | Loss: 0.00003053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [3.053419277421199e-05, 3.053419277421199e-05, 3.053419277421199e-05, 3.053419277421199e-05, 3.053419277421199e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.053419277421199e-05

Optimization complete. Final v2v error: 4.700046539306641 mm

Highest mean error: 5.1024580001831055 mm for frame 45

Lowest mean error: 4.423111438751221 mm for frame 145

Saving results

Total time: 35.199098110198975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977005
Iteration 2/25 | Loss: 0.00221535
Iteration 3/25 | Loss: 0.00201734
Iteration 4/25 | Loss: 0.00200030
Iteration 5/25 | Loss: 0.00195183
Iteration 6/25 | Loss: 0.00192331
Iteration 7/25 | Loss: 0.00190777
Iteration 8/25 | Loss: 0.00188835
Iteration 9/25 | Loss: 0.00188248
Iteration 10/25 | Loss: 0.00187946
Iteration 11/25 | Loss: 0.00187985
Iteration 12/25 | Loss: 0.00187766
Iteration 13/25 | Loss: 0.00187987
Iteration 14/25 | Loss: 0.00187813
Iteration 15/25 | Loss: 0.00187869
Iteration 16/25 | Loss: 0.00187755
Iteration 17/25 | Loss: 0.00187750
Iteration 18/25 | Loss: 0.00187852
Iteration 19/25 | Loss: 0.00187906
Iteration 20/25 | Loss: 0.00187802
Iteration 21/25 | Loss: 0.00187862
Iteration 22/25 | Loss: 0.00187819
Iteration 23/25 | Loss: 0.00187822
Iteration 24/25 | Loss: 0.00187806
Iteration 25/25 | Loss: 0.00187786

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.37181139
Iteration 2/25 | Loss: 0.00232584
Iteration 3/25 | Loss: 0.00227366
Iteration 4/25 | Loss: 0.00227366
Iteration 5/25 | Loss: 0.00227366
Iteration 6/25 | Loss: 0.00227366
Iteration 7/25 | Loss: 0.00227366
Iteration 8/25 | Loss: 0.00227366
Iteration 9/25 | Loss: 0.00227366
Iteration 10/25 | Loss: 0.00227366
Iteration 11/25 | Loss: 0.00227366
Iteration 12/25 | Loss: 0.00227366
Iteration 13/25 | Loss: 0.00227366
Iteration 14/25 | Loss: 0.00227366
Iteration 15/25 | Loss: 0.00227366
Iteration 16/25 | Loss: 0.00227366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002273655030876398, 0.002273655030876398, 0.002273655030876398, 0.002273655030876398, 0.002273655030876398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002273655030876398

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227366
Iteration 2/1000 | Loss: 0.00015600
Iteration 3/1000 | Loss: 0.00006712
Iteration 4/1000 | Loss: 0.00014692
Iteration 5/1000 | Loss: 0.00004804
Iteration 6/1000 | Loss: 0.00007912
Iteration 7/1000 | Loss: 0.00021637
Iteration 8/1000 | Loss: 0.00014637
Iteration 9/1000 | Loss: 0.00009817
Iteration 10/1000 | Loss: 0.00006481
Iteration 11/1000 | Loss: 0.00006164
Iteration 12/1000 | Loss: 0.00017884
Iteration 13/1000 | Loss: 0.00018170
Iteration 14/1000 | Loss: 0.00006421
Iteration 15/1000 | Loss: 0.00005017
Iteration 16/1000 | Loss: 0.00005945
Iteration 17/1000 | Loss: 0.00004464
Iteration 18/1000 | Loss: 0.00004003
Iteration 19/1000 | Loss: 0.00005897
Iteration 20/1000 | Loss: 0.00003850
Iteration 21/1000 | Loss: 0.00003685
Iteration 22/1000 | Loss: 0.00006785
Iteration 23/1000 | Loss: 0.00003596
Iteration 24/1000 | Loss: 0.00003563
Iteration 25/1000 | Loss: 0.00008452
Iteration 26/1000 | Loss: 0.00003567
Iteration 27/1000 | Loss: 0.00003528
Iteration 28/1000 | Loss: 0.00003525
Iteration 29/1000 | Loss: 0.00003525
Iteration 30/1000 | Loss: 0.00008097
Iteration 31/1000 | Loss: 0.00003534
Iteration 32/1000 | Loss: 0.00003497
Iteration 33/1000 | Loss: 0.00003494
Iteration 34/1000 | Loss: 0.00003490
Iteration 35/1000 | Loss: 0.00003490
Iteration 36/1000 | Loss: 0.00003480
Iteration 37/1000 | Loss: 0.00003474
Iteration 38/1000 | Loss: 0.00003474
Iteration 39/1000 | Loss: 0.00003473
Iteration 40/1000 | Loss: 0.00003473
Iteration 41/1000 | Loss: 0.00003471
Iteration 42/1000 | Loss: 0.00003471
Iteration 43/1000 | Loss: 0.00003470
Iteration 44/1000 | Loss: 0.00003469
Iteration 45/1000 | Loss: 0.00003469
Iteration 46/1000 | Loss: 0.00003469
Iteration 47/1000 | Loss: 0.00003469
Iteration 48/1000 | Loss: 0.00003469
Iteration 49/1000 | Loss: 0.00003468
Iteration 50/1000 | Loss: 0.00003467
Iteration 51/1000 | Loss: 0.00003465
Iteration 52/1000 | Loss: 0.00003464
Iteration 53/1000 | Loss: 0.00003463
Iteration 54/1000 | Loss: 0.00003463
Iteration 55/1000 | Loss: 0.00003462
Iteration 56/1000 | Loss: 0.00003462
Iteration 57/1000 | Loss: 0.00003462
Iteration 58/1000 | Loss: 0.00003462
Iteration 59/1000 | Loss: 0.00003462
Iteration 60/1000 | Loss: 0.00003461
Iteration 61/1000 | Loss: 0.00003461
Iteration 62/1000 | Loss: 0.00003461
Iteration 63/1000 | Loss: 0.00003460
Iteration 64/1000 | Loss: 0.00003460
Iteration 65/1000 | Loss: 0.00003460
Iteration 66/1000 | Loss: 0.00003460
Iteration 67/1000 | Loss: 0.00003459
Iteration 68/1000 | Loss: 0.00003459
Iteration 69/1000 | Loss: 0.00003458
Iteration 70/1000 | Loss: 0.00003458
Iteration 71/1000 | Loss: 0.00003458
Iteration 72/1000 | Loss: 0.00003458
Iteration 73/1000 | Loss: 0.00003458
Iteration 74/1000 | Loss: 0.00003457
Iteration 75/1000 | Loss: 0.00003457
Iteration 76/1000 | Loss: 0.00003457
Iteration 77/1000 | Loss: 0.00003457
Iteration 78/1000 | Loss: 0.00003457
Iteration 79/1000 | Loss: 0.00003457
Iteration 80/1000 | Loss: 0.00003457
Iteration 81/1000 | Loss: 0.00003457
Iteration 82/1000 | Loss: 0.00003457
Iteration 83/1000 | Loss: 0.00003457
Iteration 84/1000 | Loss: 0.00003457
Iteration 85/1000 | Loss: 0.00003456
Iteration 86/1000 | Loss: 0.00003456
Iteration 87/1000 | Loss: 0.00003456
Iteration 88/1000 | Loss: 0.00003456
Iteration 89/1000 | Loss: 0.00003456
Iteration 90/1000 | Loss: 0.00003456
Iteration 91/1000 | Loss: 0.00003456
Iteration 92/1000 | Loss: 0.00003455
Iteration 93/1000 | Loss: 0.00003455
Iteration 94/1000 | Loss: 0.00003455
Iteration 95/1000 | Loss: 0.00003455
Iteration 96/1000 | Loss: 0.00003454
Iteration 97/1000 | Loss: 0.00003454
Iteration 98/1000 | Loss: 0.00003454
Iteration 99/1000 | Loss: 0.00003454
Iteration 100/1000 | Loss: 0.00003454
Iteration 101/1000 | Loss: 0.00003454
Iteration 102/1000 | Loss: 0.00003454
Iteration 103/1000 | Loss: 0.00003454
Iteration 104/1000 | Loss: 0.00003454
Iteration 105/1000 | Loss: 0.00003453
Iteration 106/1000 | Loss: 0.00003453
Iteration 107/1000 | Loss: 0.00003453
Iteration 108/1000 | Loss: 0.00003453
Iteration 109/1000 | Loss: 0.00003453
Iteration 110/1000 | Loss: 0.00003453
Iteration 111/1000 | Loss: 0.00003453
Iteration 112/1000 | Loss: 0.00003453
Iteration 113/1000 | Loss: 0.00003453
Iteration 114/1000 | Loss: 0.00003453
Iteration 115/1000 | Loss: 0.00003453
Iteration 116/1000 | Loss: 0.00003452
Iteration 117/1000 | Loss: 0.00003452
Iteration 118/1000 | Loss: 0.00003450
Iteration 119/1000 | Loss: 0.00003450
Iteration 120/1000 | Loss: 0.00003450
Iteration 121/1000 | Loss: 0.00003450
Iteration 122/1000 | Loss: 0.00003449
Iteration 123/1000 | Loss: 0.00003449
Iteration 124/1000 | Loss: 0.00003449
Iteration 125/1000 | Loss: 0.00003449
Iteration 126/1000 | Loss: 0.00003448
Iteration 127/1000 | Loss: 0.00003448
Iteration 128/1000 | Loss: 0.00003448
Iteration 129/1000 | Loss: 0.00003447
Iteration 130/1000 | Loss: 0.00003447
Iteration 131/1000 | Loss: 0.00003447
Iteration 132/1000 | Loss: 0.00003447
Iteration 133/1000 | Loss: 0.00003447
Iteration 134/1000 | Loss: 0.00003447
Iteration 135/1000 | Loss: 0.00003447
Iteration 136/1000 | Loss: 0.00003447
Iteration 137/1000 | Loss: 0.00003447
Iteration 138/1000 | Loss: 0.00003447
Iteration 139/1000 | Loss: 0.00003447
Iteration 140/1000 | Loss: 0.00003447
Iteration 141/1000 | Loss: 0.00003447
Iteration 142/1000 | Loss: 0.00003447
Iteration 143/1000 | Loss: 0.00003446
Iteration 144/1000 | Loss: 0.00003446
Iteration 145/1000 | Loss: 0.00003446
Iteration 146/1000 | Loss: 0.00003446
Iteration 147/1000 | Loss: 0.00003446
Iteration 148/1000 | Loss: 0.00003446
Iteration 149/1000 | Loss: 0.00003446
Iteration 150/1000 | Loss: 0.00003446
Iteration 151/1000 | Loss: 0.00003446
Iteration 152/1000 | Loss: 0.00003446
Iteration 153/1000 | Loss: 0.00003446
Iteration 154/1000 | Loss: 0.00003446
Iteration 155/1000 | Loss: 0.00003446
Iteration 156/1000 | Loss: 0.00003446
Iteration 157/1000 | Loss: 0.00003446
Iteration 158/1000 | Loss: 0.00003446
Iteration 159/1000 | Loss: 0.00003446
Iteration 160/1000 | Loss: 0.00003446
Iteration 161/1000 | Loss: 0.00003446
Iteration 162/1000 | Loss: 0.00003446
Iteration 163/1000 | Loss: 0.00003446
Iteration 164/1000 | Loss: 0.00003446
Iteration 165/1000 | Loss: 0.00003446
Iteration 166/1000 | Loss: 0.00003446
Iteration 167/1000 | Loss: 0.00003446
Iteration 168/1000 | Loss: 0.00003446
Iteration 169/1000 | Loss: 0.00003446
Iteration 170/1000 | Loss: 0.00003446
Iteration 171/1000 | Loss: 0.00003446
Iteration 172/1000 | Loss: 0.00003446
Iteration 173/1000 | Loss: 0.00003446
Iteration 174/1000 | Loss: 0.00003446
Iteration 175/1000 | Loss: 0.00003446
Iteration 176/1000 | Loss: 0.00003446
Iteration 177/1000 | Loss: 0.00003446
Iteration 178/1000 | Loss: 0.00003446
Iteration 179/1000 | Loss: 0.00003446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [3.4456057619536296e-05, 3.4456057619536296e-05, 3.4456057619536296e-05, 3.4456057619536296e-05, 3.4456057619536296e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4456057619536296e-05

Optimization complete. Final v2v error: 5.047694206237793 mm

Highest mean error: 13.235600471496582 mm for frame 104

Lowest mean error: 4.53861665725708 mm for frame 51

Saving results

Total time: 104.83914303779602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448804
Iteration 2/25 | Loss: 0.00196457
Iteration 3/25 | Loss: 0.00189584
Iteration 4/25 | Loss: 0.00188851
Iteration 5/25 | Loss: 0.00188605
Iteration 6/25 | Loss: 0.00188605
Iteration 7/25 | Loss: 0.00188605
Iteration 8/25 | Loss: 0.00188605
Iteration 9/25 | Loss: 0.00188605
Iteration 10/25 | Loss: 0.00188605
Iteration 11/25 | Loss: 0.00188605
Iteration 12/25 | Loss: 0.00188605
Iteration 13/25 | Loss: 0.00188605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0018860501004382968, 0.0018860501004382968, 0.0018860501004382968, 0.0018860501004382968, 0.0018860501004382968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018860501004382968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21768367
Iteration 2/25 | Loss: 0.00273250
Iteration 3/25 | Loss: 0.00273250
Iteration 4/25 | Loss: 0.00273250
Iteration 5/25 | Loss: 0.00273250
Iteration 6/25 | Loss: 0.00273250
Iteration 7/25 | Loss: 0.00273250
Iteration 8/25 | Loss: 0.00273250
Iteration 9/25 | Loss: 0.00273250
Iteration 10/25 | Loss: 0.00273250
Iteration 11/25 | Loss: 0.00273250
Iteration 12/25 | Loss: 0.00273250
Iteration 13/25 | Loss: 0.00273250
Iteration 14/25 | Loss: 0.00273250
Iteration 15/25 | Loss: 0.00273250
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00273249763995409, 0.00273249763995409, 0.00273249763995409, 0.00273249763995409, 0.00273249763995409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00273249763995409

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273250
Iteration 2/1000 | Loss: 0.00005280
Iteration 3/1000 | Loss: 0.00004091
Iteration 4/1000 | Loss: 0.00003715
Iteration 5/1000 | Loss: 0.00003516
Iteration 6/1000 | Loss: 0.00003412
Iteration 7/1000 | Loss: 0.00003341
Iteration 8/1000 | Loss: 0.00003301
Iteration 9/1000 | Loss: 0.00003279
Iteration 10/1000 | Loss: 0.00003261
Iteration 11/1000 | Loss: 0.00003240
Iteration 12/1000 | Loss: 0.00003231
Iteration 13/1000 | Loss: 0.00003221
Iteration 14/1000 | Loss: 0.00003221
Iteration 15/1000 | Loss: 0.00003220
Iteration 16/1000 | Loss: 0.00003217
Iteration 17/1000 | Loss: 0.00003215
Iteration 18/1000 | Loss: 0.00003213
Iteration 19/1000 | Loss: 0.00003212
Iteration 20/1000 | Loss: 0.00003211
Iteration 21/1000 | Loss: 0.00003211
Iteration 22/1000 | Loss: 0.00003211
Iteration 23/1000 | Loss: 0.00003211
Iteration 24/1000 | Loss: 0.00003210
Iteration 25/1000 | Loss: 0.00003210
Iteration 26/1000 | Loss: 0.00003210
Iteration 27/1000 | Loss: 0.00003210
Iteration 28/1000 | Loss: 0.00003210
Iteration 29/1000 | Loss: 0.00003210
Iteration 30/1000 | Loss: 0.00003210
Iteration 31/1000 | Loss: 0.00003210
Iteration 32/1000 | Loss: 0.00003210
Iteration 33/1000 | Loss: 0.00003209
Iteration 34/1000 | Loss: 0.00003209
Iteration 35/1000 | Loss: 0.00003209
Iteration 36/1000 | Loss: 0.00003209
Iteration 37/1000 | Loss: 0.00003209
Iteration 38/1000 | Loss: 0.00003209
Iteration 39/1000 | Loss: 0.00003208
Iteration 40/1000 | Loss: 0.00003207
Iteration 41/1000 | Loss: 0.00003207
Iteration 42/1000 | Loss: 0.00003207
Iteration 43/1000 | Loss: 0.00003206
Iteration 44/1000 | Loss: 0.00003206
Iteration 45/1000 | Loss: 0.00003206
Iteration 46/1000 | Loss: 0.00003206
Iteration 47/1000 | Loss: 0.00003206
Iteration 48/1000 | Loss: 0.00003205
Iteration 49/1000 | Loss: 0.00003205
Iteration 50/1000 | Loss: 0.00003205
Iteration 51/1000 | Loss: 0.00003205
Iteration 52/1000 | Loss: 0.00003205
Iteration 53/1000 | Loss: 0.00003205
Iteration 54/1000 | Loss: 0.00003205
Iteration 55/1000 | Loss: 0.00003205
Iteration 56/1000 | Loss: 0.00003205
Iteration 57/1000 | Loss: 0.00003204
Iteration 58/1000 | Loss: 0.00003204
Iteration 59/1000 | Loss: 0.00003204
Iteration 60/1000 | Loss: 0.00003204
Iteration 61/1000 | Loss: 0.00003204
Iteration 62/1000 | Loss: 0.00003204
Iteration 63/1000 | Loss: 0.00003204
Iteration 64/1000 | Loss: 0.00003204
Iteration 65/1000 | Loss: 0.00003204
Iteration 66/1000 | Loss: 0.00003204
Iteration 67/1000 | Loss: 0.00003204
Iteration 68/1000 | Loss: 0.00003204
Iteration 69/1000 | Loss: 0.00003203
Iteration 70/1000 | Loss: 0.00003203
Iteration 71/1000 | Loss: 0.00003203
Iteration 72/1000 | Loss: 0.00003203
Iteration 73/1000 | Loss: 0.00003203
Iteration 74/1000 | Loss: 0.00003203
Iteration 75/1000 | Loss: 0.00003202
Iteration 76/1000 | Loss: 0.00003202
Iteration 77/1000 | Loss: 0.00003202
Iteration 78/1000 | Loss: 0.00003202
Iteration 79/1000 | Loss: 0.00003202
Iteration 80/1000 | Loss: 0.00003202
Iteration 81/1000 | Loss: 0.00003202
Iteration 82/1000 | Loss: 0.00003202
Iteration 83/1000 | Loss: 0.00003202
Iteration 84/1000 | Loss: 0.00003202
Iteration 85/1000 | Loss: 0.00003202
Iteration 86/1000 | Loss: 0.00003202
Iteration 87/1000 | Loss: 0.00003202
Iteration 88/1000 | Loss: 0.00003202
Iteration 89/1000 | Loss: 0.00003201
Iteration 90/1000 | Loss: 0.00003201
Iteration 91/1000 | Loss: 0.00003201
Iteration 92/1000 | Loss: 0.00003201
Iteration 93/1000 | Loss: 0.00003201
Iteration 94/1000 | Loss: 0.00003201
Iteration 95/1000 | Loss: 0.00003201
Iteration 96/1000 | Loss: 0.00003201
Iteration 97/1000 | Loss: 0.00003201
Iteration 98/1000 | Loss: 0.00003201
Iteration 99/1000 | Loss: 0.00003201
Iteration 100/1000 | Loss: 0.00003201
Iteration 101/1000 | Loss: 0.00003201
Iteration 102/1000 | Loss: 0.00003201
Iteration 103/1000 | Loss: 0.00003201
Iteration 104/1000 | Loss: 0.00003201
Iteration 105/1000 | Loss: 0.00003200
Iteration 106/1000 | Loss: 0.00003200
Iteration 107/1000 | Loss: 0.00003200
Iteration 108/1000 | Loss: 0.00003200
Iteration 109/1000 | Loss: 0.00003200
Iteration 110/1000 | Loss: 0.00003200
Iteration 111/1000 | Loss: 0.00003200
Iteration 112/1000 | Loss: 0.00003200
Iteration 113/1000 | Loss: 0.00003200
Iteration 114/1000 | Loss: 0.00003200
Iteration 115/1000 | Loss: 0.00003200
Iteration 116/1000 | Loss: 0.00003200
Iteration 117/1000 | Loss: 0.00003200
Iteration 118/1000 | Loss: 0.00003200
Iteration 119/1000 | Loss: 0.00003200
Iteration 120/1000 | Loss: 0.00003199
Iteration 121/1000 | Loss: 0.00003199
Iteration 122/1000 | Loss: 0.00003199
Iteration 123/1000 | Loss: 0.00003199
Iteration 124/1000 | Loss: 0.00003199
Iteration 125/1000 | Loss: 0.00003199
Iteration 126/1000 | Loss: 0.00003199
Iteration 127/1000 | Loss: 0.00003199
Iteration 128/1000 | Loss: 0.00003199
Iteration 129/1000 | Loss: 0.00003199
Iteration 130/1000 | Loss: 0.00003199
Iteration 131/1000 | Loss: 0.00003199
Iteration 132/1000 | Loss: 0.00003199
Iteration 133/1000 | Loss: 0.00003199
Iteration 134/1000 | Loss: 0.00003199
Iteration 135/1000 | Loss: 0.00003199
Iteration 136/1000 | Loss: 0.00003199
Iteration 137/1000 | Loss: 0.00003199
Iteration 138/1000 | Loss: 0.00003199
Iteration 139/1000 | Loss: 0.00003199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [3.199471757397987e-05, 3.199471757397987e-05, 3.199471757397987e-05, 3.199471757397987e-05, 3.199471757397987e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.199471757397987e-05

Optimization complete. Final v2v error: 4.826907157897949 mm

Highest mean error: 5.120810508728027 mm for frame 180

Lowest mean error: 4.573114395141602 mm for frame 235

Saving results

Total time: 34.4507532119751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068136
Iteration 2/25 | Loss: 0.00438043
Iteration 3/25 | Loss: 0.00310646
Iteration 4/25 | Loss: 0.00283808
Iteration 5/25 | Loss: 0.00266113
Iteration 6/25 | Loss: 0.00268157
Iteration 7/25 | Loss: 0.00251572
Iteration 8/25 | Loss: 0.00238827
Iteration 9/25 | Loss: 0.00225662
Iteration 10/25 | Loss: 0.00225064
Iteration 11/25 | Loss: 0.00224849
Iteration 12/25 | Loss: 0.00223027
Iteration 13/25 | Loss: 0.00216245
Iteration 14/25 | Loss: 0.00213924
Iteration 15/25 | Loss: 0.00212988
Iteration 16/25 | Loss: 0.00212390
Iteration 17/25 | Loss: 0.00212167
Iteration 18/25 | Loss: 0.00212134
Iteration 19/25 | Loss: 0.00213032
Iteration 20/25 | Loss: 0.00211563
Iteration 21/25 | Loss: 0.00211359
Iteration 22/25 | Loss: 0.00211197
Iteration 23/25 | Loss: 0.00211063
Iteration 24/25 | Loss: 0.00210987
Iteration 25/25 | Loss: 0.00210997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.30278420
Iteration 2/25 | Loss: 0.00639800
Iteration 3/25 | Loss: 0.00575039
Iteration 4/25 | Loss: 0.00575039
Iteration 5/25 | Loss: 0.00575039
Iteration 6/25 | Loss: 0.00575039
Iteration 7/25 | Loss: 0.00575039
Iteration 8/25 | Loss: 0.00575039
Iteration 9/25 | Loss: 0.00575039
Iteration 10/25 | Loss: 0.00575039
Iteration 11/25 | Loss: 0.00575039
Iteration 12/25 | Loss: 0.00575039
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.005750393029302359, 0.005750393029302359, 0.005750393029302359, 0.005750393029302359, 0.005750393029302359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005750393029302359

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00575039
Iteration 2/1000 | Loss: 0.00109273
Iteration 3/1000 | Loss: 0.00277101
Iteration 4/1000 | Loss: 0.00197152
Iteration 5/1000 | Loss: 0.00168811
Iteration 6/1000 | Loss: 0.00961174
Iteration 7/1000 | Loss: 0.00155700
Iteration 8/1000 | Loss: 0.00082029
Iteration 9/1000 | Loss: 0.00163155
Iteration 10/1000 | Loss: 0.00269761
Iteration 11/1000 | Loss: 0.00209824
Iteration 12/1000 | Loss: 0.00276782
Iteration 13/1000 | Loss: 0.00153277
Iteration 14/1000 | Loss: 0.00034567
Iteration 15/1000 | Loss: 0.00099536
Iteration 16/1000 | Loss: 0.00083079
Iteration 17/1000 | Loss: 0.00063096
Iteration 18/1000 | Loss: 0.00067001
Iteration 19/1000 | Loss: 0.00028660
Iteration 20/1000 | Loss: 0.00018510
Iteration 21/1000 | Loss: 0.00242547
Iteration 22/1000 | Loss: 0.00129742
Iteration 23/1000 | Loss: 0.00273404
Iteration 24/1000 | Loss: 0.00156915
Iteration 25/1000 | Loss: 0.00161757
Iteration 26/1000 | Loss: 0.00046874
Iteration 27/1000 | Loss: 0.00195054
Iteration 28/1000 | Loss: 0.00036715
Iteration 29/1000 | Loss: 0.00033324
Iteration 30/1000 | Loss: 0.00081764
Iteration 31/1000 | Loss: 0.00115765
Iteration 32/1000 | Loss: 0.00014650
Iteration 33/1000 | Loss: 0.00030019
Iteration 34/1000 | Loss: 0.00053550
Iteration 35/1000 | Loss: 0.00049714
Iteration 36/1000 | Loss: 0.00038303
Iteration 37/1000 | Loss: 0.00054987
Iteration 38/1000 | Loss: 0.00076593
Iteration 39/1000 | Loss: 0.00033288
Iteration 40/1000 | Loss: 0.00055341
Iteration 41/1000 | Loss: 0.00027822
Iteration 42/1000 | Loss: 0.00086026
Iteration 43/1000 | Loss: 0.00037227
Iteration 44/1000 | Loss: 0.00124984
Iteration 45/1000 | Loss: 0.00082010
Iteration 46/1000 | Loss: 0.00106927
Iteration 47/1000 | Loss: 0.00077901
Iteration 48/1000 | Loss: 0.00070342
Iteration 49/1000 | Loss: 0.00037945
Iteration 50/1000 | Loss: 0.00092221
Iteration 51/1000 | Loss: 0.00171716
Iteration 52/1000 | Loss: 0.00113480
Iteration 53/1000 | Loss: 0.00076507
Iteration 54/1000 | Loss: 0.00056567
Iteration 55/1000 | Loss: 0.00086257
Iteration 56/1000 | Loss: 0.00071622
Iteration 57/1000 | Loss: 0.00110675
Iteration 58/1000 | Loss: 0.00150438
Iteration 59/1000 | Loss: 0.00114242
Iteration 60/1000 | Loss: 0.00170536
Iteration 61/1000 | Loss: 0.00081855
Iteration 62/1000 | Loss: 0.00114967
Iteration 63/1000 | Loss: 0.00086825
Iteration 64/1000 | Loss: 0.00084879
Iteration 65/1000 | Loss: 0.00133460
Iteration 66/1000 | Loss: 0.00187117
Iteration 67/1000 | Loss: 0.00156741
Iteration 68/1000 | Loss: 0.00173072
Iteration 69/1000 | Loss: 0.00146413
Iteration 70/1000 | Loss: 0.00057855
Iteration 71/1000 | Loss: 0.00110924
Iteration 72/1000 | Loss: 0.00123663
Iteration 73/1000 | Loss: 0.00120142
Iteration 74/1000 | Loss: 0.00185595
Iteration 75/1000 | Loss: 0.00205225
Iteration 76/1000 | Loss: 0.00145546
Iteration 77/1000 | Loss: 0.00124658
Iteration 78/1000 | Loss: 0.00092843
Iteration 79/1000 | Loss: 0.00132532
Iteration 80/1000 | Loss: 0.00147539
Iteration 81/1000 | Loss: 0.00135532
Iteration 82/1000 | Loss: 0.00088890
Iteration 83/1000 | Loss: 0.00106595
Iteration 84/1000 | Loss: 0.00184802
Iteration 85/1000 | Loss: 0.00108288
Iteration 86/1000 | Loss: 0.00202276
Iteration 87/1000 | Loss: 0.00115963
Iteration 88/1000 | Loss: 0.00097190
Iteration 89/1000 | Loss: 0.00087373
Iteration 90/1000 | Loss: 0.00068283
Iteration 91/1000 | Loss: 0.00038070
Iteration 92/1000 | Loss: 0.00090892
Iteration 93/1000 | Loss: 0.00098426
Iteration 94/1000 | Loss: 0.00154274
Iteration 95/1000 | Loss: 0.00119086
Iteration 96/1000 | Loss: 0.00125360
Iteration 97/1000 | Loss: 0.00105305
Iteration 98/1000 | Loss: 0.00150184
Iteration 99/1000 | Loss: 0.00175931
Iteration 100/1000 | Loss: 0.00156406
Iteration 101/1000 | Loss: 0.00147988
Iteration 102/1000 | Loss: 0.00165960
Iteration 103/1000 | Loss: 0.00107863
Iteration 104/1000 | Loss: 0.00260164
Iteration 105/1000 | Loss: 0.00211159
Iteration 106/1000 | Loss: 0.00190443
Iteration 107/1000 | Loss: 0.00218395
Iteration 108/1000 | Loss: 0.00185170
Iteration 109/1000 | Loss: 0.00217079
Iteration 110/1000 | Loss: 0.00155626
Iteration 111/1000 | Loss: 0.00155232
Iteration 112/1000 | Loss: 0.00108503
Iteration 113/1000 | Loss: 0.00151408
Iteration 114/1000 | Loss: 0.00200327
Iteration 115/1000 | Loss: 0.00125871
Iteration 116/1000 | Loss: 0.00179939
Iteration 117/1000 | Loss: 0.00151638
Iteration 118/1000 | Loss: 0.00156457
Iteration 119/1000 | Loss: 0.00105244
Iteration 120/1000 | Loss: 0.00115331
Iteration 121/1000 | Loss: 0.00140176
Iteration 122/1000 | Loss: 0.00126209
Iteration 123/1000 | Loss: 0.00099918
Iteration 124/1000 | Loss: 0.00103220
Iteration 125/1000 | Loss: 0.00128395
Iteration 126/1000 | Loss: 0.00076239
Iteration 127/1000 | Loss: 0.00084958
Iteration 128/1000 | Loss: 0.00051661
Iteration 129/1000 | Loss: 0.00139898
Iteration 130/1000 | Loss: 0.00109226
Iteration 131/1000 | Loss: 0.00118090
Iteration 132/1000 | Loss: 0.00121437
Iteration 133/1000 | Loss: 0.00037937
Iteration 134/1000 | Loss: 0.00089004
Iteration 135/1000 | Loss: 0.00076590
Iteration 136/1000 | Loss: 0.00028852
Iteration 137/1000 | Loss: 0.00033893
Iteration 138/1000 | Loss: 0.00057703
Iteration 139/1000 | Loss: 0.00059752
Iteration 140/1000 | Loss: 0.00096855
Iteration 141/1000 | Loss: 0.00039476
Iteration 142/1000 | Loss: 0.00036982
Iteration 143/1000 | Loss: 0.00055745
Iteration 144/1000 | Loss: 0.00060904
Iteration 145/1000 | Loss: 0.00059662
Iteration 146/1000 | Loss: 0.00041748
Iteration 147/1000 | Loss: 0.00047202
Iteration 148/1000 | Loss: 0.00042076
Iteration 149/1000 | Loss: 0.00025707
Iteration 150/1000 | Loss: 0.00064687
Iteration 151/1000 | Loss: 0.00045304
Iteration 152/1000 | Loss: 0.00085398
Iteration 153/1000 | Loss: 0.00093426
Iteration 154/1000 | Loss: 0.00059791
Iteration 155/1000 | Loss: 0.00021182
Iteration 156/1000 | Loss: 0.00052132
Iteration 157/1000 | Loss: 0.00014641
Iteration 158/1000 | Loss: 0.00032875
Iteration 159/1000 | Loss: 0.00034724
Iteration 160/1000 | Loss: 0.00071535
Iteration 161/1000 | Loss: 0.00065605
Iteration 162/1000 | Loss: 0.00061000
Iteration 163/1000 | Loss: 0.00047621
Iteration 164/1000 | Loss: 0.00035113
Iteration 165/1000 | Loss: 0.00024654
Iteration 166/1000 | Loss: 0.00039090
Iteration 167/1000 | Loss: 0.00064835
Iteration 168/1000 | Loss: 0.00070135
Iteration 169/1000 | Loss: 0.00039785
Iteration 170/1000 | Loss: 0.00043357
Iteration 171/1000 | Loss: 0.00055642
Iteration 172/1000 | Loss: 0.00008514
Iteration 173/1000 | Loss: 0.00034494
Iteration 174/1000 | Loss: 0.00012699
Iteration 175/1000 | Loss: 0.00023798
Iteration 176/1000 | Loss: 0.00020339
Iteration 177/1000 | Loss: 0.00011954
Iteration 178/1000 | Loss: 0.00007899
Iteration 179/1000 | Loss: 0.00026518
Iteration 180/1000 | Loss: 0.00024081
Iteration 181/1000 | Loss: 0.00005650
Iteration 182/1000 | Loss: 0.00035766
Iteration 183/1000 | Loss: 0.00009171
Iteration 184/1000 | Loss: 0.00094748
Iteration 185/1000 | Loss: 0.00025168
Iteration 186/1000 | Loss: 0.00025967
Iteration 187/1000 | Loss: 0.00010898
Iteration 188/1000 | Loss: 0.00030460
Iteration 189/1000 | Loss: 0.00018311
Iteration 190/1000 | Loss: 0.00034368
Iteration 191/1000 | Loss: 0.00057052
Iteration 192/1000 | Loss: 0.00041520
Iteration 193/1000 | Loss: 0.00022818
Iteration 194/1000 | Loss: 0.00009718
Iteration 195/1000 | Loss: 0.00021135
Iteration 196/1000 | Loss: 0.00025106
Iteration 197/1000 | Loss: 0.00036254
Iteration 198/1000 | Loss: 0.00037788
Iteration 199/1000 | Loss: 0.00032520
Iteration 200/1000 | Loss: 0.00009095
Iteration 201/1000 | Loss: 0.00005775
Iteration 202/1000 | Loss: 0.00015065
Iteration 203/1000 | Loss: 0.00026321
Iteration 204/1000 | Loss: 0.00017560
Iteration 205/1000 | Loss: 0.00029402
Iteration 206/1000 | Loss: 0.00035524
Iteration 207/1000 | Loss: 0.00009357
Iteration 208/1000 | Loss: 0.00005060
Iteration 209/1000 | Loss: 0.00004935
Iteration 210/1000 | Loss: 0.00007990
Iteration 211/1000 | Loss: 0.00059034
Iteration 212/1000 | Loss: 0.00034197
Iteration 213/1000 | Loss: 0.00009413
Iteration 214/1000 | Loss: 0.00031485
Iteration 215/1000 | Loss: 0.00029756
Iteration 216/1000 | Loss: 0.00061438
Iteration 217/1000 | Loss: 0.00027923
Iteration 218/1000 | Loss: 0.00213240
Iteration 219/1000 | Loss: 0.00091608
Iteration 220/1000 | Loss: 0.00030261
Iteration 221/1000 | Loss: 0.00024672
Iteration 222/1000 | Loss: 0.00065295
Iteration 223/1000 | Loss: 0.00032977
Iteration 224/1000 | Loss: 0.00048181
Iteration 225/1000 | Loss: 0.00023684
Iteration 226/1000 | Loss: 0.00019748
Iteration 227/1000 | Loss: 0.00023791
Iteration 228/1000 | Loss: 0.00016131
Iteration 229/1000 | Loss: 0.00019169
Iteration 230/1000 | Loss: 0.00038756
Iteration 231/1000 | Loss: 0.00005767
Iteration 232/1000 | Loss: 0.00006957
Iteration 233/1000 | Loss: 0.00004959
Iteration 234/1000 | Loss: 0.00044949
Iteration 235/1000 | Loss: 0.00024932
Iteration 236/1000 | Loss: 0.00005971
Iteration 237/1000 | Loss: 0.00031260
Iteration 238/1000 | Loss: 0.00007931
Iteration 239/1000 | Loss: 0.00028472
Iteration 240/1000 | Loss: 0.00174900
Iteration 241/1000 | Loss: 0.00134499
Iteration 242/1000 | Loss: 0.00035626
Iteration 243/1000 | Loss: 0.00044484
Iteration 244/1000 | Loss: 0.00010002
Iteration 245/1000 | Loss: 0.00014617
Iteration 246/1000 | Loss: 0.00007977
Iteration 247/1000 | Loss: 0.00020175
Iteration 248/1000 | Loss: 0.00017905
Iteration 249/1000 | Loss: 0.00018806
Iteration 250/1000 | Loss: 0.00033274
Iteration 251/1000 | Loss: 0.00060569
Iteration 252/1000 | Loss: 0.00017285
Iteration 253/1000 | Loss: 0.00019340
Iteration 254/1000 | Loss: 0.00018370
Iteration 255/1000 | Loss: 0.00018958
Iteration 256/1000 | Loss: 0.00016887
Iteration 257/1000 | Loss: 0.00028826
Iteration 258/1000 | Loss: 0.00028453
Iteration 259/1000 | Loss: 0.00020391
Iteration 260/1000 | Loss: 0.00022127
Iteration 261/1000 | Loss: 0.00018361
Iteration 262/1000 | Loss: 0.00020652
Iteration 263/1000 | Loss: 0.00015953
Iteration 264/1000 | Loss: 0.00021037
Iteration 265/1000 | Loss: 0.00045730
Iteration 266/1000 | Loss: 0.00043626
Iteration 267/1000 | Loss: 0.00037908
Iteration 268/1000 | Loss: 0.00021604
Iteration 269/1000 | Loss: 0.00016387
Iteration 270/1000 | Loss: 0.00021637
Iteration 271/1000 | Loss: 0.00017903
Iteration 272/1000 | Loss: 0.00020991
Iteration 273/1000 | Loss: 0.00018947
Iteration 274/1000 | Loss: 0.00022562
Iteration 275/1000 | Loss: 0.00018879
Iteration 276/1000 | Loss: 0.00023145
Iteration 277/1000 | Loss: 0.00018847
Iteration 278/1000 | Loss: 0.00036271
Iteration 279/1000 | Loss: 0.00007333
Iteration 280/1000 | Loss: 0.00048316
Iteration 281/1000 | Loss: 0.00022598
Iteration 282/1000 | Loss: 0.00018651
Iteration 283/1000 | Loss: 0.00030449
Iteration 284/1000 | Loss: 0.00017655
Iteration 285/1000 | Loss: 0.00008314
Iteration 286/1000 | Loss: 0.00019682
Iteration 287/1000 | Loss: 0.00015349
Iteration 288/1000 | Loss: 0.00016423
Iteration 289/1000 | Loss: 0.00004747
Iteration 290/1000 | Loss: 0.00011716
Iteration 291/1000 | Loss: 0.00012043
Iteration 292/1000 | Loss: 0.00005036
Iteration 293/1000 | Loss: 0.00015376
Iteration 294/1000 | Loss: 0.00015537
Iteration 295/1000 | Loss: 0.00011350
Iteration 296/1000 | Loss: 0.00005202
Iteration 297/1000 | Loss: 0.00004134
Iteration 298/1000 | Loss: 0.00005192
Iteration 299/1000 | Loss: 0.00004188
Iteration 300/1000 | Loss: 0.00008848
Iteration 301/1000 | Loss: 0.00004097
Iteration 302/1000 | Loss: 0.00004997
Iteration 303/1000 | Loss: 0.00007331
Iteration 304/1000 | Loss: 0.00005166
Iteration 305/1000 | Loss: 0.00009554
Iteration 306/1000 | Loss: 0.00004127
Iteration 307/1000 | Loss: 0.00004397
Iteration 308/1000 | Loss: 0.00003908
Iteration 309/1000 | Loss: 0.00003864
Iteration 310/1000 | Loss: 0.00006080
Iteration 311/1000 | Loss: 0.00004508
Iteration 312/1000 | Loss: 0.00003970
Iteration 313/1000 | Loss: 0.00003794
Iteration 314/1000 | Loss: 0.00003776
Iteration 315/1000 | Loss: 0.00003766
Iteration 316/1000 | Loss: 0.00003765
Iteration 317/1000 | Loss: 0.00003764
Iteration 318/1000 | Loss: 0.00003764
Iteration 319/1000 | Loss: 0.00003760
Iteration 320/1000 | Loss: 0.00007255
Iteration 321/1000 | Loss: 0.00004296
Iteration 322/1000 | Loss: 0.00003758
Iteration 323/1000 | Loss: 0.00006821
Iteration 324/1000 | Loss: 0.00003763
Iteration 325/1000 | Loss: 0.00003750
Iteration 326/1000 | Loss: 0.00003749
Iteration 327/1000 | Loss: 0.00003748
Iteration 328/1000 | Loss: 0.00003745
Iteration 329/1000 | Loss: 0.00003744
Iteration 330/1000 | Loss: 0.00003743
Iteration 331/1000 | Loss: 0.00003743
Iteration 332/1000 | Loss: 0.00003742
Iteration 333/1000 | Loss: 0.00003742
Iteration 334/1000 | Loss: 0.00003742
Iteration 335/1000 | Loss: 0.00003742
Iteration 336/1000 | Loss: 0.00003741
Iteration 337/1000 | Loss: 0.00003741
Iteration 338/1000 | Loss: 0.00003741
Iteration 339/1000 | Loss: 0.00003740
Iteration 340/1000 | Loss: 0.00003736
Iteration 341/1000 | Loss: 0.00003736
Iteration 342/1000 | Loss: 0.00003736
Iteration 343/1000 | Loss: 0.00003736
Iteration 344/1000 | Loss: 0.00003736
Iteration 345/1000 | Loss: 0.00003736
Iteration 346/1000 | Loss: 0.00003736
Iteration 347/1000 | Loss: 0.00003736
Iteration 348/1000 | Loss: 0.00003736
Iteration 349/1000 | Loss: 0.00003736
Iteration 350/1000 | Loss: 0.00003736
Iteration 351/1000 | Loss: 0.00003736
Iteration 352/1000 | Loss: 0.00003736
Iteration 353/1000 | Loss: 0.00003735
Iteration 354/1000 | Loss: 0.00003735
Iteration 355/1000 | Loss: 0.00003735
Iteration 356/1000 | Loss: 0.00003735
Iteration 357/1000 | Loss: 0.00003735
Iteration 358/1000 | Loss: 0.00003735
Iteration 359/1000 | Loss: 0.00003735
Iteration 360/1000 | Loss: 0.00003735
Iteration 361/1000 | Loss: 0.00003735
Iteration 362/1000 | Loss: 0.00003735
Iteration 363/1000 | Loss: 0.00003735
Iteration 364/1000 | Loss: 0.00003735
Iteration 365/1000 | Loss: 0.00003735
Iteration 366/1000 | Loss: 0.00003734
Iteration 367/1000 | Loss: 0.00003734
Iteration 368/1000 | Loss: 0.00003734
Iteration 369/1000 | Loss: 0.00003734
Iteration 370/1000 | Loss: 0.00003734
Iteration 371/1000 | Loss: 0.00003734
Iteration 372/1000 | Loss: 0.00003734
Iteration 373/1000 | Loss: 0.00003734
Iteration 374/1000 | Loss: 0.00003734
Iteration 375/1000 | Loss: 0.00003734
Iteration 376/1000 | Loss: 0.00003733
Iteration 377/1000 | Loss: 0.00003733
Iteration 378/1000 | Loss: 0.00003733
Iteration 379/1000 | Loss: 0.00003733
Iteration 380/1000 | Loss: 0.00003732
Iteration 381/1000 | Loss: 0.00003732
Iteration 382/1000 | Loss: 0.00003732
Iteration 383/1000 | Loss: 0.00003732
Iteration 384/1000 | Loss: 0.00003732
Iteration 385/1000 | Loss: 0.00003732
Iteration 386/1000 | Loss: 0.00003732
Iteration 387/1000 | Loss: 0.00003732
Iteration 388/1000 | Loss: 0.00003732
Iteration 389/1000 | Loss: 0.00003732
Iteration 390/1000 | Loss: 0.00003732
Iteration 391/1000 | Loss: 0.00003732
Iteration 392/1000 | Loss: 0.00003732
Iteration 393/1000 | Loss: 0.00003732
Iteration 394/1000 | Loss: 0.00003732
Iteration 395/1000 | Loss: 0.00003732
Iteration 396/1000 | Loss: 0.00003731
Iteration 397/1000 | Loss: 0.00003731
Iteration 398/1000 | Loss: 0.00003731
Iteration 399/1000 | Loss: 0.00003731
Iteration 400/1000 | Loss: 0.00003731
Iteration 401/1000 | Loss: 0.00003731
Iteration 402/1000 | Loss: 0.00003731
Iteration 403/1000 | Loss: 0.00003731
Iteration 404/1000 | Loss: 0.00003731
Iteration 405/1000 | Loss: 0.00003731
Iteration 406/1000 | Loss: 0.00003731
Iteration 407/1000 | Loss: 0.00003731
Iteration 408/1000 | Loss: 0.00003731
Iteration 409/1000 | Loss: 0.00003731
Iteration 410/1000 | Loss: 0.00003731
Iteration 411/1000 | Loss: 0.00003731
Iteration 412/1000 | Loss: 0.00003731
Iteration 413/1000 | Loss: 0.00003731
Iteration 414/1000 | Loss: 0.00003731
Iteration 415/1000 | Loss: 0.00003731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 415. Stopping optimization.
Last 5 losses: [3.731239485205151e-05, 3.731239485205151e-05, 3.731239485205151e-05, 3.731239485205151e-05, 3.731239485205151e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.731239485205151e-05

Optimization complete. Final v2v error: 5.028526782989502 mm

Highest mean error: 12.460075378417969 mm for frame 48

Lowest mean error: 4.0490617752075195 mm for frame 149

Saving results

Total time: 510.0001611709595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00546579
Iteration 2/25 | Loss: 0.00206556
Iteration 3/25 | Loss: 0.00195948
Iteration 4/25 | Loss: 0.00194531
Iteration 5/25 | Loss: 0.00194068
Iteration 6/25 | Loss: 0.00194041
Iteration 7/25 | Loss: 0.00194041
Iteration 8/25 | Loss: 0.00194041
Iteration 9/25 | Loss: 0.00194041
Iteration 10/25 | Loss: 0.00194041
Iteration 11/25 | Loss: 0.00194041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001940413611009717, 0.001940413611009717, 0.001940413611009717, 0.001940413611009717, 0.001940413611009717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001940413611009717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24376619
Iteration 2/25 | Loss: 0.00266426
Iteration 3/25 | Loss: 0.00266424
Iteration 4/25 | Loss: 0.00266424
Iteration 5/25 | Loss: 0.00266424
Iteration 6/25 | Loss: 0.00266424
Iteration 7/25 | Loss: 0.00266424
Iteration 8/25 | Loss: 0.00266424
Iteration 9/25 | Loss: 0.00266424
Iteration 10/25 | Loss: 0.00266424
Iteration 11/25 | Loss: 0.00266424
Iteration 12/25 | Loss: 0.00266424
Iteration 13/25 | Loss: 0.00266424
Iteration 14/25 | Loss: 0.00266424
Iteration 15/25 | Loss: 0.00266424
Iteration 16/25 | Loss: 0.00266424
Iteration 17/25 | Loss: 0.00266424
Iteration 18/25 | Loss: 0.00266424
Iteration 19/25 | Loss: 0.00266424
Iteration 20/25 | Loss: 0.00266424
Iteration 21/25 | Loss: 0.00266424
Iteration 22/25 | Loss: 0.00266424
Iteration 23/25 | Loss: 0.00266424
Iteration 24/25 | Loss: 0.00266424
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0026642356533557177, 0.0026642356533557177, 0.0026642356533557177, 0.0026642356533557177, 0.0026642356533557177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026642356533557177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266424
Iteration 2/1000 | Loss: 0.00008709
Iteration 3/1000 | Loss: 0.00005415
Iteration 4/1000 | Loss: 0.00004597
Iteration 5/1000 | Loss: 0.00004256
Iteration 6/1000 | Loss: 0.00004074
Iteration 7/1000 | Loss: 0.00003931
Iteration 8/1000 | Loss: 0.00003812
Iteration 9/1000 | Loss: 0.00003726
Iteration 10/1000 | Loss: 0.00003668
Iteration 11/1000 | Loss: 0.00003625
Iteration 12/1000 | Loss: 0.00003597
Iteration 13/1000 | Loss: 0.00003573
Iteration 14/1000 | Loss: 0.00003553
Iteration 15/1000 | Loss: 0.00003542
Iteration 16/1000 | Loss: 0.00003541
Iteration 17/1000 | Loss: 0.00003534
Iteration 18/1000 | Loss: 0.00003530
Iteration 19/1000 | Loss: 0.00003530
Iteration 20/1000 | Loss: 0.00003529
Iteration 21/1000 | Loss: 0.00003529
Iteration 22/1000 | Loss: 0.00003527
Iteration 23/1000 | Loss: 0.00003519
Iteration 24/1000 | Loss: 0.00003518
Iteration 25/1000 | Loss: 0.00003517
Iteration 26/1000 | Loss: 0.00003517
Iteration 27/1000 | Loss: 0.00003517
Iteration 28/1000 | Loss: 0.00003517
Iteration 29/1000 | Loss: 0.00003517
Iteration 30/1000 | Loss: 0.00003517
Iteration 31/1000 | Loss: 0.00003517
Iteration 32/1000 | Loss: 0.00003517
Iteration 33/1000 | Loss: 0.00003517
Iteration 34/1000 | Loss: 0.00003517
Iteration 35/1000 | Loss: 0.00003516
Iteration 36/1000 | Loss: 0.00003516
Iteration 37/1000 | Loss: 0.00003516
Iteration 38/1000 | Loss: 0.00003516
Iteration 39/1000 | Loss: 0.00003516
Iteration 40/1000 | Loss: 0.00003516
Iteration 41/1000 | Loss: 0.00003516
Iteration 42/1000 | Loss: 0.00003516
Iteration 43/1000 | Loss: 0.00003516
Iteration 44/1000 | Loss: 0.00003515
Iteration 45/1000 | Loss: 0.00003515
Iteration 46/1000 | Loss: 0.00003514
Iteration 47/1000 | Loss: 0.00003514
Iteration 48/1000 | Loss: 0.00003514
Iteration 49/1000 | Loss: 0.00003514
Iteration 50/1000 | Loss: 0.00003514
Iteration 51/1000 | Loss: 0.00003513
Iteration 52/1000 | Loss: 0.00003513
Iteration 53/1000 | Loss: 0.00003513
Iteration 54/1000 | Loss: 0.00003513
Iteration 55/1000 | Loss: 0.00003513
Iteration 56/1000 | Loss: 0.00003513
Iteration 57/1000 | Loss: 0.00003513
Iteration 58/1000 | Loss: 0.00003513
Iteration 59/1000 | Loss: 0.00003513
Iteration 60/1000 | Loss: 0.00003513
Iteration 61/1000 | Loss: 0.00003513
Iteration 62/1000 | Loss: 0.00003512
Iteration 63/1000 | Loss: 0.00003512
Iteration 64/1000 | Loss: 0.00003512
Iteration 65/1000 | Loss: 0.00003510
Iteration 66/1000 | Loss: 0.00003510
Iteration 67/1000 | Loss: 0.00003510
Iteration 68/1000 | Loss: 0.00003509
Iteration 69/1000 | Loss: 0.00003509
Iteration 70/1000 | Loss: 0.00003509
Iteration 71/1000 | Loss: 0.00003509
Iteration 72/1000 | Loss: 0.00003509
Iteration 73/1000 | Loss: 0.00003509
Iteration 74/1000 | Loss: 0.00003509
Iteration 75/1000 | Loss: 0.00003508
Iteration 76/1000 | Loss: 0.00003508
Iteration 77/1000 | Loss: 0.00003508
Iteration 78/1000 | Loss: 0.00003507
Iteration 79/1000 | Loss: 0.00003507
Iteration 80/1000 | Loss: 0.00003507
Iteration 81/1000 | Loss: 0.00003507
Iteration 82/1000 | Loss: 0.00003507
Iteration 83/1000 | Loss: 0.00003507
Iteration 84/1000 | Loss: 0.00003507
Iteration 85/1000 | Loss: 0.00003507
Iteration 86/1000 | Loss: 0.00003507
Iteration 87/1000 | Loss: 0.00003506
Iteration 88/1000 | Loss: 0.00003506
Iteration 89/1000 | Loss: 0.00003506
Iteration 90/1000 | Loss: 0.00003506
Iteration 91/1000 | Loss: 0.00003506
Iteration 92/1000 | Loss: 0.00003505
Iteration 93/1000 | Loss: 0.00003505
Iteration 94/1000 | Loss: 0.00003505
Iteration 95/1000 | Loss: 0.00003505
Iteration 96/1000 | Loss: 0.00003504
Iteration 97/1000 | Loss: 0.00003504
Iteration 98/1000 | Loss: 0.00003504
Iteration 99/1000 | Loss: 0.00003504
Iteration 100/1000 | Loss: 0.00003504
Iteration 101/1000 | Loss: 0.00003504
Iteration 102/1000 | Loss: 0.00003504
Iteration 103/1000 | Loss: 0.00003504
Iteration 104/1000 | Loss: 0.00003504
Iteration 105/1000 | Loss: 0.00003504
Iteration 106/1000 | Loss: 0.00003503
Iteration 107/1000 | Loss: 0.00003503
Iteration 108/1000 | Loss: 0.00003503
Iteration 109/1000 | Loss: 0.00003503
Iteration 110/1000 | Loss: 0.00003503
Iteration 111/1000 | Loss: 0.00003502
Iteration 112/1000 | Loss: 0.00003502
Iteration 113/1000 | Loss: 0.00003502
Iteration 114/1000 | Loss: 0.00003502
Iteration 115/1000 | Loss: 0.00003502
Iteration 116/1000 | Loss: 0.00003501
Iteration 117/1000 | Loss: 0.00003501
Iteration 118/1000 | Loss: 0.00003501
Iteration 119/1000 | Loss: 0.00003501
Iteration 120/1000 | Loss: 0.00003501
Iteration 121/1000 | Loss: 0.00003501
Iteration 122/1000 | Loss: 0.00003501
Iteration 123/1000 | Loss: 0.00003501
Iteration 124/1000 | Loss: 0.00003501
Iteration 125/1000 | Loss: 0.00003501
Iteration 126/1000 | Loss: 0.00003501
Iteration 127/1000 | Loss: 0.00003501
Iteration 128/1000 | Loss: 0.00003500
Iteration 129/1000 | Loss: 0.00003500
Iteration 130/1000 | Loss: 0.00003500
Iteration 131/1000 | Loss: 0.00003500
Iteration 132/1000 | Loss: 0.00003500
Iteration 133/1000 | Loss: 0.00003500
Iteration 134/1000 | Loss: 0.00003500
Iteration 135/1000 | Loss: 0.00003500
Iteration 136/1000 | Loss: 0.00003500
Iteration 137/1000 | Loss: 0.00003500
Iteration 138/1000 | Loss: 0.00003500
Iteration 139/1000 | Loss: 0.00003500
Iteration 140/1000 | Loss: 0.00003499
Iteration 141/1000 | Loss: 0.00003499
Iteration 142/1000 | Loss: 0.00003499
Iteration 143/1000 | Loss: 0.00003499
Iteration 144/1000 | Loss: 0.00003499
Iteration 145/1000 | Loss: 0.00003499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [3.499450031085871e-05, 3.499450031085871e-05, 3.499450031085871e-05, 3.499450031085871e-05, 3.499450031085871e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.499450031085871e-05

Optimization complete. Final v2v error: 5.045568466186523 mm

Highest mean error: 6.285345554351807 mm for frame 214

Lowest mean error: 4.495864391326904 mm for frame 190

Saving results

Total time: 45.09951949119568
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00731348
Iteration 2/25 | Loss: 0.00206244
Iteration 3/25 | Loss: 0.00193246
Iteration 4/25 | Loss: 0.00192102
Iteration 5/25 | Loss: 0.00191703
Iteration 6/25 | Loss: 0.00191658
Iteration 7/25 | Loss: 0.00191658
Iteration 8/25 | Loss: 0.00191658
Iteration 9/25 | Loss: 0.00191658
Iteration 10/25 | Loss: 0.00191658
Iteration 11/25 | Loss: 0.00191658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001916584325954318, 0.001916584325954318, 0.001916584325954318, 0.001916584325954318, 0.001916584325954318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001916584325954318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.97407985
Iteration 2/25 | Loss: 0.00212412
Iteration 3/25 | Loss: 0.00212412
Iteration 4/25 | Loss: 0.00212412
Iteration 5/25 | Loss: 0.00212412
Iteration 6/25 | Loss: 0.00212412
Iteration 7/25 | Loss: 0.00212412
Iteration 8/25 | Loss: 0.00212412
Iteration 9/25 | Loss: 0.00212412
Iteration 10/25 | Loss: 0.00212412
Iteration 11/25 | Loss: 0.00212412
Iteration 12/25 | Loss: 0.00212412
Iteration 13/25 | Loss: 0.00212412
Iteration 14/25 | Loss: 0.00212412
Iteration 15/25 | Loss: 0.00212412
Iteration 16/25 | Loss: 0.00212412
Iteration 17/25 | Loss: 0.00212412
Iteration 18/25 | Loss: 0.00212412
Iteration 19/25 | Loss: 0.00212412
Iteration 20/25 | Loss: 0.00212412
Iteration 21/25 | Loss: 0.00212412
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0021241174545139074, 0.0021241174545139074, 0.0021241174545139074, 0.0021241174545139074, 0.0021241174545139074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021241174545139074

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212412
Iteration 2/1000 | Loss: 0.00008063
Iteration 3/1000 | Loss: 0.00004588
Iteration 4/1000 | Loss: 0.00004075
Iteration 5/1000 | Loss: 0.00003842
Iteration 6/1000 | Loss: 0.00003755
Iteration 7/1000 | Loss: 0.00003692
Iteration 8/1000 | Loss: 0.00003645
Iteration 9/1000 | Loss: 0.00003611
Iteration 10/1000 | Loss: 0.00003585
Iteration 11/1000 | Loss: 0.00003560
Iteration 12/1000 | Loss: 0.00003540
Iteration 13/1000 | Loss: 0.00003527
Iteration 14/1000 | Loss: 0.00003523
Iteration 15/1000 | Loss: 0.00003522
Iteration 16/1000 | Loss: 0.00003519
Iteration 17/1000 | Loss: 0.00003517
Iteration 18/1000 | Loss: 0.00003517
Iteration 19/1000 | Loss: 0.00003516
Iteration 20/1000 | Loss: 0.00003515
Iteration 21/1000 | Loss: 0.00003511
Iteration 22/1000 | Loss: 0.00003508
Iteration 23/1000 | Loss: 0.00003508
Iteration 24/1000 | Loss: 0.00003507
Iteration 25/1000 | Loss: 0.00003506
Iteration 26/1000 | Loss: 0.00003504
Iteration 27/1000 | Loss: 0.00003504
Iteration 28/1000 | Loss: 0.00003504
Iteration 29/1000 | Loss: 0.00003503
Iteration 30/1000 | Loss: 0.00003503
Iteration 31/1000 | Loss: 0.00003502
Iteration 32/1000 | Loss: 0.00003502
Iteration 33/1000 | Loss: 0.00003501
Iteration 34/1000 | Loss: 0.00003501
Iteration 35/1000 | Loss: 0.00003501
Iteration 36/1000 | Loss: 0.00003500
Iteration 37/1000 | Loss: 0.00003500
Iteration 38/1000 | Loss: 0.00003500
Iteration 39/1000 | Loss: 0.00003500
Iteration 40/1000 | Loss: 0.00003500
Iteration 41/1000 | Loss: 0.00003499
Iteration 42/1000 | Loss: 0.00003499
Iteration 43/1000 | Loss: 0.00003498
Iteration 44/1000 | Loss: 0.00003497
Iteration 45/1000 | Loss: 0.00003497
Iteration 46/1000 | Loss: 0.00003497
Iteration 47/1000 | Loss: 0.00003497
Iteration 48/1000 | Loss: 0.00003497
Iteration 49/1000 | Loss: 0.00003497
Iteration 50/1000 | Loss: 0.00003497
Iteration 51/1000 | Loss: 0.00003497
Iteration 52/1000 | Loss: 0.00003497
Iteration 53/1000 | Loss: 0.00003497
Iteration 54/1000 | Loss: 0.00003497
Iteration 55/1000 | Loss: 0.00003497
Iteration 56/1000 | Loss: 0.00003497
Iteration 57/1000 | Loss: 0.00003497
Iteration 58/1000 | Loss: 0.00003497
Iteration 59/1000 | Loss: 0.00003497
Iteration 60/1000 | Loss: 0.00003497
Iteration 61/1000 | Loss: 0.00003497
Iteration 62/1000 | Loss: 0.00003497
Iteration 63/1000 | Loss: 0.00003497
Iteration 64/1000 | Loss: 0.00003497
Iteration 65/1000 | Loss: 0.00003497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [3.497098805382848e-05, 3.497098805382848e-05, 3.497098805382848e-05, 3.497098805382848e-05, 3.497098805382848e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.497098805382848e-05

Optimization complete. Final v2v error: 4.981527328491211 mm

Highest mean error: 5.458808422088623 mm for frame 164

Lowest mean error: 4.5969953536987305 mm for frame 18

Saving results

Total time: 32.46349310874939
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468566
Iteration 2/25 | Loss: 0.00197808
Iteration 3/25 | Loss: 0.00190281
Iteration 4/25 | Loss: 0.00189255
Iteration 5/25 | Loss: 0.00188863
Iteration 6/25 | Loss: 0.00188698
Iteration 7/25 | Loss: 0.00188698
Iteration 8/25 | Loss: 0.00188698
Iteration 9/25 | Loss: 0.00188698
Iteration 10/25 | Loss: 0.00188698
Iteration 11/25 | Loss: 0.00188698
Iteration 12/25 | Loss: 0.00188698
Iteration 13/25 | Loss: 0.00188698
Iteration 14/25 | Loss: 0.00188698
Iteration 15/25 | Loss: 0.00188698
Iteration 16/25 | Loss: 0.00188698
Iteration 17/25 | Loss: 0.00188698
Iteration 18/25 | Loss: 0.00188698
Iteration 19/25 | Loss: 0.00188698
Iteration 20/25 | Loss: 0.00188698
Iteration 21/25 | Loss: 0.00188698
Iteration 22/25 | Loss: 0.00188698
Iteration 23/25 | Loss: 0.00188698
Iteration 24/25 | Loss: 0.00188698
Iteration 25/25 | Loss: 0.00188698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.16009998
Iteration 2/25 | Loss: 0.00292919
Iteration 3/25 | Loss: 0.00292919
Iteration 4/25 | Loss: 0.00292919
Iteration 5/25 | Loss: 0.00292919
Iteration 6/25 | Loss: 0.00292919
Iteration 7/25 | Loss: 0.00292919
Iteration 8/25 | Loss: 0.00292919
Iteration 9/25 | Loss: 0.00292919
Iteration 10/25 | Loss: 0.00292919
Iteration 11/25 | Loss: 0.00292919
Iteration 12/25 | Loss: 0.00292919
Iteration 13/25 | Loss: 0.00292919
Iteration 14/25 | Loss: 0.00292919
Iteration 15/25 | Loss: 0.00292919
Iteration 16/25 | Loss: 0.00292919
Iteration 17/25 | Loss: 0.00292919
Iteration 18/25 | Loss: 0.00292919
Iteration 19/25 | Loss: 0.00292919
Iteration 20/25 | Loss: 0.00292919
Iteration 21/25 | Loss: 0.00292919
Iteration 22/25 | Loss: 0.00292919
Iteration 23/25 | Loss: 0.00292919
Iteration 24/25 | Loss: 0.00292919
Iteration 25/25 | Loss: 0.00292919

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00292919
Iteration 2/1000 | Loss: 0.00007629
Iteration 3/1000 | Loss: 0.00004342
Iteration 4/1000 | Loss: 0.00003721
Iteration 5/1000 | Loss: 0.00003484
Iteration 6/1000 | Loss: 0.00003353
Iteration 7/1000 | Loss: 0.00003278
Iteration 8/1000 | Loss: 0.00003236
Iteration 9/1000 | Loss: 0.00003195
Iteration 10/1000 | Loss: 0.00003161
Iteration 11/1000 | Loss: 0.00003131
Iteration 12/1000 | Loss: 0.00003109
Iteration 13/1000 | Loss: 0.00003101
Iteration 14/1000 | Loss: 0.00003087
Iteration 15/1000 | Loss: 0.00003082
Iteration 16/1000 | Loss: 0.00003071
Iteration 17/1000 | Loss: 0.00003068
Iteration 18/1000 | Loss: 0.00003068
Iteration 19/1000 | Loss: 0.00003068
Iteration 20/1000 | Loss: 0.00003067
Iteration 21/1000 | Loss: 0.00003067
Iteration 22/1000 | Loss: 0.00003063
Iteration 23/1000 | Loss: 0.00003063
Iteration 24/1000 | Loss: 0.00003063
Iteration 25/1000 | Loss: 0.00003062
Iteration 26/1000 | Loss: 0.00003062
Iteration 27/1000 | Loss: 0.00003062
Iteration 28/1000 | Loss: 0.00003060
Iteration 29/1000 | Loss: 0.00003059
Iteration 30/1000 | Loss: 0.00003059
Iteration 31/1000 | Loss: 0.00003058
Iteration 32/1000 | Loss: 0.00003058
Iteration 33/1000 | Loss: 0.00003058
Iteration 34/1000 | Loss: 0.00003058
Iteration 35/1000 | Loss: 0.00003057
Iteration 36/1000 | Loss: 0.00003057
Iteration 37/1000 | Loss: 0.00003057
Iteration 38/1000 | Loss: 0.00003057
Iteration 39/1000 | Loss: 0.00003057
Iteration 40/1000 | Loss: 0.00003056
Iteration 41/1000 | Loss: 0.00003056
Iteration 42/1000 | Loss: 0.00003056
Iteration 43/1000 | Loss: 0.00003055
Iteration 44/1000 | Loss: 0.00003055
Iteration 45/1000 | Loss: 0.00003055
Iteration 46/1000 | Loss: 0.00003055
Iteration 47/1000 | Loss: 0.00003055
Iteration 48/1000 | Loss: 0.00003055
Iteration 49/1000 | Loss: 0.00003055
Iteration 50/1000 | Loss: 0.00003055
Iteration 51/1000 | Loss: 0.00003055
Iteration 52/1000 | Loss: 0.00003055
Iteration 53/1000 | Loss: 0.00003054
Iteration 54/1000 | Loss: 0.00003054
Iteration 55/1000 | Loss: 0.00003054
Iteration 56/1000 | Loss: 0.00003054
Iteration 57/1000 | Loss: 0.00003054
Iteration 58/1000 | Loss: 0.00003053
Iteration 59/1000 | Loss: 0.00003053
Iteration 60/1000 | Loss: 0.00003053
Iteration 61/1000 | Loss: 0.00003053
Iteration 62/1000 | Loss: 0.00003053
Iteration 63/1000 | Loss: 0.00003053
Iteration 64/1000 | Loss: 0.00003053
Iteration 65/1000 | Loss: 0.00003053
Iteration 66/1000 | Loss: 0.00003053
Iteration 67/1000 | Loss: 0.00003053
Iteration 68/1000 | Loss: 0.00003053
Iteration 69/1000 | Loss: 0.00003053
Iteration 70/1000 | Loss: 0.00003052
Iteration 71/1000 | Loss: 0.00003052
Iteration 72/1000 | Loss: 0.00003052
Iteration 73/1000 | Loss: 0.00003052
Iteration 74/1000 | Loss: 0.00003052
Iteration 75/1000 | Loss: 0.00003052
Iteration 76/1000 | Loss: 0.00003052
Iteration 77/1000 | Loss: 0.00003052
Iteration 78/1000 | Loss: 0.00003052
Iteration 79/1000 | Loss: 0.00003052
Iteration 80/1000 | Loss: 0.00003051
Iteration 81/1000 | Loss: 0.00003051
Iteration 82/1000 | Loss: 0.00003051
Iteration 83/1000 | Loss: 0.00003051
Iteration 84/1000 | Loss: 0.00003051
Iteration 85/1000 | Loss: 0.00003051
Iteration 86/1000 | Loss: 0.00003051
Iteration 87/1000 | Loss: 0.00003051
Iteration 88/1000 | Loss: 0.00003051
Iteration 89/1000 | Loss: 0.00003051
Iteration 90/1000 | Loss: 0.00003051
Iteration 91/1000 | Loss: 0.00003051
Iteration 92/1000 | Loss: 0.00003051
Iteration 93/1000 | Loss: 0.00003051
Iteration 94/1000 | Loss: 0.00003051
Iteration 95/1000 | Loss: 0.00003051
Iteration 96/1000 | Loss: 0.00003051
Iteration 97/1000 | Loss: 0.00003051
Iteration 98/1000 | Loss: 0.00003051
Iteration 99/1000 | Loss: 0.00003051
Iteration 100/1000 | Loss: 0.00003051
Iteration 101/1000 | Loss: 0.00003051
Iteration 102/1000 | Loss: 0.00003051
Iteration 103/1000 | Loss: 0.00003051
Iteration 104/1000 | Loss: 0.00003051
Iteration 105/1000 | Loss: 0.00003051
Iteration 106/1000 | Loss: 0.00003051
Iteration 107/1000 | Loss: 0.00003051
Iteration 108/1000 | Loss: 0.00003051
Iteration 109/1000 | Loss: 0.00003051
Iteration 110/1000 | Loss: 0.00003051
Iteration 111/1000 | Loss: 0.00003051
Iteration 112/1000 | Loss: 0.00003051
Iteration 113/1000 | Loss: 0.00003051
Iteration 114/1000 | Loss: 0.00003051
Iteration 115/1000 | Loss: 0.00003051
Iteration 116/1000 | Loss: 0.00003051
Iteration 117/1000 | Loss: 0.00003051
Iteration 118/1000 | Loss: 0.00003051
Iteration 119/1000 | Loss: 0.00003051
Iteration 120/1000 | Loss: 0.00003051
Iteration 121/1000 | Loss: 0.00003051
Iteration 122/1000 | Loss: 0.00003051
Iteration 123/1000 | Loss: 0.00003051
Iteration 124/1000 | Loss: 0.00003051
Iteration 125/1000 | Loss: 0.00003051
Iteration 126/1000 | Loss: 0.00003051
Iteration 127/1000 | Loss: 0.00003051
Iteration 128/1000 | Loss: 0.00003051
Iteration 129/1000 | Loss: 0.00003051
Iteration 130/1000 | Loss: 0.00003051
Iteration 131/1000 | Loss: 0.00003051
Iteration 132/1000 | Loss: 0.00003051
Iteration 133/1000 | Loss: 0.00003051
Iteration 134/1000 | Loss: 0.00003051
Iteration 135/1000 | Loss: 0.00003051
Iteration 136/1000 | Loss: 0.00003051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [3.0505922040902078e-05, 3.0505922040902078e-05, 3.0505922040902078e-05, 3.0505922040902078e-05, 3.0505922040902078e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0505922040902078e-05

Optimization complete. Final v2v error: 4.683483600616455 mm

Highest mean error: 5.260072708129883 mm for frame 91

Lowest mean error: 4.420860767364502 mm for frame 12

Saving results

Total time: 36.3183057308197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01166668
Iteration 2/25 | Loss: 0.00329774
Iteration 3/25 | Loss: 0.00244668
Iteration 4/25 | Loss: 0.00243532
Iteration 5/25 | Loss: 0.00232566
Iteration 6/25 | Loss: 0.00219539
Iteration 7/25 | Loss: 0.00201735
Iteration 8/25 | Loss: 0.00197201
Iteration 9/25 | Loss: 0.00190889
Iteration 10/25 | Loss: 0.00186865
Iteration 11/25 | Loss: 0.00184166
Iteration 12/25 | Loss: 0.00181689
Iteration 13/25 | Loss: 0.00179669
Iteration 14/25 | Loss: 0.00178307
Iteration 15/25 | Loss: 0.00178212
Iteration 16/25 | Loss: 0.00178354
Iteration 17/25 | Loss: 0.00178375
Iteration 18/25 | Loss: 0.00177476
Iteration 19/25 | Loss: 0.00177462
Iteration 20/25 | Loss: 0.00178969
Iteration 21/25 | Loss: 0.00177819
Iteration 22/25 | Loss: 0.00177730
Iteration 23/25 | Loss: 0.00177766
Iteration 24/25 | Loss: 0.00178783
Iteration 25/25 | Loss: 0.00176886

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22275460
Iteration 2/25 | Loss: 0.00447746
Iteration 3/25 | Loss: 0.00316303
Iteration 4/25 | Loss: 0.00316303
Iteration 5/25 | Loss: 0.00316303
Iteration 6/25 | Loss: 0.00316303
Iteration 7/25 | Loss: 0.00316303
Iteration 8/25 | Loss: 0.00316303
Iteration 9/25 | Loss: 0.00316303
Iteration 10/25 | Loss: 0.00316303
Iteration 11/25 | Loss: 0.00316303
Iteration 12/25 | Loss: 0.00316303
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.003163032466545701, 0.003163032466545701, 0.003163032466545701, 0.003163032466545701, 0.003163032466545701]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003163032466545701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00316303
Iteration 2/1000 | Loss: 0.00096554
Iteration 3/1000 | Loss: 0.00077080
Iteration 4/1000 | Loss: 0.00100015
Iteration 5/1000 | Loss: 0.00097725
Iteration 6/1000 | Loss: 0.00079464
Iteration 7/1000 | Loss: 0.00076243
Iteration 8/1000 | Loss: 0.00090840
Iteration 9/1000 | Loss: 0.00086294
Iteration 10/1000 | Loss: 0.00077935
Iteration 11/1000 | Loss: 0.00083670
Iteration 12/1000 | Loss: 0.00071667
Iteration 13/1000 | Loss: 0.00059321
Iteration 14/1000 | Loss: 0.00066311
Iteration 15/1000 | Loss: 0.00065678
Iteration 16/1000 | Loss: 0.00087270
Iteration 17/1000 | Loss: 0.00089239
Iteration 18/1000 | Loss: 0.00077728
Iteration 19/1000 | Loss: 0.00073053
Iteration 20/1000 | Loss: 0.00089264
Iteration 21/1000 | Loss: 0.00078345
Iteration 22/1000 | Loss: 0.00069626
Iteration 23/1000 | Loss: 0.00057355
Iteration 24/1000 | Loss: 0.00061713
Iteration 25/1000 | Loss: 0.00067061
Iteration 26/1000 | Loss: 0.00085040
Iteration 27/1000 | Loss: 0.00075864
Iteration 28/1000 | Loss: 0.00064453
Iteration 29/1000 | Loss: 0.00058549
Iteration 30/1000 | Loss: 0.00072264
Iteration 31/1000 | Loss: 0.00082819
Iteration 32/1000 | Loss: 0.00071340
Iteration 33/1000 | Loss: 0.00079963
Iteration 34/1000 | Loss: 0.00081291
Iteration 35/1000 | Loss: 0.00086649
Iteration 36/1000 | Loss: 0.00096095
Iteration 37/1000 | Loss: 0.00079667
Iteration 38/1000 | Loss: 0.00097200
Iteration 39/1000 | Loss: 0.00049534
Iteration 40/1000 | Loss: 0.00064665
Iteration 41/1000 | Loss: 0.00065789
Iteration 42/1000 | Loss: 0.00055792
Iteration 43/1000 | Loss: 0.00051184
Iteration 44/1000 | Loss: 0.00038678
Iteration 45/1000 | Loss: 0.00042336
Iteration 46/1000 | Loss: 0.00055445
Iteration 47/1000 | Loss: 0.00047035
Iteration 48/1000 | Loss: 0.00045222
Iteration 49/1000 | Loss: 0.00040148
Iteration 50/1000 | Loss: 0.00035007
Iteration 51/1000 | Loss: 0.00056483
Iteration 52/1000 | Loss: 0.00064887
Iteration 53/1000 | Loss: 0.00070028
Iteration 54/1000 | Loss: 0.00055420
Iteration 55/1000 | Loss: 0.00050789
Iteration 56/1000 | Loss: 0.00043083
Iteration 57/1000 | Loss: 0.00038028
Iteration 58/1000 | Loss: 0.00033678
Iteration 59/1000 | Loss: 0.00051272
Iteration 60/1000 | Loss: 0.00023952
Iteration 61/1000 | Loss: 0.00021837
Iteration 62/1000 | Loss: 0.00059375
Iteration 63/1000 | Loss: 0.00045740
Iteration 64/1000 | Loss: 0.00030187
Iteration 65/1000 | Loss: 0.00031028
Iteration 66/1000 | Loss: 0.00042192
Iteration 67/1000 | Loss: 0.00030397
Iteration 68/1000 | Loss: 0.00027807
Iteration 69/1000 | Loss: 0.00025147
Iteration 70/1000 | Loss: 0.00018221
Iteration 71/1000 | Loss: 0.00016434
Iteration 72/1000 | Loss: 0.00026463
Iteration 73/1000 | Loss: 0.00043803
Iteration 74/1000 | Loss: 0.00077389
Iteration 75/1000 | Loss: 0.00042810
Iteration 76/1000 | Loss: 0.00047562
Iteration 77/1000 | Loss: 0.00041373
Iteration 78/1000 | Loss: 0.00036442
Iteration 79/1000 | Loss: 0.00027635
Iteration 80/1000 | Loss: 0.00021613
Iteration 81/1000 | Loss: 0.00038995
Iteration 82/1000 | Loss: 0.00030963
Iteration 83/1000 | Loss: 0.00021327
Iteration 84/1000 | Loss: 0.00015799
Iteration 85/1000 | Loss: 0.00021200
Iteration 86/1000 | Loss: 0.00018031
Iteration 87/1000 | Loss: 0.00022541
Iteration 88/1000 | Loss: 0.00049053
Iteration 89/1000 | Loss: 0.00031232
Iteration 90/1000 | Loss: 0.00043340
Iteration 91/1000 | Loss: 0.00050507
Iteration 92/1000 | Loss: 0.00049557
Iteration 93/1000 | Loss: 0.00053662
Iteration 94/1000 | Loss: 0.00027925
Iteration 95/1000 | Loss: 0.00031828
Iteration 96/1000 | Loss: 0.00022866
Iteration 97/1000 | Loss: 0.00031763
Iteration 98/1000 | Loss: 0.00017699
Iteration 99/1000 | Loss: 0.00015780
Iteration 100/1000 | Loss: 0.00019716
Iteration 101/1000 | Loss: 0.00029193
Iteration 102/1000 | Loss: 0.00030099
Iteration 103/1000 | Loss: 0.00034502
Iteration 104/1000 | Loss: 0.00020458
Iteration 105/1000 | Loss: 0.00014029
Iteration 106/1000 | Loss: 0.00022361
Iteration 107/1000 | Loss: 0.00029909
Iteration 108/1000 | Loss: 0.00041287
Iteration 109/1000 | Loss: 0.00037454
Iteration 110/1000 | Loss: 0.00048799
Iteration 111/1000 | Loss: 0.00067655
Iteration 112/1000 | Loss: 0.00015586
Iteration 113/1000 | Loss: 0.00045648
Iteration 114/1000 | Loss: 0.00028495
Iteration 115/1000 | Loss: 0.00036428
Iteration 116/1000 | Loss: 0.00054175
Iteration 117/1000 | Loss: 0.00079824
Iteration 118/1000 | Loss: 0.00070026
Iteration 119/1000 | Loss: 0.00091050
Iteration 120/1000 | Loss: 0.00086930
Iteration 121/1000 | Loss: 0.00055719
Iteration 122/1000 | Loss: 0.00044594
Iteration 123/1000 | Loss: 0.00028793
Iteration 124/1000 | Loss: 0.00024428
Iteration 125/1000 | Loss: 0.00025459
Iteration 126/1000 | Loss: 0.00027113
Iteration 127/1000 | Loss: 0.00027710
Iteration 128/1000 | Loss: 0.00010877
Iteration 129/1000 | Loss: 0.00018799
Iteration 130/1000 | Loss: 0.00016527
Iteration 131/1000 | Loss: 0.00011955
Iteration 132/1000 | Loss: 0.00016102
Iteration 133/1000 | Loss: 0.00023800
Iteration 134/1000 | Loss: 0.00016612
Iteration 135/1000 | Loss: 0.00016884
Iteration 136/1000 | Loss: 0.00050585
Iteration 137/1000 | Loss: 0.00015909
Iteration 138/1000 | Loss: 0.00011909
Iteration 139/1000 | Loss: 0.00016398
Iteration 140/1000 | Loss: 0.00020311
Iteration 141/1000 | Loss: 0.00019434
Iteration 142/1000 | Loss: 0.00025608
Iteration 143/1000 | Loss: 0.00027416
Iteration 144/1000 | Loss: 0.00021412
Iteration 145/1000 | Loss: 0.00015008
Iteration 146/1000 | Loss: 0.00015224
Iteration 147/1000 | Loss: 0.00008575
Iteration 148/1000 | Loss: 0.00007431
Iteration 149/1000 | Loss: 0.00006418
Iteration 150/1000 | Loss: 0.00006328
Iteration 151/1000 | Loss: 0.00021299
Iteration 152/1000 | Loss: 0.00007059
Iteration 153/1000 | Loss: 0.00006165
Iteration 154/1000 | Loss: 0.00006600
Iteration 155/1000 | Loss: 0.00005913
Iteration 156/1000 | Loss: 0.00004590
Iteration 157/1000 | Loss: 0.00005334
Iteration 158/1000 | Loss: 0.00005616
Iteration 159/1000 | Loss: 0.00005582
Iteration 160/1000 | Loss: 0.00006203
Iteration 161/1000 | Loss: 0.00005056
Iteration 162/1000 | Loss: 0.00006062
Iteration 163/1000 | Loss: 0.00005828
Iteration 164/1000 | Loss: 0.00005838
Iteration 165/1000 | Loss: 0.00006122
Iteration 166/1000 | Loss: 0.00005365
Iteration 167/1000 | Loss: 0.00005737
Iteration 168/1000 | Loss: 0.00005689
Iteration 169/1000 | Loss: 0.00006321
Iteration 170/1000 | Loss: 0.00006626
Iteration 171/1000 | Loss: 0.00005532
Iteration 172/1000 | Loss: 0.00006059
Iteration 173/1000 | Loss: 0.00005474
Iteration 174/1000 | Loss: 0.00006096
Iteration 175/1000 | Loss: 0.00006223
Iteration 176/1000 | Loss: 0.00007266
Iteration 177/1000 | Loss: 0.00005724
Iteration 178/1000 | Loss: 0.00005800
Iteration 179/1000 | Loss: 0.00005754
Iteration 180/1000 | Loss: 0.00006143
Iteration 181/1000 | Loss: 0.00005813
Iteration 182/1000 | Loss: 0.00006565
Iteration 183/1000 | Loss: 0.00005849
Iteration 184/1000 | Loss: 0.00005617
Iteration 185/1000 | Loss: 0.00005690
Iteration 186/1000 | Loss: 0.00005866
Iteration 187/1000 | Loss: 0.00006003
Iteration 188/1000 | Loss: 0.00005763
Iteration 189/1000 | Loss: 0.00006151
Iteration 190/1000 | Loss: 0.00004300
Iteration 191/1000 | Loss: 0.00004582
Iteration 192/1000 | Loss: 0.00005775
Iteration 193/1000 | Loss: 0.00005560
Iteration 194/1000 | Loss: 0.00005016
Iteration 195/1000 | Loss: 0.00005637
Iteration 196/1000 | Loss: 0.00005264
Iteration 197/1000 | Loss: 0.00005242
Iteration 198/1000 | Loss: 0.00004851
Iteration 199/1000 | Loss: 0.00005566
Iteration 200/1000 | Loss: 0.00005617
Iteration 201/1000 | Loss: 0.00005494
Iteration 202/1000 | Loss: 0.00005632
Iteration 203/1000 | Loss: 0.00005162
Iteration 204/1000 | Loss: 0.00005016
Iteration 205/1000 | Loss: 0.00006182
Iteration 206/1000 | Loss: 0.00005565
Iteration 207/1000 | Loss: 0.00006057
Iteration 208/1000 | Loss: 0.00006128
Iteration 209/1000 | Loss: 0.00006270
Iteration 210/1000 | Loss: 0.00005884
Iteration 211/1000 | Loss: 0.00006211
Iteration 212/1000 | Loss: 0.00005822
Iteration 213/1000 | Loss: 0.00007026
Iteration 214/1000 | Loss: 0.00004537
Iteration 215/1000 | Loss: 0.00004020
Iteration 216/1000 | Loss: 0.00003826
Iteration 217/1000 | Loss: 0.00003686
Iteration 218/1000 | Loss: 0.00003611
Iteration 219/1000 | Loss: 0.00003549
Iteration 220/1000 | Loss: 0.00003500
Iteration 221/1000 | Loss: 0.00003478
Iteration 222/1000 | Loss: 0.00003485
Iteration 223/1000 | Loss: 0.00003499
Iteration 224/1000 | Loss: 0.00003491
Iteration 225/1000 | Loss: 0.00003478
Iteration 226/1000 | Loss: 0.00003471
Iteration 227/1000 | Loss: 0.00003471
Iteration 228/1000 | Loss: 0.00003471
Iteration 229/1000 | Loss: 0.00003471
Iteration 230/1000 | Loss: 0.00003470
Iteration 231/1000 | Loss: 0.00003470
Iteration 232/1000 | Loss: 0.00003469
Iteration 233/1000 | Loss: 0.00003468
Iteration 234/1000 | Loss: 0.00003468
Iteration 235/1000 | Loss: 0.00003816
Iteration 236/1000 | Loss: 0.00003815
Iteration 237/1000 | Loss: 0.00003609
Iteration 238/1000 | Loss: 0.00003449
Iteration 239/1000 | Loss: 0.00003448
Iteration 240/1000 | Loss: 0.00003447
Iteration 241/1000 | Loss: 0.00003447
Iteration 242/1000 | Loss: 0.00003446
Iteration 243/1000 | Loss: 0.00003444
Iteration 244/1000 | Loss: 0.00003443
Iteration 245/1000 | Loss: 0.00003443
Iteration 246/1000 | Loss: 0.00003442
Iteration 247/1000 | Loss: 0.00003439
Iteration 248/1000 | Loss: 0.00003439
Iteration 249/1000 | Loss: 0.00003439
Iteration 250/1000 | Loss: 0.00003439
Iteration 251/1000 | Loss: 0.00003439
Iteration 252/1000 | Loss: 0.00003439
Iteration 253/1000 | Loss: 0.00003439
Iteration 254/1000 | Loss: 0.00003437
Iteration 255/1000 | Loss: 0.00003437
Iteration 256/1000 | Loss: 0.00003436
Iteration 257/1000 | Loss: 0.00003436
Iteration 258/1000 | Loss: 0.00003744
Iteration 259/1000 | Loss: 0.00003743
Iteration 260/1000 | Loss: 0.00003597
Iteration 261/1000 | Loss: 0.00003435
Iteration 262/1000 | Loss: 0.00003434
Iteration 263/1000 | Loss: 0.00003434
Iteration 264/1000 | Loss: 0.00003434
Iteration 265/1000 | Loss: 0.00003434
Iteration 266/1000 | Loss: 0.00003434
Iteration 267/1000 | Loss: 0.00003434
Iteration 268/1000 | Loss: 0.00003434
Iteration 269/1000 | Loss: 0.00003434
Iteration 270/1000 | Loss: 0.00003434
Iteration 271/1000 | Loss: 0.00003434
Iteration 272/1000 | Loss: 0.00003433
Iteration 273/1000 | Loss: 0.00003433
Iteration 274/1000 | Loss: 0.00003433
Iteration 275/1000 | Loss: 0.00003433
Iteration 276/1000 | Loss: 0.00003432
Iteration 277/1000 | Loss: 0.00003432
Iteration 278/1000 | Loss: 0.00003432
Iteration 279/1000 | Loss: 0.00003432
Iteration 280/1000 | Loss: 0.00003431
Iteration 281/1000 | Loss: 0.00003431
Iteration 282/1000 | Loss: 0.00003431
Iteration 283/1000 | Loss: 0.00003431
Iteration 284/1000 | Loss: 0.00003431
Iteration 285/1000 | Loss: 0.00003431
Iteration 286/1000 | Loss: 0.00003431
Iteration 287/1000 | Loss: 0.00003430
Iteration 288/1000 | Loss: 0.00003430
Iteration 289/1000 | Loss: 0.00003430
Iteration 290/1000 | Loss: 0.00003429
Iteration 291/1000 | Loss: 0.00003429
Iteration 292/1000 | Loss: 0.00003429
Iteration 293/1000 | Loss: 0.00003429
Iteration 294/1000 | Loss: 0.00003429
Iteration 295/1000 | Loss: 0.00003429
Iteration 296/1000 | Loss: 0.00003429
Iteration 297/1000 | Loss: 0.00003429
Iteration 298/1000 | Loss: 0.00003428
Iteration 299/1000 | Loss: 0.00003428
Iteration 300/1000 | Loss: 0.00003428
Iteration 301/1000 | Loss: 0.00003428
Iteration 302/1000 | Loss: 0.00003428
Iteration 303/1000 | Loss: 0.00003428
Iteration 304/1000 | Loss: 0.00003428
Iteration 305/1000 | Loss: 0.00003428
Iteration 306/1000 | Loss: 0.00003428
Iteration 307/1000 | Loss: 0.00003428
Iteration 308/1000 | Loss: 0.00003428
Iteration 309/1000 | Loss: 0.00003428
Iteration 310/1000 | Loss: 0.00003428
Iteration 311/1000 | Loss: 0.00003427
Iteration 312/1000 | Loss: 0.00003427
Iteration 313/1000 | Loss: 0.00003427
Iteration 314/1000 | Loss: 0.00003427
Iteration 315/1000 | Loss: 0.00003739
Iteration 316/1000 | Loss: 0.00003585
Iteration 317/1000 | Loss: 0.00003427
Iteration 318/1000 | Loss: 0.00003427
Iteration 319/1000 | Loss: 0.00003722
Iteration 320/1000 | Loss: 0.00003557
Iteration 321/1000 | Loss: 0.00003427
Iteration 322/1000 | Loss: 0.00003427
Iteration 323/1000 | Loss: 0.00003427
Iteration 324/1000 | Loss: 0.00003427
Iteration 325/1000 | Loss: 0.00003427
Iteration 326/1000 | Loss: 0.00003427
Iteration 327/1000 | Loss: 0.00003426
Iteration 328/1000 | Loss: 0.00003426
Iteration 329/1000 | Loss: 0.00003426
Iteration 330/1000 | Loss: 0.00003426
Iteration 331/1000 | Loss: 0.00003426
Iteration 332/1000 | Loss: 0.00003426
Iteration 333/1000 | Loss: 0.00003426
Iteration 334/1000 | Loss: 0.00003426
Iteration 335/1000 | Loss: 0.00003426
Iteration 336/1000 | Loss: 0.00003426
Iteration 337/1000 | Loss: 0.00003426
Iteration 338/1000 | Loss: 0.00003426
Iteration 339/1000 | Loss: 0.00003426
Iteration 340/1000 | Loss: 0.00003426
Iteration 341/1000 | Loss: 0.00003426
Iteration 342/1000 | Loss: 0.00003426
Iteration 343/1000 | Loss: 0.00003426
Iteration 344/1000 | Loss: 0.00003426
Iteration 345/1000 | Loss: 0.00003426
Iteration 346/1000 | Loss: 0.00003426
Iteration 347/1000 | Loss: 0.00003426
Iteration 348/1000 | Loss: 0.00003426
Iteration 349/1000 | Loss: 0.00003426
Iteration 350/1000 | Loss: 0.00003426
Iteration 351/1000 | Loss: 0.00003426
Iteration 352/1000 | Loss: 0.00003426
Iteration 353/1000 | Loss: 0.00003426
Iteration 354/1000 | Loss: 0.00003426
Iteration 355/1000 | Loss: 0.00003426
Iteration 356/1000 | Loss: 0.00003426
Iteration 357/1000 | Loss: 0.00003426
Iteration 358/1000 | Loss: 0.00003426
Iteration 359/1000 | Loss: 0.00003426
Iteration 360/1000 | Loss: 0.00003426
Iteration 361/1000 | Loss: 0.00003426
Iteration 362/1000 | Loss: 0.00003426
Iteration 363/1000 | Loss: 0.00003426
Iteration 364/1000 | Loss: 0.00003426
Iteration 365/1000 | Loss: 0.00003426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 365. Stopping optimization.
Last 5 losses: [3.426397233852185e-05, 3.426397233852185e-05, 3.426397233852185e-05, 3.426397233852185e-05, 3.426397233852185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.426397233852185e-05

Optimization complete. Final v2v error: 4.679440498352051 mm

Highest mean error: 11.207908630371094 mm for frame 182

Lowest mean error: 4.0157694816589355 mm for frame 223

Saving results

Total time: 430.61180329322815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01214243
Iteration 2/25 | Loss: 0.00232878
Iteration 3/25 | Loss: 0.00177863
Iteration 4/25 | Loss: 0.00174310
Iteration 5/25 | Loss: 0.00173948
Iteration 6/25 | Loss: 0.00170918
Iteration 7/25 | Loss: 0.00169928
Iteration 8/25 | Loss: 0.00169227
Iteration 9/25 | Loss: 0.00169180
Iteration 10/25 | Loss: 0.00169170
Iteration 11/25 | Loss: 0.00169156
Iteration 12/25 | Loss: 0.00169155
Iteration 13/25 | Loss: 0.00169133
Iteration 14/25 | Loss: 0.00169159
Iteration 15/25 | Loss: 0.00169198
Iteration 16/25 | Loss: 0.00169152
Iteration 17/25 | Loss: 0.00169205
Iteration 18/25 | Loss: 0.00169161
Iteration 19/25 | Loss: 0.00169188
Iteration 20/25 | Loss: 0.00169147
Iteration 21/25 | Loss: 0.00169190
Iteration 22/25 | Loss: 0.00169115
Iteration 23/25 | Loss: 0.00169154
Iteration 24/25 | Loss: 0.00169161
Iteration 25/25 | Loss: 0.00169175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24979329
Iteration 2/25 | Loss: 0.00183060
Iteration 3/25 | Loss: 0.00183060
Iteration 4/25 | Loss: 0.00183060
Iteration 5/25 | Loss: 0.00183060
Iteration 6/25 | Loss: 0.00183060
Iteration 7/25 | Loss: 0.00183060
Iteration 8/25 | Loss: 0.00183060
Iteration 9/25 | Loss: 0.00183060
Iteration 10/25 | Loss: 0.00183060
Iteration 11/25 | Loss: 0.00183060
Iteration 12/25 | Loss: 0.00183060
Iteration 13/25 | Loss: 0.00183060
Iteration 14/25 | Loss: 0.00183060
Iteration 15/25 | Loss: 0.00183060
Iteration 16/25 | Loss: 0.00183060
Iteration 17/25 | Loss: 0.00183060
Iteration 18/25 | Loss: 0.00183060
Iteration 19/25 | Loss: 0.00183060
Iteration 20/25 | Loss: 0.00183060
Iteration 21/25 | Loss: 0.00183060
Iteration 22/25 | Loss: 0.00183060
Iteration 23/25 | Loss: 0.00183060
Iteration 24/25 | Loss: 0.00183060
Iteration 25/25 | Loss: 0.00183060
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001830597873777151, 0.001830597873777151, 0.001830597873777151, 0.001830597873777151, 0.001830597873777151]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001830597873777151

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183060
Iteration 2/1000 | Loss: 0.00008512
Iteration 3/1000 | Loss: 0.00004933
Iteration 4/1000 | Loss: 0.00004340
Iteration 5/1000 | Loss: 0.00004092
Iteration 6/1000 | Loss: 0.00003973
Iteration 7/1000 | Loss: 0.00003916
Iteration 8/1000 | Loss: 0.00003844
Iteration 9/1000 | Loss: 0.00003798
Iteration 10/1000 | Loss: 0.00003786
Iteration 11/1000 | Loss: 0.00003737
Iteration 12/1000 | Loss: 0.00003712
Iteration 13/1000 | Loss: 0.00003685
Iteration 14/1000 | Loss: 0.00003672
Iteration 15/1000 | Loss: 0.00003669
Iteration 16/1000 | Loss: 0.00003665
Iteration 17/1000 | Loss: 0.00003664
Iteration 18/1000 | Loss: 0.00003649
Iteration 19/1000 | Loss: 0.00003643
Iteration 20/1000 | Loss: 0.00003642
Iteration 21/1000 | Loss: 0.00003642
Iteration 22/1000 | Loss: 0.00003643
Iteration 23/1000 | Loss: 0.00003642
Iteration 24/1000 | Loss: 0.00003642
Iteration 25/1000 | Loss: 0.00003639
Iteration 26/1000 | Loss: 0.00003639
Iteration 27/1000 | Loss: 0.00003639
Iteration 28/1000 | Loss: 0.00003639
Iteration 29/1000 | Loss: 0.00003639
Iteration 30/1000 | Loss: 0.00003639
Iteration 31/1000 | Loss: 0.00003638
Iteration 32/1000 | Loss: 0.00003638
Iteration 33/1000 | Loss: 0.00003638
Iteration 34/1000 | Loss: 0.00003638
Iteration 35/1000 | Loss: 0.00003638
Iteration 36/1000 | Loss: 0.00003638
Iteration 37/1000 | Loss: 0.00003637
Iteration 38/1000 | Loss: 0.00003636
Iteration 39/1000 | Loss: 0.00003636
Iteration 40/1000 | Loss: 0.00003634
Iteration 41/1000 | Loss: 0.00003634
Iteration 42/1000 | Loss: 0.00003633
Iteration 43/1000 | Loss: 0.00003633
Iteration 44/1000 | Loss: 0.00003633
Iteration 45/1000 | Loss: 0.00003633
Iteration 46/1000 | Loss: 0.00003632
Iteration 47/1000 | Loss: 0.00003632
Iteration 48/1000 | Loss: 0.00003632
Iteration 49/1000 | Loss: 0.00003631
Iteration 50/1000 | Loss: 0.00003631
Iteration 51/1000 | Loss: 0.00003631
Iteration 52/1000 | Loss: 0.00003630
Iteration 53/1000 | Loss: 0.00003630
Iteration 54/1000 | Loss: 0.00003629
Iteration 55/1000 | Loss: 0.00003629
Iteration 56/1000 | Loss: 0.00003628
Iteration 57/1000 | Loss: 0.00003628
Iteration 58/1000 | Loss: 0.00003628
Iteration 59/1000 | Loss: 0.00003627
Iteration 60/1000 | Loss: 0.00003627
Iteration 61/1000 | Loss: 0.00003626
Iteration 62/1000 | Loss: 0.00003626
Iteration 63/1000 | Loss: 0.00003626
Iteration 64/1000 | Loss: 0.00003626
Iteration 65/1000 | Loss: 0.00003626
Iteration 66/1000 | Loss: 0.00003626
Iteration 67/1000 | Loss: 0.00003626
Iteration 68/1000 | Loss: 0.00003626
Iteration 69/1000 | Loss: 0.00003626
Iteration 70/1000 | Loss: 0.00003626
Iteration 71/1000 | Loss: 0.00003626
Iteration 72/1000 | Loss: 0.00003626
Iteration 73/1000 | Loss: 0.00003626
Iteration 74/1000 | Loss: 0.00003626
Iteration 75/1000 | Loss: 0.00003625
Iteration 76/1000 | Loss: 0.00003625
Iteration 77/1000 | Loss: 0.00003624
Iteration 78/1000 | Loss: 0.00003624
Iteration 79/1000 | Loss: 0.00003624
Iteration 80/1000 | Loss: 0.00003623
Iteration 81/1000 | Loss: 0.00003623
Iteration 82/1000 | Loss: 0.00003623
Iteration 83/1000 | Loss: 0.00003623
Iteration 84/1000 | Loss: 0.00003623
Iteration 85/1000 | Loss: 0.00003623
Iteration 86/1000 | Loss: 0.00003623
Iteration 87/1000 | Loss: 0.00003623
Iteration 88/1000 | Loss: 0.00003622
Iteration 89/1000 | Loss: 0.00003622
Iteration 90/1000 | Loss: 0.00003622
Iteration 91/1000 | Loss: 0.00003622
Iteration 92/1000 | Loss: 0.00003622
Iteration 93/1000 | Loss: 0.00003622
Iteration 94/1000 | Loss: 0.00003622
Iteration 95/1000 | Loss: 0.00003622
Iteration 96/1000 | Loss: 0.00003621
Iteration 97/1000 | Loss: 0.00003621
Iteration 98/1000 | Loss: 0.00003621
Iteration 99/1000 | Loss: 0.00003621
Iteration 100/1000 | Loss: 0.00003621
Iteration 101/1000 | Loss: 0.00003621
Iteration 102/1000 | Loss: 0.00003621
Iteration 103/1000 | Loss: 0.00003621
Iteration 104/1000 | Loss: 0.00003621
Iteration 105/1000 | Loss: 0.00003620
Iteration 106/1000 | Loss: 0.00003620
Iteration 107/1000 | Loss: 0.00003620
Iteration 108/1000 | Loss: 0.00003620
Iteration 109/1000 | Loss: 0.00003620
Iteration 110/1000 | Loss: 0.00003620
Iteration 111/1000 | Loss: 0.00003619
Iteration 112/1000 | Loss: 0.00003619
Iteration 113/1000 | Loss: 0.00003619
Iteration 114/1000 | Loss: 0.00003619
Iteration 115/1000 | Loss: 0.00003619
Iteration 116/1000 | Loss: 0.00003618
Iteration 117/1000 | Loss: 0.00003618
Iteration 118/1000 | Loss: 0.00003618
Iteration 119/1000 | Loss: 0.00003618
Iteration 120/1000 | Loss: 0.00003618
Iteration 121/1000 | Loss: 0.00003618
Iteration 122/1000 | Loss: 0.00003617
Iteration 123/1000 | Loss: 0.00003617
Iteration 124/1000 | Loss: 0.00003617
Iteration 125/1000 | Loss: 0.00003617
Iteration 126/1000 | Loss: 0.00003617
Iteration 127/1000 | Loss: 0.00003617
Iteration 128/1000 | Loss: 0.00003617
Iteration 129/1000 | Loss: 0.00003617
Iteration 130/1000 | Loss: 0.00003617
Iteration 131/1000 | Loss: 0.00003617
Iteration 132/1000 | Loss: 0.00003617
Iteration 133/1000 | Loss: 0.00003617
Iteration 134/1000 | Loss: 0.00003617
Iteration 135/1000 | Loss: 0.00003617
Iteration 136/1000 | Loss: 0.00003617
Iteration 137/1000 | Loss: 0.00003617
Iteration 138/1000 | Loss: 0.00003617
Iteration 139/1000 | Loss: 0.00003617
Iteration 140/1000 | Loss: 0.00003617
Iteration 141/1000 | Loss: 0.00003617
Iteration 142/1000 | Loss: 0.00003617
Iteration 143/1000 | Loss: 0.00003617
Iteration 144/1000 | Loss: 0.00003617
Iteration 145/1000 | Loss: 0.00003617
Iteration 146/1000 | Loss: 0.00003617
Iteration 147/1000 | Loss: 0.00003617
Iteration 148/1000 | Loss: 0.00003617
Iteration 149/1000 | Loss: 0.00003617
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [3.6170356906950474e-05, 3.6170356906950474e-05, 3.6170356906950474e-05, 3.6170356906950474e-05, 3.6170356906950474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6170356906950474e-05

Optimization complete. Final v2v error: 4.955557346343994 mm

Highest mean error: 11.71664810180664 mm for frame 21

Lowest mean error: 4.247946739196777 mm for frame 122

Saving results

Total time: 87.25751781463623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440854
Iteration 2/25 | Loss: 0.00195025
Iteration 3/25 | Loss: 0.00189635
Iteration 4/25 | Loss: 0.00189104
Iteration 5/25 | Loss: 0.00189008
Iteration 6/25 | Loss: 0.00189008
Iteration 7/25 | Loss: 0.00189008
Iteration 8/25 | Loss: 0.00189008
Iteration 9/25 | Loss: 0.00189008
Iteration 10/25 | Loss: 0.00189008
Iteration 11/25 | Loss: 0.00189008
Iteration 12/25 | Loss: 0.00189008
Iteration 13/25 | Loss: 0.00189008
Iteration 14/25 | Loss: 0.00189008
Iteration 15/25 | Loss: 0.00189008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0018900788854807615, 0.0018900788854807615, 0.0018900788854807615, 0.0018900788854807615, 0.0018900788854807615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018900788854807615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.92027593
Iteration 2/25 | Loss: 0.00278413
Iteration 3/25 | Loss: 0.00278413
Iteration 4/25 | Loss: 0.00278413
Iteration 5/25 | Loss: 0.00278413
Iteration 6/25 | Loss: 0.00278413
Iteration 7/25 | Loss: 0.00278413
Iteration 8/25 | Loss: 0.00278413
Iteration 9/25 | Loss: 0.00278413
Iteration 10/25 | Loss: 0.00278413
Iteration 11/25 | Loss: 0.00278413
Iteration 12/25 | Loss: 0.00278413
Iteration 13/25 | Loss: 0.00278413
Iteration 14/25 | Loss: 0.00278413
Iteration 15/25 | Loss: 0.00278413
Iteration 16/25 | Loss: 0.00278413
Iteration 17/25 | Loss: 0.00278413
Iteration 18/25 | Loss: 0.00278413
Iteration 19/25 | Loss: 0.00278413
Iteration 20/25 | Loss: 0.00278413
Iteration 21/25 | Loss: 0.00278413
Iteration 22/25 | Loss: 0.00278413
Iteration 23/25 | Loss: 0.00278413
Iteration 24/25 | Loss: 0.00278413
Iteration 25/25 | Loss: 0.00278413
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0027841299306601286, 0.0027841299306601286, 0.0027841299306601286, 0.0027841299306601286, 0.0027841299306601286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027841299306601286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00278413
Iteration 2/1000 | Loss: 0.00005967
Iteration 3/1000 | Loss: 0.00004025
Iteration 4/1000 | Loss: 0.00003617
Iteration 5/1000 | Loss: 0.00003391
Iteration 6/1000 | Loss: 0.00003271
Iteration 7/1000 | Loss: 0.00003216
Iteration 8/1000 | Loss: 0.00003161
Iteration 9/1000 | Loss: 0.00003139
Iteration 10/1000 | Loss: 0.00003119
Iteration 11/1000 | Loss: 0.00003112
Iteration 12/1000 | Loss: 0.00003106
Iteration 13/1000 | Loss: 0.00003105
Iteration 14/1000 | Loss: 0.00003100
Iteration 15/1000 | Loss: 0.00003090
Iteration 16/1000 | Loss: 0.00003090
Iteration 17/1000 | Loss: 0.00003090
Iteration 18/1000 | Loss: 0.00003089
Iteration 19/1000 | Loss: 0.00003089
Iteration 20/1000 | Loss: 0.00003089
Iteration 21/1000 | Loss: 0.00003089
Iteration 22/1000 | Loss: 0.00003089
Iteration 23/1000 | Loss: 0.00003088
Iteration 24/1000 | Loss: 0.00003084
Iteration 25/1000 | Loss: 0.00003084
Iteration 26/1000 | Loss: 0.00003078
Iteration 27/1000 | Loss: 0.00003078
Iteration 28/1000 | Loss: 0.00003077
Iteration 29/1000 | Loss: 0.00003077
Iteration 30/1000 | Loss: 0.00003076
Iteration 31/1000 | Loss: 0.00003076
Iteration 32/1000 | Loss: 0.00003075
Iteration 33/1000 | Loss: 0.00003074
Iteration 34/1000 | Loss: 0.00003074
Iteration 35/1000 | Loss: 0.00003073
Iteration 36/1000 | Loss: 0.00003073
Iteration 37/1000 | Loss: 0.00003072
Iteration 38/1000 | Loss: 0.00003072
Iteration 39/1000 | Loss: 0.00003072
Iteration 40/1000 | Loss: 0.00003071
Iteration 41/1000 | Loss: 0.00003071
Iteration 42/1000 | Loss: 0.00003071
Iteration 43/1000 | Loss: 0.00003069
Iteration 44/1000 | Loss: 0.00003069
Iteration 45/1000 | Loss: 0.00003069
Iteration 46/1000 | Loss: 0.00003068
Iteration 47/1000 | Loss: 0.00003068
Iteration 48/1000 | Loss: 0.00003068
Iteration 49/1000 | Loss: 0.00003067
Iteration 50/1000 | Loss: 0.00003065
Iteration 51/1000 | Loss: 0.00003065
Iteration 52/1000 | Loss: 0.00003065
Iteration 53/1000 | Loss: 0.00003065
Iteration 54/1000 | Loss: 0.00003065
Iteration 55/1000 | Loss: 0.00003065
Iteration 56/1000 | Loss: 0.00003064
Iteration 57/1000 | Loss: 0.00003064
Iteration 58/1000 | Loss: 0.00003064
Iteration 59/1000 | Loss: 0.00003064
Iteration 60/1000 | Loss: 0.00003064
Iteration 61/1000 | Loss: 0.00003064
Iteration 62/1000 | Loss: 0.00003063
Iteration 63/1000 | Loss: 0.00003063
Iteration 64/1000 | Loss: 0.00003063
Iteration 65/1000 | Loss: 0.00003063
Iteration 66/1000 | Loss: 0.00003063
Iteration 67/1000 | Loss: 0.00003063
Iteration 68/1000 | Loss: 0.00003062
Iteration 69/1000 | Loss: 0.00003062
Iteration 70/1000 | Loss: 0.00003062
Iteration 71/1000 | Loss: 0.00003062
Iteration 72/1000 | Loss: 0.00003062
Iteration 73/1000 | Loss: 0.00003062
Iteration 74/1000 | Loss: 0.00003062
Iteration 75/1000 | Loss: 0.00003062
Iteration 76/1000 | Loss: 0.00003062
Iteration 77/1000 | Loss: 0.00003062
Iteration 78/1000 | Loss: 0.00003062
Iteration 79/1000 | Loss: 0.00003061
Iteration 80/1000 | Loss: 0.00003061
Iteration 81/1000 | Loss: 0.00003061
Iteration 82/1000 | Loss: 0.00003061
Iteration 83/1000 | Loss: 0.00003061
Iteration 84/1000 | Loss: 0.00003061
Iteration 85/1000 | Loss: 0.00003061
Iteration 86/1000 | Loss: 0.00003061
Iteration 87/1000 | Loss: 0.00003061
Iteration 88/1000 | Loss: 0.00003061
Iteration 89/1000 | Loss: 0.00003061
Iteration 90/1000 | Loss: 0.00003061
Iteration 91/1000 | Loss: 0.00003061
Iteration 92/1000 | Loss: 0.00003061
Iteration 93/1000 | Loss: 0.00003061
Iteration 94/1000 | Loss: 0.00003061
Iteration 95/1000 | Loss: 0.00003061
Iteration 96/1000 | Loss: 0.00003061
Iteration 97/1000 | Loss: 0.00003061
Iteration 98/1000 | Loss: 0.00003061
Iteration 99/1000 | Loss: 0.00003061
Iteration 100/1000 | Loss: 0.00003061
Iteration 101/1000 | Loss: 0.00003061
Iteration 102/1000 | Loss: 0.00003061
Iteration 103/1000 | Loss: 0.00003061
Iteration 104/1000 | Loss: 0.00003061
Iteration 105/1000 | Loss: 0.00003061
Iteration 106/1000 | Loss: 0.00003061
Iteration 107/1000 | Loss: 0.00003061
Iteration 108/1000 | Loss: 0.00003061
Iteration 109/1000 | Loss: 0.00003061
Iteration 110/1000 | Loss: 0.00003061
Iteration 111/1000 | Loss: 0.00003061
Iteration 112/1000 | Loss: 0.00003061
Iteration 113/1000 | Loss: 0.00003061
Iteration 114/1000 | Loss: 0.00003061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [3.060679591726512e-05, 3.060679591726512e-05, 3.060679591726512e-05, 3.060679591726512e-05, 3.060679591726512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.060679591726512e-05

Optimization complete. Final v2v error: 4.7543535232543945 mm

Highest mean error: 5.033589839935303 mm for frame 67

Lowest mean error: 4.490739345550537 mm for frame 10

Saving results

Total time: 31.328595399856567
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544132
Iteration 2/25 | Loss: 0.00212713
Iteration 3/25 | Loss: 0.00194301
Iteration 4/25 | Loss: 0.00192285
Iteration 5/25 | Loss: 0.00191996
Iteration 6/25 | Loss: 0.00191951
Iteration 7/25 | Loss: 0.00191951
Iteration 8/25 | Loss: 0.00191951
Iteration 9/25 | Loss: 0.00191951
Iteration 10/25 | Loss: 0.00191951
Iteration 11/25 | Loss: 0.00191951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0019195110071450472, 0.0019195110071450472, 0.0019195110071450472, 0.0019195110071450472, 0.0019195110071450472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019195110071450472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24319041
Iteration 2/25 | Loss: 0.00246405
Iteration 3/25 | Loss: 0.00246405
Iteration 4/25 | Loss: 0.00246405
Iteration 5/25 | Loss: 0.00246405
Iteration 6/25 | Loss: 0.00246405
Iteration 7/25 | Loss: 0.00246405
Iteration 8/25 | Loss: 0.00246405
Iteration 9/25 | Loss: 0.00246405
Iteration 10/25 | Loss: 0.00246405
Iteration 11/25 | Loss: 0.00246405
Iteration 12/25 | Loss: 0.00246405
Iteration 13/25 | Loss: 0.00246405
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.002464045537635684, 0.002464045537635684, 0.002464045537635684, 0.002464045537635684, 0.002464045537635684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002464045537635684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00246405
Iteration 2/1000 | Loss: 0.00008042
Iteration 3/1000 | Loss: 0.00005565
Iteration 4/1000 | Loss: 0.00004486
Iteration 5/1000 | Loss: 0.00003942
Iteration 6/1000 | Loss: 0.00003761
Iteration 7/1000 | Loss: 0.00003650
Iteration 8/1000 | Loss: 0.00003566
Iteration 9/1000 | Loss: 0.00003503
Iteration 10/1000 | Loss: 0.00003471
Iteration 11/1000 | Loss: 0.00003438
Iteration 12/1000 | Loss: 0.00003413
Iteration 13/1000 | Loss: 0.00003392
Iteration 14/1000 | Loss: 0.00003373
Iteration 15/1000 | Loss: 0.00003368
Iteration 16/1000 | Loss: 0.00003366
Iteration 17/1000 | Loss: 0.00003359
Iteration 18/1000 | Loss: 0.00003355
Iteration 19/1000 | Loss: 0.00003354
Iteration 20/1000 | Loss: 0.00003352
Iteration 21/1000 | Loss: 0.00003351
Iteration 22/1000 | Loss: 0.00003350
Iteration 23/1000 | Loss: 0.00003350
Iteration 24/1000 | Loss: 0.00003350
Iteration 25/1000 | Loss: 0.00003349
Iteration 26/1000 | Loss: 0.00003345
Iteration 27/1000 | Loss: 0.00003344
Iteration 28/1000 | Loss: 0.00003344
Iteration 29/1000 | Loss: 0.00003343
Iteration 30/1000 | Loss: 0.00003343
Iteration 31/1000 | Loss: 0.00003342
Iteration 32/1000 | Loss: 0.00003341
Iteration 33/1000 | Loss: 0.00003340
Iteration 34/1000 | Loss: 0.00003340
Iteration 35/1000 | Loss: 0.00003340
Iteration 36/1000 | Loss: 0.00003339
Iteration 37/1000 | Loss: 0.00003339
Iteration 38/1000 | Loss: 0.00003339
Iteration 39/1000 | Loss: 0.00003339
Iteration 40/1000 | Loss: 0.00003339
Iteration 41/1000 | Loss: 0.00003339
Iteration 42/1000 | Loss: 0.00003338
Iteration 43/1000 | Loss: 0.00003338
Iteration 44/1000 | Loss: 0.00003338
Iteration 45/1000 | Loss: 0.00003338
Iteration 46/1000 | Loss: 0.00003338
Iteration 47/1000 | Loss: 0.00003338
Iteration 48/1000 | Loss: 0.00003338
Iteration 49/1000 | Loss: 0.00003338
Iteration 50/1000 | Loss: 0.00003337
Iteration 51/1000 | Loss: 0.00003336
Iteration 52/1000 | Loss: 0.00003335
Iteration 53/1000 | Loss: 0.00003335
Iteration 54/1000 | Loss: 0.00003335
Iteration 55/1000 | Loss: 0.00003334
Iteration 56/1000 | Loss: 0.00003334
Iteration 57/1000 | Loss: 0.00003333
Iteration 58/1000 | Loss: 0.00003333
Iteration 59/1000 | Loss: 0.00003333
Iteration 60/1000 | Loss: 0.00003333
Iteration 61/1000 | Loss: 0.00003332
Iteration 62/1000 | Loss: 0.00003332
Iteration 63/1000 | Loss: 0.00003332
Iteration 64/1000 | Loss: 0.00003332
Iteration 65/1000 | Loss: 0.00003332
Iteration 66/1000 | Loss: 0.00003332
Iteration 67/1000 | Loss: 0.00003332
Iteration 68/1000 | Loss: 0.00003332
Iteration 69/1000 | Loss: 0.00003332
Iteration 70/1000 | Loss: 0.00003332
Iteration 71/1000 | Loss: 0.00003331
Iteration 72/1000 | Loss: 0.00003331
Iteration 73/1000 | Loss: 0.00003331
Iteration 74/1000 | Loss: 0.00003331
Iteration 75/1000 | Loss: 0.00003331
Iteration 76/1000 | Loss: 0.00003331
Iteration 77/1000 | Loss: 0.00003331
Iteration 78/1000 | Loss: 0.00003331
Iteration 79/1000 | Loss: 0.00003331
Iteration 80/1000 | Loss: 0.00003330
Iteration 81/1000 | Loss: 0.00003330
Iteration 82/1000 | Loss: 0.00003330
Iteration 83/1000 | Loss: 0.00003330
Iteration 84/1000 | Loss: 0.00003330
Iteration 85/1000 | Loss: 0.00003330
Iteration 86/1000 | Loss: 0.00003330
Iteration 87/1000 | Loss: 0.00003330
Iteration 88/1000 | Loss: 0.00003330
Iteration 89/1000 | Loss: 0.00003330
Iteration 90/1000 | Loss: 0.00003330
Iteration 91/1000 | Loss: 0.00003330
Iteration 92/1000 | Loss: 0.00003330
Iteration 93/1000 | Loss: 0.00003330
Iteration 94/1000 | Loss: 0.00003329
Iteration 95/1000 | Loss: 0.00003329
Iteration 96/1000 | Loss: 0.00003329
Iteration 97/1000 | Loss: 0.00003329
Iteration 98/1000 | Loss: 0.00003329
Iteration 99/1000 | Loss: 0.00003329
Iteration 100/1000 | Loss: 0.00003329
Iteration 101/1000 | Loss: 0.00003329
Iteration 102/1000 | Loss: 0.00003329
Iteration 103/1000 | Loss: 0.00003329
Iteration 104/1000 | Loss: 0.00003329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [3.329399260110222e-05, 3.329399260110222e-05, 3.329399260110222e-05, 3.329399260110222e-05, 3.329399260110222e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.329399260110222e-05

Optimization complete. Final v2v error: 4.906291484832764 mm

Highest mean error: 5.823304176330566 mm for frame 77

Lowest mean error: 4.385163307189941 mm for frame 1

Saving results

Total time: 36.378902196884155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467820
Iteration 2/25 | Loss: 0.00198241
Iteration 3/25 | Loss: 0.00190439
Iteration 4/25 | Loss: 0.00189742
Iteration 5/25 | Loss: 0.00189332
Iteration 6/25 | Loss: 0.00189281
Iteration 7/25 | Loss: 0.00189281
Iteration 8/25 | Loss: 0.00189281
Iteration 9/25 | Loss: 0.00189281
Iteration 10/25 | Loss: 0.00189281
Iteration 11/25 | Loss: 0.00189281
Iteration 12/25 | Loss: 0.00189281
Iteration 13/25 | Loss: 0.00189281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0018928054487332702, 0.0018928054487332702, 0.0018928054487332702, 0.0018928054487332702, 0.0018928054487332702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018928054487332702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22573090
Iteration 2/25 | Loss: 0.00288091
Iteration 3/25 | Loss: 0.00288091
Iteration 4/25 | Loss: 0.00288091
Iteration 5/25 | Loss: 0.00288091
Iteration 6/25 | Loss: 0.00288091
Iteration 7/25 | Loss: 0.00288091
Iteration 8/25 | Loss: 0.00288091
Iteration 9/25 | Loss: 0.00288091
Iteration 10/25 | Loss: 0.00288091
Iteration 11/25 | Loss: 0.00288091
Iteration 12/25 | Loss: 0.00288091
Iteration 13/25 | Loss: 0.00288091
Iteration 14/25 | Loss: 0.00288091
Iteration 15/25 | Loss: 0.00288091
Iteration 16/25 | Loss: 0.00288091
Iteration 17/25 | Loss: 0.00288091
Iteration 18/25 | Loss: 0.00288091
Iteration 19/25 | Loss: 0.00288091
Iteration 20/25 | Loss: 0.00288091
Iteration 21/25 | Loss: 0.00288091
Iteration 22/25 | Loss: 0.00288091
Iteration 23/25 | Loss: 0.00288091
Iteration 24/25 | Loss: 0.00288091
Iteration 25/25 | Loss: 0.00288091

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288091
Iteration 2/1000 | Loss: 0.00005541
Iteration 3/1000 | Loss: 0.00003797
Iteration 4/1000 | Loss: 0.00003375
Iteration 5/1000 | Loss: 0.00003225
Iteration 6/1000 | Loss: 0.00003123
Iteration 7/1000 | Loss: 0.00003040
Iteration 8/1000 | Loss: 0.00002991
Iteration 9/1000 | Loss: 0.00002941
Iteration 10/1000 | Loss: 0.00002907
Iteration 11/1000 | Loss: 0.00002880
Iteration 12/1000 | Loss: 0.00002854
Iteration 13/1000 | Loss: 0.00002849
Iteration 14/1000 | Loss: 0.00002849
Iteration 15/1000 | Loss: 0.00002846
Iteration 16/1000 | Loss: 0.00002845
Iteration 17/1000 | Loss: 0.00002844
Iteration 18/1000 | Loss: 0.00002844
Iteration 19/1000 | Loss: 0.00002843
Iteration 20/1000 | Loss: 0.00002843
Iteration 21/1000 | Loss: 0.00002843
Iteration 22/1000 | Loss: 0.00002841
Iteration 23/1000 | Loss: 0.00002841
Iteration 24/1000 | Loss: 0.00002839
Iteration 25/1000 | Loss: 0.00002836
Iteration 26/1000 | Loss: 0.00002833
Iteration 27/1000 | Loss: 0.00002833
Iteration 28/1000 | Loss: 0.00002832
Iteration 29/1000 | Loss: 0.00002831
Iteration 30/1000 | Loss: 0.00002831
Iteration 31/1000 | Loss: 0.00002831
Iteration 32/1000 | Loss: 0.00002830
Iteration 33/1000 | Loss: 0.00002830
Iteration 34/1000 | Loss: 0.00002830
Iteration 35/1000 | Loss: 0.00002829
Iteration 36/1000 | Loss: 0.00002829
Iteration 37/1000 | Loss: 0.00002829
Iteration 38/1000 | Loss: 0.00002828
Iteration 39/1000 | Loss: 0.00002828
Iteration 40/1000 | Loss: 0.00002827
Iteration 41/1000 | Loss: 0.00002827
Iteration 42/1000 | Loss: 0.00002827
Iteration 43/1000 | Loss: 0.00002827
Iteration 44/1000 | Loss: 0.00002827
Iteration 45/1000 | Loss: 0.00002827
Iteration 46/1000 | Loss: 0.00002827
Iteration 47/1000 | Loss: 0.00002826
Iteration 48/1000 | Loss: 0.00002826
Iteration 49/1000 | Loss: 0.00002826
Iteration 50/1000 | Loss: 0.00002826
Iteration 51/1000 | Loss: 0.00002826
Iteration 52/1000 | Loss: 0.00002826
Iteration 53/1000 | Loss: 0.00002826
Iteration 54/1000 | Loss: 0.00002826
Iteration 55/1000 | Loss: 0.00002826
Iteration 56/1000 | Loss: 0.00002826
Iteration 57/1000 | Loss: 0.00002826
Iteration 58/1000 | Loss: 0.00002826
Iteration 59/1000 | Loss: 0.00002826
Iteration 60/1000 | Loss: 0.00002826
Iteration 61/1000 | Loss: 0.00002826
Iteration 62/1000 | Loss: 0.00002826
Iteration 63/1000 | Loss: 0.00002825
Iteration 64/1000 | Loss: 0.00002825
Iteration 65/1000 | Loss: 0.00002825
Iteration 66/1000 | Loss: 0.00002825
Iteration 67/1000 | Loss: 0.00002825
Iteration 68/1000 | Loss: 0.00002825
Iteration 69/1000 | Loss: 0.00002825
Iteration 70/1000 | Loss: 0.00002825
Iteration 71/1000 | Loss: 0.00002825
Iteration 72/1000 | Loss: 0.00002824
Iteration 73/1000 | Loss: 0.00002824
Iteration 74/1000 | Loss: 0.00002824
Iteration 75/1000 | Loss: 0.00002824
Iteration 76/1000 | Loss: 0.00002824
Iteration 77/1000 | Loss: 0.00002824
Iteration 78/1000 | Loss: 0.00002824
Iteration 79/1000 | Loss: 0.00002824
Iteration 80/1000 | Loss: 0.00002824
Iteration 81/1000 | Loss: 0.00002824
Iteration 82/1000 | Loss: 0.00002824
Iteration 83/1000 | Loss: 0.00002824
Iteration 84/1000 | Loss: 0.00002824
Iteration 85/1000 | Loss: 0.00002824
Iteration 86/1000 | Loss: 0.00002824
Iteration 87/1000 | Loss: 0.00002824
Iteration 88/1000 | Loss: 0.00002824
Iteration 89/1000 | Loss: 0.00002824
Iteration 90/1000 | Loss: 0.00002824
Iteration 91/1000 | Loss: 0.00002824
Iteration 92/1000 | Loss: 0.00002824
Iteration 93/1000 | Loss: 0.00002824
Iteration 94/1000 | Loss: 0.00002824
Iteration 95/1000 | Loss: 0.00002824
Iteration 96/1000 | Loss: 0.00002824
Iteration 97/1000 | Loss: 0.00002824
Iteration 98/1000 | Loss: 0.00002824
Iteration 99/1000 | Loss: 0.00002824
Iteration 100/1000 | Loss: 0.00002824
Iteration 101/1000 | Loss: 0.00002824
Iteration 102/1000 | Loss: 0.00002824
Iteration 103/1000 | Loss: 0.00002824
Iteration 104/1000 | Loss: 0.00002824
Iteration 105/1000 | Loss: 0.00002824
Iteration 106/1000 | Loss: 0.00002824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.8241365725989453e-05, 2.8241365725989453e-05, 2.8241365725989453e-05, 2.8241365725989453e-05, 2.8241365725989453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8241365725989453e-05

Optimization complete. Final v2v error: 4.587649345397949 mm

Highest mean error: 4.986162185668945 mm for frame 137

Lowest mean error: 4.26124382019043 mm for frame 84

Saving results

Total time: 33.2749879360199
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00653161
Iteration 2/25 | Loss: 0.00169004
Iteration 3/25 | Loss: 0.00145357
Iteration 4/25 | Loss: 0.00137284
Iteration 5/25 | Loss: 0.00140991
Iteration 6/25 | Loss: 0.00139362
Iteration 7/25 | Loss: 0.00132993
Iteration 8/25 | Loss: 0.00129567
Iteration 9/25 | Loss: 0.00128120
Iteration 10/25 | Loss: 0.00127567
Iteration 11/25 | Loss: 0.00126665
Iteration 12/25 | Loss: 0.00126472
Iteration 13/25 | Loss: 0.00126337
Iteration 14/25 | Loss: 0.00126223
Iteration 15/25 | Loss: 0.00126124
Iteration 16/25 | Loss: 0.00126038
Iteration 17/25 | Loss: 0.00125919
Iteration 18/25 | Loss: 0.00125892
Iteration 19/25 | Loss: 0.00125890
Iteration 20/25 | Loss: 0.00125890
Iteration 21/25 | Loss: 0.00125890
Iteration 22/25 | Loss: 0.00125890
Iteration 23/25 | Loss: 0.00125890
Iteration 24/25 | Loss: 0.00125890
Iteration 25/25 | Loss: 0.00125890

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15495753
Iteration 2/25 | Loss: 0.00119048
Iteration 3/25 | Loss: 0.00119040
Iteration 4/25 | Loss: 0.00119040
Iteration 5/25 | Loss: 0.00119040
Iteration 6/25 | Loss: 0.00119040
Iteration 7/25 | Loss: 0.00119040
Iteration 8/25 | Loss: 0.00119040
Iteration 9/25 | Loss: 0.00119040
Iteration 10/25 | Loss: 0.00119040
Iteration 11/25 | Loss: 0.00119040
Iteration 12/25 | Loss: 0.00119040
Iteration 13/25 | Loss: 0.00119040
Iteration 14/25 | Loss: 0.00119040
Iteration 15/25 | Loss: 0.00119040
Iteration 16/25 | Loss: 0.00119040
Iteration 17/25 | Loss: 0.00119040
Iteration 18/25 | Loss: 0.00119040
Iteration 19/25 | Loss: 0.00119040
Iteration 20/25 | Loss: 0.00119040
Iteration 21/25 | Loss: 0.00119040
Iteration 22/25 | Loss: 0.00119040
Iteration 23/25 | Loss: 0.00119040
Iteration 24/25 | Loss: 0.00119040
Iteration 25/25 | Loss: 0.00119040

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119040
Iteration 2/1000 | Loss: 0.00014644
Iteration 3/1000 | Loss: 0.00008317
Iteration 4/1000 | Loss: 0.00005694
Iteration 5/1000 | Loss: 0.00004811
Iteration 6/1000 | Loss: 0.00004442
Iteration 7/1000 | Loss: 0.00004236
Iteration 8/1000 | Loss: 0.00004110
Iteration 9/1000 | Loss: 0.00003974
Iteration 10/1000 | Loss: 0.00003882
Iteration 11/1000 | Loss: 0.00003829
Iteration 12/1000 | Loss: 0.00003783
Iteration 13/1000 | Loss: 0.00003739
Iteration 14/1000 | Loss: 0.00003710
Iteration 15/1000 | Loss: 0.00003688
Iteration 16/1000 | Loss: 0.00003666
Iteration 17/1000 | Loss: 0.00003666
Iteration 18/1000 | Loss: 0.00003653
Iteration 19/1000 | Loss: 0.00003652
Iteration 20/1000 | Loss: 0.00003651
Iteration 21/1000 | Loss: 0.00003647
Iteration 22/1000 | Loss: 0.00003646
Iteration 23/1000 | Loss: 0.00003644
Iteration 24/1000 | Loss: 0.00003643
Iteration 25/1000 | Loss: 0.00003640
Iteration 26/1000 | Loss: 0.00003639
Iteration 27/1000 | Loss: 0.00003639
Iteration 28/1000 | Loss: 0.00003638
Iteration 29/1000 | Loss: 0.00003637
Iteration 30/1000 | Loss: 0.00003637
Iteration 31/1000 | Loss: 0.00003635
Iteration 32/1000 | Loss: 0.00003633
Iteration 33/1000 | Loss: 0.00003633
Iteration 34/1000 | Loss: 0.00003632
Iteration 35/1000 | Loss: 0.00003632
Iteration 36/1000 | Loss: 0.00003629
Iteration 37/1000 | Loss: 0.00003628
Iteration 38/1000 | Loss: 0.00003628
Iteration 39/1000 | Loss: 0.00003626
Iteration 40/1000 | Loss: 0.00003625
Iteration 41/1000 | Loss: 0.00003625
Iteration 42/1000 | Loss: 0.00003625
Iteration 43/1000 | Loss: 0.00003625
Iteration 44/1000 | Loss: 0.00003624
Iteration 45/1000 | Loss: 0.00003624
Iteration 46/1000 | Loss: 0.00003623
Iteration 47/1000 | Loss: 0.00003623
Iteration 48/1000 | Loss: 0.00003622
Iteration 49/1000 | Loss: 0.00003622
Iteration 50/1000 | Loss: 0.00003621
Iteration 51/1000 | Loss: 0.00003621
Iteration 52/1000 | Loss: 0.00003620
Iteration 53/1000 | Loss: 0.00003620
Iteration 54/1000 | Loss: 0.00003619
Iteration 55/1000 | Loss: 0.00003619
Iteration 56/1000 | Loss: 0.00003619
Iteration 57/1000 | Loss: 0.00003618
Iteration 58/1000 | Loss: 0.00003617
Iteration 59/1000 | Loss: 0.00003617
Iteration 60/1000 | Loss: 0.00003617
Iteration 61/1000 | Loss: 0.00003616
Iteration 62/1000 | Loss: 0.00003616
Iteration 63/1000 | Loss: 0.00003615
Iteration 64/1000 | Loss: 0.00003615
Iteration 65/1000 | Loss: 0.00003615
Iteration 66/1000 | Loss: 0.00003614
Iteration 67/1000 | Loss: 0.00003614
Iteration 68/1000 | Loss: 0.00003614
Iteration 69/1000 | Loss: 0.00003614
Iteration 70/1000 | Loss: 0.00003614
Iteration 71/1000 | Loss: 0.00003613
Iteration 72/1000 | Loss: 0.00003613
Iteration 73/1000 | Loss: 0.00003613
Iteration 74/1000 | Loss: 0.00003613
Iteration 75/1000 | Loss: 0.00003613
Iteration 76/1000 | Loss: 0.00003613
Iteration 77/1000 | Loss: 0.00003613
Iteration 78/1000 | Loss: 0.00003613
Iteration 79/1000 | Loss: 0.00003612
Iteration 80/1000 | Loss: 0.00003612
Iteration 81/1000 | Loss: 0.00003612
Iteration 82/1000 | Loss: 0.00003612
Iteration 83/1000 | Loss: 0.00003612
Iteration 84/1000 | Loss: 0.00003612
Iteration 85/1000 | Loss: 0.00003611
Iteration 86/1000 | Loss: 0.00003611
Iteration 87/1000 | Loss: 0.00003611
Iteration 88/1000 | Loss: 0.00003611
Iteration 89/1000 | Loss: 0.00003611
Iteration 90/1000 | Loss: 0.00003610
Iteration 91/1000 | Loss: 0.00003610
Iteration 92/1000 | Loss: 0.00003610
Iteration 93/1000 | Loss: 0.00003610
Iteration 94/1000 | Loss: 0.00003609
Iteration 95/1000 | Loss: 0.00003609
Iteration 96/1000 | Loss: 0.00003609
Iteration 97/1000 | Loss: 0.00003609
Iteration 98/1000 | Loss: 0.00003608
Iteration 99/1000 | Loss: 0.00003608
Iteration 100/1000 | Loss: 0.00003608
Iteration 101/1000 | Loss: 0.00003608
Iteration 102/1000 | Loss: 0.00003608
Iteration 103/1000 | Loss: 0.00003608
Iteration 104/1000 | Loss: 0.00003608
Iteration 105/1000 | Loss: 0.00003608
Iteration 106/1000 | Loss: 0.00003608
Iteration 107/1000 | Loss: 0.00003608
Iteration 108/1000 | Loss: 0.00003608
Iteration 109/1000 | Loss: 0.00003608
Iteration 110/1000 | Loss: 0.00003608
Iteration 111/1000 | Loss: 0.00003608
Iteration 112/1000 | Loss: 0.00003608
Iteration 113/1000 | Loss: 0.00003608
Iteration 114/1000 | Loss: 0.00003608
Iteration 115/1000 | Loss: 0.00003607
Iteration 116/1000 | Loss: 0.00003607
Iteration 117/1000 | Loss: 0.00003607
Iteration 118/1000 | Loss: 0.00003607
Iteration 119/1000 | Loss: 0.00003607
Iteration 120/1000 | Loss: 0.00003607
Iteration 121/1000 | Loss: 0.00003606
Iteration 122/1000 | Loss: 0.00003606
Iteration 123/1000 | Loss: 0.00003606
Iteration 124/1000 | Loss: 0.00003606
Iteration 125/1000 | Loss: 0.00003606
Iteration 126/1000 | Loss: 0.00003606
Iteration 127/1000 | Loss: 0.00003606
Iteration 128/1000 | Loss: 0.00003605
Iteration 129/1000 | Loss: 0.00003605
Iteration 130/1000 | Loss: 0.00003605
Iteration 131/1000 | Loss: 0.00003605
Iteration 132/1000 | Loss: 0.00003605
Iteration 133/1000 | Loss: 0.00003605
Iteration 134/1000 | Loss: 0.00003605
Iteration 135/1000 | Loss: 0.00003605
Iteration 136/1000 | Loss: 0.00003605
Iteration 137/1000 | Loss: 0.00003605
Iteration 138/1000 | Loss: 0.00003605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [3.6050354538019747e-05, 3.6050354538019747e-05, 3.6050354538019747e-05, 3.6050354538019747e-05, 3.6050354538019747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6050354538019747e-05

Optimization complete. Final v2v error: 4.763279438018799 mm

Highest mean error: 6.130846977233887 mm for frame 62

Lowest mean error: 4.040689468383789 mm for frame 41

Saving results

Total time: 67.28422856330872
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00524671
Iteration 2/25 | Loss: 0.00138176
Iteration 3/25 | Loss: 0.00123224
Iteration 4/25 | Loss: 0.00120532
Iteration 5/25 | Loss: 0.00119828
Iteration 6/25 | Loss: 0.00119616
Iteration 7/25 | Loss: 0.00119560
Iteration 8/25 | Loss: 0.00119560
Iteration 9/25 | Loss: 0.00119560
Iteration 10/25 | Loss: 0.00119560
Iteration 11/25 | Loss: 0.00119560
Iteration 12/25 | Loss: 0.00119560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001195599208585918, 0.001195599208585918, 0.001195599208585918, 0.001195599208585918, 0.001195599208585918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001195599208585918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21952593
Iteration 2/25 | Loss: 0.00103270
Iteration 3/25 | Loss: 0.00103270
Iteration 4/25 | Loss: 0.00103270
Iteration 5/25 | Loss: 0.00103270
Iteration 6/25 | Loss: 0.00103270
Iteration 7/25 | Loss: 0.00103270
Iteration 8/25 | Loss: 0.00103270
Iteration 9/25 | Loss: 0.00103270
Iteration 10/25 | Loss: 0.00103270
Iteration 11/25 | Loss: 0.00103270
Iteration 12/25 | Loss: 0.00103270
Iteration 13/25 | Loss: 0.00103270
Iteration 14/25 | Loss: 0.00103270
Iteration 15/25 | Loss: 0.00103270
Iteration 16/25 | Loss: 0.00103270
Iteration 17/25 | Loss: 0.00103270
Iteration 18/25 | Loss: 0.00103270
Iteration 19/25 | Loss: 0.00103270
Iteration 20/25 | Loss: 0.00103270
Iteration 21/25 | Loss: 0.00103270
Iteration 22/25 | Loss: 0.00103270
Iteration 23/25 | Loss: 0.00103270
Iteration 24/25 | Loss: 0.00103270
Iteration 25/25 | Loss: 0.00103270

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103270
Iteration 2/1000 | Loss: 0.00005743
Iteration 3/1000 | Loss: 0.00004819
Iteration 4/1000 | Loss: 0.00004427
Iteration 5/1000 | Loss: 0.00004188
Iteration 6/1000 | Loss: 0.00004014
Iteration 7/1000 | Loss: 0.00003897
Iteration 8/1000 | Loss: 0.00003822
Iteration 9/1000 | Loss: 0.00003787
Iteration 10/1000 | Loss: 0.00003760
Iteration 11/1000 | Loss: 0.00003745
Iteration 12/1000 | Loss: 0.00003738
Iteration 13/1000 | Loss: 0.00003735
Iteration 14/1000 | Loss: 0.00003734
Iteration 15/1000 | Loss: 0.00003734
Iteration 16/1000 | Loss: 0.00003733
Iteration 17/1000 | Loss: 0.00003732
Iteration 18/1000 | Loss: 0.00003732
Iteration 19/1000 | Loss: 0.00003730
Iteration 20/1000 | Loss: 0.00003730
Iteration 21/1000 | Loss: 0.00003726
Iteration 22/1000 | Loss: 0.00003726
Iteration 23/1000 | Loss: 0.00003726
Iteration 24/1000 | Loss: 0.00003726
Iteration 25/1000 | Loss: 0.00003726
Iteration 26/1000 | Loss: 0.00003726
Iteration 27/1000 | Loss: 0.00003725
Iteration 28/1000 | Loss: 0.00003725
Iteration 29/1000 | Loss: 0.00003725
Iteration 30/1000 | Loss: 0.00003724
Iteration 31/1000 | Loss: 0.00003724
Iteration 32/1000 | Loss: 0.00003724
Iteration 33/1000 | Loss: 0.00003724
Iteration 34/1000 | Loss: 0.00003724
Iteration 35/1000 | Loss: 0.00003723
Iteration 36/1000 | Loss: 0.00003723
Iteration 37/1000 | Loss: 0.00003723
Iteration 38/1000 | Loss: 0.00003723
Iteration 39/1000 | Loss: 0.00003722
Iteration 40/1000 | Loss: 0.00003722
Iteration 41/1000 | Loss: 0.00003722
Iteration 42/1000 | Loss: 0.00003721
Iteration 43/1000 | Loss: 0.00003721
Iteration 44/1000 | Loss: 0.00003721
Iteration 45/1000 | Loss: 0.00003721
Iteration 46/1000 | Loss: 0.00003721
Iteration 47/1000 | Loss: 0.00003721
Iteration 48/1000 | Loss: 0.00003721
Iteration 49/1000 | Loss: 0.00003721
Iteration 50/1000 | Loss: 0.00003721
Iteration 51/1000 | Loss: 0.00003721
Iteration 52/1000 | Loss: 0.00003721
Iteration 53/1000 | Loss: 0.00003721
Iteration 54/1000 | Loss: 0.00003721
Iteration 55/1000 | Loss: 0.00003721
Iteration 56/1000 | Loss: 0.00003721
Iteration 57/1000 | Loss: 0.00003720
Iteration 58/1000 | Loss: 0.00003720
Iteration 59/1000 | Loss: 0.00003720
Iteration 60/1000 | Loss: 0.00003720
Iteration 61/1000 | Loss: 0.00003720
Iteration 62/1000 | Loss: 0.00003720
Iteration 63/1000 | Loss: 0.00003720
Iteration 64/1000 | Loss: 0.00003720
Iteration 65/1000 | Loss: 0.00003719
Iteration 66/1000 | Loss: 0.00003719
Iteration 67/1000 | Loss: 0.00003719
Iteration 68/1000 | Loss: 0.00003719
Iteration 69/1000 | Loss: 0.00003719
Iteration 70/1000 | Loss: 0.00003719
Iteration 71/1000 | Loss: 0.00003719
Iteration 72/1000 | Loss: 0.00003719
Iteration 73/1000 | Loss: 0.00003719
Iteration 74/1000 | Loss: 0.00003719
Iteration 75/1000 | Loss: 0.00003719
Iteration 76/1000 | Loss: 0.00003719
Iteration 77/1000 | Loss: 0.00003719
Iteration 78/1000 | Loss: 0.00003719
Iteration 79/1000 | Loss: 0.00003719
Iteration 80/1000 | Loss: 0.00003719
Iteration 81/1000 | Loss: 0.00003719
Iteration 82/1000 | Loss: 0.00003719
Iteration 83/1000 | Loss: 0.00003719
Iteration 84/1000 | Loss: 0.00003719
Iteration 85/1000 | Loss: 0.00003718
Iteration 86/1000 | Loss: 0.00003718
Iteration 87/1000 | Loss: 0.00003718
Iteration 88/1000 | Loss: 0.00003718
Iteration 89/1000 | Loss: 0.00003718
Iteration 90/1000 | Loss: 0.00003718
Iteration 91/1000 | Loss: 0.00003718
Iteration 92/1000 | Loss: 0.00003718
Iteration 93/1000 | Loss: 0.00003718
Iteration 94/1000 | Loss: 0.00003718
Iteration 95/1000 | Loss: 0.00003718
Iteration 96/1000 | Loss: 0.00003718
Iteration 97/1000 | Loss: 0.00003718
Iteration 98/1000 | Loss: 0.00003718
Iteration 99/1000 | Loss: 0.00003718
Iteration 100/1000 | Loss: 0.00003718
Iteration 101/1000 | Loss: 0.00003718
Iteration 102/1000 | Loss: 0.00003718
Iteration 103/1000 | Loss: 0.00003718
Iteration 104/1000 | Loss: 0.00003718
Iteration 105/1000 | Loss: 0.00003718
Iteration 106/1000 | Loss: 0.00003718
Iteration 107/1000 | Loss: 0.00003718
Iteration 108/1000 | Loss: 0.00003718
Iteration 109/1000 | Loss: 0.00003718
Iteration 110/1000 | Loss: 0.00003718
Iteration 111/1000 | Loss: 0.00003718
Iteration 112/1000 | Loss: 0.00003718
Iteration 113/1000 | Loss: 0.00003718
Iteration 114/1000 | Loss: 0.00003718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [3.718186781043187e-05, 3.718186781043187e-05, 3.718186781043187e-05, 3.718186781043187e-05, 3.718186781043187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.718186781043187e-05

Optimization complete. Final v2v error: 4.964096546173096 mm

Highest mean error: 6.2244873046875 mm for frame 39

Lowest mean error: 4.4516801834106445 mm for frame 73

Saving results

Total time: 33.22765922546387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426729
Iteration 2/25 | Loss: 0.00134563
Iteration 3/25 | Loss: 0.00119042
Iteration 4/25 | Loss: 0.00116777
Iteration 5/25 | Loss: 0.00116279
Iteration 6/25 | Loss: 0.00116050
Iteration 7/25 | Loss: 0.00115929
Iteration 8/25 | Loss: 0.00115916
Iteration 9/25 | Loss: 0.00115916
Iteration 10/25 | Loss: 0.00115916
Iteration 11/25 | Loss: 0.00115916
Iteration 12/25 | Loss: 0.00115916
Iteration 13/25 | Loss: 0.00115916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011591619113460183, 0.0011591619113460183, 0.0011591619113460183, 0.0011591619113460183, 0.0011591619113460183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011591619113460183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23195732
Iteration 2/25 | Loss: 0.00108403
Iteration 3/25 | Loss: 0.00108402
Iteration 4/25 | Loss: 0.00108402
Iteration 5/25 | Loss: 0.00108402
Iteration 6/25 | Loss: 0.00108402
Iteration 7/25 | Loss: 0.00108402
Iteration 8/25 | Loss: 0.00108402
Iteration 9/25 | Loss: 0.00108402
Iteration 10/25 | Loss: 0.00108402
Iteration 11/25 | Loss: 0.00108402
Iteration 12/25 | Loss: 0.00108402
Iteration 13/25 | Loss: 0.00108402
Iteration 14/25 | Loss: 0.00108402
Iteration 15/25 | Loss: 0.00108402
Iteration 16/25 | Loss: 0.00108402
Iteration 17/25 | Loss: 0.00108402
Iteration 18/25 | Loss: 0.00108402
Iteration 19/25 | Loss: 0.00108402
Iteration 20/25 | Loss: 0.00108402
Iteration 21/25 | Loss: 0.00108402
Iteration 22/25 | Loss: 0.00108402
Iteration 23/25 | Loss: 0.00108402
Iteration 24/25 | Loss: 0.00108402
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001084023155272007, 0.001084023155272007, 0.001084023155272007, 0.001084023155272007, 0.001084023155272007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001084023155272007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108402
Iteration 2/1000 | Loss: 0.00004187
Iteration 3/1000 | Loss: 0.00003263
Iteration 4/1000 | Loss: 0.00003009
Iteration 5/1000 | Loss: 0.00002843
Iteration 6/1000 | Loss: 0.00002730
Iteration 7/1000 | Loss: 0.00002647
Iteration 8/1000 | Loss: 0.00002606
Iteration 9/1000 | Loss: 0.00002581
Iteration 10/1000 | Loss: 0.00002574
Iteration 11/1000 | Loss: 0.00002557
Iteration 12/1000 | Loss: 0.00002556
Iteration 13/1000 | Loss: 0.00002555
Iteration 14/1000 | Loss: 0.00002554
Iteration 15/1000 | Loss: 0.00002549
Iteration 16/1000 | Loss: 0.00002548
Iteration 17/1000 | Loss: 0.00002547
Iteration 18/1000 | Loss: 0.00002545
Iteration 19/1000 | Loss: 0.00002544
Iteration 20/1000 | Loss: 0.00002543
Iteration 21/1000 | Loss: 0.00002541
Iteration 22/1000 | Loss: 0.00002540
Iteration 23/1000 | Loss: 0.00002540
Iteration 24/1000 | Loss: 0.00002540
Iteration 25/1000 | Loss: 0.00002539
Iteration 26/1000 | Loss: 0.00002539
Iteration 27/1000 | Loss: 0.00002538
Iteration 28/1000 | Loss: 0.00002538
Iteration 29/1000 | Loss: 0.00002535
Iteration 30/1000 | Loss: 0.00002534
Iteration 31/1000 | Loss: 0.00002534
Iteration 32/1000 | Loss: 0.00002534
Iteration 33/1000 | Loss: 0.00002534
Iteration 34/1000 | Loss: 0.00002534
Iteration 35/1000 | Loss: 0.00002532
Iteration 36/1000 | Loss: 0.00002531
Iteration 37/1000 | Loss: 0.00002531
Iteration 38/1000 | Loss: 0.00002531
Iteration 39/1000 | Loss: 0.00002530
Iteration 40/1000 | Loss: 0.00002530
Iteration 41/1000 | Loss: 0.00002529
Iteration 42/1000 | Loss: 0.00002529
Iteration 43/1000 | Loss: 0.00002528
Iteration 44/1000 | Loss: 0.00002528
Iteration 45/1000 | Loss: 0.00002528
Iteration 46/1000 | Loss: 0.00002528
Iteration 47/1000 | Loss: 0.00002528
Iteration 48/1000 | Loss: 0.00002528
Iteration 49/1000 | Loss: 0.00002527
Iteration 50/1000 | Loss: 0.00002527
Iteration 51/1000 | Loss: 0.00002527
Iteration 52/1000 | Loss: 0.00002527
Iteration 53/1000 | Loss: 0.00002527
Iteration 54/1000 | Loss: 0.00002527
Iteration 55/1000 | Loss: 0.00002527
Iteration 56/1000 | Loss: 0.00002527
Iteration 57/1000 | Loss: 0.00002526
Iteration 58/1000 | Loss: 0.00002526
Iteration 59/1000 | Loss: 0.00002525
Iteration 60/1000 | Loss: 0.00002524
Iteration 61/1000 | Loss: 0.00002524
Iteration 62/1000 | Loss: 0.00002524
Iteration 63/1000 | Loss: 0.00002524
Iteration 64/1000 | Loss: 0.00002524
Iteration 65/1000 | Loss: 0.00002524
Iteration 66/1000 | Loss: 0.00002524
Iteration 67/1000 | Loss: 0.00002523
Iteration 68/1000 | Loss: 0.00002523
Iteration 69/1000 | Loss: 0.00002523
Iteration 70/1000 | Loss: 0.00002523
Iteration 71/1000 | Loss: 0.00002522
Iteration 72/1000 | Loss: 0.00002522
Iteration 73/1000 | Loss: 0.00002520
Iteration 74/1000 | Loss: 0.00002520
Iteration 75/1000 | Loss: 0.00002519
Iteration 76/1000 | Loss: 0.00002519
Iteration 77/1000 | Loss: 0.00002518
Iteration 78/1000 | Loss: 0.00002518
Iteration 79/1000 | Loss: 0.00002518
Iteration 80/1000 | Loss: 0.00002518
Iteration 81/1000 | Loss: 0.00002517
Iteration 82/1000 | Loss: 0.00002517
Iteration 83/1000 | Loss: 0.00002517
Iteration 84/1000 | Loss: 0.00002517
Iteration 85/1000 | Loss: 0.00002517
Iteration 86/1000 | Loss: 0.00002517
Iteration 87/1000 | Loss: 0.00002517
Iteration 88/1000 | Loss: 0.00002517
Iteration 89/1000 | Loss: 0.00002517
Iteration 90/1000 | Loss: 0.00002517
Iteration 91/1000 | Loss: 0.00002517
Iteration 92/1000 | Loss: 0.00002517
Iteration 93/1000 | Loss: 0.00002516
Iteration 94/1000 | Loss: 0.00002516
Iteration 95/1000 | Loss: 0.00002516
Iteration 96/1000 | Loss: 0.00002516
Iteration 97/1000 | Loss: 0.00002516
Iteration 98/1000 | Loss: 0.00002516
Iteration 99/1000 | Loss: 0.00002515
Iteration 100/1000 | Loss: 0.00002515
Iteration 101/1000 | Loss: 0.00002515
Iteration 102/1000 | Loss: 0.00002515
Iteration 103/1000 | Loss: 0.00002515
Iteration 104/1000 | Loss: 0.00002515
Iteration 105/1000 | Loss: 0.00002515
Iteration 106/1000 | Loss: 0.00002515
Iteration 107/1000 | Loss: 0.00002515
Iteration 108/1000 | Loss: 0.00002515
Iteration 109/1000 | Loss: 0.00002514
Iteration 110/1000 | Loss: 0.00002514
Iteration 111/1000 | Loss: 0.00002514
Iteration 112/1000 | Loss: 0.00002514
Iteration 113/1000 | Loss: 0.00002514
Iteration 114/1000 | Loss: 0.00002514
Iteration 115/1000 | Loss: 0.00002514
Iteration 116/1000 | Loss: 0.00002514
Iteration 117/1000 | Loss: 0.00002514
Iteration 118/1000 | Loss: 0.00002513
Iteration 119/1000 | Loss: 0.00002513
Iteration 120/1000 | Loss: 0.00002513
Iteration 121/1000 | Loss: 0.00002513
Iteration 122/1000 | Loss: 0.00002513
Iteration 123/1000 | Loss: 0.00002513
Iteration 124/1000 | Loss: 0.00002513
Iteration 125/1000 | Loss: 0.00002513
Iteration 126/1000 | Loss: 0.00002513
Iteration 127/1000 | Loss: 0.00002513
Iteration 128/1000 | Loss: 0.00002513
Iteration 129/1000 | Loss: 0.00002512
Iteration 130/1000 | Loss: 0.00002512
Iteration 131/1000 | Loss: 0.00002512
Iteration 132/1000 | Loss: 0.00002512
Iteration 133/1000 | Loss: 0.00002512
Iteration 134/1000 | Loss: 0.00002512
Iteration 135/1000 | Loss: 0.00002512
Iteration 136/1000 | Loss: 0.00002512
Iteration 137/1000 | Loss: 0.00002512
Iteration 138/1000 | Loss: 0.00002512
Iteration 139/1000 | Loss: 0.00002512
Iteration 140/1000 | Loss: 0.00002512
Iteration 141/1000 | Loss: 0.00002512
Iteration 142/1000 | Loss: 0.00002511
Iteration 143/1000 | Loss: 0.00002511
Iteration 144/1000 | Loss: 0.00002511
Iteration 145/1000 | Loss: 0.00002511
Iteration 146/1000 | Loss: 0.00002511
Iteration 147/1000 | Loss: 0.00002511
Iteration 148/1000 | Loss: 0.00002511
Iteration 149/1000 | Loss: 0.00002511
Iteration 150/1000 | Loss: 0.00002511
Iteration 151/1000 | Loss: 0.00002511
Iteration 152/1000 | Loss: 0.00002511
Iteration 153/1000 | Loss: 0.00002511
Iteration 154/1000 | Loss: 0.00002511
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.511449929443188e-05, 2.511449929443188e-05, 2.511449929443188e-05, 2.511449929443188e-05, 2.511449929443188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.511449929443188e-05

Optimization complete. Final v2v error: 4.178887844085693 mm

Highest mean error: 4.546179294586182 mm for frame 125

Lowest mean error: 3.438873052597046 mm for frame 9

Saving results

Total time: 38.01270794868469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984217
Iteration 2/25 | Loss: 0.00330689
Iteration 3/25 | Loss: 0.00295158
Iteration 4/25 | Loss: 0.00159499
Iteration 5/25 | Loss: 0.00151297
Iteration 6/25 | Loss: 0.00159588
Iteration 7/25 | Loss: 0.00147599
Iteration 8/25 | Loss: 0.00144988
Iteration 9/25 | Loss: 0.00128423
Iteration 10/25 | Loss: 0.00125786
Iteration 11/25 | Loss: 0.00125801
Iteration 12/25 | Loss: 0.00133475
Iteration 13/25 | Loss: 0.00129049
Iteration 14/25 | Loss: 0.00127751
Iteration 15/25 | Loss: 0.00128063
Iteration 16/25 | Loss: 0.00126787
Iteration 17/25 | Loss: 0.00124558
Iteration 18/25 | Loss: 0.00123721
Iteration 19/25 | Loss: 0.00123383
Iteration 20/25 | Loss: 0.00127577
Iteration 21/25 | Loss: 0.00126715
Iteration 22/25 | Loss: 0.00125988
Iteration 23/25 | Loss: 0.00124056
Iteration 24/25 | Loss: 0.00123469
Iteration 25/25 | Loss: 0.00123354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41918850
Iteration 2/25 | Loss: 0.00101417
Iteration 3/25 | Loss: 0.00101417
Iteration 4/25 | Loss: 0.00101417
Iteration 5/25 | Loss: 0.00101417
Iteration 6/25 | Loss: 0.00101417
Iteration 7/25 | Loss: 0.00101417
Iteration 8/25 | Loss: 0.00101417
Iteration 9/25 | Loss: 0.00101417
Iteration 10/25 | Loss: 0.00101417
Iteration 11/25 | Loss: 0.00101417
Iteration 12/25 | Loss: 0.00101417
Iteration 13/25 | Loss: 0.00101417
Iteration 14/25 | Loss: 0.00101417
Iteration 15/25 | Loss: 0.00101417
Iteration 16/25 | Loss: 0.00101417
Iteration 17/25 | Loss: 0.00101417
Iteration 18/25 | Loss: 0.00101417
Iteration 19/25 | Loss: 0.00101417
Iteration 20/25 | Loss: 0.00101417
Iteration 21/25 | Loss: 0.00101417
Iteration 22/25 | Loss: 0.00101417
Iteration 23/25 | Loss: 0.00101417
Iteration 24/25 | Loss: 0.00101417
Iteration 25/25 | Loss: 0.00101417

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101417
Iteration 2/1000 | Loss: 0.00011851
Iteration 3/1000 | Loss: 0.00010202
Iteration 4/1000 | Loss: 0.00011172
Iteration 5/1000 | Loss: 0.00157083
Iteration 6/1000 | Loss: 0.00028484
Iteration 7/1000 | Loss: 0.00192707
Iteration 8/1000 | Loss: 0.00017175
Iteration 9/1000 | Loss: 0.00140187
Iteration 10/1000 | Loss: 0.00285254
Iteration 11/1000 | Loss: 0.00012448
Iteration 12/1000 | Loss: 0.00072889
Iteration 13/1000 | Loss: 0.00073150
Iteration 14/1000 | Loss: 0.00015029
Iteration 15/1000 | Loss: 0.00011523
Iteration 16/1000 | Loss: 0.00252477
Iteration 17/1000 | Loss: 0.00081231
Iteration 18/1000 | Loss: 0.00108548
Iteration 19/1000 | Loss: 0.00137213
Iteration 20/1000 | Loss: 0.00103576
Iteration 21/1000 | Loss: 0.00205359
Iteration 22/1000 | Loss: 0.00082833
Iteration 23/1000 | Loss: 0.00079823
Iteration 24/1000 | Loss: 0.00033639
Iteration 25/1000 | Loss: 0.00121315
Iteration 26/1000 | Loss: 0.00277782
Iteration 27/1000 | Loss: 0.00008919
Iteration 28/1000 | Loss: 0.00005968
Iteration 29/1000 | Loss: 0.00193479
Iteration 30/1000 | Loss: 0.00230048
Iteration 31/1000 | Loss: 0.00165076
Iteration 32/1000 | Loss: 0.00005416
Iteration 33/1000 | Loss: 0.00004991
Iteration 34/1000 | Loss: 0.00125183
Iteration 35/1000 | Loss: 0.00005575
Iteration 36/1000 | Loss: 0.00142475
Iteration 37/1000 | Loss: 0.00053602
Iteration 38/1000 | Loss: 0.00162525
Iteration 39/1000 | Loss: 0.00100774
Iteration 40/1000 | Loss: 0.00098737
Iteration 41/1000 | Loss: 0.00130141
Iteration 42/1000 | Loss: 0.00103533
Iteration 43/1000 | Loss: 0.00141474
Iteration 44/1000 | Loss: 0.00089317
Iteration 45/1000 | Loss: 0.00179183
Iteration 46/1000 | Loss: 0.00125344
Iteration 47/1000 | Loss: 0.00081657
Iteration 48/1000 | Loss: 0.00114348
Iteration 49/1000 | Loss: 0.00182822
Iteration 50/1000 | Loss: 0.00167041
Iteration 51/1000 | Loss: 0.00069417
Iteration 52/1000 | Loss: 0.00119321
Iteration 53/1000 | Loss: 0.00120588
Iteration 54/1000 | Loss: 0.00109528
Iteration 55/1000 | Loss: 0.00138180
Iteration 56/1000 | Loss: 0.00122255
Iteration 57/1000 | Loss: 0.00203987
Iteration 58/1000 | Loss: 0.00197590
Iteration 59/1000 | Loss: 0.00029120
Iteration 60/1000 | Loss: 0.00242878
Iteration 61/1000 | Loss: 0.00136465
Iteration 62/1000 | Loss: 0.00082345
Iteration 63/1000 | Loss: 0.00044796
Iteration 64/1000 | Loss: 0.00075365
Iteration 65/1000 | Loss: 0.00068106
Iteration 66/1000 | Loss: 0.00007292
Iteration 67/1000 | Loss: 0.00006180
Iteration 68/1000 | Loss: 0.00111918
Iteration 69/1000 | Loss: 0.00081834
Iteration 70/1000 | Loss: 0.00013539
Iteration 71/1000 | Loss: 0.00028588
Iteration 72/1000 | Loss: 0.00025641
Iteration 73/1000 | Loss: 0.00006014
Iteration 74/1000 | Loss: 0.00029524
Iteration 75/1000 | Loss: 0.00025206
Iteration 76/1000 | Loss: 0.00033263
Iteration 77/1000 | Loss: 0.00008711
Iteration 78/1000 | Loss: 0.00006360
Iteration 79/1000 | Loss: 0.00005255
Iteration 80/1000 | Loss: 0.00004686
Iteration 81/1000 | Loss: 0.00004390
Iteration 82/1000 | Loss: 0.00004228
Iteration 83/1000 | Loss: 0.00004146
Iteration 84/1000 | Loss: 0.00004078
Iteration 85/1000 | Loss: 0.00004048
Iteration 86/1000 | Loss: 0.00004018
Iteration 87/1000 | Loss: 0.00003990
Iteration 88/1000 | Loss: 0.00003969
Iteration 89/1000 | Loss: 0.00003951
Iteration 90/1000 | Loss: 0.00003945
Iteration 91/1000 | Loss: 0.00003941
Iteration 92/1000 | Loss: 0.00003920
Iteration 93/1000 | Loss: 0.00003899
Iteration 94/1000 | Loss: 0.00003877
Iteration 95/1000 | Loss: 0.00003869
Iteration 96/1000 | Loss: 0.00005762
Iteration 97/1000 | Loss: 0.00007943
Iteration 98/1000 | Loss: 0.00004883
Iteration 99/1000 | Loss: 0.00007037
Iteration 100/1000 | Loss: 0.00006650
Iteration 101/1000 | Loss: 0.00004543
Iteration 102/1000 | Loss: 0.00004251
Iteration 103/1000 | Loss: 0.00006761
Iteration 104/1000 | Loss: 0.00006010
Iteration 105/1000 | Loss: 0.00008915
Iteration 106/1000 | Loss: 0.00005823
Iteration 107/1000 | Loss: 0.00008458
Iteration 108/1000 | Loss: 0.00004202
Iteration 109/1000 | Loss: 0.00004042
Iteration 110/1000 | Loss: 0.00003951
Iteration 111/1000 | Loss: 0.00003904
Iteration 112/1000 | Loss: 0.00003870
Iteration 113/1000 | Loss: 0.00003851
Iteration 114/1000 | Loss: 0.00003843
Iteration 115/1000 | Loss: 0.00003837
Iteration 116/1000 | Loss: 0.00003836
Iteration 117/1000 | Loss: 0.00003832
Iteration 118/1000 | Loss: 0.00003831
Iteration 119/1000 | Loss: 0.00003830
Iteration 120/1000 | Loss: 0.00003830
Iteration 121/1000 | Loss: 0.00003828
Iteration 122/1000 | Loss: 0.00003827
Iteration 123/1000 | Loss: 0.00003827
Iteration 124/1000 | Loss: 0.00003826
Iteration 125/1000 | Loss: 0.00003826
Iteration 126/1000 | Loss: 0.00003826
Iteration 127/1000 | Loss: 0.00003825
Iteration 128/1000 | Loss: 0.00003825
Iteration 129/1000 | Loss: 0.00003825
Iteration 130/1000 | Loss: 0.00003825
Iteration 131/1000 | Loss: 0.00003825
Iteration 132/1000 | Loss: 0.00003824
Iteration 133/1000 | Loss: 0.00003824
Iteration 134/1000 | Loss: 0.00003824
Iteration 135/1000 | Loss: 0.00003824
Iteration 136/1000 | Loss: 0.00003824
Iteration 137/1000 | Loss: 0.00003824
Iteration 138/1000 | Loss: 0.00003823
Iteration 139/1000 | Loss: 0.00003823
Iteration 140/1000 | Loss: 0.00003823
Iteration 141/1000 | Loss: 0.00003823
Iteration 142/1000 | Loss: 0.00003822
Iteration 143/1000 | Loss: 0.00003822
Iteration 144/1000 | Loss: 0.00003821
Iteration 145/1000 | Loss: 0.00003821
Iteration 146/1000 | Loss: 0.00003820
Iteration 147/1000 | Loss: 0.00003820
Iteration 148/1000 | Loss: 0.00003820
Iteration 149/1000 | Loss: 0.00003819
Iteration 150/1000 | Loss: 0.00003819
Iteration 151/1000 | Loss: 0.00003819
Iteration 152/1000 | Loss: 0.00003818
Iteration 153/1000 | Loss: 0.00003818
Iteration 154/1000 | Loss: 0.00003818
Iteration 155/1000 | Loss: 0.00003818
Iteration 156/1000 | Loss: 0.00003818
Iteration 157/1000 | Loss: 0.00003818
Iteration 158/1000 | Loss: 0.00003818
Iteration 159/1000 | Loss: 0.00003818
Iteration 160/1000 | Loss: 0.00003817
Iteration 161/1000 | Loss: 0.00003817
Iteration 162/1000 | Loss: 0.00003817
Iteration 163/1000 | Loss: 0.00003817
Iteration 164/1000 | Loss: 0.00003817
Iteration 165/1000 | Loss: 0.00003817
Iteration 166/1000 | Loss: 0.00003817
Iteration 167/1000 | Loss: 0.00003817
Iteration 168/1000 | Loss: 0.00003817
Iteration 169/1000 | Loss: 0.00003817
Iteration 170/1000 | Loss: 0.00003816
Iteration 171/1000 | Loss: 0.00003816
Iteration 172/1000 | Loss: 0.00003816
Iteration 173/1000 | Loss: 0.00003816
Iteration 174/1000 | Loss: 0.00003816
Iteration 175/1000 | Loss: 0.00003816
Iteration 176/1000 | Loss: 0.00003816
Iteration 177/1000 | Loss: 0.00003816
Iteration 178/1000 | Loss: 0.00003816
Iteration 179/1000 | Loss: 0.00003816
Iteration 180/1000 | Loss: 0.00003816
Iteration 181/1000 | Loss: 0.00003816
Iteration 182/1000 | Loss: 0.00003816
Iteration 183/1000 | Loss: 0.00003816
Iteration 184/1000 | Loss: 0.00003816
Iteration 185/1000 | Loss: 0.00003815
Iteration 186/1000 | Loss: 0.00003815
Iteration 187/1000 | Loss: 0.00003815
Iteration 188/1000 | Loss: 0.00003815
Iteration 189/1000 | Loss: 0.00003815
Iteration 190/1000 | Loss: 0.00003815
Iteration 191/1000 | Loss: 0.00003815
Iteration 192/1000 | Loss: 0.00003815
Iteration 193/1000 | Loss: 0.00003815
Iteration 194/1000 | Loss: 0.00003815
Iteration 195/1000 | Loss: 0.00003815
Iteration 196/1000 | Loss: 0.00003814
Iteration 197/1000 | Loss: 0.00003814
Iteration 198/1000 | Loss: 0.00003814
Iteration 199/1000 | Loss: 0.00003814
Iteration 200/1000 | Loss: 0.00003814
Iteration 201/1000 | Loss: 0.00003814
Iteration 202/1000 | Loss: 0.00003814
Iteration 203/1000 | Loss: 0.00003814
Iteration 204/1000 | Loss: 0.00003813
Iteration 205/1000 | Loss: 0.00003813
Iteration 206/1000 | Loss: 0.00003813
Iteration 207/1000 | Loss: 0.00003813
Iteration 208/1000 | Loss: 0.00003813
Iteration 209/1000 | Loss: 0.00003812
Iteration 210/1000 | Loss: 0.00003812
Iteration 211/1000 | Loss: 0.00003812
Iteration 212/1000 | Loss: 0.00003812
Iteration 213/1000 | Loss: 0.00003812
Iteration 214/1000 | Loss: 0.00003812
Iteration 215/1000 | Loss: 0.00003812
Iteration 216/1000 | Loss: 0.00003812
Iteration 217/1000 | Loss: 0.00003812
Iteration 218/1000 | Loss: 0.00003812
Iteration 219/1000 | Loss: 0.00003812
Iteration 220/1000 | Loss: 0.00003812
Iteration 221/1000 | Loss: 0.00003812
Iteration 222/1000 | Loss: 0.00003812
Iteration 223/1000 | Loss: 0.00003812
Iteration 224/1000 | Loss: 0.00003812
Iteration 225/1000 | Loss: 0.00003812
Iteration 226/1000 | Loss: 0.00003812
Iteration 227/1000 | Loss: 0.00003812
Iteration 228/1000 | Loss: 0.00003812
Iteration 229/1000 | Loss: 0.00003812
Iteration 230/1000 | Loss: 0.00003812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [3.8115707866381854e-05, 3.8115707866381854e-05, 3.8115707866381854e-05, 3.8115707866381854e-05, 3.8115707866381854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8115707866381854e-05

Optimization complete. Final v2v error: 5.151285648345947 mm

Highest mean error: 6.944973945617676 mm for frame 70

Lowest mean error: 4.545835494995117 mm for frame 137

Saving results

Total time: 206.55764412879944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459541
Iteration 2/25 | Loss: 0.00123897
Iteration 3/25 | Loss: 0.00116811
Iteration 4/25 | Loss: 0.00115919
Iteration 5/25 | Loss: 0.00115443
Iteration 6/25 | Loss: 0.00115339
Iteration 7/25 | Loss: 0.00115339
Iteration 8/25 | Loss: 0.00115339
Iteration 9/25 | Loss: 0.00115339
Iteration 10/25 | Loss: 0.00115339
Iteration 11/25 | Loss: 0.00115339
Iteration 12/25 | Loss: 0.00115339
Iteration 13/25 | Loss: 0.00115339
Iteration 14/25 | Loss: 0.00115339
Iteration 15/25 | Loss: 0.00115339
Iteration 16/25 | Loss: 0.00115339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011533867800608277, 0.0011533867800608277, 0.0011533867800608277, 0.0011533867800608277, 0.0011533867800608277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011533867800608277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22474277
Iteration 2/25 | Loss: 0.00093384
Iteration 3/25 | Loss: 0.00093384
Iteration 4/25 | Loss: 0.00093384
Iteration 5/25 | Loss: 0.00093384
Iteration 6/25 | Loss: 0.00093384
Iteration 7/25 | Loss: 0.00093384
Iteration 8/25 | Loss: 0.00093384
Iteration 9/25 | Loss: 0.00093384
Iteration 10/25 | Loss: 0.00093384
Iteration 11/25 | Loss: 0.00093384
Iteration 12/25 | Loss: 0.00093384
Iteration 13/25 | Loss: 0.00093384
Iteration 14/25 | Loss: 0.00093384
Iteration 15/25 | Loss: 0.00093384
Iteration 16/25 | Loss: 0.00093384
Iteration 17/25 | Loss: 0.00093384
Iteration 18/25 | Loss: 0.00093384
Iteration 19/25 | Loss: 0.00093384
Iteration 20/25 | Loss: 0.00093384
Iteration 21/25 | Loss: 0.00093384
Iteration 22/25 | Loss: 0.00093384
Iteration 23/25 | Loss: 0.00093384
Iteration 24/25 | Loss: 0.00093384
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009338370873592794, 0.0009338370873592794, 0.0009338370873592794, 0.0009338370873592794, 0.0009338370873592794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009338370873592794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093384
Iteration 2/1000 | Loss: 0.00005048
Iteration 3/1000 | Loss: 0.00003703
Iteration 4/1000 | Loss: 0.00003378
Iteration 5/1000 | Loss: 0.00003182
Iteration 6/1000 | Loss: 0.00003056
Iteration 7/1000 | Loss: 0.00002982
Iteration 8/1000 | Loss: 0.00002928
Iteration 9/1000 | Loss: 0.00002919
Iteration 10/1000 | Loss: 0.00002918
Iteration 11/1000 | Loss: 0.00002903
Iteration 12/1000 | Loss: 0.00002894
Iteration 13/1000 | Loss: 0.00002886
Iteration 14/1000 | Loss: 0.00002879
Iteration 15/1000 | Loss: 0.00002879
Iteration 16/1000 | Loss: 0.00002878
Iteration 17/1000 | Loss: 0.00002878
Iteration 18/1000 | Loss: 0.00002877
Iteration 19/1000 | Loss: 0.00002874
Iteration 20/1000 | Loss: 0.00002874
Iteration 21/1000 | Loss: 0.00002873
Iteration 22/1000 | Loss: 0.00002873
Iteration 23/1000 | Loss: 0.00002872
Iteration 24/1000 | Loss: 0.00002871
Iteration 25/1000 | Loss: 0.00002870
Iteration 26/1000 | Loss: 0.00002870
Iteration 27/1000 | Loss: 0.00002870
Iteration 28/1000 | Loss: 0.00002870
Iteration 29/1000 | Loss: 0.00002870
Iteration 30/1000 | Loss: 0.00002870
Iteration 31/1000 | Loss: 0.00002870
Iteration 32/1000 | Loss: 0.00002870
Iteration 33/1000 | Loss: 0.00002870
Iteration 34/1000 | Loss: 0.00002870
Iteration 35/1000 | Loss: 0.00002869
Iteration 36/1000 | Loss: 0.00002869
Iteration 37/1000 | Loss: 0.00002869
Iteration 38/1000 | Loss: 0.00002869
Iteration 39/1000 | Loss: 0.00002868
Iteration 40/1000 | Loss: 0.00002868
Iteration 41/1000 | Loss: 0.00002868
Iteration 42/1000 | Loss: 0.00002868
Iteration 43/1000 | Loss: 0.00002867
Iteration 44/1000 | Loss: 0.00002867
Iteration 45/1000 | Loss: 0.00002867
Iteration 46/1000 | Loss: 0.00002867
Iteration 47/1000 | Loss: 0.00002867
Iteration 48/1000 | Loss: 0.00002867
Iteration 49/1000 | Loss: 0.00002867
Iteration 50/1000 | Loss: 0.00002866
Iteration 51/1000 | Loss: 0.00002866
Iteration 52/1000 | Loss: 0.00002866
Iteration 53/1000 | Loss: 0.00002866
Iteration 54/1000 | Loss: 0.00002866
Iteration 55/1000 | Loss: 0.00002866
Iteration 56/1000 | Loss: 0.00002866
Iteration 57/1000 | Loss: 0.00002866
Iteration 58/1000 | Loss: 0.00002866
Iteration 59/1000 | Loss: 0.00002866
Iteration 60/1000 | Loss: 0.00002866
Iteration 61/1000 | Loss: 0.00002866
Iteration 62/1000 | Loss: 0.00002866
Iteration 63/1000 | Loss: 0.00002866
Iteration 64/1000 | Loss: 0.00002866
Iteration 65/1000 | Loss: 0.00002866
Iteration 66/1000 | Loss: 0.00002866
Iteration 67/1000 | Loss: 0.00002865
Iteration 68/1000 | Loss: 0.00002865
Iteration 69/1000 | Loss: 0.00002865
Iteration 70/1000 | Loss: 0.00002865
Iteration 71/1000 | Loss: 0.00002865
Iteration 72/1000 | Loss: 0.00002865
Iteration 73/1000 | Loss: 0.00002865
Iteration 74/1000 | Loss: 0.00002865
Iteration 75/1000 | Loss: 0.00002864
Iteration 76/1000 | Loss: 0.00002864
Iteration 77/1000 | Loss: 0.00002864
Iteration 78/1000 | Loss: 0.00002864
Iteration 79/1000 | Loss: 0.00002864
Iteration 80/1000 | Loss: 0.00002864
Iteration 81/1000 | Loss: 0.00002864
Iteration 82/1000 | Loss: 0.00002864
Iteration 83/1000 | Loss: 0.00002864
Iteration 84/1000 | Loss: 0.00002863
Iteration 85/1000 | Loss: 0.00002863
Iteration 86/1000 | Loss: 0.00002863
Iteration 87/1000 | Loss: 0.00002863
Iteration 88/1000 | Loss: 0.00002863
Iteration 89/1000 | Loss: 0.00002863
Iteration 90/1000 | Loss: 0.00002863
Iteration 91/1000 | Loss: 0.00002863
Iteration 92/1000 | Loss: 0.00002863
Iteration 93/1000 | Loss: 0.00002863
Iteration 94/1000 | Loss: 0.00002863
Iteration 95/1000 | Loss: 0.00002863
Iteration 96/1000 | Loss: 0.00002863
Iteration 97/1000 | Loss: 0.00002863
Iteration 98/1000 | Loss: 0.00002863
Iteration 99/1000 | Loss: 0.00002863
Iteration 100/1000 | Loss: 0.00002862
Iteration 101/1000 | Loss: 0.00002862
Iteration 102/1000 | Loss: 0.00002862
Iteration 103/1000 | Loss: 0.00002862
Iteration 104/1000 | Loss: 0.00002862
Iteration 105/1000 | Loss: 0.00002862
Iteration 106/1000 | Loss: 0.00002861
Iteration 107/1000 | Loss: 0.00002861
Iteration 108/1000 | Loss: 0.00002861
Iteration 109/1000 | Loss: 0.00002860
Iteration 110/1000 | Loss: 0.00002860
Iteration 111/1000 | Loss: 0.00002859
Iteration 112/1000 | Loss: 0.00002859
Iteration 113/1000 | Loss: 0.00002859
Iteration 114/1000 | Loss: 0.00002859
Iteration 115/1000 | Loss: 0.00002858
Iteration 116/1000 | Loss: 0.00002858
Iteration 117/1000 | Loss: 0.00002857
Iteration 118/1000 | Loss: 0.00002857
Iteration 119/1000 | Loss: 0.00002857
Iteration 120/1000 | Loss: 0.00002856
Iteration 121/1000 | Loss: 0.00002856
Iteration 122/1000 | Loss: 0.00002856
Iteration 123/1000 | Loss: 0.00002855
Iteration 124/1000 | Loss: 0.00002855
Iteration 125/1000 | Loss: 0.00002855
Iteration 126/1000 | Loss: 0.00002855
Iteration 127/1000 | Loss: 0.00002855
Iteration 128/1000 | Loss: 0.00002855
Iteration 129/1000 | Loss: 0.00002855
Iteration 130/1000 | Loss: 0.00002855
Iteration 131/1000 | Loss: 0.00002855
Iteration 132/1000 | Loss: 0.00002855
Iteration 133/1000 | Loss: 0.00002855
Iteration 134/1000 | Loss: 0.00002854
Iteration 135/1000 | Loss: 0.00002854
Iteration 136/1000 | Loss: 0.00002854
Iteration 137/1000 | Loss: 0.00002854
Iteration 138/1000 | Loss: 0.00002854
Iteration 139/1000 | Loss: 0.00002854
Iteration 140/1000 | Loss: 0.00002853
Iteration 141/1000 | Loss: 0.00002853
Iteration 142/1000 | Loss: 0.00002853
Iteration 143/1000 | Loss: 0.00002853
Iteration 144/1000 | Loss: 0.00002853
Iteration 145/1000 | Loss: 0.00002853
Iteration 146/1000 | Loss: 0.00002853
Iteration 147/1000 | Loss: 0.00002853
Iteration 148/1000 | Loss: 0.00002853
Iteration 149/1000 | Loss: 0.00002853
Iteration 150/1000 | Loss: 0.00002853
Iteration 151/1000 | Loss: 0.00002853
Iteration 152/1000 | Loss: 0.00002853
Iteration 153/1000 | Loss: 0.00002853
Iteration 154/1000 | Loss: 0.00002852
Iteration 155/1000 | Loss: 0.00002852
Iteration 156/1000 | Loss: 0.00002852
Iteration 157/1000 | Loss: 0.00002852
Iteration 158/1000 | Loss: 0.00002852
Iteration 159/1000 | Loss: 0.00002852
Iteration 160/1000 | Loss: 0.00002852
Iteration 161/1000 | Loss: 0.00002851
Iteration 162/1000 | Loss: 0.00002851
Iteration 163/1000 | Loss: 0.00002851
Iteration 164/1000 | Loss: 0.00002851
Iteration 165/1000 | Loss: 0.00002851
Iteration 166/1000 | Loss: 0.00002851
Iteration 167/1000 | Loss: 0.00002851
Iteration 168/1000 | Loss: 0.00002851
Iteration 169/1000 | Loss: 0.00002850
Iteration 170/1000 | Loss: 0.00002850
Iteration 171/1000 | Loss: 0.00002850
Iteration 172/1000 | Loss: 0.00002850
Iteration 173/1000 | Loss: 0.00002849
Iteration 174/1000 | Loss: 0.00002849
Iteration 175/1000 | Loss: 0.00002849
Iteration 176/1000 | Loss: 0.00002849
Iteration 177/1000 | Loss: 0.00002849
Iteration 178/1000 | Loss: 0.00002849
Iteration 179/1000 | Loss: 0.00002849
Iteration 180/1000 | Loss: 0.00002849
Iteration 181/1000 | Loss: 0.00002849
Iteration 182/1000 | Loss: 0.00002849
Iteration 183/1000 | Loss: 0.00002849
Iteration 184/1000 | Loss: 0.00002849
Iteration 185/1000 | Loss: 0.00002848
Iteration 186/1000 | Loss: 0.00002848
Iteration 187/1000 | Loss: 0.00002848
Iteration 188/1000 | Loss: 0.00002848
Iteration 189/1000 | Loss: 0.00002848
Iteration 190/1000 | Loss: 0.00002848
Iteration 191/1000 | Loss: 0.00002848
Iteration 192/1000 | Loss: 0.00002848
Iteration 193/1000 | Loss: 0.00002848
Iteration 194/1000 | Loss: 0.00002848
Iteration 195/1000 | Loss: 0.00002848
Iteration 196/1000 | Loss: 0.00002848
Iteration 197/1000 | Loss: 0.00002848
Iteration 198/1000 | Loss: 0.00002848
Iteration 199/1000 | Loss: 0.00002848
Iteration 200/1000 | Loss: 0.00002848
Iteration 201/1000 | Loss: 0.00002848
Iteration 202/1000 | Loss: 0.00002848
Iteration 203/1000 | Loss: 0.00002848
Iteration 204/1000 | Loss: 0.00002848
Iteration 205/1000 | Loss: 0.00002848
Iteration 206/1000 | Loss: 0.00002848
Iteration 207/1000 | Loss: 0.00002848
Iteration 208/1000 | Loss: 0.00002848
Iteration 209/1000 | Loss: 0.00002848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.8476039005909115e-05, 2.8476039005909115e-05, 2.8476039005909115e-05, 2.8476039005909115e-05, 2.8476039005909115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8476039005909115e-05

Optimization complete. Final v2v error: 4.51940393447876 mm

Highest mean error: 4.7373366355896 mm for frame 149

Lowest mean error: 4.307916164398193 mm for frame 50

Saving results

Total time: 37.39693474769592
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00561984
Iteration 2/25 | Loss: 0.00165406
Iteration 3/25 | Loss: 0.00136079
Iteration 4/25 | Loss: 0.00131444
Iteration 5/25 | Loss: 0.00140406
Iteration 6/25 | Loss: 0.00137794
Iteration 7/25 | Loss: 0.00129569
Iteration 8/25 | Loss: 0.00127020
Iteration 9/25 | Loss: 0.00126493
Iteration 10/25 | Loss: 0.00126347
Iteration 11/25 | Loss: 0.00126579
Iteration 12/25 | Loss: 0.00126252
Iteration 13/25 | Loss: 0.00126456
Iteration 14/25 | Loss: 0.00126121
Iteration 15/25 | Loss: 0.00126018
Iteration 16/25 | Loss: 0.00125975
Iteration 17/25 | Loss: 0.00125962
Iteration 18/25 | Loss: 0.00125962
Iteration 19/25 | Loss: 0.00125962
Iteration 20/25 | Loss: 0.00125962
Iteration 21/25 | Loss: 0.00125962
Iteration 22/25 | Loss: 0.00125961
Iteration 23/25 | Loss: 0.00125961
Iteration 24/25 | Loss: 0.00125961
Iteration 25/25 | Loss: 0.00125961

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24041927
Iteration 2/25 | Loss: 0.00090046
Iteration 3/25 | Loss: 0.00090045
Iteration 4/25 | Loss: 0.00090045
Iteration 5/25 | Loss: 0.00090045
Iteration 6/25 | Loss: 0.00090045
Iteration 7/25 | Loss: 0.00090045
Iteration 8/25 | Loss: 0.00090045
Iteration 9/25 | Loss: 0.00090045
Iteration 10/25 | Loss: 0.00090045
Iteration 11/25 | Loss: 0.00090045
Iteration 12/25 | Loss: 0.00090045
Iteration 13/25 | Loss: 0.00090045
Iteration 14/25 | Loss: 0.00090045
Iteration 15/25 | Loss: 0.00090045
Iteration 16/25 | Loss: 0.00090045
Iteration 17/25 | Loss: 0.00090045
Iteration 18/25 | Loss: 0.00090045
Iteration 19/25 | Loss: 0.00090045
Iteration 20/25 | Loss: 0.00090045
Iteration 21/25 | Loss: 0.00090045
Iteration 22/25 | Loss: 0.00090045
Iteration 23/25 | Loss: 0.00090045
Iteration 24/25 | Loss: 0.00090045
Iteration 25/25 | Loss: 0.00090045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090045
Iteration 2/1000 | Loss: 0.00079796
Iteration 3/1000 | Loss: 0.00093233
Iteration 4/1000 | Loss: 0.00020110
Iteration 5/1000 | Loss: 0.00012430
Iteration 6/1000 | Loss: 0.00007076
Iteration 7/1000 | Loss: 0.00005118
Iteration 8/1000 | Loss: 0.00004710
Iteration 9/1000 | Loss: 0.00004296
Iteration 10/1000 | Loss: 0.00004071
Iteration 11/1000 | Loss: 0.00003975
Iteration 12/1000 | Loss: 0.00003889
Iteration 13/1000 | Loss: 0.00003828
Iteration 14/1000 | Loss: 0.00012863
Iteration 15/1000 | Loss: 0.00068545
Iteration 16/1000 | Loss: 0.00028835
Iteration 17/1000 | Loss: 0.00058438
Iteration 18/1000 | Loss: 0.00025301
Iteration 19/1000 | Loss: 0.00003889
Iteration 20/1000 | Loss: 0.00003750
Iteration 21/1000 | Loss: 0.00003686
Iteration 22/1000 | Loss: 0.00003655
Iteration 23/1000 | Loss: 0.00003632
Iteration 24/1000 | Loss: 0.00003613
Iteration 25/1000 | Loss: 0.00012281
Iteration 26/1000 | Loss: 0.00012281
Iteration 27/1000 | Loss: 0.00013109
Iteration 28/1000 | Loss: 0.00012331
Iteration 29/1000 | Loss: 0.00004192
Iteration 30/1000 | Loss: 0.00037786
Iteration 31/1000 | Loss: 0.00046324
Iteration 32/1000 | Loss: 0.00014048
Iteration 33/1000 | Loss: 0.00014149
Iteration 34/1000 | Loss: 0.00004230
Iteration 35/1000 | Loss: 0.00003920
Iteration 36/1000 | Loss: 0.00011847
Iteration 37/1000 | Loss: 0.00037076
Iteration 38/1000 | Loss: 0.00036024
Iteration 39/1000 | Loss: 0.00033135
Iteration 40/1000 | Loss: 0.00008505
Iteration 41/1000 | Loss: 0.00015430
Iteration 42/1000 | Loss: 0.00005223
Iteration 43/1000 | Loss: 0.00017404
Iteration 44/1000 | Loss: 0.00010555
Iteration 45/1000 | Loss: 0.00015235
Iteration 46/1000 | Loss: 0.00005179
Iteration 47/1000 | Loss: 0.00004984
Iteration 48/1000 | Loss: 0.00034128
Iteration 49/1000 | Loss: 0.00015529
Iteration 50/1000 | Loss: 0.00009403
Iteration 51/1000 | Loss: 0.00004484
Iteration 52/1000 | Loss: 0.00004296
Iteration 53/1000 | Loss: 0.00004188
Iteration 54/1000 | Loss: 0.00004099
Iteration 55/1000 | Loss: 0.00004748
Iteration 56/1000 | Loss: 0.00012828
Iteration 57/1000 | Loss: 0.00007828
Iteration 58/1000 | Loss: 0.00012247
Iteration 59/1000 | Loss: 0.00013593
Iteration 60/1000 | Loss: 0.00005163
Iteration 61/1000 | Loss: 0.00012818
Iteration 62/1000 | Loss: 0.00016859
Iteration 63/1000 | Loss: 0.00004593
Iteration 64/1000 | Loss: 0.00004601
Iteration 65/1000 | Loss: 0.00013800
Iteration 66/1000 | Loss: 0.00007148
Iteration 67/1000 | Loss: 0.00004379
Iteration 68/1000 | Loss: 0.00004234
Iteration 69/1000 | Loss: 0.00004081
Iteration 70/1000 | Loss: 0.00003805
Iteration 71/1000 | Loss: 0.00003723
Iteration 72/1000 | Loss: 0.00003663
Iteration 73/1000 | Loss: 0.00003620
Iteration 74/1000 | Loss: 0.00003581
Iteration 75/1000 | Loss: 0.00003539
Iteration 76/1000 | Loss: 0.00003514
Iteration 77/1000 | Loss: 0.00003486
Iteration 78/1000 | Loss: 0.00003460
Iteration 79/1000 | Loss: 0.00003446
Iteration 80/1000 | Loss: 0.00003444
Iteration 81/1000 | Loss: 0.00003443
Iteration 82/1000 | Loss: 0.00003442
Iteration 83/1000 | Loss: 0.00003442
Iteration 84/1000 | Loss: 0.00003441
Iteration 85/1000 | Loss: 0.00003441
Iteration 86/1000 | Loss: 0.00003441
Iteration 87/1000 | Loss: 0.00003441
Iteration 88/1000 | Loss: 0.00003441
Iteration 89/1000 | Loss: 0.00003441
Iteration 90/1000 | Loss: 0.00003441
Iteration 91/1000 | Loss: 0.00003441
Iteration 92/1000 | Loss: 0.00003441
Iteration 93/1000 | Loss: 0.00003440
Iteration 94/1000 | Loss: 0.00003440
Iteration 95/1000 | Loss: 0.00003440
Iteration 96/1000 | Loss: 0.00003440
Iteration 97/1000 | Loss: 0.00003440
Iteration 98/1000 | Loss: 0.00003440
Iteration 99/1000 | Loss: 0.00003440
Iteration 100/1000 | Loss: 0.00003440
Iteration 101/1000 | Loss: 0.00003440
Iteration 102/1000 | Loss: 0.00003440
Iteration 103/1000 | Loss: 0.00003440
Iteration 104/1000 | Loss: 0.00003439
Iteration 105/1000 | Loss: 0.00003439
Iteration 106/1000 | Loss: 0.00003439
Iteration 107/1000 | Loss: 0.00003439
Iteration 108/1000 | Loss: 0.00003439
Iteration 109/1000 | Loss: 0.00003439
Iteration 110/1000 | Loss: 0.00003439
Iteration 111/1000 | Loss: 0.00003438
Iteration 112/1000 | Loss: 0.00003438
Iteration 113/1000 | Loss: 0.00003438
Iteration 114/1000 | Loss: 0.00003438
Iteration 115/1000 | Loss: 0.00003438
Iteration 116/1000 | Loss: 0.00003438
Iteration 117/1000 | Loss: 0.00003438
Iteration 118/1000 | Loss: 0.00003438
Iteration 119/1000 | Loss: 0.00003438
Iteration 120/1000 | Loss: 0.00003438
Iteration 121/1000 | Loss: 0.00003438
Iteration 122/1000 | Loss: 0.00003438
Iteration 123/1000 | Loss: 0.00003438
Iteration 124/1000 | Loss: 0.00003438
Iteration 125/1000 | Loss: 0.00003438
Iteration 126/1000 | Loss: 0.00003438
Iteration 127/1000 | Loss: 0.00003438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [3.437935083638877e-05, 3.437935083638877e-05, 3.437935083638877e-05, 3.437935083638877e-05, 3.437935083638877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.437935083638877e-05

Optimization complete. Final v2v error: 4.829070568084717 mm

Highest mean error: 6.36716365814209 mm for frame 4

Lowest mean error: 4.456256866455078 mm for frame 59

Saving results

Total time: 160.61548948287964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445708
Iteration 2/25 | Loss: 0.00140396
Iteration 3/25 | Loss: 0.00113140
Iteration 4/25 | Loss: 0.00111743
Iteration 5/25 | Loss: 0.00111177
Iteration 6/25 | Loss: 0.00110973
Iteration 7/25 | Loss: 0.00110941
Iteration 8/25 | Loss: 0.00110941
Iteration 9/25 | Loss: 0.00110941
Iteration 10/25 | Loss: 0.00110941
Iteration 11/25 | Loss: 0.00110941
Iteration 12/25 | Loss: 0.00110941
Iteration 13/25 | Loss: 0.00110941
Iteration 14/25 | Loss: 0.00110941
Iteration 15/25 | Loss: 0.00110941
Iteration 16/25 | Loss: 0.00110941
Iteration 17/25 | Loss: 0.00110941
Iteration 18/25 | Loss: 0.00110941
Iteration 19/25 | Loss: 0.00110941
Iteration 20/25 | Loss: 0.00110941
Iteration 21/25 | Loss: 0.00110941
Iteration 22/25 | Loss: 0.00110941
Iteration 23/25 | Loss: 0.00110941
Iteration 24/25 | Loss: 0.00110941
Iteration 25/25 | Loss: 0.00110941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29881406
Iteration 2/25 | Loss: 0.00084643
Iteration 3/25 | Loss: 0.00084643
Iteration 4/25 | Loss: 0.00084643
Iteration 5/25 | Loss: 0.00084643
Iteration 6/25 | Loss: 0.00084643
Iteration 7/25 | Loss: 0.00084643
Iteration 8/25 | Loss: 0.00084643
Iteration 9/25 | Loss: 0.00084643
Iteration 10/25 | Loss: 0.00084643
Iteration 11/25 | Loss: 0.00084643
Iteration 12/25 | Loss: 0.00084643
Iteration 13/25 | Loss: 0.00084643
Iteration 14/25 | Loss: 0.00084643
Iteration 15/25 | Loss: 0.00084643
Iteration 16/25 | Loss: 0.00084643
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008464286802336574, 0.0008464286802336574, 0.0008464286802336574, 0.0008464286802336574, 0.0008464286802336574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008464286802336574

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084643
Iteration 2/1000 | Loss: 0.00003297
Iteration 3/1000 | Loss: 0.00002624
Iteration 4/1000 | Loss: 0.00002417
Iteration 5/1000 | Loss: 0.00002327
Iteration 6/1000 | Loss: 0.00002274
Iteration 7/1000 | Loss: 0.00002236
Iteration 8/1000 | Loss: 0.00002212
Iteration 9/1000 | Loss: 0.00002200
Iteration 10/1000 | Loss: 0.00002197
Iteration 11/1000 | Loss: 0.00002193
Iteration 12/1000 | Loss: 0.00002188
Iteration 13/1000 | Loss: 0.00002187
Iteration 14/1000 | Loss: 0.00002187
Iteration 15/1000 | Loss: 0.00002183
Iteration 16/1000 | Loss: 0.00002183
Iteration 17/1000 | Loss: 0.00002181
Iteration 18/1000 | Loss: 0.00002176
Iteration 19/1000 | Loss: 0.00002176
Iteration 20/1000 | Loss: 0.00002175
Iteration 21/1000 | Loss: 0.00002175
Iteration 22/1000 | Loss: 0.00002171
Iteration 23/1000 | Loss: 0.00002171
Iteration 24/1000 | Loss: 0.00002170
Iteration 25/1000 | Loss: 0.00002167
Iteration 26/1000 | Loss: 0.00002165
Iteration 27/1000 | Loss: 0.00002165
Iteration 28/1000 | Loss: 0.00002164
Iteration 29/1000 | Loss: 0.00002164
Iteration 30/1000 | Loss: 0.00002164
Iteration 31/1000 | Loss: 0.00002164
Iteration 32/1000 | Loss: 0.00002163
Iteration 33/1000 | Loss: 0.00002161
Iteration 34/1000 | Loss: 0.00002161
Iteration 35/1000 | Loss: 0.00002161
Iteration 36/1000 | Loss: 0.00002160
Iteration 37/1000 | Loss: 0.00002160
Iteration 38/1000 | Loss: 0.00002160
Iteration 39/1000 | Loss: 0.00002160
Iteration 40/1000 | Loss: 0.00002159
Iteration 41/1000 | Loss: 0.00002159
Iteration 42/1000 | Loss: 0.00002159
Iteration 43/1000 | Loss: 0.00002159
Iteration 44/1000 | Loss: 0.00002158
Iteration 45/1000 | Loss: 0.00002158
Iteration 46/1000 | Loss: 0.00002158
Iteration 47/1000 | Loss: 0.00002157
Iteration 48/1000 | Loss: 0.00002157
Iteration 49/1000 | Loss: 0.00002157
Iteration 50/1000 | Loss: 0.00002157
Iteration 51/1000 | Loss: 0.00002157
Iteration 52/1000 | Loss: 0.00002157
Iteration 53/1000 | Loss: 0.00002156
Iteration 54/1000 | Loss: 0.00002156
Iteration 55/1000 | Loss: 0.00002155
Iteration 56/1000 | Loss: 0.00002155
Iteration 57/1000 | Loss: 0.00002154
Iteration 58/1000 | Loss: 0.00002154
Iteration 59/1000 | Loss: 0.00002154
Iteration 60/1000 | Loss: 0.00002154
Iteration 61/1000 | Loss: 0.00002154
Iteration 62/1000 | Loss: 0.00002154
Iteration 63/1000 | Loss: 0.00002154
Iteration 64/1000 | Loss: 0.00002154
Iteration 65/1000 | Loss: 0.00002154
Iteration 66/1000 | Loss: 0.00002154
Iteration 67/1000 | Loss: 0.00002153
Iteration 68/1000 | Loss: 0.00002153
Iteration 69/1000 | Loss: 0.00002153
Iteration 70/1000 | Loss: 0.00002153
Iteration 71/1000 | Loss: 0.00002153
Iteration 72/1000 | Loss: 0.00002153
Iteration 73/1000 | Loss: 0.00002153
Iteration 74/1000 | Loss: 0.00002153
Iteration 75/1000 | Loss: 0.00002152
Iteration 76/1000 | Loss: 0.00002152
Iteration 77/1000 | Loss: 0.00002152
Iteration 78/1000 | Loss: 0.00002152
Iteration 79/1000 | Loss: 0.00002152
Iteration 80/1000 | Loss: 0.00002152
Iteration 81/1000 | Loss: 0.00002151
Iteration 82/1000 | Loss: 0.00002151
Iteration 83/1000 | Loss: 0.00002151
Iteration 84/1000 | Loss: 0.00002151
Iteration 85/1000 | Loss: 0.00002151
Iteration 86/1000 | Loss: 0.00002151
Iteration 87/1000 | Loss: 0.00002151
Iteration 88/1000 | Loss: 0.00002150
Iteration 89/1000 | Loss: 0.00002150
Iteration 90/1000 | Loss: 0.00002150
Iteration 91/1000 | Loss: 0.00002150
Iteration 92/1000 | Loss: 0.00002150
Iteration 93/1000 | Loss: 0.00002150
Iteration 94/1000 | Loss: 0.00002150
Iteration 95/1000 | Loss: 0.00002150
Iteration 96/1000 | Loss: 0.00002150
Iteration 97/1000 | Loss: 0.00002149
Iteration 98/1000 | Loss: 0.00002149
Iteration 99/1000 | Loss: 0.00002149
Iteration 100/1000 | Loss: 0.00002149
Iteration 101/1000 | Loss: 0.00002149
Iteration 102/1000 | Loss: 0.00002149
Iteration 103/1000 | Loss: 0.00002149
Iteration 104/1000 | Loss: 0.00002149
Iteration 105/1000 | Loss: 0.00002149
Iteration 106/1000 | Loss: 0.00002149
Iteration 107/1000 | Loss: 0.00002149
Iteration 108/1000 | Loss: 0.00002149
Iteration 109/1000 | Loss: 0.00002149
Iteration 110/1000 | Loss: 0.00002148
Iteration 111/1000 | Loss: 0.00002148
Iteration 112/1000 | Loss: 0.00002148
Iteration 113/1000 | Loss: 0.00002147
Iteration 114/1000 | Loss: 0.00002147
Iteration 115/1000 | Loss: 0.00002147
Iteration 116/1000 | Loss: 0.00002147
Iteration 117/1000 | Loss: 0.00002147
Iteration 118/1000 | Loss: 0.00002147
Iteration 119/1000 | Loss: 0.00002147
Iteration 120/1000 | Loss: 0.00002147
Iteration 121/1000 | Loss: 0.00002147
Iteration 122/1000 | Loss: 0.00002147
Iteration 123/1000 | Loss: 0.00002147
Iteration 124/1000 | Loss: 0.00002147
Iteration 125/1000 | Loss: 0.00002147
Iteration 126/1000 | Loss: 0.00002147
Iteration 127/1000 | Loss: 0.00002147
Iteration 128/1000 | Loss: 0.00002147
Iteration 129/1000 | Loss: 0.00002147
Iteration 130/1000 | Loss: 0.00002147
Iteration 131/1000 | Loss: 0.00002147
Iteration 132/1000 | Loss: 0.00002147
Iteration 133/1000 | Loss: 0.00002147
Iteration 134/1000 | Loss: 0.00002147
Iteration 135/1000 | Loss: 0.00002147
Iteration 136/1000 | Loss: 0.00002147
Iteration 137/1000 | Loss: 0.00002147
Iteration 138/1000 | Loss: 0.00002147
Iteration 139/1000 | Loss: 0.00002147
Iteration 140/1000 | Loss: 0.00002147
Iteration 141/1000 | Loss: 0.00002147
Iteration 142/1000 | Loss: 0.00002147
Iteration 143/1000 | Loss: 0.00002147
Iteration 144/1000 | Loss: 0.00002147
Iteration 145/1000 | Loss: 0.00002147
Iteration 146/1000 | Loss: 0.00002147
Iteration 147/1000 | Loss: 0.00002147
Iteration 148/1000 | Loss: 0.00002147
Iteration 149/1000 | Loss: 0.00002147
Iteration 150/1000 | Loss: 0.00002147
Iteration 151/1000 | Loss: 0.00002147
Iteration 152/1000 | Loss: 0.00002147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.1466155885718763e-05, 2.1466155885718763e-05, 2.1466155885718763e-05, 2.1466155885718763e-05, 2.1466155885718763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1466155885718763e-05

Optimization complete. Final v2v error: 3.902149200439453 mm

Highest mean error: 4.459018707275391 mm for frame 25

Lowest mean error: 3.5028786659240723 mm for frame 146

Saving results

Total time: 34.269755601882935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00559112
Iteration 2/25 | Loss: 0.00173267
Iteration 3/25 | Loss: 0.00126873
Iteration 4/25 | Loss: 0.00122246
Iteration 5/25 | Loss: 0.00120683
Iteration 6/25 | Loss: 0.00120211
Iteration 7/25 | Loss: 0.00120098
Iteration 8/25 | Loss: 0.00120068
Iteration 9/25 | Loss: 0.00120068
Iteration 10/25 | Loss: 0.00120068
Iteration 11/25 | Loss: 0.00120068
Iteration 12/25 | Loss: 0.00120068
Iteration 13/25 | Loss: 0.00120068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012006786419078708, 0.0012006786419078708, 0.0012006786419078708, 0.0012006786419078708, 0.0012006786419078708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012006786419078708

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41038907
Iteration 2/25 | Loss: 0.00078893
Iteration 3/25 | Loss: 0.00078891
Iteration 4/25 | Loss: 0.00078891
Iteration 5/25 | Loss: 0.00078891
Iteration 6/25 | Loss: 0.00078891
Iteration 7/25 | Loss: 0.00078891
Iteration 8/25 | Loss: 0.00078891
Iteration 9/25 | Loss: 0.00078891
Iteration 10/25 | Loss: 0.00078891
Iteration 11/25 | Loss: 0.00078891
Iteration 12/25 | Loss: 0.00078891
Iteration 13/25 | Loss: 0.00078891
Iteration 14/25 | Loss: 0.00078891
Iteration 15/25 | Loss: 0.00078891
Iteration 16/25 | Loss: 0.00078891
Iteration 17/25 | Loss: 0.00078891
Iteration 18/25 | Loss: 0.00078891
Iteration 19/25 | Loss: 0.00078891
Iteration 20/25 | Loss: 0.00078891
Iteration 21/25 | Loss: 0.00078891
Iteration 22/25 | Loss: 0.00078891
Iteration 23/25 | Loss: 0.00078891
Iteration 24/25 | Loss: 0.00078891
Iteration 25/25 | Loss: 0.00078891

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078891
Iteration 2/1000 | Loss: 0.00005338
Iteration 3/1000 | Loss: 0.00004136
Iteration 4/1000 | Loss: 0.00003802
Iteration 5/1000 | Loss: 0.00003574
Iteration 6/1000 | Loss: 0.00003455
Iteration 7/1000 | Loss: 0.00003372
Iteration 8/1000 | Loss: 0.00003304
Iteration 9/1000 | Loss: 0.00003248
Iteration 10/1000 | Loss: 0.00003215
Iteration 11/1000 | Loss: 0.00003188
Iteration 12/1000 | Loss: 0.00003171
Iteration 13/1000 | Loss: 0.00003160
Iteration 14/1000 | Loss: 0.00003157
Iteration 15/1000 | Loss: 0.00003155
Iteration 16/1000 | Loss: 0.00003154
Iteration 17/1000 | Loss: 0.00003150
Iteration 18/1000 | Loss: 0.00003144
Iteration 19/1000 | Loss: 0.00003143
Iteration 20/1000 | Loss: 0.00003137
Iteration 21/1000 | Loss: 0.00003137
Iteration 22/1000 | Loss: 0.00003136
Iteration 23/1000 | Loss: 0.00003136
Iteration 24/1000 | Loss: 0.00003135
Iteration 25/1000 | Loss: 0.00003135
Iteration 26/1000 | Loss: 0.00003134
Iteration 27/1000 | Loss: 0.00003134
Iteration 28/1000 | Loss: 0.00003134
Iteration 29/1000 | Loss: 0.00003133
Iteration 30/1000 | Loss: 0.00003132
Iteration 31/1000 | Loss: 0.00003132
Iteration 32/1000 | Loss: 0.00003132
Iteration 33/1000 | Loss: 0.00003132
Iteration 34/1000 | Loss: 0.00003132
Iteration 35/1000 | Loss: 0.00003132
Iteration 36/1000 | Loss: 0.00003132
Iteration 37/1000 | Loss: 0.00003132
Iteration 38/1000 | Loss: 0.00003132
Iteration 39/1000 | Loss: 0.00003132
Iteration 40/1000 | Loss: 0.00003131
Iteration 41/1000 | Loss: 0.00003131
Iteration 42/1000 | Loss: 0.00003131
Iteration 43/1000 | Loss: 0.00003131
Iteration 44/1000 | Loss: 0.00003131
Iteration 45/1000 | Loss: 0.00003131
Iteration 46/1000 | Loss: 0.00003130
Iteration 47/1000 | Loss: 0.00003129
Iteration 48/1000 | Loss: 0.00003128
Iteration 49/1000 | Loss: 0.00003128
Iteration 50/1000 | Loss: 0.00003127
Iteration 51/1000 | Loss: 0.00003127
Iteration 52/1000 | Loss: 0.00003127
Iteration 53/1000 | Loss: 0.00003126
Iteration 54/1000 | Loss: 0.00003126
Iteration 55/1000 | Loss: 0.00003126
Iteration 56/1000 | Loss: 0.00003125
Iteration 57/1000 | Loss: 0.00003125
Iteration 58/1000 | Loss: 0.00003125
Iteration 59/1000 | Loss: 0.00003125
Iteration 60/1000 | Loss: 0.00003125
Iteration 61/1000 | Loss: 0.00003125
Iteration 62/1000 | Loss: 0.00003125
Iteration 63/1000 | Loss: 0.00003125
Iteration 64/1000 | Loss: 0.00003125
Iteration 65/1000 | Loss: 0.00003125
Iteration 66/1000 | Loss: 0.00003124
Iteration 67/1000 | Loss: 0.00003124
Iteration 68/1000 | Loss: 0.00003124
Iteration 69/1000 | Loss: 0.00003124
Iteration 70/1000 | Loss: 0.00003123
Iteration 71/1000 | Loss: 0.00003123
Iteration 72/1000 | Loss: 0.00003123
Iteration 73/1000 | Loss: 0.00003123
Iteration 74/1000 | Loss: 0.00003122
Iteration 75/1000 | Loss: 0.00003122
Iteration 76/1000 | Loss: 0.00003122
Iteration 77/1000 | Loss: 0.00003122
Iteration 78/1000 | Loss: 0.00003122
Iteration 79/1000 | Loss: 0.00003121
Iteration 80/1000 | Loss: 0.00003121
Iteration 81/1000 | Loss: 0.00003121
Iteration 82/1000 | Loss: 0.00003121
Iteration 83/1000 | Loss: 0.00003121
Iteration 84/1000 | Loss: 0.00003120
Iteration 85/1000 | Loss: 0.00003120
Iteration 86/1000 | Loss: 0.00003120
Iteration 87/1000 | Loss: 0.00003120
Iteration 88/1000 | Loss: 0.00003120
Iteration 89/1000 | Loss: 0.00003119
Iteration 90/1000 | Loss: 0.00003119
Iteration 91/1000 | Loss: 0.00003119
Iteration 92/1000 | Loss: 0.00003119
Iteration 93/1000 | Loss: 0.00003119
Iteration 94/1000 | Loss: 0.00003118
Iteration 95/1000 | Loss: 0.00003118
Iteration 96/1000 | Loss: 0.00003118
Iteration 97/1000 | Loss: 0.00003118
Iteration 98/1000 | Loss: 0.00003118
Iteration 99/1000 | Loss: 0.00003118
Iteration 100/1000 | Loss: 0.00003118
Iteration 101/1000 | Loss: 0.00003118
Iteration 102/1000 | Loss: 0.00003118
Iteration 103/1000 | Loss: 0.00003118
Iteration 104/1000 | Loss: 0.00003118
Iteration 105/1000 | Loss: 0.00003118
Iteration 106/1000 | Loss: 0.00003118
Iteration 107/1000 | Loss: 0.00003118
Iteration 108/1000 | Loss: 0.00003118
Iteration 109/1000 | Loss: 0.00003118
Iteration 110/1000 | Loss: 0.00003118
Iteration 111/1000 | Loss: 0.00003118
Iteration 112/1000 | Loss: 0.00003118
Iteration 113/1000 | Loss: 0.00003118
Iteration 114/1000 | Loss: 0.00003118
Iteration 115/1000 | Loss: 0.00003118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [3.117544110864401e-05, 3.117544110864401e-05, 3.117544110864401e-05, 3.117544110864401e-05, 3.117544110864401e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.117544110864401e-05

Optimization complete. Final v2v error: 4.6740007400512695 mm

Highest mean error: 6.395766735076904 mm for frame 58

Lowest mean error: 3.751452922821045 mm for frame 123

Saving results

Total time: 38.84713411331177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01124345
Iteration 2/25 | Loss: 0.00240132
Iteration 3/25 | Loss: 0.00183676
Iteration 4/25 | Loss: 0.00164706
Iteration 5/25 | Loss: 0.00147927
Iteration 6/25 | Loss: 0.00142403
Iteration 7/25 | Loss: 0.00135711
Iteration 8/25 | Loss: 0.00128278
Iteration 9/25 | Loss: 0.00126150
Iteration 10/25 | Loss: 0.00124675
Iteration 11/25 | Loss: 0.00124318
Iteration 12/25 | Loss: 0.00122231
Iteration 13/25 | Loss: 0.00119261
Iteration 14/25 | Loss: 0.00119193
Iteration 15/25 | Loss: 0.00116909
Iteration 16/25 | Loss: 0.00116229
Iteration 17/25 | Loss: 0.00115848
Iteration 18/25 | Loss: 0.00116028
Iteration 19/25 | Loss: 0.00115265
Iteration 20/25 | Loss: 0.00116309
Iteration 21/25 | Loss: 0.00115204
Iteration 22/25 | Loss: 0.00115476
Iteration 23/25 | Loss: 0.00114390
Iteration 24/25 | Loss: 0.00113424
Iteration 25/25 | Loss: 0.00113337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42172921
Iteration 2/25 | Loss: 0.00186920
Iteration 3/25 | Loss: 0.00186920
Iteration 4/25 | Loss: 0.00186920
Iteration 5/25 | Loss: 0.00186920
Iteration 6/25 | Loss: 0.00186920
Iteration 7/25 | Loss: 0.00186920
Iteration 8/25 | Loss: 0.00186920
Iteration 9/25 | Loss: 0.00186920
Iteration 10/25 | Loss: 0.00186920
Iteration 11/25 | Loss: 0.00186920
Iteration 12/25 | Loss: 0.00186920
Iteration 13/25 | Loss: 0.00186920
Iteration 14/25 | Loss: 0.00186920
Iteration 15/25 | Loss: 0.00186920
Iteration 16/25 | Loss: 0.00186920
Iteration 17/25 | Loss: 0.00186920
Iteration 18/25 | Loss: 0.00186920
Iteration 19/25 | Loss: 0.00186920
Iteration 20/25 | Loss: 0.00186920
Iteration 21/25 | Loss: 0.00186920
Iteration 22/25 | Loss: 0.00186920
Iteration 23/25 | Loss: 0.00186920
Iteration 24/25 | Loss: 0.00186920
Iteration 25/25 | Loss: 0.00186920

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186920
Iteration 2/1000 | Loss: 0.00082247
Iteration 3/1000 | Loss: 0.00072450
Iteration 4/1000 | Loss: 0.00085268
Iteration 5/1000 | Loss: 0.00083296
Iteration 6/1000 | Loss: 0.00077061
Iteration 7/1000 | Loss: 0.00085954
Iteration 8/1000 | Loss: 0.00096639
Iteration 9/1000 | Loss: 0.00089236
Iteration 10/1000 | Loss: 0.00143702
Iteration 11/1000 | Loss: 0.00078675
Iteration 12/1000 | Loss: 0.00053528
Iteration 13/1000 | Loss: 0.00053365
Iteration 14/1000 | Loss: 0.00065438
Iteration 15/1000 | Loss: 0.00098089
Iteration 16/1000 | Loss: 0.00072590
Iteration 17/1000 | Loss: 0.00100467
Iteration 18/1000 | Loss: 0.00100149
Iteration 19/1000 | Loss: 0.00066757
Iteration 20/1000 | Loss: 0.00048809
Iteration 21/1000 | Loss: 0.00058059
Iteration 22/1000 | Loss: 0.00069143
Iteration 23/1000 | Loss: 0.00058447
Iteration 24/1000 | Loss: 0.00072073
Iteration 25/1000 | Loss: 0.00084281
Iteration 26/1000 | Loss: 0.00071758
Iteration 27/1000 | Loss: 0.00073358
Iteration 28/1000 | Loss: 0.00069661
Iteration 29/1000 | Loss: 0.00077031
Iteration 30/1000 | Loss: 0.00064080
Iteration 31/1000 | Loss: 0.00087124
Iteration 32/1000 | Loss: 0.00067646
Iteration 33/1000 | Loss: 0.00041409
Iteration 34/1000 | Loss: 0.00046903
Iteration 35/1000 | Loss: 0.00056123
Iteration 36/1000 | Loss: 0.00040313
Iteration 37/1000 | Loss: 0.00057683
Iteration 38/1000 | Loss: 0.00064268
Iteration 39/1000 | Loss: 0.00054760
Iteration 40/1000 | Loss: 0.00074862
Iteration 41/1000 | Loss: 0.00065696
Iteration 42/1000 | Loss: 0.00072526
Iteration 43/1000 | Loss: 0.00077512
Iteration 44/1000 | Loss: 0.00068577
Iteration 45/1000 | Loss: 0.00069721
Iteration 46/1000 | Loss: 0.00075247
Iteration 47/1000 | Loss: 0.00068328
Iteration 48/1000 | Loss: 0.00060308
Iteration 49/1000 | Loss: 0.00080531
Iteration 50/1000 | Loss: 0.00070783
Iteration 51/1000 | Loss: 0.00073832
Iteration 52/1000 | Loss: 0.00083327
Iteration 53/1000 | Loss: 0.00083683
Iteration 54/1000 | Loss: 0.00066187
Iteration 55/1000 | Loss: 0.00066417
Iteration 56/1000 | Loss: 0.00063593
Iteration 57/1000 | Loss: 0.00059532
Iteration 58/1000 | Loss: 0.00099587
Iteration 59/1000 | Loss: 0.00095291
Iteration 60/1000 | Loss: 0.00096734
Iteration 61/1000 | Loss: 0.00084951
Iteration 62/1000 | Loss: 0.00051758
Iteration 63/1000 | Loss: 0.00048830
Iteration 64/1000 | Loss: 0.00036126
Iteration 65/1000 | Loss: 0.00025057
Iteration 66/1000 | Loss: 0.00043544
Iteration 67/1000 | Loss: 0.00030721
Iteration 68/1000 | Loss: 0.00029499
Iteration 69/1000 | Loss: 0.00029358
Iteration 70/1000 | Loss: 0.00038849
Iteration 71/1000 | Loss: 0.00035533
Iteration 72/1000 | Loss: 0.00050222
Iteration 73/1000 | Loss: 0.00084562
Iteration 74/1000 | Loss: 0.00129227
Iteration 75/1000 | Loss: 0.00055727
Iteration 76/1000 | Loss: 0.00119603
Iteration 77/1000 | Loss: 0.00074385
Iteration 78/1000 | Loss: 0.00077093
Iteration 79/1000 | Loss: 0.00077935
Iteration 80/1000 | Loss: 0.00052917
Iteration 81/1000 | Loss: 0.00062915
Iteration 82/1000 | Loss: 0.00051906
Iteration 83/1000 | Loss: 0.00042577
Iteration 84/1000 | Loss: 0.00043685
Iteration 85/1000 | Loss: 0.00050163
Iteration 86/1000 | Loss: 0.00040002
Iteration 87/1000 | Loss: 0.00059293
Iteration 88/1000 | Loss: 0.00068142
Iteration 89/1000 | Loss: 0.00042528
Iteration 90/1000 | Loss: 0.00060767
Iteration 91/1000 | Loss: 0.00049408
Iteration 92/1000 | Loss: 0.00051792
Iteration 93/1000 | Loss: 0.00032220
Iteration 94/1000 | Loss: 0.00036103
Iteration 95/1000 | Loss: 0.00043027
Iteration 96/1000 | Loss: 0.00042881
Iteration 97/1000 | Loss: 0.00060573
Iteration 98/1000 | Loss: 0.00048037
Iteration 99/1000 | Loss: 0.00054532
Iteration 100/1000 | Loss: 0.00080307
Iteration 101/1000 | Loss: 0.00044871
Iteration 102/1000 | Loss: 0.00054656
Iteration 103/1000 | Loss: 0.00040065
Iteration 104/1000 | Loss: 0.00062946
Iteration 105/1000 | Loss: 0.00063924
Iteration 106/1000 | Loss: 0.00077403
Iteration 107/1000 | Loss: 0.00080438
Iteration 108/1000 | Loss: 0.00088702
Iteration 109/1000 | Loss: 0.00062334
Iteration 110/1000 | Loss: 0.00057978
Iteration 111/1000 | Loss: 0.00056963
Iteration 112/1000 | Loss: 0.00079395
Iteration 113/1000 | Loss: 0.00071227
Iteration 114/1000 | Loss: 0.00072703
Iteration 115/1000 | Loss: 0.00055333
Iteration 116/1000 | Loss: 0.00076694
Iteration 117/1000 | Loss: 0.00064810
Iteration 118/1000 | Loss: 0.00038043
Iteration 119/1000 | Loss: 0.00032350
Iteration 120/1000 | Loss: 0.00030637
Iteration 121/1000 | Loss: 0.00055367
Iteration 122/1000 | Loss: 0.00075423
Iteration 123/1000 | Loss: 0.00051265
Iteration 124/1000 | Loss: 0.00054608
Iteration 125/1000 | Loss: 0.00038911
Iteration 126/1000 | Loss: 0.00060548
Iteration 127/1000 | Loss: 0.00033124
Iteration 128/1000 | Loss: 0.00035403
Iteration 129/1000 | Loss: 0.00035650
Iteration 130/1000 | Loss: 0.00039771
Iteration 131/1000 | Loss: 0.00041848
Iteration 132/1000 | Loss: 0.00028154
Iteration 133/1000 | Loss: 0.00058643
Iteration 134/1000 | Loss: 0.00057107
Iteration 135/1000 | Loss: 0.00032963
Iteration 136/1000 | Loss: 0.00025854
Iteration 137/1000 | Loss: 0.00043999
Iteration 138/1000 | Loss: 0.00047229
Iteration 139/1000 | Loss: 0.00034773
Iteration 140/1000 | Loss: 0.00072276
Iteration 141/1000 | Loss: 0.00067367
Iteration 142/1000 | Loss: 0.00078556
Iteration 143/1000 | Loss: 0.00084882
Iteration 144/1000 | Loss: 0.00056882
Iteration 145/1000 | Loss: 0.00068526
Iteration 146/1000 | Loss: 0.00052732
Iteration 147/1000 | Loss: 0.00066150
Iteration 148/1000 | Loss: 0.00048489
Iteration 149/1000 | Loss: 0.00077087
Iteration 150/1000 | Loss: 0.00051863
Iteration 151/1000 | Loss: 0.00052757
Iteration 152/1000 | Loss: 0.00065790
Iteration 153/1000 | Loss: 0.00039726
Iteration 154/1000 | Loss: 0.00017849
Iteration 155/1000 | Loss: 0.00094717
Iteration 156/1000 | Loss: 0.00032229
Iteration 157/1000 | Loss: 0.00049295
Iteration 158/1000 | Loss: 0.00045019
Iteration 159/1000 | Loss: 0.00028043
Iteration 160/1000 | Loss: 0.00037153
Iteration 161/1000 | Loss: 0.00053339
Iteration 162/1000 | Loss: 0.00046541
Iteration 163/1000 | Loss: 0.00024698
Iteration 164/1000 | Loss: 0.00034714
Iteration 165/1000 | Loss: 0.00033616
Iteration 166/1000 | Loss: 0.00042152
Iteration 167/1000 | Loss: 0.00050362
Iteration 168/1000 | Loss: 0.00051169
Iteration 169/1000 | Loss: 0.00022537
Iteration 170/1000 | Loss: 0.00053378
Iteration 171/1000 | Loss: 0.00041561
Iteration 172/1000 | Loss: 0.00023942
Iteration 173/1000 | Loss: 0.00051825
Iteration 174/1000 | Loss: 0.00039791
Iteration 175/1000 | Loss: 0.00045568
Iteration 176/1000 | Loss: 0.00053739
Iteration 177/1000 | Loss: 0.00027810
Iteration 178/1000 | Loss: 0.00041213
Iteration 179/1000 | Loss: 0.00024169
Iteration 180/1000 | Loss: 0.00064547
Iteration 181/1000 | Loss: 0.00129299
Iteration 182/1000 | Loss: 0.00081866
Iteration 183/1000 | Loss: 0.00118522
Iteration 184/1000 | Loss: 0.00073486
Iteration 185/1000 | Loss: 0.00072291
Iteration 186/1000 | Loss: 0.00041246
Iteration 187/1000 | Loss: 0.00094219
Iteration 188/1000 | Loss: 0.00083456
Iteration 189/1000 | Loss: 0.00029141
Iteration 190/1000 | Loss: 0.00023577
Iteration 191/1000 | Loss: 0.00079962
Iteration 192/1000 | Loss: 0.00055960
Iteration 193/1000 | Loss: 0.00041785
Iteration 194/1000 | Loss: 0.00040741
Iteration 195/1000 | Loss: 0.00051378
Iteration 196/1000 | Loss: 0.00038044
Iteration 197/1000 | Loss: 0.00035106
Iteration 198/1000 | Loss: 0.00043488
Iteration 199/1000 | Loss: 0.00056678
Iteration 200/1000 | Loss: 0.00043786
Iteration 201/1000 | Loss: 0.00028898
Iteration 202/1000 | Loss: 0.00035297
Iteration 203/1000 | Loss: 0.00030290
Iteration 204/1000 | Loss: 0.00050548
Iteration 205/1000 | Loss: 0.00032343
Iteration 206/1000 | Loss: 0.00059914
Iteration 207/1000 | Loss: 0.00041061
Iteration 208/1000 | Loss: 0.00025905
Iteration 209/1000 | Loss: 0.00032009
Iteration 210/1000 | Loss: 0.00044094
Iteration 211/1000 | Loss: 0.00038665
Iteration 212/1000 | Loss: 0.00049192
Iteration 213/1000 | Loss: 0.00050515
Iteration 214/1000 | Loss: 0.00023800
Iteration 215/1000 | Loss: 0.00016229
Iteration 216/1000 | Loss: 0.00015698
Iteration 217/1000 | Loss: 0.00042932
Iteration 218/1000 | Loss: 0.00050242
Iteration 219/1000 | Loss: 0.00043347
Iteration 220/1000 | Loss: 0.00029955
Iteration 221/1000 | Loss: 0.00031097
Iteration 222/1000 | Loss: 0.00035009
Iteration 223/1000 | Loss: 0.00047174
Iteration 224/1000 | Loss: 0.00048674
Iteration 225/1000 | Loss: 0.00018556
Iteration 226/1000 | Loss: 0.00056493
Iteration 227/1000 | Loss: 0.00038542
Iteration 228/1000 | Loss: 0.00041711
Iteration 229/1000 | Loss: 0.00033547
Iteration 230/1000 | Loss: 0.00041637
Iteration 231/1000 | Loss: 0.00025612
Iteration 232/1000 | Loss: 0.00040036
Iteration 233/1000 | Loss: 0.00037155
Iteration 234/1000 | Loss: 0.00040412
Iteration 235/1000 | Loss: 0.00038354
Iteration 236/1000 | Loss: 0.00040271
Iteration 237/1000 | Loss: 0.00036167
Iteration 238/1000 | Loss: 0.00033652
Iteration 239/1000 | Loss: 0.00037612
Iteration 240/1000 | Loss: 0.00026648
Iteration 241/1000 | Loss: 0.00033374
Iteration 242/1000 | Loss: 0.00039848
Iteration 243/1000 | Loss: 0.00042828
Iteration 244/1000 | Loss: 0.00038067
Iteration 245/1000 | Loss: 0.00020774
Iteration 246/1000 | Loss: 0.00079473
Iteration 247/1000 | Loss: 0.00061121
Iteration 248/1000 | Loss: 0.00036525
Iteration 249/1000 | Loss: 0.00035483
Iteration 250/1000 | Loss: 0.00032898
Iteration 251/1000 | Loss: 0.00040327
Iteration 252/1000 | Loss: 0.00030001
Iteration 253/1000 | Loss: 0.00033612
Iteration 254/1000 | Loss: 0.00033816
Iteration 255/1000 | Loss: 0.00034153
Iteration 256/1000 | Loss: 0.00059448
Iteration 257/1000 | Loss: 0.00047634
Iteration 258/1000 | Loss: 0.00039464
Iteration 259/1000 | Loss: 0.00036735
Iteration 260/1000 | Loss: 0.00034158
Iteration 261/1000 | Loss: 0.00030519
Iteration 262/1000 | Loss: 0.00038122
Iteration 263/1000 | Loss: 0.00047781
Iteration 264/1000 | Loss: 0.00039189
Iteration 265/1000 | Loss: 0.00048594
Iteration 266/1000 | Loss: 0.00021556
Iteration 267/1000 | Loss: 0.00043288
Iteration 268/1000 | Loss: 0.00053605
Iteration 269/1000 | Loss: 0.00061266
Iteration 270/1000 | Loss: 0.00048844
Iteration 271/1000 | Loss: 0.00052874
Iteration 272/1000 | Loss: 0.00111338
Iteration 273/1000 | Loss: 0.00144938
Iteration 274/1000 | Loss: 0.00171360
Iteration 275/1000 | Loss: 0.00117341
Iteration 276/1000 | Loss: 0.00039165
Iteration 277/1000 | Loss: 0.00039378
Iteration 278/1000 | Loss: 0.00042580
Iteration 279/1000 | Loss: 0.00040235
Iteration 280/1000 | Loss: 0.00036985
Iteration 281/1000 | Loss: 0.00043325
Iteration 282/1000 | Loss: 0.00007623
Iteration 283/1000 | Loss: 0.00016806
Iteration 284/1000 | Loss: 0.00059053
Iteration 285/1000 | Loss: 0.00027635
Iteration 286/1000 | Loss: 0.00014721
Iteration 287/1000 | Loss: 0.00005694
Iteration 288/1000 | Loss: 0.00007810
Iteration 289/1000 | Loss: 0.00115504
Iteration 290/1000 | Loss: 0.00070718
Iteration 291/1000 | Loss: 0.00033917
Iteration 292/1000 | Loss: 0.00017738
Iteration 293/1000 | Loss: 0.00019147
Iteration 294/1000 | Loss: 0.00021824
Iteration 295/1000 | Loss: 0.00022669
Iteration 296/1000 | Loss: 0.00008589
Iteration 297/1000 | Loss: 0.00013666
Iteration 298/1000 | Loss: 0.00015370
Iteration 299/1000 | Loss: 0.00006420
Iteration 300/1000 | Loss: 0.00021752
Iteration 301/1000 | Loss: 0.00005843
Iteration 302/1000 | Loss: 0.00004357
Iteration 303/1000 | Loss: 0.00005898
Iteration 304/1000 | Loss: 0.00005112
Iteration 305/1000 | Loss: 0.00005205
Iteration 306/1000 | Loss: 0.00004747
Iteration 307/1000 | Loss: 0.00004572
Iteration 308/1000 | Loss: 0.00004340
Iteration 309/1000 | Loss: 0.00005965
Iteration 310/1000 | Loss: 0.00005597
Iteration 311/1000 | Loss: 0.00006471
Iteration 312/1000 | Loss: 0.00004204
Iteration 313/1000 | Loss: 0.00003750
Iteration 314/1000 | Loss: 0.00003345
Iteration 315/1000 | Loss: 0.00003187
Iteration 316/1000 | Loss: 0.00003094
Iteration 317/1000 | Loss: 0.00003030
Iteration 318/1000 | Loss: 0.00002989
Iteration 319/1000 | Loss: 0.00002950
Iteration 320/1000 | Loss: 0.00002906
Iteration 321/1000 | Loss: 0.00002892
Iteration 322/1000 | Loss: 0.00002882
Iteration 323/1000 | Loss: 0.00002882
Iteration 324/1000 | Loss: 0.00002881
Iteration 325/1000 | Loss: 0.00002880
Iteration 326/1000 | Loss: 0.00002880
Iteration 327/1000 | Loss: 0.00002879
Iteration 328/1000 | Loss: 0.00002879
Iteration 329/1000 | Loss: 0.00002877
Iteration 330/1000 | Loss: 0.00002876
Iteration 331/1000 | Loss: 0.00002876
Iteration 332/1000 | Loss: 0.00002876
Iteration 333/1000 | Loss: 0.00002875
Iteration 334/1000 | Loss: 0.00002875
Iteration 335/1000 | Loss: 0.00002875
Iteration 336/1000 | Loss: 0.00002874
Iteration 337/1000 | Loss: 0.00002873
Iteration 338/1000 | Loss: 0.00002873
Iteration 339/1000 | Loss: 0.00002873
Iteration 340/1000 | Loss: 0.00002873
Iteration 341/1000 | Loss: 0.00002872
Iteration 342/1000 | Loss: 0.00002872
Iteration 343/1000 | Loss: 0.00002872
Iteration 344/1000 | Loss: 0.00002871
Iteration 345/1000 | Loss: 0.00002871
Iteration 346/1000 | Loss: 0.00002871
Iteration 347/1000 | Loss: 0.00002871
Iteration 348/1000 | Loss: 0.00002870
Iteration 349/1000 | Loss: 0.00002870
Iteration 350/1000 | Loss: 0.00002870
Iteration 351/1000 | Loss: 0.00002870
Iteration 352/1000 | Loss: 0.00002870
Iteration 353/1000 | Loss: 0.00002869
Iteration 354/1000 | Loss: 0.00002869
Iteration 355/1000 | Loss: 0.00002869
Iteration 356/1000 | Loss: 0.00002869
Iteration 357/1000 | Loss: 0.00002869
Iteration 358/1000 | Loss: 0.00002868
Iteration 359/1000 | Loss: 0.00002868
Iteration 360/1000 | Loss: 0.00002868
Iteration 361/1000 | Loss: 0.00002868
Iteration 362/1000 | Loss: 0.00002867
Iteration 363/1000 | Loss: 0.00002867
Iteration 364/1000 | Loss: 0.00002867
Iteration 365/1000 | Loss: 0.00002867
Iteration 366/1000 | Loss: 0.00002867
Iteration 367/1000 | Loss: 0.00002867
Iteration 368/1000 | Loss: 0.00002867
Iteration 369/1000 | Loss: 0.00002867
Iteration 370/1000 | Loss: 0.00002867
Iteration 371/1000 | Loss: 0.00002867
Iteration 372/1000 | Loss: 0.00002867
Iteration 373/1000 | Loss: 0.00002867
Iteration 374/1000 | Loss: 0.00002867
Iteration 375/1000 | Loss: 0.00002867
Iteration 376/1000 | Loss: 0.00002867
Iteration 377/1000 | Loss: 0.00002867
Iteration 378/1000 | Loss: 0.00002867
Iteration 379/1000 | Loss: 0.00002867
Iteration 380/1000 | Loss: 0.00002866
Iteration 381/1000 | Loss: 0.00002866
Iteration 382/1000 | Loss: 0.00002866
Iteration 383/1000 | Loss: 0.00002866
Iteration 384/1000 | Loss: 0.00002866
Iteration 385/1000 | Loss: 0.00002866
Iteration 386/1000 | Loss: 0.00002866
Iteration 387/1000 | Loss: 0.00002865
Iteration 388/1000 | Loss: 0.00002865
Iteration 389/1000 | Loss: 0.00002865
Iteration 390/1000 | Loss: 0.00002865
Iteration 391/1000 | Loss: 0.00002865
Iteration 392/1000 | Loss: 0.00002865
Iteration 393/1000 | Loss: 0.00002864
Iteration 394/1000 | Loss: 0.00002864
Iteration 395/1000 | Loss: 0.00002864
Iteration 396/1000 | Loss: 0.00002864
Iteration 397/1000 | Loss: 0.00002864
Iteration 398/1000 | Loss: 0.00002864
Iteration 399/1000 | Loss: 0.00002864
Iteration 400/1000 | Loss: 0.00002864
Iteration 401/1000 | Loss: 0.00002864
Iteration 402/1000 | Loss: 0.00002864
Iteration 403/1000 | Loss: 0.00002864
Iteration 404/1000 | Loss: 0.00002864
Iteration 405/1000 | Loss: 0.00002864
Iteration 406/1000 | Loss: 0.00002863
Iteration 407/1000 | Loss: 0.00002863
Iteration 408/1000 | Loss: 0.00002863
Iteration 409/1000 | Loss: 0.00002863
Iteration 410/1000 | Loss: 0.00002863
Iteration 411/1000 | Loss: 0.00002863
Iteration 412/1000 | Loss: 0.00002863
Iteration 413/1000 | Loss: 0.00002863
Iteration 414/1000 | Loss: 0.00002863
Iteration 415/1000 | Loss: 0.00002863
Iteration 416/1000 | Loss: 0.00002863
Iteration 417/1000 | Loss: 0.00002862
Iteration 418/1000 | Loss: 0.00002862
Iteration 419/1000 | Loss: 0.00002862
Iteration 420/1000 | Loss: 0.00002862
Iteration 421/1000 | Loss: 0.00002862
Iteration 422/1000 | Loss: 0.00002862
Iteration 423/1000 | Loss: 0.00002862
Iteration 424/1000 | Loss: 0.00002862
Iteration 425/1000 | Loss: 0.00002861
Iteration 426/1000 | Loss: 0.00002861
Iteration 427/1000 | Loss: 0.00002861
Iteration 428/1000 | Loss: 0.00002861
Iteration 429/1000 | Loss: 0.00002861
Iteration 430/1000 | Loss: 0.00002861
Iteration 431/1000 | Loss: 0.00002861
Iteration 432/1000 | Loss: 0.00002861
Iteration 433/1000 | Loss: 0.00002861
Iteration 434/1000 | Loss: 0.00002861
Iteration 435/1000 | Loss: 0.00002861
Iteration 436/1000 | Loss: 0.00002861
Iteration 437/1000 | Loss: 0.00002861
Iteration 438/1000 | Loss: 0.00002861
Iteration 439/1000 | Loss: 0.00002861
Iteration 440/1000 | Loss: 0.00002861
Iteration 441/1000 | Loss: 0.00002861
Iteration 442/1000 | Loss: 0.00002861
Iteration 443/1000 | Loss: 0.00002861
Iteration 444/1000 | Loss: 0.00002861
Iteration 445/1000 | Loss: 0.00002861
Iteration 446/1000 | Loss: 0.00002861
Iteration 447/1000 | Loss: 0.00002861
Iteration 448/1000 | Loss: 0.00002861
Iteration 449/1000 | Loss: 0.00002861
Iteration 450/1000 | Loss: 0.00002861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 450. Stopping optimization.
Last 5 losses: [2.861076245608274e-05, 2.861076245608274e-05, 2.861076245608274e-05, 2.861076245608274e-05, 2.861076245608274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.861076245608274e-05

Optimization complete. Final v2v error: 4.2690348625183105 mm

Highest mean error: 15.069803237915039 mm for frame 94

Lowest mean error: 3.7822039127349854 mm for frame 39

Saving results

Total time: 487.5185651779175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844264
Iteration 2/25 | Loss: 0.00146948
Iteration 3/25 | Loss: 0.00118483
Iteration 4/25 | Loss: 0.00115743
Iteration 5/25 | Loss: 0.00115248
Iteration 6/25 | Loss: 0.00114992
Iteration 7/25 | Loss: 0.00114980
Iteration 8/25 | Loss: 0.00114980
Iteration 9/25 | Loss: 0.00114980
Iteration 10/25 | Loss: 0.00114980
Iteration 11/25 | Loss: 0.00114980
Iteration 12/25 | Loss: 0.00114980
Iteration 13/25 | Loss: 0.00114980
Iteration 14/25 | Loss: 0.00114980
Iteration 15/25 | Loss: 0.00114980
Iteration 16/25 | Loss: 0.00114980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011497968807816505, 0.0011497968807816505, 0.0011497968807816505, 0.0011497968807816505, 0.0011497968807816505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011497968807816505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15026891
Iteration 2/25 | Loss: 0.00080691
Iteration 3/25 | Loss: 0.00080689
Iteration 4/25 | Loss: 0.00080689
Iteration 5/25 | Loss: 0.00080689
Iteration 6/25 | Loss: 0.00080689
Iteration 7/25 | Loss: 0.00080689
Iteration 8/25 | Loss: 0.00080689
Iteration 9/25 | Loss: 0.00080689
Iteration 10/25 | Loss: 0.00080689
Iteration 11/25 | Loss: 0.00080689
Iteration 12/25 | Loss: 0.00080689
Iteration 13/25 | Loss: 0.00080689
Iteration 14/25 | Loss: 0.00080689
Iteration 15/25 | Loss: 0.00080689
Iteration 16/25 | Loss: 0.00080689
Iteration 17/25 | Loss: 0.00080689
Iteration 18/25 | Loss: 0.00080689
Iteration 19/25 | Loss: 0.00080689
Iteration 20/25 | Loss: 0.00080689
Iteration 21/25 | Loss: 0.00080689
Iteration 22/25 | Loss: 0.00080689
Iteration 23/25 | Loss: 0.00080689
Iteration 24/25 | Loss: 0.00080689
Iteration 25/25 | Loss: 0.00080689

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080689
Iteration 2/1000 | Loss: 0.00003509
Iteration 3/1000 | Loss: 0.00002980
Iteration 4/1000 | Loss: 0.00002798
Iteration 5/1000 | Loss: 0.00002686
Iteration 6/1000 | Loss: 0.00002641
Iteration 7/1000 | Loss: 0.00002605
Iteration 8/1000 | Loss: 0.00002585
Iteration 9/1000 | Loss: 0.00002567
Iteration 10/1000 | Loss: 0.00002563
Iteration 11/1000 | Loss: 0.00002552
Iteration 12/1000 | Loss: 0.00002544
Iteration 13/1000 | Loss: 0.00002536
Iteration 14/1000 | Loss: 0.00002535
Iteration 15/1000 | Loss: 0.00002535
Iteration 16/1000 | Loss: 0.00002535
Iteration 17/1000 | Loss: 0.00002534
Iteration 18/1000 | Loss: 0.00002533
Iteration 19/1000 | Loss: 0.00002528
Iteration 20/1000 | Loss: 0.00002524
Iteration 21/1000 | Loss: 0.00002522
Iteration 22/1000 | Loss: 0.00002521
Iteration 23/1000 | Loss: 0.00002521
Iteration 24/1000 | Loss: 0.00002519
Iteration 25/1000 | Loss: 0.00002518
Iteration 26/1000 | Loss: 0.00002517
Iteration 27/1000 | Loss: 0.00002517
Iteration 28/1000 | Loss: 0.00002516
Iteration 29/1000 | Loss: 0.00002516
Iteration 30/1000 | Loss: 0.00002514
Iteration 31/1000 | Loss: 0.00002514
Iteration 32/1000 | Loss: 0.00002514
Iteration 33/1000 | Loss: 0.00002514
Iteration 34/1000 | Loss: 0.00002514
Iteration 35/1000 | Loss: 0.00002514
Iteration 36/1000 | Loss: 0.00002514
Iteration 37/1000 | Loss: 0.00002514
Iteration 38/1000 | Loss: 0.00002514
Iteration 39/1000 | Loss: 0.00002510
Iteration 40/1000 | Loss: 0.00002510
Iteration 41/1000 | Loss: 0.00002510
Iteration 42/1000 | Loss: 0.00002509
Iteration 43/1000 | Loss: 0.00002509
Iteration 44/1000 | Loss: 0.00002509
Iteration 45/1000 | Loss: 0.00002509
Iteration 46/1000 | Loss: 0.00002509
Iteration 47/1000 | Loss: 0.00002509
Iteration 48/1000 | Loss: 0.00002509
Iteration 49/1000 | Loss: 0.00002509
Iteration 50/1000 | Loss: 0.00002508
Iteration 51/1000 | Loss: 0.00002508
Iteration 52/1000 | Loss: 0.00002507
Iteration 53/1000 | Loss: 0.00002507
Iteration 54/1000 | Loss: 0.00002507
Iteration 55/1000 | Loss: 0.00002507
Iteration 56/1000 | Loss: 0.00002507
Iteration 57/1000 | Loss: 0.00002507
Iteration 58/1000 | Loss: 0.00002507
Iteration 59/1000 | Loss: 0.00002507
Iteration 60/1000 | Loss: 0.00002507
Iteration 61/1000 | Loss: 0.00002506
Iteration 62/1000 | Loss: 0.00002506
Iteration 63/1000 | Loss: 0.00002506
Iteration 64/1000 | Loss: 0.00002506
Iteration 65/1000 | Loss: 0.00002505
Iteration 66/1000 | Loss: 0.00002505
Iteration 67/1000 | Loss: 0.00002505
Iteration 68/1000 | Loss: 0.00002505
Iteration 69/1000 | Loss: 0.00002505
Iteration 70/1000 | Loss: 0.00002504
Iteration 71/1000 | Loss: 0.00002504
Iteration 72/1000 | Loss: 0.00002504
Iteration 73/1000 | Loss: 0.00002504
Iteration 74/1000 | Loss: 0.00002504
Iteration 75/1000 | Loss: 0.00002503
Iteration 76/1000 | Loss: 0.00002503
Iteration 77/1000 | Loss: 0.00002502
Iteration 78/1000 | Loss: 0.00002502
Iteration 79/1000 | Loss: 0.00002502
Iteration 80/1000 | Loss: 0.00002502
Iteration 81/1000 | Loss: 0.00002501
Iteration 82/1000 | Loss: 0.00002501
Iteration 83/1000 | Loss: 0.00002501
Iteration 84/1000 | Loss: 0.00002501
Iteration 85/1000 | Loss: 0.00002501
Iteration 86/1000 | Loss: 0.00002501
Iteration 87/1000 | Loss: 0.00002501
Iteration 88/1000 | Loss: 0.00002501
Iteration 89/1000 | Loss: 0.00002501
Iteration 90/1000 | Loss: 0.00002501
Iteration 91/1000 | Loss: 0.00002501
Iteration 92/1000 | Loss: 0.00002501
Iteration 93/1000 | Loss: 0.00002501
Iteration 94/1000 | Loss: 0.00002501
Iteration 95/1000 | Loss: 0.00002501
Iteration 96/1000 | Loss: 0.00002501
Iteration 97/1000 | Loss: 0.00002501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.5013987396960147e-05, 2.5013987396960147e-05, 2.5013987396960147e-05, 2.5013987396960147e-05, 2.5013987396960147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5013987396960147e-05

Optimization complete. Final v2v error: 4.341062545776367 mm

Highest mean error: 4.561003684997559 mm for frame 130

Lowest mean error: 3.947993278503418 mm for frame 142

Saving results

Total time: 31.89696455001831
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869893
Iteration 2/25 | Loss: 0.00217583
Iteration 3/25 | Loss: 0.00177177
Iteration 4/25 | Loss: 0.00145566
Iteration 5/25 | Loss: 0.00143061
Iteration 6/25 | Loss: 0.00137119
Iteration 7/25 | Loss: 0.00131908
Iteration 8/25 | Loss: 0.00131338
Iteration 9/25 | Loss: 0.00131963
Iteration 10/25 | Loss: 0.00131694
Iteration 11/25 | Loss: 0.00130288
Iteration 12/25 | Loss: 0.00130329
Iteration 13/25 | Loss: 0.00129654
Iteration 14/25 | Loss: 0.00129203
Iteration 15/25 | Loss: 0.00128974
Iteration 16/25 | Loss: 0.00128891
Iteration 17/25 | Loss: 0.00128684
Iteration 18/25 | Loss: 0.00128727
Iteration 19/25 | Loss: 0.00128675
Iteration 20/25 | Loss: 0.00128703
Iteration 21/25 | Loss: 0.00128521
Iteration 22/25 | Loss: 0.00128883
Iteration 23/25 | Loss: 0.00128759
Iteration 24/25 | Loss: 0.00128888
Iteration 25/25 | Loss: 0.00128752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88726497
Iteration 2/25 | Loss: 0.00146949
Iteration 3/25 | Loss: 0.00116292
Iteration 4/25 | Loss: 0.00116292
Iteration 5/25 | Loss: 0.00116291
Iteration 6/25 | Loss: 0.00116291
Iteration 7/25 | Loss: 0.00116291
Iteration 8/25 | Loss: 0.00116291
Iteration 9/25 | Loss: 0.00116291
Iteration 10/25 | Loss: 0.00116291
Iteration 11/25 | Loss: 0.00116291
Iteration 12/25 | Loss: 0.00116291
Iteration 13/25 | Loss: 0.00116291
Iteration 14/25 | Loss: 0.00116291
Iteration 15/25 | Loss: 0.00116291
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011629135115072131, 0.0011629135115072131, 0.0011629135115072131, 0.0011629135115072131, 0.0011629135115072131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011629135115072131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116291
Iteration 2/1000 | Loss: 0.00007471
Iteration 3/1000 | Loss: 0.00005242
Iteration 4/1000 | Loss: 0.00022154
Iteration 5/1000 | Loss: 0.00004656
Iteration 6/1000 | Loss: 0.00004436
Iteration 7/1000 | Loss: 0.00004306
Iteration 8/1000 | Loss: 0.00070286
Iteration 9/1000 | Loss: 0.00075049
Iteration 10/1000 | Loss: 0.00027024
Iteration 11/1000 | Loss: 0.00025342
Iteration 12/1000 | Loss: 0.00004847
Iteration 13/1000 | Loss: 0.00017258
Iteration 14/1000 | Loss: 0.00012031
Iteration 15/1000 | Loss: 0.00061020
Iteration 16/1000 | Loss: 0.00004021
Iteration 17/1000 | Loss: 0.00014042
Iteration 18/1000 | Loss: 0.00015476
Iteration 19/1000 | Loss: 0.00012749
Iteration 20/1000 | Loss: 0.00005894
Iteration 21/1000 | Loss: 0.00017789
Iteration 22/1000 | Loss: 0.00005923
Iteration 23/1000 | Loss: 0.00010552
Iteration 24/1000 | Loss: 0.00003965
Iteration 25/1000 | Loss: 0.00003783
Iteration 26/1000 | Loss: 0.00003709
Iteration 27/1000 | Loss: 0.00003669
Iteration 28/1000 | Loss: 0.00003646
Iteration 29/1000 | Loss: 0.00003703
Iteration 30/1000 | Loss: 0.00003647
Iteration 31/1000 | Loss: 0.00003625
Iteration 32/1000 | Loss: 0.00003621
Iteration 33/1000 | Loss: 0.00003621
Iteration 34/1000 | Loss: 0.00003619
Iteration 35/1000 | Loss: 0.00003618
Iteration 36/1000 | Loss: 0.00003618
Iteration 37/1000 | Loss: 0.00003688
Iteration 38/1000 | Loss: 0.00003624
Iteration 39/1000 | Loss: 0.00003608
Iteration 40/1000 | Loss: 0.00003608
Iteration 41/1000 | Loss: 0.00003608
Iteration 42/1000 | Loss: 0.00003608
Iteration 43/1000 | Loss: 0.00003608
Iteration 44/1000 | Loss: 0.00003608
Iteration 45/1000 | Loss: 0.00003608
Iteration 46/1000 | Loss: 0.00003608
Iteration 47/1000 | Loss: 0.00003607
Iteration 48/1000 | Loss: 0.00003607
Iteration 49/1000 | Loss: 0.00003607
Iteration 50/1000 | Loss: 0.00003607
Iteration 51/1000 | Loss: 0.00003607
Iteration 52/1000 | Loss: 0.00003607
Iteration 53/1000 | Loss: 0.00003607
Iteration 54/1000 | Loss: 0.00003607
Iteration 55/1000 | Loss: 0.00003607
Iteration 56/1000 | Loss: 0.00003607
Iteration 57/1000 | Loss: 0.00003607
Iteration 58/1000 | Loss: 0.00003607
Iteration 59/1000 | Loss: 0.00003607
Iteration 60/1000 | Loss: 0.00003607
Iteration 61/1000 | Loss: 0.00003607
Iteration 62/1000 | Loss: 0.00003606
Iteration 63/1000 | Loss: 0.00003606
Iteration 64/1000 | Loss: 0.00003606
Iteration 65/1000 | Loss: 0.00003605
Iteration 66/1000 | Loss: 0.00003605
Iteration 67/1000 | Loss: 0.00003605
Iteration 68/1000 | Loss: 0.00003605
Iteration 69/1000 | Loss: 0.00003605
Iteration 70/1000 | Loss: 0.00003604
Iteration 71/1000 | Loss: 0.00003604
Iteration 72/1000 | Loss: 0.00003604
Iteration 73/1000 | Loss: 0.00003604
Iteration 74/1000 | Loss: 0.00003604
Iteration 75/1000 | Loss: 0.00003604
Iteration 76/1000 | Loss: 0.00003604
Iteration 77/1000 | Loss: 0.00003604
Iteration 78/1000 | Loss: 0.00003604
Iteration 79/1000 | Loss: 0.00003604
Iteration 80/1000 | Loss: 0.00003604
Iteration 81/1000 | Loss: 0.00003604
Iteration 82/1000 | Loss: 0.00003604
Iteration 83/1000 | Loss: 0.00003604
Iteration 84/1000 | Loss: 0.00003604
Iteration 85/1000 | Loss: 0.00003604
Iteration 86/1000 | Loss: 0.00003604
Iteration 87/1000 | Loss: 0.00003604
Iteration 88/1000 | Loss: 0.00003604
Iteration 89/1000 | Loss: 0.00003604
Iteration 90/1000 | Loss: 0.00003604
Iteration 91/1000 | Loss: 0.00003604
Iteration 92/1000 | Loss: 0.00003604
Iteration 93/1000 | Loss: 0.00003604
Iteration 94/1000 | Loss: 0.00003604
Iteration 95/1000 | Loss: 0.00003604
Iteration 96/1000 | Loss: 0.00003604
Iteration 97/1000 | Loss: 0.00003604
Iteration 98/1000 | Loss: 0.00003604
Iteration 99/1000 | Loss: 0.00003604
Iteration 100/1000 | Loss: 0.00003604
Iteration 101/1000 | Loss: 0.00003604
Iteration 102/1000 | Loss: 0.00003604
Iteration 103/1000 | Loss: 0.00003604
Iteration 104/1000 | Loss: 0.00003604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [3.603770528570749e-05, 3.603770528570749e-05, 3.603770528570749e-05, 3.603770528570749e-05, 3.603770528570749e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.603770528570749e-05

Optimization complete. Final v2v error: 4.746220588684082 mm

Highest mean error: 22.794906616210938 mm for frame 211

Lowest mean error: 4.371637344360352 mm for frame 35

Saving results

Total time: 108.82042026519775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945135
Iteration 2/25 | Loss: 0.00172727
Iteration 3/25 | Loss: 0.00134764
Iteration 4/25 | Loss: 0.00129372
Iteration 5/25 | Loss: 0.00127928
Iteration 6/25 | Loss: 0.00127603
Iteration 7/25 | Loss: 0.00127518
Iteration 8/25 | Loss: 0.00127518
Iteration 9/25 | Loss: 0.00127518
Iteration 10/25 | Loss: 0.00127518
Iteration 11/25 | Loss: 0.00127518
Iteration 12/25 | Loss: 0.00127518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012751806061714888, 0.0012751806061714888, 0.0012751806061714888, 0.0012751806061714888, 0.0012751806061714888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012751806061714888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85674024
Iteration 2/25 | Loss: 0.00101964
Iteration 3/25 | Loss: 0.00101964
Iteration 4/25 | Loss: 0.00101964
Iteration 5/25 | Loss: 0.00101964
Iteration 6/25 | Loss: 0.00101964
Iteration 7/25 | Loss: 0.00101964
Iteration 8/25 | Loss: 0.00101964
Iteration 9/25 | Loss: 0.00101963
Iteration 10/25 | Loss: 0.00101963
Iteration 11/25 | Loss: 0.00101963
Iteration 12/25 | Loss: 0.00101963
Iteration 13/25 | Loss: 0.00101963
Iteration 14/25 | Loss: 0.00101963
Iteration 15/25 | Loss: 0.00101963
Iteration 16/25 | Loss: 0.00101963
Iteration 17/25 | Loss: 0.00101963
Iteration 18/25 | Loss: 0.00101963
Iteration 19/25 | Loss: 0.00101963
Iteration 20/25 | Loss: 0.00101963
Iteration 21/25 | Loss: 0.00101963
Iteration 22/25 | Loss: 0.00101963
Iteration 23/25 | Loss: 0.00101963
Iteration 24/25 | Loss: 0.00101963
Iteration 25/25 | Loss: 0.00101963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101963
Iteration 2/1000 | Loss: 0.00009347
Iteration 3/1000 | Loss: 0.00006197
Iteration 4/1000 | Loss: 0.00005377
Iteration 5/1000 | Loss: 0.00004883
Iteration 6/1000 | Loss: 0.00004605
Iteration 7/1000 | Loss: 0.00004399
Iteration 8/1000 | Loss: 0.00004262
Iteration 9/1000 | Loss: 0.00004181
Iteration 10/1000 | Loss: 0.00004121
Iteration 11/1000 | Loss: 0.00004079
Iteration 12/1000 | Loss: 0.00004052
Iteration 13/1000 | Loss: 0.00004051
Iteration 14/1000 | Loss: 0.00004049
Iteration 15/1000 | Loss: 0.00004045
Iteration 16/1000 | Loss: 0.00004045
Iteration 17/1000 | Loss: 0.00004044
Iteration 18/1000 | Loss: 0.00004044
Iteration 19/1000 | Loss: 0.00004043
Iteration 20/1000 | Loss: 0.00004042
Iteration 21/1000 | Loss: 0.00004042
Iteration 22/1000 | Loss: 0.00004041
Iteration 23/1000 | Loss: 0.00004041
Iteration 24/1000 | Loss: 0.00004041
Iteration 25/1000 | Loss: 0.00004041
Iteration 26/1000 | Loss: 0.00004041
Iteration 27/1000 | Loss: 0.00004040
Iteration 28/1000 | Loss: 0.00004040
Iteration 29/1000 | Loss: 0.00004040
Iteration 30/1000 | Loss: 0.00004039
Iteration 31/1000 | Loss: 0.00004039
Iteration 32/1000 | Loss: 0.00004039
Iteration 33/1000 | Loss: 0.00004039
Iteration 34/1000 | Loss: 0.00004038
Iteration 35/1000 | Loss: 0.00004038
Iteration 36/1000 | Loss: 0.00004038
Iteration 37/1000 | Loss: 0.00004038
Iteration 38/1000 | Loss: 0.00004037
Iteration 39/1000 | Loss: 0.00004037
Iteration 40/1000 | Loss: 0.00004037
Iteration 41/1000 | Loss: 0.00004036
Iteration 42/1000 | Loss: 0.00004036
Iteration 43/1000 | Loss: 0.00004036
Iteration 44/1000 | Loss: 0.00004035
Iteration 45/1000 | Loss: 0.00004035
Iteration 46/1000 | Loss: 0.00004035
Iteration 47/1000 | Loss: 0.00004035
Iteration 48/1000 | Loss: 0.00004035
Iteration 49/1000 | Loss: 0.00004034
Iteration 50/1000 | Loss: 0.00004034
Iteration 51/1000 | Loss: 0.00004034
Iteration 52/1000 | Loss: 0.00004033
Iteration 53/1000 | Loss: 0.00004033
Iteration 54/1000 | Loss: 0.00004033
Iteration 55/1000 | Loss: 0.00004033
Iteration 56/1000 | Loss: 0.00004032
Iteration 57/1000 | Loss: 0.00004032
Iteration 58/1000 | Loss: 0.00004032
Iteration 59/1000 | Loss: 0.00004032
Iteration 60/1000 | Loss: 0.00004031
Iteration 61/1000 | Loss: 0.00004031
Iteration 62/1000 | Loss: 0.00004031
Iteration 63/1000 | Loss: 0.00004031
Iteration 64/1000 | Loss: 0.00004030
Iteration 65/1000 | Loss: 0.00004030
Iteration 66/1000 | Loss: 0.00004030
Iteration 67/1000 | Loss: 0.00004030
Iteration 68/1000 | Loss: 0.00004030
Iteration 69/1000 | Loss: 0.00004030
Iteration 70/1000 | Loss: 0.00004030
Iteration 71/1000 | Loss: 0.00004030
Iteration 72/1000 | Loss: 0.00004030
Iteration 73/1000 | Loss: 0.00004029
Iteration 74/1000 | Loss: 0.00004029
Iteration 75/1000 | Loss: 0.00004029
Iteration 76/1000 | Loss: 0.00004029
Iteration 77/1000 | Loss: 0.00004029
Iteration 78/1000 | Loss: 0.00004029
Iteration 79/1000 | Loss: 0.00004029
Iteration 80/1000 | Loss: 0.00004029
Iteration 81/1000 | Loss: 0.00004029
Iteration 82/1000 | Loss: 0.00004029
Iteration 83/1000 | Loss: 0.00004029
Iteration 84/1000 | Loss: 0.00004029
Iteration 85/1000 | Loss: 0.00004029
Iteration 86/1000 | Loss: 0.00004029
Iteration 87/1000 | Loss: 0.00004029
Iteration 88/1000 | Loss: 0.00004029
Iteration 89/1000 | Loss: 0.00004029
Iteration 90/1000 | Loss: 0.00004029
Iteration 91/1000 | Loss: 0.00004029
Iteration 92/1000 | Loss: 0.00004029
Iteration 93/1000 | Loss: 0.00004029
Iteration 94/1000 | Loss: 0.00004029
Iteration 95/1000 | Loss: 0.00004029
Iteration 96/1000 | Loss: 0.00004029
Iteration 97/1000 | Loss: 0.00004029
Iteration 98/1000 | Loss: 0.00004029
Iteration 99/1000 | Loss: 0.00004029
Iteration 100/1000 | Loss: 0.00004029
Iteration 101/1000 | Loss: 0.00004029
Iteration 102/1000 | Loss: 0.00004029
Iteration 103/1000 | Loss: 0.00004029
Iteration 104/1000 | Loss: 0.00004029
Iteration 105/1000 | Loss: 0.00004029
Iteration 106/1000 | Loss: 0.00004029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [4.028779221698642e-05, 4.028779221698642e-05, 4.028779221698642e-05, 4.028779221698642e-05, 4.028779221698642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.028779221698642e-05

Optimization complete. Final v2v error: 5.28171968460083 mm

Highest mean error: 5.7732954025268555 mm for frame 32

Lowest mean error: 4.835466384887695 mm for frame 114

Saving results

Total time: 32.82292318344116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053799
Iteration 2/25 | Loss: 0.00230123
Iteration 3/25 | Loss: 0.00171019
Iteration 4/25 | Loss: 0.00154857
Iteration 5/25 | Loss: 0.00154609
Iteration 6/25 | Loss: 0.00149102
Iteration 7/25 | Loss: 0.00141684
Iteration 8/25 | Loss: 0.00138530
Iteration 9/25 | Loss: 0.00135824
Iteration 10/25 | Loss: 0.00134536
Iteration 11/25 | Loss: 0.00134152
Iteration 12/25 | Loss: 0.00133024
Iteration 13/25 | Loss: 0.00132874
Iteration 14/25 | Loss: 0.00132798
Iteration 15/25 | Loss: 0.00132366
Iteration 16/25 | Loss: 0.00132190
Iteration 17/25 | Loss: 0.00131879
Iteration 18/25 | Loss: 0.00131804
Iteration 19/25 | Loss: 0.00131777
Iteration 20/25 | Loss: 0.00131951
Iteration 21/25 | Loss: 0.00131697
Iteration 22/25 | Loss: 0.00131646
Iteration 23/25 | Loss: 0.00131587
Iteration 24/25 | Loss: 0.00131566
Iteration 25/25 | Loss: 0.00131536

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34170628
Iteration 2/25 | Loss: 0.00228959
Iteration 3/25 | Loss: 0.00143804
Iteration 4/25 | Loss: 0.00143804
Iteration 5/25 | Loss: 0.00143804
Iteration 6/25 | Loss: 0.00143804
Iteration 7/25 | Loss: 0.00143804
Iteration 8/25 | Loss: 0.00143804
Iteration 9/25 | Loss: 0.00143804
Iteration 10/25 | Loss: 0.00143804
Iteration 11/25 | Loss: 0.00143804
Iteration 12/25 | Loss: 0.00143804
Iteration 13/25 | Loss: 0.00143804
Iteration 14/25 | Loss: 0.00143804
Iteration 15/25 | Loss: 0.00143804
Iteration 16/25 | Loss: 0.00143804
Iteration 17/25 | Loss: 0.00143804
Iteration 18/25 | Loss: 0.00143804
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014380364445969462, 0.0014380364445969462, 0.0014380364445969462, 0.0014380364445969462, 0.0014380364445969462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014380364445969462

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143804
Iteration 2/1000 | Loss: 0.00073294
Iteration 3/1000 | Loss: 0.00061171
Iteration 4/1000 | Loss: 0.00009690
Iteration 5/1000 | Loss: 0.00178920
Iteration 6/1000 | Loss: 0.00046129
Iteration 7/1000 | Loss: 0.00006979
Iteration 8/1000 | Loss: 0.00115582
Iteration 9/1000 | Loss: 0.00059885
Iteration 10/1000 | Loss: 0.00045126
Iteration 11/1000 | Loss: 0.00006004
Iteration 12/1000 | Loss: 0.00005248
Iteration 13/1000 | Loss: 0.00046758
Iteration 14/1000 | Loss: 0.00032080
Iteration 15/1000 | Loss: 0.00066588
Iteration 16/1000 | Loss: 0.00066253
Iteration 17/1000 | Loss: 0.00018734
Iteration 18/1000 | Loss: 0.00004882
Iteration 19/1000 | Loss: 0.00004660
Iteration 20/1000 | Loss: 0.00004502
Iteration 21/1000 | Loss: 0.00020527
Iteration 22/1000 | Loss: 0.00025224
Iteration 23/1000 | Loss: 0.00012925
Iteration 24/1000 | Loss: 0.00014932
Iteration 25/1000 | Loss: 0.00024133
Iteration 26/1000 | Loss: 0.00006845
Iteration 27/1000 | Loss: 0.00006353
Iteration 28/1000 | Loss: 0.00004964
Iteration 29/1000 | Loss: 0.00018371
Iteration 30/1000 | Loss: 0.00004649
Iteration 31/1000 | Loss: 0.00046566
Iteration 32/1000 | Loss: 0.00085054
Iteration 33/1000 | Loss: 0.00022859
Iteration 34/1000 | Loss: 0.00021973
Iteration 35/1000 | Loss: 0.00024295
Iteration 36/1000 | Loss: 0.00009370
Iteration 37/1000 | Loss: 0.00004724
Iteration 38/1000 | Loss: 0.00006171
Iteration 39/1000 | Loss: 0.00006040
Iteration 40/1000 | Loss: 0.00005913
Iteration 41/1000 | Loss: 0.00004196
Iteration 42/1000 | Loss: 0.00004071
Iteration 43/1000 | Loss: 0.00010583
Iteration 44/1000 | Loss: 0.00004106
Iteration 45/1000 | Loss: 0.00003929
Iteration 46/1000 | Loss: 0.00016251
Iteration 47/1000 | Loss: 0.00008108
Iteration 48/1000 | Loss: 0.00006471
Iteration 49/1000 | Loss: 0.00020347
Iteration 50/1000 | Loss: 0.00003864
Iteration 51/1000 | Loss: 0.00006639
Iteration 52/1000 | Loss: 0.00063597
Iteration 53/1000 | Loss: 0.00004904
Iteration 54/1000 | Loss: 0.00004197
Iteration 55/1000 | Loss: 0.00022831
Iteration 56/1000 | Loss: 0.00003707
Iteration 57/1000 | Loss: 0.00009810
Iteration 58/1000 | Loss: 0.00003658
Iteration 59/1000 | Loss: 0.00003572
Iteration 60/1000 | Loss: 0.00003525
Iteration 61/1000 | Loss: 0.00003485
Iteration 62/1000 | Loss: 0.00003454
Iteration 63/1000 | Loss: 0.00003428
Iteration 64/1000 | Loss: 0.00003416
Iteration 65/1000 | Loss: 0.00003414
Iteration 66/1000 | Loss: 0.00003412
Iteration 67/1000 | Loss: 0.00003412
Iteration 68/1000 | Loss: 0.00003406
Iteration 69/1000 | Loss: 0.00003406
Iteration 70/1000 | Loss: 0.00003405
Iteration 71/1000 | Loss: 0.00003405
Iteration 72/1000 | Loss: 0.00003405
Iteration 73/1000 | Loss: 0.00003405
Iteration 74/1000 | Loss: 0.00003405
Iteration 75/1000 | Loss: 0.00003405
Iteration 76/1000 | Loss: 0.00003405
Iteration 77/1000 | Loss: 0.00003405
Iteration 78/1000 | Loss: 0.00003405
Iteration 79/1000 | Loss: 0.00003404
Iteration 80/1000 | Loss: 0.00003404
Iteration 81/1000 | Loss: 0.00003404
Iteration 82/1000 | Loss: 0.00003404
Iteration 83/1000 | Loss: 0.00003404
Iteration 84/1000 | Loss: 0.00003404
Iteration 85/1000 | Loss: 0.00003403
Iteration 86/1000 | Loss: 0.00003403
Iteration 87/1000 | Loss: 0.00003403
Iteration 88/1000 | Loss: 0.00003403
Iteration 89/1000 | Loss: 0.00003403
Iteration 90/1000 | Loss: 0.00003403
Iteration 91/1000 | Loss: 0.00003402
Iteration 92/1000 | Loss: 0.00003402
Iteration 93/1000 | Loss: 0.00003402
Iteration 94/1000 | Loss: 0.00003401
Iteration 95/1000 | Loss: 0.00003401
Iteration 96/1000 | Loss: 0.00003401
Iteration 97/1000 | Loss: 0.00003401
Iteration 98/1000 | Loss: 0.00003400
Iteration 99/1000 | Loss: 0.00003400
Iteration 100/1000 | Loss: 0.00003400
Iteration 101/1000 | Loss: 0.00003400
Iteration 102/1000 | Loss: 0.00003399
Iteration 103/1000 | Loss: 0.00003399
Iteration 104/1000 | Loss: 0.00003398
Iteration 105/1000 | Loss: 0.00003398
Iteration 106/1000 | Loss: 0.00003398
Iteration 107/1000 | Loss: 0.00003397
Iteration 108/1000 | Loss: 0.00003397
Iteration 109/1000 | Loss: 0.00003397
Iteration 110/1000 | Loss: 0.00003396
Iteration 111/1000 | Loss: 0.00003396
Iteration 112/1000 | Loss: 0.00003396
Iteration 113/1000 | Loss: 0.00003396
Iteration 114/1000 | Loss: 0.00003395
Iteration 115/1000 | Loss: 0.00003395
Iteration 116/1000 | Loss: 0.00003395
Iteration 117/1000 | Loss: 0.00003394
Iteration 118/1000 | Loss: 0.00003394
Iteration 119/1000 | Loss: 0.00003394
Iteration 120/1000 | Loss: 0.00003394
Iteration 121/1000 | Loss: 0.00003394
Iteration 122/1000 | Loss: 0.00003394
Iteration 123/1000 | Loss: 0.00003394
Iteration 124/1000 | Loss: 0.00003394
Iteration 125/1000 | Loss: 0.00003393
Iteration 126/1000 | Loss: 0.00003393
Iteration 127/1000 | Loss: 0.00003393
Iteration 128/1000 | Loss: 0.00003393
Iteration 129/1000 | Loss: 0.00003393
Iteration 130/1000 | Loss: 0.00003392
Iteration 131/1000 | Loss: 0.00003392
Iteration 132/1000 | Loss: 0.00003392
Iteration 133/1000 | Loss: 0.00003392
Iteration 134/1000 | Loss: 0.00003391
Iteration 135/1000 | Loss: 0.00003391
Iteration 136/1000 | Loss: 0.00003391
Iteration 137/1000 | Loss: 0.00003391
Iteration 138/1000 | Loss: 0.00003390
Iteration 139/1000 | Loss: 0.00003390
Iteration 140/1000 | Loss: 0.00003390
Iteration 141/1000 | Loss: 0.00003390
Iteration 142/1000 | Loss: 0.00003390
Iteration 143/1000 | Loss: 0.00003390
Iteration 144/1000 | Loss: 0.00003390
Iteration 145/1000 | Loss: 0.00003390
Iteration 146/1000 | Loss: 0.00003390
Iteration 147/1000 | Loss: 0.00003389
Iteration 148/1000 | Loss: 0.00003389
Iteration 149/1000 | Loss: 0.00003389
Iteration 150/1000 | Loss: 0.00003389
Iteration 151/1000 | Loss: 0.00003389
Iteration 152/1000 | Loss: 0.00003389
Iteration 153/1000 | Loss: 0.00003388
Iteration 154/1000 | Loss: 0.00003388
Iteration 155/1000 | Loss: 0.00003388
Iteration 156/1000 | Loss: 0.00003388
Iteration 157/1000 | Loss: 0.00003388
Iteration 158/1000 | Loss: 0.00003388
Iteration 159/1000 | Loss: 0.00003388
Iteration 160/1000 | Loss: 0.00003388
Iteration 161/1000 | Loss: 0.00003388
Iteration 162/1000 | Loss: 0.00003388
Iteration 163/1000 | Loss: 0.00003388
Iteration 164/1000 | Loss: 0.00003388
Iteration 165/1000 | Loss: 0.00003388
Iteration 166/1000 | Loss: 0.00003388
Iteration 167/1000 | Loss: 0.00003388
Iteration 168/1000 | Loss: 0.00003388
Iteration 169/1000 | Loss: 0.00003388
Iteration 170/1000 | Loss: 0.00003388
Iteration 171/1000 | Loss: 0.00003388
Iteration 172/1000 | Loss: 0.00003388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [3.3878866815939546e-05, 3.3878866815939546e-05, 3.3878866815939546e-05, 3.3878866815939546e-05, 3.3878866815939546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3878866815939546e-05

Optimization complete. Final v2v error: 4.859947204589844 mm

Highest mean error: 12.33203411102295 mm for frame 26

Lowest mean error: 4.559656620025635 mm for frame 205

Saving results

Total time: 162.87343382835388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811977
Iteration 2/25 | Loss: 0.00171085
Iteration 3/25 | Loss: 0.00151757
Iteration 4/25 | Loss: 0.00169431
Iteration 5/25 | Loss: 0.00143853
Iteration 6/25 | Loss: 0.00137601
Iteration 7/25 | Loss: 0.00133535
Iteration 8/25 | Loss: 0.00143160
Iteration 9/25 | Loss: 0.00133294
Iteration 10/25 | Loss: 0.00128251
Iteration 11/25 | Loss: 0.00128060
Iteration 12/25 | Loss: 0.00127989
Iteration 13/25 | Loss: 0.00127931
Iteration 14/25 | Loss: 0.00127904
Iteration 15/25 | Loss: 0.00127889
Iteration 16/25 | Loss: 0.00127841
Iteration 17/25 | Loss: 0.00127732
Iteration 18/25 | Loss: 0.00130681
Iteration 19/25 | Loss: 0.00126588
Iteration 20/25 | Loss: 0.00126385
Iteration 21/25 | Loss: 0.00126366
Iteration 22/25 | Loss: 0.00126361
Iteration 23/25 | Loss: 0.00126361
Iteration 24/25 | Loss: 0.00126361
Iteration 25/25 | Loss: 0.00126361

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30268478
Iteration 2/25 | Loss: 0.00090123
Iteration 3/25 | Loss: 0.00090122
Iteration 4/25 | Loss: 0.00090122
Iteration 5/25 | Loss: 0.00090122
Iteration 6/25 | Loss: 0.00090122
Iteration 7/25 | Loss: 0.00090122
Iteration 8/25 | Loss: 0.00090122
Iteration 9/25 | Loss: 0.00090122
Iteration 10/25 | Loss: 0.00090122
Iteration 11/25 | Loss: 0.00090122
Iteration 12/25 | Loss: 0.00090122
Iteration 13/25 | Loss: 0.00090122
Iteration 14/25 | Loss: 0.00090122
Iteration 15/25 | Loss: 0.00090122
Iteration 16/25 | Loss: 0.00090122
Iteration 17/25 | Loss: 0.00090122
Iteration 18/25 | Loss: 0.00090122
Iteration 19/25 | Loss: 0.00090122
Iteration 20/25 | Loss: 0.00090122
Iteration 21/25 | Loss: 0.00090122
Iteration 22/25 | Loss: 0.00090122
Iteration 23/25 | Loss: 0.00090122
Iteration 24/25 | Loss: 0.00090122
Iteration 25/25 | Loss: 0.00090122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090122
Iteration 2/1000 | Loss: 0.00006187
Iteration 3/1000 | Loss: 0.00005045
Iteration 4/1000 | Loss: 0.00004556
Iteration 5/1000 | Loss: 0.00004284
Iteration 6/1000 | Loss: 0.00004174
Iteration 7/1000 | Loss: 0.00004061
Iteration 8/1000 | Loss: 0.00003976
Iteration 9/1000 | Loss: 0.00003917
Iteration 10/1000 | Loss: 0.00003870
Iteration 11/1000 | Loss: 0.00003840
Iteration 12/1000 | Loss: 0.00003828
Iteration 13/1000 | Loss: 0.00003824
Iteration 14/1000 | Loss: 0.00003823
Iteration 15/1000 | Loss: 0.00003811
Iteration 16/1000 | Loss: 0.00003800
Iteration 17/1000 | Loss: 0.00003792
Iteration 18/1000 | Loss: 0.00003791
Iteration 19/1000 | Loss: 0.00003791
Iteration 20/1000 | Loss: 0.00003790
Iteration 21/1000 | Loss: 0.00003790
Iteration 22/1000 | Loss: 0.00003787
Iteration 23/1000 | Loss: 0.00003787
Iteration 24/1000 | Loss: 0.00003787
Iteration 25/1000 | Loss: 0.00003787
Iteration 26/1000 | Loss: 0.00003786
Iteration 27/1000 | Loss: 0.00003786
Iteration 28/1000 | Loss: 0.00003786
Iteration 29/1000 | Loss: 0.00003786
Iteration 30/1000 | Loss: 0.00003786
Iteration 31/1000 | Loss: 0.00003784
Iteration 32/1000 | Loss: 0.00003783
Iteration 33/1000 | Loss: 0.00003783
Iteration 34/1000 | Loss: 0.00003782
Iteration 35/1000 | Loss: 0.00003782
Iteration 36/1000 | Loss: 0.00003781
Iteration 37/1000 | Loss: 0.00003781
Iteration 38/1000 | Loss: 0.00003780
Iteration 39/1000 | Loss: 0.00003780
Iteration 40/1000 | Loss: 0.00003780
Iteration 41/1000 | Loss: 0.00003779
Iteration 42/1000 | Loss: 0.00003779
Iteration 43/1000 | Loss: 0.00003779
Iteration 44/1000 | Loss: 0.00003779
Iteration 45/1000 | Loss: 0.00003778
Iteration 46/1000 | Loss: 0.00003778
Iteration 47/1000 | Loss: 0.00003778
Iteration 48/1000 | Loss: 0.00003778
Iteration 49/1000 | Loss: 0.00003777
Iteration 50/1000 | Loss: 0.00003777
Iteration 51/1000 | Loss: 0.00003776
Iteration 52/1000 | Loss: 0.00003776
Iteration 53/1000 | Loss: 0.00003776
Iteration 54/1000 | Loss: 0.00003775
Iteration 55/1000 | Loss: 0.00003775
Iteration 56/1000 | Loss: 0.00003775
Iteration 57/1000 | Loss: 0.00003774
Iteration 58/1000 | Loss: 0.00003774
Iteration 59/1000 | Loss: 0.00003774
Iteration 60/1000 | Loss: 0.00003773
Iteration 61/1000 | Loss: 0.00003773
Iteration 62/1000 | Loss: 0.00003773
Iteration 63/1000 | Loss: 0.00003773
Iteration 64/1000 | Loss: 0.00003772
Iteration 65/1000 | Loss: 0.00003772
Iteration 66/1000 | Loss: 0.00003772
Iteration 67/1000 | Loss: 0.00003771
Iteration 68/1000 | Loss: 0.00003771
Iteration 69/1000 | Loss: 0.00003771
Iteration 70/1000 | Loss: 0.00003771
Iteration 71/1000 | Loss: 0.00003771
Iteration 72/1000 | Loss: 0.00003770
Iteration 73/1000 | Loss: 0.00003770
Iteration 74/1000 | Loss: 0.00003770
Iteration 75/1000 | Loss: 0.00003770
Iteration 76/1000 | Loss: 0.00003770
Iteration 77/1000 | Loss: 0.00003769
Iteration 78/1000 | Loss: 0.00003769
Iteration 79/1000 | Loss: 0.00003769
Iteration 80/1000 | Loss: 0.00003769
Iteration 81/1000 | Loss: 0.00003768
Iteration 82/1000 | Loss: 0.00003768
Iteration 83/1000 | Loss: 0.00003768
Iteration 84/1000 | Loss: 0.00003768
Iteration 85/1000 | Loss: 0.00003768
Iteration 86/1000 | Loss: 0.00003767
Iteration 87/1000 | Loss: 0.00003767
Iteration 88/1000 | Loss: 0.00003767
Iteration 89/1000 | Loss: 0.00003767
Iteration 90/1000 | Loss: 0.00003767
Iteration 91/1000 | Loss: 0.00003767
Iteration 92/1000 | Loss: 0.00003767
Iteration 93/1000 | Loss: 0.00003767
Iteration 94/1000 | Loss: 0.00003767
Iteration 95/1000 | Loss: 0.00003767
Iteration 96/1000 | Loss: 0.00003767
Iteration 97/1000 | Loss: 0.00003767
Iteration 98/1000 | Loss: 0.00003766
Iteration 99/1000 | Loss: 0.00003766
Iteration 100/1000 | Loss: 0.00003766
Iteration 101/1000 | Loss: 0.00003766
Iteration 102/1000 | Loss: 0.00003766
Iteration 103/1000 | Loss: 0.00003766
Iteration 104/1000 | Loss: 0.00003766
Iteration 105/1000 | Loss: 0.00003765
Iteration 106/1000 | Loss: 0.00003765
Iteration 107/1000 | Loss: 0.00003765
Iteration 108/1000 | Loss: 0.00003765
Iteration 109/1000 | Loss: 0.00003764
Iteration 110/1000 | Loss: 0.00003764
Iteration 111/1000 | Loss: 0.00003764
Iteration 112/1000 | Loss: 0.00003764
Iteration 113/1000 | Loss: 0.00003764
Iteration 114/1000 | Loss: 0.00003764
Iteration 115/1000 | Loss: 0.00003764
Iteration 116/1000 | Loss: 0.00003764
Iteration 117/1000 | Loss: 0.00003764
Iteration 118/1000 | Loss: 0.00003764
Iteration 119/1000 | Loss: 0.00003764
Iteration 120/1000 | Loss: 0.00003764
Iteration 121/1000 | Loss: 0.00003764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [3.7638048524968326e-05, 3.7638048524968326e-05, 3.7638048524968326e-05, 3.7638048524968326e-05, 3.7638048524968326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7638048524968326e-05

Optimization complete. Final v2v error: 5.133999347686768 mm

Highest mean error: 5.81224250793457 mm for frame 88

Lowest mean error: 4.255525588989258 mm for frame 5

Saving results

Total time: 63.952006816864014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415660
Iteration 2/25 | Loss: 0.00135685
Iteration 3/25 | Loss: 0.00122607
Iteration 4/25 | Loss: 0.00119522
Iteration 5/25 | Loss: 0.00118650
Iteration 6/25 | Loss: 0.00118355
Iteration 7/25 | Loss: 0.00118232
Iteration 8/25 | Loss: 0.00118215
Iteration 9/25 | Loss: 0.00118215
Iteration 10/25 | Loss: 0.00118215
Iteration 11/25 | Loss: 0.00118215
Iteration 12/25 | Loss: 0.00118215
Iteration 13/25 | Loss: 0.00118215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011821538209915161, 0.0011821538209915161, 0.0011821538209915161, 0.0011821538209915161, 0.0011821538209915161]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011821538209915161

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22672951
Iteration 2/25 | Loss: 0.00097765
Iteration 3/25 | Loss: 0.00097764
Iteration 4/25 | Loss: 0.00097764
Iteration 5/25 | Loss: 0.00097764
Iteration 6/25 | Loss: 0.00097764
Iteration 7/25 | Loss: 0.00097764
Iteration 8/25 | Loss: 0.00097764
Iteration 9/25 | Loss: 0.00097764
Iteration 10/25 | Loss: 0.00097764
Iteration 11/25 | Loss: 0.00097764
Iteration 12/25 | Loss: 0.00097764
Iteration 13/25 | Loss: 0.00097764
Iteration 14/25 | Loss: 0.00097764
Iteration 15/25 | Loss: 0.00097764
Iteration 16/25 | Loss: 0.00097764
Iteration 17/25 | Loss: 0.00097764
Iteration 18/25 | Loss: 0.00097764
Iteration 19/25 | Loss: 0.00097764
Iteration 20/25 | Loss: 0.00097764
Iteration 21/25 | Loss: 0.00097764
Iteration 22/25 | Loss: 0.00097764
Iteration 23/25 | Loss: 0.00097764
Iteration 24/25 | Loss: 0.00097764
Iteration 25/25 | Loss: 0.00097764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097764
Iteration 2/1000 | Loss: 0.00005946
Iteration 3/1000 | Loss: 0.00003975
Iteration 4/1000 | Loss: 0.00003363
Iteration 5/1000 | Loss: 0.00003127
Iteration 6/1000 | Loss: 0.00002980
Iteration 7/1000 | Loss: 0.00002870
Iteration 8/1000 | Loss: 0.00002791
Iteration 9/1000 | Loss: 0.00002733
Iteration 10/1000 | Loss: 0.00002688
Iteration 11/1000 | Loss: 0.00002654
Iteration 12/1000 | Loss: 0.00002628
Iteration 13/1000 | Loss: 0.00002614
Iteration 14/1000 | Loss: 0.00002606
Iteration 15/1000 | Loss: 0.00002603
Iteration 16/1000 | Loss: 0.00002598
Iteration 17/1000 | Loss: 0.00002597
Iteration 18/1000 | Loss: 0.00002595
Iteration 19/1000 | Loss: 0.00002593
Iteration 20/1000 | Loss: 0.00002586
Iteration 21/1000 | Loss: 0.00002583
Iteration 22/1000 | Loss: 0.00002583
Iteration 23/1000 | Loss: 0.00002581
Iteration 24/1000 | Loss: 0.00002580
Iteration 25/1000 | Loss: 0.00002580
Iteration 26/1000 | Loss: 0.00002580
Iteration 27/1000 | Loss: 0.00002578
Iteration 28/1000 | Loss: 0.00002577
Iteration 29/1000 | Loss: 0.00002577
Iteration 30/1000 | Loss: 0.00002576
Iteration 31/1000 | Loss: 0.00002575
Iteration 32/1000 | Loss: 0.00002575
Iteration 33/1000 | Loss: 0.00002575
Iteration 34/1000 | Loss: 0.00002574
Iteration 35/1000 | Loss: 0.00002574
Iteration 36/1000 | Loss: 0.00002573
Iteration 37/1000 | Loss: 0.00002573
Iteration 38/1000 | Loss: 0.00002573
Iteration 39/1000 | Loss: 0.00002572
Iteration 40/1000 | Loss: 0.00002572
Iteration 41/1000 | Loss: 0.00002572
Iteration 42/1000 | Loss: 0.00002571
Iteration 43/1000 | Loss: 0.00002571
Iteration 44/1000 | Loss: 0.00002571
Iteration 45/1000 | Loss: 0.00002570
Iteration 46/1000 | Loss: 0.00002570
Iteration 47/1000 | Loss: 0.00002570
Iteration 48/1000 | Loss: 0.00002569
Iteration 49/1000 | Loss: 0.00002569
Iteration 50/1000 | Loss: 0.00002569
Iteration 51/1000 | Loss: 0.00002569
Iteration 52/1000 | Loss: 0.00002569
Iteration 53/1000 | Loss: 0.00002569
Iteration 54/1000 | Loss: 0.00002568
Iteration 55/1000 | Loss: 0.00002568
Iteration 56/1000 | Loss: 0.00002568
Iteration 57/1000 | Loss: 0.00002567
Iteration 58/1000 | Loss: 0.00002567
Iteration 59/1000 | Loss: 0.00002567
Iteration 60/1000 | Loss: 0.00002567
Iteration 61/1000 | Loss: 0.00002566
Iteration 62/1000 | Loss: 0.00002566
Iteration 63/1000 | Loss: 0.00002566
Iteration 64/1000 | Loss: 0.00002566
Iteration 65/1000 | Loss: 0.00002566
Iteration 66/1000 | Loss: 0.00002566
Iteration 67/1000 | Loss: 0.00002566
Iteration 68/1000 | Loss: 0.00002566
Iteration 69/1000 | Loss: 0.00002566
Iteration 70/1000 | Loss: 0.00002566
Iteration 71/1000 | Loss: 0.00002566
Iteration 72/1000 | Loss: 0.00002566
Iteration 73/1000 | Loss: 0.00002566
Iteration 74/1000 | Loss: 0.00002565
Iteration 75/1000 | Loss: 0.00002565
Iteration 76/1000 | Loss: 0.00002565
Iteration 77/1000 | Loss: 0.00002565
Iteration 78/1000 | Loss: 0.00002565
Iteration 79/1000 | Loss: 0.00002565
Iteration 80/1000 | Loss: 0.00002565
Iteration 81/1000 | Loss: 0.00002564
Iteration 82/1000 | Loss: 0.00002564
Iteration 83/1000 | Loss: 0.00002564
Iteration 84/1000 | Loss: 0.00002564
Iteration 85/1000 | Loss: 0.00002564
Iteration 86/1000 | Loss: 0.00002563
Iteration 87/1000 | Loss: 0.00002563
Iteration 88/1000 | Loss: 0.00002563
Iteration 89/1000 | Loss: 0.00002563
Iteration 90/1000 | Loss: 0.00002563
Iteration 91/1000 | Loss: 0.00002563
Iteration 92/1000 | Loss: 0.00002563
Iteration 93/1000 | Loss: 0.00002563
Iteration 94/1000 | Loss: 0.00002563
Iteration 95/1000 | Loss: 0.00002563
Iteration 96/1000 | Loss: 0.00002562
Iteration 97/1000 | Loss: 0.00002562
Iteration 98/1000 | Loss: 0.00002562
Iteration 99/1000 | Loss: 0.00002562
Iteration 100/1000 | Loss: 0.00002562
Iteration 101/1000 | Loss: 0.00002562
Iteration 102/1000 | Loss: 0.00002562
Iteration 103/1000 | Loss: 0.00002562
Iteration 104/1000 | Loss: 0.00002562
Iteration 105/1000 | Loss: 0.00002562
Iteration 106/1000 | Loss: 0.00002562
Iteration 107/1000 | Loss: 0.00002562
Iteration 108/1000 | Loss: 0.00002562
Iteration 109/1000 | Loss: 0.00002562
Iteration 110/1000 | Loss: 0.00002562
Iteration 111/1000 | Loss: 0.00002562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.5618934159865603e-05, 2.5618934159865603e-05, 2.5618934159865603e-05, 2.5618934159865603e-05, 2.5618934159865603e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5618934159865603e-05

Optimization complete. Final v2v error: 4.342782974243164 mm

Highest mean error: 5.347721576690674 mm for frame 92

Lowest mean error: 3.702688455581665 mm for frame 4

Saving results

Total time: 38.284223794937134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835997
Iteration 2/25 | Loss: 0.00138308
Iteration 3/25 | Loss: 0.00118375
Iteration 4/25 | Loss: 0.00116015
Iteration 5/25 | Loss: 0.00115313
Iteration 6/25 | Loss: 0.00115029
Iteration 7/25 | Loss: 0.00114916
Iteration 8/25 | Loss: 0.00114915
Iteration 9/25 | Loss: 0.00114915
Iteration 10/25 | Loss: 0.00114915
Iteration 11/25 | Loss: 0.00114915
Iteration 12/25 | Loss: 0.00114915
Iteration 13/25 | Loss: 0.00114915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011491505429148674, 0.0011491505429148674, 0.0011491505429148674, 0.0011491505429148674, 0.0011491505429148674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011491505429148674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81818438
Iteration 2/25 | Loss: 0.00105087
Iteration 3/25 | Loss: 0.00105087
Iteration 4/25 | Loss: 0.00105087
Iteration 5/25 | Loss: 0.00105087
Iteration 6/25 | Loss: 0.00105087
Iteration 7/25 | Loss: 0.00105087
Iteration 8/25 | Loss: 0.00105087
Iteration 9/25 | Loss: 0.00105087
Iteration 10/25 | Loss: 0.00105087
Iteration 11/25 | Loss: 0.00105087
Iteration 12/25 | Loss: 0.00105087
Iteration 13/25 | Loss: 0.00105087
Iteration 14/25 | Loss: 0.00105087
Iteration 15/25 | Loss: 0.00105087
Iteration 16/25 | Loss: 0.00105087
Iteration 17/25 | Loss: 0.00105087
Iteration 18/25 | Loss: 0.00105087
Iteration 19/25 | Loss: 0.00105087
Iteration 20/25 | Loss: 0.00105087
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010508730774745345, 0.0010508730774745345, 0.0010508730774745345, 0.0010508730774745345, 0.0010508730774745345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010508730774745345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105087
Iteration 2/1000 | Loss: 0.00004170
Iteration 3/1000 | Loss: 0.00003299
Iteration 4/1000 | Loss: 0.00002888
Iteration 5/1000 | Loss: 0.00002723
Iteration 6/1000 | Loss: 0.00002620
Iteration 7/1000 | Loss: 0.00002556
Iteration 8/1000 | Loss: 0.00002508
Iteration 9/1000 | Loss: 0.00002488
Iteration 10/1000 | Loss: 0.00002488
Iteration 11/1000 | Loss: 0.00002486
Iteration 12/1000 | Loss: 0.00002476
Iteration 13/1000 | Loss: 0.00002472
Iteration 14/1000 | Loss: 0.00002471
Iteration 15/1000 | Loss: 0.00002461
Iteration 16/1000 | Loss: 0.00002460
Iteration 17/1000 | Loss: 0.00002458
Iteration 18/1000 | Loss: 0.00002457
Iteration 19/1000 | Loss: 0.00002457
Iteration 20/1000 | Loss: 0.00002457
Iteration 21/1000 | Loss: 0.00002457
Iteration 22/1000 | Loss: 0.00002457
Iteration 23/1000 | Loss: 0.00002456
Iteration 24/1000 | Loss: 0.00002456
Iteration 25/1000 | Loss: 0.00002455
Iteration 26/1000 | Loss: 0.00002454
Iteration 27/1000 | Loss: 0.00002453
Iteration 28/1000 | Loss: 0.00002453
Iteration 29/1000 | Loss: 0.00002453
Iteration 30/1000 | Loss: 0.00002452
Iteration 31/1000 | Loss: 0.00002451
Iteration 32/1000 | Loss: 0.00002451
Iteration 33/1000 | Loss: 0.00002450
Iteration 34/1000 | Loss: 0.00002449
Iteration 35/1000 | Loss: 0.00002449
Iteration 36/1000 | Loss: 0.00002449
Iteration 37/1000 | Loss: 0.00002448
Iteration 38/1000 | Loss: 0.00002445
Iteration 39/1000 | Loss: 0.00002444
Iteration 40/1000 | Loss: 0.00002443
Iteration 41/1000 | Loss: 0.00002442
Iteration 42/1000 | Loss: 0.00002442
Iteration 43/1000 | Loss: 0.00002441
Iteration 44/1000 | Loss: 0.00002441
Iteration 45/1000 | Loss: 0.00002440
Iteration 46/1000 | Loss: 0.00002440
Iteration 47/1000 | Loss: 0.00002439
Iteration 48/1000 | Loss: 0.00002439
Iteration 49/1000 | Loss: 0.00002438
Iteration 50/1000 | Loss: 0.00002438
Iteration 51/1000 | Loss: 0.00002438
Iteration 52/1000 | Loss: 0.00002437
Iteration 53/1000 | Loss: 0.00002437
Iteration 54/1000 | Loss: 0.00002437
Iteration 55/1000 | Loss: 0.00002437
Iteration 56/1000 | Loss: 0.00002437
Iteration 57/1000 | Loss: 0.00002437
Iteration 58/1000 | Loss: 0.00002437
Iteration 59/1000 | Loss: 0.00002436
Iteration 60/1000 | Loss: 0.00002436
Iteration 61/1000 | Loss: 0.00002436
Iteration 62/1000 | Loss: 0.00002436
Iteration 63/1000 | Loss: 0.00002436
Iteration 64/1000 | Loss: 0.00002436
Iteration 65/1000 | Loss: 0.00002436
Iteration 66/1000 | Loss: 0.00002436
Iteration 67/1000 | Loss: 0.00002436
Iteration 68/1000 | Loss: 0.00002436
Iteration 69/1000 | Loss: 0.00002436
Iteration 70/1000 | Loss: 0.00002436
Iteration 71/1000 | Loss: 0.00002435
Iteration 72/1000 | Loss: 0.00002435
Iteration 73/1000 | Loss: 0.00002435
Iteration 74/1000 | Loss: 0.00002435
Iteration 75/1000 | Loss: 0.00002434
Iteration 76/1000 | Loss: 0.00002434
Iteration 77/1000 | Loss: 0.00002434
Iteration 78/1000 | Loss: 0.00002434
Iteration 79/1000 | Loss: 0.00002434
Iteration 80/1000 | Loss: 0.00002433
Iteration 81/1000 | Loss: 0.00002433
Iteration 82/1000 | Loss: 0.00002433
Iteration 83/1000 | Loss: 0.00002433
Iteration 84/1000 | Loss: 0.00002433
Iteration 85/1000 | Loss: 0.00002433
Iteration 86/1000 | Loss: 0.00002433
Iteration 87/1000 | Loss: 0.00002433
Iteration 88/1000 | Loss: 0.00002432
Iteration 89/1000 | Loss: 0.00002432
Iteration 90/1000 | Loss: 0.00002432
Iteration 91/1000 | Loss: 0.00002431
Iteration 92/1000 | Loss: 0.00002431
Iteration 93/1000 | Loss: 0.00002431
Iteration 94/1000 | Loss: 0.00002430
Iteration 95/1000 | Loss: 0.00002430
Iteration 96/1000 | Loss: 0.00002430
Iteration 97/1000 | Loss: 0.00002430
Iteration 98/1000 | Loss: 0.00002430
Iteration 99/1000 | Loss: 0.00002429
Iteration 100/1000 | Loss: 0.00002429
Iteration 101/1000 | Loss: 0.00002428
Iteration 102/1000 | Loss: 0.00002428
Iteration 103/1000 | Loss: 0.00002428
Iteration 104/1000 | Loss: 0.00002428
Iteration 105/1000 | Loss: 0.00002427
Iteration 106/1000 | Loss: 0.00002427
Iteration 107/1000 | Loss: 0.00002427
Iteration 108/1000 | Loss: 0.00002427
Iteration 109/1000 | Loss: 0.00002427
Iteration 110/1000 | Loss: 0.00002427
Iteration 111/1000 | Loss: 0.00002427
Iteration 112/1000 | Loss: 0.00002427
Iteration 113/1000 | Loss: 0.00002427
Iteration 114/1000 | Loss: 0.00002426
Iteration 115/1000 | Loss: 0.00002426
Iteration 116/1000 | Loss: 0.00002426
Iteration 117/1000 | Loss: 0.00002426
Iteration 118/1000 | Loss: 0.00002426
Iteration 119/1000 | Loss: 0.00002426
Iteration 120/1000 | Loss: 0.00002426
Iteration 121/1000 | Loss: 0.00002426
Iteration 122/1000 | Loss: 0.00002426
Iteration 123/1000 | Loss: 0.00002426
Iteration 124/1000 | Loss: 0.00002426
Iteration 125/1000 | Loss: 0.00002425
Iteration 126/1000 | Loss: 0.00002425
Iteration 127/1000 | Loss: 0.00002425
Iteration 128/1000 | Loss: 0.00002425
Iteration 129/1000 | Loss: 0.00002425
Iteration 130/1000 | Loss: 0.00002425
Iteration 131/1000 | Loss: 0.00002425
Iteration 132/1000 | Loss: 0.00002425
Iteration 133/1000 | Loss: 0.00002425
Iteration 134/1000 | Loss: 0.00002425
Iteration 135/1000 | Loss: 0.00002425
Iteration 136/1000 | Loss: 0.00002425
Iteration 137/1000 | Loss: 0.00002425
Iteration 138/1000 | Loss: 0.00002425
Iteration 139/1000 | Loss: 0.00002424
Iteration 140/1000 | Loss: 0.00002424
Iteration 141/1000 | Loss: 0.00002424
Iteration 142/1000 | Loss: 0.00002424
Iteration 143/1000 | Loss: 0.00002424
Iteration 144/1000 | Loss: 0.00002424
Iteration 145/1000 | Loss: 0.00002424
Iteration 146/1000 | Loss: 0.00002424
Iteration 147/1000 | Loss: 0.00002424
Iteration 148/1000 | Loss: 0.00002424
Iteration 149/1000 | Loss: 0.00002424
Iteration 150/1000 | Loss: 0.00002424
Iteration 151/1000 | Loss: 0.00002424
Iteration 152/1000 | Loss: 0.00002424
Iteration 153/1000 | Loss: 0.00002424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.4237333491328172e-05, 2.4237333491328172e-05, 2.4237333491328172e-05, 2.4237333491328172e-05, 2.4237333491328172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4237333491328172e-05

Optimization complete. Final v2v error: 4.160159111022949 mm

Highest mean error: 4.398718357086182 mm for frame 82

Lowest mean error: 4.016110420227051 mm for frame 0

Saving results

Total time: 36.266815185546875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056780
Iteration 2/25 | Loss: 0.00146580
Iteration 3/25 | Loss: 0.00122556
Iteration 4/25 | Loss: 0.00120051
Iteration 5/25 | Loss: 0.00119076
Iteration 6/25 | Loss: 0.00118924
Iteration 7/25 | Loss: 0.00118924
Iteration 8/25 | Loss: 0.00118924
Iteration 9/25 | Loss: 0.00118924
Iteration 10/25 | Loss: 0.00118924
Iteration 11/25 | Loss: 0.00118924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011892414186149836, 0.0011892414186149836, 0.0011892414186149836, 0.0011892414186149836, 0.0011892414186149836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011892414186149836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.52751327
Iteration 2/25 | Loss: 0.00115924
Iteration 3/25 | Loss: 0.00115923
Iteration 4/25 | Loss: 0.00115923
Iteration 5/25 | Loss: 0.00115923
Iteration 6/25 | Loss: 0.00115923
Iteration 7/25 | Loss: 0.00115923
Iteration 8/25 | Loss: 0.00115923
Iteration 9/25 | Loss: 0.00115923
Iteration 10/25 | Loss: 0.00115923
Iteration 11/25 | Loss: 0.00115923
Iteration 12/25 | Loss: 0.00115923
Iteration 13/25 | Loss: 0.00115923
Iteration 14/25 | Loss: 0.00115923
Iteration 15/25 | Loss: 0.00115923
Iteration 16/25 | Loss: 0.00115923
Iteration 17/25 | Loss: 0.00115923
Iteration 18/25 | Loss: 0.00115923
Iteration 19/25 | Loss: 0.00115923
Iteration 20/25 | Loss: 0.00115923
Iteration 21/25 | Loss: 0.00115923
Iteration 22/25 | Loss: 0.00115923
Iteration 23/25 | Loss: 0.00115923
Iteration 24/25 | Loss: 0.00115923
Iteration 25/25 | Loss: 0.00115923
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011592276860028505, 0.0011592276860028505, 0.0011592276860028505, 0.0011592276860028505, 0.0011592276860028505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011592276860028505

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115923
Iteration 2/1000 | Loss: 0.00004795
Iteration 3/1000 | Loss: 0.00003663
Iteration 4/1000 | Loss: 0.00003381
Iteration 5/1000 | Loss: 0.00003223
Iteration 6/1000 | Loss: 0.00003130
Iteration 7/1000 | Loss: 0.00003046
Iteration 8/1000 | Loss: 0.00002995
Iteration 9/1000 | Loss: 0.00002972
Iteration 10/1000 | Loss: 0.00002967
Iteration 11/1000 | Loss: 0.00002961
Iteration 12/1000 | Loss: 0.00002949
Iteration 13/1000 | Loss: 0.00002940
Iteration 14/1000 | Loss: 0.00002938
Iteration 15/1000 | Loss: 0.00002934
Iteration 16/1000 | Loss: 0.00002933
Iteration 17/1000 | Loss: 0.00002927
Iteration 18/1000 | Loss: 0.00002926
Iteration 19/1000 | Loss: 0.00002926
Iteration 20/1000 | Loss: 0.00002926
Iteration 21/1000 | Loss: 0.00002926
Iteration 22/1000 | Loss: 0.00002926
Iteration 23/1000 | Loss: 0.00002926
Iteration 24/1000 | Loss: 0.00002926
Iteration 25/1000 | Loss: 0.00002926
Iteration 26/1000 | Loss: 0.00002926
Iteration 27/1000 | Loss: 0.00002925
Iteration 28/1000 | Loss: 0.00002924
Iteration 29/1000 | Loss: 0.00002922
Iteration 30/1000 | Loss: 0.00002922
Iteration 31/1000 | Loss: 0.00002922
Iteration 32/1000 | Loss: 0.00002922
Iteration 33/1000 | Loss: 0.00002921
Iteration 34/1000 | Loss: 0.00002920
Iteration 35/1000 | Loss: 0.00002919
Iteration 36/1000 | Loss: 0.00002918
Iteration 37/1000 | Loss: 0.00002917
Iteration 38/1000 | Loss: 0.00002915
Iteration 39/1000 | Loss: 0.00002913
Iteration 40/1000 | Loss: 0.00002913
Iteration 41/1000 | Loss: 0.00002912
Iteration 42/1000 | Loss: 0.00002912
Iteration 43/1000 | Loss: 0.00002912
Iteration 44/1000 | Loss: 0.00002911
Iteration 45/1000 | Loss: 0.00002911
Iteration 46/1000 | Loss: 0.00002910
Iteration 47/1000 | Loss: 0.00002910
Iteration 48/1000 | Loss: 0.00002909
Iteration 49/1000 | Loss: 0.00002909
Iteration 50/1000 | Loss: 0.00002909
Iteration 51/1000 | Loss: 0.00002909
Iteration 52/1000 | Loss: 0.00002908
Iteration 53/1000 | Loss: 0.00002908
Iteration 54/1000 | Loss: 0.00002908
Iteration 55/1000 | Loss: 0.00002908
Iteration 56/1000 | Loss: 0.00002908
Iteration 57/1000 | Loss: 0.00002907
Iteration 58/1000 | Loss: 0.00002907
Iteration 59/1000 | Loss: 0.00002907
Iteration 60/1000 | Loss: 0.00002906
Iteration 61/1000 | Loss: 0.00002906
Iteration 62/1000 | Loss: 0.00002905
Iteration 63/1000 | Loss: 0.00002905
Iteration 64/1000 | Loss: 0.00002905
Iteration 65/1000 | Loss: 0.00002905
Iteration 66/1000 | Loss: 0.00002904
Iteration 67/1000 | Loss: 0.00002904
Iteration 68/1000 | Loss: 0.00002903
Iteration 69/1000 | Loss: 0.00002903
Iteration 70/1000 | Loss: 0.00002902
Iteration 71/1000 | Loss: 0.00002902
Iteration 72/1000 | Loss: 0.00002902
Iteration 73/1000 | Loss: 0.00002902
Iteration 74/1000 | Loss: 0.00002901
Iteration 75/1000 | Loss: 0.00002901
Iteration 76/1000 | Loss: 0.00002901
Iteration 77/1000 | Loss: 0.00002900
Iteration 78/1000 | Loss: 0.00002900
Iteration 79/1000 | Loss: 0.00002900
Iteration 80/1000 | Loss: 0.00002900
Iteration 81/1000 | Loss: 0.00002900
Iteration 82/1000 | Loss: 0.00002900
Iteration 83/1000 | Loss: 0.00002900
Iteration 84/1000 | Loss: 0.00002900
Iteration 85/1000 | Loss: 0.00002900
Iteration 86/1000 | Loss: 0.00002900
Iteration 87/1000 | Loss: 0.00002900
Iteration 88/1000 | Loss: 0.00002900
Iteration 89/1000 | Loss: 0.00002900
Iteration 90/1000 | Loss: 0.00002899
Iteration 91/1000 | Loss: 0.00002899
Iteration 92/1000 | Loss: 0.00002899
Iteration 93/1000 | Loss: 0.00002899
Iteration 94/1000 | Loss: 0.00002899
Iteration 95/1000 | Loss: 0.00002899
Iteration 96/1000 | Loss: 0.00002899
Iteration 97/1000 | Loss: 0.00002898
Iteration 98/1000 | Loss: 0.00002898
Iteration 99/1000 | Loss: 0.00002898
Iteration 100/1000 | Loss: 0.00002897
Iteration 101/1000 | Loss: 0.00002897
Iteration 102/1000 | Loss: 0.00002897
Iteration 103/1000 | Loss: 0.00002897
Iteration 104/1000 | Loss: 0.00002896
Iteration 105/1000 | Loss: 0.00002896
Iteration 106/1000 | Loss: 0.00002896
Iteration 107/1000 | Loss: 0.00002896
Iteration 108/1000 | Loss: 0.00002896
Iteration 109/1000 | Loss: 0.00002896
Iteration 110/1000 | Loss: 0.00002896
Iteration 111/1000 | Loss: 0.00002895
Iteration 112/1000 | Loss: 0.00002895
Iteration 113/1000 | Loss: 0.00002895
Iteration 114/1000 | Loss: 0.00002895
Iteration 115/1000 | Loss: 0.00002894
Iteration 116/1000 | Loss: 0.00002894
Iteration 117/1000 | Loss: 0.00002894
Iteration 118/1000 | Loss: 0.00002894
Iteration 119/1000 | Loss: 0.00002894
Iteration 120/1000 | Loss: 0.00002894
Iteration 121/1000 | Loss: 0.00002894
Iteration 122/1000 | Loss: 0.00002894
Iteration 123/1000 | Loss: 0.00002894
Iteration 124/1000 | Loss: 0.00002894
Iteration 125/1000 | Loss: 0.00002893
Iteration 126/1000 | Loss: 0.00002893
Iteration 127/1000 | Loss: 0.00002893
Iteration 128/1000 | Loss: 0.00002893
Iteration 129/1000 | Loss: 0.00002893
Iteration 130/1000 | Loss: 0.00002893
Iteration 131/1000 | Loss: 0.00002893
Iteration 132/1000 | Loss: 0.00002893
Iteration 133/1000 | Loss: 0.00002893
Iteration 134/1000 | Loss: 0.00002893
Iteration 135/1000 | Loss: 0.00002893
Iteration 136/1000 | Loss: 0.00002893
Iteration 137/1000 | Loss: 0.00002893
Iteration 138/1000 | Loss: 0.00002892
Iteration 139/1000 | Loss: 0.00002892
Iteration 140/1000 | Loss: 0.00002892
Iteration 141/1000 | Loss: 0.00002892
Iteration 142/1000 | Loss: 0.00002892
Iteration 143/1000 | Loss: 0.00002892
Iteration 144/1000 | Loss: 0.00002892
Iteration 145/1000 | Loss: 0.00002892
Iteration 146/1000 | Loss: 0.00002892
Iteration 147/1000 | Loss: 0.00002892
Iteration 148/1000 | Loss: 0.00002892
Iteration 149/1000 | Loss: 0.00002892
Iteration 150/1000 | Loss: 0.00002892
Iteration 151/1000 | Loss: 0.00002892
Iteration 152/1000 | Loss: 0.00002892
Iteration 153/1000 | Loss: 0.00002892
Iteration 154/1000 | Loss: 0.00002892
Iteration 155/1000 | Loss: 0.00002892
Iteration 156/1000 | Loss: 0.00002892
Iteration 157/1000 | Loss: 0.00002892
Iteration 158/1000 | Loss: 0.00002892
Iteration 159/1000 | Loss: 0.00002891
Iteration 160/1000 | Loss: 0.00002891
Iteration 161/1000 | Loss: 0.00002891
Iteration 162/1000 | Loss: 0.00002891
Iteration 163/1000 | Loss: 0.00002891
Iteration 164/1000 | Loss: 0.00002891
Iteration 165/1000 | Loss: 0.00002891
Iteration 166/1000 | Loss: 0.00002891
Iteration 167/1000 | Loss: 0.00002891
Iteration 168/1000 | Loss: 0.00002891
Iteration 169/1000 | Loss: 0.00002891
Iteration 170/1000 | Loss: 0.00002891
Iteration 171/1000 | Loss: 0.00002891
Iteration 172/1000 | Loss: 0.00002891
Iteration 173/1000 | Loss: 0.00002891
Iteration 174/1000 | Loss: 0.00002891
Iteration 175/1000 | Loss: 0.00002891
Iteration 176/1000 | Loss: 0.00002891
Iteration 177/1000 | Loss: 0.00002891
Iteration 178/1000 | Loss: 0.00002891
Iteration 179/1000 | Loss: 0.00002891
Iteration 180/1000 | Loss: 0.00002891
Iteration 181/1000 | Loss: 0.00002890
Iteration 182/1000 | Loss: 0.00002890
Iteration 183/1000 | Loss: 0.00002890
Iteration 184/1000 | Loss: 0.00002890
Iteration 185/1000 | Loss: 0.00002890
Iteration 186/1000 | Loss: 0.00002890
Iteration 187/1000 | Loss: 0.00002890
Iteration 188/1000 | Loss: 0.00002890
Iteration 189/1000 | Loss: 0.00002890
Iteration 190/1000 | Loss: 0.00002890
Iteration 191/1000 | Loss: 0.00002890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.8902795747853816e-05, 2.8902795747853816e-05, 2.8902795747853816e-05, 2.8902795747853816e-05, 2.8902795747853816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8902795747853816e-05

Optimization complete. Final v2v error: 4.685641288757324 mm

Highest mean error: 5.17203950881958 mm for frame 87

Lowest mean error: 4.441474437713623 mm for frame 118

Saving results

Total time: 36.926023960113525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038325
Iteration 2/25 | Loss: 0.00382426
Iteration 3/25 | Loss: 0.00247764
Iteration 4/25 | Loss: 0.00228152
Iteration 5/25 | Loss: 0.00227441
Iteration 6/25 | Loss: 0.00202256
Iteration 7/25 | Loss: 0.00199905
Iteration 8/25 | Loss: 0.00192569
Iteration 9/25 | Loss: 0.00186052
Iteration 10/25 | Loss: 0.00182967
Iteration 11/25 | Loss: 0.00180645
Iteration 12/25 | Loss: 0.00177908
Iteration 13/25 | Loss: 0.00177358
Iteration 14/25 | Loss: 0.00177312
Iteration 15/25 | Loss: 0.00176853
Iteration 16/25 | Loss: 0.00176919
Iteration 17/25 | Loss: 0.00176686
Iteration 18/25 | Loss: 0.00176229
Iteration 19/25 | Loss: 0.00176851
Iteration 20/25 | Loss: 0.00176517
Iteration 21/25 | Loss: 0.00176041
Iteration 22/25 | Loss: 0.00177164
Iteration 23/25 | Loss: 0.00175776
Iteration 24/25 | Loss: 0.00175674
Iteration 25/25 | Loss: 0.00174986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18894327
Iteration 2/25 | Loss: 0.00894076
Iteration 3/25 | Loss: 0.00703608
Iteration 4/25 | Loss: 0.00703607
Iteration 5/25 | Loss: 0.00703606
Iteration 6/25 | Loss: 0.00703606
Iteration 7/25 | Loss: 0.00703606
Iteration 8/25 | Loss: 0.00703606
Iteration 9/25 | Loss: 0.00703606
Iteration 10/25 | Loss: 0.00703606
Iteration 11/25 | Loss: 0.00703606
Iteration 12/25 | Loss: 0.00703606
Iteration 13/25 | Loss: 0.00703606
Iteration 14/25 | Loss: 0.00703606
Iteration 15/25 | Loss: 0.00703606
Iteration 16/25 | Loss: 0.00703606
Iteration 17/25 | Loss: 0.00703606
Iteration 18/25 | Loss: 0.00703606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.007036061957478523, 0.007036061957478523, 0.007036061957478523, 0.007036061957478523, 0.007036061957478523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007036061957478523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00703606
Iteration 2/1000 | Loss: 0.00221340
Iteration 3/1000 | Loss: 0.00424369
Iteration 4/1000 | Loss: 0.00269455
Iteration 5/1000 | Loss: 0.00326580
Iteration 6/1000 | Loss: 0.02531940
Iteration 7/1000 | Loss: 0.01461957
Iteration 8/1000 | Loss: 0.00100385
Iteration 9/1000 | Loss: 0.00114942
Iteration 10/1000 | Loss: 0.00063346
Iteration 11/1000 | Loss: 0.00092659
Iteration 12/1000 | Loss: 0.00094114
Iteration 13/1000 | Loss: 0.00698395
Iteration 14/1000 | Loss: 0.00196464
Iteration 15/1000 | Loss: 0.00054183
Iteration 16/1000 | Loss: 0.00071615
Iteration 17/1000 | Loss: 0.00078724
Iteration 18/1000 | Loss: 0.01079075
Iteration 19/1000 | Loss: 0.02055997
Iteration 20/1000 | Loss: 0.01205492
Iteration 21/1000 | Loss: 0.00150954
Iteration 22/1000 | Loss: 0.00304879
Iteration 23/1000 | Loss: 0.00060928
Iteration 24/1000 | Loss: 0.00152666
Iteration 25/1000 | Loss: 0.00407363
Iteration 26/1000 | Loss: 0.00312621
Iteration 27/1000 | Loss: 0.00227809
Iteration 28/1000 | Loss: 0.00393898
Iteration 29/1000 | Loss: 0.00038623
Iteration 30/1000 | Loss: 0.00049490
Iteration 31/1000 | Loss: 0.00089839
Iteration 32/1000 | Loss: 0.00011093
Iteration 33/1000 | Loss: 0.00008871
Iteration 34/1000 | Loss: 0.00030796
Iteration 35/1000 | Loss: 0.00006322
Iteration 36/1000 | Loss: 0.00046978
Iteration 37/1000 | Loss: 0.00058539
Iteration 38/1000 | Loss: 0.00012679
Iteration 39/1000 | Loss: 0.00108031
Iteration 40/1000 | Loss: 0.00316335
Iteration 41/1000 | Loss: 0.00221334
Iteration 42/1000 | Loss: 0.00238827
Iteration 43/1000 | Loss: 0.00334547
Iteration 44/1000 | Loss: 0.00112073
Iteration 45/1000 | Loss: 0.00200137
Iteration 46/1000 | Loss: 0.00173480
Iteration 47/1000 | Loss: 0.00049673
Iteration 48/1000 | Loss: 0.00045258
Iteration 49/1000 | Loss: 0.00022634
Iteration 50/1000 | Loss: 0.00014282
Iteration 51/1000 | Loss: 0.00015242
Iteration 52/1000 | Loss: 0.00008607
Iteration 53/1000 | Loss: 0.00008498
Iteration 54/1000 | Loss: 0.00047793
Iteration 55/1000 | Loss: 0.00004872
Iteration 56/1000 | Loss: 0.00005196
Iteration 57/1000 | Loss: 0.00008526
Iteration 58/1000 | Loss: 0.00003673
Iteration 59/1000 | Loss: 0.00033512
Iteration 60/1000 | Loss: 0.00080656
Iteration 61/1000 | Loss: 0.00007994
Iteration 62/1000 | Loss: 0.00003806
Iteration 63/1000 | Loss: 0.00013553
Iteration 64/1000 | Loss: 0.00005156
Iteration 65/1000 | Loss: 0.00010998
Iteration 66/1000 | Loss: 0.00004821
Iteration 67/1000 | Loss: 0.00003599
Iteration 68/1000 | Loss: 0.00003530
Iteration 69/1000 | Loss: 0.00007954
Iteration 70/1000 | Loss: 0.00003539
Iteration 71/1000 | Loss: 0.00003522
Iteration 72/1000 | Loss: 0.00003499
Iteration 73/1000 | Loss: 0.00003496
Iteration 74/1000 | Loss: 0.00003492
Iteration 75/1000 | Loss: 0.00003599
Iteration 76/1000 | Loss: 0.00003522
Iteration 77/1000 | Loss: 0.00003582
Iteration 78/1000 | Loss: 0.00003512
Iteration 79/1000 | Loss: 0.00003585
Iteration 80/1000 | Loss: 0.00003514
Iteration 81/1000 | Loss: 0.00003565
Iteration 82/1000 | Loss: 0.00003502
Iteration 83/1000 | Loss: 0.00003478
Iteration 84/1000 | Loss: 0.00003565
Iteration 85/1000 | Loss: 0.00003497
Iteration 86/1000 | Loss: 0.00003475
Iteration 87/1000 | Loss: 0.00009702
Iteration 88/1000 | Loss: 0.00004887
Iteration 89/1000 | Loss: 0.00003565
Iteration 90/1000 | Loss: 0.00006998
Iteration 91/1000 | Loss: 0.00003558
Iteration 92/1000 | Loss: 0.00003524
Iteration 93/1000 | Loss: 0.00006416
Iteration 94/1000 | Loss: 0.00004856
Iteration 95/1000 | Loss: 0.00003536
Iteration 96/1000 | Loss: 0.00003471
Iteration 97/1000 | Loss: 0.00003471
Iteration 98/1000 | Loss: 0.00003470
Iteration 99/1000 | Loss: 0.00003574
Iteration 100/1000 | Loss: 0.00003522
Iteration 101/1000 | Loss: 0.00003468
Iteration 102/1000 | Loss: 0.00003561
Iteration 103/1000 | Loss: 0.00003524
Iteration 104/1000 | Loss: 0.00003523
Iteration 105/1000 | Loss: 0.00003468
Iteration 106/1000 | Loss: 0.00003468
Iteration 107/1000 | Loss: 0.00003468
Iteration 108/1000 | Loss: 0.00003468
Iteration 109/1000 | Loss: 0.00003468
Iteration 110/1000 | Loss: 0.00003468
Iteration 111/1000 | Loss: 0.00003467
Iteration 112/1000 | Loss: 0.00003467
Iteration 113/1000 | Loss: 0.00003467
Iteration 114/1000 | Loss: 0.00003467
Iteration 115/1000 | Loss: 0.00003467
Iteration 116/1000 | Loss: 0.00003467
Iteration 117/1000 | Loss: 0.00003467
Iteration 118/1000 | Loss: 0.00003467
Iteration 119/1000 | Loss: 0.00003466
Iteration 120/1000 | Loss: 0.00003612
Iteration 121/1000 | Loss: 0.00003547
Iteration 122/1000 | Loss: 0.00003546
Iteration 123/1000 | Loss: 0.00003481
Iteration 124/1000 | Loss: 0.00003469
Iteration 125/1000 | Loss: 0.00003467
Iteration 126/1000 | Loss: 0.00003467
Iteration 127/1000 | Loss: 0.00003467
Iteration 128/1000 | Loss: 0.00003466
Iteration 129/1000 | Loss: 0.00003466
Iteration 130/1000 | Loss: 0.00003466
Iteration 131/1000 | Loss: 0.00003466
Iteration 132/1000 | Loss: 0.00003466
Iteration 133/1000 | Loss: 0.00003466
Iteration 134/1000 | Loss: 0.00003466
Iteration 135/1000 | Loss: 0.00003465
Iteration 136/1000 | Loss: 0.00003465
Iteration 137/1000 | Loss: 0.00003465
Iteration 138/1000 | Loss: 0.00003465
Iteration 139/1000 | Loss: 0.00003465
Iteration 140/1000 | Loss: 0.00003464
Iteration 141/1000 | Loss: 0.00003464
Iteration 142/1000 | Loss: 0.00003464
Iteration 143/1000 | Loss: 0.00003464
Iteration 144/1000 | Loss: 0.00003464
Iteration 145/1000 | Loss: 0.00003464
Iteration 146/1000 | Loss: 0.00003464
Iteration 147/1000 | Loss: 0.00003464
Iteration 148/1000 | Loss: 0.00003464
Iteration 149/1000 | Loss: 0.00003464
Iteration 150/1000 | Loss: 0.00003464
Iteration 151/1000 | Loss: 0.00003464
Iteration 152/1000 | Loss: 0.00003464
Iteration 153/1000 | Loss: 0.00003464
Iteration 154/1000 | Loss: 0.00003464
Iteration 155/1000 | Loss: 0.00003464
Iteration 156/1000 | Loss: 0.00003464
Iteration 157/1000 | Loss: 0.00003464
Iteration 158/1000 | Loss: 0.00003464
Iteration 159/1000 | Loss: 0.00003464
Iteration 160/1000 | Loss: 0.00003464
Iteration 161/1000 | Loss: 0.00003464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [3.463520988589153e-05, 3.463520988589153e-05, 3.463520988589153e-05, 3.463520988589153e-05, 3.463520988589153e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.463520988589153e-05

Optimization complete. Final v2v error: 4.931258678436279 mm

Highest mean error: 10.907343864440918 mm for frame 38

Lowest mean error: 4.535228729248047 mm for frame 46

Saving results

Total time: 179.81079196929932
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904464
Iteration 2/25 | Loss: 0.00146055
Iteration 3/25 | Loss: 0.00120008
Iteration 4/25 | Loss: 0.00115864
Iteration 5/25 | Loss: 0.00114907
Iteration 6/25 | Loss: 0.00114629
Iteration 7/25 | Loss: 0.00114567
Iteration 8/25 | Loss: 0.00114555
Iteration 9/25 | Loss: 0.00114555
Iteration 10/25 | Loss: 0.00114555
Iteration 11/25 | Loss: 0.00114555
Iteration 12/25 | Loss: 0.00114555
Iteration 13/25 | Loss: 0.00114555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011455521453171968, 0.0011455521453171968, 0.0011455521453171968, 0.0011455521453171968, 0.0011455521453171968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011455521453171968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.50507855
Iteration 2/25 | Loss: 0.00098908
Iteration 3/25 | Loss: 0.00098907
Iteration 4/25 | Loss: 0.00098907
Iteration 5/25 | Loss: 0.00098907
Iteration 6/25 | Loss: 0.00098907
Iteration 7/25 | Loss: 0.00098907
Iteration 8/25 | Loss: 0.00098907
Iteration 9/25 | Loss: 0.00098907
Iteration 10/25 | Loss: 0.00098907
Iteration 11/25 | Loss: 0.00098907
Iteration 12/25 | Loss: 0.00098907
Iteration 13/25 | Loss: 0.00098907
Iteration 14/25 | Loss: 0.00098907
Iteration 15/25 | Loss: 0.00098907
Iteration 16/25 | Loss: 0.00098907
Iteration 17/25 | Loss: 0.00098907
Iteration 18/25 | Loss: 0.00098907
Iteration 19/25 | Loss: 0.00098907
Iteration 20/25 | Loss: 0.00098907
Iteration 21/25 | Loss: 0.00098907
Iteration 22/25 | Loss: 0.00098907
Iteration 23/25 | Loss: 0.00098907
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009890677174553275, 0.0009890677174553275, 0.0009890677174553275, 0.0009890677174553275, 0.0009890677174553275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009890677174553275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098907
Iteration 2/1000 | Loss: 0.00003967
Iteration 3/1000 | Loss: 0.00003096
Iteration 4/1000 | Loss: 0.00002756
Iteration 5/1000 | Loss: 0.00002627
Iteration 6/1000 | Loss: 0.00002549
Iteration 7/1000 | Loss: 0.00002492
Iteration 8/1000 | Loss: 0.00002460
Iteration 9/1000 | Loss: 0.00002448
Iteration 10/1000 | Loss: 0.00002447
Iteration 11/1000 | Loss: 0.00002446
Iteration 12/1000 | Loss: 0.00002433
Iteration 13/1000 | Loss: 0.00002432
Iteration 14/1000 | Loss: 0.00002432
Iteration 15/1000 | Loss: 0.00002431
Iteration 16/1000 | Loss: 0.00002431
Iteration 17/1000 | Loss: 0.00002431
Iteration 18/1000 | Loss: 0.00002427
Iteration 19/1000 | Loss: 0.00002427
Iteration 20/1000 | Loss: 0.00002427
Iteration 21/1000 | Loss: 0.00002426
Iteration 22/1000 | Loss: 0.00002425
Iteration 23/1000 | Loss: 0.00002424
Iteration 24/1000 | Loss: 0.00002423
Iteration 25/1000 | Loss: 0.00002423
Iteration 26/1000 | Loss: 0.00002423
Iteration 27/1000 | Loss: 0.00002422
Iteration 28/1000 | Loss: 0.00002422
Iteration 29/1000 | Loss: 0.00002421
Iteration 30/1000 | Loss: 0.00002420
Iteration 31/1000 | Loss: 0.00002419
Iteration 32/1000 | Loss: 0.00002418
Iteration 33/1000 | Loss: 0.00002418
Iteration 34/1000 | Loss: 0.00002412
Iteration 35/1000 | Loss: 0.00002412
Iteration 36/1000 | Loss: 0.00002412
Iteration 37/1000 | Loss: 0.00002412
Iteration 38/1000 | Loss: 0.00002412
Iteration 39/1000 | Loss: 0.00002411
Iteration 40/1000 | Loss: 0.00002411
Iteration 41/1000 | Loss: 0.00002411
Iteration 42/1000 | Loss: 0.00002411
Iteration 43/1000 | Loss: 0.00002410
Iteration 44/1000 | Loss: 0.00002410
Iteration 45/1000 | Loss: 0.00002408
Iteration 46/1000 | Loss: 0.00002408
Iteration 47/1000 | Loss: 0.00002408
Iteration 48/1000 | Loss: 0.00002407
Iteration 49/1000 | Loss: 0.00002407
Iteration 50/1000 | Loss: 0.00002405
Iteration 51/1000 | Loss: 0.00002405
Iteration 52/1000 | Loss: 0.00002405
Iteration 53/1000 | Loss: 0.00002405
Iteration 54/1000 | Loss: 0.00002405
Iteration 55/1000 | Loss: 0.00002404
Iteration 56/1000 | Loss: 0.00002404
Iteration 57/1000 | Loss: 0.00002404
Iteration 58/1000 | Loss: 0.00002404
Iteration 59/1000 | Loss: 0.00002403
Iteration 60/1000 | Loss: 0.00002403
Iteration 61/1000 | Loss: 0.00002403
Iteration 62/1000 | Loss: 0.00002402
Iteration 63/1000 | Loss: 0.00002402
Iteration 64/1000 | Loss: 0.00002402
Iteration 65/1000 | Loss: 0.00002402
Iteration 66/1000 | Loss: 0.00002402
Iteration 67/1000 | Loss: 0.00002402
Iteration 68/1000 | Loss: 0.00002401
Iteration 69/1000 | Loss: 0.00002401
Iteration 70/1000 | Loss: 0.00002401
Iteration 71/1000 | Loss: 0.00002400
Iteration 72/1000 | Loss: 0.00002400
Iteration 73/1000 | Loss: 0.00002400
Iteration 74/1000 | Loss: 0.00002399
Iteration 75/1000 | Loss: 0.00002399
Iteration 76/1000 | Loss: 0.00002399
Iteration 77/1000 | Loss: 0.00002399
Iteration 78/1000 | Loss: 0.00002399
Iteration 79/1000 | Loss: 0.00002399
Iteration 80/1000 | Loss: 0.00002399
Iteration 81/1000 | Loss: 0.00002399
Iteration 82/1000 | Loss: 0.00002399
Iteration 83/1000 | Loss: 0.00002399
Iteration 84/1000 | Loss: 0.00002399
Iteration 85/1000 | Loss: 0.00002398
Iteration 86/1000 | Loss: 0.00002398
Iteration 87/1000 | Loss: 0.00002398
Iteration 88/1000 | Loss: 0.00002398
Iteration 89/1000 | Loss: 0.00002398
Iteration 90/1000 | Loss: 0.00002397
Iteration 91/1000 | Loss: 0.00002397
Iteration 92/1000 | Loss: 0.00002397
Iteration 93/1000 | Loss: 0.00002397
Iteration 94/1000 | Loss: 0.00002397
Iteration 95/1000 | Loss: 0.00002397
Iteration 96/1000 | Loss: 0.00002397
Iteration 97/1000 | Loss: 0.00002397
Iteration 98/1000 | Loss: 0.00002397
Iteration 99/1000 | Loss: 0.00002397
Iteration 100/1000 | Loss: 0.00002396
Iteration 101/1000 | Loss: 0.00002396
Iteration 102/1000 | Loss: 0.00002396
Iteration 103/1000 | Loss: 0.00002396
Iteration 104/1000 | Loss: 0.00002396
Iteration 105/1000 | Loss: 0.00002396
Iteration 106/1000 | Loss: 0.00002396
Iteration 107/1000 | Loss: 0.00002396
Iteration 108/1000 | Loss: 0.00002396
Iteration 109/1000 | Loss: 0.00002395
Iteration 110/1000 | Loss: 0.00002395
Iteration 111/1000 | Loss: 0.00002395
Iteration 112/1000 | Loss: 0.00002395
Iteration 113/1000 | Loss: 0.00002395
Iteration 114/1000 | Loss: 0.00002395
Iteration 115/1000 | Loss: 0.00002395
Iteration 116/1000 | Loss: 0.00002395
Iteration 117/1000 | Loss: 0.00002395
Iteration 118/1000 | Loss: 0.00002395
Iteration 119/1000 | Loss: 0.00002395
Iteration 120/1000 | Loss: 0.00002395
Iteration 121/1000 | Loss: 0.00002395
Iteration 122/1000 | Loss: 0.00002395
Iteration 123/1000 | Loss: 0.00002395
Iteration 124/1000 | Loss: 0.00002395
Iteration 125/1000 | Loss: 0.00002395
Iteration 126/1000 | Loss: 0.00002394
Iteration 127/1000 | Loss: 0.00002394
Iteration 128/1000 | Loss: 0.00002394
Iteration 129/1000 | Loss: 0.00002394
Iteration 130/1000 | Loss: 0.00002394
Iteration 131/1000 | Loss: 0.00002394
Iteration 132/1000 | Loss: 0.00002394
Iteration 133/1000 | Loss: 0.00002394
Iteration 134/1000 | Loss: 0.00002394
Iteration 135/1000 | Loss: 0.00002394
Iteration 136/1000 | Loss: 0.00002394
Iteration 137/1000 | Loss: 0.00002394
Iteration 138/1000 | Loss: 0.00002394
Iteration 139/1000 | Loss: 0.00002394
Iteration 140/1000 | Loss: 0.00002394
Iteration 141/1000 | Loss: 0.00002394
Iteration 142/1000 | Loss: 0.00002394
Iteration 143/1000 | Loss: 0.00002394
Iteration 144/1000 | Loss: 0.00002394
Iteration 145/1000 | Loss: 0.00002394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.3939235688885674e-05, 2.3939235688885674e-05, 2.3939235688885674e-05, 2.3939235688885674e-05, 2.3939235688885674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3939235688885674e-05

Optimization complete. Final v2v error: 4.143153190612793 mm

Highest mean error: 4.498226642608643 mm for frame 239

Lowest mean error: 3.872899293899536 mm for frame 152

Saving results

Total time: 40.6831214427948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01116257
Iteration 2/25 | Loss: 0.00307444
Iteration 3/25 | Loss: 0.00191993
Iteration 4/25 | Loss: 0.00174107
Iteration 5/25 | Loss: 0.00162939
Iteration 6/25 | Loss: 0.00152552
Iteration 7/25 | Loss: 0.00149290
Iteration 8/25 | Loss: 0.00144003
Iteration 9/25 | Loss: 0.00141362
Iteration 10/25 | Loss: 0.00136661
Iteration 11/25 | Loss: 0.00139096
Iteration 12/25 | Loss: 0.00131782
Iteration 13/25 | Loss: 0.00130199
Iteration 14/25 | Loss: 0.00128588
Iteration 15/25 | Loss: 0.00127420
Iteration 16/25 | Loss: 0.00127735
Iteration 17/25 | Loss: 0.00128667
Iteration 18/25 | Loss: 0.00127110
Iteration 19/25 | Loss: 0.00125800
Iteration 20/25 | Loss: 0.00125356
Iteration 21/25 | Loss: 0.00126117
Iteration 22/25 | Loss: 0.00125621
Iteration 23/25 | Loss: 0.00126564
Iteration 24/25 | Loss: 0.00126489
Iteration 25/25 | Loss: 0.00124910

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26761508
Iteration 2/25 | Loss: 0.00372908
Iteration 3/25 | Loss: 0.00234293
Iteration 4/25 | Loss: 0.00234293
Iteration 5/25 | Loss: 0.00234293
Iteration 6/25 | Loss: 0.00234293
Iteration 7/25 | Loss: 0.00234293
Iteration 8/25 | Loss: 0.00234293
Iteration 9/25 | Loss: 0.00234293
Iteration 10/25 | Loss: 0.00234293
Iteration 11/25 | Loss: 0.00234293
Iteration 12/25 | Loss: 0.00234293
Iteration 13/25 | Loss: 0.00234293
Iteration 14/25 | Loss: 0.00234293
Iteration 15/25 | Loss: 0.00234293
Iteration 16/25 | Loss: 0.00234293
Iteration 17/25 | Loss: 0.00234293
Iteration 18/25 | Loss: 0.00234293
Iteration 19/25 | Loss: 0.00234293
Iteration 20/25 | Loss: 0.00234293
Iteration 21/25 | Loss: 0.00234293
Iteration 22/25 | Loss: 0.00234293
Iteration 23/25 | Loss: 0.00234293
Iteration 24/25 | Loss: 0.00234293
Iteration 25/25 | Loss: 0.00234293
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0023429279681295156, 0.0023429279681295156, 0.0023429279681295156, 0.0023429279681295156, 0.0023429279681295156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023429279681295156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234293
Iteration 2/1000 | Loss: 0.00072464
Iteration 3/1000 | Loss: 0.00112960
Iteration 4/1000 | Loss: 0.00066777
Iteration 5/1000 | Loss: 0.00044761
Iteration 6/1000 | Loss: 0.00099498
Iteration 7/1000 | Loss: 0.00063878
Iteration 8/1000 | Loss: 0.00122950
Iteration 9/1000 | Loss: 0.00480946
Iteration 10/1000 | Loss: 0.00189858
Iteration 11/1000 | Loss: 0.00200760
Iteration 12/1000 | Loss: 0.00419877
Iteration 13/1000 | Loss: 0.00401051
Iteration 14/1000 | Loss: 0.01522496
Iteration 15/1000 | Loss: 0.00620465
Iteration 16/1000 | Loss: 0.00379443
Iteration 17/1000 | Loss: 0.00421556
Iteration 18/1000 | Loss: 0.00228462
Iteration 19/1000 | Loss: 0.00290454
Iteration 20/1000 | Loss: 0.00060573
Iteration 21/1000 | Loss: 0.00062644
Iteration 22/1000 | Loss: 0.00563663
Iteration 23/1000 | Loss: 0.00099042
Iteration 24/1000 | Loss: 0.00280735
Iteration 25/1000 | Loss: 0.00053290
Iteration 26/1000 | Loss: 0.00159099
Iteration 27/1000 | Loss: 0.00128963
Iteration 28/1000 | Loss: 0.00404117
Iteration 29/1000 | Loss: 0.00098009
Iteration 30/1000 | Loss: 0.00081255
Iteration 31/1000 | Loss: 0.00029046
Iteration 32/1000 | Loss: 0.00105721
Iteration 33/1000 | Loss: 0.00489368
Iteration 34/1000 | Loss: 0.00033038
Iteration 35/1000 | Loss: 0.00012520
Iteration 36/1000 | Loss: 0.00135530
Iteration 37/1000 | Loss: 0.00182527
Iteration 38/1000 | Loss: 0.00028456
Iteration 39/1000 | Loss: 0.00084094
Iteration 40/1000 | Loss: 0.00061889
Iteration 41/1000 | Loss: 0.00195436
Iteration 42/1000 | Loss: 0.00145395
Iteration 43/1000 | Loss: 0.00066669
Iteration 44/1000 | Loss: 0.00104745
Iteration 45/1000 | Loss: 0.00144494
Iteration 46/1000 | Loss: 0.00012800
Iteration 47/1000 | Loss: 0.00141565
Iteration 48/1000 | Loss: 0.00071726
Iteration 49/1000 | Loss: 0.00203971
Iteration 50/1000 | Loss: 0.00117692
Iteration 51/1000 | Loss: 0.00064334
Iteration 52/1000 | Loss: 0.00158154
Iteration 53/1000 | Loss: 0.00201319
Iteration 54/1000 | Loss: 0.00080587
Iteration 55/1000 | Loss: 0.00046907
Iteration 56/1000 | Loss: 0.00182117
Iteration 57/1000 | Loss: 0.00153797
Iteration 58/1000 | Loss: 0.00020918
Iteration 59/1000 | Loss: 0.00013133
Iteration 60/1000 | Loss: 0.00110767
Iteration 61/1000 | Loss: 0.00028351
Iteration 62/1000 | Loss: 0.00040915
Iteration 63/1000 | Loss: 0.00150877
Iteration 64/1000 | Loss: 0.00064163
Iteration 65/1000 | Loss: 0.00066778
Iteration 66/1000 | Loss: 0.00125536
Iteration 67/1000 | Loss: 0.00011650
Iteration 68/1000 | Loss: 0.00063250
Iteration 69/1000 | Loss: 0.00088936
Iteration 70/1000 | Loss: 0.00108284
Iteration 71/1000 | Loss: 0.00010675
Iteration 72/1000 | Loss: 0.00060368
Iteration 73/1000 | Loss: 0.00110115
Iteration 74/1000 | Loss: 0.00013245
Iteration 75/1000 | Loss: 0.00045246
Iteration 76/1000 | Loss: 0.00017365
Iteration 77/1000 | Loss: 0.00026297
Iteration 78/1000 | Loss: 0.00077001
Iteration 79/1000 | Loss: 0.00028606
Iteration 80/1000 | Loss: 0.00009302
Iteration 81/1000 | Loss: 0.00027293
Iteration 82/1000 | Loss: 0.00127642
Iteration 83/1000 | Loss: 0.00063315
Iteration 84/1000 | Loss: 0.00033129
Iteration 85/1000 | Loss: 0.00080627
Iteration 86/1000 | Loss: 0.00124973
Iteration 87/1000 | Loss: 0.00015967
Iteration 88/1000 | Loss: 0.00115015
Iteration 89/1000 | Loss: 0.00015734
Iteration 90/1000 | Loss: 0.00207006
Iteration 91/1000 | Loss: 0.00145146
Iteration 92/1000 | Loss: 0.00026417
Iteration 93/1000 | Loss: 0.00090547
Iteration 94/1000 | Loss: 0.00043691
Iteration 95/1000 | Loss: 0.00130614
Iteration 96/1000 | Loss: 0.00048832
Iteration 97/1000 | Loss: 0.00012933
Iteration 98/1000 | Loss: 0.00014719
Iteration 99/1000 | Loss: 0.00179445
Iteration 100/1000 | Loss: 0.00023154
Iteration 101/1000 | Loss: 0.00041685
Iteration 102/1000 | Loss: 0.00057494
Iteration 103/1000 | Loss: 0.00073269
Iteration 104/1000 | Loss: 0.00425726
Iteration 105/1000 | Loss: 0.00112919
Iteration 106/1000 | Loss: 0.00293708
Iteration 107/1000 | Loss: 0.00095425
Iteration 108/1000 | Loss: 0.00146408
Iteration 109/1000 | Loss: 0.00034200
Iteration 110/1000 | Loss: 0.00054829
Iteration 111/1000 | Loss: 0.00036985
Iteration 112/1000 | Loss: 0.00010165
Iteration 113/1000 | Loss: 0.00029518
Iteration 114/1000 | Loss: 0.00007842
Iteration 115/1000 | Loss: 0.00055308
Iteration 116/1000 | Loss: 0.00050881
Iteration 117/1000 | Loss: 0.00104431
Iteration 118/1000 | Loss: 0.00079361
Iteration 119/1000 | Loss: 0.00058188
Iteration 120/1000 | Loss: 0.00119862
Iteration 121/1000 | Loss: 0.00012448
Iteration 122/1000 | Loss: 0.00043364
Iteration 123/1000 | Loss: 0.00058222
Iteration 124/1000 | Loss: 0.00014009
Iteration 125/1000 | Loss: 0.00049314
Iteration 126/1000 | Loss: 0.00053459
Iteration 127/1000 | Loss: 0.00019019
Iteration 128/1000 | Loss: 0.00006975
Iteration 129/1000 | Loss: 0.00006348
Iteration 130/1000 | Loss: 0.00006121
Iteration 131/1000 | Loss: 0.00103078
Iteration 132/1000 | Loss: 0.00013071
Iteration 133/1000 | Loss: 0.00005986
Iteration 134/1000 | Loss: 0.00202135
Iteration 135/1000 | Loss: 0.00013883
Iteration 136/1000 | Loss: 0.00005914
Iteration 137/1000 | Loss: 0.00102704
Iteration 138/1000 | Loss: 0.00128323
Iteration 139/1000 | Loss: 0.00057663
Iteration 140/1000 | Loss: 0.00006244
Iteration 141/1000 | Loss: 0.00005640
Iteration 142/1000 | Loss: 0.00117289
Iteration 143/1000 | Loss: 0.00364718
Iteration 144/1000 | Loss: 0.00253520
Iteration 145/1000 | Loss: 0.00055308
Iteration 146/1000 | Loss: 0.00025191
Iteration 147/1000 | Loss: 0.00057218
Iteration 148/1000 | Loss: 0.00066249
Iteration 149/1000 | Loss: 0.00040617
Iteration 150/1000 | Loss: 0.00144785
Iteration 151/1000 | Loss: 0.00013256
Iteration 152/1000 | Loss: 0.00080712
Iteration 153/1000 | Loss: 0.00022998
Iteration 154/1000 | Loss: 0.00006480
Iteration 155/1000 | Loss: 0.00005846
Iteration 156/1000 | Loss: 0.00005551
Iteration 157/1000 | Loss: 0.00005354
Iteration 158/1000 | Loss: 0.00005238
Iteration 159/1000 | Loss: 0.00102950
Iteration 160/1000 | Loss: 0.00138780
Iteration 161/1000 | Loss: 0.00032704
Iteration 162/1000 | Loss: 0.00005367
Iteration 163/1000 | Loss: 0.00004985
Iteration 164/1000 | Loss: 0.00004890
Iteration 165/1000 | Loss: 0.00004837
Iteration 166/1000 | Loss: 0.00031076
Iteration 167/1000 | Loss: 0.00103133
Iteration 168/1000 | Loss: 0.00020201
Iteration 169/1000 | Loss: 0.00005090
Iteration 170/1000 | Loss: 0.00019768
Iteration 171/1000 | Loss: 0.00004800
Iteration 172/1000 | Loss: 0.00004690
Iteration 173/1000 | Loss: 0.00004616
Iteration 174/1000 | Loss: 0.00102272
Iteration 175/1000 | Loss: 0.00093710
Iteration 176/1000 | Loss: 0.00098079
Iteration 177/1000 | Loss: 0.00009694
Iteration 178/1000 | Loss: 0.00004837
Iteration 179/1000 | Loss: 0.00004579
Iteration 180/1000 | Loss: 0.00004546
Iteration 181/1000 | Loss: 0.00100027
Iteration 182/1000 | Loss: 0.00082151
Iteration 183/1000 | Loss: 0.00011315
Iteration 184/1000 | Loss: 0.00004563
Iteration 185/1000 | Loss: 0.00004528
Iteration 186/1000 | Loss: 0.00101119
Iteration 187/1000 | Loss: 0.00006877
Iteration 188/1000 | Loss: 0.00004757
Iteration 189/1000 | Loss: 0.00004565
Iteration 190/1000 | Loss: 0.00004520
Iteration 191/1000 | Loss: 0.00004513
Iteration 192/1000 | Loss: 0.00004510
Iteration 193/1000 | Loss: 0.00004509
Iteration 194/1000 | Loss: 0.00004509
Iteration 195/1000 | Loss: 0.00004509
Iteration 196/1000 | Loss: 0.00004508
Iteration 197/1000 | Loss: 0.00004508
Iteration 198/1000 | Loss: 0.00004507
Iteration 199/1000 | Loss: 0.00004506
Iteration 200/1000 | Loss: 0.00004506
Iteration 201/1000 | Loss: 0.00004506
Iteration 202/1000 | Loss: 0.00004504
Iteration 203/1000 | Loss: 0.00004504
Iteration 204/1000 | Loss: 0.00101811
Iteration 205/1000 | Loss: 0.00456643
Iteration 206/1000 | Loss: 0.00062529
Iteration 207/1000 | Loss: 0.00053621
Iteration 208/1000 | Loss: 0.00021471
Iteration 209/1000 | Loss: 0.00028789
Iteration 210/1000 | Loss: 0.00018991
Iteration 211/1000 | Loss: 0.00019040
Iteration 212/1000 | Loss: 0.00004571
Iteration 213/1000 | Loss: 0.00094661
Iteration 214/1000 | Loss: 0.00009055
Iteration 215/1000 | Loss: 0.00004254
Iteration 216/1000 | Loss: 0.00003868
Iteration 217/1000 | Loss: 0.00003767
Iteration 218/1000 | Loss: 0.00099272
Iteration 219/1000 | Loss: 0.00043457
Iteration 220/1000 | Loss: 0.00068297
Iteration 221/1000 | Loss: 0.00017760
Iteration 222/1000 | Loss: 0.00004243
Iteration 223/1000 | Loss: 0.00064559
Iteration 224/1000 | Loss: 0.00013314
Iteration 225/1000 | Loss: 0.00004137
Iteration 226/1000 | Loss: 0.00065628
Iteration 227/1000 | Loss: 0.00010236
Iteration 228/1000 | Loss: 0.00003852
Iteration 229/1000 | Loss: 0.00076879
Iteration 230/1000 | Loss: 0.00011351
Iteration 231/1000 | Loss: 0.00004052
Iteration 232/1000 | Loss: 0.00065209
Iteration 233/1000 | Loss: 0.00007089
Iteration 234/1000 | Loss: 0.00004972
Iteration 235/1000 | Loss: 0.00004606
Iteration 236/1000 | Loss: 0.00004187
Iteration 237/1000 | Loss: 0.00059284
Iteration 238/1000 | Loss: 0.00004673
Iteration 239/1000 | Loss: 0.00004445
Iteration 240/1000 | Loss: 0.00003662
Iteration 241/1000 | Loss: 0.00003557
Iteration 242/1000 | Loss: 0.00003474
Iteration 243/1000 | Loss: 0.00003437
Iteration 244/1000 | Loss: 0.00003415
Iteration 245/1000 | Loss: 0.00003397
Iteration 246/1000 | Loss: 0.00003395
Iteration 247/1000 | Loss: 0.00003395
Iteration 248/1000 | Loss: 0.00003395
Iteration 249/1000 | Loss: 0.00003394
Iteration 250/1000 | Loss: 0.00003385
Iteration 251/1000 | Loss: 0.00003384
Iteration 252/1000 | Loss: 0.00003382
Iteration 253/1000 | Loss: 0.00003381
Iteration 254/1000 | Loss: 0.00003377
Iteration 255/1000 | Loss: 0.00003374
Iteration 256/1000 | Loss: 0.00003373
Iteration 257/1000 | Loss: 0.00003372
Iteration 258/1000 | Loss: 0.00003369
Iteration 259/1000 | Loss: 0.00003369
Iteration 260/1000 | Loss: 0.00003368
Iteration 261/1000 | Loss: 0.00003366
Iteration 262/1000 | Loss: 0.00003366
Iteration 263/1000 | Loss: 0.00003366
Iteration 264/1000 | Loss: 0.00003366
Iteration 265/1000 | Loss: 0.00003366
Iteration 266/1000 | Loss: 0.00003366
Iteration 267/1000 | Loss: 0.00003366
Iteration 268/1000 | Loss: 0.00003366
Iteration 269/1000 | Loss: 0.00003366
Iteration 270/1000 | Loss: 0.00003365
Iteration 271/1000 | Loss: 0.00003365
Iteration 272/1000 | Loss: 0.00003365
Iteration 273/1000 | Loss: 0.00003365
Iteration 274/1000 | Loss: 0.00003365
Iteration 275/1000 | Loss: 0.00003365
Iteration 276/1000 | Loss: 0.00003365
Iteration 277/1000 | Loss: 0.00003365
Iteration 278/1000 | Loss: 0.00003365
Iteration 279/1000 | Loss: 0.00003365
Iteration 280/1000 | Loss: 0.00003364
Iteration 281/1000 | Loss: 0.00003364
Iteration 282/1000 | Loss: 0.00003364
Iteration 283/1000 | Loss: 0.00003363
Iteration 284/1000 | Loss: 0.00003363
Iteration 285/1000 | Loss: 0.00003363
Iteration 286/1000 | Loss: 0.00003362
Iteration 287/1000 | Loss: 0.00003362
Iteration 288/1000 | Loss: 0.00003362
Iteration 289/1000 | Loss: 0.00003362
Iteration 290/1000 | Loss: 0.00003362
Iteration 291/1000 | Loss: 0.00003362
Iteration 292/1000 | Loss: 0.00003362
Iteration 293/1000 | Loss: 0.00003362
Iteration 294/1000 | Loss: 0.00003362
Iteration 295/1000 | Loss: 0.00003362
Iteration 296/1000 | Loss: 0.00003362
Iteration 297/1000 | Loss: 0.00003362
Iteration 298/1000 | Loss: 0.00003362
Iteration 299/1000 | Loss: 0.00003362
Iteration 300/1000 | Loss: 0.00003362
Iteration 301/1000 | Loss: 0.00003362
Iteration 302/1000 | Loss: 0.00003362
Iteration 303/1000 | Loss: 0.00003362
Iteration 304/1000 | Loss: 0.00003362
Iteration 305/1000 | Loss: 0.00003362
Iteration 306/1000 | Loss: 0.00003362
Iteration 307/1000 | Loss: 0.00003362
Iteration 308/1000 | Loss: 0.00003362
Iteration 309/1000 | Loss: 0.00003362
Iteration 310/1000 | Loss: 0.00003362
Iteration 311/1000 | Loss: 0.00003362
Iteration 312/1000 | Loss: 0.00003362
Iteration 313/1000 | Loss: 0.00003362
Iteration 314/1000 | Loss: 0.00003362
Iteration 315/1000 | Loss: 0.00003362
Iteration 316/1000 | Loss: 0.00003362
Iteration 317/1000 | Loss: 0.00003362
Iteration 318/1000 | Loss: 0.00003362
Iteration 319/1000 | Loss: 0.00003362
Iteration 320/1000 | Loss: 0.00003362
Iteration 321/1000 | Loss: 0.00003362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 321. Stopping optimization.
Last 5 losses: [3.361620838404633e-05, 3.361620838404633e-05, 3.361620838404633e-05, 3.361620838404633e-05, 3.361620838404633e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.361620838404633e-05

Optimization complete. Final v2v error: 4.5543646812438965 mm

Highest mean error: 14.962011337280273 mm for frame 208

Lowest mean error: 3.8641786575317383 mm for frame 195

Saving results

Total time: 431.0002760887146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952580
Iteration 2/25 | Loss: 0.00195540
Iteration 3/25 | Loss: 0.00140266
Iteration 4/25 | Loss: 0.00134928
Iteration 5/25 | Loss: 0.00137604
Iteration 6/25 | Loss: 0.00132134
Iteration 7/25 | Loss: 0.00125729
Iteration 8/25 | Loss: 0.00122356
Iteration 9/25 | Loss: 0.00120743
Iteration 10/25 | Loss: 0.00120063
Iteration 11/25 | Loss: 0.00119897
Iteration 12/25 | Loss: 0.00119836
Iteration 13/25 | Loss: 0.00119812
Iteration 14/25 | Loss: 0.00119808
Iteration 15/25 | Loss: 0.00119808
Iteration 16/25 | Loss: 0.00119808
Iteration 17/25 | Loss: 0.00119808
Iteration 18/25 | Loss: 0.00119807
Iteration 19/25 | Loss: 0.00119807
Iteration 20/25 | Loss: 0.00119807
Iteration 21/25 | Loss: 0.00119807
Iteration 22/25 | Loss: 0.00119807
Iteration 23/25 | Loss: 0.00119807
Iteration 24/25 | Loss: 0.00119807
Iteration 25/25 | Loss: 0.00119807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28379643
Iteration 2/25 | Loss: 0.00097602
Iteration 3/25 | Loss: 0.00097602
Iteration 4/25 | Loss: 0.00097602
Iteration 5/25 | Loss: 0.00097602
Iteration 6/25 | Loss: 0.00097602
Iteration 7/25 | Loss: 0.00097602
Iteration 8/25 | Loss: 0.00097602
Iteration 9/25 | Loss: 0.00097602
Iteration 10/25 | Loss: 0.00097602
Iteration 11/25 | Loss: 0.00097602
Iteration 12/25 | Loss: 0.00097602
Iteration 13/25 | Loss: 0.00097602
Iteration 14/25 | Loss: 0.00097602
Iteration 15/25 | Loss: 0.00097602
Iteration 16/25 | Loss: 0.00097602
Iteration 17/25 | Loss: 0.00097602
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009760155808180571, 0.0009760155808180571, 0.0009760155808180571, 0.0009760155808180571, 0.0009760155808180571]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009760155808180571

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097602
Iteration 2/1000 | Loss: 0.00005351
Iteration 3/1000 | Loss: 0.00004109
Iteration 4/1000 | Loss: 0.00003673
Iteration 5/1000 | Loss: 0.00003482
Iteration 6/1000 | Loss: 0.00003372
Iteration 7/1000 | Loss: 0.00003280
Iteration 8/1000 | Loss: 0.00003215
Iteration 9/1000 | Loss: 0.00003163
Iteration 10/1000 | Loss: 0.00003130
Iteration 11/1000 | Loss: 0.00003111
Iteration 12/1000 | Loss: 0.00003102
Iteration 13/1000 | Loss: 0.00003097
Iteration 14/1000 | Loss: 0.00003089
Iteration 15/1000 | Loss: 0.00003088
Iteration 16/1000 | Loss: 0.00003084
Iteration 17/1000 | Loss: 0.00003084
Iteration 18/1000 | Loss: 0.00003083
Iteration 19/1000 | Loss: 0.00003080
Iteration 20/1000 | Loss: 0.00003079
Iteration 21/1000 | Loss: 0.00003078
Iteration 22/1000 | Loss: 0.00003077
Iteration 23/1000 | Loss: 0.00003077
Iteration 24/1000 | Loss: 0.00003076
Iteration 25/1000 | Loss: 0.00003076
Iteration 26/1000 | Loss: 0.00003075
Iteration 27/1000 | Loss: 0.00003075
Iteration 28/1000 | Loss: 0.00003074
Iteration 29/1000 | Loss: 0.00003073
Iteration 30/1000 | Loss: 0.00003073
Iteration 31/1000 | Loss: 0.00003072
Iteration 32/1000 | Loss: 0.00003072
Iteration 33/1000 | Loss: 0.00003072
Iteration 34/1000 | Loss: 0.00003072
Iteration 35/1000 | Loss: 0.00003072
Iteration 36/1000 | Loss: 0.00003071
Iteration 37/1000 | Loss: 0.00003071
Iteration 38/1000 | Loss: 0.00003070
Iteration 39/1000 | Loss: 0.00003070
Iteration 40/1000 | Loss: 0.00003070
Iteration 41/1000 | Loss: 0.00003070
Iteration 42/1000 | Loss: 0.00003069
Iteration 43/1000 | Loss: 0.00003069
Iteration 44/1000 | Loss: 0.00003069
Iteration 45/1000 | Loss: 0.00003069
Iteration 46/1000 | Loss: 0.00003068
Iteration 47/1000 | Loss: 0.00003068
Iteration 48/1000 | Loss: 0.00003068
Iteration 49/1000 | Loss: 0.00003068
Iteration 50/1000 | Loss: 0.00003067
Iteration 51/1000 | Loss: 0.00003067
Iteration 52/1000 | Loss: 0.00003067
Iteration 53/1000 | Loss: 0.00003067
Iteration 54/1000 | Loss: 0.00003067
Iteration 55/1000 | Loss: 0.00003067
Iteration 56/1000 | Loss: 0.00003067
Iteration 57/1000 | Loss: 0.00003067
Iteration 58/1000 | Loss: 0.00003067
Iteration 59/1000 | Loss: 0.00003066
Iteration 60/1000 | Loss: 0.00003066
Iteration 61/1000 | Loss: 0.00003066
Iteration 62/1000 | Loss: 0.00003066
Iteration 63/1000 | Loss: 0.00003066
Iteration 64/1000 | Loss: 0.00003066
Iteration 65/1000 | Loss: 0.00003066
Iteration 66/1000 | Loss: 0.00003066
Iteration 67/1000 | Loss: 0.00003065
Iteration 68/1000 | Loss: 0.00003065
Iteration 69/1000 | Loss: 0.00003065
Iteration 70/1000 | Loss: 0.00003065
Iteration 71/1000 | Loss: 0.00003065
Iteration 72/1000 | Loss: 0.00003065
Iteration 73/1000 | Loss: 0.00003065
Iteration 74/1000 | Loss: 0.00003065
Iteration 75/1000 | Loss: 0.00003065
Iteration 76/1000 | Loss: 0.00003064
Iteration 77/1000 | Loss: 0.00003064
Iteration 78/1000 | Loss: 0.00003064
Iteration 79/1000 | Loss: 0.00003064
Iteration 80/1000 | Loss: 0.00003064
Iteration 81/1000 | Loss: 0.00003064
Iteration 82/1000 | Loss: 0.00003064
Iteration 83/1000 | Loss: 0.00003064
Iteration 84/1000 | Loss: 0.00003063
Iteration 85/1000 | Loss: 0.00003063
Iteration 86/1000 | Loss: 0.00003063
Iteration 87/1000 | Loss: 0.00003063
Iteration 88/1000 | Loss: 0.00003063
Iteration 89/1000 | Loss: 0.00003063
Iteration 90/1000 | Loss: 0.00003063
Iteration 91/1000 | Loss: 0.00003063
Iteration 92/1000 | Loss: 0.00003063
Iteration 93/1000 | Loss: 0.00003062
Iteration 94/1000 | Loss: 0.00003062
Iteration 95/1000 | Loss: 0.00003062
Iteration 96/1000 | Loss: 0.00003062
Iteration 97/1000 | Loss: 0.00003062
Iteration 98/1000 | Loss: 0.00003062
Iteration 99/1000 | Loss: 0.00003062
Iteration 100/1000 | Loss: 0.00003062
Iteration 101/1000 | Loss: 0.00003062
Iteration 102/1000 | Loss: 0.00003062
Iteration 103/1000 | Loss: 0.00003062
Iteration 104/1000 | Loss: 0.00003061
Iteration 105/1000 | Loss: 0.00003061
Iteration 106/1000 | Loss: 0.00003061
Iteration 107/1000 | Loss: 0.00003061
Iteration 108/1000 | Loss: 0.00003061
Iteration 109/1000 | Loss: 0.00003061
Iteration 110/1000 | Loss: 0.00003061
Iteration 111/1000 | Loss: 0.00003061
Iteration 112/1000 | Loss: 0.00003061
Iteration 113/1000 | Loss: 0.00003061
Iteration 114/1000 | Loss: 0.00003061
Iteration 115/1000 | Loss: 0.00003061
Iteration 116/1000 | Loss: 0.00003061
Iteration 117/1000 | Loss: 0.00003061
Iteration 118/1000 | Loss: 0.00003060
Iteration 119/1000 | Loss: 0.00003060
Iteration 120/1000 | Loss: 0.00003060
Iteration 121/1000 | Loss: 0.00003060
Iteration 122/1000 | Loss: 0.00003060
Iteration 123/1000 | Loss: 0.00003060
Iteration 124/1000 | Loss: 0.00003060
Iteration 125/1000 | Loss: 0.00003060
Iteration 126/1000 | Loss: 0.00003060
Iteration 127/1000 | Loss: 0.00003060
Iteration 128/1000 | Loss: 0.00003060
Iteration 129/1000 | Loss: 0.00003060
Iteration 130/1000 | Loss: 0.00003060
Iteration 131/1000 | Loss: 0.00003060
Iteration 132/1000 | Loss: 0.00003060
Iteration 133/1000 | Loss: 0.00003060
Iteration 134/1000 | Loss: 0.00003060
Iteration 135/1000 | Loss: 0.00003060
Iteration 136/1000 | Loss: 0.00003060
Iteration 137/1000 | Loss: 0.00003060
Iteration 138/1000 | Loss: 0.00003060
Iteration 139/1000 | Loss: 0.00003060
Iteration 140/1000 | Loss: 0.00003060
Iteration 141/1000 | Loss: 0.00003060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [3.0598104785894975e-05, 3.0598104785894975e-05, 3.0598104785894975e-05, 3.0598104785894975e-05, 3.0598104785894975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0598104785894975e-05

Optimization complete. Final v2v error: 4.766169548034668 mm

Highest mean error: 5.142073154449463 mm for frame 75

Lowest mean error: 4.537709712982178 mm for frame 102

Saving results

Total time: 48.17719101905823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003394
Iteration 2/25 | Loss: 0.00159160
Iteration 3/25 | Loss: 0.00133055
Iteration 4/25 | Loss: 0.00128887
Iteration 5/25 | Loss: 0.00129764
Iteration 6/25 | Loss: 0.00127595
Iteration 7/25 | Loss: 0.00125879
Iteration 8/25 | Loss: 0.00125571
Iteration 9/25 | Loss: 0.00125075
Iteration 10/25 | Loss: 0.00124830
Iteration 11/25 | Loss: 0.00125184
Iteration 12/25 | Loss: 0.00125020
Iteration 13/25 | Loss: 0.00124948
Iteration 14/25 | Loss: 0.00124897
Iteration 15/25 | Loss: 0.00124723
Iteration 16/25 | Loss: 0.00124593
Iteration 17/25 | Loss: 0.00124576
Iteration 18/25 | Loss: 0.00124575
Iteration 19/25 | Loss: 0.00124575
Iteration 20/25 | Loss: 0.00124575
Iteration 21/25 | Loss: 0.00124575
Iteration 22/25 | Loss: 0.00124574
Iteration 23/25 | Loss: 0.00124574
Iteration 24/25 | Loss: 0.00124574
Iteration 25/25 | Loss: 0.00124574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20028567
Iteration 2/25 | Loss: 0.00085371
Iteration 3/25 | Loss: 0.00085367
Iteration 4/25 | Loss: 0.00085367
Iteration 5/25 | Loss: 0.00085367
Iteration 6/25 | Loss: 0.00085367
Iteration 7/25 | Loss: 0.00085367
Iteration 8/25 | Loss: 0.00085367
Iteration 9/25 | Loss: 0.00085367
Iteration 10/25 | Loss: 0.00085367
Iteration 11/25 | Loss: 0.00085367
Iteration 12/25 | Loss: 0.00085367
Iteration 13/25 | Loss: 0.00085367
Iteration 14/25 | Loss: 0.00085367
Iteration 15/25 | Loss: 0.00085367
Iteration 16/25 | Loss: 0.00085367
Iteration 17/25 | Loss: 0.00085367
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008536691893823445, 0.0008536691893823445, 0.0008536691893823445, 0.0008536691893823445, 0.0008536691893823445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008536691893823445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085367
Iteration 2/1000 | Loss: 0.00004674
Iteration 3/1000 | Loss: 0.00004035
Iteration 4/1000 | Loss: 0.00003790
Iteration 5/1000 | Loss: 0.00003669
Iteration 6/1000 | Loss: 0.00003575
Iteration 7/1000 | Loss: 0.00003524
Iteration 8/1000 | Loss: 0.00003472
Iteration 9/1000 | Loss: 0.00003431
Iteration 10/1000 | Loss: 0.00003408
Iteration 11/1000 | Loss: 0.00003381
Iteration 12/1000 | Loss: 0.00003366
Iteration 13/1000 | Loss: 0.00003354
Iteration 14/1000 | Loss: 0.00003354
Iteration 15/1000 | Loss: 0.00003350
Iteration 16/1000 | Loss: 0.00003345
Iteration 17/1000 | Loss: 0.00003344
Iteration 18/1000 | Loss: 0.00003339
Iteration 19/1000 | Loss: 0.00003339
Iteration 20/1000 | Loss: 0.00003337
Iteration 21/1000 | Loss: 0.00003336
Iteration 22/1000 | Loss: 0.00003336
Iteration 23/1000 | Loss: 0.00003336
Iteration 24/1000 | Loss: 0.00003336
Iteration 25/1000 | Loss: 0.00003335
Iteration 26/1000 | Loss: 0.00003335
Iteration 27/1000 | Loss: 0.00003335
Iteration 28/1000 | Loss: 0.00003335
Iteration 29/1000 | Loss: 0.00003335
Iteration 30/1000 | Loss: 0.00003335
Iteration 31/1000 | Loss: 0.00003335
Iteration 32/1000 | Loss: 0.00003335
Iteration 33/1000 | Loss: 0.00003335
Iteration 34/1000 | Loss: 0.00003335
Iteration 35/1000 | Loss: 0.00003335
Iteration 36/1000 | Loss: 0.00003334
Iteration 37/1000 | Loss: 0.00003334
Iteration 38/1000 | Loss: 0.00003334
Iteration 39/1000 | Loss: 0.00003334
Iteration 40/1000 | Loss: 0.00003334
Iteration 41/1000 | Loss: 0.00003334
Iteration 42/1000 | Loss: 0.00003334
Iteration 43/1000 | Loss: 0.00003333
Iteration 44/1000 | Loss: 0.00003333
Iteration 45/1000 | Loss: 0.00003332
Iteration 46/1000 | Loss: 0.00003332
Iteration 47/1000 | Loss: 0.00003332
Iteration 48/1000 | Loss: 0.00003331
Iteration 49/1000 | Loss: 0.00003331
Iteration 50/1000 | Loss: 0.00003331
Iteration 51/1000 | Loss: 0.00003331
Iteration 52/1000 | Loss: 0.00003331
Iteration 53/1000 | Loss: 0.00003330
Iteration 54/1000 | Loss: 0.00003330
Iteration 55/1000 | Loss: 0.00003330
Iteration 56/1000 | Loss: 0.00003330
Iteration 57/1000 | Loss: 0.00003329
Iteration 58/1000 | Loss: 0.00003329
Iteration 59/1000 | Loss: 0.00003329
Iteration 60/1000 | Loss: 0.00003329
Iteration 61/1000 | Loss: 0.00003328
Iteration 62/1000 | Loss: 0.00003328
Iteration 63/1000 | Loss: 0.00003328
Iteration 64/1000 | Loss: 0.00003328
Iteration 65/1000 | Loss: 0.00003328
Iteration 66/1000 | Loss: 0.00003328
Iteration 67/1000 | Loss: 0.00003328
Iteration 68/1000 | Loss: 0.00003328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [3.3280924981227145e-05, 3.3280924981227145e-05, 3.3280924981227145e-05, 3.3280924981227145e-05, 3.3280924981227145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3280924981227145e-05

Optimization complete. Final v2v error: 4.923116207122803 mm

Highest mean error: 5.505434989929199 mm for frame 90

Lowest mean error: 4.446584701538086 mm for frame 234

Saving results

Total time: 60.75125741958618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00590291
Iteration 2/25 | Loss: 0.00131563
Iteration 3/25 | Loss: 0.00121040
Iteration 4/25 | Loss: 0.00119519
Iteration 5/25 | Loss: 0.00118850
Iteration 6/25 | Loss: 0.00118646
Iteration 7/25 | Loss: 0.00118585
Iteration 8/25 | Loss: 0.00118585
Iteration 9/25 | Loss: 0.00118585
Iteration 10/25 | Loss: 0.00118585
Iteration 11/25 | Loss: 0.00118585
Iteration 12/25 | Loss: 0.00118585
Iteration 13/25 | Loss: 0.00118585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011858523357659578, 0.0011858523357659578, 0.0011858523357659578, 0.0011858523357659578, 0.0011858523357659578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011858523357659578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.76268053
Iteration 2/25 | Loss: 0.00108838
Iteration 3/25 | Loss: 0.00108838
Iteration 4/25 | Loss: 0.00108838
Iteration 5/25 | Loss: 0.00108838
Iteration 6/25 | Loss: 0.00108838
Iteration 7/25 | Loss: 0.00108838
Iteration 8/25 | Loss: 0.00108838
Iteration 9/25 | Loss: 0.00108838
Iteration 10/25 | Loss: 0.00108838
Iteration 11/25 | Loss: 0.00108838
Iteration 12/25 | Loss: 0.00108838
Iteration 13/25 | Loss: 0.00108838
Iteration 14/25 | Loss: 0.00108838
Iteration 15/25 | Loss: 0.00108838
Iteration 16/25 | Loss: 0.00108838
Iteration 17/25 | Loss: 0.00108838
Iteration 18/25 | Loss: 0.00108838
Iteration 19/25 | Loss: 0.00108838
Iteration 20/25 | Loss: 0.00108838
Iteration 21/25 | Loss: 0.00108838
Iteration 22/25 | Loss: 0.00108838
Iteration 23/25 | Loss: 0.00108838
Iteration 24/25 | Loss: 0.00108838
Iteration 25/25 | Loss: 0.00108838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108838
Iteration 2/1000 | Loss: 0.00004688
Iteration 3/1000 | Loss: 0.00003902
Iteration 4/1000 | Loss: 0.00003538
Iteration 5/1000 | Loss: 0.00003344
Iteration 6/1000 | Loss: 0.00003213
Iteration 7/1000 | Loss: 0.00003125
Iteration 8/1000 | Loss: 0.00003079
Iteration 9/1000 | Loss: 0.00003057
Iteration 10/1000 | Loss: 0.00003045
Iteration 11/1000 | Loss: 0.00003030
Iteration 12/1000 | Loss: 0.00003029
Iteration 13/1000 | Loss: 0.00003025
Iteration 14/1000 | Loss: 0.00003025
Iteration 15/1000 | Loss: 0.00003024
Iteration 16/1000 | Loss: 0.00003024
Iteration 17/1000 | Loss: 0.00003023
Iteration 18/1000 | Loss: 0.00003023
Iteration 19/1000 | Loss: 0.00003022
Iteration 20/1000 | Loss: 0.00003021
Iteration 21/1000 | Loss: 0.00003021
Iteration 22/1000 | Loss: 0.00003021
Iteration 23/1000 | Loss: 0.00003020
Iteration 24/1000 | Loss: 0.00003020
Iteration 25/1000 | Loss: 0.00003020
Iteration 26/1000 | Loss: 0.00003019
Iteration 27/1000 | Loss: 0.00003019
Iteration 28/1000 | Loss: 0.00003015
Iteration 29/1000 | Loss: 0.00003015
Iteration 30/1000 | Loss: 0.00003014
Iteration 31/1000 | Loss: 0.00003013
Iteration 32/1000 | Loss: 0.00003013
Iteration 33/1000 | Loss: 0.00003012
Iteration 34/1000 | Loss: 0.00003011
Iteration 35/1000 | Loss: 0.00003009
Iteration 36/1000 | Loss: 0.00003009
Iteration 37/1000 | Loss: 0.00003009
Iteration 38/1000 | Loss: 0.00003008
Iteration 39/1000 | Loss: 0.00003007
Iteration 40/1000 | Loss: 0.00003007
Iteration 41/1000 | Loss: 0.00003007
Iteration 42/1000 | Loss: 0.00003006
Iteration 43/1000 | Loss: 0.00003006
Iteration 44/1000 | Loss: 0.00003005
Iteration 45/1000 | Loss: 0.00003005
Iteration 46/1000 | Loss: 0.00003005
Iteration 47/1000 | Loss: 0.00003005
Iteration 48/1000 | Loss: 0.00003005
Iteration 49/1000 | Loss: 0.00003005
Iteration 50/1000 | Loss: 0.00003005
Iteration 51/1000 | Loss: 0.00003005
Iteration 52/1000 | Loss: 0.00003005
Iteration 53/1000 | Loss: 0.00003005
Iteration 54/1000 | Loss: 0.00003005
Iteration 55/1000 | Loss: 0.00003004
Iteration 56/1000 | Loss: 0.00003004
Iteration 57/1000 | Loss: 0.00003004
Iteration 58/1000 | Loss: 0.00003004
Iteration 59/1000 | Loss: 0.00003003
Iteration 60/1000 | Loss: 0.00003003
Iteration 61/1000 | Loss: 0.00003003
Iteration 62/1000 | Loss: 0.00003003
Iteration 63/1000 | Loss: 0.00003003
Iteration 64/1000 | Loss: 0.00003002
Iteration 65/1000 | Loss: 0.00003002
Iteration 66/1000 | Loss: 0.00003002
Iteration 67/1000 | Loss: 0.00003001
Iteration 68/1000 | Loss: 0.00003001
Iteration 69/1000 | Loss: 0.00003001
Iteration 70/1000 | Loss: 0.00003000
Iteration 71/1000 | Loss: 0.00003000
Iteration 72/1000 | Loss: 0.00003000
Iteration 73/1000 | Loss: 0.00003000
Iteration 74/1000 | Loss: 0.00003000
Iteration 75/1000 | Loss: 0.00002999
Iteration 76/1000 | Loss: 0.00002999
Iteration 77/1000 | Loss: 0.00002999
Iteration 78/1000 | Loss: 0.00002999
Iteration 79/1000 | Loss: 0.00002999
Iteration 80/1000 | Loss: 0.00002999
Iteration 81/1000 | Loss: 0.00002999
Iteration 82/1000 | Loss: 0.00002999
Iteration 83/1000 | Loss: 0.00002999
Iteration 84/1000 | Loss: 0.00002998
Iteration 85/1000 | Loss: 0.00002998
Iteration 86/1000 | Loss: 0.00002998
Iteration 87/1000 | Loss: 0.00002998
Iteration 88/1000 | Loss: 0.00002997
Iteration 89/1000 | Loss: 0.00002996
Iteration 90/1000 | Loss: 0.00002996
Iteration 91/1000 | Loss: 0.00002996
Iteration 92/1000 | Loss: 0.00002996
Iteration 93/1000 | Loss: 0.00002996
Iteration 94/1000 | Loss: 0.00002996
Iteration 95/1000 | Loss: 0.00002996
Iteration 96/1000 | Loss: 0.00002996
Iteration 97/1000 | Loss: 0.00002995
Iteration 98/1000 | Loss: 0.00002995
Iteration 99/1000 | Loss: 0.00002995
Iteration 100/1000 | Loss: 0.00002994
Iteration 101/1000 | Loss: 0.00002994
Iteration 102/1000 | Loss: 0.00002993
Iteration 103/1000 | Loss: 0.00002993
Iteration 104/1000 | Loss: 0.00002993
Iteration 105/1000 | Loss: 0.00002993
Iteration 106/1000 | Loss: 0.00002993
Iteration 107/1000 | Loss: 0.00002993
Iteration 108/1000 | Loss: 0.00002993
Iteration 109/1000 | Loss: 0.00002993
Iteration 110/1000 | Loss: 0.00002993
Iteration 111/1000 | Loss: 0.00002993
Iteration 112/1000 | Loss: 0.00002993
Iteration 113/1000 | Loss: 0.00002993
Iteration 114/1000 | Loss: 0.00002993
Iteration 115/1000 | Loss: 0.00002992
Iteration 116/1000 | Loss: 0.00002992
Iteration 117/1000 | Loss: 0.00002992
Iteration 118/1000 | Loss: 0.00002992
Iteration 119/1000 | Loss: 0.00002992
Iteration 120/1000 | Loss: 0.00002992
Iteration 121/1000 | Loss: 0.00002992
Iteration 122/1000 | Loss: 0.00002992
Iteration 123/1000 | Loss: 0.00002992
Iteration 124/1000 | Loss: 0.00002991
Iteration 125/1000 | Loss: 0.00002991
Iteration 126/1000 | Loss: 0.00002991
Iteration 127/1000 | Loss: 0.00002991
Iteration 128/1000 | Loss: 0.00002991
Iteration 129/1000 | Loss: 0.00002991
Iteration 130/1000 | Loss: 0.00002991
Iteration 131/1000 | Loss: 0.00002991
Iteration 132/1000 | Loss: 0.00002991
Iteration 133/1000 | Loss: 0.00002991
Iteration 134/1000 | Loss: 0.00002990
Iteration 135/1000 | Loss: 0.00002990
Iteration 136/1000 | Loss: 0.00002990
Iteration 137/1000 | Loss: 0.00002990
Iteration 138/1000 | Loss: 0.00002990
Iteration 139/1000 | Loss: 0.00002990
Iteration 140/1000 | Loss: 0.00002990
Iteration 141/1000 | Loss: 0.00002990
Iteration 142/1000 | Loss: 0.00002990
Iteration 143/1000 | Loss: 0.00002990
Iteration 144/1000 | Loss: 0.00002990
Iteration 145/1000 | Loss: 0.00002990
Iteration 146/1000 | Loss: 0.00002990
Iteration 147/1000 | Loss: 0.00002990
Iteration 148/1000 | Loss: 0.00002990
Iteration 149/1000 | Loss: 0.00002990
Iteration 150/1000 | Loss: 0.00002990
Iteration 151/1000 | Loss: 0.00002990
Iteration 152/1000 | Loss: 0.00002990
Iteration 153/1000 | Loss: 0.00002990
Iteration 154/1000 | Loss: 0.00002990
Iteration 155/1000 | Loss: 0.00002990
Iteration 156/1000 | Loss: 0.00002989
Iteration 157/1000 | Loss: 0.00002989
Iteration 158/1000 | Loss: 0.00002989
Iteration 159/1000 | Loss: 0.00002989
Iteration 160/1000 | Loss: 0.00002989
Iteration 161/1000 | Loss: 0.00002989
Iteration 162/1000 | Loss: 0.00002989
Iteration 163/1000 | Loss: 0.00002989
Iteration 164/1000 | Loss: 0.00002989
Iteration 165/1000 | Loss: 0.00002989
Iteration 166/1000 | Loss: 0.00002989
Iteration 167/1000 | Loss: 0.00002989
Iteration 168/1000 | Loss: 0.00002989
Iteration 169/1000 | Loss: 0.00002989
Iteration 170/1000 | Loss: 0.00002989
Iteration 171/1000 | Loss: 0.00002989
Iteration 172/1000 | Loss: 0.00002989
Iteration 173/1000 | Loss: 0.00002989
Iteration 174/1000 | Loss: 0.00002989
Iteration 175/1000 | Loss: 0.00002989
Iteration 176/1000 | Loss: 0.00002989
Iteration 177/1000 | Loss: 0.00002989
Iteration 178/1000 | Loss: 0.00002988
Iteration 179/1000 | Loss: 0.00002988
Iteration 180/1000 | Loss: 0.00002988
Iteration 181/1000 | Loss: 0.00002988
Iteration 182/1000 | Loss: 0.00002988
Iteration 183/1000 | Loss: 0.00002988
Iteration 184/1000 | Loss: 0.00002988
Iteration 185/1000 | Loss: 0.00002988
Iteration 186/1000 | Loss: 0.00002988
Iteration 187/1000 | Loss: 0.00002988
Iteration 188/1000 | Loss: 0.00002988
Iteration 189/1000 | Loss: 0.00002988
Iteration 190/1000 | Loss: 0.00002988
Iteration 191/1000 | Loss: 0.00002988
Iteration 192/1000 | Loss: 0.00002988
Iteration 193/1000 | Loss: 0.00002988
Iteration 194/1000 | Loss: 0.00002988
Iteration 195/1000 | Loss: 0.00002988
Iteration 196/1000 | Loss: 0.00002988
Iteration 197/1000 | Loss: 0.00002988
Iteration 198/1000 | Loss: 0.00002988
Iteration 199/1000 | Loss: 0.00002988
Iteration 200/1000 | Loss: 0.00002988
Iteration 201/1000 | Loss: 0.00002988
Iteration 202/1000 | Loss: 0.00002988
Iteration 203/1000 | Loss: 0.00002988
Iteration 204/1000 | Loss: 0.00002988
Iteration 205/1000 | Loss: 0.00002988
Iteration 206/1000 | Loss: 0.00002988
Iteration 207/1000 | Loss: 0.00002988
Iteration 208/1000 | Loss: 0.00002988
Iteration 209/1000 | Loss: 0.00002988
Iteration 210/1000 | Loss: 0.00002988
Iteration 211/1000 | Loss: 0.00002988
Iteration 212/1000 | Loss: 0.00002988
Iteration 213/1000 | Loss: 0.00002988
Iteration 214/1000 | Loss: 0.00002988
Iteration 215/1000 | Loss: 0.00002988
Iteration 216/1000 | Loss: 0.00002988
Iteration 217/1000 | Loss: 0.00002988
Iteration 218/1000 | Loss: 0.00002988
Iteration 219/1000 | Loss: 0.00002988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [2.9879369321861304e-05, 2.9879369321861304e-05, 2.9879369321861304e-05, 2.9879369321861304e-05, 2.9879369321861304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9879369321861304e-05

Optimization complete. Final v2v error: 4.662841320037842 mm

Highest mean error: 5.077563762664795 mm for frame 92

Lowest mean error: 4.217968940734863 mm for frame 166

Saving results

Total time: 38.72091221809387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00743832
Iteration 2/25 | Loss: 0.00171075
Iteration 3/25 | Loss: 0.00136491
Iteration 4/25 | Loss: 0.00128797
Iteration 5/25 | Loss: 0.00127404
Iteration 6/25 | Loss: 0.00126913
Iteration 7/25 | Loss: 0.00126756
Iteration 8/25 | Loss: 0.00126752
Iteration 9/25 | Loss: 0.00126752
Iteration 10/25 | Loss: 0.00126752
Iteration 11/25 | Loss: 0.00126752
Iteration 12/25 | Loss: 0.00126752
Iteration 13/25 | Loss: 0.00126752
Iteration 14/25 | Loss: 0.00126752
Iteration 15/25 | Loss: 0.00126752
Iteration 16/25 | Loss: 0.00126752
Iteration 17/25 | Loss: 0.00126752
Iteration 18/25 | Loss: 0.00126752
Iteration 19/25 | Loss: 0.00126752
Iteration 20/25 | Loss: 0.00126752
Iteration 21/25 | Loss: 0.00126752
Iteration 22/25 | Loss: 0.00126752
Iteration 23/25 | Loss: 0.00126752
Iteration 24/25 | Loss: 0.00126752
Iteration 25/25 | Loss: 0.00126752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10688913
Iteration 2/25 | Loss: 0.00083158
Iteration 3/25 | Loss: 0.00083157
Iteration 4/25 | Loss: 0.00083157
Iteration 5/25 | Loss: 0.00083157
Iteration 6/25 | Loss: 0.00083157
Iteration 7/25 | Loss: 0.00083157
Iteration 8/25 | Loss: 0.00083157
Iteration 9/25 | Loss: 0.00083157
Iteration 10/25 | Loss: 0.00083157
Iteration 11/25 | Loss: 0.00083157
Iteration 12/25 | Loss: 0.00083157
Iteration 13/25 | Loss: 0.00083157
Iteration 14/25 | Loss: 0.00083157
Iteration 15/25 | Loss: 0.00083157
Iteration 16/25 | Loss: 0.00083157
Iteration 17/25 | Loss: 0.00083157
Iteration 18/25 | Loss: 0.00083157
Iteration 19/25 | Loss: 0.00083157
Iteration 20/25 | Loss: 0.00083157
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008315709419548512, 0.0008315709419548512, 0.0008315709419548512, 0.0008315709419548512, 0.0008315709419548512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008315709419548512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083157
Iteration 2/1000 | Loss: 0.00005795
Iteration 3/1000 | Loss: 0.00004100
Iteration 4/1000 | Loss: 0.00003704
Iteration 5/1000 | Loss: 0.00003473
Iteration 6/1000 | Loss: 0.00003337
Iteration 7/1000 | Loss: 0.00003214
Iteration 8/1000 | Loss: 0.00003145
Iteration 9/1000 | Loss: 0.00003088
Iteration 10/1000 | Loss: 0.00003051
Iteration 11/1000 | Loss: 0.00003024
Iteration 12/1000 | Loss: 0.00003003
Iteration 13/1000 | Loss: 0.00002995
Iteration 14/1000 | Loss: 0.00002990
Iteration 15/1000 | Loss: 0.00002989
Iteration 16/1000 | Loss: 0.00002979
Iteration 17/1000 | Loss: 0.00002972
Iteration 18/1000 | Loss: 0.00002972
Iteration 19/1000 | Loss: 0.00002970
Iteration 20/1000 | Loss: 0.00002969
Iteration 21/1000 | Loss: 0.00002967
Iteration 22/1000 | Loss: 0.00002967
Iteration 23/1000 | Loss: 0.00002967
Iteration 24/1000 | Loss: 0.00002966
Iteration 25/1000 | Loss: 0.00002966
Iteration 26/1000 | Loss: 0.00002964
Iteration 27/1000 | Loss: 0.00002964
Iteration 28/1000 | Loss: 0.00002963
Iteration 29/1000 | Loss: 0.00002963
Iteration 30/1000 | Loss: 0.00002962
Iteration 31/1000 | Loss: 0.00002962
Iteration 32/1000 | Loss: 0.00002961
Iteration 33/1000 | Loss: 0.00002961
Iteration 34/1000 | Loss: 0.00002960
Iteration 35/1000 | Loss: 0.00002960
Iteration 36/1000 | Loss: 0.00002959
Iteration 37/1000 | Loss: 0.00002959
Iteration 38/1000 | Loss: 0.00002958
Iteration 39/1000 | Loss: 0.00002958
Iteration 40/1000 | Loss: 0.00002957
Iteration 41/1000 | Loss: 0.00002956
Iteration 42/1000 | Loss: 0.00002956
Iteration 43/1000 | Loss: 0.00002956
Iteration 44/1000 | Loss: 0.00002956
Iteration 45/1000 | Loss: 0.00002955
Iteration 46/1000 | Loss: 0.00002955
Iteration 47/1000 | Loss: 0.00002955
Iteration 48/1000 | Loss: 0.00002955
Iteration 49/1000 | Loss: 0.00002955
Iteration 50/1000 | Loss: 0.00002955
Iteration 51/1000 | Loss: 0.00002955
Iteration 52/1000 | Loss: 0.00002955
Iteration 53/1000 | Loss: 0.00002954
Iteration 54/1000 | Loss: 0.00002954
Iteration 55/1000 | Loss: 0.00002954
Iteration 56/1000 | Loss: 0.00002953
Iteration 57/1000 | Loss: 0.00002953
Iteration 58/1000 | Loss: 0.00002953
Iteration 59/1000 | Loss: 0.00002952
Iteration 60/1000 | Loss: 0.00002952
Iteration 61/1000 | Loss: 0.00002952
Iteration 62/1000 | Loss: 0.00002952
Iteration 63/1000 | Loss: 0.00002951
Iteration 64/1000 | Loss: 0.00002951
Iteration 65/1000 | Loss: 0.00002951
Iteration 66/1000 | Loss: 0.00002951
Iteration 67/1000 | Loss: 0.00002951
Iteration 68/1000 | Loss: 0.00002951
Iteration 69/1000 | Loss: 0.00002951
Iteration 70/1000 | Loss: 0.00002951
Iteration 71/1000 | Loss: 0.00002950
Iteration 72/1000 | Loss: 0.00002950
Iteration 73/1000 | Loss: 0.00002950
Iteration 74/1000 | Loss: 0.00002950
Iteration 75/1000 | Loss: 0.00002950
Iteration 76/1000 | Loss: 0.00002950
Iteration 77/1000 | Loss: 0.00002950
Iteration 78/1000 | Loss: 0.00002950
Iteration 79/1000 | Loss: 0.00002949
Iteration 80/1000 | Loss: 0.00002949
Iteration 81/1000 | Loss: 0.00002949
Iteration 82/1000 | Loss: 0.00002949
Iteration 83/1000 | Loss: 0.00002949
Iteration 84/1000 | Loss: 0.00002949
Iteration 85/1000 | Loss: 0.00002949
Iteration 86/1000 | Loss: 0.00002949
Iteration 87/1000 | Loss: 0.00002949
Iteration 88/1000 | Loss: 0.00002949
Iteration 89/1000 | Loss: 0.00002948
Iteration 90/1000 | Loss: 0.00002948
Iteration 91/1000 | Loss: 0.00002948
Iteration 92/1000 | Loss: 0.00002948
Iteration 93/1000 | Loss: 0.00002948
Iteration 94/1000 | Loss: 0.00002948
Iteration 95/1000 | Loss: 0.00002947
Iteration 96/1000 | Loss: 0.00002947
Iteration 97/1000 | Loss: 0.00002947
Iteration 98/1000 | Loss: 0.00002947
Iteration 99/1000 | Loss: 0.00002947
Iteration 100/1000 | Loss: 0.00002947
Iteration 101/1000 | Loss: 0.00002947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.9474133043549955e-05, 2.9474133043549955e-05, 2.9474133043549955e-05, 2.9474133043549955e-05, 2.9474133043549955e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9474133043549955e-05

Optimization complete. Final v2v error: 4.6594390869140625 mm

Highest mean error: 5.589517116546631 mm for frame 116

Lowest mean error: 3.816713571548462 mm for frame 77

Saving results

Total time: 42.90389847755432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_5282/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_5282/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105554
Iteration 2/25 | Loss: 0.00173414
Iteration 3/25 | Loss: 0.00128917
Iteration 4/25 | Loss: 0.00119722
Iteration 5/25 | Loss: 0.00119143
Iteration 6/25 | Loss: 0.00117712
Iteration 7/25 | Loss: 0.00115930
Iteration 8/25 | Loss: 0.00113991
Iteration 9/25 | Loss: 0.00110457
Iteration 10/25 | Loss: 0.00108792
Iteration 11/25 | Loss: 0.00106624
Iteration 12/25 | Loss: 0.00105884
Iteration 13/25 | Loss: 0.00106110
Iteration 14/25 | Loss: 0.00106389
Iteration 15/25 | Loss: 0.00105573
Iteration 16/25 | Loss: 0.00105398
Iteration 17/25 | Loss: 0.00105029
Iteration 18/25 | Loss: 0.00104054
Iteration 19/25 | Loss: 0.00103600
Iteration 20/25 | Loss: 0.00103461
Iteration 21/25 | Loss: 0.00103354
Iteration 22/25 | Loss: 0.00103141
Iteration 23/25 | Loss: 0.00102961
Iteration 24/25 | Loss: 0.00102903
Iteration 25/25 | Loss: 0.00102873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43710685
Iteration 2/25 | Loss: 0.00104472
Iteration 3/25 | Loss: 0.00104471
Iteration 4/25 | Loss: 0.00104471
Iteration 5/25 | Loss: 0.00104471
Iteration 6/25 | Loss: 0.00104471
Iteration 7/25 | Loss: 0.00104471
Iteration 8/25 | Loss: 0.00104471
Iteration 9/25 | Loss: 0.00104471
Iteration 10/25 | Loss: 0.00104471
Iteration 11/25 | Loss: 0.00104471
Iteration 12/25 | Loss: 0.00104471
Iteration 13/25 | Loss: 0.00104471
Iteration 14/25 | Loss: 0.00104471
Iteration 15/25 | Loss: 0.00104471
Iteration 16/25 | Loss: 0.00104471
Iteration 17/25 | Loss: 0.00104471
Iteration 18/25 | Loss: 0.00104471
Iteration 19/25 | Loss: 0.00104471
Iteration 20/25 | Loss: 0.00104471
Iteration 21/25 | Loss: 0.00104471
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010447067907080054, 0.0010447067907080054, 0.0010447067907080054, 0.0010447067907080054, 0.0010447067907080054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010447067907080054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104471
Iteration 2/1000 | Loss: 0.00012953
Iteration 3/1000 | Loss: 0.00009560
Iteration 4/1000 | Loss: 0.00008505
Iteration 5/1000 | Loss: 0.00007969
Iteration 6/1000 | Loss: 0.00007429
Iteration 7/1000 | Loss: 0.00067352
Iteration 8/1000 | Loss: 0.00037319
Iteration 9/1000 | Loss: 0.00035464
Iteration 10/1000 | Loss: 0.00081403
Iteration 11/1000 | Loss: 0.00025337
Iteration 12/1000 | Loss: 0.00008080
Iteration 13/1000 | Loss: 0.00007020
Iteration 14/1000 | Loss: 0.00113063
Iteration 15/1000 | Loss: 0.00111729
Iteration 16/1000 | Loss: 0.00070389
Iteration 17/1000 | Loss: 0.00008583
Iteration 18/1000 | Loss: 0.00094774
Iteration 19/1000 | Loss: 0.00053728
Iteration 20/1000 | Loss: 0.00007192
Iteration 21/1000 | Loss: 0.00006480
Iteration 22/1000 | Loss: 0.00098703
Iteration 23/1000 | Loss: 0.00111585
Iteration 24/1000 | Loss: 0.00006046
Iteration 25/1000 | Loss: 0.00005870
Iteration 26/1000 | Loss: 0.00161716
Iteration 27/1000 | Loss: 0.00115200
Iteration 28/1000 | Loss: 0.00147071
Iteration 29/1000 | Loss: 0.00132421
Iteration 30/1000 | Loss: 0.00080108
Iteration 31/1000 | Loss: 0.00107156
Iteration 32/1000 | Loss: 0.00128416
Iteration 33/1000 | Loss: 0.00065514
Iteration 34/1000 | Loss: 0.00011803
Iteration 35/1000 | Loss: 0.00053279
Iteration 36/1000 | Loss: 0.00035662
Iteration 37/1000 | Loss: 0.00112354
Iteration 38/1000 | Loss: 0.00007867
Iteration 39/1000 | Loss: 0.00005921
Iteration 40/1000 | Loss: 0.00005148
Iteration 41/1000 | Loss: 0.00004476
Iteration 42/1000 | Loss: 0.00004121
Iteration 43/1000 | Loss: 0.00003957
Iteration 44/1000 | Loss: 0.00003846
Iteration 45/1000 | Loss: 0.00003757
Iteration 46/1000 | Loss: 0.00003699
Iteration 47/1000 | Loss: 0.00003656
Iteration 48/1000 | Loss: 0.00003622
Iteration 49/1000 | Loss: 0.00003590
Iteration 50/1000 | Loss: 0.00003566
Iteration 51/1000 | Loss: 0.00003543
Iteration 52/1000 | Loss: 0.00003527
Iteration 53/1000 | Loss: 0.00003521
Iteration 54/1000 | Loss: 0.00003510
Iteration 55/1000 | Loss: 0.00003510
Iteration 56/1000 | Loss: 0.00003509
Iteration 57/1000 | Loss: 0.00003509
Iteration 58/1000 | Loss: 0.00003750
Iteration 59/1000 | Loss: 0.00003575
Iteration 60/1000 | Loss: 0.00003504
Iteration 61/1000 | Loss: 0.00003504
Iteration 62/1000 | Loss: 0.00003504
Iteration 63/1000 | Loss: 0.00003503
Iteration 64/1000 | Loss: 0.00003503
Iteration 65/1000 | Loss: 0.00003503
Iteration 66/1000 | Loss: 0.00003502
Iteration 67/1000 | Loss: 0.00003501
Iteration 68/1000 | Loss: 0.00003501
Iteration 69/1000 | Loss: 0.00003501
Iteration 70/1000 | Loss: 0.00003501
Iteration 71/1000 | Loss: 0.00003501
Iteration 72/1000 | Loss: 0.00003500
Iteration 73/1000 | Loss: 0.00003500
Iteration 74/1000 | Loss: 0.00003497
Iteration 75/1000 | Loss: 0.00003497
Iteration 76/1000 | Loss: 0.00003497
Iteration 77/1000 | Loss: 0.00003691
Iteration 78/1000 | Loss: 0.00003690
Iteration 79/1000 | Loss: 0.00003690
Iteration 80/1000 | Loss: 0.00003984
Iteration 81/1000 | Loss: 0.00003642
Iteration 82/1000 | Loss: 0.00003492
Iteration 83/1000 | Loss: 0.00003491
Iteration 84/1000 | Loss: 0.00003655
Iteration 85/1000 | Loss: 0.00003655
Iteration 86/1000 | Loss: 0.00003913
Iteration 87/1000 | Loss: 0.00003615
Iteration 88/1000 | Loss: 0.00003545
Iteration 89/1000 | Loss: 0.00003683
Iteration 90/1000 | Loss: 0.00003868
Iteration 91/1000 | Loss: 0.00003936
Iteration 92/1000 | Loss: 0.00003876
Iteration 93/1000 | Loss: 0.00003899
Iteration 94/1000 | Loss: 0.00003880
Iteration 95/1000 | Loss: 0.00003976
Iteration 96/1000 | Loss: 0.00003761
Iteration 97/1000 | Loss: 0.00003866
Iteration 98/1000 | Loss: 0.00003747
Iteration 99/1000 | Loss: 0.00003882
Iteration 100/1000 | Loss: 0.00003795
Iteration 101/1000 | Loss: 0.00003892
Iteration 102/1000 | Loss: 0.00003796
Iteration 103/1000 | Loss: 0.00003894
Iteration 104/1000 | Loss: 0.00003865
Iteration 105/1000 | Loss: 0.00003923
Iteration 106/1000 | Loss: 0.00003837
Iteration 107/1000 | Loss: 0.00003902
Iteration 108/1000 | Loss: 0.00003895
Iteration 109/1000 | Loss: 0.00004012
Iteration 110/1000 | Loss: 0.00003875
Iteration 111/1000 | Loss: 0.00003964
Iteration 112/1000 | Loss: 0.00003892
Iteration 113/1000 | Loss: 0.00004022
Iteration 114/1000 | Loss: 0.00003906
Iteration 115/1000 | Loss: 0.00003910
Iteration 116/1000 | Loss: 0.00003935
Iteration 117/1000 | Loss: 0.00004053
Iteration 118/1000 | Loss: 0.00003875
Iteration 119/1000 | Loss: 0.00003768
Iteration 120/1000 | Loss: 0.00003687
Iteration 121/1000 | Loss: 0.00003994
Iteration 122/1000 | Loss: 0.00003937
Iteration 123/1000 | Loss: 0.00004041
Iteration 124/1000 | Loss: 0.00003970
Iteration 125/1000 | Loss: 0.00004075
Iteration 126/1000 | Loss: 0.00003854
Iteration 127/1000 | Loss: 0.00003766
Iteration 128/1000 | Loss: 0.00003649
Iteration 129/1000 | Loss: 0.00004016
Iteration 130/1000 | Loss: 0.00003882
Iteration 131/1000 | Loss: 0.00004029
Iteration 132/1000 | Loss: 0.00003895
Iteration 133/1000 | Loss: 0.00003802
Iteration 134/1000 | Loss: 0.00003657
Iteration 135/1000 | Loss: 0.00004022
Iteration 136/1000 | Loss: 0.00003880
Iteration 137/1000 | Loss: 0.00004011
Iteration 138/1000 | Loss: 0.00003958
Iteration 139/1000 | Loss: 0.00003832
Iteration 140/1000 | Loss: 0.00003698
Iteration 141/1000 | Loss: 0.00003868
Iteration 142/1000 | Loss: 0.00003768
Iteration 143/1000 | Loss: 0.00004100
Iteration 144/1000 | Loss: 0.00003863
Iteration 145/1000 | Loss: 0.00003845
Iteration 146/1000 | Loss: 0.00003836
Iteration 147/1000 | Loss: 0.00003892
Iteration 148/1000 | Loss: 0.00003854
Iteration 149/1000 | Loss: 0.00003864
Iteration 150/1000 | Loss: 0.00003876
Iteration 151/1000 | Loss: 0.00003888
Iteration 152/1000 | Loss: 0.00003879
Iteration 153/1000 | Loss: 0.00003920
Iteration 154/1000 | Loss: 0.00003878
Iteration 155/1000 | Loss: 0.00004093
Iteration 156/1000 | Loss: 0.00003913
Iteration 157/1000 | Loss: 0.00004130
Iteration 158/1000 | Loss: 0.00003927
Iteration 159/1000 | Loss: 0.00003863
Iteration 160/1000 | Loss: 0.00003890
Iteration 161/1000 | Loss: 0.00004090
Iteration 162/1000 | Loss: 0.00003962
Iteration 163/1000 | Loss: 0.00004146
Iteration 164/1000 | Loss: 0.00003974
Iteration 165/1000 | Loss: 0.00004107
Iteration 166/1000 | Loss: 0.00003864
Iteration 167/1000 | Loss: 0.00004087
Iteration 168/1000 | Loss: 0.00003938
Iteration 169/1000 | Loss: 0.00004040
Iteration 170/1000 | Loss: 0.00003874
Iteration 171/1000 | Loss: 0.00004176
Iteration 172/1000 | Loss: 0.00003953
Iteration 173/1000 | Loss: 0.00004056
Iteration 174/1000 | Loss: 0.00003926
Iteration 175/1000 | Loss: 0.00004152
Iteration 176/1000 | Loss: 0.00003888
Iteration 177/1000 | Loss: 0.00004067
Iteration 178/1000 | Loss: 0.00003962
Iteration 179/1000 | Loss: 0.00004132
Iteration 180/1000 | Loss: 0.00003785
Iteration 181/1000 | Loss: 0.00003705
Iteration 182/1000 | Loss: 0.00003725
Iteration 183/1000 | Loss: 0.00003675
Iteration 184/1000 | Loss: 0.00003690
Iteration 185/1000 | Loss: 0.00003801
Iteration 186/1000 | Loss: 0.00003916
Iteration 187/1000 | Loss: 0.00003825
Iteration 188/1000 | Loss: 0.00003959
Iteration 189/1000 | Loss: 0.00003785
Iteration 190/1000 | Loss: 0.00003901
Iteration 191/1000 | Loss: 0.00003640
Iteration 192/1000 | Loss: 0.00003705
Iteration 193/1000 | Loss: 0.00003727
Iteration 194/1000 | Loss: 0.00003820
Iteration 195/1000 | Loss: 0.00003781
Iteration 196/1000 | Loss: 0.00004028
Iteration 197/1000 | Loss: 0.00003767
Iteration 198/1000 | Loss: 0.00003935
Iteration 199/1000 | Loss: 0.00003832
Iteration 200/1000 | Loss: 0.00004026
Iteration 201/1000 | Loss: 0.00003806
Iteration 202/1000 | Loss: 0.00003992
Iteration 203/1000 | Loss: 0.00003818
Iteration 204/1000 | Loss: 0.00004024
Iteration 205/1000 | Loss: 0.00003851
Iteration 206/1000 | Loss: 0.00004068
Iteration 207/1000 | Loss: 0.00003817
Iteration 208/1000 | Loss: 0.00004038
Iteration 209/1000 | Loss: 0.00003805
Iteration 210/1000 | Loss: 0.00004041
Iteration 211/1000 | Loss: 0.00003779
Iteration 212/1000 | Loss: 0.00004036
Iteration 213/1000 | Loss: 0.00003805
Iteration 214/1000 | Loss: 0.00004016
Iteration 215/1000 | Loss: 0.00003681
Iteration 216/1000 | Loss: 0.00003624
Iteration 217/1000 | Loss: 0.00003664
Iteration 218/1000 | Loss: 0.00003564
Iteration 219/1000 | Loss: 0.00003783
Iteration 220/1000 | Loss: 0.00003626
Iteration 221/1000 | Loss: 0.00003909
Iteration 222/1000 | Loss: 0.00003774
Iteration 223/1000 | Loss: 0.00003902
Iteration 224/1000 | Loss: 0.00003857
Iteration 225/1000 | Loss: 0.00003924
Iteration 226/1000 | Loss: 0.00003918
Iteration 227/1000 | Loss: 0.00003880
Iteration 228/1000 | Loss: 0.00003875
Iteration 229/1000 | Loss: 0.00003900
Iteration 230/1000 | Loss: 0.00003953
Iteration 231/1000 | Loss: 0.00003924
Iteration 232/1000 | Loss: 0.00003747
Iteration 233/1000 | Loss: 0.00003537
Iteration 234/1000 | Loss: 0.00003504
Iteration 235/1000 | Loss: 0.00003491
Iteration 236/1000 | Loss: 0.00003491
Iteration 237/1000 | Loss: 0.00003490
Iteration 238/1000 | Loss: 0.00003490
Iteration 239/1000 | Loss: 0.00003489
Iteration 240/1000 | Loss: 0.00003489
Iteration 241/1000 | Loss: 0.00003488
Iteration 242/1000 | Loss: 0.00003638
Iteration 243/1000 | Loss: 0.00003718
Iteration 244/1000 | Loss: 0.00003915
Iteration 245/1000 | Loss: 0.00003925
Iteration 246/1000 | Loss: 0.00003896
Iteration 247/1000 | Loss: 0.00003869
Iteration 248/1000 | Loss: 0.00003661
Iteration 249/1000 | Loss: 0.00003739
Iteration 250/1000 | Loss: 0.00003759
Iteration 251/1000 | Loss: 0.00003790
Iteration 252/1000 | Loss: 0.00003792
Iteration 253/1000 | Loss: 0.00003954
Iteration 254/1000 | Loss: 0.00003700
Iteration 255/1000 | Loss: 0.00003537
Iteration 256/1000 | Loss: 0.00003681
Iteration 257/1000 | Loss: 0.00003571
Iteration 258/1000 | Loss: 0.00003498
Iteration 259/1000 | Loss: 0.00003663
Iteration 260/1000 | Loss: 0.00003969
Iteration 261/1000 | Loss: 0.00003850
Iteration 262/1000 | Loss: 0.00003686
Iteration 263/1000 | Loss: 0.00003672
Iteration 264/1000 | Loss: 0.00003646
Iteration 265/1000 | Loss: 0.00003769
Iteration 266/1000 | Loss: 0.00003724
Iteration 267/1000 | Loss: 0.00003783
Iteration 268/1000 | Loss: 0.00003722
Iteration 269/1000 | Loss: 0.00003790
Iteration 270/1000 | Loss: 0.00003694
Iteration 271/1000 | Loss: 0.00003620
Iteration 272/1000 | Loss: 0.00003558
Iteration 273/1000 | Loss: 0.00003512
Iteration 274/1000 | Loss: 0.00003495
Iteration 275/1000 | Loss: 0.00003609
Iteration 276/1000 | Loss: 0.00003976
Iteration 277/1000 | Loss: 0.00003798
Iteration 278/1000 | Loss: 0.00003746
Iteration 279/1000 | Loss: 0.00003721
Iteration 280/1000 | Loss: 0.00003755
Iteration 281/1000 | Loss: 0.00003753
Iteration 282/1000 | Loss: 0.00003886
Iteration 283/1000 | Loss: 0.00003979
Iteration 284/1000 | Loss: 0.00003722
Iteration 285/1000 | Loss: 0.00003794
Iteration 286/1000 | Loss: 0.00003684
Iteration 287/1000 | Loss: 0.00003759
Iteration 288/1000 | Loss: 0.00003855
Iteration 289/1000 | Loss: 0.00004008
Iteration 290/1000 | Loss: 0.00003887
Iteration 291/1000 | Loss: 0.00003770
Iteration 292/1000 | Loss: 0.00003529
Iteration 293/1000 | Loss: 0.00003657
Iteration 294/1000 | Loss: 0.00003536
Iteration 295/1000 | Loss: 0.00003587
Iteration 296/1000 | Loss: 0.00003753
Iteration 297/1000 | Loss: 0.00003523
Iteration 298/1000 | Loss: 0.00003600
Iteration 299/1000 | Loss: 0.00003772
Iteration 300/1000 | Loss: 0.00003613
Iteration 301/1000 | Loss: 0.00003633
Iteration 302/1000 | Loss: 0.00003800
Iteration 303/1000 | Loss: 0.00003648
Iteration 304/1000 | Loss: 0.00003806
Iteration 305/1000 | Loss: 0.00003801
Iteration 306/1000 | Loss: 0.00003689
Iteration 307/1000 | Loss: 0.00003825
Iteration 308/1000 | Loss: 0.00003988
Iteration 309/1000 | Loss: 0.00003703
Iteration 310/1000 | Loss: 0.00003702
Iteration 311/1000 | Loss: 0.00004006
Iteration 312/1000 | Loss: 0.00003800
Iteration 313/1000 | Loss: 0.00003852
Iteration 314/1000 | Loss: 0.00003859
Iteration 315/1000 | Loss: 0.00003894
Iteration 316/1000 | Loss: 0.00003673
Iteration 317/1000 | Loss: 0.00003531
Iteration 318/1000 | Loss: 0.00003522
Iteration 319/1000 | Loss: 0.00003568
Iteration 320/1000 | Loss: 0.00003868
Iteration 321/1000 | Loss: 0.00003753
Iteration 322/1000 | Loss: 0.00003753
Iteration 323/1000 | Loss: 0.00003739
Iteration 324/1000 | Loss: 0.00003535
Iteration 325/1000 | Loss: 0.00003581
Iteration 326/1000 | Loss: 0.00003756
Iteration 327/1000 | Loss: 0.00003834
Iteration 328/1000 | Loss: 0.00003668
Iteration 329/1000 | Loss: 0.00003770
Iteration 330/1000 | Loss: 0.00003731
Iteration 331/1000 | Loss: 0.00003729
Iteration 332/1000 | Loss: 0.00003765
Iteration 333/1000 | Loss: 0.00003735
Iteration 334/1000 | Loss: 0.00003724
Iteration 335/1000 | Loss: 0.00003714
Iteration 336/1000 | Loss: 0.00003724
Iteration 337/1000 | Loss: 0.00003744
Iteration 338/1000 | Loss: 0.00003784
Iteration 339/1000 | Loss: 0.00003765
Iteration 340/1000 | Loss: 0.00003765
Iteration 341/1000 | Loss: 0.00003793
Iteration 342/1000 | Loss: 0.00003821
Iteration 343/1000 | Loss: 0.00003704
Iteration 344/1000 | Loss: 0.00003744
Iteration 345/1000 | Loss: 0.00003955
Iteration 346/1000 | Loss: 0.00003914
Iteration 347/1000 | Loss: 0.00003946
Iteration 348/1000 | Loss: 0.00003927
Iteration 349/1000 | Loss: 0.00003938
Iteration 350/1000 | Loss: 0.00003746
Iteration 351/1000 | Loss: 0.00003721
Iteration 352/1000 | Loss: 0.00003721
Iteration 353/1000 | Loss: 0.00003597
Iteration 354/1000 | Loss: 0.00003666
Iteration 355/1000 | Loss: 0.00003889
Iteration 356/1000 | Loss: 0.00003788
Iteration 357/1000 | Loss: 0.00003681
Iteration 358/1000 | Loss: 0.00003549
Iteration 359/1000 | Loss: 0.00003495
Iteration 360/1000 | Loss: 0.00003588
Iteration 361/1000 | Loss: 0.00003715
Iteration 362/1000 | Loss: 0.00003653
Iteration 363/1000 | Loss: 0.00003604
Iteration 364/1000 | Loss: 0.00003596
Iteration 365/1000 | Loss: 0.00003681
Iteration 366/1000 | Loss: 0.00003543
Iteration 367/1000 | Loss: 0.00003599
Iteration 368/1000 | Loss: 0.00003934
Iteration 369/1000 | Loss: 0.00003758
Iteration 370/1000 | Loss: 0.00003613
Iteration 371/1000 | Loss: 0.00003936
Iteration 372/1000 | Loss: 0.00003782
Iteration 373/1000 | Loss: 0.00003614
Iteration 374/1000 | Loss: 0.00003775
Iteration 375/1000 | Loss: 0.00003664
Iteration 376/1000 | Loss: 0.00003593
Iteration 377/1000 | Loss: 0.00003605
Iteration 378/1000 | Loss: 0.00003748
Iteration 379/1000 | Loss: 0.00003663
Iteration 380/1000 | Loss: 0.00003617
Iteration 381/1000 | Loss: 0.00003616
Iteration 382/1000 | Loss: 0.00003695
Iteration 383/1000 | Loss: 0.00003547
Iteration 384/1000 | Loss: 0.00003497
Iteration 385/1000 | Loss: 0.00003496
Iteration 386/1000 | Loss: 0.00003496
Iteration 387/1000 | Loss: 0.00003496
Iteration 388/1000 | Loss: 0.00003496
Iteration 389/1000 | Loss: 0.00003496
Iteration 390/1000 | Loss: 0.00003496
Iteration 391/1000 | Loss: 0.00003496
Iteration 392/1000 | Loss: 0.00003496
Iteration 393/1000 | Loss: 0.00003496
Iteration 394/1000 | Loss: 0.00003495
Iteration 395/1000 | Loss: 0.00003495
Iteration 396/1000 | Loss: 0.00003494
Iteration 397/1000 | Loss: 0.00003493
Iteration 398/1000 | Loss: 0.00003590
Iteration 399/1000 | Loss: 0.00003904
Iteration 400/1000 | Loss: 0.00003787
Iteration 401/1000 | Loss: 0.00003593
Iteration 402/1000 | Loss: 0.00003890
Iteration 403/1000 | Loss: 0.00003931
Iteration 404/1000 | Loss: 0.00003778
Iteration 405/1000 | Loss: 0.00003594
Iteration 406/1000 | Loss: 0.00003486
Iteration 407/1000 | Loss: 0.00003486
Iteration 408/1000 | Loss: 0.00003486
Iteration 409/1000 | Loss: 0.00003486
Iteration 410/1000 | Loss: 0.00003614
Iteration 411/1000 | Loss: 0.00003520
Iteration 412/1000 | Loss: 0.00003498
Iteration 413/1000 | Loss: 0.00003616
Iteration 414/1000 | Loss: 0.00003752
Iteration 415/1000 | Loss: 0.00003808
Iteration 416/1000 | Loss: 0.00003882
Iteration 417/1000 | Loss: 0.00003885
Iteration 418/1000 | Loss: 0.00003663
Iteration 419/1000 | Loss: 0.00003791
Iteration 420/1000 | Loss: 0.00003865
Iteration 421/1000 | Loss: 0.00003815
Iteration 422/1000 | Loss: 0.00003905
Iteration 423/1000 | Loss: 0.00003875
Iteration 424/1000 | Loss: 0.00003907
Iteration 425/1000 | Loss: 0.00003835
Iteration 426/1000 | Loss: 0.00003899
Iteration 427/1000 | Loss: 0.00003727
Iteration 428/1000 | Loss: 0.00003795
Iteration 429/1000 | Loss: 0.00003830
Iteration 430/1000 | Loss: 0.00003665
Iteration 431/1000 | Loss: 0.00003525
Iteration 432/1000 | Loss: 0.00003566
Iteration 433/1000 | Loss: 0.00003703
Iteration 434/1000 | Loss: 0.00003626
Iteration 435/1000 | Loss: 0.00003830
Iteration 436/1000 | Loss: 0.00003784
Iteration 437/1000 | Loss: 0.00003820
Iteration 438/1000 | Loss: 0.00003914
Iteration 439/1000 | Loss: 0.00003677
Iteration 440/1000 | Loss: 0.00003542
Iteration 441/1000 | Loss: 0.00003500
Iteration 442/1000 | Loss: 0.00003561
Iteration 443/1000 | Loss: 0.00003561
Iteration 444/1000 | Loss: 0.00003701
Iteration 445/1000 | Loss: 0.00003640
Iteration 446/1000 | Loss: 0.00003697
Iteration 447/1000 | Loss: 0.00003692
Iteration 448/1000 | Loss: 0.00003695
Iteration 449/1000 | Loss: 0.00003694
Iteration 450/1000 | Loss: 0.00003742
Iteration 451/1000 | Loss: 0.00003693
Iteration 452/1000 | Loss: 0.00003726
Iteration 453/1000 | Loss: 0.00003725
Iteration 454/1000 | Loss: 0.00003725
Iteration 455/1000 | Loss: 0.00003725
Iteration 456/1000 | Loss: 0.00003584
Iteration 457/1000 | Loss: 0.00003536
Iteration 458/1000 | Loss: 0.00003687
Iteration 459/1000 | Loss: 0.00003675
Iteration 460/1000 | Loss: 0.00003639
Iteration 461/1000 | Loss: 0.00003779
Iteration 462/1000 | Loss: 0.00003847
Iteration 463/1000 | Loss: 0.00003847
Iteration 464/1000 | Loss: 0.00003866
Iteration 465/1000 | Loss: 0.00003694
Iteration 466/1000 | Loss: 0.00003812
Iteration 467/1000 | Loss: 0.00003761
Iteration 468/1000 | Loss: 0.00003832
Iteration 469/1000 | Loss: 0.00003902
Iteration 470/1000 | Loss: 0.00003904
Iteration 471/1000 | Loss: 0.00003889
Iteration 472/1000 | Loss: 0.00003870
Iteration 473/1000 | Loss: 0.00003833
Iteration 474/1000 | Loss: 0.00003652
Iteration 475/1000 | Loss: 0.00003599
Iteration 476/1000 | Loss: 0.00003517
Iteration 477/1000 | Loss: 0.00003604
Iteration 478/1000 | Loss: 0.00003621
Iteration 479/1000 | Loss: 0.00003739
Iteration 480/1000 | Loss: 0.00003644
Iteration 481/1000 | Loss: 0.00003490
Iteration 482/1000 | Loss: 0.00003490
Iteration 483/1000 | Loss: 0.00003490
Iteration 484/1000 | Loss: 0.00003490
Iteration 485/1000 | Loss: 0.00003488
Iteration 486/1000 | Loss: 0.00003713
Iteration 487/1000 | Loss: 0.00003679
Iteration 488/1000 | Loss: 0.00003713
Iteration 489/1000 | Loss: 0.00003711
Iteration 490/1000 | Loss: 0.00003911
Iteration 491/1000 | Loss: 0.00003897
Iteration 492/1000 | Loss: 0.00003648
Iteration 493/1000 | Loss: 0.00003566
Iteration 494/1000 | Loss: 0.00003721
Iteration 495/1000 | Loss: 0.00003703
Iteration 496/1000 | Loss: 0.00003704
Iteration 497/1000 | Loss: 0.00003595
Iteration 498/1000 | Loss: 0.00003582
Iteration 499/1000 | Loss: 0.00003736
Iteration 500/1000 | Loss: 0.00003741
Iteration 501/1000 | Loss: 0.00003672
Iteration 502/1000 | Loss: 0.00003751
Iteration 503/1000 | Loss: 0.00003738
Iteration 504/1000 | Loss: 0.00003737
Iteration 505/1000 | Loss: 0.00003732
Iteration 506/1000 | Loss: 0.00003739
Iteration 507/1000 | Loss: 0.00003725
Iteration 508/1000 | Loss: 0.00003851
Iteration 509/1000 | Loss: 0.00003891
Iteration 510/1000 | Loss: 0.00003741
Iteration 511/1000 | Loss: 0.00003767
Iteration 512/1000 | Loss: 0.00003762
Iteration 513/1000 | Loss: 0.00003586
Iteration 514/1000 | Loss: 0.00003752
Iteration 515/1000 | Loss: 0.00003734
Iteration 516/1000 | Loss: 0.00003746
Iteration 517/1000 | Loss: 0.00003574
Iteration 518/1000 | Loss: 0.00003714
Iteration 519/1000 | Loss: 0.00003894
Iteration 520/1000 | Loss: 0.00003659
Iteration 521/1000 | Loss: 0.00003706
Iteration 522/1000 | Loss: 0.00003716
Iteration 523/1000 | Loss: 0.00003821
Iteration 524/1000 | Loss: 0.00003689
Iteration 525/1000 | Loss: 0.00003768
Iteration 526/1000 | Loss: 0.00003712
Iteration 527/1000 | Loss: 0.00003853
Iteration 528/1000 | Loss: 0.00003788
Iteration 529/1000 | Loss: 0.00003787
Iteration 530/1000 | Loss: 0.00003749
Iteration 531/1000 | Loss: 0.00003794
Iteration 532/1000 | Loss: 0.00003740
Iteration 533/1000 | Loss: 0.00003728
Iteration 534/1000 | Loss: 0.00003709
Iteration 535/1000 | Loss: 0.00003720
Iteration 536/1000 | Loss: 0.00003728
Iteration 537/1000 | Loss: 0.00003840
Iteration 538/1000 | Loss: 0.00003835
Iteration 539/1000 | Loss: 0.00003753
Iteration 540/1000 | Loss: 0.00003769
Iteration 541/1000 | Loss: 0.00003790
Iteration 542/1000 | Loss: 0.00003752
Iteration 543/1000 | Loss: 0.00003782
Iteration 544/1000 | Loss: 0.00003861
Iteration 545/1000 | Loss: 0.00003854
Iteration 546/1000 | Loss: 0.00003849
Iteration 547/1000 | Loss: 0.00003912
Iteration 548/1000 | Loss: 0.00003875
Iteration 549/1000 | Loss: 0.00003929
Iteration 550/1000 | Loss: 0.00003801
Iteration 551/1000 | Loss: 0.00003675
Iteration 552/1000 | Loss: 0.00003787
Iteration 553/1000 | Loss: 0.00003618
Iteration 554/1000 | Loss: 0.00003524
Iteration 555/1000 | Loss: 0.00003611
Iteration 556/1000 | Loss: 0.00003666
Iteration 557/1000 | Loss: 0.00003721
Iteration 558/1000 | Loss: 0.00003726
Iteration 559/1000 | Loss: 0.00003837
Iteration 560/1000 | Loss: 0.00003948
Iteration 561/1000 | Loss: 0.00003863
Iteration 562/1000 | Loss: 0.00003954
Iteration 563/1000 | Loss: 0.00003800
Iteration 564/1000 | Loss: 0.00003954
Iteration 565/1000 | Loss: 0.00003767
Iteration 566/1000 | Loss: 0.00003614
Iteration 567/1000 | Loss: 0.00003508
Iteration 568/1000 | Loss: 0.00003491
Iteration 569/1000 | Loss: 0.00003490
Iteration 570/1000 | Loss: 0.00003490
Iteration 571/1000 | Loss: 0.00003490
Iteration 572/1000 | Loss: 0.00003489
Iteration 573/1000 | Loss: 0.00003606
Iteration 574/1000 | Loss: 0.00003808
Iteration 575/1000 | Loss: 0.00003713
Iteration 576/1000 | Loss: 0.00003735
Iteration 577/1000 | Loss: 0.00003799
Iteration 578/1000 | Loss: 0.00003958
Iteration 579/1000 | Loss: 0.00003568
Iteration 580/1000 | Loss: 0.00003595
Iteration 581/1000 | Loss: 0.00003838
Iteration 582/1000 | Loss: 0.00003920
Iteration 583/1000 | Loss: 0.00003886
Iteration 584/1000 | Loss: 0.00003886
Iteration 585/1000 | Loss: 0.00003815
Iteration 586/1000 | Loss: 0.00003709
Iteration 587/1000 | Loss: 0.00003791
Iteration 588/1000 | Loss: 0.00003824
Iteration 589/1000 | Loss: 0.00003635
Iteration 590/1000 | Loss: 0.00003667
Iteration 591/1000 | Loss: 0.00003536
Iteration 592/1000 | Loss: 0.00003571
Iteration 593/1000 | Loss: 0.00003570
Iteration 594/1000 | Loss: 0.00003570
Iteration 595/1000 | Loss: 0.00003639
Iteration 596/1000 | Loss: 0.00003811
Iteration 597/1000 | Loss: 0.00003716
Iteration 598/1000 | Loss: 0.00003814
Iteration 599/1000 | Loss: 0.00003800
Iteration 600/1000 | Loss: 0.00003737
Iteration 601/1000 | Loss: 0.00003696
Iteration 602/1000 | Loss: 0.00003716
Iteration 603/1000 | Loss: 0.00003678
Iteration 604/1000 | Loss: 0.00003678
Iteration 605/1000 | Loss: 0.00003808
Iteration 606/1000 | Loss: 0.00003742
Iteration 607/1000 | Loss: 0.00003701
Iteration 608/1000 | Loss: 0.00003679
Iteration 609/1000 | Loss: 0.00003640
Iteration 610/1000 | Loss: 0.00003501
Iteration 611/1000 | Loss: 0.00003493
Iteration 612/1000 | Loss: 0.00003640
Iteration 613/1000 | Loss: 0.00003639
Iteration 614/1000 | Loss: 0.00003672
Iteration 615/1000 | Loss: 0.00003802
Iteration 616/1000 | Loss: 0.00003588
Iteration 617/1000 | Loss: 0.00003731
Iteration 618/1000 | Loss: 0.00003733
Iteration 619/1000 | Loss: 0.00003749
Iteration 620/1000 | Loss: 0.00003748
Iteration 621/1000 | Loss: 0.00003684
Iteration 622/1000 | Loss: 0.00003699
Iteration 623/1000 | Loss: 0.00003859
Iteration 624/1000 | Loss: 0.00003888
Iteration 625/1000 | Loss: 0.00003901
Iteration 626/1000 | Loss: 0.00003750
Iteration 627/1000 | Loss: 0.00003701
Iteration 628/1000 | Loss: 0.00003947
Iteration 629/1000 | Loss: 0.00003723
Iteration 630/1000 | Loss: 0.00003754
Iteration 631/1000 | Loss: 0.00003841
Iteration 632/1000 | Loss: 0.00003755
Iteration 633/1000 | Loss: 0.00003814
Iteration 634/1000 | Loss: 0.00003756
Iteration 635/1000 | Loss: 0.00003986
Iteration 636/1000 | Loss: 0.00003953
Iteration 637/1000 | Loss: 0.00003869
Iteration 638/1000 | Loss: 0.00003750
Iteration 639/1000 | Loss: 0.00003853
Iteration 640/1000 | Loss: 0.00003784
Iteration 641/1000 | Loss: 0.00003939
Iteration 642/1000 | Loss: 0.00003909
Iteration 643/1000 | Loss: 0.00003947
Iteration 644/1000 | Loss: 0.00003667
Iteration 645/1000 | Loss: 0.00003538
Iteration 646/1000 | Loss: 0.00003762
Iteration 647/1000 | Loss: 0.00003955
Iteration 648/1000 | Loss: 0.00003968
Iteration 649/1000 | Loss: 0.00004053
Iteration 650/1000 | Loss: 0.00003903
Iteration 651/1000 | Loss: 0.00003995
Iteration 652/1000 | Loss: 0.00003887
Iteration 653/1000 | Loss: 0.00004006
Iteration 654/1000 | Loss: 0.00003898
Iteration 655/1000 | Loss: 0.00003715
Iteration 656/1000 | Loss: 0.00003791
Iteration 657/1000 | Loss: 0.00003670
Iteration 658/1000 | Loss: 0.00003741
Iteration 659/1000 | Loss: 0.00003729
Iteration 660/1000 | Loss: 0.00003824
Iteration 661/1000 | Loss: 0.00003569
Iteration 662/1000 | Loss: 0.00003695
Iteration 663/1000 | Loss: 0.00003769
Iteration 664/1000 | Loss: 0.00003698
Iteration 665/1000 | Loss: 0.00003798
Iteration 666/1000 | Loss: 0.00003622
Iteration 667/1000 | Loss: 0.00003652
Iteration 668/1000 | Loss: 0.00003593
Iteration 669/1000 | Loss: 0.00003734
Iteration 670/1000 | Loss: 0.00003806
Iteration 671/1000 | Loss: 0.00003914
Iteration 672/1000 | Loss: 0.00003849
Iteration 673/1000 | Loss: 0.00003905
Iteration 674/1000 | Loss: 0.00003907
Iteration 675/1000 | Loss: 0.00003910
Iteration 676/1000 | Loss: 0.00003884
Iteration 677/1000 | Loss: 0.00003896
Iteration 678/1000 | Loss: 0.00003927
Iteration 679/1000 | Loss: 0.00003714
Iteration 680/1000 | Loss: 0.00003551
Iteration 681/1000 | Loss: 0.00003645
Iteration 682/1000 | Loss: 0.00003869
Iteration 683/1000 | Loss: 0.00003835
Iteration 684/1000 | Loss: 0.00003892
Iteration 685/1000 | Loss: 0.00003646
Iteration 686/1000 | Loss: 0.00003655
Iteration 687/1000 | Loss: 0.00003686
Iteration 688/1000 | Loss: 0.00003894
Iteration 689/1000 | Loss: 0.00003755
Iteration 690/1000 | Loss: 0.00003807
Iteration 691/1000 | Loss: 0.00003589
Iteration 692/1000 | Loss: 0.00003653
Iteration 693/1000 | Loss: 0.00003668
Iteration 694/1000 | Loss: 0.00003669
Iteration 695/1000 | Loss: 0.00003733
Iteration 696/1000 | Loss: 0.00003775
Iteration 697/1000 | Loss: 0.00003703
Iteration 698/1000 | Loss: 0.00003681
Iteration 699/1000 | Loss: 0.00003838
Iteration 700/1000 | Loss: 0.00003786
Iteration 701/1000 | Loss: 0.00003792
Iteration 702/1000 | Loss: 0.00003771
Iteration 703/1000 | Loss: 0.00003792
Iteration 704/1000 | Loss: 0.00003782
Iteration 705/1000 | Loss: 0.00003791
Iteration 706/1000 | Loss: 0.00003597
Iteration 707/1000 | Loss: 0.00003598
Iteration 708/1000 | Loss: 0.00003702
Iteration 709/1000 | Loss: 0.00003810
Iteration 710/1000 | Loss: 0.00003767
Iteration 711/1000 | Loss: 0.00003605
Iteration 712/1000 | Loss: 0.00003623
Iteration 713/1000 | Loss: 0.00003703
Iteration 714/1000 | Loss: 0.00003812
Iteration 715/1000 | Loss: 0.00003776
Iteration 716/1000 | Loss: 0.00003604
Iteration 717/1000 | Loss: 0.00003813
Iteration 718/1000 | Loss: 0.00003887
Iteration 719/1000 | Loss: 0.00003547
Iteration 720/1000 | Loss: 0.00003539
Iteration 721/1000 | Loss: 0.00003539
Iteration 722/1000 | Loss: 0.00003505
Iteration 723/1000 | Loss: 0.00003500
Iteration 724/1000 | Loss: 0.00003500
Iteration 725/1000 | Loss: 0.00003500
Iteration 726/1000 | Loss: 0.00003500
Iteration 727/1000 | Loss: 0.00003500
Iteration 728/1000 | Loss: 0.00003500
Iteration 729/1000 | Loss: 0.00003499
Iteration 730/1000 | Loss: 0.00003499
Iteration 731/1000 | Loss: 0.00003499
Iteration 732/1000 | Loss: 0.00003499
Iteration 733/1000 | Loss: 0.00003499
Iteration 734/1000 | Loss: 0.00003499
Iteration 735/1000 | Loss: 0.00003499
Iteration 736/1000 | Loss: 0.00003499
Iteration 737/1000 | Loss: 0.00003499
Iteration 738/1000 | Loss: 0.00003499
Iteration 739/1000 | Loss: 0.00003499
Iteration 740/1000 | Loss: 0.00003499
Iteration 741/1000 | Loss: 0.00003499
Iteration 742/1000 | Loss: 0.00003499
Iteration 743/1000 | Loss: 0.00003499
Iteration 744/1000 | Loss: 0.00003499
Iteration 745/1000 | Loss: 0.00003499
Iteration 746/1000 | Loss: 0.00003499
Iteration 747/1000 | Loss: 0.00003499
Iteration 748/1000 | Loss: 0.00003499
Iteration 749/1000 | Loss: 0.00003499
Iteration 750/1000 | Loss: 0.00003499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 750. Stopping optimization.
Last 5 losses: [3.499176091281697e-05, 3.499176091281697e-05, 3.499176091281697e-05, 3.499176091281697e-05, 3.499176091281697e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.499176091281697e-05

Optimization complete. Final v2v error: 4.477094650268555 mm

Highest mean error: 12.341078758239746 mm for frame 10

Lowest mean error: 3.7646398544311523 mm for frame 55

Saving results

Total time: 883.6462569236755
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819019
Iteration 2/25 | Loss: 0.00133223
Iteration 3/25 | Loss: 0.00122643
Iteration 4/25 | Loss: 0.00121390
Iteration 5/25 | Loss: 0.00121080
Iteration 6/25 | Loss: 0.00121025
Iteration 7/25 | Loss: 0.00121025
Iteration 8/25 | Loss: 0.00121025
Iteration 9/25 | Loss: 0.00121025
Iteration 10/25 | Loss: 0.00121025
Iteration 11/25 | Loss: 0.00121025
Iteration 12/25 | Loss: 0.00121025
Iteration 13/25 | Loss: 0.00121025
Iteration 14/25 | Loss: 0.00121025
Iteration 15/25 | Loss: 0.00121025
Iteration 16/25 | Loss: 0.00121025
Iteration 17/25 | Loss: 0.00121025
Iteration 18/25 | Loss: 0.00121025
Iteration 19/25 | Loss: 0.00121025
Iteration 20/25 | Loss: 0.00121025
Iteration 21/25 | Loss: 0.00121025
Iteration 22/25 | Loss: 0.00121025
Iteration 23/25 | Loss: 0.00121025
Iteration 24/25 | Loss: 0.00121025
Iteration 25/25 | Loss: 0.00121025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32404637
Iteration 2/25 | Loss: 0.00158152
Iteration 3/25 | Loss: 0.00158151
Iteration 4/25 | Loss: 0.00158151
Iteration 5/25 | Loss: 0.00158151
Iteration 6/25 | Loss: 0.00158151
Iteration 7/25 | Loss: 0.00158151
Iteration 8/25 | Loss: 0.00158151
Iteration 9/25 | Loss: 0.00158151
Iteration 10/25 | Loss: 0.00158151
Iteration 11/25 | Loss: 0.00158151
Iteration 12/25 | Loss: 0.00158151
Iteration 13/25 | Loss: 0.00158151
Iteration 14/25 | Loss: 0.00158151
Iteration 15/25 | Loss: 0.00158151
Iteration 16/25 | Loss: 0.00158151
Iteration 17/25 | Loss: 0.00158151
Iteration 18/25 | Loss: 0.00158151
Iteration 19/25 | Loss: 0.00158151
Iteration 20/25 | Loss: 0.00158151
Iteration 21/25 | Loss: 0.00158151
Iteration 22/25 | Loss: 0.00158151
Iteration 23/25 | Loss: 0.00158151
Iteration 24/25 | Loss: 0.00158151
Iteration 25/25 | Loss: 0.00158151

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158151
Iteration 2/1000 | Loss: 0.00002777
Iteration 3/1000 | Loss: 0.00001800
Iteration 4/1000 | Loss: 0.00001482
Iteration 5/1000 | Loss: 0.00001322
Iteration 6/1000 | Loss: 0.00001252
Iteration 7/1000 | Loss: 0.00001181
Iteration 8/1000 | Loss: 0.00001139
Iteration 9/1000 | Loss: 0.00001111
Iteration 10/1000 | Loss: 0.00001073
Iteration 11/1000 | Loss: 0.00001041
Iteration 12/1000 | Loss: 0.00001033
Iteration 13/1000 | Loss: 0.00001031
Iteration 14/1000 | Loss: 0.00001029
Iteration 15/1000 | Loss: 0.00001020
Iteration 16/1000 | Loss: 0.00001017
Iteration 17/1000 | Loss: 0.00001015
Iteration 18/1000 | Loss: 0.00001011
Iteration 19/1000 | Loss: 0.00001006
Iteration 20/1000 | Loss: 0.00001005
Iteration 21/1000 | Loss: 0.00001004
Iteration 22/1000 | Loss: 0.00001004
Iteration 23/1000 | Loss: 0.00001002
Iteration 24/1000 | Loss: 0.00001001
Iteration 25/1000 | Loss: 0.00001001
Iteration 26/1000 | Loss: 0.00001001
Iteration 27/1000 | Loss: 0.00001000
Iteration 28/1000 | Loss: 0.00001000
Iteration 29/1000 | Loss: 0.00000999
Iteration 30/1000 | Loss: 0.00000999
Iteration 31/1000 | Loss: 0.00000999
Iteration 32/1000 | Loss: 0.00000998
Iteration 33/1000 | Loss: 0.00000996
Iteration 34/1000 | Loss: 0.00000995
Iteration 35/1000 | Loss: 0.00000995
Iteration 36/1000 | Loss: 0.00000994
Iteration 37/1000 | Loss: 0.00000994
Iteration 38/1000 | Loss: 0.00000993
Iteration 39/1000 | Loss: 0.00000993
Iteration 40/1000 | Loss: 0.00000991
Iteration 41/1000 | Loss: 0.00000988
Iteration 42/1000 | Loss: 0.00000985
Iteration 43/1000 | Loss: 0.00000979
Iteration 44/1000 | Loss: 0.00000978
Iteration 45/1000 | Loss: 0.00000975
Iteration 46/1000 | Loss: 0.00000975
Iteration 47/1000 | Loss: 0.00000975
Iteration 48/1000 | Loss: 0.00000975
Iteration 49/1000 | Loss: 0.00000975
Iteration 50/1000 | Loss: 0.00000975
Iteration 51/1000 | Loss: 0.00000975
Iteration 52/1000 | Loss: 0.00000975
Iteration 53/1000 | Loss: 0.00000974
Iteration 54/1000 | Loss: 0.00000973
Iteration 55/1000 | Loss: 0.00000973
Iteration 56/1000 | Loss: 0.00000971
Iteration 57/1000 | Loss: 0.00000971
Iteration 58/1000 | Loss: 0.00000970
Iteration 59/1000 | Loss: 0.00000970
Iteration 60/1000 | Loss: 0.00000969
Iteration 61/1000 | Loss: 0.00000969
Iteration 62/1000 | Loss: 0.00000969
Iteration 63/1000 | Loss: 0.00000968
Iteration 64/1000 | Loss: 0.00000967
Iteration 65/1000 | Loss: 0.00000967
Iteration 66/1000 | Loss: 0.00000967
Iteration 67/1000 | Loss: 0.00000967
Iteration 68/1000 | Loss: 0.00000966
Iteration 69/1000 | Loss: 0.00000966
Iteration 70/1000 | Loss: 0.00000966
Iteration 71/1000 | Loss: 0.00000966
Iteration 72/1000 | Loss: 0.00000966
Iteration 73/1000 | Loss: 0.00000966
Iteration 74/1000 | Loss: 0.00000966
Iteration 75/1000 | Loss: 0.00000966
Iteration 76/1000 | Loss: 0.00000966
Iteration 77/1000 | Loss: 0.00000965
Iteration 78/1000 | Loss: 0.00000965
Iteration 79/1000 | Loss: 0.00000965
Iteration 80/1000 | Loss: 0.00000965
Iteration 81/1000 | Loss: 0.00000965
Iteration 82/1000 | Loss: 0.00000965
Iteration 83/1000 | Loss: 0.00000964
Iteration 84/1000 | Loss: 0.00000964
Iteration 85/1000 | Loss: 0.00000963
Iteration 86/1000 | Loss: 0.00000963
Iteration 87/1000 | Loss: 0.00000962
Iteration 88/1000 | Loss: 0.00000962
Iteration 89/1000 | Loss: 0.00000962
Iteration 90/1000 | Loss: 0.00000961
Iteration 91/1000 | Loss: 0.00000960
Iteration 92/1000 | Loss: 0.00000960
Iteration 93/1000 | Loss: 0.00000959
Iteration 94/1000 | Loss: 0.00000959
Iteration 95/1000 | Loss: 0.00000959
Iteration 96/1000 | Loss: 0.00000959
Iteration 97/1000 | Loss: 0.00000959
Iteration 98/1000 | Loss: 0.00000959
Iteration 99/1000 | Loss: 0.00000959
Iteration 100/1000 | Loss: 0.00000959
Iteration 101/1000 | Loss: 0.00000959
Iteration 102/1000 | Loss: 0.00000959
Iteration 103/1000 | Loss: 0.00000958
Iteration 104/1000 | Loss: 0.00000958
Iteration 105/1000 | Loss: 0.00000958
Iteration 106/1000 | Loss: 0.00000957
Iteration 107/1000 | Loss: 0.00000957
Iteration 108/1000 | Loss: 0.00000957
Iteration 109/1000 | Loss: 0.00000957
Iteration 110/1000 | Loss: 0.00000957
Iteration 111/1000 | Loss: 0.00000957
Iteration 112/1000 | Loss: 0.00000956
Iteration 113/1000 | Loss: 0.00000956
Iteration 114/1000 | Loss: 0.00000956
Iteration 115/1000 | Loss: 0.00000955
Iteration 116/1000 | Loss: 0.00000955
Iteration 117/1000 | Loss: 0.00000955
Iteration 118/1000 | Loss: 0.00000955
Iteration 119/1000 | Loss: 0.00000954
Iteration 120/1000 | Loss: 0.00000954
Iteration 121/1000 | Loss: 0.00000954
Iteration 122/1000 | Loss: 0.00000954
Iteration 123/1000 | Loss: 0.00000954
Iteration 124/1000 | Loss: 0.00000953
Iteration 125/1000 | Loss: 0.00000953
Iteration 126/1000 | Loss: 0.00000953
Iteration 127/1000 | Loss: 0.00000952
Iteration 128/1000 | Loss: 0.00000952
Iteration 129/1000 | Loss: 0.00000952
Iteration 130/1000 | Loss: 0.00000951
Iteration 131/1000 | Loss: 0.00000951
Iteration 132/1000 | Loss: 0.00000951
Iteration 133/1000 | Loss: 0.00000950
Iteration 134/1000 | Loss: 0.00000950
Iteration 135/1000 | Loss: 0.00000949
Iteration 136/1000 | Loss: 0.00000949
Iteration 137/1000 | Loss: 0.00000948
Iteration 138/1000 | Loss: 0.00000947
Iteration 139/1000 | Loss: 0.00000947
Iteration 140/1000 | Loss: 0.00000947
Iteration 141/1000 | Loss: 0.00000947
Iteration 142/1000 | Loss: 0.00000946
Iteration 143/1000 | Loss: 0.00000946
Iteration 144/1000 | Loss: 0.00000946
Iteration 145/1000 | Loss: 0.00000946
Iteration 146/1000 | Loss: 0.00000945
Iteration 147/1000 | Loss: 0.00000944
Iteration 148/1000 | Loss: 0.00000944
Iteration 149/1000 | Loss: 0.00000944
Iteration 150/1000 | Loss: 0.00000944
Iteration 151/1000 | Loss: 0.00000944
Iteration 152/1000 | Loss: 0.00000944
Iteration 153/1000 | Loss: 0.00000943
Iteration 154/1000 | Loss: 0.00000943
Iteration 155/1000 | Loss: 0.00000943
Iteration 156/1000 | Loss: 0.00000943
Iteration 157/1000 | Loss: 0.00000943
Iteration 158/1000 | Loss: 0.00000943
Iteration 159/1000 | Loss: 0.00000943
Iteration 160/1000 | Loss: 0.00000943
Iteration 161/1000 | Loss: 0.00000943
Iteration 162/1000 | Loss: 0.00000943
Iteration 163/1000 | Loss: 0.00000943
Iteration 164/1000 | Loss: 0.00000943
Iteration 165/1000 | Loss: 0.00000943
Iteration 166/1000 | Loss: 0.00000943
Iteration 167/1000 | Loss: 0.00000943
Iteration 168/1000 | Loss: 0.00000943
Iteration 169/1000 | Loss: 0.00000942
Iteration 170/1000 | Loss: 0.00000942
Iteration 171/1000 | Loss: 0.00000942
Iteration 172/1000 | Loss: 0.00000942
Iteration 173/1000 | Loss: 0.00000942
Iteration 174/1000 | Loss: 0.00000942
Iteration 175/1000 | Loss: 0.00000941
Iteration 176/1000 | Loss: 0.00000941
Iteration 177/1000 | Loss: 0.00000941
Iteration 178/1000 | Loss: 0.00000941
Iteration 179/1000 | Loss: 0.00000941
Iteration 180/1000 | Loss: 0.00000940
Iteration 181/1000 | Loss: 0.00000940
Iteration 182/1000 | Loss: 0.00000940
Iteration 183/1000 | Loss: 0.00000940
Iteration 184/1000 | Loss: 0.00000940
Iteration 185/1000 | Loss: 0.00000940
Iteration 186/1000 | Loss: 0.00000940
Iteration 187/1000 | Loss: 0.00000940
Iteration 188/1000 | Loss: 0.00000940
Iteration 189/1000 | Loss: 0.00000940
Iteration 190/1000 | Loss: 0.00000939
Iteration 191/1000 | Loss: 0.00000939
Iteration 192/1000 | Loss: 0.00000939
Iteration 193/1000 | Loss: 0.00000938
Iteration 194/1000 | Loss: 0.00000938
Iteration 195/1000 | Loss: 0.00000938
Iteration 196/1000 | Loss: 0.00000938
Iteration 197/1000 | Loss: 0.00000938
Iteration 198/1000 | Loss: 0.00000938
Iteration 199/1000 | Loss: 0.00000938
Iteration 200/1000 | Loss: 0.00000938
Iteration 201/1000 | Loss: 0.00000938
Iteration 202/1000 | Loss: 0.00000938
Iteration 203/1000 | Loss: 0.00000938
Iteration 204/1000 | Loss: 0.00000938
Iteration 205/1000 | Loss: 0.00000938
Iteration 206/1000 | Loss: 0.00000938
Iteration 207/1000 | Loss: 0.00000938
Iteration 208/1000 | Loss: 0.00000938
Iteration 209/1000 | Loss: 0.00000938
Iteration 210/1000 | Loss: 0.00000938
Iteration 211/1000 | Loss: 0.00000938
Iteration 212/1000 | Loss: 0.00000938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [9.377712558489293e-06, 9.377712558489293e-06, 9.377712558489293e-06, 9.377712558489293e-06, 9.377712558489293e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.377712558489293e-06

Optimization complete. Final v2v error: 2.6092400550842285 mm

Highest mean error: 3.8148372173309326 mm for frame 91

Lowest mean error: 2.334977626800537 mm for frame 12

Saving results

Total time: 49.901708364486694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00766952
Iteration 2/25 | Loss: 0.00189075
Iteration 3/25 | Loss: 0.00151336
Iteration 4/25 | Loss: 0.00140045
Iteration 5/25 | Loss: 0.00134884
Iteration 6/25 | Loss: 0.00132619
Iteration 7/25 | Loss: 0.00131612
Iteration 8/25 | Loss: 0.00131389
Iteration 9/25 | Loss: 0.00131255
Iteration 10/25 | Loss: 0.00131198
Iteration 11/25 | Loss: 0.00131144
Iteration 12/25 | Loss: 0.00130723
Iteration 13/25 | Loss: 0.00130641
Iteration 14/25 | Loss: 0.00130577
Iteration 15/25 | Loss: 0.00130548
Iteration 16/25 | Loss: 0.00130568
Iteration 17/25 | Loss: 0.00130486
Iteration 18/25 | Loss: 0.00130260
Iteration 19/25 | Loss: 0.00130226
Iteration 20/25 | Loss: 0.00130222
Iteration 21/25 | Loss: 0.00130221
Iteration 22/25 | Loss: 0.00130221
Iteration 23/25 | Loss: 0.00130221
Iteration 24/25 | Loss: 0.00130221
Iteration 25/25 | Loss: 0.00130221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26047742
Iteration 2/25 | Loss: 0.00115042
Iteration 3/25 | Loss: 0.00115042
Iteration 4/25 | Loss: 0.00115041
Iteration 5/25 | Loss: 0.00115041
Iteration 6/25 | Loss: 0.00115041
Iteration 7/25 | Loss: 0.00115041
Iteration 8/25 | Loss: 0.00115041
Iteration 9/25 | Loss: 0.00115041
Iteration 10/25 | Loss: 0.00115041
Iteration 11/25 | Loss: 0.00115041
Iteration 12/25 | Loss: 0.00115041
Iteration 13/25 | Loss: 0.00115041
Iteration 14/25 | Loss: 0.00115041
Iteration 15/25 | Loss: 0.00115041
Iteration 16/25 | Loss: 0.00115041
Iteration 17/25 | Loss: 0.00115041
Iteration 18/25 | Loss: 0.00115041
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011504126014187932, 0.0011504126014187932, 0.0011504126014187932, 0.0011504126014187932, 0.0011504126014187932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011504126014187932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115041
Iteration 2/1000 | Loss: 0.00003541
Iteration 3/1000 | Loss: 0.00002437
Iteration 4/1000 | Loss: 0.00002177
Iteration 5/1000 | Loss: 0.00002062
Iteration 6/1000 | Loss: 0.00001984
Iteration 7/1000 | Loss: 0.00001946
Iteration 8/1000 | Loss: 0.00001908
Iteration 9/1000 | Loss: 0.00001863
Iteration 10/1000 | Loss: 0.00001856
Iteration 11/1000 | Loss: 0.00001834
Iteration 12/1000 | Loss: 0.00001812
Iteration 13/1000 | Loss: 0.00001797
Iteration 14/1000 | Loss: 0.00001796
Iteration 15/1000 | Loss: 0.00001784
Iteration 16/1000 | Loss: 0.00001775
Iteration 17/1000 | Loss: 0.00001774
Iteration 18/1000 | Loss: 0.00001774
Iteration 19/1000 | Loss: 0.00001773
Iteration 20/1000 | Loss: 0.00001772
Iteration 21/1000 | Loss: 0.00001770
Iteration 22/1000 | Loss: 0.00001770
Iteration 23/1000 | Loss: 0.00001770
Iteration 24/1000 | Loss: 0.00001769
Iteration 25/1000 | Loss: 0.00001769
Iteration 26/1000 | Loss: 0.00001769
Iteration 27/1000 | Loss: 0.00001768
Iteration 28/1000 | Loss: 0.00001763
Iteration 29/1000 | Loss: 0.00001763
Iteration 30/1000 | Loss: 0.00001763
Iteration 31/1000 | Loss: 0.00001763
Iteration 32/1000 | Loss: 0.00001763
Iteration 33/1000 | Loss: 0.00001763
Iteration 34/1000 | Loss: 0.00001763
Iteration 35/1000 | Loss: 0.00001762
Iteration 36/1000 | Loss: 0.00001762
Iteration 37/1000 | Loss: 0.00001762
Iteration 38/1000 | Loss: 0.00001762
Iteration 39/1000 | Loss: 0.00001762
Iteration 40/1000 | Loss: 0.00001762
Iteration 41/1000 | Loss: 0.00001762
Iteration 42/1000 | Loss: 0.00001760
Iteration 43/1000 | Loss: 0.00001759
Iteration 44/1000 | Loss: 0.00001756
Iteration 45/1000 | Loss: 0.00001755
Iteration 46/1000 | Loss: 0.00001754
Iteration 47/1000 | Loss: 0.00001754
Iteration 48/1000 | Loss: 0.00001754
Iteration 49/1000 | Loss: 0.00001754
Iteration 50/1000 | Loss: 0.00001754
Iteration 51/1000 | Loss: 0.00001753
Iteration 52/1000 | Loss: 0.00001753
Iteration 53/1000 | Loss: 0.00001753
Iteration 54/1000 | Loss: 0.00001752
Iteration 55/1000 | Loss: 0.00001752
Iteration 56/1000 | Loss: 0.00001752
Iteration 57/1000 | Loss: 0.00001751
Iteration 58/1000 | Loss: 0.00001751
Iteration 59/1000 | Loss: 0.00001749
Iteration 60/1000 | Loss: 0.00001749
Iteration 61/1000 | Loss: 0.00001748
Iteration 62/1000 | Loss: 0.00001748
Iteration 63/1000 | Loss: 0.00001748
Iteration 64/1000 | Loss: 0.00001747
Iteration 65/1000 | Loss: 0.00001747
Iteration 66/1000 | Loss: 0.00001746
Iteration 67/1000 | Loss: 0.00001746
Iteration 68/1000 | Loss: 0.00001745
Iteration 69/1000 | Loss: 0.00001744
Iteration 70/1000 | Loss: 0.00001744
Iteration 71/1000 | Loss: 0.00001744
Iteration 72/1000 | Loss: 0.00001743
Iteration 73/1000 | Loss: 0.00001743
Iteration 74/1000 | Loss: 0.00001742
Iteration 75/1000 | Loss: 0.00001742
Iteration 76/1000 | Loss: 0.00001741
Iteration 77/1000 | Loss: 0.00001741
Iteration 78/1000 | Loss: 0.00001741
Iteration 79/1000 | Loss: 0.00001741
Iteration 80/1000 | Loss: 0.00001741
Iteration 81/1000 | Loss: 0.00001741
Iteration 82/1000 | Loss: 0.00001741
Iteration 83/1000 | Loss: 0.00001741
Iteration 84/1000 | Loss: 0.00001741
Iteration 85/1000 | Loss: 0.00001741
Iteration 86/1000 | Loss: 0.00001741
Iteration 87/1000 | Loss: 0.00001741
Iteration 88/1000 | Loss: 0.00001741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.740628431434743e-05, 1.740628431434743e-05, 1.740628431434743e-05, 1.740628431434743e-05, 1.740628431434743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.740628431434743e-05

Optimization complete. Final v2v error: 3.562228202819824 mm

Highest mean error: 3.943293809890747 mm for frame 69

Lowest mean error: 3.2265145778656006 mm for frame 8

Saving results

Total time: 67.16071081161499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418605
Iteration 2/25 | Loss: 0.00133487
Iteration 3/25 | Loss: 0.00125602
Iteration 4/25 | Loss: 0.00124470
Iteration 5/25 | Loss: 0.00124155
Iteration 6/25 | Loss: 0.00124155
Iteration 7/25 | Loss: 0.00124155
Iteration 8/25 | Loss: 0.00124155
Iteration 9/25 | Loss: 0.00124155
Iteration 10/25 | Loss: 0.00124155
Iteration 11/25 | Loss: 0.00124155
Iteration 12/25 | Loss: 0.00124155
Iteration 13/25 | Loss: 0.00124155
Iteration 14/25 | Loss: 0.00124155
Iteration 15/25 | Loss: 0.00124155
Iteration 16/25 | Loss: 0.00124155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012415535748004913, 0.0012415535748004913, 0.0012415535748004913, 0.0012415535748004913, 0.0012415535748004913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012415535748004913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32581389
Iteration 2/25 | Loss: 0.00167584
Iteration 3/25 | Loss: 0.00167584
Iteration 4/25 | Loss: 0.00167583
Iteration 5/25 | Loss: 0.00167583
Iteration 6/25 | Loss: 0.00167583
Iteration 7/25 | Loss: 0.00167583
Iteration 8/25 | Loss: 0.00167583
Iteration 9/25 | Loss: 0.00167583
Iteration 10/25 | Loss: 0.00167583
Iteration 11/25 | Loss: 0.00167583
Iteration 12/25 | Loss: 0.00167583
Iteration 13/25 | Loss: 0.00167583
Iteration 14/25 | Loss: 0.00167583
Iteration 15/25 | Loss: 0.00167583
Iteration 16/25 | Loss: 0.00167583
Iteration 17/25 | Loss: 0.00167583
Iteration 18/25 | Loss: 0.00167583
Iteration 19/25 | Loss: 0.00167583
Iteration 20/25 | Loss: 0.00167583
Iteration 21/25 | Loss: 0.00167583
Iteration 22/25 | Loss: 0.00167583
Iteration 23/25 | Loss: 0.00167583
Iteration 24/25 | Loss: 0.00167583
Iteration 25/25 | Loss: 0.00167583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167583
Iteration 2/1000 | Loss: 0.00002518
Iteration 3/1000 | Loss: 0.00001827
Iteration 4/1000 | Loss: 0.00001543
Iteration 5/1000 | Loss: 0.00001459
Iteration 6/1000 | Loss: 0.00001415
Iteration 7/1000 | Loss: 0.00001374
Iteration 8/1000 | Loss: 0.00001344
Iteration 9/1000 | Loss: 0.00001308
Iteration 10/1000 | Loss: 0.00001279
Iteration 11/1000 | Loss: 0.00001261
Iteration 12/1000 | Loss: 0.00001249
Iteration 13/1000 | Loss: 0.00001248
Iteration 14/1000 | Loss: 0.00001247
Iteration 15/1000 | Loss: 0.00001247
Iteration 16/1000 | Loss: 0.00001246
Iteration 17/1000 | Loss: 0.00001246
Iteration 18/1000 | Loss: 0.00001245
Iteration 19/1000 | Loss: 0.00001245
Iteration 20/1000 | Loss: 0.00001241
Iteration 21/1000 | Loss: 0.00001240
Iteration 22/1000 | Loss: 0.00001237
Iteration 23/1000 | Loss: 0.00001235
Iteration 24/1000 | Loss: 0.00001224
Iteration 25/1000 | Loss: 0.00001222
Iteration 26/1000 | Loss: 0.00001221
Iteration 27/1000 | Loss: 0.00001219
Iteration 28/1000 | Loss: 0.00001218
Iteration 29/1000 | Loss: 0.00001218
Iteration 30/1000 | Loss: 0.00001216
Iteration 31/1000 | Loss: 0.00001215
Iteration 32/1000 | Loss: 0.00001215
Iteration 33/1000 | Loss: 0.00001215
Iteration 34/1000 | Loss: 0.00001211
Iteration 35/1000 | Loss: 0.00001211
Iteration 36/1000 | Loss: 0.00001210
Iteration 37/1000 | Loss: 0.00001210
Iteration 38/1000 | Loss: 0.00001210
Iteration 39/1000 | Loss: 0.00001207
Iteration 40/1000 | Loss: 0.00001206
Iteration 41/1000 | Loss: 0.00001206
Iteration 42/1000 | Loss: 0.00001204
Iteration 43/1000 | Loss: 0.00001204
Iteration 44/1000 | Loss: 0.00001203
Iteration 45/1000 | Loss: 0.00001196
Iteration 46/1000 | Loss: 0.00001195
Iteration 47/1000 | Loss: 0.00001195
Iteration 48/1000 | Loss: 0.00001195
Iteration 49/1000 | Loss: 0.00001195
Iteration 50/1000 | Loss: 0.00001195
Iteration 51/1000 | Loss: 0.00001194
Iteration 52/1000 | Loss: 0.00001194
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001194
Iteration 55/1000 | Loss: 0.00001193
Iteration 56/1000 | Loss: 0.00001193
Iteration 57/1000 | Loss: 0.00001193
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001192
Iteration 60/1000 | Loss: 0.00001192
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001192
Iteration 63/1000 | Loss: 0.00001192
Iteration 64/1000 | Loss: 0.00001192
Iteration 65/1000 | Loss: 0.00001192
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001191
Iteration 71/1000 | Loss: 0.00001191
Iteration 72/1000 | Loss: 0.00001191
Iteration 73/1000 | Loss: 0.00001190
Iteration 74/1000 | Loss: 0.00001190
Iteration 75/1000 | Loss: 0.00001190
Iteration 76/1000 | Loss: 0.00001190
Iteration 77/1000 | Loss: 0.00001190
Iteration 78/1000 | Loss: 0.00001190
Iteration 79/1000 | Loss: 0.00001190
Iteration 80/1000 | Loss: 0.00001190
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001189
Iteration 84/1000 | Loss: 0.00001189
Iteration 85/1000 | Loss: 0.00001189
Iteration 86/1000 | Loss: 0.00001188
Iteration 87/1000 | Loss: 0.00001188
Iteration 88/1000 | Loss: 0.00001188
Iteration 89/1000 | Loss: 0.00001188
Iteration 90/1000 | Loss: 0.00001188
Iteration 91/1000 | Loss: 0.00001187
Iteration 92/1000 | Loss: 0.00001187
Iteration 93/1000 | Loss: 0.00001187
Iteration 94/1000 | Loss: 0.00001187
Iteration 95/1000 | Loss: 0.00001187
Iteration 96/1000 | Loss: 0.00001187
Iteration 97/1000 | Loss: 0.00001186
Iteration 98/1000 | Loss: 0.00001186
Iteration 99/1000 | Loss: 0.00001186
Iteration 100/1000 | Loss: 0.00001186
Iteration 101/1000 | Loss: 0.00001186
Iteration 102/1000 | Loss: 0.00001186
Iteration 103/1000 | Loss: 0.00001186
Iteration 104/1000 | Loss: 0.00001185
Iteration 105/1000 | Loss: 0.00001185
Iteration 106/1000 | Loss: 0.00001185
Iteration 107/1000 | Loss: 0.00001185
Iteration 108/1000 | Loss: 0.00001185
Iteration 109/1000 | Loss: 0.00001185
Iteration 110/1000 | Loss: 0.00001185
Iteration 111/1000 | Loss: 0.00001185
Iteration 112/1000 | Loss: 0.00001185
Iteration 113/1000 | Loss: 0.00001185
Iteration 114/1000 | Loss: 0.00001185
Iteration 115/1000 | Loss: 0.00001185
Iteration 116/1000 | Loss: 0.00001185
Iteration 117/1000 | Loss: 0.00001185
Iteration 118/1000 | Loss: 0.00001185
Iteration 119/1000 | Loss: 0.00001184
Iteration 120/1000 | Loss: 0.00001184
Iteration 121/1000 | Loss: 0.00001184
Iteration 122/1000 | Loss: 0.00001184
Iteration 123/1000 | Loss: 0.00001184
Iteration 124/1000 | Loss: 0.00001184
Iteration 125/1000 | Loss: 0.00001184
Iteration 126/1000 | Loss: 0.00001184
Iteration 127/1000 | Loss: 0.00001184
Iteration 128/1000 | Loss: 0.00001184
Iteration 129/1000 | Loss: 0.00001184
Iteration 130/1000 | Loss: 0.00001184
Iteration 131/1000 | Loss: 0.00001183
Iteration 132/1000 | Loss: 0.00001183
Iteration 133/1000 | Loss: 0.00001183
Iteration 134/1000 | Loss: 0.00001183
Iteration 135/1000 | Loss: 0.00001183
Iteration 136/1000 | Loss: 0.00001183
Iteration 137/1000 | Loss: 0.00001183
Iteration 138/1000 | Loss: 0.00001183
Iteration 139/1000 | Loss: 0.00001183
Iteration 140/1000 | Loss: 0.00001183
Iteration 141/1000 | Loss: 0.00001183
Iteration 142/1000 | Loss: 0.00001183
Iteration 143/1000 | Loss: 0.00001183
Iteration 144/1000 | Loss: 0.00001183
Iteration 145/1000 | Loss: 0.00001183
Iteration 146/1000 | Loss: 0.00001183
Iteration 147/1000 | Loss: 0.00001183
Iteration 148/1000 | Loss: 0.00001183
Iteration 149/1000 | Loss: 0.00001183
Iteration 150/1000 | Loss: 0.00001183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.1829841241706163e-05, 1.1829841241706163e-05, 1.1829841241706163e-05, 1.1829841241706163e-05, 1.1829841241706163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1829841241706163e-05

Optimization complete. Final v2v error: 2.9084079265594482 mm

Highest mean error: 3.0647947788238525 mm for frame 167

Lowest mean error: 2.730072021484375 mm for frame 48

Saving results

Total time: 38.54754590988159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00357738
Iteration 2/25 | Loss: 0.00137125
Iteration 3/25 | Loss: 0.00125857
Iteration 4/25 | Loss: 0.00123274
Iteration 5/25 | Loss: 0.00122412
Iteration 6/25 | Loss: 0.00122181
Iteration 7/25 | Loss: 0.00122181
Iteration 8/25 | Loss: 0.00122181
Iteration 9/25 | Loss: 0.00122181
Iteration 10/25 | Loss: 0.00122181
Iteration 11/25 | Loss: 0.00122181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012218133779242635, 0.0012218133779242635, 0.0012218133779242635, 0.0012218133779242635, 0.0012218133779242635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012218133779242635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26584768
Iteration 2/25 | Loss: 0.00219098
Iteration 3/25 | Loss: 0.00219098
Iteration 4/25 | Loss: 0.00219098
Iteration 5/25 | Loss: 0.00219098
Iteration 6/25 | Loss: 0.00219098
Iteration 7/25 | Loss: 0.00219098
Iteration 8/25 | Loss: 0.00219097
Iteration 9/25 | Loss: 0.00219097
Iteration 10/25 | Loss: 0.00219097
Iteration 11/25 | Loss: 0.00219097
Iteration 12/25 | Loss: 0.00219097
Iteration 13/25 | Loss: 0.00219097
Iteration 14/25 | Loss: 0.00219097
Iteration 15/25 | Loss: 0.00219097
Iteration 16/25 | Loss: 0.00219097
Iteration 17/25 | Loss: 0.00219097
Iteration 18/25 | Loss: 0.00219097
Iteration 19/25 | Loss: 0.00219097
Iteration 20/25 | Loss: 0.00219097
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0021909738425165415, 0.0021909738425165415, 0.0021909738425165415, 0.0021909738425165415, 0.0021909738425165415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021909738425165415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219097
Iteration 2/1000 | Loss: 0.00005384
Iteration 3/1000 | Loss: 0.00003489
Iteration 4/1000 | Loss: 0.00002702
Iteration 5/1000 | Loss: 0.00002490
Iteration 6/1000 | Loss: 0.00002347
Iteration 7/1000 | Loss: 0.00002243
Iteration 8/1000 | Loss: 0.00002177
Iteration 9/1000 | Loss: 0.00002133
Iteration 10/1000 | Loss: 0.00002099
Iteration 11/1000 | Loss: 0.00002064
Iteration 12/1000 | Loss: 0.00002035
Iteration 13/1000 | Loss: 0.00002027
Iteration 14/1000 | Loss: 0.00002019
Iteration 15/1000 | Loss: 0.00002014
Iteration 16/1000 | Loss: 0.00002011
Iteration 17/1000 | Loss: 0.00002009
Iteration 18/1000 | Loss: 0.00002006
Iteration 19/1000 | Loss: 0.00002004
Iteration 20/1000 | Loss: 0.00002002
Iteration 21/1000 | Loss: 0.00001991
Iteration 22/1000 | Loss: 0.00001985
Iteration 23/1000 | Loss: 0.00001974
Iteration 24/1000 | Loss: 0.00001973
Iteration 25/1000 | Loss: 0.00001969
Iteration 26/1000 | Loss: 0.00001967
Iteration 27/1000 | Loss: 0.00001967
Iteration 28/1000 | Loss: 0.00001966
Iteration 29/1000 | Loss: 0.00001962
Iteration 30/1000 | Loss: 0.00001961
Iteration 31/1000 | Loss: 0.00001956
Iteration 32/1000 | Loss: 0.00001954
Iteration 33/1000 | Loss: 0.00001953
Iteration 34/1000 | Loss: 0.00001953
Iteration 35/1000 | Loss: 0.00001952
Iteration 36/1000 | Loss: 0.00001952
Iteration 37/1000 | Loss: 0.00001949
Iteration 38/1000 | Loss: 0.00001949
Iteration 39/1000 | Loss: 0.00001947
Iteration 40/1000 | Loss: 0.00001946
Iteration 41/1000 | Loss: 0.00001946
Iteration 42/1000 | Loss: 0.00001945
Iteration 43/1000 | Loss: 0.00001945
Iteration 44/1000 | Loss: 0.00001944
Iteration 45/1000 | Loss: 0.00001944
Iteration 46/1000 | Loss: 0.00001944
Iteration 47/1000 | Loss: 0.00001940
Iteration 48/1000 | Loss: 0.00001940
Iteration 49/1000 | Loss: 0.00001938
Iteration 50/1000 | Loss: 0.00001937
Iteration 51/1000 | Loss: 0.00001935
Iteration 52/1000 | Loss: 0.00001935
Iteration 53/1000 | Loss: 0.00001934
Iteration 54/1000 | Loss: 0.00001934
Iteration 55/1000 | Loss: 0.00001933
Iteration 56/1000 | Loss: 0.00001933
Iteration 57/1000 | Loss: 0.00001932
Iteration 58/1000 | Loss: 0.00001932
Iteration 59/1000 | Loss: 0.00001932
Iteration 60/1000 | Loss: 0.00001931
Iteration 61/1000 | Loss: 0.00001931
Iteration 62/1000 | Loss: 0.00001931
Iteration 63/1000 | Loss: 0.00001931
Iteration 64/1000 | Loss: 0.00001931
Iteration 65/1000 | Loss: 0.00001931
Iteration 66/1000 | Loss: 0.00001930
Iteration 67/1000 | Loss: 0.00001930
Iteration 68/1000 | Loss: 0.00001930
Iteration 69/1000 | Loss: 0.00001930
Iteration 70/1000 | Loss: 0.00001930
Iteration 71/1000 | Loss: 0.00001930
Iteration 72/1000 | Loss: 0.00001930
Iteration 73/1000 | Loss: 0.00001930
Iteration 74/1000 | Loss: 0.00001930
Iteration 75/1000 | Loss: 0.00001929
Iteration 76/1000 | Loss: 0.00001929
Iteration 77/1000 | Loss: 0.00001929
Iteration 78/1000 | Loss: 0.00001929
Iteration 79/1000 | Loss: 0.00001929
Iteration 80/1000 | Loss: 0.00001928
Iteration 81/1000 | Loss: 0.00001928
Iteration 82/1000 | Loss: 0.00001928
Iteration 83/1000 | Loss: 0.00001928
Iteration 84/1000 | Loss: 0.00001928
Iteration 85/1000 | Loss: 0.00001927
Iteration 86/1000 | Loss: 0.00001927
Iteration 87/1000 | Loss: 0.00001927
Iteration 88/1000 | Loss: 0.00001927
Iteration 89/1000 | Loss: 0.00001927
Iteration 90/1000 | Loss: 0.00001927
Iteration 91/1000 | Loss: 0.00001927
Iteration 92/1000 | Loss: 0.00001927
Iteration 93/1000 | Loss: 0.00001927
Iteration 94/1000 | Loss: 0.00001927
Iteration 95/1000 | Loss: 0.00001927
Iteration 96/1000 | Loss: 0.00001927
Iteration 97/1000 | Loss: 0.00001927
Iteration 98/1000 | Loss: 0.00001926
Iteration 99/1000 | Loss: 0.00001926
Iteration 100/1000 | Loss: 0.00001926
Iteration 101/1000 | Loss: 0.00001926
Iteration 102/1000 | Loss: 0.00001926
Iteration 103/1000 | Loss: 0.00001925
Iteration 104/1000 | Loss: 0.00001925
Iteration 105/1000 | Loss: 0.00001925
Iteration 106/1000 | Loss: 0.00001925
Iteration 107/1000 | Loss: 0.00001925
Iteration 108/1000 | Loss: 0.00001925
Iteration 109/1000 | Loss: 0.00001925
Iteration 110/1000 | Loss: 0.00001925
Iteration 111/1000 | Loss: 0.00001924
Iteration 112/1000 | Loss: 0.00001924
Iteration 113/1000 | Loss: 0.00001924
Iteration 114/1000 | Loss: 0.00001924
Iteration 115/1000 | Loss: 0.00001924
Iteration 116/1000 | Loss: 0.00001923
Iteration 117/1000 | Loss: 0.00001923
Iteration 118/1000 | Loss: 0.00001923
Iteration 119/1000 | Loss: 0.00001923
Iteration 120/1000 | Loss: 0.00001923
Iteration 121/1000 | Loss: 0.00001923
Iteration 122/1000 | Loss: 0.00001923
Iteration 123/1000 | Loss: 0.00001922
Iteration 124/1000 | Loss: 0.00001922
Iteration 125/1000 | Loss: 0.00001922
Iteration 126/1000 | Loss: 0.00001922
Iteration 127/1000 | Loss: 0.00001922
Iteration 128/1000 | Loss: 0.00001922
Iteration 129/1000 | Loss: 0.00001922
Iteration 130/1000 | Loss: 0.00001922
Iteration 131/1000 | Loss: 0.00001922
Iteration 132/1000 | Loss: 0.00001922
Iteration 133/1000 | Loss: 0.00001921
Iteration 134/1000 | Loss: 0.00001921
Iteration 135/1000 | Loss: 0.00001921
Iteration 136/1000 | Loss: 0.00001921
Iteration 137/1000 | Loss: 0.00001921
Iteration 138/1000 | Loss: 0.00001921
Iteration 139/1000 | Loss: 0.00001921
Iteration 140/1000 | Loss: 0.00001920
Iteration 141/1000 | Loss: 0.00001920
Iteration 142/1000 | Loss: 0.00001920
Iteration 143/1000 | Loss: 0.00001920
Iteration 144/1000 | Loss: 0.00001920
Iteration 145/1000 | Loss: 0.00001919
Iteration 146/1000 | Loss: 0.00001919
Iteration 147/1000 | Loss: 0.00001919
Iteration 148/1000 | Loss: 0.00001918
Iteration 149/1000 | Loss: 0.00001918
Iteration 150/1000 | Loss: 0.00001918
Iteration 151/1000 | Loss: 0.00001918
Iteration 152/1000 | Loss: 0.00001918
Iteration 153/1000 | Loss: 0.00001917
Iteration 154/1000 | Loss: 0.00001917
Iteration 155/1000 | Loss: 0.00001917
Iteration 156/1000 | Loss: 0.00001917
Iteration 157/1000 | Loss: 0.00001916
Iteration 158/1000 | Loss: 0.00001916
Iteration 159/1000 | Loss: 0.00001916
Iteration 160/1000 | Loss: 0.00001916
Iteration 161/1000 | Loss: 0.00001916
Iteration 162/1000 | Loss: 0.00001915
Iteration 163/1000 | Loss: 0.00001915
Iteration 164/1000 | Loss: 0.00001915
Iteration 165/1000 | Loss: 0.00001915
Iteration 166/1000 | Loss: 0.00001915
Iteration 167/1000 | Loss: 0.00001915
Iteration 168/1000 | Loss: 0.00001915
Iteration 169/1000 | Loss: 0.00001914
Iteration 170/1000 | Loss: 0.00001914
Iteration 171/1000 | Loss: 0.00001914
Iteration 172/1000 | Loss: 0.00001914
Iteration 173/1000 | Loss: 0.00001914
Iteration 174/1000 | Loss: 0.00001914
Iteration 175/1000 | Loss: 0.00001914
Iteration 176/1000 | Loss: 0.00001914
Iteration 177/1000 | Loss: 0.00001914
Iteration 178/1000 | Loss: 0.00001914
Iteration 179/1000 | Loss: 0.00001914
Iteration 180/1000 | Loss: 0.00001913
Iteration 181/1000 | Loss: 0.00001913
Iteration 182/1000 | Loss: 0.00001913
Iteration 183/1000 | Loss: 0.00001913
Iteration 184/1000 | Loss: 0.00001913
Iteration 185/1000 | Loss: 0.00001913
Iteration 186/1000 | Loss: 0.00001913
Iteration 187/1000 | Loss: 0.00001913
Iteration 188/1000 | Loss: 0.00001913
Iteration 189/1000 | Loss: 0.00001913
Iteration 190/1000 | Loss: 0.00001912
Iteration 191/1000 | Loss: 0.00001912
Iteration 192/1000 | Loss: 0.00001912
Iteration 193/1000 | Loss: 0.00001912
Iteration 194/1000 | Loss: 0.00001912
Iteration 195/1000 | Loss: 0.00001912
Iteration 196/1000 | Loss: 0.00001912
Iteration 197/1000 | Loss: 0.00001911
Iteration 198/1000 | Loss: 0.00001911
Iteration 199/1000 | Loss: 0.00001911
Iteration 200/1000 | Loss: 0.00001911
Iteration 201/1000 | Loss: 0.00001911
Iteration 202/1000 | Loss: 0.00001911
Iteration 203/1000 | Loss: 0.00001911
Iteration 204/1000 | Loss: 0.00001911
Iteration 205/1000 | Loss: 0.00001911
Iteration 206/1000 | Loss: 0.00001910
Iteration 207/1000 | Loss: 0.00001910
Iteration 208/1000 | Loss: 0.00001910
Iteration 209/1000 | Loss: 0.00001910
Iteration 210/1000 | Loss: 0.00001910
Iteration 211/1000 | Loss: 0.00001910
Iteration 212/1000 | Loss: 0.00001910
Iteration 213/1000 | Loss: 0.00001910
Iteration 214/1000 | Loss: 0.00001910
Iteration 215/1000 | Loss: 0.00001910
Iteration 216/1000 | Loss: 0.00001910
Iteration 217/1000 | Loss: 0.00001910
Iteration 218/1000 | Loss: 0.00001910
Iteration 219/1000 | Loss: 0.00001910
Iteration 220/1000 | Loss: 0.00001910
Iteration 221/1000 | Loss: 0.00001910
Iteration 222/1000 | Loss: 0.00001910
Iteration 223/1000 | Loss: 0.00001910
Iteration 224/1000 | Loss: 0.00001910
Iteration 225/1000 | Loss: 0.00001910
Iteration 226/1000 | Loss: 0.00001910
Iteration 227/1000 | Loss: 0.00001910
Iteration 228/1000 | Loss: 0.00001910
Iteration 229/1000 | Loss: 0.00001910
Iteration 230/1000 | Loss: 0.00001910
Iteration 231/1000 | Loss: 0.00001910
Iteration 232/1000 | Loss: 0.00001910
Iteration 233/1000 | Loss: 0.00001910
Iteration 234/1000 | Loss: 0.00001910
Iteration 235/1000 | Loss: 0.00001910
Iteration 236/1000 | Loss: 0.00001910
Iteration 237/1000 | Loss: 0.00001910
Iteration 238/1000 | Loss: 0.00001910
Iteration 239/1000 | Loss: 0.00001910
Iteration 240/1000 | Loss: 0.00001910
Iteration 241/1000 | Loss: 0.00001910
Iteration 242/1000 | Loss: 0.00001910
Iteration 243/1000 | Loss: 0.00001910
Iteration 244/1000 | Loss: 0.00001910
Iteration 245/1000 | Loss: 0.00001910
Iteration 246/1000 | Loss: 0.00001910
Iteration 247/1000 | Loss: 0.00001910
Iteration 248/1000 | Loss: 0.00001910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.9096110918326303e-05, 1.9096110918326303e-05, 1.9096110918326303e-05, 1.9096110918326303e-05, 1.9096110918326303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9096110918326303e-05

Optimization complete. Final v2v error: 3.6365737915039062 mm

Highest mean error: 4.743966102600098 mm for frame 188

Lowest mean error: 2.552435874938965 mm for frame 221

Saving results

Total time: 56.929972410202026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490941
Iteration 2/25 | Loss: 0.00130164
Iteration 3/25 | Loss: 0.00122746
Iteration 4/25 | Loss: 0.00121998
Iteration 5/25 | Loss: 0.00121821
Iteration 6/25 | Loss: 0.00121821
Iteration 7/25 | Loss: 0.00121821
Iteration 8/25 | Loss: 0.00121821
Iteration 9/25 | Loss: 0.00121821
Iteration 10/25 | Loss: 0.00121821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012182086938992143, 0.0012182086938992143, 0.0012182086938992143, 0.0012182086938992143, 0.0012182086938992143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012182086938992143

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.16936445
Iteration 2/25 | Loss: 0.00147672
Iteration 3/25 | Loss: 0.00147670
Iteration 4/25 | Loss: 0.00147670
Iteration 5/25 | Loss: 0.00147670
Iteration 6/25 | Loss: 0.00147670
Iteration 7/25 | Loss: 0.00147670
Iteration 8/25 | Loss: 0.00147670
Iteration 9/25 | Loss: 0.00147670
Iteration 10/25 | Loss: 0.00147670
Iteration 11/25 | Loss: 0.00147670
Iteration 12/25 | Loss: 0.00147670
Iteration 13/25 | Loss: 0.00147670
Iteration 14/25 | Loss: 0.00147670
Iteration 15/25 | Loss: 0.00147670
Iteration 16/25 | Loss: 0.00147670
Iteration 17/25 | Loss: 0.00147670
Iteration 18/25 | Loss: 0.00147670
Iteration 19/25 | Loss: 0.00147670
Iteration 20/25 | Loss: 0.00147670
Iteration 21/25 | Loss: 0.00147670
Iteration 22/25 | Loss: 0.00147670
Iteration 23/25 | Loss: 0.00147670
Iteration 24/25 | Loss: 0.00147670
Iteration 25/25 | Loss: 0.00147670

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147670
Iteration 2/1000 | Loss: 0.00001921
Iteration 3/1000 | Loss: 0.00001562
Iteration 4/1000 | Loss: 0.00001452
Iteration 5/1000 | Loss: 0.00001366
Iteration 6/1000 | Loss: 0.00001325
Iteration 7/1000 | Loss: 0.00001278
Iteration 8/1000 | Loss: 0.00001236
Iteration 9/1000 | Loss: 0.00001212
Iteration 10/1000 | Loss: 0.00001208
Iteration 11/1000 | Loss: 0.00001191
Iteration 12/1000 | Loss: 0.00001179
Iteration 13/1000 | Loss: 0.00001177
Iteration 14/1000 | Loss: 0.00001176
Iteration 15/1000 | Loss: 0.00001165
Iteration 16/1000 | Loss: 0.00001157
Iteration 17/1000 | Loss: 0.00001152
Iteration 18/1000 | Loss: 0.00001149
Iteration 19/1000 | Loss: 0.00001138
Iteration 20/1000 | Loss: 0.00001126
Iteration 21/1000 | Loss: 0.00001123
Iteration 22/1000 | Loss: 0.00001121
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001118
Iteration 25/1000 | Loss: 0.00001117
Iteration 26/1000 | Loss: 0.00001116
Iteration 27/1000 | Loss: 0.00001110
Iteration 28/1000 | Loss: 0.00001106
Iteration 29/1000 | Loss: 0.00001104
Iteration 30/1000 | Loss: 0.00001101
Iteration 31/1000 | Loss: 0.00001101
Iteration 32/1000 | Loss: 0.00001099
Iteration 33/1000 | Loss: 0.00001099
Iteration 34/1000 | Loss: 0.00001099
Iteration 35/1000 | Loss: 0.00001098
Iteration 36/1000 | Loss: 0.00001098
Iteration 37/1000 | Loss: 0.00001096
Iteration 38/1000 | Loss: 0.00001094
Iteration 39/1000 | Loss: 0.00001092
Iteration 40/1000 | Loss: 0.00001091
Iteration 41/1000 | Loss: 0.00001086
Iteration 42/1000 | Loss: 0.00001079
Iteration 43/1000 | Loss: 0.00001079
Iteration 44/1000 | Loss: 0.00001079
Iteration 45/1000 | Loss: 0.00001077
Iteration 46/1000 | Loss: 0.00001076
Iteration 47/1000 | Loss: 0.00001076
Iteration 48/1000 | Loss: 0.00001073
Iteration 49/1000 | Loss: 0.00001073
Iteration 50/1000 | Loss: 0.00001072
Iteration 51/1000 | Loss: 0.00001071
Iteration 52/1000 | Loss: 0.00001071
Iteration 53/1000 | Loss: 0.00001070
Iteration 54/1000 | Loss: 0.00001069
Iteration 55/1000 | Loss: 0.00001068
Iteration 56/1000 | Loss: 0.00001068
Iteration 57/1000 | Loss: 0.00001068
Iteration 58/1000 | Loss: 0.00001068
Iteration 59/1000 | Loss: 0.00001068
Iteration 60/1000 | Loss: 0.00001068
Iteration 61/1000 | Loss: 0.00001068
Iteration 62/1000 | Loss: 0.00001067
Iteration 63/1000 | Loss: 0.00001066
Iteration 64/1000 | Loss: 0.00001066
Iteration 65/1000 | Loss: 0.00001066
Iteration 66/1000 | Loss: 0.00001065
Iteration 67/1000 | Loss: 0.00001064
Iteration 68/1000 | Loss: 0.00001064
Iteration 69/1000 | Loss: 0.00001064
Iteration 70/1000 | Loss: 0.00001064
Iteration 71/1000 | Loss: 0.00001063
Iteration 72/1000 | Loss: 0.00001063
Iteration 73/1000 | Loss: 0.00001063
Iteration 74/1000 | Loss: 0.00001063
Iteration 75/1000 | Loss: 0.00001063
Iteration 76/1000 | Loss: 0.00001062
Iteration 77/1000 | Loss: 0.00001062
Iteration 78/1000 | Loss: 0.00001062
Iteration 79/1000 | Loss: 0.00001062
Iteration 80/1000 | Loss: 0.00001062
Iteration 81/1000 | Loss: 0.00001062
Iteration 82/1000 | Loss: 0.00001061
Iteration 83/1000 | Loss: 0.00001061
Iteration 84/1000 | Loss: 0.00001061
Iteration 85/1000 | Loss: 0.00001061
Iteration 86/1000 | Loss: 0.00001061
Iteration 87/1000 | Loss: 0.00001061
Iteration 88/1000 | Loss: 0.00001061
Iteration 89/1000 | Loss: 0.00001061
Iteration 90/1000 | Loss: 0.00001060
Iteration 91/1000 | Loss: 0.00001060
Iteration 92/1000 | Loss: 0.00001060
Iteration 93/1000 | Loss: 0.00001060
Iteration 94/1000 | Loss: 0.00001059
Iteration 95/1000 | Loss: 0.00001059
Iteration 96/1000 | Loss: 0.00001059
Iteration 97/1000 | Loss: 0.00001059
Iteration 98/1000 | Loss: 0.00001059
Iteration 99/1000 | Loss: 0.00001058
Iteration 100/1000 | Loss: 0.00001058
Iteration 101/1000 | Loss: 0.00001058
Iteration 102/1000 | Loss: 0.00001058
Iteration 103/1000 | Loss: 0.00001058
Iteration 104/1000 | Loss: 0.00001058
Iteration 105/1000 | Loss: 0.00001058
Iteration 106/1000 | Loss: 0.00001058
Iteration 107/1000 | Loss: 0.00001058
Iteration 108/1000 | Loss: 0.00001058
Iteration 109/1000 | Loss: 0.00001058
Iteration 110/1000 | Loss: 0.00001058
Iteration 111/1000 | Loss: 0.00001058
Iteration 112/1000 | Loss: 0.00001058
Iteration 113/1000 | Loss: 0.00001058
Iteration 114/1000 | Loss: 0.00001058
Iteration 115/1000 | Loss: 0.00001057
Iteration 116/1000 | Loss: 0.00001057
Iteration 117/1000 | Loss: 0.00001057
Iteration 118/1000 | Loss: 0.00001057
Iteration 119/1000 | Loss: 0.00001057
Iteration 120/1000 | Loss: 0.00001057
Iteration 121/1000 | Loss: 0.00001057
Iteration 122/1000 | Loss: 0.00001057
Iteration 123/1000 | Loss: 0.00001057
Iteration 124/1000 | Loss: 0.00001056
Iteration 125/1000 | Loss: 0.00001056
Iteration 126/1000 | Loss: 0.00001056
Iteration 127/1000 | Loss: 0.00001056
Iteration 128/1000 | Loss: 0.00001056
Iteration 129/1000 | Loss: 0.00001056
Iteration 130/1000 | Loss: 0.00001056
Iteration 131/1000 | Loss: 0.00001056
Iteration 132/1000 | Loss: 0.00001056
Iteration 133/1000 | Loss: 0.00001056
Iteration 134/1000 | Loss: 0.00001056
Iteration 135/1000 | Loss: 0.00001055
Iteration 136/1000 | Loss: 0.00001055
Iteration 137/1000 | Loss: 0.00001055
Iteration 138/1000 | Loss: 0.00001055
Iteration 139/1000 | Loss: 0.00001055
Iteration 140/1000 | Loss: 0.00001055
Iteration 141/1000 | Loss: 0.00001055
Iteration 142/1000 | Loss: 0.00001055
Iteration 143/1000 | Loss: 0.00001055
Iteration 144/1000 | Loss: 0.00001055
Iteration 145/1000 | Loss: 0.00001055
Iteration 146/1000 | Loss: 0.00001055
Iteration 147/1000 | Loss: 0.00001055
Iteration 148/1000 | Loss: 0.00001055
Iteration 149/1000 | Loss: 0.00001055
Iteration 150/1000 | Loss: 0.00001055
Iteration 151/1000 | Loss: 0.00001055
Iteration 152/1000 | Loss: 0.00001055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.0553768333920743e-05, 1.0553768333920743e-05, 1.0553768333920743e-05, 1.0553768333920743e-05, 1.0553768333920743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0553768333920743e-05

Optimization complete. Final v2v error: 2.844200849533081 mm

Highest mean error: 3.1555347442626953 mm for frame 215

Lowest mean error: 2.6553757190704346 mm for frame 236

Saving results

Total time: 46.491074562072754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00676728
Iteration 2/25 | Loss: 0.00141611
Iteration 3/25 | Loss: 0.00132739
Iteration 4/25 | Loss: 0.00130437
Iteration 5/25 | Loss: 0.00129552
Iteration 6/25 | Loss: 0.00129379
Iteration 7/25 | Loss: 0.00129379
Iteration 8/25 | Loss: 0.00129379
Iteration 9/25 | Loss: 0.00129379
Iteration 10/25 | Loss: 0.00129379
Iteration 11/25 | Loss: 0.00129379
Iteration 12/25 | Loss: 0.00129379
Iteration 13/25 | Loss: 0.00129379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001293787732720375, 0.001293787732720375, 0.001293787732720375, 0.001293787732720375, 0.001293787732720375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001293787732720375

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55097783
Iteration 2/25 | Loss: 0.00186902
Iteration 3/25 | Loss: 0.00186901
Iteration 4/25 | Loss: 0.00186901
Iteration 5/25 | Loss: 0.00186901
Iteration 6/25 | Loss: 0.00186901
Iteration 7/25 | Loss: 0.00186901
Iteration 8/25 | Loss: 0.00186901
Iteration 9/25 | Loss: 0.00186901
Iteration 10/25 | Loss: 0.00186901
Iteration 11/25 | Loss: 0.00186901
Iteration 12/25 | Loss: 0.00186901
Iteration 13/25 | Loss: 0.00186901
Iteration 14/25 | Loss: 0.00186901
Iteration 15/25 | Loss: 0.00186901
Iteration 16/25 | Loss: 0.00186901
Iteration 17/25 | Loss: 0.00186901
Iteration 18/25 | Loss: 0.00186901
Iteration 19/25 | Loss: 0.00186901
Iteration 20/25 | Loss: 0.00186901
Iteration 21/25 | Loss: 0.00186901
Iteration 22/25 | Loss: 0.00186901
Iteration 23/25 | Loss: 0.00186901
Iteration 24/25 | Loss: 0.00186901
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00186900794506073, 0.00186900794506073, 0.00186900794506073, 0.00186900794506073, 0.00186900794506073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00186900794506073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186901
Iteration 2/1000 | Loss: 0.00007359
Iteration 3/1000 | Loss: 0.00004333
Iteration 4/1000 | Loss: 0.00003375
Iteration 5/1000 | Loss: 0.00003077
Iteration 6/1000 | Loss: 0.00002934
Iteration 7/1000 | Loss: 0.00002808
Iteration 8/1000 | Loss: 0.00002727
Iteration 9/1000 | Loss: 0.00002668
Iteration 10/1000 | Loss: 0.00002628
Iteration 11/1000 | Loss: 0.00002597
Iteration 12/1000 | Loss: 0.00002576
Iteration 13/1000 | Loss: 0.00002558
Iteration 14/1000 | Loss: 0.00002540
Iteration 15/1000 | Loss: 0.00002525
Iteration 16/1000 | Loss: 0.00002523
Iteration 17/1000 | Loss: 0.00002521
Iteration 18/1000 | Loss: 0.00002520
Iteration 19/1000 | Loss: 0.00002519
Iteration 20/1000 | Loss: 0.00002519
Iteration 21/1000 | Loss: 0.00002518
Iteration 22/1000 | Loss: 0.00002517
Iteration 23/1000 | Loss: 0.00002517
Iteration 24/1000 | Loss: 0.00002517
Iteration 25/1000 | Loss: 0.00002516
Iteration 26/1000 | Loss: 0.00002516
Iteration 27/1000 | Loss: 0.00002515
Iteration 28/1000 | Loss: 0.00002515
Iteration 29/1000 | Loss: 0.00002515
Iteration 30/1000 | Loss: 0.00002514
Iteration 31/1000 | Loss: 0.00002514
Iteration 32/1000 | Loss: 0.00002510
Iteration 33/1000 | Loss: 0.00002509
Iteration 34/1000 | Loss: 0.00002508
Iteration 35/1000 | Loss: 0.00002508
Iteration 36/1000 | Loss: 0.00002508
Iteration 37/1000 | Loss: 0.00002508
Iteration 38/1000 | Loss: 0.00002507
Iteration 39/1000 | Loss: 0.00002507
Iteration 40/1000 | Loss: 0.00002507
Iteration 41/1000 | Loss: 0.00002506
Iteration 42/1000 | Loss: 0.00002506
Iteration 43/1000 | Loss: 0.00002505
Iteration 44/1000 | Loss: 0.00002505
Iteration 45/1000 | Loss: 0.00002505
Iteration 46/1000 | Loss: 0.00002504
Iteration 47/1000 | Loss: 0.00002504
Iteration 48/1000 | Loss: 0.00002503
Iteration 49/1000 | Loss: 0.00002503
Iteration 50/1000 | Loss: 0.00002503
Iteration 51/1000 | Loss: 0.00002502
Iteration 52/1000 | Loss: 0.00002502
Iteration 53/1000 | Loss: 0.00002501
Iteration 54/1000 | Loss: 0.00002501
Iteration 55/1000 | Loss: 0.00002501
Iteration 56/1000 | Loss: 0.00002500
Iteration 57/1000 | Loss: 0.00002500
Iteration 58/1000 | Loss: 0.00002500
Iteration 59/1000 | Loss: 0.00002499
Iteration 60/1000 | Loss: 0.00002499
Iteration 61/1000 | Loss: 0.00002498
Iteration 62/1000 | Loss: 0.00002498
Iteration 63/1000 | Loss: 0.00002498
Iteration 64/1000 | Loss: 0.00002497
Iteration 65/1000 | Loss: 0.00002497
Iteration 66/1000 | Loss: 0.00002497
Iteration 67/1000 | Loss: 0.00002496
Iteration 68/1000 | Loss: 0.00002496
Iteration 69/1000 | Loss: 0.00002496
Iteration 70/1000 | Loss: 0.00002495
Iteration 71/1000 | Loss: 0.00002495
Iteration 72/1000 | Loss: 0.00002495
Iteration 73/1000 | Loss: 0.00002495
Iteration 74/1000 | Loss: 0.00002494
Iteration 75/1000 | Loss: 0.00002494
Iteration 76/1000 | Loss: 0.00002494
Iteration 77/1000 | Loss: 0.00002493
Iteration 78/1000 | Loss: 0.00002493
Iteration 79/1000 | Loss: 0.00002493
Iteration 80/1000 | Loss: 0.00002493
Iteration 81/1000 | Loss: 0.00002493
Iteration 82/1000 | Loss: 0.00002492
Iteration 83/1000 | Loss: 0.00002492
Iteration 84/1000 | Loss: 0.00002492
Iteration 85/1000 | Loss: 0.00002491
Iteration 86/1000 | Loss: 0.00002491
Iteration 87/1000 | Loss: 0.00002491
Iteration 88/1000 | Loss: 0.00002491
Iteration 89/1000 | Loss: 0.00002491
Iteration 90/1000 | Loss: 0.00002490
Iteration 91/1000 | Loss: 0.00002490
Iteration 92/1000 | Loss: 0.00002490
Iteration 93/1000 | Loss: 0.00002490
Iteration 94/1000 | Loss: 0.00002489
Iteration 95/1000 | Loss: 0.00002489
Iteration 96/1000 | Loss: 0.00002489
Iteration 97/1000 | Loss: 0.00002488
Iteration 98/1000 | Loss: 0.00002488
Iteration 99/1000 | Loss: 0.00002488
Iteration 100/1000 | Loss: 0.00002488
Iteration 101/1000 | Loss: 0.00002487
Iteration 102/1000 | Loss: 0.00002487
Iteration 103/1000 | Loss: 0.00002487
Iteration 104/1000 | Loss: 0.00002486
Iteration 105/1000 | Loss: 0.00002486
Iteration 106/1000 | Loss: 0.00002486
Iteration 107/1000 | Loss: 0.00002486
Iteration 108/1000 | Loss: 0.00002485
Iteration 109/1000 | Loss: 0.00002485
Iteration 110/1000 | Loss: 0.00002485
Iteration 111/1000 | Loss: 0.00002485
Iteration 112/1000 | Loss: 0.00002484
Iteration 113/1000 | Loss: 0.00002484
Iteration 114/1000 | Loss: 0.00002484
Iteration 115/1000 | Loss: 0.00002484
Iteration 116/1000 | Loss: 0.00002483
Iteration 117/1000 | Loss: 0.00002483
Iteration 118/1000 | Loss: 0.00002483
Iteration 119/1000 | Loss: 0.00002483
Iteration 120/1000 | Loss: 0.00002483
Iteration 121/1000 | Loss: 0.00002483
Iteration 122/1000 | Loss: 0.00002482
Iteration 123/1000 | Loss: 0.00002482
Iteration 124/1000 | Loss: 0.00002482
Iteration 125/1000 | Loss: 0.00002482
Iteration 126/1000 | Loss: 0.00002481
Iteration 127/1000 | Loss: 0.00002481
Iteration 128/1000 | Loss: 0.00002481
Iteration 129/1000 | Loss: 0.00002481
Iteration 130/1000 | Loss: 0.00002481
Iteration 131/1000 | Loss: 0.00002480
Iteration 132/1000 | Loss: 0.00002480
Iteration 133/1000 | Loss: 0.00002480
Iteration 134/1000 | Loss: 0.00002480
Iteration 135/1000 | Loss: 0.00002480
Iteration 136/1000 | Loss: 0.00002480
Iteration 137/1000 | Loss: 0.00002480
Iteration 138/1000 | Loss: 0.00002480
Iteration 139/1000 | Loss: 0.00002480
Iteration 140/1000 | Loss: 0.00002480
Iteration 141/1000 | Loss: 0.00002480
Iteration 142/1000 | Loss: 0.00002480
Iteration 143/1000 | Loss: 0.00002480
Iteration 144/1000 | Loss: 0.00002480
Iteration 145/1000 | Loss: 0.00002480
Iteration 146/1000 | Loss: 0.00002480
Iteration 147/1000 | Loss: 0.00002480
Iteration 148/1000 | Loss: 0.00002479
Iteration 149/1000 | Loss: 0.00002479
Iteration 150/1000 | Loss: 0.00002479
Iteration 151/1000 | Loss: 0.00002479
Iteration 152/1000 | Loss: 0.00002479
Iteration 153/1000 | Loss: 0.00002479
Iteration 154/1000 | Loss: 0.00002479
Iteration 155/1000 | Loss: 0.00002479
Iteration 156/1000 | Loss: 0.00002479
Iteration 157/1000 | Loss: 0.00002479
Iteration 158/1000 | Loss: 0.00002478
Iteration 159/1000 | Loss: 0.00002478
Iteration 160/1000 | Loss: 0.00002478
Iteration 161/1000 | Loss: 0.00002478
Iteration 162/1000 | Loss: 0.00002478
Iteration 163/1000 | Loss: 0.00002478
Iteration 164/1000 | Loss: 0.00002478
Iteration 165/1000 | Loss: 0.00002478
Iteration 166/1000 | Loss: 0.00002478
Iteration 167/1000 | Loss: 0.00002478
Iteration 168/1000 | Loss: 0.00002478
Iteration 169/1000 | Loss: 0.00002478
Iteration 170/1000 | Loss: 0.00002477
Iteration 171/1000 | Loss: 0.00002477
Iteration 172/1000 | Loss: 0.00002477
Iteration 173/1000 | Loss: 0.00002477
Iteration 174/1000 | Loss: 0.00002477
Iteration 175/1000 | Loss: 0.00002477
Iteration 176/1000 | Loss: 0.00002477
Iteration 177/1000 | Loss: 0.00002477
Iteration 178/1000 | Loss: 0.00002477
Iteration 179/1000 | Loss: 0.00002477
Iteration 180/1000 | Loss: 0.00002477
Iteration 181/1000 | Loss: 0.00002477
Iteration 182/1000 | Loss: 0.00002477
Iteration 183/1000 | Loss: 0.00002477
Iteration 184/1000 | Loss: 0.00002477
Iteration 185/1000 | Loss: 0.00002477
Iteration 186/1000 | Loss: 0.00002477
Iteration 187/1000 | Loss: 0.00002477
Iteration 188/1000 | Loss: 0.00002477
Iteration 189/1000 | Loss: 0.00002477
Iteration 190/1000 | Loss: 0.00002477
Iteration 191/1000 | Loss: 0.00002477
Iteration 192/1000 | Loss: 0.00002477
Iteration 193/1000 | Loss: 0.00002477
Iteration 194/1000 | Loss: 0.00002477
Iteration 195/1000 | Loss: 0.00002477
Iteration 196/1000 | Loss: 0.00002477
Iteration 197/1000 | Loss: 0.00002477
Iteration 198/1000 | Loss: 0.00002477
Iteration 199/1000 | Loss: 0.00002477
Iteration 200/1000 | Loss: 0.00002477
Iteration 201/1000 | Loss: 0.00002477
Iteration 202/1000 | Loss: 0.00002477
Iteration 203/1000 | Loss: 0.00002477
Iteration 204/1000 | Loss: 0.00002477
Iteration 205/1000 | Loss: 0.00002477
Iteration 206/1000 | Loss: 0.00002477
Iteration 207/1000 | Loss: 0.00002477
Iteration 208/1000 | Loss: 0.00002477
Iteration 209/1000 | Loss: 0.00002477
Iteration 210/1000 | Loss: 0.00002477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.477136513334699e-05, 2.477136513334699e-05, 2.477136513334699e-05, 2.477136513334699e-05, 2.477136513334699e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.477136513334699e-05

Optimization complete. Final v2v error: 4.091548919677734 mm

Highest mean error: 5.22313928604126 mm for frame 164

Lowest mean error: 3.402914524078369 mm for frame 66

Saving results

Total time: 50.453375816345215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995444
Iteration 2/25 | Loss: 0.00171844
Iteration 3/25 | Loss: 0.00138847
Iteration 4/25 | Loss: 0.00133887
Iteration 5/25 | Loss: 0.00132726
Iteration 6/25 | Loss: 0.00133756
Iteration 7/25 | Loss: 0.00132701
Iteration 8/25 | Loss: 0.00131409
Iteration 9/25 | Loss: 0.00130708
Iteration 10/25 | Loss: 0.00130570
Iteration 11/25 | Loss: 0.00130541
Iteration 12/25 | Loss: 0.00130530
Iteration 13/25 | Loss: 0.00130530
Iteration 14/25 | Loss: 0.00130529
Iteration 15/25 | Loss: 0.00130529
Iteration 16/25 | Loss: 0.00130529
Iteration 17/25 | Loss: 0.00130529
Iteration 18/25 | Loss: 0.00130529
Iteration 19/25 | Loss: 0.00130529
Iteration 20/25 | Loss: 0.00130529
Iteration 21/25 | Loss: 0.00130529
Iteration 22/25 | Loss: 0.00130529
Iteration 23/25 | Loss: 0.00130529
Iteration 24/25 | Loss: 0.00130529
Iteration 25/25 | Loss: 0.00130529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29682577
Iteration 2/25 | Loss: 0.00161931
Iteration 3/25 | Loss: 0.00161931
Iteration 4/25 | Loss: 0.00161931
Iteration 5/25 | Loss: 0.00161931
Iteration 6/25 | Loss: 0.00161931
Iteration 7/25 | Loss: 0.00161931
Iteration 8/25 | Loss: 0.00161931
Iteration 9/25 | Loss: 0.00161931
Iteration 10/25 | Loss: 0.00161931
Iteration 11/25 | Loss: 0.00161931
Iteration 12/25 | Loss: 0.00161931
Iteration 13/25 | Loss: 0.00161931
Iteration 14/25 | Loss: 0.00161931
Iteration 15/25 | Loss: 0.00161931
Iteration 16/25 | Loss: 0.00161931
Iteration 17/25 | Loss: 0.00161931
Iteration 18/25 | Loss: 0.00161931
Iteration 19/25 | Loss: 0.00161931
Iteration 20/25 | Loss: 0.00161931
Iteration 21/25 | Loss: 0.00161931
Iteration 22/25 | Loss: 0.00161931
Iteration 23/25 | Loss: 0.00161931
Iteration 24/25 | Loss: 0.00161931
Iteration 25/25 | Loss: 0.00161931
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0016193061601370573, 0.0016193061601370573, 0.0016193061601370573, 0.0016193061601370573, 0.0016193061601370573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016193061601370573

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161931
Iteration 2/1000 | Loss: 0.00002863
Iteration 3/1000 | Loss: 0.00001992
Iteration 4/1000 | Loss: 0.00001825
Iteration 5/1000 | Loss: 0.00001764
Iteration 6/1000 | Loss: 0.00001722
Iteration 7/1000 | Loss: 0.00001673
Iteration 8/1000 | Loss: 0.00001636
Iteration 9/1000 | Loss: 0.00001612
Iteration 10/1000 | Loss: 0.00001612
Iteration 11/1000 | Loss: 0.00001596
Iteration 12/1000 | Loss: 0.00001595
Iteration 13/1000 | Loss: 0.00001590
Iteration 14/1000 | Loss: 0.00001574
Iteration 15/1000 | Loss: 0.00001573
Iteration 16/1000 | Loss: 0.00001557
Iteration 17/1000 | Loss: 0.00001553
Iteration 18/1000 | Loss: 0.00001552
Iteration 19/1000 | Loss: 0.00001543
Iteration 20/1000 | Loss: 0.00001535
Iteration 21/1000 | Loss: 0.00001534
Iteration 22/1000 | Loss: 0.00001533
Iteration 23/1000 | Loss: 0.00001533
Iteration 24/1000 | Loss: 0.00001532
Iteration 25/1000 | Loss: 0.00001532
Iteration 26/1000 | Loss: 0.00001531
Iteration 27/1000 | Loss: 0.00001531
Iteration 28/1000 | Loss: 0.00001531
Iteration 29/1000 | Loss: 0.00001530
Iteration 30/1000 | Loss: 0.00001530
Iteration 31/1000 | Loss: 0.00001530
Iteration 32/1000 | Loss: 0.00001530
Iteration 33/1000 | Loss: 0.00001530
Iteration 34/1000 | Loss: 0.00001530
Iteration 35/1000 | Loss: 0.00001529
Iteration 36/1000 | Loss: 0.00001529
Iteration 37/1000 | Loss: 0.00001529
Iteration 38/1000 | Loss: 0.00001529
Iteration 39/1000 | Loss: 0.00001529
Iteration 40/1000 | Loss: 0.00001529
Iteration 41/1000 | Loss: 0.00001529
Iteration 42/1000 | Loss: 0.00001529
Iteration 43/1000 | Loss: 0.00001529
Iteration 44/1000 | Loss: 0.00001529
Iteration 45/1000 | Loss: 0.00001529
Iteration 46/1000 | Loss: 0.00001528
Iteration 47/1000 | Loss: 0.00001528
Iteration 48/1000 | Loss: 0.00001526
Iteration 49/1000 | Loss: 0.00001525
Iteration 50/1000 | Loss: 0.00001521
Iteration 51/1000 | Loss: 0.00001521
Iteration 52/1000 | Loss: 0.00001521
Iteration 53/1000 | Loss: 0.00001520
Iteration 54/1000 | Loss: 0.00001520
Iteration 55/1000 | Loss: 0.00001520
Iteration 56/1000 | Loss: 0.00001520
Iteration 57/1000 | Loss: 0.00001520
Iteration 58/1000 | Loss: 0.00001520
Iteration 59/1000 | Loss: 0.00001520
Iteration 60/1000 | Loss: 0.00001520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [1.5196949789242353e-05, 1.5196949789242353e-05, 1.5196949789242353e-05, 1.5196949789242353e-05, 1.5196949789242353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5196949789242353e-05

Optimization complete. Final v2v error: 3.403977394104004 mm

Highest mean error: 3.476405143737793 mm for frame 55

Lowest mean error: 3.3576254844665527 mm for frame 93

Saving results

Total time: 40.4051992893219
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793255
Iteration 2/25 | Loss: 0.00128638
Iteration 3/25 | Loss: 0.00121864
Iteration 4/25 | Loss: 0.00120920
Iteration 5/25 | Loss: 0.00120614
Iteration 6/25 | Loss: 0.00120572
Iteration 7/25 | Loss: 0.00120572
Iteration 8/25 | Loss: 0.00120572
Iteration 9/25 | Loss: 0.00120572
Iteration 10/25 | Loss: 0.00120572
Iteration 11/25 | Loss: 0.00120572
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001205716747790575, 0.001205716747790575, 0.001205716747790575, 0.001205716747790575, 0.001205716747790575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001205716747790575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29356587
Iteration 2/25 | Loss: 0.00147544
Iteration 3/25 | Loss: 0.00147543
Iteration 4/25 | Loss: 0.00147543
Iteration 5/25 | Loss: 0.00147543
Iteration 6/25 | Loss: 0.00147543
Iteration 7/25 | Loss: 0.00147543
Iteration 8/25 | Loss: 0.00147543
Iteration 9/25 | Loss: 0.00147543
Iteration 10/25 | Loss: 0.00147543
Iteration 11/25 | Loss: 0.00147543
Iteration 12/25 | Loss: 0.00147543
Iteration 13/25 | Loss: 0.00147543
Iteration 14/25 | Loss: 0.00147543
Iteration 15/25 | Loss: 0.00147543
Iteration 16/25 | Loss: 0.00147543
Iteration 17/25 | Loss: 0.00147543
Iteration 18/25 | Loss: 0.00147543
Iteration 19/25 | Loss: 0.00147543
Iteration 20/25 | Loss: 0.00147543
Iteration 21/25 | Loss: 0.00147543
Iteration 22/25 | Loss: 0.00147543
Iteration 23/25 | Loss: 0.00147543
Iteration 24/25 | Loss: 0.00147543
Iteration 25/25 | Loss: 0.00147543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147543
Iteration 2/1000 | Loss: 0.00001996
Iteration 3/1000 | Loss: 0.00001399
Iteration 4/1000 | Loss: 0.00001233
Iteration 5/1000 | Loss: 0.00001171
Iteration 6/1000 | Loss: 0.00001106
Iteration 7/1000 | Loss: 0.00001053
Iteration 8/1000 | Loss: 0.00001036
Iteration 9/1000 | Loss: 0.00001011
Iteration 10/1000 | Loss: 0.00000983
Iteration 11/1000 | Loss: 0.00000970
Iteration 12/1000 | Loss: 0.00000961
Iteration 13/1000 | Loss: 0.00000959
Iteration 14/1000 | Loss: 0.00000955
Iteration 15/1000 | Loss: 0.00000953
Iteration 16/1000 | Loss: 0.00000952
Iteration 17/1000 | Loss: 0.00000951
Iteration 18/1000 | Loss: 0.00000950
Iteration 19/1000 | Loss: 0.00000948
Iteration 20/1000 | Loss: 0.00000945
Iteration 21/1000 | Loss: 0.00000944
Iteration 22/1000 | Loss: 0.00000943
Iteration 23/1000 | Loss: 0.00000943
Iteration 24/1000 | Loss: 0.00000942
Iteration 25/1000 | Loss: 0.00000941
Iteration 26/1000 | Loss: 0.00000939
Iteration 27/1000 | Loss: 0.00000938
Iteration 28/1000 | Loss: 0.00000938
Iteration 29/1000 | Loss: 0.00000937
Iteration 30/1000 | Loss: 0.00000937
Iteration 31/1000 | Loss: 0.00000937
Iteration 32/1000 | Loss: 0.00000936
Iteration 33/1000 | Loss: 0.00000936
Iteration 34/1000 | Loss: 0.00000936
Iteration 35/1000 | Loss: 0.00000935
Iteration 36/1000 | Loss: 0.00000934
Iteration 37/1000 | Loss: 0.00000933
Iteration 38/1000 | Loss: 0.00000932
Iteration 39/1000 | Loss: 0.00000932
Iteration 40/1000 | Loss: 0.00000931
Iteration 41/1000 | Loss: 0.00000931
Iteration 42/1000 | Loss: 0.00000931
Iteration 43/1000 | Loss: 0.00000931
Iteration 44/1000 | Loss: 0.00000930
Iteration 45/1000 | Loss: 0.00000929
Iteration 46/1000 | Loss: 0.00000929
Iteration 47/1000 | Loss: 0.00000927
Iteration 48/1000 | Loss: 0.00000927
Iteration 49/1000 | Loss: 0.00000925
Iteration 50/1000 | Loss: 0.00000925
Iteration 51/1000 | Loss: 0.00000924
Iteration 52/1000 | Loss: 0.00000924
Iteration 53/1000 | Loss: 0.00000923
Iteration 54/1000 | Loss: 0.00000922
Iteration 55/1000 | Loss: 0.00000921
Iteration 56/1000 | Loss: 0.00000920
Iteration 57/1000 | Loss: 0.00000919
Iteration 58/1000 | Loss: 0.00000919
Iteration 59/1000 | Loss: 0.00000919
Iteration 60/1000 | Loss: 0.00000918
Iteration 61/1000 | Loss: 0.00000918
Iteration 62/1000 | Loss: 0.00000917
Iteration 63/1000 | Loss: 0.00000917
Iteration 64/1000 | Loss: 0.00000916
Iteration 65/1000 | Loss: 0.00000916
Iteration 66/1000 | Loss: 0.00000916
Iteration 67/1000 | Loss: 0.00000915
Iteration 68/1000 | Loss: 0.00000915
Iteration 69/1000 | Loss: 0.00000915
Iteration 70/1000 | Loss: 0.00000915
Iteration 71/1000 | Loss: 0.00000914
Iteration 72/1000 | Loss: 0.00000914
Iteration 73/1000 | Loss: 0.00000914
Iteration 74/1000 | Loss: 0.00000914
Iteration 75/1000 | Loss: 0.00000914
Iteration 76/1000 | Loss: 0.00000914
Iteration 77/1000 | Loss: 0.00000913
Iteration 78/1000 | Loss: 0.00000912
Iteration 79/1000 | Loss: 0.00000912
Iteration 80/1000 | Loss: 0.00000912
Iteration 81/1000 | Loss: 0.00000912
Iteration 82/1000 | Loss: 0.00000911
Iteration 83/1000 | Loss: 0.00000911
Iteration 84/1000 | Loss: 0.00000910
Iteration 85/1000 | Loss: 0.00000910
Iteration 86/1000 | Loss: 0.00000909
Iteration 87/1000 | Loss: 0.00000909
Iteration 88/1000 | Loss: 0.00000909
Iteration 89/1000 | Loss: 0.00000909
Iteration 90/1000 | Loss: 0.00000909
Iteration 91/1000 | Loss: 0.00000908
Iteration 92/1000 | Loss: 0.00000908
Iteration 93/1000 | Loss: 0.00000908
Iteration 94/1000 | Loss: 0.00000908
Iteration 95/1000 | Loss: 0.00000908
Iteration 96/1000 | Loss: 0.00000908
Iteration 97/1000 | Loss: 0.00000908
Iteration 98/1000 | Loss: 0.00000908
Iteration 99/1000 | Loss: 0.00000908
Iteration 100/1000 | Loss: 0.00000908
Iteration 101/1000 | Loss: 0.00000907
Iteration 102/1000 | Loss: 0.00000907
Iteration 103/1000 | Loss: 0.00000907
Iteration 104/1000 | Loss: 0.00000906
Iteration 105/1000 | Loss: 0.00000906
Iteration 106/1000 | Loss: 0.00000906
Iteration 107/1000 | Loss: 0.00000906
Iteration 108/1000 | Loss: 0.00000906
Iteration 109/1000 | Loss: 0.00000906
Iteration 110/1000 | Loss: 0.00000906
Iteration 111/1000 | Loss: 0.00000906
Iteration 112/1000 | Loss: 0.00000905
Iteration 113/1000 | Loss: 0.00000905
Iteration 114/1000 | Loss: 0.00000905
Iteration 115/1000 | Loss: 0.00000905
Iteration 116/1000 | Loss: 0.00000905
Iteration 117/1000 | Loss: 0.00000905
Iteration 118/1000 | Loss: 0.00000905
Iteration 119/1000 | Loss: 0.00000905
Iteration 120/1000 | Loss: 0.00000904
Iteration 121/1000 | Loss: 0.00000904
Iteration 122/1000 | Loss: 0.00000904
Iteration 123/1000 | Loss: 0.00000904
Iteration 124/1000 | Loss: 0.00000904
Iteration 125/1000 | Loss: 0.00000904
Iteration 126/1000 | Loss: 0.00000904
Iteration 127/1000 | Loss: 0.00000904
Iteration 128/1000 | Loss: 0.00000904
Iteration 129/1000 | Loss: 0.00000903
Iteration 130/1000 | Loss: 0.00000903
Iteration 131/1000 | Loss: 0.00000903
Iteration 132/1000 | Loss: 0.00000903
Iteration 133/1000 | Loss: 0.00000903
Iteration 134/1000 | Loss: 0.00000903
Iteration 135/1000 | Loss: 0.00000902
Iteration 136/1000 | Loss: 0.00000902
Iteration 137/1000 | Loss: 0.00000902
Iteration 138/1000 | Loss: 0.00000902
Iteration 139/1000 | Loss: 0.00000902
Iteration 140/1000 | Loss: 0.00000901
Iteration 141/1000 | Loss: 0.00000901
Iteration 142/1000 | Loss: 0.00000901
Iteration 143/1000 | Loss: 0.00000900
Iteration 144/1000 | Loss: 0.00000900
Iteration 145/1000 | Loss: 0.00000899
Iteration 146/1000 | Loss: 0.00000899
Iteration 147/1000 | Loss: 0.00000899
Iteration 148/1000 | Loss: 0.00000899
Iteration 149/1000 | Loss: 0.00000899
Iteration 150/1000 | Loss: 0.00000899
Iteration 151/1000 | Loss: 0.00000899
Iteration 152/1000 | Loss: 0.00000899
Iteration 153/1000 | Loss: 0.00000899
Iteration 154/1000 | Loss: 0.00000899
Iteration 155/1000 | Loss: 0.00000899
Iteration 156/1000 | Loss: 0.00000898
Iteration 157/1000 | Loss: 0.00000898
Iteration 158/1000 | Loss: 0.00000898
Iteration 159/1000 | Loss: 0.00000898
Iteration 160/1000 | Loss: 0.00000898
Iteration 161/1000 | Loss: 0.00000898
Iteration 162/1000 | Loss: 0.00000898
Iteration 163/1000 | Loss: 0.00000898
Iteration 164/1000 | Loss: 0.00000898
Iteration 165/1000 | Loss: 0.00000898
Iteration 166/1000 | Loss: 0.00000898
Iteration 167/1000 | Loss: 0.00000898
Iteration 168/1000 | Loss: 0.00000898
Iteration 169/1000 | Loss: 0.00000898
Iteration 170/1000 | Loss: 0.00000898
Iteration 171/1000 | Loss: 0.00000897
Iteration 172/1000 | Loss: 0.00000897
Iteration 173/1000 | Loss: 0.00000897
Iteration 174/1000 | Loss: 0.00000897
Iteration 175/1000 | Loss: 0.00000897
Iteration 176/1000 | Loss: 0.00000897
Iteration 177/1000 | Loss: 0.00000897
Iteration 178/1000 | Loss: 0.00000897
Iteration 179/1000 | Loss: 0.00000897
Iteration 180/1000 | Loss: 0.00000897
Iteration 181/1000 | Loss: 0.00000897
Iteration 182/1000 | Loss: 0.00000897
Iteration 183/1000 | Loss: 0.00000897
Iteration 184/1000 | Loss: 0.00000897
Iteration 185/1000 | Loss: 0.00000897
Iteration 186/1000 | Loss: 0.00000897
Iteration 187/1000 | Loss: 0.00000897
Iteration 188/1000 | Loss: 0.00000897
Iteration 189/1000 | Loss: 0.00000897
Iteration 190/1000 | Loss: 0.00000897
Iteration 191/1000 | Loss: 0.00000897
Iteration 192/1000 | Loss: 0.00000897
Iteration 193/1000 | Loss: 0.00000897
Iteration 194/1000 | Loss: 0.00000897
Iteration 195/1000 | Loss: 0.00000897
Iteration 196/1000 | Loss: 0.00000897
Iteration 197/1000 | Loss: 0.00000897
Iteration 198/1000 | Loss: 0.00000897
Iteration 199/1000 | Loss: 0.00000897
Iteration 200/1000 | Loss: 0.00000897
Iteration 201/1000 | Loss: 0.00000897
Iteration 202/1000 | Loss: 0.00000897
Iteration 203/1000 | Loss: 0.00000897
Iteration 204/1000 | Loss: 0.00000897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [8.965532288129907e-06, 8.965532288129907e-06, 8.965532288129907e-06, 8.965532288129907e-06, 8.965532288129907e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.965532288129907e-06

Optimization complete. Final v2v error: 2.6001980304718018 mm

Highest mean error: 2.807648181915283 mm for frame 107

Lowest mean error: 2.4626917839050293 mm for frame 87

Saving results

Total time: 36.78748416900635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00757818
Iteration 2/25 | Loss: 0.00166896
Iteration 3/25 | Loss: 0.00136586
Iteration 4/25 | Loss: 0.00135833
Iteration 5/25 | Loss: 0.00139122
Iteration 6/25 | Loss: 0.00132442
Iteration 7/25 | Loss: 0.00129667
Iteration 8/25 | Loss: 0.00129160
Iteration 9/25 | Loss: 0.00129007
Iteration 10/25 | Loss: 0.00128499
Iteration 11/25 | Loss: 0.00128367
Iteration 12/25 | Loss: 0.00128329
Iteration 13/25 | Loss: 0.00128303
Iteration 14/25 | Loss: 0.00128287
Iteration 15/25 | Loss: 0.00128666
Iteration 16/25 | Loss: 0.00128216
Iteration 17/25 | Loss: 0.00128133
Iteration 18/25 | Loss: 0.00128118
Iteration 19/25 | Loss: 0.00128117
Iteration 20/25 | Loss: 0.00128117
Iteration 21/25 | Loss: 0.00128117
Iteration 22/25 | Loss: 0.00128117
Iteration 23/25 | Loss: 0.00128117
Iteration 24/25 | Loss: 0.00128117
Iteration 25/25 | Loss: 0.00128117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 13.80322075
Iteration 2/25 | Loss: 0.00100573
Iteration 3/25 | Loss: 0.00100551
Iteration 4/25 | Loss: 0.00100551
Iteration 5/25 | Loss: 0.00100551
Iteration 6/25 | Loss: 0.00100551
Iteration 7/25 | Loss: 0.00100551
Iteration 8/25 | Loss: 0.00100551
Iteration 9/25 | Loss: 0.00100550
Iteration 10/25 | Loss: 0.00100550
Iteration 11/25 | Loss: 0.00100550
Iteration 12/25 | Loss: 0.00100550
Iteration 13/25 | Loss: 0.00100550
Iteration 14/25 | Loss: 0.00100550
Iteration 15/25 | Loss: 0.00100550
Iteration 16/25 | Loss: 0.00100550
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001005504629574716, 0.001005504629574716, 0.001005504629574716, 0.001005504629574716, 0.001005504629574716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001005504629574716

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100550
Iteration 2/1000 | Loss: 0.00002695
Iteration 3/1000 | Loss: 0.00002007
Iteration 4/1000 | Loss: 0.00001831
Iteration 5/1000 | Loss: 0.00001741
Iteration 6/1000 | Loss: 0.00001687
Iteration 7/1000 | Loss: 0.00001627
Iteration 8/1000 | Loss: 0.00001570
Iteration 9/1000 | Loss: 0.00001539
Iteration 10/1000 | Loss: 0.00001515
Iteration 11/1000 | Loss: 0.00001498
Iteration 12/1000 | Loss: 0.00001491
Iteration 13/1000 | Loss: 0.00001488
Iteration 14/1000 | Loss: 0.00001484
Iteration 15/1000 | Loss: 0.00001483
Iteration 16/1000 | Loss: 0.00001483
Iteration 17/1000 | Loss: 0.00001480
Iteration 18/1000 | Loss: 0.00001476
Iteration 19/1000 | Loss: 0.00001475
Iteration 20/1000 | Loss: 0.00001472
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001471
Iteration 23/1000 | Loss: 0.00001470
Iteration 24/1000 | Loss: 0.00001470
Iteration 25/1000 | Loss: 0.00001469
Iteration 26/1000 | Loss: 0.00001468
Iteration 27/1000 | Loss: 0.00001467
Iteration 28/1000 | Loss: 0.00001466
Iteration 29/1000 | Loss: 0.00001466
Iteration 30/1000 | Loss: 0.00001466
Iteration 31/1000 | Loss: 0.00001464
Iteration 32/1000 | Loss: 0.00001464
Iteration 33/1000 | Loss: 0.00001463
Iteration 34/1000 | Loss: 0.00001462
Iteration 35/1000 | Loss: 0.00001461
Iteration 36/1000 | Loss: 0.00001460
Iteration 37/1000 | Loss: 0.00001450
Iteration 38/1000 | Loss: 0.00001450
Iteration 39/1000 | Loss: 0.00001450
Iteration 40/1000 | Loss: 0.00001450
Iteration 41/1000 | Loss: 0.00001449
Iteration 42/1000 | Loss: 0.00001448
Iteration 43/1000 | Loss: 0.00001448
Iteration 44/1000 | Loss: 0.00001448
Iteration 45/1000 | Loss: 0.00001447
Iteration 46/1000 | Loss: 0.00001445
Iteration 47/1000 | Loss: 0.00001444
Iteration 48/1000 | Loss: 0.00001444
Iteration 49/1000 | Loss: 0.00001443
Iteration 50/1000 | Loss: 0.00001443
Iteration 51/1000 | Loss: 0.00001443
Iteration 52/1000 | Loss: 0.00001443
Iteration 53/1000 | Loss: 0.00001442
Iteration 54/1000 | Loss: 0.00001442
Iteration 55/1000 | Loss: 0.00001442
Iteration 56/1000 | Loss: 0.00001442
Iteration 57/1000 | Loss: 0.00001442
Iteration 58/1000 | Loss: 0.00001442
Iteration 59/1000 | Loss: 0.00001442
Iteration 60/1000 | Loss: 0.00001442
Iteration 61/1000 | Loss: 0.00001441
Iteration 62/1000 | Loss: 0.00001441
Iteration 63/1000 | Loss: 0.00001440
Iteration 64/1000 | Loss: 0.00001439
Iteration 65/1000 | Loss: 0.00001439
Iteration 66/1000 | Loss: 0.00001439
Iteration 67/1000 | Loss: 0.00001439
Iteration 68/1000 | Loss: 0.00001437
Iteration 69/1000 | Loss: 0.00001437
Iteration 70/1000 | Loss: 0.00001437
Iteration 71/1000 | Loss: 0.00001436
Iteration 72/1000 | Loss: 0.00001436
Iteration 73/1000 | Loss: 0.00001436
Iteration 74/1000 | Loss: 0.00001435
Iteration 75/1000 | Loss: 0.00001435
Iteration 76/1000 | Loss: 0.00001435
Iteration 77/1000 | Loss: 0.00001435
Iteration 78/1000 | Loss: 0.00001435
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001430
Iteration 81/1000 | Loss: 0.00001430
Iteration 82/1000 | Loss: 0.00001430
Iteration 83/1000 | Loss: 0.00001430
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001429
Iteration 86/1000 | Loss: 0.00001429
Iteration 87/1000 | Loss: 0.00001429
Iteration 88/1000 | Loss: 0.00001429
Iteration 89/1000 | Loss: 0.00001428
Iteration 90/1000 | Loss: 0.00001428
Iteration 91/1000 | Loss: 0.00001428
Iteration 92/1000 | Loss: 0.00001427
Iteration 93/1000 | Loss: 0.00001427
Iteration 94/1000 | Loss: 0.00001427
Iteration 95/1000 | Loss: 0.00001426
Iteration 96/1000 | Loss: 0.00001426
Iteration 97/1000 | Loss: 0.00001426
Iteration 98/1000 | Loss: 0.00001426
Iteration 99/1000 | Loss: 0.00001426
Iteration 100/1000 | Loss: 0.00001426
Iteration 101/1000 | Loss: 0.00001426
Iteration 102/1000 | Loss: 0.00001426
Iteration 103/1000 | Loss: 0.00001426
Iteration 104/1000 | Loss: 0.00001426
Iteration 105/1000 | Loss: 0.00001426
Iteration 106/1000 | Loss: 0.00001425
Iteration 107/1000 | Loss: 0.00001425
Iteration 108/1000 | Loss: 0.00001425
Iteration 109/1000 | Loss: 0.00001425
Iteration 110/1000 | Loss: 0.00001425
Iteration 111/1000 | Loss: 0.00001425
Iteration 112/1000 | Loss: 0.00001425
Iteration 113/1000 | Loss: 0.00001425
Iteration 114/1000 | Loss: 0.00001425
Iteration 115/1000 | Loss: 0.00001425
Iteration 116/1000 | Loss: 0.00001425
Iteration 117/1000 | Loss: 0.00001425
Iteration 118/1000 | Loss: 0.00001425
Iteration 119/1000 | Loss: 0.00001425
Iteration 120/1000 | Loss: 0.00001424
Iteration 121/1000 | Loss: 0.00001424
Iteration 122/1000 | Loss: 0.00001424
Iteration 123/1000 | Loss: 0.00001424
Iteration 124/1000 | Loss: 0.00001424
Iteration 125/1000 | Loss: 0.00001424
Iteration 126/1000 | Loss: 0.00001424
Iteration 127/1000 | Loss: 0.00001423
Iteration 128/1000 | Loss: 0.00001423
Iteration 129/1000 | Loss: 0.00001423
Iteration 130/1000 | Loss: 0.00001423
Iteration 131/1000 | Loss: 0.00001423
Iteration 132/1000 | Loss: 0.00001423
Iteration 133/1000 | Loss: 0.00001423
Iteration 134/1000 | Loss: 0.00001423
Iteration 135/1000 | Loss: 0.00001423
Iteration 136/1000 | Loss: 0.00001423
Iteration 137/1000 | Loss: 0.00001423
Iteration 138/1000 | Loss: 0.00001423
Iteration 139/1000 | Loss: 0.00001423
Iteration 140/1000 | Loss: 0.00001423
Iteration 141/1000 | Loss: 0.00001423
Iteration 142/1000 | Loss: 0.00001423
Iteration 143/1000 | Loss: 0.00001423
Iteration 144/1000 | Loss: 0.00001423
Iteration 145/1000 | Loss: 0.00001423
Iteration 146/1000 | Loss: 0.00001423
Iteration 147/1000 | Loss: 0.00001423
Iteration 148/1000 | Loss: 0.00001423
Iteration 149/1000 | Loss: 0.00001423
Iteration 150/1000 | Loss: 0.00001423
Iteration 151/1000 | Loss: 0.00001423
Iteration 152/1000 | Loss: 0.00001423
Iteration 153/1000 | Loss: 0.00001423
Iteration 154/1000 | Loss: 0.00001423
Iteration 155/1000 | Loss: 0.00001423
Iteration 156/1000 | Loss: 0.00001423
Iteration 157/1000 | Loss: 0.00001423
Iteration 158/1000 | Loss: 0.00001423
Iteration 159/1000 | Loss: 0.00001423
Iteration 160/1000 | Loss: 0.00001423
Iteration 161/1000 | Loss: 0.00001423
Iteration 162/1000 | Loss: 0.00001423
Iteration 163/1000 | Loss: 0.00001423
Iteration 164/1000 | Loss: 0.00001423
Iteration 165/1000 | Loss: 0.00001423
Iteration 166/1000 | Loss: 0.00001423
Iteration 167/1000 | Loss: 0.00001423
Iteration 168/1000 | Loss: 0.00001423
Iteration 169/1000 | Loss: 0.00001423
Iteration 170/1000 | Loss: 0.00001423
Iteration 171/1000 | Loss: 0.00001423
Iteration 172/1000 | Loss: 0.00001423
Iteration 173/1000 | Loss: 0.00001423
Iteration 174/1000 | Loss: 0.00001423
Iteration 175/1000 | Loss: 0.00001423
Iteration 176/1000 | Loss: 0.00001423
Iteration 177/1000 | Loss: 0.00001423
Iteration 178/1000 | Loss: 0.00001423
Iteration 179/1000 | Loss: 0.00001423
Iteration 180/1000 | Loss: 0.00001423
Iteration 181/1000 | Loss: 0.00001423
Iteration 182/1000 | Loss: 0.00001423
Iteration 183/1000 | Loss: 0.00001423
Iteration 184/1000 | Loss: 0.00001423
Iteration 185/1000 | Loss: 0.00001423
Iteration 186/1000 | Loss: 0.00001423
Iteration 187/1000 | Loss: 0.00001423
Iteration 188/1000 | Loss: 0.00001423
Iteration 189/1000 | Loss: 0.00001423
Iteration 190/1000 | Loss: 0.00001423
Iteration 191/1000 | Loss: 0.00001423
Iteration 192/1000 | Loss: 0.00001423
Iteration 193/1000 | Loss: 0.00001423
Iteration 194/1000 | Loss: 0.00001423
Iteration 195/1000 | Loss: 0.00001423
Iteration 196/1000 | Loss: 0.00001423
Iteration 197/1000 | Loss: 0.00001423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.423206504114205e-05, 1.423206504114205e-05, 1.423206504114205e-05, 1.423206504114205e-05, 1.423206504114205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.423206504114205e-05

Optimization complete. Final v2v error: 3.234956741333008 mm

Highest mean error: 3.463947296142578 mm for frame 12

Lowest mean error: 3.014141798019409 mm for frame 96

Saving results

Total time: 70.7923309803009
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901037
Iteration 2/25 | Loss: 0.00296618
Iteration 3/25 | Loss: 0.00228414
Iteration 4/25 | Loss: 0.00195125
Iteration 5/25 | Loss: 0.00182433
Iteration 6/25 | Loss: 0.00183806
Iteration 7/25 | Loss: 0.00189541
Iteration 8/25 | Loss: 0.00176313
Iteration 9/25 | Loss: 0.00168453
Iteration 10/25 | Loss: 0.00166246
Iteration 11/25 | Loss: 0.00165588
Iteration 12/25 | Loss: 0.00166184
Iteration 13/25 | Loss: 0.00165226
Iteration 14/25 | Loss: 0.00163391
Iteration 15/25 | Loss: 0.00163333
Iteration 16/25 | Loss: 0.00161053
Iteration 17/25 | Loss: 0.00162887
Iteration 18/25 | Loss: 0.00161466
Iteration 19/25 | Loss: 0.00161655
Iteration 20/25 | Loss: 0.00160917
Iteration 21/25 | Loss: 0.00160879
Iteration 22/25 | Loss: 0.00163627
Iteration 23/25 | Loss: 0.00163121
Iteration 24/25 | Loss: 0.00161772
Iteration 25/25 | Loss: 0.00161566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.25964403
Iteration 2/25 | Loss: 0.00728625
Iteration 3/25 | Loss: 0.00543700
Iteration 4/25 | Loss: 0.00543700
Iteration 5/25 | Loss: 0.00543700
Iteration 6/25 | Loss: 0.00543700
Iteration 7/25 | Loss: 0.00543699
Iteration 8/25 | Loss: 0.00543699
Iteration 9/25 | Loss: 0.00543699
Iteration 10/25 | Loss: 0.00543699
Iteration 11/25 | Loss: 0.00543699
Iteration 12/25 | Loss: 0.00543699
Iteration 13/25 | Loss: 0.00543699
Iteration 14/25 | Loss: 0.00543699
Iteration 15/25 | Loss: 0.00543699
Iteration 16/25 | Loss: 0.00543699
Iteration 17/25 | Loss: 0.00543699
Iteration 18/25 | Loss: 0.00543699
Iteration 19/25 | Loss: 0.00543699
Iteration 20/25 | Loss: 0.00543699
Iteration 21/25 | Loss: 0.00543699
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.005436992738395929, 0.005436992738395929, 0.005436992738395929, 0.005436992738395929, 0.005436992738395929]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005436992738395929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00543699
Iteration 2/1000 | Loss: 0.00144253
Iteration 3/1000 | Loss: 0.00234729
Iteration 4/1000 | Loss: 0.00121351
Iteration 5/1000 | Loss: 0.00149257
Iteration 6/1000 | Loss: 0.00347951
Iteration 7/1000 | Loss: 0.00283023
Iteration 8/1000 | Loss: 0.00357137
Iteration 9/1000 | Loss: 0.00173775
Iteration 10/1000 | Loss: 0.00592977
Iteration 11/1000 | Loss: 0.00539542
Iteration 12/1000 | Loss: 0.00194907
Iteration 13/1000 | Loss: 0.00742960
Iteration 14/1000 | Loss: 0.00340797
Iteration 15/1000 | Loss: 0.00457373
Iteration 16/1000 | Loss: 0.00543840
Iteration 17/1000 | Loss: 0.00395809
Iteration 18/1000 | Loss: 0.00397870
Iteration 19/1000 | Loss: 0.00405145
Iteration 20/1000 | Loss: 0.00887192
Iteration 21/1000 | Loss: 0.00238375
Iteration 22/1000 | Loss: 0.00146581
Iteration 23/1000 | Loss: 0.00099299
Iteration 24/1000 | Loss: 0.00140772
Iteration 25/1000 | Loss: 0.00365214
Iteration 26/1000 | Loss: 0.00069915
Iteration 27/1000 | Loss: 0.00096442
Iteration 28/1000 | Loss: 0.00209308
Iteration 29/1000 | Loss: 0.00086770
Iteration 30/1000 | Loss: 0.00356961
Iteration 31/1000 | Loss: 0.00139754
Iteration 32/1000 | Loss: 0.00406004
Iteration 33/1000 | Loss: 0.00138970
Iteration 34/1000 | Loss: 0.00154068
Iteration 35/1000 | Loss: 0.00084926
Iteration 36/1000 | Loss: 0.00249430
Iteration 37/1000 | Loss: 0.00211235
Iteration 38/1000 | Loss: 0.00250277
Iteration 39/1000 | Loss: 0.00120413
Iteration 40/1000 | Loss: 0.00101156
Iteration 41/1000 | Loss: 0.00051216
Iteration 42/1000 | Loss: 0.00149543
Iteration 43/1000 | Loss: 0.00272200
Iteration 44/1000 | Loss: 0.00090144
Iteration 45/1000 | Loss: 0.00031480
Iteration 46/1000 | Loss: 0.00066420
Iteration 47/1000 | Loss: 0.00068582
Iteration 48/1000 | Loss: 0.00036451
Iteration 49/1000 | Loss: 0.00044471
Iteration 50/1000 | Loss: 0.00056247
Iteration 51/1000 | Loss: 0.00035650
Iteration 52/1000 | Loss: 0.00099404
Iteration 53/1000 | Loss: 0.00026279
Iteration 54/1000 | Loss: 0.00024860
Iteration 55/1000 | Loss: 0.00055791
Iteration 56/1000 | Loss: 0.00027495
Iteration 57/1000 | Loss: 0.00016597
Iteration 58/1000 | Loss: 0.00224100
Iteration 59/1000 | Loss: 0.00017521
Iteration 60/1000 | Loss: 0.00015847
Iteration 61/1000 | Loss: 0.00067517
Iteration 62/1000 | Loss: 0.00044674
Iteration 63/1000 | Loss: 0.00038700
Iteration 64/1000 | Loss: 0.00026206
Iteration 65/1000 | Loss: 0.00081313
Iteration 66/1000 | Loss: 0.00054743
Iteration 67/1000 | Loss: 0.00066313
Iteration 68/1000 | Loss: 0.00033262
Iteration 69/1000 | Loss: 0.00205873
Iteration 70/1000 | Loss: 0.00312148
Iteration 71/1000 | Loss: 0.00041551
Iteration 72/1000 | Loss: 0.00024496
Iteration 73/1000 | Loss: 0.00082940
Iteration 74/1000 | Loss: 0.01102738
Iteration 75/1000 | Loss: 0.00862385
Iteration 76/1000 | Loss: 0.00330988
Iteration 77/1000 | Loss: 0.00076232
Iteration 78/1000 | Loss: 0.00046764
Iteration 79/1000 | Loss: 0.00104511
Iteration 80/1000 | Loss: 0.00069715
Iteration 81/1000 | Loss: 0.00464102
Iteration 82/1000 | Loss: 0.00142059
Iteration 83/1000 | Loss: 0.00076470
Iteration 84/1000 | Loss: 0.01250614
Iteration 85/1000 | Loss: 0.00524650
Iteration 86/1000 | Loss: 0.00234053
Iteration 87/1000 | Loss: 0.00246125
Iteration 88/1000 | Loss: 0.00878981
Iteration 89/1000 | Loss: 0.00300855
Iteration 90/1000 | Loss: 0.00231780
Iteration 91/1000 | Loss: 0.00544571
Iteration 92/1000 | Loss: 0.00404992
Iteration 93/1000 | Loss: 0.00645547
Iteration 94/1000 | Loss: 0.00430688
Iteration 95/1000 | Loss: 0.00502786
Iteration 96/1000 | Loss: 0.00169223
Iteration 97/1000 | Loss: 0.00336214
Iteration 98/1000 | Loss: 0.00297536
Iteration 99/1000 | Loss: 0.00487008
Iteration 100/1000 | Loss: 0.00536427
Iteration 101/1000 | Loss: 0.00554410
Iteration 102/1000 | Loss: 0.00103652
Iteration 103/1000 | Loss: 0.00139351
Iteration 104/1000 | Loss: 0.00171151
Iteration 105/1000 | Loss: 0.00101817
Iteration 106/1000 | Loss: 0.00069908
Iteration 107/1000 | Loss: 0.00113643
Iteration 108/1000 | Loss: 0.00074115
Iteration 109/1000 | Loss: 0.00057975
Iteration 110/1000 | Loss: 0.00103108
Iteration 111/1000 | Loss: 0.00047764
Iteration 112/1000 | Loss: 0.00034946
Iteration 113/1000 | Loss: 0.00028689
Iteration 114/1000 | Loss: 0.00064026
Iteration 115/1000 | Loss: 0.00053582
Iteration 116/1000 | Loss: 0.00076848
Iteration 117/1000 | Loss: 0.00057289
Iteration 118/1000 | Loss: 0.00089385
Iteration 119/1000 | Loss: 0.00092281
Iteration 120/1000 | Loss: 0.00014532
Iteration 121/1000 | Loss: 0.00042917
Iteration 122/1000 | Loss: 0.00055082
Iteration 123/1000 | Loss: 0.00051573
Iteration 124/1000 | Loss: 0.00068076
Iteration 125/1000 | Loss: 0.00021378
Iteration 126/1000 | Loss: 0.00023871
Iteration 127/1000 | Loss: 0.00010562
Iteration 128/1000 | Loss: 0.00009695
Iteration 129/1000 | Loss: 0.00009379
Iteration 130/1000 | Loss: 0.00013341
Iteration 131/1000 | Loss: 0.00053057
Iteration 132/1000 | Loss: 0.00095182
Iteration 133/1000 | Loss: 0.00102949
Iteration 134/1000 | Loss: 0.00079955
Iteration 135/1000 | Loss: 0.00030101
Iteration 136/1000 | Loss: 0.00020360
Iteration 137/1000 | Loss: 0.00117950
Iteration 138/1000 | Loss: 0.00051035
Iteration 139/1000 | Loss: 0.00045875
Iteration 140/1000 | Loss: 0.00010254
Iteration 141/1000 | Loss: 0.00038742
Iteration 142/1000 | Loss: 0.00022960
Iteration 143/1000 | Loss: 0.00136233
Iteration 144/1000 | Loss: 0.00095559
Iteration 145/1000 | Loss: 0.00233268
Iteration 146/1000 | Loss: 0.00855945
Iteration 147/1000 | Loss: 0.00392391
Iteration 148/1000 | Loss: 0.00336300
Iteration 149/1000 | Loss: 0.00146441
Iteration 150/1000 | Loss: 0.00039820
Iteration 151/1000 | Loss: 0.00022583
Iteration 152/1000 | Loss: 0.00038478
Iteration 153/1000 | Loss: 0.00013730
Iteration 154/1000 | Loss: 0.00076106
Iteration 155/1000 | Loss: 0.00392013
Iteration 156/1000 | Loss: 0.00266440
Iteration 157/1000 | Loss: 0.00319594
Iteration 158/1000 | Loss: 0.00070646
Iteration 159/1000 | Loss: 0.00049062
Iteration 160/1000 | Loss: 0.00048321
Iteration 161/1000 | Loss: 0.00284986
Iteration 162/1000 | Loss: 0.00082298
Iteration 163/1000 | Loss: 0.00068323
Iteration 164/1000 | Loss: 0.00217493
Iteration 165/1000 | Loss: 0.00066818
Iteration 166/1000 | Loss: 0.00036811
Iteration 167/1000 | Loss: 0.00076455
Iteration 168/1000 | Loss: 0.00039104
Iteration 169/1000 | Loss: 0.00172068
Iteration 170/1000 | Loss: 0.00085569
Iteration 171/1000 | Loss: 0.00092131
Iteration 172/1000 | Loss: 0.00075506
Iteration 173/1000 | Loss: 0.00047842
Iteration 174/1000 | Loss: 0.00046187
Iteration 175/1000 | Loss: 0.00043757
Iteration 176/1000 | Loss: 0.00031486
Iteration 177/1000 | Loss: 0.00027525
Iteration 178/1000 | Loss: 0.00014741
Iteration 179/1000 | Loss: 0.00010585
Iteration 180/1000 | Loss: 0.00005217
Iteration 181/1000 | Loss: 0.00004670
Iteration 182/1000 | Loss: 0.00004341
Iteration 183/1000 | Loss: 0.00060578
Iteration 184/1000 | Loss: 0.00039485
Iteration 185/1000 | Loss: 0.00040723
Iteration 186/1000 | Loss: 0.00004634
Iteration 187/1000 | Loss: 0.00004258
Iteration 188/1000 | Loss: 0.00010798
Iteration 189/1000 | Loss: 0.00005417
Iteration 190/1000 | Loss: 0.00011608
Iteration 191/1000 | Loss: 0.00008679
Iteration 192/1000 | Loss: 0.00011244
Iteration 193/1000 | Loss: 0.00007796
Iteration 194/1000 | Loss: 0.00007892
Iteration 195/1000 | Loss: 0.00009203
Iteration 196/1000 | Loss: 0.00011912
Iteration 197/1000 | Loss: 0.00012356
Iteration 198/1000 | Loss: 0.00013878
Iteration 199/1000 | Loss: 0.00011217
Iteration 200/1000 | Loss: 0.00004897
Iteration 201/1000 | Loss: 0.00003448
Iteration 202/1000 | Loss: 0.00003163
Iteration 203/1000 | Loss: 0.00147932
Iteration 204/1000 | Loss: 0.00152216
Iteration 205/1000 | Loss: 0.00051637
Iteration 206/1000 | Loss: 0.00073703
Iteration 207/1000 | Loss: 0.00040429
Iteration 208/1000 | Loss: 0.00041766
Iteration 209/1000 | Loss: 0.00035186
Iteration 210/1000 | Loss: 0.00015159
Iteration 211/1000 | Loss: 0.00011870
Iteration 212/1000 | Loss: 0.00012615
Iteration 213/1000 | Loss: 0.00037445
Iteration 214/1000 | Loss: 0.00024578
Iteration 215/1000 | Loss: 0.00014402
Iteration 216/1000 | Loss: 0.00020922
Iteration 217/1000 | Loss: 0.00012476
Iteration 218/1000 | Loss: 0.00178462
Iteration 219/1000 | Loss: 0.00036777
Iteration 220/1000 | Loss: 0.00037761
Iteration 221/1000 | Loss: 0.00024891
Iteration 222/1000 | Loss: 0.00041086
Iteration 223/1000 | Loss: 0.00288020
Iteration 224/1000 | Loss: 0.00015595
Iteration 225/1000 | Loss: 0.00040794
Iteration 226/1000 | Loss: 0.00010357
Iteration 227/1000 | Loss: 0.00004059
Iteration 228/1000 | Loss: 0.00032373
Iteration 229/1000 | Loss: 0.00041970
Iteration 230/1000 | Loss: 0.00024682
Iteration 231/1000 | Loss: 0.00056123
Iteration 232/1000 | Loss: 0.00019979
Iteration 233/1000 | Loss: 0.00034425
Iteration 234/1000 | Loss: 0.00031081
Iteration 235/1000 | Loss: 0.00013531
Iteration 236/1000 | Loss: 0.00044439
Iteration 237/1000 | Loss: 0.00016290
Iteration 238/1000 | Loss: 0.00006458
Iteration 239/1000 | Loss: 0.00067097
Iteration 240/1000 | Loss: 0.00081493
Iteration 241/1000 | Loss: 0.00065416
Iteration 242/1000 | Loss: 0.00079536
Iteration 243/1000 | Loss: 0.00059046
Iteration 244/1000 | Loss: 0.00063911
Iteration 245/1000 | Loss: 0.00108946
Iteration 246/1000 | Loss: 0.00050437
Iteration 247/1000 | Loss: 0.00084838
Iteration 248/1000 | Loss: 0.00005655
Iteration 249/1000 | Loss: 0.00037880
Iteration 250/1000 | Loss: 0.00008026
Iteration 251/1000 | Loss: 0.00003509
Iteration 252/1000 | Loss: 0.00002995
Iteration 253/1000 | Loss: 0.00003693
Iteration 254/1000 | Loss: 0.00002684
Iteration 255/1000 | Loss: 0.00003279
Iteration 256/1000 | Loss: 0.00003479
Iteration 257/1000 | Loss: 0.00002340
Iteration 258/1000 | Loss: 0.00004070
Iteration 259/1000 | Loss: 0.00002408
Iteration 260/1000 | Loss: 0.00033236
Iteration 261/1000 | Loss: 0.00008989
Iteration 262/1000 | Loss: 0.00003903
Iteration 263/1000 | Loss: 0.00002084
Iteration 264/1000 | Loss: 0.00001965
Iteration 265/1000 | Loss: 0.00001900
Iteration 266/1000 | Loss: 0.00001869
Iteration 267/1000 | Loss: 0.00001845
Iteration 268/1000 | Loss: 0.00001841
Iteration 269/1000 | Loss: 0.00001821
Iteration 270/1000 | Loss: 0.00001797
Iteration 271/1000 | Loss: 0.00001781
Iteration 272/1000 | Loss: 0.00001780
Iteration 273/1000 | Loss: 0.00001775
Iteration 274/1000 | Loss: 0.00001773
Iteration 275/1000 | Loss: 0.00001772
Iteration 276/1000 | Loss: 0.00001768
Iteration 277/1000 | Loss: 0.00001763
Iteration 278/1000 | Loss: 0.00001762
Iteration 279/1000 | Loss: 0.00001759
Iteration 280/1000 | Loss: 0.00001759
Iteration 281/1000 | Loss: 0.00001759
Iteration 282/1000 | Loss: 0.00001759
Iteration 283/1000 | Loss: 0.00001759
Iteration 284/1000 | Loss: 0.00001759
Iteration 285/1000 | Loss: 0.00001759
Iteration 286/1000 | Loss: 0.00001759
Iteration 287/1000 | Loss: 0.00001759
Iteration 288/1000 | Loss: 0.00001759
Iteration 289/1000 | Loss: 0.00001758
Iteration 290/1000 | Loss: 0.00001758
Iteration 291/1000 | Loss: 0.00001758
Iteration 292/1000 | Loss: 0.00001758
Iteration 293/1000 | Loss: 0.00001757
Iteration 294/1000 | Loss: 0.00001756
Iteration 295/1000 | Loss: 0.00001756
Iteration 296/1000 | Loss: 0.00001755
Iteration 297/1000 | Loss: 0.00001755
Iteration 298/1000 | Loss: 0.00001755
Iteration 299/1000 | Loss: 0.00001755
Iteration 300/1000 | Loss: 0.00001754
Iteration 301/1000 | Loss: 0.00001754
Iteration 302/1000 | Loss: 0.00001754
Iteration 303/1000 | Loss: 0.00001754
Iteration 304/1000 | Loss: 0.00001753
Iteration 305/1000 | Loss: 0.00001753
Iteration 306/1000 | Loss: 0.00001753
Iteration 307/1000 | Loss: 0.00001752
Iteration 308/1000 | Loss: 0.00001752
Iteration 309/1000 | Loss: 0.00001751
Iteration 310/1000 | Loss: 0.00001751
Iteration 311/1000 | Loss: 0.00001751
Iteration 312/1000 | Loss: 0.00001751
Iteration 313/1000 | Loss: 0.00001750
Iteration 314/1000 | Loss: 0.00001750
Iteration 315/1000 | Loss: 0.00001750
Iteration 316/1000 | Loss: 0.00001750
Iteration 317/1000 | Loss: 0.00001750
Iteration 318/1000 | Loss: 0.00001750
Iteration 319/1000 | Loss: 0.00001749
Iteration 320/1000 | Loss: 0.00001749
Iteration 321/1000 | Loss: 0.00001749
Iteration 322/1000 | Loss: 0.00001749
Iteration 323/1000 | Loss: 0.00001749
Iteration 324/1000 | Loss: 0.00001749
Iteration 325/1000 | Loss: 0.00001749
Iteration 326/1000 | Loss: 0.00001749
Iteration 327/1000 | Loss: 0.00001748
Iteration 328/1000 | Loss: 0.00001748
Iteration 329/1000 | Loss: 0.00001748
Iteration 330/1000 | Loss: 0.00001748
Iteration 331/1000 | Loss: 0.00001748
Iteration 332/1000 | Loss: 0.00001748
Iteration 333/1000 | Loss: 0.00001748
Iteration 334/1000 | Loss: 0.00001748
Iteration 335/1000 | Loss: 0.00001748
Iteration 336/1000 | Loss: 0.00001748
Iteration 337/1000 | Loss: 0.00001748
Iteration 338/1000 | Loss: 0.00001748
Iteration 339/1000 | Loss: 0.00001748
Iteration 340/1000 | Loss: 0.00001748
Iteration 341/1000 | Loss: 0.00001748
Iteration 342/1000 | Loss: 0.00001748
Iteration 343/1000 | Loss: 0.00001748
Iteration 344/1000 | Loss: 0.00001748
Iteration 345/1000 | Loss: 0.00001748
Iteration 346/1000 | Loss: 0.00001747
Iteration 347/1000 | Loss: 0.00001747
Iteration 348/1000 | Loss: 0.00001747
Iteration 349/1000 | Loss: 0.00001747
Iteration 350/1000 | Loss: 0.00001747
Iteration 351/1000 | Loss: 0.00001747
Iteration 352/1000 | Loss: 0.00001747
Iteration 353/1000 | Loss: 0.00001747
Iteration 354/1000 | Loss: 0.00001747
Iteration 355/1000 | Loss: 0.00001747
Iteration 356/1000 | Loss: 0.00001746
Iteration 357/1000 | Loss: 0.00001746
Iteration 358/1000 | Loss: 0.00001746
Iteration 359/1000 | Loss: 0.00001746
Iteration 360/1000 | Loss: 0.00001746
Iteration 361/1000 | Loss: 0.00001746
Iteration 362/1000 | Loss: 0.00001746
Iteration 363/1000 | Loss: 0.00001746
Iteration 364/1000 | Loss: 0.00001746
Iteration 365/1000 | Loss: 0.00001745
Iteration 366/1000 | Loss: 0.00001745
Iteration 367/1000 | Loss: 0.00001745
Iteration 368/1000 | Loss: 0.00001745
Iteration 369/1000 | Loss: 0.00001745
Iteration 370/1000 | Loss: 0.00001745
Iteration 371/1000 | Loss: 0.00001745
Iteration 372/1000 | Loss: 0.00001745
Iteration 373/1000 | Loss: 0.00001745
Iteration 374/1000 | Loss: 0.00001745
Iteration 375/1000 | Loss: 0.00001745
Iteration 376/1000 | Loss: 0.00001745
Iteration 377/1000 | Loss: 0.00001745
Iteration 378/1000 | Loss: 0.00001745
Iteration 379/1000 | Loss: 0.00001745
Iteration 380/1000 | Loss: 0.00001745
Iteration 381/1000 | Loss: 0.00001745
Iteration 382/1000 | Loss: 0.00001745
Iteration 383/1000 | Loss: 0.00001745
Iteration 384/1000 | Loss: 0.00001745
Iteration 385/1000 | Loss: 0.00001745
Iteration 386/1000 | Loss: 0.00001745
Iteration 387/1000 | Loss: 0.00001745
Iteration 388/1000 | Loss: 0.00001745
Iteration 389/1000 | Loss: 0.00001745
Iteration 390/1000 | Loss: 0.00001745
Iteration 391/1000 | Loss: 0.00001745
Iteration 392/1000 | Loss: 0.00001745
Iteration 393/1000 | Loss: 0.00001745
Iteration 394/1000 | Loss: 0.00001745
Iteration 395/1000 | Loss: 0.00001745
Iteration 396/1000 | Loss: 0.00001745
Iteration 397/1000 | Loss: 0.00001745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 397. Stopping optimization.
Last 5 losses: [1.7446292986278422e-05, 1.7446292986278422e-05, 1.7446292986278422e-05, 1.7446292986278422e-05, 1.7446292986278422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7446292986278422e-05

Optimization complete. Final v2v error: 3.479297637939453 mm

Highest mean error: 5.767422199249268 mm for frame 61

Lowest mean error: 2.659144878387451 mm for frame 21

Saving results

Total time: 463.2950487136841
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00344126
Iteration 2/25 | Loss: 0.00134622
Iteration 3/25 | Loss: 0.00122150
Iteration 4/25 | Loss: 0.00120766
Iteration 5/25 | Loss: 0.00120373
Iteration 6/25 | Loss: 0.00120365
Iteration 7/25 | Loss: 0.00120365
Iteration 8/25 | Loss: 0.00120365
Iteration 9/25 | Loss: 0.00120365
Iteration 10/25 | Loss: 0.00120365
Iteration 11/25 | Loss: 0.00120365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001203646999783814, 0.001203646999783814, 0.001203646999783814, 0.001203646999783814, 0.001203646999783814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001203646999783814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25203037
Iteration 2/25 | Loss: 0.00168895
Iteration 3/25 | Loss: 0.00168895
Iteration 4/25 | Loss: 0.00168895
Iteration 5/25 | Loss: 0.00168895
Iteration 6/25 | Loss: 0.00168894
Iteration 7/25 | Loss: 0.00168894
Iteration 8/25 | Loss: 0.00168894
Iteration 9/25 | Loss: 0.00168894
Iteration 10/25 | Loss: 0.00168894
Iteration 11/25 | Loss: 0.00168894
Iteration 12/25 | Loss: 0.00168894
Iteration 13/25 | Loss: 0.00168894
Iteration 14/25 | Loss: 0.00168894
Iteration 15/25 | Loss: 0.00168894
Iteration 16/25 | Loss: 0.00168894
Iteration 17/25 | Loss: 0.00168894
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016889433609321713, 0.0016889433609321713, 0.0016889433609321713, 0.0016889433609321713, 0.0016889433609321713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016889433609321713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168894
Iteration 2/1000 | Loss: 0.00003290
Iteration 3/1000 | Loss: 0.00002230
Iteration 4/1000 | Loss: 0.00001862
Iteration 5/1000 | Loss: 0.00001654
Iteration 6/1000 | Loss: 0.00001548
Iteration 7/1000 | Loss: 0.00001463
Iteration 8/1000 | Loss: 0.00001417
Iteration 9/1000 | Loss: 0.00001384
Iteration 10/1000 | Loss: 0.00001355
Iteration 11/1000 | Loss: 0.00001319
Iteration 12/1000 | Loss: 0.00001301
Iteration 13/1000 | Loss: 0.00001290
Iteration 14/1000 | Loss: 0.00001286
Iteration 15/1000 | Loss: 0.00001284
Iteration 16/1000 | Loss: 0.00001284
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001269
Iteration 19/1000 | Loss: 0.00001266
Iteration 20/1000 | Loss: 0.00001255
Iteration 21/1000 | Loss: 0.00001253
Iteration 22/1000 | Loss: 0.00001253
Iteration 23/1000 | Loss: 0.00001252
Iteration 24/1000 | Loss: 0.00001252
Iteration 25/1000 | Loss: 0.00001248
Iteration 26/1000 | Loss: 0.00001247
Iteration 27/1000 | Loss: 0.00001241
Iteration 28/1000 | Loss: 0.00001236
Iteration 29/1000 | Loss: 0.00001235
Iteration 30/1000 | Loss: 0.00001233
Iteration 31/1000 | Loss: 0.00001232
Iteration 32/1000 | Loss: 0.00001232
Iteration 33/1000 | Loss: 0.00001228
Iteration 34/1000 | Loss: 0.00001227
Iteration 35/1000 | Loss: 0.00001220
Iteration 36/1000 | Loss: 0.00001218
Iteration 37/1000 | Loss: 0.00001218
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001212
Iteration 40/1000 | Loss: 0.00001212
Iteration 41/1000 | Loss: 0.00001211
Iteration 42/1000 | Loss: 0.00001210
Iteration 43/1000 | Loss: 0.00001209
Iteration 44/1000 | Loss: 0.00001208
Iteration 45/1000 | Loss: 0.00001207
Iteration 46/1000 | Loss: 0.00001207
Iteration 47/1000 | Loss: 0.00001207
Iteration 48/1000 | Loss: 0.00001206
Iteration 49/1000 | Loss: 0.00001206
Iteration 50/1000 | Loss: 0.00001205
Iteration 51/1000 | Loss: 0.00001205
Iteration 52/1000 | Loss: 0.00001204
Iteration 53/1000 | Loss: 0.00001204
Iteration 54/1000 | Loss: 0.00001204
Iteration 55/1000 | Loss: 0.00001203
Iteration 56/1000 | Loss: 0.00001203
Iteration 57/1000 | Loss: 0.00001203
Iteration 58/1000 | Loss: 0.00001203
Iteration 59/1000 | Loss: 0.00001202
Iteration 60/1000 | Loss: 0.00001202
Iteration 61/1000 | Loss: 0.00001202
Iteration 62/1000 | Loss: 0.00001202
Iteration 63/1000 | Loss: 0.00001201
Iteration 64/1000 | Loss: 0.00001201
Iteration 65/1000 | Loss: 0.00001201
Iteration 66/1000 | Loss: 0.00001201
Iteration 67/1000 | Loss: 0.00001200
Iteration 68/1000 | Loss: 0.00001200
Iteration 69/1000 | Loss: 0.00001200
Iteration 70/1000 | Loss: 0.00001199
Iteration 71/1000 | Loss: 0.00001199
Iteration 72/1000 | Loss: 0.00001198
Iteration 73/1000 | Loss: 0.00001198
Iteration 74/1000 | Loss: 0.00001197
Iteration 75/1000 | Loss: 0.00001197
Iteration 76/1000 | Loss: 0.00001196
Iteration 77/1000 | Loss: 0.00001194
Iteration 78/1000 | Loss: 0.00001193
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001191
Iteration 81/1000 | Loss: 0.00001191
Iteration 82/1000 | Loss: 0.00001191
Iteration 83/1000 | Loss: 0.00001191
Iteration 84/1000 | Loss: 0.00001190
Iteration 85/1000 | Loss: 0.00001189
Iteration 86/1000 | Loss: 0.00001189
Iteration 87/1000 | Loss: 0.00001189
Iteration 88/1000 | Loss: 0.00001189
Iteration 89/1000 | Loss: 0.00001189
Iteration 90/1000 | Loss: 0.00001189
Iteration 91/1000 | Loss: 0.00001189
Iteration 92/1000 | Loss: 0.00001187
Iteration 93/1000 | Loss: 0.00001187
Iteration 94/1000 | Loss: 0.00001187
Iteration 95/1000 | Loss: 0.00001187
Iteration 96/1000 | Loss: 0.00001187
Iteration 97/1000 | Loss: 0.00001187
Iteration 98/1000 | Loss: 0.00001187
Iteration 99/1000 | Loss: 0.00001186
Iteration 100/1000 | Loss: 0.00001185
Iteration 101/1000 | Loss: 0.00001184
Iteration 102/1000 | Loss: 0.00001184
Iteration 103/1000 | Loss: 0.00001184
Iteration 104/1000 | Loss: 0.00001183
Iteration 105/1000 | Loss: 0.00001181
Iteration 106/1000 | Loss: 0.00001181
Iteration 107/1000 | Loss: 0.00001181
Iteration 108/1000 | Loss: 0.00001181
Iteration 109/1000 | Loss: 0.00001180
Iteration 110/1000 | Loss: 0.00001180
Iteration 111/1000 | Loss: 0.00001180
Iteration 112/1000 | Loss: 0.00001180
Iteration 113/1000 | Loss: 0.00001180
Iteration 114/1000 | Loss: 0.00001180
Iteration 115/1000 | Loss: 0.00001180
Iteration 116/1000 | Loss: 0.00001180
Iteration 117/1000 | Loss: 0.00001179
Iteration 118/1000 | Loss: 0.00001178
Iteration 119/1000 | Loss: 0.00001178
Iteration 120/1000 | Loss: 0.00001178
Iteration 121/1000 | Loss: 0.00001178
Iteration 122/1000 | Loss: 0.00001177
Iteration 123/1000 | Loss: 0.00001177
Iteration 124/1000 | Loss: 0.00001176
Iteration 125/1000 | Loss: 0.00001176
Iteration 126/1000 | Loss: 0.00001175
Iteration 127/1000 | Loss: 0.00001175
Iteration 128/1000 | Loss: 0.00001175
Iteration 129/1000 | Loss: 0.00001175
Iteration 130/1000 | Loss: 0.00001174
Iteration 131/1000 | Loss: 0.00001174
Iteration 132/1000 | Loss: 0.00001174
Iteration 133/1000 | Loss: 0.00001174
Iteration 134/1000 | Loss: 0.00001174
Iteration 135/1000 | Loss: 0.00001174
Iteration 136/1000 | Loss: 0.00001173
Iteration 137/1000 | Loss: 0.00001173
Iteration 138/1000 | Loss: 0.00001172
Iteration 139/1000 | Loss: 0.00001172
Iteration 140/1000 | Loss: 0.00001172
Iteration 141/1000 | Loss: 0.00001172
Iteration 142/1000 | Loss: 0.00001172
Iteration 143/1000 | Loss: 0.00001172
Iteration 144/1000 | Loss: 0.00001172
Iteration 145/1000 | Loss: 0.00001171
Iteration 146/1000 | Loss: 0.00001171
Iteration 147/1000 | Loss: 0.00001171
Iteration 148/1000 | Loss: 0.00001171
Iteration 149/1000 | Loss: 0.00001171
Iteration 150/1000 | Loss: 0.00001171
Iteration 151/1000 | Loss: 0.00001171
Iteration 152/1000 | Loss: 0.00001171
Iteration 153/1000 | Loss: 0.00001170
Iteration 154/1000 | Loss: 0.00001170
Iteration 155/1000 | Loss: 0.00001170
Iteration 156/1000 | Loss: 0.00001170
Iteration 157/1000 | Loss: 0.00001170
Iteration 158/1000 | Loss: 0.00001170
Iteration 159/1000 | Loss: 0.00001170
Iteration 160/1000 | Loss: 0.00001170
Iteration 161/1000 | Loss: 0.00001169
Iteration 162/1000 | Loss: 0.00001169
Iteration 163/1000 | Loss: 0.00001169
Iteration 164/1000 | Loss: 0.00001168
Iteration 165/1000 | Loss: 0.00001168
Iteration 166/1000 | Loss: 0.00001168
Iteration 167/1000 | Loss: 0.00001168
Iteration 168/1000 | Loss: 0.00001168
Iteration 169/1000 | Loss: 0.00001168
Iteration 170/1000 | Loss: 0.00001168
Iteration 171/1000 | Loss: 0.00001167
Iteration 172/1000 | Loss: 0.00001167
Iteration 173/1000 | Loss: 0.00001167
Iteration 174/1000 | Loss: 0.00001167
Iteration 175/1000 | Loss: 0.00001166
Iteration 176/1000 | Loss: 0.00001166
Iteration 177/1000 | Loss: 0.00001166
Iteration 178/1000 | Loss: 0.00001166
Iteration 179/1000 | Loss: 0.00001166
Iteration 180/1000 | Loss: 0.00001166
Iteration 181/1000 | Loss: 0.00001166
Iteration 182/1000 | Loss: 0.00001166
Iteration 183/1000 | Loss: 0.00001166
Iteration 184/1000 | Loss: 0.00001166
Iteration 185/1000 | Loss: 0.00001166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.166279525932623e-05, 1.166279525932623e-05, 1.166279525932623e-05, 1.166279525932623e-05, 1.166279525932623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.166279525932623e-05

Optimization complete. Final v2v error: 2.9487030506134033 mm

Highest mean error: 3.578188180923462 mm for frame 21

Lowest mean error: 2.6266181468963623 mm for frame 252

Saving results

Total time: 53.8031051158905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002976
Iteration 2/25 | Loss: 0.00227693
Iteration 3/25 | Loss: 0.00175376
Iteration 4/25 | Loss: 0.00155975
Iteration 5/25 | Loss: 0.00161334
Iteration 6/25 | Loss: 0.00149825
Iteration 7/25 | Loss: 0.00145863
Iteration 8/25 | Loss: 0.00134125
Iteration 9/25 | Loss: 0.00131182
Iteration 10/25 | Loss: 0.00131735
Iteration 11/25 | Loss: 0.00130096
Iteration 12/25 | Loss: 0.00128044
Iteration 13/25 | Loss: 0.00127905
Iteration 14/25 | Loss: 0.00127668
Iteration 15/25 | Loss: 0.00128697
Iteration 16/25 | Loss: 0.00127164
Iteration 17/25 | Loss: 0.00125988
Iteration 18/25 | Loss: 0.00125451
Iteration 19/25 | Loss: 0.00125428
Iteration 20/25 | Loss: 0.00125824
Iteration 21/25 | Loss: 0.00124957
Iteration 22/25 | Loss: 0.00124583
Iteration 23/25 | Loss: 0.00124678
Iteration 24/25 | Loss: 0.00124625
Iteration 25/25 | Loss: 0.00124519

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28975499
Iteration 2/25 | Loss: 0.00155122
Iteration 3/25 | Loss: 0.00155122
Iteration 4/25 | Loss: 0.00155122
Iteration 5/25 | Loss: 0.00155122
Iteration 6/25 | Loss: 0.00155122
Iteration 7/25 | Loss: 0.00155122
Iteration 8/25 | Loss: 0.00155122
Iteration 9/25 | Loss: 0.00155122
Iteration 10/25 | Loss: 0.00155122
Iteration 11/25 | Loss: 0.00155122
Iteration 12/25 | Loss: 0.00155122
Iteration 13/25 | Loss: 0.00155122
Iteration 14/25 | Loss: 0.00155122
Iteration 15/25 | Loss: 0.00155122
Iteration 16/25 | Loss: 0.00155122
Iteration 17/25 | Loss: 0.00155122
Iteration 18/25 | Loss: 0.00155122
Iteration 19/25 | Loss: 0.00155122
Iteration 20/25 | Loss: 0.00155122
Iteration 21/25 | Loss: 0.00155122
Iteration 22/25 | Loss: 0.00155122
Iteration 23/25 | Loss: 0.00155122
Iteration 24/25 | Loss: 0.00155122
Iteration 25/25 | Loss: 0.00155122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155122
Iteration 2/1000 | Loss: 0.00003431
Iteration 3/1000 | Loss: 0.00002419
Iteration 4/1000 | Loss: 0.00001930
Iteration 5/1000 | Loss: 0.00009998
Iteration 6/1000 | Loss: 0.00072918
Iteration 7/1000 | Loss: 0.00005613
Iteration 8/1000 | Loss: 0.00014132
Iteration 9/1000 | Loss: 0.00005922
Iteration 10/1000 | Loss: 0.00001930
Iteration 11/1000 | Loss: 0.00038395
Iteration 12/1000 | Loss: 0.00024682
Iteration 13/1000 | Loss: 0.00055144
Iteration 14/1000 | Loss: 0.00007212
Iteration 15/1000 | Loss: 0.00011882
Iteration 16/1000 | Loss: 0.00029396
Iteration 17/1000 | Loss: 0.00004344
Iteration 18/1000 | Loss: 0.00001875
Iteration 19/1000 | Loss: 0.00003527
Iteration 20/1000 | Loss: 0.00013085
Iteration 21/1000 | Loss: 0.00004159
Iteration 22/1000 | Loss: 0.00003922
Iteration 23/1000 | Loss: 0.00001913
Iteration 24/1000 | Loss: 0.00002668
Iteration 25/1000 | Loss: 0.00025715
Iteration 26/1000 | Loss: 0.00014431
Iteration 27/1000 | Loss: 0.00001573
Iteration 28/1000 | Loss: 0.00011838
Iteration 29/1000 | Loss: 0.00038538
Iteration 30/1000 | Loss: 0.00003737
Iteration 31/1000 | Loss: 0.00004194
Iteration 32/1000 | Loss: 0.00001449
Iteration 33/1000 | Loss: 0.00007760
Iteration 34/1000 | Loss: 0.00001426
Iteration 35/1000 | Loss: 0.00001376
Iteration 36/1000 | Loss: 0.00029483
Iteration 37/1000 | Loss: 0.00064925
Iteration 38/1000 | Loss: 0.00056263
Iteration 39/1000 | Loss: 0.00005013
Iteration 40/1000 | Loss: 0.00004494
Iteration 41/1000 | Loss: 0.00028095
Iteration 42/1000 | Loss: 0.00050379
Iteration 43/1000 | Loss: 0.00013824
Iteration 44/1000 | Loss: 0.00001774
Iteration 45/1000 | Loss: 0.00008346
Iteration 46/1000 | Loss: 0.00001475
Iteration 47/1000 | Loss: 0.00026213
Iteration 48/1000 | Loss: 0.00047904
Iteration 49/1000 | Loss: 0.00002521
Iteration 50/1000 | Loss: 0.00009028
Iteration 51/1000 | Loss: 0.00001827
Iteration 52/1000 | Loss: 0.00019778
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00028265
Iteration 55/1000 | Loss: 0.00027976
Iteration 56/1000 | Loss: 0.00005371
Iteration 57/1000 | Loss: 0.00007015
Iteration 58/1000 | Loss: 0.00001635
Iteration 59/1000 | Loss: 0.00001474
Iteration 60/1000 | Loss: 0.00012212
Iteration 61/1000 | Loss: 0.00004451
Iteration 62/1000 | Loss: 0.00001970
Iteration 63/1000 | Loss: 0.00002849
Iteration 64/1000 | Loss: 0.00001658
Iteration 65/1000 | Loss: 0.00001364
Iteration 66/1000 | Loss: 0.00001361
Iteration 67/1000 | Loss: 0.00001360
Iteration 68/1000 | Loss: 0.00001357
Iteration 69/1000 | Loss: 0.00001354
Iteration 70/1000 | Loss: 0.00001354
Iteration 71/1000 | Loss: 0.00001354
Iteration 72/1000 | Loss: 0.00001353
Iteration 73/1000 | Loss: 0.00001353
Iteration 74/1000 | Loss: 0.00001351
Iteration 75/1000 | Loss: 0.00001351
Iteration 76/1000 | Loss: 0.00001349
Iteration 77/1000 | Loss: 0.00009343
Iteration 78/1000 | Loss: 0.00032303
Iteration 79/1000 | Loss: 0.00041203
Iteration 80/1000 | Loss: 0.00068194
Iteration 81/1000 | Loss: 0.00086945
Iteration 82/1000 | Loss: 0.00009709
Iteration 83/1000 | Loss: 0.00042221
Iteration 84/1000 | Loss: 0.00047919
Iteration 85/1000 | Loss: 0.00025503
Iteration 86/1000 | Loss: 0.00099308
Iteration 87/1000 | Loss: 0.00002520
Iteration 88/1000 | Loss: 0.00002482
Iteration 89/1000 | Loss: 0.00027287
Iteration 90/1000 | Loss: 0.00003943
Iteration 91/1000 | Loss: 0.00001458
Iteration 92/1000 | Loss: 0.00001399
Iteration 93/1000 | Loss: 0.00001364
Iteration 94/1000 | Loss: 0.00001323
Iteration 95/1000 | Loss: 0.00014843
Iteration 96/1000 | Loss: 0.00007162
Iteration 97/1000 | Loss: 0.00009142
Iteration 98/1000 | Loss: 0.00008410
Iteration 99/1000 | Loss: 0.00002443
Iteration 100/1000 | Loss: 0.00012303
Iteration 101/1000 | Loss: 0.00001542
Iteration 102/1000 | Loss: 0.00009420
Iteration 103/1000 | Loss: 0.00003264
Iteration 104/1000 | Loss: 0.00002588
Iteration 105/1000 | Loss: 0.00001406
Iteration 106/1000 | Loss: 0.00003915
Iteration 107/1000 | Loss: 0.00001334
Iteration 108/1000 | Loss: 0.00001301
Iteration 109/1000 | Loss: 0.00001278
Iteration 110/1000 | Loss: 0.00001277
Iteration 111/1000 | Loss: 0.00001267
Iteration 112/1000 | Loss: 0.00001261
Iteration 113/1000 | Loss: 0.00001252
Iteration 114/1000 | Loss: 0.00001248
Iteration 115/1000 | Loss: 0.00022870
Iteration 116/1000 | Loss: 0.00002114
Iteration 117/1000 | Loss: 0.00001508
Iteration 118/1000 | Loss: 0.00001362
Iteration 119/1000 | Loss: 0.00002448
Iteration 120/1000 | Loss: 0.00002138
Iteration 121/1000 | Loss: 0.00003034
Iteration 122/1000 | Loss: 0.00001575
Iteration 123/1000 | Loss: 0.00001303
Iteration 124/1000 | Loss: 0.00001232
Iteration 125/1000 | Loss: 0.00001224
Iteration 126/1000 | Loss: 0.00001224
Iteration 127/1000 | Loss: 0.00001224
Iteration 128/1000 | Loss: 0.00001224
Iteration 129/1000 | Loss: 0.00001224
Iteration 130/1000 | Loss: 0.00001224
Iteration 131/1000 | Loss: 0.00001224
Iteration 132/1000 | Loss: 0.00001224
Iteration 133/1000 | Loss: 0.00001224
Iteration 134/1000 | Loss: 0.00001224
Iteration 135/1000 | Loss: 0.00001224
Iteration 136/1000 | Loss: 0.00001224
Iteration 137/1000 | Loss: 0.00001223
Iteration 138/1000 | Loss: 0.00001223
Iteration 139/1000 | Loss: 0.00001223
Iteration 140/1000 | Loss: 0.00001223
Iteration 141/1000 | Loss: 0.00001223
Iteration 142/1000 | Loss: 0.00001223
Iteration 143/1000 | Loss: 0.00001223
Iteration 144/1000 | Loss: 0.00001223
Iteration 145/1000 | Loss: 0.00001223
Iteration 146/1000 | Loss: 0.00001223
Iteration 147/1000 | Loss: 0.00001223
Iteration 148/1000 | Loss: 0.00001223
Iteration 149/1000 | Loss: 0.00001223
Iteration 150/1000 | Loss: 0.00001223
Iteration 151/1000 | Loss: 0.00001223
Iteration 152/1000 | Loss: 0.00001223
Iteration 153/1000 | Loss: 0.00001223
Iteration 154/1000 | Loss: 0.00001223
Iteration 155/1000 | Loss: 0.00001223
Iteration 156/1000 | Loss: 0.00001223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.2234093446750194e-05, 1.2234093446750194e-05, 1.2234093446750194e-05, 1.2234093446750194e-05, 1.2234093446750194e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2234093446750194e-05

Optimization complete. Final v2v error: 2.91982102394104 mm

Highest mean error: 5.440420150756836 mm for frame 41

Lowest mean error: 2.5508553981781006 mm for frame 29

Saving results

Total time: 193.73043084144592
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736023
Iteration 2/25 | Loss: 0.00159291
Iteration 3/25 | Loss: 0.00132941
Iteration 4/25 | Loss: 0.00130538
Iteration 5/25 | Loss: 0.00130016
Iteration 6/25 | Loss: 0.00130761
Iteration 7/25 | Loss: 0.00127816
Iteration 8/25 | Loss: 0.00127281
Iteration 9/25 | Loss: 0.00127188
Iteration 10/25 | Loss: 0.00127169
Iteration 11/25 | Loss: 0.00127156
Iteration 12/25 | Loss: 0.00127155
Iteration 13/25 | Loss: 0.00127155
Iteration 14/25 | Loss: 0.00127155
Iteration 15/25 | Loss: 0.00127155
Iteration 16/25 | Loss: 0.00127155
Iteration 17/25 | Loss: 0.00127155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012715531047433615, 0.0012715531047433615, 0.0012715531047433615, 0.0012715531047433615, 0.0012715531047433615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012715531047433615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.42224121
Iteration 2/25 | Loss: 0.00138080
Iteration 3/25 | Loss: 0.00138068
Iteration 4/25 | Loss: 0.00138068
Iteration 5/25 | Loss: 0.00138068
Iteration 6/25 | Loss: 0.00138068
Iteration 7/25 | Loss: 0.00138068
Iteration 8/25 | Loss: 0.00138068
Iteration 9/25 | Loss: 0.00138068
Iteration 10/25 | Loss: 0.00138068
Iteration 11/25 | Loss: 0.00138068
Iteration 12/25 | Loss: 0.00138068
Iteration 13/25 | Loss: 0.00138068
Iteration 14/25 | Loss: 0.00138068
Iteration 15/25 | Loss: 0.00138068
Iteration 16/25 | Loss: 0.00138068
Iteration 17/25 | Loss: 0.00138068
Iteration 18/25 | Loss: 0.00138068
Iteration 19/25 | Loss: 0.00138068
Iteration 20/25 | Loss: 0.00138068
Iteration 21/25 | Loss: 0.00138068
Iteration 22/25 | Loss: 0.00138068
Iteration 23/25 | Loss: 0.00138068
Iteration 24/25 | Loss: 0.00138068
Iteration 25/25 | Loss: 0.00138068

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138068
Iteration 2/1000 | Loss: 0.00002817
Iteration 3/1000 | Loss: 0.00001933
Iteration 4/1000 | Loss: 0.00001759
Iteration 5/1000 | Loss: 0.00001656
Iteration 6/1000 | Loss: 0.00001586
Iteration 7/1000 | Loss: 0.00001533
Iteration 8/1000 | Loss: 0.00001501
Iteration 9/1000 | Loss: 0.00001458
Iteration 10/1000 | Loss: 0.00001434
Iteration 11/1000 | Loss: 0.00001416
Iteration 12/1000 | Loss: 0.00001408
Iteration 13/1000 | Loss: 0.00001407
Iteration 14/1000 | Loss: 0.00001405
Iteration 15/1000 | Loss: 0.00001404
Iteration 16/1000 | Loss: 0.00001399
Iteration 17/1000 | Loss: 0.00001396
Iteration 18/1000 | Loss: 0.00001396
Iteration 19/1000 | Loss: 0.00001395
Iteration 20/1000 | Loss: 0.00001394
Iteration 21/1000 | Loss: 0.00001393
Iteration 22/1000 | Loss: 0.00001387
Iteration 23/1000 | Loss: 0.00001378
Iteration 24/1000 | Loss: 0.00001375
Iteration 25/1000 | Loss: 0.00001375
Iteration 26/1000 | Loss: 0.00001374
Iteration 27/1000 | Loss: 0.00001374
Iteration 28/1000 | Loss: 0.00001374
Iteration 29/1000 | Loss: 0.00001374
Iteration 30/1000 | Loss: 0.00001373
Iteration 31/1000 | Loss: 0.00001373
Iteration 32/1000 | Loss: 0.00001373
Iteration 33/1000 | Loss: 0.00001372
Iteration 34/1000 | Loss: 0.00001371
Iteration 35/1000 | Loss: 0.00001370
Iteration 36/1000 | Loss: 0.00001370
Iteration 37/1000 | Loss: 0.00001369
Iteration 38/1000 | Loss: 0.00001369
Iteration 39/1000 | Loss: 0.00001368
Iteration 40/1000 | Loss: 0.00001367
Iteration 41/1000 | Loss: 0.00001367
Iteration 42/1000 | Loss: 0.00001366
Iteration 43/1000 | Loss: 0.00001365
Iteration 44/1000 | Loss: 0.00001365
Iteration 45/1000 | Loss: 0.00001364
Iteration 46/1000 | Loss: 0.00001361
Iteration 47/1000 | Loss: 0.00001361
Iteration 48/1000 | Loss: 0.00001361
Iteration 49/1000 | Loss: 0.00001360
Iteration 50/1000 | Loss: 0.00001359
Iteration 51/1000 | Loss: 0.00001358
Iteration 52/1000 | Loss: 0.00001356
Iteration 53/1000 | Loss: 0.00001356
Iteration 54/1000 | Loss: 0.00001356
Iteration 55/1000 | Loss: 0.00001356
Iteration 56/1000 | Loss: 0.00001356
Iteration 57/1000 | Loss: 0.00001356
Iteration 58/1000 | Loss: 0.00001356
Iteration 59/1000 | Loss: 0.00001356
Iteration 60/1000 | Loss: 0.00001356
Iteration 61/1000 | Loss: 0.00001355
Iteration 62/1000 | Loss: 0.00001355
Iteration 63/1000 | Loss: 0.00001354
Iteration 64/1000 | Loss: 0.00001354
Iteration 65/1000 | Loss: 0.00001353
Iteration 66/1000 | Loss: 0.00001353
Iteration 67/1000 | Loss: 0.00001352
Iteration 68/1000 | Loss: 0.00001352
Iteration 69/1000 | Loss: 0.00001352
Iteration 70/1000 | Loss: 0.00001352
Iteration 71/1000 | Loss: 0.00001352
Iteration 72/1000 | Loss: 0.00001352
Iteration 73/1000 | Loss: 0.00001352
Iteration 74/1000 | Loss: 0.00001352
Iteration 75/1000 | Loss: 0.00001352
Iteration 76/1000 | Loss: 0.00001352
Iteration 77/1000 | Loss: 0.00001352
Iteration 78/1000 | Loss: 0.00001352
Iteration 79/1000 | Loss: 0.00001352
Iteration 80/1000 | Loss: 0.00001352
Iteration 81/1000 | Loss: 0.00001352
Iteration 82/1000 | Loss: 0.00001352
Iteration 83/1000 | Loss: 0.00001352
Iteration 84/1000 | Loss: 0.00001352
Iteration 85/1000 | Loss: 0.00001352
Iteration 86/1000 | Loss: 0.00001352
Iteration 87/1000 | Loss: 0.00001352
Iteration 88/1000 | Loss: 0.00001352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.3517046681954525e-05, 1.3517046681954525e-05, 1.3517046681954525e-05, 1.3517046681954525e-05, 1.3517046681954525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3517046681954525e-05

Optimization complete. Final v2v error: 3.157379627227783 mm

Highest mean error: 3.864246368408203 mm for frame 26

Lowest mean error: 2.7505524158477783 mm for frame 10

Saving results

Total time: 50.16146993637085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048945
Iteration 2/25 | Loss: 0.00269546
Iteration 3/25 | Loss: 0.00201770
Iteration 4/25 | Loss: 0.00161328
Iteration 5/25 | Loss: 0.00153214
Iteration 6/25 | Loss: 0.00145419
Iteration 7/25 | Loss: 0.00141943
Iteration 8/25 | Loss: 0.00140428
Iteration 9/25 | Loss: 0.00136043
Iteration 10/25 | Loss: 0.00134183
Iteration 11/25 | Loss: 0.00133391
Iteration 12/25 | Loss: 0.00133299
Iteration 13/25 | Loss: 0.00132673
Iteration 14/25 | Loss: 0.00132515
Iteration 15/25 | Loss: 0.00132464
Iteration 16/25 | Loss: 0.00132453
Iteration 17/25 | Loss: 0.00132448
Iteration 18/25 | Loss: 0.00132447
Iteration 19/25 | Loss: 0.00132444
Iteration 20/25 | Loss: 0.00132444
Iteration 21/25 | Loss: 0.00132443
Iteration 22/25 | Loss: 0.00132443
Iteration 23/25 | Loss: 0.00132443
Iteration 24/25 | Loss: 0.00132443
Iteration 25/25 | Loss: 0.00132443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.81563091
Iteration 2/25 | Loss: 0.00183383
Iteration 3/25 | Loss: 0.00183383
Iteration 4/25 | Loss: 0.00183383
Iteration 5/25 | Loss: 0.00183383
Iteration 6/25 | Loss: 0.00183383
Iteration 7/25 | Loss: 0.00183383
Iteration 8/25 | Loss: 0.00183383
Iteration 9/25 | Loss: 0.00183383
Iteration 10/25 | Loss: 0.00183383
Iteration 11/25 | Loss: 0.00183383
Iteration 12/25 | Loss: 0.00183383
Iteration 13/25 | Loss: 0.00183383
Iteration 14/25 | Loss: 0.00183383
Iteration 15/25 | Loss: 0.00183383
Iteration 16/25 | Loss: 0.00183383
Iteration 17/25 | Loss: 0.00183383
Iteration 18/25 | Loss: 0.00183383
Iteration 19/25 | Loss: 0.00183383
Iteration 20/25 | Loss: 0.00183383
Iteration 21/25 | Loss: 0.00183383
Iteration 22/25 | Loss: 0.00183383
Iteration 23/25 | Loss: 0.00183383
Iteration 24/25 | Loss: 0.00183383
Iteration 25/25 | Loss: 0.00183383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183383
Iteration 2/1000 | Loss: 0.00003272
Iteration 3/1000 | Loss: 0.00006501
Iteration 4/1000 | Loss: 0.00002139
Iteration 5/1000 | Loss: 0.00002677
Iteration 6/1000 | Loss: 0.00005138
Iteration 7/1000 | Loss: 0.00002009
Iteration 8/1000 | Loss: 0.00001924
Iteration 9/1000 | Loss: 0.00002167
Iteration 10/1000 | Loss: 0.00001864
Iteration 11/1000 | Loss: 0.00006996
Iteration 12/1000 | Loss: 0.00002140
Iteration 13/1000 | Loss: 0.00002476
Iteration 14/1000 | Loss: 0.00002131
Iteration 15/1000 | Loss: 0.00001913
Iteration 16/1000 | Loss: 0.00001850
Iteration 17/1000 | Loss: 0.00033857
Iteration 18/1000 | Loss: 0.00003395
Iteration 19/1000 | Loss: 0.00002409
Iteration 20/1000 | Loss: 0.00006816
Iteration 21/1000 | Loss: 0.00001826
Iteration 22/1000 | Loss: 0.00006695
Iteration 23/1000 | Loss: 0.00004661
Iteration 24/1000 | Loss: 0.00001712
Iteration 25/1000 | Loss: 0.00001681
Iteration 26/1000 | Loss: 0.00002199
Iteration 27/1000 | Loss: 0.00001649
Iteration 28/1000 | Loss: 0.00001755
Iteration 29/1000 | Loss: 0.00001630
Iteration 30/1000 | Loss: 0.00001619
Iteration 31/1000 | Loss: 0.00001619
Iteration 32/1000 | Loss: 0.00001705
Iteration 33/1000 | Loss: 0.00008235
Iteration 34/1000 | Loss: 0.00001679
Iteration 35/1000 | Loss: 0.00003322
Iteration 36/1000 | Loss: 0.00001592
Iteration 37/1000 | Loss: 0.00001702
Iteration 38/1000 | Loss: 0.00001702
Iteration 39/1000 | Loss: 0.00003215
Iteration 40/1000 | Loss: 0.00001580
Iteration 41/1000 | Loss: 0.00003847
Iteration 42/1000 | Loss: 0.00002063
Iteration 43/1000 | Loss: 0.00001813
Iteration 44/1000 | Loss: 0.00001571
Iteration 45/1000 | Loss: 0.00001571
Iteration 46/1000 | Loss: 0.00001571
Iteration 47/1000 | Loss: 0.00001836
Iteration 48/1000 | Loss: 0.00001590
Iteration 49/1000 | Loss: 0.00001616
Iteration 50/1000 | Loss: 0.00001985
Iteration 51/1000 | Loss: 0.00001579
Iteration 52/1000 | Loss: 0.00001579
Iteration 53/1000 | Loss: 0.00001571
Iteration 54/1000 | Loss: 0.00001566
Iteration 55/1000 | Loss: 0.00001566
Iteration 56/1000 | Loss: 0.00001566
Iteration 57/1000 | Loss: 0.00001566
Iteration 58/1000 | Loss: 0.00001566
Iteration 59/1000 | Loss: 0.00001566
Iteration 60/1000 | Loss: 0.00001566
Iteration 61/1000 | Loss: 0.00001566
Iteration 62/1000 | Loss: 0.00001566
Iteration 63/1000 | Loss: 0.00001566
Iteration 64/1000 | Loss: 0.00001566
Iteration 65/1000 | Loss: 0.00001566
Iteration 66/1000 | Loss: 0.00001566
Iteration 67/1000 | Loss: 0.00001566
Iteration 68/1000 | Loss: 0.00001565
Iteration 69/1000 | Loss: 0.00001565
Iteration 70/1000 | Loss: 0.00001564
Iteration 71/1000 | Loss: 0.00001564
Iteration 72/1000 | Loss: 0.00001564
Iteration 73/1000 | Loss: 0.00001564
Iteration 74/1000 | Loss: 0.00001564
Iteration 75/1000 | Loss: 0.00001564
Iteration 76/1000 | Loss: 0.00001564
Iteration 77/1000 | Loss: 0.00001564
Iteration 78/1000 | Loss: 0.00001564
Iteration 79/1000 | Loss: 0.00001564
Iteration 80/1000 | Loss: 0.00001564
Iteration 81/1000 | Loss: 0.00001564
Iteration 82/1000 | Loss: 0.00001564
Iteration 83/1000 | Loss: 0.00001563
Iteration 84/1000 | Loss: 0.00001563
Iteration 85/1000 | Loss: 0.00001562
Iteration 86/1000 | Loss: 0.00001562
Iteration 87/1000 | Loss: 0.00001562
Iteration 88/1000 | Loss: 0.00001561
Iteration 89/1000 | Loss: 0.00001561
Iteration 90/1000 | Loss: 0.00001560
Iteration 91/1000 | Loss: 0.00001560
Iteration 92/1000 | Loss: 0.00001560
Iteration 93/1000 | Loss: 0.00001560
Iteration 94/1000 | Loss: 0.00001560
Iteration 95/1000 | Loss: 0.00001560
Iteration 96/1000 | Loss: 0.00001560
Iteration 97/1000 | Loss: 0.00001560
Iteration 98/1000 | Loss: 0.00001560
Iteration 99/1000 | Loss: 0.00001560
Iteration 100/1000 | Loss: 0.00001560
Iteration 101/1000 | Loss: 0.00001560
Iteration 102/1000 | Loss: 0.00001559
Iteration 103/1000 | Loss: 0.00001559
Iteration 104/1000 | Loss: 0.00001559
Iteration 105/1000 | Loss: 0.00001559
Iteration 106/1000 | Loss: 0.00001559
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001558
Iteration 110/1000 | Loss: 0.00001558
Iteration 111/1000 | Loss: 0.00001558
Iteration 112/1000 | Loss: 0.00001558
Iteration 113/1000 | Loss: 0.00001557
Iteration 114/1000 | Loss: 0.00001557
Iteration 115/1000 | Loss: 0.00001557
Iteration 116/1000 | Loss: 0.00001557
Iteration 117/1000 | Loss: 0.00001557
Iteration 118/1000 | Loss: 0.00001557
Iteration 119/1000 | Loss: 0.00001557
Iteration 120/1000 | Loss: 0.00001557
Iteration 121/1000 | Loss: 0.00001557
Iteration 122/1000 | Loss: 0.00001556
Iteration 123/1000 | Loss: 0.00001556
Iteration 124/1000 | Loss: 0.00001556
Iteration 125/1000 | Loss: 0.00001556
Iteration 126/1000 | Loss: 0.00001556
Iteration 127/1000 | Loss: 0.00001556
Iteration 128/1000 | Loss: 0.00001556
Iteration 129/1000 | Loss: 0.00001555
Iteration 130/1000 | Loss: 0.00001555
Iteration 131/1000 | Loss: 0.00001555
Iteration 132/1000 | Loss: 0.00001555
Iteration 133/1000 | Loss: 0.00001555
Iteration 134/1000 | Loss: 0.00001555
Iteration 135/1000 | Loss: 0.00001555
Iteration 136/1000 | Loss: 0.00001555
Iteration 137/1000 | Loss: 0.00001555
Iteration 138/1000 | Loss: 0.00001555
Iteration 139/1000 | Loss: 0.00001555
Iteration 140/1000 | Loss: 0.00001555
Iteration 141/1000 | Loss: 0.00001555
Iteration 142/1000 | Loss: 0.00001555
Iteration 143/1000 | Loss: 0.00001555
Iteration 144/1000 | Loss: 0.00001555
Iteration 145/1000 | Loss: 0.00001555
Iteration 146/1000 | Loss: 0.00001555
Iteration 147/1000 | Loss: 0.00001555
Iteration 148/1000 | Loss: 0.00001555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.5552868717350066e-05, 1.5552868717350066e-05, 1.5552868717350066e-05, 1.5552868717350066e-05, 1.5552868717350066e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5552868717350066e-05

Optimization complete. Final v2v error: 3.24324369430542 mm

Highest mean error: 8.789051055908203 mm for frame 17

Lowest mean error: 2.977296829223633 mm for frame 48

Saving results

Total time: 94.73977208137512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790133
Iteration 2/25 | Loss: 0.00187411
Iteration 3/25 | Loss: 0.00140756
Iteration 4/25 | Loss: 0.00135544
Iteration 5/25 | Loss: 0.00135045
Iteration 6/25 | Loss: 0.00135035
Iteration 7/25 | Loss: 0.00135035
Iteration 8/25 | Loss: 0.00135035
Iteration 9/25 | Loss: 0.00135035
Iteration 10/25 | Loss: 0.00135035
Iteration 11/25 | Loss: 0.00135035
Iteration 12/25 | Loss: 0.00135035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013503481168299913, 0.0013503481168299913, 0.0013503481168299913, 0.0013503481168299913, 0.0013503481168299913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013503481168299913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15845907
Iteration 2/25 | Loss: 0.00109728
Iteration 3/25 | Loss: 0.00109725
Iteration 4/25 | Loss: 0.00109725
Iteration 5/25 | Loss: 0.00109725
Iteration 6/25 | Loss: 0.00109725
Iteration 7/25 | Loss: 0.00109725
Iteration 8/25 | Loss: 0.00109725
Iteration 9/25 | Loss: 0.00109725
Iteration 10/25 | Loss: 0.00109725
Iteration 11/25 | Loss: 0.00109725
Iteration 12/25 | Loss: 0.00109725
Iteration 13/25 | Loss: 0.00109725
Iteration 14/25 | Loss: 0.00109725
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010972493328154087, 0.0010972493328154087, 0.0010972493328154087, 0.0010972493328154087, 0.0010972493328154087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010972493328154087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109725
Iteration 2/1000 | Loss: 0.00003699
Iteration 3/1000 | Loss: 0.00002676
Iteration 4/1000 | Loss: 0.00002427
Iteration 5/1000 | Loss: 0.00002326
Iteration 6/1000 | Loss: 0.00002254
Iteration 7/1000 | Loss: 0.00002224
Iteration 8/1000 | Loss: 0.00002187
Iteration 9/1000 | Loss: 0.00002144
Iteration 10/1000 | Loss: 0.00002115
Iteration 11/1000 | Loss: 0.00002091
Iteration 12/1000 | Loss: 0.00002089
Iteration 13/1000 | Loss: 0.00002066
Iteration 14/1000 | Loss: 0.00002065
Iteration 15/1000 | Loss: 0.00002049
Iteration 16/1000 | Loss: 0.00002047
Iteration 17/1000 | Loss: 0.00002046
Iteration 18/1000 | Loss: 0.00002039
Iteration 19/1000 | Loss: 0.00002039
Iteration 20/1000 | Loss: 0.00002033
Iteration 21/1000 | Loss: 0.00002030
Iteration 22/1000 | Loss: 0.00002029
Iteration 23/1000 | Loss: 0.00002026
Iteration 24/1000 | Loss: 0.00002026
Iteration 25/1000 | Loss: 0.00002026
Iteration 26/1000 | Loss: 0.00002025
Iteration 27/1000 | Loss: 0.00002022
Iteration 28/1000 | Loss: 0.00002021
Iteration 29/1000 | Loss: 0.00002021
Iteration 30/1000 | Loss: 0.00002020
Iteration 31/1000 | Loss: 0.00002019
Iteration 32/1000 | Loss: 0.00002017
Iteration 33/1000 | Loss: 0.00002017
Iteration 34/1000 | Loss: 0.00002016
Iteration 35/1000 | Loss: 0.00002016
Iteration 36/1000 | Loss: 0.00002016
Iteration 37/1000 | Loss: 0.00002016
Iteration 38/1000 | Loss: 0.00002015
Iteration 39/1000 | Loss: 0.00002015
Iteration 40/1000 | Loss: 0.00002014
Iteration 41/1000 | Loss: 0.00002014
Iteration 42/1000 | Loss: 0.00002014
Iteration 43/1000 | Loss: 0.00002014
Iteration 44/1000 | Loss: 0.00002014
Iteration 45/1000 | Loss: 0.00002014
Iteration 46/1000 | Loss: 0.00002014
Iteration 47/1000 | Loss: 0.00002014
Iteration 48/1000 | Loss: 0.00002014
Iteration 49/1000 | Loss: 0.00002014
Iteration 50/1000 | Loss: 0.00002014
Iteration 51/1000 | Loss: 0.00002014
Iteration 52/1000 | Loss: 0.00002014
Iteration 53/1000 | Loss: 0.00002013
Iteration 54/1000 | Loss: 0.00002013
Iteration 55/1000 | Loss: 0.00002013
Iteration 56/1000 | Loss: 0.00002013
Iteration 57/1000 | Loss: 0.00002013
Iteration 58/1000 | Loss: 0.00002013
Iteration 59/1000 | Loss: 0.00002012
Iteration 60/1000 | Loss: 0.00002012
Iteration 61/1000 | Loss: 0.00002012
Iteration 62/1000 | Loss: 0.00002011
Iteration 63/1000 | Loss: 0.00002011
Iteration 64/1000 | Loss: 0.00002011
Iteration 65/1000 | Loss: 0.00002011
Iteration 66/1000 | Loss: 0.00002011
Iteration 67/1000 | Loss: 0.00002011
Iteration 68/1000 | Loss: 0.00002011
Iteration 69/1000 | Loss: 0.00002011
Iteration 70/1000 | Loss: 0.00002011
Iteration 71/1000 | Loss: 0.00002011
Iteration 72/1000 | Loss: 0.00002011
Iteration 73/1000 | Loss: 0.00002011
Iteration 74/1000 | Loss: 0.00002011
Iteration 75/1000 | Loss: 0.00002011
Iteration 76/1000 | Loss: 0.00002011
Iteration 77/1000 | Loss: 0.00002011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [2.01088023459306e-05, 2.01088023459306e-05, 2.01088023459306e-05, 2.01088023459306e-05, 2.01088023459306e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.01088023459306e-05

Optimization complete. Final v2v error: 3.776822090148926 mm

Highest mean error: 4.18425989151001 mm for frame 125

Lowest mean error: 3.5398895740509033 mm for frame 222

Saving results

Total time: 37.56427502632141
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817184
Iteration 2/25 | Loss: 0.00128930
Iteration 3/25 | Loss: 0.00121458
Iteration 4/25 | Loss: 0.00120759
Iteration 5/25 | Loss: 0.00120624
Iteration 6/25 | Loss: 0.00120624
Iteration 7/25 | Loss: 0.00120624
Iteration 8/25 | Loss: 0.00120624
Iteration 9/25 | Loss: 0.00120624
Iteration 10/25 | Loss: 0.00120624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012062415480613708, 0.0012062415480613708, 0.0012062415480613708, 0.0012062415480613708, 0.0012062415480613708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012062415480613708

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28263128
Iteration 2/25 | Loss: 0.00129197
Iteration 3/25 | Loss: 0.00129195
Iteration 4/25 | Loss: 0.00129195
Iteration 5/25 | Loss: 0.00129195
Iteration 6/25 | Loss: 0.00129195
Iteration 7/25 | Loss: 0.00129195
Iteration 8/25 | Loss: 0.00129195
Iteration 9/25 | Loss: 0.00129195
Iteration 10/25 | Loss: 0.00129195
Iteration 11/25 | Loss: 0.00129195
Iteration 12/25 | Loss: 0.00129195
Iteration 13/25 | Loss: 0.00129195
Iteration 14/25 | Loss: 0.00129195
Iteration 15/25 | Loss: 0.00129195
Iteration 16/25 | Loss: 0.00129195
Iteration 17/25 | Loss: 0.00129195
Iteration 18/25 | Loss: 0.00129195
Iteration 19/25 | Loss: 0.00129195
Iteration 20/25 | Loss: 0.00129195
Iteration 21/25 | Loss: 0.00129195
Iteration 22/25 | Loss: 0.00129195
Iteration 23/25 | Loss: 0.00129195
Iteration 24/25 | Loss: 0.00129195
Iteration 25/25 | Loss: 0.00129195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129195
Iteration 2/1000 | Loss: 0.00001998
Iteration 3/1000 | Loss: 0.00001417
Iteration 4/1000 | Loss: 0.00001287
Iteration 5/1000 | Loss: 0.00001176
Iteration 6/1000 | Loss: 0.00001130
Iteration 7/1000 | Loss: 0.00001073
Iteration 8/1000 | Loss: 0.00001047
Iteration 9/1000 | Loss: 0.00001027
Iteration 10/1000 | Loss: 0.00001019
Iteration 11/1000 | Loss: 0.00000992
Iteration 12/1000 | Loss: 0.00000990
Iteration 13/1000 | Loss: 0.00000989
Iteration 14/1000 | Loss: 0.00000983
Iteration 15/1000 | Loss: 0.00000981
Iteration 16/1000 | Loss: 0.00000981
Iteration 17/1000 | Loss: 0.00000979
Iteration 18/1000 | Loss: 0.00000976
Iteration 19/1000 | Loss: 0.00000972
Iteration 20/1000 | Loss: 0.00000971
Iteration 21/1000 | Loss: 0.00000970
Iteration 22/1000 | Loss: 0.00000969
Iteration 23/1000 | Loss: 0.00000961
Iteration 24/1000 | Loss: 0.00000950
Iteration 25/1000 | Loss: 0.00000942
Iteration 26/1000 | Loss: 0.00000941
Iteration 27/1000 | Loss: 0.00000940
Iteration 28/1000 | Loss: 0.00000940
Iteration 29/1000 | Loss: 0.00000939
Iteration 30/1000 | Loss: 0.00000939
Iteration 31/1000 | Loss: 0.00000939
Iteration 32/1000 | Loss: 0.00000938
Iteration 33/1000 | Loss: 0.00000938
Iteration 34/1000 | Loss: 0.00000938
Iteration 35/1000 | Loss: 0.00000938
Iteration 36/1000 | Loss: 0.00000937
Iteration 37/1000 | Loss: 0.00000937
Iteration 38/1000 | Loss: 0.00000936
Iteration 39/1000 | Loss: 0.00000935
Iteration 40/1000 | Loss: 0.00000935
Iteration 41/1000 | Loss: 0.00000934
Iteration 42/1000 | Loss: 0.00000934
Iteration 43/1000 | Loss: 0.00000934
Iteration 44/1000 | Loss: 0.00000934
Iteration 45/1000 | Loss: 0.00000933
Iteration 46/1000 | Loss: 0.00000932
Iteration 47/1000 | Loss: 0.00000932
Iteration 48/1000 | Loss: 0.00000931
Iteration 49/1000 | Loss: 0.00000930
Iteration 50/1000 | Loss: 0.00000930
Iteration 51/1000 | Loss: 0.00000929
Iteration 52/1000 | Loss: 0.00000929
Iteration 53/1000 | Loss: 0.00000928
Iteration 54/1000 | Loss: 0.00000927
Iteration 55/1000 | Loss: 0.00000927
Iteration 56/1000 | Loss: 0.00000927
Iteration 57/1000 | Loss: 0.00000926
Iteration 58/1000 | Loss: 0.00000925
Iteration 59/1000 | Loss: 0.00000925
Iteration 60/1000 | Loss: 0.00000925
Iteration 61/1000 | Loss: 0.00000925
Iteration 62/1000 | Loss: 0.00000925
Iteration 63/1000 | Loss: 0.00000925
Iteration 64/1000 | Loss: 0.00000925
Iteration 65/1000 | Loss: 0.00000925
Iteration 66/1000 | Loss: 0.00000925
Iteration 67/1000 | Loss: 0.00000925
Iteration 68/1000 | Loss: 0.00000925
Iteration 69/1000 | Loss: 0.00000925
Iteration 70/1000 | Loss: 0.00000924
Iteration 71/1000 | Loss: 0.00000923
Iteration 72/1000 | Loss: 0.00000922
Iteration 73/1000 | Loss: 0.00000921
Iteration 74/1000 | Loss: 0.00000920
Iteration 75/1000 | Loss: 0.00000920
Iteration 76/1000 | Loss: 0.00000918
Iteration 77/1000 | Loss: 0.00000917
Iteration 78/1000 | Loss: 0.00000917
Iteration 79/1000 | Loss: 0.00000916
Iteration 80/1000 | Loss: 0.00000916
Iteration 81/1000 | Loss: 0.00000915
Iteration 82/1000 | Loss: 0.00000914
Iteration 83/1000 | Loss: 0.00000914
Iteration 84/1000 | Loss: 0.00000914
Iteration 85/1000 | Loss: 0.00000914
Iteration 86/1000 | Loss: 0.00000914
Iteration 87/1000 | Loss: 0.00000914
Iteration 88/1000 | Loss: 0.00000913
Iteration 89/1000 | Loss: 0.00000913
Iteration 90/1000 | Loss: 0.00000913
Iteration 91/1000 | Loss: 0.00000913
Iteration 92/1000 | Loss: 0.00000912
Iteration 93/1000 | Loss: 0.00000911
Iteration 94/1000 | Loss: 0.00000911
Iteration 95/1000 | Loss: 0.00000911
Iteration 96/1000 | Loss: 0.00000911
Iteration 97/1000 | Loss: 0.00000910
Iteration 98/1000 | Loss: 0.00000910
Iteration 99/1000 | Loss: 0.00000909
Iteration 100/1000 | Loss: 0.00000909
Iteration 101/1000 | Loss: 0.00000909
Iteration 102/1000 | Loss: 0.00000909
Iteration 103/1000 | Loss: 0.00000908
Iteration 104/1000 | Loss: 0.00000908
Iteration 105/1000 | Loss: 0.00000908
Iteration 106/1000 | Loss: 0.00000908
Iteration 107/1000 | Loss: 0.00000908
Iteration 108/1000 | Loss: 0.00000908
Iteration 109/1000 | Loss: 0.00000908
Iteration 110/1000 | Loss: 0.00000908
Iteration 111/1000 | Loss: 0.00000908
Iteration 112/1000 | Loss: 0.00000908
Iteration 113/1000 | Loss: 0.00000908
Iteration 114/1000 | Loss: 0.00000908
Iteration 115/1000 | Loss: 0.00000907
Iteration 116/1000 | Loss: 0.00000907
Iteration 117/1000 | Loss: 0.00000907
Iteration 118/1000 | Loss: 0.00000906
Iteration 119/1000 | Loss: 0.00000906
Iteration 120/1000 | Loss: 0.00000906
Iteration 121/1000 | Loss: 0.00000906
Iteration 122/1000 | Loss: 0.00000906
Iteration 123/1000 | Loss: 0.00000905
Iteration 124/1000 | Loss: 0.00000905
Iteration 125/1000 | Loss: 0.00000905
Iteration 126/1000 | Loss: 0.00000905
Iteration 127/1000 | Loss: 0.00000905
Iteration 128/1000 | Loss: 0.00000905
Iteration 129/1000 | Loss: 0.00000905
Iteration 130/1000 | Loss: 0.00000905
Iteration 131/1000 | Loss: 0.00000904
Iteration 132/1000 | Loss: 0.00000904
Iteration 133/1000 | Loss: 0.00000904
Iteration 134/1000 | Loss: 0.00000903
Iteration 135/1000 | Loss: 0.00000903
Iteration 136/1000 | Loss: 0.00000903
Iteration 137/1000 | Loss: 0.00000903
Iteration 138/1000 | Loss: 0.00000903
Iteration 139/1000 | Loss: 0.00000903
Iteration 140/1000 | Loss: 0.00000902
Iteration 141/1000 | Loss: 0.00000902
Iteration 142/1000 | Loss: 0.00000902
Iteration 143/1000 | Loss: 0.00000902
Iteration 144/1000 | Loss: 0.00000902
Iteration 145/1000 | Loss: 0.00000901
Iteration 146/1000 | Loss: 0.00000901
Iteration 147/1000 | Loss: 0.00000901
Iteration 148/1000 | Loss: 0.00000901
Iteration 149/1000 | Loss: 0.00000901
Iteration 150/1000 | Loss: 0.00000901
Iteration 151/1000 | Loss: 0.00000901
Iteration 152/1000 | Loss: 0.00000901
Iteration 153/1000 | Loss: 0.00000901
Iteration 154/1000 | Loss: 0.00000900
Iteration 155/1000 | Loss: 0.00000900
Iteration 156/1000 | Loss: 0.00000900
Iteration 157/1000 | Loss: 0.00000900
Iteration 158/1000 | Loss: 0.00000900
Iteration 159/1000 | Loss: 0.00000900
Iteration 160/1000 | Loss: 0.00000900
Iteration 161/1000 | Loss: 0.00000900
Iteration 162/1000 | Loss: 0.00000900
Iteration 163/1000 | Loss: 0.00000900
Iteration 164/1000 | Loss: 0.00000900
Iteration 165/1000 | Loss: 0.00000900
Iteration 166/1000 | Loss: 0.00000900
Iteration 167/1000 | Loss: 0.00000900
Iteration 168/1000 | Loss: 0.00000899
Iteration 169/1000 | Loss: 0.00000899
Iteration 170/1000 | Loss: 0.00000899
Iteration 171/1000 | Loss: 0.00000899
Iteration 172/1000 | Loss: 0.00000899
Iteration 173/1000 | Loss: 0.00000899
Iteration 174/1000 | Loss: 0.00000899
Iteration 175/1000 | Loss: 0.00000898
Iteration 176/1000 | Loss: 0.00000898
Iteration 177/1000 | Loss: 0.00000898
Iteration 178/1000 | Loss: 0.00000898
Iteration 179/1000 | Loss: 0.00000898
Iteration 180/1000 | Loss: 0.00000898
Iteration 181/1000 | Loss: 0.00000898
Iteration 182/1000 | Loss: 0.00000898
Iteration 183/1000 | Loss: 0.00000898
Iteration 184/1000 | Loss: 0.00000898
Iteration 185/1000 | Loss: 0.00000898
Iteration 186/1000 | Loss: 0.00000898
Iteration 187/1000 | Loss: 0.00000898
Iteration 188/1000 | Loss: 0.00000897
Iteration 189/1000 | Loss: 0.00000897
Iteration 190/1000 | Loss: 0.00000897
Iteration 191/1000 | Loss: 0.00000897
Iteration 192/1000 | Loss: 0.00000897
Iteration 193/1000 | Loss: 0.00000897
Iteration 194/1000 | Loss: 0.00000897
Iteration 195/1000 | Loss: 0.00000897
Iteration 196/1000 | Loss: 0.00000896
Iteration 197/1000 | Loss: 0.00000896
Iteration 198/1000 | Loss: 0.00000896
Iteration 199/1000 | Loss: 0.00000896
Iteration 200/1000 | Loss: 0.00000896
Iteration 201/1000 | Loss: 0.00000896
Iteration 202/1000 | Loss: 0.00000896
Iteration 203/1000 | Loss: 0.00000896
Iteration 204/1000 | Loss: 0.00000896
Iteration 205/1000 | Loss: 0.00000896
Iteration 206/1000 | Loss: 0.00000896
Iteration 207/1000 | Loss: 0.00000896
Iteration 208/1000 | Loss: 0.00000896
Iteration 209/1000 | Loss: 0.00000896
Iteration 210/1000 | Loss: 0.00000896
Iteration 211/1000 | Loss: 0.00000896
Iteration 212/1000 | Loss: 0.00000896
Iteration 213/1000 | Loss: 0.00000896
Iteration 214/1000 | Loss: 0.00000895
Iteration 215/1000 | Loss: 0.00000895
Iteration 216/1000 | Loss: 0.00000895
Iteration 217/1000 | Loss: 0.00000895
Iteration 218/1000 | Loss: 0.00000895
Iteration 219/1000 | Loss: 0.00000895
Iteration 220/1000 | Loss: 0.00000895
Iteration 221/1000 | Loss: 0.00000895
Iteration 222/1000 | Loss: 0.00000895
Iteration 223/1000 | Loss: 0.00000895
Iteration 224/1000 | Loss: 0.00000895
Iteration 225/1000 | Loss: 0.00000895
Iteration 226/1000 | Loss: 0.00000895
Iteration 227/1000 | Loss: 0.00000894
Iteration 228/1000 | Loss: 0.00000894
Iteration 229/1000 | Loss: 0.00000894
Iteration 230/1000 | Loss: 0.00000894
Iteration 231/1000 | Loss: 0.00000894
Iteration 232/1000 | Loss: 0.00000894
Iteration 233/1000 | Loss: 0.00000894
Iteration 234/1000 | Loss: 0.00000894
Iteration 235/1000 | Loss: 0.00000894
Iteration 236/1000 | Loss: 0.00000894
Iteration 237/1000 | Loss: 0.00000894
Iteration 238/1000 | Loss: 0.00000894
Iteration 239/1000 | Loss: 0.00000894
Iteration 240/1000 | Loss: 0.00000894
Iteration 241/1000 | Loss: 0.00000894
Iteration 242/1000 | Loss: 0.00000894
Iteration 243/1000 | Loss: 0.00000894
Iteration 244/1000 | Loss: 0.00000894
Iteration 245/1000 | Loss: 0.00000894
Iteration 246/1000 | Loss: 0.00000893
Iteration 247/1000 | Loss: 0.00000893
Iteration 248/1000 | Loss: 0.00000893
Iteration 249/1000 | Loss: 0.00000893
Iteration 250/1000 | Loss: 0.00000893
Iteration 251/1000 | Loss: 0.00000893
Iteration 252/1000 | Loss: 0.00000893
Iteration 253/1000 | Loss: 0.00000893
Iteration 254/1000 | Loss: 0.00000893
Iteration 255/1000 | Loss: 0.00000893
Iteration 256/1000 | Loss: 0.00000893
Iteration 257/1000 | Loss: 0.00000893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [8.934734978538472e-06, 8.934734978538472e-06, 8.934734978538472e-06, 8.934734978538472e-06, 8.934734978538472e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.934734978538472e-06

Optimization complete. Final v2v error: 2.5983545780181885 mm

Highest mean error: 2.7586777210235596 mm for frame 26

Lowest mean error: 2.503080129623413 mm for frame 43

Saving results

Total time: 42.10498642921448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016071
Iteration 2/25 | Loss: 0.01016071
Iteration 3/25 | Loss: 0.01016071
Iteration 4/25 | Loss: 0.01016071
Iteration 5/25 | Loss: 0.01016071
Iteration 6/25 | Loss: 0.01016070
Iteration 7/25 | Loss: 0.01016070
Iteration 8/25 | Loss: 0.01016070
Iteration 9/25 | Loss: 0.01016069
Iteration 10/25 | Loss: 0.00200026
Iteration 11/25 | Loss: 0.00163678
Iteration 12/25 | Loss: 0.00149906
Iteration 13/25 | Loss: 0.00149634
Iteration 14/25 | Loss: 0.00136875
Iteration 15/25 | Loss: 0.00135648
Iteration 16/25 | Loss: 0.00134463
Iteration 17/25 | Loss: 0.00132702
Iteration 18/25 | Loss: 0.00131525
Iteration 19/25 | Loss: 0.00131344
Iteration 20/25 | Loss: 0.00131179
Iteration 21/25 | Loss: 0.00130942
Iteration 22/25 | Loss: 0.00130309
Iteration 23/25 | Loss: 0.00130489
Iteration 24/25 | Loss: 0.00130632
Iteration 25/25 | Loss: 0.00130511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29364932
Iteration 2/25 | Loss: 0.00254104
Iteration 3/25 | Loss: 0.00205538
Iteration 4/25 | Loss: 0.00205538
Iteration 5/25 | Loss: 0.00205538
Iteration 6/25 | Loss: 0.00205538
Iteration 7/25 | Loss: 0.00205538
Iteration 8/25 | Loss: 0.00205538
Iteration 9/25 | Loss: 0.00205538
Iteration 10/25 | Loss: 0.00205538
Iteration 11/25 | Loss: 0.00205538
Iteration 12/25 | Loss: 0.00205538
Iteration 13/25 | Loss: 0.00205538
Iteration 14/25 | Loss: 0.00205538
Iteration 15/25 | Loss: 0.00205538
Iteration 16/25 | Loss: 0.00205538
Iteration 17/25 | Loss: 0.00205538
Iteration 18/25 | Loss: 0.00205538
Iteration 19/25 | Loss: 0.00205538
Iteration 20/25 | Loss: 0.00205538
Iteration 21/25 | Loss: 0.00205538
Iteration 22/25 | Loss: 0.00205538
Iteration 23/25 | Loss: 0.00205538
Iteration 24/25 | Loss: 0.00205538
Iteration 25/25 | Loss: 0.00205538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205538
Iteration 2/1000 | Loss: 0.00071756
Iteration 3/1000 | Loss: 0.00055261
Iteration 4/1000 | Loss: 0.00016638
Iteration 5/1000 | Loss: 0.00053772
Iteration 6/1000 | Loss: 0.00025153
Iteration 7/1000 | Loss: 0.00055204
Iteration 8/1000 | Loss: 0.00041589
Iteration 9/1000 | Loss: 0.00011735
Iteration 10/1000 | Loss: 0.00009168
Iteration 11/1000 | Loss: 0.00014845
Iteration 12/1000 | Loss: 0.00007416
Iteration 13/1000 | Loss: 0.00018239
Iteration 14/1000 | Loss: 0.00008191
Iteration 15/1000 | Loss: 0.00052926
Iteration 16/1000 | Loss: 0.00027326
Iteration 17/1000 | Loss: 0.00024393
Iteration 18/1000 | Loss: 0.00007273
Iteration 19/1000 | Loss: 0.00026272
Iteration 20/1000 | Loss: 0.00016825
Iteration 21/1000 | Loss: 0.00009610
Iteration 22/1000 | Loss: 0.00013541
Iteration 23/1000 | Loss: 0.00007986
Iteration 24/1000 | Loss: 0.00018000
Iteration 25/1000 | Loss: 0.00014032
Iteration 26/1000 | Loss: 0.00014067
Iteration 27/1000 | Loss: 0.00030618
Iteration 28/1000 | Loss: 0.00014734
Iteration 29/1000 | Loss: 0.00006310
Iteration 30/1000 | Loss: 0.00015730
Iteration 31/1000 | Loss: 0.00014417
Iteration 32/1000 | Loss: 0.00012763
Iteration 33/1000 | Loss: 0.00017530
Iteration 34/1000 | Loss: 0.00034156
Iteration 35/1000 | Loss: 0.00015046
Iteration 36/1000 | Loss: 0.00012310
Iteration 37/1000 | Loss: 0.00022775
Iteration 38/1000 | Loss: 0.00008998
Iteration 39/1000 | Loss: 0.00015390
Iteration 40/1000 | Loss: 0.00013767
Iteration 41/1000 | Loss: 0.00013169
Iteration 42/1000 | Loss: 0.00007556
Iteration 43/1000 | Loss: 0.00018327
Iteration 44/1000 | Loss: 0.00004963
Iteration 45/1000 | Loss: 0.00017315
Iteration 46/1000 | Loss: 0.00006449
Iteration 47/1000 | Loss: 0.00004963
Iteration 48/1000 | Loss: 0.00004685
Iteration 49/1000 | Loss: 0.00020281
Iteration 50/1000 | Loss: 0.00016308
Iteration 51/1000 | Loss: 0.00006550
Iteration 52/1000 | Loss: 0.00005704
Iteration 53/1000 | Loss: 0.00019694
Iteration 54/1000 | Loss: 0.00006021
Iteration 55/1000 | Loss: 0.00016833
Iteration 56/1000 | Loss: 0.00018046
Iteration 57/1000 | Loss: 0.00004754
Iteration 58/1000 | Loss: 0.00004313
Iteration 59/1000 | Loss: 0.00006800
Iteration 60/1000 | Loss: 0.00005420
Iteration 61/1000 | Loss: 0.00004186
Iteration 62/1000 | Loss: 0.00004414
Iteration 63/1000 | Loss: 0.00004119
Iteration 64/1000 | Loss: 0.00004082
Iteration 65/1000 | Loss: 0.00071149
Iteration 66/1000 | Loss: 0.00160147
Iteration 67/1000 | Loss: 0.00082960
Iteration 68/1000 | Loss: 0.00044117
Iteration 69/1000 | Loss: 0.00054423
Iteration 70/1000 | Loss: 0.00009441
Iteration 71/1000 | Loss: 0.00008655
Iteration 72/1000 | Loss: 0.00003989
Iteration 73/1000 | Loss: 0.00025517
Iteration 74/1000 | Loss: 0.00022473
Iteration 75/1000 | Loss: 0.00025670
Iteration 76/1000 | Loss: 0.00015085
Iteration 77/1000 | Loss: 0.00005648
Iteration 78/1000 | Loss: 0.00003640
Iteration 79/1000 | Loss: 0.00007759
Iteration 80/1000 | Loss: 0.00003729
Iteration 81/1000 | Loss: 0.00003207
Iteration 82/1000 | Loss: 0.00013314
Iteration 83/1000 | Loss: 0.00009421
Iteration 84/1000 | Loss: 0.00003559
Iteration 85/1000 | Loss: 0.00005652
Iteration 86/1000 | Loss: 0.00003078
Iteration 87/1000 | Loss: 0.00003051
Iteration 88/1000 | Loss: 0.00004959
Iteration 89/1000 | Loss: 0.00003212
Iteration 90/1000 | Loss: 0.00003007
Iteration 91/1000 | Loss: 0.00011714
Iteration 92/1000 | Loss: 0.00004292
Iteration 93/1000 | Loss: 0.00010783
Iteration 94/1000 | Loss: 0.00014236
Iteration 95/1000 | Loss: 0.00010289
Iteration 96/1000 | Loss: 0.00009316
Iteration 97/1000 | Loss: 0.00009246
Iteration 98/1000 | Loss: 0.00015593
Iteration 99/1000 | Loss: 0.00007538
Iteration 100/1000 | Loss: 0.00004223
Iteration 101/1000 | Loss: 0.00004655
Iteration 102/1000 | Loss: 0.00002953
Iteration 103/1000 | Loss: 0.00003070
Iteration 104/1000 | Loss: 0.00009137
Iteration 105/1000 | Loss: 0.00019439
Iteration 106/1000 | Loss: 0.00009189
Iteration 107/1000 | Loss: 0.00012202
Iteration 108/1000 | Loss: 0.00002975
Iteration 109/1000 | Loss: 0.00002920
Iteration 110/1000 | Loss: 0.00002915
Iteration 111/1000 | Loss: 0.00002915
Iteration 112/1000 | Loss: 0.00002915
Iteration 113/1000 | Loss: 0.00002915
Iteration 114/1000 | Loss: 0.00002914
Iteration 115/1000 | Loss: 0.00002914
Iteration 116/1000 | Loss: 0.00002913
Iteration 117/1000 | Loss: 0.00002913
Iteration 118/1000 | Loss: 0.00002910
Iteration 119/1000 | Loss: 0.00002910
Iteration 120/1000 | Loss: 0.00002909
Iteration 121/1000 | Loss: 0.00002909
Iteration 122/1000 | Loss: 0.00002908
Iteration 123/1000 | Loss: 0.00002908
Iteration 124/1000 | Loss: 0.00002908
Iteration 125/1000 | Loss: 0.00002908
Iteration 126/1000 | Loss: 0.00002907
Iteration 127/1000 | Loss: 0.00002907
Iteration 128/1000 | Loss: 0.00002907
Iteration 129/1000 | Loss: 0.00002907
Iteration 130/1000 | Loss: 0.00002907
Iteration 131/1000 | Loss: 0.00002907
Iteration 132/1000 | Loss: 0.00002907
Iteration 133/1000 | Loss: 0.00002907
Iteration 134/1000 | Loss: 0.00002906
Iteration 135/1000 | Loss: 0.00002906
Iteration 136/1000 | Loss: 0.00008742
Iteration 137/1000 | Loss: 0.00008092
Iteration 138/1000 | Loss: 0.00018370
Iteration 139/1000 | Loss: 0.00005839
Iteration 140/1000 | Loss: 0.00007693
Iteration 141/1000 | Loss: 0.00003411
Iteration 142/1000 | Loss: 0.00003212
Iteration 143/1000 | Loss: 0.00002909
Iteration 144/1000 | Loss: 0.00002905
Iteration 145/1000 | Loss: 0.00002903
Iteration 146/1000 | Loss: 0.00002903
Iteration 147/1000 | Loss: 0.00009401
Iteration 148/1000 | Loss: 0.00007828
Iteration 149/1000 | Loss: 0.00003106
Iteration 150/1000 | Loss: 0.00002905
Iteration 151/1000 | Loss: 0.00002903
Iteration 152/1000 | Loss: 0.00002903
Iteration 153/1000 | Loss: 0.00002903
Iteration 154/1000 | Loss: 0.00002902
Iteration 155/1000 | Loss: 0.00002901
Iteration 156/1000 | Loss: 0.00002900
Iteration 157/1000 | Loss: 0.00002900
Iteration 158/1000 | Loss: 0.00009460
Iteration 159/1000 | Loss: 0.00007106
Iteration 160/1000 | Loss: 0.00009878
Iteration 161/1000 | Loss: 0.00005944
Iteration 162/1000 | Loss: 0.00009144
Iteration 163/1000 | Loss: 0.00009778
Iteration 164/1000 | Loss: 0.00010043
Iteration 165/1000 | Loss: 0.00008803
Iteration 166/1000 | Loss: 0.00002980
Iteration 167/1000 | Loss: 0.00002910
Iteration 168/1000 | Loss: 0.00002908
Iteration 169/1000 | Loss: 0.00002905
Iteration 170/1000 | Loss: 0.00002905
Iteration 171/1000 | Loss: 0.00002904
Iteration 172/1000 | Loss: 0.00002903
Iteration 173/1000 | Loss: 0.00002903
Iteration 174/1000 | Loss: 0.00002900
Iteration 175/1000 | Loss: 0.00002900
Iteration 176/1000 | Loss: 0.00002900
Iteration 177/1000 | Loss: 0.00002899
Iteration 178/1000 | Loss: 0.00002899
Iteration 179/1000 | Loss: 0.00002899
Iteration 180/1000 | Loss: 0.00002898
Iteration 181/1000 | Loss: 0.00002898
Iteration 182/1000 | Loss: 0.00002898
Iteration 183/1000 | Loss: 0.00002898
Iteration 184/1000 | Loss: 0.00002898
Iteration 185/1000 | Loss: 0.00002897
Iteration 186/1000 | Loss: 0.00002897
Iteration 187/1000 | Loss: 0.00002897
Iteration 188/1000 | Loss: 0.00002897
Iteration 189/1000 | Loss: 0.00002897
Iteration 190/1000 | Loss: 0.00002897
Iteration 191/1000 | Loss: 0.00002896
Iteration 192/1000 | Loss: 0.00002896
Iteration 193/1000 | Loss: 0.00002896
Iteration 194/1000 | Loss: 0.00002896
Iteration 195/1000 | Loss: 0.00002896
Iteration 196/1000 | Loss: 0.00002896
Iteration 197/1000 | Loss: 0.00002896
Iteration 198/1000 | Loss: 0.00002895
Iteration 199/1000 | Loss: 0.00002895
Iteration 200/1000 | Loss: 0.00002895
Iteration 201/1000 | Loss: 0.00002895
Iteration 202/1000 | Loss: 0.00002895
Iteration 203/1000 | Loss: 0.00002895
Iteration 204/1000 | Loss: 0.00002895
Iteration 205/1000 | Loss: 0.00002895
Iteration 206/1000 | Loss: 0.00002895
Iteration 207/1000 | Loss: 0.00002894
Iteration 208/1000 | Loss: 0.00002894
Iteration 209/1000 | Loss: 0.00002894
Iteration 210/1000 | Loss: 0.00002894
Iteration 211/1000 | Loss: 0.00002894
Iteration 212/1000 | Loss: 0.00002894
Iteration 213/1000 | Loss: 0.00002894
Iteration 214/1000 | Loss: 0.00002894
Iteration 215/1000 | Loss: 0.00002894
Iteration 216/1000 | Loss: 0.00002894
Iteration 217/1000 | Loss: 0.00002894
Iteration 218/1000 | Loss: 0.00002894
Iteration 219/1000 | Loss: 0.00002894
Iteration 220/1000 | Loss: 0.00002894
Iteration 221/1000 | Loss: 0.00002894
Iteration 222/1000 | Loss: 0.00002894
Iteration 223/1000 | Loss: 0.00002894
Iteration 224/1000 | Loss: 0.00002894
Iteration 225/1000 | Loss: 0.00002894
Iteration 226/1000 | Loss: 0.00002894
Iteration 227/1000 | Loss: 0.00002894
Iteration 228/1000 | Loss: 0.00002893
Iteration 229/1000 | Loss: 0.00002893
Iteration 230/1000 | Loss: 0.00002893
Iteration 231/1000 | Loss: 0.00002893
Iteration 232/1000 | Loss: 0.00002893
Iteration 233/1000 | Loss: 0.00002893
Iteration 234/1000 | Loss: 0.00002893
Iteration 235/1000 | Loss: 0.00002893
Iteration 236/1000 | Loss: 0.00002893
Iteration 237/1000 | Loss: 0.00002893
Iteration 238/1000 | Loss: 0.00002893
Iteration 239/1000 | Loss: 0.00002893
Iteration 240/1000 | Loss: 0.00002893
Iteration 241/1000 | Loss: 0.00002893
Iteration 242/1000 | Loss: 0.00002893
Iteration 243/1000 | Loss: 0.00002892
Iteration 244/1000 | Loss: 0.00002892
Iteration 245/1000 | Loss: 0.00002892
Iteration 246/1000 | Loss: 0.00002891
Iteration 247/1000 | Loss: 0.00002891
Iteration 248/1000 | Loss: 0.00002891
Iteration 249/1000 | Loss: 0.00002891
Iteration 250/1000 | Loss: 0.00002891
Iteration 251/1000 | Loss: 0.00002891
Iteration 252/1000 | Loss: 0.00002890
Iteration 253/1000 | Loss: 0.00002890
Iteration 254/1000 | Loss: 0.00002890
Iteration 255/1000 | Loss: 0.00002890
Iteration 256/1000 | Loss: 0.00002890
Iteration 257/1000 | Loss: 0.00002890
Iteration 258/1000 | Loss: 0.00002890
Iteration 259/1000 | Loss: 0.00002890
Iteration 260/1000 | Loss: 0.00002890
Iteration 261/1000 | Loss: 0.00002890
Iteration 262/1000 | Loss: 0.00002890
Iteration 263/1000 | Loss: 0.00002890
Iteration 264/1000 | Loss: 0.00002890
Iteration 265/1000 | Loss: 0.00002889
Iteration 266/1000 | Loss: 0.00002889
Iteration 267/1000 | Loss: 0.00002889
Iteration 268/1000 | Loss: 0.00002889
Iteration 269/1000 | Loss: 0.00002889
Iteration 270/1000 | Loss: 0.00002889
Iteration 271/1000 | Loss: 0.00002889
Iteration 272/1000 | Loss: 0.00002889
Iteration 273/1000 | Loss: 0.00002888
Iteration 274/1000 | Loss: 0.00002888
Iteration 275/1000 | Loss: 0.00002888
Iteration 276/1000 | Loss: 0.00002888
Iteration 277/1000 | Loss: 0.00002888
Iteration 278/1000 | Loss: 0.00002888
Iteration 279/1000 | Loss: 0.00002888
Iteration 280/1000 | Loss: 0.00002888
Iteration 281/1000 | Loss: 0.00002888
Iteration 282/1000 | Loss: 0.00002888
Iteration 283/1000 | Loss: 0.00002888
Iteration 284/1000 | Loss: 0.00002888
Iteration 285/1000 | Loss: 0.00002888
Iteration 286/1000 | Loss: 0.00002888
Iteration 287/1000 | Loss: 0.00002888
Iteration 288/1000 | Loss: 0.00002888
Iteration 289/1000 | Loss: 0.00002888
Iteration 290/1000 | Loss: 0.00002888
Iteration 291/1000 | Loss: 0.00002888
Iteration 292/1000 | Loss: 0.00002887
Iteration 293/1000 | Loss: 0.00002887
Iteration 294/1000 | Loss: 0.00002887
Iteration 295/1000 | Loss: 0.00002887
Iteration 296/1000 | Loss: 0.00002887
Iteration 297/1000 | Loss: 0.00002887
Iteration 298/1000 | Loss: 0.00002887
Iteration 299/1000 | Loss: 0.00002887
Iteration 300/1000 | Loss: 0.00002887
Iteration 301/1000 | Loss: 0.00002887
Iteration 302/1000 | Loss: 0.00002887
Iteration 303/1000 | Loss: 0.00002887
Iteration 304/1000 | Loss: 0.00002887
Iteration 305/1000 | Loss: 0.00002887
Iteration 306/1000 | Loss: 0.00002887
Iteration 307/1000 | Loss: 0.00002887
Iteration 308/1000 | Loss: 0.00002887
Iteration 309/1000 | Loss: 0.00002887
Iteration 310/1000 | Loss: 0.00002887
Iteration 311/1000 | Loss: 0.00002887
Iteration 312/1000 | Loss: 0.00002887
Iteration 313/1000 | Loss: 0.00002887
Iteration 314/1000 | Loss: 0.00002887
Iteration 315/1000 | Loss: 0.00002887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [2.8869801099062897e-05, 2.8869801099062897e-05, 2.8869801099062897e-05, 2.8869801099062897e-05, 2.8869801099062897e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8869801099062897e-05

Optimization complete. Final v2v error: 3.251981735229492 mm

Highest mean error: 10.285285949707031 mm for frame 181

Lowest mean error: 2.606572389602661 mm for frame 34

Saving results

Total time: 257.4728353023529
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866531
Iteration 2/25 | Loss: 0.00155068
Iteration 3/25 | Loss: 0.00129648
Iteration 4/25 | Loss: 0.00127704
Iteration 5/25 | Loss: 0.00127377
Iteration 6/25 | Loss: 0.00127377
Iteration 7/25 | Loss: 0.00127377
Iteration 8/25 | Loss: 0.00127377
Iteration 9/25 | Loss: 0.00127377
Iteration 10/25 | Loss: 0.00127377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012737681390717626, 0.0012737681390717626, 0.0012737681390717626, 0.0012737681390717626, 0.0012737681390717626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012737681390717626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86036944
Iteration 2/25 | Loss: 0.00090358
Iteration 3/25 | Loss: 0.00090358
Iteration 4/25 | Loss: 0.00090358
Iteration 5/25 | Loss: 0.00090358
Iteration 6/25 | Loss: 0.00090358
Iteration 7/25 | Loss: 0.00090358
Iteration 8/25 | Loss: 0.00090358
Iteration 9/25 | Loss: 0.00090358
Iteration 10/25 | Loss: 0.00090358
Iteration 11/25 | Loss: 0.00090358
Iteration 12/25 | Loss: 0.00090358
Iteration 13/25 | Loss: 0.00090358
Iteration 14/25 | Loss: 0.00090358
Iteration 15/25 | Loss: 0.00090358
Iteration 16/25 | Loss: 0.00090358
Iteration 17/25 | Loss: 0.00090358
Iteration 18/25 | Loss: 0.00090358
Iteration 19/25 | Loss: 0.00090358
Iteration 20/25 | Loss: 0.00090358
Iteration 21/25 | Loss: 0.00090358
Iteration 22/25 | Loss: 0.00090358
Iteration 23/25 | Loss: 0.00090358
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009035783004947007, 0.0009035783004947007, 0.0009035783004947007, 0.0009035783004947007, 0.0009035783004947007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009035783004947007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090358
Iteration 2/1000 | Loss: 0.00004020
Iteration 3/1000 | Loss: 0.00003242
Iteration 4/1000 | Loss: 0.00003065
Iteration 5/1000 | Loss: 0.00002966
Iteration 6/1000 | Loss: 0.00002880
Iteration 7/1000 | Loss: 0.00002812
Iteration 8/1000 | Loss: 0.00002783
Iteration 9/1000 | Loss: 0.00002763
Iteration 10/1000 | Loss: 0.00002733
Iteration 11/1000 | Loss: 0.00002721
Iteration 12/1000 | Loss: 0.00002701
Iteration 13/1000 | Loss: 0.00002700
Iteration 14/1000 | Loss: 0.00002681
Iteration 15/1000 | Loss: 0.00002678
Iteration 16/1000 | Loss: 0.00002667
Iteration 17/1000 | Loss: 0.00002665
Iteration 18/1000 | Loss: 0.00002665
Iteration 19/1000 | Loss: 0.00002665
Iteration 20/1000 | Loss: 0.00002665
Iteration 21/1000 | Loss: 0.00002665
Iteration 22/1000 | Loss: 0.00002665
Iteration 23/1000 | Loss: 0.00002664
Iteration 24/1000 | Loss: 0.00002664
Iteration 25/1000 | Loss: 0.00002664
Iteration 26/1000 | Loss: 0.00002664
Iteration 27/1000 | Loss: 0.00002664
Iteration 28/1000 | Loss: 0.00002664
Iteration 29/1000 | Loss: 0.00002664
Iteration 30/1000 | Loss: 0.00002664
Iteration 31/1000 | Loss: 0.00002664
Iteration 32/1000 | Loss: 0.00002664
Iteration 33/1000 | Loss: 0.00002664
Iteration 34/1000 | Loss: 0.00002663
Iteration 35/1000 | Loss: 0.00002663
Iteration 36/1000 | Loss: 0.00002662
Iteration 37/1000 | Loss: 0.00002662
Iteration 38/1000 | Loss: 0.00002662
Iteration 39/1000 | Loss: 0.00002662
Iteration 40/1000 | Loss: 0.00002662
Iteration 41/1000 | Loss: 0.00002662
Iteration 42/1000 | Loss: 0.00002662
Iteration 43/1000 | Loss: 0.00002661
Iteration 44/1000 | Loss: 0.00002660
Iteration 45/1000 | Loss: 0.00002660
Iteration 46/1000 | Loss: 0.00002660
Iteration 47/1000 | Loss: 0.00002660
Iteration 48/1000 | Loss: 0.00002659
Iteration 49/1000 | Loss: 0.00002659
Iteration 50/1000 | Loss: 0.00002659
Iteration 51/1000 | Loss: 0.00002659
Iteration 52/1000 | Loss: 0.00002659
Iteration 53/1000 | Loss: 0.00002659
Iteration 54/1000 | Loss: 0.00002659
Iteration 55/1000 | Loss: 0.00002659
Iteration 56/1000 | Loss: 0.00002658
Iteration 57/1000 | Loss: 0.00002658
Iteration 58/1000 | Loss: 0.00002658
Iteration 59/1000 | Loss: 0.00002658
Iteration 60/1000 | Loss: 0.00002658
Iteration 61/1000 | Loss: 0.00002658
Iteration 62/1000 | Loss: 0.00002658
Iteration 63/1000 | Loss: 0.00002658
Iteration 64/1000 | Loss: 0.00002658
Iteration 65/1000 | Loss: 0.00002658
Iteration 66/1000 | Loss: 0.00002657
Iteration 67/1000 | Loss: 0.00002657
Iteration 68/1000 | Loss: 0.00002657
Iteration 69/1000 | Loss: 0.00002657
Iteration 70/1000 | Loss: 0.00002657
Iteration 71/1000 | Loss: 0.00002656
Iteration 72/1000 | Loss: 0.00002656
Iteration 73/1000 | Loss: 0.00002656
Iteration 74/1000 | Loss: 0.00002656
Iteration 75/1000 | Loss: 0.00002656
Iteration 76/1000 | Loss: 0.00002656
Iteration 77/1000 | Loss: 0.00002656
Iteration 78/1000 | Loss: 0.00002656
Iteration 79/1000 | Loss: 0.00002656
Iteration 80/1000 | Loss: 0.00002656
Iteration 81/1000 | Loss: 0.00002656
Iteration 82/1000 | Loss: 0.00002655
Iteration 83/1000 | Loss: 0.00002655
Iteration 84/1000 | Loss: 0.00002654
Iteration 85/1000 | Loss: 0.00002654
Iteration 86/1000 | Loss: 0.00002654
Iteration 87/1000 | Loss: 0.00002654
Iteration 88/1000 | Loss: 0.00002654
Iteration 89/1000 | Loss: 0.00002654
Iteration 90/1000 | Loss: 0.00002654
Iteration 91/1000 | Loss: 0.00002654
Iteration 92/1000 | Loss: 0.00002654
Iteration 93/1000 | Loss: 0.00002654
Iteration 94/1000 | Loss: 0.00002654
Iteration 95/1000 | Loss: 0.00002653
Iteration 96/1000 | Loss: 0.00002653
Iteration 97/1000 | Loss: 0.00002652
Iteration 98/1000 | Loss: 0.00002652
Iteration 99/1000 | Loss: 0.00002652
Iteration 100/1000 | Loss: 0.00002652
Iteration 101/1000 | Loss: 0.00002651
Iteration 102/1000 | Loss: 0.00002651
Iteration 103/1000 | Loss: 0.00002651
Iteration 104/1000 | Loss: 0.00002651
Iteration 105/1000 | Loss: 0.00002651
Iteration 106/1000 | Loss: 0.00002651
Iteration 107/1000 | Loss: 0.00002651
Iteration 108/1000 | Loss: 0.00002651
Iteration 109/1000 | Loss: 0.00002651
Iteration 110/1000 | Loss: 0.00002651
Iteration 111/1000 | Loss: 0.00002651
Iteration 112/1000 | Loss: 0.00002651
Iteration 113/1000 | Loss: 0.00002651
Iteration 114/1000 | Loss: 0.00002651
Iteration 115/1000 | Loss: 0.00002651
Iteration 116/1000 | Loss: 0.00002651
Iteration 117/1000 | Loss: 0.00002651
Iteration 118/1000 | Loss: 0.00002651
Iteration 119/1000 | Loss: 0.00002651
Iteration 120/1000 | Loss: 0.00002651
Iteration 121/1000 | Loss: 0.00002651
Iteration 122/1000 | Loss: 0.00002651
Iteration 123/1000 | Loss: 0.00002651
Iteration 124/1000 | Loss: 0.00002651
Iteration 125/1000 | Loss: 0.00002651
Iteration 126/1000 | Loss: 0.00002651
Iteration 127/1000 | Loss: 0.00002651
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.6508418159210123e-05, 2.6508418159210123e-05, 2.6508418159210123e-05, 2.6508418159210123e-05, 2.6508418159210123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6508418159210123e-05

Optimization complete. Final v2v error: 4.400087356567383 mm

Highest mean error: 4.688746929168701 mm for frame 1

Lowest mean error: 4.072760105133057 mm for frame 81

Saving results

Total time: 32.54206562042236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967342
Iteration 2/25 | Loss: 0.00147788
Iteration 3/25 | Loss: 0.00136470
Iteration 4/25 | Loss: 0.00134741
Iteration 5/25 | Loss: 0.00134246
Iteration 6/25 | Loss: 0.00134199
Iteration 7/25 | Loss: 0.00134199
Iteration 8/25 | Loss: 0.00134199
Iteration 9/25 | Loss: 0.00134199
Iteration 10/25 | Loss: 0.00134199
Iteration 11/25 | Loss: 0.00134199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013419865863397717, 0.0013419865863397717, 0.0013419865863397717, 0.0013419865863397717, 0.0013419865863397717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013419865863397717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.57346964
Iteration 2/25 | Loss: 0.00178021
Iteration 3/25 | Loss: 0.00178019
Iteration 4/25 | Loss: 0.00178019
Iteration 5/25 | Loss: 0.00178019
Iteration 6/25 | Loss: 0.00178019
Iteration 7/25 | Loss: 0.00178019
Iteration 8/25 | Loss: 0.00178019
Iteration 9/25 | Loss: 0.00178019
Iteration 10/25 | Loss: 0.00178019
Iteration 11/25 | Loss: 0.00178019
Iteration 12/25 | Loss: 0.00178019
Iteration 13/25 | Loss: 0.00178019
Iteration 14/25 | Loss: 0.00178019
Iteration 15/25 | Loss: 0.00178019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001780192251317203, 0.001780192251317203, 0.001780192251317203, 0.001780192251317203, 0.001780192251317203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001780192251317203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178019
Iteration 2/1000 | Loss: 0.00004401
Iteration 3/1000 | Loss: 0.00002596
Iteration 4/1000 | Loss: 0.00002358
Iteration 5/1000 | Loss: 0.00002250
Iteration 6/1000 | Loss: 0.00002187
Iteration 7/1000 | Loss: 0.00002145
Iteration 8/1000 | Loss: 0.00002107
Iteration 9/1000 | Loss: 0.00002082
Iteration 10/1000 | Loss: 0.00002047
Iteration 11/1000 | Loss: 0.00002026
Iteration 12/1000 | Loss: 0.00002019
Iteration 13/1000 | Loss: 0.00002002
Iteration 14/1000 | Loss: 0.00001998
Iteration 15/1000 | Loss: 0.00001990
Iteration 16/1000 | Loss: 0.00001977
Iteration 17/1000 | Loss: 0.00001976
Iteration 18/1000 | Loss: 0.00001976
Iteration 19/1000 | Loss: 0.00001974
Iteration 20/1000 | Loss: 0.00001973
Iteration 21/1000 | Loss: 0.00001969
Iteration 22/1000 | Loss: 0.00001968
Iteration 23/1000 | Loss: 0.00001964
Iteration 24/1000 | Loss: 0.00001959
Iteration 25/1000 | Loss: 0.00001958
Iteration 26/1000 | Loss: 0.00001958
Iteration 27/1000 | Loss: 0.00001957
Iteration 28/1000 | Loss: 0.00001954
Iteration 29/1000 | Loss: 0.00001954
Iteration 30/1000 | Loss: 0.00001952
Iteration 31/1000 | Loss: 0.00001951
Iteration 32/1000 | Loss: 0.00001951
Iteration 33/1000 | Loss: 0.00001950
Iteration 34/1000 | Loss: 0.00001950
Iteration 35/1000 | Loss: 0.00001949
Iteration 36/1000 | Loss: 0.00001949
Iteration 37/1000 | Loss: 0.00001949
Iteration 38/1000 | Loss: 0.00001948
Iteration 39/1000 | Loss: 0.00001948
Iteration 40/1000 | Loss: 0.00001948
Iteration 41/1000 | Loss: 0.00001948
Iteration 42/1000 | Loss: 0.00001947
Iteration 43/1000 | Loss: 0.00001947
Iteration 44/1000 | Loss: 0.00001947
Iteration 45/1000 | Loss: 0.00001947
Iteration 46/1000 | Loss: 0.00001947
Iteration 47/1000 | Loss: 0.00001947
Iteration 48/1000 | Loss: 0.00001947
Iteration 49/1000 | Loss: 0.00001947
Iteration 50/1000 | Loss: 0.00001946
Iteration 51/1000 | Loss: 0.00001946
Iteration 52/1000 | Loss: 0.00001946
Iteration 53/1000 | Loss: 0.00001946
Iteration 54/1000 | Loss: 0.00001946
Iteration 55/1000 | Loss: 0.00001946
Iteration 56/1000 | Loss: 0.00001946
Iteration 57/1000 | Loss: 0.00001945
Iteration 58/1000 | Loss: 0.00001945
Iteration 59/1000 | Loss: 0.00001945
Iteration 60/1000 | Loss: 0.00001944
Iteration 61/1000 | Loss: 0.00001944
Iteration 62/1000 | Loss: 0.00001944
Iteration 63/1000 | Loss: 0.00001944
Iteration 64/1000 | Loss: 0.00001944
Iteration 65/1000 | Loss: 0.00001944
Iteration 66/1000 | Loss: 0.00001944
Iteration 67/1000 | Loss: 0.00001944
Iteration 68/1000 | Loss: 0.00001944
Iteration 69/1000 | Loss: 0.00001943
Iteration 70/1000 | Loss: 0.00001943
Iteration 71/1000 | Loss: 0.00001943
Iteration 72/1000 | Loss: 0.00001943
Iteration 73/1000 | Loss: 0.00001943
Iteration 74/1000 | Loss: 0.00001942
Iteration 75/1000 | Loss: 0.00001942
Iteration 76/1000 | Loss: 0.00001942
Iteration 77/1000 | Loss: 0.00001942
Iteration 78/1000 | Loss: 0.00001942
Iteration 79/1000 | Loss: 0.00001941
Iteration 80/1000 | Loss: 0.00001941
Iteration 81/1000 | Loss: 0.00001941
Iteration 82/1000 | Loss: 0.00001940
Iteration 83/1000 | Loss: 0.00001940
Iteration 84/1000 | Loss: 0.00001940
Iteration 85/1000 | Loss: 0.00001939
Iteration 86/1000 | Loss: 0.00001939
Iteration 87/1000 | Loss: 0.00001939
Iteration 88/1000 | Loss: 0.00001939
Iteration 89/1000 | Loss: 0.00001939
Iteration 90/1000 | Loss: 0.00001939
Iteration 91/1000 | Loss: 0.00001939
Iteration 92/1000 | Loss: 0.00001938
Iteration 93/1000 | Loss: 0.00001938
Iteration 94/1000 | Loss: 0.00001938
Iteration 95/1000 | Loss: 0.00001938
Iteration 96/1000 | Loss: 0.00001938
Iteration 97/1000 | Loss: 0.00001938
Iteration 98/1000 | Loss: 0.00001937
Iteration 99/1000 | Loss: 0.00001937
Iteration 100/1000 | Loss: 0.00001937
Iteration 101/1000 | Loss: 0.00001937
Iteration 102/1000 | Loss: 0.00001937
Iteration 103/1000 | Loss: 0.00001937
Iteration 104/1000 | Loss: 0.00001936
Iteration 105/1000 | Loss: 0.00001936
Iteration 106/1000 | Loss: 0.00001936
Iteration 107/1000 | Loss: 0.00001936
Iteration 108/1000 | Loss: 0.00001936
Iteration 109/1000 | Loss: 0.00001936
Iteration 110/1000 | Loss: 0.00001936
Iteration 111/1000 | Loss: 0.00001936
Iteration 112/1000 | Loss: 0.00001936
Iteration 113/1000 | Loss: 0.00001936
Iteration 114/1000 | Loss: 0.00001936
Iteration 115/1000 | Loss: 0.00001936
Iteration 116/1000 | Loss: 0.00001936
Iteration 117/1000 | Loss: 0.00001936
Iteration 118/1000 | Loss: 0.00001936
Iteration 119/1000 | Loss: 0.00001936
Iteration 120/1000 | Loss: 0.00001936
Iteration 121/1000 | Loss: 0.00001936
Iteration 122/1000 | Loss: 0.00001935
Iteration 123/1000 | Loss: 0.00001935
Iteration 124/1000 | Loss: 0.00001935
Iteration 125/1000 | Loss: 0.00001935
Iteration 126/1000 | Loss: 0.00001935
Iteration 127/1000 | Loss: 0.00001935
Iteration 128/1000 | Loss: 0.00001935
Iteration 129/1000 | Loss: 0.00001935
Iteration 130/1000 | Loss: 0.00001935
Iteration 131/1000 | Loss: 0.00001935
Iteration 132/1000 | Loss: 0.00001935
Iteration 133/1000 | Loss: 0.00001935
Iteration 134/1000 | Loss: 0.00001935
Iteration 135/1000 | Loss: 0.00001935
Iteration 136/1000 | Loss: 0.00001935
Iteration 137/1000 | Loss: 0.00001934
Iteration 138/1000 | Loss: 0.00001934
Iteration 139/1000 | Loss: 0.00001934
Iteration 140/1000 | Loss: 0.00001934
Iteration 141/1000 | Loss: 0.00001934
Iteration 142/1000 | Loss: 0.00001934
Iteration 143/1000 | Loss: 0.00001934
Iteration 144/1000 | Loss: 0.00001934
Iteration 145/1000 | Loss: 0.00001934
Iteration 146/1000 | Loss: 0.00001934
Iteration 147/1000 | Loss: 0.00001934
Iteration 148/1000 | Loss: 0.00001934
Iteration 149/1000 | Loss: 0.00001934
Iteration 150/1000 | Loss: 0.00001934
Iteration 151/1000 | Loss: 0.00001934
Iteration 152/1000 | Loss: 0.00001934
Iteration 153/1000 | Loss: 0.00001934
Iteration 154/1000 | Loss: 0.00001934
Iteration 155/1000 | Loss: 0.00001934
Iteration 156/1000 | Loss: 0.00001934
Iteration 157/1000 | Loss: 0.00001934
Iteration 158/1000 | Loss: 0.00001933
Iteration 159/1000 | Loss: 0.00001933
Iteration 160/1000 | Loss: 0.00001933
Iteration 161/1000 | Loss: 0.00001933
Iteration 162/1000 | Loss: 0.00001933
Iteration 163/1000 | Loss: 0.00001933
Iteration 164/1000 | Loss: 0.00001933
Iteration 165/1000 | Loss: 0.00001933
Iteration 166/1000 | Loss: 0.00001933
Iteration 167/1000 | Loss: 0.00001933
Iteration 168/1000 | Loss: 0.00001933
Iteration 169/1000 | Loss: 0.00001933
Iteration 170/1000 | Loss: 0.00001933
Iteration 171/1000 | Loss: 0.00001933
Iteration 172/1000 | Loss: 0.00001933
Iteration 173/1000 | Loss: 0.00001933
Iteration 174/1000 | Loss: 0.00001933
Iteration 175/1000 | Loss: 0.00001933
Iteration 176/1000 | Loss: 0.00001933
Iteration 177/1000 | Loss: 0.00001933
Iteration 178/1000 | Loss: 0.00001933
Iteration 179/1000 | Loss: 0.00001933
Iteration 180/1000 | Loss: 0.00001933
Iteration 181/1000 | Loss: 0.00001933
Iteration 182/1000 | Loss: 0.00001933
Iteration 183/1000 | Loss: 0.00001933
Iteration 184/1000 | Loss: 0.00001933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.9330456780153327e-05, 1.9330456780153327e-05, 1.9330456780153327e-05, 1.9330456780153327e-05, 1.9330456780153327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9330456780153327e-05

Optimization complete. Final v2v error: 3.7078378200531006 mm

Highest mean error: 4.000532627105713 mm for frame 21

Lowest mean error: 3.402777910232544 mm for frame 119

Saving results

Total time: 40.06910300254822
