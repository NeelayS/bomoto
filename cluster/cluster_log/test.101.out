Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=101, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5656-5711
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786206
Iteration 2/25 | Loss: 0.00097999
Iteration 3/25 | Loss: 0.00086769
Iteration 4/25 | Loss: 0.00083463
Iteration 5/25 | Loss: 0.00082132
Iteration 6/25 | Loss: 0.00081903
Iteration 7/25 | Loss: 0.00081871
Iteration 8/25 | Loss: 0.00081871
Iteration 9/25 | Loss: 0.00081871
Iteration 10/25 | Loss: 0.00081871
Iteration 11/25 | Loss: 0.00081871
Iteration 12/25 | Loss: 0.00081871
Iteration 13/25 | Loss: 0.00081871
Iteration 14/25 | Loss: 0.00081871
Iteration 15/25 | Loss: 0.00081871
Iteration 16/25 | Loss: 0.00081871
Iteration 17/25 | Loss: 0.00081871
Iteration 18/25 | Loss: 0.00081871
Iteration 19/25 | Loss: 0.00081871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008187066996470094, 0.0008187066996470094, 0.0008187066996470094, 0.0008187066996470094, 0.0008187066996470094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008187066996470094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33554232
Iteration 2/25 | Loss: 0.00048856
Iteration 3/25 | Loss: 0.00048856
Iteration 4/25 | Loss: 0.00048856
Iteration 5/25 | Loss: 0.00048856
Iteration 6/25 | Loss: 0.00048856
Iteration 7/25 | Loss: 0.00048856
Iteration 8/25 | Loss: 0.00048856
Iteration 9/25 | Loss: 0.00048856
Iteration 10/25 | Loss: 0.00048856
Iteration 11/25 | Loss: 0.00048856
Iteration 12/25 | Loss: 0.00048856
Iteration 13/25 | Loss: 0.00048856
Iteration 14/25 | Loss: 0.00048856
Iteration 15/25 | Loss: 0.00048856
Iteration 16/25 | Loss: 0.00048856
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004885575035586953, 0.0004885575035586953, 0.0004885575035586953, 0.0004885575035586953, 0.0004885575035586953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004885575035586953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048856
Iteration 2/1000 | Loss: 0.00003244
Iteration 3/1000 | Loss: 0.00002437
Iteration 4/1000 | Loss: 0.00002261
Iteration 5/1000 | Loss: 0.00002131
Iteration 6/1000 | Loss: 0.00002058
Iteration 7/1000 | Loss: 0.00001992
Iteration 8/1000 | Loss: 0.00001955
Iteration 9/1000 | Loss: 0.00001950
Iteration 10/1000 | Loss: 0.00001948
Iteration 11/1000 | Loss: 0.00001946
Iteration 12/1000 | Loss: 0.00001944
Iteration 13/1000 | Loss: 0.00001942
Iteration 14/1000 | Loss: 0.00001941
Iteration 15/1000 | Loss: 0.00001928
Iteration 16/1000 | Loss: 0.00001928
Iteration 17/1000 | Loss: 0.00001923
Iteration 18/1000 | Loss: 0.00001923
Iteration 19/1000 | Loss: 0.00001923
Iteration 20/1000 | Loss: 0.00001923
Iteration 21/1000 | Loss: 0.00001922
Iteration 22/1000 | Loss: 0.00001922
Iteration 23/1000 | Loss: 0.00001922
Iteration 24/1000 | Loss: 0.00001922
Iteration 25/1000 | Loss: 0.00001921
Iteration 26/1000 | Loss: 0.00001919
Iteration 27/1000 | Loss: 0.00001919
Iteration 28/1000 | Loss: 0.00001919
Iteration 29/1000 | Loss: 0.00001919
Iteration 30/1000 | Loss: 0.00001919
Iteration 31/1000 | Loss: 0.00001919
Iteration 32/1000 | Loss: 0.00001919
Iteration 33/1000 | Loss: 0.00001918
Iteration 34/1000 | Loss: 0.00001918
Iteration 35/1000 | Loss: 0.00001918
Iteration 36/1000 | Loss: 0.00001918
Iteration 37/1000 | Loss: 0.00001918
Iteration 38/1000 | Loss: 0.00001916
Iteration 39/1000 | Loss: 0.00001915
Iteration 40/1000 | Loss: 0.00001915
Iteration 41/1000 | Loss: 0.00001915
Iteration 42/1000 | Loss: 0.00001915
Iteration 43/1000 | Loss: 0.00001914
Iteration 44/1000 | Loss: 0.00001914
Iteration 45/1000 | Loss: 0.00001914
Iteration 46/1000 | Loss: 0.00001914
Iteration 47/1000 | Loss: 0.00001914
Iteration 48/1000 | Loss: 0.00001914
Iteration 49/1000 | Loss: 0.00001914
Iteration 50/1000 | Loss: 0.00001913
Iteration 51/1000 | Loss: 0.00001913
Iteration 52/1000 | Loss: 0.00001913
Iteration 53/1000 | Loss: 0.00001913
Iteration 54/1000 | Loss: 0.00001912
Iteration 55/1000 | Loss: 0.00001912
Iteration 56/1000 | Loss: 0.00001912
Iteration 57/1000 | Loss: 0.00001912
Iteration 58/1000 | Loss: 0.00001912
Iteration 59/1000 | Loss: 0.00001911
Iteration 60/1000 | Loss: 0.00001911
Iteration 61/1000 | Loss: 0.00001910
Iteration 62/1000 | Loss: 0.00001910
Iteration 63/1000 | Loss: 0.00001910
Iteration 64/1000 | Loss: 0.00001910
Iteration 65/1000 | Loss: 0.00001909
Iteration 66/1000 | Loss: 0.00001909
Iteration 67/1000 | Loss: 0.00001909
Iteration 68/1000 | Loss: 0.00001909
Iteration 69/1000 | Loss: 0.00001909
Iteration 70/1000 | Loss: 0.00001909
Iteration 71/1000 | Loss: 0.00001909
Iteration 72/1000 | Loss: 0.00001908
Iteration 73/1000 | Loss: 0.00001908
Iteration 74/1000 | Loss: 0.00001908
Iteration 75/1000 | Loss: 0.00001908
Iteration 76/1000 | Loss: 0.00001908
Iteration 77/1000 | Loss: 0.00001908
Iteration 78/1000 | Loss: 0.00001908
Iteration 79/1000 | Loss: 0.00001908
Iteration 80/1000 | Loss: 0.00001908
Iteration 81/1000 | Loss: 0.00001907
Iteration 82/1000 | Loss: 0.00001907
Iteration 83/1000 | Loss: 0.00001907
Iteration 84/1000 | Loss: 0.00001907
Iteration 85/1000 | Loss: 0.00001907
Iteration 86/1000 | Loss: 0.00001907
Iteration 87/1000 | Loss: 0.00001907
Iteration 88/1000 | Loss: 0.00001907
Iteration 89/1000 | Loss: 0.00001907
Iteration 90/1000 | Loss: 0.00001907
Iteration 91/1000 | Loss: 0.00001907
Iteration 92/1000 | Loss: 0.00001907
Iteration 93/1000 | Loss: 0.00001907
Iteration 94/1000 | Loss: 0.00001907
Iteration 95/1000 | Loss: 0.00001907
Iteration 96/1000 | Loss: 0.00001907
Iteration 97/1000 | Loss: 0.00001906
Iteration 98/1000 | Loss: 0.00001906
Iteration 99/1000 | Loss: 0.00001906
Iteration 100/1000 | Loss: 0.00001906
Iteration 101/1000 | Loss: 0.00001906
Iteration 102/1000 | Loss: 0.00001906
Iteration 103/1000 | Loss: 0.00001906
Iteration 104/1000 | Loss: 0.00001906
Iteration 105/1000 | Loss: 0.00001906
Iteration 106/1000 | Loss: 0.00001906
Iteration 107/1000 | Loss: 0.00001906
Iteration 108/1000 | Loss: 0.00001906
Iteration 109/1000 | Loss: 0.00001906
Iteration 110/1000 | Loss: 0.00001906
Iteration 111/1000 | Loss: 0.00001906
Iteration 112/1000 | Loss: 0.00001906
Iteration 113/1000 | Loss: 0.00001906
Iteration 114/1000 | Loss: 0.00001905
Iteration 115/1000 | Loss: 0.00001905
Iteration 116/1000 | Loss: 0.00001905
Iteration 117/1000 | Loss: 0.00001905
Iteration 118/1000 | Loss: 0.00001905
Iteration 119/1000 | Loss: 0.00001905
Iteration 120/1000 | Loss: 0.00001905
Iteration 121/1000 | Loss: 0.00001905
Iteration 122/1000 | Loss: 0.00001905
Iteration 123/1000 | Loss: 0.00001905
Iteration 124/1000 | Loss: 0.00001905
Iteration 125/1000 | Loss: 0.00001905
Iteration 126/1000 | Loss: 0.00001905
Iteration 127/1000 | Loss: 0.00001904
Iteration 128/1000 | Loss: 0.00001904
Iteration 129/1000 | Loss: 0.00001904
Iteration 130/1000 | Loss: 0.00001904
Iteration 131/1000 | Loss: 0.00001904
Iteration 132/1000 | Loss: 0.00001904
Iteration 133/1000 | Loss: 0.00001904
Iteration 134/1000 | Loss: 0.00001904
Iteration 135/1000 | Loss: 0.00001904
Iteration 136/1000 | Loss: 0.00001904
Iteration 137/1000 | Loss: 0.00001904
Iteration 138/1000 | Loss: 0.00001903
Iteration 139/1000 | Loss: 0.00001903
Iteration 140/1000 | Loss: 0.00001903
Iteration 141/1000 | Loss: 0.00001903
Iteration 142/1000 | Loss: 0.00001903
Iteration 143/1000 | Loss: 0.00001903
Iteration 144/1000 | Loss: 0.00001903
Iteration 145/1000 | Loss: 0.00001903
Iteration 146/1000 | Loss: 0.00001903
Iteration 147/1000 | Loss: 0.00001903
Iteration 148/1000 | Loss: 0.00001903
Iteration 149/1000 | Loss: 0.00001903
Iteration 150/1000 | Loss: 0.00001903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.9033197531825863e-05, 1.9033197531825863e-05, 1.9033197531825863e-05, 1.9033197531825863e-05, 1.9033197531825863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9033197531825863e-05

Optimization complete. Final v2v error: 3.6650142669677734 mm

Highest mean error: 3.8165805339813232 mm for frame 8

Lowest mean error: 3.431729793548584 mm for frame 111

Saving results

Total time: 33.87583947181702
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839549
Iteration 2/25 | Loss: 0.00150374
Iteration 3/25 | Loss: 0.00099651
Iteration 4/25 | Loss: 0.00092123
Iteration 5/25 | Loss: 0.00091174
Iteration 6/25 | Loss: 0.00091011
Iteration 7/25 | Loss: 0.00091008
Iteration 8/25 | Loss: 0.00091008
Iteration 9/25 | Loss: 0.00091008
Iteration 10/25 | Loss: 0.00091008
Iteration 11/25 | Loss: 0.00091008
Iteration 12/25 | Loss: 0.00091008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000910082075279206, 0.000910082075279206, 0.000910082075279206, 0.000910082075279206, 0.000910082075279206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000910082075279206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30747712
Iteration 2/25 | Loss: 0.00044962
Iteration 3/25 | Loss: 0.00044960
Iteration 4/25 | Loss: 0.00044960
Iteration 5/25 | Loss: 0.00044960
Iteration 6/25 | Loss: 0.00044960
Iteration 7/25 | Loss: 0.00044960
Iteration 8/25 | Loss: 0.00044960
Iteration 9/25 | Loss: 0.00044960
Iteration 10/25 | Loss: 0.00044960
Iteration 11/25 | Loss: 0.00044960
Iteration 12/25 | Loss: 0.00044960
Iteration 13/25 | Loss: 0.00044960
Iteration 14/25 | Loss: 0.00044960
Iteration 15/25 | Loss: 0.00044960
Iteration 16/25 | Loss: 0.00044960
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000449599523562938, 0.000449599523562938, 0.000449599523562938, 0.000449599523562938, 0.000449599523562938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000449599523562938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044960
Iteration 2/1000 | Loss: 0.00003698
Iteration 3/1000 | Loss: 0.00002796
Iteration 4/1000 | Loss: 0.00002576
Iteration 5/1000 | Loss: 0.00002461
Iteration 6/1000 | Loss: 0.00002393
Iteration 7/1000 | Loss: 0.00002343
Iteration 8/1000 | Loss: 0.00002298
Iteration 9/1000 | Loss: 0.00002256
Iteration 10/1000 | Loss: 0.00002231
Iteration 11/1000 | Loss: 0.00002215
Iteration 12/1000 | Loss: 0.00002215
Iteration 13/1000 | Loss: 0.00002212
Iteration 14/1000 | Loss: 0.00002210
Iteration 15/1000 | Loss: 0.00002203
Iteration 16/1000 | Loss: 0.00002203
Iteration 17/1000 | Loss: 0.00002194
Iteration 18/1000 | Loss: 0.00002193
Iteration 19/1000 | Loss: 0.00002193
Iteration 20/1000 | Loss: 0.00002191
Iteration 21/1000 | Loss: 0.00002190
Iteration 22/1000 | Loss: 0.00002186
Iteration 23/1000 | Loss: 0.00002185
Iteration 24/1000 | Loss: 0.00002184
Iteration 25/1000 | Loss: 0.00002184
Iteration 26/1000 | Loss: 0.00002183
Iteration 27/1000 | Loss: 0.00002182
Iteration 28/1000 | Loss: 0.00002182
Iteration 29/1000 | Loss: 0.00002182
Iteration 30/1000 | Loss: 0.00002182
Iteration 31/1000 | Loss: 0.00002182
Iteration 32/1000 | Loss: 0.00002182
Iteration 33/1000 | Loss: 0.00002181
Iteration 34/1000 | Loss: 0.00002180
Iteration 35/1000 | Loss: 0.00002180
Iteration 36/1000 | Loss: 0.00002180
Iteration 37/1000 | Loss: 0.00002179
Iteration 38/1000 | Loss: 0.00002179
Iteration 39/1000 | Loss: 0.00002179
Iteration 40/1000 | Loss: 0.00002179
Iteration 41/1000 | Loss: 0.00002179
Iteration 42/1000 | Loss: 0.00002179
Iteration 43/1000 | Loss: 0.00002179
Iteration 44/1000 | Loss: 0.00002178
Iteration 45/1000 | Loss: 0.00002178
Iteration 46/1000 | Loss: 0.00002178
Iteration 47/1000 | Loss: 0.00002178
Iteration 48/1000 | Loss: 0.00002178
Iteration 49/1000 | Loss: 0.00002178
Iteration 50/1000 | Loss: 0.00002177
Iteration 51/1000 | Loss: 0.00002177
Iteration 52/1000 | Loss: 0.00002177
Iteration 53/1000 | Loss: 0.00002177
Iteration 54/1000 | Loss: 0.00002177
Iteration 55/1000 | Loss: 0.00002177
Iteration 56/1000 | Loss: 0.00002176
Iteration 57/1000 | Loss: 0.00002176
Iteration 58/1000 | Loss: 0.00002176
Iteration 59/1000 | Loss: 0.00002176
Iteration 60/1000 | Loss: 0.00002176
Iteration 61/1000 | Loss: 0.00002176
Iteration 62/1000 | Loss: 0.00002176
Iteration 63/1000 | Loss: 0.00002176
Iteration 64/1000 | Loss: 0.00002176
Iteration 65/1000 | Loss: 0.00002176
Iteration 66/1000 | Loss: 0.00002176
Iteration 67/1000 | Loss: 0.00002175
Iteration 68/1000 | Loss: 0.00002175
Iteration 69/1000 | Loss: 0.00002175
Iteration 70/1000 | Loss: 0.00002175
Iteration 71/1000 | Loss: 0.00002175
Iteration 72/1000 | Loss: 0.00002175
Iteration 73/1000 | Loss: 0.00002175
Iteration 74/1000 | Loss: 0.00002175
Iteration 75/1000 | Loss: 0.00002175
Iteration 76/1000 | Loss: 0.00002175
Iteration 77/1000 | Loss: 0.00002175
Iteration 78/1000 | Loss: 0.00002174
Iteration 79/1000 | Loss: 0.00002174
Iteration 80/1000 | Loss: 0.00002174
Iteration 81/1000 | Loss: 0.00002174
Iteration 82/1000 | Loss: 0.00002174
Iteration 83/1000 | Loss: 0.00002174
Iteration 84/1000 | Loss: 0.00002174
Iteration 85/1000 | Loss: 0.00002174
Iteration 86/1000 | Loss: 0.00002174
Iteration 87/1000 | Loss: 0.00002174
Iteration 88/1000 | Loss: 0.00002173
Iteration 89/1000 | Loss: 0.00002173
Iteration 90/1000 | Loss: 0.00002173
Iteration 91/1000 | Loss: 0.00002173
Iteration 92/1000 | Loss: 0.00002173
Iteration 93/1000 | Loss: 0.00002173
Iteration 94/1000 | Loss: 0.00002173
Iteration 95/1000 | Loss: 0.00002173
Iteration 96/1000 | Loss: 0.00002173
Iteration 97/1000 | Loss: 0.00002173
Iteration 98/1000 | Loss: 0.00002172
Iteration 99/1000 | Loss: 0.00002172
Iteration 100/1000 | Loss: 0.00002172
Iteration 101/1000 | Loss: 0.00002172
Iteration 102/1000 | Loss: 0.00002172
Iteration 103/1000 | Loss: 0.00002172
Iteration 104/1000 | Loss: 0.00002172
Iteration 105/1000 | Loss: 0.00002172
Iteration 106/1000 | Loss: 0.00002172
Iteration 107/1000 | Loss: 0.00002172
Iteration 108/1000 | Loss: 0.00002172
Iteration 109/1000 | Loss: 0.00002172
Iteration 110/1000 | Loss: 0.00002172
Iteration 111/1000 | Loss: 0.00002172
Iteration 112/1000 | Loss: 0.00002172
Iteration 113/1000 | Loss: 0.00002172
Iteration 114/1000 | Loss: 0.00002172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.1719784854212776e-05, 2.1719784854212776e-05, 2.1719784854212776e-05, 2.1719784854212776e-05, 2.1719784854212776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1719784854212776e-05

Optimization complete. Final v2v error: 4.033369541168213 mm

Highest mean error: 4.304450035095215 mm for frame 53

Lowest mean error: 3.842304229736328 mm for frame 0

Saving results

Total time: 33.51061248779297
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848256
Iteration 2/25 | Loss: 0.00098928
Iteration 3/25 | Loss: 0.00082887
Iteration 4/25 | Loss: 0.00079850
Iteration 5/25 | Loss: 0.00078662
Iteration 6/25 | Loss: 0.00078296
Iteration 7/25 | Loss: 0.00078227
Iteration 8/25 | Loss: 0.00078227
Iteration 9/25 | Loss: 0.00078227
Iteration 10/25 | Loss: 0.00078227
Iteration 11/25 | Loss: 0.00078227
Iteration 12/25 | Loss: 0.00078227
Iteration 13/25 | Loss: 0.00078227
Iteration 14/25 | Loss: 0.00078227
Iteration 15/25 | Loss: 0.00078227
Iteration 16/25 | Loss: 0.00078227
Iteration 17/25 | Loss: 0.00078227
Iteration 18/25 | Loss: 0.00078227
Iteration 19/25 | Loss: 0.00078227
Iteration 20/25 | Loss: 0.00078227
Iteration 21/25 | Loss: 0.00078227
Iteration 22/25 | Loss: 0.00078227
Iteration 23/25 | Loss: 0.00078227
Iteration 24/25 | Loss: 0.00078227
Iteration 25/25 | Loss: 0.00078227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30021918
Iteration 2/25 | Loss: 0.00042587
Iteration 3/25 | Loss: 0.00042587
Iteration 4/25 | Loss: 0.00042587
Iteration 5/25 | Loss: 0.00042587
Iteration 6/25 | Loss: 0.00042587
Iteration 7/25 | Loss: 0.00042587
Iteration 8/25 | Loss: 0.00042587
Iteration 9/25 | Loss: 0.00042587
Iteration 10/25 | Loss: 0.00042587
Iteration 11/25 | Loss: 0.00042587
Iteration 12/25 | Loss: 0.00042587
Iteration 13/25 | Loss: 0.00042587
Iteration 14/25 | Loss: 0.00042587
Iteration 15/25 | Loss: 0.00042587
Iteration 16/25 | Loss: 0.00042587
Iteration 17/25 | Loss: 0.00042587
Iteration 18/25 | Loss: 0.00042587
Iteration 19/25 | Loss: 0.00042587
Iteration 20/25 | Loss: 0.00042587
Iteration 21/25 | Loss: 0.00042587
Iteration 22/25 | Loss: 0.00042587
Iteration 23/25 | Loss: 0.00042587
Iteration 24/25 | Loss: 0.00042587
Iteration 25/25 | Loss: 0.00042587

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042587
Iteration 2/1000 | Loss: 0.00002428
Iteration 3/1000 | Loss: 0.00001488
Iteration 4/1000 | Loss: 0.00001373
Iteration 5/1000 | Loss: 0.00001325
Iteration 6/1000 | Loss: 0.00001289
Iteration 7/1000 | Loss: 0.00001264
Iteration 8/1000 | Loss: 0.00001255
Iteration 9/1000 | Loss: 0.00001245
Iteration 10/1000 | Loss: 0.00001243
Iteration 11/1000 | Loss: 0.00001240
Iteration 12/1000 | Loss: 0.00001238
Iteration 13/1000 | Loss: 0.00001231
Iteration 14/1000 | Loss: 0.00001228
Iteration 15/1000 | Loss: 0.00001228
Iteration 16/1000 | Loss: 0.00001227
Iteration 17/1000 | Loss: 0.00001227
Iteration 18/1000 | Loss: 0.00001226
Iteration 19/1000 | Loss: 0.00001226
Iteration 20/1000 | Loss: 0.00001226
Iteration 21/1000 | Loss: 0.00001225
Iteration 22/1000 | Loss: 0.00001225
Iteration 23/1000 | Loss: 0.00001225
Iteration 24/1000 | Loss: 0.00001225
Iteration 25/1000 | Loss: 0.00001225
Iteration 26/1000 | Loss: 0.00001225
Iteration 27/1000 | Loss: 0.00001225
Iteration 28/1000 | Loss: 0.00001224
Iteration 29/1000 | Loss: 0.00001224
Iteration 30/1000 | Loss: 0.00001224
Iteration 31/1000 | Loss: 0.00001224
Iteration 32/1000 | Loss: 0.00001224
Iteration 33/1000 | Loss: 0.00001224
Iteration 34/1000 | Loss: 0.00001224
Iteration 35/1000 | Loss: 0.00001224
Iteration 36/1000 | Loss: 0.00001223
Iteration 37/1000 | Loss: 0.00001223
Iteration 38/1000 | Loss: 0.00001223
Iteration 39/1000 | Loss: 0.00001223
Iteration 40/1000 | Loss: 0.00001223
Iteration 41/1000 | Loss: 0.00001223
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001222
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001222
Iteration 46/1000 | Loss: 0.00001221
Iteration 47/1000 | Loss: 0.00001221
Iteration 48/1000 | Loss: 0.00001221
Iteration 49/1000 | Loss: 0.00001221
Iteration 50/1000 | Loss: 0.00001221
Iteration 51/1000 | Loss: 0.00001221
Iteration 52/1000 | Loss: 0.00001220
Iteration 53/1000 | Loss: 0.00001220
Iteration 54/1000 | Loss: 0.00001220
Iteration 55/1000 | Loss: 0.00001220
Iteration 56/1000 | Loss: 0.00001220
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001220
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001220
Iteration 65/1000 | Loss: 0.00001220
Iteration 66/1000 | Loss: 0.00001220
Iteration 67/1000 | Loss: 0.00001219
Iteration 68/1000 | Loss: 0.00001219
Iteration 69/1000 | Loss: 0.00001219
Iteration 70/1000 | Loss: 0.00001219
Iteration 71/1000 | Loss: 0.00001219
Iteration 72/1000 | Loss: 0.00001219
Iteration 73/1000 | Loss: 0.00001219
Iteration 74/1000 | Loss: 0.00001219
Iteration 75/1000 | Loss: 0.00001219
Iteration 76/1000 | Loss: 0.00001218
Iteration 77/1000 | Loss: 0.00001218
Iteration 78/1000 | Loss: 0.00001218
Iteration 79/1000 | Loss: 0.00001218
Iteration 80/1000 | Loss: 0.00001218
Iteration 81/1000 | Loss: 0.00001218
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001218
Iteration 85/1000 | Loss: 0.00001218
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001218
Iteration 89/1000 | Loss: 0.00001218
Iteration 90/1000 | Loss: 0.00001218
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001217
Iteration 94/1000 | Loss: 0.00001217
Iteration 95/1000 | Loss: 0.00001217
Iteration 96/1000 | Loss: 0.00001217
Iteration 97/1000 | Loss: 0.00001217
Iteration 98/1000 | Loss: 0.00001217
Iteration 99/1000 | Loss: 0.00001217
Iteration 100/1000 | Loss: 0.00001217
Iteration 101/1000 | Loss: 0.00001217
Iteration 102/1000 | Loss: 0.00001217
Iteration 103/1000 | Loss: 0.00001217
Iteration 104/1000 | Loss: 0.00001216
Iteration 105/1000 | Loss: 0.00001216
Iteration 106/1000 | Loss: 0.00001216
Iteration 107/1000 | Loss: 0.00001216
Iteration 108/1000 | Loss: 0.00001216
Iteration 109/1000 | Loss: 0.00001216
Iteration 110/1000 | Loss: 0.00001216
Iteration 111/1000 | Loss: 0.00001216
Iteration 112/1000 | Loss: 0.00001216
Iteration 113/1000 | Loss: 0.00001216
Iteration 114/1000 | Loss: 0.00001216
Iteration 115/1000 | Loss: 0.00001216
Iteration 116/1000 | Loss: 0.00001216
Iteration 117/1000 | Loss: 0.00001216
Iteration 118/1000 | Loss: 0.00001216
Iteration 119/1000 | Loss: 0.00001215
Iteration 120/1000 | Loss: 0.00001215
Iteration 121/1000 | Loss: 0.00001215
Iteration 122/1000 | Loss: 0.00001215
Iteration 123/1000 | Loss: 0.00001215
Iteration 124/1000 | Loss: 0.00001215
Iteration 125/1000 | Loss: 0.00001215
Iteration 126/1000 | Loss: 0.00001215
Iteration 127/1000 | Loss: 0.00001215
Iteration 128/1000 | Loss: 0.00001215
Iteration 129/1000 | Loss: 0.00001214
Iteration 130/1000 | Loss: 0.00001214
Iteration 131/1000 | Loss: 0.00001214
Iteration 132/1000 | Loss: 0.00001214
Iteration 133/1000 | Loss: 0.00001214
Iteration 134/1000 | Loss: 0.00001214
Iteration 135/1000 | Loss: 0.00001214
Iteration 136/1000 | Loss: 0.00001214
Iteration 137/1000 | Loss: 0.00001214
Iteration 138/1000 | Loss: 0.00001214
Iteration 139/1000 | Loss: 0.00001214
Iteration 140/1000 | Loss: 0.00001214
Iteration 141/1000 | Loss: 0.00001214
Iteration 142/1000 | Loss: 0.00001214
Iteration 143/1000 | Loss: 0.00001213
Iteration 144/1000 | Loss: 0.00001213
Iteration 145/1000 | Loss: 0.00001213
Iteration 146/1000 | Loss: 0.00001213
Iteration 147/1000 | Loss: 0.00001213
Iteration 148/1000 | Loss: 0.00001213
Iteration 149/1000 | Loss: 0.00001213
Iteration 150/1000 | Loss: 0.00001213
Iteration 151/1000 | Loss: 0.00001213
Iteration 152/1000 | Loss: 0.00001213
Iteration 153/1000 | Loss: 0.00001213
Iteration 154/1000 | Loss: 0.00001213
Iteration 155/1000 | Loss: 0.00001213
Iteration 156/1000 | Loss: 0.00001213
Iteration 157/1000 | Loss: 0.00001213
Iteration 158/1000 | Loss: 0.00001212
Iteration 159/1000 | Loss: 0.00001212
Iteration 160/1000 | Loss: 0.00001212
Iteration 161/1000 | Loss: 0.00001212
Iteration 162/1000 | Loss: 0.00001212
Iteration 163/1000 | Loss: 0.00001212
Iteration 164/1000 | Loss: 0.00001212
Iteration 165/1000 | Loss: 0.00001212
Iteration 166/1000 | Loss: 0.00001212
Iteration 167/1000 | Loss: 0.00001212
Iteration 168/1000 | Loss: 0.00001212
Iteration 169/1000 | Loss: 0.00001212
Iteration 170/1000 | Loss: 0.00001212
Iteration 171/1000 | Loss: 0.00001211
Iteration 172/1000 | Loss: 0.00001211
Iteration 173/1000 | Loss: 0.00001211
Iteration 174/1000 | Loss: 0.00001211
Iteration 175/1000 | Loss: 0.00001211
Iteration 176/1000 | Loss: 0.00001211
Iteration 177/1000 | Loss: 0.00001211
Iteration 178/1000 | Loss: 0.00001211
Iteration 179/1000 | Loss: 0.00001211
Iteration 180/1000 | Loss: 0.00001211
Iteration 181/1000 | Loss: 0.00001211
Iteration 182/1000 | Loss: 0.00001211
Iteration 183/1000 | Loss: 0.00001211
Iteration 184/1000 | Loss: 0.00001211
Iteration 185/1000 | Loss: 0.00001211
Iteration 186/1000 | Loss: 0.00001211
Iteration 187/1000 | Loss: 0.00001211
Iteration 188/1000 | Loss: 0.00001211
Iteration 189/1000 | Loss: 0.00001211
Iteration 190/1000 | Loss: 0.00001211
Iteration 191/1000 | Loss: 0.00001211
Iteration 192/1000 | Loss: 0.00001211
Iteration 193/1000 | Loss: 0.00001211
Iteration 194/1000 | Loss: 0.00001211
Iteration 195/1000 | Loss: 0.00001211
Iteration 196/1000 | Loss: 0.00001211
Iteration 197/1000 | Loss: 0.00001211
Iteration 198/1000 | Loss: 0.00001211
Iteration 199/1000 | Loss: 0.00001211
Iteration 200/1000 | Loss: 0.00001211
Iteration 201/1000 | Loss: 0.00001211
Iteration 202/1000 | Loss: 0.00001211
Iteration 203/1000 | Loss: 0.00001211
Iteration 204/1000 | Loss: 0.00001211
Iteration 205/1000 | Loss: 0.00001211
Iteration 206/1000 | Loss: 0.00001211
Iteration 207/1000 | Loss: 0.00001211
Iteration 208/1000 | Loss: 0.00001211
Iteration 209/1000 | Loss: 0.00001211
Iteration 210/1000 | Loss: 0.00001211
Iteration 211/1000 | Loss: 0.00001211
Iteration 212/1000 | Loss: 0.00001211
Iteration 213/1000 | Loss: 0.00001211
Iteration 214/1000 | Loss: 0.00001211
Iteration 215/1000 | Loss: 0.00001211
Iteration 216/1000 | Loss: 0.00001211
Iteration 217/1000 | Loss: 0.00001211
Iteration 218/1000 | Loss: 0.00001211
Iteration 219/1000 | Loss: 0.00001211
Iteration 220/1000 | Loss: 0.00001211
Iteration 221/1000 | Loss: 0.00001211
Iteration 222/1000 | Loss: 0.00001211
Iteration 223/1000 | Loss: 0.00001211
Iteration 224/1000 | Loss: 0.00001211
Iteration 225/1000 | Loss: 0.00001211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.2110653187846765e-05, 1.2110653187846765e-05, 1.2110653187846765e-05, 1.2110653187846765e-05, 1.2110653187846765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2110653187846765e-05

Optimization complete. Final v2v error: 2.90937876701355 mm

Highest mean error: 3.274275302886963 mm for frame 118

Lowest mean error: 2.480475902557373 mm for frame 49

Saving results

Total time: 37.653061389923096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509527
Iteration 2/25 | Loss: 0.00111914
Iteration 3/25 | Loss: 0.00089192
Iteration 4/25 | Loss: 0.00086246
Iteration 5/25 | Loss: 0.00085616
Iteration 6/25 | Loss: 0.00085503
Iteration 7/25 | Loss: 0.00085503
Iteration 8/25 | Loss: 0.00085503
Iteration 9/25 | Loss: 0.00085503
Iteration 10/25 | Loss: 0.00085503
Iteration 11/25 | Loss: 0.00085503
Iteration 12/25 | Loss: 0.00085503
Iteration 13/25 | Loss: 0.00085503
Iteration 14/25 | Loss: 0.00085503
Iteration 15/25 | Loss: 0.00085503
Iteration 16/25 | Loss: 0.00085503
Iteration 17/25 | Loss: 0.00085503
Iteration 18/25 | Loss: 0.00085503
Iteration 19/25 | Loss: 0.00085503
Iteration 20/25 | Loss: 0.00085503
Iteration 21/25 | Loss: 0.00085503
Iteration 22/25 | Loss: 0.00085503
Iteration 23/25 | Loss: 0.00085503
Iteration 24/25 | Loss: 0.00085503
Iteration 25/25 | Loss: 0.00085503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77874249
Iteration 2/25 | Loss: 0.00045531
Iteration 3/25 | Loss: 0.00045530
Iteration 4/25 | Loss: 0.00045530
Iteration 5/25 | Loss: 0.00045530
Iteration 6/25 | Loss: 0.00045530
Iteration 7/25 | Loss: 0.00045530
Iteration 8/25 | Loss: 0.00045530
Iteration 9/25 | Loss: 0.00045530
Iteration 10/25 | Loss: 0.00045530
Iteration 11/25 | Loss: 0.00045530
Iteration 12/25 | Loss: 0.00045530
Iteration 13/25 | Loss: 0.00045530
Iteration 14/25 | Loss: 0.00045530
Iteration 15/25 | Loss: 0.00045530
Iteration 16/25 | Loss: 0.00045530
Iteration 17/25 | Loss: 0.00045530
Iteration 18/25 | Loss: 0.00045530
Iteration 19/25 | Loss: 0.00045530
Iteration 20/25 | Loss: 0.00045530
Iteration 21/25 | Loss: 0.00045530
Iteration 22/25 | Loss: 0.00045530
Iteration 23/25 | Loss: 0.00045530
Iteration 24/25 | Loss: 0.00045530
Iteration 25/25 | Loss: 0.00045530

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045530
Iteration 2/1000 | Loss: 0.00002860
Iteration 3/1000 | Loss: 0.00002363
Iteration 4/1000 | Loss: 0.00002235
Iteration 5/1000 | Loss: 0.00002168
Iteration 6/1000 | Loss: 0.00002109
Iteration 7/1000 | Loss: 0.00002064
Iteration 8/1000 | Loss: 0.00002043
Iteration 9/1000 | Loss: 0.00002030
Iteration 10/1000 | Loss: 0.00002029
Iteration 11/1000 | Loss: 0.00002028
Iteration 12/1000 | Loss: 0.00002028
Iteration 13/1000 | Loss: 0.00002028
Iteration 14/1000 | Loss: 0.00002028
Iteration 15/1000 | Loss: 0.00002027
Iteration 16/1000 | Loss: 0.00002023
Iteration 17/1000 | Loss: 0.00002023
Iteration 18/1000 | Loss: 0.00002023
Iteration 19/1000 | Loss: 0.00002022
Iteration 20/1000 | Loss: 0.00002022
Iteration 21/1000 | Loss: 0.00002021
Iteration 22/1000 | Loss: 0.00002021
Iteration 23/1000 | Loss: 0.00002021
Iteration 24/1000 | Loss: 0.00002021
Iteration 25/1000 | Loss: 0.00002020
Iteration 26/1000 | Loss: 0.00002020
Iteration 27/1000 | Loss: 0.00002020
Iteration 28/1000 | Loss: 0.00002020
Iteration 29/1000 | Loss: 0.00002020
Iteration 30/1000 | Loss: 0.00002020
Iteration 31/1000 | Loss: 0.00002020
Iteration 32/1000 | Loss: 0.00002020
Iteration 33/1000 | Loss: 0.00002020
Iteration 34/1000 | Loss: 0.00002020
Iteration 35/1000 | Loss: 0.00002019
Iteration 36/1000 | Loss: 0.00002019
Iteration 37/1000 | Loss: 0.00002019
Iteration 38/1000 | Loss: 0.00002018
Iteration 39/1000 | Loss: 0.00002018
Iteration 40/1000 | Loss: 0.00002017
Iteration 41/1000 | Loss: 0.00002017
Iteration 42/1000 | Loss: 0.00002016
Iteration 43/1000 | Loss: 0.00002016
Iteration 44/1000 | Loss: 0.00002015
Iteration 45/1000 | Loss: 0.00002015
Iteration 46/1000 | Loss: 0.00002015
Iteration 47/1000 | Loss: 0.00002015
Iteration 48/1000 | Loss: 0.00002015
Iteration 49/1000 | Loss: 0.00002015
Iteration 50/1000 | Loss: 0.00002015
Iteration 51/1000 | Loss: 0.00002015
Iteration 52/1000 | Loss: 0.00002015
Iteration 53/1000 | Loss: 0.00002015
Iteration 54/1000 | Loss: 0.00002015
Iteration 55/1000 | Loss: 0.00002015
Iteration 56/1000 | Loss: 0.00002014
Iteration 57/1000 | Loss: 0.00002014
Iteration 58/1000 | Loss: 0.00002014
Iteration 59/1000 | Loss: 0.00002014
Iteration 60/1000 | Loss: 0.00002013
Iteration 61/1000 | Loss: 0.00002013
Iteration 62/1000 | Loss: 0.00002013
Iteration 63/1000 | Loss: 0.00002013
Iteration 64/1000 | Loss: 0.00002013
Iteration 65/1000 | Loss: 0.00002013
Iteration 66/1000 | Loss: 0.00002013
Iteration 67/1000 | Loss: 0.00002013
Iteration 68/1000 | Loss: 0.00002013
Iteration 69/1000 | Loss: 0.00002012
Iteration 70/1000 | Loss: 0.00002012
Iteration 71/1000 | Loss: 0.00002012
Iteration 72/1000 | Loss: 0.00002012
Iteration 73/1000 | Loss: 0.00002012
Iteration 74/1000 | Loss: 0.00002012
Iteration 75/1000 | Loss: 0.00002012
Iteration 76/1000 | Loss: 0.00002011
Iteration 77/1000 | Loss: 0.00002011
Iteration 78/1000 | Loss: 0.00002011
Iteration 79/1000 | Loss: 0.00002011
Iteration 80/1000 | Loss: 0.00002011
Iteration 81/1000 | Loss: 0.00002010
Iteration 82/1000 | Loss: 0.00002010
Iteration 83/1000 | Loss: 0.00002010
Iteration 84/1000 | Loss: 0.00002010
Iteration 85/1000 | Loss: 0.00002010
Iteration 86/1000 | Loss: 0.00002010
Iteration 87/1000 | Loss: 0.00002010
Iteration 88/1000 | Loss: 0.00002010
Iteration 89/1000 | Loss: 0.00002010
Iteration 90/1000 | Loss: 0.00002010
Iteration 91/1000 | Loss: 0.00002010
Iteration 92/1000 | Loss: 0.00002010
Iteration 93/1000 | Loss: 0.00002010
Iteration 94/1000 | Loss: 0.00002010
Iteration 95/1000 | Loss: 0.00002010
Iteration 96/1000 | Loss: 0.00002010
Iteration 97/1000 | Loss: 0.00002010
Iteration 98/1000 | Loss: 0.00002010
Iteration 99/1000 | Loss: 0.00002010
Iteration 100/1000 | Loss: 0.00002010
Iteration 101/1000 | Loss: 0.00002010
Iteration 102/1000 | Loss: 0.00002010
Iteration 103/1000 | Loss: 0.00002010
Iteration 104/1000 | Loss: 0.00002010
Iteration 105/1000 | Loss: 0.00002010
Iteration 106/1000 | Loss: 0.00002010
Iteration 107/1000 | Loss: 0.00002010
Iteration 108/1000 | Loss: 0.00002010
Iteration 109/1000 | Loss: 0.00002010
Iteration 110/1000 | Loss: 0.00002010
Iteration 111/1000 | Loss: 0.00002010
Iteration 112/1000 | Loss: 0.00002010
Iteration 113/1000 | Loss: 0.00002010
Iteration 114/1000 | Loss: 0.00002010
Iteration 115/1000 | Loss: 0.00002010
Iteration 116/1000 | Loss: 0.00002010
Iteration 117/1000 | Loss: 0.00002010
Iteration 118/1000 | Loss: 0.00002010
Iteration 119/1000 | Loss: 0.00002010
Iteration 120/1000 | Loss: 0.00002010
Iteration 121/1000 | Loss: 0.00002010
Iteration 122/1000 | Loss: 0.00002010
Iteration 123/1000 | Loss: 0.00002010
Iteration 124/1000 | Loss: 0.00002010
Iteration 125/1000 | Loss: 0.00002010
Iteration 126/1000 | Loss: 0.00002010
Iteration 127/1000 | Loss: 0.00002010
Iteration 128/1000 | Loss: 0.00002010
Iteration 129/1000 | Loss: 0.00002010
Iteration 130/1000 | Loss: 0.00002010
Iteration 131/1000 | Loss: 0.00002010
Iteration 132/1000 | Loss: 0.00002010
Iteration 133/1000 | Loss: 0.00002010
Iteration 134/1000 | Loss: 0.00002010
Iteration 135/1000 | Loss: 0.00002010
Iteration 136/1000 | Loss: 0.00002010
Iteration 137/1000 | Loss: 0.00002010
Iteration 138/1000 | Loss: 0.00002010
Iteration 139/1000 | Loss: 0.00002010
Iteration 140/1000 | Loss: 0.00002010
Iteration 141/1000 | Loss: 0.00002010
Iteration 142/1000 | Loss: 0.00002010
Iteration 143/1000 | Loss: 0.00002010
Iteration 144/1000 | Loss: 0.00002010
Iteration 145/1000 | Loss: 0.00002010
Iteration 146/1000 | Loss: 0.00002010
Iteration 147/1000 | Loss: 0.00002010
Iteration 148/1000 | Loss: 0.00002010
Iteration 149/1000 | Loss: 0.00002010
Iteration 150/1000 | Loss: 0.00002010
Iteration 151/1000 | Loss: 0.00002010
Iteration 152/1000 | Loss: 0.00002010
Iteration 153/1000 | Loss: 0.00002010
Iteration 154/1000 | Loss: 0.00002010
Iteration 155/1000 | Loss: 0.00002010
Iteration 156/1000 | Loss: 0.00002010
Iteration 157/1000 | Loss: 0.00002010
Iteration 158/1000 | Loss: 0.00002010
Iteration 159/1000 | Loss: 0.00002010
Iteration 160/1000 | Loss: 0.00002010
Iteration 161/1000 | Loss: 0.00002010
Iteration 162/1000 | Loss: 0.00002010
Iteration 163/1000 | Loss: 0.00002010
Iteration 164/1000 | Loss: 0.00002010
Iteration 165/1000 | Loss: 0.00002010
Iteration 166/1000 | Loss: 0.00002010
Iteration 167/1000 | Loss: 0.00002010
Iteration 168/1000 | Loss: 0.00002010
Iteration 169/1000 | Loss: 0.00002010
Iteration 170/1000 | Loss: 0.00002010
Iteration 171/1000 | Loss: 0.00002010
Iteration 172/1000 | Loss: 0.00002010
Iteration 173/1000 | Loss: 0.00002010
Iteration 174/1000 | Loss: 0.00002010
Iteration 175/1000 | Loss: 0.00002010
Iteration 176/1000 | Loss: 0.00002010
Iteration 177/1000 | Loss: 0.00002010
Iteration 178/1000 | Loss: 0.00002010
Iteration 179/1000 | Loss: 0.00002010
Iteration 180/1000 | Loss: 0.00002010
Iteration 181/1000 | Loss: 0.00002010
Iteration 182/1000 | Loss: 0.00002010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.0098801542189904e-05, 2.0098801542189904e-05, 2.0098801542189904e-05, 2.0098801542189904e-05, 2.0098801542189904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0098801542189904e-05

Optimization complete. Final v2v error: 3.876152276992798 mm

Highest mean error: 4.152263641357422 mm for frame 87

Lowest mean error: 3.6220109462738037 mm for frame 0

Saving results

Total time: 35.74614906311035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00359799
Iteration 2/25 | Loss: 0.00107039
Iteration 3/25 | Loss: 0.00088183
Iteration 4/25 | Loss: 0.00084263
Iteration 5/25 | Loss: 0.00083496
Iteration 6/25 | Loss: 0.00083352
Iteration 7/25 | Loss: 0.00083352
Iteration 8/25 | Loss: 0.00083352
Iteration 9/25 | Loss: 0.00083352
Iteration 10/25 | Loss: 0.00083352
Iteration 11/25 | Loss: 0.00083352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008335194434039295, 0.0008335194434039295, 0.0008335194434039295, 0.0008335194434039295, 0.0008335194434039295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008335194434039295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29894292
Iteration 2/25 | Loss: 0.00051540
Iteration 3/25 | Loss: 0.00051540
Iteration 4/25 | Loss: 0.00051540
Iteration 5/25 | Loss: 0.00051540
Iteration 6/25 | Loss: 0.00051540
Iteration 7/25 | Loss: 0.00051540
Iteration 8/25 | Loss: 0.00051540
Iteration 9/25 | Loss: 0.00051540
Iteration 10/25 | Loss: 0.00051540
Iteration 11/25 | Loss: 0.00051540
Iteration 12/25 | Loss: 0.00051540
Iteration 13/25 | Loss: 0.00051540
Iteration 14/25 | Loss: 0.00051540
Iteration 15/25 | Loss: 0.00051540
Iteration 16/25 | Loss: 0.00051540
Iteration 17/25 | Loss: 0.00051540
Iteration 18/25 | Loss: 0.00051540
Iteration 19/25 | Loss: 0.00051540
Iteration 20/25 | Loss: 0.00051540
Iteration 21/25 | Loss: 0.00051540
Iteration 22/25 | Loss: 0.00051540
Iteration 23/25 | Loss: 0.00051540
Iteration 24/25 | Loss: 0.00051540
Iteration 25/25 | Loss: 0.00051540

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051540
Iteration 2/1000 | Loss: 0.00003248
Iteration 3/1000 | Loss: 0.00002289
Iteration 4/1000 | Loss: 0.00001984
Iteration 5/1000 | Loss: 0.00001882
Iteration 6/1000 | Loss: 0.00001808
Iteration 7/1000 | Loss: 0.00001745
Iteration 8/1000 | Loss: 0.00001699
Iteration 9/1000 | Loss: 0.00001663
Iteration 10/1000 | Loss: 0.00001641
Iteration 11/1000 | Loss: 0.00001631
Iteration 12/1000 | Loss: 0.00001628
Iteration 13/1000 | Loss: 0.00001627
Iteration 14/1000 | Loss: 0.00001627
Iteration 15/1000 | Loss: 0.00001626
Iteration 16/1000 | Loss: 0.00001624
Iteration 17/1000 | Loss: 0.00001623
Iteration 18/1000 | Loss: 0.00001622
Iteration 19/1000 | Loss: 0.00001618
Iteration 20/1000 | Loss: 0.00001616
Iteration 21/1000 | Loss: 0.00001615
Iteration 22/1000 | Loss: 0.00001614
Iteration 23/1000 | Loss: 0.00001611
Iteration 24/1000 | Loss: 0.00001610
Iteration 25/1000 | Loss: 0.00001609
Iteration 26/1000 | Loss: 0.00001608
Iteration 27/1000 | Loss: 0.00001606
Iteration 28/1000 | Loss: 0.00001605
Iteration 29/1000 | Loss: 0.00001605
Iteration 30/1000 | Loss: 0.00001604
Iteration 31/1000 | Loss: 0.00001604
Iteration 32/1000 | Loss: 0.00001603
Iteration 33/1000 | Loss: 0.00001603
Iteration 34/1000 | Loss: 0.00001603
Iteration 35/1000 | Loss: 0.00001602
Iteration 36/1000 | Loss: 0.00001602
Iteration 37/1000 | Loss: 0.00001602
Iteration 38/1000 | Loss: 0.00001602
Iteration 39/1000 | Loss: 0.00001601
Iteration 40/1000 | Loss: 0.00001601
Iteration 41/1000 | Loss: 0.00001601
Iteration 42/1000 | Loss: 0.00001601
Iteration 43/1000 | Loss: 0.00001601
Iteration 44/1000 | Loss: 0.00001600
Iteration 45/1000 | Loss: 0.00001600
Iteration 46/1000 | Loss: 0.00001600
Iteration 47/1000 | Loss: 0.00001600
Iteration 48/1000 | Loss: 0.00001600
Iteration 49/1000 | Loss: 0.00001600
Iteration 50/1000 | Loss: 0.00001600
Iteration 51/1000 | Loss: 0.00001599
Iteration 52/1000 | Loss: 0.00001599
Iteration 53/1000 | Loss: 0.00001599
Iteration 54/1000 | Loss: 0.00001599
Iteration 55/1000 | Loss: 0.00001599
Iteration 56/1000 | Loss: 0.00001598
Iteration 57/1000 | Loss: 0.00001598
Iteration 58/1000 | Loss: 0.00001597
Iteration 59/1000 | Loss: 0.00001597
Iteration 60/1000 | Loss: 0.00001597
Iteration 61/1000 | Loss: 0.00001597
Iteration 62/1000 | Loss: 0.00001597
Iteration 63/1000 | Loss: 0.00001597
Iteration 64/1000 | Loss: 0.00001597
Iteration 65/1000 | Loss: 0.00001596
Iteration 66/1000 | Loss: 0.00001596
Iteration 67/1000 | Loss: 0.00001596
Iteration 68/1000 | Loss: 0.00001596
Iteration 69/1000 | Loss: 0.00001596
Iteration 70/1000 | Loss: 0.00001596
Iteration 71/1000 | Loss: 0.00001596
Iteration 72/1000 | Loss: 0.00001596
Iteration 73/1000 | Loss: 0.00001596
Iteration 74/1000 | Loss: 0.00001596
Iteration 75/1000 | Loss: 0.00001595
Iteration 76/1000 | Loss: 0.00001595
Iteration 77/1000 | Loss: 0.00001595
Iteration 78/1000 | Loss: 0.00001595
Iteration 79/1000 | Loss: 0.00001595
Iteration 80/1000 | Loss: 0.00001595
Iteration 81/1000 | Loss: 0.00001595
Iteration 82/1000 | Loss: 0.00001595
Iteration 83/1000 | Loss: 0.00001595
Iteration 84/1000 | Loss: 0.00001595
Iteration 85/1000 | Loss: 0.00001595
Iteration 86/1000 | Loss: 0.00001595
Iteration 87/1000 | Loss: 0.00001595
Iteration 88/1000 | Loss: 0.00001595
Iteration 89/1000 | Loss: 0.00001595
Iteration 90/1000 | Loss: 0.00001595
Iteration 91/1000 | Loss: 0.00001595
Iteration 92/1000 | Loss: 0.00001595
Iteration 93/1000 | Loss: 0.00001595
Iteration 94/1000 | Loss: 0.00001595
Iteration 95/1000 | Loss: 0.00001595
Iteration 96/1000 | Loss: 0.00001595
Iteration 97/1000 | Loss: 0.00001595
Iteration 98/1000 | Loss: 0.00001595
Iteration 99/1000 | Loss: 0.00001595
Iteration 100/1000 | Loss: 0.00001595
Iteration 101/1000 | Loss: 0.00001595
Iteration 102/1000 | Loss: 0.00001595
Iteration 103/1000 | Loss: 0.00001595
Iteration 104/1000 | Loss: 0.00001595
Iteration 105/1000 | Loss: 0.00001595
Iteration 106/1000 | Loss: 0.00001595
Iteration 107/1000 | Loss: 0.00001595
Iteration 108/1000 | Loss: 0.00001595
Iteration 109/1000 | Loss: 0.00001595
Iteration 110/1000 | Loss: 0.00001595
Iteration 111/1000 | Loss: 0.00001595
Iteration 112/1000 | Loss: 0.00001595
Iteration 113/1000 | Loss: 0.00001595
Iteration 114/1000 | Loss: 0.00001595
Iteration 115/1000 | Loss: 0.00001595
Iteration 116/1000 | Loss: 0.00001595
Iteration 117/1000 | Loss: 0.00001595
Iteration 118/1000 | Loss: 0.00001595
Iteration 119/1000 | Loss: 0.00001595
Iteration 120/1000 | Loss: 0.00001595
Iteration 121/1000 | Loss: 0.00001595
Iteration 122/1000 | Loss: 0.00001595
Iteration 123/1000 | Loss: 0.00001595
Iteration 124/1000 | Loss: 0.00001595
Iteration 125/1000 | Loss: 0.00001595
Iteration 126/1000 | Loss: 0.00001595
Iteration 127/1000 | Loss: 0.00001595
Iteration 128/1000 | Loss: 0.00001595
Iteration 129/1000 | Loss: 0.00001595
Iteration 130/1000 | Loss: 0.00001595
Iteration 131/1000 | Loss: 0.00001595
Iteration 132/1000 | Loss: 0.00001595
Iteration 133/1000 | Loss: 0.00001595
Iteration 134/1000 | Loss: 0.00001595
Iteration 135/1000 | Loss: 0.00001595
Iteration 136/1000 | Loss: 0.00001595
Iteration 137/1000 | Loss: 0.00001595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.5945017366902903e-05, 1.5945017366902903e-05, 1.5945017366902903e-05, 1.5945017366902903e-05, 1.5945017366902903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5945017366902903e-05

Optimization complete. Final v2v error: 3.3630499839782715 mm

Highest mean error: 3.8074517250061035 mm for frame 202

Lowest mean error: 3.0094213485717773 mm for frame 113

Saving results

Total time: 36.70589351654053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056626
Iteration 2/25 | Loss: 0.00152137
Iteration 3/25 | Loss: 0.00102334
Iteration 4/25 | Loss: 0.00094637
Iteration 5/25 | Loss: 0.00092356
Iteration 6/25 | Loss: 0.00092038
Iteration 7/25 | Loss: 0.00092036
Iteration 8/25 | Loss: 0.00092036
Iteration 9/25 | Loss: 0.00092036
Iteration 10/25 | Loss: 0.00092036
Iteration 11/25 | Loss: 0.00092036
Iteration 12/25 | Loss: 0.00092036
Iteration 13/25 | Loss: 0.00092036
Iteration 14/25 | Loss: 0.00092036
Iteration 15/25 | Loss: 0.00092036
Iteration 16/25 | Loss: 0.00092036
Iteration 17/25 | Loss: 0.00092036
Iteration 18/25 | Loss: 0.00092036
Iteration 19/25 | Loss: 0.00092036
Iteration 20/25 | Loss: 0.00092036
Iteration 21/25 | Loss: 0.00092036
Iteration 22/25 | Loss: 0.00092036
Iteration 23/25 | Loss: 0.00092036
Iteration 24/25 | Loss: 0.00092036
Iteration 25/25 | Loss: 0.00092036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94035101
Iteration 2/25 | Loss: 0.00038980
Iteration 3/25 | Loss: 0.00038980
Iteration 4/25 | Loss: 0.00038980
Iteration 5/25 | Loss: 0.00038980
Iteration 6/25 | Loss: 0.00038980
Iteration 7/25 | Loss: 0.00038980
Iteration 8/25 | Loss: 0.00038980
Iteration 9/25 | Loss: 0.00038980
Iteration 10/25 | Loss: 0.00038980
Iteration 11/25 | Loss: 0.00038980
Iteration 12/25 | Loss: 0.00038980
Iteration 13/25 | Loss: 0.00038980
Iteration 14/25 | Loss: 0.00038980
Iteration 15/25 | Loss: 0.00038980
Iteration 16/25 | Loss: 0.00038980
Iteration 17/25 | Loss: 0.00038980
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00038980148383416235, 0.00038980148383416235, 0.00038980148383416235, 0.00038980148383416235, 0.00038980148383416235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038980148383416235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038980
Iteration 2/1000 | Loss: 0.00003109
Iteration 3/1000 | Loss: 0.00002379
Iteration 4/1000 | Loss: 0.00002184
Iteration 5/1000 | Loss: 0.00002104
Iteration 6/1000 | Loss: 0.00002044
Iteration 7/1000 | Loss: 0.00002003
Iteration 8/1000 | Loss: 0.00001970
Iteration 9/1000 | Loss: 0.00001944
Iteration 10/1000 | Loss: 0.00001938
Iteration 11/1000 | Loss: 0.00001933
Iteration 12/1000 | Loss: 0.00001927
Iteration 13/1000 | Loss: 0.00001926
Iteration 14/1000 | Loss: 0.00001926
Iteration 15/1000 | Loss: 0.00001925
Iteration 16/1000 | Loss: 0.00001924
Iteration 17/1000 | Loss: 0.00001922
Iteration 18/1000 | Loss: 0.00001921
Iteration 19/1000 | Loss: 0.00001919
Iteration 20/1000 | Loss: 0.00001918
Iteration 21/1000 | Loss: 0.00001918
Iteration 22/1000 | Loss: 0.00001918
Iteration 23/1000 | Loss: 0.00001917
Iteration 24/1000 | Loss: 0.00001913
Iteration 25/1000 | Loss: 0.00001912
Iteration 26/1000 | Loss: 0.00001912
Iteration 27/1000 | Loss: 0.00001911
Iteration 28/1000 | Loss: 0.00001910
Iteration 29/1000 | Loss: 0.00001910
Iteration 30/1000 | Loss: 0.00001909
Iteration 31/1000 | Loss: 0.00001909
Iteration 32/1000 | Loss: 0.00001908
Iteration 33/1000 | Loss: 0.00001908
Iteration 34/1000 | Loss: 0.00001908
Iteration 35/1000 | Loss: 0.00001907
Iteration 36/1000 | Loss: 0.00001906
Iteration 37/1000 | Loss: 0.00001906
Iteration 38/1000 | Loss: 0.00001905
Iteration 39/1000 | Loss: 0.00001905
Iteration 40/1000 | Loss: 0.00001904
Iteration 41/1000 | Loss: 0.00001904
Iteration 42/1000 | Loss: 0.00001904
Iteration 43/1000 | Loss: 0.00001904
Iteration 44/1000 | Loss: 0.00001904
Iteration 45/1000 | Loss: 0.00001903
Iteration 46/1000 | Loss: 0.00001903
Iteration 47/1000 | Loss: 0.00001903
Iteration 48/1000 | Loss: 0.00001903
Iteration 49/1000 | Loss: 0.00001903
Iteration 50/1000 | Loss: 0.00001903
Iteration 51/1000 | Loss: 0.00001902
Iteration 52/1000 | Loss: 0.00001902
Iteration 53/1000 | Loss: 0.00001902
Iteration 54/1000 | Loss: 0.00001902
Iteration 55/1000 | Loss: 0.00001902
Iteration 56/1000 | Loss: 0.00001902
Iteration 57/1000 | Loss: 0.00001902
Iteration 58/1000 | Loss: 0.00001902
Iteration 59/1000 | Loss: 0.00001901
Iteration 60/1000 | Loss: 0.00001901
Iteration 61/1000 | Loss: 0.00001901
Iteration 62/1000 | Loss: 0.00001901
Iteration 63/1000 | Loss: 0.00001901
Iteration 64/1000 | Loss: 0.00001900
Iteration 65/1000 | Loss: 0.00001900
Iteration 66/1000 | Loss: 0.00001900
Iteration 67/1000 | Loss: 0.00001899
Iteration 68/1000 | Loss: 0.00001899
Iteration 69/1000 | Loss: 0.00001899
Iteration 70/1000 | Loss: 0.00001899
Iteration 71/1000 | Loss: 0.00001899
Iteration 72/1000 | Loss: 0.00001899
Iteration 73/1000 | Loss: 0.00001899
Iteration 74/1000 | Loss: 0.00001899
Iteration 75/1000 | Loss: 0.00001899
Iteration 76/1000 | Loss: 0.00001899
Iteration 77/1000 | Loss: 0.00001899
Iteration 78/1000 | Loss: 0.00001898
Iteration 79/1000 | Loss: 0.00001898
Iteration 80/1000 | Loss: 0.00001898
Iteration 81/1000 | Loss: 0.00001898
Iteration 82/1000 | Loss: 0.00001898
Iteration 83/1000 | Loss: 0.00001898
Iteration 84/1000 | Loss: 0.00001898
Iteration 85/1000 | Loss: 0.00001898
Iteration 86/1000 | Loss: 0.00001898
Iteration 87/1000 | Loss: 0.00001898
Iteration 88/1000 | Loss: 0.00001898
Iteration 89/1000 | Loss: 0.00001898
Iteration 90/1000 | Loss: 0.00001898
Iteration 91/1000 | Loss: 0.00001898
Iteration 92/1000 | Loss: 0.00001898
Iteration 93/1000 | Loss: 0.00001898
Iteration 94/1000 | Loss: 0.00001898
Iteration 95/1000 | Loss: 0.00001897
Iteration 96/1000 | Loss: 0.00001897
Iteration 97/1000 | Loss: 0.00001897
Iteration 98/1000 | Loss: 0.00001897
Iteration 99/1000 | Loss: 0.00001897
Iteration 100/1000 | Loss: 0.00001897
Iteration 101/1000 | Loss: 0.00001897
Iteration 102/1000 | Loss: 0.00001897
Iteration 103/1000 | Loss: 0.00001897
Iteration 104/1000 | Loss: 0.00001897
Iteration 105/1000 | Loss: 0.00001897
Iteration 106/1000 | Loss: 0.00001897
Iteration 107/1000 | Loss: 0.00001897
Iteration 108/1000 | Loss: 0.00001897
Iteration 109/1000 | Loss: 0.00001897
Iteration 110/1000 | Loss: 0.00001897
Iteration 111/1000 | Loss: 0.00001897
Iteration 112/1000 | Loss: 0.00001897
Iteration 113/1000 | Loss: 0.00001896
Iteration 114/1000 | Loss: 0.00001896
Iteration 115/1000 | Loss: 0.00001896
Iteration 116/1000 | Loss: 0.00001896
Iteration 117/1000 | Loss: 0.00001896
Iteration 118/1000 | Loss: 0.00001896
Iteration 119/1000 | Loss: 0.00001896
Iteration 120/1000 | Loss: 0.00001896
Iteration 121/1000 | Loss: 0.00001896
Iteration 122/1000 | Loss: 0.00001896
Iteration 123/1000 | Loss: 0.00001896
Iteration 124/1000 | Loss: 0.00001896
Iteration 125/1000 | Loss: 0.00001896
Iteration 126/1000 | Loss: 0.00001895
Iteration 127/1000 | Loss: 0.00001895
Iteration 128/1000 | Loss: 0.00001895
Iteration 129/1000 | Loss: 0.00001895
Iteration 130/1000 | Loss: 0.00001895
Iteration 131/1000 | Loss: 0.00001895
Iteration 132/1000 | Loss: 0.00001895
Iteration 133/1000 | Loss: 0.00001895
Iteration 134/1000 | Loss: 0.00001895
Iteration 135/1000 | Loss: 0.00001895
Iteration 136/1000 | Loss: 0.00001895
Iteration 137/1000 | Loss: 0.00001895
Iteration 138/1000 | Loss: 0.00001895
Iteration 139/1000 | Loss: 0.00001895
Iteration 140/1000 | Loss: 0.00001895
Iteration 141/1000 | Loss: 0.00001895
Iteration 142/1000 | Loss: 0.00001895
Iteration 143/1000 | Loss: 0.00001895
Iteration 144/1000 | Loss: 0.00001895
Iteration 145/1000 | Loss: 0.00001895
Iteration 146/1000 | Loss: 0.00001895
Iteration 147/1000 | Loss: 0.00001895
Iteration 148/1000 | Loss: 0.00001895
Iteration 149/1000 | Loss: 0.00001895
Iteration 150/1000 | Loss: 0.00001895
Iteration 151/1000 | Loss: 0.00001895
Iteration 152/1000 | Loss: 0.00001895
Iteration 153/1000 | Loss: 0.00001895
Iteration 154/1000 | Loss: 0.00001895
Iteration 155/1000 | Loss: 0.00001895
Iteration 156/1000 | Loss: 0.00001895
Iteration 157/1000 | Loss: 0.00001895
Iteration 158/1000 | Loss: 0.00001895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.8954946426674724e-05, 1.8954946426674724e-05, 1.8954946426674724e-05, 1.8954946426674724e-05, 1.8954946426674724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8954946426674724e-05

Optimization complete. Final v2v error: 3.6686530113220215 mm

Highest mean error: 4.072397708892822 mm for frame 156

Lowest mean error: 3.4982125759124756 mm for frame 79

Saving results

Total time: 32.430747985839844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01111657
Iteration 2/25 | Loss: 0.00261905
Iteration 3/25 | Loss: 0.00165374
Iteration 4/25 | Loss: 0.00147866
Iteration 5/25 | Loss: 0.00138208
Iteration 6/25 | Loss: 0.00137206
Iteration 7/25 | Loss: 0.00132871
Iteration 8/25 | Loss: 0.00117666
Iteration 9/25 | Loss: 0.00106069
Iteration 10/25 | Loss: 0.00103777
Iteration 11/25 | Loss: 0.00098337
Iteration 12/25 | Loss: 0.00096022
Iteration 13/25 | Loss: 0.00093604
Iteration 14/25 | Loss: 0.00094763
Iteration 15/25 | Loss: 0.00092897
Iteration 16/25 | Loss: 0.00092560
Iteration 17/25 | Loss: 0.00091432
Iteration 18/25 | Loss: 0.00091746
Iteration 19/25 | Loss: 0.00090760
Iteration 20/25 | Loss: 0.00090620
Iteration 21/25 | Loss: 0.00090124
Iteration 22/25 | Loss: 0.00089469
Iteration 23/25 | Loss: 0.00089179
Iteration 24/25 | Loss: 0.00088353
Iteration 25/25 | Loss: 0.00088328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38239813
Iteration 2/25 | Loss: 0.00124839
Iteration 3/25 | Loss: 0.00084794
Iteration 4/25 | Loss: 0.00084794
Iteration 5/25 | Loss: 0.00084794
Iteration 6/25 | Loss: 0.00084794
Iteration 7/25 | Loss: 0.00084794
Iteration 8/25 | Loss: 0.00084794
Iteration 9/25 | Loss: 0.00084794
Iteration 10/25 | Loss: 0.00084794
Iteration 11/25 | Loss: 0.00084794
Iteration 12/25 | Loss: 0.00084794
Iteration 13/25 | Loss: 0.00084794
Iteration 14/25 | Loss: 0.00084794
Iteration 15/25 | Loss: 0.00084794
Iteration 16/25 | Loss: 0.00084794
Iteration 17/25 | Loss: 0.00084794
Iteration 18/25 | Loss: 0.00084794
Iteration 19/25 | Loss: 0.00084794
Iteration 20/25 | Loss: 0.00084794
Iteration 21/25 | Loss: 0.00084794
Iteration 22/25 | Loss: 0.00084794
Iteration 23/25 | Loss: 0.00084794
Iteration 24/25 | Loss: 0.00084794
Iteration 25/25 | Loss: 0.00084794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084794
Iteration 2/1000 | Loss: 0.00110083
Iteration 3/1000 | Loss: 0.00117176
Iteration 4/1000 | Loss: 0.00083381
Iteration 5/1000 | Loss: 0.00121014
Iteration 6/1000 | Loss: 0.00074975
Iteration 7/1000 | Loss: 0.00049146
Iteration 8/1000 | Loss: 0.00052120
Iteration 9/1000 | Loss: 0.00039379
Iteration 10/1000 | Loss: 0.00046870
Iteration 11/1000 | Loss: 0.00028821
Iteration 12/1000 | Loss: 0.00027734
Iteration 13/1000 | Loss: 0.00047459
Iteration 14/1000 | Loss: 0.00029126
Iteration 15/1000 | Loss: 0.00079013
Iteration 16/1000 | Loss: 0.00063851
Iteration 17/1000 | Loss: 0.00048432
Iteration 18/1000 | Loss: 0.00039840
Iteration 19/1000 | Loss: 0.00013305
Iteration 20/1000 | Loss: 0.00010291
Iteration 21/1000 | Loss: 0.00072905
Iteration 22/1000 | Loss: 0.00037616
Iteration 23/1000 | Loss: 0.00088980
Iteration 24/1000 | Loss: 0.00061138
Iteration 25/1000 | Loss: 0.00099644
Iteration 26/1000 | Loss: 0.00045186
Iteration 27/1000 | Loss: 0.00036172
Iteration 28/1000 | Loss: 0.00045052
Iteration 29/1000 | Loss: 0.00044425
Iteration 30/1000 | Loss: 0.00032504
Iteration 31/1000 | Loss: 0.00032600
Iteration 32/1000 | Loss: 0.00030594
Iteration 33/1000 | Loss: 0.00032044
Iteration 34/1000 | Loss: 0.00035876
Iteration 35/1000 | Loss: 0.00025008
Iteration 36/1000 | Loss: 0.00005311
Iteration 37/1000 | Loss: 0.00026955
Iteration 38/1000 | Loss: 0.00033821
Iteration 39/1000 | Loss: 0.00031245
Iteration 40/1000 | Loss: 0.00018860
Iteration 41/1000 | Loss: 0.00025551
Iteration 42/1000 | Loss: 0.00019403
Iteration 43/1000 | Loss: 0.00036178
Iteration 44/1000 | Loss: 0.00012042
Iteration 45/1000 | Loss: 0.00022582
Iteration 46/1000 | Loss: 0.00014522
Iteration 47/1000 | Loss: 0.00027786
Iteration 48/1000 | Loss: 0.00023877
Iteration 49/1000 | Loss: 0.00015608
Iteration 50/1000 | Loss: 0.00023445
Iteration 51/1000 | Loss: 0.00030718
Iteration 52/1000 | Loss: 0.00033774
Iteration 53/1000 | Loss: 0.00026212
Iteration 54/1000 | Loss: 0.00024974
Iteration 55/1000 | Loss: 0.00025810
Iteration 56/1000 | Loss: 0.00028539
Iteration 57/1000 | Loss: 0.00009212
Iteration 58/1000 | Loss: 0.00015650
Iteration 59/1000 | Loss: 0.00029217
Iteration 60/1000 | Loss: 0.00019212
Iteration 61/1000 | Loss: 0.00021561
Iteration 62/1000 | Loss: 0.00015604
Iteration 63/1000 | Loss: 0.00014528
Iteration 64/1000 | Loss: 0.00013771
Iteration 65/1000 | Loss: 0.00014346
Iteration 66/1000 | Loss: 0.00003957
Iteration 67/1000 | Loss: 0.00020942
Iteration 68/1000 | Loss: 0.00023452
Iteration 69/1000 | Loss: 0.00038466
Iteration 70/1000 | Loss: 0.00005832
Iteration 71/1000 | Loss: 0.00040915
Iteration 72/1000 | Loss: 0.00047747
Iteration 73/1000 | Loss: 0.00117719
Iteration 74/1000 | Loss: 0.00029780
Iteration 75/1000 | Loss: 0.00032403
Iteration 76/1000 | Loss: 0.00062475
Iteration 77/1000 | Loss: 0.00017903
Iteration 78/1000 | Loss: 0.00004759
Iteration 79/1000 | Loss: 0.00005266
Iteration 80/1000 | Loss: 0.00008773
Iteration 81/1000 | Loss: 0.00003539
Iteration 82/1000 | Loss: 0.00003183
Iteration 83/1000 | Loss: 0.00004738
Iteration 84/1000 | Loss: 0.00002990
Iteration 85/1000 | Loss: 0.00002774
Iteration 86/1000 | Loss: 0.00002608
Iteration 87/1000 | Loss: 0.00002475
Iteration 88/1000 | Loss: 0.00002417
Iteration 89/1000 | Loss: 0.00002363
Iteration 90/1000 | Loss: 0.00002313
Iteration 91/1000 | Loss: 0.00002279
Iteration 92/1000 | Loss: 0.00002272
Iteration 93/1000 | Loss: 0.00002270
Iteration 94/1000 | Loss: 0.00002266
Iteration 95/1000 | Loss: 0.00002265
Iteration 96/1000 | Loss: 0.00002263
Iteration 97/1000 | Loss: 0.00002263
Iteration 98/1000 | Loss: 0.00002262
Iteration 99/1000 | Loss: 0.00002262
Iteration 100/1000 | Loss: 0.00002262
Iteration 101/1000 | Loss: 0.00002261
Iteration 102/1000 | Loss: 0.00002261
Iteration 103/1000 | Loss: 0.00002261
Iteration 104/1000 | Loss: 0.00002261
Iteration 105/1000 | Loss: 0.00002260
Iteration 106/1000 | Loss: 0.00002260
Iteration 107/1000 | Loss: 0.00002260
Iteration 108/1000 | Loss: 0.00002260
Iteration 109/1000 | Loss: 0.00002260
Iteration 110/1000 | Loss: 0.00002259
Iteration 111/1000 | Loss: 0.00002259
Iteration 112/1000 | Loss: 0.00002258
Iteration 113/1000 | Loss: 0.00002258
Iteration 114/1000 | Loss: 0.00002258
Iteration 115/1000 | Loss: 0.00002257
Iteration 116/1000 | Loss: 0.00002257
Iteration 117/1000 | Loss: 0.00002257
Iteration 118/1000 | Loss: 0.00002257
Iteration 119/1000 | Loss: 0.00002257
Iteration 120/1000 | Loss: 0.00002255
Iteration 121/1000 | Loss: 0.00002255
Iteration 122/1000 | Loss: 0.00002255
Iteration 123/1000 | Loss: 0.00002254
Iteration 124/1000 | Loss: 0.00002254
Iteration 125/1000 | Loss: 0.00002254
Iteration 126/1000 | Loss: 0.00002253
Iteration 127/1000 | Loss: 0.00002253
Iteration 128/1000 | Loss: 0.00002252
Iteration 129/1000 | Loss: 0.00002252
Iteration 130/1000 | Loss: 0.00002252
Iteration 131/1000 | Loss: 0.00002252
Iteration 132/1000 | Loss: 0.00002251
Iteration 133/1000 | Loss: 0.00002251
Iteration 134/1000 | Loss: 0.00002251
Iteration 135/1000 | Loss: 0.00002251
Iteration 136/1000 | Loss: 0.00002251
Iteration 137/1000 | Loss: 0.00002251
Iteration 138/1000 | Loss: 0.00002251
Iteration 139/1000 | Loss: 0.00002251
Iteration 140/1000 | Loss: 0.00002251
Iteration 141/1000 | Loss: 0.00002251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.2509711925522424e-05, 2.2509711925522424e-05, 2.2509711925522424e-05, 2.2509711925522424e-05, 2.2509711925522424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2509711925522424e-05

Optimization complete. Final v2v error: 3.974180221557617 mm

Highest mean error: 9.122884750366211 mm for frame 1

Lowest mean error: 3.5345327854156494 mm for frame 9

Saving results

Total time: 182.95945954322815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807190
Iteration 2/25 | Loss: 0.00108313
Iteration 3/25 | Loss: 0.00095139
Iteration 4/25 | Loss: 0.00090824
Iteration 5/25 | Loss: 0.00088964
Iteration 6/25 | Loss: 0.00088605
Iteration 7/25 | Loss: 0.00088459
Iteration 8/25 | Loss: 0.00088439
Iteration 9/25 | Loss: 0.00088439
Iteration 10/25 | Loss: 0.00088439
Iteration 11/25 | Loss: 0.00088439
Iteration 12/25 | Loss: 0.00088439
Iteration 13/25 | Loss: 0.00088439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008843903779052198, 0.0008843903779052198, 0.0008843903779052198, 0.0008843903779052198, 0.0008843903779052198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008843903779052198

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31844521
Iteration 2/25 | Loss: 0.00038127
Iteration 3/25 | Loss: 0.00038127
Iteration 4/25 | Loss: 0.00038127
Iteration 5/25 | Loss: 0.00038127
Iteration 6/25 | Loss: 0.00038126
Iteration 7/25 | Loss: 0.00038126
Iteration 8/25 | Loss: 0.00038126
Iteration 9/25 | Loss: 0.00038126
Iteration 10/25 | Loss: 0.00038126
Iteration 11/25 | Loss: 0.00038126
Iteration 12/25 | Loss: 0.00038126
Iteration 13/25 | Loss: 0.00038126
Iteration 14/25 | Loss: 0.00038126
Iteration 15/25 | Loss: 0.00038126
Iteration 16/25 | Loss: 0.00038126
Iteration 17/25 | Loss: 0.00038126
Iteration 18/25 | Loss: 0.00038126
Iteration 19/25 | Loss: 0.00038126
Iteration 20/25 | Loss: 0.00038126
Iteration 21/25 | Loss: 0.00038126
Iteration 22/25 | Loss: 0.00038126
Iteration 23/25 | Loss: 0.00038126
Iteration 24/25 | Loss: 0.00038126
Iteration 25/25 | Loss: 0.00038126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038126
Iteration 2/1000 | Loss: 0.00004646
Iteration 3/1000 | Loss: 0.00002682
Iteration 4/1000 | Loss: 0.00002443
Iteration 5/1000 | Loss: 0.00002313
Iteration 6/1000 | Loss: 0.00002227
Iteration 7/1000 | Loss: 0.00002181
Iteration 8/1000 | Loss: 0.00002137
Iteration 9/1000 | Loss: 0.00002101
Iteration 10/1000 | Loss: 0.00002096
Iteration 11/1000 | Loss: 0.00002087
Iteration 12/1000 | Loss: 0.00002069
Iteration 13/1000 | Loss: 0.00002066
Iteration 14/1000 | Loss: 0.00002064
Iteration 15/1000 | Loss: 0.00002062
Iteration 16/1000 | Loss: 0.00002061
Iteration 17/1000 | Loss: 0.00002060
Iteration 18/1000 | Loss: 0.00002059
Iteration 19/1000 | Loss: 0.00002057
Iteration 20/1000 | Loss: 0.00002056
Iteration 21/1000 | Loss: 0.00002055
Iteration 22/1000 | Loss: 0.00002055
Iteration 23/1000 | Loss: 0.00002054
Iteration 24/1000 | Loss: 0.00002053
Iteration 25/1000 | Loss: 0.00002052
Iteration 26/1000 | Loss: 0.00002052
Iteration 27/1000 | Loss: 0.00002052
Iteration 28/1000 | Loss: 0.00002052
Iteration 29/1000 | Loss: 0.00002051
Iteration 30/1000 | Loss: 0.00002051
Iteration 31/1000 | Loss: 0.00002051
Iteration 32/1000 | Loss: 0.00002051
Iteration 33/1000 | Loss: 0.00002050
Iteration 34/1000 | Loss: 0.00002050
Iteration 35/1000 | Loss: 0.00002050
Iteration 36/1000 | Loss: 0.00002050
Iteration 37/1000 | Loss: 0.00002050
Iteration 38/1000 | Loss: 0.00002049
Iteration 39/1000 | Loss: 0.00002049
Iteration 40/1000 | Loss: 0.00002049
Iteration 41/1000 | Loss: 0.00002049
Iteration 42/1000 | Loss: 0.00002049
Iteration 43/1000 | Loss: 0.00002048
Iteration 44/1000 | Loss: 0.00002048
Iteration 45/1000 | Loss: 0.00002047
Iteration 46/1000 | Loss: 0.00002047
Iteration 47/1000 | Loss: 0.00002047
Iteration 48/1000 | Loss: 0.00002047
Iteration 49/1000 | Loss: 0.00002047
Iteration 50/1000 | Loss: 0.00002047
Iteration 51/1000 | Loss: 0.00002047
Iteration 52/1000 | Loss: 0.00002046
Iteration 53/1000 | Loss: 0.00002046
Iteration 54/1000 | Loss: 0.00002046
Iteration 55/1000 | Loss: 0.00002046
Iteration 56/1000 | Loss: 0.00002046
Iteration 57/1000 | Loss: 0.00002046
Iteration 58/1000 | Loss: 0.00002046
Iteration 59/1000 | Loss: 0.00002045
Iteration 60/1000 | Loss: 0.00002045
Iteration 61/1000 | Loss: 0.00002045
Iteration 62/1000 | Loss: 0.00002045
Iteration 63/1000 | Loss: 0.00002045
Iteration 64/1000 | Loss: 0.00002045
Iteration 65/1000 | Loss: 0.00002044
Iteration 66/1000 | Loss: 0.00002044
Iteration 67/1000 | Loss: 0.00002044
Iteration 68/1000 | Loss: 0.00002044
Iteration 69/1000 | Loss: 0.00002044
Iteration 70/1000 | Loss: 0.00002044
Iteration 71/1000 | Loss: 0.00002044
Iteration 72/1000 | Loss: 0.00002044
Iteration 73/1000 | Loss: 0.00002044
Iteration 74/1000 | Loss: 0.00002044
Iteration 75/1000 | Loss: 0.00002044
Iteration 76/1000 | Loss: 0.00002043
Iteration 77/1000 | Loss: 0.00002043
Iteration 78/1000 | Loss: 0.00002043
Iteration 79/1000 | Loss: 0.00002043
Iteration 80/1000 | Loss: 0.00002043
Iteration 81/1000 | Loss: 0.00002043
Iteration 82/1000 | Loss: 0.00002043
Iteration 83/1000 | Loss: 0.00002043
Iteration 84/1000 | Loss: 0.00002043
Iteration 85/1000 | Loss: 0.00002043
Iteration 86/1000 | Loss: 0.00002042
Iteration 87/1000 | Loss: 0.00002042
Iteration 88/1000 | Loss: 0.00002042
Iteration 89/1000 | Loss: 0.00002042
Iteration 90/1000 | Loss: 0.00002041
Iteration 91/1000 | Loss: 0.00002041
Iteration 92/1000 | Loss: 0.00002041
Iteration 93/1000 | Loss: 0.00002041
Iteration 94/1000 | Loss: 0.00002041
Iteration 95/1000 | Loss: 0.00002041
Iteration 96/1000 | Loss: 0.00002041
Iteration 97/1000 | Loss: 0.00002041
Iteration 98/1000 | Loss: 0.00002041
Iteration 99/1000 | Loss: 0.00002041
Iteration 100/1000 | Loss: 0.00002041
Iteration 101/1000 | Loss: 0.00002041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.0406356270541437e-05, 2.0406356270541437e-05, 2.0406356270541437e-05, 2.0406356270541437e-05, 2.0406356270541437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0406356270541437e-05

Optimization complete. Final v2v error: 3.7863588333129883 mm

Highest mean error: 4.612864017486572 mm for frame 13

Lowest mean error: 2.889820098876953 mm for frame 110

Saving results

Total time: 38.486536264419556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057000
Iteration 2/25 | Loss: 0.00165529
Iteration 3/25 | Loss: 0.00114469
Iteration 4/25 | Loss: 0.00110430
Iteration 5/25 | Loss: 0.00108792
Iteration 6/25 | Loss: 0.00108243
Iteration 7/25 | Loss: 0.00108142
Iteration 8/25 | Loss: 0.00108142
Iteration 9/25 | Loss: 0.00108142
Iteration 10/25 | Loss: 0.00108142
Iteration 11/25 | Loss: 0.00108142
Iteration 12/25 | Loss: 0.00108142
Iteration 13/25 | Loss: 0.00108142
Iteration 14/25 | Loss: 0.00108142
Iteration 15/25 | Loss: 0.00108142
Iteration 16/25 | Loss: 0.00108142
Iteration 17/25 | Loss: 0.00108142
Iteration 18/25 | Loss: 0.00108142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001081422669813037, 0.001081422669813037, 0.001081422669813037, 0.001081422669813037, 0.001081422669813037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001081422669813037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95029515
Iteration 2/25 | Loss: 0.00057949
Iteration 3/25 | Loss: 0.00057948
Iteration 4/25 | Loss: 0.00057948
Iteration 5/25 | Loss: 0.00057948
Iteration 6/25 | Loss: 0.00057948
Iteration 7/25 | Loss: 0.00057948
Iteration 8/25 | Loss: 0.00057948
Iteration 9/25 | Loss: 0.00057948
Iteration 10/25 | Loss: 0.00057948
Iteration 11/25 | Loss: 0.00057948
Iteration 12/25 | Loss: 0.00057948
Iteration 13/25 | Loss: 0.00057948
Iteration 14/25 | Loss: 0.00057948
Iteration 15/25 | Loss: 0.00057948
Iteration 16/25 | Loss: 0.00057948
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005794778699055314, 0.0005794778699055314, 0.0005794778699055314, 0.0005794778699055314, 0.0005794778699055314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005794778699055314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057948
Iteration 2/1000 | Loss: 0.00007305
Iteration 3/1000 | Loss: 0.00005674
Iteration 4/1000 | Loss: 0.00005334
Iteration 5/1000 | Loss: 0.00005068
Iteration 6/1000 | Loss: 0.00004880
Iteration 7/1000 | Loss: 0.00004746
Iteration 8/1000 | Loss: 0.00004652
Iteration 9/1000 | Loss: 0.00004588
Iteration 10/1000 | Loss: 0.00004535
Iteration 11/1000 | Loss: 0.00004507
Iteration 12/1000 | Loss: 0.00004484
Iteration 13/1000 | Loss: 0.00004472
Iteration 14/1000 | Loss: 0.00004466
Iteration 15/1000 | Loss: 0.00004466
Iteration 16/1000 | Loss: 0.00004463
Iteration 17/1000 | Loss: 0.00004462
Iteration 18/1000 | Loss: 0.00004462
Iteration 19/1000 | Loss: 0.00004458
Iteration 20/1000 | Loss: 0.00004455
Iteration 21/1000 | Loss: 0.00004454
Iteration 22/1000 | Loss: 0.00004454
Iteration 23/1000 | Loss: 0.00004454
Iteration 24/1000 | Loss: 0.00004453
Iteration 25/1000 | Loss: 0.00004453
Iteration 26/1000 | Loss: 0.00004452
Iteration 27/1000 | Loss: 0.00004449
Iteration 28/1000 | Loss: 0.00004448
Iteration 29/1000 | Loss: 0.00004448
Iteration 30/1000 | Loss: 0.00004446
Iteration 31/1000 | Loss: 0.00004446
Iteration 32/1000 | Loss: 0.00004446
Iteration 33/1000 | Loss: 0.00004446
Iteration 34/1000 | Loss: 0.00004446
Iteration 35/1000 | Loss: 0.00004446
Iteration 36/1000 | Loss: 0.00004446
Iteration 37/1000 | Loss: 0.00004446
Iteration 38/1000 | Loss: 0.00004446
Iteration 39/1000 | Loss: 0.00004445
Iteration 40/1000 | Loss: 0.00004445
Iteration 41/1000 | Loss: 0.00004445
Iteration 42/1000 | Loss: 0.00004445
Iteration 43/1000 | Loss: 0.00004445
Iteration 44/1000 | Loss: 0.00004445
Iteration 45/1000 | Loss: 0.00004445
Iteration 46/1000 | Loss: 0.00004444
Iteration 47/1000 | Loss: 0.00004443
Iteration 48/1000 | Loss: 0.00004443
Iteration 49/1000 | Loss: 0.00004443
Iteration 50/1000 | Loss: 0.00004443
Iteration 51/1000 | Loss: 0.00004442
Iteration 52/1000 | Loss: 0.00004442
Iteration 53/1000 | Loss: 0.00004441
Iteration 54/1000 | Loss: 0.00004441
Iteration 55/1000 | Loss: 0.00004440
Iteration 56/1000 | Loss: 0.00004440
Iteration 57/1000 | Loss: 0.00004440
Iteration 58/1000 | Loss: 0.00004440
Iteration 59/1000 | Loss: 0.00004440
Iteration 60/1000 | Loss: 0.00004439
Iteration 61/1000 | Loss: 0.00004439
Iteration 62/1000 | Loss: 0.00004439
Iteration 63/1000 | Loss: 0.00004439
Iteration 64/1000 | Loss: 0.00004439
Iteration 65/1000 | Loss: 0.00004438
Iteration 66/1000 | Loss: 0.00004438
Iteration 67/1000 | Loss: 0.00004438
Iteration 68/1000 | Loss: 0.00004438
Iteration 69/1000 | Loss: 0.00004438
Iteration 70/1000 | Loss: 0.00004438
Iteration 71/1000 | Loss: 0.00004438
Iteration 72/1000 | Loss: 0.00004438
Iteration 73/1000 | Loss: 0.00004438
Iteration 74/1000 | Loss: 0.00004438
Iteration 75/1000 | Loss: 0.00004437
Iteration 76/1000 | Loss: 0.00004437
Iteration 77/1000 | Loss: 0.00004437
Iteration 78/1000 | Loss: 0.00004437
Iteration 79/1000 | Loss: 0.00004436
Iteration 80/1000 | Loss: 0.00004436
Iteration 81/1000 | Loss: 0.00004436
Iteration 82/1000 | Loss: 0.00004436
Iteration 83/1000 | Loss: 0.00004436
Iteration 84/1000 | Loss: 0.00004435
Iteration 85/1000 | Loss: 0.00004435
Iteration 86/1000 | Loss: 0.00004435
Iteration 87/1000 | Loss: 0.00004435
Iteration 88/1000 | Loss: 0.00004434
Iteration 89/1000 | Loss: 0.00004434
Iteration 90/1000 | Loss: 0.00004434
Iteration 91/1000 | Loss: 0.00004434
Iteration 92/1000 | Loss: 0.00004434
Iteration 93/1000 | Loss: 0.00004434
Iteration 94/1000 | Loss: 0.00004434
Iteration 95/1000 | Loss: 0.00004433
Iteration 96/1000 | Loss: 0.00004433
Iteration 97/1000 | Loss: 0.00004433
Iteration 98/1000 | Loss: 0.00004433
Iteration 99/1000 | Loss: 0.00004433
Iteration 100/1000 | Loss: 0.00004433
Iteration 101/1000 | Loss: 0.00004432
Iteration 102/1000 | Loss: 0.00004432
Iteration 103/1000 | Loss: 0.00004432
Iteration 104/1000 | Loss: 0.00004432
Iteration 105/1000 | Loss: 0.00004431
Iteration 106/1000 | Loss: 0.00004431
Iteration 107/1000 | Loss: 0.00004431
Iteration 108/1000 | Loss: 0.00004431
Iteration 109/1000 | Loss: 0.00004431
Iteration 110/1000 | Loss: 0.00004431
Iteration 111/1000 | Loss: 0.00004431
Iteration 112/1000 | Loss: 0.00004431
Iteration 113/1000 | Loss: 0.00004430
Iteration 114/1000 | Loss: 0.00004430
Iteration 115/1000 | Loss: 0.00004430
Iteration 116/1000 | Loss: 0.00004430
Iteration 117/1000 | Loss: 0.00004430
Iteration 118/1000 | Loss: 0.00004430
Iteration 119/1000 | Loss: 0.00004430
Iteration 120/1000 | Loss: 0.00004430
Iteration 121/1000 | Loss: 0.00004430
Iteration 122/1000 | Loss: 0.00004430
Iteration 123/1000 | Loss: 0.00004430
Iteration 124/1000 | Loss: 0.00004430
Iteration 125/1000 | Loss: 0.00004430
Iteration 126/1000 | Loss: 0.00004430
Iteration 127/1000 | Loss: 0.00004430
Iteration 128/1000 | Loss: 0.00004430
Iteration 129/1000 | Loss: 0.00004430
Iteration 130/1000 | Loss: 0.00004429
Iteration 131/1000 | Loss: 0.00004429
Iteration 132/1000 | Loss: 0.00004429
Iteration 133/1000 | Loss: 0.00004429
Iteration 134/1000 | Loss: 0.00004429
Iteration 135/1000 | Loss: 0.00004429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [4.429484033607878e-05, 4.429484033607878e-05, 4.429484033607878e-05, 4.429484033607878e-05, 4.429484033607878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.429484033607878e-05

Optimization complete. Final v2v error: 5.442252159118652 mm

Highest mean error: 6.4396891593933105 mm for frame 102

Lowest mean error: 4.333751201629639 mm for frame 56

Saving results

Total time: 45.57173275947571
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00577669
Iteration 2/25 | Loss: 0.00108852
Iteration 3/25 | Loss: 0.00091873
Iteration 4/25 | Loss: 0.00088427
Iteration 5/25 | Loss: 0.00087641
Iteration 6/25 | Loss: 0.00087491
Iteration 7/25 | Loss: 0.00087491
Iteration 8/25 | Loss: 0.00087491
Iteration 9/25 | Loss: 0.00087491
Iteration 10/25 | Loss: 0.00087491
Iteration 11/25 | Loss: 0.00087491
Iteration 12/25 | Loss: 0.00087491
Iteration 13/25 | Loss: 0.00087491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008749073022045195, 0.0008749073022045195, 0.0008749073022045195, 0.0008749073022045195, 0.0008749073022045195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008749073022045195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 14.23317146
Iteration 2/25 | Loss: 0.00034922
Iteration 3/25 | Loss: 0.00034919
Iteration 4/25 | Loss: 0.00034919
Iteration 5/25 | Loss: 0.00034919
Iteration 6/25 | Loss: 0.00034919
Iteration 7/25 | Loss: 0.00034919
Iteration 8/25 | Loss: 0.00034919
Iteration 9/25 | Loss: 0.00034919
Iteration 10/25 | Loss: 0.00034919
Iteration 11/25 | Loss: 0.00034919
Iteration 12/25 | Loss: 0.00034919
Iteration 13/25 | Loss: 0.00034919
Iteration 14/25 | Loss: 0.00034919
Iteration 15/25 | Loss: 0.00034919
Iteration 16/25 | Loss: 0.00034919
Iteration 17/25 | Loss: 0.00034919
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003491909883450717, 0.0003491909883450717, 0.0003491909883450717, 0.0003491909883450717, 0.0003491909883450717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003491909883450717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034919
Iteration 2/1000 | Loss: 0.00003508
Iteration 3/1000 | Loss: 0.00002417
Iteration 4/1000 | Loss: 0.00002301
Iteration 5/1000 | Loss: 0.00002243
Iteration 6/1000 | Loss: 0.00002196
Iteration 7/1000 | Loss: 0.00002187
Iteration 8/1000 | Loss: 0.00002165
Iteration 9/1000 | Loss: 0.00002142
Iteration 10/1000 | Loss: 0.00002134
Iteration 11/1000 | Loss: 0.00002131
Iteration 12/1000 | Loss: 0.00002130
Iteration 13/1000 | Loss: 0.00002130
Iteration 14/1000 | Loss: 0.00002128
Iteration 15/1000 | Loss: 0.00002128
Iteration 16/1000 | Loss: 0.00002128
Iteration 17/1000 | Loss: 0.00002128
Iteration 18/1000 | Loss: 0.00002127
Iteration 19/1000 | Loss: 0.00002127
Iteration 20/1000 | Loss: 0.00002127
Iteration 21/1000 | Loss: 0.00002127
Iteration 22/1000 | Loss: 0.00002127
Iteration 23/1000 | Loss: 0.00002127
Iteration 24/1000 | Loss: 0.00002126
Iteration 25/1000 | Loss: 0.00002125
Iteration 26/1000 | Loss: 0.00002124
Iteration 27/1000 | Loss: 0.00002124
Iteration 28/1000 | Loss: 0.00002124
Iteration 29/1000 | Loss: 0.00002124
Iteration 30/1000 | Loss: 0.00002123
Iteration 31/1000 | Loss: 0.00002123
Iteration 32/1000 | Loss: 0.00002121
Iteration 33/1000 | Loss: 0.00002121
Iteration 34/1000 | Loss: 0.00002120
Iteration 35/1000 | Loss: 0.00002120
Iteration 36/1000 | Loss: 0.00002119
Iteration 37/1000 | Loss: 0.00002119
Iteration 38/1000 | Loss: 0.00002119
Iteration 39/1000 | Loss: 0.00002119
Iteration 40/1000 | Loss: 0.00002118
Iteration 41/1000 | Loss: 0.00002118
Iteration 42/1000 | Loss: 0.00002118
Iteration 43/1000 | Loss: 0.00002117
Iteration 44/1000 | Loss: 0.00002117
Iteration 45/1000 | Loss: 0.00002117
Iteration 46/1000 | Loss: 0.00002116
Iteration 47/1000 | Loss: 0.00002116
Iteration 48/1000 | Loss: 0.00002116
Iteration 49/1000 | Loss: 0.00002115
Iteration 50/1000 | Loss: 0.00002115
Iteration 51/1000 | Loss: 0.00002115
Iteration 52/1000 | Loss: 0.00002115
Iteration 53/1000 | Loss: 0.00002115
Iteration 54/1000 | Loss: 0.00002115
Iteration 55/1000 | Loss: 0.00002115
Iteration 56/1000 | Loss: 0.00002115
Iteration 57/1000 | Loss: 0.00002114
Iteration 58/1000 | Loss: 0.00002114
Iteration 59/1000 | Loss: 0.00002114
Iteration 60/1000 | Loss: 0.00002114
Iteration 61/1000 | Loss: 0.00002114
Iteration 62/1000 | Loss: 0.00002114
Iteration 63/1000 | Loss: 0.00002114
Iteration 64/1000 | Loss: 0.00002114
Iteration 65/1000 | Loss: 0.00002114
Iteration 66/1000 | Loss: 0.00002113
Iteration 67/1000 | Loss: 0.00002113
Iteration 68/1000 | Loss: 0.00002113
Iteration 69/1000 | Loss: 0.00002113
Iteration 70/1000 | Loss: 0.00002113
Iteration 71/1000 | Loss: 0.00002113
Iteration 72/1000 | Loss: 0.00002113
Iteration 73/1000 | Loss: 0.00002113
Iteration 74/1000 | Loss: 0.00002113
Iteration 75/1000 | Loss: 0.00002112
Iteration 76/1000 | Loss: 0.00002112
Iteration 77/1000 | Loss: 0.00002112
Iteration 78/1000 | Loss: 0.00002112
Iteration 79/1000 | Loss: 0.00002112
Iteration 80/1000 | Loss: 0.00002112
Iteration 81/1000 | Loss: 0.00002112
Iteration 82/1000 | Loss: 0.00002112
Iteration 83/1000 | Loss: 0.00002112
Iteration 84/1000 | Loss: 0.00002112
Iteration 85/1000 | Loss: 0.00002112
Iteration 86/1000 | Loss: 0.00002112
Iteration 87/1000 | Loss: 0.00002111
Iteration 88/1000 | Loss: 0.00002111
Iteration 89/1000 | Loss: 0.00002111
Iteration 90/1000 | Loss: 0.00002111
Iteration 91/1000 | Loss: 0.00002111
Iteration 92/1000 | Loss: 0.00002111
Iteration 93/1000 | Loss: 0.00002111
Iteration 94/1000 | Loss: 0.00002111
Iteration 95/1000 | Loss: 0.00002111
Iteration 96/1000 | Loss: 0.00002111
Iteration 97/1000 | Loss: 0.00002111
Iteration 98/1000 | Loss: 0.00002111
Iteration 99/1000 | Loss: 0.00002111
Iteration 100/1000 | Loss: 0.00002111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.1113039110787213e-05, 2.1113039110787213e-05, 2.1113039110787213e-05, 2.1113039110787213e-05, 2.1113039110787213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1113039110787213e-05

Optimization complete. Final v2v error: 3.9164693355560303 mm

Highest mean error: 4.632284641265869 mm for frame 168

Lowest mean error: 3.514749765396118 mm for frame 3

Saving results

Total time: 31.992067098617554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084843
Iteration 2/25 | Loss: 0.00175543
Iteration 3/25 | Loss: 0.00122340
Iteration 4/25 | Loss: 0.00107936
Iteration 5/25 | Loss: 0.00100695
Iteration 6/25 | Loss: 0.00096094
Iteration 7/25 | Loss: 0.00096947
Iteration 8/25 | Loss: 0.00092068
Iteration 9/25 | Loss: 0.00094015
Iteration 10/25 | Loss: 0.00096274
Iteration 11/25 | Loss: 0.00089003
Iteration 12/25 | Loss: 0.00091180
Iteration 13/25 | Loss: 0.00087735
Iteration 14/25 | Loss: 0.00087446
Iteration 15/25 | Loss: 0.00086734
Iteration 16/25 | Loss: 0.00085901
Iteration 17/25 | Loss: 0.00085626
Iteration 18/25 | Loss: 0.00085550
Iteration 19/25 | Loss: 0.00085522
Iteration 20/25 | Loss: 0.00085521
Iteration 21/25 | Loss: 0.00085521
Iteration 22/25 | Loss: 0.00085521
Iteration 23/25 | Loss: 0.00085520
Iteration 24/25 | Loss: 0.00085520
Iteration 25/25 | Loss: 0.00085520

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47541904
Iteration 2/25 | Loss: 0.00064270
Iteration 3/25 | Loss: 0.00064270
Iteration 4/25 | Loss: 0.00064270
Iteration 5/25 | Loss: 0.00064270
Iteration 6/25 | Loss: 0.00064270
Iteration 7/25 | Loss: 0.00064270
Iteration 8/25 | Loss: 0.00064270
Iteration 9/25 | Loss: 0.00064270
Iteration 10/25 | Loss: 0.00064270
Iteration 11/25 | Loss: 0.00064270
Iteration 12/25 | Loss: 0.00064270
Iteration 13/25 | Loss: 0.00064270
Iteration 14/25 | Loss: 0.00064270
Iteration 15/25 | Loss: 0.00064270
Iteration 16/25 | Loss: 0.00064270
Iteration 17/25 | Loss: 0.00064270
Iteration 18/25 | Loss: 0.00064270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006426980835385621, 0.0006426980835385621, 0.0006426980835385621, 0.0006426980835385621, 0.0006426980835385621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006426980835385621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064270
Iteration 2/1000 | Loss: 0.00010636
Iteration 3/1000 | Loss: 0.00006262
Iteration 4/1000 | Loss: 0.00004959
Iteration 5/1000 | Loss: 0.00004168
Iteration 6/1000 | Loss: 0.00058300
Iteration 7/1000 | Loss: 0.00164505
Iteration 8/1000 | Loss: 0.00126143
Iteration 9/1000 | Loss: 0.00217111
Iteration 10/1000 | Loss: 0.00108361
Iteration 11/1000 | Loss: 0.00183079
Iteration 12/1000 | Loss: 0.00072542
Iteration 13/1000 | Loss: 0.00006604
Iteration 14/1000 | Loss: 0.00070351
Iteration 15/1000 | Loss: 0.00100545
Iteration 16/1000 | Loss: 0.00003920
Iteration 17/1000 | Loss: 0.00002901
Iteration 18/1000 | Loss: 0.00002667
Iteration 19/1000 | Loss: 0.00002534
Iteration 20/1000 | Loss: 0.00002438
Iteration 21/1000 | Loss: 0.00002377
Iteration 22/1000 | Loss: 0.00104951
Iteration 23/1000 | Loss: 0.00079379
Iteration 24/1000 | Loss: 0.00007348
Iteration 25/1000 | Loss: 0.00002582
Iteration 26/1000 | Loss: 0.00002157
Iteration 27/1000 | Loss: 0.00001987
Iteration 28/1000 | Loss: 0.00001881
Iteration 29/1000 | Loss: 0.00001825
Iteration 30/1000 | Loss: 0.00001791
Iteration 31/1000 | Loss: 0.00001775
Iteration 32/1000 | Loss: 0.00001757
Iteration 33/1000 | Loss: 0.00001752
Iteration 34/1000 | Loss: 0.00001745
Iteration 35/1000 | Loss: 0.00001744
Iteration 36/1000 | Loss: 0.00001743
Iteration 37/1000 | Loss: 0.00001742
Iteration 38/1000 | Loss: 0.00001741
Iteration 39/1000 | Loss: 0.00001741
Iteration 40/1000 | Loss: 0.00001740
Iteration 41/1000 | Loss: 0.00001740
Iteration 42/1000 | Loss: 0.00001739
Iteration 43/1000 | Loss: 0.00001736
Iteration 44/1000 | Loss: 0.00001731
Iteration 45/1000 | Loss: 0.00001730
Iteration 46/1000 | Loss: 0.00001729
Iteration 47/1000 | Loss: 0.00001724
Iteration 48/1000 | Loss: 0.00001724
Iteration 49/1000 | Loss: 0.00001723
Iteration 50/1000 | Loss: 0.00001723
Iteration 51/1000 | Loss: 0.00001723
Iteration 52/1000 | Loss: 0.00001723
Iteration 53/1000 | Loss: 0.00001723
Iteration 54/1000 | Loss: 0.00001723
Iteration 55/1000 | Loss: 0.00001722
Iteration 56/1000 | Loss: 0.00001722
Iteration 57/1000 | Loss: 0.00001722
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001722
Iteration 60/1000 | Loss: 0.00001722
Iteration 61/1000 | Loss: 0.00001722
Iteration 62/1000 | Loss: 0.00001722
Iteration 63/1000 | Loss: 0.00001722
Iteration 64/1000 | Loss: 0.00001722
Iteration 65/1000 | Loss: 0.00001722
Iteration 66/1000 | Loss: 0.00001721
Iteration 67/1000 | Loss: 0.00001721
Iteration 68/1000 | Loss: 0.00001721
Iteration 69/1000 | Loss: 0.00001721
Iteration 70/1000 | Loss: 0.00001721
Iteration 71/1000 | Loss: 0.00001721
Iteration 72/1000 | Loss: 0.00001721
Iteration 73/1000 | Loss: 0.00001721
Iteration 74/1000 | Loss: 0.00001721
Iteration 75/1000 | Loss: 0.00001721
Iteration 76/1000 | Loss: 0.00001721
Iteration 77/1000 | Loss: 0.00001721
Iteration 78/1000 | Loss: 0.00001721
Iteration 79/1000 | Loss: 0.00001721
Iteration 80/1000 | Loss: 0.00001721
Iteration 81/1000 | Loss: 0.00001720
Iteration 82/1000 | Loss: 0.00001720
Iteration 83/1000 | Loss: 0.00001720
Iteration 84/1000 | Loss: 0.00001720
Iteration 85/1000 | Loss: 0.00001720
Iteration 86/1000 | Loss: 0.00001720
Iteration 87/1000 | Loss: 0.00001720
Iteration 88/1000 | Loss: 0.00001720
Iteration 89/1000 | Loss: 0.00001720
Iteration 90/1000 | Loss: 0.00001720
Iteration 91/1000 | Loss: 0.00001719
Iteration 92/1000 | Loss: 0.00001719
Iteration 93/1000 | Loss: 0.00001719
Iteration 94/1000 | Loss: 0.00001719
Iteration 95/1000 | Loss: 0.00001719
Iteration 96/1000 | Loss: 0.00001719
Iteration 97/1000 | Loss: 0.00001719
Iteration 98/1000 | Loss: 0.00001719
Iteration 99/1000 | Loss: 0.00001719
Iteration 100/1000 | Loss: 0.00001719
Iteration 101/1000 | Loss: 0.00001719
Iteration 102/1000 | Loss: 0.00001719
Iteration 103/1000 | Loss: 0.00001719
Iteration 104/1000 | Loss: 0.00001719
Iteration 105/1000 | Loss: 0.00001719
Iteration 106/1000 | Loss: 0.00001719
Iteration 107/1000 | Loss: 0.00001719
Iteration 108/1000 | Loss: 0.00001719
Iteration 109/1000 | Loss: 0.00001718
Iteration 110/1000 | Loss: 0.00001718
Iteration 111/1000 | Loss: 0.00001718
Iteration 112/1000 | Loss: 0.00001718
Iteration 113/1000 | Loss: 0.00001718
Iteration 114/1000 | Loss: 0.00001718
Iteration 115/1000 | Loss: 0.00001718
Iteration 116/1000 | Loss: 0.00001717
Iteration 117/1000 | Loss: 0.00001717
Iteration 118/1000 | Loss: 0.00001717
Iteration 119/1000 | Loss: 0.00001717
Iteration 120/1000 | Loss: 0.00001717
Iteration 121/1000 | Loss: 0.00001717
Iteration 122/1000 | Loss: 0.00001717
Iteration 123/1000 | Loss: 0.00001716
Iteration 124/1000 | Loss: 0.00001716
Iteration 125/1000 | Loss: 0.00001716
Iteration 126/1000 | Loss: 0.00001716
Iteration 127/1000 | Loss: 0.00001716
Iteration 128/1000 | Loss: 0.00001716
Iteration 129/1000 | Loss: 0.00001716
Iteration 130/1000 | Loss: 0.00001716
Iteration 131/1000 | Loss: 0.00001716
Iteration 132/1000 | Loss: 0.00001716
Iteration 133/1000 | Loss: 0.00001716
Iteration 134/1000 | Loss: 0.00001716
Iteration 135/1000 | Loss: 0.00001716
Iteration 136/1000 | Loss: 0.00001716
Iteration 137/1000 | Loss: 0.00001716
Iteration 138/1000 | Loss: 0.00001716
Iteration 139/1000 | Loss: 0.00001716
Iteration 140/1000 | Loss: 0.00001716
Iteration 141/1000 | Loss: 0.00001716
Iteration 142/1000 | Loss: 0.00001716
Iteration 143/1000 | Loss: 0.00001716
Iteration 144/1000 | Loss: 0.00001716
Iteration 145/1000 | Loss: 0.00001716
Iteration 146/1000 | Loss: 0.00001716
Iteration 147/1000 | Loss: 0.00001716
Iteration 148/1000 | Loss: 0.00001716
Iteration 149/1000 | Loss: 0.00001716
Iteration 150/1000 | Loss: 0.00001716
Iteration 151/1000 | Loss: 0.00001716
Iteration 152/1000 | Loss: 0.00001716
Iteration 153/1000 | Loss: 0.00001716
Iteration 154/1000 | Loss: 0.00001716
Iteration 155/1000 | Loss: 0.00001716
Iteration 156/1000 | Loss: 0.00001716
Iteration 157/1000 | Loss: 0.00001716
Iteration 158/1000 | Loss: 0.00001716
Iteration 159/1000 | Loss: 0.00001716
Iteration 160/1000 | Loss: 0.00001716
Iteration 161/1000 | Loss: 0.00001716
Iteration 162/1000 | Loss: 0.00001716
Iteration 163/1000 | Loss: 0.00001716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.71581868926296e-05, 1.71581868926296e-05, 1.71581868926296e-05, 1.71581868926296e-05, 1.71581868926296e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.71581868926296e-05

Optimization complete. Final v2v error: 3.3226592540740967 mm

Highest mean error: 6.565923690795898 mm for frame 51

Lowest mean error: 2.3945930004119873 mm for frame 100

Saving results

Total time: 85.40392065048218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829314
Iteration 2/25 | Loss: 0.00167395
Iteration 3/25 | Loss: 0.00114848
Iteration 4/25 | Loss: 0.00103826
Iteration 5/25 | Loss: 0.00101802
Iteration 6/25 | Loss: 0.00100382
Iteration 7/25 | Loss: 0.00099648
Iteration 8/25 | Loss: 0.00099587
Iteration 9/25 | Loss: 0.00098877
Iteration 10/25 | Loss: 0.00099485
Iteration 11/25 | Loss: 0.00099372
Iteration 12/25 | Loss: 0.00098672
Iteration 13/25 | Loss: 0.00099188
Iteration 14/25 | Loss: 0.00098552
Iteration 15/25 | Loss: 0.00099513
Iteration 16/25 | Loss: 0.00098920
Iteration 17/25 | Loss: 0.00098612
Iteration 18/25 | Loss: 0.00098910
Iteration 19/25 | Loss: 0.00098685
Iteration 20/25 | Loss: 0.00098705
Iteration 21/25 | Loss: 0.00099379
Iteration 22/25 | Loss: 0.00097733
Iteration 23/25 | Loss: 0.00098679
Iteration 24/25 | Loss: 0.00098823
Iteration 25/25 | Loss: 0.00099273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.92682648
Iteration 2/25 | Loss: 0.00157069
Iteration 3/25 | Loss: 0.00157044
Iteration 4/25 | Loss: 0.00157044
Iteration 5/25 | Loss: 0.00157043
Iteration 6/25 | Loss: 0.00157043
Iteration 7/25 | Loss: 0.00157043
Iteration 8/25 | Loss: 0.00157043
Iteration 9/25 | Loss: 0.00157043
Iteration 10/25 | Loss: 0.00157043
Iteration 11/25 | Loss: 0.00157043
Iteration 12/25 | Loss: 0.00157043
Iteration 13/25 | Loss: 0.00157043
Iteration 14/25 | Loss: 0.00157043
Iteration 15/25 | Loss: 0.00157043
Iteration 16/25 | Loss: 0.00157043
Iteration 17/25 | Loss: 0.00157043
Iteration 18/25 | Loss: 0.00157043
Iteration 19/25 | Loss: 0.00157043
Iteration 20/25 | Loss: 0.00157043
Iteration 21/25 | Loss: 0.00157043
Iteration 22/25 | Loss: 0.00157043
Iteration 23/25 | Loss: 0.00157043
Iteration 24/25 | Loss: 0.00157043
Iteration 25/25 | Loss: 0.00157043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157043
Iteration 2/1000 | Loss: 0.00827871
Iteration 3/1000 | Loss: 0.00110048
Iteration 4/1000 | Loss: 0.01760932
Iteration 5/1000 | Loss: 0.01495329
Iteration 6/1000 | Loss: 0.00972872
Iteration 7/1000 | Loss: 0.01224257
Iteration 8/1000 | Loss: 0.00524533
Iteration 9/1000 | Loss: 0.00082583
Iteration 10/1000 | Loss: 0.00536606
Iteration 11/1000 | Loss: 0.00373835
Iteration 12/1000 | Loss: 0.00582010
Iteration 13/1000 | Loss: 0.00544246
Iteration 14/1000 | Loss: 0.00672468
Iteration 15/1000 | Loss: 0.00241610
Iteration 16/1000 | Loss: 0.00231517
Iteration 17/1000 | Loss: 0.00266151
Iteration 18/1000 | Loss: 0.00503071
Iteration 19/1000 | Loss: 0.00336868
Iteration 20/1000 | Loss: 0.00530604
Iteration 21/1000 | Loss: 0.00266048
Iteration 22/1000 | Loss: 0.00088391
Iteration 23/1000 | Loss: 0.00070110
Iteration 24/1000 | Loss: 0.00609544
Iteration 25/1000 | Loss: 0.00326213
Iteration 26/1000 | Loss: 0.00243136
Iteration 27/1000 | Loss: 0.00067229
Iteration 28/1000 | Loss: 0.00029057
Iteration 29/1000 | Loss: 0.00039032
Iteration 30/1000 | Loss: 0.00023047
Iteration 31/1000 | Loss: 0.00050543
Iteration 32/1000 | Loss: 0.00020598
Iteration 33/1000 | Loss: 0.00042817
Iteration 34/1000 | Loss: 0.00038224
Iteration 35/1000 | Loss: 0.00036559
Iteration 36/1000 | Loss: 0.00038763
Iteration 37/1000 | Loss: 0.00021908
Iteration 38/1000 | Loss: 0.00018989
Iteration 39/1000 | Loss: 0.00038908
Iteration 40/1000 | Loss: 0.00065820
Iteration 41/1000 | Loss: 0.00030749
Iteration 42/1000 | Loss: 0.00006020
Iteration 43/1000 | Loss: 0.00004883
Iteration 44/1000 | Loss: 0.00007035
Iteration 45/1000 | Loss: 0.00022821
Iteration 46/1000 | Loss: 0.00004111
Iteration 47/1000 | Loss: 0.00003577
Iteration 48/1000 | Loss: 0.00003244
Iteration 49/1000 | Loss: 0.00003070
Iteration 50/1000 | Loss: 0.00002962
Iteration 51/1000 | Loss: 0.00002894
Iteration 52/1000 | Loss: 0.00002816
Iteration 53/1000 | Loss: 0.00002759
Iteration 54/1000 | Loss: 0.00002710
Iteration 55/1000 | Loss: 0.00002676
Iteration 56/1000 | Loss: 0.00002643
Iteration 57/1000 | Loss: 0.00002624
Iteration 58/1000 | Loss: 0.00002604
Iteration 59/1000 | Loss: 0.00002594
Iteration 60/1000 | Loss: 0.00002593
Iteration 61/1000 | Loss: 0.00002590
Iteration 62/1000 | Loss: 0.00002590
Iteration 63/1000 | Loss: 0.00002589
Iteration 64/1000 | Loss: 0.00002589
Iteration 65/1000 | Loss: 0.00002587
Iteration 66/1000 | Loss: 0.00002586
Iteration 67/1000 | Loss: 0.00002586
Iteration 68/1000 | Loss: 0.00002585
Iteration 69/1000 | Loss: 0.00002584
Iteration 70/1000 | Loss: 0.00002584
Iteration 71/1000 | Loss: 0.00002584
Iteration 72/1000 | Loss: 0.00002583
Iteration 73/1000 | Loss: 0.00002583
Iteration 74/1000 | Loss: 0.00002582
Iteration 75/1000 | Loss: 0.00002582
Iteration 76/1000 | Loss: 0.00002581
Iteration 77/1000 | Loss: 0.00002581
Iteration 78/1000 | Loss: 0.00002581
Iteration 79/1000 | Loss: 0.00002580
Iteration 80/1000 | Loss: 0.00002580
Iteration 81/1000 | Loss: 0.00002580
Iteration 82/1000 | Loss: 0.00002579
Iteration 83/1000 | Loss: 0.00002579
Iteration 84/1000 | Loss: 0.00002579
Iteration 85/1000 | Loss: 0.00002579
Iteration 86/1000 | Loss: 0.00002578
Iteration 87/1000 | Loss: 0.00002578
Iteration 88/1000 | Loss: 0.00002578
Iteration 89/1000 | Loss: 0.00002577
Iteration 90/1000 | Loss: 0.00002577
Iteration 91/1000 | Loss: 0.00002577
Iteration 92/1000 | Loss: 0.00002576
Iteration 93/1000 | Loss: 0.00002576
Iteration 94/1000 | Loss: 0.00002576
Iteration 95/1000 | Loss: 0.00002575
Iteration 96/1000 | Loss: 0.00002575
Iteration 97/1000 | Loss: 0.00002574
Iteration 98/1000 | Loss: 0.00002574
Iteration 99/1000 | Loss: 0.00002573
Iteration 100/1000 | Loss: 0.00002573
Iteration 101/1000 | Loss: 0.00002572
Iteration 102/1000 | Loss: 0.00002572
Iteration 103/1000 | Loss: 0.00002572
Iteration 104/1000 | Loss: 0.00002571
Iteration 105/1000 | Loss: 0.00002571
Iteration 106/1000 | Loss: 0.00002571
Iteration 107/1000 | Loss: 0.00002570
Iteration 108/1000 | Loss: 0.00002570
Iteration 109/1000 | Loss: 0.00002569
Iteration 110/1000 | Loss: 0.00002569
Iteration 111/1000 | Loss: 0.00002569
Iteration 112/1000 | Loss: 0.00002569
Iteration 113/1000 | Loss: 0.00002568
Iteration 114/1000 | Loss: 0.00002568
Iteration 115/1000 | Loss: 0.00002568
Iteration 116/1000 | Loss: 0.00002568
Iteration 117/1000 | Loss: 0.00002568
Iteration 118/1000 | Loss: 0.00002568
Iteration 119/1000 | Loss: 0.00002567
Iteration 120/1000 | Loss: 0.00002567
Iteration 121/1000 | Loss: 0.00002567
Iteration 122/1000 | Loss: 0.00002567
Iteration 123/1000 | Loss: 0.00002567
Iteration 124/1000 | Loss: 0.00002567
Iteration 125/1000 | Loss: 0.00002567
Iteration 126/1000 | Loss: 0.00002566
Iteration 127/1000 | Loss: 0.00002566
Iteration 128/1000 | Loss: 0.00002566
Iteration 129/1000 | Loss: 0.00002566
Iteration 130/1000 | Loss: 0.00002566
Iteration 131/1000 | Loss: 0.00002565
Iteration 132/1000 | Loss: 0.00002565
Iteration 133/1000 | Loss: 0.00002565
Iteration 134/1000 | Loss: 0.00002565
Iteration 135/1000 | Loss: 0.00002565
Iteration 136/1000 | Loss: 0.00002565
Iteration 137/1000 | Loss: 0.00002564
Iteration 138/1000 | Loss: 0.00002564
Iteration 139/1000 | Loss: 0.00002564
Iteration 140/1000 | Loss: 0.00002564
Iteration 141/1000 | Loss: 0.00002564
Iteration 142/1000 | Loss: 0.00002564
Iteration 143/1000 | Loss: 0.00002564
Iteration 144/1000 | Loss: 0.00002564
Iteration 145/1000 | Loss: 0.00002564
Iteration 146/1000 | Loss: 0.00002564
Iteration 147/1000 | Loss: 0.00002564
Iteration 148/1000 | Loss: 0.00002564
Iteration 149/1000 | Loss: 0.00002564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.5643994376878254e-05, 2.5643994376878254e-05, 2.5643994376878254e-05, 2.5643994376878254e-05, 2.5643994376878254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5643994376878254e-05

Optimization complete. Final v2v error: 4.010384559631348 mm

Highest mean error: 6.304429531097412 mm for frame 33

Lowest mean error: 2.921062469482422 mm for frame 81

Saving results

Total time: 133.60803985595703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01096907
Iteration 2/25 | Loss: 0.00261110
Iteration 3/25 | Loss: 0.00168255
Iteration 4/25 | Loss: 0.00147043
Iteration 5/25 | Loss: 0.00144387
Iteration 6/25 | Loss: 0.00120331
Iteration 7/25 | Loss: 0.00107956
Iteration 8/25 | Loss: 0.00102747
Iteration 9/25 | Loss: 0.00098492
Iteration 10/25 | Loss: 0.00097034
Iteration 11/25 | Loss: 0.00095362
Iteration 12/25 | Loss: 0.00094420
Iteration 13/25 | Loss: 0.00093568
Iteration 14/25 | Loss: 0.00093680
Iteration 15/25 | Loss: 0.00093451
Iteration 16/25 | Loss: 0.00093518
Iteration 17/25 | Loss: 0.00092827
Iteration 18/25 | Loss: 0.00093304
Iteration 19/25 | Loss: 0.00093111
Iteration 20/25 | Loss: 0.00093403
Iteration 21/25 | Loss: 0.00092871
Iteration 22/25 | Loss: 0.00092611
Iteration 23/25 | Loss: 0.00092695
Iteration 24/25 | Loss: 0.00092826
Iteration 25/25 | Loss: 0.00093102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33161569
Iteration 2/25 | Loss: 0.00122918
Iteration 3/25 | Loss: 0.00122918
Iteration 4/25 | Loss: 0.00112870
Iteration 5/25 | Loss: 0.00112869
Iteration 6/25 | Loss: 0.00112869
Iteration 7/25 | Loss: 0.00112869
Iteration 8/25 | Loss: 0.00112869
Iteration 9/25 | Loss: 0.00112869
Iteration 10/25 | Loss: 0.00112869
Iteration 11/25 | Loss: 0.00112869
Iteration 12/25 | Loss: 0.00112869
Iteration 13/25 | Loss: 0.00112869
Iteration 14/25 | Loss: 0.00112869
Iteration 15/25 | Loss: 0.00112869
Iteration 16/25 | Loss: 0.00112869
Iteration 17/25 | Loss: 0.00112869
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001128686941228807, 0.001128686941228807, 0.001128686941228807, 0.001128686941228807, 0.001128686941228807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001128686941228807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112869
Iteration 2/1000 | Loss: 0.00034319
Iteration 3/1000 | Loss: 0.00016974
Iteration 4/1000 | Loss: 0.00012076
Iteration 5/1000 | Loss: 0.00059918
Iteration 6/1000 | Loss: 0.00027745
Iteration 7/1000 | Loss: 0.00017387
Iteration 8/1000 | Loss: 0.00037790
Iteration 9/1000 | Loss: 0.00023074
Iteration 10/1000 | Loss: 0.00044490
Iteration 11/1000 | Loss: 0.00018455
Iteration 12/1000 | Loss: 0.00046198
Iteration 13/1000 | Loss: 0.00042378
Iteration 14/1000 | Loss: 0.00016830
Iteration 15/1000 | Loss: 0.00024793
Iteration 16/1000 | Loss: 0.00042517
Iteration 17/1000 | Loss: 0.00008256
Iteration 18/1000 | Loss: 0.00019233
Iteration 19/1000 | Loss: 0.00023893
Iteration 20/1000 | Loss: 0.00015961
Iteration 21/1000 | Loss: 0.00027401
Iteration 22/1000 | Loss: 0.00022900
Iteration 23/1000 | Loss: 0.00015106
Iteration 24/1000 | Loss: 0.00015514
Iteration 25/1000 | Loss: 0.00024912
Iteration 26/1000 | Loss: 0.00055082
Iteration 27/1000 | Loss: 0.00034958
Iteration 28/1000 | Loss: 0.00022670
Iteration 29/1000 | Loss: 0.00062636
Iteration 30/1000 | Loss: 0.00036038
Iteration 31/1000 | Loss: 0.00027753
Iteration 32/1000 | Loss: 0.00016517
Iteration 33/1000 | Loss: 0.00038831
Iteration 34/1000 | Loss: 0.00034646
Iteration 35/1000 | Loss: 0.00044544
Iteration 36/1000 | Loss: 0.00021176
Iteration 37/1000 | Loss: 0.00017255
Iteration 38/1000 | Loss: 0.00020950
Iteration 39/1000 | Loss: 0.00028043
Iteration 40/1000 | Loss: 0.00028670
Iteration 41/1000 | Loss: 0.00029561
Iteration 42/1000 | Loss: 0.00039027
Iteration 43/1000 | Loss: 0.00014200
Iteration 44/1000 | Loss: 0.00013650
Iteration 45/1000 | Loss: 0.00013450
Iteration 46/1000 | Loss: 0.00029503
Iteration 47/1000 | Loss: 0.00055149
Iteration 48/1000 | Loss: 0.00008948
Iteration 49/1000 | Loss: 0.00024881
Iteration 50/1000 | Loss: 0.00016099
Iteration 51/1000 | Loss: 0.00011871
Iteration 52/1000 | Loss: 0.00015178
Iteration 53/1000 | Loss: 0.00010422
Iteration 54/1000 | Loss: 0.00008061
Iteration 55/1000 | Loss: 0.00003851
Iteration 56/1000 | Loss: 0.00035566
Iteration 57/1000 | Loss: 0.00005325
Iteration 58/1000 | Loss: 0.00004607
Iteration 59/1000 | Loss: 0.00005572
Iteration 60/1000 | Loss: 0.00009924
Iteration 61/1000 | Loss: 0.00009941
Iteration 62/1000 | Loss: 0.00003547
Iteration 63/1000 | Loss: 0.00003351
Iteration 64/1000 | Loss: 0.00003175
Iteration 65/1000 | Loss: 0.00003068
Iteration 66/1000 | Loss: 0.00010465
Iteration 67/1000 | Loss: 0.00003445
Iteration 68/1000 | Loss: 0.00002864
Iteration 69/1000 | Loss: 0.00002754
Iteration 70/1000 | Loss: 0.00002699
Iteration 71/1000 | Loss: 0.00002661
Iteration 72/1000 | Loss: 0.00002640
Iteration 73/1000 | Loss: 0.00002619
Iteration 74/1000 | Loss: 0.00002613
Iteration 75/1000 | Loss: 0.00002612
Iteration 76/1000 | Loss: 0.00002612
Iteration 77/1000 | Loss: 0.00002611
Iteration 78/1000 | Loss: 0.00002610
Iteration 79/1000 | Loss: 0.00002609
Iteration 80/1000 | Loss: 0.00002609
Iteration 81/1000 | Loss: 0.00002606
Iteration 82/1000 | Loss: 0.00002604
Iteration 83/1000 | Loss: 0.00002603
Iteration 84/1000 | Loss: 0.00002603
Iteration 85/1000 | Loss: 0.00002599
Iteration 86/1000 | Loss: 0.00002598
Iteration 87/1000 | Loss: 0.00002595
Iteration 88/1000 | Loss: 0.00002594
Iteration 89/1000 | Loss: 0.00002593
Iteration 90/1000 | Loss: 0.00002592
Iteration 91/1000 | Loss: 0.00002591
Iteration 92/1000 | Loss: 0.00002591
Iteration 93/1000 | Loss: 0.00002588
Iteration 94/1000 | Loss: 0.00002588
Iteration 95/1000 | Loss: 0.00002588
Iteration 96/1000 | Loss: 0.00002588
Iteration 97/1000 | Loss: 0.00002588
Iteration 98/1000 | Loss: 0.00002587
Iteration 99/1000 | Loss: 0.00002587
Iteration 100/1000 | Loss: 0.00002586
Iteration 101/1000 | Loss: 0.00002586
Iteration 102/1000 | Loss: 0.00002586
Iteration 103/1000 | Loss: 0.00002586
Iteration 104/1000 | Loss: 0.00002586
Iteration 105/1000 | Loss: 0.00002586
Iteration 106/1000 | Loss: 0.00002586
Iteration 107/1000 | Loss: 0.00002586
Iteration 108/1000 | Loss: 0.00002586
Iteration 109/1000 | Loss: 0.00002586
Iteration 110/1000 | Loss: 0.00002586
Iteration 111/1000 | Loss: 0.00002586
Iteration 112/1000 | Loss: 0.00002586
Iteration 113/1000 | Loss: 0.00002586
Iteration 114/1000 | Loss: 0.00002586
Iteration 115/1000 | Loss: 0.00002586
Iteration 116/1000 | Loss: 0.00002586
Iteration 117/1000 | Loss: 0.00002586
Iteration 118/1000 | Loss: 0.00002586
Iteration 119/1000 | Loss: 0.00002586
Iteration 120/1000 | Loss: 0.00002586
Iteration 121/1000 | Loss: 0.00002586
Iteration 122/1000 | Loss: 0.00002586
Iteration 123/1000 | Loss: 0.00002586
Iteration 124/1000 | Loss: 0.00002586
Iteration 125/1000 | Loss: 0.00002586
Iteration 126/1000 | Loss: 0.00002586
Iteration 127/1000 | Loss: 0.00002586
Iteration 128/1000 | Loss: 0.00002586
Iteration 129/1000 | Loss: 0.00002586
Iteration 130/1000 | Loss: 0.00002586
Iteration 131/1000 | Loss: 0.00002586
Iteration 132/1000 | Loss: 0.00002586
Iteration 133/1000 | Loss: 0.00002586
Iteration 134/1000 | Loss: 0.00002586
Iteration 135/1000 | Loss: 0.00002586
Iteration 136/1000 | Loss: 0.00002586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.585746733529959e-05, 2.585746733529959e-05, 2.585746733529959e-05, 2.585746733529959e-05, 2.585746733529959e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.585746733529959e-05

Optimization complete. Final v2v error: 3.8816001415252686 mm

Highest mean error: 21.831501007080078 mm for frame 107

Lowest mean error: 3.3628785610198975 mm for frame 28

Saving results

Total time: 177.33801198005676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386901
Iteration 2/25 | Loss: 0.00105765
Iteration 3/25 | Loss: 0.00086559
Iteration 4/25 | Loss: 0.00083191
Iteration 5/25 | Loss: 0.00082240
Iteration 6/25 | Loss: 0.00082014
Iteration 7/25 | Loss: 0.00081983
Iteration 8/25 | Loss: 0.00081983
Iteration 9/25 | Loss: 0.00081983
Iteration 10/25 | Loss: 0.00081983
Iteration 11/25 | Loss: 0.00081983
Iteration 12/25 | Loss: 0.00081983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008198270807042718, 0.0008198270807042718, 0.0008198270807042718, 0.0008198270807042718, 0.0008198270807042718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008198270807042718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31590319
Iteration 2/25 | Loss: 0.00042885
Iteration 3/25 | Loss: 0.00042885
Iteration 4/25 | Loss: 0.00042885
Iteration 5/25 | Loss: 0.00042885
Iteration 6/25 | Loss: 0.00042885
Iteration 7/25 | Loss: 0.00042885
Iteration 8/25 | Loss: 0.00042885
Iteration 9/25 | Loss: 0.00042885
Iteration 10/25 | Loss: 0.00042885
Iteration 11/25 | Loss: 0.00042885
Iteration 12/25 | Loss: 0.00042885
Iteration 13/25 | Loss: 0.00042885
Iteration 14/25 | Loss: 0.00042885
Iteration 15/25 | Loss: 0.00042885
Iteration 16/25 | Loss: 0.00042885
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00042884546564891934, 0.00042884546564891934, 0.00042884546564891934, 0.00042884546564891934, 0.00042884546564891934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00042884546564891934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042885
Iteration 2/1000 | Loss: 0.00002981
Iteration 3/1000 | Loss: 0.00002299
Iteration 4/1000 | Loss: 0.00002115
Iteration 5/1000 | Loss: 0.00002034
Iteration 6/1000 | Loss: 0.00001964
Iteration 7/1000 | Loss: 0.00001917
Iteration 8/1000 | Loss: 0.00001868
Iteration 9/1000 | Loss: 0.00001841
Iteration 10/1000 | Loss: 0.00001823
Iteration 11/1000 | Loss: 0.00001821
Iteration 12/1000 | Loss: 0.00001821
Iteration 13/1000 | Loss: 0.00001819
Iteration 14/1000 | Loss: 0.00001818
Iteration 15/1000 | Loss: 0.00001818
Iteration 16/1000 | Loss: 0.00001817
Iteration 17/1000 | Loss: 0.00001815
Iteration 18/1000 | Loss: 0.00001809
Iteration 19/1000 | Loss: 0.00001805
Iteration 20/1000 | Loss: 0.00001804
Iteration 21/1000 | Loss: 0.00001802
Iteration 22/1000 | Loss: 0.00001801
Iteration 23/1000 | Loss: 0.00001801
Iteration 24/1000 | Loss: 0.00001801
Iteration 25/1000 | Loss: 0.00001800
Iteration 26/1000 | Loss: 0.00001800
Iteration 27/1000 | Loss: 0.00001799
Iteration 28/1000 | Loss: 0.00001799
Iteration 29/1000 | Loss: 0.00001798
Iteration 30/1000 | Loss: 0.00001798
Iteration 31/1000 | Loss: 0.00001797
Iteration 32/1000 | Loss: 0.00001797
Iteration 33/1000 | Loss: 0.00001797
Iteration 34/1000 | Loss: 0.00001796
Iteration 35/1000 | Loss: 0.00001796
Iteration 36/1000 | Loss: 0.00001796
Iteration 37/1000 | Loss: 0.00001795
Iteration 38/1000 | Loss: 0.00001795
Iteration 39/1000 | Loss: 0.00001794
Iteration 40/1000 | Loss: 0.00001794
Iteration 41/1000 | Loss: 0.00001794
Iteration 42/1000 | Loss: 0.00001793
Iteration 43/1000 | Loss: 0.00001793
Iteration 44/1000 | Loss: 0.00001793
Iteration 45/1000 | Loss: 0.00001792
Iteration 46/1000 | Loss: 0.00001792
Iteration 47/1000 | Loss: 0.00001792
Iteration 48/1000 | Loss: 0.00001792
Iteration 49/1000 | Loss: 0.00001791
Iteration 50/1000 | Loss: 0.00001791
Iteration 51/1000 | Loss: 0.00001790
Iteration 52/1000 | Loss: 0.00001790
Iteration 53/1000 | Loss: 0.00001790
Iteration 54/1000 | Loss: 0.00001790
Iteration 55/1000 | Loss: 0.00001789
Iteration 56/1000 | Loss: 0.00001789
Iteration 57/1000 | Loss: 0.00001789
Iteration 58/1000 | Loss: 0.00001788
Iteration 59/1000 | Loss: 0.00001788
Iteration 60/1000 | Loss: 0.00001788
Iteration 61/1000 | Loss: 0.00001788
Iteration 62/1000 | Loss: 0.00001788
Iteration 63/1000 | Loss: 0.00001788
Iteration 64/1000 | Loss: 0.00001787
Iteration 65/1000 | Loss: 0.00001787
Iteration 66/1000 | Loss: 0.00001787
Iteration 67/1000 | Loss: 0.00001787
Iteration 68/1000 | Loss: 0.00001787
Iteration 69/1000 | Loss: 0.00001787
Iteration 70/1000 | Loss: 0.00001787
Iteration 71/1000 | Loss: 0.00001786
Iteration 72/1000 | Loss: 0.00001786
Iteration 73/1000 | Loss: 0.00001786
Iteration 74/1000 | Loss: 0.00001786
Iteration 75/1000 | Loss: 0.00001786
Iteration 76/1000 | Loss: 0.00001786
Iteration 77/1000 | Loss: 0.00001786
Iteration 78/1000 | Loss: 0.00001786
Iteration 79/1000 | Loss: 0.00001786
Iteration 80/1000 | Loss: 0.00001786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.7860153093351983e-05, 1.7860153093351983e-05, 1.7860153093351983e-05, 1.7860153093351983e-05, 1.7860153093351983e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7860153093351983e-05

Optimization complete. Final v2v error: 3.566711902618408 mm

Highest mean error: 4.0802226066589355 mm for frame 206

Lowest mean error: 3.1281919479370117 mm for frame 57

Saving results

Total time: 33.91525363922119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00581666
Iteration 2/25 | Loss: 0.00104957
Iteration 3/25 | Loss: 0.00087658
Iteration 4/25 | Loss: 0.00084575
Iteration 5/25 | Loss: 0.00083837
Iteration 6/25 | Loss: 0.00083687
Iteration 7/25 | Loss: 0.00083687
Iteration 8/25 | Loss: 0.00083687
Iteration 9/25 | Loss: 0.00083687
Iteration 10/25 | Loss: 0.00083687
Iteration 11/25 | Loss: 0.00083687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008368742419406772, 0.0008368742419406772, 0.0008368742419406772, 0.0008368742419406772, 0.0008368742419406772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008368742419406772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36272883
Iteration 2/25 | Loss: 0.00042790
Iteration 3/25 | Loss: 0.00042789
Iteration 4/25 | Loss: 0.00042789
Iteration 5/25 | Loss: 0.00042789
Iteration 6/25 | Loss: 0.00042789
Iteration 7/25 | Loss: 0.00042789
Iteration 8/25 | Loss: 0.00042789
Iteration 9/25 | Loss: 0.00042789
Iteration 10/25 | Loss: 0.00042789
Iteration 11/25 | Loss: 0.00042789
Iteration 12/25 | Loss: 0.00042789
Iteration 13/25 | Loss: 0.00042789
Iteration 14/25 | Loss: 0.00042789
Iteration 15/25 | Loss: 0.00042789
Iteration 16/25 | Loss: 0.00042789
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004278922569938004, 0.0004278922569938004, 0.0004278922569938004, 0.0004278922569938004, 0.0004278922569938004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004278922569938004

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042789
Iteration 2/1000 | Loss: 0.00003849
Iteration 3/1000 | Loss: 0.00002493
Iteration 4/1000 | Loss: 0.00002304
Iteration 5/1000 | Loss: 0.00002215
Iteration 6/1000 | Loss: 0.00002153
Iteration 7/1000 | Loss: 0.00002122
Iteration 8/1000 | Loss: 0.00002095
Iteration 9/1000 | Loss: 0.00002077
Iteration 10/1000 | Loss: 0.00002076
Iteration 11/1000 | Loss: 0.00002075
Iteration 12/1000 | Loss: 0.00002069
Iteration 13/1000 | Loss: 0.00002068
Iteration 14/1000 | Loss: 0.00002064
Iteration 15/1000 | Loss: 0.00002062
Iteration 16/1000 | Loss: 0.00002061
Iteration 17/1000 | Loss: 0.00002061
Iteration 18/1000 | Loss: 0.00002061
Iteration 19/1000 | Loss: 0.00002060
Iteration 20/1000 | Loss: 0.00002060
Iteration 21/1000 | Loss: 0.00002056
Iteration 22/1000 | Loss: 0.00002055
Iteration 23/1000 | Loss: 0.00002055
Iteration 24/1000 | Loss: 0.00002055
Iteration 25/1000 | Loss: 0.00002052
Iteration 26/1000 | Loss: 0.00002051
Iteration 27/1000 | Loss: 0.00002050
Iteration 28/1000 | Loss: 0.00002050
Iteration 29/1000 | Loss: 0.00002049
Iteration 30/1000 | Loss: 0.00002049
Iteration 31/1000 | Loss: 0.00002049
Iteration 32/1000 | Loss: 0.00002048
Iteration 33/1000 | Loss: 0.00002048
Iteration 34/1000 | Loss: 0.00002047
Iteration 35/1000 | Loss: 0.00002046
Iteration 36/1000 | Loss: 0.00002046
Iteration 37/1000 | Loss: 0.00002046
Iteration 38/1000 | Loss: 0.00002045
Iteration 39/1000 | Loss: 0.00002045
Iteration 40/1000 | Loss: 0.00002045
Iteration 41/1000 | Loss: 0.00002044
Iteration 42/1000 | Loss: 0.00002044
Iteration 43/1000 | Loss: 0.00002043
Iteration 44/1000 | Loss: 0.00002043
Iteration 45/1000 | Loss: 0.00002043
Iteration 46/1000 | Loss: 0.00002042
Iteration 47/1000 | Loss: 0.00002042
Iteration 48/1000 | Loss: 0.00002041
Iteration 49/1000 | Loss: 0.00002041
Iteration 50/1000 | Loss: 0.00002040
Iteration 51/1000 | Loss: 0.00002040
Iteration 52/1000 | Loss: 0.00002039
Iteration 53/1000 | Loss: 0.00002038
Iteration 54/1000 | Loss: 0.00002037
Iteration 55/1000 | Loss: 0.00002037
Iteration 56/1000 | Loss: 0.00002037
Iteration 57/1000 | Loss: 0.00002037
Iteration 58/1000 | Loss: 0.00002037
Iteration 59/1000 | Loss: 0.00002037
Iteration 60/1000 | Loss: 0.00002036
Iteration 61/1000 | Loss: 0.00002036
Iteration 62/1000 | Loss: 0.00002036
Iteration 63/1000 | Loss: 0.00002036
Iteration 64/1000 | Loss: 0.00002036
Iteration 65/1000 | Loss: 0.00002036
Iteration 66/1000 | Loss: 0.00002036
Iteration 67/1000 | Loss: 0.00002035
Iteration 68/1000 | Loss: 0.00002035
Iteration 69/1000 | Loss: 0.00002034
Iteration 70/1000 | Loss: 0.00002034
Iteration 71/1000 | Loss: 0.00002034
Iteration 72/1000 | Loss: 0.00002034
Iteration 73/1000 | Loss: 0.00002033
Iteration 74/1000 | Loss: 0.00002033
Iteration 75/1000 | Loss: 0.00002033
Iteration 76/1000 | Loss: 0.00002033
Iteration 77/1000 | Loss: 0.00002033
Iteration 78/1000 | Loss: 0.00002033
Iteration 79/1000 | Loss: 0.00002033
Iteration 80/1000 | Loss: 0.00002033
Iteration 81/1000 | Loss: 0.00002033
Iteration 82/1000 | Loss: 0.00002033
Iteration 83/1000 | Loss: 0.00002032
Iteration 84/1000 | Loss: 0.00002032
Iteration 85/1000 | Loss: 0.00002032
Iteration 86/1000 | Loss: 0.00002032
Iteration 87/1000 | Loss: 0.00002031
Iteration 88/1000 | Loss: 0.00002031
Iteration 89/1000 | Loss: 0.00002031
Iteration 90/1000 | Loss: 0.00002031
Iteration 91/1000 | Loss: 0.00002031
Iteration 92/1000 | Loss: 0.00002030
Iteration 93/1000 | Loss: 0.00002030
Iteration 94/1000 | Loss: 0.00002030
Iteration 95/1000 | Loss: 0.00002030
Iteration 96/1000 | Loss: 0.00002030
Iteration 97/1000 | Loss: 0.00002030
Iteration 98/1000 | Loss: 0.00002029
Iteration 99/1000 | Loss: 0.00002029
Iteration 100/1000 | Loss: 0.00002029
Iteration 101/1000 | Loss: 0.00002029
Iteration 102/1000 | Loss: 0.00002029
Iteration 103/1000 | Loss: 0.00002029
Iteration 104/1000 | Loss: 0.00002029
Iteration 105/1000 | Loss: 0.00002029
Iteration 106/1000 | Loss: 0.00002029
Iteration 107/1000 | Loss: 0.00002029
Iteration 108/1000 | Loss: 0.00002029
Iteration 109/1000 | Loss: 0.00002029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.0286825019866228e-05, 2.0286825019866228e-05, 2.0286825019866228e-05, 2.0286825019866228e-05, 2.0286825019866228e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0286825019866228e-05

Optimization complete. Final v2v error: 3.8335509300231934 mm

Highest mean error: 4.157285690307617 mm for frame 85

Lowest mean error: 3.5214900970458984 mm for frame 55

Saving results

Total time: 35.07976984977722
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839904
Iteration 2/25 | Loss: 0.00126904
Iteration 3/25 | Loss: 0.00092337
Iteration 4/25 | Loss: 0.00088214
Iteration 5/25 | Loss: 0.00087625
Iteration 6/25 | Loss: 0.00087564
Iteration 7/25 | Loss: 0.00087564
Iteration 8/25 | Loss: 0.00087564
Iteration 9/25 | Loss: 0.00087564
Iteration 10/25 | Loss: 0.00087564
Iteration 11/25 | Loss: 0.00087564
Iteration 12/25 | Loss: 0.00087564
Iteration 13/25 | Loss: 0.00087564
Iteration 14/25 | Loss: 0.00087564
Iteration 15/25 | Loss: 0.00087564
Iteration 16/25 | Loss: 0.00087564
Iteration 17/25 | Loss: 0.00087564
Iteration 18/25 | Loss: 0.00087564
Iteration 19/25 | Loss: 0.00087564
Iteration 20/25 | Loss: 0.00087564
Iteration 21/25 | Loss: 0.00087564
Iteration 22/25 | Loss: 0.00087564
Iteration 23/25 | Loss: 0.00087564
Iteration 24/25 | Loss: 0.00087564
Iteration 25/25 | Loss: 0.00087564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30540073
Iteration 2/25 | Loss: 0.00049137
Iteration 3/25 | Loss: 0.00049137
Iteration 4/25 | Loss: 0.00049136
Iteration 5/25 | Loss: 0.00049136
Iteration 6/25 | Loss: 0.00049136
Iteration 7/25 | Loss: 0.00049136
Iteration 8/25 | Loss: 0.00049136
Iteration 9/25 | Loss: 0.00049136
Iteration 10/25 | Loss: 0.00049136
Iteration 11/25 | Loss: 0.00049136
Iteration 12/25 | Loss: 0.00049136
Iteration 13/25 | Loss: 0.00049136
Iteration 14/25 | Loss: 0.00049136
Iteration 15/25 | Loss: 0.00049136
Iteration 16/25 | Loss: 0.00049136
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000491362763568759, 0.000491362763568759, 0.000491362763568759, 0.000491362763568759, 0.000491362763568759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000491362763568759

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049136
Iteration 2/1000 | Loss: 0.00003228
Iteration 3/1000 | Loss: 0.00002432
Iteration 4/1000 | Loss: 0.00002290
Iteration 5/1000 | Loss: 0.00002220
Iteration 6/1000 | Loss: 0.00002167
Iteration 7/1000 | Loss: 0.00002133
Iteration 8/1000 | Loss: 0.00002107
Iteration 9/1000 | Loss: 0.00002103
Iteration 10/1000 | Loss: 0.00002101
Iteration 11/1000 | Loss: 0.00002094
Iteration 12/1000 | Loss: 0.00002092
Iteration 13/1000 | Loss: 0.00002078
Iteration 14/1000 | Loss: 0.00002072
Iteration 15/1000 | Loss: 0.00002071
Iteration 16/1000 | Loss: 0.00002071
Iteration 17/1000 | Loss: 0.00002070
Iteration 18/1000 | Loss: 0.00002070
Iteration 19/1000 | Loss: 0.00002070
Iteration 20/1000 | Loss: 0.00002070
Iteration 21/1000 | Loss: 0.00002069
Iteration 22/1000 | Loss: 0.00002069
Iteration 23/1000 | Loss: 0.00002069
Iteration 24/1000 | Loss: 0.00002069
Iteration 25/1000 | Loss: 0.00002069
Iteration 26/1000 | Loss: 0.00002069
Iteration 27/1000 | Loss: 0.00002069
Iteration 28/1000 | Loss: 0.00002069
Iteration 29/1000 | Loss: 0.00002068
Iteration 30/1000 | Loss: 0.00002068
Iteration 31/1000 | Loss: 0.00002068
Iteration 32/1000 | Loss: 0.00002068
Iteration 33/1000 | Loss: 0.00002067
Iteration 34/1000 | Loss: 0.00002067
Iteration 35/1000 | Loss: 0.00002067
Iteration 36/1000 | Loss: 0.00002066
Iteration 37/1000 | Loss: 0.00002066
Iteration 38/1000 | Loss: 0.00002066
Iteration 39/1000 | Loss: 0.00002065
Iteration 40/1000 | Loss: 0.00002065
Iteration 41/1000 | Loss: 0.00002065
Iteration 42/1000 | Loss: 0.00002064
Iteration 43/1000 | Loss: 0.00002064
Iteration 44/1000 | Loss: 0.00002064
Iteration 45/1000 | Loss: 0.00002064
Iteration 46/1000 | Loss: 0.00002064
Iteration 47/1000 | Loss: 0.00002064
Iteration 48/1000 | Loss: 0.00002064
Iteration 49/1000 | Loss: 0.00002064
Iteration 50/1000 | Loss: 0.00002063
Iteration 51/1000 | Loss: 0.00002063
Iteration 52/1000 | Loss: 0.00002063
Iteration 53/1000 | Loss: 0.00002063
Iteration 54/1000 | Loss: 0.00002063
Iteration 55/1000 | Loss: 0.00002063
Iteration 56/1000 | Loss: 0.00002063
Iteration 57/1000 | Loss: 0.00002063
Iteration 58/1000 | Loss: 0.00002063
Iteration 59/1000 | Loss: 0.00002062
Iteration 60/1000 | Loss: 0.00002062
Iteration 61/1000 | Loss: 0.00002062
Iteration 62/1000 | Loss: 0.00002061
Iteration 63/1000 | Loss: 0.00002061
Iteration 64/1000 | Loss: 0.00002061
Iteration 65/1000 | Loss: 0.00002061
Iteration 66/1000 | Loss: 0.00002061
Iteration 67/1000 | Loss: 0.00002061
Iteration 68/1000 | Loss: 0.00002060
Iteration 69/1000 | Loss: 0.00002060
Iteration 70/1000 | Loss: 0.00002060
Iteration 71/1000 | Loss: 0.00002060
Iteration 72/1000 | Loss: 0.00002060
Iteration 73/1000 | Loss: 0.00002060
Iteration 74/1000 | Loss: 0.00002060
Iteration 75/1000 | Loss: 0.00002060
Iteration 76/1000 | Loss: 0.00002060
Iteration 77/1000 | Loss: 0.00002060
Iteration 78/1000 | Loss: 0.00002060
Iteration 79/1000 | Loss: 0.00002059
Iteration 80/1000 | Loss: 0.00002059
Iteration 81/1000 | Loss: 0.00002059
Iteration 82/1000 | Loss: 0.00002059
Iteration 83/1000 | Loss: 0.00002059
Iteration 84/1000 | Loss: 0.00002058
Iteration 85/1000 | Loss: 0.00002058
Iteration 86/1000 | Loss: 0.00002058
Iteration 87/1000 | Loss: 0.00002058
Iteration 88/1000 | Loss: 0.00002058
Iteration 89/1000 | Loss: 0.00002058
Iteration 90/1000 | Loss: 0.00002058
Iteration 91/1000 | Loss: 0.00002058
Iteration 92/1000 | Loss: 0.00002058
Iteration 93/1000 | Loss: 0.00002058
Iteration 94/1000 | Loss: 0.00002058
Iteration 95/1000 | Loss: 0.00002058
Iteration 96/1000 | Loss: 0.00002058
Iteration 97/1000 | Loss: 0.00002058
Iteration 98/1000 | Loss: 0.00002057
Iteration 99/1000 | Loss: 0.00002057
Iteration 100/1000 | Loss: 0.00002057
Iteration 101/1000 | Loss: 0.00002057
Iteration 102/1000 | Loss: 0.00002057
Iteration 103/1000 | Loss: 0.00002057
Iteration 104/1000 | Loss: 0.00002057
Iteration 105/1000 | Loss: 0.00002056
Iteration 106/1000 | Loss: 0.00002056
Iteration 107/1000 | Loss: 0.00002056
Iteration 108/1000 | Loss: 0.00002056
Iteration 109/1000 | Loss: 0.00002056
Iteration 110/1000 | Loss: 0.00002056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.0564439182635397e-05, 2.0564439182635397e-05, 2.0564439182635397e-05, 2.0564439182635397e-05, 2.0564439182635397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0564439182635397e-05

Optimization complete. Final v2v error: 3.818211555480957 mm

Highest mean error: 4.211583137512207 mm for frame 6

Lowest mean error: 3.5507168769836426 mm for frame 83

Saving results

Total time: 33.75205850601196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398807
Iteration 2/25 | Loss: 0.00106328
Iteration 3/25 | Loss: 0.00091464
Iteration 4/25 | Loss: 0.00088172
Iteration 5/25 | Loss: 0.00086917
Iteration 6/25 | Loss: 0.00086681
Iteration 7/25 | Loss: 0.00086615
Iteration 8/25 | Loss: 0.00086609
Iteration 9/25 | Loss: 0.00086609
Iteration 10/25 | Loss: 0.00086609
Iteration 11/25 | Loss: 0.00086609
Iteration 12/25 | Loss: 0.00086609
Iteration 13/25 | Loss: 0.00086609
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008660919265821576, 0.0008660919265821576, 0.0008660919265821576, 0.0008660919265821576, 0.0008660919265821576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008660919265821576

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27326667
Iteration 2/25 | Loss: 0.00052073
Iteration 3/25 | Loss: 0.00052073
Iteration 4/25 | Loss: 0.00052073
Iteration 5/25 | Loss: 0.00052073
Iteration 6/25 | Loss: 0.00052073
Iteration 7/25 | Loss: 0.00052073
Iteration 8/25 | Loss: 0.00052073
Iteration 9/25 | Loss: 0.00052073
Iteration 10/25 | Loss: 0.00052073
Iteration 11/25 | Loss: 0.00052073
Iteration 12/25 | Loss: 0.00052073
Iteration 13/25 | Loss: 0.00052073
Iteration 14/25 | Loss: 0.00052073
Iteration 15/25 | Loss: 0.00052073
Iteration 16/25 | Loss: 0.00052073
Iteration 17/25 | Loss: 0.00052073
Iteration 18/25 | Loss: 0.00052073
Iteration 19/25 | Loss: 0.00052073
Iteration 20/25 | Loss: 0.00052073
Iteration 21/25 | Loss: 0.00052073
Iteration 22/25 | Loss: 0.00052073
Iteration 23/25 | Loss: 0.00052073
Iteration 24/25 | Loss: 0.00052073
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005207301583141088, 0.0005207301583141088, 0.0005207301583141088, 0.0005207301583141088, 0.0005207301583141088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005207301583141088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052073
Iteration 2/1000 | Loss: 0.00006666
Iteration 3/1000 | Loss: 0.00004294
Iteration 4/1000 | Loss: 0.00003566
Iteration 5/1000 | Loss: 0.00003215
Iteration 6/1000 | Loss: 0.00003054
Iteration 7/1000 | Loss: 0.00002945
Iteration 8/1000 | Loss: 0.00002854
Iteration 9/1000 | Loss: 0.00002799
Iteration 10/1000 | Loss: 0.00002734
Iteration 11/1000 | Loss: 0.00002697
Iteration 12/1000 | Loss: 0.00002675
Iteration 13/1000 | Loss: 0.00002652
Iteration 14/1000 | Loss: 0.00002644
Iteration 15/1000 | Loss: 0.00002638
Iteration 16/1000 | Loss: 0.00002638
Iteration 17/1000 | Loss: 0.00002636
Iteration 18/1000 | Loss: 0.00002636
Iteration 19/1000 | Loss: 0.00002632
Iteration 20/1000 | Loss: 0.00002629
Iteration 21/1000 | Loss: 0.00002628
Iteration 22/1000 | Loss: 0.00002628
Iteration 23/1000 | Loss: 0.00002627
Iteration 24/1000 | Loss: 0.00002627
Iteration 25/1000 | Loss: 0.00002626
Iteration 26/1000 | Loss: 0.00002626
Iteration 27/1000 | Loss: 0.00002625
Iteration 28/1000 | Loss: 0.00002625
Iteration 29/1000 | Loss: 0.00002624
Iteration 30/1000 | Loss: 0.00002624
Iteration 31/1000 | Loss: 0.00002624
Iteration 32/1000 | Loss: 0.00002623
Iteration 33/1000 | Loss: 0.00002623
Iteration 34/1000 | Loss: 0.00002623
Iteration 35/1000 | Loss: 0.00002622
Iteration 36/1000 | Loss: 0.00002622
Iteration 37/1000 | Loss: 0.00002621
Iteration 38/1000 | Loss: 0.00002621
Iteration 39/1000 | Loss: 0.00002620
Iteration 40/1000 | Loss: 0.00002620
Iteration 41/1000 | Loss: 0.00002620
Iteration 42/1000 | Loss: 0.00002619
Iteration 43/1000 | Loss: 0.00002619
Iteration 44/1000 | Loss: 0.00002618
Iteration 45/1000 | Loss: 0.00002618
Iteration 46/1000 | Loss: 0.00002618
Iteration 47/1000 | Loss: 0.00002618
Iteration 48/1000 | Loss: 0.00002617
Iteration 49/1000 | Loss: 0.00002617
Iteration 50/1000 | Loss: 0.00002616
Iteration 51/1000 | Loss: 0.00002616
Iteration 52/1000 | Loss: 0.00002615
Iteration 53/1000 | Loss: 0.00002615
Iteration 54/1000 | Loss: 0.00002615
Iteration 55/1000 | Loss: 0.00002614
Iteration 56/1000 | Loss: 0.00002614
Iteration 57/1000 | Loss: 0.00002614
Iteration 58/1000 | Loss: 0.00002614
Iteration 59/1000 | Loss: 0.00002614
Iteration 60/1000 | Loss: 0.00002614
Iteration 61/1000 | Loss: 0.00002613
Iteration 62/1000 | Loss: 0.00002613
Iteration 63/1000 | Loss: 0.00002613
Iteration 64/1000 | Loss: 0.00002612
Iteration 65/1000 | Loss: 0.00002612
Iteration 66/1000 | Loss: 0.00002612
Iteration 67/1000 | Loss: 0.00002612
Iteration 68/1000 | Loss: 0.00002612
Iteration 69/1000 | Loss: 0.00002612
Iteration 70/1000 | Loss: 0.00002611
Iteration 71/1000 | Loss: 0.00002611
Iteration 72/1000 | Loss: 0.00002611
Iteration 73/1000 | Loss: 0.00002610
Iteration 74/1000 | Loss: 0.00002610
Iteration 75/1000 | Loss: 0.00002610
Iteration 76/1000 | Loss: 0.00002610
Iteration 77/1000 | Loss: 0.00002610
Iteration 78/1000 | Loss: 0.00002609
Iteration 79/1000 | Loss: 0.00002609
Iteration 80/1000 | Loss: 0.00002609
Iteration 81/1000 | Loss: 0.00002609
Iteration 82/1000 | Loss: 0.00002609
Iteration 83/1000 | Loss: 0.00002609
Iteration 84/1000 | Loss: 0.00002609
Iteration 85/1000 | Loss: 0.00002609
Iteration 86/1000 | Loss: 0.00002609
Iteration 87/1000 | Loss: 0.00002609
Iteration 88/1000 | Loss: 0.00002608
Iteration 89/1000 | Loss: 0.00002608
Iteration 90/1000 | Loss: 0.00002608
Iteration 91/1000 | Loss: 0.00002608
Iteration 92/1000 | Loss: 0.00002608
Iteration 93/1000 | Loss: 0.00002608
Iteration 94/1000 | Loss: 0.00002608
Iteration 95/1000 | Loss: 0.00002607
Iteration 96/1000 | Loss: 0.00002607
Iteration 97/1000 | Loss: 0.00002607
Iteration 98/1000 | Loss: 0.00002607
Iteration 99/1000 | Loss: 0.00002607
Iteration 100/1000 | Loss: 0.00002607
Iteration 101/1000 | Loss: 0.00002607
Iteration 102/1000 | Loss: 0.00002607
Iteration 103/1000 | Loss: 0.00002607
Iteration 104/1000 | Loss: 0.00002607
Iteration 105/1000 | Loss: 0.00002607
Iteration 106/1000 | Loss: 0.00002607
Iteration 107/1000 | Loss: 0.00002607
Iteration 108/1000 | Loss: 0.00002607
Iteration 109/1000 | Loss: 0.00002606
Iteration 110/1000 | Loss: 0.00002606
Iteration 111/1000 | Loss: 0.00002606
Iteration 112/1000 | Loss: 0.00002606
Iteration 113/1000 | Loss: 0.00002606
Iteration 114/1000 | Loss: 0.00002606
Iteration 115/1000 | Loss: 0.00002606
Iteration 116/1000 | Loss: 0.00002606
Iteration 117/1000 | Loss: 0.00002606
Iteration 118/1000 | Loss: 0.00002606
Iteration 119/1000 | Loss: 0.00002606
Iteration 120/1000 | Loss: 0.00002606
Iteration 121/1000 | Loss: 0.00002605
Iteration 122/1000 | Loss: 0.00002605
Iteration 123/1000 | Loss: 0.00002605
Iteration 124/1000 | Loss: 0.00002605
Iteration 125/1000 | Loss: 0.00002605
Iteration 126/1000 | Loss: 0.00002605
Iteration 127/1000 | Loss: 0.00002605
Iteration 128/1000 | Loss: 0.00002605
Iteration 129/1000 | Loss: 0.00002605
Iteration 130/1000 | Loss: 0.00002605
Iteration 131/1000 | Loss: 0.00002605
Iteration 132/1000 | Loss: 0.00002604
Iteration 133/1000 | Loss: 0.00002604
Iteration 134/1000 | Loss: 0.00002604
Iteration 135/1000 | Loss: 0.00002604
Iteration 136/1000 | Loss: 0.00002604
Iteration 137/1000 | Loss: 0.00002604
Iteration 138/1000 | Loss: 0.00002604
Iteration 139/1000 | Loss: 0.00002604
Iteration 140/1000 | Loss: 0.00002604
Iteration 141/1000 | Loss: 0.00002604
Iteration 142/1000 | Loss: 0.00002604
Iteration 143/1000 | Loss: 0.00002604
Iteration 144/1000 | Loss: 0.00002604
Iteration 145/1000 | Loss: 0.00002604
Iteration 146/1000 | Loss: 0.00002604
Iteration 147/1000 | Loss: 0.00002604
Iteration 148/1000 | Loss: 0.00002603
Iteration 149/1000 | Loss: 0.00002603
Iteration 150/1000 | Loss: 0.00002603
Iteration 151/1000 | Loss: 0.00002603
Iteration 152/1000 | Loss: 0.00002603
Iteration 153/1000 | Loss: 0.00002603
Iteration 154/1000 | Loss: 0.00002603
Iteration 155/1000 | Loss: 0.00002603
Iteration 156/1000 | Loss: 0.00002603
Iteration 157/1000 | Loss: 0.00002603
Iteration 158/1000 | Loss: 0.00002603
Iteration 159/1000 | Loss: 0.00002603
Iteration 160/1000 | Loss: 0.00002603
Iteration 161/1000 | Loss: 0.00002602
Iteration 162/1000 | Loss: 0.00002602
Iteration 163/1000 | Loss: 0.00002602
Iteration 164/1000 | Loss: 0.00002602
Iteration 165/1000 | Loss: 0.00002602
Iteration 166/1000 | Loss: 0.00002602
Iteration 167/1000 | Loss: 0.00002602
Iteration 168/1000 | Loss: 0.00002602
Iteration 169/1000 | Loss: 0.00002602
Iteration 170/1000 | Loss: 0.00002602
Iteration 171/1000 | Loss: 0.00002602
Iteration 172/1000 | Loss: 0.00002602
Iteration 173/1000 | Loss: 0.00002602
Iteration 174/1000 | Loss: 0.00002602
Iteration 175/1000 | Loss: 0.00002602
Iteration 176/1000 | Loss: 0.00002602
Iteration 177/1000 | Loss: 0.00002602
Iteration 178/1000 | Loss: 0.00002602
Iteration 179/1000 | Loss: 0.00002602
Iteration 180/1000 | Loss: 0.00002602
Iteration 181/1000 | Loss: 0.00002601
Iteration 182/1000 | Loss: 0.00002601
Iteration 183/1000 | Loss: 0.00002601
Iteration 184/1000 | Loss: 0.00002601
Iteration 185/1000 | Loss: 0.00002601
Iteration 186/1000 | Loss: 0.00002601
Iteration 187/1000 | Loss: 0.00002601
Iteration 188/1000 | Loss: 0.00002601
Iteration 189/1000 | Loss: 0.00002601
Iteration 190/1000 | Loss: 0.00002601
Iteration 191/1000 | Loss: 0.00002601
Iteration 192/1000 | Loss: 0.00002601
Iteration 193/1000 | Loss: 0.00002601
Iteration 194/1000 | Loss: 0.00002601
Iteration 195/1000 | Loss: 0.00002601
Iteration 196/1000 | Loss: 0.00002601
Iteration 197/1000 | Loss: 0.00002601
Iteration 198/1000 | Loss: 0.00002601
Iteration 199/1000 | Loss: 0.00002601
Iteration 200/1000 | Loss: 0.00002601
Iteration 201/1000 | Loss: 0.00002600
Iteration 202/1000 | Loss: 0.00002600
Iteration 203/1000 | Loss: 0.00002600
Iteration 204/1000 | Loss: 0.00002600
Iteration 205/1000 | Loss: 0.00002600
Iteration 206/1000 | Loss: 0.00002600
Iteration 207/1000 | Loss: 0.00002600
Iteration 208/1000 | Loss: 0.00002600
Iteration 209/1000 | Loss: 0.00002600
Iteration 210/1000 | Loss: 0.00002600
Iteration 211/1000 | Loss: 0.00002600
Iteration 212/1000 | Loss: 0.00002600
Iteration 213/1000 | Loss: 0.00002600
Iteration 214/1000 | Loss: 0.00002600
Iteration 215/1000 | Loss: 0.00002600
Iteration 216/1000 | Loss: 0.00002600
Iteration 217/1000 | Loss: 0.00002600
Iteration 218/1000 | Loss: 0.00002600
Iteration 219/1000 | Loss: 0.00002600
Iteration 220/1000 | Loss: 0.00002600
Iteration 221/1000 | Loss: 0.00002600
Iteration 222/1000 | Loss: 0.00002600
Iteration 223/1000 | Loss: 0.00002600
Iteration 224/1000 | Loss: 0.00002600
Iteration 225/1000 | Loss: 0.00002600
Iteration 226/1000 | Loss: 0.00002600
Iteration 227/1000 | Loss: 0.00002600
Iteration 228/1000 | Loss: 0.00002600
Iteration 229/1000 | Loss: 0.00002600
Iteration 230/1000 | Loss: 0.00002600
Iteration 231/1000 | Loss: 0.00002600
Iteration 232/1000 | Loss: 0.00002600
Iteration 233/1000 | Loss: 0.00002600
Iteration 234/1000 | Loss: 0.00002600
Iteration 235/1000 | Loss: 0.00002600
Iteration 236/1000 | Loss: 0.00002600
Iteration 237/1000 | Loss: 0.00002600
Iteration 238/1000 | Loss: 0.00002600
Iteration 239/1000 | Loss: 0.00002600
Iteration 240/1000 | Loss: 0.00002600
Iteration 241/1000 | Loss: 0.00002600
Iteration 242/1000 | Loss: 0.00002600
Iteration 243/1000 | Loss: 0.00002600
Iteration 244/1000 | Loss: 0.00002600
Iteration 245/1000 | Loss: 0.00002600
Iteration 246/1000 | Loss: 0.00002600
Iteration 247/1000 | Loss: 0.00002600
Iteration 248/1000 | Loss: 0.00002600
Iteration 249/1000 | Loss: 0.00002600
Iteration 250/1000 | Loss: 0.00002600
Iteration 251/1000 | Loss: 0.00002600
Iteration 252/1000 | Loss: 0.00002600
Iteration 253/1000 | Loss: 0.00002600
Iteration 254/1000 | Loss: 0.00002600
Iteration 255/1000 | Loss: 0.00002600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [2.6001085643656552e-05, 2.6001085643656552e-05, 2.6001085643656552e-05, 2.6001085643656552e-05, 2.6001085643656552e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6001085643656552e-05

Optimization complete. Final v2v error: 4.183428764343262 mm

Highest mean error: 4.8467793464660645 mm for frame 91

Lowest mean error: 3.239875555038452 mm for frame 80

Saving results

Total time: 44.10267090797424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400043
Iteration 2/25 | Loss: 0.00092326
Iteration 3/25 | Loss: 0.00081327
Iteration 4/25 | Loss: 0.00079708
Iteration 5/25 | Loss: 0.00079126
Iteration 6/25 | Loss: 0.00078917
Iteration 7/25 | Loss: 0.00078880
Iteration 8/25 | Loss: 0.00078880
Iteration 9/25 | Loss: 0.00078880
Iteration 10/25 | Loss: 0.00078880
Iteration 11/25 | Loss: 0.00078880
Iteration 12/25 | Loss: 0.00078880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007887965766713023, 0.0007887965766713023, 0.0007887965766713023, 0.0007887965766713023, 0.0007887965766713023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007887965766713023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30633593
Iteration 2/25 | Loss: 0.00047038
Iteration 3/25 | Loss: 0.00047038
Iteration 4/25 | Loss: 0.00047038
Iteration 5/25 | Loss: 0.00047038
Iteration 6/25 | Loss: 0.00047037
Iteration 7/25 | Loss: 0.00047037
Iteration 8/25 | Loss: 0.00047037
Iteration 9/25 | Loss: 0.00047037
Iteration 10/25 | Loss: 0.00047037
Iteration 11/25 | Loss: 0.00047037
Iteration 12/25 | Loss: 0.00047037
Iteration 13/25 | Loss: 0.00047037
Iteration 14/25 | Loss: 0.00047037
Iteration 15/25 | Loss: 0.00047037
Iteration 16/25 | Loss: 0.00047037
Iteration 17/25 | Loss: 0.00047037
Iteration 18/25 | Loss: 0.00047037
Iteration 19/25 | Loss: 0.00047037
Iteration 20/25 | Loss: 0.00047037
Iteration 21/25 | Loss: 0.00047037
Iteration 22/25 | Loss: 0.00047037
Iteration 23/25 | Loss: 0.00047037
Iteration 24/25 | Loss: 0.00047037
Iteration 25/25 | Loss: 0.00047037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047037
Iteration 2/1000 | Loss: 0.00002904
Iteration 3/1000 | Loss: 0.00002135
Iteration 4/1000 | Loss: 0.00001961
Iteration 5/1000 | Loss: 0.00001858
Iteration 6/1000 | Loss: 0.00001803
Iteration 7/1000 | Loss: 0.00001746
Iteration 8/1000 | Loss: 0.00001711
Iteration 9/1000 | Loss: 0.00001689
Iteration 10/1000 | Loss: 0.00001688
Iteration 11/1000 | Loss: 0.00001688
Iteration 12/1000 | Loss: 0.00001687
Iteration 13/1000 | Loss: 0.00001682
Iteration 14/1000 | Loss: 0.00001682
Iteration 15/1000 | Loss: 0.00001682
Iteration 16/1000 | Loss: 0.00001682
Iteration 17/1000 | Loss: 0.00001682
Iteration 18/1000 | Loss: 0.00001682
Iteration 19/1000 | Loss: 0.00001682
Iteration 20/1000 | Loss: 0.00001681
Iteration 21/1000 | Loss: 0.00001676
Iteration 22/1000 | Loss: 0.00001676
Iteration 23/1000 | Loss: 0.00001675
Iteration 24/1000 | Loss: 0.00001675
Iteration 25/1000 | Loss: 0.00001674
Iteration 26/1000 | Loss: 0.00001670
Iteration 27/1000 | Loss: 0.00001668
Iteration 28/1000 | Loss: 0.00001667
Iteration 29/1000 | Loss: 0.00001667
Iteration 30/1000 | Loss: 0.00001667
Iteration 31/1000 | Loss: 0.00001666
Iteration 32/1000 | Loss: 0.00001666
Iteration 33/1000 | Loss: 0.00001665
Iteration 34/1000 | Loss: 0.00001664
Iteration 35/1000 | Loss: 0.00001664
Iteration 36/1000 | Loss: 0.00001664
Iteration 37/1000 | Loss: 0.00001663
Iteration 38/1000 | Loss: 0.00001663
Iteration 39/1000 | Loss: 0.00001663
Iteration 40/1000 | Loss: 0.00001662
Iteration 41/1000 | Loss: 0.00001662
Iteration 42/1000 | Loss: 0.00001661
Iteration 43/1000 | Loss: 0.00001661
Iteration 44/1000 | Loss: 0.00001659
Iteration 45/1000 | Loss: 0.00001658
Iteration 46/1000 | Loss: 0.00001658
Iteration 47/1000 | Loss: 0.00001658
Iteration 48/1000 | Loss: 0.00001657
Iteration 49/1000 | Loss: 0.00001657
Iteration 50/1000 | Loss: 0.00001656
Iteration 51/1000 | Loss: 0.00001655
Iteration 52/1000 | Loss: 0.00001655
Iteration 53/1000 | Loss: 0.00001654
Iteration 54/1000 | Loss: 0.00001654
Iteration 55/1000 | Loss: 0.00001654
Iteration 56/1000 | Loss: 0.00001654
Iteration 57/1000 | Loss: 0.00001653
Iteration 58/1000 | Loss: 0.00001653
Iteration 59/1000 | Loss: 0.00001653
Iteration 60/1000 | Loss: 0.00001653
Iteration 61/1000 | Loss: 0.00001653
Iteration 62/1000 | Loss: 0.00001653
Iteration 63/1000 | Loss: 0.00001652
Iteration 64/1000 | Loss: 0.00001652
Iteration 65/1000 | Loss: 0.00001651
Iteration 66/1000 | Loss: 0.00001651
Iteration 67/1000 | Loss: 0.00001651
Iteration 68/1000 | Loss: 0.00001651
Iteration 69/1000 | Loss: 0.00001650
Iteration 70/1000 | Loss: 0.00001650
Iteration 71/1000 | Loss: 0.00001650
Iteration 72/1000 | Loss: 0.00001649
Iteration 73/1000 | Loss: 0.00001649
Iteration 74/1000 | Loss: 0.00001649
Iteration 75/1000 | Loss: 0.00001649
Iteration 76/1000 | Loss: 0.00001649
Iteration 77/1000 | Loss: 0.00001649
Iteration 78/1000 | Loss: 0.00001649
Iteration 79/1000 | Loss: 0.00001649
Iteration 80/1000 | Loss: 0.00001649
Iteration 81/1000 | Loss: 0.00001649
Iteration 82/1000 | Loss: 0.00001649
Iteration 83/1000 | Loss: 0.00001649
Iteration 84/1000 | Loss: 0.00001649
Iteration 85/1000 | Loss: 0.00001649
Iteration 86/1000 | Loss: 0.00001649
Iteration 87/1000 | Loss: 0.00001649
Iteration 88/1000 | Loss: 0.00001649
Iteration 89/1000 | Loss: 0.00001649
Iteration 90/1000 | Loss: 0.00001649
Iteration 91/1000 | Loss: 0.00001649
Iteration 92/1000 | Loss: 0.00001649
Iteration 93/1000 | Loss: 0.00001649
Iteration 94/1000 | Loss: 0.00001649
Iteration 95/1000 | Loss: 0.00001649
Iteration 96/1000 | Loss: 0.00001649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.6485504602314904e-05, 1.6485504602314904e-05, 1.6485504602314904e-05, 1.6485504602314904e-05, 1.6485504602314904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6485504602314904e-05

Optimization complete. Final v2v error: 3.4227688312530518 mm

Highest mean error: 3.675985813140869 mm for frame 211

Lowest mean error: 2.9623796939849854 mm for frame 64

Saving results

Total time: 35.482640743255615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01095832
Iteration 2/25 | Loss: 0.00125697
Iteration 3/25 | Loss: 0.00096880
Iteration 4/25 | Loss: 0.00091413
Iteration 5/25 | Loss: 0.00090593
Iteration 6/25 | Loss: 0.00090334
Iteration 7/25 | Loss: 0.00090310
Iteration 8/25 | Loss: 0.00090310
Iteration 9/25 | Loss: 0.00090310
Iteration 10/25 | Loss: 0.00090310
Iteration 11/25 | Loss: 0.00090310
Iteration 12/25 | Loss: 0.00090310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000903099135030061, 0.000903099135030061, 0.000903099135030061, 0.000903099135030061, 0.000903099135030061]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000903099135030061

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42135954
Iteration 2/25 | Loss: 0.00037320
Iteration 3/25 | Loss: 0.00037320
Iteration 4/25 | Loss: 0.00037319
Iteration 5/25 | Loss: 0.00037319
Iteration 6/25 | Loss: 0.00037319
Iteration 7/25 | Loss: 0.00037319
Iteration 8/25 | Loss: 0.00037319
Iteration 9/25 | Loss: 0.00037319
Iteration 10/25 | Loss: 0.00037319
Iteration 11/25 | Loss: 0.00037319
Iteration 12/25 | Loss: 0.00037319
Iteration 13/25 | Loss: 0.00037319
Iteration 14/25 | Loss: 0.00037319
Iteration 15/25 | Loss: 0.00037319
Iteration 16/25 | Loss: 0.00037319
Iteration 17/25 | Loss: 0.00037319
Iteration 18/25 | Loss: 0.00037319
Iteration 19/25 | Loss: 0.00037319
Iteration 20/25 | Loss: 0.00037319
Iteration 21/25 | Loss: 0.00037319
Iteration 22/25 | Loss: 0.00037319
Iteration 23/25 | Loss: 0.00037319
Iteration 24/25 | Loss: 0.00037319
Iteration 25/25 | Loss: 0.00037319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037319
Iteration 2/1000 | Loss: 0.00003927
Iteration 3/1000 | Loss: 0.00002918
Iteration 4/1000 | Loss: 0.00002746
Iteration 5/1000 | Loss: 0.00002621
Iteration 6/1000 | Loss: 0.00002543
Iteration 7/1000 | Loss: 0.00002492
Iteration 8/1000 | Loss: 0.00002452
Iteration 9/1000 | Loss: 0.00002424
Iteration 10/1000 | Loss: 0.00002409
Iteration 11/1000 | Loss: 0.00002401
Iteration 12/1000 | Loss: 0.00002396
Iteration 13/1000 | Loss: 0.00002396
Iteration 14/1000 | Loss: 0.00002395
Iteration 15/1000 | Loss: 0.00002394
Iteration 16/1000 | Loss: 0.00002393
Iteration 17/1000 | Loss: 0.00002391
Iteration 18/1000 | Loss: 0.00002391
Iteration 19/1000 | Loss: 0.00002390
Iteration 20/1000 | Loss: 0.00002390
Iteration 21/1000 | Loss: 0.00002389
Iteration 22/1000 | Loss: 0.00002388
Iteration 23/1000 | Loss: 0.00002388
Iteration 24/1000 | Loss: 0.00002388
Iteration 25/1000 | Loss: 0.00002388
Iteration 26/1000 | Loss: 0.00002387
Iteration 27/1000 | Loss: 0.00002387
Iteration 28/1000 | Loss: 0.00002387
Iteration 29/1000 | Loss: 0.00002387
Iteration 30/1000 | Loss: 0.00002387
Iteration 31/1000 | Loss: 0.00002387
Iteration 32/1000 | Loss: 0.00002387
Iteration 33/1000 | Loss: 0.00002387
Iteration 34/1000 | Loss: 0.00002387
Iteration 35/1000 | Loss: 0.00002386
Iteration 36/1000 | Loss: 0.00002386
Iteration 37/1000 | Loss: 0.00002386
Iteration 38/1000 | Loss: 0.00002386
Iteration 39/1000 | Loss: 0.00002386
Iteration 40/1000 | Loss: 0.00002386
Iteration 41/1000 | Loss: 0.00002385
Iteration 42/1000 | Loss: 0.00002385
Iteration 43/1000 | Loss: 0.00002385
Iteration 44/1000 | Loss: 0.00002385
Iteration 45/1000 | Loss: 0.00002385
Iteration 46/1000 | Loss: 0.00002384
Iteration 47/1000 | Loss: 0.00002384
Iteration 48/1000 | Loss: 0.00002384
Iteration 49/1000 | Loss: 0.00002384
Iteration 50/1000 | Loss: 0.00002384
Iteration 51/1000 | Loss: 0.00002384
Iteration 52/1000 | Loss: 0.00002384
Iteration 53/1000 | Loss: 0.00002384
Iteration 54/1000 | Loss: 0.00002384
Iteration 55/1000 | Loss: 0.00002383
Iteration 56/1000 | Loss: 0.00002383
Iteration 57/1000 | Loss: 0.00002383
Iteration 58/1000 | Loss: 0.00002383
Iteration 59/1000 | Loss: 0.00002383
Iteration 60/1000 | Loss: 0.00002383
Iteration 61/1000 | Loss: 0.00002383
Iteration 62/1000 | Loss: 0.00002383
Iteration 63/1000 | Loss: 0.00002383
Iteration 64/1000 | Loss: 0.00002383
Iteration 65/1000 | Loss: 0.00002383
Iteration 66/1000 | Loss: 0.00002383
Iteration 67/1000 | Loss: 0.00002383
Iteration 68/1000 | Loss: 0.00002383
Iteration 69/1000 | Loss: 0.00002382
Iteration 70/1000 | Loss: 0.00002382
Iteration 71/1000 | Loss: 0.00002382
Iteration 72/1000 | Loss: 0.00002382
Iteration 73/1000 | Loss: 0.00002382
Iteration 74/1000 | Loss: 0.00002382
Iteration 75/1000 | Loss: 0.00002382
Iteration 76/1000 | Loss: 0.00002382
Iteration 77/1000 | Loss: 0.00002382
Iteration 78/1000 | Loss: 0.00002382
Iteration 79/1000 | Loss: 0.00002382
Iteration 80/1000 | Loss: 0.00002382
Iteration 81/1000 | Loss: 0.00002381
Iteration 82/1000 | Loss: 0.00002381
Iteration 83/1000 | Loss: 0.00002381
Iteration 84/1000 | Loss: 0.00002381
Iteration 85/1000 | Loss: 0.00002381
Iteration 86/1000 | Loss: 0.00002381
Iteration 87/1000 | Loss: 0.00002381
Iteration 88/1000 | Loss: 0.00002381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [2.3814560336177237e-05, 2.3814560336177237e-05, 2.3814560336177237e-05, 2.3814560336177237e-05, 2.3814560336177237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3814560336177237e-05

Optimization complete. Final v2v error: 4.1135053634643555 mm

Highest mean error: 4.540689468383789 mm for frame 176

Lowest mean error: 3.6344845294952393 mm for frame 84

Saving results

Total time: 31.758273363113403
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00601482
Iteration 2/25 | Loss: 0.00104980
Iteration 3/25 | Loss: 0.00085244
Iteration 4/25 | Loss: 0.00080852
Iteration 5/25 | Loss: 0.00078879
Iteration 6/25 | Loss: 0.00078354
Iteration 7/25 | Loss: 0.00078165
Iteration 8/25 | Loss: 0.00078131
Iteration 9/25 | Loss: 0.00078131
Iteration 10/25 | Loss: 0.00078131
Iteration 11/25 | Loss: 0.00078131
Iteration 12/25 | Loss: 0.00078131
Iteration 13/25 | Loss: 0.00078131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007813050760887563, 0.0007813050760887563, 0.0007813050760887563, 0.0007813050760887563, 0.0007813050760887563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007813050760887563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08089137
Iteration 2/25 | Loss: 0.00037475
Iteration 3/25 | Loss: 0.00037467
Iteration 4/25 | Loss: 0.00037466
Iteration 5/25 | Loss: 0.00037466
Iteration 6/25 | Loss: 0.00037466
Iteration 7/25 | Loss: 0.00037466
Iteration 8/25 | Loss: 0.00037466
Iteration 9/25 | Loss: 0.00037466
Iteration 10/25 | Loss: 0.00037466
Iteration 11/25 | Loss: 0.00037466
Iteration 12/25 | Loss: 0.00037466
Iteration 13/25 | Loss: 0.00037466
Iteration 14/25 | Loss: 0.00037466
Iteration 15/25 | Loss: 0.00037466
Iteration 16/25 | Loss: 0.00037466
Iteration 17/25 | Loss: 0.00037466
Iteration 18/25 | Loss: 0.00037466
Iteration 19/25 | Loss: 0.00037466
Iteration 20/25 | Loss: 0.00037466
Iteration 21/25 | Loss: 0.00037466
Iteration 22/25 | Loss: 0.00037466
Iteration 23/25 | Loss: 0.00037466
Iteration 24/25 | Loss: 0.00037466
Iteration 25/25 | Loss: 0.00037466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037466
Iteration 2/1000 | Loss: 0.00007639
Iteration 3/1000 | Loss: 0.00004246
Iteration 4/1000 | Loss: 0.00003281
Iteration 5/1000 | Loss: 0.00003079
Iteration 6/1000 | Loss: 0.00002945
Iteration 7/1000 | Loss: 0.00002886
Iteration 8/1000 | Loss: 0.00002835
Iteration 9/1000 | Loss: 0.00002791
Iteration 10/1000 | Loss: 0.00002760
Iteration 11/1000 | Loss: 0.00002737
Iteration 12/1000 | Loss: 0.00002716
Iteration 13/1000 | Loss: 0.00002710
Iteration 14/1000 | Loss: 0.00002701
Iteration 15/1000 | Loss: 0.00002700
Iteration 16/1000 | Loss: 0.00002700
Iteration 17/1000 | Loss: 0.00002700
Iteration 18/1000 | Loss: 0.00002683
Iteration 19/1000 | Loss: 0.00002683
Iteration 20/1000 | Loss: 0.00002679
Iteration 21/1000 | Loss: 0.00002669
Iteration 22/1000 | Loss: 0.00002669
Iteration 23/1000 | Loss: 0.00002669
Iteration 24/1000 | Loss: 0.00002669
Iteration 25/1000 | Loss: 0.00002669
Iteration 26/1000 | Loss: 0.00002669
Iteration 27/1000 | Loss: 0.00002669
Iteration 28/1000 | Loss: 0.00002669
Iteration 29/1000 | Loss: 0.00002669
Iteration 30/1000 | Loss: 0.00002669
Iteration 31/1000 | Loss: 0.00002668
Iteration 32/1000 | Loss: 0.00002664
Iteration 33/1000 | Loss: 0.00002664
Iteration 34/1000 | Loss: 0.00002664
Iteration 35/1000 | Loss: 0.00002664
Iteration 36/1000 | Loss: 0.00002664
Iteration 37/1000 | Loss: 0.00002664
Iteration 38/1000 | Loss: 0.00002664
Iteration 39/1000 | Loss: 0.00002664
Iteration 40/1000 | Loss: 0.00002664
Iteration 41/1000 | Loss: 0.00002663
Iteration 42/1000 | Loss: 0.00002663
Iteration 43/1000 | Loss: 0.00002663
Iteration 44/1000 | Loss: 0.00002663
Iteration 45/1000 | Loss: 0.00002663
Iteration 46/1000 | Loss: 0.00002662
Iteration 47/1000 | Loss: 0.00002662
Iteration 48/1000 | Loss: 0.00002662
Iteration 49/1000 | Loss: 0.00002662
Iteration 50/1000 | Loss: 0.00002661
Iteration 51/1000 | Loss: 0.00002661
Iteration 52/1000 | Loss: 0.00002661
Iteration 53/1000 | Loss: 0.00002661
Iteration 54/1000 | Loss: 0.00002660
Iteration 55/1000 | Loss: 0.00002660
Iteration 56/1000 | Loss: 0.00002660
Iteration 57/1000 | Loss: 0.00002659
Iteration 58/1000 | Loss: 0.00002659
Iteration 59/1000 | Loss: 0.00002659
Iteration 60/1000 | Loss: 0.00002659
Iteration 61/1000 | Loss: 0.00002659
Iteration 62/1000 | Loss: 0.00002658
Iteration 63/1000 | Loss: 0.00002658
Iteration 64/1000 | Loss: 0.00002657
Iteration 65/1000 | Loss: 0.00002657
Iteration 66/1000 | Loss: 0.00002657
Iteration 67/1000 | Loss: 0.00002657
Iteration 68/1000 | Loss: 0.00002657
Iteration 69/1000 | Loss: 0.00002657
Iteration 70/1000 | Loss: 0.00002656
Iteration 71/1000 | Loss: 0.00002656
Iteration 72/1000 | Loss: 0.00002656
Iteration 73/1000 | Loss: 0.00002655
Iteration 74/1000 | Loss: 0.00002653
Iteration 75/1000 | Loss: 0.00002653
Iteration 76/1000 | Loss: 0.00002653
Iteration 77/1000 | Loss: 0.00002653
Iteration 78/1000 | Loss: 0.00002653
Iteration 79/1000 | Loss: 0.00002653
Iteration 80/1000 | Loss: 0.00002653
Iteration 81/1000 | Loss: 0.00002652
Iteration 82/1000 | Loss: 0.00002652
Iteration 83/1000 | Loss: 0.00002652
Iteration 84/1000 | Loss: 0.00002651
Iteration 85/1000 | Loss: 0.00002651
Iteration 86/1000 | Loss: 0.00002648
Iteration 87/1000 | Loss: 0.00002647
Iteration 88/1000 | Loss: 0.00002644
Iteration 89/1000 | Loss: 0.00002644
Iteration 90/1000 | Loss: 0.00002644
Iteration 91/1000 | Loss: 0.00002644
Iteration 92/1000 | Loss: 0.00002643
Iteration 93/1000 | Loss: 0.00002643
Iteration 94/1000 | Loss: 0.00002643
Iteration 95/1000 | Loss: 0.00002643
Iteration 96/1000 | Loss: 0.00002643
Iteration 97/1000 | Loss: 0.00002642
Iteration 98/1000 | Loss: 0.00002641
Iteration 99/1000 | Loss: 0.00002641
Iteration 100/1000 | Loss: 0.00002640
Iteration 101/1000 | Loss: 0.00002640
Iteration 102/1000 | Loss: 0.00002640
Iteration 103/1000 | Loss: 0.00002639
Iteration 104/1000 | Loss: 0.00002639
Iteration 105/1000 | Loss: 0.00002638
Iteration 106/1000 | Loss: 0.00002637
Iteration 107/1000 | Loss: 0.00002637
Iteration 108/1000 | Loss: 0.00002637
Iteration 109/1000 | Loss: 0.00002637
Iteration 110/1000 | Loss: 0.00002637
Iteration 111/1000 | Loss: 0.00002637
Iteration 112/1000 | Loss: 0.00002637
Iteration 113/1000 | Loss: 0.00002636
Iteration 114/1000 | Loss: 0.00002634
Iteration 115/1000 | Loss: 0.00002634
Iteration 116/1000 | Loss: 0.00002633
Iteration 117/1000 | Loss: 0.00002633
Iteration 118/1000 | Loss: 0.00002633
Iteration 119/1000 | Loss: 0.00002633
Iteration 120/1000 | Loss: 0.00002633
Iteration 121/1000 | Loss: 0.00002633
Iteration 122/1000 | Loss: 0.00002633
Iteration 123/1000 | Loss: 0.00002633
Iteration 124/1000 | Loss: 0.00002632
Iteration 125/1000 | Loss: 0.00002632
Iteration 126/1000 | Loss: 0.00002631
Iteration 127/1000 | Loss: 0.00002629
Iteration 128/1000 | Loss: 0.00002629
Iteration 129/1000 | Loss: 0.00002629
Iteration 130/1000 | Loss: 0.00002629
Iteration 131/1000 | Loss: 0.00002629
Iteration 132/1000 | Loss: 0.00002629
Iteration 133/1000 | Loss: 0.00002629
Iteration 134/1000 | Loss: 0.00002629
Iteration 135/1000 | Loss: 0.00002629
Iteration 136/1000 | Loss: 0.00002629
Iteration 137/1000 | Loss: 0.00002628
Iteration 138/1000 | Loss: 0.00002628
Iteration 139/1000 | Loss: 0.00002628
Iteration 140/1000 | Loss: 0.00002628
Iteration 141/1000 | Loss: 0.00002627
Iteration 142/1000 | Loss: 0.00002627
Iteration 143/1000 | Loss: 0.00002627
Iteration 144/1000 | Loss: 0.00002627
Iteration 145/1000 | Loss: 0.00002627
Iteration 146/1000 | Loss: 0.00002627
Iteration 147/1000 | Loss: 0.00002627
Iteration 148/1000 | Loss: 0.00002626
Iteration 149/1000 | Loss: 0.00002626
Iteration 150/1000 | Loss: 0.00002625
Iteration 151/1000 | Loss: 0.00002625
Iteration 152/1000 | Loss: 0.00002625
Iteration 153/1000 | Loss: 0.00002625
Iteration 154/1000 | Loss: 0.00002625
Iteration 155/1000 | Loss: 0.00002625
Iteration 156/1000 | Loss: 0.00002625
Iteration 157/1000 | Loss: 0.00002625
Iteration 158/1000 | Loss: 0.00002625
Iteration 159/1000 | Loss: 0.00002625
Iteration 160/1000 | Loss: 0.00002625
Iteration 161/1000 | Loss: 0.00002625
Iteration 162/1000 | Loss: 0.00002624
Iteration 163/1000 | Loss: 0.00002624
Iteration 164/1000 | Loss: 0.00002624
Iteration 165/1000 | Loss: 0.00002624
Iteration 166/1000 | Loss: 0.00002624
Iteration 167/1000 | Loss: 0.00002623
Iteration 168/1000 | Loss: 0.00002623
Iteration 169/1000 | Loss: 0.00002623
Iteration 170/1000 | Loss: 0.00002623
Iteration 171/1000 | Loss: 0.00002622
Iteration 172/1000 | Loss: 0.00002622
Iteration 173/1000 | Loss: 0.00002622
Iteration 174/1000 | Loss: 0.00002621
Iteration 175/1000 | Loss: 0.00002621
Iteration 176/1000 | Loss: 0.00002621
Iteration 177/1000 | Loss: 0.00002621
Iteration 178/1000 | Loss: 0.00002621
Iteration 179/1000 | Loss: 0.00002621
Iteration 180/1000 | Loss: 0.00002621
Iteration 181/1000 | Loss: 0.00002621
Iteration 182/1000 | Loss: 0.00002621
Iteration 183/1000 | Loss: 0.00002621
Iteration 184/1000 | Loss: 0.00002621
Iteration 185/1000 | Loss: 0.00002621
Iteration 186/1000 | Loss: 0.00002621
Iteration 187/1000 | Loss: 0.00002621
Iteration 188/1000 | Loss: 0.00002620
Iteration 189/1000 | Loss: 0.00002620
Iteration 190/1000 | Loss: 0.00002620
Iteration 191/1000 | Loss: 0.00002620
Iteration 192/1000 | Loss: 0.00002620
Iteration 193/1000 | Loss: 0.00002620
Iteration 194/1000 | Loss: 0.00002619
Iteration 195/1000 | Loss: 0.00002619
Iteration 196/1000 | Loss: 0.00002619
Iteration 197/1000 | Loss: 0.00002619
Iteration 198/1000 | Loss: 0.00002618
Iteration 199/1000 | Loss: 0.00002618
Iteration 200/1000 | Loss: 0.00002618
Iteration 201/1000 | Loss: 0.00002618
Iteration 202/1000 | Loss: 0.00002618
Iteration 203/1000 | Loss: 0.00002618
Iteration 204/1000 | Loss: 0.00002618
Iteration 205/1000 | Loss: 0.00002618
Iteration 206/1000 | Loss: 0.00002618
Iteration 207/1000 | Loss: 0.00002618
Iteration 208/1000 | Loss: 0.00002618
Iteration 209/1000 | Loss: 0.00002618
Iteration 210/1000 | Loss: 0.00002617
Iteration 211/1000 | Loss: 0.00002617
Iteration 212/1000 | Loss: 0.00002617
Iteration 213/1000 | Loss: 0.00002617
Iteration 214/1000 | Loss: 0.00002617
Iteration 215/1000 | Loss: 0.00002617
Iteration 216/1000 | Loss: 0.00002617
Iteration 217/1000 | Loss: 0.00002617
Iteration 218/1000 | Loss: 0.00002617
Iteration 219/1000 | Loss: 0.00002617
Iteration 220/1000 | Loss: 0.00002617
Iteration 221/1000 | Loss: 0.00002617
Iteration 222/1000 | Loss: 0.00002617
Iteration 223/1000 | Loss: 0.00002617
Iteration 224/1000 | Loss: 0.00002617
Iteration 225/1000 | Loss: 0.00002617
Iteration 226/1000 | Loss: 0.00002617
Iteration 227/1000 | Loss: 0.00002617
Iteration 228/1000 | Loss: 0.00002616
Iteration 229/1000 | Loss: 0.00002616
Iteration 230/1000 | Loss: 0.00002616
Iteration 231/1000 | Loss: 0.00002616
Iteration 232/1000 | Loss: 0.00002616
Iteration 233/1000 | Loss: 0.00002616
Iteration 234/1000 | Loss: 0.00002616
Iteration 235/1000 | Loss: 0.00002616
Iteration 236/1000 | Loss: 0.00002616
Iteration 237/1000 | Loss: 0.00002616
Iteration 238/1000 | Loss: 0.00002615
Iteration 239/1000 | Loss: 0.00002615
Iteration 240/1000 | Loss: 0.00002615
Iteration 241/1000 | Loss: 0.00002615
Iteration 242/1000 | Loss: 0.00002615
Iteration 243/1000 | Loss: 0.00002615
Iteration 244/1000 | Loss: 0.00002615
Iteration 245/1000 | Loss: 0.00002615
Iteration 246/1000 | Loss: 0.00002615
Iteration 247/1000 | Loss: 0.00002615
Iteration 248/1000 | Loss: 0.00002615
Iteration 249/1000 | Loss: 0.00002615
Iteration 250/1000 | Loss: 0.00002615
Iteration 251/1000 | Loss: 0.00002615
Iteration 252/1000 | Loss: 0.00002615
Iteration 253/1000 | Loss: 0.00002615
Iteration 254/1000 | Loss: 0.00002614
Iteration 255/1000 | Loss: 0.00002614
Iteration 256/1000 | Loss: 0.00002614
Iteration 257/1000 | Loss: 0.00002614
Iteration 258/1000 | Loss: 0.00002614
Iteration 259/1000 | Loss: 0.00002614
Iteration 260/1000 | Loss: 0.00002614
Iteration 261/1000 | Loss: 0.00002614
Iteration 262/1000 | Loss: 0.00002614
Iteration 263/1000 | Loss: 0.00002614
Iteration 264/1000 | Loss: 0.00002614
Iteration 265/1000 | Loss: 0.00002614
Iteration 266/1000 | Loss: 0.00002614
Iteration 267/1000 | Loss: 0.00002614
Iteration 268/1000 | Loss: 0.00002614
Iteration 269/1000 | Loss: 0.00002614
Iteration 270/1000 | Loss: 0.00002614
Iteration 271/1000 | Loss: 0.00002614
Iteration 272/1000 | Loss: 0.00002614
Iteration 273/1000 | Loss: 0.00002614
Iteration 274/1000 | Loss: 0.00002614
Iteration 275/1000 | Loss: 0.00002614
Iteration 276/1000 | Loss: 0.00002614
Iteration 277/1000 | Loss: 0.00002614
Iteration 278/1000 | Loss: 0.00002614
Iteration 279/1000 | Loss: 0.00002614
Iteration 280/1000 | Loss: 0.00002614
Iteration 281/1000 | Loss: 0.00002614
Iteration 282/1000 | Loss: 0.00002614
Iteration 283/1000 | Loss: 0.00002614
Iteration 284/1000 | Loss: 0.00002614
Iteration 285/1000 | Loss: 0.00002614
Iteration 286/1000 | Loss: 0.00002614
Iteration 287/1000 | Loss: 0.00002614
Iteration 288/1000 | Loss: 0.00002614
Iteration 289/1000 | Loss: 0.00002614
Iteration 290/1000 | Loss: 0.00002614
Iteration 291/1000 | Loss: 0.00002614
Iteration 292/1000 | Loss: 0.00002614
Iteration 293/1000 | Loss: 0.00002614
Iteration 294/1000 | Loss: 0.00002614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [2.613711512822192e-05, 2.613711512822192e-05, 2.613711512822192e-05, 2.613711512822192e-05, 2.613711512822192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.613711512822192e-05

Optimization complete. Final v2v error: 4.234926223754883 mm

Highest mean error: 4.487809181213379 mm for frame 69

Lowest mean error: 3.981372833251953 mm for frame 10

Saving results

Total time: 49.8006489276886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794720
Iteration 2/25 | Loss: 0.00125586
Iteration 3/25 | Loss: 0.00087053
Iteration 4/25 | Loss: 0.00078380
Iteration 5/25 | Loss: 0.00077187
Iteration 6/25 | Loss: 0.00076848
Iteration 7/25 | Loss: 0.00076719
Iteration 8/25 | Loss: 0.00076708
Iteration 9/25 | Loss: 0.00076708
Iteration 10/25 | Loss: 0.00076708
Iteration 11/25 | Loss: 0.00076708
Iteration 12/25 | Loss: 0.00076708
Iteration 13/25 | Loss: 0.00076708
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007670794147998095, 0.0007670794147998095, 0.0007670794147998095, 0.0007670794147998095, 0.0007670794147998095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007670794147998095

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48812199
Iteration 2/25 | Loss: 0.00048675
Iteration 3/25 | Loss: 0.00048675
Iteration 4/25 | Loss: 0.00048675
Iteration 5/25 | Loss: 0.00048675
Iteration 6/25 | Loss: 0.00048674
Iteration 7/25 | Loss: 0.00048674
Iteration 8/25 | Loss: 0.00048674
Iteration 9/25 | Loss: 0.00048674
Iteration 10/25 | Loss: 0.00048674
Iteration 11/25 | Loss: 0.00048674
Iteration 12/25 | Loss: 0.00048674
Iteration 13/25 | Loss: 0.00048674
Iteration 14/25 | Loss: 0.00048674
Iteration 15/25 | Loss: 0.00048674
Iteration 16/25 | Loss: 0.00048674
Iteration 17/25 | Loss: 0.00048674
Iteration 18/25 | Loss: 0.00048674
Iteration 19/25 | Loss: 0.00048674
Iteration 20/25 | Loss: 0.00048674
Iteration 21/25 | Loss: 0.00048674
Iteration 22/25 | Loss: 0.00048674
Iteration 23/25 | Loss: 0.00048674
Iteration 24/25 | Loss: 0.00048674
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00048674384015612304, 0.00048674384015612304, 0.00048674384015612304, 0.00048674384015612304, 0.00048674384015612304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00048674384015612304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048674
Iteration 2/1000 | Loss: 0.00004858
Iteration 3/1000 | Loss: 0.00003567
Iteration 4/1000 | Loss: 0.00002785
Iteration 5/1000 | Loss: 0.00002626
Iteration 6/1000 | Loss: 0.00002521
Iteration 7/1000 | Loss: 0.00002443
Iteration 8/1000 | Loss: 0.00002404
Iteration 9/1000 | Loss: 0.00002365
Iteration 10/1000 | Loss: 0.00002337
Iteration 11/1000 | Loss: 0.00002316
Iteration 12/1000 | Loss: 0.00002300
Iteration 13/1000 | Loss: 0.00002298
Iteration 14/1000 | Loss: 0.00002284
Iteration 15/1000 | Loss: 0.00002283
Iteration 16/1000 | Loss: 0.00002279
Iteration 17/1000 | Loss: 0.00002279
Iteration 18/1000 | Loss: 0.00002279
Iteration 19/1000 | Loss: 0.00002278
Iteration 20/1000 | Loss: 0.00002278
Iteration 21/1000 | Loss: 0.00002278
Iteration 22/1000 | Loss: 0.00002277
Iteration 23/1000 | Loss: 0.00002276
Iteration 24/1000 | Loss: 0.00002276
Iteration 25/1000 | Loss: 0.00002275
Iteration 26/1000 | Loss: 0.00002275
Iteration 27/1000 | Loss: 0.00002275
Iteration 28/1000 | Loss: 0.00002274
Iteration 29/1000 | Loss: 0.00002274
Iteration 30/1000 | Loss: 0.00002274
Iteration 31/1000 | Loss: 0.00002273
Iteration 32/1000 | Loss: 0.00002273
Iteration 33/1000 | Loss: 0.00002272
Iteration 34/1000 | Loss: 0.00002272
Iteration 35/1000 | Loss: 0.00002272
Iteration 36/1000 | Loss: 0.00002270
Iteration 37/1000 | Loss: 0.00002270
Iteration 38/1000 | Loss: 0.00002268
Iteration 39/1000 | Loss: 0.00002267
Iteration 40/1000 | Loss: 0.00002267
Iteration 41/1000 | Loss: 0.00002267
Iteration 42/1000 | Loss: 0.00002266
Iteration 43/1000 | Loss: 0.00002266
Iteration 44/1000 | Loss: 0.00002265
Iteration 45/1000 | Loss: 0.00002265
Iteration 46/1000 | Loss: 0.00002265
Iteration 47/1000 | Loss: 0.00002264
Iteration 48/1000 | Loss: 0.00002263
Iteration 49/1000 | Loss: 0.00002263
Iteration 50/1000 | Loss: 0.00002263
Iteration 51/1000 | Loss: 0.00002263
Iteration 52/1000 | Loss: 0.00002263
Iteration 53/1000 | Loss: 0.00002263
Iteration 54/1000 | Loss: 0.00002262
Iteration 55/1000 | Loss: 0.00002262
Iteration 56/1000 | Loss: 0.00002261
Iteration 57/1000 | Loss: 0.00002261
Iteration 58/1000 | Loss: 0.00002261
Iteration 59/1000 | Loss: 0.00002260
Iteration 60/1000 | Loss: 0.00002260
Iteration 61/1000 | Loss: 0.00002260
Iteration 62/1000 | Loss: 0.00002260
Iteration 63/1000 | Loss: 0.00002260
Iteration 64/1000 | Loss: 0.00002259
Iteration 65/1000 | Loss: 0.00002259
Iteration 66/1000 | Loss: 0.00002259
Iteration 67/1000 | Loss: 0.00002259
Iteration 68/1000 | Loss: 0.00002258
Iteration 69/1000 | Loss: 0.00002258
Iteration 70/1000 | Loss: 0.00002258
Iteration 71/1000 | Loss: 0.00002258
Iteration 72/1000 | Loss: 0.00002257
Iteration 73/1000 | Loss: 0.00002257
Iteration 74/1000 | Loss: 0.00002257
Iteration 75/1000 | Loss: 0.00002257
Iteration 76/1000 | Loss: 0.00002256
Iteration 77/1000 | Loss: 0.00002256
Iteration 78/1000 | Loss: 0.00002256
Iteration 79/1000 | Loss: 0.00002256
Iteration 80/1000 | Loss: 0.00002255
Iteration 81/1000 | Loss: 0.00002255
Iteration 82/1000 | Loss: 0.00002255
Iteration 83/1000 | Loss: 0.00002255
Iteration 84/1000 | Loss: 0.00002255
Iteration 85/1000 | Loss: 0.00002255
Iteration 86/1000 | Loss: 0.00002255
Iteration 87/1000 | Loss: 0.00002255
Iteration 88/1000 | Loss: 0.00002255
Iteration 89/1000 | Loss: 0.00002255
Iteration 90/1000 | Loss: 0.00002255
Iteration 91/1000 | Loss: 0.00002255
Iteration 92/1000 | Loss: 0.00002255
Iteration 93/1000 | Loss: 0.00002254
Iteration 94/1000 | Loss: 0.00002254
Iteration 95/1000 | Loss: 0.00002254
Iteration 96/1000 | Loss: 0.00002254
Iteration 97/1000 | Loss: 0.00002254
Iteration 98/1000 | Loss: 0.00002254
Iteration 99/1000 | Loss: 0.00002254
Iteration 100/1000 | Loss: 0.00002254
Iteration 101/1000 | Loss: 0.00002254
Iteration 102/1000 | Loss: 0.00002254
Iteration 103/1000 | Loss: 0.00002254
Iteration 104/1000 | Loss: 0.00002254
Iteration 105/1000 | Loss: 0.00002254
Iteration 106/1000 | Loss: 0.00002254
Iteration 107/1000 | Loss: 0.00002253
Iteration 108/1000 | Loss: 0.00002253
Iteration 109/1000 | Loss: 0.00002253
Iteration 110/1000 | Loss: 0.00002253
Iteration 111/1000 | Loss: 0.00002253
Iteration 112/1000 | Loss: 0.00002253
Iteration 113/1000 | Loss: 0.00002253
Iteration 114/1000 | Loss: 0.00002253
Iteration 115/1000 | Loss: 0.00002253
Iteration 116/1000 | Loss: 0.00002253
Iteration 117/1000 | Loss: 0.00002253
Iteration 118/1000 | Loss: 0.00002252
Iteration 119/1000 | Loss: 0.00002252
Iteration 120/1000 | Loss: 0.00002252
Iteration 121/1000 | Loss: 0.00002252
Iteration 122/1000 | Loss: 0.00002252
Iteration 123/1000 | Loss: 0.00002252
Iteration 124/1000 | Loss: 0.00002252
Iteration 125/1000 | Loss: 0.00002252
Iteration 126/1000 | Loss: 0.00002252
Iteration 127/1000 | Loss: 0.00002252
Iteration 128/1000 | Loss: 0.00002252
Iteration 129/1000 | Loss: 0.00002252
Iteration 130/1000 | Loss: 0.00002252
Iteration 131/1000 | Loss: 0.00002252
Iteration 132/1000 | Loss: 0.00002252
Iteration 133/1000 | Loss: 0.00002252
Iteration 134/1000 | Loss: 0.00002252
Iteration 135/1000 | Loss: 0.00002252
Iteration 136/1000 | Loss: 0.00002252
Iteration 137/1000 | Loss: 0.00002252
Iteration 138/1000 | Loss: 0.00002252
Iteration 139/1000 | Loss: 0.00002252
Iteration 140/1000 | Loss: 0.00002252
Iteration 141/1000 | Loss: 0.00002252
Iteration 142/1000 | Loss: 0.00002252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.2520758648170158e-05, 2.2520758648170158e-05, 2.2520758648170158e-05, 2.2520758648170158e-05, 2.2520758648170158e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2520758648170158e-05

Optimization complete. Final v2v error: 3.9217350482940674 mm

Highest mean error: 5.2345685958862305 mm for frame 26

Lowest mean error: 3.3630988597869873 mm for frame 5

Saving results

Total time: 39.798299074172974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406835
Iteration 2/25 | Loss: 0.00083966
Iteration 3/25 | Loss: 0.00065939
Iteration 4/25 | Loss: 0.00063605
Iteration 5/25 | Loss: 0.00062861
Iteration 6/25 | Loss: 0.00062731
Iteration 7/25 | Loss: 0.00062703
Iteration 8/25 | Loss: 0.00062703
Iteration 9/25 | Loss: 0.00062703
Iteration 10/25 | Loss: 0.00062703
Iteration 11/25 | Loss: 0.00062703
Iteration 12/25 | Loss: 0.00062703
Iteration 13/25 | Loss: 0.00062703
Iteration 14/25 | Loss: 0.00062703
Iteration 15/25 | Loss: 0.00062703
Iteration 16/25 | Loss: 0.00062703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006270277663134038, 0.0006270277663134038, 0.0006270277663134038, 0.0006270277663134038, 0.0006270277663134038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006270277663134038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50086606
Iteration 2/25 | Loss: 0.00031631
Iteration 3/25 | Loss: 0.00031631
Iteration 4/25 | Loss: 0.00031631
Iteration 5/25 | Loss: 0.00031631
Iteration 6/25 | Loss: 0.00031631
Iteration 7/25 | Loss: 0.00031631
Iteration 8/25 | Loss: 0.00031631
Iteration 9/25 | Loss: 0.00031631
Iteration 10/25 | Loss: 0.00031631
Iteration 11/25 | Loss: 0.00031631
Iteration 12/25 | Loss: 0.00031631
Iteration 13/25 | Loss: 0.00031631
Iteration 14/25 | Loss: 0.00031631
Iteration 15/25 | Loss: 0.00031631
Iteration 16/25 | Loss: 0.00031631
Iteration 17/25 | Loss: 0.00031631
Iteration 18/25 | Loss: 0.00031631
Iteration 19/25 | Loss: 0.00031631
Iteration 20/25 | Loss: 0.00031631
Iteration 21/25 | Loss: 0.00031631
Iteration 22/25 | Loss: 0.00031631
Iteration 23/25 | Loss: 0.00031631
Iteration 24/25 | Loss: 0.00031631
Iteration 25/25 | Loss: 0.00031631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031631
Iteration 2/1000 | Loss: 0.00002305
Iteration 3/1000 | Loss: 0.00001574
Iteration 4/1000 | Loss: 0.00001484
Iteration 5/1000 | Loss: 0.00001422
Iteration 6/1000 | Loss: 0.00001399
Iteration 7/1000 | Loss: 0.00001374
Iteration 8/1000 | Loss: 0.00001357
Iteration 9/1000 | Loss: 0.00001351
Iteration 10/1000 | Loss: 0.00001342
Iteration 11/1000 | Loss: 0.00001335
Iteration 12/1000 | Loss: 0.00001332
Iteration 13/1000 | Loss: 0.00001332
Iteration 14/1000 | Loss: 0.00001332
Iteration 15/1000 | Loss: 0.00001331
Iteration 16/1000 | Loss: 0.00001331
Iteration 17/1000 | Loss: 0.00001331
Iteration 18/1000 | Loss: 0.00001331
Iteration 19/1000 | Loss: 0.00001331
Iteration 20/1000 | Loss: 0.00001330
Iteration 21/1000 | Loss: 0.00001330
Iteration 22/1000 | Loss: 0.00001330
Iteration 23/1000 | Loss: 0.00001330
Iteration 24/1000 | Loss: 0.00001330
Iteration 25/1000 | Loss: 0.00001330
Iteration 26/1000 | Loss: 0.00001330
Iteration 27/1000 | Loss: 0.00001330
Iteration 28/1000 | Loss: 0.00001330
Iteration 29/1000 | Loss: 0.00001330
Iteration 30/1000 | Loss: 0.00001328
Iteration 31/1000 | Loss: 0.00001328
Iteration 32/1000 | Loss: 0.00001327
Iteration 33/1000 | Loss: 0.00001326
Iteration 34/1000 | Loss: 0.00001326
Iteration 35/1000 | Loss: 0.00001325
Iteration 36/1000 | Loss: 0.00001325
Iteration 37/1000 | Loss: 0.00001325
Iteration 38/1000 | Loss: 0.00001324
Iteration 39/1000 | Loss: 0.00001324
Iteration 40/1000 | Loss: 0.00001324
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001320
Iteration 43/1000 | Loss: 0.00001320
Iteration 44/1000 | Loss: 0.00001320
Iteration 45/1000 | Loss: 0.00001318
Iteration 46/1000 | Loss: 0.00001318
Iteration 47/1000 | Loss: 0.00001317
Iteration 48/1000 | Loss: 0.00001316
Iteration 49/1000 | Loss: 0.00001316
Iteration 50/1000 | Loss: 0.00001316
Iteration 51/1000 | Loss: 0.00001316
Iteration 52/1000 | Loss: 0.00001316
Iteration 53/1000 | Loss: 0.00001316
Iteration 54/1000 | Loss: 0.00001316
Iteration 55/1000 | Loss: 0.00001315
Iteration 56/1000 | Loss: 0.00001315
Iteration 57/1000 | Loss: 0.00001315
Iteration 58/1000 | Loss: 0.00001314
Iteration 59/1000 | Loss: 0.00001314
Iteration 60/1000 | Loss: 0.00001314
Iteration 61/1000 | Loss: 0.00001313
Iteration 62/1000 | Loss: 0.00001313
Iteration 63/1000 | Loss: 0.00001313
Iteration 64/1000 | Loss: 0.00001313
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001312
Iteration 67/1000 | Loss: 0.00001312
Iteration 68/1000 | Loss: 0.00001312
Iteration 69/1000 | Loss: 0.00001312
Iteration 70/1000 | Loss: 0.00001311
Iteration 71/1000 | Loss: 0.00001311
Iteration 72/1000 | Loss: 0.00001311
Iteration 73/1000 | Loss: 0.00001311
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001310
Iteration 76/1000 | Loss: 0.00001310
Iteration 77/1000 | Loss: 0.00001310
Iteration 78/1000 | Loss: 0.00001310
Iteration 79/1000 | Loss: 0.00001309
Iteration 80/1000 | Loss: 0.00001309
Iteration 81/1000 | Loss: 0.00001309
Iteration 82/1000 | Loss: 0.00001309
Iteration 83/1000 | Loss: 0.00001309
Iteration 84/1000 | Loss: 0.00001309
Iteration 85/1000 | Loss: 0.00001309
Iteration 86/1000 | Loss: 0.00001309
Iteration 87/1000 | Loss: 0.00001308
Iteration 88/1000 | Loss: 0.00001308
Iteration 89/1000 | Loss: 0.00001308
Iteration 90/1000 | Loss: 0.00001307
Iteration 91/1000 | Loss: 0.00001307
Iteration 92/1000 | Loss: 0.00001307
Iteration 93/1000 | Loss: 0.00001307
Iteration 94/1000 | Loss: 0.00001307
Iteration 95/1000 | Loss: 0.00001307
Iteration 96/1000 | Loss: 0.00001307
Iteration 97/1000 | Loss: 0.00001307
Iteration 98/1000 | Loss: 0.00001307
Iteration 99/1000 | Loss: 0.00001306
Iteration 100/1000 | Loss: 0.00001306
Iteration 101/1000 | Loss: 0.00001306
Iteration 102/1000 | Loss: 0.00001306
Iteration 103/1000 | Loss: 0.00001306
Iteration 104/1000 | Loss: 0.00001306
Iteration 105/1000 | Loss: 0.00001306
Iteration 106/1000 | Loss: 0.00001306
Iteration 107/1000 | Loss: 0.00001306
Iteration 108/1000 | Loss: 0.00001305
Iteration 109/1000 | Loss: 0.00001305
Iteration 110/1000 | Loss: 0.00001305
Iteration 111/1000 | Loss: 0.00001305
Iteration 112/1000 | Loss: 0.00001305
Iteration 113/1000 | Loss: 0.00001305
Iteration 114/1000 | Loss: 0.00001305
Iteration 115/1000 | Loss: 0.00001305
Iteration 116/1000 | Loss: 0.00001305
Iteration 117/1000 | Loss: 0.00001305
Iteration 118/1000 | Loss: 0.00001304
Iteration 119/1000 | Loss: 0.00001304
Iteration 120/1000 | Loss: 0.00001304
Iteration 121/1000 | Loss: 0.00001304
Iteration 122/1000 | Loss: 0.00001304
Iteration 123/1000 | Loss: 0.00001304
Iteration 124/1000 | Loss: 0.00001303
Iteration 125/1000 | Loss: 0.00001303
Iteration 126/1000 | Loss: 0.00001303
Iteration 127/1000 | Loss: 0.00001303
Iteration 128/1000 | Loss: 0.00001303
Iteration 129/1000 | Loss: 0.00001303
Iteration 130/1000 | Loss: 0.00001303
Iteration 131/1000 | Loss: 0.00001303
Iteration 132/1000 | Loss: 0.00001303
Iteration 133/1000 | Loss: 0.00001303
Iteration 134/1000 | Loss: 0.00001303
Iteration 135/1000 | Loss: 0.00001303
Iteration 136/1000 | Loss: 0.00001303
Iteration 137/1000 | Loss: 0.00001303
Iteration 138/1000 | Loss: 0.00001303
Iteration 139/1000 | Loss: 0.00001303
Iteration 140/1000 | Loss: 0.00001303
Iteration 141/1000 | Loss: 0.00001303
Iteration 142/1000 | Loss: 0.00001303
Iteration 143/1000 | Loss: 0.00001303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.3030025002080947e-05, 1.3030025002080947e-05, 1.3030025002080947e-05, 1.3030025002080947e-05, 1.3030025002080947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3030025002080947e-05

Optimization complete. Final v2v error: 3.070797920227051 mm

Highest mean error: 3.2115797996520996 mm for frame 141

Lowest mean error: 2.9546353816986084 mm for frame 186

Saving results

Total time: 33.27223825454712
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420563
Iteration 2/25 | Loss: 0.00083545
Iteration 3/25 | Loss: 0.00067751
Iteration 4/25 | Loss: 0.00064795
Iteration 5/25 | Loss: 0.00063789
Iteration 6/25 | Loss: 0.00063779
Iteration 7/25 | Loss: 0.00063779
Iteration 8/25 | Loss: 0.00063779
Iteration 9/25 | Loss: 0.00063779
Iteration 10/25 | Loss: 0.00063779
Iteration 11/25 | Loss: 0.00063779
Iteration 12/25 | Loss: 0.00063779
Iteration 13/25 | Loss: 0.00063779
Iteration 14/25 | Loss: 0.00063779
Iteration 15/25 | Loss: 0.00063779
Iteration 16/25 | Loss: 0.00063779
Iteration 17/25 | Loss: 0.00063779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006377891986630857, 0.0006377891986630857, 0.0006377891986630857, 0.0006377891986630857, 0.0006377891986630857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006377891986630857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22919762
Iteration 2/25 | Loss: 0.00030457
Iteration 3/25 | Loss: 0.00030457
Iteration 4/25 | Loss: 0.00030457
Iteration 5/25 | Loss: 0.00030457
Iteration 6/25 | Loss: 0.00030457
Iteration 7/25 | Loss: 0.00030457
Iteration 8/25 | Loss: 0.00030457
Iteration 9/25 | Loss: 0.00030457
Iteration 10/25 | Loss: 0.00030457
Iteration 11/25 | Loss: 0.00030457
Iteration 12/25 | Loss: 0.00030457
Iteration 13/25 | Loss: 0.00030457
Iteration 14/25 | Loss: 0.00030457
Iteration 15/25 | Loss: 0.00030457
Iteration 16/25 | Loss: 0.00030457
Iteration 17/25 | Loss: 0.00030457
Iteration 18/25 | Loss: 0.00030457
Iteration 19/25 | Loss: 0.00030457
Iteration 20/25 | Loss: 0.00030457
Iteration 21/25 | Loss: 0.00030457
Iteration 22/25 | Loss: 0.00030457
Iteration 23/25 | Loss: 0.00030457
Iteration 24/25 | Loss: 0.00030457
Iteration 25/25 | Loss: 0.00030457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030457
Iteration 2/1000 | Loss: 0.00003394
Iteration 3/1000 | Loss: 0.00002113
Iteration 4/1000 | Loss: 0.00001827
Iteration 5/1000 | Loss: 0.00001707
Iteration 6/1000 | Loss: 0.00001657
Iteration 7/1000 | Loss: 0.00001595
Iteration 8/1000 | Loss: 0.00001575
Iteration 9/1000 | Loss: 0.00001575
Iteration 10/1000 | Loss: 0.00001549
Iteration 11/1000 | Loss: 0.00001543
Iteration 12/1000 | Loss: 0.00001536
Iteration 13/1000 | Loss: 0.00001534
Iteration 14/1000 | Loss: 0.00001533
Iteration 15/1000 | Loss: 0.00001519
Iteration 16/1000 | Loss: 0.00001519
Iteration 17/1000 | Loss: 0.00001518
Iteration 18/1000 | Loss: 0.00001518
Iteration 19/1000 | Loss: 0.00001517
Iteration 20/1000 | Loss: 0.00001517
Iteration 21/1000 | Loss: 0.00001515
Iteration 22/1000 | Loss: 0.00001514
Iteration 23/1000 | Loss: 0.00001514
Iteration 24/1000 | Loss: 0.00001514
Iteration 25/1000 | Loss: 0.00001513
Iteration 26/1000 | Loss: 0.00001512
Iteration 27/1000 | Loss: 0.00001511
Iteration 28/1000 | Loss: 0.00001510
Iteration 29/1000 | Loss: 0.00001510
Iteration 30/1000 | Loss: 0.00001510
Iteration 31/1000 | Loss: 0.00001510
Iteration 32/1000 | Loss: 0.00001509
Iteration 33/1000 | Loss: 0.00001509
Iteration 34/1000 | Loss: 0.00001509
Iteration 35/1000 | Loss: 0.00001509
Iteration 36/1000 | Loss: 0.00001508
Iteration 37/1000 | Loss: 0.00001508
Iteration 38/1000 | Loss: 0.00001508
Iteration 39/1000 | Loss: 0.00001508
Iteration 40/1000 | Loss: 0.00001508
Iteration 41/1000 | Loss: 0.00001507
Iteration 42/1000 | Loss: 0.00001507
Iteration 43/1000 | Loss: 0.00001506
Iteration 44/1000 | Loss: 0.00001505
Iteration 45/1000 | Loss: 0.00001505
Iteration 46/1000 | Loss: 0.00001505
Iteration 47/1000 | Loss: 0.00001505
Iteration 48/1000 | Loss: 0.00001505
Iteration 49/1000 | Loss: 0.00001505
Iteration 50/1000 | Loss: 0.00001505
Iteration 51/1000 | Loss: 0.00001505
Iteration 52/1000 | Loss: 0.00001505
Iteration 53/1000 | Loss: 0.00001504
Iteration 54/1000 | Loss: 0.00001504
Iteration 55/1000 | Loss: 0.00001504
Iteration 56/1000 | Loss: 0.00001504
Iteration 57/1000 | Loss: 0.00001504
Iteration 58/1000 | Loss: 0.00001504
Iteration 59/1000 | Loss: 0.00001504
Iteration 60/1000 | Loss: 0.00001504
Iteration 61/1000 | Loss: 0.00001504
Iteration 62/1000 | Loss: 0.00001504
Iteration 63/1000 | Loss: 0.00001503
Iteration 64/1000 | Loss: 0.00001503
Iteration 65/1000 | Loss: 0.00001503
Iteration 66/1000 | Loss: 0.00001502
Iteration 67/1000 | Loss: 0.00001502
Iteration 68/1000 | Loss: 0.00001502
Iteration 69/1000 | Loss: 0.00001502
Iteration 70/1000 | Loss: 0.00001502
Iteration 71/1000 | Loss: 0.00001502
Iteration 72/1000 | Loss: 0.00001501
Iteration 73/1000 | Loss: 0.00001501
Iteration 74/1000 | Loss: 0.00001501
Iteration 75/1000 | Loss: 0.00001501
Iteration 76/1000 | Loss: 0.00001501
Iteration 77/1000 | Loss: 0.00001501
Iteration 78/1000 | Loss: 0.00001501
Iteration 79/1000 | Loss: 0.00001501
Iteration 80/1000 | Loss: 0.00001501
Iteration 81/1000 | Loss: 0.00001500
Iteration 82/1000 | Loss: 0.00001500
Iteration 83/1000 | Loss: 0.00001500
Iteration 84/1000 | Loss: 0.00001500
Iteration 85/1000 | Loss: 0.00001500
Iteration 86/1000 | Loss: 0.00001500
Iteration 87/1000 | Loss: 0.00001500
Iteration 88/1000 | Loss: 0.00001500
Iteration 89/1000 | Loss: 0.00001500
Iteration 90/1000 | Loss: 0.00001500
Iteration 91/1000 | Loss: 0.00001500
Iteration 92/1000 | Loss: 0.00001500
Iteration 93/1000 | Loss: 0.00001500
Iteration 94/1000 | Loss: 0.00001500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.5001419342297595e-05, 1.5001419342297595e-05, 1.5001419342297595e-05, 1.5001419342297595e-05, 1.5001419342297595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5001419342297595e-05

Optimization complete. Final v2v error: 3.354041814804077 mm

Highest mean error: 3.398205280303955 mm for frame 45

Lowest mean error: 3.318697929382324 mm for frame 93

Saving results

Total time: 26.339948654174805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957222
Iteration 2/25 | Loss: 0.00118068
Iteration 3/25 | Loss: 0.00087400
Iteration 4/25 | Loss: 0.00072292
Iteration 5/25 | Loss: 0.00069839
Iteration 6/25 | Loss: 0.00069479
Iteration 7/25 | Loss: 0.00069256
Iteration 8/25 | Loss: 0.00069468
Iteration 9/25 | Loss: 0.00069580
Iteration 10/25 | Loss: 0.00069458
Iteration 11/25 | Loss: 0.00069398
Iteration 12/25 | Loss: 0.00069329
Iteration 13/25 | Loss: 0.00069108
Iteration 14/25 | Loss: 0.00069064
Iteration 15/25 | Loss: 0.00069523
Iteration 16/25 | Loss: 0.00068704
Iteration 17/25 | Loss: 0.00068663
Iteration 18/25 | Loss: 0.00068645
Iteration 19/25 | Loss: 0.00068645
Iteration 20/25 | Loss: 0.00068645
Iteration 21/25 | Loss: 0.00068645
Iteration 22/25 | Loss: 0.00068645
Iteration 23/25 | Loss: 0.00068645
Iteration 24/25 | Loss: 0.00068645
Iteration 25/25 | Loss: 0.00068645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57291234
Iteration 2/25 | Loss: 0.00029873
Iteration 3/25 | Loss: 0.00029870
Iteration 4/25 | Loss: 0.00029870
Iteration 5/25 | Loss: 0.00029870
Iteration 6/25 | Loss: 0.00029870
Iteration 7/25 | Loss: 0.00029870
Iteration 8/25 | Loss: 0.00029870
Iteration 9/25 | Loss: 0.00029870
Iteration 10/25 | Loss: 0.00029870
Iteration 11/25 | Loss: 0.00029870
Iteration 12/25 | Loss: 0.00029870
Iteration 13/25 | Loss: 0.00029870
Iteration 14/25 | Loss: 0.00029870
Iteration 15/25 | Loss: 0.00029870
Iteration 16/25 | Loss: 0.00029870
Iteration 17/25 | Loss: 0.00029870
Iteration 18/25 | Loss: 0.00029870
Iteration 19/25 | Loss: 0.00029870
Iteration 20/25 | Loss: 0.00029870
Iteration 21/25 | Loss: 0.00029870
Iteration 22/25 | Loss: 0.00029870
Iteration 23/25 | Loss: 0.00029870
Iteration 24/25 | Loss: 0.00029870
Iteration 25/25 | Loss: 0.00029870

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029870
Iteration 2/1000 | Loss: 0.00003514
Iteration 3/1000 | Loss: 0.00002604
Iteration 4/1000 | Loss: 0.00002434
Iteration 5/1000 | Loss: 0.00002311
Iteration 6/1000 | Loss: 0.00002209
Iteration 7/1000 | Loss: 0.00002140
Iteration 8/1000 | Loss: 0.00002092
Iteration 9/1000 | Loss: 0.00002058
Iteration 10/1000 | Loss: 0.00002056
Iteration 11/1000 | Loss: 0.00002035
Iteration 12/1000 | Loss: 0.00002018
Iteration 13/1000 | Loss: 0.00002009
Iteration 14/1000 | Loss: 0.00002005
Iteration 15/1000 | Loss: 0.00002004
Iteration 16/1000 | Loss: 0.00002004
Iteration 17/1000 | Loss: 0.00002003
Iteration 18/1000 | Loss: 0.00002000
Iteration 19/1000 | Loss: 0.00002000
Iteration 20/1000 | Loss: 0.00002000
Iteration 21/1000 | Loss: 0.00002000
Iteration 22/1000 | Loss: 0.00002000
Iteration 23/1000 | Loss: 0.00002000
Iteration 24/1000 | Loss: 0.00002000
Iteration 25/1000 | Loss: 0.00001999
Iteration 26/1000 | Loss: 0.00001999
Iteration 27/1000 | Loss: 0.00001999
Iteration 28/1000 | Loss: 0.00001999
Iteration 29/1000 | Loss: 0.00001998
Iteration 30/1000 | Loss: 0.00001997
Iteration 31/1000 | Loss: 0.00001997
Iteration 32/1000 | Loss: 0.00001997
Iteration 33/1000 | Loss: 0.00001996
Iteration 34/1000 | Loss: 0.00001996
Iteration 35/1000 | Loss: 0.00001996
Iteration 36/1000 | Loss: 0.00001996
Iteration 37/1000 | Loss: 0.00001996
Iteration 38/1000 | Loss: 0.00001996
Iteration 39/1000 | Loss: 0.00001995
Iteration 40/1000 | Loss: 0.00001995
Iteration 41/1000 | Loss: 0.00001995
Iteration 42/1000 | Loss: 0.00001995
Iteration 43/1000 | Loss: 0.00001995
Iteration 44/1000 | Loss: 0.00001995
Iteration 45/1000 | Loss: 0.00001995
Iteration 46/1000 | Loss: 0.00001995
Iteration 47/1000 | Loss: 0.00001995
Iteration 48/1000 | Loss: 0.00001995
Iteration 49/1000 | Loss: 0.00001995
Iteration 50/1000 | Loss: 0.00001995
Iteration 51/1000 | Loss: 0.00001994
Iteration 52/1000 | Loss: 0.00001994
Iteration 53/1000 | Loss: 0.00001994
Iteration 54/1000 | Loss: 0.00001994
Iteration 55/1000 | Loss: 0.00001994
Iteration 56/1000 | Loss: 0.00001994
Iteration 57/1000 | Loss: 0.00001994
Iteration 58/1000 | Loss: 0.00001993
Iteration 59/1000 | Loss: 0.00001992
Iteration 60/1000 | Loss: 0.00001992
Iteration 61/1000 | Loss: 0.00001992
Iteration 62/1000 | Loss: 0.00001992
Iteration 63/1000 | Loss: 0.00001991
Iteration 64/1000 | Loss: 0.00001991
Iteration 65/1000 | Loss: 0.00001991
Iteration 66/1000 | Loss: 0.00001991
Iteration 67/1000 | Loss: 0.00001991
Iteration 68/1000 | Loss: 0.00001990
Iteration 69/1000 | Loss: 0.00001990
Iteration 70/1000 | Loss: 0.00001990
Iteration 71/1000 | Loss: 0.00001990
Iteration 72/1000 | Loss: 0.00001990
Iteration 73/1000 | Loss: 0.00001990
Iteration 74/1000 | Loss: 0.00001990
Iteration 75/1000 | Loss: 0.00001989
Iteration 76/1000 | Loss: 0.00001989
Iteration 77/1000 | Loss: 0.00001989
Iteration 78/1000 | Loss: 0.00001989
Iteration 79/1000 | Loss: 0.00001989
Iteration 80/1000 | Loss: 0.00001989
Iteration 81/1000 | Loss: 0.00001989
Iteration 82/1000 | Loss: 0.00001989
Iteration 83/1000 | Loss: 0.00001988
Iteration 84/1000 | Loss: 0.00001988
Iteration 85/1000 | Loss: 0.00001988
Iteration 86/1000 | Loss: 0.00001988
Iteration 87/1000 | Loss: 0.00001988
Iteration 88/1000 | Loss: 0.00001988
Iteration 89/1000 | Loss: 0.00001988
Iteration 90/1000 | Loss: 0.00001988
Iteration 91/1000 | Loss: 0.00001988
Iteration 92/1000 | Loss: 0.00001988
Iteration 93/1000 | Loss: 0.00001988
Iteration 94/1000 | Loss: 0.00001987
Iteration 95/1000 | Loss: 0.00001987
Iteration 96/1000 | Loss: 0.00001987
Iteration 97/1000 | Loss: 0.00001987
Iteration 98/1000 | Loss: 0.00001987
Iteration 99/1000 | Loss: 0.00001987
Iteration 100/1000 | Loss: 0.00001987
Iteration 101/1000 | Loss: 0.00001987
Iteration 102/1000 | Loss: 0.00001986
Iteration 103/1000 | Loss: 0.00001986
Iteration 104/1000 | Loss: 0.00001986
Iteration 105/1000 | Loss: 0.00001986
Iteration 106/1000 | Loss: 0.00001986
Iteration 107/1000 | Loss: 0.00001986
Iteration 108/1000 | Loss: 0.00001986
Iteration 109/1000 | Loss: 0.00001986
Iteration 110/1000 | Loss: 0.00001986
Iteration 111/1000 | Loss: 0.00001986
Iteration 112/1000 | Loss: 0.00001986
Iteration 113/1000 | Loss: 0.00001986
Iteration 114/1000 | Loss: 0.00001986
Iteration 115/1000 | Loss: 0.00001986
Iteration 116/1000 | Loss: 0.00001986
Iteration 117/1000 | Loss: 0.00001986
Iteration 118/1000 | Loss: 0.00001986
Iteration 119/1000 | Loss: 0.00001986
Iteration 120/1000 | Loss: 0.00001986
Iteration 121/1000 | Loss: 0.00001986
Iteration 122/1000 | Loss: 0.00001986
Iteration 123/1000 | Loss: 0.00001986
Iteration 124/1000 | Loss: 0.00001986
Iteration 125/1000 | Loss: 0.00001986
Iteration 126/1000 | Loss: 0.00001986
Iteration 127/1000 | Loss: 0.00001985
Iteration 128/1000 | Loss: 0.00001985
Iteration 129/1000 | Loss: 0.00001985
Iteration 130/1000 | Loss: 0.00001985
Iteration 131/1000 | Loss: 0.00001985
Iteration 132/1000 | Loss: 0.00001985
Iteration 133/1000 | Loss: 0.00001985
Iteration 134/1000 | Loss: 0.00001985
Iteration 135/1000 | Loss: 0.00001985
Iteration 136/1000 | Loss: 0.00001985
Iteration 137/1000 | Loss: 0.00001985
Iteration 138/1000 | Loss: 0.00001985
Iteration 139/1000 | Loss: 0.00001985
Iteration 140/1000 | Loss: 0.00001985
Iteration 141/1000 | Loss: 0.00001985
Iteration 142/1000 | Loss: 0.00001985
Iteration 143/1000 | Loss: 0.00001985
Iteration 144/1000 | Loss: 0.00001985
Iteration 145/1000 | Loss: 0.00001985
Iteration 146/1000 | Loss: 0.00001985
Iteration 147/1000 | Loss: 0.00001985
Iteration 148/1000 | Loss: 0.00001985
Iteration 149/1000 | Loss: 0.00001985
Iteration 150/1000 | Loss: 0.00001985
Iteration 151/1000 | Loss: 0.00001985
Iteration 152/1000 | Loss: 0.00001985
Iteration 153/1000 | Loss: 0.00001985
Iteration 154/1000 | Loss: 0.00001985
Iteration 155/1000 | Loss: 0.00001985
Iteration 156/1000 | Loss: 0.00001985
Iteration 157/1000 | Loss: 0.00001985
Iteration 158/1000 | Loss: 0.00001985
Iteration 159/1000 | Loss: 0.00001985
Iteration 160/1000 | Loss: 0.00001985
Iteration 161/1000 | Loss: 0.00001985
Iteration 162/1000 | Loss: 0.00001985
Iteration 163/1000 | Loss: 0.00001985
Iteration 164/1000 | Loss: 0.00001985
Iteration 165/1000 | Loss: 0.00001985
Iteration 166/1000 | Loss: 0.00001985
Iteration 167/1000 | Loss: 0.00001985
Iteration 168/1000 | Loss: 0.00001985
Iteration 169/1000 | Loss: 0.00001985
Iteration 170/1000 | Loss: 0.00001985
Iteration 171/1000 | Loss: 0.00001985
Iteration 172/1000 | Loss: 0.00001985
Iteration 173/1000 | Loss: 0.00001985
Iteration 174/1000 | Loss: 0.00001985
Iteration 175/1000 | Loss: 0.00001985
Iteration 176/1000 | Loss: 0.00001985
Iteration 177/1000 | Loss: 0.00001985
Iteration 178/1000 | Loss: 0.00001985
Iteration 179/1000 | Loss: 0.00001985
Iteration 180/1000 | Loss: 0.00001985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.98521538550267e-05, 1.98521538550267e-05, 1.98521538550267e-05, 1.98521538550267e-05, 1.98521538550267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.98521538550267e-05

Optimization complete. Final v2v error: 3.770296812057495 mm

Highest mean error: 4.7863078117370605 mm for frame 39

Lowest mean error: 3.2856738567352295 mm for frame 121

Saving results

Total time: 57.51902484893799
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00615073
Iteration 2/25 | Loss: 0.00074799
Iteration 3/25 | Loss: 0.00063316
Iteration 4/25 | Loss: 0.00061434
Iteration 5/25 | Loss: 0.00060767
Iteration 6/25 | Loss: 0.00060581
Iteration 7/25 | Loss: 0.00060534
Iteration 8/25 | Loss: 0.00060534
Iteration 9/25 | Loss: 0.00060534
Iteration 10/25 | Loss: 0.00060534
Iteration 11/25 | Loss: 0.00060534
Iteration 12/25 | Loss: 0.00060534
Iteration 13/25 | Loss: 0.00060534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006053413962945342, 0.0006053413962945342, 0.0006053413962945342, 0.0006053413962945342, 0.0006053413962945342]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006053413962945342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.16746330
Iteration 2/25 | Loss: 0.00026379
Iteration 3/25 | Loss: 0.00026378
Iteration 4/25 | Loss: 0.00026377
Iteration 5/25 | Loss: 0.00026377
Iteration 6/25 | Loss: 0.00026377
Iteration 7/25 | Loss: 0.00026377
Iteration 8/25 | Loss: 0.00026377
Iteration 9/25 | Loss: 0.00026377
Iteration 10/25 | Loss: 0.00026377
Iteration 11/25 | Loss: 0.00026377
Iteration 12/25 | Loss: 0.00026377
Iteration 13/25 | Loss: 0.00026377
Iteration 14/25 | Loss: 0.00026377
Iteration 15/25 | Loss: 0.00026377
Iteration 16/25 | Loss: 0.00026377
Iteration 17/25 | Loss: 0.00026377
Iteration 18/25 | Loss: 0.00026377
Iteration 19/25 | Loss: 0.00026377
Iteration 20/25 | Loss: 0.00026377
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002637718862388283, 0.0002637718862388283, 0.0002637718862388283, 0.0002637718862388283, 0.0002637718862388283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002637718862388283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026377
Iteration 2/1000 | Loss: 0.00002177
Iteration 3/1000 | Loss: 0.00001370
Iteration 4/1000 | Loss: 0.00001249
Iteration 5/1000 | Loss: 0.00001184
Iteration 6/1000 | Loss: 0.00001147
Iteration 7/1000 | Loss: 0.00001126
Iteration 8/1000 | Loss: 0.00001113
Iteration 9/1000 | Loss: 0.00001113
Iteration 10/1000 | Loss: 0.00001111
Iteration 11/1000 | Loss: 0.00001105
Iteration 12/1000 | Loss: 0.00001105
Iteration 13/1000 | Loss: 0.00001105
Iteration 14/1000 | Loss: 0.00001105
Iteration 15/1000 | Loss: 0.00001104
Iteration 16/1000 | Loss: 0.00001103
Iteration 17/1000 | Loss: 0.00001101
Iteration 18/1000 | Loss: 0.00001099
Iteration 19/1000 | Loss: 0.00001098
Iteration 20/1000 | Loss: 0.00001097
Iteration 21/1000 | Loss: 0.00001094
Iteration 22/1000 | Loss: 0.00001093
Iteration 23/1000 | Loss: 0.00001093
Iteration 24/1000 | Loss: 0.00001093
Iteration 25/1000 | Loss: 0.00001092
Iteration 26/1000 | Loss: 0.00001089
Iteration 27/1000 | Loss: 0.00001089
Iteration 28/1000 | Loss: 0.00001088
Iteration 29/1000 | Loss: 0.00001088
Iteration 30/1000 | Loss: 0.00001088
Iteration 31/1000 | Loss: 0.00001087
Iteration 32/1000 | Loss: 0.00001087
Iteration 33/1000 | Loss: 0.00001086
Iteration 34/1000 | Loss: 0.00001085
Iteration 35/1000 | Loss: 0.00001085
Iteration 36/1000 | Loss: 0.00001084
Iteration 37/1000 | Loss: 0.00001084
Iteration 38/1000 | Loss: 0.00001084
Iteration 39/1000 | Loss: 0.00001084
Iteration 40/1000 | Loss: 0.00001083
Iteration 41/1000 | Loss: 0.00001082
Iteration 42/1000 | Loss: 0.00001082
Iteration 43/1000 | Loss: 0.00001081
Iteration 44/1000 | Loss: 0.00001080
Iteration 45/1000 | Loss: 0.00001079
Iteration 46/1000 | Loss: 0.00001079
Iteration 47/1000 | Loss: 0.00001079
Iteration 48/1000 | Loss: 0.00001078
Iteration 49/1000 | Loss: 0.00001078
Iteration 50/1000 | Loss: 0.00001077
Iteration 51/1000 | Loss: 0.00001077
Iteration 52/1000 | Loss: 0.00001077
Iteration 53/1000 | Loss: 0.00001077
Iteration 54/1000 | Loss: 0.00001076
Iteration 55/1000 | Loss: 0.00001076
Iteration 56/1000 | Loss: 0.00001076
Iteration 57/1000 | Loss: 0.00001075
Iteration 58/1000 | Loss: 0.00001075
Iteration 59/1000 | Loss: 0.00001074
Iteration 60/1000 | Loss: 0.00001074
Iteration 61/1000 | Loss: 0.00001074
Iteration 62/1000 | Loss: 0.00001073
Iteration 63/1000 | Loss: 0.00001073
Iteration 64/1000 | Loss: 0.00001073
Iteration 65/1000 | Loss: 0.00001073
Iteration 66/1000 | Loss: 0.00001073
Iteration 67/1000 | Loss: 0.00001073
Iteration 68/1000 | Loss: 0.00001073
Iteration 69/1000 | Loss: 0.00001073
Iteration 70/1000 | Loss: 0.00001072
Iteration 71/1000 | Loss: 0.00001072
Iteration 72/1000 | Loss: 0.00001072
Iteration 73/1000 | Loss: 0.00001072
Iteration 74/1000 | Loss: 0.00001071
Iteration 75/1000 | Loss: 0.00001071
Iteration 76/1000 | Loss: 0.00001071
Iteration 77/1000 | Loss: 0.00001071
Iteration 78/1000 | Loss: 0.00001070
Iteration 79/1000 | Loss: 0.00001070
Iteration 80/1000 | Loss: 0.00001070
Iteration 81/1000 | Loss: 0.00001070
Iteration 82/1000 | Loss: 0.00001070
Iteration 83/1000 | Loss: 0.00001070
Iteration 84/1000 | Loss: 0.00001070
Iteration 85/1000 | Loss: 0.00001070
Iteration 86/1000 | Loss: 0.00001070
Iteration 87/1000 | Loss: 0.00001070
Iteration 88/1000 | Loss: 0.00001070
Iteration 89/1000 | Loss: 0.00001070
Iteration 90/1000 | Loss: 0.00001070
Iteration 91/1000 | Loss: 0.00001069
Iteration 92/1000 | Loss: 0.00001069
Iteration 93/1000 | Loss: 0.00001069
Iteration 94/1000 | Loss: 0.00001069
Iteration 95/1000 | Loss: 0.00001069
Iteration 96/1000 | Loss: 0.00001069
Iteration 97/1000 | Loss: 0.00001069
Iteration 98/1000 | Loss: 0.00001069
Iteration 99/1000 | Loss: 0.00001069
Iteration 100/1000 | Loss: 0.00001069
Iteration 101/1000 | Loss: 0.00001069
Iteration 102/1000 | Loss: 0.00001069
Iteration 103/1000 | Loss: 0.00001069
Iteration 104/1000 | Loss: 0.00001069
Iteration 105/1000 | Loss: 0.00001069
Iteration 106/1000 | Loss: 0.00001069
Iteration 107/1000 | Loss: 0.00001069
Iteration 108/1000 | Loss: 0.00001069
Iteration 109/1000 | Loss: 0.00001069
Iteration 110/1000 | Loss: 0.00001069
Iteration 111/1000 | Loss: 0.00001069
Iteration 112/1000 | Loss: 0.00001069
Iteration 113/1000 | Loss: 0.00001069
Iteration 114/1000 | Loss: 0.00001069
Iteration 115/1000 | Loss: 0.00001069
Iteration 116/1000 | Loss: 0.00001069
Iteration 117/1000 | Loss: 0.00001069
Iteration 118/1000 | Loss: 0.00001069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.0693421245377976e-05, 1.0693421245377976e-05, 1.0693421245377976e-05, 1.0693421245377976e-05, 1.0693421245377976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0693421245377976e-05

Optimization complete. Final v2v error: 2.8038787841796875 mm

Highest mean error: 3.145174503326416 mm for frame 2

Lowest mean error: 2.5902059078216553 mm for frame 43

Saving results

Total time: 31.501227140426636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863079
Iteration 2/25 | Loss: 0.00076110
Iteration 3/25 | Loss: 0.00065498
Iteration 4/25 | Loss: 0.00063409
Iteration 5/25 | Loss: 0.00062912
Iteration 6/25 | Loss: 0.00062834
Iteration 7/25 | Loss: 0.00062834
Iteration 8/25 | Loss: 0.00062834
Iteration 9/25 | Loss: 0.00062834
Iteration 10/25 | Loss: 0.00062834
Iteration 11/25 | Loss: 0.00062834
Iteration 12/25 | Loss: 0.00062834
Iteration 13/25 | Loss: 0.00062834
Iteration 14/25 | Loss: 0.00062834
Iteration 15/25 | Loss: 0.00062834
Iteration 16/25 | Loss: 0.00062834
Iteration 17/25 | Loss: 0.00062834
Iteration 18/25 | Loss: 0.00062834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006283363327383995, 0.0006283363327383995, 0.0006283363327383995, 0.0006283363327383995, 0.0006283363327383995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006283363327383995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46963990
Iteration 2/25 | Loss: 0.00022386
Iteration 3/25 | Loss: 0.00022386
Iteration 4/25 | Loss: 0.00022385
Iteration 5/25 | Loss: 0.00022385
Iteration 6/25 | Loss: 0.00022385
Iteration 7/25 | Loss: 0.00022385
Iteration 8/25 | Loss: 0.00022385
Iteration 9/25 | Loss: 0.00022385
Iteration 10/25 | Loss: 0.00022385
Iteration 11/25 | Loss: 0.00022385
Iteration 12/25 | Loss: 0.00022385
Iteration 13/25 | Loss: 0.00022385
Iteration 14/25 | Loss: 0.00022385
Iteration 15/25 | Loss: 0.00022385
Iteration 16/25 | Loss: 0.00022385
Iteration 17/25 | Loss: 0.00022385
Iteration 18/25 | Loss: 0.00022385
Iteration 19/25 | Loss: 0.00022385
Iteration 20/25 | Loss: 0.00022385
Iteration 21/25 | Loss: 0.00022385
Iteration 22/25 | Loss: 0.00022385
Iteration 23/25 | Loss: 0.00022385
Iteration 24/25 | Loss: 0.00022385
Iteration 25/25 | Loss: 0.00022385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022385
Iteration 2/1000 | Loss: 0.00003063
Iteration 3/1000 | Loss: 0.00002277
Iteration 4/1000 | Loss: 0.00002040
Iteration 5/1000 | Loss: 0.00001915
Iteration 6/1000 | Loss: 0.00001844
Iteration 7/1000 | Loss: 0.00001794
Iteration 8/1000 | Loss: 0.00001736
Iteration 9/1000 | Loss: 0.00001715
Iteration 10/1000 | Loss: 0.00001698
Iteration 11/1000 | Loss: 0.00001695
Iteration 12/1000 | Loss: 0.00001686
Iteration 13/1000 | Loss: 0.00001682
Iteration 14/1000 | Loss: 0.00001679
Iteration 15/1000 | Loss: 0.00001677
Iteration 16/1000 | Loss: 0.00001677
Iteration 17/1000 | Loss: 0.00001677
Iteration 18/1000 | Loss: 0.00001674
Iteration 19/1000 | Loss: 0.00001673
Iteration 20/1000 | Loss: 0.00001673
Iteration 21/1000 | Loss: 0.00001672
Iteration 22/1000 | Loss: 0.00001672
Iteration 23/1000 | Loss: 0.00001671
Iteration 24/1000 | Loss: 0.00001670
Iteration 25/1000 | Loss: 0.00001670
Iteration 26/1000 | Loss: 0.00001669
Iteration 27/1000 | Loss: 0.00001669
Iteration 28/1000 | Loss: 0.00001669
Iteration 29/1000 | Loss: 0.00001669
Iteration 30/1000 | Loss: 0.00001668
Iteration 31/1000 | Loss: 0.00001668
Iteration 32/1000 | Loss: 0.00001668
Iteration 33/1000 | Loss: 0.00001667
Iteration 34/1000 | Loss: 0.00001667
Iteration 35/1000 | Loss: 0.00001667
Iteration 36/1000 | Loss: 0.00001666
Iteration 37/1000 | Loss: 0.00001666
Iteration 38/1000 | Loss: 0.00001666
Iteration 39/1000 | Loss: 0.00001665
Iteration 40/1000 | Loss: 0.00001665
Iteration 41/1000 | Loss: 0.00001665
Iteration 42/1000 | Loss: 0.00001665
Iteration 43/1000 | Loss: 0.00001665
Iteration 44/1000 | Loss: 0.00001665
Iteration 45/1000 | Loss: 0.00001665
Iteration 46/1000 | Loss: 0.00001665
Iteration 47/1000 | Loss: 0.00001665
Iteration 48/1000 | Loss: 0.00001664
Iteration 49/1000 | Loss: 0.00001664
Iteration 50/1000 | Loss: 0.00001664
Iteration 51/1000 | Loss: 0.00001663
Iteration 52/1000 | Loss: 0.00001663
Iteration 53/1000 | Loss: 0.00001663
Iteration 54/1000 | Loss: 0.00001663
Iteration 55/1000 | Loss: 0.00001663
Iteration 56/1000 | Loss: 0.00001662
Iteration 57/1000 | Loss: 0.00001662
Iteration 58/1000 | Loss: 0.00001662
Iteration 59/1000 | Loss: 0.00001661
Iteration 60/1000 | Loss: 0.00001661
Iteration 61/1000 | Loss: 0.00001660
Iteration 62/1000 | Loss: 0.00001660
Iteration 63/1000 | Loss: 0.00001660
Iteration 64/1000 | Loss: 0.00001660
Iteration 65/1000 | Loss: 0.00001660
Iteration 66/1000 | Loss: 0.00001660
Iteration 67/1000 | Loss: 0.00001660
Iteration 68/1000 | Loss: 0.00001659
Iteration 69/1000 | Loss: 0.00001659
Iteration 70/1000 | Loss: 0.00001659
Iteration 71/1000 | Loss: 0.00001659
Iteration 72/1000 | Loss: 0.00001658
Iteration 73/1000 | Loss: 0.00001658
Iteration 74/1000 | Loss: 0.00001658
Iteration 75/1000 | Loss: 0.00001658
Iteration 76/1000 | Loss: 0.00001658
Iteration 77/1000 | Loss: 0.00001657
Iteration 78/1000 | Loss: 0.00001657
Iteration 79/1000 | Loss: 0.00001657
Iteration 80/1000 | Loss: 0.00001657
Iteration 81/1000 | Loss: 0.00001657
Iteration 82/1000 | Loss: 0.00001656
Iteration 83/1000 | Loss: 0.00001656
Iteration 84/1000 | Loss: 0.00001656
Iteration 85/1000 | Loss: 0.00001656
Iteration 86/1000 | Loss: 0.00001656
Iteration 87/1000 | Loss: 0.00001656
Iteration 88/1000 | Loss: 0.00001655
Iteration 89/1000 | Loss: 0.00001655
Iteration 90/1000 | Loss: 0.00001655
Iteration 91/1000 | Loss: 0.00001654
Iteration 92/1000 | Loss: 0.00001654
Iteration 93/1000 | Loss: 0.00001654
Iteration 94/1000 | Loss: 0.00001653
Iteration 95/1000 | Loss: 0.00001653
Iteration 96/1000 | Loss: 0.00001652
Iteration 97/1000 | Loss: 0.00001652
Iteration 98/1000 | Loss: 0.00001651
Iteration 99/1000 | Loss: 0.00001651
Iteration 100/1000 | Loss: 0.00001651
Iteration 101/1000 | Loss: 0.00001651
Iteration 102/1000 | Loss: 0.00001651
Iteration 103/1000 | Loss: 0.00001651
Iteration 104/1000 | Loss: 0.00001651
Iteration 105/1000 | Loss: 0.00001650
Iteration 106/1000 | Loss: 0.00001650
Iteration 107/1000 | Loss: 0.00001649
Iteration 108/1000 | Loss: 0.00001649
Iteration 109/1000 | Loss: 0.00001649
Iteration 110/1000 | Loss: 0.00001649
Iteration 111/1000 | Loss: 0.00001649
Iteration 112/1000 | Loss: 0.00001649
Iteration 113/1000 | Loss: 0.00001649
Iteration 114/1000 | Loss: 0.00001649
Iteration 115/1000 | Loss: 0.00001649
Iteration 116/1000 | Loss: 0.00001649
Iteration 117/1000 | Loss: 0.00001648
Iteration 118/1000 | Loss: 0.00001648
Iteration 119/1000 | Loss: 0.00001648
Iteration 120/1000 | Loss: 0.00001648
Iteration 121/1000 | Loss: 0.00001647
Iteration 122/1000 | Loss: 0.00001647
Iteration 123/1000 | Loss: 0.00001647
Iteration 124/1000 | Loss: 0.00001647
Iteration 125/1000 | Loss: 0.00001647
Iteration 126/1000 | Loss: 0.00001647
Iteration 127/1000 | Loss: 0.00001647
Iteration 128/1000 | Loss: 0.00001646
Iteration 129/1000 | Loss: 0.00001646
Iteration 130/1000 | Loss: 0.00001646
Iteration 131/1000 | Loss: 0.00001646
Iteration 132/1000 | Loss: 0.00001646
Iteration 133/1000 | Loss: 0.00001646
Iteration 134/1000 | Loss: 0.00001646
Iteration 135/1000 | Loss: 0.00001646
Iteration 136/1000 | Loss: 0.00001646
Iteration 137/1000 | Loss: 0.00001646
Iteration 138/1000 | Loss: 0.00001646
Iteration 139/1000 | Loss: 0.00001646
Iteration 140/1000 | Loss: 0.00001646
Iteration 141/1000 | Loss: 0.00001645
Iteration 142/1000 | Loss: 0.00001645
Iteration 143/1000 | Loss: 0.00001645
Iteration 144/1000 | Loss: 0.00001645
Iteration 145/1000 | Loss: 0.00001645
Iteration 146/1000 | Loss: 0.00001645
Iteration 147/1000 | Loss: 0.00001645
Iteration 148/1000 | Loss: 0.00001645
Iteration 149/1000 | Loss: 0.00001645
Iteration 150/1000 | Loss: 0.00001645
Iteration 151/1000 | Loss: 0.00001645
Iteration 152/1000 | Loss: 0.00001645
Iteration 153/1000 | Loss: 0.00001645
Iteration 154/1000 | Loss: 0.00001645
Iteration 155/1000 | Loss: 0.00001645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.6454128854093142e-05, 1.6454128854093142e-05, 1.6454128854093142e-05, 1.6454128854093142e-05, 1.6454128854093142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6454128854093142e-05

Optimization complete. Final v2v error: 3.3923146724700928 mm

Highest mean error: 3.944988250732422 mm for frame 97

Lowest mean error: 3.2729930877685547 mm for frame 43

Saving results

Total time: 36.998191595077515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828504
Iteration 2/25 | Loss: 0.00151754
Iteration 3/25 | Loss: 0.00086482
Iteration 4/25 | Loss: 0.00070838
Iteration 5/25 | Loss: 0.00069432
Iteration 6/25 | Loss: 0.00069271
Iteration 7/25 | Loss: 0.00069241
Iteration 8/25 | Loss: 0.00069241
Iteration 9/25 | Loss: 0.00069241
Iteration 10/25 | Loss: 0.00069240
Iteration 11/25 | Loss: 0.00069240
Iteration 12/25 | Loss: 0.00069240
Iteration 13/25 | Loss: 0.00069240
Iteration 14/25 | Loss: 0.00069240
Iteration 15/25 | Loss: 0.00069240
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006923998007550836, 0.0006923998007550836, 0.0006923998007550836, 0.0006923998007550836, 0.0006923998007550836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006923998007550836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35386240
Iteration 2/25 | Loss: 0.00028783
Iteration 3/25 | Loss: 0.00028780
Iteration 4/25 | Loss: 0.00028780
Iteration 5/25 | Loss: 0.00028780
Iteration 6/25 | Loss: 0.00028780
Iteration 7/25 | Loss: 0.00028780
Iteration 8/25 | Loss: 0.00028780
Iteration 9/25 | Loss: 0.00028780
Iteration 10/25 | Loss: 0.00028780
Iteration 11/25 | Loss: 0.00028780
Iteration 12/25 | Loss: 0.00028780
Iteration 13/25 | Loss: 0.00028780
Iteration 14/25 | Loss: 0.00028780
Iteration 15/25 | Loss: 0.00028780
Iteration 16/25 | Loss: 0.00028780
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0002878002414945513, 0.0002878002414945513, 0.0002878002414945513, 0.0002878002414945513, 0.0002878002414945513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002878002414945513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028780
Iteration 2/1000 | Loss: 0.00002997
Iteration 3/1000 | Loss: 0.00002225
Iteration 4/1000 | Loss: 0.00001956
Iteration 5/1000 | Loss: 0.00001821
Iteration 6/1000 | Loss: 0.00001761
Iteration 7/1000 | Loss: 0.00001722
Iteration 8/1000 | Loss: 0.00001691
Iteration 9/1000 | Loss: 0.00001660
Iteration 10/1000 | Loss: 0.00001651
Iteration 11/1000 | Loss: 0.00001630
Iteration 12/1000 | Loss: 0.00001627
Iteration 13/1000 | Loss: 0.00001615
Iteration 14/1000 | Loss: 0.00001609
Iteration 15/1000 | Loss: 0.00001602
Iteration 16/1000 | Loss: 0.00001602
Iteration 17/1000 | Loss: 0.00001602
Iteration 18/1000 | Loss: 0.00001602
Iteration 19/1000 | Loss: 0.00001601
Iteration 20/1000 | Loss: 0.00001601
Iteration 21/1000 | Loss: 0.00001600
Iteration 22/1000 | Loss: 0.00001599
Iteration 23/1000 | Loss: 0.00001598
Iteration 24/1000 | Loss: 0.00001598
Iteration 25/1000 | Loss: 0.00001598
Iteration 26/1000 | Loss: 0.00001597
Iteration 27/1000 | Loss: 0.00001597
Iteration 28/1000 | Loss: 0.00001597
Iteration 29/1000 | Loss: 0.00001597
Iteration 30/1000 | Loss: 0.00001597
Iteration 31/1000 | Loss: 0.00001597
Iteration 32/1000 | Loss: 0.00001597
Iteration 33/1000 | Loss: 0.00001596
Iteration 34/1000 | Loss: 0.00001596
Iteration 35/1000 | Loss: 0.00001595
Iteration 36/1000 | Loss: 0.00001595
Iteration 37/1000 | Loss: 0.00001595
Iteration 38/1000 | Loss: 0.00001595
Iteration 39/1000 | Loss: 0.00001595
Iteration 40/1000 | Loss: 0.00001595
Iteration 41/1000 | Loss: 0.00001594
Iteration 42/1000 | Loss: 0.00001594
Iteration 43/1000 | Loss: 0.00001594
Iteration 44/1000 | Loss: 0.00001594
Iteration 45/1000 | Loss: 0.00001594
Iteration 46/1000 | Loss: 0.00001594
Iteration 47/1000 | Loss: 0.00001594
Iteration 48/1000 | Loss: 0.00001594
Iteration 49/1000 | Loss: 0.00001593
Iteration 50/1000 | Loss: 0.00001593
Iteration 51/1000 | Loss: 0.00001593
Iteration 52/1000 | Loss: 0.00001593
Iteration 53/1000 | Loss: 0.00001593
Iteration 54/1000 | Loss: 0.00001593
Iteration 55/1000 | Loss: 0.00001593
Iteration 56/1000 | Loss: 0.00001593
Iteration 57/1000 | Loss: 0.00001593
Iteration 58/1000 | Loss: 0.00001593
Iteration 59/1000 | Loss: 0.00001593
Iteration 60/1000 | Loss: 0.00001593
Iteration 61/1000 | Loss: 0.00001593
Iteration 62/1000 | Loss: 0.00001593
Iteration 63/1000 | Loss: 0.00001593
Iteration 64/1000 | Loss: 0.00001593
Iteration 65/1000 | Loss: 0.00001593
Iteration 66/1000 | Loss: 0.00001593
Iteration 67/1000 | Loss: 0.00001593
Iteration 68/1000 | Loss: 0.00001593
Iteration 69/1000 | Loss: 0.00001593
Iteration 70/1000 | Loss: 0.00001593
Iteration 71/1000 | Loss: 0.00001593
Iteration 72/1000 | Loss: 0.00001593
Iteration 73/1000 | Loss: 0.00001593
Iteration 74/1000 | Loss: 0.00001593
Iteration 75/1000 | Loss: 0.00001593
Iteration 76/1000 | Loss: 0.00001593
Iteration 77/1000 | Loss: 0.00001593
Iteration 78/1000 | Loss: 0.00001593
Iteration 79/1000 | Loss: 0.00001593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.5925936168059707e-05, 1.5925936168059707e-05, 1.5925936168059707e-05, 1.5925936168059707e-05, 1.5925936168059707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5925936168059707e-05

Optimization complete. Final v2v error: 3.349783420562744 mm

Highest mean error: 3.69553279876709 mm for frame 117

Lowest mean error: 3.1151390075683594 mm for frame 43

Saving results

Total time: 31.41829562187195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01097081
Iteration 2/25 | Loss: 0.00674608
Iteration 3/25 | Loss: 0.00389363
Iteration 4/25 | Loss: 0.00464712
Iteration 5/25 | Loss: 0.00206147
Iteration 6/25 | Loss: 0.00178210
Iteration 7/25 | Loss: 0.00225740
Iteration 8/25 | Loss: 0.00160632
Iteration 9/25 | Loss: 0.00115204
Iteration 10/25 | Loss: 0.00105645
Iteration 11/25 | Loss: 0.00103039
Iteration 12/25 | Loss: 0.00100766
Iteration 13/25 | Loss: 0.00101766
Iteration 14/25 | Loss: 0.00101668
Iteration 15/25 | Loss: 0.00100403
Iteration 16/25 | Loss: 0.00101774
Iteration 17/25 | Loss: 0.00097469
Iteration 18/25 | Loss: 0.00099570
Iteration 19/25 | Loss: 0.00097228
Iteration 20/25 | Loss: 0.00096993
Iteration 21/25 | Loss: 0.00095981
Iteration 22/25 | Loss: 0.00096014
Iteration 23/25 | Loss: 0.00095818
Iteration 24/25 | Loss: 0.00095798
Iteration 25/25 | Loss: 0.00095792

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.44709000
Iteration 2/25 | Loss: 0.00034404
Iteration 3/25 | Loss: 0.00031091
Iteration 4/25 | Loss: 0.00031090
Iteration 5/25 | Loss: 0.00031090
Iteration 6/25 | Loss: 0.00031090
Iteration 7/25 | Loss: 0.00031090
Iteration 8/25 | Loss: 0.00031090
Iteration 9/25 | Loss: 0.00031090
Iteration 10/25 | Loss: 0.00031090
Iteration 11/25 | Loss: 0.00031090
Iteration 12/25 | Loss: 0.00031090
Iteration 13/25 | Loss: 0.00031090
Iteration 14/25 | Loss: 0.00031090
Iteration 15/25 | Loss: 0.00031090
Iteration 16/25 | Loss: 0.00031090
Iteration 17/25 | Loss: 0.00031090
Iteration 18/25 | Loss: 0.00031090
Iteration 19/25 | Loss: 0.00031090
Iteration 20/25 | Loss: 0.00031090
Iteration 21/25 | Loss: 0.00031090
Iteration 22/25 | Loss: 0.00031090
Iteration 23/25 | Loss: 0.00031090
Iteration 24/25 | Loss: 0.00031090
Iteration 25/25 | Loss: 0.00031090

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031090
Iteration 2/1000 | Loss: 0.00010138
Iteration 3/1000 | Loss: 0.00034750
Iteration 4/1000 | Loss: 0.00011536
Iteration 5/1000 | Loss: 0.00004407
Iteration 6/1000 | Loss: 0.00003209
Iteration 7/1000 | Loss: 0.00003092
Iteration 8/1000 | Loss: 0.00003021
Iteration 9/1000 | Loss: 0.00051652
Iteration 10/1000 | Loss: 0.00016808
Iteration 11/1000 | Loss: 0.00003121
Iteration 12/1000 | Loss: 0.00009217
Iteration 13/1000 | Loss: 0.00003303
Iteration 14/1000 | Loss: 0.00002944
Iteration 15/1000 | Loss: 0.00009421
Iteration 16/1000 | Loss: 0.00002874
Iteration 17/1000 | Loss: 0.00002763
Iteration 18/1000 | Loss: 0.00002842
Iteration 19/1000 | Loss: 0.00002714
Iteration 20/1000 | Loss: 0.00002700
Iteration 21/1000 | Loss: 0.00002689
Iteration 22/1000 | Loss: 0.00002681
Iteration 23/1000 | Loss: 0.00002680
Iteration 24/1000 | Loss: 0.00002679
Iteration 25/1000 | Loss: 0.00002679
Iteration 26/1000 | Loss: 0.00002678
Iteration 27/1000 | Loss: 0.00002678
Iteration 28/1000 | Loss: 0.00002678
Iteration 29/1000 | Loss: 0.00002678
Iteration 30/1000 | Loss: 0.00002678
Iteration 31/1000 | Loss: 0.00002678
Iteration 32/1000 | Loss: 0.00002678
Iteration 33/1000 | Loss: 0.00002678
Iteration 34/1000 | Loss: 0.00002678
Iteration 35/1000 | Loss: 0.00002677
Iteration 36/1000 | Loss: 0.00002677
Iteration 37/1000 | Loss: 0.00011710
Iteration 38/1000 | Loss: 0.00003073
Iteration 39/1000 | Loss: 0.00003030
Iteration 40/1000 | Loss: 0.00002746
Iteration 41/1000 | Loss: 0.00002673
Iteration 42/1000 | Loss: 0.00002662
Iteration 43/1000 | Loss: 0.00003338
Iteration 44/1000 | Loss: 0.00002663
Iteration 45/1000 | Loss: 0.00002661
Iteration 46/1000 | Loss: 0.00002660
Iteration 47/1000 | Loss: 0.00002660
Iteration 48/1000 | Loss: 0.00002660
Iteration 49/1000 | Loss: 0.00002659
Iteration 50/1000 | Loss: 0.00002659
Iteration 51/1000 | Loss: 0.00002659
Iteration 52/1000 | Loss: 0.00002659
Iteration 53/1000 | Loss: 0.00002659
Iteration 54/1000 | Loss: 0.00002659
Iteration 55/1000 | Loss: 0.00002659
Iteration 56/1000 | Loss: 0.00002659
Iteration 57/1000 | Loss: 0.00002659
Iteration 58/1000 | Loss: 0.00002659
Iteration 59/1000 | Loss: 0.00002659
Iteration 60/1000 | Loss: 0.00002659
Iteration 61/1000 | Loss: 0.00002659
Iteration 62/1000 | Loss: 0.00002658
Iteration 63/1000 | Loss: 0.00002658
Iteration 64/1000 | Loss: 0.00002658
Iteration 65/1000 | Loss: 0.00002658
Iteration 66/1000 | Loss: 0.00002658
Iteration 67/1000 | Loss: 0.00002658
Iteration 68/1000 | Loss: 0.00002658
Iteration 69/1000 | Loss: 0.00002658
Iteration 70/1000 | Loss: 0.00002658
Iteration 71/1000 | Loss: 0.00002658
Iteration 72/1000 | Loss: 0.00002658
Iteration 73/1000 | Loss: 0.00002658
Iteration 74/1000 | Loss: 0.00002658
Iteration 75/1000 | Loss: 0.00002658
Iteration 76/1000 | Loss: 0.00002658
Iteration 77/1000 | Loss: 0.00002658
Iteration 78/1000 | Loss: 0.00002658
Iteration 79/1000 | Loss: 0.00002658
Iteration 80/1000 | Loss: 0.00002658
Iteration 81/1000 | Loss: 0.00002658
Iteration 82/1000 | Loss: 0.00002658
Iteration 83/1000 | Loss: 0.00002658
Iteration 84/1000 | Loss: 0.00002658
Iteration 85/1000 | Loss: 0.00002658
Iteration 86/1000 | Loss: 0.00002658
Iteration 87/1000 | Loss: 0.00002658
Iteration 88/1000 | Loss: 0.00002658
Iteration 89/1000 | Loss: 0.00002658
Iteration 90/1000 | Loss: 0.00002658
Iteration 91/1000 | Loss: 0.00002658
Iteration 92/1000 | Loss: 0.00002658
Iteration 93/1000 | Loss: 0.00002658
Iteration 94/1000 | Loss: 0.00002658
Iteration 95/1000 | Loss: 0.00002658
Iteration 96/1000 | Loss: 0.00002658
Iteration 97/1000 | Loss: 0.00002658
Iteration 98/1000 | Loss: 0.00002658
Iteration 99/1000 | Loss: 0.00002658
Iteration 100/1000 | Loss: 0.00002658
Iteration 101/1000 | Loss: 0.00002658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.6579964469419792e-05, 2.6579964469419792e-05, 2.6579964469419792e-05, 2.6579964469419792e-05, 2.6579964469419792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6579964469419792e-05

Optimization complete. Final v2v error: 4.216962814331055 mm

Highest mean error: 4.7384724617004395 mm for frame 34

Lowest mean error: 3.987427234649658 mm for frame 30

Saving results

Total time: 86.80925130844116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383746
Iteration 2/25 | Loss: 0.00074993
Iteration 3/25 | Loss: 0.00064387
Iteration 4/25 | Loss: 0.00062401
Iteration 5/25 | Loss: 0.00061384
Iteration 6/25 | Loss: 0.00061272
Iteration 7/25 | Loss: 0.00061272
Iteration 8/25 | Loss: 0.00061272
Iteration 9/25 | Loss: 0.00061272
Iteration 10/25 | Loss: 0.00061272
Iteration 11/25 | Loss: 0.00061272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006127189844846725, 0.0006127189844846725, 0.0006127189844846725, 0.0006127189844846725, 0.0006127189844846725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006127189844846725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46580815
Iteration 2/25 | Loss: 0.00027306
Iteration 3/25 | Loss: 0.00027306
Iteration 4/25 | Loss: 0.00027306
Iteration 5/25 | Loss: 0.00027306
Iteration 6/25 | Loss: 0.00027306
Iteration 7/25 | Loss: 0.00027306
Iteration 8/25 | Loss: 0.00027306
Iteration 9/25 | Loss: 0.00027306
Iteration 10/25 | Loss: 0.00027306
Iteration 11/25 | Loss: 0.00027306
Iteration 12/25 | Loss: 0.00027306
Iteration 13/25 | Loss: 0.00027306
Iteration 14/25 | Loss: 0.00027306
Iteration 15/25 | Loss: 0.00027306
Iteration 16/25 | Loss: 0.00027306
Iteration 17/25 | Loss: 0.00027306
Iteration 18/25 | Loss: 0.00027306
Iteration 19/25 | Loss: 0.00027306
Iteration 20/25 | Loss: 0.00027306
Iteration 21/25 | Loss: 0.00027306
Iteration 22/25 | Loss: 0.00027306
Iteration 23/25 | Loss: 0.00027306
Iteration 24/25 | Loss: 0.00027306
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00027305851108394563, 0.00027305851108394563, 0.00027305851108394563, 0.00027305851108394563, 0.00027305851108394563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027305851108394563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027306
Iteration 2/1000 | Loss: 0.00002343
Iteration 3/1000 | Loss: 0.00001606
Iteration 4/1000 | Loss: 0.00001502
Iteration 5/1000 | Loss: 0.00001418
Iteration 6/1000 | Loss: 0.00001373
Iteration 7/1000 | Loss: 0.00001347
Iteration 8/1000 | Loss: 0.00001341
Iteration 9/1000 | Loss: 0.00001335
Iteration 10/1000 | Loss: 0.00001334
Iteration 11/1000 | Loss: 0.00001327
Iteration 12/1000 | Loss: 0.00001325
Iteration 13/1000 | Loss: 0.00001324
Iteration 14/1000 | Loss: 0.00001324
Iteration 15/1000 | Loss: 0.00001315
Iteration 16/1000 | Loss: 0.00001311
Iteration 17/1000 | Loss: 0.00001311
Iteration 18/1000 | Loss: 0.00001309
Iteration 19/1000 | Loss: 0.00001303
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001300
Iteration 22/1000 | Loss: 0.00001300
Iteration 23/1000 | Loss: 0.00001297
Iteration 24/1000 | Loss: 0.00001296
Iteration 25/1000 | Loss: 0.00001296
Iteration 26/1000 | Loss: 0.00001295
Iteration 27/1000 | Loss: 0.00001294
Iteration 28/1000 | Loss: 0.00001294
Iteration 29/1000 | Loss: 0.00001293
Iteration 30/1000 | Loss: 0.00001293
Iteration 31/1000 | Loss: 0.00001292
Iteration 32/1000 | Loss: 0.00001291
Iteration 33/1000 | Loss: 0.00001290
Iteration 34/1000 | Loss: 0.00001290
Iteration 35/1000 | Loss: 0.00001290
Iteration 36/1000 | Loss: 0.00001289
Iteration 37/1000 | Loss: 0.00001289
Iteration 38/1000 | Loss: 0.00001288
Iteration 39/1000 | Loss: 0.00001288
Iteration 40/1000 | Loss: 0.00001287
Iteration 41/1000 | Loss: 0.00001287
Iteration 42/1000 | Loss: 0.00001287
Iteration 43/1000 | Loss: 0.00001287
Iteration 44/1000 | Loss: 0.00001287
Iteration 45/1000 | Loss: 0.00001287
Iteration 46/1000 | Loss: 0.00001286
Iteration 47/1000 | Loss: 0.00001286
Iteration 48/1000 | Loss: 0.00001286
Iteration 49/1000 | Loss: 0.00001286
Iteration 50/1000 | Loss: 0.00001286
Iteration 51/1000 | Loss: 0.00001286
Iteration 52/1000 | Loss: 0.00001285
Iteration 53/1000 | Loss: 0.00001285
Iteration 54/1000 | Loss: 0.00001284
Iteration 55/1000 | Loss: 0.00001284
Iteration 56/1000 | Loss: 0.00001284
Iteration 57/1000 | Loss: 0.00001283
Iteration 58/1000 | Loss: 0.00001283
Iteration 59/1000 | Loss: 0.00001283
Iteration 60/1000 | Loss: 0.00001283
Iteration 61/1000 | Loss: 0.00001282
Iteration 62/1000 | Loss: 0.00001281
Iteration 63/1000 | Loss: 0.00001281
Iteration 64/1000 | Loss: 0.00001281
Iteration 65/1000 | Loss: 0.00001281
Iteration 66/1000 | Loss: 0.00001281
Iteration 67/1000 | Loss: 0.00001281
Iteration 68/1000 | Loss: 0.00001281
Iteration 69/1000 | Loss: 0.00001280
Iteration 70/1000 | Loss: 0.00001280
Iteration 71/1000 | Loss: 0.00001280
Iteration 72/1000 | Loss: 0.00001280
Iteration 73/1000 | Loss: 0.00001278
Iteration 74/1000 | Loss: 0.00001278
Iteration 75/1000 | Loss: 0.00001278
Iteration 76/1000 | Loss: 0.00001278
Iteration 77/1000 | Loss: 0.00001278
Iteration 78/1000 | Loss: 0.00001278
Iteration 79/1000 | Loss: 0.00001278
Iteration 80/1000 | Loss: 0.00001278
Iteration 81/1000 | Loss: 0.00001278
Iteration 82/1000 | Loss: 0.00001277
Iteration 83/1000 | Loss: 0.00001277
Iteration 84/1000 | Loss: 0.00001277
Iteration 85/1000 | Loss: 0.00001277
Iteration 86/1000 | Loss: 0.00001276
Iteration 87/1000 | Loss: 0.00001276
Iteration 88/1000 | Loss: 0.00001274
Iteration 89/1000 | Loss: 0.00001274
Iteration 90/1000 | Loss: 0.00001273
Iteration 91/1000 | Loss: 0.00001273
Iteration 92/1000 | Loss: 0.00001273
Iteration 93/1000 | Loss: 0.00001273
Iteration 94/1000 | Loss: 0.00001273
Iteration 95/1000 | Loss: 0.00001273
Iteration 96/1000 | Loss: 0.00001273
Iteration 97/1000 | Loss: 0.00001271
Iteration 98/1000 | Loss: 0.00001271
Iteration 99/1000 | Loss: 0.00001271
Iteration 100/1000 | Loss: 0.00001271
Iteration 101/1000 | Loss: 0.00001270
Iteration 102/1000 | Loss: 0.00001270
Iteration 103/1000 | Loss: 0.00001270
Iteration 104/1000 | Loss: 0.00001270
Iteration 105/1000 | Loss: 0.00001269
Iteration 106/1000 | Loss: 0.00001269
Iteration 107/1000 | Loss: 0.00001269
Iteration 108/1000 | Loss: 0.00001269
Iteration 109/1000 | Loss: 0.00001269
Iteration 110/1000 | Loss: 0.00001269
Iteration 111/1000 | Loss: 0.00001269
Iteration 112/1000 | Loss: 0.00001269
Iteration 113/1000 | Loss: 0.00001269
Iteration 114/1000 | Loss: 0.00001269
Iteration 115/1000 | Loss: 0.00001269
Iteration 116/1000 | Loss: 0.00001269
Iteration 117/1000 | Loss: 0.00001269
Iteration 118/1000 | Loss: 0.00001269
Iteration 119/1000 | Loss: 0.00001269
Iteration 120/1000 | Loss: 0.00001269
Iteration 121/1000 | Loss: 0.00001269
Iteration 122/1000 | Loss: 0.00001269
Iteration 123/1000 | Loss: 0.00001269
Iteration 124/1000 | Loss: 0.00001269
Iteration 125/1000 | Loss: 0.00001269
Iteration 126/1000 | Loss: 0.00001269
Iteration 127/1000 | Loss: 0.00001269
Iteration 128/1000 | Loss: 0.00001269
Iteration 129/1000 | Loss: 0.00001269
Iteration 130/1000 | Loss: 0.00001269
Iteration 131/1000 | Loss: 0.00001269
Iteration 132/1000 | Loss: 0.00001269
Iteration 133/1000 | Loss: 0.00001269
Iteration 134/1000 | Loss: 0.00001269
Iteration 135/1000 | Loss: 0.00001269
Iteration 136/1000 | Loss: 0.00001269
Iteration 137/1000 | Loss: 0.00001269
Iteration 138/1000 | Loss: 0.00001269
Iteration 139/1000 | Loss: 0.00001269
Iteration 140/1000 | Loss: 0.00001269
Iteration 141/1000 | Loss: 0.00001269
Iteration 142/1000 | Loss: 0.00001269
Iteration 143/1000 | Loss: 0.00001269
Iteration 144/1000 | Loss: 0.00001269
Iteration 145/1000 | Loss: 0.00001269
Iteration 146/1000 | Loss: 0.00001269
Iteration 147/1000 | Loss: 0.00001269
Iteration 148/1000 | Loss: 0.00001269
Iteration 149/1000 | Loss: 0.00001269
Iteration 150/1000 | Loss: 0.00001269
Iteration 151/1000 | Loss: 0.00001269
Iteration 152/1000 | Loss: 0.00001269
Iteration 153/1000 | Loss: 0.00001269
Iteration 154/1000 | Loss: 0.00001269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.2685105502896477e-05, 1.2685105502896477e-05, 1.2685105502896477e-05, 1.2685105502896477e-05, 1.2685105502896477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2685105502896477e-05

Optimization complete. Final v2v error: 3.043548345565796 mm

Highest mean error: 3.1843113899230957 mm for frame 38

Lowest mean error: 2.917820930480957 mm for frame 0

Saving results

Total time: 32.73208212852478
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00720254
Iteration 2/25 | Loss: 0.00111913
Iteration 3/25 | Loss: 0.00076334
Iteration 4/25 | Loss: 0.00069888
Iteration 5/25 | Loss: 0.00068261
Iteration 6/25 | Loss: 0.00068055
Iteration 7/25 | Loss: 0.00068049
Iteration 8/25 | Loss: 0.00068049
Iteration 9/25 | Loss: 0.00068049
Iteration 10/25 | Loss: 0.00068049
Iteration 11/25 | Loss: 0.00068049
Iteration 12/25 | Loss: 0.00068049
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006804937729611993, 0.0006804937729611993, 0.0006804937729611993, 0.0006804937729611993, 0.0006804937729611993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006804937729611993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42485881
Iteration 2/25 | Loss: 0.00020916
Iteration 3/25 | Loss: 0.00020914
Iteration 4/25 | Loss: 0.00020914
Iteration 5/25 | Loss: 0.00020914
Iteration 6/25 | Loss: 0.00020913
Iteration 7/25 | Loss: 0.00020913
Iteration 8/25 | Loss: 0.00020913
Iteration 9/25 | Loss: 0.00020913
Iteration 10/25 | Loss: 0.00020913
Iteration 11/25 | Loss: 0.00020913
Iteration 12/25 | Loss: 0.00020913
Iteration 13/25 | Loss: 0.00020913
Iteration 14/25 | Loss: 0.00020913
Iteration 15/25 | Loss: 0.00020913
Iteration 16/25 | Loss: 0.00020913
Iteration 17/25 | Loss: 0.00020913
Iteration 18/25 | Loss: 0.00020913
Iteration 19/25 | Loss: 0.00020913
Iteration 20/25 | Loss: 0.00020913
Iteration 21/25 | Loss: 0.00020913
Iteration 22/25 | Loss: 0.00020913
Iteration 23/25 | Loss: 0.00020913
Iteration 24/25 | Loss: 0.00020913
Iteration 25/25 | Loss: 0.00020913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020913
Iteration 2/1000 | Loss: 0.00002519
Iteration 3/1000 | Loss: 0.00001996
Iteration 4/1000 | Loss: 0.00001897
Iteration 5/1000 | Loss: 0.00001817
Iteration 6/1000 | Loss: 0.00001788
Iteration 7/1000 | Loss: 0.00001749
Iteration 8/1000 | Loss: 0.00001721
Iteration 9/1000 | Loss: 0.00001703
Iteration 10/1000 | Loss: 0.00001700
Iteration 11/1000 | Loss: 0.00001690
Iteration 12/1000 | Loss: 0.00001681
Iteration 13/1000 | Loss: 0.00001663
Iteration 14/1000 | Loss: 0.00001647
Iteration 15/1000 | Loss: 0.00001641
Iteration 16/1000 | Loss: 0.00001638
Iteration 17/1000 | Loss: 0.00001636
Iteration 18/1000 | Loss: 0.00001633
Iteration 19/1000 | Loss: 0.00001628
Iteration 20/1000 | Loss: 0.00001628
Iteration 21/1000 | Loss: 0.00001627
Iteration 22/1000 | Loss: 0.00001626
Iteration 23/1000 | Loss: 0.00001626
Iteration 24/1000 | Loss: 0.00001626
Iteration 25/1000 | Loss: 0.00001625
Iteration 26/1000 | Loss: 0.00001625
Iteration 27/1000 | Loss: 0.00001623
Iteration 28/1000 | Loss: 0.00001623
Iteration 29/1000 | Loss: 0.00001623
Iteration 30/1000 | Loss: 0.00001623
Iteration 31/1000 | Loss: 0.00001623
Iteration 32/1000 | Loss: 0.00001622
Iteration 33/1000 | Loss: 0.00001621
Iteration 34/1000 | Loss: 0.00001621
Iteration 35/1000 | Loss: 0.00001621
Iteration 36/1000 | Loss: 0.00001620
Iteration 37/1000 | Loss: 0.00001620
Iteration 38/1000 | Loss: 0.00001620
Iteration 39/1000 | Loss: 0.00001620
Iteration 40/1000 | Loss: 0.00001620
Iteration 41/1000 | Loss: 0.00001620
Iteration 42/1000 | Loss: 0.00001620
Iteration 43/1000 | Loss: 0.00001620
Iteration 44/1000 | Loss: 0.00001620
Iteration 45/1000 | Loss: 0.00001620
Iteration 46/1000 | Loss: 0.00001620
Iteration 47/1000 | Loss: 0.00001620
Iteration 48/1000 | Loss: 0.00001620
Iteration 49/1000 | Loss: 0.00001620
Iteration 50/1000 | Loss: 0.00001620
Iteration 51/1000 | Loss: 0.00001620
Iteration 52/1000 | Loss: 0.00001619
Iteration 53/1000 | Loss: 0.00001619
Iteration 54/1000 | Loss: 0.00001619
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001619
Iteration 57/1000 | Loss: 0.00001619
Iteration 58/1000 | Loss: 0.00001619
Iteration 59/1000 | Loss: 0.00001619
Iteration 60/1000 | Loss: 0.00001619
Iteration 61/1000 | Loss: 0.00001619
Iteration 62/1000 | Loss: 0.00001619
Iteration 63/1000 | Loss: 0.00001619
Iteration 64/1000 | Loss: 0.00001619
Iteration 65/1000 | Loss: 0.00001619
Iteration 66/1000 | Loss: 0.00001619
Iteration 67/1000 | Loss: 0.00001619
Iteration 68/1000 | Loss: 0.00001619
Iteration 69/1000 | Loss: 0.00001619
Iteration 70/1000 | Loss: 0.00001619
Iteration 71/1000 | Loss: 0.00001619
Iteration 72/1000 | Loss: 0.00001619
Iteration 73/1000 | Loss: 0.00001619
Iteration 74/1000 | Loss: 0.00001619
Iteration 75/1000 | Loss: 0.00001619
Iteration 76/1000 | Loss: 0.00001619
Iteration 77/1000 | Loss: 0.00001619
Iteration 78/1000 | Loss: 0.00001619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.619360591575969e-05, 1.619360591575969e-05, 1.619360591575969e-05, 1.619360591575969e-05, 1.619360591575969e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.619360591575969e-05

Optimization complete. Final v2v error: 3.4718503952026367 mm

Highest mean error: 3.6811163425445557 mm for frame 3

Lowest mean error: 3.2386553287506104 mm for frame 131

Saving results

Total time: 34.693286657333374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789317
Iteration 2/25 | Loss: 0.00110086
Iteration 3/25 | Loss: 0.00071261
Iteration 4/25 | Loss: 0.00067674
Iteration 5/25 | Loss: 0.00066896
Iteration 6/25 | Loss: 0.00066762
Iteration 7/25 | Loss: 0.00066762
Iteration 8/25 | Loss: 0.00066762
Iteration 9/25 | Loss: 0.00066762
Iteration 10/25 | Loss: 0.00066762
Iteration 11/25 | Loss: 0.00066762
Iteration 12/25 | Loss: 0.00066762
Iteration 13/25 | Loss: 0.00066762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006676228949800134, 0.0006676228949800134, 0.0006676228949800134, 0.0006676228949800134, 0.0006676228949800134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006676228949800134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49657273
Iteration 2/25 | Loss: 0.00029094
Iteration 3/25 | Loss: 0.00029093
Iteration 4/25 | Loss: 0.00029093
Iteration 5/25 | Loss: 0.00029093
Iteration 6/25 | Loss: 0.00029093
Iteration 7/25 | Loss: 0.00029093
Iteration 8/25 | Loss: 0.00029093
Iteration 9/25 | Loss: 0.00029093
Iteration 10/25 | Loss: 0.00029093
Iteration 11/25 | Loss: 0.00029093
Iteration 12/25 | Loss: 0.00029093
Iteration 13/25 | Loss: 0.00029093
Iteration 14/25 | Loss: 0.00029093
Iteration 15/25 | Loss: 0.00029093
Iteration 16/25 | Loss: 0.00029093
Iteration 17/25 | Loss: 0.00029093
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00029092704062350094, 0.00029092704062350094, 0.00029092704062350094, 0.00029092704062350094, 0.00029092704062350094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029092704062350094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029093
Iteration 2/1000 | Loss: 0.00003500
Iteration 3/1000 | Loss: 0.00002663
Iteration 4/1000 | Loss: 0.00002363
Iteration 5/1000 | Loss: 0.00002186
Iteration 6/1000 | Loss: 0.00002074
Iteration 7/1000 | Loss: 0.00002005
Iteration 8/1000 | Loss: 0.00001956
Iteration 9/1000 | Loss: 0.00001912
Iteration 10/1000 | Loss: 0.00001909
Iteration 11/1000 | Loss: 0.00001887
Iteration 12/1000 | Loss: 0.00001867
Iteration 13/1000 | Loss: 0.00001867
Iteration 14/1000 | Loss: 0.00001863
Iteration 15/1000 | Loss: 0.00001856
Iteration 16/1000 | Loss: 0.00001854
Iteration 17/1000 | Loss: 0.00001852
Iteration 18/1000 | Loss: 0.00001851
Iteration 19/1000 | Loss: 0.00001851
Iteration 20/1000 | Loss: 0.00001851
Iteration 21/1000 | Loss: 0.00001845
Iteration 22/1000 | Loss: 0.00001841
Iteration 23/1000 | Loss: 0.00001841
Iteration 24/1000 | Loss: 0.00001841
Iteration 25/1000 | Loss: 0.00001840
Iteration 26/1000 | Loss: 0.00001840
Iteration 27/1000 | Loss: 0.00001839
Iteration 28/1000 | Loss: 0.00001838
Iteration 29/1000 | Loss: 0.00001838
Iteration 30/1000 | Loss: 0.00001837
Iteration 31/1000 | Loss: 0.00001837
Iteration 32/1000 | Loss: 0.00001836
Iteration 33/1000 | Loss: 0.00001834
Iteration 34/1000 | Loss: 0.00001834
Iteration 35/1000 | Loss: 0.00001834
Iteration 36/1000 | Loss: 0.00001834
Iteration 37/1000 | Loss: 0.00001833
Iteration 38/1000 | Loss: 0.00001832
Iteration 39/1000 | Loss: 0.00001832
Iteration 40/1000 | Loss: 0.00001831
Iteration 41/1000 | Loss: 0.00001830
Iteration 42/1000 | Loss: 0.00001830
Iteration 43/1000 | Loss: 0.00001829
Iteration 44/1000 | Loss: 0.00001829
Iteration 45/1000 | Loss: 0.00001828
Iteration 46/1000 | Loss: 0.00001828
Iteration 47/1000 | Loss: 0.00001827
Iteration 48/1000 | Loss: 0.00001826
Iteration 49/1000 | Loss: 0.00001825
Iteration 50/1000 | Loss: 0.00001825
Iteration 51/1000 | Loss: 0.00001824
Iteration 52/1000 | Loss: 0.00001824
Iteration 53/1000 | Loss: 0.00001823
Iteration 54/1000 | Loss: 0.00001823
Iteration 55/1000 | Loss: 0.00001822
Iteration 56/1000 | Loss: 0.00001822
Iteration 57/1000 | Loss: 0.00001822
Iteration 58/1000 | Loss: 0.00001821
Iteration 59/1000 | Loss: 0.00001821
Iteration 60/1000 | Loss: 0.00001821
Iteration 61/1000 | Loss: 0.00001820
Iteration 62/1000 | Loss: 0.00001820
Iteration 63/1000 | Loss: 0.00001820
Iteration 64/1000 | Loss: 0.00001820
Iteration 65/1000 | Loss: 0.00001819
Iteration 66/1000 | Loss: 0.00001819
Iteration 67/1000 | Loss: 0.00001819
Iteration 68/1000 | Loss: 0.00001819
Iteration 69/1000 | Loss: 0.00001818
Iteration 70/1000 | Loss: 0.00001818
Iteration 71/1000 | Loss: 0.00001818
Iteration 72/1000 | Loss: 0.00001818
Iteration 73/1000 | Loss: 0.00001818
Iteration 74/1000 | Loss: 0.00001818
Iteration 75/1000 | Loss: 0.00001817
Iteration 76/1000 | Loss: 0.00001817
Iteration 77/1000 | Loss: 0.00001817
Iteration 78/1000 | Loss: 0.00001816
Iteration 79/1000 | Loss: 0.00001816
Iteration 80/1000 | Loss: 0.00001816
Iteration 81/1000 | Loss: 0.00001816
Iteration 82/1000 | Loss: 0.00001816
Iteration 83/1000 | Loss: 0.00001815
Iteration 84/1000 | Loss: 0.00001815
Iteration 85/1000 | Loss: 0.00001815
Iteration 86/1000 | Loss: 0.00001815
Iteration 87/1000 | Loss: 0.00001815
Iteration 88/1000 | Loss: 0.00001814
Iteration 89/1000 | Loss: 0.00001814
Iteration 90/1000 | Loss: 0.00001814
Iteration 91/1000 | Loss: 0.00001814
Iteration 92/1000 | Loss: 0.00001814
Iteration 93/1000 | Loss: 0.00001814
Iteration 94/1000 | Loss: 0.00001813
Iteration 95/1000 | Loss: 0.00001813
Iteration 96/1000 | Loss: 0.00001813
Iteration 97/1000 | Loss: 0.00001813
Iteration 98/1000 | Loss: 0.00001813
Iteration 99/1000 | Loss: 0.00001813
Iteration 100/1000 | Loss: 0.00001813
Iteration 101/1000 | Loss: 0.00001813
Iteration 102/1000 | Loss: 0.00001813
Iteration 103/1000 | Loss: 0.00001813
Iteration 104/1000 | Loss: 0.00001813
Iteration 105/1000 | Loss: 0.00001813
Iteration 106/1000 | Loss: 0.00001813
Iteration 107/1000 | Loss: 0.00001813
Iteration 108/1000 | Loss: 0.00001813
Iteration 109/1000 | Loss: 0.00001813
Iteration 110/1000 | Loss: 0.00001813
Iteration 111/1000 | Loss: 0.00001812
Iteration 112/1000 | Loss: 0.00001812
Iteration 113/1000 | Loss: 0.00001812
Iteration 114/1000 | Loss: 0.00001812
Iteration 115/1000 | Loss: 0.00001812
Iteration 116/1000 | Loss: 0.00001812
Iteration 117/1000 | Loss: 0.00001812
Iteration 118/1000 | Loss: 0.00001812
Iteration 119/1000 | Loss: 0.00001812
Iteration 120/1000 | Loss: 0.00001812
Iteration 121/1000 | Loss: 0.00001812
Iteration 122/1000 | Loss: 0.00001811
Iteration 123/1000 | Loss: 0.00001811
Iteration 124/1000 | Loss: 0.00001811
Iteration 125/1000 | Loss: 0.00001811
Iteration 126/1000 | Loss: 0.00001811
Iteration 127/1000 | Loss: 0.00001811
Iteration 128/1000 | Loss: 0.00001811
Iteration 129/1000 | Loss: 0.00001811
Iteration 130/1000 | Loss: 0.00001811
Iteration 131/1000 | Loss: 0.00001810
Iteration 132/1000 | Loss: 0.00001810
Iteration 133/1000 | Loss: 0.00001810
Iteration 134/1000 | Loss: 0.00001810
Iteration 135/1000 | Loss: 0.00001810
Iteration 136/1000 | Loss: 0.00001810
Iteration 137/1000 | Loss: 0.00001810
Iteration 138/1000 | Loss: 0.00001810
Iteration 139/1000 | Loss: 0.00001810
Iteration 140/1000 | Loss: 0.00001810
Iteration 141/1000 | Loss: 0.00001810
Iteration 142/1000 | Loss: 0.00001810
Iteration 143/1000 | Loss: 0.00001810
Iteration 144/1000 | Loss: 0.00001810
Iteration 145/1000 | Loss: 0.00001810
Iteration 146/1000 | Loss: 0.00001810
Iteration 147/1000 | Loss: 0.00001810
Iteration 148/1000 | Loss: 0.00001809
Iteration 149/1000 | Loss: 0.00001809
Iteration 150/1000 | Loss: 0.00001809
Iteration 151/1000 | Loss: 0.00001809
Iteration 152/1000 | Loss: 0.00001809
Iteration 153/1000 | Loss: 0.00001809
Iteration 154/1000 | Loss: 0.00001809
Iteration 155/1000 | Loss: 0.00001809
Iteration 156/1000 | Loss: 0.00001809
Iteration 157/1000 | Loss: 0.00001809
Iteration 158/1000 | Loss: 0.00001809
Iteration 159/1000 | Loss: 0.00001809
Iteration 160/1000 | Loss: 0.00001809
Iteration 161/1000 | Loss: 0.00001809
Iteration 162/1000 | Loss: 0.00001809
Iteration 163/1000 | Loss: 0.00001809
Iteration 164/1000 | Loss: 0.00001808
Iteration 165/1000 | Loss: 0.00001808
Iteration 166/1000 | Loss: 0.00001808
Iteration 167/1000 | Loss: 0.00001808
Iteration 168/1000 | Loss: 0.00001808
Iteration 169/1000 | Loss: 0.00001808
Iteration 170/1000 | Loss: 0.00001808
Iteration 171/1000 | Loss: 0.00001808
Iteration 172/1000 | Loss: 0.00001808
Iteration 173/1000 | Loss: 0.00001808
Iteration 174/1000 | Loss: 0.00001808
Iteration 175/1000 | Loss: 0.00001808
Iteration 176/1000 | Loss: 0.00001808
Iteration 177/1000 | Loss: 0.00001808
Iteration 178/1000 | Loss: 0.00001808
Iteration 179/1000 | Loss: 0.00001808
Iteration 180/1000 | Loss: 0.00001808
Iteration 181/1000 | Loss: 0.00001807
Iteration 182/1000 | Loss: 0.00001807
Iteration 183/1000 | Loss: 0.00001807
Iteration 184/1000 | Loss: 0.00001807
Iteration 185/1000 | Loss: 0.00001807
Iteration 186/1000 | Loss: 0.00001807
Iteration 187/1000 | Loss: 0.00001807
Iteration 188/1000 | Loss: 0.00001807
Iteration 189/1000 | Loss: 0.00001807
Iteration 190/1000 | Loss: 0.00001807
Iteration 191/1000 | Loss: 0.00001807
Iteration 192/1000 | Loss: 0.00001807
Iteration 193/1000 | Loss: 0.00001807
Iteration 194/1000 | Loss: 0.00001807
Iteration 195/1000 | Loss: 0.00001807
Iteration 196/1000 | Loss: 0.00001807
Iteration 197/1000 | Loss: 0.00001807
Iteration 198/1000 | Loss: 0.00001807
Iteration 199/1000 | Loss: 0.00001807
Iteration 200/1000 | Loss: 0.00001807
Iteration 201/1000 | Loss: 0.00001807
Iteration 202/1000 | Loss: 0.00001807
Iteration 203/1000 | Loss: 0.00001807
Iteration 204/1000 | Loss: 0.00001807
Iteration 205/1000 | Loss: 0.00001807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.8069535144604743e-05, 1.8069535144604743e-05, 1.8069535144604743e-05, 1.8069535144604743e-05, 1.8069535144604743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8069535144604743e-05

Optimization complete. Final v2v error: 3.544748306274414 mm

Highest mean error: 4.758314609527588 mm for frame 1

Lowest mean error: 3.2155709266662598 mm for frame 146

Saving results

Total time: 46.397841930389404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409407
Iteration 2/25 | Loss: 0.00112218
Iteration 3/25 | Loss: 0.00074760
Iteration 4/25 | Loss: 0.00066081
Iteration 5/25 | Loss: 0.00064041
Iteration 6/25 | Loss: 0.00063637
Iteration 7/25 | Loss: 0.00063560
Iteration 8/25 | Loss: 0.00063534
Iteration 9/25 | Loss: 0.00063534
Iteration 10/25 | Loss: 0.00063534
Iteration 11/25 | Loss: 0.00063534
Iteration 12/25 | Loss: 0.00063534
Iteration 13/25 | Loss: 0.00063534
Iteration 14/25 | Loss: 0.00063534
Iteration 15/25 | Loss: 0.00063534
Iteration 16/25 | Loss: 0.00063534
Iteration 17/25 | Loss: 0.00063534
Iteration 18/25 | Loss: 0.00063534
Iteration 19/25 | Loss: 0.00063534
Iteration 20/25 | Loss: 0.00063534
Iteration 21/25 | Loss: 0.00063534
Iteration 22/25 | Loss: 0.00063534
Iteration 23/25 | Loss: 0.00063534
Iteration 24/25 | Loss: 0.00063534
Iteration 25/25 | Loss: 0.00063534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44714308
Iteration 2/25 | Loss: 0.00030383
Iteration 3/25 | Loss: 0.00030383
Iteration 4/25 | Loss: 0.00030383
Iteration 5/25 | Loss: 0.00030383
Iteration 6/25 | Loss: 0.00030383
Iteration 7/25 | Loss: 0.00030383
Iteration 8/25 | Loss: 0.00030383
Iteration 9/25 | Loss: 0.00030383
Iteration 10/25 | Loss: 0.00030383
Iteration 11/25 | Loss: 0.00030383
Iteration 12/25 | Loss: 0.00030383
Iteration 13/25 | Loss: 0.00030383
Iteration 14/25 | Loss: 0.00030383
Iteration 15/25 | Loss: 0.00030383
Iteration 16/25 | Loss: 0.00030383
Iteration 17/25 | Loss: 0.00030383
Iteration 18/25 | Loss: 0.00030383
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00030382536351680756, 0.00030382536351680756, 0.00030382536351680756, 0.00030382536351680756, 0.00030382536351680756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030382536351680756

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030383
Iteration 2/1000 | Loss: 0.00003335
Iteration 3/1000 | Loss: 0.00002289
Iteration 4/1000 | Loss: 0.00001840
Iteration 5/1000 | Loss: 0.00001725
Iteration 6/1000 | Loss: 0.00001619
Iteration 7/1000 | Loss: 0.00001567
Iteration 8/1000 | Loss: 0.00001522
Iteration 9/1000 | Loss: 0.00001492
Iteration 10/1000 | Loss: 0.00001471
Iteration 11/1000 | Loss: 0.00001456
Iteration 12/1000 | Loss: 0.00001435
Iteration 13/1000 | Loss: 0.00001431
Iteration 14/1000 | Loss: 0.00001428
Iteration 15/1000 | Loss: 0.00001426
Iteration 16/1000 | Loss: 0.00001416
Iteration 17/1000 | Loss: 0.00001414
Iteration 18/1000 | Loss: 0.00001413
Iteration 19/1000 | Loss: 0.00001413
Iteration 20/1000 | Loss: 0.00001413
Iteration 21/1000 | Loss: 0.00001412
Iteration 22/1000 | Loss: 0.00001412
Iteration 23/1000 | Loss: 0.00001412
Iteration 24/1000 | Loss: 0.00001411
Iteration 25/1000 | Loss: 0.00001409
Iteration 26/1000 | Loss: 0.00001408
Iteration 27/1000 | Loss: 0.00001408
Iteration 28/1000 | Loss: 0.00001407
Iteration 29/1000 | Loss: 0.00001407
Iteration 30/1000 | Loss: 0.00001406
Iteration 31/1000 | Loss: 0.00001406
Iteration 32/1000 | Loss: 0.00001404
Iteration 33/1000 | Loss: 0.00001404
Iteration 34/1000 | Loss: 0.00001403
Iteration 35/1000 | Loss: 0.00001403
Iteration 36/1000 | Loss: 0.00001402
Iteration 37/1000 | Loss: 0.00001401
Iteration 38/1000 | Loss: 0.00001401
Iteration 39/1000 | Loss: 0.00001401
Iteration 40/1000 | Loss: 0.00001400
Iteration 41/1000 | Loss: 0.00001400
Iteration 42/1000 | Loss: 0.00001399
Iteration 43/1000 | Loss: 0.00001399
Iteration 44/1000 | Loss: 0.00001399
Iteration 45/1000 | Loss: 0.00001398
Iteration 46/1000 | Loss: 0.00001398
Iteration 47/1000 | Loss: 0.00001398
Iteration 48/1000 | Loss: 0.00001397
Iteration 49/1000 | Loss: 0.00001396
Iteration 50/1000 | Loss: 0.00001396
Iteration 51/1000 | Loss: 0.00001395
Iteration 52/1000 | Loss: 0.00001395
Iteration 53/1000 | Loss: 0.00001395
Iteration 54/1000 | Loss: 0.00001395
Iteration 55/1000 | Loss: 0.00001394
Iteration 56/1000 | Loss: 0.00001394
Iteration 57/1000 | Loss: 0.00001394
Iteration 58/1000 | Loss: 0.00001394
Iteration 59/1000 | Loss: 0.00001394
Iteration 60/1000 | Loss: 0.00001394
Iteration 61/1000 | Loss: 0.00001394
Iteration 62/1000 | Loss: 0.00001394
Iteration 63/1000 | Loss: 0.00001394
Iteration 64/1000 | Loss: 0.00001394
Iteration 65/1000 | Loss: 0.00001394
Iteration 66/1000 | Loss: 0.00001394
Iteration 67/1000 | Loss: 0.00001394
Iteration 68/1000 | Loss: 0.00001394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.393879392708186e-05, 1.393879392708186e-05, 1.393879392708186e-05, 1.393879392708186e-05, 1.393879392708186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.393879392708186e-05

Optimization complete. Final v2v error: 3.1908442974090576 mm

Highest mean error: 3.9515881538391113 mm for frame 72

Lowest mean error: 2.8176023960113525 mm for frame 104

Saving results

Total time: 34.82893705368042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377439
Iteration 2/25 | Loss: 0.00071499
Iteration 3/25 | Loss: 0.00062502
Iteration 4/25 | Loss: 0.00060596
Iteration 5/25 | Loss: 0.00060144
Iteration 6/25 | Loss: 0.00060104
Iteration 7/25 | Loss: 0.00060104
Iteration 8/25 | Loss: 0.00060104
Iteration 9/25 | Loss: 0.00060104
Iteration 10/25 | Loss: 0.00060104
Iteration 11/25 | Loss: 0.00060104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006010390352457762, 0.0006010390352457762, 0.0006010390352457762, 0.0006010390352457762, 0.0006010390352457762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006010390352457762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44168496
Iteration 2/25 | Loss: 0.00027436
Iteration 3/25 | Loss: 0.00027436
Iteration 4/25 | Loss: 0.00027436
Iteration 5/25 | Loss: 0.00027436
Iteration 6/25 | Loss: 0.00027436
Iteration 7/25 | Loss: 0.00027436
Iteration 8/25 | Loss: 0.00027436
Iteration 9/25 | Loss: 0.00027436
Iteration 10/25 | Loss: 0.00027436
Iteration 11/25 | Loss: 0.00027436
Iteration 12/25 | Loss: 0.00027436
Iteration 13/25 | Loss: 0.00027436
Iteration 14/25 | Loss: 0.00027436
Iteration 15/25 | Loss: 0.00027436
Iteration 16/25 | Loss: 0.00027436
Iteration 17/25 | Loss: 0.00027436
Iteration 18/25 | Loss: 0.00027436
Iteration 19/25 | Loss: 0.00027436
Iteration 20/25 | Loss: 0.00027436
Iteration 21/25 | Loss: 0.00027436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0002743589866440743, 0.0002743589866440743, 0.0002743589866440743, 0.0002743589866440743, 0.0002743589866440743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002743589866440743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027436
Iteration 2/1000 | Loss: 0.00001910
Iteration 3/1000 | Loss: 0.00001537
Iteration 4/1000 | Loss: 0.00001414
Iteration 5/1000 | Loss: 0.00001361
Iteration 6/1000 | Loss: 0.00001316
Iteration 7/1000 | Loss: 0.00001290
Iteration 8/1000 | Loss: 0.00001287
Iteration 9/1000 | Loss: 0.00001269
Iteration 10/1000 | Loss: 0.00001264
Iteration 11/1000 | Loss: 0.00001259
Iteration 12/1000 | Loss: 0.00001259
Iteration 13/1000 | Loss: 0.00001258
Iteration 14/1000 | Loss: 0.00001248
Iteration 15/1000 | Loss: 0.00001247
Iteration 16/1000 | Loss: 0.00001243
Iteration 17/1000 | Loss: 0.00001241
Iteration 18/1000 | Loss: 0.00001241
Iteration 19/1000 | Loss: 0.00001240
Iteration 20/1000 | Loss: 0.00001239
Iteration 21/1000 | Loss: 0.00001239
Iteration 22/1000 | Loss: 0.00001238
Iteration 23/1000 | Loss: 0.00001238
Iteration 24/1000 | Loss: 0.00001238
Iteration 25/1000 | Loss: 0.00001237
Iteration 26/1000 | Loss: 0.00001237
Iteration 27/1000 | Loss: 0.00001237
Iteration 28/1000 | Loss: 0.00001237
Iteration 29/1000 | Loss: 0.00001237
Iteration 30/1000 | Loss: 0.00001236
Iteration 31/1000 | Loss: 0.00001235
Iteration 32/1000 | Loss: 0.00001231
Iteration 33/1000 | Loss: 0.00001229
Iteration 34/1000 | Loss: 0.00001228
Iteration 35/1000 | Loss: 0.00001228
Iteration 36/1000 | Loss: 0.00001228
Iteration 37/1000 | Loss: 0.00001227
Iteration 38/1000 | Loss: 0.00001227
Iteration 39/1000 | Loss: 0.00001227
Iteration 40/1000 | Loss: 0.00001226
Iteration 41/1000 | Loss: 0.00001226
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001225
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001224
Iteration 46/1000 | Loss: 0.00001224
Iteration 47/1000 | Loss: 0.00001223
Iteration 48/1000 | Loss: 0.00001223
Iteration 49/1000 | Loss: 0.00001221
Iteration 50/1000 | Loss: 0.00001221
Iteration 51/1000 | Loss: 0.00001220
Iteration 52/1000 | Loss: 0.00001219
Iteration 53/1000 | Loss: 0.00001219
Iteration 54/1000 | Loss: 0.00001219
Iteration 55/1000 | Loss: 0.00001218
Iteration 56/1000 | Loss: 0.00001218
Iteration 57/1000 | Loss: 0.00001218
Iteration 58/1000 | Loss: 0.00001218
Iteration 59/1000 | Loss: 0.00001217
Iteration 60/1000 | Loss: 0.00001217
Iteration 61/1000 | Loss: 0.00001217
Iteration 62/1000 | Loss: 0.00001217
Iteration 63/1000 | Loss: 0.00001216
Iteration 64/1000 | Loss: 0.00001216
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001216
Iteration 68/1000 | Loss: 0.00001216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.2164018698967993e-05, 1.2164018698967993e-05, 1.2164018698967993e-05, 1.2164018698967993e-05, 1.2164018698967993e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2164018698967993e-05

Optimization complete. Final v2v error: 2.966167449951172 mm

Highest mean error: 2.999318838119507 mm for frame 80

Lowest mean error: 2.9370405673980713 mm for frame 266

Saving results

Total time: 31.983051300048828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857447
Iteration 2/25 | Loss: 0.00146515
Iteration 3/25 | Loss: 0.00099675
Iteration 4/25 | Loss: 0.00090613
Iteration 5/25 | Loss: 0.00086354
Iteration 6/25 | Loss: 0.00083205
Iteration 7/25 | Loss: 0.00082607
Iteration 8/25 | Loss: 0.00081679
Iteration 9/25 | Loss: 0.00081601
Iteration 10/25 | Loss: 0.00081123
Iteration 11/25 | Loss: 0.00080272
Iteration 12/25 | Loss: 0.00080154
Iteration 13/25 | Loss: 0.00079558
Iteration 14/25 | Loss: 0.00078705
Iteration 15/25 | Loss: 0.00078715
Iteration 16/25 | Loss: 0.00078555
Iteration 17/25 | Loss: 0.00078992
Iteration 18/25 | Loss: 0.00078840
Iteration 19/25 | Loss: 0.00078243
Iteration 20/25 | Loss: 0.00077975
Iteration 21/25 | Loss: 0.00077883
Iteration 22/25 | Loss: 0.00077610
Iteration 23/25 | Loss: 0.00077553
Iteration 24/25 | Loss: 0.00077526
Iteration 25/25 | Loss: 0.00077511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60455310
Iteration 2/25 | Loss: 0.00059692
Iteration 3/25 | Loss: 0.00050600
Iteration 4/25 | Loss: 0.00050600
Iteration 5/25 | Loss: 0.00050600
Iteration 6/25 | Loss: 0.00050600
Iteration 7/25 | Loss: 0.00050600
Iteration 8/25 | Loss: 0.00050600
Iteration 9/25 | Loss: 0.00050600
Iteration 10/25 | Loss: 0.00050600
Iteration 11/25 | Loss: 0.00050600
Iteration 12/25 | Loss: 0.00050600
Iteration 13/25 | Loss: 0.00050600
Iteration 14/25 | Loss: 0.00050600
Iteration 15/25 | Loss: 0.00050600
Iteration 16/25 | Loss: 0.00050600
Iteration 17/25 | Loss: 0.00050600
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005059955292381346, 0.0005059955292381346, 0.0005059955292381346, 0.0005059955292381346, 0.0005059955292381346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005059955292381346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050600
Iteration 2/1000 | Loss: 0.00028257
Iteration 3/1000 | Loss: 0.00009038
Iteration 4/1000 | Loss: 0.00006015
Iteration 5/1000 | Loss: 0.00057164
Iteration 6/1000 | Loss: 0.00028794
Iteration 7/1000 | Loss: 0.00034307
Iteration 8/1000 | Loss: 0.00049236
Iteration 9/1000 | Loss: 0.00009643
Iteration 10/1000 | Loss: 0.00047523
Iteration 11/1000 | Loss: 0.00048284
Iteration 12/1000 | Loss: 0.00015352
Iteration 13/1000 | Loss: 0.00045083
Iteration 14/1000 | Loss: 0.00048264
Iteration 15/1000 | Loss: 0.00016270
Iteration 16/1000 | Loss: 0.00005792
Iteration 17/1000 | Loss: 0.00012987
Iteration 18/1000 | Loss: 0.00009407
Iteration 19/1000 | Loss: 0.00011658
Iteration 20/1000 | Loss: 0.00009272
Iteration 21/1000 | Loss: 0.00004470
Iteration 22/1000 | Loss: 0.00011877
Iteration 23/1000 | Loss: 0.00013509
Iteration 24/1000 | Loss: 0.00111964
Iteration 25/1000 | Loss: 0.00011266
Iteration 26/1000 | Loss: 0.00007604
Iteration 27/1000 | Loss: 0.00011578
Iteration 28/1000 | Loss: 0.00026758
Iteration 29/1000 | Loss: 0.00017748
Iteration 30/1000 | Loss: 0.00020566
Iteration 31/1000 | Loss: 0.00014595
Iteration 32/1000 | Loss: 0.00022701
Iteration 33/1000 | Loss: 0.00021552
Iteration 34/1000 | Loss: 0.00015411
Iteration 35/1000 | Loss: 0.00012165
Iteration 36/1000 | Loss: 0.00009378
Iteration 37/1000 | Loss: 0.00016555
Iteration 38/1000 | Loss: 0.00026270
Iteration 39/1000 | Loss: 0.00024440
Iteration 40/1000 | Loss: 0.00006109
Iteration 41/1000 | Loss: 0.00017717
Iteration 42/1000 | Loss: 0.00015118
Iteration 43/1000 | Loss: 0.00013720
Iteration 44/1000 | Loss: 0.00014764
Iteration 45/1000 | Loss: 0.00005692
Iteration 46/1000 | Loss: 0.00005778
Iteration 47/1000 | Loss: 0.00003748
Iteration 48/1000 | Loss: 0.00012901
Iteration 49/1000 | Loss: 0.00004180
Iteration 50/1000 | Loss: 0.00003887
Iteration 51/1000 | Loss: 0.00008833
Iteration 52/1000 | Loss: 0.00015965
Iteration 53/1000 | Loss: 0.00009253
Iteration 54/1000 | Loss: 0.00016557
Iteration 55/1000 | Loss: 0.00010843
Iteration 56/1000 | Loss: 0.00020445
Iteration 57/1000 | Loss: 0.00014459
Iteration 58/1000 | Loss: 0.00015780
Iteration 59/1000 | Loss: 0.00013280
Iteration 60/1000 | Loss: 0.00014303
Iteration 61/1000 | Loss: 0.00013469
Iteration 62/1000 | Loss: 0.00019517
Iteration 63/1000 | Loss: 0.00016856
Iteration 64/1000 | Loss: 0.00007731
Iteration 65/1000 | Loss: 0.00012556
Iteration 66/1000 | Loss: 0.00007196
Iteration 67/1000 | Loss: 0.00004173
Iteration 68/1000 | Loss: 0.00061142
Iteration 69/1000 | Loss: 0.00057143
Iteration 70/1000 | Loss: 0.00012033
Iteration 71/1000 | Loss: 0.00012082
Iteration 72/1000 | Loss: 0.00004713
Iteration 73/1000 | Loss: 0.00023684
Iteration 74/1000 | Loss: 0.00011904
Iteration 75/1000 | Loss: 0.00074860
Iteration 76/1000 | Loss: 0.00004991
Iteration 77/1000 | Loss: 0.00003935
Iteration 78/1000 | Loss: 0.00003455
Iteration 79/1000 | Loss: 0.00003123
Iteration 80/1000 | Loss: 0.00003001
Iteration 81/1000 | Loss: 0.00002951
Iteration 82/1000 | Loss: 0.00002916
Iteration 83/1000 | Loss: 0.00002892
Iteration 84/1000 | Loss: 0.00002872
Iteration 85/1000 | Loss: 0.00002847
Iteration 86/1000 | Loss: 0.00002831
Iteration 87/1000 | Loss: 0.00002831
Iteration 88/1000 | Loss: 0.00002820
Iteration 89/1000 | Loss: 0.00002818
Iteration 90/1000 | Loss: 0.00002811
Iteration 91/1000 | Loss: 0.00002807
Iteration 92/1000 | Loss: 0.00002806
Iteration 93/1000 | Loss: 0.00002798
Iteration 94/1000 | Loss: 0.00002793
Iteration 95/1000 | Loss: 0.00002791
Iteration 96/1000 | Loss: 0.00002791
Iteration 97/1000 | Loss: 0.00002790
Iteration 98/1000 | Loss: 0.00002787
Iteration 99/1000 | Loss: 0.00002787
Iteration 100/1000 | Loss: 0.00002786
Iteration 101/1000 | Loss: 0.00002785
Iteration 102/1000 | Loss: 0.00002785
Iteration 103/1000 | Loss: 0.00002782
Iteration 104/1000 | Loss: 0.00002782
Iteration 105/1000 | Loss: 0.00002781
Iteration 106/1000 | Loss: 0.00002778
Iteration 107/1000 | Loss: 0.00002778
Iteration 108/1000 | Loss: 0.00002778
Iteration 109/1000 | Loss: 0.00002778
Iteration 110/1000 | Loss: 0.00002777
Iteration 111/1000 | Loss: 0.00002777
Iteration 112/1000 | Loss: 0.00002777
Iteration 113/1000 | Loss: 0.00002776
Iteration 114/1000 | Loss: 0.00002776
Iteration 115/1000 | Loss: 0.00002776
Iteration 116/1000 | Loss: 0.00002776
Iteration 117/1000 | Loss: 0.00002776
Iteration 118/1000 | Loss: 0.00002776
Iteration 119/1000 | Loss: 0.00002776
Iteration 120/1000 | Loss: 0.00002775
Iteration 121/1000 | Loss: 0.00002775
Iteration 122/1000 | Loss: 0.00002775
Iteration 123/1000 | Loss: 0.00002775
Iteration 124/1000 | Loss: 0.00002775
Iteration 125/1000 | Loss: 0.00002775
Iteration 126/1000 | Loss: 0.00002775
Iteration 127/1000 | Loss: 0.00002775
Iteration 128/1000 | Loss: 0.00002775
Iteration 129/1000 | Loss: 0.00002775
Iteration 130/1000 | Loss: 0.00002775
Iteration 131/1000 | Loss: 0.00002775
Iteration 132/1000 | Loss: 0.00002774
Iteration 133/1000 | Loss: 0.00002774
Iteration 134/1000 | Loss: 0.00002774
Iteration 135/1000 | Loss: 0.00002773
Iteration 136/1000 | Loss: 0.00002773
Iteration 137/1000 | Loss: 0.00002773
Iteration 138/1000 | Loss: 0.00002772
Iteration 139/1000 | Loss: 0.00002772
Iteration 140/1000 | Loss: 0.00002772
Iteration 141/1000 | Loss: 0.00002772
Iteration 142/1000 | Loss: 0.00002771
Iteration 143/1000 | Loss: 0.00002771
Iteration 144/1000 | Loss: 0.00002771
Iteration 145/1000 | Loss: 0.00002771
Iteration 146/1000 | Loss: 0.00002771
Iteration 147/1000 | Loss: 0.00002770
Iteration 148/1000 | Loss: 0.00002770
Iteration 149/1000 | Loss: 0.00002770
Iteration 150/1000 | Loss: 0.00002770
Iteration 151/1000 | Loss: 0.00002770
Iteration 152/1000 | Loss: 0.00002770
Iteration 153/1000 | Loss: 0.00002769
Iteration 154/1000 | Loss: 0.00002769
Iteration 155/1000 | Loss: 0.00002769
Iteration 156/1000 | Loss: 0.00002768
Iteration 157/1000 | Loss: 0.00002768
Iteration 158/1000 | Loss: 0.00002768
Iteration 159/1000 | Loss: 0.00002768
Iteration 160/1000 | Loss: 0.00002768
Iteration 161/1000 | Loss: 0.00002767
Iteration 162/1000 | Loss: 0.00002767
Iteration 163/1000 | Loss: 0.00002767
Iteration 164/1000 | Loss: 0.00002767
Iteration 165/1000 | Loss: 0.00002767
Iteration 166/1000 | Loss: 0.00002767
Iteration 167/1000 | Loss: 0.00002767
Iteration 168/1000 | Loss: 0.00002767
Iteration 169/1000 | Loss: 0.00002767
Iteration 170/1000 | Loss: 0.00002766
Iteration 171/1000 | Loss: 0.00002766
Iteration 172/1000 | Loss: 0.00002766
Iteration 173/1000 | Loss: 0.00002766
Iteration 174/1000 | Loss: 0.00002766
Iteration 175/1000 | Loss: 0.00002765
Iteration 176/1000 | Loss: 0.00002765
Iteration 177/1000 | Loss: 0.00002765
Iteration 178/1000 | Loss: 0.00002765
Iteration 179/1000 | Loss: 0.00002765
Iteration 180/1000 | Loss: 0.00002764
Iteration 181/1000 | Loss: 0.00002764
Iteration 182/1000 | Loss: 0.00002764
Iteration 183/1000 | Loss: 0.00002764
Iteration 184/1000 | Loss: 0.00002764
Iteration 185/1000 | Loss: 0.00002763
Iteration 186/1000 | Loss: 0.00002763
Iteration 187/1000 | Loss: 0.00002763
Iteration 188/1000 | Loss: 0.00002763
Iteration 189/1000 | Loss: 0.00002763
Iteration 190/1000 | Loss: 0.00002763
Iteration 191/1000 | Loss: 0.00002763
Iteration 192/1000 | Loss: 0.00002763
Iteration 193/1000 | Loss: 0.00002763
Iteration 194/1000 | Loss: 0.00002763
Iteration 195/1000 | Loss: 0.00002763
Iteration 196/1000 | Loss: 0.00002763
Iteration 197/1000 | Loss: 0.00002763
Iteration 198/1000 | Loss: 0.00002762
Iteration 199/1000 | Loss: 0.00002762
Iteration 200/1000 | Loss: 0.00002762
Iteration 201/1000 | Loss: 0.00002762
Iteration 202/1000 | Loss: 0.00002762
Iteration 203/1000 | Loss: 0.00002762
Iteration 204/1000 | Loss: 0.00002762
Iteration 205/1000 | Loss: 0.00002762
Iteration 206/1000 | Loss: 0.00002762
Iteration 207/1000 | Loss: 0.00002762
Iteration 208/1000 | Loss: 0.00002762
Iteration 209/1000 | Loss: 0.00002762
Iteration 210/1000 | Loss: 0.00002762
Iteration 211/1000 | Loss: 0.00002762
Iteration 212/1000 | Loss: 0.00002762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [2.7623347705230117e-05, 2.7623347705230117e-05, 2.7623347705230117e-05, 2.7623347705230117e-05, 2.7623347705230117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7623347705230117e-05

Optimization complete. Final v2v error: 4.388739585876465 mm

Highest mean error: 6.137213706970215 mm for frame 184

Lowest mean error: 3.4002087116241455 mm for frame 42

Saving results

Total time: 204.97883653640747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00699400
Iteration 2/25 | Loss: 0.00143007
Iteration 3/25 | Loss: 0.00095143
Iteration 4/25 | Loss: 0.00079355
Iteration 5/25 | Loss: 0.00074645
Iteration 6/25 | Loss: 0.00074429
Iteration 7/25 | Loss: 0.00073235
Iteration 8/25 | Loss: 0.00073350
Iteration 9/25 | Loss: 0.00073400
Iteration 10/25 | Loss: 0.00072746
Iteration 11/25 | Loss: 0.00071826
Iteration 12/25 | Loss: 0.00071501
Iteration 13/25 | Loss: 0.00071403
Iteration 14/25 | Loss: 0.00071335
Iteration 15/25 | Loss: 0.00071426
Iteration 16/25 | Loss: 0.00071076
Iteration 17/25 | Loss: 0.00070950
Iteration 18/25 | Loss: 0.00070904
Iteration 19/25 | Loss: 0.00070891
Iteration 20/25 | Loss: 0.00070890
Iteration 21/25 | Loss: 0.00070889
Iteration 22/25 | Loss: 0.00070889
Iteration 23/25 | Loss: 0.00070889
Iteration 24/25 | Loss: 0.00070889
Iteration 25/25 | Loss: 0.00070889

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.34205127
Iteration 2/25 | Loss: 0.00037531
Iteration 3/25 | Loss: 0.00037526
Iteration 4/25 | Loss: 0.00037526
Iteration 5/25 | Loss: 0.00037526
Iteration 6/25 | Loss: 0.00037526
Iteration 7/25 | Loss: 0.00037526
Iteration 8/25 | Loss: 0.00037526
Iteration 9/25 | Loss: 0.00037526
Iteration 10/25 | Loss: 0.00037526
Iteration 11/25 | Loss: 0.00037526
Iteration 12/25 | Loss: 0.00037526
Iteration 13/25 | Loss: 0.00037526
Iteration 14/25 | Loss: 0.00037526
Iteration 15/25 | Loss: 0.00037526
Iteration 16/25 | Loss: 0.00037526
Iteration 17/25 | Loss: 0.00037526
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003752612683456391, 0.0003752612683456391, 0.0003752612683456391, 0.0003752612683456391, 0.0003752612683456391]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003752612683456391

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037526
Iteration 2/1000 | Loss: 0.00003245
Iteration 3/1000 | Loss: 0.00002343
Iteration 4/1000 | Loss: 0.00002154
Iteration 5/1000 | Loss: 0.00002032
Iteration 6/1000 | Loss: 0.00001944
Iteration 7/1000 | Loss: 0.00001889
Iteration 8/1000 | Loss: 0.00001857
Iteration 9/1000 | Loss: 0.00012749
Iteration 10/1000 | Loss: 0.00001939
Iteration 11/1000 | Loss: 0.00001829
Iteration 12/1000 | Loss: 0.00001729
Iteration 13/1000 | Loss: 0.00001665
Iteration 14/1000 | Loss: 0.00001627
Iteration 15/1000 | Loss: 0.00001609
Iteration 16/1000 | Loss: 0.00001605
Iteration 17/1000 | Loss: 0.00001603
Iteration 18/1000 | Loss: 0.00001597
Iteration 19/1000 | Loss: 0.00001597
Iteration 20/1000 | Loss: 0.00001590
Iteration 21/1000 | Loss: 0.00001589
Iteration 22/1000 | Loss: 0.00001586
Iteration 23/1000 | Loss: 0.00001586
Iteration 24/1000 | Loss: 0.00001585
Iteration 25/1000 | Loss: 0.00001585
Iteration 26/1000 | Loss: 0.00001585
Iteration 27/1000 | Loss: 0.00001584
Iteration 28/1000 | Loss: 0.00001584
Iteration 29/1000 | Loss: 0.00001584
Iteration 30/1000 | Loss: 0.00001584
Iteration 31/1000 | Loss: 0.00001584
Iteration 32/1000 | Loss: 0.00001584
Iteration 33/1000 | Loss: 0.00001583
Iteration 34/1000 | Loss: 0.00001583
Iteration 35/1000 | Loss: 0.00001583
Iteration 36/1000 | Loss: 0.00001583
Iteration 37/1000 | Loss: 0.00001582
Iteration 38/1000 | Loss: 0.00001582
Iteration 39/1000 | Loss: 0.00001582
Iteration 40/1000 | Loss: 0.00001582
Iteration 41/1000 | Loss: 0.00001582
Iteration 42/1000 | Loss: 0.00001581
Iteration 43/1000 | Loss: 0.00001581
Iteration 44/1000 | Loss: 0.00001580
Iteration 45/1000 | Loss: 0.00001580
Iteration 46/1000 | Loss: 0.00001580
Iteration 47/1000 | Loss: 0.00001580
Iteration 48/1000 | Loss: 0.00001580
Iteration 49/1000 | Loss: 0.00001580
Iteration 50/1000 | Loss: 0.00001580
Iteration 51/1000 | Loss: 0.00001580
Iteration 52/1000 | Loss: 0.00001580
Iteration 53/1000 | Loss: 0.00001579
Iteration 54/1000 | Loss: 0.00001579
Iteration 55/1000 | Loss: 0.00001579
Iteration 56/1000 | Loss: 0.00001579
Iteration 57/1000 | Loss: 0.00001579
Iteration 58/1000 | Loss: 0.00001579
Iteration 59/1000 | Loss: 0.00001578
Iteration 60/1000 | Loss: 0.00001578
Iteration 61/1000 | Loss: 0.00001578
Iteration 62/1000 | Loss: 0.00001578
Iteration 63/1000 | Loss: 0.00001577
Iteration 64/1000 | Loss: 0.00001577
Iteration 65/1000 | Loss: 0.00001577
Iteration 66/1000 | Loss: 0.00001577
Iteration 67/1000 | Loss: 0.00001576
Iteration 68/1000 | Loss: 0.00001576
Iteration 69/1000 | Loss: 0.00001576
Iteration 70/1000 | Loss: 0.00001575
Iteration 71/1000 | Loss: 0.00001575
Iteration 72/1000 | Loss: 0.00001574
Iteration 73/1000 | Loss: 0.00001574
Iteration 74/1000 | Loss: 0.00001574
Iteration 75/1000 | Loss: 0.00001574
Iteration 76/1000 | Loss: 0.00001573
Iteration 77/1000 | Loss: 0.00001573
Iteration 78/1000 | Loss: 0.00001573
Iteration 79/1000 | Loss: 0.00001573
Iteration 80/1000 | Loss: 0.00001573
Iteration 81/1000 | Loss: 0.00001572
Iteration 82/1000 | Loss: 0.00001572
Iteration 83/1000 | Loss: 0.00001572
Iteration 84/1000 | Loss: 0.00001572
Iteration 85/1000 | Loss: 0.00001572
Iteration 86/1000 | Loss: 0.00001572
Iteration 87/1000 | Loss: 0.00001572
Iteration 88/1000 | Loss: 0.00001572
Iteration 89/1000 | Loss: 0.00001572
Iteration 90/1000 | Loss: 0.00001572
Iteration 91/1000 | Loss: 0.00001572
Iteration 92/1000 | Loss: 0.00001572
Iteration 93/1000 | Loss: 0.00001572
Iteration 94/1000 | Loss: 0.00001572
Iteration 95/1000 | Loss: 0.00001572
Iteration 96/1000 | Loss: 0.00001572
Iteration 97/1000 | Loss: 0.00001572
Iteration 98/1000 | Loss: 0.00001572
Iteration 99/1000 | Loss: 0.00001572
Iteration 100/1000 | Loss: 0.00001572
Iteration 101/1000 | Loss: 0.00001572
Iteration 102/1000 | Loss: 0.00001572
Iteration 103/1000 | Loss: 0.00001572
Iteration 104/1000 | Loss: 0.00001572
Iteration 105/1000 | Loss: 0.00001572
Iteration 106/1000 | Loss: 0.00001572
Iteration 107/1000 | Loss: 0.00001572
Iteration 108/1000 | Loss: 0.00001572
Iteration 109/1000 | Loss: 0.00001572
Iteration 110/1000 | Loss: 0.00001572
Iteration 111/1000 | Loss: 0.00001572
Iteration 112/1000 | Loss: 0.00001571
Iteration 113/1000 | Loss: 0.00001571
Iteration 114/1000 | Loss: 0.00001571
Iteration 115/1000 | Loss: 0.00001571
Iteration 116/1000 | Loss: 0.00001571
Iteration 117/1000 | Loss: 0.00001571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.5714882465545088e-05, 1.5714882465545088e-05, 1.5714882465545088e-05, 1.5714882465545088e-05, 1.5714882465545088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5714882465545088e-05

Optimization complete. Final v2v error: 3.387664794921875 mm

Highest mean error: 4.240565299987793 mm for frame 18

Lowest mean error: 2.8803060054779053 mm for frame 132

Saving results

Total time: 62.98697376251221
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061170
Iteration 2/25 | Loss: 0.00249621
Iteration 3/25 | Loss: 0.00113706
Iteration 4/25 | Loss: 0.00088242
Iteration 5/25 | Loss: 0.00093089
Iteration 6/25 | Loss: 0.00090857
Iteration 7/25 | Loss: 0.00081872
Iteration 8/25 | Loss: 0.00079770
Iteration 9/25 | Loss: 0.00078541
Iteration 10/25 | Loss: 0.00077729
Iteration 11/25 | Loss: 0.00082339
Iteration 12/25 | Loss: 0.00075751
Iteration 13/25 | Loss: 0.00075652
Iteration 14/25 | Loss: 0.00073861
Iteration 15/25 | Loss: 0.00072888
Iteration 16/25 | Loss: 0.00071062
Iteration 17/25 | Loss: 0.00068398
Iteration 18/25 | Loss: 0.00066829
Iteration 19/25 | Loss: 0.00065901
Iteration 20/25 | Loss: 0.00065799
Iteration 21/25 | Loss: 0.00064960
Iteration 22/25 | Loss: 0.00064975
Iteration 23/25 | Loss: 0.00064787
Iteration 24/25 | Loss: 0.00064269
Iteration 25/25 | Loss: 0.00064058

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51785088
Iteration 2/25 | Loss: 0.00034942
Iteration 3/25 | Loss: 0.00034942
Iteration 4/25 | Loss: 0.00034942
Iteration 5/25 | Loss: 0.00034942
Iteration 6/25 | Loss: 0.00034942
Iteration 7/25 | Loss: 0.00034942
Iteration 8/25 | Loss: 0.00034942
Iteration 9/25 | Loss: 0.00034942
Iteration 10/25 | Loss: 0.00034942
Iteration 11/25 | Loss: 0.00034942
Iteration 12/25 | Loss: 0.00034942
Iteration 13/25 | Loss: 0.00034942
Iteration 14/25 | Loss: 0.00034942
Iteration 15/25 | Loss: 0.00034942
Iteration 16/25 | Loss: 0.00034942
Iteration 17/25 | Loss: 0.00034942
Iteration 18/25 | Loss: 0.00034942
Iteration 19/25 | Loss: 0.00034942
Iteration 20/25 | Loss: 0.00034942
Iteration 21/25 | Loss: 0.00034942
Iteration 22/25 | Loss: 0.00034942
Iteration 23/25 | Loss: 0.00034942
Iteration 24/25 | Loss: 0.00034942
Iteration 25/25 | Loss: 0.00034942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034942
Iteration 2/1000 | Loss: 0.00004880
Iteration 3/1000 | Loss: 0.00003071
Iteration 4/1000 | Loss: 0.00003214
Iteration 5/1000 | Loss: 0.00002431
Iteration 6/1000 | Loss: 0.00002937
Iteration 7/1000 | Loss: 0.00002593
Iteration 8/1000 | Loss: 0.00003328
Iteration 9/1000 | Loss: 0.00003547
Iteration 10/1000 | Loss: 0.00003492
Iteration 11/1000 | Loss: 0.00003310
Iteration 12/1000 | Loss: 0.00002930
Iteration 13/1000 | Loss: 0.00003362
Iteration 14/1000 | Loss: 0.00003224
Iteration 15/1000 | Loss: 0.00003444
Iteration 16/1000 | Loss: 0.00003507
Iteration 17/1000 | Loss: 0.00003644
Iteration 18/1000 | Loss: 0.00002931
Iteration 19/1000 | Loss: 0.00004921
Iteration 20/1000 | Loss: 0.00002353
Iteration 21/1000 | Loss: 0.00001752
Iteration 22/1000 | Loss: 0.00001609
Iteration 23/1000 | Loss: 0.00001452
Iteration 24/1000 | Loss: 0.00001386
Iteration 25/1000 | Loss: 0.00001356
Iteration 26/1000 | Loss: 0.00001342
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001330
Iteration 29/1000 | Loss: 0.00001329
Iteration 30/1000 | Loss: 0.00001329
Iteration 31/1000 | Loss: 0.00001314
Iteration 32/1000 | Loss: 0.00001311
Iteration 33/1000 | Loss: 0.00001307
Iteration 34/1000 | Loss: 0.00001307
Iteration 35/1000 | Loss: 0.00001306
Iteration 36/1000 | Loss: 0.00001304
Iteration 37/1000 | Loss: 0.00001302
Iteration 38/1000 | Loss: 0.00001300
Iteration 39/1000 | Loss: 0.00001299
Iteration 40/1000 | Loss: 0.00001296
Iteration 41/1000 | Loss: 0.00001296
Iteration 42/1000 | Loss: 0.00001292
Iteration 43/1000 | Loss: 0.00001292
Iteration 44/1000 | Loss: 0.00001291
Iteration 45/1000 | Loss: 0.00001291
Iteration 46/1000 | Loss: 0.00001291
Iteration 47/1000 | Loss: 0.00001291
Iteration 48/1000 | Loss: 0.00001291
Iteration 49/1000 | Loss: 0.00001291
Iteration 50/1000 | Loss: 0.00001291
Iteration 51/1000 | Loss: 0.00001291
Iteration 52/1000 | Loss: 0.00001291
Iteration 53/1000 | Loss: 0.00001291
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001282
Iteration 56/1000 | Loss: 0.00001281
Iteration 57/1000 | Loss: 0.00001280
Iteration 58/1000 | Loss: 0.00001280
Iteration 59/1000 | Loss: 0.00001280
Iteration 60/1000 | Loss: 0.00001279
Iteration 61/1000 | Loss: 0.00001279
Iteration 62/1000 | Loss: 0.00001278
Iteration 63/1000 | Loss: 0.00001277
Iteration 64/1000 | Loss: 0.00001277
Iteration 65/1000 | Loss: 0.00001277
Iteration 66/1000 | Loss: 0.00001276
Iteration 67/1000 | Loss: 0.00001276
Iteration 68/1000 | Loss: 0.00001276
Iteration 69/1000 | Loss: 0.00001276
Iteration 70/1000 | Loss: 0.00001276
Iteration 71/1000 | Loss: 0.00001276
Iteration 72/1000 | Loss: 0.00001276
Iteration 73/1000 | Loss: 0.00001276
Iteration 74/1000 | Loss: 0.00001276
Iteration 75/1000 | Loss: 0.00001276
Iteration 76/1000 | Loss: 0.00001275
Iteration 77/1000 | Loss: 0.00001274
Iteration 78/1000 | Loss: 0.00001274
Iteration 79/1000 | Loss: 0.00001274
Iteration 80/1000 | Loss: 0.00001274
Iteration 81/1000 | Loss: 0.00001274
Iteration 82/1000 | Loss: 0.00001273
Iteration 83/1000 | Loss: 0.00001273
Iteration 84/1000 | Loss: 0.00001272
Iteration 85/1000 | Loss: 0.00001272
Iteration 86/1000 | Loss: 0.00001272
Iteration 87/1000 | Loss: 0.00001272
Iteration 88/1000 | Loss: 0.00001271
Iteration 89/1000 | Loss: 0.00001271
Iteration 90/1000 | Loss: 0.00001271
Iteration 91/1000 | Loss: 0.00001271
Iteration 92/1000 | Loss: 0.00001271
Iteration 93/1000 | Loss: 0.00001271
Iteration 94/1000 | Loss: 0.00001271
Iteration 95/1000 | Loss: 0.00001271
Iteration 96/1000 | Loss: 0.00001271
Iteration 97/1000 | Loss: 0.00001271
Iteration 98/1000 | Loss: 0.00001271
Iteration 99/1000 | Loss: 0.00001271
Iteration 100/1000 | Loss: 0.00001271
Iteration 101/1000 | Loss: 0.00001271
Iteration 102/1000 | Loss: 0.00001271
Iteration 103/1000 | Loss: 0.00001271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.2707509085885249e-05, 1.2707509085885249e-05, 1.2707509085885249e-05, 1.2707509085885249e-05, 1.2707509085885249e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2707509085885249e-05

Optimization complete. Final v2v error: 2.960995674133301 mm

Highest mean error: 4.206779956817627 mm for frame 63

Lowest mean error: 2.543166160583496 mm for frame 8

Saving results

Total time: 91.77456212043762
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393473
Iteration 2/25 | Loss: 0.00089690
Iteration 3/25 | Loss: 0.00079768
Iteration 4/25 | Loss: 0.00076078
Iteration 5/25 | Loss: 0.00074753
Iteration 6/25 | Loss: 0.00074499
Iteration 7/25 | Loss: 0.00074356
Iteration 8/25 | Loss: 0.00074352
Iteration 9/25 | Loss: 0.00074352
Iteration 10/25 | Loss: 0.00074352
Iteration 11/25 | Loss: 0.00074352
Iteration 12/25 | Loss: 0.00074352
Iteration 13/25 | Loss: 0.00074352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007435232400894165, 0.0007435232400894165, 0.0007435232400894165, 0.0007435232400894165, 0.0007435232400894165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007435232400894165

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.72175741
Iteration 2/25 | Loss: 0.00035183
Iteration 3/25 | Loss: 0.00035182
Iteration 4/25 | Loss: 0.00035182
Iteration 5/25 | Loss: 0.00035182
Iteration 6/25 | Loss: 0.00035182
Iteration 7/25 | Loss: 0.00035182
Iteration 8/25 | Loss: 0.00035182
Iteration 9/25 | Loss: 0.00035182
Iteration 10/25 | Loss: 0.00035182
Iteration 11/25 | Loss: 0.00035182
Iteration 12/25 | Loss: 0.00035182
Iteration 13/25 | Loss: 0.00035182
Iteration 14/25 | Loss: 0.00035182
Iteration 15/25 | Loss: 0.00035182
Iteration 16/25 | Loss: 0.00035182
Iteration 17/25 | Loss: 0.00035182
Iteration 18/25 | Loss: 0.00035182
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00035181790008209646, 0.00035181790008209646, 0.00035181790008209646, 0.00035181790008209646, 0.00035181790008209646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00035181790008209646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035182
Iteration 2/1000 | Loss: 0.00004359
Iteration 3/1000 | Loss: 0.00003552
Iteration 4/1000 | Loss: 0.00003376
Iteration 5/1000 | Loss: 0.00003239
Iteration 6/1000 | Loss: 0.00003163
Iteration 7/1000 | Loss: 0.00003098
Iteration 8/1000 | Loss: 0.00003064
Iteration 9/1000 | Loss: 0.00003036
Iteration 10/1000 | Loss: 0.00003016
Iteration 11/1000 | Loss: 0.00003009
Iteration 12/1000 | Loss: 0.00003005
Iteration 13/1000 | Loss: 0.00003000
Iteration 14/1000 | Loss: 0.00003000
Iteration 15/1000 | Loss: 0.00003000
Iteration 16/1000 | Loss: 0.00002998
Iteration 17/1000 | Loss: 0.00002998
Iteration 18/1000 | Loss: 0.00002996
Iteration 19/1000 | Loss: 0.00002996
Iteration 20/1000 | Loss: 0.00002996
Iteration 21/1000 | Loss: 0.00002995
Iteration 22/1000 | Loss: 0.00002992
Iteration 23/1000 | Loss: 0.00002992
Iteration 24/1000 | Loss: 0.00002992
Iteration 25/1000 | Loss: 0.00002991
Iteration 26/1000 | Loss: 0.00002991
Iteration 27/1000 | Loss: 0.00002990
Iteration 28/1000 | Loss: 0.00002990
Iteration 29/1000 | Loss: 0.00002990
Iteration 30/1000 | Loss: 0.00002990
Iteration 31/1000 | Loss: 0.00002990
Iteration 32/1000 | Loss: 0.00002989
Iteration 33/1000 | Loss: 0.00002989
Iteration 34/1000 | Loss: 0.00002987
Iteration 35/1000 | Loss: 0.00002986
Iteration 36/1000 | Loss: 0.00002982
Iteration 37/1000 | Loss: 0.00002982
Iteration 38/1000 | Loss: 0.00002981
Iteration 39/1000 | Loss: 0.00002981
Iteration 40/1000 | Loss: 0.00002979
Iteration 41/1000 | Loss: 0.00002979
Iteration 42/1000 | Loss: 0.00002979
Iteration 43/1000 | Loss: 0.00002978
Iteration 44/1000 | Loss: 0.00002976
Iteration 45/1000 | Loss: 0.00002976
Iteration 46/1000 | Loss: 0.00002976
Iteration 47/1000 | Loss: 0.00002976
Iteration 48/1000 | Loss: 0.00002976
Iteration 49/1000 | Loss: 0.00002975
Iteration 50/1000 | Loss: 0.00002975
Iteration 51/1000 | Loss: 0.00002975
Iteration 52/1000 | Loss: 0.00002975
Iteration 53/1000 | Loss: 0.00002975
Iteration 54/1000 | Loss: 0.00002975
Iteration 55/1000 | Loss: 0.00002975
Iteration 56/1000 | Loss: 0.00002975
Iteration 57/1000 | Loss: 0.00002973
Iteration 58/1000 | Loss: 0.00002973
Iteration 59/1000 | Loss: 0.00002973
Iteration 60/1000 | Loss: 0.00002973
Iteration 61/1000 | Loss: 0.00002973
Iteration 62/1000 | Loss: 0.00002973
Iteration 63/1000 | Loss: 0.00002972
Iteration 64/1000 | Loss: 0.00002972
Iteration 65/1000 | Loss: 0.00002972
Iteration 66/1000 | Loss: 0.00002971
Iteration 67/1000 | Loss: 0.00002971
Iteration 68/1000 | Loss: 0.00002970
Iteration 69/1000 | Loss: 0.00002970
Iteration 70/1000 | Loss: 0.00002970
Iteration 71/1000 | Loss: 0.00002969
Iteration 72/1000 | Loss: 0.00002969
Iteration 73/1000 | Loss: 0.00002969
Iteration 74/1000 | Loss: 0.00002969
Iteration 75/1000 | Loss: 0.00002969
Iteration 76/1000 | Loss: 0.00002968
Iteration 77/1000 | Loss: 0.00002968
Iteration 78/1000 | Loss: 0.00002968
Iteration 79/1000 | Loss: 0.00002967
Iteration 80/1000 | Loss: 0.00002967
Iteration 81/1000 | Loss: 0.00002967
Iteration 82/1000 | Loss: 0.00002967
Iteration 83/1000 | Loss: 0.00002967
Iteration 84/1000 | Loss: 0.00002967
Iteration 85/1000 | Loss: 0.00002967
Iteration 86/1000 | Loss: 0.00002967
Iteration 87/1000 | Loss: 0.00002966
Iteration 88/1000 | Loss: 0.00002966
Iteration 89/1000 | Loss: 0.00002966
Iteration 90/1000 | Loss: 0.00002966
Iteration 91/1000 | Loss: 0.00002966
Iteration 92/1000 | Loss: 0.00002966
Iteration 93/1000 | Loss: 0.00002966
Iteration 94/1000 | Loss: 0.00002965
Iteration 95/1000 | Loss: 0.00002965
Iteration 96/1000 | Loss: 0.00002965
Iteration 97/1000 | Loss: 0.00002965
Iteration 98/1000 | Loss: 0.00002965
Iteration 99/1000 | Loss: 0.00002965
Iteration 100/1000 | Loss: 0.00002965
Iteration 101/1000 | Loss: 0.00002965
Iteration 102/1000 | Loss: 0.00002965
Iteration 103/1000 | Loss: 0.00002965
Iteration 104/1000 | Loss: 0.00002965
Iteration 105/1000 | Loss: 0.00002965
Iteration 106/1000 | Loss: 0.00002965
Iteration 107/1000 | Loss: 0.00002965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.964696795970667e-05, 2.964696795970667e-05, 2.964696795970667e-05, 2.964696795970667e-05, 2.964696795970667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.964696795970667e-05

Optimization complete. Final v2v error: 4.533819675445557 mm

Highest mean error: 4.824212551116943 mm for frame 74

Lowest mean error: 4.277179718017578 mm for frame 10

Saving results

Total time: 36.81485438346863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00371675
Iteration 2/25 | Loss: 0.00069856
Iteration 3/25 | Loss: 0.00060009
Iteration 4/25 | Loss: 0.00058046
Iteration 5/25 | Loss: 0.00057402
Iteration 6/25 | Loss: 0.00057282
Iteration 7/25 | Loss: 0.00057274
Iteration 8/25 | Loss: 0.00057274
Iteration 9/25 | Loss: 0.00057274
Iteration 10/25 | Loss: 0.00057274
Iteration 11/25 | Loss: 0.00057274
Iteration 12/25 | Loss: 0.00057274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005727430689148605, 0.0005727430689148605, 0.0005727430689148605, 0.0005727430689148605, 0.0005727430689148605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005727430689148605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53419113
Iteration 2/25 | Loss: 0.00026123
Iteration 3/25 | Loss: 0.00026123
Iteration 4/25 | Loss: 0.00026123
Iteration 5/25 | Loss: 0.00026123
Iteration 6/25 | Loss: 0.00026123
Iteration 7/25 | Loss: 0.00026123
Iteration 8/25 | Loss: 0.00026123
Iteration 9/25 | Loss: 0.00026123
Iteration 10/25 | Loss: 0.00026123
Iteration 11/25 | Loss: 0.00026123
Iteration 12/25 | Loss: 0.00026123
Iteration 13/25 | Loss: 0.00026123
Iteration 14/25 | Loss: 0.00026123
Iteration 15/25 | Loss: 0.00026123
Iteration 16/25 | Loss: 0.00026123
Iteration 17/25 | Loss: 0.00026123
Iteration 18/25 | Loss: 0.00026123
Iteration 19/25 | Loss: 0.00026123
Iteration 20/25 | Loss: 0.00026123
Iteration 21/25 | Loss: 0.00026123
Iteration 22/25 | Loss: 0.00026123
Iteration 23/25 | Loss: 0.00026123
Iteration 24/25 | Loss: 0.00026123
Iteration 25/25 | Loss: 0.00026123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026123
Iteration 2/1000 | Loss: 0.00001888
Iteration 3/1000 | Loss: 0.00001326
Iteration 4/1000 | Loss: 0.00001231
Iteration 5/1000 | Loss: 0.00001172
Iteration 6/1000 | Loss: 0.00001128
Iteration 7/1000 | Loss: 0.00001125
Iteration 8/1000 | Loss: 0.00001115
Iteration 9/1000 | Loss: 0.00001106
Iteration 10/1000 | Loss: 0.00001102
Iteration 11/1000 | Loss: 0.00001098
Iteration 12/1000 | Loss: 0.00001096
Iteration 13/1000 | Loss: 0.00001096
Iteration 14/1000 | Loss: 0.00001095
Iteration 15/1000 | Loss: 0.00001093
Iteration 16/1000 | Loss: 0.00001084
Iteration 17/1000 | Loss: 0.00001079
Iteration 18/1000 | Loss: 0.00001079
Iteration 19/1000 | Loss: 0.00001079
Iteration 20/1000 | Loss: 0.00001079
Iteration 21/1000 | Loss: 0.00001079
Iteration 22/1000 | Loss: 0.00001079
Iteration 23/1000 | Loss: 0.00001078
Iteration 24/1000 | Loss: 0.00001078
Iteration 25/1000 | Loss: 0.00001078
Iteration 26/1000 | Loss: 0.00001078
Iteration 27/1000 | Loss: 0.00001078
Iteration 28/1000 | Loss: 0.00001078
Iteration 29/1000 | Loss: 0.00001078
Iteration 30/1000 | Loss: 0.00001078
Iteration 31/1000 | Loss: 0.00001078
Iteration 32/1000 | Loss: 0.00001078
Iteration 33/1000 | Loss: 0.00001078
Iteration 34/1000 | Loss: 0.00001078
Iteration 35/1000 | Loss: 0.00001077
Iteration 36/1000 | Loss: 0.00001077
Iteration 37/1000 | Loss: 0.00001077
Iteration 38/1000 | Loss: 0.00001076
Iteration 39/1000 | Loss: 0.00001075
Iteration 40/1000 | Loss: 0.00001075
Iteration 41/1000 | Loss: 0.00001074
Iteration 42/1000 | Loss: 0.00001074
Iteration 43/1000 | Loss: 0.00001074
Iteration 44/1000 | Loss: 0.00001074
Iteration 45/1000 | Loss: 0.00001074
Iteration 46/1000 | Loss: 0.00001074
Iteration 47/1000 | Loss: 0.00001073
Iteration 48/1000 | Loss: 0.00001073
Iteration 49/1000 | Loss: 0.00001073
Iteration 50/1000 | Loss: 0.00001072
Iteration 51/1000 | Loss: 0.00001072
Iteration 52/1000 | Loss: 0.00001072
Iteration 53/1000 | Loss: 0.00001071
Iteration 54/1000 | Loss: 0.00001071
Iteration 55/1000 | Loss: 0.00001068
Iteration 56/1000 | Loss: 0.00001066
Iteration 57/1000 | Loss: 0.00001065
Iteration 58/1000 | Loss: 0.00001065
Iteration 59/1000 | Loss: 0.00001065
Iteration 60/1000 | Loss: 0.00001065
Iteration 61/1000 | Loss: 0.00001065
Iteration 62/1000 | Loss: 0.00001065
Iteration 63/1000 | Loss: 0.00001065
Iteration 64/1000 | Loss: 0.00001065
Iteration 65/1000 | Loss: 0.00001065
Iteration 66/1000 | Loss: 0.00001065
Iteration 67/1000 | Loss: 0.00001064
Iteration 68/1000 | Loss: 0.00001064
Iteration 69/1000 | Loss: 0.00001064
Iteration 70/1000 | Loss: 0.00001063
Iteration 71/1000 | Loss: 0.00001063
Iteration 72/1000 | Loss: 0.00001063
Iteration 73/1000 | Loss: 0.00001063
Iteration 74/1000 | Loss: 0.00001063
Iteration 75/1000 | Loss: 0.00001062
Iteration 76/1000 | Loss: 0.00001062
Iteration 77/1000 | Loss: 0.00001062
Iteration 78/1000 | Loss: 0.00001062
Iteration 79/1000 | Loss: 0.00001062
Iteration 80/1000 | Loss: 0.00001062
Iteration 81/1000 | Loss: 0.00001061
Iteration 82/1000 | Loss: 0.00001061
Iteration 83/1000 | Loss: 0.00001061
Iteration 84/1000 | Loss: 0.00001061
Iteration 85/1000 | Loss: 0.00001060
Iteration 86/1000 | Loss: 0.00001060
Iteration 87/1000 | Loss: 0.00001060
Iteration 88/1000 | Loss: 0.00001060
Iteration 89/1000 | Loss: 0.00001060
Iteration 90/1000 | Loss: 0.00001060
Iteration 91/1000 | Loss: 0.00001059
Iteration 92/1000 | Loss: 0.00001059
Iteration 93/1000 | Loss: 0.00001059
Iteration 94/1000 | Loss: 0.00001059
Iteration 95/1000 | Loss: 0.00001059
Iteration 96/1000 | Loss: 0.00001059
Iteration 97/1000 | Loss: 0.00001059
Iteration 98/1000 | Loss: 0.00001059
Iteration 99/1000 | Loss: 0.00001059
Iteration 100/1000 | Loss: 0.00001059
Iteration 101/1000 | Loss: 0.00001059
Iteration 102/1000 | Loss: 0.00001058
Iteration 103/1000 | Loss: 0.00001058
Iteration 104/1000 | Loss: 0.00001058
Iteration 105/1000 | Loss: 0.00001058
Iteration 106/1000 | Loss: 0.00001058
Iteration 107/1000 | Loss: 0.00001058
Iteration 108/1000 | Loss: 0.00001058
Iteration 109/1000 | Loss: 0.00001058
Iteration 110/1000 | Loss: 0.00001058
Iteration 111/1000 | Loss: 0.00001058
Iteration 112/1000 | Loss: 0.00001058
Iteration 113/1000 | Loss: 0.00001057
Iteration 114/1000 | Loss: 0.00001057
Iteration 115/1000 | Loss: 0.00001057
Iteration 116/1000 | Loss: 0.00001057
Iteration 117/1000 | Loss: 0.00001056
Iteration 118/1000 | Loss: 0.00001056
Iteration 119/1000 | Loss: 0.00001056
Iteration 120/1000 | Loss: 0.00001056
Iteration 121/1000 | Loss: 0.00001056
Iteration 122/1000 | Loss: 0.00001055
Iteration 123/1000 | Loss: 0.00001055
Iteration 124/1000 | Loss: 0.00001055
Iteration 125/1000 | Loss: 0.00001054
Iteration 126/1000 | Loss: 0.00001054
Iteration 127/1000 | Loss: 0.00001054
Iteration 128/1000 | Loss: 0.00001054
Iteration 129/1000 | Loss: 0.00001054
Iteration 130/1000 | Loss: 0.00001054
Iteration 131/1000 | Loss: 0.00001054
Iteration 132/1000 | Loss: 0.00001053
Iteration 133/1000 | Loss: 0.00001053
Iteration 134/1000 | Loss: 0.00001053
Iteration 135/1000 | Loss: 0.00001052
Iteration 136/1000 | Loss: 0.00001051
Iteration 137/1000 | Loss: 0.00001051
Iteration 138/1000 | Loss: 0.00001051
Iteration 139/1000 | Loss: 0.00001050
Iteration 140/1000 | Loss: 0.00001050
Iteration 141/1000 | Loss: 0.00001050
Iteration 142/1000 | Loss: 0.00001050
Iteration 143/1000 | Loss: 0.00001050
Iteration 144/1000 | Loss: 0.00001050
Iteration 145/1000 | Loss: 0.00001050
Iteration 146/1000 | Loss: 0.00001050
Iteration 147/1000 | Loss: 0.00001049
Iteration 148/1000 | Loss: 0.00001049
Iteration 149/1000 | Loss: 0.00001049
Iteration 150/1000 | Loss: 0.00001049
Iteration 151/1000 | Loss: 0.00001049
Iteration 152/1000 | Loss: 0.00001049
Iteration 153/1000 | Loss: 0.00001049
Iteration 154/1000 | Loss: 0.00001049
Iteration 155/1000 | Loss: 0.00001049
Iteration 156/1000 | Loss: 0.00001049
Iteration 157/1000 | Loss: 0.00001048
Iteration 158/1000 | Loss: 0.00001048
Iteration 159/1000 | Loss: 0.00001048
Iteration 160/1000 | Loss: 0.00001048
Iteration 161/1000 | Loss: 0.00001048
Iteration 162/1000 | Loss: 0.00001048
Iteration 163/1000 | Loss: 0.00001048
Iteration 164/1000 | Loss: 0.00001048
Iteration 165/1000 | Loss: 0.00001048
Iteration 166/1000 | Loss: 0.00001048
Iteration 167/1000 | Loss: 0.00001048
Iteration 168/1000 | Loss: 0.00001048
Iteration 169/1000 | Loss: 0.00001048
Iteration 170/1000 | Loss: 0.00001048
Iteration 171/1000 | Loss: 0.00001047
Iteration 172/1000 | Loss: 0.00001047
Iteration 173/1000 | Loss: 0.00001047
Iteration 174/1000 | Loss: 0.00001047
Iteration 175/1000 | Loss: 0.00001047
Iteration 176/1000 | Loss: 0.00001047
Iteration 177/1000 | Loss: 0.00001047
Iteration 178/1000 | Loss: 0.00001047
Iteration 179/1000 | Loss: 0.00001047
Iteration 180/1000 | Loss: 0.00001047
Iteration 181/1000 | Loss: 0.00001047
Iteration 182/1000 | Loss: 0.00001047
Iteration 183/1000 | Loss: 0.00001046
Iteration 184/1000 | Loss: 0.00001046
Iteration 185/1000 | Loss: 0.00001046
Iteration 186/1000 | Loss: 0.00001046
Iteration 187/1000 | Loss: 0.00001046
Iteration 188/1000 | Loss: 0.00001046
Iteration 189/1000 | Loss: 0.00001046
Iteration 190/1000 | Loss: 0.00001046
Iteration 191/1000 | Loss: 0.00001046
Iteration 192/1000 | Loss: 0.00001046
Iteration 193/1000 | Loss: 0.00001046
Iteration 194/1000 | Loss: 0.00001046
Iteration 195/1000 | Loss: 0.00001046
Iteration 196/1000 | Loss: 0.00001046
Iteration 197/1000 | Loss: 0.00001046
Iteration 198/1000 | Loss: 0.00001046
Iteration 199/1000 | Loss: 0.00001046
Iteration 200/1000 | Loss: 0.00001046
Iteration 201/1000 | Loss: 0.00001046
Iteration 202/1000 | Loss: 0.00001046
Iteration 203/1000 | Loss: 0.00001046
Iteration 204/1000 | Loss: 0.00001046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.0456836207595188e-05, 1.0456836207595188e-05, 1.0456836207595188e-05, 1.0456836207595188e-05, 1.0456836207595188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0456836207595188e-05

Optimization complete. Final v2v error: 2.767751932144165 mm

Highest mean error: 2.8754074573516846 mm for frame 34

Lowest mean error: 2.674586057662964 mm for frame 161

Saving results

Total time: 35.16637992858887
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00740749
Iteration 2/25 | Loss: 0.00118478
Iteration 3/25 | Loss: 0.00086251
Iteration 4/25 | Loss: 0.00080572
Iteration 5/25 | Loss: 0.00078378
Iteration 6/25 | Loss: 0.00077963
Iteration 7/25 | Loss: 0.00077655
Iteration 8/25 | Loss: 0.00077574
Iteration 9/25 | Loss: 0.00077487
Iteration 10/25 | Loss: 0.00077421
Iteration 11/25 | Loss: 0.00077641
Iteration 12/25 | Loss: 0.00077085
Iteration 13/25 | Loss: 0.00076909
Iteration 14/25 | Loss: 0.00076839
Iteration 15/25 | Loss: 0.00077436
Iteration 16/25 | Loss: 0.00077487
Iteration 17/25 | Loss: 0.00076722
Iteration 18/25 | Loss: 0.00076406
Iteration 19/25 | Loss: 0.00076282
Iteration 20/25 | Loss: 0.00076232
Iteration 21/25 | Loss: 0.00076207
Iteration 22/25 | Loss: 0.00076188
Iteration 23/25 | Loss: 0.00076182
Iteration 24/25 | Loss: 0.00076181
Iteration 25/25 | Loss: 0.00076181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34459722
Iteration 2/25 | Loss: 0.00067428
Iteration 3/25 | Loss: 0.00067423
Iteration 4/25 | Loss: 0.00067423
Iteration 5/25 | Loss: 0.00067423
Iteration 6/25 | Loss: 0.00067423
Iteration 7/25 | Loss: 0.00067423
Iteration 8/25 | Loss: 0.00067423
Iteration 9/25 | Loss: 0.00067423
Iteration 10/25 | Loss: 0.00067423
Iteration 11/25 | Loss: 0.00067423
Iteration 12/25 | Loss: 0.00067423
Iteration 13/25 | Loss: 0.00067423
Iteration 14/25 | Loss: 0.00067423
Iteration 15/25 | Loss: 0.00067423
Iteration 16/25 | Loss: 0.00067423
Iteration 17/25 | Loss: 0.00067423
Iteration 18/25 | Loss: 0.00067423
Iteration 19/25 | Loss: 0.00067423
Iteration 20/25 | Loss: 0.00067423
Iteration 21/25 | Loss: 0.00067423
Iteration 22/25 | Loss: 0.00067423
Iteration 23/25 | Loss: 0.00067423
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006742277764715254, 0.0006742277764715254, 0.0006742277764715254, 0.0006742277764715254, 0.0006742277764715254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006742277764715254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067423
Iteration 2/1000 | Loss: 0.00016913
Iteration 3/1000 | Loss: 0.00007704
Iteration 4/1000 | Loss: 0.00005567
Iteration 5/1000 | Loss: 0.00005083
Iteration 6/1000 | Loss: 0.00004837
Iteration 7/1000 | Loss: 0.00004638
Iteration 8/1000 | Loss: 0.00004538
Iteration 9/1000 | Loss: 0.00004440
Iteration 10/1000 | Loss: 0.00004355
Iteration 11/1000 | Loss: 0.00004249
Iteration 12/1000 | Loss: 0.00004148
Iteration 13/1000 | Loss: 0.00004054
Iteration 14/1000 | Loss: 0.00003973
Iteration 15/1000 | Loss: 0.00003883
Iteration 16/1000 | Loss: 0.00003837
Iteration 17/1000 | Loss: 0.00003796
Iteration 18/1000 | Loss: 0.00089884
Iteration 19/1000 | Loss: 0.00116806
Iteration 20/1000 | Loss: 0.00211438
Iteration 21/1000 | Loss: 0.00248501
Iteration 22/1000 | Loss: 0.00259149
Iteration 23/1000 | Loss: 0.00127314
Iteration 24/1000 | Loss: 0.00172122
Iteration 25/1000 | Loss: 0.00058498
Iteration 26/1000 | Loss: 0.00034375
Iteration 27/1000 | Loss: 0.00007802
Iteration 28/1000 | Loss: 0.00006066
Iteration 29/1000 | Loss: 0.00005307
Iteration 30/1000 | Loss: 0.00010136
Iteration 31/1000 | Loss: 0.00004414
Iteration 32/1000 | Loss: 0.00003991
Iteration 33/1000 | Loss: 0.00003633
Iteration 34/1000 | Loss: 0.00008622
Iteration 35/1000 | Loss: 0.00003508
Iteration 36/1000 | Loss: 0.00047059
Iteration 37/1000 | Loss: 0.00003359
Iteration 38/1000 | Loss: 0.00002977
Iteration 39/1000 | Loss: 0.00002871
Iteration 40/1000 | Loss: 0.00002793
Iteration 41/1000 | Loss: 0.00002714
Iteration 42/1000 | Loss: 0.00002663
Iteration 43/1000 | Loss: 0.00002638
Iteration 44/1000 | Loss: 0.00002624
Iteration 45/1000 | Loss: 0.00002622
Iteration 46/1000 | Loss: 0.00002615
Iteration 47/1000 | Loss: 0.00002610
Iteration 48/1000 | Loss: 0.00002598
Iteration 49/1000 | Loss: 0.00002595
Iteration 50/1000 | Loss: 0.00002595
Iteration 51/1000 | Loss: 0.00002595
Iteration 52/1000 | Loss: 0.00002594
Iteration 53/1000 | Loss: 0.00002594
Iteration 54/1000 | Loss: 0.00002592
Iteration 55/1000 | Loss: 0.00002592
Iteration 56/1000 | Loss: 0.00002591
Iteration 57/1000 | Loss: 0.00002591
Iteration 58/1000 | Loss: 0.00002590
Iteration 59/1000 | Loss: 0.00002590
Iteration 60/1000 | Loss: 0.00002590
Iteration 61/1000 | Loss: 0.00002587
Iteration 62/1000 | Loss: 0.00002587
Iteration 63/1000 | Loss: 0.00002587
Iteration 64/1000 | Loss: 0.00002587
Iteration 65/1000 | Loss: 0.00002586
Iteration 66/1000 | Loss: 0.00002585
Iteration 67/1000 | Loss: 0.00002584
Iteration 68/1000 | Loss: 0.00002584
Iteration 69/1000 | Loss: 0.00002584
Iteration 70/1000 | Loss: 0.00002584
Iteration 71/1000 | Loss: 0.00002584
Iteration 72/1000 | Loss: 0.00002584
Iteration 73/1000 | Loss: 0.00002584
Iteration 74/1000 | Loss: 0.00002584
Iteration 75/1000 | Loss: 0.00002584
Iteration 76/1000 | Loss: 0.00002584
Iteration 77/1000 | Loss: 0.00002583
Iteration 78/1000 | Loss: 0.00002583
Iteration 79/1000 | Loss: 0.00002583
Iteration 80/1000 | Loss: 0.00002583
Iteration 81/1000 | Loss: 0.00002583
Iteration 82/1000 | Loss: 0.00002583
Iteration 83/1000 | Loss: 0.00002583
Iteration 84/1000 | Loss: 0.00002583
Iteration 85/1000 | Loss: 0.00002583
Iteration 86/1000 | Loss: 0.00002582
Iteration 87/1000 | Loss: 0.00002582
Iteration 88/1000 | Loss: 0.00002581
Iteration 89/1000 | Loss: 0.00002581
Iteration 90/1000 | Loss: 0.00002581
Iteration 91/1000 | Loss: 0.00002581
Iteration 92/1000 | Loss: 0.00002581
Iteration 93/1000 | Loss: 0.00002581
Iteration 94/1000 | Loss: 0.00002581
Iteration 95/1000 | Loss: 0.00002581
Iteration 96/1000 | Loss: 0.00002581
Iteration 97/1000 | Loss: 0.00002580
Iteration 98/1000 | Loss: 0.00002580
Iteration 99/1000 | Loss: 0.00002580
Iteration 100/1000 | Loss: 0.00002579
Iteration 101/1000 | Loss: 0.00002579
Iteration 102/1000 | Loss: 0.00002579
Iteration 103/1000 | Loss: 0.00002579
Iteration 104/1000 | Loss: 0.00002579
Iteration 105/1000 | Loss: 0.00002579
Iteration 106/1000 | Loss: 0.00002579
Iteration 107/1000 | Loss: 0.00002578
Iteration 108/1000 | Loss: 0.00002578
Iteration 109/1000 | Loss: 0.00002578
Iteration 110/1000 | Loss: 0.00002577
Iteration 111/1000 | Loss: 0.00002577
Iteration 112/1000 | Loss: 0.00002577
Iteration 113/1000 | Loss: 0.00002577
Iteration 114/1000 | Loss: 0.00002576
Iteration 115/1000 | Loss: 0.00002576
Iteration 116/1000 | Loss: 0.00002576
Iteration 117/1000 | Loss: 0.00002576
Iteration 118/1000 | Loss: 0.00002575
Iteration 119/1000 | Loss: 0.00002575
Iteration 120/1000 | Loss: 0.00002575
Iteration 121/1000 | Loss: 0.00002574
Iteration 122/1000 | Loss: 0.00002574
Iteration 123/1000 | Loss: 0.00002574
Iteration 124/1000 | Loss: 0.00002574
Iteration 125/1000 | Loss: 0.00002574
Iteration 126/1000 | Loss: 0.00002574
Iteration 127/1000 | Loss: 0.00002574
Iteration 128/1000 | Loss: 0.00002574
Iteration 129/1000 | Loss: 0.00002574
Iteration 130/1000 | Loss: 0.00002574
Iteration 131/1000 | Loss: 0.00002573
Iteration 132/1000 | Loss: 0.00002573
Iteration 133/1000 | Loss: 0.00002573
Iteration 134/1000 | Loss: 0.00002573
Iteration 135/1000 | Loss: 0.00002573
Iteration 136/1000 | Loss: 0.00002573
Iteration 137/1000 | Loss: 0.00002573
Iteration 138/1000 | Loss: 0.00002573
Iteration 139/1000 | Loss: 0.00002573
Iteration 140/1000 | Loss: 0.00002572
Iteration 141/1000 | Loss: 0.00002572
Iteration 142/1000 | Loss: 0.00002572
Iteration 143/1000 | Loss: 0.00002572
Iteration 144/1000 | Loss: 0.00002572
Iteration 145/1000 | Loss: 0.00002572
Iteration 146/1000 | Loss: 0.00002572
Iteration 147/1000 | Loss: 0.00002572
Iteration 148/1000 | Loss: 0.00002572
Iteration 149/1000 | Loss: 0.00002572
Iteration 150/1000 | Loss: 0.00002572
Iteration 151/1000 | Loss: 0.00002572
Iteration 152/1000 | Loss: 0.00002572
Iteration 153/1000 | Loss: 0.00002572
Iteration 154/1000 | Loss: 0.00002572
Iteration 155/1000 | Loss: 0.00002572
Iteration 156/1000 | Loss: 0.00002571
Iteration 157/1000 | Loss: 0.00002571
Iteration 158/1000 | Loss: 0.00002571
Iteration 159/1000 | Loss: 0.00002571
Iteration 160/1000 | Loss: 0.00002571
Iteration 161/1000 | Loss: 0.00002570
Iteration 162/1000 | Loss: 0.00002570
Iteration 163/1000 | Loss: 0.00002570
Iteration 164/1000 | Loss: 0.00002570
Iteration 165/1000 | Loss: 0.00002570
Iteration 166/1000 | Loss: 0.00002570
Iteration 167/1000 | Loss: 0.00002570
Iteration 168/1000 | Loss: 0.00002570
Iteration 169/1000 | Loss: 0.00002570
Iteration 170/1000 | Loss: 0.00002569
Iteration 171/1000 | Loss: 0.00002569
Iteration 172/1000 | Loss: 0.00002569
Iteration 173/1000 | Loss: 0.00002569
Iteration 174/1000 | Loss: 0.00002569
Iteration 175/1000 | Loss: 0.00002569
Iteration 176/1000 | Loss: 0.00002569
Iteration 177/1000 | Loss: 0.00002569
Iteration 178/1000 | Loss: 0.00002569
Iteration 179/1000 | Loss: 0.00002569
Iteration 180/1000 | Loss: 0.00002569
Iteration 181/1000 | Loss: 0.00002569
Iteration 182/1000 | Loss: 0.00002569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.5692886993056163e-05, 2.5692886993056163e-05, 2.5692886993056163e-05, 2.5692886993056163e-05, 2.5692886993056163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5692886993056163e-05

Optimization complete. Final v2v error: 3.7399520874023438 mm

Highest mean error: 12.250896453857422 mm for frame 142

Lowest mean error: 3.0627200603485107 mm for frame 8

Saving results

Total time: 130.25865125656128
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00659668
Iteration 2/25 | Loss: 0.00140375
Iteration 3/25 | Loss: 0.00088817
Iteration 4/25 | Loss: 0.00083627
Iteration 5/25 | Loss: 0.00082534
Iteration 6/25 | Loss: 0.00082412
Iteration 7/25 | Loss: 0.00082408
Iteration 8/25 | Loss: 0.00082408
Iteration 9/25 | Loss: 0.00082408
Iteration 10/25 | Loss: 0.00082408
Iteration 11/25 | Loss: 0.00082408
Iteration 12/25 | Loss: 0.00082408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008240762981586158, 0.0008240762981586158, 0.0008240762981586158, 0.0008240762981586158, 0.0008240762981586158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008240762981586158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53559220
Iteration 2/25 | Loss: 0.00048248
Iteration 3/25 | Loss: 0.00048245
Iteration 4/25 | Loss: 0.00048245
Iteration 5/25 | Loss: 0.00048245
Iteration 6/25 | Loss: 0.00048245
Iteration 7/25 | Loss: 0.00048245
Iteration 8/25 | Loss: 0.00048245
Iteration 9/25 | Loss: 0.00048245
Iteration 10/25 | Loss: 0.00048245
Iteration 11/25 | Loss: 0.00048245
Iteration 12/25 | Loss: 0.00048245
Iteration 13/25 | Loss: 0.00048245
Iteration 14/25 | Loss: 0.00048245
Iteration 15/25 | Loss: 0.00048245
Iteration 16/25 | Loss: 0.00048245
Iteration 17/25 | Loss: 0.00048245
Iteration 18/25 | Loss: 0.00048245
Iteration 19/25 | Loss: 0.00048245
Iteration 20/25 | Loss: 0.00048245
Iteration 21/25 | Loss: 0.00048245
Iteration 22/25 | Loss: 0.00048245
Iteration 23/25 | Loss: 0.00048245
Iteration 24/25 | Loss: 0.00048245
Iteration 25/25 | Loss: 0.00048245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048245
Iteration 2/1000 | Loss: 0.00004319
Iteration 3/1000 | Loss: 0.00002816
Iteration 4/1000 | Loss: 0.00002486
Iteration 5/1000 | Loss: 0.00002394
Iteration 6/1000 | Loss: 0.00002320
Iteration 7/1000 | Loss: 0.00002277
Iteration 8/1000 | Loss: 0.00002253
Iteration 9/1000 | Loss: 0.00002229
Iteration 10/1000 | Loss: 0.00002226
Iteration 11/1000 | Loss: 0.00002211
Iteration 12/1000 | Loss: 0.00002206
Iteration 13/1000 | Loss: 0.00002206
Iteration 14/1000 | Loss: 0.00002205
Iteration 15/1000 | Loss: 0.00002201
Iteration 16/1000 | Loss: 0.00002201
Iteration 17/1000 | Loss: 0.00002201
Iteration 18/1000 | Loss: 0.00002199
Iteration 19/1000 | Loss: 0.00002199
Iteration 20/1000 | Loss: 0.00002198
Iteration 21/1000 | Loss: 0.00002198
Iteration 22/1000 | Loss: 0.00002198
Iteration 23/1000 | Loss: 0.00002198
Iteration 24/1000 | Loss: 0.00002198
Iteration 25/1000 | Loss: 0.00002198
Iteration 26/1000 | Loss: 0.00002197
Iteration 27/1000 | Loss: 0.00002197
Iteration 28/1000 | Loss: 0.00002197
Iteration 29/1000 | Loss: 0.00002196
Iteration 30/1000 | Loss: 0.00002196
Iteration 31/1000 | Loss: 0.00002196
Iteration 32/1000 | Loss: 0.00002196
Iteration 33/1000 | Loss: 0.00002196
Iteration 34/1000 | Loss: 0.00002196
Iteration 35/1000 | Loss: 0.00002196
Iteration 36/1000 | Loss: 0.00002196
Iteration 37/1000 | Loss: 0.00002195
Iteration 38/1000 | Loss: 0.00002195
Iteration 39/1000 | Loss: 0.00002195
Iteration 40/1000 | Loss: 0.00002195
Iteration 41/1000 | Loss: 0.00002195
Iteration 42/1000 | Loss: 0.00002195
Iteration 43/1000 | Loss: 0.00002195
Iteration 44/1000 | Loss: 0.00002195
Iteration 45/1000 | Loss: 0.00002194
Iteration 46/1000 | Loss: 0.00002194
Iteration 47/1000 | Loss: 0.00002194
Iteration 48/1000 | Loss: 0.00002194
Iteration 49/1000 | Loss: 0.00002194
Iteration 50/1000 | Loss: 0.00002194
Iteration 51/1000 | Loss: 0.00002193
Iteration 52/1000 | Loss: 0.00002193
Iteration 53/1000 | Loss: 0.00002193
Iteration 54/1000 | Loss: 0.00002193
Iteration 55/1000 | Loss: 0.00002193
Iteration 56/1000 | Loss: 0.00002192
Iteration 57/1000 | Loss: 0.00002192
Iteration 58/1000 | Loss: 0.00002192
Iteration 59/1000 | Loss: 0.00002192
Iteration 60/1000 | Loss: 0.00002192
Iteration 61/1000 | Loss: 0.00002192
Iteration 62/1000 | Loss: 0.00002192
Iteration 63/1000 | Loss: 0.00002192
Iteration 64/1000 | Loss: 0.00002192
Iteration 65/1000 | Loss: 0.00002192
Iteration 66/1000 | Loss: 0.00002192
Iteration 67/1000 | Loss: 0.00002192
Iteration 68/1000 | Loss: 0.00002192
Iteration 69/1000 | Loss: 0.00002191
Iteration 70/1000 | Loss: 0.00002191
Iteration 71/1000 | Loss: 0.00002191
Iteration 72/1000 | Loss: 0.00002191
Iteration 73/1000 | Loss: 0.00002190
Iteration 74/1000 | Loss: 0.00002190
Iteration 75/1000 | Loss: 0.00002190
Iteration 76/1000 | Loss: 0.00002190
Iteration 77/1000 | Loss: 0.00002190
Iteration 78/1000 | Loss: 0.00002190
Iteration 79/1000 | Loss: 0.00002190
Iteration 80/1000 | Loss: 0.00002190
Iteration 81/1000 | Loss: 0.00002190
Iteration 82/1000 | Loss: 0.00002190
Iteration 83/1000 | Loss: 0.00002190
Iteration 84/1000 | Loss: 0.00002190
Iteration 85/1000 | Loss: 0.00002190
Iteration 86/1000 | Loss: 0.00002190
Iteration 87/1000 | Loss: 0.00002189
Iteration 88/1000 | Loss: 0.00002189
Iteration 89/1000 | Loss: 0.00002189
Iteration 90/1000 | Loss: 0.00002189
Iteration 91/1000 | Loss: 0.00002188
Iteration 92/1000 | Loss: 0.00002188
Iteration 93/1000 | Loss: 0.00002188
Iteration 94/1000 | Loss: 0.00002188
Iteration 95/1000 | Loss: 0.00002188
Iteration 96/1000 | Loss: 0.00002188
Iteration 97/1000 | Loss: 0.00002187
Iteration 98/1000 | Loss: 0.00002187
Iteration 99/1000 | Loss: 0.00002187
Iteration 100/1000 | Loss: 0.00002187
Iteration 101/1000 | Loss: 0.00002187
Iteration 102/1000 | Loss: 0.00002187
Iteration 103/1000 | Loss: 0.00002187
Iteration 104/1000 | Loss: 0.00002187
Iteration 105/1000 | Loss: 0.00002187
Iteration 106/1000 | Loss: 0.00002187
Iteration 107/1000 | Loss: 0.00002186
Iteration 108/1000 | Loss: 0.00002186
Iteration 109/1000 | Loss: 0.00002186
Iteration 110/1000 | Loss: 0.00002186
Iteration 111/1000 | Loss: 0.00002186
Iteration 112/1000 | Loss: 0.00002186
Iteration 113/1000 | Loss: 0.00002186
Iteration 114/1000 | Loss: 0.00002186
Iteration 115/1000 | Loss: 0.00002186
Iteration 116/1000 | Loss: 0.00002186
Iteration 117/1000 | Loss: 0.00002185
Iteration 118/1000 | Loss: 0.00002185
Iteration 119/1000 | Loss: 0.00002185
Iteration 120/1000 | Loss: 0.00002185
Iteration 121/1000 | Loss: 0.00002185
Iteration 122/1000 | Loss: 0.00002185
Iteration 123/1000 | Loss: 0.00002185
Iteration 124/1000 | Loss: 0.00002185
Iteration 125/1000 | Loss: 0.00002185
Iteration 126/1000 | Loss: 0.00002184
Iteration 127/1000 | Loss: 0.00002184
Iteration 128/1000 | Loss: 0.00002184
Iteration 129/1000 | Loss: 0.00002184
Iteration 130/1000 | Loss: 0.00002184
Iteration 131/1000 | Loss: 0.00002184
Iteration 132/1000 | Loss: 0.00002184
Iteration 133/1000 | Loss: 0.00002184
Iteration 134/1000 | Loss: 0.00002184
Iteration 135/1000 | Loss: 0.00002184
Iteration 136/1000 | Loss: 0.00002184
Iteration 137/1000 | Loss: 0.00002184
Iteration 138/1000 | Loss: 0.00002184
Iteration 139/1000 | Loss: 0.00002183
Iteration 140/1000 | Loss: 0.00002183
Iteration 141/1000 | Loss: 0.00002183
Iteration 142/1000 | Loss: 0.00002183
Iteration 143/1000 | Loss: 0.00002183
Iteration 144/1000 | Loss: 0.00002183
Iteration 145/1000 | Loss: 0.00002183
Iteration 146/1000 | Loss: 0.00002183
Iteration 147/1000 | Loss: 0.00002183
Iteration 148/1000 | Loss: 0.00002183
Iteration 149/1000 | Loss: 0.00002183
Iteration 150/1000 | Loss: 0.00002183
Iteration 151/1000 | Loss: 0.00002183
Iteration 152/1000 | Loss: 0.00002183
Iteration 153/1000 | Loss: 0.00002183
Iteration 154/1000 | Loss: 0.00002183
Iteration 155/1000 | Loss: 0.00002183
Iteration 156/1000 | Loss: 0.00002182
Iteration 157/1000 | Loss: 0.00002182
Iteration 158/1000 | Loss: 0.00002182
Iteration 159/1000 | Loss: 0.00002182
Iteration 160/1000 | Loss: 0.00002182
Iteration 161/1000 | Loss: 0.00002182
Iteration 162/1000 | Loss: 0.00002182
Iteration 163/1000 | Loss: 0.00002182
Iteration 164/1000 | Loss: 0.00002182
Iteration 165/1000 | Loss: 0.00002182
Iteration 166/1000 | Loss: 0.00002182
Iteration 167/1000 | Loss: 0.00002182
Iteration 168/1000 | Loss: 0.00002182
Iteration 169/1000 | Loss: 0.00002182
Iteration 170/1000 | Loss: 0.00002182
Iteration 171/1000 | Loss: 0.00002182
Iteration 172/1000 | Loss: 0.00002182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.182051321142353e-05, 2.182051321142353e-05, 2.182051321142353e-05, 2.182051321142353e-05, 2.182051321142353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.182051321142353e-05

Optimization complete. Final v2v error: 3.726750612258911 mm

Highest mean error: 4.112412452697754 mm for frame 87

Lowest mean error: 3.480069637298584 mm for frame 121

Saving results

Total time: 35.08937382698059
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00916705
Iteration 2/25 | Loss: 0.00100718
Iteration 3/25 | Loss: 0.00083088
Iteration 4/25 | Loss: 0.00079155
Iteration 5/25 | Loss: 0.00077849
Iteration 6/25 | Loss: 0.00077578
Iteration 7/25 | Loss: 0.00077520
Iteration 8/25 | Loss: 0.00077520
Iteration 9/25 | Loss: 0.00077520
Iteration 10/25 | Loss: 0.00077520
Iteration 11/25 | Loss: 0.00077520
Iteration 12/25 | Loss: 0.00077520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007752010715194046, 0.0007752010715194046, 0.0007752010715194046, 0.0007752010715194046, 0.0007752010715194046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007752010715194046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39131331
Iteration 2/25 | Loss: 0.00021596
Iteration 3/25 | Loss: 0.00021589
Iteration 4/25 | Loss: 0.00021589
Iteration 5/25 | Loss: 0.00021589
Iteration 6/25 | Loss: 0.00021589
Iteration 7/25 | Loss: 0.00021589
Iteration 8/25 | Loss: 0.00021589
Iteration 9/25 | Loss: 0.00021589
Iteration 10/25 | Loss: 0.00021589
Iteration 11/25 | Loss: 0.00021589
Iteration 12/25 | Loss: 0.00021589
Iteration 13/25 | Loss: 0.00021589
Iteration 14/25 | Loss: 0.00021589
Iteration 15/25 | Loss: 0.00021589
Iteration 16/25 | Loss: 0.00021589
Iteration 17/25 | Loss: 0.00021589
Iteration 18/25 | Loss: 0.00021589
Iteration 19/25 | Loss: 0.00021589
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00021588572417385876, 0.00021588572417385876, 0.00021588572417385876, 0.00021588572417385876, 0.00021588572417385876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021588572417385876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021589
Iteration 2/1000 | Loss: 0.00004219
Iteration 3/1000 | Loss: 0.00003122
Iteration 4/1000 | Loss: 0.00002900
Iteration 5/1000 | Loss: 0.00002770
Iteration 6/1000 | Loss: 0.00002684
Iteration 7/1000 | Loss: 0.00002634
Iteration 8/1000 | Loss: 0.00002592
Iteration 9/1000 | Loss: 0.00002555
Iteration 10/1000 | Loss: 0.00002526
Iteration 11/1000 | Loss: 0.00002514
Iteration 12/1000 | Loss: 0.00002506
Iteration 13/1000 | Loss: 0.00002488
Iteration 14/1000 | Loss: 0.00002472
Iteration 15/1000 | Loss: 0.00002468
Iteration 16/1000 | Loss: 0.00002467
Iteration 17/1000 | Loss: 0.00002465
Iteration 18/1000 | Loss: 0.00002460
Iteration 19/1000 | Loss: 0.00002459
Iteration 20/1000 | Loss: 0.00002454
Iteration 21/1000 | Loss: 0.00002453
Iteration 22/1000 | Loss: 0.00002452
Iteration 23/1000 | Loss: 0.00002451
Iteration 24/1000 | Loss: 0.00002450
Iteration 25/1000 | Loss: 0.00002449
Iteration 26/1000 | Loss: 0.00002448
Iteration 27/1000 | Loss: 0.00002448
Iteration 28/1000 | Loss: 0.00002446
Iteration 29/1000 | Loss: 0.00002445
Iteration 30/1000 | Loss: 0.00002445
Iteration 31/1000 | Loss: 0.00002444
Iteration 32/1000 | Loss: 0.00002444
Iteration 33/1000 | Loss: 0.00002444
Iteration 34/1000 | Loss: 0.00002444
Iteration 35/1000 | Loss: 0.00002443
Iteration 36/1000 | Loss: 0.00002443
Iteration 37/1000 | Loss: 0.00002443
Iteration 38/1000 | Loss: 0.00002443
Iteration 39/1000 | Loss: 0.00002442
Iteration 40/1000 | Loss: 0.00002442
Iteration 41/1000 | Loss: 0.00002441
Iteration 42/1000 | Loss: 0.00002441
Iteration 43/1000 | Loss: 0.00002441
Iteration 44/1000 | Loss: 0.00002440
Iteration 45/1000 | Loss: 0.00002440
Iteration 46/1000 | Loss: 0.00002440
Iteration 47/1000 | Loss: 0.00002440
Iteration 48/1000 | Loss: 0.00002439
Iteration 49/1000 | Loss: 0.00002439
Iteration 50/1000 | Loss: 0.00002439
Iteration 51/1000 | Loss: 0.00002438
Iteration 52/1000 | Loss: 0.00002438
Iteration 53/1000 | Loss: 0.00002438
Iteration 54/1000 | Loss: 0.00002438
Iteration 55/1000 | Loss: 0.00002437
Iteration 56/1000 | Loss: 0.00002437
Iteration 57/1000 | Loss: 0.00002437
Iteration 58/1000 | Loss: 0.00002437
Iteration 59/1000 | Loss: 0.00002437
Iteration 60/1000 | Loss: 0.00002437
Iteration 61/1000 | Loss: 0.00002437
Iteration 62/1000 | Loss: 0.00002436
Iteration 63/1000 | Loss: 0.00002436
Iteration 64/1000 | Loss: 0.00002436
Iteration 65/1000 | Loss: 0.00002436
Iteration 66/1000 | Loss: 0.00002436
Iteration 67/1000 | Loss: 0.00002436
Iteration 68/1000 | Loss: 0.00002436
Iteration 69/1000 | Loss: 0.00002435
Iteration 70/1000 | Loss: 0.00002435
Iteration 71/1000 | Loss: 0.00002435
Iteration 72/1000 | Loss: 0.00002435
Iteration 73/1000 | Loss: 0.00002435
Iteration 74/1000 | Loss: 0.00002435
Iteration 75/1000 | Loss: 0.00002435
Iteration 76/1000 | Loss: 0.00002435
Iteration 77/1000 | Loss: 0.00002434
Iteration 78/1000 | Loss: 0.00002434
Iteration 79/1000 | Loss: 0.00002434
Iteration 80/1000 | Loss: 0.00002434
Iteration 81/1000 | Loss: 0.00002434
Iteration 82/1000 | Loss: 0.00002434
Iteration 83/1000 | Loss: 0.00002433
Iteration 84/1000 | Loss: 0.00002433
Iteration 85/1000 | Loss: 0.00002433
Iteration 86/1000 | Loss: 0.00002433
Iteration 87/1000 | Loss: 0.00002433
Iteration 88/1000 | Loss: 0.00002433
Iteration 89/1000 | Loss: 0.00002433
Iteration 90/1000 | Loss: 0.00002433
Iteration 91/1000 | Loss: 0.00002433
Iteration 92/1000 | Loss: 0.00002433
Iteration 93/1000 | Loss: 0.00002433
Iteration 94/1000 | Loss: 0.00002433
Iteration 95/1000 | Loss: 0.00002433
Iteration 96/1000 | Loss: 0.00002433
Iteration 97/1000 | Loss: 0.00002433
Iteration 98/1000 | Loss: 0.00002433
Iteration 99/1000 | Loss: 0.00002433
Iteration 100/1000 | Loss: 0.00002433
Iteration 101/1000 | Loss: 0.00002433
Iteration 102/1000 | Loss: 0.00002433
Iteration 103/1000 | Loss: 0.00002433
Iteration 104/1000 | Loss: 0.00002433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.4328386643901467e-05, 2.4328386643901467e-05, 2.4328386643901467e-05, 2.4328386643901467e-05, 2.4328386643901467e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4328386643901467e-05

Optimization complete. Final v2v error: 4.134698867797852 mm

Highest mean error: 4.4567036628723145 mm for frame 155

Lowest mean error: 3.7752363681793213 mm for frame 29

Saving results

Total time: 41.376264333724976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403594
Iteration 2/25 | Loss: 0.00078502
Iteration 3/25 | Loss: 0.00067785
Iteration 4/25 | Loss: 0.00064722
Iteration 5/25 | Loss: 0.00063362
Iteration 6/25 | Loss: 0.00063143
Iteration 7/25 | Loss: 0.00063086
Iteration 8/25 | Loss: 0.00063086
Iteration 9/25 | Loss: 0.00063086
Iteration 10/25 | Loss: 0.00063086
Iteration 11/25 | Loss: 0.00063086
Iteration 12/25 | Loss: 0.00063086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000630864582490176, 0.000630864582490176, 0.000630864582490176, 0.000630864582490176, 0.000630864582490176]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000630864582490176

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51948416
Iteration 2/25 | Loss: 0.00027511
Iteration 3/25 | Loss: 0.00027511
Iteration 4/25 | Loss: 0.00027511
Iteration 5/25 | Loss: 0.00027511
Iteration 6/25 | Loss: 0.00027511
Iteration 7/25 | Loss: 0.00027511
Iteration 8/25 | Loss: 0.00027511
Iteration 9/25 | Loss: 0.00027511
Iteration 10/25 | Loss: 0.00027511
Iteration 11/25 | Loss: 0.00027511
Iteration 12/25 | Loss: 0.00027511
Iteration 13/25 | Loss: 0.00027511
Iteration 14/25 | Loss: 0.00027511
Iteration 15/25 | Loss: 0.00027511
Iteration 16/25 | Loss: 0.00027511
Iteration 17/25 | Loss: 0.00027511
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00027510535437613726, 0.00027510535437613726, 0.00027510535437613726, 0.00027510535437613726, 0.00027510535437613726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027510535437613726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027511
Iteration 2/1000 | Loss: 0.00003021
Iteration 3/1000 | Loss: 0.00001953
Iteration 4/1000 | Loss: 0.00001821
Iteration 5/1000 | Loss: 0.00001714
Iteration 6/1000 | Loss: 0.00001672
Iteration 7/1000 | Loss: 0.00001621
Iteration 8/1000 | Loss: 0.00001620
Iteration 9/1000 | Loss: 0.00001600
Iteration 10/1000 | Loss: 0.00001582
Iteration 11/1000 | Loss: 0.00001577
Iteration 12/1000 | Loss: 0.00001576
Iteration 13/1000 | Loss: 0.00001576
Iteration 14/1000 | Loss: 0.00001575
Iteration 15/1000 | Loss: 0.00001574
Iteration 16/1000 | Loss: 0.00001573
Iteration 17/1000 | Loss: 0.00001573
Iteration 18/1000 | Loss: 0.00001573
Iteration 19/1000 | Loss: 0.00001573
Iteration 20/1000 | Loss: 0.00001572
Iteration 21/1000 | Loss: 0.00001572
Iteration 22/1000 | Loss: 0.00001571
Iteration 23/1000 | Loss: 0.00001568
Iteration 24/1000 | Loss: 0.00001568
Iteration 25/1000 | Loss: 0.00001566
Iteration 26/1000 | Loss: 0.00001565
Iteration 27/1000 | Loss: 0.00001564
Iteration 28/1000 | Loss: 0.00001564
Iteration 29/1000 | Loss: 0.00001563
Iteration 30/1000 | Loss: 0.00001562
Iteration 31/1000 | Loss: 0.00001561
Iteration 32/1000 | Loss: 0.00001560
Iteration 33/1000 | Loss: 0.00001559
Iteration 34/1000 | Loss: 0.00001559
Iteration 35/1000 | Loss: 0.00001559
Iteration 36/1000 | Loss: 0.00001558
Iteration 37/1000 | Loss: 0.00001556
Iteration 38/1000 | Loss: 0.00001555
Iteration 39/1000 | Loss: 0.00001554
Iteration 40/1000 | Loss: 0.00001554
Iteration 41/1000 | Loss: 0.00001554
Iteration 42/1000 | Loss: 0.00001553
Iteration 43/1000 | Loss: 0.00001553
Iteration 44/1000 | Loss: 0.00001553
Iteration 45/1000 | Loss: 0.00001552
Iteration 46/1000 | Loss: 0.00001552
Iteration 47/1000 | Loss: 0.00001551
Iteration 48/1000 | Loss: 0.00001551
Iteration 49/1000 | Loss: 0.00001551
Iteration 50/1000 | Loss: 0.00001551
Iteration 51/1000 | Loss: 0.00001551
Iteration 52/1000 | Loss: 0.00001551
Iteration 53/1000 | Loss: 0.00001551
Iteration 54/1000 | Loss: 0.00001551
Iteration 55/1000 | Loss: 0.00001551
Iteration 56/1000 | Loss: 0.00001551
Iteration 57/1000 | Loss: 0.00001550
Iteration 58/1000 | Loss: 0.00001550
Iteration 59/1000 | Loss: 0.00001549
Iteration 60/1000 | Loss: 0.00001549
Iteration 61/1000 | Loss: 0.00001549
Iteration 62/1000 | Loss: 0.00001549
Iteration 63/1000 | Loss: 0.00001549
Iteration 64/1000 | Loss: 0.00001549
Iteration 65/1000 | Loss: 0.00001549
Iteration 66/1000 | Loss: 0.00001549
Iteration 67/1000 | Loss: 0.00001548
Iteration 68/1000 | Loss: 0.00001548
Iteration 69/1000 | Loss: 0.00001548
Iteration 70/1000 | Loss: 0.00001548
Iteration 71/1000 | Loss: 0.00001547
Iteration 72/1000 | Loss: 0.00001547
Iteration 73/1000 | Loss: 0.00001547
Iteration 74/1000 | Loss: 0.00001546
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001546
Iteration 77/1000 | Loss: 0.00001545
Iteration 78/1000 | Loss: 0.00001545
Iteration 79/1000 | Loss: 0.00001545
Iteration 80/1000 | Loss: 0.00001545
Iteration 81/1000 | Loss: 0.00001545
Iteration 82/1000 | Loss: 0.00001545
Iteration 83/1000 | Loss: 0.00001545
Iteration 84/1000 | Loss: 0.00001544
Iteration 85/1000 | Loss: 0.00001543
Iteration 86/1000 | Loss: 0.00001543
Iteration 87/1000 | Loss: 0.00001543
Iteration 88/1000 | Loss: 0.00001542
Iteration 89/1000 | Loss: 0.00001542
Iteration 90/1000 | Loss: 0.00001542
Iteration 91/1000 | Loss: 0.00001541
Iteration 92/1000 | Loss: 0.00001541
Iteration 93/1000 | Loss: 0.00001538
Iteration 94/1000 | Loss: 0.00001538
Iteration 95/1000 | Loss: 0.00001537
Iteration 96/1000 | Loss: 0.00001537
Iteration 97/1000 | Loss: 0.00001537
Iteration 98/1000 | Loss: 0.00001536
Iteration 99/1000 | Loss: 0.00001536
Iteration 100/1000 | Loss: 0.00001536
Iteration 101/1000 | Loss: 0.00001535
Iteration 102/1000 | Loss: 0.00001535
Iteration 103/1000 | Loss: 0.00001535
Iteration 104/1000 | Loss: 0.00001535
Iteration 105/1000 | Loss: 0.00001534
Iteration 106/1000 | Loss: 0.00001534
Iteration 107/1000 | Loss: 0.00001534
Iteration 108/1000 | Loss: 0.00001534
Iteration 109/1000 | Loss: 0.00001533
Iteration 110/1000 | Loss: 0.00001533
Iteration 111/1000 | Loss: 0.00001532
Iteration 112/1000 | Loss: 0.00001532
Iteration 113/1000 | Loss: 0.00001532
Iteration 114/1000 | Loss: 0.00001531
Iteration 115/1000 | Loss: 0.00001531
Iteration 116/1000 | Loss: 0.00001531
Iteration 117/1000 | Loss: 0.00001531
Iteration 118/1000 | Loss: 0.00001531
Iteration 119/1000 | Loss: 0.00001531
Iteration 120/1000 | Loss: 0.00001531
Iteration 121/1000 | Loss: 0.00001531
Iteration 122/1000 | Loss: 0.00001531
Iteration 123/1000 | Loss: 0.00001531
Iteration 124/1000 | Loss: 0.00001530
Iteration 125/1000 | Loss: 0.00001530
Iteration 126/1000 | Loss: 0.00001530
Iteration 127/1000 | Loss: 0.00001530
Iteration 128/1000 | Loss: 0.00001530
Iteration 129/1000 | Loss: 0.00001530
Iteration 130/1000 | Loss: 0.00001530
Iteration 131/1000 | Loss: 0.00001530
Iteration 132/1000 | Loss: 0.00001530
Iteration 133/1000 | Loss: 0.00001530
Iteration 134/1000 | Loss: 0.00001530
Iteration 135/1000 | Loss: 0.00001530
Iteration 136/1000 | Loss: 0.00001530
Iteration 137/1000 | Loss: 0.00001530
Iteration 138/1000 | Loss: 0.00001530
Iteration 139/1000 | Loss: 0.00001529
Iteration 140/1000 | Loss: 0.00001529
Iteration 141/1000 | Loss: 0.00001529
Iteration 142/1000 | Loss: 0.00001529
Iteration 143/1000 | Loss: 0.00001529
Iteration 144/1000 | Loss: 0.00001529
Iteration 145/1000 | Loss: 0.00001529
Iteration 146/1000 | Loss: 0.00001529
Iteration 147/1000 | Loss: 0.00001529
Iteration 148/1000 | Loss: 0.00001529
Iteration 149/1000 | Loss: 0.00001529
Iteration 150/1000 | Loss: 0.00001529
Iteration 151/1000 | Loss: 0.00001529
Iteration 152/1000 | Loss: 0.00001529
Iteration 153/1000 | Loss: 0.00001529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.5291285308194347e-05, 1.5291285308194347e-05, 1.5291285308194347e-05, 1.5291285308194347e-05, 1.5291285308194347e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5291285308194347e-05

Optimization complete. Final v2v error: 3.3114709854125977 mm

Highest mean error: 3.776991844177246 mm for frame 80

Lowest mean error: 3.0434982776641846 mm for frame 88

Saving results

Total time: 34.55673050880432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00362984
Iteration 2/25 | Loss: 0.00069292
Iteration 3/25 | Loss: 0.00058351
Iteration 4/25 | Loss: 0.00056975
Iteration 5/25 | Loss: 0.00056519
Iteration 6/25 | Loss: 0.00056378
Iteration 7/25 | Loss: 0.00056349
Iteration 8/25 | Loss: 0.00056349
Iteration 9/25 | Loss: 0.00056349
Iteration 10/25 | Loss: 0.00056349
Iteration 11/25 | Loss: 0.00056349
Iteration 12/25 | Loss: 0.00056349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005634936387650669, 0.0005634936387650669, 0.0005634936387650669, 0.0005634936387650669, 0.0005634936387650669]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005634936387650669

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46395433
Iteration 2/25 | Loss: 0.00025388
Iteration 3/25 | Loss: 0.00025387
Iteration 4/25 | Loss: 0.00025387
Iteration 5/25 | Loss: 0.00025387
Iteration 6/25 | Loss: 0.00025387
Iteration 7/25 | Loss: 0.00025387
Iteration 8/25 | Loss: 0.00025387
Iteration 9/25 | Loss: 0.00025387
Iteration 10/25 | Loss: 0.00025387
Iteration 11/25 | Loss: 0.00025387
Iteration 12/25 | Loss: 0.00025387
Iteration 13/25 | Loss: 0.00025387
Iteration 14/25 | Loss: 0.00025387
Iteration 15/25 | Loss: 0.00025387
Iteration 16/25 | Loss: 0.00025387
Iteration 17/25 | Loss: 0.00025387
Iteration 18/25 | Loss: 0.00025387
Iteration 19/25 | Loss: 0.00025387
Iteration 20/25 | Loss: 0.00025387
Iteration 21/25 | Loss: 0.00025387
Iteration 22/25 | Loss: 0.00025387
Iteration 23/25 | Loss: 0.00025387
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002538688131608069, 0.0002538688131608069, 0.0002538688131608069, 0.0002538688131608069, 0.0002538688131608069]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002538688131608069

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025387
Iteration 2/1000 | Loss: 0.00001676
Iteration 3/1000 | Loss: 0.00001085
Iteration 4/1000 | Loss: 0.00001012
Iteration 5/1000 | Loss: 0.00000948
Iteration 6/1000 | Loss: 0.00000924
Iteration 7/1000 | Loss: 0.00000919
Iteration 8/1000 | Loss: 0.00000907
Iteration 9/1000 | Loss: 0.00000904
Iteration 10/1000 | Loss: 0.00000903
Iteration 11/1000 | Loss: 0.00000903
Iteration 12/1000 | Loss: 0.00000903
Iteration 13/1000 | Loss: 0.00000903
Iteration 14/1000 | Loss: 0.00000902
Iteration 15/1000 | Loss: 0.00000901
Iteration 16/1000 | Loss: 0.00000901
Iteration 17/1000 | Loss: 0.00000900
Iteration 18/1000 | Loss: 0.00000900
Iteration 19/1000 | Loss: 0.00000899
Iteration 20/1000 | Loss: 0.00000898
Iteration 21/1000 | Loss: 0.00000897
Iteration 22/1000 | Loss: 0.00000897
Iteration 23/1000 | Loss: 0.00000896
Iteration 24/1000 | Loss: 0.00000896
Iteration 25/1000 | Loss: 0.00000896
Iteration 26/1000 | Loss: 0.00000896
Iteration 27/1000 | Loss: 0.00000896
Iteration 28/1000 | Loss: 0.00000895
Iteration 29/1000 | Loss: 0.00000894
Iteration 30/1000 | Loss: 0.00000893
Iteration 31/1000 | Loss: 0.00000892
Iteration 32/1000 | Loss: 0.00000892
Iteration 33/1000 | Loss: 0.00000892
Iteration 34/1000 | Loss: 0.00000891
Iteration 35/1000 | Loss: 0.00000890
Iteration 36/1000 | Loss: 0.00000890
Iteration 37/1000 | Loss: 0.00000889
Iteration 38/1000 | Loss: 0.00000888
Iteration 39/1000 | Loss: 0.00000888
Iteration 40/1000 | Loss: 0.00000887
Iteration 41/1000 | Loss: 0.00000887
Iteration 42/1000 | Loss: 0.00000886
Iteration 43/1000 | Loss: 0.00000886
Iteration 44/1000 | Loss: 0.00000885
Iteration 45/1000 | Loss: 0.00000885
Iteration 46/1000 | Loss: 0.00000885
Iteration 47/1000 | Loss: 0.00000884
Iteration 48/1000 | Loss: 0.00000884
Iteration 49/1000 | Loss: 0.00000882
Iteration 50/1000 | Loss: 0.00000882
Iteration 51/1000 | Loss: 0.00000882
Iteration 52/1000 | Loss: 0.00000882
Iteration 53/1000 | Loss: 0.00000881
Iteration 54/1000 | Loss: 0.00000881
Iteration 55/1000 | Loss: 0.00000881
Iteration 56/1000 | Loss: 0.00000881
Iteration 57/1000 | Loss: 0.00000880
Iteration 58/1000 | Loss: 0.00000880
Iteration 59/1000 | Loss: 0.00000880
Iteration 60/1000 | Loss: 0.00000879
Iteration 61/1000 | Loss: 0.00000879
Iteration 62/1000 | Loss: 0.00000879
Iteration 63/1000 | Loss: 0.00000878
Iteration 64/1000 | Loss: 0.00000878
Iteration 65/1000 | Loss: 0.00000878
Iteration 66/1000 | Loss: 0.00000878
Iteration 67/1000 | Loss: 0.00000877
Iteration 68/1000 | Loss: 0.00000877
Iteration 69/1000 | Loss: 0.00000876
Iteration 70/1000 | Loss: 0.00000876
Iteration 71/1000 | Loss: 0.00000875
Iteration 72/1000 | Loss: 0.00000875
Iteration 73/1000 | Loss: 0.00000874
Iteration 74/1000 | Loss: 0.00000874
Iteration 75/1000 | Loss: 0.00000874
Iteration 76/1000 | Loss: 0.00000874
Iteration 77/1000 | Loss: 0.00000873
Iteration 78/1000 | Loss: 0.00000873
Iteration 79/1000 | Loss: 0.00000873
Iteration 80/1000 | Loss: 0.00000873
Iteration 81/1000 | Loss: 0.00000872
Iteration 82/1000 | Loss: 0.00000872
Iteration 83/1000 | Loss: 0.00000872
Iteration 84/1000 | Loss: 0.00000872
Iteration 85/1000 | Loss: 0.00000872
Iteration 86/1000 | Loss: 0.00000871
Iteration 87/1000 | Loss: 0.00000871
Iteration 88/1000 | Loss: 0.00000871
Iteration 89/1000 | Loss: 0.00000870
Iteration 90/1000 | Loss: 0.00000870
Iteration 91/1000 | Loss: 0.00000870
Iteration 92/1000 | Loss: 0.00000869
Iteration 93/1000 | Loss: 0.00000869
Iteration 94/1000 | Loss: 0.00000869
Iteration 95/1000 | Loss: 0.00000869
Iteration 96/1000 | Loss: 0.00000869
Iteration 97/1000 | Loss: 0.00000868
Iteration 98/1000 | Loss: 0.00000868
Iteration 99/1000 | Loss: 0.00000868
Iteration 100/1000 | Loss: 0.00000867
Iteration 101/1000 | Loss: 0.00000867
Iteration 102/1000 | Loss: 0.00000867
Iteration 103/1000 | Loss: 0.00000867
Iteration 104/1000 | Loss: 0.00000867
Iteration 105/1000 | Loss: 0.00000867
Iteration 106/1000 | Loss: 0.00000867
Iteration 107/1000 | Loss: 0.00000867
Iteration 108/1000 | Loss: 0.00000867
Iteration 109/1000 | Loss: 0.00000867
Iteration 110/1000 | Loss: 0.00000867
Iteration 111/1000 | Loss: 0.00000866
Iteration 112/1000 | Loss: 0.00000866
Iteration 113/1000 | Loss: 0.00000866
Iteration 114/1000 | Loss: 0.00000865
Iteration 115/1000 | Loss: 0.00000865
Iteration 116/1000 | Loss: 0.00000865
Iteration 117/1000 | Loss: 0.00000865
Iteration 118/1000 | Loss: 0.00000864
Iteration 119/1000 | Loss: 0.00000864
Iteration 120/1000 | Loss: 0.00000864
Iteration 121/1000 | Loss: 0.00000864
Iteration 122/1000 | Loss: 0.00000864
Iteration 123/1000 | Loss: 0.00000863
Iteration 124/1000 | Loss: 0.00000863
Iteration 125/1000 | Loss: 0.00000863
Iteration 126/1000 | Loss: 0.00000863
Iteration 127/1000 | Loss: 0.00000863
Iteration 128/1000 | Loss: 0.00000863
Iteration 129/1000 | Loss: 0.00000863
Iteration 130/1000 | Loss: 0.00000863
Iteration 131/1000 | Loss: 0.00000863
Iteration 132/1000 | Loss: 0.00000863
Iteration 133/1000 | Loss: 0.00000863
Iteration 134/1000 | Loss: 0.00000862
Iteration 135/1000 | Loss: 0.00000862
Iteration 136/1000 | Loss: 0.00000862
Iteration 137/1000 | Loss: 0.00000862
Iteration 138/1000 | Loss: 0.00000861
Iteration 139/1000 | Loss: 0.00000861
Iteration 140/1000 | Loss: 0.00000861
Iteration 141/1000 | Loss: 0.00000861
Iteration 142/1000 | Loss: 0.00000860
Iteration 143/1000 | Loss: 0.00000860
Iteration 144/1000 | Loss: 0.00000860
Iteration 145/1000 | Loss: 0.00000860
Iteration 146/1000 | Loss: 0.00000860
Iteration 147/1000 | Loss: 0.00000860
Iteration 148/1000 | Loss: 0.00000860
Iteration 149/1000 | Loss: 0.00000860
Iteration 150/1000 | Loss: 0.00000860
Iteration 151/1000 | Loss: 0.00000859
Iteration 152/1000 | Loss: 0.00000859
Iteration 153/1000 | Loss: 0.00000859
Iteration 154/1000 | Loss: 0.00000859
Iteration 155/1000 | Loss: 0.00000859
Iteration 156/1000 | Loss: 0.00000859
Iteration 157/1000 | Loss: 0.00000859
Iteration 158/1000 | Loss: 0.00000859
Iteration 159/1000 | Loss: 0.00000859
Iteration 160/1000 | Loss: 0.00000859
Iteration 161/1000 | Loss: 0.00000859
Iteration 162/1000 | Loss: 0.00000859
Iteration 163/1000 | Loss: 0.00000859
Iteration 164/1000 | Loss: 0.00000859
Iteration 165/1000 | Loss: 0.00000858
Iteration 166/1000 | Loss: 0.00000858
Iteration 167/1000 | Loss: 0.00000858
Iteration 168/1000 | Loss: 0.00000858
Iteration 169/1000 | Loss: 0.00000858
Iteration 170/1000 | Loss: 0.00000858
Iteration 171/1000 | Loss: 0.00000858
Iteration 172/1000 | Loss: 0.00000858
Iteration 173/1000 | Loss: 0.00000858
Iteration 174/1000 | Loss: 0.00000858
Iteration 175/1000 | Loss: 0.00000858
Iteration 176/1000 | Loss: 0.00000858
Iteration 177/1000 | Loss: 0.00000857
Iteration 178/1000 | Loss: 0.00000857
Iteration 179/1000 | Loss: 0.00000857
Iteration 180/1000 | Loss: 0.00000857
Iteration 181/1000 | Loss: 0.00000857
Iteration 182/1000 | Loss: 0.00000857
Iteration 183/1000 | Loss: 0.00000857
Iteration 184/1000 | Loss: 0.00000857
Iteration 185/1000 | Loss: 0.00000857
Iteration 186/1000 | Loss: 0.00000857
Iteration 187/1000 | Loss: 0.00000857
Iteration 188/1000 | Loss: 0.00000857
Iteration 189/1000 | Loss: 0.00000857
Iteration 190/1000 | Loss: 0.00000857
Iteration 191/1000 | Loss: 0.00000857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [8.570288628106937e-06, 8.570288628106937e-06, 8.570288628106937e-06, 8.570288628106937e-06, 8.570288628106937e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.570288628106937e-06

Optimization complete. Final v2v error: 2.506194829940796 mm

Highest mean error: 2.7101056575775146 mm for frame 79

Lowest mean error: 2.4489939212799072 mm for frame 116

Saving results

Total time: 33.52646064758301
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775000
Iteration 2/25 | Loss: 0.00160898
Iteration 3/25 | Loss: 0.00099580
Iteration 4/25 | Loss: 0.00081940
Iteration 5/25 | Loss: 0.00080734
Iteration 6/25 | Loss: 0.00080688
Iteration 7/25 | Loss: 0.00080688
Iteration 8/25 | Loss: 0.00080688
Iteration 9/25 | Loss: 0.00080688
Iteration 10/25 | Loss: 0.00080688
Iteration 11/25 | Loss: 0.00080688
Iteration 12/25 | Loss: 0.00080688
Iteration 13/25 | Loss: 0.00080688
Iteration 14/25 | Loss: 0.00080688
Iteration 15/25 | Loss: 0.00080688
Iteration 16/25 | Loss: 0.00080688
Iteration 17/25 | Loss: 0.00080688
Iteration 18/25 | Loss: 0.00080688
Iteration 19/25 | Loss: 0.00080688
Iteration 20/25 | Loss: 0.00080688
Iteration 21/25 | Loss: 0.00080688
Iteration 22/25 | Loss: 0.00080688
Iteration 23/25 | Loss: 0.00080688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00080687744775787, 0.00080687744775787, 0.00080687744775787, 0.00080687744775787, 0.00080687744775787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00080687744775787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49266386
Iteration 2/25 | Loss: 0.00029130
Iteration 3/25 | Loss: 0.00029128
Iteration 4/25 | Loss: 0.00029128
Iteration 5/25 | Loss: 0.00029127
Iteration 6/25 | Loss: 0.00029127
Iteration 7/25 | Loss: 0.00029127
Iteration 8/25 | Loss: 0.00029127
Iteration 9/25 | Loss: 0.00029127
Iteration 10/25 | Loss: 0.00029127
Iteration 11/25 | Loss: 0.00029127
Iteration 12/25 | Loss: 0.00029127
Iteration 13/25 | Loss: 0.00029127
Iteration 14/25 | Loss: 0.00029127
Iteration 15/25 | Loss: 0.00029127
Iteration 16/25 | Loss: 0.00029127
Iteration 17/25 | Loss: 0.00029127
Iteration 18/25 | Loss: 0.00029127
Iteration 19/25 | Loss: 0.00029127
Iteration 20/25 | Loss: 0.00029127
Iteration 21/25 | Loss: 0.00029127
Iteration 22/25 | Loss: 0.00029127
Iteration 23/25 | Loss: 0.00029127
Iteration 24/25 | Loss: 0.00029127
Iteration 25/25 | Loss: 0.00029127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029127
Iteration 2/1000 | Loss: 0.00003796
Iteration 3/1000 | Loss: 0.00002963
Iteration 4/1000 | Loss: 0.00002705
Iteration 5/1000 | Loss: 0.00002574
Iteration 6/1000 | Loss: 0.00002517
Iteration 7/1000 | Loss: 0.00002469
Iteration 8/1000 | Loss: 0.00002428
Iteration 9/1000 | Loss: 0.00002391
Iteration 10/1000 | Loss: 0.00002368
Iteration 11/1000 | Loss: 0.00002359
Iteration 12/1000 | Loss: 0.00002358
Iteration 13/1000 | Loss: 0.00002353
Iteration 14/1000 | Loss: 0.00002339
Iteration 15/1000 | Loss: 0.00002336
Iteration 16/1000 | Loss: 0.00002330
Iteration 17/1000 | Loss: 0.00002329
Iteration 18/1000 | Loss: 0.00002328
Iteration 19/1000 | Loss: 0.00002328
Iteration 20/1000 | Loss: 0.00002328
Iteration 21/1000 | Loss: 0.00002328
Iteration 22/1000 | Loss: 0.00002327
Iteration 23/1000 | Loss: 0.00002322
Iteration 24/1000 | Loss: 0.00002322
Iteration 25/1000 | Loss: 0.00002321
Iteration 26/1000 | Loss: 0.00002320
Iteration 27/1000 | Loss: 0.00002320
Iteration 28/1000 | Loss: 0.00002320
Iteration 29/1000 | Loss: 0.00002320
Iteration 30/1000 | Loss: 0.00002320
Iteration 31/1000 | Loss: 0.00002320
Iteration 32/1000 | Loss: 0.00002320
Iteration 33/1000 | Loss: 0.00002320
Iteration 34/1000 | Loss: 0.00002320
Iteration 35/1000 | Loss: 0.00002320
Iteration 36/1000 | Loss: 0.00002319
Iteration 37/1000 | Loss: 0.00002319
Iteration 38/1000 | Loss: 0.00002319
Iteration 39/1000 | Loss: 0.00002319
Iteration 40/1000 | Loss: 0.00002319
Iteration 41/1000 | Loss: 0.00002318
Iteration 42/1000 | Loss: 0.00002318
Iteration 43/1000 | Loss: 0.00002318
Iteration 44/1000 | Loss: 0.00002318
Iteration 45/1000 | Loss: 0.00002318
Iteration 46/1000 | Loss: 0.00002318
Iteration 47/1000 | Loss: 0.00002318
Iteration 48/1000 | Loss: 0.00002318
Iteration 49/1000 | Loss: 0.00002318
Iteration 50/1000 | Loss: 0.00002318
Iteration 51/1000 | Loss: 0.00002318
Iteration 52/1000 | Loss: 0.00002318
Iteration 53/1000 | Loss: 0.00002318
Iteration 54/1000 | Loss: 0.00002318
Iteration 55/1000 | Loss: 0.00002318
Iteration 56/1000 | Loss: 0.00002318
Iteration 57/1000 | Loss: 0.00002318
Iteration 58/1000 | Loss: 0.00002318
Iteration 59/1000 | Loss: 0.00002318
Iteration 60/1000 | Loss: 0.00002317
Iteration 61/1000 | Loss: 0.00002317
Iteration 62/1000 | Loss: 0.00002317
Iteration 63/1000 | Loss: 0.00002317
Iteration 64/1000 | Loss: 0.00002317
Iteration 65/1000 | Loss: 0.00002317
Iteration 66/1000 | Loss: 0.00002317
Iteration 67/1000 | Loss: 0.00002317
Iteration 68/1000 | Loss: 0.00002317
Iteration 69/1000 | Loss: 0.00002317
Iteration 70/1000 | Loss: 0.00002317
Iteration 71/1000 | Loss: 0.00002317
Iteration 72/1000 | Loss: 0.00002316
Iteration 73/1000 | Loss: 0.00002316
Iteration 74/1000 | Loss: 0.00002316
Iteration 75/1000 | Loss: 0.00002316
Iteration 76/1000 | Loss: 0.00002316
Iteration 77/1000 | Loss: 0.00002316
Iteration 78/1000 | Loss: 0.00002316
Iteration 79/1000 | Loss: 0.00002316
Iteration 80/1000 | Loss: 0.00002316
Iteration 81/1000 | Loss: 0.00002316
Iteration 82/1000 | Loss: 0.00002316
Iteration 83/1000 | Loss: 0.00002316
Iteration 84/1000 | Loss: 0.00002315
Iteration 85/1000 | Loss: 0.00002315
Iteration 86/1000 | Loss: 0.00002315
Iteration 87/1000 | Loss: 0.00002315
Iteration 88/1000 | Loss: 0.00002315
Iteration 89/1000 | Loss: 0.00002315
Iteration 90/1000 | Loss: 0.00002315
Iteration 91/1000 | Loss: 0.00002315
Iteration 92/1000 | Loss: 0.00002315
Iteration 93/1000 | Loss: 0.00002315
Iteration 94/1000 | Loss: 0.00002315
Iteration 95/1000 | Loss: 0.00002315
Iteration 96/1000 | Loss: 0.00002315
Iteration 97/1000 | Loss: 0.00002315
Iteration 98/1000 | Loss: 0.00002315
Iteration 99/1000 | Loss: 0.00002315
Iteration 100/1000 | Loss: 0.00002315
Iteration 101/1000 | Loss: 0.00002315
Iteration 102/1000 | Loss: 0.00002314
Iteration 103/1000 | Loss: 0.00002314
Iteration 104/1000 | Loss: 0.00002314
Iteration 105/1000 | Loss: 0.00002314
Iteration 106/1000 | Loss: 0.00002314
Iteration 107/1000 | Loss: 0.00002314
Iteration 108/1000 | Loss: 0.00002314
Iteration 109/1000 | Loss: 0.00002314
Iteration 110/1000 | Loss: 0.00002314
Iteration 111/1000 | Loss: 0.00002314
Iteration 112/1000 | Loss: 0.00002314
Iteration 113/1000 | Loss: 0.00002314
Iteration 114/1000 | Loss: 0.00002314
Iteration 115/1000 | Loss: 0.00002314
Iteration 116/1000 | Loss: 0.00002314
Iteration 117/1000 | Loss: 0.00002314
Iteration 118/1000 | Loss: 0.00002313
Iteration 119/1000 | Loss: 0.00002313
Iteration 120/1000 | Loss: 0.00002313
Iteration 121/1000 | Loss: 0.00002313
Iteration 122/1000 | Loss: 0.00002313
Iteration 123/1000 | Loss: 0.00002313
Iteration 124/1000 | Loss: 0.00002313
Iteration 125/1000 | Loss: 0.00002313
Iteration 126/1000 | Loss: 0.00002313
Iteration 127/1000 | Loss: 0.00002313
Iteration 128/1000 | Loss: 0.00002313
Iteration 129/1000 | Loss: 0.00002313
Iteration 130/1000 | Loss: 0.00002313
Iteration 131/1000 | Loss: 0.00002313
Iteration 132/1000 | Loss: 0.00002313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [2.3130665795179084e-05, 2.3130665795179084e-05, 2.3130665795179084e-05, 2.3130665795179084e-05, 2.3130665795179084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3130665795179084e-05

Optimization complete. Final v2v error: 4.10727596282959 mm

Highest mean error: 4.445590019226074 mm for frame 10

Lowest mean error: 3.8808252811431885 mm for frame 60

Saving results

Total time: 38.73775386810303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935500
Iteration 2/25 | Loss: 0.00097424
Iteration 3/25 | Loss: 0.00071528
Iteration 4/25 | Loss: 0.00066695
Iteration 5/25 | Loss: 0.00065424
Iteration 6/25 | Loss: 0.00065251
Iteration 7/25 | Loss: 0.00065246
Iteration 8/25 | Loss: 0.00065246
Iteration 9/25 | Loss: 0.00065246
Iteration 10/25 | Loss: 0.00065246
Iteration 11/25 | Loss: 0.00065246
Iteration 12/25 | Loss: 0.00065246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006524565978907049, 0.0006524565978907049, 0.0006524565978907049, 0.0006524565978907049, 0.0006524565978907049]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006524565978907049

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45811856
Iteration 2/25 | Loss: 0.00018034
Iteration 3/25 | Loss: 0.00018031
Iteration 4/25 | Loss: 0.00018031
Iteration 5/25 | Loss: 0.00018030
Iteration 6/25 | Loss: 0.00018030
Iteration 7/25 | Loss: 0.00018030
Iteration 8/25 | Loss: 0.00018030
Iteration 9/25 | Loss: 0.00018030
Iteration 10/25 | Loss: 0.00018030
Iteration 11/25 | Loss: 0.00018030
Iteration 12/25 | Loss: 0.00018030
Iteration 13/25 | Loss: 0.00018030
Iteration 14/25 | Loss: 0.00018030
Iteration 15/25 | Loss: 0.00018030
Iteration 16/25 | Loss: 0.00018030
Iteration 17/25 | Loss: 0.00018030
Iteration 18/25 | Loss: 0.00018030
Iteration 19/25 | Loss: 0.00018030
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00018030373030342162, 0.00018030373030342162, 0.00018030373030342162, 0.00018030373030342162, 0.00018030373030342162]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018030373030342162

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018030
Iteration 2/1000 | Loss: 0.00002349
Iteration 3/1000 | Loss: 0.00001841
Iteration 4/1000 | Loss: 0.00001655
Iteration 5/1000 | Loss: 0.00001518
Iteration 6/1000 | Loss: 0.00001440
Iteration 7/1000 | Loss: 0.00001401
Iteration 8/1000 | Loss: 0.00001383
Iteration 9/1000 | Loss: 0.00001356
Iteration 10/1000 | Loss: 0.00001338
Iteration 11/1000 | Loss: 0.00001338
Iteration 12/1000 | Loss: 0.00001333
Iteration 13/1000 | Loss: 0.00001332
Iteration 14/1000 | Loss: 0.00001332
Iteration 15/1000 | Loss: 0.00001331
Iteration 16/1000 | Loss: 0.00001326
Iteration 17/1000 | Loss: 0.00001324
Iteration 18/1000 | Loss: 0.00001322
Iteration 19/1000 | Loss: 0.00001322
Iteration 20/1000 | Loss: 0.00001320
Iteration 21/1000 | Loss: 0.00001319
Iteration 22/1000 | Loss: 0.00001319
Iteration 23/1000 | Loss: 0.00001318
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001318
Iteration 26/1000 | Loss: 0.00001317
Iteration 27/1000 | Loss: 0.00001317
Iteration 28/1000 | Loss: 0.00001317
Iteration 29/1000 | Loss: 0.00001317
Iteration 30/1000 | Loss: 0.00001317
Iteration 31/1000 | Loss: 0.00001316
Iteration 32/1000 | Loss: 0.00001315
Iteration 33/1000 | Loss: 0.00001314
Iteration 34/1000 | Loss: 0.00001314
Iteration 35/1000 | Loss: 0.00001314
Iteration 36/1000 | Loss: 0.00001314
Iteration 37/1000 | Loss: 0.00001314
Iteration 38/1000 | Loss: 0.00001313
Iteration 39/1000 | Loss: 0.00001313
Iteration 40/1000 | Loss: 0.00001313
Iteration 41/1000 | Loss: 0.00001309
Iteration 42/1000 | Loss: 0.00001309
Iteration 43/1000 | Loss: 0.00001308
Iteration 44/1000 | Loss: 0.00001305
Iteration 45/1000 | Loss: 0.00001305
Iteration 46/1000 | Loss: 0.00001305
Iteration 47/1000 | Loss: 0.00001305
Iteration 48/1000 | Loss: 0.00001305
Iteration 49/1000 | Loss: 0.00001304
Iteration 50/1000 | Loss: 0.00001304
Iteration 51/1000 | Loss: 0.00001304
Iteration 52/1000 | Loss: 0.00001303
Iteration 53/1000 | Loss: 0.00001302
Iteration 54/1000 | Loss: 0.00001301
Iteration 55/1000 | Loss: 0.00001301
Iteration 56/1000 | Loss: 0.00001301
Iteration 57/1000 | Loss: 0.00001301
Iteration 58/1000 | Loss: 0.00001300
Iteration 59/1000 | Loss: 0.00001300
Iteration 60/1000 | Loss: 0.00001300
Iteration 61/1000 | Loss: 0.00001300
Iteration 62/1000 | Loss: 0.00001300
Iteration 63/1000 | Loss: 0.00001300
Iteration 64/1000 | Loss: 0.00001300
Iteration 65/1000 | Loss: 0.00001300
Iteration 66/1000 | Loss: 0.00001300
Iteration 67/1000 | Loss: 0.00001299
Iteration 68/1000 | Loss: 0.00001299
Iteration 69/1000 | Loss: 0.00001299
Iteration 70/1000 | Loss: 0.00001299
Iteration 71/1000 | Loss: 0.00001298
Iteration 72/1000 | Loss: 0.00001298
Iteration 73/1000 | Loss: 0.00001298
Iteration 74/1000 | Loss: 0.00001298
Iteration 75/1000 | Loss: 0.00001297
Iteration 76/1000 | Loss: 0.00001297
Iteration 77/1000 | Loss: 0.00001297
Iteration 78/1000 | Loss: 0.00001297
Iteration 79/1000 | Loss: 0.00001297
Iteration 80/1000 | Loss: 0.00001297
Iteration 81/1000 | Loss: 0.00001297
Iteration 82/1000 | Loss: 0.00001296
Iteration 83/1000 | Loss: 0.00001296
Iteration 84/1000 | Loss: 0.00001296
Iteration 85/1000 | Loss: 0.00001295
Iteration 86/1000 | Loss: 0.00001295
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001295
Iteration 89/1000 | Loss: 0.00001295
Iteration 90/1000 | Loss: 0.00001295
Iteration 91/1000 | Loss: 0.00001295
Iteration 92/1000 | Loss: 0.00001295
Iteration 93/1000 | Loss: 0.00001294
Iteration 94/1000 | Loss: 0.00001294
Iteration 95/1000 | Loss: 0.00001294
Iteration 96/1000 | Loss: 0.00001294
Iteration 97/1000 | Loss: 0.00001293
Iteration 98/1000 | Loss: 0.00001293
Iteration 99/1000 | Loss: 0.00001293
Iteration 100/1000 | Loss: 0.00001293
Iteration 101/1000 | Loss: 0.00001293
Iteration 102/1000 | Loss: 0.00001293
Iteration 103/1000 | Loss: 0.00001293
Iteration 104/1000 | Loss: 0.00001293
Iteration 105/1000 | Loss: 0.00001293
Iteration 106/1000 | Loss: 0.00001293
Iteration 107/1000 | Loss: 0.00001293
Iteration 108/1000 | Loss: 0.00001293
Iteration 109/1000 | Loss: 0.00001293
Iteration 110/1000 | Loss: 0.00001293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.2926711860927753e-05, 1.2926711860927753e-05, 1.2926711860927753e-05, 1.2926711860927753e-05, 1.2926711860927753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2926711860927753e-05

Optimization complete. Final v2v error: 3.095886468887329 mm

Highest mean error: 3.293600082397461 mm for frame 96

Lowest mean error: 2.847058057785034 mm for frame 163

Saving results

Total time: 37.28247046470642
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540428
Iteration 2/25 | Loss: 0.00110259
Iteration 3/25 | Loss: 0.00085326
Iteration 4/25 | Loss: 0.00082283
Iteration 5/25 | Loss: 0.00081890
Iteration 6/25 | Loss: 0.00081803
Iteration 7/25 | Loss: 0.00081797
Iteration 8/25 | Loss: 0.00081797
Iteration 9/25 | Loss: 0.00081797
Iteration 10/25 | Loss: 0.00081797
Iteration 11/25 | Loss: 0.00081797
Iteration 12/25 | Loss: 0.00081797
Iteration 13/25 | Loss: 0.00081797
Iteration 14/25 | Loss: 0.00081797
Iteration 15/25 | Loss: 0.00081797
Iteration 16/25 | Loss: 0.00081797
Iteration 17/25 | Loss: 0.00081797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008179720607586205, 0.0008179720607586205, 0.0008179720607586205, 0.0008179720607586205, 0.0008179720607586205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008179720607586205

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40729678
Iteration 2/25 | Loss: 0.00053976
Iteration 3/25 | Loss: 0.00053972
Iteration 4/25 | Loss: 0.00053971
Iteration 5/25 | Loss: 0.00053971
Iteration 6/25 | Loss: 0.00053971
Iteration 7/25 | Loss: 0.00053971
Iteration 8/25 | Loss: 0.00053971
Iteration 9/25 | Loss: 0.00053971
Iteration 10/25 | Loss: 0.00053971
Iteration 11/25 | Loss: 0.00053971
Iteration 12/25 | Loss: 0.00053971
Iteration 13/25 | Loss: 0.00053971
Iteration 14/25 | Loss: 0.00053971
Iteration 15/25 | Loss: 0.00053971
Iteration 16/25 | Loss: 0.00053971
Iteration 17/25 | Loss: 0.00053971
Iteration 18/25 | Loss: 0.00053971
Iteration 19/25 | Loss: 0.00053971
Iteration 20/25 | Loss: 0.00053971
Iteration 21/25 | Loss: 0.00053971
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005397122586145997, 0.0005397122586145997, 0.0005397122586145997, 0.0005397122586145997, 0.0005397122586145997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005397122586145997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053971
Iteration 2/1000 | Loss: 0.00004445
Iteration 3/1000 | Loss: 0.00002907
Iteration 4/1000 | Loss: 0.00002507
Iteration 5/1000 | Loss: 0.00002344
Iteration 6/1000 | Loss: 0.00002230
Iteration 7/1000 | Loss: 0.00002157
Iteration 8/1000 | Loss: 0.00002085
Iteration 9/1000 | Loss: 0.00002047
Iteration 10/1000 | Loss: 0.00002021
Iteration 11/1000 | Loss: 0.00002001
Iteration 12/1000 | Loss: 0.00001986
Iteration 13/1000 | Loss: 0.00001980
Iteration 14/1000 | Loss: 0.00001970
Iteration 15/1000 | Loss: 0.00001967
Iteration 16/1000 | Loss: 0.00001964
Iteration 17/1000 | Loss: 0.00001964
Iteration 18/1000 | Loss: 0.00001963
Iteration 19/1000 | Loss: 0.00001959
Iteration 20/1000 | Loss: 0.00001958
Iteration 21/1000 | Loss: 0.00001957
Iteration 22/1000 | Loss: 0.00001957
Iteration 23/1000 | Loss: 0.00001957
Iteration 24/1000 | Loss: 0.00001956
Iteration 25/1000 | Loss: 0.00001956
Iteration 26/1000 | Loss: 0.00001956
Iteration 27/1000 | Loss: 0.00001956
Iteration 28/1000 | Loss: 0.00001955
Iteration 29/1000 | Loss: 0.00001955
Iteration 30/1000 | Loss: 0.00001955
Iteration 31/1000 | Loss: 0.00001955
Iteration 32/1000 | Loss: 0.00001955
Iteration 33/1000 | Loss: 0.00001954
Iteration 34/1000 | Loss: 0.00001954
Iteration 35/1000 | Loss: 0.00001954
Iteration 36/1000 | Loss: 0.00001953
Iteration 37/1000 | Loss: 0.00001953
Iteration 38/1000 | Loss: 0.00001953
Iteration 39/1000 | Loss: 0.00001953
Iteration 40/1000 | Loss: 0.00001953
Iteration 41/1000 | Loss: 0.00001952
Iteration 42/1000 | Loss: 0.00001952
Iteration 43/1000 | Loss: 0.00001952
Iteration 44/1000 | Loss: 0.00001952
Iteration 45/1000 | Loss: 0.00001951
Iteration 46/1000 | Loss: 0.00001951
Iteration 47/1000 | Loss: 0.00001951
Iteration 48/1000 | Loss: 0.00001951
Iteration 49/1000 | Loss: 0.00001951
Iteration 50/1000 | Loss: 0.00001951
Iteration 51/1000 | Loss: 0.00001951
Iteration 52/1000 | Loss: 0.00001950
Iteration 53/1000 | Loss: 0.00001950
Iteration 54/1000 | Loss: 0.00001950
Iteration 55/1000 | Loss: 0.00001950
Iteration 56/1000 | Loss: 0.00001950
Iteration 57/1000 | Loss: 0.00001950
Iteration 58/1000 | Loss: 0.00001950
Iteration 59/1000 | Loss: 0.00001950
Iteration 60/1000 | Loss: 0.00001950
Iteration 61/1000 | Loss: 0.00001950
Iteration 62/1000 | Loss: 0.00001949
Iteration 63/1000 | Loss: 0.00001949
Iteration 64/1000 | Loss: 0.00001949
Iteration 65/1000 | Loss: 0.00001949
Iteration 66/1000 | Loss: 0.00001949
Iteration 67/1000 | Loss: 0.00001949
Iteration 68/1000 | Loss: 0.00001949
Iteration 69/1000 | Loss: 0.00001949
Iteration 70/1000 | Loss: 0.00001948
Iteration 71/1000 | Loss: 0.00001948
Iteration 72/1000 | Loss: 0.00001948
Iteration 73/1000 | Loss: 0.00001948
Iteration 74/1000 | Loss: 0.00001948
Iteration 75/1000 | Loss: 0.00001948
Iteration 76/1000 | Loss: 0.00001948
Iteration 77/1000 | Loss: 0.00001948
Iteration 78/1000 | Loss: 0.00001948
Iteration 79/1000 | Loss: 0.00001948
Iteration 80/1000 | Loss: 0.00001948
Iteration 81/1000 | Loss: 0.00001948
Iteration 82/1000 | Loss: 0.00001948
Iteration 83/1000 | Loss: 0.00001947
Iteration 84/1000 | Loss: 0.00001947
Iteration 85/1000 | Loss: 0.00001947
Iteration 86/1000 | Loss: 0.00001947
Iteration 87/1000 | Loss: 0.00001947
Iteration 88/1000 | Loss: 0.00001947
Iteration 89/1000 | Loss: 0.00001947
Iteration 90/1000 | Loss: 0.00001947
Iteration 91/1000 | Loss: 0.00001947
Iteration 92/1000 | Loss: 0.00001947
Iteration 93/1000 | Loss: 0.00001947
Iteration 94/1000 | Loss: 0.00001947
Iteration 95/1000 | Loss: 0.00001947
Iteration 96/1000 | Loss: 0.00001947
Iteration 97/1000 | Loss: 0.00001946
Iteration 98/1000 | Loss: 0.00001946
Iteration 99/1000 | Loss: 0.00001946
Iteration 100/1000 | Loss: 0.00001946
Iteration 101/1000 | Loss: 0.00001945
Iteration 102/1000 | Loss: 0.00001945
Iteration 103/1000 | Loss: 0.00001945
Iteration 104/1000 | Loss: 0.00001945
Iteration 105/1000 | Loss: 0.00001945
Iteration 106/1000 | Loss: 0.00001945
Iteration 107/1000 | Loss: 0.00001945
Iteration 108/1000 | Loss: 0.00001945
Iteration 109/1000 | Loss: 0.00001945
Iteration 110/1000 | Loss: 0.00001945
Iteration 111/1000 | Loss: 0.00001945
Iteration 112/1000 | Loss: 0.00001945
Iteration 113/1000 | Loss: 0.00001945
Iteration 114/1000 | Loss: 0.00001945
Iteration 115/1000 | Loss: 0.00001945
Iteration 116/1000 | Loss: 0.00001945
Iteration 117/1000 | Loss: 0.00001945
Iteration 118/1000 | Loss: 0.00001945
Iteration 119/1000 | Loss: 0.00001945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.945295480254572e-05, 1.945295480254572e-05, 1.945295480254572e-05, 1.945295480254572e-05, 1.945295480254572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.945295480254572e-05

Optimization complete. Final v2v error: 3.608515977859497 mm

Highest mean error: 4.206143856048584 mm for frame 142

Lowest mean error: 2.9739601612091064 mm for frame 19

Saving results

Total time: 35.95948767662048
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011968
Iteration 2/25 | Loss: 0.00228273
Iteration 3/25 | Loss: 0.00180832
Iteration 4/25 | Loss: 0.00144802
Iteration 5/25 | Loss: 0.00103631
Iteration 6/25 | Loss: 0.00105471
Iteration 7/25 | Loss: 0.00102965
Iteration 8/25 | Loss: 0.00096182
Iteration 9/25 | Loss: 0.00086492
Iteration 10/25 | Loss: 0.00086598
Iteration 11/25 | Loss: 0.00086214
Iteration 12/25 | Loss: 0.00087948
Iteration 13/25 | Loss: 0.00085475
Iteration 14/25 | Loss: 0.00089825
Iteration 15/25 | Loss: 0.00091175
Iteration 16/25 | Loss: 0.00093097
Iteration 17/25 | Loss: 0.00092831
Iteration 18/25 | Loss: 0.00096288
Iteration 19/25 | Loss: 0.00090179
Iteration 20/25 | Loss: 0.00088328
Iteration 21/25 | Loss: 0.00082355
Iteration 22/25 | Loss: 0.00081250
Iteration 23/25 | Loss: 0.00081471
Iteration 24/25 | Loss: 0.00079475
Iteration 25/25 | Loss: 0.00081071

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45065069
Iteration 2/25 | Loss: 0.00065870
Iteration 3/25 | Loss: 0.00065870
Iteration 4/25 | Loss: 0.00065869
Iteration 5/25 | Loss: 0.00065869
Iteration 6/25 | Loss: 0.00065869
Iteration 7/25 | Loss: 0.00065869
Iteration 8/25 | Loss: 0.00065869
Iteration 9/25 | Loss: 0.00065869
Iteration 10/25 | Loss: 0.00065869
Iteration 11/25 | Loss: 0.00065869
Iteration 12/25 | Loss: 0.00065869
Iteration 13/25 | Loss: 0.00065869
Iteration 14/25 | Loss: 0.00065869
Iteration 15/25 | Loss: 0.00065869
Iteration 16/25 | Loss: 0.00065869
Iteration 17/25 | Loss: 0.00065869
Iteration 18/25 | Loss: 0.00065869
Iteration 19/25 | Loss: 0.00065869
Iteration 20/25 | Loss: 0.00065869
Iteration 21/25 | Loss: 0.00065869
Iteration 22/25 | Loss: 0.00065869
Iteration 23/25 | Loss: 0.00065869
Iteration 24/25 | Loss: 0.00065869
Iteration 25/25 | Loss: 0.00065869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065869
Iteration 2/1000 | Loss: 0.00286215
Iteration 3/1000 | Loss: 0.00234689
Iteration 4/1000 | Loss: 0.00208138
Iteration 5/1000 | Loss: 0.00106714
Iteration 6/1000 | Loss: 0.00157036
Iteration 7/1000 | Loss: 0.00128998
Iteration 8/1000 | Loss: 0.00125991
Iteration 9/1000 | Loss: 0.00171472
Iteration 10/1000 | Loss: 0.00169577
Iteration 11/1000 | Loss: 0.00146058
Iteration 12/1000 | Loss: 0.00165715
Iteration 13/1000 | Loss: 0.00101474
Iteration 14/1000 | Loss: 0.00117366
Iteration 15/1000 | Loss: 0.00096242
Iteration 16/1000 | Loss: 0.00078315
Iteration 17/1000 | Loss: 0.00152317
Iteration 18/1000 | Loss: 0.00175539
Iteration 19/1000 | Loss: 0.00179373
Iteration 20/1000 | Loss: 0.00117592
Iteration 21/1000 | Loss: 0.00100241
Iteration 22/1000 | Loss: 0.00044882
Iteration 23/1000 | Loss: 0.00018369
Iteration 24/1000 | Loss: 0.00050513
Iteration 25/1000 | Loss: 0.00066081
Iteration 26/1000 | Loss: 0.00062662
Iteration 27/1000 | Loss: 0.00051878
Iteration 28/1000 | Loss: 0.00079875
Iteration 29/1000 | Loss: 0.00054596
Iteration 30/1000 | Loss: 0.00064054
Iteration 31/1000 | Loss: 0.00048532
Iteration 32/1000 | Loss: 0.00111860
Iteration 33/1000 | Loss: 0.00080326
Iteration 34/1000 | Loss: 0.00186750
Iteration 35/1000 | Loss: 0.00104661
Iteration 36/1000 | Loss: 0.00206426
Iteration 37/1000 | Loss: 0.00090215
Iteration 38/1000 | Loss: 0.00171463
Iteration 39/1000 | Loss: 0.00096900
Iteration 40/1000 | Loss: 0.00120912
Iteration 41/1000 | Loss: 0.00118265
Iteration 42/1000 | Loss: 0.00069116
Iteration 43/1000 | Loss: 0.00051395
Iteration 44/1000 | Loss: 0.00068052
Iteration 45/1000 | Loss: 0.00092799
Iteration 46/1000 | Loss: 0.00079879
Iteration 47/1000 | Loss: 0.00097674
Iteration 48/1000 | Loss: 0.00074098
Iteration 49/1000 | Loss: 0.00107161
Iteration 50/1000 | Loss: 0.00111775
Iteration 51/1000 | Loss: 0.00094786
Iteration 52/1000 | Loss: 0.00076817
Iteration 53/1000 | Loss: 0.00083012
Iteration 54/1000 | Loss: 0.00103401
Iteration 55/1000 | Loss: 0.00158614
Iteration 56/1000 | Loss: 0.00136055
Iteration 57/1000 | Loss: 0.00033742
Iteration 58/1000 | Loss: 0.00029032
Iteration 59/1000 | Loss: 0.00005629
Iteration 60/1000 | Loss: 0.00004304
Iteration 61/1000 | Loss: 0.00003602
Iteration 62/1000 | Loss: 0.00003334
Iteration 63/1000 | Loss: 0.00034363
Iteration 64/1000 | Loss: 0.00019676
Iteration 65/1000 | Loss: 0.00057473
Iteration 66/1000 | Loss: 0.00007972
Iteration 67/1000 | Loss: 0.00073341
Iteration 68/1000 | Loss: 0.00020229
Iteration 69/1000 | Loss: 0.00008310
Iteration 70/1000 | Loss: 0.00003186
Iteration 71/1000 | Loss: 0.00002784
Iteration 72/1000 | Loss: 0.00002599
Iteration 73/1000 | Loss: 0.00002487
Iteration 74/1000 | Loss: 0.00002368
Iteration 75/1000 | Loss: 0.00002267
Iteration 76/1000 | Loss: 0.00002202
Iteration 77/1000 | Loss: 0.00010924
Iteration 78/1000 | Loss: 0.00002443
Iteration 79/1000 | Loss: 0.00003607
Iteration 80/1000 | Loss: 0.00002253
Iteration 81/1000 | Loss: 0.00002056
Iteration 82/1000 | Loss: 0.00002791
Iteration 83/1000 | Loss: 0.00002012
Iteration 84/1000 | Loss: 0.00001942
Iteration 85/1000 | Loss: 0.00001883
Iteration 86/1000 | Loss: 0.00001868
Iteration 87/1000 | Loss: 0.00001860
Iteration 88/1000 | Loss: 0.00001837
Iteration 89/1000 | Loss: 0.00001826
Iteration 90/1000 | Loss: 0.00001816
Iteration 91/1000 | Loss: 0.00001810
Iteration 92/1000 | Loss: 0.00001810
Iteration 93/1000 | Loss: 0.00001808
Iteration 94/1000 | Loss: 0.00001808
Iteration 95/1000 | Loss: 0.00001808
Iteration 96/1000 | Loss: 0.00001807
Iteration 97/1000 | Loss: 0.00001807
Iteration 98/1000 | Loss: 0.00001807
Iteration 99/1000 | Loss: 0.00001806
Iteration 100/1000 | Loss: 0.00001805
Iteration 101/1000 | Loss: 0.00001803
Iteration 102/1000 | Loss: 0.00001803
Iteration 103/1000 | Loss: 0.00001803
Iteration 104/1000 | Loss: 0.00001803
Iteration 105/1000 | Loss: 0.00001803
Iteration 106/1000 | Loss: 0.00001803
Iteration 107/1000 | Loss: 0.00001803
Iteration 108/1000 | Loss: 0.00001803
Iteration 109/1000 | Loss: 0.00001803
Iteration 110/1000 | Loss: 0.00001803
Iteration 111/1000 | Loss: 0.00001802
Iteration 112/1000 | Loss: 0.00001802
Iteration 113/1000 | Loss: 0.00001802
Iteration 114/1000 | Loss: 0.00001802
Iteration 115/1000 | Loss: 0.00001801
Iteration 116/1000 | Loss: 0.00001801
Iteration 117/1000 | Loss: 0.00001801
Iteration 118/1000 | Loss: 0.00001801
Iteration 119/1000 | Loss: 0.00001801
Iteration 120/1000 | Loss: 0.00001801
Iteration 121/1000 | Loss: 0.00001800
Iteration 122/1000 | Loss: 0.00001800
Iteration 123/1000 | Loss: 0.00001800
Iteration 124/1000 | Loss: 0.00001800
Iteration 125/1000 | Loss: 0.00001799
Iteration 126/1000 | Loss: 0.00001799
Iteration 127/1000 | Loss: 0.00001799
Iteration 128/1000 | Loss: 0.00001799
Iteration 129/1000 | Loss: 0.00001799
Iteration 130/1000 | Loss: 0.00001799
Iteration 131/1000 | Loss: 0.00001799
Iteration 132/1000 | Loss: 0.00001799
Iteration 133/1000 | Loss: 0.00001798
Iteration 134/1000 | Loss: 0.00001798
Iteration 135/1000 | Loss: 0.00001798
Iteration 136/1000 | Loss: 0.00001798
Iteration 137/1000 | Loss: 0.00001798
Iteration 138/1000 | Loss: 0.00001798
Iteration 139/1000 | Loss: 0.00001798
Iteration 140/1000 | Loss: 0.00001798
Iteration 141/1000 | Loss: 0.00001798
Iteration 142/1000 | Loss: 0.00001798
Iteration 143/1000 | Loss: 0.00001798
Iteration 144/1000 | Loss: 0.00001798
Iteration 145/1000 | Loss: 0.00001798
Iteration 146/1000 | Loss: 0.00001798
Iteration 147/1000 | Loss: 0.00001798
Iteration 148/1000 | Loss: 0.00001798
Iteration 149/1000 | Loss: 0.00001798
Iteration 150/1000 | Loss: 0.00001797
Iteration 151/1000 | Loss: 0.00001797
Iteration 152/1000 | Loss: 0.00001797
Iteration 153/1000 | Loss: 0.00001797
Iteration 154/1000 | Loss: 0.00001797
Iteration 155/1000 | Loss: 0.00001797
Iteration 156/1000 | Loss: 0.00001797
Iteration 157/1000 | Loss: 0.00001797
Iteration 158/1000 | Loss: 0.00001797
Iteration 159/1000 | Loss: 0.00001797
Iteration 160/1000 | Loss: 0.00001797
Iteration 161/1000 | Loss: 0.00001797
Iteration 162/1000 | Loss: 0.00001797
Iteration 163/1000 | Loss: 0.00001797
Iteration 164/1000 | Loss: 0.00001797
Iteration 165/1000 | Loss: 0.00001797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.7968446627492085e-05, 1.7968446627492085e-05, 1.7968446627492085e-05, 1.7968446627492085e-05, 1.7968446627492085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7968446627492085e-05

Optimization complete. Final v2v error: 3.555216073989868 mm

Highest mean error: 5.143165588378906 mm for frame 22

Lowest mean error: 3.3228049278259277 mm for frame 58

Saving results

Total time: 202.51035070419312
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00510505
Iteration 2/25 | Loss: 0.00124114
Iteration 3/25 | Loss: 0.00079832
Iteration 4/25 | Loss: 0.00074115
Iteration 5/25 | Loss: 0.00072646
Iteration 6/25 | Loss: 0.00072302
Iteration 7/25 | Loss: 0.00072233
Iteration 8/25 | Loss: 0.00072233
Iteration 9/25 | Loss: 0.00072233
Iteration 10/25 | Loss: 0.00072233
Iteration 11/25 | Loss: 0.00072233
Iteration 12/25 | Loss: 0.00072233
Iteration 13/25 | Loss: 0.00072233
Iteration 14/25 | Loss: 0.00072233
Iteration 15/25 | Loss: 0.00072233
Iteration 16/25 | Loss: 0.00072233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007223341381177306, 0.0007223341381177306, 0.0007223341381177306, 0.0007223341381177306, 0.0007223341381177306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007223341381177306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11485791
Iteration 2/25 | Loss: 0.00032580
Iteration 3/25 | Loss: 0.00032575
Iteration 4/25 | Loss: 0.00032575
Iteration 5/25 | Loss: 0.00032575
Iteration 6/25 | Loss: 0.00032575
Iteration 7/25 | Loss: 0.00032575
Iteration 8/25 | Loss: 0.00032575
Iteration 9/25 | Loss: 0.00032575
Iteration 10/25 | Loss: 0.00032575
Iteration 11/25 | Loss: 0.00032575
Iteration 12/25 | Loss: 0.00032575
Iteration 13/25 | Loss: 0.00032575
Iteration 14/25 | Loss: 0.00032575
Iteration 15/25 | Loss: 0.00032575
Iteration 16/25 | Loss: 0.00032575
Iteration 17/25 | Loss: 0.00032575
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003257462230976671, 0.0003257462230976671, 0.0003257462230976671, 0.0003257462230976671, 0.0003257462230976671]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003257462230976671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032575
Iteration 2/1000 | Loss: 0.00004828
Iteration 3/1000 | Loss: 0.00003401
Iteration 4/1000 | Loss: 0.00002894
Iteration 5/1000 | Loss: 0.00002698
Iteration 6/1000 | Loss: 0.00002524
Iteration 7/1000 | Loss: 0.00002461
Iteration 8/1000 | Loss: 0.00002379
Iteration 9/1000 | Loss: 0.00002330
Iteration 10/1000 | Loss: 0.00002292
Iteration 11/1000 | Loss: 0.00002267
Iteration 12/1000 | Loss: 0.00002241
Iteration 13/1000 | Loss: 0.00002232
Iteration 14/1000 | Loss: 0.00002213
Iteration 15/1000 | Loss: 0.00002194
Iteration 16/1000 | Loss: 0.00002180
Iteration 17/1000 | Loss: 0.00002179
Iteration 18/1000 | Loss: 0.00002175
Iteration 19/1000 | Loss: 0.00002174
Iteration 20/1000 | Loss: 0.00002172
Iteration 21/1000 | Loss: 0.00002170
Iteration 22/1000 | Loss: 0.00002168
Iteration 23/1000 | Loss: 0.00002167
Iteration 24/1000 | Loss: 0.00002167
Iteration 25/1000 | Loss: 0.00002166
Iteration 26/1000 | Loss: 0.00002166
Iteration 27/1000 | Loss: 0.00002165
Iteration 28/1000 | Loss: 0.00002165
Iteration 29/1000 | Loss: 0.00002165
Iteration 30/1000 | Loss: 0.00002164
Iteration 31/1000 | Loss: 0.00002163
Iteration 32/1000 | Loss: 0.00002163
Iteration 33/1000 | Loss: 0.00002162
Iteration 34/1000 | Loss: 0.00002160
Iteration 35/1000 | Loss: 0.00002158
Iteration 36/1000 | Loss: 0.00002157
Iteration 37/1000 | Loss: 0.00002157
Iteration 38/1000 | Loss: 0.00002156
Iteration 39/1000 | Loss: 0.00002156
Iteration 40/1000 | Loss: 0.00002156
Iteration 41/1000 | Loss: 0.00002155
Iteration 42/1000 | Loss: 0.00002152
Iteration 43/1000 | Loss: 0.00002152
Iteration 44/1000 | Loss: 0.00002150
Iteration 45/1000 | Loss: 0.00002150
Iteration 46/1000 | Loss: 0.00002150
Iteration 47/1000 | Loss: 0.00002150
Iteration 48/1000 | Loss: 0.00002149
Iteration 49/1000 | Loss: 0.00002149
Iteration 50/1000 | Loss: 0.00002149
Iteration 51/1000 | Loss: 0.00002149
Iteration 52/1000 | Loss: 0.00002149
Iteration 53/1000 | Loss: 0.00002149
Iteration 54/1000 | Loss: 0.00002149
Iteration 55/1000 | Loss: 0.00002149
Iteration 56/1000 | Loss: 0.00002149
Iteration 57/1000 | Loss: 0.00002149
Iteration 58/1000 | Loss: 0.00002149
Iteration 59/1000 | Loss: 0.00002149
Iteration 60/1000 | Loss: 0.00002148
Iteration 61/1000 | Loss: 0.00002147
Iteration 62/1000 | Loss: 0.00002147
Iteration 63/1000 | Loss: 0.00002147
Iteration 64/1000 | Loss: 0.00002147
Iteration 65/1000 | Loss: 0.00002147
Iteration 66/1000 | Loss: 0.00002147
Iteration 67/1000 | Loss: 0.00002147
Iteration 68/1000 | Loss: 0.00002147
Iteration 69/1000 | Loss: 0.00002147
Iteration 70/1000 | Loss: 0.00002146
Iteration 71/1000 | Loss: 0.00002146
Iteration 72/1000 | Loss: 0.00002146
Iteration 73/1000 | Loss: 0.00002145
Iteration 74/1000 | Loss: 0.00002145
Iteration 75/1000 | Loss: 0.00002145
Iteration 76/1000 | Loss: 0.00002145
Iteration 77/1000 | Loss: 0.00002144
Iteration 78/1000 | Loss: 0.00002144
Iteration 79/1000 | Loss: 0.00002144
Iteration 80/1000 | Loss: 0.00002144
Iteration 81/1000 | Loss: 0.00002143
Iteration 82/1000 | Loss: 0.00002143
Iteration 83/1000 | Loss: 0.00002143
Iteration 84/1000 | Loss: 0.00002142
Iteration 85/1000 | Loss: 0.00002142
Iteration 86/1000 | Loss: 0.00002142
Iteration 87/1000 | Loss: 0.00002142
Iteration 88/1000 | Loss: 0.00002142
Iteration 89/1000 | Loss: 0.00002142
Iteration 90/1000 | Loss: 0.00002141
Iteration 91/1000 | Loss: 0.00002141
Iteration 92/1000 | Loss: 0.00002141
Iteration 93/1000 | Loss: 0.00002141
Iteration 94/1000 | Loss: 0.00002141
Iteration 95/1000 | Loss: 0.00002140
Iteration 96/1000 | Loss: 0.00002140
Iteration 97/1000 | Loss: 0.00002140
Iteration 98/1000 | Loss: 0.00002140
Iteration 99/1000 | Loss: 0.00002140
Iteration 100/1000 | Loss: 0.00002140
Iteration 101/1000 | Loss: 0.00002140
Iteration 102/1000 | Loss: 0.00002140
Iteration 103/1000 | Loss: 0.00002140
Iteration 104/1000 | Loss: 0.00002140
Iteration 105/1000 | Loss: 0.00002140
Iteration 106/1000 | Loss: 0.00002140
Iteration 107/1000 | Loss: 0.00002140
Iteration 108/1000 | Loss: 0.00002139
Iteration 109/1000 | Loss: 0.00002139
Iteration 110/1000 | Loss: 0.00002139
Iteration 111/1000 | Loss: 0.00002139
Iteration 112/1000 | Loss: 0.00002139
Iteration 113/1000 | Loss: 0.00002139
Iteration 114/1000 | Loss: 0.00002138
Iteration 115/1000 | Loss: 0.00002138
Iteration 116/1000 | Loss: 0.00002138
Iteration 117/1000 | Loss: 0.00002138
Iteration 118/1000 | Loss: 0.00002137
Iteration 119/1000 | Loss: 0.00002137
Iteration 120/1000 | Loss: 0.00002137
Iteration 121/1000 | Loss: 0.00002137
Iteration 122/1000 | Loss: 0.00002137
Iteration 123/1000 | Loss: 0.00002137
Iteration 124/1000 | Loss: 0.00002137
Iteration 125/1000 | Loss: 0.00002137
Iteration 126/1000 | Loss: 0.00002137
Iteration 127/1000 | Loss: 0.00002137
Iteration 128/1000 | Loss: 0.00002136
Iteration 129/1000 | Loss: 0.00002136
Iteration 130/1000 | Loss: 0.00002136
Iteration 131/1000 | Loss: 0.00002136
Iteration 132/1000 | Loss: 0.00002136
Iteration 133/1000 | Loss: 0.00002136
Iteration 134/1000 | Loss: 0.00002136
Iteration 135/1000 | Loss: 0.00002136
Iteration 136/1000 | Loss: 0.00002136
Iteration 137/1000 | Loss: 0.00002136
Iteration 138/1000 | Loss: 0.00002136
Iteration 139/1000 | Loss: 0.00002136
Iteration 140/1000 | Loss: 0.00002135
Iteration 141/1000 | Loss: 0.00002135
Iteration 142/1000 | Loss: 0.00002135
Iteration 143/1000 | Loss: 0.00002135
Iteration 144/1000 | Loss: 0.00002135
Iteration 145/1000 | Loss: 0.00002135
Iteration 146/1000 | Loss: 0.00002135
Iteration 147/1000 | Loss: 0.00002134
Iteration 148/1000 | Loss: 0.00002134
Iteration 149/1000 | Loss: 0.00002134
Iteration 150/1000 | Loss: 0.00002134
Iteration 151/1000 | Loss: 0.00002134
Iteration 152/1000 | Loss: 0.00002134
Iteration 153/1000 | Loss: 0.00002134
Iteration 154/1000 | Loss: 0.00002134
Iteration 155/1000 | Loss: 0.00002134
Iteration 156/1000 | Loss: 0.00002134
Iteration 157/1000 | Loss: 0.00002134
Iteration 158/1000 | Loss: 0.00002134
Iteration 159/1000 | Loss: 0.00002134
Iteration 160/1000 | Loss: 0.00002134
Iteration 161/1000 | Loss: 0.00002134
Iteration 162/1000 | Loss: 0.00002134
Iteration 163/1000 | Loss: 0.00002134
Iteration 164/1000 | Loss: 0.00002134
Iteration 165/1000 | Loss: 0.00002134
Iteration 166/1000 | Loss: 0.00002134
Iteration 167/1000 | Loss: 0.00002134
Iteration 168/1000 | Loss: 0.00002134
Iteration 169/1000 | Loss: 0.00002134
Iteration 170/1000 | Loss: 0.00002134
Iteration 171/1000 | Loss: 0.00002134
Iteration 172/1000 | Loss: 0.00002134
Iteration 173/1000 | Loss: 0.00002134
Iteration 174/1000 | Loss: 0.00002134
Iteration 175/1000 | Loss: 0.00002134
Iteration 176/1000 | Loss: 0.00002134
Iteration 177/1000 | Loss: 0.00002134
Iteration 178/1000 | Loss: 0.00002134
Iteration 179/1000 | Loss: 0.00002134
Iteration 180/1000 | Loss: 0.00002134
Iteration 181/1000 | Loss: 0.00002134
Iteration 182/1000 | Loss: 0.00002134
Iteration 183/1000 | Loss: 0.00002134
Iteration 184/1000 | Loss: 0.00002134
Iteration 185/1000 | Loss: 0.00002134
Iteration 186/1000 | Loss: 0.00002134
Iteration 187/1000 | Loss: 0.00002134
Iteration 188/1000 | Loss: 0.00002134
Iteration 189/1000 | Loss: 0.00002134
Iteration 190/1000 | Loss: 0.00002134
Iteration 191/1000 | Loss: 0.00002134
Iteration 192/1000 | Loss: 0.00002134
Iteration 193/1000 | Loss: 0.00002134
Iteration 194/1000 | Loss: 0.00002134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.1336774807423353e-05, 2.1336774807423353e-05, 2.1336774807423353e-05, 2.1336774807423353e-05, 2.1336774807423353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1336774807423353e-05

Optimization complete. Final v2v error: 3.8276784420013428 mm

Highest mean error: 5.379108905792236 mm for frame 161

Lowest mean error: 2.7356090545654297 mm for frame 195

Saving results

Total time: 47.92545533180237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846376
Iteration 2/25 | Loss: 0.00079594
Iteration 3/25 | Loss: 0.00062934
Iteration 4/25 | Loss: 0.00059871
Iteration 5/25 | Loss: 0.00059097
Iteration 6/25 | Loss: 0.00058899
Iteration 7/25 | Loss: 0.00058873
Iteration 8/25 | Loss: 0.00058873
Iteration 9/25 | Loss: 0.00058872
Iteration 10/25 | Loss: 0.00058872
Iteration 11/25 | Loss: 0.00058872
Iteration 12/25 | Loss: 0.00058872
Iteration 13/25 | Loss: 0.00058872
Iteration 14/25 | Loss: 0.00058872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0005887176375836134, 0.0005887176375836134, 0.0005887176375836134, 0.0005887176375836134, 0.0005887176375836134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005887176375836134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45832360
Iteration 2/25 | Loss: 0.00024855
Iteration 3/25 | Loss: 0.00024855
Iteration 4/25 | Loss: 0.00024855
Iteration 5/25 | Loss: 0.00024855
Iteration 6/25 | Loss: 0.00024855
Iteration 7/25 | Loss: 0.00024855
Iteration 8/25 | Loss: 0.00024855
Iteration 9/25 | Loss: 0.00024855
Iteration 10/25 | Loss: 0.00024855
Iteration 11/25 | Loss: 0.00024855
Iteration 12/25 | Loss: 0.00024855
Iteration 13/25 | Loss: 0.00024855
Iteration 14/25 | Loss: 0.00024855
Iteration 15/25 | Loss: 0.00024855
Iteration 16/25 | Loss: 0.00024855
Iteration 17/25 | Loss: 0.00024855
Iteration 18/25 | Loss: 0.00024855
Iteration 19/25 | Loss: 0.00024855
Iteration 20/25 | Loss: 0.00024855
Iteration 21/25 | Loss: 0.00024855
Iteration 22/25 | Loss: 0.00024855
Iteration 23/25 | Loss: 0.00024855
Iteration 24/25 | Loss: 0.00024855
Iteration 25/25 | Loss: 0.00024855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024855
Iteration 2/1000 | Loss: 0.00002050
Iteration 3/1000 | Loss: 0.00001449
Iteration 4/1000 | Loss: 0.00001338
Iteration 5/1000 | Loss: 0.00001250
Iteration 6/1000 | Loss: 0.00001212
Iteration 7/1000 | Loss: 0.00001182
Iteration 8/1000 | Loss: 0.00001181
Iteration 9/1000 | Loss: 0.00001173
Iteration 10/1000 | Loss: 0.00001171
Iteration 11/1000 | Loss: 0.00001161
Iteration 12/1000 | Loss: 0.00001154
Iteration 13/1000 | Loss: 0.00001149
Iteration 14/1000 | Loss: 0.00001147
Iteration 15/1000 | Loss: 0.00001146
Iteration 16/1000 | Loss: 0.00001146
Iteration 17/1000 | Loss: 0.00001145
Iteration 18/1000 | Loss: 0.00001144
Iteration 19/1000 | Loss: 0.00001143
Iteration 20/1000 | Loss: 0.00001142
Iteration 21/1000 | Loss: 0.00001142
Iteration 22/1000 | Loss: 0.00001141
Iteration 23/1000 | Loss: 0.00001141
Iteration 24/1000 | Loss: 0.00001141
Iteration 25/1000 | Loss: 0.00001141
Iteration 26/1000 | Loss: 0.00001140
Iteration 27/1000 | Loss: 0.00001135
Iteration 28/1000 | Loss: 0.00001134
Iteration 29/1000 | Loss: 0.00001133
Iteration 30/1000 | Loss: 0.00001133
Iteration 31/1000 | Loss: 0.00001132
Iteration 32/1000 | Loss: 0.00001132
Iteration 33/1000 | Loss: 0.00001132
Iteration 34/1000 | Loss: 0.00001132
Iteration 35/1000 | Loss: 0.00001131
Iteration 36/1000 | Loss: 0.00001131
Iteration 37/1000 | Loss: 0.00001131
Iteration 38/1000 | Loss: 0.00001130
Iteration 39/1000 | Loss: 0.00001129
Iteration 40/1000 | Loss: 0.00001128
Iteration 41/1000 | Loss: 0.00001128
Iteration 42/1000 | Loss: 0.00001127
Iteration 43/1000 | Loss: 0.00001126
Iteration 44/1000 | Loss: 0.00001126
Iteration 45/1000 | Loss: 0.00001126
Iteration 46/1000 | Loss: 0.00001125
Iteration 47/1000 | Loss: 0.00001125
Iteration 48/1000 | Loss: 0.00001124
Iteration 49/1000 | Loss: 0.00001124
Iteration 50/1000 | Loss: 0.00001123
Iteration 51/1000 | Loss: 0.00001122
Iteration 52/1000 | Loss: 0.00001122
Iteration 53/1000 | Loss: 0.00001122
Iteration 54/1000 | Loss: 0.00001122
Iteration 55/1000 | Loss: 0.00001122
Iteration 56/1000 | Loss: 0.00001122
Iteration 57/1000 | Loss: 0.00001121
Iteration 58/1000 | Loss: 0.00001121
Iteration 59/1000 | Loss: 0.00001121
Iteration 60/1000 | Loss: 0.00001121
Iteration 61/1000 | Loss: 0.00001121
Iteration 62/1000 | Loss: 0.00001121
Iteration 63/1000 | Loss: 0.00001121
Iteration 64/1000 | Loss: 0.00001121
Iteration 65/1000 | Loss: 0.00001120
Iteration 66/1000 | Loss: 0.00001120
Iteration 67/1000 | Loss: 0.00001120
Iteration 68/1000 | Loss: 0.00001119
Iteration 69/1000 | Loss: 0.00001119
Iteration 70/1000 | Loss: 0.00001119
Iteration 71/1000 | Loss: 0.00001119
Iteration 72/1000 | Loss: 0.00001119
Iteration 73/1000 | Loss: 0.00001119
Iteration 74/1000 | Loss: 0.00001119
Iteration 75/1000 | Loss: 0.00001119
Iteration 76/1000 | Loss: 0.00001119
Iteration 77/1000 | Loss: 0.00001119
Iteration 78/1000 | Loss: 0.00001118
Iteration 79/1000 | Loss: 0.00001118
Iteration 80/1000 | Loss: 0.00001118
Iteration 81/1000 | Loss: 0.00001118
Iteration 82/1000 | Loss: 0.00001118
Iteration 83/1000 | Loss: 0.00001118
Iteration 84/1000 | Loss: 0.00001118
Iteration 85/1000 | Loss: 0.00001118
Iteration 86/1000 | Loss: 0.00001117
Iteration 87/1000 | Loss: 0.00001117
Iteration 88/1000 | Loss: 0.00001116
Iteration 89/1000 | Loss: 0.00001116
Iteration 90/1000 | Loss: 0.00001116
Iteration 91/1000 | Loss: 0.00001113
Iteration 92/1000 | Loss: 0.00001113
Iteration 93/1000 | Loss: 0.00001112
Iteration 94/1000 | Loss: 0.00001111
Iteration 95/1000 | Loss: 0.00001111
Iteration 96/1000 | Loss: 0.00001111
Iteration 97/1000 | Loss: 0.00001110
Iteration 98/1000 | Loss: 0.00001110
Iteration 99/1000 | Loss: 0.00001110
Iteration 100/1000 | Loss: 0.00001110
Iteration 101/1000 | Loss: 0.00001109
Iteration 102/1000 | Loss: 0.00001109
Iteration 103/1000 | Loss: 0.00001108
Iteration 104/1000 | Loss: 0.00001108
Iteration 105/1000 | Loss: 0.00001108
Iteration 106/1000 | Loss: 0.00001108
Iteration 107/1000 | Loss: 0.00001108
Iteration 108/1000 | Loss: 0.00001107
Iteration 109/1000 | Loss: 0.00001107
Iteration 110/1000 | Loss: 0.00001107
Iteration 111/1000 | Loss: 0.00001106
Iteration 112/1000 | Loss: 0.00001106
Iteration 113/1000 | Loss: 0.00001106
Iteration 114/1000 | Loss: 0.00001106
Iteration 115/1000 | Loss: 0.00001105
Iteration 116/1000 | Loss: 0.00001105
Iteration 117/1000 | Loss: 0.00001105
Iteration 118/1000 | Loss: 0.00001105
Iteration 119/1000 | Loss: 0.00001104
Iteration 120/1000 | Loss: 0.00001104
Iteration 121/1000 | Loss: 0.00001104
Iteration 122/1000 | Loss: 0.00001104
Iteration 123/1000 | Loss: 0.00001104
Iteration 124/1000 | Loss: 0.00001104
Iteration 125/1000 | Loss: 0.00001104
Iteration 126/1000 | Loss: 0.00001104
Iteration 127/1000 | Loss: 0.00001103
Iteration 128/1000 | Loss: 0.00001103
Iteration 129/1000 | Loss: 0.00001103
Iteration 130/1000 | Loss: 0.00001103
Iteration 131/1000 | Loss: 0.00001103
Iteration 132/1000 | Loss: 0.00001103
Iteration 133/1000 | Loss: 0.00001103
Iteration 134/1000 | Loss: 0.00001103
Iteration 135/1000 | Loss: 0.00001103
Iteration 136/1000 | Loss: 0.00001103
Iteration 137/1000 | Loss: 0.00001103
Iteration 138/1000 | Loss: 0.00001103
Iteration 139/1000 | Loss: 0.00001103
Iteration 140/1000 | Loss: 0.00001103
Iteration 141/1000 | Loss: 0.00001102
Iteration 142/1000 | Loss: 0.00001102
Iteration 143/1000 | Loss: 0.00001102
Iteration 144/1000 | Loss: 0.00001102
Iteration 145/1000 | Loss: 0.00001102
Iteration 146/1000 | Loss: 0.00001102
Iteration 147/1000 | Loss: 0.00001102
Iteration 148/1000 | Loss: 0.00001102
Iteration 149/1000 | Loss: 0.00001102
Iteration 150/1000 | Loss: 0.00001102
Iteration 151/1000 | Loss: 0.00001102
Iteration 152/1000 | Loss: 0.00001101
Iteration 153/1000 | Loss: 0.00001101
Iteration 154/1000 | Loss: 0.00001101
Iteration 155/1000 | Loss: 0.00001101
Iteration 156/1000 | Loss: 0.00001101
Iteration 157/1000 | Loss: 0.00001101
Iteration 158/1000 | Loss: 0.00001101
Iteration 159/1000 | Loss: 0.00001101
Iteration 160/1000 | Loss: 0.00001101
Iteration 161/1000 | Loss: 0.00001101
Iteration 162/1000 | Loss: 0.00001101
Iteration 163/1000 | Loss: 0.00001101
Iteration 164/1000 | Loss: 0.00001101
Iteration 165/1000 | Loss: 0.00001101
Iteration 166/1000 | Loss: 0.00001101
Iteration 167/1000 | Loss: 0.00001100
Iteration 168/1000 | Loss: 0.00001100
Iteration 169/1000 | Loss: 0.00001100
Iteration 170/1000 | Loss: 0.00001100
Iteration 171/1000 | Loss: 0.00001100
Iteration 172/1000 | Loss: 0.00001100
Iteration 173/1000 | Loss: 0.00001100
Iteration 174/1000 | Loss: 0.00001100
Iteration 175/1000 | Loss: 0.00001100
Iteration 176/1000 | Loss: 0.00001100
Iteration 177/1000 | Loss: 0.00001100
Iteration 178/1000 | Loss: 0.00001100
Iteration 179/1000 | Loss: 0.00001100
Iteration 180/1000 | Loss: 0.00001100
Iteration 181/1000 | Loss: 0.00001100
Iteration 182/1000 | Loss: 0.00001100
Iteration 183/1000 | Loss: 0.00001100
Iteration 184/1000 | Loss: 0.00001099
Iteration 185/1000 | Loss: 0.00001099
Iteration 186/1000 | Loss: 0.00001099
Iteration 187/1000 | Loss: 0.00001099
Iteration 188/1000 | Loss: 0.00001099
Iteration 189/1000 | Loss: 0.00001099
Iteration 190/1000 | Loss: 0.00001099
Iteration 191/1000 | Loss: 0.00001099
Iteration 192/1000 | Loss: 0.00001099
Iteration 193/1000 | Loss: 0.00001099
Iteration 194/1000 | Loss: 0.00001099
Iteration 195/1000 | Loss: 0.00001099
Iteration 196/1000 | Loss: 0.00001099
Iteration 197/1000 | Loss: 0.00001099
Iteration 198/1000 | Loss: 0.00001099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.0990918781317305e-05, 1.0990918781317305e-05, 1.0990918781317305e-05, 1.0990918781317305e-05, 1.0990918781317305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0990918781317305e-05

Optimization complete. Final v2v error: 2.786574602127075 mm

Highest mean error: 3.1168694496154785 mm for frame 59

Lowest mean error: 2.585343837738037 mm for frame 5

Saving results

Total time: 41.84741497039795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021970
Iteration 2/25 | Loss: 0.00245956
Iteration 3/25 | Loss: 0.00161072
Iteration 4/25 | Loss: 0.00138936
Iteration 5/25 | Loss: 0.00132648
Iteration 6/25 | Loss: 0.00135121
Iteration 7/25 | Loss: 0.00139871
Iteration 8/25 | Loss: 0.00133776
Iteration 9/25 | Loss: 0.00129641
Iteration 10/25 | Loss: 0.00121593
Iteration 11/25 | Loss: 0.00117610
Iteration 12/25 | Loss: 0.00109824
Iteration 13/25 | Loss: 0.00100954
Iteration 14/25 | Loss: 0.00096079
Iteration 15/25 | Loss: 0.00092714
Iteration 16/25 | Loss: 0.00091046
Iteration 17/25 | Loss: 0.00090004
Iteration 18/25 | Loss: 0.00090692
Iteration 19/25 | Loss: 0.00088753
Iteration 20/25 | Loss: 0.00088142
Iteration 21/25 | Loss: 0.00087633
Iteration 22/25 | Loss: 0.00087524
Iteration 23/25 | Loss: 0.00087646
Iteration 24/25 | Loss: 0.00087492
Iteration 25/25 | Loss: 0.00087893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53596902
Iteration 2/25 | Loss: 0.00507269
Iteration 3/25 | Loss: 0.00284589
Iteration 4/25 | Loss: 0.00284588
Iteration 5/25 | Loss: 0.00284588
Iteration 6/25 | Loss: 0.00284588
Iteration 7/25 | Loss: 0.00284588
Iteration 8/25 | Loss: 0.00284588
Iteration 9/25 | Loss: 0.00284588
Iteration 10/25 | Loss: 0.00284588
Iteration 11/25 | Loss: 0.00284588
Iteration 12/25 | Loss: 0.00284588
Iteration 13/25 | Loss: 0.00284588
Iteration 14/25 | Loss: 0.00284588
Iteration 15/25 | Loss: 0.00284588
Iteration 16/25 | Loss: 0.00284588
Iteration 17/25 | Loss: 0.00284588
Iteration 18/25 | Loss: 0.00284588
Iteration 19/25 | Loss: 0.00284588
Iteration 20/25 | Loss: 0.00284588
Iteration 21/25 | Loss: 0.00284588
Iteration 22/25 | Loss: 0.00284588
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002845876617357135, 0.002845876617357135, 0.002845876617357135, 0.002845876617357135, 0.002845876617357135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002845876617357135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00284588
Iteration 2/1000 | Loss: 0.00169823
Iteration 3/1000 | Loss: 0.00510113
Iteration 4/1000 | Loss: 0.00677475
Iteration 5/1000 | Loss: 0.00798522
Iteration 6/1000 | Loss: 0.00792437
Iteration 7/1000 | Loss: 0.00318065
Iteration 8/1000 | Loss: 0.00417967
Iteration 9/1000 | Loss: 0.00502620
Iteration 10/1000 | Loss: 0.00431595
Iteration 11/1000 | Loss: 0.00233087
Iteration 12/1000 | Loss: 0.00272507
Iteration 13/1000 | Loss: 0.00481972
Iteration 14/1000 | Loss: 0.00414313
Iteration 15/1000 | Loss: 0.00331004
Iteration 16/1000 | Loss: 0.00520580
Iteration 17/1000 | Loss: 0.00305247
Iteration 18/1000 | Loss: 0.00560905
Iteration 19/1000 | Loss: 0.00811823
Iteration 20/1000 | Loss: 0.01210751
Iteration 21/1000 | Loss: 0.00715095
Iteration 22/1000 | Loss: 0.00736477
Iteration 23/1000 | Loss: 0.00836965
Iteration 24/1000 | Loss: 0.00320371
Iteration 25/1000 | Loss: 0.00440715
Iteration 26/1000 | Loss: 0.00434024
Iteration 27/1000 | Loss: 0.00416577
Iteration 28/1000 | Loss: 0.00138095
Iteration 29/1000 | Loss: 0.00213426
Iteration 30/1000 | Loss: 0.00304363
Iteration 31/1000 | Loss: 0.00553129
Iteration 32/1000 | Loss: 0.00250689
Iteration 33/1000 | Loss: 0.00248383
Iteration 34/1000 | Loss: 0.00478488
Iteration 35/1000 | Loss: 0.00433165
Iteration 36/1000 | Loss: 0.00432813
Iteration 37/1000 | Loss: 0.00217839
Iteration 38/1000 | Loss: 0.00191884
Iteration 39/1000 | Loss: 0.00274526
Iteration 40/1000 | Loss: 0.00371138
Iteration 41/1000 | Loss: 0.00394913
Iteration 42/1000 | Loss: 0.00282857
Iteration 43/1000 | Loss: 0.00268960
Iteration 44/1000 | Loss: 0.00188867
Iteration 45/1000 | Loss: 0.00466426
Iteration 46/1000 | Loss: 0.00177345
Iteration 47/1000 | Loss: 0.00185476
Iteration 48/1000 | Loss: 0.00269742
Iteration 49/1000 | Loss: 0.00232414
Iteration 50/1000 | Loss: 0.00438250
Iteration 51/1000 | Loss: 0.00229483
Iteration 52/1000 | Loss: 0.00167271
Iteration 53/1000 | Loss: 0.00275267
Iteration 54/1000 | Loss: 0.00169611
Iteration 55/1000 | Loss: 0.00140784
Iteration 56/1000 | Loss: 0.00140018
Iteration 57/1000 | Loss: 0.00284031
Iteration 58/1000 | Loss: 0.00224031
Iteration 59/1000 | Loss: 0.00100919
Iteration 60/1000 | Loss: 0.00233148
Iteration 61/1000 | Loss: 0.00347540
Iteration 62/1000 | Loss: 0.00165902
Iteration 63/1000 | Loss: 0.00098664
Iteration 64/1000 | Loss: 0.00112330
Iteration 65/1000 | Loss: 0.00151120
Iteration 66/1000 | Loss: 0.00160537
Iteration 67/1000 | Loss: 0.00211123
Iteration 68/1000 | Loss: 0.00168362
Iteration 69/1000 | Loss: 0.00214391
Iteration 70/1000 | Loss: 0.00112167
Iteration 71/1000 | Loss: 0.00313166
Iteration 72/1000 | Loss: 0.00171795
Iteration 73/1000 | Loss: 0.00136565
Iteration 74/1000 | Loss: 0.00246116
Iteration 75/1000 | Loss: 0.00084533
Iteration 76/1000 | Loss: 0.00109252
Iteration 77/1000 | Loss: 0.00199755
Iteration 78/1000 | Loss: 0.00189732
Iteration 79/1000 | Loss: 0.00247323
Iteration 80/1000 | Loss: 0.00168412
Iteration 81/1000 | Loss: 0.00058263
Iteration 82/1000 | Loss: 0.00211156
Iteration 83/1000 | Loss: 0.00077867
Iteration 84/1000 | Loss: 0.00111159
Iteration 85/1000 | Loss: 0.00128575
Iteration 86/1000 | Loss: 0.00120676
Iteration 87/1000 | Loss: 0.00129869
Iteration 88/1000 | Loss: 0.00151570
Iteration 89/1000 | Loss: 0.00039630
Iteration 90/1000 | Loss: 0.00060526
Iteration 91/1000 | Loss: 0.00145642
Iteration 92/1000 | Loss: 0.00110681
Iteration 93/1000 | Loss: 0.00084277
Iteration 94/1000 | Loss: 0.00066935
Iteration 95/1000 | Loss: 0.00144526
Iteration 96/1000 | Loss: 0.00088258
Iteration 97/1000 | Loss: 0.00173539
Iteration 98/1000 | Loss: 0.00158990
Iteration 99/1000 | Loss: 0.00103015
Iteration 100/1000 | Loss: 0.00089298
Iteration 101/1000 | Loss: 0.00096936
Iteration 102/1000 | Loss: 0.00057643
Iteration 103/1000 | Loss: 0.00067281
Iteration 104/1000 | Loss: 0.00087006
Iteration 105/1000 | Loss: 0.00074072
Iteration 106/1000 | Loss: 0.00156804
Iteration 107/1000 | Loss: 0.00059299
Iteration 108/1000 | Loss: 0.00113283
Iteration 109/1000 | Loss: 0.00099931
Iteration 110/1000 | Loss: 0.00087370
Iteration 111/1000 | Loss: 0.00083031
Iteration 112/1000 | Loss: 0.00248146
Iteration 113/1000 | Loss: 0.00162343
Iteration 114/1000 | Loss: 0.00089206
Iteration 115/1000 | Loss: 0.00113420
Iteration 116/1000 | Loss: 0.00088769
Iteration 117/1000 | Loss: 0.00125769
Iteration 118/1000 | Loss: 0.00138238
Iteration 119/1000 | Loss: 0.00149650
Iteration 120/1000 | Loss: 0.00069576
Iteration 121/1000 | Loss: 0.00099222
Iteration 122/1000 | Loss: 0.00092883
Iteration 123/1000 | Loss: 0.00064453
Iteration 124/1000 | Loss: 0.00099111
Iteration 125/1000 | Loss: 0.00066879
Iteration 126/1000 | Loss: 0.00135207
Iteration 127/1000 | Loss: 0.00091495
Iteration 128/1000 | Loss: 0.00029276
Iteration 129/1000 | Loss: 0.00044642
Iteration 130/1000 | Loss: 0.00123088
Iteration 131/1000 | Loss: 0.00110399
Iteration 132/1000 | Loss: 0.00053886
Iteration 133/1000 | Loss: 0.00150950
Iteration 134/1000 | Loss: 0.00025705
Iteration 135/1000 | Loss: 0.00025518
Iteration 136/1000 | Loss: 0.00037140
Iteration 137/1000 | Loss: 0.00041890
Iteration 138/1000 | Loss: 0.00049040
Iteration 139/1000 | Loss: 0.00034359
Iteration 140/1000 | Loss: 0.00104886
Iteration 141/1000 | Loss: 0.00104769
Iteration 142/1000 | Loss: 0.00101973
Iteration 143/1000 | Loss: 0.00038494
Iteration 144/1000 | Loss: 0.00040605
Iteration 145/1000 | Loss: 0.00107284
Iteration 146/1000 | Loss: 0.00107651
Iteration 147/1000 | Loss: 0.00132224
Iteration 148/1000 | Loss: 0.00146498
Iteration 149/1000 | Loss: 0.00069176
Iteration 150/1000 | Loss: 0.00058843
Iteration 151/1000 | Loss: 0.00035013
Iteration 152/1000 | Loss: 0.00040014
Iteration 153/1000 | Loss: 0.00035141
Iteration 154/1000 | Loss: 0.00014062
Iteration 155/1000 | Loss: 0.00012815
Iteration 156/1000 | Loss: 0.00055560
Iteration 157/1000 | Loss: 0.00042007
Iteration 158/1000 | Loss: 0.00063479
Iteration 159/1000 | Loss: 0.00063144
Iteration 160/1000 | Loss: 0.00051090
Iteration 161/1000 | Loss: 0.00063169
Iteration 162/1000 | Loss: 0.00068944
Iteration 163/1000 | Loss: 0.00068612
Iteration 164/1000 | Loss: 0.00059526
Iteration 165/1000 | Loss: 0.00091311
Iteration 166/1000 | Loss: 0.00117067
Iteration 167/1000 | Loss: 0.00091571
Iteration 168/1000 | Loss: 0.00102442
Iteration 169/1000 | Loss: 0.00099759
Iteration 170/1000 | Loss: 0.00064764
Iteration 171/1000 | Loss: 0.00121722
Iteration 172/1000 | Loss: 0.00110774
Iteration 173/1000 | Loss: 0.00066514
Iteration 174/1000 | Loss: 0.00112859
Iteration 175/1000 | Loss: 0.00098506
Iteration 176/1000 | Loss: 0.00063927
Iteration 177/1000 | Loss: 0.00051935
Iteration 178/1000 | Loss: 0.00067463
Iteration 179/1000 | Loss: 0.00085921
Iteration 180/1000 | Loss: 0.00069510
Iteration 181/1000 | Loss: 0.00098001
Iteration 182/1000 | Loss: 0.00098229
Iteration 183/1000 | Loss: 0.00118449
Iteration 184/1000 | Loss: 0.00097177
Iteration 185/1000 | Loss: 0.00077713
Iteration 186/1000 | Loss: 0.00083739
Iteration 187/1000 | Loss: 0.00061315
Iteration 188/1000 | Loss: 0.00124486
Iteration 189/1000 | Loss: 0.00047613
Iteration 190/1000 | Loss: 0.00077973
Iteration 191/1000 | Loss: 0.00147218
Iteration 192/1000 | Loss: 0.00026541
Iteration 193/1000 | Loss: 0.00018499
Iteration 194/1000 | Loss: 0.00023327
Iteration 195/1000 | Loss: 0.00074595
Iteration 196/1000 | Loss: 0.00017857
Iteration 197/1000 | Loss: 0.00053064
Iteration 198/1000 | Loss: 0.00053816
Iteration 199/1000 | Loss: 0.00016297
Iteration 200/1000 | Loss: 0.00014494
Iteration 201/1000 | Loss: 0.00017814
Iteration 202/1000 | Loss: 0.00033556
Iteration 203/1000 | Loss: 0.00030068
Iteration 204/1000 | Loss: 0.00035967
Iteration 205/1000 | Loss: 0.00045988
Iteration 206/1000 | Loss: 0.00020622
Iteration 207/1000 | Loss: 0.00044816
Iteration 208/1000 | Loss: 0.00036999
Iteration 209/1000 | Loss: 0.00038586
Iteration 210/1000 | Loss: 0.00034164
Iteration 211/1000 | Loss: 0.00036371
Iteration 212/1000 | Loss: 0.00024478
Iteration 213/1000 | Loss: 0.00091938
Iteration 214/1000 | Loss: 0.00028034
Iteration 215/1000 | Loss: 0.00034836
Iteration 216/1000 | Loss: 0.00055930
Iteration 217/1000 | Loss: 0.00049114
Iteration 218/1000 | Loss: 0.00053331
Iteration 219/1000 | Loss: 0.00077633
Iteration 220/1000 | Loss: 0.00079899
Iteration 221/1000 | Loss: 0.00057708
Iteration 222/1000 | Loss: 0.00085111
Iteration 223/1000 | Loss: 0.00034891
Iteration 224/1000 | Loss: 0.00028324
Iteration 225/1000 | Loss: 0.00033569
Iteration 226/1000 | Loss: 0.00039866
Iteration 227/1000 | Loss: 0.00019466
Iteration 228/1000 | Loss: 0.00020649
Iteration 229/1000 | Loss: 0.00104803
Iteration 230/1000 | Loss: 0.00135143
Iteration 231/1000 | Loss: 0.00059022
Iteration 232/1000 | Loss: 0.00150066
Iteration 233/1000 | Loss: 0.00040210
Iteration 234/1000 | Loss: 0.00020071
Iteration 235/1000 | Loss: 0.00013448
Iteration 236/1000 | Loss: 0.00018501
Iteration 237/1000 | Loss: 0.00110875
Iteration 238/1000 | Loss: 0.00053005
Iteration 239/1000 | Loss: 0.00033640
Iteration 240/1000 | Loss: 0.00025418
Iteration 241/1000 | Loss: 0.00019510
Iteration 242/1000 | Loss: 0.00031815
Iteration 243/1000 | Loss: 0.00029956
Iteration 244/1000 | Loss: 0.00040185
Iteration 245/1000 | Loss: 0.00034279
Iteration 246/1000 | Loss: 0.00036794
Iteration 247/1000 | Loss: 0.00038512
Iteration 248/1000 | Loss: 0.00036672
Iteration 249/1000 | Loss: 0.00021328
Iteration 250/1000 | Loss: 0.00029038
Iteration 251/1000 | Loss: 0.00035721
Iteration 252/1000 | Loss: 0.00035882
Iteration 253/1000 | Loss: 0.00020574
Iteration 254/1000 | Loss: 0.00028819
Iteration 255/1000 | Loss: 0.00034276
Iteration 256/1000 | Loss: 0.00034449
Iteration 257/1000 | Loss: 0.00072915
Iteration 258/1000 | Loss: 0.00039837
Iteration 259/1000 | Loss: 0.00062094
Iteration 260/1000 | Loss: 0.00085551
Iteration 261/1000 | Loss: 0.00103495
Iteration 262/1000 | Loss: 0.00059077
Iteration 263/1000 | Loss: 0.00042149
Iteration 264/1000 | Loss: 0.00029660
Iteration 265/1000 | Loss: 0.00020727
Iteration 266/1000 | Loss: 0.00039246
Iteration 267/1000 | Loss: 0.00039944
Iteration 268/1000 | Loss: 0.00035543
Iteration 269/1000 | Loss: 0.00028074
Iteration 270/1000 | Loss: 0.00028892
Iteration 271/1000 | Loss: 0.00039166
Iteration 272/1000 | Loss: 0.00037121
Iteration 273/1000 | Loss: 0.00040217
Iteration 274/1000 | Loss: 0.00037372
Iteration 275/1000 | Loss: 0.00042688
Iteration 276/1000 | Loss: 0.00036387
Iteration 277/1000 | Loss: 0.00030922
Iteration 278/1000 | Loss: 0.00051536
Iteration 279/1000 | Loss: 0.00030681
Iteration 280/1000 | Loss: 0.00037235
Iteration 281/1000 | Loss: 0.00025098
Iteration 282/1000 | Loss: 0.00161128
Iteration 283/1000 | Loss: 0.00171307
Iteration 284/1000 | Loss: 0.00162983
Iteration 285/1000 | Loss: 0.00078343
Iteration 286/1000 | Loss: 0.00019671
Iteration 287/1000 | Loss: 0.00037893
Iteration 288/1000 | Loss: 0.00042671
Iteration 289/1000 | Loss: 0.00018463
Iteration 290/1000 | Loss: 0.00021280
Iteration 291/1000 | Loss: 0.00148822
Iteration 292/1000 | Loss: 0.00047052
Iteration 293/1000 | Loss: 0.00146412
Iteration 294/1000 | Loss: 0.00202571
Iteration 295/1000 | Loss: 0.00148792
Iteration 296/1000 | Loss: 0.00185866
Iteration 297/1000 | Loss: 0.00138310
Iteration 298/1000 | Loss: 0.00051979
Iteration 299/1000 | Loss: 0.00027692
Iteration 300/1000 | Loss: 0.00046496
Iteration 301/1000 | Loss: 0.00023028
Iteration 302/1000 | Loss: 0.00040100
Iteration 303/1000 | Loss: 0.00040889
Iteration 304/1000 | Loss: 0.00020433
Iteration 305/1000 | Loss: 0.00026130
Iteration 306/1000 | Loss: 0.00033462
Iteration 307/1000 | Loss: 0.00021870
Iteration 308/1000 | Loss: 0.00035775
Iteration 309/1000 | Loss: 0.00029541
Iteration 310/1000 | Loss: 0.00032342
Iteration 311/1000 | Loss: 0.00016058
Iteration 312/1000 | Loss: 0.00039646
Iteration 313/1000 | Loss: 0.00023222
Iteration 314/1000 | Loss: 0.00022577
Iteration 315/1000 | Loss: 0.00037871
Iteration 316/1000 | Loss: 0.00033297
Iteration 317/1000 | Loss: 0.00030759
Iteration 318/1000 | Loss: 0.00029361
Iteration 319/1000 | Loss: 0.00035363
Iteration 320/1000 | Loss: 0.00022988
Iteration 321/1000 | Loss: 0.00040617
Iteration 322/1000 | Loss: 0.00042356
Iteration 323/1000 | Loss: 0.00034894
Iteration 324/1000 | Loss: 0.00038639
Iteration 325/1000 | Loss: 0.00032618
Iteration 326/1000 | Loss: 0.00031013
Iteration 327/1000 | Loss: 0.00021011
Iteration 328/1000 | Loss: 0.00035721
Iteration 329/1000 | Loss: 0.00017604
Iteration 330/1000 | Loss: 0.00012354
Iteration 331/1000 | Loss: 0.00010281
Iteration 332/1000 | Loss: 0.00029955
Iteration 333/1000 | Loss: 0.00031178
Iteration 334/1000 | Loss: 0.00028438
Iteration 335/1000 | Loss: 0.00032918
Iteration 336/1000 | Loss: 0.00030654
Iteration 337/1000 | Loss: 0.00031835
Iteration 338/1000 | Loss: 0.00027837
Iteration 339/1000 | Loss: 0.00027052
Iteration 340/1000 | Loss: 0.00032801
Iteration 341/1000 | Loss: 0.00037254
Iteration 342/1000 | Loss: 0.00026152
Iteration 343/1000 | Loss: 0.00032712
Iteration 344/1000 | Loss: 0.00052051
Iteration 345/1000 | Loss: 0.00040710
Iteration 346/1000 | Loss: 0.00050923
Iteration 347/1000 | Loss: 0.00057697
Iteration 348/1000 | Loss: 0.00033486
Iteration 349/1000 | Loss: 0.00024747
Iteration 350/1000 | Loss: 0.00028463
Iteration 351/1000 | Loss: 0.00021996
Iteration 352/1000 | Loss: 0.00032569
Iteration 353/1000 | Loss: 0.00021566
Iteration 354/1000 | Loss: 0.00032607
Iteration 355/1000 | Loss: 0.00047805
Iteration 356/1000 | Loss: 0.00031466
Iteration 357/1000 | Loss: 0.00024389
Iteration 358/1000 | Loss: 0.00028857
Iteration 359/1000 | Loss: 0.00031197
Iteration 360/1000 | Loss: 0.00029814
Iteration 361/1000 | Loss: 0.00024095
Iteration 362/1000 | Loss: 0.00036206
Iteration 363/1000 | Loss: 0.00024693
Iteration 364/1000 | Loss: 0.00035948
Iteration 365/1000 | Loss: 0.00024613
Iteration 366/1000 | Loss: 0.00035220
Iteration 367/1000 | Loss: 0.00025194
Iteration 368/1000 | Loss: 0.00096465
Iteration 369/1000 | Loss: 0.00022344
Iteration 370/1000 | Loss: 0.00032784
Iteration 371/1000 | Loss: 0.00030020
Iteration 372/1000 | Loss: 0.00030452
Iteration 373/1000 | Loss: 0.00027981
Iteration 374/1000 | Loss: 0.00033732
Iteration 375/1000 | Loss: 0.00027839
Iteration 376/1000 | Loss: 0.00026773
Iteration 377/1000 | Loss: 0.00035716
Iteration 378/1000 | Loss: 0.00030747
Iteration 379/1000 | Loss: 0.00036299
Iteration 380/1000 | Loss: 0.00036937
Iteration 381/1000 | Loss: 0.00012630
Iteration 382/1000 | Loss: 0.00023893
Iteration 383/1000 | Loss: 0.00107750
Iteration 384/1000 | Loss: 0.00032783
Iteration 385/1000 | Loss: 0.00021329
Iteration 386/1000 | Loss: 0.00028864
Iteration 387/1000 | Loss: 0.00018905
Iteration 388/1000 | Loss: 0.00048165
Iteration 389/1000 | Loss: 0.00009804
Iteration 390/1000 | Loss: 0.00006101
Iteration 391/1000 | Loss: 0.00004185
Iteration 392/1000 | Loss: 0.00003267
Iteration 393/1000 | Loss: 0.00002823
Iteration 394/1000 | Loss: 0.00004091
Iteration 395/1000 | Loss: 0.00002736
Iteration 396/1000 | Loss: 0.00012658
Iteration 397/1000 | Loss: 0.00002334
Iteration 398/1000 | Loss: 0.00002159
Iteration 399/1000 | Loss: 0.00003613
Iteration 400/1000 | Loss: 0.00003381
Iteration 401/1000 | Loss: 0.00003496
Iteration 402/1000 | Loss: 0.00003254
Iteration 403/1000 | Loss: 0.00003341
Iteration 404/1000 | Loss: 0.00002073
Iteration 405/1000 | Loss: 0.00003414
Iteration 406/1000 | Loss: 0.00002847
Iteration 407/1000 | Loss: 0.00003226
Iteration 408/1000 | Loss: 0.00002098
Iteration 409/1000 | Loss: 0.00003280
Iteration 410/1000 | Loss: 0.00001857
Iteration 411/1000 | Loss: 0.00003393
Iteration 412/1000 | Loss: 0.00003288
Iteration 413/1000 | Loss: 0.00003407
Iteration 414/1000 | Loss: 0.00003480
Iteration 415/1000 | Loss: 0.00003332
Iteration 416/1000 | Loss: 0.00003283
Iteration 417/1000 | Loss: 0.00003359
Iteration 418/1000 | Loss: 0.00003231
Iteration 419/1000 | Loss: 0.00003304
Iteration 420/1000 | Loss: 0.00002367
Iteration 421/1000 | Loss: 0.00002160
Iteration 422/1000 | Loss: 0.00002895
Iteration 423/1000 | Loss: 0.00002002
Iteration 424/1000 | Loss: 0.00003289
Iteration 425/1000 | Loss: 0.00003006
Iteration 426/1000 | Loss: 0.00002053
Iteration 427/1000 | Loss: 0.00001866
Iteration 428/1000 | Loss: 0.00001754
Iteration 429/1000 | Loss: 0.00001681
Iteration 430/1000 | Loss: 0.00001590
Iteration 431/1000 | Loss: 0.00001514
Iteration 432/1000 | Loss: 0.00001488
Iteration 433/1000 | Loss: 0.00001473
Iteration 434/1000 | Loss: 0.00001471
Iteration 435/1000 | Loss: 0.00001471
Iteration 436/1000 | Loss: 0.00001467
Iteration 437/1000 | Loss: 0.00001467
Iteration 438/1000 | Loss: 0.00001465
Iteration 439/1000 | Loss: 0.00001465
Iteration 440/1000 | Loss: 0.00001464
Iteration 441/1000 | Loss: 0.00001462
Iteration 442/1000 | Loss: 0.00001461
Iteration 443/1000 | Loss: 0.00001461
Iteration 444/1000 | Loss: 0.00001461
Iteration 445/1000 | Loss: 0.00001459
Iteration 446/1000 | Loss: 0.00001459
Iteration 447/1000 | Loss: 0.00001458
Iteration 448/1000 | Loss: 0.00001458
Iteration 449/1000 | Loss: 0.00001458
Iteration 450/1000 | Loss: 0.00001457
Iteration 451/1000 | Loss: 0.00001457
Iteration 452/1000 | Loss: 0.00001457
Iteration 453/1000 | Loss: 0.00001456
Iteration 454/1000 | Loss: 0.00001456
Iteration 455/1000 | Loss: 0.00001456
Iteration 456/1000 | Loss: 0.00001455
Iteration 457/1000 | Loss: 0.00001455
Iteration 458/1000 | Loss: 0.00001455
Iteration 459/1000 | Loss: 0.00001455
Iteration 460/1000 | Loss: 0.00001454
Iteration 461/1000 | Loss: 0.00001454
Iteration 462/1000 | Loss: 0.00001454
Iteration 463/1000 | Loss: 0.00001453
Iteration 464/1000 | Loss: 0.00001453
Iteration 465/1000 | Loss: 0.00001453
Iteration 466/1000 | Loss: 0.00001453
Iteration 467/1000 | Loss: 0.00001452
Iteration 468/1000 | Loss: 0.00001452
Iteration 469/1000 | Loss: 0.00001452
Iteration 470/1000 | Loss: 0.00001452
Iteration 471/1000 | Loss: 0.00001452
Iteration 472/1000 | Loss: 0.00001451
Iteration 473/1000 | Loss: 0.00001451
Iteration 474/1000 | Loss: 0.00001451
Iteration 475/1000 | Loss: 0.00001451
Iteration 476/1000 | Loss: 0.00001450
Iteration 477/1000 | Loss: 0.00001450
Iteration 478/1000 | Loss: 0.00001450
Iteration 479/1000 | Loss: 0.00001450
Iteration 480/1000 | Loss: 0.00001450
Iteration 481/1000 | Loss: 0.00001449
Iteration 482/1000 | Loss: 0.00001449
Iteration 483/1000 | Loss: 0.00001449
Iteration 484/1000 | Loss: 0.00001449
Iteration 485/1000 | Loss: 0.00001449
Iteration 486/1000 | Loss: 0.00001449
Iteration 487/1000 | Loss: 0.00001449
Iteration 488/1000 | Loss: 0.00001449
Iteration 489/1000 | Loss: 0.00001449
Iteration 490/1000 | Loss: 0.00001448
Iteration 491/1000 | Loss: 0.00001448
Iteration 492/1000 | Loss: 0.00001448
Iteration 493/1000 | Loss: 0.00001448
Iteration 494/1000 | Loss: 0.00001447
Iteration 495/1000 | Loss: 0.00001447
Iteration 496/1000 | Loss: 0.00001447
Iteration 497/1000 | Loss: 0.00001446
Iteration 498/1000 | Loss: 0.00001446
Iteration 499/1000 | Loss: 0.00001446
Iteration 500/1000 | Loss: 0.00001445
Iteration 501/1000 | Loss: 0.00001445
Iteration 502/1000 | Loss: 0.00001444
Iteration 503/1000 | Loss: 0.00001444
Iteration 504/1000 | Loss: 0.00001443
Iteration 505/1000 | Loss: 0.00001443
Iteration 506/1000 | Loss: 0.00001442
Iteration 507/1000 | Loss: 0.00001441
Iteration 508/1000 | Loss: 0.00001441
Iteration 509/1000 | Loss: 0.00001441
Iteration 510/1000 | Loss: 0.00001441
Iteration 511/1000 | Loss: 0.00001441
Iteration 512/1000 | Loss: 0.00001440
Iteration 513/1000 | Loss: 0.00001440
Iteration 514/1000 | Loss: 0.00001439
Iteration 515/1000 | Loss: 0.00001439
Iteration 516/1000 | Loss: 0.00001439
Iteration 517/1000 | Loss: 0.00001438
Iteration 518/1000 | Loss: 0.00001438
Iteration 519/1000 | Loss: 0.00001438
Iteration 520/1000 | Loss: 0.00001438
Iteration 521/1000 | Loss: 0.00001438
Iteration 522/1000 | Loss: 0.00001438
Iteration 523/1000 | Loss: 0.00001438
Iteration 524/1000 | Loss: 0.00001438
Iteration 525/1000 | Loss: 0.00001438
Iteration 526/1000 | Loss: 0.00001438
Iteration 527/1000 | Loss: 0.00001438
Iteration 528/1000 | Loss: 0.00001437
Iteration 529/1000 | Loss: 0.00001437
Iteration 530/1000 | Loss: 0.00001437
Iteration 531/1000 | Loss: 0.00001437
Iteration 532/1000 | Loss: 0.00001436
Iteration 533/1000 | Loss: 0.00001436
Iteration 534/1000 | Loss: 0.00001436
Iteration 535/1000 | Loss: 0.00001436
Iteration 536/1000 | Loss: 0.00001435
Iteration 537/1000 | Loss: 0.00001435
Iteration 538/1000 | Loss: 0.00001435
Iteration 539/1000 | Loss: 0.00001435
Iteration 540/1000 | Loss: 0.00001435
Iteration 541/1000 | Loss: 0.00001435
Iteration 542/1000 | Loss: 0.00001434
Iteration 543/1000 | Loss: 0.00001434
Iteration 544/1000 | Loss: 0.00001434
Iteration 545/1000 | Loss: 0.00001434
Iteration 546/1000 | Loss: 0.00001434
Iteration 547/1000 | Loss: 0.00001434
Iteration 548/1000 | Loss: 0.00001433
Iteration 549/1000 | Loss: 0.00001433
Iteration 550/1000 | Loss: 0.00001433
Iteration 551/1000 | Loss: 0.00001433
Iteration 552/1000 | Loss: 0.00001433
Iteration 553/1000 | Loss: 0.00001433
Iteration 554/1000 | Loss: 0.00001433
Iteration 555/1000 | Loss: 0.00001433
Iteration 556/1000 | Loss: 0.00001433
Iteration 557/1000 | Loss: 0.00001433
Iteration 558/1000 | Loss: 0.00001433
Iteration 559/1000 | Loss: 0.00001433
Iteration 560/1000 | Loss: 0.00001433
Iteration 561/1000 | Loss: 0.00001433
Iteration 562/1000 | Loss: 0.00001433
Iteration 563/1000 | Loss: 0.00001433
Iteration 564/1000 | Loss: 0.00001433
Iteration 565/1000 | Loss: 0.00001433
Iteration 566/1000 | Loss: 0.00001432
Iteration 567/1000 | Loss: 0.00001432
Iteration 568/1000 | Loss: 0.00001432
Iteration 569/1000 | Loss: 0.00001432
Iteration 570/1000 | Loss: 0.00001432
Iteration 571/1000 | Loss: 0.00001432
Iteration 572/1000 | Loss: 0.00001432
Iteration 573/1000 | Loss: 0.00001432
Iteration 574/1000 | Loss: 0.00001432
Iteration 575/1000 | Loss: 0.00001432
Iteration 576/1000 | Loss: 0.00001431
Iteration 577/1000 | Loss: 0.00001431
Iteration 578/1000 | Loss: 0.00001431
Iteration 579/1000 | Loss: 0.00001431
Iteration 580/1000 | Loss: 0.00001431
Iteration 581/1000 | Loss: 0.00001431
Iteration 582/1000 | Loss: 0.00001431
Iteration 583/1000 | Loss: 0.00001431
Iteration 584/1000 | Loss: 0.00001431
Iteration 585/1000 | Loss: 0.00001431
Iteration 586/1000 | Loss: 0.00001430
Iteration 587/1000 | Loss: 0.00001430
Iteration 588/1000 | Loss: 0.00001430
Iteration 589/1000 | Loss: 0.00001430
Iteration 590/1000 | Loss: 0.00001430
Iteration 591/1000 | Loss: 0.00001430
Iteration 592/1000 | Loss: 0.00001430
Iteration 593/1000 | Loss: 0.00001430
Iteration 594/1000 | Loss: 0.00001430
Iteration 595/1000 | Loss: 0.00001430
Iteration 596/1000 | Loss: 0.00001430
Iteration 597/1000 | Loss: 0.00001430
Iteration 598/1000 | Loss: 0.00001430
Iteration 599/1000 | Loss: 0.00001430
Iteration 600/1000 | Loss: 0.00001430
Iteration 601/1000 | Loss: 0.00001430
Iteration 602/1000 | Loss: 0.00001430
Iteration 603/1000 | Loss: 0.00001430
Iteration 604/1000 | Loss: 0.00001430
Iteration 605/1000 | Loss: 0.00001430
Iteration 606/1000 | Loss: 0.00001430
Iteration 607/1000 | Loss: 0.00001430
Iteration 608/1000 | Loss: 0.00001430
Iteration 609/1000 | Loss: 0.00001430
Iteration 610/1000 | Loss: 0.00001430
Iteration 611/1000 | Loss: 0.00001430
Iteration 612/1000 | Loss: 0.00001430
Iteration 613/1000 | Loss: 0.00001430
Iteration 614/1000 | Loss: 0.00001430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 614. Stopping optimization.
Last 5 losses: [1.4303003808890935e-05, 1.4303003808890935e-05, 1.4303003808890935e-05, 1.4303003808890935e-05, 1.4303003808890935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4303003808890935e-05

Optimization complete. Final v2v error: 3.2001843452453613 mm

Highest mean error: 4.6968584060668945 mm for frame 47

Lowest mean error: 2.8333170413970947 mm for frame 12

Saving results

Total time: 648.4166080951691
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00756712
Iteration 2/25 | Loss: 0.00152822
Iteration 3/25 | Loss: 0.00091517
Iteration 4/25 | Loss: 0.00080813
Iteration 5/25 | Loss: 0.00079314
Iteration 6/25 | Loss: 0.00082051
Iteration 7/25 | Loss: 0.00079496
Iteration 8/25 | Loss: 0.00079041
Iteration 9/25 | Loss: 0.00078459
Iteration 10/25 | Loss: 0.00077932
Iteration 11/25 | Loss: 0.00078320
Iteration 12/25 | Loss: 0.00082150
Iteration 13/25 | Loss: 0.00078636
Iteration 14/25 | Loss: 0.00078076
Iteration 15/25 | Loss: 0.00077672
Iteration 16/25 | Loss: 0.00077133
Iteration 17/25 | Loss: 0.00077118
Iteration 18/25 | Loss: 0.00077146
Iteration 19/25 | Loss: 0.00077068
Iteration 20/25 | Loss: 0.00076949
Iteration 21/25 | Loss: 0.00076807
Iteration 22/25 | Loss: 0.00076762
Iteration 23/25 | Loss: 0.00076738
Iteration 24/25 | Loss: 0.00076915
Iteration 25/25 | Loss: 0.00076789

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74392748
Iteration 2/25 | Loss: 0.00038447
Iteration 3/25 | Loss: 0.00038447
Iteration 4/25 | Loss: 0.00038447
Iteration 5/25 | Loss: 0.00038447
Iteration 6/25 | Loss: 0.00038447
Iteration 7/25 | Loss: 0.00038447
Iteration 8/25 | Loss: 0.00038447
Iteration 9/25 | Loss: 0.00038447
Iteration 10/25 | Loss: 0.00038447
Iteration 11/25 | Loss: 0.00038447
Iteration 12/25 | Loss: 0.00038446
Iteration 13/25 | Loss: 0.00038446
Iteration 14/25 | Loss: 0.00038447
Iteration 15/25 | Loss: 0.00038447
Iteration 16/25 | Loss: 0.00038447
Iteration 17/25 | Loss: 0.00038447
Iteration 18/25 | Loss: 0.00038447
Iteration 19/25 | Loss: 0.00038447
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00038446500548161566, 0.00038446500548161566, 0.00038446500548161566, 0.00038446500548161566, 0.00038446500548161566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038446500548161566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038447
Iteration 2/1000 | Loss: 0.00004810
Iteration 3/1000 | Loss: 0.00003199
Iteration 4/1000 | Loss: 0.00002860
Iteration 5/1000 | Loss: 0.00002695
Iteration 6/1000 | Loss: 0.00002576
Iteration 7/1000 | Loss: 0.00002512
Iteration 8/1000 | Loss: 0.00002466
Iteration 9/1000 | Loss: 0.00002430
Iteration 10/1000 | Loss: 0.00002402
Iteration 11/1000 | Loss: 0.00002378
Iteration 12/1000 | Loss: 0.00002356
Iteration 13/1000 | Loss: 0.00002333
Iteration 14/1000 | Loss: 0.00002315
Iteration 15/1000 | Loss: 0.00002308
Iteration 16/1000 | Loss: 0.00002295
Iteration 17/1000 | Loss: 0.00002291
Iteration 18/1000 | Loss: 0.00002291
Iteration 19/1000 | Loss: 0.00002290
Iteration 20/1000 | Loss: 0.00002288
Iteration 21/1000 | Loss: 0.00002286
Iteration 22/1000 | Loss: 0.00002285
Iteration 23/1000 | Loss: 0.00002285
Iteration 24/1000 | Loss: 0.00002283
Iteration 25/1000 | Loss: 0.00002279
Iteration 26/1000 | Loss: 0.00002277
Iteration 27/1000 | Loss: 0.00002277
Iteration 28/1000 | Loss: 0.00002277
Iteration 29/1000 | Loss: 0.00002276
Iteration 30/1000 | Loss: 0.00002275
Iteration 31/1000 | Loss: 0.00002274
Iteration 32/1000 | Loss: 0.00002274
Iteration 33/1000 | Loss: 0.00002274
Iteration 34/1000 | Loss: 0.00002273
Iteration 35/1000 | Loss: 0.00002273
Iteration 36/1000 | Loss: 0.00002272
Iteration 37/1000 | Loss: 0.00002272
Iteration 38/1000 | Loss: 0.00002271
Iteration 39/1000 | Loss: 0.00002270
Iteration 40/1000 | Loss: 0.00002270
Iteration 41/1000 | Loss: 0.00002270
Iteration 42/1000 | Loss: 0.00002269
Iteration 43/1000 | Loss: 0.00002269
Iteration 44/1000 | Loss: 0.00002268
Iteration 45/1000 | Loss: 0.00002268
Iteration 46/1000 | Loss: 0.00002268
Iteration 47/1000 | Loss: 0.00002267
Iteration 48/1000 | Loss: 0.00002265
Iteration 49/1000 | Loss: 0.00002265
Iteration 50/1000 | Loss: 0.00002265
Iteration 51/1000 | Loss: 0.00002265
Iteration 52/1000 | Loss: 0.00002265
Iteration 53/1000 | Loss: 0.00002265
Iteration 54/1000 | Loss: 0.00002264
Iteration 55/1000 | Loss: 0.00002264
Iteration 56/1000 | Loss: 0.00002263
Iteration 57/1000 | Loss: 0.00002263
Iteration 58/1000 | Loss: 0.00002263
Iteration 59/1000 | Loss: 0.00002262
Iteration 60/1000 | Loss: 0.00002262
Iteration 61/1000 | Loss: 0.00002262
Iteration 62/1000 | Loss: 0.00002261
Iteration 63/1000 | Loss: 0.00002261
Iteration 64/1000 | Loss: 0.00002261
Iteration 65/1000 | Loss: 0.00002261
Iteration 66/1000 | Loss: 0.00002261
Iteration 67/1000 | Loss: 0.00002261
Iteration 68/1000 | Loss: 0.00002260
Iteration 69/1000 | Loss: 0.00002260
Iteration 70/1000 | Loss: 0.00002260
Iteration 71/1000 | Loss: 0.00002259
Iteration 72/1000 | Loss: 0.00002259
Iteration 73/1000 | Loss: 0.00002259
Iteration 74/1000 | Loss: 0.00002259
Iteration 75/1000 | Loss: 0.00002258
Iteration 76/1000 | Loss: 0.00002258
Iteration 77/1000 | Loss: 0.00002258
Iteration 78/1000 | Loss: 0.00002257
Iteration 79/1000 | Loss: 0.00002257
Iteration 80/1000 | Loss: 0.00002257
Iteration 81/1000 | Loss: 0.00002256
Iteration 82/1000 | Loss: 0.00002256
Iteration 83/1000 | Loss: 0.00002256
Iteration 84/1000 | Loss: 0.00002256
Iteration 85/1000 | Loss: 0.00002256
Iteration 86/1000 | Loss: 0.00002255
Iteration 87/1000 | Loss: 0.00002255
Iteration 88/1000 | Loss: 0.00002255
Iteration 89/1000 | Loss: 0.00002255
Iteration 90/1000 | Loss: 0.00002254
Iteration 91/1000 | Loss: 0.00002254
Iteration 92/1000 | Loss: 0.00002254
Iteration 93/1000 | Loss: 0.00002254
Iteration 94/1000 | Loss: 0.00002254
Iteration 95/1000 | Loss: 0.00002254
Iteration 96/1000 | Loss: 0.00002253
Iteration 97/1000 | Loss: 0.00002253
Iteration 98/1000 | Loss: 0.00002253
Iteration 99/1000 | Loss: 0.00002253
Iteration 100/1000 | Loss: 0.00002252
Iteration 101/1000 | Loss: 0.00002252
Iteration 102/1000 | Loss: 0.00002252
Iteration 103/1000 | Loss: 0.00002252
Iteration 104/1000 | Loss: 0.00002252
Iteration 105/1000 | Loss: 0.00002252
Iteration 106/1000 | Loss: 0.00002251
Iteration 107/1000 | Loss: 0.00002251
Iteration 108/1000 | Loss: 0.00002251
Iteration 109/1000 | Loss: 0.00002251
Iteration 110/1000 | Loss: 0.00002251
Iteration 111/1000 | Loss: 0.00002251
Iteration 112/1000 | Loss: 0.00002251
Iteration 113/1000 | Loss: 0.00002251
Iteration 114/1000 | Loss: 0.00002251
Iteration 115/1000 | Loss: 0.00002251
Iteration 116/1000 | Loss: 0.00002251
Iteration 117/1000 | Loss: 0.00002251
Iteration 118/1000 | Loss: 0.00002251
Iteration 119/1000 | Loss: 0.00002251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.250703619210981e-05, 2.250703619210981e-05, 2.250703619210981e-05, 2.250703619210981e-05, 2.250703619210981e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.250703619210981e-05

Optimization complete. Final v2v error: 3.7538065910339355 mm

Highest mean error: 4.975086212158203 mm for frame 77

Lowest mean error: 2.7517526149749756 mm for frame 188

Saving results

Total time: 87.47001910209656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413000
Iteration 2/25 | Loss: 0.00076005
Iteration 3/25 | Loss: 0.00065482
Iteration 4/25 | Loss: 0.00063092
Iteration 5/25 | Loss: 0.00061990
Iteration 6/25 | Loss: 0.00061803
Iteration 7/25 | Loss: 0.00061784
Iteration 8/25 | Loss: 0.00061784
Iteration 9/25 | Loss: 0.00061784
Iteration 10/25 | Loss: 0.00061784
Iteration 11/25 | Loss: 0.00061784
Iteration 12/25 | Loss: 0.00061784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006178373005241156, 0.0006178373005241156, 0.0006178373005241156, 0.0006178373005241156, 0.0006178373005241156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006178373005241156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.60597992
Iteration 2/25 | Loss: 0.00028650
Iteration 3/25 | Loss: 0.00028649
Iteration 4/25 | Loss: 0.00028649
Iteration 5/25 | Loss: 0.00028649
Iteration 6/25 | Loss: 0.00028649
Iteration 7/25 | Loss: 0.00028649
Iteration 8/25 | Loss: 0.00028648
Iteration 9/25 | Loss: 0.00028648
Iteration 10/25 | Loss: 0.00028648
Iteration 11/25 | Loss: 0.00028648
Iteration 12/25 | Loss: 0.00028648
Iteration 13/25 | Loss: 0.00028648
Iteration 14/25 | Loss: 0.00028648
Iteration 15/25 | Loss: 0.00028648
Iteration 16/25 | Loss: 0.00028648
Iteration 17/25 | Loss: 0.00028648
Iteration 18/25 | Loss: 0.00028648
Iteration 19/25 | Loss: 0.00028648
Iteration 20/25 | Loss: 0.00028648
Iteration 21/25 | Loss: 0.00028648
Iteration 22/25 | Loss: 0.00028648
Iteration 23/25 | Loss: 0.00028648
Iteration 24/25 | Loss: 0.00028648
Iteration 25/25 | Loss: 0.00028648

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028648
Iteration 2/1000 | Loss: 0.00002504
Iteration 3/1000 | Loss: 0.00001902
Iteration 4/1000 | Loss: 0.00001785
Iteration 5/1000 | Loss: 0.00001687
Iteration 6/1000 | Loss: 0.00001638
Iteration 7/1000 | Loss: 0.00001605
Iteration 8/1000 | Loss: 0.00001578
Iteration 9/1000 | Loss: 0.00001577
Iteration 10/1000 | Loss: 0.00001577
Iteration 11/1000 | Loss: 0.00001559
Iteration 12/1000 | Loss: 0.00001554
Iteration 13/1000 | Loss: 0.00001551
Iteration 14/1000 | Loss: 0.00001550
Iteration 15/1000 | Loss: 0.00001550
Iteration 16/1000 | Loss: 0.00001549
Iteration 17/1000 | Loss: 0.00001549
Iteration 18/1000 | Loss: 0.00001548
Iteration 19/1000 | Loss: 0.00001548
Iteration 20/1000 | Loss: 0.00001548
Iteration 21/1000 | Loss: 0.00001547
Iteration 22/1000 | Loss: 0.00001546
Iteration 23/1000 | Loss: 0.00001546
Iteration 24/1000 | Loss: 0.00001545
Iteration 25/1000 | Loss: 0.00001545
Iteration 26/1000 | Loss: 0.00001545
Iteration 27/1000 | Loss: 0.00001545
Iteration 28/1000 | Loss: 0.00001544
Iteration 29/1000 | Loss: 0.00001544
Iteration 30/1000 | Loss: 0.00001543
Iteration 31/1000 | Loss: 0.00001541
Iteration 32/1000 | Loss: 0.00001541
Iteration 33/1000 | Loss: 0.00001541
Iteration 34/1000 | Loss: 0.00001540
Iteration 35/1000 | Loss: 0.00001540
Iteration 36/1000 | Loss: 0.00001539
Iteration 37/1000 | Loss: 0.00001536
Iteration 38/1000 | Loss: 0.00001536
Iteration 39/1000 | Loss: 0.00001536
Iteration 40/1000 | Loss: 0.00001535
Iteration 41/1000 | Loss: 0.00001535
Iteration 42/1000 | Loss: 0.00001535
Iteration 43/1000 | Loss: 0.00001534
Iteration 44/1000 | Loss: 0.00001533
Iteration 45/1000 | Loss: 0.00001533
Iteration 46/1000 | Loss: 0.00001533
Iteration 47/1000 | Loss: 0.00001532
Iteration 48/1000 | Loss: 0.00001532
Iteration 49/1000 | Loss: 0.00001532
Iteration 50/1000 | Loss: 0.00001532
Iteration 51/1000 | Loss: 0.00001532
Iteration 52/1000 | Loss: 0.00001531
Iteration 53/1000 | Loss: 0.00001531
Iteration 54/1000 | Loss: 0.00001530
Iteration 55/1000 | Loss: 0.00001530
Iteration 56/1000 | Loss: 0.00001530
Iteration 57/1000 | Loss: 0.00001530
Iteration 58/1000 | Loss: 0.00001530
Iteration 59/1000 | Loss: 0.00001530
Iteration 60/1000 | Loss: 0.00001529
Iteration 61/1000 | Loss: 0.00001529
Iteration 62/1000 | Loss: 0.00001529
Iteration 63/1000 | Loss: 0.00001529
Iteration 64/1000 | Loss: 0.00001529
Iteration 65/1000 | Loss: 0.00001529
Iteration 66/1000 | Loss: 0.00001528
Iteration 67/1000 | Loss: 0.00001528
Iteration 68/1000 | Loss: 0.00001528
Iteration 69/1000 | Loss: 0.00001528
Iteration 70/1000 | Loss: 0.00001528
Iteration 71/1000 | Loss: 0.00001528
Iteration 72/1000 | Loss: 0.00001528
Iteration 73/1000 | Loss: 0.00001527
Iteration 74/1000 | Loss: 0.00001527
Iteration 75/1000 | Loss: 0.00001527
Iteration 76/1000 | Loss: 0.00001527
Iteration 77/1000 | Loss: 0.00001527
Iteration 78/1000 | Loss: 0.00001527
Iteration 79/1000 | Loss: 0.00001527
Iteration 80/1000 | Loss: 0.00001527
Iteration 81/1000 | Loss: 0.00001527
Iteration 82/1000 | Loss: 0.00001527
Iteration 83/1000 | Loss: 0.00001527
Iteration 84/1000 | Loss: 0.00001527
Iteration 85/1000 | Loss: 0.00001527
Iteration 86/1000 | Loss: 0.00001527
Iteration 87/1000 | Loss: 0.00001527
Iteration 88/1000 | Loss: 0.00001527
Iteration 89/1000 | Loss: 0.00001527
Iteration 90/1000 | Loss: 0.00001527
Iteration 91/1000 | Loss: 0.00001527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.526921369077172e-05, 1.526921369077172e-05, 1.526921369077172e-05, 1.526921369077172e-05, 1.526921369077172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.526921369077172e-05

Optimization complete. Final v2v error: 3.309535503387451 mm

Highest mean error: 3.7121667861938477 mm for frame 123

Lowest mean error: 3.0295000076293945 mm for frame 191

Saving results

Total time: 31.300094604492188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00914661
Iteration 2/25 | Loss: 0.00147790
Iteration 3/25 | Loss: 0.00079464
Iteration 4/25 | Loss: 0.00093911
Iteration 5/25 | Loss: 0.00078458
Iteration 6/25 | Loss: 0.00072270
Iteration 7/25 | Loss: 0.00068526
Iteration 8/25 | Loss: 0.00067159
Iteration 9/25 | Loss: 0.00066976
Iteration 10/25 | Loss: 0.00066889
Iteration 11/25 | Loss: 0.00066778
Iteration 12/25 | Loss: 0.00066703
Iteration 13/25 | Loss: 0.00066658
Iteration 14/25 | Loss: 0.00066628
Iteration 15/25 | Loss: 0.00066614
Iteration 16/25 | Loss: 0.00066603
Iteration 17/25 | Loss: 0.00066593
Iteration 18/25 | Loss: 0.00066593
Iteration 19/25 | Loss: 0.00066593
Iteration 20/25 | Loss: 0.00066593
Iteration 21/25 | Loss: 0.00066592
Iteration 22/25 | Loss: 0.00066592
Iteration 23/25 | Loss: 0.00066592
Iteration 24/25 | Loss: 0.00066592
Iteration 25/25 | Loss: 0.00066592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.56226683
Iteration 2/25 | Loss: 0.00029697
Iteration 3/25 | Loss: 0.00029697
Iteration 4/25 | Loss: 0.00029697
Iteration 5/25 | Loss: 0.00029697
Iteration 6/25 | Loss: 0.00029697
Iteration 7/25 | Loss: 0.00029697
Iteration 8/25 | Loss: 0.00029697
Iteration 9/25 | Loss: 0.00029697
Iteration 10/25 | Loss: 0.00029697
Iteration 11/25 | Loss: 0.00029697
Iteration 12/25 | Loss: 0.00029697
Iteration 13/25 | Loss: 0.00029697
Iteration 14/25 | Loss: 0.00029697
Iteration 15/25 | Loss: 0.00029697
Iteration 16/25 | Loss: 0.00029697
Iteration 17/25 | Loss: 0.00029697
Iteration 18/25 | Loss: 0.00029697
Iteration 19/25 | Loss: 0.00029697
Iteration 20/25 | Loss: 0.00029697
Iteration 21/25 | Loss: 0.00029697
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0002969678898807615, 0.0002969678898807615, 0.0002969678898807615, 0.0002969678898807615, 0.0002969678898807615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002969678898807615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029697
Iteration 2/1000 | Loss: 0.00002446
Iteration 3/1000 | Loss: 0.00001942
Iteration 4/1000 | Loss: 0.00001824
Iteration 5/1000 | Loss: 0.00001753
Iteration 6/1000 | Loss: 0.00001726
Iteration 7/1000 | Loss: 0.00001694
Iteration 8/1000 | Loss: 0.00001668
Iteration 9/1000 | Loss: 0.00001651
Iteration 10/1000 | Loss: 0.00001644
Iteration 11/1000 | Loss: 0.00001638
Iteration 12/1000 | Loss: 0.00001632
Iteration 13/1000 | Loss: 0.00001631
Iteration 14/1000 | Loss: 0.00001630
Iteration 15/1000 | Loss: 0.00001630
Iteration 16/1000 | Loss: 0.00001629
Iteration 17/1000 | Loss: 0.00001629
Iteration 18/1000 | Loss: 0.00001627
Iteration 19/1000 | Loss: 0.00001625
Iteration 20/1000 | Loss: 0.00001621
Iteration 21/1000 | Loss: 0.00001618
Iteration 22/1000 | Loss: 0.00001617
Iteration 23/1000 | Loss: 0.00001617
Iteration 24/1000 | Loss: 0.00001617
Iteration 25/1000 | Loss: 0.00001617
Iteration 26/1000 | Loss: 0.00001615
Iteration 27/1000 | Loss: 0.00001615
Iteration 28/1000 | Loss: 0.00001614
Iteration 29/1000 | Loss: 0.00001614
Iteration 30/1000 | Loss: 0.00001614
Iteration 31/1000 | Loss: 0.00001613
Iteration 32/1000 | Loss: 0.00001613
Iteration 33/1000 | Loss: 0.00001613
Iteration 34/1000 | Loss: 0.00001612
Iteration 35/1000 | Loss: 0.00001612
Iteration 36/1000 | Loss: 0.00001612
Iteration 37/1000 | Loss: 0.00001612
Iteration 38/1000 | Loss: 0.00001611
Iteration 39/1000 | Loss: 0.00001611
Iteration 40/1000 | Loss: 0.00001610
Iteration 41/1000 | Loss: 0.00001610
Iteration 42/1000 | Loss: 0.00001609
Iteration 43/1000 | Loss: 0.00001609
Iteration 44/1000 | Loss: 0.00001608
Iteration 45/1000 | Loss: 0.00001607
Iteration 46/1000 | Loss: 0.00001606
Iteration 47/1000 | Loss: 0.00001606
Iteration 48/1000 | Loss: 0.00001605
Iteration 49/1000 | Loss: 0.00001605
Iteration 50/1000 | Loss: 0.00001604
Iteration 51/1000 | Loss: 0.00001604
Iteration 52/1000 | Loss: 0.00001603
Iteration 53/1000 | Loss: 0.00001603
Iteration 54/1000 | Loss: 0.00001602
Iteration 55/1000 | Loss: 0.00001602
Iteration 56/1000 | Loss: 0.00001601
Iteration 57/1000 | Loss: 0.00001601
Iteration 58/1000 | Loss: 0.00001600
Iteration 59/1000 | Loss: 0.00001600
Iteration 60/1000 | Loss: 0.00001600
Iteration 61/1000 | Loss: 0.00001599
Iteration 62/1000 | Loss: 0.00001599
Iteration 63/1000 | Loss: 0.00001598
Iteration 64/1000 | Loss: 0.00001598
Iteration 65/1000 | Loss: 0.00001598
Iteration 66/1000 | Loss: 0.00001598
Iteration 67/1000 | Loss: 0.00001598
Iteration 68/1000 | Loss: 0.00001598
Iteration 69/1000 | Loss: 0.00001598
Iteration 70/1000 | Loss: 0.00001597
Iteration 71/1000 | Loss: 0.00001597
Iteration 72/1000 | Loss: 0.00001597
Iteration 73/1000 | Loss: 0.00001597
Iteration 74/1000 | Loss: 0.00001597
Iteration 75/1000 | Loss: 0.00001596
Iteration 76/1000 | Loss: 0.00001596
Iteration 77/1000 | Loss: 0.00001596
Iteration 78/1000 | Loss: 0.00001596
Iteration 79/1000 | Loss: 0.00001595
Iteration 80/1000 | Loss: 0.00001595
Iteration 81/1000 | Loss: 0.00001595
Iteration 82/1000 | Loss: 0.00001595
Iteration 83/1000 | Loss: 0.00001595
Iteration 84/1000 | Loss: 0.00001595
Iteration 85/1000 | Loss: 0.00001595
Iteration 86/1000 | Loss: 0.00001595
Iteration 87/1000 | Loss: 0.00001595
Iteration 88/1000 | Loss: 0.00001595
Iteration 89/1000 | Loss: 0.00001594
Iteration 90/1000 | Loss: 0.00001594
Iteration 91/1000 | Loss: 0.00001594
Iteration 92/1000 | Loss: 0.00001594
Iteration 93/1000 | Loss: 0.00001594
Iteration 94/1000 | Loss: 0.00001594
Iteration 95/1000 | Loss: 0.00001594
Iteration 96/1000 | Loss: 0.00001594
Iteration 97/1000 | Loss: 0.00001594
Iteration 98/1000 | Loss: 0.00001593
Iteration 99/1000 | Loss: 0.00001593
Iteration 100/1000 | Loss: 0.00001593
Iteration 101/1000 | Loss: 0.00001593
Iteration 102/1000 | Loss: 0.00001593
Iteration 103/1000 | Loss: 0.00001593
Iteration 104/1000 | Loss: 0.00001593
Iteration 105/1000 | Loss: 0.00001593
Iteration 106/1000 | Loss: 0.00001593
Iteration 107/1000 | Loss: 0.00001593
Iteration 108/1000 | Loss: 0.00001593
Iteration 109/1000 | Loss: 0.00001593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.5934672774164937e-05, 1.5934672774164937e-05, 1.5934672774164937e-05, 1.5934672774164937e-05, 1.5934672774164937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5934672774164937e-05

Optimization complete. Final v2v error: 3.319060802459717 mm

Highest mean error: 3.731106996536255 mm for frame 200

Lowest mean error: 2.8006300926208496 mm for frame 0

Saving results

Total time: 59.585007190704346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789671
Iteration 2/25 | Loss: 0.00156928
Iteration 3/25 | Loss: 0.00093298
Iteration 4/25 | Loss: 0.00082555
Iteration 5/25 | Loss: 0.00079804
Iteration 6/25 | Loss: 0.00077261
Iteration 7/25 | Loss: 0.00074313
Iteration 8/25 | Loss: 0.00071378
Iteration 9/25 | Loss: 0.00070437
Iteration 10/25 | Loss: 0.00070096
Iteration 11/25 | Loss: 0.00069494
Iteration 12/25 | Loss: 0.00069013
Iteration 13/25 | Loss: 0.00069123
Iteration 14/25 | Loss: 0.00068886
Iteration 15/25 | Loss: 0.00068759
Iteration 16/25 | Loss: 0.00068740
Iteration 17/25 | Loss: 0.00068740
Iteration 18/25 | Loss: 0.00068740
Iteration 19/25 | Loss: 0.00068739
Iteration 20/25 | Loss: 0.00068739
Iteration 21/25 | Loss: 0.00068739
Iteration 22/25 | Loss: 0.00068739
Iteration 23/25 | Loss: 0.00068739
Iteration 24/25 | Loss: 0.00068739
Iteration 25/25 | Loss: 0.00068739

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98021531
Iteration 2/25 | Loss: 0.00036658
Iteration 3/25 | Loss: 0.00033577
Iteration 4/25 | Loss: 0.00033577
Iteration 5/25 | Loss: 0.00033577
Iteration 6/25 | Loss: 0.00033576
Iteration 7/25 | Loss: 0.00033576
Iteration 8/25 | Loss: 0.00033576
Iteration 9/25 | Loss: 0.00033576
Iteration 10/25 | Loss: 0.00033576
Iteration 11/25 | Loss: 0.00033576
Iteration 12/25 | Loss: 0.00033576
Iteration 13/25 | Loss: 0.00033576
Iteration 14/25 | Loss: 0.00033576
Iteration 15/25 | Loss: 0.00033576
Iteration 16/25 | Loss: 0.00033576
Iteration 17/25 | Loss: 0.00033576
Iteration 18/25 | Loss: 0.00033576
Iteration 19/25 | Loss: 0.00033576
Iteration 20/25 | Loss: 0.00033576
Iteration 21/25 | Loss: 0.00033576
Iteration 22/25 | Loss: 0.00033576
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00033576288842596114, 0.00033576288842596114, 0.00033576288842596114, 0.00033576288842596114, 0.00033576288842596114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033576288842596114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033576
Iteration 2/1000 | Loss: 0.00005799
Iteration 3/1000 | Loss: 0.00005767
Iteration 4/1000 | Loss: 0.00002064
Iteration 5/1000 | Loss: 0.00001844
Iteration 6/1000 | Loss: 0.00003508
Iteration 7/1000 | Loss: 0.00003376
Iteration 8/1000 | Loss: 0.00001669
Iteration 9/1000 | Loss: 0.00003038
Iteration 10/1000 | Loss: 0.00002115
Iteration 11/1000 | Loss: 0.00001708
Iteration 12/1000 | Loss: 0.00002144
Iteration 13/1000 | Loss: 0.00001658
Iteration 14/1000 | Loss: 0.00001591
Iteration 15/1000 | Loss: 0.00002509
Iteration 16/1000 | Loss: 0.00001749
Iteration 17/1000 | Loss: 0.00001795
Iteration 18/1000 | Loss: 0.00001680
Iteration 19/1000 | Loss: 0.00001578
Iteration 20/1000 | Loss: 0.00001577
Iteration 21/1000 | Loss: 0.00001577
Iteration 22/1000 | Loss: 0.00001550
Iteration 23/1000 | Loss: 0.00001549
Iteration 24/1000 | Loss: 0.00001548
Iteration 25/1000 | Loss: 0.00001548
Iteration 26/1000 | Loss: 0.00001548
Iteration 27/1000 | Loss: 0.00001548
Iteration 28/1000 | Loss: 0.00001548
Iteration 29/1000 | Loss: 0.00001547
Iteration 30/1000 | Loss: 0.00001547
Iteration 31/1000 | Loss: 0.00001546
Iteration 32/1000 | Loss: 0.00001537
Iteration 33/1000 | Loss: 0.00001535
Iteration 34/1000 | Loss: 0.00001535
Iteration 35/1000 | Loss: 0.00001530
Iteration 36/1000 | Loss: 0.00001530
Iteration 37/1000 | Loss: 0.00001529
Iteration 38/1000 | Loss: 0.00001529
Iteration 39/1000 | Loss: 0.00001528
Iteration 40/1000 | Loss: 0.00001528
Iteration 41/1000 | Loss: 0.00001526
Iteration 42/1000 | Loss: 0.00001526
Iteration 43/1000 | Loss: 0.00001526
Iteration 44/1000 | Loss: 0.00001525
Iteration 45/1000 | Loss: 0.00001525
Iteration 46/1000 | Loss: 0.00001525
Iteration 47/1000 | Loss: 0.00001525
Iteration 48/1000 | Loss: 0.00001525
Iteration 49/1000 | Loss: 0.00001524
Iteration 50/1000 | Loss: 0.00001524
Iteration 51/1000 | Loss: 0.00001524
Iteration 52/1000 | Loss: 0.00001524
Iteration 53/1000 | Loss: 0.00001523
Iteration 54/1000 | Loss: 0.00001523
Iteration 55/1000 | Loss: 0.00001523
Iteration 56/1000 | Loss: 0.00001523
Iteration 57/1000 | Loss: 0.00001522
Iteration 58/1000 | Loss: 0.00001522
Iteration 59/1000 | Loss: 0.00001522
Iteration 60/1000 | Loss: 0.00001522
Iteration 61/1000 | Loss: 0.00001522
Iteration 62/1000 | Loss: 0.00001522
Iteration 63/1000 | Loss: 0.00001522
Iteration 64/1000 | Loss: 0.00001522
Iteration 65/1000 | Loss: 0.00001522
Iteration 66/1000 | Loss: 0.00001522
Iteration 67/1000 | Loss: 0.00001521
Iteration 68/1000 | Loss: 0.00001521
Iteration 69/1000 | Loss: 0.00001521
Iteration 70/1000 | Loss: 0.00001520
Iteration 71/1000 | Loss: 0.00001520
Iteration 72/1000 | Loss: 0.00001519
Iteration 73/1000 | Loss: 0.00001519
Iteration 74/1000 | Loss: 0.00001519
Iteration 75/1000 | Loss: 0.00001519
Iteration 76/1000 | Loss: 0.00001519
Iteration 77/1000 | Loss: 0.00001519
Iteration 78/1000 | Loss: 0.00001519
Iteration 79/1000 | Loss: 0.00001519
Iteration 80/1000 | Loss: 0.00001518
Iteration 81/1000 | Loss: 0.00001518
Iteration 82/1000 | Loss: 0.00001518
Iteration 83/1000 | Loss: 0.00001518
Iteration 84/1000 | Loss: 0.00001518
Iteration 85/1000 | Loss: 0.00001518
Iteration 86/1000 | Loss: 0.00001517
Iteration 87/1000 | Loss: 0.00001517
Iteration 88/1000 | Loss: 0.00001517
Iteration 89/1000 | Loss: 0.00001516
Iteration 90/1000 | Loss: 0.00001516
Iteration 91/1000 | Loss: 0.00001515
Iteration 92/1000 | Loss: 0.00001515
Iteration 93/1000 | Loss: 0.00001515
Iteration 94/1000 | Loss: 0.00001515
Iteration 95/1000 | Loss: 0.00001512
Iteration 96/1000 | Loss: 0.00001512
Iteration 97/1000 | Loss: 0.00001512
Iteration 98/1000 | Loss: 0.00001511
Iteration 99/1000 | Loss: 0.00001511
Iteration 100/1000 | Loss: 0.00001511
Iteration 101/1000 | Loss: 0.00001507
Iteration 102/1000 | Loss: 0.00001507
Iteration 103/1000 | Loss: 0.00001507
Iteration 104/1000 | Loss: 0.00001506
Iteration 105/1000 | Loss: 0.00001506
Iteration 106/1000 | Loss: 0.00001506
Iteration 107/1000 | Loss: 0.00001506
Iteration 108/1000 | Loss: 0.00001506
Iteration 109/1000 | Loss: 0.00001506
Iteration 110/1000 | Loss: 0.00001506
Iteration 111/1000 | Loss: 0.00001506
Iteration 112/1000 | Loss: 0.00001505
Iteration 113/1000 | Loss: 0.00001505
Iteration 114/1000 | Loss: 0.00001505
Iteration 115/1000 | Loss: 0.00001504
Iteration 116/1000 | Loss: 0.00001504
Iteration 117/1000 | Loss: 0.00001504
Iteration 118/1000 | Loss: 0.00001504
Iteration 119/1000 | Loss: 0.00001504
Iteration 120/1000 | Loss: 0.00001503
Iteration 121/1000 | Loss: 0.00001503
Iteration 122/1000 | Loss: 0.00001503
Iteration 123/1000 | Loss: 0.00001503
Iteration 124/1000 | Loss: 0.00001503
Iteration 125/1000 | Loss: 0.00001503
Iteration 126/1000 | Loss: 0.00001503
Iteration 127/1000 | Loss: 0.00001503
Iteration 128/1000 | Loss: 0.00001503
Iteration 129/1000 | Loss: 0.00001503
Iteration 130/1000 | Loss: 0.00001503
Iteration 131/1000 | Loss: 0.00001503
Iteration 132/1000 | Loss: 0.00001503
Iteration 133/1000 | Loss: 0.00001503
Iteration 134/1000 | Loss: 0.00001502
Iteration 135/1000 | Loss: 0.00001502
Iteration 136/1000 | Loss: 0.00001502
Iteration 137/1000 | Loss: 0.00001502
Iteration 138/1000 | Loss: 0.00001502
Iteration 139/1000 | Loss: 0.00001502
Iteration 140/1000 | Loss: 0.00001502
Iteration 141/1000 | Loss: 0.00001502
Iteration 142/1000 | Loss: 0.00001502
Iteration 143/1000 | Loss: 0.00001502
Iteration 144/1000 | Loss: 0.00001502
Iteration 145/1000 | Loss: 0.00001502
Iteration 146/1000 | Loss: 0.00001502
Iteration 147/1000 | Loss: 0.00001502
Iteration 148/1000 | Loss: 0.00001502
Iteration 149/1000 | Loss: 0.00001502
Iteration 150/1000 | Loss: 0.00001502
Iteration 151/1000 | Loss: 0.00001502
Iteration 152/1000 | Loss: 0.00001502
Iteration 153/1000 | Loss: 0.00001502
Iteration 154/1000 | Loss: 0.00001502
Iteration 155/1000 | Loss: 0.00001502
Iteration 156/1000 | Loss: 0.00001502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.5017075384093914e-05, 1.5017075384093914e-05, 1.5017075384093914e-05, 1.5017075384093914e-05, 1.5017075384093914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5017075384093914e-05

Optimization complete. Final v2v error: 3.2202718257904053 mm

Highest mean error: 9.016081809997559 mm for frame 133

Lowest mean error: 2.918231964111328 mm for frame 115

Saving results

Total time: 66.08591532707214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01070417
Iteration 2/25 | Loss: 0.00209713
Iteration 3/25 | Loss: 0.00105041
Iteration 4/25 | Loss: 0.00090365
Iteration 5/25 | Loss: 0.00084179
Iteration 6/25 | Loss: 0.00082448
Iteration 7/25 | Loss: 0.00075894
Iteration 8/25 | Loss: 0.00073520
Iteration 9/25 | Loss: 0.00072973
Iteration 10/25 | Loss: 0.00070485
Iteration 11/25 | Loss: 0.00069559
Iteration 12/25 | Loss: 0.00069954
Iteration 13/25 | Loss: 0.00068879
Iteration 14/25 | Loss: 0.00068045
Iteration 15/25 | Loss: 0.00067984
Iteration 16/25 | Loss: 0.00067762
Iteration 17/25 | Loss: 0.00067618
Iteration 18/25 | Loss: 0.00067887
Iteration 19/25 | Loss: 0.00067709
Iteration 20/25 | Loss: 0.00067677
Iteration 21/25 | Loss: 0.00067676
Iteration 22/25 | Loss: 0.00067737
Iteration 23/25 | Loss: 0.00067900
Iteration 24/25 | Loss: 0.00067956
Iteration 25/25 | Loss: 0.00067696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52159500
Iteration 2/25 | Loss: 0.00041297
Iteration 3/25 | Loss: 0.00040818
Iteration 4/25 | Loss: 0.00040818
Iteration 5/25 | Loss: 0.00040818
Iteration 6/25 | Loss: 0.00040818
Iteration 7/25 | Loss: 0.00040817
Iteration 8/25 | Loss: 0.00040817
Iteration 9/25 | Loss: 0.00040817
Iteration 10/25 | Loss: 0.00040817
Iteration 11/25 | Loss: 0.00040817
Iteration 12/25 | Loss: 0.00040817
Iteration 13/25 | Loss: 0.00040817
Iteration 14/25 | Loss: 0.00040817
Iteration 15/25 | Loss: 0.00040817
Iteration 16/25 | Loss: 0.00040817
Iteration 17/25 | Loss: 0.00040817
Iteration 18/25 | Loss: 0.00040817
Iteration 19/25 | Loss: 0.00040817
Iteration 20/25 | Loss: 0.00040817
Iteration 21/25 | Loss: 0.00040817
Iteration 22/25 | Loss: 0.00040817
Iteration 23/25 | Loss: 0.00040817
Iteration 24/25 | Loss: 0.00040817
Iteration 25/25 | Loss: 0.00040817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040817
Iteration 2/1000 | Loss: 0.00003150
Iteration 3/1000 | Loss: 0.00002820
Iteration 4/1000 | Loss: 0.00003307
Iteration 5/1000 | Loss: 0.00001634
Iteration 6/1000 | Loss: 0.00001312
Iteration 7/1000 | Loss: 0.00007797
Iteration 8/1000 | Loss: 0.00001767
Iteration 9/1000 | Loss: 0.00007407
Iteration 10/1000 | Loss: 0.00001388
Iteration 11/1000 | Loss: 0.00005093
Iteration 12/1000 | Loss: 0.00003177
Iteration 13/1000 | Loss: 0.00001509
Iteration 14/1000 | Loss: 0.00001964
Iteration 15/1000 | Loss: 0.00004831
Iteration 16/1000 | Loss: 0.00001521
Iteration 17/1000 | Loss: 0.00001231
Iteration 18/1000 | Loss: 0.00001398
Iteration 19/1000 | Loss: 0.00002419
Iteration 20/1000 | Loss: 0.00001324
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001198
Iteration 23/1000 | Loss: 0.00001198
Iteration 24/1000 | Loss: 0.00001198
Iteration 25/1000 | Loss: 0.00001198
Iteration 26/1000 | Loss: 0.00001197
Iteration 27/1000 | Loss: 0.00001197
Iteration 28/1000 | Loss: 0.00001197
Iteration 29/1000 | Loss: 0.00001197
Iteration 30/1000 | Loss: 0.00001197
Iteration 31/1000 | Loss: 0.00001359
Iteration 32/1000 | Loss: 0.00001946
Iteration 33/1000 | Loss: 0.00001193
Iteration 34/1000 | Loss: 0.00001193
Iteration 35/1000 | Loss: 0.00001193
Iteration 36/1000 | Loss: 0.00001193
Iteration 37/1000 | Loss: 0.00001193
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001193
Iteration 40/1000 | Loss: 0.00001193
Iteration 41/1000 | Loss: 0.00001193
Iteration 42/1000 | Loss: 0.00001193
Iteration 43/1000 | Loss: 0.00001192
Iteration 44/1000 | Loss: 0.00001192
Iteration 45/1000 | Loss: 0.00001192
Iteration 46/1000 | Loss: 0.00001191
Iteration 47/1000 | Loss: 0.00001191
Iteration 48/1000 | Loss: 0.00001190
Iteration 49/1000 | Loss: 0.00001190
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001188
Iteration 52/1000 | Loss: 0.00001809
Iteration 53/1000 | Loss: 0.00001534
Iteration 54/1000 | Loss: 0.00001959
Iteration 55/1000 | Loss: 0.00001203
Iteration 56/1000 | Loss: 0.00001184
Iteration 57/1000 | Loss: 0.00001468
Iteration 58/1000 | Loss: 0.00001438
Iteration 59/1000 | Loss: 0.00002216
Iteration 60/1000 | Loss: 0.00001218
Iteration 61/1000 | Loss: 0.00001180
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001185
Iteration 65/1000 | Loss: 0.00001179
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001178
Iteration 68/1000 | Loss: 0.00001710
Iteration 69/1000 | Loss: 0.00001710
Iteration 70/1000 | Loss: 0.00002211
Iteration 71/1000 | Loss: 0.00009369
Iteration 72/1000 | Loss: 0.00001825
Iteration 73/1000 | Loss: 0.00001175
Iteration 74/1000 | Loss: 0.00001172
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001572
Iteration 77/1000 | Loss: 0.00001297
Iteration 78/1000 | Loss: 0.00001487
Iteration 79/1000 | Loss: 0.00004576
Iteration 80/1000 | Loss: 0.00001343
Iteration 81/1000 | Loss: 0.00001170
Iteration 82/1000 | Loss: 0.00001170
Iteration 83/1000 | Loss: 0.00001170
Iteration 84/1000 | Loss: 0.00001539
Iteration 85/1000 | Loss: 0.00001175
Iteration 86/1000 | Loss: 0.00001169
Iteration 87/1000 | Loss: 0.00001169
Iteration 88/1000 | Loss: 0.00001169
Iteration 89/1000 | Loss: 0.00001169
Iteration 90/1000 | Loss: 0.00001169
Iteration 91/1000 | Loss: 0.00001169
Iteration 92/1000 | Loss: 0.00001169
Iteration 93/1000 | Loss: 0.00001169
Iteration 94/1000 | Loss: 0.00001169
Iteration 95/1000 | Loss: 0.00001169
Iteration 96/1000 | Loss: 0.00001168
Iteration 97/1000 | Loss: 0.00001168
Iteration 98/1000 | Loss: 0.00001168
Iteration 99/1000 | Loss: 0.00001168
Iteration 100/1000 | Loss: 0.00001168
Iteration 101/1000 | Loss: 0.00001168
Iteration 102/1000 | Loss: 0.00001168
Iteration 103/1000 | Loss: 0.00001168
Iteration 104/1000 | Loss: 0.00001168
Iteration 105/1000 | Loss: 0.00001168
Iteration 106/1000 | Loss: 0.00001168
Iteration 107/1000 | Loss: 0.00001168
Iteration 108/1000 | Loss: 0.00001168
Iteration 109/1000 | Loss: 0.00001168
Iteration 110/1000 | Loss: 0.00001168
Iteration 111/1000 | Loss: 0.00001168
Iteration 112/1000 | Loss: 0.00001168
Iteration 113/1000 | Loss: 0.00001168
Iteration 114/1000 | Loss: 0.00001168
Iteration 115/1000 | Loss: 0.00001168
Iteration 116/1000 | Loss: 0.00001168
Iteration 117/1000 | Loss: 0.00001168
Iteration 118/1000 | Loss: 0.00001168
Iteration 119/1000 | Loss: 0.00001168
Iteration 120/1000 | Loss: 0.00001168
Iteration 121/1000 | Loss: 0.00001168
Iteration 122/1000 | Loss: 0.00001168
Iteration 123/1000 | Loss: 0.00001168
Iteration 124/1000 | Loss: 0.00001168
Iteration 125/1000 | Loss: 0.00001168
Iteration 126/1000 | Loss: 0.00001168
Iteration 127/1000 | Loss: 0.00001168
Iteration 128/1000 | Loss: 0.00001168
Iteration 129/1000 | Loss: 0.00001168
Iteration 130/1000 | Loss: 0.00001168
Iteration 131/1000 | Loss: 0.00001168
Iteration 132/1000 | Loss: 0.00001168
Iteration 133/1000 | Loss: 0.00001168
Iteration 134/1000 | Loss: 0.00001168
Iteration 135/1000 | Loss: 0.00001168
Iteration 136/1000 | Loss: 0.00001168
Iteration 137/1000 | Loss: 0.00001168
Iteration 138/1000 | Loss: 0.00001168
Iteration 139/1000 | Loss: 0.00001168
Iteration 140/1000 | Loss: 0.00001168
Iteration 141/1000 | Loss: 0.00001168
Iteration 142/1000 | Loss: 0.00001168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.168427934317151e-05, 1.168427934317151e-05, 1.168427934317151e-05, 1.168427934317151e-05, 1.168427934317151e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.168427934317151e-05

Optimization complete. Final v2v error: 2.696270704269409 mm

Highest mean error: 9.545872688293457 mm for frame 78

Lowest mean error: 2.39105224609375 mm for frame 94

Saving results

Total time: 96.31824684143066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055387
Iteration 2/25 | Loss: 0.01055386
Iteration 3/25 | Loss: 0.01055386
Iteration 4/25 | Loss: 0.01055386
Iteration 5/25 | Loss: 0.01055386
Iteration 6/25 | Loss: 0.01055386
Iteration 7/25 | Loss: 0.01055386
Iteration 8/25 | Loss: 0.01055386
Iteration 9/25 | Loss: 0.01055386
Iteration 10/25 | Loss: 0.01055386
Iteration 11/25 | Loss: 0.01055386
Iteration 12/25 | Loss: 0.01055385
Iteration 13/25 | Loss: 0.01055385
Iteration 14/25 | Loss: 0.01055385
Iteration 15/25 | Loss: 0.01055385
Iteration 16/25 | Loss: 0.01055385
Iteration 17/25 | Loss: 0.01055385
Iteration 18/25 | Loss: 0.01055385
Iteration 19/25 | Loss: 0.01055385
Iteration 20/25 | Loss: 0.01055385
Iteration 21/25 | Loss: 0.01055385
Iteration 22/25 | Loss: 0.01055385
Iteration 23/25 | Loss: 0.01055385
Iteration 24/25 | Loss: 0.01055384
Iteration 25/25 | Loss: 0.01055384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99684238
Iteration 2/25 | Loss: 0.08839998
Iteration 3/25 | Loss: 0.08830243
Iteration 4/25 | Loss: 0.08798715
Iteration 5/25 | Loss: 0.08798662
Iteration 6/25 | Loss: 0.08798662
Iteration 7/25 | Loss: 0.08798662
Iteration 8/25 | Loss: 0.08798662
Iteration 9/25 | Loss: 0.08798662
Iteration 10/25 | Loss: 0.08798662
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.08798661828041077, 0.08798661828041077, 0.08798661828041077, 0.08798661828041077, 0.08798661828041077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08798661828041077

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08798662
Iteration 2/1000 | Loss: 0.00537919
Iteration 3/1000 | Loss: 0.00255776
Iteration 4/1000 | Loss: 0.00167264
Iteration 5/1000 | Loss: 0.00390609
Iteration 6/1000 | Loss: 0.00897529
Iteration 7/1000 | Loss: 0.00039999
Iteration 8/1000 | Loss: 0.00031896
Iteration 9/1000 | Loss: 0.00124714
Iteration 10/1000 | Loss: 0.00144348
Iteration 11/1000 | Loss: 0.00011282
Iteration 12/1000 | Loss: 0.00129910
Iteration 13/1000 | Loss: 0.00012776
Iteration 14/1000 | Loss: 0.00007644
Iteration 15/1000 | Loss: 0.00019716
Iteration 16/1000 | Loss: 0.00116638
Iteration 17/1000 | Loss: 0.00079152
Iteration 18/1000 | Loss: 0.00040052
Iteration 19/1000 | Loss: 0.00068273
Iteration 20/1000 | Loss: 0.00049892
Iteration 21/1000 | Loss: 0.00004626
Iteration 22/1000 | Loss: 0.00054968
Iteration 23/1000 | Loss: 0.00006016
Iteration 24/1000 | Loss: 0.00003739
Iteration 25/1000 | Loss: 0.00003402
Iteration 26/1000 | Loss: 0.00007916
Iteration 27/1000 | Loss: 0.00049135
Iteration 28/1000 | Loss: 0.00003051
Iteration 29/1000 | Loss: 0.00005870
Iteration 30/1000 | Loss: 0.00002765
Iteration 31/1000 | Loss: 0.00010684
Iteration 32/1000 | Loss: 0.00002692
Iteration 33/1000 | Loss: 0.00002477
Iteration 34/1000 | Loss: 0.00016901
Iteration 35/1000 | Loss: 0.00027294
Iteration 36/1000 | Loss: 0.00003037
Iteration 37/1000 | Loss: 0.00002600
Iteration 38/1000 | Loss: 0.00002180
Iteration 39/1000 | Loss: 0.00016951
Iteration 40/1000 | Loss: 0.00002459
Iteration 41/1000 | Loss: 0.00002232
Iteration 42/1000 | Loss: 0.00024397
Iteration 43/1000 | Loss: 0.00002050
Iteration 44/1000 | Loss: 0.00001995
Iteration 45/1000 | Loss: 0.00010070
Iteration 46/1000 | Loss: 0.00011481
Iteration 47/1000 | Loss: 0.00001973
Iteration 48/1000 | Loss: 0.00001900
Iteration 49/1000 | Loss: 0.00001880
Iteration 50/1000 | Loss: 0.00019471
Iteration 51/1000 | Loss: 0.00048555
Iteration 52/1000 | Loss: 0.00002433
Iteration 53/1000 | Loss: 0.00002206
Iteration 54/1000 | Loss: 0.00019304
Iteration 55/1000 | Loss: 0.00071086
Iteration 56/1000 | Loss: 0.00012560
Iteration 57/1000 | Loss: 0.00012621
Iteration 58/1000 | Loss: 0.00023461
Iteration 59/1000 | Loss: 0.00001896
Iteration 60/1000 | Loss: 0.00006633
Iteration 61/1000 | Loss: 0.00001825
Iteration 62/1000 | Loss: 0.00001806
Iteration 63/1000 | Loss: 0.00003508
Iteration 64/1000 | Loss: 0.00016522
Iteration 65/1000 | Loss: 0.00053177
Iteration 66/1000 | Loss: 0.00004515
Iteration 67/1000 | Loss: 0.00004657
Iteration 68/1000 | Loss: 0.00002062
Iteration 69/1000 | Loss: 0.00002326
Iteration 70/1000 | Loss: 0.00001939
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00012147
Iteration 73/1000 | Loss: 0.00016615
Iteration 74/1000 | Loss: 0.00039894
Iteration 75/1000 | Loss: 0.00022706
Iteration 76/1000 | Loss: 0.00009995
Iteration 77/1000 | Loss: 0.00003843
Iteration 78/1000 | Loss: 0.00019829
Iteration 79/1000 | Loss: 0.00002686
Iteration 80/1000 | Loss: 0.00003527
Iteration 81/1000 | Loss: 0.00004085
Iteration 82/1000 | Loss: 0.00005459
Iteration 83/1000 | Loss: 0.00005521
Iteration 84/1000 | Loss: 0.00013934
Iteration 85/1000 | Loss: 0.00023937
Iteration 86/1000 | Loss: 0.00009523
Iteration 87/1000 | Loss: 0.00009388
Iteration 88/1000 | Loss: 0.00004742
Iteration 89/1000 | Loss: 0.00003626
Iteration 90/1000 | Loss: 0.00005518
Iteration 91/1000 | Loss: 0.00005323
Iteration 92/1000 | Loss: 0.00002438
Iteration 93/1000 | Loss: 0.00002198
Iteration 94/1000 | Loss: 0.00002946
Iteration 95/1000 | Loss: 0.00002657
Iteration 96/1000 | Loss: 0.00002314
Iteration 97/1000 | Loss: 0.00003562
Iteration 98/1000 | Loss: 0.00002394
Iteration 99/1000 | Loss: 0.00002593
Iteration 100/1000 | Loss: 0.00003738
Iteration 101/1000 | Loss: 0.00017561
Iteration 102/1000 | Loss: 0.00002100
Iteration 103/1000 | Loss: 0.00003578
Iteration 104/1000 | Loss: 0.00004832
Iteration 105/1000 | Loss: 0.00002690
Iteration 106/1000 | Loss: 0.00002021
Iteration 107/1000 | Loss: 0.00003344
Iteration 108/1000 | Loss: 0.00002855
Iteration 109/1000 | Loss: 0.00002375
Iteration 110/1000 | Loss: 0.00002618
Iteration 111/1000 | Loss: 0.00002331
Iteration 112/1000 | Loss: 0.00002993
Iteration 113/1000 | Loss: 0.00003036
Iteration 114/1000 | Loss: 0.00002868
Iteration 115/1000 | Loss: 0.00005120
Iteration 116/1000 | Loss: 0.00002950
Iteration 117/1000 | Loss: 0.00002456
Iteration 118/1000 | Loss: 0.00004478
Iteration 119/1000 | Loss: 0.00002436
Iteration 120/1000 | Loss: 0.00003271
Iteration 121/1000 | Loss: 0.00017910
Iteration 122/1000 | Loss: 0.00006193
Iteration 123/1000 | Loss: 0.00002676
Iteration 124/1000 | Loss: 0.00003334
Iteration 125/1000 | Loss: 0.00015357
Iteration 126/1000 | Loss: 0.00007994
Iteration 127/1000 | Loss: 0.00004699
Iteration 128/1000 | Loss: 0.00002570
Iteration 129/1000 | Loss: 0.00002343
Iteration 130/1000 | Loss: 0.00002335
Iteration 131/1000 | Loss: 0.00001998
Iteration 132/1000 | Loss: 0.00003880
Iteration 133/1000 | Loss: 0.00003303
Iteration 134/1000 | Loss: 0.00004601
Iteration 135/1000 | Loss: 0.00003102
Iteration 136/1000 | Loss: 0.00004817
Iteration 137/1000 | Loss: 0.00012375
Iteration 138/1000 | Loss: 0.00009456
Iteration 139/1000 | Loss: 0.00007058
Iteration 140/1000 | Loss: 0.00003528
Iteration 141/1000 | Loss: 0.00004193
Iteration 142/1000 | Loss: 0.00002747
Iteration 143/1000 | Loss: 0.00002822
Iteration 144/1000 | Loss: 0.00003275
Iteration 145/1000 | Loss: 0.00006790
Iteration 146/1000 | Loss: 0.00002416
Iteration 147/1000 | Loss: 0.00002747
Iteration 148/1000 | Loss: 0.00005519
Iteration 149/1000 | Loss: 0.00002779
Iteration 150/1000 | Loss: 0.00001862
Iteration 151/1000 | Loss: 0.00002840
Iteration 152/1000 | Loss: 0.00021733
Iteration 153/1000 | Loss: 0.00004279
Iteration 154/1000 | Loss: 0.00002599
Iteration 155/1000 | Loss: 0.00002793
Iteration 156/1000 | Loss: 0.00002436
Iteration 157/1000 | Loss: 0.00003225
Iteration 158/1000 | Loss: 0.00002244
Iteration 159/1000 | Loss: 0.00002457
Iteration 160/1000 | Loss: 0.00002225
Iteration 161/1000 | Loss: 0.00002385
Iteration 162/1000 | Loss: 0.00003152
Iteration 163/1000 | Loss: 0.00002962
Iteration 164/1000 | Loss: 0.00002565
Iteration 165/1000 | Loss: 0.00002551
Iteration 166/1000 | Loss: 0.00003944
Iteration 167/1000 | Loss: 0.00002541
Iteration 168/1000 | Loss: 0.00003527
Iteration 169/1000 | Loss: 0.00002881
Iteration 170/1000 | Loss: 0.00003255
Iteration 171/1000 | Loss: 0.00002844
Iteration 172/1000 | Loss: 0.00002756
Iteration 173/1000 | Loss: 0.00007752
Iteration 174/1000 | Loss: 0.00003389
Iteration 175/1000 | Loss: 0.00003979
Iteration 176/1000 | Loss: 0.00003333
Iteration 177/1000 | Loss: 0.00003453
Iteration 178/1000 | Loss: 0.00004195
Iteration 179/1000 | Loss: 0.00002814
Iteration 180/1000 | Loss: 0.00002821
Iteration 181/1000 | Loss: 0.00003024
Iteration 182/1000 | Loss: 0.00002668
Iteration 183/1000 | Loss: 0.00003869
Iteration 184/1000 | Loss: 0.00001839
Iteration 185/1000 | Loss: 0.00001752
Iteration 186/1000 | Loss: 0.00001751
Iteration 187/1000 | Loss: 0.00001751
Iteration 188/1000 | Loss: 0.00001747
Iteration 189/1000 | Loss: 0.00001739
Iteration 190/1000 | Loss: 0.00001739
Iteration 191/1000 | Loss: 0.00001739
Iteration 192/1000 | Loss: 0.00001738
Iteration 193/1000 | Loss: 0.00001738
Iteration 194/1000 | Loss: 0.00001738
Iteration 195/1000 | Loss: 0.00001737
Iteration 196/1000 | Loss: 0.00001737
Iteration 197/1000 | Loss: 0.00001736
Iteration 198/1000 | Loss: 0.00001736
Iteration 199/1000 | Loss: 0.00001736
Iteration 200/1000 | Loss: 0.00001736
Iteration 201/1000 | Loss: 0.00001736
Iteration 202/1000 | Loss: 0.00001735
Iteration 203/1000 | Loss: 0.00001735
Iteration 204/1000 | Loss: 0.00001735
Iteration 205/1000 | Loss: 0.00001733
Iteration 206/1000 | Loss: 0.00001733
Iteration 207/1000 | Loss: 0.00001733
Iteration 208/1000 | Loss: 0.00001733
Iteration 209/1000 | Loss: 0.00001733
Iteration 210/1000 | Loss: 0.00001733
Iteration 211/1000 | Loss: 0.00001733
Iteration 212/1000 | Loss: 0.00005794
Iteration 213/1000 | Loss: 0.00003217
Iteration 214/1000 | Loss: 0.00003412
Iteration 215/1000 | Loss: 0.00003009
Iteration 216/1000 | Loss: 0.00002555
Iteration 217/1000 | Loss: 0.00002295
Iteration 218/1000 | Loss: 0.00001736
Iteration 219/1000 | Loss: 0.00001735
Iteration 220/1000 | Loss: 0.00001734
Iteration 221/1000 | Loss: 0.00001734
Iteration 222/1000 | Loss: 0.00001733
Iteration 223/1000 | Loss: 0.00001733
Iteration 224/1000 | Loss: 0.00001733
Iteration 225/1000 | Loss: 0.00001733
Iteration 226/1000 | Loss: 0.00001733
Iteration 227/1000 | Loss: 0.00001733
Iteration 228/1000 | Loss: 0.00001732
Iteration 229/1000 | Loss: 0.00001732
Iteration 230/1000 | Loss: 0.00001732
Iteration 231/1000 | Loss: 0.00001732
Iteration 232/1000 | Loss: 0.00001732
Iteration 233/1000 | Loss: 0.00001732
Iteration 234/1000 | Loss: 0.00001732
Iteration 235/1000 | Loss: 0.00001732
Iteration 236/1000 | Loss: 0.00001732
Iteration 237/1000 | Loss: 0.00001732
Iteration 238/1000 | Loss: 0.00001732
Iteration 239/1000 | Loss: 0.00001732
Iteration 240/1000 | Loss: 0.00001732
Iteration 241/1000 | Loss: 0.00001732
Iteration 242/1000 | Loss: 0.00001732
Iteration 243/1000 | Loss: 0.00001732
Iteration 244/1000 | Loss: 0.00001732
Iteration 245/1000 | Loss: 0.00001732
Iteration 246/1000 | Loss: 0.00001732
Iteration 247/1000 | Loss: 0.00001732
Iteration 248/1000 | Loss: 0.00001732
Iteration 249/1000 | Loss: 0.00001732
Iteration 250/1000 | Loss: 0.00001732
Iteration 251/1000 | Loss: 0.00001732
Iteration 252/1000 | Loss: 0.00001732
Iteration 253/1000 | Loss: 0.00001732
Iteration 254/1000 | Loss: 0.00001732
Iteration 255/1000 | Loss: 0.00001732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [1.7316127923550084e-05, 1.7316127923550084e-05, 1.7316127923550084e-05, 1.7316127923550084e-05, 1.7316127923550084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7316127923550084e-05

Optimization complete. Final v2v error: 3.268219232559204 mm

Highest mean error: 20.19747543334961 mm for frame 172

Lowest mean error: 2.6581099033355713 mm for frame 122

Saving results

Total time: 323.13226532936096
