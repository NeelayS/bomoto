Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=74, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4144-4199
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006276
Iteration 2/25 | Loss: 0.00228768
Iteration 3/25 | Loss: 0.00189660
Iteration 4/25 | Loss: 0.00181167
Iteration 5/25 | Loss: 0.00186390
Iteration 6/25 | Loss: 0.00173436
Iteration 7/25 | Loss: 0.00166505
Iteration 8/25 | Loss: 0.00163768
Iteration 9/25 | Loss: 0.00160040
Iteration 10/25 | Loss: 0.00157535
Iteration 11/25 | Loss: 0.00157893
Iteration 12/25 | Loss: 0.00155296
Iteration 13/25 | Loss: 0.00153257
Iteration 14/25 | Loss: 0.00152513
Iteration 15/25 | Loss: 0.00151932
Iteration 16/25 | Loss: 0.00151639
Iteration 17/25 | Loss: 0.00151402
Iteration 18/25 | Loss: 0.00151295
Iteration 19/25 | Loss: 0.00151216
Iteration 20/25 | Loss: 0.00151399
Iteration 21/25 | Loss: 0.00151041
Iteration 22/25 | Loss: 0.00150964
Iteration 23/25 | Loss: 0.00150931
Iteration 24/25 | Loss: 0.00150926
Iteration 25/25 | Loss: 0.00150926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38950014
Iteration 2/25 | Loss: 0.00417136
Iteration 3/25 | Loss: 0.00417136
Iteration 4/25 | Loss: 0.00417136
Iteration 5/25 | Loss: 0.00417136
Iteration 6/25 | Loss: 0.00417136
Iteration 7/25 | Loss: 0.00417136
Iteration 8/25 | Loss: 0.00417136
Iteration 9/25 | Loss: 0.00417136
Iteration 10/25 | Loss: 0.00417136
Iteration 11/25 | Loss: 0.00417136
Iteration 12/25 | Loss: 0.00417136
Iteration 13/25 | Loss: 0.00417136
Iteration 14/25 | Loss: 0.00417136
Iteration 15/25 | Loss: 0.00417136
Iteration 16/25 | Loss: 0.00417136
Iteration 17/25 | Loss: 0.00417136
Iteration 18/25 | Loss: 0.00417136
Iteration 19/25 | Loss: 0.00417136
Iteration 20/25 | Loss: 0.00417136
Iteration 21/25 | Loss: 0.00417136
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.004171355161815882, 0.004171355161815882, 0.004171355161815882, 0.004171355161815882, 0.004171355161815882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004171355161815882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00417136
Iteration 2/1000 | Loss: 0.00101042
Iteration 3/1000 | Loss: 0.00042746
Iteration 4/1000 | Loss: 0.00026733
Iteration 5/1000 | Loss: 0.00021575
Iteration 6/1000 | Loss: 0.00016709
Iteration 7/1000 | Loss: 0.00013381
Iteration 8/1000 | Loss: 0.00009019
Iteration 9/1000 | Loss: 0.00007025
Iteration 10/1000 | Loss: 0.00005954
Iteration 11/1000 | Loss: 0.00004846
Iteration 12/1000 | Loss: 0.00004197
Iteration 13/1000 | Loss: 0.00003702
Iteration 14/1000 | Loss: 0.00003367
Iteration 15/1000 | Loss: 0.00003124
Iteration 16/1000 | Loss: 0.00002938
Iteration 17/1000 | Loss: 0.00002817
Iteration 18/1000 | Loss: 0.00002726
Iteration 19/1000 | Loss: 0.00002672
Iteration 20/1000 | Loss: 0.00002618
Iteration 21/1000 | Loss: 0.00002586
Iteration 22/1000 | Loss: 0.00002558
Iteration 23/1000 | Loss: 0.00002555
Iteration 24/1000 | Loss: 0.00002532
Iteration 25/1000 | Loss: 0.00002512
Iteration 26/1000 | Loss: 0.00002490
Iteration 27/1000 | Loss: 0.00002475
Iteration 28/1000 | Loss: 0.00002462
Iteration 29/1000 | Loss: 0.00002456
Iteration 30/1000 | Loss: 0.00002449
Iteration 31/1000 | Loss: 0.00002445
Iteration 32/1000 | Loss: 0.00002440
Iteration 33/1000 | Loss: 0.00002437
Iteration 34/1000 | Loss: 0.00002424
Iteration 35/1000 | Loss: 0.00002423
Iteration 36/1000 | Loss: 0.00002415
Iteration 37/1000 | Loss: 0.00002413
Iteration 38/1000 | Loss: 0.00002412
Iteration 39/1000 | Loss: 0.00002412
Iteration 40/1000 | Loss: 0.00002411
Iteration 41/1000 | Loss: 0.00002410
Iteration 42/1000 | Loss: 0.00002406
Iteration 43/1000 | Loss: 0.00002406
Iteration 44/1000 | Loss: 0.00002404
Iteration 45/1000 | Loss: 0.00002404
Iteration 46/1000 | Loss: 0.00002403
Iteration 47/1000 | Loss: 0.00002402
Iteration 48/1000 | Loss: 0.00002401
Iteration 49/1000 | Loss: 0.00002401
Iteration 50/1000 | Loss: 0.00002401
Iteration 51/1000 | Loss: 0.00002401
Iteration 52/1000 | Loss: 0.00002401
Iteration 53/1000 | Loss: 0.00002401
Iteration 54/1000 | Loss: 0.00002400
Iteration 55/1000 | Loss: 0.00002400
Iteration 56/1000 | Loss: 0.00002400
Iteration 57/1000 | Loss: 0.00002398
Iteration 58/1000 | Loss: 0.00002397
Iteration 59/1000 | Loss: 0.00002396
Iteration 60/1000 | Loss: 0.00002396
Iteration 61/1000 | Loss: 0.00002395
Iteration 62/1000 | Loss: 0.00002395
Iteration 63/1000 | Loss: 0.00002395
Iteration 64/1000 | Loss: 0.00002395
Iteration 65/1000 | Loss: 0.00002391
Iteration 66/1000 | Loss: 0.00002391
Iteration 67/1000 | Loss: 0.00002391
Iteration 68/1000 | Loss: 0.00002391
Iteration 69/1000 | Loss: 0.00002391
Iteration 70/1000 | Loss: 0.00002391
Iteration 71/1000 | Loss: 0.00002391
Iteration 72/1000 | Loss: 0.00002390
Iteration 73/1000 | Loss: 0.00002390
Iteration 74/1000 | Loss: 0.00002389
Iteration 75/1000 | Loss: 0.00002389
Iteration 76/1000 | Loss: 0.00002389
Iteration 77/1000 | Loss: 0.00002388
Iteration 78/1000 | Loss: 0.00002388
Iteration 79/1000 | Loss: 0.00002388
Iteration 80/1000 | Loss: 0.00002388
Iteration 81/1000 | Loss: 0.00002387
Iteration 82/1000 | Loss: 0.00002387
Iteration 83/1000 | Loss: 0.00002387
Iteration 84/1000 | Loss: 0.00002386
Iteration 85/1000 | Loss: 0.00002386
Iteration 86/1000 | Loss: 0.00002386
Iteration 87/1000 | Loss: 0.00002386
Iteration 88/1000 | Loss: 0.00002385
Iteration 89/1000 | Loss: 0.00002385
Iteration 90/1000 | Loss: 0.00002385
Iteration 91/1000 | Loss: 0.00002385
Iteration 92/1000 | Loss: 0.00002385
Iteration 93/1000 | Loss: 0.00002385
Iteration 94/1000 | Loss: 0.00002385
Iteration 95/1000 | Loss: 0.00002385
Iteration 96/1000 | Loss: 0.00002385
Iteration 97/1000 | Loss: 0.00002384
Iteration 98/1000 | Loss: 0.00002384
Iteration 99/1000 | Loss: 0.00002384
Iteration 100/1000 | Loss: 0.00002384
Iteration 101/1000 | Loss: 0.00002384
Iteration 102/1000 | Loss: 0.00002384
Iteration 103/1000 | Loss: 0.00002383
Iteration 104/1000 | Loss: 0.00002383
Iteration 105/1000 | Loss: 0.00002382
Iteration 106/1000 | Loss: 0.00002382
Iteration 107/1000 | Loss: 0.00002382
Iteration 108/1000 | Loss: 0.00002382
Iteration 109/1000 | Loss: 0.00002382
Iteration 110/1000 | Loss: 0.00002381
Iteration 111/1000 | Loss: 0.00002381
Iteration 112/1000 | Loss: 0.00002381
Iteration 113/1000 | Loss: 0.00002381
Iteration 114/1000 | Loss: 0.00002381
Iteration 115/1000 | Loss: 0.00002381
Iteration 116/1000 | Loss: 0.00002381
Iteration 117/1000 | Loss: 0.00002381
Iteration 118/1000 | Loss: 0.00002381
Iteration 119/1000 | Loss: 0.00002381
Iteration 120/1000 | Loss: 0.00002381
Iteration 121/1000 | Loss: 0.00002381
Iteration 122/1000 | Loss: 0.00002381
Iteration 123/1000 | Loss: 0.00002380
Iteration 124/1000 | Loss: 0.00002380
Iteration 125/1000 | Loss: 0.00002380
Iteration 126/1000 | Loss: 0.00002380
Iteration 127/1000 | Loss: 0.00002380
Iteration 128/1000 | Loss: 0.00002380
Iteration 129/1000 | Loss: 0.00002380
Iteration 130/1000 | Loss: 0.00002380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.3804775992175564e-05, 2.3804775992175564e-05, 2.3804775992175564e-05, 2.3804775992175564e-05, 2.3804775992175564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3804775992175564e-05

Optimization complete. Final v2v error: 3.6037728786468506 mm

Highest mean error: 12.527831077575684 mm for frame 6

Lowest mean error: 3.3455939292907715 mm for frame 20

Saving results

Total time: 104.98346781730652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003982
Iteration 2/25 | Loss: 0.01003982
Iteration 3/25 | Loss: 0.00374277
Iteration 4/25 | Loss: 0.00206327
Iteration 5/25 | Loss: 0.00191248
Iteration 6/25 | Loss: 0.00191843
Iteration 7/25 | Loss: 0.00181374
Iteration 8/25 | Loss: 0.00173356
Iteration 9/25 | Loss: 0.00178117
Iteration 10/25 | Loss: 0.00165849
Iteration 11/25 | Loss: 0.00163530
Iteration 12/25 | Loss: 0.00162527
Iteration 13/25 | Loss: 0.00160131
Iteration 14/25 | Loss: 0.00158827
Iteration 15/25 | Loss: 0.00157905
Iteration 16/25 | Loss: 0.00160434
Iteration 17/25 | Loss: 0.00155378
Iteration 18/25 | Loss: 0.00156641
Iteration 19/25 | Loss: 0.00156059
Iteration 20/25 | Loss: 0.00155444
Iteration 21/25 | Loss: 0.00154790
Iteration 22/25 | Loss: 0.00154871
Iteration 23/25 | Loss: 0.00155004
Iteration 24/25 | Loss: 0.00154118
Iteration 25/25 | Loss: 0.00153683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45253170
Iteration 2/25 | Loss: 0.00516437
Iteration 3/25 | Loss: 0.00331784
Iteration 4/25 | Loss: 0.00331784
Iteration 5/25 | Loss: 0.00331784
Iteration 6/25 | Loss: 0.00331784
Iteration 7/25 | Loss: 0.00331784
Iteration 8/25 | Loss: 0.00331784
Iteration 9/25 | Loss: 0.00331784
Iteration 10/25 | Loss: 0.00331783
Iteration 11/25 | Loss: 0.00331783
Iteration 12/25 | Loss: 0.00331783
Iteration 13/25 | Loss: 0.00331783
Iteration 14/25 | Loss: 0.00331783
Iteration 15/25 | Loss: 0.00331783
Iteration 16/25 | Loss: 0.00331783
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003317834110930562, 0.003317834110930562, 0.003317834110930562, 0.003317834110930562, 0.003317834110930562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003317834110930562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00331783
Iteration 2/1000 | Loss: 0.00301505
Iteration 3/1000 | Loss: 0.00089538
Iteration 4/1000 | Loss: 0.00274646
Iteration 5/1000 | Loss: 0.00080940
Iteration 6/1000 | Loss: 0.00103114
Iteration 7/1000 | Loss: 0.00052219
Iteration 8/1000 | Loss: 0.00051627
Iteration 9/1000 | Loss: 0.00125940
Iteration 10/1000 | Loss: 0.00428271
Iteration 11/1000 | Loss: 0.00414565
Iteration 12/1000 | Loss: 0.00456900
Iteration 13/1000 | Loss: 0.00524491
Iteration 14/1000 | Loss: 0.00051968
Iteration 15/1000 | Loss: 0.00079217
Iteration 16/1000 | Loss: 0.00044178
Iteration 17/1000 | Loss: 0.00087296
Iteration 18/1000 | Loss: 0.00101255
Iteration 19/1000 | Loss: 0.00023199
Iteration 20/1000 | Loss: 0.00119544
Iteration 21/1000 | Loss: 0.00016303
Iteration 22/1000 | Loss: 0.00043769
Iteration 23/1000 | Loss: 0.00035509
Iteration 24/1000 | Loss: 0.00064707
Iteration 25/1000 | Loss: 0.00424176
Iteration 26/1000 | Loss: 0.00072918
Iteration 27/1000 | Loss: 0.00033537
Iteration 28/1000 | Loss: 0.00040807
Iteration 29/1000 | Loss: 0.00024846
Iteration 30/1000 | Loss: 0.00015448
Iteration 31/1000 | Loss: 0.00017825
Iteration 32/1000 | Loss: 0.00122294
Iteration 33/1000 | Loss: 0.00314809
Iteration 34/1000 | Loss: 0.00059955
Iteration 35/1000 | Loss: 0.00160245
Iteration 36/1000 | Loss: 0.00062329
Iteration 37/1000 | Loss: 0.00034547
Iteration 38/1000 | Loss: 0.00057818
Iteration 39/1000 | Loss: 0.00052468
Iteration 40/1000 | Loss: 0.00012235
Iteration 41/1000 | Loss: 0.00019591
Iteration 42/1000 | Loss: 0.00014327
Iteration 43/1000 | Loss: 0.00054178
Iteration 44/1000 | Loss: 0.00033211
Iteration 45/1000 | Loss: 0.00021424
Iteration 46/1000 | Loss: 0.00055109
Iteration 47/1000 | Loss: 0.00045434
Iteration 48/1000 | Loss: 0.00022505
Iteration 49/1000 | Loss: 0.00011245
Iteration 50/1000 | Loss: 0.00066885
Iteration 51/1000 | Loss: 0.00040542
Iteration 52/1000 | Loss: 0.00058094
Iteration 53/1000 | Loss: 0.00022031
Iteration 54/1000 | Loss: 0.00065360
Iteration 55/1000 | Loss: 0.00091796
Iteration 56/1000 | Loss: 0.00016878
Iteration 57/1000 | Loss: 0.00062670
Iteration 58/1000 | Loss: 0.00013350
Iteration 59/1000 | Loss: 0.00027268
Iteration 60/1000 | Loss: 0.00050938
Iteration 61/1000 | Loss: 0.00010080
Iteration 62/1000 | Loss: 0.00028040
Iteration 63/1000 | Loss: 0.00010498
Iteration 64/1000 | Loss: 0.00010302
Iteration 65/1000 | Loss: 0.00026794
Iteration 66/1000 | Loss: 0.00013780
Iteration 67/1000 | Loss: 0.00008379
Iteration 68/1000 | Loss: 0.00023198
Iteration 69/1000 | Loss: 0.00079202
Iteration 70/1000 | Loss: 0.00096626
Iteration 71/1000 | Loss: 0.00026994
Iteration 72/1000 | Loss: 0.00009995
Iteration 73/1000 | Loss: 0.00007521
Iteration 74/1000 | Loss: 0.00007280
Iteration 75/1000 | Loss: 0.00035967
Iteration 76/1000 | Loss: 0.00052380
Iteration 77/1000 | Loss: 0.00018300
Iteration 78/1000 | Loss: 0.00007942
Iteration 79/1000 | Loss: 0.00012596
Iteration 80/1000 | Loss: 0.00006216
Iteration 81/1000 | Loss: 0.00011848
Iteration 82/1000 | Loss: 0.00006761
Iteration 83/1000 | Loss: 0.00041754
Iteration 84/1000 | Loss: 0.00087153
Iteration 85/1000 | Loss: 0.00008635
Iteration 86/1000 | Loss: 0.00007369
Iteration 87/1000 | Loss: 0.00009763
Iteration 88/1000 | Loss: 0.00013286
Iteration 89/1000 | Loss: 0.00006841
Iteration 90/1000 | Loss: 0.00007563
Iteration 91/1000 | Loss: 0.00029646
Iteration 92/1000 | Loss: 0.00094042
Iteration 93/1000 | Loss: 0.00030985
Iteration 94/1000 | Loss: 0.00012108
Iteration 95/1000 | Loss: 0.00009139
Iteration 96/1000 | Loss: 0.00010537
Iteration 97/1000 | Loss: 0.00031043
Iteration 98/1000 | Loss: 0.00019165
Iteration 99/1000 | Loss: 0.00013533
Iteration 100/1000 | Loss: 0.00022173
Iteration 101/1000 | Loss: 0.00005479
Iteration 102/1000 | Loss: 0.00005078
Iteration 103/1000 | Loss: 0.00028258
Iteration 104/1000 | Loss: 0.00032770
Iteration 105/1000 | Loss: 0.00026361
Iteration 106/1000 | Loss: 0.00016089
Iteration 107/1000 | Loss: 0.00005556
Iteration 108/1000 | Loss: 0.00005029
Iteration 109/1000 | Loss: 0.00006296
Iteration 110/1000 | Loss: 0.00004939
Iteration 111/1000 | Loss: 0.00017045
Iteration 112/1000 | Loss: 0.00044122
Iteration 113/1000 | Loss: 0.00034667
Iteration 114/1000 | Loss: 0.00005320
Iteration 115/1000 | Loss: 0.00004801
Iteration 116/1000 | Loss: 0.00018448
Iteration 117/1000 | Loss: 0.00033655
Iteration 118/1000 | Loss: 0.00024015
Iteration 119/1000 | Loss: 0.00007235
Iteration 120/1000 | Loss: 0.00005335
Iteration 121/1000 | Loss: 0.00005022
Iteration 122/1000 | Loss: 0.00004761
Iteration 123/1000 | Loss: 0.00006764
Iteration 124/1000 | Loss: 0.00018668
Iteration 125/1000 | Loss: 0.00004921
Iteration 126/1000 | Loss: 0.00006799
Iteration 127/1000 | Loss: 0.00004677
Iteration 128/1000 | Loss: 0.00005231
Iteration 129/1000 | Loss: 0.00006411
Iteration 130/1000 | Loss: 0.00061287
Iteration 131/1000 | Loss: 0.00086535
Iteration 132/1000 | Loss: 0.00005265
Iteration 133/1000 | Loss: 0.00009274
Iteration 134/1000 | Loss: 0.00007627
Iteration 135/1000 | Loss: 0.00004056
Iteration 136/1000 | Loss: 0.00009471
Iteration 137/1000 | Loss: 0.00006398
Iteration 138/1000 | Loss: 0.00005323
Iteration 139/1000 | Loss: 0.00004512
Iteration 140/1000 | Loss: 0.00003935
Iteration 141/1000 | Loss: 0.00041557
Iteration 142/1000 | Loss: 0.00025153
Iteration 143/1000 | Loss: 0.00022505
Iteration 144/1000 | Loss: 0.00029105
Iteration 145/1000 | Loss: 0.00021218
Iteration 146/1000 | Loss: 0.00004133
Iteration 147/1000 | Loss: 0.00003749
Iteration 148/1000 | Loss: 0.00003952
Iteration 149/1000 | Loss: 0.00003953
Iteration 150/1000 | Loss: 0.00009657
Iteration 151/1000 | Loss: 0.00004729
Iteration 152/1000 | Loss: 0.00007464
Iteration 153/1000 | Loss: 0.00003509
Iteration 154/1000 | Loss: 0.00003439
Iteration 155/1000 | Loss: 0.00003439
Iteration 156/1000 | Loss: 0.00003530
Iteration 157/1000 | Loss: 0.00003404
Iteration 158/1000 | Loss: 0.00003404
Iteration 159/1000 | Loss: 0.00003404
Iteration 160/1000 | Loss: 0.00003404
Iteration 161/1000 | Loss: 0.00003404
Iteration 162/1000 | Loss: 0.00003404
Iteration 163/1000 | Loss: 0.00003404
Iteration 164/1000 | Loss: 0.00003404
Iteration 165/1000 | Loss: 0.00003404
Iteration 166/1000 | Loss: 0.00003404
Iteration 167/1000 | Loss: 0.00003404
Iteration 168/1000 | Loss: 0.00003404
Iteration 169/1000 | Loss: 0.00004242
Iteration 170/1000 | Loss: 0.00004977
Iteration 171/1000 | Loss: 0.00003665
Iteration 172/1000 | Loss: 0.00004444
Iteration 173/1000 | Loss: 0.00041942
Iteration 174/1000 | Loss: 0.00016596
Iteration 175/1000 | Loss: 0.00014316
Iteration 176/1000 | Loss: 0.00006728
Iteration 177/1000 | Loss: 0.00003933
Iteration 178/1000 | Loss: 0.00003472
Iteration 179/1000 | Loss: 0.00005352
Iteration 180/1000 | Loss: 0.00009625
Iteration 181/1000 | Loss: 0.00003333
Iteration 182/1000 | Loss: 0.00003201
Iteration 183/1000 | Loss: 0.00008159
Iteration 184/1000 | Loss: 0.00006533
Iteration 185/1000 | Loss: 0.00003694
Iteration 186/1000 | Loss: 0.00003861
Iteration 187/1000 | Loss: 0.00005963
Iteration 188/1000 | Loss: 0.00016041
Iteration 189/1000 | Loss: 0.00007429
Iteration 190/1000 | Loss: 0.00004544
Iteration 191/1000 | Loss: 0.00003389
Iteration 192/1000 | Loss: 0.00004345
Iteration 193/1000 | Loss: 0.00004438
Iteration 194/1000 | Loss: 0.00004380
Iteration 195/1000 | Loss: 0.00006402
Iteration 196/1000 | Loss: 0.00003097
Iteration 197/1000 | Loss: 0.00004366
Iteration 198/1000 | Loss: 0.00005748
Iteration 199/1000 | Loss: 0.00003272
Iteration 200/1000 | Loss: 0.00007534
Iteration 201/1000 | Loss: 0.00003201
Iteration 202/1000 | Loss: 0.00003115
Iteration 203/1000 | Loss: 0.00003069
Iteration 204/1000 | Loss: 0.00003068
Iteration 205/1000 | Loss: 0.00003068
Iteration 206/1000 | Loss: 0.00003068
Iteration 207/1000 | Loss: 0.00003068
Iteration 208/1000 | Loss: 0.00003068
Iteration 209/1000 | Loss: 0.00003068
Iteration 210/1000 | Loss: 0.00003068
Iteration 211/1000 | Loss: 0.00003068
Iteration 212/1000 | Loss: 0.00003068
Iteration 213/1000 | Loss: 0.00003815
Iteration 214/1000 | Loss: 0.00007975
Iteration 215/1000 | Loss: 0.00003317
Iteration 216/1000 | Loss: 0.00003060
Iteration 217/1000 | Loss: 0.00003060
Iteration 218/1000 | Loss: 0.00003060
Iteration 219/1000 | Loss: 0.00003059
Iteration 220/1000 | Loss: 0.00003059
Iteration 221/1000 | Loss: 0.00003342
Iteration 222/1000 | Loss: 0.00003059
Iteration 223/1000 | Loss: 0.00003059
Iteration 224/1000 | Loss: 0.00003059
Iteration 225/1000 | Loss: 0.00003059
Iteration 226/1000 | Loss: 0.00003059
Iteration 227/1000 | Loss: 0.00003059
Iteration 228/1000 | Loss: 0.00003059
Iteration 229/1000 | Loss: 0.00003059
Iteration 230/1000 | Loss: 0.00003059
Iteration 231/1000 | Loss: 0.00003058
Iteration 232/1000 | Loss: 0.00003058
Iteration 233/1000 | Loss: 0.00003058
Iteration 234/1000 | Loss: 0.00003057
Iteration 235/1000 | Loss: 0.00003057
Iteration 236/1000 | Loss: 0.00003057
Iteration 237/1000 | Loss: 0.00003057
Iteration 238/1000 | Loss: 0.00005092
Iteration 239/1000 | Loss: 0.00006997
Iteration 240/1000 | Loss: 0.00003242
Iteration 241/1000 | Loss: 0.00003209
Iteration 242/1000 | Loss: 0.00003047
Iteration 243/1000 | Loss: 0.00003047
Iteration 244/1000 | Loss: 0.00003047
Iteration 245/1000 | Loss: 0.00003047
Iteration 246/1000 | Loss: 0.00003047
Iteration 247/1000 | Loss: 0.00003047
Iteration 248/1000 | Loss: 0.00003046
Iteration 249/1000 | Loss: 0.00003046
Iteration 250/1000 | Loss: 0.00003046
Iteration 251/1000 | Loss: 0.00003045
Iteration 252/1000 | Loss: 0.00003045
Iteration 253/1000 | Loss: 0.00003044
Iteration 254/1000 | Loss: 0.00003209
Iteration 255/1000 | Loss: 0.00003044
Iteration 256/1000 | Loss: 0.00003278
Iteration 257/1000 | Loss: 0.00003144
Iteration 258/1000 | Loss: 0.00003348
Iteration 259/1000 | Loss: 0.00003041
Iteration 260/1000 | Loss: 0.00003041
Iteration 261/1000 | Loss: 0.00003041
Iteration 262/1000 | Loss: 0.00003041
Iteration 263/1000 | Loss: 0.00003041
Iteration 264/1000 | Loss: 0.00003041
Iteration 265/1000 | Loss: 0.00003041
Iteration 266/1000 | Loss: 0.00003041
Iteration 267/1000 | Loss: 0.00003041
Iteration 268/1000 | Loss: 0.00003041
Iteration 269/1000 | Loss: 0.00003041
Iteration 270/1000 | Loss: 0.00003041
Iteration 271/1000 | Loss: 0.00003041
Iteration 272/1000 | Loss: 0.00003041
Iteration 273/1000 | Loss: 0.00003041
Iteration 274/1000 | Loss: 0.00003041
Iteration 275/1000 | Loss: 0.00003041
Iteration 276/1000 | Loss: 0.00003041
Iteration 277/1000 | Loss: 0.00003041
Iteration 278/1000 | Loss: 0.00003041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 278. Stopping optimization.
Last 5 losses: [3.0410181352635846e-05, 3.0410181352635846e-05, 3.0410181352635846e-05, 3.0410181352635846e-05, 3.0410181352635846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0410181352635846e-05

Optimization complete. Final v2v error: 3.815613269805908 mm

Highest mean error: 11.04370403289795 mm for frame 50

Lowest mean error: 3.077678918838501 mm for frame 18

Saving results

Total time: 364.41145873069763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00663087
Iteration 2/25 | Loss: 0.00128781
Iteration 3/25 | Loss: 0.00120789
Iteration 4/25 | Loss: 0.00119877
Iteration 5/25 | Loss: 0.00119564
Iteration 6/25 | Loss: 0.00119498
Iteration 7/25 | Loss: 0.00119498
Iteration 8/25 | Loss: 0.00119498
Iteration 9/25 | Loss: 0.00119498
Iteration 10/25 | Loss: 0.00119498
Iteration 11/25 | Loss: 0.00119498
Iteration 12/25 | Loss: 0.00119498
Iteration 13/25 | Loss: 0.00119498
Iteration 14/25 | Loss: 0.00119498
Iteration 15/25 | Loss: 0.00119498
Iteration 16/25 | Loss: 0.00119498
Iteration 17/25 | Loss: 0.00119498
Iteration 18/25 | Loss: 0.00119498
Iteration 19/25 | Loss: 0.00119498
Iteration 20/25 | Loss: 0.00119498
Iteration 21/25 | Loss: 0.00119498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011949798790737987, 0.0011949798790737987, 0.0011949798790737987, 0.0011949798790737987, 0.0011949798790737987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011949798790737987

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.67952633
Iteration 2/25 | Loss: 0.00071729
Iteration 3/25 | Loss: 0.00071728
Iteration 4/25 | Loss: 0.00071728
Iteration 5/25 | Loss: 0.00071728
Iteration 6/25 | Loss: 0.00071728
Iteration 7/25 | Loss: 0.00071728
Iteration 8/25 | Loss: 0.00071728
Iteration 9/25 | Loss: 0.00071728
Iteration 10/25 | Loss: 0.00071728
Iteration 11/25 | Loss: 0.00071728
Iteration 12/25 | Loss: 0.00071728
Iteration 13/25 | Loss: 0.00071728
Iteration 14/25 | Loss: 0.00071728
Iteration 15/25 | Loss: 0.00071728
Iteration 16/25 | Loss: 0.00071728
Iteration 17/25 | Loss: 0.00071728
Iteration 18/25 | Loss: 0.00071728
Iteration 19/25 | Loss: 0.00071728
Iteration 20/25 | Loss: 0.00071728
Iteration 21/25 | Loss: 0.00071728
Iteration 22/25 | Loss: 0.00071728
Iteration 23/25 | Loss: 0.00071728
Iteration 24/25 | Loss: 0.00071728
Iteration 25/25 | Loss: 0.00071728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071728
Iteration 2/1000 | Loss: 0.00003672
Iteration 3/1000 | Loss: 0.00001789
Iteration 4/1000 | Loss: 0.00001507
Iteration 5/1000 | Loss: 0.00001396
Iteration 6/1000 | Loss: 0.00001338
Iteration 7/1000 | Loss: 0.00001304
Iteration 8/1000 | Loss: 0.00001281
Iteration 9/1000 | Loss: 0.00001278
Iteration 10/1000 | Loss: 0.00001275
Iteration 11/1000 | Loss: 0.00001274
Iteration 12/1000 | Loss: 0.00001273
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001257
Iteration 15/1000 | Loss: 0.00001248
Iteration 16/1000 | Loss: 0.00001248
Iteration 17/1000 | Loss: 0.00001245
Iteration 18/1000 | Loss: 0.00001244
Iteration 19/1000 | Loss: 0.00001243
Iteration 20/1000 | Loss: 0.00001242
Iteration 21/1000 | Loss: 0.00001241
Iteration 22/1000 | Loss: 0.00001241
Iteration 23/1000 | Loss: 0.00001240
Iteration 24/1000 | Loss: 0.00001233
Iteration 25/1000 | Loss: 0.00001232
Iteration 26/1000 | Loss: 0.00001227
Iteration 27/1000 | Loss: 0.00001225
Iteration 28/1000 | Loss: 0.00001225
Iteration 29/1000 | Loss: 0.00001224
Iteration 30/1000 | Loss: 0.00001224
Iteration 31/1000 | Loss: 0.00001223
Iteration 32/1000 | Loss: 0.00001223
Iteration 33/1000 | Loss: 0.00001222
Iteration 34/1000 | Loss: 0.00001220
Iteration 35/1000 | Loss: 0.00001220
Iteration 36/1000 | Loss: 0.00001220
Iteration 37/1000 | Loss: 0.00001219
Iteration 38/1000 | Loss: 0.00001218
Iteration 39/1000 | Loss: 0.00001216
Iteration 40/1000 | Loss: 0.00001215
Iteration 41/1000 | Loss: 0.00001215
Iteration 42/1000 | Loss: 0.00001214
Iteration 43/1000 | Loss: 0.00001214
Iteration 44/1000 | Loss: 0.00001214
Iteration 45/1000 | Loss: 0.00001213
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001212
Iteration 49/1000 | Loss: 0.00001211
Iteration 50/1000 | Loss: 0.00001211
Iteration 51/1000 | Loss: 0.00001211
Iteration 52/1000 | Loss: 0.00001210
Iteration 53/1000 | Loss: 0.00001210
Iteration 54/1000 | Loss: 0.00001210
Iteration 55/1000 | Loss: 0.00001209
Iteration 56/1000 | Loss: 0.00001209
Iteration 57/1000 | Loss: 0.00001209
Iteration 58/1000 | Loss: 0.00001208
Iteration 59/1000 | Loss: 0.00001208
Iteration 60/1000 | Loss: 0.00001207
Iteration 61/1000 | Loss: 0.00001207
Iteration 62/1000 | Loss: 0.00001207
Iteration 63/1000 | Loss: 0.00001206
Iteration 64/1000 | Loss: 0.00001206
Iteration 65/1000 | Loss: 0.00001206
Iteration 66/1000 | Loss: 0.00001206
Iteration 67/1000 | Loss: 0.00001206
Iteration 68/1000 | Loss: 0.00001206
Iteration 69/1000 | Loss: 0.00001205
Iteration 70/1000 | Loss: 0.00001205
Iteration 71/1000 | Loss: 0.00001205
Iteration 72/1000 | Loss: 0.00001205
Iteration 73/1000 | Loss: 0.00001205
Iteration 74/1000 | Loss: 0.00001205
Iteration 75/1000 | Loss: 0.00001204
Iteration 76/1000 | Loss: 0.00001204
Iteration 77/1000 | Loss: 0.00001204
Iteration 78/1000 | Loss: 0.00001204
Iteration 79/1000 | Loss: 0.00001203
Iteration 80/1000 | Loss: 0.00001203
Iteration 81/1000 | Loss: 0.00001203
Iteration 82/1000 | Loss: 0.00001203
Iteration 83/1000 | Loss: 0.00001203
Iteration 84/1000 | Loss: 0.00001202
Iteration 85/1000 | Loss: 0.00001202
Iteration 86/1000 | Loss: 0.00001202
Iteration 87/1000 | Loss: 0.00001202
Iteration 88/1000 | Loss: 0.00001202
Iteration 89/1000 | Loss: 0.00001202
Iteration 90/1000 | Loss: 0.00001202
Iteration 91/1000 | Loss: 0.00001202
Iteration 92/1000 | Loss: 0.00001202
Iteration 93/1000 | Loss: 0.00001201
Iteration 94/1000 | Loss: 0.00001201
Iteration 95/1000 | Loss: 0.00001201
Iteration 96/1000 | Loss: 0.00001201
Iteration 97/1000 | Loss: 0.00001201
Iteration 98/1000 | Loss: 0.00001201
Iteration 99/1000 | Loss: 0.00001200
Iteration 100/1000 | Loss: 0.00001200
Iteration 101/1000 | Loss: 0.00001200
Iteration 102/1000 | Loss: 0.00001200
Iteration 103/1000 | Loss: 0.00001199
Iteration 104/1000 | Loss: 0.00001199
Iteration 105/1000 | Loss: 0.00001199
Iteration 106/1000 | Loss: 0.00001199
Iteration 107/1000 | Loss: 0.00001198
Iteration 108/1000 | Loss: 0.00001198
Iteration 109/1000 | Loss: 0.00001198
Iteration 110/1000 | Loss: 0.00001198
Iteration 111/1000 | Loss: 0.00001198
Iteration 112/1000 | Loss: 0.00001198
Iteration 113/1000 | Loss: 0.00001198
Iteration 114/1000 | Loss: 0.00001198
Iteration 115/1000 | Loss: 0.00001197
Iteration 116/1000 | Loss: 0.00001197
Iteration 117/1000 | Loss: 0.00001197
Iteration 118/1000 | Loss: 0.00001197
Iteration 119/1000 | Loss: 0.00001197
Iteration 120/1000 | Loss: 0.00001197
Iteration 121/1000 | Loss: 0.00001197
Iteration 122/1000 | Loss: 0.00001197
Iteration 123/1000 | Loss: 0.00001196
Iteration 124/1000 | Loss: 0.00001196
Iteration 125/1000 | Loss: 0.00001196
Iteration 126/1000 | Loss: 0.00001196
Iteration 127/1000 | Loss: 0.00001196
Iteration 128/1000 | Loss: 0.00001196
Iteration 129/1000 | Loss: 0.00001196
Iteration 130/1000 | Loss: 0.00001196
Iteration 131/1000 | Loss: 0.00001196
Iteration 132/1000 | Loss: 0.00001196
Iteration 133/1000 | Loss: 0.00001195
Iteration 134/1000 | Loss: 0.00001195
Iteration 135/1000 | Loss: 0.00001195
Iteration 136/1000 | Loss: 0.00001195
Iteration 137/1000 | Loss: 0.00001195
Iteration 138/1000 | Loss: 0.00001195
Iteration 139/1000 | Loss: 0.00001195
Iteration 140/1000 | Loss: 0.00001194
Iteration 141/1000 | Loss: 0.00001194
Iteration 142/1000 | Loss: 0.00001194
Iteration 143/1000 | Loss: 0.00001194
Iteration 144/1000 | Loss: 0.00001194
Iteration 145/1000 | Loss: 0.00001194
Iteration 146/1000 | Loss: 0.00001194
Iteration 147/1000 | Loss: 0.00001194
Iteration 148/1000 | Loss: 0.00001194
Iteration 149/1000 | Loss: 0.00001194
Iteration 150/1000 | Loss: 0.00001194
Iteration 151/1000 | Loss: 0.00001194
Iteration 152/1000 | Loss: 0.00001193
Iteration 153/1000 | Loss: 0.00001193
Iteration 154/1000 | Loss: 0.00001193
Iteration 155/1000 | Loss: 0.00001193
Iteration 156/1000 | Loss: 0.00001193
Iteration 157/1000 | Loss: 0.00001193
Iteration 158/1000 | Loss: 0.00001193
Iteration 159/1000 | Loss: 0.00001192
Iteration 160/1000 | Loss: 0.00001192
Iteration 161/1000 | Loss: 0.00001192
Iteration 162/1000 | Loss: 0.00001192
Iteration 163/1000 | Loss: 0.00001192
Iteration 164/1000 | Loss: 0.00001192
Iteration 165/1000 | Loss: 0.00001191
Iteration 166/1000 | Loss: 0.00001191
Iteration 167/1000 | Loss: 0.00001191
Iteration 168/1000 | Loss: 0.00001191
Iteration 169/1000 | Loss: 0.00001191
Iteration 170/1000 | Loss: 0.00001191
Iteration 171/1000 | Loss: 0.00001191
Iteration 172/1000 | Loss: 0.00001190
Iteration 173/1000 | Loss: 0.00001190
Iteration 174/1000 | Loss: 0.00001190
Iteration 175/1000 | Loss: 0.00001190
Iteration 176/1000 | Loss: 0.00001190
Iteration 177/1000 | Loss: 0.00001190
Iteration 178/1000 | Loss: 0.00001189
Iteration 179/1000 | Loss: 0.00001189
Iteration 180/1000 | Loss: 0.00001189
Iteration 181/1000 | Loss: 0.00001189
Iteration 182/1000 | Loss: 0.00001189
Iteration 183/1000 | Loss: 0.00001189
Iteration 184/1000 | Loss: 0.00001189
Iteration 185/1000 | Loss: 0.00001189
Iteration 186/1000 | Loss: 0.00001189
Iteration 187/1000 | Loss: 0.00001189
Iteration 188/1000 | Loss: 0.00001189
Iteration 189/1000 | Loss: 0.00001189
Iteration 190/1000 | Loss: 0.00001189
Iteration 191/1000 | Loss: 0.00001189
Iteration 192/1000 | Loss: 0.00001189
Iteration 193/1000 | Loss: 0.00001189
Iteration 194/1000 | Loss: 0.00001189
Iteration 195/1000 | Loss: 0.00001189
Iteration 196/1000 | Loss: 0.00001189
Iteration 197/1000 | Loss: 0.00001189
Iteration 198/1000 | Loss: 0.00001189
Iteration 199/1000 | Loss: 0.00001189
Iteration 200/1000 | Loss: 0.00001189
Iteration 201/1000 | Loss: 0.00001189
Iteration 202/1000 | Loss: 0.00001189
Iteration 203/1000 | Loss: 0.00001189
Iteration 204/1000 | Loss: 0.00001189
Iteration 205/1000 | Loss: 0.00001189
Iteration 206/1000 | Loss: 0.00001189
Iteration 207/1000 | Loss: 0.00001189
Iteration 208/1000 | Loss: 0.00001189
Iteration 209/1000 | Loss: 0.00001189
Iteration 210/1000 | Loss: 0.00001189
Iteration 211/1000 | Loss: 0.00001189
Iteration 212/1000 | Loss: 0.00001189
Iteration 213/1000 | Loss: 0.00001189
Iteration 214/1000 | Loss: 0.00001189
Iteration 215/1000 | Loss: 0.00001189
Iteration 216/1000 | Loss: 0.00001189
Iteration 217/1000 | Loss: 0.00001189
Iteration 218/1000 | Loss: 0.00001189
Iteration 219/1000 | Loss: 0.00001189
Iteration 220/1000 | Loss: 0.00001189
Iteration 221/1000 | Loss: 0.00001189
Iteration 222/1000 | Loss: 0.00001189
Iteration 223/1000 | Loss: 0.00001189
Iteration 224/1000 | Loss: 0.00001189
Iteration 225/1000 | Loss: 0.00001189
Iteration 226/1000 | Loss: 0.00001189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.1886488209711388e-05, 1.1886488209711388e-05, 1.1886488209711388e-05, 1.1886488209711388e-05, 1.1886488209711388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1886488209711388e-05

Optimization complete. Final v2v error: 2.9554784297943115 mm

Highest mean error: 3.149630308151245 mm for frame 42

Lowest mean error: 2.750546932220459 mm for frame 136

Saving results

Total time: 37.6210572719574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833201
Iteration 2/25 | Loss: 0.00186083
Iteration 3/25 | Loss: 0.00157869
Iteration 4/25 | Loss: 0.00156968
Iteration 5/25 | Loss: 0.00156607
Iteration 6/25 | Loss: 0.00156509
Iteration 7/25 | Loss: 0.00156508
Iteration 8/25 | Loss: 0.00156508
Iteration 9/25 | Loss: 0.00156508
Iteration 10/25 | Loss: 0.00156508
Iteration 11/25 | Loss: 0.00156508
Iteration 12/25 | Loss: 0.00156508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015650758286938071, 0.0015650758286938071, 0.0015650758286938071, 0.0015650758286938071, 0.0015650758286938071]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015650758286938071

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62462300
Iteration 2/25 | Loss: 0.00115404
Iteration 3/25 | Loss: 0.00115404
Iteration 4/25 | Loss: 0.00115404
Iteration 5/25 | Loss: 0.00115404
Iteration 6/25 | Loss: 0.00115404
Iteration 7/25 | Loss: 0.00115404
Iteration 8/25 | Loss: 0.00115404
Iteration 9/25 | Loss: 0.00115404
Iteration 10/25 | Loss: 0.00115404
Iteration 11/25 | Loss: 0.00115404
Iteration 12/25 | Loss: 0.00115404
Iteration 13/25 | Loss: 0.00115404
Iteration 14/25 | Loss: 0.00115404
Iteration 15/25 | Loss: 0.00115404
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001154037774540484, 0.001154037774540484, 0.001154037774540484, 0.001154037774540484, 0.001154037774540484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001154037774540484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115404
Iteration 2/1000 | Loss: 0.00011657
Iteration 3/1000 | Loss: 0.00006654
Iteration 4/1000 | Loss: 0.00005421
Iteration 5/1000 | Loss: 0.00005077
Iteration 6/1000 | Loss: 0.00004871
Iteration 7/1000 | Loss: 0.00004781
Iteration 8/1000 | Loss: 0.00004675
Iteration 9/1000 | Loss: 0.00004584
Iteration 10/1000 | Loss: 0.00004514
Iteration 11/1000 | Loss: 0.00004451
Iteration 12/1000 | Loss: 0.00004391
Iteration 13/1000 | Loss: 0.00004338
Iteration 14/1000 | Loss: 0.00004288
Iteration 15/1000 | Loss: 0.00004247
Iteration 16/1000 | Loss: 0.00004214
Iteration 17/1000 | Loss: 0.00004191
Iteration 18/1000 | Loss: 0.00004165
Iteration 19/1000 | Loss: 0.00004144
Iteration 20/1000 | Loss: 0.00004132
Iteration 21/1000 | Loss: 0.00004131
Iteration 22/1000 | Loss: 0.00004128
Iteration 23/1000 | Loss: 0.00004128
Iteration 24/1000 | Loss: 0.00004128
Iteration 25/1000 | Loss: 0.00004127
Iteration 26/1000 | Loss: 0.00004127
Iteration 27/1000 | Loss: 0.00004127
Iteration 28/1000 | Loss: 0.00004127
Iteration 29/1000 | Loss: 0.00004127
Iteration 30/1000 | Loss: 0.00004127
Iteration 31/1000 | Loss: 0.00004127
Iteration 32/1000 | Loss: 0.00004124
Iteration 33/1000 | Loss: 0.00004118
Iteration 34/1000 | Loss: 0.00004110
Iteration 35/1000 | Loss: 0.00004109
Iteration 36/1000 | Loss: 0.00004099
Iteration 37/1000 | Loss: 0.00004096
Iteration 38/1000 | Loss: 0.00004095
Iteration 39/1000 | Loss: 0.00004093
Iteration 40/1000 | Loss: 0.00004093
Iteration 41/1000 | Loss: 0.00004092
Iteration 42/1000 | Loss: 0.00004092
Iteration 43/1000 | Loss: 0.00004092
Iteration 44/1000 | Loss: 0.00004091
Iteration 45/1000 | Loss: 0.00004091
Iteration 46/1000 | Loss: 0.00004090
Iteration 47/1000 | Loss: 0.00004089
Iteration 48/1000 | Loss: 0.00004089
Iteration 49/1000 | Loss: 0.00004089
Iteration 50/1000 | Loss: 0.00004089
Iteration 51/1000 | Loss: 0.00004089
Iteration 52/1000 | Loss: 0.00004089
Iteration 53/1000 | Loss: 0.00004089
Iteration 54/1000 | Loss: 0.00004088
Iteration 55/1000 | Loss: 0.00004088
Iteration 56/1000 | Loss: 0.00004088
Iteration 57/1000 | Loss: 0.00004088
Iteration 58/1000 | Loss: 0.00004087
Iteration 59/1000 | Loss: 0.00004087
Iteration 60/1000 | Loss: 0.00004087
Iteration 61/1000 | Loss: 0.00004086
Iteration 62/1000 | Loss: 0.00004086
Iteration 63/1000 | Loss: 0.00004086
Iteration 64/1000 | Loss: 0.00004086
Iteration 65/1000 | Loss: 0.00004086
Iteration 66/1000 | Loss: 0.00004085
Iteration 67/1000 | Loss: 0.00004085
Iteration 68/1000 | Loss: 0.00004085
Iteration 69/1000 | Loss: 0.00004085
Iteration 70/1000 | Loss: 0.00004085
Iteration 71/1000 | Loss: 0.00004084
Iteration 72/1000 | Loss: 0.00004084
Iteration 73/1000 | Loss: 0.00004084
Iteration 74/1000 | Loss: 0.00004084
Iteration 75/1000 | Loss: 0.00004084
Iteration 76/1000 | Loss: 0.00004084
Iteration 77/1000 | Loss: 0.00004084
Iteration 78/1000 | Loss: 0.00004083
Iteration 79/1000 | Loss: 0.00004083
Iteration 80/1000 | Loss: 0.00004083
Iteration 81/1000 | Loss: 0.00004083
Iteration 82/1000 | Loss: 0.00004083
Iteration 83/1000 | Loss: 0.00004083
Iteration 84/1000 | Loss: 0.00004083
Iteration 85/1000 | Loss: 0.00004082
Iteration 86/1000 | Loss: 0.00004082
Iteration 87/1000 | Loss: 0.00004082
Iteration 88/1000 | Loss: 0.00004082
Iteration 89/1000 | Loss: 0.00004082
Iteration 90/1000 | Loss: 0.00004082
Iteration 91/1000 | Loss: 0.00004081
Iteration 92/1000 | Loss: 0.00004081
Iteration 93/1000 | Loss: 0.00004081
Iteration 94/1000 | Loss: 0.00004081
Iteration 95/1000 | Loss: 0.00004081
Iteration 96/1000 | Loss: 0.00004081
Iteration 97/1000 | Loss: 0.00004081
Iteration 98/1000 | Loss: 0.00004081
Iteration 99/1000 | Loss: 0.00004081
Iteration 100/1000 | Loss: 0.00004081
Iteration 101/1000 | Loss: 0.00004080
Iteration 102/1000 | Loss: 0.00004080
Iteration 103/1000 | Loss: 0.00004080
Iteration 104/1000 | Loss: 0.00004080
Iteration 105/1000 | Loss: 0.00004080
Iteration 106/1000 | Loss: 0.00004080
Iteration 107/1000 | Loss: 0.00004079
Iteration 108/1000 | Loss: 0.00004079
Iteration 109/1000 | Loss: 0.00004079
Iteration 110/1000 | Loss: 0.00004079
Iteration 111/1000 | Loss: 0.00004079
Iteration 112/1000 | Loss: 0.00004079
Iteration 113/1000 | Loss: 0.00004079
Iteration 114/1000 | Loss: 0.00004079
Iteration 115/1000 | Loss: 0.00004079
Iteration 116/1000 | Loss: 0.00004079
Iteration 117/1000 | Loss: 0.00004078
Iteration 118/1000 | Loss: 0.00004078
Iteration 119/1000 | Loss: 0.00004078
Iteration 120/1000 | Loss: 0.00004078
Iteration 121/1000 | Loss: 0.00004077
Iteration 122/1000 | Loss: 0.00004077
Iteration 123/1000 | Loss: 0.00004077
Iteration 124/1000 | Loss: 0.00004077
Iteration 125/1000 | Loss: 0.00004077
Iteration 126/1000 | Loss: 0.00004076
Iteration 127/1000 | Loss: 0.00004076
Iteration 128/1000 | Loss: 0.00004076
Iteration 129/1000 | Loss: 0.00004076
Iteration 130/1000 | Loss: 0.00004076
Iteration 131/1000 | Loss: 0.00004076
Iteration 132/1000 | Loss: 0.00004075
Iteration 133/1000 | Loss: 0.00004075
Iteration 134/1000 | Loss: 0.00004075
Iteration 135/1000 | Loss: 0.00004075
Iteration 136/1000 | Loss: 0.00004075
Iteration 137/1000 | Loss: 0.00004075
Iteration 138/1000 | Loss: 0.00004075
Iteration 139/1000 | Loss: 0.00004075
Iteration 140/1000 | Loss: 0.00004075
Iteration 141/1000 | Loss: 0.00004075
Iteration 142/1000 | Loss: 0.00004074
Iteration 143/1000 | Loss: 0.00004074
Iteration 144/1000 | Loss: 0.00004074
Iteration 145/1000 | Loss: 0.00004074
Iteration 146/1000 | Loss: 0.00004074
Iteration 147/1000 | Loss: 0.00004074
Iteration 148/1000 | Loss: 0.00004074
Iteration 149/1000 | Loss: 0.00004073
Iteration 150/1000 | Loss: 0.00004073
Iteration 151/1000 | Loss: 0.00004073
Iteration 152/1000 | Loss: 0.00004073
Iteration 153/1000 | Loss: 0.00004073
Iteration 154/1000 | Loss: 0.00004072
Iteration 155/1000 | Loss: 0.00004072
Iteration 156/1000 | Loss: 0.00004072
Iteration 157/1000 | Loss: 0.00004072
Iteration 158/1000 | Loss: 0.00004072
Iteration 159/1000 | Loss: 0.00004072
Iteration 160/1000 | Loss: 0.00004072
Iteration 161/1000 | Loss: 0.00004071
Iteration 162/1000 | Loss: 0.00004071
Iteration 163/1000 | Loss: 0.00004071
Iteration 164/1000 | Loss: 0.00004071
Iteration 165/1000 | Loss: 0.00004071
Iteration 166/1000 | Loss: 0.00004071
Iteration 167/1000 | Loss: 0.00004071
Iteration 168/1000 | Loss: 0.00004071
Iteration 169/1000 | Loss: 0.00004071
Iteration 170/1000 | Loss: 0.00004071
Iteration 171/1000 | Loss: 0.00004071
Iteration 172/1000 | Loss: 0.00004071
Iteration 173/1000 | Loss: 0.00004071
Iteration 174/1000 | Loss: 0.00004070
Iteration 175/1000 | Loss: 0.00004070
Iteration 176/1000 | Loss: 0.00004070
Iteration 177/1000 | Loss: 0.00004070
Iteration 178/1000 | Loss: 0.00004070
Iteration 179/1000 | Loss: 0.00004070
Iteration 180/1000 | Loss: 0.00004070
Iteration 181/1000 | Loss: 0.00004070
Iteration 182/1000 | Loss: 0.00004070
Iteration 183/1000 | Loss: 0.00004070
Iteration 184/1000 | Loss: 0.00004070
Iteration 185/1000 | Loss: 0.00004070
Iteration 186/1000 | Loss: 0.00004069
Iteration 187/1000 | Loss: 0.00004069
Iteration 188/1000 | Loss: 0.00004069
Iteration 189/1000 | Loss: 0.00004069
Iteration 190/1000 | Loss: 0.00004069
Iteration 191/1000 | Loss: 0.00004069
Iteration 192/1000 | Loss: 0.00004069
Iteration 193/1000 | Loss: 0.00004069
Iteration 194/1000 | Loss: 0.00004069
Iteration 195/1000 | Loss: 0.00004069
Iteration 196/1000 | Loss: 0.00004069
Iteration 197/1000 | Loss: 0.00004069
Iteration 198/1000 | Loss: 0.00004069
Iteration 199/1000 | Loss: 0.00004069
Iteration 200/1000 | Loss: 0.00004069
Iteration 201/1000 | Loss: 0.00004069
Iteration 202/1000 | Loss: 0.00004069
Iteration 203/1000 | Loss: 0.00004069
Iteration 204/1000 | Loss: 0.00004069
Iteration 205/1000 | Loss: 0.00004069
Iteration 206/1000 | Loss: 0.00004069
Iteration 207/1000 | Loss: 0.00004069
Iteration 208/1000 | Loss: 0.00004069
Iteration 209/1000 | Loss: 0.00004069
Iteration 210/1000 | Loss: 0.00004069
Iteration 211/1000 | Loss: 0.00004069
Iteration 212/1000 | Loss: 0.00004069
Iteration 213/1000 | Loss: 0.00004069
Iteration 214/1000 | Loss: 0.00004069
Iteration 215/1000 | Loss: 0.00004069
Iteration 216/1000 | Loss: 0.00004069
Iteration 217/1000 | Loss: 0.00004069
Iteration 218/1000 | Loss: 0.00004069
Iteration 219/1000 | Loss: 0.00004069
Iteration 220/1000 | Loss: 0.00004069
Iteration 221/1000 | Loss: 0.00004069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [4.0689461457077414e-05, 4.0689461457077414e-05, 4.0689461457077414e-05, 4.0689461457077414e-05, 4.0689461457077414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.0689461457077414e-05

Optimization complete. Final v2v error: 5.205913066864014 mm

Highest mean error: 6.214316368103027 mm for frame 164

Lowest mean error: 4.301966190338135 mm for frame 10

Saving results

Total time: 53.807612657547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058582
Iteration 2/25 | Loss: 0.00140692
Iteration 3/25 | Loss: 0.00122833
Iteration 4/25 | Loss: 0.00120539
Iteration 5/25 | Loss: 0.00120140
Iteration 6/25 | Loss: 0.00120110
Iteration 7/25 | Loss: 0.00120110
Iteration 8/25 | Loss: 0.00120110
Iteration 9/25 | Loss: 0.00120110
Iteration 10/25 | Loss: 0.00120110
Iteration 11/25 | Loss: 0.00120110
Iteration 12/25 | Loss: 0.00120110
Iteration 13/25 | Loss: 0.00120110
Iteration 14/25 | Loss: 0.00120110
Iteration 15/25 | Loss: 0.00120110
Iteration 16/25 | Loss: 0.00120110
Iteration 17/25 | Loss: 0.00120110
Iteration 18/25 | Loss: 0.00120110
Iteration 19/25 | Loss: 0.00120110
Iteration 20/25 | Loss: 0.00120110
Iteration 21/25 | Loss: 0.00120110
Iteration 22/25 | Loss: 0.00120110
Iteration 23/25 | Loss: 0.00120110
Iteration 24/25 | Loss: 0.00120110
Iteration 25/25 | Loss: 0.00120110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.24869084
Iteration 2/25 | Loss: 0.00055290
Iteration 3/25 | Loss: 0.00055290
Iteration 4/25 | Loss: 0.00055290
Iteration 5/25 | Loss: 0.00055290
Iteration 6/25 | Loss: 0.00055290
Iteration 7/25 | Loss: 0.00055290
Iteration 8/25 | Loss: 0.00055290
Iteration 9/25 | Loss: 0.00055290
Iteration 10/25 | Loss: 0.00055290
Iteration 11/25 | Loss: 0.00055290
Iteration 12/25 | Loss: 0.00055290
Iteration 13/25 | Loss: 0.00055290
Iteration 14/25 | Loss: 0.00055290
Iteration 15/25 | Loss: 0.00055290
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005528953042812645, 0.0005528953042812645, 0.0005528953042812645, 0.0005528953042812645, 0.0005528953042812645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005528953042812645

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055290
Iteration 2/1000 | Loss: 0.00003109
Iteration 3/1000 | Loss: 0.00002040
Iteration 4/1000 | Loss: 0.00001726
Iteration 5/1000 | Loss: 0.00001561
Iteration 6/1000 | Loss: 0.00001470
Iteration 7/1000 | Loss: 0.00001405
Iteration 8/1000 | Loss: 0.00001357
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001290
Iteration 11/1000 | Loss: 0.00001265
Iteration 12/1000 | Loss: 0.00001249
Iteration 13/1000 | Loss: 0.00001239
Iteration 14/1000 | Loss: 0.00001229
Iteration 15/1000 | Loss: 0.00001229
Iteration 16/1000 | Loss: 0.00001227
Iteration 17/1000 | Loss: 0.00001227
Iteration 18/1000 | Loss: 0.00001226
Iteration 19/1000 | Loss: 0.00001226
Iteration 20/1000 | Loss: 0.00001226
Iteration 21/1000 | Loss: 0.00001224
Iteration 22/1000 | Loss: 0.00001224
Iteration 23/1000 | Loss: 0.00001223
Iteration 24/1000 | Loss: 0.00001223
Iteration 25/1000 | Loss: 0.00001223
Iteration 26/1000 | Loss: 0.00001223
Iteration 27/1000 | Loss: 0.00001223
Iteration 28/1000 | Loss: 0.00001223
Iteration 29/1000 | Loss: 0.00001221
Iteration 30/1000 | Loss: 0.00001221
Iteration 31/1000 | Loss: 0.00001221
Iteration 32/1000 | Loss: 0.00001221
Iteration 33/1000 | Loss: 0.00001221
Iteration 34/1000 | Loss: 0.00001220
Iteration 35/1000 | Loss: 0.00001220
Iteration 36/1000 | Loss: 0.00001218
Iteration 37/1000 | Loss: 0.00001218
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001217
Iteration 40/1000 | Loss: 0.00001217
Iteration 41/1000 | Loss: 0.00001217
Iteration 42/1000 | Loss: 0.00001217
Iteration 43/1000 | Loss: 0.00001216
Iteration 44/1000 | Loss: 0.00001216
Iteration 45/1000 | Loss: 0.00001216
Iteration 46/1000 | Loss: 0.00001216
Iteration 47/1000 | Loss: 0.00001216
Iteration 48/1000 | Loss: 0.00001216
Iteration 49/1000 | Loss: 0.00001216
Iteration 50/1000 | Loss: 0.00001216
Iteration 51/1000 | Loss: 0.00001216
Iteration 52/1000 | Loss: 0.00001215
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001212
Iteration 55/1000 | Loss: 0.00001212
Iteration 56/1000 | Loss: 0.00001212
Iteration 57/1000 | Loss: 0.00001212
Iteration 58/1000 | Loss: 0.00001212
Iteration 59/1000 | Loss: 0.00001212
Iteration 60/1000 | Loss: 0.00001212
Iteration 61/1000 | Loss: 0.00001212
Iteration 62/1000 | Loss: 0.00001212
Iteration 63/1000 | Loss: 0.00001212
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001211
Iteration 66/1000 | Loss: 0.00001210
Iteration 67/1000 | Loss: 0.00001210
Iteration 68/1000 | Loss: 0.00001210
Iteration 69/1000 | Loss: 0.00001209
Iteration 70/1000 | Loss: 0.00001209
Iteration 71/1000 | Loss: 0.00001209
Iteration 72/1000 | Loss: 0.00001209
Iteration 73/1000 | Loss: 0.00001207
Iteration 74/1000 | Loss: 0.00001207
Iteration 75/1000 | Loss: 0.00001207
Iteration 76/1000 | Loss: 0.00001207
Iteration 77/1000 | Loss: 0.00001207
Iteration 78/1000 | Loss: 0.00001207
Iteration 79/1000 | Loss: 0.00001207
Iteration 80/1000 | Loss: 0.00001207
Iteration 81/1000 | Loss: 0.00001207
Iteration 82/1000 | Loss: 0.00001207
Iteration 83/1000 | Loss: 0.00001207
Iteration 84/1000 | Loss: 0.00001207
Iteration 85/1000 | Loss: 0.00001207
Iteration 86/1000 | Loss: 0.00001207
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001207
Iteration 91/1000 | Loss: 0.00001207
Iteration 92/1000 | Loss: 0.00001207
Iteration 93/1000 | Loss: 0.00001207
Iteration 94/1000 | Loss: 0.00001207
Iteration 95/1000 | Loss: 0.00001207
Iteration 96/1000 | Loss: 0.00001207
Iteration 97/1000 | Loss: 0.00001207
Iteration 98/1000 | Loss: 0.00001207
Iteration 99/1000 | Loss: 0.00001207
Iteration 100/1000 | Loss: 0.00001207
Iteration 101/1000 | Loss: 0.00001207
Iteration 102/1000 | Loss: 0.00001207
Iteration 103/1000 | Loss: 0.00001207
Iteration 104/1000 | Loss: 0.00001207
Iteration 105/1000 | Loss: 0.00001207
Iteration 106/1000 | Loss: 0.00001207
Iteration 107/1000 | Loss: 0.00001207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.206678007292794e-05, 1.206678007292794e-05, 1.206678007292794e-05, 1.206678007292794e-05, 1.206678007292794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.206678007292794e-05

Optimization complete. Final v2v error: 2.949155330657959 mm

Highest mean error: 3.1786139011383057 mm for frame 32

Lowest mean error: 2.8067739009857178 mm for frame 134

Saving results

Total time: 37.203084230422974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00298794
Iteration 2/25 | Loss: 0.00140937
Iteration 3/25 | Loss: 0.00121681
Iteration 4/25 | Loss: 0.00119185
Iteration 5/25 | Loss: 0.00118370
Iteration 6/25 | Loss: 0.00118088
Iteration 7/25 | Loss: 0.00117991
Iteration 8/25 | Loss: 0.00117945
Iteration 9/25 | Loss: 0.00117939
Iteration 10/25 | Loss: 0.00117939
Iteration 11/25 | Loss: 0.00117939
Iteration 12/25 | Loss: 0.00117939
Iteration 13/25 | Loss: 0.00117939
Iteration 14/25 | Loss: 0.00117939
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001179393264465034, 0.001179393264465034, 0.001179393264465034, 0.001179393264465034, 0.001179393264465034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001179393264465034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43580830
Iteration 2/25 | Loss: 0.00051173
Iteration 3/25 | Loss: 0.00051173
Iteration 4/25 | Loss: 0.00051173
Iteration 5/25 | Loss: 0.00051173
Iteration 6/25 | Loss: 0.00051173
Iteration 7/25 | Loss: 0.00051173
Iteration 8/25 | Loss: 0.00051173
Iteration 9/25 | Loss: 0.00051173
Iteration 10/25 | Loss: 0.00051173
Iteration 11/25 | Loss: 0.00051173
Iteration 12/25 | Loss: 0.00051173
Iteration 13/25 | Loss: 0.00051173
Iteration 14/25 | Loss: 0.00051173
Iteration 15/25 | Loss: 0.00051173
Iteration 16/25 | Loss: 0.00051173
Iteration 17/25 | Loss: 0.00051173
Iteration 18/25 | Loss: 0.00051173
Iteration 19/25 | Loss: 0.00051173
Iteration 20/25 | Loss: 0.00051173
Iteration 21/25 | Loss: 0.00051173
Iteration 22/25 | Loss: 0.00051173
Iteration 23/25 | Loss: 0.00051173
Iteration 24/25 | Loss: 0.00051173
Iteration 25/25 | Loss: 0.00051173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051173
Iteration 2/1000 | Loss: 0.00005045
Iteration 3/1000 | Loss: 0.00002918
Iteration 4/1000 | Loss: 0.00002227
Iteration 5/1000 | Loss: 0.00002065
Iteration 6/1000 | Loss: 0.00001958
Iteration 7/1000 | Loss: 0.00001877
Iteration 8/1000 | Loss: 0.00001827
Iteration 9/1000 | Loss: 0.00001779
Iteration 10/1000 | Loss: 0.00001749
Iteration 11/1000 | Loss: 0.00001723
Iteration 12/1000 | Loss: 0.00001719
Iteration 13/1000 | Loss: 0.00001698
Iteration 14/1000 | Loss: 0.00001677
Iteration 15/1000 | Loss: 0.00001665
Iteration 16/1000 | Loss: 0.00001660
Iteration 17/1000 | Loss: 0.00001660
Iteration 18/1000 | Loss: 0.00001657
Iteration 19/1000 | Loss: 0.00001657
Iteration 20/1000 | Loss: 0.00001656
Iteration 21/1000 | Loss: 0.00001656
Iteration 22/1000 | Loss: 0.00001655
Iteration 23/1000 | Loss: 0.00001655
Iteration 24/1000 | Loss: 0.00001654
Iteration 25/1000 | Loss: 0.00001653
Iteration 26/1000 | Loss: 0.00001653
Iteration 27/1000 | Loss: 0.00001652
Iteration 28/1000 | Loss: 0.00001651
Iteration 29/1000 | Loss: 0.00001650
Iteration 30/1000 | Loss: 0.00001649
Iteration 31/1000 | Loss: 0.00001649
Iteration 32/1000 | Loss: 0.00001648
Iteration 33/1000 | Loss: 0.00001648
Iteration 34/1000 | Loss: 0.00001648
Iteration 35/1000 | Loss: 0.00001647
Iteration 36/1000 | Loss: 0.00001647
Iteration 37/1000 | Loss: 0.00001647
Iteration 38/1000 | Loss: 0.00001646
Iteration 39/1000 | Loss: 0.00001646
Iteration 40/1000 | Loss: 0.00001646
Iteration 41/1000 | Loss: 0.00001645
Iteration 42/1000 | Loss: 0.00001645
Iteration 43/1000 | Loss: 0.00001645
Iteration 44/1000 | Loss: 0.00001645
Iteration 45/1000 | Loss: 0.00001644
Iteration 46/1000 | Loss: 0.00001644
Iteration 47/1000 | Loss: 0.00001644
Iteration 48/1000 | Loss: 0.00001643
Iteration 49/1000 | Loss: 0.00001643
Iteration 50/1000 | Loss: 0.00001643
Iteration 51/1000 | Loss: 0.00001642
Iteration 52/1000 | Loss: 0.00001642
Iteration 53/1000 | Loss: 0.00001641
Iteration 54/1000 | Loss: 0.00001640
Iteration 55/1000 | Loss: 0.00001639
Iteration 56/1000 | Loss: 0.00001639
Iteration 57/1000 | Loss: 0.00001639
Iteration 58/1000 | Loss: 0.00001639
Iteration 59/1000 | Loss: 0.00001639
Iteration 60/1000 | Loss: 0.00001639
Iteration 61/1000 | Loss: 0.00001638
Iteration 62/1000 | Loss: 0.00001638
Iteration 63/1000 | Loss: 0.00001638
Iteration 64/1000 | Loss: 0.00001638
Iteration 65/1000 | Loss: 0.00001637
Iteration 66/1000 | Loss: 0.00001637
Iteration 67/1000 | Loss: 0.00001636
Iteration 68/1000 | Loss: 0.00001636
Iteration 69/1000 | Loss: 0.00001636
Iteration 70/1000 | Loss: 0.00001635
Iteration 71/1000 | Loss: 0.00001635
Iteration 72/1000 | Loss: 0.00001634
Iteration 73/1000 | Loss: 0.00001634
Iteration 74/1000 | Loss: 0.00001634
Iteration 75/1000 | Loss: 0.00001634
Iteration 76/1000 | Loss: 0.00001633
Iteration 77/1000 | Loss: 0.00001633
Iteration 78/1000 | Loss: 0.00001633
Iteration 79/1000 | Loss: 0.00001633
Iteration 80/1000 | Loss: 0.00001633
Iteration 81/1000 | Loss: 0.00001633
Iteration 82/1000 | Loss: 0.00001632
Iteration 83/1000 | Loss: 0.00001632
Iteration 84/1000 | Loss: 0.00001632
Iteration 85/1000 | Loss: 0.00001632
Iteration 86/1000 | Loss: 0.00001631
Iteration 87/1000 | Loss: 0.00001631
Iteration 88/1000 | Loss: 0.00001631
Iteration 89/1000 | Loss: 0.00001631
Iteration 90/1000 | Loss: 0.00001630
Iteration 91/1000 | Loss: 0.00001630
Iteration 92/1000 | Loss: 0.00001630
Iteration 93/1000 | Loss: 0.00001630
Iteration 94/1000 | Loss: 0.00001630
Iteration 95/1000 | Loss: 0.00001629
Iteration 96/1000 | Loss: 0.00001629
Iteration 97/1000 | Loss: 0.00001629
Iteration 98/1000 | Loss: 0.00001628
Iteration 99/1000 | Loss: 0.00001628
Iteration 100/1000 | Loss: 0.00001628
Iteration 101/1000 | Loss: 0.00001628
Iteration 102/1000 | Loss: 0.00001627
Iteration 103/1000 | Loss: 0.00001627
Iteration 104/1000 | Loss: 0.00001627
Iteration 105/1000 | Loss: 0.00001627
Iteration 106/1000 | Loss: 0.00001626
Iteration 107/1000 | Loss: 0.00001626
Iteration 108/1000 | Loss: 0.00001626
Iteration 109/1000 | Loss: 0.00001626
Iteration 110/1000 | Loss: 0.00001626
Iteration 111/1000 | Loss: 0.00001626
Iteration 112/1000 | Loss: 0.00001626
Iteration 113/1000 | Loss: 0.00001626
Iteration 114/1000 | Loss: 0.00001625
Iteration 115/1000 | Loss: 0.00001625
Iteration 116/1000 | Loss: 0.00001625
Iteration 117/1000 | Loss: 0.00001625
Iteration 118/1000 | Loss: 0.00001625
Iteration 119/1000 | Loss: 0.00001625
Iteration 120/1000 | Loss: 0.00001624
Iteration 121/1000 | Loss: 0.00001624
Iteration 122/1000 | Loss: 0.00001624
Iteration 123/1000 | Loss: 0.00001624
Iteration 124/1000 | Loss: 0.00001623
Iteration 125/1000 | Loss: 0.00001623
Iteration 126/1000 | Loss: 0.00001623
Iteration 127/1000 | Loss: 0.00001623
Iteration 128/1000 | Loss: 0.00001623
Iteration 129/1000 | Loss: 0.00001623
Iteration 130/1000 | Loss: 0.00001623
Iteration 131/1000 | Loss: 0.00001623
Iteration 132/1000 | Loss: 0.00001623
Iteration 133/1000 | Loss: 0.00001623
Iteration 134/1000 | Loss: 0.00001623
Iteration 135/1000 | Loss: 0.00001622
Iteration 136/1000 | Loss: 0.00001622
Iteration 137/1000 | Loss: 0.00001622
Iteration 138/1000 | Loss: 0.00001622
Iteration 139/1000 | Loss: 0.00001622
Iteration 140/1000 | Loss: 0.00001622
Iteration 141/1000 | Loss: 0.00001622
Iteration 142/1000 | Loss: 0.00001622
Iteration 143/1000 | Loss: 0.00001622
Iteration 144/1000 | Loss: 0.00001622
Iteration 145/1000 | Loss: 0.00001621
Iteration 146/1000 | Loss: 0.00001621
Iteration 147/1000 | Loss: 0.00001621
Iteration 148/1000 | Loss: 0.00001621
Iteration 149/1000 | Loss: 0.00001620
Iteration 150/1000 | Loss: 0.00001620
Iteration 151/1000 | Loss: 0.00001620
Iteration 152/1000 | Loss: 0.00001620
Iteration 153/1000 | Loss: 0.00001620
Iteration 154/1000 | Loss: 0.00001620
Iteration 155/1000 | Loss: 0.00001620
Iteration 156/1000 | Loss: 0.00001620
Iteration 157/1000 | Loss: 0.00001620
Iteration 158/1000 | Loss: 0.00001620
Iteration 159/1000 | Loss: 0.00001620
Iteration 160/1000 | Loss: 0.00001620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.620436887606047e-05, 1.620436887606047e-05, 1.620436887606047e-05, 1.620436887606047e-05, 1.620436887606047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.620436887606047e-05

Optimization complete. Final v2v error: 3.3864681720733643 mm

Highest mean error: 4.063336372375488 mm for frame 107

Lowest mean error: 3.069227695465088 mm for frame 48

Saving results

Total time: 43.524232387542725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487004
Iteration 2/25 | Loss: 0.00128263
Iteration 3/25 | Loss: 0.00120441
Iteration 4/25 | Loss: 0.00119440
Iteration 5/25 | Loss: 0.00119129
Iteration 6/25 | Loss: 0.00119084
Iteration 7/25 | Loss: 0.00119084
Iteration 8/25 | Loss: 0.00119084
Iteration 9/25 | Loss: 0.00119084
Iteration 10/25 | Loss: 0.00119084
Iteration 11/25 | Loss: 0.00119084
Iteration 12/25 | Loss: 0.00119084
Iteration 13/25 | Loss: 0.00119084
Iteration 14/25 | Loss: 0.00119084
Iteration 15/25 | Loss: 0.00119084
Iteration 16/25 | Loss: 0.00119084
Iteration 17/25 | Loss: 0.00119084
Iteration 18/25 | Loss: 0.00119084
Iteration 19/25 | Loss: 0.00119084
Iteration 20/25 | Loss: 0.00119084
Iteration 21/25 | Loss: 0.00119084
Iteration 22/25 | Loss: 0.00119084
Iteration 23/25 | Loss: 0.00119084
Iteration 24/25 | Loss: 0.00119084
Iteration 25/25 | Loss: 0.00119084

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.69565105
Iteration 2/25 | Loss: 0.00065514
Iteration 3/25 | Loss: 0.00065514
Iteration 4/25 | Loss: 0.00065514
Iteration 5/25 | Loss: 0.00065514
Iteration 6/25 | Loss: 0.00065514
Iteration 7/25 | Loss: 0.00065514
Iteration 8/25 | Loss: 0.00065514
Iteration 9/25 | Loss: 0.00065513
Iteration 10/25 | Loss: 0.00065513
Iteration 11/25 | Loss: 0.00065513
Iteration 12/25 | Loss: 0.00065513
Iteration 13/25 | Loss: 0.00065513
Iteration 14/25 | Loss: 0.00065513
Iteration 15/25 | Loss: 0.00065513
Iteration 16/25 | Loss: 0.00065513
Iteration 17/25 | Loss: 0.00065513
Iteration 18/25 | Loss: 0.00065513
Iteration 19/25 | Loss: 0.00065513
Iteration 20/25 | Loss: 0.00065513
Iteration 21/25 | Loss: 0.00065513
Iteration 22/25 | Loss: 0.00065513
Iteration 23/25 | Loss: 0.00065513
Iteration 24/25 | Loss: 0.00065513
Iteration 25/25 | Loss: 0.00065513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065513
Iteration 2/1000 | Loss: 0.00002754
Iteration 3/1000 | Loss: 0.00001976
Iteration 4/1000 | Loss: 0.00001779
Iteration 5/1000 | Loss: 0.00001668
Iteration 6/1000 | Loss: 0.00001580
Iteration 7/1000 | Loss: 0.00001535
Iteration 8/1000 | Loss: 0.00001498
Iteration 9/1000 | Loss: 0.00001483
Iteration 10/1000 | Loss: 0.00001475
Iteration 11/1000 | Loss: 0.00001472
Iteration 12/1000 | Loss: 0.00001465
Iteration 13/1000 | Loss: 0.00001442
Iteration 14/1000 | Loss: 0.00001437
Iteration 15/1000 | Loss: 0.00001433
Iteration 16/1000 | Loss: 0.00001431
Iteration 17/1000 | Loss: 0.00001431
Iteration 18/1000 | Loss: 0.00001430
Iteration 19/1000 | Loss: 0.00001417
Iteration 20/1000 | Loss: 0.00001412
Iteration 21/1000 | Loss: 0.00001407
Iteration 22/1000 | Loss: 0.00001404
Iteration 23/1000 | Loss: 0.00001404
Iteration 24/1000 | Loss: 0.00001403
Iteration 25/1000 | Loss: 0.00001402
Iteration 26/1000 | Loss: 0.00001401
Iteration 27/1000 | Loss: 0.00001401
Iteration 28/1000 | Loss: 0.00001399
Iteration 29/1000 | Loss: 0.00001398
Iteration 30/1000 | Loss: 0.00001398
Iteration 31/1000 | Loss: 0.00001398
Iteration 32/1000 | Loss: 0.00001393
Iteration 33/1000 | Loss: 0.00001392
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001386
Iteration 36/1000 | Loss: 0.00001385
Iteration 37/1000 | Loss: 0.00001384
Iteration 38/1000 | Loss: 0.00001383
Iteration 39/1000 | Loss: 0.00001382
Iteration 40/1000 | Loss: 0.00001382
Iteration 41/1000 | Loss: 0.00001381
Iteration 42/1000 | Loss: 0.00001380
Iteration 43/1000 | Loss: 0.00001380
Iteration 44/1000 | Loss: 0.00001379
Iteration 45/1000 | Loss: 0.00001379
Iteration 46/1000 | Loss: 0.00001378
Iteration 47/1000 | Loss: 0.00001378
Iteration 48/1000 | Loss: 0.00001377
Iteration 49/1000 | Loss: 0.00001377
Iteration 50/1000 | Loss: 0.00001376
Iteration 51/1000 | Loss: 0.00001376
Iteration 52/1000 | Loss: 0.00001376
Iteration 53/1000 | Loss: 0.00001376
Iteration 54/1000 | Loss: 0.00001376
Iteration 55/1000 | Loss: 0.00001376
Iteration 56/1000 | Loss: 0.00001375
Iteration 57/1000 | Loss: 0.00001375
Iteration 58/1000 | Loss: 0.00001375
Iteration 59/1000 | Loss: 0.00001374
Iteration 60/1000 | Loss: 0.00001374
Iteration 61/1000 | Loss: 0.00001372
Iteration 62/1000 | Loss: 0.00001372
Iteration 63/1000 | Loss: 0.00001370
Iteration 64/1000 | Loss: 0.00001368
Iteration 65/1000 | Loss: 0.00001368
Iteration 66/1000 | Loss: 0.00001368
Iteration 67/1000 | Loss: 0.00001367
Iteration 68/1000 | Loss: 0.00001367
Iteration 69/1000 | Loss: 0.00001367
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001367
Iteration 72/1000 | Loss: 0.00001367
Iteration 73/1000 | Loss: 0.00001367
Iteration 74/1000 | Loss: 0.00001365
Iteration 75/1000 | Loss: 0.00001365
Iteration 76/1000 | Loss: 0.00001365
Iteration 77/1000 | Loss: 0.00001365
Iteration 78/1000 | Loss: 0.00001365
Iteration 79/1000 | Loss: 0.00001365
Iteration 80/1000 | Loss: 0.00001365
Iteration 81/1000 | Loss: 0.00001364
Iteration 82/1000 | Loss: 0.00001364
Iteration 83/1000 | Loss: 0.00001364
Iteration 84/1000 | Loss: 0.00001364
Iteration 85/1000 | Loss: 0.00001363
Iteration 86/1000 | Loss: 0.00001363
Iteration 87/1000 | Loss: 0.00001363
Iteration 88/1000 | Loss: 0.00001362
Iteration 89/1000 | Loss: 0.00001362
Iteration 90/1000 | Loss: 0.00001361
Iteration 91/1000 | Loss: 0.00001360
Iteration 92/1000 | Loss: 0.00001360
Iteration 93/1000 | Loss: 0.00001360
Iteration 94/1000 | Loss: 0.00001360
Iteration 95/1000 | Loss: 0.00001360
Iteration 96/1000 | Loss: 0.00001360
Iteration 97/1000 | Loss: 0.00001360
Iteration 98/1000 | Loss: 0.00001360
Iteration 99/1000 | Loss: 0.00001359
Iteration 100/1000 | Loss: 0.00001359
Iteration 101/1000 | Loss: 0.00001359
Iteration 102/1000 | Loss: 0.00001359
Iteration 103/1000 | Loss: 0.00001359
Iteration 104/1000 | Loss: 0.00001359
Iteration 105/1000 | Loss: 0.00001359
Iteration 106/1000 | Loss: 0.00001358
Iteration 107/1000 | Loss: 0.00001358
Iteration 108/1000 | Loss: 0.00001358
Iteration 109/1000 | Loss: 0.00001358
Iteration 110/1000 | Loss: 0.00001358
Iteration 111/1000 | Loss: 0.00001358
Iteration 112/1000 | Loss: 0.00001358
Iteration 113/1000 | Loss: 0.00001358
Iteration 114/1000 | Loss: 0.00001357
Iteration 115/1000 | Loss: 0.00001357
Iteration 116/1000 | Loss: 0.00001357
Iteration 117/1000 | Loss: 0.00001357
Iteration 118/1000 | Loss: 0.00001357
Iteration 119/1000 | Loss: 0.00001357
Iteration 120/1000 | Loss: 0.00001357
Iteration 121/1000 | Loss: 0.00001357
Iteration 122/1000 | Loss: 0.00001357
Iteration 123/1000 | Loss: 0.00001357
Iteration 124/1000 | Loss: 0.00001356
Iteration 125/1000 | Loss: 0.00001356
Iteration 126/1000 | Loss: 0.00001356
Iteration 127/1000 | Loss: 0.00001356
Iteration 128/1000 | Loss: 0.00001356
Iteration 129/1000 | Loss: 0.00001355
Iteration 130/1000 | Loss: 0.00001355
Iteration 131/1000 | Loss: 0.00001355
Iteration 132/1000 | Loss: 0.00001355
Iteration 133/1000 | Loss: 0.00001355
Iteration 134/1000 | Loss: 0.00001355
Iteration 135/1000 | Loss: 0.00001355
Iteration 136/1000 | Loss: 0.00001354
Iteration 137/1000 | Loss: 0.00001354
Iteration 138/1000 | Loss: 0.00001354
Iteration 139/1000 | Loss: 0.00001354
Iteration 140/1000 | Loss: 0.00001354
Iteration 141/1000 | Loss: 0.00001354
Iteration 142/1000 | Loss: 0.00001354
Iteration 143/1000 | Loss: 0.00001353
Iteration 144/1000 | Loss: 0.00001353
Iteration 145/1000 | Loss: 0.00001353
Iteration 146/1000 | Loss: 0.00001353
Iteration 147/1000 | Loss: 0.00001353
Iteration 148/1000 | Loss: 0.00001353
Iteration 149/1000 | Loss: 0.00001353
Iteration 150/1000 | Loss: 0.00001353
Iteration 151/1000 | Loss: 0.00001353
Iteration 152/1000 | Loss: 0.00001353
Iteration 153/1000 | Loss: 0.00001353
Iteration 154/1000 | Loss: 0.00001353
Iteration 155/1000 | Loss: 0.00001353
Iteration 156/1000 | Loss: 0.00001353
Iteration 157/1000 | Loss: 0.00001353
Iteration 158/1000 | Loss: 0.00001353
Iteration 159/1000 | Loss: 0.00001352
Iteration 160/1000 | Loss: 0.00001352
Iteration 161/1000 | Loss: 0.00001352
Iteration 162/1000 | Loss: 0.00001352
Iteration 163/1000 | Loss: 0.00001352
Iteration 164/1000 | Loss: 0.00001352
Iteration 165/1000 | Loss: 0.00001352
Iteration 166/1000 | Loss: 0.00001352
Iteration 167/1000 | Loss: 0.00001351
Iteration 168/1000 | Loss: 0.00001351
Iteration 169/1000 | Loss: 0.00001351
Iteration 170/1000 | Loss: 0.00001351
Iteration 171/1000 | Loss: 0.00001351
Iteration 172/1000 | Loss: 0.00001351
Iteration 173/1000 | Loss: 0.00001351
Iteration 174/1000 | Loss: 0.00001351
Iteration 175/1000 | Loss: 0.00001351
Iteration 176/1000 | Loss: 0.00001351
Iteration 177/1000 | Loss: 0.00001351
Iteration 178/1000 | Loss: 0.00001351
Iteration 179/1000 | Loss: 0.00001351
Iteration 180/1000 | Loss: 0.00001351
Iteration 181/1000 | Loss: 0.00001351
Iteration 182/1000 | Loss: 0.00001351
Iteration 183/1000 | Loss: 0.00001351
Iteration 184/1000 | Loss: 0.00001350
Iteration 185/1000 | Loss: 0.00001350
Iteration 186/1000 | Loss: 0.00001350
Iteration 187/1000 | Loss: 0.00001350
Iteration 188/1000 | Loss: 0.00001350
Iteration 189/1000 | Loss: 0.00001350
Iteration 190/1000 | Loss: 0.00001350
Iteration 191/1000 | Loss: 0.00001350
Iteration 192/1000 | Loss: 0.00001350
Iteration 193/1000 | Loss: 0.00001350
Iteration 194/1000 | Loss: 0.00001350
Iteration 195/1000 | Loss: 0.00001350
Iteration 196/1000 | Loss: 0.00001350
Iteration 197/1000 | Loss: 0.00001350
Iteration 198/1000 | Loss: 0.00001350
Iteration 199/1000 | Loss: 0.00001350
Iteration 200/1000 | Loss: 0.00001350
Iteration 201/1000 | Loss: 0.00001350
Iteration 202/1000 | Loss: 0.00001349
Iteration 203/1000 | Loss: 0.00001349
Iteration 204/1000 | Loss: 0.00001349
Iteration 205/1000 | Loss: 0.00001349
Iteration 206/1000 | Loss: 0.00001349
Iteration 207/1000 | Loss: 0.00001349
Iteration 208/1000 | Loss: 0.00001349
Iteration 209/1000 | Loss: 0.00001349
Iteration 210/1000 | Loss: 0.00001349
Iteration 211/1000 | Loss: 0.00001349
Iteration 212/1000 | Loss: 0.00001349
Iteration 213/1000 | Loss: 0.00001349
Iteration 214/1000 | Loss: 0.00001349
Iteration 215/1000 | Loss: 0.00001349
Iteration 216/1000 | Loss: 0.00001349
Iteration 217/1000 | Loss: 0.00001349
Iteration 218/1000 | Loss: 0.00001349
Iteration 219/1000 | Loss: 0.00001349
Iteration 220/1000 | Loss: 0.00001349
Iteration 221/1000 | Loss: 0.00001349
Iteration 222/1000 | Loss: 0.00001349
Iteration 223/1000 | Loss: 0.00001349
Iteration 224/1000 | Loss: 0.00001349
Iteration 225/1000 | Loss: 0.00001349
Iteration 226/1000 | Loss: 0.00001349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.3487104297382757e-05, 1.3487104297382757e-05, 1.3487104297382757e-05, 1.3487104297382757e-05, 1.3487104297382757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3487104297382757e-05

Optimization complete. Final v2v error: 3.116468906402588 mm

Highest mean error: 3.6083388328552246 mm for frame 62

Lowest mean error: 2.823375701904297 mm for frame 125

Saving results

Total time: 42.837220907211304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00348781
Iteration 2/25 | Loss: 0.00154422
Iteration 3/25 | Loss: 0.00130942
Iteration 4/25 | Loss: 0.00122714
Iteration 5/25 | Loss: 0.00121215
Iteration 6/25 | Loss: 0.00120289
Iteration 7/25 | Loss: 0.00120185
Iteration 8/25 | Loss: 0.00120464
Iteration 9/25 | Loss: 0.00120434
Iteration 10/25 | Loss: 0.00120395
Iteration 11/25 | Loss: 0.00120124
Iteration 12/25 | Loss: 0.00120086
Iteration 13/25 | Loss: 0.00120051
Iteration 14/25 | Loss: 0.00119864
Iteration 15/25 | Loss: 0.00119827
Iteration 16/25 | Loss: 0.00119819
Iteration 17/25 | Loss: 0.00119819
Iteration 18/25 | Loss: 0.00119819
Iteration 19/25 | Loss: 0.00119819
Iteration 20/25 | Loss: 0.00119819
Iteration 21/25 | Loss: 0.00119818
Iteration 22/25 | Loss: 0.00119818
Iteration 23/25 | Loss: 0.00119818
Iteration 24/25 | Loss: 0.00119818
Iteration 25/25 | Loss: 0.00119818

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46326923
Iteration 2/25 | Loss: 0.00060566
Iteration 3/25 | Loss: 0.00060566
Iteration 4/25 | Loss: 0.00060566
Iteration 5/25 | Loss: 0.00060566
Iteration 6/25 | Loss: 0.00060566
Iteration 7/25 | Loss: 0.00060566
Iteration 8/25 | Loss: 0.00060566
Iteration 9/25 | Loss: 0.00060566
Iteration 10/25 | Loss: 0.00060566
Iteration 11/25 | Loss: 0.00060566
Iteration 12/25 | Loss: 0.00060566
Iteration 13/25 | Loss: 0.00060566
Iteration 14/25 | Loss: 0.00060566
Iteration 15/25 | Loss: 0.00060566
Iteration 16/25 | Loss: 0.00060566
Iteration 17/25 | Loss: 0.00060566
Iteration 18/25 | Loss: 0.00060566
Iteration 19/25 | Loss: 0.00060566
Iteration 20/25 | Loss: 0.00060566
Iteration 21/25 | Loss: 0.00060566
Iteration 22/25 | Loss: 0.00060566
Iteration 23/25 | Loss: 0.00060566
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006056553684175014, 0.0006056553684175014, 0.0006056553684175014, 0.0006056553684175014, 0.0006056553684175014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006056553684175014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060566
Iteration 2/1000 | Loss: 0.00003936
Iteration 3/1000 | Loss: 0.00002733
Iteration 4/1000 | Loss: 0.00002344
Iteration 5/1000 | Loss: 0.00002208
Iteration 6/1000 | Loss: 0.00002088
Iteration 7/1000 | Loss: 0.00002019
Iteration 8/1000 | Loss: 0.00001957
Iteration 9/1000 | Loss: 0.00001910
Iteration 10/1000 | Loss: 0.00001873
Iteration 11/1000 | Loss: 0.00001849
Iteration 12/1000 | Loss: 0.00001826
Iteration 13/1000 | Loss: 0.00001812
Iteration 14/1000 | Loss: 0.00001808
Iteration 15/1000 | Loss: 0.00001807
Iteration 16/1000 | Loss: 0.00001806
Iteration 17/1000 | Loss: 0.00001806
Iteration 18/1000 | Loss: 0.00001805
Iteration 19/1000 | Loss: 0.00001805
Iteration 20/1000 | Loss: 0.00001803
Iteration 21/1000 | Loss: 0.00001803
Iteration 22/1000 | Loss: 0.00001802
Iteration 23/1000 | Loss: 0.00001799
Iteration 24/1000 | Loss: 0.00001797
Iteration 25/1000 | Loss: 0.00001792
Iteration 26/1000 | Loss: 0.00001791
Iteration 27/1000 | Loss: 0.00001790
Iteration 28/1000 | Loss: 0.00001790
Iteration 29/1000 | Loss: 0.00001789
Iteration 30/1000 | Loss: 0.00001789
Iteration 31/1000 | Loss: 0.00001788
Iteration 32/1000 | Loss: 0.00001788
Iteration 33/1000 | Loss: 0.00001788
Iteration 34/1000 | Loss: 0.00001787
Iteration 35/1000 | Loss: 0.00001785
Iteration 36/1000 | Loss: 0.00001784
Iteration 37/1000 | Loss: 0.00001783
Iteration 38/1000 | Loss: 0.00001783
Iteration 39/1000 | Loss: 0.00001782
Iteration 40/1000 | Loss: 0.00001782
Iteration 41/1000 | Loss: 0.00001782
Iteration 42/1000 | Loss: 0.00001782
Iteration 43/1000 | Loss: 0.00001782
Iteration 44/1000 | Loss: 0.00001782
Iteration 45/1000 | Loss: 0.00001782
Iteration 46/1000 | Loss: 0.00001781
Iteration 47/1000 | Loss: 0.00001781
Iteration 48/1000 | Loss: 0.00001781
Iteration 49/1000 | Loss: 0.00001781
Iteration 50/1000 | Loss: 0.00001781
Iteration 51/1000 | Loss: 0.00001780
Iteration 52/1000 | Loss: 0.00001780
Iteration 53/1000 | Loss: 0.00001780
Iteration 54/1000 | Loss: 0.00001779
Iteration 55/1000 | Loss: 0.00001779
Iteration 56/1000 | Loss: 0.00001779
Iteration 57/1000 | Loss: 0.00001779
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001779
Iteration 60/1000 | Loss: 0.00001779
Iteration 61/1000 | Loss: 0.00001779
Iteration 62/1000 | Loss: 0.00001778
Iteration 63/1000 | Loss: 0.00001777
Iteration 64/1000 | Loss: 0.00001777
Iteration 65/1000 | Loss: 0.00001777
Iteration 66/1000 | Loss: 0.00001777
Iteration 67/1000 | Loss: 0.00001777
Iteration 68/1000 | Loss: 0.00001777
Iteration 69/1000 | Loss: 0.00001777
Iteration 70/1000 | Loss: 0.00001777
Iteration 71/1000 | Loss: 0.00001777
Iteration 72/1000 | Loss: 0.00001777
Iteration 73/1000 | Loss: 0.00001777
Iteration 74/1000 | Loss: 0.00001776
Iteration 75/1000 | Loss: 0.00001776
Iteration 76/1000 | Loss: 0.00001776
Iteration 77/1000 | Loss: 0.00001775
Iteration 78/1000 | Loss: 0.00001775
Iteration 79/1000 | Loss: 0.00001775
Iteration 80/1000 | Loss: 0.00001775
Iteration 81/1000 | Loss: 0.00001774
Iteration 82/1000 | Loss: 0.00001774
Iteration 83/1000 | Loss: 0.00001774
Iteration 84/1000 | Loss: 0.00001774
Iteration 85/1000 | Loss: 0.00001774
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001774
Iteration 88/1000 | Loss: 0.00001774
Iteration 89/1000 | Loss: 0.00001773
Iteration 90/1000 | Loss: 0.00001773
Iteration 91/1000 | Loss: 0.00001773
Iteration 92/1000 | Loss: 0.00001773
Iteration 93/1000 | Loss: 0.00001773
Iteration 94/1000 | Loss: 0.00001773
Iteration 95/1000 | Loss: 0.00001772
Iteration 96/1000 | Loss: 0.00001772
Iteration 97/1000 | Loss: 0.00001772
Iteration 98/1000 | Loss: 0.00001772
Iteration 99/1000 | Loss: 0.00001772
Iteration 100/1000 | Loss: 0.00001771
Iteration 101/1000 | Loss: 0.00001771
Iteration 102/1000 | Loss: 0.00001771
Iteration 103/1000 | Loss: 0.00001771
Iteration 104/1000 | Loss: 0.00001770
Iteration 105/1000 | Loss: 0.00001770
Iteration 106/1000 | Loss: 0.00001770
Iteration 107/1000 | Loss: 0.00001770
Iteration 108/1000 | Loss: 0.00001770
Iteration 109/1000 | Loss: 0.00001770
Iteration 110/1000 | Loss: 0.00001769
Iteration 111/1000 | Loss: 0.00001769
Iteration 112/1000 | Loss: 0.00001769
Iteration 113/1000 | Loss: 0.00001769
Iteration 114/1000 | Loss: 0.00001769
Iteration 115/1000 | Loss: 0.00001769
Iteration 116/1000 | Loss: 0.00001769
Iteration 117/1000 | Loss: 0.00001769
Iteration 118/1000 | Loss: 0.00001768
Iteration 119/1000 | Loss: 0.00001768
Iteration 120/1000 | Loss: 0.00001768
Iteration 121/1000 | Loss: 0.00001768
Iteration 122/1000 | Loss: 0.00001768
Iteration 123/1000 | Loss: 0.00001768
Iteration 124/1000 | Loss: 0.00001768
Iteration 125/1000 | Loss: 0.00001767
Iteration 126/1000 | Loss: 0.00001767
Iteration 127/1000 | Loss: 0.00001767
Iteration 128/1000 | Loss: 0.00001767
Iteration 129/1000 | Loss: 0.00001767
Iteration 130/1000 | Loss: 0.00001767
Iteration 131/1000 | Loss: 0.00001767
Iteration 132/1000 | Loss: 0.00001766
Iteration 133/1000 | Loss: 0.00001766
Iteration 134/1000 | Loss: 0.00001766
Iteration 135/1000 | Loss: 0.00001766
Iteration 136/1000 | Loss: 0.00001766
Iteration 137/1000 | Loss: 0.00001766
Iteration 138/1000 | Loss: 0.00001765
Iteration 139/1000 | Loss: 0.00001765
Iteration 140/1000 | Loss: 0.00001765
Iteration 141/1000 | Loss: 0.00001765
Iteration 142/1000 | Loss: 0.00001765
Iteration 143/1000 | Loss: 0.00001765
Iteration 144/1000 | Loss: 0.00001765
Iteration 145/1000 | Loss: 0.00001765
Iteration 146/1000 | Loss: 0.00001765
Iteration 147/1000 | Loss: 0.00001765
Iteration 148/1000 | Loss: 0.00001765
Iteration 149/1000 | Loss: 0.00001765
Iteration 150/1000 | Loss: 0.00001765
Iteration 151/1000 | Loss: 0.00001764
Iteration 152/1000 | Loss: 0.00001764
Iteration 153/1000 | Loss: 0.00001764
Iteration 154/1000 | Loss: 0.00001764
Iteration 155/1000 | Loss: 0.00001764
Iteration 156/1000 | Loss: 0.00001764
Iteration 157/1000 | Loss: 0.00001764
Iteration 158/1000 | Loss: 0.00001764
Iteration 159/1000 | Loss: 0.00001764
Iteration 160/1000 | Loss: 0.00001764
Iteration 161/1000 | Loss: 0.00001763
Iteration 162/1000 | Loss: 0.00001763
Iteration 163/1000 | Loss: 0.00001763
Iteration 164/1000 | Loss: 0.00001763
Iteration 165/1000 | Loss: 0.00001763
Iteration 166/1000 | Loss: 0.00001763
Iteration 167/1000 | Loss: 0.00001763
Iteration 168/1000 | Loss: 0.00001763
Iteration 169/1000 | Loss: 0.00001763
Iteration 170/1000 | Loss: 0.00001763
Iteration 171/1000 | Loss: 0.00001763
Iteration 172/1000 | Loss: 0.00001763
Iteration 173/1000 | Loss: 0.00001763
Iteration 174/1000 | Loss: 0.00001763
Iteration 175/1000 | Loss: 0.00001763
Iteration 176/1000 | Loss: 0.00001763
Iteration 177/1000 | Loss: 0.00001762
Iteration 178/1000 | Loss: 0.00001762
Iteration 179/1000 | Loss: 0.00001762
Iteration 180/1000 | Loss: 0.00001762
Iteration 181/1000 | Loss: 0.00001762
Iteration 182/1000 | Loss: 0.00001762
Iteration 183/1000 | Loss: 0.00001762
Iteration 184/1000 | Loss: 0.00001762
Iteration 185/1000 | Loss: 0.00001762
Iteration 186/1000 | Loss: 0.00001761
Iteration 187/1000 | Loss: 0.00001761
Iteration 188/1000 | Loss: 0.00001761
Iteration 189/1000 | Loss: 0.00001761
Iteration 190/1000 | Loss: 0.00001761
Iteration 191/1000 | Loss: 0.00001761
Iteration 192/1000 | Loss: 0.00001761
Iteration 193/1000 | Loss: 0.00001761
Iteration 194/1000 | Loss: 0.00001761
Iteration 195/1000 | Loss: 0.00001761
Iteration 196/1000 | Loss: 0.00001761
Iteration 197/1000 | Loss: 0.00001761
Iteration 198/1000 | Loss: 0.00001760
Iteration 199/1000 | Loss: 0.00001760
Iteration 200/1000 | Loss: 0.00001760
Iteration 201/1000 | Loss: 0.00001760
Iteration 202/1000 | Loss: 0.00001760
Iteration 203/1000 | Loss: 0.00001760
Iteration 204/1000 | Loss: 0.00001760
Iteration 205/1000 | Loss: 0.00001760
Iteration 206/1000 | Loss: 0.00001760
Iteration 207/1000 | Loss: 0.00001760
Iteration 208/1000 | Loss: 0.00001760
Iteration 209/1000 | Loss: 0.00001760
Iteration 210/1000 | Loss: 0.00001760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.7600510545889847e-05, 1.7600510545889847e-05, 1.7600510545889847e-05, 1.7600510545889847e-05, 1.7600510545889847e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7600510545889847e-05

Optimization complete. Final v2v error: 3.516002893447876 mm

Highest mean error: 4.254193305969238 mm for frame 70

Lowest mean error: 2.9049465656280518 mm for frame 0

Saving results

Total time: 58.44182276725769
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420743
Iteration 2/25 | Loss: 0.00128935
Iteration 3/25 | Loss: 0.00120249
Iteration 4/25 | Loss: 0.00118693
Iteration 5/25 | Loss: 0.00118233
Iteration 6/25 | Loss: 0.00118139
Iteration 7/25 | Loss: 0.00118139
Iteration 8/25 | Loss: 0.00118139
Iteration 9/25 | Loss: 0.00118139
Iteration 10/25 | Loss: 0.00118139
Iteration 11/25 | Loss: 0.00118139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011813947930932045, 0.0011813947930932045, 0.0011813947930932045, 0.0011813947930932045, 0.0011813947930932045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011813947930932045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.54699922
Iteration 2/25 | Loss: 0.00065750
Iteration 3/25 | Loss: 0.00065750
Iteration 4/25 | Loss: 0.00065750
Iteration 5/25 | Loss: 0.00065750
Iteration 6/25 | Loss: 0.00065750
Iteration 7/25 | Loss: 0.00065750
Iteration 8/25 | Loss: 0.00065750
Iteration 9/25 | Loss: 0.00065750
Iteration 10/25 | Loss: 0.00065750
Iteration 11/25 | Loss: 0.00065750
Iteration 12/25 | Loss: 0.00065750
Iteration 13/25 | Loss: 0.00065750
Iteration 14/25 | Loss: 0.00065750
Iteration 15/25 | Loss: 0.00065750
Iteration 16/25 | Loss: 0.00065750
Iteration 17/25 | Loss: 0.00065750
Iteration 18/25 | Loss: 0.00065750
Iteration 19/25 | Loss: 0.00065750
Iteration 20/25 | Loss: 0.00065750
Iteration 21/25 | Loss: 0.00065750
Iteration 22/25 | Loss: 0.00065750
Iteration 23/25 | Loss: 0.00065750
Iteration 24/25 | Loss: 0.00065750
Iteration 25/25 | Loss: 0.00065750

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065750
Iteration 2/1000 | Loss: 0.00002399
Iteration 3/1000 | Loss: 0.00001670
Iteration 4/1000 | Loss: 0.00001525
Iteration 5/1000 | Loss: 0.00001468
Iteration 6/1000 | Loss: 0.00001426
Iteration 7/1000 | Loss: 0.00001389
Iteration 8/1000 | Loss: 0.00001371
Iteration 9/1000 | Loss: 0.00001368
Iteration 10/1000 | Loss: 0.00001362
Iteration 11/1000 | Loss: 0.00001361
Iteration 12/1000 | Loss: 0.00001349
Iteration 13/1000 | Loss: 0.00001338
Iteration 14/1000 | Loss: 0.00001331
Iteration 15/1000 | Loss: 0.00001322
Iteration 16/1000 | Loss: 0.00001321
Iteration 17/1000 | Loss: 0.00001318
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001308
Iteration 20/1000 | Loss: 0.00001306
Iteration 21/1000 | Loss: 0.00001304
Iteration 22/1000 | Loss: 0.00001302
Iteration 23/1000 | Loss: 0.00001301
Iteration 24/1000 | Loss: 0.00001301
Iteration 25/1000 | Loss: 0.00001300
Iteration 26/1000 | Loss: 0.00001300
Iteration 27/1000 | Loss: 0.00001300
Iteration 28/1000 | Loss: 0.00001299
Iteration 29/1000 | Loss: 0.00001299
Iteration 30/1000 | Loss: 0.00001298
Iteration 31/1000 | Loss: 0.00001298
Iteration 32/1000 | Loss: 0.00001297
Iteration 33/1000 | Loss: 0.00001294
Iteration 34/1000 | Loss: 0.00001294
Iteration 35/1000 | Loss: 0.00001293
Iteration 36/1000 | Loss: 0.00001292
Iteration 37/1000 | Loss: 0.00001292
Iteration 38/1000 | Loss: 0.00001291
Iteration 39/1000 | Loss: 0.00001291
Iteration 40/1000 | Loss: 0.00001289
Iteration 41/1000 | Loss: 0.00001288
Iteration 42/1000 | Loss: 0.00001288
Iteration 43/1000 | Loss: 0.00001287
Iteration 44/1000 | Loss: 0.00001287
Iteration 45/1000 | Loss: 0.00001286
Iteration 46/1000 | Loss: 0.00001284
Iteration 47/1000 | Loss: 0.00001283
Iteration 48/1000 | Loss: 0.00001283
Iteration 49/1000 | Loss: 0.00001282
Iteration 50/1000 | Loss: 0.00001282
Iteration 51/1000 | Loss: 0.00001282
Iteration 52/1000 | Loss: 0.00001281
Iteration 53/1000 | Loss: 0.00001281
Iteration 54/1000 | Loss: 0.00001281
Iteration 55/1000 | Loss: 0.00001279
Iteration 56/1000 | Loss: 0.00001279
Iteration 57/1000 | Loss: 0.00001279
Iteration 58/1000 | Loss: 0.00001279
Iteration 59/1000 | Loss: 0.00001279
Iteration 60/1000 | Loss: 0.00001279
Iteration 61/1000 | Loss: 0.00001278
Iteration 62/1000 | Loss: 0.00001278
Iteration 63/1000 | Loss: 0.00001278
Iteration 64/1000 | Loss: 0.00001277
Iteration 65/1000 | Loss: 0.00001277
Iteration 66/1000 | Loss: 0.00001276
Iteration 67/1000 | Loss: 0.00001276
Iteration 68/1000 | Loss: 0.00001275
Iteration 69/1000 | Loss: 0.00001274
Iteration 70/1000 | Loss: 0.00001273
Iteration 71/1000 | Loss: 0.00001273
Iteration 72/1000 | Loss: 0.00001273
Iteration 73/1000 | Loss: 0.00001273
Iteration 74/1000 | Loss: 0.00001272
Iteration 75/1000 | Loss: 0.00001272
Iteration 76/1000 | Loss: 0.00001272
Iteration 77/1000 | Loss: 0.00001272
Iteration 78/1000 | Loss: 0.00001271
Iteration 79/1000 | Loss: 0.00001271
Iteration 80/1000 | Loss: 0.00001271
Iteration 81/1000 | Loss: 0.00001271
Iteration 82/1000 | Loss: 0.00001271
Iteration 83/1000 | Loss: 0.00001271
Iteration 84/1000 | Loss: 0.00001270
Iteration 85/1000 | Loss: 0.00001270
Iteration 86/1000 | Loss: 0.00001269
Iteration 87/1000 | Loss: 0.00001269
Iteration 88/1000 | Loss: 0.00001269
Iteration 89/1000 | Loss: 0.00001268
Iteration 90/1000 | Loss: 0.00001268
Iteration 91/1000 | Loss: 0.00001268
Iteration 92/1000 | Loss: 0.00001267
Iteration 93/1000 | Loss: 0.00001267
Iteration 94/1000 | Loss: 0.00001267
Iteration 95/1000 | Loss: 0.00001266
Iteration 96/1000 | Loss: 0.00001266
Iteration 97/1000 | Loss: 0.00001266
Iteration 98/1000 | Loss: 0.00001266
Iteration 99/1000 | Loss: 0.00001265
Iteration 100/1000 | Loss: 0.00001265
Iteration 101/1000 | Loss: 0.00001265
Iteration 102/1000 | Loss: 0.00001265
Iteration 103/1000 | Loss: 0.00001265
Iteration 104/1000 | Loss: 0.00001264
Iteration 105/1000 | Loss: 0.00001264
Iteration 106/1000 | Loss: 0.00001264
Iteration 107/1000 | Loss: 0.00001264
Iteration 108/1000 | Loss: 0.00001263
Iteration 109/1000 | Loss: 0.00001263
Iteration 110/1000 | Loss: 0.00001263
Iteration 111/1000 | Loss: 0.00001262
Iteration 112/1000 | Loss: 0.00001262
Iteration 113/1000 | Loss: 0.00001262
Iteration 114/1000 | Loss: 0.00001262
Iteration 115/1000 | Loss: 0.00001262
Iteration 116/1000 | Loss: 0.00001262
Iteration 117/1000 | Loss: 0.00001262
Iteration 118/1000 | Loss: 0.00001262
Iteration 119/1000 | Loss: 0.00001262
Iteration 120/1000 | Loss: 0.00001261
Iteration 121/1000 | Loss: 0.00001261
Iteration 122/1000 | Loss: 0.00001261
Iteration 123/1000 | Loss: 0.00001261
Iteration 124/1000 | Loss: 0.00001261
Iteration 125/1000 | Loss: 0.00001260
Iteration 126/1000 | Loss: 0.00001260
Iteration 127/1000 | Loss: 0.00001260
Iteration 128/1000 | Loss: 0.00001260
Iteration 129/1000 | Loss: 0.00001260
Iteration 130/1000 | Loss: 0.00001260
Iteration 131/1000 | Loss: 0.00001259
Iteration 132/1000 | Loss: 0.00001259
Iteration 133/1000 | Loss: 0.00001259
Iteration 134/1000 | Loss: 0.00001259
Iteration 135/1000 | Loss: 0.00001259
Iteration 136/1000 | Loss: 0.00001259
Iteration 137/1000 | Loss: 0.00001259
Iteration 138/1000 | Loss: 0.00001259
Iteration 139/1000 | Loss: 0.00001258
Iteration 140/1000 | Loss: 0.00001258
Iteration 141/1000 | Loss: 0.00001258
Iteration 142/1000 | Loss: 0.00001258
Iteration 143/1000 | Loss: 0.00001258
Iteration 144/1000 | Loss: 0.00001258
Iteration 145/1000 | Loss: 0.00001258
Iteration 146/1000 | Loss: 0.00001258
Iteration 147/1000 | Loss: 0.00001258
Iteration 148/1000 | Loss: 0.00001258
Iteration 149/1000 | Loss: 0.00001258
Iteration 150/1000 | Loss: 0.00001258
Iteration 151/1000 | Loss: 0.00001258
Iteration 152/1000 | Loss: 0.00001258
Iteration 153/1000 | Loss: 0.00001258
Iteration 154/1000 | Loss: 0.00001258
Iteration 155/1000 | Loss: 0.00001258
Iteration 156/1000 | Loss: 0.00001257
Iteration 157/1000 | Loss: 0.00001257
Iteration 158/1000 | Loss: 0.00001257
Iteration 159/1000 | Loss: 0.00001257
Iteration 160/1000 | Loss: 0.00001257
Iteration 161/1000 | Loss: 0.00001257
Iteration 162/1000 | Loss: 0.00001257
Iteration 163/1000 | Loss: 0.00001257
Iteration 164/1000 | Loss: 0.00001257
Iteration 165/1000 | Loss: 0.00001257
Iteration 166/1000 | Loss: 0.00001257
Iteration 167/1000 | Loss: 0.00001257
Iteration 168/1000 | Loss: 0.00001257
Iteration 169/1000 | Loss: 0.00001257
Iteration 170/1000 | Loss: 0.00001257
Iteration 171/1000 | Loss: 0.00001257
Iteration 172/1000 | Loss: 0.00001257
Iteration 173/1000 | Loss: 0.00001256
Iteration 174/1000 | Loss: 0.00001256
Iteration 175/1000 | Loss: 0.00001256
Iteration 176/1000 | Loss: 0.00001256
Iteration 177/1000 | Loss: 0.00001256
Iteration 178/1000 | Loss: 0.00001256
Iteration 179/1000 | Loss: 0.00001256
Iteration 180/1000 | Loss: 0.00001256
Iteration 181/1000 | Loss: 0.00001256
Iteration 182/1000 | Loss: 0.00001256
Iteration 183/1000 | Loss: 0.00001256
Iteration 184/1000 | Loss: 0.00001256
Iteration 185/1000 | Loss: 0.00001256
Iteration 186/1000 | Loss: 0.00001256
Iteration 187/1000 | Loss: 0.00001256
Iteration 188/1000 | Loss: 0.00001256
Iteration 189/1000 | Loss: 0.00001256
Iteration 190/1000 | Loss: 0.00001256
Iteration 191/1000 | Loss: 0.00001256
Iteration 192/1000 | Loss: 0.00001256
Iteration 193/1000 | Loss: 0.00001256
Iteration 194/1000 | Loss: 0.00001256
Iteration 195/1000 | Loss: 0.00001256
Iteration 196/1000 | Loss: 0.00001256
Iteration 197/1000 | Loss: 0.00001255
Iteration 198/1000 | Loss: 0.00001255
Iteration 199/1000 | Loss: 0.00001255
Iteration 200/1000 | Loss: 0.00001255
Iteration 201/1000 | Loss: 0.00001255
Iteration 202/1000 | Loss: 0.00001255
Iteration 203/1000 | Loss: 0.00001255
Iteration 204/1000 | Loss: 0.00001255
Iteration 205/1000 | Loss: 0.00001255
Iteration 206/1000 | Loss: 0.00001255
Iteration 207/1000 | Loss: 0.00001255
Iteration 208/1000 | Loss: 0.00001255
Iteration 209/1000 | Loss: 0.00001255
Iteration 210/1000 | Loss: 0.00001255
Iteration 211/1000 | Loss: 0.00001255
Iteration 212/1000 | Loss: 0.00001255
Iteration 213/1000 | Loss: 0.00001255
Iteration 214/1000 | Loss: 0.00001255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.2554552085930482e-05, 1.2554552085930482e-05, 1.2554552085930482e-05, 1.2554552085930482e-05, 1.2554552085930482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2554552085930482e-05

Optimization complete. Final v2v error: 3.0166995525360107 mm

Highest mean error: 3.4479336738586426 mm for frame 88

Lowest mean error: 2.7672057151794434 mm for frame 123

Saving results

Total time: 38.83037281036377
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404068
Iteration 2/25 | Loss: 0.00137038
Iteration 3/25 | Loss: 0.00121852
Iteration 4/25 | Loss: 0.00120378
Iteration 5/25 | Loss: 0.00119843
Iteration 6/25 | Loss: 0.00119806
Iteration 7/25 | Loss: 0.00119806
Iteration 8/25 | Loss: 0.00119806
Iteration 9/25 | Loss: 0.00119806
Iteration 10/25 | Loss: 0.00119806
Iteration 11/25 | Loss: 0.00119806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011980594135820866, 0.0011980594135820866, 0.0011980594135820866, 0.0011980594135820866, 0.0011980594135820866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011980594135820866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47462308
Iteration 2/25 | Loss: 0.00059447
Iteration 3/25 | Loss: 0.00059447
Iteration 4/25 | Loss: 0.00059447
Iteration 5/25 | Loss: 0.00059447
Iteration 6/25 | Loss: 0.00059447
Iteration 7/25 | Loss: 0.00059447
Iteration 8/25 | Loss: 0.00059447
Iteration 9/25 | Loss: 0.00059447
Iteration 10/25 | Loss: 0.00059447
Iteration 11/25 | Loss: 0.00059447
Iteration 12/25 | Loss: 0.00059447
Iteration 13/25 | Loss: 0.00059447
Iteration 14/25 | Loss: 0.00059447
Iteration 15/25 | Loss: 0.00059447
Iteration 16/25 | Loss: 0.00059447
Iteration 17/25 | Loss: 0.00059447
Iteration 18/25 | Loss: 0.00059447
Iteration 19/25 | Loss: 0.00059447
Iteration 20/25 | Loss: 0.00059447
Iteration 21/25 | Loss: 0.00059447
Iteration 22/25 | Loss: 0.00059447
Iteration 23/25 | Loss: 0.00059447
Iteration 24/25 | Loss: 0.00059447
Iteration 25/25 | Loss: 0.00059447

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059447
Iteration 2/1000 | Loss: 0.00005158
Iteration 3/1000 | Loss: 0.00003452
Iteration 4/1000 | Loss: 0.00002819
Iteration 5/1000 | Loss: 0.00002483
Iteration 6/1000 | Loss: 0.00002277
Iteration 7/1000 | Loss: 0.00002102
Iteration 8/1000 | Loss: 0.00002013
Iteration 9/1000 | Loss: 0.00001943
Iteration 10/1000 | Loss: 0.00001882
Iteration 11/1000 | Loss: 0.00001848
Iteration 12/1000 | Loss: 0.00001811
Iteration 13/1000 | Loss: 0.00001789
Iteration 14/1000 | Loss: 0.00001773
Iteration 15/1000 | Loss: 0.00001756
Iteration 16/1000 | Loss: 0.00001753
Iteration 17/1000 | Loss: 0.00001744
Iteration 18/1000 | Loss: 0.00001741
Iteration 19/1000 | Loss: 0.00001736
Iteration 20/1000 | Loss: 0.00001728
Iteration 21/1000 | Loss: 0.00001728
Iteration 22/1000 | Loss: 0.00001726
Iteration 23/1000 | Loss: 0.00001724
Iteration 24/1000 | Loss: 0.00001724
Iteration 25/1000 | Loss: 0.00001723
Iteration 26/1000 | Loss: 0.00001721
Iteration 27/1000 | Loss: 0.00001720
Iteration 28/1000 | Loss: 0.00001720
Iteration 29/1000 | Loss: 0.00001719
Iteration 30/1000 | Loss: 0.00001718
Iteration 31/1000 | Loss: 0.00001718
Iteration 32/1000 | Loss: 0.00001718
Iteration 33/1000 | Loss: 0.00001717
Iteration 34/1000 | Loss: 0.00001717
Iteration 35/1000 | Loss: 0.00001716
Iteration 36/1000 | Loss: 0.00001716
Iteration 37/1000 | Loss: 0.00001716
Iteration 38/1000 | Loss: 0.00001715
Iteration 39/1000 | Loss: 0.00001715
Iteration 40/1000 | Loss: 0.00001714
Iteration 41/1000 | Loss: 0.00001714
Iteration 42/1000 | Loss: 0.00001713
Iteration 43/1000 | Loss: 0.00001713
Iteration 44/1000 | Loss: 0.00001713
Iteration 45/1000 | Loss: 0.00001713
Iteration 46/1000 | Loss: 0.00001712
Iteration 47/1000 | Loss: 0.00001712
Iteration 48/1000 | Loss: 0.00001711
Iteration 49/1000 | Loss: 0.00001711
Iteration 50/1000 | Loss: 0.00001710
Iteration 51/1000 | Loss: 0.00001710
Iteration 52/1000 | Loss: 0.00001710
Iteration 53/1000 | Loss: 0.00001709
Iteration 54/1000 | Loss: 0.00001709
Iteration 55/1000 | Loss: 0.00001709
Iteration 56/1000 | Loss: 0.00001708
Iteration 57/1000 | Loss: 0.00001708
Iteration 58/1000 | Loss: 0.00001708
Iteration 59/1000 | Loss: 0.00001708
Iteration 60/1000 | Loss: 0.00001707
Iteration 61/1000 | Loss: 0.00001707
Iteration 62/1000 | Loss: 0.00001707
Iteration 63/1000 | Loss: 0.00001706
Iteration 64/1000 | Loss: 0.00001706
Iteration 65/1000 | Loss: 0.00001706
Iteration 66/1000 | Loss: 0.00001706
Iteration 67/1000 | Loss: 0.00001705
Iteration 68/1000 | Loss: 0.00001705
Iteration 69/1000 | Loss: 0.00001705
Iteration 70/1000 | Loss: 0.00001704
Iteration 71/1000 | Loss: 0.00001704
Iteration 72/1000 | Loss: 0.00001704
Iteration 73/1000 | Loss: 0.00001703
Iteration 74/1000 | Loss: 0.00001703
Iteration 75/1000 | Loss: 0.00001703
Iteration 76/1000 | Loss: 0.00001703
Iteration 77/1000 | Loss: 0.00001702
Iteration 78/1000 | Loss: 0.00001702
Iteration 79/1000 | Loss: 0.00001702
Iteration 80/1000 | Loss: 0.00001702
Iteration 81/1000 | Loss: 0.00001701
Iteration 82/1000 | Loss: 0.00001701
Iteration 83/1000 | Loss: 0.00001701
Iteration 84/1000 | Loss: 0.00001701
Iteration 85/1000 | Loss: 0.00001701
Iteration 86/1000 | Loss: 0.00001701
Iteration 87/1000 | Loss: 0.00001700
Iteration 88/1000 | Loss: 0.00001700
Iteration 89/1000 | Loss: 0.00001700
Iteration 90/1000 | Loss: 0.00001700
Iteration 91/1000 | Loss: 0.00001699
Iteration 92/1000 | Loss: 0.00001699
Iteration 93/1000 | Loss: 0.00001699
Iteration 94/1000 | Loss: 0.00001698
Iteration 95/1000 | Loss: 0.00001698
Iteration 96/1000 | Loss: 0.00001698
Iteration 97/1000 | Loss: 0.00001698
Iteration 98/1000 | Loss: 0.00001698
Iteration 99/1000 | Loss: 0.00001698
Iteration 100/1000 | Loss: 0.00001698
Iteration 101/1000 | Loss: 0.00001697
Iteration 102/1000 | Loss: 0.00001697
Iteration 103/1000 | Loss: 0.00001697
Iteration 104/1000 | Loss: 0.00001697
Iteration 105/1000 | Loss: 0.00001697
Iteration 106/1000 | Loss: 0.00001697
Iteration 107/1000 | Loss: 0.00001697
Iteration 108/1000 | Loss: 0.00001697
Iteration 109/1000 | Loss: 0.00001697
Iteration 110/1000 | Loss: 0.00001697
Iteration 111/1000 | Loss: 0.00001697
Iteration 112/1000 | Loss: 0.00001696
Iteration 113/1000 | Loss: 0.00001696
Iteration 114/1000 | Loss: 0.00001696
Iteration 115/1000 | Loss: 0.00001696
Iteration 116/1000 | Loss: 0.00001696
Iteration 117/1000 | Loss: 0.00001696
Iteration 118/1000 | Loss: 0.00001696
Iteration 119/1000 | Loss: 0.00001695
Iteration 120/1000 | Loss: 0.00001695
Iteration 121/1000 | Loss: 0.00001695
Iteration 122/1000 | Loss: 0.00001695
Iteration 123/1000 | Loss: 0.00001695
Iteration 124/1000 | Loss: 0.00001695
Iteration 125/1000 | Loss: 0.00001695
Iteration 126/1000 | Loss: 0.00001695
Iteration 127/1000 | Loss: 0.00001695
Iteration 128/1000 | Loss: 0.00001695
Iteration 129/1000 | Loss: 0.00001695
Iteration 130/1000 | Loss: 0.00001694
Iteration 131/1000 | Loss: 0.00001694
Iteration 132/1000 | Loss: 0.00001694
Iteration 133/1000 | Loss: 0.00001694
Iteration 134/1000 | Loss: 0.00001694
Iteration 135/1000 | Loss: 0.00001694
Iteration 136/1000 | Loss: 0.00001694
Iteration 137/1000 | Loss: 0.00001694
Iteration 138/1000 | Loss: 0.00001694
Iteration 139/1000 | Loss: 0.00001694
Iteration 140/1000 | Loss: 0.00001694
Iteration 141/1000 | Loss: 0.00001694
Iteration 142/1000 | Loss: 0.00001694
Iteration 143/1000 | Loss: 0.00001693
Iteration 144/1000 | Loss: 0.00001693
Iteration 145/1000 | Loss: 0.00001693
Iteration 146/1000 | Loss: 0.00001693
Iteration 147/1000 | Loss: 0.00001693
Iteration 148/1000 | Loss: 0.00001693
Iteration 149/1000 | Loss: 0.00001693
Iteration 150/1000 | Loss: 0.00001693
Iteration 151/1000 | Loss: 0.00001693
Iteration 152/1000 | Loss: 0.00001693
Iteration 153/1000 | Loss: 0.00001693
Iteration 154/1000 | Loss: 0.00001692
Iteration 155/1000 | Loss: 0.00001692
Iteration 156/1000 | Loss: 0.00001692
Iteration 157/1000 | Loss: 0.00001692
Iteration 158/1000 | Loss: 0.00001692
Iteration 159/1000 | Loss: 0.00001692
Iteration 160/1000 | Loss: 0.00001692
Iteration 161/1000 | Loss: 0.00001692
Iteration 162/1000 | Loss: 0.00001692
Iteration 163/1000 | Loss: 0.00001692
Iteration 164/1000 | Loss: 0.00001692
Iteration 165/1000 | Loss: 0.00001692
Iteration 166/1000 | Loss: 0.00001692
Iteration 167/1000 | Loss: 0.00001692
Iteration 168/1000 | Loss: 0.00001692
Iteration 169/1000 | Loss: 0.00001692
Iteration 170/1000 | Loss: 0.00001691
Iteration 171/1000 | Loss: 0.00001691
Iteration 172/1000 | Loss: 0.00001691
Iteration 173/1000 | Loss: 0.00001691
Iteration 174/1000 | Loss: 0.00001691
Iteration 175/1000 | Loss: 0.00001691
Iteration 176/1000 | Loss: 0.00001691
Iteration 177/1000 | Loss: 0.00001691
Iteration 178/1000 | Loss: 0.00001691
Iteration 179/1000 | Loss: 0.00001691
Iteration 180/1000 | Loss: 0.00001691
Iteration 181/1000 | Loss: 0.00001691
Iteration 182/1000 | Loss: 0.00001691
Iteration 183/1000 | Loss: 0.00001691
Iteration 184/1000 | Loss: 0.00001691
Iteration 185/1000 | Loss: 0.00001691
Iteration 186/1000 | Loss: 0.00001691
Iteration 187/1000 | Loss: 0.00001691
Iteration 188/1000 | Loss: 0.00001691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.690960198175162e-05, 1.690960198175162e-05, 1.690960198175162e-05, 1.690960198175162e-05, 1.690960198175162e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.690960198175162e-05

Optimization complete. Final v2v error: 3.4871456623077393 mm

Highest mean error: 4.467403888702393 mm for frame 5

Lowest mean error: 2.9069244861602783 mm for frame 128

Saving results

Total time: 50.46552491188049
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833496
Iteration 2/25 | Loss: 0.00190207
Iteration 3/25 | Loss: 0.00159000
Iteration 4/25 | Loss: 0.00157637
Iteration 5/25 | Loss: 0.00157417
Iteration 6/25 | Loss: 0.00157376
Iteration 7/25 | Loss: 0.00157376
Iteration 8/25 | Loss: 0.00157376
Iteration 9/25 | Loss: 0.00157376
Iteration 10/25 | Loss: 0.00157376
Iteration 11/25 | Loss: 0.00157376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015737562207505107, 0.0015737562207505107, 0.0015737562207505107, 0.0015737562207505107, 0.0015737562207505107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015737562207505107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.39791057
Iteration 2/25 | Loss: 0.00113460
Iteration 3/25 | Loss: 0.00113460
Iteration 4/25 | Loss: 0.00113460
Iteration 5/25 | Loss: 0.00113460
Iteration 6/25 | Loss: 0.00113460
Iteration 7/25 | Loss: 0.00113460
Iteration 8/25 | Loss: 0.00113460
Iteration 9/25 | Loss: 0.00113460
Iteration 10/25 | Loss: 0.00113460
Iteration 11/25 | Loss: 0.00113460
Iteration 12/25 | Loss: 0.00113460
Iteration 13/25 | Loss: 0.00113460
Iteration 14/25 | Loss: 0.00113460
Iteration 15/25 | Loss: 0.00113460
Iteration 16/25 | Loss: 0.00113460
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011345965322107077, 0.0011345965322107077, 0.0011345965322107077, 0.0011345965322107077, 0.0011345965322107077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011345965322107077

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113460
Iteration 2/1000 | Loss: 0.00011001
Iteration 3/1000 | Loss: 0.00007054
Iteration 4/1000 | Loss: 0.00005630
Iteration 5/1000 | Loss: 0.00005292
Iteration 6/1000 | Loss: 0.00005146
Iteration 7/1000 | Loss: 0.00004974
Iteration 8/1000 | Loss: 0.00004787
Iteration 9/1000 | Loss: 0.00004679
Iteration 10/1000 | Loss: 0.00004615
Iteration 11/1000 | Loss: 0.00004530
Iteration 12/1000 | Loss: 0.00004477
Iteration 13/1000 | Loss: 0.00004432
Iteration 14/1000 | Loss: 0.00004384
Iteration 15/1000 | Loss: 0.00004351
Iteration 16/1000 | Loss: 0.00004329
Iteration 17/1000 | Loss: 0.00004308
Iteration 18/1000 | Loss: 0.00004292
Iteration 19/1000 | Loss: 0.00004284
Iteration 20/1000 | Loss: 0.00004282
Iteration 21/1000 | Loss: 0.00004278
Iteration 22/1000 | Loss: 0.00004278
Iteration 23/1000 | Loss: 0.00004278
Iteration 24/1000 | Loss: 0.00004278
Iteration 25/1000 | Loss: 0.00004278
Iteration 26/1000 | Loss: 0.00004277
Iteration 27/1000 | Loss: 0.00004277
Iteration 28/1000 | Loss: 0.00004271
Iteration 29/1000 | Loss: 0.00004271
Iteration 30/1000 | Loss: 0.00004270
Iteration 31/1000 | Loss: 0.00004270
Iteration 32/1000 | Loss: 0.00004270
Iteration 33/1000 | Loss: 0.00004270
Iteration 34/1000 | Loss: 0.00004270
Iteration 35/1000 | Loss: 0.00004270
Iteration 36/1000 | Loss: 0.00004270
Iteration 37/1000 | Loss: 0.00004268
Iteration 38/1000 | Loss: 0.00004267
Iteration 39/1000 | Loss: 0.00004264
Iteration 40/1000 | Loss: 0.00004253
Iteration 41/1000 | Loss: 0.00004252
Iteration 42/1000 | Loss: 0.00004252
Iteration 43/1000 | Loss: 0.00004252
Iteration 44/1000 | Loss: 0.00004249
Iteration 45/1000 | Loss: 0.00004247
Iteration 46/1000 | Loss: 0.00004243
Iteration 47/1000 | Loss: 0.00004242
Iteration 48/1000 | Loss: 0.00004242
Iteration 49/1000 | Loss: 0.00004242
Iteration 50/1000 | Loss: 0.00004242
Iteration 51/1000 | Loss: 0.00004241
Iteration 52/1000 | Loss: 0.00004241
Iteration 53/1000 | Loss: 0.00004241
Iteration 54/1000 | Loss: 0.00004241
Iteration 55/1000 | Loss: 0.00004240
Iteration 56/1000 | Loss: 0.00004239
Iteration 57/1000 | Loss: 0.00004239
Iteration 58/1000 | Loss: 0.00004239
Iteration 59/1000 | Loss: 0.00004239
Iteration 60/1000 | Loss: 0.00004239
Iteration 61/1000 | Loss: 0.00004239
Iteration 62/1000 | Loss: 0.00004239
Iteration 63/1000 | Loss: 0.00004239
Iteration 64/1000 | Loss: 0.00004239
Iteration 65/1000 | Loss: 0.00004239
Iteration 66/1000 | Loss: 0.00004238
Iteration 67/1000 | Loss: 0.00004238
Iteration 68/1000 | Loss: 0.00004238
Iteration 69/1000 | Loss: 0.00004236
Iteration 70/1000 | Loss: 0.00004236
Iteration 71/1000 | Loss: 0.00004235
Iteration 72/1000 | Loss: 0.00004235
Iteration 73/1000 | Loss: 0.00004235
Iteration 74/1000 | Loss: 0.00004235
Iteration 75/1000 | Loss: 0.00004235
Iteration 76/1000 | Loss: 0.00004235
Iteration 77/1000 | Loss: 0.00004235
Iteration 78/1000 | Loss: 0.00004234
Iteration 79/1000 | Loss: 0.00004233
Iteration 80/1000 | Loss: 0.00004233
Iteration 81/1000 | Loss: 0.00004233
Iteration 82/1000 | Loss: 0.00004232
Iteration 83/1000 | Loss: 0.00004232
Iteration 84/1000 | Loss: 0.00004232
Iteration 85/1000 | Loss: 0.00004232
Iteration 86/1000 | Loss: 0.00004232
Iteration 87/1000 | Loss: 0.00004232
Iteration 88/1000 | Loss: 0.00004232
Iteration 89/1000 | Loss: 0.00004231
Iteration 90/1000 | Loss: 0.00004231
Iteration 91/1000 | Loss: 0.00004231
Iteration 92/1000 | Loss: 0.00004231
Iteration 93/1000 | Loss: 0.00004231
Iteration 94/1000 | Loss: 0.00004231
Iteration 95/1000 | Loss: 0.00004230
Iteration 96/1000 | Loss: 0.00004230
Iteration 97/1000 | Loss: 0.00004230
Iteration 98/1000 | Loss: 0.00004230
Iteration 99/1000 | Loss: 0.00004230
Iteration 100/1000 | Loss: 0.00004230
Iteration 101/1000 | Loss: 0.00004229
Iteration 102/1000 | Loss: 0.00004229
Iteration 103/1000 | Loss: 0.00004229
Iteration 104/1000 | Loss: 0.00004229
Iteration 105/1000 | Loss: 0.00004229
Iteration 106/1000 | Loss: 0.00004229
Iteration 107/1000 | Loss: 0.00004229
Iteration 108/1000 | Loss: 0.00004229
Iteration 109/1000 | Loss: 0.00004229
Iteration 110/1000 | Loss: 0.00004229
Iteration 111/1000 | Loss: 0.00004229
Iteration 112/1000 | Loss: 0.00004229
Iteration 113/1000 | Loss: 0.00004229
Iteration 114/1000 | Loss: 0.00004229
Iteration 115/1000 | Loss: 0.00004229
Iteration 116/1000 | Loss: 0.00004229
Iteration 117/1000 | Loss: 0.00004229
Iteration 118/1000 | Loss: 0.00004229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [4.2289248085580766e-05, 4.2289248085580766e-05, 4.2289248085580766e-05, 4.2289248085580766e-05, 4.2289248085580766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.2289248085580766e-05

Optimization complete. Final v2v error: 5.258302211761475 mm

Highest mean error: 5.417933464050293 mm for frame 95

Lowest mean error: 5.159246921539307 mm for frame 18

Saving results

Total time: 43.729756593704224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456736
Iteration 2/25 | Loss: 0.00153475
Iteration 3/25 | Loss: 0.00126276
Iteration 4/25 | Loss: 0.00122519
Iteration 5/25 | Loss: 0.00121823
Iteration 6/25 | Loss: 0.00121651
Iteration 7/25 | Loss: 0.00121648
Iteration 8/25 | Loss: 0.00121648
Iteration 9/25 | Loss: 0.00121648
Iteration 10/25 | Loss: 0.00121648
Iteration 11/25 | Loss: 0.00121648
Iteration 12/25 | Loss: 0.00121648
Iteration 13/25 | Loss: 0.00121648
Iteration 14/25 | Loss: 0.00121648
Iteration 15/25 | Loss: 0.00121648
Iteration 16/25 | Loss: 0.00121648
Iteration 17/25 | Loss: 0.00121648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012164840009063482, 0.0012164840009063482, 0.0012164840009063482, 0.0012164840009063482, 0.0012164840009063482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012164840009063482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56767082
Iteration 2/25 | Loss: 0.00067144
Iteration 3/25 | Loss: 0.00067144
Iteration 4/25 | Loss: 0.00067144
Iteration 5/25 | Loss: 0.00067144
Iteration 6/25 | Loss: 0.00067144
Iteration 7/25 | Loss: 0.00067144
Iteration 8/25 | Loss: 0.00067144
Iteration 9/25 | Loss: 0.00067144
Iteration 10/25 | Loss: 0.00067144
Iteration 11/25 | Loss: 0.00067144
Iteration 12/25 | Loss: 0.00067144
Iteration 13/25 | Loss: 0.00067144
Iteration 14/25 | Loss: 0.00067144
Iteration 15/25 | Loss: 0.00067144
Iteration 16/25 | Loss: 0.00067144
Iteration 17/25 | Loss: 0.00067144
Iteration 18/25 | Loss: 0.00067144
Iteration 19/25 | Loss: 0.00067144
Iteration 20/25 | Loss: 0.00067144
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006714370683766901, 0.0006714370683766901, 0.0006714370683766901, 0.0006714370683766901, 0.0006714370683766901]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006714370683766901

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067144
Iteration 2/1000 | Loss: 0.00003125
Iteration 3/1000 | Loss: 0.00002124
Iteration 4/1000 | Loss: 0.00001883
Iteration 5/1000 | Loss: 0.00001797
Iteration 6/1000 | Loss: 0.00001733
Iteration 7/1000 | Loss: 0.00001681
Iteration 8/1000 | Loss: 0.00001647
Iteration 9/1000 | Loss: 0.00001626
Iteration 10/1000 | Loss: 0.00001606
Iteration 11/1000 | Loss: 0.00001597
Iteration 12/1000 | Loss: 0.00001592
Iteration 13/1000 | Loss: 0.00001591
Iteration 14/1000 | Loss: 0.00001590
Iteration 15/1000 | Loss: 0.00001582
Iteration 16/1000 | Loss: 0.00001582
Iteration 17/1000 | Loss: 0.00001580
Iteration 18/1000 | Loss: 0.00001578
Iteration 19/1000 | Loss: 0.00001577
Iteration 20/1000 | Loss: 0.00001577
Iteration 21/1000 | Loss: 0.00001573
Iteration 22/1000 | Loss: 0.00001572
Iteration 23/1000 | Loss: 0.00001572
Iteration 24/1000 | Loss: 0.00001571
Iteration 25/1000 | Loss: 0.00001570
Iteration 26/1000 | Loss: 0.00001569
Iteration 27/1000 | Loss: 0.00001569
Iteration 28/1000 | Loss: 0.00001569
Iteration 29/1000 | Loss: 0.00001568
Iteration 30/1000 | Loss: 0.00001565
Iteration 31/1000 | Loss: 0.00001565
Iteration 32/1000 | Loss: 0.00001564
Iteration 33/1000 | Loss: 0.00001564
Iteration 34/1000 | Loss: 0.00001564
Iteration 35/1000 | Loss: 0.00001563
Iteration 36/1000 | Loss: 0.00001563
Iteration 37/1000 | Loss: 0.00001562
Iteration 38/1000 | Loss: 0.00001562
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001561
Iteration 41/1000 | Loss: 0.00001561
Iteration 42/1000 | Loss: 0.00001560
Iteration 43/1000 | Loss: 0.00001560
Iteration 44/1000 | Loss: 0.00001560
Iteration 45/1000 | Loss: 0.00001559
Iteration 46/1000 | Loss: 0.00001558
Iteration 47/1000 | Loss: 0.00001558
Iteration 48/1000 | Loss: 0.00001557
Iteration 49/1000 | Loss: 0.00001557
Iteration 50/1000 | Loss: 0.00001556
Iteration 51/1000 | Loss: 0.00001556
Iteration 52/1000 | Loss: 0.00001555
Iteration 53/1000 | Loss: 0.00001555
Iteration 54/1000 | Loss: 0.00001555
Iteration 55/1000 | Loss: 0.00001554
Iteration 56/1000 | Loss: 0.00001554
Iteration 57/1000 | Loss: 0.00001554
Iteration 58/1000 | Loss: 0.00001554
Iteration 59/1000 | Loss: 0.00001554
Iteration 60/1000 | Loss: 0.00001553
Iteration 61/1000 | Loss: 0.00001553
Iteration 62/1000 | Loss: 0.00001553
Iteration 63/1000 | Loss: 0.00001552
Iteration 64/1000 | Loss: 0.00001552
Iteration 65/1000 | Loss: 0.00001552
Iteration 66/1000 | Loss: 0.00001552
Iteration 67/1000 | Loss: 0.00001551
Iteration 68/1000 | Loss: 0.00001551
Iteration 69/1000 | Loss: 0.00001551
Iteration 70/1000 | Loss: 0.00001550
Iteration 71/1000 | Loss: 0.00001549
Iteration 72/1000 | Loss: 0.00001549
Iteration 73/1000 | Loss: 0.00001549
Iteration 74/1000 | Loss: 0.00001549
Iteration 75/1000 | Loss: 0.00001549
Iteration 76/1000 | Loss: 0.00001548
Iteration 77/1000 | Loss: 0.00001548
Iteration 78/1000 | Loss: 0.00001548
Iteration 79/1000 | Loss: 0.00001547
Iteration 80/1000 | Loss: 0.00001546
Iteration 81/1000 | Loss: 0.00001546
Iteration 82/1000 | Loss: 0.00001546
Iteration 83/1000 | Loss: 0.00001545
Iteration 84/1000 | Loss: 0.00001545
Iteration 85/1000 | Loss: 0.00001545
Iteration 86/1000 | Loss: 0.00001545
Iteration 87/1000 | Loss: 0.00001544
Iteration 88/1000 | Loss: 0.00001544
Iteration 89/1000 | Loss: 0.00001544
Iteration 90/1000 | Loss: 0.00001544
Iteration 91/1000 | Loss: 0.00001544
Iteration 92/1000 | Loss: 0.00001543
Iteration 93/1000 | Loss: 0.00001543
Iteration 94/1000 | Loss: 0.00001543
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001542
Iteration 98/1000 | Loss: 0.00001542
Iteration 99/1000 | Loss: 0.00001542
Iteration 100/1000 | Loss: 0.00001541
Iteration 101/1000 | Loss: 0.00001541
Iteration 102/1000 | Loss: 0.00001541
Iteration 103/1000 | Loss: 0.00001540
Iteration 104/1000 | Loss: 0.00001540
Iteration 105/1000 | Loss: 0.00001540
Iteration 106/1000 | Loss: 0.00001540
Iteration 107/1000 | Loss: 0.00001539
Iteration 108/1000 | Loss: 0.00001539
Iteration 109/1000 | Loss: 0.00001539
Iteration 110/1000 | Loss: 0.00001539
Iteration 111/1000 | Loss: 0.00001539
Iteration 112/1000 | Loss: 0.00001539
Iteration 113/1000 | Loss: 0.00001538
Iteration 114/1000 | Loss: 0.00001538
Iteration 115/1000 | Loss: 0.00001538
Iteration 116/1000 | Loss: 0.00001538
Iteration 117/1000 | Loss: 0.00001538
Iteration 118/1000 | Loss: 0.00001537
Iteration 119/1000 | Loss: 0.00001537
Iteration 120/1000 | Loss: 0.00001537
Iteration 121/1000 | Loss: 0.00001537
Iteration 122/1000 | Loss: 0.00001537
Iteration 123/1000 | Loss: 0.00001537
Iteration 124/1000 | Loss: 0.00001537
Iteration 125/1000 | Loss: 0.00001536
Iteration 126/1000 | Loss: 0.00001536
Iteration 127/1000 | Loss: 0.00001535
Iteration 128/1000 | Loss: 0.00001535
Iteration 129/1000 | Loss: 0.00001535
Iteration 130/1000 | Loss: 0.00001535
Iteration 131/1000 | Loss: 0.00001535
Iteration 132/1000 | Loss: 0.00001534
Iteration 133/1000 | Loss: 0.00001534
Iteration 134/1000 | Loss: 0.00001534
Iteration 135/1000 | Loss: 0.00001534
Iteration 136/1000 | Loss: 0.00001533
Iteration 137/1000 | Loss: 0.00001533
Iteration 138/1000 | Loss: 0.00001533
Iteration 139/1000 | Loss: 0.00001533
Iteration 140/1000 | Loss: 0.00001533
Iteration 141/1000 | Loss: 0.00001533
Iteration 142/1000 | Loss: 0.00001533
Iteration 143/1000 | Loss: 0.00001532
Iteration 144/1000 | Loss: 0.00001532
Iteration 145/1000 | Loss: 0.00001532
Iteration 146/1000 | Loss: 0.00001531
Iteration 147/1000 | Loss: 0.00001531
Iteration 148/1000 | Loss: 0.00001531
Iteration 149/1000 | Loss: 0.00001531
Iteration 150/1000 | Loss: 0.00001531
Iteration 151/1000 | Loss: 0.00001530
Iteration 152/1000 | Loss: 0.00001530
Iteration 153/1000 | Loss: 0.00001530
Iteration 154/1000 | Loss: 0.00001530
Iteration 155/1000 | Loss: 0.00001529
Iteration 156/1000 | Loss: 0.00001529
Iteration 157/1000 | Loss: 0.00001529
Iteration 158/1000 | Loss: 0.00001529
Iteration 159/1000 | Loss: 0.00001529
Iteration 160/1000 | Loss: 0.00001529
Iteration 161/1000 | Loss: 0.00001529
Iteration 162/1000 | Loss: 0.00001529
Iteration 163/1000 | Loss: 0.00001529
Iteration 164/1000 | Loss: 0.00001529
Iteration 165/1000 | Loss: 0.00001529
Iteration 166/1000 | Loss: 0.00001528
Iteration 167/1000 | Loss: 0.00001528
Iteration 168/1000 | Loss: 0.00001527
Iteration 169/1000 | Loss: 0.00001527
Iteration 170/1000 | Loss: 0.00001527
Iteration 171/1000 | Loss: 0.00001527
Iteration 172/1000 | Loss: 0.00001527
Iteration 173/1000 | Loss: 0.00001527
Iteration 174/1000 | Loss: 0.00001527
Iteration 175/1000 | Loss: 0.00001527
Iteration 176/1000 | Loss: 0.00001527
Iteration 177/1000 | Loss: 0.00001527
Iteration 178/1000 | Loss: 0.00001527
Iteration 179/1000 | Loss: 0.00001526
Iteration 180/1000 | Loss: 0.00001526
Iteration 181/1000 | Loss: 0.00001526
Iteration 182/1000 | Loss: 0.00001526
Iteration 183/1000 | Loss: 0.00001526
Iteration 184/1000 | Loss: 0.00001526
Iteration 185/1000 | Loss: 0.00001526
Iteration 186/1000 | Loss: 0.00001525
Iteration 187/1000 | Loss: 0.00001525
Iteration 188/1000 | Loss: 0.00001525
Iteration 189/1000 | Loss: 0.00001525
Iteration 190/1000 | Loss: 0.00001525
Iteration 191/1000 | Loss: 0.00001525
Iteration 192/1000 | Loss: 0.00001525
Iteration 193/1000 | Loss: 0.00001525
Iteration 194/1000 | Loss: 0.00001525
Iteration 195/1000 | Loss: 0.00001525
Iteration 196/1000 | Loss: 0.00001525
Iteration 197/1000 | Loss: 0.00001525
Iteration 198/1000 | Loss: 0.00001525
Iteration 199/1000 | Loss: 0.00001525
Iteration 200/1000 | Loss: 0.00001525
Iteration 201/1000 | Loss: 0.00001525
Iteration 202/1000 | Loss: 0.00001525
Iteration 203/1000 | Loss: 0.00001525
Iteration 204/1000 | Loss: 0.00001525
Iteration 205/1000 | Loss: 0.00001525
Iteration 206/1000 | Loss: 0.00001525
Iteration 207/1000 | Loss: 0.00001525
Iteration 208/1000 | Loss: 0.00001525
Iteration 209/1000 | Loss: 0.00001525
Iteration 210/1000 | Loss: 0.00001525
Iteration 211/1000 | Loss: 0.00001525
Iteration 212/1000 | Loss: 0.00001525
Iteration 213/1000 | Loss: 0.00001525
Iteration 214/1000 | Loss: 0.00001525
Iteration 215/1000 | Loss: 0.00001525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.5248735508066602e-05, 1.5248735508066602e-05, 1.5248735508066602e-05, 1.5248735508066602e-05, 1.5248735508066602e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5248735508066602e-05

Optimization complete. Final v2v error: 3.261230945587158 mm

Highest mean error: 4.235497951507568 mm for frame 105

Lowest mean error: 2.9650027751922607 mm for frame 158

Saving results

Total time: 46.18212628364563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802392
Iteration 2/25 | Loss: 0.00135559
Iteration 3/25 | Loss: 0.00120290
Iteration 4/25 | Loss: 0.00118297
Iteration 5/25 | Loss: 0.00117837
Iteration 6/25 | Loss: 0.00117822
Iteration 7/25 | Loss: 0.00117822
Iteration 8/25 | Loss: 0.00117822
Iteration 9/25 | Loss: 0.00117822
Iteration 10/25 | Loss: 0.00117822
Iteration 11/25 | Loss: 0.00117822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001178217469714582, 0.001178217469714582, 0.001178217469714582, 0.001178217469714582, 0.001178217469714582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001178217469714582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45436895
Iteration 2/25 | Loss: 0.00058562
Iteration 3/25 | Loss: 0.00058562
Iteration 4/25 | Loss: 0.00058562
Iteration 5/25 | Loss: 0.00058562
Iteration 6/25 | Loss: 0.00058562
Iteration 7/25 | Loss: 0.00058562
Iteration 8/25 | Loss: 0.00058562
Iteration 9/25 | Loss: 0.00058562
Iteration 10/25 | Loss: 0.00058562
Iteration 11/25 | Loss: 0.00058562
Iteration 12/25 | Loss: 0.00058562
Iteration 13/25 | Loss: 0.00058562
Iteration 14/25 | Loss: 0.00058562
Iteration 15/25 | Loss: 0.00058562
Iteration 16/25 | Loss: 0.00058562
Iteration 17/25 | Loss: 0.00058562
Iteration 18/25 | Loss: 0.00058562
Iteration 19/25 | Loss: 0.00058562
Iteration 20/25 | Loss: 0.00058562
Iteration 21/25 | Loss: 0.00058562
Iteration 22/25 | Loss: 0.00058562
Iteration 23/25 | Loss: 0.00058562
Iteration 24/25 | Loss: 0.00058562
Iteration 25/25 | Loss: 0.00058562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058562
Iteration 2/1000 | Loss: 0.00002787
Iteration 3/1000 | Loss: 0.00001793
Iteration 4/1000 | Loss: 0.00001584
Iteration 5/1000 | Loss: 0.00001464
Iteration 6/1000 | Loss: 0.00001392
Iteration 7/1000 | Loss: 0.00001338
Iteration 8/1000 | Loss: 0.00001303
Iteration 9/1000 | Loss: 0.00001272
Iteration 10/1000 | Loss: 0.00001243
Iteration 11/1000 | Loss: 0.00001236
Iteration 12/1000 | Loss: 0.00001228
Iteration 13/1000 | Loss: 0.00001222
Iteration 14/1000 | Loss: 0.00001218
Iteration 15/1000 | Loss: 0.00001217
Iteration 16/1000 | Loss: 0.00001216
Iteration 17/1000 | Loss: 0.00001213
Iteration 18/1000 | Loss: 0.00001213
Iteration 19/1000 | Loss: 0.00001212
Iteration 20/1000 | Loss: 0.00001211
Iteration 21/1000 | Loss: 0.00001211
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001210
Iteration 24/1000 | Loss: 0.00001210
Iteration 25/1000 | Loss: 0.00001209
Iteration 26/1000 | Loss: 0.00001209
Iteration 27/1000 | Loss: 0.00001207
Iteration 28/1000 | Loss: 0.00001207
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001207
Iteration 31/1000 | Loss: 0.00001206
Iteration 32/1000 | Loss: 0.00001205
Iteration 33/1000 | Loss: 0.00001205
Iteration 34/1000 | Loss: 0.00001204
Iteration 35/1000 | Loss: 0.00001204
Iteration 36/1000 | Loss: 0.00001204
Iteration 37/1000 | Loss: 0.00001204
Iteration 38/1000 | Loss: 0.00001204
Iteration 39/1000 | Loss: 0.00001203
Iteration 40/1000 | Loss: 0.00001203
Iteration 41/1000 | Loss: 0.00001203
Iteration 42/1000 | Loss: 0.00001203
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001202
Iteration 45/1000 | Loss: 0.00001202
Iteration 46/1000 | Loss: 0.00001201
Iteration 47/1000 | Loss: 0.00001201
Iteration 48/1000 | Loss: 0.00001201
Iteration 49/1000 | Loss: 0.00001200
Iteration 50/1000 | Loss: 0.00001200
Iteration 51/1000 | Loss: 0.00001200
Iteration 52/1000 | Loss: 0.00001199
Iteration 53/1000 | Loss: 0.00001199
Iteration 54/1000 | Loss: 0.00001198
Iteration 55/1000 | Loss: 0.00001198
Iteration 56/1000 | Loss: 0.00001198
Iteration 57/1000 | Loss: 0.00001197
Iteration 58/1000 | Loss: 0.00001197
Iteration 59/1000 | Loss: 0.00001197
Iteration 60/1000 | Loss: 0.00001196
Iteration 61/1000 | Loss: 0.00001196
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001195
Iteration 64/1000 | Loss: 0.00001195
Iteration 65/1000 | Loss: 0.00001195
Iteration 66/1000 | Loss: 0.00001195
Iteration 67/1000 | Loss: 0.00001194
Iteration 68/1000 | Loss: 0.00001194
Iteration 69/1000 | Loss: 0.00001194
Iteration 70/1000 | Loss: 0.00001194
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001193
Iteration 74/1000 | Loss: 0.00001193
Iteration 75/1000 | Loss: 0.00001193
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001193
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001193
Iteration 81/1000 | Loss: 0.00001193
Iteration 82/1000 | Loss: 0.00001193
Iteration 83/1000 | Loss: 0.00001193
Iteration 84/1000 | Loss: 0.00001192
Iteration 85/1000 | Loss: 0.00001192
Iteration 86/1000 | Loss: 0.00001192
Iteration 87/1000 | Loss: 0.00001192
Iteration 88/1000 | Loss: 0.00001192
Iteration 89/1000 | Loss: 0.00001192
Iteration 90/1000 | Loss: 0.00001192
Iteration 91/1000 | Loss: 0.00001192
Iteration 92/1000 | Loss: 0.00001192
Iteration 93/1000 | Loss: 0.00001192
Iteration 94/1000 | Loss: 0.00001192
Iteration 95/1000 | Loss: 0.00001191
Iteration 96/1000 | Loss: 0.00001191
Iteration 97/1000 | Loss: 0.00001191
Iteration 98/1000 | Loss: 0.00001191
Iteration 99/1000 | Loss: 0.00001191
Iteration 100/1000 | Loss: 0.00001191
Iteration 101/1000 | Loss: 0.00001190
Iteration 102/1000 | Loss: 0.00001190
Iteration 103/1000 | Loss: 0.00001190
Iteration 104/1000 | Loss: 0.00001190
Iteration 105/1000 | Loss: 0.00001190
Iteration 106/1000 | Loss: 0.00001190
Iteration 107/1000 | Loss: 0.00001190
Iteration 108/1000 | Loss: 0.00001190
Iteration 109/1000 | Loss: 0.00001190
Iteration 110/1000 | Loss: 0.00001190
Iteration 111/1000 | Loss: 0.00001189
Iteration 112/1000 | Loss: 0.00001189
Iteration 113/1000 | Loss: 0.00001189
Iteration 114/1000 | Loss: 0.00001189
Iteration 115/1000 | Loss: 0.00001189
Iteration 116/1000 | Loss: 0.00001189
Iteration 117/1000 | Loss: 0.00001189
Iteration 118/1000 | Loss: 0.00001189
Iteration 119/1000 | Loss: 0.00001189
Iteration 120/1000 | Loss: 0.00001188
Iteration 121/1000 | Loss: 0.00001188
Iteration 122/1000 | Loss: 0.00001188
Iteration 123/1000 | Loss: 0.00001188
Iteration 124/1000 | Loss: 0.00001188
Iteration 125/1000 | Loss: 0.00001188
Iteration 126/1000 | Loss: 0.00001188
Iteration 127/1000 | Loss: 0.00001188
Iteration 128/1000 | Loss: 0.00001188
Iteration 129/1000 | Loss: 0.00001188
Iteration 130/1000 | Loss: 0.00001188
Iteration 131/1000 | Loss: 0.00001188
Iteration 132/1000 | Loss: 0.00001188
Iteration 133/1000 | Loss: 0.00001188
Iteration 134/1000 | Loss: 0.00001188
Iteration 135/1000 | Loss: 0.00001188
Iteration 136/1000 | Loss: 0.00001188
Iteration 137/1000 | Loss: 0.00001188
Iteration 138/1000 | Loss: 0.00001188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.1880581041623373e-05, 1.1880581041623373e-05, 1.1880581041623373e-05, 1.1880581041623373e-05, 1.1880581041623373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1880581041623373e-05

Optimization complete. Final v2v error: 2.9615964889526367 mm

Highest mean error: 3.177791118621826 mm for frame 137

Lowest mean error: 2.8421552181243896 mm for frame 34

Saving results

Total time: 39.84406757354736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889061
Iteration 2/25 | Loss: 0.00142106
Iteration 3/25 | Loss: 0.00130186
Iteration 4/25 | Loss: 0.00128891
Iteration 5/25 | Loss: 0.00128589
Iteration 6/25 | Loss: 0.00128589
Iteration 7/25 | Loss: 0.00128589
Iteration 8/25 | Loss: 0.00128589
Iteration 9/25 | Loss: 0.00128589
Iteration 10/25 | Loss: 0.00128589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001285886624827981, 0.001285886624827981, 0.001285886624827981, 0.001285886624827981, 0.001285886624827981]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001285886624827981

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38737392
Iteration 2/25 | Loss: 0.00063424
Iteration 3/25 | Loss: 0.00063415
Iteration 4/25 | Loss: 0.00063415
Iteration 5/25 | Loss: 0.00063415
Iteration 6/25 | Loss: 0.00063415
Iteration 7/25 | Loss: 0.00063414
Iteration 8/25 | Loss: 0.00063414
Iteration 9/25 | Loss: 0.00063414
Iteration 10/25 | Loss: 0.00063414
Iteration 11/25 | Loss: 0.00063414
Iteration 12/25 | Loss: 0.00063414
Iteration 13/25 | Loss: 0.00063414
Iteration 14/25 | Loss: 0.00063414
Iteration 15/25 | Loss: 0.00063414
Iteration 16/25 | Loss: 0.00063414
Iteration 17/25 | Loss: 0.00063414
Iteration 18/25 | Loss: 0.00063414
Iteration 19/25 | Loss: 0.00063414
Iteration 20/25 | Loss: 0.00063414
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006341438856907189, 0.0006341438856907189, 0.0006341438856907189, 0.0006341438856907189, 0.0006341438856907189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006341438856907189

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063414
Iteration 2/1000 | Loss: 0.00005841
Iteration 3/1000 | Loss: 0.00004307
Iteration 4/1000 | Loss: 0.00004004
Iteration 5/1000 | Loss: 0.00003853
Iteration 6/1000 | Loss: 0.00003768
Iteration 7/1000 | Loss: 0.00003696
Iteration 8/1000 | Loss: 0.00003655
Iteration 9/1000 | Loss: 0.00003613
Iteration 10/1000 | Loss: 0.00003568
Iteration 11/1000 | Loss: 0.00003537
Iteration 12/1000 | Loss: 0.00003510
Iteration 13/1000 | Loss: 0.00003475
Iteration 14/1000 | Loss: 0.00003459
Iteration 15/1000 | Loss: 0.00003457
Iteration 16/1000 | Loss: 0.00003455
Iteration 17/1000 | Loss: 0.00003453
Iteration 18/1000 | Loss: 0.00003445
Iteration 19/1000 | Loss: 0.00003444
Iteration 20/1000 | Loss: 0.00003443
Iteration 21/1000 | Loss: 0.00003443
Iteration 22/1000 | Loss: 0.00003442
Iteration 23/1000 | Loss: 0.00003442
Iteration 24/1000 | Loss: 0.00003441
Iteration 25/1000 | Loss: 0.00003441
Iteration 26/1000 | Loss: 0.00003440
Iteration 27/1000 | Loss: 0.00003440
Iteration 28/1000 | Loss: 0.00003440
Iteration 29/1000 | Loss: 0.00003440
Iteration 30/1000 | Loss: 0.00003440
Iteration 31/1000 | Loss: 0.00003440
Iteration 32/1000 | Loss: 0.00003439
Iteration 33/1000 | Loss: 0.00003439
Iteration 34/1000 | Loss: 0.00003439
Iteration 35/1000 | Loss: 0.00003435
Iteration 36/1000 | Loss: 0.00003434
Iteration 37/1000 | Loss: 0.00003434
Iteration 38/1000 | Loss: 0.00003433
Iteration 39/1000 | Loss: 0.00003433
Iteration 40/1000 | Loss: 0.00003431
Iteration 41/1000 | Loss: 0.00003431
Iteration 42/1000 | Loss: 0.00003431
Iteration 43/1000 | Loss: 0.00003430
Iteration 44/1000 | Loss: 0.00003429
Iteration 45/1000 | Loss: 0.00003427
Iteration 46/1000 | Loss: 0.00003427
Iteration 47/1000 | Loss: 0.00003426
Iteration 48/1000 | Loss: 0.00003426
Iteration 49/1000 | Loss: 0.00003426
Iteration 50/1000 | Loss: 0.00003426
Iteration 51/1000 | Loss: 0.00003425
Iteration 52/1000 | Loss: 0.00003425
Iteration 53/1000 | Loss: 0.00003424
Iteration 54/1000 | Loss: 0.00003424
Iteration 55/1000 | Loss: 0.00003424
Iteration 56/1000 | Loss: 0.00003424
Iteration 57/1000 | Loss: 0.00003424
Iteration 58/1000 | Loss: 0.00003424
Iteration 59/1000 | Loss: 0.00003424
Iteration 60/1000 | Loss: 0.00003424
Iteration 61/1000 | Loss: 0.00003424
Iteration 62/1000 | Loss: 0.00003424
Iteration 63/1000 | Loss: 0.00003423
Iteration 64/1000 | Loss: 0.00003423
Iteration 65/1000 | Loss: 0.00003423
Iteration 66/1000 | Loss: 0.00003423
Iteration 67/1000 | Loss: 0.00003423
Iteration 68/1000 | Loss: 0.00003423
Iteration 69/1000 | Loss: 0.00003423
Iteration 70/1000 | Loss: 0.00003423
Iteration 71/1000 | Loss: 0.00003423
Iteration 72/1000 | Loss: 0.00003423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [3.423299131100066e-05, 3.423299131100066e-05, 3.423299131100066e-05, 3.423299131100066e-05, 3.423299131100066e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.423299131100066e-05

Optimization complete. Final v2v error: 4.729231834411621 mm

Highest mean error: 4.942460060119629 mm for frame 5

Lowest mean error: 4.56063175201416 mm for frame 141

Saving results

Total time: 37.94224953651428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399533
Iteration 2/25 | Loss: 0.00130001
Iteration 3/25 | Loss: 0.00119453
Iteration 4/25 | Loss: 0.00118165
Iteration 5/25 | Loss: 0.00117848
Iteration 6/25 | Loss: 0.00117797
Iteration 7/25 | Loss: 0.00117797
Iteration 8/25 | Loss: 0.00117797
Iteration 9/25 | Loss: 0.00117797
Iteration 10/25 | Loss: 0.00117797
Iteration 11/25 | Loss: 0.00117797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011779684573411942, 0.0011779684573411942, 0.0011779684573411942, 0.0011779684573411942, 0.0011779684573411942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011779684573411942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46378696
Iteration 2/25 | Loss: 0.00064479
Iteration 3/25 | Loss: 0.00064479
Iteration 4/25 | Loss: 0.00064478
Iteration 5/25 | Loss: 0.00064478
Iteration 6/25 | Loss: 0.00064478
Iteration 7/25 | Loss: 0.00064478
Iteration 8/25 | Loss: 0.00064478
Iteration 9/25 | Loss: 0.00064478
Iteration 10/25 | Loss: 0.00064478
Iteration 11/25 | Loss: 0.00064478
Iteration 12/25 | Loss: 0.00064478
Iteration 13/25 | Loss: 0.00064478
Iteration 14/25 | Loss: 0.00064478
Iteration 15/25 | Loss: 0.00064478
Iteration 16/25 | Loss: 0.00064478
Iteration 17/25 | Loss: 0.00064478
Iteration 18/25 | Loss: 0.00064478
Iteration 19/25 | Loss: 0.00064478
Iteration 20/25 | Loss: 0.00064478
Iteration 21/25 | Loss: 0.00064478
Iteration 22/25 | Loss: 0.00064478
Iteration 23/25 | Loss: 0.00064478
Iteration 24/25 | Loss: 0.00064478
Iteration 25/25 | Loss: 0.00064478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064478
Iteration 2/1000 | Loss: 0.00003142
Iteration 3/1000 | Loss: 0.00001670
Iteration 4/1000 | Loss: 0.00001434
Iteration 5/1000 | Loss: 0.00001329
Iteration 6/1000 | Loss: 0.00001259
Iteration 7/1000 | Loss: 0.00001209
Iteration 8/1000 | Loss: 0.00001185
Iteration 9/1000 | Loss: 0.00001181
Iteration 10/1000 | Loss: 0.00001174
Iteration 11/1000 | Loss: 0.00001172
Iteration 12/1000 | Loss: 0.00001170
Iteration 13/1000 | Loss: 0.00001154
Iteration 14/1000 | Loss: 0.00001147
Iteration 15/1000 | Loss: 0.00001145
Iteration 16/1000 | Loss: 0.00001144
Iteration 17/1000 | Loss: 0.00001141
Iteration 18/1000 | Loss: 0.00001140
Iteration 19/1000 | Loss: 0.00001139
Iteration 20/1000 | Loss: 0.00001138
Iteration 21/1000 | Loss: 0.00001137
Iteration 22/1000 | Loss: 0.00001133
Iteration 23/1000 | Loss: 0.00001131
Iteration 24/1000 | Loss: 0.00001128
Iteration 25/1000 | Loss: 0.00001127
Iteration 26/1000 | Loss: 0.00001126
Iteration 27/1000 | Loss: 0.00001126
Iteration 28/1000 | Loss: 0.00001125
Iteration 29/1000 | Loss: 0.00001124
Iteration 30/1000 | Loss: 0.00001124
Iteration 31/1000 | Loss: 0.00001124
Iteration 32/1000 | Loss: 0.00001123
Iteration 33/1000 | Loss: 0.00001123
Iteration 34/1000 | Loss: 0.00001123
Iteration 35/1000 | Loss: 0.00001122
Iteration 36/1000 | Loss: 0.00001122
Iteration 37/1000 | Loss: 0.00001121
Iteration 38/1000 | Loss: 0.00001121
Iteration 39/1000 | Loss: 0.00001121
Iteration 40/1000 | Loss: 0.00001120
Iteration 41/1000 | Loss: 0.00001120
Iteration 42/1000 | Loss: 0.00001120
Iteration 43/1000 | Loss: 0.00001120
Iteration 44/1000 | Loss: 0.00001119
Iteration 45/1000 | Loss: 0.00001119
Iteration 46/1000 | Loss: 0.00001119
Iteration 47/1000 | Loss: 0.00001118
Iteration 48/1000 | Loss: 0.00001118
Iteration 49/1000 | Loss: 0.00001118
Iteration 50/1000 | Loss: 0.00001118
Iteration 51/1000 | Loss: 0.00001117
Iteration 52/1000 | Loss: 0.00001117
Iteration 53/1000 | Loss: 0.00001117
Iteration 54/1000 | Loss: 0.00001116
Iteration 55/1000 | Loss: 0.00001116
Iteration 56/1000 | Loss: 0.00001116
Iteration 57/1000 | Loss: 0.00001116
Iteration 58/1000 | Loss: 0.00001116
Iteration 59/1000 | Loss: 0.00001115
Iteration 60/1000 | Loss: 0.00001115
Iteration 61/1000 | Loss: 0.00001114
Iteration 62/1000 | Loss: 0.00001113
Iteration 63/1000 | Loss: 0.00001112
Iteration 64/1000 | Loss: 0.00001112
Iteration 65/1000 | Loss: 0.00001111
Iteration 66/1000 | Loss: 0.00001110
Iteration 67/1000 | Loss: 0.00001110
Iteration 68/1000 | Loss: 0.00001110
Iteration 69/1000 | Loss: 0.00001110
Iteration 70/1000 | Loss: 0.00001110
Iteration 71/1000 | Loss: 0.00001109
Iteration 72/1000 | Loss: 0.00001109
Iteration 73/1000 | Loss: 0.00001109
Iteration 74/1000 | Loss: 0.00001109
Iteration 75/1000 | Loss: 0.00001109
Iteration 76/1000 | Loss: 0.00001108
Iteration 77/1000 | Loss: 0.00001108
Iteration 78/1000 | Loss: 0.00001108
Iteration 79/1000 | Loss: 0.00001108
Iteration 80/1000 | Loss: 0.00001108
Iteration 81/1000 | Loss: 0.00001108
Iteration 82/1000 | Loss: 0.00001108
Iteration 83/1000 | Loss: 0.00001108
Iteration 84/1000 | Loss: 0.00001108
Iteration 85/1000 | Loss: 0.00001108
Iteration 86/1000 | Loss: 0.00001108
Iteration 87/1000 | Loss: 0.00001108
Iteration 88/1000 | Loss: 0.00001108
Iteration 89/1000 | Loss: 0.00001108
Iteration 90/1000 | Loss: 0.00001108
Iteration 91/1000 | Loss: 0.00001107
Iteration 92/1000 | Loss: 0.00001106
Iteration 93/1000 | Loss: 0.00001106
Iteration 94/1000 | Loss: 0.00001105
Iteration 95/1000 | Loss: 0.00001105
Iteration 96/1000 | Loss: 0.00001105
Iteration 97/1000 | Loss: 0.00001105
Iteration 98/1000 | Loss: 0.00001104
Iteration 99/1000 | Loss: 0.00001104
Iteration 100/1000 | Loss: 0.00001104
Iteration 101/1000 | Loss: 0.00001103
Iteration 102/1000 | Loss: 0.00001103
Iteration 103/1000 | Loss: 0.00001103
Iteration 104/1000 | Loss: 0.00001103
Iteration 105/1000 | Loss: 0.00001102
Iteration 106/1000 | Loss: 0.00001102
Iteration 107/1000 | Loss: 0.00001102
Iteration 108/1000 | Loss: 0.00001100
Iteration 109/1000 | Loss: 0.00001100
Iteration 110/1000 | Loss: 0.00001100
Iteration 111/1000 | Loss: 0.00001100
Iteration 112/1000 | Loss: 0.00001100
Iteration 113/1000 | Loss: 0.00001100
Iteration 114/1000 | Loss: 0.00001100
Iteration 115/1000 | Loss: 0.00001099
Iteration 116/1000 | Loss: 0.00001098
Iteration 117/1000 | Loss: 0.00001098
Iteration 118/1000 | Loss: 0.00001098
Iteration 119/1000 | Loss: 0.00001098
Iteration 120/1000 | Loss: 0.00001098
Iteration 121/1000 | Loss: 0.00001098
Iteration 122/1000 | Loss: 0.00001097
Iteration 123/1000 | Loss: 0.00001097
Iteration 124/1000 | Loss: 0.00001097
Iteration 125/1000 | Loss: 0.00001097
Iteration 126/1000 | Loss: 0.00001097
Iteration 127/1000 | Loss: 0.00001097
Iteration 128/1000 | Loss: 0.00001096
Iteration 129/1000 | Loss: 0.00001096
Iteration 130/1000 | Loss: 0.00001096
Iteration 131/1000 | Loss: 0.00001096
Iteration 132/1000 | Loss: 0.00001096
Iteration 133/1000 | Loss: 0.00001095
Iteration 134/1000 | Loss: 0.00001095
Iteration 135/1000 | Loss: 0.00001095
Iteration 136/1000 | Loss: 0.00001095
Iteration 137/1000 | Loss: 0.00001095
Iteration 138/1000 | Loss: 0.00001095
Iteration 139/1000 | Loss: 0.00001095
Iteration 140/1000 | Loss: 0.00001095
Iteration 141/1000 | Loss: 0.00001095
Iteration 142/1000 | Loss: 0.00001095
Iteration 143/1000 | Loss: 0.00001094
Iteration 144/1000 | Loss: 0.00001094
Iteration 145/1000 | Loss: 0.00001094
Iteration 146/1000 | Loss: 0.00001094
Iteration 147/1000 | Loss: 0.00001094
Iteration 148/1000 | Loss: 0.00001094
Iteration 149/1000 | Loss: 0.00001094
Iteration 150/1000 | Loss: 0.00001094
Iteration 151/1000 | Loss: 0.00001094
Iteration 152/1000 | Loss: 0.00001094
Iteration 153/1000 | Loss: 0.00001094
Iteration 154/1000 | Loss: 0.00001094
Iteration 155/1000 | Loss: 0.00001094
Iteration 156/1000 | Loss: 0.00001094
Iteration 157/1000 | Loss: 0.00001093
Iteration 158/1000 | Loss: 0.00001093
Iteration 159/1000 | Loss: 0.00001093
Iteration 160/1000 | Loss: 0.00001093
Iteration 161/1000 | Loss: 0.00001093
Iteration 162/1000 | Loss: 0.00001093
Iteration 163/1000 | Loss: 0.00001093
Iteration 164/1000 | Loss: 0.00001093
Iteration 165/1000 | Loss: 0.00001093
Iteration 166/1000 | Loss: 0.00001093
Iteration 167/1000 | Loss: 0.00001093
Iteration 168/1000 | Loss: 0.00001092
Iteration 169/1000 | Loss: 0.00001092
Iteration 170/1000 | Loss: 0.00001092
Iteration 171/1000 | Loss: 0.00001092
Iteration 172/1000 | Loss: 0.00001092
Iteration 173/1000 | Loss: 0.00001092
Iteration 174/1000 | Loss: 0.00001092
Iteration 175/1000 | Loss: 0.00001091
Iteration 176/1000 | Loss: 0.00001091
Iteration 177/1000 | Loss: 0.00001091
Iteration 178/1000 | Loss: 0.00001091
Iteration 179/1000 | Loss: 0.00001091
Iteration 180/1000 | Loss: 0.00001091
Iteration 181/1000 | Loss: 0.00001091
Iteration 182/1000 | Loss: 0.00001091
Iteration 183/1000 | Loss: 0.00001091
Iteration 184/1000 | Loss: 0.00001091
Iteration 185/1000 | Loss: 0.00001091
Iteration 186/1000 | Loss: 0.00001091
Iteration 187/1000 | Loss: 0.00001091
Iteration 188/1000 | Loss: 0.00001090
Iteration 189/1000 | Loss: 0.00001090
Iteration 190/1000 | Loss: 0.00001090
Iteration 191/1000 | Loss: 0.00001090
Iteration 192/1000 | Loss: 0.00001090
Iteration 193/1000 | Loss: 0.00001090
Iteration 194/1000 | Loss: 0.00001090
Iteration 195/1000 | Loss: 0.00001090
Iteration 196/1000 | Loss: 0.00001090
Iteration 197/1000 | Loss: 0.00001089
Iteration 198/1000 | Loss: 0.00001089
Iteration 199/1000 | Loss: 0.00001089
Iteration 200/1000 | Loss: 0.00001089
Iteration 201/1000 | Loss: 0.00001089
Iteration 202/1000 | Loss: 0.00001089
Iteration 203/1000 | Loss: 0.00001089
Iteration 204/1000 | Loss: 0.00001089
Iteration 205/1000 | Loss: 0.00001089
Iteration 206/1000 | Loss: 0.00001089
Iteration 207/1000 | Loss: 0.00001089
Iteration 208/1000 | Loss: 0.00001089
Iteration 209/1000 | Loss: 0.00001089
Iteration 210/1000 | Loss: 0.00001089
Iteration 211/1000 | Loss: 0.00001089
Iteration 212/1000 | Loss: 0.00001089
Iteration 213/1000 | Loss: 0.00001089
Iteration 214/1000 | Loss: 0.00001089
Iteration 215/1000 | Loss: 0.00001089
Iteration 216/1000 | Loss: 0.00001089
Iteration 217/1000 | Loss: 0.00001089
Iteration 218/1000 | Loss: 0.00001089
Iteration 219/1000 | Loss: 0.00001089
Iteration 220/1000 | Loss: 0.00001089
Iteration 221/1000 | Loss: 0.00001089
Iteration 222/1000 | Loss: 0.00001089
Iteration 223/1000 | Loss: 0.00001089
Iteration 224/1000 | Loss: 0.00001089
Iteration 225/1000 | Loss: 0.00001089
Iteration 226/1000 | Loss: 0.00001089
Iteration 227/1000 | Loss: 0.00001089
Iteration 228/1000 | Loss: 0.00001089
Iteration 229/1000 | Loss: 0.00001089
Iteration 230/1000 | Loss: 0.00001089
Iteration 231/1000 | Loss: 0.00001089
Iteration 232/1000 | Loss: 0.00001089
Iteration 233/1000 | Loss: 0.00001089
Iteration 234/1000 | Loss: 0.00001089
Iteration 235/1000 | Loss: 0.00001089
Iteration 236/1000 | Loss: 0.00001089
Iteration 237/1000 | Loss: 0.00001089
Iteration 238/1000 | Loss: 0.00001089
Iteration 239/1000 | Loss: 0.00001089
Iteration 240/1000 | Loss: 0.00001089
Iteration 241/1000 | Loss: 0.00001089
Iteration 242/1000 | Loss: 0.00001089
Iteration 243/1000 | Loss: 0.00001089
Iteration 244/1000 | Loss: 0.00001089
Iteration 245/1000 | Loss: 0.00001089
Iteration 246/1000 | Loss: 0.00001089
Iteration 247/1000 | Loss: 0.00001089
Iteration 248/1000 | Loss: 0.00001089
Iteration 249/1000 | Loss: 0.00001089
Iteration 250/1000 | Loss: 0.00001089
Iteration 251/1000 | Loss: 0.00001089
Iteration 252/1000 | Loss: 0.00001089
Iteration 253/1000 | Loss: 0.00001089
Iteration 254/1000 | Loss: 0.00001089
Iteration 255/1000 | Loss: 0.00001089
Iteration 256/1000 | Loss: 0.00001089
Iteration 257/1000 | Loss: 0.00001089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.0888057659030892e-05, 1.0888057659030892e-05, 1.0888057659030892e-05, 1.0888057659030892e-05, 1.0888057659030892e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0888057659030892e-05

Optimization complete. Final v2v error: 2.815946578979492 mm

Highest mean error: 3.6152477264404297 mm for frame 64

Lowest mean error: 2.6507925987243652 mm for frame 91

Saving results

Total time: 40.93249773979187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411015
Iteration 2/25 | Loss: 0.00137513
Iteration 3/25 | Loss: 0.00124298
Iteration 4/25 | Loss: 0.00121973
Iteration 5/25 | Loss: 0.00121258
Iteration 6/25 | Loss: 0.00121045
Iteration 7/25 | Loss: 0.00121001
Iteration 8/25 | Loss: 0.00121001
Iteration 9/25 | Loss: 0.00121001
Iteration 10/25 | Loss: 0.00121001
Iteration 11/25 | Loss: 0.00121001
Iteration 12/25 | Loss: 0.00121001
Iteration 13/25 | Loss: 0.00121001
Iteration 14/25 | Loss: 0.00121001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012100068852305412, 0.0012100068852305412, 0.0012100068852305412, 0.0012100068852305412, 0.0012100068852305412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012100068852305412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42472434
Iteration 2/25 | Loss: 0.00072219
Iteration 3/25 | Loss: 0.00072218
Iteration 4/25 | Loss: 0.00072218
Iteration 5/25 | Loss: 0.00072218
Iteration 6/25 | Loss: 0.00072218
Iteration 7/25 | Loss: 0.00072218
Iteration 8/25 | Loss: 0.00072218
Iteration 9/25 | Loss: 0.00072218
Iteration 10/25 | Loss: 0.00072218
Iteration 11/25 | Loss: 0.00072218
Iteration 12/25 | Loss: 0.00072218
Iteration 13/25 | Loss: 0.00072218
Iteration 14/25 | Loss: 0.00072218
Iteration 15/25 | Loss: 0.00072218
Iteration 16/25 | Loss: 0.00072218
Iteration 17/25 | Loss: 0.00072218
Iteration 18/25 | Loss: 0.00072218
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007221769774332643, 0.0007221769774332643, 0.0007221769774332643, 0.0007221769774332643, 0.0007221769774332643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007221769774332643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072218
Iteration 2/1000 | Loss: 0.00007588
Iteration 3/1000 | Loss: 0.00004209
Iteration 4/1000 | Loss: 0.00003120
Iteration 5/1000 | Loss: 0.00002695
Iteration 6/1000 | Loss: 0.00002493
Iteration 7/1000 | Loss: 0.00002313
Iteration 8/1000 | Loss: 0.00002219
Iteration 9/1000 | Loss: 0.00002143
Iteration 10/1000 | Loss: 0.00002097
Iteration 11/1000 | Loss: 0.00002066
Iteration 12/1000 | Loss: 0.00002052
Iteration 13/1000 | Loss: 0.00002029
Iteration 14/1000 | Loss: 0.00002020
Iteration 15/1000 | Loss: 0.00002000
Iteration 16/1000 | Loss: 0.00001996
Iteration 17/1000 | Loss: 0.00001995
Iteration 18/1000 | Loss: 0.00001994
Iteration 19/1000 | Loss: 0.00001986
Iteration 20/1000 | Loss: 0.00001985
Iteration 21/1000 | Loss: 0.00001983
Iteration 22/1000 | Loss: 0.00001981
Iteration 23/1000 | Loss: 0.00001977
Iteration 24/1000 | Loss: 0.00001975
Iteration 25/1000 | Loss: 0.00001974
Iteration 26/1000 | Loss: 0.00001973
Iteration 27/1000 | Loss: 0.00001973
Iteration 28/1000 | Loss: 0.00001972
Iteration 29/1000 | Loss: 0.00001972
Iteration 30/1000 | Loss: 0.00001971
Iteration 31/1000 | Loss: 0.00001971
Iteration 32/1000 | Loss: 0.00001971
Iteration 33/1000 | Loss: 0.00001970
Iteration 34/1000 | Loss: 0.00001970
Iteration 35/1000 | Loss: 0.00001969
Iteration 36/1000 | Loss: 0.00001969
Iteration 37/1000 | Loss: 0.00001969
Iteration 38/1000 | Loss: 0.00001968
Iteration 39/1000 | Loss: 0.00001968
Iteration 40/1000 | Loss: 0.00001967
Iteration 41/1000 | Loss: 0.00001966
Iteration 42/1000 | Loss: 0.00001966
Iteration 43/1000 | Loss: 0.00001966
Iteration 44/1000 | Loss: 0.00001965
Iteration 45/1000 | Loss: 0.00001964
Iteration 46/1000 | Loss: 0.00001964
Iteration 47/1000 | Loss: 0.00001964
Iteration 48/1000 | Loss: 0.00001964
Iteration 49/1000 | Loss: 0.00001964
Iteration 50/1000 | Loss: 0.00001964
Iteration 51/1000 | Loss: 0.00001964
Iteration 52/1000 | Loss: 0.00001964
Iteration 53/1000 | Loss: 0.00001963
Iteration 54/1000 | Loss: 0.00001963
Iteration 55/1000 | Loss: 0.00001963
Iteration 56/1000 | Loss: 0.00001962
Iteration 57/1000 | Loss: 0.00001962
Iteration 58/1000 | Loss: 0.00001962
Iteration 59/1000 | Loss: 0.00001961
Iteration 60/1000 | Loss: 0.00001961
Iteration 61/1000 | Loss: 0.00001961
Iteration 62/1000 | Loss: 0.00001961
Iteration 63/1000 | Loss: 0.00001960
Iteration 64/1000 | Loss: 0.00001960
Iteration 65/1000 | Loss: 0.00001960
Iteration 66/1000 | Loss: 0.00001960
Iteration 67/1000 | Loss: 0.00001959
Iteration 68/1000 | Loss: 0.00001959
Iteration 69/1000 | Loss: 0.00001959
Iteration 70/1000 | Loss: 0.00001959
Iteration 71/1000 | Loss: 0.00001959
Iteration 72/1000 | Loss: 0.00001959
Iteration 73/1000 | Loss: 0.00001958
Iteration 74/1000 | Loss: 0.00001958
Iteration 75/1000 | Loss: 0.00001958
Iteration 76/1000 | Loss: 0.00001958
Iteration 77/1000 | Loss: 0.00001957
Iteration 78/1000 | Loss: 0.00001957
Iteration 79/1000 | Loss: 0.00001957
Iteration 80/1000 | Loss: 0.00001957
Iteration 81/1000 | Loss: 0.00001957
Iteration 82/1000 | Loss: 0.00001956
Iteration 83/1000 | Loss: 0.00001956
Iteration 84/1000 | Loss: 0.00001956
Iteration 85/1000 | Loss: 0.00001956
Iteration 86/1000 | Loss: 0.00001956
Iteration 87/1000 | Loss: 0.00001956
Iteration 88/1000 | Loss: 0.00001955
Iteration 89/1000 | Loss: 0.00001955
Iteration 90/1000 | Loss: 0.00001955
Iteration 91/1000 | Loss: 0.00001955
Iteration 92/1000 | Loss: 0.00001955
Iteration 93/1000 | Loss: 0.00001955
Iteration 94/1000 | Loss: 0.00001954
Iteration 95/1000 | Loss: 0.00001954
Iteration 96/1000 | Loss: 0.00001954
Iteration 97/1000 | Loss: 0.00001954
Iteration 98/1000 | Loss: 0.00001954
Iteration 99/1000 | Loss: 0.00001954
Iteration 100/1000 | Loss: 0.00001954
Iteration 101/1000 | Loss: 0.00001954
Iteration 102/1000 | Loss: 0.00001954
Iteration 103/1000 | Loss: 0.00001953
Iteration 104/1000 | Loss: 0.00001953
Iteration 105/1000 | Loss: 0.00001953
Iteration 106/1000 | Loss: 0.00001953
Iteration 107/1000 | Loss: 0.00001953
Iteration 108/1000 | Loss: 0.00001953
Iteration 109/1000 | Loss: 0.00001953
Iteration 110/1000 | Loss: 0.00001953
Iteration 111/1000 | Loss: 0.00001953
Iteration 112/1000 | Loss: 0.00001953
Iteration 113/1000 | Loss: 0.00001953
Iteration 114/1000 | Loss: 0.00001953
Iteration 115/1000 | Loss: 0.00001952
Iteration 116/1000 | Loss: 0.00001952
Iteration 117/1000 | Loss: 0.00001952
Iteration 118/1000 | Loss: 0.00001952
Iteration 119/1000 | Loss: 0.00001952
Iteration 120/1000 | Loss: 0.00001952
Iteration 121/1000 | Loss: 0.00001952
Iteration 122/1000 | Loss: 0.00001952
Iteration 123/1000 | Loss: 0.00001952
Iteration 124/1000 | Loss: 0.00001952
Iteration 125/1000 | Loss: 0.00001952
Iteration 126/1000 | Loss: 0.00001951
Iteration 127/1000 | Loss: 0.00001951
Iteration 128/1000 | Loss: 0.00001951
Iteration 129/1000 | Loss: 0.00001951
Iteration 130/1000 | Loss: 0.00001950
Iteration 131/1000 | Loss: 0.00001950
Iteration 132/1000 | Loss: 0.00001950
Iteration 133/1000 | Loss: 0.00001950
Iteration 134/1000 | Loss: 0.00001950
Iteration 135/1000 | Loss: 0.00001950
Iteration 136/1000 | Loss: 0.00001949
Iteration 137/1000 | Loss: 0.00001949
Iteration 138/1000 | Loss: 0.00001949
Iteration 139/1000 | Loss: 0.00001949
Iteration 140/1000 | Loss: 0.00001949
Iteration 141/1000 | Loss: 0.00001948
Iteration 142/1000 | Loss: 0.00001948
Iteration 143/1000 | Loss: 0.00001948
Iteration 144/1000 | Loss: 0.00001948
Iteration 145/1000 | Loss: 0.00001948
Iteration 146/1000 | Loss: 0.00001948
Iteration 147/1000 | Loss: 0.00001948
Iteration 148/1000 | Loss: 0.00001948
Iteration 149/1000 | Loss: 0.00001948
Iteration 150/1000 | Loss: 0.00001947
Iteration 151/1000 | Loss: 0.00001947
Iteration 152/1000 | Loss: 0.00001947
Iteration 153/1000 | Loss: 0.00001947
Iteration 154/1000 | Loss: 0.00001947
Iteration 155/1000 | Loss: 0.00001947
Iteration 156/1000 | Loss: 0.00001946
Iteration 157/1000 | Loss: 0.00001946
Iteration 158/1000 | Loss: 0.00001946
Iteration 159/1000 | Loss: 0.00001946
Iteration 160/1000 | Loss: 0.00001946
Iteration 161/1000 | Loss: 0.00001945
Iteration 162/1000 | Loss: 0.00001945
Iteration 163/1000 | Loss: 0.00001945
Iteration 164/1000 | Loss: 0.00001945
Iteration 165/1000 | Loss: 0.00001945
Iteration 166/1000 | Loss: 0.00001945
Iteration 167/1000 | Loss: 0.00001945
Iteration 168/1000 | Loss: 0.00001945
Iteration 169/1000 | Loss: 0.00001945
Iteration 170/1000 | Loss: 0.00001945
Iteration 171/1000 | Loss: 0.00001945
Iteration 172/1000 | Loss: 0.00001945
Iteration 173/1000 | Loss: 0.00001944
Iteration 174/1000 | Loss: 0.00001944
Iteration 175/1000 | Loss: 0.00001944
Iteration 176/1000 | Loss: 0.00001944
Iteration 177/1000 | Loss: 0.00001944
Iteration 178/1000 | Loss: 0.00001944
Iteration 179/1000 | Loss: 0.00001944
Iteration 180/1000 | Loss: 0.00001944
Iteration 181/1000 | Loss: 0.00001944
Iteration 182/1000 | Loss: 0.00001944
Iteration 183/1000 | Loss: 0.00001943
Iteration 184/1000 | Loss: 0.00001943
Iteration 185/1000 | Loss: 0.00001943
Iteration 186/1000 | Loss: 0.00001943
Iteration 187/1000 | Loss: 0.00001943
Iteration 188/1000 | Loss: 0.00001943
Iteration 189/1000 | Loss: 0.00001943
Iteration 190/1000 | Loss: 0.00001943
Iteration 191/1000 | Loss: 0.00001942
Iteration 192/1000 | Loss: 0.00001942
Iteration 193/1000 | Loss: 0.00001942
Iteration 194/1000 | Loss: 0.00001942
Iteration 195/1000 | Loss: 0.00001942
Iteration 196/1000 | Loss: 0.00001942
Iteration 197/1000 | Loss: 0.00001942
Iteration 198/1000 | Loss: 0.00001942
Iteration 199/1000 | Loss: 0.00001942
Iteration 200/1000 | Loss: 0.00001942
Iteration 201/1000 | Loss: 0.00001942
Iteration 202/1000 | Loss: 0.00001942
Iteration 203/1000 | Loss: 0.00001942
Iteration 204/1000 | Loss: 0.00001942
Iteration 205/1000 | Loss: 0.00001942
Iteration 206/1000 | Loss: 0.00001942
Iteration 207/1000 | Loss: 0.00001942
Iteration 208/1000 | Loss: 0.00001942
Iteration 209/1000 | Loss: 0.00001942
Iteration 210/1000 | Loss: 0.00001942
Iteration 211/1000 | Loss: 0.00001942
Iteration 212/1000 | Loss: 0.00001942
Iteration 213/1000 | Loss: 0.00001942
Iteration 214/1000 | Loss: 0.00001942
Iteration 215/1000 | Loss: 0.00001942
Iteration 216/1000 | Loss: 0.00001942
Iteration 217/1000 | Loss: 0.00001942
Iteration 218/1000 | Loss: 0.00001942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.941623122547753e-05, 1.941623122547753e-05, 1.941623122547753e-05, 1.941623122547753e-05, 1.941623122547753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.941623122547753e-05

Optimization complete. Final v2v error: 3.6567506790161133 mm

Highest mean error: 4.610671520233154 mm for frame 97

Lowest mean error: 2.79797625541687 mm for frame 105

Saving results

Total time: 44.05956292152405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033203
Iteration 2/25 | Loss: 0.00183122
Iteration 3/25 | Loss: 0.00146843
Iteration 4/25 | Loss: 0.00150524
Iteration 5/25 | Loss: 0.00142276
Iteration 6/25 | Loss: 0.00132208
Iteration 7/25 | Loss: 0.00124291
Iteration 8/25 | Loss: 0.00133586
Iteration 9/25 | Loss: 0.00123420
Iteration 10/25 | Loss: 0.00128956
Iteration 11/25 | Loss: 0.00128761
Iteration 12/25 | Loss: 0.00123409
Iteration 13/25 | Loss: 0.00121560
Iteration 14/25 | Loss: 0.00125040
Iteration 15/25 | Loss: 0.00120400
Iteration 16/25 | Loss: 0.00120237
Iteration 17/25 | Loss: 0.00120228
Iteration 18/25 | Loss: 0.00120228
Iteration 19/25 | Loss: 0.00120227
Iteration 20/25 | Loss: 0.00120226
Iteration 21/25 | Loss: 0.00120226
Iteration 22/25 | Loss: 0.00120226
Iteration 23/25 | Loss: 0.00120226
Iteration 24/25 | Loss: 0.00120226
Iteration 25/25 | Loss: 0.00120225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62514186
Iteration 2/25 | Loss: 0.00071173
Iteration 3/25 | Loss: 0.00071173
Iteration 4/25 | Loss: 0.00071173
Iteration 5/25 | Loss: 0.00071173
Iteration 6/25 | Loss: 0.00071173
Iteration 7/25 | Loss: 0.00071173
Iteration 8/25 | Loss: 0.00071173
Iteration 9/25 | Loss: 0.00071173
Iteration 10/25 | Loss: 0.00071173
Iteration 11/25 | Loss: 0.00071173
Iteration 12/25 | Loss: 0.00071173
Iteration 13/25 | Loss: 0.00071173
Iteration 14/25 | Loss: 0.00071173
Iteration 15/25 | Loss: 0.00071173
Iteration 16/25 | Loss: 0.00071173
Iteration 17/25 | Loss: 0.00071173
Iteration 18/25 | Loss: 0.00071173
Iteration 19/25 | Loss: 0.00071173
Iteration 20/25 | Loss: 0.00071173
Iteration 21/25 | Loss: 0.00071173
Iteration 22/25 | Loss: 0.00071173
Iteration 23/25 | Loss: 0.00071173
Iteration 24/25 | Loss: 0.00071173
Iteration 25/25 | Loss: 0.00071173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071173
Iteration 2/1000 | Loss: 0.00002366
Iteration 3/1000 | Loss: 0.00001763
Iteration 4/1000 | Loss: 0.00003378
Iteration 5/1000 | Loss: 0.00033298
Iteration 6/1000 | Loss: 0.00041796
Iteration 7/1000 | Loss: 0.00002093
Iteration 8/1000 | Loss: 0.00001484
Iteration 9/1000 | Loss: 0.00001450
Iteration 10/1000 | Loss: 0.00001422
Iteration 11/1000 | Loss: 0.00030304
Iteration 12/1000 | Loss: 0.00011370
Iteration 13/1000 | Loss: 0.00001745
Iteration 14/1000 | Loss: 0.00001419
Iteration 15/1000 | Loss: 0.00001391
Iteration 16/1000 | Loss: 0.00001387
Iteration 17/1000 | Loss: 0.00001387
Iteration 18/1000 | Loss: 0.00029499
Iteration 19/1000 | Loss: 0.00003958
Iteration 20/1000 | Loss: 0.00001403
Iteration 21/1000 | Loss: 0.00001592
Iteration 22/1000 | Loss: 0.00001371
Iteration 23/1000 | Loss: 0.00001371
Iteration 24/1000 | Loss: 0.00001371
Iteration 25/1000 | Loss: 0.00001371
Iteration 26/1000 | Loss: 0.00001371
Iteration 27/1000 | Loss: 0.00001371
Iteration 28/1000 | Loss: 0.00001371
Iteration 29/1000 | Loss: 0.00001371
Iteration 30/1000 | Loss: 0.00001371
Iteration 31/1000 | Loss: 0.00001371
Iteration 32/1000 | Loss: 0.00001371
Iteration 33/1000 | Loss: 0.00001369
Iteration 34/1000 | Loss: 0.00001366
Iteration 35/1000 | Loss: 0.00001366
Iteration 36/1000 | Loss: 0.00001365
Iteration 37/1000 | Loss: 0.00001365
Iteration 38/1000 | Loss: 0.00001365
Iteration 39/1000 | Loss: 0.00001365
Iteration 40/1000 | Loss: 0.00001364
Iteration 41/1000 | Loss: 0.00001364
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001360
Iteration 45/1000 | Loss: 0.00001359
Iteration 46/1000 | Loss: 0.00001359
Iteration 47/1000 | Loss: 0.00001358
Iteration 48/1000 | Loss: 0.00001358
Iteration 49/1000 | Loss: 0.00001358
Iteration 50/1000 | Loss: 0.00001357
Iteration 51/1000 | Loss: 0.00001357
Iteration 52/1000 | Loss: 0.00001357
Iteration 53/1000 | Loss: 0.00001357
Iteration 54/1000 | Loss: 0.00001357
Iteration 55/1000 | Loss: 0.00001356
Iteration 56/1000 | Loss: 0.00001356
Iteration 57/1000 | Loss: 0.00001355
Iteration 58/1000 | Loss: 0.00001355
Iteration 59/1000 | Loss: 0.00001355
Iteration 60/1000 | Loss: 0.00001355
Iteration 61/1000 | Loss: 0.00001355
Iteration 62/1000 | Loss: 0.00001355
Iteration 63/1000 | Loss: 0.00001355
Iteration 64/1000 | Loss: 0.00001355
Iteration 65/1000 | Loss: 0.00001355
Iteration 66/1000 | Loss: 0.00001355
Iteration 67/1000 | Loss: 0.00001354
Iteration 68/1000 | Loss: 0.00001354
Iteration 69/1000 | Loss: 0.00001354
Iteration 70/1000 | Loss: 0.00001354
Iteration 71/1000 | Loss: 0.00001354
Iteration 72/1000 | Loss: 0.00001354
Iteration 73/1000 | Loss: 0.00001354
Iteration 74/1000 | Loss: 0.00001354
Iteration 75/1000 | Loss: 0.00001354
Iteration 76/1000 | Loss: 0.00001353
Iteration 77/1000 | Loss: 0.00001353
Iteration 78/1000 | Loss: 0.00001353
Iteration 79/1000 | Loss: 0.00001353
Iteration 80/1000 | Loss: 0.00001353
Iteration 81/1000 | Loss: 0.00001353
Iteration 82/1000 | Loss: 0.00001353
Iteration 83/1000 | Loss: 0.00001353
Iteration 84/1000 | Loss: 0.00001353
Iteration 85/1000 | Loss: 0.00001353
Iteration 86/1000 | Loss: 0.00001353
Iteration 87/1000 | Loss: 0.00001353
Iteration 88/1000 | Loss: 0.00001353
Iteration 89/1000 | Loss: 0.00001353
Iteration 90/1000 | Loss: 0.00001352
Iteration 91/1000 | Loss: 0.00001352
Iteration 92/1000 | Loss: 0.00001352
Iteration 93/1000 | Loss: 0.00001352
Iteration 94/1000 | Loss: 0.00001352
Iteration 95/1000 | Loss: 0.00001352
Iteration 96/1000 | Loss: 0.00001352
Iteration 97/1000 | Loss: 0.00001352
Iteration 98/1000 | Loss: 0.00001352
Iteration 99/1000 | Loss: 0.00001352
Iteration 100/1000 | Loss: 0.00001352
Iteration 101/1000 | Loss: 0.00001352
Iteration 102/1000 | Loss: 0.00001352
Iteration 103/1000 | Loss: 0.00001352
Iteration 104/1000 | Loss: 0.00001351
Iteration 105/1000 | Loss: 0.00001351
Iteration 106/1000 | Loss: 0.00001351
Iteration 107/1000 | Loss: 0.00001351
Iteration 108/1000 | Loss: 0.00001351
Iteration 109/1000 | Loss: 0.00001350
Iteration 110/1000 | Loss: 0.00001350
Iteration 111/1000 | Loss: 0.00001349
Iteration 112/1000 | Loss: 0.00001349
Iteration 113/1000 | Loss: 0.00001349
Iteration 114/1000 | Loss: 0.00001349
Iteration 115/1000 | Loss: 0.00001348
Iteration 116/1000 | Loss: 0.00001348
Iteration 117/1000 | Loss: 0.00001348
Iteration 118/1000 | Loss: 0.00001348
Iteration 119/1000 | Loss: 0.00001348
Iteration 120/1000 | Loss: 0.00001348
Iteration 121/1000 | Loss: 0.00001348
Iteration 122/1000 | Loss: 0.00001348
Iteration 123/1000 | Loss: 0.00001347
Iteration 124/1000 | Loss: 0.00001347
Iteration 125/1000 | Loss: 0.00001347
Iteration 126/1000 | Loss: 0.00001347
Iteration 127/1000 | Loss: 0.00001346
Iteration 128/1000 | Loss: 0.00001346
Iteration 129/1000 | Loss: 0.00001346
Iteration 130/1000 | Loss: 0.00001346
Iteration 131/1000 | Loss: 0.00001346
Iteration 132/1000 | Loss: 0.00001345
Iteration 133/1000 | Loss: 0.00001345
Iteration 134/1000 | Loss: 0.00001345
Iteration 135/1000 | Loss: 0.00001345
Iteration 136/1000 | Loss: 0.00001345
Iteration 137/1000 | Loss: 0.00001345
Iteration 138/1000 | Loss: 0.00001345
Iteration 139/1000 | Loss: 0.00001345
Iteration 140/1000 | Loss: 0.00001344
Iteration 141/1000 | Loss: 0.00001344
Iteration 142/1000 | Loss: 0.00001344
Iteration 143/1000 | Loss: 0.00001344
Iteration 144/1000 | Loss: 0.00001344
Iteration 145/1000 | Loss: 0.00001344
Iteration 146/1000 | Loss: 0.00001344
Iteration 147/1000 | Loss: 0.00001343
Iteration 148/1000 | Loss: 0.00001343
Iteration 149/1000 | Loss: 0.00001343
Iteration 150/1000 | Loss: 0.00001343
Iteration 151/1000 | Loss: 0.00001343
Iteration 152/1000 | Loss: 0.00001343
Iteration 153/1000 | Loss: 0.00001342
Iteration 154/1000 | Loss: 0.00001342
Iteration 155/1000 | Loss: 0.00001342
Iteration 156/1000 | Loss: 0.00001341
Iteration 157/1000 | Loss: 0.00001341
Iteration 158/1000 | Loss: 0.00001341
Iteration 159/1000 | Loss: 0.00001341
Iteration 160/1000 | Loss: 0.00001340
Iteration 161/1000 | Loss: 0.00001340
Iteration 162/1000 | Loss: 0.00001340
Iteration 163/1000 | Loss: 0.00001339
Iteration 164/1000 | Loss: 0.00001339
Iteration 165/1000 | Loss: 0.00001339
Iteration 166/1000 | Loss: 0.00001339
Iteration 167/1000 | Loss: 0.00001339
Iteration 168/1000 | Loss: 0.00001339
Iteration 169/1000 | Loss: 0.00001339
Iteration 170/1000 | Loss: 0.00001339
Iteration 171/1000 | Loss: 0.00001339
Iteration 172/1000 | Loss: 0.00001339
Iteration 173/1000 | Loss: 0.00001338
Iteration 174/1000 | Loss: 0.00001338
Iteration 175/1000 | Loss: 0.00001338
Iteration 176/1000 | Loss: 0.00001337
Iteration 177/1000 | Loss: 0.00001337
Iteration 178/1000 | Loss: 0.00001337
Iteration 179/1000 | Loss: 0.00001337
Iteration 180/1000 | Loss: 0.00001337
Iteration 181/1000 | Loss: 0.00001336
Iteration 182/1000 | Loss: 0.00001336
Iteration 183/1000 | Loss: 0.00001336
Iteration 184/1000 | Loss: 0.00001336
Iteration 185/1000 | Loss: 0.00001336
Iteration 186/1000 | Loss: 0.00001336
Iteration 187/1000 | Loss: 0.00001336
Iteration 188/1000 | Loss: 0.00001336
Iteration 189/1000 | Loss: 0.00001335
Iteration 190/1000 | Loss: 0.00004539
Iteration 191/1000 | Loss: 0.00001341
Iteration 192/1000 | Loss: 0.00001334
Iteration 193/1000 | Loss: 0.00001334
Iteration 194/1000 | Loss: 0.00001334
Iteration 195/1000 | Loss: 0.00001334
Iteration 196/1000 | Loss: 0.00001334
Iteration 197/1000 | Loss: 0.00001333
Iteration 198/1000 | Loss: 0.00001333
Iteration 199/1000 | Loss: 0.00001333
Iteration 200/1000 | Loss: 0.00001333
Iteration 201/1000 | Loss: 0.00001333
Iteration 202/1000 | Loss: 0.00001333
Iteration 203/1000 | Loss: 0.00001333
Iteration 204/1000 | Loss: 0.00001333
Iteration 205/1000 | Loss: 0.00001333
Iteration 206/1000 | Loss: 0.00001332
Iteration 207/1000 | Loss: 0.00001332
Iteration 208/1000 | Loss: 0.00001332
Iteration 209/1000 | Loss: 0.00001332
Iteration 210/1000 | Loss: 0.00001332
Iteration 211/1000 | Loss: 0.00001332
Iteration 212/1000 | Loss: 0.00001332
Iteration 213/1000 | Loss: 0.00001332
Iteration 214/1000 | Loss: 0.00001332
Iteration 215/1000 | Loss: 0.00001332
Iteration 216/1000 | Loss: 0.00001332
Iteration 217/1000 | Loss: 0.00001332
Iteration 218/1000 | Loss: 0.00001332
Iteration 219/1000 | Loss: 0.00001332
Iteration 220/1000 | Loss: 0.00001332
Iteration 221/1000 | Loss: 0.00001332
Iteration 222/1000 | Loss: 0.00001332
Iteration 223/1000 | Loss: 0.00001332
Iteration 224/1000 | Loss: 0.00001332
Iteration 225/1000 | Loss: 0.00001332
Iteration 226/1000 | Loss: 0.00001332
Iteration 227/1000 | Loss: 0.00001332
Iteration 228/1000 | Loss: 0.00001332
Iteration 229/1000 | Loss: 0.00001332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.3316707736521494e-05, 1.3316707736521494e-05, 1.3316707736521494e-05, 1.3316707736521494e-05, 1.3316707736521494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3316707736521494e-05

Optimization complete. Final v2v error: 3.0541470050811768 mm

Highest mean error: 3.665701150894165 mm for frame 60

Lowest mean error: 2.811858654022217 mm for frame 138

Saving results

Total time: 71.4873538017273
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01119093
Iteration 2/25 | Loss: 0.00192255
Iteration 3/25 | Loss: 0.00142575
Iteration 4/25 | Loss: 0.00138530
Iteration 5/25 | Loss: 0.00137800
Iteration 6/25 | Loss: 0.00137694
Iteration 7/25 | Loss: 0.00137694
Iteration 8/25 | Loss: 0.00137694
Iteration 9/25 | Loss: 0.00137694
Iteration 10/25 | Loss: 0.00137694
Iteration 11/25 | Loss: 0.00137694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013769351644441485, 0.0013769351644441485, 0.0013769351644441485, 0.0013769351644441485, 0.0013769351644441485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013769351644441485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98950887
Iteration 2/25 | Loss: 0.00083412
Iteration 3/25 | Loss: 0.00083408
Iteration 4/25 | Loss: 0.00083408
Iteration 5/25 | Loss: 0.00083408
Iteration 6/25 | Loss: 0.00083408
Iteration 7/25 | Loss: 0.00083408
Iteration 8/25 | Loss: 0.00083408
Iteration 9/25 | Loss: 0.00083408
Iteration 10/25 | Loss: 0.00083408
Iteration 11/25 | Loss: 0.00083408
Iteration 12/25 | Loss: 0.00083408
Iteration 13/25 | Loss: 0.00083408
Iteration 14/25 | Loss: 0.00083408
Iteration 15/25 | Loss: 0.00083408
Iteration 16/25 | Loss: 0.00083408
Iteration 17/25 | Loss: 0.00083408
Iteration 18/25 | Loss: 0.00083408
Iteration 19/25 | Loss: 0.00083408
Iteration 20/25 | Loss: 0.00083408
Iteration 21/25 | Loss: 0.00083408
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008340754429809749, 0.0008340754429809749, 0.0008340754429809749, 0.0008340754429809749, 0.0008340754429809749]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008340754429809749

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083408
Iteration 2/1000 | Loss: 0.00008151
Iteration 3/1000 | Loss: 0.00004592
Iteration 4/1000 | Loss: 0.00003747
Iteration 5/1000 | Loss: 0.00003513
Iteration 6/1000 | Loss: 0.00003344
Iteration 7/1000 | Loss: 0.00003232
Iteration 8/1000 | Loss: 0.00003175
Iteration 9/1000 | Loss: 0.00003122
Iteration 10/1000 | Loss: 0.00003073
Iteration 11/1000 | Loss: 0.00003045
Iteration 12/1000 | Loss: 0.00003019
Iteration 13/1000 | Loss: 0.00002995
Iteration 14/1000 | Loss: 0.00002971
Iteration 15/1000 | Loss: 0.00002956
Iteration 16/1000 | Loss: 0.00002953
Iteration 17/1000 | Loss: 0.00002948
Iteration 18/1000 | Loss: 0.00002937
Iteration 19/1000 | Loss: 0.00002936
Iteration 20/1000 | Loss: 0.00002934
Iteration 21/1000 | Loss: 0.00002928
Iteration 22/1000 | Loss: 0.00002921
Iteration 23/1000 | Loss: 0.00002915
Iteration 24/1000 | Loss: 0.00002913
Iteration 25/1000 | Loss: 0.00002910
Iteration 26/1000 | Loss: 0.00002909
Iteration 27/1000 | Loss: 0.00002909
Iteration 28/1000 | Loss: 0.00002907
Iteration 29/1000 | Loss: 0.00002903
Iteration 30/1000 | Loss: 0.00002900
Iteration 31/1000 | Loss: 0.00002898
Iteration 32/1000 | Loss: 0.00002897
Iteration 33/1000 | Loss: 0.00002897
Iteration 34/1000 | Loss: 0.00002895
Iteration 35/1000 | Loss: 0.00002895
Iteration 36/1000 | Loss: 0.00002895
Iteration 37/1000 | Loss: 0.00002895
Iteration 38/1000 | Loss: 0.00002895
Iteration 39/1000 | Loss: 0.00002895
Iteration 40/1000 | Loss: 0.00002894
Iteration 41/1000 | Loss: 0.00002894
Iteration 42/1000 | Loss: 0.00002894
Iteration 43/1000 | Loss: 0.00002894
Iteration 44/1000 | Loss: 0.00002894
Iteration 45/1000 | Loss: 0.00002893
Iteration 46/1000 | Loss: 0.00002892
Iteration 47/1000 | Loss: 0.00002892
Iteration 48/1000 | Loss: 0.00002892
Iteration 49/1000 | Loss: 0.00002891
Iteration 50/1000 | Loss: 0.00002891
Iteration 51/1000 | Loss: 0.00002890
Iteration 52/1000 | Loss: 0.00002890
Iteration 53/1000 | Loss: 0.00002890
Iteration 54/1000 | Loss: 0.00002890
Iteration 55/1000 | Loss: 0.00002890
Iteration 56/1000 | Loss: 0.00002890
Iteration 57/1000 | Loss: 0.00002890
Iteration 58/1000 | Loss: 0.00002890
Iteration 59/1000 | Loss: 0.00002889
Iteration 60/1000 | Loss: 0.00002888
Iteration 61/1000 | Loss: 0.00002888
Iteration 62/1000 | Loss: 0.00002888
Iteration 63/1000 | Loss: 0.00002888
Iteration 64/1000 | Loss: 0.00002888
Iteration 65/1000 | Loss: 0.00002888
Iteration 66/1000 | Loss: 0.00002888
Iteration 67/1000 | Loss: 0.00002888
Iteration 68/1000 | Loss: 0.00002888
Iteration 69/1000 | Loss: 0.00002888
Iteration 70/1000 | Loss: 0.00002888
Iteration 71/1000 | Loss: 0.00002888
Iteration 72/1000 | Loss: 0.00002887
Iteration 73/1000 | Loss: 0.00002887
Iteration 74/1000 | Loss: 0.00002886
Iteration 75/1000 | Loss: 0.00002886
Iteration 76/1000 | Loss: 0.00002886
Iteration 77/1000 | Loss: 0.00002885
Iteration 78/1000 | Loss: 0.00002885
Iteration 79/1000 | Loss: 0.00002885
Iteration 80/1000 | Loss: 0.00002885
Iteration 81/1000 | Loss: 0.00002885
Iteration 82/1000 | Loss: 0.00002885
Iteration 83/1000 | Loss: 0.00002885
Iteration 84/1000 | Loss: 0.00002885
Iteration 85/1000 | Loss: 0.00002884
Iteration 86/1000 | Loss: 0.00002884
Iteration 87/1000 | Loss: 0.00002884
Iteration 88/1000 | Loss: 0.00002884
Iteration 89/1000 | Loss: 0.00002884
Iteration 90/1000 | Loss: 0.00002884
Iteration 91/1000 | Loss: 0.00002884
Iteration 92/1000 | Loss: 0.00002883
Iteration 93/1000 | Loss: 0.00002883
Iteration 94/1000 | Loss: 0.00002883
Iteration 95/1000 | Loss: 0.00002883
Iteration 96/1000 | Loss: 0.00002883
Iteration 97/1000 | Loss: 0.00002883
Iteration 98/1000 | Loss: 0.00002883
Iteration 99/1000 | Loss: 0.00002883
Iteration 100/1000 | Loss: 0.00002883
Iteration 101/1000 | Loss: 0.00002883
Iteration 102/1000 | Loss: 0.00002882
Iteration 103/1000 | Loss: 0.00002882
Iteration 104/1000 | Loss: 0.00002882
Iteration 105/1000 | Loss: 0.00002882
Iteration 106/1000 | Loss: 0.00002881
Iteration 107/1000 | Loss: 0.00002881
Iteration 108/1000 | Loss: 0.00002881
Iteration 109/1000 | Loss: 0.00002881
Iteration 110/1000 | Loss: 0.00002881
Iteration 111/1000 | Loss: 0.00002881
Iteration 112/1000 | Loss: 0.00002881
Iteration 113/1000 | Loss: 0.00002881
Iteration 114/1000 | Loss: 0.00002881
Iteration 115/1000 | Loss: 0.00002881
Iteration 116/1000 | Loss: 0.00002881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.8813930839532986e-05, 2.8813930839532986e-05, 2.8813930839532986e-05, 2.8813930839532986e-05, 2.8813930839532986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8813930839532986e-05

Optimization complete. Final v2v error: 4.423744201660156 mm

Highest mean error: 5.743016242980957 mm for frame 67

Lowest mean error: 3.5689144134521484 mm for frame 27

Saving results

Total time: 49.77684950828552
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800564
Iteration 2/25 | Loss: 0.00135306
Iteration 3/25 | Loss: 0.00120116
Iteration 4/25 | Loss: 0.00118483
Iteration 5/25 | Loss: 0.00118085
Iteration 6/25 | Loss: 0.00118080
Iteration 7/25 | Loss: 0.00118080
Iteration 8/25 | Loss: 0.00118080
Iteration 9/25 | Loss: 0.00118080
Iteration 10/25 | Loss: 0.00118080
Iteration 11/25 | Loss: 0.00118080
Iteration 12/25 | Loss: 0.00118080
Iteration 13/25 | Loss: 0.00118080
Iteration 14/25 | Loss: 0.00118080
Iteration 15/25 | Loss: 0.00118080
Iteration 16/25 | Loss: 0.00118080
Iteration 17/25 | Loss: 0.00118080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011808020062744617, 0.0011808020062744617, 0.0011808020062744617, 0.0011808020062744617, 0.0011808020062744617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011808020062744617

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45509160
Iteration 2/25 | Loss: 0.00058406
Iteration 3/25 | Loss: 0.00058406
Iteration 4/25 | Loss: 0.00058406
Iteration 5/25 | Loss: 0.00058406
Iteration 6/25 | Loss: 0.00058406
Iteration 7/25 | Loss: 0.00058406
Iteration 8/25 | Loss: 0.00058406
Iteration 9/25 | Loss: 0.00058406
Iteration 10/25 | Loss: 0.00058406
Iteration 11/25 | Loss: 0.00058406
Iteration 12/25 | Loss: 0.00058406
Iteration 13/25 | Loss: 0.00058406
Iteration 14/25 | Loss: 0.00058406
Iteration 15/25 | Loss: 0.00058406
Iteration 16/25 | Loss: 0.00058406
Iteration 17/25 | Loss: 0.00058406
Iteration 18/25 | Loss: 0.00058406
Iteration 19/25 | Loss: 0.00058406
Iteration 20/25 | Loss: 0.00058406
Iteration 21/25 | Loss: 0.00058406
Iteration 22/25 | Loss: 0.00058406
Iteration 23/25 | Loss: 0.00058406
Iteration 24/25 | Loss: 0.00058406
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005840570665895939, 0.0005840570665895939, 0.0005840570665895939, 0.0005840570665895939, 0.0005840570665895939]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005840570665895939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058406
Iteration 2/1000 | Loss: 0.00002954
Iteration 3/1000 | Loss: 0.00001825
Iteration 4/1000 | Loss: 0.00001612
Iteration 5/1000 | Loss: 0.00001501
Iteration 6/1000 | Loss: 0.00001418
Iteration 7/1000 | Loss: 0.00001365
Iteration 8/1000 | Loss: 0.00001330
Iteration 9/1000 | Loss: 0.00001304
Iteration 10/1000 | Loss: 0.00001272
Iteration 11/1000 | Loss: 0.00001260
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001258
Iteration 14/1000 | Loss: 0.00001258
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001256
Iteration 17/1000 | Loss: 0.00001252
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001242
Iteration 20/1000 | Loss: 0.00001242
Iteration 21/1000 | Loss: 0.00001241
Iteration 22/1000 | Loss: 0.00001240
Iteration 23/1000 | Loss: 0.00001239
Iteration 24/1000 | Loss: 0.00001239
Iteration 25/1000 | Loss: 0.00001239
Iteration 26/1000 | Loss: 0.00001237
Iteration 27/1000 | Loss: 0.00001237
Iteration 28/1000 | Loss: 0.00001233
Iteration 29/1000 | Loss: 0.00001233
Iteration 30/1000 | Loss: 0.00001232
Iteration 31/1000 | Loss: 0.00001231
Iteration 32/1000 | Loss: 0.00001231
Iteration 33/1000 | Loss: 0.00001230
Iteration 34/1000 | Loss: 0.00001230
Iteration 35/1000 | Loss: 0.00001230
Iteration 36/1000 | Loss: 0.00001229
Iteration 37/1000 | Loss: 0.00001229
Iteration 38/1000 | Loss: 0.00001229
Iteration 39/1000 | Loss: 0.00001228
Iteration 40/1000 | Loss: 0.00001228
Iteration 41/1000 | Loss: 0.00001228
Iteration 42/1000 | Loss: 0.00001228
Iteration 43/1000 | Loss: 0.00001227
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001227
Iteration 47/1000 | Loss: 0.00001227
Iteration 48/1000 | Loss: 0.00001227
Iteration 49/1000 | Loss: 0.00001227
Iteration 50/1000 | Loss: 0.00001226
Iteration 51/1000 | Loss: 0.00001226
Iteration 52/1000 | Loss: 0.00001226
Iteration 53/1000 | Loss: 0.00001226
Iteration 54/1000 | Loss: 0.00001226
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001226
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001223
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001222
Iteration 70/1000 | Loss: 0.00001222
Iteration 71/1000 | Loss: 0.00001222
Iteration 72/1000 | Loss: 0.00001222
Iteration 73/1000 | Loss: 0.00001222
Iteration 74/1000 | Loss: 0.00001221
Iteration 75/1000 | Loss: 0.00001221
Iteration 76/1000 | Loss: 0.00001221
Iteration 77/1000 | Loss: 0.00001221
Iteration 78/1000 | Loss: 0.00001221
Iteration 79/1000 | Loss: 0.00001220
Iteration 80/1000 | Loss: 0.00001220
Iteration 81/1000 | Loss: 0.00001220
Iteration 82/1000 | Loss: 0.00001220
Iteration 83/1000 | Loss: 0.00001219
Iteration 84/1000 | Loss: 0.00001219
Iteration 85/1000 | Loss: 0.00001219
Iteration 86/1000 | Loss: 0.00001219
Iteration 87/1000 | Loss: 0.00001219
Iteration 88/1000 | Loss: 0.00001219
Iteration 89/1000 | Loss: 0.00001219
Iteration 90/1000 | Loss: 0.00001219
Iteration 91/1000 | Loss: 0.00001219
Iteration 92/1000 | Loss: 0.00001218
Iteration 93/1000 | Loss: 0.00001218
Iteration 94/1000 | Loss: 0.00001217
Iteration 95/1000 | Loss: 0.00001217
Iteration 96/1000 | Loss: 0.00001217
Iteration 97/1000 | Loss: 0.00001217
Iteration 98/1000 | Loss: 0.00001217
Iteration 99/1000 | Loss: 0.00001217
Iteration 100/1000 | Loss: 0.00001217
Iteration 101/1000 | Loss: 0.00001216
Iteration 102/1000 | Loss: 0.00001216
Iteration 103/1000 | Loss: 0.00001216
Iteration 104/1000 | Loss: 0.00001216
Iteration 105/1000 | Loss: 0.00001216
Iteration 106/1000 | Loss: 0.00001216
Iteration 107/1000 | Loss: 0.00001216
Iteration 108/1000 | Loss: 0.00001216
Iteration 109/1000 | Loss: 0.00001215
Iteration 110/1000 | Loss: 0.00001215
Iteration 111/1000 | Loss: 0.00001215
Iteration 112/1000 | Loss: 0.00001215
Iteration 113/1000 | Loss: 0.00001215
Iteration 114/1000 | Loss: 0.00001215
Iteration 115/1000 | Loss: 0.00001215
Iteration 116/1000 | Loss: 0.00001215
Iteration 117/1000 | Loss: 0.00001215
Iteration 118/1000 | Loss: 0.00001215
Iteration 119/1000 | Loss: 0.00001215
Iteration 120/1000 | Loss: 0.00001215
Iteration 121/1000 | Loss: 0.00001215
Iteration 122/1000 | Loss: 0.00001214
Iteration 123/1000 | Loss: 0.00001214
Iteration 124/1000 | Loss: 0.00001214
Iteration 125/1000 | Loss: 0.00001214
Iteration 126/1000 | Loss: 0.00001214
Iteration 127/1000 | Loss: 0.00001214
Iteration 128/1000 | Loss: 0.00001214
Iteration 129/1000 | Loss: 0.00001214
Iteration 130/1000 | Loss: 0.00001214
Iteration 131/1000 | Loss: 0.00001214
Iteration 132/1000 | Loss: 0.00001214
Iteration 133/1000 | Loss: 0.00001213
Iteration 134/1000 | Loss: 0.00001213
Iteration 135/1000 | Loss: 0.00001213
Iteration 136/1000 | Loss: 0.00001213
Iteration 137/1000 | Loss: 0.00001213
Iteration 138/1000 | Loss: 0.00001213
Iteration 139/1000 | Loss: 0.00001213
Iteration 140/1000 | Loss: 0.00001213
Iteration 141/1000 | Loss: 0.00001213
Iteration 142/1000 | Loss: 0.00001212
Iteration 143/1000 | Loss: 0.00001212
Iteration 144/1000 | Loss: 0.00001212
Iteration 145/1000 | Loss: 0.00001212
Iteration 146/1000 | Loss: 0.00001212
Iteration 147/1000 | Loss: 0.00001212
Iteration 148/1000 | Loss: 0.00001212
Iteration 149/1000 | Loss: 0.00001212
Iteration 150/1000 | Loss: 0.00001212
Iteration 151/1000 | Loss: 0.00001212
Iteration 152/1000 | Loss: 0.00001212
Iteration 153/1000 | Loss: 0.00001212
Iteration 154/1000 | Loss: 0.00001212
Iteration 155/1000 | Loss: 0.00001212
Iteration 156/1000 | Loss: 0.00001212
Iteration 157/1000 | Loss: 0.00001212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.2115358003939036e-05, 1.2115358003939036e-05, 1.2115358003939036e-05, 1.2115358003939036e-05, 1.2115358003939036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2115358003939036e-05

Optimization complete. Final v2v error: 2.9853336811065674 mm

Highest mean error: 3.166963815689087 mm for frame 110

Lowest mean error: 2.837783098220825 mm for frame 130

Saving results

Total time: 40.99787878990173
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00572131
Iteration 2/25 | Loss: 0.00123864
Iteration 3/25 | Loss: 0.00118699
Iteration 4/25 | Loss: 0.00117846
Iteration 5/25 | Loss: 0.00117578
Iteration 6/25 | Loss: 0.00117554
Iteration 7/25 | Loss: 0.00117554
Iteration 8/25 | Loss: 0.00117554
Iteration 9/25 | Loss: 0.00117554
Iteration 10/25 | Loss: 0.00117554
Iteration 11/25 | Loss: 0.00117554
Iteration 12/25 | Loss: 0.00117554
Iteration 13/25 | Loss: 0.00117554
Iteration 14/25 | Loss: 0.00117554
Iteration 15/25 | Loss: 0.00117554
Iteration 16/25 | Loss: 0.00117554
Iteration 17/25 | Loss: 0.00117554
Iteration 18/25 | Loss: 0.00117554
Iteration 19/25 | Loss: 0.00117554
Iteration 20/25 | Loss: 0.00117554
Iteration 21/25 | Loss: 0.00117554
Iteration 22/25 | Loss: 0.00117554
Iteration 23/25 | Loss: 0.00117554
Iteration 24/25 | Loss: 0.00117554
Iteration 25/25 | Loss: 0.00117554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94442701
Iteration 2/25 | Loss: 0.00068075
Iteration 3/25 | Loss: 0.00068075
Iteration 4/25 | Loss: 0.00068075
Iteration 5/25 | Loss: 0.00068075
Iteration 6/25 | Loss: 0.00068075
Iteration 7/25 | Loss: 0.00068075
Iteration 8/25 | Loss: 0.00068075
Iteration 9/25 | Loss: 0.00068075
Iteration 10/25 | Loss: 0.00068075
Iteration 11/25 | Loss: 0.00068075
Iteration 12/25 | Loss: 0.00068075
Iteration 13/25 | Loss: 0.00068075
Iteration 14/25 | Loss: 0.00068075
Iteration 15/25 | Loss: 0.00068075
Iteration 16/25 | Loss: 0.00068075
Iteration 17/25 | Loss: 0.00068075
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006807540776208043, 0.0006807540776208043, 0.0006807540776208043, 0.0006807540776208043, 0.0006807540776208043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006807540776208043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068075
Iteration 2/1000 | Loss: 0.00002121
Iteration 3/1000 | Loss: 0.00001502
Iteration 4/1000 | Loss: 0.00001368
Iteration 5/1000 | Loss: 0.00001308
Iteration 6/1000 | Loss: 0.00001283
Iteration 7/1000 | Loss: 0.00001247
Iteration 8/1000 | Loss: 0.00001245
Iteration 9/1000 | Loss: 0.00001242
Iteration 10/1000 | Loss: 0.00001232
Iteration 11/1000 | Loss: 0.00001210
Iteration 12/1000 | Loss: 0.00001205
Iteration 13/1000 | Loss: 0.00001198
Iteration 14/1000 | Loss: 0.00001187
Iteration 15/1000 | Loss: 0.00001186
Iteration 16/1000 | Loss: 0.00001182
Iteration 17/1000 | Loss: 0.00001181
Iteration 18/1000 | Loss: 0.00001180
Iteration 19/1000 | Loss: 0.00001174
Iteration 20/1000 | Loss: 0.00001174
Iteration 21/1000 | Loss: 0.00001174
Iteration 22/1000 | Loss: 0.00001174
Iteration 23/1000 | Loss: 0.00001174
Iteration 24/1000 | Loss: 0.00001173
Iteration 25/1000 | Loss: 0.00001173
Iteration 26/1000 | Loss: 0.00001173
Iteration 27/1000 | Loss: 0.00001172
Iteration 28/1000 | Loss: 0.00001171
Iteration 29/1000 | Loss: 0.00001171
Iteration 30/1000 | Loss: 0.00001171
Iteration 31/1000 | Loss: 0.00001171
Iteration 32/1000 | Loss: 0.00001170
Iteration 33/1000 | Loss: 0.00001170
Iteration 34/1000 | Loss: 0.00001170
Iteration 35/1000 | Loss: 0.00001170
Iteration 36/1000 | Loss: 0.00001170
Iteration 37/1000 | Loss: 0.00001170
Iteration 38/1000 | Loss: 0.00001170
Iteration 39/1000 | Loss: 0.00001170
Iteration 40/1000 | Loss: 0.00001170
Iteration 41/1000 | Loss: 0.00001170
Iteration 42/1000 | Loss: 0.00001170
Iteration 43/1000 | Loss: 0.00001170
Iteration 44/1000 | Loss: 0.00001170
Iteration 45/1000 | Loss: 0.00001170
Iteration 46/1000 | Loss: 0.00001170
Iteration 47/1000 | Loss: 0.00001170
Iteration 48/1000 | Loss: 0.00001169
Iteration 49/1000 | Loss: 0.00001169
Iteration 50/1000 | Loss: 0.00001169
Iteration 51/1000 | Loss: 0.00001168
Iteration 52/1000 | Loss: 0.00001168
Iteration 53/1000 | Loss: 0.00001167
Iteration 54/1000 | Loss: 0.00001166
Iteration 55/1000 | Loss: 0.00001166
Iteration 56/1000 | Loss: 0.00001166
Iteration 57/1000 | Loss: 0.00001165
Iteration 58/1000 | Loss: 0.00001165
Iteration 59/1000 | Loss: 0.00001165
Iteration 60/1000 | Loss: 0.00001164
Iteration 61/1000 | Loss: 0.00001164
Iteration 62/1000 | Loss: 0.00001164
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001162
Iteration 66/1000 | Loss: 0.00001162
Iteration 67/1000 | Loss: 0.00001162
Iteration 68/1000 | Loss: 0.00001162
Iteration 69/1000 | Loss: 0.00001162
Iteration 70/1000 | Loss: 0.00001162
Iteration 71/1000 | Loss: 0.00001162
Iteration 72/1000 | Loss: 0.00001162
Iteration 73/1000 | Loss: 0.00001161
Iteration 74/1000 | Loss: 0.00001161
Iteration 75/1000 | Loss: 0.00001161
Iteration 76/1000 | Loss: 0.00001161
Iteration 77/1000 | Loss: 0.00001161
Iteration 78/1000 | Loss: 0.00001161
Iteration 79/1000 | Loss: 0.00001161
Iteration 80/1000 | Loss: 0.00001161
Iteration 81/1000 | Loss: 0.00001161
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001160
Iteration 85/1000 | Loss: 0.00001160
Iteration 86/1000 | Loss: 0.00001160
Iteration 87/1000 | Loss: 0.00001160
Iteration 88/1000 | Loss: 0.00001159
Iteration 89/1000 | Loss: 0.00001159
Iteration 90/1000 | Loss: 0.00001158
Iteration 91/1000 | Loss: 0.00001158
Iteration 92/1000 | Loss: 0.00001157
Iteration 93/1000 | Loss: 0.00001154
Iteration 94/1000 | Loss: 0.00001151
Iteration 95/1000 | Loss: 0.00001150
Iteration 96/1000 | Loss: 0.00001150
Iteration 97/1000 | Loss: 0.00001150
Iteration 98/1000 | Loss: 0.00001149
Iteration 99/1000 | Loss: 0.00001149
Iteration 100/1000 | Loss: 0.00001148
Iteration 101/1000 | Loss: 0.00001148
Iteration 102/1000 | Loss: 0.00001147
Iteration 103/1000 | Loss: 0.00001147
Iteration 104/1000 | Loss: 0.00001147
Iteration 105/1000 | Loss: 0.00001146
Iteration 106/1000 | Loss: 0.00001146
Iteration 107/1000 | Loss: 0.00001146
Iteration 108/1000 | Loss: 0.00001146
Iteration 109/1000 | Loss: 0.00001146
Iteration 110/1000 | Loss: 0.00001146
Iteration 111/1000 | Loss: 0.00001146
Iteration 112/1000 | Loss: 0.00001146
Iteration 113/1000 | Loss: 0.00001146
Iteration 114/1000 | Loss: 0.00001146
Iteration 115/1000 | Loss: 0.00001146
Iteration 116/1000 | Loss: 0.00001146
Iteration 117/1000 | Loss: 0.00001146
Iteration 118/1000 | Loss: 0.00001146
Iteration 119/1000 | Loss: 0.00001146
Iteration 120/1000 | Loss: 0.00001146
Iteration 121/1000 | Loss: 0.00001146
Iteration 122/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.1455161256890278e-05, 1.1455161256890278e-05, 1.1455161256890278e-05, 1.1455161256890278e-05, 1.1455161256890278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1455161256890278e-05

Optimization complete. Final v2v error: 2.905951976776123 mm

Highest mean error: 3.3406524658203125 mm for frame 118

Lowest mean error: 2.785371780395508 mm for frame 7

Saving results

Total time: 31.509538412094116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412573
Iteration 2/25 | Loss: 0.00132004
Iteration 3/25 | Loss: 0.00120566
Iteration 4/25 | Loss: 0.00119358
Iteration 5/25 | Loss: 0.00119108
Iteration 6/25 | Loss: 0.00119076
Iteration 7/25 | Loss: 0.00119076
Iteration 8/25 | Loss: 0.00119076
Iteration 9/25 | Loss: 0.00119076
Iteration 10/25 | Loss: 0.00119076
Iteration 11/25 | Loss: 0.00119076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011907585430890322, 0.0011907585430890322, 0.0011907585430890322, 0.0011907585430890322, 0.0011907585430890322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011907585430890322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45194578
Iteration 2/25 | Loss: 0.00053799
Iteration 3/25 | Loss: 0.00053798
Iteration 4/25 | Loss: 0.00053798
Iteration 5/25 | Loss: 0.00053798
Iteration 6/25 | Loss: 0.00053798
Iteration 7/25 | Loss: 0.00053798
Iteration 8/25 | Loss: 0.00053798
Iteration 9/25 | Loss: 0.00053798
Iteration 10/25 | Loss: 0.00053798
Iteration 11/25 | Loss: 0.00053798
Iteration 12/25 | Loss: 0.00053798
Iteration 13/25 | Loss: 0.00053798
Iteration 14/25 | Loss: 0.00053798
Iteration 15/25 | Loss: 0.00053798
Iteration 16/25 | Loss: 0.00053798
Iteration 17/25 | Loss: 0.00053798
Iteration 18/25 | Loss: 0.00053798
Iteration 19/25 | Loss: 0.00053798
Iteration 20/25 | Loss: 0.00053798
Iteration 21/25 | Loss: 0.00053798
Iteration 22/25 | Loss: 0.00053798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000537978601641953, 0.000537978601641953, 0.000537978601641953, 0.000537978601641953, 0.000537978601641953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000537978601641953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053798
Iteration 2/1000 | Loss: 0.00002754
Iteration 3/1000 | Loss: 0.00002061
Iteration 4/1000 | Loss: 0.00001898
Iteration 5/1000 | Loss: 0.00001795
Iteration 6/1000 | Loss: 0.00001734
Iteration 7/1000 | Loss: 0.00001671
Iteration 8/1000 | Loss: 0.00001644
Iteration 9/1000 | Loss: 0.00001616
Iteration 10/1000 | Loss: 0.00001593
Iteration 11/1000 | Loss: 0.00001583
Iteration 12/1000 | Loss: 0.00001582
Iteration 13/1000 | Loss: 0.00001578
Iteration 14/1000 | Loss: 0.00001577
Iteration 15/1000 | Loss: 0.00001576
Iteration 16/1000 | Loss: 0.00001574
Iteration 17/1000 | Loss: 0.00001572
Iteration 18/1000 | Loss: 0.00001571
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001559
Iteration 22/1000 | Loss: 0.00001552
Iteration 23/1000 | Loss: 0.00001552
Iteration 24/1000 | Loss: 0.00001546
Iteration 25/1000 | Loss: 0.00001544
Iteration 26/1000 | Loss: 0.00001544
Iteration 27/1000 | Loss: 0.00001543
Iteration 28/1000 | Loss: 0.00001543
Iteration 29/1000 | Loss: 0.00001540
Iteration 30/1000 | Loss: 0.00001540
Iteration 31/1000 | Loss: 0.00001539
Iteration 32/1000 | Loss: 0.00001538
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001532
Iteration 35/1000 | Loss: 0.00001532
Iteration 36/1000 | Loss: 0.00001523
Iteration 37/1000 | Loss: 0.00001520
Iteration 38/1000 | Loss: 0.00001518
Iteration 39/1000 | Loss: 0.00001517
Iteration 40/1000 | Loss: 0.00001516
Iteration 41/1000 | Loss: 0.00001515
Iteration 42/1000 | Loss: 0.00001515
Iteration 43/1000 | Loss: 0.00001515
Iteration 44/1000 | Loss: 0.00001514
Iteration 45/1000 | Loss: 0.00001514
Iteration 46/1000 | Loss: 0.00001513
Iteration 47/1000 | Loss: 0.00001512
Iteration 48/1000 | Loss: 0.00001512
Iteration 49/1000 | Loss: 0.00001511
Iteration 50/1000 | Loss: 0.00001511
Iteration 51/1000 | Loss: 0.00001511
Iteration 52/1000 | Loss: 0.00001511
Iteration 53/1000 | Loss: 0.00001511
Iteration 54/1000 | Loss: 0.00001511
Iteration 55/1000 | Loss: 0.00001510
Iteration 56/1000 | Loss: 0.00001510
Iteration 57/1000 | Loss: 0.00001510
Iteration 58/1000 | Loss: 0.00001510
Iteration 59/1000 | Loss: 0.00001510
Iteration 60/1000 | Loss: 0.00001509
Iteration 61/1000 | Loss: 0.00001509
Iteration 62/1000 | Loss: 0.00001509
Iteration 63/1000 | Loss: 0.00001509
Iteration 64/1000 | Loss: 0.00001508
Iteration 65/1000 | Loss: 0.00001508
Iteration 66/1000 | Loss: 0.00001508
Iteration 67/1000 | Loss: 0.00001508
Iteration 68/1000 | Loss: 0.00001508
Iteration 69/1000 | Loss: 0.00001507
Iteration 70/1000 | Loss: 0.00001507
Iteration 71/1000 | Loss: 0.00001507
Iteration 72/1000 | Loss: 0.00001507
Iteration 73/1000 | Loss: 0.00001507
Iteration 74/1000 | Loss: 0.00001507
Iteration 75/1000 | Loss: 0.00001507
Iteration 76/1000 | Loss: 0.00001507
Iteration 77/1000 | Loss: 0.00001507
Iteration 78/1000 | Loss: 0.00001507
Iteration 79/1000 | Loss: 0.00001507
Iteration 80/1000 | Loss: 0.00001507
Iteration 81/1000 | Loss: 0.00001506
Iteration 82/1000 | Loss: 0.00001506
Iteration 83/1000 | Loss: 0.00001506
Iteration 84/1000 | Loss: 0.00001506
Iteration 85/1000 | Loss: 0.00001505
Iteration 86/1000 | Loss: 0.00001505
Iteration 87/1000 | Loss: 0.00001505
Iteration 88/1000 | Loss: 0.00001505
Iteration 89/1000 | Loss: 0.00001505
Iteration 90/1000 | Loss: 0.00001504
Iteration 91/1000 | Loss: 0.00001504
Iteration 92/1000 | Loss: 0.00001504
Iteration 93/1000 | Loss: 0.00001504
Iteration 94/1000 | Loss: 0.00001504
Iteration 95/1000 | Loss: 0.00001504
Iteration 96/1000 | Loss: 0.00001503
Iteration 97/1000 | Loss: 0.00001503
Iteration 98/1000 | Loss: 0.00001503
Iteration 99/1000 | Loss: 0.00001502
Iteration 100/1000 | Loss: 0.00001502
Iteration 101/1000 | Loss: 0.00001501
Iteration 102/1000 | Loss: 0.00001501
Iteration 103/1000 | Loss: 0.00001501
Iteration 104/1000 | Loss: 0.00001501
Iteration 105/1000 | Loss: 0.00001501
Iteration 106/1000 | Loss: 0.00001501
Iteration 107/1000 | Loss: 0.00001501
Iteration 108/1000 | Loss: 0.00001501
Iteration 109/1000 | Loss: 0.00001501
Iteration 110/1000 | Loss: 0.00001501
Iteration 111/1000 | Loss: 0.00001501
Iteration 112/1000 | Loss: 0.00001499
Iteration 113/1000 | Loss: 0.00001498
Iteration 114/1000 | Loss: 0.00001498
Iteration 115/1000 | Loss: 0.00001497
Iteration 116/1000 | Loss: 0.00001497
Iteration 117/1000 | Loss: 0.00001497
Iteration 118/1000 | Loss: 0.00001497
Iteration 119/1000 | Loss: 0.00001497
Iteration 120/1000 | Loss: 0.00001497
Iteration 121/1000 | Loss: 0.00001497
Iteration 122/1000 | Loss: 0.00001497
Iteration 123/1000 | Loss: 0.00001497
Iteration 124/1000 | Loss: 0.00001497
Iteration 125/1000 | Loss: 0.00001497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.4968984942242969e-05, 1.4968984942242969e-05, 1.4968984942242969e-05, 1.4968984942242969e-05, 1.4968984942242969e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4968984942242969e-05

Optimization complete. Final v2v error: 3.2878577709198 mm

Highest mean error: 3.532163143157959 mm for frame 115

Lowest mean error: 3.0373404026031494 mm for frame 7

Saving results

Total time: 36.96733999252319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951724
Iteration 2/25 | Loss: 0.00951724
Iteration 3/25 | Loss: 0.00558883
Iteration 4/25 | Loss: 0.00229714
Iteration 5/25 | Loss: 0.00199297
Iteration 6/25 | Loss: 0.00188472
Iteration 7/25 | Loss: 0.00174623
Iteration 8/25 | Loss: 0.00176766
Iteration 9/25 | Loss: 0.00164833
Iteration 10/25 | Loss: 0.00158251
Iteration 11/25 | Loss: 0.00153062
Iteration 12/25 | Loss: 0.00149634
Iteration 13/25 | Loss: 0.00147413
Iteration 14/25 | Loss: 0.00146430
Iteration 15/25 | Loss: 0.00146300
Iteration 16/25 | Loss: 0.00145979
Iteration 17/25 | Loss: 0.00146846
Iteration 18/25 | Loss: 0.00146478
Iteration 19/25 | Loss: 0.00145723
Iteration 20/25 | Loss: 0.00144509
Iteration 21/25 | Loss: 0.00145025
Iteration 22/25 | Loss: 0.00143497
Iteration 23/25 | Loss: 0.00143050
Iteration 24/25 | Loss: 0.00142637
Iteration 25/25 | Loss: 0.00142047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45355618
Iteration 2/25 | Loss: 0.00406983
Iteration 3/25 | Loss: 0.00198772
Iteration 4/25 | Loss: 0.00198772
Iteration 5/25 | Loss: 0.00198772
Iteration 6/25 | Loss: 0.00198772
Iteration 7/25 | Loss: 0.00198772
Iteration 8/25 | Loss: 0.00198772
Iteration 9/25 | Loss: 0.00198772
Iteration 10/25 | Loss: 0.00198772
Iteration 11/25 | Loss: 0.00198772
Iteration 12/25 | Loss: 0.00198772
Iteration 13/25 | Loss: 0.00198772
Iteration 14/25 | Loss: 0.00198772
Iteration 15/25 | Loss: 0.00198772
Iteration 16/25 | Loss: 0.00198772
Iteration 17/25 | Loss: 0.00198772
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019877171143889427, 0.0019877171143889427, 0.0019877171143889427, 0.0019877171143889427, 0.0019877171143889427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019877171143889427

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198772
Iteration 2/1000 | Loss: 0.00241107
Iteration 3/1000 | Loss: 0.00190421
Iteration 4/1000 | Loss: 0.00390132
Iteration 5/1000 | Loss: 0.00120902
Iteration 6/1000 | Loss: 0.00053098
Iteration 7/1000 | Loss: 0.00090022
Iteration 8/1000 | Loss: 0.00070941
Iteration 9/1000 | Loss: 0.00166255
Iteration 10/1000 | Loss: 0.00162367
Iteration 11/1000 | Loss: 0.00197842
Iteration 12/1000 | Loss: 0.00222140
Iteration 13/1000 | Loss: 0.00347796
Iteration 14/1000 | Loss: 0.00192026
Iteration 15/1000 | Loss: 0.00505681
Iteration 16/1000 | Loss: 0.00328599
Iteration 17/1000 | Loss: 0.00475271
Iteration 18/1000 | Loss: 0.00287103
Iteration 19/1000 | Loss: 0.00356230
Iteration 20/1000 | Loss: 0.00267155
Iteration 21/1000 | Loss: 0.00222085
Iteration 22/1000 | Loss: 0.00139059
Iteration 23/1000 | Loss: 0.00102680
Iteration 24/1000 | Loss: 0.00090433
Iteration 25/1000 | Loss: 0.00095003
Iteration 26/1000 | Loss: 0.00070679
Iteration 27/1000 | Loss: 0.00072269
Iteration 28/1000 | Loss: 0.00164180
Iteration 29/1000 | Loss: 0.00074791
Iteration 30/1000 | Loss: 0.00053595
Iteration 31/1000 | Loss: 0.00015342
Iteration 32/1000 | Loss: 0.00038372
Iteration 33/1000 | Loss: 0.00075431
Iteration 34/1000 | Loss: 0.00064862
Iteration 35/1000 | Loss: 0.00022640
Iteration 36/1000 | Loss: 0.00015805
Iteration 37/1000 | Loss: 0.00009037
Iteration 38/1000 | Loss: 0.00017863
Iteration 39/1000 | Loss: 0.00028853
Iteration 40/1000 | Loss: 0.00045974
Iteration 41/1000 | Loss: 0.00065767
Iteration 42/1000 | Loss: 0.00058117
Iteration 43/1000 | Loss: 0.00023036
Iteration 44/1000 | Loss: 0.00034444
Iteration 45/1000 | Loss: 0.00022990
Iteration 46/1000 | Loss: 0.00027022
Iteration 47/1000 | Loss: 0.00037459
Iteration 48/1000 | Loss: 0.00014816
Iteration 49/1000 | Loss: 0.00013128
Iteration 50/1000 | Loss: 0.00008598
Iteration 51/1000 | Loss: 0.00031694
Iteration 52/1000 | Loss: 0.00015865
Iteration 53/1000 | Loss: 0.00023927
Iteration 54/1000 | Loss: 0.00019280
Iteration 55/1000 | Loss: 0.00016295
Iteration 56/1000 | Loss: 0.00016873
Iteration 57/1000 | Loss: 0.00020394
Iteration 58/1000 | Loss: 0.00024028
Iteration 59/1000 | Loss: 0.00042599
Iteration 60/1000 | Loss: 0.00016230
Iteration 61/1000 | Loss: 0.00005971
Iteration 62/1000 | Loss: 0.00008375
Iteration 63/1000 | Loss: 0.00011656
Iteration 64/1000 | Loss: 0.00033201
Iteration 65/1000 | Loss: 0.00019006
Iteration 66/1000 | Loss: 0.00014227
Iteration 67/1000 | Loss: 0.00019620
Iteration 68/1000 | Loss: 0.00014904
Iteration 69/1000 | Loss: 0.00010886
Iteration 70/1000 | Loss: 0.00007398
Iteration 71/1000 | Loss: 0.00006306
Iteration 72/1000 | Loss: 0.00007439
Iteration 73/1000 | Loss: 0.00009020
Iteration 74/1000 | Loss: 0.00028345
Iteration 75/1000 | Loss: 0.00068829
Iteration 76/1000 | Loss: 0.00043015
Iteration 77/1000 | Loss: 0.00046526
Iteration 78/1000 | Loss: 0.00022067
Iteration 79/1000 | Loss: 0.00019712
Iteration 80/1000 | Loss: 0.00023126
Iteration 81/1000 | Loss: 0.00021524
Iteration 82/1000 | Loss: 0.00058881
Iteration 83/1000 | Loss: 0.00037826
Iteration 84/1000 | Loss: 0.00036011
Iteration 85/1000 | Loss: 0.00036629
Iteration 86/1000 | Loss: 0.00033145
Iteration 87/1000 | Loss: 0.00025575
Iteration 88/1000 | Loss: 0.00009272
Iteration 89/1000 | Loss: 0.00010585
Iteration 90/1000 | Loss: 0.00025067
Iteration 91/1000 | Loss: 0.00016989
Iteration 92/1000 | Loss: 0.00027783
Iteration 93/1000 | Loss: 0.00016437
Iteration 94/1000 | Loss: 0.00006690
Iteration 95/1000 | Loss: 0.00022173
Iteration 96/1000 | Loss: 0.00017541
Iteration 97/1000 | Loss: 0.00018698
Iteration 98/1000 | Loss: 0.00005130
Iteration 99/1000 | Loss: 0.00005975
Iteration 100/1000 | Loss: 0.00016847
Iteration 101/1000 | Loss: 0.00021531
Iteration 102/1000 | Loss: 0.00006587
Iteration 103/1000 | Loss: 0.00006120
Iteration 104/1000 | Loss: 0.00010067
Iteration 105/1000 | Loss: 0.00098940
Iteration 106/1000 | Loss: 0.00043090
Iteration 107/1000 | Loss: 0.00011316
Iteration 108/1000 | Loss: 0.00038607
Iteration 109/1000 | Loss: 0.00012578
Iteration 110/1000 | Loss: 0.00025805
Iteration 111/1000 | Loss: 0.00045677
Iteration 112/1000 | Loss: 0.00030116
Iteration 113/1000 | Loss: 0.00031130
Iteration 114/1000 | Loss: 0.00024402
Iteration 115/1000 | Loss: 0.00024005
Iteration 116/1000 | Loss: 0.00063724
Iteration 117/1000 | Loss: 0.00057184
Iteration 118/1000 | Loss: 0.00026298
Iteration 119/1000 | Loss: 0.00014379
Iteration 120/1000 | Loss: 0.00007089
Iteration 121/1000 | Loss: 0.00012576
Iteration 122/1000 | Loss: 0.00010951
Iteration 123/1000 | Loss: 0.00010943
Iteration 124/1000 | Loss: 0.00013152
Iteration 125/1000 | Loss: 0.00010751
Iteration 126/1000 | Loss: 0.00009156
Iteration 127/1000 | Loss: 0.00006504
Iteration 128/1000 | Loss: 0.00010320
Iteration 129/1000 | Loss: 0.00015977
Iteration 130/1000 | Loss: 0.00009730
Iteration 131/1000 | Loss: 0.00013441
Iteration 132/1000 | Loss: 0.00009905
Iteration 133/1000 | Loss: 0.00040039
Iteration 134/1000 | Loss: 0.00010218
Iteration 135/1000 | Loss: 0.00030625
Iteration 136/1000 | Loss: 0.00011509
Iteration 137/1000 | Loss: 0.00014704
Iteration 138/1000 | Loss: 0.00010870
Iteration 139/1000 | Loss: 0.00006846
Iteration 140/1000 | Loss: 0.00022461
Iteration 141/1000 | Loss: 0.00008205
Iteration 142/1000 | Loss: 0.00009963
Iteration 143/1000 | Loss: 0.00025818
Iteration 144/1000 | Loss: 0.00044217
Iteration 145/1000 | Loss: 0.00016573
Iteration 146/1000 | Loss: 0.00010338
Iteration 147/1000 | Loss: 0.00048657
Iteration 148/1000 | Loss: 0.00033345
Iteration 149/1000 | Loss: 0.00026443
Iteration 150/1000 | Loss: 0.00016708
Iteration 151/1000 | Loss: 0.00018889
Iteration 152/1000 | Loss: 0.00035854
Iteration 153/1000 | Loss: 0.00033701
Iteration 154/1000 | Loss: 0.00024034
Iteration 155/1000 | Loss: 0.00051269
Iteration 156/1000 | Loss: 0.00046324
Iteration 157/1000 | Loss: 0.00014618
Iteration 158/1000 | Loss: 0.00041021
Iteration 159/1000 | Loss: 0.00017174
Iteration 160/1000 | Loss: 0.00044887
Iteration 161/1000 | Loss: 0.00017529
Iteration 162/1000 | Loss: 0.00022627
Iteration 163/1000 | Loss: 0.00027856
Iteration 164/1000 | Loss: 0.00022889
Iteration 165/1000 | Loss: 0.00012878
Iteration 166/1000 | Loss: 0.00020668
Iteration 167/1000 | Loss: 0.00017880
Iteration 168/1000 | Loss: 0.00028420
Iteration 169/1000 | Loss: 0.00014443
Iteration 170/1000 | Loss: 0.00030187
Iteration 171/1000 | Loss: 0.00027988
Iteration 172/1000 | Loss: 0.00045332
Iteration 173/1000 | Loss: 0.00019348
Iteration 174/1000 | Loss: 0.00012635
Iteration 175/1000 | Loss: 0.00037910
Iteration 176/1000 | Loss: 0.00018623
Iteration 177/1000 | Loss: 0.00018662
Iteration 178/1000 | Loss: 0.00016337
Iteration 179/1000 | Loss: 0.00008694
Iteration 180/1000 | Loss: 0.00007171
Iteration 181/1000 | Loss: 0.00009079
Iteration 182/1000 | Loss: 0.00013095
Iteration 183/1000 | Loss: 0.00011210
Iteration 184/1000 | Loss: 0.00015026
Iteration 185/1000 | Loss: 0.00008552
Iteration 186/1000 | Loss: 0.00009732
Iteration 187/1000 | Loss: 0.00006709
Iteration 188/1000 | Loss: 0.00011112
Iteration 189/1000 | Loss: 0.00013054
Iteration 190/1000 | Loss: 0.00006346
Iteration 191/1000 | Loss: 0.00017485
Iteration 192/1000 | Loss: 0.00012705
Iteration 193/1000 | Loss: 0.00010459
Iteration 194/1000 | Loss: 0.00011630
Iteration 195/1000 | Loss: 0.00009244
Iteration 196/1000 | Loss: 0.00018457
Iteration 197/1000 | Loss: 0.00013093
Iteration 198/1000 | Loss: 0.00024747
Iteration 199/1000 | Loss: 0.00015258
Iteration 200/1000 | Loss: 0.00014963
Iteration 201/1000 | Loss: 0.00010778
Iteration 202/1000 | Loss: 0.00013066
Iteration 203/1000 | Loss: 0.00034438
Iteration 204/1000 | Loss: 0.00011184
Iteration 205/1000 | Loss: 0.00014782
Iteration 206/1000 | Loss: 0.00011506
Iteration 207/1000 | Loss: 0.00007869
Iteration 208/1000 | Loss: 0.00007137
Iteration 209/1000 | Loss: 0.00012679
Iteration 210/1000 | Loss: 0.00009086
Iteration 211/1000 | Loss: 0.00010567
Iteration 212/1000 | Loss: 0.00004688
Iteration 213/1000 | Loss: 0.00009151
Iteration 214/1000 | Loss: 0.00033071
Iteration 215/1000 | Loss: 0.00010784
Iteration 216/1000 | Loss: 0.00016411
Iteration 217/1000 | Loss: 0.00012852
Iteration 218/1000 | Loss: 0.00006906
Iteration 219/1000 | Loss: 0.00040002
Iteration 220/1000 | Loss: 0.00029902
Iteration 221/1000 | Loss: 0.00009390
Iteration 222/1000 | Loss: 0.00007303
Iteration 223/1000 | Loss: 0.00005997
Iteration 224/1000 | Loss: 0.00009185
Iteration 225/1000 | Loss: 0.00010623
Iteration 226/1000 | Loss: 0.00008732
Iteration 227/1000 | Loss: 0.00005331
Iteration 228/1000 | Loss: 0.00018742
Iteration 229/1000 | Loss: 0.00012728
Iteration 230/1000 | Loss: 0.00018833
Iteration 231/1000 | Loss: 0.00015048
Iteration 232/1000 | Loss: 0.00014316
Iteration 233/1000 | Loss: 0.00009702
Iteration 234/1000 | Loss: 0.00004869
Iteration 235/1000 | Loss: 0.00031788
Iteration 236/1000 | Loss: 0.00046736
Iteration 237/1000 | Loss: 0.00013037
Iteration 238/1000 | Loss: 0.00008109
Iteration 239/1000 | Loss: 0.00017284
Iteration 240/1000 | Loss: 0.00019489
Iteration 241/1000 | Loss: 0.00005310
Iteration 242/1000 | Loss: 0.00016144
Iteration 243/1000 | Loss: 0.00006792
Iteration 244/1000 | Loss: 0.00028230
Iteration 245/1000 | Loss: 0.00008041
Iteration 246/1000 | Loss: 0.00008283
Iteration 247/1000 | Loss: 0.00006828
Iteration 248/1000 | Loss: 0.00006457
Iteration 249/1000 | Loss: 0.00007121
Iteration 250/1000 | Loss: 0.00004449
Iteration 251/1000 | Loss: 0.00009150
Iteration 252/1000 | Loss: 0.00005188
Iteration 253/1000 | Loss: 0.00005553
Iteration 254/1000 | Loss: 0.00006007
Iteration 255/1000 | Loss: 0.00007508
Iteration 256/1000 | Loss: 0.00003683
Iteration 257/1000 | Loss: 0.00003615
Iteration 258/1000 | Loss: 0.00008565
Iteration 259/1000 | Loss: 0.00019372
Iteration 260/1000 | Loss: 0.00015656
Iteration 261/1000 | Loss: 0.00018842
Iteration 262/1000 | Loss: 0.00029254
Iteration 263/1000 | Loss: 0.00011192
Iteration 264/1000 | Loss: 0.00005919
Iteration 265/1000 | Loss: 0.00029640
Iteration 266/1000 | Loss: 0.00005194
Iteration 267/1000 | Loss: 0.00010704
Iteration 268/1000 | Loss: 0.00005343
Iteration 269/1000 | Loss: 0.00003642
Iteration 270/1000 | Loss: 0.00008401
Iteration 271/1000 | Loss: 0.00004383
Iteration 272/1000 | Loss: 0.00003365
Iteration 273/1000 | Loss: 0.00005989
Iteration 274/1000 | Loss: 0.00004915
Iteration 275/1000 | Loss: 0.00007814
Iteration 276/1000 | Loss: 0.00006635
Iteration 277/1000 | Loss: 0.00003563
Iteration 278/1000 | Loss: 0.00011696
Iteration 279/1000 | Loss: 0.00011845
Iteration 280/1000 | Loss: 0.00010596
Iteration 281/1000 | Loss: 0.00006820
Iteration 282/1000 | Loss: 0.00005440
Iteration 283/1000 | Loss: 0.00003241
Iteration 284/1000 | Loss: 0.00015470
Iteration 285/1000 | Loss: 0.00004675
Iteration 286/1000 | Loss: 0.00052573
Iteration 287/1000 | Loss: 0.00073191
Iteration 288/1000 | Loss: 0.00036377
Iteration 289/1000 | Loss: 0.00006254
Iteration 290/1000 | Loss: 0.00004761
Iteration 291/1000 | Loss: 0.00004163
Iteration 292/1000 | Loss: 0.00007011
Iteration 293/1000 | Loss: 0.00003340
Iteration 294/1000 | Loss: 0.00003657
Iteration 295/1000 | Loss: 0.00003019
Iteration 296/1000 | Loss: 0.00003073
Iteration 297/1000 | Loss: 0.00049316
Iteration 298/1000 | Loss: 0.00041945
Iteration 299/1000 | Loss: 0.00009818
Iteration 300/1000 | Loss: 0.00004241
Iteration 301/1000 | Loss: 0.00003379
Iteration 302/1000 | Loss: 0.00003082
Iteration 303/1000 | Loss: 0.00007302
Iteration 304/1000 | Loss: 0.00002782
Iteration 305/1000 | Loss: 0.00008293
Iteration 306/1000 | Loss: 0.00003906
Iteration 307/1000 | Loss: 0.00006386
Iteration 308/1000 | Loss: 0.00003451
Iteration 309/1000 | Loss: 0.00006354
Iteration 310/1000 | Loss: 0.00002880
Iteration 311/1000 | Loss: 0.00004177
Iteration 312/1000 | Loss: 0.00002975
Iteration 313/1000 | Loss: 0.00004149
Iteration 314/1000 | Loss: 0.00012586
Iteration 315/1000 | Loss: 0.00009366
Iteration 316/1000 | Loss: 0.00003365
Iteration 317/1000 | Loss: 0.00010210
Iteration 318/1000 | Loss: 0.00004556
Iteration 319/1000 | Loss: 0.00002741
Iteration 320/1000 | Loss: 0.00010648
Iteration 321/1000 | Loss: 0.00003511
Iteration 322/1000 | Loss: 0.00005837
Iteration 323/1000 | Loss: 0.00008790
Iteration 324/1000 | Loss: 0.00003791
Iteration 325/1000 | Loss: 0.00005517
Iteration 326/1000 | Loss: 0.00003963
Iteration 327/1000 | Loss: 0.00002777
Iteration 328/1000 | Loss: 0.00004173
Iteration 329/1000 | Loss: 0.00002657
Iteration 330/1000 | Loss: 0.00003813
Iteration 331/1000 | Loss: 0.00002625
Iteration 332/1000 | Loss: 0.00002615
Iteration 333/1000 | Loss: 0.00002609
Iteration 334/1000 | Loss: 0.00002607
Iteration 335/1000 | Loss: 0.00002606
Iteration 336/1000 | Loss: 0.00002606
Iteration 337/1000 | Loss: 0.00002603
Iteration 338/1000 | Loss: 0.00002603
Iteration 339/1000 | Loss: 0.00002600
Iteration 340/1000 | Loss: 0.00002600
Iteration 341/1000 | Loss: 0.00005151
Iteration 342/1000 | Loss: 0.00002672
Iteration 343/1000 | Loss: 0.00007939
Iteration 344/1000 | Loss: 0.00002605
Iteration 345/1000 | Loss: 0.00002600
Iteration 346/1000 | Loss: 0.00002596
Iteration 347/1000 | Loss: 0.00002596
Iteration 348/1000 | Loss: 0.00002595
Iteration 349/1000 | Loss: 0.00002595
Iteration 350/1000 | Loss: 0.00002595
Iteration 351/1000 | Loss: 0.00002594
Iteration 352/1000 | Loss: 0.00002594
Iteration 353/1000 | Loss: 0.00002594
Iteration 354/1000 | Loss: 0.00002593
Iteration 355/1000 | Loss: 0.00002593
Iteration 356/1000 | Loss: 0.00002593
Iteration 357/1000 | Loss: 0.00002592
Iteration 358/1000 | Loss: 0.00002592
Iteration 359/1000 | Loss: 0.00002592
Iteration 360/1000 | Loss: 0.00002592
Iteration 361/1000 | Loss: 0.00002592
Iteration 362/1000 | Loss: 0.00002591
Iteration 363/1000 | Loss: 0.00002591
Iteration 364/1000 | Loss: 0.00002591
Iteration 365/1000 | Loss: 0.00002590
Iteration 366/1000 | Loss: 0.00002590
Iteration 367/1000 | Loss: 0.00002590
Iteration 368/1000 | Loss: 0.00002590
Iteration 369/1000 | Loss: 0.00002589
Iteration 370/1000 | Loss: 0.00002589
Iteration 371/1000 | Loss: 0.00002589
Iteration 372/1000 | Loss: 0.00002589
Iteration 373/1000 | Loss: 0.00002589
Iteration 374/1000 | Loss: 0.00002588
Iteration 375/1000 | Loss: 0.00002588
Iteration 376/1000 | Loss: 0.00002588
Iteration 377/1000 | Loss: 0.00002588
Iteration 378/1000 | Loss: 0.00002588
Iteration 379/1000 | Loss: 0.00002588
Iteration 380/1000 | Loss: 0.00002588
Iteration 381/1000 | Loss: 0.00002588
Iteration 382/1000 | Loss: 0.00002588
Iteration 383/1000 | Loss: 0.00002588
Iteration 384/1000 | Loss: 0.00002588
Iteration 385/1000 | Loss: 0.00002588
Iteration 386/1000 | Loss: 0.00002588
Iteration 387/1000 | Loss: 0.00002588
Iteration 388/1000 | Loss: 0.00002588
Iteration 389/1000 | Loss: 0.00002588
Iteration 390/1000 | Loss: 0.00002588
Iteration 391/1000 | Loss: 0.00002587
Iteration 392/1000 | Loss: 0.00002587
Iteration 393/1000 | Loss: 0.00002587
Iteration 394/1000 | Loss: 0.00002587
Iteration 395/1000 | Loss: 0.00002587
Iteration 396/1000 | Loss: 0.00002587
Iteration 397/1000 | Loss: 0.00002587
Iteration 398/1000 | Loss: 0.00002587
Iteration 399/1000 | Loss: 0.00002587
Iteration 400/1000 | Loss: 0.00002587
Iteration 401/1000 | Loss: 0.00002587
Iteration 402/1000 | Loss: 0.00002587
Iteration 403/1000 | Loss: 0.00002587
Iteration 404/1000 | Loss: 0.00002587
Iteration 405/1000 | Loss: 0.00002587
Iteration 406/1000 | Loss: 0.00002587
Iteration 407/1000 | Loss: 0.00002587
Iteration 408/1000 | Loss: 0.00002587
Iteration 409/1000 | Loss: 0.00002587
Iteration 410/1000 | Loss: 0.00002586
Iteration 411/1000 | Loss: 0.00002586
Iteration 412/1000 | Loss: 0.00002586
Iteration 413/1000 | Loss: 0.00002586
Iteration 414/1000 | Loss: 0.00002586
Iteration 415/1000 | Loss: 0.00002586
Iteration 416/1000 | Loss: 0.00002586
Iteration 417/1000 | Loss: 0.00002586
Iteration 418/1000 | Loss: 0.00002586
Iteration 419/1000 | Loss: 0.00002586
Iteration 420/1000 | Loss: 0.00002586
Iteration 421/1000 | Loss: 0.00002586
Iteration 422/1000 | Loss: 0.00002586
Iteration 423/1000 | Loss: 0.00002586
Iteration 424/1000 | Loss: 0.00002586
Iteration 425/1000 | Loss: 0.00002586
Iteration 426/1000 | Loss: 0.00002586
Iteration 427/1000 | Loss: 0.00002586
Iteration 428/1000 | Loss: 0.00002586
Iteration 429/1000 | Loss: 0.00002586
Iteration 430/1000 | Loss: 0.00002586
Iteration 431/1000 | Loss: 0.00002586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 431. Stopping optimization.
Last 5 losses: [2.5859526431304403e-05, 2.5859526431304403e-05, 2.5859526431304403e-05, 2.5859526431304403e-05, 2.5859526431304403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5859526431304403e-05

Optimization complete. Final v2v error: 4.257112979888916 mm

Highest mean error: 6.040700435638428 mm for frame 239

Lowest mean error: 3.3182640075683594 mm for frame 27

Saving results

Total time: 588.6560709476471
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056281
Iteration 2/25 | Loss: 0.01056281
Iteration 3/25 | Loss: 0.01056280
Iteration 4/25 | Loss: 0.01056280
Iteration 5/25 | Loss: 0.01056280
Iteration 6/25 | Loss: 0.00248828
Iteration 7/25 | Loss: 0.00174528
Iteration 8/25 | Loss: 0.00164771
Iteration 9/25 | Loss: 0.00160299
Iteration 10/25 | Loss: 0.00159522
Iteration 11/25 | Loss: 0.00156539
Iteration 12/25 | Loss: 0.00153840
Iteration 13/25 | Loss: 0.00151105
Iteration 14/25 | Loss: 0.00147921
Iteration 15/25 | Loss: 0.00145041
Iteration 16/25 | Loss: 0.00143633
Iteration 17/25 | Loss: 0.00142711
Iteration 18/25 | Loss: 0.00142918
Iteration 19/25 | Loss: 0.00140493
Iteration 20/25 | Loss: 0.00139521
Iteration 21/25 | Loss: 0.00139627
Iteration 22/25 | Loss: 0.00139098
Iteration 23/25 | Loss: 0.00138717
Iteration 24/25 | Loss: 0.00138944
Iteration 25/25 | Loss: 0.00138644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36334777
Iteration 2/25 | Loss: 0.00268488
Iteration 3/25 | Loss: 0.00159538
Iteration 4/25 | Loss: 0.00159538
Iteration 5/25 | Loss: 0.00159538
Iteration 6/25 | Loss: 0.00159538
Iteration 7/25 | Loss: 0.00159538
Iteration 8/25 | Loss: 0.00159537
Iteration 9/25 | Loss: 0.00159537
Iteration 10/25 | Loss: 0.00159537
Iteration 11/25 | Loss: 0.00159537
Iteration 12/25 | Loss: 0.00159537
Iteration 13/25 | Loss: 0.00159537
Iteration 14/25 | Loss: 0.00159537
Iteration 15/25 | Loss: 0.00159537
Iteration 16/25 | Loss: 0.00159537
Iteration 17/25 | Loss: 0.00159537
Iteration 18/25 | Loss: 0.00159537
Iteration 19/25 | Loss: 0.00159537
Iteration 20/25 | Loss: 0.00159537
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015953740803524852, 0.0015953740803524852, 0.0015953740803524852, 0.0015953740803524852, 0.0015953740803524852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015953740803524852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159537
Iteration 2/1000 | Loss: 0.00095029
Iteration 3/1000 | Loss: 0.00165353
Iteration 4/1000 | Loss: 0.00076849
Iteration 5/1000 | Loss: 0.00014829
Iteration 6/1000 | Loss: 0.00121126
Iteration 7/1000 | Loss: 0.00029974
Iteration 8/1000 | Loss: 0.00027324
Iteration 9/1000 | Loss: 0.00136148
Iteration 10/1000 | Loss: 0.00024787
Iteration 11/1000 | Loss: 0.00032453
Iteration 12/1000 | Loss: 0.00040937
Iteration 13/1000 | Loss: 0.00078916
Iteration 14/1000 | Loss: 0.00282839
Iteration 15/1000 | Loss: 0.00149227
Iteration 16/1000 | Loss: 0.00181363
Iteration 17/1000 | Loss: 0.00161558
Iteration 18/1000 | Loss: 0.00398654
Iteration 19/1000 | Loss: 0.00256502
Iteration 20/1000 | Loss: 0.00247150
Iteration 21/1000 | Loss: 0.00365297
Iteration 22/1000 | Loss: 0.00406211
Iteration 23/1000 | Loss: 0.00392237
Iteration 24/1000 | Loss: 0.00476465
Iteration 25/1000 | Loss: 0.00299091
Iteration 26/1000 | Loss: 0.00315728
Iteration 27/1000 | Loss: 0.00494649
Iteration 28/1000 | Loss: 0.00436668
Iteration 29/1000 | Loss: 0.00210594
Iteration 30/1000 | Loss: 0.00301889
Iteration 31/1000 | Loss: 0.00318786
Iteration 32/1000 | Loss: 0.00248822
Iteration 33/1000 | Loss: 0.00265427
Iteration 34/1000 | Loss: 0.00106202
Iteration 35/1000 | Loss: 0.00030149
Iteration 36/1000 | Loss: 0.00063954
Iteration 37/1000 | Loss: 0.00024234
Iteration 38/1000 | Loss: 0.00073418
Iteration 39/1000 | Loss: 0.00020423
Iteration 40/1000 | Loss: 0.00058044
Iteration 41/1000 | Loss: 0.00040890
Iteration 42/1000 | Loss: 0.00090572
Iteration 43/1000 | Loss: 0.00068599
Iteration 44/1000 | Loss: 0.00071186
Iteration 45/1000 | Loss: 0.00278339
Iteration 46/1000 | Loss: 0.00086264
Iteration 47/1000 | Loss: 0.00057136
Iteration 48/1000 | Loss: 0.00038607
Iteration 49/1000 | Loss: 0.00046249
Iteration 50/1000 | Loss: 0.00049684
Iteration 51/1000 | Loss: 0.00033572
Iteration 52/1000 | Loss: 0.00046956
Iteration 53/1000 | Loss: 0.00006354
Iteration 54/1000 | Loss: 0.00005820
Iteration 55/1000 | Loss: 0.00045097
Iteration 56/1000 | Loss: 0.00027251
Iteration 57/1000 | Loss: 0.00005349
Iteration 58/1000 | Loss: 0.00008820
Iteration 59/1000 | Loss: 0.00147123
Iteration 60/1000 | Loss: 0.00486868
Iteration 61/1000 | Loss: 0.00255203
Iteration 62/1000 | Loss: 0.00253663
Iteration 63/1000 | Loss: 0.00277800
Iteration 64/1000 | Loss: 0.00092826
Iteration 65/1000 | Loss: 0.00013064
Iteration 66/1000 | Loss: 0.00385675
Iteration 67/1000 | Loss: 0.00188121
Iteration 68/1000 | Loss: 0.00041520
Iteration 69/1000 | Loss: 0.00087613
Iteration 70/1000 | Loss: 0.00327646
Iteration 71/1000 | Loss: 0.00087693
Iteration 72/1000 | Loss: 0.00044791
Iteration 73/1000 | Loss: 0.00090225
Iteration 74/1000 | Loss: 0.00326481
Iteration 75/1000 | Loss: 0.00007693
Iteration 76/1000 | Loss: 0.00005811
Iteration 77/1000 | Loss: 0.00005526
Iteration 78/1000 | Loss: 0.00006026
Iteration 79/1000 | Loss: 0.00063555
Iteration 80/1000 | Loss: 0.00172021
Iteration 81/1000 | Loss: 0.00039759
Iteration 82/1000 | Loss: 0.00050427
Iteration 83/1000 | Loss: 0.00177690
Iteration 84/1000 | Loss: 0.00094989
Iteration 85/1000 | Loss: 0.00039310
Iteration 86/1000 | Loss: 0.00059397
Iteration 87/1000 | Loss: 0.00041920
Iteration 88/1000 | Loss: 0.00078539
Iteration 89/1000 | Loss: 0.00036935
Iteration 90/1000 | Loss: 0.00019486
Iteration 91/1000 | Loss: 0.00063490
Iteration 92/1000 | Loss: 0.00090860
Iteration 93/1000 | Loss: 0.00026012
Iteration 94/1000 | Loss: 0.00018866
Iteration 95/1000 | Loss: 0.00020405
Iteration 96/1000 | Loss: 0.00045736
Iteration 97/1000 | Loss: 0.00039024
Iteration 98/1000 | Loss: 0.00035206
Iteration 99/1000 | Loss: 0.00017253
Iteration 100/1000 | Loss: 0.00112150
Iteration 101/1000 | Loss: 0.00051049
Iteration 102/1000 | Loss: 0.00025755
Iteration 103/1000 | Loss: 0.00042272
Iteration 104/1000 | Loss: 0.00029094
Iteration 105/1000 | Loss: 0.00064642
Iteration 106/1000 | Loss: 0.00069364
Iteration 107/1000 | Loss: 0.00022310
Iteration 108/1000 | Loss: 0.00041847
Iteration 109/1000 | Loss: 0.00027947
Iteration 110/1000 | Loss: 0.00033451
Iteration 111/1000 | Loss: 0.00038627
Iteration 112/1000 | Loss: 0.00059395
Iteration 113/1000 | Loss: 0.00062183
Iteration 114/1000 | Loss: 0.00035759
Iteration 115/1000 | Loss: 0.00227149
Iteration 116/1000 | Loss: 0.00094161
Iteration 117/1000 | Loss: 0.00044214
Iteration 118/1000 | Loss: 0.00169661
Iteration 119/1000 | Loss: 0.00006695
Iteration 120/1000 | Loss: 0.00003159
Iteration 121/1000 | Loss: 0.00044407
Iteration 122/1000 | Loss: 0.00002852
Iteration 123/1000 | Loss: 0.00002648
Iteration 124/1000 | Loss: 0.00002529
Iteration 125/1000 | Loss: 0.00002410
Iteration 126/1000 | Loss: 0.00002349
Iteration 127/1000 | Loss: 0.00002305
Iteration 128/1000 | Loss: 0.00002270
Iteration 129/1000 | Loss: 0.00002243
Iteration 130/1000 | Loss: 0.00002224
Iteration 131/1000 | Loss: 0.00002199
Iteration 132/1000 | Loss: 0.00002187
Iteration 133/1000 | Loss: 0.00036335
Iteration 134/1000 | Loss: 0.00004684
Iteration 135/1000 | Loss: 0.00002202
Iteration 136/1000 | Loss: 0.00036044
Iteration 137/1000 | Loss: 0.00050177
Iteration 138/1000 | Loss: 0.00003003
Iteration 139/1000 | Loss: 0.00002492
Iteration 140/1000 | Loss: 0.00002342
Iteration 141/1000 | Loss: 0.00002285
Iteration 142/1000 | Loss: 0.00002257
Iteration 143/1000 | Loss: 0.00002239
Iteration 144/1000 | Loss: 0.00002221
Iteration 145/1000 | Loss: 0.00002218
Iteration 146/1000 | Loss: 0.00002214
Iteration 147/1000 | Loss: 0.00002212
Iteration 148/1000 | Loss: 0.00002211
Iteration 149/1000 | Loss: 0.00002210
Iteration 150/1000 | Loss: 0.00002207
Iteration 151/1000 | Loss: 0.00002206
Iteration 152/1000 | Loss: 0.00002205
Iteration 153/1000 | Loss: 0.00002205
Iteration 154/1000 | Loss: 0.00002205
Iteration 155/1000 | Loss: 0.00002204
Iteration 156/1000 | Loss: 0.00002204
Iteration 157/1000 | Loss: 0.00002204
Iteration 158/1000 | Loss: 0.00002203
Iteration 159/1000 | Loss: 0.00002203
Iteration 160/1000 | Loss: 0.00003929
Iteration 161/1000 | Loss: 0.00053541
Iteration 162/1000 | Loss: 0.00030558
Iteration 163/1000 | Loss: 0.00040890
Iteration 164/1000 | Loss: 0.00024589
Iteration 165/1000 | Loss: 0.00149936
Iteration 166/1000 | Loss: 0.00039822
Iteration 167/1000 | Loss: 0.00003106
Iteration 168/1000 | Loss: 0.00002598
Iteration 169/1000 | Loss: 0.00002428
Iteration 170/1000 | Loss: 0.00002312
Iteration 171/1000 | Loss: 0.00002251
Iteration 172/1000 | Loss: 0.00041579
Iteration 173/1000 | Loss: 0.00043917
Iteration 174/1000 | Loss: 0.00039249
Iteration 175/1000 | Loss: 0.00115757
Iteration 176/1000 | Loss: 0.00043202
Iteration 177/1000 | Loss: 0.00005047
Iteration 178/1000 | Loss: 0.00005373
Iteration 179/1000 | Loss: 0.00028678
Iteration 180/1000 | Loss: 0.00004673
Iteration 181/1000 | Loss: 0.00003000
Iteration 182/1000 | Loss: 0.00002224
Iteration 183/1000 | Loss: 0.00006706
Iteration 184/1000 | Loss: 0.00005716
Iteration 185/1000 | Loss: 0.00002062
Iteration 186/1000 | Loss: 0.00004662
Iteration 187/1000 | Loss: 0.00002002
Iteration 188/1000 | Loss: 0.00001983
Iteration 189/1000 | Loss: 0.00001969
Iteration 190/1000 | Loss: 0.00001967
Iteration 191/1000 | Loss: 0.00001963
Iteration 192/1000 | Loss: 0.00001963
Iteration 193/1000 | Loss: 0.00001963
Iteration 194/1000 | Loss: 0.00001962
Iteration 195/1000 | Loss: 0.00001962
Iteration 196/1000 | Loss: 0.00001962
Iteration 197/1000 | Loss: 0.00001961
Iteration 198/1000 | Loss: 0.00001961
Iteration 199/1000 | Loss: 0.00001961
Iteration 200/1000 | Loss: 0.00001960
Iteration 201/1000 | Loss: 0.00001960
Iteration 202/1000 | Loss: 0.00001959
Iteration 203/1000 | Loss: 0.00001959
Iteration 204/1000 | Loss: 0.00001959
Iteration 205/1000 | Loss: 0.00001958
Iteration 206/1000 | Loss: 0.00001957
Iteration 207/1000 | Loss: 0.00001956
Iteration 208/1000 | Loss: 0.00001955
Iteration 209/1000 | Loss: 0.00001955
Iteration 210/1000 | Loss: 0.00001955
Iteration 211/1000 | Loss: 0.00001955
Iteration 212/1000 | Loss: 0.00001954
Iteration 213/1000 | Loss: 0.00001954
Iteration 214/1000 | Loss: 0.00001953
Iteration 215/1000 | Loss: 0.00001953
Iteration 216/1000 | Loss: 0.00001953
Iteration 217/1000 | Loss: 0.00001952
Iteration 218/1000 | Loss: 0.00001952
Iteration 219/1000 | Loss: 0.00001952
Iteration 220/1000 | Loss: 0.00001952
Iteration 221/1000 | Loss: 0.00001952
Iteration 222/1000 | Loss: 0.00001952
Iteration 223/1000 | Loss: 0.00001952
Iteration 224/1000 | Loss: 0.00001952
Iteration 225/1000 | Loss: 0.00001951
Iteration 226/1000 | Loss: 0.00001951
Iteration 227/1000 | Loss: 0.00001951
Iteration 228/1000 | Loss: 0.00001951
Iteration 229/1000 | Loss: 0.00001951
Iteration 230/1000 | Loss: 0.00001951
Iteration 231/1000 | Loss: 0.00001951
Iteration 232/1000 | Loss: 0.00001951
Iteration 233/1000 | Loss: 0.00001951
Iteration 234/1000 | Loss: 0.00001951
Iteration 235/1000 | Loss: 0.00001950
Iteration 236/1000 | Loss: 0.00001950
Iteration 237/1000 | Loss: 0.00001950
Iteration 238/1000 | Loss: 0.00001950
Iteration 239/1000 | Loss: 0.00001950
Iteration 240/1000 | Loss: 0.00001950
Iteration 241/1000 | Loss: 0.00001950
Iteration 242/1000 | Loss: 0.00001950
Iteration 243/1000 | Loss: 0.00001949
Iteration 244/1000 | Loss: 0.00001949
Iteration 245/1000 | Loss: 0.00001949
Iteration 246/1000 | Loss: 0.00001949
Iteration 247/1000 | Loss: 0.00001949
Iteration 248/1000 | Loss: 0.00001949
Iteration 249/1000 | Loss: 0.00001949
Iteration 250/1000 | Loss: 0.00001949
Iteration 251/1000 | Loss: 0.00001949
Iteration 252/1000 | Loss: 0.00001949
Iteration 253/1000 | Loss: 0.00001949
Iteration 254/1000 | Loss: 0.00001949
Iteration 255/1000 | Loss: 0.00001949
Iteration 256/1000 | Loss: 0.00001948
Iteration 257/1000 | Loss: 0.00001948
Iteration 258/1000 | Loss: 0.00001948
Iteration 259/1000 | Loss: 0.00001947
Iteration 260/1000 | Loss: 0.00001947
Iteration 261/1000 | Loss: 0.00001947
Iteration 262/1000 | Loss: 0.00001947
Iteration 263/1000 | Loss: 0.00001946
Iteration 264/1000 | Loss: 0.00001946
Iteration 265/1000 | Loss: 0.00001946
Iteration 266/1000 | Loss: 0.00001946
Iteration 267/1000 | Loss: 0.00001946
Iteration 268/1000 | Loss: 0.00001946
Iteration 269/1000 | Loss: 0.00001946
Iteration 270/1000 | Loss: 0.00001946
Iteration 271/1000 | Loss: 0.00001946
Iteration 272/1000 | Loss: 0.00001946
Iteration 273/1000 | Loss: 0.00001945
Iteration 274/1000 | Loss: 0.00001945
Iteration 275/1000 | Loss: 0.00001945
Iteration 276/1000 | Loss: 0.00001945
Iteration 277/1000 | Loss: 0.00001945
Iteration 278/1000 | Loss: 0.00001945
Iteration 279/1000 | Loss: 0.00001944
Iteration 280/1000 | Loss: 0.00001944
Iteration 281/1000 | Loss: 0.00001944
Iteration 282/1000 | Loss: 0.00001944
Iteration 283/1000 | Loss: 0.00001944
Iteration 284/1000 | Loss: 0.00001944
Iteration 285/1000 | Loss: 0.00001943
Iteration 286/1000 | Loss: 0.00001943
Iteration 287/1000 | Loss: 0.00001943
Iteration 288/1000 | Loss: 0.00001943
Iteration 289/1000 | Loss: 0.00001943
Iteration 290/1000 | Loss: 0.00001943
Iteration 291/1000 | Loss: 0.00001943
Iteration 292/1000 | Loss: 0.00001943
Iteration 293/1000 | Loss: 0.00001943
Iteration 294/1000 | Loss: 0.00001942
Iteration 295/1000 | Loss: 0.00001942
Iteration 296/1000 | Loss: 0.00001942
Iteration 297/1000 | Loss: 0.00001942
Iteration 298/1000 | Loss: 0.00001941
Iteration 299/1000 | Loss: 0.00001941
Iteration 300/1000 | Loss: 0.00001941
Iteration 301/1000 | Loss: 0.00001941
Iteration 302/1000 | Loss: 0.00001941
Iteration 303/1000 | Loss: 0.00001941
Iteration 304/1000 | Loss: 0.00001941
Iteration 305/1000 | Loss: 0.00001941
Iteration 306/1000 | Loss: 0.00001941
Iteration 307/1000 | Loss: 0.00001941
Iteration 308/1000 | Loss: 0.00001941
Iteration 309/1000 | Loss: 0.00001940
Iteration 310/1000 | Loss: 0.00001940
Iteration 311/1000 | Loss: 0.00001940
Iteration 312/1000 | Loss: 0.00001940
Iteration 313/1000 | Loss: 0.00001939
Iteration 314/1000 | Loss: 0.00007721
Iteration 315/1000 | Loss: 0.00001947
Iteration 316/1000 | Loss: 0.00004064
Iteration 317/1000 | Loss: 0.00001943
Iteration 318/1000 | Loss: 0.00001938
Iteration 319/1000 | Loss: 0.00001938
Iteration 320/1000 | Loss: 0.00001937
Iteration 321/1000 | Loss: 0.00001937
Iteration 322/1000 | Loss: 0.00001937
Iteration 323/1000 | Loss: 0.00001937
Iteration 324/1000 | Loss: 0.00001936
Iteration 325/1000 | Loss: 0.00001936
Iteration 326/1000 | Loss: 0.00001936
Iteration 327/1000 | Loss: 0.00001935
Iteration 328/1000 | Loss: 0.00001935
Iteration 329/1000 | Loss: 0.00001935
Iteration 330/1000 | Loss: 0.00001935
Iteration 331/1000 | Loss: 0.00001935
Iteration 332/1000 | Loss: 0.00001934
Iteration 333/1000 | Loss: 0.00001934
Iteration 334/1000 | Loss: 0.00001934
Iteration 335/1000 | Loss: 0.00001934
Iteration 336/1000 | Loss: 0.00001934
Iteration 337/1000 | Loss: 0.00001934
Iteration 338/1000 | Loss: 0.00001934
Iteration 339/1000 | Loss: 0.00001934
Iteration 340/1000 | Loss: 0.00001934
Iteration 341/1000 | Loss: 0.00001934
Iteration 342/1000 | Loss: 0.00001934
Iteration 343/1000 | Loss: 0.00001933
Iteration 344/1000 | Loss: 0.00001933
Iteration 345/1000 | Loss: 0.00001933
Iteration 346/1000 | Loss: 0.00001933
Iteration 347/1000 | Loss: 0.00001933
Iteration 348/1000 | Loss: 0.00001933
Iteration 349/1000 | Loss: 0.00004528
Iteration 350/1000 | Loss: 0.00001937
Iteration 351/1000 | Loss: 0.00001937
Iteration 352/1000 | Loss: 0.00001935
Iteration 353/1000 | Loss: 0.00001934
Iteration 354/1000 | Loss: 0.00001934
Iteration 355/1000 | Loss: 0.00001934
Iteration 356/1000 | Loss: 0.00001934
Iteration 357/1000 | Loss: 0.00001934
Iteration 358/1000 | Loss: 0.00001934
Iteration 359/1000 | Loss: 0.00001934
Iteration 360/1000 | Loss: 0.00001934
Iteration 361/1000 | Loss: 0.00001934
Iteration 362/1000 | Loss: 0.00001934
Iteration 363/1000 | Loss: 0.00001934
Iteration 364/1000 | Loss: 0.00001934
Iteration 365/1000 | Loss: 0.00001934
Iteration 366/1000 | Loss: 0.00001934
Iteration 367/1000 | Loss: 0.00001934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 367. Stopping optimization.
Last 5 losses: [1.9338796846568584e-05, 1.9338796846568584e-05, 1.9338796846568584e-05, 1.9338796846568584e-05, 1.9338796846568584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9338796846568584e-05

Optimization complete. Final v2v error: 3.6590447425842285 mm

Highest mean error: 6.375733375549316 mm for frame 172

Lowest mean error: 3.1846704483032227 mm for frame 233

Saving results

Total time: 340.0531041622162
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795716
Iteration 2/25 | Loss: 0.00145695
Iteration 3/25 | Loss: 0.00126809
Iteration 4/25 | Loss: 0.00124824
Iteration 5/25 | Loss: 0.00124051
Iteration 6/25 | Loss: 0.00123978
Iteration 7/25 | Loss: 0.00123978
Iteration 8/25 | Loss: 0.00123978
Iteration 9/25 | Loss: 0.00123978
Iteration 10/25 | Loss: 0.00123978
Iteration 11/25 | Loss: 0.00123978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012397832470014691, 0.0012397832470014691, 0.0012397832470014691, 0.0012397832470014691, 0.0012397832470014691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012397832470014691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86471057
Iteration 2/25 | Loss: 0.00053063
Iteration 3/25 | Loss: 0.00053063
Iteration 4/25 | Loss: 0.00053063
Iteration 5/25 | Loss: 0.00053063
Iteration 6/25 | Loss: 0.00053063
Iteration 7/25 | Loss: 0.00053063
Iteration 8/25 | Loss: 0.00053063
Iteration 9/25 | Loss: 0.00053063
Iteration 10/25 | Loss: 0.00053063
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0005306322709657252, 0.0005306322709657252, 0.0005306322709657252, 0.0005306322709657252, 0.0005306322709657252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005306322709657252

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053063
Iteration 2/1000 | Loss: 0.00004601
Iteration 3/1000 | Loss: 0.00003218
Iteration 4/1000 | Loss: 0.00002733
Iteration 5/1000 | Loss: 0.00002573
Iteration 6/1000 | Loss: 0.00002460
Iteration 7/1000 | Loss: 0.00002373
Iteration 8/1000 | Loss: 0.00002330
Iteration 9/1000 | Loss: 0.00002289
Iteration 10/1000 | Loss: 0.00002249
Iteration 11/1000 | Loss: 0.00002226
Iteration 12/1000 | Loss: 0.00002216
Iteration 13/1000 | Loss: 0.00002207
Iteration 14/1000 | Loss: 0.00002207
Iteration 15/1000 | Loss: 0.00002195
Iteration 16/1000 | Loss: 0.00002191
Iteration 17/1000 | Loss: 0.00002183
Iteration 18/1000 | Loss: 0.00002176
Iteration 19/1000 | Loss: 0.00002175
Iteration 20/1000 | Loss: 0.00002175
Iteration 21/1000 | Loss: 0.00002171
Iteration 22/1000 | Loss: 0.00002171
Iteration 23/1000 | Loss: 0.00002170
Iteration 24/1000 | Loss: 0.00002170
Iteration 25/1000 | Loss: 0.00002169
Iteration 26/1000 | Loss: 0.00002169
Iteration 27/1000 | Loss: 0.00002169
Iteration 28/1000 | Loss: 0.00002167
Iteration 29/1000 | Loss: 0.00002166
Iteration 30/1000 | Loss: 0.00002166
Iteration 31/1000 | Loss: 0.00002166
Iteration 32/1000 | Loss: 0.00002163
Iteration 33/1000 | Loss: 0.00002163
Iteration 34/1000 | Loss: 0.00002162
Iteration 35/1000 | Loss: 0.00002161
Iteration 36/1000 | Loss: 0.00002161
Iteration 37/1000 | Loss: 0.00002161
Iteration 38/1000 | Loss: 0.00002160
Iteration 39/1000 | Loss: 0.00002160
Iteration 40/1000 | Loss: 0.00002160
Iteration 41/1000 | Loss: 0.00002160
Iteration 42/1000 | Loss: 0.00002160
Iteration 43/1000 | Loss: 0.00002159
Iteration 44/1000 | Loss: 0.00002159
Iteration 45/1000 | Loss: 0.00002159
Iteration 46/1000 | Loss: 0.00002159
Iteration 47/1000 | Loss: 0.00002158
Iteration 48/1000 | Loss: 0.00002158
Iteration 49/1000 | Loss: 0.00002158
Iteration 50/1000 | Loss: 0.00002158
Iteration 51/1000 | Loss: 0.00002158
Iteration 52/1000 | Loss: 0.00002158
Iteration 53/1000 | Loss: 0.00002157
Iteration 54/1000 | Loss: 0.00002157
Iteration 55/1000 | Loss: 0.00002157
Iteration 56/1000 | Loss: 0.00002157
Iteration 57/1000 | Loss: 0.00002157
Iteration 58/1000 | Loss: 0.00002157
Iteration 59/1000 | Loss: 0.00002157
Iteration 60/1000 | Loss: 0.00002157
Iteration 61/1000 | Loss: 0.00002157
Iteration 62/1000 | Loss: 0.00002157
Iteration 63/1000 | Loss: 0.00002156
Iteration 64/1000 | Loss: 0.00002156
Iteration 65/1000 | Loss: 0.00002156
Iteration 66/1000 | Loss: 0.00002156
Iteration 67/1000 | Loss: 0.00002156
Iteration 68/1000 | Loss: 0.00002156
Iteration 69/1000 | Loss: 0.00002156
Iteration 70/1000 | Loss: 0.00002156
Iteration 71/1000 | Loss: 0.00002156
Iteration 72/1000 | Loss: 0.00002156
Iteration 73/1000 | Loss: 0.00002156
Iteration 74/1000 | Loss: 0.00002156
Iteration 75/1000 | Loss: 0.00002156
Iteration 76/1000 | Loss: 0.00002156
Iteration 77/1000 | Loss: 0.00002156
Iteration 78/1000 | Loss: 0.00002156
Iteration 79/1000 | Loss: 0.00002156
Iteration 80/1000 | Loss: 0.00002156
Iteration 81/1000 | Loss: 0.00002156
Iteration 82/1000 | Loss: 0.00002156
Iteration 83/1000 | Loss: 0.00002156
Iteration 84/1000 | Loss: 0.00002156
Iteration 85/1000 | Loss: 0.00002156
Iteration 86/1000 | Loss: 0.00002156
Iteration 87/1000 | Loss: 0.00002156
Iteration 88/1000 | Loss: 0.00002156
Iteration 89/1000 | Loss: 0.00002156
Iteration 90/1000 | Loss: 0.00002156
Iteration 91/1000 | Loss: 0.00002156
Iteration 92/1000 | Loss: 0.00002156
Iteration 93/1000 | Loss: 0.00002156
Iteration 94/1000 | Loss: 0.00002156
Iteration 95/1000 | Loss: 0.00002156
Iteration 96/1000 | Loss: 0.00002156
Iteration 97/1000 | Loss: 0.00002156
Iteration 98/1000 | Loss: 0.00002156
Iteration 99/1000 | Loss: 0.00002156
Iteration 100/1000 | Loss: 0.00002156
Iteration 101/1000 | Loss: 0.00002156
Iteration 102/1000 | Loss: 0.00002156
Iteration 103/1000 | Loss: 0.00002156
Iteration 104/1000 | Loss: 0.00002156
Iteration 105/1000 | Loss: 0.00002156
Iteration 106/1000 | Loss: 0.00002156
Iteration 107/1000 | Loss: 0.00002156
Iteration 108/1000 | Loss: 0.00002156
Iteration 109/1000 | Loss: 0.00002156
Iteration 110/1000 | Loss: 0.00002156
Iteration 111/1000 | Loss: 0.00002156
Iteration 112/1000 | Loss: 0.00002156
Iteration 113/1000 | Loss: 0.00002156
Iteration 114/1000 | Loss: 0.00002156
Iteration 115/1000 | Loss: 0.00002156
Iteration 116/1000 | Loss: 0.00002156
Iteration 117/1000 | Loss: 0.00002156
Iteration 118/1000 | Loss: 0.00002156
Iteration 119/1000 | Loss: 0.00002156
Iteration 120/1000 | Loss: 0.00002156
Iteration 121/1000 | Loss: 0.00002156
Iteration 122/1000 | Loss: 0.00002156
Iteration 123/1000 | Loss: 0.00002156
Iteration 124/1000 | Loss: 0.00002156
Iteration 125/1000 | Loss: 0.00002156
Iteration 126/1000 | Loss: 0.00002156
Iteration 127/1000 | Loss: 0.00002156
Iteration 128/1000 | Loss: 0.00002156
Iteration 129/1000 | Loss: 0.00002156
Iteration 130/1000 | Loss: 0.00002156
Iteration 131/1000 | Loss: 0.00002156
Iteration 132/1000 | Loss: 0.00002156
Iteration 133/1000 | Loss: 0.00002156
Iteration 134/1000 | Loss: 0.00002156
Iteration 135/1000 | Loss: 0.00002156
Iteration 136/1000 | Loss: 0.00002156
Iteration 137/1000 | Loss: 0.00002156
Iteration 138/1000 | Loss: 0.00002156
Iteration 139/1000 | Loss: 0.00002156
Iteration 140/1000 | Loss: 0.00002156
Iteration 141/1000 | Loss: 0.00002156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.1563595510087907e-05, 2.1563595510087907e-05, 2.1563595510087907e-05, 2.1563595510087907e-05, 2.1563595510087907e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1563595510087907e-05

Optimization complete. Final v2v error: 3.923863649368286 mm

Highest mean error: 4.249990463256836 mm for frame 99

Lowest mean error: 3.5767874717712402 mm for frame 40

Saving results

Total time: 38.57246255874634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010884
Iteration 2/25 | Loss: 0.00277372
Iteration 3/25 | Loss: 0.00214343
Iteration 4/25 | Loss: 0.00193876
Iteration 5/25 | Loss: 0.00166803
Iteration 6/25 | Loss: 0.00187584
Iteration 7/25 | Loss: 0.00146700
Iteration 8/25 | Loss: 0.00130035
Iteration 9/25 | Loss: 0.00127034
Iteration 10/25 | Loss: 0.00126972
Iteration 11/25 | Loss: 0.00126947
Iteration 12/25 | Loss: 0.00126936
Iteration 13/25 | Loss: 0.00126933
Iteration 14/25 | Loss: 0.00126932
Iteration 15/25 | Loss: 0.00126932
Iteration 16/25 | Loss: 0.00126932
Iteration 17/25 | Loss: 0.00126932
Iteration 18/25 | Loss: 0.00126932
Iteration 19/25 | Loss: 0.00126932
Iteration 20/25 | Loss: 0.00126932
Iteration 21/25 | Loss: 0.00126932
Iteration 22/25 | Loss: 0.00126932
Iteration 23/25 | Loss: 0.00126931
Iteration 24/25 | Loss: 0.00126931
Iteration 25/25 | Loss: 0.00126931

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42267668
Iteration 2/25 | Loss: 0.00067318
Iteration 3/25 | Loss: 0.00067318
Iteration 4/25 | Loss: 0.00067317
Iteration 5/25 | Loss: 0.00067317
Iteration 6/25 | Loss: 0.00067317
Iteration 7/25 | Loss: 0.00067317
Iteration 8/25 | Loss: 0.00067317
Iteration 9/25 | Loss: 0.00067317
Iteration 10/25 | Loss: 0.00067317
Iteration 11/25 | Loss: 0.00067317
Iteration 12/25 | Loss: 0.00067317
Iteration 13/25 | Loss: 0.00067317
Iteration 14/25 | Loss: 0.00067317
Iteration 15/25 | Loss: 0.00067317
Iteration 16/25 | Loss: 0.00067317
Iteration 17/25 | Loss: 0.00067317
Iteration 18/25 | Loss: 0.00067317
Iteration 19/25 | Loss: 0.00067317
Iteration 20/25 | Loss: 0.00067317
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000673172005917877, 0.000673172005917877, 0.000673172005917877, 0.000673172005917877, 0.000673172005917877]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000673172005917877

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067317
Iteration 2/1000 | Loss: 0.00003602
Iteration 3/1000 | Loss: 0.00002491
Iteration 4/1000 | Loss: 0.00002242
Iteration 5/1000 | Loss: 0.00002156
Iteration 6/1000 | Loss: 0.00002109
Iteration 7/1000 | Loss: 0.00002071
Iteration 8/1000 | Loss: 0.00002050
Iteration 9/1000 | Loss: 0.00002033
Iteration 10/1000 | Loss: 0.00002033
Iteration 11/1000 | Loss: 0.00002031
Iteration 12/1000 | Loss: 0.00002027
Iteration 13/1000 | Loss: 0.00002015
Iteration 14/1000 | Loss: 0.00002001
Iteration 15/1000 | Loss: 0.00002000
Iteration 16/1000 | Loss: 0.00001998
Iteration 17/1000 | Loss: 0.00001995
Iteration 18/1000 | Loss: 0.00001991
Iteration 19/1000 | Loss: 0.00001991
Iteration 20/1000 | Loss: 0.00001990
Iteration 21/1000 | Loss: 0.00001989
Iteration 22/1000 | Loss: 0.00001989
Iteration 23/1000 | Loss: 0.00001989
Iteration 24/1000 | Loss: 0.00001988
Iteration 25/1000 | Loss: 0.00001988
Iteration 26/1000 | Loss: 0.00001987
Iteration 27/1000 | Loss: 0.00001987
Iteration 28/1000 | Loss: 0.00001987
Iteration 29/1000 | Loss: 0.00001986
Iteration 30/1000 | Loss: 0.00001986
Iteration 31/1000 | Loss: 0.00001986
Iteration 32/1000 | Loss: 0.00001986
Iteration 33/1000 | Loss: 0.00001986
Iteration 34/1000 | Loss: 0.00001985
Iteration 35/1000 | Loss: 0.00001985
Iteration 36/1000 | Loss: 0.00001985
Iteration 37/1000 | Loss: 0.00001985
Iteration 38/1000 | Loss: 0.00001985
Iteration 39/1000 | Loss: 0.00001984
Iteration 40/1000 | Loss: 0.00001984
Iteration 41/1000 | Loss: 0.00001983
Iteration 42/1000 | Loss: 0.00001983
Iteration 43/1000 | Loss: 0.00001982
Iteration 44/1000 | Loss: 0.00001982
Iteration 45/1000 | Loss: 0.00001982
Iteration 46/1000 | Loss: 0.00001982
Iteration 47/1000 | Loss: 0.00001982
Iteration 48/1000 | Loss: 0.00001982
Iteration 49/1000 | Loss: 0.00001982
Iteration 50/1000 | Loss: 0.00001982
Iteration 51/1000 | Loss: 0.00001982
Iteration 52/1000 | Loss: 0.00001982
Iteration 53/1000 | Loss: 0.00001982
Iteration 54/1000 | Loss: 0.00001982
Iteration 55/1000 | Loss: 0.00001982
Iteration 56/1000 | Loss: 0.00001982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [1.9816738131339662e-05, 1.9816738131339662e-05, 1.9816738131339662e-05, 1.9816738131339662e-05, 1.9816738131339662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9816738131339662e-05

Optimization complete. Final v2v error: 3.6939995288848877 mm

Highest mean error: 3.7345404624938965 mm for frame 100

Lowest mean error: 3.582043170928955 mm for frame 19

Saving results

Total time: 40.45377826690674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031076
Iteration 2/25 | Loss: 0.01031076
Iteration 3/25 | Loss: 0.01031076
Iteration 4/25 | Loss: 0.01031076
Iteration 5/25 | Loss: 0.01031076
Iteration 6/25 | Loss: 0.01031076
Iteration 7/25 | Loss: 0.01031075
Iteration 8/25 | Loss: 0.01031075
Iteration 9/25 | Loss: 0.01031075
Iteration 10/25 | Loss: 0.01031075
Iteration 11/25 | Loss: 0.01031075
Iteration 12/25 | Loss: 0.01031075
Iteration 13/25 | Loss: 0.01031075
Iteration 14/25 | Loss: 0.01031075
Iteration 15/25 | Loss: 0.01031074
Iteration 16/25 | Loss: 0.01031074
Iteration 17/25 | Loss: 0.01031074
Iteration 18/25 | Loss: 0.01031074
Iteration 19/25 | Loss: 0.01031074
Iteration 20/25 | Loss: 0.01031074
Iteration 21/25 | Loss: 0.01031074
Iteration 22/25 | Loss: 0.01031074
Iteration 23/25 | Loss: 0.01031074
Iteration 24/25 | Loss: 0.01031074
Iteration 25/25 | Loss: 0.01031073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41164231
Iteration 2/25 | Loss: 0.18322536
Iteration 3/25 | Loss: 0.18134186
Iteration 4/25 | Loss: 0.17842452
Iteration 5/25 | Loss: 0.17835583
Iteration 6/25 | Loss: 0.17835577
Iteration 7/25 | Loss: 0.17835577
Iteration 8/25 | Loss: 0.17835577
Iteration 9/25 | Loss: 0.17835577
Iteration 10/25 | Loss: 0.17835577
Iteration 11/25 | Loss: 0.17835577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.17835576832294464, 0.17835576832294464, 0.17835576832294464, 0.17835576832294464, 0.17835576832294464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17835576832294464

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17835577
Iteration 2/1000 | Loss: 0.00611163
Iteration 3/1000 | Loss: 0.00064604
Iteration 4/1000 | Loss: 0.00255960
Iteration 5/1000 | Loss: 0.00045288
Iteration 6/1000 | Loss: 0.00033067
Iteration 7/1000 | Loss: 0.00009572
Iteration 8/1000 | Loss: 0.00003292
Iteration 9/1000 | Loss: 0.00016496
Iteration 10/1000 | Loss: 0.00080779
Iteration 11/1000 | Loss: 0.00018477
Iteration 12/1000 | Loss: 0.00010558
Iteration 13/1000 | Loss: 0.00002344
Iteration 14/1000 | Loss: 0.00007917
Iteration 15/1000 | Loss: 0.00010042
Iteration 16/1000 | Loss: 0.00001930
Iteration 17/1000 | Loss: 0.00001807
Iteration 18/1000 | Loss: 0.00010619
Iteration 19/1000 | Loss: 0.00001688
Iteration 20/1000 | Loss: 0.00017027
Iteration 21/1000 | Loss: 0.00051568
Iteration 22/1000 | Loss: 0.00003155
Iteration 23/1000 | Loss: 0.00017422
Iteration 24/1000 | Loss: 0.00002452
Iteration 25/1000 | Loss: 0.00002711
Iteration 26/1000 | Loss: 0.00001538
Iteration 27/1000 | Loss: 0.00001506
Iteration 28/1000 | Loss: 0.00001476
Iteration 29/1000 | Loss: 0.00001444
Iteration 30/1000 | Loss: 0.00001424
Iteration 31/1000 | Loss: 0.00001408
Iteration 32/1000 | Loss: 0.00020057
Iteration 33/1000 | Loss: 0.00019128
Iteration 34/1000 | Loss: 0.00014141
Iteration 35/1000 | Loss: 0.00001411
Iteration 36/1000 | Loss: 0.00012727
Iteration 37/1000 | Loss: 0.00002815
Iteration 38/1000 | Loss: 0.00001391
Iteration 39/1000 | Loss: 0.00001388
Iteration 40/1000 | Loss: 0.00004363
Iteration 41/1000 | Loss: 0.00001380
Iteration 42/1000 | Loss: 0.00001363
Iteration 43/1000 | Loss: 0.00001357
Iteration 44/1000 | Loss: 0.00001346
Iteration 45/1000 | Loss: 0.00001346
Iteration 46/1000 | Loss: 0.00001341
Iteration 47/1000 | Loss: 0.00001341
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001338
Iteration 51/1000 | Loss: 0.00001337
Iteration 52/1000 | Loss: 0.00001336
Iteration 53/1000 | Loss: 0.00001336
Iteration 54/1000 | Loss: 0.00001335
Iteration 55/1000 | Loss: 0.00001335
Iteration 56/1000 | Loss: 0.00001334
Iteration 57/1000 | Loss: 0.00001334
Iteration 58/1000 | Loss: 0.00001334
Iteration 59/1000 | Loss: 0.00001334
Iteration 60/1000 | Loss: 0.00001334
Iteration 61/1000 | Loss: 0.00001333
Iteration 62/1000 | Loss: 0.00001333
Iteration 63/1000 | Loss: 0.00001333
Iteration 64/1000 | Loss: 0.00001333
Iteration 65/1000 | Loss: 0.00001333
Iteration 66/1000 | Loss: 0.00001333
Iteration 67/1000 | Loss: 0.00001333
Iteration 68/1000 | Loss: 0.00001333
Iteration 69/1000 | Loss: 0.00001333
Iteration 70/1000 | Loss: 0.00001333
Iteration 71/1000 | Loss: 0.00001332
Iteration 72/1000 | Loss: 0.00001332
Iteration 73/1000 | Loss: 0.00001332
Iteration 74/1000 | Loss: 0.00001331
Iteration 75/1000 | Loss: 0.00001331
Iteration 76/1000 | Loss: 0.00001331
Iteration 77/1000 | Loss: 0.00001331
Iteration 78/1000 | Loss: 0.00001331
Iteration 79/1000 | Loss: 0.00001331
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001328
Iteration 87/1000 | Loss: 0.00001328
Iteration 88/1000 | Loss: 0.00001328
Iteration 89/1000 | Loss: 0.00001327
Iteration 90/1000 | Loss: 0.00001327
Iteration 91/1000 | Loss: 0.00001327
Iteration 92/1000 | Loss: 0.00001323
Iteration 93/1000 | Loss: 0.00001323
Iteration 94/1000 | Loss: 0.00001323
Iteration 95/1000 | Loss: 0.00001322
Iteration 96/1000 | Loss: 0.00001321
Iteration 97/1000 | Loss: 0.00001321
Iteration 98/1000 | Loss: 0.00001320
Iteration 99/1000 | Loss: 0.00001320
Iteration 100/1000 | Loss: 0.00001320
Iteration 101/1000 | Loss: 0.00001319
Iteration 102/1000 | Loss: 0.00001319
Iteration 103/1000 | Loss: 0.00001319
Iteration 104/1000 | Loss: 0.00001318
Iteration 105/1000 | Loss: 0.00001318
Iteration 106/1000 | Loss: 0.00001318
Iteration 107/1000 | Loss: 0.00001318
Iteration 108/1000 | Loss: 0.00001317
Iteration 109/1000 | Loss: 0.00001317
Iteration 110/1000 | Loss: 0.00001317
Iteration 111/1000 | Loss: 0.00001316
Iteration 112/1000 | Loss: 0.00001316
Iteration 113/1000 | Loss: 0.00001316
Iteration 114/1000 | Loss: 0.00001316
Iteration 115/1000 | Loss: 0.00001315
Iteration 116/1000 | Loss: 0.00001315
Iteration 117/1000 | Loss: 0.00001315
Iteration 118/1000 | Loss: 0.00001315
Iteration 119/1000 | Loss: 0.00001315
Iteration 120/1000 | Loss: 0.00001314
Iteration 121/1000 | Loss: 0.00001314
Iteration 122/1000 | Loss: 0.00001314
Iteration 123/1000 | Loss: 0.00001314
Iteration 124/1000 | Loss: 0.00001314
Iteration 125/1000 | Loss: 0.00001314
Iteration 126/1000 | Loss: 0.00001314
Iteration 127/1000 | Loss: 0.00001314
Iteration 128/1000 | Loss: 0.00001314
Iteration 129/1000 | Loss: 0.00001314
Iteration 130/1000 | Loss: 0.00001314
Iteration 131/1000 | Loss: 0.00001313
Iteration 132/1000 | Loss: 0.00001313
Iteration 133/1000 | Loss: 0.00001313
Iteration 134/1000 | Loss: 0.00001313
Iteration 135/1000 | Loss: 0.00001313
Iteration 136/1000 | Loss: 0.00001313
Iteration 137/1000 | Loss: 0.00001313
Iteration 138/1000 | Loss: 0.00001313
Iteration 139/1000 | Loss: 0.00001313
Iteration 140/1000 | Loss: 0.00001313
Iteration 141/1000 | Loss: 0.00001313
Iteration 142/1000 | Loss: 0.00001313
Iteration 143/1000 | Loss: 0.00001313
Iteration 144/1000 | Loss: 0.00001312
Iteration 145/1000 | Loss: 0.00001312
Iteration 146/1000 | Loss: 0.00001312
Iteration 147/1000 | Loss: 0.00001312
Iteration 148/1000 | Loss: 0.00001311
Iteration 149/1000 | Loss: 0.00001311
Iteration 150/1000 | Loss: 0.00001311
Iteration 151/1000 | Loss: 0.00001311
Iteration 152/1000 | Loss: 0.00001310
Iteration 153/1000 | Loss: 0.00001310
Iteration 154/1000 | Loss: 0.00001310
Iteration 155/1000 | Loss: 0.00001310
Iteration 156/1000 | Loss: 0.00001310
Iteration 157/1000 | Loss: 0.00001309
Iteration 158/1000 | Loss: 0.00001309
Iteration 159/1000 | Loss: 0.00001309
Iteration 160/1000 | Loss: 0.00001309
Iteration 161/1000 | Loss: 0.00001309
Iteration 162/1000 | Loss: 0.00001309
Iteration 163/1000 | Loss: 0.00001309
Iteration 164/1000 | Loss: 0.00001308
Iteration 165/1000 | Loss: 0.00001308
Iteration 166/1000 | Loss: 0.00001308
Iteration 167/1000 | Loss: 0.00001308
Iteration 168/1000 | Loss: 0.00001308
Iteration 169/1000 | Loss: 0.00001308
Iteration 170/1000 | Loss: 0.00001308
Iteration 171/1000 | Loss: 0.00001308
Iteration 172/1000 | Loss: 0.00001308
Iteration 173/1000 | Loss: 0.00001308
Iteration 174/1000 | Loss: 0.00001308
Iteration 175/1000 | Loss: 0.00001308
Iteration 176/1000 | Loss: 0.00001308
Iteration 177/1000 | Loss: 0.00001308
Iteration 178/1000 | Loss: 0.00001308
Iteration 179/1000 | Loss: 0.00001308
Iteration 180/1000 | Loss: 0.00001308
Iteration 181/1000 | Loss: 0.00001308
Iteration 182/1000 | Loss: 0.00001308
Iteration 183/1000 | Loss: 0.00001308
Iteration 184/1000 | Loss: 0.00001308
Iteration 185/1000 | Loss: 0.00001307
Iteration 186/1000 | Loss: 0.00001307
Iteration 187/1000 | Loss: 0.00001307
Iteration 188/1000 | Loss: 0.00001307
Iteration 189/1000 | Loss: 0.00001307
Iteration 190/1000 | Loss: 0.00001307
Iteration 191/1000 | Loss: 0.00001307
Iteration 192/1000 | Loss: 0.00001307
Iteration 193/1000 | Loss: 0.00001307
Iteration 194/1000 | Loss: 0.00001307
Iteration 195/1000 | Loss: 0.00001307
Iteration 196/1000 | Loss: 0.00001307
Iteration 197/1000 | Loss: 0.00001307
Iteration 198/1000 | Loss: 0.00001307
Iteration 199/1000 | Loss: 0.00001307
Iteration 200/1000 | Loss: 0.00001307
Iteration 201/1000 | Loss: 0.00001307
Iteration 202/1000 | Loss: 0.00001307
Iteration 203/1000 | Loss: 0.00001307
Iteration 204/1000 | Loss: 0.00001306
Iteration 205/1000 | Loss: 0.00001306
Iteration 206/1000 | Loss: 0.00001306
Iteration 207/1000 | Loss: 0.00001306
Iteration 208/1000 | Loss: 0.00001306
Iteration 209/1000 | Loss: 0.00001306
Iteration 210/1000 | Loss: 0.00001306
Iteration 211/1000 | Loss: 0.00001306
Iteration 212/1000 | Loss: 0.00001306
Iteration 213/1000 | Loss: 0.00001306
Iteration 214/1000 | Loss: 0.00001306
Iteration 215/1000 | Loss: 0.00001306
Iteration 216/1000 | Loss: 0.00001306
Iteration 217/1000 | Loss: 0.00001306
Iteration 218/1000 | Loss: 0.00001305
Iteration 219/1000 | Loss: 0.00001305
Iteration 220/1000 | Loss: 0.00001305
Iteration 221/1000 | Loss: 0.00001305
Iteration 222/1000 | Loss: 0.00001305
Iteration 223/1000 | Loss: 0.00001305
Iteration 224/1000 | Loss: 0.00001305
Iteration 225/1000 | Loss: 0.00001305
Iteration 226/1000 | Loss: 0.00001305
Iteration 227/1000 | Loss: 0.00001305
Iteration 228/1000 | Loss: 0.00001305
Iteration 229/1000 | Loss: 0.00001305
Iteration 230/1000 | Loss: 0.00001305
Iteration 231/1000 | Loss: 0.00001305
Iteration 232/1000 | Loss: 0.00001304
Iteration 233/1000 | Loss: 0.00001304
Iteration 234/1000 | Loss: 0.00001304
Iteration 235/1000 | Loss: 0.00001304
Iteration 236/1000 | Loss: 0.00001304
Iteration 237/1000 | Loss: 0.00001304
Iteration 238/1000 | Loss: 0.00001304
Iteration 239/1000 | Loss: 0.00001304
Iteration 240/1000 | Loss: 0.00001304
Iteration 241/1000 | Loss: 0.00001304
Iteration 242/1000 | Loss: 0.00001304
Iteration 243/1000 | Loss: 0.00001304
Iteration 244/1000 | Loss: 0.00001303
Iteration 245/1000 | Loss: 0.00001303
Iteration 246/1000 | Loss: 0.00001303
Iteration 247/1000 | Loss: 0.00001303
Iteration 248/1000 | Loss: 0.00001303
Iteration 249/1000 | Loss: 0.00001303
Iteration 250/1000 | Loss: 0.00001303
Iteration 251/1000 | Loss: 0.00001303
Iteration 252/1000 | Loss: 0.00001303
Iteration 253/1000 | Loss: 0.00001303
Iteration 254/1000 | Loss: 0.00001303
Iteration 255/1000 | Loss: 0.00001303
Iteration 256/1000 | Loss: 0.00001303
Iteration 257/1000 | Loss: 0.00001303
Iteration 258/1000 | Loss: 0.00001303
Iteration 259/1000 | Loss: 0.00001303
Iteration 260/1000 | Loss: 0.00001303
Iteration 261/1000 | Loss: 0.00001303
Iteration 262/1000 | Loss: 0.00001303
Iteration 263/1000 | Loss: 0.00001303
Iteration 264/1000 | Loss: 0.00001303
Iteration 265/1000 | Loss: 0.00001303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [1.3033512914262246e-05, 1.3033512914262246e-05, 1.3033512914262246e-05, 1.3033512914262246e-05, 1.3033512914262246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3033512914262246e-05

Optimization complete. Final v2v error: 3.0197269916534424 mm

Highest mean error: 3.4753220081329346 mm for frame 149

Lowest mean error: 2.8836863040924072 mm for frame 198

Saving results

Total time: 90.5648124217987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925800
Iteration 2/25 | Loss: 0.00175056
Iteration 3/25 | Loss: 0.00140836
Iteration 4/25 | Loss: 0.00137790
Iteration 5/25 | Loss: 0.00136837
Iteration 6/25 | Loss: 0.00136693
Iteration 7/25 | Loss: 0.00136693
Iteration 8/25 | Loss: 0.00136693
Iteration 9/25 | Loss: 0.00136693
Iteration 10/25 | Loss: 0.00136693
Iteration 11/25 | Loss: 0.00136693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00136693159583956, 0.00136693159583956, 0.00136693159583956, 0.00136693159583956, 0.00136693159583956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00136693159583956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01249313
Iteration 2/25 | Loss: 0.00075716
Iteration 3/25 | Loss: 0.00075715
Iteration 4/25 | Loss: 0.00075715
Iteration 5/25 | Loss: 0.00075715
Iteration 6/25 | Loss: 0.00075715
Iteration 7/25 | Loss: 0.00075715
Iteration 8/25 | Loss: 0.00075715
Iteration 9/25 | Loss: 0.00075715
Iteration 10/25 | Loss: 0.00075715
Iteration 11/25 | Loss: 0.00075715
Iteration 12/25 | Loss: 0.00075715
Iteration 13/25 | Loss: 0.00075715
Iteration 14/25 | Loss: 0.00075715
Iteration 15/25 | Loss: 0.00075715
Iteration 16/25 | Loss: 0.00075715
Iteration 17/25 | Loss: 0.00075715
Iteration 18/25 | Loss: 0.00075715
Iteration 19/25 | Loss: 0.00075715
Iteration 20/25 | Loss: 0.00075715
Iteration 21/25 | Loss: 0.00075715
Iteration 22/25 | Loss: 0.00075715
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000757149769924581, 0.000757149769924581, 0.000757149769924581, 0.000757149769924581, 0.000757149769924581]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000757149769924581

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075715
Iteration 2/1000 | Loss: 0.00006478
Iteration 3/1000 | Loss: 0.00004796
Iteration 4/1000 | Loss: 0.00004276
Iteration 5/1000 | Loss: 0.00004138
Iteration 6/1000 | Loss: 0.00003978
Iteration 7/1000 | Loss: 0.00003875
Iteration 8/1000 | Loss: 0.00003807
Iteration 9/1000 | Loss: 0.00003756
Iteration 10/1000 | Loss: 0.00003712
Iteration 11/1000 | Loss: 0.00003673
Iteration 12/1000 | Loss: 0.00003635
Iteration 13/1000 | Loss: 0.00003597
Iteration 14/1000 | Loss: 0.00003564
Iteration 15/1000 | Loss: 0.00003536
Iteration 16/1000 | Loss: 0.00003507
Iteration 17/1000 | Loss: 0.00003484
Iteration 18/1000 | Loss: 0.00003465
Iteration 19/1000 | Loss: 0.00003450
Iteration 20/1000 | Loss: 0.00003437
Iteration 21/1000 | Loss: 0.00003435
Iteration 22/1000 | Loss: 0.00003427
Iteration 23/1000 | Loss: 0.00003423
Iteration 24/1000 | Loss: 0.00003423
Iteration 25/1000 | Loss: 0.00003423
Iteration 26/1000 | Loss: 0.00003422
Iteration 27/1000 | Loss: 0.00003422
Iteration 28/1000 | Loss: 0.00003421
Iteration 29/1000 | Loss: 0.00003418
Iteration 30/1000 | Loss: 0.00003418
Iteration 31/1000 | Loss: 0.00003417
Iteration 32/1000 | Loss: 0.00003416
Iteration 33/1000 | Loss: 0.00003416
Iteration 34/1000 | Loss: 0.00003415
Iteration 35/1000 | Loss: 0.00003415
Iteration 36/1000 | Loss: 0.00003414
Iteration 37/1000 | Loss: 0.00003414
Iteration 38/1000 | Loss: 0.00003413
Iteration 39/1000 | Loss: 0.00003411
Iteration 40/1000 | Loss: 0.00003410
Iteration 41/1000 | Loss: 0.00003409
Iteration 42/1000 | Loss: 0.00003409
Iteration 43/1000 | Loss: 0.00003409
Iteration 44/1000 | Loss: 0.00003409
Iteration 45/1000 | Loss: 0.00003409
Iteration 46/1000 | Loss: 0.00003409
Iteration 47/1000 | Loss: 0.00003409
Iteration 48/1000 | Loss: 0.00003409
Iteration 49/1000 | Loss: 0.00003409
Iteration 50/1000 | Loss: 0.00003409
Iteration 51/1000 | Loss: 0.00003408
Iteration 52/1000 | Loss: 0.00003408
Iteration 53/1000 | Loss: 0.00003408
Iteration 54/1000 | Loss: 0.00003408
Iteration 55/1000 | Loss: 0.00003407
Iteration 56/1000 | Loss: 0.00003407
Iteration 57/1000 | Loss: 0.00003407
Iteration 58/1000 | Loss: 0.00003406
Iteration 59/1000 | Loss: 0.00003406
Iteration 60/1000 | Loss: 0.00003406
Iteration 61/1000 | Loss: 0.00003406
Iteration 62/1000 | Loss: 0.00003406
Iteration 63/1000 | Loss: 0.00003405
Iteration 64/1000 | Loss: 0.00003405
Iteration 65/1000 | Loss: 0.00003405
Iteration 66/1000 | Loss: 0.00003405
Iteration 67/1000 | Loss: 0.00003404
Iteration 68/1000 | Loss: 0.00003404
Iteration 69/1000 | Loss: 0.00003404
Iteration 70/1000 | Loss: 0.00003404
Iteration 71/1000 | Loss: 0.00003404
Iteration 72/1000 | Loss: 0.00003404
Iteration 73/1000 | Loss: 0.00003403
Iteration 74/1000 | Loss: 0.00003403
Iteration 75/1000 | Loss: 0.00003403
Iteration 76/1000 | Loss: 0.00003403
Iteration 77/1000 | Loss: 0.00003403
Iteration 78/1000 | Loss: 0.00003403
Iteration 79/1000 | Loss: 0.00003403
Iteration 80/1000 | Loss: 0.00003402
Iteration 81/1000 | Loss: 0.00003402
Iteration 82/1000 | Loss: 0.00003402
Iteration 83/1000 | Loss: 0.00003401
Iteration 84/1000 | Loss: 0.00003401
Iteration 85/1000 | Loss: 0.00003401
Iteration 86/1000 | Loss: 0.00003401
Iteration 87/1000 | Loss: 0.00003400
Iteration 88/1000 | Loss: 0.00003400
Iteration 89/1000 | Loss: 0.00003400
Iteration 90/1000 | Loss: 0.00003400
Iteration 91/1000 | Loss: 0.00003400
Iteration 92/1000 | Loss: 0.00003400
Iteration 93/1000 | Loss: 0.00003400
Iteration 94/1000 | Loss: 0.00003399
Iteration 95/1000 | Loss: 0.00003399
Iteration 96/1000 | Loss: 0.00003399
Iteration 97/1000 | Loss: 0.00003399
Iteration 98/1000 | Loss: 0.00003399
Iteration 99/1000 | Loss: 0.00003399
Iteration 100/1000 | Loss: 0.00003399
Iteration 101/1000 | Loss: 0.00003398
Iteration 102/1000 | Loss: 0.00003398
Iteration 103/1000 | Loss: 0.00003398
Iteration 104/1000 | Loss: 0.00003398
Iteration 105/1000 | Loss: 0.00003397
Iteration 106/1000 | Loss: 0.00003397
Iteration 107/1000 | Loss: 0.00003397
Iteration 108/1000 | Loss: 0.00003397
Iteration 109/1000 | Loss: 0.00003397
Iteration 110/1000 | Loss: 0.00003397
Iteration 111/1000 | Loss: 0.00003397
Iteration 112/1000 | Loss: 0.00003397
Iteration 113/1000 | Loss: 0.00003397
Iteration 114/1000 | Loss: 0.00003396
Iteration 115/1000 | Loss: 0.00003396
Iteration 116/1000 | Loss: 0.00003396
Iteration 117/1000 | Loss: 0.00003396
Iteration 118/1000 | Loss: 0.00003396
Iteration 119/1000 | Loss: 0.00003395
Iteration 120/1000 | Loss: 0.00003395
Iteration 121/1000 | Loss: 0.00003395
Iteration 122/1000 | Loss: 0.00003395
Iteration 123/1000 | Loss: 0.00003395
Iteration 124/1000 | Loss: 0.00003394
Iteration 125/1000 | Loss: 0.00003394
Iteration 126/1000 | Loss: 0.00003394
Iteration 127/1000 | Loss: 0.00003394
Iteration 128/1000 | Loss: 0.00003394
Iteration 129/1000 | Loss: 0.00003394
Iteration 130/1000 | Loss: 0.00003394
Iteration 131/1000 | Loss: 0.00003394
Iteration 132/1000 | Loss: 0.00003394
Iteration 133/1000 | Loss: 0.00003393
Iteration 134/1000 | Loss: 0.00003393
Iteration 135/1000 | Loss: 0.00003393
Iteration 136/1000 | Loss: 0.00003393
Iteration 137/1000 | Loss: 0.00003393
Iteration 138/1000 | Loss: 0.00003393
Iteration 139/1000 | Loss: 0.00003393
Iteration 140/1000 | Loss: 0.00003392
Iteration 141/1000 | Loss: 0.00003392
Iteration 142/1000 | Loss: 0.00003392
Iteration 143/1000 | Loss: 0.00003392
Iteration 144/1000 | Loss: 0.00003392
Iteration 145/1000 | Loss: 0.00003392
Iteration 146/1000 | Loss: 0.00003391
Iteration 147/1000 | Loss: 0.00003391
Iteration 148/1000 | Loss: 0.00003391
Iteration 149/1000 | Loss: 0.00003391
Iteration 150/1000 | Loss: 0.00003391
Iteration 151/1000 | Loss: 0.00003391
Iteration 152/1000 | Loss: 0.00003391
Iteration 153/1000 | Loss: 0.00003391
Iteration 154/1000 | Loss: 0.00003391
Iteration 155/1000 | Loss: 0.00003391
Iteration 156/1000 | Loss: 0.00003390
Iteration 157/1000 | Loss: 0.00003390
Iteration 158/1000 | Loss: 0.00003390
Iteration 159/1000 | Loss: 0.00003390
Iteration 160/1000 | Loss: 0.00003390
Iteration 161/1000 | Loss: 0.00003390
Iteration 162/1000 | Loss: 0.00003390
Iteration 163/1000 | Loss: 0.00003390
Iteration 164/1000 | Loss: 0.00003390
Iteration 165/1000 | Loss: 0.00003390
Iteration 166/1000 | Loss: 0.00003389
Iteration 167/1000 | Loss: 0.00003389
Iteration 168/1000 | Loss: 0.00003389
Iteration 169/1000 | Loss: 0.00003389
Iteration 170/1000 | Loss: 0.00003389
Iteration 171/1000 | Loss: 0.00003389
Iteration 172/1000 | Loss: 0.00003389
Iteration 173/1000 | Loss: 0.00003388
Iteration 174/1000 | Loss: 0.00003388
Iteration 175/1000 | Loss: 0.00003388
Iteration 176/1000 | Loss: 0.00003388
Iteration 177/1000 | Loss: 0.00003388
Iteration 178/1000 | Loss: 0.00003388
Iteration 179/1000 | Loss: 0.00003388
Iteration 180/1000 | Loss: 0.00003388
Iteration 181/1000 | Loss: 0.00003388
Iteration 182/1000 | Loss: 0.00003388
Iteration 183/1000 | Loss: 0.00003388
Iteration 184/1000 | Loss: 0.00003388
Iteration 185/1000 | Loss: 0.00003388
Iteration 186/1000 | Loss: 0.00003388
Iteration 187/1000 | Loss: 0.00003388
Iteration 188/1000 | Loss: 0.00003388
Iteration 189/1000 | Loss: 0.00003388
Iteration 190/1000 | Loss: 0.00003388
Iteration 191/1000 | Loss: 0.00003387
Iteration 192/1000 | Loss: 0.00003387
Iteration 193/1000 | Loss: 0.00003387
Iteration 194/1000 | Loss: 0.00003387
Iteration 195/1000 | Loss: 0.00003387
Iteration 196/1000 | Loss: 0.00003387
Iteration 197/1000 | Loss: 0.00003387
Iteration 198/1000 | Loss: 0.00003387
Iteration 199/1000 | Loss: 0.00003387
Iteration 200/1000 | Loss: 0.00003386
Iteration 201/1000 | Loss: 0.00003386
Iteration 202/1000 | Loss: 0.00003386
Iteration 203/1000 | Loss: 0.00003386
Iteration 204/1000 | Loss: 0.00003386
Iteration 205/1000 | Loss: 0.00003386
Iteration 206/1000 | Loss: 0.00003386
Iteration 207/1000 | Loss: 0.00003386
Iteration 208/1000 | Loss: 0.00003386
Iteration 209/1000 | Loss: 0.00003386
Iteration 210/1000 | Loss: 0.00003386
Iteration 211/1000 | Loss: 0.00003386
Iteration 212/1000 | Loss: 0.00003385
Iteration 213/1000 | Loss: 0.00003385
Iteration 214/1000 | Loss: 0.00003385
Iteration 215/1000 | Loss: 0.00003385
Iteration 216/1000 | Loss: 0.00003385
Iteration 217/1000 | Loss: 0.00003385
Iteration 218/1000 | Loss: 0.00003385
Iteration 219/1000 | Loss: 0.00003385
Iteration 220/1000 | Loss: 0.00003385
Iteration 221/1000 | Loss: 0.00003385
Iteration 222/1000 | Loss: 0.00003385
Iteration 223/1000 | Loss: 0.00003385
Iteration 224/1000 | Loss: 0.00003385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [3.3851985790533945e-05, 3.3851985790533945e-05, 3.3851985790533945e-05, 3.3851985790533945e-05, 3.3851985790533945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3851985790533945e-05

Optimization complete. Final v2v error: 4.770295143127441 mm

Highest mean error: 5.849327564239502 mm for frame 118

Lowest mean error: 3.9811208248138428 mm for frame 18

Saving results

Total time: 61.77285957336426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01114820
Iteration 2/25 | Loss: 0.01114820
Iteration 3/25 | Loss: 0.01114819
Iteration 4/25 | Loss: 0.01114819
Iteration 5/25 | Loss: 0.01114819
Iteration 6/25 | Loss: 0.01114819
Iteration 7/25 | Loss: 0.01114819
Iteration 8/25 | Loss: 0.01114819
Iteration 9/25 | Loss: 0.01114818
Iteration 10/25 | Loss: 0.01114818
Iteration 11/25 | Loss: 0.01114818
Iteration 12/25 | Loss: 0.01114817
Iteration 13/25 | Loss: 0.01114817
Iteration 14/25 | Loss: 0.01114817
Iteration 15/25 | Loss: 0.01114817
Iteration 16/25 | Loss: 0.01114817
Iteration 17/25 | Loss: 0.01114816
Iteration 18/25 | Loss: 0.01114816
Iteration 19/25 | Loss: 0.01114816
Iteration 20/25 | Loss: 0.01114816
Iteration 21/25 | Loss: 0.01114816
Iteration 22/25 | Loss: 0.01114815
Iteration 23/25 | Loss: 0.01114815
Iteration 24/25 | Loss: 0.01114815
Iteration 25/25 | Loss: 0.01114815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35482275
Iteration 2/25 | Loss: 0.17468837
Iteration 3/25 | Loss: 0.17463046
Iteration 4/25 | Loss: 0.17463046
Iteration 5/25 | Loss: 0.17463042
Iteration 6/25 | Loss: 0.17463042
Iteration 7/25 | Loss: 0.17463042
Iteration 8/25 | Loss: 0.17463040
Iteration 9/25 | Loss: 0.17463040
Iteration 10/25 | Loss: 0.17463040
Iteration 11/25 | Loss: 0.17463040
Iteration 12/25 | Loss: 0.17463040
Iteration 13/25 | Loss: 0.17463040
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.17463040351867676, 0.17463040351867676, 0.17463040351867676, 0.17463040351867676, 0.17463040351867676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17463040351867676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17463040
Iteration 2/1000 | Loss: 0.00328931
Iteration 3/1000 | Loss: 0.00091813
Iteration 4/1000 | Loss: 0.00126035
Iteration 5/1000 | Loss: 0.00016558
Iteration 6/1000 | Loss: 0.00009997
Iteration 7/1000 | Loss: 0.00007108
Iteration 8/1000 | Loss: 0.00005501
Iteration 9/1000 | Loss: 0.00004422
Iteration 10/1000 | Loss: 0.00003720
Iteration 11/1000 | Loss: 0.00003184
Iteration 12/1000 | Loss: 0.00002917
Iteration 13/1000 | Loss: 0.00002767
Iteration 14/1000 | Loss: 0.00002642
Iteration 15/1000 | Loss: 0.00002558
Iteration 16/1000 | Loss: 0.00002502
Iteration 17/1000 | Loss: 0.00002483
Iteration 18/1000 | Loss: 0.00002476
Iteration 19/1000 | Loss: 0.00002475
Iteration 20/1000 | Loss: 0.00002454
Iteration 21/1000 | Loss: 0.00002435
Iteration 22/1000 | Loss: 0.00002431
Iteration 23/1000 | Loss: 0.00002415
Iteration 24/1000 | Loss: 0.00002413
Iteration 25/1000 | Loss: 0.00002413
Iteration 26/1000 | Loss: 0.00002412
Iteration 27/1000 | Loss: 0.00002408
Iteration 28/1000 | Loss: 0.00002406
Iteration 29/1000 | Loss: 0.00002405
Iteration 30/1000 | Loss: 0.00002405
Iteration 31/1000 | Loss: 0.00002404
Iteration 32/1000 | Loss: 0.00002399
Iteration 33/1000 | Loss: 0.00002394
Iteration 34/1000 | Loss: 0.00002393
Iteration 35/1000 | Loss: 0.00002393
Iteration 36/1000 | Loss: 0.00002392
Iteration 37/1000 | Loss: 0.00002391
Iteration 38/1000 | Loss: 0.00002391
Iteration 39/1000 | Loss: 0.00002390
Iteration 40/1000 | Loss: 0.00002387
Iteration 41/1000 | Loss: 0.00002386
Iteration 42/1000 | Loss: 0.00002386
Iteration 43/1000 | Loss: 0.00002382
Iteration 44/1000 | Loss: 0.00002382
Iteration 45/1000 | Loss: 0.00002382
Iteration 46/1000 | Loss: 0.00002382
Iteration 47/1000 | Loss: 0.00002381
Iteration 48/1000 | Loss: 0.00002381
Iteration 49/1000 | Loss: 0.00002381
Iteration 50/1000 | Loss: 0.00002381
Iteration 51/1000 | Loss: 0.00002381
Iteration 52/1000 | Loss: 0.00002381
Iteration 53/1000 | Loss: 0.00002381
Iteration 54/1000 | Loss: 0.00002381
Iteration 55/1000 | Loss: 0.00002381
Iteration 56/1000 | Loss: 0.00002381
Iteration 57/1000 | Loss: 0.00002381
Iteration 58/1000 | Loss: 0.00002380
Iteration 59/1000 | Loss: 0.00002380
Iteration 60/1000 | Loss: 0.00002380
Iteration 61/1000 | Loss: 0.00002380
Iteration 62/1000 | Loss: 0.00002380
Iteration 63/1000 | Loss: 0.00002379
Iteration 64/1000 | Loss: 0.00002378
Iteration 65/1000 | Loss: 0.00002378
Iteration 66/1000 | Loss: 0.00002378
Iteration 67/1000 | Loss: 0.00002377
Iteration 68/1000 | Loss: 0.00002377
Iteration 69/1000 | Loss: 0.00002376
Iteration 70/1000 | Loss: 0.00002376
Iteration 71/1000 | Loss: 0.00002376
Iteration 72/1000 | Loss: 0.00002376
Iteration 73/1000 | Loss: 0.00002376
Iteration 74/1000 | Loss: 0.00002376
Iteration 75/1000 | Loss: 0.00002376
Iteration 76/1000 | Loss: 0.00002376
Iteration 77/1000 | Loss: 0.00002376
Iteration 78/1000 | Loss: 0.00002375
Iteration 79/1000 | Loss: 0.00002375
Iteration 80/1000 | Loss: 0.00002375
Iteration 81/1000 | Loss: 0.00002375
Iteration 82/1000 | Loss: 0.00002375
Iteration 83/1000 | Loss: 0.00002375
Iteration 84/1000 | Loss: 0.00002375
Iteration 85/1000 | Loss: 0.00002375
Iteration 86/1000 | Loss: 0.00002375
Iteration 87/1000 | Loss: 0.00002375
Iteration 88/1000 | Loss: 0.00002375
Iteration 89/1000 | Loss: 0.00002375
Iteration 90/1000 | Loss: 0.00002375
Iteration 91/1000 | Loss: 0.00002375
Iteration 92/1000 | Loss: 0.00002374
Iteration 93/1000 | Loss: 0.00002374
Iteration 94/1000 | Loss: 0.00002374
Iteration 95/1000 | Loss: 0.00002374
Iteration 96/1000 | Loss: 0.00002374
Iteration 97/1000 | Loss: 0.00002374
Iteration 98/1000 | Loss: 0.00002374
Iteration 99/1000 | Loss: 0.00002374
Iteration 100/1000 | Loss: 0.00002374
Iteration 101/1000 | Loss: 0.00002374
Iteration 102/1000 | Loss: 0.00002373
Iteration 103/1000 | Loss: 0.00002373
Iteration 104/1000 | Loss: 0.00002373
Iteration 105/1000 | Loss: 0.00002373
Iteration 106/1000 | Loss: 0.00002373
Iteration 107/1000 | Loss: 0.00002373
Iteration 108/1000 | Loss: 0.00002373
Iteration 109/1000 | Loss: 0.00002373
Iteration 110/1000 | Loss: 0.00002373
Iteration 111/1000 | Loss: 0.00002373
Iteration 112/1000 | Loss: 0.00002373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [2.3732598492642865e-05, 2.3732598492642865e-05, 2.3732598492642865e-05, 2.3732598492642865e-05, 2.3732598492642865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3732598492642865e-05

Optimization complete. Final v2v error: 3.9277803897857666 mm

Highest mean error: 4.513143539428711 mm for frame 155

Lowest mean error: 3.51271390914917 mm for frame 27

Saving results

Total time: 50.10438919067383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868299
Iteration 2/25 | Loss: 0.00242359
Iteration 3/25 | Loss: 0.00201319
Iteration 4/25 | Loss: 0.00156236
Iteration 5/25 | Loss: 0.00159694
Iteration 6/25 | Loss: 0.00152750
Iteration 7/25 | Loss: 0.00151157
Iteration 8/25 | Loss: 0.00149605
Iteration 9/25 | Loss: 0.00148638
Iteration 10/25 | Loss: 0.00147643
Iteration 11/25 | Loss: 0.00147968
Iteration 12/25 | Loss: 0.00147552
Iteration 13/25 | Loss: 0.00146907
Iteration 14/25 | Loss: 0.00147182
Iteration 15/25 | Loss: 0.00146644
Iteration 16/25 | Loss: 0.00146358
Iteration 17/25 | Loss: 0.00146374
Iteration 18/25 | Loss: 0.00146238
Iteration 19/25 | Loss: 0.00145857
Iteration 20/25 | Loss: 0.00146006
Iteration 21/25 | Loss: 0.00145793
Iteration 22/25 | Loss: 0.00145710
Iteration 23/25 | Loss: 0.00145853
Iteration 24/25 | Loss: 0.00145600
Iteration 25/25 | Loss: 0.00145890

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.88210773
Iteration 2/25 | Loss: 0.00309131
Iteration 3/25 | Loss: 0.00309131
Iteration 4/25 | Loss: 0.00309131
Iteration 5/25 | Loss: 0.00309131
Iteration 6/25 | Loss: 0.00309131
Iteration 7/25 | Loss: 0.00309131
Iteration 8/25 | Loss: 0.00309131
Iteration 9/25 | Loss: 0.00309131
Iteration 10/25 | Loss: 0.00309131
Iteration 11/25 | Loss: 0.00309131
Iteration 12/25 | Loss: 0.00309131
Iteration 13/25 | Loss: 0.00309131
Iteration 14/25 | Loss: 0.00309131
Iteration 15/25 | Loss: 0.00309131
Iteration 16/25 | Loss: 0.00309131
Iteration 17/25 | Loss: 0.00309131
Iteration 18/25 | Loss: 0.00309131
Iteration 19/25 | Loss: 0.00309131
Iteration 20/25 | Loss: 0.00309131
Iteration 21/25 | Loss: 0.00309131
Iteration 22/25 | Loss: 0.00309131
Iteration 23/25 | Loss: 0.00309131
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0030913078226149082, 0.0030913078226149082, 0.0030913078226149082, 0.0030913078226149082, 0.0030913078226149082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030913078226149082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00309131
Iteration 2/1000 | Loss: 0.00051638
Iteration 3/1000 | Loss: 0.00108789
Iteration 4/1000 | Loss: 0.00339737
Iteration 5/1000 | Loss: 0.00384784
Iteration 6/1000 | Loss: 0.00337866
Iteration 7/1000 | Loss: 0.00071559
Iteration 8/1000 | Loss: 0.00048388
Iteration 9/1000 | Loss: 0.00021434
Iteration 10/1000 | Loss: 0.00066505
Iteration 11/1000 | Loss: 0.00011174
Iteration 12/1000 | Loss: 0.00013931
Iteration 13/1000 | Loss: 0.00011788
Iteration 14/1000 | Loss: 0.00011896
Iteration 15/1000 | Loss: 0.00075016
Iteration 16/1000 | Loss: 0.00100942
Iteration 17/1000 | Loss: 0.00039476
Iteration 18/1000 | Loss: 0.00006231
Iteration 19/1000 | Loss: 0.00005047
Iteration 20/1000 | Loss: 0.00004973
Iteration 21/1000 | Loss: 0.00004190
Iteration 22/1000 | Loss: 0.00016040
Iteration 23/1000 | Loss: 0.00013244
Iteration 24/1000 | Loss: 0.00012067
Iteration 25/1000 | Loss: 0.00098893
Iteration 26/1000 | Loss: 0.00016635
Iteration 27/1000 | Loss: 0.00007024
Iteration 28/1000 | Loss: 0.00060595
Iteration 29/1000 | Loss: 0.00061957
Iteration 30/1000 | Loss: 0.00039135
Iteration 31/1000 | Loss: 0.00006161
Iteration 32/1000 | Loss: 0.00011401
Iteration 33/1000 | Loss: 0.00040547
Iteration 34/1000 | Loss: 0.00013465
Iteration 35/1000 | Loss: 0.00003878
Iteration 36/1000 | Loss: 0.00003691
Iteration 37/1000 | Loss: 0.00003523
Iteration 38/1000 | Loss: 0.00004931
Iteration 39/1000 | Loss: 0.00018447
Iteration 40/1000 | Loss: 0.00091020
Iteration 41/1000 | Loss: 0.00032884
Iteration 42/1000 | Loss: 0.00007148
Iteration 43/1000 | Loss: 0.00004837
Iteration 44/1000 | Loss: 0.00071351
Iteration 45/1000 | Loss: 0.00017883
Iteration 46/1000 | Loss: 0.00015628
Iteration 47/1000 | Loss: 0.00027620
Iteration 48/1000 | Loss: 0.00003627
Iteration 49/1000 | Loss: 0.00006214
Iteration 50/1000 | Loss: 0.00049384
Iteration 51/1000 | Loss: 0.00012095
Iteration 52/1000 | Loss: 0.00031466
Iteration 53/1000 | Loss: 0.00020636
Iteration 54/1000 | Loss: 0.00006005
Iteration 55/1000 | Loss: 0.00040191
Iteration 56/1000 | Loss: 0.00028717
Iteration 57/1000 | Loss: 0.00025474
Iteration 58/1000 | Loss: 0.00017035
Iteration 59/1000 | Loss: 0.00005876
Iteration 60/1000 | Loss: 0.00011689
Iteration 61/1000 | Loss: 0.00021216
Iteration 62/1000 | Loss: 0.00013686
Iteration 63/1000 | Loss: 0.00004209
Iteration 64/1000 | Loss: 0.00040612
Iteration 65/1000 | Loss: 0.00020046
Iteration 66/1000 | Loss: 0.00009916
Iteration 67/1000 | Loss: 0.00003545
Iteration 68/1000 | Loss: 0.00024360
Iteration 69/1000 | Loss: 0.00017393
Iteration 70/1000 | Loss: 0.00008122
Iteration 71/1000 | Loss: 0.00003270
Iteration 72/1000 | Loss: 0.00002977
Iteration 73/1000 | Loss: 0.00002859
Iteration 74/1000 | Loss: 0.00002802
Iteration 75/1000 | Loss: 0.00002755
Iteration 76/1000 | Loss: 0.00002715
Iteration 77/1000 | Loss: 0.00002678
Iteration 78/1000 | Loss: 0.00026714
Iteration 79/1000 | Loss: 0.00012762
Iteration 80/1000 | Loss: 0.00010978
Iteration 81/1000 | Loss: 0.00006369
Iteration 82/1000 | Loss: 0.00020539
Iteration 83/1000 | Loss: 0.00016800
Iteration 84/1000 | Loss: 0.00012954
Iteration 85/1000 | Loss: 0.00003429
Iteration 86/1000 | Loss: 0.00003201
Iteration 87/1000 | Loss: 0.00013675
Iteration 88/1000 | Loss: 0.00003112
Iteration 89/1000 | Loss: 0.00002968
Iteration 90/1000 | Loss: 0.00002859
Iteration 91/1000 | Loss: 0.00002748
Iteration 92/1000 | Loss: 0.00002978
Iteration 93/1000 | Loss: 0.00002650
Iteration 94/1000 | Loss: 0.00002571
Iteration 95/1000 | Loss: 0.00002515
Iteration 96/1000 | Loss: 0.00002482
Iteration 97/1000 | Loss: 0.00002453
Iteration 98/1000 | Loss: 0.00002437
Iteration 99/1000 | Loss: 0.00002418
Iteration 100/1000 | Loss: 0.00002414
Iteration 101/1000 | Loss: 0.00002408
Iteration 102/1000 | Loss: 0.00002404
Iteration 103/1000 | Loss: 0.00002404
Iteration 104/1000 | Loss: 0.00002403
Iteration 105/1000 | Loss: 0.00002403
Iteration 106/1000 | Loss: 0.00002403
Iteration 107/1000 | Loss: 0.00002403
Iteration 108/1000 | Loss: 0.00002402
Iteration 109/1000 | Loss: 0.00002402
Iteration 110/1000 | Loss: 0.00002401
Iteration 111/1000 | Loss: 0.00002401
Iteration 112/1000 | Loss: 0.00002400
Iteration 113/1000 | Loss: 0.00002400
Iteration 114/1000 | Loss: 0.00002399
Iteration 115/1000 | Loss: 0.00002399
Iteration 116/1000 | Loss: 0.00002399
Iteration 117/1000 | Loss: 0.00002399
Iteration 118/1000 | Loss: 0.00002398
Iteration 119/1000 | Loss: 0.00002398
Iteration 120/1000 | Loss: 0.00002398
Iteration 121/1000 | Loss: 0.00002397
Iteration 122/1000 | Loss: 0.00002397
Iteration 123/1000 | Loss: 0.00002397
Iteration 124/1000 | Loss: 0.00002396
Iteration 125/1000 | Loss: 0.00002396
Iteration 126/1000 | Loss: 0.00002396
Iteration 127/1000 | Loss: 0.00002396
Iteration 128/1000 | Loss: 0.00002395
Iteration 129/1000 | Loss: 0.00002395
Iteration 130/1000 | Loss: 0.00002395
Iteration 131/1000 | Loss: 0.00002395
Iteration 132/1000 | Loss: 0.00002395
Iteration 133/1000 | Loss: 0.00002395
Iteration 134/1000 | Loss: 0.00002395
Iteration 135/1000 | Loss: 0.00002395
Iteration 136/1000 | Loss: 0.00002395
Iteration 137/1000 | Loss: 0.00002395
Iteration 138/1000 | Loss: 0.00002395
Iteration 139/1000 | Loss: 0.00002395
Iteration 140/1000 | Loss: 0.00002395
Iteration 141/1000 | Loss: 0.00002395
Iteration 142/1000 | Loss: 0.00002395
Iteration 143/1000 | Loss: 0.00002395
Iteration 144/1000 | Loss: 0.00002395
Iteration 145/1000 | Loss: 0.00002395
Iteration 146/1000 | Loss: 0.00002395
Iteration 147/1000 | Loss: 0.00002395
Iteration 148/1000 | Loss: 0.00002395
Iteration 149/1000 | Loss: 0.00002395
Iteration 150/1000 | Loss: 0.00002395
Iteration 151/1000 | Loss: 0.00002395
Iteration 152/1000 | Loss: 0.00002395
Iteration 153/1000 | Loss: 0.00002395
Iteration 154/1000 | Loss: 0.00002395
Iteration 155/1000 | Loss: 0.00002395
Iteration 156/1000 | Loss: 0.00002395
Iteration 157/1000 | Loss: 0.00002395
Iteration 158/1000 | Loss: 0.00002395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.3946588044054806e-05, 2.3946588044054806e-05, 2.3946588044054806e-05, 2.3946588044054806e-05, 2.3946588044054806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3946588044054806e-05

Optimization complete. Final v2v error: 3.5401268005371094 mm

Highest mean error: 13.216385841369629 mm for frame 78

Lowest mean error: 2.73713755607605 mm for frame 114

Saving results

Total time: 193.95983862876892
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006878
Iteration 2/25 | Loss: 0.01006878
Iteration 3/25 | Loss: 0.00455551
Iteration 4/25 | Loss: 0.00339515
Iteration 5/25 | Loss: 0.00232039
Iteration 6/25 | Loss: 0.00216978
Iteration 7/25 | Loss: 0.00220382
Iteration 8/25 | Loss: 0.00220267
Iteration 9/25 | Loss: 0.00205598
Iteration 10/25 | Loss: 0.00195236
Iteration 11/25 | Loss: 0.00178837
Iteration 12/25 | Loss: 0.00178819
Iteration 13/25 | Loss: 0.00171901
Iteration 14/25 | Loss: 0.00167251
Iteration 15/25 | Loss: 0.00164368
Iteration 16/25 | Loss: 0.00161443
Iteration 17/25 | Loss: 0.00159200
Iteration 18/25 | Loss: 0.00158864
Iteration 19/25 | Loss: 0.00161833
Iteration 20/25 | Loss: 0.00155699
Iteration 21/25 | Loss: 0.00156052
Iteration 22/25 | Loss: 0.00157359
Iteration 23/25 | Loss: 0.00154818
Iteration 24/25 | Loss: 0.00153572
Iteration 25/25 | Loss: 0.00153054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40959144
Iteration 2/25 | Loss: 0.00331652
Iteration 3/25 | Loss: 0.00223980
Iteration 4/25 | Loss: 0.00223972
Iteration 5/25 | Loss: 0.00223972
Iteration 6/25 | Loss: 0.00223972
Iteration 7/25 | Loss: 0.00223972
Iteration 8/25 | Loss: 0.00223972
Iteration 9/25 | Loss: 0.00223972
Iteration 10/25 | Loss: 0.00223972
Iteration 11/25 | Loss: 0.00223972
Iteration 12/25 | Loss: 0.00223972
Iteration 13/25 | Loss: 0.00223972
Iteration 14/25 | Loss: 0.00223972
Iteration 15/25 | Loss: 0.00223972
Iteration 16/25 | Loss: 0.00223972
Iteration 17/25 | Loss: 0.00223972
Iteration 18/25 | Loss: 0.00223972
Iteration 19/25 | Loss: 0.00223972
Iteration 20/25 | Loss: 0.00223972
Iteration 21/25 | Loss: 0.00223972
Iteration 22/25 | Loss: 0.00223972
Iteration 23/25 | Loss: 0.00223972
Iteration 24/25 | Loss: 0.00223972
Iteration 25/25 | Loss: 0.00223972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223972
Iteration 2/1000 | Loss: 0.00802909
Iteration 3/1000 | Loss: 0.00090639
Iteration 4/1000 | Loss: 0.00078774
Iteration 5/1000 | Loss: 0.00072999
Iteration 6/1000 | Loss: 0.00086938
Iteration 7/1000 | Loss: 0.00083518
Iteration 8/1000 | Loss: 0.00072407
Iteration 9/1000 | Loss: 0.00050984
Iteration 10/1000 | Loss: 0.00071759
Iteration 11/1000 | Loss: 0.00105217
Iteration 12/1000 | Loss: 0.00143148
Iteration 13/1000 | Loss: 0.00234407
Iteration 14/1000 | Loss: 0.00281595
Iteration 15/1000 | Loss: 0.00259385
Iteration 16/1000 | Loss: 0.00406344
Iteration 17/1000 | Loss: 0.00411284
Iteration 18/1000 | Loss: 0.00421620
Iteration 19/1000 | Loss: 0.00208585
Iteration 20/1000 | Loss: 0.00123085
Iteration 21/1000 | Loss: 0.00065988
Iteration 22/1000 | Loss: 0.00042561
Iteration 23/1000 | Loss: 0.00017973
Iteration 24/1000 | Loss: 0.00049869
Iteration 25/1000 | Loss: 0.00026613
Iteration 26/1000 | Loss: 0.00036301
Iteration 27/1000 | Loss: 0.00018222
Iteration 28/1000 | Loss: 0.00050276
Iteration 29/1000 | Loss: 0.00013373
Iteration 30/1000 | Loss: 0.00052411
Iteration 31/1000 | Loss: 0.00050741
Iteration 32/1000 | Loss: 0.00015461
Iteration 33/1000 | Loss: 0.00042530
Iteration 34/1000 | Loss: 0.00106450
Iteration 35/1000 | Loss: 0.00015426
Iteration 36/1000 | Loss: 0.00026663
Iteration 37/1000 | Loss: 0.00112309
Iteration 38/1000 | Loss: 0.00054545
Iteration 39/1000 | Loss: 0.00123521
Iteration 40/1000 | Loss: 0.00290440
Iteration 41/1000 | Loss: 0.00302887
Iteration 42/1000 | Loss: 0.00033347
Iteration 43/1000 | Loss: 0.00029419
Iteration 44/1000 | Loss: 0.00041719
Iteration 45/1000 | Loss: 0.00017341
Iteration 46/1000 | Loss: 0.00019118
Iteration 47/1000 | Loss: 0.00016864
Iteration 48/1000 | Loss: 0.00040707
Iteration 49/1000 | Loss: 0.00019530
Iteration 50/1000 | Loss: 0.00009720
Iteration 51/1000 | Loss: 0.00026931
Iteration 52/1000 | Loss: 0.00005113
Iteration 53/1000 | Loss: 0.00008817
Iteration 54/1000 | Loss: 0.00007940
Iteration 55/1000 | Loss: 0.00017592
Iteration 56/1000 | Loss: 0.00009892
Iteration 57/1000 | Loss: 0.00003735
Iteration 58/1000 | Loss: 0.00023583
Iteration 59/1000 | Loss: 0.00007987
Iteration 60/1000 | Loss: 0.00019793
Iteration 61/1000 | Loss: 0.00043640
Iteration 62/1000 | Loss: 0.00008266
Iteration 63/1000 | Loss: 0.00004813
Iteration 64/1000 | Loss: 0.00007668
Iteration 65/1000 | Loss: 0.00019121
Iteration 66/1000 | Loss: 0.00040618
Iteration 67/1000 | Loss: 0.00027816
Iteration 68/1000 | Loss: 0.00033992
Iteration 69/1000 | Loss: 0.00022954
Iteration 70/1000 | Loss: 0.00007866
Iteration 71/1000 | Loss: 0.00005314
Iteration 72/1000 | Loss: 0.00005300
Iteration 73/1000 | Loss: 0.00008827
Iteration 74/1000 | Loss: 0.00003341
Iteration 75/1000 | Loss: 0.00022469
Iteration 76/1000 | Loss: 0.00028368
Iteration 77/1000 | Loss: 0.00005995
Iteration 78/1000 | Loss: 0.00014960
Iteration 79/1000 | Loss: 0.00004357
Iteration 80/1000 | Loss: 0.00022538
Iteration 81/1000 | Loss: 0.00020901
Iteration 82/1000 | Loss: 0.00024883
Iteration 83/1000 | Loss: 0.00005139
Iteration 84/1000 | Loss: 0.00006366
Iteration 85/1000 | Loss: 0.00005059
Iteration 86/1000 | Loss: 0.00007496
Iteration 87/1000 | Loss: 0.00004018
Iteration 88/1000 | Loss: 0.00011219
Iteration 89/1000 | Loss: 0.00017538
Iteration 90/1000 | Loss: 0.00028598
Iteration 91/1000 | Loss: 0.00040386
Iteration 92/1000 | Loss: 0.00020423
Iteration 93/1000 | Loss: 0.00015595
Iteration 94/1000 | Loss: 0.00008437
Iteration 95/1000 | Loss: 0.00015966
Iteration 96/1000 | Loss: 0.00022491
Iteration 97/1000 | Loss: 0.00004535
Iteration 98/1000 | Loss: 0.00004729
Iteration 99/1000 | Loss: 0.00006353
Iteration 100/1000 | Loss: 0.00005352
Iteration 101/1000 | Loss: 0.00007184
Iteration 102/1000 | Loss: 0.00003092
Iteration 103/1000 | Loss: 0.00007094
Iteration 104/1000 | Loss: 0.00004425
Iteration 105/1000 | Loss: 0.00003064
Iteration 106/1000 | Loss: 0.00003291
Iteration 107/1000 | Loss: 0.00007226
Iteration 108/1000 | Loss: 0.00006273
Iteration 109/1000 | Loss: 0.00003665
Iteration 110/1000 | Loss: 0.00010737
Iteration 111/1000 | Loss: 0.00004723
Iteration 112/1000 | Loss: 0.00003824
Iteration 113/1000 | Loss: 0.00004110
Iteration 114/1000 | Loss: 0.00005028
Iteration 115/1000 | Loss: 0.00036679
Iteration 116/1000 | Loss: 0.00007632
Iteration 117/1000 | Loss: 0.00005689
Iteration 118/1000 | Loss: 0.00002831
Iteration 119/1000 | Loss: 0.00003607
Iteration 120/1000 | Loss: 0.00002735
Iteration 121/1000 | Loss: 0.00002716
Iteration 122/1000 | Loss: 0.00002715
Iteration 123/1000 | Loss: 0.00002715
Iteration 124/1000 | Loss: 0.00006721
Iteration 125/1000 | Loss: 0.00005751
Iteration 126/1000 | Loss: 0.00009948
Iteration 127/1000 | Loss: 0.00003302
Iteration 128/1000 | Loss: 0.00004056
Iteration 129/1000 | Loss: 0.00003243
Iteration 130/1000 | Loss: 0.00002770
Iteration 131/1000 | Loss: 0.00002862
Iteration 132/1000 | Loss: 0.00006061
Iteration 133/1000 | Loss: 0.00003264
Iteration 134/1000 | Loss: 0.00002671
Iteration 135/1000 | Loss: 0.00002671
Iteration 136/1000 | Loss: 0.00002671
Iteration 137/1000 | Loss: 0.00002671
Iteration 138/1000 | Loss: 0.00002670
Iteration 139/1000 | Loss: 0.00002670
Iteration 140/1000 | Loss: 0.00004129
Iteration 141/1000 | Loss: 0.00002666
Iteration 142/1000 | Loss: 0.00002666
Iteration 143/1000 | Loss: 0.00002666
Iteration 144/1000 | Loss: 0.00002666
Iteration 145/1000 | Loss: 0.00002666
Iteration 146/1000 | Loss: 0.00002666
Iteration 147/1000 | Loss: 0.00002666
Iteration 148/1000 | Loss: 0.00002666
Iteration 149/1000 | Loss: 0.00002665
Iteration 150/1000 | Loss: 0.00002665
Iteration 151/1000 | Loss: 0.00002665
Iteration 152/1000 | Loss: 0.00002665
Iteration 153/1000 | Loss: 0.00002665
Iteration 154/1000 | Loss: 0.00002665
Iteration 155/1000 | Loss: 0.00002665
Iteration 156/1000 | Loss: 0.00002665
Iteration 157/1000 | Loss: 0.00002664
Iteration 158/1000 | Loss: 0.00002664
Iteration 159/1000 | Loss: 0.00002664
Iteration 160/1000 | Loss: 0.00002663
Iteration 161/1000 | Loss: 0.00002663
Iteration 162/1000 | Loss: 0.00002663
Iteration 163/1000 | Loss: 0.00002662
Iteration 164/1000 | Loss: 0.00002662
Iteration 165/1000 | Loss: 0.00002662
Iteration 166/1000 | Loss: 0.00002662
Iteration 167/1000 | Loss: 0.00002662
Iteration 168/1000 | Loss: 0.00002662
Iteration 169/1000 | Loss: 0.00002662
Iteration 170/1000 | Loss: 0.00002662
Iteration 171/1000 | Loss: 0.00002662
Iteration 172/1000 | Loss: 0.00002661
Iteration 173/1000 | Loss: 0.00002661
Iteration 174/1000 | Loss: 0.00002661
Iteration 175/1000 | Loss: 0.00002661
Iteration 176/1000 | Loss: 0.00002661
Iteration 177/1000 | Loss: 0.00002661
Iteration 178/1000 | Loss: 0.00002661
Iteration 179/1000 | Loss: 0.00002661
Iteration 180/1000 | Loss: 0.00002661
Iteration 181/1000 | Loss: 0.00002661
Iteration 182/1000 | Loss: 0.00002661
Iteration 183/1000 | Loss: 0.00002661
Iteration 184/1000 | Loss: 0.00002661
Iteration 185/1000 | Loss: 0.00002660
Iteration 186/1000 | Loss: 0.00002660
Iteration 187/1000 | Loss: 0.00002660
Iteration 188/1000 | Loss: 0.00002660
Iteration 189/1000 | Loss: 0.00002660
Iteration 190/1000 | Loss: 0.00002660
Iteration 191/1000 | Loss: 0.00002660
Iteration 192/1000 | Loss: 0.00002659
Iteration 193/1000 | Loss: 0.00002659
Iteration 194/1000 | Loss: 0.00002659
Iteration 195/1000 | Loss: 0.00002659
Iteration 196/1000 | Loss: 0.00002659
Iteration 197/1000 | Loss: 0.00002659
Iteration 198/1000 | Loss: 0.00002659
Iteration 199/1000 | Loss: 0.00002659
Iteration 200/1000 | Loss: 0.00002659
Iteration 201/1000 | Loss: 0.00002659
Iteration 202/1000 | Loss: 0.00002659
Iteration 203/1000 | Loss: 0.00002658
Iteration 204/1000 | Loss: 0.00002658
Iteration 205/1000 | Loss: 0.00002658
Iteration 206/1000 | Loss: 0.00002658
Iteration 207/1000 | Loss: 0.00002658
Iteration 208/1000 | Loss: 0.00002658
Iteration 209/1000 | Loss: 0.00002658
Iteration 210/1000 | Loss: 0.00002658
Iteration 211/1000 | Loss: 0.00002658
Iteration 212/1000 | Loss: 0.00002658
Iteration 213/1000 | Loss: 0.00002658
Iteration 214/1000 | Loss: 0.00002658
Iteration 215/1000 | Loss: 0.00002658
Iteration 216/1000 | Loss: 0.00002658
Iteration 217/1000 | Loss: 0.00002658
Iteration 218/1000 | Loss: 0.00002658
Iteration 219/1000 | Loss: 0.00002657
Iteration 220/1000 | Loss: 0.00002657
Iteration 221/1000 | Loss: 0.00002657
Iteration 222/1000 | Loss: 0.00002657
Iteration 223/1000 | Loss: 0.00002657
Iteration 224/1000 | Loss: 0.00002657
Iteration 225/1000 | Loss: 0.00002657
Iteration 226/1000 | Loss: 0.00002657
Iteration 227/1000 | Loss: 0.00002657
Iteration 228/1000 | Loss: 0.00002657
Iteration 229/1000 | Loss: 0.00002657
Iteration 230/1000 | Loss: 0.00002657
Iteration 231/1000 | Loss: 0.00002657
Iteration 232/1000 | Loss: 0.00002657
Iteration 233/1000 | Loss: 0.00002657
Iteration 234/1000 | Loss: 0.00002656
Iteration 235/1000 | Loss: 0.00002656
Iteration 236/1000 | Loss: 0.00002656
Iteration 237/1000 | Loss: 0.00002656
Iteration 238/1000 | Loss: 0.00002656
Iteration 239/1000 | Loss: 0.00002656
Iteration 240/1000 | Loss: 0.00002656
Iteration 241/1000 | Loss: 0.00002656
Iteration 242/1000 | Loss: 0.00002656
Iteration 243/1000 | Loss: 0.00002655
Iteration 244/1000 | Loss: 0.00002655
Iteration 245/1000 | Loss: 0.00002851
Iteration 246/1000 | Loss: 0.00002654
Iteration 247/1000 | Loss: 0.00002653
Iteration 248/1000 | Loss: 0.00002653
Iteration 249/1000 | Loss: 0.00002653
Iteration 250/1000 | Loss: 0.00002653
Iteration 251/1000 | Loss: 0.00002653
Iteration 252/1000 | Loss: 0.00002653
Iteration 253/1000 | Loss: 0.00002653
Iteration 254/1000 | Loss: 0.00002653
Iteration 255/1000 | Loss: 0.00002653
Iteration 256/1000 | Loss: 0.00002653
Iteration 257/1000 | Loss: 0.00002653
Iteration 258/1000 | Loss: 0.00002653
Iteration 259/1000 | Loss: 0.00002653
Iteration 260/1000 | Loss: 0.00002652
Iteration 261/1000 | Loss: 0.00002696
Iteration 262/1000 | Loss: 0.00002655
Iteration 263/1000 | Loss: 0.00002652
Iteration 264/1000 | Loss: 0.00002651
Iteration 265/1000 | Loss: 0.00002651
Iteration 266/1000 | Loss: 0.00002651
Iteration 267/1000 | Loss: 0.00002651
Iteration 268/1000 | Loss: 0.00002651
Iteration 269/1000 | Loss: 0.00002651
Iteration 270/1000 | Loss: 0.00002651
Iteration 271/1000 | Loss: 0.00002651
Iteration 272/1000 | Loss: 0.00002651
Iteration 273/1000 | Loss: 0.00002651
Iteration 274/1000 | Loss: 0.00002651
Iteration 275/1000 | Loss: 0.00002651
Iteration 276/1000 | Loss: 0.00002651
Iteration 277/1000 | Loss: 0.00002651
Iteration 278/1000 | Loss: 0.00002651
Iteration 279/1000 | Loss: 0.00002651
Iteration 280/1000 | Loss: 0.00002651
Iteration 281/1000 | Loss: 0.00002650
Iteration 282/1000 | Loss: 0.00002650
Iteration 283/1000 | Loss: 0.00002650
Iteration 284/1000 | Loss: 0.00002650
Iteration 285/1000 | Loss: 0.00002650
Iteration 286/1000 | Loss: 0.00002649
Iteration 287/1000 | Loss: 0.00002649
Iteration 288/1000 | Loss: 0.00002654
Iteration 289/1000 | Loss: 0.00002653
Iteration 290/1000 | Loss: 0.00002649
Iteration 291/1000 | Loss: 0.00002649
Iteration 292/1000 | Loss: 0.00002648
Iteration 293/1000 | Loss: 0.00002648
Iteration 294/1000 | Loss: 0.00002648
Iteration 295/1000 | Loss: 0.00004366
Iteration 296/1000 | Loss: 0.00002649
Iteration 297/1000 | Loss: 0.00002648
Iteration 298/1000 | Loss: 0.00002648
Iteration 299/1000 | Loss: 0.00002648
Iteration 300/1000 | Loss: 0.00002647
Iteration 301/1000 | Loss: 0.00002647
Iteration 302/1000 | Loss: 0.00002646
Iteration 303/1000 | Loss: 0.00002646
Iteration 304/1000 | Loss: 0.00002646
Iteration 305/1000 | Loss: 0.00002646
Iteration 306/1000 | Loss: 0.00002646
Iteration 307/1000 | Loss: 0.00002646
Iteration 308/1000 | Loss: 0.00002646
Iteration 309/1000 | Loss: 0.00002646
Iteration 310/1000 | Loss: 0.00002646
Iteration 311/1000 | Loss: 0.00002646
Iteration 312/1000 | Loss: 0.00002645
Iteration 313/1000 | Loss: 0.00002673
Iteration 314/1000 | Loss: 0.00004094
Iteration 315/1000 | Loss: 0.00009583
Iteration 316/1000 | Loss: 0.00004174
Iteration 317/1000 | Loss: 0.00003986
Iteration 318/1000 | Loss: 0.00005157
Iteration 319/1000 | Loss: 0.00002929
Iteration 320/1000 | Loss: 0.00003306
Iteration 321/1000 | Loss: 0.00004088
Iteration 322/1000 | Loss: 0.00004088
Iteration 323/1000 | Loss: 0.00007693
Iteration 324/1000 | Loss: 0.00004296
Iteration 325/1000 | Loss: 0.00002732
Iteration 326/1000 | Loss: 0.00002640
Iteration 327/1000 | Loss: 0.00002639
Iteration 328/1000 | Loss: 0.00002639
Iteration 329/1000 | Loss: 0.00002639
Iteration 330/1000 | Loss: 0.00002639
Iteration 331/1000 | Loss: 0.00002638
Iteration 332/1000 | Loss: 0.00002638
Iteration 333/1000 | Loss: 0.00002638
Iteration 334/1000 | Loss: 0.00002638
Iteration 335/1000 | Loss: 0.00002638
Iteration 336/1000 | Loss: 0.00002638
Iteration 337/1000 | Loss: 0.00002638
Iteration 338/1000 | Loss: 0.00002638
Iteration 339/1000 | Loss: 0.00002638
Iteration 340/1000 | Loss: 0.00002638
Iteration 341/1000 | Loss: 0.00002638
Iteration 342/1000 | Loss: 0.00002638
Iteration 343/1000 | Loss: 0.00002637
Iteration 344/1000 | Loss: 0.00002637
Iteration 345/1000 | Loss: 0.00002637
Iteration 346/1000 | Loss: 0.00002637
Iteration 347/1000 | Loss: 0.00002637
Iteration 348/1000 | Loss: 0.00002637
Iteration 349/1000 | Loss: 0.00002637
Iteration 350/1000 | Loss: 0.00002637
Iteration 351/1000 | Loss: 0.00002637
Iteration 352/1000 | Loss: 0.00002637
Iteration 353/1000 | Loss: 0.00002637
Iteration 354/1000 | Loss: 0.00002636
Iteration 355/1000 | Loss: 0.00002636
Iteration 356/1000 | Loss: 0.00002636
Iteration 357/1000 | Loss: 0.00002636
Iteration 358/1000 | Loss: 0.00002636
Iteration 359/1000 | Loss: 0.00002635
Iteration 360/1000 | Loss: 0.00002635
Iteration 361/1000 | Loss: 0.00002635
Iteration 362/1000 | Loss: 0.00002635
Iteration 363/1000 | Loss: 0.00002635
Iteration 364/1000 | Loss: 0.00002635
Iteration 365/1000 | Loss: 0.00002635
Iteration 366/1000 | Loss: 0.00002635
Iteration 367/1000 | Loss: 0.00002635
Iteration 368/1000 | Loss: 0.00002635
Iteration 369/1000 | Loss: 0.00002635
Iteration 370/1000 | Loss: 0.00002635
Iteration 371/1000 | Loss: 0.00002635
Iteration 372/1000 | Loss: 0.00002635
Iteration 373/1000 | Loss: 0.00002635
Iteration 374/1000 | Loss: 0.00002696
Iteration 375/1000 | Loss: 0.00002635
Iteration 376/1000 | Loss: 0.00002635
Iteration 377/1000 | Loss: 0.00002635
Iteration 378/1000 | Loss: 0.00002635
Iteration 379/1000 | Loss: 0.00002635
Iteration 380/1000 | Loss: 0.00002635
Iteration 381/1000 | Loss: 0.00002635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 381. Stopping optimization.
Last 5 losses: [2.6348659957875498e-05, 2.6348659957875498e-05, 2.6348659957875498e-05, 2.6348659957875498e-05, 2.6348659957875498e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6348659957875498e-05

Optimization complete. Final v2v error: 3.824246644973755 mm

Highest mean error: 11.055399894714355 mm for frame 83

Lowest mean error: 3.415785551071167 mm for frame 129

Saving results

Total time: 295.6181688308716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00523261
Iteration 2/25 | Loss: 0.00138291
Iteration 3/25 | Loss: 0.00127495
Iteration 4/25 | Loss: 0.00126688
Iteration 5/25 | Loss: 0.00126404
Iteration 6/25 | Loss: 0.00126385
Iteration 7/25 | Loss: 0.00126385
Iteration 8/25 | Loss: 0.00126385
Iteration 9/25 | Loss: 0.00126385
Iteration 10/25 | Loss: 0.00126385
Iteration 11/25 | Loss: 0.00126385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001263850717805326, 0.001263850717805326, 0.001263850717805326, 0.001263850717805326, 0.001263850717805326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001263850717805326

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46848083
Iteration 2/25 | Loss: 0.00073398
Iteration 3/25 | Loss: 0.00073397
Iteration 4/25 | Loss: 0.00073397
Iteration 5/25 | Loss: 0.00073397
Iteration 6/25 | Loss: 0.00073397
Iteration 7/25 | Loss: 0.00073397
Iteration 8/25 | Loss: 0.00073397
Iteration 9/25 | Loss: 0.00073397
Iteration 10/25 | Loss: 0.00073397
Iteration 11/25 | Loss: 0.00073397
Iteration 12/25 | Loss: 0.00073397
Iteration 13/25 | Loss: 0.00073397
Iteration 14/25 | Loss: 0.00073397
Iteration 15/25 | Loss: 0.00073397
Iteration 16/25 | Loss: 0.00073397
Iteration 17/25 | Loss: 0.00073397
Iteration 18/25 | Loss: 0.00073397
Iteration 19/25 | Loss: 0.00073397
Iteration 20/25 | Loss: 0.00073397
Iteration 21/25 | Loss: 0.00073397
Iteration 22/25 | Loss: 0.00073397
Iteration 23/25 | Loss: 0.00073397
Iteration 24/25 | Loss: 0.00073397
Iteration 25/25 | Loss: 0.00073397

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073397
Iteration 2/1000 | Loss: 0.00004778
Iteration 3/1000 | Loss: 0.00003358
Iteration 4/1000 | Loss: 0.00003006
Iteration 5/1000 | Loss: 0.00002832
Iteration 6/1000 | Loss: 0.00002725
Iteration 7/1000 | Loss: 0.00002642
Iteration 8/1000 | Loss: 0.00002602
Iteration 9/1000 | Loss: 0.00002564
Iteration 10/1000 | Loss: 0.00002517
Iteration 11/1000 | Loss: 0.00002499
Iteration 12/1000 | Loss: 0.00002477
Iteration 13/1000 | Loss: 0.00002471
Iteration 14/1000 | Loss: 0.00002463
Iteration 15/1000 | Loss: 0.00002453
Iteration 16/1000 | Loss: 0.00002448
Iteration 17/1000 | Loss: 0.00002446
Iteration 18/1000 | Loss: 0.00002441
Iteration 19/1000 | Loss: 0.00002441
Iteration 20/1000 | Loss: 0.00002440
Iteration 21/1000 | Loss: 0.00002439
Iteration 22/1000 | Loss: 0.00002436
Iteration 23/1000 | Loss: 0.00002436
Iteration 24/1000 | Loss: 0.00002432
Iteration 25/1000 | Loss: 0.00002432
Iteration 26/1000 | Loss: 0.00002431
Iteration 27/1000 | Loss: 0.00002430
Iteration 28/1000 | Loss: 0.00002430
Iteration 29/1000 | Loss: 0.00002430
Iteration 30/1000 | Loss: 0.00002429
Iteration 31/1000 | Loss: 0.00002428
Iteration 32/1000 | Loss: 0.00002428
Iteration 33/1000 | Loss: 0.00002427
Iteration 34/1000 | Loss: 0.00002427
Iteration 35/1000 | Loss: 0.00002427
Iteration 36/1000 | Loss: 0.00002427
Iteration 37/1000 | Loss: 0.00002427
Iteration 38/1000 | Loss: 0.00002427
Iteration 39/1000 | Loss: 0.00002427
Iteration 40/1000 | Loss: 0.00002427
Iteration 41/1000 | Loss: 0.00002426
Iteration 42/1000 | Loss: 0.00002426
Iteration 43/1000 | Loss: 0.00002426
Iteration 44/1000 | Loss: 0.00002426
Iteration 45/1000 | Loss: 0.00002425
Iteration 46/1000 | Loss: 0.00002424
Iteration 47/1000 | Loss: 0.00002424
Iteration 48/1000 | Loss: 0.00002422
Iteration 49/1000 | Loss: 0.00002421
Iteration 50/1000 | Loss: 0.00002421
Iteration 51/1000 | Loss: 0.00002420
Iteration 52/1000 | Loss: 0.00002420
Iteration 53/1000 | Loss: 0.00002420
Iteration 54/1000 | Loss: 0.00002419
Iteration 55/1000 | Loss: 0.00002419
Iteration 56/1000 | Loss: 0.00002418
Iteration 57/1000 | Loss: 0.00002417
Iteration 58/1000 | Loss: 0.00002416
Iteration 59/1000 | Loss: 0.00002416
Iteration 60/1000 | Loss: 0.00002415
Iteration 61/1000 | Loss: 0.00002415
Iteration 62/1000 | Loss: 0.00002415
Iteration 63/1000 | Loss: 0.00002415
Iteration 64/1000 | Loss: 0.00002415
Iteration 65/1000 | Loss: 0.00002414
Iteration 66/1000 | Loss: 0.00002414
Iteration 67/1000 | Loss: 0.00002413
Iteration 68/1000 | Loss: 0.00002412
Iteration 69/1000 | Loss: 0.00002412
Iteration 70/1000 | Loss: 0.00002412
Iteration 71/1000 | Loss: 0.00002412
Iteration 72/1000 | Loss: 0.00002412
Iteration 73/1000 | Loss: 0.00002411
Iteration 74/1000 | Loss: 0.00002411
Iteration 75/1000 | Loss: 0.00002411
Iteration 76/1000 | Loss: 0.00002410
Iteration 77/1000 | Loss: 0.00002409
Iteration 78/1000 | Loss: 0.00002409
Iteration 79/1000 | Loss: 0.00002409
Iteration 80/1000 | Loss: 0.00002409
Iteration 81/1000 | Loss: 0.00002409
Iteration 82/1000 | Loss: 0.00002409
Iteration 83/1000 | Loss: 0.00002409
Iteration 84/1000 | Loss: 0.00002409
Iteration 85/1000 | Loss: 0.00002409
Iteration 86/1000 | Loss: 0.00002409
Iteration 87/1000 | Loss: 0.00002408
Iteration 88/1000 | Loss: 0.00002408
Iteration 89/1000 | Loss: 0.00002408
Iteration 90/1000 | Loss: 0.00002407
Iteration 91/1000 | Loss: 0.00002407
Iteration 92/1000 | Loss: 0.00002407
Iteration 93/1000 | Loss: 0.00002407
Iteration 94/1000 | Loss: 0.00002407
Iteration 95/1000 | Loss: 0.00002407
Iteration 96/1000 | Loss: 0.00002407
Iteration 97/1000 | Loss: 0.00002407
Iteration 98/1000 | Loss: 0.00002406
Iteration 99/1000 | Loss: 0.00002406
Iteration 100/1000 | Loss: 0.00002406
Iteration 101/1000 | Loss: 0.00002406
Iteration 102/1000 | Loss: 0.00002406
Iteration 103/1000 | Loss: 0.00002406
Iteration 104/1000 | Loss: 0.00002406
Iteration 105/1000 | Loss: 0.00002405
Iteration 106/1000 | Loss: 0.00002405
Iteration 107/1000 | Loss: 0.00002405
Iteration 108/1000 | Loss: 0.00002405
Iteration 109/1000 | Loss: 0.00002405
Iteration 110/1000 | Loss: 0.00002405
Iteration 111/1000 | Loss: 0.00002405
Iteration 112/1000 | Loss: 0.00002405
Iteration 113/1000 | Loss: 0.00002405
Iteration 114/1000 | Loss: 0.00002405
Iteration 115/1000 | Loss: 0.00002405
Iteration 116/1000 | Loss: 0.00002404
Iteration 117/1000 | Loss: 0.00002404
Iteration 118/1000 | Loss: 0.00002404
Iteration 119/1000 | Loss: 0.00002404
Iteration 120/1000 | Loss: 0.00002403
Iteration 121/1000 | Loss: 0.00002403
Iteration 122/1000 | Loss: 0.00002403
Iteration 123/1000 | Loss: 0.00002403
Iteration 124/1000 | Loss: 0.00002403
Iteration 125/1000 | Loss: 0.00002403
Iteration 126/1000 | Loss: 0.00002403
Iteration 127/1000 | Loss: 0.00002403
Iteration 128/1000 | Loss: 0.00002403
Iteration 129/1000 | Loss: 0.00002402
Iteration 130/1000 | Loss: 0.00002402
Iteration 131/1000 | Loss: 0.00002402
Iteration 132/1000 | Loss: 0.00002402
Iteration 133/1000 | Loss: 0.00002402
Iteration 134/1000 | Loss: 0.00002402
Iteration 135/1000 | Loss: 0.00002402
Iteration 136/1000 | Loss: 0.00002402
Iteration 137/1000 | Loss: 0.00002402
Iteration 138/1000 | Loss: 0.00002402
Iteration 139/1000 | Loss: 0.00002402
Iteration 140/1000 | Loss: 0.00002402
Iteration 141/1000 | Loss: 0.00002401
Iteration 142/1000 | Loss: 0.00002401
Iteration 143/1000 | Loss: 0.00002401
Iteration 144/1000 | Loss: 0.00002401
Iteration 145/1000 | Loss: 0.00002401
Iteration 146/1000 | Loss: 0.00002401
Iteration 147/1000 | Loss: 0.00002401
Iteration 148/1000 | Loss: 0.00002401
Iteration 149/1000 | Loss: 0.00002400
Iteration 150/1000 | Loss: 0.00002400
Iteration 151/1000 | Loss: 0.00002400
Iteration 152/1000 | Loss: 0.00002400
Iteration 153/1000 | Loss: 0.00002400
Iteration 154/1000 | Loss: 0.00002400
Iteration 155/1000 | Loss: 0.00002400
Iteration 156/1000 | Loss: 0.00002400
Iteration 157/1000 | Loss: 0.00002400
Iteration 158/1000 | Loss: 0.00002400
Iteration 159/1000 | Loss: 0.00002400
Iteration 160/1000 | Loss: 0.00002400
Iteration 161/1000 | Loss: 0.00002400
Iteration 162/1000 | Loss: 0.00002400
Iteration 163/1000 | Loss: 0.00002400
Iteration 164/1000 | Loss: 0.00002400
Iteration 165/1000 | Loss: 0.00002400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.4002851205295883e-05, 2.4002851205295883e-05, 2.4002851205295883e-05, 2.4002851205295883e-05, 2.4002851205295883e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4002851205295883e-05

Optimization complete. Final v2v error: 3.8992536067962646 mm

Highest mean error: 4.5377960205078125 mm for frame 10

Lowest mean error: 3.0437674522399902 mm for frame 63

Saving results

Total time: 40.271204471588135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862690
Iteration 2/25 | Loss: 0.00226882
Iteration 3/25 | Loss: 0.00190227
Iteration 4/25 | Loss: 0.00173978
Iteration 5/25 | Loss: 0.00149360
Iteration 6/25 | Loss: 0.00141529
Iteration 7/25 | Loss: 0.00140643
Iteration 8/25 | Loss: 0.00140640
Iteration 9/25 | Loss: 0.00140167
Iteration 10/25 | Loss: 0.00139555
Iteration 11/25 | Loss: 0.00139341
Iteration 12/25 | Loss: 0.00139340
Iteration 13/25 | Loss: 0.00139311
Iteration 14/25 | Loss: 0.00139344
Iteration 15/25 | Loss: 0.00139394
Iteration 16/25 | Loss: 0.00139256
Iteration 17/25 | Loss: 0.00139348
Iteration 18/25 | Loss: 0.00139328
Iteration 19/25 | Loss: 0.00139315
Iteration 20/25 | Loss: 0.00139331
Iteration 21/25 | Loss: 0.00139328
Iteration 22/25 | Loss: 0.00139330
Iteration 23/25 | Loss: 0.00139324
Iteration 24/25 | Loss: 0.00139347
Iteration 25/25 | Loss: 0.00139297

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44597781
Iteration 2/25 | Loss: 0.00068207
Iteration 3/25 | Loss: 0.00068205
Iteration 4/25 | Loss: 0.00068205
Iteration 5/25 | Loss: 0.00068205
Iteration 6/25 | Loss: 0.00068205
Iteration 7/25 | Loss: 0.00068205
Iteration 8/25 | Loss: 0.00068204
Iteration 9/25 | Loss: 0.00068204
Iteration 10/25 | Loss: 0.00068204
Iteration 11/25 | Loss: 0.00068204
Iteration 12/25 | Loss: 0.00068204
Iteration 13/25 | Loss: 0.00068204
Iteration 14/25 | Loss: 0.00068204
Iteration 15/25 | Loss: 0.00068204
Iteration 16/25 | Loss: 0.00068204
Iteration 17/25 | Loss: 0.00068204
Iteration 18/25 | Loss: 0.00068204
Iteration 19/25 | Loss: 0.00068204
Iteration 20/25 | Loss: 0.00068204
Iteration 21/25 | Loss: 0.00068204
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006820443668402731, 0.0006820443668402731, 0.0006820443668402731, 0.0006820443668402731, 0.0006820443668402731]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006820443668402731

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068204
Iteration 2/1000 | Loss: 0.00005371
Iteration 3/1000 | Loss: 0.00003884
Iteration 4/1000 | Loss: 0.00003158
Iteration 5/1000 | Loss: 0.00003833
Iteration 6/1000 | Loss: 0.00002947
Iteration 7/1000 | Loss: 0.00003470
Iteration 8/1000 | Loss: 0.00003854
Iteration 9/1000 | Loss: 0.00003573
Iteration 10/1000 | Loss: 0.00003578
Iteration 11/1000 | Loss: 0.00003255
Iteration 12/1000 | Loss: 0.00003826
Iteration 13/1000 | Loss: 0.00004405
Iteration 14/1000 | Loss: 0.00002907
Iteration 15/1000 | Loss: 0.00003304
Iteration 16/1000 | Loss: 0.00003636
Iteration 17/1000 | Loss: 0.00004451
Iteration 18/1000 | Loss: 0.00003695
Iteration 19/1000 | Loss: 0.00003709
Iteration 20/1000 | Loss: 0.00003488
Iteration 21/1000 | Loss: 0.00002862
Iteration 22/1000 | Loss: 0.00002695
Iteration 23/1000 | Loss: 0.00002610
Iteration 24/1000 | Loss: 0.00002572
Iteration 25/1000 | Loss: 0.00002545
Iteration 26/1000 | Loss: 0.00002540
Iteration 27/1000 | Loss: 0.00002530
Iteration 28/1000 | Loss: 0.00002529
Iteration 29/1000 | Loss: 0.00002519
Iteration 30/1000 | Loss: 0.00002518
Iteration 31/1000 | Loss: 0.00002510
Iteration 32/1000 | Loss: 0.00002509
Iteration 33/1000 | Loss: 0.00002508
Iteration 34/1000 | Loss: 0.00002508
Iteration 35/1000 | Loss: 0.00002507
Iteration 36/1000 | Loss: 0.00002507
Iteration 37/1000 | Loss: 0.00002507
Iteration 38/1000 | Loss: 0.00002506
Iteration 39/1000 | Loss: 0.00002506
Iteration 40/1000 | Loss: 0.00002506
Iteration 41/1000 | Loss: 0.00002506
Iteration 42/1000 | Loss: 0.00002506
Iteration 43/1000 | Loss: 0.00002504
Iteration 44/1000 | Loss: 0.00002504
Iteration 45/1000 | Loss: 0.00002504
Iteration 46/1000 | Loss: 0.00002504
Iteration 47/1000 | Loss: 0.00002503
Iteration 48/1000 | Loss: 0.00002503
Iteration 49/1000 | Loss: 0.00002503
Iteration 50/1000 | Loss: 0.00002503
Iteration 51/1000 | Loss: 0.00002503
Iteration 52/1000 | Loss: 0.00002503
Iteration 53/1000 | Loss: 0.00002503
Iteration 54/1000 | Loss: 0.00002502
Iteration 55/1000 | Loss: 0.00002502
Iteration 56/1000 | Loss: 0.00002502
Iteration 57/1000 | Loss: 0.00002502
Iteration 58/1000 | Loss: 0.00002501
Iteration 59/1000 | Loss: 0.00002501
Iteration 60/1000 | Loss: 0.00002501
Iteration 61/1000 | Loss: 0.00002501
Iteration 62/1000 | Loss: 0.00002500
Iteration 63/1000 | Loss: 0.00002500
Iteration 64/1000 | Loss: 0.00002499
Iteration 65/1000 | Loss: 0.00002499
Iteration 66/1000 | Loss: 0.00002499
Iteration 67/1000 | Loss: 0.00002499
Iteration 68/1000 | Loss: 0.00002499
Iteration 69/1000 | Loss: 0.00002499
Iteration 70/1000 | Loss: 0.00002498
Iteration 71/1000 | Loss: 0.00002498
Iteration 72/1000 | Loss: 0.00002498
Iteration 73/1000 | Loss: 0.00002498
Iteration 74/1000 | Loss: 0.00002497
Iteration 75/1000 | Loss: 0.00002497
Iteration 76/1000 | Loss: 0.00002497
Iteration 77/1000 | Loss: 0.00002497
Iteration 78/1000 | Loss: 0.00002497
Iteration 79/1000 | Loss: 0.00002496
Iteration 80/1000 | Loss: 0.00002496
Iteration 81/1000 | Loss: 0.00002496
Iteration 82/1000 | Loss: 0.00002496
Iteration 83/1000 | Loss: 0.00002496
Iteration 84/1000 | Loss: 0.00002496
Iteration 85/1000 | Loss: 0.00002496
Iteration 86/1000 | Loss: 0.00002495
Iteration 87/1000 | Loss: 0.00002494
Iteration 88/1000 | Loss: 0.00002494
Iteration 89/1000 | Loss: 0.00002494
Iteration 90/1000 | Loss: 0.00002494
Iteration 91/1000 | Loss: 0.00002494
Iteration 92/1000 | Loss: 0.00002494
Iteration 93/1000 | Loss: 0.00002494
Iteration 94/1000 | Loss: 0.00002494
Iteration 95/1000 | Loss: 0.00002494
Iteration 96/1000 | Loss: 0.00002494
Iteration 97/1000 | Loss: 0.00002494
Iteration 98/1000 | Loss: 0.00002494
Iteration 99/1000 | Loss: 0.00002494
Iteration 100/1000 | Loss: 0.00002494
Iteration 101/1000 | Loss: 0.00002494
Iteration 102/1000 | Loss: 0.00002494
Iteration 103/1000 | Loss: 0.00002494
Iteration 104/1000 | Loss: 0.00002494
Iteration 105/1000 | Loss: 0.00002494
Iteration 106/1000 | Loss: 0.00002494
Iteration 107/1000 | Loss: 0.00002494
Iteration 108/1000 | Loss: 0.00002494
Iteration 109/1000 | Loss: 0.00002494
Iteration 110/1000 | Loss: 0.00002494
Iteration 111/1000 | Loss: 0.00002494
Iteration 112/1000 | Loss: 0.00002494
Iteration 113/1000 | Loss: 0.00002494
Iteration 114/1000 | Loss: 0.00002494
Iteration 115/1000 | Loss: 0.00002494
Iteration 116/1000 | Loss: 0.00002494
Iteration 117/1000 | Loss: 0.00002494
Iteration 118/1000 | Loss: 0.00002494
Iteration 119/1000 | Loss: 0.00002494
Iteration 120/1000 | Loss: 0.00002494
Iteration 121/1000 | Loss: 0.00002494
Iteration 122/1000 | Loss: 0.00002494
Iteration 123/1000 | Loss: 0.00002494
Iteration 124/1000 | Loss: 0.00002494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.494267391739413e-05, 2.494267391739413e-05, 2.494267391739413e-05, 2.494267391739413e-05, 2.494267391739413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.494267391739413e-05

Optimization complete. Final v2v error: 4.158534049987793 mm

Highest mean error: 5.196091175079346 mm for frame 103

Lowest mean error: 4.052427291870117 mm for frame 185

Saving results

Total time: 101.45052337646484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783661
Iteration 2/25 | Loss: 0.00138501
Iteration 3/25 | Loss: 0.00128091
Iteration 4/25 | Loss: 0.00127290
Iteration 5/25 | Loss: 0.00127053
Iteration 6/25 | Loss: 0.00127053
Iteration 7/25 | Loss: 0.00127015
Iteration 8/25 | Loss: 0.00127015
Iteration 9/25 | Loss: 0.00127015
Iteration 10/25 | Loss: 0.00127015
Iteration 11/25 | Loss: 0.00127015
Iteration 12/25 | Loss: 0.00127015
Iteration 13/25 | Loss: 0.00127015
Iteration 14/25 | Loss: 0.00127015
Iteration 15/25 | Loss: 0.00127015
Iteration 16/25 | Loss: 0.00127015
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012701494852080941, 0.0012701494852080941, 0.0012701494852080941, 0.0012701494852080941, 0.0012701494852080941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012701494852080941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39891803
Iteration 2/25 | Loss: 0.00055380
Iteration 3/25 | Loss: 0.00055375
Iteration 4/25 | Loss: 0.00055375
Iteration 5/25 | Loss: 0.00055375
Iteration 6/25 | Loss: 0.00055375
Iteration 7/25 | Loss: 0.00055375
Iteration 8/25 | Loss: 0.00055375
Iteration 9/25 | Loss: 0.00055375
Iteration 10/25 | Loss: 0.00055375
Iteration 11/25 | Loss: 0.00055375
Iteration 12/25 | Loss: 0.00055375
Iteration 13/25 | Loss: 0.00055375
Iteration 14/25 | Loss: 0.00055375
Iteration 15/25 | Loss: 0.00055375
Iteration 16/25 | Loss: 0.00055375
Iteration 17/25 | Loss: 0.00055375
Iteration 18/25 | Loss: 0.00055375
Iteration 19/25 | Loss: 0.00055375
Iteration 20/25 | Loss: 0.00055375
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005537469987757504, 0.0005537469987757504, 0.0005537469987757504, 0.0005537469987757504, 0.0005537469987757504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005537469987757504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055375
Iteration 2/1000 | Loss: 0.00002853
Iteration 3/1000 | Loss: 0.00002078
Iteration 4/1000 | Loss: 0.00001910
Iteration 5/1000 | Loss: 0.00001839
Iteration 6/1000 | Loss: 0.00001796
Iteration 7/1000 | Loss: 0.00001768
Iteration 8/1000 | Loss: 0.00001744
Iteration 9/1000 | Loss: 0.00001718
Iteration 10/1000 | Loss: 0.00001704
Iteration 11/1000 | Loss: 0.00001693
Iteration 12/1000 | Loss: 0.00001679
Iteration 13/1000 | Loss: 0.00001673
Iteration 14/1000 | Loss: 0.00001665
Iteration 15/1000 | Loss: 0.00001664
Iteration 16/1000 | Loss: 0.00001664
Iteration 17/1000 | Loss: 0.00001664
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001663
Iteration 20/1000 | Loss: 0.00001662
Iteration 21/1000 | Loss: 0.00001661
Iteration 22/1000 | Loss: 0.00001661
Iteration 23/1000 | Loss: 0.00001661
Iteration 24/1000 | Loss: 0.00001661
Iteration 25/1000 | Loss: 0.00001661
Iteration 26/1000 | Loss: 0.00001660
Iteration 27/1000 | Loss: 0.00001660
Iteration 28/1000 | Loss: 0.00001660
Iteration 29/1000 | Loss: 0.00001659
Iteration 30/1000 | Loss: 0.00001659
Iteration 31/1000 | Loss: 0.00001659
Iteration 32/1000 | Loss: 0.00001659
Iteration 33/1000 | Loss: 0.00001659
Iteration 34/1000 | Loss: 0.00001658
Iteration 35/1000 | Loss: 0.00001656
Iteration 36/1000 | Loss: 0.00001656
Iteration 37/1000 | Loss: 0.00001656
Iteration 38/1000 | Loss: 0.00001656
Iteration 39/1000 | Loss: 0.00001655
Iteration 40/1000 | Loss: 0.00001655
Iteration 41/1000 | Loss: 0.00001655
Iteration 42/1000 | Loss: 0.00001655
Iteration 43/1000 | Loss: 0.00001654
Iteration 44/1000 | Loss: 0.00001651
Iteration 45/1000 | Loss: 0.00001646
Iteration 46/1000 | Loss: 0.00001646
Iteration 47/1000 | Loss: 0.00001644
Iteration 48/1000 | Loss: 0.00001643
Iteration 49/1000 | Loss: 0.00001643
Iteration 50/1000 | Loss: 0.00001642
Iteration 51/1000 | Loss: 0.00001642
Iteration 52/1000 | Loss: 0.00001641
Iteration 53/1000 | Loss: 0.00001641
Iteration 54/1000 | Loss: 0.00001641
Iteration 55/1000 | Loss: 0.00001641
Iteration 56/1000 | Loss: 0.00001641
Iteration 57/1000 | Loss: 0.00001638
Iteration 58/1000 | Loss: 0.00001638
Iteration 59/1000 | Loss: 0.00001638
Iteration 60/1000 | Loss: 0.00001637
Iteration 61/1000 | Loss: 0.00001637
Iteration 62/1000 | Loss: 0.00001635
Iteration 63/1000 | Loss: 0.00001635
Iteration 64/1000 | Loss: 0.00001634
Iteration 65/1000 | Loss: 0.00001633
Iteration 66/1000 | Loss: 0.00001632
Iteration 67/1000 | Loss: 0.00001632
Iteration 68/1000 | Loss: 0.00001631
Iteration 69/1000 | Loss: 0.00001630
Iteration 70/1000 | Loss: 0.00001630
Iteration 71/1000 | Loss: 0.00001630
Iteration 72/1000 | Loss: 0.00001630
Iteration 73/1000 | Loss: 0.00001630
Iteration 74/1000 | Loss: 0.00001630
Iteration 75/1000 | Loss: 0.00001630
Iteration 76/1000 | Loss: 0.00001630
Iteration 77/1000 | Loss: 0.00001630
Iteration 78/1000 | Loss: 0.00001630
Iteration 79/1000 | Loss: 0.00001630
Iteration 80/1000 | Loss: 0.00001630
Iteration 81/1000 | Loss: 0.00001630
Iteration 82/1000 | Loss: 0.00001630
Iteration 83/1000 | Loss: 0.00001630
Iteration 84/1000 | Loss: 0.00001630
Iteration 85/1000 | Loss: 0.00001630
Iteration 86/1000 | Loss: 0.00001630
Iteration 87/1000 | Loss: 0.00001630
Iteration 88/1000 | Loss: 0.00001630
Iteration 89/1000 | Loss: 0.00001630
Iteration 90/1000 | Loss: 0.00001630
Iteration 91/1000 | Loss: 0.00001630
Iteration 92/1000 | Loss: 0.00001630
Iteration 93/1000 | Loss: 0.00001630
Iteration 94/1000 | Loss: 0.00001630
Iteration 95/1000 | Loss: 0.00001630
Iteration 96/1000 | Loss: 0.00001630
Iteration 97/1000 | Loss: 0.00001630
Iteration 98/1000 | Loss: 0.00001630
Iteration 99/1000 | Loss: 0.00001630
Iteration 100/1000 | Loss: 0.00001630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.629957478144206e-05, 1.629957478144206e-05, 1.629957478144206e-05, 1.629957478144206e-05, 1.629957478144206e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.629957478144206e-05

Optimization complete. Final v2v error: 3.403935432434082 mm

Highest mean error: 3.5508718490600586 mm for frame 43

Lowest mean error: 3.3032193183898926 mm for frame 129

Saving results

Total time: 32.7967414855957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429767
Iteration 2/25 | Loss: 0.00134035
Iteration 3/25 | Loss: 0.00123068
Iteration 4/25 | Loss: 0.00121529
Iteration 5/25 | Loss: 0.00121082
Iteration 6/25 | Loss: 0.00120945
Iteration 7/25 | Loss: 0.00120931
Iteration 8/25 | Loss: 0.00120931
Iteration 9/25 | Loss: 0.00120931
Iteration 10/25 | Loss: 0.00120931
Iteration 11/25 | Loss: 0.00120931
Iteration 12/25 | Loss: 0.00120931
Iteration 13/25 | Loss: 0.00120931
Iteration 14/25 | Loss: 0.00120931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012093114200979471, 0.0012093114200979471, 0.0012093114200979471, 0.0012093114200979471, 0.0012093114200979471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012093114200979471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45475292
Iteration 2/25 | Loss: 0.00072746
Iteration 3/25 | Loss: 0.00072746
Iteration 4/25 | Loss: 0.00072746
Iteration 5/25 | Loss: 0.00072746
Iteration 6/25 | Loss: 0.00072746
Iteration 7/25 | Loss: 0.00072746
Iteration 8/25 | Loss: 0.00072746
Iteration 9/25 | Loss: 0.00072746
Iteration 10/25 | Loss: 0.00072746
Iteration 11/25 | Loss: 0.00072746
Iteration 12/25 | Loss: 0.00072746
Iteration 13/25 | Loss: 0.00072746
Iteration 14/25 | Loss: 0.00072746
Iteration 15/25 | Loss: 0.00072746
Iteration 16/25 | Loss: 0.00072746
Iteration 17/25 | Loss: 0.00072746
Iteration 18/25 | Loss: 0.00072746
Iteration 19/25 | Loss: 0.00072746
Iteration 20/25 | Loss: 0.00072746
Iteration 21/25 | Loss: 0.00072746
Iteration 22/25 | Loss: 0.00072746
Iteration 23/25 | Loss: 0.00072746
Iteration 24/25 | Loss: 0.00072746
Iteration 25/25 | Loss: 0.00072746
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007274554809555411, 0.0007274554809555411, 0.0007274554809555411, 0.0007274554809555411, 0.0007274554809555411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007274554809555411

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072746
Iteration 2/1000 | Loss: 0.00003302
Iteration 3/1000 | Loss: 0.00001941
Iteration 4/1000 | Loss: 0.00001689
Iteration 5/1000 | Loss: 0.00001595
Iteration 6/1000 | Loss: 0.00001525
Iteration 7/1000 | Loss: 0.00001481
Iteration 8/1000 | Loss: 0.00001445
Iteration 9/1000 | Loss: 0.00001425
Iteration 10/1000 | Loss: 0.00001403
Iteration 11/1000 | Loss: 0.00001387
Iteration 12/1000 | Loss: 0.00001383
Iteration 13/1000 | Loss: 0.00001380
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001377
Iteration 16/1000 | Loss: 0.00001376
Iteration 17/1000 | Loss: 0.00001375
Iteration 18/1000 | Loss: 0.00001374
Iteration 19/1000 | Loss: 0.00001373
Iteration 20/1000 | Loss: 0.00001371
Iteration 21/1000 | Loss: 0.00001371
Iteration 22/1000 | Loss: 0.00001370
Iteration 23/1000 | Loss: 0.00001370
Iteration 24/1000 | Loss: 0.00001369
Iteration 25/1000 | Loss: 0.00001367
Iteration 26/1000 | Loss: 0.00001366
Iteration 27/1000 | Loss: 0.00001366
Iteration 28/1000 | Loss: 0.00001366
Iteration 29/1000 | Loss: 0.00001365
Iteration 30/1000 | Loss: 0.00001365
Iteration 31/1000 | Loss: 0.00001365
Iteration 32/1000 | Loss: 0.00001364
Iteration 33/1000 | Loss: 0.00001364
Iteration 34/1000 | Loss: 0.00001364
Iteration 35/1000 | Loss: 0.00001363
Iteration 36/1000 | Loss: 0.00001362
Iteration 37/1000 | Loss: 0.00001361
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001360
Iteration 40/1000 | Loss: 0.00001360
Iteration 41/1000 | Loss: 0.00001359
Iteration 42/1000 | Loss: 0.00001359
Iteration 43/1000 | Loss: 0.00001359
Iteration 44/1000 | Loss: 0.00001358
Iteration 45/1000 | Loss: 0.00001358
Iteration 46/1000 | Loss: 0.00001357
Iteration 47/1000 | Loss: 0.00001356
Iteration 48/1000 | Loss: 0.00001356
Iteration 49/1000 | Loss: 0.00001356
Iteration 50/1000 | Loss: 0.00001355
Iteration 51/1000 | Loss: 0.00001355
Iteration 52/1000 | Loss: 0.00001355
Iteration 53/1000 | Loss: 0.00001355
Iteration 54/1000 | Loss: 0.00001355
Iteration 55/1000 | Loss: 0.00001355
Iteration 56/1000 | Loss: 0.00001355
Iteration 57/1000 | Loss: 0.00001354
Iteration 58/1000 | Loss: 0.00001354
Iteration 59/1000 | Loss: 0.00001354
Iteration 60/1000 | Loss: 0.00001353
Iteration 61/1000 | Loss: 0.00001353
Iteration 62/1000 | Loss: 0.00001353
Iteration 63/1000 | Loss: 0.00001353
Iteration 64/1000 | Loss: 0.00001352
Iteration 65/1000 | Loss: 0.00001352
Iteration 66/1000 | Loss: 0.00001352
Iteration 67/1000 | Loss: 0.00001352
Iteration 68/1000 | Loss: 0.00001352
Iteration 69/1000 | Loss: 0.00001352
Iteration 70/1000 | Loss: 0.00001352
Iteration 71/1000 | Loss: 0.00001352
Iteration 72/1000 | Loss: 0.00001352
Iteration 73/1000 | Loss: 0.00001352
Iteration 74/1000 | Loss: 0.00001352
Iteration 75/1000 | Loss: 0.00001352
Iteration 76/1000 | Loss: 0.00001352
Iteration 77/1000 | Loss: 0.00001351
Iteration 78/1000 | Loss: 0.00001351
Iteration 79/1000 | Loss: 0.00001351
Iteration 80/1000 | Loss: 0.00001351
Iteration 81/1000 | Loss: 0.00001351
Iteration 82/1000 | Loss: 0.00001351
Iteration 83/1000 | Loss: 0.00001351
Iteration 84/1000 | Loss: 0.00001351
Iteration 85/1000 | Loss: 0.00001350
Iteration 86/1000 | Loss: 0.00001350
Iteration 87/1000 | Loss: 0.00001350
Iteration 88/1000 | Loss: 0.00001350
Iteration 89/1000 | Loss: 0.00001350
Iteration 90/1000 | Loss: 0.00001350
Iteration 91/1000 | Loss: 0.00001350
Iteration 92/1000 | Loss: 0.00001350
Iteration 93/1000 | Loss: 0.00001350
Iteration 94/1000 | Loss: 0.00001350
Iteration 95/1000 | Loss: 0.00001349
Iteration 96/1000 | Loss: 0.00001349
Iteration 97/1000 | Loss: 0.00001349
Iteration 98/1000 | Loss: 0.00001349
Iteration 99/1000 | Loss: 0.00001349
Iteration 100/1000 | Loss: 0.00001349
Iteration 101/1000 | Loss: 0.00001349
Iteration 102/1000 | Loss: 0.00001348
Iteration 103/1000 | Loss: 0.00001348
Iteration 104/1000 | Loss: 0.00001348
Iteration 105/1000 | Loss: 0.00001347
Iteration 106/1000 | Loss: 0.00001347
Iteration 107/1000 | Loss: 0.00001347
Iteration 108/1000 | Loss: 0.00001347
Iteration 109/1000 | Loss: 0.00001347
Iteration 110/1000 | Loss: 0.00001347
Iteration 111/1000 | Loss: 0.00001347
Iteration 112/1000 | Loss: 0.00001346
Iteration 113/1000 | Loss: 0.00001346
Iteration 114/1000 | Loss: 0.00001346
Iteration 115/1000 | Loss: 0.00001345
Iteration 116/1000 | Loss: 0.00001345
Iteration 117/1000 | Loss: 0.00001345
Iteration 118/1000 | Loss: 0.00001344
Iteration 119/1000 | Loss: 0.00001344
Iteration 120/1000 | Loss: 0.00001344
Iteration 121/1000 | Loss: 0.00001344
Iteration 122/1000 | Loss: 0.00001343
Iteration 123/1000 | Loss: 0.00001343
Iteration 124/1000 | Loss: 0.00001343
Iteration 125/1000 | Loss: 0.00001343
Iteration 126/1000 | Loss: 0.00001343
Iteration 127/1000 | Loss: 0.00001343
Iteration 128/1000 | Loss: 0.00001342
Iteration 129/1000 | Loss: 0.00001342
Iteration 130/1000 | Loss: 0.00001342
Iteration 131/1000 | Loss: 0.00001342
Iteration 132/1000 | Loss: 0.00001342
Iteration 133/1000 | Loss: 0.00001342
Iteration 134/1000 | Loss: 0.00001342
Iteration 135/1000 | Loss: 0.00001341
Iteration 136/1000 | Loss: 0.00001341
Iteration 137/1000 | Loss: 0.00001341
Iteration 138/1000 | Loss: 0.00001341
Iteration 139/1000 | Loss: 0.00001341
Iteration 140/1000 | Loss: 0.00001341
Iteration 141/1000 | Loss: 0.00001341
Iteration 142/1000 | Loss: 0.00001341
Iteration 143/1000 | Loss: 0.00001341
Iteration 144/1000 | Loss: 0.00001341
Iteration 145/1000 | Loss: 0.00001340
Iteration 146/1000 | Loss: 0.00001340
Iteration 147/1000 | Loss: 0.00001340
Iteration 148/1000 | Loss: 0.00001340
Iteration 149/1000 | Loss: 0.00001340
Iteration 150/1000 | Loss: 0.00001340
Iteration 151/1000 | Loss: 0.00001340
Iteration 152/1000 | Loss: 0.00001340
Iteration 153/1000 | Loss: 0.00001340
Iteration 154/1000 | Loss: 0.00001340
Iteration 155/1000 | Loss: 0.00001340
Iteration 156/1000 | Loss: 0.00001340
Iteration 157/1000 | Loss: 0.00001340
Iteration 158/1000 | Loss: 0.00001340
Iteration 159/1000 | Loss: 0.00001340
Iteration 160/1000 | Loss: 0.00001339
Iteration 161/1000 | Loss: 0.00001339
Iteration 162/1000 | Loss: 0.00001339
Iteration 163/1000 | Loss: 0.00001339
Iteration 164/1000 | Loss: 0.00001339
Iteration 165/1000 | Loss: 0.00001339
Iteration 166/1000 | Loss: 0.00001339
Iteration 167/1000 | Loss: 0.00001339
Iteration 168/1000 | Loss: 0.00001339
Iteration 169/1000 | Loss: 0.00001339
Iteration 170/1000 | Loss: 0.00001338
Iteration 171/1000 | Loss: 0.00001338
Iteration 172/1000 | Loss: 0.00001338
Iteration 173/1000 | Loss: 0.00001338
Iteration 174/1000 | Loss: 0.00001338
Iteration 175/1000 | Loss: 0.00001338
Iteration 176/1000 | Loss: 0.00001338
Iteration 177/1000 | Loss: 0.00001338
Iteration 178/1000 | Loss: 0.00001338
Iteration 179/1000 | Loss: 0.00001338
Iteration 180/1000 | Loss: 0.00001338
Iteration 181/1000 | Loss: 0.00001338
Iteration 182/1000 | Loss: 0.00001338
Iteration 183/1000 | Loss: 0.00001338
Iteration 184/1000 | Loss: 0.00001338
Iteration 185/1000 | Loss: 0.00001338
Iteration 186/1000 | Loss: 0.00001338
Iteration 187/1000 | Loss: 0.00001337
Iteration 188/1000 | Loss: 0.00001337
Iteration 189/1000 | Loss: 0.00001337
Iteration 190/1000 | Loss: 0.00001337
Iteration 191/1000 | Loss: 0.00001337
Iteration 192/1000 | Loss: 0.00001337
Iteration 193/1000 | Loss: 0.00001337
Iteration 194/1000 | Loss: 0.00001337
Iteration 195/1000 | Loss: 0.00001337
Iteration 196/1000 | Loss: 0.00001337
Iteration 197/1000 | Loss: 0.00001337
Iteration 198/1000 | Loss: 0.00001337
Iteration 199/1000 | Loss: 0.00001336
Iteration 200/1000 | Loss: 0.00001336
Iteration 201/1000 | Loss: 0.00001336
Iteration 202/1000 | Loss: 0.00001336
Iteration 203/1000 | Loss: 0.00001336
Iteration 204/1000 | Loss: 0.00001336
Iteration 205/1000 | Loss: 0.00001336
Iteration 206/1000 | Loss: 0.00001336
Iteration 207/1000 | Loss: 0.00001336
Iteration 208/1000 | Loss: 0.00001336
Iteration 209/1000 | Loss: 0.00001336
Iteration 210/1000 | Loss: 0.00001336
Iteration 211/1000 | Loss: 0.00001336
Iteration 212/1000 | Loss: 0.00001336
Iteration 213/1000 | Loss: 0.00001336
Iteration 214/1000 | Loss: 0.00001336
Iteration 215/1000 | Loss: 0.00001336
Iteration 216/1000 | Loss: 0.00001336
Iteration 217/1000 | Loss: 0.00001336
Iteration 218/1000 | Loss: 0.00001336
Iteration 219/1000 | Loss: 0.00001336
Iteration 220/1000 | Loss: 0.00001336
Iteration 221/1000 | Loss: 0.00001336
Iteration 222/1000 | Loss: 0.00001336
Iteration 223/1000 | Loss: 0.00001336
Iteration 224/1000 | Loss: 0.00001336
Iteration 225/1000 | Loss: 0.00001336
Iteration 226/1000 | Loss: 0.00001336
Iteration 227/1000 | Loss: 0.00001336
Iteration 228/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.3357362149690744e-05, 1.3357362149690744e-05, 1.3357362149690744e-05, 1.3357362149690744e-05, 1.3357362149690744e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3357362149690744e-05

Optimization complete. Final v2v error: 3.093768358230591 mm

Highest mean error: 4.10257625579834 mm for frame 64

Lowest mean error: 2.8408000469207764 mm for frame 182

Saving results

Total time: 46.29079055786133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00942099
Iteration 2/25 | Loss: 0.00176660
Iteration 3/25 | Loss: 0.00142420
Iteration 4/25 | Loss: 0.00140067
Iteration 5/25 | Loss: 0.00139265
Iteration 6/25 | Loss: 0.00139151
Iteration 7/25 | Loss: 0.00139151
Iteration 8/25 | Loss: 0.00139151
Iteration 9/25 | Loss: 0.00139151
Iteration 10/25 | Loss: 0.00139151
Iteration 11/25 | Loss: 0.00139151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013915062882006168, 0.0013915062882006168, 0.0013915062882006168, 0.0013915062882006168, 0.0013915062882006168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013915062882006168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89804244
Iteration 2/25 | Loss: 0.00084835
Iteration 3/25 | Loss: 0.00084835
Iteration 4/25 | Loss: 0.00084835
Iteration 5/25 | Loss: 0.00084835
Iteration 6/25 | Loss: 0.00084835
Iteration 7/25 | Loss: 0.00084835
Iteration 8/25 | Loss: 0.00084835
Iteration 9/25 | Loss: 0.00084835
Iteration 10/25 | Loss: 0.00084835
Iteration 11/25 | Loss: 0.00084835
Iteration 12/25 | Loss: 0.00084835
Iteration 13/25 | Loss: 0.00084835
Iteration 14/25 | Loss: 0.00084835
Iteration 15/25 | Loss: 0.00084835
Iteration 16/25 | Loss: 0.00084835
Iteration 17/25 | Loss: 0.00084835
Iteration 18/25 | Loss: 0.00084835
Iteration 19/25 | Loss: 0.00084835
Iteration 20/25 | Loss: 0.00084835
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008483454585075378, 0.0008483454585075378, 0.0008483454585075378, 0.0008483454585075378, 0.0008483454585075378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008483454585075378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084835
Iteration 2/1000 | Loss: 0.00006981
Iteration 3/1000 | Loss: 0.00004381
Iteration 4/1000 | Loss: 0.00003995
Iteration 5/1000 | Loss: 0.00003787
Iteration 6/1000 | Loss: 0.00003683
Iteration 7/1000 | Loss: 0.00003595
Iteration 8/1000 | Loss: 0.00003530
Iteration 9/1000 | Loss: 0.00003475
Iteration 10/1000 | Loss: 0.00003440
Iteration 11/1000 | Loss: 0.00003406
Iteration 12/1000 | Loss: 0.00003376
Iteration 13/1000 | Loss: 0.00003341
Iteration 14/1000 | Loss: 0.00003315
Iteration 15/1000 | Loss: 0.00003287
Iteration 16/1000 | Loss: 0.00003262
Iteration 17/1000 | Loss: 0.00003242
Iteration 18/1000 | Loss: 0.00003238
Iteration 19/1000 | Loss: 0.00003232
Iteration 20/1000 | Loss: 0.00003222
Iteration 21/1000 | Loss: 0.00003211
Iteration 22/1000 | Loss: 0.00003202
Iteration 23/1000 | Loss: 0.00003200
Iteration 24/1000 | Loss: 0.00003195
Iteration 25/1000 | Loss: 0.00003192
Iteration 26/1000 | Loss: 0.00003191
Iteration 27/1000 | Loss: 0.00003191
Iteration 28/1000 | Loss: 0.00003190
Iteration 29/1000 | Loss: 0.00003189
Iteration 30/1000 | Loss: 0.00003187
Iteration 31/1000 | Loss: 0.00003187
Iteration 32/1000 | Loss: 0.00003187
Iteration 33/1000 | Loss: 0.00003187
Iteration 34/1000 | Loss: 0.00003187
Iteration 35/1000 | Loss: 0.00003187
Iteration 36/1000 | Loss: 0.00003187
Iteration 37/1000 | Loss: 0.00003187
Iteration 38/1000 | Loss: 0.00003187
Iteration 39/1000 | Loss: 0.00003187
Iteration 40/1000 | Loss: 0.00003186
Iteration 41/1000 | Loss: 0.00003186
Iteration 42/1000 | Loss: 0.00003186
Iteration 43/1000 | Loss: 0.00003186
Iteration 44/1000 | Loss: 0.00003186
Iteration 45/1000 | Loss: 0.00003186
Iteration 46/1000 | Loss: 0.00003186
Iteration 47/1000 | Loss: 0.00003186
Iteration 48/1000 | Loss: 0.00003185
Iteration 49/1000 | Loss: 0.00003185
Iteration 50/1000 | Loss: 0.00003185
Iteration 51/1000 | Loss: 0.00003184
Iteration 52/1000 | Loss: 0.00003183
Iteration 53/1000 | Loss: 0.00003183
Iteration 54/1000 | Loss: 0.00003183
Iteration 55/1000 | Loss: 0.00003183
Iteration 56/1000 | Loss: 0.00003183
Iteration 57/1000 | Loss: 0.00003183
Iteration 58/1000 | Loss: 0.00003183
Iteration 59/1000 | Loss: 0.00003183
Iteration 60/1000 | Loss: 0.00003183
Iteration 61/1000 | Loss: 0.00003183
Iteration 62/1000 | Loss: 0.00003183
Iteration 63/1000 | Loss: 0.00003182
Iteration 64/1000 | Loss: 0.00003182
Iteration 65/1000 | Loss: 0.00003181
Iteration 66/1000 | Loss: 0.00003181
Iteration 67/1000 | Loss: 0.00003181
Iteration 68/1000 | Loss: 0.00003180
Iteration 69/1000 | Loss: 0.00003180
Iteration 70/1000 | Loss: 0.00003180
Iteration 71/1000 | Loss: 0.00003179
Iteration 72/1000 | Loss: 0.00003179
Iteration 73/1000 | Loss: 0.00003178
Iteration 74/1000 | Loss: 0.00003177
Iteration 75/1000 | Loss: 0.00003177
Iteration 76/1000 | Loss: 0.00003177
Iteration 77/1000 | Loss: 0.00003177
Iteration 78/1000 | Loss: 0.00003177
Iteration 79/1000 | Loss: 0.00003177
Iteration 80/1000 | Loss: 0.00003177
Iteration 81/1000 | Loss: 0.00003177
Iteration 82/1000 | Loss: 0.00003177
Iteration 83/1000 | Loss: 0.00003177
Iteration 84/1000 | Loss: 0.00003177
Iteration 85/1000 | Loss: 0.00003177
Iteration 86/1000 | Loss: 0.00003176
Iteration 87/1000 | Loss: 0.00003176
Iteration 88/1000 | Loss: 0.00003176
Iteration 89/1000 | Loss: 0.00003176
Iteration 90/1000 | Loss: 0.00003176
Iteration 91/1000 | Loss: 0.00003175
Iteration 92/1000 | Loss: 0.00003175
Iteration 93/1000 | Loss: 0.00003175
Iteration 94/1000 | Loss: 0.00003175
Iteration 95/1000 | Loss: 0.00003175
Iteration 96/1000 | Loss: 0.00003175
Iteration 97/1000 | Loss: 0.00003175
Iteration 98/1000 | Loss: 0.00003175
Iteration 99/1000 | Loss: 0.00003175
Iteration 100/1000 | Loss: 0.00003174
Iteration 101/1000 | Loss: 0.00003174
Iteration 102/1000 | Loss: 0.00003174
Iteration 103/1000 | Loss: 0.00003174
Iteration 104/1000 | Loss: 0.00003174
Iteration 105/1000 | Loss: 0.00003173
Iteration 106/1000 | Loss: 0.00003173
Iteration 107/1000 | Loss: 0.00003173
Iteration 108/1000 | Loss: 0.00003173
Iteration 109/1000 | Loss: 0.00003173
Iteration 110/1000 | Loss: 0.00003173
Iteration 111/1000 | Loss: 0.00003173
Iteration 112/1000 | Loss: 0.00003173
Iteration 113/1000 | Loss: 0.00003172
Iteration 114/1000 | Loss: 0.00003172
Iteration 115/1000 | Loss: 0.00003172
Iteration 116/1000 | Loss: 0.00003172
Iteration 117/1000 | Loss: 0.00003171
Iteration 118/1000 | Loss: 0.00003171
Iteration 119/1000 | Loss: 0.00003171
Iteration 120/1000 | Loss: 0.00003171
Iteration 121/1000 | Loss: 0.00003170
Iteration 122/1000 | Loss: 0.00003170
Iteration 123/1000 | Loss: 0.00003170
Iteration 124/1000 | Loss: 0.00003169
Iteration 125/1000 | Loss: 0.00003169
Iteration 126/1000 | Loss: 0.00003169
Iteration 127/1000 | Loss: 0.00003168
Iteration 128/1000 | Loss: 0.00003168
Iteration 129/1000 | Loss: 0.00003168
Iteration 130/1000 | Loss: 0.00003168
Iteration 131/1000 | Loss: 0.00003168
Iteration 132/1000 | Loss: 0.00003168
Iteration 133/1000 | Loss: 0.00003168
Iteration 134/1000 | Loss: 0.00003168
Iteration 135/1000 | Loss: 0.00003168
Iteration 136/1000 | Loss: 0.00003167
Iteration 137/1000 | Loss: 0.00003167
Iteration 138/1000 | Loss: 0.00003167
Iteration 139/1000 | Loss: 0.00003167
Iteration 140/1000 | Loss: 0.00003166
Iteration 141/1000 | Loss: 0.00003166
Iteration 142/1000 | Loss: 0.00003166
Iteration 143/1000 | Loss: 0.00003166
Iteration 144/1000 | Loss: 0.00003166
Iteration 145/1000 | Loss: 0.00003166
Iteration 146/1000 | Loss: 0.00003166
Iteration 147/1000 | Loss: 0.00003166
Iteration 148/1000 | Loss: 0.00003166
Iteration 149/1000 | Loss: 0.00003165
Iteration 150/1000 | Loss: 0.00003165
Iteration 151/1000 | Loss: 0.00003165
Iteration 152/1000 | Loss: 0.00003165
Iteration 153/1000 | Loss: 0.00003165
Iteration 154/1000 | Loss: 0.00003165
Iteration 155/1000 | Loss: 0.00003165
Iteration 156/1000 | Loss: 0.00003165
Iteration 157/1000 | Loss: 0.00003165
Iteration 158/1000 | Loss: 0.00003165
Iteration 159/1000 | Loss: 0.00003165
Iteration 160/1000 | Loss: 0.00003164
Iteration 161/1000 | Loss: 0.00003164
Iteration 162/1000 | Loss: 0.00003164
Iteration 163/1000 | Loss: 0.00003164
Iteration 164/1000 | Loss: 0.00003164
Iteration 165/1000 | Loss: 0.00003164
Iteration 166/1000 | Loss: 0.00003164
Iteration 167/1000 | Loss: 0.00003164
Iteration 168/1000 | Loss: 0.00003164
Iteration 169/1000 | Loss: 0.00003164
Iteration 170/1000 | Loss: 0.00003164
Iteration 171/1000 | Loss: 0.00003164
Iteration 172/1000 | Loss: 0.00003164
Iteration 173/1000 | Loss: 0.00003164
Iteration 174/1000 | Loss: 0.00003164
Iteration 175/1000 | Loss: 0.00003164
Iteration 176/1000 | Loss: 0.00003164
Iteration 177/1000 | Loss: 0.00003164
Iteration 178/1000 | Loss: 0.00003164
Iteration 179/1000 | Loss: 0.00003164
Iteration 180/1000 | Loss: 0.00003164
Iteration 181/1000 | Loss: 0.00003164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [3.16441037284676e-05, 3.16441037284676e-05, 3.16441037284676e-05, 3.16441037284676e-05, 3.16441037284676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.16441037284676e-05

Optimization complete. Final v2v error: 4.648658752441406 mm

Highest mean error: 5.775795936584473 mm for frame 120

Lowest mean error: 3.74033522605896 mm for frame 0

Saving results

Total time: 54.820340156555176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446604
Iteration 2/25 | Loss: 0.00135999
Iteration 3/25 | Loss: 0.00124803
Iteration 4/25 | Loss: 0.00122983
Iteration 5/25 | Loss: 0.00122394
Iteration 6/25 | Loss: 0.00122326
Iteration 7/25 | Loss: 0.00122326
Iteration 8/25 | Loss: 0.00122326
Iteration 9/25 | Loss: 0.00122326
Iteration 10/25 | Loss: 0.00122326
Iteration 11/25 | Loss: 0.00122326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012232564622536302, 0.0012232564622536302, 0.0012232564622536302, 0.0012232564622536302, 0.0012232564622536302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012232564622536302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43004704
Iteration 2/25 | Loss: 0.00056544
Iteration 3/25 | Loss: 0.00056544
Iteration 4/25 | Loss: 0.00056544
Iteration 5/25 | Loss: 0.00056544
Iteration 6/25 | Loss: 0.00056543
Iteration 7/25 | Loss: 0.00056543
Iteration 8/25 | Loss: 0.00056543
Iteration 9/25 | Loss: 0.00056543
Iteration 10/25 | Loss: 0.00056543
Iteration 11/25 | Loss: 0.00056543
Iteration 12/25 | Loss: 0.00056543
Iteration 13/25 | Loss: 0.00056543
Iteration 14/25 | Loss: 0.00056543
Iteration 15/25 | Loss: 0.00056543
Iteration 16/25 | Loss: 0.00056543
Iteration 17/25 | Loss: 0.00056543
Iteration 18/25 | Loss: 0.00056543
Iteration 19/25 | Loss: 0.00056543
Iteration 20/25 | Loss: 0.00056543
Iteration 21/25 | Loss: 0.00056543
Iteration 22/25 | Loss: 0.00056543
Iteration 23/25 | Loss: 0.00056543
Iteration 24/25 | Loss: 0.00056543
Iteration 25/25 | Loss: 0.00056543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056543
Iteration 2/1000 | Loss: 0.00003449
Iteration 3/1000 | Loss: 0.00002489
Iteration 4/1000 | Loss: 0.00002221
Iteration 5/1000 | Loss: 0.00002134
Iteration 6/1000 | Loss: 0.00002069
Iteration 7/1000 | Loss: 0.00002023
Iteration 8/1000 | Loss: 0.00002000
Iteration 9/1000 | Loss: 0.00001975
Iteration 10/1000 | Loss: 0.00001945
Iteration 11/1000 | Loss: 0.00001938
Iteration 12/1000 | Loss: 0.00001927
Iteration 13/1000 | Loss: 0.00001926
Iteration 14/1000 | Loss: 0.00001923
Iteration 15/1000 | Loss: 0.00001921
Iteration 16/1000 | Loss: 0.00001921
Iteration 17/1000 | Loss: 0.00001916
Iteration 18/1000 | Loss: 0.00001915
Iteration 19/1000 | Loss: 0.00001909
Iteration 20/1000 | Loss: 0.00001894
Iteration 21/1000 | Loss: 0.00001894
Iteration 22/1000 | Loss: 0.00001890
Iteration 23/1000 | Loss: 0.00001890
Iteration 24/1000 | Loss: 0.00001889
Iteration 25/1000 | Loss: 0.00001889
Iteration 26/1000 | Loss: 0.00001888
Iteration 27/1000 | Loss: 0.00001881
Iteration 28/1000 | Loss: 0.00001874
Iteration 29/1000 | Loss: 0.00001874
Iteration 30/1000 | Loss: 0.00001872
Iteration 31/1000 | Loss: 0.00001872
Iteration 32/1000 | Loss: 0.00001871
Iteration 33/1000 | Loss: 0.00001871
Iteration 34/1000 | Loss: 0.00001870
Iteration 35/1000 | Loss: 0.00001870
Iteration 36/1000 | Loss: 0.00001869
Iteration 37/1000 | Loss: 0.00001869
Iteration 38/1000 | Loss: 0.00001868
Iteration 39/1000 | Loss: 0.00001867
Iteration 40/1000 | Loss: 0.00001867
Iteration 41/1000 | Loss: 0.00001865
Iteration 42/1000 | Loss: 0.00001864
Iteration 43/1000 | Loss: 0.00001864
Iteration 44/1000 | Loss: 0.00001864
Iteration 45/1000 | Loss: 0.00001863
Iteration 46/1000 | Loss: 0.00001863
Iteration 47/1000 | Loss: 0.00001862
Iteration 48/1000 | Loss: 0.00001862
Iteration 49/1000 | Loss: 0.00001862
Iteration 50/1000 | Loss: 0.00001861
Iteration 51/1000 | Loss: 0.00001861
Iteration 52/1000 | Loss: 0.00001861
Iteration 53/1000 | Loss: 0.00001861
Iteration 54/1000 | Loss: 0.00001860
Iteration 55/1000 | Loss: 0.00001859
Iteration 56/1000 | Loss: 0.00001859
Iteration 57/1000 | Loss: 0.00001859
Iteration 58/1000 | Loss: 0.00001859
Iteration 59/1000 | Loss: 0.00001858
Iteration 60/1000 | Loss: 0.00001858
Iteration 61/1000 | Loss: 0.00001858
Iteration 62/1000 | Loss: 0.00001858
Iteration 63/1000 | Loss: 0.00001858
Iteration 64/1000 | Loss: 0.00001858
Iteration 65/1000 | Loss: 0.00001857
Iteration 66/1000 | Loss: 0.00001857
Iteration 67/1000 | Loss: 0.00001857
Iteration 68/1000 | Loss: 0.00001857
Iteration 69/1000 | Loss: 0.00001857
Iteration 70/1000 | Loss: 0.00001857
Iteration 71/1000 | Loss: 0.00001857
Iteration 72/1000 | Loss: 0.00001856
Iteration 73/1000 | Loss: 0.00001856
Iteration 74/1000 | Loss: 0.00001856
Iteration 75/1000 | Loss: 0.00001855
Iteration 76/1000 | Loss: 0.00001855
Iteration 77/1000 | Loss: 0.00001855
Iteration 78/1000 | Loss: 0.00001855
Iteration 79/1000 | Loss: 0.00001855
Iteration 80/1000 | Loss: 0.00001854
Iteration 81/1000 | Loss: 0.00001854
Iteration 82/1000 | Loss: 0.00001854
Iteration 83/1000 | Loss: 0.00001853
Iteration 84/1000 | Loss: 0.00001853
Iteration 85/1000 | Loss: 0.00001853
Iteration 86/1000 | Loss: 0.00001852
Iteration 87/1000 | Loss: 0.00001852
Iteration 88/1000 | Loss: 0.00001852
Iteration 89/1000 | Loss: 0.00001851
Iteration 90/1000 | Loss: 0.00001851
Iteration 91/1000 | Loss: 0.00001851
Iteration 92/1000 | Loss: 0.00001850
Iteration 93/1000 | Loss: 0.00001850
Iteration 94/1000 | Loss: 0.00001850
Iteration 95/1000 | Loss: 0.00001849
Iteration 96/1000 | Loss: 0.00001849
Iteration 97/1000 | Loss: 0.00001849
Iteration 98/1000 | Loss: 0.00001848
Iteration 99/1000 | Loss: 0.00001848
Iteration 100/1000 | Loss: 0.00001848
Iteration 101/1000 | Loss: 0.00001847
Iteration 102/1000 | Loss: 0.00001847
Iteration 103/1000 | Loss: 0.00001847
Iteration 104/1000 | Loss: 0.00001846
Iteration 105/1000 | Loss: 0.00001846
Iteration 106/1000 | Loss: 0.00001846
Iteration 107/1000 | Loss: 0.00001846
Iteration 108/1000 | Loss: 0.00001846
Iteration 109/1000 | Loss: 0.00001846
Iteration 110/1000 | Loss: 0.00001845
Iteration 111/1000 | Loss: 0.00001845
Iteration 112/1000 | Loss: 0.00001845
Iteration 113/1000 | Loss: 0.00001845
Iteration 114/1000 | Loss: 0.00001845
Iteration 115/1000 | Loss: 0.00001844
Iteration 116/1000 | Loss: 0.00001844
Iteration 117/1000 | Loss: 0.00001844
Iteration 118/1000 | Loss: 0.00001844
Iteration 119/1000 | Loss: 0.00001844
Iteration 120/1000 | Loss: 0.00001844
Iteration 121/1000 | Loss: 0.00001844
Iteration 122/1000 | Loss: 0.00001844
Iteration 123/1000 | Loss: 0.00001844
Iteration 124/1000 | Loss: 0.00001844
Iteration 125/1000 | Loss: 0.00001844
Iteration 126/1000 | Loss: 0.00001844
Iteration 127/1000 | Loss: 0.00001843
Iteration 128/1000 | Loss: 0.00001843
Iteration 129/1000 | Loss: 0.00001843
Iteration 130/1000 | Loss: 0.00001843
Iteration 131/1000 | Loss: 0.00001843
Iteration 132/1000 | Loss: 0.00001843
Iteration 133/1000 | Loss: 0.00001843
Iteration 134/1000 | Loss: 0.00001843
Iteration 135/1000 | Loss: 0.00001842
Iteration 136/1000 | Loss: 0.00001842
Iteration 137/1000 | Loss: 0.00001842
Iteration 138/1000 | Loss: 0.00001842
Iteration 139/1000 | Loss: 0.00001842
Iteration 140/1000 | Loss: 0.00001842
Iteration 141/1000 | Loss: 0.00001842
Iteration 142/1000 | Loss: 0.00001842
Iteration 143/1000 | Loss: 0.00001842
Iteration 144/1000 | Loss: 0.00001842
Iteration 145/1000 | Loss: 0.00001842
Iteration 146/1000 | Loss: 0.00001841
Iteration 147/1000 | Loss: 0.00001841
Iteration 148/1000 | Loss: 0.00001841
Iteration 149/1000 | Loss: 0.00001841
Iteration 150/1000 | Loss: 0.00001841
Iteration 151/1000 | Loss: 0.00001841
Iteration 152/1000 | Loss: 0.00001841
Iteration 153/1000 | Loss: 0.00001841
Iteration 154/1000 | Loss: 0.00001841
Iteration 155/1000 | Loss: 0.00001841
Iteration 156/1000 | Loss: 0.00001841
Iteration 157/1000 | Loss: 0.00001841
Iteration 158/1000 | Loss: 0.00001841
Iteration 159/1000 | Loss: 0.00001841
Iteration 160/1000 | Loss: 0.00001841
Iteration 161/1000 | Loss: 0.00001841
Iteration 162/1000 | Loss: 0.00001840
Iteration 163/1000 | Loss: 0.00001840
Iteration 164/1000 | Loss: 0.00001840
Iteration 165/1000 | Loss: 0.00001840
Iteration 166/1000 | Loss: 0.00001840
Iteration 167/1000 | Loss: 0.00001840
Iteration 168/1000 | Loss: 0.00001840
Iteration 169/1000 | Loss: 0.00001839
Iteration 170/1000 | Loss: 0.00001839
Iteration 171/1000 | Loss: 0.00001839
Iteration 172/1000 | Loss: 0.00001839
Iteration 173/1000 | Loss: 0.00001839
Iteration 174/1000 | Loss: 0.00001839
Iteration 175/1000 | Loss: 0.00001839
Iteration 176/1000 | Loss: 0.00001839
Iteration 177/1000 | Loss: 0.00001839
Iteration 178/1000 | Loss: 0.00001839
Iteration 179/1000 | Loss: 0.00001839
Iteration 180/1000 | Loss: 0.00001838
Iteration 181/1000 | Loss: 0.00001838
Iteration 182/1000 | Loss: 0.00001838
Iteration 183/1000 | Loss: 0.00001838
Iteration 184/1000 | Loss: 0.00001838
Iteration 185/1000 | Loss: 0.00001838
Iteration 186/1000 | Loss: 0.00001838
Iteration 187/1000 | Loss: 0.00001838
Iteration 188/1000 | Loss: 0.00001838
Iteration 189/1000 | Loss: 0.00001838
Iteration 190/1000 | Loss: 0.00001838
Iteration 191/1000 | Loss: 0.00001838
Iteration 192/1000 | Loss: 0.00001838
Iteration 193/1000 | Loss: 0.00001838
Iteration 194/1000 | Loss: 0.00001838
Iteration 195/1000 | Loss: 0.00001838
Iteration 196/1000 | Loss: 0.00001838
Iteration 197/1000 | Loss: 0.00001838
Iteration 198/1000 | Loss: 0.00001838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.83813608600758e-05, 1.83813608600758e-05, 1.83813608600758e-05, 1.83813608600758e-05, 1.83813608600758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.83813608600758e-05

Optimization complete. Final v2v error: 3.6050992012023926 mm

Highest mean error: 3.8641321659088135 mm for frame 179

Lowest mean error: 3.2341179847717285 mm for frame 190

Saving results

Total time: 46.20241141319275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446605
Iteration 2/25 | Loss: 0.00133677
Iteration 3/25 | Loss: 0.00124060
Iteration 4/25 | Loss: 0.00122736
Iteration 5/25 | Loss: 0.00122336
Iteration 6/25 | Loss: 0.00122327
Iteration 7/25 | Loss: 0.00122327
Iteration 8/25 | Loss: 0.00122327
Iteration 9/25 | Loss: 0.00122327
Iteration 10/25 | Loss: 0.00122327
Iteration 11/25 | Loss: 0.00122327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012232710141688585, 0.0012232710141688585, 0.0012232710141688585, 0.0012232710141688585, 0.0012232710141688585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012232710141688585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42967701
Iteration 2/25 | Loss: 0.00056327
Iteration 3/25 | Loss: 0.00056327
Iteration 4/25 | Loss: 0.00056327
Iteration 5/25 | Loss: 0.00056327
Iteration 6/25 | Loss: 0.00056327
Iteration 7/25 | Loss: 0.00056326
Iteration 8/25 | Loss: 0.00056326
Iteration 9/25 | Loss: 0.00056326
Iteration 10/25 | Loss: 0.00056326
Iteration 11/25 | Loss: 0.00056326
Iteration 12/25 | Loss: 0.00056326
Iteration 13/25 | Loss: 0.00056326
Iteration 14/25 | Loss: 0.00056326
Iteration 15/25 | Loss: 0.00056326
Iteration 16/25 | Loss: 0.00056326
Iteration 17/25 | Loss: 0.00056326
Iteration 18/25 | Loss: 0.00056326
Iteration 19/25 | Loss: 0.00056326
Iteration 20/25 | Loss: 0.00056326
Iteration 21/25 | Loss: 0.00056326
Iteration 22/25 | Loss: 0.00056326
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005632636602967978, 0.0005632636602967978, 0.0005632636602967978, 0.0005632636602967978, 0.0005632636602967978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005632636602967978

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056326
Iteration 2/1000 | Loss: 0.00003477
Iteration 3/1000 | Loss: 0.00002475
Iteration 4/1000 | Loss: 0.00002236
Iteration 5/1000 | Loss: 0.00002131
Iteration 6/1000 | Loss: 0.00002073
Iteration 7/1000 | Loss: 0.00002025
Iteration 8/1000 | Loss: 0.00001997
Iteration 9/1000 | Loss: 0.00001969
Iteration 10/1000 | Loss: 0.00001938
Iteration 11/1000 | Loss: 0.00001936
Iteration 12/1000 | Loss: 0.00001922
Iteration 13/1000 | Loss: 0.00001903
Iteration 14/1000 | Loss: 0.00001900
Iteration 15/1000 | Loss: 0.00001898
Iteration 16/1000 | Loss: 0.00001891
Iteration 17/1000 | Loss: 0.00001884
Iteration 18/1000 | Loss: 0.00001881
Iteration 19/1000 | Loss: 0.00001881
Iteration 20/1000 | Loss: 0.00001880
Iteration 21/1000 | Loss: 0.00001880
Iteration 22/1000 | Loss: 0.00001878
Iteration 23/1000 | Loss: 0.00001876
Iteration 24/1000 | Loss: 0.00001875
Iteration 25/1000 | Loss: 0.00001875
Iteration 26/1000 | Loss: 0.00001870
Iteration 27/1000 | Loss: 0.00001870
Iteration 28/1000 | Loss: 0.00001868
Iteration 29/1000 | Loss: 0.00001868
Iteration 30/1000 | Loss: 0.00001867
Iteration 31/1000 | Loss: 0.00001865
Iteration 32/1000 | Loss: 0.00001865
Iteration 33/1000 | Loss: 0.00001863
Iteration 34/1000 | Loss: 0.00001863
Iteration 35/1000 | Loss: 0.00001863
Iteration 36/1000 | Loss: 0.00001863
Iteration 37/1000 | Loss: 0.00001863
Iteration 38/1000 | Loss: 0.00001863
Iteration 39/1000 | Loss: 0.00001863
Iteration 40/1000 | Loss: 0.00001863
Iteration 41/1000 | Loss: 0.00001863
Iteration 42/1000 | Loss: 0.00001862
Iteration 43/1000 | Loss: 0.00001862
Iteration 44/1000 | Loss: 0.00001861
Iteration 45/1000 | Loss: 0.00001861
Iteration 46/1000 | Loss: 0.00001861
Iteration 47/1000 | Loss: 0.00001861
Iteration 48/1000 | Loss: 0.00001860
Iteration 49/1000 | Loss: 0.00001860
Iteration 50/1000 | Loss: 0.00001859
Iteration 51/1000 | Loss: 0.00001859
Iteration 52/1000 | Loss: 0.00001859
Iteration 53/1000 | Loss: 0.00001859
Iteration 54/1000 | Loss: 0.00001859
Iteration 55/1000 | Loss: 0.00001859
Iteration 56/1000 | Loss: 0.00001858
Iteration 57/1000 | Loss: 0.00001858
Iteration 58/1000 | Loss: 0.00001858
Iteration 59/1000 | Loss: 0.00001858
Iteration 60/1000 | Loss: 0.00001858
Iteration 61/1000 | Loss: 0.00001858
Iteration 62/1000 | Loss: 0.00001858
Iteration 63/1000 | Loss: 0.00001858
Iteration 64/1000 | Loss: 0.00001858
Iteration 65/1000 | Loss: 0.00001857
Iteration 66/1000 | Loss: 0.00001857
Iteration 67/1000 | Loss: 0.00001857
Iteration 68/1000 | Loss: 0.00001857
Iteration 69/1000 | Loss: 0.00001857
Iteration 70/1000 | Loss: 0.00001856
Iteration 71/1000 | Loss: 0.00001856
Iteration 72/1000 | Loss: 0.00001856
Iteration 73/1000 | Loss: 0.00001856
Iteration 74/1000 | Loss: 0.00001855
Iteration 75/1000 | Loss: 0.00001855
Iteration 76/1000 | Loss: 0.00001855
Iteration 77/1000 | Loss: 0.00001855
Iteration 78/1000 | Loss: 0.00001855
Iteration 79/1000 | Loss: 0.00001855
Iteration 80/1000 | Loss: 0.00001855
Iteration 81/1000 | Loss: 0.00001854
Iteration 82/1000 | Loss: 0.00001854
Iteration 83/1000 | Loss: 0.00001853
Iteration 84/1000 | Loss: 0.00001853
Iteration 85/1000 | Loss: 0.00001853
Iteration 86/1000 | Loss: 0.00001853
Iteration 87/1000 | Loss: 0.00001853
Iteration 88/1000 | Loss: 0.00001852
Iteration 89/1000 | Loss: 0.00001852
Iteration 90/1000 | Loss: 0.00001852
Iteration 91/1000 | Loss: 0.00001852
Iteration 92/1000 | Loss: 0.00001852
Iteration 93/1000 | Loss: 0.00001852
Iteration 94/1000 | Loss: 0.00001851
Iteration 95/1000 | Loss: 0.00001851
Iteration 96/1000 | Loss: 0.00001851
Iteration 97/1000 | Loss: 0.00001851
Iteration 98/1000 | Loss: 0.00001851
Iteration 99/1000 | Loss: 0.00001851
Iteration 100/1000 | Loss: 0.00001851
Iteration 101/1000 | Loss: 0.00001851
Iteration 102/1000 | Loss: 0.00001850
Iteration 103/1000 | Loss: 0.00001850
Iteration 104/1000 | Loss: 0.00001850
Iteration 105/1000 | Loss: 0.00001850
Iteration 106/1000 | Loss: 0.00001850
Iteration 107/1000 | Loss: 0.00001850
Iteration 108/1000 | Loss: 0.00001849
Iteration 109/1000 | Loss: 0.00001848
Iteration 110/1000 | Loss: 0.00001848
Iteration 111/1000 | Loss: 0.00001848
Iteration 112/1000 | Loss: 0.00001848
Iteration 113/1000 | Loss: 0.00001847
Iteration 114/1000 | Loss: 0.00001847
Iteration 115/1000 | Loss: 0.00001847
Iteration 116/1000 | Loss: 0.00001847
Iteration 117/1000 | Loss: 0.00001847
Iteration 118/1000 | Loss: 0.00001847
Iteration 119/1000 | Loss: 0.00001847
Iteration 120/1000 | Loss: 0.00001846
Iteration 121/1000 | Loss: 0.00001846
Iteration 122/1000 | Loss: 0.00001846
Iteration 123/1000 | Loss: 0.00001846
Iteration 124/1000 | Loss: 0.00001846
Iteration 125/1000 | Loss: 0.00001846
Iteration 126/1000 | Loss: 0.00001846
Iteration 127/1000 | Loss: 0.00001846
Iteration 128/1000 | Loss: 0.00001845
Iteration 129/1000 | Loss: 0.00001845
Iteration 130/1000 | Loss: 0.00001845
Iteration 131/1000 | Loss: 0.00001845
Iteration 132/1000 | Loss: 0.00001845
Iteration 133/1000 | Loss: 0.00001845
Iteration 134/1000 | Loss: 0.00001844
Iteration 135/1000 | Loss: 0.00001844
Iteration 136/1000 | Loss: 0.00001844
Iteration 137/1000 | Loss: 0.00001844
Iteration 138/1000 | Loss: 0.00001844
Iteration 139/1000 | Loss: 0.00001844
Iteration 140/1000 | Loss: 0.00001844
Iteration 141/1000 | Loss: 0.00001844
Iteration 142/1000 | Loss: 0.00001844
Iteration 143/1000 | Loss: 0.00001844
Iteration 144/1000 | Loss: 0.00001843
Iteration 145/1000 | Loss: 0.00001843
Iteration 146/1000 | Loss: 0.00001843
Iteration 147/1000 | Loss: 0.00001843
Iteration 148/1000 | Loss: 0.00001842
Iteration 149/1000 | Loss: 0.00001842
Iteration 150/1000 | Loss: 0.00001842
Iteration 151/1000 | Loss: 0.00001842
Iteration 152/1000 | Loss: 0.00001842
Iteration 153/1000 | Loss: 0.00001842
Iteration 154/1000 | Loss: 0.00001841
Iteration 155/1000 | Loss: 0.00001841
Iteration 156/1000 | Loss: 0.00001841
Iteration 157/1000 | Loss: 0.00001841
Iteration 158/1000 | Loss: 0.00001841
Iteration 159/1000 | Loss: 0.00001841
Iteration 160/1000 | Loss: 0.00001841
Iteration 161/1000 | Loss: 0.00001841
Iteration 162/1000 | Loss: 0.00001840
Iteration 163/1000 | Loss: 0.00001840
Iteration 164/1000 | Loss: 0.00001840
Iteration 165/1000 | Loss: 0.00001840
Iteration 166/1000 | Loss: 0.00001839
Iteration 167/1000 | Loss: 0.00001839
Iteration 168/1000 | Loss: 0.00001839
Iteration 169/1000 | Loss: 0.00001839
Iteration 170/1000 | Loss: 0.00001839
Iteration 171/1000 | Loss: 0.00001839
Iteration 172/1000 | Loss: 0.00001839
Iteration 173/1000 | Loss: 0.00001839
Iteration 174/1000 | Loss: 0.00001839
Iteration 175/1000 | Loss: 0.00001839
Iteration 176/1000 | Loss: 0.00001838
Iteration 177/1000 | Loss: 0.00001838
Iteration 178/1000 | Loss: 0.00001838
Iteration 179/1000 | Loss: 0.00001838
Iteration 180/1000 | Loss: 0.00001838
Iteration 181/1000 | Loss: 0.00001838
Iteration 182/1000 | Loss: 0.00001838
Iteration 183/1000 | Loss: 0.00001838
Iteration 184/1000 | Loss: 0.00001838
Iteration 185/1000 | Loss: 0.00001838
Iteration 186/1000 | Loss: 0.00001838
Iteration 187/1000 | Loss: 0.00001838
Iteration 188/1000 | Loss: 0.00001838
Iteration 189/1000 | Loss: 0.00001838
Iteration 190/1000 | Loss: 0.00001837
Iteration 191/1000 | Loss: 0.00001837
Iteration 192/1000 | Loss: 0.00001837
Iteration 193/1000 | Loss: 0.00001837
Iteration 194/1000 | Loss: 0.00001837
Iteration 195/1000 | Loss: 0.00001837
Iteration 196/1000 | Loss: 0.00001837
Iteration 197/1000 | Loss: 0.00001837
Iteration 198/1000 | Loss: 0.00001837
Iteration 199/1000 | Loss: 0.00001837
Iteration 200/1000 | Loss: 0.00001837
Iteration 201/1000 | Loss: 0.00001837
Iteration 202/1000 | Loss: 0.00001837
Iteration 203/1000 | Loss: 0.00001837
Iteration 204/1000 | Loss: 0.00001837
Iteration 205/1000 | Loss: 0.00001837
Iteration 206/1000 | Loss: 0.00001837
Iteration 207/1000 | Loss: 0.00001837
Iteration 208/1000 | Loss: 0.00001837
Iteration 209/1000 | Loss: 0.00001837
Iteration 210/1000 | Loss: 0.00001837
Iteration 211/1000 | Loss: 0.00001837
Iteration 212/1000 | Loss: 0.00001837
Iteration 213/1000 | Loss: 0.00001837
Iteration 214/1000 | Loss: 0.00001837
Iteration 215/1000 | Loss: 0.00001837
Iteration 216/1000 | Loss: 0.00001837
Iteration 217/1000 | Loss: 0.00001837
Iteration 218/1000 | Loss: 0.00001837
Iteration 219/1000 | Loss: 0.00001837
Iteration 220/1000 | Loss: 0.00001837
Iteration 221/1000 | Loss: 0.00001837
Iteration 222/1000 | Loss: 0.00001837
Iteration 223/1000 | Loss: 0.00001837
Iteration 224/1000 | Loss: 0.00001837
Iteration 225/1000 | Loss: 0.00001837
Iteration 226/1000 | Loss: 0.00001837
Iteration 227/1000 | Loss: 0.00001837
Iteration 228/1000 | Loss: 0.00001837
Iteration 229/1000 | Loss: 0.00001837
Iteration 230/1000 | Loss: 0.00001837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.8373484635958448e-05, 1.8373484635958448e-05, 1.8373484635958448e-05, 1.8373484635958448e-05, 1.8373484635958448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8373484635958448e-05

Optimization complete. Final v2v error: 3.605837821960449 mm

Highest mean error: 3.862299680709839 mm for frame 179

Lowest mean error: 3.2379825115203857 mm for frame 190

Saving results

Total time: 47.45251774787903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791096
Iteration 2/25 | Loss: 0.00151764
Iteration 3/25 | Loss: 0.00142219
Iteration 4/25 | Loss: 0.00141305
Iteration 5/25 | Loss: 0.00140875
Iteration 6/25 | Loss: 0.00140803
Iteration 7/25 | Loss: 0.00140803
Iteration 8/25 | Loss: 0.00140803
Iteration 9/25 | Loss: 0.00140803
Iteration 10/25 | Loss: 0.00140803
Iteration 11/25 | Loss: 0.00140803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014080324908718467, 0.0014080324908718467, 0.0014080324908718467, 0.0014080324908718467, 0.0014080324908718467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014080324908718467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20107138
Iteration 2/25 | Loss: 0.00088295
Iteration 3/25 | Loss: 0.00088295
Iteration 4/25 | Loss: 0.00088295
Iteration 5/25 | Loss: 0.00088295
Iteration 6/25 | Loss: 0.00088295
Iteration 7/25 | Loss: 0.00088295
Iteration 8/25 | Loss: 0.00088295
Iteration 9/25 | Loss: 0.00088295
Iteration 10/25 | Loss: 0.00088295
Iteration 11/25 | Loss: 0.00088295
Iteration 12/25 | Loss: 0.00088295
Iteration 13/25 | Loss: 0.00088295
Iteration 14/25 | Loss: 0.00088295
Iteration 15/25 | Loss: 0.00088295
Iteration 16/25 | Loss: 0.00088295
Iteration 17/25 | Loss: 0.00088295
Iteration 18/25 | Loss: 0.00088295
Iteration 19/25 | Loss: 0.00088295
Iteration 20/25 | Loss: 0.00088295
Iteration 21/25 | Loss: 0.00088295
Iteration 22/25 | Loss: 0.00088295
Iteration 23/25 | Loss: 0.00088295
Iteration 24/25 | Loss: 0.00088295
Iteration 25/25 | Loss: 0.00088295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088295
Iteration 2/1000 | Loss: 0.00005010
Iteration 3/1000 | Loss: 0.00003736
Iteration 4/1000 | Loss: 0.00003420
Iteration 5/1000 | Loss: 0.00003287
Iteration 6/1000 | Loss: 0.00003213
Iteration 7/1000 | Loss: 0.00003149
Iteration 8/1000 | Loss: 0.00003096
Iteration 9/1000 | Loss: 0.00003053
Iteration 10/1000 | Loss: 0.00003026
Iteration 11/1000 | Loss: 0.00003005
Iteration 12/1000 | Loss: 0.00002985
Iteration 13/1000 | Loss: 0.00002969
Iteration 14/1000 | Loss: 0.00002953
Iteration 15/1000 | Loss: 0.00002952
Iteration 16/1000 | Loss: 0.00002943
Iteration 17/1000 | Loss: 0.00002939
Iteration 18/1000 | Loss: 0.00002938
Iteration 19/1000 | Loss: 0.00002936
Iteration 20/1000 | Loss: 0.00002935
Iteration 21/1000 | Loss: 0.00002934
Iteration 22/1000 | Loss: 0.00002934
Iteration 23/1000 | Loss: 0.00002933
Iteration 24/1000 | Loss: 0.00002933
Iteration 25/1000 | Loss: 0.00002933
Iteration 26/1000 | Loss: 0.00002932
Iteration 27/1000 | Loss: 0.00002927
Iteration 28/1000 | Loss: 0.00002923
Iteration 29/1000 | Loss: 0.00002918
Iteration 30/1000 | Loss: 0.00002918
Iteration 31/1000 | Loss: 0.00002918
Iteration 32/1000 | Loss: 0.00002915
Iteration 33/1000 | Loss: 0.00002915
Iteration 34/1000 | Loss: 0.00002912
Iteration 35/1000 | Loss: 0.00002911
Iteration 36/1000 | Loss: 0.00002910
Iteration 37/1000 | Loss: 0.00002909
Iteration 38/1000 | Loss: 0.00002909
Iteration 39/1000 | Loss: 0.00002908
Iteration 40/1000 | Loss: 0.00002908
Iteration 41/1000 | Loss: 0.00002908
Iteration 42/1000 | Loss: 0.00002908
Iteration 43/1000 | Loss: 0.00002908
Iteration 44/1000 | Loss: 0.00002908
Iteration 45/1000 | Loss: 0.00002907
Iteration 46/1000 | Loss: 0.00002907
Iteration 47/1000 | Loss: 0.00002907
Iteration 48/1000 | Loss: 0.00002906
Iteration 49/1000 | Loss: 0.00002906
Iteration 50/1000 | Loss: 0.00002905
Iteration 51/1000 | Loss: 0.00002905
Iteration 52/1000 | Loss: 0.00002905
Iteration 53/1000 | Loss: 0.00002904
Iteration 54/1000 | Loss: 0.00002904
Iteration 55/1000 | Loss: 0.00002903
Iteration 56/1000 | Loss: 0.00002902
Iteration 57/1000 | Loss: 0.00002902
Iteration 58/1000 | Loss: 0.00002902
Iteration 59/1000 | Loss: 0.00002902
Iteration 60/1000 | Loss: 0.00002902
Iteration 61/1000 | Loss: 0.00002902
Iteration 62/1000 | Loss: 0.00002902
Iteration 63/1000 | Loss: 0.00002901
Iteration 64/1000 | Loss: 0.00002901
Iteration 65/1000 | Loss: 0.00002901
Iteration 66/1000 | Loss: 0.00002901
Iteration 67/1000 | Loss: 0.00002901
Iteration 68/1000 | Loss: 0.00002901
Iteration 69/1000 | Loss: 0.00002901
Iteration 70/1000 | Loss: 0.00002901
Iteration 71/1000 | Loss: 0.00002901
Iteration 72/1000 | Loss: 0.00002901
Iteration 73/1000 | Loss: 0.00002901
Iteration 74/1000 | Loss: 0.00002900
Iteration 75/1000 | Loss: 0.00002900
Iteration 76/1000 | Loss: 0.00002900
Iteration 77/1000 | Loss: 0.00002900
Iteration 78/1000 | Loss: 0.00002900
Iteration 79/1000 | Loss: 0.00002900
Iteration 80/1000 | Loss: 0.00002900
Iteration 81/1000 | Loss: 0.00002900
Iteration 82/1000 | Loss: 0.00002900
Iteration 83/1000 | Loss: 0.00002899
Iteration 84/1000 | Loss: 0.00002899
Iteration 85/1000 | Loss: 0.00002899
Iteration 86/1000 | Loss: 0.00002898
Iteration 87/1000 | Loss: 0.00002898
Iteration 88/1000 | Loss: 0.00002898
Iteration 89/1000 | Loss: 0.00002898
Iteration 90/1000 | Loss: 0.00002898
Iteration 91/1000 | Loss: 0.00002897
Iteration 92/1000 | Loss: 0.00002897
Iteration 93/1000 | Loss: 0.00002897
Iteration 94/1000 | Loss: 0.00002897
Iteration 95/1000 | Loss: 0.00002897
Iteration 96/1000 | Loss: 0.00002897
Iteration 97/1000 | Loss: 0.00002897
Iteration 98/1000 | Loss: 0.00002897
Iteration 99/1000 | Loss: 0.00002896
Iteration 100/1000 | Loss: 0.00002896
Iteration 101/1000 | Loss: 0.00002896
Iteration 102/1000 | Loss: 0.00002896
Iteration 103/1000 | Loss: 0.00002896
Iteration 104/1000 | Loss: 0.00002896
Iteration 105/1000 | Loss: 0.00002896
Iteration 106/1000 | Loss: 0.00002896
Iteration 107/1000 | Loss: 0.00002896
Iteration 108/1000 | Loss: 0.00002895
Iteration 109/1000 | Loss: 0.00002895
Iteration 110/1000 | Loss: 0.00002895
Iteration 111/1000 | Loss: 0.00002895
Iteration 112/1000 | Loss: 0.00002895
Iteration 113/1000 | Loss: 0.00002895
Iteration 114/1000 | Loss: 0.00002895
Iteration 115/1000 | Loss: 0.00002895
Iteration 116/1000 | Loss: 0.00002895
Iteration 117/1000 | Loss: 0.00002894
Iteration 118/1000 | Loss: 0.00002894
Iteration 119/1000 | Loss: 0.00002894
Iteration 120/1000 | Loss: 0.00002894
Iteration 121/1000 | Loss: 0.00002894
Iteration 122/1000 | Loss: 0.00002894
Iteration 123/1000 | Loss: 0.00002894
Iteration 124/1000 | Loss: 0.00002894
Iteration 125/1000 | Loss: 0.00002894
Iteration 126/1000 | Loss: 0.00002894
Iteration 127/1000 | Loss: 0.00002893
Iteration 128/1000 | Loss: 0.00002893
Iteration 129/1000 | Loss: 0.00002893
Iteration 130/1000 | Loss: 0.00002893
Iteration 131/1000 | Loss: 0.00002893
Iteration 132/1000 | Loss: 0.00002892
Iteration 133/1000 | Loss: 0.00002892
Iteration 134/1000 | Loss: 0.00002892
Iteration 135/1000 | Loss: 0.00002892
Iteration 136/1000 | Loss: 0.00002892
Iteration 137/1000 | Loss: 0.00002892
Iteration 138/1000 | Loss: 0.00002892
Iteration 139/1000 | Loss: 0.00002891
Iteration 140/1000 | Loss: 0.00002891
Iteration 141/1000 | Loss: 0.00002891
Iteration 142/1000 | Loss: 0.00002891
Iteration 143/1000 | Loss: 0.00002891
Iteration 144/1000 | Loss: 0.00002891
Iteration 145/1000 | Loss: 0.00002891
Iteration 146/1000 | Loss: 0.00002891
Iteration 147/1000 | Loss: 0.00002891
Iteration 148/1000 | Loss: 0.00002891
Iteration 149/1000 | Loss: 0.00002891
Iteration 150/1000 | Loss: 0.00002891
Iteration 151/1000 | Loss: 0.00002891
Iteration 152/1000 | Loss: 0.00002891
Iteration 153/1000 | Loss: 0.00002891
Iteration 154/1000 | Loss: 0.00002891
Iteration 155/1000 | Loss: 0.00002890
Iteration 156/1000 | Loss: 0.00002890
Iteration 157/1000 | Loss: 0.00002890
Iteration 158/1000 | Loss: 0.00002890
Iteration 159/1000 | Loss: 0.00002890
Iteration 160/1000 | Loss: 0.00002890
Iteration 161/1000 | Loss: 0.00002890
Iteration 162/1000 | Loss: 0.00002890
Iteration 163/1000 | Loss: 0.00002890
Iteration 164/1000 | Loss: 0.00002890
Iteration 165/1000 | Loss: 0.00002889
Iteration 166/1000 | Loss: 0.00002889
Iteration 167/1000 | Loss: 0.00002889
Iteration 168/1000 | Loss: 0.00002889
Iteration 169/1000 | Loss: 0.00002889
Iteration 170/1000 | Loss: 0.00002889
Iteration 171/1000 | Loss: 0.00002889
Iteration 172/1000 | Loss: 0.00002889
Iteration 173/1000 | Loss: 0.00002889
Iteration 174/1000 | Loss: 0.00002889
Iteration 175/1000 | Loss: 0.00002889
Iteration 176/1000 | Loss: 0.00002889
Iteration 177/1000 | Loss: 0.00002889
Iteration 178/1000 | Loss: 0.00002889
Iteration 179/1000 | Loss: 0.00002889
Iteration 180/1000 | Loss: 0.00002888
Iteration 181/1000 | Loss: 0.00002888
Iteration 182/1000 | Loss: 0.00002888
Iteration 183/1000 | Loss: 0.00002888
Iteration 184/1000 | Loss: 0.00002888
Iteration 185/1000 | Loss: 0.00002888
Iteration 186/1000 | Loss: 0.00002888
Iteration 187/1000 | Loss: 0.00002888
Iteration 188/1000 | Loss: 0.00002888
Iteration 189/1000 | Loss: 0.00002888
Iteration 190/1000 | Loss: 0.00002888
Iteration 191/1000 | Loss: 0.00002887
Iteration 192/1000 | Loss: 0.00002887
Iteration 193/1000 | Loss: 0.00002887
Iteration 194/1000 | Loss: 0.00002887
Iteration 195/1000 | Loss: 0.00002887
Iteration 196/1000 | Loss: 0.00002887
Iteration 197/1000 | Loss: 0.00002887
Iteration 198/1000 | Loss: 0.00002887
Iteration 199/1000 | Loss: 0.00002887
Iteration 200/1000 | Loss: 0.00002887
Iteration 201/1000 | Loss: 0.00002887
Iteration 202/1000 | Loss: 0.00002887
Iteration 203/1000 | Loss: 0.00002887
Iteration 204/1000 | Loss: 0.00002887
Iteration 205/1000 | Loss: 0.00002887
Iteration 206/1000 | Loss: 0.00002886
Iteration 207/1000 | Loss: 0.00002886
Iteration 208/1000 | Loss: 0.00002886
Iteration 209/1000 | Loss: 0.00002886
Iteration 210/1000 | Loss: 0.00002886
Iteration 211/1000 | Loss: 0.00002886
Iteration 212/1000 | Loss: 0.00002886
Iteration 213/1000 | Loss: 0.00002886
Iteration 214/1000 | Loss: 0.00002886
Iteration 215/1000 | Loss: 0.00002886
Iteration 216/1000 | Loss: 0.00002886
Iteration 217/1000 | Loss: 0.00002886
Iteration 218/1000 | Loss: 0.00002886
Iteration 219/1000 | Loss: 0.00002886
Iteration 220/1000 | Loss: 0.00002886
Iteration 221/1000 | Loss: 0.00002886
Iteration 222/1000 | Loss: 0.00002886
Iteration 223/1000 | Loss: 0.00002886
Iteration 224/1000 | Loss: 0.00002886
Iteration 225/1000 | Loss: 0.00002886
Iteration 226/1000 | Loss: 0.00002886
Iteration 227/1000 | Loss: 0.00002886
Iteration 228/1000 | Loss: 0.00002886
Iteration 229/1000 | Loss: 0.00002886
Iteration 230/1000 | Loss: 0.00002886
Iteration 231/1000 | Loss: 0.00002886
Iteration 232/1000 | Loss: 0.00002886
Iteration 233/1000 | Loss: 0.00002886
Iteration 234/1000 | Loss: 0.00002886
Iteration 235/1000 | Loss: 0.00002886
Iteration 236/1000 | Loss: 0.00002886
Iteration 237/1000 | Loss: 0.00002886
Iteration 238/1000 | Loss: 0.00002886
Iteration 239/1000 | Loss: 0.00002886
Iteration 240/1000 | Loss: 0.00002886
Iteration 241/1000 | Loss: 0.00002886
Iteration 242/1000 | Loss: 0.00002886
Iteration 243/1000 | Loss: 0.00002886
Iteration 244/1000 | Loss: 0.00002886
Iteration 245/1000 | Loss: 0.00002886
Iteration 246/1000 | Loss: 0.00002886
Iteration 247/1000 | Loss: 0.00002886
Iteration 248/1000 | Loss: 0.00002886
Iteration 249/1000 | Loss: 0.00002886
Iteration 250/1000 | Loss: 0.00002886
Iteration 251/1000 | Loss: 0.00002886
Iteration 252/1000 | Loss: 0.00002886
Iteration 253/1000 | Loss: 0.00002886
Iteration 254/1000 | Loss: 0.00002886
Iteration 255/1000 | Loss: 0.00002886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [2.885910726035945e-05, 2.885910726035945e-05, 2.885910726035945e-05, 2.885910726035945e-05, 2.885910726035945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.885910726035945e-05

Optimization complete. Final v2v error: 4.403092384338379 mm

Highest mean error: 4.586921691894531 mm for frame 98

Lowest mean error: 4.201539039611816 mm for frame 5

Saving results

Total time: 44.368507385253906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996968
Iteration 2/25 | Loss: 0.00300927
Iteration 3/25 | Loss: 0.00221398
Iteration 4/25 | Loss: 0.00221214
Iteration 5/25 | Loss: 0.00194659
Iteration 6/25 | Loss: 0.00151148
Iteration 7/25 | Loss: 0.00143767
Iteration 8/25 | Loss: 0.00135547
Iteration 9/25 | Loss: 0.00134831
Iteration 10/25 | Loss: 0.00134735
Iteration 11/25 | Loss: 0.00134123
Iteration 12/25 | Loss: 0.00134017
Iteration 13/25 | Loss: 0.00133970
Iteration 14/25 | Loss: 0.00133914
Iteration 15/25 | Loss: 0.00133893
Iteration 16/25 | Loss: 0.00133883
Iteration 17/25 | Loss: 0.00133880
Iteration 18/25 | Loss: 0.00133880
Iteration 19/25 | Loss: 0.00133880
Iteration 20/25 | Loss: 0.00133880
Iteration 21/25 | Loss: 0.00133880
Iteration 22/25 | Loss: 0.00133880
Iteration 23/25 | Loss: 0.00133879
Iteration 24/25 | Loss: 0.00133879
Iteration 25/25 | Loss: 0.00133879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45996666
Iteration 2/25 | Loss: 0.00076145
Iteration 3/25 | Loss: 0.00076145
Iteration 4/25 | Loss: 0.00076145
Iteration 5/25 | Loss: 0.00076145
Iteration 6/25 | Loss: 0.00076145
Iteration 7/25 | Loss: 0.00076145
Iteration 8/25 | Loss: 0.00076145
Iteration 9/25 | Loss: 0.00076145
Iteration 10/25 | Loss: 0.00076145
Iteration 11/25 | Loss: 0.00076145
Iteration 12/25 | Loss: 0.00076145
Iteration 13/25 | Loss: 0.00076145
Iteration 14/25 | Loss: 0.00076145
Iteration 15/25 | Loss: 0.00076145
Iteration 16/25 | Loss: 0.00076145
Iteration 17/25 | Loss: 0.00076145
Iteration 18/25 | Loss: 0.00076145
Iteration 19/25 | Loss: 0.00076145
Iteration 20/25 | Loss: 0.00076145
Iteration 21/25 | Loss: 0.00076145
Iteration 22/25 | Loss: 0.00076145
Iteration 23/25 | Loss: 0.00076145
Iteration 24/25 | Loss: 0.00076145
Iteration 25/25 | Loss: 0.00076145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076145
Iteration 2/1000 | Loss: 0.00006254
Iteration 3/1000 | Loss: 0.00004497
Iteration 4/1000 | Loss: 0.00004051
Iteration 5/1000 | Loss: 0.00003876
Iteration 6/1000 | Loss: 0.00003747
Iteration 7/1000 | Loss: 0.00003662
Iteration 8/1000 | Loss: 0.00003593
Iteration 9/1000 | Loss: 0.00003545
Iteration 10/1000 | Loss: 0.00003503
Iteration 11/1000 | Loss: 0.00003467
Iteration 12/1000 | Loss: 0.00109933
Iteration 13/1000 | Loss: 0.00004416
Iteration 14/1000 | Loss: 0.00003799
Iteration 15/1000 | Loss: 0.00003344
Iteration 16/1000 | Loss: 0.00002952
Iteration 17/1000 | Loss: 0.00002721
Iteration 18/1000 | Loss: 0.00002622
Iteration 19/1000 | Loss: 0.00002534
Iteration 20/1000 | Loss: 0.00002484
Iteration 21/1000 | Loss: 0.00002451
Iteration 22/1000 | Loss: 0.00002427
Iteration 23/1000 | Loss: 0.00002409
Iteration 24/1000 | Loss: 0.00002396
Iteration 25/1000 | Loss: 0.00002390
Iteration 26/1000 | Loss: 0.00002387
Iteration 27/1000 | Loss: 0.00002386
Iteration 28/1000 | Loss: 0.00002386
Iteration 29/1000 | Loss: 0.00002385
Iteration 30/1000 | Loss: 0.00002385
Iteration 31/1000 | Loss: 0.00002385
Iteration 32/1000 | Loss: 0.00002385
Iteration 33/1000 | Loss: 0.00002384
Iteration 34/1000 | Loss: 0.00002384
Iteration 35/1000 | Loss: 0.00002384
Iteration 36/1000 | Loss: 0.00002383
Iteration 37/1000 | Loss: 0.00002383
Iteration 38/1000 | Loss: 0.00002383
Iteration 39/1000 | Loss: 0.00002383
Iteration 40/1000 | Loss: 0.00002383
Iteration 41/1000 | Loss: 0.00002383
Iteration 42/1000 | Loss: 0.00002382
Iteration 43/1000 | Loss: 0.00002381
Iteration 44/1000 | Loss: 0.00002381
Iteration 45/1000 | Loss: 0.00002381
Iteration 46/1000 | Loss: 0.00002380
Iteration 47/1000 | Loss: 0.00002380
Iteration 48/1000 | Loss: 0.00002380
Iteration 49/1000 | Loss: 0.00002380
Iteration 50/1000 | Loss: 0.00002380
Iteration 51/1000 | Loss: 0.00002379
Iteration 52/1000 | Loss: 0.00002379
Iteration 53/1000 | Loss: 0.00002379
Iteration 54/1000 | Loss: 0.00002379
Iteration 55/1000 | Loss: 0.00002378
Iteration 56/1000 | Loss: 0.00002378
Iteration 57/1000 | Loss: 0.00002377
Iteration 58/1000 | Loss: 0.00002377
Iteration 59/1000 | Loss: 0.00002376
Iteration 60/1000 | Loss: 0.00002376
Iteration 61/1000 | Loss: 0.00002376
Iteration 62/1000 | Loss: 0.00002376
Iteration 63/1000 | Loss: 0.00002376
Iteration 64/1000 | Loss: 0.00002376
Iteration 65/1000 | Loss: 0.00002375
Iteration 66/1000 | Loss: 0.00002375
Iteration 67/1000 | Loss: 0.00002375
Iteration 68/1000 | Loss: 0.00002375
Iteration 69/1000 | Loss: 0.00002374
Iteration 70/1000 | Loss: 0.00002372
Iteration 71/1000 | Loss: 0.00002372
Iteration 72/1000 | Loss: 0.00002371
Iteration 73/1000 | Loss: 0.00002371
Iteration 74/1000 | Loss: 0.00002371
Iteration 75/1000 | Loss: 0.00002371
Iteration 76/1000 | Loss: 0.00002371
Iteration 77/1000 | Loss: 0.00002371
Iteration 78/1000 | Loss: 0.00002370
Iteration 79/1000 | Loss: 0.00002370
Iteration 80/1000 | Loss: 0.00002370
Iteration 81/1000 | Loss: 0.00002370
Iteration 82/1000 | Loss: 0.00002370
Iteration 83/1000 | Loss: 0.00002369
Iteration 84/1000 | Loss: 0.00002369
Iteration 85/1000 | Loss: 0.00002369
Iteration 86/1000 | Loss: 0.00002369
Iteration 87/1000 | Loss: 0.00002369
Iteration 88/1000 | Loss: 0.00002369
Iteration 89/1000 | Loss: 0.00002369
Iteration 90/1000 | Loss: 0.00002369
Iteration 91/1000 | Loss: 0.00002369
Iteration 92/1000 | Loss: 0.00002369
Iteration 93/1000 | Loss: 0.00002369
Iteration 94/1000 | Loss: 0.00002369
Iteration 95/1000 | Loss: 0.00002369
Iteration 96/1000 | Loss: 0.00002368
Iteration 97/1000 | Loss: 0.00002368
Iteration 98/1000 | Loss: 0.00002368
Iteration 99/1000 | Loss: 0.00002368
Iteration 100/1000 | Loss: 0.00002368
Iteration 101/1000 | Loss: 0.00002368
Iteration 102/1000 | Loss: 0.00002368
Iteration 103/1000 | Loss: 0.00002368
Iteration 104/1000 | Loss: 0.00002368
Iteration 105/1000 | Loss: 0.00002368
Iteration 106/1000 | Loss: 0.00002368
Iteration 107/1000 | Loss: 0.00002368
Iteration 108/1000 | Loss: 0.00002368
Iteration 109/1000 | Loss: 0.00002367
Iteration 110/1000 | Loss: 0.00002367
Iteration 111/1000 | Loss: 0.00002367
Iteration 112/1000 | Loss: 0.00002367
Iteration 113/1000 | Loss: 0.00002367
Iteration 114/1000 | Loss: 0.00002367
Iteration 115/1000 | Loss: 0.00002367
Iteration 116/1000 | Loss: 0.00002367
Iteration 117/1000 | Loss: 0.00002367
Iteration 118/1000 | Loss: 0.00002367
Iteration 119/1000 | Loss: 0.00002367
Iteration 120/1000 | Loss: 0.00002367
Iteration 121/1000 | Loss: 0.00002367
Iteration 122/1000 | Loss: 0.00002367
Iteration 123/1000 | Loss: 0.00002367
Iteration 124/1000 | Loss: 0.00002367
Iteration 125/1000 | Loss: 0.00002367
Iteration 126/1000 | Loss: 0.00002367
Iteration 127/1000 | Loss: 0.00002367
Iteration 128/1000 | Loss: 0.00002367
Iteration 129/1000 | Loss: 0.00002367
Iteration 130/1000 | Loss: 0.00002367
Iteration 131/1000 | Loss: 0.00002367
Iteration 132/1000 | Loss: 0.00002367
Iteration 133/1000 | Loss: 0.00002367
Iteration 134/1000 | Loss: 0.00002367
Iteration 135/1000 | Loss: 0.00002367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.366638000239618e-05, 2.366638000239618e-05, 2.366638000239618e-05, 2.366638000239618e-05, 2.366638000239618e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.366638000239618e-05

Optimization complete. Final v2v error: 4.0767903327941895 mm

Highest mean error: 4.363316059112549 mm for frame 150

Lowest mean error: 3.7320733070373535 mm for frame 2

Saving results

Total time: 68.95932817459106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00912417
Iteration 2/25 | Loss: 0.00173903
Iteration 3/25 | Loss: 0.00137703
Iteration 4/25 | Loss: 0.00131686
Iteration 5/25 | Loss: 0.00130740
Iteration 6/25 | Loss: 0.00130544
Iteration 7/25 | Loss: 0.00130494
Iteration 8/25 | Loss: 0.00130494
Iteration 9/25 | Loss: 0.00130494
Iteration 10/25 | Loss: 0.00130494
Iteration 11/25 | Loss: 0.00130494
Iteration 12/25 | Loss: 0.00130494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001304935896769166, 0.001304935896769166, 0.001304935896769166, 0.001304935896769166, 0.001304935896769166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001304935896769166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55307341
Iteration 2/25 | Loss: 0.00082502
Iteration 3/25 | Loss: 0.00082493
Iteration 4/25 | Loss: 0.00082493
Iteration 5/25 | Loss: 0.00082493
Iteration 6/25 | Loss: 0.00082492
Iteration 7/25 | Loss: 0.00082492
Iteration 8/25 | Loss: 0.00082492
Iteration 9/25 | Loss: 0.00082492
Iteration 10/25 | Loss: 0.00082492
Iteration 11/25 | Loss: 0.00082492
Iteration 12/25 | Loss: 0.00082492
Iteration 13/25 | Loss: 0.00082492
Iteration 14/25 | Loss: 0.00082492
Iteration 15/25 | Loss: 0.00082492
Iteration 16/25 | Loss: 0.00082492
Iteration 17/25 | Loss: 0.00082492
Iteration 18/25 | Loss: 0.00082492
Iteration 19/25 | Loss: 0.00082492
Iteration 20/25 | Loss: 0.00082492
Iteration 21/25 | Loss: 0.00082492
Iteration 22/25 | Loss: 0.00082492
Iteration 23/25 | Loss: 0.00082492
Iteration 24/25 | Loss: 0.00082492
Iteration 25/25 | Loss: 0.00082492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082492
Iteration 2/1000 | Loss: 0.00008553
Iteration 3/1000 | Loss: 0.00005711
Iteration 4/1000 | Loss: 0.00004474
Iteration 5/1000 | Loss: 0.00004038
Iteration 6/1000 | Loss: 0.00003801
Iteration 7/1000 | Loss: 0.00003662
Iteration 8/1000 | Loss: 0.00003567
Iteration 9/1000 | Loss: 0.00003500
Iteration 10/1000 | Loss: 0.00003443
Iteration 11/1000 | Loss: 0.00003398
Iteration 12/1000 | Loss: 0.00003368
Iteration 13/1000 | Loss: 0.00003343
Iteration 14/1000 | Loss: 0.00003319
Iteration 15/1000 | Loss: 0.00003297
Iteration 16/1000 | Loss: 0.00003280
Iteration 17/1000 | Loss: 0.00003264
Iteration 18/1000 | Loss: 0.00003259
Iteration 19/1000 | Loss: 0.00003247
Iteration 20/1000 | Loss: 0.00003242
Iteration 21/1000 | Loss: 0.00003239
Iteration 22/1000 | Loss: 0.00003238
Iteration 23/1000 | Loss: 0.00003237
Iteration 24/1000 | Loss: 0.00003237
Iteration 25/1000 | Loss: 0.00003235
Iteration 26/1000 | Loss: 0.00003234
Iteration 27/1000 | Loss: 0.00003221
Iteration 28/1000 | Loss: 0.00003220
Iteration 29/1000 | Loss: 0.00003217
Iteration 30/1000 | Loss: 0.00003216
Iteration 31/1000 | Loss: 0.00003215
Iteration 32/1000 | Loss: 0.00003214
Iteration 33/1000 | Loss: 0.00003212
Iteration 34/1000 | Loss: 0.00003212
Iteration 35/1000 | Loss: 0.00003211
Iteration 36/1000 | Loss: 0.00003210
Iteration 37/1000 | Loss: 0.00003210
Iteration 38/1000 | Loss: 0.00003208
Iteration 39/1000 | Loss: 0.00003207
Iteration 40/1000 | Loss: 0.00003205
Iteration 41/1000 | Loss: 0.00003205
Iteration 42/1000 | Loss: 0.00003205
Iteration 43/1000 | Loss: 0.00003205
Iteration 44/1000 | Loss: 0.00003205
Iteration 45/1000 | Loss: 0.00003205
Iteration 46/1000 | Loss: 0.00003205
Iteration 47/1000 | Loss: 0.00003205
Iteration 48/1000 | Loss: 0.00003204
Iteration 49/1000 | Loss: 0.00003204
Iteration 50/1000 | Loss: 0.00003204
Iteration 51/1000 | Loss: 0.00003204
Iteration 52/1000 | Loss: 0.00003204
Iteration 53/1000 | Loss: 0.00003204
Iteration 54/1000 | Loss: 0.00003204
Iteration 55/1000 | Loss: 0.00003204
Iteration 56/1000 | Loss: 0.00003204
Iteration 57/1000 | Loss: 0.00003204
Iteration 58/1000 | Loss: 0.00003204
Iteration 59/1000 | Loss: 0.00003203
Iteration 60/1000 | Loss: 0.00003203
Iteration 61/1000 | Loss: 0.00003202
Iteration 62/1000 | Loss: 0.00003202
Iteration 63/1000 | Loss: 0.00003201
Iteration 64/1000 | Loss: 0.00003201
Iteration 65/1000 | Loss: 0.00003201
Iteration 66/1000 | Loss: 0.00003200
Iteration 67/1000 | Loss: 0.00003200
Iteration 68/1000 | Loss: 0.00003200
Iteration 69/1000 | Loss: 0.00003200
Iteration 70/1000 | Loss: 0.00003200
Iteration 71/1000 | Loss: 0.00003199
Iteration 72/1000 | Loss: 0.00003199
Iteration 73/1000 | Loss: 0.00003199
Iteration 74/1000 | Loss: 0.00003199
Iteration 75/1000 | Loss: 0.00003198
Iteration 76/1000 | Loss: 0.00003198
Iteration 77/1000 | Loss: 0.00003198
Iteration 78/1000 | Loss: 0.00003198
Iteration 79/1000 | Loss: 0.00003198
Iteration 80/1000 | Loss: 0.00003198
Iteration 81/1000 | Loss: 0.00003198
Iteration 82/1000 | Loss: 0.00003198
Iteration 83/1000 | Loss: 0.00003197
Iteration 84/1000 | Loss: 0.00003197
Iteration 85/1000 | Loss: 0.00003197
Iteration 86/1000 | Loss: 0.00003197
Iteration 87/1000 | Loss: 0.00003197
Iteration 88/1000 | Loss: 0.00003197
Iteration 89/1000 | Loss: 0.00003197
Iteration 90/1000 | Loss: 0.00003197
Iteration 91/1000 | Loss: 0.00003197
Iteration 92/1000 | Loss: 0.00003197
Iteration 93/1000 | Loss: 0.00003197
Iteration 94/1000 | Loss: 0.00003197
Iteration 95/1000 | Loss: 0.00003197
Iteration 96/1000 | Loss: 0.00003197
Iteration 97/1000 | Loss: 0.00003197
Iteration 98/1000 | Loss: 0.00003197
Iteration 99/1000 | Loss: 0.00003197
Iteration 100/1000 | Loss: 0.00003197
Iteration 101/1000 | Loss: 0.00003197
Iteration 102/1000 | Loss: 0.00003197
Iteration 103/1000 | Loss: 0.00003197
Iteration 104/1000 | Loss: 0.00003197
Iteration 105/1000 | Loss: 0.00003197
Iteration 106/1000 | Loss: 0.00003197
Iteration 107/1000 | Loss: 0.00003197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [3.197132173227146e-05, 3.197132173227146e-05, 3.197132173227146e-05, 3.197132173227146e-05, 3.197132173227146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.197132173227146e-05

Optimization complete. Final v2v error: 4.6029276847839355 mm

Highest mean error: 6.7877302169799805 mm for frame 82

Lowest mean error: 3.414529323577881 mm for frame 30

Saving results

Total time: 43.80732870101929
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032192
Iteration 2/25 | Loss: 0.00210597
Iteration 3/25 | Loss: 0.00155591
Iteration 4/25 | Loss: 0.00152967
Iteration 5/25 | Loss: 0.00152515
Iteration 6/25 | Loss: 0.00152354
Iteration 7/25 | Loss: 0.00152347
Iteration 8/25 | Loss: 0.00152347
Iteration 9/25 | Loss: 0.00152347
Iteration 10/25 | Loss: 0.00152347
Iteration 11/25 | Loss: 0.00152347
Iteration 12/25 | Loss: 0.00152342
Iteration 13/25 | Loss: 0.00152342
Iteration 14/25 | Loss: 0.00152342
Iteration 15/25 | Loss: 0.00152342
Iteration 16/25 | Loss: 0.00152342
Iteration 17/25 | Loss: 0.00152342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001523422310128808, 0.001523422310128808, 0.001523422310128808, 0.001523422310128808, 0.001523422310128808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001523422310128808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67600352
Iteration 2/25 | Loss: 0.00101932
Iteration 3/25 | Loss: 0.00101929
Iteration 4/25 | Loss: 0.00101929
Iteration 5/25 | Loss: 0.00101929
Iteration 6/25 | Loss: 0.00101929
Iteration 7/25 | Loss: 0.00101929
Iteration 8/25 | Loss: 0.00101929
Iteration 9/25 | Loss: 0.00101929
Iteration 10/25 | Loss: 0.00101929
Iteration 11/25 | Loss: 0.00101929
Iteration 12/25 | Loss: 0.00101929
Iteration 13/25 | Loss: 0.00101929
Iteration 14/25 | Loss: 0.00101929
Iteration 15/25 | Loss: 0.00101929
Iteration 16/25 | Loss: 0.00101929
Iteration 17/25 | Loss: 0.00101929
Iteration 18/25 | Loss: 0.00101929
Iteration 19/25 | Loss: 0.00101929
Iteration 20/25 | Loss: 0.00101929
Iteration 21/25 | Loss: 0.00101929
Iteration 22/25 | Loss: 0.00101929
Iteration 23/25 | Loss: 0.00101929
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010192908812314272, 0.0010192908812314272, 0.0010192908812314272, 0.0010192908812314272, 0.0010192908812314272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010192908812314272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101929
Iteration 2/1000 | Loss: 0.00012579
Iteration 3/1000 | Loss: 0.00007272
Iteration 4/1000 | Loss: 0.00005823
Iteration 5/1000 | Loss: 0.00005526
Iteration 6/1000 | Loss: 0.00005349
Iteration 7/1000 | Loss: 0.00005243
Iteration 8/1000 | Loss: 0.00005162
Iteration 9/1000 | Loss: 0.00005058
Iteration 10/1000 | Loss: 0.00004998
Iteration 11/1000 | Loss: 0.00004956
Iteration 12/1000 | Loss: 0.00004921
Iteration 13/1000 | Loss: 0.00004878
Iteration 14/1000 | Loss: 0.00004837
Iteration 15/1000 | Loss: 0.00004808
Iteration 16/1000 | Loss: 0.00004781
Iteration 17/1000 | Loss: 0.00004775
Iteration 18/1000 | Loss: 0.00004763
Iteration 19/1000 | Loss: 0.00004746
Iteration 20/1000 | Loss: 0.00004732
Iteration 21/1000 | Loss: 0.00004729
Iteration 22/1000 | Loss: 0.00004721
Iteration 23/1000 | Loss: 0.00004719
Iteration 24/1000 | Loss: 0.00004718
Iteration 25/1000 | Loss: 0.00004718
Iteration 26/1000 | Loss: 0.00004715
Iteration 27/1000 | Loss: 0.00004714
Iteration 28/1000 | Loss: 0.00004714
Iteration 29/1000 | Loss: 0.00004711
Iteration 30/1000 | Loss: 0.00004711
Iteration 31/1000 | Loss: 0.00004709
Iteration 32/1000 | Loss: 0.00004709
Iteration 33/1000 | Loss: 0.00004708
Iteration 34/1000 | Loss: 0.00004708
Iteration 35/1000 | Loss: 0.00004708
Iteration 36/1000 | Loss: 0.00004708
Iteration 37/1000 | Loss: 0.00004708
Iteration 38/1000 | Loss: 0.00004708
Iteration 39/1000 | Loss: 0.00004708
Iteration 40/1000 | Loss: 0.00004708
Iteration 41/1000 | Loss: 0.00004708
Iteration 42/1000 | Loss: 0.00004708
Iteration 43/1000 | Loss: 0.00004707
Iteration 44/1000 | Loss: 0.00004707
Iteration 45/1000 | Loss: 0.00004707
Iteration 46/1000 | Loss: 0.00004707
Iteration 47/1000 | Loss: 0.00004707
Iteration 48/1000 | Loss: 0.00004707
Iteration 49/1000 | Loss: 0.00004707
Iteration 50/1000 | Loss: 0.00004706
Iteration 51/1000 | Loss: 0.00004706
Iteration 52/1000 | Loss: 0.00004706
Iteration 53/1000 | Loss: 0.00004705
Iteration 54/1000 | Loss: 0.00004705
Iteration 55/1000 | Loss: 0.00004705
Iteration 56/1000 | Loss: 0.00004705
Iteration 57/1000 | Loss: 0.00004705
Iteration 58/1000 | Loss: 0.00004705
Iteration 59/1000 | Loss: 0.00004703
Iteration 60/1000 | Loss: 0.00004703
Iteration 61/1000 | Loss: 0.00004703
Iteration 62/1000 | Loss: 0.00004703
Iteration 63/1000 | Loss: 0.00004703
Iteration 64/1000 | Loss: 0.00004702
Iteration 65/1000 | Loss: 0.00004702
Iteration 66/1000 | Loss: 0.00004702
Iteration 67/1000 | Loss: 0.00004701
Iteration 68/1000 | Loss: 0.00004700
Iteration 69/1000 | Loss: 0.00004700
Iteration 70/1000 | Loss: 0.00004700
Iteration 71/1000 | Loss: 0.00004700
Iteration 72/1000 | Loss: 0.00004700
Iteration 73/1000 | Loss: 0.00004700
Iteration 74/1000 | Loss: 0.00004699
Iteration 75/1000 | Loss: 0.00004699
Iteration 76/1000 | Loss: 0.00004699
Iteration 77/1000 | Loss: 0.00004699
Iteration 78/1000 | Loss: 0.00004699
Iteration 79/1000 | Loss: 0.00004699
Iteration 80/1000 | Loss: 0.00004699
Iteration 81/1000 | Loss: 0.00004698
Iteration 82/1000 | Loss: 0.00004698
Iteration 83/1000 | Loss: 0.00004698
Iteration 84/1000 | Loss: 0.00004698
Iteration 85/1000 | Loss: 0.00004698
Iteration 86/1000 | Loss: 0.00004698
Iteration 87/1000 | Loss: 0.00004697
Iteration 88/1000 | Loss: 0.00004697
Iteration 89/1000 | Loss: 0.00004697
Iteration 90/1000 | Loss: 0.00004697
Iteration 91/1000 | Loss: 0.00004696
Iteration 92/1000 | Loss: 0.00004696
Iteration 93/1000 | Loss: 0.00004696
Iteration 94/1000 | Loss: 0.00004696
Iteration 95/1000 | Loss: 0.00004696
Iteration 96/1000 | Loss: 0.00004695
Iteration 97/1000 | Loss: 0.00004695
Iteration 98/1000 | Loss: 0.00004695
Iteration 99/1000 | Loss: 0.00004695
Iteration 100/1000 | Loss: 0.00004694
Iteration 101/1000 | Loss: 0.00004694
Iteration 102/1000 | Loss: 0.00004694
Iteration 103/1000 | Loss: 0.00004694
Iteration 104/1000 | Loss: 0.00004694
Iteration 105/1000 | Loss: 0.00004694
Iteration 106/1000 | Loss: 0.00004693
Iteration 107/1000 | Loss: 0.00004693
Iteration 108/1000 | Loss: 0.00004693
Iteration 109/1000 | Loss: 0.00004693
Iteration 110/1000 | Loss: 0.00004693
Iteration 111/1000 | Loss: 0.00004692
Iteration 112/1000 | Loss: 0.00004692
Iteration 113/1000 | Loss: 0.00004692
Iteration 114/1000 | Loss: 0.00004692
Iteration 115/1000 | Loss: 0.00004692
Iteration 116/1000 | Loss: 0.00004692
Iteration 117/1000 | Loss: 0.00004692
Iteration 118/1000 | Loss: 0.00004692
Iteration 119/1000 | Loss: 0.00004692
Iteration 120/1000 | Loss: 0.00004692
Iteration 121/1000 | Loss: 0.00004691
Iteration 122/1000 | Loss: 0.00004691
Iteration 123/1000 | Loss: 0.00004691
Iteration 124/1000 | Loss: 0.00004691
Iteration 125/1000 | Loss: 0.00004691
Iteration 126/1000 | Loss: 0.00004691
Iteration 127/1000 | Loss: 0.00004691
Iteration 128/1000 | Loss: 0.00004691
Iteration 129/1000 | Loss: 0.00004691
Iteration 130/1000 | Loss: 0.00004691
Iteration 131/1000 | Loss: 0.00004691
Iteration 132/1000 | Loss: 0.00004691
Iteration 133/1000 | Loss: 0.00004691
Iteration 134/1000 | Loss: 0.00004691
Iteration 135/1000 | Loss: 0.00004691
Iteration 136/1000 | Loss: 0.00004691
Iteration 137/1000 | Loss: 0.00004691
Iteration 138/1000 | Loss: 0.00004691
Iteration 139/1000 | Loss: 0.00004691
Iteration 140/1000 | Loss: 0.00004691
Iteration 141/1000 | Loss: 0.00004691
Iteration 142/1000 | Loss: 0.00004691
Iteration 143/1000 | Loss: 0.00004691
Iteration 144/1000 | Loss: 0.00004691
Iteration 145/1000 | Loss: 0.00004691
Iteration 146/1000 | Loss: 0.00004691
Iteration 147/1000 | Loss: 0.00004691
Iteration 148/1000 | Loss: 0.00004691
Iteration 149/1000 | Loss: 0.00004691
Iteration 150/1000 | Loss: 0.00004691
Iteration 151/1000 | Loss: 0.00004691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [4.691013600677252e-05, 4.691013600677252e-05, 4.691013600677252e-05, 4.691013600677252e-05, 4.691013600677252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.691013600677252e-05

Optimization complete. Final v2v error: 5.207139015197754 mm

Highest mean error: 5.784026145935059 mm for frame 64

Lowest mean error: 3.9284210205078125 mm for frame 19

Saving results

Total time: 46.90110206604004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00602270
Iteration 2/25 | Loss: 0.00152591
Iteration 3/25 | Loss: 0.00130513
Iteration 4/25 | Loss: 0.00125208
Iteration 5/25 | Loss: 0.00124794
Iteration 6/25 | Loss: 0.00124563
Iteration 7/25 | Loss: 0.00124210
Iteration 8/25 | Loss: 0.00123994
Iteration 9/25 | Loss: 0.00123876
Iteration 10/25 | Loss: 0.00123822
Iteration 11/25 | Loss: 0.00123794
Iteration 12/25 | Loss: 0.00123781
Iteration 13/25 | Loss: 0.00123780
Iteration 14/25 | Loss: 0.00123780
Iteration 15/25 | Loss: 0.00123780
Iteration 16/25 | Loss: 0.00123779
Iteration 17/25 | Loss: 0.00123779
Iteration 18/25 | Loss: 0.00123779
Iteration 19/25 | Loss: 0.00123779
Iteration 20/25 | Loss: 0.00123779
Iteration 21/25 | Loss: 0.00123779
Iteration 22/25 | Loss: 0.00123779
Iteration 23/25 | Loss: 0.00123779
Iteration 24/25 | Loss: 0.00123779
Iteration 25/25 | Loss: 0.00123778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.73198462
Iteration 2/25 | Loss: 0.00063782
Iteration 3/25 | Loss: 0.00062590
Iteration 4/25 | Loss: 0.00062590
Iteration 5/25 | Loss: 0.00062590
Iteration 6/25 | Loss: 0.00062590
Iteration 7/25 | Loss: 0.00062590
Iteration 8/25 | Loss: 0.00062590
Iteration 9/25 | Loss: 0.00062590
Iteration 10/25 | Loss: 0.00062590
Iteration 11/25 | Loss: 0.00062590
Iteration 12/25 | Loss: 0.00062589
Iteration 13/25 | Loss: 0.00062589
Iteration 14/25 | Loss: 0.00062589
Iteration 15/25 | Loss: 0.00062589
Iteration 16/25 | Loss: 0.00062589
Iteration 17/25 | Loss: 0.00062589
Iteration 18/25 | Loss: 0.00062589
Iteration 19/25 | Loss: 0.00062589
Iteration 20/25 | Loss: 0.00062589
Iteration 21/25 | Loss: 0.00062589
Iteration 22/25 | Loss: 0.00062589
Iteration 23/25 | Loss: 0.00062589
Iteration 24/25 | Loss: 0.00062589
Iteration 25/25 | Loss: 0.00062589

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062589
Iteration 2/1000 | Loss: 0.00006437
Iteration 3/1000 | Loss: 0.00002465
Iteration 4/1000 | Loss: 0.00003144
Iteration 5/1000 | Loss: 0.00002624
Iteration 6/1000 | Loss: 0.00002049
Iteration 7/1000 | Loss: 0.00001954
Iteration 8/1000 | Loss: 0.00001899
Iteration 9/1000 | Loss: 0.00001876
Iteration 10/1000 | Loss: 0.00001855
Iteration 11/1000 | Loss: 0.00001828
Iteration 12/1000 | Loss: 0.00001827
Iteration 13/1000 | Loss: 0.00001816
Iteration 14/1000 | Loss: 0.00001814
Iteration 15/1000 | Loss: 0.00001806
Iteration 16/1000 | Loss: 0.00001803
Iteration 17/1000 | Loss: 0.00001802
Iteration 18/1000 | Loss: 0.00001801
Iteration 19/1000 | Loss: 0.00001801
Iteration 20/1000 | Loss: 0.00001797
Iteration 21/1000 | Loss: 0.00001797
Iteration 22/1000 | Loss: 0.00001794
Iteration 23/1000 | Loss: 0.00001791
Iteration 24/1000 | Loss: 0.00001790
Iteration 25/1000 | Loss: 0.00001789
Iteration 26/1000 | Loss: 0.00001788
Iteration 27/1000 | Loss: 0.00001787
Iteration 28/1000 | Loss: 0.00001787
Iteration 29/1000 | Loss: 0.00001785
Iteration 30/1000 | Loss: 0.00001785
Iteration 31/1000 | Loss: 0.00001784
Iteration 32/1000 | Loss: 0.00001784
Iteration 33/1000 | Loss: 0.00001783
Iteration 34/1000 | Loss: 0.00001783
Iteration 35/1000 | Loss: 0.00001782
Iteration 36/1000 | Loss: 0.00001781
Iteration 37/1000 | Loss: 0.00001781
Iteration 38/1000 | Loss: 0.00001781
Iteration 39/1000 | Loss: 0.00001780
Iteration 40/1000 | Loss: 0.00001780
Iteration 41/1000 | Loss: 0.00001779
Iteration 42/1000 | Loss: 0.00001779
Iteration 43/1000 | Loss: 0.00001779
Iteration 44/1000 | Loss: 0.00001778
Iteration 45/1000 | Loss: 0.00001777
Iteration 46/1000 | Loss: 0.00001777
Iteration 47/1000 | Loss: 0.00001777
Iteration 48/1000 | Loss: 0.00001777
Iteration 49/1000 | Loss: 0.00001776
Iteration 50/1000 | Loss: 0.00001776
Iteration 51/1000 | Loss: 0.00001775
Iteration 52/1000 | Loss: 0.00001775
Iteration 53/1000 | Loss: 0.00001774
Iteration 54/1000 | Loss: 0.00001773
Iteration 55/1000 | Loss: 0.00001773
Iteration 56/1000 | Loss: 0.00001772
Iteration 57/1000 | Loss: 0.00001771
Iteration 58/1000 | Loss: 0.00001771
Iteration 59/1000 | Loss: 0.00001766
Iteration 60/1000 | Loss: 0.00001765
Iteration 61/1000 | Loss: 0.00001761
Iteration 62/1000 | Loss: 0.00001761
Iteration 63/1000 | Loss: 0.00001761
Iteration 64/1000 | Loss: 0.00001760
Iteration 65/1000 | Loss: 0.00001760
Iteration 66/1000 | Loss: 0.00001760
Iteration 67/1000 | Loss: 0.00001759
Iteration 68/1000 | Loss: 0.00001759
Iteration 69/1000 | Loss: 0.00001758
Iteration 70/1000 | Loss: 0.00001758
Iteration 71/1000 | Loss: 0.00001758
Iteration 72/1000 | Loss: 0.00001758
Iteration 73/1000 | Loss: 0.00001758
Iteration 74/1000 | Loss: 0.00001758
Iteration 75/1000 | Loss: 0.00001757
Iteration 76/1000 | Loss: 0.00001757
Iteration 77/1000 | Loss: 0.00001757
Iteration 78/1000 | Loss: 0.00001757
Iteration 79/1000 | Loss: 0.00001756
Iteration 80/1000 | Loss: 0.00001756
Iteration 81/1000 | Loss: 0.00001756
Iteration 82/1000 | Loss: 0.00001755
Iteration 83/1000 | Loss: 0.00001755
Iteration 84/1000 | Loss: 0.00001754
Iteration 85/1000 | Loss: 0.00001754
Iteration 86/1000 | Loss: 0.00001754
Iteration 87/1000 | Loss: 0.00001754
Iteration 88/1000 | Loss: 0.00001753
Iteration 89/1000 | Loss: 0.00001753
Iteration 90/1000 | Loss: 0.00001752
Iteration 91/1000 | Loss: 0.00001752
Iteration 92/1000 | Loss: 0.00001752
Iteration 93/1000 | Loss: 0.00001752
Iteration 94/1000 | Loss: 0.00001752
Iteration 95/1000 | Loss: 0.00001752
Iteration 96/1000 | Loss: 0.00001752
Iteration 97/1000 | Loss: 0.00001751
Iteration 98/1000 | Loss: 0.00001751
Iteration 99/1000 | Loss: 0.00001751
Iteration 100/1000 | Loss: 0.00001751
Iteration 101/1000 | Loss: 0.00001751
Iteration 102/1000 | Loss: 0.00001751
Iteration 103/1000 | Loss: 0.00001751
Iteration 104/1000 | Loss: 0.00001751
Iteration 105/1000 | Loss: 0.00001750
Iteration 106/1000 | Loss: 0.00001750
Iteration 107/1000 | Loss: 0.00001750
Iteration 108/1000 | Loss: 0.00001750
Iteration 109/1000 | Loss: 0.00001749
Iteration 110/1000 | Loss: 0.00001749
Iteration 111/1000 | Loss: 0.00001748
Iteration 112/1000 | Loss: 0.00001748
Iteration 113/1000 | Loss: 0.00001748
Iteration 114/1000 | Loss: 0.00001748
Iteration 115/1000 | Loss: 0.00001747
Iteration 116/1000 | Loss: 0.00001747
Iteration 117/1000 | Loss: 0.00001747
Iteration 118/1000 | Loss: 0.00001747
Iteration 119/1000 | Loss: 0.00001746
Iteration 120/1000 | Loss: 0.00001746
Iteration 121/1000 | Loss: 0.00001746
Iteration 122/1000 | Loss: 0.00001746
Iteration 123/1000 | Loss: 0.00001746
Iteration 124/1000 | Loss: 0.00001745
Iteration 125/1000 | Loss: 0.00001745
Iteration 126/1000 | Loss: 0.00001745
Iteration 127/1000 | Loss: 0.00001745
Iteration 128/1000 | Loss: 0.00001745
Iteration 129/1000 | Loss: 0.00001745
Iteration 130/1000 | Loss: 0.00001745
Iteration 131/1000 | Loss: 0.00001745
Iteration 132/1000 | Loss: 0.00001744
Iteration 133/1000 | Loss: 0.00001744
Iteration 134/1000 | Loss: 0.00001744
Iteration 135/1000 | Loss: 0.00001744
Iteration 136/1000 | Loss: 0.00001744
Iteration 137/1000 | Loss: 0.00001744
Iteration 138/1000 | Loss: 0.00001744
Iteration 139/1000 | Loss: 0.00001744
Iteration 140/1000 | Loss: 0.00001744
Iteration 141/1000 | Loss: 0.00001744
Iteration 142/1000 | Loss: 0.00001744
Iteration 143/1000 | Loss: 0.00001743
Iteration 144/1000 | Loss: 0.00001743
Iteration 145/1000 | Loss: 0.00001743
Iteration 146/1000 | Loss: 0.00001743
Iteration 147/1000 | Loss: 0.00001743
Iteration 148/1000 | Loss: 0.00001743
Iteration 149/1000 | Loss: 0.00001743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.743390384945087e-05, 1.743390384945087e-05, 1.743390384945087e-05, 1.743390384945087e-05, 1.743390384945087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.743390384945087e-05

Optimization complete. Final v2v error: 3.4890806674957275 mm

Highest mean error: 4.011185169219971 mm for frame 173

Lowest mean error: 3.1902167797088623 mm for frame 198

Saving results

Total time: 58.200425148010254
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00657207
Iteration 2/25 | Loss: 0.00154248
Iteration 3/25 | Loss: 0.00131288
Iteration 4/25 | Loss: 0.00127310
Iteration 5/25 | Loss: 0.00126144
Iteration 6/25 | Loss: 0.00125950
Iteration 7/25 | Loss: 0.00126374
Iteration 8/25 | Loss: 0.00125559
Iteration 9/25 | Loss: 0.00125063
Iteration 10/25 | Loss: 0.00124382
Iteration 11/25 | Loss: 0.00124217
Iteration 12/25 | Loss: 0.00124211
Iteration 13/25 | Loss: 0.00124211
Iteration 14/25 | Loss: 0.00124211
Iteration 15/25 | Loss: 0.00124211
Iteration 16/25 | Loss: 0.00124210
Iteration 17/25 | Loss: 0.00124210
Iteration 18/25 | Loss: 0.00124210
Iteration 19/25 | Loss: 0.00124210
Iteration 20/25 | Loss: 0.00124210
Iteration 21/25 | Loss: 0.00124210
Iteration 22/25 | Loss: 0.00124210
Iteration 23/25 | Loss: 0.00124210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012421031715348363, 0.0012421031715348363, 0.0012421031715348363, 0.0012421031715348363, 0.0012421031715348363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012421031715348363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07702184
Iteration 2/25 | Loss: 0.00077243
Iteration 3/25 | Loss: 0.00077210
Iteration 4/25 | Loss: 0.00077210
Iteration 5/25 | Loss: 0.00077210
Iteration 6/25 | Loss: 0.00077210
Iteration 7/25 | Loss: 0.00077210
Iteration 8/25 | Loss: 0.00077210
Iteration 9/25 | Loss: 0.00077210
Iteration 10/25 | Loss: 0.00077210
Iteration 11/25 | Loss: 0.00077210
Iteration 12/25 | Loss: 0.00077210
Iteration 13/25 | Loss: 0.00077210
Iteration 14/25 | Loss: 0.00077210
Iteration 15/25 | Loss: 0.00077210
Iteration 16/25 | Loss: 0.00077210
Iteration 17/25 | Loss: 0.00077210
Iteration 18/25 | Loss: 0.00077210
Iteration 19/25 | Loss: 0.00077210
Iteration 20/25 | Loss: 0.00077210
Iteration 21/25 | Loss: 0.00077210
Iteration 22/25 | Loss: 0.00077210
Iteration 23/25 | Loss: 0.00077210
Iteration 24/25 | Loss: 0.00077210
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007720961002632976, 0.0007720961002632976, 0.0007720961002632976, 0.0007720961002632976, 0.0007720961002632976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007720961002632976

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077210
Iteration 2/1000 | Loss: 0.00006877
Iteration 3/1000 | Loss: 0.00004252
Iteration 4/1000 | Loss: 0.00003243
Iteration 5/1000 | Loss: 0.00002899
Iteration 6/1000 | Loss: 0.00002744
Iteration 7/1000 | Loss: 0.00002636
Iteration 8/1000 | Loss: 0.00002561
Iteration 9/1000 | Loss: 0.00002495
Iteration 10/1000 | Loss: 0.00002454
Iteration 11/1000 | Loss: 0.00002420
Iteration 12/1000 | Loss: 0.00002392
Iteration 13/1000 | Loss: 0.00002368
Iteration 14/1000 | Loss: 0.00002359
Iteration 15/1000 | Loss: 0.00002344
Iteration 16/1000 | Loss: 0.00002344
Iteration 17/1000 | Loss: 0.00002327
Iteration 18/1000 | Loss: 0.00002325
Iteration 19/1000 | Loss: 0.00002323
Iteration 20/1000 | Loss: 0.00002323
Iteration 21/1000 | Loss: 0.00002322
Iteration 22/1000 | Loss: 0.00002315
Iteration 23/1000 | Loss: 0.00002314
Iteration 24/1000 | Loss: 0.00002313
Iteration 25/1000 | Loss: 0.00002309
Iteration 26/1000 | Loss: 0.00002309
Iteration 27/1000 | Loss: 0.00002306
Iteration 28/1000 | Loss: 0.00002306
Iteration 29/1000 | Loss: 0.00002305
Iteration 30/1000 | Loss: 0.00002298
Iteration 31/1000 | Loss: 0.00002298
Iteration 32/1000 | Loss: 0.00002296
Iteration 33/1000 | Loss: 0.00002295
Iteration 34/1000 | Loss: 0.00002293
Iteration 35/1000 | Loss: 0.00002293
Iteration 36/1000 | Loss: 0.00002292
Iteration 37/1000 | Loss: 0.00002292
Iteration 38/1000 | Loss: 0.00002291
Iteration 39/1000 | Loss: 0.00002291
Iteration 40/1000 | Loss: 0.00002291
Iteration 41/1000 | Loss: 0.00002290
Iteration 42/1000 | Loss: 0.00002289
Iteration 43/1000 | Loss: 0.00002288
Iteration 44/1000 | Loss: 0.00002287
Iteration 45/1000 | Loss: 0.00002286
Iteration 46/1000 | Loss: 0.00002286
Iteration 47/1000 | Loss: 0.00002286
Iteration 48/1000 | Loss: 0.00002285
Iteration 49/1000 | Loss: 0.00002285
Iteration 50/1000 | Loss: 0.00002285
Iteration 51/1000 | Loss: 0.00002285
Iteration 52/1000 | Loss: 0.00002284
Iteration 53/1000 | Loss: 0.00002283
Iteration 54/1000 | Loss: 0.00002283
Iteration 55/1000 | Loss: 0.00002283
Iteration 56/1000 | Loss: 0.00002283
Iteration 57/1000 | Loss: 0.00002282
Iteration 58/1000 | Loss: 0.00002281
Iteration 59/1000 | Loss: 0.00002281
Iteration 60/1000 | Loss: 0.00002281
Iteration 61/1000 | Loss: 0.00002281
Iteration 62/1000 | Loss: 0.00002280
Iteration 63/1000 | Loss: 0.00002280
Iteration 64/1000 | Loss: 0.00002280
Iteration 65/1000 | Loss: 0.00002279
Iteration 66/1000 | Loss: 0.00002278
Iteration 67/1000 | Loss: 0.00002278
Iteration 68/1000 | Loss: 0.00002277
Iteration 69/1000 | Loss: 0.00002277
Iteration 70/1000 | Loss: 0.00002277
Iteration 71/1000 | Loss: 0.00002277
Iteration 72/1000 | Loss: 0.00002276
Iteration 73/1000 | Loss: 0.00002276
Iteration 74/1000 | Loss: 0.00002276
Iteration 75/1000 | Loss: 0.00002276
Iteration 76/1000 | Loss: 0.00002276
Iteration 77/1000 | Loss: 0.00002276
Iteration 78/1000 | Loss: 0.00002276
Iteration 79/1000 | Loss: 0.00002275
Iteration 80/1000 | Loss: 0.00002275
Iteration 81/1000 | Loss: 0.00002275
Iteration 82/1000 | Loss: 0.00002275
Iteration 83/1000 | Loss: 0.00002274
Iteration 84/1000 | Loss: 0.00002274
Iteration 85/1000 | Loss: 0.00002274
Iteration 86/1000 | Loss: 0.00002273
Iteration 87/1000 | Loss: 0.00002273
Iteration 88/1000 | Loss: 0.00002273
Iteration 89/1000 | Loss: 0.00002273
Iteration 90/1000 | Loss: 0.00002272
Iteration 91/1000 | Loss: 0.00002272
Iteration 92/1000 | Loss: 0.00002271
Iteration 93/1000 | Loss: 0.00002271
Iteration 94/1000 | Loss: 0.00002271
Iteration 95/1000 | Loss: 0.00002271
Iteration 96/1000 | Loss: 0.00002270
Iteration 97/1000 | Loss: 0.00002270
Iteration 98/1000 | Loss: 0.00002270
Iteration 99/1000 | Loss: 0.00002270
Iteration 100/1000 | Loss: 0.00002270
Iteration 101/1000 | Loss: 0.00002270
Iteration 102/1000 | Loss: 0.00002269
Iteration 103/1000 | Loss: 0.00002269
Iteration 104/1000 | Loss: 0.00002269
Iteration 105/1000 | Loss: 0.00002268
Iteration 106/1000 | Loss: 0.00002268
Iteration 107/1000 | Loss: 0.00002268
Iteration 108/1000 | Loss: 0.00002268
Iteration 109/1000 | Loss: 0.00002267
Iteration 110/1000 | Loss: 0.00002267
Iteration 111/1000 | Loss: 0.00002267
Iteration 112/1000 | Loss: 0.00002266
Iteration 113/1000 | Loss: 0.00002266
Iteration 114/1000 | Loss: 0.00002266
Iteration 115/1000 | Loss: 0.00002266
Iteration 116/1000 | Loss: 0.00002265
Iteration 117/1000 | Loss: 0.00002265
Iteration 118/1000 | Loss: 0.00002265
Iteration 119/1000 | Loss: 0.00002265
Iteration 120/1000 | Loss: 0.00002264
Iteration 121/1000 | Loss: 0.00002264
Iteration 122/1000 | Loss: 0.00002264
Iteration 123/1000 | Loss: 0.00002264
Iteration 124/1000 | Loss: 0.00002263
Iteration 125/1000 | Loss: 0.00002263
Iteration 126/1000 | Loss: 0.00002262
Iteration 127/1000 | Loss: 0.00002262
Iteration 128/1000 | Loss: 0.00002262
Iteration 129/1000 | Loss: 0.00002262
Iteration 130/1000 | Loss: 0.00002262
Iteration 131/1000 | Loss: 0.00002261
Iteration 132/1000 | Loss: 0.00002261
Iteration 133/1000 | Loss: 0.00002261
Iteration 134/1000 | Loss: 0.00002260
Iteration 135/1000 | Loss: 0.00002260
Iteration 136/1000 | Loss: 0.00002260
Iteration 137/1000 | Loss: 0.00002260
Iteration 138/1000 | Loss: 0.00002260
Iteration 139/1000 | Loss: 0.00002260
Iteration 140/1000 | Loss: 0.00002259
Iteration 141/1000 | Loss: 0.00002259
Iteration 142/1000 | Loss: 0.00002259
Iteration 143/1000 | Loss: 0.00002258
Iteration 144/1000 | Loss: 0.00002258
Iteration 145/1000 | Loss: 0.00002258
Iteration 146/1000 | Loss: 0.00002258
Iteration 147/1000 | Loss: 0.00002258
Iteration 148/1000 | Loss: 0.00002258
Iteration 149/1000 | Loss: 0.00002257
Iteration 150/1000 | Loss: 0.00002257
Iteration 151/1000 | Loss: 0.00002257
Iteration 152/1000 | Loss: 0.00002257
Iteration 153/1000 | Loss: 0.00002257
Iteration 154/1000 | Loss: 0.00002257
Iteration 155/1000 | Loss: 0.00002257
Iteration 156/1000 | Loss: 0.00002257
Iteration 157/1000 | Loss: 0.00002257
Iteration 158/1000 | Loss: 0.00002257
Iteration 159/1000 | Loss: 0.00002256
Iteration 160/1000 | Loss: 0.00002256
Iteration 161/1000 | Loss: 0.00002256
Iteration 162/1000 | Loss: 0.00002256
Iteration 163/1000 | Loss: 0.00002256
Iteration 164/1000 | Loss: 0.00002256
Iteration 165/1000 | Loss: 0.00002256
Iteration 166/1000 | Loss: 0.00002256
Iteration 167/1000 | Loss: 0.00002256
Iteration 168/1000 | Loss: 0.00002255
Iteration 169/1000 | Loss: 0.00002255
Iteration 170/1000 | Loss: 0.00002255
Iteration 171/1000 | Loss: 0.00002255
Iteration 172/1000 | Loss: 0.00002255
Iteration 173/1000 | Loss: 0.00002255
Iteration 174/1000 | Loss: 0.00002255
Iteration 175/1000 | Loss: 0.00002255
Iteration 176/1000 | Loss: 0.00002255
Iteration 177/1000 | Loss: 0.00002255
Iteration 178/1000 | Loss: 0.00002255
Iteration 179/1000 | Loss: 0.00002255
Iteration 180/1000 | Loss: 0.00002255
Iteration 181/1000 | Loss: 0.00002255
Iteration 182/1000 | Loss: 0.00002254
Iteration 183/1000 | Loss: 0.00002254
Iteration 184/1000 | Loss: 0.00002254
Iteration 185/1000 | Loss: 0.00002254
Iteration 186/1000 | Loss: 0.00002254
Iteration 187/1000 | Loss: 0.00002254
Iteration 188/1000 | Loss: 0.00002254
Iteration 189/1000 | Loss: 0.00002254
Iteration 190/1000 | Loss: 0.00002254
Iteration 191/1000 | Loss: 0.00002254
Iteration 192/1000 | Loss: 0.00002254
Iteration 193/1000 | Loss: 0.00002254
Iteration 194/1000 | Loss: 0.00002254
Iteration 195/1000 | Loss: 0.00002254
Iteration 196/1000 | Loss: 0.00002254
Iteration 197/1000 | Loss: 0.00002254
Iteration 198/1000 | Loss: 0.00002254
Iteration 199/1000 | Loss: 0.00002254
Iteration 200/1000 | Loss: 0.00002254
Iteration 201/1000 | Loss: 0.00002254
Iteration 202/1000 | Loss: 0.00002254
Iteration 203/1000 | Loss: 0.00002254
Iteration 204/1000 | Loss: 0.00002254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.2536640244652517e-05, 2.2536640244652517e-05, 2.2536640244652517e-05, 2.2536640244652517e-05, 2.2536640244652517e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2536640244652517e-05

Optimization complete. Final v2v error: 3.813062906265259 mm

Highest mean error: 6.17185640335083 mm for frame 120

Lowest mean error: 3.0154945850372314 mm for frame 10

Saving results

Total time: 67.42426609992981
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00712494
Iteration 2/25 | Loss: 0.00147250
Iteration 3/25 | Loss: 0.00135282
Iteration 4/25 | Loss: 0.00134391
Iteration 5/25 | Loss: 0.00134172
Iteration 6/25 | Loss: 0.00134172
Iteration 7/25 | Loss: 0.00134172
Iteration 8/25 | Loss: 0.00134172
Iteration 9/25 | Loss: 0.00134172
Iteration 10/25 | Loss: 0.00134172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013417185982689261, 0.0013417185982689261, 0.0013417185982689261, 0.0013417185982689261, 0.0013417185982689261]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013417185982689261

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.90531301
Iteration 2/25 | Loss: 0.00076222
Iteration 3/25 | Loss: 0.00076222
Iteration 4/25 | Loss: 0.00076222
Iteration 5/25 | Loss: 0.00076222
Iteration 6/25 | Loss: 0.00076221
Iteration 7/25 | Loss: 0.00076221
Iteration 8/25 | Loss: 0.00076221
Iteration 9/25 | Loss: 0.00076221
Iteration 10/25 | Loss: 0.00076221
Iteration 11/25 | Loss: 0.00076221
Iteration 12/25 | Loss: 0.00076221
Iteration 13/25 | Loss: 0.00076221
Iteration 14/25 | Loss: 0.00076221
Iteration 15/25 | Loss: 0.00076221
Iteration 16/25 | Loss: 0.00076221
Iteration 17/25 | Loss: 0.00076221
Iteration 18/25 | Loss: 0.00076221
Iteration 19/25 | Loss: 0.00076221
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007622139528393745, 0.0007622139528393745, 0.0007622139528393745, 0.0007622139528393745, 0.0007622139528393745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007622139528393745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076221
Iteration 2/1000 | Loss: 0.00004904
Iteration 3/1000 | Loss: 0.00003208
Iteration 4/1000 | Loss: 0.00002734
Iteration 5/1000 | Loss: 0.00002588
Iteration 6/1000 | Loss: 0.00002500
Iteration 7/1000 | Loss: 0.00002440
Iteration 8/1000 | Loss: 0.00002392
Iteration 9/1000 | Loss: 0.00002361
Iteration 10/1000 | Loss: 0.00002355
Iteration 11/1000 | Loss: 0.00002332
Iteration 12/1000 | Loss: 0.00002320
Iteration 13/1000 | Loss: 0.00002300
Iteration 14/1000 | Loss: 0.00002294
Iteration 15/1000 | Loss: 0.00002281
Iteration 16/1000 | Loss: 0.00002278
Iteration 17/1000 | Loss: 0.00002273
Iteration 18/1000 | Loss: 0.00002265
Iteration 19/1000 | Loss: 0.00002262
Iteration 20/1000 | Loss: 0.00002261
Iteration 21/1000 | Loss: 0.00002258
Iteration 22/1000 | Loss: 0.00002258
Iteration 23/1000 | Loss: 0.00002257
Iteration 24/1000 | Loss: 0.00002257
Iteration 25/1000 | Loss: 0.00002255
Iteration 26/1000 | Loss: 0.00002255
Iteration 27/1000 | Loss: 0.00002253
Iteration 28/1000 | Loss: 0.00002253
Iteration 29/1000 | Loss: 0.00002252
Iteration 30/1000 | Loss: 0.00002252
Iteration 31/1000 | Loss: 0.00002252
Iteration 32/1000 | Loss: 0.00002252
Iteration 33/1000 | Loss: 0.00002252
Iteration 34/1000 | Loss: 0.00002252
Iteration 35/1000 | Loss: 0.00002252
Iteration 36/1000 | Loss: 0.00002251
Iteration 37/1000 | Loss: 0.00002251
Iteration 38/1000 | Loss: 0.00002250
Iteration 39/1000 | Loss: 0.00002248
Iteration 40/1000 | Loss: 0.00002247
Iteration 41/1000 | Loss: 0.00002245
Iteration 42/1000 | Loss: 0.00002244
Iteration 43/1000 | Loss: 0.00002242
Iteration 44/1000 | Loss: 0.00002242
Iteration 45/1000 | Loss: 0.00002241
Iteration 46/1000 | Loss: 0.00002241
Iteration 47/1000 | Loss: 0.00002236
Iteration 48/1000 | Loss: 0.00002236
Iteration 49/1000 | Loss: 0.00002236
Iteration 50/1000 | Loss: 0.00002236
Iteration 51/1000 | Loss: 0.00002236
Iteration 52/1000 | Loss: 0.00002234
Iteration 53/1000 | Loss: 0.00002233
Iteration 54/1000 | Loss: 0.00002230
Iteration 55/1000 | Loss: 0.00002229
Iteration 56/1000 | Loss: 0.00002229
Iteration 57/1000 | Loss: 0.00002229
Iteration 58/1000 | Loss: 0.00002229
Iteration 59/1000 | Loss: 0.00002229
Iteration 60/1000 | Loss: 0.00002228
Iteration 61/1000 | Loss: 0.00002228
Iteration 62/1000 | Loss: 0.00002227
Iteration 63/1000 | Loss: 0.00002227
Iteration 64/1000 | Loss: 0.00002227
Iteration 65/1000 | Loss: 0.00002226
Iteration 66/1000 | Loss: 0.00002226
Iteration 67/1000 | Loss: 0.00002226
Iteration 68/1000 | Loss: 0.00002226
Iteration 69/1000 | Loss: 0.00002226
Iteration 70/1000 | Loss: 0.00002226
Iteration 71/1000 | Loss: 0.00002225
Iteration 72/1000 | Loss: 0.00002225
Iteration 73/1000 | Loss: 0.00002225
Iteration 74/1000 | Loss: 0.00002224
Iteration 75/1000 | Loss: 0.00002224
Iteration 76/1000 | Loss: 0.00002224
Iteration 77/1000 | Loss: 0.00002223
Iteration 78/1000 | Loss: 0.00002223
Iteration 79/1000 | Loss: 0.00002223
Iteration 80/1000 | Loss: 0.00002223
Iteration 81/1000 | Loss: 0.00002223
Iteration 82/1000 | Loss: 0.00002223
Iteration 83/1000 | Loss: 0.00002223
Iteration 84/1000 | Loss: 0.00002223
Iteration 85/1000 | Loss: 0.00002223
Iteration 86/1000 | Loss: 0.00002223
Iteration 87/1000 | Loss: 0.00002223
Iteration 88/1000 | Loss: 0.00002223
Iteration 89/1000 | Loss: 0.00002223
Iteration 90/1000 | Loss: 0.00002223
Iteration 91/1000 | Loss: 0.00002223
Iteration 92/1000 | Loss: 0.00002223
Iteration 93/1000 | Loss: 0.00002223
Iteration 94/1000 | Loss: 0.00002223
Iteration 95/1000 | Loss: 0.00002223
Iteration 96/1000 | Loss: 0.00002223
Iteration 97/1000 | Loss: 0.00002223
Iteration 98/1000 | Loss: 0.00002223
Iteration 99/1000 | Loss: 0.00002223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.2231050024856813e-05, 2.2231050024856813e-05, 2.2231050024856813e-05, 2.2231050024856813e-05, 2.2231050024856813e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2231050024856813e-05

Optimization complete. Final v2v error: 3.8827779293060303 mm

Highest mean error: 4.3690385818481445 mm for frame 198

Lowest mean error: 3.494896173477173 mm for frame 233

Saving results

Total time: 39.30705237388611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967914
Iteration 2/25 | Loss: 0.00398689
Iteration 3/25 | Loss: 0.00284688
Iteration 4/25 | Loss: 0.00249500
Iteration 5/25 | Loss: 0.00254906
Iteration 6/25 | Loss: 0.00240044
Iteration 7/25 | Loss: 0.00222291
Iteration 8/25 | Loss: 0.00205034
Iteration 9/25 | Loss: 0.00194666
Iteration 10/25 | Loss: 0.00189424
Iteration 11/25 | Loss: 0.00182369
Iteration 12/25 | Loss: 0.00180306
Iteration 13/25 | Loss: 0.00173429
Iteration 14/25 | Loss: 0.00171929
Iteration 15/25 | Loss: 0.00168562
Iteration 16/25 | Loss: 0.00174911
Iteration 17/25 | Loss: 0.00172245
Iteration 18/25 | Loss: 0.00169262
Iteration 19/25 | Loss: 0.00165254
Iteration 20/25 | Loss: 0.00164878
Iteration 21/25 | Loss: 0.00164485
Iteration 22/25 | Loss: 0.00164262
Iteration 23/25 | Loss: 0.00164183
Iteration 24/25 | Loss: 0.00164134
Iteration 25/25 | Loss: 0.00164511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.43637133
Iteration 2/25 | Loss: 0.00587729
Iteration 3/25 | Loss: 0.00401928
Iteration 4/25 | Loss: 0.00401924
Iteration 5/25 | Loss: 0.00401924
Iteration 6/25 | Loss: 0.00401924
Iteration 7/25 | Loss: 0.00401924
Iteration 8/25 | Loss: 0.00401924
Iteration 9/25 | Loss: 0.00401924
Iteration 10/25 | Loss: 0.00401924
Iteration 11/25 | Loss: 0.00401924
Iteration 12/25 | Loss: 0.00401924
Iteration 13/25 | Loss: 0.00401924
Iteration 14/25 | Loss: 0.00401924
Iteration 15/25 | Loss: 0.00401924
Iteration 16/25 | Loss: 0.00401924
Iteration 17/25 | Loss: 0.00401924
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004019238520413637, 0.004019238520413637, 0.004019238520413637, 0.004019238520413637, 0.004019238520413637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004019238520413637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00401924
Iteration 2/1000 | Loss: 0.00797471
Iteration 3/1000 | Loss: 0.00095237
Iteration 4/1000 | Loss: 0.00045434
Iteration 5/1000 | Loss: 0.00466234
Iteration 6/1000 | Loss: 0.01142605
Iteration 7/1000 | Loss: 0.00851999
Iteration 8/1000 | Loss: 0.00830772
Iteration 9/1000 | Loss: 0.00465006
Iteration 10/1000 | Loss: 0.00549818
Iteration 11/1000 | Loss: 0.00572691
Iteration 12/1000 | Loss: 0.00041420
Iteration 13/1000 | Loss: 0.00033223
Iteration 14/1000 | Loss: 0.00040565
Iteration 15/1000 | Loss: 0.00142459
Iteration 16/1000 | Loss: 0.00031900
Iteration 17/1000 | Loss: 0.00286501
Iteration 18/1000 | Loss: 0.00112580
Iteration 19/1000 | Loss: 0.00107322
Iteration 20/1000 | Loss: 0.00077812
Iteration 21/1000 | Loss: 0.00276177
Iteration 22/1000 | Loss: 0.00074422
Iteration 23/1000 | Loss: 0.00096709
Iteration 24/1000 | Loss: 0.00029612
Iteration 25/1000 | Loss: 0.00014950
Iteration 26/1000 | Loss: 0.00015157
Iteration 27/1000 | Loss: 0.00175326
Iteration 28/1000 | Loss: 0.00877500
Iteration 29/1000 | Loss: 0.00309871
Iteration 30/1000 | Loss: 0.00449670
Iteration 31/1000 | Loss: 0.00171645
Iteration 32/1000 | Loss: 0.00150184
Iteration 33/1000 | Loss: 0.00146760
Iteration 34/1000 | Loss: 0.00101502
Iteration 35/1000 | Loss: 0.00096000
Iteration 36/1000 | Loss: 0.00068347
Iteration 37/1000 | Loss: 0.00036798
Iteration 38/1000 | Loss: 0.00012674
Iteration 39/1000 | Loss: 0.00072909
Iteration 40/1000 | Loss: 0.00071138
Iteration 41/1000 | Loss: 0.00094269
Iteration 42/1000 | Loss: 0.00123569
Iteration 43/1000 | Loss: 0.00082401
Iteration 44/1000 | Loss: 0.00008366
Iteration 45/1000 | Loss: 0.00026248
Iteration 46/1000 | Loss: 0.00060814
Iteration 47/1000 | Loss: 0.00008601
Iteration 48/1000 | Loss: 0.00017002
Iteration 49/1000 | Loss: 0.00015113
Iteration 50/1000 | Loss: 0.00014934
Iteration 51/1000 | Loss: 0.00014316
Iteration 52/1000 | Loss: 0.00020994
Iteration 53/1000 | Loss: 0.00004446
Iteration 54/1000 | Loss: 0.00003971
Iteration 55/1000 | Loss: 0.00036581
Iteration 56/1000 | Loss: 0.00003692
Iteration 57/1000 | Loss: 0.00003445
Iteration 58/1000 | Loss: 0.00028374
Iteration 59/1000 | Loss: 0.00003770
Iteration 60/1000 | Loss: 0.00003385
Iteration 61/1000 | Loss: 0.00039117
Iteration 62/1000 | Loss: 0.00030881
Iteration 63/1000 | Loss: 0.00053972
Iteration 64/1000 | Loss: 0.00062861
Iteration 65/1000 | Loss: 0.00035696
Iteration 66/1000 | Loss: 0.00038859
Iteration 67/1000 | Loss: 0.00060714
Iteration 68/1000 | Loss: 0.00015911
Iteration 69/1000 | Loss: 0.00003252
Iteration 70/1000 | Loss: 0.00028492
Iteration 71/1000 | Loss: 0.00002936
Iteration 72/1000 | Loss: 0.00003241
Iteration 73/1000 | Loss: 0.00023264
Iteration 74/1000 | Loss: 0.00003566
Iteration 75/1000 | Loss: 0.00015499
Iteration 76/1000 | Loss: 0.00005554
Iteration 77/1000 | Loss: 0.00002884
Iteration 78/1000 | Loss: 0.00002814
Iteration 79/1000 | Loss: 0.00002669
Iteration 80/1000 | Loss: 0.00002767
Iteration 81/1000 | Loss: 0.00002691
Iteration 82/1000 | Loss: 0.00002727
Iteration 83/1000 | Loss: 0.00002705
Iteration 84/1000 | Loss: 0.00002772
Iteration 85/1000 | Loss: 0.00002733
Iteration 86/1000 | Loss: 0.00002786
Iteration 87/1000 | Loss: 0.00002712
Iteration 88/1000 | Loss: 0.00038130
Iteration 89/1000 | Loss: 0.00014849
Iteration 90/1000 | Loss: 0.00014685
Iteration 91/1000 | Loss: 0.00015971
Iteration 92/1000 | Loss: 0.00003140
Iteration 93/1000 | Loss: 0.00002846
Iteration 94/1000 | Loss: 0.00002696
Iteration 95/1000 | Loss: 0.00030705
Iteration 96/1000 | Loss: 0.00002576
Iteration 97/1000 | Loss: 0.00002495
Iteration 98/1000 | Loss: 0.00002471
Iteration 99/1000 | Loss: 0.00007070
Iteration 100/1000 | Loss: 0.00002996
Iteration 101/1000 | Loss: 0.00002373
Iteration 102/1000 | Loss: 0.00002655
Iteration 103/1000 | Loss: 0.00002708
Iteration 104/1000 | Loss: 0.00019735
Iteration 105/1000 | Loss: 0.00002367
Iteration 106/1000 | Loss: 0.00002230
Iteration 107/1000 | Loss: 0.00002209
Iteration 108/1000 | Loss: 0.00002209
Iteration 109/1000 | Loss: 0.00002190
Iteration 110/1000 | Loss: 0.00002187
Iteration 111/1000 | Loss: 0.00002180
Iteration 112/1000 | Loss: 0.00002174
Iteration 113/1000 | Loss: 0.00002173
Iteration 114/1000 | Loss: 0.00002173
Iteration 115/1000 | Loss: 0.00002172
Iteration 116/1000 | Loss: 0.00026394
Iteration 117/1000 | Loss: 0.00002234
Iteration 118/1000 | Loss: 0.00002168
Iteration 119/1000 | Loss: 0.00002156
Iteration 120/1000 | Loss: 0.00002156
Iteration 121/1000 | Loss: 0.00002155
Iteration 122/1000 | Loss: 0.00002154
Iteration 123/1000 | Loss: 0.00002154
Iteration 124/1000 | Loss: 0.00002154
Iteration 125/1000 | Loss: 0.00002154
Iteration 126/1000 | Loss: 0.00002154
Iteration 127/1000 | Loss: 0.00002154
Iteration 128/1000 | Loss: 0.00002154
Iteration 129/1000 | Loss: 0.00002154
Iteration 130/1000 | Loss: 0.00002154
Iteration 131/1000 | Loss: 0.00002154
Iteration 132/1000 | Loss: 0.00002154
Iteration 133/1000 | Loss: 0.00002154
Iteration 134/1000 | Loss: 0.00002153
Iteration 135/1000 | Loss: 0.00002153
Iteration 136/1000 | Loss: 0.00002153
Iteration 137/1000 | Loss: 0.00002153
Iteration 138/1000 | Loss: 0.00002152
Iteration 139/1000 | Loss: 0.00002152
Iteration 140/1000 | Loss: 0.00002152
Iteration 141/1000 | Loss: 0.00002151
Iteration 142/1000 | Loss: 0.00002151
Iteration 143/1000 | Loss: 0.00002151
Iteration 144/1000 | Loss: 0.00002150
Iteration 145/1000 | Loss: 0.00002150
Iteration 146/1000 | Loss: 0.00002150
Iteration 147/1000 | Loss: 0.00002150
Iteration 148/1000 | Loss: 0.00002150
Iteration 149/1000 | Loss: 0.00002149
Iteration 150/1000 | Loss: 0.00002149
Iteration 151/1000 | Loss: 0.00002160
Iteration 152/1000 | Loss: 0.00002160
Iteration 153/1000 | Loss: 0.00002169
Iteration 154/1000 | Loss: 0.00002169
Iteration 155/1000 | Loss: 0.00002156
Iteration 156/1000 | Loss: 0.00002151
Iteration 157/1000 | Loss: 0.00002150
Iteration 158/1000 | Loss: 0.00002150
Iteration 159/1000 | Loss: 0.00002150
Iteration 160/1000 | Loss: 0.00002149
Iteration 161/1000 | Loss: 0.00002149
Iteration 162/1000 | Loss: 0.00002148
Iteration 163/1000 | Loss: 0.00002148
Iteration 164/1000 | Loss: 0.00002148
Iteration 165/1000 | Loss: 0.00002148
Iteration 166/1000 | Loss: 0.00002148
Iteration 167/1000 | Loss: 0.00002148
Iteration 168/1000 | Loss: 0.00002148
Iteration 169/1000 | Loss: 0.00002148
Iteration 170/1000 | Loss: 0.00002148
Iteration 171/1000 | Loss: 0.00002148
Iteration 172/1000 | Loss: 0.00002148
Iteration 173/1000 | Loss: 0.00002148
Iteration 174/1000 | Loss: 0.00002147
Iteration 175/1000 | Loss: 0.00002147
Iteration 176/1000 | Loss: 0.00002147
Iteration 177/1000 | Loss: 0.00002147
Iteration 178/1000 | Loss: 0.00002147
Iteration 179/1000 | Loss: 0.00002147
Iteration 180/1000 | Loss: 0.00002147
Iteration 181/1000 | Loss: 0.00002147
Iteration 182/1000 | Loss: 0.00002147
Iteration 183/1000 | Loss: 0.00002147
Iteration 184/1000 | Loss: 0.00002147
Iteration 185/1000 | Loss: 0.00002147
Iteration 186/1000 | Loss: 0.00002147
Iteration 187/1000 | Loss: 0.00002147
Iteration 188/1000 | Loss: 0.00002147
Iteration 189/1000 | Loss: 0.00002147
Iteration 190/1000 | Loss: 0.00002147
Iteration 191/1000 | Loss: 0.00002147
Iteration 192/1000 | Loss: 0.00002147
Iteration 193/1000 | Loss: 0.00002147
Iteration 194/1000 | Loss: 0.00002147
Iteration 195/1000 | Loss: 0.00002147
Iteration 196/1000 | Loss: 0.00002147
Iteration 197/1000 | Loss: 0.00002147
Iteration 198/1000 | Loss: 0.00002147
Iteration 199/1000 | Loss: 0.00002147
Iteration 200/1000 | Loss: 0.00002147
Iteration 201/1000 | Loss: 0.00002147
Iteration 202/1000 | Loss: 0.00002147
Iteration 203/1000 | Loss: 0.00002147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [2.1465462850756012e-05, 2.1465462850756012e-05, 2.1465462850756012e-05, 2.1465462850756012e-05, 2.1465462850756012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1465462850756012e-05

Optimization complete. Final v2v error: 3.7258553504943848 mm

Highest mean error: 5.163258075714111 mm for frame 44

Lowest mean error: 2.9871864318847656 mm for frame 156

Saving results

Total time: 217.77318143844604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403882
Iteration 2/25 | Loss: 0.00127845
Iteration 3/25 | Loss: 0.00119750
Iteration 4/25 | Loss: 0.00119180
Iteration 5/25 | Loss: 0.00119052
Iteration 6/25 | Loss: 0.00119052
Iteration 7/25 | Loss: 0.00119052
Iteration 8/25 | Loss: 0.00119052
Iteration 9/25 | Loss: 0.00119052
Iteration 10/25 | Loss: 0.00119052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011905175633728504, 0.0011905175633728504, 0.0011905175633728504, 0.0011905175633728504, 0.0011905175633728504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011905175633728504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44791937
Iteration 2/25 | Loss: 0.00052704
Iteration 3/25 | Loss: 0.00052704
Iteration 4/25 | Loss: 0.00052704
Iteration 5/25 | Loss: 0.00052704
Iteration 6/25 | Loss: 0.00052703
Iteration 7/25 | Loss: 0.00052703
Iteration 8/25 | Loss: 0.00052703
Iteration 9/25 | Loss: 0.00052703
Iteration 10/25 | Loss: 0.00052703
Iteration 11/25 | Loss: 0.00052703
Iteration 12/25 | Loss: 0.00052703
Iteration 13/25 | Loss: 0.00052703
Iteration 14/25 | Loss: 0.00052703
Iteration 15/25 | Loss: 0.00052703
Iteration 16/25 | Loss: 0.00052703
Iteration 17/25 | Loss: 0.00052703
Iteration 18/25 | Loss: 0.00052703
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005270328838378191, 0.0005270328838378191, 0.0005270328838378191, 0.0005270328838378191, 0.0005270328838378191]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005270328838378191

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052703
Iteration 2/1000 | Loss: 0.00003677
Iteration 3/1000 | Loss: 0.00002478
Iteration 4/1000 | Loss: 0.00002217
Iteration 5/1000 | Loss: 0.00002081
Iteration 6/1000 | Loss: 0.00001970
Iteration 7/1000 | Loss: 0.00001900
Iteration 8/1000 | Loss: 0.00001862
Iteration 9/1000 | Loss: 0.00001830
Iteration 10/1000 | Loss: 0.00001794
Iteration 11/1000 | Loss: 0.00001774
Iteration 12/1000 | Loss: 0.00001754
Iteration 13/1000 | Loss: 0.00001737
Iteration 14/1000 | Loss: 0.00001736
Iteration 15/1000 | Loss: 0.00001731
Iteration 16/1000 | Loss: 0.00001730
Iteration 17/1000 | Loss: 0.00001726
Iteration 18/1000 | Loss: 0.00001724
Iteration 19/1000 | Loss: 0.00001723
Iteration 20/1000 | Loss: 0.00001723
Iteration 21/1000 | Loss: 0.00001720
Iteration 22/1000 | Loss: 0.00001719
Iteration 23/1000 | Loss: 0.00001718
Iteration 24/1000 | Loss: 0.00001717
Iteration 25/1000 | Loss: 0.00001712
Iteration 26/1000 | Loss: 0.00001712
Iteration 27/1000 | Loss: 0.00001711
Iteration 28/1000 | Loss: 0.00001710
Iteration 29/1000 | Loss: 0.00001710
Iteration 30/1000 | Loss: 0.00001704
Iteration 31/1000 | Loss: 0.00001696
Iteration 32/1000 | Loss: 0.00001696
Iteration 33/1000 | Loss: 0.00001694
Iteration 34/1000 | Loss: 0.00001693
Iteration 35/1000 | Loss: 0.00001692
Iteration 36/1000 | Loss: 0.00001691
Iteration 37/1000 | Loss: 0.00001690
Iteration 38/1000 | Loss: 0.00001690
Iteration 39/1000 | Loss: 0.00001689
Iteration 40/1000 | Loss: 0.00001689
Iteration 41/1000 | Loss: 0.00001688
Iteration 42/1000 | Loss: 0.00001687
Iteration 43/1000 | Loss: 0.00001687
Iteration 44/1000 | Loss: 0.00001686
Iteration 45/1000 | Loss: 0.00001686
Iteration 46/1000 | Loss: 0.00001686
Iteration 47/1000 | Loss: 0.00001685
Iteration 48/1000 | Loss: 0.00001685
Iteration 49/1000 | Loss: 0.00001685
Iteration 50/1000 | Loss: 0.00001685
Iteration 51/1000 | Loss: 0.00001684
Iteration 52/1000 | Loss: 0.00001684
Iteration 53/1000 | Loss: 0.00001684
Iteration 54/1000 | Loss: 0.00001684
Iteration 55/1000 | Loss: 0.00001684
Iteration 56/1000 | Loss: 0.00001683
Iteration 57/1000 | Loss: 0.00001683
Iteration 58/1000 | Loss: 0.00001683
Iteration 59/1000 | Loss: 0.00001683
Iteration 60/1000 | Loss: 0.00001683
Iteration 61/1000 | Loss: 0.00001683
Iteration 62/1000 | Loss: 0.00001683
Iteration 63/1000 | Loss: 0.00001682
Iteration 64/1000 | Loss: 0.00001682
Iteration 65/1000 | Loss: 0.00001682
Iteration 66/1000 | Loss: 0.00001682
Iteration 67/1000 | Loss: 0.00001682
Iteration 68/1000 | Loss: 0.00001681
Iteration 69/1000 | Loss: 0.00001681
Iteration 70/1000 | Loss: 0.00001681
Iteration 71/1000 | Loss: 0.00001681
Iteration 72/1000 | Loss: 0.00001680
Iteration 73/1000 | Loss: 0.00001680
Iteration 74/1000 | Loss: 0.00001680
Iteration 75/1000 | Loss: 0.00001680
Iteration 76/1000 | Loss: 0.00001680
Iteration 77/1000 | Loss: 0.00001680
Iteration 78/1000 | Loss: 0.00001680
Iteration 79/1000 | Loss: 0.00001680
Iteration 80/1000 | Loss: 0.00001679
Iteration 81/1000 | Loss: 0.00001679
Iteration 82/1000 | Loss: 0.00001679
Iteration 83/1000 | Loss: 0.00001678
Iteration 84/1000 | Loss: 0.00001678
Iteration 85/1000 | Loss: 0.00001678
Iteration 86/1000 | Loss: 0.00001678
Iteration 87/1000 | Loss: 0.00001678
Iteration 88/1000 | Loss: 0.00001677
Iteration 89/1000 | Loss: 0.00001677
Iteration 90/1000 | Loss: 0.00001676
Iteration 91/1000 | Loss: 0.00001676
Iteration 92/1000 | Loss: 0.00001676
Iteration 93/1000 | Loss: 0.00001676
Iteration 94/1000 | Loss: 0.00001676
Iteration 95/1000 | Loss: 0.00001676
Iteration 96/1000 | Loss: 0.00001676
Iteration 97/1000 | Loss: 0.00001676
Iteration 98/1000 | Loss: 0.00001675
Iteration 99/1000 | Loss: 0.00001675
Iteration 100/1000 | Loss: 0.00001675
Iteration 101/1000 | Loss: 0.00001674
Iteration 102/1000 | Loss: 0.00001674
Iteration 103/1000 | Loss: 0.00001674
Iteration 104/1000 | Loss: 0.00001674
Iteration 105/1000 | Loss: 0.00001674
Iteration 106/1000 | Loss: 0.00001674
Iteration 107/1000 | Loss: 0.00001673
Iteration 108/1000 | Loss: 0.00001673
Iteration 109/1000 | Loss: 0.00001672
Iteration 110/1000 | Loss: 0.00001672
Iteration 111/1000 | Loss: 0.00001672
Iteration 112/1000 | Loss: 0.00001671
Iteration 113/1000 | Loss: 0.00001671
Iteration 114/1000 | Loss: 0.00001671
Iteration 115/1000 | Loss: 0.00001671
Iteration 116/1000 | Loss: 0.00001670
Iteration 117/1000 | Loss: 0.00001670
Iteration 118/1000 | Loss: 0.00001670
Iteration 119/1000 | Loss: 0.00001669
Iteration 120/1000 | Loss: 0.00001669
Iteration 121/1000 | Loss: 0.00001669
Iteration 122/1000 | Loss: 0.00001669
Iteration 123/1000 | Loss: 0.00001669
Iteration 124/1000 | Loss: 0.00001669
Iteration 125/1000 | Loss: 0.00001669
Iteration 126/1000 | Loss: 0.00001669
Iteration 127/1000 | Loss: 0.00001669
Iteration 128/1000 | Loss: 0.00001669
Iteration 129/1000 | Loss: 0.00001669
Iteration 130/1000 | Loss: 0.00001669
Iteration 131/1000 | Loss: 0.00001669
Iteration 132/1000 | Loss: 0.00001668
Iteration 133/1000 | Loss: 0.00001668
Iteration 134/1000 | Loss: 0.00001668
Iteration 135/1000 | Loss: 0.00001668
Iteration 136/1000 | Loss: 0.00001668
Iteration 137/1000 | Loss: 0.00001668
Iteration 138/1000 | Loss: 0.00001668
Iteration 139/1000 | Loss: 0.00001668
Iteration 140/1000 | Loss: 0.00001668
Iteration 141/1000 | Loss: 0.00001668
Iteration 142/1000 | Loss: 0.00001668
Iteration 143/1000 | Loss: 0.00001668
Iteration 144/1000 | Loss: 0.00001667
Iteration 145/1000 | Loss: 0.00001667
Iteration 146/1000 | Loss: 0.00001667
Iteration 147/1000 | Loss: 0.00001667
Iteration 148/1000 | Loss: 0.00001667
Iteration 149/1000 | Loss: 0.00001666
Iteration 150/1000 | Loss: 0.00001666
Iteration 151/1000 | Loss: 0.00001666
Iteration 152/1000 | Loss: 0.00001666
Iteration 153/1000 | Loss: 0.00001665
Iteration 154/1000 | Loss: 0.00001665
Iteration 155/1000 | Loss: 0.00001665
Iteration 156/1000 | Loss: 0.00001665
Iteration 157/1000 | Loss: 0.00001665
Iteration 158/1000 | Loss: 0.00001665
Iteration 159/1000 | Loss: 0.00001665
Iteration 160/1000 | Loss: 0.00001665
Iteration 161/1000 | Loss: 0.00001665
Iteration 162/1000 | Loss: 0.00001665
Iteration 163/1000 | Loss: 0.00001665
Iteration 164/1000 | Loss: 0.00001664
Iteration 165/1000 | Loss: 0.00001664
Iteration 166/1000 | Loss: 0.00001664
Iteration 167/1000 | Loss: 0.00001664
Iteration 168/1000 | Loss: 0.00001664
Iteration 169/1000 | Loss: 0.00001664
Iteration 170/1000 | Loss: 0.00001664
Iteration 171/1000 | Loss: 0.00001664
Iteration 172/1000 | Loss: 0.00001664
Iteration 173/1000 | Loss: 0.00001664
Iteration 174/1000 | Loss: 0.00001664
Iteration 175/1000 | Loss: 0.00001664
Iteration 176/1000 | Loss: 0.00001664
Iteration 177/1000 | Loss: 0.00001664
Iteration 178/1000 | Loss: 0.00001664
Iteration 179/1000 | Loss: 0.00001664
Iteration 180/1000 | Loss: 0.00001664
Iteration 181/1000 | Loss: 0.00001664
Iteration 182/1000 | Loss: 0.00001664
Iteration 183/1000 | Loss: 0.00001664
Iteration 184/1000 | Loss: 0.00001664
Iteration 185/1000 | Loss: 0.00001664
Iteration 186/1000 | Loss: 0.00001664
Iteration 187/1000 | Loss: 0.00001664
Iteration 188/1000 | Loss: 0.00001664
Iteration 189/1000 | Loss: 0.00001664
Iteration 190/1000 | Loss: 0.00001664
Iteration 191/1000 | Loss: 0.00001664
Iteration 192/1000 | Loss: 0.00001664
Iteration 193/1000 | Loss: 0.00001664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.6643349226797e-05, 1.6643349226797e-05, 1.6643349226797e-05, 1.6643349226797e-05, 1.6643349226797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6643349226797e-05

Optimization complete. Final v2v error: 3.4076108932495117 mm

Highest mean error: 3.898287773132324 mm for frame 100

Lowest mean error: 2.9872002601623535 mm for frame 14

Saving results

Total time: 42.22242212295532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476615
Iteration 2/25 | Loss: 0.00131808
Iteration 3/25 | Loss: 0.00125707
Iteration 4/25 | Loss: 0.00124508
Iteration 5/25 | Loss: 0.00124119
Iteration 6/25 | Loss: 0.00124119
Iteration 7/25 | Loss: 0.00124119
Iteration 8/25 | Loss: 0.00124119
Iteration 9/25 | Loss: 0.00124119
Iteration 10/25 | Loss: 0.00124119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00124118581879884, 0.00124118581879884, 0.00124118581879884, 0.00124118581879884, 0.00124118581879884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00124118581879884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44590497
Iteration 2/25 | Loss: 0.00063071
Iteration 3/25 | Loss: 0.00063069
Iteration 4/25 | Loss: 0.00063069
Iteration 5/25 | Loss: 0.00063069
Iteration 6/25 | Loss: 0.00063069
Iteration 7/25 | Loss: 0.00063069
Iteration 8/25 | Loss: 0.00063069
Iteration 9/25 | Loss: 0.00063069
Iteration 10/25 | Loss: 0.00063069
Iteration 11/25 | Loss: 0.00063069
Iteration 12/25 | Loss: 0.00063069
Iteration 13/25 | Loss: 0.00063069
Iteration 14/25 | Loss: 0.00063069
Iteration 15/25 | Loss: 0.00063069
Iteration 16/25 | Loss: 0.00063069
Iteration 17/25 | Loss: 0.00063069
Iteration 18/25 | Loss: 0.00063069
Iteration 19/25 | Loss: 0.00063069
Iteration 20/25 | Loss: 0.00063069
Iteration 21/25 | Loss: 0.00063069
Iteration 22/25 | Loss: 0.00063069
Iteration 23/25 | Loss: 0.00063069
Iteration 24/25 | Loss: 0.00063069
Iteration 25/25 | Loss: 0.00063069

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063069
Iteration 2/1000 | Loss: 0.00003212
Iteration 3/1000 | Loss: 0.00002212
Iteration 4/1000 | Loss: 0.00002000
Iteration 5/1000 | Loss: 0.00001913
Iteration 6/1000 | Loss: 0.00001847
Iteration 7/1000 | Loss: 0.00001819
Iteration 8/1000 | Loss: 0.00001798
Iteration 9/1000 | Loss: 0.00001773
Iteration 10/1000 | Loss: 0.00001755
Iteration 11/1000 | Loss: 0.00001747
Iteration 12/1000 | Loss: 0.00001746
Iteration 13/1000 | Loss: 0.00001745
Iteration 14/1000 | Loss: 0.00001743
Iteration 15/1000 | Loss: 0.00001742
Iteration 16/1000 | Loss: 0.00001741
Iteration 17/1000 | Loss: 0.00001741
Iteration 18/1000 | Loss: 0.00001740
Iteration 19/1000 | Loss: 0.00001739
Iteration 20/1000 | Loss: 0.00001736
Iteration 21/1000 | Loss: 0.00001735
Iteration 22/1000 | Loss: 0.00001734
Iteration 23/1000 | Loss: 0.00001733
Iteration 24/1000 | Loss: 0.00001733
Iteration 25/1000 | Loss: 0.00001730
Iteration 26/1000 | Loss: 0.00001728
Iteration 27/1000 | Loss: 0.00001724
Iteration 28/1000 | Loss: 0.00001719
Iteration 29/1000 | Loss: 0.00001717
Iteration 30/1000 | Loss: 0.00001713
Iteration 31/1000 | Loss: 0.00001711
Iteration 32/1000 | Loss: 0.00001710
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001709
Iteration 35/1000 | Loss: 0.00001707
Iteration 36/1000 | Loss: 0.00001707
Iteration 37/1000 | Loss: 0.00001707
Iteration 38/1000 | Loss: 0.00001706
Iteration 39/1000 | Loss: 0.00001706
Iteration 40/1000 | Loss: 0.00001706
Iteration 41/1000 | Loss: 0.00001705
Iteration 42/1000 | Loss: 0.00001705
Iteration 43/1000 | Loss: 0.00001704
Iteration 44/1000 | Loss: 0.00001703
Iteration 45/1000 | Loss: 0.00001703
Iteration 46/1000 | Loss: 0.00001702
Iteration 47/1000 | Loss: 0.00001701
Iteration 48/1000 | Loss: 0.00001701
Iteration 49/1000 | Loss: 0.00001697
Iteration 50/1000 | Loss: 0.00001696
Iteration 51/1000 | Loss: 0.00001696
Iteration 52/1000 | Loss: 0.00001694
Iteration 53/1000 | Loss: 0.00001693
Iteration 54/1000 | Loss: 0.00001693
Iteration 55/1000 | Loss: 0.00001692
Iteration 56/1000 | Loss: 0.00001692
Iteration 57/1000 | Loss: 0.00001691
Iteration 58/1000 | Loss: 0.00001690
Iteration 59/1000 | Loss: 0.00001689
Iteration 60/1000 | Loss: 0.00001689
Iteration 61/1000 | Loss: 0.00001689
Iteration 62/1000 | Loss: 0.00001688
Iteration 63/1000 | Loss: 0.00001688
Iteration 64/1000 | Loss: 0.00001688
Iteration 65/1000 | Loss: 0.00001687
Iteration 66/1000 | Loss: 0.00001687
Iteration 67/1000 | Loss: 0.00001686
Iteration 68/1000 | Loss: 0.00001686
Iteration 69/1000 | Loss: 0.00001685
Iteration 70/1000 | Loss: 0.00001685
Iteration 71/1000 | Loss: 0.00001685
Iteration 72/1000 | Loss: 0.00001684
Iteration 73/1000 | Loss: 0.00001684
Iteration 74/1000 | Loss: 0.00001684
Iteration 75/1000 | Loss: 0.00001684
Iteration 76/1000 | Loss: 0.00001684
Iteration 77/1000 | Loss: 0.00001683
Iteration 78/1000 | Loss: 0.00001683
Iteration 79/1000 | Loss: 0.00001682
Iteration 80/1000 | Loss: 0.00001681
Iteration 81/1000 | Loss: 0.00001680
Iteration 82/1000 | Loss: 0.00001680
Iteration 83/1000 | Loss: 0.00001680
Iteration 84/1000 | Loss: 0.00001680
Iteration 85/1000 | Loss: 0.00001680
Iteration 86/1000 | Loss: 0.00001680
Iteration 87/1000 | Loss: 0.00001680
Iteration 88/1000 | Loss: 0.00001680
Iteration 89/1000 | Loss: 0.00001680
Iteration 90/1000 | Loss: 0.00001680
Iteration 91/1000 | Loss: 0.00001679
Iteration 92/1000 | Loss: 0.00001679
Iteration 93/1000 | Loss: 0.00001679
Iteration 94/1000 | Loss: 0.00001679
Iteration 95/1000 | Loss: 0.00001679
Iteration 96/1000 | Loss: 0.00001678
Iteration 97/1000 | Loss: 0.00001678
Iteration 98/1000 | Loss: 0.00001678
Iteration 99/1000 | Loss: 0.00001677
Iteration 100/1000 | Loss: 0.00001677
Iteration 101/1000 | Loss: 0.00001677
Iteration 102/1000 | Loss: 0.00001677
Iteration 103/1000 | Loss: 0.00001677
Iteration 104/1000 | Loss: 0.00001677
Iteration 105/1000 | Loss: 0.00001677
Iteration 106/1000 | Loss: 0.00001676
Iteration 107/1000 | Loss: 0.00001676
Iteration 108/1000 | Loss: 0.00001676
Iteration 109/1000 | Loss: 0.00001676
Iteration 110/1000 | Loss: 0.00001676
Iteration 111/1000 | Loss: 0.00001676
Iteration 112/1000 | Loss: 0.00001676
Iteration 113/1000 | Loss: 0.00001676
Iteration 114/1000 | Loss: 0.00001676
Iteration 115/1000 | Loss: 0.00001675
Iteration 116/1000 | Loss: 0.00001675
Iteration 117/1000 | Loss: 0.00001675
Iteration 118/1000 | Loss: 0.00001675
Iteration 119/1000 | Loss: 0.00001674
Iteration 120/1000 | Loss: 0.00001674
Iteration 121/1000 | Loss: 0.00001674
Iteration 122/1000 | Loss: 0.00001674
Iteration 123/1000 | Loss: 0.00001674
Iteration 124/1000 | Loss: 0.00001673
Iteration 125/1000 | Loss: 0.00001673
Iteration 126/1000 | Loss: 0.00001673
Iteration 127/1000 | Loss: 0.00001673
Iteration 128/1000 | Loss: 0.00001672
Iteration 129/1000 | Loss: 0.00001672
Iteration 130/1000 | Loss: 0.00001672
Iteration 131/1000 | Loss: 0.00001672
Iteration 132/1000 | Loss: 0.00001672
Iteration 133/1000 | Loss: 0.00001671
Iteration 134/1000 | Loss: 0.00001671
Iteration 135/1000 | Loss: 0.00001671
Iteration 136/1000 | Loss: 0.00001671
Iteration 137/1000 | Loss: 0.00001671
Iteration 138/1000 | Loss: 0.00001671
Iteration 139/1000 | Loss: 0.00001671
Iteration 140/1000 | Loss: 0.00001671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.671450627327431e-05, 1.671450627327431e-05, 1.671450627327431e-05, 1.671450627327431e-05, 1.671450627327431e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.671450627327431e-05

Optimization complete. Final v2v error: 3.376493215560913 mm

Highest mean error: 3.712554693222046 mm for frame 177

Lowest mean error: 3.1482574939727783 mm for frame 223

Saving results

Total time: 40.631311655044556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972320
Iteration 2/25 | Loss: 0.00972320
Iteration 3/25 | Loss: 0.00249744
Iteration 4/25 | Loss: 0.00183860
Iteration 5/25 | Loss: 0.00172566
Iteration 6/25 | Loss: 0.00157331
Iteration 7/25 | Loss: 0.00147544
Iteration 8/25 | Loss: 0.00143301
Iteration 9/25 | Loss: 0.00142555
Iteration 10/25 | Loss: 0.00140416
Iteration 11/25 | Loss: 0.00137170
Iteration 12/25 | Loss: 0.00133607
Iteration 13/25 | Loss: 0.00130579
Iteration 14/25 | Loss: 0.00129888
Iteration 15/25 | Loss: 0.00128288
Iteration 16/25 | Loss: 0.00128286
Iteration 17/25 | Loss: 0.00126142
Iteration 18/25 | Loss: 0.00125933
Iteration 19/25 | Loss: 0.00126441
Iteration 20/25 | Loss: 0.00125763
Iteration 21/25 | Loss: 0.00125334
Iteration 22/25 | Loss: 0.00125987
Iteration 23/25 | Loss: 0.00125217
Iteration 24/25 | Loss: 0.00124335
Iteration 25/25 | Loss: 0.00123967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46848273
Iteration 2/25 | Loss: 0.00097691
Iteration 3/25 | Loss: 0.00061932
Iteration 4/25 | Loss: 0.00061860
Iteration 5/25 | Loss: 0.00061860
Iteration 6/25 | Loss: 0.00061860
Iteration 7/25 | Loss: 0.00061860
Iteration 8/25 | Loss: 0.00061860
Iteration 9/25 | Loss: 0.00061860
Iteration 10/25 | Loss: 0.00061860
Iteration 11/25 | Loss: 0.00061860
Iteration 12/25 | Loss: 0.00061860
Iteration 13/25 | Loss: 0.00061860
Iteration 14/25 | Loss: 0.00061860
Iteration 15/25 | Loss: 0.00061860
Iteration 16/25 | Loss: 0.00061860
Iteration 17/25 | Loss: 0.00061860
Iteration 18/25 | Loss: 0.00061860
Iteration 19/25 | Loss: 0.00061860
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006186000537127256, 0.0006186000537127256, 0.0006186000537127256, 0.0006186000537127256, 0.0006186000537127256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006186000537127256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061860
Iteration 2/1000 | Loss: 0.00061915
Iteration 3/1000 | Loss: 0.00275687
Iteration 4/1000 | Loss: 0.00007338
Iteration 5/1000 | Loss: 0.00004458
Iteration 6/1000 | Loss: 0.00006622
Iteration 7/1000 | Loss: 0.00014300
Iteration 8/1000 | Loss: 0.00004962
Iteration 9/1000 | Loss: 0.00004528
Iteration 10/1000 | Loss: 0.00004507
Iteration 11/1000 | Loss: 0.00003515
Iteration 12/1000 | Loss: 0.00005275
Iteration 13/1000 | Loss: 0.00005357
Iteration 14/1000 | Loss: 0.00003022
Iteration 15/1000 | Loss: 0.00003261
Iteration 16/1000 | Loss: 0.00002811
Iteration 17/1000 | Loss: 0.00002335
Iteration 18/1000 | Loss: 0.00003337
Iteration 19/1000 | Loss: 0.00006165
Iteration 20/1000 | Loss: 0.00002421
Iteration 21/1000 | Loss: 0.00002114
Iteration 22/1000 | Loss: 0.00002547
Iteration 23/1000 | Loss: 0.00002286
Iteration 24/1000 | Loss: 0.00003488
Iteration 25/1000 | Loss: 0.00002039
Iteration 26/1000 | Loss: 0.00002042
Iteration 27/1000 | Loss: 0.00002946
Iteration 28/1000 | Loss: 0.00003332
Iteration 29/1000 | Loss: 0.00001965
Iteration 30/1000 | Loss: 0.00001961
Iteration 31/1000 | Loss: 0.00002675
Iteration 32/1000 | Loss: 0.00026327
Iteration 33/1000 | Loss: 0.00007991
Iteration 34/1000 | Loss: 0.00003756
Iteration 35/1000 | Loss: 0.00020461
Iteration 36/1000 | Loss: 0.00004795
Iteration 37/1000 | Loss: 0.00008570
Iteration 38/1000 | Loss: 0.00002319
Iteration 39/1000 | Loss: 0.00003299
Iteration 40/1000 | Loss: 0.00001859
Iteration 41/1000 | Loss: 0.00004412
Iteration 42/1000 | Loss: 0.00001846
Iteration 43/1000 | Loss: 0.00001816
Iteration 44/1000 | Loss: 0.00002328
Iteration 45/1000 | Loss: 0.00001774
Iteration 46/1000 | Loss: 0.00004117
Iteration 47/1000 | Loss: 0.00001765
Iteration 48/1000 | Loss: 0.00002501
Iteration 49/1000 | Loss: 0.00001775
Iteration 50/1000 | Loss: 0.00003581
Iteration 51/1000 | Loss: 0.00003705
Iteration 52/1000 | Loss: 0.00002400
Iteration 53/1000 | Loss: 0.00002288
Iteration 54/1000 | Loss: 0.00003002
Iteration 55/1000 | Loss: 0.00009340
Iteration 56/1000 | Loss: 0.00002173
Iteration 57/1000 | Loss: 0.00003730
Iteration 58/1000 | Loss: 0.00001757
Iteration 59/1000 | Loss: 0.00001818
Iteration 60/1000 | Loss: 0.00002020
Iteration 61/1000 | Loss: 0.00001881
Iteration 62/1000 | Loss: 0.00001733
Iteration 63/1000 | Loss: 0.00001724
Iteration 64/1000 | Loss: 0.00001692
Iteration 65/1000 | Loss: 0.00001682
Iteration 66/1000 | Loss: 0.00001676
Iteration 67/1000 | Loss: 0.00002603
Iteration 68/1000 | Loss: 0.00002603
Iteration 69/1000 | Loss: 0.00002253
Iteration 70/1000 | Loss: 0.00001676
Iteration 71/1000 | Loss: 0.00002393
Iteration 72/1000 | Loss: 0.00001654
Iteration 73/1000 | Loss: 0.00001654
Iteration 74/1000 | Loss: 0.00001654
Iteration 75/1000 | Loss: 0.00001654
Iteration 76/1000 | Loss: 0.00001654
Iteration 77/1000 | Loss: 0.00001654
Iteration 78/1000 | Loss: 0.00001654
Iteration 79/1000 | Loss: 0.00001654
Iteration 80/1000 | Loss: 0.00001654
Iteration 81/1000 | Loss: 0.00001654
Iteration 82/1000 | Loss: 0.00001654
Iteration 83/1000 | Loss: 0.00001653
Iteration 84/1000 | Loss: 0.00001653
Iteration 85/1000 | Loss: 0.00001653
Iteration 86/1000 | Loss: 0.00001653
Iteration 87/1000 | Loss: 0.00001653
Iteration 88/1000 | Loss: 0.00001653
Iteration 89/1000 | Loss: 0.00001652
Iteration 90/1000 | Loss: 0.00001652
Iteration 91/1000 | Loss: 0.00001652
Iteration 92/1000 | Loss: 0.00001651
Iteration 93/1000 | Loss: 0.00001785
Iteration 94/1000 | Loss: 0.00002240
Iteration 95/1000 | Loss: 0.00001648
Iteration 96/1000 | Loss: 0.00001648
Iteration 97/1000 | Loss: 0.00001648
Iteration 98/1000 | Loss: 0.00001648
Iteration 99/1000 | Loss: 0.00001647
Iteration 100/1000 | Loss: 0.00001647
Iteration 101/1000 | Loss: 0.00001647
Iteration 102/1000 | Loss: 0.00001647
Iteration 103/1000 | Loss: 0.00001649
Iteration 104/1000 | Loss: 0.00001647
Iteration 105/1000 | Loss: 0.00001646
Iteration 106/1000 | Loss: 0.00001646
Iteration 107/1000 | Loss: 0.00001646
Iteration 108/1000 | Loss: 0.00001646
Iteration 109/1000 | Loss: 0.00001646
Iteration 110/1000 | Loss: 0.00001646
Iteration 111/1000 | Loss: 0.00001646
Iteration 112/1000 | Loss: 0.00001646
Iteration 113/1000 | Loss: 0.00001646
Iteration 114/1000 | Loss: 0.00001646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.6462174244225025e-05, 1.6462174244225025e-05, 1.6462174244225025e-05, 1.6462174244225025e-05, 1.6462174244225025e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6462174244225025e-05

Optimization complete. Final v2v error: 3.4118800163269043 mm

Highest mean error: 5.456279754638672 mm for frame 238

Lowest mean error: 2.9306912422180176 mm for frame 80

Saving results

Total time: 157.81629347801208
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00512383
Iteration 2/25 | Loss: 0.00136122
Iteration 3/25 | Loss: 0.00127710
Iteration 4/25 | Loss: 0.00127036
Iteration 5/25 | Loss: 0.00126848
Iteration 6/25 | Loss: 0.00126831
Iteration 7/25 | Loss: 0.00126831
Iteration 8/25 | Loss: 0.00126831
Iteration 9/25 | Loss: 0.00126831
Iteration 10/25 | Loss: 0.00126831
Iteration 11/25 | Loss: 0.00126831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001268312567844987, 0.001268312567844987, 0.001268312567844987, 0.001268312567844987, 0.001268312567844987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001268312567844987

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46802783
Iteration 2/25 | Loss: 0.00075429
Iteration 3/25 | Loss: 0.00075424
Iteration 4/25 | Loss: 0.00075424
Iteration 5/25 | Loss: 0.00075424
Iteration 6/25 | Loss: 0.00075424
Iteration 7/25 | Loss: 0.00075424
Iteration 8/25 | Loss: 0.00075424
Iteration 9/25 | Loss: 0.00075424
Iteration 10/25 | Loss: 0.00075424
Iteration 11/25 | Loss: 0.00075424
Iteration 12/25 | Loss: 0.00075424
Iteration 13/25 | Loss: 0.00075424
Iteration 14/25 | Loss: 0.00075424
Iteration 15/25 | Loss: 0.00075424
Iteration 16/25 | Loss: 0.00075424
Iteration 17/25 | Loss: 0.00075424
Iteration 18/25 | Loss: 0.00075424
Iteration 19/25 | Loss: 0.00075424
Iteration 20/25 | Loss: 0.00075424
Iteration 21/25 | Loss: 0.00075424
Iteration 22/25 | Loss: 0.00075424
Iteration 23/25 | Loss: 0.00075424
Iteration 24/25 | Loss: 0.00075424
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007542396197095513, 0.0007542396197095513, 0.0007542396197095513, 0.0007542396197095513, 0.0007542396197095513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007542396197095513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075424
Iteration 2/1000 | Loss: 0.00004608
Iteration 3/1000 | Loss: 0.00002952
Iteration 4/1000 | Loss: 0.00002361
Iteration 5/1000 | Loss: 0.00002226
Iteration 6/1000 | Loss: 0.00002136
Iteration 7/1000 | Loss: 0.00002080
Iteration 8/1000 | Loss: 0.00002032
Iteration 9/1000 | Loss: 0.00001989
Iteration 10/1000 | Loss: 0.00001965
Iteration 11/1000 | Loss: 0.00001958
Iteration 12/1000 | Loss: 0.00001955
Iteration 13/1000 | Loss: 0.00001947
Iteration 14/1000 | Loss: 0.00001945
Iteration 15/1000 | Loss: 0.00001939
Iteration 16/1000 | Loss: 0.00001938
Iteration 17/1000 | Loss: 0.00001932
Iteration 18/1000 | Loss: 0.00001931
Iteration 19/1000 | Loss: 0.00001930
Iteration 20/1000 | Loss: 0.00001929
Iteration 21/1000 | Loss: 0.00001928
Iteration 22/1000 | Loss: 0.00001928
Iteration 23/1000 | Loss: 0.00001924
Iteration 24/1000 | Loss: 0.00001924
Iteration 25/1000 | Loss: 0.00001923
Iteration 26/1000 | Loss: 0.00001922
Iteration 27/1000 | Loss: 0.00001921
Iteration 28/1000 | Loss: 0.00001920
Iteration 29/1000 | Loss: 0.00001919
Iteration 30/1000 | Loss: 0.00001918
Iteration 31/1000 | Loss: 0.00001917
Iteration 32/1000 | Loss: 0.00001917
Iteration 33/1000 | Loss: 0.00001917
Iteration 34/1000 | Loss: 0.00001917
Iteration 35/1000 | Loss: 0.00001916
Iteration 36/1000 | Loss: 0.00001916
Iteration 37/1000 | Loss: 0.00001916
Iteration 38/1000 | Loss: 0.00001915
Iteration 39/1000 | Loss: 0.00001915
Iteration 40/1000 | Loss: 0.00001914
Iteration 41/1000 | Loss: 0.00001913
Iteration 42/1000 | Loss: 0.00001912
Iteration 43/1000 | Loss: 0.00001912
Iteration 44/1000 | Loss: 0.00001911
Iteration 45/1000 | Loss: 0.00001911
Iteration 46/1000 | Loss: 0.00001910
Iteration 47/1000 | Loss: 0.00001910
Iteration 48/1000 | Loss: 0.00001909
Iteration 49/1000 | Loss: 0.00001909
Iteration 50/1000 | Loss: 0.00001909
Iteration 51/1000 | Loss: 0.00001909
Iteration 52/1000 | Loss: 0.00001909
Iteration 53/1000 | Loss: 0.00001909
Iteration 54/1000 | Loss: 0.00001909
Iteration 55/1000 | Loss: 0.00001909
Iteration 56/1000 | Loss: 0.00001909
Iteration 57/1000 | Loss: 0.00001909
Iteration 58/1000 | Loss: 0.00001909
Iteration 59/1000 | Loss: 0.00001908
Iteration 60/1000 | Loss: 0.00001908
Iteration 61/1000 | Loss: 0.00001907
Iteration 62/1000 | Loss: 0.00001906
Iteration 63/1000 | Loss: 0.00001905
Iteration 64/1000 | Loss: 0.00001905
Iteration 65/1000 | Loss: 0.00001904
Iteration 66/1000 | Loss: 0.00001903
Iteration 67/1000 | Loss: 0.00001903
Iteration 68/1000 | Loss: 0.00001902
Iteration 69/1000 | Loss: 0.00001901
Iteration 70/1000 | Loss: 0.00001900
Iteration 71/1000 | Loss: 0.00001900
Iteration 72/1000 | Loss: 0.00001899
Iteration 73/1000 | Loss: 0.00001899
Iteration 74/1000 | Loss: 0.00001898
Iteration 75/1000 | Loss: 0.00001898
Iteration 76/1000 | Loss: 0.00001897
Iteration 77/1000 | Loss: 0.00001897
Iteration 78/1000 | Loss: 0.00001896
Iteration 79/1000 | Loss: 0.00001896
Iteration 80/1000 | Loss: 0.00001896
Iteration 81/1000 | Loss: 0.00001894
Iteration 82/1000 | Loss: 0.00001893
Iteration 83/1000 | Loss: 0.00001893
Iteration 84/1000 | Loss: 0.00001893
Iteration 85/1000 | Loss: 0.00001893
Iteration 86/1000 | Loss: 0.00001893
Iteration 87/1000 | Loss: 0.00001893
Iteration 88/1000 | Loss: 0.00001893
Iteration 89/1000 | Loss: 0.00001893
Iteration 90/1000 | Loss: 0.00001893
Iteration 91/1000 | Loss: 0.00001893
Iteration 92/1000 | Loss: 0.00001892
Iteration 93/1000 | Loss: 0.00001892
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001891
Iteration 97/1000 | Loss: 0.00001891
Iteration 98/1000 | Loss: 0.00001890
Iteration 99/1000 | Loss: 0.00001890
Iteration 100/1000 | Loss: 0.00001889
Iteration 101/1000 | Loss: 0.00001889
Iteration 102/1000 | Loss: 0.00001888
Iteration 103/1000 | Loss: 0.00001888
Iteration 104/1000 | Loss: 0.00001888
Iteration 105/1000 | Loss: 0.00001888
Iteration 106/1000 | Loss: 0.00001888
Iteration 107/1000 | Loss: 0.00001888
Iteration 108/1000 | Loss: 0.00001887
Iteration 109/1000 | Loss: 0.00001887
Iteration 110/1000 | Loss: 0.00001887
Iteration 111/1000 | Loss: 0.00001887
Iteration 112/1000 | Loss: 0.00001887
Iteration 113/1000 | Loss: 0.00001886
Iteration 114/1000 | Loss: 0.00001886
Iteration 115/1000 | Loss: 0.00001886
Iteration 116/1000 | Loss: 0.00001886
Iteration 117/1000 | Loss: 0.00001885
Iteration 118/1000 | Loss: 0.00001885
Iteration 119/1000 | Loss: 0.00001885
Iteration 120/1000 | Loss: 0.00001885
Iteration 121/1000 | Loss: 0.00001884
Iteration 122/1000 | Loss: 0.00001884
Iteration 123/1000 | Loss: 0.00001884
Iteration 124/1000 | Loss: 0.00001884
Iteration 125/1000 | Loss: 0.00001884
Iteration 126/1000 | Loss: 0.00001884
Iteration 127/1000 | Loss: 0.00001884
Iteration 128/1000 | Loss: 0.00001884
Iteration 129/1000 | Loss: 0.00001884
Iteration 130/1000 | Loss: 0.00001883
Iteration 131/1000 | Loss: 0.00001883
Iteration 132/1000 | Loss: 0.00001883
Iteration 133/1000 | Loss: 0.00001883
Iteration 134/1000 | Loss: 0.00001883
Iteration 135/1000 | Loss: 0.00001883
Iteration 136/1000 | Loss: 0.00001883
Iteration 137/1000 | Loss: 0.00001882
Iteration 138/1000 | Loss: 0.00001882
Iteration 139/1000 | Loss: 0.00001882
Iteration 140/1000 | Loss: 0.00001882
Iteration 141/1000 | Loss: 0.00001881
Iteration 142/1000 | Loss: 0.00001881
Iteration 143/1000 | Loss: 0.00001880
Iteration 144/1000 | Loss: 0.00001880
Iteration 145/1000 | Loss: 0.00001880
Iteration 146/1000 | Loss: 0.00001880
Iteration 147/1000 | Loss: 0.00001879
Iteration 148/1000 | Loss: 0.00001879
Iteration 149/1000 | Loss: 0.00001879
Iteration 150/1000 | Loss: 0.00001879
Iteration 151/1000 | Loss: 0.00001879
Iteration 152/1000 | Loss: 0.00001879
Iteration 153/1000 | Loss: 0.00001879
Iteration 154/1000 | Loss: 0.00001879
Iteration 155/1000 | Loss: 0.00001879
Iteration 156/1000 | Loss: 0.00001879
Iteration 157/1000 | Loss: 0.00001879
Iteration 158/1000 | Loss: 0.00001879
Iteration 159/1000 | Loss: 0.00001879
Iteration 160/1000 | Loss: 0.00001879
Iteration 161/1000 | Loss: 0.00001879
Iteration 162/1000 | Loss: 0.00001879
Iteration 163/1000 | Loss: 0.00001879
Iteration 164/1000 | Loss: 0.00001879
Iteration 165/1000 | Loss: 0.00001879
Iteration 166/1000 | Loss: 0.00001879
Iteration 167/1000 | Loss: 0.00001879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.878548937384039e-05, 1.878548937384039e-05, 1.878548937384039e-05, 1.878548937384039e-05, 1.878548937384039e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.878548937384039e-05

Optimization complete. Final v2v error: 3.4472711086273193 mm

Highest mean error: 5.220048427581787 mm for frame 59

Lowest mean error: 2.8751206398010254 mm for frame 87

Saving results

Total time: 36.65766906738281
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015126
Iteration 2/25 | Loss: 0.00148838
Iteration 3/25 | Loss: 0.00125203
Iteration 4/25 | Loss: 0.00122392
Iteration 5/25 | Loss: 0.00121922
Iteration 6/25 | Loss: 0.00121863
Iteration 7/25 | Loss: 0.00121863
Iteration 8/25 | Loss: 0.00121863
Iteration 9/25 | Loss: 0.00121863
Iteration 10/25 | Loss: 0.00121863
Iteration 11/25 | Loss: 0.00121863
Iteration 12/25 | Loss: 0.00121863
Iteration 13/25 | Loss: 0.00121863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012186297681182623, 0.0012186297681182623, 0.0012186297681182623, 0.0012186297681182623, 0.0012186297681182623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012186297681182623

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75055313
Iteration 2/25 | Loss: 0.00060325
Iteration 3/25 | Loss: 0.00060325
Iteration 4/25 | Loss: 0.00060325
Iteration 5/25 | Loss: 0.00060325
Iteration 6/25 | Loss: 0.00060325
Iteration 7/25 | Loss: 0.00060325
Iteration 8/25 | Loss: 0.00060325
Iteration 9/25 | Loss: 0.00060325
Iteration 10/25 | Loss: 0.00060325
Iteration 11/25 | Loss: 0.00060325
Iteration 12/25 | Loss: 0.00060325
Iteration 13/25 | Loss: 0.00060325
Iteration 14/25 | Loss: 0.00060325
Iteration 15/25 | Loss: 0.00060325
Iteration 16/25 | Loss: 0.00060325
Iteration 17/25 | Loss: 0.00060325
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000603251566644758, 0.000603251566644758, 0.000603251566644758, 0.000603251566644758, 0.000603251566644758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000603251566644758

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060325
Iteration 2/1000 | Loss: 0.00002650
Iteration 3/1000 | Loss: 0.00001879
Iteration 4/1000 | Loss: 0.00001744
Iteration 5/1000 | Loss: 0.00001621
Iteration 6/1000 | Loss: 0.00001561
Iteration 7/1000 | Loss: 0.00001520
Iteration 8/1000 | Loss: 0.00001489
Iteration 9/1000 | Loss: 0.00001452
Iteration 10/1000 | Loss: 0.00001438
Iteration 11/1000 | Loss: 0.00001431
Iteration 12/1000 | Loss: 0.00001421
Iteration 13/1000 | Loss: 0.00001409
Iteration 14/1000 | Loss: 0.00001408
Iteration 15/1000 | Loss: 0.00001405
Iteration 16/1000 | Loss: 0.00001401
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001398
Iteration 19/1000 | Loss: 0.00001397
Iteration 20/1000 | Loss: 0.00001396
Iteration 21/1000 | Loss: 0.00001396
Iteration 22/1000 | Loss: 0.00001395
Iteration 23/1000 | Loss: 0.00001394
Iteration 24/1000 | Loss: 0.00001394
Iteration 25/1000 | Loss: 0.00001394
Iteration 26/1000 | Loss: 0.00001394
Iteration 27/1000 | Loss: 0.00001394
Iteration 28/1000 | Loss: 0.00001394
Iteration 29/1000 | Loss: 0.00001393
Iteration 30/1000 | Loss: 0.00001393
Iteration 31/1000 | Loss: 0.00001393
Iteration 32/1000 | Loss: 0.00001393
Iteration 33/1000 | Loss: 0.00001393
Iteration 34/1000 | Loss: 0.00001393
Iteration 35/1000 | Loss: 0.00001393
Iteration 36/1000 | Loss: 0.00001393
Iteration 37/1000 | Loss: 0.00001393
Iteration 38/1000 | Loss: 0.00001393
Iteration 39/1000 | Loss: 0.00001393
Iteration 40/1000 | Loss: 0.00001392
Iteration 41/1000 | Loss: 0.00001391
Iteration 42/1000 | Loss: 0.00001391
Iteration 43/1000 | Loss: 0.00001389
Iteration 44/1000 | Loss: 0.00001389
Iteration 45/1000 | Loss: 0.00001389
Iteration 46/1000 | Loss: 0.00001389
Iteration 47/1000 | Loss: 0.00001389
Iteration 48/1000 | Loss: 0.00001389
Iteration 49/1000 | Loss: 0.00001388
Iteration 50/1000 | Loss: 0.00001388
Iteration 51/1000 | Loss: 0.00001388
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001387
Iteration 54/1000 | Loss: 0.00001386
Iteration 55/1000 | Loss: 0.00001386
Iteration 56/1000 | Loss: 0.00001386
Iteration 57/1000 | Loss: 0.00001386
Iteration 58/1000 | Loss: 0.00001385
Iteration 59/1000 | Loss: 0.00001385
Iteration 60/1000 | Loss: 0.00001385
Iteration 61/1000 | Loss: 0.00001384
Iteration 62/1000 | Loss: 0.00001384
Iteration 63/1000 | Loss: 0.00001383
Iteration 64/1000 | Loss: 0.00001382
Iteration 65/1000 | Loss: 0.00001382
Iteration 66/1000 | Loss: 0.00001381
Iteration 67/1000 | Loss: 0.00001381
Iteration 68/1000 | Loss: 0.00001381
Iteration 69/1000 | Loss: 0.00001381
Iteration 70/1000 | Loss: 0.00001381
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001380
Iteration 74/1000 | Loss: 0.00001380
Iteration 75/1000 | Loss: 0.00001379
Iteration 76/1000 | Loss: 0.00001379
Iteration 77/1000 | Loss: 0.00001379
Iteration 78/1000 | Loss: 0.00001379
Iteration 79/1000 | Loss: 0.00001379
Iteration 80/1000 | Loss: 0.00001379
Iteration 81/1000 | Loss: 0.00001379
Iteration 82/1000 | Loss: 0.00001378
Iteration 83/1000 | Loss: 0.00001378
Iteration 84/1000 | Loss: 0.00001378
Iteration 85/1000 | Loss: 0.00001378
Iteration 86/1000 | Loss: 0.00001377
Iteration 87/1000 | Loss: 0.00001377
Iteration 88/1000 | Loss: 0.00001377
Iteration 89/1000 | Loss: 0.00001377
Iteration 90/1000 | Loss: 0.00001377
Iteration 91/1000 | Loss: 0.00001377
Iteration 92/1000 | Loss: 0.00001377
Iteration 93/1000 | Loss: 0.00001377
Iteration 94/1000 | Loss: 0.00001377
Iteration 95/1000 | Loss: 0.00001377
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001376
Iteration 99/1000 | Loss: 0.00001376
Iteration 100/1000 | Loss: 0.00001376
Iteration 101/1000 | Loss: 0.00001376
Iteration 102/1000 | Loss: 0.00001375
Iteration 103/1000 | Loss: 0.00001375
Iteration 104/1000 | Loss: 0.00001375
Iteration 105/1000 | Loss: 0.00001375
Iteration 106/1000 | Loss: 0.00001375
Iteration 107/1000 | Loss: 0.00001375
Iteration 108/1000 | Loss: 0.00001375
Iteration 109/1000 | Loss: 0.00001374
Iteration 110/1000 | Loss: 0.00001374
Iteration 111/1000 | Loss: 0.00001374
Iteration 112/1000 | Loss: 0.00001374
Iteration 113/1000 | Loss: 0.00001374
Iteration 114/1000 | Loss: 0.00001374
Iteration 115/1000 | Loss: 0.00001374
Iteration 116/1000 | Loss: 0.00001374
Iteration 117/1000 | Loss: 0.00001374
Iteration 118/1000 | Loss: 0.00001374
Iteration 119/1000 | Loss: 0.00001374
Iteration 120/1000 | Loss: 0.00001374
Iteration 121/1000 | Loss: 0.00001374
Iteration 122/1000 | Loss: 0.00001374
Iteration 123/1000 | Loss: 0.00001374
Iteration 124/1000 | Loss: 0.00001374
Iteration 125/1000 | Loss: 0.00001374
Iteration 126/1000 | Loss: 0.00001373
Iteration 127/1000 | Loss: 0.00001373
Iteration 128/1000 | Loss: 0.00001373
Iteration 129/1000 | Loss: 0.00001373
Iteration 130/1000 | Loss: 0.00001373
Iteration 131/1000 | Loss: 0.00001373
Iteration 132/1000 | Loss: 0.00001373
Iteration 133/1000 | Loss: 0.00001373
Iteration 134/1000 | Loss: 0.00001373
Iteration 135/1000 | Loss: 0.00001372
Iteration 136/1000 | Loss: 0.00001372
Iteration 137/1000 | Loss: 0.00001372
Iteration 138/1000 | Loss: 0.00001372
Iteration 139/1000 | Loss: 0.00001372
Iteration 140/1000 | Loss: 0.00001372
Iteration 141/1000 | Loss: 0.00001372
Iteration 142/1000 | Loss: 0.00001372
Iteration 143/1000 | Loss: 0.00001372
Iteration 144/1000 | Loss: 0.00001372
Iteration 145/1000 | Loss: 0.00001372
Iteration 146/1000 | Loss: 0.00001372
Iteration 147/1000 | Loss: 0.00001372
Iteration 148/1000 | Loss: 0.00001372
Iteration 149/1000 | Loss: 0.00001372
Iteration 150/1000 | Loss: 0.00001372
Iteration 151/1000 | Loss: 0.00001372
Iteration 152/1000 | Loss: 0.00001372
Iteration 153/1000 | Loss: 0.00001372
Iteration 154/1000 | Loss: 0.00001371
Iteration 155/1000 | Loss: 0.00001371
Iteration 156/1000 | Loss: 0.00001371
Iteration 157/1000 | Loss: 0.00001371
Iteration 158/1000 | Loss: 0.00001371
Iteration 159/1000 | Loss: 0.00001371
Iteration 160/1000 | Loss: 0.00001371
Iteration 161/1000 | Loss: 0.00001371
Iteration 162/1000 | Loss: 0.00001371
Iteration 163/1000 | Loss: 0.00001370
Iteration 164/1000 | Loss: 0.00001370
Iteration 165/1000 | Loss: 0.00001370
Iteration 166/1000 | Loss: 0.00001370
Iteration 167/1000 | Loss: 0.00001370
Iteration 168/1000 | Loss: 0.00001370
Iteration 169/1000 | Loss: 0.00001370
Iteration 170/1000 | Loss: 0.00001370
Iteration 171/1000 | Loss: 0.00001370
Iteration 172/1000 | Loss: 0.00001370
Iteration 173/1000 | Loss: 0.00001370
Iteration 174/1000 | Loss: 0.00001370
Iteration 175/1000 | Loss: 0.00001370
Iteration 176/1000 | Loss: 0.00001369
Iteration 177/1000 | Loss: 0.00001369
Iteration 178/1000 | Loss: 0.00001369
Iteration 179/1000 | Loss: 0.00001369
Iteration 180/1000 | Loss: 0.00001368
Iteration 181/1000 | Loss: 0.00001368
Iteration 182/1000 | Loss: 0.00001368
Iteration 183/1000 | Loss: 0.00001368
Iteration 184/1000 | Loss: 0.00001368
Iteration 185/1000 | Loss: 0.00001367
Iteration 186/1000 | Loss: 0.00001367
Iteration 187/1000 | Loss: 0.00001367
Iteration 188/1000 | Loss: 0.00001367
Iteration 189/1000 | Loss: 0.00001367
Iteration 190/1000 | Loss: 0.00001367
Iteration 191/1000 | Loss: 0.00001367
Iteration 192/1000 | Loss: 0.00001367
Iteration 193/1000 | Loss: 0.00001367
Iteration 194/1000 | Loss: 0.00001367
Iteration 195/1000 | Loss: 0.00001367
Iteration 196/1000 | Loss: 0.00001367
Iteration 197/1000 | Loss: 0.00001367
Iteration 198/1000 | Loss: 0.00001366
Iteration 199/1000 | Loss: 0.00001366
Iteration 200/1000 | Loss: 0.00001366
Iteration 201/1000 | Loss: 0.00001366
Iteration 202/1000 | Loss: 0.00001366
Iteration 203/1000 | Loss: 0.00001366
Iteration 204/1000 | Loss: 0.00001366
Iteration 205/1000 | Loss: 0.00001366
Iteration 206/1000 | Loss: 0.00001366
Iteration 207/1000 | Loss: 0.00001366
Iteration 208/1000 | Loss: 0.00001366
Iteration 209/1000 | Loss: 0.00001366
Iteration 210/1000 | Loss: 0.00001366
Iteration 211/1000 | Loss: 0.00001365
Iteration 212/1000 | Loss: 0.00001365
Iteration 213/1000 | Loss: 0.00001365
Iteration 214/1000 | Loss: 0.00001365
Iteration 215/1000 | Loss: 0.00001365
Iteration 216/1000 | Loss: 0.00001365
Iteration 217/1000 | Loss: 0.00001365
Iteration 218/1000 | Loss: 0.00001365
Iteration 219/1000 | Loss: 0.00001365
Iteration 220/1000 | Loss: 0.00001365
Iteration 221/1000 | Loss: 0.00001365
Iteration 222/1000 | Loss: 0.00001365
Iteration 223/1000 | Loss: 0.00001364
Iteration 224/1000 | Loss: 0.00001364
Iteration 225/1000 | Loss: 0.00001364
Iteration 226/1000 | Loss: 0.00001364
Iteration 227/1000 | Loss: 0.00001364
Iteration 228/1000 | Loss: 0.00001364
Iteration 229/1000 | Loss: 0.00001364
Iteration 230/1000 | Loss: 0.00001364
Iteration 231/1000 | Loss: 0.00001364
Iteration 232/1000 | Loss: 0.00001364
Iteration 233/1000 | Loss: 0.00001364
Iteration 234/1000 | Loss: 0.00001364
Iteration 235/1000 | Loss: 0.00001364
Iteration 236/1000 | Loss: 0.00001364
Iteration 237/1000 | Loss: 0.00001364
Iteration 238/1000 | Loss: 0.00001363
Iteration 239/1000 | Loss: 0.00001363
Iteration 240/1000 | Loss: 0.00001363
Iteration 241/1000 | Loss: 0.00001363
Iteration 242/1000 | Loss: 0.00001363
Iteration 243/1000 | Loss: 0.00001363
Iteration 244/1000 | Loss: 0.00001363
Iteration 245/1000 | Loss: 0.00001363
Iteration 246/1000 | Loss: 0.00001363
Iteration 247/1000 | Loss: 0.00001363
Iteration 248/1000 | Loss: 0.00001363
Iteration 249/1000 | Loss: 0.00001363
Iteration 250/1000 | Loss: 0.00001363
Iteration 251/1000 | Loss: 0.00001363
Iteration 252/1000 | Loss: 0.00001363
Iteration 253/1000 | Loss: 0.00001363
Iteration 254/1000 | Loss: 0.00001363
Iteration 255/1000 | Loss: 0.00001363
Iteration 256/1000 | Loss: 0.00001363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [1.363014325761469e-05, 1.363014325761469e-05, 1.363014325761469e-05, 1.363014325761469e-05, 1.363014325761469e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.363014325761469e-05

Optimization complete. Final v2v error: 3.118945598602295 mm

Highest mean error: 3.454895496368408 mm for frame 96

Lowest mean error: 2.922776222229004 mm for frame 247

Saving results

Total time: 46.609055519104004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847844
Iteration 2/25 | Loss: 0.00125366
Iteration 3/25 | Loss: 0.00118950
Iteration 4/25 | Loss: 0.00117939
Iteration 5/25 | Loss: 0.00117616
Iteration 6/25 | Loss: 0.00117605
Iteration 7/25 | Loss: 0.00117605
Iteration 8/25 | Loss: 0.00117605
Iteration 9/25 | Loss: 0.00117605
Iteration 10/25 | Loss: 0.00117605
Iteration 11/25 | Loss: 0.00117605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011760516790673137, 0.0011760516790673137, 0.0011760516790673137, 0.0011760516790673137, 0.0011760516790673137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011760516790673137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74843895
Iteration 2/25 | Loss: 0.00062329
Iteration 3/25 | Loss: 0.00062328
Iteration 4/25 | Loss: 0.00062328
Iteration 5/25 | Loss: 0.00062328
Iteration 6/25 | Loss: 0.00062328
Iteration 7/25 | Loss: 0.00062328
Iteration 8/25 | Loss: 0.00062328
Iteration 9/25 | Loss: 0.00062328
Iteration 10/25 | Loss: 0.00062328
Iteration 11/25 | Loss: 0.00062328
Iteration 12/25 | Loss: 0.00062328
Iteration 13/25 | Loss: 0.00062328
Iteration 14/25 | Loss: 0.00062328
Iteration 15/25 | Loss: 0.00062328
Iteration 16/25 | Loss: 0.00062328
Iteration 17/25 | Loss: 0.00062328
Iteration 18/25 | Loss: 0.00062328
Iteration 19/25 | Loss: 0.00062328
Iteration 20/25 | Loss: 0.00062328
Iteration 21/25 | Loss: 0.00062328
Iteration 22/25 | Loss: 0.00062328
Iteration 23/25 | Loss: 0.00062328
Iteration 24/25 | Loss: 0.00062328
Iteration 25/25 | Loss: 0.00062328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062328
Iteration 2/1000 | Loss: 0.00002372
Iteration 3/1000 | Loss: 0.00001746
Iteration 4/1000 | Loss: 0.00001557
Iteration 5/1000 | Loss: 0.00001467
Iteration 6/1000 | Loss: 0.00001396
Iteration 7/1000 | Loss: 0.00001353
Iteration 8/1000 | Loss: 0.00001336
Iteration 9/1000 | Loss: 0.00001324
Iteration 10/1000 | Loss: 0.00001318
Iteration 11/1000 | Loss: 0.00001310
Iteration 12/1000 | Loss: 0.00001288
Iteration 13/1000 | Loss: 0.00001280
Iteration 14/1000 | Loss: 0.00001278
Iteration 15/1000 | Loss: 0.00001278
Iteration 16/1000 | Loss: 0.00001277
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001277
Iteration 19/1000 | Loss: 0.00001276
Iteration 20/1000 | Loss: 0.00001276
Iteration 21/1000 | Loss: 0.00001274
Iteration 22/1000 | Loss: 0.00001273
Iteration 23/1000 | Loss: 0.00001273
Iteration 24/1000 | Loss: 0.00001273
Iteration 25/1000 | Loss: 0.00001272
Iteration 26/1000 | Loss: 0.00001272
Iteration 27/1000 | Loss: 0.00001266
Iteration 28/1000 | Loss: 0.00001257
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001250
Iteration 31/1000 | Loss: 0.00001250
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001247
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001245
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001242
Iteration 40/1000 | Loss: 0.00001242
Iteration 41/1000 | Loss: 0.00001241
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001239
Iteration 46/1000 | Loss: 0.00001236
Iteration 47/1000 | Loss: 0.00001236
Iteration 48/1000 | Loss: 0.00001235
Iteration 49/1000 | Loss: 0.00001234
Iteration 50/1000 | Loss: 0.00001234
Iteration 51/1000 | Loss: 0.00001233
Iteration 52/1000 | Loss: 0.00001233
Iteration 53/1000 | Loss: 0.00001232
Iteration 54/1000 | Loss: 0.00001231
Iteration 55/1000 | Loss: 0.00001229
Iteration 56/1000 | Loss: 0.00001228
Iteration 57/1000 | Loss: 0.00001228
Iteration 58/1000 | Loss: 0.00001228
Iteration 59/1000 | Loss: 0.00001228
Iteration 60/1000 | Loss: 0.00001228
Iteration 61/1000 | Loss: 0.00001228
Iteration 62/1000 | Loss: 0.00001228
Iteration 63/1000 | Loss: 0.00001228
Iteration 64/1000 | Loss: 0.00001227
Iteration 65/1000 | Loss: 0.00001227
Iteration 66/1000 | Loss: 0.00001227
Iteration 67/1000 | Loss: 0.00001226
Iteration 68/1000 | Loss: 0.00001224
Iteration 69/1000 | Loss: 0.00001224
Iteration 70/1000 | Loss: 0.00001223
Iteration 71/1000 | Loss: 0.00001223
Iteration 72/1000 | Loss: 0.00001218
Iteration 73/1000 | Loss: 0.00001218
Iteration 74/1000 | Loss: 0.00001218
Iteration 75/1000 | Loss: 0.00001217
Iteration 76/1000 | Loss: 0.00001217
Iteration 77/1000 | Loss: 0.00001216
Iteration 78/1000 | Loss: 0.00001216
Iteration 79/1000 | Loss: 0.00001215
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001215
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001214
Iteration 85/1000 | Loss: 0.00001213
Iteration 86/1000 | Loss: 0.00001212
Iteration 87/1000 | Loss: 0.00001212
Iteration 88/1000 | Loss: 0.00001212
Iteration 89/1000 | Loss: 0.00001212
Iteration 90/1000 | Loss: 0.00001212
Iteration 91/1000 | Loss: 0.00001211
Iteration 92/1000 | Loss: 0.00001211
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001211
Iteration 95/1000 | Loss: 0.00001211
Iteration 96/1000 | Loss: 0.00001210
Iteration 97/1000 | Loss: 0.00001210
Iteration 98/1000 | Loss: 0.00001210
Iteration 99/1000 | Loss: 0.00001210
Iteration 100/1000 | Loss: 0.00001210
Iteration 101/1000 | Loss: 0.00001209
Iteration 102/1000 | Loss: 0.00001209
Iteration 103/1000 | Loss: 0.00001209
Iteration 104/1000 | Loss: 0.00001209
Iteration 105/1000 | Loss: 0.00001209
Iteration 106/1000 | Loss: 0.00001208
Iteration 107/1000 | Loss: 0.00001208
Iteration 108/1000 | Loss: 0.00001208
Iteration 109/1000 | Loss: 0.00001208
Iteration 110/1000 | Loss: 0.00001208
Iteration 111/1000 | Loss: 0.00001208
Iteration 112/1000 | Loss: 0.00001208
Iteration 113/1000 | Loss: 0.00001208
Iteration 114/1000 | Loss: 0.00001207
Iteration 115/1000 | Loss: 0.00001207
Iteration 116/1000 | Loss: 0.00001207
Iteration 117/1000 | Loss: 0.00001207
Iteration 118/1000 | Loss: 0.00001207
Iteration 119/1000 | Loss: 0.00001207
Iteration 120/1000 | Loss: 0.00001207
Iteration 121/1000 | Loss: 0.00001207
Iteration 122/1000 | Loss: 0.00001207
Iteration 123/1000 | Loss: 0.00001206
Iteration 124/1000 | Loss: 0.00001206
Iteration 125/1000 | Loss: 0.00001206
Iteration 126/1000 | Loss: 0.00001206
Iteration 127/1000 | Loss: 0.00001206
Iteration 128/1000 | Loss: 0.00001206
Iteration 129/1000 | Loss: 0.00001206
Iteration 130/1000 | Loss: 0.00001206
Iteration 131/1000 | Loss: 0.00001206
Iteration 132/1000 | Loss: 0.00001206
Iteration 133/1000 | Loss: 0.00001206
Iteration 134/1000 | Loss: 0.00001206
Iteration 135/1000 | Loss: 0.00001206
Iteration 136/1000 | Loss: 0.00001206
Iteration 137/1000 | Loss: 0.00001206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.205801800097106e-05, 1.205801800097106e-05, 1.205801800097106e-05, 1.205801800097106e-05, 1.205801800097106e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.205801800097106e-05

Optimization complete. Final v2v error: 2.9174282550811768 mm

Highest mean error: 3.609253406524658 mm for frame 44

Lowest mean error: 2.739795207977295 mm for frame 100

Saving results

Total time: 35.226890087127686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746777
Iteration 2/25 | Loss: 0.00188466
Iteration 3/25 | Loss: 0.00142645
Iteration 4/25 | Loss: 0.00137914
Iteration 5/25 | Loss: 0.00136715
Iteration 6/25 | Loss: 0.00135928
Iteration 7/25 | Loss: 0.00133909
Iteration 8/25 | Loss: 0.00132894
Iteration 9/25 | Loss: 0.00132251
Iteration 10/25 | Loss: 0.00132137
Iteration 11/25 | Loss: 0.00132119
Iteration 12/25 | Loss: 0.00132118
Iteration 13/25 | Loss: 0.00132117
Iteration 14/25 | Loss: 0.00132117
Iteration 15/25 | Loss: 0.00132117
Iteration 16/25 | Loss: 0.00132117
Iteration 17/25 | Loss: 0.00132117
Iteration 18/25 | Loss: 0.00132117
Iteration 19/25 | Loss: 0.00132117
Iteration 20/25 | Loss: 0.00132117
Iteration 21/25 | Loss: 0.00132117
Iteration 22/25 | Loss: 0.00132117
Iteration 23/25 | Loss: 0.00132117
Iteration 24/25 | Loss: 0.00132117
Iteration 25/25 | Loss: 0.00132117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43691278
Iteration 2/25 | Loss: 0.00056566
Iteration 3/25 | Loss: 0.00056562
Iteration 4/25 | Loss: 0.00056562
Iteration 5/25 | Loss: 0.00056562
Iteration 6/25 | Loss: 0.00056562
Iteration 7/25 | Loss: 0.00056562
Iteration 8/25 | Loss: 0.00056562
Iteration 9/25 | Loss: 0.00056562
Iteration 10/25 | Loss: 0.00056562
Iteration 11/25 | Loss: 0.00056561
Iteration 12/25 | Loss: 0.00056561
Iteration 13/25 | Loss: 0.00056561
Iteration 14/25 | Loss: 0.00056561
Iteration 15/25 | Loss: 0.00056561
Iteration 16/25 | Loss: 0.00056561
Iteration 17/25 | Loss: 0.00056561
Iteration 18/25 | Loss: 0.00056561
Iteration 19/25 | Loss: 0.00056561
Iteration 20/25 | Loss: 0.00056561
Iteration 21/25 | Loss: 0.00056561
Iteration 22/25 | Loss: 0.00056561
Iteration 23/25 | Loss: 0.00056561
Iteration 24/25 | Loss: 0.00056561
Iteration 25/25 | Loss: 0.00056561

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056561
Iteration 2/1000 | Loss: 0.00004606
Iteration 3/1000 | Loss: 0.00003359
Iteration 4/1000 | Loss: 0.00003099
Iteration 5/1000 | Loss: 0.00002953
Iteration 6/1000 | Loss: 0.00002864
Iteration 7/1000 | Loss: 0.00002782
Iteration 8/1000 | Loss: 0.00002715
Iteration 9/1000 | Loss: 0.00002669
Iteration 10/1000 | Loss: 0.00002629
Iteration 11/1000 | Loss: 0.00002601
Iteration 12/1000 | Loss: 0.00002594
Iteration 13/1000 | Loss: 0.00002588
Iteration 14/1000 | Loss: 0.00002566
Iteration 15/1000 | Loss: 0.00002552
Iteration 16/1000 | Loss: 0.00002540
Iteration 17/1000 | Loss: 0.00002539
Iteration 18/1000 | Loss: 0.00002538
Iteration 19/1000 | Loss: 0.00002534
Iteration 20/1000 | Loss: 0.00002534
Iteration 21/1000 | Loss: 0.00002533
Iteration 22/1000 | Loss: 0.00002532
Iteration 23/1000 | Loss: 0.00002532
Iteration 24/1000 | Loss: 0.00002532
Iteration 25/1000 | Loss: 0.00002531
Iteration 26/1000 | Loss: 0.00002531
Iteration 27/1000 | Loss: 0.00002531
Iteration 28/1000 | Loss: 0.00002531
Iteration 29/1000 | Loss: 0.00002530
Iteration 30/1000 | Loss: 0.00002530
Iteration 31/1000 | Loss: 0.00002529
Iteration 32/1000 | Loss: 0.00002529
Iteration 33/1000 | Loss: 0.00002529
Iteration 34/1000 | Loss: 0.00002528
Iteration 35/1000 | Loss: 0.00002527
Iteration 36/1000 | Loss: 0.00002527
Iteration 37/1000 | Loss: 0.00002524
Iteration 38/1000 | Loss: 0.00002524
Iteration 39/1000 | Loss: 0.00002524
Iteration 40/1000 | Loss: 0.00002524
Iteration 41/1000 | Loss: 0.00002524
Iteration 42/1000 | Loss: 0.00002524
Iteration 43/1000 | Loss: 0.00002524
Iteration 44/1000 | Loss: 0.00002523
Iteration 45/1000 | Loss: 0.00002523
Iteration 46/1000 | Loss: 0.00002521
Iteration 47/1000 | Loss: 0.00002521
Iteration 48/1000 | Loss: 0.00002520
Iteration 49/1000 | Loss: 0.00002520
Iteration 50/1000 | Loss: 0.00002519
Iteration 51/1000 | Loss: 0.00002519
Iteration 52/1000 | Loss: 0.00002519
Iteration 53/1000 | Loss: 0.00002518
Iteration 54/1000 | Loss: 0.00002518
Iteration 55/1000 | Loss: 0.00002518
Iteration 56/1000 | Loss: 0.00002518
Iteration 57/1000 | Loss: 0.00002517
Iteration 58/1000 | Loss: 0.00002517
Iteration 59/1000 | Loss: 0.00002517
Iteration 60/1000 | Loss: 0.00002517
Iteration 61/1000 | Loss: 0.00002516
Iteration 62/1000 | Loss: 0.00002516
Iteration 63/1000 | Loss: 0.00002516
Iteration 64/1000 | Loss: 0.00002515
Iteration 65/1000 | Loss: 0.00002515
Iteration 66/1000 | Loss: 0.00002515
Iteration 67/1000 | Loss: 0.00002515
Iteration 68/1000 | Loss: 0.00002515
Iteration 69/1000 | Loss: 0.00002515
Iteration 70/1000 | Loss: 0.00002515
Iteration 71/1000 | Loss: 0.00002515
Iteration 72/1000 | Loss: 0.00002515
Iteration 73/1000 | Loss: 0.00002514
Iteration 74/1000 | Loss: 0.00002514
Iteration 75/1000 | Loss: 0.00002514
Iteration 76/1000 | Loss: 0.00002514
Iteration 77/1000 | Loss: 0.00002514
Iteration 78/1000 | Loss: 0.00002514
Iteration 79/1000 | Loss: 0.00002514
Iteration 80/1000 | Loss: 0.00002513
Iteration 81/1000 | Loss: 0.00002513
Iteration 82/1000 | Loss: 0.00002513
Iteration 83/1000 | Loss: 0.00002513
Iteration 84/1000 | Loss: 0.00002513
Iteration 85/1000 | Loss: 0.00002512
Iteration 86/1000 | Loss: 0.00002512
Iteration 87/1000 | Loss: 0.00002512
Iteration 88/1000 | Loss: 0.00002512
Iteration 89/1000 | Loss: 0.00002511
Iteration 90/1000 | Loss: 0.00002511
Iteration 91/1000 | Loss: 0.00002511
Iteration 92/1000 | Loss: 0.00002510
Iteration 93/1000 | Loss: 0.00002510
Iteration 94/1000 | Loss: 0.00002510
Iteration 95/1000 | Loss: 0.00002510
Iteration 96/1000 | Loss: 0.00002510
Iteration 97/1000 | Loss: 0.00002510
Iteration 98/1000 | Loss: 0.00002510
Iteration 99/1000 | Loss: 0.00002510
Iteration 100/1000 | Loss: 0.00002510
Iteration 101/1000 | Loss: 0.00002510
Iteration 102/1000 | Loss: 0.00002510
Iteration 103/1000 | Loss: 0.00002510
Iteration 104/1000 | Loss: 0.00002510
Iteration 105/1000 | Loss: 0.00002510
Iteration 106/1000 | Loss: 0.00002510
Iteration 107/1000 | Loss: 0.00002509
Iteration 108/1000 | Loss: 0.00002509
Iteration 109/1000 | Loss: 0.00002509
Iteration 110/1000 | Loss: 0.00002509
Iteration 111/1000 | Loss: 0.00002509
Iteration 112/1000 | Loss: 0.00002509
Iteration 113/1000 | Loss: 0.00002509
Iteration 114/1000 | Loss: 0.00002508
Iteration 115/1000 | Loss: 0.00002508
Iteration 116/1000 | Loss: 0.00002508
Iteration 117/1000 | Loss: 0.00002508
Iteration 118/1000 | Loss: 0.00002508
Iteration 119/1000 | Loss: 0.00002508
Iteration 120/1000 | Loss: 0.00002508
Iteration 121/1000 | Loss: 0.00002508
Iteration 122/1000 | Loss: 0.00002508
Iteration 123/1000 | Loss: 0.00002508
Iteration 124/1000 | Loss: 0.00002508
Iteration 125/1000 | Loss: 0.00002508
Iteration 126/1000 | Loss: 0.00002508
Iteration 127/1000 | Loss: 0.00002508
Iteration 128/1000 | Loss: 0.00002507
Iteration 129/1000 | Loss: 0.00002507
Iteration 130/1000 | Loss: 0.00002507
Iteration 131/1000 | Loss: 0.00002507
Iteration 132/1000 | Loss: 0.00002507
Iteration 133/1000 | Loss: 0.00002507
Iteration 134/1000 | Loss: 0.00002507
Iteration 135/1000 | Loss: 0.00002507
Iteration 136/1000 | Loss: 0.00002507
Iteration 137/1000 | Loss: 0.00002507
Iteration 138/1000 | Loss: 0.00002507
Iteration 139/1000 | Loss: 0.00002507
Iteration 140/1000 | Loss: 0.00002507
Iteration 141/1000 | Loss: 0.00002507
Iteration 142/1000 | Loss: 0.00002507
Iteration 143/1000 | Loss: 0.00002507
Iteration 144/1000 | Loss: 0.00002507
Iteration 145/1000 | Loss: 0.00002507
Iteration 146/1000 | Loss: 0.00002507
Iteration 147/1000 | Loss: 0.00002507
Iteration 148/1000 | Loss: 0.00002507
Iteration 149/1000 | Loss: 0.00002507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.5067982278415002e-05, 2.5067982278415002e-05, 2.5067982278415002e-05, 2.5067982278415002e-05, 2.5067982278415002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5067982278415002e-05

Optimization complete. Final v2v error: 4.20529317855835 mm

Highest mean error: 5.184507846832275 mm for frame 181

Lowest mean error: 3.781313419342041 mm for frame 214

Saving results

Total time: 56.98514151573181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798374
Iteration 2/25 | Loss: 0.00126891
Iteration 3/25 | Loss: 0.00118271
Iteration 4/25 | Loss: 0.00117754
Iteration 5/25 | Loss: 0.00117732
Iteration 6/25 | Loss: 0.00117732
Iteration 7/25 | Loss: 0.00117732
Iteration 8/25 | Loss: 0.00117732
Iteration 9/25 | Loss: 0.00117732
Iteration 10/25 | Loss: 0.00117732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011773239821195602, 0.0011773239821195602, 0.0011773239821195602, 0.0011773239821195602, 0.0011773239821195602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011773239821195602

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45059383
Iteration 2/25 | Loss: 0.00054908
Iteration 3/25 | Loss: 0.00054908
Iteration 4/25 | Loss: 0.00054907
Iteration 5/25 | Loss: 0.00054907
Iteration 6/25 | Loss: 0.00054907
Iteration 7/25 | Loss: 0.00054907
Iteration 8/25 | Loss: 0.00054907
Iteration 9/25 | Loss: 0.00054907
Iteration 10/25 | Loss: 0.00054907
Iteration 11/25 | Loss: 0.00054907
Iteration 12/25 | Loss: 0.00054907
Iteration 13/25 | Loss: 0.00054907
Iteration 14/25 | Loss: 0.00054907
Iteration 15/25 | Loss: 0.00054907
Iteration 16/25 | Loss: 0.00054907
Iteration 17/25 | Loss: 0.00054907
Iteration 18/25 | Loss: 0.00054907
Iteration 19/25 | Loss: 0.00054907
Iteration 20/25 | Loss: 0.00054907
Iteration 21/25 | Loss: 0.00054907
Iteration 22/25 | Loss: 0.00054907
Iteration 23/25 | Loss: 0.00054907
Iteration 24/25 | Loss: 0.00054907
Iteration 25/25 | Loss: 0.00054907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054907
Iteration 2/1000 | Loss: 0.00002635
Iteration 3/1000 | Loss: 0.00001942
Iteration 4/1000 | Loss: 0.00001719
Iteration 5/1000 | Loss: 0.00001608
Iteration 6/1000 | Loss: 0.00001516
Iteration 7/1000 | Loss: 0.00001485
Iteration 8/1000 | Loss: 0.00001442
Iteration 9/1000 | Loss: 0.00001408
Iteration 10/1000 | Loss: 0.00001386
Iteration 11/1000 | Loss: 0.00001385
Iteration 12/1000 | Loss: 0.00001381
Iteration 13/1000 | Loss: 0.00001377
Iteration 14/1000 | Loss: 0.00001376
Iteration 15/1000 | Loss: 0.00001375
Iteration 16/1000 | Loss: 0.00001364
Iteration 17/1000 | Loss: 0.00001352
Iteration 18/1000 | Loss: 0.00001351
Iteration 19/1000 | Loss: 0.00001350
Iteration 20/1000 | Loss: 0.00001349
Iteration 21/1000 | Loss: 0.00001349
Iteration 22/1000 | Loss: 0.00001348
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001346
Iteration 25/1000 | Loss: 0.00001345
Iteration 26/1000 | Loss: 0.00001345
Iteration 27/1000 | Loss: 0.00001342
Iteration 28/1000 | Loss: 0.00001341
Iteration 29/1000 | Loss: 0.00001340
Iteration 30/1000 | Loss: 0.00001340
Iteration 31/1000 | Loss: 0.00001340
Iteration 32/1000 | Loss: 0.00001330
Iteration 33/1000 | Loss: 0.00001319
Iteration 34/1000 | Loss: 0.00001318
Iteration 35/1000 | Loss: 0.00001317
Iteration 36/1000 | Loss: 0.00001317
Iteration 37/1000 | Loss: 0.00001316
Iteration 38/1000 | Loss: 0.00001316
Iteration 39/1000 | Loss: 0.00001311
Iteration 40/1000 | Loss: 0.00001311
Iteration 41/1000 | Loss: 0.00001311
Iteration 42/1000 | Loss: 0.00001311
Iteration 43/1000 | Loss: 0.00001308
Iteration 44/1000 | Loss: 0.00001308
Iteration 45/1000 | Loss: 0.00001307
Iteration 46/1000 | Loss: 0.00001307
Iteration 47/1000 | Loss: 0.00001306
Iteration 48/1000 | Loss: 0.00001304
Iteration 49/1000 | Loss: 0.00001304
Iteration 50/1000 | Loss: 0.00001303
Iteration 51/1000 | Loss: 0.00001303
Iteration 52/1000 | Loss: 0.00001303
Iteration 53/1000 | Loss: 0.00001302
Iteration 54/1000 | Loss: 0.00001302
Iteration 55/1000 | Loss: 0.00001302
Iteration 56/1000 | Loss: 0.00001301
Iteration 57/1000 | Loss: 0.00001301
Iteration 58/1000 | Loss: 0.00001300
Iteration 59/1000 | Loss: 0.00001300
Iteration 60/1000 | Loss: 0.00001300
Iteration 61/1000 | Loss: 0.00001299
Iteration 62/1000 | Loss: 0.00001299
Iteration 63/1000 | Loss: 0.00001299
Iteration 64/1000 | Loss: 0.00001298
Iteration 65/1000 | Loss: 0.00001297
Iteration 66/1000 | Loss: 0.00001297
Iteration 67/1000 | Loss: 0.00001297
Iteration 68/1000 | Loss: 0.00001297
Iteration 69/1000 | Loss: 0.00001297
Iteration 70/1000 | Loss: 0.00001297
Iteration 71/1000 | Loss: 0.00001296
Iteration 72/1000 | Loss: 0.00001296
Iteration 73/1000 | Loss: 0.00001295
Iteration 74/1000 | Loss: 0.00001294
Iteration 75/1000 | Loss: 0.00001294
Iteration 76/1000 | Loss: 0.00001293
Iteration 77/1000 | Loss: 0.00001292
Iteration 78/1000 | Loss: 0.00001292
Iteration 79/1000 | Loss: 0.00001291
Iteration 80/1000 | Loss: 0.00001291
Iteration 81/1000 | Loss: 0.00001291
Iteration 82/1000 | Loss: 0.00001291
Iteration 83/1000 | Loss: 0.00001291
Iteration 84/1000 | Loss: 0.00001291
Iteration 85/1000 | Loss: 0.00001291
Iteration 86/1000 | Loss: 0.00001291
Iteration 87/1000 | Loss: 0.00001289
Iteration 88/1000 | Loss: 0.00001289
Iteration 89/1000 | Loss: 0.00001288
Iteration 90/1000 | Loss: 0.00001288
Iteration 91/1000 | Loss: 0.00001288
Iteration 92/1000 | Loss: 0.00001288
Iteration 93/1000 | Loss: 0.00001287
Iteration 94/1000 | Loss: 0.00001287
Iteration 95/1000 | Loss: 0.00001287
Iteration 96/1000 | Loss: 0.00001287
Iteration 97/1000 | Loss: 0.00001286
Iteration 98/1000 | Loss: 0.00001286
Iteration 99/1000 | Loss: 0.00001286
Iteration 100/1000 | Loss: 0.00001286
Iteration 101/1000 | Loss: 0.00001286
Iteration 102/1000 | Loss: 0.00001286
Iteration 103/1000 | Loss: 0.00001285
Iteration 104/1000 | Loss: 0.00001285
Iteration 105/1000 | Loss: 0.00001285
Iteration 106/1000 | Loss: 0.00001284
Iteration 107/1000 | Loss: 0.00001284
Iteration 108/1000 | Loss: 0.00001284
Iteration 109/1000 | Loss: 0.00001283
Iteration 110/1000 | Loss: 0.00001282
Iteration 111/1000 | Loss: 0.00001282
Iteration 112/1000 | Loss: 0.00001282
Iteration 113/1000 | Loss: 0.00001282
Iteration 114/1000 | Loss: 0.00001282
Iteration 115/1000 | Loss: 0.00001281
Iteration 116/1000 | Loss: 0.00001281
Iteration 117/1000 | Loss: 0.00001281
Iteration 118/1000 | Loss: 0.00001280
Iteration 119/1000 | Loss: 0.00001280
Iteration 120/1000 | Loss: 0.00001280
Iteration 121/1000 | Loss: 0.00001280
Iteration 122/1000 | Loss: 0.00001280
Iteration 123/1000 | Loss: 0.00001280
Iteration 124/1000 | Loss: 0.00001280
Iteration 125/1000 | Loss: 0.00001280
Iteration 126/1000 | Loss: 0.00001280
Iteration 127/1000 | Loss: 0.00001279
Iteration 128/1000 | Loss: 0.00001279
Iteration 129/1000 | Loss: 0.00001279
Iteration 130/1000 | Loss: 0.00001279
Iteration 131/1000 | Loss: 0.00001279
Iteration 132/1000 | Loss: 0.00001279
Iteration 133/1000 | Loss: 0.00001279
Iteration 134/1000 | Loss: 0.00001279
Iteration 135/1000 | Loss: 0.00001279
Iteration 136/1000 | Loss: 0.00001279
Iteration 137/1000 | Loss: 0.00001279
Iteration 138/1000 | Loss: 0.00001279
Iteration 139/1000 | Loss: 0.00001279
Iteration 140/1000 | Loss: 0.00001279
Iteration 141/1000 | Loss: 0.00001279
Iteration 142/1000 | Loss: 0.00001279
Iteration 143/1000 | Loss: 0.00001279
Iteration 144/1000 | Loss: 0.00001279
Iteration 145/1000 | Loss: 0.00001279
Iteration 146/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.2792476809408981e-05, 1.2792476809408981e-05, 1.2792476809408981e-05, 1.2792476809408981e-05, 1.2792476809408981e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2792476809408981e-05

Optimization complete. Final v2v error: 3.0420498847961426 mm

Highest mean error: 3.110713243484497 mm for frame 125

Lowest mean error: 2.9047975540161133 mm for frame 221

Saving results

Total time: 41.910649061203
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073836
Iteration 2/25 | Loss: 0.00218920
Iteration 3/25 | Loss: 0.00177789
Iteration 4/25 | Loss: 0.00156878
Iteration 5/25 | Loss: 0.00143192
Iteration 6/25 | Loss: 0.00133942
Iteration 7/25 | Loss: 0.00130339
Iteration 8/25 | Loss: 0.00126515
Iteration 9/25 | Loss: 0.00124926
Iteration 10/25 | Loss: 0.00123198
Iteration 11/25 | Loss: 0.00122735
Iteration 12/25 | Loss: 0.00122216
Iteration 13/25 | Loss: 0.00122116
Iteration 14/25 | Loss: 0.00122076
Iteration 15/25 | Loss: 0.00122041
Iteration 16/25 | Loss: 0.00122023
Iteration 17/25 | Loss: 0.00122020
Iteration 18/25 | Loss: 0.00122020
Iteration 19/25 | Loss: 0.00122020
Iteration 20/25 | Loss: 0.00122019
Iteration 21/25 | Loss: 0.00122018
Iteration 22/25 | Loss: 0.00122018
Iteration 23/25 | Loss: 0.00122018
Iteration 24/25 | Loss: 0.00122018
Iteration 25/25 | Loss: 0.00122018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46295345
Iteration 2/25 | Loss: 0.00068926
Iteration 3/25 | Loss: 0.00068926
Iteration 4/25 | Loss: 0.00068926
Iteration 5/25 | Loss: 0.00068926
Iteration 6/25 | Loss: 0.00068926
Iteration 7/25 | Loss: 0.00068926
Iteration 8/25 | Loss: 0.00068926
Iteration 9/25 | Loss: 0.00068926
Iteration 10/25 | Loss: 0.00068926
Iteration 11/25 | Loss: 0.00068926
Iteration 12/25 | Loss: 0.00068926
Iteration 13/25 | Loss: 0.00068926
Iteration 14/25 | Loss: 0.00068926
Iteration 15/25 | Loss: 0.00068926
Iteration 16/25 | Loss: 0.00068926
Iteration 17/25 | Loss: 0.00068926
Iteration 18/25 | Loss: 0.00068926
Iteration 19/25 | Loss: 0.00068926
Iteration 20/25 | Loss: 0.00068926
Iteration 21/25 | Loss: 0.00068926
Iteration 22/25 | Loss: 0.00068926
Iteration 23/25 | Loss: 0.00068926
Iteration 24/25 | Loss: 0.00068926
Iteration 25/25 | Loss: 0.00068926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068926
Iteration 2/1000 | Loss: 0.00003350
Iteration 3/1000 | Loss: 0.00002541
Iteration 4/1000 | Loss: 0.00002352
Iteration 5/1000 | Loss: 0.00002262
Iteration 6/1000 | Loss: 0.00002197
Iteration 7/1000 | Loss: 0.00002145
Iteration 8/1000 | Loss: 0.00002116
Iteration 9/1000 | Loss: 0.00042421
Iteration 10/1000 | Loss: 0.00002407
Iteration 11/1000 | Loss: 0.00002109
Iteration 12/1000 | Loss: 0.00002009
Iteration 13/1000 | Loss: 0.00001937
Iteration 14/1000 | Loss: 0.00001899
Iteration 15/1000 | Loss: 0.00001873
Iteration 16/1000 | Loss: 0.00001862
Iteration 17/1000 | Loss: 0.00001861
Iteration 18/1000 | Loss: 0.00001860
Iteration 19/1000 | Loss: 0.00001859
Iteration 20/1000 | Loss: 0.00001859
Iteration 21/1000 | Loss: 0.00001853
Iteration 22/1000 | Loss: 0.00001835
Iteration 23/1000 | Loss: 0.00001831
Iteration 24/1000 | Loss: 0.00001828
Iteration 25/1000 | Loss: 0.00001827
Iteration 26/1000 | Loss: 0.00001814
Iteration 27/1000 | Loss: 0.00001812
Iteration 28/1000 | Loss: 0.00001808
Iteration 29/1000 | Loss: 0.00001807
Iteration 30/1000 | Loss: 0.00001807
Iteration 31/1000 | Loss: 0.00001806
Iteration 32/1000 | Loss: 0.00001805
Iteration 33/1000 | Loss: 0.00001804
Iteration 34/1000 | Loss: 0.00001804
Iteration 35/1000 | Loss: 0.00001803
Iteration 36/1000 | Loss: 0.00001803
Iteration 37/1000 | Loss: 0.00001802
Iteration 38/1000 | Loss: 0.00001802
Iteration 39/1000 | Loss: 0.00001802
Iteration 40/1000 | Loss: 0.00001801
Iteration 41/1000 | Loss: 0.00001801
Iteration 42/1000 | Loss: 0.00001801
Iteration 43/1000 | Loss: 0.00001800
Iteration 44/1000 | Loss: 0.00001800
Iteration 45/1000 | Loss: 0.00001799
Iteration 46/1000 | Loss: 0.00001799
Iteration 47/1000 | Loss: 0.00001798
Iteration 48/1000 | Loss: 0.00001798
Iteration 49/1000 | Loss: 0.00001798
Iteration 50/1000 | Loss: 0.00001797
Iteration 51/1000 | Loss: 0.00001797
Iteration 52/1000 | Loss: 0.00001796
Iteration 53/1000 | Loss: 0.00001796
Iteration 54/1000 | Loss: 0.00001796
Iteration 55/1000 | Loss: 0.00001795
Iteration 56/1000 | Loss: 0.00001795
Iteration 57/1000 | Loss: 0.00001795
Iteration 58/1000 | Loss: 0.00001795
Iteration 59/1000 | Loss: 0.00001794
Iteration 60/1000 | Loss: 0.00001794
Iteration 61/1000 | Loss: 0.00001794
Iteration 62/1000 | Loss: 0.00001794
Iteration 63/1000 | Loss: 0.00001793
Iteration 64/1000 | Loss: 0.00001793
Iteration 65/1000 | Loss: 0.00001793
Iteration 66/1000 | Loss: 0.00001793
Iteration 67/1000 | Loss: 0.00001792
Iteration 68/1000 | Loss: 0.00001792
Iteration 69/1000 | Loss: 0.00001791
Iteration 70/1000 | Loss: 0.00001791
Iteration 71/1000 | Loss: 0.00001791
Iteration 72/1000 | Loss: 0.00001791
Iteration 73/1000 | Loss: 0.00001791
Iteration 74/1000 | Loss: 0.00001790
Iteration 75/1000 | Loss: 0.00001790
Iteration 76/1000 | Loss: 0.00001790
Iteration 77/1000 | Loss: 0.00001790
Iteration 78/1000 | Loss: 0.00001790
Iteration 79/1000 | Loss: 0.00001790
Iteration 80/1000 | Loss: 0.00001790
Iteration 81/1000 | Loss: 0.00001789
Iteration 82/1000 | Loss: 0.00001789
Iteration 83/1000 | Loss: 0.00001789
Iteration 84/1000 | Loss: 0.00001789
Iteration 85/1000 | Loss: 0.00001789
Iteration 86/1000 | Loss: 0.00001789
Iteration 87/1000 | Loss: 0.00001789
Iteration 88/1000 | Loss: 0.00001789
Iteration 89/1000 | Loss: 0.00001789
Iteration 90/1000 | Loss: 0.00001789
Iteration 91/1000 | Loss: 0.00001789
Iteration 92/1000 | Loss: 0.00001789
Iteration 93/1000 | Loss: 0.00001789
Iteration 94/1000 | Loss: 0.00001789
Iteration 95/1000 | Loss: 0.00001789
Iteration 96/1000 | Loss: 0.00001788
Iteration 97/1000 | Loss: 0.00001788
Iteration 98/1000 | Loss: 0.00001788
Iteration 99/1000 | Loss: 0.00001788
Iteration 100/1000 | Loss: 0.00001788
Iteration 101/1000 | Loss: 0.00001788
Iteration 102/1000 | Loss: 0.00001787
Iteration 103/1000 | Loss: 0.00001787
Iteration 104/1000 | Loss: 0.00001787
Iteration 105/1000 | Loss: 0.00001787
Iteration 106/1000 | Loss: 0.00001787
Iteration 107/1000 | Loss: 0.00001787
Iteration 108/1000 | Loss: 0.00001787
Iteration 109/1000 | Loss: 0.00001787
Iteration 110/1000 | Loss: 0.00001786
Iteration 111/1000 | Loss: 0.00001786
Iteration 112/1000 | Loss: 0.00001786
Iteration 113/1000 | Loss: 0.00001786
Iteration 114/1000 | Loss: 0.00001786
Iteration 115/1000 | Loss: 0.00001786
Iteration 116/1000 | Loss: 0.00001786
Iteration 117/1000 | Loss: 0.00001786
Iteration 118/1000 | Loss: 0.00001786
Iteration 119/1000 | Loss: 0.00001786
Iteration 120/1000 | Loss: 0.00001786
Iteration 121/1000 | Loss: 0.00001786
Iteration 122/1000 | Loss: 0.00001786
Iteration 123/1000 | Loss: 0.00001786
Iteration 124/1000 | Loss: 0.00001785
Iteration 125/1000 | Loss: 0.00001785
Iteration 126/1000 | Loss: 0.00001785
Iteration 127/1000 | Loss: 0.00001785
Iteration 128/1000 | Loss: 0.00001785
Iteration 129/1000 | Loss: 0.00001785
Iteration 130/1000 | Loss: 0.00001785
Iteration 131/1000 | Loss: 0.00001785
Iteration 132/1000 | Loss: 0.00001785
Iteration 133/1000 | Loss: 0.00001785
Iteration 134/1000 | Loss: 0.00001785
Iteration 135/1000 | Loss: 0.00001785
Iteration 136/1000 | Loss: 0.00001785
Iteration 137/1000 | Loss: 0.00001785
Iteration 138/1000 | Loss: 0.00001784
Iteration 139/1000 | Loss: 0.00001784
Iteration 140/1000 | Loss: 0.00001784
Iteration 141/1000 | Loss: 0.00001784
Iteration 142/1000 | Loss: 0.00001784
Iteration 143/1000 | Loss: 0.00001784
Iteration 144/1000 | Loss: 0.00001784
Iteration 145/1000 | Loss: 0.00001784
Iteration 146/1000 | Loss: 0.00001784
Iteration 147/1000 | Loss: 0.00001784
Iteration 148/1000 | Loss: 0.00001784
Iteration 149/1000 | Loss: 0.00001784
Iteration 150/1000 | Loss: 0.00001784
Iteration 151/1000 | Loss: 0.00001784
Iteration 152/1000 | Loss: 0.00001784
Iteration 153/1000 | Loss: 0.00001784
Iteration 154/1000 | Loss: 0.00001783
Iteration 155/1000 | Loss: 0.00001783
Iteration 156/1000 | Loss: 0.00001783
Iteration 157/1000 | Loss: 0.00001783
Iteration 158/1000 | Loss: 0.00001783
Iteration 159/1000 | Loss: 0.00001783
Iteration 160/1000 | Loss: 0.00001783
Iteration 161/1000 | Loss: 0.00001783
Iteration 162/1000 | Loss: 0.00001783
Iteration 163/1000 | Loss: 0.00001783
Iteration 164/1000 | Loss: 0.00001783
Iteration 165/1000 | Loss: 0.00001783
Iteration 166/1000 | Loss: 0.00001783
Iteration 167/1000 | Loss: 0.00001783
Iteration 168/1000 | Loss: 0.00001783
Iteration 169/1000 | Loss: 0.00001783
Iteration 170/1000 | Loss: 0.00001783
Iteration 171/1000 | Loss: 0.00001783
Iteration 172/1000 | Loss: 0.00001783
Iteration 173/1000 | Loss: 0.00001783
Iteration 174/1000 | Loss: 0.00001783
Iteration 175/1000 | Loss: 0.00001783
Iteration 176/1000 | Loss: 0.00001783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.7826585462898947e-05, 1.7826585462898947e-05, 1.7826585462898947e-05, 1.7826585462898947e-05, 1.7826585462898947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7826585462898947e-05

Optimization complete. Final v2v error: 3.5286567211151123 mm

Highest mean error: 4.734792232513428 mm for frame 67

Lowest mean error: 3.1544272899627686 mm for frame 44

Saving results

Total time: 66.41451358795166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00550964
Iteration 2/25 | Loss: 0.00152469
Iteration 3/25 | Loss: 0.00131614
Iteration 4/25 | Loss: 0.00130303
Iteration 5/25 | Loss: 0.00129998
Iteration 6/25 | Loss: 0.00129905
Iteration 7/25 | Loss: 0.00129898
Iteration 8/25 | Loss: 0.00129898
Iteration 9/25 | Loss: 0.00129898
Iteration 10/25 | Loss: 0.00129898
Iteration 11/25 | Loss: 0.00129898
Iteration 12/25 | Loss: 0.00129898
Iteration 13/25 | Loss: 0.00129898
Iteration 14/25 | Loss: 0.00129898
Iteration 15/25 | Loss: 0.00129898
Iteration 16/25 | Loss: 0.00129898
Iteration 17/25 | Loss: 0.00129898
Iteration 18/25 | Loss: 0.00129898
Iteration 19/25 | Loss: 0.00129898
Iteration 20/25 | Loss: 0.00129898
Iteration 21/25 | Loss: 0.00129898
Iteration 22/25 | Loss: 0.00129898
Iteration 23/25 | Loss: 0.00129898
Iteration 24/25 | Loss: 0.00129898
Iteration 25/25 | Loss: 0.00129898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80645013
Iteration 2/25 | Loss: 0.00076271
Iteration 3/25 | Loss: 0.00076271
Iteration 4/25 | Loss: 0.00076271
Iteration 5/25 | Loss: 0.00076271
Iteration 6/25 | Loss: 0.00076271
Iteration 7/25 | Loss: 0.00076271
Iteration 8/25 | Loss: 0.00076270
Iteration 9/25 | Loss: 0.00076270
Iteration 10/25 | Loss: 0.00076270
Iteration 11/25 | Loss: 0.00076270
Iteration 12/25 | Loss: 0.00076270
Iteration 13/25 | Loss: 0.00076270
Iteration 14/25 | Loss: 0.00076270
Iteration 15/25 | Loss: 0.00076270
Iteration 16/25 | Loss: 0.00076270
Iteration 17/25 | Loss: 0.00076270
Iteration 18/25 | Loss: 0.00076270
Iteration 19/25 | Loss: 0.00076270
Iteration 20/25 | Loss: 0.00076270
Iteration 21/25 | Loss: 0.00076270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007627040613442659, 0.0007627040613442659, 0.0007627040613442659, 0.0007627040613442659, 0.0007627040613442659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007627040613442659

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076270
Iteration 2/1000 | Loss: 0.00004408
Iteration 3/1000 | Loss: 0.00003081
Iteration 4/1000 | Loss: 0.00002670
Iteration 5/1000 | Loss: 0.00002542
Iteration 6/1000 | Loss: 0.00002450
Iteration 7/1000 | Loss: 0.00002382
Iteration 8/1000 | Loss: 0.00002334
Iteration 9/1000 | Loss: 0.00002298
Iteration 10/1000 | Loss: 0.00002275
Iteration 11/1000 | Loss: 0.00002237
Iteration 12/1000 | Loss: 0.00002208
Iteration 13/1000 | Loss: 0.00002179
Iteration 14/1000 | Loss: 0.00002153
Iteration 15/1000 | Loss: 0.00002132
Iteration 16/1000 | Loss: 0.00002113
Iteration 17/1000 | Loss: 0.00002092
Iteration 18/1000 | Loss: 0.00002078
Iteration 19/1000 | Loss: 0.00002076
Iteration 20/1000 | Loss: 0.00002068
Iteration 21/1000 | Loss: 0.00002064
Iteration 22/1000 | Loss: 0.00002058
Iteration 23/1000 | Loss: 0.00002053
Iteration 24/1000 | Loss: 0.00002052
Iteration 25/1000 | Loss: 0.00002048
Iteration 26/1000 | Loss: 0.00002048
Iteration 27/1000 | Loss: 0.00002048
Iteration 28/1000 | Loss: 0.00002048
Iteration 29/1000 | Loss: 0.00002048
Iteration 30/1000 | Loss: 0.00002048
Iteration 31/1000 | Loss: 0.00002048
Iteration 32/1000 | Loss: 0.00002048
Iteration 33/1000 | Loss: 0.00002048
Iteration 34/1000 | Loss: 0.00002048
Iteration 35/1000 | Loss: 0.00002048
Iteration 36/1000 | Loss: 0.00002048
Iteration 37/1000 | Loss: 0.00002047
Iteration 38/1000 | Loss: 0.00002047
Iteration 39/1000 | Loss: 0.00002046
Iteration 40/1000 | Loss: 0.00002045
Iteration 41/1000 | Loss: 0.00002044
Iteration 42/1000 | Loss: 0.00002043
Iteration 43/1000 | Loss: 0.00002043
Iteration 44/1000 | Loss: 0.00002043
Iteration 45/1000 | Loss: 0.00002043
Iteration 46/1000 | Loss: 0.00002043
Iteration 47/1000 | Loss: 0.00002042
Iteration 48/1000 | Loss: 0.00002042
Iteration 49/1000 | Loss: 0.00002042
Iteration 50/1000 | Loss: 0.00002042
Iteration 51/1000 | Loss: 0.00002041
Iteration 52/1000 | Loss: 0.00002041
Iteration 53/1000 | Loss: 0.00002041
Iteration 54/1000 | Loss: 0.00002040
Iteration 55/1000 | Loss: 0.00002040
Iteration 56/1000 | Loss: 0.00002040
Iteration 57/1000 | Loss: 0.00002039
Iteration 58/1000 | Loss: 0.00002039
Iteration 59/1000 | Loss: 0.00002039
Iteration 60/1000 | Loss: 0.00002039
Iteration 61/1000 | Loss: 0.00002038
Iteration 62/1000 | Loss: 0.00002038
Iteration 63/1000 | Loss: 0.00002037
Iteration 64/1000 | Loss: 0.00002037
Iteration 65/1000 | Loss: 0.00002037
Iteration 66/1000 | Loss: 0.00002036
Iteration 67/1000 | Loss: 0.00002035
Iteration 68/1000 | Loss: 0.00002034
Iteration 69/1000 | Loss: 0.00002033
Iteration 70/1000 | Loss: 0.00002033
Iteration 71/1000 | Loss: 0.00002033
Iteration 72/1000 | Loss: 0.00002033
Iteration 73/1000 | Loss: 0.00002033
Iteration 74/1000 | Loss: 0.00002030
Iteration 75/1000 | Loss: 0.00002030
Iteration 76/1000 | Loss: 0.00002029
Iteration 77/1000 | Loss: 0.00002029
Iteration 78/1000 | Loss: 0.00002029
Iteration 79/1000 | Loss: 0.00002029
Iteration 80/1000 | Loss: 0.00002026
Iteration 81/1000 | Loss: 0.00002026
Iteration 82/1000 | Loss: 0.00002026
Iteration 83/1000 | Loss: 0.00002026
Iteration 84/1000 | Loss: 0.00002026
Iteration 85/1000 | Loss: 0.00002026
Iteration 86/1000 | Loss: 0.00002025
Iteration 87/1000 | Loss: 0.00002025
Iteration 88/1000 | Loss: 0.00002024
Iteration 89/1000 | Loss: 0.00002023
Iteration 90/1000 | Loss: 0.00002023
Iteration 91/1000 | Loss: 0.00002022
Iteration 92/1000 | Loss: 0.00002022
Iteration 93/1000 | Loss: 0.00002022
Iteration 94/1000 | Loss: 0.00002022
Iteration 95/1000 | Loss: 0.00002022
Iteration 96/1000 | Loss: 0.00002022
Iteration 97/1000 | Loss: 0.00002022
Iteration 98/1000 | Loss: 0.00002022
Iteration 99/1000 | Loss: 0.00002021
Iteration 100/1000 | Loss: 0.00002021
Iteration 101/1000 | Loss: 0.00002021
Iteration 102/1000 | Loss: 0.00002021
Iteration 103/1000 | Loss: 0.00002021
Iteration 104/1000 | Loss: 0.00002021
Iteration 105/1000 | Loss: 0.00002020
Iteration 106/1000 | Loss: 0.00002020
Iteration 107/1000 | Loss: 0.00002020
Iteration 108/1000 | Loss: 0.00002020
Iteration 109/1000 | Loss: 0.00002020
Iteration 110/1000 | Loss: 0.00002019
Iteration 111/1000 | Loss: 0.00002019
Iteration 112/1000 | Loss: 0.00002019
Iteration 113/1000 | Loss: 0.00002019
Iteration 114/1000 | Loss: 0.00002019
Iteration 115/1000 | Loss: 0.00002019
Iteration 116/1000 | Loss: 0.00002019
Iteration 117/1000 | Loss: 0.00002019
Iteration 118/1000 | Loss: 0.00002018
Iteration 119/1000 | Loss: 0.00002018
Iteration 120/1000 | Loss: 0.00002018
Iteration 121/1000 | Loss: 0.00002018
Iteration 122/1000 | Loss: 0.00002017
Iteration 123/1000 | Loss: 0.00002017
Iteration 124/1000 | Loss: 0.00002016
Iteration 125/1000 | Loss: 0.00002016
Iteration 126/1000 | Loss: 0.00002016
Iteration 127/1000 | Loss: 0.00002016
Iteration 128/1000 | Loss: 0.00002016
Iteration 129/1000 | Loss: 0.00002015
Iteration 130/1000 | Loss: 0.00002015
Iteration 131/1000 | Loss: 0.00002015
Iteration 132/1000 | Loss: 0.00002015
Iteration 133/1000 | Loss: 0.00002015
Iteration 134/1000 | Loss: 0.00002014
Iteration 135/1000 | Loss: 0.00002014
Iteration 136/1000 | Loss: 0.00002014
Iteration 137/1000 | Loss: 0.00002014
Iteration 138/1000 | Loss: 0.00002014
Iteration 139/1000 | Loss: 0.00002014
Iteration 140/1000 | Loss: 0.00002013
Iteration 141/1000 | Loss: 0.00002013
Iteration 142/1000 | Loss: 0.00002013
Iteration 143/1000 | Loss: 0.00002013
Iteration 144/1000 | Loss: 0.00002013
Iteration 145/1000 | Loss: 0.00002013
Iteration 146/1000 | Loss: 0.00002013
Iteration 147/1000 | Loss: 0.00002013
Iteration 148/1000 | Loss: 0.00002013
Iteration 149/1000 | Loss: 0.00002013
Iteration 150/1000 | Loss: 0.00002012
Iteration 151/1000 | Loss: 0.00002012
Iteration 152/1000 | Loss: 0.00002012
Iteration 153/1000 | Loss: 0.00002012
Iteration 154/1000 | Loss: 0.00002012
Iteration 155/1000 | Loss: 0.00002011
Iteration 156/1000 | Loss: 0.00002011
Iteration 157/1000 | Loss: 0.00002011
Iteration 158/1000 | Loss: 0.00002011
Iteration 159/1000 | Loss: 0.00002010
Iteration 160/1000 | Loss: 0.00002010
Iteration 161/1000 | Loss: 0.00002010
Iteration 162/1000 | Loss: 0.00002010
Iteration 163/1000 | Loss: 0.00002009
Iteration 164/1000 | Loss: 0.00002009
Iteration 165/1000 | Loss: 0.00002009
Iteration 166/1000 | Loss: 0.00002009
Iteration 167/1000 | Loss: 0.00002009
Iteration 168/1000 | Loss: 0.00002009
Iteration 169/1000 | Loss: 0.00002009
Iteration 170/1000 | Loss: 0.00002008
Iteration 171/1000 | Loss: 0.00002008
Iteration 172/1000 | Loss: 0.00002008
Iteration 173/1000 | Loss: 0.00002008
Iteration 174/1000 | Loss: 0.00002007
Iteration 175/1000 | Loss: 0.00002007
Iteration 176/1000 | Loss: 0.00002007
Iteration 177/1000 | Loss: 0.00002007
Iteration 178/1000 | Loss: 0.00002007
Iteration 179/1000 | Loss: 0.00002007
Iteration 180/1000 | Loss: 0.00002007
Iteration 181/1000 | Loss: 0.00002007
Iteration 182/1000 | Loss: 0.00002007
Iteration 183/1000 | Loss: 0.00002007
Iteration 184/1000 | Loss: 0.00002007
Iteration 185/1000 | Loss: 0.00002007
Iteration 186/1000 | Loss: 0.00002007
Iteration 187/1000 | Loss: 0.00002007
Iteration 188/1000 | Loss: 0.00002007
Iteration 189/1000 | Loss: 0.00002007
Iteration 190/1000 | Loss: 0.00002007
Iteration 191/1000 | Loss: 0.00002007
Iteration 192/1000 | Loss: 0.00002007
Iteration 193/1000 | Loss: 0.00002006
Iteration 194/1000 | Loss: 0.00002006
Iteration 195/1000 | Loss: 0.00002006
Iteration 196/1000 | Loss: 0.00002006
Iteration 197/1000 | Loss: 0.00002006
Iteration 198/1000 | Loss: 0.00002006
Iteration 199/1000 | Loss: 0.00002006
Iteration 200/1000 | Loss: 0.00002006
Iteration 201/1000 | Loss: 0.00002006
Iteration 202/1000 | Loss: 0.00002006
Iteration 203/1000 | Loss: 0.00002006
Iteration 204/1000 | Loss: 0.00002006
Iteration 205/1000 | Loss: 0.00002006
Iteration 206/1000 | Loss: 0.00002006
Iteration 207/1000 | Loss: 0.00002006
Iteration 208/1000 | Loss: 0.00002006
Iteration 209/1000 | Loss: 0.00002005
Iteration 210/1000 | Loss: 0.00002005
Iteration 211/1000 | Loss: 0.00002005
Iteration 212/1000 | Loss: 0.00002005
Iteration 213/1000 | Loss: 0.00002005
Iteration 214/1000 | Loss: 0.00002005
Iteration 215/1000 | Loss: 0.00002005
Iteration 216/1000 | Loss: 0.00002005
Iteration 217/1000 | Loss: 0.00002005
Iteration 218/1000 | Loss: 0.00002005
Iteration 219/1000 | Loss: 0.00002005
Iteration 220/1000 | Loss: 0.00002005
Iteration 221/1000 | Loss: 0.00002004
Iteration 222/1000 | Loss: 0.00002004
Iteration 223/1000 | Loss: 0.00002004
Iteration 224/1000 | Loss: 0.00002004
Iteration 225/1000 | Loss: 0.00002004
Iteration 226/1000 | Loss: 0.00002004
Iteration 227/1000 | Loss: 0.00002004
Iteration 228/1000 | Loss: 0.00002004
Iteration 229/1000 | Loss: 0.00002004
Iteration 230/1000 | Loss: 0.00002004
Iteration 231/1000 | Loss: 0.00002004
Iteration 232/1000 | Loss: 0.00002004
Iteration 233/1000 | Loss: 0.00002004
Iteration 234/1000 | Loss: 0.00002004
Iteration 235/1000 | Loss: 0.00002003
Iteration 236/1000 | Loss: 0.00002003
Iteration 237/1000 | Loss: 0.00002003
Iteration 238/1000 | Loss: 0.00002003
Iteration 239/1000 | Loss: 0.00002003
Iteration 240/1000 | Loss: 0.00002003
Iteration 241/1000 | Loss: 0.00002003
Iteration 242/1000 | Loss: 0.00002003
Iteration 243/1000 | Loss: 0.00002003
Iteration 244/1000 | Loss: 0.00002003
Iteration 245/1000 | Loss: 0.00002003
Iteration 246/1000 | Loss: 0.00002002
Iteration 247/1000 | Loss: 0.00002002
Iteration 248/1000 | Loss: 0.00002002
Iteration 249/1000 | Loss: 0.00002002
Iteration 250/1000 | Loss: 0.00002002
Iteration 251/1000 | Loss: 0.00002002
Iteration 252/1000 | Loss: 0.00002002
Iteration 253/1000 | Loss: 0.00002002
Iteration 254/1000 | Loss: 0.00002002
Iteration 255/1000 | Loss: 0.00002002
Iteration 256/1000 | Loss: 0.00002002
Iteration 257/1000 | Loss: 0.00002002
Iteration 258/1000 | Loss: 0.00002002
Iteration 259/1000 | Loss: 0.00002002
Iteration 260/1000 | Loss: 0.00002002
Iteration 261/1000 | Loss: 0.00002002
Iteration 262/1000 | Loss: 0.00002002
Iteration 263/1000 | Loss: 0.00002001
Iteration 264/1000 | Loss: 0.00002001
Iteration 265/1000 | Loss: 0.00002001
Iteration 266/1000 | Loss: 0.00002001
Iteration 267/1000 | Loss: 0.00002001
Iteration 268/1000 | Loss: 0.00002001
Iteration 269/1000 | Loss: 0.00002001
Iteration 270/1000 | Loss: 0.00002001
Iteration 271/1000 | Loss: 0.00002001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [2.001379289140459e-05, 2.001379289140459e-05, 2.001379289140459e-05, 2.001379289140459e-05, 2.001379289140459e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.001379289140459e-05

Optimization complete. Final v2v error: 3.693835973739624 mm

Highest mean error: 4.12778377532959 mm for frame 9

Lowest mean error: 3.4314181804656982 mm for frame 44

Saving results

Total time: 55.23777437210083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_003/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_003/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081351
Iteration 2/25 | Loss: 0.00199385
Iteration 3/25 | Loss: 0.00149751
Iteration 4/25 | Loss: 0.00143188
Iteration 5/25 | Loss: 0.00141859
Iteration 6/25 | Loss: 0.00141523
Iteration 7/25 | Loss: 0.00141478
Iteration 8/25 | Loss: 0.00141478
Iteration 9/25 | Loss: 0.00141478
Iteration 10/25 | Loss: 0.00141478
Iteration 11/25 | Loss: 0.00141478
Iteration 12/25 | Loss: 0.00141478
Iteration 13/25 | Loss: 0.00141478
Iteration 14/25 | Loss: 0.00141478
Iteration 15/25 | Loss: 0.00141478
Iteration 16/25 | Loss: 0.00141478
Iteration 17/25 | Loss: 0.00141478
Iteration 18/25 | Loss: 0.00141478
Iteration 19/25 | Loss: 0.00141478
Iteration 20/25 | Loss: 0.00141478
Iteration 21/25 | Loss: 0.00141478
Iteration 22/25 | Loss: 0.00141478
Iteration 23/25 | Loss: 0.00141478
Iteration 24/25 | Loss: 0.00141478
Iteration 25/25 | Loss: 0.00141478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92586040
Iteration 2/25 | Loss: 0.00090505
Iteration 3/25 | Loss: 0.00090491
Iteration 4/25 | Loss: 0.00090491
Iteration 5/25 | Loss: 0.00090491
Iteration 6/25 | Loss: 0.00090491
Iteration 7/25 | Loss: 0.00090491
Iteration 8/25 | Loss: 0.00090491
Iteration 9/25 | Loss: 0.00090491
Iteration 10/25 | Loss: 0.00090491
Iteration 11/25 | Loss: 0.00090491
Iteration 12/25 | Loss: 0.00090491
Iteration 13/25 | Loss: 0.00090491
Iteration 14/25 | Loss: 0.00090491
Iteration 15/25 | Loss: 0.00090491
Iteration 16/25 | Loss: 0.00090491
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009049080545082688, 0.0009049080545082688, 0.0009049080545082688, 0.0009049080545082688, 0.0009049080545082688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009049080545082688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090491
Iteration 2/1000 | Loss: 0.00007956
Iteration 3/1000 | Loss: 0.00005419
Iteration 4/1000 | Loss: 0.00004622
Iteration 5/1000 | Loss: 0.00004459
Iteration 6/1000 | Loss: 0.00004305
Iteration 7/1000 | Loss: 0.00004161
Iteration 8/1000 | Loss: 0.00004076
Iteration 9/1000 | Loss: 0.00004022
Iteration 10/1000 | Loss: 0.00003981
Iteration 11/1000 | Loss: 0.00003953
Iteration 12/1000 | Loss: 0.00003925
Iteration 13/1000 | Loss: 0.00003908
Iteration 14/1000 | Loss: 0.00003892
Iteration 15/1000 | Loss: 0.00003891
Iteration 16/1000 | Loss: 0.00003877
Iteration 17/1000 | Loss: 0.00003872
Iteration 18/1000 | Loss: 0.00003866
Iteration 19/1000 | Loss: 0.00003866
Iteration 20/1000 | Loss: 0.00003863
Iteration 21/1000 | Loss: 0.00003863
Iteration 22/1000 | Loss: 0.00003863
Iteration 23/1000 | Loss: 0.00003863
Iteration 24/1000 | Loss: 0.00003861
Iteration 25/1000 | Loss: 0.00003859
Iteration 26/1000 | Loss: 0.00003859
Iteration 27/1000 | Loss: 0.00003859
Iteration 28/1000 | Loss: 0.00003859
Iteration 29/1000 | Loss: 0.00003859
Iteration 30/1000 | Loss: 0.00003859
Iteration 31/1000 | Loss: 0.00003859
Iteration 32/1000 | Loss: 0.00003859
Iteration 33/1000 | Loss: 0.00003858
Iteration 34/1000 | Loss: 0.00003858
Iteration 35/1000 | Loss: 0.00003858
Iteration 36/1000 | Loss: 0.00003857
Iteration 37/1000 | Loss: 0.00003856
Iteration 38/1000 | Loss: 0.00003854
Iteration 39/1000 | Loss: 0.00003854
Iteration 40/1000 | Loss: 0.00003854
Iteration 41/1000 | Loss: 0.00003853
Iteration 42/1000 | Loss: 0.00003852
Iteration 43/1000 | Loss: 0.00003851
Iteration 44/1000 | Loss: 0.00003851
Iteration 45/1000 | Loss: 0.00003851
Iteration 46/1000 | Loss: 0.00003851
Iteration 47/1000 | Loss: 0.00003850
Iteration 48/1000 | Loss: 0.00003850
Iteration 49/1000 | Loss: 0.00003850
Iteration 50/1000 | Loss: 0.00003849
Iteration 51/1000 | Loss: 0.00003849
Iteration 52/1000 | Loss: 0.00003848
Iteration 53/1000 | Loss: 0.00003845
Iteration 54/1000 | Loss: 0.00003845
Iteration 55/1000 | Loss: 0.00003844
Iteration 56/1000 | Loss: 0.00003844
Iteration 57/1000 | Loss: 0.00003843
Iteration 58/1000 | Loss: 0.00003843
Iteration 59/1000 | Loss: 0.00003842
Iteration 60/1000 | Loss: 0.00003841
Iteration 61/1000 | Loss: 0.00003841
Iteration 62/1000 | Loss: 0.00003840
Iteration 63/1000 | Loss: 0.00003840
Iteration 64/1000 | Loss: 0.00003840
Iteration 65/1000 | Loss: 0.00003839
Iteration 66/1000 | Loss: 0.00003839
Iteration 67/1000 | Loss: 0.00003839
Iteration 68/1000 | Loss: 0.00003839
Iteration 69/1000 | Loss: 0.00003839
Iteration 70/1000 | Loss: 0.00003839
Iteration 71/1000 | Loss: 0.00003838
Iteration 72/1000 | Loss: 0.00003838
Iteration 73/1000 | Loss: 0.00003838
Iteration 74/1000 | Loss: 0.00003838
Iteration 75/1000 | Loss: 0.00003838
Iteration 76/1000 | Loss: 0.00003837
Iteration 77/1000 | Loss: 0.00003837
Iteration 78/1000 | Loss: 0.00003837
Iteration 79/1000 | Loss: 0.00003836
Iteration 80/1000 | Loss: 0.00003836
Iteration 81/1000 | Loss: 0.00003836
Iteration 82/1000 | Loss: 0.00003835
Iteration 83/1000 | Loss: 0.00003835
Iteration 84/1000 | Loss: 0.00003835
Iteration 85/1000 | Loss: 0.00003835
Iteration 86/1000 | Loss: 0.00003835
Iteration 87/1000 | Loss: 0.00003835
Iteration 88/1000 | Loss: 0.00003835
Iteration 89/1000 | Loss: 0.00003835
Iteration 90/1000 | Loss: 0.00003835
Iteration 91/1000 | Loss: 0.00003835
Iteration 92/1000 | Loss: 0.00003835
Iteration 93/1000 | Loss: 0.00003835
Iteration 94/1000 | Loss: 0.00003834
Iteration 95/1000 | Loss: 0.00003834
Iteration 96/1000 | Loss: 0.00003834
Iteration 97/1000 | Loss: 0.00003834
Iteration 98/1000 | Loss: 0.00003833
Iteration 99/1000 | Loss: 0.00003833
Iteration 100/1000 | Loss: 0.00003833
Iteration 101/1000 | Loss: 0.00003833
Iteration 102/1000 | Loss: 0.00003833
Iteration 103/1000 | Loss: 0.00003833
Iteration 104/1000 | Loss: 0.00003833
Iteration 105/1000 | Loss: 0.00003832
Iteration 106/1000 | Loss: 0.00003832
Iteration 107/1000 | Loss: 0.00003832
Iteration 108/1000 | Loss: 0.00003832
Iteration 109/1000 | Loss: 0.00003832
Iteration 110/1000 | Loss: 0.00003832
Iteration 111/1000 | Loss: 0.00003831
Iteration 112/1000 | Loss: 0.00003831
Iteration 113/1000 | Loss: 0.00003831
Iteration 114/1000 | Loss: 0.00003831
Iteration 115/1000 | Loss: 0.00003831
Iteration 116/1000 | Loss: 0.00003831
Iteration 117/1000 | Loss: 0.00003831
Iteration 118/1000 | Loss: 0.00003830
Iteration 119/1000 | Loss: 0.00003830
Iteration 120/1000 | Loss: 0.00003830
Iteration 121/1000 | Loss: 0.00003830
Iteration 122/1000 | Loss: 0.00003830
Iteration 123/1000 | Loss: 0.00003830
Iteration 124/1000 | Loss: 0.00003830
Iteration 125/1000 | Loss: 0.00003830
Iteration 126/1000 | Loss: 0.00003830
Iteration 127/1000 | Loss: 0.00003830
Iteration 128/1000 | Loss: 0.00003830
Iteration 129/1000 | Loss: 0.00003830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [3.829778506769799e-05, 3.829778506769799e-05, 3.829778506769799e-05, 3.829778506769799e-05, 3.829778506769799e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.829778506769799e-05

Optimization complete. Final v2v error: 5.096834182739258 mm

Highest mean error: 5.851987361907959 mm for frame 232

Lowest mean error: 4.258662700653076 mm for frame 162

Saving results

Total time: 47.95129370689392
