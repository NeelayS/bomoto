Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=47, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2632-2687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868573
Iteration 2/25 | Loss: 0.00138436
Iteration 3/25 | Loss: 0.00090508
Iteration 4/25 | Loss: 0.00072260
Iteration 5/25 | Loss: 0.00069306
Iteration 6/25 | Loss: 0.00068624
Iteration 7/25 | Loss: 0.00068550
Iteration 8/25 | Loss: 0.00068550
Iteration 9/25 | Loss: 0.00068550
Iteration 10/25 | Loss: 0.00068550
Iteration 11/25 | Loss: 0.00068550
Iteration 12/25 | Loss: 0.00068550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006855020183138549, 0.0006855020183138549, 0.0006855020183138549, 0.0006855020183138549, 0.0006855020183138549]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006855020183138549

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41077971
Iteration 2/25 | Loss: 0.00015805
Iteration 3/25 | Loss: 0.00015805
Iteration 4/25 | Loss: 0.00015805
Iteration 5/25 | Loss: 0.00015805
Iteration 6/25 | Loss: 0.00015805
Iteration 7/25 | Loss: 0.00015805
Iteration 8/25 | Loss: 0.00015805
Iteration 9/25 | Loss: 0.00015805
Iteration 10/25 | Loss: 0.00015805
Iteration 11/25 | Loss: 0.00015805
Iteration 12/25 | Loss: 0.00015805
Iteration 13/25 | Loss: 0.00015805
Iteration 14/25 | Loss: 0.00015805
Iteration 15/25 | Loss: 0.00015805
Iteration 16/25 | Loss: 0.00015805
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00015804833674337715, 0.00015804833674337715, 0.00015804833674337715, 0.00015804833674337715, 0.00015804833674337715]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00015804833674337715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015805
Iteration 2/1000 | Loss: 0.00003314
Iteration 3/1000 | Loss: 0.00002772
Iteration 4/1000 | Loss: 0.00002524
Iteration 5/1000 | Loss: 0.00002420
Iteration 6/1000 | Loss: 0.00002340
Iteration 7/1000 | Loss: 0.00002286
Iteration 8/1000 | Loss: 0.00002245
Iteration 9/1000 | Loss: 0.00002214
Iteration 10/1000 | Loss: 0.00002175
Iteration 11/1000 | Loss: 0.00002158
Iteration 12/1000 | Loss: 0.00002142
Iteration 13/1000 | Loss: 0.00002131
Iteration 14/1000 | Loss: 0.00002119
Iteration 15/1000 | Loss: 0.00002118
Iteration 16/1000 | Loss: 0.00002118
Iteration 17/1000 | Loss: 0.00002118
Iteration 18/1000 | Loss: 0.00002112
Iteration 19/1000 | Loss: 0.00002110
Iteration 20/1000 | Loss: 0.00002107
Iteration 21/1000 | Loss: 0.00002104
Iteration 22/1000 | Loss: 0.00002104
Iteration 23/1000 | Loss: 0.00002104
Iteration 24/1000 | Loss: 0.00002104
Iteration 25/1000 | Loss: 0.00002103
Iteration 26/1000 | Loss: 0.00002101
Iteration 27/1000 | Loss: 0.00002101
Iteration 28/1000 | Loss: 0.00002100
Iteration 29/1000 | Loss: 0.00002100
Iteration 30/1000 | Loss: 0.00002100
Iteration 31/1000 | Loss: 0.00002100
Iteration 32/1000 | Loss: 0.00002099
Iteration 33/1000 | Loss: 0.00002099
Iteration 34/1000 | Loss: 0.00002099
Iteration 35/1000 | Loss: 0.00002099
Iteration 36/1000 | Loss: 0.00002099
Iteration 37/1000 | Loss: 0.00002099
Iteration 38/1000 | Loss: 0.00002099
Iteration 39/1000 | Loss: 0.00002099
Iteration 40/1000 | Loss: 0.00002099
Iteration 41/1000 | Loss: 0.00002099
Iteration 42/1000 | Loss: 0.00002098
Iteration 43/1000 | Loss: 0.00002098
Iteration 44/1000 | Loss: 0.00002097
Iteration 45/1000 | Loss: 0.00002097
Iteration 46/1000 | Loss: 0.00002097
Iteration 47/1000 | Loss: 0.00002097
Iteration 48/1000 | Loss: 0.00002097
Iteration 49/1000 | Loss: 0.00002097
Iteration 50/1000 | Loss: 0.00002097
Iteration 51/1000 | Loss: 0.00002097
Iteration 52/1000 | Loss: 0.00002096
Iteration 53/1000 | Loss: 0.00002096
Iteration 54/1000 | Loss: 0.00002095
Iteration 55/1000 | Loss: 0.00002094
Iteration 56/1000 | Loss: 0.00002094
Iteration 57/1000 | Loss: 0.00002094
Iteration 58/1000 | Loss: 0.00002094
Iteration 59/1000 | Loss: 0.00002094
Iteration 60/1000 | Loss: 0.00002094
Iteration 61/1000 | Loss: 0.00002094
Iteration 62/1000 | Loss: 0.00002094
Iteration 63/1000 | Loss: 0.00002093
Iteration 64/1000 | Loss: 0.00002093
Iteration 65/1000 | Loss: 0.00002093
Iteration 66/1000 | Loss: 0.00002093
Iteration 67/1000 | Loss: 0.00002093
Iteration 68/1000 | Loss: 0.00002093
Iteration 69/1000 | Loss: 0.00002093
Iteration 70/1000 | Loss: 0.00002093
Iteration 71/1000 | Loss: 0.00002093
Iteration 72/1000 | Loss: 0.00002092
Iteration 73/1000 | Loss: 0.00002092
Iteration 74/1000 | Loss: 0.00002092
Iteration 75/1000 | Loss: 0.00002092
Iteration 76/1000 | Loss: 0.00002092
Iteration 77/1000 | Loss: 0.00002092
Iteration 78/1000 | Loss: 0.00002092
Iteration 79/1000 | Loss: 0.00002092
Iteration 80/1000 | Loss: 0.00002092
Iteration 81/1000 | Loss: 0.00002092
Iteration 82/1000 | Loss: 0.00002092
Iteration 83/1000 | Loss: 0.00002092
Iteration 84/1000 | Loss: 0.00002092
Iteration 85/1000 | Loss: 0.00002091
Iteration 86/1000 | Loss: 0.00002091
Iteration 87/1000 | Loss: 0.00002091
Iteration 88/1000 | Loss: 0.00002091
Iteration 89/1000 | Loss: 0.00002091
Iteration 90/1000 | Loss: 0.00002091
Iteration 91/1000 | Loss: 0.00002091
Iteration 92/1000 | Loss: 0.00002091
Iteration 93/1000 | Loss: 0.00002091
Iteration 94/1000 | Loss: 0.00002091
Iteration 95/1000 | Loss: 0.00002091
Iteration 96/1000 | Loss: 0.00002091
Iteration 97/1000 | Loss: 0.00002091
Iteration 98/1000 | Loss: 0.00002091
Iteration 99/1000 | Loss: 0.00002091
Iteration 100/1000 | Loss: 0.00002091
Iteration 101/1000 | Loss: 0.00002091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.0907591533614323e-05, 2.0907591533614323e-05, 2.0907591533614323e-05, 2.0907591533614323e-05, 2.0907591533614323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0907591533614323e-05

Optimization complete. Final v2v error: 3.862996816635132 mm

Highest mean error: 4.304074287414551 mm for frame 100

Lowest mean error: 3.5316381454467773 mm for frame 168

Saving results

Total time: 39.31450939178467
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466098
Iteration 2/25 | Loss: 0.00073614
Iteration 3/25 | Loss: 0.00063045
Iteration 4/25 | Loss: 0.00061200
Iteration 5/25 | Loss: 0.00060617
Iteration 6/25 | Loss: 0.00060468
Iteration 7/25 | Loss: 0.00060459
Iteration 8/25 | Loss: 0.00060459
Iteration 9/25 | Loss: 0.00060459
Iteration 10/25 | Loss: 0.00060459
Iteration 11/25 | Loss: 0.00060459
Iteration 12/25 | Loss: 0.00060459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006045869668014348, 0.0006045869668014348, 0.0006045869668014348, 0.0006045869668014348, 0.0006045869668014348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006045869668014348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53771603
Iteration 2/25 | Loss: 0.00021792
Iteration 3/25 | Loss: 0.00021792
Iteration 4/25 | Loss: 0.00021792
Iteration 5/25 | Loss: 0.00021792
Iteration 6/25 | Loss: 0.00021792
Iteration 7/25 | Loss: 0.00021792
Iteration 8/25 | Loss: 0.00021792
Iteration 9/25 | Loss: 0.00021792
Iteration 10/25 | Loss: 0.00021792
Iteration 11/25 | Loss: 0.00021792
Iteration 12/25 | Loss: 0.00021792
Iteration 13/25 | Loss: 0.00021792
Iteration 14/25 | Loss: 0.00021792
Iteration 15/25 | Loss: 0.00021792
Iteration 16/25 | Loss: 0.00021792
Iteration 17/25 | Loss: 0.00021792
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00021791683684568852, 0.00021791683684568852, 0.00021791683684568852, 0.00021791683684568852, 0.00021791683684568852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021791683684568852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021792
Iteration 2/1000 | Loss: 0.00002756
Iteration 3/1000 | Loss: 0.00002095
Iteration 4/1000 | Loss: 0.00001911
Iteration 5/1000 | Loss: 0.00001828
Iteration 6/1000 | Loss: 0.00001747
Iteration 7/1000 | Loss: 0.00001678
Iteration 8/1000 | Loss: 0.00001645
Iteration 9/1000 | Loss: 0.00001627
Iteration 10/1000 | Loss: 0.00001620
Iteration 11/1000 | Loss: 0.00001619
Iteration 12/1000 | Loss: 0.00001615
Iteration 13/1000 | Loss: 0.00001610
Iteration 14/1000 | Loss: 0.00001602
Iteration 15/1000 | Loss: 0.00001601
Iteration 16/1000 | Loss: 0.00001600
Iteration 17/1000 | Loss: 0.00001598
Iteration 18/1000 | Loss: 0.00001595
Iteration 19/1000 | Loss: 0.00001593
Iteration 20/1000 | Loss: 0.00001588
Iteration 21/1000 | Loss: 0.00001587
Iteration 22/1000 | Loss: 0.00001587
Iteration 23/1000 | Loss: 0.00001587
Iteration 24/1000 | Loss: 0.00001587
Iteration 25/1000 | Loss: 0.00001587
Iteration 26/1000 | Loss: 0.00001587
Iteration 27/1000 | Loss: 0.00001587
Iteration 28/1000 | Loss: 0.00001587
Iteration 29/1000 | Loss: 0.00001586
Iteration 30/1000 | Loss: 0.00001586
Iteration 31/1000 | Loss: 0.00001586
Iteration 32/1000 | Loss: 0.00001586
Iteration 33/1000 | Loss: 0.00001586
Iteration 34/1000 | Loss: 0.00001586
Iteration 35/1000 | Loss: 0.00001586
Iteration 36/1000 | Loss: 0.00001584
Iteration 37/1000 | Loss: 0.00001584
Iteration 38/1000 | Loss: 0.00001584
Iteration 39/1000 | Loss: 0.00001583
Iteration 40/1000 | Loss: 0.00001583
Iteration 41/1000 | Loss: 0.00001583
Iteration 42/1000 | Loss: 0.00001583
Iteration 43/1000 | Loss: 0.00001583
Iteration 44/1000 | Loss: 0.00001581
Iteration 45/1000 | Loss: 0.00001581
Iteration 46/1000 | Loss: 0.00001580
Iteration 47/1000 | Loss: 0.00001580
Iteration 48/1000 | Loss: 0.00001580
Iteration 49/1000 | Loss: 0.00001579
Iteration 50/1000 | Loss: 0.00001579
Iteration 51/1000 | Loss: 0.00001578
Iteration 52/1000 | Loss: 0.00001578
Iteration 53/1000 | Loss: 0.00001574
Iteration 54/1000 | Loss: 0.00001574
Iteration 55/1000 | Loss: 0.00001571
Iteration 56/1000 | Loss: 0.00001571
Iteration 57/1000 | Loss: 0.00001569
Iteration 58/1000 | Loss: 0.00001569
Iteration 59/1000 | Loss: 0.00001568
Iteration 60/1000 | Loss: 0.00001568
Iteration 61/1000 | Loss: 0.00001568
Iteration 62/1000 | Loss: 0.00001568
Iteration 63/1000 | Loss: 0.00001568
Iteration 64/1000 | Loss: 0.00001568
Iteration 65/1000 | Loss: 0.00001567
Iteration 66/1000 | Loss: 0.00001567
Iteration 67/1000 | Loss: 0.00001567
Iteration 68/1000 | Loss: 0.00001566
Iteration 69/1000 | Loss: 0.00001566
Iteration 70/1000 | Loss: 0.00001565
Iteration 71/1000 | Loss: 0.00001565
Iteration 72/1000 | Loss: 0.00001565
Iteration 73/1000 | Loss: 0.00001565
Iteration 74/1000 | Loss: 0.00001565
Iteration 75/1000 | Loss: 0.00001564
Iteration 76/1000 | Loss: 0.00001564
Iteration 77/1000 | Loss: 0.00001564
Iteration 78/1000 | Loss: 0.00001564
Iteration 79/1000 | Loss: 0.00001563
Iteration 80/1000 | Loss: 0.00001563
Iteration 81/1000 | Loss: 0.00001563
Iteration 82/1000 | Loss: 0.00001563
Iteration 83/1000 | Loss: 0.00001563
Iteration 84/1000 | Loss: 0.00001563
Iteration 85/1000 | Loss: 0.00001563
Iteration 86/1000 | Loss: 0.00001563
Iteration 87/1000 | Loss: 0.00001563
Iteration 88/1000 | Loss: 0.00001563
Iteration 89/1000 | Loss: 0.00001563
Iteration 90/1000 | Loss: 0.00001563
Iteration 91/1000 | Loss: 0.00001563
Iteration 92/1000 | Loss: 0.00001562
Iteration 93/1000 | Loss: 0.00001562
Iteration 94/1000 | Loss: 0.00001562
Iteration 95/1000 | Loss: 0.00001562
Iteration 96/1000 | Loss: 0.00001562
Iteration 97/1000 | Loss: 0.00001562
Iteration 98/1000 | Loss: 0.00001562
Iteration 99/1000 | Loss: 0.00001562
Iteration 100/1000 | Loss: 0.00001562
Iteration 101/1000 | Loss: 0.00001561
Iteration 102/1000 | Loss: 0.00001561
Iteration 103/1000 | Loss: 0.00001561
Iteration 104/1000 | Loss: 0.00001561
Iteration 105/1000 | Loss: 0.00001561
Iteration 106/1000 | Loss: 0.00001561
Iteration 107/1000 | Loss: 0.00001561
Iteration 108/1000 | Loss: 0.00001561
Iteration 109/1000 | Loss: 0.00001561
Iteration 110/1000 | Loss: 0.00001560
Iteration 111/1000 | Loss: 0.00001560
Iteration 112/1000 | Loss: 0.00001560
Iteration 113/1000 | Loss: 0.00001560
Iteration 114/1000 | Loss: 0.00001560
Iteration 115/1000 | Loss: 0.00001560
Iteration 116/1000 | Loss: 0.00001560
Iteration 117/1000 | Loss: 0.00001560
Iteration 118/1000 | Loss: 0.00001560
Iteration 119/1000 | Loss: 0.00001560
Iteration 120/1000 | Loss: 0.00001560
Iteration 121/1000 | Loss: 0.00001560
Iteration 122/1000 | Loss: 0.00001560
Iteration 123/1000 | Loss: 0.00001560
Iteration 124/1000 | Loss: 0.00001560
Iteration 125/1000 | Loss: 0.00001560
Iteration 126/1000 | Loss: 0.00001560
Iteration 127/1000 | Loss: 0.00001560
Iteration 128/1000 | Loss: 0.00001560
Iteration 129/1000 | Loss: 0.00001560
Iteration 130/1000 | Loss: 0.00001560
Iteration 131/1000 | Loss: 0.00001560
Iteration 132/1000 | Loss: 0.00001560
Iteration 133/1000 | Loss: 0.00001560
Iteration 134/1000 | Loss: 0.00001560
Iteration 135/1000 | Loss: 0.00001560
Iteration 136/1000 | Loss: 0.00001560
Iteration 137/1000 | Loss: 0.00001560
Iteration 138/1000 | Loss: 0.00001560
Iteration 139/1000 | Loss: 0.00001560
Iteration 140/1000 | Loss: 0.00001560
Iteration 141/1000 | Loss: 0.00001560
Iteration 142/1000 | Loss: 0.00001560
Iteration 143/1000 | Loss: 0.00001560
Iteration 144/1000 | Loss: 0.00001559
Iteration 145/1000 | Loss: 0.00001559
Iteration 146/1000 | Loss: 0.00001559
Iteration 147/1000 | Loss: 0.00001559
Iteration 148/1000 | Loss: 0.00001559
Iteration 149/1000 | Loss: 0.00001559
Iteration 150/1000 | Loss: 0.00001559
Iteration 151/1000 | Loss: 0.00001559
Iteration 152/1000 | Loss: 0.00001559
Iteration 153/1000 | Loss: 0.00001559
Iteration 154/1000 | Loss: 0.00001559
Iteration 155/1000 | Loss: 0.00001559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.559434167575091e-05, 1.559434167575091e-05, 1.559434167575091e-05, 1.559434167575091e-05, 1.559434167575091e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.559434167575091e-05

Optimization complete. Final v2v error: 3.4045257568359375 mm

Highest mean error: 3.6612937450408936 mm for frame 40

Lowest mean error: 3.1945278644561768 mm for frame 60

Saving results

Total time: 37.13716101646423
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437825
Iteration 2/25 | Loss: 0.00099480
Iteration 3/25 | Loss: 0.00068402
Iteration 4/25 | Loss: 0.00063501
Iteration 5/25 | Loss: 0.00063259
Iteration 6/25 | Loss: 0.00063226
Iteration 7/25 | Loss: 0.00063226
Iteration 8/25 | Loss: 0.00063226
Iteration 9/25 | Loss: 0.00063226
Iteration 10/25 | Loss: 0.00063226
Iteration 11/25 | Loss: 0.00063226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006322647095657885, 0.0006322647095657885, 0.0006322647095657885, 0.0006322647095657885, 0.0006322647095657885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006322647095657885

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43115258
Iteration 2/25 | Loss: 0.00028872
Iteration 3/25 | Loss: 0.00028872
Iteration 4/25 | Loss: 0.00028872
Iteration 5/25 | Loss: 0.00028871
Iteration 6/25 | Loss: 0.00028871
Iteration 7/25 | Loss: 0.00028871
Iteration 8/25 | Loss: 0.00028871
Iteration 9/25 | Loss: 0.00028871
Iteration 10/25 | Loss: 0.00028871
Iteration 11/25 | Loss: 0.00028871
Iteration 12/25 | Loss: 0.00028871
Iteration 13/25 | Loss: 0.00028871
Iteration 14/25 | Loss: 0.00028871
Iteration 15/25 | Loss: 0.00028871
Iteration 16/25 | Loss: 0.00028871
Iteration 17/25 | Loss: 0.00028871
Iteration 18/25 | Loss: 0.00028871
Iteration 19/25 | Loss: 0.00028871
Iteration 20/25 | Loss: 0.00028871
Iteration 21/25 | Loss: 0.00028871
Iteration 22/25 | Loss: 0.00028871
Iteration 23/25 | Loss: 0.00028871
Iteration 24/25 | Loss: 0.00028871
Iteration 25/25 | Loss: 0.00028871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028871
Iteration 2/1000 | Loss: 0.00002285
Iteration 3/1000 | Loss: 0.00001708
Iteration 4/1000 | Loss: 0.00001582
Iteration 5/1000 | Loss: 0.00001518
Iteration 6/1000 | Loss: 0.00001461
Iteration 7/1000 | Loss: 0.00001427
Iteration 8/1000 | Loss: 0.00001397
Iteration 9/1000 | Loss: 0.00001379
Iteration 10/1000 | Loss: 0.00001376
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001373
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001367
Iteration 15/1000 | Loss: 0.00001367
Iteration 16/1000 | Loss: 0.00001367
Iteration 17/1000 | Loss: 0.00001366
Iteration 18/1000 | Loss: 0.00001366
Iteration 19/1000 | Loss: 0.00001365
Iteration 20/1000 | Loss: 0.00001365
Iteration 21/1000 | Loss: 0.00001365
Iteration 22/1000 | Loss: 0.00001363
Iteration 23/1000 | Loss: 0.00001363
Iteration 24/1000 | Loss: 0.00001362
Iteration 25/1000 | Loss: 0.00001359
Iteration 26/1000 | Loss: 0.00001359
Iteration 27/1000 | Loss: 0.00001359
Iteration 28/1000 | Loss: 0.00001358
Iteration 29/1000 | Loss: 0.00001358
Iteration 30/1000 | Loss: 0.00001357
Iteration 31/1000 | Loss: 0.00001357
Iteration 32/1000 | Loss: 0.00001356
Iteration 33/1000 | Loss: 0.00001356
Iteration 34/1000 | Loss: 0.00001356
Iteration 35/1000 | Loss: 0.00001355
Iteration 36/1000 | Loss: 0.00001355
Iteration 37/1000 | Loss: 0.00001355
Iteration 38/1000 | Loss: 0.00001355
Iteration 39/1000 | Loss: 0.00001355
Iteration 40/1000 | Loss: 0.00001355
Iteration 41/1000 | Loss: 0.00001354
Iteration 42/1000 | Loss: 0.00001354
Iteration 43/1000 | Loss: 0.00001354
Iteration 44/1000 | Loss: 0.00001354
Iteration 45/1000 | Loss: 0.00001354
Iteration 46/1000 | Loss: 0.00001354
Iteration 47/1000 | Loss: 0.00001354
Iteration 48/1000 | Loss: 0.00001354
Iteration 49/1000 | Loss: 0.00001353
Iteration 50/1000 | Loss: 0.00001353
Iteration 51/1000 | Loss: 0.00001353
Iteration 52/1000 | Loss: 0.00001353
Iteration 53/1000 | Loss: 0.00001353
Iteration 54/1000 | Loss: 0.00001353
Iteration 55/1000 | Loss: 0.00001353
Iteration 56/1000 | Loss: 0.00001353
Iteration 57/1000 | Loss: 0.00001353
Iteration 58/1000 | Loss: 0.00001353
Iteration 59/1000 | Loss: 0.00001353
Iteration 60/1000 | Loss: 0.00001353
Iteration 61/1000 | Loss: 0.00001353
Iteration 62/1000 | Loss: 0.00001353
Iteration 63/1000 | Loss: 0.00001352
Iteration 64/1000 | Loss: 0.00001352
Iteration 65/1000 | Loss: 0.00001352
Iteration 66/1000 | Loss: 0.00001352
Iteration 67/1000 | Loss: 0.00001352
Iteration 68/1000 | Loss: 0.00001352
Iteration 69/1000 | Loss: 0.00001352
Iteration 70/1000 | Loss: 0.00001352
Iteration 71/1000 | Loss: 0.00001352
Iteration 72/1000 | Loss: 0.00001352
Iteration 73/1000 | Loss: 0.00001352
Iteration 74/1000 | Loss: 0.00001352
Iteration 75/1000 | Loss: 0.00001352
Iteration 76/1000 | Loss: 0.00001352
Iteration 77/1000 | Loss: 0.00001352
Iteration 78/1000 | Loss: 0.00001352
Iteration 79/1000 | Loss: 0.00001352
Iteration 80/1000 | Loss: 0.00001352
Iteration 81/1000 | Loss: 0.00001352
Iteration 82/1000 | Loss: 0.00001352
Iteration 83/1000 | Loss: 0.00001352
Iteration 84/1000 | Loss: 0.00001352
Iteration 85/1000 | Loss: 0.00001352
Iteration 86/1000 | Loss: 0.00001352
Iteration 87/1000 | Loss: 0.00001352
Iteration 88/1000 | Loss: 0.00001352
Iteration 89/1000 | Loss: 0.00001352
Iteration 90/1000 | Loss: 0.00001352
Iteration 91/1000 | Loss: 0.00001352
Iteration 92/1000 | Loss: 0.00001352
Iteration 93/1000 | Loss: 0.00001352
Iteration 94/1000 | Loss: 0.00001352
Iteration 95/1000 | Loss: 0.00001352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.3515173122868873e-05, 1.3515173122868873e-05, 1.3515173122868873e-05, 1.3515173122868873e-05, 1.3515173122868873e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3515173122868873e-05

Optimization complete. Final v2v error: 3.0221621990203857 mm

Highest mean error: 3.340308904647827 mm for frame 24

Lowest mean error: 2.662039041519165 mm for frame 231

Saving results

Total time: 30.502222537994385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00528389
Iteration 2/25 | Loss: 0.00101145
Iteration 3/25 | Loss: 0.00063592
Iteration 4/25 | Loss: 0.00060535
Iteration 5/25 | Loss: 0.00059942
Iteration 6/25 | Loss: 0.00059760
Iteration 7/25 | Loss: 0.00059735
Iteration 8/25 | Loss: 0.00059735
Iteration 9/25 | Loss: 0.00059735
Iteration 10/25 | Loss: 0.00059735
Iteration 11/25 | Loss: 0.00059735
Iteration 12/25 | Loss: 0.00059735
Iteration 13/25 | Loss: 0.00059735
Iteration 14/25 | Loss: 0.00059735
Iteration 15/25 | Loss: 0.00059735
Iteration 16/25 | Loss: 0.00059735
Iteration 17/25 | Loss: 0.00059735
Iteration 18/25 | Loss: 0.00059735
Iteration 19/25 | Loss: 0.00059735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005973457009531558, 0.0005973457009531558, 0.0005973457009531558, 0.0005973457009531558, 0.0005973457009531558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005973457009531558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84414065
Iteration 2/25 | Loss: 0.00017443
Iteration 3/25 | Loss: 0.00017443
Iteration 4/25 | Loss: 0.00017443
Iteration 5/25 | Loss: 0.00017442
Iteration 6/25 | Loss: 0.00017442
Iteration 7/25 | Loss: 0.00017442
Iteration 8/25 | Loss: 0.00017442
Iteration 9/25 | Loss: 0.00017442
Iteration 10/25 | Loss: 0.00017442
Iteration 11/25 | Loss: 0.00017442
Iteration 12/25 | Loss: 0.00017442
Iteration 13/25 | Loss: 0.00017442
Iteration 14/25 | Loss: 0.00017442
Iteration 15/25 | Loss: 0.00017442
Iteration 16/25 | Loss: 0.00017442
Iteration 17/25 | Loss: 0.00017442
Iteration 18/25 | Loss: 0.00017442
Iteration 19/25 | Loss: 0.00017442
Iteration 20/25 | Loss: 0.00017442
Iteration 21/25 | Loss: 0.00017442
Iteration 22/25 | Loss: 0.00017442
Iteration 23/25 | Loss: 0.00017442
Iteration 24/25 | Loss: 0.00017442
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0001744233159115538, 0.0001744233159115538, 0.0001744233159115538, 0.0001744233159115538, 0.0001744233159115538]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001744233159115538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017442
Iteration 2/1000 | Loss: 0.00002025
Iteration 3/1000 | Loss: 0.00001456
Iteration 4/1000 | Loss: 0.00001335
Iteration 5/1000 | Loss: 0.00001286
Iteration 6/1000 | Loss: 0.00001247
Iteration 7/1000 | Loss: 0.00001205
Iteration 8/1000 | Loss: 0.00001180
Iteration 9/1000 | Loss: 0.00001161
Iteration 10/1000 | Loss: 0.00001147
Iteration 11/1000 | Loss: 0.00001146
Iteration 12/1000 | Loss: 0.00001145
Iteration 13/1000 | Loss: 0.00001144
Iteration 14/1000 | Loss: 0.00001141
Iteration 15/1000 | Loss: 0.00001132
Iteration 16/1000 | Loss: 0.00001128
Iteration 17/1000 | Loss: 0.00001127
Iteration 18/1000 | Loss: 0.00001126
Iteration 19/1000 | Loss: 0.00001126
Iteration 20/1000 | Loss: 0.00001125
Iteration 21/1000 | Loss: 0.00001122
Iteration 22/1000 | Loss: 0.00001122
Iteration 23/1000 | Loss: 0.00001122
Iteration 24/1000 | Loss: 0.00001121
Iteration 25/1000 | Loss: 0.00001121
Iteration 26/1000 | Loss: 0.00001121
Iteration 27/1000 | Loss: 0.00001121
Iteration 28/1000 | Loss: 0.00001120
Iteration 29/1000 | Loss: 0.00001120
Iteration 30/1000 | Loss: 0.00001120
Iteration 31/1000 | Loss: 0.00001119
Iteration 32/1000 | Loss: 0.00001117
Iteration 33/1000 | Loss: 0.00001117
Iteration 34/1000 | Loss: 0.00001117
Iteration 35/1000 | Loss: 0.00001117
Iteration 36/1000 | Loss: 0.00001117
Iteration 37/1000 | Loss: 0.00001117
Iteration 38/1000 | Loss: 0.00001117
Iteration 39/1000 | Loss: 0.00001117
Iteration 40/1000 | Loss: 0.00001117
Iteration 41/1000 | Loss: 0.00001117
Iteration 42/1000 | Loss: 0.00001116
Iteration 43/1000 | Loss: 0.00001116
Iteration 44/1000 | Loss: 0.00001116
Iteration 45/1000 | Loss: 0.00001116
Iteration 46/1000 | Loss: 0.00001116
Iteration 47/1000 | Loss: 0.00001116
Iteration 48/1000 | Loss: 0.00001115
Iteration 49/1000 | Loss: 0.00001115
Iteration 50/1000 | Loss: 0.00001115
Iteration 51/1000 | Loss: 0.00001115
Iteration 52/1000 | Loss: 0.00001114
Iteration 53/1000 | Loss: 0.00001114
Iteration 54/1000 | Loss: 0.00001114
Iteration 55/1000 | Loss: 0.00001114
Iteration 56/1000 | Loss: 0.00001114
Iteration 57/1000 | Loss: 0.00001114
Iteration 58/1000 | Loss: 0.00001114
Iteration 59/1000 | Loss: 0.00001114
Iteration 60/1000 | Loss: 0.00001113
Iteration 61/1000 | Loss: 0.00001113
Iteration 62/1000 | Loss: 0.00001113
Iteration 63/1000 | Loss: 0.00001113
Iteration 64/1000 | Loss: 0.00001113
Iteration 65/1000 | Loss: 0.00001113
Iteration 66/1000 | Loss: 0.00001113
Iteration 67/1000 | Loss: 0.00001113
Iteration 68/1000 | Loss: 0.00001113
Iteration 69/1000 | Loss: 0.00001113
Iteration 70/1000 | Loss: 0.00001113
Iteration 71/1000 | Loss: 0.00001113
Iteration 72/1000 | Loss: 0.00001113
Iteration 73/1000 | Loss: 0.00001113
Iteration 74/1000 | Loss: 0.00001113
Iteration 75/1000 | Loss: 0.00001113
Iteration 76/1000 | Loss: 0.00001113
Iteration 77/1000 | Loss: 0.00001113
Iteration 78/1000 | Loss: 0.00001113
Iteration 79/1000 | Loss: 0.00001113
Iteration 80/1000 | Loss: 0.00001113
Iteration 81/1000 | Loss: 0.00001113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.1133030966448132e-05, 1.1133030966448132e-05, 1.1133030966448132e-05, 1.1133030966448132e-05, 1.1133030966448132e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1133030966448132e-05

Optimization complete. Final v2v error: 2.8372058868408203 mm

Highest mean error: 3.0922889709472656 mm for frame 57

Lowest mean error: 2.5167953968048096 mm for frame 216

Saving results

Total time: 35.140204429626465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384792
Iteration 2/25 | Loss: 0.00073601
Iteration 3/25 | Loss: 0.00058422
Iteration 4/25 | Loss: 0.00056602
Iteration 5/25 | Loss: 0.00056197
Iteration 6/25 | Loss: 0.00056084
Iteration 7/25 | Loss: 0.00056065
Iteration 8/25 | Loss: 0.00056065
Iteration 9/25 | Loss: 0.00056065
Iteration 10/25 | Loss: 0.00056065
Iteration 11/25 | Loss: 0.00056065
Iteration 12/25 | Loss: 0.00056065
Iteration 13/25 | Loss: 0.00056065
Iteration 14/25 | Loss: 0.00056065
Iteration 15/25 | Loss: 0.00056065
Iteration 16/25 | Loss: 0.00056065
Iteration 17/25 | Loss: 0.00056065
Iteration 18/25 | Loss: 0.00056065
Iteration 19/25 | Loss: 0.00056065
Iteration 20/25 | Loss: 0.00056065
Iteration 21/25 | Loss: 0.00056065
Iteration 22/25 | Loss: 0.00056065
Iteration 23/25 | Loss: 0.00056065
Iteration 24/25 | Loss: 0.00056065
Iteration 25/25 | Loss: 0.00056065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42255640
Iteration 2/25 | Loss: 0.00022720
Iteration 3/25 | Loss: 0.00022720
Iteration 4/25 | Loss: 0.00022720
Iteration 5/25 | Loss: 0.00022720
Iteration 6/25 | Loss: 0.00022720
Iteration 7/25 | Loss: 0.00022720
Iteration 8/25 | Loss: 0.00022720
Iteration 9/25 | Loss: 0.00022719
Iteration 10/25 | Loss: 0.00022719
Iteration 11/25 | Loss: 0.00022719
Iteration 12/25 | Loss: 0.00022719
Iteration 13/25 | Loss: 0.00022719
Iteration 14/25 | Loss: 0.00022719
Iteration 15/25 | Loss: 0.00022719
Iteration 16/25 | Loss: 0.00022719
Iteration 17/25 | Loss: 0.00022719
Iteration 18/25 | Loss: 0.00022719
Iteration 19/25 | Loss: 0.00022719
Iteration 20/25 | Loss: 0.00022719
Iteration 21/25 | Loss: 0.00022719
Iteration 22/25 | Loss: 0.00022719
Iteration 23/25 | Loss: 0.00022719
Iteration 24/25 | Loss: 0.00022719
Iteration 25/25 | Loss: 0.00022719

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022719
Iteration 2/1000 | Loss: 0.00001797
Iteration 3/1000 | Loss: 0.00001360
Iteration 4/1000 | Loss: 0.00001202
Iteration 5/1000 | Loss: 0.00001149
Iteration 6/1000 | Loss: 0.00001110
Iteration 7/1000 | Loss: 0.00001064
Iteration 8/1000 | Loss: 0.00001047
Iteration 9/1000 | Loss: 0.00001042
Iteration 10/1000 | Loss: 0.00001041
Iteration 11/1000 | Loss: 0.00001039
Iteration 12/1000 | Loss: 0.00001038
Iteration 13/1000 | Loss: 0.00001036
Iteration 14/1000 | Loss: 0.00001035
Iteration 15/1000 | Loss: 0.00001035
Iteration 16/1000 | Loss: 0.00001035
Iteration 17/1000 | Loss: 0.00001034
Iteration 18/1000 | Loss: 0.00001034
Iteration 19/1000 | Loss: 0.00001033
Iteration 20/1000 | Loss: 0.00001029
Iteration 21/1000 | Loss: 0.00001027
Iteration 22/1000 | Loss: 0.00001027
Iteration 23/1000 | Loss: 0.00001026
Iteration 24/1000 | Loss: 0.00001020
Iteration 25/1000 | Loss: 0.00001020
Iteration 26/1000 | Loss: 0.00001020
Iteration 27/1000 | Loss: 0.00001018
Iteration 28/1000 | Loss: 0.00001018
Iteration 29/1000 | Loss: 0.00001017
Iteration 30/1000 | Loss: 0.00001017
Iteration 31/1000 | Loss: 0.00001017
Iteration 32/1000 | Loss: 0.00001016
Iteration 33/1000 | Loss: 0.00001016
Iteration 34/1000 | Loss: 0.00001015
Iteration 35/1000 | Loss: 0.00001014
Iteration 36/1000 | Loss: 0.00001013
Iteration 37/1000 | Loss: 0.00001012
Iteration 38/1000 | Loss: 0.00001011
Iteration 39/1000 | Loss: 0.00001011
Iteration 40/1000 | Loss: 0.00001010
Iteration 41/1000 | Loss: 0.00001010
Iteration 42/1000 | Loss: 0.00001009
Iteration 43/1000 | Loss: 0.00001008
Iteration 44/1000 | Loss: 0.00001008
Iteration 45/1000 | Loss: 0.00001008
Iteration 46/1000 | Loss: 0.00001006
Iteration 47/1000 | Loss: 0.00001006
Iteration 48/1000 | Loss: 0.00001006
Iteration 49/1000 | Loss: 0.00001005
Iteration 50/1000 | Loss: 0.00001003
Iteration 51/1000 | Loss: 0.00001003
Iteration 52/1000 | Loss: 0.00001003
Iteration 53/1000 | Loss: 0.00001002
Iteration 54/1000 | Loss: 0.00001001
Iteration 55/1000 | Loss: 0.00001001
Iteration 56/1000 | Loss: 0.00001001
Iteration 57/1000 | Loss: 0.00001000
Iteration 58/1000 | Loss: 0.00001000
Iteration 59/1000 | Loss: 0.00000999
Iteration 60/1000 | Loss: 0.00000998
Iteration 61/1000 | Loss: 0.00000997
Iteration 62/1000 | Loss: 0.00000997
Iteration 63/1000 | Loss: 0.00000997
Iteration 64/1000 | Loss: 0.00000996
Iteration 65/1000 | Loss: 0.00000996
Iteration 66/1000 | Loss: 0.00000995
Iteration 67/1000 | Loss: 0.00000995
Iteration 68/1000 | Loss: 0.00000995
Iteration 69/1000 | Loss: 0.00000995
Iteration 70/1000 | Loss: 0.00000995
Iteration 71/1000 | Loss: 0.00000995
Iteration 72/1000 | Loss: 0.00000994
Iteration 73/1000 | Loss: 0.00000994
Iteration 74/1000 | Loss: 0.00000994
Iteration 75/1000 | Loss: 0.00000994
Iteration 76/1000 | Loss: 0.00000994
Iteration 77/1000 | Loss: 0.00000994
Iteration 78/1000 | Loss: 0.00000994
Iteration 79/1000 | Loss: 0.00000994
Iteration 80/1000 | Loss: 0.00000994
Iteration 81/1000 | Loss: 0.00000994
Iteration 82/1000 | Loss: 0.00000994
Iteration 83/1000 | Loss: 0.00000994
Iteration 84/1000 | Loss: 0.00000994
Iteration 85/1000 | Loss: 0.00000994
Iteration 86/1000 | Loss: 0.00000994
Iteration 87/1000 | Loss: 0.00000994
Iteration 88/1000 | Loss: 0.00000994
Iteration 89/1000 | Loss: 0.00000994
Iteration 90/1000 | Loss: 0.00000994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [9.935069101629779e-06, 9.935069101629779e-06, 9.935069101629779e-06, 9.935069101629779e-06, 9.935069101629779e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.935069101629779e-06

Optimization complete. Final v2v error: 2.6787469387054443 mm

Highest mean error: 3.4120848178863525 mm for frame 60

Lowest mean error: 2.325932502746582 mm for frame 0

Saving results

Total time: 30.332101583480835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00512766
Iteration 2/25 | Loss: 0.00085366
Iteration 3/25 | Loss: 0.00069648
Iteration 4/25 | Loss: 0.00065764
Iteration 5/25 | Loss: 0.00064275
Iteration 6/25 | Loss: 0.00064033
Iteration 7/25 | Loss: 0.00063995
Iteration 8/25 | Loss: 0.00063995
Iteration 9/25 | Loss: 0.00063995
Iteration 10/25 | Loss: 0.00063995
Iteration 11/25 | Loss: 0.00063995
Iteration 12/25 | Loss: 0.00063995
Iteration 13/25 | Loss: 0.00063995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006399544072337449, 0.0006399544072337449, 0.0006399544072337449, 0.0006399544072337449, 0.0006399544072337449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006399544072337449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.82318592
Iteration 2/25 | Loss: 0.00021382
Iteration 3/25 | Loss: 0.00021381
Iteration 4/25 | Loss: 0.00021381
Iteration 5/25 | Loss: 0.00021381
Iteration 6/25 | Loss: 0.00021381
Iteration 7/25 | Loss: 0.00021381
Iteration 8/25 | Loss: 0.00021381
Iteration 9/25 | Loss: 0.00021381
Iteration 10/25 | Loss: 0.00021381
Iteration 11/25 | Loss: 0.00021381
Iteration 12/25 | Loss: 0.00021381
Iteration 13/25 | Loss: 0.00021381
Iteration 14/25 | Loss: 0.00021381
Iteration 15/25 | Loss: 0.00021381
Iteration 16/25 | Loss: 0.00021381
Iteration 17/25 | Loss: 0.00021381
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00021381040278356522, 0.00021381040278356522, 0.00021381040278356522, 0.00021381040278356522, 0.00021381040278356522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021381040278356522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021381
Iteration 2/1000 | Loss: 0.00003048
Iteration 3/1000 | Loss: 0.00002555
Iteration 4/1000 | Loss: 0.00002366
Iteration 5/1000 | Loss: 0.00002240
Iteration 6/1000 | Loss: 0.00002146
Iteration 7/1000 | Loss: 0.00002075
Iteration 8/1000 | Loss: 0.00002037
Iteration 9/1000 | Loss: 0.00002009
Iteration 10/1000 | Loss: 0.00001999
Iteration 11/1000 | Loss: 0.00001992
Iteration 12/1000 | Loss: 0.00001991
Iteration 13/1000 | Loss: 0.00001990
Iteration 14/1000 | Loss: 0.00001987
Iteration 15/1000 | Loss: 0.00001983
Iteration 16/1000 | Loss: 0.00001983
Iteration 17/1000 | Loss: 0.00001982
Iteration 18/1000 | Loss: 0.00001982
Iteration 19/1000 | Loss: 0.00001974
Iteration 20/1000 | Loss: 0.00001973
Iteration 21/1000 | Loss: 0.00001971
Iteration 22/1000 | Loss: 0.00001971
Iteration 23/1000 | Loss: 0.00001970
Iteration 24/1000 | Loss: 0.00001970
Iteration 25/1000 | Loss: 0.00001970
Iteration 26/1000 | Loss: 0.00001970
Iteration 27/1000 | Loss: 0.00001969
Iteration 28/1000 | Loss: 0.00001969
Iteration 29/1000 | Loss: 0.00001968
Iteration 30/1000 | Loss: 0.00001968
Iteration 31/1000 | Loss: 0.00001968
Iteration 32/1000 | Loss: 0.00001968
Iteration 33/1000 | Loss: 0.00001968
Iteration 34/1000 | Loss: 0.00001967
Iteration 35/1000 | Loss: 0.00001967
Iteration 36/1000 | Loss: 0.00001967
Iteration 37/1000 | Loss: 0.00001966
Iteration 38/1000 | Loss: 0.00001966
Iteration 39/1000 | Loss: 0.00001966
Iteration 40/1000 | Loss: 0.00001965
Iteration 41/1000 | Loss: 0.00001965
Iteration 42/1000 | Loss: 0.00001965
Iteration 43/1000 | Loss: 0.00001965
Iteration 44/1000 | Loss: 0.00001964
Iteration 45/1000 | Loss: 0.00001964
Iteration 46/1000 | Loss: 0.00001964
Iteration 47/1000 | Loss: 0.00001964
Iteration 48/1000 | Loss: 0.00001964
Iteration 49/1000 | Loss: 0.00001964
Iteration 50/1000 | Loss: 0.00001964
Iteration 51/1000 | Loss: 0.00001963
Iteration 52/1000 | Loss: 0.00001963
Iteration 53/1000 | Loss: 0.00001963
Iteration 54/1000 | Loss: 0.00001963
Iteration 55/1000 | Loss: 0.00001963
Iteration 56/1000 | Loss: 0.00001963
Iteration 57/1000 | Loss: 0.00001963
Iteration 58/1000 | Loss: 0.00001963
Iteration 59/1000 | Loss: 0.00001963
Iteration 60/1000 | Loss: 0.00001962
Iteration 61/1000 | Loss: 0.00001962
Iteration 62/1000 | Loss: 0.00001961
Iteration 63/1000 | Loss: 0.00001961
Iteration 64/1000 | Loss: 0.00001961
Iteration 65/1000 | Loss: 0.00001961
Iteration 66/1000 | Loss: 0.00001961
Iteration 67/1000 | Loss: 0.00001961
Iteration 68/1000 | Loss: 0.00001960
Iteration 69/1000 | Loss: 0.00001960
Iteration 70/1000 | Loss: 0.00001960
Iteration 71/1000 | Loss: 0.00001960
Iteration 72/1000 | Loss: 0.00001960
Iteration 73/1000 | Loss: 0.00001960
Iteration 74/1000 | Loss: 0.00001960
Iteration 75/1000 | Loss: 0.00001960
Iteration 76/1000 | Loss: 0.00001959
Iteration 77/1000 | Loss: 0.00001959
Iteration 78/1000 | Loss: 0.00001959
Iteration 79/1000 | Loss: 0.00001959
Iteration 80/1000 | Loss: 0.00001959
Iteration 81/1000 | Loss: 0.00001959
Iteration 82/1000 | Loss: 0.00001958
Iteration 83/1000 | Loss: 0.00001958
Iteration 84/1000 | Loss: 0.00001958
Iteration 85/1000 | Loss: 0.00001958
Iteration 86/1000 | Loss: 0.00001958
Iteration 87/1000 | Loss: 0.00001957
Iteration 88/1000 | Loss: 0.00001957
Iteration 89/1000 | Loss: 0.00001957
Iteration 90/1000 | Loss: 0.00001956
Iteration 91/1000 | Loss: 0.00001956
Iteration 92/1000 | Loss: 0.00001956
Iteration 93/1000 | Loss: 0.00001955
Iteration 94/1000 | Loss: 0.00001955
Iteration 95/1000 | Loss: 0.00001955
Iteration 96/1000 | Loss: 0.00001955
Iteration 97/1000 | Loss: 0.00001954
Iteration 98/1000 | Loss: 0.00001954
Iteration 99/1000 | Loss: 0.00001954
Iteration 100/1000 | Loss: 0.00001953
Iteration 101/1000 | Loss: 0.00001953
Iteration 102/1000 | Loss: 0.00001953
Iteration 103/1000 | Loss: 0.00001953
Iteration 104/1000 | Loss: 0.00001953
Iteration 105/1000 | Loss: 0.00001952
Iteration 106/1000 | Loss: 0.00001952
Iteration 107/1000 | Loss: 0.00001952
Iteration 108/1000 | Loss: 0.00001952
Iteration 109/1000 | Loss: 0.00001951
Iteration 110/1000 | Loss: 0.00001951
Iteration 111/1000 | Loss: 0.00001951
Iteration 112/1000 | Loss: 0.00001951
Iteration 113/1000 | Loss: 0.00001951
Iteration 114/1000 | Loss: 0.00001951
Iteration 115/1000 | Loss: 0.00001951
Iteration 116/1000 | Loss: 0.00001951
Iteration 117/1000 | Loss: 0.00001951
Iteration 118/1000 | Loss: 0.00001951
Iteration 119/1000 | Loss: 0.00001951
Iteration 120/1000 | Loss: 0.00001951
Iteration 121/1000 | Loss: 0.00001951
Iteration 122/1000 | Loss: 0.00001951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.9509268895490095e-05, 1.9509268895490095e-05, 1.9509268895490095e-05, 1.9509268895490095e-05, 1.9509268895490095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9509268895490095e-05

Optimization complete. Final v2v error: 3.6567211151123047 mm

Highest mean error: 4.3057379722595215 mm for frame 92

Lowest mean error: 2.943366527557373 mm for frame 134

Saving results

Total time: 39.534111738204956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033700
Iteration 2/25 | Loss: 0.00336174
Iteration 3/25 | Loss: 0.00234488
Iteration 4/25 | Loss: 0.00176291
Iteration 5/25 | Loss: 0.00182691
Iteration 6/25 | Loss: 0.00170917
Iteration 7/25 | Loss: 0.00151142
Iteration 8/25 | Loss: 0.00143264
Iteration 9/25 | Loss: 0.00131953
Iteration 10/25 | Loss: 0.00128110
Iteration 11/25 | Loss: 0.00126727
Iteration 12/25 | Loss: 0.00122045
Iteration 13/25 | Loss: 0.00119259
Iteration 14/25 | Loss: 0.00119748
Iteration 15/25 | Loss: 0.00119294
Iteration 16/25 | Loss: 0.00118042
Iteration 17/25 | Loss: 0.00118026
Iteration 18/25 | Loss: 0.00117706
Iteration 19/25 | Loss: 0.00117972
Iteration 20/25 | Loss: 0.00117924
Iteration 21/25 | Loss: 0.00117496
Iteration 22/25 | Loss: 0.00119396
Iteration 23/25 | Loss: 0.00117679
Iteration 24/25 | Loss: 0.00115209
Iteration 25/25 | Loss: 0.00113925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.44932795
Iteration 2/25 | Loss: 0.00692496
Iteration 3/25 | Loss: 0.00471536
Iteration 4/25 | Loss: 0.00471536
Iteration 5/25 | Loss: 0.00471535
Iteration 6/25 | Loss: 0.00471535
Iteration 7/25 | Loss: 0.00471535
Iteration 8/25 | Loss: 0.00471535
Iteration 9/25 | Loss: 0.00471535
Iteration 10/25 | Loss: 0.00471535
Iteration 11/25 | Loss: 0.00471535
Iteration 12/25 | Loss: 0.00471535
Iteration 13/25 | Loss: 0.00471535
Iteration 14/25 | Loss: 0.00471535
Iteration 15/25 | Loss: 0.00471535
Iteration 16/25 | Loss: 0.00471535
Iteration 17/25 | Loss: 0.00471535
Iteration 18/25 | Loss: 0.00471535
Iteration 19/25 | Loss: 0.00471535
Iteration 20/25 | Loss: 0.00471535
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004715352319180965, 0.004715352319180965, 0.004715352319180965, 0.004715352319180965, 0.004715352319180965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004715352319180965

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00471535
Iteration 2/1000 | Loss: 0.00204016
Iteration 3/1000 | Loss: 0.00458334
Iteration 4/1000 | Loss: 0.00354460
Iteration 5/1000 | Loss: 0.00445449
Iteration 6/1000 | Loss: 0.00676071
Iteration 7/1000 | Loss: 0.01174262
Iteration 8/1000 | Loss: 0.00453951
Iteration 9/1000 | Loss: 0.00166185
Iteration 10/1000 | Loss: 0.00258921
Iteration 11/1000 | Loss: 0.00260473
Iteration 12/1000 | Loss: 0.00298074
Iteration 13/1000 | Loss: 0.00212780
Iteration 14/1000 | Loss: 0.00161099
Iteration 15/1000 | Loss: 0.00061690
Iteration 16/1000 | Loss: 0.00116029
Iteration 17/1000 | Loss: 0.00065657
Iteration 18/1000 | Loss: 0.00042761
Iteration 19/1000 | Loss: 0.00013986
Iteration 20/1000 | Loss: 0.00058335
Iteration 21/1000 | Loss: 0.00033018
Iteration 22/1000 | Loss: 0.00007523
Iteration 23/1000 | Loss: 0.00010644
Iteration 24/1000 | Loss: 0.00007656
Iteration 25/1000 | Loss: 0.00007319
Iteration 26/1000 | Loss: 0.00038397
Iteration 27/1000 | Loss: 0.00012959
Iteration 28/1000 | Loss: 0.00032416
Iteration 29/1000 | Loss: 0.00039780
Iteration 30/1000 | Loss: 0.00156749
Iteration 31/1000 | Loss: 0.00048061
Iteration 32/1000 | Loss: 0.00015497
Iteration 33/1000 | Loss: 0.00016551
Iteration 34/1000 | Loss: 0.00052685
Iteration 35/1000 | Loss: 0.00018936
Iteration 36/1000 | Loss: 0.00040680
Iteration 37/1000 | Loss: 0.00015097
Iteration 38/1000 | Loss: 0.00018934
Iteration 39/1000 | Loss: 0.00038852
Iteration 40/1000 | Loss: 0.00008008
Iteration 41/1000 | Loss: 0.00013156
Iteration 42/1000 | Loss: 0.00116478
Iteration 43/1000 | Loss: 0.00005371
Iteration 44/1000 | Loss: 0.00004350
Iteration 45/1000 | Loss: 0.00003992
Iteration 46/1000 | Loss: 0.00003814
Iteration 47/1000 | Loss: 0.00003718
Iteration 48/1000 | Loss: 0.00003629
Iteration 49/1000 | Loss: 0.00135036
Iteration 50/1000 | Loss: 0.00007147
Iteration 51/1000 | Loss: 0.00003631
Iteration 52/1000 | Loss: 0.00131584
Iteration 53/1000 | Loss: 0.00004702
Iteration 54/1000 | Loss: 0.00043266
Iteration 55/1000 | Loss: 0.00003933
Iteration 56/1000 | Loss: 0.00003599
Iteration 57/1000 | Loss: 0.00003482
Iteration 58/1000 | Loss: 0.00003426
Iteration 59/1000 | Loss: 0.00003385
Iteration 60/1000 | Loss: 0.00003342
Iteration 61/1000 | Loss: 0.00003303
Iteration 62/1000 | Loss: 0.00003265
Iteration 63/1000 | Loss: 0.00003236
Iteration 64/1000 | Loss: 0.00003220
Iteration 65/1000 | Loss: 0.00003205
Iteration 66/1000 | Loss: 0.00003202
Iteration 67/1000 | Loss: 0.00003186
Iteration 68/1000 | Loss: 0.00003183
Iteration 69/1000 | Loss: 0.00003182
Iteration 70/1000 | Loss: 0.00003182
Iteration 71/1000 | Loss: 0.00003181
Iteration 72/1000 | Loss: 0.00003181
Iteration 73/1000 | Loss: 0.00003180
Iteration 74/1000 | Loss: 0.00003180
Iteration 75/1000 | Loss: 0.00003180
Iteration 76/1000 | Loss: 0.00003178
Iteration 77/1000 | Loss: 0.00003177
Iteration 78/1000 | Loss: 0.00003174
Iteration 79/1000 | Loss: 0.00003171
Iteration 80/1000 | Loss: 0.00003170
Iteration 81/1000 | Loss: 0.00003168
Iteration 82/1000 | Loss: 0.00003167
Iteration 83/1000 | Loss: 0.00003166
Iteration 84/1000 | Loss: 0.00003161
Iteration 85/1000 | Loss: 0.00003159
Iteration 86/1000 | Loss: 0.00003158
Iteration 87/1000 | Loss: 0.00003158
Iteration 88/1000 | Loss: 0.00003158
Iteration 89/1000 | Loss: 0.00003157
Iteration 90/1000 | Loss: 0.00003157
Iteration 91/1000 | Loss: 0.00003157
Iteration 92/1000 | Loss: 0.00003156
Iteration 93/1000 | Loss: 0.00003156
Iteration 94/1000 | Loss: 0.00003156
Iteration 95/1000 | Loss: 0.00003156
Iteration 96/1000 | Loss: 0.00003156
Iteration 97/1000 | Loss: 0.00003155
Iteration 98/1000 | Loss: 0.00003155
Iteration 99/1000 | Loss: 0.00003155
Iteration 100/1000 | Loss: 0.00003155
Iteration 101/1000 | Loss: 0.00003155
Iteration 102/1000 | Loss: 0.00003154
Iteration 103/1000 | Loss: 0.00003154
Iteration 104/1000 | Loss: 0.00003154
Iteration 105/1000 | Loss: 0.00003154
Iteration 106/1000 | Loss: 0.00003154
Iteration 107/1000 | Loss: 0.00003154
Iteration 108/1000 | Loss: 0.00003154
Iteration 109/1000 | Loss: 0.00003154
Iteration 110/1000 | Loss: 0.00003153
Iteration 111/1000 | Loss: 0.00003153
Iteration 112/1000 | Loss: 0.00003153
Iteration 113/1000 | Loss: 0.00003153
Iteration 114/1000 | Loss: 0.00003153
Iteration 115/1000 | Loss: 0.00003153
Iteration 116/1000 | Loss: 0.00003153
Iteration 117/1000 | Loss: 0.00003153
Iteration 118/1000 | Loss: 0.00003153
Iteration 119/1000 | Loss: 0.00003152
Iteration 120/1000 | Loss: 0.00003152
Iteration 121/1000 | Loss: 0.00003152
Iteration 122/1000 | Loss: 0.00003152
Iteration 123/1000 | Loss: 0.00003152
Iteration 124/1000 | Loss: 0.00003152
Iteration 125/1000 | Loss: 0.00003152
Iteration 126/1000 | Loss: 0.00003152
Iteration 127/1000 | Loss: 0.00003152
Iteration 128/1000 | Loss: 0.00003152
Iteration 129/1000 | Loss: 0.00003152
Iteration 130/1000 | Loss: 0.00003152
Iteration 131/1000 | Loss: 0.00003152
Iteration 132/1000 | Loss: 0.00003152
Iteration 133/1000 | Loss: 0.00003152
Iteration 134/1000 | Loss: 0.00003152
Iteration 135/1000 | Loss: 0.00003152
Iteration 136/1000 | Loss: 0.00003152
Iteration 137/1000 | Loss: 0.00003152
Iteration 138/1000 | Loss: 0.00003152
Iteration 139/1000 | Loss: 0.00003152
Iteration 140/1000 | Loss: 0.00003152
Iteration 141/1000 | Loss: 0.00003152
Iteration 142/1000 | Loss: 0.00003152
Iteration 143/1000 | Loss: 0.00003152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [3.151993951178156e-05, 3.151993951178156e-05, 3.151993951178156e-05, 3.151993951178156e-05, 3.151993951178156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.151993951178156e-05

Optimization complete. Final v2v error: 4.047662734985352 mm

Highest mean error: 14.066642761230469 mm for frame 45

Lowest mean error: 2.807145357131958 mm for frame 146

Saving results

Total time: 157.5191261768341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059715
Iteration 2/25 | Loss: 0.01059715
Iteration 3/25 | Loss: 0.01059715
Iteration 4/25 | Loss: 0.01059715
Iteration 5/25 | Loss: 0.01059714
Iteration 6/25 | Loss: 0.01059714
Iteration 7/25 | Loss: 0.01059714
Iteration 8/25 | Loss: 0.01059714
Iteration 9/25 | Loss: 0.01059714
Iteration 10/25 | Loss: 0.01059714
Iteration 11/25 | Loss: 0.01059713
Iteration 12/25 | Loss: 0.01059713
Iteration 13/25 | Loss: 0.01059713
Iteration 14/25 | Loss: 0.01059713
Iteration 15/25 | Loss: 0.01059713
Iteration 16/25 | Loss: 0.01059712
Iteration 17/25 | Loss: 0.01059712
Iteration 18/25 | Loss: 0.01059712
Iteration 19/25 | Loss: 0.01059712
Iteration 20/25 | Loss: 0.01059712
Iteration 21/25 | Loss: 0.01059712
Iteration 22/25 | Loss: 0.01059711
Iteration 23/25 | Loss: 0.01059711
Iteration 24/25 | Loss: 0.01059711
Iteration 25/25 | Loss: 0.01059711

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55358493
Iteration 2/25 | Loss: 0.16178268
Iteration 3/25 | Loss: 0.16141084
Iteration 4/25 | Loss: 0.16136979
Iteration 5/25 | Loss: 0.16136977
Iteration 6/25 | Loss: 0.16136977
Iteration 7/25 | Loss: 0.16136974
Iteration 8/25 | Loss: 0.16136974
Iteration 9/25 | Loss: 0.16136974
Iteration 10/25 | Loss: 0.16136974
Iteration 11/25 | Loss: 0.16136974
Iteration 12/25 | Loss: 0.16136974
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.16136974096298218, 0.16136974096298218, 0.16136974096298218, 0.16136974096298218, 0.16136974096298218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.16136974096298218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16136976
Iteration 2/1000 | Loss: 0.01512601
Iteration 3/1000 | Loss: 0.00198955
Iteration 4/1000 | Loss: 0.00084258
Iteration 5/1000 | Loss: 0.00046423
Iteration 6/1000 | Loss: 0.00025091
Iteration 7/1000 | Loss: 0.00035093
Iteration 8/1000 | Loss: 0.00013522
Iteration 9/1000 | Loss: 0.00008608
Iteration 10/1000 | Loss: 0.00023676
Iteration 11/1000 | Loss: 0.00007592
Iteration 12/1000 | Loss: 0.00030462
Iteration 13/1000 | Loss: 0.00005043
Iteration 14/1000 | Loss: 0.00015804
Iteration 15/1000 | Loss: 0.00013628
Iteration 16/1000 | Loss: 0.00047235
Iteration 17/1000 | Loss: 0.00007745
Iteration 18/1000 | Loss: 0.00030308
Iteration 19/1000 | Loss: 0.00042014
Iteration 20/1000 | Loss: 0.00003012
Iteration 21/1000 | Loss: 0.00034888
Iteration 22/1000 | Loss: 0.00002797
Iteration 23/1000 | Loss: 0.00002529
Iteration 24/1000 | Loss: 0.00002509
Iteration 25/1000 | Loss: 0.00002305
Iteration 26/1000 | Loss: 0.00002191
Iteration 27/1000 | Loss: 0.00023980
Iteration 28/1000 | Loss: 0.00003005
Iteration 29/1000 | Loss: 0.00003639
Iteration 30/1000 | Loss: 0.00002035
Iteration 31/1000 | Loss: 0.00010935
Iteration 32/1000 | Loss: 0.00002069
Iteration 33/1000 | Loss: 0.00001932
Iteration 34/1000 | Loss: 0.00001895
Iteration 35/1000 | Loss: 0.00001863
Iteration 36/1000 | Loss: 0.00001901
Iteration 37/1000 | Loss: 0.00001832
Iteration 38/1000 | Loss: 0.00001826
Iteration 39/1000 | Loss: 0.00001823
Iteration 40/1000 | Loss: 0.00001821
Iteration 41/1000 | Loss: 0.00001820
Iteration 42/1000 | Loss: 0.00011761
Iteration 43/1000 | Loss: 0.00001842
Iteration 44/1000 | Loss: 0.00001808
Iteration 45/1000 | Loss: 0.00001796
Iteration 46/1000 | Loss: 0.00001796
Iteration 47/1000 | Loss: 0.00001792
Iteration 48/1000 | Loss: 0.00001792
Iteration 49/1000 | Loss: 0.00001791
Iteration 50/1000 | Loss: 0.00001791
Iteration 51/1000 | Loss: 0.00001789
Iteration 52/1000 | Loss: 0.00001786
Iteration 53/1000 | Loss: 0.00001784
Iteration 54/1000 | Loss: 0.00001783
Iteration 55/1000 | Loss: 0.00001781
Iteration 56/1000 | Loss: 0.00001780
Iteration 57/1000 | Loss: 0.00001780
Iteration 58/1000 | Loss: 0.00001780
Iteration 59/1000 | Loss: 0.00001780
Iteration 60/1000 | Loss: 0.00001780
Iteration 61/1000 | Loss: 0.00001780
Iteration 62/1000 | Loss: 0.00001780
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001779
Iteration 65/1000 | Loss: 0.00001779
Iteration 66/1000 | Loss: 0.00001778
Iteration 67/1000 | Loss: 0.00001778
Iteration 68/1000 | Loss: 0.00001776
Iteration 69/1000 | Loss: 0.00001776
Iteration 70/1000 | Loss: 0.00001777
Iteration 71/1000 | Loss: 0.00001776
Iteration 72/1000 | Loss: 0.00001775
Iteration 73/1000 | Loss: 0.00001776
Iteration 74/1000 | Loss: 0.00001776
Iteration 75/1000 | Loss: 0.00001776
Iteration 76/1000 | Loss: 0.00001775
Iteration 77/1000 | Loss: 0.00001775
Iteration 78/1000 | Loss: 0.00001775
Iteration 79/1000 | Loss: 0.00001775
Iteration 80/1000 | Loss: 0.00001775
Iteration 81/1000 | Loss: 0.00001775
Iteration 82/1000 | Loss: 0.00001775
Iteration 83/1000 | Loss: 0.00001774
Iteration 84/1000 | Loss: 0.00001774
Iteration 85/1000 | Loss: 0.00001774
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001775
Iteration 88/1000 | Loss: 0.00001775
Iteration 89/1000 | Loss: 0.00001773
Iteration 90/1000 | Loss: 0.00001773
Iteration 91/1000 | Loss: 0.00001775
Iteration 92/1000 | Loss: 0.00001775
Iteration 93/1000 | Loss: 0.00001774
Iteration 94/1000 | Loss: 0.00001774
Iteration 95/1000 | Loss: 0.00001774
Iteration 96/1000 | Loss: 0.00001774
Iteration 97/1000 | Loss: 0.00001774
Iteration 98/1000 | Loss: 0.00001774
Iteration 99/1000 | Loss: 0.00001774
Iteration 100/1000 | Loss: 0.00001773
Iteration 101/1000 | Loss: 0.00001773
Iteration 102/1000 | Loss: 0.00001773
Iteration 103/1000 | Loss: 0.00001773
Iteration 104/1000 | Loss: 0.00001773
Iteration 105/1000 | Loss: 0.00001773
Iteration 106/1000 | Loss: 0.00001773
Iteration 107/1000 | Loss: 0.00001773
Iteration 108/1000 | Loss: 0.00001773
Iteration 109/1000 | Loss: 0.00001773
Iteration 110/1000 | Loss: 0.00001773
Iteration 111/1000 | Loss: 0.00001773
Iteration 112/1000 | Loss: 0.00001773
Iteration 113/1000 | Loss: 0.00001772
Iteration 114/1000 | Loss: 0.00001772
Iteration 115/1000 | Loss: 0.00001772
Iteration 116/1000 | Loss: 0.00001772
Iteration 117/1000 | Loss: 0.00001772
Iteration 118/1000 | Loss: 0.00001772
Iteration 119/1000 | Loss: 0.00001772
Iteration 120/1000 | Loss: 0.00001772
Iteration 121/1000 | Loss: 0.00001771
Iteration 122/1000 | Loss: 0.00001771
Iteration 123/1000 | Loss: 0.00001771
Iteration 124/1000 | Loss: 0.00001771
Iteration 125/1000 | Loss: 0.00001771
Iteration 126/1000 | Loss: 0.00001771
Iteration 127/1000 | Loss: 0.00001771
Iteration 128/1000 | Loss: 0.00001771
Iteration 129/1000 | Loss: 0.00001771
Iteration 130/1000 | Loss: 0.00001771
Iteration 131/1000 | Loss: 0.00001771
Iteration 132/1000 | Loss: 0.00001771
Iteration 133/1000 | Loss: 0.00001771
Iteration 134/1000 | Loss: 0.00001771
Iteration 135/1000 | Loss: 0.00001771
Iteration 136/1000 | Loss: 0.00001771
Iteration 137/1000 | Loss: 0.00001770
Iteration 138/1000 | Loss: 0.00001770
Iteration 139/1000 | Loss: 0.00001770
Iteration 140/1000 | Loss: 0.00001770
Iteration 141/1000 | Loss: 0.00001769
Iteration 142/1000 | Loss: 0.00001769
Iteration 143/1000 | Loss: 0.00001769
Iteration 144/1000 | Loss: 0.00001769
Iteration 145/1000 | Loss: 0.00001769
Iteration 146/1000 | Loss: 0.00001769
Iteration 147/1000 | Loss: 0.00001769
Iteration 148/1000 | Loss: 0.00001769
Iteration 149/1000 | Loss: 0.00001768
Iteration 150/1000 | Loss: 0.00001768
Iteration 151/1000 | Loss: 0.00001768
Iteration 152/1000 | Loss: 0.00001768
Iteration 153/1000 | Loss: 0.00001768
Iteration 154/1000 | Loss: 0.00001768
Iteration 155/1000 | Loss: 0.00001768
Iteration 156/1000 | Loss: 0.00001768
Iteration 157/1000 | Loss: 0.00001768
Iteration 158/1000 | Loss: 0.00001768
Iteration 159/1000 | Loss: 0.00001768
Iteration 160/1000 | Loss: 0.00001768
Iteration 161/1000 | Loss: 0.00001768
Iteration 162/1000 | Loss: 0.00001768
Iteration 163/1000 | Loss: 0.00001768
Iteration 164/1000 | Loss: 0.00001768
Iteration 165/1000 | Loss: 0.00001767
Iteration 166/1000 | Loss: 0.00001767
Iteration 167/1000 | Loss: 0.00001767
Iteration 168/1000 | Loss: 0.00001767
Iteration 169/1000 | Loss: 0.00001767
Iteration 170/1000 | Loss: 0.00001767
Iteration 171/1000 | Loss: 0.00001767
Iteration 172/1000 | Loss: 0.00001767
Iteration 173/1000 | Loss: 0.00001767
Iteration 174/1000 | Loss: 0.00001767
Iteration 175/1000 | Loss: 0.00001767
Iteration 176/1000 | Loss: 0.00001767
Iteration 177/1000 | Loss: 0.00001767
Iteration 178/1000 | Loss: 0.00001767
Iteration 179/1000 | Loss: 0.00001766
Iteration 180/1000 | Loss: 0.00001766
Iteration 181/1000 | Loss: 0.00001766
Iteration 182/1000 | Loss: 0.00001766
Iteration 183/1000 | Loss: 0.00001766
Iteration 184/1000 | Loss: 0.00001766
Iteration 185/1000 | Loss: 0.00001766
Iteration 186/1000 | Loss: 0.00001766
Iteration 187/1000 | Loss: 0.00001766
Iteration 188/1000 | Loss: 0.00001768
Iteration 189/1000 | Loss: 0.00001768
Iteration 190/1000 | Loss: 0.00001768
Iteration 191/1000 | Loss: 0.00001768
Iteration 192/1000 | Loss: 0.00001767
Iteration 193/1000 | Loss: 0.00001767
Iteration 194/1000 | Loss: 0.00001767
Iteration 195/1000 | Loss: 0.00001767
Iteration 196/1000 | Loss: 0.00001767
Iteration 197/1000 | Loss: 0.00001767
Iteration 198/1000 | Loss: 0.00001767
Iteration 199/1000 | Loss: 0.00001767
Iteration 200/1000 | Loss: 0.00001767
Iteration 201/1000 | Loss: 0.00001767
Iteration 202/1000 | Loss: 0.00001767
Iteration 203/1000 | Loss: 0.00001767
Iteration 204/1000 | Loss: 0.00001767
Iteration 205/1000 | Loss: 0.00001767
Iteration 206/1000 | Loss: 0.00001767
Iteration 207/1000 | Loss: 0.00001767
Iteration 208/1000 | Loss: 0.00001767
Iteration 209/1000 | Loss: 0.00001767
Iteration 210/1000 | Loss: 0.00001767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.7672842659521848e-05, 1.7672842659521848e-05, 1.7672842659521848e-05, 1.7672842659521848e-05, 1.7672842659521848e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7672842659521848e-05

Optimization complete. Final v2v error: 3.5676746368408203 mm

Highest mean error: 9.288143157958984 mm for frame 224

Lowest mean error: 3.1983888149261475 mm for frame 103

Saving results

Total time: 85.95040845870972
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408592
Iteration 2/25 | Loss: 0.00093691
Iteration 3/25 | Loss: 0.00072463
Iteration 4/25 | Loss: 0.00069597
Iteration 5/25 | Loss: 0.00068547
Iteration 6/25 | Loss: 0.00068260
Iteration 7/25 | Loss: 0.00068156
Iteration 8/25 | Loss: 0.00068124
Iteration 9/25 | Loss: 0.00068124
Iteration 10/25 | Loss: 0.00068124
Iteration 11/25 | Loss: 0.00068124
Iteration 12/25 | Loss: 0.00068124
Iteration 13/25 | Loss: 0.00068124
Iteration 14/25 | Loss: 0.00068124
Iteration 15/25 | Loss: 0.00068124
Iteration 16/25 | Loss: 0.00068124
Iteration 17/25 | Loss: 0.00068124
Iteration 18/25 | Loss: 0.00068124
Iteration 19/25 | Loss: 0.00068124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006812385399825871, 0.0006812385399825871, 0.0006812385399825871, 0.0006812385399825871, 0.0006812385399825871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006812385399825871

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.91737413
Iteration 2/25 | Loss: 0.00029742
Iteration 3/25 | Loss: 0.00029739
Iteration 4/25 | Loss: 0.00029739
Iteration 5/25 | Loss: 0.00029739
Iteration 6/25 | Loss: 0.00029739
Iteration 7/25 | Loss: 0.00029739
Iteration 8/25 | Loss: 0.00029739
Iteration 9/25 | Loss: 0.00029739
Iteration 10/25 | Loss: 0.00029739
Iteration 11/25 | Loss: 0.00029739
Iteration 12/25 | Loss: 0.00029739
Iteration 13/25 | Loss: 0.00029739
Iteration 14/25 | Loss: 0.00029739
Iteration 15/25 | Loss: 0.00029739
Iteration 16/25 | Loss: 0.00029739
Iteration 17/25 | Loss: 0.00029739
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002973868977278471, 0.0002973868977278471, 0.0002973868977278471, 0.0002973868977278471, 0.0002973868977278471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002973868977278471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029739
Iteration 2/1000 | Loss: 0.00003768
Iteration 3/1000 | Loss: 0.00002784
Iteration 4/1000 | Loss: 0.00002528
Iteration 5/1000 | Loss: 0.00002415
Iteration 6/1000 | Loss: 0.00002338
Iteration 7/1000 | Loss: 0.00002267
Iteration 8/1000 | Loss: 0.00002218
Iteration 9/1000 | Loss: 0.00002199
Iteration 10/1000 | Loss: 0.00002177
Iteration 11/1000 | Loss: 0.00002170
Iteration 12/1000 | Loss: 0.00002166
Iteration 13/1000 | Loss: 0.00002166
Iteration 14/1000 | Loss: 0.00002165
Iteration 15/1000 | Loss: 0.00002152
Iteration 16/1000 | Loss: 0.00002152
Iteration 17/1000 | Loss: 0.00002152
Iteration 18/1000 | Loss: 0.00002151
Iteration 19/1000 | Loss: 0.00002148
Iteration 20/1000 | Loss: 0.00002148
Iteration 21/1000 | Loss: 0.00002146
Iteration 22/1000 | Loss: 0.00002146
Iteration 23/1000 | Loss: 0.00002145
Iteration 24/1000 | Loss: 0.00002144
Iteration 25/1000 | Loss: 0.00002144
Iteration 26/1000 | Loss: 0.00002143
Iteration 27/1000 | Loss: 0.00002143
Iteration 28/1000 | Loss: 0.00002143
Iteration 29/1000 | Loss: 0.00002143
Iteration 30/1000 | Loss: 0.00002142
Iteration 31/1000 | Loss: 0.00002142
Iteration 32/1000 | Loss: 0.00002142
Iteration 33/1000 | Loss: 0.00002142
Iteration 34/1000 | Loss: 0.00002142
Iteration 35/1000 | Loss: 0.00002141
Iteration 36/1000 | Loss: 0.00002141
Iteration 37/1000 | Loss: 0.00002141
Iteration 38/1000 | Loss: 0.00002141
Iteration 39/1000 | Loss: 0.00002140
Iteration 40/1000 | Loss: 0.00002140
Iteration 41/1000 | Loss: 0.00002140
Iteration 42/1000 | Loss: 0.00002140
Iteration 43/1000 | Loss: 0.00002139
Iteration 44/1000 | Loss: 0.00002139
Iteration 45/1000 | Loss: 0.00002139
Iteration 46/1000 | Loss: 0.00002139
Iteration 47/1000 | Loss: 0.00002139
Iteration 48/1000 | Loss: 0.00002139
Iteration 49/1000 | Loss: 0.00002139
Iteration 50/1000 | Loss: 0.00002139
Iteration 51/1000 | Loss: 0.00002139
Iteration 52/1000 | Loss: 0.00002138
Iteration 53/1000 | Loss: 0.00002138
Iteration 54/1000 | Loss: 0.00002138
Iteration 55/1000 | Loss: 0.00002138
Iteration 56/1000 | Loss: 0.00002138
Iteration 57/1000 | Loss: 0.00002138
Iteration 58/1000 | Loss: 0.00002138
Iteration 59/1000 | Loss: 0.00002138
Iteration 60/1000 | Loss: 0.00002138
Iteration 61/1000 | Loss: 0.00002137
Iteration 62/1000 | Loss: 0.00002137
Iteration 63/1000 | Loss: 0.00002137
Iteration 64/1000 | Loss: 0.00002136
Iteration 65/1000 | Loss: 0.00002136
Iteration 66/1000 | Loss: 0.00002136
Iteration 67/1000 | Loss: 0.00002136
Iteration 68/1000 | Loss: 0.00002135
Iteration 69/1000 | Loss: 0.00002135
Iteration 70/1000 | Loss: 0.00002135
Iteration 71/1000 | Loss: 0.00002135
Iteration 72/1000 | Loss: 0.00002135
Iteration 73/1000 | Loss: 0.00002135
Iteration 74/1000 | Loss: 0.00002135
Iteration 75/1000 | Loss: 0.00002134
Iteration 76/1000 | Loss: 0.00002134
Iteration 77/1000 | Loss: 0.00002134
Iteration 78/1000 | Loss: 0.00002133
Iteration 79/1000 | Loss: 0.00002133
Iteration 80/1000 | Loss: 0.00002133
Iteration 81/1000 | Loss: 0.00002133
Iteration 82/1000 | Loss: 0.00002133
Iteration 83/1000 | Loss: 0.00002133
Iteration 84/1000 | Loss: 0.00002133
Iteration 85/1000 | Loss: 0.00002133
Iteration 86/1000 | Loss: 0.00002133
Iteration 87/1000 | Loss: 0.00002133
Iteration 88/1000 | Loss: 0.00002132
Iteration 89/1000 | Loss: 0.00002132
Iteration 90/1000 | Loss: 0.00002132
Iteration 91/1000 | Loss: 0.00002132
Iteration 92/1000 | Loss: 0.00002131
Iteration 93/1000 | Loss: 0.00002131
Iteration 94/1000 | Loss: 0.00002131
Iteration 95/1000 | Loss: 0.00002131
Iteration 96/1000 | Loss: 0.00002131
Iteration 97/1000 | Loss: 0.00002131
Iteration 98/1000 | Loss: 0.00002130
Iteration 99/1000 | Loss: 0.00002130
Iteration 100/1000 | Loss: 0.00002130
Iteration 101/1000 | Loss: 0.00002130
Iteration 102/1000 | Loss: 0.00002129
Iteration 103/1000 | Loss: 0.00002129
Iteration 104/1000 | Loss: 0.00002129
Iteration 105/1000 | Loss: 0.00002129
Iteration 106/1000 | Loss: 0.00002129
Iteration 107/1000 | Loss: 0.00002129
Iteration 108/1000 | Loss: 0.00002129
Iteration 109/1000 | Loss: 0.00002128
Iteration 110/1000 | Loss: 0.00002128
Iteration 111/1000 | Loss: 0.00002128
Iteration 112/1000 | Loss: 0.00002128
Iteration 113/1000 | Loss: 0.00002128
Iteration 114/1000 | Loss: 0.00002128
Iteration 115/1000 | Loss: 0.00002128
Iteration 116/1000 | Loss: 0.00002128
Iteration 117/1000 | Loss: 0.00002128
Iteration 118/1000 | Loss: 0.00002128
Iteration 119/1000 | Loss: 0.00002128
Iteration 120/1000 | Loss: 0.00002128
Iteration 121/1000 | Loss: 0.00002127
Iteration 122/1000 | Loss: 0.00002127
Iteration 123/1000 | Loss: 0.00002127
Iteration 124/1000 | Loss: 0.00002127
Iteration 125/1000 | Loss: 0.00002127
Iteration 126/1000 | Loss: 0.00002126
Iteration 127/1000 | Loss: 0.00002126
Iteration 128/1000 | Loss: 0.00002126
Iteration 129/1000 | Loss: 0.00002126
Iteration 130/1000 | Loss: 0.00002126
Iteration 131/1000 | Loss: 0.00002126
Iteration 132/1000 | Loss: 0.00002126
Iteration 133/1000 | Loss: 0.00002126
Iteration 134/1000 | Loss: 0.00002126
Iteration 135/1000 | Loss: 0.00002126
Iteration 136/1000 | Loss: 0.00002126
Iteration 137/1000 | Loss: 0.00002126
Iteration 138/1000 | Loss: 0.00002126
Iteration 139/1000 | Loss: 0.00002126
Iteration 140/1000 | Loss: 0.00002126
Iteration 141/1000 | Loss: 0.00002126
Iteration 142/1000 | Loss: 0.00002126
Iteration 143/1000 | Loss: 0.00002125
Iteration 144/1000 | Loss: 0.00002125
Iteration 145/1000 | Loss: 0.00002125
Iteration 146/1000 | Loss: 0.00002125
Iteration 147/1000 | Loss: 0.00002125
Iteration 148/1000 | Loss: 0.00002125
Iteration 149/1000 | Loss: 0.00002125
Iteration 150/1000 | Loss: 0.00002125
Iteration 151/1000 | Loss: 0.00002125
Iteration 152/1000 | Loss: 0.00002125
Iteration 153/1000 | Loss: 0.00002125
Iteration 154/1000 | Loss: 0.00002125
Iteration 155/1000 | Loss: 0.00002125
Iteration 156/1000 | Loss: 0.00002125
Iteration 157/1000 | Loss: 0.00002125
Iteration 158/1000 | Loss: 0.00002125
Iteration 159/1000 | Loss: 0.00002124
Iteration 160/1000 | Loss: 0.00002124
Iteration 161/1000 | Loss: 0.00002124
Iteration 162/1000 | Loss: 0.00002124
Iteration 163/1000 | Loss: 0.00002124
Iteration 164/1000 | Loss: 0.00002124
Iteration 165/1000 | Loss: 0.00002124
Iteration 166/1000 | Loss: 0.00002124
Iteration 167/1000 | Loss: 0.00002124
Iteration 168/1000 | Loss: 0.00002124
Iteration 169/1000 | Loss: 0.00002124
Iteration 170/1000 | Loss: 0.00002124
Iteration 171/1000 | Loss: 0.00002124
Iteration 172/1000 | Loss: 0.00002124
Iteration 173/1000 | Loss: 0.00002124
Iteration 174/1000 | Loss: 0.00002124
Iteration 175/1000 | Loss: 0.00002124
Iteration 176/1000 | Loss: 0.00002124
Iteration 177/1000 | Loss: 0.00002124
Iteration 178/1000 | Loss: 0.00002124
Iteration 179/1000 | Loss: 0.00002124
Iteration 180/1000 | Loss: 0.00002124
Iteration 181/1000 | Loss: 0.00002124
Iteration 182/1000 | Loss: 0.00002124
Iteration 183/1000 | Loss: 0.00002124
Iteration 184/1000 | Loss: 0.00002124
Iteration 185/1000 | Loss: 0.00002124
Iteration 186/1000 | Loss: 0.00002124
Iteration 187/1000 | Loss: 0.00002124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.1236004613456316e-05, 2.1236004613456316e-05, 2.1236004613456316e-05, 2.1236004613456316e-05, 2.1236004613456316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1236004613456316e-05

Optimization complete. Final v2v error: 3.8064184188842773 mm

Highest mean error: 4.311319351196289 mm for frame 49

Lowest mean error: 3.435478448867798 mm for frame 85

Saving results

Total time: 38.07116103172302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052154
Iteration 2/25 | Loss: 0.00192813
Iteration 3/25 | Loss: 0.00128331
Iteration 4/25 | Loss: 0.00138306
Iteration 5/25 | Loss: 0.00100345
Iteration 6/25 | Loss: 0.00105095
Iteration 7/25 | Loss: 0.00101216
Iteration 8/25 | Loss: 0.00088370
Iteration 9/25 | Loss: 0.00076829
Iteration 10/25 | Loss: 0.00072091
Iteration 11/25 | Loss: 0.00068457
Iteration 12/25 | Loss: 0.00067321
Iteration 13/25 | Loss: 0.00066964
Iteration 14/25 | Loss: 0.00065383
Iteration 15/25 | Loss: 0.00064756
Iteration 16/25 | Loss: 0.00064331
Iteration 17/25 | Loss: 0.00063445
Iteration 18/25 | Loss: 0.00063236
Iteration 19/25 | Loss: 0.00062758
Iteration 20/25 | Loss: 0.00062564
Iteration 21/25 | Loss: 0.00062666
Iteration 22/25 | Loss: 0.00062221
Iteration 23/25 | Loss: 0.00062490
Iteration 24/25 | Loss: 0.00061677
Iteration 25/25 | Loss: 0.00061349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50646842
Iteration 2/25 | Loss: 0.00043904
Iteration 3/25 | Loss: 0.00042160
Iteration 4/25 | Loss: 0.00042160
Iteration 5/25 | Loss: 0.00042160
Iteration 6/25 | Loss: 0.00042159
Iteration 7/25 | Loss: 0.00042159
Iteration 8/25 | Loss: 0.00042159
Iteration 9/25 | Loss: 0.00042159
Iteration 10/25 | Loss: 0.00042159
Iteration 11/25 | Loss: 0.00042159
Iteration 12/25 | Loss: 0.00042159
Iteration 13/25 | Loss: 0.00042159
Iteration 14/25 | Loss: 0.00042159
Iteration 15/25 | Loss: 0.00042159
Iteration 16/25 | Loss: 0.00042159
Iteration 17/25 | Loss: 0.00042159
Iteration 18/25 | Loss: 0.00042159
Iteration 19/25 | Loss: 0.00042159
Iteration 20/25 | Loss: 0.00042159
Iteration 21/25 | Loss: 0.00042159
Iteration 22/25 | Loss: 0.00042159
Iteration 23/25 | Loss: 0.00042159
Iteration 24/25 | Loss: 0.00042159
Iteration 25/25 | Loss: 0.00042159

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042159
Iteration 2/1000 | Loss: 0.00015844
Iteration 3/1000 | Loss: 0.00015559
Iteration 4/1000 | Loss: 0.00016751
Iteration 5/1000 | Loss: 0.00017319
Iteration 6/1000 | Loss: 0.00022482
Iteration 7/1000 | Loss: 0.00035966
Iteration 8/1000 | Loss: 0.00028077
Iteration 9/1000 | Loss: 0.00019584
Iteration 10/1000 | Loss: 0.00027388
Iteration 11/1000 | Loss: 0.00016982
Iteration 12/1000 | Loss: 0.00028057
Iteration 13/1000 | Loss: 0.00016648
Iteration 14/1000 | Loss: 0.00017295
Iteration 15/1000 | Loss: 0.00016966
Iteration 16/1000 | Loss: 0.00018959
Iteration 17/1000 | Loss: 0.00010610
Iteration 18/1000 | Loss: 0.00011914
Iteration 19/1000 | Loss: 0.00025901
Iteration 20/1000 | Loss: 0.00006908
Iteration 21/1000 | Loss: 0.00032383
Iteration 22/1000 | Loss: 0.00019335
Iteration 23/1000 | Loss: 0.00030404
Iteration 24/1000 | Loss: 0.00014789
Iteration 25/1000 | Loss: 0.00034516
Iteration 26/1000 | Loss: 0.00022307
Iteration 27/1000 | Loss: 0.00009637
Iteration 28/1000 | Loss: 0.00007341
Iteration 29/1000 | Loss: 0.00011775
Iteration 30/1000 | Loss: 0.00052119
Iteration 31/1000 | Loss: 0.00020444
Iteration 32/1000 | Loss: 0.00034766
Iteration 33/1000 | Loss: 0.00018101
Iteration 34/1000 | Loss: 0.00027150
Iteration 35/1000 | Loss: 0.00007088
Iteration 36/1000 | Loss: 0.00002026
Iteration 37/1000 | Loss: 0.00012405
Iteration 38/1000 | Loss: 0.00020373
Iteration 39/1000 | Loss: 0.00019442
Iteration 40/1000 | Loss: 0.00008222
Iteration 41/1000 | Loss: 0.00030679
Iteration 42/1000 | Loss: 0.00009308
Iteration 43/1000 | Loss: 0.00015290
Iteration 44/1000 | Loss: 0.00017810
Iteration 45/1000 | Loss: 0.00014132
Iteration 46/1000 | Loss: 0.00009295
Iteration 47/1000 | Loss: 0.00008073
Iteration 48/1000 | Loss: 0.00004497
Iteration 49/1000 | Loss: 0.00016054
Iteration 50/1000 | Loss: 0.00017078
Iteration 51/1000 | Loss: 0.00018306
Iteration 52/1000 | Loss: 0.00030439
Iteration 53/1000 | Loss: 0.00018827
Iteration 54/1000 | Loss: 0.00024354
Iteration 55/1000 | Loss: 0.00018337
Iteration 56/1000 | Loss: 0.00023531
Iteration 57/1000 | Loss: 0.00025985
Iteration 58/1000 | Loss: 0.00013276
Iteration 59/1000 | Loss: 0.00022187
Iteration 60/1000 | Loss: 0.00015852
Iteration 61/1000 | Loss: 0.00023190
Iteration 62/1000 | Loss: 0.00013917
Iteration 63/1000 | Loss: 0.00002860
Iteration 64/1000 | Loss: 0.00002047
Iteration 65/1000 | Loss: 0.00001787
Iteration 66/1000 | Loss: 0.00007208
Iteration 67/1000 | Loss: 0.00001789
Iteration 68/1000 | Loss: 0.00001705
Iteration 69/1000 | Loss: 0.00001648
Iteration 70/1000 | Loss: 0.00001694
Iteration 71/1000 | Loss: 0.00001567
Iteration 72/1000 | Loss: 0.00001539
Iteration 73/1000 | Loss: 0.00001511
Iteration 74/1000 | Loss: 0.00001504
Iteration 75/1000 | Loss: 0.00001499
Iteration 76/1000 | Loss: 0.00001490
Iteration 77/1000 | Loss: 0.00001483
Iteration 78/1000 | Loss: 0.00001481
Iteration 79/1000 | Loss: 0.00001480
Iteration 80/1000 | Loss: 0.00001480
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001478
Iteration 84/1000 | Loss: 0.00001477
Iteration 85/1000 | Loss: 0.00001477
Iteration 86/1000 | Loss: 0.00001477
Iteration 87/1000 | Loss: 0.00001469
Iteration 88/1000 | Loss: 0.00001468
Iteration 89/1000 | Loss: 0.00001467
Iteration 90/1000 | Loss: 0.00001466
Iteration 91/1000 | Loss: 0.00001466
Iteration 92/1000 | Loss: 0.00001466
Iteration 93/1000 | Loss: 0.00001466
Iteration 94/1000 | Loss: 0.00001466
Iteration 95/1000 | Loss: 0.00001466
Iteration 96/1000 | Loss: 0.00001466
Iteration 97/1000 | Loss: 0.00001466
Iteration 98/1000 | Loss: 0.00001465
Iteration 99/1000 | Loss: 0.00001465
Iteration 100/1000 | Loss: 0.00001465
Iteration 101/1000 | Loss: 0.00001465
Iteration 102/1000 | Loss: 0.00001465
Iteration 103/1000 | Loss: 0.00001465
Iteration 104/1000 | Loss: 0.00001465
Iteration 105/1000 | Loss: 0.00001465
Iteration 106/1000 | Loss: 0.00001465
Iteration 107/1000 | Loss: 0.00001465
Iteration 108/1000 | Loss: 0.00001464
Iteration 109/1000 | Loss: 0.00001464
Iteration 110/1000 | Loss: 0.00001463
Iteration 111/1000 | Loss: 0.00001463
Iteration 112/1000 | Loss: 0.00001463
Iteration 113/1000 | Loss: 0.00001463
Iteration 114/1000 | Loss: 0.00001463
Iteration 115/1000 | Loss: 0.00001462
Iteration 116/1000 | Loss: 0.00001462
Iteration 117/1000 | Loss: 0.00001462
Iteration 118/1000 | Loss: 0.00001462
Iteration 119/1000 | Loss: 0.00001462
Iteration 120/1000 | Loss: 0.00001462
Iteration 121/1000 | Loss: 0.00001461
Iteration 122/1000 | Loss: 0.00001461
Iteration 123/1000 | Loss: 0.00001461
Iteration 124/1000 | Loss: 0.00001461
Iteration 125/1000 | Loss: 0.00001461
Iteration 126/1000 | Loss: 0.00001460
Iteration 127/1000 | Loss: 0.00001460
Iteration 128/1000 | Loss: 0.00001460
Iteration 129/1000 | Loss: 0.00001460
Iteration 130/1000 | Loss: 0.00001460
Iteration 131/1000 | Loss: 0.00001460
Iteration 132/1000 | Loss: 0.00001460
Iteration 133/1000 | Loss: 0.00001460
Iteration 134/1000 | Loss: 0.00001459
Iteration 135/1000 | Loss: 0.00001459
Iteration 136/1000 | Loss: 0.00001459
Iteration 137/1000 | Loss: 0.00001459
Iteration 138/1000 | Loss: 0.00001458
Iteration 139/1000 | Loss: 0.00001458
Iteration 140/1000 | Loss: 0.00001458
Iteration 141/1000 | Loss: 0.00001457
Iteration 142/1000 | Loss: 0.00001457
Iteration 143/1000 | Loss: 0.00001457
Iteration 144/1000 | Loss: 0.00001457
Iteration 145/1000 | Loss: 0.00001457
Iteration 146/1000 | Loss: 0.00001456
Iteration 147/1000 | Loss: 0.00001456
Iteration 148/1000 | Loss: 0.00001456
Iteration 149/1000 | Loss: 0.00001456
Iteration 150/1000 | Loss: 0.00001455
Iteration 151/1000 | Loss: 0.00001455
Iteration 152/1000 | Loss: 0.00001455
Iteration 153/1000 | Loss: 0.00001455
Iteration 154/1000 | Loss: 0.00001455
Iteration 155/1000 | Loss: 0.00001455
Iteration 156/1000 | Loss: 0.00001455
Iteration 157/1000 | Loss: 0.00001455
Iteration 158/1000 | Loss: 0.00001455
Iteration 159/1000 | Loss: 0.00001454
Iteration 160/1000 | Loss: 0.00001454
Iteration 161/1000 | Loss: 0.00001454
Iteration 162/1000 | Loss: 0.00001454
Iteration 163/1000 | Loss: 0.00001454
Iteration 164/1000 | Loss: 0.00001454
Iteration 165/1000 | Loss: 0.00001454
Iteration 166/1000 | Loss: 0.00001454
Iteration 167/1000 | Loss: 0.00001454
Iteration 168/1000 | Loss: 0.00001454
Iteration 169/1000 | Loss: 0.00001454
Iteration 170/1000 | Loss: 0.00001453
Iteration 171/1000 | Loss: 0.00001453
Iteration 172/1000 | Loss: 0.00001453
Iteration 173/1000 | Loss: 0.00001453
Iteration 174/1000 | Loss: 0.00001453
Iteration 175/1000 | Loss: 0.00001453
Iteration 176/1000 | Loss: 0.00001453
Iteration 177/1000 | Loss: 0.00001453
Iteration 178/1000 | Loss: 0.00001452
Iteration 179/1000 | Loss: 0.00001452
Iteration 180/1000 | Loss: 0.00001452
Iteration 181/1000 | Loss: 0.00001451
Iteration 182/1000 | Loss: 0.00001451
Iteration 183/1000 | Loss: 0.00001451
Iteration 184/1000 | Loss: 0.00001451
Iteration 185/1000 | Loss: 0.00001450
Iteration 186/1000 | Loss: 0.00001450
Iteration 187/1000 | Loss: 0.00001450
Iteration 188/1000 | Loss: 0.00001449
Iteration 189/1000 | Loss: 0.00001449
Iteration 190/1000 | Loss: 0.00001449
Iteration 191/1000 | Loss: 0.00001449
Iteration 192/1000 | Loss: 0.00001448
Iteration 193/1000 | Loss: 0.00001448
Iteration 194/1000 | Loss: 0.00001448
Iteration 195/1000 | Loss: 0.00001448
Iteration 196/1000 | Loss: 0.00001448
Iteration 197/1000 | Loss: 0.00001448
Iteration 198/1000 | Loss: 0.00001448
Iteration 199/1000 | Loss: 0.00001448
Iteration 200/1000 | Loss: 0.00001448
Iteration 201/1000 | Loss: 0.00001448
Iteration 202/1000 | Loss: 0.00001448
Iteration 203/1000 | Loss: 0.00001447
Iteration 204/1000 | Loss: 0.00001447
Iteration 205/1000 | Loss: 0.00001447
Iteration 206/1000 | Loss: 0.00001447
Iteration 207/1000 | Loss: 0.00001447
Iteration 208/1000 | Loss: 0.00001447
Iteration 209/1000 | Loss: 0.00001447
Iteration 210/1000 | Loss: 0.00001447
Iteration 211/1000 | Loss: 0.00001447
Iteration 212/1000 | Loss: 0.00001447
Iteration 213/1000 | Loss: 0.00001447
Iteration 214/1000 | Loss: 0.00001447
Iteration 215/1000 | Loss: 0.00001447
Iteration 216/1000 | Loss: 0.00001447
Iteration 217/1000 | Loss: 0.00001447
Iteration 218/1000 | Loss: 0.00001447
Iteration 219/1000 | Loss: 0.00001447
Iteration 220/1000 | Loss: 0.00001447
Iteration 221/1000 | Loss: 0.00001446
Iteration 222/1000 | Loss: 0.00001446
Iteration 223/1000 | Loss: 0.00001446
Iteration 224/1000 | Loss: 0.00001446
Iteration 225/1000 | Loss: 0.00001446
Iteration 226/1000 | Loss: 0.00001446
Iteration 227/1000 | Loss: 0.00001446
Iteration 228/1000 | Loss: 0.00001446
Iteration 229/1000 | Loss: 0.00001446
Iteration 230/1000 | Loss: 0.00001446
Iteration 231/1000 | Loss: 0.00001446
Iteration 232/1000 | Loss: 0.00001446
Iteration 233/1000 | Loss: 0.00001446
Iteration 234/1000 | Loss: 0.00001446
Iteration 235/1000 | Loss: 0.00001446
Iteration 236/1000 | Loss: 0.00001446
Iteration 237/1000 | Loss: 0.00001446
Iteration 238/1000 | Loss: 0.00007415
Iteration 239/1000 | Loss: 0.00001477
Iteration 240/1000 | Loss: 0.00001451
Iteration 241/1000 | Loss: 0.00001448
Iteration 242/1000 | Loss: 0.00001448
Iteration 243/1000 | Loss: 0.00001447
Iteration 244/1000 | Loss: 0.00001447
Iteration 245/1000 | Loss: 0.00001447
Iteration 246/1000 | Loss: 0.00001447
Iteration 247/1000 | Loss: 0.00001447
Iteration 248/1000 | Loss: 0.00001447
Iteration 249/1000 | Loss: 0.00001446
Iteration 250/1000 | Loss: 0.00001446
Iteration 251/1000 | Loss: 0.00001446
Iteration 252/1000 | Loss: 0.00001446
Iteration 253/1000 | Loss: 0.00001446
Iteration 254/1000 | Loss: 0.00001446
Iteration 255/1000 | Loss: 0.00001446
Iteration 256/1000 | Loss: 0.00001446
Iteration 257/1000 | Loss: 0.00001446
Iteration 258/1000 | Loss: 0.00001446
Iteration 259/1000 | Loss: 0.00001446
Iteration 260/1000 | Loss: 0.00001446
Iteration 261/1000 | Loss: 0.00001445
Iteration 262/1000 | Loss: 0.00001445
Iteration 263/1000 | Loss: 0.00001445
Iteration 264/1000 | Loss: 0.00001445
Iteration 265/1000 | Loss: 0.00001445
Iteration 266/1000 | Loss: 0.00001445
Iteration 267/1000 | Loss: 0.00001445
Iteration 268/1000 | Loss: 0.00001445
Iteration 269/1000 | Loss: 0.00001445
Iteration 270/1000 | Loss: 0.00001445
Iteration 271/1000 | Loss: 0.00001445
Iteration 272/1000 | Loss: 0.00001445
Iteration 273/1000 | Loss: 0.00001445
Iteration 274/1000 | Loss: 0.00001444
Iteration 275/1000 | Loss: 0.00001444
Iteration 276/1000 | Loss: 0.00001444
Iteration 277/1000 | Loss: 0.00001444
Iteration 278/1000 | Loss: 0.00001444
Iteration 279/1000 | Loss: 0.00001444
Iteration 280/1000 | Loss: 0.00001444
Iteration 281/1000 | Loss: 0.00001444
Iteration 282/1000 | Loss: 0.00001444
Iteration 283/1000 | Loss: 0.00001444
Iteration 284/1000 | Loss: 0.00001444
Iteration 285/1000 | Loss: 0.00001444
Iteration 286/1000 | Loss: 0.00001444
Iteration 287/1000 | Loss: 0.00001444
Iteration 288/1000 | Loss: 0.00001444
Iteration 289/1000 | Loss: 0.00001444
Iteration 290/1000 | Loss: 0.00001444
Iteration 291/1000 | Loss: 0.00001444
Iteration 292/1000 | Loss: 0.00001444
Iteration 293/1000 | Loss: 0.00001444
Iteration 294/1000 | Loss: 0.00001443
Iteration 295/1000 | Loss: 0.00001443
Iteration 296/1000 | Loss: 0.00001443
Iteration 297/1000 | Loss: 0.00001443
Iteration 298/1000 | Loss: 0.00001443
Iteration 299/1000 | Loss: 0.00001443
Iteration 300/1000 | Loss: 0.00001443
Iteration 301/1000 | Loss: 0.00001443
Iteration 302/1000 | Loss: 0.00001443
Iteration 303/1000 | Loss: 0.00001443
Iteration 304/1000 | Loss: 0.00001443
Iteration 305/1000 | Loss: 0.00001443
Iteration 306/1000 | Loss: 0.00001443
Iteration 307/1000 | Loss: 0.00001443
Iteration 308/1000 | Loss: 0.00001443
Iteration 309/1000 | Loss: 0.00001443
Iteration 310/1000 | Loss: 0.00001443
Iteration 311/1000 | Loss: 0.00001443
Iteration 312/1000 | Loss: 0.00001443
Iteration 313/1000 | Loss: 0.00001443
Iteration 314/1000 | Loss: 0.00001443
Iteration 315/1000 | Loss: 0.00001443
Iteration 316/1000 | Loss: 0.00001443
Iteration 317/1000 | Loss: 0.00001443
Iteration 318/1000 | Loss: 0.00001443
Iteration 319/1000 | Loss: 0.00001443
Iteration 320/1000 | Loss: 0.00001443
Iteration 321/1000 | Loss: 0.00001443
Iteration 322/1000 | Loss: 0.00001443
Iteration 323/1000 | Loss: 0.00001443
Iteration 324/1000 | Loss: 0.00001443
Iteration 325/1000 | Loss: 0.00001443
Iteration 326/1000 | Loss: 0.00001443
Iteration 327/1000 | Loss: 0.00001443
Iteration 328/1000 | Loss: 0.00001443
Iteration 329/1000 | Loss: 0.00001443
Iteration 330/1000 | Loss: 0.00001443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 330. Stopping optimization.
Last 5 losses: [1.443256223865319e-05, 1.443256223865319e-05, 1.443256223865319e-05, 1.443256223865319e-05, 1.443256223865319e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.443256223865319e-05

Optimization complete. Final v2v error: 3.1764376163482666 mm

Highest mean error: 4.05556583404541 mm for frame 86

Lowest mean error: 2.5608572959899902 mm for frame 44

Saving results

Total time: 167.04395937919617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01134064
Iteration 2/25 | Loss: 0.01134064
Iteration 3/25 | Loss: 0.01134064
Iteration 4/25 | Loss: 0.01134063
Iteration 5/25 | Loss: 0.01134063
Iteration 6/25 | Loss: 0.01134063
Iteration 7/25 | Loss: 0.01134063
Iteration 8/25 | Loss: 0.01134063
Iteration 9/25 | Loss: 0.01134063
Iteration 10/25 | Loss: 0.01134063
Iteration 11/25 | Loss: 0.01134063
Iteration 12/25 | Loss: 0.01134063
Iteration 13/25 | Loss: 0.01134062
Iteration 14/25 | Loss: 0.01134062
Iteration 15/25 | Loss: 0.01134062
Iteration 16/25 | Loss: 0.01134062
Iteration 17/25 | Loss: 0.01134062
Iteration 18/25 | Loss: 0.01134062
Iteration 19/25 | Loss: 0.01134062
Iteration 20/25 | Loss: 0.01134062
Iteration 21/25 | Loss: 0.01134062
Iteration 22/25 | Loss: 0.01134062
Iteration 23/25 | Loss: 0.01134061
Iteration 24/25 | Loss: 0.01134061
Iteration 25/25 | Loss: 0.01134061

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77022111
Iteration 2/25 | Loss: 0.14892870
Iteration 3/25 | Loss: 0.14892705
Iteration 4/25 | Loss: 0.14892703
Iteration 5/25 | Loss: 0.14892703
Iteration 6/25 | Loss: 0.14892703
Iteration 7/25 | Loss: 0.14892703
Iteration 8/25 | Loss: 0.14892702
Iteration 9/25 | Loss: 0.14892702
Iteration 10/25 | Loss: 0.14892702
Iteration 11/25 | Loss: 0.14892702
Iteration 12/25 | Loss: 0.14892702
Iteration 13/25 | Loss: 0.14892702
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.1489270180463791, 0.1489270180463791, 0.1489270180463791, 0.1489270180463791, 0.1489270180463791]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1489270180463791

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.14892702
Iteration 2/1000 | Loss: 0.00556951
Iteration 3/1000 | Loss: 0.00112711
Iteration 4/1000 | Loss: 0.00024692
Iteration 5/1000 | Loss: 0.00025132
Iteration 6/1000 | Loss: 0.00018066
Iteration 7/1000 | Loss: 0.00005757
Iteration 8/1000 | Loss: 0.00012594
Iteration 9/1000 | Loss: 0.00033734
Iteration 10/1000 | Loss: 0.00063958
Iteration 11/1000 | Loss: 0.00005229
Iteration 12/1000 | Loss: 0.00015320
Iteration 13/1000 | Loss: 0.00003740
Iteration 14/1000 | Loss: 0.00030401
Iteration 15/1000 | Loss: 0.00003354
Iteration 16/1000 | Loss: 0.00020807
Iteration 17/1000 | Loss: 0.00008656
Iteration 18/1000 | Loss: 0.00003863
Iteration 19/1000 | Loss: 0.00007429
Iteration 20/1000 | Loss: 0.00003517
Iteration 21/1000 | Loss: 0.00007250
Iteration 22/1000 | Loss: 0.00004631
Iteration 23/1000 | Loss: 0.00002656
Iteration 24/1000 | Loss: 0.00003898
Iteration 25/1000 | Loss: 0.00003168
Iteration 26/1000 | Loss: 0.00002529
Iteration 27/1000 | Loss: 0.00003027
Iteration 28/1000 | Loss: 0.00006484
Iteration 29/1000 | Loss: 0.00002375
Iteration 30/1000 | Loss: 0.00012809
Iteration 31/1000 | Loss: 0.00003838
Iteration 32/1000 | Loss: 0.00005614
Iteration 33/1000 | Loss: 0.00005126
Iteration 34/1000 | Loss: 0.00002486
Iteration 35/1000 | Loss: 0.00002480
Iteration 36/1000 | Loss: 0.00006917
Iteration 37/1000 | Loss: 0.00009205
Iteration 38/1000 | Loss: 0.00009632
Iteration 39/1000 | Loss: 0.00016699
Iteration 40/1000 | Loss: 0.00006694
Iteration 41/1000 | Loss: 0.00002385
Iteration 42/1000 | Loss: 0.00002433
Iteration 43/1000 | Loss: 0.00002599
Iteration 44/1000 | Loss: 0.00004648
Iteration 45/1000 | Loss: 0.00002175
Iteration 46/1000 | Loss: 0.00002143
Iteration 47/1000 | Loss: 0.00002135
Iteration 48/1000 | Loss: 0.00002124
Iteration 49/1000 | Loss: 0.00002121
Iteration 50/1000 | Loss: 0.00002121
Iteration 51/1000 | Loss: 0.00002121
Iteration 52/1000 | Loss: 0.00002121
Iteration 53/1000 | Loss: 0.00002121
Iteration 54/1000 | Loss: 0.00002121
Iteration 55/1000 | Loss: 0.00002121
Iteration 56/1000 | Loss: 0.00002121
Iteration 57/1000 | Loss: 0.00002121
Iteration 58/1000 | Loss: 0.00002120
Iteration 59/1000 | Loss: 0.00002120
Iteration 60/1000 | Loss: 0.00002120
Iteration 61/1000 | Loss: 0.00002120
Iteration 62/1000 | Loss: 0.00002120
Iteration 63/1000 | Loss: 0.00002120
Iteration 64/1000 | Loss: 0.00002120
Iteration 65/1000 | Loss: 0.00002120
Iteration 66/1000 | Loss: 0.00002120
Iteration 67/1000 | Loss: 0.00002119
Iteration 68/1000 | Loss: 0.00002119
Iteration 69/1000 | Loss: 0.00002118
Iteration 70/1000 | Loss: 0.00002118
Iteration 71/1000 | Loss: 0.00002118
Iteration 72/1000 | Loss: 0.00002117
Iteration 73/1000 | Loss: 0.00002117
Iteration 74/1000 | Loss: 0.00002117
Iteration 75/1000 | Loss: 0.00002117
Iteration 76/1000 | Loss: 0.00002117
Iteration 77/1000 | Loss: 0.00002116
Iteration 78/1000 | Loss: 0.00002116
Iteration 79/1000 | Loss: 0.00002116
Iteration 80/1000 | Loss: 0.00002116
Iteration 81/1000 | Loss: 0.00002116
Iteration 82/1000 | Loss: 0.00002116
Iteration 83/1000 | Loss: 0.00002115
Iteration 84/1000 | Loss: 0.00002115
Iteration 85/1000 | Loss: 0.00002115
Iteration 86/1000 | Loss: 0.00002115
Iteration 87/1000 | Loss: 0.00002115
Iteration 88/1000 | Loss: 0.00002114
Iteration 89/1000 | Loss: 0.00002114
Iteration 90/1000 | Loss: 0.00002114
Iteration 91/1000 | Loss: 0.00002113
Iteration 92/1000 | Loss: 0.00002113
Iteration 93/1000 | Loss: 0.00002113
Iteration 94/1000 | Loss: 0.00002113
Iteration 95/1000 | Loss: 0.00002113
Iteration 96/1000 | Loss: 0.00002113
Iteration 97/1000 | Loss: 0.00002324
Iteration 98/1000 | Loss: 0.00002114
Iteration 99/1000 | Loss: 0.00002114
Iteration 100/1000 | Loss: 0.00002112
Iteration 101/1000 | Loss: 0.00002112
Iteration 102/1000 | Loss: 0.00002112
Iteration 103/1000 | Loss: 0.00002112
Iteration 104/1000 | Loss: 0.00002112
Iteration 105/1000 | Loss: 0.00002112
Iteration 106/1000 | Loss: 0.00002112
Iteration 107/1000 | Loss: 0.00002112
Iteration 108/1000 | Loss: 0.00002112
Iteration 109/1000 | Loss: 0.00002111
Iteration 110/1000 | Loss: 0.00002111
Iteration 111/1000 | Loss: 0.00002111
Iteration 112/1000 | Loss: 0.00002111
Iteration 113/1000 | Loss: 0.00002111
Iteration 114/1000 | Loss: 0.00002111
Iteration 115/1000 | Loss: 0.00002110
Iteration 116/1000 | Loss: 0.00002110
Iteration 117/1000 | Loss: 0.00002110
Iteration 118/1000 | Loss: 0.00002110
Iteration 119/1000 | Loss: 0.00002110
Iteration 120/1000 | Loss: 0.00002109
Iteration 121/1000 | Loss: 0.00002109
Iteration 122/1000 | Loss: 0.00002109
Iteration 123/1000 | Loss: 0.00002109
Iteration 124/1000 | Loss: 0.00002109
Iteration 125/1000 | Loss: 0.00002109
Iteration 126/1000 | Loss: 0.00002109
Iteration 127/1000 | Loss: 0.00002109
Iteration 128/1000 | Loss: 0.00002109
Iteration 129/1000 | Loss: 0.00002109
Iteration 130/1000 | Loss: 0.00002109
Iteration 131/1000 | Loss: 0.00002109
Iteration 132/1000 | Loss: 0.00002109
Iteration 133/1000 | Loss: 0.00002109
Iteration 134/1000 | Loss: 0.00002109
Iteration 135/1000 | Loss: 0.00002109
Iteration 136/1000 | Loss: 0.00006812
Iteration 137/1000 | Loss: 0.00002365
Iteration 138/1000 | Loss: 0.00003217
Iteration 139/1000 | Loss: 0.00005468
Iteration 140/1000 | Loss: 0.00009974
Iteration 141/1000 | Loss: 0.00005004
Iteration 142/1000 | Loss: 0.00002180
Iteration 143/1000 | Loss: 0.00002176
Iteration 144/1000 | Loss: 0.00002108
Iteration 145/1000 | Loss: 0.00002107
Iteration 146/1000 | Loss: 0.00002107
Iteration 147/1000 | Loss: 0.00002107
Iteration 148/1000 | Loss: 0.00002107
Iteration 149/1000 | Loss: 0.00002107
Iteration 150/1000 | Loss: 0.00002107
Iteration 151/1000 | Loss: 0.00002107
Iteration 152/1000 | Loss: 0.00002107
Iteration 153/1000 | Loss: 0.00002107
Iteration 154/1000 | Loss: 0.00002106
Iteration 155/1000 | Loss: 0.00002106
Iteration 156/1000 | Loss: 0.00002106
Iteration 157/1000 | Loss: 0.00002106
Iteration 158/1000 | Loss: 0.00002106
Iteration 159/1000 | Loss: 0.00002106
Iteration 160/1000 | Loss: 0.00002115
Iteration 161/1000 | Loss: 0.00002115
Iteration 162/1000 | Loss: 0.00002146
Iteration 163/1000 | Loss: 0.00002105
Iteration 164/1000 | Loss: 0.00002104
Iteration 165/1000 | Loss: 0.00002104
Iteration 166/1000 | Loss: 0.00002104
Iteration 167/1000 | Loss: 0.00002103
Iteration 168/1000 | Loss: 0.00002103
Iteration 169/1000 | Loss: 0.00002103
Iteration 170/1000 | Loss: 0.00002103
Iteration 171/1000 | Loss: 0.00002103
Iteration 172/1000 | Loss: 0.00002103
Iteration 173/1000 | Loss: 0.00002103
Iteration 174/1000 | Loss: 0.00002103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.1034338715253398e-05, 2.1034338715253398e-05, 2.1034338715253398e-05, 2.1034338715253398e-05, 2.1034338715253398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1034338715253398e-05

Optimization complete. Final v2v error: 3.7834243774414062 mm

Highest mean error: 9.53024959564209 mm for frame 108

Lowest mean error: 3.2130956649780273 mm for frame 16

Saving results

Total time: 102.13150978088379
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817427
Iteration 2/25 | Loss: 0.00092086
Iteration 3/25 | Loss: 0.00062217
Iteration 4/25 | Loss: 0.00058773
Iteration 5/25 | Loss: 0.00058023
Iteration 6/25 | Loss: 0.00057802
Iteration 7/25 | Loss: 0.00057786
Iteration 8/25 | Loss: 0.00057786
Iteration 9/25 | Loss: 0.00057786
Iteration 10/25 | Loss: 0.00057786
Iteration 11/25 | Loss: 0.00057786
Iteration 12/25 | Loss: 0.00057786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005778589402325451, 0.0005778589402325451, 0.0005778589402325451, 0.0005778589402325451, 0.0005778589402325451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005778589402325451

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.46678567
Iteration 2/25 | Loss: 0.00017859
Iteration 3/25 | Loss: 0.00017856
Iteration 4/25 | Loss: 0.00017856
Iteration 5/25 | Loss: 0.00017856
Iteration 6/25 | Loss: 0.00017856
Iteration 7/25 | Loss: 0.00017856
Iteration 8/25 | Loss: 0.00017856
Iteration 9/25 | Loss: 0.00017856
Iteration 10/25 | Loss: 0.00017856
Iteration 11/25 | Loss: 0.00017856
Iteration 12/25 | Loss: 0.00017856
Iteration 13/25 | Loss: 0.00017856
Iteration 14/25 | Loss: 0.00017856
Iteration 15/25 | Loss: 0.00017856
Iteration 16/25 | Loss: 0.00017856
Iteration 17/25 | Loss: 0.00017856
Iteration 18/25 | Loss: 0.00017856
Iteration 19/25 | Loss: 0.00017856
Iteration 20/25 | Loss: 0.00017856
Iteration 21/25 | Loss: 0.00017856
Iteration 22/25 | Loss: 0.00017856
Iteration 23/25 | Loss: 0.00017856
Iteration 24/25 | Loss: 0.00017856
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00017856126942206174, 0.00017856126942206174, 0.00017856126942206174, 0.00017856126942206174, 0.00017856126942206174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00017856126942206174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017856
Iteration 2/1000 | Loss: 0.00002330
Iteration 3/1000 | Loss: 0.00001676
Iteration 4/1000 | Loss: 0.00001470
Iteration 5/1000 | Loss: 0.00001376
Iteration 6/1000 | Loss: 0.00001315
Iteration 7/1000 | Loss: 0.00001266
Iteration 8/1000 | Loss: 0.00001233
Iteration 9/1000 | Loss: 0.00001211
Iteration 10/1000 | Loss: 0.00001190
Iteration 11/1000 | Loss: 0.00001179
Iteration 12/1000 | Loss: 0.00001171
Iteration 13/1000 | Loss: 0.00001166
Iteration 14/1000 | Loss: 0.00001165
Iteration 15/1000 | Loss: 0.00001165
Iteration 16/1000 | Loss: 0.00001165
Iteration 17/1000 | Loss: 0.00001164
Iteration 18/1000 | Loss: 0.00001164
Iteration 19/1000 | Loss: 0.00001163
Iteration 20/1000 | Loss: 0.00001161
Iteration 21/1000 | Loss: 0.00001161
Iteration 22/1000 | Loss: 0.00001161
Iteration 23/1000 | Loss: 0.00001161
Iteration 24/1000 | Loss: 0.00001161
Iteration 25/1000 | Loss: 0.00001161
Iteration 26/1000 | Loss: 0.00001160
Iteration 27/1000 | Loss: 0.00001160
Iteration 28/1000 | Loss: 0.00001160
Iteration 29/1000 | Loss: 0.00001160
Iteration 30/1000 | Loss: 0.00001160
Iteration 31/1000 | Loss: 0.00001160
Iteration 32/1000 | Loss: 0.00001160
Iteration 33/1000 | Loss: 0.00001160
Iteration 34/1000 | Loss: 0.00001160
Iteration 35/1000 | Loss: 0.00001159
Iteration 36/1000 | Loss: 0.00001158
Iteration 37/1000 | Loss: 0.00001158
Iteration 38/1000 | Loss: 0.00001158
Iteration 39/1000 | Loss: 0.00001157
Iteration 40/1000 | Loss: 0.00001157
Iteration 41/1000 | Loss: 0.00001157
Iteration 42/1000 | Loss: 0.00001157
Iteration 43/1000 | Loss: 0.00001157
Iteration 44/1000 | Loss: 0.00001157
Iteration 45/1000 | Loss: 0.00001157
Iteration 46/1000 | Loss: 0.00001156
Iteration 47/1000 | Loss: 0.00001156
Iteration 48/1000 | Loss: 0.00001156
Iteration 49/1000 | Loss: 0.00001156
Iteration 50/1000 | Loss: 0.00001156
Iteration 51/1000 | Loss: 0.00001156
Iteration 52/1000 | Loss: 0.00001156
Iteration 53/1000 | Loss: 0.00001155
Iteration 54/1000 | Loss: 0.00001155
Iteration 55/1000 | Loss: 0.00001155
Iteration 56/1000 | Loss: 0.00001154
Iteration 57/1000 | Loss: 0.00001154
Iteration 58/1000 | Loss: 0.00001154
Iteration 59/1000 | Loss: 0.00001154
Iteration 60/1000 | Loss: 0.00001154
Iteration 61/1000 | Loss: 0.00001154
Iteration 62/1000 | Loss: 0.00001154
Iteration 63/1000 | Loss: 0.00001154
Iteration 64/1000 | Loss: 0.00001154
Iteration 65/1000 | Loss: 0.00001154
Iteration 66/1000 | Loss: 0.00001153
Iteration 67/1000 | Loss: 0.00001153
Iteration 68/1000 | Loss: 0.00001153
Iteration 69/1000 | Loss: 0.00001153
Iteration 70/1000 | Loss: 0.00001152
Iteration 71/1000 | Loss: 0.00001152
Iteration 72/1000 | Loss: 0.00001152
Iteration 73/1000 | Loss: 0.00001152
Iteration 74/1000 | Loss: 0.00001151
Iteration 75/1000 | Loss: 0.00001151
Iteration 76/1000 | Loss: 0.00001151
Iteration 77/1000 | Loss: 0.00001151
Iteration 78/1000 | Loss: 0.00001151
Iteration 79/1000 | Loss: 0.00001151
Iteration 80/1000 | Loss: 0.00001151
Iteration 81/1000 | Loss: 0.00001151
Iteration 82/1000 | Loss: 0.00001151
Iteration 83/1000 | Loss: 0.00001151
Iteration 84/1000 | Loss: 0.00001151
Iteration 85/1000 | Loss: 0.00001151
Iteration 86/1000 | Loss: 0.00001151
Iteration 87/1000 | Loss: 0.00001150
Iteration 88/1000 | Loss: 0.00001150
Iteration 89/1000 | Loss: 0.00001150
Iteration 90/1000 | Loss: 0.00001149
Iteration 91/1000 | Loss: 0.00001149
Iteration 92/1000 | Loss: 0.00001149
Iteration 93/1000 | Loss: 0.00001149
Iteration 94/1000 | Loss: 0.00001149
Iteration 95/1000 | Loss: 0.00001149
Iteration 96/1000 | Loss: 0.00001149
Iteration 97/1000 | Loss: 0.00001149
Iteration 98/1000 | Loss: 0.00001149
Iteration 99/1000 | Loss: 0.00001149
Iteration 100/1000 | Loss: 0.00001149
Iteration 101/1000 | Loss: 0.00001149
Iteration 102/1000 | Loss: 0.00001149
Iteration 103/1000 | Loss: 0.00001149
Iteration 104/1000 | Loss: 0.00001148
Iteration 105/1000 | Loss: 0.00001148
Iteration 106/1000 | Loss: 0.00001148
Iteration 107/1000 | Loss: 0.00001148
Iteration 108/1000 | Loss: 0.00001148
Iteration 109/1000 | Loss: 0.00001147
Iteration 110/1000 | Loss: 0.00001147
Iteration 111/1000 | Loss: 0.00001147
Iteration 112/1000 | Loss: 0.00001147
Iteration 113/1000 | Loss: 0.00001147
Iteration 114/1000 | Loss: 0.00001147
Iteration 115/1000 | Loss: 0.00001147
Iteration 116/1000 | Loss: 0.00001147
Iteration 117/1000 | Loss: 0.00001147
Iteration 118/1000 | Loss: 0.00001147
Iteration 119/1000 | Loss: 0.00001147
Iteration 120/1000 | Loss: 0.00001146
Iteration 121/1000 | Loss: 0.00001146
Iteration 122/1000 | Loss: 0.00001146
Iteration 123/1000 | Loss: 0.00001146
Iteration 124/1000 | Loss: 0.00001146
Iteration 125/1000 | Loss: 0.00001146
Iteration 126/1000 | Loss: 0.00001146
Iteration 127/1000 | Loss: 0.00001146
Iteration 128/1000 | Loss: 0.00001146
Iteration 129/1000 | Loss: 0.00001146
Iteration 130/1000 | Loss: 0.00001146
Iteration 131/1000 | Loss: 0.00001146
Iteration 132/1000 | Loss: 0.00001146
Iteration 133/1000 | Loss: 0.00001146
Iteration 134/1000 | Loss: 0.00001146
Iteration 135/1000 | Loss: 0.00001146
Iteration 136/1000 | Loss: 0.00001146
Iteration 137/1000 | Loss: 0.00001146
Iteration 138/1000 | Loss: 0.00001146
Iteration 139/1000 | Loss: 0.00001146
Iteration 140/1000 | Loss: 0.00001146
Iteration 141/1000 | Loss: 0.00001146
Iteration 142/1000 | Loss: 0.00001146
Iteration 143/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.1464173439890146e-05, 1.1464173439890146e-05, 1.1464173439890146e-05, 1.1464173439890146e-05, 1.1464173439890146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1464173439890146e-05

Optimization complete. Final v2v error: 2.8464109897613525 mm

Highest mean error: 3.554687023162842 mm for frame 82

Lowest mean error: 2.439512014389038 mm for frame 110

Saving results

Total time: 38.83146381378174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058394
Iteration 2/25 | Loss: 0.00214920
Iteration 3/25 | Loss: 0.00123147
Iteration 4/25 | Loss: 0.00102702
Iteration 5/25 | Loss: 0.00107568
Iteration 6/25 | Loss: 0.00095570
Iteration 7/25 | Loss: 0.00094348
Iteration 8/25 | Loss: 0.00088370
Iteration 9/25 | Loss: 0.00085606
Iteration 10/25 | Loss: 0.00092331
Iteration 11/25 | Loss: 0.00089043
Iteration 12/25 | Loss: 0.00081376
Iteration 13/25 | Loss: 0.00074193
Iteration 14/25 | Loss: 0.00072392
Iteration 15/25 | Loss: 0.00073616
Iteration 16/25 | Loss: 0.00076527
Iteration 17/25 | Loss: 0.00069833
Iteration 18/25 | Loss: 0.00070429
Iteration 19/25 | Loss: 0.00073356
Iteration 20/25 | Loss: 0.00068314
Iteration 21/25 | Loss: 0.00066750
Iteration 22/25 | Loss: 0.00065734
Iteration 23/25 | Loss: 0.00065013
Iteration 24/25 | Loss: 0.00066865
Iteration 25/25 | Loss: 0.00065793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63824141
Iteration 2/25 | Loss: 0.00107666
Iteration 3/25 | Loss: 0.00107666
Iteration 4/25 | Loss: 0.00107665
Iteration 5/25 | Loss: 0.00107665
Iteration 6/25 | Loss: 0.00107665
Iteration 7/25 | Loss: 0.00107665
Iteration 8/25 | Loss: 0.00107665
Iteration 9/25 | Loss: 0.00107665
Iteration 10/25 | Loss: 0.00108106
Iteration 11/25 | Loss: 0.00108106
Iteration 12/25 | Loss: 0.00108106
Iteration 13/25 | Loss: 0.00108106
Iteration 14/25 | Loss: 0.00108106
Iteration 15/25 | Loss: 0.00108106
Iteration 16/25 | Loss: 0.00108106
Iteration 17/25 | Loss: 0.00108106
Iteration 18/25 | Loss: 0.00108106
Iteration 19/25 | Loss: 0.00108106
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010810630628839135, 0.0010810630628839135, 0.0010810630628839135, 0.0010810630628839135, 0.0010810630628839135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010810630628839135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108106
Iteration 2/1000 | Loss: 0.00157484
Iteration 3/1000 | Loss: 0.00066178
Iteration 4/1000 | Loss: 0.00047330
Iteration 5/1000 | Loss: 0.00202753
Iteration 6/1000 | Loss: 0.00015682
Iteration 7/1000 | Loss: 0.00014609
Iteration 8/1000 | Loss: 0.00027648
Iteration 9/1000 | Loss: 0.00005801
Iteration 10/1000 | Loss: 0.00260875
Iteration 11/1000 | Loss: 0.00201942
Iteration 12/1000 | Loss: 0.00020018
Iteration 13/1000 | Loss: 0.00018413
Iteration 14/1000 | Loss: 0.00308123
Iteration 15/1000 | Loss: 0.00037242
Iteration 16/1000 | Loss: 0.00048951
Iteration 17/1000 | Loss: 0.00049352
Iteration 18/1000 | Loss: 0.00008960
Iteration 19/1000 | Loss: 0.00020949
Iteration 20/1000 | Loss: 0.00023657
Iteration 21/1000 | Loss: 0.00022141
Iteration 22/1000 | Loss: 0.00009517
Iteration 23/1000 | Loss: 0.00042805
Iteration 24/1000 | Loss: 0.00036165
Iteration 25/1000 | Loss: 0.00054345
Iteration 26/1000 | Loss: 0.00021751
Iteration 27/1000 | Loss: 0.00025427
Iteration 28/1000 | Loss: 0.00014936
Iteration 29/1000 | Loss: 0.00021404
Iteration 30/1000 | Loss: 0.00014754
Iteration 31/1000 | Loss: 0.00021223
Iteration 32/1000 | Loss: 0.00031803
Iteration 33/1000 | Loss: 0.00017314
Iteration 34/1000 | Loss: 0.00020770
Iteration 35/1000 | Loss: 0.00015551
Iteration 36/1000 | Loss: 0.00021659
Iteration 37/1000 | Loss: 0.00018566
Iteration 38/1000 | Loss: 0.00021654
Iteration 39/1000 | Loss: 0.00015332
Iteration 40/1000 | Loss: 0.00022569
Iteration 41/1000 | Loss: 0.00022883
Iteration 42/1000 | Loss: 0.00022330
Iteration 43/1000 | Loss: 0.00020226
Iteration 44/1000 | Loss: 0.00003396
Iteration 45/1000 | Loss: 0.00016241
Iteration 46/1000 | Loss: 0.00014916
Iteration 47/1000 | Loss: 0.00019078
Iteration 48/1000 | Loss: 0.00023077
Iteration 49/1000 | Loss: 0.00008297
Iteration 50/1000 | Loss: 0.00022020
Iteration 51/1000 | Loss: 0.00015989
Iteration 52/1000 | Loss: 0.00015682
Iteration 53/1000 | Loss: 0.00009245
Iteration 54/1000 | Loss: 0.00020322
Iteration 55/1000 | Loss: 0.00022913
Iteration 56/1000 | Loss: 0.00031797
Iteration 57/1000 | Loss: 0.00022274
Iteration 58/1000 | Loss: 0.00012845
Iteration 59/1000 | Loss: 0.00003420
Iteration 60/1000 | Loss: 0.00015933
Iteration 61/1000 | Loss: 0.00014299
Iteration 62/1000 | Loss: 0.00017516
Iteration 63/1000 | Loss: 0.00015192
Iteration 64/1000 | Loss: 0.00021301
Iteration 65/1000 | Loss: 0.00022355
Iteration 66/1000 | Loss: 0.00015295
Iteration 67/1000 | Loss: 0.00008084
Iteration 68/1000 | Loss: 0.00015336
Iteration 69/1000 | Loss: 0.00009643
Iteration 70/1000 | Loss: 0.00016189
Iteration 71/1000 | Loss: 0.00038592
Iteration 72/1000 | Loss: 0.00017564
Iteration 73/1000 | Loss: 0.00029169
Iteration 74/1000 | Loss: 0.00022657
Iteration 75/1000 | Loss: 0.00033243
Iteration 76/1000 | Loss: 0.00035916
Iteration 77/1000 | Loss: 0.00022753
Iteration 78/1000 | Loss: 0.00076640
Iteration 79/1000 | Loss: 0.00004269
Iteration 80/1000 | Loss: 0.00002618
Iteration 81/1000 | Loss: 0.00002201
Iteration 82/1000 | Loss: 0.00001956
Iteration 83/1000 | Loss: 0.00001740
Iteration 84/1000 | Loss: 0.00001642
Iteration 85/1000 | Loss: 0.00001784
Iteration 86/1000 | Loss: 0.00001533
Iteration 87/1000 | Loss: 0.00002102
Iteration 88/1000 | Loss: 0.00001332
Iteration 89/1000 | Loss: 0.00001840
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001586
Iteration 92/1000 | Loss: 0.00028724
Iteration 93/1000 | Loss: 0.00069068
Iteration 94/1000 | Loss: 0.00011391
Iteration 95/1000 | Loss: 0.00002630
Iteration 96/1000 | Loss: 0.00002194
Iteration 97/1000 | Loss: 0.00001682
Iteration 98/1000 | Loss: 0.00001527
Iteration 99/1000 | Loss: 0.00001557
Iteration 100/1000 | Loss: 0.00001525
Iteration 101/1000 | Loss: 0.00022157
Iteration 102/1000 | Loss: 0.00016050
Iteration 103/1000 | Loss: 0.00021026
Iteration 104/1000 | Loss: 0.00002088
Iteration 105/1000 | Loss: 0.00027471
Iteration 106/1000 | Loss: 0.00002644
Iteration 107/1000 | Loss: 0.00001794
Iteration 108/1000 | Loss: 0.00001664
Iteration 109/1000 | Loss: 0.00004122
Iteration 110/1000 | Loss: 0.00001593
Iteration 111/1000 | Loss: 0.00001534
Iteration 112/1000 | Loss: 0.00001522
Iteration 113/1000 | Loss: 0.00001269
Iteration 114/1000 | Loss: 0.00001262
Iteration 115/1000 | Loss: 0.00001217
Iteration 116/1000 | Loss: 0.00001186
Iteration 117/1000 | Loss: 0.00001389
Iteration 118/1000 | Loss: 0.00001150
Iteration 119/1000 | Loss: 0.00001273
Iteration 120/1000 | Loss: 0.00001138
Iteration 121/1000 | Loss: 0.00001137
Iteration 122/1000 | Loss: 0.00001137
Iteration 123/1000 | Loss: 0.00001137
Iteration 124/1000 | Loss: 0.00001137
Iteration 125/1000 | Loss: 0.00001137
Iteration 126/1000 | Loss: 0.00001137
Iteration 127/1000 | Loss: 0.00001136
Iteration 128/1000 | Loss: 0.00001136
Iteration 129/1000 | Loss: 0.00001136
Iteration 130/1000 | Loss: 0.00001136
Iteration 131/1000 | Loss: 0.00001136
Iteration 132/1000 | Loss: 0.00001136
Iteration 133/1000 | Loss: 0.00001136
Iteration 134/1000 | Loss: 0.00001135
Iteration 135/1000 | Loss: 0.00001135
Iteration 136/1000 | Loss: 0.00001135
Iteration 137/1000 | Loss: 0.00001135
Iteration 138/1000 | Loss: 0.00001135
Iteration 139/1000 | Loss: 0.00001135
Iteration 140/1000 | Loss: 0.00001135
Iteration 141/1000 | Loss: 0.00001135
Iteration 142/1000 | Loss: 0.00001134
Iteration 143/1000 | Loss: 0.00001134
Iteration 144/1000 | Loss: 0.00001272
Iteration 145/1000 | Loss: 0.00001166
Iteration 146/1000 | Loss: 0.00001130
Iteration 147/1000 | Loss: 0.00001129
Iteration 148/1000 | Loss: 0.00001129
Iteration 149/1000 | Loss: 0.00001136
Iteration 150/1000 | Loss: 0.00001128
Iteration 151/1000 | Loss: 0.00001128
Iteration 152/1000 | Loss: 0.00001128
Iteration 153/1000 | Loss: 0.00001128
Iteration 154/1000 | Loss: 0.00001128
Iteration 155/1000 | Loss: 0.00001128
Iteration 156/1000 | Loss: 0.00001128
Iteration 157/1000 | Loss: 0.00001128
Iteration 158/1000 | Loss: 0.00001128
Iteration 159/1000 | Loss: 0.00001128
Iteration 160/1000 | Loss: 0.00001128
Iteration 161/1000 | Loss: 0.00001128
Iteration 162/1000 | Loss: 0.00001128
Iteration 163/1000 | Loss: 0.00001128
Iteration 164/1000 | Loss: 0.00001128
Iteration 165/1000 | Loss: 0.00001128
Iteration 166/1000 | Loss: 0.00001128
Iteration 167/1000 | Loss: 0.00001127
Iteration 168/1000 | Loss: 0.00001127
Iteration 169/1000 | Loss: 0.00001127
Iteration 170/1000 | Loss: 0.00001127
Iteration 171/1000 | Loss: 0.00001126
Iteration 172/1000 | Loss: 0.00001126
Iteration 173/1000 | Loss: 0.00001126
Iteration 174/1000 | Loss: 0.00001125
Iteration 175/1000 | Loss: 0.00001125
Iteration 176/1000 | Loss: 0.00001149
Iteration 177/1000 | Loss: 0.00001123
Iteration 178/1000 | Loss: 0.00001123
Iteration 179/1000 | Loss: 0.00001123
Iteration 180/1000 | Loss: 0.00001123
Iteration 181/1000 | Loss: 0.00001123
Iteration 182/1000 | Loss: 0.00001123
Iteration 183/1000 | Loss: 0.00001123
Iteration 184/1000 | Loss: 0.00001122
Iteration 185/1000 | Loss: 0.00001122
Iteration 186/1000 | Loss: 0.00001122
Iteration 187/1000 | Loss: 0.00001122
Iteration 188/1000 | Loss: 0.00001122
Iteration 189/1000 | Loss: 0.00001122
Iteration 190/1000 | Loss: 0.00001122
Iteration 191/1000 | Loss: 0.00001122
Iteration 192/1000 | Loss: 0.00001122
Iteration 193/1000 | Loss: 0.00001122
Iteration 194/1000 | Loss: 0.00001122
Iteration 195/1000 | Loss: 0.00001122
Iteration 196/1000 | Loss: 0.00001122
Iteration 197/1000 | Loss: 0.00001122
Iteration 198/1000 | Loss: 0.00001122
Iteration 199/1000 | Loss: 0.00001122
Iteration 200/1000 | Loss: 0.00001122
Iteration 201/1000 | Loss: 0.00001121
Iteration 202/1000 | Loss: 0.00001121
Iteration 203/1000 | Loss: 0.00001121
Iteration 204/1000 | Loss: 0.00001121
Iteration 205/1000 | Loss: 0.00001121
Iteration 206/1000 | Loss: 0.00001121
Iteration 207/1000 | Loss: 0.00001121
Iteration 208/1000 | Loss: 0.00001121
Iteration 209/1000 | Loss: 0.00001121
Iteration 210/1000 | Loss: 0.00001120
Iteration 211/1000 | Loss: 0.00001120
Iteration 212/1000 | Loss: 0.00001120
Iteration 213/1000 | Loss: 0.00001120
Iteration 214/1000 | Loss: 0.00001120
Iteration 215/1000 | Loss: 0.00001120
Iteration 216/1000 | Loss: 0.00001119
Iteration 217/1000 | Loss: 0.00001119
Iteration 218/1000 | Loss: 0.00001119
Iteration 219/1000 | Loss: 0.00001118
Iteration 220/1000 | Loss: 0.00001118
Iteration 221/1000 | Loss: 0.00001118
Iteration 222/1000 | Loss: 0.00001117
Iteration 223/1000 | Loss: 0.00001117
Iteration 224/1000 | Loss: 0.00001117
Iteration 225/1000 | Loss: 0.00001116
Iteration 226/1000 | Loss: 0.00001116
Iteration 227/1000 | Loss: 0.00001116
Iteration 228/1000 | Loss: 0.00001116
Iteration 229/1000 | Loss: 0.00001116
Iteration 230/1000 | Loss: 0.00001116
Iteration 231/1000 | Loss: 0.00001116
Iteration 232/1000 | Loss: 0.00001116
Iteration 233/1000 | Loss: 0.00001116
Iteration 234/1000 | Loss: 0.00001116
Iteration 235/1000 | Loss: 0.00001116
Iteration 236/1000 | Loss: 0.00001115
Iteration 237/1000 | Loss: 0.00001115
Iteration 238/1000 | Loss: 0.00001115
Iteration 239/1000 | Loss: 0.00001115
Iteration 240/1000 | Loss: 0.00001114
Iteration 241/1000 | Loss: 0.00001114
Iteration 242/1000 | Loss: 0.00001113
Iteration 243/1000 | Loss: 0.00001113
Iteration 244/1000 | Loss: 0.00001113
Iteration 245/1000 | Loss: 0.00001112
Iteration 246/1000 | Loss: 0.00001112
Iteration 247/1000 | Loss: 0.00001112
Iteration 248/1000 | Loss: 0.00001112
Iteration 249/1000 | Loss: 0.00001112
Iteration 250/1000 | Loss: 0.00001112
Iteration 251/1000 | Loss: 0.00001112
Iteration 252/1000 | Loss: 0.00001111
Iteration 253/1000 | Loss: 0.00001111
Iteration 254/1000 | Loss: 0.00001111
Iteration 255/1000 | Loss: 0.00001111
Iteration 256/1000 | Loss: 0.00001110
Iteration 257/1000 | Loss: 0.00001110
Iteration 258/1000 | Loss: 0.00001110
Iteration 259/1000 | Loss: 0.00001110
Iteration 260/1000 | Loss: 0.00001110
Iteration 261/1000 | Loss: 0.00001110
Iteration 262/1000 | Loss: 0.00001110
Iteration 263/1000 | Loss: 0.00001110
Iteration 264/1000 | Loss: 0.00001110
Iteration 265/1000 | Loss: 0.00001109
Iteration 266/1000 | Loss: 0.00001109
Iteration 267/1000 | Loss: 0.00001109
Iteration 268/1000 | Loss: 0.00001109
Iteration 269/1000 | Loss: 0.00001109
Iteration 270/1000 | Loss: 0.00001109
Iteration 271/1000 | Loss: 0.00001109
Iteration 272/1000 | Loss: 0.00001109
Iteration 273/1000 | Loss: 0.00001109
Iteration 274/1000 | Loss: 0.00001109
Iteration 275/1000 | Loss: 0.00001109
Iteration 276/1000 | Loss: 0.00001109
Iteration 277/1000 | Loss: 0.00001109
Iteration 278/1000 | Loss: 0.00001109
Iteration 279/1000 | Loss: 0.00001108
Iteration 280/1000 | Loss: 0.00001108
Iteration 281/1000 | Loss: 0.00001108
Iteration 282/1000 | Loss: 0.00001107
Iteration 283/1000 | Loss: 0.00001107
Iteration 284/1000 | Loss: 0.00001107
Iteration 285/1000 | Loss: 0.00001107
Iteration 286/1000 | Loss: 0.00001107
Iteration 287/1000 | Loss: 0.00001107
Iteration 288/1000 | Loss: 0.00001107
Iteration 289/1000 | Loss: 0.00001107
Iteration 290/1000 | Loss: 0.00001107
Iteration 291/1000 | Loss: 0.00001107
Iteration 292/1000 | Loss: 0.00001107
Iteration 293/1000 | Loss: 0.00001107
Iteration 294/1000 | Loss: 0.00001107
Iteration 295/1000 | Loss: 0.00001107
Iteration 296/1000 | Loss: 0.00001107
Iteration 297/1000 | Loss: 0.00001107
Iteration 298/1000 | Loss: 0.00001107
Iteration 299/1000 | Loss: 0.00001106
Iteration 300/1000 | Loss: 0.00001106
Iteration 301/1000 | Loss: 0.00001106
Iteration 302/1000 | Loss: 0.00001106
Iteration 303/1000 | Loss: 0.00001106
Iteration 304/1000 | Loss: 0.00001106
Iteration 305/1000 | Loss: 0.00001106
Iteration 306/1000 | Loss: 0.00001106
Iteration 307/1000 | Loss: 0.00001106
Iteration 308/1000 | Loss: 0.00001106
Iteration 309/1000 | Loss: 0.00001106
Iteration 310/1000 | Loss: 0.00001106
Iteration 311/1000 | Loss: 0.00001106
Iteration 312/1000 | Loss: 0.00001105
Iteration 313/1000 | Loss: 0.00001105
Iteration 314/1000 | Loss: 0.00001105
Iteration 315/1000 | Loss: 0.00001105
Iteration 316/1000 | Loss: 0.00001105
Iteration 317/1000 | Loss: 0.00001105
Iteration 318/1000 | Loss: 0.00001105
Iteration 319/1000 | Loss: 0.00001105
Iteration 320/1000 | Loss: 0.00001105
Iteration 321/1000 | Loss: 0.00001105
Iteration 322/1000 | Loss: 0.00001105
Iteration 323/1000 | Loss: 0.00001105
Iteration 324/1000 | Loss: 0.00001105
Iteration 325/1000 | Loss: 0.00001105
Iteration 326/1000 | Loss: 0.00001105
Iteration 327/1000 | Loss: 0.00001105
Iteration 328/1000 | Loss: 0.00001105
Iteration 329/1000 | Loss: 0.00001105
Iteration 330/1000 | Loss: 0.00001105
Iteration 331/1000 | Loss: 0.00001105
Iteration 332/1000 | Loss: 0.00001105
Iteration 333/1000 | Loss: 0.00001105
Iteration 334/1000 | Loss: 0.00001105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 334. Stopping optimization.
Last 5 losses: [1.1053628441004548e-05, 1.1053628441004548e-05, 1.1053628441004548e-05, 1.1053628441004548e-05, 1.1053628441004548e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1053628441004548e-05

Optimization complete. Final v2v error: 2.752042293548584 mm

Highest mean error: 5.353410243988037 mm for frame 86

Lowest mean error: 2.2016913890838623 mm for frame 40

Saving results

Total time: 223.5842423439026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454396
Iteration 2/25 | Loss: 0.00078376
Iteration 3/25 | Loss: 0.00065746
Iteration 4/25 | Loss: 0.00063412
Iteration 5/25 | Loss: 0.00062455
Iteration 6/25 | Loss: 0.00062228
Iteration 7/25 | Loss: 0.00062213
Iteration 8/25 | Loss: 0.00062213
Iteration 9/25 | Loss: 0.00062213
Iteration 10/25 | Loss: 0.00062213
Iteration 11/25 | Loss: 0.00062213
Iteration 12/25 | Loss: 0.00062213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006221331423148513, 0.0006221331423148513, 0.0006221331423148513, 0.0006221331423148513, 0.0006221331423148513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006221331423148513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41609204
Iteration 2/25 | Loss: 0.00027436
Iteration 3/25 | Loss: 0.00027434
Iteration 4/25 | Loss: 0.00027434
Iteration 5/25 | Loss: 0.00027434
Iteration 6/25 | Loss: 0.00027434
Iteration 7/25 | Loss: 0.00027434
Iteration 8/25 | Loss: 0.00027434
Iteration 9/25 | Loss: 0.00027434
Iteration 10/25 | Loss: 0.00027434
Iteration 11/25 | Loss: 0.00027434
Iteration 12/25 | Loss: 0.00027434
Iteration 13/25 | Loss: 0.00027434
Iteration 14/25 | Loss: 0.00027434
Iteration 15/25 | Loss: 0.00027434
Iteration 16/25 | Loss: 0.00027434
Iteration 17/25 | Loss: 0.00027434
Iteration 18/25 | Loss: 0.00027434
Iteration 19/25 | Loss: 0.00027434
Iteration 20/25 | Loss: 0.00027434
Iteration 21/25 | Loss: 0.00027434
Iteration 22/25 | Loss: 0.00027434
Iteration 23/25 | Loss: 0.00027434
Iteration 24/25 | Loss: 0.00027434
Iteration 25/25 | Loss: 0.00027434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027434
Iteration 2/1000 | Loss: 0.00002387
Iteration 3/1000 | Loss: 0.00001894
Iteration 4/1000 | Loss: 0.00001750
Iteration 5/1000 | Loss: 0.00001669
Iteration 6/1000 | Loss: 0.00001599
Iteration 7/1000 | Loss: 0.00001577
Iteration 8/1000 | Loss: 0.00001551
Iteration 9/1000 | Loss: 0.00001547
Iteration 10/1000 | Loss: 0.00001546
Iteration 11/1000 | Loss: 0.00001533
Iteration 12/1000 | Loss: 0.00001525
Iteration 13/1000 | Loss: 0.00001523
Iteration 14/1000 | Loss: 0.00001523
Iteration 15/1000 | Loss: 0.00001520
Iteration 16/1000 | Loss: 0.00001519
Iteration 17/1000 | Loss: 0.00001515
Iteration 18/1000 | Loss: 0.00001515
Iteration 19/1000 | Loss: 0.00001515
Iteration 20/1000 | Loss: 0.00001515
Iteration 21/1000 | Loss: 0.00001515
Iteration 22/1000 | Loss: 0.00001514
Iteration 23/1000 | Loss: 0.00001512
Iteration 24/1000 | Loss: 0.00001512
Iteration 25/1000 | Loss: 0.00001511
Iteration 26/1000 | Loss: 0.00001511
Iteration 27/1000 | Loss: 0.00001510
Iteration 28/1000 | Loss: 0.00001510
Iteration 29/1000 | Loss: 0.00001510
Iteration 30/1000 | Loss: 0.00001509
Iteration 31/1000 | Loss: 0.00001509
Iteration 32/1000 | Loss: 0.00001509
Iteration 33/1000 | Loss: 0.00001508
Iteration 34/1000 | Loss: 0.00001508
Iteration 35/1000 | Loss: 0.00001508
Iteration 36/1000 | Loss: 0.00001508
Iteration 37/1000 | Loss: 0.00001508
Iteration 38/1000 | Loss: 0.00001507
Iteration 39/1000 | Loss: 0.00001507
Iteration 40/1000 | Loss: 0.00001507
Iteration 41/1000 | Loss: 0.00001507
Iteration 42/1000 | Loss: 0.00001507
Iteration 43/1000 | Loss: 0.00001507
Iteration 44/1000 | Loss: 0.00001507
Iteration 45/1000 | Loss: 0.00001506
Iteration 46/1000 | Loss: 0.00001506
Iteration 47/1000 | Loss: 0.00001506
Iteration 48/1000 | Loss: 0.00001506
Iteration 49/1000 | Loss: 0.00001506
Iteration 50/1000 | Loss: 0.00001506
Iteration 51/1000 | Loss: 0.00001506
Iteration 52/1000 | Loss: 0.00001506
Iteration 53/1000 | Loss: 0.00001505
Iteration 54/1000 | Loss: 0.00001505
Iteration 55/1000 | Loss: 0.00001505
Iteration 56/1000 | Loss: 0.00001505
Iteration 57/1000 | Loss: 0.00001505
Iteration 58/1000 | Loss: 0.00001505
Iteration 59/1000 | Loss: 0.00001505
Iteration 60/1000 | Loss: 0.00001505
Iteration 61/1000 | Loss: 0.00001505
Iteration 62/1000 | Loss: 0.00001505
Iteration 63/1000 | Loss: 0.00001505
Iteration 64/1000 | Loss: 0.00001505
Iteration 65/1000 | Loss: 0.00001505
Iteration 66/1000 | Loss: 0.00001505
Iteration 67/1000 | Loss: 0.00001504
Iteration 68/1000 | Loss: 0.00001504
Iteration 69/1000 | Loss: 0.00001504
Iteration 70/1000 | Loss: 0.00001504
Iteration 71/1000 | Loss: 0.00001504
Iteration 72/1000 | Loss: 0.00001504
Iteration 73/1000 | Loss: 0.00001504
Iteration 74/1000 | Loss: 0.00001504
Iteration 75/1000 | Loss: 0.00001504
Iteration 76/1000 | Loss: 0.00001504
Iteration 77/1000 | Loss: 0.00001504
Iteration 78/1000 | Loss: 0.00001504
Iteration 79/1000 | Loss: 0.00001504
Iteration 80/1000 | Loss: 0.00001504
Iteration 81/1000 | Loss: 0.00001504
Iteration 82/1000 | Loss: 0.00001504
Iteration 83/1000 | Loss: 0.00001503
Iteration 84/1000 | Loss: 0.00001503
Iteration 85/1000 | Loss: 0.00001503
Iteration 86/1000 | Loss: 0.00001503
Iteration 87/1000 | Loss: 0.00001503
Iteration 88/1000 | Loss: 0.00001503
Iteration 89/1000 | Loss: 0.00001503
Iteration 90/1000 | Loss: 0.00001503
Iteration 91/1000 | Loss: 0.00001503
Iteration 92/1000 | Loss: 0.00001503
Iteration 93/1000 | Loss: 0.00001503
Iteration 94/1000 | Loss: 0.00001503
Iteration 95/1000 | Loss: 0.00001503
Iteration 96/1000 | Loss: 0.00001502
Iteration 97/1000 | Loss: 0.00001502
Iteration 98/1000 | Loss: 0.00001502
Iteration 99/1000 | Loss: 0.00001502
Iteration 100/1000 | Loss: 0.00001502
Iteration 101/1000 | Loss: 0.00001502
Iteration 102/1000 | Loss: 0.00001502
Iteration 103/1000 | Loss: 0.00001502
Iteration 104/1000 | Loss: 0.00001502
Iteration 105/1000 | Loss: 0.00001502
Iteration 106/1000 | Loss: 0.00001502
Iteration 107/1000 | Loss: 0.00001502
Iteration 108/1000 | Loss: 0.00001502
Iteration 109/1000 | Loss: 0.00001502
Iteration 110/1000 | Loss: 0.00001502
Iteration 111/1000 | Loss: 0.00001502
Iteration 112/1000 | Loss: 0.00001502
Iteration 113/1000 | Loss: 0.00001502
Iteration 114/1000 | Loss: 0.00001502
Iteration 115/1000 | Loss: 0.00001502
Iteration 116/1000 | Loss: 0.00001502
Iteration 117/1000 | Loss: 0.00001502
Iteration 118/1000 | Loss: 0.00001502
Iteration 119/1000 | Loss: 0.00001502
Iteration 120/1000 | Loss: 0.00001502
Iteration 121/1000 | Loss: 0.00001502
Iteration 122/1000 | Loss: 0.00001502
Iteration 123/1000 | Loss: 0.00001502
Iteration 124/1000 | Loss: 0.00001502
Iteration 125/1000 | Loss: 0.00001502
Iteration 126/1000 | Loss: 0.00001502
Iteration 127/1000 | Loss: 0.00001502
Iteration 128/1000 | Loss: 0.00001502
Iteration 129/1000 | Loss: 0.00001502
Iteration 130/1000 | Loss: 0.00001502
Iteration 131/1000 | Loss: 0.00001502
Iteration 132/1000 | Loss: 0.00001502
Iteration 133/1000 | Loss: 0.00001502
Iteration 134/1000 | Loss: 0.00001502
Iteration 135/1000 | Loss: 0.00001502
Iteration 136/1000 | Loss: 0.00001502
Iteration 137/1000 | Loss: 0.00001502
Iteration 138/1000 | Loss: 0.00001502
Iteration 139/1000 | Loss: 0.00001502
Iteration 140/1000 | Loss: 0.00001502
Iteration 141/1000 | Loss: 0.00001502
Iteration 142/1000 | Loss: 0.00001502
Iteration 143/1000 | Loss: 0.00001502
Iteration 144/1000 | Loss: 0.00001502
Iteration 145/1000 | Loss: 0.00001502
Iteration 146/1000 | Loss: 0.00001502
Iteration 147/1000 | Loss: 0.00001502
Iteration 148/1000 | Loss: 0.00001502
Iteration 149/1000 | Loss: 0.00001502
Iteration 150/1000 | Loss: 0.00001502
Iteration 151/1000 | Loss: 0.00001502
Iteration 152/1000 | Loss: 0.00001502
Iteration 153/1000 | Loss: 0.00001502
Iteration 154/1000 | Loss: 0.00001502
Iteration 155/1000 | Loss: 0.00001502
Iteration 156/1000 | Loss: 0.00001502
Iteration 157/1000 | Loss: 0.00001502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.5019429156382103e-05, 1.5019429156382103e-05, 1.5019429156382103e-05, 1.5019429156382103e-05, 1.5019429156382103e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5019429156382103e-05

Optimization complete. Final v2v error: 3.2725212574005127 mm

Highest mean error: 3.8145649433135986 mm for frame 199

Lowest mean error: 2.7286746501922607 mm for frame 235

Saving results

Total time: 35.6832971572876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425401
Iteration 2/25 | Loss: 0.00084537
Iteration 3/25 | Loss: 0.00070810
Iteration 4/25 | Loss: 0.00067820
Iteration 5/25 | Loss: 0.00066699
Iteration 6/25 | Loss: 0.00066503
Iteration 7/25 | Loss: 0.00066459
Iteration 8/25 | Loss: 0.00066459
Iteration 9/25 | Loss: 0.00066459
Iteration 10/25 | Loss: 0.00066459
Iteration 11/25 | Loss: 0.00066459
Iteration 12/25 | Loss: 0.00066459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006645933608524501, 0.0006645933608524501, 0.0006645933608524501, 0.0006645933608524501, 0.0006645933608524501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006645933608524501

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79703498
Iteration 2/25 | Loss: 0.00024086
Iteration 3/25 | Loss: 0.00024086
Iteration 4/25 | Loss: 0.00024086
Iteration 5/25 | Loss: 0.00024086
Iteration 6/25 | Loss: 0.00024086
Iteration 7/25 | Loss: 0.00024086
Iteration 8/25 | Loss: 0.00024086
Iteration 9/25 | Loss: 0.00024086
Iteration 10/25 | Loss: 0.00024086
Iteration 11/25 | Loss: 0.00024086
Iteration 12/25 | Loss: 0.00024085
Iteration 13/25 | Loss: 0.00024085
Iteration 14/25 | Loss: 0.00024085
Iteration 15/25 | Loss: 0.00024085
Iteration 16/25 | Loss: 0.00024085
Iteration 17/25 | Loss: 0.00024085
Iteration 18/25 | Loss: 0.00024085
Iteration 19/25 | Loss: 0.00024085
Iteration 20/25 | Loss: 0.00024085
Iteration 21/25 | Loss: 0.00024085
Iteration 22/25 | Loss: 0.00024085
Iteration 23/25 | Loss: 0.00024085
Iteration 24/25 | Loss: 0.00024085
Iteration 25/25 | Loss: 0.00024085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024085
Iteration 2/1000 | Loss: 0.00004349
Iteration 3/1000 | Loss: 0.00003282
Iteration 4/1000 | Loss: 0.00002973
Iteration 5/1000 | Loss: 0.00002823
Iteration 6/1000 | Loss: 0.00002702
Iteration 7/1000 | Loss: 0.00002624
Iteration 8/1000 | Loss: 0.00002577
Iteration 9/1000 | Loss: 0.00002558
Iteration 10/1000 | Loss: 0.00002539
Iteration 11/1000 | Loss: 0.00002532
Iteration 12/1000 | Loss: 0.00002526
Iteration 13/1000 | Loss: 0.00002523
Iteration 14/1000 | Loss: 0.00002523
Iteration 15/1000 | Loss: 0.00002508
Iteration 16/1000 | Loss: 0.00002508
Iteration 17/1000 | Loss: 0.00002505
Iteration 18/1000 | Loss: 0.00002504
Iteration 19/1000 | Loss: 0.00002502
Iteration 20/1000 | Loss: 0.00002501
Iteration 21/1000 | Loss: 0.00002497
Iteration 22/1000 | Loss: 0.00002497
Iteration 23/1000 | Loss: 0.00002496
Iteration 24/1000 | Loss: 0.00002493
Iteration 25/1000 | Loss: 0.00002493
Iteration 26/1000 | Loss: 0.00002493
Iteration 27/1000 | Loss: 0.00002492
Iteration 28/1000 | Loss: 0.00002492
Iteration 29/1000 | Loss: 0.00002492
Iteration 30/1000 | Loss: 0.00002492
Iteration 31/1000 | Loss: 0.00002492
Iteration 32/1000 | Loss: 0.00002492
Iteration 33/1000 | Loss: 0.00002492
Iteration 34/1000 | Loss: 0.00002490
Iteration 35/1000 | Loss: 0.00002490
Iteration 36/1000 | Loss: 0.00002489
Iteration 37/1000 | Loss: 0.00002488
Iteration 38/1000 | Loss: 0.00002488
Iteration 39/1000 | Loss: 0.00002488
Iteration 40/1000 | Loss: 0.00002488
Iteration 41/1000 | Loss: 0.00002488
Iteration 42/1000 | Loss: 0.00002488
Iteration 43/1000 | Loss: 0.00002488
Iteration 44/1000 | Loss: 0.00002487
Iteration 45/1000 | Loss: 0.00002487
Iteration 46/1000 | Loss: 0.00002487
Iteration 47/1000 | Loss: 0.00002486
Iteration 48/1000 | Loss: 0.00002484
Iteration 49/1000 | Loss: 0.00002484
Iteration 50/1000 | Loss: 0.00002484
Iteration 51/1000 | Loss: 0.00002484
Iteration 52/1000 | Loss: 0.00002484
Iteration 53/1000 | Loss: 0.00002484
Iteration 54/1000 | Loss: 0.00002483
Iteration 55/1000 | Loss: 0.00002483
Iteration 56/1000 | Loss: 0.00002483
Iteration 57/1000 | Loss: 0.00002483
Iteration 58/1000 | Loss: 0.00002482
Iteration 59/1000 | Loss: 0.00002482
Iteration 60/1000 | Loss: 0.00002482
Iteration 61/1000 | Loss: 0.00002482
Iteration 62/1000 | Loss: 0.00002481
Iteration 63/1000 | Loss: 0.00002481
Iteration 64/1000 | Loss: 0.00002481
Iteration 65/1000 | Loss: 0.00002481
Iteration 66/1000 | Loss: 0.00002480
Iteration 67/1000 | Loss: 0.00002480
Iteration 68/1000 | Loss: 0.00002480
Iteration 69/1000 | Loss: 0.00002480
Iteration 70/1000 | Loss: 0.00002480
Iteration 71/1000 | Loss: 0.00002480
Iteration 72/1000 | Loss: 0.00002479
Iteration 73/1000 | Loss: 0.00002479
Iteration 74/1000 | Loss: 0.00002479
Iteration 75/1000 | Loss: 0.00002479
Iteration 76/1000 | Loss: 0.00002479
Iteration 77/1000 | Loss: 0.00002479
Iteration 78/1000 | Loss: 0.00002478
Iteration 79/1000 | Loss: 0.00002478
Iteration 80/1000 | Loss: 0.00002478
Iteration 81/1000 | Loss: 0.00002478
Iteration 82/1000 | Loss: 0.00002478
Iteration 83/1000 | Loss: 0.00002478
Iteration 84/1000 | Loss: 0.00002478
Iteration 85/1000 | Loss: 0.00002478
Iteration 86/1000 | Loss: 0.00002478
Iteration 87/1000 | Loss: 0.00002478
Iteration 88/1000 | Loss: 0.00002477
Iteration 89/1000 | Loss: 0.00002477
Iteration 90/1000 | Loss: 0.00002477
Iteration 91/1000 | Loss: 0.00002477
Iteration 92/1000 | Loss: 0.00002477
Iteration 93/1000 | Loss: 0.00002477
Iteration 94/1000 | Loss: 0.00002477
Iteration 95/1000 | Loss: 0.00002476
Iteration 96/1000 | Loss: 0.00002476
Iteration 97/1000 | Loss: 0.00002476
Iteration 98/1000 | Loss: 0.00002476
Iteration 99/1000 | Loss: 0.00002476
Iteration 100/1000 | Loss: 0.00002475
Iteration 101/1000 | Loss: 0.00002475
Iteration 102/1000 | Loss: 0.00002475
Iteration 103/1000 | Loss: 0.00002475
Iteration 104/1000 | Loss: 0.00002475
Iteration 105/1000 | Loss: 0.00002475
Iteration 106/1000 | Loss: 0.00002475
Iteration 107/1000 | Loss: 0.00002475
Iteration 108/1000 | Loss: 0.00002474
Iteration 109/1000 | Loss: 0.00002474
Iteration 110/1000 | Loss: 0.00002474
Iteration 111/1000 | Loss: 0.00002474
Iteration 112/1000 | Loss: 0.00002474
Iteration 113/1000 | Loss: 0.00002474
Iteration 114/1000 | Loss: 0.00002474
Iteration 115/1000 | Loss: 0.00002474
Iteration 116/1000 | Loss: 0.00002473
Iteration 117/1000 | Loss: 0.00002473
Iteration 118/1000 | Loss: 0.00002473
Iteration 119/1000 | Loss: 0.00002473
Iteration 120/1000 | Loss: 0.00002473
Iteration 121/1000 | Loss: 0.00002472
Iteration 122/1000 | Loss: 0.00002472
Iteration 123/1000 | Loss: 0.00002472
Iteration 124/1000 | Loss: 0.00002472
Iteration 125/1000 | Loss: 0.00002472
Iteration 126/1000 | Loss: 0.00002472
Iteration 127/1000 | Loss: 0.00002472
Iteration 128/1000 | Loss: 0.00002472
Iteration 129/1000 | Loss: 0.00002472
Iteration 130/1000 | Loss: 0.00002472
Iteration 131/1000 | Loss: 0.00002472
Iteration 132/1000 | Loss: 0.00002472
Iteration 133/1000 | Loss: 0.00002472
Iteration 134/1000 | Loss: 0.00002472
Iteration 135/1000 | Loss: 0.00002471
Iteration 136/1000 | Loss: 0.00002471
Iteration 137/1000 | Loss: 0.00002471
Iteration 138/1000 | Loss: 0.00002471
Iteration 139/1000 | Loss: 0.00002471
Iteration 140/1000 | Loss: 0.00002471
Iteration 141/1000 | Loss: 0.00002471
Iteration 142/1000 | Loss: 0.00002471
Iteration 143/1000 | Loss: 0.00002471
Iteration 144/1000 | Loss: 0.00002470
Iteration 145/1000 | Loss: 0.00002470
Iteration 146/1000 | Loss: 0.00002470
Iteration 147/1000 | Loss: 0.00002470
Iteration 148/1000 | Loss: 0.00002470
Iteration 149/1000 | Loss: 0.00002469
Iteration 150/1000 | Loss: 0.00002469
Iteration 151/1000 | Loss: 0.00002469
Iteration 152/1000 | Loss: 0.00002469
Iteration 153/1000 | Loss: 0.00002469
Iteration 154/1000 | Loss: 0.00002469
Iteration 155/1000 | Loss: 0.00002469
Iteration 156/1000 | Loss: 0.00002469
Iteration 157/1000 | Loss: 0.00002469
Iteration 158/1000 | Loss: 0.00002468
Iteration 159/1000 | Loss: 0.00002468
Iteration 160/1000 | Loss: 0.00002468
Iteration 161/1000 | Loss: 0.00002468
Iteration 162/1000 | Loss: 0.00002468
Iteration 163/1000 | Loss: 0.00002468
Iteration 164/1000 | Loss: 0.00002468
Iteration 165/1000 | Loss: 0.00002468
Iteration 166/1000 | Loss: 0.00002468
Iteration 167/1000 | Loss: 0.00002467
Iteration 168/1000 | Loss: 0.00002467
Iteration 169/1000 | Loss: 0.00002467
Iteration 170/1000 | Loss: 0.00002467
Iteration 171/1000 | Loss: 0.00002467
Iteration 172/1000 | Loss: 0.00002467
Iteration 173/1000 | Loss: 0.00002467
Iteration 174/1000 | Loss: 0.00002467
Iteration 175/1000 | Loss: 0.00002467
Iteration 176/1000 | Loss: 0.00002467
Iteration 177/1000 | Loss: 0.00002467
Iteration 178/1000 | Loss: 0.00002467
Iteration 179/1000 | Loss: 0.00002467
Iteration 180/1000 | Loss: 0.00002467
Iteration 181/1000 | Loss: 0.00002467
Iteration 182/1000 | Loss: 0.00002467
Iteration 183/1000 | Loss: 0.00002466
Iteration 184/1000 | Loss: 0.00002466
Iteration 185/1000 | Loss: 0.00002466
Iteration 186/1000 | Loss: 0.00002466
Iteration 187/1000 | Loss: 0.00002466
Iteration 188/1000 | Loss: 0.00002466
Iteration 189/1000 | Loss: 0.00002466
Iteration 190/1000 | Loss: 0.00002466
Iteration 191/1000 | Loss: 0.00002466
Iteration 192/1000 | Loss: 0.00002466
Iteration 193/1000 | Loss: 0.00002466
Iteration 194/1000 | Loss: 0.00002466
Iteration 195/1000 | Loss: 0.00002466
Iteration 196/1000 | Loss: 0.00002466
Iteration 197/1000 | Loss: 0.00002466
Iteration 198/1000 | Loss: 0.00002466
Iteration 199/1000 | Loss: 0.00002465
Iteration 200/1000 | Loss: 0.00002465
Iteration 201/1000 | Loss: 0.00002465
Iteration 202/1000 | Loss: 0.00002465
Iteration 203/1000 | Loss: 0.00002465
Iteration 204/1000 | Loss: 0.00002465
Iteration 205/1000 | Loss: 0.00002465
Iteration 206/1000 | Loss: 0.00002465
Iteration 207/1000 | Loss: 0.00002465
Iteration 208/1000 | Loss: 0.00002465
Iteration 209/1000 | Loss: 0.00002465
Iteration 210/1000 | Loss: 0.00002465
Iteration 211/1000 | Loss: 0.00002465
Iteration 212/1000 | Loss: 0.00002465
Iteration 213/1000 | Loss: 0.00002465
Iteration 214/1000 | Loss: 0.00002465
Iteration 215/1000 | Loss: 0.00002465
Iteration 216/1000 | Loss: 0.00002465
Iteration 217/1000 | Loss: 0.00002465
Iteration 218/1000 | Loss: 0.00002465
Iteration 219/1000 | Loss: 0.00002465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [2.4652723368490115e-05, 2.4652723368490115e-05, 2.4652723368490115e-05, 2.4652723368490115e-05, 2.4652723368490115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4652723368490115e-05

Optimization complete. Final v2v error: 4.0904669761657715 mm

Highest mean error: 4.758878707885742 mm for frame 111

Lowest mean error: 3.5096001625061035 mm for frame 22

Saving results

Total time: 40.84993028640747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827393
Iteration 2/25 | Loss: 0.00110233
Iteration 3/25 | Loss: 0.00078172
Iteration 4/25 | Loss: 0.00075952
Iteration 5/25 | Loss: 0.00075581
Iteration 6/25 | Loss: 0.00075485
Iteration 7/25 | Loss: 0.00075485
Iteration 8/25 | Loss: 0.00075485
Iteration 9/25 | Loss: 0.00075485
Iteration 10/25 | Loss: 0.00075485
Iteration 11/25 | Loss: 0.00075485
Iteration 12/25 | Loss: 0.00075485
Iteration 13/25 | Loss: 0.00075485
Iteration 14/25 | Loss: 0.00075485
Iteration 15/25 | Loss: 0.00075485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007548534194938838, 0.0007548534194938838, 0.0007548534194938838, 0.0007548534194938838, 0.0007548534194938838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007548534194938838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41347051
Iteration 2/25 | Loss: 0.00022579
Iteration 3/25 | Loss: 0.00022579
Iteration 4/25 | Loss: 0.00022579
Iteration 5/25 | Loss: 0.00022579
Iteration 6/25 | Loss: 0.00022579
Iteration 7/25 | Loss: 0.00022579
Iteration 8/25 | Loss: 0.00022579
Iteration 9/25 | Loss: 0.00022579
Iteration 10/25 | Loss: 0.00022579
Iteration 11/25 | Loss: 0.00022579
Iteration 12/25 | Loss: 0.00022579
Iteration 13/25 | Loss: 0.00022579
Iteration 14/25 | Loss: 0.00022579
Iteration 15/25 | Loss: 0.00022579
Iteration 16/25 | Loss: 0.00022579
Iteration 17/25 | Loss: 0.00022579
Iteration 18/25 | Loss: 0.00022579
Iteration 19/25 | Loss: 0.00022579
Iteration 20/25 | Loss: 0.00022579
Iteration 21/25 | Loss: 0.00022579
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00022578985954169184, 0.00022578985954169184, 0.00022578985954169184, 0.00022578985954169184, 0.00022578985954169184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022578985954169184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022579
Iteration 2/1000 | Loss: 0.00004810
Iteration 3/1000 | Loss: 0.00003931
Iteration 4/1000 | Loss: 0.00003698
Iteration 5/1000 | Loss: 0.00003520
Iteration 6/1000 | Loss: 0.00003370
Iteration 7/1000 | Loss: 0.00003262
Iteration 8/1000 | Loss: 0.00003195
Iteration 9/1000 | Loss: 0.00003159
Iteration 10/1000 | Loss: 0.00003132
Iteration 11/1000 | Loss: 0.00003116
Iteration 12/1000 | Loss: 0.00003114
Iteration 13/1000 | Loss: 0.00003113
Iteration 14/1000 | Loss: 0.00003112
Iteration 15/1000 | Loss: 0.00003111
Iteration 16/1000 | Loss: 0.00003111
Iteration 17/1000 | Loss: 0.00003110
Iteration 18/1000 | Loss: 0.00003109
Iteration 19/1000 | Loss: 0.00003109
Iteration 20/1000 | Loss: 0.00003109
Iteration 21/1000 | Loss: 0.00003108
Iteration 22/1000 | Loss: 0.00003108
Iteration 23/1000 | Loss: 0.00003108
Iteration 24/1000 | Loss: 0.00003108
Iteration 25/1000 | Loss: 0.00003108
Iteration 26/1000 | Loss: 0.00003106
Iteration 27/1000 | Loss: 0.00003106
Iteration 28/1000 | Loss: 0.00003106
Iteration 29/1000 | Loss: 0.00003106
Iteration 30/1000 | Loss: 0.00003106
Iteration 31/1000 | Loss: 0.00003106
Iteration 32/1000 | Loss: 0.00003106
Iteration 33/1000 | Loss: 0.00003106
Iteration 34/1000 | Loss: 0.00003106
Iteration 35/1000 | Loss: 0.00003106
Iteration 36/1000 | Loss: 0.00003106
Iteration 37/1000 | Loss: 0.00003105
Iteration 38/1000 | Loss: 0.00003105
Iteration 39/1000 | Loss: 0.00003105
Iteration 40/1000 | Loss: 0.00003105
Iteration 41/1000 | Loss: 0.00003105
Iteration 42/1000 | Loss: 0.00003105
Iteration 43/1000 | Loss: 0.00003104
Iteration 44/1000 | Loss: 0.00003104
Iteration 45/1000 | Loss: 0.00003104
Iteration 46/1000 | Loss: 0.00003104
Iteration 47/1000 | Loss: 0.00003103
Iteration 48/1000 | Loss: 0.00003103
Iteration 49/1000 | Loss: 0.00003103
Iteration 50/1000 | Loss: 0.00003103
Iteration 51/1000 | Loss: 0.00003102
Iteration 52/1000 | Loss: 0.00003102
Iteration 53/1000 | Loss: 0.00003102
Iteration 54/1000 | Loss: 0.00003102
Iteration 55/1000 | Loss: 0.00003101
Iteration 56/1000 | Loss: 0.00003101
Iteration 57/1000 | Loss: 0.00003101
Iteration 58/1000 | Loss: 0.00003101
Iteration 59/1000 | Loss: 0.00003101
Iteration 60/1000 | Loss: 0.00003101
Iteration 61/1000 | Loss: 0.00003101
Iteration 62/1000 | Loss: 0.00003101
Iteration 63/1000 | Loss: 0.00003100
Iteration 64/1000 | Loss: 0.00003100
Iteration 65/1000 | Loss: 0.00003100
Iteration 66/1000 | Loss: 0.00003100
Iteration 67/1000 | Loss: 0.00003100
Iteration 68/1000 | Loss: 0.00003100
Iteration 69/1000 | Loss: 0.00003100
Iteration 70/1000 | Loss: 0.00003100
Iteration 71/1000 | Loss: 0.00003100
Iteration 72/1000 | Loss: 0.00003100
Iteration 73/1000 | Loss: 0.00003100
Iteration 74/1000 | Loss: 0.00003100
Iteration 75/1000 | Loss: 0.00003100
Iteration 76/1000 | Loss: 0.00003100
Iteration 77/1000 | Loss: 0.00003099
Iteration 78/1000 | Loss: 0.00003099
Iteration 79/1000 | Loss: 0.00003099
Iteration 80/1000 | Loss: 0.00003099
Iteration 81/1000 | Loss: 0.00003099
Iteration 82/1000 | Loss: 0.00003099
Iteration 83/1000 | Loss: 0.00003099
Iteration 84/1000 | Loss: 0.00003098
Iteration 85/1000 | Loss: 0.00003098
Iteration 86/1000 | Loss: 0.00003098
Iteration 87/1000 | Loss: 0.00003098
Iteration 88/1000 | Loss: 0.00003098
Iteration 89/1000 | Loss: 0.00003098
Iteration 90/1000 | Loss: 0.00003098
Iteration 91/1000 | Loss: 0.00003098
Iteration 92/1000 | Loss: 0.00003098
Iteration 93/1000 | Loss: 0.00003098
Iteration 94/1000 | Loss: 0.00003098
Iteration 95/1000 | Loss: 0.00003098
Iteration 96/1000 | Loss: 0.00003098
Iteration 97/1000 | Loss: 0.00003098
Iteration 98/1000 | Loss: 0.00003098
Iteration 99/1000 | Loss: 0.00003098
Iteration 100/1000 | Loss: 0.00003098
Iteration 101/1000 | Loss: 0.00003097
Iteration 102/1000 | Loss: 0.00003097
Iteration 103/1000 | Loss: 0.00003097
Iteration 104/1000 | Loss: 0.00003097
Iteration 105/1000 | Loss: 0.00003097
Iteration 106/1000 | Loss: 0.00003097
Iteration 107/1000 | Loss: 0.00003097
Iteration 108/1000 | Loss: 0.00003097
Iteration 109/1000 | Loss: 0.00003097
Iteration 110/1000 | Loss: 0.00003097
Iteration 111/1000 | Loss: 0.00003097
Iteration 112/1000 | Loss: 0.00003097
Iteration 113/1000 | Loss: 0.00003097
Iteration 114/1000 | Loss: 0.00003097
Iteration 115/1000 | Loss: 0.00003097
Iteration 116/1000 | Loss: 0.00003097
Iteration 117/1000 | Loss: 0.00003097
Iteration 118/1000 | Loss: 0.00003097
Iteration 119/1000 | Loss: 0.00003096
Iteration 120/1000 | Loss: 0.00003096
Iteration 121/1000 | Loss: 0.00003096
Iteration 122/1000 | Loss: 0.00003096
Iteration 123/1000 | Loss: 0.00003096
Iteration 124/1000 | Loss: 0.00003096
Iteration 125/1000 | Loss: 0.00003096
Iteration 126/1000 | Loss: 0.00003096
Iteration 127/1000 | Loss: 0.00003096
Iteration 128/1000 | Loss: 0.00003096
Iteration 129/1000 | Loss: 0.00003096
Iteration 130/1000 | Loss: 0.00003096
Iteration 131/1000 | Loss: 0.00003096
Iteration 132/1000 | Loss: 0.00003096
Iteration 133/1000 | Loss: 0.00003096
Iteration 134/1000 | Loss: 0.00003096
Iteration 135/1000 | Loss: 0.00003096
Iteration 136/1000 | Loss: 0.00003096
Iteration 137/1000 | Loss: 0.00003096
Iteration 138/1000 | Loss: 0.00003096
Iteration 139/1000 | Loss: 0.00003096
Iteration 140/1000 | Loss: 0.00003096
Iteration 141/1000 | Loss: 0.00003096
Iteration 142/1000 | Loss: 0.00003096
Iteration 143/1000 | Loss: 0.00003096
Iteration 144/1000 | Loss: 0.00003096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [3.0960065487306565e-05, 3.0960065487306565e-05, 3.0960065487306565e-05, 3.0960065487306565e-05, 3.0960065487306565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0960065487306565e-05

Optimization complete. Final v2v error: 4.5064897537231445 mm

Highest mean error: 4.841762542724609 mm for frame 0

Lowest mean error: 4.096848964691162 mm for frame 121

Saving results

Total time: 31.072607040405273
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825256
Iteration 2/25 | Loss: 0.00099114
Iteration 3/25 | Loss: 0.00072259
Iteration 4/25 | Loss: 0.00067924
Iteration 5/25 | Loss: 0.00066606
Iteration 6/25 | Loss: 0.00066201
Iteration 7/25 | Loss: 0.00066175
Iteration 8/25 | Loss: 0.00066175
Iteration 9/25 | Loss: 0.00066175
Iteration 10/25 | Loss: 0.00066175
Iteration 11/25 | Loss: 0.00066175
Iteration 12/25 | Loss: 0.00066175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006617491599172354, 0.0006617491599172354, 0.0006617491599172354, 0.0006617491599172354, 0.0006617491599172354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006617491599172354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80343592
Iteration 2/25 | Loss: 0.00024524
Iteration 3/25 | Loss: 0.00024524
Iteration 4/25 | Loss: 0.00024524
Iteration 5/25 | Loss: 0.00024524
Iteration 6/25 | Loss: 0.00024524
Iteration 7/25 | Loss: 0.00024523
Iteration 8/25 | Loss: 0.00024523
Iteration 9/25 | Loss: 0.00024523
Iteration 10/25 | Loss: 0.00024523
Iteration 11/25 | Loss: 0.00024523
Iteration 12/25 | Loss: 0.00024523
Iteration 13/25 | Loss: 0.00024523
Iteration 14/25 | Loss: 0.00024523
Iteration 15/25 | Loss: 0.00024523
Iteration 16/25 | Loss: 0.00024523
Iteration 17/25 | Loss: 0.00024523
Iteration 18/25 | Loss: 0.00024523
Iteration 19/25 | Loss: 0.00024523
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0002452342305332422, 0.0002452342305332422, 0.0002452342305332422, 0.0002452342305332422, 0.0002452342305332422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002452342305332422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024523
Iteration 2/1000 | Loss: 0.00003298
Iteration 3/1000 | Loss: 0.00002089
Iteration 4/1000 | Loss: 0.00001868
Iteration 5/1000 | Loss: 0.00001786
Iteration 6/1000 | Loss: 0.00001718
Iteration 7/1000 | Loss: 0.00001706
Iteration 8/1000 | Loss: 0.00001662
Iteration 9/1000 | Loss: 0.00001628
Iteration 10/1000 | Loss: 0.00001601
Iteration 11/1000 | Loss: 0.00001593
Iteration 12/1000 | Loss: 0.00001588
Iteration 13/1000 | Loss: 0.00001580
Iteration 14/1000 | Loss: 0.00001576
Iteration 15/1000 | Loss: 0.00001572
Iteration 16/1000 | Loss: 0.00001571
Iteration 17/1000 | Loss: 0.00001571
Iteration 18/1000 | Loss: 0.00001570
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001567
Iteration 22/1000 | Loss: 0.00001566
Iteration 23/1000 | Loss: 0.00001566
Iteration 24/1000 | Loss: 0.00001565
Iteration 25/1000 | Loss: 0.00001565
Iteration 26/1000 | Loss: 0.00001562
Iteration 27/1000 | Loss: 0.00001562
Iteration 28/1000 | Loss: 0.00001562
Iteration 29/1000 | Loss: 0.00001561
Iteration 30/1000 | Loss: 0.00001561
Iteration 31/1000 | Loss: 0.00001560
Iteration 32/1000 | Loss: 0.00001560
Iteration 33/1000 | Loss: 0.00001560
Iteration 34/1000 | Loss: 0.00001560
Iteration 35/1000 | Loss: 0.00001560
Iteration 36/1000 | Loss: 0.00001560
Iteration 37/1000 | Loss: 0.00001559
Iteration 38/1000 | Loss: 0.00001558
Iteration 39/1000 | Loss: 0.00001558
Iteration 40/1000 | Loss: 0.00001557
Iteration 41/1000 | Loss: 0.00001557
Iteration 42/1000 | Loss: 0.00001557
Iteration 43/1000 | Loss: 0.00001557
Iteration 44/1000 | Loss: 0.00001557
Iteration 45/1000 | Loss: 0.00001557
Iteration 46/1000 | Loss: 0.00001557
Iteration 47/1000 | Loss: 0.00001557
Iteration 48/1000 | Loss: 0.00001556
Iteration 49/1000 | Loss: 0.00001555
Iteration 50/1000 | Loss: 0.00001554
Iteration 51/1000 | Loss: 0.00001554
Iteration 52/1000 | Loss: 0.00001553
Iteration 53/1000 | Loss: 0.00001553
Iteration 54/1000 | Loss: 0.00001553
Iteration 55/1000 | Loss: 0.00001552
Iteration 56/1000 | Loss: 0.00001552
Iteration 57/1000 | Loss: 0.00001552
Iteration 58/1000 | Loss: 0.00001551
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001550
Iteration 61/1000 | Loss: 0.00001550
Iteration 62/1000 | Loss: 0.00001549
Iteration 63/1000 | Loss: 0.00001549
Iteration 64/1000 | Loss: 0.00001549
Iteration 65/1000 | Loss: 0.00001549
Iteration 66/1000 | Loss: 0.00001549
Iteration 67/1000 | Loss: 0.00001549
Iteration 68/1000 | Loss: 0.00001549
Iteration 69/1000 | Loss: 0.00001549
Iteration 70/1000 | Loss: 0.00001549
Iteration 71/1000 | Loss: 0.00001548
Iteration 72/1000 | Loss: 0.00001547
Iteration 73/1000 | Loss: 0.00001547
Iteration 74/1000 | Loss: 0.00001547
Iteration 75/1000 | Loss: 0.00001547
Iteration 76/1000 | Loss: 0.00001546
Iteration 77/1000 | Loss: 0.00001546
Iteration 78/1000 | Loss: 0.00001546
Iteration 79/1000 | Loss: 0.00001546
Iteration 80/1000 | Loss: 0.00001546
Iteration 81/1000 | Loss: 0.00001546
Iteration 82/1000 | Loss: 0.00001545
Iteration 83/1000 | Loss: 0.00001545
Iteration 84/1000 | Loss: 0.00001544
Iteration 85/1000 | Loss: 0.00001544
Iteration 86/1000 | Loss: 0.00001543
Iteration 87/1000 | Loss: 0.00001543
Iteration 88/1000 | Loss: 0.00001543
Iteration 89/1000 | Loss: 0.00001543
Iteration 90/1000 | Loss: 0.00001543
Iteration 91/1000 | Loss: 0.00001543
Iteration 92/1000 | Loss: 0.00001543
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001542
Iteration 98/1000 | Loss: 0.00001542
Iteration 99/1000 | Loss: 0.00001542
Iteration 100/1000 | Loss: 0.00001541
Iteration 101/1000 | Loss: 0.00001541
Iteration 102/1000 | Loss: 0.00001541
Iteration 103/1000 | Loss: 0.00001541
Iteration 104/1000 | Loss: 0.00001541
Iteration 105/1000 | Loss: 0.00001540
Iteration 106/1000 | Loss: 0.00001540
Iteration 107/1000 | Loss: 0.00001540
Iteration 108/1000 | Loss: 0.00001540
Iteration 109/1000 | Loss: 0.00001540
Iteration 110/1000 | Loss: 0.00001540
Iteration 111/1000 | Loss: 0.00001540
Iteration 112/1000 | Loss: 0.00001540
Iteration 113/1000 | Loss: 0.00001540
Iteration 114/1000 | Loss: 0.00001540
Iteration 115/1000 | Loss: 0.00001540
Iteration 116/1000 | Loss: 0.00001540
Iteration 117/1000 | Loss: 0.00001540
Iteration 118/1000 | Loss: 0.00001540
Iteration 119/1000 | Loss: 0.00001539
Iteration 120/1000 | Loss: 0.00001539
Iteration 121/1000 | Loss: 0.00001539
Iteration 122/1000 | Loss: 0.00001539
Iteration 123/1000 | Loss: 0.00001539
Iteration 124/1000 | Loss: 0.00001539
Iteration 125/1000 | Loss: 0.00001539
Iteration 126/1000 | Loss: 0.00001539
Iteration 127/1000 | Loss: 0.00001539
Iteration 128/1000 | Loss: 0.00001539
Iteration 129/1000 | Loss: 0.00001539
Iteration 130/1000 | Loss: 0.00001539
Iteration 131/1000 | Loss: 0.00001539
Iteration 132/1000 | Loss: 0.00001539
Iteration 133/1000 | Loss: 0.00001539
Iteration 134/1000 | Loss: 0.00001539
Iteration 135/1000 | Loss: 0.00001538
Iteration 136/1000 | Loss: 0.00001538
Iteration 137/1000 | Loss: 0.00001538
Iteration 138/1000 | Loss: 0.00001538
Iteration 139/1000 | Loss: 0.00001538
Iteration 140/1000 | Loss: 0.00001538
Iteration 141/1000 | Loss: 0.00001538
Iteration 142/1000 | Loss: 0.00001538
Iteration 143/1000 | Loss: 0.00001538
Iteration 144/1000 | Loss: 0.00001538
Iteration 145/1000 | Loss: 0.00001538
Iteration 146/1000 | Loss: 0.00001538
Iteration 147/1000 | Loss: 0.00001538
Iteration 148/1000 | Loss: 0.00001538
Iteration 149/1000 | Loss: 0.00001538
Iteration 150/1000 | Loss: 0.00001538
Iteration 151/1000 | Loss: 0.00001538
Iteration 152/1000 | Loss: 0.00001538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.538477408757899e-05, 1.538477408757899e-05, 1.538477408757899e-05, 1.538477408757899e-05, 1.538477408757899e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.538477408757899e-05

Optimization complete. Final v2v error: 3.3691110610961914 mm

Highest mean error: 4.064841270446777 mm for frame 199

Lowest mean error: 2.968881845474243 mm for frame 103

Saving results

Total time: 38.93053102493286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_6656/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_6656/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383891
Iteration 2/25 | Loss: 0.00072797
Iteration 3/25 | Loss: 0.00056992
Iteration 4/25 | Loss: 0.00055034
Iteration 5/25 | Loss: 0.00054517
Iteration 6/25 | Loss: 0.00054373
Iteration 7/25 | Loss: 0.00054348
Iteration 8/25 | Loss: 0.00054348
Iteration 9/25 | Loss: 0.00054348
Iteration 10/25 | Loss: 0.00054348
Iteration 11/25 | Loss: 0.00054348
Iteration 12/25 | Loss: 0.00054348
Iteration 13/25 | Loss: 0.00054348
Iteration 14/25 | Loss: 0.00054348
Iteration 15/25 | Loss: 0.00054348
Iteration 16/25 | Loss: 0.00054348
Iteration 17/25 | Loss: 0.00054348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005434812628664076, 0.0005434812628664076, 0.0005434812628664076, 0.0005434812628664076, 0.0005434812628664076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005434812628664076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42696428
Iteration 2/25 | Loss: 0.00023483
Iteration 3/25 | Loss: 0.00023482
Iteration 4/25 | Loss: 0.00023482
Iteration 5/25 | Loss: 0.00023482
Iteration 6/25 | Loss: 0.00023482
Iteration 7/25 | Loss: 0.00023482
Iteration 8/25 | Loss: 0.00023482
Iteration 9/25 | Loss: 0.00023482
Iteration 10/25 | Loss: 0.00023482
Iteration 11/25 | Loss: 0.00023482
Iteration 12/25 | Loss: 0.00023482
Iteration 13/25 | Loss: 0.00023482
Iteration 14/25 | Loss: 0.00023482
Iteration 15/25 | Loss: 0.00023482
Iteration 16/25 | Loss: 0.00023482
Iteration 17/25 | Loss: 0.00023482
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00023482099641114473, 0.00023482099641114473, 0.00023482099641114473, 0.00023482099641114473, 0.00023482099641114473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023482099641114473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023482
Iteration 2/1000 | Loss: 0.00001711
Iteration 3/1000 | Loss: 0.00001255
Iteration 4/1000 | Loss: 0.00001030
Iteration 5/1000 | Loss: 0.00000966
Iteration 6/1000 | Loss: 0.00000929
Iteration 7/1000 | Loss: 0.00000900
Iteration 8/1000 | Loss: 0.00000882
Iteration 9/1000 | Loss: 0.00000880
Iteration 10/1000 | Loss: 0.00000876
Iteration 11/1000 | Loss: 0.00000876
Iteration 12/1000 | Loss: 0.00000875
Iteration 13/1000 | Loss: 0.00000875
Iteration 14/1000 | Loss: 0.00000874
Iteration 15/1000 | Loss: 0.00000874
Iteration 16/1000 | Loss: 0.00000873
Iteration 17/1000 | Loss: 0.00000872
Iteration 18/1000 | Loss: 0.00000872
Iteration 19/1000 | Loss: 0.00000871
Iteration 20/1000 | Loss: 0.00000870
Iteration 21/1000 | Loss: 0.00000869
Iteration 22/1000 | Loss: 0.00000865
Iteration 23/1000 | Loss: 0.00000864
Iteration 24/1000 | Loss: 0.00000864
Iteration 25/1000 | Loss: 0.00000863
Iteration 26/1000 | Loss: 0.00000863
Iteration 27/1000 | Loss: 0.00000863
Iteration 28/1000 | Loss: 0.00000863
Iteration 29/1000 | Loss: 0.00000863
Iteration 30/1000 | Loss: 0.00000862
Iteration 31/1000 | Loss: 0.00000862
Iteration 32/1000 | Loss: 0.00000862
Iteration 33/1000 | Loss: 0.00000862
Iteration 34/1000 | Loss: 0.00000862
Iteration 35/1000 | Loss: 0.00000862
Iteration 36/1000 | Loss: 0.00000862
Iteration 37/1000 | Loss: 0.00000862
Iteration 38/1000 | Loss: 0.00000862
Iteration 39/1000 | Loss: 0.00000861
Iteration 40/1000 | Loss: 0.00000861
Iteration 41/1000 | Loss: 0.00000859
Iteration 42/1000 | Loss: 0.00000859
Iteration 43/1000 | Loss: 0.00000859
Iteration 44/1000 | Loss: 0.00000858
Iteration 45/1000 | Loss: 0.00000858
Iteration 46/1000 | Loss: 0.00000858
Iteration 47/1000 | Loss: 0.00000858
Iteration 48/1000 | Loss: 0.00000857
Iteration 49/1000 | Loss: 0.00000857
Iteration 50/1000 | Loss: 0.00000857
Iteration 51/1000 | Loss: 0.00000857
Iteration 52/1000 | Loss: 0.00000856
Iteration 53/1000 | Loss: 0.00000856
Iteration 54/1000 | Loss: 0.00000856
Iteration 55/1000 | Loss: 0.00000855
Iteration 56/1000 | Loss: 0.00000855
Iteration 57/1000 | Loss: 0.00000855
Iteration 58/1000 | Loss: 0.00000855
Iteration 59/1000 | Loss: 0.00000855
Iteration 60/1000 | Loss: 0.00000854
Iteration 61/1000 | Loss: 0.00000854
Iteration 62/1000 | Loss: 0.00000854
Iteration 63/1000 | Loss: 0.00000853
Iteration 64/1000 | Loss: 0.00000852
Iteration 65/1000 | Loss: 0.00000852
Iteration 66/1000 | Loss: 0.00000851
Iteration 67/1000 | Loss: 0.00000851
Iteration 68/1000 | Loss: 0.00000851
Iteration 69/1000 | Loss: 0.00000850
Iteration 70/1000 | Loss: 0.00000850
Iteration 71/1000 | Loss: 0.00000850
Iteration 72/1000 | Loss: 0.00000849
Iteration 73/1000 | Loss: 0.00000847
Iteration 74/1000 | Loss: 0.00000847
Iteration 75/1000 | Loss: 0.00000846
Iteration 76/1000 | Loss: 0.00000846
Iteration 77/1000 | Loss: 0.00000845
Iteration 78/1000 | Loss: 0.00000845
Iteration 79/1000 | Loss: 0.00000845
Iteration 80/1000 | Loss: 0.00000845
Iteration 81/1000 | Loss: 0.00000845
Iteration 82/1000 | Loss: 0.00000845
Iteration 83/1000 | Loss: 0.00000845
Iteration 84/1000 | Loss: 0.00000845
Iteration 85/1000 | Loss: 0.00000844
Iteration 86/1000 | Loss: 0.00000844
Iteration 87/1000 | Loss: 0.00000844
Iteration 88/1000 | Loss: 0.00000844
Iteration 89/1000 | Loss: 0.00000844
Iteration 90/1000 | Loss: 0.00000844
Iteration 91/1000 | Loss: 0.00000844
Iteration 92/1000 | Loss: 0.00000843
Iteration 93/1000 | Loss: 0.00000843
Iteration 94/1000 | Loss: 0.00000843
Iteration 95/1000 | Loss: 0.00000843
Iteration 96/1000 | Loss: 0.00000842
Iteration 97/1000 | Loss: 0.00000842
Iteration 98/1000 | Loss: 0.00000842
Iteration 99/1000 | Loss: 0.00000842
Iteration 100/1000 | Loss: 0.00000842
Iteration 101/1000 | Loss: 0.00000842
Iteration 102/1000 | Loss: 0.00000842
Iteration 103/1000 | Loss: 0.00000841
Iteration 104/1000 | Loss: 0.00000841
Iteration 105/1000 | Loss: 0.00000841
Iteration 106/1000 | Loss: 0.00000840
Iteration 107/1000 | Loss: 0.00000840
Iteration 108/1000 | Loss: 0.00000840
Iteration 109/1000 | Loss: 0.00000840
Iteration 110/1000 | Loss: 0.00000840
Iteration 111/1000 | Loss: 0.00000840
Iteration 112/1000 | Loss: 0.00000840
Iteration 113/1000 | Loss: 0.00000840
Iteration 114/1000 | Loss: 0.00000840
Iteration 115/1000 | Loss: 0.00000840
Iteration 116/1000 | Loss: 0.00000840
Iteration 117/1000 | Loss: 0.00000840
Iteration 118/1000 | Loss: 0.00000840
Iteration 119/1000 | Loss: 0.00000840
Iteration 120/1000 | Loss: 0.00000840
Iteration 121/1000 | Loss: 0.00000840
Iteration 122/1000 | Loss: 0.00000839
Iteration 123/1000 | Loss: 0.00000839
Iteration 124/1000 | Loss: 0.00000839
Iteration 125/1000 | Loss: 0.00000839
Iteration 126/1000 | Loss: 0.00000839
Iteration 127/1000 | Loss: 0.00000839
Iteration 128/1000 | Loss: 0.00000839
Iteration 129/1000 | Loss: 0.00000839
Iteration 130/1000 | Loss: 0.00000839
Iteration 131/1000 | Loss: 0.00000839
Iteration 132/1000 | Loss: 0.00000839
Iteration 133/1000 | Loss: 0.00000839
Iteration 134/1000 | Loss: 0.00000839
Iteration 135/1000 | Loss: 0.00000839
Iteration 136/1000 | Loss: 0.00000839
Iteration 137/1000 | Loss: 0.00000839
Iteration 138/1000 | Loss: 0.00000838
Iteration 139/1000 | Loss: 0.00000838
Iteration 140/1000 | Loss: 0.00000838
Iteration 141/1000 | Loss: 0.00000838
Iteration 142/1000 | Loss: 0.00000838
Iteration 143/1000 | Loss: 0.00000838
Iteration 144/1000 | Loss: 0.00000838
Iteration 145/1000 | Loss: 0.00000838
Iteration 146/1000 | Loss: 0.00000838
Iteration 147/1000 | Loss: 0.00000837
Iteration 148/1000 | Loss: 0.00000837
Iteration 149/1000 | Loss: 0.00000837
Iteration 150/1000 | Loss: 0.00000837
Iteration 151/1000 | Loss: 0.00000837
Iteration 152/1000 | Loss: 0.00000837
Iteration 153/1000 | Loss: 0.00000837
Iteration 154/1000 | Loss: 0.00000837
Iteration 155/1000 | Loss: 0.00000837
Iteration 156/1000 | Loss: 0.00000837
Iteration 157/1000 | Loss: 0.00000836
Iteration 158/1000 | Loss: 0.00000836
Iteration 159/1000 | Loss: 0.00000836
Iteration 160/1000 | Loss: 0.00000836
Iteration 161/1000 | Loss: 0.00000836
Iteration 162/1000 | Loss: 0.00000836
Iteration 163/1000 | Loss: 0.00000836
Iteration 164/1000 | Loss: 0.00000836
Iteration 165/1000 | Loss: 0.00000836
Iteration 166/1000 | Loss: 0.00000836
Iteration 167/1000 | Loss: 0.00000835
Iteration 168/1000 | Loss: 0.00000835
Iteration 169/1000 | Loss: 0.00000835
Iteration 170/1000 | Loss: 0.00000835
Iteration 171/1000 | Loss: 0.00000835
Iteration 172/1000 | Loss: 0.00000835
Iteration 173/1000 | Loss: 0.00000835
Iteration 174/1000 | Loss: 0.00000835
Iteration 175/1000 | Loss: 0.00000835
Iteration 176/1000 | Loss: 0.00000834
Iteration 177/1000 | Loss: 0.00000834
Iteration 178/1000 | Loss: 0.00000834
Iteration 179/1000 | Loss: 0.00000834
Iteration 180/1000 | Loss: 0.00000834
Iteration 181/1000 | Loss: 0.00000834
Iteration 182/1000 | Loss: 0.00000833
Iteration 183/1000 | Loss: 0.00000833
Iteration 184/1000 | Loss: 0.00000833
Iteration 185/1000 | Loss: 0.00000833
Iteration 186/1000 | Loss: 0.00000833
Iteration 187/1000 | Loss: 0.00000833
Iteration 188/1000 | Loss: 0.00000833
Iteration 189/1000 | Loss: 0.00000833
Iteration 190/1000 | Loss: 0.00000833
Iteration 191/1000 | Loss: 0.00000832
Iteration 192/1000 | Loss: 0.00000832
Iteration 193/1000 | Loss: 0.00000832
Iteration 194/1000 | Loss: 0.00000832
Iteration 195/1000 | Loss: 0.00000832
Iteration 196/1000 | Loss: 0.00000832
Iteration 197/1000 | Loss: 0.00000832
Iteration 198/1000 | Loss: 0.00000832
Iteration 199/1000 | Loss: 0.00000832
Iteration 200/1000 | Loss: 0.00000832
Iteration 201/1000 | Loss: 0.00000832
Iteration 202/1000 | Loss: 0.00000832
Iteration 203/1000 | Loss: 0.00000832
Iteration 204/1000 | Loss: 0.00000832
Iteration 205/1000 | Loss: 0.00000831
Iteration 206/1000 | Loss: 0.00000831
Iteration 207/1000 | Loss: 0.00000831
Iteration 208/1000 | Loss: 0.00000831
Iteration 209/1000 | Loss: 0.00000831
Iteration 210/1000 | Loss: 0.00000831
Iteration 211/1000 | Loss: 0.00000831
Iteration 212/1000 | Loss: 0.00000831
Iteration 213/1000 | Loss: 0.00000831
Iteration 214/1000 | Loss: 0.00000831
Iteration 215/1000 | Loss: 0.00000831
Iteration 216/1000 | Loss: 0.00000831
Iteration 217/1000 | Loss: 0.00000831
Iteration 218/1000 | Loss: 0.00000831
Iteration 219/1000 | Loss: 0.00000831
Iteration 220/1000 | Loss: 0.00000831
Iteration 221/1000 | Loss: 0.00000831
Iteration 222/1000 | Loss: 0.00000831
Iteration 223/1000 | Loss: 0.00000831
Iteration 224/1000 | Loss: 0.00000831
Iteration 225/1000 | Loss: 0.00000831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [8.31105080578709e-06, 8.31105080578709e-06, 8.31105080578709e-06, 8.31105080578709e-06, 8.31105080578709e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.31105080578709e-06

Optimization complete. Final v2v error: 2.4432082176208496 mm

Highest mean error: 3.248354196548462 mm for frame 72

Lowest mean error: 2.145874261856079 mm for frame 0

Saving results

Total time: 36.77839803695679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059017
Iteration 2/25 | Loss: 0.00187991
Iteration 3/25 | Loss: 0.00127479
Iteration 4/25 | Loss: 0.00114403
Iteration 5/25 | Loss: 0.00105203
Iteration 6/25 | Loss: 0.00101914
Iteration 7/25 | Loss: 0.00101514
Iteration 8/25 | Loss: 0.00101255
Iteration 9/25 | Loss: 0.00101090
Iteration 10/25 | Loss: 0.00100983
Iteration 11/25 | Loss: 0.00100944
Iteration 12/25 | Loss: 0.00100915
Iteration 13/25 | Loss: 0.00100905
Iteration 14/25 | Loss: 0.00100905
Iteration 15/25 | Loss: 0.00100905
Iteration 16/25 | Loss: 0.00100904
Iteration 17/25 | Loss: 0.00100904
Iteration 18/25 | Loss: 0.00100904
Iteration 19/25 | Loss: 0.00100904
Iteration 20/25 | Loss: 0.00100904
Iteration 21/25 | Loss: 0.00100904
Iteration 22/25 | Loss: 0.00100904
Iteration 23/25 | Loss: 0.00100904
Iteration 24/25 | Loss: 0.00100904
Iteration 25/25 | Loss: 0.00100904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25206578
Iteration 2/25 | Loss: 0.00043003
Iteration 3/25 | Loss: 0.00043003
Iteration 4/25 | Loss: 0.00043003
Iteration 5/25 | Loss: 0.00043003
Iteration 6/25 | Loss: 0.00043003
Iteration 7/25 | Loss: 0.00043002
Iteration 8/25 | Loss: 0.00043002
Iteration 9/25 | Loss: 0.00043002
Iteration 10/25 | Loss: 0.00043002
Iteration 11/25 | Loss: 0.00043002
Iteration 12/25 | Loss: 0.00043002
Iteration 13/25 | Loss: 0.00043002
Iteration 14/25 | Loss: 0.00043002
Iteration 15/25 | Loss: 0.00043002
Iteration 16/25 | Loss: 0.00043002
Iteration 17/25 | Loss: 0.00043002
Iteration 18/25 | Loss: 0.00043002
Iteration 19/25 | Loss: 0.00043002
Iteration 20/25 | Loss: 0.00043002
Iteration 21/25 | Loss: 0.00043002
Iteration 22/25 | Loss: 0.00043002
Iteration 23/25 | Loss: 0.00043002
Iteration 24/25 | Loss: 0.00043002
Iteration 25/25 | Loss: 0.00043002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043002
Iteration 2/1000 | Loss: 0.00004099
Iteration 3/1000 | Loss: 0.00011312
Iteration 4/1000 | Loss: 0.00003156
Iteration 5/1000 | Loss: 0.00002269
Iteration 6/1000 | Loss: 0.00002071
Iteration 7/1000 | Loss: 0.00001978
Iteration 8/1000 | Loss: 0.00001908
Iteration 9/1000 | Loss: 0.00028521
Iteration 10/1000 | Loss: 0.00001894
Iteration 11/1000 | Loss: 0.00001823
Iteration 12/1000 | Loss: 0.00001794
Iteration 13/1000 | Loss: 0.00012976
Iteration 14/1000 | Loss: 0.00004672
Iteration 15/1000 | Loss: 0.00001778
Iteration 16/1000 | Loss: 0.00009990
Iteration 17/1000 | Loss: 0.00004766
Iteration 18/1000 | Loss: 0.00001777
Iteration 19/1000 | Loss: 0.00005827
Iteration 20/1000 | Loss: 0.00002578
Iteration 21/1000 | Loss: 0.00001761
Iteration 22/1000 | Loss: 0.00003388
Iteration 23/1000 | Loss: 0.00001755
Iteration 24/1000 | Loss: 0.00001745
Iteration 25/1000 | Loss: 0.00001741
Iteration 26/1000 | Loss: 0.00001741
Iteration 27/1000 | Loss: 0.00001740
Iteration 28/1000 | Loss: 0.00001740
Iteration 29/1000 | Loss: 0.00001740
Iteration 30/1000 | Loss: 0.00001740
Iteration 31/1000 | Loss: 0.00001740
Iteration 32/1000 | Loss: 0.00001740
Iteration 33/1000 | Loss: 0.00001740
Iteration 34/1000 | Loss: 0.00001739
Iteration 35/1000 | Loss: 0.00001739
Iteration 36/1000 | Loss: 0.00001739
Iteration 37/1000 | Loss: 0.00001739
Iteration 38/1000 | Loss: 0.00001738
Iteration 39/1000 | Loss: 0.00001738
Iteration 40/1000 | Loss: 0.00001738
Iteration 41/1000 | Loss: 0.00001738
Iteration 42/1000 | Loss: 0.00001738
Iteration 43/1000 | Loss: 0.00001738
Iteration 44/1000 | Loss: 0.00001738
Iteration 45/1000 | Loss: 0.00001738
Iteration 46/1000 | Loss: 0.00001737
Iteration 47/1000 | Loss: 0.00001737
Iteration 48/1000 | Loss: 0.00001737
Iteration 49/1000 | Loss: 0.00001737
Iteration 50/1000 | Loss: 0.00001737
Iteration 51/1000 | Loss: 0.00001737
Iteration 52/1000 | Loss: 0.00001737
Iteration 53/1000 | Loss: 0.00001737
Iteration 54/1000 | Loss: 0.00001737
Iteration 55/1000 | Loss: 0.00001737
Iteration 56/1000 | Loss: 0.00001737
Iteration 57/1000 | Loss: 0.00001737
Iteration 58/1000 | Loss: 0.00001737
Iteration 59/1000 | Loss: 0.00001737
Iteration 60/1000 | Loss: 0.00001737
Iteration 61/1000 | Loss: 0.00001737
Iteration 62/1000 | Loss: 0.00001737
Iteration 63/1000 | Loss: 0.00001737
Iteration 64/1000 | Loss: 0.00001737
Iteration 65/1000 | Loss: 0.00001737
Iteration 66/1000 | Loss: 0.00001737
Iteration 67/1000 | Loss: 0.00001737
Iteration 68/1000 | Loss: 0.00001737
Iteration 69/1000 | Loss: 0.00001737
Iteration 70/1000 | Loss: 0.00001737
Iteration 71/1000 | Loss: 0.00001737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.737262209644541e-05, 1.737262209644541e-05, 1.737262209644541e-05, 1.737262209644541e-05, 1.737262209644541e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.737262209644541e-05

Optimization complete. Final v2v error: 3.3938934803009033 mm

Highest mean error: 5.273321628570557 mm for frame 217

Lowest mean error: 2.8227458000183105 mm for frame 9

Saving results

Total time: 63.72379779815674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435756
Iteration 2/25 | Loss: 0.00100409
Iteration 3/25 | Loss: 0.00088529
Iteration 4/25 | Loss: 0.00086865
Iteration 5/25 | Loss: 0.00086253
Iteration 6/25 | Loss: 0.00086110
Iteration 7/25 | Loss: 0.00086059
Iteration 8/25 | Loss: 0.00086058
Iteration 9/25 | Loss: 0.00086058
Iteration 10/25 | Loss: 0.00086058
Iteration 11/25 | Loss: 0.00086058
Iteration 12/25 | Loss: 0.00086058
Iteration 13/25 | Loss: 0.00086058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008605849579907954, 0.0008605849579907954, 0.0008605849579907954, 0.0008605849579907954, 0.0008605849579907954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008605849579907954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24829173
Iteration 2/25 | Loss: 0.00058052
Iteration 3/25 | Loss: 0.00058049
Iteration 4/25 | Loss: 0.00058049
Iteration 5/25 | Loss: 0.00058049
Iteration 6/25 | Loss: 0.00058049
Iteration 7/25 | Loss: 0.00058049
Iteration 8/25 | Loss: 0.00058049
Iteration 9/25 | Loss: 0.00058049
Iteration 10/25 | Loss: 0.00058049
Iteration 11/25 | Loss: 0.00058049
Iteration 12/25 | Loss: 0.00058049
Iteration 13/25 | Loss: 0.00058049
Iteration 14/25 | Loss: 0.00058049
Iteration 15/25 | Loss: 0.00058049
Iteration 16/25 | Loss: 0.00058049
Iteration 17/25 | Loss: 0.00058049
Iteration 18/25 | Loss: 0.00058049
Iteration 19/25 | Loss: 0.00058049
Iteration 20/25 | Loss: 0.00058049
Iteration 21/25 | Loss: 0.00058049
Iteration 22/25 | Loss: 0.00058049
Iteration 23/25 | Loss: 0.00058049
Iteration 24/25 | Loss: 0.00058049
Iteration 25/25 | Loss: 0.00058049

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058049
Iteration 2/1000 | Loss: 0.00003065
Iteration 3/1000 | Loss: 0.00001923
Iteration 4/1000 | Loss: 0.00001443
Iteration 5/1000 | Loss: 0.00001266
Iteration 6/1000 | Loss: 0.00001203
Iteration 7/1000 | Loss: 0.00001158
Iteration 8/1000 | Loss: 0.00001110
Iteration 9/1000 | Loss: 0.00001080
Iteration 10/1000 | Loss: 0.00001060
Iteration 11/1000 | Loss: 0.00001044
Iteration 12/1000 | Loss: 0.00001027
Iteration 13/1000 | Loss: 0.00001022
Iteration 14/1000 | Loss: 0.00001020
Iteration 15/1000 | Loss: 0.00001019
Iteration 16/1000 | Loss: 0.00001018
Iteration 17/1000 | Loss: 0.00001017
Iteration 18/1000 | Loss: 0.00001017
Iteration 19/1000 | Loss: 0.00001016
Iteration 20/1000 | Loss: 0.00001016
Iteration 21/1000 | Loss: 0.00001015
Iteration 22/1000 | Loss: 0.00001015
Iteration 23/1000 | Loss: 0.00001015
Iteration 24/1000 | Loss: 0.00001015
Iteration 25/1000 | Loss: 0.00001015
Iteration 26/1000 | Loss: 0.00001015
Iteration 27/1000 | Loss: 0.00001015
Iteration 28/1000 | Loss: 0.00001015
Iteration 29/1000 | Loss: 0.00001015
Iteration 30/1000 | Loss: 0.00001014
Iteration 31/1000 | Loss: 0.00001014
Iteration 32/1000 | Loss: 0.00001014
Iteration 33/1000 | Loss: 0.00001014
Iteration 34/1000 | Loss: 0.00001014
Iteration 35/1000 | Loss: 0.00001013
Iteration 36/1000 | Loss: 0.00001012
Iteration 37/1000 | Loss: 0.00001010
Iteration 38/1000 | Loss: 0.00001010
Iteration 39/1000 | Loss: 0.00001010
Iteration 40/1000 | Loss: 0.00001010
Iteration 41/1000 | Loss: 0.00001010
Iteration 42/1000 | Loss: 0.00001010
Iteration 43/1000 | Loss: 0.00001010
Iteration 44/1000 | Loss: 0.00001009
Iteration 45/1000 | Loss: 0.00001009
Iteration 46/1000 | Loss: 0.00001009
Iteration 47/1000 | Loss: 0.00001008
Iteration 48/1000 | Loss: 0.00001007
Iteration 49/1000 | Loss: 0.00001007
Iteration 50/1000 | Loss: 0.00001006
Iteration 51/1000 | Loss: 0.00001006
Iteration 52/1000 | Loss: 0.00001005
Iteration 53/1000 | Loss: 0.00001005
Iteration 54/1000 | Loss: 0.00001005
Iteration 55/1000 | Loss: 0.00001005
Iteration 56/1000 | Loss: 0.00001005
Iteration 57/1000 | Loss: 0.00001005
Iteration 58/1000 | Loss: 0.00001005
Iteration 59/1000 | Loss: 0.00001004
Iteration 60/1000 | Loss: 0.00001004
Iteration 61/1000 | Loss: 0.00001004
Iteration 62/1000 | Loss: 0.00001004
Iteration 63/1000 | Loss: 0.00001004
Iteration 64/1000 | Loss: 0.00001004
Iteration 65/1000 | Loss: 0.00001004
Iteration 66/1000 | Loss: 0.00001004
Iteration 67/1000 | Loss: 0.00001004
Iteration 68/1000 | Loss: 0.00001003
Iteration 69/1000 | Loss: 0.00001003
Iteration 70/1000 | Loss: 0.00001002
Iteration 71/1000 | Loss: 0.00001002
Iteration 72/1000 | Loss: 0.00001002
Iteration 73/1000 | Loss: 0.00001001
Iteration 74/1000 | Loss: 0.00001001
Iteration 75/1000 | Loss: 0.00001001
Iteration 76/1000 | Loss: 0.00001001
Iteration 77/1000 | Loss: 0.00001001
Iteration 78/1000 | Loss: 0.00001001
Iteration 79/1000 | Loss: 0.00001001
Iteration 80/1000 | Loss: 0.00001001
Iteration 81/1000 | Loss: 0.00001001
Iteration 82/1000 | Loss: 0.00001001
Iteration 83/1000 | Loss: 0.00001001
Iteration 84/1000 | Loss: 0.00001001
Iteration 85/1000 | Loss: 0.00001001
Iteration 86/1000 | Loss: 0.00001001
Iteration 87/1000 | Loss: 0.00001001
Iteration 88/1000 | Loss: 0.00001001
Iteration 89/1000 | Loss: 0.00001001
Iteration 90/1000 | Loss: 0.00001001
Iteration 91/1000 | Loss: 0.00001000
Iteration 92/1000 | Loss: 0.00001000
Iteration 93/1000 | Loss: 0.00001000
Iteration 94/1000 | Loss: 0.00001000
Iteration 95/1000 | Loss: 0.00001000
Iteration 96/1000 | Loss: 0.00001000
Iteration 97/1000 | Loss: 0.00001000
Iteration 98/1000 | Loss: 0.00000999
Iteration 99/1000 | Loss: 0.00000999
Iteration 100/1000 | Loss: 0.00000999
Iteration 101/1000 | Loss: 0.00000999
Iteration 102/1000 | Loss: 0.00000999
Iteration 103/1000 | Loss: 0.00000999
Iteration 104/1000 | Loss: 0.00000999
Iteration 105/1000 | Loss: 0.00000999
Iteration 106/1000 | Loss: 0.00000999
Iteration 107/1000 | Loss: 0.00000999
Iteration 108/1000 | Loss: 0.00000999
Iteration 109/1000 | Loss: 0.00000999
Iteration 110/1000 | Loss: 0.00000998
Iteration 111/1000 | Loss: 0.00000998
Iteration 112/1000 | Loss: 0.00000998
Iteration 113/1000 | Loss: 0.00000998
Iteration 114/1000 | Loss: 0.00000998
Iteration 115/1000 | Loss: 0.00000998
Iteration 116/1000 | Loss: 0.00000998
Iteration 117/1000 | Loss: 0.00000998
Iteration 118/1000 | Loss: 0.00000998
Iteration 119/1000 | Loss: 0.00000998
Iteration 120/1000 | Loss: 0.00000998
Iteration 121/1000 | Loss: 0.00000998
Iteration 122/1000 | Loss: 0.00000998
Iteration 123/1000 | Loss: 0.00000998
Iteration 124/1000 | Loss: 0.00000998
Iteration 125/1000 | Loss: 0.00000997
Iteration 126/1000 | Loss: 0.00000997
Iteration 127/1000 | Loss: 0.00000997
Iteration 128/1000 | Loss: 0.00000997
Iteration 129/1000 | Loss: 0.00000997
Iteration 130/1000 | Loss: 0.00000997
Iteration 131/1000 | Loss: 0.00000997
Iteration 132/1000 | Loss: 0.00000997
Iteration 133/1000 | Loss: 0.00000997
Iteration 134/1000 | Loss: 0.00000997
Iteration 135/1000 | Loss: 0.00000997
Iteration 136/1000 | Loss: 0.00000997
Iteration 137/1000 | Loss: 0.00000997
Iteration 138/1000 | Loss: 0.00000997
Iteration 139/1000 | Loss: 0.00000997
Iteration 140/1000 | Loss: 0.00000997
Iteration 141/1000 | Loss: 0.00000997
Iteration 142/1000 | Loss: 0.00000997
Iteration 143/1000 | Loss: 0.00000997
Iteration 144/1000 | Loss: 0.00000997
Iteration 145/1000 | Loss: 0.00000997
Iteration 146/1000 | Loss: 0.00000997
Iteration 147/1000 | Loss: 0.00000997
Iteration 148/1000 | Loss: 0.00000997
Iteration 149/1000 | Loss: 0.00000997
Iteration 150/1000 | Loss: 0.00000997
Iteration 151/1000 | Loss: 0.00000997
Iteration 152/1000 | Loss: 0.00000997
Iteration 153/1000 | Loss: 0.00000997
Iteration 154/1000 | Loss: 0.00000997
Iteration 155/1000 | Loss: 0.00000997
Iteration 156/1000 | Loss: 0.00000997
Iteration 157/1000 | Loss: 0.00000997
Iteration 158/1000 | Loss: 0.00000997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [9.96596736513311e-06, 9.96596736513311e-06, 9.96596736513311e-06, 9.96596736513311e-06, 9.96596736513311e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.96596736513311e-06

Optimization complete. Final v2v error: 2.7634153366088867 mm

Highest mean error: 3.0875325202941895 mm for frame 3

Lowest mean error: 2.394181251525879 mm for frame 134

Saving results

Total time: 36.641953229904175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468156
Iteration 2/25 | Loss: 0.00093304
Iteration 3/25 | Loss: 0.00084867
Iteration 4/25 | Loss: 0.00083802
Iteration 5/25 | Loss: 0.00083441
Iteration 6/25 | Loss: 0.00083331
Iteration 7/25 | Loss: 0.00083331
Iteration 8/25 | Loss: 0.00083331
Iteration 9/25 | Loss: 0.00083331
Iteration 10/25 | Loss: 0.00083331
Iteration 11/25 | Loss: 0.00083331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008333081495948136, 0.0008333081495948136, 0.0008333081495948136, 0.0008333081495948136, 0.0008333081495948136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008333081495948136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.54248738
Iteration 2/25 | Loss: 0.00062072
Iteration 3/25 | Loss: 0.00062072
Iteration 4/25 | Loss: 0.00062072
Iteration 5/25 | Loss: 0.00062072
Iteration 6/25 | Loss: 0.00062072
Iteration 7/25 | Loss: 0.00062072
Iteration 8/25 | Loss: 0.00062071
Iteration 9/25 | Loss: 0.00062071
Iteration 10/25 | Loss: 0.00062071
Iteration 11/25 | Loss: 0.00062071
Iteration 12/25 | Loss: 0.00062071
Iteration 13/25 | Loss: 0.00062071
Iteration 14/25 | Loss: 0.00062071
Iteration 15/25 | Loss: 0.00062071
Iteration 16/25 | Loss: 0.00062071
Iteration 17/25 | Loss: 0.00062071
Iteration 18/25 | Loss: 0.00062071
Iteration 19/25 | Loss: 0.00062071
Iteration 20/25 | Loss: 0.00062071
Iteration 21/25 | Loss: 0.00062071
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006207135156728327, 0.0006207135156728327, 0.0006207135156728327, 0.0006207135156728327, 0.0006207135156728327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006207135156728327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062071
Iteration 2/1000 | Loss: 0.00003221
Iteration 3/1000 | Loss: 0.00001892
Iteration 4/1000 | Loss: 0.00001354
Iteration 5/1000 | Loss: 0.00001200
Iteration 6/1000 | Loss: 0.00001151
Iteration 7/1000 | Loss: 0.00001129
Iteration 8/1000 | Loss: 0.00001090
Iteration 9/1000 | Loss: 0.00001077
Iteration 10/1000 | Loss: 0.00001073
Iteration 11/1000 | Loss: 0.00001065
Iteration 12/1000 | Loss: 0.00001065
Iteration 13/1000 | Loss: 0.00001057
Iteration 14/1000 | Loss: 0.00001056
Iteration 15/1000 | Loss: 0.00001054
Iteration 16/1000 | Loss: 0.00001054
Iteration 17/1000 | Loss: 0.00001053
Iteration 18/1000 | Loss: 0.00001049
Iteration 19/1000 | Loss: 0.00001049
Iteration 20/1000 | Loss: 0.00001048
Iteration 21/1000 | Loss: 0.00001048
Iteration 22/1000 | Loss: 0.00001047
Iteration 23/1000 | Loss: 0.00001047
Iteration 24/1000 | Loss: 0.00001046
Iteration 25/1000 | Loss: 0.00001046
Iteration 26/1000 | Loss: 0.00001045
Iteration 27/1000 | Loss: 0.00001045
Iteration 28/1000 | Loss: 0.00001045
Iteration 29/1000 | Loss: 0.00001044
Iteration 30/1000 | Loss: 0.00001044
Iteration 31/1000 | Loss: 0.00001044
Iteration 32/1000 | Loss: 0.00001044
Iteration 33/1000 | Loss: 0.00001044
Iteration 34/1000 | Loss: 0.00001044
Iteration 35/1000 | Loss: 0.00001043
Iteration 36/1000 | Loss: 0.00001043
Iteration 37/1000 | Loss: 0.00001043
Iteration 38/1000 | Loss: 0.00001042
Iteration 39/1000 | Loss: 0.00001042
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001042
Iteration 43/1000 | Loss: 0.00001042
Iteration 44/1000 | Loss: 0.00001042
Iteration 45/1000 | Loss: 0.00001042
Iteration 46/1000 | Loss: 0.00001041
Iteration 47/1000 | Loss: 0.00001041
Iteration 48/1000 | Loss: 0.00001041
Iteration 49/1000 | Loss: 0.00001041
Iteration 50/1000 | Loss: 0.00001040
Iteration 51/1000 | Loss: 0.00001040
Iteration 52/1000 | Loss: 0.00001040
Iteration 53/1000 | Loss: 0.00001039
Iteration 54/1000 | Loss: 0.00001039
Iteration 55/1000 | Loss: 0.00001039
Iteration 56/1000 | Loss: 0.00001039
Iteration 57/1000 | Loss: 0.00001039
Iteration 58/1000 | Loss: 0.00001039
Iteration 59/1000 | Loss: 0.00001039
Iteration 60/1000 | Loss: 0.00001038
Iteration 61/1000 | Loss: 0.00001038
Iteration 62/1000 | Loss: 0.00001038
Iteration 63/1000 | Loss: 0.00001038
Iteration 64/1000 | Loss: 0.00001038
Iteration 65/1000 | Loss: 0.00001038
Iteration 66/1000 | Loss: 0.00001037
Iteration 67/1000 | Loss: 0.00001037
Iteration 68/1000 | Loss: 0.00001037
Iteration 69/1000 | Loss: 0.00001037
Iteration 70/1000 | Loss: 0.00001037
Iteration 71/1000 | Loss: 0.00001036
Iteration 72/1000 | Loss: 0.00001036
Iteration 73/1000 | Loss: 0.00001036
Iteration 74/1000 | Loss: 0.00001036
Iteration 75/1000 | Loss: 0.00001036
Iteration 76/1000 | Loss: 0.00001036
Iteration 77/1000 | Loss: 0.00001036
Iteration 78/1000 | Loss: 0.00001036
Iteration 79/1000 | Loss: 0.00001036
Iteration 80/1000 | Loss: 0.00001036
Iteration 81/1000 | Loss: 0.00001036
Iteration 82/1000 | Loss: 0.00001035
Iteration 83/1000 | Loss: 0.00001035
Iteration 84/1000 | Loss: 0.00001035
Iteration 85/1000 | Loss: 0.00001035
Iteration 86/1000 | Loss: 0.00001035
Iteration 87/1000 | Loss: 0.00001035
Iteration 88/1000 | Loss: 0.00001035
Iteration 89/1000 | Loss: 0.00001035
Iteration 90/1000 | Loss: 0.00001035
Iteration 91/1000 | Loss: 0.00001035
Iteration 92/1000 | Loss: 0.00001035
Iteration 93/1000 | Loss: 0.00001034
Iteration 94/1000 | Loss: 0.00001034
Iteration 95/1000 | Loss: 0.00001034
Iteration 96/1000 | Loss: 0.00001034
Iteration 97/1000 | Loss: 0.00001033
Iteration 98/1000 | Loss: 0.00001033
Iteration 99/1000 | Loss: 0.00001033
Iteration 100/1000 | Loss: 0.00001033
Iteration 101/1000 | Loss: 0.00001033
Iteration 102/1000 | Loss: 0.00001033
Iteration 103/1000 | Loss: 0.00001033
Iteration 104/1000 | Loss: 0.00001033
Iteration 105/1000 | Loss: 0.00001033
Iteration 106/1000 | Loss: 0.00001033
Iteration 107/1000 | Loss: 0.00001033
Iteration 108/1000 | Loss: 0.00001033
Iteration 109/1000 | Loss: 0.00001033
Iteration 110/1000 | Loss: 0.00001033
Iteration 111/1000 | Loss: 0.00001032
Iteration 112/1000 | Loss: 0.00001032
Iteration 113/1000 | Loss: 0.00001032
Iteration 114/1000 | Loss: 0.00001032
Iteration 115/1000 | Loss: 0.00001032
Iteration 116/1000 | Loss: 0.00001032
Iteration 117/1000 | Loss: 0.00001032
Iteration 118/1000 | Loss: 0.00001032
Iteration 119/1000 | Loss: 0.00001032
Iteration 120/1000 | Loss: 0.00001032
Iteration 121/1000 | Loss: 0.00001032
Iteration 122/1000 | Loss: 0.00001032
Iteration 123/1000 | Loss: 0.00001031
Iteration 124/1000 | Loss: 0.00001031
Iteration 125/1000 | Loss: 0.00001031
Iteration 126/1000 | Loss: 0.00001031
Iteration 127/1000 | Loss: 0.00001031
Iteration 128/1000 | Loss: 0.00001031
Iteration 129/1000 | Loss: 0.00001030
Iteration 130/1000 | Loss: 0.00001030
Iteration 131/1000 | Loss: 0.00001030
Iteration 132/1000 | Loss: 0.00001030
Iteration 133/1000 | Loss: 0.00001030
Iteration 134/1000 | Loss: 0.00001030
Iteration 135/1000 | Loss: 0.00001030
Iteration 136/1000 | Loss: 0.00001030
Iteration 137/1000 | Loss: 0.00001030
Iteration 138/1000 | Loss: 0.00001030
Iteration 139/1000 | Loss: 0.00001030
Iteration 140/1000 | Loss: 0.00001030
Iteration 141/1000 | Loss: 0.00001030
Iteration 142/1000 | Loss: 0.00001030
Iteration 143/1000 | Loss: 0.00001030
Iteration 144/1000 | Loss: 0.00001030
Iteration 145/1000 | Loss: 0.00001030
Iteration 146/1000 | Loss: 0.00001030
Iteration 147/1000 | Loss: 0.00001029
Iteration 148/1000 | Loss: 0.00001029
Iteration 149/1000 | Loss: 0.00001029
Iteration 150/1000 | Loss: 0.00001029
Iteration 151/1000 | Loss: 0.00001029
Iteration 152/1000 | Loss: 0.00001029
Iteration 153/1000 | Loss: 0.00001029
Iteration 154/1000 | Loss: 0.00001029
Iteration 155/1000 | Loss: 0.00001029
Iteration 156/1000 | Loss: 0.00001029
Iteration 157/1000 | Loss: 0.00001029
Iteration 158/1000 | Loss: 0.00001029
Iteration 159/1000 | Loss: 0.00001029
Iteration 160/1000 | Loss: 0.00001029
Iteration 161/1000 | Loss: 0.00001028
Iteration 162/1000 | Loss: 0.00001028
Iteration 163/1000 | Loss: 0.00001028
Iteration 164/1000 | Loss: 0.00001028
Iteration 165/1000 | Loss: 0.00001028
Iteration 166/1000 | Loss: 0.00001028
Iteration 167/1000 | Loss: 0.00001028
Iteration 168/1000 | Loss: 0.00001028
Iteration 169/1000 | Loss: 0.00001028
Iteration 170/1000 | Loss: 0.00001028
Iteration 171/1000 | Loss: 0.00001028
Iteration 172/1000 | Loss: 0.00001028
Iteration 173/1000 | Loss: 0.00001028
Iteration 174/1000 | Loss: 0.00001028
Iteration 175/1000 | Loss: 0.00001028
Iteration 176/1000 | Loss: 0.00001028
Iteration 177/1000 | Loss: 0.00001028
Iteration 178/1000 | Loss: 0.00001028
Iteration 179/1000 | Loss: 0.00001028
Iteration 180/1000 | Loss: 0.00001028
Iteration 181/1000 | Loss: 0.00001027
Iteration 182/1000 | Loss: 0.00001027
Iteration 183/1000 | Loss: 0.00001027
Iteration 184/1000 | Loss: 0.00001027
Iteration 185/1000 | Loss: 0.00001027
Iteration 186/1000 | Loss: 0.00001027
Iteration 187/1000 | Loss: 0.00001027
Iteration 188/1000 | Loss: 0.00001027
Iteration 189/1000 | Loss: 0.00001027
Iteration 190/1000 | Loss: 0.00001027
Iteration 191/1000 | Loss: 0.00001027
Iteration 192/1000 | Loss: 0.00001027
Iteration 193/1000 | Loss: 0.00001026
Iteration 194/1000 | Loss: 0.00001026
Iteration 195/1000 | Loss: 0.00001026
Iteration 196/1000 | Loss: 0.00001026
Iteration 197/1000 | Loss: 0.00001026
Iteration 198/1000 | Loss: 0.00001026
Iteration 199/1000 | Loss: 0.00001026
Iteration 200/1000 | Loss: 0.00001026
Iteration 201/1000 | Loss: 0.00001026
Iteration 202/1000 | Loss: 0.00001026
Iteration 203/1000 | Loss: 0.00001026
Iteration 204/1000 | Loss: 0.00001026
Iteration 205/1000 | Loss: 0.00001026
Iteration 206/1000 | Loss: 0.00001026
Iteration 207/1000 | Loss: 0.00001026
Iteration 208/1000 | Loss: 0.00001026
Iteration 209/1000 | Loss: 0.00001026
Iteration 210/1000 | Loss: 0.00001025
Iteration 211/1000 | Loss: 0.00001025
Iteration 212/1000 | Loss: 0.00001025
Iteration 213/1000 | Loss: 0.00001025
Iteration 214/1000 | Loss: 0.00001025
Iteration 215/1000 | Loss: 0.00001025
Iteration 216/1000 | Loss: 0.00001025
Iteration 217/1000 | Loss: 0.00001025
Iteration 218/1000 | Loss: 0.00001025
Iteration 219/1000 | Loss: 0.00001024
Iteration 220/1000 | Loss: 0.00001024
Iteration 221/1000 | Loss: 0.00001024
Iteration 222/1000 | Loss: 0.00001024
Iteration 223/1000 | Loss: 0.00001024
Iteration 224/1000 | Loss: 0.00001024
Iteration 225/1000 | Loss: 0.00001024
Iteration 226/1000 | Loss: 0.00001024
Iteration 227/1000 | Loss: 0.00001024
Iteration 228/1000 | Loss: 0.00001024
Iteration 229/1000 | Loss: 0.00001024
Iteration 230/1000 | Loss: 0.00001024
Iteration 231/1000 | Loss: 0.00001024
Iteration 232/1000 | Loss: 0.00001024
Iteration 233/1000 | Loss: 0.00001024
Iteration 234/1000 | Loss: 0.00001024
Iteration 235/1000 | Loss: 0.00001024
Iteration 236/1000 | Loss: 0.00001024
Iteration 237/1000 | Loss: 0.00001024
Iteration 238/1000 | Loss: 0.00001024
Iteration 239/1000 | Loss: 0.00001024
Iteration 240/1000 | Loss: 0.00001024
Iteration 241/1000 | Loss: 0.00001024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.0239376933895983e-05, 1.0239376933895983e-05, 1.0239376933895983e-05, 1.0239376933895983e-05, 1.0239376933895983e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0239376933895983e-05

Optimization complete. Final v2v error: 2.6669068336486816 mm

Highest mean error: 3.252795696258545 mm for frame 54

Lowest mean error: 2.3361635208129883 mm for frame 106

Saving results

Total time: 36.29723381996155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955806
Iteration 2/25 | Loss: 0.00132502
Iteration 3/25 | Loss: 0.00104868
Iteration 4/25 | Loss: 0.00103166
Iteration 5/25 | Loss: 0.00102923
Iteration 6/25 | Loss: 0.00102907
Iteration 7/25 | Loss: 0.00102907
Iteration 8/25 | Loss: 0.00102907
Iteration 9/25 | Loss: 0.00102907
Iteration 10/25 | Loss: 0.00102907
Iteration 11/25 | Loss: 0.00102907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010290690697729588, 0.0010290690697729588, 0.0010290690697729588, 0.0010290690697729588, 0.0010290690697729588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010290690697729588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67185867
Iteration 2/25 | Loss: 0.00069136
Iteration 3/25 | Loss: 0.00069135
Iteration 4/25 | Loss: 0.00069135
Iteration 5/25 | Loss: 0.00069135
Iteration 6/25 | Loss: 0.00069135
Iteration 7/25 | Loss: 0.00069135
Iteration 8/25 | Loss: 0.00069135
Iteration 9/25 | Loss: 0.00069135
Iteration 10/25 | Loss: 0.00069135
Iteration 11/25 | Loss: 0.00069135
Iteration 12/25 | Loss: 0.00069135
Iteration 13/25 | Loss: 0.00069135
Iteration 14/25 | Loss: 0.00069135
Iteration 15/25 | Loss: 0.00069135
Iteration 16/25 | Loss: 0.00069135
Iteration 17/25 | Loss: 0.00069135
Iteration 18/25 | Loss: 0.00069135
Iteration 19/25 | Loss: 0.00069135
Iteration 20/25 | Loss: 0.00069135
Iteration 21/25 | Loss: 0.00069135
Iteration 22/25 | Loss: 0.00069135
Iteration 23/25 | Loss: 0.00069135
Iteration 24/25 | Loss: 0.00069135
Iteration 25/25 | Loss: 0.00069135
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006913455436006188, 0.0006913455436006188, 0.0006913455436006188, 0.0006913455436006188, 0.0006913455436006188]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006913455436006188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069135
Iteration 2/1000 | Loss: 0.00004218
Iteration 3/1000 | Loss: 0.00002445
Iteration 4/1000 | Loss: 0.00002042
Iteration 5/1000 | Loss: 0.00001926
Iteration 6/1000 | Loss: 0.00001860
Iteration 7/1000 | Loss: 0.00001821
Iteration 8/1000 | Loss: 0.00001794
Iteration 9/1000 | Loss: 0.00001767
Iteration 10/1000 | Loss: 0.00001743
Iteration 11/1000 | Loss: 0.00001742
Iteration 12/1000 | Loss: 0.00001739
Iteration 13/1000 | Loss: 0.00001726
Iteration 14/1000 | Loss: 0.00001725
Iteration 15/1000 | Loss: 0.00001723
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001722
Iteration 18/1000 | Loss: 0.00001722
Iteration 19/1000 | Loss: 0.00001722
Iteration 20/1000 | Loss: 0.00001721
Iteration 21/1000 | Loss: 0.00001720
Iteration 22/1000 | Loss: 0.00001720
Iteration 23/1000 | Loss: 0.00001719
Iteration 24/1000 | Loss: 0.00001719
Iteration 25/1000 | Loss: 0.00001717
Iteration 26/1000 | Loss: 0.00001717
Iteration 27/1000 | Loss: 0.00001717
Iteration 28/1000 | Loss: 0.00001717
Iteration 29/1000 | Loss: 0.00001717
Iteration 30/1000 | Loss: 0.00001717
Iteration 31/1000 | Loss: 0.00001717
Iteration 32/1000 | Loss: 0.00001717
Iteration 33/1000 | Loss: 0.00001717
Iteration 34/1000 | Loss: 0.00001716
Iteration 35/1000 | Loss: 0.00001716
Iteration 36/1000 | Loss: 0.00001716
Iteration 37/1000 | Loss: 0.00001716
Iteration 38/1000 | Loss: 0.00001716
Iteration 39/1000 | Loss: 0.00001715
Iteration 40/1000 | Loss: 0.00001715
Iteration 41/1000 | Loss: 0.00001714
Iteration 42/1000 | Loss: 0.00001714
Iteration 43/1000 | Loss: 0.00001714
Iteration 44/1000 | Loss: 0.00001714
Iteration 45/1000 | Loss: 0.00001714
Iteration 46/1000 | Loss: 0.00001713
Iteration 47/1000 | Loss: 0.00001713
Iteration 48/1000 | Loss: 0.00001713
Iteration 49/1000 | Loss: 0.00001712
Iteration 50/1000 | Loss: 0.00001712
Iteration 51/1000 | Loss: 0.00001711
Iteration 52/1000 | Loss: 0.00001711
Iteration 53/1000 | Loss: 0.00001711
Iteration 54/1000 | Loss: 0.00001710
Iteration 55/1000 | Loss: 0.00001710
Iteration 56/1000 | Loss: 0.00001710
Iteration 57/1000 | Loss: 0.00001709
Iteration 58/1000 | Loss: 0.00001709
Iteration 59/1000 | Loss: 0.00001709
Iteration 60/1000 | Loss: 0.00001708
Iteration 61/1000 | Loss: 0.00001708
Iteration 62/1000 | Loss: 0.00001708
Iteration 63/1000 | Loss: 0.00001708
Iteration 64/1000 | Loss: 0.00001708
Iteration 65/1000 | Loss: 0.00001708
Iteration 66/1000 | Loss: 0.00001708
Iteration 67/1000 | Loss: 0.00001708
Iteration 68/1000 | Loss: 0.00001708
Iteration 69/1000 | Loss: 0.00001707
Iteration 70/1000 | Loss: 0.00001707
Iteration 71/1000 | Loss: 0.00001707
Iteration 72/1000 | Loss: 0.00001707
Iteration 73/1000 | Loss: 0.00001706
Iteration 74/1000 | Loss: 0.00001706
Iteration 75/1000 | Loss: 0.00001706
Iteration 76/1000 | Loss: 0.00001706
Iteration 77/1000 | Loss: 0.00001706
Iteration 78/1000 | Loss: 0.00001706
Iteration 79/1000 | Loss: 0.00001706
Iteration 80/1000 | Loss: 0.00001706
Iteration 81/1000 | Loss: 0.00001706
Iteration 82/1000 | Loss: 0.00001706
Iteration 83/1000 | Loss: 0.00001706
Iteration 84/1000 | Loss: 0.00001705
Iteration 85/1000 | Loss: 0.00001705
Iteration 86/1000 | Loss: 0.00001705
Iteration 87/1000 | Loss: 0.00001705
Iteration 88/1000 | Loss: 0.00001705
Iteration 89/1000 | Loss: 0.00001705
Iteration 90/1000 | Loss: 0.00001705
Iteration 91/1000 | Loss: 0.00001705
Iteration 92/1000 | Loss: 0.00001705
Iteration 93/1000 | Loss: 0.00001705
Iteration 94/1000 | Loss: 0.00001704
Iteration 95/1000 | Loss: 0.00001704
Iteration 96/1000 | Loss: 0.00001704
Iteration 97/1000 | Loss: 0.00001704
Iteration 98/1000 | Loss: 0.00001704
Iteration 99/1000 | Loss: 0.00001704
Iteration 100/1000 | Loss: 0.00001704
Iteration 101/1000 | Loss: 0.00001704
Iteration 102/1000 | Loss: 0.00001704
Iteration 103/1000 | Loss: 0.00001704
Iteration 104/1000 | Loss: 0.00001704
Iteration 105/1000 | Loss: 0.00001704
Iteration 106/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.7037036741385236e-05, 1.7037036741385236e-05, 1.7037036741385236e-05, 1.7037036741385236e-05, 1.7037036741385236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7037036741385236e-05

Optimization complete. Final v2v error: 3.424485683441162 mm

Highest mean error: 4.409165859222412 mm for frame 5

Lowest mean error: 2.7348544597625732 mm for frame 225

Saving results

Total time: 34.338149547576904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382840
Iteration 2/25 | Loss: 0.00098289
Iteration 3/25 | Loss: 0.00089308
Iteration 4/25 | Loss: 0.00088100
Iteration 5/25 | Loss: 0.00087784
Iteration 6/25 | Loss: 0.00087678
Iteration 7/25 | Loss: 0.00087678
Iteration 8/25 | Loss: 0.00087678
Iteration 9/25 | Loss: 0.00087678
Iteration 10/25 | Loss: 0.00087678
Iteration 11/25 | Loss: 0.00087678
Iteration 12/25 | Loss: 0.00087678
Iteration 13/25 | Loss: 0.00087678
Iteration 14/25 | Loss: 0.00087678
Iteration 15/25 | Loss: 0.00087678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008767816470935941, 0.0008767816470935941, 0.0008767816470935941, 0.0008767816470935941, 0.0008767816470935941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008767816470935941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39212739
Iteration 2/25 | Loss: 0.00066368
Iteration 3/25 | Loss: 0.00066368
Iteration 4/25 | Loss: 0.00066368
Iteration 5/25 | Loss: 0.00066368
Iteration 6/25 | Loss: 0.00066368
Iteration 7/25 | Loss: 0.00066368
Iteration 8/25 | Loss: 0.00066368
Iteration 9/25 | Loss: 0.00066368
Iteration 10/25 | Loss: 0.00066368
Iteration 11/25 | Loss: 0.00066368
Iteration 12/25 | Loss: 0.00066368
Iteration 13/25 | Loss: 0.00066368
Iteration 14/25 | Loss: 0.00066368
Iteration 15/25 | Loss: 0.00066368
Iteration 16/25 | Loss: 0.00066368
Iteration 17/25 | Loss: 0.00066368
Iteration 18/25 | Loss: 0.00066368
Iteration 19/25 | Loss: 0.00066368
Iteration 20/25 | Loss: 0.00066368
Iteration 21/25 | Loss: 0.00066368
Iteration 22/25 | Loss: 0.00066368
Iteration 23/25 | Loss: 0.00066368
Iteration 24/25 | Loss: 0.00066368
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000663678627461195, 0.000663678627461195, 0.000663678627461195, 0.000663678627461195, 0.000663678627461195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000663678627461195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066368
Iteration 2/1000 | Loss: 0.00001888
Iteration 3/1000 | Loss: 0.00001234
Iteration 4/1000 | Loss: 0.00001113
Iteration 5/1000 | Loss: 0.00001060
Iteration 6/1000 | Loss: 0.00001017
Iteration 7/1000 | Loss: 0.00001014
Iteration 8/1000 | Loss: 0.00001008
Iteration 9/1000 | Loss: 0.00000990
Iteration 10/1000 | Loss: 0.00000970
Iteration 11/1000 | Loss: 0.00000969
Iteration 12/1000 | Loss: 0.00000966
Iteration 13/1000 | Loss: 0.00000964
Iteration 14/1000 | Loss: 0.00000964
Iteration 15/1000 | Loss: 0.00000964
Iteration 16/1000 | Loss: 0.00000964
Iteration 17/1000 | Loss: 0.00000964
Iteration 18/1000 | Loss: 0.00000963
Iteration 19/1000 | Loss: 0.00000962
Iteration 20/1000 | Loss: 0.00000961
Iteration 21/1000 | Loss: 0.00000960
Iteration 22/1000 | Loss: 0.00000959
Iteration 23/1000 | Loss: 0.00000959
Iteration 24/1000 | Loss: 0.00000959
Iteration 25/1000 | Loss: 0.00000959
Iteration 26/1000 | Loss: 0.00000958
Iteration 27/1000 | Loss: 0.00000958
Iteration 28/1000 | Loss: 0.00000958
Iteration 29/1000 | Loss: 0.00000956
Iteration 30/1000 | Loss: 0.00000956
Iteration 31/1000 | Loss: 0.00000956
Iteration 32/1000 | Loss: 0.00000956
Iteration 33/1000 | Loss: 0.00000956
Iteration 34/1000 | Loss: 0.00000956
Iteration 35/1000 | Loss: 0.00000956
Iteration 36/1000 | Loss: 0.00000956
Iteration 37/1000 | Loss: 0.00000956
Iteration 38/1000 | Loss: 0.00000956
Iteration 39/1000 | Loss: 0.00000956
Iteration 40/1000 | Loss: 0.00000956
Iteration 41/1000 | Loss: 0.00000955
Iteration 42/1000 | Loss: 0.00000955
Iteration 43/1000 | Loss: 0.00000954
Iteration 44/1000 | Loss: 0.00000954
Iteration 45/1000 | Loss: 0.00000953
Iteration 46/1000 | Loss: 0.00000953
Iteration 47/1000 | Loss: 0.00000952
Iteration 48/1000 | Loss: 0.00000951
Iteration 49/1000 | Loss: 0.00000951
Iteration 50/1000 | Loss: 0.00000950
Iteration 51/1000 | Loss: 0.00000950
Iteration 52/1000 | Loss: 0.00000949
Iteration 53/1000 | Loss: 0.00000949
Iteration 54/1000 | Loss: 0.00000949
Iteration 55/1000 | Loss: 0.00000949
Iteration 56/1000 | Loss: 0.00000949
Iteration 57/1000 | Loss: 0.00000949
Iteration 58/1000 | Loss: 0.00000949
Iteration 59/1000 | Loss: 0.00000949
Iteration 60/1000 | Loss: 0.00000949
Iteration 61/1000 | Loss: 0.00000949
Iteration 62/1000 | Loss: 0.00000948
Iteration 63/1000 | Loss: 0.00000947
Iteration 64/1000 | Loss: 0.00000947
Iteration 65/1000 | Loss: 0.00000947
Iteration 66/1000 | Loss: 0.00000947
Iteration 67/1000 | Loss: 0.00000947
Iteration 68/1000 | Loss: 0.00000947
Iteration 69/1000 | Loss: 0.00000947
Iteration 70/1000 | Loss: 0.00000947
Iteration 71/1000 | Loss: 0.00000947
Iteration 72/1000 | Loss: 0.00000947
Iteration 73/1000 | Loss: 0.00000946
Iteration 74/1000 | Loss: 0.00000946
Iteration 75/1000 | Loss: 0.00000946
Iteration 76/1000 | Loss: 0.00000946
Iteration 77/1000 | Loss: 0.00000946
Iteration 78/1000 | Loss: 0.00000946
Iteration 79/1000 | Loss: 0.00000946
Iteration 80/1000 | Loss: 0.00000946
Iteration 81/1000 | Loss: 0.00000946
Iteration 82/1000 | Loss: 0.00000946
Iteration 83/1000 | Loss: 0.00000946
Iteration 84/1000 | Loss: 0.00000946
Iteration 85/1000 | Loss: 0.00000946
Iteration 86/1000 | Loss: 0.00000946
Iteration 87/1000 | Loss: 0.00000945
Iteration 88/1000 | Loss: 0.00000945
Iteration 89/1000 | Loss: 0.00000945
Iteration 90/1000 | Loss: 0.00000945
Iteration 91/1000 | Loss: 0.00000945
Iteration 92/1000 | Loss: 0.00000945
Iteration 93/1000 | Loss: 0.00000945
Iteration 94/1000 | Loss: 0.00000945
Iteration 95/1000 | Loss: 0.00000945
Iteration 96/1000 | Loss: 0.00000945
Iteration 97/1000 | Loss: 0.00000945
Iteration 98/1000 | Loss: 0.00000945
Iteration 99/1000 | Loss: 0.00000945
Iteration 100/1000 | Loss: 0.00000945
Iteration 101/1000 | Loss: 0.00000945
Iteration 102/1000 | Loss: 0.00000945
Iteration 103/1000 | Loss: 0.00000945
Iteration 104/1000 | Loss: 0.00000945
Iteration 105/1000 | Loss: 0.00000945
Iteration 106/1000 | Loss: 0.00000945
Iteration 107/1000 | Loss: 0.00000944
Iteration 108/1000 | Loss: 0.00000944
Iteration 109/1000 | Loss: 0.00000944
Iteration 110/1000 | Loss: 0.00000944
Iteration 111/1000 | Loss: 0.00000944
Iteration 112/1000 | Loss: 0.00000944
Iteration 113/1000 | Loss: 0.00000944
Iteration 114/1000 | Loss: 0.00000944
Iteration 115/1000 | Loss: 0.00000944
Iteration 116/1000 | Loss: 0.00000944
Iteration 117/1000 | Loss: 0.00000944
Iteration 118/1000 | Loss: 0.00000944
Iteration 119/1000 | Loss: 0.00000944
Iteration 120/1000 | Loss: 0.00000944
Iteration 121/1000 | Loss: 0.00000944
Iteration 122/1000 | Loss: 0.00000944
Iteration 123/1000 | Loss: 0.00000944
Iteration 124/1000 | Loss: 0.00000944
Iteration 125/1000 | Loss: 0.00000944
Iteration 126/1000 | Loss: 0.00000944
Iteration 127/1000 | Loss: 0.00000944
Iteration 128/1000 | Loss: 0.00000944
Iteration 129/1000 | Loss: 0.00000944
Iteration 130/1000 | Loss: 0.00000944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [9.443931048735976e-06, 9.443931048735976e-06, 9.443931048735976e-06, 9.443931048735976e-06, 9.443931048735976e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.443931048735976e-06

Optimization complete. Final v2v error: 2.6377127170562744 mm

Highest mean error: 2.894531726837158 mm for frame 104

Lowest mean error: 2.386502504348755 mm for frame 141

Saving results

Total time: 32.25183320045471
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381397
Iteration 2/25 | Loss: 0.00105621
Iteration 3/25 | Loss: 0.00095152
Iteration 4/25 | Loss: 0.00094122
Iteration 5/25 | Loss: 0.00093860
Iteration 6/25 | Loss: 0.00093755
Iteration 7/25 | Loss: 0.00093738
Iteration 8/25 | Loss: 0.00093738
Iteration 9/25 | Loss: 0.00093738
Iteration 10/25 | Loss: 0.00093738
Iteration 11/25 | Loss: 0.00093738
Iteration 12/25 | Loss: 0.00093738
Iteration 13/25 | Loss: 0.00093738
Iteration 14/25 | Loss: 0.00093738
Iteration 15/25 | Loss: 0.00093738
Iteration 16/25 | Loss: 0.00093738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009373790817335248, 0.0009373790817335248, 0.0009373790817335248, 0.0009373790817335248, 0.0009373790817335248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009373790817335248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47590327
Iteration 2/25 | Loss: 0.00077692
Iteration 3/25 | Loss: 0.00077692
Iteration 4/25 | Loss: 0.00077692
Iteration 5/25 | Loss: 0.00077692
Iteration 6/25 | Loss: 0.00077692
Iteration 7/25 | Loss: 0.00077692
Iteration 8/25 | Loss: 0.00077692
Iteration 9/25 | Loss: 0.00077692
Iteration 10/25 | Loss: 0.00077692
Iteration 11/25 | Loss: 0.00077692
Iteration 12/25 | Loss: 0.00077692
Iteration 13/25 | Loss: 0.00077692
Iteration 14/25 | Loss: 0.00077692
Iteration 15/25 | Loss: 0.00077692
Iteration 16/25 | Loss: 0.00077692
Iteration 17/25 | Loss: 0.00077692
Iteration 18/25 | Loss: 0.00077692
Iteration 19/25 | Loss: 0.00077692
Iteration 20/25 | Loss: 0.00077692
Iteration 21/25 | Loss: 0.00077692
Iteration 22/25 | Loss: 0.00077692
Iteration 23/25 | Loss: 0.00077692
Iteration 24/25 | Loss: 0.00077692
Iteration 25/25 | Loss: 0.00077692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077692
Iteration 2/1000 | Loss: 0.00004889
Iteration 3/1000 | Loss: 0.00002783
Iteration 4/1000 | Loss: 0.00001816
Iteration 5/1000 | Loss: 0.00001602
Iteration 6/1000 | Loss: 0.00001476
Iteration 7/1000 | Loss: 0.00001399
Iteration 8/1000 | Loss: 0.00001351
Iteration 9/1000 | Loss: 0.00001299
Iteration 10/1000 | Loss: 0.00001275
Iteration 11/1000 | Loss: 0.00001251
Iteration 12/1000 | Loss: 0.00001242
Iteration 13/1000 | Loss: 0.00001222
Iteration 14/1000 | Loss: 0.00001218
Iteration 15/1000 | Loss: 0.00001217
Iteration 16/1000 | Loss: 0.00001216
Iteration 17/1000 | Loss: 0.00001211
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001209
Iteration 20/1000 | Loss: 0.00001208
Iteration 21/1000 | Loss: 0.00001207
Iteration 22/1000 | Loss: 0.00001204
Iteration 23/1000 | Loss: 0.00001199
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001196
Iteration 27/1000 | Loss: 0.00001195
Iteration 28/1000 | Loss: 0.00001195
Iteration 29/1000 | Loss: 0.00001195
Iteration 30/1000 | Loss: 0.00001194
Iteration 31/1000 | Loss: 0.00001194
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001193
Iteration 34/1000 | Loss: 0.00001193
Iteration 35/1000 | Loss: 0.00001193
Iteration 36/1000 | Loss: 0.00001192
Iteration 37/1000 | Loss: 0.00001192
Iteration 38/1000 | Loss: 0.00001192
Iteration 39/1000 | Loss: 0.00001191
Iteration 40/1000 | Loss: 0.00001191
Iteration 41/1000 | Loss: 0.00001191
Iteration 42/1000 | Loss: 0.00001190
Iteration 43/1000 | Loss: 0.00001189
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001188
Iteration 46/1000 | Loss: 0.00001188
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001188
Iteration 49/1000 | Loss: 0.00001187
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001187
Iteration 52/1000 | Loss: 0.00001186
Iteration 53/1000 | Loss: 0.00001186
Iteration 54/1000 | Loss: 0.00001186
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001186
Iteration 57/1000 | Loss: 0.00001185
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001185
Iteration 60/1000 | Loss: 0.00001185
Iteration 61/1000 | Loss: 0.00001185
Iteration 62/1000 | Loss: 0.00001185
Iteration 63/1000 | Loss: 0.00001185
Iteration 64/1000 | Loss: 0.00001184
Iteration 65/1000 | Loss: 0.00001184
Iteration 66/1000 | Loss: 0.00001184
Iteration 67/1000 | Loss: 0.00001184
Iteration 68/1000 | Loss: 0.00001184
Iteration 69/1000 | Loss: 0.00001184
Iteration 70/1000 | Loss: 0.00001184
Iteration 71/1000 | Loss: 0.00001184
Iteration 72/1000 | Loss: 0.00001184
Iteration 73/1000 | Loss: 0.00001184
Iteration 74/1000 | Loss: 0.00001184
Iteration 75/1000 | Loss: 0.00001184
Iteration 76/1000 | Loss: 0.00001183
Iteration 77/1000 | Loss: 0.00001183
Iteration 78/1000 | Loss: 0.00001183
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001183
Iteration 82/1000 | Loss: 0.00001183
Iteration 83/1000 | Loss: 0.00001183
Iteration 84/1000 | Loss: 0.00001183
Iteration 85/1000 | Loss: 0.00001183
Iteration 86/1000 | Loss: 0.00001183
Iteration 87/1000 | Loss: 0.00001183
Iteration 88/1000 | Loss: 0.00001183
Iteration 89/1000 | Loss: 0.00001183
Iteration 90/1000 | Loss: 0.00001183
Iteration 91/1000 | Loss: 0.00001182
Iteration 92/1000 | Loss: 0.00001182
Iteration 93/1000 | Loss: 0.00001182
Iteration 94/1000 | Loss: 0.00001182
Iteration 95/1000 | Loss: 0.00001182
Iteration 96/1000 | Loss: 0.00001182
Iteration 97/1000 | Loss: 0.00001182
Iteration 98/1000 | Loss: 0.00001182
Iteration 99/1000 | Loss: 0.00001182
Iteration 100/1000 | Loss: 0.00001182
Iteration 101/1000 | Loss: 0.00001182
Iteration 102/1000 | Loss: 0.00001182
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001182
Iteration 105/1000 | Loss: 0.00001182
Iteration 106/1000 | Loss: 0.00001182
Iteration 107/1000 | Loss: 0.00001182
Iteration 108/1000 | Loss: 0.00001182
Iteration 109/1000 | Loss: 0.00001181
Iteration 110/1000 | Loss: 0.00001181
Iteration 111/1000 | Loss: 0.00001181
Iteration 112/1000 | Loss: 0.00001181
Iteration 113/1000 | Loss: 0.00001181
Iteration 114/1000 | Loss: 0.00001181
Iteration 115/1000 | Loss: 0.00001181
Iteration 116/1000 | Loss: 0.00001181
Iteration 117/1000 | Loss: 0.00001181
Iteration 118/1000 | Loss: 0.00001181
Iteration 119/1000 | Loss: 0.00001181
Iteration 120/1000 | Loss: 0.00001181
Iteration 121/1000 | Loss: 0.00001181
Iteration 122/1000 | Loss: 0.00001181
Iteration 123/1000 | Loss: 0.00001181
Iteration 124/1000 | Loss: 0.00001181
Iteration 125/1000 | Loss: 0.00001181
Iteration 126/1000 | Loss: 0.00001181
Iteration 127/1000 | Loss: 0.00001181
Iteration 128/1000 | Loss: 0.00001181
Iteration 129/1000 | Loss: 0.00001181
Iteration 130/1000 | Loss: 0.00001181
Iteration 131/1000 | Loss: 0.00001181
Iteration 132/1000 | Loss: 0.00001181
Iteration 133/1000 | Loss: 0.00001181
Iteration 134/1000 | Loss: 0.00001180
Iteration 135/1000 | Loss: 0.00001180
Iteration 136/1000 | Loss: 0.00001180
Iteration 137/1000 | Loss: 0.00001180
Iteration 138/1000 | Loss: 0.00001180
Iteration 139/1000 | Loss: 0.00001180
Iteration 140/1000 | Loss: 0.00001180
Iteration 141/1000 | Loss: 0.00001180
Iteration 142/1000 | Loss: 0.00001180
Iteration 143/1000 | Loss: 0.00001180
Iteration 144/1000 | Loss: 0.00001180
Iteration 145/1000 | Loss: 0.00001180
Iteration 146/1000 | Loss: 0.00001180
Iteration 147/1000 | Loss: 0.00001180
Iteration 148/1000 | Loss: 0.00001180
Iteration 149/1000 | Loss: 0.00001180
Iteration 150/1000 | Loss: 0.00001180
Iteration 151/1000 | Loss: 0.00001180
Iteration 152/1000 | Loss: 0.00001180
Iteration 153/1000 | Loss: 0.00001180
Iteration 154/1000 | Loss: 0.00001180
Iteration 155/1000 | Loss: 0.00001180
Iteration 156/1000 | Loss: 0.00001180
Iteration 157/1000 | Loss: 0.00001180
Iteration 158/1000 | Loss: 0.00001180
Iteration 159/1000 | Loss: 0.00001180
Iteration 160/1000 | Loss: 0.00001180
Iteration 161/1000 | Loss: 0.00001180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.1804715541074984e-05, 1.1804715541074984e-05, 1.1804715541074984e-05, 1.1804715541074984e-05, 1.1804715541074984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1804715541074984e-05

Optimization complete. Final v2v error: 2.9490599632263184 mm

Highest mean error: 3.9054620265960693 mm for frame 61

Lowest mean error: 2.6115574836730957 mm for frame 31

Saving results

Total time: 36.53726553916931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01149862
Iteration 2/25 | Loss: 0.01149862
Iteration 3/25 | Loss: 0.01149862
Iteration 4/25 | Loss: 0.01149862
Iteration 5/25 | Loss: 0.01149862
Iteration 6/25 | Loss: 0.01149861
Iteration 7/25 | Loss: 0.01149861
Iteration 8/25 | Loss: 0.01149861
Iteration 9/25 | Loss: 0.01149861
Iteration 10/25 | Loss: 0.01149861
Iteration 11/25 | Loss: 0.01149861
Iteration 12/25 | Loss: 0.01149861
Iteration 13/25 | Loss: 0.01149861
Iteration 14/25 | Loss: 0.01149861
Iteration 15/25 | Loss: 0.01149861
Iteration 16/25 | Loss: 0.01149861
Iteration 17/25 | Loss: 0.01149861
Iteration 18/25 | Loss: 0.01149861
Iteration 19/25 | Loss: 0.01149861
Iteration 20/25 | Loss: 0.01149861
Iteration 21/25 | Loss: 0.01149860
Iteration 22/25 | Loss: 0.01149860
Iteration 23/25 | Loss: 0.01149860
Iteration 24/25 | Loss: 0.01149860
Iteration 25/25 | Loss: 0.01149860

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.56145573
Iteration 2/25 | Loss: 0.18860447
Iteration 3/25 | Loss: 0.18621600
Iteration 4/25 | Loss: 0.18534037
Iteration 5/25 | Loss: 0.18521109
Iteration 6/25 | Loss: 0.18521105
Iteration 7/25 | Loss: 0.18521105
Iteration 8/25 | Loss: 0.18521102
Iteration 9/25 | Loss: 0.18521103
Iteration 10/25 | Loss: 0.18521103
Iteration 11/25 | Loss: 0.18521102
Iteration 12/25 | Loss: 0.18521102
Iteration 13/25 | Loss: 0.18521102
Iteration 14/25 | Loss: 0.18521099
Iteration 15/25 | Loss: 0.18521099
Iteration 16/25 | Loss: 0.18521099
Iteration 17/25 | Loss: 0.18521099
Iteration 18/25 | Loss: 0.18521099
Iteration 19/25 | Loss: 0.18521099
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.18521098792552948, 0.18521098792552948, 0.18521098792552948, 0.18521098792552948, 0.18521098792552948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18521098792552948

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18521099
Iteration 2/1000 | Loss: 0.00525564
Iteration 3/1000 | Loss: 0.00117721
Iteration 4/1000 | Loss: 0.00077427
Iteration 5/1000 | Loss: 0.00035691
Iteration 6/1000 | Loss: 0.00041512
Iteration 7/1000 | Loss: 0.00015901
Iteration 8/1000 | Loss: 0.00010572
Iteration 9/1000 | Loss: 0.00005723
Iteration 10/1000 | Loss: 0.00013001
Iteration 11/1000 | Loss: 0.00005883
Iteration 12/1000 | Loss: 0.00015542
Iteration 13/1000 | Loss: 0.00003704
Iteration 14/1000 | Loss: 0.00006993
Iteration 15/1000 | Loss: 0.00013876
Iteration 16/1000 | Loss: 0.00003111
Iteration 17/1000 | Loss: 0.00010904
Iteration 18/1000 | Loss: 0.00004605
Iteration 19/1000 | Loss: 0.00003710
Iteration 20/1000 | Loss: 0.00024299
Iteration 21/1000 | Loss: 0.00005940
Iteration 22/1000 | Loss: 0.00007824
Iteration 23/1000 | Loss: 0.00002505
Iteration 24/1000 | Loss: 0.00002375
Iteration 25/1000 | Loss: 0.00011218
Iteration 26/1000 | Loss: 0.00013350
Iteration 27/1000 | Loss: 0.00005833
Iteration 28/1000 | Loss: 0.00009365
Iteration 29/1000 | Loss: 0.00003330
Iteration 30/1000 | Loss: 0.00003317
Iteration 31/1000 | Loss: 0.00008144
Iteration 32/1000 | Loss: 0.00005281
Iteration 33/1000 | Loss: 0.00002547
Iteration 34/1000 | Loss: 0.00004107
Iteration 35/1000 | Loss: 0.00014685
Iteration 36/1000 | Loss: 0.00002485
Iteration 37/1000 | Loss: 0.00005596
Iteration 38/1000 | Loss: 0.00002057
Iteration 39/1000 | Loss: 0.00005268
Iteration 40/1000 | Loss: 0.00005038
Iteration 41/1000 | Loss: 0.00006542
Iteration 42/1000 | Loss: 0.00002117
Iteration 43/1000 | Loss: 0.00004245
Iteration 44/1000 | Loss: 0.00005612
Iteration 45/1000 | Loss: 0.00003218
Iteration 46/1000 | Loss: 0.00002004
Iteration 47/1000 | Loss: 0.00003951
Iteration 48/1000 | Loss: 0.00007152
Iteration 49/1000 | Loss: 0.00003043
Iteration 50/1000 | Loss: 0.00002791
Iteration 51/1000 | Loss: 0.00002382
Iteration 52/1000 | Loss: 0.00002674
Iteration 53/1000 | Loss: 0.00001921
Iteration 54/1000 | Loss: 0.00002630
Iteration 55/1000 | Loss: 0.00010256
Iteration 56/1000 | Loss: 0.00003183
Iteration 57/1000 | Loss: 0.00001921
Iteration 58/1000 | Loss: 0.00004022
Iteration 59/1000 | Loss: 0.00005983
Iteration 60/1000 | Loss: 0.00004538
Iteration 61/1000 | Loss: 0.00001910
Iteration 62/1000 | Loss: 0.00001883
Iteration 63/1000 | Loss: 0.00001880
Iteration 64/1000 | Loss: 0.00001880
Iteration 65/1000 | Loss: 0.00001879
Iteration 66/1000 | Loss: 0.00004381
Iteration 67/1000 | Loss: 0.00001882
Iteration 68/1000 | Loss: 0.00001877
Iteration 69/1000 | Loss: 0.00001876
Iteration 70/1000 | Loss: 0.00001876
Iteration 71/1000 | Loss: 0.00001875
Iteration 72/1000 | Loss: 0.00001875
Iteration 73/1000 | Loss: 0.00001875
Iteration 74/1000 | Loss: 0.00001875
Iteration 75/1000 | Loss: 0.00001874
Iteration 76/1000 | Loss: 0.00001874
Iteration 77/1000 | Loss: 0.00001874
Iteration 78/1000 | Loss: 0.00001874
Iteration 79/1000 | Loss: 0.00001873
Iteration 80/1000 | Loss: 0.00001873
Iteration 81/1000 | Loss: 0.00001873
Iteration 82/1000 | Loss: 0.00001873
Iteration 83/1000 | Loss: 0.00001873
Iteration 84/1000 | Loss: 0.00001873
Iteration 85/1000 | Loss: 0.00001873
Iteration 86/1000 | Loss: 0.00001873
Iteration 87/1000 | Loss: 0.00001873
Iteration 88/1000 | Loss: 0.00001873
Iteration 89/1000 | Loss: 0.00001873
Iteration 90/1000 | Loss: 0.00001873
Iteration 91/1000 | Loss: 0.00001872
Iteration 92/1000 | Loss: 0.00001872
Iteration 93/1000 | Loss: 0.00001872
Iteration 94/1000 | Loss: 0.00001872
Iteration 95/1000 | Loss: 0.00001872
Iteration 96/1000 | Loss: 0.00001872
Iteration 97/1000 | Loss: 0.00001872
Iteration 98/1000 | Loss: 0.00001872
Iteration 99/1000 | Loss: 0.00001872
Iteration 100/1000 | Loss: 0.00001872
Iteration 101/1000 | Loss: 0.00001872
Iteration 102/1000 | Loss: 0.00001871
Iteration 103/1000 | Loss: 0.00001871
Iteration 104/1000 | Loss: 0.00001871
Iteration 105/1000 | Loss: 0.00001871
Iteration 106/1000 | Loss: 0.00001871
Iteration 107/1000 | Loss: 0.00001871
Iteration 108/1000 | Loss: 0.00001871
Iteration 109/1000 | Loss: 0.00001871
Iteration 110/1000 | Loss: 0.00001871
Iteration 111/1000 | Loss: 0.00001871
Iteration 112/1000 | Loss: 0.00001871
Iteration 113/1000 | Loss: 0.00001871
Iteration 114/1000 | Loss: 0.00001871
Iteration 115/1000 | Loss: 0.00001871
Iteration 116/1000 | Loss: 0.00001871
Iteration 117/1000 | Loss: 0.00001871
Iteration 118/1000 | Loss: 0.00001871
Iteration 119/1000 | Loss: 0.00001871
Iteration 120/1000 | Loss: 0.00001871
Iteration 121/1000 | Loss: 0.00001871
Iteration 122/1000 | Loss: 0.00001871
Iteration 123/1000 | Loss: 0.00001871
Iteration 124/1000 | Loss: 0.00001871
Iteration 125/1000 | Loss: 0.00001871
Iteration 126/1000 | Loss: 0.00001871
Iteration 127/1000 | Loss: 0.00001871
Iteration 128/1000 | Loss: 0.00001871
Iteration 129/1000 | Loss: 0.00001871
Iteration 130/1000 | Loss: 0.00001871
Iteration 131/1000 | Loss: 0.00001871
Iteration 132/1000 | Loss: 0.00001871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.870534470072016e-05, 1.870534470072016e-05, 1.870534470072016e-05, 1.870534470072016e-05, 1.870534470072016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.870534470072016e-05

Optimization complete. Final v2v error: 3.4818263053894043 mm

Highest mean error: 10.657185554504395 mm for frame 11

Lowest mean error: 3.0704965591430664 mm for frame 211

Saving results

Total time: 112.57389068603516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829740
Iteration 2/25 | Loss: 0.00114278
Iteration 3/25 | Loss: 0.00100404
Iteration 4/25 | Loss: 0.00099257
Iteration 5/25 | Loss: 0.00099002
Iteration 6/25 | Loss: 0.00099002
Iteration 7/25 | Loss: 0.00099002
Iteration 8/25 | Loss: 0.00099002
Iteration 9/25 | Loss: 0.00099002
Iteration 10/25 | Loss: 0.00099002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009900183649733663, 0.0009900183649733663, 0.0009900183649733663, 0.0009900183649733663, 0.0009900183649733663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009900183649733663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26634228
Iteration 2/25 | Loss: 0.00048931
Iteration 3/25 | Loss: 0.00048925
Iteration 4/25 | Loss: 0.00048925
Iteration 5/25 | Loss: 0.00048925
Iteration 6/25 | Loss: 0.00048925
Iteration 7/25 | Loss: 0.00048925
Iteration 8/25 | Loss: 0.00048925
Iteration 9/25 | Loss: 0.00048925
Iteration 10/25 | Loss: 0.00048925
Iteration 11/25 | Loss: 0.00048925
Iteration 12/25 | Loss: 0.00048925
Iteration 13/25 | Loss: 0.00048925
Iteration 14/25 | Loss: 0.00048925
Iteration 15/25 | Loss: 0.00048925
Iteration 16/25 | Loss: 0.00048925
Iteration 17/25 | Loss: 0.00048925
Iteration 18/25 | Loss: 0.00048925
Iteration 19/25 | Loss: 0.00048925
Iteration 20/25 | Loss: 0.00048925
Iteration 21/25 | Loss: 0.00048925
Iteration 22/25 | Loss: 0.00048925
Iteration 23/25 | Loss: 0.00048925
Iteration 24/25 | Loss: 0.00048925
Iteration 25/25 | Loss: 0.00048925
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0004892501165159047, 0.0004892501165159047, 0.0004892501165159047, 0.0004892501165159047, 0.0004892501165159047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004892501165159047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048925
Iteration 2/1000 | Loss: 0.00003994
Iteration 3/1000 | Loss: 0.00002720
Iteration 4/1000 | Loss: 0.00002505
Iteration 5/1000 | Loss: 0.00002366
Iteration 6/1000 | Loss: 0.00002262
Iteration 7/1000 | Loss: 0.00002187
Iteration 8/1000 | Loss: 0.00002142
Iteration 9/1000 | Loss: 0.00002102
Iteration 10/1000 | Loss: 0.00002073
Iteration 11/1000 | Loss: 0.00002069
Iteration 12/1000 | Loss: 0.00002063
Iteration 13/1000 | Loss: 0.00002062
Iteration 14/1000 | Loss: 0.00002052
Iteration 15/1000 | Loss: 0.00002045
Iteration 16/1000 | Loss: 0.00002036
Iteration 17/1000 | Loss: 0.00002034
Iteration 18/1000 | Loss: 0.00002032
Iteration 19/1000 | Loss: 0.00002031
Iteration 20/1000 | Loss: 0.00002031
Iteration 21/1000 | Loss: 0.00002029
Iteration 22/1000 | Loss: 0.00002028
Iteration 23/1000 | Loss: 0.00002028
Iteration 24/1000 | Loss: 0.00002027
Iteration 25/1000 | Loss: 0.00002025
Iteration 26/1000 | Loss: 0.00002025
Iteration 27/1000 | Loss: 0.00002025
Iteration 28/1000 | Loss: 0.00002025
Iteration 29/1000 | Loss: 0.00002025
Iteration 30/1000 | Loss: 0.00002025
Iteration 31/1000 | Loss: 0.00002025
Iteration 32/1000 | Loss: 0.00002025
Iteration 33/1000 | Loss: 0.00002025
Iteration 34/1000 | Loss: 0.00002025
Iteration 35/1000 | Loss: 0.00002024
Iteration 36/1000 | Loss: 0.00002024
Iteration 37/1000 | Loss: 0.00002023
Iteration 38/1000 | Loss: 0.00002023
Iteration 39/1000 | Loss: 0.00002022
Iteration 40/1000 | Loss: 0.00002021
Iteration 41/1000 | Loss: 0.00002021
Iteration 42/1000 | Loss: 0.00002021
Iteration 43/1000 | Loss: 0.00002021
Iteration 44/1000 | Loss: 0.00002021
Iteration 45/1000 | Loss: 0.00002021
Iteration 46/1000 | Loss: 0.00002020
Iteration 47/1000 | Loss: 0.00002020
Iteration 48/1000 | Loss: 0.00002020
Iteration 49/1000 | Loss: 0.00002020
Iteration 50/1000 | Loss: 0.00002020
Iteration 51/1000 | Loss: 0.00002020
Iteration 52/1000 | Loss: 0.00002019
Iteration 53/1000 | Loss: 0.00002019
Iteration 54/1000 | Loss: 0.00002019
Iteration 55/1000 | Loss: 0.00002019
Iteration 56/1000 | Loss: 0.00002019
Iteration 57/1000 | Loss: 0.00002019
Iteration 58/1000 | Loss: 0.00002019
Iteration 59/1000 | Loss: 0.00002019
Iteration 60/1000 | Loss: 0.00002019
Iteration 61/1000 | Loss: 0.00002018
Iteration 62/1000 | Loss: 0.00002018
Iteration 63/1000 | Loss: 0.00002018
Iteration 64/1000 | Loss: 0.00002017
Iteration 65/1000 | Loss: 0.00002017
Iteration 66/1000 | Loss: 0.00002017
Iteration 67/1000 | Loss: 0.00002017
Iteration 68/1000 | Loss: 0.00002017
Iteration 69/1000 | Loss: 0.00002016
Iteration 70/1000 | Loss: 0.00002016
Iteration 71/1000 | Loss: 0.00002016
Iteration 72/1000 | Loss: 0.00002015
Iteration 73/1000 | Loss: 0.00002015
Iteration 74/1000 | Loss: 0.00002015
Iteration 75/1000 | Loss: 0.00002015
Iteration 76/1000 | Loss: 0.00002015
Iteration 77/1000 | Loss: 0.00002015
Iteration 78/1000 | Loss: 0.00002015
Iteration 79/1000 | Loss: 0.00002015
Iteration 80/1000 | Loss: 0.00002015
Iteration 81/1000 | Loss: 0.00002015
Iteration 82/1000 | Loss: 0.00002015
Iteration 83/1000 | Loss: 0.00002015
Iteration 84/1000 | Loss: 0.00002014
Iteration 85/1000 | Loss: 0.00002014
Iteration 86/1000 | Loss: 0.00002014
Iteration 87/1000 | Loss: 0.00002014
Iteration 88/1000 | Loss: 0.00002014
Iteration 89/1000 | Loss: 0.00002014
Iteration 90/1000 | Loss: 0.00002014
Iteration 91/1000 | Loss: 0.00002014
Iteration 92/1000 | Loss: 0.00002014
Iteration 93/1000 | Loss: 0.00002014
Iteration 94/1000 | Loss: 0.00002014
Iteration 95/1000 | Loss: 0.00002014
Iteration 96/1000 | Loss: 0.00002014
Iteration 97/1000 | Loss: 0.00002013
Iteration 98/1000 | Loss: 0.00002013
Iteration 99/1000 | Loss: 0.00002013
Iteration 100/1000 | Loss: 0.00002013
Iteration 101/1000 | Loss: 0.00002013
Iteration 102/1000 | Loss: 0.00002013
Iteration 103/1000 | Loss: 0.00002013
Iteration 104/1000 | Loss: 0.00002013
Iteration 105/1000 | Loss: 0.00002013
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.0128329197177663e-05, 2.0128329197177663e-05, 2.0128329197177663e-05, 2.0128329197177663e-05, 2.0128329197177663e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0128329197177663e-05

Optimization complete. Final v2v error: 3.8466713428497314 mm

Highest mean error: 4.362674713134766 mm for frame 41

Lowest mean error: 3.380068302154541 mm for frame 237

Saving results

Total time: 36.42213678359985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00706226
Iteration 2/25 | Loss: 0.00129368
Iteration 3/25 | Loss: 0.00108263
Iteration 4/25 | Loss: 0.00104778
Iteration 5/25 | Loss: 0.00103432
Iteration 6/25 | Loss: 0.00103209
Iteration 7/25 | Loss: 0.00103201
Iteration 8/25 | Loss: 0.00103201
Iteration 9/25 | Loss: 0.00103201
Iteration 10/25 | Loss: 0.00103201
Iteration 11/25 | Loss: 0.00103201
Iteration 12/25 | Loss: 0.00103201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010320120491087437, 0.0010320120491087437, 0.0010320120491087437, 0.0010320120491087437, 0.0010320120491087437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010320120491087437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.62016726
Iteration 2/25 | Loss: 0.00080261
Iteration 3/25 | Loss: 0.00080260
Iteration 4/25 | Loss: 0.00080260
Iteration 5/25 | Loss: 0.00080260
Iteration 6/25 | Loss: 0.00080259
Iteration 7/25 | Loss: 0.00080259
Iteration 8/25 | Loss: 0.00080259
Iteration 9/25 | Loss: 0.00080259
Iteration 10/25 | Loss: 0.00080259
Iteration 11/25 | Loss: 0.00080259
Iteration 12/25 | Loss: 0.00080259
Iteration 13/25 | Loss: 0.00080259
Iteration 14/25 | Loss: 0.00080259
Iteration 15/25 | Loss: 0.00080259
Iteration 16/25 | Loss: 0.00080259
Iteration 17/25 | Loss: 0.00080259
Iteration 18/25 | Loss: 0.00080259
Iteration 19/25 | Loss: 0.00080259
Iteration 20/25 | Loss: 0.00080259
Iteration 21/25 | Loss: 0.00080259
Iteration 22/25 | Loss: 0.00080259
Iteration 23/25 | Loss: 0.00080259
Iteration 24/25 | Loss: 0.00080259
Iteration 25/25 | Loss: 0.00080259

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080259
Iteration 2/1000 | Loss: 0.00007739
Iteration 3/1000 | Loss: 0.00004898
Iteration 4/1000 | Loss: 0.00003879
Iteration 5/1000 | Loss: 0.00003585
Iteration 6/1000 | Loss: 0.00003442
Iteration 7/1000 | Loss: 0.00003325
Iteration 8/1000 | Loss: 0.00003234
Iteration 9/1000 | Loss: 0.00003176
Iteration 10/1000 | Loss: 0.00003129
Iteration 11/1000 | Loss: 0.00003088
Iteration 12/1000 | Loss: 0.00003065
Iteration 13/1000 | Loss: 0.00003043
Iteration 14/1000 | Loss: 0.00003027
Iteration 15/1000 | Loss: 0.00003012
Iteration 16/1000 | Loss: 0.00003005
Iteration 17/1000 | Loss: 0.00002997
Iteration 18/1000 | Loss: 0.00002994
Iteration 19/1000 | Loss: 0.00002991
Iteration 20/1000 | Loss: 0.00002985
Iteration 21/1000 | Loss: 0.00002982
Iteration 22/1000 | Loss: 0.00002982
Iteration 23/1000 | Loss: 0.00002982
Iteration 24/1000 | Loss: 0.00002981
Iteration 25/1000 | Loss: 0.00002981
Iteration 26/1000 | Loss: 0.00002980
Iteration 27/1000 | Loss: 0.00002980
Iteration 28/1000 | Loss: 0.00002979
Iteration 29/1000 | Loss: 0.00002979
Iteration 30/1000 | Loss: 0.00002978
Iteration 31/1000 | Loss: 0.00002977
Iteration 32/1000 | Loss: 0.00002977
Iteration 33/1000 | Loss: 0.00002976
Iteration 34/1000 | Loss: 0.00002976
Iteration 35/1000 | Loss: 0.00002975
Iteration 36/1000 | Loss: 0.00002974
Iteration 37/1000 | Loss: 0.00002972
Iteration 38/1000 | Loss: 0.00002972
Iteration 39/1000 | Loss: 0.00002972
Iteration 40/1000 | Loss: 0.00002972
Iteration 41/1000 | Loss: 0.00002971
Iteration 42/1000 | Loss: 0.00002970
Iteration 43/1000 | Loss: 0.00002970
Iteration 44/1000 | Loss: 0.00002969
Iteration 45/1000 | Loss: 0.00002968
Iteration 46/1000 | Loss: 0.00002968
Iteration 47/1000 | Loss: 0.00002967
Iteration 48/1000 | Loss: 0.00002967
Iteration 49/1000 | Loss: 0.00002966
Iteration 50/1000 | Loss: 0.00002966
Iteration 51/1000 | Loss: 0.00002966
Iteration 52/1000 | Loss: 0.00002966
Iteration 53/1000 | Loss: 0.00002966
Iteration 54/1000 | Loss: 0.00002966
Iteration 55/1000 | Loss: 0.00002965
Iteration 56/1000 | Loss: 0.00002965
Iteration 57/1000 | Loss: 0.00002965
Iteration 58/1000 | Loss: 0.00002964
Iteration 59/1000 | Loss: 0.00002964
Iteration 60/1000 | Loss: 0.00002964
Iteration 61/1000 | Loss: 0.00002964
Iteration 62/1000 | Loss: 0.00002964
Iteration 63/1000 | Loss: 0.00002964
Iteration 64/1000 | Loss: 0.00002964
Iteration 65/1000 | Loss: 0.00002964
Iteration 66/1000 | Loss: 0.00002963
Iteration 67/1000 | Loss: 0.00002963
Iteration 68/1000 | Loss: 0.00002963
Iteration 69/1000 | Loss: 0.00002963
Iteration 70/1000 | Loss: 0.00002963
Iteration 71/1000 | Loss: 0.00002963
Iteration 72/1000 | Loss: 0.00002962
Iteration 73/1000 | Loss: 0.00002962
Iteration 74/1000 | Loss: 0.00002962
Iteration 75/1000 | Loss: 0.00002961
Iteration 76/1000 | Loss: 0.00002961
Iteration 77/1000 | Loss: 0.00002961
Iteration 78/1000 | Loss: 0.00002960
Iteration 79/1000 | Loss: 0.00002960
Iteration 80/1000 | Loss: 0.00002960
Iteration 81/1000 | Loss: 0.00002960
Iteration 82/1000 | Loss: 0.00002959
Iteration 83/1000 | Loss: 0.00002959
Iteration 84/1000 | Loss: 0.00002959
Iteration 85/1000 | Loss: 0.00002959
Iteration 86/1000 | Loss: 0.00002958
Iteration 87/1000 | Loss: 0.00002958
Iteration 88/1000 | Loss: 0.00002958
Iteration 89/1000 | Loss: 0.00002957
Iteration 90/1000 | Loss: 0.00002957
Iteration 91/1000 | Loss: 0.00002957
Iteration 92/1000 | Loss: 0.00002957
Iteration 93/1000 | Loss: 0.00002956
Iteration 94/1000 | Loss: 0.00002956
Iteration 95/1000 | Loss: 0.00002956
Iteration 96/1000 | Loss: 0.00002956
Iteration 97/1000 | Loss: 0.00002956
Iteration 98/1000 | Loss: 0.00002956
Iteration 99/1000 | Loss: 0.00002955
Iteration 100/1000 | Loss: 0.00002955
Iteration 101/1000 | Loss: 0.00002955
Iteration 102/1000 | Loss: 0.00002955
Iteration 103/1000 | Loss: 0.00002955
Iteration 104/1000 | Loss: 0.00002954
Iteration 105/1000 | Loss: 0.00002954
Iteration 106/1000 | Loss: 0.00002954
Iteration 107/1000 | Loss: 0.00002954
Iteration 108/1000 | Loss: 0.00002954
Iteration 109/1000 | Loss: 0.00002953
Iteration 110/1000 | Loss: 0.00002953
Iteration 111/1000 | Loss: 0.00002953
Iteration 112/1000 | Loss: 0.00002953
Iteration 113/1000 | Loss: 0.00002953
Iteration 114/1000 | Loss: 0.00002953
Iteration 115/1000 | Loss: 0.00002953
Iteration 116/1000 | Loss: 0.00002953
Iteration 117/1000 | Loss: 0.00002953
Iteration 118/1000 | Loss: 0.00002953
Iteration 119/1000 | Loss: 0.00002953
Iteration 120/1000 | Loss: 0.00002952
Iteration 121/1000 | Loss: 0.00002952
Iteration 122/1000 | Loss: 0.00002952
Iteration 123/1000 | Loss: 0.00002952
Iteration 124/1000 | Loss: 0.00002952
Iteration 125/1000 | Loss: 0.00002952
Iteration 126/1000 | Loss: 0.00002952
Iteration 127/1000 | Loss: 0.00002952
Iteration 128/1000 | Loss: 0.00002952
Iteration 129/1000 | Loss: 0.00002951
Iteration 130/1000 | Loss: 0.00002951
Iteration 131/1000 | Loss: 0.00002951
Iteration 132/1000 | Loss: 0.00002951
Iteration 133/1000 | Loss: 0.00002950
Iteration 134/1000 | Loss: 0.00002950
Iteration 135/1000 | Loss: 0.00002950
Iteration 136/1000 | Loss: 0.00002950
Iteration 137/1000 | Loss: 0.00002950
Iteration 138/1000 | Loss: 0.00002950
Iteration 139/1000 | Loss: 0.00002950
Iteration 140/1000 | Loss: 0.00002950
Iteration 141/1000 | Loss: 0.00002950
Iteration 142/1000 | Loss: 0.00002950
Iteration 143/1000 | Loss: 0.00002950
Iteration 144/1000 | Loss: 0.00002949
Iteration 145/1000 | Loss: 0.00002949
Iteration 146/1000 | Loss: 0.00002949
Iteration 147/1000 | Loss: 0.00002948
Iteration 148/1000 | Loss: 0.00002948
Iteration 149/1000 | Loss: 0.00002948
Iteration 150/1000 | Loss: 0.00002948
Iteration 151/1000 | Loss: 0.00002947
Iteration 152/1000 | Loss: 0.00002947
Iteration 153/1000 | Loss: 0.00002947
Iteration 154/1000 | Loss: 0.00002946
Iteration 155/1000 | Loss: 0.00002946
Iteration 156/1000 | Loss: 0.00002945
Iteration 157/1000 | Loss: 0.00002945
Iteration 158/1000 | Loss: 0.00002945
Iteration 159/1000 | Loss: 0.00002944
Iteration 160/1000 | Loss: 0.00002944
Iteration 161/1000 | Loss: 0.00002944
Iteration 162/1000 | Loss: 0.00002944
Iteration 163/1000 | Loss: 0.00002944
Iteration 164/1000 | Loss: 0.00002944
Iteration 165/1000 | Loss: 0.00002944
Iteration 166/1000 | Loss: 0.00002944
Iteration 167/1000 | Loss: 0.00002943
Iteration 168/1000 | Loss: 0.00002943
Iteration 169/1000 | Loss: 0.00002943
Iteration 170/1000 | Loss: 0.00002943
Iteration 171/1000 | Loss: 0.00002942
Iteration 172/1000 | Loss: 0.00002942
Iteration 173/1000 | Loss: 0.00002942
Iteration 174/1000 | Loss: 0.00002941
Iteration 175/1000 | Loss: 0.00002941
Iteration 176/1000 | Loss: 0.00002941
Iteration 177/1000 | Loss: 0.00002941
Iteration 178/1000 | Loss: 0.00002941
Iteration 179/1000 | Loss: 0.00002940
Iteration 180/1000 | Loss: 0.00002940
Iteration 181/1000 | Loss: 0.00002940
Iteration 182/1000 | Loss: 0.00002940
Iteration 183/1000 | Loss: 0.00002940
Iteration 184/1000 | Loss: 0.00002940
Iteration 185/1000 | Loss: 0.00002940
Iteration 186/1000 | Loss: 0.00002940
Iteration 187/1000 | Loss: 0.00002940
Iteration 188/1000 | Loss: 0.00002940
Iteration 189/1000 | Loss: 0.00002939
Iteration 190/1000 | Loss: 0.00002939
Iteration 191/1000 | Loss: 0.00002939
Iteration 192/1000 | Loss: 0.00002938
Iteration 193/1000 | Loss: 0.00002938
Iteration 194/1000 | Loss: 0.00002938
Iteration 195/1000 | Loss: 0.00002938
Iteration 196/1000 | Loss: 0.00002938
Iteration 197/1000 | Loss: 0.00002937
Iteration 198/1000 | Loss: 0.00002937
Iteration 199/1000 | Loss: 0.00002937
Iteration 200/1000 | Loss: 0.00002937
Iteration 201/1000 | Loss: 0.00002937
Iteration 202/1000 | Loss: 0.00002936
Iteration 203/1000 | Loss: 0.00002936
Iteration 204/1000 | Loss: 0.00002936
Iteration 205/1000 | Loss: 0.00002936
Iteration 206/1000 | Loss: 0.00002936
Iteration 207/1000 | Loss: 0.00002935
Iteration 208/1000 | Loss: 0.00002935
Iteration 209/1000 | Loss: 0.00002935
Iteration 210/1000 | Loss: 0.00002935
Iteration 211/1000 | Loss: 0.00002935
Iteration 212/1000 | Loss: 0.00002935
Iteration 213/1000 | Loss: 0.00002934
Iteration 214/1000 | Loss: 0.00002934
Iteration 215/1000 | Loss: 0.00002934
Iteration 216/1000 | Loss: 0.00002934
Iteration 217/1000 | Loss: 0.00002934
Iteration 218/1000 | Loss: 0.00002934
Iteration 219/1000 | Loss: 0.00002934
Iteration 220/1000 | Loss: 0.00002934
Iteration 221/1000 | Loss: 0.00002934
Iteration 222/1000 | Loss: 0.00002933
Iteration 223/1000 | Loss: 0.00002933
Iteration 224/1000 | Loss: 0.00002933
Iteration 225/1000 | Loss: 0.00002933
Iteration 226/1000 | Loss: 0.00002933
Iteration 227/1000 | Loss: 0.00002932
Iteration 228/1000 | Loss: 0.00002932
Iteration 229/1000 | Loss: 0.00002932
Iteration 230/1000 | Loss: 0.00002932
Iteration 231/1000 | Loss: 0.00002932
Iteration 232/1000 | Loss: 0.00002932
Iteration 233/1000 | Loss: 0.00002932
Iteration 234/1000 | Loss: 0.00002932
Iteration 235/1000 | Loss: 0.00002932
Iteration 236/1000 | Loss: 0.00002931
Iteration 237/1000 | Loss: 0.00002931
Iteration 238/1000 | Loss: 0.00002931
Iteration 239/1000 | Loss: 0.00002931
Iteration 240/1000 | Loss: 0.00002931
Iteration 241/1000 | Loss: 0.00002931
Iteration 242/1000 | Loss: 0.00002931
Iteration 243/1000 | Loss: 0.00002930
Iteration 244/1000 | Loss: 0.00002930
Iteration 245/1000 | Loss: 0.00002930
Iteration 246/1000 | Loss: 0.00002930
Iteration 247/1000 | Loss: 0.00002930
Iteration 248/1000 | Loss: 0.00002930
Iteration 249/1000 | Loss: 0.00002930
Iteration 250/1000 | Loss: 0.00002930
Iteration 251/1000 | Loss: 0.00002930
Iteration 252/1000 | Loss: 0.00002930
Iteration 253/1000 | Loss: 0.00002930
Iteration 254/1000 | Loss: 0.00002930
Iteration 255/1000 | Loss: 0.00002930
Iteration 256/1000 | Loss: 0.00002930
Iteration 257/1000 | Loss: 0.00002930
Iteration 258/1000 | Loss: 0.00002930
Iteration 259/1000 | Loss: 0.00002930
Iteration 260/1000 | Loss: 0.00002930
Iteration 261/1000 | Loss: 0.00002930
Iteration 262/1000 | Loss: 0.00002930
Iteration 263/1000 | Loss: 0.00002930
Iteration 264/1000 | Loss: 0.00002930
Iteration 265/1000 | Loss: 0.00002930
Iteration 266/1000 | Loss: 0.00002930
Iteration 267/1000 | Loss: 0.00002930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [2.9297381843207404e-05, 2.9297381843207404e-05, 2.9297381843207404e-05, 2.9297381843207404e-05, 2.9297381843207404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9297381843207404e-05

Optimization complete. Final v2v error: 4.3986124992370605 mm

Highest mean error: 6.299643039703369 mm for frame 199

Lowest mean error: 3.1110808849334717 mm for frame 235

Saving results

Total time: 59.670811891555786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827061
Iteration 2/25 | Loss: 0.00166771
Iteration 3/25 | Loss: 0.00143121
Iteration 4/25 | Loss: 0.00150634
Iteration 5/25 | Loss: 0.00148114
Iteration 6/25 | Loss: 0.00135332
Iteration 7/25 | Loss: 0.00110301
Iteration 8/25 | Loss: 0.00103660
Iteration 9/25 | Loss: 0.00096715
Iteration 10/25 | Loss: 0.00093368
Iteration 11/25 | Loss: 0.00091223
Iteration 12/25 | Loss: 0.00091339
Iteration 13/25 | Loss: 0.00089979
Iteration 14/25 | Loss: 0.00089956
Iteration 15/25 | Loss: 0.00089955
Iteration 16/25 | Loss: 0.00089955
Iteration 17/25 | Loss: 0.00089955
Iteration 18/25 | Loss: 0.00089955
Iteration 19/25 | Loss: 0.00089955
Iteration 20/25 | Loss: 0.00089955
Iteration 21/25 | Loss: 0.00089955
Iteration 22/25 | Loss: 0.00089954
Iteration 23/25 | Loss: 0.00089954
Iteration 24/25 | Loss: 0.00089954
Iteration 25/25 | Loss: 0.00089954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35173559
Iteration 2/25 | Loss: 0.00073972
Iteration 3/25 | Loss: 0.00073972
Iteration 4/25 | Loss: 0.00073972
Iteration 5/25 | Loss: 0.00073972
Iteration 6/25 | Loss: 0.00073972
Iteration 7/25 | Loss: 0.00073972
Iteration 8/25 | Loss: 0.00073971
Iteration 9/25 | Loss: 0.00073971
Iteration 10/25 | Loss: 0.00073971
Iteration 11/25 | Loss: 0.00073971
Iteration 12/25 | Loss: 0.00073971
Iteration 13/25 | Loss: 0.00073971
Iteration 14/25 | Loss: 0.00073971
Iteration 15/25 | Loss: 0.00073971
Iteration 16/25 | Loss: 0.00073971
Iteration 17/25 | Loss: 0.00073971
Iteration 18/25 | Loss: 0.00073971
Iteration 19/25 | Loss: 0.00073971
Iteration 20/25 | Loss: 0.00073971
Iteration 21/25 | Loss: 0.00073971
Iteration 22/25 | Loss: 0.00073971
Iteration 23/25 | Loss: 0.00073971
Iteration 24/25 | Loss: 0.00073971
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007397132576443255, 0.0007397132576443255, 0.0007397132576443255, 0.0007397132576443255, 0.0007397132576443255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007397132576443255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073971
Iteration 2/1000 | Loss: 0.00003356
Iteration 3/1000 | Loss: 0.00001813
Iteration 4/1000 | Loss: 0.00001620
Iteration 5/1000 | Loss: 0.00001540
Iteration 6/1000 | Loss: 0.00001478
Iteration 7/1000 | Loss: 0.00001433
Iteration 8/1000 | Loss: 0.00001401
Iteration 9/1000 | Loss: 0.00001373
Iteration 10/1000 | Loss: 0.00001360
Iteration 11/1000 | Loss: 0.00001353
Iteration 12/1000 | Loss: 0.00001352
Iteration 13/1000 | Loss: 0.00001351
Iteration 14/1000 | Loss: 0.00001351
Iteration 15/1000 | Loss: 0.00001350
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001348
Iteration 18/1000 | Loss: 0.00001348
Iteration 19/1000 | Loss: 0.00001348
Iteration 20/1000 | Loss: 0.00001348
Iteration 21/1000 | Loss: 0.00001347
Iteration 22/1000 | Loss: 0.00001347
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001347
Iteration 26/1000 | Loss: 0.00001347
Iteration 27/1000 | Loss: 0.00001347
Iteration 28/1000 | Loss: 0.00001347
Iteration 29/1000 | Loss: 0.00001347
Iteration 30/1000 | Loss: 0.00001347
Iteration 31/1000 | Loss: 0.00001347
Iteration 32/1000 | Loss: 0.00001347
Iteration 33/1000 | Loss: 0.00001346
Iteration 34/1000 | Loss: 0.00001346
Iteration 35/1000 | Loss: 0.00001346
Iteration 36/1000 | Loss: 0.00001346
Iteration 37/1000 | Loss: 0.00001346
Iteration 38/1000 | Loss: 0.00001346
Iteration 39/1000 | Loss: 0.00001346
Iteration 40/1000 | Loss: 0.00001346
Iteration 41/1000 | Loss: 0.00001346
Iteration 42/1000 | Loss: 0.00001346
Iteration 43/1000 | Loss: 0.00001345
Iteration 44/1000 | Loss: 0.00001345
Iteration 45/1000 | Loss: 0.00001345
Iteration 46/1000 | Loss: 0.00001344
Iteration 47/1000 | Loss: 0.00001344
Iteration 48/1000 | Loss: 0.00001343
Iteration 49/1000 | Loss: 0.00001343
Iteration 50/1000 | Loss: 0.00001342
Iteration 51/1000 | Loss: 0.00001342
Iteration 52/1000 | Loss: 0.00001342
Iteration 53/1000 | Loss: 0.00001342
Iteration 54/1000 | Loss: 0.00001342
Iteration 55/1000 | Loss: 0.00001341
Iteration 56/1000 | Loss: 0.00001341
Iteration 57/1000 | Loss: 0.00001341
Iteration 58/1000 | Loss: 0.00001340
Iteration 59/1000 | Loss: 0.00001340
Iteration 60/1000 | Loss: 0.00001340
Iteration 61/1000 | Loss: 0.00001340
Iteration 62/1000 | Loss: 0.00001340
Iteration 63/1000 | Loss: 0.00001340
Iteration 64/1000 | Loss: 0.00001340
Iteration 65/1000 | Loss: 0.00001339
Iteration 66/1000 | Loss: 0.00001339
Iteration 67/1000 | Loss: 0.00001339
Iteration 68/1000 | Loss: 0.00001339
Iteration 69/1000 | Loss: 0.00001338
Iteration 70/1000 | Loss: 0.00001338
Iteration 71/1000 | Loss: 0.00001338
Iteration 72/1000 | Loss: 0.00001337
Iteration 73/1000 | Loss: 0.00001337
Iteration 74/1000 | Loss: 0.00001337
Iteration 75/1000 | Loss: 0.00001337
Iteration 76/1000 | Loss: 0.00001337
Iteration 77/1000 | Loss: 0.00001337
Iteration 78/1000 | Loss: 0.00001336
Iteration 79/1000 | Loss: 0.00001336
Iteration 80/1000 | Loss: 0.00001336
Iteration 81/1000 | Loss: 0.00001336
Iteration 82/1000 | Loss: 0.00001336
Iteration 83/1000 | Loss: 0.00001336
Iteration 84/1000 | Loss: 0.00001336
Iteration 85/1000 | Loss: 0.00001336
Iteration 86/1000 | Loss: 0.00001336
Iteration 87/1000 | Loss: 0.00001336
Iteration 88/1000 | Loss: 0.00001336
Iteration 89/1000 | Loss: 0.00001336
Iteration 90/1000 | Loss: 0.00001336
Iteration 91/1000 | Loss: 0.00001335
Iteration 92/1000 | Loss: 0.00001335
Iteration 93/1000 | Loss: 0.00001335
Iteration 94/1000 | Loss: 0.00001335
Iteration 95/1000 | Loss: 0.00001335
Iteration 96/1000 | Loss: 0.00001335
Iteration 97/1000 | Loss: 0.00001335
Iteration 98/1000 | Loss: 0.00001335
Iteration 99/1000 | Loss: 0.00001335
Iteration 100/1000 | Loss: 0.00001335
Iteration 101/1000 | Loss: 0.00001335
Iteration 102/1000 | Loss: 0.00001335
Iteration 103/1000 | Loss: 0.00001335
Iteration 104/1000 | Loss: 0.00001335
Iteration 105/1000 | Loss: 0.00001335
Iteration 106/1000 | Loss: 0.00001335
Iteration 107/1000 | Loss: 0.00001335
Iteration 108/1000 | Loss: 0.00001334
Iteration 109/1000 | Loss: 0.00001334
Iteration 110/1000 | Loss: 0.00001334
Iteration 111/1000 | Loss: 0.00001334
Iteration 112/1000 | Loss: 0.00001334
Iteration 113/1000 | Loss: 0.00001334
Iteration 114/1000 | Loss: 0.00001334
Iteration 115/1000 | Loss: 0.00001334
Iteration 116/1000 | Loss: 0.00001334
Iteration 117/1000 | Loss: 0.00001334
Iteration 118/1000 | Loss: 0.00001334
Iteration 119/1000 | Loss: 0.00001334
Iteration 120/1000 | Loss: 0.00001334
Iteration 121/1000 | Loss: 0.00001334
Iteration 122/1000 | Loss: 0.00001334
Iteration 123/1000 | Loss: 0.00001334
Iteration 124/1000 | Loss: 0.00001334
Iteration 125/1000 | Loss: 0.00001333
Iteration 126/1000 | Loss: 0.00001333
Iteration 127/1000 | Loss: 0.00001333
Iteration 128/1000 | Loss: 0.00001333
Iteration 129/1000 | Loss: 0.00001333
Iteration 130/1000 | Loss: 0.00001333
Iteration 131/1000 | Loss: 0.00001333
Iteration 132/1000 | Loss: 0.00001333
Iteration 133/1000 | Loss: 0.00001333
Iteration 134/1000 | Loss: 0.00001333
Iteration 135/1000 | Loss: 0.00001333
Iteration 136/1000 | Loss: 0.00001332
Iteration 137/1000 | Loss: 0.00001332
Iteration 138/1000 | Loss: 0.00001332
Iteration 139/1000 | Loss: 0.00001332
Iteration 140/1000 | Loss: 0.00001332
Iteration 141/1000 | Loss: 0.00001332
Iteration 142/1000 | Loss: 0.00001332
Iteration 143/1000 | Loss: 0.00001332
Iteration 144/1000 | Loss: 0.00001332
Iteration 145/1000 | Loss: 0.00001332
Iteration 146/1000 | Loss: 0.00001332
Iteration 147/1000 | Loss: 0.00001332
Iteration 148/1000 | Loss: 0.00001332
Iteration 149/1000 | Loss: 0.00001332
Iteration 150/1000 | Loss: 0.00001332
Iteration 151/1000 | Loss: 0.00001331
Iteration 152/1000 | Loss: 0.00001331
Iteration 153/1000 | Loss: 0.00001331
Iteration 154/1000 | Loss: 0.00001331
Iteration 155/1000 | Loss: 0.00001331
Iteration 156/1000 | Loss: 0.00001331
Iteration 157/1000 | Loss: 0.00001331
Iteration 158/1000 | Loss: 0.00001331
Iteration 159/1000 | Loss: 0.00001331
Iteration 160/1000 | Loss: 0.00001331
Iteration 161/1000 | Loss: 0.00001331
Iteration 162/1000 | Loss: 0.00001331
Iteration 163/1000 | Loss: 0.00001331
Iteration 164/1000 | Loss: 0.00001331
Iteration 165/1000 | Loss: 0.00001331
Iteration 166/1000 | Loss: 0.00001331
Iteration 167/1000 | Loss: 0.00001330
Iteration 168/1000 | Loss: 0.00001330
Iteration 169/1000 | Loss: 0.00001330
Iteration 170/1000 | Loss: 0.00001330
Iteration 171/1000 | Loss: 0.00001330
Iteration 172/1000 | Loss: 0.00001330
Iteration 173/1000 | Loss: 0.00001330
Iteration 174/1000 | Loss: 0.00001330
Iteration 175/1000 | Loss: 0.00001330
Iteration 176/1000 | Loss: 0.00001330
Iteration 177/1000 | Loss: 0.00001330
Iteration 178/1000 | Loss: 0.00001330
Iteration 179/1000 | Loss: 0.00001330
Iteration 180/1000 | Loss: 0.00001330
Iteration 181/1000 | Loss: 0.00001330
Iteration 182/1000 | Loss: 0.00001330
Iteration 183/1000 | Loss: 0.00001330
Iteration 184/1000 | Loss: 0.00001330
Iteration 185/1000 | Loss: 0.00001330
Iteration 186/1000 | Loss: 0.00001330
Iteration 187/1000 | Loss: 0.00001330
Iteration 188/1000 | Loss: 0.00001330
Iteration 189/1000 | Loss: 0.00001330
Iteration 190/1000 | Loss: 0.00001330
Iteration 191/1000 | Loss: 0.00001330
Iteration 192/1000 | Loss: 0.00001330
Iteration 193/1000 | Loss: 0.00001330
Iteration 194/1000 | Loss: 0.00001330
Iteration 195/1000 | Loss: 0.00001330
Iteration 196/1000 | Loss: 0.00001330
Iteration 197/1000 | Loss: 0.00001330
Iteration 198/1000 | Loss: 0.00001330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.3296920769789722e-05, 1.3296920769789722e-05, 1.3296920769789722e-05, 1.3296920769789722e-05, 1.3296920769789722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3296920769789722e-05

Optimization complete. Final v2v error: 2.985093116760254 mm

Highest mean error: 4.010959148406982 mm for frame 50

Lowest mean error: 2.379404306411743 mm for frame 0

Saving results

Total time: 55.496970653533936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923595
Iteration 2/25 | Loss: 0.00294712
Iteration 3/25 | Loss: 0.00191753
Iteration 4/25 | Loss: 0.00179895
Iteration 5/25 | Loss: 0.00156026
Iteration 6/25 | Loss: 0.00157698
Iteration 7/25 | Loss: 0.00135740
Iteration 8/25 | Loss: 0.00127679
Iteration 9/25 | Loss: 0.00120274
Iteration 10/25 | Loss: 0.00121664
Iteration 11/25 | Loss: 0.00117612
Iteration 12/25 | Loss: 0.00115649
Iteration 13/25 | Loss: 0.00111402
Iteration 14/25 | Loss: 0.00111121
Iteration 15/25 | Loss: 0.00111240
Iteration 16/25 | Loss: 0.00111006
Iteration 17/25 | Loss: 0.00111404
Iteration 18/25 | Loss: 0.00110958
Iteration 19/25 | Loss: 0.00113533
Iteration 20/25 | Loss: 0.00111690
Iteration 21/25 | Loss: 0.00111217
Iteration 22/25 | Loss: 0.00111694
Iteration 23/25 | Loss: 0.00110037
Iteration 24/25 | Loss: 0.00109705
Iteration 25/25 | Loss: 0.00111496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.54541206
Iteration 2/25 | Loss: 0.00451830
Iteration 3/25 | Loss: 0.00260584
Iteration 4/25 | Loss: 0.00260584
Iteration 5/25 | Loss: 0.00260584
Iteration 6/25 | Loss: 0.00260584
Iteration 7/25 | Loss: 0.00260584
Iteration 8/25 | Loss: 0.00260584
Iteration 9/25 | Loss: 0.00260584
Iteration 10/25 | Loss: 0.00260584
Iteration 11/25 | Loss: 0.00260584
Iteration 12/25 | Loss: 0.00260584
Iteration 13/25 | Loss: 0.00260584
Iteration 14/25 | Loss: 0.00260584
Iteration 15/25 | Loss: 0.00260584
Iteration 16/25 | Loss: 0.00260584
Iteration 17/25 | Loss: 0.00260584
Iteration 18/25 | Loss: 0.00260584
Iteration 19/25 | Loss: 0.00260584
Iteration 20/25 | Loss: 0.00260584
Iteration 21/25 | Loss: 0.00260584
Iteration 22/25 | Loss: 0.00260584
Iteration 23/25 | Loss: 0.00260584
Iteration 24/25 | Loss: 0.00260584
Iteration 25/25 | Loss: 0.00260584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00260584
Iteration 2/1000 | Loss: 0.00141450
Iteration 3/1000 | Loss: 0.00093977
Iteration 4/1000 | Loss: 0.00161326
Iteration 5/1000 | Loss: 0.00141913
Iteration 6/1000 | Loss: 0.00028577
Iteration 7/1000 | Loss: 0.00213444
Iteration 8/1000 | Loss: 0.00257775
Iteration 9/1000 | Loss: 0.00475058
Iteration 10/1000 | Loss: 0.00161592
Iteration 11/1000 | Loss: 0.00187825
Iteration 12/1000 | Loss: 0.00071471
Iteration 13/1000 | Loss: 0.00113259
Iteration 14/1000 | Loss: 0.00012461
Iteration 15/1000 | Loss: 0.00020572
Iteration 16/1000 | Loss: 0.00330526
Iteration 17/1000 | Loss: 0.00126011
Iteration 18/1000 | Loss: 0.00012617
Iteration 19/1000 | Loss: 0.00008523
Iteration 20/1000 | Loss: 0.00160837
Iteration 21/1000 | Loss: 0.00007585
Iteration 22/1000 | Loss: 0.00006404
Iteration 23/1000 | Loss: 0.00032190
Iteration 24/1000 | Loss: 0.00070120
Iteration 25/1000 | Loss: 0.00037043
Iteration 26/1000 | Loss: 0.00034932
Iteration 27/1000 | Loss: 0.00052960
Iteration 28/1000 | Loss: 0.00040322
Iteration 29/1000 | Loss: 0.00030237
Iteration 30/1000 | Loss: 0.00006556
Iteration 31/1000 | Loss: 0.00005274
Iteration 32/1000 | Loss: 0.00110432
Iteration 33/1000 | Loss: 0.00040961
Iteration 34/1000 | Loss: 0.00097761
Iteration 35/1000 | Loss: 0.00018754
Iteration 36/1000 | Loss: 0.00014317
Iteration 37/1000 | Loss: 0.00004580
Iteration 38/1000 | Loss: 0.00003934
Iteration 39/1000 | Loss: 0.00027371
Iteration 40/1000 | Loss: 0.00016855
Iteration 41/1000 | Loss: 0.00053245
Iteration 42/1000 | Loss: 0.00059482
Iteration 43/1000 | Loss: 0.00041544
Iteration 44/1000 | Loss: 0.00065102
Iteration 45/1000 | Loss: 0.00224143
Iteration 46/1000 | Loss: 0.00048065
Iteration 47/1000 | Loss: 0.00003248
Iteration 48/1000 | Loss: 0.00002673
Iteration 49/1000 | Loss: 0.00002399
Iteration 50/1000 | Loss: 0.00002194
Iteration 51/1000 | Loss: 0.00002084
Iteration 52/1000 | Loss: 0.00001991
Iteration 53/1000 | Loss: 0.00059758
Iteration 54/1000 | Loss: 0.00004006
Iteration 55/1000 | Loss: 0.00002573
Iteration 56/1000 | Loss: 0.00002163
Iteration 57/1000 | Loss: 0.00001963
Iteration 58/1000 | Loss: 0.00001747
Iteration 59/1000 | Loss: 0.00001567
Iteration 60/1000 | Loss: 0.00001487
Iteration 61/1000 | Loss: 0.00001421
Iteration 62/1000 | Loss: 0.00001393
Iteration 63/1000 | Loss: 0.00001385
Iteration 64/1000 | Loss: 0.00001378
Iteration 65/1000 | Loss: 0.00001376
Iteration 66/1000 | Loss: 0.00001366
Iteration 67/1000 | Loss: 0.00001357
Iteration 68/1000 | Loss: 0.00001355
Iteration 69/1000 | Loss: 0.00001349
Iteration 70/1000 | Loss: 0.00001346
Iteration 71/1000 | Loss: 0.00001345
Iteration 72/1000 | Loss: 0.00001344
Iteration 73/1000 | Loss: 0.00001343
Iteration 74/1000 | Loss: 0.00001336
Iteration 75/1000 | Loss: 0.00001336
Iteration 76/1000 | Loss: 0.00001336
Iteration 77/1000 | Loss: 0.00001336
Iteration 78/1000 | Loss: 0.00001336
Iteration 79/1000 | Loss: 0.00001336
Iteration 80/1000 | Loss: 0.00001335
Iteration 81/1000 | Loss: 0.00001335
Iteration 82/1000 | Loss: 0.00001335
Iteration 83/1000 | Loss: 0.00001334
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001331
Iteration 86/1000 | Loss: 0.00001331
Iteration 87/1000 | Loss: 0.00001330
Iteration 88/1000 | Loss: 0.00001330
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001327
Iteration 91/1000 | Loss: 0.00001327
Iteration 92/1000 | Loss: 0.00001325
Iteration 93/1000 | Loss: 0.00001325
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001324
Iteration 96/1000 | Loss: 0.00001324
Iteration 97/1000 | Loss: 0.00001324
Iteration 98/1000 | Loss: 0.00001324
Iteration 99/1000 | Loss: 0.00001324
Iteration 100/1000 | Loss: 0.00001324
Iteration 101/1000 | Loss: 0.00001324
Iteration 102/1000 | Loss: 0.00001324
Iteration 103/1000 | Loss: 0.00001324
Iteration 104/1000 | Loss: 0.00001324
Iteration 105/1000 | Loss: 0.00001324
Iteration 106/1000 | Loss: 0.00001323
Iteration 107/1000 | Loss: 0.00001323
Iteration 108/1000 | Loss: 0.00001323
Iteration 109/1000 | Loss: 0.00001323
Iteration 110/1000 | Loss: 0.00001323
Iteration 111/1000 | Loss: 0.00001323
Iteration 112/1000 | Loss: 0.00001323
Iteration 113/1000 | Loss: 0.00001322
Iteration 114/1000 | Loss: 0.00001322
Iteration 115/1000 | Loss: 0.00001322
Iteration 116/1000 | Loss: 0.00001321
Iteration 117/1000 | Loss: 0.00001321
Iteration 118/1000 | Loss: 0.00001321
Iteration 119/1000 | Loss: 0.00001320
Iteration 120/1000 | Loss: 0.00001319
Iteration 121/1000 | Loss: 0.00001319
Iteration 122/1000 | Loss: 0.00001319
Iteration 123/1000 | Loss: 0.00001319
Iteration 124/1000 | Loss: 0.00001319
Iteration 125/1000 | Loss: 0.00001319
Iteration 126/1000 | Loss: 0.00001319
Iteration 127/1000 | Loss: 0.00001318
Iteration 128/1000 | Loss: 0.00001318
Iteration 129/1000 | Loss: 0.00001317
Iteration 130/1000 | Loss: 0.00001317
Iteration 131/1000 | Loss: 0.00001317
Iteration 132/1000 | Loss: 0.00001316
Iteration 133/1000 | Loss: 0.00001316
Iteration 134/1000 | Loss: 0.00001316
Iteration 135/1000 | Loss: 0.00001315
Iteration 136/1000 | Loss: 0.00001315
Iteration 137/1000 | Loss: 0.00001315
Iteration 138/1000 | Loss: 0.00001315
Iteration 139/1000 | Loss: 0.00001315
Iteration 140/1000 | Loss: 0.00001315
Iteration 141/1000 | Loss: 0.00001315
Iteration 142/1000 | Loss: 0.00001315
Iteration 143/1000 | Loss: 0.00001314
Iteration 144/1000 | Loss: 0.00001314
Iteration 145/1000 | Loss: 0.00001314
Iteration 146/1000 | Loss: 0.00001314
Iteration 147/1000 | Loss: 0.00001314
Iteration 148/1000 | Loss: 0.00001314
Iteration 149/1000 | Loss: 0.00001313
Iteration 150/1000 | Loss: 0.00001313
Iteration 151/1000 | Loss: 0.00001313
Iteration 152/1000 | Loss: 0.00001312
Iteration 153/1000 | Loss: 0.00001312
Iteration 154/1000 | Loss: 0.00001311
Iteration 155/1000 | Loss: 0.00001311
Iteration 156/1000 | Loss: 0.00001310
Iteration 157/1000 | Loss: 0.00001310
Iteration 158/1000 | Loss: 0.00001310
Iteration 159/1000 | Loss: 0.00001310
Iteration 160/1000 | Loss: 0.00001310
Iteration 161/1000 | Loss: 0.00001309
Iteration 162/1000 | Loss: 0.00001309
Iteration 163/1000 | Loss: 0.00001309
Iteration 164/1000 | Loss: 0.00001308
Iteration 165/1000 | Loss: 0.00001308
Iteration 166/1000 | Loss: 0.00001308
Iteration 167/1000 | Loss: 0.00001308
Iteration 168/1000 | Loss: 0.00001308
Iteration 169/1000 | Loss: 0.00001308
Iteration 170/1000 | Loss: 0.00001308
Iteration 171/1000 | Loss: 0.00001308
Iteration 172/1000 | Loss: 0.00001308
Iteration 173/1000 | Loss: 0.00001308
Iteration 174/1000 | Loss: 0.00001308
Iteration 175/1000 | Loss: 0.00001308
Iteration 176/1000 | Loss: 0.00001308
Iteration 177/1000 | Loss: 0.00001308
Iteration 178/1000 | Loss: 0.00001308
Iteration 179/1000 | Loss: 0.00001308
Iteration 180/1000 | Loss: 0.00001308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.3079573363938835e-05, 1.3079573363938835e-05, 1.3079573363938835e-05, 1.3079573363938835e-05, 1.3079573363938835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3079573363938835e-05

Optimization complete. Final v2v error: 3.0296881198883057 mm

Highest mean error: 4.830246448516846 mm for frame 67

Lowest mean error: 2.379201889038086 mm for frame 13

Saving results

Total time: 154.96297812461853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069806
Iteration 2/25 | Loss: 0.01069806
Iteration 3/25 | Loss: 0.01069806
Iteration 4/25 | Loss: 0.01069806
Iteration 5/25 | Loss: 0.01069806
Iteration 6/25 | Loss: 0.01069806
Iteration 7/25 | Loss: 0.01069805
Iteration 8/25 | Loss: 0.01069805
Iteration 9/25 | Loss: 0.01069805
Iteration 10/25 | Loss: 0.01069805
Iteration 11/25 | Loss: 0.01069805
Iteration 12/25 | Loss: 0.01069805
Iteration 13/25 | Loss: 0.01069805
Iteration 14/25 | Loss: 0.01069804
Iteration 15/25 | Loss: 0.01069804
Iteration 16/25 | Loss: 0.01069804
Iteration 17/25 | Loss: 0.01069804
Iteration 18/25 | Loss: 0.01069804
Iteration 19/25 | Loss: 0.01069803
Iteration 20/25 | Loss: 0.01069803
Iteration 21/25 | Loss: 0.01069803
Iteration 22/25 | Loss: 0.01069803
Iteration 23/25 | Loss: 0.01069803
Iteration 24/25 | Loss: 0.01069803
Iteration 25/25 | Loss: 0.01069802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97737181
Iteration 2/25 | Loss: 0.16941954
Iteration 3/25 | Loss: 0.16508421
Iteration 4/25 | Loss: 0.16344154
Iteration 5/25 | Loss: 0.16300945
Iteration 6/25 | Loss: 0.16300943
Iteration 7/25 | Loss: 0.16300942
Iteration 8/25 | Loss: 0.16300942
Iteration 9/25 | Loss: 0.16300939
Iteration 10/25 | Loss: 0.16300939
Iteration 11/25 | Loss: 0.16300939
Iteration 12/25 | Loss: 0.16300939
Iteration 13/25 | Loss: 0.16300939
Iteration 14/25 | Loss: 0.16300939
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.1630093902349472, 0.1630093902349472, 0.1630093902349472, 0.1630093902349472, 0.1630093902349472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1630093902349472

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16300938
Iteration 2/1000 | Loss: 0.00517909
Iteration 3/1000 | Loss: 0.00490155
Iteration 4/1000 | Loss: 0.00304415
Iteration 5/1000 | Loss: 0.00726402
Iteration 6/1000 | Loss: 0.00043197
Iteration 7/1000 | Loss: 0.00229685
Iteration 8/1000 | Loss: 0.00285496
Iteration 9/1000 | Loss: 0.00030298
Iteration 10/1000 | Loss: 0.00018814
Iteration 11/1000 | Loss: 0.00019169
Iteration 12/1000 | Loss: 0.00007425
Iteration 13/1000 | Loss: 0.00024699
Iteration 14/1000 | Loss: 0.00009484
Iteration 15/1000 | Loss: 0.00025163
Iteration 16/1000 | Loss: 0.00005624
Iteration 17/1000 | Loss: 0.00007593
Iteration 18/1000 | Loss: 0.00004423
Iteration 19/1000 | Loss: 0.00018604
Iteration 20/1000 | Loss: 0.00003945
Iteration 21/1000 | Loss: 0.00024075
Iteration 22/1000 | Loss: 0.00010538
Iteration 23/1000 | Loss: 0.00025536
Iteration 24/1000 | Loss: 0.00031300
Iteration 25/1000 | Loss: 0.00023660
Iteration 26/1000 | Loss: 0.00027052
Iteration 27/1000 | Loss: 0.00015772
Iteration 28/1000 | Loss: 0.00015547
Iteration 29/1000 | Loss: 0.00012229
Iteration 30/1000 | Loss: 0.00010268
Iteration 31/1000 | Loss: 0.00020794
Iteration 32/1000 | Loss: 0.00025506
Iteration 33/1000 | Loss: 0.00005758
Iteration 34/1000 | Loss: 0.00020374
Iteration 35/1000 | Loss: 0.00033089
Iteration 36/1000 | Loss: 0.00038110
Iteration 37/1000 | Loss: 0.00027134
Iteration 38/1000 | Loss: 0.00081753
Iteration 39/1000 | Loss: 0.00011156
Iteration 40/1000 | Loss: 0.00003952
Iteration 41/1000 | Loss: 0.00009897
Iteration 42/1000 | Loss: 0.00007108
Iteration 43/1000 | Loss: 0.00029906
Iteration 44/1000 | Loss: 0.00003988
Iteration 45/1000 | Loss: 0.00005227
Iteration 46/1000 | Loss: 0.00004279
Iteration 47/1000 | Loss: 0.00007882
Iteration 48/1000 | Loss: 0.00022249
Iteration 49/1000 | Loss: 0.00034655
Iteration 50/1000 | Loss: 0.00018956
Iteration 51/1000 | Loss: 0.00004064
Iteration 52/1000 | Loss: 0.00016128
Iteration 53/1000 | Loss: 0.00004341
Iteration 54/1000 | Loss: 0.00005667
Iteration 55/1000 | Loss: 0.00009116
Iteration 56/1000 | Loss: 0.00017646
Iteration 57/1000 | Loss: 0.00027211
Iteration 58/1000 | Loss: 0.00037273
Iteration 59/1000 | Loss: 0.00033567
Iteration 60/1000 | Loss: 0.00035619
Iteration 61/1000 | Loss: 0.00051810
Iteration 62/1000 | Loss: 0.00061924
Iteration 63/1000 | Loss: 0.00038690
Iteration 64/1000 | Loss: 0.00032827
Iteration 65/1000 | Loss: 0.00020246
Iteration 66/1000 | Loss: 0.00014459
Iteration 67/1000 | Loss: 0.00007646
Iteration 68/1000 | Loss: 0.00038948
Iteration 69/1000 | Loss: 0.00026665
Iteration 70/1000 | Loss: 0.00010819
Iteration 71/1000 | Loss: 0.00004592
Iteration 72/1000 | Loss: 0.00003406
Iteration 73/1000 | Loss: 0.00006684
Iteration 74/1000 | Loss: 0.00009067
Iteration 75/1000 | Loss: 0.00011526
Iteration 76/1000 | Loss: 0.00043864
Iteration 77/1000 | Loss: 0.00018060
Iteration 78/1000 | Loss: 0.00003191
Iteration 79/1000 | Loss: 0.00007112
Iteration 80/1000 | Loss: 0.00003687
Iteration 81/1000 | Loss: 0.00005126
Iteration 82/1000 | Loss: 0.00002826
Iteration 83/1000 | Loss: 0.00003766
Iteration 84/1000 | Loss: 0.00002773
Iteration 85/1000 | Loss: 0.00003206
Iteration 86/1000 | Loss: 0.00011206
Iteration 87/1000 | Loss: 0.00022674
Iteration 88/1000 | Loss: 0.00004173
Iteration 89/1000 | Loss: 0.00005957
Iteration 90/1000 | Loss: 0.00003176
Iteration 91/1000 | Loss: 0.00009203
Iteration 92/1000 | Loss: 0.00003442
Iteration 93/1000 | Loss: 0.00019051
Iteration 94/1000 | Loss: 0.00009810
Iteration 95/1000 | Loss: 0.00004523
Iteration 96/1000 | Loss: 0.00006812
Iteration 97/1000 | Loss: 0.00002803
Iteration 98/1000 | Loss: 0.00006044
Iteration 99/1000 | Loss: 0.00002762
Iteration 100/1000 | Loss: 0.00005674
Iteration 101/1000 | Loss: 0.00008782
Iteration 102/1000 | Loss: 0.00003755
Iteration 103/1000 | Loss: 0.00006743
Iteration 104/1000 | Loss: 0.00002700
Iteration 105/1000 | Loss: 0.00002698
Iteration 106/1000 | Loss: 0.00002698
Iteration 107/1000 | Loss: 0.00002697
Iteration 108/1000 | Loss: 0.00002697
Iteration 109/1000 | Loss: 0.00002684
Iteration 110/1000 | Loss: 0.00002684
Iteration 111/1000 | Loss: 0.00002678
Iteration 112/1000 | Loss: 0.00012482
Iteration 113/1000 | Loss: 0.00002753
Iteration 114/1000 | Loss: 0.00008960
Iteration 115/1000 | Loss: 0.00002657
Iteration 116/1000 | Loss: 0.00002719
Iteration 117/1000 | Loss: 0.00002874
Iteration 118/1000 | Loss: 0.00016262
Iteration 119/1000 | Loss: 0.00019589
Iteration 120/1000 | Loss: 0.00028848
Iteration 121/1000 | Loss: 0.00050133
Iteration 122/1000 | Loss: 0.00012817
Iteration 123/1000 | Loss: 0.00003392
Iteration 124/1000 | Loss: 0.00002610
Iteration 125/1000 | Loss: 0.00002564
Iteration 126/1000 | Loss: 0.00002550
Iteration 127/1000 | Loss: 0.00002548
Iteration 128/1000 | Loss: 0.00002546
Iteration 129/1000 | Loss: 0.00002539
Iteration 130/1000 | Loss: 0.00002538
Iteration 131/1000 | Loss: 0.00007345
Iteration 132/1000 | Loss: 0.00002539
Iteration 133/1000 | Loss: 0.00002527
Iteration 134/1000 | Loss: 0.00002519
Iteration 135/1000 | Loss: 0.00002518
Iteration 136/1000 | Loss: 0.00002518
Iteration 137/1000 | Loss: 0.00004984
Iteration 138/1000 | Loss: 0.00002520
Iteration 139/1000 | Loss: 0.00002510
Iteration 140/1000 | Loss: 0.00002507
Iteration 141/1000 | Loss: 0.00002506
Iteration 142/1000 | Loss: 0.00004326
Iteration 143/1000 | Loss: 0.00002499
Iteration 144/1000 | Loss: 0.00002483
Iteration 145/1000 | Loss: 0.00002597
Iteration 146/1000 | Loss: 0.00002597
Iteration 147/1000 | Loss: 0.00005724
Iteration 148/1000 | Loss: 0.00005852
Iteration 149/1000 | Loss: 0.00003721
Iteration 150/1000 | Loss: 0.00002495
Iteration 151/1000 | Loss: 0.00003827
Iteration 152/1000 | Loss: 0.00003326
Iteration 153/1000 | Loss: 0.00002445
Iteration 154/1000 | Loss: 0.00002444
Iteration 155/1000 | Loss: 0.00002444
Iteration 156/1000 | Loss: 0.00002444
Iteration 157/1000 | Loss: 0.00002444
Iteration 158/1000 | Loss: 0.00002444
Iteration 159/1000 | Loss: 0.00002442
Iteration 160/1000 | Loss: 0.00002517
Iteration 161/1000 | Loss: 0.00002436
Iteration 162/1000 | Loss: 0.00002436
Iteration 163/1000 | Loss: 0.00002436
Iteration 164/1000 | Loss: 0.00002435
Iteration 165/1000 | Loss: 0.00002432
Iteration 166/1000 | Loss: 0.00002432
Iteration 167/1000 | Loss: 0.00002431
Iteration 168/1000 | Loss: 0.00002431
Iteration 169/1000 | Loss: 0.00002431
Iteration 170/1000 | Loss: 0.00002430
Iteration 171/1000 | Loss: 0.00002430
Iteration 172/1000 | Loss: 0.00002430
Iteration 173/1000 | Loss: 0.00002429
Iteration 174/1000 | Loss: 0.00002429
Iteration 175/1000 | Loss: 0.00002429
Iteration 176/1000 | Loss: 0.00002429
Iteration 177/1000 | Loss: 0.00002429
Iteration 178/1000 | Loss: 0.00002428
Iteration 179/1000 | Loss: 0.00002428
Iteration 180/1000 | Loss: 0.00002427
Iteration 181/1000 | Loss: 0.00002427
Iteration 182/1000 | Loss: 0.00002427
Iteration 183/1000 | Loss: 0.00002427
Iteration 184/1000 | Loss: 0.00002663
Iteration 185/1000 | Loss: 0.00002520
Iteration 186/1000 | Loss: 0.00002426
Iteration 187/1000 | Loss: 0.00002426
Iteration 188/1000 | Loss: 0.00002426
Iteration 189/1000 | Loss: 0.00002425
Iteration 190/1000 | Loss: 0.00002425
Iteration 191/1000 | Loss: 0.00002425
Iteration 192/1000 | Loss: 0.00002425
Iteration 193/1000 | Loss: 0.00002425
Iteration 194/1000 | Loss: 0.00002425
Iteration 195/1000 | Loss: 0.00002425
Iteration 196/1000 | Loss: 0.00002425
Iteration 197/1000 | Loss: 0.00002425
Iteration 198/1000 | Loss: 0.00002425
Iteration 199/1000 | Loss: 0.00002425
Iteration 200/1000 | Loss: 0.00002425
Iteration 201/1000 | Loss: 0.00002425
Iteration 202/1000 | Loss: 0.00002425
Iteration 203/1000 | Loss: 0.00002425
Iteration 204/1000 | Loss: 0.00002425
Iteration 205/1000 | Loss: 0.00002425
Iteration 206/1000 | Loss: 0.00002425
Iteration 207/1000 | Loss: 0.00002425
Iteration 208/1000 | Loss: 0.00002425
Iteration 209/1000 | Loss: 0.00002425
Iteration 210/1000 | Loss: 0.00002425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.4251909053418785e-05, 2.4251909053418785e-05, 2.4251909053418785e-05, 2.4251909053418785e-05, 2.4251909053418785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4251909053418785e-05

Optimization complete. Final v2v error: 3.680713415145874 mm

Highest mean error: 22.373666763305664 mm for frame 92

Lowest mean error: 2.8566067218780518 mm for frame 36

Saving results

Total time: 231.7583544254303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043061
Iteration 2/25 | Loss: 0.00242792
Iteration 3/25 | Loss: 0.00187115
Iteration 4/25 | Loss: 0.00172181
Iteration 5/25 | Loss: 0.00166671
Iteration 6/25 | Loss: 0.00162329
Iteration 7/25 | Loss: 0.00156601
Iteration 8/25 | Loss: 0.00153019
Iteration 9/25 | Loss: 0.00151728
Iteration 10/25 | Loss: 0.00149808
Iteration 11/25 | Loss: 0.00150873
Iteration 12/25 | Loss: 0.00149050
Iteration 13/25 | Loss: 0.00147538
Iteration 14/25 | Loss: 0.00147430
Iteration 15/25 | Loss: 0.00146435
Iteration 16/25 | Loss: 0.00146078
Iteration 17/25 | Loss: 0.00145933
Iteration 18/25 | Loss: 0.00145820
Iteration 19/25 | Loss: 0.00145741
Iteration 20/25 | Loss: 0.00145866
Iteration 21/25 | Loss: 0.00145525
Iteration 22/25 | Loss: 0.00145490
Iteration 23/25 | Loss: 0.00145477
Iteration 24/25 | Loss: 0.00145473
Iteration 25/25 | Loss: 0.00145473

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33158708
Iteration 2/25 | Loss: 0.00516511
Iteration 3/25 | Loss: 0.00516511
Iteration 4/25 | Loss: 0.00516511
Iteration 5/25 | Loss: 0.00516511
Iteration 6/25 | Loss: 0.00516511
Iteration 7/25 | Loss: 0.00516511
Iteration 8/25 | Loss: 0.00516511
Iteration 9/25 | Loss: 0.00516510
Iteration 10/25 | Loss: 0.00516511
Iteration 11/25 | Loss: 0.00516511
Iteration 12/25 | Loss: 0.00516511
Iteration 13/25 | Loss: 0.00516511
Iteration 14/25 | Loss: 0.00516511
Iteration 15/25 | Loss: 0.00516510
Iteration 16/25 | Loss: 0.00516511
Iteration 17/25 | Loss: 0.00516510
Iteration 18/25 | Loss: 0.00516510
Iteration 19/25 | Loss: 0.00516511
Iteration 20/25 | Loss: 0.00516510
Iteration 21/25 | Loss: 0.00516510
Iteration 22/25 | Loss: 0.00516511
Iteration 23/25 | Loss: 0.00516510
Iteration 24/25 | Loss: 0.00516511
Iteration 25/25 | Loss: 0.00516511

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00516511
Iteration 2/1000 | Loss: 0.00092640
Iteration 3/1000 | Loss: 0.00082044
Iteration 4/1000 | Loss: 0.00043399
Iteration 5/1000 | Loss: 0.00036575
Iteration 6/1000 | Loss: 0.00031237
Iteration 7/1000 | Loss: 0.00027285
Iteration 8/1000 | Loss: 0.00024517
Iteration 9/1000 | Loss: 0.00022330
Iteration 10/1000 | Loss: 0.00020925
Iteration 11/1000 | Loss: 0.00019862
Iteration 12/1000 | Loss: 0.00019270
Iteration 13/1000 | Loss: 0.00018795
Iteration 14/1000 | Loss: 0.00018469
Iteration 15/1000 | Loss: 0.00018152
Iteration 16/1000 | Loss: 0.00017971
Iteration 17/1000 | Loss: 0.00017771
Iteration 18/1000 | Loss: 0.00017599
Iteration 19/1000 | Loss: 0.00017444
Iteration 20/1000 | Loss: 0.00017312
Iteration 21/1000 | Loss: 0.00017200
Iteration 22/1000 | Loss: 0.00037098
Iteration 23/1000 | Loss: 0.00050691
Iteration 24/1000 | Loss: 0.00229841
Iteration 25/1000 | Loss: 0.00435986
Iteration 26/1000 | Loss: 0.00273549
Iteration 27/1000 | Loss: 0.00153289
Iteration 28/1000 | Loss: 0.00066314
Iteration 29/1000 | Loss: 0.00097429
Iteration 30/1000 | Loss: 0.00050713
Iteration 31/1000 | Loss: 0.00063792
Iteration 32/1000 | Loss: 0.00055013
Iteration 33/1000 | Loss: 0.00037090
Iteration 34/1000 | Loss: 0.00031179
Iteration 35/1000 | Loss: 0.00016839
Iteration 36/1000 | Loss: 0.00034596
Iteration 37/1000 | Loss: 0.00037558
Iteration 38/1000 | Loss: 0.00029771
Iteration 39/1000 | Loss: 0.00023157
Iteration 40/1000 | Loss: 0.00031350
Iteration 41/1000 | Loss: 0.00032851
Iteration 42/1000 | Loss: 0.00015373
Iteration 43/1000 | Loss: 0.00024468
Iteration 44/1000 | Loss: 0.00027250
Iteration 45/1000 | Loss: 0.00022817
Iteration 46/1000 | Loss: 0.00022985
Iteration 47/1000 | Loss: 0.00017224
Iteration 48/1000 | Loss: 0.00018709
Iteration 49/1000 | Loss: 0.00023973
Iteration 50/1000 | Loss: 0.00047500
Iteration 51/1000 | Loss: 0.00018368
Iteration 52/1000 | Loss: 0.00042985
Iteration 53/1000 | Loss: 0.00021012
Iteration 54/1000 | Loss: 0.00066856
Iteration 55/1000 | Loss: 0.00118567
Iteration 56/1000 | Loss: 0.00039588
Iteration 57/1000 | Loss: 0.00045264
Iteration 58/1000 | Loss: 0.00045387
Iteration 59/1000 | Loss: 0.00020277
Iteration 60/1000 | Loss: 0.00062330
Iteration 61/1000 | Loss: 0.00027155
Iteration 62/1000 | Loss: 0.00024445
Iteration 63/1000 | Loss: 0.00005961
Iteration 64/1000 | Loss: 0.00036235
Iteration 65/1000 | Loss: 0.00013784
Iteration 66/1000 | Loss: 0.00013635
Iteration 67/1000 | Loss: 0.00012920
Iteration 68/1000 | Loss: 0.00022651
Iteration 69/1000 | Loss: 0.00009681
Iteration 70/1000 | Loss: 0.00016303
Iteration 71/1000 | Loss: 0.00030931
Iteration 72/1000 | Loss: 0.00032887
Iteration 73/1000 | Loss: 0.00006526
Iteration 74/1000 | Loss: 0.00005458
Iteration 75/1000 | Loss: 0.00005000
Iteration 76/1000 | Loss: 0.00004796
Iteration 77/1000 | Loss: 0.00004522
Iteration 78/1000 | Loss: 0.00004361
Iteration 79/1000 | Loss: 0.00004241
Iteration 80/1000 | Loss: 0.00004147
Iteration 81/1000 | Loss: 0.00004081
Iteration 82/1000 | Loss: 0.00004034
Iteration 83/1000 | Loss: 0.00010262
Iteration 84/1000 | Loss: 0.00004552
Iteration 85/1000 | Loss: 0.00004155
Iteration 86/1000 | Loss: 0.00004071
Iteration 87/1000 | Loss: 0.00003970
Iteration 88/1000 | Loss: 0.00007288
Iteration 89/1000 | Loss: 0.00004305
Iteration 90/1000 | Loss: 0.00006913
Iteration 91/1000 | Loss: 0.00009459
Iteration 92/1000 | Loss: 0.00006499
Iteration 93/1000 | Loss: 0.00006950
Iteration 94/1000 | Loss: 0.00010867
Iteration 95/1000 | Loss: 0.00012347
Iteration 96/1000 | Loss: 0.00021194
Iteration 97/1000 | Loss: 0.00004144
Iteration 98/1000 | Loss: 0.00005098
Iteration 99/1000 | Loss: 0.00003867
Iteration 100/1000 | Loss: 0.00003837
Iteration 101/1000 | Loss: 0.00003837
Iteration 102/1000 | Loss: 0.00003809
Iteration 103/1000 | Loss: 0.00003807
Iteration 104/1000 | Loss: 0.00003777
Iteration 105/1000 | Loss: 0.00003741
Iteration 106/1000 | Loss: 0.00003704
Iteration 107/1000 | Loss: 0.00003689
Iteration 108/1000 | Loss: 0.00003677
Iteration 109/1000 | Loss: 0.00003675
Iteration 110/1000 | Loss: 0.00003673
Iteration 111/1000 | Loss: 0.00003672
Iteration 112/1000 | Loss: 0.00003669
Iteration 113/1000 | Loss: 0.00003669
Iteration 114/1000 | Loss: 0.00003668
Iteration 115/1000 | Loss: 0.00003666
Iteration 116/1000 | Loss: 0.00003665
Iteration 117/1000 | Loss: 0.00003659
Iteration 118/1000 | Loss: 0.00003655
Iteration 119/1000 | Loss: 0.00003654
Iteration 120/1000 | Loss: 0.00003654
Iteration 121/1000 | Loss: 0.00003653
Iteration 122/1000 | Loss: 0.00003653
Iteration 123/1000 | Loss: 0.00003653
Iteration 124/1000 | Loss: 0.00003651
Iteration 125/1000 | Loss: 0.00003649
Iteration 126/1000 | Loss: 0.00003649
Iteration 127/1000 | Loss: 0.00003648
Iteration 128/1000 | Loss: 0.00003647
Iteration 129/1000 | Loss: 0.00003647
Iteration 130/1000 | Loss: 0.00003647
Iteration 131/1000 | Loss: 0.00003646
Iteration 132/1000 | Loss: 0.00003645
Iteration 133/1000 | Loss: 0.00003645
Iteration 134/1000 | Loss: 0.00003645
Iteration 135/1000 | Loss: 0.00003645
Iteration 136/1000 | Loss: 0.00003644
Iteration 137/1000 | Loss: 0.00003644
Iteration 138/1000 | Loss: 0.00003643
Iteration 139/1000 | Loss: 0.00003643
Iteration 140/1000 | Loss: 0.00003643
Iteration 141/1000 | Loss: 0.00003642
Iteration 142/1000 | Loss: 0.00003642
Iteration 143/1000 | Loss: 0.00003642
Iteration 144/1000 | Loss: 0.00003642
Iteration 145/1000 | Loss: 0.00003642
Iteration 146/1000 | Loss: 0.00003642
Iteration 147/1000 | Loss: 0.00003642
Iteration 148/1000 | Loss: 0.00003641
Iteration 149/1000 | Loss: 0.00003641
Iteration 150/1000 | Loss: 0.00003641
Iteration 151/1000 | Loss: 0.00003640
Iteration 152/1000 | Loss: 0.00003640
Iteration 153/1000 | Loss: 0.00003640
Iteration 154/1000 | Loss: 0.00003640
Iteration 155/1000 | Loss: 0.00003640
Iteration 156/1000 | Loss: 0.00003640
Iteration 157/1000 | Loss: 0.00003640
Iteration 158/1000 | Loss: 0.00003640
Iteration 159/1000 | Loss: 0.00003639
Iteration 160/1000 | Loss: 0.00003639
Iteration 161/1000 | Loss: 0.00003638
Iteration 162/1000 | Loss: 0.00003638
Iteration 163/1000 | Loss: 0.00003638
Iteration 164/1000 | Loss: 0.00003638
Iteration 165/1000 | Loss: 0.00003638
Iteration 166/1000 | Loss: 0.00003638
Iteration 167/1000 | Loss: 0.00003638
Iteration 168/1000 | Loss: 0.00003638
Iteration 169/1000 | Loss: 0.00003637
Iteration 170/1000 | Loss: 0.00003637
Iteration 171/1000 | Loss: 0.00003637
Iteration 172/1000 | Loss: 0.00003637
Iteration 173/1000 | Loss: 0.00003637
Iteration 174/1000 | Loss: 0.00003637
Iteration 175/1000 | Loss: 0.00003637
Iteration 176/1000 | Loss: 0.00003637
Iteration 177/1000 | Loss: 0.00003637
Iteration 178/1000 | Loss: 0.00003637
Iteration 179/1000 | Loss: 0.00003636
Iteration 180/1000 | Loss: 0.00003636
Iteration 181/1000 | Loss: 0.00003636
Iteration 182/1000 | Loss: 0.00003636
Iteration 183/1000 | Loss: 0.00003636
Iteration 184/1000 | Loss: 0.00003636
Iteration 185/1000 | Loss: 0.00003636
Iteration 186/1000 | Loss: 0.00003636
Iteration 187/1000 | Loss: 0.00003636
Iteration 188/1000 | Loss: 0.00003636
Iteration 189/1000 | Loss: 0.00003636
Iteration 190/1000 | Loss: 0.00003636
Iteration 191/1000 | Loss: 0.00003636
Iteration 192/1000 | Loss: 0.00003636
Iteration 193/1000 | Loss: 0.00003636
Iteration 194/1000 | Loss: 0.00003636
Iteration 195/1000 | Loss: 0.00003636
Iteration 196/1000 | Loss: 0.00003636
Iteration 197/1000 | Loss: 0.00003635
Iteration 198/1000 | Loss: 0.00003635
Iteration 199/1000 | Loss: 0.00003635
Iteration 200/1000 | Loss: 0.00003635
Iteration 201/1000 | Loss: 0.00003635
Iteration 202/1000 | Loss: 0.00003635
Iteration 203/1000 | Loss: 0.00003635
Iteration 204/1000 | Loss: 0.00003635
Iteration 205/1000 | Loss: 0.00003635
Iteration 206/1000 | Loss: 0.00003635
Iteration 207/1000 | Loss: 0.00003635
Iteration 208/1000 | Loss: 0.00003635
Iteration 209/1000 | Loss: 0.00003635
Iteration 210/1000 | Loss: 0.00003635
Iteration 211/1000 | Loss: 0.00003635
Iteration 212/1000 | Loss: 0.00003635
Iteration 213/1000 | Loss: 0.00003635
Iteration 214/1000 | Loss: 0.00003635
Iteration 215/1000 | Loss: 0.00003635
Iteration 216/1000 | Loss: 0.00003635
Iteration 217/1000 | Loss: 0.00003635
Iteration 218/1000 | Loss: 0.00003635
Iteration 219/1000 | Loss: 0.00003635
Iteration 220/1000 | Loss: 0.00003635
Iteration 221/1000 | Loss: 0.00003635
Iteration 222/1000 | Loss: 0.00003635
Iteration 223/1000 | Loss: 0.00003635
Iteration 224/1000 | Loss: 0.00003634
Iteration 225/1000 | Loss: 0.00003634
Iteration 226/1000 | Loss: 0.00003634
Iteration 227/1000 | Loss: 0.00003634
Iteration 228/1000 | Loss: 0.00003634
Iteration 229/1000 | Loss: 0.00003634
Iteration 230/1000 | Loss: 0.00003634
Iteration 231/1000 | Loss: 0.00003634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [3.6344830732559785e-05, 3.6344830732559785e-05, 3.6344830732559785e-05, 3.6344830732559785e-05, 3.6344830732559785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6344830732559785e-05

Optimization complete. Final v2v error: 3.3722681999206543 mm

Highest mean error: 16.071887969970703 mm for frame 190

Lowest mean error: 2.72719669342041 mm for frame 212

Saving results

Total time: 225.68615818023682
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003202
Iteration 2/25 | Loss: 0.00404686
Iteration 3/25 | Loss: 0.00273612
Iteration 4/25 | Loss: 0.00230207
Iteration 5/25 | Loss: 0.00184537
Iteration 6/25 | Loss: 0.00181909
Iteration 7/25 | Loss: 0.00191924
Iteration 8/25 | Loss: 0.00185546
Iteration 9/25 | Loss: 0.00174704
Iteration 10/25 | Loss: 0.00166565
Iteration 11/25 | Loss: 0.00163916
Iteration 12/25 | Loss: 0.00162197
Iteration 13/25 | Loss: 0.00161922
Iteration 14/25 | Loss: 0.00161761
Iteration 15/25 | Loss: 0.00162218
Iteration 16/25 | Loss: 0.00161340
Iteration 17/25 | Loss: 0.00161231
Iteration 18/25 | Loss: 0.00161413
Iteration 19/25 | Loss: 0.00160608
Iteration 20/25 | Loss: 0.00161950
Iteration 21/25 | Loss: 0.00162196
Iteration 22/25 | Loss: 0.00160296
Iteration 23/25 | Loss: 0.00160184
Iteration 24/25 | Loss: 0.00159887
Iteration 25/25 | Loss: 0.00159517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34919095
Iteration 2/25 | Loss: 0.01777784
Iteration 3/25 | Loss: 0.00779519
Iteration 4/25 | Loss: 0.00759943
Iteration 5/25 | Loss: 0.00759942
Iteration 6/25 | Loss: 0.00759942
Iteration 7/25 | Loss: 0.00759942
Iteration 8/25 | Loss: 0.00759942
Iteration 9/25 | Loss: 0.00759942
Iteration 10/25 | Loss: 0.00759942
Iteration 11/25 | Loss: 0.00759942
Iteration 12/25 | Loss: 0.00759942
Iteration 13/25 | Loss: 0.00759942
Iteration 14/25 | Loss: 0.00759942
Iteration 15/25 | Loss: 0.00759942
Iteration 16/25 | Loss: 0.00759942
Iteration 17/25 | Loss: 0.00759942
Iteration 18/25 | Loss: 0.00759942
Iteration 19/25 | Loss: 0.00759942
Iteration 20/25 | Loss: 0.00759942
Iteration 21/25 | Loss: 0.00759942
Iteration 22/25 | Loss: 0.00759942
Iteration 23/25 | Loss: 0.00759942
Iteration 24/25 | Loss: 0.00759942
Iteration 25/25 | Loss: 0.00759942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00759942
Iteration 2/1000 | Loss: 0.00243529
Iteration 3/1000 | Loss: 0.00179646
Iteration 4/1000 | Loss: 0.00060945
Iteration 5/1000 | Loss: 0.01380233
Iteration 6/1000 | Loss: 0.00226292
Iteration 7/1000 | Loss: 0.00133885
Iteration 8/1000 | Loss: 0.00061018
Iteration 9/1000 | Loss: 0.00117336
Iteration 10/1000 | Loss: 0.00065183
Iteration 11/1000 | Loss: 0.00095239
Iteration 12/1000 | Loss: 0.00053182
Iteration 13/1000 | Loss: 0.00038652
Iteration 14/1000 | Loss: 0.00101240
Iteration 15/1000 | Loss: 0.00058430
Iteration 16/1000 | Loss: 0.00096806
Iteration 17/1000 | Loss: 0.00042410
Iteration 18/1000 | Loss: 0.00044132
Iteration 19/1000 | Loss: 0.00044505
Iteration 20/1000 | Loss: 0.00040753
Iteration 21/1000 | Loss: 0.00041837
Iteration 22/1000 | Loss: 0.00034239
Iteration 23/1000 | Loss: 0.00033600
Iteration 24/1000 | Loss: 0.00048796
Iteration 25/1000 | Loss: 0.00043869
Iteration 26/1000 | Loss: 0.00040714
Iteration 27/1000 | Loss: 0.00034463
Iteration 28/1000 | Loss: 0.00068265
Iteration 29/1000 | Loss: 0.00030724
Iteration 30/1000 | Loss: 0.00084234
Iteration 31/1000 | Loss: 0.00116054
Iteration 32/1000 | Loss: 0.00157816
Iteration 33/1000 | Loss: 0.00171910
Iteration 34/1000 | Loss: 0.00073376
Iteration 35/1000 | Loss: 0.00069458
Iteration 36/1000 | Loss: 0.00049953
Iteration 37/1000 | Loss: 0.00037591
Iteration 38/1000 | Loss: 0.00068838
Iteration 39/1000 | Loss: 0.00076390
Iteration 40/1000 | Loss: 0.00054993
Iteration 41/1000 | Loss: 0.00050565
Iteration 42/1000 | Loss: 0.00079869
Iteration 43/1000 | Loss: 0.00116025
Iteration 44/1000 | Loss: 0.00128521
Iteration 45/1000 | Loss: 0.00109590
Iteration 46/1000 | Loss: 0.00045059
Iteration 47/1000 | Loss: 0.00092649
Iteration 48/1000 | Loss: 0.00028881
Iteration 49/1000 | Loss: 0.00027273
Iteration 50/1000 | Loss: 0.00025887
Iteration 51/1000 | Loss: 0.00042745
Iteration 52/1000 | Loss: 0.00025791
Iteration 53/1000 | Loss: 0.00044775
Iteration 54/1000 | Loss: 0.00050801
Iteration 55/1000 | Loss: 0.00105506
Iteration 56/1000 | Loss: 0.00028085
Iteration 57/1000 | Loss: 0.00037522
Iteration 58/1000 | Loss: 0.00146892
Iteration 59/1000 | Loss: 0.00458550
Iteration 60/1000 | Loss: 0.01514097
Iteration 61/1000 | Loss: 0.00737127
Iteration 62/1000 | Loss: 0.00789763
Iteration 63/1000 | Loss: 0.00429594
Iteration 64/1000 | Loss: 0.00078032
Iteration 65/1000 | Loss: 0.00107501
Iteration 66/1000 | Loss: 0.00165275
Iteration 67/1000 | Loss: 0.00259344
Iteration 68/1000 | Loss: 0.00177111
Iteration 69/1000 | Loss: 0.00173274
Iteration 70/1000 | Loss: 0.00133787
Iteration 71/1000 | Loss: 0.00082321
Iteration 72/1000 | Loss: 0.00363415
Iteration 73/1000 | Loss: 0.00380458
Iteration 74/1000 | Loss: 0.00192891
Iteration 75/1000 | Loss: 0.00269940
Iteration 76/1000 | Loss: 0.00190451
Iteration 77/1000 | Loss: 0.00205532
Iteration 78/1000 | Loss: 0.00136983
Iteration 79/1000 | Loss: 0.00124988
Iteration 80/1000 | Loss: 0.00733347
Iteration 81/1000 | Loss: 0.00142594
Iteration 82/1000 | Loss: 0.00077061
Iteration 83/1000 | Loss: 0.00050211
Iteration 84/1000 | Loss: 0.00053242
Iteration 85/1000 | Loss: 0.00135842
Iteration 86/1000 | Loss: 0.00086261
Iteration 87/1000 | Loss: 0.00138889
Iteration 88/1000 | Loss: 0.00512433
Iteration 89/1000 | Loss: 0.00265621
Iteration 90/1000 | Loss: 0.00178209
Iteration 91/1000 | Loss: 0.00356517
Iteration 92/1000 | Loss: 0.00294485
Iteration 93/1000 | Loss: 0.00280379
Iteration 94/1000 | Loss: 0.00192642
Iteration 95/1000 | Loss: 0.00265759
Iteration 96/1000 | Loss: 0.00051026
Iteration 97/1000 | Loss: 0.00051580
Iteration 98/1000 | Loss: 0.00034021
Iteration 99/1000 | Loss: 0.00025080
Iteration 100/1000 | Loss: 0.00022170
Iteration 101/1000 | Loss: 0.00125581
Iteration 102/1000 | Loss: 0.00410002
Iteration 103/1000 | Loss: 0.00069308
Iteration 104/1000 | Loss: 0.00158726
Iteration 105/1000 | Loss: 0.00006087
Iteration 106/1000 | Loss: 0.00008968
Iteration 107/1000 | Loss: 0.00009332
Iteration 108/1000 | Loss: 0.00018105
Iteration 109/1000 | Loss: 0.00028878
Iteration 110/1000 | Loss: 0.00007369
Iteration 111/1000 | Loss: 0.00031399
Iteration 112/1000 | Loss: 0.00004147
Iteration 113/1000 | Loss: 0.00021532
Iteration 114/1000 | Loss: 0.00019582
Iteration 115/1000 | Loss: 0.00253561
Iteration 116/1000 | Loss: 0.00025844
Iteration 117/1000 | Loss: 0.00024553
Iteration 118/1000 | Loss: 0.00011132
Iteration 119/1000 | Loss: 0.00063321
Iteration 120/1000 | Loss: 0.00013053
Iteration 121/1000 | Loss: 0.00013347
Iteration 122/1000 | Loss: 0.00003849
Iteration 123/1000 | Loss: 0.00003774
Iteration 124/1000 | Loss: 0.00062364
Iteration 125/1000 | Loss: 0.00004425
Iteration 126/1000 | Loss: 0.00026448
Iteration 127/1000 | Loss: 0.00005359
Iteration 128/1000 | Loss: 0.00004829
Iteration 129/1000 | Loss: 0.00018664
Iteration 130/1000 | Loss: 0.00021356
Iteration 131/1000 | Loss: 0.00003795
Iteration 132/1000 | Loss: 0.00011182
Iteration 133/1000 | Loss: 0.00018507
Iteration 134/1000 | Loss: 0.00012044
Iteration 135/1000 | Loss: 0.00044774
Iteration 136/1000 | Loss: 0.00003768
Iteration 137/1000 | Loss: 0.00003673
Iteration 138/1000 | Loss: 0.00003659
Iteration 139/1000 | Loss: 0.00003654
Iteration 140/1000 | Loss: 0.00003653
Iteration 141/1000 | Loss: 0.00003651
Iteration 142/1000 | Loss: 0.00003650
Iteration 143/1000 | Loss: 0.00011143
Iteration 144/1000 | Loss: 0.00003922
Iteration 145/1000 | Loss: 0.00010396
Iteration 146/1000 | Loss: 0.00012657
Iteration 147/1000 | Loss: 0.00004544
Iteration 148/1000 | Loss: 0.00005794
Iteration 149/1000 | Loss: 0.00003676
Iteration 150/1000 | Loss: 0.00005291
Iteration 151/1000 | Loss: 0.00003659
Iteration 152/1000 | Loss: 0.00003638
Iteration 153/1000 | Loss: 0.00003619
Iteration 154/1000 | Loss: 0.00010783
Iteration 155/1000 | Loss: 0.00003751
Iteration 156/1000 | Loss: 0.00003627
Iteration 157/1000 | Loss: 0.00003611
Iteration 158/1000 | Loss: 0.00003610
Iteration 159/1000 | Loss: 0.00003609
Iteration 160/1000 | Loss: 0.00003608
Iteration 161/1000 | Loss: 0.00003608
Iteration 162/1000 | Loss: 0.00003608
Iteration 163/1000 | Loss: 0.00003608
Iteration 164/1000 | Loss: 0.00003608
Iteration 165/1000 | Loss: 0.00003608
Iteration 166/1000 | Loss: 0.00003607
Iteration 167/1000 | Loss: 0.00003607
Iteration 168/1000 | Loss: 0.00003607
Iteration 169/1000 | Loss: 0.00003607
Iteration 170/1000 | Loss: 0.00003607
Iteration 171/1000 | Loss: 0.00003607
Iteration 172/1000 | Loss: 0.00003607
Iteration 173/1000 | Loss: 0.00003606
Iteration 174/1000 | Loss: 0.00003606
Iteration 175/1000 | Loss: 0.00003606
Iteration 176/1000 | Loss: 0.00003606
Iteration 177/1000 | Loss: 0.00003606
Iteration 178/1000 | Loss: 0.00003606
Iteration 179/1000 | Loss: 0.00003606
Iteration 180/1000 | Loss: 0.00003606
Iteration 181/1000 | Loss: 0.00003606
Iteration 182/1000 | Loss: 0.00003606
Iteration 183/1000 | Loss: 0.00003606
Iteration 184/1000 | Loss: 0.00003606
Iteration 185/1000 | Loss: 0.00003606
Iteration 186/1000 | Loss: 0.00003606
Iteration 187/1000 | Loss: 0.00003606
Iteration 188/1000 | Loss: 0.00003606
Iteration 189/1000 | Loss: 0.00003606
Iteration 190/1000 | Loss: 0.00003606
Iteration 191/1000 | Loss: 0.00003606
Iteration 192/1000 | Loss: 0.00003606
Iteration 193/1000 | Loss: 0.00003606
Iteration 194/1000 | Loss: 0.00003606
Iteration 195/1000 | Loss: 0.00003606
Iteration 196/1000 | Loss: 0.00003606
Iteration 197/1000 | Loss: 0.00003606
Iteration 198/1000 | Loss: 0.00003606
Iteration 199/1000 | Loss: 0.00003606
Iteration 200/1000 | Loss: 0.00003606
Iteration 201/1000 | Loss: 0.00003606
Iteration 202/1000 | Loss: 0.00003606
Iteration 203/1000 | Loss: 0.00003606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [3.6056611861567944e-05, 3.6056611861567944e-05, 3.6056611861567944e-05, 3.6056611861567944e-05, 3.6056611861567944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6056611861567944e-05

Optimization complete. Final v2v error: 3.950270891189575 mm

Highest mean error: 11.583322525024414 mm for frame 78

Lowest mean error: 3.2151060104370117 mm for frame 14

Saving results

Total time: 296.4240827560425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01135293
Iteration 2/25 | Loss: 0.00365908
Iteration 3/25 | Loss: 0.00312674
Iteration 4/25 | Loss: 0.00183214
Iteration 5/25 | Loss: 0.00182782
Iteration 6/25 | Loss: 0.00161243
Iteration 7/25 | Loss: 0.00148924
Iteration 8/25 | Loss: 0.00144969
Iteration 9/25 | Loss: 0.00149110
Iteration 10/25 | Loss: 0.00151627
Iteration 11/25 | Loss: 0.00145245
Iteration 12/25 | Loss: 0.00142904
Iteration 13/25 | Loss: 0.00146049
Iteration 14/25 | Loss: 0.00142396
Iteration 15/25 | Loss: 0.00144877
Iteration 16/25 | Loss: 0.00142927
Iteration 17/25 | Loss: 0.00140722
Iteration 18/25 | Loss: 0.00141161
Iteration 19/25 | Loss: 0.00140063
Iteration 20/25 | Loss: 0.00139682
Iteration 21/25 | Loss: 0.00139592
Iteration 22/25 | Loss: 0.00139573
Iteration 23/25 | Loss: 0.00139572
Iteration 24/25 | Loss: 0.00139571
Iteration 25/25 | Loss: 0.00139571

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.42211944
Iteration 2/25 | Loss: 0.00726239
Iteration 3/25 | Loss: 0.00053685
Iteration 4/25 | Loss: 0.00053685
Iteration 5/25 | Loss: 0.00053685
Iteration 6/25 | Loss: 0.00053685
Iteration 7/25 | Loss: 0.00053685
Iteration 8/25 | Loss: 0.00053685
Iteration 9/25 | Loss: 0.00053685
Iteration 10/25 | Loss: 0.00053685
Iteration 11/25 | Loss: 0.00053685
Iteration 12/25 | Loss: 0.00053685
Iteration 13/25 | Loss: 0.00053685
Iteration 14/25 | Loss: 0.00053685
Iteration 15/25 | Loss: 0.00053685
Iteration 16/25 | Loss: 0.00053685
Iteration 17/25 | Loss: 0.00053685
Iteration 18/25 | Loss: 0.00053685
Iteration 19/25 | Loss: 0.00053685
Iteration 20/25 | Loss: 0.00053685
Iteration 21/25 | Loss: 0.00053685
Iteration 22/25 | Loss: 0.00053685
Iteration 23/25 | Loss: 0.00053685
Iteration 24/25 | Loss: 0.00053685
Iteration 25/25 | Loss: 0.00053685

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053685
Iteration 2/1000 | Loss: 0.00298617
Iteration 3/1000 | Loss: 0.00092790
Iteration 4/1000 | Loss: 0.00040385
Iteration 5/1000 | Loss: 0.00027553
Iteration 6/1000 | Loss: 0.00018826
Iteration 7/1000 | Loss: 0.00016247
Iteration 8/1000 | Loss: 0.00014505
Iteration 9/1000 | Loss: 0.00022337
Iteration 10/1000 | Loss: 0.00093053
Iteration 11/1000 | Loss: 0.00074738
Iteration 12/1000 | Loss: 0.00016865
Iteration 13/1000 | Loss: 0.00012964
Iteration 14/1000 | Loss: 0.00011042
Iteration 15/1000 | Loss: 0.00009450
Iteration 16/1000 | Loss: 0.00008338
Iteration 17/1000 | Loss: 0.00007533
Iteration 18/1000 | Loss: 0.00006897
Iteration 19/1000 | Loss: 0.00015589
Iteration 20/1000 | Loss: 0.00007029
Iteration 21/1000 | Loss: 0.00006614
Iteration 22/1000 | Loss: 0.00006139
Iteration 23/1000 | Loss: 0.00005945
Iteration 24/1000 | Loss: 0.00005797
Iteration 25/1000 | Loss: 0.00005672
Iteration 26/1000 | Loss: 0.00005590
Iteration 27/1000 | Loss: 0.00005478
Iteration 28/1000 | Loss: 0.00005394
Iteration 29/1000 | Loss: 0.00005340
Iteration 30/1000 | Loss: 0.00014298
Iteration 31/1000 | Loss: 0.00006195
Iteration 32/1000 | Loss: 0.00005856
Iteration 33/1000 | Loss: 0.00005730
Iteration 34/1000 | Loss: 0.00005663
Iteration 35/1000 | Loss: 0.00005607
Iteration 36/1000 | Loss: 0.00005511
Iteration 37/1000 | Loss: 0.00012922
Iteration 38/1000 | Loss: 0.00005661
Iteration 39/1000 | Loss: 0.00005518
Iteration 40/1000 | Loss: 0.00005439
Iteration 41/1000 | Loss: 0.00005343
Iteration 42/1000 | Loss: 0.00005287
Iteration 43/1000 | Loss: 0.00005251
Iteration 44/1000 | Loss: 0.00005240
Iteration 45/1000 | Loss: 0.00005238
Iteration 46/1000 | Loss: 0.00005238
Iteration 47/1000 | Loss: 0.00005237
Iteration 48/1000 | Loss: 0.00005237
Iteration 49/1000 | Loss: 0.00005237
Iteration 50/1000 | Loss: 0.00005235
Iteration 51/1000 | Loss: 0.00005235
Iteration 52/1000 | Loss: 0.00005232
Iteration 53/1000 | Loss: 0.00005227
Iteration 54/1000 | Loss: 0.00005227
Iteration 55/1000 | Loss: 0.00005224
Iteration 56/1000 | Loss: 0.00005224
Iteration 57/1000 | Loss: 0.00005223
Iteration 58/1000 | Loss: 0.00005222
Iteration 59/1000 | Loss: 0.00005221
Iteration 60/1000 | Loss: 0.00005221
Iteration 61/1000 | Loss: 0.00005220
Iteration 62/1000 | Loss: 0.00005220
Iteration 63/1000 | Loss: 0.00005210
Iteration 64/1000 | Loss: 0.00005201
Iteration 65/1000 | Loss: 0.00005195
Iteration 66/1000 | Loss: 0.00007557
Iteration 67/1000 | Loss: 0.00007089
Iteration 68/1000 | Loss: 0.00006883
Iteration 69/1000 | Loss: 0.00005710
Iteration 70/1000 | Loss: 0.00005603
Iteration 71/1000 | Loss: 0.00005378
Iteration 72/1000 | Loss: 0.00005272
Iteration 73/1000 | Loss: 0.00005233
Iteration 74/1000 | Loss: 0.00005215
Iteration 75/1000 | Loss: 0.00005212
Iteration 76/1000 | Loss: 0.00005193
Iteration 77/1000 | Loss: 0.00005174
Iteration 78/1000 | Loss: 0.00005161
Iteration 79/1000 | Loss: 0.00005151
Iteration 80/1000 | Loss: 0.00005146
Iteration 81/1000 | Loss: 0.00005142
Iteration 82/1000 | Loss: 0.00005142
Iteration 83/1000 | Loss: 0.00005142
Iteration 84/1000 | Loss: 0.00005142
Iteration 85/1000 | Loss: 0.00005142
Iteration 86/1000 | Loss: 0.00005142
Iteration 87/1000 | Loss: 0.00005141
Iteration 88/1000 | Loss: 0.00005141
Iteration 89/1000 | Loss: 0.00005141
Iteration 90/1000 | Loss: 0.00005141
Iteration 91/1000 | Loss: 0.00005141
Iteration 92/1000 | Loss: 0.00005141
Iteration 93/1000 | Loss: 0.00005141
Iteration 94/1000 | Loss: 0.00005141
Iteration 95/1000 | Loss: 0.00005140
Iteration 96/1000 | Loss: 0.00005138
Iteration 97/1000 | Loss: 0.00005138
Iteration 98/1000 | Loss: 0.00005138
Iteration 99/1000 | Loss: 0.00005138
Iteration 100/1000 | Loss: 0.00005138
Iteration 101/1000 | Loss: 0.00005138
Iteration 102/1000 | Loss: 0.00005137
Iteration 103/1000 | Loss: 0.00005137
Iteration 104/1000 | Loss: 0.00005137
Iteration 105/1000 | Loss: 0.00005137
Iteration 106/1000 | Loss: 0.00005133
Iteration 107/1000 | Loss: 0.00005133
Iteration 108/1000 | Loss: 0.00005133
Iteration 109/1000 | Loss: 0.00005133
Iteration 110/1000 | Loss: 0.00005133
Iteration 111/1000 | Loss: 0.00005133
Iteration 112/1000 | Loss: 0.00005133
Iteration 113/1000 | Loss: 0.00005133
Iteration 114/1000 | Loss: 0.00005133
Iteration 115/1000 | Loss: 0.00005133
Iteration 116/1000 | Loss: 0.00005133
Iteration 117/1000 | Loss: 0.00005132
Iteration 118/1000 | Loss: 0.00005132
Iteration 119/1000 | Loss: 0.00005131
Iteration 120/1000 | Loss: 0.00005131
Iteration 121/1000 | Loss: 0.00005130
Iteration 122/1000 | Loss: 0.00005130
Iteration 123/1000 | Loss: 0.00005130
Iteration 124/1000 | Loss: 0.00005128
Iteration 125/1000 | Loss: 0.00005128
Iteration 126/1000 | Loss: 0.00005127
Iteration 127/1000 | Loss: 0.00005126
Iteration 128/1000 | Loss: 0.00005126
Iteration 129/1000 | Loss: 0.00005126
Iteration 130/1000 | Loss: 0.00005126
Iteration 131/1000 | Loss: 0.00005126
Iteration 132/1000 | Loss: 0.00005125
Iteration 133/1000 | Loss: 0.00005125
Iteration 134/1000 | Loss: 0.00005125
Iteration 135/1000 | Loss: 0.00005122
Iteration 136/1000 | Loss: 0.00005122
Iteration 137/1000 | Loss: 0.00005122
Iteration 138/1000 | Loss: 0.00005122
Iteration 139/1000 | Loss: 0.00005122
Iteration 140/1000 | Loss: 0.00005122
Iteration 141/1000 | Loss: 0.00005122
Iteration 142/1000 | Loss: 0.00005122
Iteration 143/1000 | Loss: 0.00005121
Iteration 144/1000 | Loss: 0.00005121
Iteration 145/1000 | Loss: 0.00005121
Iteration 146/1000 | Loss: 0.00005121
Iteration 147/1000 | Loss: 0.00005117
Iteration 148/1000 | Loss: 0.00005116
Iteration 149/1000 | Loss: 0.00005116
Iteration 150/1000 | Loss: 0.00005115
Iteration 151/1000 | Loss: 0.00005115
Iteration 152/1000 | Loss: 0.00005115
Iteration 153/1000 | Loss: 0.00005115
Iteration 154/1000 | Loss: 0.00005115
Iteration 155/1000 | Loss: 0.00005115
Iteration 156/1000 | Loss: 0.00005115
Iteration 157/1000 | Loss: 0.00005115
Iteration 158/1000 | Loss: 0.00005115
Iteration 159/1000 | Loss: 0.00005115
Iteration 160/1000 | Loss: 0.00005115
Iteration 161/1000 | Loss: 0.00005114
Iteration 162/1000 | Loss: 0.00005114
Iteration 163/1000 | Loss: 0.00005114
Iteration 164/1000 | Loss: 0.00005114
Iteration 165/1000 | Loss: 0.00005114
Iteration 166/1000 | Loss: 0.00005114
Iteration 167/1000 | Loss: 0.00005114
Iteration 168/1000 | Loss: 0.00005114
Iteration 169/1000 | Loss: 0.00005114
Iteration 170/1000 | Loss: 0.00005113
Iteration 171/1000 | Loss: 0.00005113
Iteration 172/1000 | Loss: 0.00005113
Iteration 173/1000 | Loss: 0.00005113
Iteration 174/1000 | Loss: 0.00005113
Iteration 175/1000 | Loss: 0.00005113
Iteration 176/1000 | Loss: 0.00005113
Iteration 177/1000 | Loss: 0.00005113
Iteration 178/1000 | Loss: 0.00005112
Iteration 179/1000 | Loss: 0.00005112
Iteration 180/1000 | Loss: 0.00005112
Iteration 181/1000 | Loss: 0.00005112
Iteration 182/1000 | Loss: 0.00005112
Iteration 183/1000 | Loss: 0.00005112
Iteration 184/1000 | Loss: 0.00005112
Iteration 185/1000 | Loss: 0.00005111
Iteration 186/1000 | Loss: 0.00005111
Iteration 187/1000 | Loss: 0.00005111
Iteration 188/1000 | Loss: 0.00005111
Iteration 189/1000 | Loss: 0.00005111
Iteration 190/1000 | Loss: 0.00005110
Iteration 191/1000 | Loss: 0.00005109
Iteration 192/1000 | Loss: 0.00005109
Iteration 193/1000 | Loss: 0.00005108
Iteration 194/1000 | Loss: 0.00005108
Iteration 195/1000 | Loss: 0.00005108
Iteration 196/1000 | Loss: 0.00005107
Iteration 197/1000 | Loss: 0.00005107
Iteration 198/1000 | Loss: 0.00005107
Iteration 199/1000 | Loss: 0.00005107
Iteration 200/1000 | Loss: 0.00005106
Iteration 201/1000 | Loss: 0.00005106
Iteration 202/1000 | Loss: 0.00005106
Iteration 203/1000 | Loss: 0.00005106
Iteration 204/1000 | Loss: 0.00005106
Iteration 205/1000 | Loss: 0.00005106
Iteration 206/1000 | Loss: 0.00005106
Iteration 207/1000 | Loss: 0.00005106
Iteration 208/1000 | Loss: 0.00005106
Iteration 209/1000 | Loss: 0.00005106
Iteration 210/1000 | Loss: 0.00005106
Iteration 211/1000 | Loss: 0.00005106
Iteration 212/1000 | Loss: 0.00005106
Iteration 213/1000 | Loss: 0.00005106
Iteration 214/1000 | Loss: 0.00005106
Iteration 215/1000 | Loss: 0.00005106
Iteration 216/1000 | Loss: 0.00005106
Iteration 217/1000 | Loss: 0.00005106
Iteration 218/1000 | Loss: 0.00005106
Iteration 219/1000 | Loss: 0.00005106
Iteration 220/1000 | Loss: 0.00005106
Iteration 221/1000 | Loss: 0.00005106
Iteration 222/1000 | Loss: 0.00005106
Iteration 223/1000 | Loss: 0.00005106
Iteration 224/1000 | Loss: 0.00005106
Iteration 225/1000 | Loss: 0.00005106
Iteration 226/1000 | Loss: 0.00005105
Iteration 227/1000 | Loss: 0.00005105
Iteration 228/1000 | Loss: 0.00005105
Iteration 229/1000 | Loss: 0.00005105
Iteration 230/1000 | Loss: 0.00005105
Iteration 231/1000 | Loss: 0.00005105
Iteration 232/1000 | Loss: 0.00005105
Iteration 233/1000 | Loss: 0.00005105
Iteration 234/1000 | Loss: 0.00005105
Iteration 235/1000 | Loss: 0.00005105
Iteration 236/1000 | Loss: 0.00005104
Iteration 237/1000 | Loss: 0.00005104
Iteration 238/1000 | Loss: 0.00005104
Iteration 239/1000 | Loss: 0.00005104
Iteration 240/1000 | Loss: 0.00005104
Iteration 241/1000 | Loss: 0.00005104
Iteration 242/1000 | Loss: 0.00005104
Iteration 243/1000 | Loss: 0.00005104
Iteration 244/1000 | Loss: 0.00005104
Iteration 245/1000 | Loss: 0.00005104
Iteration 246/1000 | Loss: 0.00005104
Iteration 247/1000 | Loss: 0.00005104
Iteration 248/1000 | Loss: 0.00005104
Iteration 249/1000 | Loss: 0.00005104
Iteration 250/1000 | Loss: 0.00005104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [5.1043130952166393e-05, 5.1043130952166393e-05, 5.1043130952166393e-05, 5.1043130952166393e-05, 5.1043130952166393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.1043130952166393e-05

Optimization complete. Final v2v error: 5.742116451263428 mm

Highest mean error: 6.860561847686768 mm for frame 43

Lowest mean error: 4.960685729980469 mm for frame 121

Saving results

Total time: 138.64941930770874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423503
Iteration 2/25 | Loss: 0.00119254
Iteration 3/25 | Loss: 0.00097586
Iteration 4/25 | Loss: 0.00095350
Iteration 5/25 | Loss: 0.00094884
Iteration 6/25 | Loss: 0.00094725
Iteration 7/25 | Loss: 0.00094700
Iteration 8/25 | Loss: 0.00094700
Iteration 9/25 | Loss: 0.00094700
Iteration 10/25 | Loss: 0.00094700
Iteration 11/25 | Loss: 0.00094700
Iteration 12/25 | Loss: 0.00094700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009469997021369636, 0.0009469997021369636, 0.0009469997021369636, 0.0009469997021369636, 0.0009469997021369636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009469997021369636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.35263252
Iteration 2/25 | Loss: 0.00052878
Iteration 3/25 | Loss: 0.00052875
Iteration 4/25 | Loss: 0.00052875
Iteration 5/25 | Loss: 0.00052875
Iteration 6/25 | Loss: 0.00052875
Iteration 7/25 | Loss: 0.00052875
Iteration 8/25 | Loss: 0.00052875
Iteration 9/25 | Loss: 0.00052875
Iteration 10/25 | Loss: 0.00052875
Iteration 11/25 | Loss: 0.00052875
Iteration 12/25 | Loss: 0.00052875
Iteration 13/25 | Loss: 0.00052875
Iteration 14/25 | Loss: 0.00052875
Iteration 15/25 | Loss: 0.00052875
Iteration 16/25 | Loss: 0.00052875
Iteration 17/25 | Loss: 0.00052875
Iteration 18/25 | Loss: 0.00052875
Iteration 19/25 | Loss: 0.00052875
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005287476815283298, 0.0005287476815283298, 0.0005287476815283298, 0.0005287476815283298, 0.0005287476815283298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005287476815283298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052875
Iteration 2/1000 | Loss: 0.00004387
Iteration 3/1000 | Loss: 0.00002593
Iteration 4/1000 | Loss: 0.00002237
Iteration 5/1000 | Loss: 0.00002103
Iteration 6/1000 | Loss: 0.00002022
Iteration 7/1000 | Loss: 0.00001944
Iteration 8/1000 | Loss: 0.00001907
Iteration 9/1000 | Loss: 0.00001867
Iteration 10/1000 | Loss: 0.00001838
Iteration 11/1000 | Loss: 0.00001835
Iteration 12/1000 | Loss: 0.00001827
Iteration 13/1000 | Loss: 0.00001821
Iteration 14/1000 | Loss: 0.00001816
Iteration 15/1000 | Loss: 0.00001814
Iteration 16/1000 | Loss: 0.00001813
Iteration 17/1000 | Loss: 0.00001812
Iteration 18/1000 | Loss: 0.00001811
Iteration 19/1000 | Loss: 0.00001809
Iteration 20/1000 | Loss: 0.00001808
Iteration 21/1000 | Loss: 0.00001808
Iteration 22/1000 | Loss: 0.00001807
Iteration 23/1000 | Loss: 0.00001807
Iteration 24/1000 | Loss: 0.00001804
Iteration 25/1000 | Loss: 0.00001803
Iteration 26/1000 | Loss: 0.00001803
Iteration 27/1000 | Loss: 0.00001803
Iteration 28/1000 | Loss: 0.00001803
Iteration 29/1000 | Loss: 0.00001803
Iteration 30/1000 | Loss: 0.00001803
Iteration 31/1000 | Loss: 0.00001802
Iteration 32/1000 | Loss: 0.00001801
Iteration 33/1000 | Loss: 0.00001801
Iteration 34/1000 | Loss: 0.00001801
Iteration 35/1000 | Loss: 0.00001801
Iteration 36/1000 | Loss: 0.00001801
Iteration 37/1000 | Loss: 0.00001800
Iteration 38/1000 | Loss: 0.00001800
Iteration 39/1000 | Loss: 0.00001799
Iteration 40/1000 | Loss: 0.00001799
Iteration 41/1000 | Loss: 0.00001798
Iteration 42/1000 | Loss: 0.00001798
Iteration 43/1000 | Loss: 0.00001798
Iteration 44/1000 | Loss: 0.00001797
Iteration 45/1000 | Loss: 0.00001797
Iteration 46/1000 | Loss: 0.00001797
Iteration 47/1000 | Loss: 0.00001796
Iteration 48/1000 | Loss: 0.00001796
Iteration 49/1000 | Loss: 0.00001796
Iteration 50/1000 | Loss: 0.00001795
Iteration 51/1000 | Loss: 0.00001795
Iteration 52/1000 | Loss: 0.00001794
Iteration 53/1000 | Loss: 0.00001794
Iteration 54/1000 | Loss: 0.00001793
Iteration 55/1000 | Loss: 0.00001793
Iteration 56/1000 | Loss: 0.00001792
Iteration 57/1000 | Loss: 0.00001792
Iteration 58/1000 | Loss: 0.00001792
Iteration 59/1000 | Loss: 0.00001792
Iteration 60/1000 | Loss: 0.00001792
Iteration 61/1000 | Loss: 0.00001791
Iteration 62/1000 | Loss: 0.00001791
Iteration 63/1000 | Loss: 0.00001791
Iteration 64/1000 | Loss: 0.00001790
Iteration 65/1000 | Loss: 0.00001790
Iteration 66/1000 | Loss: 0.00001790
Iteration 67/1000 | Loss: 0.00001790
Iteration 68/1000 | Loss: 0.00001790
Iteration 69/1000 | Loss: 0.00001790
Iteration 70/1000 | Loss: 0.00001789
Iteration 71/1000 | Loss: 0.00001789
Iteration 72/1000 | Loss: 0.00001789
Iteration 73/1000 | Loss: 0.00001789
Iteration 74/1000 | Loss: 0.00001789
Iteration 75/1000 | Loss: 0.00001789
Iteration 76/1000 | Loss: 0.00001788
Iteration 77/1000 | Loss: 0.00001788
Iteration 78/1000 | Loss: 0.00001788
Iteration 79/1000 | Loss: 0.00001787
Iteration 80/1000 | Loss: 0.00001787
Iteration 81/1000 | Loss: 0.00001787
Iteration 82/1000 | Loss: 0.00001787
Iteration 83/1000 | Loss: 0.00001787
Iteration 84/1000 | Loss: 0.00001787
Iteration 85/1000 | Loss: 0.00001786
Iteration 86/1000 | Loss: 0.00001786
Iteration 87/1000 | Loss: 0.00001786
Iteration 88/1000 | Loss: 0.00001786
Iteration 89/1000 | Loss: 0.00001786
Iteration 90/1000 | Loss: 0.00001786
Iteration 91/1000 | Loss: 0.00001786
Iteration 92/1000 | Loss: 0.00001785
Iteration 93/1000 | Loss: 0.00001785
Iteration 94/1000 | Loss: 0.00001785
Iteration 95/1000 | Loss: 0.00001785
Iteration 96/1000 | Loss: 0.00001785
Iteration 97/1000 | Loss: 0.00001785
Iteration 98/1000 | Loss: 0.00001785
Iteration 99/1000 | Loss: 0.00001785
Iteration 100/1000 | Loss: 0.00001785
Iteration 101/1000 | Loss: 0.00001785
Iteration 102/1000 | Loss: 0.00001785
Iteration 103/1000 | Loss: 0.00001785
Iteration 104/1000 | Loss: 0.00001784
Iteration 105/1000 | Loss: 0.00001784
Iteration 106/1000 | Loss: 0.00001784
Iteration 107/1000 | Loss: 0.00001784
Iteration 108/1000 | Loss: 0.00001784
Iteration 109/1000 | Loss: 0.00001784
Iteration 110/1000 | Loss: 0.00001784
Iteration 111/1000 | Loss: 0.00001784
Iteration 112/1000 | Loss: 0.00001784
Iteration 113/1000 | Loss: 0.00001784
Iteration 114/1000 | Loss: 0.00001784
Iteration 115/1000 | Loss: 0.00001783
Iteration 116/1000 | Loss: 0.00001783
Iteration 117/1000 | Loss: 0.00001783
Iteration 118/1000 | Loss: 0.00001783
Iteration 119/1000 | Loss: 0.00001783
Iteration 120/1000 | Loss: 0.00001783
Iteration 121/1000 | Loss: 0.00001783
Iteration 122/1000 | Loss: 0.00001783
Iteration 123/1000 | Loss: 0.00001783
Iteration 124/1000 | Loss: 0.00001783
Iteration 125/1000 | Loss: 0.00001783
Iteration 126/1000 | Loss: 0.00001782
Iteration 127/1000 | Loss: 0.00001782
Iteration 128/1000 | Loss: 0.00001782
Iteration 129/1000 | Loss: 0.00001782
Iteration 130/1000 | Loss: 0.00001782
Iteration 131/1000 | Loss: 0.00001782
Iteration 132/1000 | Loss: 0.00001782
Iteration 133/1000 | Loss: 0.00001782
Iteration 134/1000 | Loss: 0.00001782
Iteration 135/1000 | Loss: 0.00001782
Iteration 136/1000 | Loss: 0.00001781
Iteration 137/1000 | Loss: 0.00001781
Iteration 138/1000 | Loss: 0.00001781
Iteration 139/1000 | Loss: 0.00001781
Iteration 140/1000 | Loss: 0.00001781
Iteration 141/1000 | Loss: 0.00001780
Iteration 142/1000 | Loss: 0.00001780
Iteration 143/1000 | Loss: 0.00001780
Iteration 144/1000 | Loss: 0.00001780
Iteration 145/1000 | Loss: 0.00001779
Iteration 146/1000 | Loss: 0.00001779
Iteration 147/1000 | Loss: 0.00001779
Iteration 148/1000 | Loss: 0.00001779
Iteration 149/1000 | Loss: 0.00001779
Iteration 150/1000 | Loss: 0.00001779
Iteration 151/1000 | Loss: 0.00001779
Iteration 152/1000 | Loss: 0.00001779
Iteration 153/1000 | Loss: 0.00001778
Iteration 154/1000 | Loss: 0.00001778
Iteration 155/1000 | Loss: 0.00001778
Iteration 156/1000 | Loss: 0.00001778
Iteration 157/1000 | Loss: 0.00001778
Iteration 158/1000 | Loss: 0.00001777
Iteration 159/1000 | Loss: 0.00001777
Iteration 160/1000 | Loss: 0.00001777
Iteration 161/1000 | Loss: 0.00001777
Iteration 162/1000 | Loss: 0.00001777
Iteration 163/1000 | Loss: 0.00001777
Iteration 164/1000 | Loss: 0.00001777
Iteration 165/1000 | Loss: 0.00001777
Iteration 166/1000 | Loss: 0.00001777
Iteration 167/1000 | Loss: 0.00001776
Iteration 168/1000 | Loss: 0.00001776
Iteration 169/1000 | Loss: 0.00001776
Iteration 170/1000 | Loss: 0.00001776
Iteration 171/1000 | Loss: 0.00001776
Iteration 172/1000 | Loss: 0.00001776
Iteration 173/1000 | Loss: 0.00001776
Iteration 174/1000 | Loss: 0.00001776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.776272983988747e-05, 1.776272983988747e-05, 1.776272983988747e-05, 1.776272983988747e-05, 1.776272983988747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.776272983988747e-05

Optimization complete. Final v2v error: 3.603454351425171 mm

Highest mean error: 4.366159915924072 mm for frame 144

Lowest mean error: 3.2790846824645996 mm for frame 51

Saving results

Total time: 38.715131759643555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387705
Iteration 2/25 | Loss: 0.00103440
Iteration 3/25 | Loss: 0.00092219
Iteration 4/25 | Loss: 0.00091138
Iteration 5/25 | Loss: 0.00090895
Iteration 6/25 | Loss: 0.00090844
Iteration 7/25 | Loss: 0.00090844
Iteration 8/25 | Loss: 0.00090844
Iteration 9/25 | Loss: 0.00090844
Iteration 10/25 | Loss: 0.00090844
Iteration 11/25 | Loss: 0.00090844
Iteration 12/25 | Loss: 0.00090844
Iteration 13/25 | Loss: 0.00090844
Iteration 14/25 | Loss: 0.00090844
Iteration 15/25 | Loss: 0.00090844
Iteration 16/25 | Loss: 0.00090844
Iteration 17/25 | Loss: 0.00090844
Iteration 18/25 | Loss: 0.00090844
Iteration 19/25 | Loss: 0.00090844
Iteration 20/25 | Loss: 0.00090844
Iteration 21/25 | Loss: 0.00090844
Iteration 22/25 | Loss: 0.00090844
Iteration 23/25 | Loss: 0.00090844
Iteration 24/25 | Loss: 0.00090844
Iteration 25/25 | Loss: 0.00090844

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36973333
Iteration 2/25 | Loss: 0.00060442
Iteration 3/25 | Loss: 0.00060442
Iteration 4/25 | Loss: 0.00060442
Iteration 5/25 | Loss: 0.00060442
Iteration 6/25 | Loss: 0.00060442
Iteration 7/25 | Loss: 0.00060442
Iteration 8/25 | Loss: 0.00060442
Iteration 9/25 | Loss: 0.00060442
Iteration 10/25 | Loss: 0.00060442
Iteration 11/25 | Loss: 0.00060442
Iteration 12/25 | Loss: 0.00060442
Iteration 13/25 | Loss: 0.00060442
Iteration 14/25 | Loss: 0.00060442
Iteration 15/25 | Loss: 0.00060442
Iteration 16/25 | Loss: 0.00060442
Iteration 17/25 | Loss: 0.00060442
Iteration 18/25 | Loss: 0.00060442
Iteration 19/25 | Loss: 0.00060442
Iteration 20/25 | Loss: 0.00060442
Iteration 21/25 | Loss: 0.00060442
Iteration 22/25 | Loss: 0.00060442
Iteration 23/25 | Loss: 0.00060442
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006044207839295268, 0.0006044207839295268, 0.0006044207839295268, 0.0006044207839295268, 0.0006044207839295268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006044207839295268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060442
Iteration 2/1000 | Loss: 0.00004153
Iteration 3/1000 | Loss: 0.00002031
Iteration 4/1000 | Loss: 0.00001612
Iteration 5/1000 | Loss: 0.00001489
Iteration 6/1000 | Loss: 0.00001394
Iteration 7/1000 | Loss: 0.00001318
Iteration 8/1000 | Loss: 0.00001286
Iteration 9/1000 | Loss: 0.00001254
Iteration 10/1000 | Loss: 0.00001232
Iteration 11/1000 | Loss: 0.00001228
Iteration 12/1000 | Loss: 0.00001217
Iteration 13/1000 | Loss: 0.00001215
Iteration 14/1000 | Loss: 0.00001211
Iteration 15/1000 | Loss: 0.00001211
Iteration 16/1000 | Loss: 0.00001210
Iteration 17/1000 | Loss: 0.00001210
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001208
Iteration 20/1000 | Loss: 0.00001207
Iteration 21/1000 | Loss: 0.00001206
Iteration 22/1000 | Loss: 0.00001205
Iteration 23/1000 | Loss: 0.00001204
Iteration 24/1000 | Loss: 0.00001204
Iteration 25/1000 | Loss: 0.00001203
Iteration 26/1000 | Loss: 0.00001203
Iteration 27/1000 | Loss: 0.00001202
Iteration 28/1000 | Loss: 0.00001202
Iteration 29/1000 | Loss: 0.00001202
Iteration 30/1000 | Loss: 0.00001202
Iteration 31/1000 | Loss: 0.00001202
Iteration 32/1000 | Loss: 0.00001202
Iteration 33/1000 | Loss: 0.00001202
Iteration 34/1000 | Loss: 0.00001202
Iteration 35/1000 | Loss: 0.00001202
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001202
Iteration 38/1000 | Loss: 0.00001202
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001202
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001202
Iteration 45/1000 | Loss: 0.00001202
Iteration 46/1000 | Loss: 0.00001202
Iteration 47/1000 | Loss: 0.00001202
Iteration 48/1000 | Loss: 0.00001202
Iteration 49/1000 | Loss: 0.00001202
Iteration 50/1000 | Loss: 0.00001202
Iteration 51/1000 | Loss: 0.00001202
Iteration 52/1000 | Loss: 0.00001202
Iteration 53/1000 | Loss: 0.00001202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 53. Stopping optimization.
Last 5 losses: [1.2018074812658597e-05, 1.2018074812658597e-05, 1.2018074812658597e-05, 1.2018074812658597e-05, 1.2018074812658597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2018074812658597e-05

Optimization complete. Final v2v error: 2.8910486698150635 mm

Highest mean error: 3.3725814819335938 mm for frame 73

Lowest mean error: 2.360666036605835 mm for frame 166

Saving results

Total time: 26.832420825958252
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414022
Iteration 2/25 | Loss: 0.00108925
Iteration 3/25 | Loss: 0.00091994
Iteration 4/25 | Loss: 0.00090504
Iteration 5/25 | Loss: 0.00090222
Iteration 6/25 | Loss: 0.00090136
Iteration 7/25 | Loss: 0.00090118
Iteration 8/25 | Loss: 0.00090118
Iteration 9/25 | Loss: 0.00090118
Iteration 10/25 | Loss: 0.00090118
Iteration 11/25 | Loss: 0.00090118
Iteration 12/25 | Loss: 0.00090118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009011842194013298, 0.0009011842194013298, 0.0009011842194013298, 0.0009011842194013298, 0.0009011842194013298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009011842194013298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43133342
Iteration 2/25 | Loss: 0.00065106
Iteration 3/25 | Loss: 0.00065106
Iteration 4/25 | Loss: 0.00065106
Iteration 5/25 | Loss: 0.00065106
Iteration 6/25 | Loss: 0.00065106
Iteration 7/25 | Loss: 0.00065106
Iteration 8/25 | Loss: 0.00065106
Iteration 9/25 | Loss: 0.00065106
Iteration 10/25 | Loss: 0.00065106
Iteration 11/25 | Loss: 0.00065106
Iteration 12/25 | Loss: 0.00065106
Iteration 13/25 | Loss: 0.00065106
Iteration 14/25 | Loss: 0.00065106
Iteration 15/25 | Loss: 0.00065106
Iteration 16/25 | Loss: 0.00065106
Iteration 17/25 | Loss: 0.00065106
Iteration 18/25 | Loss: 0.00065106
Iteration 19/25 | Loss: 0.00065106
Iteration 20/25 | Loss: 0.00065106
Iteration 21/25 | Loss: 0.00065106
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006510561797767878, 0.0006510561797767878, 0.0006510561797767878, 0.0006510561797767878, 0.0006510561797767878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006510561797767878

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065106
Iteration 2/1000 | Loss: 0.00004259
Iteration 3/1000 | Loss: 0.00002013
Iteration 4/1000 | Loss: 0.00001635
Iteration 5/1000 | Loss: 0.00001486
Iteration 6/1000 | Loss: 0.00001414
Iteration 7/1000 | Loss: 0.00001367
Iteration 8/1000 | Loss: 0.00001355
Iteration 9/1000 | Loss: 0.00001330
Iteration 10/1000 | Loss: 0.00001315
Iteration 11/1000 | Loss: 0.00001306
Iteration 12/1000 | Loss: 0.00001301
Iteration 13/1000 | Loss: 0.00001298
Iteration 14/1000 | Loss: 0.00001297
Iteration 15/1000 | Loss: 0.00001296
Iteration 16/1000 | Loss: 0.00001296
Iteration 17/1000 | Loss: 0.00001295
Iteration 18/1000 | Loss: 0.00001294
Iteration 19/1000 | Loss: 0.00001292
Iteration 20/1000 | Loss: 0.00001288
Iteration 21/1000 | Loss: 0.00001287
Iteration 22/1000 | Loss: 0.00001287
Iteration 23/1000 | Loss: 0.00001287
Iteration 24/1000 | Loss: 0.00001287
Iteration 25/1000 | Loss: 0.00001287
Iteration 26/1000 | Loss: 0.00001287
Iteration 27/1000 | Loss: 0.00001285
Iteration 28/1000 | Loss: 0.00001281
Iteration 29/1000 | Loss: 0.00001280
Iteration 30/1000 | Loss: 0.00001279
Iteration 31/1000 | Loss: 0.00001278
Iteration 32/1000 | Loss: 0.00001278
Iteration 33/1000 | Loss: 0.00001277
Iteration 34/1000 | Loss: 0.00001277
Iteration 35/1000 | Loss: 0.00001276
Iteration 36/1000 | Loss: 0.00001276
Iteration 37/1000 | Loss: 0.00001276
Iteration 38/1000 | Loss: 0.00001276
Iteration 39/1000 | Loss: 0.00001276
Iteration 40/1000 | Loss: 0.00001276
Iteration 41/1000 | Loss: 0.00001276
Iteration 42/1000 | Loss: 0.00001276
Iteration 43/1000 | Loss: 0.00001276
Iteration 44/1000 | Loss: 0.00001275
Iteration 45/1000 | Loss: 0.00001275
Iteration 46/1000 | Loss: 0.00001275
Iteration 47/1000 | Loss: 0.00001275
Iteration 48/1000 | Loss: 0.00001275
Iteration 49/1000 | Loss: 0.00001275
Iteration 50/1000 | Loss: 0.00001275
Iteration 51/1000 | Loss: 0.00001275
Iteration 52/1000 | Loss: 0.00001274
Iteration 53/1000 | Loss: 0.00001274
Iteration 54/1000 | Loss: 0.00001274
Iteration 55/1000 | Loss: 0.00001273
Iteration 56/1000 | Loss: 0.00001273
Iteration 57/1000 | Loss: 0.00001273
Iteration 58/1000 | Loss: 0.00001273
Iteration 59/1000 | Loss: 0.00001273
Iteration 60/1000 | Loss: 0.00001272
Iteration 61/1000 | Loss: 0.00001272
Iteration 62/1000 | Loss: 0.00001272
Iteration 63/1000 | Loss: 0.00001272
Iteration 64/1000 | Loss: 0.00001272
Iteration 65/1000 | Loss: 0.00001272
Iteration 66/1000 | Loss: 0.00001271
Iteration 67/1000 | Loss: 0.00001271
Iteration 68/1000 | Loss: 0.00001271
Iteration 69/1000 | Loss: 0.00001271
Iteration 70/1000 | Loss: 0.00001271
Iteration 71/1000 | Loss: 0.00001271
Iteration 72/1000 | Loss: 0.00001271
Iteration 73/1000 | Loss: 0.00001271
Iteration 74/1000 | Loss: 0.00001271
Iteration 75/1000 | Loss: 0.00001271
Iteration 76/1000 | Loss: 0.00001271
Iteration 77/1000 | Loss: 0.00001271
Iteration 78/1000 | Loss: 0.00001271
Iteration 79/1000 | Loss: 0.00001271
Iteration 80/1000 | Loss: 0.00001270
Iteration 81/1000 | Loss: 0.00001270
Iteration 82/1000 | Loss: 0.00001270
Iteration 83/1000 | Loss: 0.00001270
Iteration 84/1000 | Loss: 0.00001269
Iteration 85/1000 | Loss: 0.00001269
Iteration 86/1000 | Loss: 0.00001269
Iteration 87/1000 | Loss: 0.00001269
Iteration 88/1000 | Loss: 0.00001269
Iteration 89/1000 | Loss: 0.00001269
Iteration 90/1000 | Loss: 0.00001269
Iteration 91/1000 | Loss: 0.00001268
Iteration 92/1000 | Loss: 0.00001268
Iteration 93/1000 | Loss: 0.00001268
Iteration 94/1000 | Loss: 0.00001268
Iteration 95/1000 | Loss: 0.00001268
Iteration 96/1000 | Loss: 0.00001268
Iteration 97/1000 | Loss: 0.00001267
Iteration 98/1000 | Loss: 0.00001267
Iteration 99/1000 | Loss: 0.00001267
Iteration 100/1000 | Loss: 0.00001267
Iteration 101/1000 | Loss: 0.00001267
Iteration 102/1000 | Loss: 0.00001267
Iteration 103/1000 | Loss: 0.00001267
Iteration 104/1000 | Loss: 0.00001267
Iteration 105/1000 | Loss: 0.00001267
Iteration 106/1000 | Loss: 0.00001267
Iteration 107/1000 | Loss: 0.00001267
Iteration 108/1000 | Loss: 0.00001266
Iteration 109/1000 | Loss: 0.00001266
Iteration 110/1000 | Loss: 0.00001266
Iteration 111/1000 | Loss: 0.00001266
Iteration 112/1000 | Loss: 0.00001266
Iteration 113/1000 | Loss: 0.00001266
Iteration 114/1000 | Loss: 0.00001266
Iteration 115/1000 | Loss: 0.00001266
Iteration 116/1000 | Loss: 0.00001266
Iteration 117/1000 | Loss: 0.00001266
Iteration 118/1000 | Loss: 0.00001266
Iteration 119/1000 | Loss: 0.00001266
Iteration 120/1000 | Loss: 0.00001266
Iteration 121/1000 | Loss: 0.00001266
Iteration 122/1000 | Loss: 0.00001266
Iteration 123/1000 | Loss: 0.00001266
Iteration 124/1000 | Loss: 0.00001265
Iteration 125/1000 | Loss: 0.00001265
Iteration 126/1000 | Loss: 0.00001265
Iteration 127/1000 | Loss: 0.00001265
Iteration 128/1000 | Loss: 0.00001265
Iteration 129/1000 | Loss: 0.00001265
Iteration 130/1000 | Loss: 0.00001265
Iteration 131/1000 | Loss: 0.00001265
Iteration 132/1000 | Loss: 0.00001265
Iteration 133/1000 | Loss: 0.00001265
Iteration 134/1000 | Loss: 0.00001265
Iteration 135/1000 | Loss: 0.00001265
Iteration 136/1000 | Loss: 0.00001265
Iteration 137/1000 | Loss: 0.00001265
Iteration 138/1000 | Loss: 0.00001265
Iteration 139/1000 | Loss: 0.00001265
Iteration 140/1000 | Loss: 0.00001265
Iteration 141/1000 | Loss: 0.00001265
Iteration 142/1000 | Loss: 0.00001265
Iteration 143/1000 | Loss: 0.00001265
Iteration 144/1000 | Loss: 0.00001265
Iteration 145/1000 | Loss: 0.00001265
Iteration 146/1000 | Loss: 0.00001265
Iteration 147/1000 | Loss: 0.00001265
Iteration 148/1000 | Loss: 0.00001265
Iteration 149/1000 | Loss: 0.00001265
Iteration 150/1000 | Loss: 0.00001265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.2646454706555232e-05, 1.2646454706555232e-05, 1.2646454706555232e-05, 1.2646454706555232e-05, 1.2646454706555232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2646454706555232e-05

Optimization complete. Final v2v error: 2.8569724559783936 mm

Highest mean error: 4.352646827697754 mm for frame 38

Lowest mean error: 2.4218931198120117 mm for frame 78

Saving results

Total time: 33.50980186462402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00482754
Iteration 2/25 | Loss: 0.00112021
Iteration 3/25 | Loss: 0.00092497
Iteration 4/25 | Loss: 0.00090480
Iteration 5/25 | Loss: 0.00090247
Iteration 6/25 | Loss: 0.00090212
Iteration 7/25 | Loss: 0.00090212
Iteration 8/25 | Loss: 0.00090212
Iteration 9/25 | Loss: 0.00090212
Iteration 10/25 | Loss: 0.00090212
Iteration 11/25 | Loss: 0.00090212
Iteration 12/25 | Loss: 0.00090212
Iteration 13/25 | Loss: 0.00090212
Iteration 14/25 | Loss: 0.00090212
Iteration 15/25 | Loss: 0.00090212
Iteration 16/25 | Loss: 0.00090212
Iteration 17/25 | Loss: 0.00090212
Iteration 18/25 | Loss: 0.00090212
Iteration 19/25 | Loss: 0.00090212
Iteration 20/25 | Loss: 0.00090212
Iteration 21/25 | Loss: 0.00090212
Iteration 22/25 | Loss: 0.00090212
Iteration 23/25 | Loss: 0.00090212
Iteration 24/25 | Loss: 0.00090212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009021180449053645, 0.0009021180449053645, 0.0009021180449053645, 0.0009021180449053645, 0.0009021180449053645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009021180449053645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45381105
Iteration 2/25 | Loss: 0.00057838
Iteration 3/25 | Loss: 0.00057836
Iteration 4/25 | Loss: 0.00057836
Iteration 5/25 | Loss: 0.00057836
Iteration 6/25 | Loss: 0.00057836
Iteration 7/25 | Loss: 0.00057836
Iteration 8/25 | Loss: 0.00057836
Iteration 9/25 | Loss: 0.00057836
Iteration 10/25 | Loss: 0.00057836
Iteration 11/25 | Loss: 0.00057836
Iteration 12/25 | Loss: 0.00057836
Iteration 13/25 | Loss: 0.00057836
Iteration 14/25 | Loss: 0.00057836
Iteration 15/25 | Loss: 0.00057836
Iteration 16/25 | Loss: 0.00057836
Iteration 17/25 | Loss: 0.00057836
Iteration 18/25 | Loss: 0.00057836
Iteration 19/25 | Loss: 0.00057836
Iteration 20/25 | Loss: 0.00057836
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005783612141385674, 0.0005783612141385674, 0.0005783612141385674, 0.0005783612141385674, 0.0005783612141385674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005783612141385674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057836
Iteration 2/1000 | Loss: 0.00002873
Iteration 3/1000 | Loss: 0.00001748
Iteration 4/1000 | Loss: 0.00001413
Iteration 5/1000 | Loss: 0.00001335
Iteration 6/1000 | Loss: 0.00001275
Iteration 7/1000 | Loss: 0.00001231
Iteration 8/1000 | Loss: 0.00001190
Iteration 9/1000 | Loss: 0.00001166
Iteration 10/1000 | Loss: 0.00001139
Iteration 11/1000 | Loss: 0.00001133
Iteration 12/1000 | Loss: 0.00001133
Iteration 13/1000 | Loss: 0.00001129
Iteration 14/1000 | Loss: 0.00001128
Iteration 15/1000 | Loss: 0.00001123
Iteration 16/1000 | Loss: 0.00001123
Iteration 17/1000 | Loss: 0.00001123
Iteration 18/1000 | Loss: 0.00001123
Iteration 19/1000 | Loss: 0.00001123
Iteration 20/1000 | Loss: 0.00001122
Iteration 21/1000 | Loss: 0.00001122
Iteration 22/1000 | Loss: 0.00001122
Iteration 23/1000 | Loss: 0.00001122
Iteration 24/1000 | Loss: 0.00001122
Iteration 25/1000 | Loss: 0.00001122
Iteration 26/1000 | Loss: 0.00001122
Iteration 27/1000 | Loss: 0.00001122
Iteration 28/1000 | Loss: 0.00001122
Iteration 29/1000 | Loss: 0.00001122
Iteration 30/1000 | Loss: 0.00001122
Iteration 31/1000 | Loss: 0.00001121
Iteration 32/1000 | Loss: 0.00001120
Iteration 33/1000 | Loss: 0.00001120
Iteration 34/1000 | Loss: 0.00001120
Iteration 35/1000 | Loss: 0.00001119
Iteration 36/1000 | Loss: 0.00001119
Iteration 37/1000 | Loss: 0.00001119
Iteration 38/1000 | Loss: 0.00001119
Iteration 39/1000 | Loss: 0.00001119
Iteration 40/1000 | Loss: 0.00001118
Iteration 41/1000 | Loss: 0.00001118
Iteration 42/1000 | Loss: 0.00001118
Iteration 43/1000 | Loss: 0.00001118
Iteration 44/1000 | Loss: 0.00001117
Iteration 45/1000 | Loss: 0.00001117
Iteration 46/1000 | Loss: 0.00001116
Iteration 47/1000 | Loss: 0.00001116
Iteration 48/1000 | Loss: 0.00001116
Iteration 49/1000 | Loss: 0.00001116
Iteration 50/1000 | Loss: 0.00001116
Iteration 51/1000 | Loss: 0.00001116
Iteration 52/1000 | Loss: 0.00001116
Iteration 53/1000 | Loss: 0.00001116
Iteration 54/1000 | Loss: 0.00001116
Iteration 55/1000 | Loss: 0.00001116
Iteration 56/1000 | Loss: 0.00001116
Iteration 57/1000 | Loss: 0.00001116
Iteration 58/1000 | Loss: 0.00001115
Iteration 59/1000 | Loss: 0.00001115
Iteration 60/1000 | Loss: 0.00001115
Iteration 61/1000 | Loss: 0.00001114
Iteration 62/1000 | Loss: 0.00001114
Iteration 63/1000 | Loss: 0.00001114
Iteration 64/1000 | Loss: 0.00001114
Iteration 65/1000 | Loss: 0.00001113
Iteration 66/1000 | Loss: 0.00001113
Iteration 67/1000 | Loss: 0.00001113
Iteration 68/1000 | Loss: 0.00001113
Iteration 69/1000 | Loss: 0.00001113
Iteration 70/1000 | Loss: 0.00001113
Iteration 71/1000 | Loss: 0.00001113
Iteration 72/1000 | Loss: 0.00001113
Iteration 73/1000 | Loss: 0.00001113
Iteration 74/1000 | Loss: 0.00001113
Iteration 75/1000 | Loss: 0.00001113
Iteration 76/1000 | Loss: 0.00001113
Iteration 77/1000 | Loss: 0.00001112
Iteration 78/1000 | Loss: 0.00001112
Iteration 79/1000 | Loss: 0.00001112
Iteration 80/1000 | Loss: 0.00001112
Iteration 81/1000 | Loss: 0.00001112
Iteration 82/1000 | Loss: 0.00001112
Iteration 83/1000 | Loss: 0.00001112
Iteration 84/1000 | Loss: 0.00001112
Iteration 85/1000 | Loss: 0.00001112
Iteration 86/1000 | Loss: 0.00001112
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001112
Iteration 89/1000 | Loss: 0.00001112
Iteration 90/1000 | Loss: 0.00001111
Iteration 91/1000 | Loss: 0.00001111
Iteration 92/1000 | Loss: 0.00001111
Iteration 93/1000 | Loss: 0.00001111
Iteration 94/1000 | Loss: 0.00001111
Iteration 95/1000 | Loss: 0.00001111
Iteration 96/1000 | Loss: 0.00001111
Iteration 97/1000 | Loss: 0.00001111
Iteration 98/1000 | Loss: 0.00001111
Iteration 99/1000 | Loss: 0.00001111
Iteration 100/1000 | Loss: 0.00001111
Iteration 101/1000 | Loss: 0.00001111
Iteration 102/1000 | Loss: 0.00001111
Iteration 103/1000 | Loss: 0.00001111
Iteration 104/1000 | Loss: 0.00001111
Iteration 105/1000 | Loss: 0.00001111
Iteration 106/1000 | Loss: 0.00001111
Iteration 107/1000 | Loss: 0.00001111
Iteration 108/1000 | Loss: 0.00001111
Iteration 109/1000 | Loss: 0.00001111
Iteration 110/1000 | Loss: 0.00001111
Iteration 111/1000 | Loss: 0.00001111
Iteration 112/1000 | Loss: 0.00001111
Iteration 113/1000 | Loss: 0.00001111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.1112937500001863e-05, 1.1112937500001863e-05, 1.1112937500001863e-05, 1.1112937500001863e-05, 1.1112937500001863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1112937500001863e-05

Optimization complete. Final v2v error: 2.7758476734161377 mm

Highest mean error: 3.3794748783111572 mm for frame 47

Lowest mean error: 2.2870051860809326 mm for frame 0

Saving results

Total time: 30.23470401763916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00760642
Iteration 2/25 | Loss: 0.00140048
Iteration 3/25 | Loss: 0.00103595
Iteration 4/25 | Loss: 0.00096999
Iteration 5/25 | Loss: 0.00095534
Iteration 6/25 | Loss: 0.00095088
Iteration 7/25 | Loss: 0.00094943
Iteration 8/25 | Loss: 0.00094894
Iteration 9/25 | Loss: 0.00094872
Iteration 10/25 | Loss: 0.00094854
Iteration 11/25 | Loss: 0.00094741
Iteration 12/25 | Loss: 0.00094657
Iteration 13/25 | Loss: 0.00094604
Iteration 14/25 | Loss: 0.00094560
Iteration 15/25 | Loss: 0.00094548
Iteration 16/25 | Loss: 0.00094546
Iteration 17/25 | Loss: 0.00094546
Iteration 18/25 | Loss: 0.00094546
Iteration 19/25 | Loss: 0.00094546
Iteration 20/25 | Loss: 0.00094546
Iteration 21/25 | Loss: 0.00094546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009454591781832278, 0.0009454591781832278, 0.0009454591781832278, 0.0009454591781832278, 0.0009454591781832278]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009454591781832278

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.70590591
Iteration 2/25 | Loss: 0.00075611
Iteration 3/25 | Loss: 0.00075599
Iteration 4/25 | Loss: 0.00075599
Iteration 5/25 | Loss: 0.00075599
Iteration 6/25 | Loss: 0.00075598
Iteration 7/25 | Loss: 0.00075598
Iteration 8/25 | Loss: 0.00075598
Iteration 9/25 | Loss: 0.00075598
Iteration 10/25 | Loss: 0.00075598
Iteration 11/25 | Loss: 0.00075598
Iteration 12/25 | Loss: 0.00075598
Iteration 13/25 | Loss: 0.00075598
Iteration 14/25 | Loss: 0.00075598
Iteration 15/25 | Loss: 0.00075598
Iteration 16/25 | Loss: 0.00075598
Iteration 17/25 | Loss: 0.00075598
Iteration 18/25 | Loss: 0.00075598
Iteration 19/25 | Loss: 0.00075598
Iteration 20/25 | Loss: 0.00075598
Iteration 21/25 | Loss: 0.00075598
Iteration 22/25 | Loss: 0.00075598
Iteration 23/25 | Loss: 0.00075598
Iteration 24/25 | Loss: 0.00075598
Iteration 25/25 | Loss: 0.00075598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075598
Iteration 2/1000 | Loss: 0.00004842
Iteration 3/1000 | Loss: 0.00002849
Iteration 4/1000 | Loss: 0.00002133
Iteration 5/1000 | Loss: 0.00001948
Iteration 6/1000 | Loss: 0.00002506
Iteration 7/1000 | Loss: 0.00001785
Iteration 8/1000 | Loss: 0.00001714
Iteration 9/1000 | Loss: 0.00001678
Iteration 10/1000 | Loss: 0.00001667
Iteration 11/1000 | Loss: 0.00007244
Iteration 12/1000 | Loss: 0.00001620
Iteration 13/1000 | Loss: 0.00001602
Iteration 14/1000 | Loss: 0.00001599
Iteration 15/1000 | Loss: 0.00001598
Iteration 16/1000 | Loss: 0.00001587
Iteration 17/1000 | Loss: 0.00001587
Iteration 18/1000 | Loss: 0.00001586
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001584
Iteration 21/1000 | Loss: 0.00001583
Iteration 22/1000 | Loss: 0.00001583
Iteration 23/1000 | Loss: 0.00001581
Iteration 24/1000 | Loss: 0.00001580
Iteration 25/1000 | Loss: 0.00001579
Iteration 26/1000 | Loss: 0.00001578
Iteration 27/1000 | Loss: 0.00001576
Iteration 28/1000 | Loss: 0.00001575
Iteration 29/1000 | Loss: 0.00001574
Iteration 30/1000 | Loss: 0.00001574
Iteration 31/1000 | Loss: 0.00001574
Iteration 32/1000 | Loss: 0.00001573
Iteration 33/1000 | Loss: 0.00001573
Iteration 34/1000 | Loss: 0.00001573
Iteration 35/1000 | Loss: 0.00001573
Iteration 36/1000 | Loss: 0.00001572
Iteration 37/1000 | Loss: 0.00001572
Iteration 38/1000 | Loss: 0.00001570
Iteration 39/1000 | Loss: 0.00001567
Iteration 40/1000 | Loss: 0.00001566
Iteration 41/1000 | Loss: 0.00001566
Iteration 42/1000 | Loss: 0.00001566
Iteration 43/1000 | Loss: 0.00001565
Iteration 44/1000 | Loss: 0.00001565
Iteration 45/1000 | Loss: 0.00001565
Iteration 46/1000 | Loss: 0.00001565
Iteration 47/1000 | Loss: 0.00001564
Iteration 48/1000 | Loss: 0.00001564
Iteration 49/1000 | Loss: 0.00001564
Iteration 50/1000 | Loss: 0.00001564
Iteration 51/1000 | Loss: 0.00001564
Iteration 52/1000 | Loss: 0.00001563
Iteration 53/1000 | Loss: 0.00001563
Iteration 54/1000 | Loss: 0.00001563
Iteration 55/1000 | Loss: 0.00001563
Iteration 56/1000 | Loss: 0.00001563
Iteration 57/1000 | Loss: 0.00001562
Iteration 58/1000 | Loss: 0.00001562
Iteration 59/1000 | Loss: 0.00001562
Iteration 60/1000 | Loss: 0.00001562
Iteration 61/1000 | Loss: 0.00001561
Iteration 62/1000 | Loss: 0.00001561
Iteration 63/1000 | Loss: 0.00001559
Iteration 64/1000 | Loss: 0.00001559
Iteration 65/1000 | Loss: 0.00001558
Iteration 66/1000 | Loss: 0.00001558
Iteration 67/1000 | Loss: 0.00001558
Iteration 68/1000 | Loss: 0.00001558
Iteration 69/1000 | Loss: 0.00001557
Iteration 70/1000 | Loss: 0.00001557
Iteration 71/1000 | Loss: 0.00001557
Iteration 72/1000 | Loss: 0.00001557
Iteration 73/1000 | Loss: 0.00001557
Iteration 74/1000 | Loss: 0.00001557
Iteration 75/1000 | Loss: 0.00001556
Iteration 76/1000 | Loss: 0.00001556
Iteration 77/1000 | Loss: 0.00001556
Iteration 78/1000 | Loss: 0.00001556
Iteration 79/1000 | Loss: 0.00001556
Iteration 80/1000 | Loss: 0.00001556
Iteration 81/1000 | Loss: 0.00001556
Iteration 82/1000 | Loss: 0.00001555
Iteration 83/1000 | Loss: 0.00001555
Iteration 84/1000 | Loss: 0.00001555
Iteration 85/1000 | Loss: 0.00001555
Iteration 86/1000 | Loss: 0.00001555
Iteration 87/1000 | Loss: 0.00001555
Iteration 88/1000 | Loss: 0.00001555
Iteration 89/1000 | Loss: 0.00001555
Iteration 90/1000 | Loss: 0.00001554
Iteration 91/1000 | Loss: 0.00001554
Iteration 92/1000 | Loss: 0.00001554
Iteration 93/1000 | Loss: 0.00001554
Iteration 94/1000 | Loss: 0.00001554
Iteration 95/1000 | Loss: 0.00001554
Iteration 96/1000 | Loss: 0.00001554
Iteration 97/1000 | Loss: 0.00001553
Iteration 98/1000 | Loss: 0.00001553
Iteration 99/1000 | Loss: 0.00001553
Iteration 100/1000 | Loss: 0.00001553
Iteration 101/1000 | Loss: 0.00001553
Iteration 102/1000 | Loss: 0.00001553
Iteration 103/1000 | Loss: 0.00001553
Iteration 104/1000 | Loss: 0.00001552
Iteration 105/1000 | Loss: 0.00001552
Iteration 106/1000 | Loss: 0.00001552
Iteration 107/1000 | Loss: 0.00001552
Iteration 108/1000 | Loss: 0.00001552
Iteration 109/1000 | Loss: 0.00001552
Iteration 110/1000 | Loss: 0.00001552
Iteration 111/1000 | Loss: 0.00001551
Iteration 112/1000 | Loss: 0.00001551
Iteration 113/1000 | Loss: 0.00001551
Iteration 114/1000 | Loss: 0.00001551
Iteration 115/1000 | Loss: 0.00001551
Iteration 116/1000 | Loss: 0.00001551
Iteration 117/1000 | Loss: 0.00001551
Iteration 118/1000 | Loss: 0.00001551
Iteration 119/1000 | Loss: 0.00001551
Iteration 120/1000 | Loss: 0.00001551
Iteration 121/1000 | Loss: 0.00001551
Iteration 122/1000 | Loss: 0.00001551
Iteration 123/1000 | Loss: 0.00001550
Iteration 124/1000 | Loss: 0.00001550
Iteration 125/1000 | Loss: 0.00001550
Iteration 126/1000 | Loss: 0.00001550
Iteration 127/1000 | Loss: 0.00001550
Iteration 128/1000 | Loss: 0.00001550
Iteration 129/1000 | Loss: 0.00001549
Iteration 130/1000 | Loss: 0.00001549
Iteration 131/1000 | Loss: 0.00001549
Iteration 132/1000 | Loss: 0.00001549
Iteration 133/1000 | Loss: 0.00001549
Iteration 134/1000 | Loss: 0.00001549
Iteration 135/1000 | Loss: 0.00001549
Iteration 136/1000 | Loss: 0.00001549
Iteration 137/1000 | Loss: 0.00001549
Iteration 138/1000 | Loss: 0.00001549
Iteration 139/1000 | Loss: 0.00001549
Iteration 140/1000 | Loss: 0.00001549
Iteration 141/1000 | Loss: 0.00001549
Iteration 142/1000 | Loss: 0.00001549
Iteration 143/1000 | Loss: 0.00001549
Iteration 144/1000 | Loss: 0.00001548
Iteration 145/1000 | Loss: 0.00001548
Iteration 146/1000 | Loss: 0.00001548
Iteration 147/1000 | Loss: 0.00001548
Iteration 148/1000 | Loss: 0.00001548
Iteration 149/1000 | Loss: 0.00001548
Iteration 150/1000 | Loss: 0.00001548
Iteration 151/1000 | Loss: 0.00001548
Iteration 152/1000 | Loss: 0.00001548
Iteration 153/1000 | Loss: 0.00001548
Iteration 154/1000 | Loss: 0.00001548
Iteration 155/1000 | Loss: 0.00001548
Iteration 156/1000 | Loss: 0.00001548
Iteration 157/1000 | Loss: 0.00001548
Iteration 158/1000 | Loss: 0.00001548
Iteration 159/1000 | Loss: 0.00001548
Iteration 160/1000 | Loss: 0.00001548
Iteration 161/1000 | Loss: 0.00001548
Iteration 162/1000 | Loss: 0.00001548
Iteration 163/1000 | Loss: 0.00001548
Iteration 164/1000 | Loss: 0.00001548
Iteration 165/1000 | Loss: 0.00001548
Iteration 166/1000 | Loss: 0.00001548
Iteration 167/1000 | Loss: 0.00001548
Iteration 168/1000 | Loss: 0.00001548
Iteration 169/1000 | Loss: 0.00001548
Iteration 170/1000 | Loss: 0.00001548
Iteration 171/1000 | Loss: 0.00001548
Iteration 172/1000 | Loss: 0.00001548
Iteration 173/1000 | Loss: 0.00001548
Iteration 174/1000 | Loss: 0.00001548
Iteration 175/1000 | Loss: 0.00001548
Iteration 176/1000 | Loss: 0.00001548
Iteration 177/1000 | Loss: 0.00001548
Iteration 178/1000 | Loss: 0.00001548
Iteration 179/1000 | Loss: 0.00001548
Iteration 180/1000 | Loss: 0.00001548
Iteration 181/1000 | Loss: 0.00001548
Iteration 182/1000 | Loss: 0.00001548
Iteration 183/1000 | Loss: 0.00001548
Iteration 184/1000 | Loss: 0.00001548
Iteration 185/1000 | Loss: 0.00001548
Iteration 186/1000 | Loss: 0.00001548
Iteration 187/1000 | Loss: 0.00001548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.5477431588806212e-05, 1.5477431588806212e-05, 1.5477431588806212e-05, 1.5477431588806212e-05, 1.5477431588806212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5477431588806212e-05

Optimization complete. Final v2v error: 3.23024320602417 mm

Highest mean error: 8.980852127075195 mm for frame 52

Lowest mean error: 2.336291551589966 mm for frame 93

Saving results

Total time: 64.81517910957336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026997
Iteration 2/25 | Loss: 0.00372341
Iteration 3/25 | Loss: 0.00243102
Iteration 4/25 | Loss: 0.00214009
Iteration 5/25 | Loss: 0.00191137
Iteration 6/25 | Loss: 0.00179090
Iteration 7/25 | Loss: 0.00173736
Iteration 8/25 | Loss: 0.00168487
Iteration 9/25 | Loss: 0.00167051
Iteration 10/25 | Loss: 0.00164649
Iteration 11/25 | Loss: 0.00157580
Iteration 12/25 | Loss: 0.00152104
Iteration 13/25 | Loss: 0.00148817
Iteration 14/25 | Loss: 0.00147592
Iteration 15/25 | Loss: 0.00146855
Iteration 16/25 | Loss: 0.00146173
Iteration 17/25 | Loss: 0.00145120
Iteration 18/25 | Loss: 0.00145184
Iteration 19/25 | Loss: 0.00144500
Iteration 20/25 | Loss: 0.00144388
Iteration 21/25 | Loss: 0.00144230
Iteration 22/25 | Loss: 0.00143963
Iteration 23/25 | Loss: 0.00144134
Iteration 24/25 | Loss: 0.00144158
Iteration 25/25 | Loss: 0.00143520

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33603537
Iteration 2/25 | Loss: 0.00826787
Iteration 3/25 | Loss: 0.00494472
Iteration 4/25 | Loss: 0.00494472
Iteration 5/25 | Loss: 0.00494472
Iteration 6/25 | Loss: 0.00494472
Iteration 7/25 | Loss: 0.00494472
Iteration 8/25 | Loss: 0.00494472
Iteration 9/25 | Loss: 0.00494472
Iteration 10/25 | Loss: 0.00494472
Iteration 11/25 | Loss: 0.00494472
Iteration 12/25 | Loss: 0.00494472
Iteration 13/25 | Loss: 0.00494472
Iteration 14/25 | Loss: 0.00494472
Iteration 15/25 | Loss: 0.00494472
Iteration 16/25 | Loss: 0.00494472
Iteration 17/25 | Loss: 0.00494472
Iteration 18/25 | Loss: 0.00494472
Iteration 19/25 | Loss: 0.00494472
Iteration 20/25 | Loss: 0.00494472
Iteration 21/25 | Loss: 0.00494472
Iteration 22/25 | Loss: 0.00494472
Iteration 23/25 | Loss: 0.00494472
Iteration 24/25 | Loss: 0.00494472
Iteration 25/25 | Loss: 0.00494472

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00494472
Iteration 2/1000 | Loss: 0.00528998
Iteration 3/1000 | Loss: 0.02506558
Iteration 4/1000 | Loss: 0.00100979
Iteration 5/1000 | Loss: 0.00118324
Iteration 6/1000 | Loss: 0.00319662
Iteration 7/1000 | Loss: 0.00593459
Iteration 8/1000 | Loss: 0.00249482
Iteration 9/1000 | Loss: 0.00259077
Iteration 10/1000 | Loss: 0.00100667
Iteration 11/1000 | Loss: 0.00076452
Iteration 12/1000 | Loss: 0.00095538
Iteration 13/1000 | Loss: 0.00087109
Iteration 14/1000 | Loss: 0.00590859
Iteration 15/1000 | Loss: 0.00197459
Iteration 16/1000 | Loss: 0.00629437
Iteration 17/1000 | Loss: 0.00505000
Iteration 18/1000 | Loss: 0.00229968
Iteration 19/1000 | Loss: 0.00152733
Iteration 20/1000 | Loss: 0.00051264
Iteration 21/1000 | Loss: 0.00163620
Iteration 22/1000 | Loss: 0.00048343
Iteration 23/1000 | Loss: 0.00055387
Iteration 24/1000 | Loss: 0.00129451
Iteration 25/1000 | Loss: 0.00037188
Iteration 26/1000 | Loss: 0.00051571
Iteration 27/1000 | Loss: 0.00278881
Iteration 28/1000 | Loss: 0.00048676
Iteration 29/1000 | Loss: 0.00026969
Iteration 30/1000 | Loss: 0.00180489
Iteration 31/1000 | Loss: 0.00070242
Iteration 32/1000 | Loss: 0.00052226
Iteration 33/1000 | Loss: 0.00019234
Iteration 34/1000 | Loss: 0.00065909
Iteration 35/1000 | Loss: 0.00101460
Iteration 36/1000 | Loss: 0.00030474
Iteration 37/1000 | Loss: 0.00080825
Iteration 38/1000 | Loss: 0.00111881
Iteration 39/1000 | Loss: 0.00033947
Iteration 40/1000 | Loss: 0.00046737
Iteration 41/1000 | Loss: 0.00055027
Iteration 42/1000 | Loss: 0.00103218
Iteration 43/1000 | Loss: 0.00096863
Iteration 44/1000 | Loss: 0.00071359
Iteration 45/1000 | Loss: 0.00018172
Iteration 46/1000 | Loss: 0.00018480
Iteration 47/1000 | Loss: 0.00102239
Iteration 48/1000 | Loss: 0.00050600
Iteration 49/1000 | Loss: 0.00155971
Iteration 50/1000 | Loss: 0.00251137
Iteration 51/1000 | Loss: 0.00043799
Iteration 52/1000 | Loss: 0.00021211
Iteration 53/1000 | Loss: 0.00045149
Iteration 54/1000 | Loss: 0.00040208
Iteration 55/1000 | Loss: 0.00039986
Iteration 56/1000 | Loss: 0.00051923
Iteration 57/1000 | Loss: 0.00016624
Iteration 58/1000 | Loss: 0.00018047
Iteration 59/1000 | Loss: 0.00017322
Iteration 60/1000 | Loss: 0.00017854
Iteration 61/1000 | Loss: 0.00019241
Iteration 62/1000 | Loss: 0.00022702
Iteration 63/1000 | Loss: 0.00106604
Iteration 64/1000 | Loss: 0.00181413
Iteration 65/1000 | Loss: 0.00049811
Iteration 66/1000 | Loss: 0.00134227
Iteration 67/1000 | Loss: 0.00070504
Iteration 68/1000 | Loss: 0.00135606
Iteration 69/1000 | Loss: 0.00021175
Iteration 70/1000 | Loss: 0.00016587
Iteration 71/1000 | Loss: 0.00027665
Iteration 72/1000 | Loss: 0.00050568
Iteration 73/1000 | Loss: 0.00016835
Iteration 74/1000 | Loss: 0.00025799
Iteration 75/1000 | Loss: 0.00019320
Iteration 76/1000 | Loss: 0.00044073
Iteration 77/1000 | Loss: 0.00016716
Iteration 78/1000 | Loss: 0.00016054
Iteration 79/1000 | Loss: 0.00115395
Iteration 80/1000 | Loss: 0.00018860
Iteration 81/1000 | Loss: 0.00089247
Iteration 82/1000 | Loss: 0.00085490
Iteration 83/1000 | Loss: 0.00030533
Iteration 84/1000 | Loss: 0.00069604
Iteration 85/1000 | Loss: 0.00049600
Iteration 86/1000 | Loss: 0.00016501
Iteration 87/1000 | Loss: 0.00018232
Iteration 88/1000 | Loss: 0.00015027
Iteration 89/1000 | Loss: 0.00095058
Iteration 90/1000 | Loss: 0.00064716
Iteration 91/1000 | Loss: 0.00119647
Iteration 92/1000 | Loss: 0.00054376
Iteration 93/1000 | Loss: 0.00102739
Iteration 94/1000 | Loss: 0.00056835
Iteration 95/1000 | Loss: 0.00020104
Iteration 96/1000 | Loss: 0.00022745
Iteration 97/1000 | Loss: 0.00020056
Iteration 98/1000 | Loss: 0.00025299
Iteration 99/1000 | Loss: 0.00081163
Iteration 100/1000 | Loss: 0.00037338
Iteration 101/1000 | Loss: 0.00032669
Iteration 102/1000 | Loss: 0.00030519
Iteration 103/1000 | Loss: 0.00029527
Iteration 104/1000 | Loss: 0.00016162
Iteration 105/1000 | Loss: 0.00031326
Iteration 106/1000 | Loss: 0.00031471
Iteration 107/1000 | Loss: 0.00028820
Iteration 108/1000 | Loss: 0.00013871
Iteration 109/1000 | Loss: 0.00034609
Iteration 110/1000 | Loss: 0.00030206
Iteration 111/1000 | Loss: 0.00028606
Iteration 112/1000 | Loss: 0.00013711
Iteration 113/1000 | Loss: 0.00013499
Iteration 114/1000 | Loss: 0.00038505
Iteration 115/1000 | Loss: 0.00066748
Iteration 116/1000 | Loss: 0.00112065
Iteration 117/1000 | Loss: 0.00109055
Iteration 118/1000 | Loss: 0.00029540
Iteration 119/1000 | Loss: 0.00076803
Iteration 120/1000 | Loss: 0.00016544
Iteration 121/1000 | Loss: 0.00014825
Iteration 122/1000 | Loss: 0.00046687
Iteration 123/1000 | Loss: 0.00130802
Iteration 124/1000 | Loss: 0.00060981
Iteration 125/1000 | Loss: 0.00071603
Iteration 126/1000 | Loss: 0.00055557
Iteration 127/1000 | Loss: 0.00046659
Iteration 128/1000 | Loss: 0.00031680
Iteration 129/1000 | Loss: 0.00023993
Iteration 130/1000 | Loss: 0.00028748
Iteration 131/1000 | Loss: 0.00013905
Iteration 132/1000 | Loss: 0.00040073
Iteration 133/1000 | Loss: 0.00171004
Iteration 134/1000 | Loss: 0.00161562
Iteration 135/1000 | Loss: 0.00216050
Iteration 136/1000 | Loss: 0.00041365
Iteration 137/1000 | Loss: 0.00046992
Iteration 138/1000 | Loss: 0.00039611
Iteration 139/1000 | Loss: 0.00025549
Iteration 140/1000 | Loss: 0.00046226
Iteration 141/1000 | Loss: 0.00048422
Iteration 142/1000 | Loss: 0.00030987
Iteration 143/1000 | Loss: 0.00013782
Iteration 144/1000 | Loss: 0.00043999
Iteration 145/1000 | Loss: 0.00065738
Iteration 146/1000 | Loss: 0.00150780
Iteration 147/1000 | Loss: 0.00064210
Iteration 148/1000 | Loss: 0.00015679
Iteration 149/1000 | Loss: 0.00016827
Iteration 150/1000 | Loss: 0.00011040
Iteration 151/1000 | Loss: 0.00009794
Iteration 152/1000 | Loss: 0.00009015
Iteration 153/1000 | Loss: 0.00008336
Iteration 154/1000 | Loss: 0.00008021
Iteration 155/1000 | Loss: 0.00007820
Iteration 156/1000 | Loss: 0.00007658
Iteration 157/1000 | Loss: 0.00007542
Iteration 158/1000 | Loss: 0.00079309
Iteration 159/1000 | Loss: 0.00043550
Iteration 160/1000 | Loss: 0.00060743
Iteration 161/1000 | Loss: 0.00009925
Iteration 162/1000 | Loss: 0.00007964
Iteration 163/1000 | Loss: 0.00007202
Iteration 164/1000 | Loss: 0.00007093
Iteration 165/1000 | Loss: 0.00020908
Iteration 166/1000 | Loss: 0.00007717
Iteration 167/1000 | Loss: 0.00007211
Iteration 168/1000 | Loss: 0.00007004
Iteration 169/1000 | Loss: 0.00006800
Iteration 170/1000 | Loss: 0.00006679
Iteration 171/1000 | Loss: 0.00014399
Iteration 172/1000 | Loss: 0.00006571
Iteration 173/1000 | Loss: 0.00006538
Iteration 174/1000 | Loss: 0.00017917
Iteration 175/1000 | Loss: 0.00006550
Iteration 176/1000 | Loss: 0.00006493
Iteration 177/1000 | Loss: 0.00006492
Iteration 178/1000 | Loss: 0.00006492
Iteration 179/1000 | Loss: 0.00006490
Iteration 180/1000 | Loss: 0.00006474
Iteration 181/1000 | Loss: 0.00006461
Iteration 182/1000 | Loss: 0.00006455
Iteration 183/1000 | Loss: 0.00006454
Iteration 184/1000 | Loss: 0.00006454
Iteration 185/1000 | Loss: 0.00006454
Iteration 186/1000 | Loss: 0.00006454
Iteration 187/1000 | Loss: 0.00006454
Iteration 188/1000 | Loss: 0.00006454
Iteration 189/1000 | Loss: 0.00006453
Iteration 190/1000 | Loss: 0.00006453
Iteration 191/1000 | Loss: 0.00006453
Iteration 192/1000 | Loss: 0.00006453
Iteration 193/1000 | Loss: 0.00006453
Iteration 194/1000 | Loss: 0.00006453
Iteration 195/1000 | Loss: 0.00006453
Iteration 196/1000 | Loss: 0.00006452
Iteration 197/1000 | Loss: 0.00006452
Iteration 198/1000 | Loss: 0.00006451
Iteration 199/1000 | Loss: 0.00006450
Iteration 200/1000 | Loss: 0.00006450
Iteration 201/1000 | Loss: 0.00006443
Iteration 202/1000 | Loss: 0.00006443
Iteration 203/1000 | Loss: 0.00006443
Iteration 204/1000 | Loss: 0.00006443
Iteration 205/1000 | Loss: 0.00006442
Iteration 206/1000 | Loss: 0.00006442
Iteration 207/1000 | Loss: 0.00006442
Iteration 208/1000 | Loss: 0.00006442
Iteration 209/1000 | Loss: 0.00006442
Iteration 210/1000 | Loss: 0.00006442
Iteration 211/1000 | Loss: 0.00006442
Iteration 212/1000 | Loss: 0.00006442
Iteration 213/1000 | Loss: 0.00006442
Iteration 214/1000 | Loss: 0.00006442
Iteration 215/1000 | Loss: 0.00006442
Iteration 216/1000 | Loss: 0.00006442
Iteration 217/1000 | Loss: 0.00006441
Iteration 218/1000 | Loss: 0.00006441
Iteration 219/1000 | Loss: 0.00006441
Iteration 220/1000 | Loss: 0.00006441
Iteration 221/1000 | Loss: 0.00006441
Iteration 222/1000 | Loss: 0.00006441
Iteration 223/1000 | Loss: 0.00006441
Iteration 224/1000 | Loss: 0.00006440
Iteration 225/1000 | Loss: 0.00006440
Iteration 226/1000 | Loss: 0.00006440
Iteration 227/1000 | Loss: 0.00006439
Iteration 228/1000 | Loss: 0.00006439
Iteration 229/1000 | Loss: 0.00006438
Iteration 230/1000 | Loss: 0.00006438
Iteration 231/1000 | Loss: 0.00006438
Iteration 232/1000 | Loss: 0.00006437
Iteration 233/1000 | Loss: 0.00006437
Iteration 234/1000 | Loss: 0.00006437
Iteration 235/1000 | Loss: 0.00048085
Iteration 236/1000 | Loss: 0.00059572
Iteration 237/1000 | Loss: 0.00177359
Iteration 238/1000 | Loss: 0.00154202
Iteration 239/1000 | Loss: 0.00123837
Iteration 240/1000 | Loss: 0.00330971
Iteration 241/1000 | Loss: 0.00007208
Iteration 242/1000 | Loss: 0.00006502
Iteration 243/1000 | Loss: 0.00018281
Iteration 244/1000 | Loss: 0.00028549
Iteration 245/1000 | Loss: 0.00011665
Iteration 246/1000 | Loss: 0.00012666
Iteration 247/1000 | Loss: 0.00006342
Iteration 248/1000 | Loss: 0.00015882
Iteration 249/1000 | Loss: 0.00006698
Iteration 250/1000 | Loss: 0.00006444
Iteration 251/1000 | Loss: 0.00029309
Iteration 252/1000 | Loss: 0.00006216
Iteration 253/1000 | Loss: 0.00006200
Iteration 254/1000 | Loss: 0.00006199
Iteration 255/1000 | Loss: 0.00006198
Iteration 256/1000 | Loss: 0.00006189
Iteration 257/1000 | Loss: 0.00006166
Iteration 258/1000 | Loss: 0.00006148
Iteration 259/1000 | Loss: 0.00006123
Iteration 260/1000 | Loss: 0.00006119
Iteration 261/1000 | Loss: 0.00006118
Iteration 262/1000 | Loss: 0.00006117
Iteration 263/1000 | Loss: 0.00006110
Iteration 264/1000 | Loss: 0.00006109
Iteration 265/1000 | Loss: 0.00006108
Iteration 266/1000 | Loss: 0.00006108
Iteration 267/1000 | Loss: 0.00006108
Iteration 268/1000 | Loss: 0.00006107
Iteration 269/1000 | Loss: 0.00006107
Iteration 270/1000 | Loss: 0.00006106
Iteration 271/1000 | Loss: 0.00006105
Iteration 272/1000 | Loss: 0.00006105
Iteration 273/1000 | Loss: 0.00006105
Iteration 274/1000 | Loss: 0.00006104
Iteration 275/1000 | Loss: 0.00006103
Iteration 276/1000 | Loss: 0.00006103
Iteration 277/1000 | Loss: 0.00006103
Iteration 278/1000 | Loss: 0.00006102
Iteration 279/1000 | Loss: 0.00006102
Iteration 280/1000 | Loss: 0.00006102
Iteration 281/1000 | Loss: 0.00006100
Iteration 282/1000 | Loss: 0.00006100
Iteration 283/1000 | Loss: 0.00006100
Iteration 284/1000 | Loss: 0.00006100
Iteration 285/1000 | Loss: 0.00006100
Iteration 286/1000 | Loss: 0.00006099
Iteration 287/1000 | Loss: 0.00006099
Iteration 288/1000 | Loss: 0.00006099
Iteration 289/1000 | Loss: 0.00006098
Iteration 290/1000 | Loss: 0.00006097
Iteration 291/1000 | Loss: 0.00006097
Iteration 292/1000 | Loss: 0.00006097
Iteration 293/1000 | Loss: 0.00006096
Iteration 294/1000 | Loss: 0.00006095
Iteration 295/1000 | Loss: 0.00006095
Iteration 296/1000 | Loss: 0.00006094
Iteration 297/1000 | Loss: 0.00006094
Iteration 298/1000 | Loss: 0.00006094
Iteration 299/1000 | Loss: 0.00006094
Iteration 300/1000 | Loss: 0.00006093
Iteration 301/1000 | Loss: 0.00006093
Iteration 302/1000 | Loss: 0.00006093
Iteration 303/1000 | Loss: 0.00006092
Iteration 304/1000 | Loss: 0.00006092
Iteration 305/1000 | Loss: 0.00006092
Iteration 306/1000 | Loss: 0.00006092
Iteration 307/1000 | Loss: 0.00006092
Iteration 308/1000 | Loss: 0.00006092
Iteration 309/1000 | Loss: 0.00006092
Iteration 310/1000 | Loss: 0.00006092
Iteration 311/1000 | Loss: 0.00006092
Iteration 312/1000 | Loss: 0.00006092
Iteration 313/1000 | Loss: 0.00006091
Iteration 314/1000 | Loss: 0.00006091
Iteration 315/1000 | Loss: 0.00006091
Iteration 316/1000 | Loss: 0.00006091
Iteration 317/1000 | Loss: 0.00006091
Iteration 318/1000 | Loss: 0.00006091
Iteration 319/1000 | Loss: 0.00006091
Iteration 320/1000 | Loss: 0.00006091
Iteration 321/1000 | Loss: 0.00006090
Iteration 322/1000 | Loss: 0.00006090
Iteration 323/1000 | Loss: 0.00006090
Iteration 324/1000 | Loss: 0.00006090
Iteration 325/1000 | Loss: 0.00006090
Iteration 326/1000 | Loss: 0.00006090
Iteration 327/1000 | Loss: 0.00006090
Iteration 328/1000 | Loss: 0.00006090
Iteration 329/1000 | Loss: 0.00006090
Iteration 330/1000 | Loss: 0.00006090
Iteration 331/1000 | Loss: 0.00006090
Iteration 332/1000 | Loss: 0.00006090
Iteration 333/1000 | Loss: 0.00006090
Iteration 334/1000 | Loss: 0.00006090
Iteration 335/1000 | Loss: 0.00006089
Iteration 336/1000 | Loss: 0.00006089
Iteration 337/1000 | Loss: 0.00006089
Iteration 338/1000 | Loss: 0.00006089
Iteration 339/1000 | Loss: 0.00006089
Iteration 340/1000 | Loss: 0.00006089
Iteration 341/1000 | Loss: 0.00006089
Iteration 342/1000 | Loss: 0.00006089
Iteration 343/1000 | Loss: 0.00006089
Iteration 344/1000 | Loss: 0.00006089
Iteration 345/1000 | Loss: 0.00006089
Iteration 346/1000 | Loss: 0.00006089
Iteration 347/1000 | Loss: 0.00006088
Iteration 348/1000 | Loss: 0.00006088
Iteration 349/1000 | Loss: 0.00006088
Iteration 350/1000 | Loss: 0.00006088
Iteration 351/1000 | Loss: 0.00006088
Iteration 352/1000 | Loss: 0.00006088
Iteration 353/1000 | Loss: 0.00006088
Iteration 354/1000 | Loss: 0.00006088
Iteration 355/1000 | Loss: 0.00006088
Iteration 356/1000 | Loss: 0.00006088
Iteration 357/1000 | Loss: 0.00006088
Iteration 358/1000 | Loss: 0.00006088
Iteration 359/1000 | Loss: 0.00006088
Iteration 360/1000 | Loss: 0.00006088
Iteration 361/1000 | Loss: 0.00006088
Iteration 362/1000 | Loss: 0.00006088
Iteration 363/1000 | Loss: 0.00006088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 363. Stopping optimization.
Last 5 losses: [6.088221198297106e-05, 6.088221198297106e-05, 6.088221198297106e-05, 6.088221198297106e-05, 6.088221198297106e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.088221198297106e-05

Optimization complete. Final v2v error: 4.236875534057617 mm

Highest mean error: 11.958053588867188 mm for frame 18

Lowest mean error: 2.9891180992126465 mm for frame 97

Saving results

Total time: 335.8201153278351
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045344
Iteration 2/25 | Loss: 0.00174158
Iteration 3/25 | Loss: 0.00138560
Iteration 4/25 | Loss: 0.00128833
Iteration 5/25 | Loss: 0.00127485
Iteration 6/25 | Loss: 0.00116935
Iteration 7/25 | Loss: 0.00109864
Iteration 8/25 | Loss: 0.00108423
Iteration 9/25 | Loss: 0.00105672
Iteration 10/25 | Loss: 0.00097791
Iteration 11/25 | Loss: 0.00097736
Iteration 12/25 | Loss: 0.00094142
Iteration 13/25 | Loss: 0.00093125
Iteration 14/25 | Loss: 0.00093240
Iteration 15/25 | Loss: 0.00092975
Iteration 16/25 | Loss: 0.00092797
Iteration 17/25 | Loss: 0.00092593
Iteration 18/25 | Loss: 0.00093348
Iteration 19/25 | Loss: 0.00093481
Iteration 20/25 | Loss: 0.00093416
Iteration 21/25 | Loss: 0.00093582
Iteration 22/25 | Loss: 0.00093770
Iteration 23/25 | Loss: 0.00093323
Iteration 24/25 | Loss: 0.00092581
Iteration 25/25 | Loss: 0.00092392

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49476516
Iteration 2/25 | Loss: 0.00087092
Iteration 3/25 | Loss: 0.00087092
Iteration 4/25 | Loss: 0.00087092
Iteration 5/25 | Loss: 0.00087092
Iteration 6/25 | Loss: 0.00087092
Iteration 7/25 | Loss: 0.00087092
Iteration 8/25 | Loss: 0.00087092
Iteration 9/25 | Loss: 0.00087092
Iteration 10/25 | Loss: 0.00087092
Iteration 11/25 | Loss: 0.00087092
Iteration 12/25 | Loss: 0.00087092
Iteration 13/25 | Loss: 0.00087092
Iteration 14/25 | Loss: 0.00087092
Iteration 15/25 | Loss: 0.00087092
Iteration 16/25 | Loss: 0.00087092
Iteration 17/25 | Loss: 0.00087092
Iteration 18/25 | Loss: 0.00087092
Iteration 19/25 | Loss: 0.00087092
Iteration 20/25 | Loss: 0.00087092
Iteration 21/25 | Loss: 0.00087092
Iteration 22/25 | Loss: 0.00087092
Iteration 23/25 | Loss: 0.00087092
Iteration 24/25 | Loss: 0.00087092
Iteration 25/25 | Loss: 0.00087092

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087092
Iteration 2/1000 | Loss: 0.00005890
Iteration 3/1000 | Loss: 0.00008780
Iteration 4/1000 | Loss: 0.00019957
Iteration 5/1000 | Loss: 0.00014652
Iteration 6/1000 | Loss: 0.00012585
Iteration 7/1000 | Loss: 0.00015167
Iteration 8/1000 | Loss: 0.00017285
Iteration 9/1000 | Loss: 0.00016553
Iteration 10/1000 | Loss: 0.00006131
Iteration 11/1000 | Loss: 0.00006766
Iteration 12/1000 | Loss: 0.00009691
Iteration 13/1000 | Loss: 0.00008309
Iteration 14/1000 | Loss: 0.00011684
Iteration 15/1000 | Loss: 0.00008371
Iteration 16/1000 | Loss: 0.00016781
Iteration 17/1000 | Loss: 0.00019046
Iteration 18/1000 | Loss: 0.00011592
Iteration 19/1000 | Loss: 0.00011620
Iteration 20/1000 | Loss: 0.00012685
Iteration 21/1000 | Loss: 0.00012292
Iteration 22/1000 | Loss: 0.00011578
Iteration 23/1000 | Loss: 0.00011487
Iteration 24/1000 | Loss: 0.00009760
Iteration 25/1000 | Loss: 0.00018025
Iteration 26/1000 | Loss: 0.00017459
Iteration 27/1000 | Loss: 0.00016224
Iteration 28/1000 | Loss: 0.00016268
Iteration 29/1000 | Loss: 0.00021810
Iteration 30/1000 | Loss: 0.00020772
Iteration 31/1000 | Loss: 0.00007476
Iteration 32/1000 | Loss: 0.00015481
Iteration 33/1000 | Loss: 0.00015100
Iteration 34/1000 | Loss: 0.00023314
Iteration 35/1000 | Loss: 0.00019528
Iteration 36/1000 | Loss: 0.00006930
Iteration 37/1000 | Loss: 0.00034175
Iteration 38/1000 | Loss: 0.00025350
Iteration 39/1000 | Loss: 0.00007249
Iteration 40/1000 | Loss: 0.00018675
Iteration 41/1000 | Loss: 0.00020426
Iteration 42/1000 | Loss: 0.00020483
Iteration 43/1000 | Loss: 0.00009671
Iteration 44/1000 | Loss: 0.00011891
Iteration 45/1000 | Loss: 0.00023183
Iteration 46/1000 | Loss: 0.00018449
Iteration 47/1000 | Loss: 0.00010274
Iteration 48/1000 | Loss: 0.00011798
Iteration 49/1000 | Loss: 0.00022933
Iteration 50/1000 | Loss: 0.00029050
Iteration 51/1000 | Loss: 0.00010961
Iteration 52/1000 | Loss: 0.00010985
Iteration 53/1000 | Loss: 0.00031738
Iteration 54/1000 | Loss: 0.00015587
Iteration 55/1000 | Loss: 0.00013585
Iteration 56/1000 | Loss: 0.00007283
Iteration 57/1000 | Loss: 0.00017451
Iteration 58/1000 | Loss: 0.00017372
Iteration 59/1000 | Loss: 0.00009791
Iteration 60/1000 | Loss: 0.00011325
Iteration 61/1000 | Loss: 0.00012042
Iteration 62/1000 | Loss: 0.00031354
Iteration 63/1000 | Loss: 0.00007620
Iteration 64/1000 | Loss: 0.00012014
Iteration 65/1000 | Loss: 0.00014308
Iteration 66/1000 | Loss: 0.00013024
Iteration 67/1000 | Loss: 0.00013333
Iteration 68/1000 | Loss: 0.00017075
Iteration 69/1000 | Loss: 0.00003172
Iteration 70/1000 | Loss: 0.00006528
Iteration 71/1000 | Loss: 0.00006634
Iteration 72/1000 | Loss: 0.00004952
Iteration 73/1000 | Loss: 0.00013222
Iteration 74/1000 | Loss: 0.00004439
Iteration 75/1000 | Loss: 0.00004094
Iteration 76/1000 | Loss: 0.00002066
Iteration 77/1000 | Loss: 0.00009622
Iteration 78/1000 | Loss: 0.00009558
Iteration 79/1000 | Loss: 0.00002699
Iteration 80/1000 | Loss: 0.00002366
Iteration 81/1000 | Loss: 0.00002159
Iteration 82/1000 | Loss: 0.00002061
Iteration 83/1000 | Loss: 0.00001997
Iteration 84/1000 | Loss: 0.00011916
Iteration 85/1000 | Loss: 0.00006738
Iteration 86/1000 | Loss: 0.00008839
Iteration 87/1000 | Loss: 0.00005624
Iteration 88/1000 | Loss: 0.00007644
Iteration 89/1000 | Loss: 0.00005069
Iteration 90/1000 | Loss: 0.00008405
Iteration 91/1000 | Loss: 0.00004626
Iteration 92/1000 | Loss: 0.00016239
Iteration 93/1000 | Loss: 0.00018233
Iteration 94/1000 | Loss: 0.00004128
Iteration 95/1000 | Loss: 0.00015470
Iteration 96/1000 | Loss: 0.00011912
Iteration 97/1000 | Loss: 0.00011918
Iteration 98/1000 | Loss: 0.00014866
Iteration 99/1000 | Loss: 0.00010731
Iteration 100/1000 | Loss: 0.00012575
Iteration 101/1000 | Loss: 0.00002752
Iteration 102/1000 | Loss: 0.00023280
Iteration 103/1000 | Loss: 0.00025527
Iteration 104/1000 | Loss: 0.00026390
Iteration 105/1000 | Loss: 0.00074335
Iteration 106/1000 | Loss: 0.00045254
Iteration 107/1000 | Loss: 0.00064262
Iteration 108/1000 | Loss: 0.00014462
Iteration 109/1000 | Loss: 0.00002749
Iteration 110/1000 | Loss: 0.00005650
Iteration 111/1000 | Loss: 0.00008925
Iteration 112/1000 | Loss: 0.00014923
Iteration 113/1000 | Loss: 0.00002464
Iteration 114/1000 | Loss: 0.00004448
Iteration 115/1000 | Loss: 0.00017191
Iteration 116/1000 | Loss: 0.00007336
Iteration 117/1000 | Loss: 0.00023495
Iteration 118/1000 | Loss: 0.00007051
Iteration 119/1000 | Loss: 0.00010089
Iteration 120/1000 | Loss: 0.00007959
Iteration 121/1000 | Loss: 0.00016247
Iteration 122/1000 | Loss: 0.00017212
Iteration 123/1000 | Loss: 0.00017149
Iteration 124/1000 | Loss: 0.00002551
Iteration 125/1000 | Loss: 0.00003708
Iteration 126/1000 | Loss: 0.00023985
Iteration 127/1000 | Loss: 0.00003446
Iteration 128/1000 | Loss: 0.00003323
Iteration 129/1000 | Loss: 0.00003070
Iteration 130/1000 | Loss: 0.00002997
Iteration 131/1000 | Loss: 0.00003251
Iteration 132/1000 | Loss: 0.00002767
Iteration 133/1000 | Loss: 0.00001782
Iteration 134/1000 | Loss: 0.00002695
Iteration 135/1000 | Loss: 0.00055623
Iteration 136/1000 | Loss: 0.00041672
Iteration 137/1000 | Loss: 0.00035271
Iteration 138/1000 | Loss: 0.00023050
Iteration 139/1000 | Loss: 0.00003073
Iteration 140/1000 | Loss: 0.00001798
Iteration 141/1000 | Loss: 0.00038116
Iteration 142/1000 | Loss: 0.00009375
Iteration 143/1000 | Loss: 0.00018434
Iteration 144/1000 | Loss: 0.00002080
Iteration 145/1000 | Loss: 0.00001664
Iteration 146/1000 | Loss: 0.00001532
Iteration 147/1000 | Loss: 0.00001404
Iteration 148/1000 | Loss: 0.00001330
Iteration 149/1000 | Loss: 0.00001281
Iteration 150/1000 | Loss: 0.00001252
Iteration 151/1000 | Loss: 0.00001230
Iteration 152/1000 | Loss: 0.00001222
Iteration 153/1000 | Loss: 0.00001218
Iteration 154/1000 | Loss: 0.00001218
Iteration 155/1000 | Loss: 0.00001217
Iteration 156/1000 | Loss: 0.00001217
Iteration 157/1000 | Loss: 0.00001216
Iteration 158/1000 | Loss: 0.00001216
Iteration 159/1000 | Loss: 0.00001216
Iteration 160/1000 | Loss: 0.00001215
Iteration 161/1000 | Loss: 0.00001214
Iteration 162/1000 | Loss: 0.00001214
Iteration 163/1000 | Loss: 0.00001213
Iteration 164/1000 | Loss: 0.00001213
Iteration 165/1000 | Loss: 0.00001213
Iteration 166/1000 | Loss: 0.00001212
Iteration 167/1000 | Loss: 0.00001211
Iteration 168/1000 | Loss: 0.00001210
Iteration 169/1000 | Loss: 0.00001210
Iteration 170/1000 | Loss: 0.00001209
Iteration 171/1000 | Loss: 0.00001207
Iteration 172/1000 | Loss: 0.00001207
Iteration 173/1000 | Loss: 0.00001201
Iteration 174/1000 | Loss: 0.00001199
Iteration 175/1000 | Loss: 0.00001198
Iteration 176/1000 | Loss: 0.00001198
Iteration 177/1000 | Loss: 0.00001198
Iteration 178/1000 | Loss: 0.00001198
Iteration 179/1000 | Loss: 0.00001197
Iteration 180/1000 | Loss: 0.00001197
Iteration 181/1000 | Loss: 0.00001196
Iteration 182/1000 | Loss: 0.00001196
Iteration 183/1000 | Loss: 0.00001195
Iteration 184/1000 | Loss: 0.00001194
Iteration 185/1000 | Loss: 0.00001194
Iteration 186/1000 | Loss: 0.00001194
Iteration 187/1000 | Loss: 0.00001193
Iteration 188/1000 | Loss: 0.00001193
Iteration 189/1000 | Loss: 0.00001193
Iteration 190/1000 | Loss: 0.00001193
Iteration 191/1000 | Loss: 0.00001192
Iteration 192/1000 | Loss: 0.00001192
Iteration 193/1000 | Loss: 0.00001192
Iteration 194/1000 | Loss: 0.00001192
Iteration 195/1000 | Loss: 0.00001191
Iteration 196/1000 | Loss: 0.00001191
Iteration 197/1000 | Loss: 0.00001191
Iteration 198/1000 | Loss: 0.00001191
Iteration 199/1000 | Loss: 0.00001191
Iteration 200/1000 | Loss: 0.00001190
Iteration 201/1000 | Loss: 0.00001190
Iteration 202/1000 | Loss: 0.00001190
Iteration 203/1000 | Loss: 0.00001190
Iteration 204/1000 | Loss: 0.00001189
Iteration 205/1000 | Loss: 0.00001189
Iteration 206/1000 | Loss: 0.00001189
Iteration 207/1000 | Loss: 0.00001188
Iteration 208/1000 | Loss: 0.00001188
Iteration 209/1000 | Loss: 0.00001188
Iteration 210/1000 | Loss: 0.00001187
Iteration 211/1000 | Loss: 0.00001187
Iteration 212/1000 | Loss: 0.00001187
Iteration 213/1000 | Loss: 0.00001187
Iteration 214/1000 | Loss: 0.00001187
Iteration 215/1000 | Loss: 0.00001187
Iteration 216/1000 | Loss: 0.00001187
Iteration 217/1000 | Loss: 0.00001187
Iteration 218/1000 | Loss: 0.00001187
Iteration 219/1000 | Loss: 0.00001186
Iteration 220/1000 | Loss: 0.00001186
Iteration 221/1000 | Loss: 0.00001186
Iteration 222/1000 | Loss: 0.00001186
Iteration 223/1000 | Loss: 0.00001186
Iteration 224/1000 | Loss: 0.00001185
Iteration 225/1000 | Loss: 0.00001185
Iteration 226/1000 | Loss: 0.00001185
Iteration 227/1000 | Loss: 0.00001185
Iteration 228/1000 | Loss: 0.00001185
Iteration 229/1000 | Loss: 0.00001185
Iteration 230/1000 | Loss: 0.00001185
Iteration 231/1000 | Loss: 0.00001185
Iteration 232/1000 | Loss: 0.00001185
Iteration 233/1000 | Loss: 0.00001185
Iteration 234/1000 | Loss: 0.00001185
Iteration 235/1000 | Loss: 0.00001185
Iteration 236/1000 | Loss: 0.00001185
Iteration 237/1000 | Loss: 0.00001184
Iteration 238/1000 | Loss: 0.00001184
Iteration 239/1000 | Loss: 0.00001184
Iteration 240/1000 | Loss: 0.00001184
Iteration 241/1000 | Loss: 0.00001184
Iteration 242/1000 | Loss: 0.00001184
Iteration 243/1000 | Loss: 0.00001184
Iteration 244/1000 | Loss: 0.00001184
Iteration 245/1000 | Loss: 0.00001184
Iteration 246/1000 | Loss: 0.00001184
Iteration 247/1000 | Loss: 0.00001183
Iteration 248/1000 | Loss: 0.00001183
Iteration 249/1000 | Loss: 0.00001183
Iteration 250/1000 | Loss: 0.00001183
Iteration 251/1000 | Loss: 0.00001183
Iteration 252/1000 | Loss: 0.00001183
Iteration 253/1000 | Loss: 0.00001183
Iteration 254/1000 | Loss: 0.00001183
Iteration 255/1000 | Loss: 0.00001182
Iteration 256/1000 | Loss: 0.00001182
Iteration 257/1000 | Loss: 0.00001182
Iteration 258/1000 | Loss: 0.00001182
Iteration 259/1000 | Loss: 0.00001182
Iteration 260/1000 | Loss: 0.00001182
Iteration 261/1000 | Loss: 0.00001182
Iteration 262/1000 | Loss: 0.00001182
Iteration 263/1000 | Loss: 0.00001182
Iteration 264/1000 | Loss: 0.00001182
Iteration 265/1000 | Loss: 0.00001181
Iteration 266/1000 | Loss: 0.00001181
Iteration 267/1000 | Loss: 0.00001181
Iteration 268/1000 | Loss: 0.00001181
Iteration 269/1000 | Loss: 0.00001181
Iteration 270/1000 | Loss: 0.00001181
Iteration 271/1000 | Loss: 0.00001181
Iteration 272/1000 | Loss: 0.00001181
Iteration 273/1000 | Loss: 0.00001181
Iteration 274/1000 | Loss: 0.00001181
Iteration 275/1000 | Loss: 0.00001181
Iteration 276/1000 | Loss: 0.00001181
Iteration 277/1000 | Loss: 0.00001180
Iteration 278/1000 | Loss: 0.00001180
Iteration 279/1000 | Loss: 0.00001180
Iteration 280/1000 | Loss: 0.00001180
Iteration 281/1000 | Loss: 0.00001180
Iteration 282/1000 | Loss: 0.00001180
Iteration 283/1000 | Loss: 0.00001180
Iteration 284/1000 | Loss: 0.00001180
Iteration 285/1000 | Loss: 0.00001180
Iteration 286/1000 | Loss: 0.00001180
Iteration 287/1000 | Loss: 0.00001180
Iteration 288/1000 | Loss: 0.00001180
Iteration 289/1000 | Loss: 0.00001180
Iteration 290/1000 | Loss: 0.00001180
Iteration 291/1000 | Loss: 0.00001180
Iteration 292/1000 | Loss: 0.00001180
Iteration 293/1000 | Loss: 0.00001180
Iteration 294/1000 | Loss: 0.00001180
Iteration 295/1000 | Loss: 0.00001180
Iteration 296/1000 | Loss: 0.00001180
Iteration 297/1000 | Loss: 0.00001180
Iteration 298/1000 | Loss: 0.00001180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 298. Stopping optimization.
Last 5 losses: [1.179751143354224e-05, 1.179751143354224e-05, 1.179751143354224e-05, 1.179751143354224e-05, 1.179751143354224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.179751143354224e-05

Optimization complete. Final v2v error: 2.9026429653167725 mm

Highest mean error: 4.25410795211792 mm for frame 72

Lowest mean error: 2.1936988830566406 mm for frame 1

Saving results

Total time: 267.344158411026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952002
Iteration 2/25 | Loss: 0.00111045
Iteration 3/25 | Loss: 0.00088042
Iteration 4/25 | Loss: 0.00086467
Iteration 5/25 | Loss: 0.00086014
Iteration 6/25 | Loss: 0.00085872
Iteration 7/25 | Loss: 0.00085872
Iteration 8/25 | Loss: 0.00085872
Iteration 9/25 | Loss: 0.00085872
Iteration 10/25 | Loss: 0.00085872
Iteration 11/25 | Loss: 0.00085872
Iteration 12/25 | Loss: 0.00085872
Iteration 13/25 | Loss: 0.00085870
Iteration 14/25 | Loss: 0.00085870
Iteration 15/25 | Loss: 0.00085870
Iteration 16/25 | Loss: 0.00085870
Iteration 17/25 | Loss: 0.00085870
Iteration 18/25 | Loss: 0.00085870
Iteration 19/25 | Loss: 0.00085870
Iteration 20/25 | Loss: 0.00085870
Iteration 21/25 | Loss: 0.00085870
Iteration 22/25 | Loss: 0.00085870
Iteration 23/25 | Loss: 0.00085870
Iteration 24/25 | Loss: 0.00085870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008586950716562569, 0.0008586950716562569, 0.0008586950716562569, 0.0008586950716562569, 0.0008586950716562569]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008586950716562569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.47241235
Iteration 2/25 | Loss: 0.00048539
Iteration 3/25 | Loss: 0.00048539
Iteration 4/25 | Loss: 0.00048539
Iteration 5/25 | Loss: 0.00048539
Iteration 6/25 | Loss: 0.00048539
Iteration 7/25 | Loss: 0.00048539
Iteration 8/25 | Loss: 0.00048539
Iteration 9/25 | Loss: 0.00048539
Iteration 10/25 | Loss: 0.00048539
Iteration 11/25 | Loss: 0.00048539
Iteration 12/25 | Loss: 0.00048539
Iteration 13/25 | Loss: 0.00048539
Iteration 14/25 | Loss: 0.00048539
Iteration 15/25 | Loss: 0.00048539
Iteration 16/25 | Loss: 0.00048539
Iteration 17/25 | Loss: 0.00048539
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004853895807173103, 0.0004853895807173103, 0.0004853895807173103, 0.0004853895807173103, 0.0004853895807173103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004853895807173103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048539
Iteration 2/1000 | Loss: 0.00003609
Iteration 3/1000 | Loss: 0.00001943
Iteration 4/1000 | Loss: 0.00001583
Iteration 5/1000 | Loss: 0.00001453
Iteration 6/1000 | Loss: 0.00001377
Iteration 7/1000 | Loss: 0.00001317
Iteration 8/1000 | Loss: 0.00001274
Iteration 9/1000 | Loss: 0.00001243
Iteration 10/1000 | Loss: 0.00001211
Iteration 11/1000 | Loss: 0.00001202
Iteration 12/1000 | Loss: 0.00001199
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001198
Iteration 15/1000 | Loss: 0.00001196
Iteration 16/1000 | Loss: 0.00001193
Iteration 17/1000 | Loss: 0.00001192
Iteration 18/1000 | Loss: 0.00001191
Iteration 19/1000 | Loss: 0.00001189
Iteration 20/1000 | Loss: 0.00001188
Iteration 21/1000 | Loss: 0.00001181
Iteration 22/1000 | Loss: 0.00001179
Iteration 23/1000 | Loss: 0.00001179
Iteration 24/1000 | Loss: 0.00001179
Iteration 25/1000 | Loss: 0.00001179
Iteration 26/1000 | Loss: 0.00001178
Iteration 27/1000 | Loss: 0.00001177
Iteration 28/1000 | Loss: 0.00001176
Iteration 29/1000 | Loss: 0.00001176
Iteration 30/1000 | Loss: 0.00001176
Iteration 31/1000 | Loss: 0.00001175
Iteration 32/1000 | Loss: 0.00001175
Iteration 33/1000 | Loss: 0.00001175
Iteration 34/1000 | Loss: 0.00001174
Iteration 35/1000 | Loss: 0.00001174
Iteration 36/1000 | Loss: 0.00001173
Iteration 37/1000 | Loss: 0.00001173
Iteration 38/1000 | Loss: 0.00001172
Iteration 39/1000 | Loss: 0.00001172
Iteration 40/1000 | Loss: 0.00001172
Iteration 41/1000 | Loss: 0.00001171
Iteration 42/1000 | Loss: 0.00001171
Iteration 43/1000 | Loss: 0.00001171
Iteration 44/1000 | Loss: 0.00001170
Iteration 45/1000 | Loss: 0.00001170
Iteration 46/1000 | Loss: 0.00001170
Iteration 47/1000 | Loss: 0.00001170
Iteration 48/1000 | Loss: 0.00001170
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001170
Iteration 51/1000 | Loss: 0.00001170
Iteration 52/1000 | Loss: 0.00001169
Iteration 53/1000 | Loss: 0.00001169
Iteration 54/1000 | Loss: 0.00001169
Iteration 55/1000 | Loss: 0.00001169
Iteration 56/1000 | Loss: 0.00001169
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001169
Iteration 59/1000 | Loss: 0.00001169
Iteration 60/1000 | Loss: 0.00001169
Iteration 61/1000 | Loss: 0.00001168
Iteration 62/1000 | Loss: 0.00001168
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001168
Iteration 65/1000 | Loss: 0.00001168
Iteration 66/1000 | Loss: 0.00001168
Iteration 67/1000 | Loss: 0.00001168
Iteration 68/1000 | Loss: 0.00001168
Iteration 69/1000 | Loss: 0.00001168
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001167
Iteration 72/1000 | Loss: 0.00001167
Iteration 73/1000 | Loss: 0.00001167
Iteration 74/1000 | Loss: 0.00001166
Iteration 75/1000 | Loss: 0.00001166
Iteration 76/1000 | Loss: 0.00001166
Iteration 77/1000 | Loss: 0.00001166
Iteration 78/1000 | Loss: 0.00001166
Iteration 79/1000 | Loss: 0.00001165
Iteration 80/1000 | Loss: 0.00001165
Iteration 81/1000 | Loss: 0.00001165
Iteration 82/1000 | Loss: 0.00001165
Iteration 83/1000 | Loss: 0.00001165
Iteration 84/1000 | Loss: 0.00001165
Iteration 85/1000 | Loss: 0.00001165
Iteration 86/1000 | Loss: 0.00001165
Iteration 87/1000 | Loss: 0.00001165
Iteration 88/1000 | Loss: 0.00001165
Iteration 89/1000 | Loss: 0.00001164
Iteration 90/1000 | Loss: 0.00001164
Iteration 91/1000 | Loss: 0.00001164
Iteration 92/1000 | Loss: 0.00001164
Iteration 93/1000 | Loss: 0.00001164
Iteration 94/1000 | Loss: 0.00001164
Iteration 95/1000 | Loss: 0.00001164
Iteration 96/1000 | Loss: 0.00001164
Iteration 97/1000 | Loss: 0.00001164
Iteration 98/1000 | Loss: 0.00001163
Iteration 99/1000 | Loss: 0.00001163
Iteration 100/1000 | Loss: 0.00001163
Iteration 101/1000 | Loss: 0.00001163
Iteration 102/1000 | Loss: 0.00001163
Iteration 103/1000 | Loss: 0.00001163
Iteration 104/1000 | Loss: 0.00001163
Iteration 105/1000 | Loss: 0.00001162
Iteration 106/1000 | Loss: 0.00001162
Iteration 107/1000 | Loss: 0.00001161
Iteration 108/1000 | Loss: 0.00001161
Iteration 109/1000 | Loss: 0.00001161
Iteration 110/1000 | Loss: 0.00001161
Iteration 111/1000 | Loss: 0.00001161
Iteration 112/1000 | Loss: 0.00001161
Iteration 113/1000 | Loss: 0.00001161
Iteration 114/1000 | Loss: 0.00001161
Iteration 115/1000 | Loss: 0.00001161
Iteration 116/1000 | Loss: 0.00001161
Iteration 117/1000 | Loss: 0.00001160
Iteration 118/1000 | Loss: 0.00001160
Iteration 119/1000 | Loss: 0.00001160
Iteration 120/1000 | Loss: 0.00001160
Iteration 121/1000 | Loss: 0.00001160
Iteration 122/1000 | Loss: 0.00001160
Iteration 123/1000 | Loss: 0.00001160
Iteration 124/1000 | Loss: 0.00001160
Iteration 125/1000 | Loss: 0.00001160
Iteration 126/1000 | Loss: 0.00001160
Iteration 127/1000 | Loss: 0.00001160
Iteration 128/1000 | Loss: 0.00001160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.1597948287089821e-05, 1.1597948287089821e-05, 1.1597948287089821e-05, 1.1597948287089821e-05, 1.1597948287089821e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1597948287089821e-05

Optimization complete. Final v2v error: 2.84570574760437 mm

Highest mean error: 3.448479652404785 mm for frame 13

Lowest mean error: 2.2894623279571533 mm for frame 219

Saving results

Total time: 38.92031383514404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079898
Iteration 2/25 | Loss: 0.01079898
Iteration 3/25 | Loss: 0.01079897
Iteration 4/25 | Loss: 0.00169541
Iteration 5/25 | Loss: 0.00117830
Iteration 6/25 | Loss: 0.00104462
Iteration 7/25 | Loss: 0.00104548
Iteration 8/25 | Loss: 0.00104345
Iteration 9/25 | Loss: 0.00103982
Iteration 10/25 | Loss: 0.00101555
Iteration 11/25 | Loss: 0.00099833
Iteration 12/25 | Loss: 0.00099311
Iteration 13/25 | Loss: 0.00098888
Iteration 14/25 | Loss: 0.00099223
Iteration 15/25 | Loss: 0.00099142
Iteration 16/25 | Loss: 0.00099150
Iteration 17/25 | Loss: 0.00098373
Iteration 18/25 | Loss: 0.00098644
Iteration 19/25 | Loss: 0.00097839
Iteration 20/25 | Loss: 0.00097887
Iteration 21/25 | Loss: 0.00097858
Iteration 22/25 | Loss: 0.00097623
Iteration 23/25 | Loss: 0.00097595
Iteration 24/25 | Loss: 0.00097579
Iteration 25/25 | Loss: 0.00097778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64541984
Iteration 2/25 | Loss: 0.00089139
Iteration 3/25 | Loss: 0.00089139
Iteration 4/25 | Loss: 0.00089139
Iteration 5/25 | Loss: 0.00089139
Iteration 6/25 | Loss: 0.00089139
Iteration 7/25 | Loss: 0.00089139
Iteration 8/25 | Loss: 0.00089139
Iteration 9/25 | Loss: 0.00089139
Iteration 10/25 | Loss: 0.00089139
Iteration 11/25 | Loss: 0.00089139
Iteration 12/25 | Loss: 0.00089139
Iteration 13/25 | Loss: 0.00089139
Iteration 14/25 | Loss: 0.00089139
Iteration 15/25 | Loss: 0.00089139
Iteration 16/25 | Loss: 0.00089139
Iteration 17/25 | Loss: 0.00089139
Iteration 18/25 | Loss: 0.00089139
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008913887431845069, 0.0008913887431845069, 0.0008913887431845069, 0.0008913887431845069, 0.0008913887431845069]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008913887431845069

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089139
Iteration 2/1000 | Loss: 0.00035158
Iteration 3/1000 | Loss: 0.00033127
Iteration 4/1000 | Loss: 0.00028905
Iteration 5/1000 | Loss: 0.00040539
Iteration 6/1000 | Loss: 0.00003215
Iteration 7/1000 | Loss: 0.00028410
Iteration 8/1000 | Loss: 0.00013176
Iteration 9/1000 | Loss: 0.00010716
Iteration 10/1000 | Loss: 0.00011493
Iteration 11/1000 | Loss: 0.00011606
Iteration 12/1000 | Loss: 0.00009166
Iteration 13/1000 | Loss: 0.00009323
Iteration 14/1000 | Loss: 0.00007410
Iteration 15/1000 | Loss: 0.00005772
Iteration 16/1000 | Loss: 0.00002303
Iteration 17/1000 | Loss: 0.00017333
Iteration 18/1000 | Loss: 0.00001755
Iteration 19/1000 | Loss: 0.00013557
Iteration 20/1000 | Loss: 0.00007938
Iteration 21/1000 | Loss: 0.00001636
Iteration 22/1000 | Loss: 0.00001584
Iteration 23/1000 | Loss: 0.00001545
Iteration 24/1000 | Loss: 0.00001513
Iteration 25/1000 | Loss: 0.00001493
Iteration 26/1000 | Loss: 0.00001472
Iteration 27/1000 | Loss: 0.00001468
Iteration 28/1000 | Loss: 0.00001464
Iteration 29/1000 | Loss: 0.00001463
Iteration 30/1000 | Loss: 0.00001458
Iteration 31/1000 | Loss: 0.00001458
Iteration 32/1000 | Loss: 0.00001457
Iteration 33/1000 | Loss: 0.00001456
Iteration 34/1000 | Loss: 0.00001456
Iteration 35/1000 | Loss: 0.00001456
Iteration 36/1000 | Loss: 0.00001456
Iteration 37/1000 | Loss: 0.00001455
Iteration 38/1000 | Loss: 0.00001455
Iteration 39/1000 | Loss: 0.00001455
Iteration 40/1000 | Loss: 0.00001455
Iteration 41/1000 | Loss: 0.00001455
Iteration 42/1000 | Loss: 0.00001455
Iteration 43/1000 | Loss: 0.00001455
Iteration 44/1000 | Loss: 0.00001455
Iteration 45/1000 | Loss: 0.00001455
Iteration 46/1000 | Loss: 0.00001455
Iteration 47/1000 | Loss: 0.00001455
Iteration 48/1000 | Loss: 0.00001455
Iteration 49/1000 | Loss: 0.00001455
Iteration 50/1000 | Loss: 0.00001455
Iteration 51/1000 | Loss: 0.00001455
Iteration 52/1000 | Loss: 0.00001455
Iteration 53/1000 | Loss: 0.00001455
Iteration 54/1000 | Loss: 0.00001455
Iteration 55/1000 | Loss: 0.00001455
Iteration 56/1000 | Loss: 0.00001454
Iteration 57/1000 | Loss: 0.00001454
Iteration 58/1000 | Loss: 0.00001454
Iteration 59/1000 | Loss: 0.00001454
Iteration 60/1000 | Loss: 0.00001453
Iteration 61/1000 | Loss: 0.00001453
Iteration 62/1000 | Loss: 0.00001452
Iteration 63/1000 | Loss: 0.00001452
Iteration 64/1000 | Loss: 0.00001451
Iteration 65/1000 | Loss: 0.00001451
Iteration 66/1000 | Loss: 0.00001451
Iteration 67/1000 | Loss: 0.00001450
Iteration 68/1000 | Loss: 0.00001450
Iteration 69/1000 | Loss: 0.00001450
Iteration 70/1000 | Loss: 0.00001450
Iteration 71/1000 | Loss: 0.00001450
Iteration 72/1000 | Loss: 0.00001450
Iteration 73/1000 | Loss: 0.00001450
Iteration 74/1000 | Loss: 0.00001450
Iteration 75/1000 | Loss: 0.00001449
Iteration 76/1000 | Loss: 0.00001449
Iteration 77/1000 | Loss: 0.00001449
Iteration 78/1000 | Loss: 0.00001449
Iteration 79/1000 | Loss: 0.00001448
Iteration 80/1000 | Loss: 0.00001448
Iteration 81/1000 | Loss: 0.00001448
Iteration 82/1000 | Loss: 0.00001448
Iteration 83/1000 | Loss: 0.00001448
Iteration 84/1000 | Loss: 0.00001447
Iteration 85/1000 | Loss: 0.00001447
Iteration 86/1000 | Loss: 0.00001447
Iteration 87/1000 | Loss: 0.00001446
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001446
Iteration 90/1000 | Loss: 0.00001445
Iteration 91/1000 | Loss: 0.00001445
Iteration 92/1000 | Loss: 0.00001445
Iteration 93/1000 | Loss: 0.00001445
Iteration 94/1000 | Loss: 0.00001444
Iteration 95/1000 | Loss: 0.00001444
Iteration 96/1000 | Loss: 0.00001444
Iteration 97/1000 | Loss: 0.00001444
Iteration 98/1000 | Loss: 0.00001444
Iteration 99/1000 | Loss: 0.00001444
Iteration 100/1000 | Loss: 0.00001443
Iteration 101/1000 | Loss: 0.00001443
Iteration 102/1000 | Loss: 0.00001443
Iteration 103/1000 | Loss: 0.00001443
Iteration 104/1000 | Loss: 0.00001443
Iteration 105/1000 | Loss: 0.00001443
Iteration 106/1000 | Loss: 0.00001443
Iteration 107/1000 | Loss: 0.00001443
Iteration 108/1000 | Loss: 0.00001443
Iteration 109/1000 | Loss: 0.00001443
Iteration 110/1000 | Loss: 0.00001443
Iteration 111/1000 | Loss: 0.00001443
Iteration 112/1000 | Loss: 0.00001443
Iteration 113/1000 | Loss: 0.00001443
Iteration 114/1000 | Loss: 0.00001443
Iteration 115/1000 | Loss: 0.00001443
Iteration 116/1000 | Loss: 0.00001443
Iteration 117/1000 | Loss: 0.00001443
Iteration 118/1000 | Loss: 0.00001443
Iteration 119/1000 | Loss: 0.00001443
Iteration 120/1000 | Loss: 0.00001443
Iteration 121/1000 | Loss: 0.00001443
Iteration 122/1000 | Loss: 0.00001443
Iteration 123/1000 | Loss: 0.00001443
Iteration 124/1000 | Loss: 0.00001443
Iteration 125/1000 | Loss: 0.00001443
Iteration 126/1000 | Loss: 0.00001443
Iteration 127/1000 | Loss: 0.00001443
Iteration 128/1000 | Loss: 0.00001443
Iteration 129/1000 | Loss: 0.00001443
Iteration 130/1000 | Loss: 0.00001443
Iteration 131/1000 | Loss: 0.00001443
Iteration 132/1000 | Loss: 0.00001443
Iteration 133/1000 | Loss: 0.00001443
Iteration 134/1000 | Loss: 0.00001443
Iteration 135/1000 | Loss: 0.00001443
Iteration 136/1000 | Loss: 0.00001443
Iteration 137/1000 | Loss: 0.00001443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.4426368579734117e-05, 1.4426368579734117e-05, 1.4426368579734117e-05, 1.4426368579734117e-05, 1.4426368579734117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4426368579734117e-05

Optimization complete. Final v2v error: 3.267125129699707 mm

Highest mean error: 4.752133846282959 mm for frame 211

Lowest mean error: 3.072255849838257 mm for frame 108

Saving results

Total time: 96.67481732368469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_5811/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_5811/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072200
Iteration 2/25 | Loss: 0.01072200
Iteration 3/25 | Loss: 0.01072200
Iteration 4/25 | Loss: 0.00251586
Iteration 5/25 | Loss: 0.00170820
Iteration 6/25 | Loss: 0.00118050
Iteration 7/25 | Loss: 0.00113264
Iteration 8/25 | Loss: 0.00109185
Iteration 9/25 | Loss: 0.00107390
Iteration 10/25 | Loss: 0.00105748
Iteration 11/25 | Loss: 0.00104159
Iteration 12/25 | Loss: 0.00102713
Iteration 13/25 | Loss: 0.00101776
Iteration 14/25 | Loss: 0.00101416
Iteration 15/25 | Loss: 0.00100819
Iteration 16/25 | Loss: 0.00100358
Iteration 17/25 | Loss: 0.00100618
Iteration 18/25 | Loss: 0.00100303
Iteration 19/25 | Loss: 0.00099842
Iteration 20/25 | Loss: 0.00099577
Iteration 21/25 | Loss: 0.00099558
Iteration 22/25 | Loss: 0.00099558
Iteration 23/25 | Loss: 0.00099558
Iteration 24/25 | Loss: 0.00099558
Iteration 25/25 | Loss: 0.00099558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31376445
Iteration 2/25 | Loss: 0.00242771
Iteration 3/25 | Loss: 0.00167911
Iteration 4/25 | Loss: 0.00167907
Iteration 5/25 | Loss: 0.00167907
Iteration 6/25 | Loss: 0.00167907
Iteration 7/25 | Loss: 0.00167907
Iteration 8/25 | Loss: 0.00167907
Iteration 9/25 | Loss: 0.00167907
Iteration 10/25 | Loss: 0.00167907
Iteration 11/25 | Loss: 0.00167907
Iteration 12/25 | Loss: 0.00167907
Iteration 13/25 | Loss: 0.00167906
Iteration 14/25 | Loss: 0.00167906
Iteration 15/25 | Loss: 0.00167906
Iteration 16/25 | Loss: 0.00167906
Iteration 17/25 | Loss: 0.00167906
Iteration 18/25 | Loss: 0.00167906
Iteration 19/25 | Loss: 0.00167906
Iteration 20/25 | Loss: 0.00167906
Iteration 21/25 | Loss: 0.00167906
Iteration 22/25 | Loss: 0.00167906
Iteration 23/25 | Loss: 0.00167906
Iteration 24/25 | Loss: 0.00167906
Iteration 25/25 | Loss: 0.00167906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167906
Iteration 2/1000 | Loss: 0.00051042
Iteration 3/1000 | Loss: 0.00065959
Iteration 4/1000 | Loss: 0.00030936
Iteration 5/1000 | Loss: 0.00032340
Iteration 6/1000 | Loss: 0.00012257
Iteration 7/1000 | Loss: 0.00123530
Iteration 8/1000 | Loss: 0.00013264
Iteration 9/1000 | Loss: 0.00078367
Iteration 10/1000 | Loss: 0.00012390
Iteration 11/1000 | Loss: 0.00032487
Iteration 12/1000 | Loss: 0.00008851
Iteration 13/1000 | Loss: 0.00043949
Iteration 14/1000 | Loss: 0.00012860
Iteration 15/1000 | Loss: 0.00025633
Iteration 16/1000 | Loss: 0.00006286
Iteration 17/1000 | Loss: 0.00005974
Iteration 18/1000 | Loss: 0.00005697
Iteration 19/1000 | Loss: 0.00005486
Iteration 20/1000 | Loss: 0.00019469
Iteration 21/1000 | Loss: 0.00147323
Iteration 22/1000 | Loss: 0.00214856
Iteration 23/1000 | Loss: 0.00083253
Iteration 24/1000 | Loss: 0.00818231
Iteration 25/1000 | Loss: 0.00101078
Iteration 26/1000 | Loss: 0.00011575
Iteration 27/1000 | Loss: 0.00005698
Iteration 28/1000 | Loss: 0.00026200
Iteration 29/1000 | Loss: 0.00003320
Iteration 30/1000 | Loss: 0.00003949
Iteration 31/1000 | Loss: 0.00004939
Iteration 32/1000 | Loss: 0.00006964
Iteration 33/1000 | Loss: 0.00002002
Iteration 34/1000 | Loss: 0.00002685
Iteration 35/1000 | Loss: 0.00011015
Iteration 36/1000 | Loss: 0.00003232
Iteration 37/1000 | Loss: 0.00001422
Iteration 38/1000 | Loss: 0.00001356
Iteration 39/1000 | Loss: 0.00001290
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00006972
Iteration 42/1000 | Loss: 0.00001208
Iteration 43/1000 | Loss: 0.00003649
Iteration 44/1000 | Loss: 0.00007565
Iteration 45/1000 | Loss: 0.00001192
Iteration 46/1000 | Loss: 0.00001174
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001172
Iteration 49/1000 | Loss: 0.00001163
Iteration 50/1000 | Loss: 0.00003529
Iteration 51/1000 | Loss: 0.00001157
Iteration 52/1000 | Loss: 0.00001154
Iteration 53/1000 | Loss: 0.00001150
Iteration 54/1000 | Loss: 0.00001149
Iteration 55/1000 | Loss: 0.00001149
Iteration 56/1000 | Loss: 0.00001149
Iteration 57/1000 | Loss: 0.00001149
Iteration 58/1000 | Loss: 0.00001149
Iteration 59/1000 | Loss: 0.00001149
Iteration 60/1000 | Loss: 0.00001149
Iteration 61/1000 | Loss: 0.00001149
Iteration 62/1000 | Loss: 0.00001149
Iteration 63/1000 | Loss: 0.00001149
Iteration 64/1000 | Loss: 0.00001149
Iteration 65/1000 | Loss: 0.00001148
Iteration 66/1000 | Loss: 0.00001148
Iteration 67/1000 | Loss: 0.00001148
Iteration 68/1000 | Loss: 0.00001148
Iteration 69/1000 | Loss: 0.00001148
Iteration 70/1000 | Loss: 0.00001148
Iteration 71/1000 | Loss: 0.00001147
Iteration 72/1000 | Loss: 0.00001147
Iteration 73/1000 | Loss: 0.00001147
Iteration 74/1000 | Loss: 0.00001146
Iteration 75/1000 | Loss: 0.00001146
Iteration 76/1000 | Loss: 0.00001145
Iteration 77/1000 | Loss: 0.00001145
Iteration 78/1000 | Loss: 0.00001144
Iteration 79/1000 | Loss: 0.00001144
Iteration 80/1000 | Loss: 0.00001144
Iteration 81/1000 | Loss: 0.00001144
Iteration 82/1000 | Loss: 0.00001144
Iteration 83/1000 | Loss: 0.00001144
Iteration 84/1000 | Loss: 0.00001144
Iteration 85/1000 | Loss: 0.00001144
Iteration 86/1000 | Loss: 0.00001143
Iteration 87/1000 | Loss: 0.00001143
Iteration 88/1000 | Loss: 0.00001143
Iteration 89/1000 | Loss: 0.00001143
Iteration 90/1000 | Loss: 0.00001143
Iteration 91/1000 | Loss: 0.00001143
Iteration 92/1000 | Loss: 0.00001143
Iteration 93/1000 | Loss: 0.00001143
Iteration 94/1000 | Loss: 0.00001143
Iteration 95/1000 | Loss: 0.00001143
Iteration 96/1000 | Loss: 0.00001142
Iteration 97/1000 | Loss: 0.00001142
Iteration 98/1000 | Loss: 0.00001142
Iteration 99/1000 | Loss: 0.00001142
Iteration 100/1000 | Loss: 0.00001142
Iteration 101/1000 | Loss: 0.00001142
Iteration 102/1000 | Loss: 0.00001141
Iteration 103/1000 | Loss: 0.00001141
Iteration 104/1000 | Loss: 0.00001141
Iteration 105/1000 | Loss: 0.00001139
Iteration 106/1000 | Loss: 0.00001139
Iteration 107/1000 | Loss: 0.00001139
Iteration 108/1000 | Loss: 0.00001138
Iteration 109/1000 | Loss: 0.00001138
Iteration 110/1000 | Loss: 0.00001138
Iteration 111/1000 | Loss: 0.00001137
Iteration 112/1000 | Loss: 0.00001137
Iteration 113/1000 | Loss: 0.00001137
Iteration 114/1000 | Loss: 0.00004932
Iteration 115/1000 | Loss: 0.00001370
Iteration 116/1000 | Loss: 0.00001180
Iteration 117/1000 | Loss: 0.00001143
Iteration 118/1000 | Loss: 0.00001142
Iteration 119/1000 | Loss: 0.00001141
Iteration 120/1000 | Loss: 0.00001140
Iteration 121/1000 | Loss: 0.00001139
Iteration 122/1000 | Loss: 0.00001138
Iteration 123/1000 | Loss: 0.00001137
Iteration 124/1000 | Loss: 0.00001137
Iteration 125/1000 | Loss: 0.00001137
Iteration 126/1000 | Loss: 0.00001136
Iteration 127/1000 | Loss: 0.00001136
Iteration 128/1000 | Loss: 0.00001136
Iteration 129/1000 | Loss: 0.00001136
Iteration 130/1000 | Loss: 0.00001136
Iteration 131/1000 | Loss: 0.00001136
Iteration 132/1000 | Loss: 0.00001136
Iteration 133/1000 | Loss: 0.00001136
Iteration 134/1000 | Loss: 0.00001136
Iteration 135/1000 | Loss: 0.00001135
Iteration 136/1000 | Loss: 0.00001135
Iteration 137/1000 | Loss: 0.00001135
Iteration 138/1000 | Loss: 0.00001135
Iteration 139/1000 | Loss: 0.00001135
Iteration 140/1000 | Loss: 0.00001135
Iteration 141/1000 | Loss: 0.00001135
Iteration 142/1000 | Loss: 0.00001135
Iteration 143/1000 | Loss: 0.00001135
Iteration 144/1000 | Loss: 0.00001135
Iteration 145/1000 | Loss: 0.00001135
Iteration 146/1000 | Loss: 0.00001135
Iteration 147/1000 | Loss: 0.00001135
Iteration 148/1000 | Loss: 0.00001134
Iteration 149/1000 | Loss: 0.00001134
Iteration 150/1000 | Loss: 0.00001134
Iteration 151/1000 | Loss: 0.00001134
Iteration 152/1000 | Loss: 0.00001134
Iteration 153/1000 | Loss: 0.00001134
Iteration 154/1000 | Loss: 0.00003372
Iteration 155/1000 | Loss: 0.00003372
Iteration 156/1000 | Loss: 0.00001245
Iteration 157/1000 | Loss: 0.00001183
Iteration 158/1000 | Loss: 0.00001133
Iteration 159/1000 | Loss: 0.00001133
Iteration 160/1000 | Loss: 0.00001132
Iteration 161/1000 | Loss: 0.00001132
Iteration 162/1000 | Loss: 0.00001132
Iteration 163/1000 | Loss: 0.00001132
Iteration 164/1000 | Loss: 0.00001132
Iteration 165/1000 | Loss: 0.00002866
Iteration 166/1000 | Loss: 0.00001180
Iteration 167/1000 | Loss: 0.00001389
Iteration 168/1000 | Loss: 0.00001219
Iteration 169/1000 | Loss: 0.00002830
Iteration 170/1000 | Loss: 0.00001217
Iteration 171/1000 | Loss: 0.00001136
Iteration 172/1000 | Loss: 0.00001136
Iteration 173/1000 | Loss: 0.00001135
Iteration 174/1000 | Loss: 0.00001134
Iteration 175/1000 | Loss: 0.00001134
Iteration 176/1000 | Loss: 0.00001133
Iteration 177/1000 | Loss: 0.00001132
Iteration 178/1000 | Loss: 0.00001132
Iteration 179/1000 | Loss: 0.00001132
Iteration 180/1000 | Loss: 0.00001132
Iteration 181/1000 | Loss: 0.00001132
Iteration 182/1000 | Loss: 0.00001132
Iteration 183/1000 | Loss: 0.00001132
Iteration 184/1000 | Loss: 0.00001132
Iteration 185/1000 | Loss: 0.00001131
Iteration 186/1000 | Loss: 0.00001131
Iteration 187/1000 | Loss: 0.00001131
Iteration 188/1000 | Loss: 0.00001131
Iteration 189/1000 | Loss: 0.00001131
Iteration 190/1000 | Loss: 0.00001131
Iteration 191/1000 | Loss: 0.00001131
Iteration 192/1000 | Loss: 0.00001131
Iteration 193/1000 | Loss: 0.00001131
Iteration 194/1000 | Loss: 0.00001131
Iteration 195/1000 | Loss: 0.00001131
Iteration 196/1000 | Loss: 0.00001131
Iteration 197/1000 | Loss: 0.00001131
Iteration 198/1000 | Loss: 0.00001131
Iteration 199/1000 | Loss: 0.00001131
Iteration 200/1000 | Loss: 0.00001131
Iteration 201/1000 | Loss: 0.00001131
Iteration 202/1000 | Loss: 0.00001131
Iteration 203/1000 | Loss: 0.00001131
Iteration 204/1000 | Loss: 0.00001131
Iteration 205/1000 | Loss: 0.00001131
Iteration 206/1000 | Loss: 0.00001131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.1307470231258776e-05, 1.1307470231258776e-05, 1.1307470231258776e-05, 1.1307470231258776e-05, 1.1307470231258776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1307470231258776e-05

Optimization complete. Final v2v error: 2.8753788471221924 mm

Highest mean error: 4.214399337768555 mm for frame 222

Lowest mean error: 2.4012861251831055 mm for frame 13

Saving results

Total time: 144.81194186210632
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060430
Iteration 2/25 | Loss: 0.00479027
Iteration 3/25 | Loss: 0.00268155
Iteration 4/25 | Loss: 0.00236591
Iteration 5/25 | Loss: 0.00224339
Iteration 6/25 | Loss: 0.00219665
Iteration 7/25 | Loss: 0.00214279
Iteration 8/25 | Loss: 0.00211585
Iteration 9/25 | Loss: 0.00210921
Iteration 10/25 | Loss: 0.00210717
Iteration 11/25 | Loss: 0.00210598
Iteration 12/25 | Loss: 0.00210527
Iteration 13/25 | Loss: 0.00210493
Iteration 14/25 | Loss: 0.00210477
Iteration 15/25 | Loss: 0.00210477
Iteration 16/25 | Loss: 0.00210477
Iteration 17/25 | Loss: 0.00210477
Iteration 18/25 | Loss: 0.00210477
Iteration 19/25 | Loss: 0.00210477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002104773186147213, 0.002104773186147213, 0.002104773186147213, 0.002104773186147213, 0.002104773186147213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002104773186147213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24397326
Iteration 2/25 | Loss: 0.00401564
Iteration 3/25 | Loss: 0.00401563
Iteration 4/25 | Loss: 0.00401563
Iteration 5/25 | Loss: 0.00401563
Iteration 6/25 | Loss: 0.00401563
Iteration 7/25 | Loss: 0.00401563
Iteration 8/25 | Loss: 0.00401563
Iteration 9/25 | Loss: 0.00401563
Iteration 10/25 | Loss: 0.00401563
Iteration 11/25 | Loss: 0.00401563
Iteration 12/25 | Loss: 0.00401563
Iteration 13/25 | Loss: 0.00401563
Iteration 14/25 | Loss: 0.00401563
Iteration 15/25 | Loss: 0.00401563
Iteration 16/25 | Loss: 0.00401563
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004015632439404726, 0.004015632439404726, 0.004015632439404726, 0.004015632439404726, 0.004015632439404726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004015632439404726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00401563
Iteration 2/1000 | Loss: 0.00036099
Iteration 3/1000 | Loss: 0.00025803
Iteration 4/1000 | Loss: 0.00021156
Iteration 5/1000 | Loss: 0.00030668
Iteration 6/1000 | Loss: 0.00019291
Iteration 7/1000 | Loss: 0.00017694
Iteration 8/1000 | Loss: 0.00016828
Iteration 9/1000 | Loss: 0.00016164
Iteration 10/1000 | Loss: 0.00253273
Iteration 11/1000 | Loss: 0.01198462
Iteration 12/1000 | Loss: 0.00130088
Iteration 13/1000 | Loss: 0.00075500
Iteration 14/1000 | Loss: 0.00022793
Iteration 15/1000 | Loss: 0.00014416
Iteration 16/1000 | Loss: 0.00016205
Iteration 17/1000 | Loss: 0.00032182
Iteration 18/1000 | Loss: 0.00008444
Iteration 19/1000 | Loss: 0.00007284
Iteration 20/1000 | Loss: 0.00005825
Iteration 21/1000 | Loss: 0.00013083
Iteration 22/1000 | Loss: 0.00020477
Iteration 23/1000 | Loss: 0.00014853
Iteration 24/1000 | Loss: 0.00015124
Iteration 25/1000 | Loss: 0.00022457
Iteration 26/1000 | Loss: 0.00014340
Iteration 27/1000 | Loss: 0.00004714
Iteration 28/1000 | Loss: 0.00004285
Iteration 29/1000 | Loss: 0.00004012
Iteration 30/1000 | Loss: 0.00003865
Iteration 31/1000 | Loss: 0.00003726
Iteration 32/1000 | Loss: 0.00003594
Iteration 33/1000 | Loss: 0.00003518
Iteration 34/1000 | Loss: 0.00003459
Iteration 35/1000 | Loss: 0.00003407
Iteration 36/1000 | Loss: 0.00003378
Iteration 37/1000 | Loss: 0.00003352
Iteration 38/1000 | Loss: 0.00020675
Iteration 39/1000 | Loss: 0.00010098
Iteration 40/1000 | Loss: 0.00003515
Iteration 41/1000 | Loss: 0.00003344
Iteration 42/1000 | Loss: 0.00023026
Iteration 43/1000 | Loss: 0.00006651
Iteration 44/1000 | Loss: 0.00003330
Iteration 45/1000 | Loss: 0.00024009
Iteration 46/1000 | Loss: 0.00011424
Iteration 47/1000 | Loss: 0.00009321
Iteration 48/1000 | Loss: 0.00004654
Iteration 49/1000 | Loss: 0.00006105
Iteration 50/1000 | Loss: 0.00004481
Iteration 51/1000 | Loss: 0.00003742
Iteration 52/1000 | Loss: 0.00004677
Iteration 53/1000 | Loss: 0.00003732
Iteration 54/1000 | Loss: 0.00003511
Iteration 55/1000 | Loss: 0.00003361
Iteration 56/1000 | Loss: 0.00003199
Iteration 57/1000 | Loss: 0.00003154
Iteration 58/1000 | Loss: 0.00003118
Iteration 59/1000 | Loss: 0.00003082
Iteration 60/1000 | Loss: 0.00003053
Iteration 61/1000 | Loss: 0.00003033
Iteration 62/1000 | Loss: 0.00003033
Iteration 63/1000 | Loss: 0.00003029
Iteration 64/1000 | Loss: 0.00003029
Iteration 65/1000 | Loss: 0.00003028
Iteration 66/1000 | Loss: 0.00003028
Iteration 67/1000 | Loss: 0.00003027
Iteration 68/1000 | Loss: 0.00003027
Iteration 69/1000 | Loss: 0.00003025
Iteration 70/1000 | Loss: 0.00003025
Iteration 71/1000 | Loss: 0.00003024
Iteration 72/1000 | Loss: 0.00003024
Iteration 73/1000 | Loss: 0.00003024
Iteration 74/1000 | Loss: 0.00003024
Iteration 75/1000 | Loss: 0.00003024
Iteration 76/1000 | Loss: 0.00003024
Iteration 77/1000 | Loss: 0.00003023
Iteration 78/1000 | Loss: 0.00003023
Iteration 79/1000 | Loss: 0.00003023
Iteration 80/1000 | Loss: 0.00003023
Iteration 81/1000 | Loss: 0.00003022
Iteration 82/1000 | Loss: 0.00003022
Iteration 83/1000 | Loss: 0.00003021
Iteration 84/1000 | Loss: 0.00003021
Iteration 85/1000 | Loss: 0.00003020
Iteration 86/1000 | Loss: 0.00003020
Iteration 87/1000 | Loss: 0.00003020
Iteration 88/1000 | Loss: 0.00003019
Iteration 89/1000 | Loss: 0.00003019
Iteration 90/1000 | Loss: 0.00003017
Iteration 91/1000 | Loss: 0.00003017
Iteration 92/1000 | Loss: 0.00003017
Iteration 93/1000 | Loss: 0.00003017
Iteration 94/1000 | Loss: 0.00003016
Iteration 95/1000 | Loss: 0.00003016
Iteration 96/1000 | Loss: 0.00003016
Iteration 97/1000 | Loss: 0.00003016
Iteration 98/1000 | Loss: 0.00003016
Iteration 99/1000 | Loss: 0.00003016
Iteration 100/1000 | Loss: 0.00003016
Iteration 101/1000 | Loss: 0.00003016
Iteration 102/1000 | Loss: 0.00003016
Iteration 103/1000 | Loss: 0.00003016
Iteration 104/1000 | Loss: 0.00003016
Iteration 105/1000 | Loss: 0.00003016
Iteration 106/1000 | Loss: 0.00003016
Iteration 107/1000 | Loss: 0.00003016
Iteration 108/1000 | Loss: 0.00003015
Iteration 109/1000 | Loss: 0.00003015
Iteration 110/1000 | Loss: 0.00003015
Iteration 111/1000 | Loss: 0.00003015
Iteration 112/1000 | Loss: 0.00003015
Iteration 113/1000 | Loss: 0.00003015
Iteration 114/1000 | Loss: 0.00003015
Iteration 115/1000 | Loss: 0.00003015
Iteration 116/1000 | Loss: 0.00003015
Iteration 117/1000 | Loss: 0.00003014
Iteration 118/1000 | Loss: 0.00003014
Iteration 119/1000 | Loss: 0.00003014
Iteration 120/1000 | Loss: 0.00003014
Iteration 121/1000 | Loss: 0.00003014
Iteration 122/1000 | Loss: 0.00003014
Iteration 123/1000 | Loss: 0.00003013
Iteration 124/1000 | Loss: 0.00003013
Iteration 125/1000 | Loss: 0.00003013
Iteration 126/1000 | Loss: 0.00003013
Iteration 127/1000 | Loss: 0.00003013
Iteration 128/1000 | Loss: 0.00003013
Iteration 129/1000 | Loss: 0.00003013
Iteration 130/1000 | Loss: 0.00003013
Iteration 131/1000 | Loss: 0.00003013
Iteration 132/1000 | Loss: 0.00003013
Iteration 133/1000 | Loss: 0.00003013
Iteration 134/1000 | Loss: 0.00003013
Iteration 135/1000 | Loss: 0.00003013
Iteration 136/1000 | Loss: 0.00003013
Iteration 137/1000 | Loss: 0.00003013
Iteration 138/1000 | Loss: 0.00003013
Iteration 139/1000 | Loss: 0.00003013
Iteration 140/1000 | Loss: 0.00003013
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [3.0128188882372342e-05, 3.0128188882372342e-05, 3.0128188882372342e-05, 3.0128188882372342e-05, 3.0128188882372342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0128188882372342e-05

Optimization complete. Final v2v error: 4.733862400054932 mm

Highest mean error: 5.857706069946289 mm for frame 22

Lowest mean error: 4.350862979888916 mm for frame 115

Saving results

Total time: 114.61623167991638
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00721119
Iteration 2/25 | Loss: 0.00196474
Iteration 3/25 | Loss: 0.00190475
Iteration 4/25 | Loss: 0.00189717
Iteration 5/25 | Loss: 0.00189324
Iteration 6/25 | Loss: 0.00189223
Iteration 7/25 | Loss: 0.00189223
Iteration 8/25 | Loss: 0.00189223
Iteration 9/25 | Loss: 0.00189223
Iteration 10/25 | Loss: 0.00189223
Iteration 11/25 | Loss: 0.00189223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0018922332674264908, 0.0018922332674264908, 0.0018922332674264908, 0.0018922332674264908, 0.0018922332674264908]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018922332674264908

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26105738
Iteration 2/25 | Loss: 0.00261090
Iteration 3/25 | Loss: 0.00261090
Iteration 4/25 | Loss: 0.00261090
Iteration 5/25 | Loss: 0.00261090
Iteration 6/25 | Loss: 0.00261090
Iteration 7/25 | Loss: 0.00261090
Iteration 8/25 | Loss: 0.00261090
Iteration 9/25 | Loss: 0.00261090
Iteration 10/25 | Loss: 0.00261090
Iteration 11/25 | Loss: 0.00261090
Iteration 12/25 | Loss: 0.00261090
Iteration 13/25 | Loss: 0.00261090
Iteration 14/25 | Loss: 0.00261090
Iteration 15/25 | Loss: 0.00261090
Iteration 16/25 | Loss: 0.00261090
Iteration 17/25 | Loss: 0.00261090
Iteration 18/25 | Loss: 0.00261090
Iteration 19/25 | Loss: 0.00261090
Iteration 20/25 | Loss: 0.00261090
Iteration 21/25 | Loss: 0.00261090
Iteration 22/25 | Loss: 0.00261090
Iteration 23/25 | Loss: 0.00261090
Iteration 24/25 | Loss: 0.00261090
Iteration 25/25 | Loss: 0.00261090

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00261090
Iteration 2/1000 | Loss: 0.00006236
Iteration 3/1000 | Loss: 0.00004327
Iteration 4/1000 | Loss: 0.00003824
Iteration 5/1000 | Loss: 0.00003592
Iteration 6/1000 | Loss: 0.00003473
Iteration 7/1000 | Loss: 0.00003382
Iteration 8/1000 | Loss: 0.00003316
Iteration 9/1000 | Loss: 0.00003269
Iteration 10/1000 | Loss: 0.00003233
Iteration 11/1000 | Loss: 0.00003197
Iteration 12/1000 | Loss: 0.00003176
Iteration 13/1000 | Loss: 0.00003173
Iteration 14/1000 | Loss: 0.00003159
Iteration 15/1000 | Loss: 0.00003157
Iteration 16/1000 | Loss: 0.00003155
Iteration 17/1000 | Loss: 0.00003152
Iteration 18/1000 | Loss: 0.00003148
Iteration 19/1000 | Loss: 0.00003146
Iteration 20/1000 | Loss: 0.00003143
Iteration 21/1000 | Loss: 0.00003143
Iteration 22/1000 | Loss: 0.00003143
Iteration 23/1000 | Loss: 0.00003142
Iteration 24/1000 | Loss: 0.00003142
Iteration 25/1000 | Loss: 0.00003142
Iteration 26/1000 | Loss: 0.00003142
Iteration 27/1000 | Loss: 0.00003142
Iteration 28/1000 | Loss: 0.00003141
Iteration 29/1000 | Loss: 0.00003139
Iteration 30/1000 | Loss: 0.00003139
Iteration 31/1000 | Loss: 0.00003138
Iteration 32/1000 | Loss: 0.00003138
Iteration 33/1000 | Loss: 0.00003138
Iteration 34/1000 | Loss: 0.00003137
Iteration 35/1000 | Loss: 0.00003137
Iteration 36/1000 | Loss: 0.00003137
Iteration 37/1000 | Loss: 0.00003136
Iteration 38/1000 | Loss: 0.00003136
Iteration 39/1000 | Loss: 0.00003135
Iteration 40/1000 | Loss: 0.00003135
Iteration 41/1000 | Loss: 0.00003135
Iteration 42/1000 | Loss: 0.00003135
Iteration 43/1000 | Loss: 0.00003135
Iteration 44/1000 | Loss: 0.00003135
Iteration 45/1000 | Loss: 0.00003135
Iteration 46/1000 | Loss: 0.00003135
Iteration 47/1000 | Loss: 0.00003135
Iteration 48/1000 | Loss: 0.00003134
Iteration 49/1000 | Loss: 0.00003134
Iteration 50/1000 | Loss: 0.00003134
Iteration 51/1000 | Loss: 0.00003134
Iteration 52/1000 | Loss: 0.00003134
Iteration 53/1000 | Loss: 0.00003134
Iteration 54/1000 | Loss: 0.00003134
Iteration 55/1000 | Loss: 0.00003133
Iteration 56/1000 | Loss: 0.00003133
Iteration 57/1000 | Loss: 0.00003133
Iteration 58/1000 | Loss: 0.00003133
Iteration 59/1000 | Loss: 0.00003133
Iteration 60/1000 | Loss: 0.00003133
Iteration 61/1000 | Loss: 0.00003133
Iteration 62/1000 | Loss: 0.00003133
Iteration 63/1000 | Loss: 0.00003133
Iteration 64/1000 | Loss: 0.00003133
Iteration 65/1000 | Loss: 0.00003133
Iteration 66/1000 | Loss: 0.00003133
Iteration 67/1000 | Loss: 0.00003133
Iteration 68/1000 | Loss: 0.00003133
Iteration 69/1000 | Loss: 0.00003133
Iteration 70/1000 | Loss: 0.00003133
Iteration 71/1000 | Loss: 0.00003132
Iteration 72/1000 | Loss: 0.00003132
Iteration 73/1000 | Loss: 0.00003132
Iteration 74/1000 | Loss: 0.00003132
Iteration 75/1000 | Loss: 0.00003132
Iteration 76/1000 | Loss: 0.00003132
Iteration 77/1000 | Loss: 0.00003132
Iteration 78/1000 | Loss: 0.00003132
Iteration 79/1000 | Loss: 0.00003132
Iteration 80/1000 | Loss: 0.00003132
Iteration 81/1000 | Loss: 0.00003132
Iteration 82/1000 | Loss: 0.00003132
Iteration 83/1000 | Loss: 0.00003131
Iteration 84/1000 | Loss: 0.00003131
Iteration 85/1000 | Loss: 0.00003131
Iteration 86/1000 | Loss: 0.00003131
Iteration 87/1000 | Loss: 0.00003131
Iteration 88/1000 | Loss: 0.00003130
Iteration 89/1000 | Loss: 0.00003130
Iteration 90/1000 | Loss: 0.00003130
Iteration 91/1000 | Loss: 0.00003130
Iteration 92/1000 | Loss: 0.00003130
Iteration 93/1000 | Loss: 0.00003130
Iteration 94/1000 | Loss: 0.00003130
Iteration 95/1000 | Loss: 0.00003130
Iteration 96/1000 | Loss: 0.00003130
Iteration 97/1000 | Loss: 0.00003130
Iteration 98/1000 | Loss: 0.00003130
Iteration 99/1000 | Loss: 0.00003130
Iteration 100/1000 | Loss: 0.00003129
Iteration 101/1000 | Loss: 0.00003129
Iteration 102/1000 | Loss: 0.00003129
Iteration 103/1000 | Loss: 0.00003129
Iteration 104/1000 | Loss: 0.00003129
Iteration 105/1000 | Loss: 0.00003129
Iteration 106/1000 | Loss: 0.00003129
Iteration 107/1000 | Loss: 0.00003129
Iteration 108/1000 | Loss: 0.00003129
Iteration 109/1000 | Loss: 0.00003128
Iteration 110/1000 | Loss: 0.00003128
Iteration 111/1000 | Loss: 0.00003128
Iteration 112/1000 | Loss: 0.00003128
Iteration 113/1000 | Loss: 0.00003128
Iteration 114/1000 | Loss: 0.00003128
Iteration 115/1000 | Loss: 0.00003128
Iteration 116/1000 | Loss: 0.00003128
Iteration 117/1000 | Loss: 0.00003127
Iteration 118/1000 | Loss: 0.00003127
Iteration 119/1000 | Loss: 0.00003127
Iteration 120/1000 | Loss: 0.00003127
Iteration 121/1000 | Loss: 0.00003127
Iteration 122/1000 | Loss: 0.00003127
Iteration 123/1000 | Loss: 0.00003127
Iteration 124/1000 | Loss: 0.00003127
Iteration 125/1000 | Loss: 0.00003127
Iteration 126/1000 | Loss: 0.00003127
Iteration 127/1000 | Loss: 0.00003127
Iteration 128/1000 | Loss: 0.00003127
Iteration 129/1000 | Loss: 0.00003127
Iteration 130/1000 | Loss: 0.00003127
Iteration 131/1000 | Loss: 0.00003127
Iteration 132/1000 | Loss: 0.00003126
Iteration 133/1000 | Loss: 0.00003126
Iteration 134/1000 | Loss: 0.00003126
Iteration 135/1000 | Loss: 0.00003126
Iteration 136/1000 | Loss: 0.00003126
Iteration 137/1000 | Loss: 0.00003126
Iteration 138/1000 | Loss: 0.00003126
Iteration 139/1000 | Loss: 0.00003126
Iteration 140/1000 | Loss: 0.00003126
Iteration 141/1000 | Loss: 0.00003126
Iteration 142/1000 | Loss: 0.00003126
Iteration 143/1000 | Loss: 0.00003126
Iteration 144/1000 | Loss: 0.00003126
Iteration 145/1000 | Loss: 0.00003126
Iteration 146/1000 | Loss: 0.00003125
Iteration 147/1000 | Loss: 0.00003125
Iteration 148/1000 | Loss: 0.00003125
Iteration 149/1000 | Loss: 0.00003125
Iteration 150/1000 | Loss: 0.00003125
Iteration 151/1000 | Loss: 0.00003125
Iteration 152/1000 | Loss: 0.00003125
Iteration 153/1000 | Loss: 0.00003125
Iteration 154/1000 | Loss: 0.00003125
Iteration 155/1000 | Loss: 0.00003125
Iteration 156/1000 | Loss: 0.00003125
Iteration 157/1000 | Loss: 0.00003124
Iteration 158/1000 | Loss: 0.00003124
Iteration 159/1000 | Loss: 0.00003124
Iteration 160/1000 | Loss: 0.00003124
Iteration 161/1000 | Loss: 0.00003124
Iteration 162/1000 | Loss: 0.00003124
Iteration 163/1000 | Loss: 0.00003124
Iteration 164/1000 | Loss: 0.00003124
Iteration 165/1000 | Loss: 0.00003124
Iteration 166/1000 | Loss: 0.00003124
Iteration 167/1000 | Loss: 0.00003123
Iteration 168/1000 | Loss: 0.00003123
Iteration 169/1000 | Loss: 0.00003123
Iteration 170/1000 | Loss: 0.00003123
Iteration 171/1000 | Loss: 0.00003123
Iteration 172/1000 | Loss: 0.00003123
Iteration 173/1000 | Loss: 0.00003123
Iteration 174/1000 | Loss: 0.00003123
Iteration 175/1000 | Loss: 0.00003123
Iteration 176/1000 | Loss: 0.00003123
Iteration 177/1000 | Loss: 0.00003123
Iteration 178/1000 | Loss: 0.00003123
Iteration 179/1000 | Loss: 0.00003123
Iteration 180/1000 | Loss: 0.00003123
Iteration 181/1000 | Loss: 0.00003123
Iteration 182/1000 | Loss: 0.00003123
Iteration 183/1000 | Loss: 0.00003123
Iteration 184/1000 | Loss: 0.00003123
Iteration 185/1000 | Loss: 0.00003123
Iteration 186/1000 | Loss: 0.00003123
Iteration 187/1000 | Loss: 0.00003123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [3.123034548480064e-05, 3.123034548480064e-05, 3.123034548480064e-05, 3.123034548480064e-05, 3.123034548480064e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.123034548480064e-05

Optimization complete. Final v2v error: 4.853693962097168 mm

Highest mean error: 5.102683067321777 mm for frame 32

Lowest mean error: 4.601962566375732 mm for frame 67

Saving results

Total time: 37.380868673324585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01128260
Iteration 2/25 | Loss: 0.00327520
Iteration 3/25 | Loss: 0.00295662
Iteration 4/25 | Loss: 0.00280598
Iteration 5/25 | Loss: 0.00255387
Iteration 6/25 | Loss: 0.00230215
Iteration 7/25 | Loss: 0.00208188
Iteration 8/25 | Loss: 0.00203652
Iteration 9/25 | Loss: 0.00198392
Iteration 10/25 | Loss: 0.00196977
Iteration 11/25 | Loss: 0.00196693
Iteration 12/25 | Loss: 0.00196345
Iteration 13/25 | Loss: 0.00195360
Iteration 14/25 | Loss: 0.00195580
Iteration 15/25 | Loss: 0.00195251
Iteration 16/25 | Loss: 0.00195260
Iteration 17/25 | Loss: 0.00195151
Iteration 18/25 | Loss: 0.00195195
Iteration 19/25 | Loss: 0.00195114
Iteration 20/25 | Loss: 0.00195441
Iteration 21/25 | Loss: 0.00195270
Iteration 22/25 | Loss: 0.00195128
Iteration 23/25 | Loss: 0.00195228
Iteration 24/25 | Loss: 0.00195209
Iteration 25/25 | Loss: 0.00195186

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24840164
Iteration 2/25 | Loss: 0.00537989
Iteration 3/25 | Loss: 0.00389479
Iteration 4/25 | Loss: 0.00358753
Iteration 5/25 | Loss: 0.00358753
Iteration 6/25 | Loss: 0.00358753
Iteration 7/25 | Loss: 0.00358753
Iteration 8/25 | Loss: 0.00358753
Iteration 9/25 | Loss: 0.00358753
Iteration 10/25 | Loss: 0.00358753
Iteration 11/25 | Loss: 0.00358753
Iteration 12/25 | Loss: 0.00358753
Iteration 13/25 | Loss: 0.00358753
Iteration 14/25 | Loss: 0.00358753
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0035875258035957813, 0.0035875258035957813, 0.0035875258035957813, 0.0035875258035957813, 0.0035875258035957813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035875258035957813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00358753
Iteration 2/1000 | Loss: 0.00186826
Iteration 3/1000 | Loss: 0.00119089
Iteration 4/1000 | Loss: 0.00050237
Iteration 5/1000 | Loss: 0.00081431
Iteration 6/1000 | Loss: 0.00167419
Iteration 7/1000 | Loss: 0.00048572
Iteration 8/1000 | Loss: 0.00085452
Iteration 9/1000 | Loss: 0.00197929
Iteration 10/1000 | Loss: 0.00054129
Iteration 11/1000 | Loss: 0.00037010
Iteration 12/1000 | Loss: 0.00033096
Iteration 13/1000 | Loss: 0.00032943
Iteration 14/1000 | Loss: 0.00062424
Iteration 15/1000 | Loss: 0.00036087
Iteration 16/1000 | Loss: 0.00021861
Iteration 17/1000 | Loss: 0.00040132
Iteration 18/1000 | Loss: 0.00029475
Iteration 19/1000 | Loss: 0.00037983
Iteration 20/1000 | Loss: 0.00028090
Iteration 21/1000 | Loss: 0.00018338
Iteration 22/1000 | Loss: 0.00050959
Iteration 23/1000 | Loss: 0.00180247
Iteration 24/1000 | Loss: 0.00059874
Iteration 25/1000 | Loss: 0.00042318
Iteration 26/1000 | Loss: 0.00017598
Iteration 27/1000 | Loss: 0.00015312
Iteration 28/1000 | Loss: 0.00030202
Iteration 29/1000 | Loss: 0.00015360
Iteration 30/1000 | Loss: 0.00026110
Iteration 31/1000 | Loss: 0.00049350
Iteration 32/1000 | Loss: 0.00142285
Iteration 33/1000 | Loss: 0.00064214
Iteration 34/1000 | Loss: 0.00026900
Iteration 35/1000 | Loss: 0.00065615
Iteration 36/1000 | Loss: 0.00046698
Iteration 37/1000 | Loss: 0.00025710
Iteration 38/1000 | Loss: 0.00021140
Iteration 39/1000 | Loss: 0.00064489
Iteration 40/1000 | Loss: 0.00046792
Iteration 41/1000 | Loss: 0.00017897
Iteration 42/1000 | Loss: 0.00013999
Iteration 43/1000 | Loss: 0.00048415
Iteration 44/1000 | Loss: 0.00036616
Iteration 45/1000 | Loss: 0.00027534
Iteration 46/1000 | Loss: 0.00066157
Iteration 47/1000 | Loss: 0.00025253
Iteration 48/1000 | Loss: 0.00024609
Iteration 49/1000 | Loss: 0.00027724
Iteration 50/1000 | Loss: 0.00017655
Iteration 51/1000 | Loss: 0.00011391
Iteration 52/1000 | Loss: 0.00018161
Iteration 53/1000 | Loss: 0.00021264
Iteration 54/1000 | Loss: 0.00046758
Iteration 55/1000 | Loss: 0.00029641
Iteration 56/1000 | Loss: 0.00038857
Iteration 57/1000 | Loss: 0.00040754
Iteration 58/1000 | Loss: 0.00032136
Iteration 59/1000 | Loss: 0.00029931
Iteration 60/1000 | Loss: 0.00048207
Iteration 61/1000 | Loss: 0.00089843
Iteration 62/1000 | Loss: 0.00077016
Iteration 63/1000 | Loss: 0.00017625
Iteration 64/1000 | Loss: 0.00013420
Iteration 65/1000 | Loss: 0.00011285
Iteration 66/1000 | Loss: 0.00018683
Iteration 67/1000 | Loss: 0.00019770
Iteration 68/1000 | Loss: 0.00066472
Iteration 69/1000 | Loss: 0.00042323
Iteration 70/1000 | Loss: 0.00013029
Iteration 71/1000 | Loss: 0.00010801
Iteration 72/1000 | Loss: 0.00010531
Iteration 73/1000 | Loss: 0.00018640
Iteration 74/1000 | Loss: 0.00010439
Iteration 75/1000 | Loss: 0.00010656
Iteration 76/1000 | Loss: 0.00010022
Iteration 77/1000 | Loss: 0.00018607
Iteration 78/1000 | Loss: 0.00128578
Iteration 79/1000 | Loss: 0.00076584
Iteration 80/1000 | Loss: 0.00053626
Iteration 81/1000 | Loss: 0.00049301
Iteration 82/1000 | Loss: 0.00011263
Iteration 83/1000 | Loss: 0.00024048
Iteration 84/1000 | Loss: 0.00054855
Iteration 85/1000 | Loss: 0.00070193
Iteration 86/1000 | Loss: 0.00051534
Iteration 87/1000 | Loss: 0.00039192
Iteration 88/1000 | Loss: 0.00016116
Iteration 89/1000 | Loss: 0.00039856
Iteration 90/1000 | Loss: 0.00014632
Iteration 91/1000 | Loss: 0.00035187
Iteration 92/1000 | Loss: 0.00040853
Iteration 93/1000 | Loss: 0.00021785
Iteration 94/1000 | Loss: 0.00024187
Iteration 95/1000 | Loss: 0.00058747
Iteration 96/1000 | Loss: 0.00017932
Iteration 97/1000 | Loss: 0.00018012
Iteration 98/1000 | Loss: 0.00025631
Iteration 99/1000 | Loss: 0.00009873
Iteration 100/1000 | Loss: 0.00009627
Iteration 101/1000 | Loss: 0.00081504
Iteration 102/1000 | Loss: 0.00057571
Iteration 103/1000 | Loss: 0.00020773
Iteration 104/1000 | Loss: 0.00015164
Iteration 105/1000 | Loss: 0.00038282
Iteration 106/1000 | Loss: 0.00029673
Iteration 107/1000 | Loss: 0.00011190
Iteration 108/1000 | Loss: 0.00007536
Iteration 109/1000 | Loss: 0.00022308
Iteration 110/1000 | Loss: 0.00078182
Iteration 111/1000 | Loss: 0.00007413
Iteration 112/1000 | Loss: 0.00006800
Iteration 113/1000 | Loss: 0.00020479
Iteration 114/1000 | Loss: 0.00017959
Iteration 115/1000 | Loss: 0.00022061
Iteration 116/1000 | Loss: 0.00011733
Iteration 117/1000 | Loss: 0.00011775
Iteration 118/1000 | Loss: 0.00018523
Iteration 119/1000 | Loss: 0.00005920
Iteration 120/1000 | Loss: 0.00006375
Iteration 121/1000 | Loss: 0.00005061
Iteration 122/1000 | Loss: 0.00004874
Iteration 123/1000 | Loss: 0.00005775
Iteration 124/1000 | Loss: 0.00032022
Iteration 125/1000 | Loss: 0.00009042
Iteration 126/1000 | Loss: 0.00010488
Iteration 127/1000 | Loss: 0.00007010
Iteration 128/1000 | Loss: 0.00005544
Iteration 129/1000 | Loss: 0.00004980
Iteration 130/1000 | Loss: 0.00022538
Iteration 131/1000 | Loss: 0.00022636
Iteration 132/1000 | Loss: 0.00015654
Iteration 133/1000 | Loss: 0.00011417
Iteration 134/1000 | Loss: 0.00022578
Iteration 135/1000 | Loss: 0.00006438
Iteration 136/1000 | Loss: 0.00014472
Iteration 137/1000 | Loss: 0.00008551
Iteration 138/1000 | Loss: 0.00006269
Iteration 139/1000 | Loss: 0.00005588
Iteration 140/1000 | Loss: 0.00004632
Iteration 141/1000 | Loss: 0.00005545
Iteration 142/1000 | Loss: 0.00019361
Iteration 143/1000 | Loss: 0.00008197
Iteration 144/1000 | Loss: 0.00011244
Iteration 145/1000 | Loss: 0.00005335
Iteration 146/1000 | Loss: 0.00005021
Iteration 147/1000 | Loss: 0.00004749
Iteration 148/1000 | Loss: 0.00014790
Iteration 149/1000 | Loss: 0.00006016
Iteration 150/1000 | Loss: 0.00006146
Iteration 151/1000 | Loss: 0.00005637
Iteration 152/1000 | Loss: 0.00016415
Iteration 153/1000 | Loss: 0.00006570
Iteration 154/1000 | Loss: 0.00005470
Iteration 155/1000 | Loss: 0.00004420
Iteration 156/1000 | Loss: 0.00004085
Iteration 157/1000 | Loss: 0.00033922
Iteration 158/1000 | Loss: 0.00021269
Iteration 159/1000 | Loss: 0.00003922
Iteration 160/1000 | Loss: 0.00024961
Iteration 161/1000 | Loss: 0.00003875
Iteration 162/1000 | Loss: 0.00003789
Iteration 163/1000 | Loss: 0.00003747
Iteration 164/1000 | Loss: 0.00011696
Iteration 165/1000 | Loss: 0.00008955
Iteration 166/1000 | Loss: 0.00004833
Iteration 167/1000 | Loss: 0.00004138
Iteration 168/1000 | Loss: 0.00003759
Iteration 169/1000 | Loss: 0.00003637
Iteration 170/1000 | Loss: 0.00003573
Iteration 171/1000 | Loss: 0.00003543
Iteration 172/1000 | Loss: 0.00003516
Iteration 173/1000 | Loss: 0.00003508
Iteration 174/1000 | Loss: 0.00003507
Iteration 175/1000 | Loss: 0.00003496
Iteration 176/1000 | Loss: 0.00003496
Iteration 177/1000 | Loss: 0.00003495
Iteration 178/1000 | Loss: 0.00003495
Iteration 179/1000 | Loss: 0.00003494
Iteration 180/1000 | Loss: 0.00003494
Iteration 181/1000 | Loss: 0.00003494
Iteration 182/1000 | Loss: 0.00003494
Iteration 183/1000 | Loss: 0.00003494
Iteration 184/1000 | Loss: 0.00003493
Iteration 185/1000 | Loss: 0.00003493
Iteration 186/1000 | Loss: 0.00003493
Iteration 187/1000 | Loss: 0.00003493
Iteration 188/1000 | Loss: 0.00003493
Iteration 189/1000 | Loss: 0.00003492
Iteration 190/1000 | Loss: 0.00003492
Iteration 191/1000 | Loss: 0.00003492
Iteration 192/1000 | Loss: 0.00003492
Iteration 193/1000 | Loss: 0.00003492
Iteration 194/1000 | Loss: 0.00003492
Iteration 195/1000 | Loss: 0.00003492
Iteration 196/1000 | Loss: 0.00003491
Iteration 197/1000 | Loss: 0.00003491
Iteration 198/1000 | Loss: 0.00003491
Iteration 199/1000 | Loss: 0.00003491
Iteration 200/1000 | Loss: 0.00003491
Iteration 201/1000 | Loss: 0.00003491
Iteration 202/1000 | Loss: 0.00003491
Iteration 203/1000 | Loss: 0.00003491
Iteration 204/1000 | Loss: 0.00003491
Iteration 205/1000 | Loss: 0.00003491
Iteration 206/1000 | Loss: 0.00003491
Iteration 207/1000 | Loss: 0.00003491
Iteration 208/1000 | Loss: 0.00003491
Iteration 209/1000 | Loss: 0.00003491
Iteration 210/1000 | Loss: 0.00003491
Iteration 211/1000 | Loss: 0.00003491
Iteration 212/1000 | Loss: 0.00003491
Iteration 213/1000 | Loss: 0.00003491
Iteration 214/1000 | Loss: 0.00003491
Iteration 215/1000 | Loss: 0.00003491
Iteration 216/1000 | Loss: 0.00003491
Iteration 217/1000 | Loss: 0.00003491
Iteration 218/1000 | Loss: 0.00003491
Iteration 219/1000 | Loss: 0.00003491
Iteration 220/1000 | Loss: 0.00003491
Iteration 221/1000 | Loss: 0.00003491
Iteration 222/1000 | Loss: 0.00003491
Iteration 223/1000 | Loss: 0.00003491
Iteration 224/1000 | Loss: 0.00003491
Iteration 225/1000 | Loss: 0.00003491
Iteration 226/1000 | Loss: 0.00003491
Iteration 227/1000 | Loss: 0.00003491
Iteration 228/1000 | Loss: 0.00003491
Iteration 229/1000 | Loss: 0.00003491
Iteration 230/1000 | Loss: 0.00003491
Iteration 231/1000 | Loss: 0.00003491
Iteration 232/1000 | Loss: 0.00003491
Iteration 233/1000 | Loss: 0.00003491
Iteration 234/1000 | Loss: 0.00003491
Iteration 235/1000 | Loss: 0.00003491
Iteration 236/1000 | Loss: 0.00003491
Iteration 237/1000 | Loss: 0.00003491
Iteration 238/1000 | Loss: 0.00003491
Iteration 239/1000 | Loss: 0.00003491
Iteration 240/1000 | Loss: 0.00003491
Iteration 241/1000 | Loss: 0.00003491
Iteration 242/1000 | Loss: 0.00003491
Iteration 243/1000 | Loss: 0.00003491
Iteration 244/1000 | Loss: 0.00003491
Iteration 245/1000 | Loss: 0.00003491
Iteration 246/1000 | Loss: 0.00003491
Iteration 247/1000 | Loss: 0.00003491
Iteration 248/1000 | Loss: 0.00003491
Iteration 249/1000 | Loss: 0.00003491
Iteration 250/1000 | Loss: 0.00003491
Iteration 251/1000 | Loss: 0.00003491
Iteration 252/1000 | Loss: 0.00003491
Iteration 253/1000 | Loss: 0.00003491
Iteration 254/1000 | Loss: 0.00003491
Iteration 255/1000 | Loss: 0.00003491
Iteration 256/1000 | Loss: 0.00003491
Iteration 257/1000 | Loss: 0.00003491
Iteration 258/1000 | Loss: 0.00003491
Iteration 259/1000 | Loss: 0.00003491
Iteration 260/1000 | Loss: 0.00003491
Iteration 261/1000 | Loss: 0.00003491
Iteration 262/1000 | Loss: 0.00003491
Iteration 263/1000 | Loss: 0.00003491
Iteration 264/1000 | Loss: 0.00003491
Iteration 265/1000 | Loss: 0.00003491
Iteration 266/1000 | Loss: 0.00003491
Iteration 267/1000 | Loss: 0.00003491
Iteration 268/1000 | Loss: 0.00003491
Iteration 269/1000 | Loss: 0.00003491
Iteration 270/1000 | Loss: 0.00003491
Iteration 271/1000 | Loss: 0.00003491
Iteration 272/1000 | Loss: 0.00003491
Iteration 273/1000 | Loss: 0.00003491
Iteration 274/1000 | Loss: 0.00003491
Iteration 275/1000 | Loss: 0.00003491
Iteration 276/1000 | Loss: 0.00003491
Iteration 277/1000 | Loss: 0.00003491
Iteration 278/1000 | Loss: 0.00003491
Iteration 279/1000 | Loss: 0.00003491
Iteration 280/1000 | Loss: 0.00003491
Iteration 281/1000 | Loss: 0.00003491
Iteration 282/1000 | Loss: 0.00003491
Iteration 283/1000 | Loss: 0.00003491
Iteration 284/1000 | Loss: 0.00003491
Iteration 285/1000 | Loss: 0.00003491
Iteration 286/1000 | Loss: 0.00003491
Iteration 287/1000 | Loss: 0.00003491
Iteration 288/1000 | Loss: 0.00003491
Iteration 289/1000 | Loss: 0.00003491
Iteration 290/1000 | Loss: 0.00003491
Iteration 291/1000 | Loss: 0.00003491
Iteration 292/1000 | Loss: 0.00003491
Iteration 293/1000 | Loss: 0.00003491
Iteration 294/1000 | Loss: 0.00003491
Iteration 295/1000 | Loss: 0.00003491
Iteration 296/1000 | Loss: 0.00003491
Iteration 297/1000 | Loss: 0.00003491
Iteration 298/1000 | Loss: 0.00003491
Iteration 299/1000 | Loss: 0.00003491
Iteration 300/1000 | Loss: 0.00003491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [3.4909284295281395e-05, 3.4909284295281395e-05, 3.4909284295281395e-05, 3.4909284295281395e-05, 3.4909284295281395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4909284295281395e-05

Optimization complete. Final v2v error: 4.7387776374816895 mm

Highest mean error: 11.481611251831055 mm for frame 75

Lowest mean error: 4.224627494812012 mm for frame 106

Saving results

Total time: 316.153516292572
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839972
Iteration 2/25 | Loss: 0.00272733
Iteration 3/25 | Loss: 0.00224659
Iteration 4/25 | Loss: 0.00215646
Iteration 5/25 | Loss: 0.00208644
Iteration 6/25 | Loss: 0.00206168
Iteration 7/25 | Loss: 0.00205723
Iteration 8/25 | Loss: 0.00205390
Iteration 9/25 | Loss: 0.00205361
Iteration 10/25 | Loss: 0.00205167
Iteration 11/25 | Loss: 0.00205152
Iteration 12/25 | Loss: 0.00205152
Iteration 13/25 | Loss: 0.00205151
Iteration 14/25 | Loss: 0.00205151
Iteration 15/25 | Loss: 0.00205151
Iteration 16/25 | Loss: 0.00205151
Iteration 17/25 | Loss: 0.00205151
Iteration 18/25 | Loss: 0.00205404
Iteration 19/25 | Loss: 0.00205317
Iteration 20/25 | Loss: 0.00205238
Iteration 21/25 | Loss: 0.00205252
Iteration 22/25 | Loss: 0.00205538
Iteration 23/25 | Loss: 0.00205477
Iteration 24/25 | Loss: 0.00205402
Iteration 25/25 | Loss: 0.00205128

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20011556
Iteration 2/25 | Loss: 0.00294620
Iteration 3/25 | Loss: 0.00294618
Iteration 4/25 | Loss: 0.00294618
Iteration 5/25 | Loss: 0.00294618
Iteration 6/25 | Loss: 0.00294618
Iteration 7/25 | Loss: 0.00294618
Iteration 8/25 | Loss: 0.00294618
Iteration 9/25 | Loss: 0.00294618
Iteration 10/25 | Loss: 0.00294618
Iteration 11/25 | Loss: 0.00294618
Iteration 12/25 | Loss: 0.00294618
Iteration 13/25 | Loss: 0.00294618
Iteration 14/25 | Loss: 0.00294618
Iteration 15/25 | Loss: 0.00294618
Iteration 16/25 | Loss: 0.00294618
Iteration 17/25 | Loss: 0.00294618
Iteration 18/25 | Loss: 0.00294618
Iteration 19/25 | Loss: 0.00294618
Iteration 20/25 | Loss: 0.00294618
Iteration 21/25 | Loss: 0.00294618
Iteration 22/25 | Loss: 0.00294618
Iteration 23/25 | Loss: 0.00294618
Iteration 24/25 | Loss: 0.00294618
Iteration 25/25 | Loss: 0.00294618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00294618
Iteration 2/1000 | Loss: 0.00010947
Iteration 3/1000 | Loss: 0.00006665
Iteration 4/1000 | Loss: 0.00006685
Iteration 5/1000 | Loss: 0.00004896
Iteration 6/1000 | Loss: 0.00004745
Iteration 7/1000 | Loss: 0.00004904
Iteration 8/1000 | Loss: 0.00004575
Iteration 9/1000 | Loss: 0.00004479
Iteration 10/1000 | Loss: 0.00005918
Iteration 11/1000 | Loss: 0.00004337
Iteration 12/1000 | Loss: 0.00005142
Iteration 13/1000 | Loss: 0.00005145
Iteration 14/1000 | Loss: 0.00005465
Iteration 15/1000 | Loss: 0.00004833
Iteration 16/1000 | Loss: 0.00005450
Iteration 17/1000 | Loss: 0.00004960
Iteration 18/1000 | Loss: 0.00006007
Iteration 19/1000 | Loss: 0.00004933
Iteration 20/1000 | Loss: 0.00005924
Iteration 21/1000 | Loss: 0.00004911
Iteration 22/1000 | Loss: 0.00004939
Iteration 23/1000 | Loss: 0.00004983
Iteration 24/1000 | Loss: 0.00006072
Iteration 25/1000 | Loss: 0.00004628
Iteration 26/1000 | Loss: 0.00004621
Iteration 27/1000 | Loss: 0.00004564
Iteration 28/1000 | Loss: 0.00005485
Iteration 29/1000 | Loss: 0.00004367
Iteration 30/1000 | Loss: 0.00004002
Iteration 31/1000 | Loss: 0.00003847
Iteration 32/1000 | Loss: 0.00003786
Iteration 33/1000 | Loss: 0.00003761
Iteration 34/1000 | Loss: 0.00003742
Iteration 35/1000 | Loss: 0.00003736
Iteration 36/1000 | Loss: 0.00003727
Iteration 37/1000 | Loss: 0.00003726
Iteration 38/1000 | Loss: 0.00003726
Iteration 39/1000 | Loss: 0.00003726
Iteration 40/1000 | Loss: 0.00003726
Iteration 41/1000 | Loss: 0.00003726
Iteration 42/1000 | Loss: 0.00003726
Iteration 43/1000 | Loss: 0.00003726
Iteration 44/1000 | Loss: 0.00003726
Iteration 45/1000 | Loss: 0.00003726
Iteration 46/1000 | Loss: 0.00003726
Iteration 47/1000 | Loss: 0.00003725
Iteration 48/1000 | Loss: 0.00003724
Iteration 49/1000 | Loss: 0.00003724
Iteration 50/1000 | Loss: 0.00003724
Iteration 51/1000 | Loss: 0.00003724
Iteration 52/1000 | Loss: 0.00003724
Iteration 53/1000 | Loss: 0.00003724
Iteration 54/1000 | Loss: 0.00003724
Iteration 55/1000 | Loss: 0.00003723
Iteration 56/1000 | Loss: 0.00003723
Iteration 57/1000 | Loss: 0.00003723
Iteration 58/1000 | Loss: 0.00003722
Iteration 59/1000 | Loss: 0.00003721
Iteration 60/1000 | Loss: 0.00003721
Iteration 61/1000 | Loss: 0.00003721
Iteration 62/1000 | Loss: 0.00003721
Iteration 63/1000 | Loss: 0.00003721
Iteration 64/1000 | Loss: 0.00003721
Iteration 65/1000 | Loss: 0.00003720
Iteration 66/1000 | Loss: 0.00003720
Iteration 67/1000 | Loss: 0.00003720
Iteration 68/1000 | Loss: 0.00003720
Iteration 69/1000 | Loss: 0.00003720
Iteration 70/1000 | Loss: 0.00003719
Iteration 71/1000 | Loss: 0.00003719
Iteration 72/1000 | Loss: 0.00003719
Iteration 73/1000 | Loss: 0.00003719
Iteration 74/1000 | Loss: 0.00003719
Iteration 75/1000 | Loss: 0.00003719
Iteration 76/1000 | Loss: 0.00003719
Iteration 77/1000 | Loss: 0.00003719
Iteration 78/1000 | Loss: 0.00003719
Iteration 79/1000 | Loss: 0.00003718
Iteration 80/1000 | Loss: 0.00003718
Iteration 81/1000 | Loss: 0.00003718
Iteration 82/1000 | Loss: 0.00003718
Iteration 83/1000 | Loss: 0.00003718
Iteration 84/1000 | Loss: 0.00003718
Iteration 85/1000 | Loss: 0.00003718
Iteration 86/1000 | Loss: 0.00003718
Iteration 87/1000 | Loss: 0.00003718
Iteration 88/1000 | Loss: 0.00003717
Iteration 89/1000 | Loss: 0.00003717
Iteration 90/1000 | Loss: 0.00003717
Iteration 91/1000 | Loss: 0.00003717
Iteration 92/1000 | Loss: 0.00003717
Iteration 93/1000 | Loss: 0.00003717
Iteration 94/1000 | Loss: 0.00003717
Iteration 95/1000 | Loss: 0.00003717
Iteration 96/1000 | Loss: 0.00003716
Iteration 97/1000 | Loss: 0.00003716
Iteration 98/1000 | Loss: 0.00003716
Iteration 99/1000 | Loss: 0.00003715
Iteration 100/1000 | Loss: 0.00003715
Iteration 101/1000 | Loss: 0.00003715
Iteration 102/1000 | Loss: 0.00003715
Iteration 103/1000 | Loss: 0.00003715
Iteration 104/1000 | Loss: 0.00003715
Iteration 105/1000 | Loss: 0.00003714
Iteration 106/1000 | Loss: 0.00003714
Iteration 107/1000 | Loss: 0.00003714
Iteration 108/1000 | Loss: 0.00003714
Iteration 109/1000 | Loss: 0.00003714
Iteration 110/1000 | Loss: 0.00003714
Iteration 111/1000 | Loss: 0.00003714
Iteration 112/1000 | Loss: 0.00003714
Iteration 113/1000 | Loss: 0.00003713
Iteration 114/1000 | Loss: 0.00003713
Iteration 115/1000 | Loss: 0.00003713
Iteration 116/1000 | Loss: 0.00003713
Iteration 117/1000 | Loss: 0.00003713
Iteration 118/1000 | Loss: 0.00003713
Iteration 119/1000 | Loss: 0.00003713
Iteration 120/1000 | Loss: 0.00003713
Iteration 121/1000 | Loss: 0.00003713
Iteration 122/1000 | Loss: 0.00003713
Iteration 123/1000 | Loss: 0.00003713
Iteration 124/1000 | Loss: 0.00003713
Iteration 125/1000 | Loss: 0.00003713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [3.713352998602204e-05, 3.713352998602204e-05, 3.713352998602204e-05, 3.713352998602204e-05, 3.713352998602204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.713352998602204e-05

Optimization complete. Final v2v error: 5.345525741577148 mm

Highest mean error: 6.007495403289795 mm for frame 232

Lowest mean error: 4.975210666656494 mm for frame 106

Saving results

Total time: 100.06508803367615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031268
Iteration 2/25 | Loss: 0.01031268
Iteration 3/25 | Loss: 0.00305352
Iteration 4/25 | Loss: 0.00217821
Iteration 5/25 | Loss: 0.00193311
Iteration 6/25 | Loss: 0.00194405
Iteration 7/25 | Loss: 0.00183738
Iteration 8/25 | Loss: 0.00167017
Iteration 9/25 | Loss: 0.00156876
Iteration 10/25 | Loss: 0.00150852
Iteration 11/25 | Loss: 0.00146725
Iteration 12/25 | Loss: 0.00144879
Iteration 13/25 | Loss: 0.00143916
Iteration 14/25 | Loss: 0.00143158
Iteration 15/25 | Loss: 0.00142487
Iteration 16/25 | Loss: 0.00141782
Iteration 17/25 | Loss: 0.00140773
Iteration 18/25 | Loss: 0.00140780
Iteration 19/25 | Loss: 0.00140151
Iteration 20/25 | Loss: 0.00139763
Iteration 21/25 | Loss: 0.00139398
Iteration 22/25 | Loss: 0.00138634
Iteration 23/25 | Loss: 0.00138313
Iteration 24/25 | Loss: 0.00138604
Iteration 25/25 | Loss: 0.00138522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18423045
Iteration 2/25 | Loss: 0.00261349
Iteration 3/25 | Loss: 0.00256106
Iteration 4/25 | Loss: 0.00256106
Iteration 5/25 | Loss: 0.00256106
Iteration 6/25 | Loss: 0.00256106
Iteration 7/25 | Loss: 0.00256106
Iteration 8/25 | Loss: 0.00256106
Iteration 9/25 | Loss: 0.00256106
Iteration 10/25 | Loss: 0.00256106
Iteration 11/25 | Loss: 0.00256106
Iteration 12/25 | Loss: 0.00256106
Iteration 13/25 | Loss: 0.00256106
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.002561060478910804, 0.002561060478910804, 0.002561060478910804, 0.002561060478910804, 0.002561060478910804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002561060478910804

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00256106
Iteration 2/1000 | Loss: 0.00030372
Iteration 3/1000 | Loss: 0.00022381
Iteration 4/1000 | Loss: 0.00049359
Iteration 5/1000 | Loss: 0.00017267
Iteration 6/1000 | Loss: 0.00021601
Iteration 7/1000 | Loss: 0.00024531
Iteration 8/1000 | Loss: 0.00027726
Iteration 9/1000 | Loss: 0.00026671
Iteration 10/1000 | Loss: 0.00040858
Iteration 11/1000 | Loss: 0.00019963
Iteration 12/1000 | Loss: 0.00018277
Iteration 13/1000 | Loss: 0.00028663
Iteration 14/1000 | Loss: 0.00009252
Iteration 15/1000 | Loss: 0.00020168
Iteration 16/1000 | Loss: 0.00022206
Iteration 17/1000 | Loss: 0.00010786
Iteration 18/1000 | Loss: 0.00009901
Iteration 19/1000 | Loss: 0.00023602
Iteration 20/1000 | Loss: 0.00050641
Iteration 21/1000 | Loss: 0.00017953
Iteration 22/1000 | Loss: 0.00009479
Iteration 23/1000 | Loss: 0.00009719
Iteration 24/1000 | Loss: 0.00010304
Iteration 25/1000 | Loss: 0.00025641
Iteration 26/1000 | Loss: 0.00010431
Iteration 27/1000 | Loss: 0.00008716
Iteration 28/1000 | Loss: 0.00008743
Iteration 29/1000 | Loss: 0.00009486
Iteration 30/1000 | Loss: 0.00008280
Iteration 31/1000 | Loss: 0.00008861
Iteration 32/1000 | Loss: 0.00008391
Iteration 33/1000 | Loss: 0.00008529
Iteration 34/1000 | Loss: 0.00009602
Iteration 35/1000 | Loss: 0.00007589
Iteration 36/1000 | Loss: 0.00010629
Iteration 37/1000 | Loss: 0.00019071
Iteration 38/1000 | Loss: 0.00012257
Iteration 39/1000 | Loss: 0.00007438
Iteration 40/1000 | Loss: 0.00006240
Iteration 41/1000 | Loss: 0.00007925
Iteration 42/1000 | Loss: 0.00007695
Iteration 43/1000 | Loss: 0.00006521
Iteration 44/1000 | Loss: 0.00031157
Iteration 45/1000 | Loss: 0.00140882
Iteration 46/1000 | Loss: 0.00008412
Iteration 47/1000 | Loss: 0.00028267
Iteration 48/1000 | Loss: 0.00007080
Iteration 49/1000 | Loss: 0.00013226
Iteration 50/1000 | Loss: 0.00005682
Iteration 51/1000 | Loss: 0.00016932
Iteration 52/1000 | Loss: 0.00012396
Iteration 53/1000 | Loss: 0.00004458
Iteration 54/1000 | Loss: 0.00017656
Iteration 55/1000 | Loss: 0.00008157
Iteration 56/1000 | Loss: 0.00008702
Iteration 57/1000 | Loss: 0.00008185
Iteration 58/1000 | Loss: 0.00008705
Iteration 59/1000 | Loss: 0.00015163
Iteration 60/1000 | Loss: 0.00020275
Iteration 61/1000 | Loss: 0.00013367
Iteration 62/1000 | Loss: 0.00017077
Iteration 63/1000 | Loss: 0.00007022
Iteration 64/1000 | Loss: 0.00004592
Iteration 65/1000 | Loss: 0.00004474
Iteration 66/1000 | Loss: 0.00010616
Iteration 67/1000 | Loss: 0.00008195
Iteration 68/1000 | Loss: 0.00004306
Iteration 69/1000 | Loss: 0.00018899
Iteration 70/1000 | Loss: 0.00012040
Iteration 71/1000 | Loss: 0.00004122
Iteration 72/1000 | Loss: 0.00003887
Iteration 73/1000 | Loss: 0.00003847
Iteration 74/1000 | Loss: 0.00003760
Iteration 75/1000 | Loss: 0.00003613
Iteration 76/1000 | Loss: 0.00003578
Iteration 77/1000 | Loss: 0.00003483
Iteration 78/1000 | Loss: 0.00003412
Iteration 79/1000 | Loss: 0.00003471
Iteration 80/1000 | Loss: 0.00003420
Iteration 81/1000 | Loss: 0.00003378
Iteration 82/1000 | Loss: 0.00003375
Iteration 83/1000 | Loss: 0.00003442
Iteration 84/1000 | Loss: 0.00003321
Iteration 85/1000 | Loss: 0.00014920
Iteration 86/1000 | Loss: 0.00006449
Iteration 87/1000 | Loss: 0.00004576
Iteration 88/1000 | Loss: 0.00003831
Iteration 89/1000 | Loss: 0.00005007
Iteration 90/1000 | Loss: 0.00004086
Iteration 91/1000 | Loss: 0.00003946
Iteration 92/1000 | Loss: 0.00003596
Iteration 93/1000 | Loss: 0.00003483
Iteration 94/1000 | Loss: 0.00003465
Iteration 95/1000 | Loss: 0.00003395
Iteration 96/1000 | Loss: 0.00003715
Iteration 97/1000 | Loss: 0.00003530
Iteration 98/1000 | Loss: 0.00003666
Iteration 99/1000 | Loss: 0.00003486
Iteration 100/1000 | Loss: 0.00003426
Iteration 101/1000 | Loss: 0.00003403
Iteration 102/1000 | Loss: 0.00003325
Iteration 103/1000 | Loss: 0.00003470
Iteration 104/1000 | Loss: 0.00003337
Iteration 105/1000 | Loss: 0.00003317
Iteration 106/1000 | Loss: 0.00003304
Iteration 107/1000 | Loss: 0.00003313
Iteration 108/1000 | Loss: 0.00003294
Iteration 109/1000 | Loss: 0.00003370
Iteration 110/1000 | Loss: 0.00003319
Iteration 111/1000 | Loss: 0.00003329
Iteration 112/1000 | Loss: 0.00003375
Iteration 113/1000 | Loss: 0.00003325
Iteration 114/1000 | Loss: 0.00003362
Iteration 115/1000 | Loss: 0.00003275
Iteration 116/1000 | Loss: 0.00003308
Iteration 117/1000 | Loss: 0.00003290
Iteration 118/1000 | Loss: 0.00003294
Iteration 119/1000 | Loss: 0.00003403
Iteration 120/1000 | Loss: 0.00003604
Iteration 121/1000 | Loss: 0.00012799
Iteration 122/1000 | Loss: 0.00004023
Iteration 123/1000 | Loss: 0.00003621
Iteration 124/1000 | Loss: 0.00003476
Iteration 125/1000 | Loss: 0.00003606
Iteration 126/1000 | Loss: 0.00003387
Iteration 127/1000 | Loss: 0.00003499
Iteration 128/1000 | Loss: 0.00003530
Iteration 129/1000 | Loss: 0.00003331
Iteration 130/1000 | Loss: 0.00003346
Iteration 131/1000 | Loss: 0.00003293
Iteration 132/1000 | Loss: 0.00003353
Iteration 133/1000 | Loss: 0.00003290
Iteration 134/1000 | Loss: 0.00003273
Iteration 135/1000 | Loss: 0.00003425
Iteration 136/1000 | Loss: 0.00003349
Iteration 137/1000 | Loss: 0.00003337
Iteration 138/1000 | Loss: 0.00003322
Iteration 139/1000 | Loss: 0.00003278
Iteration 140/1000 | Loss: 0.00003596
Iteration 141/1000 | Loss: 0.00003420
Iteration 142/1000 | Loss: 0.00003408
Iteration 143/1000 | Loss: 0.00003360
Iteration 144/1000 | Loss: 0.00003430
Iteration 145/1000 | Loss: 0.00003289
Iteration 146/1000 | Loss: 0.00003424
Iteration 147/1000 | Loss: 0.00003329
Iteration 148/1000 | Loss: 0.00003263
Iteration 149/1000 | Loss: 0.00003441
Iteration 150/1000 | Loss: 0.00003452
Iteration 151/1000 | Loss: 0.00003650
Iteration 152/1000 | Loss: 0.00003550
Iteration 153/1000 | Loss: 0.00013347
Iteration 154/1000 | Loss: 0.00004643
Iteration 155/1000 | Loss: 0.00006153
Iteration 156/1000 | Loss: 0.00003266
Iteration 157/1000 | Loss: 0.00012400
Iteration 158/1000 | Loss: 0.00016911
Iteration 159/1000 | Loss: 0.00017193
Iteration 160/1000 | Loss: 0.00006735
Iteration 161/1000 | Loss: 0.00004986
Iteration 162/1000 | Loss: 0.00004171
Iteration 163/1000 | Loss: 0.00003974
Iteration 164/1000 | Loss: 0.00003991
Iteration 165/1000 | Loss: 0.00012635
Iteration 166/1000 | Loss: 0.00005697
Iteration 167/1000 | Loss: 0.00004210
Iteration 168/1000 | Loss: 0.00014010
Iteration 169/1000 | Loss: 0.00020760
Iteration 170/1000 | Loss: 0.00006682
Iteration 171/1000 | Loss: 0.00004080
Iteration 172/1000 | Loss: 0.00003779
Iteration 173/1000 | Loss: 0.00003675
Iteration 174/1000 | Loss: 0.00003684
Iteration 175/1000 | Loss: 0.00003697
Iteration 176/1000 | Loss: 0.00003591
Iteration 177/1000 | Loss: 0.00003585
Iteration 178/1000 | Loss: 0.00003598
Iteration 179/1000 | Loss: 0.00003715
Iteration 180/1000 | Loss: 0.00003548
Iteration 181/1000 | Loss: 0.00003625
Iteration 182/1000 | Loss: 0.00003585
Iteration 183/1000 | Loss: 0.00003958
Iteration 184/1000 | Loss: 0.00003664
Iteration 185/1000 | Loss: 0.00003913
Iteration 186/1000 | Loss: 0.00003907
Iteration 187/1000 | Loss: 0.00018333
Iteration 188/1000 | Loss: 0.00014148
Iteration 189/1000 | Loss: 0.00017323
Iteration 190/1000 | Loss: 0.00004211
Iteration 191/1000 | Loss: 0.00003874
Iteration 192/1000 | Loss: 0.00003756
Iteration 193/1000 | Loss: 0.00003751
Iteration 194/1000 | Loss: 0.00003615
Iteration 195/1000 | Loss: 0.00003825
Iteration 196/1000 | Loss: 0.00003919
Iteration 197/1000 | Loss: 0.00004076
Iteration 198/1000 | Loss: 0.00004370
Iteration 199/1000 | Loss: 0.00004162
Iteration 200/1000 | Loss: 0.00004180
Iteration 201/1000 | Loss: 0.00004078
Iteration 202/1000 | Loss: 0.00004013
Iteration 203/1000 | Loss: 0.00003921
Iteration 204/1000 | Loss: 0.00003958
Iteration 205/1000 | Loss: 0.00004132
Iteration 206/1000 | Loss: 0.00004191
Iteration 207/1000 | Loss: 0.00003980
Iteration 208/1000 | Loss: 0.00003771
Iteration 209/1000 | Loss: 0.00003807
Iteration 210/1000 | Loss: 0.00004019
Iteration 211/1000 | Loss: 0.00003962
Iteration 212/1000 | Loss: 0.00013049
Iteration 213/1000 | Loss: 0.00008282
Iteration 214/1000 | Loss: 0.00011575
Iteration 215/1000 | Loss: 0.00007659
Iteration 216/1000 | Loss: 0.00024971
Iteration 217/1000 | Loss: 0.00008507
Iteration 218/1000 | Loss: 0.00014129
Iteration 219/1000 | Loss: 0.00010028
Iteration 220/1000 | Loss: 0.00012572
Iteration 221/1000 | Loss: 0.00010074
Iteration 222/1000 | Loss: 0.00010497
Iteration 223/1000 | Loss: 0.00007507
Iteration 224/1000 | Loss: 0.00003664
Iteration 225/1000 | Loss: 0.00011012
Iteration 226/1000 | Loss: 0.00007956
Iteration 227/1000 | Loss: 0.00009470
Iteration 228/1000 | Loss: 0.00007440
Iteration 229/1000 | Loss: 0.00009240
Iteration 230/1000 | Loss: 0.00007461
Iteration 231/1000 | Loss: 0.00009315
Iteration 232/1000 | Loss: 0.00007064
Iteration 233/1000 | Loss: 0.00008625
Iteration 234/1000 | Loss: 0.00006866
Iteration 235/1000 | Loss: 0.00008671
Iteration 236/1000 | Loss: 0.00006725
Iteration 237/1000 | Loss: 0.00008303
Iteration 238/1000 | Loss: 0.00006606
Iteration 239/1000 | Loss: 0.00008484
Iteration 240/1000 | Loss: 0.00010392
Iteration 241/1000 | Loss: 0.00005672
Iteration 242/1000 | Loss: 0.00004107
Iteration 243/1000 | Loss: 0.00003701
Iteration 244/1000 | Loss: 0.00004859
Iteration 245/1000 | Loss: 0.00004451
Iteration 246/1000 | Loss: 0.00005202
Iteration 247/1000 | Loss: 0.00004014
Iteration 248/1000 | Loss: 0.00004087
Iteration 249/1000 | Loss: 0.00004268
Iteration 250/1000 | Loss: 0.00004130
Iteration 251/1000 | Loss: 0.00003746
Iteration 252/1000 | Loss: 0.00005002
Iteration 253/1000 | Loss: 0.00004811
Iteration 254/1000 | Loss: 0.00004225
Iteration 255/1000 | Loss: 0.00004147
Iteration 256/1000 | Loss: 0.00003773
Iteration 257/1000 | Loss: 0.00003645
Iteration 258/1000 | Loss: 0.00003600
Iteration 259/1000 | Loss: 0.00003546
Iteration 260/1000 | Loss: 0.00003445
Iteration 261/1000 | Loss: 0.00003478
Iteration 262/1000 | Loss: 0.00003456
Iteration 263/1000 | Loss: 0.00003570
Iteration 264/1000 | Loss: 0.00003475
Iteration 265/1000 | Loss: 0.00003542
Iteration 266/1000 | Loss: 0.00003562
Iteration 267/1000 | Loss: 0.00003726
Iteration 268/1000 | Loss: 0.00003633
Iteration 269/1000 | Loss: 0.00042505
Iteration 270/1000 | Loss: 0.00011541
Iteration 271/1000 | Loss: 0.00004896
Iteration 272/1000 | Loss: 0.00004047
Iteration 273/1000 | Loss: 0.00003808
Iteration 274/1000 | Loss: 0.00003465
Iteration 275/1000 | Loss: 0.00003418
Iteration 276/1000 | Loss: 0.00003500
Iteration 277/1000 | Loss: 0.00003660
Iteration 278/1000 | Loss: 0.00003750
Iteration 279/1000 | Loss: 0.00003783
Iteration 280/1000 | Loss: 0.00003658
Iteration 281/1000 | Loss: 0.00003984
Iteration 282/1000 | Loss: 0.00003680
Iteration 283/1000 | Loss: 0.00003987
Iteration 284/1000 | Loss: 0.00003637
Iteration 285/1000 | Loss: 0.00003481
Iteration 286/1000 | Loss: 0.00003362
Iteration 287/1000 | Loss: 0.00003422
Iteration 288/1000 | Loss: 0.00003782
Iteration 289/1000 | Loss: 0.00003620
Iteration 290/1000 | Loss: 0.00003860
Iteration 291/1000 | Loss: 0.00003756
Iteration 292/1000 | Loss: 0.00004596
Iteration 293/1000 | Loss: 0.00004046
Iteration 294/1000 | Loss: 0.00004905
Iteration 295/1000 | Loss: 0.00005742
Iteration 296/1000 | Loss: 0.00004772
Iteration 297/1000 | Loss: 0.00004325
Iteration 298/1000 | Loss: 0.00003464
Iteration 299/1000 | Loss: 0.00003411
Iteration 300/1000 | Loss: 0.00003478
Iteration 301/1000 | Loss: 0.00003505
Iteration 302/1000 | Loss: 0.00003754
Iteration 303/1000 | Loss: 0.00003988
Iteration 304/1000 | Loss: 0.00004217
Iteration 305/1000 | Loss: 0.00004158
Iteration 306/1000 | Loss: 0.00004393
Iteration 307/1000 | Loss: 0.00004540
Iteration 308/1000 | Loss: 0.00004036
Iteration 309/1000 | Loss: 0.00004314
Iteration 310/1000 | Loss: 0.00004658
Iteration 311/1000 | Loss: 0.00004559
Iteration 312/1000 | Loss: 0.00004504
Iteration 313/1000 | Loss: 0.00004456
Iteration 314/1000 | Loss: 0.00004127
Iteration 315/1000 | Loss: 0.00004106
Iteration 316/1000 | Loss: 0.00004857
Iteration 317/1000 | Loss: 0.00004274
Iteration 318/1000 | Loss: 0.00004443
Iteration 319/1000 | Loss: 0.00004509
Iteration 320/1000 | Loss: 0.00004650
Iteration 321/1000 | Loss: 0.00004613
Iteration 322/1000 | Loss: 0.00004226
Iteration 323/1000 | Loss: 0.00004626
Iteration 324/1000 | Loss: 0.00004710
Iteration 325/1000 | Loss: 0.00005493
Iteration 326/1000 | Loss: 0.00004474
Iteration 327/1000 | Loss: 0.00015829
Iteration 328/1000 | Loss: 0.00013557
Iteration 329/1000 | Loss: 0.00004545
Iteration 330/1000 | Loss: 0.00004718
Iteration 331/1000 | Loss: 0.00004041
Iteration 332/1000 | Loss: 0.00004282
Iteration 333/1000 | Loss: 0.00004638
Iteration 334/1000 | Loss: 0.00004424
Iteration 335/1000 | Loss: 0.00004663
Iteration 336/1000 | Loss: 0.00004287
Iteration 337/1000 | Loss: 0.00004675
Iteration 338/1000 | Loss: 0.00004378
Iteration 339/1000 | Loss: 0.00003622
Iteration 340/1000 | Loss: 0.00003456
Iteration 341/1000 | Loss: 0.00004065
Iteration 342/1000 | Loss: 0.00003962
Iteration 343/1000 | Loss: 0.00004734
Iteration 344/1000 | Loss: 0.00004591
Iteration 345/1000 | Loss: 0.00004520
Iteration 346/1000 | Loss: 0.00014928
Iteration 347/1000 | Loss: 0.00006056
Iteration 348/1000 | Loss: 0.00004578
Iteration 349/1000 | Loss: 0.00004002
Iteration 350/1000 | Loss: 0.00004169
Iteration 351/1000 | Loss: 0.00003980
Iteration 352/1000 | Loss: 0.00004152
Iteration 353/1000 | Loss: 0.00003751
Iteration 354/1000 | Loss: 0.00004036
Iteration 355/1000 | Loss: 0.00003620
Iteration 356/1000 | Loss: 0.00003475
Iteration 357/1000 | Loss: 0.00003245
Iteration 358/1000 | Loss: 0.00003297
Iteration 359/1000 | Loss: 0.00003216
Iteration 360/1000 | Loss: 0.00003750
Iteration 361/1000 | Loss: 0.00003468
Iteration 362/1000 | Loss: 0.00003893
Iteration 363/1000 | Loss: 0.00003602
Iteration 364/1000 | Loss: 0.00003976
Iteration 365/1000 | Loss: 0.00003820
Iteration 366/1000 | Loss: 0.00004154
Iteration 367/1000 | Loss: 0.00004042
Iteration 368/1000 | Loss: 0.00004437
Iteration 369/1000 | Loss: 0.00004180
Iteration 370/1000 | Loss: 0.00004028
Iteration 371/1000 | Loss: 0.00003916
Iteration 372/1000 | Loss: 0.00004696
Iteration 373/1000 | Loss: 0.00004283
Iteration 374/1000 | Loss: 0.00004278
Iteration 375/1000 | Loss: 0.00004414
Iteration 376/1000 | Loss: 0.00003947
Iteration 377/1000 | Loss: 0.00003925
Iteration 378/1000 | Loss: 0.00004178
Iteration 379/1000 | Loss: 0.00004355
Iteration 380/1000 | Loss: 0.00004297
Iteration 381/1000 | Loss: 0.00004002
Iteration 382/1000 | Loss: 0.00004206
Iteration 383/1000 | Loss: 0.00004152
Iteration 384/1000 | Loss: 0.00003958
Iteration 385/1000 | Loss: 0.00004090
Iteration 386/1000 | Loss: 0.00006191
Iteration 387/1000 | Loss: 0.00004054
Iteration 388/1000 | Loss: 0.00004576
Iteration 389/1000 | Loss: 0.00004019
Iteration 390/1000 | Loss: 0.00005040
Iteration 391/1000 | Loss: 0.00004026
Iteration 392/1000 | Loss: 0.00003630
Iteration 393/1000 | Loss: 0.00003353
Iteration 394/1000 | Loss: 0.00003400
Iteration 395/1000 | Loss: 0.00003159
Iteration 396/1000 | Loss: 0.00003183
Iteration 397/1000 | Loss: 0.00003147
Iteration 398/1000 | Loss: 0.00003149
Iteration 399/1000 | Loss: 0.00003297
Iteration 400/1000 | Loss: 0.00003245
Iteration 401/1000 | Loss: 0.00004107
Iteration 402/1000 | Loss: 0.00003386
Iteration 403/1000 | Loss: 0.00003784
Iteration 404/1000 | Loss: 0.00003470
Iteration 405/1000 | Loss: 0.00004053
Iteration 406/1000 | Loss: 0.00003531
Iteration 407/1000 | Loss: 0.00003580
Iteration 408/1000 | Loss: 0.00003437
Iteration 409/1000 | Loss: 0.00003532
Iteration 410/1000 | Loss: 0.00003305
Iteration 411/1000 | Loss: 0.00004037
Iteration 412/1000 | Loss: 0.00003399
Iteration 413/1000 | Loss: 0.00004175
Iteration 414/1000 | Loss: 0.00003465
Iteration 415/1000 | Loss: 0.00004293
Iteration 416/1000 | Loss: 0.00003489
Iteration 417/1000 | Loss: 0.00004282
Iteration 418/1000 | Loss: 0.00003533
Iteration 419/1000 | Loss: 0.00004414
Iteration 420/1000 | Loss: 0.00003816
Iteration 421/1000 | Loss: 0.00004338
Iteration 422/1000 | Loss: 0.00003853
Iteration 423/1000 | Loss: 0.00004103
Iteration 424/1000 | Loss: 0.00004198
Iteration 425/1000 | Loss: 0.00004648
Iteration 426/1000 | Loss: 0.00004175
Iteration 427/1000 | Loss: 0.00003739
Iteration 428/1000 | Loss: 0.00004130
Iteration 429/1000 | Loss: 0.00004316
Iteration 430/1000 | Loss: 0.00004033
Iteration 431/1000 | Loss: 0.00004175
Iteration 432/1000 | Loss: 0.00004002
Iteration 433/1000 | Loss: 0.00003849
Iteration 434/1000 | Loss: 0.00003749
Iteration 435/1000 | Loss: 0.00003470
Iteration 436/1000 | Loss: 0.00003692
Iteration 437/1000 | Loss: 0.00003620
Iteration 438/1000 | Loss: 0.00003591
Iteration 439/1000 | Loss: 0.00003457
Iteration 440/1000 | Loss: 0.00004140
Iteration 441/1000 | Loss: 0.00003807
Iteration 442/1000 | Loss: 0.00004047
Iteration 443/1000 | Loss: 0.00003731
Iteration 444/1000 | Loss: 0.00004075
Iteration 445/1000 | Loss: 0.00003660
Iteration 446/1000 | Loss: 0.00003702
Iteration 447/1000 | Loss: 0.00003481
Iteration 448/1000 | Loss: 0.00003353
Iteration 449/1000 | Loss: 0.00004321
Iteration 450/1000 | Loss: 0.00003601
Iteration 451/1000 | Loss: 0.00003625
Iteration 452/1000 | Loss: 0.00003393
Iteration 453/1000 | Loss: 0.00003846
Iteration 454/1000 | Loss: 0.00003271
Iteration 455/1000 | Loss: 0.00003205
Iteration 456/1000 | Loss: 0.00003128
Iteration 457/1000 | Loss: 0.00003812
Iteration 458/1000 | Loss: 0.00003648
Iteration 459/1000 | Loss: 0.00004187
Iteration 460/1000 | Loss: 0.00003615
Iteration 461/1000 | Loss: 0.00003813
Iteration 462/1000 | Loss: 0.00003886
Iteration 463/1000 | Loss: 0.00003825
Iteration 464/1000 | Loss: 0.00003770
Iteration 465/1000 | Loss: 0.00003880
Iteration 466/1000 | Loss: 0.00003645
Iteration 467/1000 | Loss: 0.00003879
Iteration 468/1000 | Loss: 0.00003684
Iteration 469/1000 | Loss: 0.00004355
Iteration 470/1000 | Loss: 0.00003673
Iteration 471/1000 | Loss: 0.00003669
Iteration 472/1000 | Loss: 0.00003513
Iteration 473/1000 | Loss: 0.00004060
Iteration 474/1000 | Loss: 0.00004087
Iteration 475/1000 | Loss: 0.00004432
Iteration 476/1000 | Loss: 0.00004053
Iteration 477/1000 | Loss: 0.00004053
Iteration 478/1000 | Loss: 0.00004193
Iteration 479/1000 | Loss: 0.00004017
Iteration 480/1000 | Loss: 0.00004835
Iteration 481/1000 | Loss: 0.00003879
Iteration 482/1000 | Loss: 0.00003422
Iteration 483/1000 | Loss: 0.00003268
Iteration 484/1000 | Loss: 0.00003857
Iteration 485/1000 | Loss: 0.00003618
Iteration 486/1000 | Loss: 0.00003630
Iteration 487/1000 | Loss: 0.00003530
Iteration 488/1000 | Loss: 0.00004255
Iteration 489/1000 | Loss: 0.00003527
Iteration 490/1000 | Loss: 0.00003710
Iteration 491/1000 | Loss: 0.00003643
Iteration 492/1000 | Loss: 0.00004500
Iteration 493/1000 | Loss: 0.00003645
Iteration 494/1000 | Loss: 0.00004312
Iteration 495/1000 | Loss: 0.00003629
Iteration 496/1000 | Loss: 0.00003731
Iteration 497/1000 | Loss: 0.00003772
Iteration 498/1000 | Loss: 0.00003849
Iteration 499/1000 | Loss: 0.00003785
Iteration 500/1000 | Loss: 0.00003966
Iteration 501/1000 | Loss: 0.00003843
Iteration 502/1000 | Loss: 0.00003861
Iteration 503/1000 | Loss: 0.00004151
Iteration 504/1000 | Loss: 0.00004072
Iteration 505/1000 | Loss: 0.00003947
Iteration 506/1000 | Loss: 0.00004087
Iteration 507/1000 | Loss: 0.00003924
Iteration 508/1000 | Loss: 0.00003941
Iteration 509/1000 | Loss: 0.00003856
Iteration 510/1000 | Loss: 0.00003502
Iteration 511/1000 | Loss: 0.00003407
Iteration 512/1000 | Loss: 0.00003164
Iteration 513/1000 | Loss: 0.00003326
Iteration 514/1000 | Loss: 0.00003376
Iteration 515/1000 | Loss: 0.00003371
Iteration 516/1000 | Loss: 0.00003229
Iteration 517/1000 | Loss: 0.00003438
Iteration 518/1000 | Loss: 0.00003637
Iteration 519/1000 | Loss: 0.00003813
Iteration 520/1000 | Loss: 0.00003589
Iteration 521/1000 | Loss: 0.00003705
Iteration 522/1000 | Loss: 0.00003630
Iteration 523/1000 | Loss: 0.00003581
Iteration 524/1000 | Loss: 0.00003378
Iteration 525/1000 | Loss: 0.00003309
Iteration 526/1000 | Loss: 0.00003511
Iteration 527/1000 | Loss: 0.00003679
Iteration 528/1000 | Loss: 0.00003834
Iteration 529/1000 | Loss: 0.00004013
Iteration 530/1000 | Loss: 0.00003736
Iteration 531/1000 | Loss: 0.00004162
Iteration 532/1000 | Loss: 0.00003697
Iteration 533/1000 | Loss: 0.00003402
Iteration 534/1000 | Loss: 0.00003327
Iteration 535/1000 | Loss: 0.00003920
Iteration 536/1000 | Loss: 0.00003866
Iteration 537/1000 | Loss: 0.00003293
Iteration 538/1000 | Loss: 0.00003513
Iteration 539/1000 | Loss: 0.00003555
Iteration 540/1000 | Loss: 0.00003376
Iteration 541/1000 | Loss: 0.00003217
Iteration 542/1000 | Loss: 0.00003372
Iteration 543/1000 | Loss: 0.00003217
Iteration 544/1000 | Loss: 0.00003206
Iteration 545/1000 | Loss: 0.00003740
Iteration 546/1000 | Loss: 0.00004189
Iteration 547/1000 | Loss: 0.00003261
Iteration 548/1000 | Loss: 0.00003451
Iteration 549/1000 | Loss: 0.00003233
Iteration 550/1000 | Loss: 0.00003615
Iteration 551/1000 | Loss: 0.00004144
Iteration 552/1000 | Loss: 0.00003265
Iteration 553/1000 | Loss: 0.00003524
Iteration 554/1000 | Loss: 0.00003694
Iteration 555/1000 | Loss: 0.00003583
Iteration 556/1000 | Loss: 0.00003316
Iteration 557/1000 | Loss: 0.00003479
Iteration 558/1000 | Loss: 0.00003212
Iteration 559/1000 | Loss: 0.00004043
Iteration 560/1000 | Loss: 0.00003355
Iteration 561/1000 | Loss: 0.00003713
Iteration 562/1000 | Loss: 0.00003213
Iteration 563/1000 | Loss: 0.00003672
Iteration 564/1000 | Loss: 0.00004405
Iteration 565/1000 | Loss: 0.00004487
Iteration 566/1000 | Loss: 0.00003490
Iteration 567/1000 | Loss: 0.00003331
Iteration 568/1000 | Loss: 0.00003757
Iteration 569/1000 | Loss: 0.00003382
Iteration 570/1000 | Loss: 0.00003224
Iteration 571/1000 | Loss: 0.00004097
Iteration 572/1000 | Loss: 0.00003525
Iteration 573/1000 | Loss: 0.00004306
Iteration 574/1000 | Loss: 0.00003798
Iteration 575/1000 | Loss: 0.00004106
Iteration 576/1000 | Loss: 0.00004066
Iteration 577/1000 | Loss: 0.00004249
Iteration 578/1000 | Loss: 0.00003893
Iteration 579/1000 | Loss: 0.00004347
Iteration 580/1000 | Loss: 0.00003812
Iteration 581/1000 | Loss: 0.00004743
Iteration 582/1000 | Loss: 0.00004078
Iteration 583/1000 | Loss: 0.00004192
Iteration 584/1000 | Loss: 0.00004097
Iteration 585/1000 | Loss: 0.00004465
Iteration 586/1000 | Loss: 0.00003847
Iteration 587/1000 | Loss: 0.00005248
Iteration 588/1000 | Loss: 0.00003903
Iteration 589/1000 | Loss: 0.00004448
Iteration 590/1000 | Loss: 0.00004521
Iteration 591/1000 | Loss: 0.00004071
Iteration 592/1000 | Loss: 0.00004725
Iteration 593/1000 | Loss: 0.00004689
Iteration 594/1000 | Loss: 0.00004783
Iteration 595/1000 | Loss: 0.00004377
Iteration 596/1000 | Loss: 0.00003713
Iteration 597/1000 | Loss: 0.00003481
Iteration 598/1000 | Loss: 0.00003462
Iteration 599/1000 | Loss: 0.00003198
Iteration 600/1000 | Loss: 0.00003361
Iteration 601/1000 | Loss: 0.00003857
Iteration 602/1000 | Loss: 0.00004399
Iteration 603/1000 | Loss: 0.00003521
Iteration 604/1000 | Loss: 0.00003377
Iteration 605/1000 | Loss: 0.00003305
Iteration 606/1000 | Loss: 0.00003884
Iteration 607/1000 | Loss: 0.00003882
Iteration 608/1000 | Loss: 0.00003686
Iteration 609/1000 | Loss: 0.00003641
Iteration 610/1000 | Loss: 0.00003673
Iteration 611/1000 | Loss: 0.00003558
Iteration 612/1000 | Loss: 0.00003678
Iteration 613/1000 | Loss: 0.00003552
Iteration 614/1000 | Loss: 0.00004457
Iteration 615/1000 | Loss: 0.00003864
Iteration 616/1000 | Loss: 0.00004088
Iteration 617/1000 | Loss: 0.00003916
Iteration 618/1000 | Loss: 0.00004709
Iteration 619/1000 | Loss: 0.00003941
Iteration 620/1000 | Loss: 0.00004284
Iteration 621/1000 | Loss: 0.00004167
Iteration 622/1000 | Loss: 0.00004020
Iteration 623/1000 | Loss: 0.00004054
Iteration 624/1000 | Loss: 0.00003976
Iteration 625/1000 | Loss: 0.00003814
Iteration 626/1000 | Loss: 0.00003394
Iteration 627/1000 | Loss: 0.00003851
Iteration 628/1000 | Loss: 0.00004221
Iteration 629/1000 | Loss: 0.00003740
Iteration 630/1000 | Loss: 0.00004308
Iteration 631/1000 | Loss: 0.00004000
Iteration 632/1000 | Loss: 0.00004244
Iteration 633/1000 | Loss: 0.00003857
Iteration 634/1000 | Loss: 0.00004523
Iteration 635/1000 | Loss: 0.00003909
Iteration 636/1000 | Loss: 0.00003624
Iteration 637/1000 | Loss: 0.00003264
Iteration 638/1000 | Loss: 0.00004184
Iteration 639/1000 | Loss: 0.00003426
Iteration 640/1000 | Loss: 0.00003404
Iteration 641/1000 | Loss: 0.00003469
Iteration 642/1000 | Loss: 0.00003318
Iteration 643/1000 | Loss: 0.00003368
Iteration 644/1000 | Loss: 0.00003794
Iteration 645/1000 | Loss: 0.00003628
Iteration 646/1000 | Loss: 0.00003413
Iteration 647/1000 | Loss: 0.00003557
Iteration 648/1000 | Loss: 0.00003882
Iteration 649/1000 | Loss: 0.00003664
Iteration 650/1000 | Loss: 0.00004040
Iteration 651/1000 | Loss: 0.00004185
Iteration 652/1000 | Loss: 0.00004831
Iteration 653/1000 | Loss: 0.00003900
Iteration 654/1000 | Loss: 0.00003391
Iteration 655/1000 | Loss: 0.00003286
Iteration 656/1000 | Loss: 0.00003707
Iteration 657/1000 | Loss: 0.00003925
Iteration 658/1000 | Loss: 0.00003369
Iteration 659/1000 | Loss: 0.00003487
Iteration 660/1000 | Loss: 0.00003243
Iteration 661/1000 | Loss: 0.00003247
Iteration 662/1000 | Loss: 0.00003209
Iteration 663/1000 | Loss: 0.00003248
Iteration 664/1000 | Loss: 0.00003329
Iteration 665/1000 | Loss: 0.00003710
Iteration 666/1000 | Loss: 0.00003332
Iteration 667/1000 | Loss: 0.00003582
Iteration 668/1000 | Loss: 0.00003319
Iteration 669/1000 | Loss: 0.00003615
Iteration 670/1000 | Loss: 0.00003253
Iteration 671/1000 | Loss: 0.00003348
Iteration 672/1000 | Loss: 0.00003323
Iteration 673/1000 | Loss: 0.00003246
Iteration 674/1000 | Loss: 0.00003475
Iteration 675/1000 | Loss: 0.00003372
Iteration 676/1000 | Loss: 0.00003721
Iteration 677/1000 | Loss: 0.00003422
Iteration 678/1000 | Loss: 0.00003958
Iteration 679/1000 | Loss: 0.00003613
Iteration 680/1000 | Loss: 0.00003401
Iteration 681/1000 | Loss: 0.00003330
Iteration 682/1000 | Loss: 0.00003470
Iteration 683/1000 | Loss: 0.00003434
Iteration 684/1000 | Loss: 0.00003891
Iteration 685/1000 | Loss: 0.00003577
Iteration 686/1000 | Loss: 0.00004075
Iteration 687/1000 | Loss: 0.00003769
Iteration 688/1000 | Loss: 0.00004328
Iteration 689/1000 | Loss: 0.00003705
Iteration 690/1000 | Loss: 0.00003659
Iteration 691/1000 | Loss: 0.00003530
Iteration 692/1000 | Loss: 0.00004000
Iteration 693/1000 | Loss: 0.00004165
Iteration 694/1000 | Loss: 0.00004246
Iteration 695/1000 | Loss: 0.00004442
Iteration 696/1000 | Loss: 0.00004113
Iteration 697/1000 | Loss: 0.00004195
Iteration 698/1000 | Loss: 0.00004151
Iteration 699/1000 | Loss: 0.00003949
Iteration 700/1000 | Loss: 0.00003845
Iteration 701/1000 | Loss: 0.00004295
Iteration 702/1000 | Loss: 0.00003615
Iteration 703/1000 | Loss: 0.00003884
Iteration 704/1000 | Loss: 0.00004513
Iteration 705/1000 | Loss: 0.00003797
Iteration 706/1000 | Loss: 0.00004324
Iteration 707/1000 | Loss: 0.00003885
Iteration 708/1000 | Loss: 0.00004574
Iteration 709/1000 | Loss: 0.00004195
Iteration 710/1000 | Loss: 0.00004261
Iteration 711/1000 | Loss: 0.00003946
Iteration 712/1000 | Loss: 0.00004456
Iteration 713/1000 | Loss: 0.00003905
Iteration 714/1000 | Loss: 0.00004075
Iteration 715/1000 | Loss: 0.00004156
Iteration 716/1000 | Loss: 0.00004056
Iteration 717/1000 | Loss: 0.00004397
Iteration 718/1000 | Loss: 0.00004353
Iteration 719/1000 | Loss: 0.00004147
Iteration 720/1000 | Loss: 0.00004045
Iteration 721/1000 | Loss: 0.00004809
Iteration 722/1000 | Loss: 0.00003972
Iteration 723/1000 | Loss: 0.00004799
Iteration 724/1000 | Loss: 0.00003900
Iteration 725/1000 | Loss: 0.00005104
Iteration 726/1000 | Loss: 0.00003880
Iteration 727/1000 | Loss: 0.00004471
Iteration 728/1000 | Loss: 0.00003733
Iteration 729/1000 | Loss: 0.00004097
Iteration 730/1000 | Loss: 0.00004448
Iteration 731/1000 | Loss: 0.00004036
Iteration 732/1000 | Loss: 0.00005091
Iteration 733/1000 | Loss: 0.00004002
Iteration 734/1000 | Loss: 0.00004551
Iteration 735/1000 | Loss: 0.00003749
Iteration 736/1000 | Loss: 0.00003932
Iteration 737/1000 | Loss: 0.00004079
Iteration 738/1000 | Loss: 0.00003884
Iteration 739/1000 | Loss: 0.00003890
Iteration 740/1000 | Loss: 0.00003837
Iteration 741/1000 | Loss: 0.00003857
Iteration 742/1000 | Loss: 0.00003786
Iteration 743/1000 | Loss: 0.00004125
Iteration 744/1000 | Loss: 0.00003751
Iteration 745/1000 | Loss: 0.00004001
Iteration 746/1000 | Loss: 0.00003714
Iteration 747/1000 | Loss: 0.00004029
Iteration 748/1000 | Loss: 0.00003732
Iteration 749/1000 | Loss: 0.00003871
Iteration 750/1000 | Loss: 0.00003596
Iteration 751/1000 | Loss: 0.00003418
Iteration 752/1000 | Loss: 0.00003673
Iteration 753/1000 | Loss: 0.00003766
Iteration 754/1000 | Loss: 0.00003672
Iteration 755/1000 | Loss: 0.00004004
Iteration 756/1000 | Loss: 0.00003758
Iteration 757/1000 | Loss: 0.00003812
Iteration 758/1000 | Loss: 0.00003763
Iteration 759/1000 | Loss: 0.00003770
Iteration 760/1000 | Loss: 0.00003792
Iteration 761/1000 | Loss: 0.00003570
Iteration 762/1000 | Loss: 0.00003826
Iteration 763/1000 | Loss: 0.00003673
Iteration 764/1000 | Loss: 0.00003788
Iteration 765/1000 | Loss: 0.00003624
Iteration 766/1000 | Loss: 0.00003694
Iteration 767/1000 | Loss: 0.00003611
Iteration 768/1000 | Loss: 0.00003801
Iteration 769/1000 | Loss: 0.00003539
Iteration 770/1000 | Loss: 0.00003901
Iteration 771/1000 | Loss: 0.00003542
Iteration 772/1000 | Loss: 0.00003540
Iteration 773/1000 | Loss: 0.00003718
Iteration 774/1000 | Loss: 0.00003519
Iteration 775/1000 | Loss: 0.00003595
Iteration 776/1000 | Loss: 0.00003483
Iteration 777/1000 | Loss: 0.00003698
Iteration 778/1000 | Loss: 0.00003451
Iteration 779/1000 | Loss: 0.00003740
Iteration 780/1000 | Loss: 0.00003436
Iteration 781/1000 | Loss: 0.00003759
Iteration 782/1000 | Loss: 0.00003485
Iteration 783/1000 | Loss: 0.00004019
Iteration 784/1000 | Loss: 0.00003643
Iteration 785/1000 | Loss: 0.00003913
Iteration 786/1000 | Loss: 0.00003581
Iteration 787/1000 | Loss: 0.00003736
Iteration 788/1000 | Loss: 0.00003589
Iteration 789/1000 | Loss: 0.00003777
Iteration 790/1000 | Loss: 0.00003605
Iteration 791/1000 | Loss: 0.00003903
Iteration 792/1000 | Loss: 0.00003578
Iteration 793/1000 | Loss: 0.00003767
Iteration 794/1000 | Loss: 0.00003860
Iteration 795/1000 | Loss: 0.00003743
Iteration 796/1000 | Loss: 0.00003824
Iteration 797/1000 | Loss: 0.00004201
Iteration 798/1000 | Loss: 0.00003753
Iteration 799/1000 | Loss: 0.00003869
Iteration 800/1000 | Loss: 0.00003710
Iteration 801/1000 | Loss: 0.00004120
Iteration 802/1000 | Loss: 0.00003674
Iteration 803/1000 | Loss: 0.00003970
Iteration 804/1000 | Loss: 0.00003589
Iteration 805/1000 | Loss: 0.00004090
Iteration 806/1000 | Loss: 0.00003513
Iteration 807/1000 | Loss: 0.00004027
Iteration 808/1000 | Loss: 0.00003524
Iteration 809/1000 | Loss: 0.00004105
Iteration 810/1000 | Loss: 0.00003498
Iteration 811/1000 | Loss: 0.00004501
Iteration 812/1000 | Loss: 0.00003500
Iteration 813/1000 | Loss: 0.00004022
Iteration 814/1000 | Loss: 0.00003490
Iteration 815/1000 | Loss: 0.00003490
Iteration 816/1000 | Loss: 0.00003489
Iteration 817/1000 | Loss: 0.00003489
Iteration 818/1000 | Loss: 0.00004139
Iteration 819/1000 | Loss: 0.00003520
Iteration 820/1000 | Loss: 0.00003852
Iteration 821/1000 | Loss: 0.00003597
Iteration 822/1000 | Loss: 0.00003753
Iteration 823/1000 | Loss: 0.00003664
Iteration 824/1000 | Loss: 0.00004260
Iteration 825/1000 | Loss: 0.00003520
Iteration 826/1000 | Loss: 0.00004066
Iteration 827/1000 | Loss: 0.00003498
Iteration 828/1000 | Loss: 0.00004375
Iteration 829/1000 | Loss: 0.00003451
Iteration 830/1000 | Loss: 0.00004050
Iteration 831/1000 | Loss: 0.00003528
Iteration 832/1000 | Loss: 0.00003889
Iteration 833/1000 | Loss: 0.00003685
Iteration 834/1000 | Loss: 0.00003975
Iteration 835/1000 | Loss: 0.00003611
Iteration 836/1000 | Loss: 0.00003958
Iteration 837/1000 | Loss: 0.00003566
Iteration 838/1000 | Loss: 0.00004057
Iteration 839/1000 | Loss: 0.00003614
Iteration 840/1000 | Loss: 0.00003938
Iteration 841/1000 | Loss: 0.00003717
Iteration 842/1000 | Loss: 0.00003839
Iteration 843/1000 | Loss: 0.00003601
Iteration 844/1000 | Loss: 0.00004223
Iteration 845/1000 | Loss: 0.00003534
Iteration 846/1000 | Loss: 0.00004258
Iteration 847/1000 | Loss: 0.00003476
Iteration 848/1000 | Loss: 0.00004294
Iteration 849/1000 | Loss: 0.00003542
Iteration 850/1000 | Loss: 0.00004272
Iteration 851/1000 | Loss: 0.00003613
Iteration 852/1000 | Loss: 0.00004203
Iteration 853/1000 | Loss: 0.00003510
Iteration 854/1000 | Loss: 0.00003999
Iteration 855/1000 | Loss: 0.00003475
Iteration 856/1000 | Loss: 0.00003403
Iteration 857/1000 | Loss: 0.00003180
Iteration 858/1000 | Loss: 0.00003489
Iteration 859/1000 | Loss: 0.00003243
Iteration 860/1000 | Loss: 0.00004844
Iteration 861/1000 | Loss: 0.00003848
Iteration 862/1000 | Loss: 0.00003319
Iteration 863/1000 | Loss: 0.00004300
Iteration 864/1000 | Loss: 0.00003966
Iteration 865/1000 | Loss: 0.00004820
Iteration 866/1000 | Loss: 0.00003542
Iteration 867/1000 | Loss: 0.00003327
Iteration 868/1000 | Loss: 0.00003948
Iteration 869/1000 | Loss: 0.00003387
Iteration 870/1000 | Loss: 0.00003296
Iteration 871/1000 | Loss: 0.00004438
Iteration 872/1000 | Loss: 0.00004061
Iteration 873/1000 | Loss: 0.00003256
Iteration 874/1000 | Loss: 0.00003674
Iteration 875/1000 | Loss: 0.00003304
Iteration 876/1000 | Loss: 0.00003300
Iteration 877/1000 | Loss: 0.00003565
Iteration 878/1000 | Loss: 0.00003293
Iteration 879/1000 | Loss: 0.00003953
Iteration 880/1000 | Loss: 0.00003282
Iteration 881/1000 | Loss: 0.00003271
Iteration 882/1000 | Loss: 0.00003709
Iteration 883/1000 | Loss: 0.00003994
Iteration 884/1000 | Loss: 0.00003341
Iteration 885/1000 | Loss: 0.00003319
Iteration 886/1000 | Loss: 0.00004820
Iteration 887/1000 | Loss: 0.00003385
Iteration 888/1000 | Loss: 0.00003318
Iteration 889/1000 | Loss: 0.00003905
Iteration 890/1000 | Loss: 0.00003333
Iteration 891/1000 | Loss: 0.00003801
Iteration 892/1000 | Loss: 0.00003280
Iteration 893/1000 | Loss: 0.00003255
Iteration 894/1000 | Loss: 0.00004281
Iteration 895/1000 | Loss: 0.00003365
Iteration 896/1000 | Loss: 0.00003441
Iteration 897/1000 | Loss: 0.00003464
Iteration 898/1000 | Loss: 0.00003896
Iteration 899/1000 | Loss: 0.00004475
Iteration 900/1000 | Loss: 0.00003348
Iteration 901/1000 | Loss: 0.00003419
Iteration 902/1000 | Loss: 0.00004373
Iteration 903/1000 | Loss: 0.00004107
Iteration 904/1000 | Loss: 0.00003388
Iteration 905/1000 | Loss: 0.00005793
Iteration 906/1000 | Loss: 0.00004307
Iteration 907/1000 | Loss: 0.00004983
Iteration 908/1000 | Loss: 0.00004041
Iteration 909/1000 | Loss: 0.00004468
Iteration 910/1000 | Loss: 0.00003972
Iteration 911/1000 | Loss: 0.00003971
Iteration 912/1000 | Loss: 0.00003684
Iteration 913/1000 | Loss: 0.00003503
Iteration 914/1000 | Loss: 0.00003905
Iteration 915/1000 | Loss: 0.00003243
Iteration 916/1000 | Loss: 0.00004534
Iteration 917/1000 | Loss: 0.00003449
Iteration 918/1000 | Loss: 0.00003245
Iteration 919/1000 | Loss: 0.00004012
Iteration 920/1000 | Loss: 0.00003616
Iteration 921/1000 | Loss: 0.00003781
Iteration 922/1000 | Loss: 0.00003241
Iteration 923/1000 | Loss: 0.00003224
Iteration 924/1000 | Loss: 0.00003128
Iteration 925/1000 | Loss: 0.00003149
Iteration 926/1000 | Loss: 0.00003177
Iteration 927/1000 | Loss: 0.00003150
Iteration 928/1000 | Loss: 0.00003200
Iteration 929/1000 | Loss: 0.00003493
Iteration 930/1000 | Loss: 0.00004423
Iteration 931/1000 | Loss: 0.00003750
Iteration 932/1000 | Loss: 0.00004471
Iteration 933/1000 | Loss: 0.00003885
Iteration 934/1000 | Loss: 0.00004580
Iteration 935/1000 | Loss: 0.00003757
Iteration 936/1000 | Loss: 0.00004323
Iteration 937/1000 | Loss: 0.00003693
Iteration 938/1000 | Loss: 0.00003950
Iteration 939/1000 | Loss: 0.00003608
Iteration 940/1000 | Loss: 0.00003919
Iteration 941/1000 | Loss: 0.00003603
Iteration 942/1000 | Loss: 0.00003920
Iteration 943/1000 | Loss: 0.00003591
Iteration 944/1000 | Loss: 0.00004171
Iteration 945/1000 | Loss: 0.00003752
Iteration 946/1000 | Loss: 0.00003957
Iteration 947/1000 | Loss: 0.00003997
Iteration 948/1000 | Loss: 0.00004008
Iteration 949/1000 | Loss: 0.00003951
Iteration 950/1000 | Loss: 0.00004246
Iteration 951/1000 | Loss: 0.00003894
Iteration 952/1000 | Loss: 0.00004009
Iteration 953/1000 | Loss: 0.00003830
Iteration 954/1000 | Loss: 0.00003970
Iteration 955/1000 | Loss: 0.00003820
Iteration 956/1000 | Loss: 0.00003819
Iteration 957/1000 | Loss: 0.00003819
Iteration 958/1000 | Loss: 0.00003819
Iteration 959/1000 | Loss: 0.00003819
Iteration 960/1000 | Loss: 0.00003819
Iteration 961/1000 | Loss: 0.00003819
Iteration 962/1000 | Loss: 0.00003643
Iteration 963/1000 | Loss: 0.00003509
Iteration 964/1000 | Loss: 0.00004055
Iteration 965/1000 | Loss: 0.00003337
Iteration 966/1000 | Loss: 0.00004141
Iteration 967/1000 | Loss: 0.00003499
Iteration 968/1000 | Loss: 0.00004316
Iteration 969/1000 | Loss: 0.00003312
Iteration 970/1000 | Loss: 0.00004328
Iteration 971/1000 | Loss: 0.00003651
Iteration 972/1000 | Loss: 0.00004452
Iteration 973/1000 | Loss: 0.00003669
Iteration 974/1000 | Loss: 0.00004270
Iteration 975/1000 | Loss: 0.00003839
Iteration 976/1000 | Loss: 0.00004292
Iteration 977/1000 | Loss: 0.00003756
Iteration 978/1000 | Loss: 0.00004344
Iteration 979/1000 | Loss: 0.00003688
Iteration 980/1000 | Loss: 0.00004566
Iteration 981/1000 | Loss: 0.00004150
Iteration 982/1000 | Loss: 0.00003362
Iteration 983/1000 | Loss: 0.00003496
Iteration 984/1000 | Loss: 0.00003233
Iteration 985/1000 | Loss: 0.00003979
Iteration 986/1000 | Loss: 0.00003363
Iteration 987/1000 | Loss: 0.00003833
Iteration 988/1000 | Loss: 0.00003385
Iteration 989/1000 | Loss: 0.00004789
Iteration 990/1000 | Loss: 0.00003415
Iteration 991/1000 | Loss: 0.00004519
Iteration 992/1000 | Loss: 0.00003407
Iteration 993/1000 | Loss: 0.00004676
Iteration 994/1000 | Loss: 0.00003393
Iteration 995/1000 | Loss: 0.00004462
Iteration 996/1000 | Loss: 0.00003375
Iteration 997/1000 | Loss: 0.00004767
Iteration 998/1000 | Loss: 0.00003365
Iteration 999/1000 | Loss: 0.00004661
Iteration 1000/1000 | Loss: 0.00003348

Optimization complete. Final v2v error: 4.409879684448242 mm

Highest mean error: 20.3167724609375 mm for frame 81

Lowest mean error: 3.187842607498169 mm for frame 153

Saving results

Total time: 1612.701473236084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924586
Iteration 2/25 | Loss: 0.00213480
Iteration 3/25 | Loss: 0.00199509
Iteration 4/25 | Loss: 0.00196979
Iteration 5/25 | Loss: 0.00196274
Iteration 6/25 | Loss: 0.00196172
Iteration 7/25 | Loss: 0.00196172
Iteration 8/25 | Loss: 0.00196172
Iteration 9/25 | Loss: 0.00196172
Iteration 10/25 | Loss: 0.00196172
Iteration 11/25 | Loss: 0.00196172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0019617220386862755, 0.0019617220386862755, 0.0019617220386862755, 0.0019617220386862755, 0.0019617220386862755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019617220386862755

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12408805
Iteration 2/25 | Loss: 0.00255474
Iteration 3/25 | Loss: 0.00255471
Iteration 4/25 | Loss: 0.00255471
Iteration 5/25 | Loss: 0.00255471
Iteration 6/25 | Loss: 0.00255471
Iteration 7/25 | Loss: 0.00255471
Iteration 8/25 | Loss: 0.00255471
Iteration 9/25 | Loss: 0.00255471
Iteration 10/25 | Loss: 0.00255471
Iteration 11/25 | Loss: 0.00255471
Iteration 12/25 | Loss: 0.00255471
Iteration 13/25 | Loss: 0.00255471
Iteration 14/25 | Loss: 0.00255471
Iteration 15/25 | Loss: 0.00255471
Iteration 16/25 | Loss: 0.00255471
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0025547060649842024, 0.0025547060649842024, 0.0025547060649842024, 0.0025547060649842024, 0.0025547060649842024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025547060649842024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00255471
Iteration 2/1000 | Loss: 0.00011737
Iteration 3/1000 | Loss: 0.00006271
Iteration 4/1000 | Loss: 0.00004701
Iteration 5/1000 | Loss: 0.00004220
Iteration 6/1000 | Loss: 0.00003901
Iteration 7/1000 | Loss: 0.00003716
Iteration 8/1000 | Loss: 0.00003574
Iteration 9/1000 | Loss: 0.00003447
Iteration 10/1000 | Loss: 0.00003355
Iteration 11/1000 | Loss: 0.00003294
Iteration 12/1000 | Loss: 0.00003236
Iteration 13/1000 | Loss: 0.00003184
Iteration 14/1000 | Loss: 0.00003152
Iteration 15/1000 | Loss: 0.00003124
Iteration 16/1000 | Loss: 0.00003103
Iteration 17/1000 | Loss: 0.00003098
Iteration 18/1000 | Loss: 0.00003094
Iteration 19/1000 | Loss: 0.00003078
Iteration 20/1000 | Loss: 0.00003070
Iteration 21/1000 | Loss: 0.00003070
Iteration 22/1000 | Loss: 0.00003070
Iteration 23/1000 | Loss: 0.00003070
Iteration 24/1000 | Loss: 0.00003070
Iteration 25/1000 | Loss: 0.00003070
Iteration 26/1000 | Loss: 0.00003070
Iteration 27/1000 | Loss: 0.00003069
Iteration 28/1000 | Loss: 0.00003069
Iteration 29/1000 | Loss: 0.00003068
Iteration 30/1000 | Loss: 0.00003068
Iteration 31/1000 | Loss: 0.00003067
Iteration 32/1000 | Loss: 0.00003067
Iteration 33/1000 | Loss: 0.00003067
Iteration 34/1000 | Loss: 0.00003067
Iteration 35/1000 | Loss: 0.00003066
Iteration 36/1000 | Loss: 0.00003066
Iteration 37/1000 | Loss: 0.00003066
Iteration 38/1000 | Loss: 0.00003066
Iteration 39/1000 | Loss: 0.00003066
Iteration 40/1000 | Loss: 0.00003066
Iteration 41/1000 | Loss: 0.00003065
Iteration 42/1000 | Loss: 0.00003065
Iteration 43/1000 | Loss: 0.00003065
Iteration 44/1000 | Loss: 0.00003065
Iteration 45/1000 | Loss: 0.00003065
Iteration 46/1000 | Loss: 0.00003065
Iteration 47/1000 | Loss: 0.00003065
Iteration 48/1000 | Loss: 0.00003065
Iteration 49/1000 | Loss: 0.00003065
Iteration 50/1000 | Loss: 0.00003064
Iteration 51/1000 | Loss: 0.00003064
Iteration 52/1000 | Loss: 0.00003064
Iteration 53/1000 | Loss: 0.00003064
Iteration 54/1000 | Loss: 0.00003064
Iteration 55/1000 | Loss: 0.00003064
Iteration 56/1000 | Loss: 0.00003064
Iteration 57/1000 | Loss: 0.00003064
Iteration 58/1000 | Loss: 0.00003064
Iteration 59/1000 | Loss: 0.00003064
Iteration 60/1000 | Loss: 0.00003064
Iteration 61/1000 | Loss: 0.00003063
Iteration 62/1000 | Loss: 0.00003063
Iteration 63/1000 | Loss: 0.00003063
Iteration 64/1000 | Loss: 0.00003063
Iteration 65/1000 | Loss: 0.00003063
Iteration 66/1000 | Loss: 0.00003063
Iteration 67/1000 | Loss: 0.00003063
Iteration 68/1000 | Loss: 0.00003063
Iteration 69/1000 | Loss: 0.00003062
Iteration 70/1000 | Loss: 0.00003062
Iteration 71/1000 | Loss: 0.00003062
Iteration 72/1000 | Loss: 0.00003062
Iteration 73/1000 | Loss: 0.00003062
Iteration 74/1000 | Loss: 0.00003062
Iteration 75/1000 | Loss: 0.00003062
Iteration 76/1000 | Loss: 0.00003062
Iteration 77/1000 | Loss: 0.00003062
Iteration 78/1000 | Loss: 0.00003062
Iteration 79/1000 | Loss: 0.00003062
Iteration 80/1000 | Loss: 0.00003062
Iteration 81/1000 | Loss: 0.00003062
Iteration 82/1000 | Loss: 0.00003062
Iteration 83/1000 | Loss: 0.00003062
Iteration 84/1000 | Loss: 0.00003062
Iteration 85/1000 | Loss: 0.00003062
Iteration 86/1000 | Loss: 0.00003062
Iteration 87/1000 | Loss: 0.00003061
Iteration 88/1000 | Loss: 0.00003061
Iteration 89/1000 | Loss: 0.00003061
Iteration 90/1000 | Loss: 0.00003061
Iteration 91/1000 | Loss: 0.00003061
Iteration 92/1000 | Loss: 0.00003061
Iteration 93/1000 | Loss: 0.00003061
Iteration 94/1000 | Loss: 0.00003061
Iteration 95/1000 | Loss: 0.00003061
Iteration 96/1000 | Loss: 0.00003061
Iteration 97/1000 | Loss: 0.00003061
Iteration 98/1000 | Loss: 0.00003061
Iteration 99/1000 | Loss: 0.00003061
Iteration 100/1000 | Loss: 0.00003061
Iteration 101/1000 | Loss: 0.00003061
Iteration 102/1000 | Loss: 0.00003061
Iteration 103/1000 | Loss: 0.00003061
Iteration 104/1000 | Loss: 0.00003061
Iteration 105/1000 | Loss: 0.00003061
Iteration 106/1000 | Loss: 0.00003061
Iteration 107/1000 | Loss: 0.00003061
Iteration 108/1000 | Loss: 0.00003061
Iteration 109/1000 | Loss: 0.00003061
Iteration 110/1000 | Loss: 0.00003061
Iteration 111/1000 | Loss: 0.00003061
Iteration 112/1000 | Loss: 0.00003061
Iteration 113/1000 | Loss: 0.00003061
Iteration 114/1000 | Loss: 0.00003061
Iteration 115/1000 | Loss: 0.00003061
Iteration 116/1000 | Loss: 0.00003061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [3.060715243918821e-05, 3.060715243918821e-05, 3.060715243918821e-05, 3.060715243918821e-05, 3.060715243918821e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.060715243918821e-05

Optimization complete. Final v2v error: 4.8263468742370605 mm

Highest mean error: 5.186251163482666 mm for frame 94

Lowest mean error: 4.580516815185547 mm for frame 29

Saving results

Total time: 40.21009612083435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950320
Iteration 2/25 | Loss: 0.00235927
Iteration 3/25 | Loss: 0.00208490
Iteration 4/25 | Loss: 0.00206554
Iteration 5/25 | Loss: 0.00200318
Iteration 6/25 | Loss: 0.00199321
Iteration 7/25 | Loss: 0.00199227
Iteration 8/25 | Loss: 0.00199178
Iteration 9/25 | Loss: 0.00199476
Iteration 10/25 | Loss: 0.00198899
Iteration 11/25 | Loss: 0.00198824
Iteration 12/25 | Loss: 0.00198626
Iteration 13/25 | Loss: 0.00198512
Iteration 14/25 | Loss: 0.00198533
Iteration 15/25 | Loss: 0.00198477
Iteration 16/25 | Loss: 0.00198537
Iteration 17/25 | Loss: 0.00198557
Iteration 18/25 | Loss: 0.00198524
Iteration 19/25 | Loss: 0.00198528
Iteration 20/25 | Loss: 0.00198513
Iteration 21/25 | Loss: 0.00198522
Iteration 22/25 | Loss: 0.00198508
Iteration 23/25 | Loss: 0.00198511
Iteration 24/25 | Loss: 0.00198532
Iteration 25/25 | Loss: 0.00198546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79340690
Iteration 2/25 | Loss: 0.00210894
Iteration 3/25 | Loss: 0.00210894
Iteration 4/25 | Loss: 0.00210894
Iteration 5/25 | Loss: 0.00210894
Iteration 6/25 | Loss: 0.00210894
Iteration 7/25 | Loss: 0.00210894
Iteration 8/25 | Loss: 0.00210894
Iteration 9/25 | Loss: 0.00210894
Iteration 10/25 | Loss: 0.00210894
Iteration 11/25 | Loss: 0.00210894
Iteration 12/25 | Loss: 0.00210894
Iteration 13/25 | Loss: 0.00210894
Iteration 14/25 | Loss: 0.00210894
Iteration 15/25 | Loss: 0.00210894
Iteration 16/25 | Loss: 0.00210894
Iteration 17/25 | Loss: 0.00210894
Iteration 18/25 | Loss: 0.00210894
Iteration 19/25 | Loss: 0.00210894
Iteration 20/25 | Loss: 0.00210894
Iteration 21/25 | Loss: 0.00210894
Iteration 22/25 | Loss: 0.00210894
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002108937129378319, 0.002108937129378319, 0.002108937129378319, 0.002108937129378319, 0.002108937129378319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002108937129378319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210894
Iteration 2/1000 | Loss: 0.00010555
Iteration 3/1000 | Loss: 0.00009345
Iteration 4/1000 | Loss: 0.00011879
Iteration 5/1000 | Loss: 0.00006745
Iteration 6/1000 | Loss: 0.00007941
Iteration 7/1000 | Loss: 0.00005793
Iteration 8/1000 | Loss: 0.00006794
Iteration 9/1000 | Loss: 0.00006373
Iteration 10/1000 | Loss: 0.00006201
Iteration 11/1000 | Loss: 0.00006500
Iteration 12/1000 | Loss: 0.00004936
Iteration 13/1000 | Loss: 0.00005753
Iteration 14/1000 | Loss: 0.00006376
Iteration 15/1000 | Loss: 0.00005878
Iteration 16/1000 | Loss: 0.00006660
Iteration 17/1000 | Loss: 0.00004778
Iteration 18/1000 | Loss: 0.00005037
Iteration 19/1000 | Loss: 0.00005058
Iteration 20/1000 | Loss: 0.00007005
Iteration 21/1000 | Loss: 0.00005788
Iteration 22/1000 | Loss: 0.00004470
Iteration 23/1000 | Loss: 0.00006328
Iteration 24/1000 | Loss: 0.00005803
Iteration 25/1000 | Loss: 0.00006172
Iteration 26/1000 | Loss: 0.00006371
Iteration 27/1000 | Loss: 0.00006119
Iteration 28/1000 | Loss: 0.00007676
Iteration 29/1000 | Loss: 0.00005663
Iteration 30/1000 | Loss: 0.00006475
Iteration 31/1000 | Loss: 0.00004858
Iteration 32/1000 | Loss: 0.00006939
Iteration 33/1000 | Loss: 0.00005608
Iteration 34/1000 | Loss: 0.00004795
Iteration 35/1000 | Loss: 0.00004877
Iteration 36/1000 | Loss: 0.00007297
Iteration 37/1000 | Loss: 0.00004441
Iteration 38/1000 | Loss: 0.00004269
Iteration 39/1000 | Loss: 0.00004217
Iteration 40/1000 | Loss: 0.00004166
Iteration 41/1000 | Loss: 0.00004144
Iteration 42/1000 | Loss: 0.00004139
Iteration 43/1000 | Loss: 0.00004131
Iteration 44/1000 | Loss: 0.00004128
Iteration 45/1000 | Loss: 0.00004127
Iteration 46/1000 | Loss: 0.00004127
Iteration 47/1000 | Loss: 0.00004126
Iteration 48/1000 | Loss: 0.00004125
Iteration 49/1000 | Loss: 0.00004124
Iteration 50/1000 | Loss: 0.00004124
Iteration 51/1000 | Loss: 0.00004124
Iteration 52/1000 | Loss: 0.00004123
Iteration 53/1000 | Loss: 0.00004123
Iteration 54/1000 | Loss: 0.00004121
Iteration 55/1000 | Loss: 0.00004121
Iteration 56/1000 | Loss: 0.00004121
Iteration 57/1000 | Loss: 0.00004120
Iteration 58/1000 | Loss: 0.00004120
Iteration 59/1000 | Loss: 0.00004119
Iteration 60/1000 | Loss: 0.00004119
Iteration 61/1000 | Loss: 0.00004119
Iteration 62/1000 | Loss: 0.00004118
Iteration 63/1000 | Loss: 0.00004118
Iteration 64/1000 | Loss: 0.00004118
Iteration 65/1000 | Loss: 0.00004117
Iteration 66/1000 | Loss: 0.00004117
Iteration 67/1000 | Loss: 0.00004117
Iteration 68/1000 | Loss: 0.00004116
Iteration 69/1000 | Loss: 0.00004116
Iteration 70/1000 | Loss: 0.00004116
Iteration 71/1000 | Loss: 0.00004116
Iteration 72/1000 | Loss: 0.00004116
Iteration 73/1000 | Loss: 0.00004116
Iteration 74/1000 | Loss: 0.00004116
Iteration 75/1000 | Loss: 0.00004115
Iteration 76/1000 | Loss: 0.00004115
Iteration 77/1000 | Loss: 0.00004115
Iteration 78/1000 | Loss: 0.00004115
Iteration 79/1000 | Loss: 0.00004115
Iteration 80/1000 | Loss: 0.00004115
Iteration 81/1000 | Loss: 0.00004115
Iteration 82/1000 | Loss: 0.00004114
Iteration 83/1000 | Loss: 0.00004114
Iteration 84/1000 | Loss: 0.00004113
Iteration 85/1000 | Loss: 0.00004113
Iteration 86/1000 | Loss: 0.00004112
Iteration 87/1000 | Loss: 0.00004112
Iteration 88/1000 | Loss: 0.00004112
Iteration 89/1000 | Loss: 0.00004111
Iteration 90/1000 | Loss: 0.00004111
Iteration 91/1000 | Loss: 0.00004109
Iteration 92/1000 | Loss: 0.00004109
Iteration 93/1000 | Loss: 0.00004109
Iteration 94/1000 | Loss: 0.00004109
Iteration 95/1000 | Loss: 0.00004109
Iteration 96/1000 | Loss: 0.00004109
Iteration 97/1000 | Loss: 0.00004109
Iteration 98/1000 | Loss: 0.00004109
Iteration 99/1000 | Loss: 0.00004109
Iteration 100/1000 | Loss: 0.00004109
Iteration 101/1000 | Loss: 0.00004109
Iteration 102/1000 | Loss: 0.00004109
Iteration 103/1000 | Loss: 0.00004109
Iteration 104/1000 | Loss: 0.00004109
Iteration 105/1000 | Loss: 0.00004109
Iteration 106/1000 | Loss: 0.00004108
Iteration 107/1000 | Loss: 0.00004108
Iteration 108/1000 | Loss: 0.00004108
Iteration 109/1000 | Loss: 0.00004108
Iteration 110/1000 | Loss: 0.00004108
Iteration 111/1000 | Loss: 0.00004108
Iteration 112/1000 | Loss: 0.00004108
Iteration 113/1000 | Loss: 0.00004108
Iteration 114/1000 | Loss: 0.00004108
Iteration 115/1000 | Loss: 0.00004107
Iteration 116/1000 | Loss: 0.00004107
Iteration 117/1000 | Loss: 0.00004107
Iteration 118/1000 | Loss: 0.00004107
Iteration 119/1000 | Loss: 0.00004107
Iteration 120/1000 | Loss: 0.00004106
Iteration 121/1000 | Loss: 0.00004106
Iteration 122/1000 | Loss: 0.00004106
Iteration 123/1000 | Loss: 0.00004105
Iteration 124/1000 | Loss: 0.00004105
Iteration 125/1000 | Loss: 0.00004105
Iteration 126/1000 | Loss: 0.00004105
Iteration 127/1000 | Loss: 0.00004103
Iteration 128/1000 | Loss: 0.00004103
Iteration 129/1000 | Loss: 0.00004103
Iteration 130/1000 | Loss: 0.00004102
Iteration 131/1000 | Loss: 0.00004102
Iteration 132/1000 | Loss: 0.00004102
Iteration 133/1000 | Loss: 0.00004102
Iteration 134/1000 | Loss: 0.00004102
Iteration 135/1000 | Loss: 0.00004101
Iteration 136/1000 | Loss: 0.00004101
Iteration 137/1000 | Loss: 0.00004101
Iteration 138/1000 | Loss: 0.00004101
Iteration 139/1000 | Loss: 0.00004101
Iteration 140/1000 | Loss: 0.00004101
Iteration 141/1000 | Loss: 0.00004101
Iteration 142/1000 | Loss: 0.00004101
Iteration 143/1000 | Loss: 0.00004101
Iteration 144/1000 | Loss: 0.00004101
Iteration 145/1000 | Loss: 0.00004100
Iteration 146/1000 | Loss: 0.00004100
Iteration 147/1000 | Loss: 0.00004100
Iteration 148/1000 | Loss: 0.00004100
Iteration 149/1000 | Loss: 0.00004100
Iteration 150/1000 | Loss: 0.00004100
Iteration 151/1000 | Loss: 0.00004100
Iteration 152/1000 | Loss: 0.00004100
Iteration 153/1000 | Loss: 0.00004099
Iteration 154/1000 | Loss: 0.00004099
Iteration 155/1000 | Loss: 0.00004099
Iteration 156/1000 | Loss: 0.00004099
Iteration 157/1000 | Loss: 0.00004098
Iteration 158/1000 | Loss: 0.00004098
Iteration 159/1000 | Loss: 0.00004098
Iteration 160/1000 | Loss: 0.00004098
Iteration 161/1000 | Loss: 0.00004098
Iteration 162/1000 | Loss: 0.00004098
Iteration 163/1000 | Loss: 0.00004098
Iteration 164/1000 | Loss: 0.00004098
Iteration 165/1000 | Loss: 0.00004097
Iteration 166/1000 | Loss: 0.00004097
Iteration 167/1000 | Loss: 0.00004097
Iteration 168/1000 | Loss: 0.00004097
Iteration 169/1000 | Loss: 0.00004097
Iteration 170/1000 | Loss: 0.00004097
Iteration 171/1000 | Loss: 0.00004097
Iteration 172/1000 | Loss: 0.00004097
Iteration 173/1000 | Loss: 0.00004097
Iteration 174/1000 | Loss: 0.00004097
Iteration 175/1000 | Loss: 0.00004097
Iteration 176/1000 | Loss: 0.00004096
Iteration 177/1000 | Loss: 0.00004096
Iteration 178/1000 | Loss: 0.00004096
Iteration 179/1000 | Loss: 0.00004096
Iteration 180/1000 | Loss: 0.00004096
Iteration 181/1000 | Loss: 0.00004096
Iteration 182/1000 | Loss: 0.00004095
Iteration 183/1000 | Loss: 0.00004095
Iteration 184/1000 | Loss: 0.00004095
Iteration 185/1000 | Loss: 0.00004095
Iteration 186/1000 | Loss: 0.00004095
Iteration 187/1000 | Loss: 0.00004095
Iteration 188/1000 | Loss: 0.00004095
Iteration 189/1000 | Loss: 0.00004095
Iteration 190/1000 | Loss: 0.00004095
Iteration 191/1000 | Loss: 0.00004095
Iteration 192/1000 | Loss: 0.00004095
Iteration 193/1000 | Loss: 0.00004095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [4.0952872950583696e-05, 4.0952872950583696e-05, 4.0952872950583696e-05, 4.0952872950583696e-05, 4.0952872950583696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.0952872950583696e-05

Optimization complete. Final v2v error: 5.4662184715271 mm

Highest mean error: 5.778093338012695 mm for frame 132

Lowest mean error: 5.312239170074463 mm for frame 44

Saving results

Total time: 114.71447682380676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01106351
Iteration 2/25 | Loss: 0.00377699
Iteration 3/25 | Loss: 0.00294016
Iteration 4/25 | Loss: 0.00255560
Iteration 5/25 | Loss: 0.00281737
Iteration 6/25 | Loss: 0.00257986
Iteration 7/25 | Loss: 0.00253825
Iteration 8/25 | Loss: 0.00237069
Iteration 9/25 | Loss: 0.00228754
Iteration 10/25 | Loss: 0.00228619
Iteration 11/25 | Loss: 0.00229185
Iteration 12/25 | Loss: 0.00226400
Iteration 13/25 | Loss: 0.00222211
Iteration 14/25 | Loss: 0.00221394
Iteration 15/25 | Loss: 0.00220697
Iteration 16/25 | Loss: 0.00220940
Iteration 17/25 | Loss: 0.00220658
Iteration 18/25 | Loss: 0.00219967
Iteration 19/25 | Loss: 0.00220092
Iteration 20/25 | Loss: 0.00219555
Iteration 21/25 | Loss: 0.00219945
Iteration 22/25 | Loss: 0.00220048
Iteration 23/25 | Loss: 0.00219756
Iteration 24/25 | Loss: 0.00219906
Iteration 25/25 | Loss: 0.00219680

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.25691462
Iteration 2/25 | Loss: 0.00645363
Iteration 3/25 | Loss: 0.00632522
Iteration 4/25 | Loss: 0.00632522
Iteration 5/25 | Loss: 0.00632522
Iteration 6/25 | Loss: 0.00632522
Iteration 7/25 | Loss: 0.00632522
Iteration 8/25 | Loss: 0.00632522
Iteration 9/25 | Loss: 0.00632522
Iteration 10/25 | Loss: 0.00632522
Iteration 11/25 | Loss: 0.00632522
Iteration 12/25 | Loss: 0.00632522
Iteration 13/25 | Loss: 0.00632522
Iteration 14/25 | Loss: 0.00632522
Iteration 15/25 | Loss: 0.00632522
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.006325218826532364, 0.006325218826532364, 0.006325218826532364, 0.006325218826532364, 0.006325218826532364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006325218826532364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00632522
Iteration 2/1000 | Loss: 0.00238007
Iteration 3/1000 | Loss: 0.00404327
Iteration 4/1000 | Loss: 0.00751472
Iteration 5/1000 | Loss: 0.00149819
Iteration 6/1000 | Loss: 0.00415260
Iteration 7/1000 | Loss: 0.00201206
Iteration 8/1000 | Loss: 0.00049269
Iteration 9/1000 | Loss: 0.00038674
Iteration 10/1000 | Loss: 0.00292419
Iteration 11/1000 | Loss: 0.00109236
Iteration 12/1000 | Loss: 0.00095965
Iteration 13/1000 | Loss: 0.00070168
Iteration 14/1000 | Loss: 0.00050506
Iteration 15/1000 | Loss: 0.00070681
Iteration 16/1000 | Loss: 0.00150006
Iteration 17/1000 | Loss: 0.00106013
Iteration 18/1000 | Loss: 0.00075514
Iteration 19/1000 | Loss: 0.00023958
Iteration 20/1000 | Loss: 0.00079716
Iteration 21/1000 | Loss: 0.00044042
Iteration 22/1000 | Loss: 0.00050032
Iteration 23/1000 | Loss: 0.00034718
Iteration 24/1000 | Loss: 0.00028989
Iteration 25/1000 | Loss: 0.00064808
Iteration 26/1000 | Loss: 0.00066668
Iteration 27/1000 | Loss: 0.00103851
Iteration 28/1000 | Loss: 0.00074366
Iteration 29/1000 | Loss: 0.00135099
Iteration 30/1000 | Loss: 0.00149542
Iteration 31/1000 | Loss: 0.00182244
Iteration 32/1000 | Loss: 0.00123937
Iteration 33/1000 | Loss: 0.00117710
Iteration 34/1000 | Loss: 0.00070007
Iteration 35/1000 | Loss: 0.00014176
Iteration 36/1000 | Loss: 0.00078065
Iteration 37/1000 | Loss: 0.00040980
Iteration 38/1000 | Loss: 0.00019312
Iteration 39/1000 | Loss: 0.00010265
Iteration 40/1000 | Loss: 0.00038052
Iteration 41/1000 | Loss: 0.00078377
Iteration 42/1000 | Loss: 0.00072364
Iteration 43/1000 | Loss: 0.00088215
Iteration 44/1000 | Loss: 0.00117914
Iteration 45/1000 | Loss: 0.00189972
Iteration 46/1000 | Loss: 0.00074783
Iteration 47/1000 | Loss: 0.00051745
Iteration 48/1000 | Loss: 0.00029788
Iteration 49/1000 | Loss: 0.00023940
Iteration 50/1000 | Loss: 0.00019664
Iteration 51/1000 | Loss: 0.00026239
Iteration 52/1000 | Loss: 0.00012513
Iteration 53/1000 | Loss: 0.00017706
Iteration 54/1000 | Loss: 0.00010126
Iteration 55/1000 | Loss: 0.00010658
Iteration 56/1000 | Loss: 0.00009496
Iteration 57/1000 | Loss: 0.00012731
Iteration 58/1000 | Loss: 0.00007301
Iteration 59/1000 | Loss: 0.00015060
Iteration 60/1000 | Loss: 0.00014393
Iteration 61/1000 | Loss: 0.00015301
Iteration 62/1000 | Loss: 0.00008131
Iteration 63/1000 | Loss: 0.00016778
Iteration 64/1000 | Loss: 0.00019593
Iteration 65/1000 | Loss: 0.00014293
Iteration 66/1000 | Loss: 0.00010214
Iteration 67/1000 | Loss: 0.00019486
Iteration 68/1000 | Loss: 0.00015446
Iteration 69/1000 | Loss: 0.00020328
Iteration 70/1000 | Loss: 0.00015570
Iteration 71/1000 | Loss: 0.00009030
Iteration 72/1000 | Loss: 0.00007436
Iteration 73/1000 | Loss: 0.00010729
Iteration 74/1000 | Loss: 0.00013628
Iteration 75/1000 | Loss: 0.00011267
Iteration 76/1000 | Loss: 0.00011246
Iteration 77/1000 | Loss: 0.00012452
Iteration 78/1000 | Loss: 0.00011981
Iteration 79/1000 | Loss: 0.00013470
Iteration 80/1000 | Loss: 0.00012826
Iteration 81/1000 | Loss: 0.00010136
Iteration 82/1000 | Loss: 0.00014625
Iteration 83/1000 | Loss: 0.00010437
Iteration 84/1000 | Loss: 0.00012457
Iteration 85/1000 | Loss: 0.00011810
Iteration 86/1000 | Loss: 0.00006663
Iteration 87/1000 | Loss: 0.00013508
Iteration 88/1000 | Loss: 0.00029245
Iteration 89/1000 | Loss: 0.00019734
Iteration 90/1000 | Loss: 0.00011648
Iteration 91/1000 | Loss: 0.00014644
Iteration 92/1000 | Loss: 0.00013965
Iteration 93/1000 | Loss: 0.00014429
Iteration 94/1000 | Loss: 0.00007539
Iteration 95/1000 | Loss: 0.00020558
Iteration 96/1000 | Loss: 0.00021057
Iteration 97/1000 | Loss: 0.00013902
Iteration 98/1000 | Loss: 0.00022411
Iteration 99/1000 | Loss: 0.00029334
Iteration 100/1000 | Loss: 0.00020376
Iteration 101/1000 | Loss: 0.00019722
Iteration 102/1000 | Loss: 0.00010648
Iteration 103/1000 | Loss: 0.00017597
Iteration 104/1000 | Loss: 0.00022811
Iteration 105/1000 | Loss: 0.00020230
Iteration 106/1000 | Loss: 0.00009575
Iteration 107/1000 | Loss: 0.00017359
Iteration 108/1000 | Loss: 0.00014699
Iteration 109/1000 | Loss: 0.00016943
Iteration 110/1000 | Loss: 0.00016322
Iteration 111/1000 | Loss: 0.00015815
Iteration 112/1000 | Loss: 0.00070104
Iteration 113/1000 | Loss: 0.00024202
Iteration 114/1000 | Loss: 0.00018105
Iteration 115/1000 | Loss: 0.00017676
Iteration 116/1000 | Loss: 0.00009109
Iteration 117/1000 | Loss: 0.00012586
Iteration 118/1000 | Loss: 0.00014749
Iteration 119/1000 | Loss: 0.00013326
Iteration 120/1000 | Loss: 0.00015858
Iteration 121/1000 | Loss: 0.00013236
Iteration 122/1000 | Loss: 0.00010193
Iteration 123/1000 | Loss: 0.00013752
Iteration 124/1000 | Loss: 0.00013276
Iteration 125/1000 | Loss: 0.00008257
Iteration 126/1000 | Loss: 0.00009009
Iteration 127/1000 | Loss: 0.00007789
Iteration 128/1000 | Loss: 0.00010372
Iteration 129/1000 | Loss: 0.00009143
Iteration 130/1000 | Loss: 0.00010630
Iteration 131/1000 | Loss: 0.00007183
Iteration 132/1000 | Loss: 0.00019199
Iteration 133/1000 | Loss: 0.00016551
Iteration 134/1000 | Loss: 0.00011225
Iteration 135/1000 | Loss: 0.00016976
Iteration 136/1000 | Loss: 0.00019726
Iteration 137/1000 | Loss: 0.00008526
Iteration 138/1000 | Loss: 0.00007573
Iteration 139/1000 | Loss: 0.00007016
Iteration 140/1000 | Loss: 0.00034926
Iteration 141/1000 | Loss: 0.00014787
Iteration 142/1000 | Loss: 0.00020058
Iteration 143/1000 | Loss: 0.00050606
Iteration 144/1000 | Loss: 0.00039393
Iteration 145/1000 | Loss: 0.00038989
Iteration 146/1000 | Loss: 0.00037189
Iteration 147/1000 | Loss: 0.00037376
Iteration 148/1000 | Loss: 0.00108355
Iteration 149/1000 | Loss: 0.00033424
Iteration 150/1000 | Loss: 0.00025980
Iteration 151/1000 | Loss: 0.00007340
Iteration 152/1000 | Loss: 0.00011816
Iteration 153/1000 | Loss: 0.00017458
Iteration 154/1000 | Loss: 0.00013160
Iteration 155/1000 | Loss: 0.00012777
Iteration 156/1000 | Loss: 0.00012776
Iteration 157/1000 | Loss: 0.00008914
Iteration 158/1000 | Loss: 0.00006201
Iteration 159/1000 | Loss: 0.00005978
Iteration 160/1000 | Loss: 0.00005893
Iteration 161/1000 | Loss: 0.00005848
Iteration 162/1000 | Loss: 0.00005834
Iteration 163/1000 | Loss: 0.00005798
Iteration 164/1000 | Loss: 0.00005762
Iteration 165/1000 | Loss: 0.00125173
Iteration 166/1000 | Loss: 0.00311041
Iteration 167/1000 | Loss: 0.00047873
Iteration 168/1000 | Loss: 0.00008070
Iteration 169/1000 | Loss: 0.00010870
Iteration 170/1000 | Loss: 0.00010387
Iteration 171/1000 | Loss: 0.00006621
Iteration 172/1000 | Loss: 0.00006245
Iteration 173/1000 | Loss: 0.00058426
Iteration 174/1000 | Loss: 0.00036513
Iteration 175/1000 | Loss: 0.00011042
Iteration 176/1000 | Loss: 0.00006136
Iteration 177/1000 | Loss: 0.00014420
Iteration 178/1000 | Loss: 0.00025024
Iteration 179/1000 | Loss: 0.00015617
Iteration 180/1000 | Loss: 0.00006154
Iteration 181/1000 | Loss: 0.00018997
Iteration 182/1000 | Loss: 0.00019733
Iteration 183/1000 | Loss: 0.00006955
Iteration 184/1000 | Loss: 0.00014599
Iteration 185/1000 | Loss: 0.00015330
Iteration 186/1000 | Loss: 0.00015491
Iteration 187/1000 | Loss: 0.00005749
Iteration 188/1000 | Loss: 0.00005657
Iteration 189/1000 | Loss: 0.00005548
Iteration 190/1000 | Loss: 0.00005461
Iteration 191/1000 | Loss: 0.00045308
Iteration 192/1000 | Loss: 0.00014290
Iteration 193/1000 | Loss: 0.00038229
Iteration 194/1000 | Loss: 0.00012957
Iteration 195/1000 | Loss: 0.00036841
Iteration 196/1000 | Loss: 0.00005521
Iteration 197/1000 | Loss: 0.00005302
Iteration 198/1000 | Loss: 0.00005184
Iteration 199/1000 | Loss: 0.00005122
Iteration 200/1000 | Loss: 0.00005074
Iteration 201/1000 | Loss: 0.00005045
Iteration 202/1000 | Loss: 0.00005018
Iteration 203/1000 | Loss: 0.00005002
Iteration 204/1000 | Loss: 0.00004998
Iteration 205/1000 | Loss: 0.00004998
Iteration 206/1000 | Loss: 0.00004998
Iteration 207/1000 | Loss: 0.00004997
Iteration 208/1000 | Loss: 0.00004997
Iteration 209/1000 | Loss: 0.00004996
Iteration 210/1000 | Loss: 0.00004996
Iteration 211/1000 | Loss: 0.00004996
Iteration 212/1000 | Loss: 0.00004995
Iteration 213/1000 | Loss: 0.00004995
Iteration 214/1000 | Loss: 0.00004995
Iteration 215/1000 | Loss: 0.00004993
Iteration 216/1000 | Loss: 0.00004992
Iteration 217/1000 | Loss: 0.00004992
Iteration 218/1000 | Loss: 0.00004991
Iteration 219/1000 | Loss: 0.00004991
Iteration 220/1000 | Loss: 0.00004991
Iteration 221/1000 | Loss: 0.00004991
Iteration 222/1000 | Loss: 0.00004991
Iteration 223/1000 | Loss: 0.00004991
Iteration 224/1000 | Loss: 0.00004990
Iteration 225/1000 | Loss: 0.00004990
Iteration 226/1000 | Loss: 0.00004990
Iteration 227/1000 | Loss: 0.00004990
Iteration 228/1000 | Loss: 0.00004990
Iteration 229/1000 | Loss: 0.00004990
Iteration 230/1000 | Loss: 0.00004990
Iteration 231/1000 | Loss: 0.00004990
Iteration 232/1000 | Loss: 0.00004990
Iteration 233/1000 | Loss: 0.00004990
Iteration 234/1000 | Loss: 0.00004990
Iteration 235/1000 | Loss: 0.00004990
Iteration 236/1000 | Loss: 0.00004989
Iteration 237/1000 | Loss: 0.00004989
Iteration 238/1000 | Loss: 0.00004989
Iteration 239/1000 | Loss: 0.00004989
Iteration 240/1000 | Loss: 0.00004989
Iteration 241/1000 | Loss: 0.00004988
Iteration 242/1000 | Loss: 0.00004988
Iteration 243/1000 | Loss: 0.00004988
Iteration 244/1000 | Loss: 0.00004988
Iteration 245/1000 | Loss: 0.00062087
Iteration 246/1000 | Loss: 0.00026442
Iteration 247/1000 | Loss: 0.00005088
Iteration 248/1000 | Loss: 0.00005001
Iteration 249/1000 | Loss: 0.00004983
Iteration 250/1000 | Loss: 0.00004982
Iteration 251/1000 | Loss: 0.00004982
Iteration 252/1000 | Loss: 0.00004982
Iteration 253/1000 | Loss: 0.00004982
Iteration 254/1000 | Loss: 0.00004982
Iteration 255/1000 | Loss: 0.00004982
Iteration 256/1000 | Loss: 0.00004982
Iteration 257/1000 | Loss: 0.00004982
Iteration 258/1000 | Loss: 0.00004982
Iteration 259/1000 | Loss: 0.00004982
Iteration 260/1000 | Loss: 0.00004982
Iteration 261/1000 | Loss: 0.00004981
Iteration 262/1000 | Loss: 0.00004981
Iteration 263/1000 | Loss: 0.00004981
Iteration 264/1000 | Loss: 0.00004980
Iteration 265/1000 | Loss: 0.00004980
Iteration 266/1000 | Loss: 0.00004980
Iteration 267/1000 | Loss: 0.00004979
Iteration 268/1000 | Loss: 0.00004979
Iteration 269/1000 | Loss: 0.00004979
Iteration 270/1000 | Loss: 0.00004979
Iteration 271/1000 | Loss: 0.00004979
Iteration 272/1000 | Loss: 0.00004979
Iteration 273/1000 | Loss: 0.00062539
Iteration 274/1000 | Loss: 0.00028655
Iteration 275/1000 | Loss: 0.00005077
Iteration 276/1000 | Loss: 0.00004990
Iteration 277/1000 | Loss: 0.00004983
Iteration 278/1000 | Loss: 0.00004980
Iteration 279/1000 | Loss: 0.00004979
Iteration 280/1000 | Loss: 0.00004978
Iteration 281/1000 | Loss: 0.00004978
Iteration 282/1000 | Loss: 0.00004978
Iteration 283/1000 | Loss: 0.00004977
Iteration 284/1000 | Loss: 0.00004977
Iteration 285/1000 | Loss: 0.00004977
Iteration 286/1000 | Loss: 0.00004976
Iteration 287/1000 | Loss: 0.00004976
Iteration 288/1000 | Loss: 0.00004976
Iteration 289/1000 | Loss: 0.00004976
Iteration 290/1000 | Loss: 0.00004976
Iteration 291/1000 | Loss: 0.00004976
Iteration 292/1000 | Loss: 0.00004975
Iteration 293/1000 | Loss: 0.00004975
Iteration 294/1000 | Loss: 0.00004974
Iteration 295/1000 | Loss: 0.00004974
Iteration 296/1000 | Loss: 0.00004974
Iteration 297/1000 | Loss: 0.00004974
Iteration 298/1000 | Loss: 0.00004974
Iteration 299/1000 | Loss: 0.00004974
Iteration 300/1000 | Loss: 0.00004974
Iteration 301/1000 | Loss: 0.00004974
Iteration 302/1000 | Loss: 0.00004974
Iteration 303/1000 | Loss: 0.00004973
Iteration 304/1000 | Loss: 0.00004973
Iteration 305/1000 | Loss: 0.00004973
Iteration 306/1000 | Loss: 0.00004973
Iteration 307/1000 | Loss: 0.00004973
Iteration 308/1000 | Loss: 0.00004972
Iteration 309/1000 | Loss: 0.00004972
Iteration 310/1000 | Loss: 0.00004972
Iteration 311/1000 | Loss: 0.00004972
Iteration 312/1000 | Loss: 0.00004972
Iteration 313/1000 | Loss: 0.00004972
Iteration 314/1000 | Loss: 0.00004972
Iteration 315/1000 | Loss: 0.00004971
Iteration 316/1000 | Loss: 0.00004971
Iteration 317/1000 | Loss: 0.00004971
Iteration 318/1000 | Loss: 0.00004971
Iteration 319/1000 | Loss: 0.00004971
Iteration 320/1000 | Loss: 0.00004971
Iteration 321/1000 | Loss: 0.00004971
Iteration 322/1000 | Loss: 0.00004971
Iteration 323/1000 | Loss: 0.00004971
Iteration 324/1000 | Loss: 0.00004971
Iteration 325/1000 | Loss: 0.00004971
Iteration 326/1000 | Loss: 0.00004971
Iteration 327/1000 | Loss: 0.00004970
Iteration 328/1000 | Loss: 0.00004970
Iteration 329/1000 | Loss: 0.00004970
Iteration 330/1000 | Loss: 0.00004970
Iteration 331/1000 | Loss: 0.00004969
Iteration 332/1000 | Loss: 0.00004969
Iteration 333/1000 | Loss: 0.00004969
Iteration 334/1000 | Loss: 0.00004969
Iteration 335/1000 | Loss: 0.00004969
Iteration 336/1000 | Loss: 0.00004969
Iteration 337/1000 | Loss: 0.00004969
Iteration 338/1000 | Loss: 0.00004969
Iteration 339/1000 | Loss: 0.00004969
Iteration 340/1000 | Loss: 0.00004969
Iteration 341/1000 | Loss: 0.00004968
Iteration 342/1000 | Loss: 0.00004968
Iteration 343/1000 | Loss: 0.00004968
Iteration 344/1000 | Loss: 0.00004968
Iteration 345/1000 | Loss: 0.00004968
Iteration 346/1000 | Loss: 0.00004968
Iteration 347/1000 | Loss: 0.00004968
Iteration 348/1000 | Loss: 0.00004968
Iteration 349/1000 | Loss: 0.00004968
Iteration 350/1000 | Loss: 0.00004968
Iteration 351/1000 | Loss: 0.00004968
Iteration 352/1000 | Loss: 0.00004968
Iteration 353/1000 | Loss: 0.00004968
Iteration 354/1000 | Loss: 0.00004968
Iteration 355/1000 | Loss: 0.00004968
Iteration 356/1000 | Loss: 0.00004968
Iteration 357/1000 | Loss: 0.00004968
Iteration 358/1000 | Loss: 0.00004968
Iteration 359/1000 | Loss: 0.00004968
Iteration 360/1000 | Loss: 0.00004968
Iteration 361/1000 | Loss: 0.00004968
Iteration 362/1000 | Loss: 0.00004968
Iteration 363/1000 | Loss: 0.00004968
Iteration 364/1000 | Loss: 0.00004968
Iteration 365/1000 | Loss: 0.00004968
Iteration 366/1000 | Loss: 0.00004968
Iteration 367/1000 | Loss: 0.00004968
Iteration 368/1000 | Loss: 0.00004968
Iteration 369/1000 | Loss: 0.00004968
Iteration 370/1000 | Loss: 0.00004968
Iteration 371/1000 | Loss: 0.00004968
Iteration 372/1000 | Loss: 0.00004968
Iteration 373/1000 | Loss: 0.00004968
Iteration 374/1000 | Loss: 0.00004968
Iteration 375/1000 | Loss: 0.00004968
Iteration 376/1000 | Loss: 0.00004968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 376. Stopping optimization.
Last 5 losses: [4.9683287215884775e-05, 4.9683287215884775e-05, 4.9683287215884775e-05, 4.9683287215884775e-05, 4.9683287215884775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.9683287215884775e-05

Optimization complete. Final v2v error: 5.429040431976318 mm

Highest mean error: 14.020273208618164 mm for frame 93

Lowest mean error: 4.416199207305908 mm for frame 195

Saving results

Total time: 372.2843849658966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01235451
Iteration 2/25 | Loss: 0.01235451
Iteration 3/25 | Loss: 0.01235451
Iteration 4/25 | Loss: 0.00492241
Iteration 5/25 | Loss: 0.00352176
Iteration 6/25 | Loss: 0.00288053
Iteration 7/25 | Loss: 0.00259921
Iteration 8/25 | Loss: 0.00239717
Iteration 9/25 | Loss: 0.00240842
Iteration 10/25 | Loss: 0.00228170
Iteration 11/25 | Loss: 0.00233235
Iteration 12/25 | Loss: 0.00224434
Iteration 13/25 | Loss: 0.00222357
Iteration 14/25 | Loss: 0.00222303
Iteration 15/25 | Loss: 0.00220616
Iteration 16/25 | Loss: 0.00220295
Iteration 17/25 | Loss: 0.00220495
Iteration 18/25 | Loss: 0.00220412
Iteration 19/25 | Loss: 0.00219706
Iteration 20/25 | Loss: 0.00219979
Iteration 21/25 | Loss: 0.00220202
Iteration 22/25 | Loss: 0.00219102
Iteration 23/25 | Loss: 0.00218737
Iteration 24/25 | Loss: 0.00219102
Iteration 25/25 | Loss: 0.00218696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52611893
Iteration 2/25 | Loss: 0.00693199
Iteration 3/25 | Loss: 0.00693199
Iteration 4/25 | Loss: 0.00693199
Iteration 5/25 | Loss: 0.00693199
Iteration 6/25 | Loss: 0.00693199
Iteration 7/25 | Loss: 0.00693199
Iteration 8/25 | Loss: 0.00693199
Iteration 9/25 | Loss: 0.00693199
Iteration 10/25 | Loss: 0.00693199
Iteration 11/25 | Loss: 0.00693199
Iteration 12/25 | Loss: 0.00693199
Iteration 13/25 | Loss: 0.00693199
Iteration 14/25 | Loss: 0.00693199
Iteration 15/25 | Loss: 0.00693199
Iteration 16/25 | Loss: 0.00693199
Iteration 17/25 | Loss: 0.00693199
Iteration 18/25 | Loss: 0.00693199
Iteration 19/25 | Loss: 0.00693199
Iteration 20/25 | Loss: 0.00693199
Iteration 21/25 | Loss: 0.00693199
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.006931991782039404, 0.006931991782039404, 0.006931991782039404, 0.006931991782039404, 0.006931991782039404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006931991782039404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00693199
Iteration 2/1000 | Loss: 0.00177805
Iteration 3/1000 | Loss: 0.00244436
Iteration 4/1000 | Loss: 0.00211007
Iteration 5/1000 | Loss: 0.00302746
Iteration 6/1000 | Loss: 0.00131517
Iteration 7/1000 | Loss: 0.00494314
Iteration 8/1000 | Loss: 0.02066648
Iteration 9/1000 | Loss: 0.00553334
Iteration 10/1000 | Loss: 0.00940807
Iteration 11/1000 | Loss: 0.00194239
Iteration 12/1000 | Loss: 0.00370696
Iteration 13/1000 | Loss: 0.00071862
Iteration 14/1000 | Loss: 0.00099215
Iteration 15/1000 | Loss: 0.00063831
Iteration 16/1000 | Loss: 0.00065279
Iteration 17/1000 | Loss: 0.00056670
Iteration 18/1000 | Loss: 0.00049179
Iteration 19/1000 | Loss: 0.00045531
Iteration 20/1000 | Loss: 0.00051459
Iteration 21/1000 | Loss: 0.00043663
Iteration 22/1000 | Loss: 0.00042723
Iteration 23/1000 | Loss: 0.00041999
Iteration 24/1000 | Loss: 0.00040671
Iteration 25/1000 | Loss: 0.00040818
Iteration 26/1000 | Loss: 0.00040249
Iteration 27/1000 | Loss: 0.00038374
Iteration 28/1000 | Loss: 0.00369533
Iteration 29/1000 | Loss: 0.00126606
Iteration 30/1000 | Loss: 0.00042789
Iteration 31/1000 | Loss: 0.00053675
Iteration 32/1000 | Loss: 0.00038313
Iteration 33/1000 | Loss: 0.00037042
Iteration 34/1000 | Loss: 0.00036262
Iteration 35/1000 | Loss: 0.00073263
Iteration 36/1000 | Loss: 0.00038933
Iteration 37/1000 | Loss: 0.00035880
Iteration 38/1000 | Loss: 0.00034806
Iteration 39/1000 | Loss: 0.00034243
Iteration 40/1000 | Loss: 0.00033723
Iteration 41/1000 | Loss: 0.00033320
Iteration 42/1000 | Loss: 0.00051343
Iteration 43/1000 | Loss: 0.00052091
Iteration 44/1000 | Loss: 0.00033427
Iteration 45/1000 | Loss: 0.00032892
Iteration 46/1000 | Loss: 0.00032556
Iteration 47/1000 | Loss: 0.00032256
Iteration 48/1000 | Loss: 0.00032130
Iteration 49/1000 | Loss: 0.00032041
Iteration 50/1000 | Loss: 0.00031972
Iteration 51/1000 | Loss: 0.00031914
Iteration 52/1000 | Loss: 0.00031867
Iteration 53/1000 | Loss: 0.00031824
Iteration 54/1000 | Loss: 0.00031795
Iteration 55/1000 | Loss: 0.00031772
Iteration 56/1000 | Loss: 0.00031752
Iteration 57/1000 | Loss: 0.00031735
Iteration 58/1000 | Loss: 0.00031732
Iteration 59/1000 | Loss: 0.00031722
Iteration 60/1000 | Loss: 0.00031710
Iteration 61/1000 | Loss: 0.00031705
Iteration 62/1000 | Loss: 0.00031702
Iteration 63/1000 | Loss: 0.00031702
Iteration 64/1000 | Loss: 0.00031700
Iteration 65/1000 | Loss: 0.00031700
Iteration 66/1000 | Loss: 0.00031699
Iteration 67/1000 | Loss: 0.00031699
Iteration 68/1000 | Loss: 0.00031699
Iteration 69/1000 | Loss: 0.00031699
Iteration 70/1000 | Loss: 0.00031699
Iteration 71/1000 | Loss: 0.00031699
Iteration 72/1000 | Loss: 0.00031699
Iteration 73/1000 | Loss: 0.00031699
Iteration 74/1000 | Loss: 0.00031698
Iteration 75/1000 | Loss: 0.00031698
Iteration 76/1000 | Loss: 0.00031698
Iteration 77/1000 | Loss: 0.00031698
Iteration 78/1000 | Loss: 0.00031698
Iteration 79/1000 | Loss: 0.00031697
Iteration 80/1000 | Loss: 0.00031696
Iteration 81/1000 | Loss: 0.00031696
Iteration 82/1000 | Loss: 0.00031695
Iteration 83/1000 | Loss: 0.00031692
Iteration 84/1000 | Loss: 0.00031692
Iteration 85/1000 | Loss: 0.00031692
Iteration 86/1000 | Loss: 0.00031692
Iteration 87/1000 | Loss: 0.00031692
Iteration 88/1000 | Loss: 0.00031692
Iteration 89/1000 | Loss: 0.00031692
Iteration 90/1000 | Loss: 0.00031691
Iteration 91/1000 | Loss: 0.00031691
Iteration 92/1000 | Loss: 0.00031691
Iteration 93/1000 | Loss: 0.00031691
Iteration 94/1000 | Loss: 0.00031691
Iteration 95/1000 | Loss: 0.00031691
Iteration 96/1000 | Loss: 0.00031691
Iteration 97/1000 | Loss: 0.00031691
Iteration 98/1000 | Loss: 0.00031691
Iteration 99/1000 | Loss: 0.00031690
Iteration 100/1000 | Loss: 0.00031690
Iteration 101/1000 | Loss: 0.00031689
Iteration 102/1000 | Loss: 0.00031689
Iteration 103/1000 | Loss: 0.00031689
Iteration 104/1000 | Loss: 0.00031689
Iteration 105/1000 | Loss: 0.00031689
Iteration 106/1000 | Loss: 0.00031689
Iteration 107/1000 | Loss: 0.00031689
Iteration 108/1000 | Loss: 0.00031689
Iteration 109/1000 | Loss: 0.00031689
Iteration 110/1000 | Loss: 0.00031689
Iteration 111/1000 | Loss: 0.00031688
Iteration 112/1000 | Loss: 0.00031688
Iteration 113/1000 | Loss: 0.00031688
Iteration 114/1000 | Loss: 0.00031688
Iteration 115/1000 | Loss: 0.00031688
Iteration 116/1000 | Loss: 0.00031688
Iteration 117/1000 | Loss: 0.00031688
Iteration 118/1000 | Loss: 0.00031688
Iteration 119/1000 | Loss: 0.00031688
Iteration 120/1000 | Loss: 0.00031688
Iteration 121/1000 | Loss: 0.00031688
Iteration 122/1000 | Loss: 0.00031688
Iteration 123/1000 | Loss: 0.00031688
Iteration 124/1000 | Loss: 0.00031688
Iteration 125/1000 | Loss: 0.00031688
Iteration 126/1000 | Loss: 0.00031688
Iteration 127/1000 | Loss: 0.00031688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [0.00031688297167420387, 0.00031688297167420387, 0.00031688297167420387, 0.00031688297167420387, 0.00031688297167420387]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031688297167420387

Optimization complete. Final v2v error: 10.871479988098145 mm

Highest mean error: 13.911255836486816 mm for frame 213

Lowest mean error: 5.924027442932129 mm for frame 1

Saving results

Total time: 150.65565657615662
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491544
Iteration 2/25 | Loss: 0.00204515
Iteration 3/25 | Loss: 0.00195479
Iteration 4/25 | Loss: 0.00194309
Iteration 5/25 | Loss: 0.00193999
Iteration 6/25 | Loss: 0.00193921
Iteration 7/25 | Loss: 0.00193921
Iteration 8/25 | Loss: 0.00193921
Iteration 9/25 | Loss: 0.00193921
Iteration 10/25 | Loss: 0.00193921
Iteration 11/25 | Loss: 0.00193921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001939206151291728, 0.001939206151291728, 0.001939206151291728, 0.001939206151291728, 0.001939206151291728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001939206151291728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22338593
Iteration 2/25 | Loss: 0.00300219
Iteration 3/25 | Loss: 0.00300219
Iteration 4/25 | Loss: 0.00300219
Iteration 5/25 | Loss: 0.00300219
Iteration 6/25 | Loss: 0.00300219
Iteration 7/25 | Loss: 0.00300219
Iteration 8/25 | Loss: 0.00300219
Iteration 9/25 | Loss: 0.00300219
Iteration 10/25 | Loss: 0.00300219
Iteration 11/25 | Loss: 0.00300219
Iteration 12/25 | Loss: 0.00300219
Iteration 13/25 | Loss: 0.00300219
Iteration 14/25 | Loss: 0.00300219
Iteration 15/25 | Loss: 0.00300219
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0030021921265870333, 0.0030021921265870333, 0.0030021921265870333, 0.0030021921265870333, 0.0030021921265870333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030021921265870333

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00300219
Iteration 2/1000 | Loss: 0.00009441
Iteration 3/1000 | Loss: 0.00005448
Iteration 4/1000 | Loss: 0.00004372
Iteration 5/1000 | Loss: 0.00003963
Iteration 6/1000 | Loss: 0.00003745
Iteration 7/1000 | Loss: 0.00003665
Iteration 8/1000 | Loss: 0.00003591
Iteration 9/1000 | Loss: 0.00003521
Iteration 10/1000 | Loss: 0.00003478
Iteration 11/1000 | Loss: 0.00003444
Iteration 12/1000 | Loss: 0.00003410
Iteration 13/1000 | Loss: 0.00003384
Iteration 14/1000 | Loss: 0.00003359
Iteration 15/1000 | Loss: 0.00003341
Iteration 16/1000 | Loss: 0.00003340
Iteration 17/1000 | Loss: 0.00003333
Iteration 18/1000 | Loss: 0.00003331
Iteration 19/1000 | Loss: 0.00003331
Iteration 20/1000 | Loss: 0.00003324
Iteration 21/1000 | Loss: 0.00003324
Iteration 22/1000 | Loss: 0.00003324
Iteration 23/1000 | Loss: 0.00003323
Iteration 24/1000 | Loss: 0.00003315
Iteration 25/1000 | Loss: 0.00003315
Iteration 26/1000 | Loss: 0.00003310
Iteration 27/1000 | Loss: 0.00003310
Iteration 28/1000 | Loss: 0.00003309
Iteration 29/1000 | Loss: 0.00003307
Iteration 30/1000 | Loss: 0.00003307
Iteration 31/1000 | Loss: 0.00003307
Iteration 32/1000 | Loss: 0.00003306
Iteration 33/1000 | Loss: 0.00003306
Iteration 34/1000 | Loss: 0.00003306
Iteration 35/1000 | Loss: 0.00003306
Iteration 36/1000 | Loss: 0.00003306
Iteration 37/1000 | Loss: 0.00003306
Iteration 38/1000 | Loss: 0.00003306
Iteration 39/1000 | Loss: 0.00003306
Iteration 40/1000 | Loss: 0.00003306
Iteration 41/1000 | Loss: 0.00003306
Iteration 42/1000 | Loss: 0.00003306
Iteration 43/1000 | Loss: 0.00003306
Iteration 44/1000 | Loss: 0.00003306
Iteration 45/1000 | Loss: 0.00003305
Iteration 46/1000 | Loss: 0.00003305
Iteration 47/1000 | Loss: 0.00003304
Iteration 48/1000 | Loss: 0.00003304
Iteration 49/1000 | Loss: 0.00003304
Iteration 50/1000 | Loss: 0.00003304
Iteration 51/1000 | Loss: 0.00003304
Iteration 52/1000 | Loss: 0.00003304
Iteration 53/1000 | Loss: 0.00003303
Iteration 54/1000 | Loss: 0.00003303
Iteration 55/1000 | Loss: 0.00003303
Iteration 56/1000 | Loss: 0.00003303
Iteration 57/1000 | Loss: 0.00003303
Iteration 58/1000 | Loss: 0.00003303
Iteration 59/1000 | Loss: 0.00003302
Iteration 60/1000 | Loss: 0.00003302
Iteration 61/1000 | Loss: 0.00003302
Iteration 62/1000 | Loss: 0.00003302
Iteration 63/1000 | Loss: 0.00003301
Iteration 64/1000 | Loss: 0.00003301
Iteration 65/1000 | Loss: 0.00003301
Iteration 66/1000 | Loss: 0.00003301
Iteration 67/1000 | Loss: 0.00003301
Iteration 68/1000 | Loss: 0.00003301
Iteration 69/1000 | Loss: 0.00003301
Iteration 70/1000 | Loss: 0.00003301
Iteration 71/1000 | Loss: 0.00003301
Iteration 72/1000 | Loss: 0.00003301
Iteration 73/1000 | Loss: 0.00003300
Iteration 74/1000 | Loss: 0.00003300
Iteration 75/1000 | Loss: 0.00003300
Iteration 76/1000 | Loss: 0.00003300
Iteration 77/1000 | Loss: 0.00003300
Iteration 78/1000 | Loss: 0.00003300
Iteration 79/1000 | Loss: 0.00003300
Iteration 80/1000 | Loss: 0.00003299
Iteration 81/1000 | Loss: 0.00003299
Iteration 82/1000 | Loss: 0.00003299
Iteration 83/1000 | Loss: 0.00003299
Iteration 84/1000 | Loss: 0.00003299
Iteration 85/1000 | Loss: 0.00003299
Iteration 86/1000 | Loss: 0.00003299
Iteration 87/1000 | Loss: 0.00003298
Iteration 88/1000 | Loss: 0.00003298
Iteration 89/1000 | Loss: 0.00003298
Iteration 90/1000 | Loss: 0.00003298
Iteration 91/1000 | Loss: 0.00003298
Iteration 92/1000 | Loss: 0.00003298
Iteration 93/1000 | Loss: 0.00003298
Iteration 94/1000 | Loss: 0.00003298
Iteration 95/1000 | Loss: 0.00003298
Iteration 96/1000 | Loss: 0.00003298
Iteration 97/1000 | Loss: 0.00003298
Iteration 98/1000 | Loss: 0.00003298
Iteration 99/1000 | Loss: 0.00003297
Iteration 100/1000 | Loss: 0.00003297
Iteration 101/1000 | Loss: 0.00003297
Iteration 102/1000 | Loss: 0.00003297
Iteration 103/1000 | Loss: 0.00003297
Iteration 104/1000 | Loss: 0.00003297
Iteration 105/1000 | Loss: 0.00003297
Iteration 106/1000 | Loss: 0.00003297
Iteration 107/1000 | Loss: 0.00003297
Iteration 108/1000 | Loss: 0.00003297
Iteration 109/1000 | Loss: 0.00003297
Iteration 110/1000 | Loss: 0.00003297
Iteration 111/1000 | Loss: 0.00003296
Iteration 112/1000 | Loss: 0.00003296
Iteration 113/1000 | Loss: 0.00003296
Iteration 114/1000 | Loss: 0.00003296
Iteration 115/1000 | Loss: 0.00003296
Iteration 116/1000 | Loss: 0.00003295
Iteration 117/1000 | Loss: 0.00003295
Iteration 118/1000 | Loss: 0.00003295
Iteration 119/1000 | Loss: 0.00003295
Iteration 120/1000 | Loss: 0.00003295
Iteration 121/1000 | Loss: 0.00003295
Iteration 122/1000 | Loss: 0.00003295
Iteration 123/1000 | Loss: 0.00003294
Iteration 124/1000 | Loss: 0.00003294
Iteration 125/1000 | Loss: 0.00003294
Iteration 126/1000 | Loss: 0.00003294
Iteration 127/1000 | Loss: 0.00003294
Iteration 128/1000 | Loss: 0.00003294
Iteration 129/1000 | Loss: 0.00003294
Iteration 130/1000 | Loss: 0.00003294
Iteration 131/1000 | Loss: 0.00003294
Iteration 132/1000 | Loss: 0.00003294
Iteration 133/1000 | Loss: 0.00003294
Iteration 134/1000 | Loss: 0.00003294
Iteration 135/1000 | Loss: 0.00003294
Iteration 136/1000 | Loss: 0.00003294
Iteration 137/1000 | Loss: 0.00003294
Iteration 138/1000 | Loss: 0.00003294
Iteration 139/1000 | Loss: 0.00003294
Iteration 140/1000 | Loss: 0.00003294
Iteration 141/1000 | Loss: 0.00003294
Iteration 142/1000 | Loss: 0.00003294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [3.294239650131203e-05, 3.294239650131203e-05, 3.294239650131203e-05, 3.294239650131203e-05, 3.294239650131203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.294239650131203e-05

Optimization complete. Final v2v error: 4.954145908355713 mm

Highest mean error: 5.699512004852295 mm for frame 56

Lowest mean error: 4.5704145431518555 mm for frame 0

Saving results

Total time: 45.98658871650696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896785
Iteration 2/25 | Loss: 0.00291221
Iteration 3/25 | Loss: 0.00255792
Iteration 4/25 | Loss: 0.00246907
Iteration 5/25 | Loss: 0.00232556
Iteration 6/25 | Loss: 0.00229811
Iteration 7/25 | Loss: 0.00228579
Iteration 8/25 | Loss: 0.00219469
Iteration 9/25 | Loss: 0.00224497
Iteration 10/25 | Loss: 0.00214058
Iteration 11/25 | Loss: 0.00205952
Iteration 12/25 | Loss: 0.00203812
Iteration 13/25 | Loss: 0.00201330
Iteration 14/25 | Loss: 0.00200461
Iteration 15/25 | Loss: 0.00200135
Iteration 16/25 | Loss: 0.00200916
Iteration 17/25 | Loss: 0.00200690
Iteration 18/25 | Loss: 0.00199777
Iteration 19/25 | Loss: 0.00199656
Iteration 20/25 | Loss: 0.00199627
Iteration 21/25 | Loss: 0.00199605
Iteration 22/25 | Loss: 0.00199592
Iteration 23/25 | Loss: 0.00199589
Iteration 24/25 | Loss: 0.00199585
Iteration 25/25 | Loss: 0.00199585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71361554
Iteration 2/25 | Loss: 0.00586695
Iteration 3/25 | Loss: 0.00400345
Iteration 4/25 | Loss: 0.00400345
Iteration 5/25 | Loss: 0.00400345
Iteration 6/25 | Loss: 0.00400345
Iteration 7/25 | Loss: 0.00400345
Iteration 8/25 | Loss: 0.00400345
Iteration 9/25 | Loss: 0.00400345
Iteration 10/25 | Loss: 0.00400345
Iteration 11/25 | Loss: 0.00400345
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.004003451205790043, 0.004003451205790043, 0.004003451205790043, 0.004003451205790043, 0.004003451205790043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004003451205790043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00400345
Iteration 2/1000 | Loss: 0.00190137
Iteration 3/1000 | Loss: 0.00021838
Iteration 4/1000 | Loss: 0.00177161
Iteration 5/1000 | Loss: 0.00127389
Iteration 6/1000 | Loss: 0.00014282
Iteration 7/1000 | Loss: 0.00013016
Iteration 8/1000 | Loss: 0.00174554
Iteration 9/1000 | Loss: 0.00090726
Iteration 10/1000 | Loss: 0.00063347
Iteration 11/1000 | Loss: 0.00010956
Iteration 12/1000 | Loss: 0.00116599
Iteration 13/1000 | Loss: 0.00283447
Iteration 14/1000 | Loss: 0.00627076
Iteration 15/1000 | Loss: 0.00239046
Iteration 16/1000 | Loss: 0.00453868
Iteration 17/1000 | Loss: 0.00070414
Iteration 18/1000 | Loss: 0.00056937
Iteration 19/1000 | Loss: 0.00038560
Iteration 20/1000 | Loss: 0.00013551
Iteration 21/1000 | Loss: 0.00048100
Iteration 22/1000 | Loss: 0.00203131
Iteration 23/1000 | Loss: 0.00017025
Iteration 24/1000 | Loss: 0.00007673
Iteration 25/1000 | Loss: 0.00006011
Iteration 26/1000 | Loss: 0.00033598
Iteration 27/1000 | Loss: 0.00004683
Iteration 28/1000 | Loss: 0.00003940
Iteration 29/1000 | Loss: 0.00018310
Iteration 30/1000 | Loss: 0.00003552
Iteration 31/1000 | Loss: 0.00003404
Iteration 32/1000 | Loss: 0.00025979
Iteration 33/1000 | Loss: 0.00023049
Iteration 34/1000 | Loss: 0.00071636
Iteration 35/1000 | Loss: 0.00008112
Iteration 36/1000 | Loss: 0.00005917
Iteration 37/1000 | Loss: 0.00023160
Iteration 38/1000 | Loss: 0.00005616
Iteration 39/1000 | Loss: 0.00004124
Iteration 40/1000 | Loss: 0.00003086
Iteration 41/1000 | Loss: 0.00003029
Iteration 42/1000 | Loss: 0.00002961
Iteration 43/1000 | Loss: 0.00002923
Iteration 44/1000 | Loss: 0.00002891
Iteration 45/1000 | Loss: 0.00024426
Iteration 46/1000 | Loss: 0.00004863
Iteration 47/1000 | Loss: 0.00003235
Iteration 48/1000 | Loss: 0.00031137
Iteration 49/1000 | Loss: 0.00002883
Iteration 50/1000 | Loss: 0.00002855
Iteration 51/1000 | Loss: 0.00002855
Iteration 52/1000 | Loss: 0.00002854
Iteration 53/1000 | Loss: 0.00002854
Iteration 54/1000 | Loss: 0.00002853
Iteration 55/1000 | Loss: 0.00002853
Iteration 56/1000 | Loss: 0.00002853
Iteration 57/1000 | Loss: 0.00002852
Iteration 58/1000 | Loss: 0.00002852
Iteration 59/1000 | Loss: 0.00002852
Iteration 60/1000 | Loss: 0.00002851
Iteration 61/1000 | Loss: 0.00002851
Iteration 62/1000 | Loss: 0.00002851
Iteration 63/1000 | Loss: 0.00002851
Iteration 64/1000 | Loss: 0.00002850
Iteration 65/1000 | Loss: 0.00002850
Iteration 66/1000 | Loss: 0.00002849
Iteration 67/1000 | Loss: 0.00002849
Iteration 68/1000 | Loss: 0.00002849
Iteration 69/1000 | Loss: 0.00002848
Iteration 70/1000 | Loss: 0.00002848
Iteration 71/1000 | Loss: 0.00002848
Iteration 72/1000 | Loss: 0.00002848
Iteration 73/1000 | Loss: 0.00002848
Iteration 74/1000 | Loss: 0.00002848
Iteration 75/1000 | Loss: 0.00002847
Iteration 76/1000 | Loss: 0.00002847
Iteration 77/1000 | Loss: 0.00002847
Iteration 78/1000 | Loss: 0.00002847
Iteration 79/1000 | Loss: 0.00002847
Iteration 80/1000 | Loss: 0.00002847
Iteration 81/1000 | Loss: 0.00002847
Iteration 82/1000 | Loss: 0.00002846
Iteration 83/1000 | Loss: 0.00002846
Iteration 84/1000 | Loss: 0.00002846
Iteration 85/1000 | Loss: 0.00002845
Iteration 86/1000 | Loss: 0.00002845
Iteration 87/1000 | Loss: 0.00002845
Iteration 88/1000 | Loss: 0.00002845
Iteration 89/1000 | Loss: 0.00002845
Iteration 90/1000 | Loss: 0.00002845
Iteration 91/1000 | Loss: 0.00002845
Iteration 92/1000 | Loss: 0.00002845
Iteration 93/1000 | Loss: 0.00002844
Iteration 94/1000 | Loss: 0.00002844
Iteration 95/1000 | Loss: 0.00002844
Iteration 96/1000 | Loss: 0.00002844
Iteration 97/1000 | Loss: 0.00002844
Iteration 98/1000 | Loss: 0.00002844
Iteration 99/1000 | Loss: 0.00002843
Iteration 100/1000 | Loss: 0.00002843
Iteration 101/1000 | Loss: 0.00002843
Iteration 102/1000 | Loss: 0.00002843
Iteration 103/1000 | Loss: 0.00002843
Iteration 104/1000 | Loss: 0.00002843
Iteration 105/1000 | Loss: 0.00002843
Iteration 106/1000 | Loss: 0.00002843
Iteration 107/1000 | Loss: 0.00002843
Iteration 108/1000 | Loss: 0.00002842
Iteration 109/1000 | Loss: 0.00002842
Iteration 110/1000 | Loss: 0.00002842
Iteration 111/1000 | Loss: 0.00002842
Iteration 112/1000 | Loss: 0.00002842
Iteration 113/1000 | Loss: 0.00002842
Iteration 114/1000 | Loss: 0.00002842
Iteration 115/1000 | Loss: 0.00002842
Iteration 116/1000 | Loss: 0.00002842
Iteration 117/1000 | Loss: 0.00002842
Iteration 118/1000 | Loss: 0.00002842
Iteration 119/1000 | Loss: 0.00002842
Iteration 120/1000 | Loss: 0.00002842
Iteration 121/1000 | Loss: 0.00002841
Iteration 122/1000 | Loss: 0.00002841
Iteration 123/1000 | Loss: 0.00002841
Iteration 124/1000 | Loss: 0.00002841
Iteration 125/1000 | Loss: 0.00002841
Iteration 126/1000 | Loss: 0.00002841
Iteration 127/1000 | Loss: 0.00002841
Iteration 128/1000 | Loss: 0.00002841
Iteration 129/1000 | Loss: 0.00002841
Iteration 130/1000 | Loss: 0.00002841
Iteration 131/1000 | Loss: 0.00002841
Iteration 132/1000 | Loss: 0.00002841
Iteration 133/1000 | Loss: 0.00002841
Iteration 134/1000 | Loss: 0.00002841
Iteration 135/1000 | Loss: 0.00002841
Iteration 136/1000 | Loss: 0.00002841
Iteration 137/1000 | Loss: 0.00002841
Iteration 138/1000 | Loss: 0.00002840
Iteration 139/1000 | Loss: 0.00002840
Iteration 140/1000 | Loss: 0.00002840
Iteration 141/1000 | Loss: 0.00002840
Iteration 142/1000 | Loss: 0.00002840
Iteration 143/1000 | Loss: 0.00002840
Iteration 144/1000 | Loss: 0.00002840
Iteration 145/1000 | Loss: 0.00002840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.8404147087712772e-05, 2.8404147087712772e-05, 2.8404147087712772e-05, 2.8404147087712772e-05, 2.8404147087712772e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8404147087712772e-05

Optimization complete. Final v2v error: 4.462144374847412 mm

Highest mean error: 10.909039497375488 mm for frame 81

Lowest mean error: 4.232909202575684 mm for frame 125

Saving results

Total time: 113.12709951400757
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01138968
Iteration 2/25 | Loss: 0.01138966
Iteration 3/25 | Loss: 0.00314563
Iteration 4/25 | Loss: 0.00213626
Iteration 5/25 | Loss: 0.00201427
Iteration 6/25 | Loss: 0.00199400
Iteration 7/25 | Loss: 0.00197556
Iteration 8/25 | Loss: 0.00198952
Iteration 9/25 | Loss: 0.00205837
Iteration 10/25 | Loss: 0.00198862
Iteration 11/25 | Loss: 0.00195250
Iteration 12/25 | Loss: 0.00192999
Iteration 13/25 | Loss: 0.00192883
Iteration 14/25 | Loss: 0.00193230
Iteration 15/25 | Loss: 0.00192408
Iteration 16/25 | Loss: 0.00192031
Iteration 17/25 | Loss: 0.00191991
Iteration 18/25 | Loss: 0.00191987
Iteration 19/25 | Loss: 0.00191987
Iteration 20/25 | Loss: 0.00191987
Iteration 21/25 | Loss: 0.00191987
Iteration 22/25 | Loss: 0.00191987
Iteration 23/25 | Loss: 0.00191987
Iteration 24/25 | Loss: 0.00191986
Iteration 25/25 | Loss: 0.00191986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18163717
Iteration 2/25 | Loss: 0.00217576
Iteration 3/25 | Loss: 0.00217576
Iteration 4/25 | Loss: 0.00217576
Iteration 5/25 | Loss: 0.00217576
Iteration 6/25 | Loss: 0.00217576
Iteration 7/25 | Loss: 0.00217576
Iteration 8/25 | Loss: 0.00217576
Iteration 9/25 | Loss: 0.00217576
Iteration 10/25 | Loss: 0.00217576
Iteration 11/25 | Loss: 0.00217576
Iteration 12/25 | Loss: 0.00217576
Iteration 13/25 | Loss: 0.00217576
Iteration 14/25 | Loss: 0.00217576
Iteration 15/25 | Loss: 0.00217576
Iteration 16/25 | Loss: 0.00217576
Iteration 17/25 | Loss: 0.00217576
Iteration 18/25 | Loss: 0.00217576
Iteration 19/25 | Loss: 0.00217576
Iteration 20/25 | Loss: 0.00217576
Iteration 21/25 | Loss: 0.00217576
Iteration 22/25 | Loss: 0.00217576
Iteration 23/25 | Loss: 0.00217576
Iteration 24/25 | Loss: 0.00217576
Iteration 25/25 | Loss: 0.00217576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217576
Iteration 2/1000 | Loss: 0.00006376
Iteration 3/1000 | Loss: 0.00004239
Iteration 4/1000 | Loss: 0.00003622
Iteration 5/1000 | Loss: 0.00003361
Iteration 6/1000 | Loss: 0.00003282
Iteration 7/1000 | Loss: 0.00003212
Iteration 8/1000 | Loss: 0.00003164
Iteration 9/1000 | Loss: 0.00003132
Iteration 10/1000 | Loss: 0.00003112
Iteration 11/1000 | Loss: 0.00003104
Iteration 12/1000 | Loss: 0.00003095
Iteration 13/1000 | Loss: 0.00003089
Iteration 14/1000 | Loss: 0.00003082
Iteration 15/1000 | Loss: 0.00003081
Iteration 16/1000 | Loss: 0.00003081
Iteration 17/1000 | Loss: 0.00003081
Iteration 18/1000 | Loss: 0.00003080
Iteration 19/1000 | Loss: 0.00003080
Iteration 20/1000 | Loss: 0.00003080
Iteration 21/1000 | Loss: 0.00003080
Iteration 22/1000 | Loss: 0.00003080
Iteration 23/1000 | Loss: 0.00003080
Iteration 24/1000 | Loss: 0.00003079
Iteration 25/1000 | Loss: 0.00003079
Iteration 26/1000 | Loss: 0.00003078
Iteration 27/1000 | Loss: 0.00003072
Iteration 28/1000 | Loss: 0.00003072
Iteration 29/1000 | Loss: 0.00003072
Iteration 30/1000 | Loss: 0.00003071
Iteration 31/1000 | Loss: 0.00003071
Iteration 32/1000 | Loss: 0.00003071
Iteration 33/1000 | Loss: 0.00003071
Iteration 34/1000 | Loss: 0.00003071
Iteration 35/1000 | Loss: 0.00003071
Iteration 36/1000 | Loss: 0.00003071
Iteration 37/1000 | Loss: 0.00003071
Iteration 38/1000 | Loss: 0.00003071
Iteration 39/1000 | Loss: 0.00003071
Iteration 40/1000 | Loss: 0.00003071
Iteration 41/1000 | Loss: 0.00003070
Iteration 42/1000 | Loss: 0.00003070
Iteration 43/1000 | Loss: 0.00003069
Iteration 44/1000 | Loss: 0.00003069
Iteration 45/1000 | Loss: 0.00003069
Iteration 46/1000 | Loss: 0.00003069
Iteration 47/1000 | Loss: 0.00003069
Iteration 48/1000 | Loss: 0.00003068
Iteration 49/1000 | Loss: 0.00003068
Iteration 50/1000 | Loss: 0.00003067
Iteration 51/1000 | Loss: 0.00003066
Iteration 52/1000 | Loss: 0.00003066
Iteration 53/1000 | Loss: 0.00003066
Iteration 54/1000 | Loss: 0.00003066
Iteration 55/1000 | Loss: 0.00003066
Iteration 56/1000 | Loss: 0.00003066
Iteration 57/1000 | Loss: 0.00003066
Iteration 58/1000 | Loss: 0.00003066
Iteration 59/1000 | Loss: 0.00003066
Iteration 60/1000 | Loss: 0.00003065
Iteration 61/1000 | Loss: 0.00003065
Iteration 62/1000 | Loss: 0.00003065
Iteration 63/1000 | Loss: 0.00003065
Iteration 64/1000 | Loss: 0.00003065
Iteration 65/1000 | Loss: 0.00003065
Iteration 66/1000 | Loss: 0.00003065
Iteration 67/1000 | Loss: 0.00003064
Iteration 68/1000 | Loss: 0.00003063
Iteration 69/1000 | Loss: 0.00003063
Iteration 70/1000 | Loss: 0.00003063
Iteration 71/1000 | Loss: 0.00003062
Iteration 72/1000 | Loss: 0.00003062
Iteration 73/1000 | Loss: 0.00003059
Iteration 74/1000 | Loss: 0.00003058
Iteration 75/1000 | Loss: 0.00003058
Iteration 76/1000 | Loss: 0.00003058
Iteration 77/1000 | Loss: 0.00003058
Iteration 78/1000 | Loss: 0.00003058
Iteration 79/1000 | Loss: 0.00003058
Iteration 80/1000 | Loss: 0.00003058
Iteration 81/1000 | Loss: 0.00003058
Iteration 82/1000 | Loss: 0.00003058
Iteration 83/1000 | Loss: 0.00003058
Iteration 84/1000 | Loss: 0.00003058
Iteration 85/1000 | Loss: 0.00003058
Iteration 86/1000 | Loss: 0.00003057
Iteration 87/1000 | Loss: 0.00003057
Iteration 88/1000 | Loss: 0.00003057
Iteration 89/1000 | Loss: 0.00003056
Iteration 90/1000 | Loss: 0.00003056
Iteration 91/1000 | Loss: 0.00003056
Iteration 92/1000 | Loss: 0.00003056
Iteration 93/1000 | Loss: 0.00003056
Iteration 94/1000 | Loss: 0.00003056
Iteration 95/1000 | Loss: 0.00003055
Iteration 96/1000 | Loss: 0.00003055
Iteration 97/1000 | Loss: 0.00003055
Iteration 98/1000 | Loss: 0.00003055
Iteration 99/1000 | Loss: 0.00003055
Iteration 100/1000 | Loss: 0.00003055
Iteration 101/1000 | Loss: 0.00003055
Iteration 102/1000 | Loss: 0.00003055
Iteration 103/1000 | Loss: 0.00003055
Iteration 104/1000 | Loss: 0.00003054
Iteration 105/1000 | Loss: 0.00003054
Iteration 106/1000 | Loss: 0.00003054
Iteration 107/1000 | Loss: 0.00003054
Iteration 108/1000 | Loss: 0.00003054
Iteration 109/1000 | Loss: 0.00003054
Iteration 110/1000 | Loss: 0.00003054
Iteration 111/1000 | Loss: 0.00003054
Iteration 112/1000 | Loss: 0.00003054
Iteration 113/1000 | Loss: 0.00003054
Iteration 114/1000 | Loss: 0.00003054
Iteration 115/1000 | Loss: 0.00003054
Iteration 116/1000 | Loss: 0.00003054
Iteration 117/1000 | Loss: 0.00003054
Iteration 118/1000 | Loss: 0.00003054
Iteration 119/1000 | Loss: 0.00003054
Iteration 120/1000 | Loss: 0.00003054
Iteration 121/1000 | Loss: 0.00003054
Iteration 122/1000 | Loss: 0.00003054
Iteration 123/1000 | Loss: 0.00003054
Iteration 124/1000 | Loss: 0.00003054
Iteration 125/1000 | Loss: 0.00003054
Iteration 126/1000 | Loss: 0.00003054
Iteration 127/1000 | Loss: 0.00003054
Iteration 128/1000 | Loss: 0.00003054
Iteration 129/1000 | Loss: 0.00003054
Iteration 130/1000 | Loss: 0.00003054
Iteration 131/1000 | Loss: 0.00003054
Iteration 132/1000 | Loss: 0.00003054
Iteration 133/1000 | Loss: 0.00003054
Iteration 134/1000 | Loss: 0.00003054
Iteration 135/1000 | Loss: 0.00003054
Iteration 136/1000 | Loss: 0.00003054
Iteration 137/1000 | Loss: 0.00003054
Iteration 138/1000 | Loss: 0.00003054
Iteration 139/1000 | Loss: 0.00003054
Iteration 140/1000 | Loss: 0.00003054
Iteration 141/1000 | Loss: 0.00003054
Iteration 142/1000 | Loss: 0.00003054
Iteration 143/1000 | Loss: 0.00003054
Iteration 144/1000 | Loss: 0.00003054
Iteration 145/1000 | Loss: 0.00003054
Iteration 146/1000 | Loss: 0.00003054
Iteration 147/1000 | Loss: 0.00003054
Iteration 148/1000 | Loss: 0.00003054
Iteration 149/1000 | Loss: 0.00003054
Iteration 150/1000 | Loss: 0.00003054
Iteration 151/1000 | Loss: 0.00003054
Iteration 152/1000 | Loss: 0.00003054
Iteration 153/1000 | Loss: 0.00003054
Iteration 154/1000 | Loss: 0.00003054
Iteration 155/1000 | Loss: 0.00003054
Iteration 156/1000 | Loss: 0.00003054
Iteration 157/1000 | Loss: 0.00003054
Iteration 158/1000 | Loss: 0.00003054
Iteration 159/1000 | Loss: 0.00003054
Iteration 160/1000 | Loss: 0.00003054
Iteration 161/1000 | Loss: 0.00003054
Iteration 162/1000 | Loss: 0.00003054
Iteration 163/1000 | Loss: 0.00003054
Iteration 164/1000 | Loss: 0.00003054
Iteration 165/1000 | Loss: 0.00003054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [3.053605541936122e-05, 3.053605541936122e-05, 3.053605541936122e-05, 3.053605541936122e-05, 3.053605541936122e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.053605541936122e-05

Optimization complete. Final v2v error: 4.727519512176514 mm

Highest mean error: 4.869926929473877 mm for frame 147

Lowest mean error: 4.580165863037109 mm for frame 3

Saving results

Total time: 56.937989711761475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_53_us_2399/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_53_us_2399/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467714
Iteration 2/25 | Loss: 0.00200475
Iteration 3/25 | Loss: 0.00194105
Iteration 4/25 | Loss: 0.00192777
Iteration 5/25 | Loss: 0.00192434
Iteration 6/25 | Loss: 0.00192365
Iteration 7/25 | Loss: 0.00192365
Iteration 8/25 | Loss: 0.00192365
Iteration 9/25 | Loss: 0.00192365
Iteration 10/25 | Loss: 0.00192365
Iteration 11/25 | Loss: 0.00192365
Iteration 12/25 | Loss: 0.00192365
Iteration 13/25 | Loss: 0.00192365
Iteration 14/25 | Loss: 0.00192365
Iteration 15/25 | Loss: 0.00192365
Iteration 16/25 | Loss: 0.00192365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019236535299569368, 0.0019236535299569368, 0.0019236535299569368, 0.0019236535299569368, 0.0019236535299569368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019236535299569368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20387590
Iteration 2/25 | Loss: 0.00299322
Iteration 3/25 | Loss: 0.00299322
Iteration 4/25 | Loss: 0.00299322
Iteration 5/25 | Loss: 0.00299322
Iteration 6/25 | Loss: 0.00299322
Iteration 7/25 | Loss: 0.00299322
Iteration 8/25 | Loss: 0.00299322
Iteration 9/25 | Loss: 0.00299322
Iteration 10/25 | Loss: 0.00299322
Iteration 11/25 | Loss: 0.00299322
Iteration 12/25 | Loss: 0.00299322
Iteration 13/25 | Loss: 0.00299322
Iteration 14/25 | Loss: 0.00299322
Iteration 15/25 | Loss: 0.00299322
Iteration 16/25 | Loss: 0.00299322
Iteration 17/25 | Loss: 0.00299322
Iteration 18/25 | Loss: 0.00299322
Iteration 19/25 | Loss: 0.00299322
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0029932167381048203, 0.0029932167381048203, 0.0029932167381048203, 0.0029932167381048203, 0.0029932167381048203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029932167381048203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00299322
Iteration 2/1000 | Loss: 0.00008631
Iteration 3/1000 | Loss: 0.00005876
Iteration 4/1000 | Loss: 0.00005014
Iteration 5/1000 | Loss: 0.00004744
Iteration 6/1000 | Loss: 0.00004547
Iteration 7/1000 | Loss: 0.00004391
Iteration 8/1000 | Loss: 0.00004305
Iteration 9/1000 | Loss: 0.00004256
Iteration 10/1000 | Loss: 0.00004223
Iteration 11/1000 | Loss: 0.00004197
Iteration 12/1000 | Loss: 0.00004189
Iteration 13/1000 | Loss: 0.00004178
Iteration 14/1000 | Loss: 0.00004162
Iteration 15/1000 | Loss: 0.00004147
Iteration 16/1000 | Loss: 0.00004144
Iteration 17/1000 | Loss: 0.00004138
Iteration 18/1000 | Loss: 0.00004138
Iteration 19/1000 | Loss: 0.00004138
Iteration 20/1000 | Loss: 0.00004134
Iteration 21/1000 | Loss: 0.00004134
Iteration 22/1000 | Loss: 0.00004134
Iteration 23/1000 | Loss: 0.00004134
Iteration 24/1000 | Loss: 0.00004134
Iteration 25/1000 | Loss: 0.00004134
Iteration 26/1000 | Loss: 0.00004134
Iteration 27/1000 | Loss: 0.00004134
Iteration 28/1000 | Loss: 0.00004134
Iteration 29/1000 | Loss: 0.00004134
Iteration 30/1000 | Loss: 0.00004133
Iteration 31/1000 | Loss: 0.00004133
Iteration 32/1000 | Loss: 0.00004133
Iteration 33/1000 | Loss: 0.00004133
Iteration 34/1000 | Loss: 0.00004132
Iteration 35/1000 | Loss: 0.00004131
Iteration 36/1000 | Loss: 0.00004131
Iteration 37/1000 | Loss: 0.00004130
Iteration 38/1000 | Loss: 0.00004130
Iteration 39/1000 | Loss: 0.00004130
Iteration 40/1000 | Loss: 0.00004130
Iteration 41/1000 | Loss: 0.00004130
Iteration 42/1000 | Loss: 0.00004130
Iteration 43/1000 | Loss: 0.00004130
Iteration 44/1000 | Loss: 0.00004130
Iteration 45/1000 | Loss: 0.00004129
Iteration 46/1000 | Loss: 0.00004129
Iteration 47/1000 | Loss: 0.00004129
Iteration 48/1000 | Loss: 0.00004129
Iteration 49/1000 | Loss: 0.00004128
Iteration 50/1000 | Loss: 0.00004128
Iteration 51/1000 | Loss: 0.00004128
Iteration 52/1000 | Loss: 0.00004127
Iteration 53/1000 | Loss: 0.00004127
Iteration 54/1000 | Loss: 0.00004127
Iteration 55/1000 | Loss: 0.00004126
Iteration 56/1000 | Loss: 0.00004126
Iteration 57/1000 | Loss: 0.00004126
Iteration 58/1000 | Loss: 0.00004125
Iteration 59/1000 | Loss: 0.00004125
Iteration 60/1000 | Loss: 0.00004125
Iteration 61/1000 | Loss: 0.00004124
Iteration 62/1000 | Loss: 0.00004123
Iteration 63/1000 | Loss: 0.00004123
Iteration 64/1000 | Loss: 0.00004123
Iteration 65/1000 | Loss: 0.00004123
Iteration 66/1000 | Loss: 0.00004123
Iteration 67/1000 | Loss: 0.00004123
Iteration 68/1000 | Loss: 0.00004123
Iteration 69/1000 | Loss: 0.00004123
Iteration 70/1000 | Loss: 0.00004122
Iteration 71/1000 | Loss: 0.00004122
Iteration 72/1000 | Loss: 0.00004122
Iteration 73/1000 | Loss: 0.00004122
Iteration 74/1000 | Loss: 0.00004122
Iteration 75/1000 | Loss: 0.00004122
Iteration 76/1000 | Loss: 0.00004122
Iteration 77/1000 | Loss: 0.00004122
Iteration 78/1000 | Loss: 0.00004122
Iteration 79/1000 | Loss: 0.00004122
Iteration 80/1000 | Loss: 0.00004122
Iteration 81/1000 | Loss: 0.00004122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [4.122101017856039e-05, 4.122101017856039e-05, 4.122101017856039e-05, 4.122101017856039e-05, 4.122101017856039e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.122101017856039e-05

Optimization complete. Final v2v error: 5.3396382331848145 mm

Highest mean error: 5.72074031829834 mm for frame 141

Lowest mean error: 5.146175861358643 mm for frame 32

Saving results

Total time: 33.28662300109863
