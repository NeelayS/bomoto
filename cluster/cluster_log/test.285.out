Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=285, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15960-16015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395439
Iteration 2/25 | Loss: 0.00090426
Iteration 3/25 | Loss: 0.00080514
Iteration 4/25 | Loss: 0.00079720
Iteration 5/25 | Loss: 0.00079538
Iteration 6/25 | Loss: 0.00079480
Iteration 7/25 | Loss: 0.00079480
Iteration 8/25 | Loss: 0.00079480
Iteration 9/25 | Loss: 0.00079480
Iteration 10/25 | Loss: 0.00079480
Iteration 11/25 | Loss: 0.00079480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007947979029268026, 0.0007947979029268026, 0.0007947979029268026, 0.0007947979029268026, 0.0007947979029268026]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007947979029268026

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40038669
Iteration 2/25 | Loss: 0.00047725
Iteration 3/25 | Loss: 0.00047724
Iteration 4/25 | Loss: 0.00047724
Iteration 5/25 | Loss: 0.00047724
Iteration 6/25 | Loss: 0.00047724
Iteration 7/25 | Loss: 0.00047724
Iteration 8/25 | Loss: 0.00047724
Iteration 9/25 | Loss: 0.00047724
Iteration 10/25 | Loss: 0.00047724
Iteration 11/25 | Loss: 0.00047724
Iteration 12/25 | Loss: 0.00047724
Iteration 13/25 | Loss: 0.00047724
Iteration 14/25 | Loss: 0.00047724
Iteration 15/25 | Loss: 0.00047724
Iteration 16/25 | Loss: 0.00047724
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004772398970089853, 0.0004772398970089853, 0.0004772398970089853, 0.0004772398970089853, 0.0004772398970089853]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004772398970089853

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047724
Iteration 2/1000 | Loss: 0.00001604
Iteration 3/1000 | Loss: 0.00000974
Iteration 4/1000 | Loss: 0.00000834
Iteration 5/1000 | Loss: 0.00000776
Iteration 6/1000 | Loss: 0.00000743
Iteration 7/1000 | Loss: 0.00000721
Iteration 8/1000 | Loss: 0.00000710
Iteration 9/1000 | Loss: 0.00000709
Iteration 10/1000 | Loss: 0.00000708
Iteration 11/1000 | Loss: 0.00000708
Iteration 12/1000 | Loss: 0.00000707
Iteration 13/1000 | Loss: 0.00000707
Iteration 14/1000 | Loss: 0.00000706
Iteration 15/1000 | Loss: 0.00000706
Iteration 16/1000 | Loss: 0.00000704
Iteration 17/1000 | Loss: 0.00000703
Iteration 18/1000 | Loss: 0.00000703
Iteration 19/1000 | Loss: 0.00000703
Iteration 20/1000 | Loss: 0.00000703
Iteration 21/1000 | Loss: 0.00000702
Iteration 22/1000 | Loss: 0.00000702
Iteration 23/1000 | Loss: 0.00000702
Iteration 24/1000 | Loss: 0.00000701
Iteration 25/1000 | Loss: 0.00000701
Iteration 26/1000 | Loss: 0.00000700
Iteration 27/1000 | Loss: 0.00000699
Iteration 28/1000 | Loss: 0.00000699
Iteration 29/1000 | Loss: 0.00000699
Iteration 30/1000 | Loss: 0.00000699
Iteration 31/1000 | Loss: 0.00000699
Iteration 32/1000 | Loss: 0.00000699
Iteration 33/1000 | Loss: 0.00000699
Iteration 34/1000 | Loss: 0.00000698
Iteration 35/1000 | Loss: 0.00000698
Iteration 36/1000 | Loss: 0.00000698
Iteration 37/1000 | Loss: 0.00000698
Iteration 38/1000 | Loss: 0.00000693
Iteration 39/1000 | Loss: 0.00000691
Iteration 40/1000 | Loss: 0.00000690
Iteration 41/1000 | Loss: 0.00000689
Iteration 42/1000 | Loss: 0.00000689
Iteration 43/1000 | Loss: 0.00000688
Iteration 44/1000 | Loss: 0.00000688
Iteration 45/1000 | Loss: 0.00000688
Iteration 46/1000 | Loss: 0.00000688
Iteration 47/1000 | Loss: 0.00000688
Iteration 48/1000 | Loss: 0.00000688
Iteration 49/1000 | Loss: 0.00000687
Iteration 50/1000 | Loss: 0.00000687
Iteration 51/1000 | Loss: 0.00000687
Iteration 52/1000 | Loss: 0.00000686
Iteration 53/1000 | Loss: 0.00000686
Iteration 54/1000 | Loss: 0.00000686
Iteration 55/1000 | Loss: 0.00000686
Iteration 56/1000 | Loss: 0.00000685
Iteration 57/1000 | Loss: 0.00000685
Iteration 58/1000 | Loss: 0.00000685
Iteration 59/1000 | Loss: 0.00000685
Iteration 60/1000 | Loss: 0.00000684
Iteration 61/1000 | Loss: 0.00000684
Iteration 62/1000 | Loss: 0.00000684
Iteration 63/1000 | Loss: 0.00000684
Iteration 64/1000 | Loss: 0.00000683
Iteration 65/1000 | Loss: 0.00000683
Iteration 66/1000 | Loss: 0.00000682
Iteration 67/1000 | Loss: 0.00000682
Iteration 68/1000 | Loss: 0.00000682
Iteration 69/1000 | Loss: 0.00000682
Iteration 70/1000 | Loss: 0.00000681
Iteration 71/1000 | Loss: 0.00000681
Iteration 72/1000 | Loss: 0.00000681
Iteration 73/1000 | Loss: 0.00000681
Iteration 74/1000 | Loss: 0.00000680
Iteration 75/1000 | Loss: 0.00000680
Iteration 76/1000 | Loss: 0.00000679
Iteration 77/1000 | Loss: 0.00000679
Iteration 78/1000 | Loss: 0.00000678
Iteration 79/1000 | Loss: 0.00000678
Iteration 80/1000 | Loss: 0.00000678
Iteration 81/1000 | Loss: 0.00000677
Iteration 82/1000 | Loss: 0.00000677
Iteration 83/1000 | Loss: 0.00000674
Iteration 84/1000 | Loss: 0.00000674
Iteration 85/1000 | Loss: 0.00000674
Iteration 86/1000 | Loss: 0.00000674
Iteration 87/1000 | Loss: 0.00000674
Iteration 88/1000 | Loss: 0.00000673
Iteration 89/1000 | Loss: 0.00000673
Iteration 90/1000 | Loss: 0.00000673
Iteration 91/1000 | Loss: 0.00000673
Iteration 92/1000 | Loss: 0.00000672
Iteration 93/1000 | Loss: 0.00000672
Iteration 94/1000 | Loss: 0.00000672
Iteration 95/1000 | Loss: 0.00000672
Iteration 96/1000 | Loss: 0.00000672
Iteration 97/1000 | Loss: 0.00000672
Iteration 98/1000 | Loss: 0.00000672
Iteration 99/1000 | Loss: 0.00000672
Iteration 100/1000 | Loss: 0.00000672
Iteration 101/1000 | Loss: 0.00000671
Iteration 102/1000 | Loss: 0.00000671
Iteration 103/1000 | Loss: 0.00000671
Iteration 104/1000 | Loss: 0.00000671
Iteration 105/1000 | Loss: 0.00000670
Iteration 106/1000 | Loss: 0.00000670
Iteration 107/1000 | Loss: 0.00000670
Iteration 108/1000 | Loss: 0.00000670
Iteration 109/1000 | Loss: 0.00000670
Iteration 110/1000 | Loss: 0.00000670
Iteration 111/1000 | Loss: 0.00000670
Iteration 112/1000 | Loss: 0.00000670
Iteration 113/1000 | Loss: 0.00000670
Iteration 114/1000 | Loss: 0.00000670
Iteration 115/1000 | Loss: 0.00000669
Iteration 116/1000 | Loss: 0.00000669
Iteration 117/1000 | Loss: 0.00000669
Iteration 118/1000 | Loss: 0.00000669
Iteration 119/1000 | Loss: 0.00000669
Iteration 120/1000 | Loss: 0.00000669
Iteration 121/1000 | Loss: 0.00000669
Iteration 122/1000 | Loss: 0.00000668
Iteration 123/1000 | Loss: 0.00000668
Iteration 124/1000 | Loss: 0.00000668
Iteration 125/1000 | Loss: 0.00000668
Iteration 126/1000 | Loss: 0.00000668
Iteration 127/1000 | Loss: 0.00000668
Iteration 128/1000 | Loss: 0.00000668
Iteration 129/1000 | Loss: 0.00000668
Iteration 130/1000 | Loss: 0.00000668
Iteration 131/1000 | Loss: 0.00000668
Iteration 132/1000 | Loss: 0.00000668
Iteration 133/1000 | Loss: 0.00000668
Iteration 134/1000 | Loss: 0.00000668
Iteration 135/1000 | Loss: 0.00000668
Iteration 136/1000 | Loss: 0.00000668
Iteration 137/1000 | Loss: 0.00000667
Iteration 138/1000 | Loss: 0.00000667
Iteration 139/1000 | Loss: 0.00000667
Iteration 140/1000 | Loss: 0.00000667
Iteration 141/1000 | Loss: 0.00000667
Iteration 142/1000 | Loss: 0.00000667
Iteration 143/1000 | Loss: 0.00000667
Iteration 144/1000 | Loss: 0.00000667
Iteration 145/1000 | Loss: 0.00000667
Iteration 146/1000 | Loss: 0.00000667
Iteration 147/1000 | Loss: 0.00000667
Iteration 148/1000 | Loss: 0.00000667
Iteration 149/1000 | Loss: 0.00000666
Iteration 150/1000 | Loss: 0.00000666
Iteration 151/1000 | Loss: 0.00000666
Iteration 152/1000 | Loss: 0.00000666
Iteration 153/1000 | Loss: 0.00000666
Iteration 154/1000 | Loss: 0.00000666
Iteration 155/1000 | Loss: 0.00000666
Iteration 156/1000 | Loss: 0.00000666
Iteration 157/1000 | Loss: 0.00000666
Iteration 158/1000 | Loss: 0.00000666
Iteration 159/1000 | Loss: 0.00000665
Iteration 160/1000 | Loss: 0.00000665
Iteration 161/1000 | Loss: 0.00000665
Iteration 162/1000 | Loss: 0.00000665
Iteration 163/1000 | Loss: 0.00000665
Iteration 164/1000 | Loss: 0.00000665
Iteration 165/1000 | Loss: 0.00000665
Iteration 166/1000 | Loss: 0.00000665
Iteration 167/1000 | Loss: 0.00000664
Iteration 168/1000 | Loss: 0.00000664
Iteration 169/1000 | Loss: 0.00000664
Iteration 170/1000 | Loss: 0.00000664
Iteration 171/1000 | Loss: 0.00000664
Iteration 172/1000 | Loss: 0.00000664
Iteration 173/1000 | Loss: 0.00000664
Iteration 174/1000 | Loss: 0.00000664
Iteration 175/1000 | Loss: 0.00000664
Iteration 176/1000 | Loss: 0.00000664
Iteration 177/1000 | Loss: 0.00000664
Iteration 178/1000 | Loss: 0.00000664
Iteration 179/1000 | Loss: 0.00000664
Iteration 180/1000 | Loss: 0.00000664
Iteration 181/1000 | Loss: 0.00000663
Iteration 182/1000 | Loss: 0.00000663
Iteration 183/1000 | Loss: 0.00000663
Iteration 184/1000 | Loss: 0.00000663
Iteration 185/1000 | Loss: 0.00000663
Iteration 186/1000 | Loss: 0.00000663
Iteration 187/1000 | Loss: 0.00000663
Iteration 188/1000 | Loss: 0.00000663
Iteration 189/1000 | Loss: 0.00000663
Iteration 190/1000 | Loss: 0.00000663
Iteration 191/1000 | Loss: 0.00000663
Iteration 192/1000 | Loss: 0.00000663
Iteration 193/1000 | Loss: 0.00000663
Iteration 194/1000 | Loss: 0.00000663
Iteration 195/1000 | Loss: 0.00000662
Iteration 196/1000 | Loss: 0.00000662
Iteration 197/1000 | Loss: 0.00000662
Iteration 198/1000 | Loss: 0.00000662
Iteration 199/1000 | Loss: 0.00000662
Iteration 200/1000 | Loss: 0.00000662
Iteration 201/1000 | Loss: 0.00000662
Iteration 202/1000 | Loss: 0.00000662
Iteration 203/1000 | Loss: 0.00000662
Iteration 204/1000 | Loss: 0.00000662
Iteration 205/1000 | Loss: 0.00000662
Iteration 206/1000 | Loss: 0.00000662
Iteration 207/1000 | Loss: 0.00000662
Iteration 208/1000 | Loss: 0.00000662
Iteration 209/1000 | Loss: 0.00000662
Iteration 210/1000 | Loss: 0.00000662
Iteration 211/1000 | Loss: 0.00000662
Iteration 212/1000 | Loss: 0.00000662
Iteration 213/1000 | Loss: 0.00000662
Iteration 214/1000 | Loss: 0.00000662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [6.623482022405369e-06, 6.623482022405369e-06, 6.623482022405369e-06, 6.623482022405369e-06, 6.623482022405369e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.623482022405369e-06

Optimization complete. Final v2v error: 2.1729447841644287 mm

Highest mean error: 3.192983627319336 mm for frame 54

Lowest mean error: 2.002472400665283 mm for frame 31

Saving results

Total time: 35.23664975166321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411577
Iteration 2/25 | Loss: 0.00112681
Iteration 3/25 | Loss: 0.00094761
Iteration 4/25 | Loss: 0.00092417
Iteration 5/25 | Loss: 0.00091829
Iteration 6/25 | Loss: 0.00091647
Iteration 7/25 | Loss: 0.00091584
Iteration 8/25 | Loss: 0.00091580
Iteration 9/25 | Loss: 0.00091580
Iteration 10/25 | Loss: 0.00091580
Iteration 11/25 | Loss: 0.00091580
Iteration 12/25 | Loss: 0.00091580
Iteration 13/25 | Loss: 0.00091580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009157981839962304, 0.0009157981839962304, 0.0009157981839962304, 0.0009157981839962304, 0.0009157981839962304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009157981839962304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.91762877
Iteration 2/25 | Loss: 0.00046127
Iteration 3/25 | Loss: 0.00046125
Iteration 4/25 | Loss: 0.00046125
Iteration 5/25 | Loss: 0.00046124
Iteration 6/25 | Loss: 0.00046124
Iteration 7/25 | Loss: 0.00046124
Iteration 8/25 | Loss: 0.00046124
Iteration 9/25 | Loss: 0.00046124
Iteration 10/25 | Loss: 0.00046124
Iteration 11/25 | Loss: 0.00046124
Iteration 12/25 | Loss: 0.00046124
Iteration 13/25 | Loss: 0.00046124
Iteration 14/25 | Loss: 0.00046124
Iteration 15/25 | Loss: 0.00046124
Iteration 16/25 | Loss: 0.00046124
Iteration 17/25 | Loss: 0.00046124
Iteration 18/25 | Loss: 0.00046124
Iteration 19/25 | Loss: 0.00046124
Iteration 20/25 | Loss: 0.00046124
Iteration 21/25 | Loss: 0.00046124
Iteration 22/25 | Loss: 0.00046124
Iteration 23/25 | Loss: 0.00046124
Iteration 24/25 | Loss: 0.00046124
Iteration 25/25 | Loss: 0.00046124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046124
Iteration 2/1000 | Loss: 0.00003758
Iteration 3/1000 | Loss: 0.00002138
Iteration 4/1000 | Loss: 0.00001944
Iteration 5/1000 | Loss: 0.00001846
Iteration 6/1000 | Loss: 0.00001797
Iteration 7/1000 | Loss: 0.00001753
Iteration 8/1000 | Loss: 0.00001720
Iteration 9/1000 | Loss: 0.00001698
Iteration 10/1000 | Loss: 0.00001682
Iteration 11/1000 | Loss: 0.00001680
Iteration 12/1000 | Loss: 0.00001666
Iteration 13/1000 | Loss: 0.00001665
Iteration 14/1000 | Loss: 0.00001663
Iteration 15/1000 | Loss: 0.00001661
Iteration 16/1000 | Loss: 0.00001660
Iteration 17/1000 | Loss: 0.00001660
Iteration 18/1000 | Loss: 0.00001659
Iteration 19/1000 | Loss: 0.00001654
Iteration 20/1000 | Loss: 0.00001654
Iteration 21/1000 | Loss: 0.00001650
Iteration 22/1000 | Loss: 0.00001650
Iteration 23/1000 | Loss: 0.00001650
Iteration 24/1000 | Loss: 0.00001650
Iteration 25/1000 | Loss: 0.00001650
Iteration 26/1000 | Loss: 0.00001650
Iteration 27/1000 | Loss: 0.00001649
Iteration 28/1000 | Loss: 0.00001647
Iteration 29/1000 | Loss: 0.00001647
Iteration 30/1000 | Loss: 0.00001647
Iteration 31/1000 | Loss: 0.00001647
Iteration 32/1000 | Loss: 0.00001646
Iteration 33/1000 | Loss: 0.00001646
Iteration 34/1000 | Loss: 0.00001646
Iteration 35/1000 | Loss: 0.00001646
Iteration 36/1000 | Loss: 0.00001646
Iteration 37/1000 | Loss: 0.00001646
Iteration 38/1000 | Loss: 0.00001646
Iteration 39/1000 | Loss: 0.00001646
Iteration 40/1000 | Loss: 0.00001646
Iteration 41/1000 | Loss: 0.00001645
Iteration 42/1000 | Loss: 0.00001645
Iteration 43/1000 | Loss: 0.00001644
Iteration 44/1000 | Loss: 0.00001643
Iteration 45/1000 | Loss: 0.00001643
Iteration 46/1000 | Loss: 0.00001643
Iteration 47/1000 | Loss: 0.00001642
Iteration 48/1000 | Loss: 0.00001642
Iteration 49/1000 | Loss: 0.00001642
Iteration 50/1000 | Loss: 0.00001642
Iteration 51/1000 | Loss: 0.00001642
Iteration 52/1000 | Loss: 0.00001642
Iteration 53/1000 | Loss: 0.00001641
Iteration 54/1000 | Loss: 0.00001641
Iteration 55/1000 | Loss: 0.00001641
Iteration 56/1000 | Loss: 0.00001640
Iteration 57/1000 | Loss: 0.00001640
Iteration 58/1000 | Loss: 0.00001640
Iteration 59/1000 | Loss: 0.00001640
Iteration 60/1000 | Loss: 0.00001640
Iteration 61/1000 | Loss: 0.00001639
Iteration 62/1000 | Loss: 0.00001639
Iteration 63/1000 | Loss: 0.00001639
Iteration 64/1000 | Loss: 0.00001639
Iteration 65/1000 | Loss: 0.00001639
Iteration 66/1000 | Loss: 0.00001638
Iteration 67/1000 | Loss: 0.00001638
Iteration 68/1000 | Loss: 0.00001638
Iteration 69/1000 | Loss: 0.00001637
Iteration 70/1000 | Loss: 0.00001637
Iteration 71/1000 | Loss: 0.00001637
Iteration 72/1000 | Loss: 0.00001637
Iteration 73/1000 | Loss: 0.00001637
Iteration 74/1000 | Loss: 0.00001637
Iteration 75/1000 | Loss: 0.00001637
Iteration 76/1000 | Loss: 0.00001637
Iteration 77/1000 | Loss: 0.00001637
Iteration 78/1000 | Loss: 0.00001637
Iteration 79/1000 | Loss: 0.00001637
Iteration 80/1000 | Loss: 0.00001637
Iteration 81/1000 | Loss: 0.00001637
Iteration 82/1000 | Loss: 0.00001636
Iteration 83/1000 | Loss: 0.00001636
Iteration 84/1000 | Loss: 0.00001636
Iteration 85/1000 | Loss: 0.00001636
Iteration 86/1000 | Loss: 0.00001636
Iteration 87/1000 | Loss: 0.00001635
Iteration 88/1000 | Loss: 0.00001635
Iteration 89/1000 | Loss: 0.00001635
Iteration 90/1000 | Loss: 0.00001635
Iteration 91/1000 | Loss: 0.00001635
Iteration 92/1000 | Loss: 0.00001635
Iteration 93/1000 | Loss: 0.00001635
Iteration 94/1000 | Loss: 0.00001635
Iteration 95/1000 | Loss: 0.00001635
Iteration 96/1000 | Loss: 0.00001635
Iteration 97/1000 | Loss: 0.00001634
Iteration 98/1000 | Loss: 0.00001634
Iteration 99/1000 | Loss: 0.00001634
Iteration 100/1000 | Loss: 0.00001634
Iteration 101/1000 | Loss: 0.00001634
Iteration 102/1000 | Loss: 0.00001634
Iteration 103/1000 | Loss: 0.00001633
Iteration 104/1000 | Loss: 0.00001633
Iteration 105/1000 | Loss: 0.00001633
Iteration 106/1000 | Loss: 0.00001633
Iteration 107/1000 | Loss: 0.00001633
Iteration 108/1000 | Loss: 0.00001632
Iteration 109/1000 | Loss: 0.00001632
Iteration 110/1000 | Loss: 0.00001632
Iteration 111/1000 | Loss: 0.00001632
Iteration 112/1000 | Loss: 0.00001632
Iteration 113/1000 | Loss: 0.00001632
Iteration 114/1000 | Loss: 0.00001632
Iteration 115/1000 | Loss: 0.00001632
Iteration 116/1000 | Loss: 0.00001632
Iteration 117/1000 | Loss: 0.00001632
Iteration 118/1000 | Loss: 0.00001632
Iteration 119/1000 | Loss: 0.00001632
Iteration 120/1000 | Loss: 0.00001632
Iteration 121/1000 | Loss: 0.00001631
Iteration 122/1000 | Loss: 0.00001631
Iteration 123/1000 | Loss: 0.00001631
Iteration 124/1000 | Loss: 0.00001631
Iteration 125/1000 | Loss: 0.00001631
Iteration 126/1000 | Loss: 0.00001631
Iteration 127/1000 | Loss: 0.00001631
Iteration 128/1000 | Loss: 0.00001630
Iteration 129/1000 | Loss: 0.00001630
Iteration 130/1000 | Loss: 0.00001630
Iteration 131/1000 | Loss: 0.00001630
Iteration 132/1000 | Loss: 0.00001630
Iteration 133/1000 | Loss: 0.00001630
Iteration 134/1000 | Loss: 0.00001630
Iteration 135/1000 | Loss: 0.00001630
Iteration 136/1000 | Loss: 0.00001630
Iteration 137/1000 | Loss: 0.00001630
Iteration 138/1000 | Loss: 0.00001630
Iteration 139/1000 | Loss: 0.00001630
Iteration 140/1000 | Loss: 0.00001630
Iteration 141/1000 | Loss: 0.00001630
Iteration 142/1000 | Loss: 0.00001630
Iteration 143/1000 | Loss: 0.00001630
Iteration 144/1000 | Loss: 0.00001630
Iteration 145/1000 | Loss: 0.00001630
Iteration 146/1000 | Loss: 0.00001630
Iteration 147/1000 | Loss: 0.00001630
Iteration 148/1000 | Loss: 0.00001630
Iteration 149/1000 | Loss: 0.00001630
Iteration 150/1000 | Loss: 0.00001630
Iteration 151/1000 | Loss: 0.00001630
Iteration 152/1000 | Loss: 0.00001630
Iteration 153/1000 | Loss: 0.00001630
Iteration 154/1000 | Loss: 0.00001630
Iteration 155/1000 | Loss: 0.00001630
Iteration 156/1000 | Loss: 0.00001630
Iteration 157/1000 | Loss: 0.00001630
Iteration 158/1000 | Loss: 0.00001630
Iteration 159/1000 | Loss: 0.00001630
Iteration 160/1000 | Loss: 0.00001630
Iteration 161/1000 | Loss: 0.00001630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.6300742572639138e-05, 1.6300742572639138e-05, 1.6300742572639138e-05, 1.6300742572639138e-05, 1.6300742572639138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6300742572639138e-05

Optimization complete. Final v2v error: 3.4001195430755615 mm

Highest mean error: 4.080445289611816 mm for frame 49

Lowest mean error: 3.0827934741973877 mm for frame 1

Saving results

Total time: 36.48018956184387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861267
Iteration 2/25 | Loss: 0.00136322
Iteration 3/25 | Loss: 0.00100819
Iteration 4/25 | Loss: 0.00097685
Iteration 5/25 | Loss: 0.00097103
Iteration 6/25 | Loss: 0.00096895
Iteration 7/25 | Loss: 0.00096884
Iteration 8/25 | Loss: 0.00096884
Iteration 9/25 | Loss: 0.00096884
Iteration 10/25 | Loss: 0.00096884
Iteration 11/25 | Loss: 0.00096884
Iteration 12/25 | Loss: 0.00096884
Iteration 13/25 | Loss: 0.00096884
Iteration 14/25 | Loss: 0.00096884
Iteration 15/25 | Loss: 0.00096884
Iteration 16/25 | Loss: 0.00096884
Iteration 17/25 | Loss: 0.00096884
Iteration 18/25 | Loss: 0.00096884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000968844979070127, 0.000968844979070127, 0.000968844979070127, 0.000968844979070127, 0.000968844979070127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000968844979070127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40978754
Iteration 2/25 | Loss: 0.00047940
Iteration 3/25 | Loss: 0.00047939
Iteration 4/25 | Loss: 0.00047939
Iteration 5/25 | Loss: 0.00047939
Iteration 6/25 | Loss: 0.00047939
Iteration 7/25 | Loss: 0.00047939
Iteration 8/25 | Loss: 0.00047939
Iteration 9/25 | Loss: 0.00047939
Iteration 10/25 | Loss: 0.00047939
Iteration 11/25 | Loss: 0.00047939
Iteration 12/25 | Loss: 0.00047939
Iteration 13/25 | Loss: 0.00047939
Iteration 14/25 | Loss: 0.00047939
Iteration 15/25 | Loss: 0.00047939
Iteration 16/25 | Loss: 0.00047939
Iteration 17/25 | Loss: 0.00047939
Iteration 18/25 | Loss: 0.00047939
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00047938968054950237, 0.00047938968054950237, 0.00047938968054950237, 0.00047938968054950237, 0.00047938968054950237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00047938968054950237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047939
Iteration 2/1000 | Loss: 0.00004840
Iteration 3/1000 | Loss: 0.00003056
Iteration 4/1000 | Loss: 0.00002516
Iteration 5/1000 | Loss: 0.00002364
Iteration 6/1000 | Loss: 0.00002287
Iteration 7/1000 | Loss: 0.00002230
Iteration 8/1000 | Loss: 0.00002179
Iteration 9/1000 | Loss: 0.00002141
Iteration 10/1000 | Loss: 0.00002115
Iteration 11/1000 | Loss: 0.00002090
Iteration 12/1000 | Loss: 0.00002074
Iteration 13/1000 | Loss: 0.00002060
Iteration 14/1000 | Loss: 0.00002052
Iteration 15/1000 | Loss: 0.00002043
Iteration 16/1000 | Loss: 0.00002038
Iteration 17/1000 | Loss: 0.00002035
Iteration 18/1000 | Loss: 0.00002034
Iteration 19/1000 | Loss: 0.00002034
Iteration 20/1000 | Loss: 0.00002033
Iteration 21/1000 | Loss: 0.00002033
Iteration 22/1000 | Loss: 0.00002031
Iteration 23/1000 | Loss: 0.00002031
Iteration 24/1000 | Loss: 0.00002031
Iteration 25/1000 | Loss: 0.00002031
Iteration 26/1000 | Loss: 0.00002031
Iteration 27/1000 | Loss: 0.00002031
Iteration 28/1000 | Loss: 0.00002030
Iteration 29/1000 | Loss: 0.00002030
Iteration 30/1000 | Loss: 0.00002030
Iteration 31/1000 | Loss: 0.00002030
Iteration 32/1000 | Loss: 0.00002030
Iteration 33/1000 | Loss: 0.00002030
Iteration 34/1000 | Loss: 0.00002028
Iteration 35/1000 | Loss: 0.00002028
Iteration 36/1000 | Loss: 0.00002027
Iteration 37/1000 | Loss: 0.00002027
Iteration 38/1000 | Loss: 0.00002027
Iteration 39/1000 | Loss: 0.00002026
Iteration 40/1000 | Loss: 0.00002025
Iteration 41/1000 | Loss: 0.00002023
Iteration 42/1000 | Loss: 0.00002023
Iteration 43/1000 | Loss: 0.00002023
Iteration 44/1000 | Loss: 0.00002022
Iteration 45/1000 | Loss: 0.00002021
Iteration 46/1000 | Loss: 0.00002021
Iteration 47/1000 | Loss: 0.00002021
Iteration 48/1000 | Loss: 0.00002020
Iteration 49/1000 | Loss: 0.00002019
Iteration 50/1000 | Loss: 0.00002019
Iteration 51/1000 | Loss: 0.00002019
Iteration 52/1000 | Loss: 0.00002018
Iteration 53/1000 | Loss: 0.00002018
Iteration 54/1000 | Loss: 0.00002018
Iteration 55/1000 | Loss: 0.00002017
Iteration 56/1000 | Loss: 0.00002017
Iteration 57/1000 | Loss: 0.00002017
Iteration 58/1000 | Loss: 0.00002016
Iteration 59/1000 | Loss: 0.00002016
Iteration 60/1000 | Loss: 0.00002016
Iteration 61/1000 | Loss: 0.00002016
Iteration 62/1000 | Loss: 0.00002016
Iteration 63/1000 | Loss: 0.00002016
Iteration 64/1000 | Loss: 0.00002015
Iteration 65/1000 | Loss: 0.00002015
Iteration 66/1000 | Loss: 0.00002015
Iteration 67/1000 | Loss: 0.00002015
Iteration 68/1000 | Loss: 0.00002015
Iteration 69/1000 | Loss: 0.00002015
Iteration 70/1000 | Loss: 0.00002015
Iteration 71/1000 | Loss: 0.00002014
Iteration 72/1000 | Loss: 0.00002014
Iteration 73/1000 | Loss: 0.00002014
Iteration 74/1000 | Loss: 0.00002013
Iteration 75/1000 | Loss: 0.00002013
Iteration 76/1000 | Loss: 0.00002013
Iteration 77/1000 | Loss: 0.00002013
Iteration 78/1000 | Loss: 0.00002012
Iteration 79/1000 | Loss: 0.00002012
Iteration 80/1000 | Loss: 0.00002012
Iteration 81/1000 | Loss: 0.00002012
Iteration 82/1000 | Loss: 0.00002012
Iteration 83/1000 | Loss: 0.00002011
Iteration 84/1000 | Loss: 0.00002011
Iteration 85/1000 | Loss: 0.00002011
Iteration 86/1000 | Loss: 0.00002011
Iteration 87/1000 | Loss: 0.00002011
Iteration 88/1000 | Loss: 0.00002010
Iteration 89/1000 | Loss: 0.00002010
Iteration 90/1000 | Loss: 0.00002010
Iteration 91/1000 | Loss: 0.00002010
Iteration 92/1000 | Loss: 0.00002010
Iteration 93/1000 | Loss: 0.00002010
Iteration 94/1000 | Loss: 0.00002010
Iteration 95/1000 | Loss: 0.00002010
Iteration 96/1000 | Loss: 0.00002009
Iteration 97/1000 | Loss: 0.00002009
Iteration 98/1000 | Loss: 0.00002009
Iteration 99/1000 | Loss: 0.00002009
Iteration 100/1000 | Loss: 0.00002009
Iteration 101/1000 | Loss: 0.00002009
Iteration 102/1000 | Loss: 0.00002009
Iteration 103/1000 | Loss: 0.00002008
Iteration 104/1000 | Loss: 0.00002008
Iteration 105/1000 | Loss: 0.00002008
Iteration 106/1000 | Loss: 0.00002008
Iteration 107/1000 | Loss: 0.00002008
Iteration 108/1000 | Loss: 0.00002008
Iteration 109/1000 | Loss: 0.00002008
Iteration 110/1000 | Loss: 0.00002008
Iteration 111/1000 | Loss: 0.00002008
Iteration 112/1000 | Loss: 0.00002008
Iteration 113/1000 | Loss: 0.00002007
Iteration 114/1000 | Loss: 0.00002007
Iteration 115/1000 | Loss: 0.00002007
Iteration 116/1000 | Loss: 0.00002007
Iteration 117/1000 | Loss: 0.00002007
Iteration 118/1000 | Loss: 0.00002007
Iteration 119/1000 | Loss: 0.00002007
Iteration 120/1000 | Loss: 0.00002006
Iteration 121/1000 | Loss: 0.00002006
Iteration 122/1000 | Loss: 0.00002006
Iteration 123/1000 | Loss: 0.00002006
Iteration 124/1000 | Loss: 0.00002006
Iteration 125/1000 | Loss: 0.00002006
Iteration 126/1000 | Loss: 0.00002006
Iteration 127/1000 | Loss: 0.00002006
Iteration 128/1000 | Loss: 0.00002006
Iteration 129/1000 | Loss: 0.00002005
Iteration 130/1000 | Loss: 0.00002005
Iteration 131/1000 | Loss: 0.00002005
Iteration 132/1000 | Loss: 0.00002005
Iteration 133/1000 | Loss: 0.00002005
Iteration 134/1000 | Loss: 0.00002005
Iteration 135/1000 | Loss: 0.00002005
Iteration 136/1000 | Loss: 0.00002005
Iteration 137/1000 | Loss: 0.00002005
Iteration 138/1000 | Loss: 0.00002005
Iteration 139/1000 | Loss: 0.00002005
Iteration 140/1000 | Loss: 0.00002004
Iteration 141/1000 | Loss: 0.00002004
Iteration 142/1000 | Loss: 0.00002004
Iteration 143/1000 | Loss: 0.00002004
Iteration 144/1000 | Loss: 0.00002004
Iteration 145/1000 | Loss: 0.00002004
Iteration 146/1000 | Loss: 0.00002004
Iteration 147/1000 | Loss: 0.00002004
Iteration 148/1000 | Loss: 0.00002004
Iteration 149/1000 | Loss: 0.00002004
Iteration 150/1000 | Loss: 0.00002004
Iteration 151/1000 | Loss: 0.00002004
Iteration 152/1000 | Loss: 0.00002004
Iteration 153/1000 | Loss: 0.00002004
Iteration 154/1000 | Loss: 0.00002003
Iteration 155/1000 | Loss: 0.00002003
Iteration 156/1000 | Loss: 0.00002003
Iteration 157/1000 | Loss: 0.00002003
Iteration 158/1000 | Loss: 0.00002003
Iteration 159/1000 | Loss: 0.00002003
Iteration 160/1000 | Loss: 0.00002003
Iteration 161/1000 | Loss: 0.00002003
Iteration 162/1000 | Loss: 0.00002003
Iteration 163/1000 | Loss: 0.00002003
Iteration 164/1000 | Loss: 0.00002003
Iteration 165/1000 | Loss: 0.00002003
Iteration 166/1000 | Loss: 0.00002003
Iteration 167/1000 | Loss: 0.00002002
Iteration 168/1000 | Loss: 0.00002002
Iteration 169/1000 | Loss: 0.00002002
Iteration 170/1000 | Loss: 0.00002002
Iteration 171/1000 | Loss: 0.00002002
Iteration 172/1000 | Loss: 0.00002002
Iteration 173/1000 | Loss: 0.00002002
Iteration 174/1000 | Loss: 0.00002002
Iteration 175/1000 | Loss: 0.00002002
Iteration 176/1000 | Loss: 0.00002002
Iteration 177/1000 | Loss: 0.00002001
Iteration 178/1000 | Loss: 0.00002001
Iteration 179/1000 | Loss: 0.00002001
Iteration 180/1000 | Loss: 0.00002001
Iteration 181/1000 | Loss: 0.00002001
Iteration 182/1000 | Loss: 0.00002001
Iteration 183/1000 | Loss: 0.00002001
Iteration 184/1000 | Loss: 0.00002001
Iteration 185/1000 | Loss: 0.00002001
Iteration 186/1000 | Loss: 0.00002001
Iteration 187/1000 | Loss: 0.00002000
Iteration 188/1000 | Loss: 0.00002000
Iteration 189/1000 | Loss: 0.00002000
Iteration 190/1000 | Loss: 0.00002000
Iteration 191/1000 | Loss: 0.00002000
Iteration 192/1000 | Loss: 0.00002000
Iteration 193/1000 | Loss: 0.00002000
Iteration 194/1000 | Loss: 0.00002000
Iteration 195/1000 | Loss: 0.00002000
Iteration 196/1000 | Loss: 0.00002000
Iteration 197/1000 | Loss: 0.00002000
Iteration 198/1000 | Loss: 0.00002000
Iteration 199/1000 | Loss: 0.00002000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.000313179451041e-05, 2.000313179451041e-05, 2.000313179451041e-05, 2.000313179451041e-05, 2.000313179451041e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.000313179451041e-05

Optimization complete. Final v2v error: 3.5442776679992676 mm

Highest mean error: 5.272632598876953 mm for frame 150

Lowest mean error: 2.3383495807647705 mm for frame 6

Saving results

Total time: 42.74329161643982
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086121
Iteration 2/25 | Loss: 0.01086121
Iteration 3/25 | Loss: 0.01086121
Iteration 4/25 | Loss: 0.01086121
Iteration 5/25 | Loss: 0.01086120
Iteration 6/25 | Loss: 0.01086120
Iteration 7/25 | Loss: 0.01086120
Iteration 8/25 | Loss: 0.01086120
Iteration 9/25 | Loss: 0.01086120
Iteration 10/25 | Loss: 0.01086120
Iteration 11/25 | Loss: 0.01086120
Iteration 12/25 | Loss: 0.01086120
Iteration 13/25 | Loss: 0.01086120
Iteration 14/25 | Loss: 0.01086120
Iteration 15/25 | Loss: 0.01086120
Iteration 16/25 | Loss: 0.01086120
Iteration 17/25 | Loss: 0.01086120
Iteration 18/25 | Loss: 0.01086120
Iteration 19/25 | Loss: 0.01086120
Iteration 20/25 | Loss: 0.01086120
Iteration 21/25 | Loss: 0.01086120
Iteration 22/25 | Loss: 0.01086120
Iteration 23/25 | Loss: 0.01086120
Iteration 24/25 | Loss: 0.01086120
Iteration 25/25 | Loss: 0.01086119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86482608
Iteration 2/25 | Loss: 0.12027711
Iteration 3/25 | Loss: 0.08393482
Iteration 4/25 | Loss: 0.08302645
Iteration 5/25 | Loss: 0.08302642
Iteration 6/25 | Loss: 0.08302641
Iteration 7/25 | Loss: 0.08302641
Iteration 8/25 | Loss: 0.08302641
Iteration 9/25 | Loss: 0.08302641
Iteration 10/25 | Loss: 0.08302641
Iteration 11/25 | Loss: 0.08302641
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.08302640914916992, 0.08302640914916992, 0.08302640914916992, 0.08302640914916992, 0.08302640914916992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08302640914916992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08302641
Iteration 2/1000 | Loss: 0.01234593
Iteration 3/1000 | Loss: 0.00366950
Iteration 4/1000 | Loss: 0.00171165
Iteration 5/1000 | Loss: 0.00531294
Iteration 6/1000 | Loss: 0.00037127
Iteration 7/1000 | Loss: 0.00099101
Iteration 8/1000 | Loss: 0.00006693
Iteration 9/1000 | Loss: 0.00012629
Iteration 10/1000 | Loss: 0.00004137
Iteration 11/1000 | Loss: 0.00012111
Iteration 12/1000 | Loss: 0.00015848
Iteration 13/1000 | Loss: 0.00002866
Iteration 14/1000 | Loss: 0.00012207
Iteration 15/1000 | Loss: 0.00018425
Iteration 16/1000 | Loss: 0.00002408
Iteration 17/1000 | Loss: 0.00003262
Iteration 18/1000 | Loss: 0.00002209
Iteration 19/1000 | Loss: 0.00012896
Iteration 20/1000 | Loss: 0.00002090
Iteration 21/1000 | Loss: 0.00001951
Iteration 22/1000 | Loss: 0.00004677
Iteration 23/1000 | Loss: 0.00001818
Iteration 24/1000 | Loss: 0.00004390
Iteration 25/1000 | Loss: 0.00001722
Iteration 26/1000 | Loss: 0.00001678
Iteration 27/1000 | Loss: 0.00001625
Iteration 28/1000 | Loss: 0.00003752
Iteration 29/1000 | Loss: 0.00001581
Iteration 30/1000 | Loss: 0.00005566
Iteration 31/1000 | Loss: 0.00001541
Iteration 32/1000 | Loss: 0.00001508
Iteration 33/1000 | Loss: 0.00005109
Iteration 34/1000 | Loss: 0.00004995
Iteration 35/1000 | Loss: 0.00001474
Iteration 36/1000 | Loss: 0.00001467
Iteration 37/1000 | Loss: 0.00001467
Iteration 38/1000 | Loss: 0.00003849
Iteration 39/1000 | Loss: 0.00009460
Iteration 40/1000 | Loss: 0.00052548
Iteration 41/1000 | Loss: 0.00064492
Iteration 42/1000 | Loss: 0.00151660
Iteration 43/1000 | Loss: 0.00152596
Iteration 44/1000 | Loss: 0.00005606
Iteration 45/1000 | Loss: 0.00003942
Iteration 46/1000 | Loss: 0.00001613
Iteration 47/1000 | Loss: 0.00001500
Iteration 48/1000 | Loss: 0.00001443
Iteration 49/1000 | Loss: 0.00003946
Iteration 50/1000 | Loss: 0.00001449
Iteration 51/1000 | Loss: 0.00001436
Iteration 52/1000 | Loss: 0.00001434
Iteration 53/1000 | Loss: 0.00001434
Iteration 54/1000 | Loss: 0.00001433
Iteration 55/1000 | Loss: 0.00001433
Iteration 56/1000 | Loss: 0.00001433
Iteration 57/1000 | Loss: 0.00001432
Iteration 58/1000 | Loss: 0.00001432
Iteration 59/1000 | Loss: 0.00001432
Iteration 60/1000 | Loss: 0.00001431
Iteration 61/1000 | Loss: 0.00001431
Iteration 62/1000 | Loss: 0.00001431
Iteration 63/1000 | Loss: 0.00001431
Iteration 64/1000 | Loss: 0.00001431
Iteration 65/1000 | Loss: 0.00001431
Iteration 66/1000 | Loss: 0.00001431
Iteration 67/1000 | Loss: 0.00001430
Iteration 68/1000 | Loss: 0.00001430
Iteration 69/1000 | Loss: 0.00001430
Iteration 70/1000 | Loss: 0.00001430
Iteration 71/1000 | Loss: 0.00001461
Iteration 72/1000 | Loss: 0.00001438
Iteration 73/1000 | Loss: 0.00001426
Iteration 74/1000 | Loss: 0.00001426
Iteration 75/1000 | Loss: 0.00001426
Iteration 76/1000 | Loss: 0.00001426
Iteration 77/1000 | Loss: 0.00001426
Iteration 78/1000 | Loss: 0.00001426
Iteration 79/1000 | Loss: 0.00001426
Iteration 80/1000 | Loss: 0.00001425
Iteration 81/1000 | Loss: 0.00001425
Iteration 82/1000 | Loss: 0.00001425
Iteration 83/1000 | Loss: 0.00001425
Iteration 84/1000 | Loss: 0.00001425
Iteration 85/1000 | Loss: 0.00001425
Iteration 86/1000 | Loss: 0.00001425
Iteration 87/1000 | Loss: 0.00001425
Iteration 88/1000 | Loss: 0.00001425
Iteration 89/1000 | Loss: 0.00001423
Iteration 90/1000 | Loss: 0.00001423
Iteration 91/1000 | Loss: 0.00001423
Iteration 92/1000 | Loss: 0.00001423
Iteration 93/1000 | Loss: 0.00001423
Iteration 94/1000 | Loss: 0.00001423
Iteration 95/1000 | Loss: 0.00001423
Iteration 96/1000 | Loss: 0.00001423
Iteration 97/1000 | Loss: 0.00001422
Iteration 98/1000 | Loss: 0.00001422
Iteration 99/1000 | Loss: 0.00001422
Iteration 100/1000 | Loss: 0.00001422
Iteration 101/1000 | Loss: 0.00001422
Iteration 102/1000 | Loss: 0.00001422
Iteration 103/1000 | Loss: 0.00001422
Iteration 104/1000 | Loss: 0.00001422
Iteration 105/1000 | Loss: 0.00001422
Iteration 106/1000 | Loss: 0.00001422
Iteration 107/1000 | Loss: 0.00001422
Iteration 108/1000 | Loss: 0.00001422
Iteration 109/1000 | Loss: 0.00001422
Iteration 110/1000 | Loss: 0.00001421
Iteration 111/1000 | Loss: 0.00001421
Iteration 112/1000 | Loss: 0.00001421
Iteration 113/1000 | Loss: 0.00001421
Iteration 114/1000 | Loss: 0.00001421
Iteration 115/1000 | Loss: 0.00001421
Iteration 116/1000 | Loss: 0.00001421
Iteration 117/1000 | Loss: 0.00001421
Iteration 118/1000 | Loss: 0.00001421
Iteration 119/1000 | Loss: 0.00001421
Iteration 120/1000 | Loss: 0.00001421
Iteration 121/1000 | Loss: 0.00001421
Iteration 122/1000 | Loss: 0.00001421
Iteration 123/1000 | Loss: 0.00001421
Iteration 124/1000 | Loss: 0.00001421
Iteration 125/1000 | Loss: 0.00001421
Iteration 126/1000 | Loss: 0.00001421
Iteration 127/1000 | Loss: 0.00001421
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001421
Iteration 130/1000 | Loss: 0.00001420
Iteration 131/1000 | Loss: 0.00001420
Iteration 132/1000 | Loss: 0.00001420
Iteration 133/1000 | Loss: 0.00001420
Iteration 134/1000 | Loss: 0.00001420
Iteration 135/1000 | Loss: 0.00001420
Iteration 136/1000 | Loss: 0.00001420
Iteration 137/1000 | Loss: 0.00001419
Iteration 138/1000 | Loss: 0.00001419
Iteration 139/1000 | Loss: 0.00003436
Iteration 140/1000 | Loss: 0.00007075
Iteration 141/1000 | Loss: 0.00003439
Iteration 142/1000 | Loss: 0.00001506
Iteration 143/1000 | Loss: 0.00003416
Iteration 144/1000 | Loss: 0.00011368
Iteration 145/1000 | Loss: 0.00003419
Iteration 146/1000 | Loss: 0.00003006
Iteration 147/1000 | Loss: 0.00003361
Iteration 148/1000 | Loss: 0.00002606
Iteration 149/1000 | Loss: 0.00003330
Iteration 150/1000 | Loss: 0.00002473
Iteration 151/1000 | Loss: 0.00003274
Iteration 152/1000 | Loss: 0.00002391
Iteration 153/1000 | Loss: 0.00005328
Iteration 154/1000 | Loss: 0.00002292
Iteration 155/1000 | Loss: 0.00003026
Iteration 156/1000 | Loss: 0.00007797
Iteration 157/1000 | Loss: 0.00003359
Iteration 158/1000 | Loss: 0.00002531
Iteration 159/1000 | Loss: 0.00003101
Iteration 160/1000 | Loss: 0.00005668
Iteration 161/1000 | Loss: 0.00004777
Iteration 162/1000 | Loss: 0.00002558
Iteration 163/1000 | Loss: 0.00001469
Iteration 164/1000 | Loss: 0.00003927
Iteration 165/1000 | Loss: 0.00002014
Iteration 166/1000 | Loss: 0.00002417
Iteration 167/1000 | Loss: 0.00001433
Iteration 168/1000 | Loss: 0.00001428
Iteration 169/1000 | Loss: 0.00001427
Iteration 170/1000 | Loss: 0.00001425
Iteration 171/1000 | Loss: 0.00001425
Iteration 172/1000 | Loss: 0.00001425
Iteration 173/1000 | Loss: 0.00001425
Iteration 174/1000 | Loss: 0.00001424
Iteration 175/1000 | Loss: 0.00001424
Iteration 176/1000 | Loss: 0.00001424
Iteration 177/1000 | Loss: 0.00001424
Iteration 178/1000 | Loss: 0.00001423
Iteration 179/1000 | Loss: 0.00001423
Iteration 180/1000 | Loss: 0.00001423
Iteration 181/1000 | Loss: 0.00001422
Iteration 182/1000 | Loss: 0.00001422
Iteration 183/1000 | Loss: 0.00001422
Iteration 184/1000 | Loss: 0.00002675
Iteration 185/1000 | Loss: 0.00003134
Iteration 186/1000 | Loss: 0.00002742
Iteration 187/1000 | Loss: 0.00001857
Iteration 188/1000 | Loss: 0.00002606
Iteration 189/1000 | Loss: 0.00002570
Iteration 190/1000 | Loss: 0.00002641
Iteration 191/1000 | Loss: 0.00001838
Iteration 192/1000 | Loss: 0.00002393
Iteration 193/1000 | Loss: 0.00002585
Iteration 194/1000 | Loss: 0.00002351
Iteration 195/1000 | Loss: 0.00001776
Iteration 196/1000 | Loss: 0.00002259
Iteration 197/1000 | Loss: 0.00003660
Iteration 198/1000 | Loss: 0.00001551
Iteration 199/1000 | Loss: 0.00001551
Iteration 200/1000 | Loss: 0.00001870
Iteration 201/1000 | Loss: 0.00002520
Iteration 202/1000 | Loss: 0.00001457
Iteration 203/1000 | Loss: 0.00003459
Iteration 204/1000 | Loss: 0.00001429
Iteration 205/1000 | Loss: 0.00001420
Iteration 206/1000 | Loss: 0.00001414
Iteration 207/1000 | Loss: 0.00001405
Iteration 208/1000 | Loss: 0.00001404
Iteration 209/1000 | Loss: 0.00001403
Iteration 210/1000 | Loss: 0.00001403
Iteration 211/1000 | Loss: 0.00001403
Iteration 212/1000 | Loss: 0.00006857
Iteration 213/1000 | Loss: 0.00006857
Iteration 214/1000 | Loss: 0.00012919
Iteration 215/1000 | Loss: 0.00057941
Iteration 216/1000 | Loss: 0.00032767
Iteration 217/1000 | Loss: 0.00002606
Iteration 218/1000 | Loss: 0.00034984
Iteration 219/1000 | Loss: 0.00001430
Iteration 220/1000 | Loss: 0.00001403
Iteration 221/1000 | Loss: 0.00001403
Iteration 222/1000 | Loss: 0.00001398
Iteration 223/1000 | Loss: 0.00004173
Iteration 224/1000 | Loss: 0.00001400
Iteration 225/1000 | Loss: 0.00001836
Iteration 226/1000 | Loss: 0.00002896
Iteration 227/1000 | Loss: 0.00001398
Iteration 228/1000 | Loss: 0.00001398
Iteration 229/1000 | Loss: 0.00001398
Iteration 230/1000 | Loss: 0.00001398
Iteration 231/1000 | Loss: 0.00001398
Iteration 232/1000 | Loss: 0.00001398
Iteration 233/1000 | Loss: 0.00001398
Iteration 234/1000 | Loss: 0.00001398
Iteration 235/1000 | Loss: 0.00001398
Iteration 236/1000 | Loss: 0.00001398
Iteration 237/1000 | Loss: 0.00001397
Iteration 238/1000 | Loss: 0.00001397
Iteration 239/1000 | Loss: 0.00001397
Iteration 240/1000 | Loss: 0.00001397
Iteration 241/1000 | Loss: 0.00001397
Iteration 242/1000 | Loss: 0.00001396
Iteration 243/1000 | Loss: 0.00001396
Iteration 244/1000 | Loss: 0.00001396
Iteration 245/1000 | Loss: 0.00001396
Iteration 246/1000 | Loss: 0.00001396
Iteration 247/1000 | Loss: 0.00001396
Iteration 248/1000 | Loss: 0.00001532
Iteration 249/1000 | Loss: 0.00001398
Iteration 250/1000 | Loss: 0.00001397
Iteration 251/1000 | Loss: 0.00001397
Iteration 252/1000 | Loss: 0.00001397
Iteration 253/1000 | Loss: 0.00001397
Iteration 254/1000 | Loss: 0.00001397
Iteration 255/1000 | Loss: 0.00001397
Iteration 256/1000 | Loss: 0.00001397
Iteration 257/1000 | Loss: 0.00001397
Iteration 258/1000 | Loss: 0.00001397
Iteration 259/1000 | Loss: 0.00001397
Iteration 260/1000 | Loss: 0.00001397
Iteration 261/1000 | Loss: 0.00001397
Iteration 262/1000 | Loss: 0.00001397
Iteration 263/1000 | Loss: 0.00001397
Iteration 264/1000 | Loss: 0.00001397
Iteration 265/1000 | Loss: 0.00001397
Iteration 266/1000 | Loss: 0.00001397
Iteration 267/1000 | Loss: 0.00001397
Iteration 268/1000 | Loss: 0.00001397
Iteration 269/1000 | Loss: 0.00001397
Iteration 270/1000 | Loss: 0.00001397
Iteration 271/1000 | Loss: 0.00001397
Iteration 272/1000 | Loss: 0.00001397
Iteration 273/1000 | Loss: 0.00001397
Iteration 274/1000 | Loss: 0.00001397
Iteration 275/1000 | Loss: 0.00001396
Iteration 276/1000 | Loss: 0.00001396
Iteration 277/1000 | Loss: 0.00001396
Iteration 278/1000 | Loss: 0.00001396
Iteration 279/1000 | Loss: 0.00001396
Iteration 280/1000 | Loss: 0.00001396
Iteration 281/1000 | Loss: 0.00001396
Iteration 282/1000 | Loss: 0.00001396
Iteration 283/1000 | Loss: 0.00001396
Iteration 284/1000 | Loss: 0.00001396
Iteration 285/1000 | Loss: 0.00001396
Iteration 286/1000 | Loss: 0.00001396
Iteration 287/1000 | Loss: 0.00001396
Iteration 288/1000 | Loss: 0.00001396
Iteration 289/1000 | Loss: 0.00001396
Iteration 290/1000 | Loss: 0.00001396
Iteration 291/1000 | Loss: 0.00001396
Iteration 292/1000 | Loss: 0.00001396
Iteration 293/1000 | Loss: 0.00001396
Iteration 294/1000 | Loss: 0.00001396
Iteration 295/1000 | Loss: 0.00001396
Iteration 296/1000 | Loss: 0.00001396
Iteration 297/1000 | Loss: 0.00001396
Iteration 298/1000 | Loss: 0.00001395
Iteration 299/1000 | Loss: 0.00001395
Iteration 300/1000 | Loss: 0.00001395
Iteration 301/1000 | Loss: 0.00001395
Iteration 302/1000 | Loss: 0.00001395
Iteration 303/1000 | Loss: 0.00001395
Iteration 304/1000 | Loss: 0.00001395
Iteration 305/1000 | Loss: 0.00001395
Iteration 306/1000 | Loss: 0.00001395
Iteration 307/1000 | Loss: 0.00001395
Iteration 308/1000 | Loss: 0.00001395
Iteration 309/1000 | Loss: 0.00001395
Iteration 310/1000 | Loss: 0.00001395
Iteration 311/1000 | Loss: 0.00001395
Iteration 312/1000 | Loss: 0.00001395
Iteration 313/1000 | Loss: 0.00001395
Iteration 314/1000 | Loss: 0.00001395
Iteration 315/1000 | Loss: 0.00001395
Iteration 316/1000 | Loss: 0.00001395
Iteration 317/1000 | Loss: 0.00001394
Iteration 318/1000 | Loss: 0.00001394
Iteration 319/1000 | Loss: 0.00001394
Iteration 320/1000 | Loss: 0.00001394
Iteration 321/1000 | Loss: 0.00001394
Iteration 322/1000 | Loss: 0.00001394
Iteration 323/1000 | Loss: 0.00001394
Iteration 324/1000 | Loss: 0.00001394
Iteration 325/1000 | Loss: 0.00001394
Iteration 326/1000 | Loss: 0.00001394
Iteration 327/1000 | Loss: 0.00001394
Iteration 328/1000 | Loss: 0.00001394
Iteration 329/1000 | Loss: 0.00001394
Iteration 330/1000 | Loss: 0.00001394
Iteration 331/1000 | Loss: 0.00001394
Iteration 332/1000 | Loss: 0.00001394
Iteration 333/1000 | Loss: 0.00001394
Iteration 334/1000 | Loss: 0.00001393
Iteration 335/1000 | Loss: 0.00001393
Iteration 336/1000 | Loss: 0.00001393
Iteration 337/1000 | Loss: 0.00001393
Iteration 338/1000 | Loss: 0.00001393
Iteration 339/1000 | Loss: 0.00001560
Iteration 340/1000 | Loss: 0.00001395
Iteration 341/1000 | Loss: 0.00001395
Iteration 342/1000 | Loss: 0.00001395
Iteration 343/1000 | Loss: 0.00001395
Iteration 344/1000 | Loss: 0.00001395
Iteration 345/1000 | Loss: 0.00001395
Iteration 346/1000 | Loss: 0.00001395
Iteration 347/1000 | Loss: 0.00001395
Iteration 348/1000 | Loss: 0.00001395
Iteration 349/1000 | Loss: 0.00001395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 349. Stopping optimization.
Last 5 losses: [1.3948847481515259e-05, 1.3948847481515259e-05, 1.3948847481515259e-05, 1.3948847481515259e-05, 1.3948847481515259e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3948847481515259e-05

Optimization complete. Final v2v error: 3.0805747509002686 mm

Highest mean error: 9.677308082580566 mm for frame 6

Lowest mean error: 2.6095712184906006 mm for frame 129

Saving results

Total time: 199.5144817829132
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024679
Iteration 2/25 | Loss: 0.01024679
Iteration 3/25 | Loss: 0.01024679
Iteration 4/25 | Loss: 0.01024679
Iteration 5/25 | Loss: 0.01024678
Iteration 6/25 | Loss: 0.01024678
Iteration 7/25 | Loss: 0.01024678
Iteration 8/25 | Loss: 0.01024678
Iteration 9/25 | Loss: 0.01024678
Iteration 10/25 | Loss: 0.01024677
Iteration 11/25 | Loss: 0.01024677
Iteration 12/25 | Loss: 0.01024677
Iteration 13/25 | Loss: 0.01024677
Iteration 14/25 | Loss: 0.01024676
Iteration 15/25 | Loss: 0.01024676
Iteration 16/25 | Loss: 0.01024676
Iteration 17/25 | Loss: 0.01024676
Iteration 18/25 | Loss: 0.01024676
Iteration 19/25 | Loss: 0.01024675
Iteration 20/25 | Loss: 0.01024675
Iteration 21/25 | Loss: 0.01024675
Iteration 22/25 | Loss: 0.01024675
Iteration 23/25 | Loss: 0.01024675
Iteration 24/25 | Loss: 0.01024675
Iteration 25/25 | Loss: 0.01024674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69381225
Iteration 2/25 | Loss: 0.13007084
Iteration 3/25 | Loss: 0.12864031
Iteration 4/25 | Loss: 0.12675807
Iteration 5/25 | Loss: 0.12675802
Iteration 6/25 | Loss: 0.12675802
Iteration 7/25 | Loss: 0.12675799
Iteration 8/25 | Loss: 0.12675799
Iteration 9/25 | Loss: 0.12675799
Iteration 10/25 | Loss: 0.12675798
Iteration 11/25 | Loss: 0.12675798
Iteration 12/25 | Loss: 0.12675798
Iteration 13/25 | Loss: 0.12675798
Iteration 14/25 | Loss: 0.12675798
Iteration 15/25 | Loss: 0.12675798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.12675797939300537, 0.12675797939300537, 0.12675797939300537, 0.12675797939300537, 0.12675797939300537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12675797939300537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12675798
Iteration 2/1000 | Loss: 0.00316066
Iteration 3/1000 | Loss: 0.00128022
Iteration 4/1000 | Loss: 0.00101077
Iteration 5/1000 | Loss: 0.00159286
Iteration 6/1000 | Loss: 0.00050142
Iteration 7/1000 | Loss: 0.00135865
Iteration 8/1000 | Loss: 0.00510304
Iteration 9/1000 | Loss: 0.00328914
Iteration 10/1000 | Loss: 0.00256921
Iteration 11/1000 | Loss: 0.00284326
Iteration 12/1000 | Loss: 0.00063088
Iteration 13/1000 | Loss: 0.00163747
Iteration 14/1000 | Loss: 0.00008609
Iteration 15/1000 | Loss: 0.00016644
Iteration 16/1000 | Loss: 0.00124110
Iteration 17/1000 | Loss: 0.00137331
Iteration 18/1000 | Loss: 0.00148949
Iteration 19/1000 | Loss: 0.00103605
Iteration 20/1000 | Loss: 0.00126314
Iteration 21/1000 | Loss: 0.00084945
Iteration 22/1000 | Loss: 0.00123838
Iteration 23/1000 | Loss: 0.00147730
Iteration 24/1000 | Loss: 0.00149750
Iteration 25/1000 | Loss: 0.00132067
Iteration 26/1000 | Loss: 0.00089195
Iteration 27/1000 | Loss: 0.00141553
Iteration 28/1000 | Loss: 0.00032212
Iteration 29/1000 | Loss: 0.00091787
Iteration 30/1000 | Loss: 0.00006240
Iteration 31/1000 | Loss: 0.00007409
Iteration 32/1000 | Loss: 0.00172659
Iteration 33/1000 | Loss: 0.00038917
Iteration 34/1000 | Loss: 0.00057010
Iteration 35/1000 | Loss: 0.00007879
Iteration 36/1000 | Loss: 0.00028444
Iteration 37/1000 | Loss: 0.00031967
Iteration 38/1000 | Loss: 0.00023160
Iteration 39/1000 | Loss: 0.00032963
Iteration 40/1000 | Loss: 0.00344163
Iteration 41/1000 | Loss: 0.00392155
Iteration 42/1000 | Loss: 0.00242315
Iteration 43/1000 | Loss: 0.00032052
Iteration 44/1000 | Loss: 0.00038756
Iteration 45/1000 | Loss: 0.00017623
Iteration 46/1000 | Loss: 0.00013330
Iteration 47/1000 | Loss: 0.00055660
Iteration 48/1000 | Loss: 0.00049305
Iteration 49/1000 | Loss: 0.00296860
Iteration 50/1000 | Loss: 0.00147454
Iteration 51/1000 | Loss: 0.00083947
Iteration 52/1000 | Loss: 0.00237334
Iteration 53/1000 | Loss: 0.00004479
Iteration 54/1000 | Loss: 0.00003799
Iteration 55/1000 | Loss: 0.00047300
Iteration 56/1000 | Loss: 0.00009415
Iteration 57/1000 | Loss: 0.00013296
Iteration 58/1000 | Loss: 0.00011718
Iteration 59/1000 | Loss: 0.00014677
Iteration 60/1000 | Loss: 0.00019550
Iteration 61/1000 | Loss: 0.00007066
Iteration 62/1000 | Loss: 0.00016084
Iteration 63/1000 | Loss: 0.00049337
Iteration 64/1000 | Loss: 0.00008636
Iteration 65/1000 | Loss: 0.00009664
Iteration 66/1000 | Loss: 0.00002510
Iteration 67/1000 | Loss: 0.00009726
Iteration 68/1000 | Loss: 0.00002080
Iteration 69/1000 | Loss: 0.00014616
Iteration 70/1000 | Loss: 0.00010679
Iteration 71/1000 | Loss: 0.00012648
Iteration 72/1000 | Loss: 0.00010078
Iteration 73/1000 | Loss: 0.00012698
Iteration 74/1000 | Loss: 0.00017787
Iteration 75/1000 | Loss: 0.00002920
Iteration 76/1000 | Loss: 0.00001880
Iteration 77/1000 | Loss: 0.00015574
Iteration 78/1000 | Loss: 0.00001733
Iteration 79/1000 | Loss: 0.00013987
Iteration 80/1000 | Loss: 0.00001671
Iteration 81/1000 | Loss: 0.00001599
Iteration 82/1000 | Loss: 0.00001538
Iteration 83/1000 | Loss: 0.00014738
Iteration 84/1000 | Loss: 0.00027775
Iteration 85/1000 | Loss: 0.00003975
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00006705
Iteration 88/1000 | Loss: 0.00001453
Iteration 89/1000 | Loss: 0.00001416
Iteration 90/1000 | Loss: 0.00013940
Iteration 91/1000 | Loss: 0.00002275
Iteration 92/1000 | Loss: 0.00005230
Iteration 93/1000 | Loss: 0.00001375
Iteration 94/1000 | Loss: 0.00001355
Iteration 95/1000 | Loss: 0.00001350
Iteration 96/1000 | Loss: 0.00001345
Iteration 97/1000 | Loss: 0.00001344
Iteration 98/1000 | Loss: 0.00014029
Iteration 99/1000 | Loss: 0.00003078
Iteration 100/1000 | Loss: 0.00003363
Iteration 101/1000 | Loss: 0.00009129
Iteration 102/1000 | Loss: 0.00001320
Iteration 103/1000 | Loss: 0.00001313
Iteration 104/1000 | Loss: 0.00001307
Iteration 105/1000 | Loss: 0.00001307
Iteration 106/1000 | Loss: 0.00001307
Iteration 107/1000 | Loss: 0.00001307
Iteration 108/1000 | Loss: 0.00001307
Iteration 109/1000 | Loss: 0.00001306
Iteration 110/1000 | Loss: 0.00001305
Iteration 111/1000 | Loss: 0.00001304
Iteration 112/1000 | Loss: 0.00001303
Iteration 113/1000 | Loss: 0.00001302
Iteration 114/1000 | Loss: 0.00001302
Iteration 115/1000 | Loss: 0.00001301
Iteration 116/1000 | Loss: 0.00001301
Iteration 117/1000 | Loss: 0.00001301
Iteration 118/1000 | Loss: 0.00001300
Iteration 119/1000 | Loss: 0.00001299
Iteration 120/1000 | Loss: 0.00001299
Iteration 121/1000 | Loss: 0.00001299
Iteration 122/1000 | Loss: 0.00001298
Iteration 123/1000 | Loss: 0.00001298
Iteration 124/1000 | Loss: 0.00001298
Iteration 125/1000 | Loss: 0.00001297
Iteration 126/1000 | Loss: 0.00001295
Iteration 127/1000 | Loss: 0.00001294
Iteration 128/1000 | Loss: 0.00001293
Iteration 129/1000 | Loss: 0.00001293
Iteration 130/1000 | Loss: 0.00001292
Iteration 131/1000 | Loss: 0.00001292
Iteration 132/1000 | Loss: 0.00001292
Iteration 133/1000 | Loss: 0.00001291
Iteration 134/1000 | Loss: 0.00001291
Iteration 135/1000 | Loss: 0.00001290
Iteration 136/1000 | Loss: 0.00001290
Iteration 137/1000 | Loss: 0.00001290
Iteration 138/1000 | Loss: 0.00001290
Iteration 139/1000 | Loss: 0.00001290
Iteration 140/1000 | Loss: 0.00001290
Iteration 141/1000 | Loss: 0.00001290
Iteration 142/1000 | Loss: 0.00001290
Iteration 143/1000 | Loss: 0.00001289
Iteration 144/1000 | Loss: 0.00001289
Iteration 145/1000 | Loss: 0.00001289
Iteration 146/1000 | Loss: 0.00001289
Iteration 147/1000 | Loss: 0.00001288
Iteration 148/1000 | Loss: 0.00001288
Iteration 149/1000 | Loss: 0.00001288
Iteration 150/1000 | Loss: 0.00001288
Iteration 151/1000 | Loss: 0.00001287
Iteration 152/1000 | Loss: 0.00001287
Iteration 153/1000 | Loss: 0.00001287
Iteration 154/1000 | Loss: 0.00001287
Iteration 155/1000 | Loss: 0.00001287
Iteration 156/1000 | Loss: 0.00001287
Iteration 157/1000 | Loss: 0.00001287
Iteration 158/1000 | Loss: 0.00001287
Iteration 159/1000 | Loss: 0.00001287
Iteration 160/1000 | Loss: 0.00001286
Iteration 161/1000 | Loss: 0.00001286
Iteration 162/1000 | Loss: 0.00001286
Iteration 163/1000 | Loss: 0.00001286
Iteration 164/1000 | Loss: 0.00001286
Iteration 165/1000 | Loss: 0.00001286
Iteration 166/1000 | Loss: 0.00001286
Iteration 167/1000 | Loss: 0.00001286
Iteration 168/1000 | Loss: 0.00001286
Iteration 169/1000 | Loss: 0.00001286
Iteration 170/1000 | Loss: 0.00001286
Iteration 171/1000 | Loss: 0.00001286
Iteration 172/1000 | Loss: 0.00001285
Iteration 173/1000 | Loss: 0.00001285
Iteration 174/1000 | Loss: 0.00001285
Iteration 175/1000 | Loss: 0.00001285
Iteration 176/1000 | Loss: 0.00001285
Iteration 177/1000 | Loss: 0.00001285
Iteration 178/1000 | Loss: 0.00001285
Iteration 179/1000 | Loss: 0.00001285
Iteration 180/1000 | Loss: 0.00001285
Iteration 181/1000 | Loss: 0.00001285
Iteration 182/1000 | Loss: 0.00001285
Iteration 183/1000 | Loss: 0.00001285
Iteration 184/1000 | Loss: 0.00001285
Iteration 185/1000 | Loss: 0.00001285
Iteration 186/1000 | Loss: 0.00001285
Iteration 187/1000 | Loss: 0.00001285
Iteration 188/1000 | Loss: 0.00001285
Iteration 189/1000 | Loss: 0.00001285
Iteration 190/1000 | Loss: 0.00001285
Iteration 191/1000 | Loss: 0.00001285
Iteration 192/1000 | Loss: 0.00001285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.2850514394813217e-05, 1.2850514394813217e-05, 1.2850514394813217e-05, 1.2850514394813217e-05, 1.2850514394813217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2850514394813217e-05

Optimization complete. Final v2v error: 3.0159153938293457 mm

Highest mean error: 4.09930944442749 mm for frame 238

Lowest mean error: 2.2997310161590576 mm for frame 29

Saving results

Total time: 175.77947115898132
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007476
Iteration 2/25 | Loss: 0.00305268
Iteration 3/25 | Loss: 0.00195336
Iteration 4/25 | Loss: 0.00173371
Iteration 5/25 | Loss: 0.00171659
Iteration 6/25 | Loss: 0.00151772
Iteration 7/25 | Loss: 0.00148687
Iteration 8/25 | Loss: 0.00147921
Iteration 9/25 | Loss: 0.00140801
Iteration 10/25 | Loss: 0.00139534
Iteration 11/25 | Loss: 0.00137583
Iteration 12/25 | Loss: 0.00137444
Iteration 13/25 | Loss: 0.00136139
Iteration 14/25 | Loss: 0.00137267
Iteration 15/25 | Loss: 0.00135439
Iteration 16/25 | Loss: 0.00135157
Iteration 17/25 | Loss: 0.00134496
Iteration 18/25 | Loss: 0.00135039
Iteration 19/25 | Loss: 0.00135240
Iteration 20/25 | Loss: 0.00134001
Iteration 21/25 | Loss: 0.00134672
Iteration 22/25 | Loss: 0.00134154
Iteration 23/25 | Loss: 0.00133442
Iteration 24/25 | Loss: 0.00133663
Iteration 25/25 | Loss: 0.00133467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39263153
Iteration 2/25 | Loss: 0.00394039
Iteration 3/25 | Loss: 0.00394039
Iteration 4/25 | Loss: 0.00387127
Iteration 5/25 | Loss: 0.00387127
Iteration 6/25 | Loss: 0.00387126
Iteration 7/25 | Loss: 0.00387126
Iteration 8/25 | Loss: 0.00387126
Iteration 9/25 | Loss: 0.00387126
Iteration 10/25 | Loss: 0.00387126
Iteration 11/25 | Loss: 0.00387126
Iteration 12/25 | Loss: 0.00387126
Iteration 13/25 | Loss: 0.00387126
Iteration 14/25 | Loss: 0.00387126
Iteration 15/25 | Loss: 0.00387126
Iteration 16/25 | Loss: 0.00387126
Iteration 17/25 | Loss: 0.00387126
Iteration 18/25 | Loss: 0.00387126
Iteration 19/25 | Loss: 0.00387126
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003871261840686202, 0.003871261840686202, 0.003871261840686202, 0.003871261840686202, 0.003871261840686202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003871261840686202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00387126
Iteration 2/1000 | Loss: 0.00085839
Iteration 3/1000 | Loss: 0.00064206
Iteration 4/1000 | Loss: 0.00053790
Iteration 5/1000 | Loss: 0.00226613
Iteration 6/1000 | Loss: 0.00502076
Iteration 7/1000 | Loss: 0.00100493
Iteration 8/1000 | Loss: 0.00119371
Iteration 9/1000 | Loss: 0.00201035
Iteration 10/1000 | Loss: 0.00050838
Iteration 11/1000 | Loss: 0.00077886
Iteration 12/1000 | Loss: 0.00054529
Iteration 13/1000 | Loss: 0.00044969
Iteration 14/1000 | Loss: 0.00048866
Iteration 15/1000 | Loss: 0.00036277
Iteration 16/1000 | Loss: 0.00031780
Iteration 17/1000 | Loss: 0.00052360
Iteration 18/1000 | Loss: 0.00040117
Iteration 19/1000 | Loss: 0.00023805
Iteration 20/1000 | Loss: 0.00052704
Iteration 21/1000 | Loss: 0.00176190
Iteration 22/1000 | Loss: 0.00306277
Iteration 23/1000 | Loss: 0.00407637
Iteration 24/1000 | Loss: 0.00282694
Iteration 25/1000 | Loss: 0.00245826
Iteration 26/1000 | Loss: 0.00050985
Iteration 27/1000 | Loss: 0.00043051
Iteration 28/1000 | Loss: 0.00036703
Iteration 29/1000 | Loss: 0.00021599
Iteration 30/1000 | Loss: 0.00047973
Iteration 31/1000 | Loss: 0.00021319
Iteration 32/1000 | Loss: 0.00034058
Iteration 33/1000 | Loss: 0.00034386
Iteration 34/1000 | Loss: 0.00149115
Iteration 35/1000 | Loss: 0.00028155
Iteration 36/1000 | Loss: 0.00036447
Iteration 37/1000 | Loss: 0.00014244
Iteration 38/1000 | Loss: 0.00037829
Iteration 39/1000 | Loss: 0.00036367
Iteration 40/1000 | Loss: 0.00316530
Iteration 41/1000 | Loss: 0.00101322
Iteration 42/1000 | Loss: 0.00087905
Iteration 43/1000 | Loss: 0.00047266
Iteration 44/1000 | Loss: 0.00205449
Iteration 45/1000 | Loss: 0.00028464
Iteration 46/1000 | Loss: 0.00040563
Iteration 47/1000 | Loss: 0.00319261
Iteration 48/1000 | Loss: 0.00110017
Iteration 49/1000 | Loss: 0.00040000
Iteration 50/1000 | Loss: 0.00284028
Iteration 51/1000 | Loss: 0.00055331
Iteration 52/1000 | Loss: 0.00020440
Iteration 53/1000 | Loss: 0.00027622
Iteration 54/1000 | Loss: 0.00022835
Iteration 55/1000 | Loss: 0.00040107
Iteration 56/1000 | Loss: 0.00051129
Iteration 57/1000 | Loss: 0.00018077
Iteration 58/1000 | Loss: 0.00009414
Iteration 59/1000 | Loss: 0.00020440
Iteration 60/1000 | Loss: 0.00064353
Iteration 61/1000 | Loss: 0.00041151
Iteration 62/1000 | Loss: 0.00028912
Iteration 63/1000 | Loss: 0.00019348
Iteration 64/1000 | Loss: 0.00060272
Iteration 65/1000 | Loss: 0.00013083
Iteration 66/1000 | Loss: 0.00007788
Iteration 67/1000 | Loss: 0.00030091
Iteration 68/1000 | Loss: 0.00018246
Iteration 69/1000 | Loss: 0.00014335
Iteration 70/1000 | Loss: 0.00006267
Iteration 71/1000 | Loss: 0.00021656
Iteration 72/1000 | Loss: 0.00024963
Iteration 73/1000 | Loss: 0.00198196
Iteration 74/1000 | Loss: 0.00036131
Iteration 75/1000 | Loss: 0.00020696
Iteration 76/1000 | Loss: 0.00011320
Iteration 77/1000 | Loss: 0.00023382
Iteration 78/1000 | Loss: 0.00022929
Iteration 79/1000 | Loss: 0.00039142
Iteration 80/1000 | Loss: 0.00350545
Iteration 81/1000 | Loss: 0.00159617
Iteration 82/1000 | Loss: 0.00214494
Iteration 83/1000 | Loss: 0.00033194
Iteration 84/1000 | Loss: 0.00008265
Iteration 85/1000 | Loss: 0.00005963
Iteration 86/1000 | Loss: 0.00013811
Iteration 87/1000 | Loss: 0.00026076
Iteration 88/1000 | Loss: 0.00017630
Iteration 89/1000 | Loss: 0.00009825
Iteration 90/1000 | Loss: 0.00040285
Iteration 91/1000 | Loss: 0.00007920
Iteration 92/1000 | Loss: 0.00005821
Iteration 93/1000 | Loss: 0.00004723
Iteration 94/1000 | Loss: 0.00005275
Iteration 95/1000 | Loss: 0.00023006
Iteration 96/1000 | Loss: 0.00048284
Iteration 97/1000 | Loss: 0.00015101
Iteration 98/1000 | Loss: 0.00054742
Iteration 99/1000 | Loss: 0.00040492
Iteration 100/1000 | Loss: 0.00045848
Iteration 101/1000 | Loss: 0.00010760
Iteration 102/1000 | Loss: 0.00011395
Iteration 103/1000 | Loss: 0.00031361
Iteration 104/1000 | Loss: 0.00027196
Iteration 105/1000 | Loss: 0.00013862
Iteration 106/1000 | Loss: 0.00020143
Iteration 107/1000 | Loss: 0.00012274
Iteration 108/1000 | Loss: 0.00010852
Iteration 109/1000 | Loss: 0.00011383
Iteration 110/1000 | Loss: 0.00014080
Iteration 111/1000 | Loss: 0.00011973
Iteration 112/1000 | Loss: 0.00020918
Iteration 113/1000 | Loss: 0.00012358
Iteration 114/1000 | Loss: 0.00054583
Iteration 115/1000 | Loss: 0.00058379
Iteration 116/1000 | Loss: 0.00048363
Iteration 117/1000 | Loss: 0.00003997
Iteration 118/1000 | Loss: 0.00004203
Iteration 119/1000 | Loss: 0.00003231
Iteration 120/1000 | Loss: 0.00003132
Iteration 121/1000 | Loss: 0.00032112
Iteration 122/1000 | Loss: 0.00067451
Iteration 123/1000 | Loss: 0.00030415
Iteration 124/1000 | Loss: 0.00025155
Iteration 125/1000 | Loss: 0.00020584
Iteration 126/1000 | Loss: 0.00005142
Iteration 127/1000 | Loss: 0.00004031
Iteration 128/1000 | Loss: 0.00010043
Iteration 129/1000 | Loss: 0.00005236
Iteration 130/1000 | Loss: 0.00003069
Iteration 131/1000 | Loss: 0.00007702
Iteration 132/1000 | Loss: 0.00002707
Iteration 133/1000 | Loss: 0.00004171
Iteration 134/1000 | Loss: 0.00002440
Iteration 135/1000 | Loss: 0.00002378
Iteration 136/1000 | Loss: 0.00006020
Iteration 137/1000 | Loss: 0.00003041
Iteration 138/1000 | Loss: 0.00002391
Iteration 139/1000 | Loss: 0.00002352
Iteration 140/1000 | Loss: 0.00002599
Iteration 141/1000 | Loss: 0.00002260
Iteration 142/1000 | Loss: 0.00002225
Iteration 143/1000 | Loss: 0.00002193
Iteration 144/1000 | Loss: 0.00002298
Iteration 145/1000 | Loss: 0.00002209
Iteration 146/1000 | Loss: 0.00002148
Iteration 147/1000 | Loss: 0.00002146
Iteration 148/1000 | Loss: 0.00002299
Iteration 149/1000 | Loss: 0.00002115
Iteration 150/1000 | Loss: 0.00002100
Iteration 151/1000 | Loss: 0.00002098
Iteration 152/1000 | Loss: 0.00002097
Iteration 153/1000 | Loss: 0.00002097
Iteration 154/1000 | Loss: 0.00002115
Iteration 155/1000 | Loss: 0.00002381
Iteration 156/1000 | Loss: 0.00007330
Iteration 157/1000 | Loss: 0.00002370
Iteration 158/1000 | Loss: 0.00002447
Iteration 159/1000 | Loss: 0.00002433
Iteration 160/1000 | Loss: 0.00003191
Iteration 161/1000 | Loss: 0.00004365
Iteration 162/1000 | Loss: 0.00002115
Iteration 163/1000 | Loss: 0.00002848
Iteration 164/1000 | Loss: 0.00002211
Iteration 165/1000 | Loss: 0.00002156
Iteration 166/1000 | Loss: 0.00002070
Iteration 167/1000 | Loss: 0.00002067
Iteration 168/1000 | Loss: 0.00002067
Iteration 169/1000 | Loss: 0.00002067
Iteration 170/1000 | Loss: 0.00002067
Iteration 171/1000 | Loss: 0.00002067
Iteration 172/1000 | Loss: 0.00002067
Iteration 173/1000 | Loss: 0.00002067
Iteration 174/1000 | Loss: 0.00002067
Iteration 175/1000 | Loss: 0.00002067
Iteration 176/1000 | Loss: 0.00002067
Iteration 177/1000 | Loss: 0.00002067
Iteration 178/1000 | Loss: 0.00002067
Iteration 179/1000 | Loss: 0.00002067
Iteration 180/1000 | Loss: 0.00002067
Iteration 181/1000 | Loss: 0.00002067
Iteration 182/1000 | Loss: 0.00002066
Iteration 183/1000 | Loss: 0.00002066
Iteration 184/1000 | Loss: 0.00002066
Iteration 185/1000 | Loss: 0.00002066
Iteration 186/1000 | Loss: 0.00002065
Iteration 187/1000 | Loss: 0.00002065
Iteration 188/1000 | Loss: 0.00002065
Iteration 189/1000 | Loss: 0.00002065
Iteration 190/1000 | Loss: 0.00002065
Iteration 191/1000 | Loss: 0.00002065
Iteration 192/1000 | Loss: 0.00002065
Iteration 193/1000 | Loss: 0.00002065
Iteration 194/1000 | Loss: 0.00002065
Iteration 195/1000 | Loss: 0.00002065
Iteration 196/1000 | Loss: 0.00002065
Iteration 197/1000 | Loss: 0.00002064
Iteration 198/1000 | Loss: 0.00002064
Iteration 199/1000 | Loss: 0.00002064
Iteration 200/1000 | Loss: 0.00011714
Iteration 201/1000 | Loss: 0.00011507
Iteration 202/1000 | Loss: 0.00013867
Iteration 203/1000 | Loss: 0.00002225
Iteration 204/1000 | Loss: 0.00002052
Iteration 205/1000 | Loss: 0.00004161
Iteration 206/1000 | Loss: 0.00001813
Iteration 207/1000 | Loss: 0.00003099
Iteration 208/1000 | Loss: 0.00001737
Iteration 209/1000 | Loss: 0.00001707
Iteration 210/1000 | Loss: 0.00001993
Iteration 211/1000 | Loss: 0.00001673
Iteration 212/1000 | Loss: 0.00001673
Iteration 213/1000 | Loss: 0.00001673
Iteration 214/1000 | Loss: 0.00001672
Iteration 215/1000 | Loss: 0.00001672
Iteration 216/1000 | Loss: 0.00001671
Iteration 217/1000 | Loss: 0.00001671
Iteration 218/1000 | Loss: 0.00001671
Iteration 219/1000 | Loss: 0.00001671
Iteration 220/1000 | Loss: 0.00001670
Iteration 221/1000 | Loss: 0.00001670
Iteration 222/1000 | Loss: 0.00001670
Iteration 223/1000 | Loss: 0.00001670
Iteration 224/1000 | Loss: 0.00001669
Iteration 225/1000 | Loss: 0.00001669
Iteration 226/1000 | Loss: 0.00001669
Iteration 227/1000 | Loss: 0.00001669
Iteration 228/1000 | Loss: 0.00001668
Iteration 229/1000 | Loss: 0.00001667
Iteration 230/1000 | Loss: 0.00004125
Iteration 231/1000 | Loss: 0.00002374
Iteration 232/1000 | Loss: 0.00001645
Iteration 233/1000 | Loss: 0.00001642
Iteration 234/1000 | Loss: 0.00001641
Iteration 235/1000 | Loss: 0.00001640
Iteration 236/1000 | Loss: 0.00001639
Iteration 237/1000 | Loss: 0.00001639
Iteration 238/1000 | Loss: 0.00001638
Iteration 239/1000 | Loss: 0.00001638
Iteration 240/1000 | Loss: 0.00001638
Iteration 241/1000 | Loss: 0.00001638
Iteration 242/1000 | Loss: 0.00001638
Iteration 243/1000 | Loss: 0.00001638
Iteration 244/1000 | Loss: 0.00001638
Iteration 245/1000 | Loss: 0.00001637
Iteration 246/1000 | Loss: 0.00001637
Iteration 247/1000 | Loss: 0.00001637
Iteration 248/1000 | Loss: 0.00001636
Iteration 249/1000 | Loss: 0.00001636
Iteration 250/1000 | Loss: 0.00001635
Iteration 251/1000 | Loss: 0.00001635
Iteration 252/1000 | Loss: 0.00001635
Iteration 253/1000 | Loss: 0.00001635
Iteration 254/1000 | Loss: 0.00001635
Iteration 255/1000 | Loss: 0.00001635
Iteration 256/1000 | Loss: 0.00001635
Iteration 257/1000 | Loss: 0.00001634
Iteration 258/1000 | Loss: 0.00001634
Iteration 259/1000 | Loss: 0.00001634
Iteration 260/1000 | Loss: 0.00001631
Iteration 261/1000 | Loss: 0.00001630
Iteration 262/1000 | Loss: 0.00001629
Iteration 263/1000 | Loss: 0.00001629
Iteration 264/1000 | Loss: 0.00001628
Iteration 265/1000 | Loss: 0.00001628
Iteration 266/1000 | Loss: 0.00001623
Iteration 267/1000 | Loss: 0.00001623
Iteration 268/1000 | Loss: 0.00001623
Iteration 269/1000 | Loss: 0.00001622
Iteration 270/1000 | Loss: 0.00001622
Iteration 271/1000 | Loss: 0.00001621
Iteration 272/1000 | Loss: 0.00001621
Iteration 273/1000 | Loss: 0.00001620
Iteration 274/1000 | Loss: 0.00001620
Iteration 275/1000 | Loss: 0.00001620
Iteration 276/1000 | Loss: 0.00004776
Iteration 277/1000 | Loss: 0.00001652
Iteration 278/1000 | Loss: 0.00001610
Iteration 279/1000 | Loss: 0.00001608
Iteration 280/1000 | Loss: 0.00001608
Iteration 281/1000 | Loss: 0.00001606
Iteration 282/1000 | Loss: 0.00001606
Iteration 283/1000 | Loss: 0.00001606
Iteration 284/1000 | Loss: 0.00001606
Iteration 285/1000 | Loss: 0.00001606
Iteration 286/1000 | Loss: 0.00001606
Iteration 287/1000 | Loss: 0.00001606
Iteration 288/1000 | Loss: 0.00001606
Iteration 289/1000 | Loss: 0.00001606
Iteration 290/1000 | Loss: 0.00001605
Iteration 291/1000 | Loss: 0.00001605
Iteration 292/1000 | Loss: 0.00001605
Iteration 293/1000 | Loss: 0.00001605
Iteration 294/1000 | Loss: 0.00001605
Iteration 295/1000 | Loss: 0.00001604
Iteration 296/1000 | Loss: 0.00001603
Iteration 297/1000 | Loss: 0.00001603
Iteration 298/1000 | Loss: 0.00001603
Iteration 299/1000 | Loss: 0.00001603
Iteration 300/1000 | Loss: 0.00001603
Iteration 301/1000 | Loss: 0.00001603
Iteration 302/1000 | Loss: 0.00001603
Iteration 303/1000 | Loss: 0.00001603
Iteration 304/1000 | Loss: 0.00001602
Iteration 305/1000 | Loss: 0.00001602
Iteration 306/1000 | Loss: 0.00001602
Iteration 307/1000 | Loss: 0.00001602
Iteration 308/1000 | Loss: 0.00001602
Iteration 309/1000 | Loss: 0.00001602
Iteration 310/1000 | Loss: 0.00001602
Iteration 311/1000 | Loss: 0.00001602
Iteration 312/1000 | Loss: 0.00001602
Iteration 313/1000 | Loss: 0.00001602
Iteration 314/1000 | Loss: 0.00001602
Iteration 315/1000 | Loss: 0.00001602
Iteration 316/1000 | Loss: 0.00001602
Iteration 317/1000 | Loss: 0.00001602
Iteration 318/1000 | Loss: 0.00001602
Iteration 319/1000 | Loss: 0.00001602
Iteration 320/1000 | Loss: 0.00001602
Iteration 321/1000 | Loss: 0.00001601
Iteration 322/1000 | Loss: 0.00001601
Iteration 323/1000 | Loss: 0.00001601
Iteration 324/1000 | Loss: 0.00001601
Iteration 325/1000 | Loss: 0.00004209
Iteration 326/1000 | Loss: 0.00003107
Iteration 327/1000 | Loss: 0.00001945
Iteration 328/1000 | Loss: 0.00001585
Iteration 329/1000 | Loss: 0.00001585
Iteration 330/1000 | Loss: 0.00001585
Iteration 331/1000 | Loss: 0.00001585
Iteration 332/1000 | Loss: 0.00001585
Iteration 333/1000 | Loss: 0.00001585
Iteration 334/1000 | Loss: 0.00001584
Iteration 335/1000 | Loss: 0.00001580
Iteration 336/1000 | Loss: 0.00001580
Iteration 337/1000 | Loss: 0.00001580
Iteration 338/1000 | Loss: 0.00001580
Iteration 339/1000 | Loss: 0.00001580
Iteration 340/1000 | Loss: 0.00001580
Iteration 341/1000 | Loss: 0.00001580
Iteration 342/1000 | Loss: 0.00001580
Iteration 343/1000 | Loss: 0.00001579
Iteration 344/1000 | Loss: 0.00001579
Iteration 345/1000 | Loss: 0.00001579
Iteration 346/1000 | Loss: 0.00001579
Iteration 347/1000 | Loss: 0.00001579
Iteration 348/1000 | Loss: 0.00001579
Iteration 349/1000 | Loss: 0.00001579
Iteration 350/1000 | Loss: 0.00001579
Iteration 351/1000 | Loss: 0.00001579
Iteration 352/1000 | Loss: 0.00001579
Iteration 353/1000 | Loss: 0.00001579
Iteration 354/1000 | Loss: 0.00001579
Iteration 355/1000 | Loss: 0.00001578
Iteration 356/1000 | Loss: 0.00001578
Iteration 357/1000 | Loss: 0.00001577
Iteration 358/1000 | Loss: 0.00001577
Iteration 359/1000 | Loss: 0.00001577
Iteration 360/1000 | Loss: 0.00001576
Iteration 361/1000 | Loss: 0.00001576
Iteration 362/1000 | Loss: 0.00001576
Iteration 363/1000 | Loss: 0.00001576
Iteration 364/1000 | Loss: 0.00001576
Iteration 365/1000 | Loss: 0.00001576
Iteration 366/1000 | Loss: 0.00001576
Iteration 367/1000 | Loss: 0.00001576
Iteration 368/1000 | Loss: 0.00001575
Iteration 369/1000 | Loss: 0.00001575
Iteration 370/1000 | Loss: 0.00001575
Iteration 371/1000 | Loss: 0.00001574
Iteration 372/1000 | Loss: 0.00001574
Iteration 373/1000 | Loss: 0.00001574
Iteration 374/1000 | Loss: 0.00001573
Iteration 375/1000 | Loss: 0.00001573
Iteration 376/1000 | Loss: 0.00001572
Iteration 377/1000 | Loss: 0.00003076
Iteration 378/1000 | Loss: 0.00003075
Iteration 379/1000 | Loss: 0.00003734
Iteration 380/1000 | Loss: 0.00009221
Iteration 381/1000 | Loss: 0.00024520
Iteration 382/1000 | Loss: 0.00003649
Iteration 383/1000 | Loss: 0.00001795
Iteration 384/1000 | Loss: 0.00002095
Iteration 385/1000 | Loss: 0.00001906
Iteration 386/1000 | Loss: 0.00002807
Iteration 387/1000 | Loss: 0.00001269
Iteration 388/1000 | Loss: 0.00001237
Iteration 389/1000 | Loss: 0.00001259
Iteration 390/1000 | Loss: 0.00001204
Iteration 391/1000 | Loss: 0.00001204
Iteration 392/1000 | Loss: 0.00001198
Iteration 393/1000 | Loss: 0.00001195
Iteration 394/1000 | Loss: 0.00001195
Iteration 395/1000 | Loss: 0.00001194
Iteration 396/1000 | Loss: 0.00001194
Iteration 397/1000 | Loss: 0.00002690
Iteration 398/1000 | Loss: 0.00001190
Iteration 399/1000 | Loss: 0.00001181
Iteration 400/1000 | Loss: 0.00002074
Iteration 401/1000 | Loss: 0.00001179
Iteration 402/1000 | Loss: 0.00001174
Iteration 403/1000 | Loss: 0.00001174
Iteration 404/1000 | Loss: 0.00001446
Iteration 405/1000 | Loss: 0.00001174
Iteration 406/1000 | Loss: 0.00001173
Iteration 407/1000 | Loss: 0.00001173
Iteration 408/1000 | Loss: 0.00001173
Iteration 409/1000 | Loss: 0.00001172
Iteration 410/1000 | Loss: 0.00001172
Iteration 411/1000 | Loss: 0.00001171
Iteration 412/1000 | Loss: 0.00001469
Iteration 413/1000 | Loss: 0.00001169
Iteration 414/1000 | Loss: 0.00001169
Iteration 415/1000 | Loss: 0.00001169
Iteration 416/1000 | Loss: 0.00001169
Iteration 417/1000 | Loss: 0.00001169
Iteration 418/1000 | Loss: 0.00001168
Iteration 419/1000 | Loss: 0.00001166
Iteration 420/1000 | Loss: 0.00003881
Iteration 421/1000 | Loss: 0.00001163
Iteration 422/1000 | Loss: 0.00001160
Iteration 423/1000 | Loss: 0.00001160
Iteration 424/1000 | Loss: 0.00001160
Iteration 425/1000 | Loss: 0.00001160
Iteration 426/1000 | Loss: 0.00001160
Iteration 427/1000 | Loss: 0.00001160
Iteration 428/1000 | Loss: 0.00001160
Iteration 429/1000 | Loss: 0.00001160
Iteration 430/1000 | Loss: 0.00001160
Iteration 431/1000 | Loss: 0.00001160
Iteration 432/1000 | Loss: 0.00001160
Iteration 433/1000 | Loss: 0.00001160
Iteration 434/1000 | Loss: 0.00001160
Iteration 435/1000 | Loss: 0.00001160
Iteration 436/1000 | Loss: 0.00001160
Iteration 437/1000 | Loss: 0.00001160
Iteration 438/1000 | Loss: 0.00001160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 438. Stopping optimization.
Last 5 losses: [1.1602682207012549e-05, 1.1602682207012549e-05, 1.1602682207012549e-05, 1.1602682207012549e-05, 1.1602682207012549e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1602682207012549e-05

Optimization complete. Final v2v error: 2.6117818355560303 mm

Highest mean error: 11.628462791442871 mm for frame 182

Lowest mean error: 2.203217029571533 mm for frame 121

Saving results

Total time: 387.0792484283447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440360
Iteration 2/25 | Loss: 0.00091306
Iteration 3/25 | Loss: 0.00081034
Iteration 4/25 | Loss: 0.00079206
Iteration 5/25 | Loss: 0.00078654
Iteration 6/25 | Loss: 0.00078477
Iteration 7/25 | Loss: 0.00078466
Iteration 8/25 | Loss: 0.00078466
Iteration 9/25 | Loss: 0.00078466
Iteration 10/25 | Loss: 0.00078466
Iteration 11/25 | Loss: 0.00078466
Iteration 12/25 | Loss: 0.00078466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007846615044400096, 0.0007846615044400096, 0.0007846615044400096, 0.0007846615044400096, 0.0007846615044400096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007846615044400096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53022826
Iteration 2/25 | Loss: 0.00040017
Iteration 3/25 | Loss: 0.00040017
Iteration 4/25 | Loss: 0.00040017
Iteration 5/25 | Loss: 0.00040017
Iteration 6/25 | Loss: 0.00040017
Iteration 7/25 | Loss: 0.00040017
Iteration 8/25 | Loss: 0.00040016
Iteration 9/25 | Loss: 0.00040016
Iteration 10/25 | Loss: 0.00040016
Iteration 11/25 | Loss: 0.00040016
Iteration 12/25 | Loss: 0.00040016
Iteration 13/25 | Loss: 0.00040016
Iteration 14/25 | Loss: 0.00040016
Iteration 15/25 | Loss: 0.00040016
Iteration 16/25 | Loss: 0.00040016
Iteration 17/25 | Loss: 0.00040016
Iteration 18/25 | Loss: 0.00040016
Iteration 19/25 | Loss: 0.00040016
Iteration 20/25 | Loss: 0.00040016
Iteration 21/25 | Loss: 0.00040016
Iteration 22/25 | Loss: 0.00040016
Iteration 23/25 | Loss: 0.00040016
Iteration 24/25 | Loss: 0.00040016
Iteration 25/25 | Loss: 0.00040016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040016
Iteration 2/1000 | Loss: 0.00001760
Iteration 3/1000 | Loss: 0.00000984
Iteration 4/1000 | Loss: 0.00000878
Iteration 5/1000 | Loss: 0.00000826
Iteration 6/1000 | Loss: 0.00000793
Iteration 7/1000 | Loss: 0.00000765
Iteration 8/1000 | Loss: 0.00000747
Iteration 9/1000 | Loss: 0.00000747
Iteration 10/1000 | Loss: 0.00000747
Iteration 11/1000 | Loss: 0.00000747
Iteration 12/1000 | Loss: 0.00000747
Iteration 13/1000 | Loss: 0.00000747
Iteration 14/1000 | Loss: 0.00000747
Iteration 15/1000 | Loss: 0.00000747
Iteration 16/1000 | Loss: 0.00000747
Iteration 17/1000 | Loss: 0.00000747
Iteration 18/1000 | Loss: 0.00000747
Iteration 19/1000 | Loss: 0.00000744
Iteration 20/1000 | Loss: 0.00000743
Iteration 21/1000 | Loss: 0.00000742
Iteration 22/1000 | Loss: 0.00000742
Iteration 23/1000 | Loss: 0.00000741
Iteration 24/1000 | Loss: 0.00000741
Iteration 25/1000 | Loss: 0.00000740
Iteration 26/1000 | Loss: 0.00000739
Iteration 27/1000 | Loss: 0.00000738
Iteration 28/1000 | Loss: 0.00000738
Iteration 29/1000 | Loss: 0.00000737
Iteration 30/1000 | Loss: 0.00000737
Iteration 31/1000 | Loss: 0.00000736
Iteration 32/1000 | Loss: 0.00000733
Iteration 33/1000 | Loss: 0.00000732
Iteration 34/1000 | Loss: 0.00000732
Iteration 35/1000 | Loss: 0.00000731
Iteration 36/1000 | Loss: 0.00000731
Iteration 37/1000 | Loss: 0.00000731
Iteration 38/1000 | Loss: 0.00000731
Iteration 39/1000 | Loss: 0.00000730
Iteration 40/1000 | Loss: 0.00000730
Iteration 41/1000 | Loss: 0.00000729
Iteration 42/1000 | Loss: 0.00000729
Iteration 43/1000 | Loss: 0.00000728
Iteration 44/1000 | Loss: 0.00000728
Iteration 45/1000 | Loss: 0.00000727
Iteration 46/1000 | Loss: 0.00000727
Iteration 47/1000 | Loss: 0.00000727
Iteration 48/1000 | Loss: 0.00000726
Iteration 49/1000 | Loss: 0.00000726
Iteration 50/1000 | Loss: 0.00000726
Iteration 51/1000 | Loss: 0.00000725
Iteration 52/1000 | Loss: 0.00000725
Iteration 53/1000 | Loss: 0.00000725
Iteration 54/1000 | Loss: 0.00000725
Iteration 55/1000 | Loss: 0.00000725
Iteration 56/1000 | Loss: 0.00000725
Iteration 57/1000 | Loss: 0.00000724
Iteration 58/1000 | Loss: 0.00000724
Iteration 59/1000 | Loss: 0.00000724
Iteration 60/1000 | Loss: 0.00000724
Iteration 61/1000 | Loss: 0.00000724
Iteration 62/1000 | Loss: 0.00000724
Iteration 63/1000 | Loss: 0.00000724
Iteration 64/1000 | Loss: 0.00000723
Iteration 65/1000 | Loss: 0.00000723
Iteration 66/1000 | Loss: 0.00000722
Iteration 67/1000 | Loss: 0.00000722
Iteration 68/1000 | Loss: 0.00000722
Iteration 69/1000 | Loss: 0.00000722
Iteration 70/1000 | Loss: 0.00000721
Iteration 71/1000 | Loss: 0.00000721
Iteration 72/1000 | Loss: 0.00000721
Iteration 73/1000 | Loss: 0.00000721
Iteration 74/1000 | Loss: 0.00000721
Iteration 75/1000 | Loss: 0.00000721
Iteration 76/1000 | Loss: 0.00000721
Iteration 77/1000 | Loss: 0.00000721
Iteration 78/1000 | Loss: 0.00000721
Iteration 79/1000 | Loss: 0.00000720
Iteration 80/1000 | Loss: 0.00000720
Iteration 81/1000 | Loss: 0.00000720
Iteration 82/1000 | Loss: 0.00000720
Iteration 83/1000 | Loss: 0.00000720
Iteration 84/1000 | Loss: 0.00000720
Iteration 85/1000 | Loss: 0.00000719
Iteration 86/1000 | Loss: 0.00000719
Iteration 87/1000 | Loss: 0.00000719
Iteration 88/1000 | Loss: 0.00000718
Iteration 89/1000 | Loss: 0.00000718
Iteration 90/1000 | Loss: 0.00000718
Iteration 91/1000 | Loss: 0.00000718
Iteration 92/1000 | Loss: 0.00000718
Iteration 93/1000 | Loss: 0.00000718
Iteration 94/1000 | Loss: 0.00000717
Iteration 95/1000 | Loss: 0.00000717
Iteration 96/1000 | Loss: 0.00000717
Iteration 97/1000 | Loss: 0.00000717
Iteration 98/1000 | Loss: 0.00000717
Iteration 99/1000 | Loss: 0.00000717
Iteration 100/1000 | Loss: 0.00000716
Iteration 101/1000 | Loss: 0.00000716
Iteration 102/1000 | Loss: 0.00000716
Iteration 103/1000 | Loss: 0.00000716
Iteration 104/1000 | Loss: 0.00000715
Iteration 105/1000 | Loss: 0.00000715
Iteration 106/1000 | Loss: 0.00000714
Iteration 107/1000 | Loss: 0.00000714
Iteration 108/1000 | Loss: 0.00000714
Iteration 109/1000 | Loss: 0.00000714
Iteration 110/1000 | Loss: 0.00000714
Iteration 111/1000 | Loss: 0.00000714
Iteration 112/1000 | Loss: 0.00000714
Iteration 113/1000 | Loss: 0.00000714
Iteration 114/1000 | Loss: 0.00000714
Iteration 115/1000 | Loss: 0.00000713
Iteration 116/1000 | Loss: 0.00000713
Iteration 117/1000 | Loss: 0.00000713
Iteration 118/1000 | Loss: 0.00000713
Iteration 119/1000 | Loss: 0.00000713
Iteration 120/1000 | Loss: 0.00000713
Iteration 121/1000 | Loss: 0.00000712
Iteration 122/1000 | Loss: 0.00000712
Iteration 123/1000 | Loss: 0.00000712
Iteration 124/1000 | Loss: 0.00000712
Iteration 125/1000 | Loss: 0.00000712
Iteration 126/1000 | Loss: 0.00000712
Iteration 127/1000 | Loss: 0.00000712
Iteration 128/1000 | Loss: 0.00000712
Iteration 129/1000 | Loss: 0.00000712
Iteration 130/1000 | Loss: 0.00000712
Iteration 131/1000 | Loss: 0.00000712
Iteration 132/1000 | Loss: 0.00000711
Iteration 133/1000 | Loss: 0.00000711
Iteration 134/1000 | Loss: 0.00000711
Iteration 135/1000 | Loss: 0.00000711
Iteration 136/1000 | Loss: 0.00000711
Iteration 137/1000 | Loss: 0.00000711
Iteration 138/1000 | Loss: 0.00000711
Iteration 139/1000 | Loss: 0.00000711
Iteration 140/1000 | Loss: 0.00000710
Iteration 141/1000 | Loss: 0.00000710
Iteration 142/1000 | Loss: 0.00000710
Iteration 143/1000 | Loss: 0.00000710
Iteration 144/1000 | Loss: 0.00000710
Iteration 145/1000 | Loss: 0.00000710
Iteration 146/1000 | Loss: 0.00000710
Iteration 147/1000 | Loss: 0.00000710
Iteration 148/1000 | Loss: 0.00000710
Iteration 149/1000 | Loss: 0.00000710
Iteration 150/1000 | Loss: 0.00000710
Iteration 151/1000 | Loss: 0.00000710
Iteration 152/1000 | Loss: 0.00000709
Iteration 153/1000 | Loss: 0.00000709
Iteration 154/1000 | Loss: 0.00000709
Iteration 155/1000 | Loss: 0.00000709
Iteration 156/1000 | Loss: 0.00000709
Iteration 157/1000 | Loss: 0.00000709
Iteration 158/1000 | Loss: 0.00000709
Iteration 159/1000 | Loss: 0.00000709
Iteration 160/1000 | Loss: 0.00000709
Iteration 161/1000 | Loss: 0.00000709
Iteration 162/1000 | Loss: 0.00000709
Iteration 163/1000 | Loss: 0.00000709
Iteration 164/1000 | Loss: 0.00000709
Iteration 165/1000 | Loss: 0.00000709
Iteration 166/1000 | Loss: 0.00000709
Iteration 167/1000 | Loss: 0.00000709
Iteration 168/1000 | Loss: 0.00000709
Iteration 169/1000 | Loss: 0.00000709
Iteration 170/1000 | Loss: 0.00000709
Iteration 171/1000 | Loss: 0.00000709
Iteration 172/1000 | Loss: 0.00000709
Iteration 173/1000 | Loss: 0.00000709
Iteration 174/1000 | Loss: 0.00000709
Iteration 175/1000 | Loss: 0.00000709
Iteration 176/1000 | Loss: 0.00000709
Iteration 177/1000 | Loss: 0.00000709
Iteration 178/1000 | Loss: 0.00000709
Iteration 179/1000 | Loss: 0.00000709
Iteration 180/1000 | Loss: 0.00000709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [7.090073722793022e-06, 7.090073722793022e-06, 7.090073722793022e-06, 7.090073722793022e-06, 7.090073722793022e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.090073722793022e-06

Optimization complete. Final v2v error: 2.3003408908843994 mm

Highest mean error: 2.4537017345428467 mm for frame 30

Lowest mean error: 2.1733455657958984 mm for frame 111

Saving results

Total time: 32.31041622161865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381913
Iteration 2/25 | Loss: 0.00095441
Iteration 3/25 | Loss: 0.00083320
Iteration 4/25 | Loss: 0.00082108
Iteration 5/25 | Loss: 0.00081763
Iteration 6/25 | Loss: 0.00081754
Iteration 7/25 | Loss: 0.00081754
Iteration 8/25 | Loss: 0.00081754
Iteration 9/25 | Loss: 0.00081754
Iteration 10/25 | Loss: 0.00081754
Iteration 11/25 | Loss: 0.00081754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008175431867130101, 0.0008175431867130101, 0.0008175431867130101, 0.0008175431867130101, 0.0008175431867130101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008175431867130101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64131820
Iteration 2/25 | Loss: 0.00030886
Iteration 3/25 | Loss: 0.00030886
Iteration 4/25 | Loss: 0.00030886
Iteration 5/25 | Loss: 0.00030886
Iteration 6/25 | Loss: 0.00030886
Iteration 7/25 | Loss: 0.00030886
Iteration 8/25 | Loss: 0.00030886
Iteration 9/25 | Loss: 0.00030885
Iteration 10/25 | Loss: 0.00030885
Iteration 11/25 | Loss: 0.00030885
Iteration 12/25 | Loss: 0.00030885
Iteration 13/25 | Loss: 0.00030885
Iteration 14/25 | Loss: 0.00030885
Iteration 15/25 | Loss: 0.00030885
Iteration 16/25 | Loss: 0.00030885
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00030885462183505297, 0.00030885462183505297, 0.00030885462183505297, 0.00030885462183505297, 0.00030885462183505297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030885462183505297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030885
Iteration 2/1000 | Loss: 0.00002307
Iteration 3/1000 | Loss: 0.00001387
Iteration 4/1000 | Loss: 0.00001008
Iteration 5/1000 | Loss: 0.00000939
Iteration 6/1000 | Loss: 0.00000885
Iteration 7/1000 | Loss: 0.00000853
Iteration 8/1000 | Loss: 0.00000827
Iteration 9/1000 | Loss: 0.00000807
Iteration 10/1000 | Loss: 0.00000788
Iteration 11/1000 | Loss: 0.00000783
Iteration 12/1000 | Loss: 0.00000783
Iteration 13/1000 | Loss: 0.00000782
Iteration 14/1000 | Loss: 0.00000781
Iteration 15/1000 | Loss: 0.00000781
Iteration 16/1000 | Loss: 0.00000780
Iteration 17/1000 | Loss: 0.00000776
Iteration 18/1000 | Loss: 0.00000776
Iteration 19/1000 | Loss: 0.00000775
Iteration 20/1000 | Loss: 0.00000774
Iteration 21/1000 | Loss: 0.00000774
Iteration 22/1000 | Loss: 0.00000774
Iteration 23/1000 | Loss: 0.00000774
Iteration 24/1000 | Loss: 0.00000770
Iteration 25/1000 | Loss: 0.00000767
Iteration 26/1000 | Loss: 0.00000767
Iteration 27/1000 | Loss: 0.00000766
Iteration 28/1000 | Loss: 0.00000765
Iteration 29/1000 | Loss: 0.00000765
Iteration 30/1000 | Loss: 0.00000764
Iteration 31/1000 | Loss: 0.00000764
Iteration 32/1000 | Loss: 0.00000764
Iteration 33/1000 | Loss: 0.00000763
Iteration 34/1000 | Loss: 0.00000763
Iteration 35/1000 | Loss: 0.00000762
Iteration 36/1000 | Loss: 0.00000761
Iteration 37/1000 | Loss: 0.00000761
Iteration 38/1000 | Loss: 0.00000760
Iteration 39/1000 | Loss: 0.00000759
Iteration 40/1000 | Loss: 0.00000758
Iteration 41/1000 | Loss: 0.00000757
Iteration 42/1000 | Loss: 0.00000756
Iteration 43/1000 | Loss: 0.00000753
Iteration 44/1000 | Loss: 0.00000750
Iteration 45/1000 | Loss: 0.00000748
Iteration 46/1000 | Loss: 0.00000748
Iteration 47/1000 | Loss: 0.00000748
Iteration 48/1000 | Loss: 0.00000748
Iteration 49/1000 | Loss: 0.00000748
Iteration 50/1000 | Loss: 0.00000748
Iteration 51/1000 | Loss: 0.00000748
Iteration 52/1000 | Loss: 0.00000748
Iteration 53/1000 | Loss: 0.00000748
Iteration 54/1000 | Loss: 0.00000748
Iteration 55/1000 | Loss: 0.00000747
Iteration 56/1000 | Loss: 0.00000746
Iteration 57/1000 | Loss: 0.00000746
Iteration 58/1000 | Loss: 0.00000745
Iteration 59/1000 | Loss: 0.00000745
Iteration 60/1000 | Loss: 0.00000745
Iteration 61/1000 | Loss: 0.00000744
Iteration 62/1000 | Loss: 0.00000744
Iteration 63/1000 | Loss: 0.00000744
Iteration 64/1000 | Loss: 0.00000743
Iteration 65/1000 | Loss: 0.00000743
Iteration 66/1000 | Loss: 0.00000743
Iteration 67/1000 | Loss: 0.00000743
Iteration 68/1000 | Loss: 0.00000742
Iteration 69/1000 | Loss: 0.00000742
Iteration 70/1000 | Loss: 0.00000742
Iteration 71/1000 | Loss: 0.00000742
Iteration 72/1000 | Loss: 0.00000741
Iteration 73/1000 | Loss: 0.00000741
Iteration 74/1000 | Loss: 0.00000741
Iteration 75/1000 | Loss: 0.00000740
Iteration 76/1000 | Loss: 0.00000740
Iteration 77/1000 | Loss: 0.00000740
Iteration 78/1000 | Loss: 0.00000740
Iteration 79/1000 | Loss: 0.00000740
Iteration 80/1000 | Loss: 0.00000740
Iteration 81/1000 | Loss: 0.00000739
Iteration 82/1000 | Loss: 0.00000739
Iteration 83/1000 | Loss: 0.00000739
Iteration 84/1000 | Loss: 0.00000739
Iteration 85/1000 | Loss: 0.00000738
Iteration 86/1000 | Loss: 0.00000738
Iteration 87/1000 | Loss: 0.00000738
Iteration 88/1000 | Loss: 0.00000738
Iteration 89/1000 | Loss: 0.00000737
Iteration 90/1000 | Loss: 0.00000737
Iteration 91/1000 | Loss: 0.00000737
Iteration 92/1000 | Loss: 0.00000737
Iteration 93/1000 | Loss: 0.00000737
Iteration 94/1000 | Loss: 0.00000737
Iteration 95/1000 | Loss: 0.00000737
Iteration 96/1000 | Loss: 0.00000737
Iteration 97/1000 | Loss: 0.00000737
Iteration 98/1000 | Loss: 0.00000737
Iteration 99/1000 | Loss: 0.00000737
Iteration 100/1000 | Loss: 0.00000737
Iteration 101/1000 | Loss: 0.00000737
Iteration 102/1000 | Loss: 0.00000737
Iteration 103/1000 | Loss: 0.00000737
Iteration 104/1000 | Loss: 0.00000737
Iteration 105/1000 | Loss: 0.00000737
Iteration 106/1000 | Loss: 0.00000737
Iteration 107/1000 | Loss: 0.00000737
Iteration 108/1000 | Loss: 0.00000737
Iteration 109/1000 | Loss: 0.00000737
Iteration 110/1000 | Loss: 0.00000737
Iteration 111/1000 | Loss: 0.00000737
Iteration 112/1000 | Loss: 0.00000737
Iteration 113/1000 | Loss: 0.00000737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [7.371450919890776e-06, 7.371450919890776e-06, 7.371450919890776e-06, 7.371450919890776e-06, 7.371450919890776e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.371450919890776e-06

Optimization complete. Final v2v error: 2.3233859539031982 mm

Highest mean error: 2.7454674243927 mm for frame 218

Lowest mean error: 2.0404086112976074 mm for frame 244

Saving results

Total time: 36.37071681022644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495134
Iteration 2/25 | Loss: 0.00109356
Iteration 3/25 | Loss: 0.00096253
Iteration 4/25 | Loss: 0.00093387
Iteration 5/25 | Loss: 0.00092471
Iteration 6/25 | Loss: 0.00092220
Iteration 7/25 | Loss: 0.00092121
Iteration 8/25 | Loss: 0.00092121
Iteration 9/25 | Loss: 0.00092121
Iteration 10/25 | Loss: 0.00092121
Iteration 11/25 | Loss: 0.00092121
Iteration 12/25 | Loss: 0.00092121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009212125441990793, 0.0009212125441990793, 0.0009212125441990793, 0.0009212125441990793, 0.0009212125441990793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009212125441990793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56942904
Iteration 2/25 | Loss: 0.00062529
Iteration 3/25 | Loss: 0.00062527
Iteration 4/25 | Loss: 0.00062527
Iteration 5/25 | Loss: 0.00062527
Iteration 6/25 | Loss: 0.00062527
Iteration 7/25 | Loss: 0.00062527
Iteration 8/25 | Loss: 0.00062527
Iteration 9/25 | Loss: 0.00062527
Iteration 10/25 | Loss: 0.00062527
Iteration 11/25 | Loss: 0.00062527
Iteration 12/25 | Loss: 0.00062527
Iteration 13/25 | Loss: 0.00062527
Iteration 14/25 | Loss: 0.00062527
Iteration 15/25 | Loss: 0.00062527
Iteration 16/25 | Loss: 0.00062527
Iteration 17/25 | Loss: 0.00062527
Iteration 18/25 | Loss: 0.00062527
Iteration 19/25 | Loss: 0.00062527
Iteration 20/25 | Loss: 0.00062527
Iteration 21/25 | Loss: 0.00062527
Iteration 22/25 | Loss: 0.00062527
Iteration 23/25 | Loss: 0.00062527
Iteration 24/25 | Loss: 0.00062527
Iteration 25/25 | Loss: 0.00062527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062527
Iteration 2/1000 | Loss: 0.00004766
Iteration 3/1000 | Loss: 0.00003249
Iteration 4/1000 | Loss: 0.00002747
Iteration 5/1000 | Loss: 0.00002579
Iteration 6/1000 | Loss: 0.00002444
Iteration 7/1000 | Loss: 0.00002368
Iteration 8/1000 | Loss: 0.00002316
Iteration 9/1000 | Loss: 0.00002276
Iteration 10/1000 | Loss: 0.00002250
Iteration 11/1000 | Loss: 0.00002228
Iteration 12/1000 | Loss: 0.00002207
Iteration 13/1000 | Loss: 0.00002204
Iteration 14/1000 | Loss: 0.00002200
Iteration 15/1000 | Loss: 0.00002198
Iteration 16/1000 | Loss: 0.00002196
Iteration 17/1000 | Loss: 0.00002195
Iteration 18/1000 | Loss: 0.00002183
Iteration 19/1000 | Loss: 0.00002180
Iteration 20/1000 | Loss: 0.00002179
Iteration 21/1000 | Loss: 0.00002178
Iteration 22/1000 | Loss: 0.00002177
Iteration 23/1000 | Loss: 0.00002177
Iteration 24/1000 | Loss: 0.00002176
Iteration 25/1000 | Loss: 0.00002172
Iteration 26/1000 | Loss: 0.00002170
Iteration 27/1000 | Loss: 0.00002170
Iteration 28/1000 | Loss: 0.00002169
Iteration 29/1000 | Loss: 0.00002168
Iteration 30/1000 | Loss: 0.00002167
Iteration 31/1000 | Loss: 0.00002166
Iteration 32/1000 | Loss: 0.00002166
Iteration 33/1000 | Loss: 0.00002165
Iteration 34/1000 | Loss: 0.00002165
Iteration 35/1000 | Loss: 0.00002164
Iteration 36/1000 | Loss: 0.00002164
Iteration 37/1000 | Loss: 0.00002160
Iteration 38/1000 | Loss: 0.00002157
Iteration 39/1000 | Loss: 0.00002157
Iteration 40/1000 | Loss: 0.00002157
Iteration 41/1000 | Loss: 0.00002155
Iteration 42/1000 | Loss: 0.00002154
Iteration 43/1000 | Loss: 0.00002151
Iteration 44/1000 | Loss: 0.00002151
Iteration 45/1000 | Loss: 0.00002150
Iteration 46/1000 | Loss: 0.00002149
Iteration 47/1000 | Loss: 0.00002149
Iteration 48/1000 | Loss: 0.00002148
Iteration 49/1000 | Loss: 0.00002148
Iteration 50/1000 | Loss: 0.00002148
Iteration 51/1000 | Loss: 0.00002147
Iteration 52/1000 | Loss: 0.00002147
Iteration 53/1000 | Loss: 0.00002146
Iteration 54/1000 | Loss: 0.00002146
Iteration 55/1000 | Loss: 0.00002146
Iteration 56/1000 | Loss: 0.00002146
Iteration 57/1000 | Loss: 0.00002145
Iteration 58/1000 | Loss: 0.00002145
Iteration 59/1000 | Loss: 0.00002145
Iteration 60/1000 | Loss: 0.00002145
Iteration 61/1000 | Loss: 0.00002145
Iteration 62/1000 | Loss: 0.00002145
Iteration 63/1000 | Loss: 0.00002145
Iteration 64/1000 | Loss: 0.00002144
Iteration 65/1000 | Loss: 0.00002144
Iteration 66/1000 | Loss: 0.00002144
Iteration 67/1000 | Loss: 0.00002143
Iteration 68/1000 | Loss: 0.00002143
Iteration 69/1000 | Loss: 0.00002143
Iteration 70/1000 | Loss: 0.00002143
Iteration 71/1000 | Loss: 0.00002143
Iteration 72/1000 | Loss: 0.00002143
Iteration 73/1000 | Loss: 0.00002143
Iteration 74/1000 | Loss: 0.00002142
Iteration 75/1000 | Loss: 0.00002142
Iteration 76/1000 | Loss: 0.00002142
Iteration 77/1000 | Loss: 0.00002142
Iteration 78/1000 | Loss: 0.00002142
Iteration 79/1000 | Loss: 0.00002142
Iteration 80/1000 | Loss: 0.00002141
Iteration 81/1000 | Loss: 0.00002141
Iteration 82/1000 | Loss: 0.00002141
Iteration 83/1000 | Loss: 0.00002141
Iteration 84/1000 | Loss: 0.00002141
Iteration 85/1000 | Loss: 0.00002141
Iteration 86/1000 | Loss: 0.00002141
Iteration 87/1000 | Loss: 0.00002141
Iteration 88/1000 | Loss: 0.00002141
Iteration 89/1000 | Loss: 0.00002141
Iteration 90/1000 | Loss: 0.00002141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [2.1412073692772537e-05, 2.1412073692772537e-05, 2.1412073692772537e-05, 2.1412073692772537e-05, 2.1412073692772537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1412073692772537e-05

Optimization complete. Final v2v error: 3.7353951930999756 mm

Highest mean error: 4.8863749504089355 mm for frame 40

Lowest mean error: 2.928152561187744 mm for frame 174

Saving results

Total time: 43.638360023498535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412383
Iteration 2/25 | Loss: 0.00092409
Iteration 3/25 | Loss: 0.00084702
Iteration 4/25 | Loss: 0.00082843
Iteration 5/25 | Loss: 0.00082211
Iteration 6/25 | Loss: 0.00082052
Iteration 7/25 | Loss: 0.00082000
Iteration 8/25 | Loss: 0.00081989
Iteration 9/25 | Loss: 0.00081989
Iteration 10/25 | Loss: 0.00081989
Iteration 11/25 | Loss: 0.00081989
Iteration 12/25 | Loss: 0.00081989
Iteration 13/25 | Loss: 0.00081989
Iteration 14/25 | Loss: 0.00081989
Iteration 15/25 | Loss: 0.00081989
Iteration 16/25 | Loss: 0.00081989
Iteration 17/25 | Loss: 0.00081989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008198882569558918, 0.0008198882569558918, 0.0008198882569558918, 0.0008198882569558918, 0.0008198882569558918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008198882569558918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56098855
Iteration 2/25 | Loss: 0.00046040
Iteration 3/25 | Loss: 0.00046040
Iteration 4/25 | Loss: 0.00046040
Iteration 5/25 | Loss: 0.00046040
Iteration 6/25 | Loss: 0.00046040
Iteration 7/25 | Loss: 0.00046040
Iteration 8/25 | Loss: 0.00046040
Iteration 9/25 | Loss: 0.00046040
Iteration 10/25 | Loss: 0.00046040
Iteration 11/25 | Loss: 0.00046040
Iteration 12/25 | Loss: 0.00046040
Iteration 13/25 | Loss: 0.00046040
Iteration 14/25 | Loss: 0.00046040
Iteration 15/25 | Loss: 0.00046040
Iteration 16/25 | Loss: 0.00046040
Iteration 17/25 | Loss: 0.00046040
Iteration 18/25 | Loss: 0.00046040
Iteration 19/25 | Loss: 0.00046040
Iteration 20/25 | Loss: 0.00046040
Iteration 21/25 | Loss: 0.00046040
Iteration 22/25 | Loss: 0.00046040
Iteration 23/25 | Loss: 0.00046040
Iteration 24/25 | Loss: 0.00046040
Iteration 25/25 | Loss: 0.00046040

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046040
Iteration 2/1000 | Loss: 0.00002335
Iteration 3/1000 | Loss: 0.00001456
Iteration 4/1000 | Loss: 0.00001248
Iteration 5/1000 | Loss: 0.00001163
Iteration 6/1000 | Loss: 0.00001114
Iteration 7/1000 | Loss: 0.00001090
Iteration 8/1000 | Loss: 0.00001059
Iteration 9/1000 | Loss: 0.00001053
Iteration 10/1000 | Loss: 0.00001043
Iteration 11/1000 | Loss: 0.00001043
Iteration 12/1000 | Loss: 0.00001039
Iteration 13/1000 | Loss: 0.00001028
Iteration 14/1000 | Loss: 0.00001027
Iteration 15/1000 | Loss: 0.00001026
Iteration 16/1000 | Loss: 0.00001022
Iteration 17/1000 | Loss: 0.00001021
Iteration 18/1000 | Loss: 0.00001020
Iteration 19/1000 | Loss: 0.00001020
Iteration 20/1000 | Loss: 0.00001019
Iteration 21/1000 | Loss: 0.00001019
Iteration 22/1000 | Loss: 0.00001016
Iteration 23/1000 | Loss: 0.00001016
Iteration 24/1000 | Loss: 0.00001016
Iteration 25/1000 | Loss: 0.00001016
Iteration 26/1000 | Loss: 0.00001016
Iteration 27/1000 | Loss: 0.00001015
Iteration 28/1000 | Loss: 0.00001015
Iteration 29/1000 | Loss: 0.00001014
Iteration 30/1000 | Loss: 0.00001014
Iteration 31/1000 | Loss: 0.00001013
Iteration 32/1000 | Loss: 0.00001012
Iteration 33/1000 | Loss: 0.00001012
Iteration 34/1000 | Loss: 0.00001011
Iteration 35/1000 | Loss: 0.00001011
Iteration 36/1000 | Loss: 0.00001011
Iteration 37/1000 | Loss: 0.00001010
Iteration 38/1000 | Loss: 0.00001009
Iteration 39/1000 | Loss: 0.00001009
Iteration 40/1000 | Loss: 0.00001009
Iteration 41/1000 | Loss: 0.00001009
Iteration 42/1000 | Loss: 0.00001009
Iteration 43/1000 | Loss: 0.00001008
Iteration 44/1000 | Loss: 0.00001008
Iteration 45/1000 | Loss: 0.00001008
Iteration 46/1000 | Loss: 0.00001008
Iteration 47/1000 | Loss: 0.00001007
Iteration 48/1000 | Loss: 0.00001007
Iteration 49/1000 | Loss: 0.00001007
Iteration 50/1000 | Loss: 0.00001006
Iteration 51/1000 | Loss: 0.00001006
Iteration 52/1000 | Loss: 0.00001005
Iteration 53/1000 | Loss: 0.00001004
Iteration 54/1000 | Loss: 0.00001004
Iteration 55/1000 | Loss: 0.00001003
Iteration 56/1000 | Loss: 0.00001003
Iteration 57/1000 | Loss: 0.00001003
Iteration 58/1000 | Loss: 0.00001002
Iteration 59/1000 | Loss: 0.00001002
Iteration 60/1000 | Loss: 0.00001002
Iteration 61/1000 | Loss: 0.00001001
Iteration 62/1000 | Loss: 0.00001001
Iteration 63/1000 | Loss: 0.00001000
Iteration 64/1000 | Loss: 0.00001000
Iteration 65/1000 | Loss: 0.00001000
Iteration 66/1000 | Loss: 0.00000999
Iteration 67/1000 | Loss: 0.00000999
Iteration 68/1000 | Loss: 0.00000999
Iteration 69/1000 | Loss: 0.00000998
Iteration 70/1000 | Loss: 0.00000998
Iteration 71/1000 | Loss: 0.00000998
Iteration 72/1000 | Loss: 0.00000998
Iteration 73/1000 | Loss: 0.00000998
Iteration 74/1000 | Loss: 0.00000998
Iteration 75/1000 | Loss: 0.00000997
Iteration 76/1000 | Loss: 0.00000997
Iteration 77/1000 | Loss: 0.00000997
Iteration 78/1000 | Loss: 0.00000997
Iteration 79/1000 | Loss: 0.00000996
Iteration 80/1000 | Loss: 0.00000996
Iteration 81/1000 | Loss: 0.00000996
Iteration 82/1000 | Loss: 0.00000996
Iteration 83/1000 | Loss: 0.00000996
Iteration 84/1000 | Loss: 0.00000996
Iteration 85/1000 | Loss: 0.00000996
Iteration 86/1000 | Loss: 0.00000996
Iteration 87/1000 | Loss: 0.00000996
Iteration 88/1000 | Loss: 0.00000996
Iteration 89/1000 | Loss: 0.00000996
Iteration 90/1000 | Loss: 0.00000995
Iteration 91/1000 | Loss: 0.00000995
Iteration 92/1000 | Loss: 0.00000995
Iteration 93/1000 | Loss: 0.00000995
Iteration 94/1000 | Loss: 0.00000995
Iteration 95/1000 | Loss: 0.00000994
Iteration 96/1000 | Loss: 0.00000994
Iteration 97/1000 | Loss: 0.00000994
Iteration 98/1000 | Loss: 0.00000994
Iteration 99/1000 | Loss: 0.00000994
Iteration 100/1000 | Loss: 0.00000994
Iteration 101/1000 | Loss: 0.00000993
Iteration 102/1000 | Loss: 0.00000993
Iteration 103/1000 | Loss: 0.00000993
Iteration 104/1000 | Loss: 0.00000993
Iteration 105/1000 | Loss: 0.00000993
Iteration 106/1000 | Loss: 0.00000992
Iteration 107/1000 | Loss: 0.00000992
Iteration 108/1000 | Loss: 0.00000992
Iteration 109/1000 | Loss: 0.00000992
Iteration 110/1000 | Loss: 0.00000992
Iteration 111/1000 | Loss: 0.00000992
Iteration 112/1000 | Loss: 0.00000991
Iteration 113/1000 | Loss: 0.00000991
Iteration 114/1000 | Loss: 0.00000991
Iteration 115/1000 | Loss: 0.00000991
Iteration 116/1000 | Loss: 0.00000990
Iteration 117/1000 | Loss: 0.00000990
Iteration 118/1000 | Loss: 0.00000990
Iteration 119/1000 | Loss: 0.00000990
Iteration 120/1000 | Loss: 0.00000990
Iteration 121/1000 | Loss: 0.00000990
Iteration 122/1000 | Loss: 0.00000989
Iteration 123/1000 | Loss: 0.00000989
Iteration 124/1000 | Loss: 0.00000989
Iteration 125/1000 | Loss: 0.00000989
Iteration 126/1000 | Loss: 0.00000989
Iteration 127/1000 | Loss: 0.00000988
Iteration 128/1000 | Loss: 0.00000988
Iteration 129/1000 | Loss: 0.00000988
Iteration 130/1000 | Loss: 0.00000988
Iteration 131/1000 | Loss: 0.00000987
Iteration 132/1000 | Loss: 0.00000987
Iteration 133/1000 | Loss: 0.00000987
Iteration 134/1000 | Loss: 0.00000987
Iteration 135/1000 | Loss: 0.00000987
Iteration 136/1000 | Loss: 0.00000987
Iteration 137/1000 | Loss: 0.00000987
Iteration 138/1000 | Loss: 0.00000987
Iteration 139/1000 | Loss: 0.00000987
Iteration 140/1000 | Loss: 0.00000987
Iteration 141/1000 | Loss: 0.00000987
Iteration 142/1000 | Loss: 0.00000987
Iteration 143/1000 | Loss: 0.00000987
Iteration 144/1000 | Loss: 0.00000987
Iteration 145/1000 | Loss: 0.00000987
Iteration 146/1000 | Loss: 0.00000987
Iteration 147/1000 | Loss: 0.00000987
Iteration 148/1000 | Loss: 0.00000987
Iteration 149/1000 | Loss: 0.00000987
Iteration 150/1000 | Loss: 0.00000987
Iteration 151/1000 | Loss: 0.00000987
Iteration 152/1000 | Loss: 0.00000987
Iteration 153/1000 | Loss: 0.00000987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [9.869382665783633e-06, 9.869382665783633e-06, 9.869382665783633e-06, 9.869382665783633e-06, 9.869382665783633e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.869382665783633e-06

Optimization complete. Final v2v error: 2.696387767791748 mm

Highest mean error: 3.087470054626465 mm for frame 78

Lowest mean error: 2.5511746406555176 mm for frame 108

Saving results

Total time: 36.130187034606934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01094592
Iteration 2/25 | Loss: 0.01094592
Iteration 3/25 | Loss: 0.00484510
Iteration 4/25 | Loss: 0.00254824
Iteration 5/25 | Loss: 0.00196767
Iteration 6/25 | Loss: 0.00180403
Iteration 7/25 | Loss: 0.00208283
Iteration 8/25 | Loss: 0.00159560
Iteration 9/25 | Loss: 0.00150007
Iteration 10/25 | Loss: 0.00145913
Iteration 11/25 | Loss: 0.00142847
Iteration 12/25 | Loss: 0.00140021
Iteration 13/25 | Loss: 0.00138892
Iteration 14/25 | Loss: 0.00138064
Iteration 15/25 | Loss: 0.00137466
Iteration 16/25 | Loss: 0.00136754
Iteration 17/25 | Loss: 0.00135860
Iteration 18/25 | Loss: 0.00135726
Iteration 19/25 | Loss: 0.00135833
Iteration 20/25 | Loss: 0.00136190
Iteration 21/25 | Loss: 0.00136361
Iteration 22/25 | Loss: 0.00135955
Iteration 23/25 | Loss: 0.00136351
Iteration 24/25 | Loss: 0.00135742
Iteration 25/25 | Loss: 0.00135677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56024444
Iteration 2/25 | Loss: 0.00296540
Iteration 3/25 | Loss: 0.00211881
Iteration 4/25 | Loss: 0.00211845
Iteration 5/25 | Loss: 0.00211845
Iteration 6/25 | Loss: 0.00211845
Iteration 7/25 | Loss: 0.00211845
Iteration 8/25 | Loss: 0.00211845
Iteration 9/25 | Loss: 0.00211845
Iteration 10/25 | Loss: 0.00211845
Iteration 11/25 | Loss: 0.00211845
Iteration 12/25 | Loss: 0.00211845
Iteration 13/25 | Loss: 0.00211845
Iteration 14/25 | Loss: 0.00211845
Iteration 15/25 | Loss: 0.00211845
Iteration 16/25 | Loss: 0.00211845
Iteration 17/25 | Loss: 0.00211845
Iteration 18/25 | Loss: 0.00211845
Iteration 19/25 | Loss: 0.00211845
Iteration 20/25 | Loss: 0.00211845
Iteration 21/25 | Loss: 0.00211845
Iteration 22/25 | Loss: 0.00211845
Iteration 23/25 | Loss: 0.00211845
Iteration 24/25 | Loss: 0.00211845
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0021184454672038555, 0.0021184454672038555, 0.0021184454672038555, 0.0021184454672038555, 0.0021184454672038555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021184454672038555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211845
Iteration 2/1000 | Loss: 0.00044930
Iteration 3/1000 | Loss: 0.00092743
Iteration 4/1000 | Loss: 0.00040908
Iteration 5/1000 | Loss: 0.00111676
Iteration 6/1000 | Loss: 0.00022890
Iteration 7/1000 | Loss: 0.00023157
Iteration 8/1000 | Loss: 0.00020535
Iteration 9/1000 | Loss: 0.00035074
Iteration 10/1000 | Loss: 0.00032919
Iteration 11/1000 | Loss: 0.00030024
Iteration 12/1000 | Loss: 0.00036676
Iteration 13/1000 | Loss: 0.00019481
Iteration 14/1000 | Loss: 0.00056996
Iteration 15/1000 | Loss: 0.00027510
Iteration 16/1000 | Loss: 0.00030590
Iteration 17/1000 | Loss: 0.00031124
Iteration 18/1000 | Loss: 0.00026528
Iteration 19/1000 | Loss: 0.00019796
Iteration 20/1000 | Loss: 0.00018263
Iteration 21/1000 | Loss: 0.00019846
Iteration 22/1000 | Loss: 0.00043067
Iteration 23/1000 | Loss: 0.00162804
Iteration 24/1000 | Loss: 0.00589033
Iteration 25/1000 | Loss: 0.00271513
Iteration 26/1000 | Loss: 0.00045584
Iteration 27/1000 | Loss: 0.00046112
Iteration 28/1000 | Loss: 0.00034728
Iteration 29/1000 | Loss: 0.00029054
Iteration 30/1000 | Loss: 0.00022864
Iteration 31/1000 | Loss: 0.00040886
Iteration 32/1000 | Loss: 0.00014577
Iteration 33/1000 | Loss: 0.00010591
Iteration 34/1000 | Loss: 0.00085552
Iteration 35/1000 | Loss: 0.00012876
Iteration 36/1000 | Loss: 0.00009637
Iteration 37/1000 | Loss: 0.00006906
Iteration 38/1000 | Loss: 0.00011713
Iteration 39/1000 | Loss: 0.00005059
Iteration 40/1000 | Loss: 0.00008028
Iteration 41/1000 | Loss: 0.00014598
Iteration 42/1000 | Loss: 0.00003317
Iteration 43/1000 | Loss: 0.00038319
Iteration 44/1000 | Loss: 0.00020924
Iteration 45/1000 | Loss: 0.00003124
Iteration 46/1000 | Loss: 0.00002368
Iteration 47/1000 | Loss: 0.00004633
Iteration 48/1000 | Loss: 0.00004645
Iteration 49/1000 | Loss: 0.00010607
Iteration 50/1000 | Loss: 0.00045236
Iteration 51/1000 | Loss: 0.00006488
Iteration 52/1000 | Loss: 0.00004018
Iteration 53/1000 | Loss: 0.00002506
Iteration 54/1000 | Loss: 0.00004643
Iteration 55/1000 | Loss: 0.00002333
Iteration 56/1000 | Loss: 0.00002025
Iteration 57/1000 | Loss: 0.00002525
Iteration 58/1000 | Loss: 0.00001987
Iteration 59/1000 | Loss: 0.00002497
Iteration 60/1000 | Loss: 0.00007513
Iteration 61/1000 | Loss: 0.00003678
Iteration 62/1000 | Loss: 0.00008650
Iteration 63/1000 | Loss: 0.00008031
Iteration 64/1000 | Loss: 0.00006201
Iteration 65/1000 | Loss: 0.00003521
Iteration 66/1000 | Loss: 0.00006213
Iteration 67/1000 | Loss: 0.00003782
Iteration 68/1000 | Loss: 0.00006324
Iteration 69/1000 | Loss: 0.00003266
Iteration 70/1000 | Loss: 0.00004050
Iteration 71/1000 | Loss: 0.00002044
Iteration 72/1000 | Loss: 0.00003379
Iteration 73/1000 | Loss: 0.00003819
Iteration 74/1000 | Loss: 0.00003243
Iteration 75/1000 | Loss: 0.00002617
Iteration 76/1000 | Loss: 0.00003109
Iteration 77/1000 | Loss: 0.00002196
Iteration 78/1000 | Loss: 0.00003638
Iteration 79/1000 | Loss: 0.00002109
Iteration 80/1000 | Loss: 0.00003703
Iteration 81/1000 | Loss: 0.00002216
Iteration 82/1000 | Loss: 0.00004361
Iteration 83/1000 | Loss: 0.00033521
Iteration 84/1000 | Loss: 0.00029128
Iteration 85/1000 | Loss: 0.00017988
Iteration 86/1000 | Loss: 0.00007445
Iteration 87/1000 | Loss: 0.00004089
Iteration 88/1000 | Loss: 0.00020063
Iteration 89/1000 | Loss: 0.00002917
Iteration 90/1000 | Loss: 0.00001939
Iteration 91/1000 | Loss: 0.00004005
Iteration 92/1000 | Loss: 0.00001906
Iteration 93/1000 | Loss: 0.00001892
Iteration 94/1000 | Loss: 0.00001889
Iteration 95/1000 | Loss: 0.00002368
Iteration 96/1000 | Loss: 0.00001882
Iteration 97/1000 | Loss: 0.00001881
Iteration 98/1000 | Loss: 0.00001881
Iteration 99/1000 | Loss: 0.00001880
Iteration 100/1000 | Loss: 0.00001879
Iteration 101/1000 | Loss: 0.00002274
Iteration 102/1000 | Loss: 0.00001875
Iteration 103/1000 | Loss: 0.00001875
Iteration 104/1000 | Loss: 0.00001875
Iteration 105/1000 | Loss: 0.00001875
Iteration 106/1000 | Loss: 0.00001874
Iteration 107/1000 | Loss: 0.00001873
Iteration 108/1000 | Loss: 0.00001873
Iteration 109/1000 | Loss: 0.00001873
Iteration 110/1000 | Loss: 0.00001873
Iteration 111/1000 | Loss: 0.00001873
Iteration 112/1000 | Loss: 0.00001873
Iteration 113/1000 | Loss: 0.00001872
Iteration 114/1000 | Loss: 0.00001872
Iteration 115/1000 | Loss: 0.00001872
Iteration 116/1000 | Loss: 0.00001872
Iteration 117/1000 | Loss: 0.00001872
Iteration 118/1000 | Loss: 0.00001871
Iteration 119/1000 | Loss: 0.00001871
Iteration 120/1000 | Loss: 0.00001871
Iteration 121/1000 | Loss: 0.00001871
Iteration 122/1000 | Loss: 0.00001871
Iteration 123/1000 | Loss: 0.00001871
Iteration 124/1000 | Loss: 0.00001870
Iteration 125/1000 | Loss: 0.00001870
Iteration 126/1000 | Loss: 0.00001870
Iteration 127/1000 | Loss: 0.00001870
Iteration 128/1000 | Loss: 0.00001870
Iteration 129/1000 | Loss: 0.00001870
Iteration 130/1000 | Loss: 0.00001870
Iteration 131/1000 | Loss: 0.00001870
Iteration 132/1000 | Loss: 0.00001870
Iteration 133/1000 | Loss: 0.00001870
Iteration 134/1000 | Loss: 0.00001870
Iteration 135/1000 | Loss: 0.00001870
Iteration 136/1000 | Loss: 0.00001869
Iteration 137/1000 | Loss: 0.00001869
Iteration 138/1000 | Loss: 0.00002671
Iteration 139/1000 | Loss: 0.00001868
Iteration 140/1000 | Loss: 0.00001867
Iteration 141/1000 | Loss: 0.00001867
Iteration 142/1000 | Loss: 0.00001867
Iteration 143/1000 | Loss: 0.00001867
Iteration 144/1000 | Loss: 0.00001867
Iteration 145/1000 | Loss: 0.00001867
Iteration 146/1000 | Loss: 0.00001867
Iteration 147/1000 | Loss: 0.00001867
Iteration 148/1000 | Loss: 0.00001867
Iteration 149/1000 | Loss: 0.00001867
Iteration 150/1000 | Loss: 0.00001866
Iteration 151/1000 | Loss: 0.00001866
Iteration 152/1000 | Loss: 0.00001866
Iteration 153/1000 | Loss: 0.00001866
Iteration 154/1000 | Loss: 0.00001866
Iteration 155/1000 | Loss: 0.00001866
Iteration 156/1000 | Loss: 0.00001866
Iteration 157/1000 | Loss: 0.00001866
Iteration 158/1000 | Loss: 0.00001866
Iteration 159/1000 | Loss: 0.00001866
Iteration 160/1000 | Loss: 0.00001866
Iteration 161/1000 | Loss: 0.00001866
Iteration 162/1000 | Loss: 0.00001866
Iteration 163/1000 | Loss: 0.00001866
Iteration 164/1000 | Loss: 0.00001866
Iteration 165/1000 | Loss: 0.00001866
Iteration 166/1000 | Loss: 0.00001866
Iteration 167/1000 | Loss: 0.00001866
Iteration 168/1000 | Loss: 0.00001866
Iteration 169/1000 | Loss: 0.00001866
Iteration 170/1000 | Loss: 0.00001866
Iteration 171/1000 | Loss: 0.00001866
Iteration 172/1000 | Loss: 0.00001866
Iteration 173/1000 | Loss: 0.00001866
Iteration 174/1000 | Loss: 0.00001866
Iteration 175/1000 | Loss: 0.00001866
Iteration 176/1000 | Loss: 0.00001866
Iteration 177/1000 | Loss: 0.00001866
Iteration 178/1000 | Loss: 0.00001866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.8662127331481315e-05, 1.8662127331481315e-05, 1.8662127331481315e-05, 1.8662127331481315e-05, 1.8662127331481315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8662127331481315e-05

Optimization complete. Final v2v error: 3.5626444816589355 mm

Highest mean error: 4.315193176269531 mm for frame 122

Lowest mean error: 3.2827725410461426 mm for frame 123

Saving results

Total time: 188.7391266822815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00594465
Iteration 2/25 | Loss: 0.00120233
Iteration 3/25 | Loss: 0.00104173
Iteration 4/25 | Loss: 0.00086098
Iteration 5/25 | Loss: 0.00084564
Iteration 6/25 | Loss: 0.00085469
Iteration 7/25 | Loss: 0.00083833
Iteration 8/25 | Loss: 0.00083306
Iteration 9/25 | Loss: 0.00082880
Iteration 10/25 | Loss: 0.00082710
Iteration 11/25 | Loss: 0.00082626
Iteration 12/25 | Loss: 0.00082593
Iteration 13/25 | Loss: 0.00082579
Iteration 14/25 | Loss: 0.00082578
Iteration 15/25 | Loss: 0.00082578
Iteration 16/25 | Loss: 0.00082578
Iteration 17/25 | Loss: 0.00082577
Iteration 18/25 | Loss: 0.00082577
Iteration 19/25 | Loss: 0.00082577
Iteration 20/25 | Loss: 0.00082577
Iteration 21/25 | Loss: 0.00082577
Iteration 22/25 | Loss: 0.00082577
Iteration 23/25 | Loss: 0.00082577
Iteration 24/25 | Loss: 0.00082577
Iteration 25/25 | Loss: 0.00082577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.23304558
Iteration 2/25 | Loss: 0.00044084
Iteration 3/25 | Loss: 0.00044084
Iteration 4/25 | Loss: 0.00044083
Iteration 5/25 | Loss: 0.00044083
Iteration 6/25 | Loss: 0.00044083
Iteration 7/25 | Loss: 0.00044083
Iteration 8/25 | Loss: 0.00044083
Iteration 9/25 | Loss: 0.00044083
Iteration 10/25 | Loss: 0.00044083
Iteration 11/25 | Loss: 0.00044083
Iteration 12/25 | Loss: 0.00044083
Iteration 13/25 | Loss: 0.00044083
Iteration 14/25 | Loss: 0.00044083
Iteration 15/25 | Loss: 0.00044083
Iteration 16/25 | Loss: 0.00044083
Iteration 17/25 | Loss: 0.00044083
Iteration 18/25 | Loss: 0.00044083
Iteration 19/25 | Loss: 0.00044083
Iteration 20/25 | Loss: 0.00044083
Iteration 21/25 | Loss: 0.00044083
Iteration 22/25 | Loss: 0.00044083
Iteration 23/25 | Loss: 0.00044083
Iteration 24/25 | Loss: 0.00044083
Iteration 25/25 | Loss: 0.00044083

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044083
Iteration 2/1000 | Loss: 0.00001962
Iteration 3/1000 | Loss: 0.00001183
Iteration 4/1000 | Loss: 0.00001015
Iteration 5/1000 | Loss: 0.00000951
Iteration 6/1000 | Loss: 0.00000915
Iteration 7/1000 | Loss: 0.00000886
Iteration 8/1000 | Loss: 0.00000878
Iteration 9/1000 | Loss: 0.00000873
Iteration 10/1000 | Loss: 0.00000870
Iteration 11/1000 | Loss: 0.00000869
Iteration 12/1000 | Loss: 0.00000862
Iteration 13/1000 | Loss: 0.00000852
Iteration 14/1000 | Loss: 0.00000846
Iteration 15/1000 | Loss: 0.00000844
Iteration 16/1000 | Loss: 0.00000844
Iteration 17/1000 | Loss: 0.00000843
Iteration 18/1000 | Loss: 0.00000843
Iteration 19/1000 | Loss: 0.00000842
Iteration 20/1000 | Loss: 0.00000841
Iteration 21/1000 | Loss: 0.00000841
Iteration 22/1000 | Loss: 0.00000839
Iteration 23/1000 | Loss: 0.00000839
Iteration 24/1000 | Loss: 0.00000839
Iteration 25/1000 | Loss: 0.00000838
Iteration 26/1000 | Loss: 0.00000838
Iteration 27/1000 | Loss: 0.00000838
Iteration 28/1000 | Loss: 0.00000838
Iteration 29/1000 | Loss: 0.00000838
Iteration 30/1000 | Loss: 0.00000838
Iteration 31/1000 | Loss: 0.00000838
Iteration 32/1000 | Loss: 0.00000838
Iteration 33/1000 | Loss: 0.00000838
Iteration 34/1000 | Loss: 0.00000838
Iteration 35/1000 | Loss: 0.00000838
Iteration 36/1000 | Loss: 0.00000838
Iteration 37/1000 | Loss: 0.00000838
Iteration 38/1000 | Loss: 0.00000838
Iteration 39/1000 | Loss: 0.00000838
Iteration 40/1000 | Loss: 0.00000838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 40. Stopping optimization.
Last 5 losses: [8.38160212879302e-06, 8.38160212879302e-06, 8.38160212879302e-06, 8.38160212879302e-06, 8.38160212879302e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.38160212879302e-06

Optimization complete. Final v2v error: 2.4519541263580322 mm

Highest mean error: 2.79194974899292 mm for frame 66

Lowest mean error: 2.152155876159668 mm for frame 128

Saving results

Total time: 42.947861433029175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891882
Iteration 2/25 | Loss: 0.00120358
Iteration 3/25 | Loss: 0.00089922
Iteration 4/25 | Loss: 0.00088430
Iteration 5/25 | Loss: 0.00088093
Iteration 6/25 | Loss: 0.00087948
Iteration 7/25 | Loss: 0.00087938
Iteration 8/25 | Loss: 0.00087938
Iteration 9/25 | Loss: 0.00087938
Iteration 10/25 | Loss: 0.00087938
Iteration 11/25 | Loss: 0.00087938
Iteration 12/25 | Loss: 0.00087938
Iteration 13/25 | Loss: 0.00087938
Iteration 14/25 | Loss: 0.00087938
Iteration 15/25 | Loss: 0.00087938
Iteration 16/25 | Loss: 0.00087938
Iteration 17/25 | Loss: 0.00087938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008793797460384667, 0.0008793797460384667, 0.0008793797460384667, 0.0008793797460384667, 0.0008793797460384667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008793797460384667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36420035
Iteration 2/25 | Loss: 0.00043137
Iteration 3/25 | Loss: 0.00043134
Iteration 4/25 | Loss: 0.00043134
Iteration 5/25 | Loss: 0.00043134
Iteration 6/25 | Loss: 0.00043134
Iteration 7/25 | Loss: 0.00043134
Iteration 8/25 | Loss: 0.00043134
Iteration 9/25 | Loss: 0.00043134
Iteration 10/25 | Loss: 0.00043134
Iteration 11/25 | Loss: 0.00043134
Iteration 12/25 | Loss: 0.00043134
Iteration 13/25 | Loss: 0.00043134
Iteration 14/25 | Loss: 0.00043134
Iteration 15/25 | Loss: 0.00043134
Iteration 16/25 | Loss: 0.00043134
Iteration 17/25 | Loss: 0.00043134
Iteration 18/25 | Loss: 0.00043134
Iteration 19/25 | Loss: 0.00043134
Iteration 20/25 | Loss: 0.00043134
Iteration 21/25 | Loss: 0.00043134
Iteration 22/25 | Loss: 0.00043134
Iteration 23/25 | Loss: 0.00043134
Iteration 24/25 | Loss: 0.00043134
Iteration 25/25 | Loss: 0.00043134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043134
Iteration 2/1000 | Loss: 0.00001828
Iteration 3/1000 | Loss: 0.00001389
Iteration 4/1000 | Loss: 0.00001279
Iteration 5/1000 | Loss: 0.00001235
Iteration 6/1000 | Loss: 0.00001210
Iteration 7/1000 | Loss: 0.00001191
Iteration 8/1000 | Loss: 0.00001181
Iteration 9/1000 | Loss: 0.00001180
Iteration 10/1000 | Loss: 0.00001175
Iteration 11/1000 | Loss: 0.00001174
Iteration 12/1000 | Loss: 0.00001174
Iteration 13/1000 | Loss: 0.00001173
Iteration 14/1000 | Loss: 0.00001173
Iteration 15/1000 | Loss: 0.00001169
Iteration 16/1000 | Loss: 0.00001168
Iteration 17/1000 | Loss: 0.00001165
Iteration 18/1000 | Loss: 0.00001150
Iteration 19/1000 | Loss: 0.00001141
Iteration 20/1000 | Loss: 0.00001139
Iteration 21/1000 | Loss: 0.00001139
Iteration 22/1000 | Loss: 0.00001138
Iteration 23/1000 | Loss: 0.00001138
Iteration 24/1000 | Loss: 0.00001138
Iteration 25/1000 | Loss: 0.00001137
Iteration 26/1000 | Loss: 0.00001136
Iteration 27/1000 | Loss: 0.00001136
Iteration 28/1000 | Loss: 0.00001135
Iteration 29/1000 | Loss: 0.00001134
Iteration 30/1000 | Loss: 0.00001134
Iteration 31/1000 | Loss: 0.00001134
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001133
Iteration 34/1000 | Loss: 0.00001133
Iteration 35/1000 | Loss: 0.00001132
Iteration 36/1000 | Loss: 0.00001131
Iteration 37/1000 | Loss: 0.00001131
Iteration 38/1000 | Loss: 0.00001131
Iteration 39/1000 | Loss: 0.00001131
Iteration 40/1000 | Loss: 0.00001131
Iteration 41/1000 | Loss: 0.00001131
Iteration 42/1000 | Loss: 0.00001131
Iteration 43/1000 | Loss: 0.00001131
Iteration 44/1000 | Loss: 0.00001131
Iteration 45/1000 | Loss: 0.00001130
Iteration 46/1000 | Loss: 0.00001130
Iteration 47/1000 | Loss: 0.00001130
Iteration 48/1000 | Loss: 0.00001129
Iteration 49/1000 | Loss: 0.00001129
Iteration 50/1000 | Loss: 0.00001128
Iteration 51/1000 | Loss: 0.00001128
Iteration 52/1000 | Loss: 0.00001128
Iteration 53/1000 | Loss: 0.00001128
Iteration 54/1000 | Loss: 0.00001128
Iteration 55/1000 | Loss: 0.00001128
Iteration 56/1000 | Loss: 0.00001128
Iteration 57/1000 | Loss: 0.00001128
Iteration 58/1000 | Loss: 0.00001128
Iteration 59/1000 | Loss: 0.00001127
Iteration 60/1000 | Loss: 0.00001127
Iteration 61/1000 | Loss: 0.00001127
Iteration 62/1000 | Loss: 0.00001127
Iteration 63/1000 | Loss: 0.00001127
Iteration 64/1000 | Loss: 0.00001126
Iteration 65/1000 | Loss: 0.00001125
Iteration 66/1000 | Loss: 0.00001125
Iteration 67/1000 | Loss: 0.00001125
Iteration 68/1000 | Loss: 0.00001125
Iteration 69/1000 | Loss: 0.00001124
Iteration 70/1000 | Loss: 0.00001124
Iteration 71/1000 | Loss: 0.00001124
Iteration 72/1000 | Loss: 0.00001124
Iteration 73/1000 | Loss: 0.00001123
Iteration 74/1000 | Loss: 0.00001123
Iteration 75/1000 | Loss: 0.00001123
Iteration 76/1000 | Loss: 0.00001122
Iteration 77/1000 | Loss: 0.00001122
Iteration 78/1000 | Loss: 0.00001122
Iteration 79/1000 | Loss: 0.00001122
Iteration 80/1000 | Loss: 0.00001121
Iteration 81/1000 | Loss: 0.00001121
Iteration 82/1000 | Loss: 0.00001121
Iteration 83/1000 | Loss: 0.00001121
Iteration 84/1000 | Loss: 0.00001121
Iteration 85/1000 | Loss: 0.00001121
Iteration 86/1000 | Loss: 0.00001121
Iteration 87/1000 | Loss: 0.00001121
Iteration 88/1000 | Loss: 0.00001121
Iteration 89/1000 | Loss: 0.00001121
Iteration 90/1000 | Loss: 0.00001121
Iteration 91/1000 | Loss: 0.00001121
Iteration 92/1000 | Loss: 0.00001120
Iteration 93/1000 | Loss: 0.00001120
Iteration 94/1000 | Loss: 0.00001120
Iteration 95/1000 | Loss: 0.00001120
Iteration 96/1000 | Loss: 0.00001120
Iteration 97/1000 | Loss: 0.00001120
Iteration 98/1000 | Loss: 0.00001120
Iteration 99/1000 | Loss: 0.00001120
Iteration 100/1000 | Loss: 0.00001120
Iteration 101/1000 | Loss: 0.00001120
Iteration 102/1000 | Loss: 0.00001120
Iteration 103/1000 | Loss: 0.00001120
Iteration 104/1000 | Loss: 0.00001120
Iteration 105/1000 | Loss: 0.00001120
Iteration 106/1000 | Loss: 0.00001120
Iteration 107/1000 | Loss: 0.00001120
Iteration 108/1000 | Loss: 0.00001119
Iteration 109/1000 | Loss: 0.00001119
Iteration 110/1000 | Loss: 0.00001119
Iteration 111/1000 | Loss: 0.00001119
Iteration 112/1000 | Loss: 0.00001119
Iteration 113/1000 | Loss: 0.00001119
Iteration 114/1000 | Loss: 0.00001119
Iteration 115/1000 | Loss: 0.00001119
Iteration 116/1000 | Loss: 0.00001119
Iteration 117/1000 | Loss: 0.00001119
Iteration 118/1000 | Loss: 0.00001119
Iteration 119/1000 | Loss: 0.00001119
Iteration 120/1000 | Loss: 0.00001119
Iteration 121/1000 | Loss: 0.00001119
Iteration 122/1000 | Loss: 0.00001119
Iteration 123/1000 | Loss: 0.00001119
Iteration 124/1000 | Loss: 0.00001119
Iteration 125/1000 | Loss: 0.00001119
Iteration 126/1000 | Loss: 0.00001119
Iteration 127/1000 | Loss: 0.00001119
Iteration 128/1000 | Loss: 0.00001119
Iteration 129/1000 | Loss: 0.00001119
Iteration 130/1000 | Loss: 0.00001119
Iteration 131/1000 | Loss: 0.00001119
Iteration 132/1000 | Loss: 0.00001119
Iteration 133/1000 | Loss: 0.00001119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.1192657439096365e-05, 1.1192657439096365e-05, 1.1192657439096365e-05, 1.1192657439096365e-05, 1.1192657439096365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1192657439096365e-05

Optimization complete. Final v2v error: 2.7848918437957764 mm

Highest mean error: 2.923450469970703 mm for frame 25

Lowest mean error: 2.4440109729766846 mm for frame 0

Saving results

Total time: 33.663495779037476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020009
Iteration 2/25 | Loss: 0.01020008
Iteration 3/25 | Loss: 0.01020007
Iteration 4/25 | Loss: 0.01020007
Iteration 5/25 | Loss: 0.00350391
Iteration 6/25 | Loss: 0.00207659
Iteration 7/25 | Loss: 0.00170456
Iteration 8/25 | Loss: 0.00153536
Iteration 9/25 | Loss: 0.00158322
Iteration 10/25 | Loss: 0.00162335
Iteration 11/25 | Loss: 0.00144729
Iteration 12/25 | Loss: 0.00135168
Iteration 13/25 | Loss: 0.00131905
Iteration 14/25 | Loss: 0.00131005
Iteration 15/25 | Loss: 0.00128177
Iteration 16/25 | Loss: 0.00126745
Iteration 17/25 | Loss: 0.00124595
Iteration 18/25 | Loss: 0.00124664
Iteration 19/25 | Loss: 0.00124328
Iteration 20/25 | Loss: 0.00123739
Iteration 21/25 | Loss: 0.00123734
Iteration 22/25 | Loss: 0.00122940
Iteration 23/25 | Loss: 0.00123309
Iteration 24/25 | Loss: 0.00123009
Iteration 25/25 | Loss: 0.00122616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38757193
Iteration 2/25 | Loss: 0.00630527
Iteration 3/25 | Loss: 0.00386803
Iteration 4/25 | Loss: 0.00386802
Iteration 5/25 | Loss: 0.00386802
Iteration 6/25 | Loss: 0.00386802
Iteration 7/25 | Loss: 0.00386802
Iteration 8/25 | Loss: 0.00386802
Iteration 9/25 | Loss: 0.00386802
Iteration 10/25 | Loss: 0.00386802
Iteration 11/25 | Loss: 0.00386802
Iteration 12/25 | Loss: 0.00386802
Iteration 13/25 | Loss: 0.00386802
Iteration 14/25 | Loss: 0.00386802
Iteration 15/25 | Loss: 0.00386802
Iteration 16/25 | Loss: 0.00386802
Iteration 17/25 | Loss: 0.00386802
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00386802083812654, 0.00386802083812654, 0.00386802083812654, 0.00386802083812654, 0.00386802083812654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00386802083812654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00386802
Iteration 2/1000 | Loss: 0.00361372
Iteration 3/1000 | Loss: 0.00087494
Iteration 4/1000 | Loss: 0.00058513
Iteration 5/1000 | Loss: 0.00099997
Iteration 6/1000 | Loss: 0.00051954
Iteration 7/1000 | Loss: 0.00044726
Iteration 8/1000 | Loss: 0.00036973
Iteration 9/1000 | Loss: 0.00040966
Iteration 10/1000 | Loss: 0.00026452
Iteration 11/1000 | Loss: 0.00027415
Iteration 12/1000 | Loss: 0.00030062
Iteration 13/1000 | Loss: 0.00025253
Iteration 14/1000 | Loss: 0.00025860
Iteration 15/1000 | Loss: 0.00023151
Iteration 16/1000 | Loss: 0.00029480
Iteration 17/1000 | Loss: 0.00026204
Iteration 18/1000 | Loss: 0.00026517
Iteration 19/1000 | Loss: 0.00026227
Iteration 20/1000 | Loss: 0.00028698
Iteration 21/1000 | Loss: 0.00061941
Iteration 22/1000 | Loss: 0.00055239
Iteration 23/1000 | Loss: 0.00085540
Iteration 24/1000 | Loss: 0.00042061
Iteration 25/1000 | Loss: 0.00046244
Iteration 26/1000 | Loss: 0.00033475
Iteration 27/1000 | Loss: 0.00022428
Iteration 28/1000 | Loss: 0.00103287
Iteration 29/1000 | Loss: 0.00042838
Iteration 30/1000 | Loss: 0.00073439
Iteration 31/1000 | Loss: 0.00031019
Iteration 32/1000 | Loss: 0.00058698
Iteration 33/1000 | Loss: 0.00127460
Iteration 34/1000 | Loss: 0.00150200
Iteration 35/1000 | Loss: 0.00042868
Iteration 36/1000 | Loss: 0.00063234
Iteration 37/1000 | Loss: 0.00033489
Iteration 38/1000 | Loss: 0.00042067
Iteration 39/1000 | Loss: 0.00024212
Iteration 40/1000 | Loss: 0.00041954
Iteration 41/1000 | Loss: 0.00031674
Iteration 42/1000 | Loss: 0.00025219
Iteration 43/1000 | Loss: 0.00044863
Iteration 44/1000 | Loss: 0.00022933
Iteration 45/1000 | Loss: 0.00040805
Iteration 46/1000 | Loss: 0.00035449
Iteration 47/1000 | Loss: 0.00033312
Iteration 48/1000 | Loss: 0.00040838
Iteration 49/1000 | Loss: 0.00067699
Iteration 50/1000 | Loss: 0.00107855
Iteration 51/1000 | Loss: 0.00066908
Iteration 52/1000 | Loss: 0.00035430
Iteration 53/1000 | Loss: 0.00043783
Iteration 54/1000 | Loss: 0.00022564
Iteration 55/1000 | Loss: 0.00023916
Iteration 56/1000 | Loss: 0.00023860
Iteration 57/1000 | Loss: 0.00023066
Iteration 58/1000 | Loss: 0.00023998
Iteration 59/1000 | Loss: 0.00025008
Iteration 60/1000 | Loss: 0.00024305
Iteration 61/1000 | Loss: 0.00024605
Iteration 62/1000 | Loss: 0.00024667
Iteration 63/1000 | Loss: 0.00023825
Iteration 64/1000 | Loss: 0.00023574
Iteration 65/1000 | Loss: 0.00021622
Iteration 66/1000 | Loss: 0.00024129
Iteration 67/1000 | Loss: 0.00029867
Iteration 68/1000 | Loss: 0.00061230
Iteration 69/1000 | Loss: 0.00032910
Iteration 70/1000 | Loss: 0.00060119
Iteration 71/1000 | Loss: 0.00023928
Iteration 72/1000 | Loss: 0.00027131
Iteration 73/1000 | Loss: 0.00023596
Iteration 74/1000 | Loss: 0.00052906
Iteration 75/1000 | Loss: 0.00022781
Iteration 76/1000 | Loss: 0.00055106
Iteration 77/1000 | Loss: 0.00023505
Iteration 78/1000 | Loss: 0.00019811
Iteration 79/1000 | Loss: 0.00021570
Iteration 80/1000 | Loss: 0.00022795
Iteration 81/1000 | Loss: 0.00022152
Iteration 82/1000 | Loss: 0.00021760
Iteration 83/1000 | Loss: 0.00027000
Iteration 84/1000 | Loss: 0.00030138
Iteration 85/1000 | Loss: 0.00018530
Iteration 86/1000 | Loss: 0.00018791
Iteration 87/1000 | Loss: 0.00017596
Iteration 88/1000 | Loss: 0.00017512
Iteration 89/1000 | Loss: 0.00016825
Iteration 90/1000 | Loss: 0.00019784
Iteration 91/1000 | Loss: 0.00021307
Iteration 92/1000 | Loss: 0.00020903
Iteration 93/1000 | Loss: 0.00022529
Iteration 94/1000 | Loss: 0.00021061
Iteration 95/1000 | Loss: 0.00017634
Iteration 96/1000 | Loss: 0.00021752
Iteration 97/1000 | Loss: 0.00029697
Iteration 98/1000 | Loss: 0.00022993
Iteration 99/1000 | Loss: 0.00022546
Iteration 100/1000 | Loss: 0.00021621
Iteration 101/1000 | Loss: 0.00027432
Iteration 102/1000 | Loss: 0.00051991
Iteration 103/1000 | Loss: 0.00020697
Iteration 104/1000 | Loss: 0.00020844
Iteration 105/1000 | Loss: 0.00019075
Iteration 106/1000 | Loss: 0.00022868
Iteration 107/1000 | Loss: 0.00020631
Iteration 108/1000 | Loss: 0.00018576
Iteration 109/1000 | Loss: 0.00022127
Iteration 110/1000 | Loss: 0.00022824
Iteration 111/1000 | Loss: 0.00019136
Iteration 112/1000 | Loss: 0.00025569
Iteration 113/1000 | Loss: 0.00022502
Iteration 114/1000 | Loss: 0.00021976
Iteration 115/1000 | Loss: 0.00016005
Iteration 116/1000 | Loss: 0.00014831
Iteration 117/1000 | Loss: 0.00017929
Iteration 118/1000 | Loss: 0.00016182
Iteration 119/1000 | Loss: 0.00046226
Iteration 120/1000 | Loss: 0.00027648
Iteration 121/1000 | Loss: 0.00058690
Iteration 122/1000 | Loss: 0.00059901
Iteration 123/1000 | Loss: 0.00020775
Iteration 124/1000 | Loss: 0.00051982
Iteration 125/1000 | Loss: 0.00014340
Iteration 126/1000 | Loss: 0.00015688
Iteration 127/1000 | Loss: 0.00014345
Iteration 128/1000 | Loss: 0.00015208
Iteration 129/1000 | Loss: 0.00023677
Iteration 130/1000 | Loss: 0.00015646
Iteration 131/1000 | Loss: 0.00015253
Iteration 132/1000 | Loss: 0.00015006
Iteration 133/1000 | Loss: 0.00015077
Iteration 134/1000 | Loss: 0.00015062
Iteration 135/1000 | Loss: 0.00014976
Iteration 136/1000 | Loss: 0.00014740
Iteration 137/1000 | Loss: 0.00014759
Iteration 138/1000 | Loss: 0.00014891
Iteration 139/1000 | Loss: 0.00014815
Iteration 140/1000 | Loss: 0.00014596
Iteration 141/1000 | Loss: 0.00057522
Iteration 142/1000 | Loss: 0.00014888
Iteration 143/1000 | Loss: 0.00014817
Iteration 144/1000 | Loss: 0.00014594
Iteration 145/1000 | Loss: 0.00014699
Iteration 146/1000 | Loss: 0.00014341
Iteration 147/1000 | Loss: 0.00013403
Iteration 148/1000 | Loss: 0.00013004
Iteration 149/1000 | Loss: 0.00012699
Iteration 150/1000 | Loss: 0.00012640
Iteration 151/1000 | Loss: 0.00012575
Iteration 152/1000 | Loss: 0.00012527
Iteration 153/1000 | Loss: 0.00031224
Iteration 154/1000 | Loss: 0.00097624
Iteration 155/1000 | Loss: 0.00047366
Iteration 156/1000 | Loss: 0.00038696
Iteration 157/1000 | Loss: 0.00020687
Iteration 158/1000 | Loss: 0.00014888
Iteration 159/1000 | Loss: 0.00027143
Iteration 160/1000 | Loss: 0.00017069
Iteration 161/1000 | Loss: 0.00012394
Iteration 162/1000 | Loss: 0.00018025
Iteration 163/1000 | Loss: 0.00011361
Iteration 164/1000 | Loss: 0.00023752
Iteration 165/1000 | Loss: 0.00011491
Iteration 166/1000 | Loss: 0.00011033
Iteration 167/1000 | Loss: 0.00024493
Iteration 168/1000 | Loss: 0.00051774
Iteration 169/1000 | Loss: 0.00017783
Iteration 170/1000 | Loss: 0.00018602
Iteration 171/1000 | Loss: 0.00011229
Iteration 172/1000 | Loss: 0.00010755
Iteration 173/1000 | Loss: 0.00010275
Iteration 174/1000 | Loss: 0.00009983
Iteration 175/1000 | Loss: 0.00009805
Iteration 176/1000 | Loss: 0.00022048
Iteration 177/1000 | Loss: 0.00010680
Iteration 178/1000 | Loss: 0.00010007
Iteration 179/1000 | Loss: 0.00036518
Iteration 180/1000 | Loss: 0.00010799
Iteration 181/1000 | Loss: 0.00010008
Iteration 182/1000 | Loss: 0.00009610
Iteration 183/1000 | Loss: 0.00009296
Iteration 184/1000 | Loss: 0.00021335
Iteration 185/1000 | Loss: 0.00021570
Iteration 186/1000 | Loss: 0.00020826
Iteration 187/1000 | Loss: 0.00013702
Iteration 188/1000 | Loss: 0.00016852
Iteration 189/1000 | Loss: 0.00015505
Iteration 190/1000 | Loss: 0.00016176
Iteration 191/1000 | Loss: 0.00014544
Iteration 192/1000 | Loss: 0.00014257
Iteration 193/1000 | Loss: 0.00009933
Iteration 194/1000 | Loss: 0.00009422
Iteration 195/1000 | Loss: 0.00009026
Iteration 196/1000 | Loss: 0.00008849
Iteration 197/1000 | Loss: 0.00008708
Iteration 198/1000 | Loss: 0.00008633
Iteration 199/1000 | Loss: 0.00008583
Iteration 200/1000 | Loss: 0.00020001
Iteration 201/1000 | Loss: 0.00009102
Iteration 202/1000 | Loss: 0.00008723
Iteration 203/1000 | Loss: 0.00019839
Iteration 204/1000 | Loss: 0.00009558
Iteration 205/1000 | Loss: 0.00008849
Iteration 206/1000 | Loss: 0.00008525
Iteration 207/1000 | Loss: 0.00008336
Iteration 208/1000 | Loss: 0.00008225
Iteration 209/1000 | Loss: 0.00008141
Iteration 210/1000 | Loss: 0.00008105
Iteration 211/1000 | Loss: 0.00022449
Iteration 212/1000 | Loss: 0.00016642
Iteration 213/1000 | Loss: 0.00031705
Iteration 214/1000 | Loss: 0.00031640
Iteration 215/1000 | Loss: 0.00025151
Iteration 216/1000 | Loss: 0.00008629
Iteration 217/1000 | Loss: 0.00008323
Iteration 218/1000 | Loss: 0.00033951
Iteration 219/1000 | Loss: 0.00045682
Iteration 220/1000 | Loss: 0.00024450
Iteration 221/1000 | Loss: 0.00025733
Iteration 222/1000 | Loss: 0.00020815
Iteration 223/1000 | Loss: 0.00018809
Iteration 224/1000 | Loss: 0.00017164
Iteration 225/1000 | Loss: 0.00013138
Iteration 226/1000 | Loss: 0.00014169
Iteration 227/1000 | Loss: 0.00016974
Iteration 228/1000 | Loss: 0.00013711
Iteration 229/1000 | Loss: 0.00019395
Iteration 230/1000 | Loss: 0.00008294
Iteration 231/1000 | Loss: 0.00007578
Iteration 232/1000 | Loss: 0.00007363
Iteration 233/1000 | Loss: 0.00007094
Iteration 234/1000 | Loss: 0.00006974
Iteration 235/1000 | Loss: 0.00019167
Iteration 236/1000 | Loss: 0.00007435
Iteration 237/1000 | Loss: 0.00018174
Iteration 238/1000 | Loss: 0.00032289
Iteration 239/1000 | Loss: 0.00010005
Iteration 240/1000 | Loss: 0.00007579
Iteration 241/1000 | Loss: 0.00007042
Iteration 242/1000 | Loss: 0.00014621
Iteration 243/1000 | Loss: 0.00008550
Iteration 244/1000 | Loss: 0.00012321
Iteration 245/1000 | Loss: 0.00017764
Iteration 246/1000 | Loss: 0.00006973
Iteration 247/1000 | Loss: 0.00006510
Iteration 248/1000 | Loss: 0.00006273
Iteration 249/1000 | Loss: 0.00006077
Iteration 250/1000 | Loss: 0.00017539
Iteration 251/1000 | Loss: 0.00006994
Iteration 252/1000 | Loss: 0.00006317
Iteration 253/1000 | Loss: 0.00006026
Iteration 254/1000 | Loss: 0.00005860
Iteration 255/1000 | Loss: 0.00005744
Iteration 256/1000 | Loss: 0.00005666
Iteration 257/1000 | Loss: 0.00005625
Iteration 258/1000 | Loss: 0.00013473
Iteration 259/1000 | Loss: 0.00013472
Iteration 260/1000 | Loss: 0.00021763
Iteration 261/1000 | Loss: 0.00037763
Iteration 262/1000 | Loss: 0.00007771
Iteration 263/1000 | Loss: 0.00038998
Iteration 264/1000 | Loss: 0.00006013
Iteration 265/1000 | Loss: 0.00005688
Iteration 266/1000 | Loss: 0.00005524
Iteration 267/1000 | Loss: 0.00005393
Iteration 268/1000 | Loss: 0.00021326
Iteration 269/1000 | Loss: 0.00006387
Iteration 270/1000 | Loss: 0.00007524
Iteration 271/1000 | Loss: 0.00005240
Iteration 272/1000 | Loss: 0.00005758
Iteration 273/1000 | Loss: 0.00005182
Iteration 274/1000 | Loss: 0.00005159
Iteration 275/1000 | Loss: 0.00005137
Iteration 276/1000 | Loss: 0.00005124
Iteration 277/1000 | Loss: 0.00005121
Iteration 278/1000 | Loss: 0.00005121
Iteration 279/1000 | Loss: 0.00005121
Iteration 280/1000 | Loss: 0.00005121
Iteration 281/1000 | Loss: 0.00005121
Iteration 282/1000 | Loss: 0.00005120
Iteration 283/1000 | Loss: 0.00005119
Iteration 284/1000 | Loss: 0.00005115
Iteration 285/1000 | Loss: 0.00005114
Iteration 286/1000 | Loss: 0.00005114
Iteration 287/1000 | Loss: 0.00005114
Iteration 288/1000 | Loss: 0.00005114
Iteration 289/1000 | Loss: 0.00005114
Iteration 290/1000 | Loss: 0.00005113
Iteration 291/1000 | Loss: 0.00005113
Iteration 292/1000 | Loss: 0.00005113
Iteration 293/1000 | Loss: 0.00005113
Iteration 294/1000 | Loss: 0.00005113
Iteration 295/1000 | Loss: 0.00005113
Iteration 296/1000 | Loss: 0.00005113
Iteration 297/1000 | Loss: 0.00005112
Iteration 298/1000 | Loss: 0.00005112
Iteration 299/1000 | Loss: 0.00005112
Iteration 300/1000 | Loss: 0.00005111
Iteration 301/1000 | Loss: 0.00005111
Iteration 302/1000 | Loss: 0.00005110
Iteration 303/1000 | Loss: 0.00005110
Iteration 304/1000 | Loss: 0.00005110
Iteration 305/1000 | Loss: 0.00005110
Iteration 306/1000 | Loss: 0.00005110
Iteration 307/1000 | Loss: 0.00005109
Iteration 308/1000 | Loss: 0.00005109
Iteration 309/1000 | Loss: 0.00005109
Iteration 310/1000 | Loss: 0.00005109
Iteration 311/1000 | Loss: 0.00005109
Iteration 312/1000 | Loss: 0.00005109
Iteration 313/1000 | Loss: 0.00005109
Iteration 314/1000 | Loss: 0.00005108
Iteration 315/1000 | Loss: 0.00005108
Iteration 316/1000 | Loss: 0.00005108
Iteration 317/1000 | Loss: 0.00005108
Iteration 318/1000 | Loss: 0.00005108
Iteration 319/1000 | Loss: 0.00005108
Iteration 320/1000 | Loss: 0.00005108
Iteration 321/1000 | Loss: 0.00005108
Iteration 322/1000 | Loss: 0.00005108
Iteration 323/1000 | Loss: 0.00005108
Iteration 324/1000 | Loss: 0.00005108
Iteration 325/1000 | Loss: 0.00005108
Iteration 326/1000 | Loss: 0.00005107
Iteration 327/1000 | Loss: 0.00005107
Iteration 328/1000 | Loss: 0.00005107
Iteration 329/1000 | Loss: 0.00005107
Iteration 330/1000 | Loss: 0.00005107
Iteration 331/1000 | Loss: 0.00005107
Iteration 332/1000 | Loss: 0.00005107
Iteration 333/1000 | Loss: 0.00005107
Iteration 334/1000 | Loss: 0.00005107
Iteration 335/1000 | Loss: 0.00005107
Iteration 336/1000 | Loss: 0.00005107
Iteration 337/1000 | Loss: 0.00005107
Iteration 338/1000 | Loss: 0.00005107
Iteration 339/1000 | Loss: 0.00005107
Iteration 340/1000 | Loss: 0.00005107
Iteration 341/1000 | Loss: 0.00005107
Iteration 342/1000 | Loss: 0.00005107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 342. Stopping optimization.
Last 5 losses: [5.107072138343938e-05, 5.107072138343938e-05, 5.107072138343938e-05, 5.107072138343938e-05, 5.107072138343938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.107072138343938e-05

Optimization complete. Final v2v error: 3.493478298187256 mm

Highest mean error: 12.623213768005371 mm for frame 111

Lowest mean error: 2.375004768371582 mm for frame 71

Saving results

Total time: 486.401793718338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00525253
Iteration 2/25 | Loss: 0.00098521
Iteration 3/25 | Loss: 0.00087552
Iteration 4/25 | Loss: 0.00086066
Iteration 5/25 | Loss: 0.00085651
Iteration 6/25 | Loss: 0.00085525
Iteration 7/25 | Loss: 0.00085525
Iteration 8/25 | Loss: 0.00085525
Iteration 9/25 | Loss: 0.00085525
Iteration 10/25 | Loss: 0.00085525
Iteration 11/25 | Loss: 0.00085525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008552535437047482, 0.0008552535437047482, 0.0008552535437047482, 0.0008552535437047482, 0.0008552535437047482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008552535437047482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.31848717
Iteration 2/25 | Loss: 0.00041475
Iteration 3/25 | Loss: 0.00041474
Iteration 4/25 | Loss: 0.00041474
Iteration 5/25 | Loss: 0.00041474
Iteration 6/25 | Loss: 0.00041474
Iteration 7/25 | Loss: 0.00041474
Iteration 8/25 | Loss: 0.00041474
Iteration 9/25 | Loss: 0.00041474
Iteration 10/25 | Loss: 0.00041474
Iteration 11/25 | Loss: 0.00041474
Iteration 12/25 | Loss: 0.00041474
Iteration 13/25 | Loss: 0.00041474
Iteration 14/25 | Loss: 0.00041474
Iteration 15/25 | Loss: 0.00041474
Iteration 16/25 | Loss: 0.00041474
Iteration 17/25 | Loss: 0.00041474
Iteration 18/25 | Loss: 0.00041474
Iteration 19/25 | Loss: 0.00041474
Iteration 20/25 | Loss: 0.00041474
Iteration 21/25 | Loss: 0.00041474
Iteration 22/25 | Loss: 0.00041474
Iteration 23/25 | Loss: 0.00041474
Iteration 24/25 | Loss: 0.00041474
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0004147397412452847, 0.0004147397412452847, 0.0004147397412452847, 0.0004147397412452847, 0.0004147397412452847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004147397412452847

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041474
Iteration 2/1000 | Loss: 0.00001844
Iteration 3/1000 | Loss: 0.00001203
Iteration 4/1000 | Loss: 0.00001114
Iteration 5/1000 | Loss: 0.00001066
Iteration 6/1000 | Loss: 0.00001043
Iteration 7/1000 | Loss: 0.00001032
Iteration 8/1000 | Loss: 0.00001021
Iteration 9/1000 | Loss: 0.00001016
Iteration 10/1000 | Loss: 0.00001012
Iteration 11/1000 | Loss: 0.00001010
Iteration 12/1000 | Loss: 0.00001004
Iteration 13/1000 | Loss: 0.00001000
Iteration 14/1000 | Loss: 0.00001000
Iteration 15/1000 | Loss: 0.00001000
Iteration 16/1000 | Loss: 0.00000999
Iteration 17/1000 | Loss: 0.00000999
Iteration 18/1000 | Loss: 0.00000996
Iteration 19/1000 | Loss: 0.00000996
Iteration 20/1000 | Loss: 0.00000996
Iteration 21/1000 | Loss: 0.00000996
Iteration 22/1000 | Loss: 0.00000995
Iteration 23/1000 | Loss: 0.00000995
Iteration 24/1000 | Loss: 0.00000995
Iteration 25/1000 | Loss: 0.00000994
Iteration 26/1000 | Loss: 0.00000993
Iteration 27/1000 | Loss: 0.00000992
Iteration 28/1000 | Loss: 0.00000992
Iteration 29/1000 | Loss: 0.00000991
Iteration 30/1000 | Loss: 0.00000991
Iteration 31/1000 | Loss: 0.00000991
Iteration 32/1000 | Loss: 0.00000991
Iteration 33/1000 | Loss: 0.00000990
Iteration 34/1000 | Loss: 0.00000990
Iteration 35/1000 | Loss: 0.00000989
Iteration 36/1000 | Loss: 0.00000989
Iteration 37/1000 | Loss: 0.00000988
Iteration 38/1000 | Loss: 0.00000988
Iteration 39/1000 | Loss: 0.00000987
Iteration 40/1000 | Loss: 0.00000987
Iteration 41/1000 | Loss: 0.00000987
Iteration 42/1000 | Loss: 0.00000987
Iteration 43/1000 | Loss: 0.00000986
Iteration 44/1000 | Loss: 0.00000986
Iteration 45/1000 | Loss: 0.00000986
Iteration 46/1000 | Loss: 0.00000986
Iteration 47/1000 | Loss: 0.00000986
Iteration 48/1000 | Loss: 0.00000985
Iteration 49/1000 | Loss: 0.00000985
Iteration 50/1000 | Loss: 0.00000985
Iteration 51/1000 | Loss: 0.00000985
Iteration 52/1000 | Loss: 0.00000985
Iteration 53/1000 | Loss: 0.00000985
Iteration 54/1000 | Loss: 0.00000984
Iteration 55/1000 | Loss: 0.00000984
Iteration 56/1000 | Loss: 0.00000984
Iteration 57/1000 | Loss: 0.00000984
Iteration 58/1000 | Loss: 0.00000984
Iteration 59/1000 | Loss: 0.00000984
Iteration 60/1000 | Loss: 0.00000984
Iteration 61/1000 | Loss: 0.00000983
Iteration 62/1000 | Loss: 0.00000983
Iteration 63/1000 | Loss: 0.00000983
Iteration 64/1000 | Loss: 0.00000983
Iteration 65/1000 | Loss: 0.00000982
Iteration 66/1000 | Loss: 0.00000982
Iteration 67/1000 | Loss: 0.00000982
Iteration 68/1000 | Loss: 0.00000982
Iteration 69/1000 | Loss: 0.00000982
Iteration 70/1000 | Loss: 0.00000982
Iteration 71/1000 | Loss: 0.00000982
Iteration 72/1000 | Loss: 0.00000982
Iteration 73/1000 | Loss: 0.00000982
Iteration 74/1000 | Loss: 0.00000982
Iteration 75/1000 | Loss: 0.00000982
Iteration 76/1000 | Loss: 0.00000982
Iteration 77/1000 | Loss: 0.00000982
Iteration 78/1000 | Loss: 0.00000982
Iteration 79/1000 | Loss: 0.00000982
Iteration 80/1000 | Loss: 0.00000981
Iteration 81/1000 | Loss: 0.00000981
Iteration 82/1000 | Loss: 0.00000981
Iteration 83/1000 | Loss: 0.00000981
Iteration 84/1000 | Loss: 0.00000981
Iteration 85/1000 | Loss: 0.00000981
Iteration 86/1000 | Loss: 0.00000981
Iteration 87/1000 | Loss: 0.00000981
Iteration 88/1000 | Loss: 0.00000981
Iteration 89/1000 | Loss: 0.00000980
Iteration 90/1000 | Loss: 0.00000980
Iteration 91/1000 | Loss: 0.00000980
Iteration 92/1000 | Loss: 0.00000980
Iteration 93/1000 | Loss: 0.00000980
Iteration 94/1000 | Loss: 0.00000980
Iteration 95/1000 | Loss: 0.00000980
Iteration 96/1000 | Loss: 0.00000980
Iteration 97/1000 | Loss: 0.00000980
Iteration 98/1000 | Loss: 0.00000980
Iteration 99/1000 | Loss: 0.00000980
Iteration 100/1000 | Loss: 0.00000980
Iteration 101/1000 | Loss: 0.00000979
Iteration 102/1000 | Loss: 0.00000979
Iteration 103/1000 | Loss: 0.00000979
Iteration 104/1000 | Loss: 0.00000979
Iteration 105/1000 | Loss: 0.00000979
Iteration 106/1000 | Loss: 0.00000979
Iteration 107/1000 | Loss: 0.00000979
Iteration 108/1000 | Loss: 0.00000979
Iteration 109/1000 | Loss: 0.00000979
Iteration 110/1000 | Loss: 0.00000979
Iteration 111/1000 | Loss: 0.00000979
Iteration 112/1000 | Loss: 0.00000979
Iteration 113/1000 | Loss: 0.00000979
Iteration 114/1000 | Loss: 0.00000979
Iteration 115/1000 | Loss: 0.00000979
Iteration 116/1000 | Loss: 0.00000979
Iteration 117/1000 | Loss: 0.00000978
Iteration 118/1000 | Loss: 0.00000978
Iteration 119/1000 | Loss: 0.00000978
Iteration 120/1000 | Loss: 0.00000978
Iteration 121/1000 | Loss: 0.00000978
Iteration 122/1000 | Loss: 0.00000978
Iteration 123/1000 | Loss: 0.00000978
Iteration 124/1000 | Loss: 0.00000978
Iteration 125/1000 | Loss: 0.00000978
Iteration 126/1000 | Loss: 0.00000978
Iteration 127/1000 | Loss: 0.00000978
Iteration 128/1000 | Loss: 0.00000978
Iteration 129/1000 | Loss: 0.00000978
Iteration 130/1000 | Loss: 0.00000978
Iteration 131/1000 | Loss: 0.00000978
Iteration 132/1000 | Loss: 0.00000978
Iteration 133/1000 | Loss: 0.00000978
Iteration 134/1000 | Loss: 0.00000978
Iteration 135/1000 | Loss: 0.00000978
Iteration 136/1000 | Loss: 0.00000978
Iteration 137/1000 | Loss: 0.00000978
Iteration 138/1000 | Loss: 0.00000978
Iteration 139/1000 | Loss: 0.00000978
Iteration 140/1000 | Loss: 0.00000978
Iteration 141/1000 | Loss: 0.00000978
Iteration 142/1000 | Loss: 0.00000978
Iteration 143/1000 | Loss: 0.00000978
Iteration 144/1000 | Loss: 0.00000978
Iteration 145/1000 | Loss: 0.00000978
Iteration 146/1000 | Loss: 0.00000978
Iteration 147/1000 | Loss: 0.00000978
Iteration 148/1000 | Loss: 0.00000978
Iteration 149/1000 | Loss: 0.00000978
Iteration 150/1000 | Loss: 0.00000978
Iteration 151/1000 | Loss: 0.00000978
Iteration 152/1000 | Loss: 0.00000978
Iteration 153/1000 | Loss: 0.00000978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [9.779189895198215e-06, 9.779189895198215e-06, 9.779189895198215e-06, 9.779189895198215e-06, 9.779189895198215e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.779189895198215e-06

Optimization complete. Final v2v error: 2.6619505882263184 mm

Highest mean error: 2.9204254150390625 mm for frame 41

Lowest mean error: 2.3882694244384766 mm for frame 30

Saving results

Total time: 34.68804144859314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408285
Iteration 2/25 | Loss: 0.00098360
Iteration 3/25 | Loss: 0.00088511
Iteration 4/25 | Loss: 0.00087445
Iteration 5/25 | Loss: 0.00087103
Iteration 6/25 | Loss: 0.00087044
Iteration 7/25 | Loss: 0.00087044
Iteration 8/25 | Loss: 0.00087044
Iteration 9/25 | Loss: 0.00087044
Iteration 10/25 | Loss: 0.00087044
Iteration 11/25 | Loss: 0.00087044
Iteration 12/25 | Loss: 0.00087044
Iteration 13/25 | Loss: 0.00087044
Iteration 14/25 | Loss: 0.00087044
Iteration 15/25 | Loss: 0.00087044
Iteration 16/25 | Loss: 0.00087044
Iteration 17/25 | Loss: 0.00087044
Iteration 18/25 | Loss: 0.00087044
Iteration 19/25 | Loss: 0.00087044
Iteration 20/25 | Loss: 0.00087044
Iteration 21/25 | Loss: 0.00087044
Iteration 22/25 | Loss: 0.00087044
Iteration 23/25 | Loss: 0.00087044
Iteration 24/25 | Loss: 0.00087044
Iteration 25/25 | Loss: 0.00087044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41403782
Iteration 2/25 | Loss: 0.00045268
Iteration 3/25 | Loss: 0.00045267
Iteration 4/25 | Loss: 0.00045267
Iteration 5/25 | Loss: 0.00045267
Iteration 6/25 | Loss: 0.00045267
Iteration 7/25 | Loss: 0.00045267
Iteration 8/25 | Loss: 0.00045267
Iteration 9/25 | Loss: 0.00045267
Iteration 10/25 | Loss: 0.00045267
Iteration 11/25 | Loss: 0.00045267
Iteration 12/25 | Loss: 0.00045267
Iteration 13/25 | Loss: 0.00045267
Iteration 14/25 | Loss: 0.00045267
Iteration 15/25 | Loss: 0.00045267
Iteration 16/25 | Loss: 0.00045267
Iteration 17/25 | Loss: 0.00045267
Iteration 18/25 | Loss: 0.00045267
Iteration 19/25 | Loss: 0.00045267
Iteration 20/25 | Loss: 0.00045267
Iteration 21/25 | Loss: 0.00045267
Iteration 22/25 | Loss: 0.00045267
Iteration 23/25 | Loss: 0.00045267
Iteration 24/25 | Loss: 0.00045267
Iteration 25/25 | Loss: 0.00045267

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045267
Iteration 2/1000 | Loss: 0.00002747
Iteration 3/1000 | Loss: 0.00001977
Iteration 4/1000 | Loss: 0.00001692
Iteration 5/1000 | Loss: 0.00001615
Iteration 6/1000 | Loss: 0.00001551
Iteration 7/1000 | Loss: 0.00001507
Iteration 8/1000 | Loss: 0.00001471
Iteration 9/1000 | Loss: 0.00001458
Iteration 10/1000 | Loss: 0.00001448
Iteration 11/1000 | Loss: 0.00001442
Iteration 12/1000 | Loss: 0.00001432
Iteration 13/1000 | Loss: 0.00001431
Iteration 14/1000 | Loss: 0.00001430
Iteration 15/1000 | Loss: 0.00001424
Iteration 16/1000 | Loss: 0.00001424
Iteration 17/1000 | Loss: 0.00001423
Iteration 18/1000 | Loss: 0.00001421
Iteration 19/1000 | Loss: 0.00001421
Iteration 20/1000 | Loss: 0.00001419
Iteration 21/1000 | Loss: 0.00001419
Iteration 22/1000 | Loss: 0.00001419
Iteration 23/1000 | Loss: 0.00001419
Iteration 24/1000 | Loss: 0.00001419
Iteration 25/1000 | Loss: 0.00001418
Iteration 26/1000 | Loss: 0.00001418
Iteration 27/1000 | Loss: 0.00001418
Iteration 28/1000 | Loss: 0.00001416
Iteration 29/1000 | Loss: 0.00001414
Iteration 30/1000 | Loss: 0.00001411
Iteration 31/1000 | Loss: 0.00001411
Iteration 32/1000 | Loss: 0.00001411
Iteration 33/1000 | Loss: 0.00001411
Iteration 34/1000 | Loss: 0.00001411
Iteration 35/1000 | Loss: 0.00001410
Iteration 36/1000 | Loss: 0.00001409
Iteration 37/1000 | Loss: 0.00001408
Iteration 38/1000 | Loss: 0.00001408
Iteration 39/1000 | Loss: 0.00001408
Iteration 40/1000 | Loss: 0.00001407
Iteration 41/1000 | Loss: 0.00001406
Iteration 42/1000 | Loss: 0.00001406
Iteration 43/1000 | Loss: 0.00001406
Iteration 44/1000 | Loss: 0.00001406
Iteration 45/1000 | Loss: 0.00001406
Iteration 46/1000 | Loss: 0.00001406
Iteration 47/1000 | Loss: 0.00001406
Iteration 48/1000 | Loss: 0.00001406
Iteration 49/1000 | Loss: 0.00001406
Iteration 50/1000 | Loss: 0.00001406
Iteration 51/1000 | Loss: 0.00001406
Iteration 52/1000 | Loss: 0.00001405
Iteration 53/1000 | Loss: 0.00001405
Iteration 54/1000 | Loss: 0.00001404
Iteration 55/1000 | Loss: 0.00001404
Iteration 56/1000 | Loss: 0.00001404
Iteration 57/1000 | Loss: 0.00001403
Iteration 58/1000 | Loss: 0.00001403
Iteration 59/1000 | Loss: 0.00001403
Iteration 60/1000 | Loss: 0.00001403
Iteration 61/1000 | Loss: 0.00001401
Iteration 62/1000 | Loss: 0.00001401
Iteration 63/1000 | Loss: 0.00001401
Iteration 64/1000 | Loss: 0.00001400
Iteration 65/1000 | Loss: 0.00001400
Iteration 66/1000 | Loss: 0.00001400
Iteration 67/1000 | Loss: 0.00001400
Iteration 68/1000 | Loss: 0.00001400
Iteration 69/1000 | Loss: 0.00001400
Iteration 70/1000 | Loss: 0.00001400
Iteration 71/1000 | Loss: 0.00001400
Iteration 72/1000 | Loss: 0.00001400
Iteration 73/1000 | Loss: 0.00001400
Iteration 74/1000 | Loss: 0.00001400
Iteration 75/1000 | Loss: 0.00001400
Iteration 76/1000 | Loss: 0.00001400
Iteration 77/1000 | Loss: 0.00001400
Iteration 78/1000 | Loss: 0.00001400
Iteration 79/1000 | Loss: 0.00001400
Iteration 80/1000 | Loss: 0.00001398
Iteration 81/1000 | Loss: 0.00001397
Iteration 82/1000 | Loss: 0.00001397
Iteration 83/1000 | Loss: 0.00001397
Iteration 84/1000 | Loss: 0.00001397
Iteration 85/1000 | Loss: 0.00001397
Iteration 86/1000 | Loss: 0.00001397
Iteration 87/1000 | Loss: 0.00001397
Iteration 88/1000 | Loss: 0.00001397
Iteration 89/1000 | Loss: 0.00001397
Iteration 90/1000 | Loss: 0.00001397
Iteration 91/1000 | Loss: 0.00001397
Iteration 92/1000 | Loss: 0.00001397
Iteration 93/1000 | Loss: 0.00001396
Iteration 94/1000 | Loss: 0.00001396
Iteration 95/1000 | Loss: 0.00001396
Iteration 96/1000 | Loss: 0.00001396
Iteration 97/1000 | Loss: 0.00001396
Iteration 98/1000 | Loss: 0.00001396
Iteration 99/1000 | Loss: 0.00001396
Iteration 100/1000 | Loss: 0.00001396
Iteration 101/1000 | Loss: 0.00001395
Iteration 102/1000 | Loss: 0.00001395
Iteration 103/1000 | Loss: 0.00001395
Iteration 104/1000 | Loss: 0.00001394
Iteration 105/1000 | Loss: 0.00001394
Iteration 106/1000 | Loss: 0.00001393
Iteration 107/1000 | Loss: 0.00001393
Iteration 108/1000 | Loss: 0.00001393
Iteration 109/1000 | Loss: 0.00001393
Iteration 110/1000 | Loss: 0.00001393
Iteration 111/1000 | Loss: 0.00001393
Iteration 112/1000 | Loss: 0.00001393
Iteration 113/1000 | Loss: 0.00001392
Iteration 114/1000 | Loss: 0.00001392
Iteration 115/1000 | Loss: 0.00001392
Iteration 116/1000 | Loss: 0.00001392
Iteration 117/1000 | Loss: 0.00001391
Iteration 118/1000 | Loss: 0.00001391
Iteration 119/1000 | Loss: 0.00001391
Iteration 120/1000 | Loss: 0.00001391
Iteration 121/1000 | Loss: 0.00001391
Iteration 122/1000 | Loss: 0.00001391
Iteration 123/1000 | Loss: 0.00001391
Iteration 124/1000 | Loss: 0.00001391
Iteration 125/1000 | Loss: 0.00001391
Iteration 126/1000 | Loss: 0.00001391
Iteration 127/1000 | Loss: 0.00001390
Iteration 128/1000 | Loss: 0.00001390
Iteration 129/1000 | Loss: 0.00001390
Iteration 130/1000 | Loss: 0.00001390
Iteration 131/1000 | Loss: 0.00001389
Iteration 132/1000 | Loss: 0.00001389
Iteration 133/1000 | Loss: 0.00001389
Iteration 134/1000 | Loss: 0.00001389
Iteration 135/1000 | Loss: 0.00001388
Iteration 136/1000 | Loss: 0.00001388
Iteration 137/1000 | Loss: 0.00001388
Iteration 138/1000 | Loss: 0.00001388
Iteration 139/1000 | Loss: 0.00001388
Iteration 140/1000 | Loss: 0.00001388
Iteration 141/1000 | Loss: 0.00001388
Iteration 142/1000 | Loss: 0.00001388
Iteration 143/1000 | Loss: 0.00001388
Iteration 144/1000 | Loss: 0.00001387
Iteration 145/1000 | Loss: 0.00001387
Iteration 146/1000 | Loss: 0.00001387
Iteration 147/1000 | Loss: 0.00001387
Iteration 148/1000 | Loss: 0.00001387
Iteration 149/1000 | Loss: 0.00001386
Iteration 150/1000 | Loss: 0.00001386
Iteration 151/1000 | Loss: 0.00001386
Iteration 152/1000 | Loss: 0.00001386
Iteration 153/1000 | Loss: 0.00001386
Iteration 154/1000 | Loss: 0.00001386
Iteration 155/1000 | Loss: 0.00001386
Iteration 156/1000 | Loss: 0.00001386
Iteration 157/1000 | Loss: 0.00001386
Iteration 158/1000 | Loss: 0.00001386
Iteration 159/1000 | Loss: 0.00001386
Iteration 160/1000 | Loss: 0.00001385
Iteration 161/1000 | Loss: 0.00001385
Iteration 162/1000 | Loss: 0.00001385
Iteration 163/1000 | Loss: 0.00001385
Iteration 164/1000 | Loss: 0.00001385
Iteration 165/1000 | Loss: 0.00001385
Iteration 166/1000 | Loss: 0.00001384
Iteration 167/1000 | Loss: 0.00001384
Iteration 168/1000 | Loss: 0.00001384
Iteration 169/1000 | Loss: 0.00001384
Iteration 170/1000 | Loss: 0.00001384
Iteration 171/1000 | Loss: 0.00001384
Iteration 172/1000 | Loss: 0.00001384
Iteration 173/1000 | Loss: 0.00001384
Iteration 174/1000 | Loss: 0.00001384
Iteration 175/1000 | Loss: 0.00001384
Iteration 176/1000 | Loss: 0.00001384
Iteration 177/1000 | Loss: 0.00001384
Iteration 178/1000 | Loss: 0.00001384
Iteration 179/1000 | Loss: 0.00001383
Iteration 180/1000 | Loss: 0.00001383
Iteration 181/1000 | Loss: 0.00001383
Iteration 182/1000 | Loss: 0.00001383
Iteration 183/1000 | Loss: 0.00001383
Iteration 184/1000 | Loss: 0.00001383
Iteration 185/1000 | Loss: 0.00001383
Iteration 186/1000 | Loss: 0.00001383
Iteration 187/1000 | Loss: 0.00001383
Iteration 188/1000 | Loss: 0.00001383
Iteration 189/1000 | Loss: 0.00001383
Iteration 190/1000 | Loss: 0.00001383
Iteration 191/1000 | Loss: 0.00001383
Iteration 192/1000 | Loss: 0.00001383
Iteration 193/1000 | Loss: 0.00001382
Iteration 194/1000 | Loss: 0.00001382
Iteration 195/1000 | Loss: 0.00001382
Iteration 196/1000 | Loss: 0.00001382
Iteration 197/1000 | Loss: 0.00001382
Iteration 198/1000 | Loss: 0.00001382
Iteration 199/1000 | Loss: 0.00001382
Iteration 200/1000 | Loss: 0.00001382
Iteration 201/1000 | Loss: 0.00001382
Iteration 202/1000 | Loss: 0.00001382
Iteration 203/1000 | Loss: 0.00001382
Iteration 204/1000 | Loss: 0.00001382
Iteration 205/1000 | Loss: 0.00001382
Iteration 206/1000 | Loss: 0.00001382
Iteration 207/1000 | Loss: 0.00001381
Iteration 208/1000 | Loss: 0.00001381
Iteration 209/1000 | Loss: 0.00001381
Iteration 210/1000 | Loss: 0.00001381
Iteration 211/1000 | Loss: 0.00001381
Iteration 212/1000 | Loss: 0.00001381
Iteration 213/1000 | Loss: 0.00001381
Iteration 214/1000 | Loss: 0.00001381
Iteration 215/1000 | Loss: 0.00001381
Iteration 216/1000 | Loss: 0.00001381
Iteration 217/1000 | Loss: 0.00001381
Iteration 218/1000 | Loss: 0.00001381
Iteration 219/1000 | Loss: 0.00001381
Iteration 220/1000 | Loss: 0.00001381
Iteration 221/1000 | Loss: 0.00001381
Iteration 222/1000 | Loss: 0.00001381
Iteration 223/1000 | Loss: 0.00001381
Iteration 224/1000 | Loss: 0.00001381
Iteration 225/1000 | Loss: 0.00001381
Iteration 226/1000 | Loss: 0.00001381
Iteration 227/1000 | Loss: 0.00001381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.3811840290145483e-05, 1.3811840290145483e-05, 1.3811840290145483e-05, 1.3811840290145483e-05, 1.3811840290145483e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3811840290145483e-05

Optimization complete. Final v2v error: 3.107207775115967 mm

Highest mean error: 3.607221841812134 mm for frame 13

Lowest mean error: 2.8440964221954346 mm for frame 33

Saving results

Total time: 38.663002729415894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485056
Iteration 2/25 | Loss: 0.00097144
Iteration 3/25 | Loss: 0.00084617
Iteration 4/25 | Loss: 0.00082822
Iteration 5/25 | Loss: 0.00082295
Iteration 6/25 | Loss: 0.00082121
Iteration 7/25 | Loss: 0.00082101
Iteration 8/25 | Loss: 0.00082101
Iteration 9/25 | Loss: 0.00082101
Iteration 10/25 | Loss: 0.00082101
Iteration 11/25 | Loss: 0.00082101
Iteration 12/25 | Loss: 0.00082101
Iteration 13/25 | Loss: 0.00082101
Iteration 14/25 | Loss: 0.00082101
Iteration 15/25 | Loss: 0.00082101
Iteration 16/25 | Loss: 0.00082101
Iteration 17/25 | Loss: 0.00082101
Iteration 18/25 | Loss: 0.00082101
Iteration 19/25 | Loss: 0.00082101
Iteration 20/25 | Loss: 0.00082101
Iteration 21/25 | Loss: 0.00082101
Iteration 22/25 | Loss: 0.00082101
Iteration 23/25 | Loss: 0.00082101
Iteration 24/25 | Loss: 0.00082101
Iteration 25/25 | Loss: 0.00082101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39442015
Iteration 2/25 | Loss: 0.00044053
Iteration 3/25 | Loss: 0.00044049
Iteration 4/25 | Loss: 0.00044049
Iteration 5/25 | Loss: 0.00044049
Iteration 6/25 | Loss: 0.00044049
Iteration 7/25 | Loss: 0.00044049
Iteration 8/25 | Loss: 0.00044049
Iteration 9/25 | Loss: 0.00044049
Iteration 10/25 | Loss: 0.00044049
Iteration 11/25 | Loss: 0.00044049
Iteration 12/25 | Loss: 0.00044049
Iteration 13/25 | Loss: 0.00044049
Iteration 14/25 | Loss: 0.00044049
Iteration 15/25 | Loss: 0.00044049
Iteration 16/25 | Loss: 0.00044049
Iteration 17/25 | Loss: 0.00044049
Iteration 18/25 | Loss: 0.00044049
Iteration 19/25 | Loss: 0.00044049
Iteration 20/25 | Loss: 0.00044049
Iteration 21/25 | Loss: 0.00044049
Iteration 22/25 | Loss: 0.00044049
Iteration 23/25 | Loss: 0.00044049
Iteration 24/25 | Loss: 0.00044049
Iteration 25/25 | Loss: 0.00044049

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044049
Iteration 2/1000 | Loss: 0.00002561
Iteration 3/1000 | Loss: 0.00001456
Iteration 4/1000 | Loss: 0.00001061
Iteration 5/1000 | Loss: 0.00000964
Iteration 6/1000 | Loss: 0.00000902
Iteration 7/1000 | Loss: 0.00000884
Iteration 8/1000 | Loss: 0.00000865
Iteration 9/1000 | Loss: 0.00000845
Iteration 10/1000 | Loss: 0.00000843
Iteration 11/1000 | Loss: 0.00000843
Iteration 12/1000 | Loss: 0.00000841
Iteration 13/1000 | Loss: 0.00000840
Iteration 14/1000 | Loss: 0.00000840
Iteration 15/1000 | Loss: 0.00000839
Iteration 16/1000 | Loss: 0.00000838
Iteration 17/1000 | Loss: 0.00000838
Iteration 18/1000 | Loss: 0.00000836
Iteration 19/1000 | Loss: 0.00000835
Iteration 20/1000 | Loss: 0.00000835
Iteration 21/1000 | Loss: 0.00000834
Iteration 22/1000 | Loss: 0.00000834
Iteration 23/1000 | Loss: 0.00000833
Iteration 24/1000 | Loss: 0.00000833
Iteration 25/1000 | Loss: 0.00000833
Iteration 26/1000 | Loss: 0.00000832
Iteration 27/1000 | Loss: 0.00000831
Iteration 28/1000 | Loss: 0.00000831
Iteration 29/1000 | Loss: 0.00000830
Iteration 30/1000 | Loss: 0.00000830
Iteration 31/1000 | Loss: 0.00000830
Iteration 32/1000 | Loss: 0.00000830
Iteration 33/1000 | Loss: 0.00000829
Iteration 34/1000 | Loss: 0.00000829
Iteration 35/1000 | Loss: 0.00000828
Iteration 36/1000 | Loss: 0.00000828
Iteration 37/1000 | Loss: 0.00000828
Iteration 38/1000 | Loss: 0.00000827
Iteration 39/1000 | Loss: 0.00000827
Iteration 40/1000 | Loss: 0.00000827
Iteration 41/1000 | Loss: 0.00000826
Iteration 42/1000 | Loss: 0.00000825
Iteration 43/1000 | Loss: 0.00000825
Iteration 44/1000 | Loss: 0.00000824
Iteration 45/1000 | Loss: 0.00000824
Iteration 46/1000 | Loss: 0.00000824
Iteration 47/1000 | Loss: 0.00000823
Iteration 48/1000 | Loss: 0.00000823
Iteration 49/1000 | Loss: 0.00000822
Iteration 50/1000 | Loss: 0.00000822
Iteration 51/1000 | Loss: 0.00000822
Iteration 52/1000 | Loss: 0.00000822
Iteration 53/1000 | Loss: 0.00000822
Iteration 54/1000 | Loss: 0.00000822
Iteration 55/1000 | Loss: 0.00000822
Iteration 56/1000 | Loss: 0.00000822
Iteration 57/1000 | Loss: 0.00000822
Iteration 58/1000 | Loss: 0.00000821
Iteration 59/1000 | Loss: 0.00000821
Iteration 60/1000 | Loss: 0.00000821
Iteration 61/1000 | Loss: 0.00000821
Iteration 62/1000 | Loss: 0.00000820
Iteration 63/1000 | Loss: 0.00000820
Iteration 64/1000 | Loss: 0.00000820
Iteration 65/1000 | Loss: 0.00000820
Iteration 66/1000 | Loss: 0.00000819
Iteration 67/1000 | Loss: 0.00000819
Iteration 68/1000 | Loss: 0.00000819
Iteration 69/1000 | Loss: 0.00000819
Iteration 70/1000 | Loss: 0.00000818
Iteration 71/1000 | Loss: 0.00000818
Iteration 72/1000 | Loss: 0.00000818
Iteration 73/1000 | Loss: 0.00000818
Iteration 74/1000 | Loss: 0.00000818
Iteration 75/1000 | Loss: 0.00000818
Iteration 76/1000 | Loss: 0.00000818
Iteration 77/1000 | Loss: 0.00000818
Iteration 78/1000 | Loss: 0.00000818
Iteration 79/1000 | Loss: 0.00000818
Iteration 80/1000 | Loss: 0.00000817
Iteration 81/1000 | Loss: 0.00000817
Iteration 82/1000 | Loss: 0.00000817
Iteration 83/1000 | Loss: 0.00000817
Iteration 84/1000 | Loss: 0.00000817
Iteration 85/1000 | Loss: 0.00000817
Iteration 86/1000 | Loss: 0.00000817
Iteration 87/1000 | Loss: 0.00000817
Iteration 88/1000 | Loss: 0.00000817
Iteration 89/1000 | Loss: 0.00000816
Iteration 90/1000 | Loss: 0.00000816
Iteration 91/1000 | Loss: 0.00000816
Iteration 92/1000 | Loss: 0.00000816
Iteration 93/1000 | Loss: 0.00000816
Iteration 94/1000 | Loss: 0.00000816
Iteration 95/1000 | Loss: 0.00000816
Iteration 96/1000 | Loss: 0.00000816
Iteration 97/1000 | Loss: 0.00000816
Iteration 98/1000 | Loss: 0.00000816
Iteration 99/1000 | Loss: 0.00000815
Iteration 100/1000 | Loss: 0.00000815
Iteration 101/1000 | Loss: 0.00000815
Iteration 102/1000 | Loss: 0.00000815
Iteration 103/1000 | Loss: 0.00000815
Iteration 104/1000 | Loss: 0.00000815
Iteration 105/1000 | Loss: 0.00000815
Iteration 106/1000 | Loss: 0.00000814
Iteration 107/1000 | Loss: 0.00000814
Iteration 108/1000 | Loss: 0.00000814
Iteration 109/1000 | Loss: 0.00000814
Iteration 110/1000 | Loss: 0.00000814
Iteration 111/1000 | Loss: 0.00000814
Iteration 112/1000 | Loss: 0.00000814
Iteration 113/1000 | Loss: 0.00000813
Iteration 114/1000 | Loss: 0.00000813
Iteration 115/1000 | Loss: 0.00000813
Iteration 116/1000 | Loss: 0.00000813
Iteration 117/1000 | Loss: 0.00000813
Iteration 118/1000 | Loss: 0.00000813
Iteration 119/1000 | Loss: 0.00000813
Iteration 120/1000 | Loss: 0.00000813
Iteration 121/1000 | Loss: 0.00000813
Iteration 122/1000 | Loss: 0.00000813
Iteration 123/1000 | Loss: 0.00000812
Iteration 124/1000 | Loss: 0.00000812
Iteration 125/1000 | Loss: 0.00000812
Iteration 126/1000 | Loss: 0.00000812
Iteration 127/1000 | Loss: 0.00000811
Iteration 128/1000 | Loss: 0.00000811
Iteration 129/1000 | Loss: 0.00000811
Iteration 130/1000 | Loss: 0.00000811
Iteration 131/1000 | Loss: 0.00000811
Iteration 132/1000 | Loss: 0.00000811
Iteration 133/1000 | Loss: 0.00000811
Iteration 134/1000 | Loss: 0.00000811
Iteration 135/1000 | Loss: 0.00000811
Iteration 136/1000 | Loss: 0.00000811
Iteration 137/1000 | Loss: 0.00000811
Iteration 138/1000 | Loss: 0.00000810
Iteration 139/1000 | Loss: 0.00000810
Iteration 140/1000 | Loss: 0.00000810
Iteration 141/1000 | Loss: 0.00000810
Iteration 142/1000 | Loss: 0.00000810
Iteration 143/1000 | Loss: 0.00000810
Iteration 144/1000 | Loss: 0.00000809
Iteration 145/1000 | Loss: 0.00000809
Iteration 146/1000 | Loss: 0.00000808
Iteration 147/1000 | Loss: 0.00000808
Iteration 148/1000 | Loss: 0.00000808
Iteration 149/1000 | Loss: 0.00000808
Iteration 150/1000 | Loss: 0.00000808
Iteration 151/1000 | Loss: 0.00000808
Iteration 152/1000 | Loss: 0.00000808
Iteration 153/1000 | Loss: 0.00000808
Iteration 154/1000 | Loss: 0.00000808
Iteration 155/1000 | Loss: 0.00000808
Iteration 156/1000 | Loss: 0.00000808
Iteration 157/1000 | Loss: 0.00000808
Iteration 158/1000 | Loss: 0.00000808
Iteration 159/1000 | Loss: 0.00000808
Iteration 160/1000 | Loss: 0.00000808
Iteration 161/1000 | Loss: 0.00000807
Iteration 162/1000 | Loss: 0.00000807
Iteration 163/1000 | Loss: 0.00000807
Iteration 164/1000 | Loss: 0.00000807
Iteration 165/1000 | Loss: 0.00000807
Iteration 166/1000 | Loss: 0.00000807
Iteration 167/1000 | Loss: 0.00000807
Iteration 168/1000 | Loss: 0.00000807
Iteration 169/1000 | Loss: 0.00000807
Iteration 170/1000 | Loss: 0.00000807
Iteration 171/1000 | Loss: 0.00000807
Iteration 172/1000 | Loss: 0.00000807
Iteration 173/1000 | Loss: 0.00000807
Iteration 174/1000 | Loss: 0.00000806
Iteration 175/1000 | Loss: 0.00000806
Iteration 176/1000 | Loss: 0.00000806
Iteration 177/1000 | Loss: 0.00000806
Iteration 178/1000 | Loss: 0.00000806
Iteration 179/1000 | Loss: 0.00000806
Iteration 180/1000 | Loss: 0.00000806
Iteration 181/1000 | Loss: 0.00000806
Iteration 182/1000 | Loss: 0.00000806
Iteration 183/1000 | Loss: 0.00000806
Iteration 184/1000 | Loss: 0.00000806
Iteration 185/1000 | Loss: 0.00000806
Iteration 186/1000 | Loss: 0.00000806
Iteration 187/1000 | Loss: 0.00000806
Iteration 188/1000 | Loss: 0.00000806
Iteration 189/1000 | Loss: 0.00000806
Iteration 190/1000 | Loss: 0.00000806
Iteration 191/1000 | Loss: 0.00000805
Iteration 192/1000 | Loss: 0.00000805
Iteration 193/1000 | Loss: 0.00000805
Iteration 194/1000 | Loss: 0.00000805
Iteration 195/1000 | Loss: 0.00000805
Iteration 196/1000 | Loss: 0.00000805
Iteration 197/1000 | Loss: 0.00000805
Iteration 198/1000 | Loss: 0.00000805
Iteration 199/1000 | Loss: 0.00000805
Iteration 200/1000 | Loss: 0.00000805
Iteration 201/1000 | Loss: 0.00000805
Iteration 202/1000 | Loss: 0.00000805
Iteration 203/1000 | Loss: 0.00000805
Iteration 204/1000 | Loss: 0.00000805
Iteration 205/1000 | Loss: 0.00000805
Iteration 206/1000 | Loss: 0.00000805
Iteration 207/1000 | Loss: 0.00000805
Iteration 208/1000 | Loss: 0.00000805
Iteration 209/1000 | Loss: 0.00000804
Iteration 210/1000 | Loss: 0.00000804
Iteration 211/1000 | Loss: 0.00000804
Iteration 212/1000 | Loss: 0.00000804
Iteration 213/1000 | Loss: 0.00000804
Iteration 214/1000 | Loss: 0.00000804
Iteration 215/1000 | Loss: 0.00000804
Iteration 216/1000 | Loss: 0.00000804
Iteration 217/1000 | Loss: 0.00000804
Iteration 218/1000 | Loss: 0.00000804
Iteration 219/1000 | Loss: 0.00000804
Iteration 220/1000 | Loss: 0.00000804
Iteration 221/1000 | Loss: 0.00000804
Iteration 222/1000 | Loss: 0.00000804
Iteration 223/1000 | Loss: 0.00000804
Iteration 224/1000 | Loss: 0.00000804
Iteration 225/1000 | Loss: 0.00000804
Iteration 226/1000 | Loss: 0.00000804
Iteration 227/1000 | Loss: 0.00000804
Iteration 228/1000 | Loss: 0.00000804
Iteration 229/1000 | Loss: 0.00000804
Iteration 230/1000 | Loss: 0.00000804
Iteration 231/1000 | Loss: 0.00000804
Iteration 232/1000 | Loss: 0.00000804
Iteration 233/1000 | Loss: 0.00000804
Iteration 234/1000 | Loss: 0.00000804
Iteration 235/1000 | Loss: 0.00000804
Iteration 236/1000 | Loss: 0.00000804
Iteration 237/1000 | Loss: 0.00000804
Iteration 238/1000 | Loss: 0.00000804
Iteration 239/1000 | Loss: 0.00000804
Iteration 240/1000 | Loss: 0.00000804
Iteration 241/1000 | Loss: 0.00000804
Iteration 242/1000 | Loss: 0.00000804
Iteration 243/1000 | Loss: 0.00000804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [8.03771945356857e-06, 8.03771945356857e-06, 8.03771945356857e-06, 8.03771945356857e-06, 8.03771945356857e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.03771945356857e-06

Optimization complete. Final v2v error: 2.385876417160034 mm

Highest mean error: 2.81907320022583 mm for frame 68

Lowest mean error: 2.0244624614715576 mm for frame 14

Saving results

Total time: 37.42546224594116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440996
Iteration 2/25 | Loss: 0.00085912
Iteration 3/25 | Loss: 0.00079020
Iteration 4/25 | Loss: 0.00078324
Iteration 5/25 | Loss: 0.00078116
Iteration 6/25 | Loss: 0.00078093
Iteration 7/25 | Loss: 0.00078093
Iteration 8/25 | Loss: 0.00078093
Iteration 9/25 | Loss: 0.00078093
Iteration 10/25 | Loss: 0.00078093
Iteration 11/25 | Loss: 0.00078093
Iteration 12/25 | Loss: 0.00078093
Iteration 13/25 | Loss: 0.00078093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007809344679117203, 0.0007809344679117203, 0.0007809344679117203, 0.0007809344679117203, 0.0007809344679117203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007809344679117203

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97916675
Iteration 2/25 | Loss: 0.00028353
Iteration 3/25 | Loss: 0.00028349
Iteration 4/25 | Loss: 0.00028349
Iteration 5/25 | Loss: 0.00028349
Iteration 6/25 | Loss: 0.00028349
Iteration 7/25 | Loss: 0.00028349
Iteration 8/25 | Loss: 0.00028349
Iteration 9/25 | Loss: 0.00028349
Iteration 10/25 | Loss: 0.00028349
Iteration 11/25 | Loss: 0.00028349
Iteration 12/25 | Loss: 0.00028349
Iteration 13/25 | Loss: 0.00028349
Iteration 14/25 | Loss: 0.00028349
Iteration 15/25 | Loss: 0.00028349
Iteration 16/25 | Loss: 0.00028349
Iteration 17/25 | Loss: 0.00028349
Iteration 18/25 | Loss: 0.00028349
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00028348853811621666, 0.00028348853811621666, 0.00028348853811621666, 0.00028348853811621666, 0.00028348853811621666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028348853811621666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028349
Iteration 2/1000 | Loss: 0.00001397
Iteration 3/1000 | Loss: 0.00001005
Iteration 4/1000 | Loss: 0.00000890
Iteration 5/1000 | Loss: 0.00000796
Iteration 6/1000 | Loss: 0.00000761
Iteration 7/1000 | Loss: 0.00000738
Iteration 8/1000 | Loss: 0.00000726
Iteration 9/1000 | Loss: 0.00000717
Iteration 10/1000 | Loss: 0.00000716
Iteration 11/1000 | Loss: 0.00000711
Iteration 12/1000 | Loss: 0.00000708
Iteration 13/1000 | Loss: 0.00000707
Iteration 14/1000 | Loss: 0.00000706
Iteration 15/1000 | Loss: 0.00000705
Iteration 16/1000 | Loss: 0.00000704
Iteration 17/1000 | Loss: 0.00000699
Iteration 18/1000 | Loss: 0.00000698
Iteration 19/1000 | Loss: 0.00000697
Iteration 20/1000 | Loss: 0.00000697
Iteration 21/1000 | Loss: 0.00000696
Iteration 22/1000 | Loss: 0.00000696
Iteration 23/1000 | Loss: 0.00000693
Iteration 24/1000 | Loss: 0.00000693
Iteration 25/1000 | Loss: 0.00000693
Iteration 26/1000 | Loss: 0.00000692
Iteration 27/1000 | Loss: 0.00000690
Iteration 28/1000 | Loss: 0.00000690
Iteration 29/1000 | Loss: 0.00000689
Iteration 30/1000 | Loss: 0.00000689
Iteration 31/1000 | Loss: 0.00000689
Iteration 32/1000 | Loss: 0.00000689
Iteration 33/1000 | Loss: 0.00000686
Iteration 34/1000 | Loss: 0.00000685
Iteration 35/1000 | Loss: 0.00000685
Iteration 36/1000 | Loss: 0.00000684
Iteration 37/1000 | Loss: 0.00000684
Iteration 38/1000 | Loss: 0.00000683
Iteration 39/1000 | Loss: 0.00000681
Iteration 40/1000 | Loss: 0.00000681
Iteration 41/1000 | Loss: 0.00000680
Iteration 42/1000 | Loss: 0.00000678
Iteration 43/1000 | Loss: 0.00000678
Iteration 44/1000 | Loss: 0.00000678
Iteration 45/1000 | Loss: 0.00000678
Iteration 46/1000 | Loss: 0.00000676
Iteration 47/1000 | Loss: 0.00000676
Iteration 48/1000 | Loss: 0.00000675
Iteration 49/1000 | Loss: 0.00000675
Iteration 50/1000 | Loss: 0.00000675
Iteration 51/1000 | Loss: 0.00000674
Iteration 52/1000 | Loss: 0.00000674
Iteration 53/1000 | Loss: 0.00000673
Iteration 54/1000 | Loss: 0.00000673
Iteration 55/1000 | Loss: 0.00000673
Iteration 56/1000 | Loss: 0.00000673
Iteration 57/1000 | Loss: 0.00000673
Iteration 58/1000 | Loss: 0.00000672
Iteration 59/1000 | Loss: 0.00000672
Iteration 60/1000 | Loss: 0.00000671
Iteration 61/1000 | Loss: 0.00000671
Iteration 62/1000 | Loss: 0.00000671
Iteration 63/1000 | Loss: 0.00000671
Iteration 64/1000 | Loss: 0.00000671
Iteration 65/1000 | Loss: 0.00000670
Iteration 66/1000 | Loss: 0.00000670
Iteration 67/1000 | Loss: 0.00000669
Iteration 68/1000 | Loss: 0.00000669
Iteration 69/1000 | Loss: 0.00000669
Iteration 70/1000 | Loss: 0.00000669
Iteration 71/1000 | Loss: 0.00000669
Iteration 72/1000 | Loss: 0.00000668
Iteration 73/1000 | Loss: 0.00000668
Iteration 74/1000 | Loss: 0.00000668
Iteration 75/1000 | Loss: 0.00000668
Iteration 76/1000 | Loss: 0.00000668
Iteration 77/1000 | Loss: 0.00000668
Iteration 78/1000 | Loss: 0.00000667
Iteration 79/1000 | Loss: 0.00000667
Iteration 80/1000 | Loss: 0.00000666
Iteration 81/1000 | Loss: 0.00000666
Iteration 82/1000 | Loss: 0.00000665
Iteration 83/1000 | Loss: 0.00000665
Iteration 84/1000 | Loss: 0.00000665
Iteration 85/1000 | Loss: 0.00000665
Iteration 86/1000 | Loss: 0.00000665
Iteration 87/1000 | Loss: 0.00000665
Iteration 88/1000 | Loss: 0.00000665
Iteration 89/1000 | Loss: 0.00000665
Iteration 90/1000 | Loss: 0.00000665
Iteration 91/1000 | Loss: 0.00000665
Iteration 92/1000 | Loss: 0.00000664
Iteration 93/1000 | Loss: 0.00000664
Iteration 94/1000 | Loss: 0.00000664
Iteration 95/1000 | Loss: 0.00000663
Iteration 96/1000 | Loss: 0.00000663
Iteration 97/1000 | Loss: 0.00000663
Iteration 98/1000 | Loss: 0.00000662
Iteration 99/1000 | Loss: 0.00000661
Iteration 100/1000 | Loss: 0.00000661
Iteration 101/1000 | Loss: 0.00000661
Iteration 102/1000 | Loss: 0.00000661
Iteration 103/1000 | Loss: 0.00000660
Iteration 104/1000 | Loss: 0.00000660
Iteration 105/1000 | Loss: 0.00000660
Iteration 106/1000 | Loss: 0.00000660
Iteration 107/1000 | Loss: 0.00000660
Iteration 108/1000 | Loss: 0.00000660
Iteration 109/1000 | Loss: 0.00000660
Iteration 110/1000 | Loss: 0.00000660
Iteration 111/1000 | Loss: 0.00000660
Iteration 112/1000 | Loss: 0.00000660
Iteration 113/1000 | Loss: 0.00000660
Iteration 114/1000 | Loss: 0.00000660
Iteration 115/1000 | Loss: 0.00000660
Iteration 116/1000 | Loss: 0.00000660
Iteration 117/1000 | Loss: 0.00000660
Iteration 118/1000 | Loss: 0.00000660
Iteration 119/1000 | Loss: 0.00000660
Iteration 120/1000 | Loss: 0.00000660
Iteration 121/1000 | Loss: 0.00000660
Iteration 122/1000 | Loss: 0.00000660
Iteration 123/1000 | Loss: 0.00000660
Iteration 124/1000 | Loss: 0.00000660
Iteration 125/1000 | Loss: 0.00000660
Iteration 126/1000 | Loss: 0.00000660
Iteration 127/1000 | Loss: 0.00000660
Iteration 128/1000 | Loss: 0.00000660
Iteration 129/1000 | Loss: 0.00000660
Iteration 130/1000 | Loss: 0.00000660
Iteration 131/1000 | Loss: 0.00000660
Iteration 132/1000 | Loss: 0.00000660
Iteration 133/1000 | Loss: 0.00000660
Iteration 134/1000 | Loss: 0.00000660
Iteration 135/1000 | Loss: 0.00000660
Iteration 136/1000 | Loss: 0.00000660
Iteration 137/1000 | Loss: 0.00000660
Iteration 138/1000 | Loss: 0.00000660
Iteration 139/1000 | Loss: 0.00000660
Iteration 140/1000 | Loss: 0.00000660
Iteration 141/1000 | Loss: 0.00000660
Iteration 142/1000 | Loss: 0.00000660
Iteration 143/1000 | Loss: 0.00000660
Iteration 144/1000 | Loss: 0.00000660
Iteration 145/1000 | Loss: 0.00000660
Iteration 146/1000 | Loss: 0.00000660
Iteration 147/1000 | Loss: 0.00000660
Iteration 148/1000 | Loss: 0.00000660
Iteration 149/1000 | Loss: 0.00000660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [6.600694177905098e-06, 6.600694177905098e-06, 6.600694177905098e-06, 6.600694177905098e-06, 6.600694177905098e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.600694177905098e-06

Optimization complete. Final v2v error: 2.2483317852020264 mm

Highest mean error: 2.6764938831329346 mm for frame 143

Lowest mean error: 2.083120107650757 mm for frame 125

Saving results

Total time: 35.306076526641846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043448
Iteration 2/25 | Loss: 0.00219626
Iteration 3/25 | Loss: 0.00163654
Iteration 4/25 | Loss: 0.00140230
Iteration 5/25 | Loss: 0.00120301
Iteration 6/25 | Loss: 0.00121069
Iteration 7/25 | Loss: 0.00113676
Iteration 8/25 | Loss: 0.00112109
Iteration 9/25 | Loss: 0.00109164
Iteration 10/25 | Loss: 0.00107892
Iteration 11/25 | Loss: 0.00106474
Iteration 12/25 | Loss: 0.00105744
Iteration 13/25 | Loss: 0.00103809
Iteration 14/25 | Loss: 0.00102396
Iteration 15/25 | Loss: 0.00103191
Iteration 16/25 | Loss: 0.00101006
Iteration 17/25 | Loss: 0.00098640
Iteration 18/25 | Loss: 0.00099471
Iteration 19/25 | Loss: 0.00097906
Iteration 20/25 | Loss: 0.00097816
Iteration 21/25 | Loss: 0.00097094
Iteration 22/25 | Loss: 0.00097130
Iteration 23/25 | Loss: 0.00096305
Iteration 24/25 | Loss: 0.00096205
Iteration 25/25 | Loss: 0.00096180

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44266343
Iteration 2/25 | Loss: 0.00075356
Iteration 3/25 | Loss: 0.00075355
Iteration 4/25 | Loss: 0.00075355
Iteration 5/25 | Loss: 0.00075355
Iteration 6/25 | Loss: 0.00075355
Iteration 7/25 | Loss: 0.00075355
Iteration 8/25 | Loss: 0.00075355
Iteration 9/25 | Loss: 0.00075355
Iteration 10/25 | Loss: 0.00075355
Iteration 11/25 | Loss: 0.00075355
Iteration 12/25 | Loss: 0.00075355
Iteration 13/25 | Loss: 0.00075355
Iteration 14/25 | Loss: 0.00075355
Iteration 15/25 | Loss: 0.00075355
Iteration 16/25 | Loss: 0.00075355
Iteration 17/25 | Loss: 0.00075355
Iteration 18/25 | Loss: 0.00075355
Iteration 19/25 | Loss: 0.00075355
Iteration 20/25 | Loss: 0.00075355
Iteration 21/25 | Loss: 0.00075355
Iteration 22/25 | Loss: 0.00075355
Iteration 23/25 | Loss: 0.00075355
Iteration 24/25 | Loss: 0.00075355
Iteration 25/25 | Loss: 0.00075355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075355
Iteration 2/1000 | Loss: 0.00012266
Iteration 3/1000 | Loss: 0.00031557
Iteration 4/1000 | Loss: 0.00034881
Iteration 5/1000 | Loss: 0.00007678
Iteration 6/1000 | Loss: 0.00005760
Iteration 7/1000 | Loss: 0.00007460
Iteration 8/1000 | Loss: 0.00011353
Iteration 9/1000 | Loss: 0.00009855
Iteration 10/1000 | Loss: 0.00003855
Iteration 11/1000 | Loss: 0.00010345
Iteration 12/1000 | Loss: 0.00187169
Iteration 13/1000 | Loss: 0.00046207
Iteration 14/1000 | Loss: 0.00017587
Iteration 15/1000 | Loss: 0.00005246
Iteration 16/1000 | Loss: 0.00026506
Iteration 17/1000 | Loss: 0.00011866
Iteration 18/1000 | Loss: 0.00162090
Iteration 19/1000 | Loss: 0.00174252
Iteration 20/1000 | Loss: 0.00008578
Iteration 21/1000 | Loss: 0.00002380
Iteration 22/1000 | Loss: 0.00030545
Iteration 23/1000 | Loss: 0.00002103
Iteration 24/1000 | Loss: 0.00001955
Iteration 25/1000 | Loss: 0.00013086
Iteration 26/1000 | Loss: 0.00094958
Iteration 27/1000 | Loss: 0.00004365
Iteration 28/1000 | Loss: 0.00009713
Iteration 29/1000 | Loss: 0.00002389
Iteration 30/1000 | Loss: 0.00008078
Iteration 31/1000 | Loss: 0.00002053
Iteration 32/1000 | Loss: 0.00002643
Iteration 33/1000 | Loss: 0.00001711
Iteration 34/1000 | Loss: 0.00014118
Iteration 35/1000 | Loss: 0.00002257
Iteration 36/1000 | Loss: 0.00011743
Iteration 37/1000 | Loss: 0.00001664
Iteration 38/1000 | Loss: 0.00008870
Iteration 39/1000 | Loss: 0.00005800
Iteration 40/1000 | Loss: 0.00004472
Iteration 41/1000 | Loss: 0.00001607
Iteration 42/1000 | Loss: 0.00001598
Iteration 43/1000 | Loss: 0.00001590
Iteration 44/1000 | Loss: 0.00007899
Iteration 45/1000 | Loss: 0.00014063
Iteration 46/1000 | Loss: 0.00020214
Iteration 47/1000 | Loss: 0.00004343
Iteration 48/1000 | Loss: 0.00004369
Iteration 49/1000 | Loss: 0.00001602
Iteration 50/1000 | Loss: 0.00011655
Iteration 51/1000 | Loss: 0.00011655
Iteration 52/1000 | Loss: 0.00094269
Iteration 53/1000 | Loss: 0.00023212
Iteration 54/1000 | Loss: 0.00007969
Iteration 55/1000 | Loss: 0.00012031
Iteration 56/1000 | Loss: 0.00003607
Iteration 57/1000 | Loss: 0.00002338
Iteration 58/1000 | Loss: 0.00001603
Iteration 59/1000 | Loss: 0.00003744
Iteration 60/1000 | Loss: 0.00004927
Iteration 61/1000 | Loss: 0.00001574
Iteration 62/1000 | Loss: 0.00001565
Iteration 63/1000 | Loss: 0.00004550
Iteration 64/1000 | Loss: 0.00001577
Iteration 65/1000 | Loss: 0.00001559
Iteration 66/1000 | Loss: 0.00001558
Iteration 67/1000 | Loss: 0.00001557
Iteration 68/1000 | Loss: 0.00001557
Iteration 69/1000 | Loss: 0.00001557
Iteration 70/1000 | Loss: 0.00001556
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00001555
Iteration 75/1000 | Loss: 0.00001555
Iteration 76/1000 | Loss: 0.00001555
Iteration 77/1000 | Loss: 0.00001555
Iteration 78/1000 | Loss: 0.00001554
Iteration 79/1000 | Loss: 0.00001554
Iteration 80/1000 | Loss: 0.00001554
Iteration 81/1000 | Loss: 0.00001554
Iteration 82/1000 | Loss: 0.00001554
Iteration 83/1000 | Loss: 0.00001553
Iteration 84/1000 | Loss: 0.00001553
Iteration 85/1000 | Loss: 0.00001553
Iteration 86/1000 | Loss: 0.00001552
Iteration 87/1000 | Loss: 0.00001552
Iteration 88/1000 | Loss: 0.00001552
Iteration 89/1000 | Loss: 0.00001552
Iteration 90/1000 | Loss: 0.00001552
Iteration 91/1000 | Loss: 0.00001552
Iteration 92/1000 | Loss: 0.00001551
Iteration 93/1000 | Loss: 0.00001551
Iteration 94/1000 | Loss: 0.00001551
Iteration 95/1000 | Loss: 0.00001551
Iteration 96/1000 | Loss: 0.00001550
Iteration 97/1000 | Loss: 0.00001550
Iteration 98/1000 | Loss: 0.00001550
Iteration 99/1000 | Loss: 0.00001550
Iteration 100/1000 | Loss: 0.00001550
Iteration 101/1000 | Loss: 0.00001549
Iteration 102/1000 | Loss: 0.00001549
Iteration 103/1000 | Loss: 0.00001549
Iteration 104/1000 | Loss: 0.00001549
Iteration 105/1000 | Loss: 0.00001549
Iteration 106/1000 | Loss: 0.00001549
Iteration 107/1000 | Loss: 0.00001549
Iteration 108/1000 | Loss: 0.00001549
Iteration 109/1000 | Loss: 0.00001549
Iteration 110/1000 | Loss: 0.00001549
Iteration 111/1000 | Loss: 0.00001549
Iteration 112/1000 | Loss: 0.00001549
Iteration 113/1000 | Loss: 0.00001549
Iteration 114/1000 | Loss: 0.00001549
Iteration 115/1000 | Loss: 0.00001549
Iteration 116/1000 | Loss: 0.00001549
Iteration 117/1000 | Loss: 0.00001549
Iteration 118/1000 | Loss: 0.00001549
Iteration 119/1000 | Loss: 0.00001549
Iteration 120/1000 | Loss: 0.00001549
Iteration 121/1000 | Loss: 0.00001549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.5488350982195698e-05, 1.5488350982195698e-05, 1.5488350982195698e-05, 1.5488350982195698e-05, 1.5488350982195698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5488350982195698e-05

Optimization complete. Final v2v error: 3.264031171798706 mm

Highest mean error: 3.8998031616210938 mm for frame 81

Lowest mean error: 2.867305040359497 mm for frame 36

Saving results

Total time: 133.0940763950348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782244
Iteration 2/25 | Loss: 0.00113343
Iteration 3/25 | Loss: 0.00088715
Iteration 4/25 | Loss: 0.00084687
Iteration 5/25 | Loss: 0.00084010
Iteration 6/25 | Loss: 0.00083840
Iteration 7/25 | Loss: 0.00083840
Iteration 8/25 | Loss: 0.00083840
Iteration 9/25 | Loss: 0.00083840
Iteration 10/25 | Loss: 0.00083840
Iteration 11/25 | Loss: 0.00083840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000838397245388478, 0.000838397245388478, 0.000838397245388478, 0.000838397245388478, 0.000838397245388478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000838397245388478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35159075
Iteration 2/25 | Loss: 0.00029713
Iteration 3/25 | Loss: 0.00029710
Iteration 4/25 | Loss: 0.00029710
Iteration 5/25 | Loss: 0.00029710
Iteration 6/25 | Loss: 0.00029710
Iteration 7/25 | Loss: 0.00029710
Iteration 8/25 | Loss: 0.00029710
Iteration 9/25 | Loss: 0.00029710
Iteration 10/25 | Loss: 0.00029710
Iteration 11/25 | Loss: 0.00029710
Iteration 12/25 | Loss: 0.00029710
Iteration 13/25 | Loss: 0.00029710
Iteration 14/25 | Loss: 0.00029710
Iteration 15/25 | Loss: 0.00029710
Iteration 16/25 | Loss: 0.00029710
Iteration 17/25 | Loss: 0.00029710
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00029709847876802087, 0.00029709847876802087, 0.00029709847876802087, 0.00029709847876802087, 0.00029709847876802087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029709847876802087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029710
Iteration 2/1000 | Loss: 0.00002362
Iteration 3/1000 | Loss: 0.00001615
Iteration 4/1000 | Loss: 0.00001469
Iteration 5/1000 | Loss: 0.00001389
Iteration 6/1000 | Loss: 0.00001321
Iteration 7/1000 | Loss: 0.00001288
Iteration 8/1000 | Loss: 0.00001258
Iteration 9/1000 | Loss: 0.00001240
Iteration 10/1000 | Loss: 0.00001221
Iteration 11/1000 | Loss: 0.00001213
Iteration 12/1000 | Loss: 0.00001204
Iteration 13/1000 | Loss: 0.00001197
Iteration 14/1000 | Loss: 0.00001190
Iteration 15/1000 | Loss: 0.00001189
Iteration 16/1000 | Loss: 0.00001186
Iteration 17/1000 | Loss: 0.00001185
Iteration 18/1000 | Loss: 0.00001185
Iteration 19/1000 | Loss: 0.00001185
Iteration 20/1000 | Loss: 0.00001184
Iteration 21/1000 | Loss: 0.00001184
Iteration 22/1000 | Loss: 0.00001184
Iteration 23/1000 | Loss: 0.00001183
Iteration 24/1000 | Loss: 0.00001183
Iteration 25/1000 | Loss: 0.00001182
Iteration 26/1000 | Loss: 0.00001182
Iteration 27/1000 | Loss: 0.00001179
Iteration 28/1000 | Loss: 0.00001178
Iteration 29/1000 | Loss: 0.00001177
Iteration 30/1000 | Loss: 0.00001172
Iteration 31/1000 | Loss: 0.00001172
Iteration 32/1000 | Loss: 0.00001172
Iteration 33/1000 | Loss: 0.00001172
Iteration 34/1000 | Loss: 0.00001172
Iteration 35/1000 | Loss: 0.00001172
Iteration 36/1000 | Loss: 0.00001171
Iteration 37/1000 | Loss: 0.00001170
Iteration 38/1000 | Loss: 0.00001169
Iteration 39/1000 | Loss: 0.00001169
Iteration 40/1000 | Loss: 0.00001169
Iteration 41/1000 | Loss: 0.00001168
Iteration 42/1000 | Loss: 0.00001168
Iteration 43/1000 | Loss: 0.00001167
Iteration 44/1000 | Loss: 0.00001167
Iteration 45/1000 | Loss: 0.00001166
Iteration 46/1000 | Loss: 0.00001166
Iteration 47/1000 | Loss: 0.00001166
Iteration 48/1000 | Loss: 0.00001165
Iteration 49/1000 | Loss: 0.00001165
Iteration 50/1000 | Loss: 0.00001164
Iteration 51/1000 | Loss: 0.00001164
Iteration 52/1000 | Loss: 0.00001164
Iteration 53/1000 | Loss: 0.00001164
Iteration 54/1000 | Loss: 0.00001164
Iteration 55/1000 | Loss: 0.00001164
Iteration 56/1000 | Loss: 0.00001164
Iteration 57/1000 | Loss: 0.00001164
Iteration 58/1000 | Loss: 0.00001164
Iteration 59/1000 | Loss: 0.00001163
Iteration 60/1000 | Loss: 0.00001163
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001162
Iteration 65/1000 | Loss: 0.00001162
Iteration 66/1000 | Loss: 0.00001161
Iteration 67/1000 | Loss: 0.00001161
Iteration 68/1000 | Loss: 0.00001160
Iteration 69/1000 | Loss: 0.00001159
Iteration 70/1000 | Loss: 0.00001159
Iteration 71/1000 | Loss: 0.00001159
Iteration 72/1000 | Loss: 0.00001159
Iteration 73/1000 | Loss: 0.00001159
Iteration 74/1000 | Loss: 0.00001158
Iteration 75/1000 | Loss: 0.00001158
Iteration 76/1000 | Loss: 0.00001158
Iteration 77/1000 | Loss: 0.00001158
Iteration 78/1000 | Loss: 0.00001158
Iteration 79/1000 | Loss: 0.00001158
Iteration 80/1000 | Loss: 0.00001157
Iteration 81/1000 | Loss: 0.00001157
Iteration 82/1000 | Loss: 0.00001157
Iteration 83/1000 | Loss: 0.00001157
Iteration 84/1000 | Loss: 0.00001156
Iteration 85/1000 | Loss: 0.00001156
Iteration 86/1000 | Loss: 0.00001156
Iteration 87/1000 | Loss: 0.00001156
Iteration 88/1000 | Loss: 0.00001156
Iteration 89/1000 | Loss: 0.00001156
Iteration 90/1000 | Loss: 0.00001156
Iteration 91/1000 | Loss: 0.00001156
Iteration 92/1000 | Loss: 0.00001156
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001156
Iteration 95/1000 | Loss: 0.00001156
Iteration 96/1000 | Loss: 0.00001156
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001156
Iteration 99/1000 | Loss: 0.00001156
Iteration 100/1000 | Loss: 0.00001156
Iteration 101/1000 | Loss: 0.00001156
Iteration 102/1000 | Loss: 0.00001156
Iteration 103/1000 | Loss: 0.00001156
Iteration 104/1000 | Loss: 0.00001156
Iteration 105/1000 | Loss: 0.00001156
Iteration 106/1000 | Loss: 0.00001156
Iteration 107/1000 | Loss: 0.00001156
Iteration 108/1000 | Loss: 0.00001156
Iteration 109/1000 | Loss: 0.00001156
Iteration 110/1000 | Loss: 0.00001156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.1557060133782215e-05, 1.1557060133782215e-05, 1.1557060133782215e-05, 1.1557060133782215e-05, 1.1557060133782215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1557060133782215e-05

Optimization complete. Final v2v error: 2.904905319213867 mm

Highest mean error: 3.286111831665039 mm for frame 202

Lowest mean error: 2.6992011070251465 mm for frame 217

Saving results

Total time: 38.56813716888428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386975
Iteration 2/25 | Loss: 0.00097271
Iteration 3/25 | Loss: 0.00083742
Iteration 4/25 | Loss: 0.00082683
Iteration 5/25 | Loss: 0.00082381
Iteration 6/25 | Loss: 0.00082301
Iteration 7/25 | Loss: 0.00082297
Iteration 8/25 | Loss: 0.00082297
Iteration 9/25 | Loss: 0.00082297
Iteration 10/25 | Loss: 0.00082297
Iteration 11/25 | Loss: 0.00082297
Iteration 12/25 | Loss: 0.00082297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008229748345911503, 0.0008229748345911503, 0.0008229748345911503, 0.0008229748345911503, 0.0008229748345911503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008229748345911503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37572336
Iteration 2/25 | Loss: 0.00034654
Iteration 3/25 | Loss: 0.00034654
Iteration 4/25 | Loss: 0.00034654
Iteration 5/25 | Loss: 0.00034654
Iteration 6/25 | Loss: 0.00034654
Iteration 7/25 | Loss: 0.00034654
Iteration 8/25 | Loss: 0.00034654
Iteration 9/25 | Loss: 0.00034654
Iteration 10/25 | Loss: 0.00034654
Iteration 11/25 | Loss: 0.00034654
Iteration 12/25 | Loss: 0.00034654
Iteration 13/25 | Loss: 0.00034654
Iteration 14/25 | Loss: 0.00034654
Iteration 15/25 | Loss: 0.00034654
Iteration 16/25 | Loss: 0.00034654
Iteration 17/25 | Loss: 0.00034654
Iteration 18/25 | Loss: 0.00034654
Iteration 19/25 | Loss: 0.00034654
Iteration 20/25 | Loss: 0.00034654
Iteration 21/25 | Loss: 0.00034654
Iteration 22/25 | Loss: 0.00034654
Iteration 23/25 | Loss: 0.00034654
Iteration 24/25 | Loss: 0.00034654
Iteration 25/25 | Loss: 0.00034654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034654
Iteration 2/1000 | Loss: 0.00003039
Iteration 3/1000 | Loss: 0.00001810
Iteration 4/1000 | Loss: 0.00001357
Iteration 5/1000 | Loss: 0.00001226
Iteration 6/1000 | Loss: 0.00001152
Iteration 7/1000 | Loss: 0.00001090
Iteration 8/1000 | Loss: 0.00001064
Iteration 9/1000 | Loss: 0.00001041
Iteration 10/1000 | Loss: 0.00001035
Iteration 11/1000 | Loss: 0.00001024
Iteration 12/1000 | Loss: 0.00001024
Iteration 13/1000 | Loss: 0.00001018
Iteration 14/1000 | Loss: 0.00001016
Iteration 15/1000 | Loss: 0.00001016
Iteration 16/1000 | Loss: 0.00001012
Iteration 17/1000 | Loss: 0.00001012
Iteration 18/1000 | Loss: 0.00001012
Iteration 19/1000 | Loss: 0.00001011
Iteration 20/1000 | Loss: 0.00001011
Iteration 21/1000 | Loss: 0.00001010
Iteration 22/1000 | Loss: 0.00001009
Iteration 23/1000 | Loss: 0.00001008
Iteration 24/1000 | Loss: 0.00001004
Iteration 25/1000 | Loss: 0.00001002
Iteration 26/1000 | Loss: 0.00001002
Iteration 27/1000 | Loss: 0.00001001
Iteration 28/1000 | Loss: 0.00001001
Iteration 29/1000 | Loss: 0.00001000
Iteration 30/1000 | Loss: 0.00001000
Iteration 31/1000 | Loss: 0.00000999
Iteration 32/1000 | Loss: 0.00000999
Iteration 33/1000 | Loss: 0.00000999
Iteration 34/1000 | Loss: 0.00000998
Iteration 35/1000 | Loss: 0.00000995
Iteration 36/1000 | Loss: 0.00000995
Iteration 37/1000 | Loss: 0.00000994
Iteration 38/1000 | Loss: 0.00000994
Iteration 39/1000 | Loss: 0.00000993
Iteration 40/1000 | Loss: 0.00000993
Iteration 41/1000 | Loss: 0.00000992
Iteration 42/1000 | Loss: 0.00000992
Iteration 43/1000 | Loss: 0.00000992
Iteration 44/1000 | Loss: 0.00000992
Iteration 45/1000 | Loss: 0.00000991
Iteration 46/1000 | Loss: 0.00000991
Iteration 47/1000 | Loss: 0.00000991
Iteration 48/1000 | Loss: 0.00000991
Iteration 49/1000 | Loss: 0.00000991
Iteration 50/1000 | Loss: 0.00000990
Iteration 51/1000 | Loss: 0.00000990
Iteration 52/1000 | Loss: 0.00000989
Iteration 53/1000 | Loss: 0.00000989
Iteration 54/1000 | Loss: 0.00000988
Iteration 55/1000 | Loss: 0.00000988
Iteration 56/1000 | Loss: 0.00000987
Iteration 57/1000 | Loss: 0.00000987
Iteration 58/1000 | Loss: 0.00000987
Iteration 59/1000 | Loss: 0.00000987
Iteration 60/1000 | Loss: 0.00000986
Iteration 61/1000 | Loss: 0.00000986
Iteration 62/1000 | Loss: 0.00000986
Iteration 63/1000 | Loss: 0.00000986
Iteration 64/1000 | Loss: 0.00000986
Iteration 65/1000 | Loss: 0.00000986
Iteration 66/1000 | Loss: 0.00000986
Iteration 67/1000 | Loss: 0.00000986
Iteration 68/1000 | Loss: 0.00000985
Iteration 69/1000 | Loss: 0.00000985
Iteration 70/1000 | Loss: 0.00000985
Iteration 71/1000 | Loss: 0.00000985
Iteration 72/1000 | Loss: 0.00000985
Iteration 73/1000 | Loss: 0.00000985
Iteration 74/1000 | Loss: 0.00000985
Iteration 75/1000 | Loss: 0.00000985
Iteration 76/1000 | Loss: 0.00000984
Iteration 77/1000 | Loss: 0.00000984
Iteration 78/1000 | Loss: 0.00000984
Iteration 79/1000 | Loss: 0.00000984
Iteration 80/1000 | Loss: 0.00000984
Iteration 81/1000 | Loss: 0.00000984
Iteration 82/1000 | Loss: 0.00000983
Iteration 83/1000 | Loss: 0.00000982
Iteration 84/1000 | Loss: 0.00000982
Iteration 85/1000 | Loss: 0.00000982
Iteration 86/1000 | Loss: 0.00000982
Iteration 87/1000 | Loss: 0.00000982
Iteration 88/1000 | Loss: 0.00000982
Iteration 89/1000 | Loss: 0.00000981
Iteration 90/1000 | Loss: 0.00000981
Iteration 91/1000 | Loss: 0.00000981
Iteration 92/1000 | Loss: 0.00000981
Iteration 93/1000 | Loss: 0.00000980
Iteration 94/1000 | Loss: 0.00000980
Iteration 95/1000 | Loss: 0.00000980
Iteration 96/1000 | Loss: 0.00000980
Iteration 97/1000 | Loss: 0.00000979
Iteration 98/1000 | Loss: 0.00000979
Iteration 99/1000 | Loss: 0.00000979
Iteration 100/1000 | Loss: 0.00000979
Iteration 101/1000 | Loss: 0.00000978
Iteration 102/1000 | Loss: 0.00000978
Iteration 103/1000 | Loss: 0.00000978
Iteration 104/1000 | Loss: 0.00000978
Iteration 105/1000 | Loss: 0.00000978
Iteration 106/1000 | Loss: 0.00000978
Iteration 107/1000 | Loss: 0.00000978
Iteration 108/1000 | Loss: 0.00000977
Iteration 109/1000 | Loss: 0.00000977
Iteration 110/1000 | Loss: 0.00000977
Iteration 111/1000 | Loss: 0.00000977
Iteration 112/1000 | Loss: 0.00000977
Iteration 113/1000 | Loss: 0.00000977
Iteration 114/1000 | Loss: 0.00000977
Iteration 115/1000 | Loss: 0.00000977
Iteration 116/1000 | Loss: 0.00000977
Iteration 117/1000 | Loss: 0.00000977
Iteration 118/1000 | Loss: 0.00000977
Iteration 119/1000 | Loss: 0.00000977
Iteration 120/1000 | Loss: 0.00000977
Iteration 121/1000 | Loss: 0.00000976
Iteration 122/1000 | Loss: 0.00000976
Iteration 123/1000 | Loss: 0.00000976
Iteration 124/1000 | Loss: 0.00000976
Iteration 125/1000 | Loss: 0.00000976
Iteration 126/1000 | Loss: 0.00000976
Iteration 127/1000 | Loss: 0.00000976
Iteration 128/1000 | Loss: 0.00000976
Iteration 129/1000 | Loss: 0.00000975
Iteration 130/1000 | Loss: 0.00000975
Iteration 131/1000 | Loss: 0.00000975
Iteration 132/1000 | Loss: 0.00000975
Iteration 133/1000 | Loss: 0.00000975
Iteration 134/1000 | Loss: 0.00000975
Iteration 135/1000 | Loss: 0.00000975
Iteration 136/1000 | Loss: 0.00000975
Iteration 137/1000 | Loss: 0.00000975
Iteration 138/1000 | Loss: 0.00000975
Iteration 139/1000 | Loss: 0.00000975
Iteration 140/1000 | Loss: 0.00000975
Iteration 141/1000 | Loss: 0.00000975
Iteration 142/1000 | Loss: 0.00000975
Iteration 143/1000 | Loss: 0.00000975
Iteration 144/1000 | Loss: 0.00000975
Iteration 145/1000 | Loss: 0.00000974
Iteration 146/1000 | Loss: 0.00000974
Iteration 147/1000 | Loss: 0.00000974
Iteration 148/1000 | Loss: 0.00000974
Iteration 149/1000 | Loss: 0.00000974
Iteration 150/1000 | Loss: 0.00000974
Iteration 151/1000 | Loss: 0.00000974
Iteration 152/1000 | Loss: 0.00000974
Iteration 153/1000 | Loss: 0.00000974
Iteration 154/1000 | Loss: 0.00000974
Iteration 155/1000 | Loss: 0.00000974
Iteration 156/1000 | Loss: 0.00000974
Iteration 157/1000 | Loss: 0.00000974
Iteration 158/1000 | Loss: 0.00000973
Iteration 159/1000 | Loss: 0.00000973
Iteration 160/1000 | Loss: 0.00000973
Iteration 161/1000 | Loss: 0.00000973
Iteration 162/1000 | Loss: 0.00000973
Iteration 163/1000 | Loss: 0.00000973
Iteration 164/1000 | Loss: 0.00000973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [9.734089871926699e-06, 9.734089871926699e-06, 9.734089871926699e-06, 9.734089871926699e-06, 9.734089871926699e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.734089871926699e-06

Optimization complete. Final v2v error: 2.5869057178497314 mm

Highest mean error: 3.1601905822753906 mm for frame 67

Lowest mean error: 2.2506182193756104 mm for frame 12

Saving results

Total time: 35.401524782180786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541679
Iteration 2/25 | Loss: 0.00106649
Iteration 3/25 | Loss: 0.00089574
Iteration 4/25 | Loss: 0.00086252
Iteration 5/25 | Loss: 0.00085338
Iteration 6/25 | Loss: 0.00085130
Iteration 7/25 | Loss: 0.00085126
Iteration 8/25 | Loss: 0.00085126
Iteration 9/25 | Loss: 0.00085126
Iteration 10/25 | Loss: 0.00085126
Iteration 11/25 | Loss: 0.00085126
Iteration 12/25 | Loss: 0.00085126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00085126340854913, 0.00085126340854913, 0.00085126340854913, 0.00085126340854913, 0.00085126340854913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00085126340854913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.99785471
Iteration 2/25 | Loss: 0.00041165
Iteration 3/25 | Loss: 0.00041164
Iteration 4/25 | Loss: 0.00041164
Iteration 5/25 | Loss: 0.00041164
Iteration 6/25 | Loss: 0.00041164
Iteration 7/25 | Loss: 0.00041163
Iteration 8/25 | Loss: 0.00041163
Iteration 9/25 | Loss: 0.00041163
Iteration 10/25 | Loss: 0.00041163
Iteration 11/25 | Loss: 0.00041163
Iteration 12/25 | Loss: 0.00041163
Iteration 13/25 | Loss: 0.00041163
Iteration 14/25 | Loss: 0.00041163
Iteration 15/25 | Loss: 0.00041163
Iteration 16/25 | Loss: 0.00041163
Iteration 17/25 | Loss: 0.00041163
Iteration 18/25 | Loss: 0.00041163
Iteration 19/25 | Loss: 0.00041163
Iteration 20/25 | Loss: 0.00041163
Iteration 21/25 | Loss: 0.00041163
Iteration 22/25 | Loss: 0.00041163
Iteration 23/25 | Loss: 0.00041163
Iteration 24/25 | Loss: 0.00041163
Iteration 25/25 | Loss: 0.00041163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041163
Iteration 2/1000 | Loss: 0.00002287
Iteration 3/1000 | Loss: 0.00001655
Iteration 4/1000 | Loss: 0.00001504
Iteration 5/1000 | Loss: 0.00001418
Iteration 6/1000 | Loss: 0.00001364
Iteration 7/1000 | Loss: 0.00001320
Iteration 8/1000 | Loss: 0.00001306
Iteration 9/1000 | Loss: 0.00001284
Iteration 10/1000 | Loss: 0.00001276
Iteration 11/1000 | Loss: 0.00001268
Iteration 12/1000 | Loss: 0.00001266
Iteration 13/1000 | Loss: 0.00001262
Iteration 14/1000 | Loss: 0.00001260
Iteration 15/1000 | Loss: 0.00001259
Iteration 16/1000 | Loss: 0.00001259
Iteration 17/1000 | Loss: 0.00001255
Iteration 18/1000 | Loss: 0.00001254
Iteration 19/1000 | Loss: 0.00001253
Iteration 20/1000 | Loss: 0.00001253
Iteration 21/1000 | Loss: 0.00001251
Iteration 22/1000 | Loss: 0.00001246
Iteration 23/1000 | Loss: 0.00001246
Iteration 24/1000 | Loss: 0.00001243
Iteration 25/1000 | Loss: 0.00001242
Iteration 26/1000 | Loss: 0.00001240
Iteration 27/1000 | Loss: 0.00001240
Iteration 28/1000 | Loss: 0.00001240
Iteration 29/1000 | Loss: 0.00001239
Iteration 30/1000 | Loss: 0.00001239
Iteration 31/1000 | Loss: 0.00001239
Iteration 32/1000 | Loss: 0.00001239
Iteration 33/1000 | Loss: 0.00001238
Iteration 34/1000 | Loss: 0.00001238
Iteration 35/1000 | Loss: 0.00001237
Iteration 36/1000 | Loss: 0.00001235
Iteration 37/1000 | Loss: 0.00001235
Iteration 38/1000 | Loss: 0.00001235
Iteration 39/1000 | Loss: 0.00001235
Iteration 40/1000 | Loss: 0.00001235
Iteration 41/1000 | Loss: 0.00001235
Iteration 42/1000 | Loss: 0.00001235
Iteration 43/1000 | Loss: 0.00001234
Iteration 44/1000 | Loss: 0.00001234
Iteration 45/1000 | Loss: 0.00001234
Iteration 46/1000 | Loss: 0.00001233
Iteration 47/1000 | Loss: 0.00001233
Iteration 48/1000 | Loss: 0.00001232
Iteration 49/1000 | Loss: 0.00001231
Iteration 50/1000 | Loss: 0.00001231
Iteration 51/1000 | Loss: 0.00001230
Iteration 52/1000 | Loss: 0.00001230
Iteration 53/1000 | Loss: 0.00001229
Iteration 54/1000 | Loss: 0.00001229
Iteration 55/1000 | Loss: 0.00001229
Iteration 56/1000 | Loss: 0.00001229
Iteration 57/1000 | Loss: 0.00001228
Iteration 58/1000 | Loss: 0.00001228
Iteration 59/1000 | Loss: 0.00001228
Iteration 60/1000 | Loss: 0.00001228
Iteration 61/1000 | Loss: 0.00001227
Iteration 62/1000 | Loss: 0.00001227
Iteration 63/1000 | Loss: 0.00001227
Iteration 64/1000 | Loss: 0.00001227
Iteration 65/1000 | Loss: 0.00001226
Iteration 66/1000 | Loss: 0.00001226
Iteration 67/1000 | Loss: 0.00001226
Iteration 68/1000 | Loss: 0.00001226
Iteration 69/1000 | Loss: 0.00001226
Iteration 70/1000 | Loss: 0.00001226
Iteration 71/1000 | Loss: 0.00001226
Iteration 72/1000 | Loss: 0.00001224
Iteration 73/1000 | Loss: 0.00001224
Iteration 74/1000 | Loss: 0.00001223
Iteration 75/1000 | Loss: 0.00001223
Iteration 76/1000 | Loss: 0.00001223
Iteration 77/1000 | Loss: 0.00001223
Iteration 78/1000 | Loss: 0.00001222
Iteration 79/1000 | Loss: 0.00001222
Iteration 80/1000 | Loss: 0.00001222
Iteration 81/1000 | Loss: 0.00001222
Iteration 82/1000 | Loss: 0.00001222
Iteration 83/1000 | Loss: 0.00001222
Iteration 84/1000 | Loss: 0.00001222
Iteration 85/1000 | Loss: 0.00001222
Iteration 86/1000 | Loss: 0.00001221
Iteration 87/1000 | Loss: 0.00001221
Iteration 88/1000 | Loss: 0.00001221
Iteration 89/1000 | Loss: 0.00001220
Iteration 90/1000 | Loss: 0.00001220
Iteration 91/1000 | Loss: 0.00001220
Iteration 92/1000 | Loss: 0.00001220
Iteration 93/1000 | Loss: 0.00001219
Iteration 94/1000 | Loss: 0.00001219
Iteration 95/1000 | Loss: 0.00001219
Iteration 96/1000 | Loss: 0.00001219
Iteration 97/1000 | Loss: 0.00001219
Iteration 98/1000 | Loss: 0.00001218
Iteration 99/1000 | Loss: 0.00001218
Iteration 100/1000 | Loss: 0.00001218
Iteration 101/1000 | Loss: 0.00001218
Iteration 102/1000 | Loss: 0.00001218
Iteration 103/1000 | Loss: 0.00001217
Iteration 104/1000 | Loss: 0.00001217
Iteration 105/1000 | Loss: 0.00001217
Iteration 106/1000 | Loss: 0.00001217
Iteration 107/1000 | Loss: 0.00001217
Iteration 108/1000 | Loss: 0.00001217
Iteration 109/1000 | Loss: 0.00001217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.2169299225206487e-05, 1.2169299225206487e-05, 1.2169299225206487e-05, 1.2169299225206487e-05, 1.2169299225206487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2169299225206487e-05

Optimization complete. Final v2v error: 2.9364399909973145 mm

Highest mean error: 3.5518174171447754 mm for frame 125

Lowest mean error: 2.549206256866455 mm for frame 83

Saving results

Total time: 33.50445342063904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00511356
Iteration 2/25 | Loss: 0.00107899
Iteration 3/25 | Loss: 0.00092818
Iteration 4/25 | Loss: 0.00091714
Iteration 5/25 | Loss: 0.00091521
Iteration 6/25 | Loss: 0.00091521
Iteration 7/25 | Loss: 0.00091521
Iteration 8/25 | Loss: 0.00091521
Iteration 9/25 | Loss: 0.00091521
Iteration 10/25 | Loss: 0.00091521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009152083657681942, 0.0009152083657681942, 0.0009152083657681942, 0.0009152083657681942, 0.0009152083657681942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009152083657681942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83764577
Iteration 2/25 | Loss: 0.00030548
Iteration 3/25 | Loss: 0.00030547
Iteration 4/25 | Loss: 0.00030547
Iteration 5/25 | Loss: 0.00030547
Iteration 6/25 | Loss: 0.00030547
Iteration 7/25 | Loss: 0.00030547
Iteration 8/25 | Loss: 0.00030547
Iteration 9/25 | Loss: 0.00030547
Iteration 10/25 | Loss: 0.00030547
Iteration 11/25 | Loss: 0.00030547
Iteration 12/25 | Loss: 0.00030547
Iteration 13/25 | Loss: 0.00030547
Iteration 14/25 | Loss: 0.00030547
Iteration 15/25 | Loss: 0.00030547
Iteration 16/25 | Loss: 0.00030547
Iteration 17/25 | Loss: 0.00030547
Iteration 18/25 | Loss: 0.00030547
Iteration 19/25 | Loss: 0.00030547
Iteration 20/25 | Loss: 0.00030547
Iteration 21/25 | Loss: 0.00030547
Iteration 22/25 | Loss: 0.00030547
Iteration 23/25 | Loss: 0.00030547
Iteration 24/25 | Loss: 0.00030547
Iteration 25/25 | Loss: 0.00030547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030547
Iteration 2/1000 | Loss: 0.00001856
Iteration 3/1000 | Loss: 0.00001347
Iteration 4/1000 | Loss: 0.00001252
Iteration 5/1000 | Loss: 0.00001214
Iteration 6/1000 | Loss: 0.00001189
Iteration 7/1000 | Loss: 0.00001188
Iteration 8/1000 | Loss: 0.00001161
Iteration 9/1000 | Loss: 0.00001140
Iteration 10/1000 | Loss: 0.00001138
Iteration 11/1000 | Loss: 0.00001129
Iteration 12/1000 | Loss: 0.00001129
Iteration 13/1000 | Loss: 0.00001126
Iteration 14/1000 | Loss: 0.00001126
Iteration 15/1000 | Loss: 0.00001123
Iteration 16/1000 | Loss: 0.00001120
Iteration 17/1000 | Loss: 0.00001119
Iteration 18/1000 | Loss: 0.00001119
Iteration 19/1000 | Loss: 0.00001119
Iteration 20/1000 | Loss: 0.00001118
Iteration 21/1000 | Loss: 0.00001118
Iteration 22/1000 | Loss: 0.00001118
Iteration 23/1000 | Loss: 0.00001118
Iteration 24/1000 | Loss: 0.00001117
Iteration 25/1000 | Loss: 0.00001115
Iteration 26/1000 | Loss: 0.00001115
Iteration 27/1000 | Loss: 0.00001114
Iteration 28/1000 | Loss: 0.00001114
Iteration 29/1000 | Loss: 0.00001114
Iteration 30/1000 | Loss: 0.00001114
Iteration 31/1000 | Loss: 0.00001114
Iteration 32/1000 | Loss: 0.00001114
Iteration 33/1000 | Loss: 0.00001114
Iteration 34/1000 | Loss: 0.00001113
Iteration 35/1000 | Loss: 0.00001113
Iteration 36/1000 | Loss: 0.00001113
Iteration 37/1000 | Loss: 0.00001112
Iteration 38/1000 | Loss: 0.00001112
Iteration 39/1000 | Loss: 0.00001112
Iteration 40/1000 | Loss: 0.00001112
Iteration 41/1000 | Loss: 0.00001111
Iteration 42/1000 | Loss: 0.00001111
Iteration 43/1000 | Loss: 0.00001111
Iteration 44/1000 | Loss: 0.00001111
Iteration 45/1000 | Loss: 0.00001111
Iteration 46/1000 | Loss: 0.00001110
Iteration 47/1000 | Loss: 0.00001110
Iteration 48/1000 | Loss: 0.00001109
Iteration 49/1000 | Loss: 0.00001109
Iteration 50/1000 | Loss: 0.00001107
Iteration 51/1000 | Loss: 0.00001107
Iteration 52/1000 | Loss: 0.00001106
Iteration 53/1000 | Loss: 0.00001106
Iteration 54/1000 | Loss: 0.00001106
Iteration 55/1000 | Loss: 0.00001105
Iteration 56/1000 | Loss: 0.00001105
Iteration 57/1000 | Loss: 0.00001105
Iteration 58/1000 | Loss: 0.00001105
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001104
Iteration 62/1000 | Loss: 0.00001104
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001104
Iteration 67/1000 | Loss: 0.00001104
Iteration 68/1000 | Loss: 0.00001104
Iteration 69/1000 | Loss: 0.00001104
Iteration 70/1000 | Loss: 0.00001104
Iteration 71/1000 | Loss: 0.00001104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.1041811376344413e-05, 1.1041811376344413e-05, 1.1041811376344413e-05, 1.1041811376344413e-05, 1.1041811376344413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1041811376344413e-05

Optimization complete. Final v2v error: 2.809871196746826 mm

Highest mean error: 2.846463441848755 mm for frame 1

Lowest mean error: 2.7749016284942627 mm for frame 245

Saving results

Total time: 29.566564321517944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077134
Iteration 2/25 | Loss: 0.00305510
Iteration 3/25 | Loss: 0.00132174
Iteration 4/25 | Loss: 0.00131862
Iteration 5/25 | Loss: 0.00133265
Iteration 6/25 | Loss: 0.00118558
Iteration 7/25 | Loss: 0.00105054
Iteration 8/25 | Loss: 0.00107859
Iteration 9/25 | Loss: 0.00098108
Iteration 10/25 | Loss: 0.00099673
Iteration 11/25 | Loss: 0.00094204
Iteration 12/25 | Loss: 0.00091581
Iteration 13/25 | Loss: 0.00089361
Iteration 14/25 | Loss: 0.00088891
Iteration 15/25 | Loss: 0.00089357
Iteration 16/25 | Loss: 0.00088697
Iteration 17/25 | Loss: 0.00087925
Iteration 18/25 | Loss: 0.00091536
Iteration 19/25 | Loss: 0.00088251
Iteration 20/25 | Loss: 0.00090156
Iteration 21/25 | Loss: 0.00084849
Iteration 22/25 | Loss: 0.00084631
Iteration 23/25 | Loss: 0.00084520
Iteration 24/25 | Loss: 0.00084468
Iteration 25/25 | Loss: 0.00085161

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51289821
Iteration 2/25 | Loss: 0.00068604
Iteration 3/25 | Loss: 0.00065371
Iteration 4/25 | Loss: 0.00065371
Iteration 5/25 | Loss: 0.00065371
Iteration 6/25 | Loss: 0.00065371
Iteration 7/25 | Loss: 0.00065371
Iteration 8/25 | Loss: 0.00065371
Iteration 9/25 | Loss: 0.00065371
Iteration 10/25 | Loss: 0.00065371
Iteration 11/25 | Loss: 0.00065370
Iteration 12/25 | Loss: 0.00065370
Iteration 13/25 | Loss: 0.00065370
Iteration 14/25 | Loss: 0.00065370
Iteration 15/25 | Loss: 0.00065370
Iteration 16/25 | Loss: 0.00065370
Iteration 17/25 | Loss: 0.00065370
Iteration 18/25 | Loss: 0.00065370
Iteration 19/25 | Loss: 0.00065370
Iteration 20/25 | Loss: 0.00065370
Iteration 21/25 | Loss: 0.00065370
Iteration 22/25 | Loss: 0.00065370
Iteration 23/25 | Loss: 0.00065370
Iteration 24/25 | Loss: 0.00065370
Iteration 25/25 | Loss: 0.00065370

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065370
Iteration 2/1000 | Loss: 0.00036468
Iteration 3/1000 | Loss: 0.00015386
Iteration 4/1000 | Loss: 0.00015355
Iteration 5/1000 | Loss: 0.00315571
Iteration 6/1000 | Loss: 0.00017485
Iteration 7/1000 | Loss: 0.00017368
Iteration 8/1000 | Loss: 0.00020745
Iteration 9/1000 | Loss: 0.00019403
Iteration 10/1000 | Loss: 0.00020436
Iteration 11/1000 | Loss: 0.00017927
Iteration 12/1000 | Loss: 0.00019464
Iteration 13/1000 | Loss: 0.00023244
Iteration 14/1000 | Loss: 0.00006692
Iteration 15/1000 | Loss: 0.00021705
Iteration 16/1000 | Loss: 0.00039031
Iteration 17/1000 | Loss: 0.00026477
Iteration 18/1000 | Loss: 0.00044122
Iteration 19/1000 | Loss: 0.00004230
Iteration 20/1000 | Loss: 0.00002844
Iteration 21/1000 | Loss: 0.00035195
Iteration 22/1000 | Loss: 0.00434932
Iteration 23/1000 | Loss: 0.00043406
Iteration 24/1000 | Loss: 0.00030539
Iteration 25/1000 | Loss: 0.00037222
Iteration 26/1000 | Loss: 0.00019049
Iteration 27/1000 | Loss: 0.00022671
Iteration 28/1000 | Loss: 0.00022639
Iteration 29/1000 | Loss: 0.00022469
Iteration 30/1000 | Loss: 0.00018719
Iteration 31/1000 | Loss: 0.00033225
Iteration 32/1000 | Loss: 0.00035919
Iteration 33/1000 | Loss: 0.00008365
Iteration 34/1000 | Loss: 0.00031307
Iteration 35/1000 | Loss: 0.00019266
Iteration 36/1000 | Loss: 0.00022185
Iteration 37/1000 | Loss: 0.00017761
Iteration 38/1000 | Loss: 0.00004067
Iteration 39/1000 | Loss: 0.00002159
Iteration 40/1000 | Loss: 0.00004235
Iteration 41/1000 | Loss: 0.00003617
Iteration 42/1000 | Loss: 0.00004078
Iteration 43/1000 | Loss: 0.00025937
Iteration 44/1000 | Loss: 0.00020410
Iteration 45/1000 | Loss: 0.00025641
Iteration 46/1000 | Loss: 0.00019891
Iteration 47/1000 | Loss: 0.00018069
Iteration 48/1000 | Loss: 0.00004775
Iteration 49/1000 | Loss: 0.00003527
Iteration 50/1000 | Loss: 0.00021818
Iteration 51/1000 | Loss: 0.00039216
Iteration 52/1000 | Loss: 0.00065950
Iteration 53/1000 | Loss: 0.00019075
Iteration 54/1000 | Loss: 0.00011557
Iteration 55/1000 | Loss: 0.00003346
Iteration 56/1000 | Loss: 0.00025769
Iteration 57/1000 | Loss: 0.00010117
Iteration 58/1000 | Loss: 0.00002709
Iteration 59/1000 | Loss: 0.00013378
Iteration 60/1000 | Loss: 0.00003860
Iteration 61/1000 | Loss: 0.00004193
Iteration 62/1000 | Loss: 0.00002566
Iteration 63/1000 | Loss: 0.00007179
Iteration 64/1000 | Loss: 0.00005126
Iteration 65/1000 | Loss: 0.00020193
Iteration 66/1000 | Loss: 0.00012633
Iteration 67/1000 | Loss: 0.00017281
Iteration 68/1000 | Loss: 0.00015002
Iteration 69/1000 | Loss: 0.00014136
Iteration 70/1000 | Loss: 0.00003239
Iteration 71/1000 | Loss: 0.00004771
Iteration 72/1000 | Loss: 0.00014221
Iteration 73/1000 | Loss: 0.00011749
Iteration 74/1000 | Loss: 0.00016337
Iteration 75/1000 | Loss: 0.00004094
Iteration 76/1000 | Loss: 0.00016338
Iteration 77/1000 | Loss: 0.00016052
Iteration 78/1000 | Loss: 0.00016943
Iteration 79/1000 | Loss: 0.00003830
Iteration 80/1000 | Loss: 0.00004690
Iteration 81/1000 | Loss: 0.00004418
Iteration 82/1000 | Loss: 0.00008646
Iteration 83/1000 | Loss: 0.00003664
Iteration 84/1000 | Loss: 0.00009048
Iteration 85/1000 | Loss: 0.00003680
Iteration 86/1000 | Loss: 0.00016501
Iteration 87/1000 | Loss: 0.00006423
Iteration 88/1000 | Loss: 0.00011697
Iteration 89/1000 | Loss: 0.00004861
Iteration 90/1000 | Loss: 0.00009204
Iteration 91/1000 | Loss: 0.00004559
Iteration 92/1000 | Loss: 0.00020382
Iteration 93/1000 | Loss: 0.00020223
Iteration 94/1000 | Loss: 0.00009710
Iteration 95/1000 | Loss: 0.00008507
Iteration 96/1000 | Loss: 0.00004113
Iteration 97/1000 | Loss: 0.00021624
Iteration 98/1000 | Loss: 0.00015315
Iteration 99/1000 | Loss: 0.00016076
Iteration 100/1000 | Loss: 0.00016457
Iteration 101/1000 | Loss: 0.00015736
Iteration 102/1000 | Loss: 0.00015244
Iteration 103/1000 | Loss: 0.00015048
Iteration 104/1000 | Loss: 0.00008563
Iteration 105/1000 | Loss: 0.00014642
Iteration 106/1000 | Loss: 0.00019449
Iteration 107/1000 | Loss: 0.00015889
Iteration 108/1000 | Loss: 0.00019013
Iteration 109/1000 | Loss: 0.00012700
Iteration 110/1000 | Loss: 0.00017685
Iteration 111/1000 | Loss: 0.00012188
Iteration 112/1000 | Loss: 0.00012729
Iteration 113/1000 | Loss: 0.00020431
Iteration 114/1000 | Loss: 0.00022453
Iteration 115/1000 | Loss: 0.00013150
Iteration 116/1000 | Loss: 0.00020484
Iteration 117/1000 | Loss: 0.00013566
Iteration 118/1000 | Loss: 0.00017127
Iteration 119/1000 | Loss: 0.00011646
Iteration 120/1000 | Loss: 0.00020151
Iteration 121/1000 | Loss: 0.00005258
Iteration 122/1000 | Loss: 0.00026049
Iteration 123/1000 | Loss: 0.00014696
Iteration 124/1000 | Loss: 0.00015070
Iteration 125/1000 | Loss: 0.00009299
Iteration 126/1000 | Loss: 0.00011891
Iteration 127/1000 | Loss: 0.00012539
Iteration 128/1000 | Loss: 0.00014327
Iteration 129/1000 | Loss: 0.00015627
Iteration 130/1000 | Loss: 0.00016594
Iteration 131/1000 | Loss: 0.00016260
Iteration 132/1000 | Loss: 0.00018392
Iteration 133/1000 | Loss: 0.00015809
Iteration 134/1000 | Loss: 0.00005821
Iteration 135/1000 | Loss: 0.00018965
Iteration 136/1000 | Loss: 0.00008024
Iteration 137/1000 | Loss: 0.00017824
Iteration 138/1000 | Loss: 0.00008656
Iteration 139/1000 | Loss: 0.00020119
Iteration 140/1000 | Loss: 0.00017362
Iteration 141/1000 | Loss: 0.00005612
Iteration 142/1000 | Loss: 0.00011564
Iteration 143/1000 | Loss: 0.00011791
Iteration 144/1000 | Loss: 0.00004279
Iteration 145/1000 | Loss: 0.00004241
Iteration 146/1000 | Loss: 0.00003430
Iteration 147/1000 | Loss: 0.00003613
Iteration 148/1000 | Loss: 0.00003532
Iteration 149/1000 | Loss: 0.00002710
Iteration 150/1000 | Loss: 0.00001732
Iteration 151/1000 | Loss: 0.00003331
Iteration 152/1000 | Loss: 0.00002995
Iteration 153/1000 | Loss: 0.00003400
Iteration 154/1000 | Loss: 0.00002753
Iteration 155/1000 | Loss: 0.00004052
Iteration 156/1000 | Loss: 0.00002754
Iteration 157/1000 | Loss: 0.00003250
Iteration 158/1000 | Loss: 0.00002658
Iteration 159/1000 | Loss: 0.00003724
Iteration 160/1000 | Loss: 0.00002762
Iteration 161/1000 | Loss: 0.00003176
Iteration 162/1000 | Loss: 0.00002631
Iteration 163/1000 | Loss: 0.00003151
Iteration 164/1000 | Loss: 0.00002534
Iteration 165/1000 | Loss: 0.00002596
Iteration 166/1000 | Loss: 0.00002565
Iteration 167/1000 | Loss: 0.00003321
Iteration 168/1000 | Loss: 0.00002658
Iteration 169/1000 | Loss: 0.00003044
Iteration 170/1000 | Loss: 0.00002759
Iteration 171/1000 | Loss: 0.00003469
Iteration 172/1000 | Loss: 0.00002614
Iteration 173/1000 | Loss: 0.00003162
Iteration 174/1000 | Loss: 0.00002567
Iteration 175/1000 | Loss: 0.00003121
Iteration 176/1000 | Loss: 0.00002603
Iteration 177/1000 | Loss: 0.00003007
Iteration 178/1000 | Loss: 0.00002594
Iteration 179/1000 | Loss: 0.00003379
Iteration 180/1000 | Loss: 0.00002599
Iteration 181/1000 | Loss: 0.00003075
Iteration 182/1000 | Loss: 0.00002764
Iteration 183/1000 | Loss: 0.00003071
Iteration 184/1000 | Loss: 0.00002870
Iteration 185/1000 | Loss: 0.00003055
Iteration 186/1000 | Loss: 0.00002689
Iteration 187/1000 | Loss: 0.00003029
Iteration 188/1000 | Loss: 0.00003022
Iteration 189/1000 | Loss: 0.00003214
Iteration 190/1000 | Loss: 0.00004190
Iteration 191/1000 | Loss: 0.00003082
Iteration 192/1000 | Loss: 0.00003942
Iteration 193/1000 | Loss: 0.00002772
Iteration 194/1000 | Loss: 0.00002796
Iteration 195/1000 | Loss: 0.00004135
Iteration 196/1000 | Loss: 0.00002931
Iteration 197/1000 | Loss: 0.00005505
Iteration 198/1000 | Loss: 0.00002792
Iteration 199/1000 | Loss: 0.00002839
Iteration 200/1000 | Loss: 0.00003076
Iteration 201/1000 | Loss: 0.00004099
Iteration 202/1000 | Loss: 0.00003181
Iteration 203/1000 | Loss: 0.00003048
Iteration 204/1000 | Loss: 0.00002900
Iteration 205/1000 | Loss: 0.00003009
Iteration 206/1000 | Loss: 0.00002915
Iteration 207/1000 | Loss: 0.00003328
Iteration 208/1000 | Loss: 0.00003161
Iteration 209/1000 | Loss: 0.00002793
Iteration 210/1000 | Loss: 0.00002914
Iteration 211/1000 | Loss: 0.00003010
Iteration 212/1000 | Loss: 0.00002851
Iteration 213/1000 | Loss: 0.00003540
Iteration 214/1000 | Loss: 0.00002935
Iteration 215/1000 | Loss: 0.00002863
Iteration 216/1000 | Loss: 0.00005343
Iteration 217/1000 | Loss: 0.00004431
Iteration 218/1000 | Loss: 0.00003123
Iteration 219/1000 | Loss: 0.00003870
Iteration 220/1000 | Loss: 0.00002666
Iteration 221/1000 | Loss: 0.00003157
Iteration 222/1000 | Loss: 0.00002970
Iteration 223/1000 | Loss: 0.00002865
Iteration 224/1000 | Loss: 0.00002907
Iteration 225/1000 | Loss: 0.00003657
Iteration 226/1000 | Loss: 0.00003281
Iteration 227/1000 | Loss: 0.00003546
Iteration 228/1000 | Loss: 0.00004707
Iteration 229/1000 | Loss: 0.00002699
Iteration 230/1000 | Loss: 0.00003319
Iteration 231/1000 | Loss: 0.00004078
Iteration 232/1000 | Loss: 0.00003921
Iteration 233/1000 | Loss: 0.00001540
Iteration 234/1000 | Loss: 0.00001334
Iteration 235/1000 | Loss: 0.00001091
Iteration 236/1000 | Loss: 0.00000976
Iteration 237/1000 | Loss: 0.00000902
Iteration 238/1000 | Loss: 0.00000876
Iteration 239/1000 | Loss: 0.00000872
Iteration 240/1000 | Loss: 0.00000868
Iteration 241/1000 | Loss: 0.00000867
Iteration 242/1000 | Loss: 0.00000867
Iteration 243/1000 | Loss: 0.00000866
Iteration 244/1000 | Loss: 0.00000866
Iteration 245/1000 | Loss: 0.00000859
Iteration 246/1000 | Loss: 0.00000859
Iteration 247/1000 | Loss: 0.00000858
Iteration 248/1000 | Loss: 0.00000858
Iteration 249/1000 | Loss: 0.00000857
Iteration 250/1000 | Loss: 0.00000856
Iteration 251/1000 | Loss: 0.00000855
Iteration 252/1000 | Loss: 0.00000854
Iteration 253/1000 | Loss: 0.00000853
Iteration 254/1000 | Loss: 0.00000853
Iteration 255/1000 | Loss: 0.00000853
Iteration 256/1000 | Loss: 0.00000853
Iteration 257/1000 | Loss: 0.00000852
Iteration 258/1000 | Loss: 0.00000852
Iteration 259/1000 | Loss: 0.00000852
Iteration 260/1000 | Loss: 0.00000850
Iteration 261/1000 | Loss: 0.00000850
Iteration 262/1000 | Loss: 0.00000850
Iteration 263/1000 | Loss: 0.00000850
Iteration 264/1000 | Loss: 0.00000849
Iteration 265/1000 | Loss: 0.00000849
Iteration 266/1000 | Loss: 0.00000849
Iteration 267/1000 | Loss: 0.00000849
Iteration 268/1000 | Loss: 0.00000849
Iteration 269/1000 | Loss: 0.00000848
Iteration 270/1000 | Loss: 0.00000848
Iteration 271/1000 | Loss: 0.00000848
Iteration 272/1000 | Loss: 0.00000848
Iteration 273/1000 | Loss: 0.00000847
Iteration 274/1000 | Loss: 0.00000847
Iteration 275/1000 | Loss: 0.00000846
Iteration 276/1000 | Loss: 0.00000846
Iteration 277/1000 | Loss: 0.00000845
Iteration 278/1000 | Loss: 0.00000845
Iteration 279/1000 | Loss: 0.00000845
Iteration 280/1000 | Loss: 0.00000844
Iteration 281/1000 | Loss: 0.00000844
Iteration 282/1000 | Loss: 0.00000843
Iteration 283/1000 | Loss: 0.00000843
Iteration 284/1000 | Loss: 0.00000843
Iteration 285/1000 | Loss: 0.00000843
Iteration 286/1000 | Loss: 0.00000842
Iteration 287/1000 | Loss: 0.00000842
Iteration 288/1000 | Loss: 0.00000842
Iteration 289/1000 | Loss: 0.00000841
Iteration 290/1000 | Loss: 0.00000841
Iteration 291/1000 | Loss: 0.00000841
Iteration 292/1000 | Loss: 0.00000840
Iteration 293/1000 | Loss: 0.00000840
Iteration 294/1000 | Loss: 0.00000840
Iteration 295/1000 | Loss: 0.00000840
Iteration 296/1000 | Loss: 0.00000840
Iteration 297/1000 | Loss: 0.00000839
Iteration 298/1000 | Loss: 0.00000839
Iteration 299/1000 | Loss: 0.00000839
Iteration 300/1000 | Loss: 0.00000839
Iteration 301/1000 | Loss: 0.00000839
Iteration 302/1000 | Loss: 0.00000839
Iteration 303/1000 | Loss: 0.00000839
Iteration 304/1000 | Loss: 0.00000838
Iteration 305/1000 | Loss: 0.00000838
Iteration 306/1000 | Loss: 0.00000838
Iteration 307/1000 | Loss: 0.00000837
Iteration 308/1000 | Loss: 0.00000837
Iteration 309/1000 | Loss: 0.00000837
Iteration 310/1000 | Loss: 0.00000837
Iteration 311/1000 | Loss: 0.00000837
Iteration 312/1000 | Loss: 0.00000837
Iteration 313/1000 | Loss: 0.00000837
Iteration 314/1000 | Loss: 0.00000837
Iteration 315/1000 | Loss: 0.00000837
Iteration 316/1000 | Loss: 0.00000837
Iteration 317/1000 | Loss: 0.00000836
Iteration 318/1000 | Loss: 0.00000836
Iteration 319/1000 | Loss: 0.00000836
Iteration 320/1000 | Loss: 0.00000836
Iteration 321/1000 | Loss: 0.00000836
Iteration 322/1000 | Loss: 0.00000836
Iteration 323/1000 | Loss: 0.00000836
Iteration 324/1000 | Loss: 0.00000836
Iteration 325/1000 | Loss: 0.00000836
Iteration 326/1000 | Loss: 0.00000836
Iteration 327/1000 | Loss: 0.00000836
Iteration 328/1000 | Loss: 0.00000836
Iteration 329/1000 | Loss: 0.00000836
Iteration 330/1000 | Loss: 0.00000836
Iteration 331/1000 | Loss: 0.00000836
Iteration 332/1000 | Loss: 0.00000836
Iteration 333/1000 | Loss: 0.00000836
Iteration 334/1000 | Loss: 0.00000836
Iteration 335/1000 | Loss: 0.00000836
Iteration 336/1000 | Loss: 0.00000836
Iteration 337/1000 | Loss: 0.00000836
Iteration 338/1000 | Loss: 0.00000836
Iteration 339/1000 | Loss: 0.00000836
Iteration 340/1000 | Loss: 0.00000836
Iteration 341/1000 | Loss: 0.00000836
Iteration 342/1000 | Loss: 0.00000836
Iteration 343/1000 | Loss: 0.00000836
Iteration 344/1000 | Loss: 0.00000836
Iteration 345/1000 | Loss: 0.00000836
Iteration 346/1000 | Loss: 0.00000836
Iteration 347/1000 | Loss: 0.00000836
Iteration 348/1000 | Loss: 0.00000836
Iteration 349/1000 | Loss: 0.00000836
Iteration 350/1000 | Loss: 0.00000836
Iteration 351/1000 | Loss: 0.00000836
Iteration 352/1000 | Loss: 0.00000836
Iteration 353/1000 | Loss: 0.00000836
Iteration 354/1000 | Loss: 0.00000836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 354. Stopping optimization.
Last 5 losses: [8.361625077668577e-06, 8.361625077668577e-06, 8.361625077668577e-06, 8.361625077668577e-06, 8.361625077668577e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.361625077668577e-06

Optimization complete. Final v2v error: 2.4127135276794434 mm

Highest mean error: 3.5912282466888428 mm for frame 58

Lowest mean error: 2.067960023880005 mm for frame 44

Saving results

Total time: 385.62239265441895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090469
Iteration 2/25 | Loss: 0.00337046
Iteration 3/25 | Loss: 0.00136361
Iteration 4/25 | Loss: 0.00112291
Iteration 5/25 | Loss: 0.00106091
Iteration 6/25 | Loss: 0.00103503
Iteration 7/25 | Loss: 0.00101699
Iteration 8/25 | Loss: 0.00094033
Iteration 9/25 | Loss: 0.00091422
Iteration 10/25 | Loss: 0.00090364
Iteration 11/25 | Loss: 0.00090181
Iteration 12/25 | Loss: 0.00089691
Iteration 13/25 | Loss: 0.00089729
Iteration 14/25 | Loss: 0.00088975
Iteration 15/25 | Loss: 0.00088451
Iteration 16/25 | Loss: 0.00088596
Iteration 17/25 | Loss: 0.00088364
Iteration 18/25 | Loss: 0.00088094
Iteration 19/25 | Loss: 0.00088117
Iteration 20/25 | Loss: 0.00087982
Iteration 21/25 | Loss: 0.00088006
Iteration 22/25 | Loss: 0.00087952
Iteration 23/25 | Loss: 0.00088431
Iteration 24/25 | Loss: 0.00088204
Iteration 25/25 | Loss: 0.00088216

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40363657
Iteration 2/25 | Loss: 0.00055958
Iteration 3/25 | Loss: 0.00055958
Iteration 4/25 | Loss: 0.00055958
Iteration 5/25 | Loss: 0.00055958
Iteration 6/25 | Loss: 0.00055958
Iteration 7/25 | Loss: 0.00055958
Iteration 8/25 | Loss: 0.00055958
Iteration 9/25 | Loss: 0.00055958
Iteration 10/25 | Loss: 0.00055958
Iteration 11/25 | Loss: 0.00055958
Iteration 12/25 | Loss: 0.00055958
Iteration 13/25 | Loss: 0.00055958
Iteration 14/25 | Loss: 0.00055958
Iteration 15/25 | Loss: 0.00055958
Iteration 16/25 | Loss: 0.00055958
Iteration 17/25 | Loss: 0.00055958
Iteration 18/25 | Loss: 0.00055958
Iteration 19/25 | Loss: 0.00055958
Iteration 20/25 | Loss: 0.00055958
Iteration 21/25 | Loss: 0.00055958
Iteration 22/25 | Loss: 0.00055958
Iteration 23/25 | Loss: 0.00055958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005595777183771133, 0.0005595777183771133, 0.0005595777183771133, 0.0005595777183771133, 0.0005595777183771133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005595777183771133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055958
Iteration 2/1000 | Loss: 0.00006448
Iteration 3/1000 | Loss: 0.00003456
Iteration 4/1000 | Loss: 0.00003527
Iteration 5/1000 | Loss: 0.00003712
Iteration 6/1000 | Loss: 0.00020651
Iteration 7/1000 | Loss: 0.00014630
Iteration 8/1000 | Loss: 0.00008915
Iteration 9/1000 | Loss: 0.00003422
Iteration 10/1000 | Loss: 0.00002919
Iteration 11/1000 | Loss: 0.00002748
Iteration 12/1000 | Loss: 0.00018114
Iteration 13/1000 | Loss: 0.00039581
Iteration 14/1000 | Loss: 0.00019366
Iteration 15/1000 | Loss: 0.00016521
Iteration 16/1000 | Loss: 0.00032715
Iteration 17/1000 | Loss: 0.00003736
Iteration 18/1000 | Loss: 0.00003178
Iteration 19/1000 | Loss: 0.00002934
Iteration 20/1000 | Loss: 0.00002787
Iteration 21/1000 | Loss: 0.00004067
Iteration 22/1000 | Loss: 0.00002839
Iteration 23/1000 | Loss: 0.00002678
Iteration 24/1000 | Loss: 0.00002603
Iteration 25/1000 | Loss: 0.00059739
Iteration 26/1000 | Loss: 0.00042448
Iteration 27/1000 | Loss: 0.00042461
Iteration 28/1000 | Loss: 0.00022295
Iteration 29/1000 | Loss: 0.00004852
Iteration 30/1000 | Loss: 0.00002430
Iteration 31/1000 | Loss: 0.00002304
Iteration 32/1000 | Loss: 0.00002212
Iteration 33/1000 | Loss: 0.00002153
Iteration 34/1000 | Loss: 0.00002111
Iteration 35/1000 | Loss: 0.00002078
Iteration 36/1000 | Loss: 0.00002174
Iteration 37/1000 | Loss: 0.00002022
Iteration 38/1000 | Loss: 0.00001977
Iteration 39/1000 | Loss: 0.00001946
Iteration 40/1000 | Loss: 0.00001939
Iteration 41/1000 | Loss: 0.00001930
Iteration 42/1000 | Loss: 0.00001912
Iteration 43/1000 | Loss: 0.00001912
Iteration 44/1000 | Loss: 0.00001907
Iteration 45/1000 | Loss: 0.00001906
Iteration 46/1000 | Loss: 0.00021234
Iteration 47/1000 | Loss: 0.00015173
Iteration 48/1000 | Loss: 0.00019499
Iteration 49/1000 | Loss: 0.00022442
Iteration 50/1000 | Loss: 0.00019717
Iteration 51/1000 | Loss: 0.00004435
Iteration 52/1000 | Loss: 0.00003130
Iteration 53/1000 | Loss: 0.00002596
Iteration 54/1000 | Loss: 0.00004352
Iteration 55/1000 | Loss: 0.00002500
Iteration 56/1000 | Loss: 0.00002185
Iteration 57/1000 | Loss: 0.00002067
Iteration 58/1000 | Loss: 0.00002018
Iteration 59/1000 | Loss: 0.00001955
Iteration 60/1000 | Loss: 0.00001894
Iteration 61/1000 | Loss: 0.00001870
Iteration 62/1000 | Loss: 0.00001860
Iteration 63/1000 | Loss: 0.00001852
Iteration 64/1000 | Loss: 0.00001845
Iteration 65/1000 | Loss: 0.00001841
Iteration 66/1000 | Loss: 0.00001834
Iteration 67/1000 | Loss: 0.00001828
Iteration 68/1000 | Loss: 0.00001828
Iteration 69/1000 | Loss: 0.00001826
Iteration 70/1000 | Loss: 0.00001825
Iteration 71/1000 | Loss: 0.00001824
Iteration 72/1000 | Loss: 0.00001824
Iteration 73/1000 | Loss: 0.00001823
Iteration 74/1000 | Loss: 0.00001823
Iteration 75/1000 | Loss: 0.00001822
Iteration 76/1000 | Loss: 0.00001822
Iteration 77/1000 | Loss: 0.00001821
Iteration 78/1000 | Loss: 0.00001821
Iteration 79/1000 | Loss: 0.00001821
Iteration 80/1000 | Loss: 0.00001820
Iteration 81/1000 | Loss: 0.00001820
Iteration 82/1000 | Loss: 0.00001819
Iteration 83/1000 | Loss: 0.00001819
Iteration 84/1000 | Loss: 0.00001819
Iteration 85/1000 | Loss: 0.00001818
Iteration 86/1000 | Loss: 0.00001818
Iteration 87/1000 | Loss: 0.00001818
Iteration 88/1000 | Loss: 0.00001817
Iteration 89/1000 | Loss: 0.00001817
Iteration 90/1000 | Loss: 0.00001817
Iteration 91/1000 | Loss: 0.00001816
Iteration 92/1000 | Loss: 0.00001816
Iteration 93/1000 | Loss: 0.00001816
Iteration 94/1000 | Loss: 0.00001816
Iteration 95/1000 | Loss: 0.00001815
Iteration 96/1000 | Loss: 0.00001815
Iteration 97/1000 | Loss: 0.00001815
Iteration 98/1000 | Loss: 0.00001815
Iteration 99/1000 | Loss: 0.00001815
Iteration 100/1000 | Loss: 0.00001815
Iteration 101/1000 | Loss: 0.00001815
Iteration 102/1000 | Loss: 0.00001815
Iteration 103/1000 | Loss: 0.00001814
Iteration 104/1000 | Loss: 0.00001814
Iteration 105/1000 | Loss: 0.00001814
Iteration 106/1000 | Loss: 0.00001813
Iteration 107/1000 | Loss: 0.00001813
Iteration 108/1000 | Loss: 0.00001813
Iteration 109/1000 | Loss: 0.00001813
Iteration 110/1000 | Loss: 0.00001813
Iteration 111/1000 | Loss: 0.00001813
Iteration 112/1000 | Loss: 0.00001813
Iteration 113/1000 | Loss: 0.00001812
Iteration 114/1000 | Loss: 0.00001812
Iteration 115/1000 | Loss: 0.00001812
Iteration 116/1000 | Loss: 0.00001812
Iteration 117/1000 | Loss: 0.00001812
Iteration 118/1000 | Loss: 0.00001812
Iteration 119/1000 | Loss: 0.00001812
Iteration 120/1000 | Loss: 0.00001812
Iteration 121/1000 | Loss: 0.00001812
Iteration 122/1000 | Loss: 0.00001812
Iteration 123/1000 | Loss: 0.00001812
Iteration 124/1000 | Loss: 0.00001811
Iteration 125/1000 | Loss: 0.00001811
Iteration 126/1000 | Loss: 0.00001811
Iteration 127/1000 | Loss: 0.00001811
Iteration 128/1000 | Loss: 0.00001811
Iteration 129/1000 | Loss: 0.00001811
Iteration 130/1000 | Loss: 0.00001810
Iteration 131/1000 | Loss: 0.00001810
Iteration 132/1000 | Loss: 0.00001810
Iteration 133/1000 | Loss: 0.00001810
Iteration 134/1000 | Loss: 0.00001810
Iteration 135/1000 | Loss: 0.00001810
Iteration 136/1000 | Loss: 0.00001810
Iteration 137/1000 | Loss: 0.00001810
Iteration 138/1000 | Loss: 0.00001810
Iteration 139/1000 | Loss: 0.00001810
Iteration 140/1000 | Loss: 0.00001810
Iteration 141/1000 | Loss: 0.00001810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.810062167351134e-05, 1.810062167351134e-05, 1.810062167351134e-05, 1.810062167351134e-05, 1.810062167351134e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.810062167351134e-05

Optimization complete. Final v2v error: 3.0783071517944336 mm

Highest mean error: 20.06810760498047 mm for frame 126

Lowest mean error: 2.736260175704956 mm for frame 48

Saving results

Total time: 138.53106236457825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00563127
Iteration 2/25 | Loss: 0.00105421
Iteration 3/25 | Loss: 0.00092233
Iteration 4/25 | Loss: 0.00089341
Iteration 5/25 | Loss: 0.00088634
Iteration 6/25 | Loss: 0.00088384
Iteration 7/25 | Loss: 0.00088288
Iteration 8/25 | Loss: 0.00088281
Iteration 9/25 | Loss: 0.00088281
Iteration 10/25 | Loss: 0.00088281
Iteration 11/25 | Loss: 0.00088281
Iteration 12/25 | Loss: 0.00088281
Iteration 13/25 | Loss: 0.00088281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000882807478774339, 0.000882807478774339, 0.000882807478774339, 0.000882807478774339, 0.000882807478774339]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000882807478774339

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01849926
Iteration 2/25 | Loss: 0.00021814
Iteration 3/25 | Loss: 0.00021814
Iteration 4/25 | Loss: 0.00021814
Iteration 5/25 | Loss: 0.00021814
Iteration 6/25 | Loss: 0.00021814
Iteration 7/25 | Loss: 0.00021814
Iteration 8/25 | Loss: 0.00021814
Iteration 9/25 | Loss: 0.00021814
Iteration 10/25 | Loss: 0.00021814
Iteration 11/25 | Loss: 0.00021814
Iteration 12/25 | Loss: 0.00021814
Iteration 13/25 | Loss: 0.00021814
Iteration 14/25 | Loss: 0.00021814
Iteration 15/25 | Loss: 0.00021814
Iteration 16/25 | Loss: 0.00021814
Iteration 17/25 | Loss: 0.00021814
Iteration 18/25 | Loss: 0.00021814
Iteration 19/25 | Loss: 0.00021814
Iteration 20/25 | Loss: 0.00021814
Iteration 21/25 | Loss: 0.00021814
Iteration 22/25 | Loss: 0.00021814
Iteration 23/25 | Loss: 0.00021814
Iteration 24/25 | Loss: 0.00021814
Iteration 25/25 | Loss: 0.00021814

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021814
Iteration 2/1000 | Loss: 0.00003801
Iteration 3/1000 | Loss: 0.00003226
Iteration 4/1000 | Loss: 0.00002973
Iteration 5/1000 | Loss: 0.00002790
Iteration 6/1000 | Loss: 0.00002669
Iteration 7/1000 | Loss: 0.00002597
Iteration 8/1000 | Loss: 0.00002537
Iteration 9/1000 | Loss: 0.00002511
Iteration 10/1000 | Loss: 0.00002483
Iteration 11/1000 | Loss: 0.00002459
Iteration 12/1000 | Loss: 0.00002446
Iteration 13/1000 | Loss: 0.00002437
Iteration 14/1000 | Loss: 0.00002429
Iteration 15/1000 | Loss: 0.00002428
Iteration 16/1000 | Loss: 0.00002428
Iteration 17/1000 | Loss: 0.00002427
Iteration 18/1000 | Loss: 0.00002427
Iteration 19/1000 | Loss: 0.00002427
Iteration 20/1000 | Loss: 0.00002427
Iteration 21/1000 | Loss: 0.00002427
Iteration 22/1000 | Loss: 0.00002427
Iteration 23/1000 | Loss: 0.00002427
Iteration 24/1000 | Loss: 0.00002427
Iteration 25/1000 | Loss: 0.00002427
Iteration 26/1000 | Loss: 0.00002427
Iteration 27/1000 | Loss: 0.00002427
Iteration 28/1000 | Loss: 0.00002427
Iteration 29/1000 | Loss: 0.00002426
Iteration 30/1000 | Loss: 0.00002426
Iteration 31/1000 | Loss: 0.00002426
Iteration 32/1000 | Loss: 0.00002426
Iteration 33/1000 | Loss: 0.00002426
Iteration 34/1000 | Loss: 0.00002426
Iteration 35/1000 | Loss: 0.00002426
Iteration 36/1000 | Loss: 0.00002426
Iteration 37/1000 | Loss: 0.00002426
Iteration 38/1000 | Loss: 0.00002426
Iteration 39/1000 | Loss: 0.00002425
Iteration 40/1000 | Loss: 0.00002425
Iteration 41/1000 | Loss: 0.00002425
Iteration 42/1000 | Loss: 0.00002425
Iteration 43/1000 | Loss: 0.00002424
Iteration 44/1000 | Loss: 0.00002424
Iteration 45/1000 | Loss: 0.00002424
Iteration 46/1000 | Loss: 0.00002424
Iteration 47/1000 | Loss: 0.00002424
Iteration 48/1000 | Loss: 0.00002423
Iteration 49/1000 | Loss: 0.00002423
Iteration 50/1000 | Loss: 0.00002423
Iteration 51/1000 | Loss: 0.00002423
Iteration 52/1000 | Loss: 0.00002423
Iteration 53/1000 | Loss: 0.00002423
Iteration 54/1000 | Loss: 0.00002423
Iteration 55/1000 | Loss: 0.00002423
Iteration 56/1000 | Loss: 0.00002423
Iteration 57/1000 | Loss: 0.00002423
Iteration 58/1000 | Loss: 0.00002422
Iteration 59/1000 | Loss: 0.00002422
Iteration 60/1000 | Loss: 0.00002422
Iteration 61/1000 | Loss: 0.00002422
Iteration 62/1000 | Loss: 0.00002422
Iteration 63/1000 | Loss: 0.00002422
Iteration 64/1000 | Loss: 0.00002422
Iteration 65/1000 | Loss: 0.00002421
Iteration 66/1000 | Loss: 0.00002421
Iteration 67/1000 | Loss: 0.00002421
Iteration 68/1000 | Loss: 0.00002421
Iteration 69/1000 | Loss: 0.00002421
Iteration 70/1000 | Loss: 0.00002421
Iteration 71/1000 | Loss: 0.00002421
Iteration 72/1000 | Loss: 0.00002421
Iteration 73/1000 | Loss: 0.00002421
Iteration 74/1000 | Loss: 0.00002421
Iteration 75/1000 | Loss: 0.00002420
Iteration 76/1000 | Loss: 0.00002420
Iteration 77/1000 | Loss: 0.00002420
Iteration 78/1000 | Loss: 0.00002420
Iteration 79/1000 | Loss: 0.00002420
Iteration 80/1000 | Loss: 0.00002420
Iteration 81/1000 | Loss: 0.00002419
Iteration 82/1000 | Loss: 0.00002418
Iteration 83/1000 | Loss: 0.00002418
Iteration 84/1000 | Loss: 0.00002418
Iteration 85/1000 | Loss: 0.00002417
Iteration 86/1000 | Loss: 0.00002417
Iteration 87/1000 | Loss: 0.00002417
Iteration 88/1000 | Loss: 0.00002417
Iteration 89/1000 | Loss: 0.00002417
Iteration 90/1000 | Loss: 0.00002417
Iteration 91/1000 | Loss: 0.00002416
Iteration 92/1000 | Loss: 0.00002416
Iteration 93/1000 | Loss: 0.00002416
Iteration 94/1000 | Loss: 0.00002415
Iteration 95/1000 | Loss: 0.00002415
Iteration 96/1000 | Loss: 0.00002415
Iteration 97/1000 | Loss: 0.00002415
Iteration 98/1000 | Loss: 0.00002415
Iteration 99/1000 | Loss: 0.00002415
Iteration 100/1000 | Loss: 0.00002415
Iteration 101/1000 | Loss: 0.00002415
Iteration 102/1000 | Loss: 0.00002415
Iteration 103/1000 | Loss: 0.00002414
Iteration 104/1000 | Loss: 0.00002414
Iteration 105/1000 | Loss: 0.00002412
Iteration 106/1000 | Loss: 0.00002412
Iteration 107/1000 | Loss: 0.00002412
Iteration 108/1000 | Loss: 0.00002411
Iteration 109/1000 | Loss: 0.00002411
Iteration 110/1000 | Loss: 0.00002411
Iteration 111/1000 | Loss: 0.00002411
Iteration 112/1000 | Loss: 0.00002411
Iteration 113/1000 | Loss: 0.00002411
Iteration 114/1000 | Loss: 0.00002411
Iteration 115/1000 | Loss: 0.00002411
Iteration 116/1000 | Loss: 0.00002409
Iteration 117/1000 | Loss: 0.00002408
Iteration 118/1000 | Loss: 0.00002407
Iteration 119/1000 | Loss: 0.00002407
Iteration 120/1000 | Loss: 0.00002407
Iteration 121/1000 | Loss: 0.00002407
Iteration 122/1000 | Loss: 0.00002407
Iteration 123/1000 | Loss: 0.00002407
Iteration 124/1000 | Loss: 0.00002407
Iteration 125/1000 | Loss: 0.00002407
Iteration 126/1000 | Loss: 0.00002405
Iteration 127/1000 | Loss: 0.00002405
Iteration 128/1000 | Loss: 0.00002404
Iteration 129/1000 | Loss: 0.00002404
Iteration 130/1000 | Loss: 0.00002404
Iteration 131/1000 | Loss: 0.00002404
Iteration 132/1000 | Loss: 0.00002404
Iteration 133/1000 | Loss: 0.00002404
Iteration 134/1000 | Loss: 0.00002404
Iteration 135/1000 | Loss: 0.00002404
Iteration 136/1000 | Loss: 0.00002403
Iteration 137/1000 | Loss: 0.00002402
Iteration 138/1000 | Loss: 0.00002402
Iteration 139/1000 | Loss: 0.00002402
Iteration 140/1000 | Loss: 0.00002402
Iteration 141/1000 | Loss: 0.00002402
Iteration 142/1000 | Loss: 0.00002401
Iteration 143/1000 | Loss: 0.00002401
Iteration 144/1000 | Loss: 0.00002401
Iteration 145/1000 | Loss: 0.00002401
Iteration 146/1000 | Loss: 0.00002401
Iteration 147/1000 | Loss: 0.00002401
Iteration 148/1000 | Loss: 0.00002401
Iteration 149/1000 | Loss: 0.00002401
Iteration 150/1000 | Loss: 0.00002401
Iteration 151/1000 | Loss: 0.00002400
Iteration 152/1000 | Loss: 0.00002400
Iteration 153/1000 | Loss: 0.00002400
Iteration 154/1000 | Loss: 0.00002400
Iteration 155/1000 | Loss: 0.00002400
Iteration 156/1000 | Loss: 0.00002400
Iteration 157/1000 | Loss: 0.00002400
Iteration 158/1000 | Loss: 0.00002400
Iteration 159/1000 | Loss: 0.00002400
Iteration 160/1000 | Loss: 0.00002400
Iteration 161/1000 | Loss: 0.00002400
Iteration 162/1000 | Loss: 0.00002400
Iteration 163/1000 | Loss: 0.00002400
Iteration 164/1000 | Loss: 0.00002400
Iteration 165/1000 | Loss: 0.00002400
Iteration 166/1000 | Loss: 0.00002400
Iteration 167/1000 | Loss: 0.00002400
Iteration 168/1000 | Loss: 0.00002400
Iteration 169/1000 | Loss: 0.00002400
Iteration 170/1000 | Loss: 0.00002400
Iteration 171/1000 | Loss: 0.00002400
Iteration 172/1000 | Loss: 0.00002400
Iteration 173/1000 | Loss: 0.00002400
Iteration 174/1000 | Loss: 0.00002400
Iteration 175/1000 | Loss: 0.00002400
Iteration 176/1000 | Loss: 0.00002400
Iteration 177/1000 | Loss: 0.00002400
Iteration 178/1000 | Loss: 0.00002400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.4001319616218098e-05, 2.4001319616218098e-05, 2.4001319616218098e-05, 2.4001319616218098e-05, 2.4001319616218098e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4001319616218098e-05

Optimization complete. Final v2v error: 4.1683454513549805 mm

Highest mean error: 4.30850887298584 mm for frame 15

Lowest mean error: 3.97805118560791 mm for frame 142

Saving results

Total time: 39.33109164237976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921966
Iteration 2/25 | Loss: 0.00093623
Iteration 3/25 | Loss: 0.00085027
Iteration 4/25 | Loss: 0.00083509
Iteration 5/25 | Loss: 0.00083016
Iteration 6/25 | Loss: 0.00082975
Iteration 7/25 | Loss: 0.00082975
Iteration 8/25 | Loss: 0.00082975
Iteration 9/25 | Loss: 0.00082975
Iteration 10/25 | Loss: 0.00082975
Iteration 11/25 | Loss: 0.00082975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000829747470561415, 0.000829747470561415, 0.000829747470561415, 0.000829747470561415, 0.000829747470561415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000829747470561415

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46904182
Iteration 2/25 | Loss: 0.00042794
Iteration 3/25 | Loss: 0.00042794
Iteration 4/25 | Loss: 0.00042794
Iteration 5/25 | Loss: 0.00042794
Iteration 6/25 | Loss: 0.00042794
Iteration 7/25 | Loss: 0.00042794
Iteration 8/25 | Loss: 0.00042794
Iteration 9/25 | Loss: 0.00042794
Iteration 10/25 | Loss: 0.00042794
Iteration 11/25 | Loss: 0.00042794
Iteration 12/25 | Loss: 0.00042794
Iteration 13/25 | Loss: 0.00042794
Iteration 14/25 | Loss: 0.00042794
Iteration 15/25 | Loss: 0.00042794
Iteration 16/25 | Loss: 0.00042794
Iteration 17/25 | Loss: 0.00042794
Iteration 18/25 | Loss: 0.00042794
Iteration 19/25 | Loss: 0.00042794
Iteration 20/25 | Loss: 0.00042794
Iteration 21/25 | Loss: 0.00042794
Iteration 22/25 | Loss: 0.00042794
Iteration 23/25 | Loss: 0.00042794
Iteration 24/25 | Loss: 0.00042794
Iteration 25/25 | Loss: 0.00042794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042794
Iteration 2/1000 | Loss: 0.00001355
Iteration 3/1000 | Loss: 0.00001099
Iteration 4/1000 | Loss: 0.00001056
Iteration 5/1000 | Loss: 0.00001019
Iteration 6/1000 | Loss: 0.00000984
Iteration 7/1000 | Loss: 0.00000960
Iteration 8/1000 | Loss: 0.00000959
Iteration 9/1000 | Loss: 0.00000946
Iteration 10/1000 | Loss: 0.00000930
Iteration 11/1000 | Loss: 0.00000929
Iteration 12/1000 | Loss: 0.00000929
Iteration 13/1000 | Loss: 0.00000928
Iteration 14/1000 | Loss: 0.00000927
Iteration 15/1000 | Loss: 0.00000927
Iteration 16/1000 | Loss: 0.00000927
Iteration 17/1000 | Loss: 0.00000927
Iteration 18/1000 | Loss: 0.00000925
Iteration 19/1000 | Loss: 0.00000925
Iteration 20/1000 | Loss: 0.00000924
Iteration 21/1000 | Loss: 0.00000924
Iteration 22/1000 | Loss: 0.00000921
Iteration 23/1000 | Loss: 0.00000921
Iteration 24/1000 | Loss: 0.00000921
Iteration 25/1000 | Loss: 0.00000921
Iteration 26/1000 | Loss: 0.00000921
Iteration 27/1000 | Loss: 0.00000921
Iteration 28/1000 | Loss: 0.00000921
Iteration 29/1000 | Loss: 0.00000921
Iteration 30/1000 | Loss: 0.00000920
Iteration 31/1000 | Loss: 0.00000920
Iteration 32/1000 | Loss: 0.00000920
Iteration 33/1000 | Loss: 0.00000920
Iteration 34/1000 | Loss: 0.00000920
Iteration 35/1000 | Loss: 0.00000920
Iteration 36/1000 | Loss: 0.00000920
Iteration 37/1000 | Loss: 0.00000919
Iteration 38/1000 | Loss: 0.00000919
Iteration 39/1000 | Loss: 0.00000918
Iteration 40/1000 | Loss: 0.00000917
Iteration 41/1000 | Loss: 0.00000916
Iteration 42/1000 | Loss: 0.00000916
Iteration 43/1000 | Loss: 0.00000915
Iteration 44/1000 | Loss: 0.00000915
Iteration 45/1000 | Loss: 0.00000915
Iteration 46/1000 | Loss: 0.00000914
Iteration 47/1000 | Loss: 0.00000914
Iteration 48/1000 | Loss: 0.00000914
Iteration 49/1000 | Loss: 0.00000913
Iteration 50/1000 | Loss: 0.00000913
Iteration 51/1000 | Loss: 0.00000913
Iteration 52/1000 | Loss: 0.00000913
Iteration 53/1000 | Loss: 0.00000913
Iteration 54/1000 | Loss: 0.00000913
Iteration 55/1000 | Loss: 0.00000913
Iteration 56/1000 | Loss: 0.00000913
Iteration 57/1000 | Loss: 0.00000912
Iteration 58/1000 | Loss: 0.00000912
Iteration 59/1000 | Loss: 0.00000912
Iteration 60/1000 | Loss: 0.00000911
Iteration 61/1000 | Loss: 0.00000911
Iteration 62/1000 | Loss: 0.00000911
Iteration 63/1000 | Loss: 0.00000910
Iteration 64/1000 | Loss: 0.00000910
Iteration 65/1000 | Loss: 0.00000910
Iteration 66/1000 | Loss: 0.00000910
Iteration 67/1000 | Loss: 0.00000910
Iteration 68/1000 | Loss: 0.00000910
Iteration 69/1000 | Loss: 0.00000910
Iteration 70/1000 | Loss: 0.00000909
Iteration 71/1000 | Loss: 0.00000909
Iteration 72/1000 | Loss: 0.00000909
Iteration 73/1000 | Loss: 0.00000909
Iteration 74/1000 | Loss: 0.00000909
Iteration 75/1000 | Loss: 0.00000909
Iteration 76/1000 | Loss: 0.00000908
Iteration 77/1000 | Loss: 0.00000908
Iteration 78/1000 | Loss: 0.00000907
Iteration 79/1000 | Loss: 0.00000907
Iteration 80/1000 | Loss: 0.00000907
Iteration 81/1000 | Loss: 0.00000906
Iteration 82/1000 | Loss: 0.00000906
Iteration 83/1000 | Loss: 0.00000906
Iteration 84/1000 | Loss: 0.00000906
Iteration 85/1000 | Loss: 0.00000906
Iteration 86/1000 | Loss: 0.00000905
Iteration 87/1000 | Loss: 0.00000905
Iteration 88/1000 | Loss: 0.00000905
Iteration 89/1000 | Loss: 0.00000905
Iteration 90/1000 | Loss: 0.00000905
Iteration 91/1000 | Loss: 0.00000904
Iteration 92/1000 | Loss: 0.00000904
Iteration 93/1000 | Loss: 0.00000903
Iteration 94/1000 | Loss: 0.00000903
Iteration 95/1000 | Loss: 0.00000903
Iteration 96/1000 | Loss: 0.00000902
Iteration 97/1000 | Loss: 0.00000902
Iteration 98/1000 | Loss: 0.00000902
Iteration 99/1000 | Loss: 0.00000902
Iteration 100/1000 | Loss: 0.00000902
Iteration 101/1000 | Loss: 0.00000902
Iteration 102/1000 | Loss: 0.00000901
Iteration 103/1000 | Loss: 0.00000901
Iteration 104/1000 | Loss: 0.00000900
Iteration 105/1000 | Loss: 0.00000900
Iteration 106/1000 | Loss: 0.00000900
Iteration 107/1000 | Loss: 0.00000899
Iteration 108/1000 | Loss: 0.00000899
Iteration 109/1000 | Loss: 0.00000899
Iteration 110/1000 | Loss: 0.00000899
Iteration 111/1000 | Loss: 0.00000899
Iteration 112/1000 | Loss: 0.00000899
Iteration 113/1000 | Loss: 0.00000898
Iteration 114/1000 | Loss: 0.00000898
Iteration 115/1000 | Loss: 0.00000898
Iteration 116/1000 | Loss: 0.00000898
Iteration 117/1000 | Loss: 0.00000897
Iteration 118/1000 | Loss: 0.00000897
Iteration 119/1000 | Loss: 0.00000897
Iteration 120/1000 | Loss: 0.00000897
Iteration 121/1000 | Loss: 0.00000897
Iteration 122/1000 | Loss: 0.00000896
Iteration 123/1000 | Loss: 0.00000896
Iteration 124/1000 | Loss: 0.00000896
Iteration 125/1000 | Loss: 0.00000896
Iteration 126/1000 | Loss: 0.00000895
Iteration 127/1000 | Loss: 0.00000895
Iteration 128/1000 | Loss: 0.00000895
Iteration 129/1000 | Loss: 0.00000895
Iteration 130/1000 | Loss: 0.00000895
Iteration 131/1000 | Loss: 0.00000895
Iteration 132/1000 | Loss: 0.00000895
Iteration 133/1000 | Loss: 0.00000895
Iteration 134/1000 | Loss: 0.00000895
Iteration 135/1000 | Loss: 0.00000894
Iteration 136/1000 | Loss: 0.00000894
Iteration 137/1000 | Loss: 0.00000894
Iteration 138/1000 | Loss: 0.00000894
Iteration 139/1000 | Loss: 0.00000894
Iteration 140/1000 | Loss: 0.00000894
Iteration 141/1000 | Loss: 0.00000894
Iteration 142/1000 | Loss: 0.00000894
Iteration 143/1000 | Loss: 0.00000894
Iteration 144/1000 | Loss: 0.00000894
Iteration 145/1000 | Loss: 0.00000894
Iteration 146/1000 | Loss: 0.00000894
Iteration 147/1000 | Loss: 0.00000894
Iteration 148/1000 | Loss: 0.00000894
Iteration 149/1000 | Loss: 0.00000894
Iteration 150/1000 | Loss: 0.00000894
Iteration 151/1000 | Loss: 0.00000894
Iteration 152/1000 | Loss: 0.00000894
Iteration 153/1000 | Loss: 0.00000893
Iteration 154/1000 | Loss: 0.00000893
Iteration 155/1000 | Loss: 0.00000893
Iteration 156/1000 | Loss: 0.00000893
Iteration 157/1000 | Loss: 0.00000893
Iteration 158/1000 | Loss: 0.00000893
Iteration 159/1000 | Loss: 0.00000893
Iteration 160/1000 | Loss: 0.00000893
Iteration 161/1000 | Loss: 0.00000893
Iteration 162/1000 | Loss: 0.00000893
Iteration 163/1000 | Loss: 0.00000893
Iteration 164/1000 | Loss: 0.00000892
Iteration 165/1000 | Loss: 0.00000892
Iteration 166/1000 | Loss: 0.00000892
Iteration 167/1000 | Loss: 0.00000892
Iteration 168/1000 | Loss: 0.00000892
Iteration 169/1000 | Loss: 0.00000892
Iteration 170/1000 | Loss: 0.00000892
Iteration 171/1000 | Loss: 0.00000892
Iteration 172/1000 | Loss: 0.00000891
Iteration 173/1000 | Loss: 0.00000891
Iteration 174/1000 | Loss: 0.00000891
Iteration 175/1000 | Loss: 0.00000891
Iteration 176/1000 | Loss: 0.00000891
Iteration 177/1000 | Loss: 0.00000891
Iteration 178/1000 | Loss: 0.00000891
Iteration 179/1000 | Loss: 0.00000891
Iteration 180/1000 | Loss: 0.00000891
Iteration 181/1000 | Loss: 0.00000891
Iteration 182/1000 | Loss: 0.00000891
Iteration 183/1000 | Loss: 0.00000891
Iteration 184/1000 | Loss: 0.00000891
Iteration 185/1000 | Loss: 0.00000891
Iteration 186/1000 | Loss: 0.00000890
Iteration 187/1000 | Loss: 0.00000890
Iteration 188/1000 | Loss: 0.00000890
Iteration 189/1000 | Loss: 0.00000890
Iteration 190/1000 | Loss: 0.00000890
Iteration 191/1000 | Loss: 0.00000890
Iteration 192/1000 | Loss: 0.00000890
Iteration 193/1000 | Loss: 0.00000890
Iteration 194/1000 | Loss: 0.00000890
Iteration 195/1000 | Loss: 0.00000890
Iteration 196/1000 | Loss: 0.00000890
Iteration 197/1000 | Loss: 0.00000890
Iteration 198/1000 | Loss: 0.00000890
Iteration 199/1000 | Loss: 0.00000890
Iteration 200/1000 | Loss: 0.00000890
Iteration 201/1000 | Loss: 0.00000890
Iteration 202/1000 | Loss: 0.00000890
Iteration 203/1000 | Loss: 0.00000890
Iteration 204/1000 | Loss: 0.00000890
Iteration 205/1000 | Loss: 0.00000890
Iteration 206/1000 | Loss: 0.00000890
Iteration 207/1000 | Loss: 0.00000890
Iteration 208/1000 | Loss: 0.00000890
Iteration 209/1000 | Loss: 0.00000890
Iteration 210/1000 | Loss: 0.00000890
Iteration 211/1000 | Loss: 0.00000890
Iteration 212/1000 | Loss: 0.00000890
Iteration 213/1000 | Loss: 0.00000890
Iteration 214/1000 | Loss: 0.00000890
Iteration 215/1000 | Loss: 0.00000890
Iteration 216/1000 | Loss: 0.00000890
Iteration 217/1000 | Loss: 0.00000890
Iteration 218/1000 | Loss: 0.00000890
Iteration 219/1000 | Loss: 0.00000890
Iteration 220/1000 | Loss: 0.00000890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [8.902332410798408e-06, 8.902332410798408e-06, 8.902332410798408e-06, 8.902332410798408e-06, 8.902332410798408e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.902332410798408e-06

Optimization complete. Final v2v error: 2.541311025619507 mm

Highest mean error: 3.0404069423675537 mm for frame 142

Lowest mean error: 2.455756425857544 mm for frame 200

Saving results

Total time: 38.1425666809082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00927747
Iteration 2/25 | Loss: 0.00168044
Iteration 3/25 | Loss: 0.00125528
Iteration 4/25 | Loss: 0.00117331
Iteration 5/25 | Loss: 0.00115536
Iteration 6/25 | Loss: 0.00107329
Iteration 7/25 | Loss: 0.00104125
Iteration 8/25 | Loss: 0.00102095
Iteration 9/25 | Loss: 0.00100864
Iteration 10/25 | Loss: 0.00100478
Iteration 11/25 | Loss: 0.00100366
Iteration 12/25 | Loss: 0.00102923
Iteration 13/25 | Loss: 0.00101582
Iteration 14/25 | Loss: 0.00099235
Iteration 15/25 | Loss: 0.00098163
Iteration 16/25 | Loss: 0.00097921
Iteration 17/25 | Loss: 0.00097847
Iteration 18/25 | Loss: 0.00097824
Iteration 19/25 | Loss: 0.00097816
Iteration 20/25 | Loss: 0.00097816
Iteration 21/25 | Loss: 0.00097816
Iteration 22/25 | Loss: 0.00097816
Iteration 23/25 | Loss: 0.00097816
Iteration 24/25 | Loss: 0.00097816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000978162162937224, 0.000978162162937224, 0.000978162162937224, 0.000978162162937224, 0.000978162162937224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000978162162937224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.32877827
Iteration 2/25 | Loss: 0.00053682
Iteration 3/25 | Loss: 0.00053680
Iteration 4/25 | Loss: 0.00053680
Iteration 5/25 | Loss: 0.00053680
Iteration 6/25 | Loss: 0.00053680
Iteration 7/25 | Loss: 0.00053680
Iteration 8/25 | Loss: 0.00053680
Iteration 9/25 | Loss: 0.00053680
Iteration 10/25 | Loss: 0.00053680
Iteration 11/25 | Loss: 0.00053680
Iteration 12/25 | Loss: 0.00053680
Iteration 13/25 | Loss: 0.00053680
Iteration 14/25 | Loss: 0.00053680
Iteration 15/25 | Loss: 0.00053680
Iteration 16/25 | Loss: 0.00053680
Iteration 17/25 | Loss: 0.00053680
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005368004203774035, 0.0005368004203774035, 0.0005368004203774035, 0.0005368004203774035, 0.0005368004203774035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005368004203774035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053680
Iteration 2/1000 | Loss: 0.00022533
Iteration 3/1000 | Loss: 0.00027931
Iteration 4/1000 | Loss: 0.00025254
Iteration 5/1000 | Loss: 0.00020719
Iteration 6/1000 | Loss: 0.00012468
Iteration 7/1000 | Loss: 0.00009659
Iteration 8/1000 | Loss: 0.00008146
Iteration 9/1000 | Loss: 0.00010324
Iteration 10/1000 | Loss: 0.00009631
Iteration 11/1000 | Loss: 0.00021578
Iteration 12/1000 | Loss: 0.00011193
Iteration 13/1000 | Loss: 0.00016162
Iteration 14/1000 | Loss: 0.00009356
Iteration 15/1000 | Loss: 0.00015884
Iteration 16/1000 | Loss: 0.00007939
Iteration 17/1000 | Loss: 0.00011748
Iteration 18/1000 | Loss: 0.00006942
Iteration 19/1000 | Loss: 0.00006406
Iteration 20/1000 | Loss: 0.00008965
Iteration 21/1000 | Loss: 0.00006977
Iteration 22/1000 | Loss: 0.00005729
Iteration 23/1000 | Loss: 0.00005141
Iteration 24/1000 | Loss: 0.00004547
Iteration 25/1000 | Loss: 0.00004131
Iteration 26/1000 | Loss: 0.00003791
Iteration 27/1000 | Loss: 0.00003562
Iteration 28/1000 | Loss: 0.00003780
Iteration 29/1000 | Loss: 0.00003353
Iteration 30/1000 | Loss: 0.00003209
Iteration 31/1000 | Loss: 0.00003110
Iteration 32/1000 | Loss: 0.00003000
Iteration 33/1000 | Loss: 0.00002853
Iteration 34/1000 | Loss: 0.00002724
Iteration 35/1000 | Loss: 0.00002646
Iteration 36/1000 | Loss: 0.00002575
Iteration 37/1000 | Loss: 0.00005608
Iteration 38/1000 | Loss: 0.00005188
Iteration 39/1000 | Loss: 0.00003727
Iteration 40/1000 | Loss: 0.00004476
Iteration 41/1000 | Loss: 0.00004159
Iteration 42/1000 | Loss: 0.00004134
Iteration 43/1000 | Loss: 0.00005378
Iteration 44/1000 | Loss: 0.00005282
Iteration 45/1000 | Loss: 0.00005469
Iteration 46/1000 | Loss: 0.00005257
Iteration 47/1000 | Loss: 0.00005263
Iteration 48/1000 | Loss: 0.00003773
Iteration 49/1000 | Loss: 0.00002891
Iteration 50/1000 | Loss: 0.00002848
Iteration 51/1000 | Loss: 0.00003512
Iteration 52/1000 | Loss: 0.00004595
Iteration 53/1000 | Loss: 0.00004651
Iteration 54/1000 | Loss: 0.00004505
Iteration 55/1000 | Loss: 0.00003067
Iteration 56/1000 | Loss: 0.00003610
Iteration 57/1000 | Loss: 0.00003592
Iteration 58/1000 | Loss: 0.00003753
Iteration 59/1000 | Loss: 0.00003455
Iteration 60/1000 | Loss: 0.00003040
Iteration 61/1000 | Loss: 0.00003949
Iteration 62/1000 | Loss: 0.00003336
Iteration 63/1000 | Loss: 0.00003530
Iteration 64/1000 | Loss: 0.00003034
Iteration 65/1000 | Loss: 0.00003295
Iteration 66/1000 | Loss: 0.00003023
Iteration 67/1000 | Loss: 0.00003624
Iteration 68/1000 | Loss: 0.00003119
Iteration 69/1000 | Loss: 0.00003078
Iteration 70/1000 | Loss: 0.00004087
Iteration 71/1000 | Loss: 0.00003259
Iteration 72/1000 | Loss: 0.00003699
Iteration 73/1000 | Loss: 0.00003336
Iteration 74/1000 | Loss: 0.00003733
Iteration 75/1000 | Loss: 0.00003341
Iteration 76/1000 | Loss: 0.00003692
Iteration 77/1000 | Loss: 0.00003282
Iteration 78/1000 | Loss: 0.00002891
Iteration 79/1000 | Loss: 0.00003287
Iteration 80/1000 | Loss: 0.00003109
Iteration 81/1000 | Loss: 0.00003237
Iteration 82/1000 | Loss: 0.00003197
Iteration 83/1000 | Loss: 0.00003165
Iteration 84/1000 | Loss: 0.00002996
Iteration 85/1000 | Loss: 0.00003052
Iteration 86/1000 | Loss: 0.00002957
Iteration 87/1000 | Loss: 0.00002958
Iteration 88/1000 | Loss: 0.00003070
Iteration 89/1000 | Loss: 0.00003081
Iteration 90/1000 | Loss: 0.00003368
Iteration 91/1000 | Loss: 0.00002989
Iteration 92/1000 | Loss: 0.00002871
Iteration 93/1000 | Loss: 0.00003817
Iteration 94/1000 | Loss: 0.00003155
Iteration 95/1000 | Loss: 0.00003030
Iteration 96/1000 | Loss: 0.00003495
Iteration 97/1000 | Loss: 0.00003122
Iteration 98/1000 | Loss: 0.00002905
Iteration 99/1000 | Loss: 0.00002921
Iteration 100/1000 | Loss: 0.00002891
Iteration 101/1000 | Loss: 0.00003205
Iteration 102/1000 | Loss: 0.00003473
Iteration 103/1000 | Loss: 0.00002931
Iteration 104/1000 | Loss: 0.00003589
Iteration 105/1000 | Loss: 0.00005029
Iteration 106/1000 | Loss: 0.00003168
Iteration 107/1000 | Loss: 0.00003802
Iteration 108/1000 | Loss: 0.00004490
Iteration 109/1000 | Loss: 0.00004455
Iteration 110/1000 | Loss: 0.00003156
Iteration 111/1000 | Loss: 0.00003602
Iteration 112/1000 | Loss: 0.00003463
Iteration 113/1000 | Loss: 0.00003256
Iteration 114/1000 | Loss: 0.00003076
Iteration 115/1000 | Loss: 0.00002897
Iteration 116/1000 | Loss: 0.00002837
Iteration 117/1000 | Loss: 0.00002786
Iteration 118/1000 | Loss: 0.00002688
Iteration 119/1000 | Loss: 0.00003225
Iteration 120/1000 | Loss: 0.00002910
Iteration 121/1000 | Loss: 0.00002799
Iteration 122/1000 | Loss: 0.00002648
Iteration 123/1000 | Loss: 0.00002523
Iteration 124/1000 | Loss: 0.00002447
Iteration 125/1000 | Loss: 0.00002408
Iteration 126/1000 | Loss: 0.00002399
Iteration 127/1000 | Loss: 0.00002384
Iteration 128/1000 | Loss: 0.00002377
Iteration 129/1000 | Loss: 0.00002377
Iteration 130/1000 | Loss: 0.00002374
Iteration 131/1000 | Loss: 0.00002374
Iteration 132/1000 | Loss: 0.00003276
Iteration 133/1000 | Loss: 0.00003161
Iteration 134/1000 | Loss: 0.00003406
Iteration 135/1000 | Loss: 0.00002769
Iteration 136/1000 | Loss: 0.00002602
Iteration 137/1000 | Loss: 0.00002485
Iteration 138/1000 | Loss: 0.00002394
Iteration 139/1000 | Loss: 0.00002317
Iteration 140/1000 | Loss: 0.00002265
Iteration 141/1000 | Loss: 0.00002236
Iteration 142/1000 | Loss: 0.00002222
Iteration 143/1000 | Loss: 0.00002216
Iteration 144/1000 | Loss: 0.00002203
Iteration 145/1000 | Loss: 0.00002201
Iteration 146/1000 | Loss: 0.00002201
Iteration 147/1000 | Loss: 0.00002200
Iteration 148/1000 | Loss: 0.00002200
Iteration 149/1000 | Loss: 0.00002200
Iteration 150/1000 | Loss: 0.00002200
Iteration 151/1000 | Loss: 0.00002200
Iteration 152/1000 | Loss: 0.00002199
Iteration 153/1000 | Loss: 0.00002199
Iteration 154/1000 | Loss: 0.00002199
Iteration 155/1000 | Loss: 0.00002199
Iteration 156/1000 | Loss: 0.00002199
Iteration 157/1000 | Loss: 0.00002199
Iteration 158/1000 | Loss: 0.00002199
Iteration 159/1000 | Loss: 0.00002199
Iteration 160/1000 | Loss: 0.00002198
Iteration 161/1000 | Loss: 0.00002198
Iteration 162/1000 | Loss: 0.00002198
Iteration 163/1000 | Loss: 0.00002198
Iteration 164/1000 | Loss: 0.00002198
Iteration 165/1000 | Loss: 0.00002197
Iteration 166/1000 | Loss: 0.00002197
Iteration 167/1000 | Loss: 0.00002196
Iteration 168/1000 | Loss: 0.00002195
Iteration 169/1000 | Loss: 0.00002193
Iteration 170/1000 | Loss: 0.00002193
Iteration 171/1000 | Loss: 0.00002193
Iteration 172/1000 | Loss: 0.00002192
Iteration 173/1000 | Loss: 0.00002192
Iteration 174/1000 | Loss: 0.00002192
Iteration 175/1000 | Loss: 0.00002191
Iteration 176/1000 | Loss: 0.00002191
Iteration 177/1000 | Loss: 0.00002191
Iteration 178/1000 | Loss: 0.00002190
Iteration 179/1000 | Loss: 0.00002190
Iteration 180/1000 | Loss: 0.00002190
Iteration 181/1000 | Loss: 0.00002190
Iteration 182/1000 | Loss: 0.00002190
Iteration 183/1000 | Loss: 0.00002189
Iteration 184/1000 | Loss: 0.00002189
Iteration 185/1000 | Loss: 0.00002189
Iteration 186/1000 | Loss: 0.00002189
Iteration 187/1000 | Loss: 0.00002189
Iteration 188/1000 | Loss: 0.00002189
Iteration 189/1000 | Loss: 0.00002189
Iteration 190/1000 | Loss: 0.00002189
Iteration 191/1000 | Loss: 0.00002189
Iteration 192/1000 | Loss: 0.00002189
Iteration 193/1000 | Loss: 0.00002189
Iteration 194/1000 | Loss: 0.00002189
Iteration 195/1000 | Loss: 0.00002189
Iteration 196/1000 | Loss: 0.00002189
Iteration 197/1000 | Loss: 0.00002189
Iteration 198/1000 | Loss: 0.00002189
Iteration 199/1000 | Loss: 0.00002189
Iteration 200/1000 | Loss: 0.00002189
Iteration 201/1000 | Loss: 0.00002189
Iteration 202/1000 | Loss: 0.00002189
Iteration 203/1000 | Loss: 0.00002189
Iteration 204/1000 | Loss: 0.00002189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.1885389287490398e-05, 2.1885389287490398e-05, 2.1885389287490398e-05, 2.1885389287490398e-05, 2.1885389287490398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1885389287490398e-05

Optimization complete. Final v2v error: 3.6461398601531982 mm

Highest mean error: 7.67906379699707 mm for frame 103

Lowest mean error: 2.5940983295440674 mm for frame 74

Saving results

Total time: 242.3423774242401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408218
Iteration 2/25 | Loss: 0.00096710
Iteration 3/25 | Loss: 0.00087680
Iteration 4/25 | Loss: 0.00086155
Iteration 5/25 | Loss: 0.00085641
Iteration 6/25 | Loss: 0.00085549
Iteration 7/25 | Loss: 0.00085549
Iteration 8/25 | Loss: 0.00085549
Iteration 9/25 | Loss: 0.00085549
Iteration 10/25 | Loss: 0.00085549
Iteration 11/25 | Loss: 0.00085549
Iteration 12/25 | Loss: 0.00085549
Iteration 13/25 | Loss: 0.00085549
Iteration 14/25 | Loss: 0.00085549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008554898668080568, 0.0008554898668080568, 0.0008554898668080568, 0.0008554898668080568, 0.0008554898668080568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008554898668080568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42629492
Iteration 2/25 | Loss: 0.00044315
Iteration 3/25 | Loss: 0.00044315
Iteration 4/25 | Loss: 0.00044314
Iteration 5/25 | Loss: 0.00044314
Iteration 6/25 | Loss: 0.00044314
Iteration 7/25 | Loss: 0.00044314
Iteration 8/25 | Loss: 0.00044314
Iteration 9/25 | Loss: 0.00044314
Iteration 10/25 | Loss: 0.00044314
Iteration 11/25 | Loss: 0.00044314
Iteration 12/25 | Loss: 0.00044314
Iteration 13/25 | Loss: 0.00044314
Iteration 14/25 | Loss: 0.00044314
Iteration 15/25 | Loss: 0.00044314
Iteration 16/25 | Loss: 0.00044314
Iteration 17/25 | Loss: 0.00044314
Iteration 18/25 | Loss: 0.00044314
Iteration 19/25 | Loss: 0.00044314
Iteration 20/25 | Loss: 0.00044314
Iteration 21/25 | Loss: 0.00044314
Iteration 22/25 | Loss: 0.00044314
Iteration 23/25 | Loss: 0.00044314
Iteration 24/25 | Loss: 0.00044314
Iteration 25/25 | Loss: 0.00044314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044314
Iteration 2/1000 | Loss: 0.00002850
Iteration 3/1000 | Loss: 0.00001886
Iteration 4/1000 | Loss: 0.00001672
Iteration 5/1000 | Loss: 0.00001577
Iteration 6/1000 | Loss: 0.00001508
Iteration 7/1000 | Loss: 0.00001467
Iteration 8/1000 | Loss: 0.00001431
Iteration 9/1000 | Loss: 0.00001402
Iteration 10/1000 | Loss: 0.00001391
Iteration 11/1000 | Loss: 0.00001384
Iteration 12/1000 | Loss: 0.00001370
Iteration 13/1000 | Loss: 0.00001366
Iteration 14/1000 | Loss: 0.00001365
Iteration 15/1000 | Loss: 0.00001365
Iteration 16/1000 | Loss: 0.00001364
Iteration 17/1000 | Loss: 0.00001361
Iteration 18/1000 | Loss: 0.00001361
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001359
Iteration 22/1000 | Loss: 0.00001359
Iteration 23/1000 | Loss: 0.00001354
Iteration 24/1000 | Loss: 0.00001353
Iteration 25/1000 | Loss: 0.00001353
Iteration 26/1000 | Loss: 0.00001349
Iteration 27/1000 | Loss: 0.00001348
Iteration 28/1000 | Loss: 0.00001348
Iteration 29/1000 | Loss: 0.00001344
Iteration 30/1000 | Loss: 0.00001344
Iteration 31/1000 | Loss: 0.00001343
Iteration 32/1000 | Loss: 0.00001343
Iteration 33/1000 | Loss: 0.00001343
Iteration 34/1000 | Loss: 0.00001342
Iteration 35/1000 | Loss: 0.00001342
Iteration 36/1000 | Loss: 0.00001342
Iteration 37/1000 | Loss: 0.00001342
Iteration 38/1000 | Loss: 0.00001341
Iteration 39/1000 | Loss: 0.00001340
Iteration 40/1000 | Loss: 0.00001340
Iteration 41/1000 | Loss: 0.00001340
Iteration 42/1000 | Loss: 0.00001339
Iteration 43/1000 | Loss: 0.00001339
Iteration 44/1000 | Loss: 0.00001339
Iteration 45/1000 | Loss: 0.00001339
Iteration 46/1000 | Loss: 0.00001339
Iteration 47/1000 | Loss: 0.00001339
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001338
Iteration 50/1000 | Loss: 0.00001338
Iteration 51/1000 | Loss: 0.00001338
Iteration 52/1000 | Loss: 0.00001337
Iteration 53/1000 | Loss: 0.00001337
Iteration 54/1000 | Loss: 0.00001336
Iteration 55/1000 | Loss: 0.00001336
Iteration 56/1000 | Loss: 0.00001336
Iteration 57/1000 | Loss: 0.00001336
Iteration 58/1000 | Loss: 0.00001335
Iteration 59/1000 | Loss: 0.00001335
Iteration 60/1000 | Loss: 0.00001335
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001334
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001333
Iteration 68/1000 | Loss: 0.00001333
Iteration 69/1000 | Loss: 0.00001333
Iteration 70/1000 | Loss: 0.00001333
Iteration 71/1000 | Loss: 0.00001332
Iteration 72/1000 | Loss: 0.00001332
Iteration 73/1000 | Loss: 0.00001332
Iteration 74/1000 | Loss: 0.00001332
Iteration 75/1000 | Loss: 0.00001332
Iteration 76/1000 | Loss: 0.00001332
Iteration 77/1000 | Loss: 0.00001331
Iteration 78/1000 | Loss: 0.00001331
Iteration 79/1000 | Loss: 0.00001331
Iteration 80/1000 | Loss: 0.00001331
Iteration 81/1000 | Loss: 0.00001331
Iteration 82/1000 | Loss: 0.00001330
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001328
Iteration 87/1000 | Loss: 0.00001328
Iteration 88/1000 | Loss: 0.00001328
Iteration 89/1000 | Loss: 0.00001328
Iteration 90/1000 | Loss: 0.00001327
Iteration 91/1000 | Loss: 0.00001327
Iteration 92/1000 | Loss: 0.00001327
Iteration 93/1000 | Loss: 0.00001327
Iteration 94/1000 | Loss: 0.00001327
Iteration 95/1000 | Loss: 0.00001326
Iteration 96/1000 | Loss: 0.00001326
Iteration 97/1000 | Loss: 0.00001325
Iteration 98/1000 | Loss: 0.00001325
Iteration 99/1000 | Loss: 0.00001325
Iteration 100/1000 | Loss: 0.00001325
Iteration 101/1000 | Loss: 0.00001325
Iteration 102/1000 | Loss: 0.00001324
Iteration 103/1000 | Loss: 0.00001324
Iteration 104/1000 | Loss: 0.00001324
Iteration 105/1000 | Loss: 0.00001324
Iteration 106/1000 | Loss: 0.00001324
Iteration 107/1000 | Loss: 0.00001324
Iteration 108/1000 | Loss: 0.00001324
Iteration 109/1000 | Loss: 0.00001324
Iteration 110/1000 | Loss: 0.00001324
Iteration 111/1000 | Loss: 0.00001323
Iteration 112/1000 | Loss: 0.00001323
Iteration 113/1000 | Loss: 0.00001323
Iteration 114/1000 | Loss: 0.00001323
Iteration 115/1000 | Loss: 0.00001323
Iteration 116/1000 | Loss: 0.00001323
Iteration 117/1000 | Loss: 0.00001322
Iteration 118/1000 | Loss: 0.00001322
Iteration 119/1000 | Loss: 0.00001322
Iteration 120/1000 | Loss: 0.00001322
Iteration 121/1000 | Loss: 0.00001322
Iteration 122/1000 | Loss: 0.00001322
Iteration 123/1000 | Loss: 0.00001322
Iteration 124/1000 | Loss: 0.00001322
Iteration 125/1000 | Loss: 0.00001322
Iteration 126/1000 | Loss: 0.00001322
Iteration 127/1000 | Loss: 0.00001322
Iteration 128/1000 | Loss: 0.00001322
Iteration 129/1000 | Loss: 0.00001322
Iteration 130/1000 | Loss: 0.00001321
Iteration 131/1000 | Loss: 0.00001321
Iteration 132/1000 | Loss: 0.00001321
Iteration 133/1000 | Loss: 0.00001321
Iteration 134/1000 | Loss: 0.00001321
Iteration 135/1000 | Loss: 0.00001321
Iteration 136/1000 | Loss: 0.00001321
Iteration 137/1000 | Loss: 0.00001321
Iteration 138/1000 | Loss: 0.00001321
Iteration 139/1000 | Loss: 0.00001320
Iteration 140/1000 | Loss: 0.00001320
Iteration 141/1000 | Loss: 0.00001320
Iteration 142/1000 | Loss: 0.00001320
Iteration 143/1000 | Loss: 0.00001320
Iteration 144/1000 | Loss: 0.00001320
Iteration 145/1000 | Loss: 0.00001320
Iteration 146/1000 | Loss: 0.00001320
Iteration 147/1000 | Loss: 0.00001320
Iteration 148/1000 | Loss: 0.00001320
Iteration 149/1000 | Loss: 0.00001320
Iteration 150/1000 | Loss: 0.00001320
Iteration 151/1000 | Loss: 0.00001320
Iteration 152/1000 | Loss: 0.00001320
Iteration 153/1000 | Loss: 0.00001320
Iteration 154/1000 | Loss: 0.00001320
Iteration 155/1000 | Loss: 0.00001319
Iteration 156/1000 | Loss: 0.00001319
Iteration 157/1000 | Loss: 0.00001319
Iteration 158/1000 | Loss: 0.00001319
Iteration 159/1000 | Loss: 0.00001319
Iteration 160/1000 | Loss: 0.00001319
Iteration 161/1000 | Loss: 0.00001319
Iteration 162/1000 | Loss: 0.00001319
Iteration 163/1000 | Loss: 0.00001319
Iteration 164/1000 | Loss: 0.00001318
Iteration 165/1000 | Loss: 0.00001318
Iteration 166/1000 | Loss: 0.00001318
Iteration 167/1000 | Loss: 0.00001318
Iteration 168/1000 | Loss: 0.00001318
Iteration 169/1000 | Loss: 0.00001318
Iteration 170/1000 | Loss: 0.00001318
Iteration 171/1000 | Loss: 0.00001318
Iteration 172/1000 | Loss: 0.00001318
Iteration 173/1000 | Loss: 0.00001318
Iteration 174/1000 | Loss: 0.00001318
Iteration 175/1000 | Loss: 0.00001318
Iteration 176/1000 | Loss: 0.00001318
Iteration 177/1000 | Loss: 0.00001317
Iteration 178/1000 | Loss: 0.00001317
Iteration 179/1000 | Loss: 0.00001317
Iteration 180/1000 | Loss: 0.00001317
Iteration 181/1000 | Loss: 0.00001317
Iteration 182/1000 | Loss: 0.00001317
Iteration 183/1000 | Loss: 0.00001317
Iteration 184/1000 | Loss: 0.00001317
Iteration 185/1000 | Loss: 0.00001317
Iteration 186/1000 | Loss: 0.00001317
Iteration 187/1000 | Loss: 0.00001317
Iteration 188/1000 | Loss: 0.00001317
Iteration 189/1000 | Loss: 0.00001317
Iteration 190/1000 | Loss: 0.00001317
Iteration 191/1000 | Loss: 0.00001317
Iteration 192/1000 | Loss: 0.00001317
Iteration 193/1000 | Loss: 0.00001317
Iteration 194/1000 | Loss: 0.00001317
Iteration 195/1000 | Loss: 0.00001317
Iteration 196/1000 | Loss: 0.00001317
Iteration 197/1000 | Loss: 0.00001316
Iteration 198/1000 | Loss: 0.00001316
Iteration 199/1000 | Loss: 0.00001316
Iteration 200/1000 | Loss: 0.00001316
Iteration 201/1000 | Loss: 0.00001316
Iteration 202/1000 | Loss: 0.00001316
Iteration 203/1000 | Loss: 0.00001316
Iteration 204/1000 | Loss: 0.00001316
Iteration 205/1000 | Loss: 0.00001316
Iteration 206/1000 | Loss: 0.00001316
Iteration 207/1000 | Loss: 0.00001316
Iteration 208/1000 | Loss: 0.00001316
Iteration 209/1000 | Loss: 0.00001316
Iteration 210/1000 | Loss: 0.00001316
Iteration 211/1000 | Loss: 0.00001316
Iteration 212/1000 | Loss: 0.00001316
Iteration 213/1000 | Loss: 0.00001316
Iteration 214/1000 | Loss: 0.00001316
Iteration 215/1000 | Loss: 0.00001316
Iteration 216/1000 | Loss: 0.00001315
Iteration 217/1000 | Loss: 0.00001315
Iteration 218/1000 | Loss: 0.00001315
Iteration 219/1000 | Loss: 0.00001315
Iteration 220/1000 | Loss: 0.00001315
Iteration 221/1000 | Loss: 0.00001315
Iteration 222/1000 | Loss: 0.00001315
Iteration 223/1000 | Loss: 0.00001315
Iteration 224/1000 | Loss: 0.00001315
Iteration 225/1000 | Loss: 0.00001315
Iteration 226/1000 | Loss: 0.00001315
Iteration 227/1000 | Loss: 0.00001315
Iteration 228/1000 | Loss: 0.00001315
Iteration 229/1000 | Loss: 0.00001315
Iteration 230/1000 | Loss: 0.00001315
Iteration 231/1000 | Loss: 0.00001315
Iteration 232/1000 | Loss: 0.00001315
Iteration 233/1000 | Loss: 0.00001315
Iteration 234/1000 | Loss: 0.00001315
Iteration 235/1000 | Loss: 0.00001315
Iteration 236/1000 | Loss: 0.00001315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.3150219274393748e-05, 1.3150219274393748e-05, 1.3150219274393748e-05, 1.3150219274393748e-05, 1.3150219274393748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3150219274393748e-05

Optimization complete. Final v2v error: 3.0614936351776123 mm

Highest mean error: 3.5293056964874268 mm for frame 25

Lowest mean error: 2.642803430557251 mm for frame 46

Saving results

Total time: 40.680466651916504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00731753
Iteration 2/25 | Loss: 0.00108246
Iteration 3/25 | Loss: 0.00093873
Iteration 4/25 | Loss: 0.00092495
Iteration 5/25 | Loss: 0.00092229
Iteration 6/25 | Loss: 0.00092174
Iteration 7/25 | Loss: 0.00092174
Iteration 8/25 | Loss: 0.00092174
Iteration 9/25 | Loss: 0.00092174
Iteration 10/25 | Loss: 0.00092174
Iteration 11/25 | Loss: 0.00092174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000921740080229938, 0.000921740080229938, 0.000921740080229938, 0.000921740080229938, 0.000921740080229938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000921740080229938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31990910
Iteration 2/25 | Loss: 0.00055109
Iteration 3/25 | Loss: 0.00055107
Iteration 4/25 | Loss: 0.00055107
Iteration 5/25 | Loss: 0.00055107
Iteration 6/25 | Loss: 0.00055107
Iteration 7/25 | Loss: 0.00055107
Iteration 8/25 | Loss: 0.00055107
Iteration 9/25 | Loss: 0.00055107
Iteration 10/25 | Loss: 0.00055107
Iteration 11/25 | Loss: 0.00055107
Iteration 12/25 | Loss: 0.00055107
Iteration 13/25 | Loss: 0.00055107
Iteration 14/25 | Loss: 0.00055107
Iteration 15/25 | Loss: 0.00055107
Iteration 16/25 | Loss: 0.00055107
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005510696209967136, 0.0005510696209967136, 0.0005510696209967136, 0.0005510696209967136, 0.0005510696209967136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005510696209967136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055107
Iteration 2/1000 | Loss: 0.00003084
Iteration 3/1000 | Loss: 0.00002131
Iteration 4/1000 | Loss: 0.00001859
Iteration 5/1000 | Loss: 0.00001740
Iteration 6/1000 | Loss: 0.00001663
Iteration 7/1000 | Loss: 0.00001608
Iteration 8/1000 | Loss: 0.00001568
Iteration 9/1000 | Loss: 0.00001546
Iteration 10/1000 | Loss: 0.00001544
Iteration 11/1000 | Loss: 0.00001539
Iteration 12/1000 | Loss: 0.00001531
Iteration 13/1000 | Loss: 0.00001531
Iteration 14/1000 | Loss: 0.00001522
Iteration 15/1000 | Loss: 0.00001515
Iteration 16/1000 | Loss: 0.00001514
Iteration 17/1000 | Loss: 0.00001514
Iteration 18/1000 | Loss: 0.00001514
Iteration 19/1000 | Loss: 0.00001511
Iteration 20/1000 | Loss: 0.00001510
Iteration 21/1000 | Loss: 0.00001510
Iteration 22/1000 | Loss: 0.00001510
Iteration 23/1000 | Loss: 0.00001509
Iteration 24/1000 | Loss: 0.00001509
Iteration 25/1000 | Loss: 0.00001509
Iteration 26/1000 | Loss: 0.00001506
Iteration 27/1000 | Loss: 0.00001505
Iteration 28/1000 | Loss: 0.00001504
Iteration 29/1000 | Loss: 0.00001501
Iteration 30/1000 | Loss: 0.00001497
Iteration 31/1000 | Loss: 0.00001497
Iteration 32/1000 | Loss: 0.00001496
Iteration 33/1000 | Loss: 0.00001495
Iteration 34/1000 | Loss: 0.00001495
Iteration 35/1000 | Loss: 0.00001493
Iteration 36/1000 | Loss: 0.00001493
Iteration 37/1000 | Loss: 0.00001492
Iteration 38/1000 | Loss: 0.00001492
Iteration 39/1000 | Loss: 0.00001492
Iteration 40/1000 | Loss: 0.00001492
Iteration 41/1000 | Loss: 0.00001492
Iteration 42/1000 | Loss: 0.00001492
Iteration 43/1000 | Loss: 0.00001492
Iteration 44/1000 | Loss: 0.00001492
Iteration 45/1000 | Loss: 0.00001491
Iteration 46/1000 | Loss: 0.00001491
Iteration 47/1000 | Loss: 0.00001491
Iteration 48/1000 | Loss: 0.00001491
Iteration 49/1000 | Loss: 0.00001491
Iteration 50/1000 | Loss: 0.00001491
Iteration 51/1000 | Loss: 0.00001491
Iteration 52/1000 | Loss: 0.00001490
Iteration 53/1000 | Loss: 0.00001490
Iteration 54/1000 | Loss: 0.00001490
Iteration 55/1000 | Loss: 0.00001490
Iteration 56/1000 | Loss: 0.00001489
Iteration 57/1000 | Loss: 0.00001489
Iteration 58/1000 | Loss: 0.00001489
Iteration 59/1000 | Loss: 0.00001489
Iteration 60/1000 | Loss: 0.00001489
Iteration 61/1000 | Loss: 0.00001489
Iteration 62/1000 | Loss: 0.00001489
Iteration 63/1000 | Loss: 0.00001489
Iteration 64/1000 | Loss: 0.00001489
Iteration 65/1000 | Loss: 0.00001489
Iteration 66/1000 | Loss: 0.00001488
Iteration 67/1000 | Loss: 0.00001488
Iteration 68/1000 | Loss: 0.00001488
Iteration 69/1000 | Loss: 0.00001488
Iteration 70/1000 | Loss: 0.00001488
Iteration 71/1000 | Loss: 0.00001488
Iteration 72/1000 | Loss: 0.00001487
Iteration 73/1000 | Loss: 0.00001487
Iteration 74/1000 | Loss: 0.00001487
Iteration 75/1000 | Loss: 0.00001487
Iteration 76/1000 | Loss: 0.00001487
Iteration 77/1000 | Loss: 0.00001487
Iteration 78/1000 | Loss: 0.00001487
Iteration 79/1000 | Loss: 0.00001487
Iteration 80/1000 | Loss: 0.00001487
Iteration 81/1000 | Loss: 0.00001487
Iteration 82/1000 | Loss: 0.00001487
Iteration 83/1000 | Loss: 0.00001487
Iteration 84/1000 | Loss: 0.00001487
Iteration 85/1000 | Loss: 0.00001487
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001487
Iteration 88/1000 | Loss: 0.00001487
Iteration 89/1000 | Loss: 0.00001487
Iteration 90/1000 | Loss: 0.00001487
Iteration 91/1000 | Loss: 0.00001487
Iteration 92/1000 | Loss: 0.00001487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.4873112377244979e-05, 1.4873112377244979e-05, 1.4873112377244979e-05, 1.4873112377244979e-05, 1.4873112377244979e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4873112377244979e-05

Optimization complete. Final v2v error: 3.1656055450439453 mm

Highest mean error: 3.4504587650299072 mm for frame 27

Lowest mean error: 2.923975944519043 mm for frame 75

Saving results

Total time: 30.45480704307556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00373065
Iteration 2/25 | Loss: 0.00091255
Iteration 3/25 | Loss: 0.00079852
Iteration 4/25 | Loss: 0.00079252
Iteration 5/25 | Loss: 0.00079076
Iteration 6/25 | Loss: 0.00079076
Iteration 7/25 | Loss: 0.00079076
Iteration 8/25 | Loss: 0.00079076
Iteration 9/25 | Loss: 0.00079076
Iteration 10/25 | Loss: 0.00079076
Iteration 11/25 | Loss: 0.00079076
Iteration 12/25 | Loss: 0.00079076
Iteration 13/25 | Loss: 0.00079076
Iteration 14/25 | Loss: 0.00079076
Iteration 15/25 | Loss: 0.00079076
Iteration 16/25 | Loss: 0.00079076
Iteration 17/25 | Loss: 0.00079076
Iteration 18/25 | Loss: 0.00079076
Iteration 19/25 | Loss: 0.00079076
Iteration 20/25 | Loss: 0.00079076
Iteration 21/25 | Loss: 0.00079076
Iteration 22/25 | Loss: 0.00079076
Iteration 23/25 | Loss: 0.00079076
Iteration 24/25 | Loss: 0.00079076
Iteration 25/25 | Loss: 0.00079076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37745786
Iteration 2/25 | Loss: 0.00031812
Iteration 3/25 | Loss: 0.00031812
Iteration 4/25 | Loss: 0.00031812
Iteration 5/25 | Loss: 0.00031812
Iteration 6/25 | Loss: 0.00031812
Iteration 7/25 | Loss: 0.00031811
Iteration 8/25 | Loss: 0.00031811
Iteration 9/25 | Loss: 0.00031811
Iteration 10/25 | Loss: 0.00031811
Iteration 11/25 | Loss: 0.00031811
Iteration 12/25 | Loss: 0.00031811
Iteration 13/25 | Loss: 0.00031811
Iteration 14/25 | Loss: 0.00031811
Iteration 15/25 | Loss: 0.00031811
Iteration 16/25 | Loss: 0.00031811
Iteration 17/25 | Loss: 0.00031811
Iteration 18/25 | Loss: 0.00031811
Iteration 19/25 | Loss: 0.00031811
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003181138017680496, 0.0003181138017680496, 0.0003181138017680496, 0.0003181138017680496, 0.0003181138017680496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003181138017680496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031811
Iteration 2/1000 | Loss: 0.00001397
Iteration 3/1000 | Loss: 0.00001038
Iteration 4/1000 | Loss: 0.00000935
Iteration 5/1000 | Loss: 0.00000867
Iteration 6/1000 | Loss: 0.00000836
Iteration 7/1000 | Loss: 0.00000802
Iteration 8/1000 | Loss: 0.00000774
Iteration 9/1000 | Loss: 0.00000768
Iteration 10/1000 | Loss: 0.00000764
Iteration 11/1000 | Loss: 0.00000761
Iteration 12/1000 | Loss: 0.00000760
Iteration 13/1000 | Loss: 0.00000760
Iteration 14/1000 | Loss: 0.00000760
Iteration 15/1000 | Loss: 0.00000760
Iteration 16/1000 | Loss: 0.00000759
Iteration 17/1000 | Loss: 0.00000759
Iteration 18/1000 | Loss: 0.00000757
Iteration 19/1000 | Loss: 0.00000756
Iteration 20/1000 | Loss: 0.00000756
Iteration 21/1000 | Loss: 0.00000755
Iteration 22/1000 | Loss: 0.00000755
Iteration 23/1000 | Loss: 0.00000754
Iteration 24/1000 | Loss: 0.00000754
Iteration 25/1000 | Loss: 0.00000753
Iteration 26/1000 | Loss: 0.00000752
Iteration 27/1000 | Loss: 0.00000752
Iteration 28/1000 | Loss: 0.00000752
Iteration 29/1000 | Loss: 0.00000751
Iteration 30/1000 | Loss: 0.00000751
Iteration 31/1000 | Loss: 0.00000750
Iteration 32/1000 | Loss: 0.00000748
Iteration 33/1000 | Loss: 0.00000748
Iteration 34/1000 | Loss: 0.00000748
Iteration 35/1000 | Loss: 0.00000748
Iteration 36/1000 | Loss: 0.00000748
Iteration 37/1000 | Loss: 0.00000748
Iteration 38/1000 | Loss: 0.00000747
Iteration 39/1000 | Loss: 0.00000747
Iteration 40/1000 | Loss: 0.00000747
Iteration 41/1000 | Loss: 0.00000747
Iteration 42/1000 | Loss: 0.00000747
Iteration 43/1000 | Loss: 0.00000743
Iteration 44/1000 | Loss: 0.00000743
Iteration 45/1000 | Loss: 0.00000743
Iteration 46/1000 | Loss: 0.00000743
Iteration 47/1000 | Loss: 0.00000743
Iteration 48/1000 | Loss: 0.00000743
Iteration 49/1000 | Loss: 0.00000743
Iteration 50/1000 | Loss: 0.00000742
Iteration 51/1000 | Loss: 0.00000742
Iteration 52/1000 | Loss: 0.00000742
Iteration 53/1000 | Loss: 0.00000742
Iteration 54/1000 | Loss: 0.00000742
Iteration 55/1000 | Loss: 0.00000741
Iteration 56/1000 | Loss: 0.00000741
Iteration 57/1000 | Loss: 0.00000741
Iteration 58/1000 | Loss: 0.00000741
Iteration 59/1000 | Loss: 0.00000741
Iteration 60/1000 | Loss: 0.00000740
Iteration 61/1000 | Loss: 0.00000740
Iteration 62/1000 | Loss: 0.00000740
Iteration 63/1000 | Loss: 0.00000740
Iteration 64/1000 | Loss: 0.00000740
Iteration 65/1000 | Loss: 0.00000739
Iteration 66/1000 | Loss: 0.00000739
Iteration 67/1000 | Loss: 0.00000739
Iteration 68/1000 | Loss: 0.00000739
Iteration 69/1000 | Loss: 0.00000739
Iteration 70/1000 | Loss: 0.00000738
Iteration 71/1000 | Loss: 0.00000738
Iteration 72/1000 | Loss: 0.00000738
Iteration 73/1000 | Loss: 0.00000738
Iteration 74/1000 | Loss: 0.00000738
Iteration 75/1000 | Loss: 0.00000738
Iteration 76/1000 | Loss: 0.00000738
Iteration 77/1000 | Loss: 0.00000737
Iteration 78/1000 | Loss: 0.00000737
Iteration 79/1000 | Loss: 0.00000736
Iteration 80/1000 | Loss: 0.00000736
Iteration 81/1000 | Loss: 0.00000736
Iteration 82/1000 | Loss: 0.00000735
Iteration 83/1000 | Loss: 0.00000735
Iteration 84/1000 | Loss: 0.00000735
Iteration 85/1000 | Loss: 0.00000735
Iteration 86/1000 | Loss: 0.00000735
Iteration 87/1000 | Loss: 0.00000735
Iteration 88/1000 | Loss: 0.00000735
Iteration 89/1000 | Loss: 0.00000735
Iteration 90/1000 | Loss: 0.00000735
Iteration 91/1000 | Loss: 0.00000735
Iteration 92/1000 | Loss: 0.00000735
Iteration 93/1000 | Loss: 0.00000735
Iteration 94/1000 | Loss: 0.00000734
Iteration 95/1000 | Loss: 0.00000734
Iteration 96/1000 | Loss: 0.00000734
Iteration 97/1000 | Loss: 0.00000733
Iteration 98/1000 | Loss: 0.00000733
Iteration 99/1000 | Loss: 0.00000733
Iteration 100/1000 | Loss: 0.00000732
Iteration 101/1000 | Loss: 0.00000732
Iteration 102/1000 | Loss: 0.00000731
Iteration 103/1000 | Loss: 0.00000730
Iteration 104/1000 | Loss: 0.00000730
Iteration 105/1000 | Loss: 0.00000729
Iteration 106/1000 | Loss: 0.00000729
Iteration 107/1000 | Loss: 0.00000729
Iteration 108/1000 | Loss: 0.00000729
Iteration 109/1000 | Loss: 0.00000729
Iteration 110/1000 | Loss: 0.00000729
Iteration 111/1000 | Loss: 0.00000729
Iteration 112/1000 | Loss: 0.00000729
Iteration 113/1000 | Loss: 0.00000729
Iteration 114/1000 | Loss: 0.00000729
Iteration 115/1000 | Loss: 0.00000729
Iteration 116/1000 | Loss: 0.00000728
Iteration 117/1000 | Loss: 0.00000728
Iteration 118/1000 | Loss: 0.00000728
Iteration 119/1000 | Loss: 0.00000728
Iteration 120/1000 | Loss: 0.00000728
Iteration 121/1000 | Loss: 0.00000728
Iteration 122/1000 | Loss: 0.00000728
Iteration 123/1000 | Loss: 0.00000728
Iteration 124/1000 | Loss: 0.00000728
Iteration 125/1000 | Loss: 0.00000728
Iteration 126/1000 | Loss: 0.00000727
Iteration 127/1000 | Loss: 0.00000727
Iteration 128/1000 | Loss: 0.00000727
Iteration 129/1000 | Loss: 0.00000727
Iteration 130/1000 | Loss: 0.00000727
Iteration 131/1000 | Loss: 0.00000727
Iteration 132/1000 | Loss: 0.00000727
Iteration 133/1000 | Loss: 0.00000727
Iteration 134/1000 | Loss: 0.00000727
Iteration 135/1000 | Loss: 0.00000727
Iteration 136/1000 | Loss: 0.00000727
Iteration 137/1000 | Loss: 0.00000727
Iteration 138/1000 | Loss: 0.00000726
Iteration 139/1000 | Loss: 0.00000726
Iteration 140/1000 | Loss: 0.00000726
Iteration 141/1000 | Loss: 0.00000726
Iteration 142/1000 | Loss: 0.00000726
Iteration 143/1000 | Loss: 0.00000725
Iteration 144/1000 | Loss: 0.00000725
Iteration 145/1000 | Loss: 0.00000725
Iteration 146/1000 | Loss: 0.00000725
Iteration 147/1000 | Loss: 0.00000725
Iteration 148/1000 | Loss: 0.00000725
Iteration 149/1000 | Loss: 0.00000725
Iteration 150/1000 | Loss: 0.00000725
Iteration 151/1000 | Loss: 0.00000725
Iteration 152/1000 | Loss: 0.00000725
Iteration 153/1000 | Loss: 0.00000725
Iteration 154/1000 | Loss: 0.00000725
Iteration 155/1000 | Loss: 0.00000725
Iteration 156/1000 | Loss: 0.00000725
Iteration 157/1000 | Loss: 0.00000725
Iteration 158/1000 | Loss: 0.00000725
Iteration 159/1000 | Loss: 0.00000725
Iteration 160/1000 | Loss: 0.00000725
Iteration 161/1000 | Loss: 0.00000725
Iteration 162/1000 | Loss: 0.00000725
Iteration 163/1000 | Loss: 0.00000725
Iteration 164/1000 | Loss: 0.00000725
Iteration 165/1000 | Loss: 0.00000725
Iteration 166/1000 | Loss: 0.00000725
Iteration 167/1000 | Loss: 0.00000725
Iteration 168/1000 | Loss: 0.00000725
Iteration 169/1000 | Loss: 0.00000725
Iteration 170/1000 | Loss: 0.00000725
Iteration 171/1000 | Loss: 0.00000725
Iteration 172/1000 | Loss: 0.00000725
Iteration 173/1000 | Loss: 0.00000725
Iteration 174/1000 | Loss: 0.00000725
Iteration 175/1000 | Loss: 0.00000725
Iteration 176/1000 | Loss: 0.00000725
Iteration 177/1000 | Loss: 0.00000725
Iteration 178/1000 | Loss: 0.00000725
Iteration 179/1000 | Loss: 0.00000725
Iteration 180/1000 | Loss: 0.00000725
Iteration 181/1000 | Loss: 0.00000725
Iteration 182/1000 | Loss: 0.00000725
Iteration 183/1000 | Loss: 0.00000725
Iteration 184/1000 | Loss: 0.00000725
Iteration 185/1000 | Loss: 0.00000725
Iteration 186/1000 | Loss: 0.00000725
Iteration 187/1000 | Loss: 0.00000725
Iteration 188/1000 | Loss: 0.00000725
Iteration 189/1000 | Loss: 0.00000725
Iteration 190/1000 | Loss: 0.00000725
Iteration 191/1000 | Loss: 0.00000725
Iteration 192/1000 | Loss: 0.00000725
Iteration 193/1000 | Loss: 0.00000725
Iteration 194/1000 | Loss: 0.00000725
Iteration 195/1000 | Loss: 0.00000725
Iteration 196/1000 | Loss: 0.00000725
Iteration 197/1000 | Loss: 0.00000725
Iteration 198/1000 | Loss: 0.00000725
Iteration 199/1000 | Loss: 0.00000725
Iteration 200/1000 | Loss: 0.00000725
Iteration 201/1000 | Loss: 0.00000725
Iteration 202/1000 | Loss: 0.00000725
Iteration 203/1000 | Loss: 0.00000725
Iteration 204/1000 | Loss: 0.00000725
Iteration 205/1000 | Loss: 0.00000725
Iteration 206/1000 | Loss: 0.00000725
Iteration 207/1000 | Loss: 0.00000725
Iteration 208/1000 | Loss: 0.00000725
Iteration 209/1000 | Loss: 0.00000725
Iteration 210/1000 | Loss: 0.00000725
Iteration 211/1000 | Loss: 0.00000725
Iteration 212/1000 | Loss: 0.00000725
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [7.245757387863705e-06, 7.245757387863705e-06, 7.245757387863705e-06, 7.245757387863705e-06, 7.245757387863705e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.245757387863705e-06

Optimization complete. Final v2v error: 2.3062288761138916 mm

Highest mean error: 2.3904457092285156 mm for frame 142

Lowest mean error: 2.1697940826416016 mm for frame 236

Saving results

Total time: 36.97356390953064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784930
Iteration 2/25 | Loss: 0.00158521
Iteration 3/25 | Loss: 0.00110244
Iteration 4/25 | Loss: 0.00104792
Iteration 5/25 | Loss: 0.00108108
Iteration 6/25 | Loss: 0.00105693
Iteration 7/25 | Loss: 0.00100391
Iteration 8/25 | Loss: 0.00099149
Iteration 9/25 | Loss: 0.00097338
Iteration 10/25 | Loss: 0.00097205
Iteration 11/25 | Loss: 0.00098163
Iteration 12/25 | Loss: 0.00097205
Iteration 13/25 | Loss: 0.00097742
Iteration 14/25 | Loss: 0.00098233
Iteration 15/25 | Loss: 0.00097823
Iteration 16/25 | Loss: 0.00096968
Iteration 17/25 | Loss: 0.00097648
Iteration 18/25 | Loss: 0.00097383
Iteration 19/25 | Loss: 0.00096949
Iteration 20/25 | Loss: 0.00096766
Iteration 21/25 | Loss: 0.00096728
Iteration 22/25 | Loss: 0.00096718
Iteration 23/25 | Loss: 0.00096718
Iteration 24/25 | Loss: 0.00096717
Iteration 25/25 | Loss: 0.00096717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56134677
Iteration 2/25 | Loss: 0.00032518
Iteration 3/25 | Loss: 0.00032513
Iteration 4/25 | Loss: 0.00032513
Iteration 5/25 | Loss: 0.00032512
Iteration 6/25 | Loss: 0.00032512
Iteration 7/25 | Loss: 0.00032512
Iteration 8/25 | Loss: 0.00032512
Iteration 9/25 | Loss: 0.00032512
Iteration 10/25 | Loss: 0.00032512
Iteration 11/25 | Loss: 0.00032512
Iteration 12/25 | Loss: 0.00032512
Iteration 13/25 | Loss: 0.00032512
Iteration 14/25 | Loss: 0.00032512
Iteration 15/25 | Loss: 0.00032512
Iteration 16/25 | Loss: 0.00032512
Iteration 17/25 | Loss: 0.00032512
Iteration 18/25 | Loss: 0.00032512
Iteration 19/25 | Loss: 0.00032512
Iteration 20/25 | Loss: 0.00032512
Iteration 21/25 | Loss: 0.00032512
Iteration 22/25 | Loss: 0.00032512
Iteration 23/25 | Loss: 0.00032512
Iteration 24/25 | Loss: 0.00032512
Iteration 25/25 | Loss: 0.00032512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032512
Iteration 2/1000 | Loss: 0.00002655
Iteration 3/1000 | Loss: 0.00001812
Iteration 4/1000 | Loss: 0.00001660
Iteration 5/1000 | Loss: 0.00001604
Iteration 6/1000 | Loss: 0.00001568
Iteration 7/1000 | Loss: 0.00001549
Iteration 8/1000 | Loss: 0.00001522
Iteration 9/1000 | Loss: 0.00001500
Iteration 10/1000 | Loss: 0.00001494
Iteration 11/1000 | Loss: 0.00001490
Iteration 12/1000 | Loss: 0.00001490
Iteration 13/1000 | Loss: 0.00001489
Iteration 14/1000 | Loss: 0.00001484
Iteration 15/1000 | Loss: 0.00001475
Iteration 16/1000 | Loss: 0.00001475
Iteration 17/1000 | Loss: 0.00001475
Iteration 18/1000 | Loss: 0.00001475
Iteration 19/1000 | Loss: 0.00001475
Iteration 20/1000 | Loss: 0.00001475
Iteration 21/1000 | Loss: 0.00001475
Iteration 22/1000 | Loss: 0.00001475
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001473
Iteration 25/1000 | Loss: 0.00001471
Iteration 26/1000 | Loss: 0.00001471
Iteration 27/1000 | Loss: 0.00001471
Iteration 28/1000 | Loss: 0.00001471
Iteration 29/1000 | Loss: 0.00001471
Iteration 30/1000 | Loss: 0.00001471
Iteration 31/1000 | Loss: 0.00001471
Iteration 32/1000 | Loss: 0.00001471
Iteration 33/1000 | Loss: 0.00001471
Iteration 34/1000 | Loss: 0.00001471
Iteration 35/1000 | Loss: 0.00001471
Iteration 36/1000 | Loss: 0.00001471
Iteration 37/1000 | Loss: 0.00001471
Iteration 38/1000 | Loss: 0.00001471
Iteration 39/1000 | Loss: 0.00001471
Iteration 40/1000 | Loss: 0.00001470
Iteration 41/1000 | Loss: 0.00001468
Iteration 42/1000 | Loss: 0.00001468
Iteration 43/1000 | Loss: 0.00001468
Iteration 44/1000 | Loss: 0.00001468
Iteration 45/1000 | Loss: 0.00001467
Iteration 46/1000 | Loss: 0.00001467
Iteration 47/1000 | Loss: 0.00001467
Iteration 48/1000 | Loss: 0.00001467
Iteration 49/1000 | Loss: 0.00001467
Iteration 50/1000 | Loss: 0.00001466
Iteration 51/1000 | Loss: 0.00001466
Iteration 52/1000 | Loss: 0.00001466
Iteration 53/1000 | Loss: 0.00001466
Iteration 54/1000 | Loss: 0.00001466
Iteration 55/1000 | Loss: 0.00001465
Iteration 56/1000 | Loss: 0.00001465
Iteration 57/1000 | Loss: 0.00001465
Iteration 58/1000 | Loss: 0.00001465
Iteration 59/1000 | Loss: 0.00001465
Iteration 60/1000 | Loss: 0.00001465
Iteration 61/1000 | Loss: 0.00001465
Iteration 62/1000 | Loss: 0.00001465
Iteration 63/1000 | Loss: 0.00001464
Iteration 64/1000 | Loss: 0.00001464
Iteration 65/1000 | Loss: 0.00001464
Iteration 66/1000 | Loss: 0.00001464
Iteration 67/1000 | Loss: 0.00001464
Iteration 68/1000 | Loss: 0.00001463
Iteration 69/1000 | Loss: 0.00001463
Iteration 70/1000 | Loss: 0.00001461
Iteration 71/1000 | Loss: 0.00001461
Iteration 72/1000 | Loss: 0.00001461
Iteration 73/1000 | Loss: 0.00001461
Iteration 74/1000 | Loss: 0.00001461
Iteration 75/1000 | Loss: 0.00001461
Iteration 76/1000 | Loss: 0.00001461
Iteration 77/1000 | Loss: 0.00001461
Iteration 78/1000 | Loss: 0.00001460
Iteration 79/1000 | Loss: 0.00001460
Iteration 80/1000 | Loss: 0.00001460
Iteration 81/1000 | Loss: 0.00001460
Iteration 82/1000 | Loss: 0.00001459
Iteration 83/1000 | Loss: 0.00001459
Iteration 84/1000 | Loss: 0.00001459
Iteration 85/1000 | Loss: 0.00001459
Iteration 86/1000 | Loss: 0.00001459
Iteration 87/1000 | Loss: 0.00001459
Iteration 88/1000 | Loss: 0.00001459
Iteration 89/1000 | Loss: 0.00001459
Iteration 90/1000 | Loss: 0.00001459
Iteration 91/1000 | Loss: 0.00001459
Iteration 92/1000 | Loss: 0.00001458
Iteration 93/1000 | Loss: 0.00001458
Iteration 94/1000 | Loss: 0.00001458
Iteration 95/1000 | Loss: 0.00001458
Iteration 96/1000 | Loss: 0.00001458
Iteration 97/1000 | Loss: 0.00001458
Iteration 98/1000 | Loss: 0.00001458
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001458
Iteration 101/1000 | Loss: 0.00001458
Iteration 102/1000 | Loss: 0.00001458
Iteration 103/1000 | Loss: 0.00001458
Iteration 104/1000 | Loss: 0.00001458
Iteration 105/1000 | Loss: 0.00001458
Iteration 106/1000 | Loss: 0.00001458
Iteration 107/1000 | Loss: 0.00001458
Iteration 108/1000 | Loss: 0.00001458
Iteration 109/1000 | Loss: 0.00001458
Iteration 110/1000 | Loss: 0.00001458
Iteration 111/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.4578383343177848e-05, 1.4578383343177848e-05, 1.4578383343177848e-05, 1.4578383343177848e-05, 1.4578383343177848e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4578383343177848e-05

Optimization complete. Final v2v error: 3.1519458293914795 mm

Highest mean error: 10.277382850646973 mm for frame 13

Lowest mean error: 2.805769681930542 mm for frame 134

Saving results

Total time: 67.21849775314331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059759
Iteration 2/25 | Loss: 0.00140719
Iteration 3/25 | Loss: 0.00101027
Iteration 4/25 | Loss: 0.00097444
Iteration 5/25 | Loss: 0.00097787
Iteration 6/25 | Loss: 0.00098110
Iteration 7/25 | Loss: 0.00098883
Iteration 8/25 | Loss: 0.00096930
Iteration 9/25 | Loss: 0.00094503
Iteration 10/25 | Loss: 0.00093676
Iteration 11/25 | Loss: 0.00092611
Iteration 12/25 | Loss: 0.00091827
Iteration 13/25 | Loss: 0.00091777
Iteration 14/25 | Loss: 0.00092276
Iteration 15/25 | Loss: 0.00092219
Iteration 16/25 | Loss: 0.00091946
Iteration 17/25 | Loss: 0.00091954
Iteration 18/25 | Loss: 0.00092169
Iteration 19/25 | Loss: 0.00092150
Iteration 20/25 | Loss: 0.00092116
Iteration 21/25 | Loss: 0.00092092
Iteration 22/25 | Loss: 0.00091615
Iteration 23/25 | Loss: 0.00091339
Iteration 24/25 | Loss: 0.00091517
Iteration 25/25 | Loss: 0.00091567

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59509456
Iteration 2/25 | Loss: 0.00081542
Iteration 3/25 | Loss: 0.00081540
Iteration 4/25 | Loss: 0.00081540
Iteration 5/25 | Loss: 0.00081540
Iteration 6/25 | Loss: 0.00081540
Iteration 7/25 | Loss: 0.00081540
Iteration 8/25 | Loss: 0.00081540
Iteration 9/25 | Loss: 0.00081540
Iteration 10/25 | Loss: 0.00081540
Iteration 11/25 | Loss: 0.00081540
Iteration 12/25 | Loss: 0.00081540
Iteration 13/25 | Loss: 0.00081540
Iteration 14/25 | Loss: 0.00081540
Iteration 15/25 | Loss: 0.00081540
Iteration 16/25 | Loss: 0.00081540
Iteration 17/25 | Loss: 0.00081540
Iteration 18/25 | Loss: 0.00081540
Iteration 19/25 | Loss: 0.00081540
Iteration 20/25 | Loss: 0.00081540
Iteration 21/25 | Loss: 0.00081540
Iteration 22/25 | Loss: 0.00081540
Iteration 23/25 | Loss: 0.00081540
Iteration 24/25 | Loss: 0.00081540
Iteration 25/25 | Loss: 0.00081540

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081540
Iteration 2/1000 | Loss: 0.00009220
Iteration 3/1000 | Loss: 0.00025308
Iteration 4/1000 | Loss: 0.00013514
Iteration 5/1000 | Loss: 0.00026763
Iteration 6/1000 | Loss: 0.00023895
Iteration 7/1000 | Loss: 0.00015734
Iteration 8/1000 | Loss: 0.00031115
Iteration 9/1000 | Loss: 0.00030899
Iteration 10/1000 | Loss: 0.00019579
Iteration 11/1000 | Loss: 0.00021221
Iteration 12/1000 | Loss: 0.00020667
Iteration 13/1000 | Loss: 0.00013179
Iteration 14/1000 | Loss: 0.00022531
Iteration 15/1000 | Loss: 0.00016814
Iteration 16/1000 | Loss: 0.00027619
Iteration 17/1000 | Loss: 0.00034393
Iteration 18/1000 | Loss: 0.00019121
Iteration 19/1000 | Loss: 0.00029873
Iteration 20/1000 | Loss: 0.00021688
Iteration 21/1000 | Loss: 0.00024339
Iteration 22/1000 | Loss: 0.00015247
Iteration 23/1000 | Loss: 0.00018736
Iteration 24/1000 | Loss: 0.00030246
Iteration 25/1000 | Loss: 0.00013623
Iteration 26/1000 | Loss: 0.00003455
Iteration 27/1000 | Loss: 0.00011646
Iteration 28/1000 | Loss: 0.00011411
Iteration 29/1000 | Loss: 0.00015171
Iteration 30/1000 | Loss: 0.00014458
Iteration 31/1000 | Loss: 0.00019719
Iteration 32/1000 | Loss: 0.00014642
Iteration 33/1000 | Loss: 0.00016011
Iteration 34/1000 | Loss: 0.00004824
Iteration 35/1000 | Loss: 0.00016380
Iteration 36/1000 | Loss: 0.00003201
Iteration 37/1000 | Loss: 0.00026953
Iteration 38/1000 | Loss: 0.00028521
Iteration 39/1000 | Loss: 0.00107406
Iteration 40/1000 | Loss: 0.00092332
Iteration 41/1000 | Loss: 0.00001979
Iteration 42/1000 | Loss: 0.00001767
Iteration 43/1000 | Loss: 0.00001619
Iteration 44/1000 | Loss: 0.00001521
Iteration 45/1000 | Loss: 0.00001468
Iteration 46/1000 | Loss: 0.00001400
Iteration 47/1000 | Loss: 0.00001359
Iteration 48/1000 | Loss: 0.00001336
Iteration 49/1000 | Loss: 0.00001328
Iteration 50/1000 | Loss: 0.00001318
Iteration 51/1000 | Loss: 0.00001316
Iteration 52/1000 | Loss: 0.00001307
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001296
Iteration 55/1000 | Loss: 0.00001294
Iteration 56/1000 | Loss: 0.00001293
Iteration 57/1000 | Loss: 0.00001287
Iteration 58/1000 | Loss: 0.00001286
Iteration 59/1000 | Loss: 0.00001280
Iteration 60/1000 | Loss: 0.00001271
Iteration 61/1000 | Loss: 0.00001265
Iteration 62/1000 | Loss: 0.00001264
Iteration 63/1000 | Loss: 0.00001263
Iteration 64/1000 | Loss: 0.00001262
Iteration 65/1000 | Loss: 0.00001261
Iteration 66/1000 | Loss: 0.00001261
Iteration 67/1000 | Loss: 0.00001260
Iteration 68/1000 | Loss: 0.00001260
Iteration 69/1000 | Loss: 0.00001259
Iteration 70/1000 | Loss: 0.00001259
Iteration 71/1000 | Loss: 0.00001259
Iteration 72/1000 | Loss: 0.00001259
Iteration 73/1000 | Loss: 0.00001259
Iteration 74/1000 | Loss: 0.00001258
Iteration 75/1000 | Loss: 0.00001258
Iteration 76/1000 | Loss: 0.00001258
Iteration 77/1000 | Loss: 0.00001258
Iteration 78/1000 | Loss: 0.00001258
Iteration 79/1000 | Loss: 0.00001258
Iteration 80/1000 | Loss: 0.00001258
Iteration 81/1000 | Loss: 0.00001258
Iteration 82/1000 | Loss: 0.00001258
Iteration 83/1000 | Loss: 0.00001257
Iteration 84/1000 | Loss: 0.00001257
Iteration 85/1000 | Loss: 0.00001257
Iteration 86/1000 | Loss: 0.00001257
Iteration 87/1000 | Loss: 0.00001256
Iteration 88/1000 | Loss: 0.00001256
Iteration 89/1000 | Loss: 0.00001256
Iteration 90/1000 | Loss: 0.00001256
Iteration 91/1000 | Loss: 0.00001256
Iteration 92/1000 | Loss: 0.00001256
Iteration 93/1000 | Loss: 0.00001256
Iteration 94/1000 | Loss: 0.00001256
Iteration 95/1000 | Loss: 0.00001256
Iteration 96/1000 | Loss: 0.00001256
Iteration 97/1000 | Loss: 0.00001256
Iteration 98/1000 | Loss: 0.00001256
Iteration 99/1000 | Loss: 0.00001256
Iteration 100/1000 | Loss: 0.00001256
Iteration 101/1000 | Loss: 0.00001256
Iteration 102/1000 | Loss: 0.00001255
Iteration 103/1000 | Loss: 0.00001255
Iteration 104/1000 | Loss: 0.00001255
Iteration 105/1000 | Loss: 0.00001255
Iteration 106/1000 | Loss: 0.00001255
Iteration 107/1000 | Loss: 0.00001254
Iteration 108/1000 | Loss: 0.00001254
Iteration 109/1000 | Loss: 0.00001254
Iteration 110/1000 | Loss: 0.00001254
Iteration 111/1000 | Loss: 0.00001254
Iteration 112/1000 | Loss: 0.00001254
Iteration 113/1000 | Loss: 0.00001254
Iteration 114/1000 | Loss: 0.00001253
Iteration 115/1000 | Loss: 0.00001253
Iteration 116/1000 | Loss: 0.00001253
Iteration 117/1000 | Loss: 0.00001253
Iteration 118/1000 | Loss: 0.00001253
Iteration 119/1000 | Loss: 0.00001253
Iteration 120/1000 | Loss: 0.00001253
Iteration 121/1000 | Loss: 0.00001253
Iteration 122/1000 | Loss: 0.00001253
Iteration 123/1000 | Loss: 0.00001253
Iteration 124/1000 | Loss: 0.00001253
Iteration 125/1000 | Loss: 0.00001253
Iteration 126/1000 | Loss: 0.00001253
Iteration 127/1000 | Loss: 0.00001253
Iteration 128/1000 | Loss: 0.00001253
Iteration 129/1000 | Loss: 0.00001253
Iteration 130/1000 | Loss: 0.00001253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.2530664207588416e-05, 1.2530664207588416e-05, 1.2530664207588416e-05, 1.2530664207588416e-05, 1.2530664207588416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2530664207588416e-05

Optimization complete. Final v2v error: 2.8334054946899414 mm

Highest mean error: 4.545565605163574 mm for frame 0

Lowest mean error: 2.4004406929016113 mm for frame 56

Saving results

Total time: 123.58516335487366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854207
Iteration 2/25 | Loss: 0.00099510
Iteration 3/25 | Loss: 0.00086370
Iteration 4/25 | Loss: 0.00084814
Iteration 5/25 | Loss: 0.00084417
Iteration 6/25 | Loss: 0.00084285
Iteration 7/25 | Loss: 0.00084277
Iteration 8/25 | Loss: 0.00084277
Iteration 9/25 | Loss: 0.00084277
Iteration 10/25 | Loss: 0.00084277
Iteration 11/25 | Loss: 0.00084277
Iteration 12/25 | Loss: 0.00084277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008427685243077576, 0.0008427685243077576, 0.0008427685243077576, 0.0008427685243077576, 0.0008427685243077576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008427685243077576

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.24751806
Iteration 2/25 | Loss: 0.00052027
Iteration 3/25 | Loss: 0.00052026
Iteration 4/25 | Loss: 0.00052026
Iteration 5/25 | Loss: 0.00052026
Iteration 6/25 | Loss: 0.00052026
Iteration 7/25 | Loss: 0.00052026
Iteration 8/25 | Loss: 0.00052026
Iteration 9/25 | Loss: 0.00052026
Iteration 10/25 | Loss: 0.00052026
Iteration 11/25 | Loss: 0.00052026
Iteration 12/25 | Loss: 0.00052026
Iteration 13/25 | Loss: 0.00052026
Iteration 14/25 | Loss: 0.00052026
Iteration 15/25 | Loss: 0.00052026
Iteration 16/25 | Loss: 0.00052026
Iteration 17/25 | Loss: 0.00052026
Iteration 18/25 | Loss: 0.00052026
Iteration 19/25 | Loss: 0.00052026
Iteration 20/25 | Loss: 0.00052026
Iteration 21/25 | Loss: 0.00052026
Iteration 22/25 | Loss: 0.00052026
Iteration 23/25 | Loss: 0.00052026
Iteration 24/25 | Loss: 0.00052026
Iteration 25/25 | Loss: 0.00052026

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052026
Iteration 2/1000 | Loss: 0.00002206
Iteration 3/1000 | Loss: 0.00001455
Iteration 4/1000 | Loss: 0.00001166
Iteration 5/1000 | Loss: 0.00001106
Iteration 6/1000 | Loss: 0.00001069
Iteration 7/1000 | Loss: 0.00001054
Iteration 8/1000 | Loss: 0.00001027
Iteration 9/1000 | Loss: 0.00001019
Iteration 10/1000 | Loss: 0.00001013
Iteration 11/1000 | Loss: 0.00001012
Iteration 12/1000 | Loss: 0.00001012
Iteration 13/1000 | Loss: 0.00001009
Iteration 14/1000 | Loss: 0.00001007
Iteration 15/1000 | Loss: 0.00001007
Iteration 16/1000 | Loss: 0.00001006
Iteration 17/1000 | Loss: 0.00001004
Iteration 18/1000 | Loss: 0.00001003
Iteration 19/1000 | Loss: 0.00000997
Iteration 20/1000 | Loss: 0.00000992
Iteration 21/1000 | Loss: 0.00000992
Iteration 22/1000 | Loss: 0.00000990
Iteration 23/1000 | Loss: 0.00000988
Iteration 24/1000 | Loss: 0.00000988
Iteration 25/1000 | Loss: 0.00000987
Iteration 26/1000 | Loss: 0.00000986
Iteration 27/1000 | Loss: 0.00000985
Iteration 28/1000 | Loss: 0.00000985
Iteration 29/1000 | Loss: 0.00000985
Iteration 30/1000 | Loss: 0.00000985
Iteration 31/1000 | Loss: 0.00000985
Iteration 32/1000 | Loss: 0.00000985
Iteration 33/1000 | Loss: 0.00000985
Iteration 34/1000 | Loss: 0.00000985
Iteration 35/1000 | Loss: 0.00000984
Iteration 36/1000 | Loss: 0.00000983
Iteration 37/1000 | Loss: 0.00000983
Iteration 38/1000 | Loss: 0.00000982
Iteration 39/1000 | Loss: 0.00000982
Iteration 40/1000 | Loss: 0.00000982
Iteration 41/1000 | Loss: 0.00000982
Iteration 42/1000 | Loss: 0.00000982
Iteration 43/1000 | Loss: 0.00000981
Iteration 44/1000 | Loss: 0.00000981
Iteration 45/1000 | Loss: 0.00000980
Iteration 46/1000 | Loss: 0.00000980
Iteration 47/1000 | Loss: 0.00000979
Iteration 48/1000 | Loss: 0.00000979
Iteration 49/1000 | Loss: 0.00000978
Iteration 50/1000 | Loss: 0.00000978
Iteration 51/1000 | Loss: 0.00000977
Iteration 52/1000 | Loss: 0.00000977
Iteration 53/1000 | Loss: 0.00000977
Iteration 54/1000 | Loss: 0.00000976
Iteration 55/1000 | Loss: 0.00000976
Iteration 56/1000 | Loss: 0.00000976
Iteration 57/1000 | Loss: 0.00000976
Iteration 58/1000 | Loss: 0.00000975
Iteration 59/1000 | Loss: 0.00000975
Iteration 60/1000 | Loss: 0.00000975
Iteration 61/1000 | Loss: 0.00000975
Iteration 62/1000 | Loss: 0.00000975
Iteration 63/1000 | Loss: 0.00000974
Iteration 64/1000 | Loss: 0.00000974
Iteration 65/1000 | Loss: 0.00000974
Iteration 66/1000 | Loss: 0.00000973
Iteration 67/1000 | Loss: 0.00000973
Iteration 68/1000 | Loss: 0.00000973
Iteration 69/1000 | Loss: 0.00000973
Iteration 70/1000 | Loss: 0.00000973
Iteration 71/1000 | Loss: 0.00000973
Iteration 72/1000 | Loss: 0.00000973
Iteration 73/1000 | Loss: 0.00000972
Iteration 74/1000 | Loss: 0.00000972
Iteration 75/1000 | Loss: 0.00000972
Iteration 76/1000 | Loss: 0.00000971
Iteration 77/1000 | Loss: 0.00000971
Iteration 78/1000 | Loss: 0.00000971
Iteration 79/1000 | Loss: 0.00000971
Iteration 80/1000 | Loss: 0.00000970
Iteration 81/1000 | Loss: 0.00000970
Iteration 82/1000 | Loss: 0.00000970
Iteration 83/1000 | Loss: 0.00000970
Iteration 84/1000 | Loss: 0.00000969
Iteration 85/1000 | Loss: 0.00000969
Iteration 86/1000 | Loss: 0.00000969
Iteration 87/1000 | Loss: 0.00000969
Iteration 88/1000 | Loss: 0.00000969
Iteration 89/1000 | Loss: 0.00000969
Iteration 90/1000 | Loss: 0.00000968
Iteration 91/1000 | Loss: 0.00000968
Iteration 92/1000 | Loss: 0.00000968
Iteration 93/1000 | Loss: 0.00000968
Iteration 94/1000 | Loss: 0.00000968
Iteration 95/1000 | Loss: 0.00000968
Iteration 96/1000 | Loss: 0.00000968
Iteration 97/1000 | Loss: 0.00000968
Iteration 98/1000 | Loss: 0.00000968
Iteration 99/1000 | Loss: 0.00000968
Iteration 100/1000 | Loss: 0.00000968
Iteration 101/1000 | Loss: 0.00000967
Iteration 102/1000 | Loss: 0.00000967
Iteration 103/1000 | Loss: 0.00000967
Iteration 104/1000 | Loss: 0.00000967
Iteration 105/1000 | Loss: 0.00000967
Iteration 106/1000 | Loss: 0.00000967
Iteration 107/1000 | Loss: 0.00000967
Iteration 108/1000 | Loss: 0.00000967
Iteration 109/1000 | Loss: 0.00000967
Iteration 110/1000 | Loss: 0.00000967
Iteration 111/1000 | Loss: 0.00000967
Iteration 112/1000 | Loss: 0.00000967
Iteration 113/1000 | Loss: 0.00000967
Iteration 114/1000 | Loss: 0.00000967
Iteration 115/1000 | Loss: 0.00000967
Iteration 116/1000 | Loss: 0.00000967
Iteration 117/1000 | Loss: 0.00000967
Iteration 118/1000 | Loss: 0.00000967
Iteration 119/1000 | Loss: 0.00000966
Iteration 120/1000 | Loss: 0.00000966
Iteration 121/1000 | Loss: 0.00000966
Iteration 122/1000 | Loss: 0.00000966
Iteration 123/1000 | Loss: 0.00000966
Iteration 124/1000 | Loss: 0.00000966
Iteration 125/1000 | Loss: 0.00000966
Iteration 126/1000 | Loss: 0.00000966
Iteration 127/1000 | Loss: 0.00000965
Iteration 128/1000 | Loss: 0.00000965
Iteration 129/1000 | Loss: 0.00000965
Iteration 130/1000 | Loss: 0.00000965
Iteration 131/1000 | Loss: 0.00000965
Iteration 132/1000 | Loss: 0.00000965
Iteration 133/1000 | Loss: 0.00000965
Iteration 134/1000 | Loss: 0.00000965
Iteration 135/1000 | Loss: 0.00000965
Iteration 136/1000 | Loss: 0.00000965
Iteration 137/1000 | Loss: 0.00000965
Iteration 138/1000 | Loss: 0.00000965
Iteration 139/1000 | Loss: 0.00000965
Iteration 140/1000 | Loss: 0.00000965
Iteration 141/1000 | Loss: 0.00000964
Iteration 142/1000 | Loss: 0.00000964
Iteration 143/1000 | Loss: 0.00000964
Iteration 144/1000 | Loss: 0.00000964
Iteration 145/1000 | Loss: 0.00000964
Iteration 146/1000 | Loss: 0.00000964
Iteration 147/1000 | Loss: 0.00000964
Iteration 148/1000 | Loss: 0.00000964
Iteration 149/1000 | Loss: 0.00000964
Iteration 150/1000 | Loss: 0.00000963
Iteration 151/1000 | Loss: 0.00000963
Iteration 152/1000 | Loss: 0.00000963
Iteration 153/1000 | Loss: 0.00000963
Iteration 154/1000 | Loss: 0.00000963
Iteration 155/1000 | Loss: 0.00000963
Iteration 156/1000 | Loss: 0.00000963
Iteration 157/1000 | Loss: 0.00000963
Iteration 158/1000 | Loss: 0.00000963
Iteration 159/1000 | Loss: 0.00000963
Iteration 160/1000 | Loss: 0.00000963
Iteration 161/1000 | Loss: 0.00000963
Iteration 162/1000 | Loss: 0.00000963
Iteration 163/1000 | Loss: 0.00000962
Iteration 164/1000 | Loss: 0.00000962
Iteration 165/1000 | Loss: 0.00000962
Iteration 166/1000 | Loss: 0.00000962
Iteration 167/1000 | Loss: 0.00000962
Iteration 168/1000 | Loss: 0.00000962
Iteration 169/1000 | Loss: 0.00000962
Iteration 170/1000 | Loss: 0.00000962
Iteration 171/1000 | Loss: 0.00000962
Iteration 172/1000 | Loss: 0.00000962
Iteration 173/1000 | Loss: 0.00000962
Iteration 174/1000 | Loss: 0.00000962
Iteration 175/1000 | Loss: 0.00000962
Iteration 176/1000 | Loss: 0.00000962
Iteration 177/1000 | Loss: 0.00000961
Iteration 178/1000 | Loss: 0.00000961
Iteration 179/1000 | Loss: 0.00000961
Iteration 180/1000 | Loss: 0.00000961
Iteration 181/1000 | Loss: 0.00000961
Iteration 182/1000 | Loss: 0.00000961
Iteration 183/1000 | Loss: 0.00000961
Iteration 184/1000 | Loss: 0.00000961
Iteration 185/1000 | Loss: 0.00000960
Iteration 186/1000 | Loss: 0.00000960
Iteration 187/1000 | Loss: 0.00000960
Iteration 188/1000 | Loss: 0.00000960
Iteration 189/1000 | Loss: 0.00000960
Iteration 190/1000 | Loss: 0.00000960
Iteration 191/1000 | Loss: 0.00000960
Iteration 192/1000 | Loss: 0.00000960
Iteration 193/1000 | Loss: 0.00000959
Iteration 194/1000 | Loss: 0.00000959
Iteration 195/1000 | Loss: 0.00000959
Iteration 196/1000 | Loss: 0.00000959
Iteration 197/1000 | Loss: 0.00000959
Iteration 198/1000 | Loss: 0.00000959
Iteration 199/1000 | Loss: 0.00000959
Iteration 200/1000 | Loss: 0.00000959
Iteration 201/1000 | Loss: 0.00000959
Iteration 202/1000 | Loss: 0.00000959
Iteration 203/1000 | Loss: 0.00000959
Iteration 204/1000 | Loss: 0.00000959
Iteration 205/1000 | Loss: 0.00000959
Iteration 206/1000 | Loss: 0.00000959
Iteration 207/1000 | Loss: 0.00000959
Iteration 208/1000 | Loss: 0.00000959
Iteration 209/1000 | Loss: 0.00000959
Iteration 210/1000 | Loss: 0.00000958
Iteration 211/1000 | Loss: 0.00000958
Iteration 212/1000 | Loss: 0.00000958
Iteration 213/1000 | Loss: 0.00000958
Iteration 214/1000 | Loss: 0.00000958
Iteration 215/1000 | Loss: 0.00000958
Iteration 216/1000 | Loss: 0.00000958
Iteration 217/1000 | Loss: 0.00000958
Iteration 218/1000 | Loss: 0.00000958
Iteration 219/1000 | Loss: 0.00000958
Iteration 220/1000 | Loss: 0.00000958
Iteration 221/1000 | Loss: 0.00000958
Iteration 222/1000 | Loss: 0.00000958
Iteration 223/1000 | Loss: 0.00000958
Iteration 224/1000 | Loss: 0.00000958
Iteration 225/1000 | Loss: 0.00000958
Iteration 226/1000 | Loss: 0.00000958
Iteration 227/1000 | Loss: 0.00000958
Iteration 228/1000 | Loss: 0.00000958
Iteration 229/1000 | Loss: 0.00000958
Iteration 230/1000 | Loss: 0.00000958
Iteration 231/1000 | Loss: 0.00000958
Iteration 232/1000 | Loss: 0.00000958
Iteration 233/1000 | Loss: 0.00000958
Iteration 234/1000 | Loss: 0.00000958
Iteration 235/1000 | Loss: 0.00000958
Iteration 236/1000 | Loss: 0.00000958
Iteration 237/1000 | Loss: 0.00000958
Iteration 238/1000 | Loss: 0.00000958
Iteration 239/1000 | Loss: 0.00000958
Iteration 240/1000 | Loss: 0.00000958
Iteration 241/1000 | Loss: 0.00000958
Iteration 242/1000 | Loss: 0.00000958
Iteration 243/1000 | Loss: 0.00000958
Iteration 244/1000 | Loss: 0.00000958
Iteration 245/1000 | Loss: 0.00000958
Iteration 246/1000 | Loss: 0.00000958
Iteration 247/1000 | Loss: 0.00000958
Iteration 248/1000 | Loss: 0.00000958
Iteration 249/1000 | Loss: 0.00000958
Iteration 250/1000 | Loss: 0.00000958
Iteration 251/1000 | Loss: 0.00000958
Iteration 252/1000 | Loss: 0.00000958
Iteration 253/1000 | Loss: 0.00000958
Iteration 254/1000 | Loss: 0.00000958
Iteration 255/1000 | Loss: 0.00000958
Iteration 256/1000 | Loss: 0.00000958
Iteration 257/1000 | Loss: 0.00000958
Iteration 258/1000 | Loss: 0.00000958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [9.584115105099045e-06, 9.584115105099045e-06, 9.584115105099045e-06, 9.584115105099045e-06, 9.584115105099045e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.584115105099045e-06

Optimization complete. Final v2v error: 2.5760161876678467 mm

Highest mean error: 3.50814151763916 mm for frame 90

Lowest mean error: 2.2307231426239014 mm for frame 15

Saving results

Total time: 37.621031284332275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013884
Iteration 2/25 | Loss: 0.00211035
Iteration 3/25 | Loss: 0.00162008
Iteration 4/25 | Loss: 0.00124871
Iteration 5/25 | Loss: 0.00129932
Iteration 6/25 | Loss: 0.00127199
Iteration 7/25 | Loss: 0.00111700
Iteration 8/25 | Loss: 0.00103417
Iteration 9/25 | Loss: 0.00096484
Iteration 10/25 | Loss: 0.00093420
Iteration 11/25 | Loss: 0.00094132
Iteration 12/25 | Loss: 0.00089645
Iteration 13/25 | Loss: 0.00091149
Iteration 14/25 | Loss: 0.00092601
Iteration 15/25 | Loss: 0.00092660
Iteration 16/25 | Loss: 0.00090117
Iteration 17/25 | Loss: 0.00085931
Iteration 18/25 | Loss: 0.00085574
Iteration 19/25 | Loss: 0.00085248
Iteration 20/25 | Loss: 0.00084808
Iteration 21/25 | Loss: 0.00084944
Iteration 22/25 | Loss: 0.00084606
Iteration 23/25 | Loss: 0.00084600
Iteration 24/25 | Loss: 0.00084600
Iteration 25/25 | Loss: 0.00084600

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49322057
Iteration 2/25 | Loss: 0.00039637
Iteration 3/25 | Loss: 0.00039636
Iteration 4/25 | Loss: 0.00039636
Iteration 5/25 | Loss: 0.00039636
Iteration 6/25 | Loss: 0.00039636
Iteration 7/25 | Loss: 0.00039636
Iteration 8/25 | Loss: 0.00039636
Iteration 9/25 | Loss: 0.00039636
Iteration 10/25 | Loss: 0.00039636
Iteration 11/25 | Loss: 0.00039636
Iteration 12/25 | Loss: 0.00039636
Iteration 13/25 | Loss: 0.00039636
Iteration 14/25 | Loss: 0.00039636
Iteration 15/25 | Loss: 0.00039636
Iteration 16/25 | Loss: 0.00039636
Iteration 17/25 | Loss: 0.00039636
Iteration 18/25 | Loss: 0.00039636
Iteration 19/25 | Loss: 0.00039636
Iteration 20/25 | Loss: 0.00039636
Iteration 21/25 | Loss: 0.00039636
Iteration 22/25 | Loss: 0.00039636
Iteration 23/25 | Loss: 0.00039636
Iteration 24/25 | Loss: 0.00039636
Iteration 25/25 | Loss: 0.00039636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039636
Iteration 2/1000 | Loss: 0.00002955
Iteration 3/1000 | Loss: 0.00001820
Iteration 4/1000 | Loss: 0.00001315
Iteration 5/1000 | Loss: 0.00002211
Iteration 6/1000 | Loss: 0.00001170
Iteration 7/1000 | Loss: 0.00001114
Iteration 8/1000 | Loss: 0.00001085
Iteration 9/1000 | Loss: 0.00001054
Iteration 10/1000 | Loss: 0.00001044
Iteration 11/1000 | Loss: 0.00001030
Iteration 12/1000 | Loss: 0.00001020
Iteration 13/1000 | Loss: 0.00001020
Iteration 14/1000 | Loss: 0.00001018
Iteration 15/1000 | Loss: 0.00001017
Iteration 16/1000 | Loss: 0.00001017
Iteration 17/1000 | Loss: 0.00001013
Iteration 18/1000 | Loss: 0.00001011
Iteration 19/1000 | Loss: 0.00001010
Iteration 20/1000 | Loss: 0.00001010
Iteration 21/1000 | Loss: 0.00001009
Iteration 22/1000 | Loss: 0.00001008
Iteration 23/1000 | Loss: 0.00001008
Iteration 24/1000 | Loss: 0.00001007
Iteration 25/1000 | Loss: 0.00001006
Iteration 26/1000 | Loss: 0.00001006
Iteration 27/1000 | Loss: 0.00001006
Iteration 28/1000 | Loss: 0.00001005
Iteration 29/1000 | Loss: 0.00001005
Iteration 30/1000 | Loss: 0.00001004
Iteration 31/1000 | Loss: 0.00001003
Iteration 32/1000 | Loss: 0.00001002
Iteration 33/1000 | Loss: 0.00001002
Iteration 34/1000 | Loss: 0.00001002
Iteration 35/1000 | Loss: 0.00001002
Iteration 36/1000 | Loss: 0.00001001
Iteration 37/1000 | Loss: 0.00001001
Iteration 38/1000 | Loss: 0.00000999
Iteration 39/1000 | Loss: 0.00000998
Iteration 40/1000 | Loss: 0.00000998
Iteration 41/1000 | Loss: 0.00000998
Iteration 42/1000 | Loss: 0.00000998
Iteration 43/1000 | Loss: 0.00000998
Iteration 44/1000 | Loss: 0.00000998
Iteration 45/1000 | Loss: 0.00000998
Iteration 46/1000 | Loss: 0.00000998
Iteration 47/1000 | Loss: 0.00000998
Iteration 48/1000 | Loss: 0.00000998
Iteration 49/1000 | Loss: 0.00000998
Iteration 50/1000 | Loss: 0.00000997
Iteration 51/1000 | Loss: 0.00000997
Iteration 52/1000 | Loss: 0.00000997
Iteration 53/1000 | Loss: 0.00000997
Iteration 54/1000 | Loss: 0.00000997
Iteration 55/1000 | Loss: 0.00000996
Iteration 56/1000 | Loss: 0.00000996
Iteration 57/1000 | Loss: 0.00000996
Iteration 58/1000 | Loss: 0.00000996
Iteration 59/1000 | Loss: 0.00000995
Iteration 60/1000 | Loss: 0.00000995
Iteration 61/1000 | Loss: 0.00000995
Iteration 62/1000 | Loss: 0.00000995
Iteration 63/1000 | Loss: 0.00000994
Iteration 64/1000 | Loss: 0.00000994
Iteration 65/1000 | Loss: 0.00000994
Iteration 66/1000 | Loss: 0.00000994
Iteration 67/1000 | Loss: 0.00000993
Iteration 68/1000 | Loss: 0.00000993
Iteration 69/1000 | Loss: 0.00000993
Iteration 70/1000 | Loss: 0.00000993
Iteration 71/1000 | Loss: 0.00000993
Iteration 72/1000 | Loss: 0.00000993
Iteration 73/1000 | Loss: 0.00000992
Iteration 74/1000 | Loss: 0.00000992
Iteration 75/1000 | Loss: 0.00000992
Iteration 76/1000 | Loss: 0.00000992
Iteration 77/1000 | Loss: 0.00000992
Iteration 78/1000 | Loss: 0.00000992
Iteration 79/1000 | Loss: 0.00000992
Iteration 80/1000 | Loss: 0.00000992
Iteration 81/1000 | Loss: 0.00000991
Iteration 82/1000 | Loss: 0.00000991
Iteration 83/1000 | Loss: 0.00000991
Iteration 84/1000 | Loss: 0.00000991
Iteration 85/1000 | Loss: 0.00000990
Iteration 86/1000 | Loss: 0.00000990
Iteration 87/1000 | Loss: 0.00000990
Iteration 88/1000 | Loss: 0.00000989
Iteration 89/1000 | Loss: 0.00000989
Iteration 90/1000 | Loss: 0.00000989
Iteration 91/1000 | Loss: 0.00000989
Iteration 92/1000 | Loss: 0.00000989
Iteration 93/1000 | Loss: 0.00000988
Iteration 94/1000 | Loss: 0.00000988
Iteration 95/1000 | Loss: 0.00000988
Iteration 96/1000 | Loss: 0.00000988
Iteration 97/1000 | Loss: 0.00000988
Iteration 98/1000 | Loss: 0.00000988
Iteration 99/1000 | Loss: 0.00000987
Iteration 100/1000 | Loss: 0.00000987
Iteration 101/1000 | Loss: 0.00000987
Iteration 102/1000 | Loss: 0.00000987
Iteration 103/1000 | Loss: 0.00000987
Iteration 104/1000 | Loss: 0.00000987
Iteration 105/1000 | Loss: 0.00000986
Iteration 106/1000 | Loss: 0.00000986
Iteration 107/1000 | Loss: 0.00000986
Iteration 108/1000 | Loss: 0.00000986
Iteration 109/1000 | Loss: 0.00000986
Iteration 110/1000 | Loss: 0.00000986
Iteration 111/1000 | Loss: 0.00000985
Iteration 112/1000 | Loss: 0.00000985
Iteration 113/1000 | Loss: 0.00000984
Iteration 114/1000 | Loss: 0.00000984
Iteration 115/1000 | Loss: 0.00000984
Iteration 116/1000 | Loss: 0.00000984
Iteration 117/1000 | Loss: 0.00000984
Iteration 118/1000 | Loss: 0.00000983
Iteration 119/1000 | Loss: 0.00000983
Iteration 120/1000 | Loss: 0.00000983
Iteration 121/1000 | Loss: 0.00000982
Iteration 122/1000 | Loss: 0.00000982
Iteration 123/1000 | Loss: 0.00000982
Iteration 124/1000 | Loss: 0.00000982
Iteration 125/1000 | Loss: 0.00000981
Iteration 126/1000 | Loss: 0.00000981
Iteration 127/1000 | Loss: 0.00000981
Iteration 128/1000 | Loss: 0.00000981
Iteration 129/1000 | Loss: 0.00000981
Iteration 130/1000 | Loss: 0.00000981
Iteration 131/1000 | Loss: 0.00000981
Iteration 132/1000 | Loss: 0.00000981
Iteration 133/1000 | Loss: 0.00000981
Iteration 134/1000 | Loss: 0.00000981
Iteration 135/1000 | Loss: 0.00000980
Iteration 136/1000 | Loss: 0.00000980
Iteration 137/1000 | Loss: 0.00000980
Iteration 138/1000 | Loss: 0.00000980
Iteration 139/1000 | Loss: 0.00000980
Iteration 140/1000 | Loss: 0.00000980
Iteration 141/1000 | Loss: 0.00000980
Iteration 142/1000 | Loss: 0.00000980
Iteration 143/1000 | Loss: 0.00000980
Iteration 144/1000 | Loss: 0.00000980
Iteration 145/1000 | Loss: 0.00000980
Iteration 146/1000 | Loss: 0.00000980
Iteration 147/1000 | Loss: 0.00000980
Iteration 148/1000 | Loss: 0.00000980
Iteration 149/1000 | Loss: 0.00000980
Iteration 150/1000 | Loss: 0.00000980
Iteration 151/1000 | Loss: 0.00000979
Iteration 152/1000 | Loss: 0.00000979
Iteration 153/1000 | Loss: 0.00000979
Iteration 154/1000 | Loss: 0.00000979
Iteration 155/1000 | Loss: 0.00000979
Iteration 156/1000 | Loss: 0.00000979
Iteration 157/1000 | Loss: 0.00000979
Iteration 158/1000 | Loss: 0.00000979
Iteration 159/1000 | Loss: 0.00000979
Iteration 160/1000 | Loss: 0.00000979
Iteration 161/1000 | Loss: 0.00000979
Iteration 162/1000 | Loss: 0.00000979
Iteration 163/1000 | Loss: 0.00000979
Iteration 164/1000 | Loss: 0.00000979
Iteration 165/1000 | Loss: 0.00000979
Iteration 166/1000 | Loss: 0.00000978
Iteration 167/1000 | Loss: 0.00000978
Iteration 168/1000 | Loss: 0.00000978
Iteration 169/1000 | Loss: 0.00000978
Iteration 170/1000 | Loss: 0.00000978
Iteration 171/1000 | Loss: 0.00000978
Iteration 172/1000 | Loss: 0.00000978
Iteration 173/1000 | Loss: 0.00000978
Iteration 174/1000 | Loss: 0.00000978
Iteration 175/1000 | Loss: 0.00000978
Iteration 176/1000 | Loss: 0.00000978
Iteration 177/1000 | Loss: 0.00000977
Iteration 178/1000 | Loss: 0.00000977
Iteration 179/1000 | Loss: 0.00000977
Iteration 180/1000 | Loss: 0.00000977
Iteration 181/1000 | Loss: 0.00000977
Iteration 182/1000 | Loss: 0.00000977
Iteration 183/1000 | Loss: 0.00000977
Iteration 184/1000 | Loss: 0.00000976
Iteration 185/1000 | Loss: 0.00000976
Iteration 186/1000 | Loss: 0.00000976
Iteration 187/1000 | Loss: 0.00000976
Iteration 188/1000 | Loss: 0.00000976
Iteration 189/1000 | Loss: 0.00000976
Iteration 190/1000 | Loss: 0.00000976
Iteration 191/1000 | Loss: 0.00000976
Iteration 192/1000 | Loss: 0.00000976
Iteration 193/1000 | Loss: 0.00000976
Iteration 194/1000 | Loss: 0.00000975
Iteration 195/1000 | Loss: 0.00000975
Iteration 196/1000 | Loss: 0.00000975
Iteration 197/1000 | Loss: 0.00002127
Iteration 198/1000 | Loss: 0.00000976
Iteration 199/1000 | Loss: 0.00001201
Iteration 200/1000 | Loss: 0.00000985
Iteration 201/1000 | Loss: 0.00000975
Iteration 202/1000 | Loss: 0.00000975
Iteration 203/1000 | Loss: 0.00000975
Iteration 204/1000 | Loss: 0.00000996
Iteration 205/1000 | Loss: 0.00000983
Iteration 206/1000 | Loss: 0.00000975
Iteration 207/1000 | Loss: 0.00000975
Iteration 208/1000 | Loss: 0.00000975
Iteration 209/1000 | Loss: 0.00000975
Iteration 210/1000 | Loss: 0.00000983
Iteration 211/1000 | Loss: 0.00000975
Iteration 212/1000 | Loss: 0.00000975
Iteration 213/1000 | Loss: 0.00000975
Iteration 214/1000 | Loss: 0.00000975
Iteration 215/1000 | Loss: 0.00000975
Iteration 216/1000 | Loss: 0.00000975
Iteration 217/1000 | Loss: 0.00000975
Iteration 218/1000 | Loss: 0.00000975
Iteration 219/1000 | Loss: 0.00000975
Iteration 220/1000 | Loss: 0.00000975
Iteration 221/1000 | Loss: 0.00000975
Iteration 222/1000 | Loss: 0.00000975
Iteration 223/1000 | Loss: 0.00000975
Iteration 224/1000 | Loss: 0.00000975
Iteration 225/1000 | Loss: 0.00000975
Iteration 226/1000 | Loss: 0.00000975
Iteration 227/1000 | Loss: 0.00000975
Iteration 228/1000 | Loss: 0.00000975
Iteration 229/1000 | Loss: 0.00000975
Iteration 230/1000 | Loss: 0.00000975
Iteration 231/1000 | Loss: 0.00000975
Iteration 232/1000 | Loss: 0.00000975
Iteration 233/1000 | Loss: 0.00000975
Iteration 234/1000 | Loss: 0.00000975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [9.748317097546533e-06, 9.748317097546533e-06, 9.748317097546533e-06, 9.748317097546533e-06, 9.748317097546533e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.748317097546533e-06

Optimization complete. Final v2v error: 2.5887038707733154 mm

Highest mean error: 4.318465232849121 mm for frame 79

Lowest mean error: 2.174571990966797 mm for frame 52

Saving results

Total time: 74.40366172790527
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033491
Iteration 2/25 | Loss: 0.00142427
Iteration 3/25 | Loss: 0.00103310
Iteration 4/25 | Loss: 0.00097872
Iteration 5/25 | Loss: 0.00096354
Iteration 6/25 | Loss: 0.00097722
Iteration 7/25 | Loss: 0.00095255
Iteration 8/25 | Loss: 0.00093719
Iteration 9/25 | Loss: 0.00093262
Iteration 10/25 | Loss: 0.00093191
Iteration 11/25 | Loss: 0.00093169
Iteration 12/25 | Loss: 0.00093163
Iteration 13/25 | Loss: 0.00093163
Iteration 14/25 | Loss: 0.00093162
Iteration 15/25 | Loss: 0.00093162
Iteration 16/25 | Loss: 0.00093162
Iteration 17/25 | Loss: 0.00093162
Iteration 18/25 | Loss: 0.00093162
Iteration 19/25 | Loss: 0.00093162
Iteration 20/25 | Loss: 0.00093162
Iteration 21/25 | Loss: 0.00093161
Iteration 22/25 | Loss: 0.00093161
Iteration 23/25 | Loss: 0.00093161
Iteration 24/25 | Loss: 0.00093161
Iteration 25/25 | Loss: 0.00093161

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39356232
Iteration 2/25 | Loss: 0.00050140
Iteration 3/25 | Loss: 0.00050140
Iteration 4/25 | Loss: 0.00050140
Iteration 5/25 | Loss: 0.00050140
Iteration 6/25 | Loss: 0.00050140
Iteration 7/25 | Loss: 0.00050140
Iteration 8/25 | Loss: 0.00050140
Iteration 9/25 | Loss: 0.00050140
Iteration 10/25 | Loss: 0.00050140
Iteration 11/25 | Loss: 0.00050140
Iteration 12/25 | Loss: 0.00050140
Iteration 13/25 | Loss: 0.00050140
Iteration 14/25 | Loss: 0.00050140
Iteration 15/25 | Loss: 0.00050140
Iteration 16/25 | Loss: 0.00050140
Iteration 17/25 | Loss: 0.00050140
Iteration 18/25 | Loss: 0.00050140
Iteration 19/25 | Loss: 0.00050140
Iteration 20/25 | Loss: 0.00050140
Iteration 21/25 | Loss: 0.00050140
Iteration 22/25 | Loss: 0.00050140
Iteration 23/25 | Loss: 0.00050140
Iteration 24/25 | Loss: 0.00050140
Iteration 25/25 | Loss: 0.00050140

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050140
Iteration 2/1000 | Loss: 0.00002617
Iteration 3/1000 | Loss: 0.00001628
Iteration 4/1000 | Loss: 0.00001502
Iteration 5/1000 | Loss: 0.00001468
Iteration 6/1000 | Loss: 0.00001429
Iteration 7/1000 | Loss: 0.00001427
Iteration 8/1000 | Loss: 0.00001427
Iteration 9/1000 | Loss: 0.00001427
Iteration 10/1000 | Loss: 0.00001427
Iteration 11/1000 | Loss: 0.00001427
Iteration 12/1000 | Loss: 0.00001426
Iteration 13/1000 | Loss: 0.00001402
Iteration 14/1000 | Loss: 0.00001382
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001378
Iteration 17/1000 | Loss: 0.00001377
Iteration 18/1000 | Loss: 0.00001377
Iteration 19/1000 | Loss: 0.00001376
Iteration 20/1000 | Loss: 0.00001376
Iteration 21/1000 | Loss: 0.00001376
Iteration 22/1000 | Loss: 0.00001375
Iteration 23/1000 | Loss: 0.00001375
Iteration 24/1000 | Loss: 0.00001369
Iteration 25/1000 | Loss: 0.00001361
Iteration 26/1000 | Loss: 0.00001360
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001352
Iteration 29/1000 | Loss: 0.00001352
Iteration 30/1000 | Loss: 0.00001352
Iteration 31/1000 | Loss: 0.00001351
Iteration 32/1000 | Loss: 0.00001349
Iteration 33/1000 | Loss: 0.00001349
Iteration 34/1000 | Loss: 0.00001348
Iteration 35/1000 | Loss: 0.00001348
Iteration 36/1000 | Loss: 0.00001348
Iteration 37/1000 | Loss: 0.00001348
Iteration 38/1000 | Loss: 0.00001347
Iteration 39/1000 | Loss: 0.00001346
Iteration 40/1000 | Loss: 0.00001345
Iteration 41/1000 | Loss: 0.00001345
Iteration 42/1000 | Loss: 0.00001344
Iteration 43/1000 | Loss: 0.00001344
Iteration 44/1000 | Loss: 0.00001343
Iteration 45/1000 | Loss: 0.00001343
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001340
Iteration 50/1000 | Loss: 0.00001340
Iteration 51/1000 | Loss: 0.00001340
Iteration 52/1000 | Loss: 0.00001340
Iteration 53/1000 | Loss: 0.00001340
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001339
Iteration 56/1000 | Loss: 0.00001339
Iteration 57/1000 | Loss: 0.00001335
Iteration 58/1000 | Loss: 0.00001335
Iteration 59/1000 | Loss: 0.00001335
Iteration 60/1000 | Loss: 0.00001335
Iteration 61/1000 | Loss: 0.00001334
Iteration 62/1000 | Loss: 0.00001334
Iteration 63/1000 | Loss: 0.00001334
Iteration 64/1000 | Loss: 0.00001334
Iteration 65/1000 | Loss: 0.00001333
Iteration 66/1000 | Loss: 0.00001333
Iteration 67/1000 | Loss: 0.00001332
Iteration 68/1000 | Loss: 0.00001331
Iteration 69/1000 | Loss: 0.00001331
Iteration 70/1000 | Loss: 0.00001331
Iteration 71/1000 | Loss: 0.00001331
Iteration 72/1000 | Loss: 0.00001330
Iteration 73/1000 | Loss: 0.00001330
Iteration 74/1000 | Loss: 0.00001330
Iteration 75/1000 | Loss: 0.00001330
Iteration 76/1000 | Loss: 0.00001330
Iteration 77/1000 | Loss: 0.00001330
Iteration 78/1000 | Loss: 0.00001330
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001329
Iteration 87/1000 | Loss: 0.00001329
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001329
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001328
Iteration 93/1000 | Loss: 0.00001328
Iteration 94/1000 | Loss: 0.00001328
Iteration 95/1000 | Loss: 0.00001328
Iteration 96/1000 | Loss: 0.00001327
Iteration 97/1000 | Loss: 0.00001327
Iteration 98/1000 | Loss: 0.00001326
Iteration 99/1000 | Loss: 0.00001326
Iteration 100/1000 | Loss: 0.00001326
Iteration 101/1000 | Loss: 0.00001326
Iteration 102/1000 | Loss: 0.00001325
Iteration 103/1000 | Loss: 0.00001325
Iteration 104/1000 | Loss: 0.00001325
Iteration 105/1000 | Loss: 0.00001325
Iteration 106/1000 | Loss: 0.00001325
Iteration 107/1000 | Loss: 0.00001324
Iteration 108/1000 | Loss: 0.00001324
Iteration 109/1000 | Loss: 0.00001324
Iteration 110/1000 | Loss: 0.00001324
Iteration 111/1000 | Loss: 0.00001324
Iteration 112/1000 | Loss: 0.00001324
Iteration 113/1000 | Loss: 0.00001324
Iteration 114/1000 | Loss: 0.00001324
Iteration 115/1000 | Loss: 0.00001324
Iteration 116/1000 | Loss: 0.00001324
Iteration 117/1000 | Loss: 0.00001323
Iteration 118/1000 | Loss: 0.00001323
Iteration 119/1000 | Loss: 0.00001323
Iteration 120/1000 | Loss: 0.00001323
Iteration 121/1000 | Loss: 0.00001323
Iteration 122/1000 | Loss: 0.00001323
Iteration 123/1000 | Loss: 0.00001323
Iteration 124/1000 | Loss: 0.00001323
Iteration 125/1000 | Loss: 0.00001323
Iteration 126/1000 | Loss: 0.00001323
Iteration 127/1000 | Loss: 0.00001323
Iteration 128/1000 | Loss: 0.00001323
Iteration 129/1000 | Loss: 0.00001323
Iteration 130/1000 | Loss: 0.00001322
Iteration 131/1000 | Loss: 0.00001322
Iteration 132/1000 | Loss: 0.00001322
Iteration 133/1000 | Loss: 0.00001322
Iteration 134/1000 | Loss: 0.00001322
Iteration 135/1000 | Loss: 0.00001322
Iteration 136/1000 | Loss: 0.00001322
Iteration 137/1000 | Loss: 0.00001321
Iteration 138/1000 | Loss: 0.00001321
Iteration 139/1000 | Loss: 0.00001321
Iteration 140/1000 | Loss: 0.00001321
Iteration 141/1000 | Loss: 0.00001321
Iteration 142/1000 | Loss: 0.00001321
Iteration 143/1000 | Loss: 0.00001321
Iteration 144/1000 | Loss: 0.00001321
Iteration 145/1000 | Loss: 0.00001321
Iteration 146/1000 | Loss: 0.00001321
Iteration 147/1000 | Loss: 0.00001321
Iteration 148/1000 | Loss: 0.00001321
Iteration 149/1000 | Loss: 0.00001321
Iteration 150/1000 | Loss: 0.00001321
Iteration 151/1000 | Loss: 0.00001321
Iteration 152/1000 | Loss: 0.00001321
Iteration 153/1000 | Loss: 0.00001321
Iteration 154/1000 | Loss: 0.00001321
Iteration 155/1000 | Loss: 0.00001321
Iteration 156/1000 | Loss: 0.00001321
Iteration 157/1000 | Loss: 0.00001321
Iteration 158/1000 | Loss: 0.00001321
Iteration 159/1000 | Loss: 0.00001321
Iteration 160/1000 | Loss: 0.00001321
Iteration 161/1000 | Loss: 0.00001321
Iteration 162/1000 | Loss: 0.00001321
Iteration 163/1000 | Loss: 0.00001321
Iteration 164/1000 | Loss: 0.00001321
Iteration 165/1000 | Loss: 0.00001321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.320967021456454e-05, 1.320967021456454e-05, 1.320967021456454e-05, 1.320967021456454e-05, 1.320967021456454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.320967021456454e-05

Optimization complete. Final v2v error: 3.13857364654541 mm

Highest mean error: 3.199284315109253 mm for frame 25

Lowest mean error: 3.0904855728149414 mm for frame 94

Saving results

Total time: 42.439714670181274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394828
Iteration 2/25 | Loss: 0.00104123
Iteration 3/25 | Loss: 0.00094347
Iteration 4/25 | Loss: 0.00091600
Iteration 5/25 | Loss: 0.00090600
Iteration 6/25 | Loss: 0.00090288
Iteration 7/25 | Loss: 0.00090288
Iteration 8/25 | Loss: 0.00090288
Iteration 9/25 | Loss: 0.00090288
Iteration 10/25 | Loss: 0.00090288
Iteration 11/25 | Loss: 0.00090288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009028841159306467, 0.0009028841159306467, 0.0009028841159306467, 0.0009028841159306467, 0.0009028841159306467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009028841159306467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.68976068
Iteration 2/25 | Loss: 0.00052078
Iteration 3/25 | Loss: 0.00052077
Iteration 4/25 | Loss: 0.00052077
Iteration 5/25 | Loss: 0.00052077
Iteration 6/25 | Loss: 0.00052077
Iteration 7/25 | Loss: 0.00052077
Iteration 8/25 | Loss: 0.00052077
Iteration 9/25 | Loss: 0.00052077
Iteration 10/25 | Loss: 0.00052077
Iteration 11/25 | Loss: 0.00052077
Iteration 12/25 | Loss: 0.00052077
Iteration 13/25 | Loss: 0.00052077
Iteration 14/25 | Loss: 0.00052077
Iteration 15/25 | Loss: 0.00052077
Iteration 16/25 | Loss: 0.00052077
Iteration 17/25 | Loss: 0.00052077
Iteration 18/25 | Loss: 0.00052077
Iteration 19/25 | Loss: 0.00052077
Iteration 20/25 | Loss: 0.00052077
Iteration 21/25 | Loss: 0.00052077
Iteration 22/25 | Loss: 0.00052077
Iteration 23/25 | Loss: 0.00052077
Iteration 24/25 | Loss: 0.00052077
Iteration 25/25 | Loss: 0.00052077

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052077
Iteration 2/1000 | Loss: 0.00002790
Iteration 3/1000 | Loss: 0.00002202
Iteration 4/1000 | Loss: 0.00002095
Iteration 5/1000 | Loss: 0.00002038
Iteration 6/1000 | Loss: 0.00001995
Iteration 7/1000 | Loss: 0.00001968
Iteration 8/1000 | Loss: 0.00001946
Iteration 9/1000 | Loss: 0.00001922
Iteration 10/1000 | Loss: 0.00001909
Iteration 11/1000 | Loss: 0.00001906
Iteration 12/1000 | Loss: 0.00001903
Iteration 13/1000 | Loss: 0.00001895
Iteration 14/1000 | Loss: 0.00001894
Iteration 15/1000 | Loss: 0.00001894
Iteration 16/1000 | Loss: 0.00001893
Iteration 17/1000 | Loss: 0.00001893
Iteration 18/1000 | Loss: 0.00001892
Iteration 19/1000 | Loss: 0.00001890
Iteration 20/1000 | Loss: 0.00001890
Iteration 21/1000 | Loss: 0.00001889
Iteration 22/1000 | Loss: 0.00001889
Iteration 23/1000 | Loss: 0.00001887
Iteration 24/1000 | Loss: 0.00001886
Iteration 25/1000 | Loss: 0.00001886
Iteration 26/1000 | Loss: 0.00001884
Iteration 27/1000 | Loss: 0.00001884
Iteration 28/1000 | Loss: 0.00001883
Iteration 29/1000 | Loss: 0.00001883
Iteration 30/1000 | Loss: 0.00001883
Iteration 31/1000 | Loss: 0.00001883
Iteration 32/1000 | Loss: 0.00001882
Iteration 33/1000 | Loss: 0.00001882
Iteration 34/1000 | Loss: 0.00001882
Iteration 35/1000 | Loss: 0.00001882
Iteration 36/1000 | Loss: 0.00001882
Iteration 37/1000 | Loss: 0.00001882
Iteration 38/1000 | Loss: 0.00001882
Iteration 39/1000 | Loss: 0.00001882
Iteration 40/1000 | Loss: 0.00001882
Iteration 41/1000 | Loss: 0.00001882
Iteration 42/1000 | Loss: 0.00001882
Iteration 43/1000 | Loss: 0.00001881
Iteration 44/1000 | Loss: 0.00001881
Iteration 45/1000 | Loss: 0.00001881
Iteration 46/1000 | Loss: 0.00001881
Iteration 47/1000 | Loss: 0.00001881
Iteration 48/1000 | Loss: 0.00001881
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001881
Iteration 52/1000 | Loss: 0.00001881
Iteration 53/1000 | Loss: 0.00001881
Iteration 54/1000 | Loss: 0.00001881
Iteration 55/1000 | Loss: 0.00001881
Iteration 56/1000 | Loss: 0.00001880
Iteration 57/1000 | Loss: 0.00001880
Iteration 58/1000 | Loss: 0.00001879
Iteration 59/1000 | Loss: 0.00001879
Iteration 60/1000 | Loss: 0.00001879
Iteration 61/1000 | Loss: 0.00001879
Iteration 62/1000 | Loss: 0.00001879
Iteration 63/1000 | Loss: 0.00001879
Iteration 64/1000 | Loss: 0.00001879
Iteration 65/1000 | Loss: 0.00001879
Iteration 66/1000 | Loss: 0.00001879
Iteration 67/1000 | Loss: 0.00001879
Iteration 68/1000 | Loss: 0.00001879
Iteration 69/1000 | Loss: 0.00001879
Iteration 70/1000 | Loss: 0.00001879
Iteration 71/1000 | Loss: 0.00001878
Iteration 72/1000 | Loss: 0.00001878
Iteration 73/1000 | Loss: 0.00001878
Iteration 74/1000 | Loss: 0.00001878
Iteration 75/1000 | Loss: 0.00001877
Iteration 76/1000 | Loss: 0.00001877
Iteration 77/1000 | Loss: 0.00001877
Iteration 78/1000 | Loss: 0.00001876
Iteration 79/1000 | Loss: 0.00001876
Iteration 80/1000 | Loss: 0.00001876
Iteration 81/1000 | Loss: 0.00001876
Iteration 82/1000 | Loss: 0.00001875
Iteration 83/1000 | Loss: 0.00001875
Iteration 84/1000 | Loss: 0.00001875
Iteration 85/1000 | Loss: 0.00001875
Iteration 86/1000 | Loss: 0.00001875
Iteration 87/1000 | Loss: 0.00001874
Iteration 88/1000 | Loss: 0.00001874
Iteration 89/1000 | Loss: 0.00001874
Iteration 90/1000 | Loss: 0.00001874
Iteration 91/1000 | Loss: 0.00001874
Iteration 92/1000 | Loss: 0.00001874
Iteration 93/1000 | Loss: 0.00001873
Iteration 94/1000 | Loss: 0.00001873
Iteration 95/1000 | Loss: 0.00001873
Iteration 96/1000 | Loss: 0.00001873
Iteration 97/1000 | Loss: 0.00001873
Iteration 98/1000 | Loss: 0.00001873
Iteration 99/1000 | Loss: 0.00001872
Iteration 100/1000 | Loss: 0.00001872
Iteration 101/1000 | Loss: 0.00001872
Iteration 102/1000 | Loss: 0.00001872
Iteration 103/1000 | Loss: 0.00001872
Iteration 104/1000 | Loss: 0.00001872
Iteration 105/1000 | Loss: 0.00001872
Iteration 106/1000 | Loss: 0.00001872
Iteration 107/1000 | Loss: 0.00001872
Iteration 108/1000 | Loss: 0.00001871
Iteration 109/1000 | Loss: 0.00001871
Iteration 110/1000 | Loss: 0.00001871
Iteration 111/1000 | Loss: 0.00001871
Iteration 112/1000 | Loss: 0.00001870
Iteration 113/1000 | Loss: 0.00001870
Iteration 114/1000 | Loss: 0.00001870
Iteration 115/1000 | Loss: 0.00001869
Iteration 116/1000 | Loss: 0.00001869
Iteration 117/1000 | Loss: 0.00001869
Iteration 118/1000 | Loss: 0.00001869
Iteration 119/1000 | Loss: 0.00001868
Iteration 120/1000 | Loss: 0.00001868
Iteration 121/1000 | Loss: 0.00001868
Iteration 122/1000 | Loss: 0.00001868
Iteration 123/1000 | Loss: 0.00001868
Iteration 124/1000 | Loss: 0.00001868
Iteration 125/1000 | Loss: 0.00001868
Iteration 126/1000 | Loss: 0.00001868
Iteration 127/1000 | Loss: 0.00001867
Iteration 128/1000 | Loss: 0.00001867
Iteration 129/1000 | Loss: 0.00001867
Iteration 130/1000 | Loss: 0.00001867
Iteration 131/1000 | Loss: 0.00001866
Iteration 132/1000 | Loss: 0.00001866
Iteration 133/1000 | Loss: 0.00001866
Iteration 134/1000 | Loss: 0.00001866
Iteration 135/1000 | Loss: 0.00001866
Iteration 136/1000 | Loss: 0.00001866
Iteration 137/1000 | Loss: 0.00001866
Iteration 138/1000 | Loss: 0.00001866
Iteration 139/1000 | Loss: 0.00001866
Iteration 140/1000 | Loss: 0.00001866
Iteration 141/1000 | Loss: 0.00001866
Iteration 142/1000 | Loss: 0.00001866
Iteration 143/1000 | Loss: 0.00001865
Iteration 144/1000 | Loss: 0.00001865
Iteration 145/1000 | Loss: 0.00001865
Iteration 146/1000 | Loss: 0.00001865
Iteration 147/1000 | Loss: 0.00001865
Iteration 148/1000 | Loss: 0.00001864
Iteration 149/1000 | Loss: 0.00001864
Iteration 150/1000 | Loss: 0.00001864
Iteration 151/1000 | Loss: 0.00001864
Iteration 152/1000 | Loss: 0.00001864
Iteration 153/1000 | Loss: 0.00001864
Iteration 154/1000 | Loss: 0.00001864
Iteration 155/1000 | Loss: 0.00001864
Iteration 156/1000 | Loss: 0.00001864
Iteration 157/1000 | Loss: 0.00001864
Iteration 158/1000 | Loss: 0.00001864
Iteration 159/1000 | Loss: 0.00001864
Iteration 160/1000 | Loss: 0.00001864
Iteration 161/1000 | Loss: 0.00001864
Iteration 162/1000 | Loss: 0.00001864
Iteration 163/1000 | Loss: 0.00001864
Iteration 164/1000 | Loss: 0.00001864
Iteration 165/1000 | Loss: 0.00001864
Iteration 166/1000 | Loss: 0.00001864
Iteration 167/1000 | Loss: 0.00001864
Iteration 168/1000 | Loss: 0.00001864
Iteration 169/1000 | Loss: 0.00001864
Iteration 170/1000 | Loss: 0.00001864
Iteration 171/1000 | Loss: 0.00001864
Iteration 172/1000 | Loss: 0.00001864
Iteration 173/1000 | Loss: 0.00001864
Iteration 174/1000 | Loss: 0.00001864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.8638966139405966e-05, 1.8638966139405966e-05, 1.8638966139405966e-05, 1.8638966139405966e-05, 1.8638966139405966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8638966139405966e-05

Optimization complete. Final v2v error: 3.6255686283111572 mm

Highest mean error: 3.934993028640747 mm for frame 20

Lowest mean error: 3.3067917823791504 mm for frame 11

Saving results

Total time: 36.61478781700134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924496
Iteration 2/25 | Loss: 0.00145197
Iteration 3/25 | Loss: 0.00101015
Iteration 4/25 | Loss: 0.00096852
Iteration 5/25 | Loss: 0.00095985
Iteration 6/25 | Loss: 0.00095736
Iteration 7/25 | Loss: 0.00095698
Iteration 8/25 | Loss: 0.00095698
Iteration 9/25 | Loss: 0.00095698
Iteration 10/25 | Loss: 0.00095698
Iteration 11/25 | Loss: 0.00095698
Iteration 12/25 | Loss: 0.00095698
Iteration 13/25 | Loss: 0.00095698
Iteration 14/25 | Loss: 0.00095698
Iteration 15/25 | Loss: 0.00095698
Iteration 16/25 | Loss: 0.00095698
Iteration 17/25 | Loss: 0.00095698
Iteration 18/25 | Loss: 0.00095698
Iteration 19/25 | Loss: 0.00095698
Iteration 20/25 | Loss: 0.00095698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009569842368364334, 0.0009569842368364334, 0.0009569842368364334, 0.0009569842368364334, 0.0009569842368364334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009569842368364334

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09558868
Iteration 2/25 | Loss: 0.00050036
Iteration 3/25 | Loss: 0.00050035
Iteration 4/25 | Loss: 0.00050034
Iteration 5/25 | Loss: 0.00050034
Iteration 6/25 | Loss: 0.00050034
Iteration 7/25 | Loss: 0.00050034
Iteration 8/25 | Loss: 0.00050034
Iteration 9/25 | Loss: 0.00050034
Iteration 10/25 | Loss: 0.00050034
Iteration 11/25 | Loss: 0.00050034
Iteration 12/25 | Loss: 0.00050034
Iteration 13/25 | Loss: 0.00050034
Iteration 14/25 | Loss: 0.00050034
Iteration 15/25 | Loss: 0.00050034
Iteration 16/25 | Loss: 0.00050034
Iteration 17/25 | Loss: 0.00050034
Iteration 18/25 | Loss: 0.00050034
Iteration 19/25 | Loss: 0.00050034
Iteration 20/25 | Loss: 0.00050034
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000500342168379575, 0.000500342168379575, 0.000500342168379575, 0.000500342168379575, 0.000500342168379575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000500342168379575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050034
Iteration 2/1000 | Loss: 0.00005321
Iteration 3/1000 | Loss: 0.00003575
Iteration 4/1000 | Loss: 0.00002796
Iteration 5/1000 | Loss: 0.00002630
Iteration 6/1000 | Loss: 0.00002503
Iteration 7/1000 | Loss: 0.00002431
Iteration 8/1000 | Loss: 0.00002362
Iteration 9/1000 | Loss: 0.00002290
Iteration 10/1000 | Loss: 0.00002260
Iteration 11/1000 | Loss: 0.00002232
Iteration 12/1000 | Loss: 0.00002211
Iteration 13/1000 | Loss: 0.00002191
Iteration 14/1000 | Loss: 0.00002171
Iteration 15/1000 | Loss: 0.00002161
Iteration 16/1000 | Loss: 0.00002155
Iteration 17/1000 | Loss: 0.00002153
Iteration 18/1000 | Loss: 0.00002149
Iteration 19/1000 | Loss: 0.00002142
Iteration 20/1000 | Loss: 0.00002126
Iteration 21/1000 | Loss: 0.00002122
Iteration 22/1000 | Loss: 0.00002116
Iteration 23/1000 | Loss: 0.00002116
Iteration 24/1000 | Loss: 0.00002114
Iteration 25/1000 | Loss: 0.00002113
Iteration 26/1000 | Loss: 0.00002109
Iteration 27/1000 | Loss: 0.00002103
Iteration 28/1000 | Loss: 0.00002096
Iteration 29/1000 | Loss: 0.00002093
Iteration 30/1000 | Loss: 0.00002093
Iteration 31/1000 | Loss: 0.00002092
Iteration 32/1000 | Loss: 0.00002092
Iteration 33/1000 | Loss: 0.00002092
Iteration 34/1000 | Loss: 0.00002091
Iteration 35/1000 | Loss: 0.00002090
Iteration 36/1000 | Loss: 0.00002088
Iteration 37/1000 | Loss: 0.00002088
Iteration 38/1000 | Loss: 0.00002088
Iteration 39/1000 | Loss: 0.00002088
Iteration 40/1000 | Loss: 0.00002088
Iteration 41/1000 | Loss: 0.00002088
Iteration 42/1000 | Loss: 0.00002088
Iteration 43/1000 | Loss: 0.00002088
Iteration 44/1000 | Loss: 0.00002087
Iteration 45/1000 | Loss: 0.00002087
Iteration 46/1000 | Loss: 0.00002087
Iteration 47/1000 | Loss: 0.00002086
Iteration 48/1000 | Loss: 0.00002086
Iteration 49/1000 | Loss: 0.00002085
Iteration 50/1000 | Loss: 0.00002085
Iteration 51/1000 | Loss: 0.00002085
Iteration 52/1000 | Loss: 0.00002085
Iteration 53/1000 | Loss: 0.00002084
Iteration 54/1000 | Loss: 0.00002084
Iteration 55/1000 | Loss: 0.00002084
Iteration 56/1000 | Loss: 0.00002084
Iteration 57/1000 | Loss: 0.00002084
Iteration 58/1000 | Loss: 0.00002083
Iteration 59/1000 | Loss: 0.00002083
Iteration 60/1000 | Loss: 0.00002083
Iteration 61/1000 | Loss: 0.00002083
Iteration 62/1000 | Loss: 0.00002082
Iteration 63/1000 | Loss: 0.00002082
Iteration 64/1000 | Loss: 0.00002082
Iteration 65/1000 | Loss: 0.00002081
Iteration 66/1000 | Loss: 0.00002081
Iteration 67/1000 | Loss: 0.00002081
Iteration 68/1000 | Loss: 0.00002081
Iteration 69/1000 | Loss: 0.00002081
Iteration 70/1000 | Loss: 0.00002081
Iteration 71/1000 | Loss: 0.00002081
Iteration 72/1000 | Loss: 0.00002081
Iteration 73/1000 | Loss: 0.00002080
Iteration 74/1000 | Loss: 0.00002080
Iteration 75/1000 | Loss: 0.00002080
Iteration 76/1000 | Loss: 0.00002080
Iteration 77/1000 | Loss: 0.00002080
Iteration 78/1000 | Loss: 0.00002080
Iteration 79/1000 | Loss: 0.00002080
Iteration 80/1000 | Loss: 0.00002080
Iteration 81/1000 | Loss: 0.00002080
Iteration 82/1000 | Loss: 0.00002080
Iteration 83/1000 | Loss: 0.00002080
Iteration 84/1000 | Loss: 0.00002080
Iteration 85/1000 | Loss: 0.00002080
Iteration 86/1000 | Loss: 0.00002079
Iteration 87/1000 | Loss: 0.00002079
Iteration 88/1000 | Loss: 0.00002079
Iteration 89/1000 | Loss: 0.00002079
Iteration 90/1000 | Loss: 0.00002079
Iteration 91/1000 | Loss: 0.00002079
Iteration 92/1000 | Loss: 0.00002079
Iteration 93/1000 | Loss: 0.00002079
Iteration 94/1000 | Loss: 0.00002079
Iteration 95/1000 | Loss: 0.00002079
Iteration 96/1000 | Loss: 0.00002079
Iteration 97/1000 | Loss: 0.00002078
Iteration 98/1000 | Loss: 0.00002078
Iteration 99/1000 | Loss: 0.00002078
Iteration 100/1000 | Loss: 0.00002078
Iteration 101/1000 | Loss: 0.00002078
Iteration 102/1000 | Loss: 0.00002078
Iteration 103/1000 | Loss: 0.00002078
Iteration 104/1000 | Loss: 0.00002077
Iteration 105/1000 | Loss: 0.00002077
Iteration 106/1000 | Loss: 0.00002077
Iteration 107/1000 | Loss: 0.00002077
Iteration 108/1000 | Loss: 0.00002077
Iteration 109/1000 | Loss: 0.00002077
Iteration 110/1000 | Loss: 0.00002077
Iteration 111/1000 | Loss: 0.00002076
Iteration 112/1000 | Loss: 0.00002076
Iteration 113/1000 | Loss: 0.00002076
Iteration 114/1000 | Loss: 0.00002076
Iteration 115/1000 | Loss: 0.00002076
Iteration 116/1000 | Loss: 0.00002076
Iteration 117/1000 | Loss: 0.00002076
Iteration 118/1000 | Loss: 0.00002076
Iteration 119/1000 | Loss: 0.00002076
Iteration 120/1000 | Loss: 0.00002076
Iteration 121/1000 | Loss: 0.00002076
Iteration 122/1000 | Loss: 0.00002075
Iteration 123/1000 | Loss: 0.00002075
Iteration 124/1000 | Loss: 0.00002075
Iteration 125/1000 | Loss: 0.00002075
Iteration 126/1000 | Loss: 0.00002075
Iteration 127/1000 | Loss: 0.00002075
Iteration 128/1000 | Loss: 0.00002075
Iteration 129/1000 | Loss: 0.00002075
Iteration 130/1000 | Loss: 0.00002075
Iteration 131/1000 | Loss: 0.00002075
Iteration 132/1000 | Loss: 0.00002075
Iteration 133/1000 | Loss: 0.00002075
Iteration 134/1000 | Loss: 0.00002075
Iteration 135/1000 | Loss: 0.00002074
Iteration 136/1000 | Loss: 0.00002074
Iteration 137/1000 | Loss: 0.00002074
Iteration 138/1000 | Loss: 0.00002074
Iteration 139/1000 | Loss: 0.00002074
Iteration 140/1000 | Loss: 0.00002074
Iteration 141/1000 | Loss: 0.00002074
Iteration 142/1000 | Loss: 0.00002074
Iteration 143/1000 | Loss: 0.00002074
Iteration 144/1000 | Loss: 0.00002074
Iteration 145/1000 | Loss: 0.00002074
Iteration 146/1000 | Loss: 0.00002074
Iteration 147/1000 | Loss: 0.00002073
Iteration 148/1000 | Loss: 0.00002073
Iteration 149/1000 | Loss: 0.00002073
Iteration 150/1000 | Loss: 0.00002073
Iteration 151/1000 | Loss: 0.00002073
Iteration 152/1000 | Loss: 0.00002073
Iteration 153/1000 | Loss: 0.00002073
Iteration 154/1000 | Loss: 0.00002073
Iteration 155/1000 | Loss: 0.00002073
Iteration 156/1000 | Loss: 0.00002073
Iteration 157/1000 | Loss: 0.00002073
Iteration 158/1000 | Loss: 0.00002073
Iteration 159/1000 | Loss: 0.00002073
Iteration 160/1000 | Loss: 0.00002073
Iteration 161/1000 | Loss: 0.00002073
Iteration 162/1000 | Loss: 0.00002073
Iteration 163/1000 | Loss: 0.00002073
Iteration 164/1000 | Loss: 0.00002073
Iteration 165/1000 | Loss: 0.00002073
Iteration 166/1000 | Loss: 0.00002073
Iteration 167/1000 | Loss: 0.00002072
Iteration 168/1000 | Loss: 0.00002072
Iteration 169/1000 | Loss: 0.00002072
Iteration 170/1000 | Loss: 0.00002072
Iteration 171/1000 | Loss: 0.00002072
Iteration 172/1000 | Loss: 0.00002072
Iteration 173/1000 | Loss: 0.00002072
Iteration 174/1000 | Loss: 0.00002072
Iteration 175/1000 | Loss: 0.00002072
Iteration 176/1000 | Loss: 0.00002072
Iteration 177/1000 | Loss: 0.00002072
Iteration 178/1000 | Loss: 0.00002072
Iteration 179/1000 | Loss: 0.00002072
Iteration 180/1000 | Loss: 0.00002072
Iteration 181/1000 | Loss: 0.00002072
Iteration 182/1000 | Loss: 0.00002072
Iteration 183/1000 | Loss: 0.00002072
Iteration 184/1000 | Loss: 0.00002072
Iteration 185/1000 | Loss: 0.00002072
Iteration 186/1000 | Loss: 0.00002072
Iteration 187/1000 | Loss: 0.00002072
Iteration 188/1000 | Loss: 0.00002072
Iteration 189/1000 | Loss: 0.00002072
Iteration 190/1000 | Loss: 0.00002072
Iteration 191/1000 | Loss: 0.00002072
Iteration 192/1000 | Loss: 0.00002072
Iteration 193/1000 | Loss: 0.00002072
Iteration 194/1000 | Loss: 0.00002072
Iteration 195/1000 | Loss: 0.00002072
Iteration 196/1000 | Loss: 0.00002072
Iteration 197/1000 | Loss: 0.00002072
Iteration 198/1000 | Loss: 0.00002072
Iteration 199/1000 | Loss: 0.00002072
Iteration 200/1000 | Loss: 0.00002072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.0717736333608627e-05, 2.0717736333608627e-05, 2.0717736333608627e-05, 2.0717736333608627e-05, 2.0717736333608627e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0717736333608627e-05

Optimization complete. Final v2v error: 3.6877634525299072 mm

Highest mean error: 5.095466613769531 mm for frame 103

Lowest mean error: 2.867753505706787 mm for frame 130

Saving results

Total time: 49.662930727005005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849305
Iteration 2/25 | Loss: 0.00128895
Iteration 3/25 | Loss: 0.00095051
Iteration 4/25 | Loss: 0.00090800
Iteration 5/25 | Loss: 0.00090088
Iteration 6/25 | Loss: 0.00091446
Iteration 7/25 | Loss: 0.00089063
Iteration 8/25 | Loss: 0.00088386
Iteration 9/25 | Loss: 0.00088196
Iteration 10/25 | Loss: 0.00088166
Iteration 11/25 | Loss: 0.00088166
Iteration 12/25 | Loss: 0.00088166
Iteration 13/25 | Loss: 0.00088166
Iteration 14/25 | Loss: 0.00088166
Iteration 15/25 | Loss: 0.00088166
Iteration 16/25 | Loss: 0.00088166
Iteration 17/25 | Loss: 0.00088166
Iteration 18/25 | Loss: 0.00088166
Iteration 19/25 | Loss: 0.00088166
Iteration 20/25 | Loss: 0.00088166
Iteration 21/25 | Loss: 0.00088166
Iteration 22/25 | Loss: 0.00088166
Iteration 23/25 | Loss: 0.00088166
Iteration 24/25 | Loss: 0.00088166
Iteration 25/25 | Loss: 0.00088166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37187684
Iteration 2/25 | Loss: 0.00026741
Iteration 3/25 | Loss: 0.00026738
Iteration 4/25 | Loss: 0.00026738
Iteration 5/25 | Loss: 0.00026738
Iteration 6/25 | Loss: 0.00026738
Iteration 7/25 | Loss: 0.00026738
Iteration 8/25 | Loss: 0.00026738
Iteration 9/25 | Loss: 0.00026738
Iteration 10/25 | Loss: 0.00026738
Iteration 11/25 | Loss: 0.00026738
Iteration 12/25 | Loss: 0.00026738
Iteration 13/25 | Loss: 0.00026738
Iteration 14/25 | Loss: 0.00026738
Iteration 15/25 | Loss: 0.00026738
Iteration 16/25 | Loss: 0.00026738
Iteration 17/25 | Loss: 0.00026738
Iteration 18/25 | Loss: 0.00026738
Iteration 19/25 | Loss: 0.00026738
Iteration 20/25 | Loss: 0.00026738
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002673765702638775, 0.0002673765702638775, 0.0002673765702638775, 0.0002673765702638775, 0.0002673765702638775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002673765702638775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026738
Iteration 2/1000 | Loss: 0.00002965
Iteration 3/1000 | Loss: 0.00002128
Iteration 4/1000 | Loss: 0.00001941
Iteration 5/1000 | Loss: 0.00001838
Iteration 6/1000 | Loss: 0.00001764
Iteration 7/1000 | Loss: 0.00001721
Iteration 8/1000 | Loss: 0.00001673
Iteration 9/1000 | Loss: 0.00001645
Iteration 10/1000 | Loss: 0.00001623
Iteration 11/1000 | Loss: 0.00001618
Iteration 12/1000 | Loss: 0.00001615
Iteration 13/1000 | Loss: 0.00001614
Iteration 14/1000 | Loss: 0.00001611
Iteration 15/1000 | Loss: 0.00001611
Iteration 16/1000 | Loss: 0.00001601
Iteration 17/1000 | Loss: 0.00001586
Iteration 18/1000 | Loss: 0.00001583
Iteration 19/1000 | Loss: 0.00001578
Iteration 20/1000 | Loss: 0.00001577
Iteration 21/1000 | Loss: 0.00001576
Iteration 22/1000 | Loss: 0.00001575
Iteration 23/1000 | Loss: 0.00001573
Iteration 24/1000 | Loss: 0.00001570
Iteration 25/1000 | Loss: 0.00001567
Iteration 26/1000 | Loss: 0.00001566
Iteration 27/1000 | Loss: 0.00001566
Iteration 28/1000 | Loss: 0.00001565
Iteration 29/1000 | Loss: 0.00001564
Iteration 30/1000 | Loss: 0.00001564
Iteration 31/1000 | Loss: 0.00001563
Iteration 32/1000 | Loss: 0.00001563
Iteration 33/1000 | Loss: 0.00001562
Iteration 34/1000 | Loss: 0.00001562
Iteration 35/1000 | Loss: 0.00001562
Iteration 36/1000 | Loss: 0.00001562
Iteration 37/1000 | Loss: 0.00001561
Iteration 38/1000 | Loss: 0.00001561
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001560
Iteration 41/1000 | Loss: 0.00001560
Iteration 42/1000 | Loss: 0.00001559
Iteration 43/1000 | Loss: 0.00001559
Iteration 44/1000 | Loss: 0.00001558
Iteration 45/1000 | Loss: 0.00001558
Iteration 46/1000 | Loss: 0.00001557
Iteration 47/1000 | Loss: 0.00001557
Iteration 48/1000 | Loss: 0.00001556
Iteration 49/1000 | Loss: 0.00001556
Iteration 50/1000 | Loss: 0.00001555
Iteration 51/1000 | Loss: 0.00001555
Iteration 52/1000 | Loss: 0.00001555
Iteration 53/1000 | Loss: 0.00001554
Iteration 54/1000 | Loss: 0.00001554
Iteration 55/1000 | Loss: 0.00001554
Iteration 56/1000 | Loss: 0.00001554
Iteration 57/1000 | Loss: 0.00001553
Iteration 58/1000 | Loss: 0.00001553
Iteration 59/1000 | Loss: 0.00001553
Iteration 60/1000 | Loss: 0.00001553
Iteration 61/1000 | Loss: 0.00001552
Iteration 62/1000 | Loss: 0.00001552
Iteration 63/1000 | Loss: 0.00001552
Iteration 64/1000 | Loss: 0.00001552
Iteration 65/1000 | Loss: 0.00001552
Iteration 66/1000 | Loss: 0.00001551
Iteration 67/1000 | Loss: 0.00001551
Iteration 68/1000 | Loss: 0.00001551
Iteration 69/1000 | Loss: 0.00001551
Iteration 70/1000 | Loss: 0.00001551
Iteration 71/1000 | Loss: 0.00001551
Iteration 72/1000 | Loss: 0.00001551
Iteration 73/1000 | Loss: 0.00001551
Iteration 74/1000 | Loss: 0.00001550
Iteration 75/1000 | Loss: 0.00001550
Iteration 76/1000 | Loss: 0.00001550
Iteration 77/1000 | Loss: 0.00001550
Iteration 78/1000 | Loss: 0.00001550
Iteration 79/1000 | Loss: 0.00001550
Iteration 80/1000 | Loss: 0.00001549
Iteration 81/1000 | Loss: 0.00001549
Iteration 82/1000 | Loss: 0.00001549
Iteration 83/1000 | Loss: 0.00001549
Iteration 84/1000 | Loss: 0.00001549
Iteration 85/1000 | Loss: 0.00001549
Iteration 86/1000 | Loss: 0.00001549
Iteration 87/1000 | Loss: 0.00001549
Iteration 88/1000 | Loss: 0.00001549
Iteration 89/1000 | Loss: 0.00001549
Iteration 90/1000 | Loss: 0.00001549
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001548
Iteration 93/1000 | Loss: 0.00001548
Iteration 94/1000 | Loss: 0.00001548
Iteration 95/1000 | Loss: 0.00001548
Iteration 96/1000 | Loss: 0.00001548
Iteration 97/1000 | Loss: 0.00001547
Iteration 98/1000 | Loss: 0.00001547
Iteration 99/1000 | Loss: 0.00001547
Iteration 100/1000 | Loss: 0.00001547
Iteration 101/1000 | Loss: 0.00001547
Iteration 102/1000 | Loss: 0.00001547
Iteration 103/1000 | Loss: 0.00001547
Iteration 104/1000 | Loss: 0.00001546
Iteration 105/1000 | Loss: 0.00001546
Iteration 106/1000 | Loss: 0.00001546
Iteration 107/1000 | Loss: 0.00001546
Iteration 108/1000 | Loss: 0.00001545
Iteration 109/1000 | Loss: 0.00001545
Iteration 110/1000 | Loss: 0.00001545
Iteration 111/1000 | Loss: 0.00001545
Iteration 112/1000 | Loss: 0.00001544
Iteration 113/1000 | Loss: 0.00001544
Iteration 114/1000 | Loss: 0.00001544
Iteration 115/1000 | Loss: 0.00001544
Iteration 116/1000 | Loss: 0.00001544
Iteration 117/1000 | Loss: 0.00001544
Iteration 118/1000 | Loss: 0.00001544
Iteration 119/1000 | Loss: 0.00001543
Iteration 120/1000 | Loss: 0.00001543
Iteration 121/1000 | Loss: 0.00001543
Iteration 122/1000 | Loss: 0.00001542
Iteration 123/1000 | Loss: 0.00001542
Iteration 124/1000 | Loss: 0.00001542
Iteration 125/1000 | Loss: 0.00001542
Iteration 126/1000 | Loss: 0.00001542
Iteration 127/1000 | Loss: 0.00001542
Iteration 128/1000 | Loss: 0.00001542
Iteration 129/1000 | Loss: 0.00001541
Iteration 130/1000 | Loss: 0.00001541
Iteration 131/1000 | Loss: 0.00001541
Iteration 132/1000 | Loss: 0.00001541
Iteration 133/1000 | Loss: 0.00001541
Iteration 134/1000 | Loss: 0.00001541
Iteration 135/1000 | Loss: 0.00001541
Iteration 136/1000 | Loss: 0.00001541
Iteration 137/1000 | Loss: 0.00001540
Iteration 138/1000 | Loss: 0.00001540
Iteration 139/1000 | Loss: 0.00001540
Iteration 140/1000 | Loss: 0.00001540
Iteration 141/1000 | Loss: 0.00001540
Iteration 142/1000 | Loss: 0.00001540
Iteration 143/1000 | Loss: 0.00001540
Iteration 144/1000 | Loss: 0.00001540
Iteration 145/1000 | Loss: 0.00001540
Iteration 146/1000 | Loss: 0.00001540
Iteration 147/1000 | Loss: 0.00001539
Iteration 148/1000 | Loss: 0.00001539
Iteration 149/1000 | Loss: 0.00001539
Iteration 150/1000 | Loss: 0.00001539
Iteration 151/1000 | Loss: 0.00001539
Iteration 152/1000 | Loss: 0.00001539
Iteration 153/1000 | Loss: 0.00001539
Iteration 154/1000 | Loss: 0.00001539
Iteration 155/1000 | Loss: 0.00001539
Iteration 156/1000 | Loss: 0.00001539
Iteration 157/1000 | Loss: 0.00001539
Iteration 158/1000 | Loss: 0.00001539
Iteration 159/1000 | Loss: 0.00001538
Iteration 160/1000 | Loss: 0.00001538
Iteration 161/1000 | Loss: 0.00001538
Iteration 162/1000 | Loss: 0.00001538
Iteration 163/1000 | Loss: 0.00001538
Iteration 164/1000 | Loss: 0.00001538
Iteration 165/1000 | Loss: 0.00001538
Iteration 166/1000 | Loss: 0.00001538
Iteration 167/1000 | Loss: 0.00001538
Iteration 168/1000 | Loss: 0.00001538
Iteration 169/1000 | Loss: 0.00001538
Iteration 170/1000 | Loss: 0.00001538
Iteration 171/1000 | Loss: 0.00001538
Iteration 172/1000 | Loss: 0.00001538
Iteration 173/1000 | Loss: 0.00001537
Iteration 174/1000 | Loss: 0.00001537
Iteration 175/1000 | Loss: 0.00001537
Iteration 176/1000 | Loss: 0.00001537
Iteration 177/1000 | Loss: 0.00001537
Iteration 178/1000 | Loss: 0.00001537
Iteration 179/1000 | Loss: 0.00001537
Iteration 180/1000 | Loss: 0.00001537
Iteration 181/1000 | Loss: 0.00001537
Iteration 182/1000 | Loss: 0.00001537
Iteration 183/1000 | Loss: 0.00001537
Iteration 184/1000 | Loss: 0.00001537
Iteration 185/1000 | Loss: 0.00001537
Iteration 186/1000 | Loss: 0.00001537
Iteration 187/1000 | Loss: 0.00001537
Iteration 188/1000 | Loss: 0.00001537
Iteration 189/1000 | Loss: 0.00001537
Iteration 190/1000 | Loss: 0.00001537
Iteration 191/1000 | Loss: 0.00001536
Iteration 192/1000 | Loss: 0.00001536
Iteration 193/1000 | Loss: 0.00001536
Iteration 194/1000 | Loss: 0.00001536
Iteration 195/1000 | Loss: 0.00001536
Iteration 196/1000 | Loss: 0.00001536
Iteration 197/1000 | Loss: 0.00001536
Iteration 198/1000 | Loss: 0.00001536
Iteration 199/1000 | Loss: 0.00001536
Iteration 200/1000 | Loss: 0.00001536
Iteration 201/1000 | Loss: 0.00001536
Iteration 202/1000 | Loss: 0.00001536
Iteration 203/1000 | Loss: 0.00001536
Iteration 204/1000 | Loss: 0.00001536
Iteration 205/1000 | Loss: 0.00001536
Iteration 206/1000 | Loss: 0.00001536
Iteration 207/1000 | Loss: 0.00001535
Iteration 208/1000 | Loss: 0.00001535
Iteration 209/1000 | Loss: 0.00001535
Iteration 210/1000 | Loss: 0.00001535
Iteration 211/1000 | Loss: 0.00001535
Iteration 212/1000 | Loss: 0.00001535
Iteration 213/1000 | Loss: 0.00001535
Iteration 214/1000 | Loss: 0.00001535
Iteration 215/1000 | Loss: 0.00001535
Iteration 216/1000 | Loss: 0.00001535
Iteration 217/1000 | Loss: 0.00001535
Iteration 218/1000 | Loss: 0.00001535
Iteration 219/1000 | Loss: 0.00001535
Iteration 220/1000 | Loss: 0.00001535
Iteration 221/1000 | Loss: 0.00001535
Iteration 222/1000 | Loss: 0.00001535
Iteration 223/1000 | Loss: 0.00001535
Iteration 224/1000 | Loss: 0.00001535
Iteration 225/1000 | Loss: 0.00001535
Iteration 226/1000 | Loss: 0.00001534
Iteration 227/1000 | Loss: 0.00001534
Iteration 228/1000 | Loss: 0.00001534
Iteration 229/1000 | Loss: 0.00001534
Iteration 230/1000 | Loss: 0.00001534
Iteration 231/1000 | Loss: 0.00001534
Iteration 232/1000 | Loss: 0.00001534
Iteration 233/1000 | Loss: 0.00001534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.5344528947025537e-05, 1.5344528947025537e-05, 1.5344528947025537e-05, 1.5344528947025537e-05, 1.5344528947025537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5344528947025537e-05

Optimization complete. Final v2v error: 3.195418119430542 mm

Highest mean error: 4.062678337097168 mm for frame 32

Lowest mean error: 2.6214652061462402 mm for frame 153

Saving results

Total time: 57.890000343322754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858675
Iteration 2/25 | Loss: 0.00274643
Iteration 3/25 | Loss: 0.00173307
Iteration 4/25 | Loss: 0.00137109
Iteration 5/25 | Loss: 0.00122050
Iteration 6/25 | Loss: 0.00117929
Iteration 7/25 | Loss: 0.00114967
Iteration 8/25 | Loss: 0.00112567
Iteration 9/25 | Loss: 0.00112044
Iteration 10/25 | Loss: 0.00112231
Iteration 11/25 | Loss: 0.00111150
Iteration 12/25 | Loss: 0.00110998
Iteration 13/25 | Loss: 0.00110908
Iteration 14/25 | Loss: 0.00110854
Iteration 15/25 | Loss: 0.00110798
Iteration 16/25 | Loss: 0.00110724
Iteration 17/25 | Loss: 0.00110640
Iteration 18/25 | Loss: 0.00110564
Iteration 19/25 | Loss: 0.00110522
Iteration 20/25 | Loss: 0.00110494
Iteration 21/25 | Loss: 0.00110479
Iteration 22/25 | Loss: 0.00110465
Iteration 23/25 | Loss: 0.00110455
Iteration 24/25 | Loss: 0.00110446
Iteration 25/25 | Loss: 0.00110437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.44713068
Iteration 2/25 | Loss: 0.00384417
Iteration 3/25 | Loss: 0.00273724
Iteration 4/25 | Loss: 0.00273724
Iteration 5/25 | Loss: 0.00273723
Iteration 6/25 | Loss: 0.00273723
Iteration 7/25 | Loss: 0.00273723
Iteration 8/25 | Loss: 0.00273723
Iteration 9/25 | Loss: 0.00273723
Iteration 10/25 | Loss: 0.00273723
Iteration 11/25 | Loss: 0.00273723
Iteration 12/25 | Loss: 0.00273723
Iteration 13/25 | Loss: 0.00273723
Iteration 14/25 | Loss: 0.00273723
Iteration 15/25 | Loss: 0.00273723
Iteration 16/25 | Loss: 0.00273723
Iteration 17/25 | Loss: 0.00273723
Iteration 18/25 | Loss: 0.00273723
Iteration 19/25 | Loss: 0.00273723
Iteration 20/25 | Loss: 0.00273723
Iteration 21/25 | Loss: 0.00273723
Iteration 22/25 | Loss: 0.00273723
Iteration 23/25 | Loss: 0.00273723
Iteration 24/25 | Loss: 0.00273723
Iteration 25/25 | Loss: 0.00273723
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.002737232018262148, 0.002737232018262148, 0.002737232018262148, 0.002737232018262148, 0.002737232018262148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002737232018262148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273723
Iteration 2/1000 | Loss: 0.00788261
Iteration 3/1000 | Loss: 0.00231210
Iteration 4/1000 | Loss: 0.00075185
Iteration 5/1000 | Loss: 0.00126571
Iteration 6/1000 | Loss: 0.00237947
Iteration 7/1000 | Loss: 0.00119514
Iteration 8/1000 | Loss: 0.00106505
Iteration 9/1000 | Loss: 0.00143700
Iteration 10/1000 | Loss: 0.00009839
Iteration 11/1000 | Loss: 0.00153038
Iteration 12/1000 | Loss: 0.00009898
Iteration 13/1000 | Loss: 0.00006680
Iteration 14/1000 | Loss: 0.00090143
Iteration 15/1000 | Loss: 0.00083488
Iteration 16/1000 | Loss: 0.00009096
Iteration 17/1000 | Loss: 0.00005024
Iteration 18/1000 | Loss: 0.00003809
Iteration 19/1000 | Loss: 0.00003328
Iteration 20/1000 | Loss: 0.00002989
Iteration 21/1000 | Loss: 0.00089317
Iteration 22/1000 | Loss: 0.00013812
Iteration 23/1000 | Loss: 0.00044590
Iteration 24/1000 | Loss: 0.00002880
Iteration 25/1000 | Loss: 0.00002604
Iteration 26/1000 | Loss: 0.00002454
Iteration 27/1000 | Loss: 0.00002334
Iteration 28/1000 | Loss: 0.00002240
Iteration 29/1000 | Loss: 0.00002177
Iteration 30/1000 | Loss: 0.00012393
Iteration 31/1000 | Loss: 0.00009063
Iteration 32/1000 | Loss: 0.00002374
Iteration 33/1000 | Loss: 0.00002170
Iteration 34/1000 | Loss: 0.00003689
Iteration 35/1000 | Loss: 0.00002068
Iteration 36/1000 | Loss: 0.00002017
Iteration 37/1000 | Loss: 0.00017722
Iteration 38/1000 | Loss: 0.00030012
Iteration 39/1000 | Loss: 0.00034393
Iteration 40/1000 | Loss: 0.00042018
Iteration 41/1000 | Loss: 0.00013944
Iteration 42/1000 | Loss: 0.00005372
Iteration 43/1000 | Loss: 0.00009058
Iteration 44/1000 | Loss: 0.00004836
Iteration 45/1000 | Loss: 0.00004478
Iteration 46/1000 | Loss: 0.00002108
Iteration 47/1000 | Loss: 0.00001870
Iteration 48/1000 | Loss: 0.00001754
Iteration 49/1000 | Loss: 0.00001676
Iteration 50/1000 | Loss: 0.00001622
Iteration 51/1000 | Loss: 0.00001571
Iteration 52/1000 | Loss: 0.00001534
Iteration 53/1000 | Loss: 0.00001502
Iteration 54/1000 | Loss: 0.00001478
Iteration 55/1000 | Loss: 0.00001459
Iteration 56/1000 | Loss: 0.00001451
Iteration 57/1000 | Loss: 0.00037423
Iteration 58/1000 | Loss: 0.00023584
Iteration 59/1000 | Loss: 0.00032690
Iteration 60/1000 | Loss: 0.00038354
Iteration 61/1000 | Loss: 0.00021297
Iteration 62/1000 | Loss: 0.00011972
Iteration 63/1000 | Loss: 0.00071453
Iteration 64/1000 | Loss: 0.00041071
Iteration 65/1000 | Loss: 0.00068559
Iteration 66/1000 | Loss: 0.00004481
Iteration 67/1000 | Loss: 0.00014703
Iteration 68/1000 | Loss: 0.00001582
Iteration 69/1000 | Loss: 0.00001766
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001285
Iteration 72/1000 | Loss: 0.00001262
Iteration 73/1000 | Loss: 0.00001246
Iteration 74/1000 | Loss: 0.00001245
Iteration 75/1000 | Loss: 0.00001239
Iteration 76/1000 | Loss: 0.00001239
Iteration 77/1000 | Loss: 0.00001234
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001231
Iteration 80/1000 | Loss: 0.00001230
Iteration 81/1000 | Loss: 0.00001230
Iteration 82/1000 | Loss: 0.00001222
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001209
Iteration 85/1000 | Loss: 0.00001209
Iteration 86/1000 | Loss: 0.00001207
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001206
Iteration 89/1000 | Loss: 0.00001206
Iteration 90/1000 | Loss: 0.00001206
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001206
Iteration 93/1000 | Loss: 0.00001205
Iteration 94/1000 | Loss: 0.00001205
Iteration 95/1000 | Loss: 0.00001205
Iteration 96/1000 | Loss: 0.00001205
Iteration 97/1000 | Loss: 0.00001204
Iteration 98/1000 | Loss: 0.00001204
Iteration 99/1000 | Loss: 0.00001203
Iteration 100/1000 | Loss: 0.00001202
Iteration 101/1000 | Loss: 0.00001202
Iteration 102/1000 | Loss: 0.00001202
Iteration 103/1000 | Loss: 0.00001202
Iteration 104/1000 | Loss: 0.00001202
Iteration 105/1000 | Loss: 0.00001202
Iteration 106/1000 | Loss: 0.00001201
Iteration 107/1000 | Loss: 0.00001201
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001200
Iteration 111/1000 | Loss: 0.00001199
Iteration 112/1000 | Loss: 0.00001199
Iteration 113/1000 | Loss: 0.00001199
Iteration 114/1000 | Loss: 0.00001199
Iteration 115/1000 | Loss: 0.00001199
Iteration 116/1000 | Loss: 0.00001198
Iteration 117/1000 | Loss: 0.00001198
Iteration 118/1000 | Loss: 0.00001198
Iteration 119/1000 | Loss: 0.00001198
Iteration 120/1000 | Loss: 0.00001197
Iteration 121/1000 | Loss: 0.00001197
Iteration 122/1000 | Loss: 0.00001197
Iteration 123/1000 | Loss: 0.00001197
Iteration 124/1000 | Loss: 0.00001197
Iteration 125/1000 | Loss: 0.00001197
Iteration 126/1000 | Loss: 0.00001197
Iteration 127/1000 | Loss: 0.00001197
Iteration 128/1000 | Loss: 0.00001196
Iteration 129/1000 | Loss: 0.00001196
Iteration 130/1000 | Loss: 0.00001196
Iteration 131/1000 | Loss: 0.00001196
Iteration 132/1000 | Loss: 0.00001196
Iteration 133/1000 | Loss: 0.00001196
Iteration 134/1000 | Loss: 0.00001196
Iteration 135/1000 | Loss: 0.00001196
Iteration 136/1000 | Loss: 0.00001196
Iteration 137/1000 | Loss: 0.00001196
Iteration 138/1000 | Loss: 0.00001195
Iteration 139/1000 | Loss: 0.00001195
Iteration 140/1000 | Loss: 0.00001195
Iteration 141/1000 | Loss: 0.00001195
Iteration 142/1000 | Loss: 0.00001194
Iteration 143/1000 | Loss: 0.00001194
Iteration 144/1000 | Loss: 0.00001194
Iteration 145/1000 | Loss: 0.00001193
Iteration 146/1000 | Loss: 0.00001193
Iteration 147/1000 | Loss: 0.00001193
Iteration 148/1000 | Loss: 0.00001193
Iteration 149/1000 | Loss: 0.00001193
Iteration 150/1000 | Loss: 0.00001193
Iteration 151/1000 | Loss: 0.00001192
Iteration 152/1000 | Loss: 0.00001192
Iteration 153/1000 | Loss: 0.00001192
Iteration 154/1000 | Loss: 0.00001192
Iteration 155/1000 | Loss: 0.00001192
Iteration 156/1000 | Loss: 0.00001191
Iteration 157/1000 | Loss: 0.00001191
Iteration 158/1000 | Loss: 0.00001191
Iteration 159/1000 | Loss: 0.00001191
Iteration 160/1000 | Loss: 0.00001191
Iteration 161/1000 | Loss: 0.00001190
Iteration 162/1000 | Loss: 0.00001190
Iteration 163/1000 | Loss: 0.00001190
Iteration 164/1000 | Loss: 0.00001190
Iteration 165/1000 | Loss: 0.00001190
Iteration 166/1000 | Loss: 0.00001190
Iteration 167/1000 | Loss: 0.00001190
Iteration 168/1000 | Loss: 0.00001190
Iteration 169/1000 | Loss: 0.00001190
Iteration 170/1000 | Loss: 0.00001190
Iteration 171/1000 | Loss: 0.00001190
Iteration 172/1000 | Loss: 0.00001189
Iteration 173/1000 | Loss: 0.00001189
Iteration 174/1000 | Loss: 0.00001189
Iteration 175/1000 | Loss: 0.00001189
Iteration 176/1000 | Loss: 0.00001189
Iteration 177/1000 | Loss: 0.00001189
Iteration 178/1000 | Loss: 0.00001189
Iteration 179/1000 | Loss: 0.00001189
Iteration 180/1000 | Loss: 0.00001189
Iteration 181/1000 | Loss: 0.00001189
Iteration 182/1000 | Loss: 0.00001189
Iteration 183/1000 | Loss: 0.00001189
Iteration 184/1000 | Loss: 0.00001188
Iteration 185/1000 | Loss: 0.00001188
Iteration 186/1000 | Loss: 0.00001188
Iteration 187/1000 | Loss: 0.00001188
Iteration 188/1000 | Loss: 0.00001188
Iteration 189/1000 | Loss: 0.00001188
Iteration 190/1000 | Loss: 0.00001188
Iteration 191/1000 | Loss: 0.00001188
Iteration 192/1000 | Loss: 0.00001188
Iteration 193/1000 | Loss: 0.00001188
Iteration 194/1000 | Loss: 0.00001187
Iteration 195/1000 | Loss: 0.00001187
Iteration 196/1000 | Loss: 0.00001187
Iteration 197/1000 | Loss: 0.00001187
Iteration 198/1000 | Loss: 0.00001187
Iteration 199/1000 | Loss: 0.00001187
Iteration 200/1000 | Loss: 0.00001187
Iteration 201/1000 | Loss: 0.00001187
Iteration 202/1000 | Loss: 0.00001187
Iteration 203/1000 | Loss: 0.00001187
Iteration 204/1000 | Loss: 0.00001187
Iteration 205/1000 | Loss: 0.00001187
Iteration 206/1000 | Loss: 0.00001187
Iteration 207/1000 | Loss: 0.00001187
Iteration 208/1000 | Loss: 0.00001187
Iteration 209/1000 | Loss: 0.00001187
Iteration 210/1000 | Loss: 0.00001187
Iteration 211/1000 | Loss: 0.00001187
Iteration 212/1000 | Loss: 0.00001187
Iteration 213/1000 | Loss: 0.00001187
Iteration 214/1000 | Loss: 0.00001187
Iteration 215/1000 | Loss: 0.00001187
Iteration 216/1000 | Loss: 0.00001187
Iteration 217/1000 | Loss: 0.00001187
Iteration 218/1000 | Loss: 0.00001187
Iteration 219/1000 | Loss: 0.00001187
Iteration 220/1000 | Loss: 0.00001187
Iteration 221/1000 | Loss: 0.00001187
Iteration 222/1000 | Loss: 0.00001187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.1868522960867267e-05, 1.1868522960867267e-05, 1.1868522960867267e-05, 1.1868522960867267e-05, 1.1868522960867267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1868522960867267e-05

Optimization complete. Final v2v error: 2.831054210662842 mm

Highest mean error: 4.3962578773498535 mm for frame 68

Lowest mean error: 2.2332844734191895 mm for frame 153

Saving results

Total time: 169.3413450717926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426783
Iteration 2/25 | Loss: 0.00111098
Iteration 3/25 | Loss: 0.00087719
Iteration 4/25 | Loss: 0.00085317
Iteration 5/25 | Loss: 0.00084929
Iteration 6/25 | Loss: 0.00084827
Iteration 7/25 | Loss: 0.00084816
Iteration 8/25 | Loss: 0.00084816
Iteration 9/25 | Loss: 0.00084816
Iteration 10/25 | Loss: 0.00084816
Iteration 11/25 | Loss: 0.00084816
Iteration 12/25 | Loss: 0.00084816
Iteration 13/25 | Loss: 0.00084816
Iteration 14/25 | Loss: 0.00084816
Iteration 15/25 | Loss: 0.00084816
Iteration 16/25 | Loss: 0.00084816
Iteration 17/25 | Loss: 0.00084816
Iteration 18/25 | Loss: 0.00084816
Iteration 19/25 | Loss: 0.00084816
Iteration 20/25 | Loss: 0.00084816
Iteration 21/25 | Loss: 0.00084816
Iteration 22/25 | Loss: 0.00084816
Iteration 23/25 | Loss: 0.00084816
Iteration 24/25 | Loss: 0.00084816
Iteration 25/25 | Loss: 0.00084816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69529045
Iteration 2/25 | Loss: 0.00319947
Iteration 3/25 | Loss: 0.00319946
Iteration 4/25 | Loss: 0.00319946
Iteration 5/25 | Loss: 0.00319946
Iteration 6/25 | Loss: 0.00319946
Iteration 7/25 | Loss: 0.00319946
Iteration 8/25 | Loss: 0.00319946
Iteration 9/25 | Loss: 0.00319946
Iteration 10/25 | Loss: 0.00319946
Iteration 11/25 | Loss: 0.00319946
Iteration 12/25 | Loss: 0.00319946
Iteration 13/25 | Loss: 0.00319946
Iteration 14/25 | Loss: 0.00319946
Iteration 15/25 | Loss: 0.00319946
Iteration 16/25 | Loss: 0.00319946
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0031994585879147053, 0.0031994585879147053, 0.0031994585879147053, 0.0031994585879147053, 0.0031994585879147053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031994585879147053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00319946
Iteration 2/1000 | Loss: 0.00003164
Iteration 3/1000 | Loss: 0.00001828
Iteration 4/1000 | Loss: 0.00001523
Iteration 5/1000 | Loss: 0.00001400
Iteration 6/1000 | Loss: 0.00001336
Iteration 7/1000 | Loss: 0.00001305
Iteration 8/1000 | Loss: 0.00001275
Iteration 9/1000 | Loss: 0.00001263
Iteration 10/1000 | Loss: 0.00001246
Iteration 11/1000 | Loss: 0.00001245
Iteration 12/1000 | Loss: 0.00001244
Iteration 13/1000 | Loss: 0.00001244
Iteration 14/1000 | Loss: 0.00001239
Iteration 15/1000 | Loss: 0.00001239
Iteration 16/1000 | Loss: 0.00001235
Iteration 17/1000 | Loss: 0.00001227
Iteration 18/1000 | Loss: 0.00001227
Iteration 19/1000 | Loss: 0.00001226
Iteration 20/1000 | Loss: 0.00001225
Iteration 21/1000 | Loss: 0.00001224
Iteration 22/1000 | Loss: 0.00001222
Iteration 23/1000 | Loss: 0.00001221
Iteration 24/1000 | Loss: 0.00001220
Iteration 25/1000 | Loss: 0.00001219
Iteration 26/1000 | Loss: 0.00001218
Iteration 27/1000 | Loss: 0.00001218
Iteration 28/1000 | Loss: 0.00001217
Iteration 29/1000 | Loss: 0.00001215
Iteration 30/1000 | Loss: 0.00001214
Iteration 31/1000 | Loss: 0.00001214
Iteration 32/1000 | Loss: 0.00001214
Iteration 33/1000 | Loss: 0.00001214
Iteration 34/1000 | Loss: 0.00001214
Iteration 35/1000 | Loss: 0.00001214
Iteration 36/1000 | Loss: 0.00001214
Iteration 37/1000 | Loss: 0.00001214
Iteration 38/1000 | Loss: 0.00001214
Iteration 39/1000 | Loss: 0.00001214
Iteration 40/1000 | Loss: 0.00001214
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001214
Iteration 43/1000 | Loss: 0.00001214
Iteration 44/1000 | Loss: 0.00001214
Iteration 45/1000 | Loss: 0.00001214
Iteration 46/1000 | Loss: 0.00001214
Iteration 47/1000 | Loss: 0.00001214
Iteration 48/1000 | Loss: 0.00001214
Iteration 49/1000 | Loss: 0.00001214
Iteration 50/1000 | Loss: 0.00001214
Iteration 51/1000 | Loss: 0.00001214
Iteration 52/1000 | Loss: 0.00001214
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001214
Iteration 56/1000 | Loss: 0.00001214
Iteration 57/1000 | Loss: 0.00001214
Iteration 58/1000 | Loss: 0.00001214
Iteration 59/1000 | Loss: 0.00001214
Iteration 60/1000 | Loss: 0.00001214
Iteration 61/1000 | Loss: 0.00001214
Iteration 62/1000 | Loss: 0.00001214
Iteration 63/1000 | Loss: 0.00001214
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001214
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001214
Iteration 69/1000 | Loss: 0.00001214
Iteration 70/1000 | Loss: 0.00001214
Iteration 71/1000 | Loss: 0.00001214
Iteration 72/1000 | Loss: 0.00001214
Iteration 73/1000 | Loss: 0.00001214
Iteration 74/1000 | Loss: 0.00001214
Iteration 75/1000 | Loss: 0.00001214
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001214
Iteration 78/1000 | Loss: 0.00001214
Iteration 79/1000 | Loss: 0.00001214
Iteration 80/1000 | Loss: 0.00001214
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001214
Iteration 85/1000 | Loss: 0.00001214
Iteration 86/1000 | Loss: 0.00001214
Iteration 87/1000 | Loss: 0.00001214
Iteration 88/1000 | Loss: 0.00001214
Iteration 89/1000 | Loss: 0.00001214
Iteration 90/1000 | Loss: 0.00001214
Iteration 91/1000 | Loss: 0.00001214
Iteration 92/1000 | Loss: 0.00001214
Iteration 93/1000 | Loss: 0.00001214
Iteration 94/1000 | Loss: 0.00001214
Iteration 95/1000 | Loss: 0.00001214
Iteration 96/1000 | Loss: 0.00001214
Iteration 97/1000 | Loss: 0.00001214
Iteration 98/1000 | Loss: 0.00001214
Iteration 99/1000 | Loss: 0.00001214
Iteration 100/1000 | Loss: 0.00001214
Iteration 101/1000 | Loss: 0.00001214
Iteration 102/1000 | Loss: 0.00001214
Iteration 103/1000 | Loss: 0.00001214
Iteration 104/1000 | Loss: 0.00001214
Iteration 105/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.2136596524214838e-05, 1.2136596524214838e-05, 1.2136596524214838e-05, 1.2136596524214838e-05, 1.2136596524214838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2136596524214838e-05

Optimization complete. Final v2v error: 2.9745728969573975 mm

Highest mean error: 3.3397068977355957 mm for frame 114

Lowest mean error: 2.6805167198181152 mm for frame 99

Saving results

Total time: 30.12989068031311
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405392
Iteration 2/25 | Loss: 0.00092029
Iteration 3/25 | Loss: 0.00084064
Iteration 4/25 | Loss: 0.00082529
Iteration 5/25 | Loss: 0.00081967
Iteration 6/25 | Loss: 0.00081899
Iteration 7/25 | Loss: 0.00081899
Iteration 8/25 | Loss: 0.00081899
Iteration 9/25 | Loss: 0.00081899
Iteration 10/25 | Loss: 0.00081899
Iteration 11/25 | Loss: 0.00081899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008189865620806813, 0.0008189865620806813, 0.0008189865620806813, 0.0008189865620806813, 0.0008189865620806813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008189865620806813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66930366
Iteration 2/25 | Loss: 0.00312003
Iteration 3/25 | Loss: 0.00312003
Iteration 4/25 | Loss: 0.00312003
Iteration 5/25 | Loss: 0.00312003
Iteration 6/25 | Loss: 0.00312003
Iteration 7/25 | Loss: 0.00312003
Iteration 8/25 | Loss: 0.00312003
Iteration 9/25 | Loss: 0.00312003
Iteration 10/25 | Loss: 0.00312003
Iteration 11/25 | Loss: 0.00312003
Iteration 12/25 | Loss: 0.00312003
Iteration 13/25 | Loss: 0.00312003
Iteration 14/25 | Loss: 0.00312003
Iteration 15/25 | Loss: 0.00312003
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0031200277153402567, 0.0031200277153402567, 0.0031200277153402567, 0.0031200277153402567, 0.0031200277153402567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031200277153402567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00312003
Iteration 2/1000 | Loss: 0.00002182
Iteration 3/1000 | Loss: 0.00001488
Iteration 4/1000 | Loss: 0.00001288
Iteration 5/1000 | Loss: 0.00001227
Iteration 6/1000 | Loss: 0.00001204
Iteration 7/1000 | Loss: 0.00001204
Iteration 8/1000 | Loss: 0.00001179
Iteration 9/1000 | Loss: 0.00001154
Iteration 10/1000 | Loss: 0.00001141
Iteration 11/1000 | Loss: 0.00001125
Iteration 12/1000 | Loss: 0.00001116
Iteration 13/1000 | Loss: 0.00001116
Iteration 14/1000 | Loss: 0.00001116
Iteration 15/1000 | Loss: 0.00001115
Iteration 16/1000 | Loss: 0.00001114
Iteration 17/1000 | Loss: 0.00001113
Iteration 18/1000 | Loss: 0.00001112
Iteration 19/1000 | Loss: 0.00001109
Iteration 20/1000 | Loss: 0.00001109
Iteration 21/1000 | Loss: 0.00001109
Iteration 22/1000 | Loss: 0.00001109
Iteration 23/1000 | Loss: 0.00001108
Iteration 24/1000 | Loss: 0.00001108
Iteration 25/1000 | Loss: 0.00001108
Iteration 26/1000 | Loss: 0.00001108
Iteration 27/1000 | Loss: 0.00001108
Iteration 28/1000 | Loss: 0.00001107
Iteration 29/1000 | Loss: 0.00001107
Iteration 30/1000 | Loss: 0.00001107
Iteration 31/1000 | Loss: 0.00001107
Iteration 32/1000 | Loss: 0.00001107
Iteration 33/1000 | Loss: 0.00001106
Iteration 34/1000 | Loss: 0.00001106
Iteration 35/1000 | Loss: 0.00001106
Iteration 36/1000 | Loss: 0.00001105
Iteration 37/1000 | Loss: 0.00001105
Iteration 38/1000 | Loss: 0.00001105
Iteration 39/1000 | Loss: 0.00001104
Iteration 40/1000 | Loss: 0.00001104
Iteration 41/1000 | Loss: 0.00001103
Iteration 42/1000 | Loss: 0.00001102
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001102
Iteration 45/1000 | Loss: 0.00001102
Iteration 46/1000 | Loss: 0.00001102
Iteration 47/1000 | Loss: 0.00001102
Iteration 48/1000 | Loss: 0.00001102
Iteration 49/1000 | Loss: 0.00001102
Iteration 50/1000 | Loss: 0.00001102
Iteration 51/1000 | Loss: 0.00001101
Iteration 52/1000 | Loss: 0.00001101
Iteration 53/1000 | Loss: 0.00001101
Iteration 54/1000 | Loss: 0.00001100
Iteration 55/1000 | Loss: 0.00001100
Iteration 56/1000 | Loss: 0.00001100
Iteration 57/1000 | Loss: 0.00001099
Iteration 58/1000 | Loss: 0.00001099
Iteration 59/1000 | Loss: 0.00001099
Iteration 60/1000 | Loss: 0.00001099
Iteration 61/1000 | Loss: 0.00001099
Iteration 62/1000 | Loss: 0.00001099
Iteration 63/1000 | Loss: 0.00001099
Iteration 64/1000 | Loss: 0.00001099
Iteration 65/1000 | Loss: 0.00001099
Iteration 66/1000 | Loss: 0.00001099
Iteration 67/1000 | Loss: 0.00001099
Iteration 68/1000 | Loss: 0.00001099
Iteration 69/1000 | Loss: 0.00001099
Iteration 70/1000 | Loss: 0.00001099
Iteration 71/1000 | Loss: 0.00001099
Iteration 72/1000 | Loss: 0.00001099
Iteration 73/1000 | Loss: 0.00001099
Iteration 74/1000 | Loss: 0.00001099
Iteration 75/1000 | Loss: 0.00001098
Iteration 76/1000 | Loss: 0.00001098
Iteration 77/1000 | Loss: 0.00001098
Iteration 78/1000 | Loss: 0.00001098
Iteration 79/1000 | Loss: 0.00001098
Iteration 80/1000 | Loss: 0.00001098
Iteration 81/1000 | Loss: 0.00001098
Iteration 82/1000 | Loss: 0.00001098
Iteration 83/1000 | Loss: 0.00001097
Iteration 84/1000 | Loss: 0.00001097
Iteration 85/1000 | Loss: 0.00001097
Iteration 86/1000 | Loss: 0.00001097
Iteration 87/1000 | Loss: 0.00001097
Iteration 88/1000 | Loss: 0.00001097
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001096
Iteration 91/1000 | Loss: 0.00001096
Iteration 92/1000 | Loss: 0.00001096
Iteration 93/1000 | Loss: 0.00001096
Iteration 94/1000 | Loss: 0.00001096
Iteration 95/1000 | Loss: 0.00001096
Iteration 96/1000 | Loss: 0.00001096
Iteration 97/1000 | Loss: 0.00001096
Iteration 98/1000 | Loss: 0.00001096
Iteration 99/1000 | Loss: 0.00001096
Iteration 100/1000 | Loss: 0.00001096
Iteration 101/1000 | Loss: 0.00001096
Iteration 102/1000 | Loss: 0.00001096
Iteration 103/1000 | Loss: 0.00001096
Iteration 104/1000 | Loss: 0.00001096
Iteration 105/1000 | Loss: 0.00001096
Iteration 106/1000 | Loss: 0.00001096
Iteration 107/1000 | Loss: 0.00001096
Iteration 108/1000 | Loss: 0.00001096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.0961811312881764e-05, 1.0961811312881764e-05, 1.0961811312881764e-05, 1.0961811312881764e-05, 1.0961811312881764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0961811312881764e-05

Optimization complete. Final v2v error: 2.8411612510681152 mm

Highest mean error: 3.109980821609497 mm for frame 126

Lowest mean error: 2.6606268882751465 mm for frame 182

Saving results

Total time: 31.55120015144348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909675
Iteration 2/25 | Loss: 0.00135997
Iteration 3/25 | Loss: 0.00098440
Iteration 4/25 | Loss: 0.00093965
Iteration 5/25 | Loss: 0.00093274
Iteration 6/25 | Loss: 0.00093231
Iteration 7/25 | Loss: 0.00093231
Iteration 8/25 | Loss: 0.00093231
Iteration 9/25 | Loss: 0.00093231
Iteration 10/25 | Loss: 0.00093231
Iteration 11/25 | Loss: 0.00093231
Iteration 12/25 | Loss: 0.00093231
Iteration 13/25 | Loss: 0.00093231
Iteration 14/25 | Loss: 0.00093231
Iteration 15/25 | Loss: 0.00093231
Iteration 16/25 | Loss: 0.00093231
Iteration 17/25 | Loss: 0.00093231
Iteration 18/25 | Loss: 0.00093231
Iteration 19/25 | Loss: 0.00093231
Iteration 20/25 | Loss: 0.00093231
Iteration 21/25 | Loss: 0.00093231
Iteration 22/25 | Loss: 0.00093231
Iteration 23/25 | Loss: 0.00093231
Iteration 24/25 | Loss: 0.00093231
Iteration 25/25 | Loss: 0.00093231

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.20174897
Iteration 2/25 | Loss: 0.00439578
Iteration 3/25 | Loss: 0.00439576
Iteration 4/25 | Loss: 0.00439576
Iteration 5/25 | Loss: 0.00439576
Iteration 6/25 | Loss: 0.00439576
Iteration 7/25 | Loss: 0.00439576
Iteration 8/25 | Loss: 0.00439576
Iteration 9/25 | Loss: 0.00439576
Iteration 10/25 | Loss: 0.00439576
Iteration 11/25 | Loss: 0.00439576
Iteration 12/25 | Loss: 0.00439576
Iteration 13/25 | Loss: 0.00439576
Iteration 14/25 | Loss: 0.00439576
Iteration 15/25 | Loss: 0.00439576
Iteration 16/25 | Loss: 0.00439576
Iteration 17/25 | Loss: 0.00439576
Iteration 18/25 | Loss: 0.00439576
Iteration 19/25 | Loss: 0.00439576
Iteration 20/25 | Loss: 0.00439576
Iteration 21/25 | Loss: 0.00439576
Iteration 22/25 | Loss: 0.00439576
Iteration 23/25 | Loss: 0.00439576
Iteration 24/25 | Loss: 0.00439576
Iteration 25/25 | Loss: 0.00439576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00439576
Iteration 2/1000 | Loss: 0.00005890
Iteration 3/1000 | Loss: 0.00003018
Iteration 4/1000 | Loss: 0.00002432
Iteration 5/1000 | Loss: 0.00002344
Iteration 6/1000 | Loss: 0.00002244
Iteration 7/1000 | Loss: 0.00002158
Iteration 8/1000 | Loss: 0.00002105
Iteration 9/1000 | Loss: 0.00002053
Iteration 10/1000 | Loss: 0.00002009
Iteration 11/1000 | Loss: 0.00001978
Iteration 12/1000 | Loss: 0.00001958
Iteration 13/1000 | Loss: 0.00001955
Iteration 14/1000 | Loss: 0.00001936
Iteration 15/1000 | Loss: 0.00001933
Iteration 16/1000 | Loss: 0.00001917
Iteration 17/1000 | Loss: 0.00001914
Iteration 18/1000 | Loss: 0.00001907
Iteration 19/1000 | Loss: 0.00001904
Iteration 20/1000 | Loss: 0.00001898
Iteration 21/1000 | Loss: 0.00001897
Iteration 22/1000 | Loss: 0.00001897
Iteration 23/1000 | Loss: 0.00001894
Iteration 24/1000 | Loss: 0.00001891
Iteration 25/1000 | Loss: 0.00001891
Iteration 26/1000 | Loss: 0.00001890
Iteration 27/1000 | Loss: 0.00001890
Iteration 28/1000 | Loss: 0.00001889
Iteration 29/1000 | Loss: 0.00001889
Iteration 30/1000 | Loss: 0.00001883
Iteration 31/1000 | Loss: 0.00001876
Iteration 32/1000 | Loss: 0.00001876
Iteration 33/1000 | Loss: 0.00001874
Iteration 34/1000 | Loss: 0.00001874
Iteration 35/1000 | Loss: 0.00001873
Iteration 36/1000 | Loss: 0.00001873
Iteration 37/1000 | Loss: 0.00001873
Iteration 38/1000 | Loss: 0.00001872
Iteration 39/1000 | Loss: 0.00001871
Iteration 40/1000 | Loss: 0.00001871
Iteration 41/1000 | Loss: 0.00001870
Iteration 42/1000 | Loss: 0.00001870
Iteration 43/1000 | Loss: 0.00001868
Iteration 44/1000 | Loss: 0.00001868
Iteration 45/1000 | Loss: 0.00001868
Iteration 46/1000 | Loss: 0.00001868
Iteration 47/1000 | Loss: 0.00001867
Iteration 48/1000 | Loss: 0.00001867
Iteration 49/1000 | Loss: 0.00001867
Iteration 50/1000 | Loss: 0.00001867
Iteration 51/1000 | Loss: 0.00001867
Iteration 52/1000 | Loss: 0.00001867
Iteration 53/1000 | Loss: 0.00001867
Iteration 54/1000 | Loss: 0.00001867
Iteration 55/1000 | Loss: 0.00001867
Iteration 56/1000 | Loss: 0.00001867
Iteration 57/1000 | Loss: 0.00001867
Iteration 58/1000 | Loss: 0.00001866
Iteration 59/1000 | Loss: 0.00001866
Iteration 60/1000 | Loss: 0.00001866
Iteration 61/1000 | Loss: 0.00001866
Iteration 62/1000 | Loss: 0.00001866
Iteration 63/1000 | Loss: 0.00001866
Iteration 64/1000 | Loss: 0.00001866
Iteration 65/1000 | Loss: 0.00001866
Iteration 66/1000 | Loss: 0.00001866
Iteration 67/1000 | Loss: 0.00001866
Iteration 68/1000 | Loss: 0.00001865
Iteration 69/1000 | Loss: 0.00001865
Iteration 70/1000 | Loss: 0.00001865
Iteration 71/1000 | Loss: 0.00001864
Iteration 72/1000 | Loss: 0.00001864
Iteration 73/1000 | Loss: 0.00001864
Iteration 74/1000 | Loss: 0.00001863
Iteration 75/1000 | Loss: 0.00001863
Iteration 76/1000 | Loss: 0.00001862
Iteration 77/1000 | Loss: 0.00001862
Iteration 78/1000 | Loss: 0.00001861
Iteration 79/1000 | Loss: 0.00001861
Iteration 80/1000 | Loss: 0.00001861
Iteration 81/1000 | Loss: 0.00001860
Iteration 82/1000 | Loss: 0.00001860
Iteration 83/1000 | Loss: 0.00001860
Iteration 84/1000 | Loss: 0.00001860
Iteration 85/1000 | Loss: 0.00001860
Iteration 86/1000 | Loss: 0.00001860
Iteration 87/1000 | Loss: 0.00001859
Iteration 88/1000 | Loss: 0.00001859
Iteration 89/1000 | Loss: 0.00001859
Iteration 90/1000 | Loss: 0.00001859
Iteration 91/1000 | Loss: 0.00001859
Iteration 92/1000 | Loss: 0.00001859
Iteration 93/1000 | Loss: 0.00001859
Iteration 94/1000 | Loss: 0.00001858
Iteration 95/1000 | Loss: 0.00001858
Iteration 96/1000 | Loss: 0.00001858
Iteration 97/1000 | Loss: 0.00001858
Iteration 98/1000 | Loss: 0.00001858
Iteration 99/1000 | Loss: 0.00001858
Iteration 100/1000 | Loss: 0.00001858
Iteration 101/1000 | Loss: 0.00001858
Iteration 102/1000 | Loss: 0.00001858
Iteration 103/1000 | Loss: 0.00001858
Iteration 104/1000 | Loss: 0.00001858
Iteration 105/1000 | Loss: 0.00001857
Iteration 106/1000 | Loss: 0.00001857
Iteration 107/1000 | Loss: 0.00001857
Iteration 108/1000 | Loss: 0.00001857
Iteration 109/1000 | Loss: 0.00001857
Iteration 110/1000 | Loss: 0.00001856
Iteration 111/1000 | Loss: 0.00001856
Iteration 112/1000 | Loss: 0.00001856
Iteration 113/1000 | Loss: 0.00001856
Iteration 114/1000 | Loss: 0.00001855
Iteration 115/1000 | Loss: 0.00001855
Iteration 116/1000 | Loss: 0.00001854
Iteration 117/1000 | Loss: 0.00001854
Iteration 118/1000 | Loss: 0.00001854
Iteration 119/1000 | Loss: 0.00001854
Iteration 120/1000 | Loss: 0.00001854
Iteration 121/1000 | Loss: 0.00001854
Iteration 122/1000 | Loss: 0.00001854
Iteration 123/1000 | Loss: 0.00001853
Iteration 124/1000 | Loss: 0.00001853
Iteration 125/1000 | Loss: 0.00001853
Iteration 126/1000 | Loss: 0.00001853
Iteration 127/1000 | Loss: 0.00001853
Iteration 128/1000 | Loss: 0.00001853
Iteration 129/1000 | Loss: 0.00001853
Iteration 130/1000 | Loss: 0.00001853
Iteration 131/1000 | Loss: 0.00001853
Iteration 132/1000 | Loss: 0.00001853
Iteration 133/1000 | Loss: 0.00001853
Iteration 134/1000 | Loss: 0.00001853
Iteration 135/1000 | Loss: 0.00001853
Iteration 136/1000 | Loss: 0.00001853
Iteration 137/1000 | Loss: 0.00001853
Iteration 138/1000 | Loss: 0.00001853
Iteration 139/1000 | Loss: 0.00001853
Iteration 140/1000 | Loss: 0.00001853
Iteration 141/1000 | Loss: 0.00001853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.8528835425968282e-05, 1.8528835425968282e-05, 1.8528835425968282e-05, 1.8528835425968282e-05, 1.8528835425968282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8528835425968282e-05

Optimization complete. Final v2v error: 3.655975103378296 mm

Highest mean error: 4.074521064758301 mm for frame 158

Lowest mean error: 3.1847100257873535 mm for frame 105

Saving results

Total time: 47.12692046165466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105045
Iteration 2/25 | Loss: 0.00183712
Iteration 3/25 | Loss: 0.00146540
Iteration 4/25 | Loss: 0.00119375
Iteration 5/25 | Loss: 0.00123180
Iteration 6/25 | Loss: 0.00119062
Iteration 7/25 | Loss: 0.00107859
Iteration 8/25 | Loss: 0.00097490
Iteration 9/25 | Loss: 0.00093102
Iteration 10/25 | Loss: 0.00091680
Iteration 11/25 | Loss: 0.00091332
Iteration 12/25 | Loss: 0.00091720
Iteration 13/25 | Loss: 0.00091037
Iteration 14/25 | Loss: 0.00090525
Iteration 15/25 | Loss: 0.00090355
Iteration 16/25 | Loss: 0.00090514
Iteration 17/25 | Loss: 0.00090776
Iteration 18/25 | Loss: 0.00090597
Iteration 19/25 | Loss: 0.00090142
Iteration 20/25 | Loss: 0.00090648
Iteration 21/25 | Loss: 0.00090047
Iteration 22/25 | Loss: 0.00089668
Iteration 23/25 | Loss: 0.00089147
Iteration 24/25 | Loss: 0.00089013
Iteration 25/25 | Loss: 0.00088957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75087464
Iteration 2/25 | Loss: 0.00325002
Iteration 3/25 | Loss: 0.00325001
Iteration 4/25 | Loss: 0.00325001
Iteration 5/25 | Loss: 0.00325001
Iteration 6/25 | Loss: 0.00325001
Iteration 7/25 | Loss: 0.00325001
Iteration 8/25 | Loss: 0.00325001
Iteration 9/25 | Loss: 0.00325001
Iteration 10/25 | Loss: 0.00325001
Iteration 11/25 | Loss: 0.00325001
Iteration 12/25 | Loss: 0.00325001
Iteration 13/25 | Loss: 0.00325001
Iteration 14/25 | Loss: 0.00325001
Iteration 15/25 | Loss: 0.00325001
Iteration 16/25 | Loss: 0.00325001
Iteration 17/25 | Loss: 0.00325001
Iteration 18/25 | Loss: 0.00325001
Iteration 19/25 | Loss: 0.00325001
Iteration 20/25 | Loss: 0.00325001
Iteration 21/25 | Loss: 0.00325001
Iteration 22/25 | Loss: 0.00325001
Iteration 23/25 | Loss: 0.00325001
Iteration 24/25 | Loss: 0.00325001
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0032500133384019136, 0.0032500133384019136, 0.0032500133384019136, 0.0032500133384019136, 0.0032500133384019136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032500133384019136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00325001
Iteration 2/1000 | Loss: 0.00004023
Iteration 3/1000 | Loss: 0.00002725
Iteration 4/1000 | Loss: 0.00002179
Iteration 5/1000 | Loss: 0.00002027
Iteration 6/1000 | Loss: 0.00001958
Iteration 7/1000 | Loss: 0.00034096
Iteration 8/1000 | Loss: 0.00002704
Iteration 9/1000 | Loss: 0.00002315
Iteration 10/1000 | Loss: 0.00002116
Iteration 11/1000 | Loss: 0.00002037
Iteration 12/1000 | Loss: 0.00001990
Iteration 13/1000 | Loss: 0.00001940
Iteration 14/1000 | Loss: 0.00001888
Iteration 15/1000 | Loss: 0.00001848
Iteration 16/1000 | Loss: 0.00001807
Iteration 17/1000 | Loss: 0.00001800
Iteration 18/1000 | Loss: 0.00001791
Iteration 19/1000 | Loss: 0.00001773
Iteration 20/1000 | Loss: 0.00001764
Iteration 21/1000 | Loss: 0.00001756
Iteration 22/1000 | Loss: 0.00031642
Iteration 23/1000 | Loss: 0.00002706
Iteration 24/1000 | Loss: 0.00002334
Iteration 25/1000 | Loss: 0.00002076
Iteration 26/1000 | Loss: 0.00001973
Iteration 27/1000 | Loss: 0.00001916
Iteration 28/1000 | Loss: 0.00030961
Iteration 29/1000 | Loss: 0.00002850
Iteration 30/1000 | Loss: 0.00002383
Iteration 31/1000 | Loss: 0.00002160
Iteration 32/1000 | Loss: 0.00044384
Iteration 33/1000 | Loss: 0.00003077
Iteration 34/1000 | Loss: 0.00003470
Iteration 35/1000 | Loss: 0.00002978
Iteration 36/1000 | Loss: 0.00003108
Iteration 37/1000 | Loss: 0.00002063
Iteration 38/1000 | Loss: 0.00001875
Iteration 39/1000 | Loss: 0.00001814
Iteration 40/1000 | Loss: 0.00001755
Iteration 41/1000 | Loss: 0.00001699
Iteration 42/1000 | Loss: 0.00032691
Iteration 43/1000 | Loss: 0.00002687
Iteration 44/1000 | Loss: 0.00002237
Iteration 45/1000 | Loss: 0.00002005
Iteration 46/1000 | Loss: 0.00001912
Iteration 47/1000 | Loss: 0.00001854
Iteration 48/1000 | Loss: 0.00001817
Iteration 49/1000 | Loss: 0.00001782
Iteration 50/1000 | Loss: 0.00001744
Iteration 51/1000 | Loss: 0.00001715
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001703
Iteration 54/1000 | Loss: 0.00001703
Iteration 55/1000 | Loss: 0.00001702
Iteration 56/1000 | Loss: 0.00001701
Iteration 57/1000 | Loss: 0.00001701
Iteration 58/1000 | Loss: 0.00001700
Iteration 59/1000 | Loss: 0.00001699
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001698
Iteration 62/1000 | Loss: 0.00001697
Iteration 63/1000 | Loss: 0.00001697
Iteration 64/1000 | Loss: 0.00001696
Iteration 65/1000 | Loss: 0.00001693
Iteration 66/1000 | Loss: 0.00001689
Iteration 67/1000 | Loss: 0.00001681
Iteration 68/1000 | Loss: 0.00001681
Iteration 69/1000 | Loss: 0.00001680
Iteration 70/1000 | Loss: 0.00001680
Iteration 71/1000 | Loss: 0.00001680
Iteration 72/1000 | Loss: 0.00001679
Iteration 73/1000 | Loss: 0.00001679
Iteration 74/1000 | Loss: 0.00001679
Iteration 75/1000 | Loss: 0.00001678
Iteration 76/1000 | Loss: 0.00001678
Iteration 77/1000 | Loss: 0.00001678
Iteration 78/1000 | Loss: 0.00001677
Iteration 79/1000 | Loss: 0.00001676
Iteration 80/1000 | Loss: 0.00001675
Iteration 81/1000 | Loss: 0.00001674
Iteration 82/1000 | Loss: 0.00001670
Iteration 83/1000 | Loss: 0.00001667
Iteration 84/1000 | Loss: 0.00031796
Iteration 85/1000 | Loss: 0.00002481
Iteration 86/1000 | Loss: 0.00002127
Iteration 87/1000 | Loss: 0.00001931
Iteration 88/1000 | Loss: 0.00001871
Iteration 89/1000 | Loss: 0.00001824
Iteration 90/1000 | Loss: 0.00001787
Iteration 91/1000 | Loss: 0.00001746
Iteration 92/1000 | Loss: 0.00001717
Iteration 93/1000 | Loss: 0.00001688
Iteration 94/1000 | Loss: 0.00031684
Iteration 95/1000 | Loss: 0.00002830
Iteration 96/1000 | Loss: 0.00002386
Iteration 97/1000 | Loss: 0.00002104
Iteration 98/1000 | Loss: 0.00001936
Iteration 99/1000 | Loss: 0.00001842
Iteration 100/1000 | Loss: 0.00001797
Iteration 101/1000 | Loss: 0.00001764
Iteration 102/1000 | Loss: 0.00001734
Iteration 103/1000 | Loss: 0.00001709
Iteration 104/1000 | Loss: 0.00001694
Iteration 105/1000 | Loss: 0.00001692
Iteration 106/1000 | Loss: 0.00001690
Iteration 107/1000 | Loss: 0.00001690
Iteration 108/1000 | Loss: 0.00001690
Iteration 109/1000 | Loss: 0.00001690
Iteration 110/1000 | Loss: 0.00001690
Iteration 111/1000 | Loss: 0.00001690
Iteration 112/1000 | Loss: 0.00001690
Iteration 113/1000 | Loss: 0.00001690
Iteration 114/1000 | Loss: 0.00001690
Iteration 115/1000 | Loss: 0.00001689
Iteration 116/1000 | Loss: 0.00001689
Iteration 117/1000 | Loss: 0.00001688
Iteration 118/1000 | Loss: 0.00001688
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001687
Iteration 121/1000 | Loss: 0.00001686
Iteration 122/1000 | Loss: 0.00001683
Iteration 123/1000 | Loss: 0.00001680
Iteration 124/1000 | Loss: 0.00001671
Iteration 125/1000 | Loss: 0.00001668
Iteration 126/1000 | Loss: 0.00001667
Iteration 127/1000 | Loss: 0.00031474
Iteration 128/1000 | Loss: 0.00002789
Iteration 129/1000 | Loss: 0.00002313
Iteration 130/1000 | Loss: 0.00002107
Iteration 131/1000 | Loss: 0.00002013
Iteration 132/1000 | Loss: 0.00039346
Iteration 133/1000 | Loss: 0.00002932
Iteration 134/1000 | Loss: 0.00002406
Iteration 135/1000 | Loss: 0.00002167
Iteration 136/1000 | Loss: 0.00001891
Iteration 137/1000 | Loss: 0.00001783
Iteration 138/1000 | Loss: 0.00001720
Iteration 139/1000 | Loss: 0.00001687
Iteration 140/1000 | Loss: 0.00001646
Iteration 141/1000 | Loss: 0.00001628
Iteration 142/1000 | Loss: 0.00001627
Iteration 143/1000 | Loss: 0.00001626
Iteration 144/1000 | Loss: 0.00001626
Iteration 145/1000 | Loss: 0.00001625
Iteration 146/1000 | Loss: 0.00001624
Iteration 147/1000 | Loss: 0.00001624
Iteration 148/1000 | Loss: 0.00001624
Iteration 149/1000 | Loss: 0.00001624
Iteration 150/1000 | Loss: 0.00001623
Iteration 151/1000 | Loss: 0.00001623
Iteration 152/1000 | Loss: 0.00001623
Iteration 153/1000 | Loss: 0.00001622
Iteration 154/1000 | Loss: 0.00001622
Iteration 155/1000 | Loss: 0.00001622
Iteration 156/1000 | Loss: 0.00001621
Iteration 157/1000 | Loss: 0.00001621
Iteration 158/1000 | Loss: 0.00001620
Iteration 159/1000 | Loss: 0.00001620
Iteration 160/1000 | Loss: 0.00001619
Iteration 161/1000 | Loss: 0.00001619
Iteration 162/1000 | Loss: 0.00001619
Iteration 163/1000 | Loss: 0.00001618
Iteration 164/1000 | Loss: 0.00001618
Iteration 165/1000 | Loss: 0.00001618
Iteration 166/1000 | Loss: 0.00001617
Iteration 167/1000 | Loss: 0.00001617
Iteration 168/1000 | Loss: 0.00001617
Iteration 169/1000 | Loss: 0.00001617
Iteration 170/1000 | Loss: 0.00001617
Iteration 171/1000 | Loss: 0.00001617
Iteration 172/1000 | Loss: 0.00001617
Iteration 173/1000 | Loss: 0.00001616
Iteration 174/1000 | Loss: 0.00001616
Iteration 175/1000 | Loss: 0.00001616
Iteration 176/1000 | Loss: 0.00001616
Iteration 177/1000 | Loss: 0.00001616
Iteration 178/1000 | Loss: 0.00001615
Iteration 179/1000 | Loss: 0.00001615
Iteration 180/1000 | Loss: 0.00001615
Iteration 181/1000 | Loss: 0.00001615
Iteration 182/1000 | Loss: 0.00001615
Iteration 183/1000 | Loss: 0.00001615
Iteration 184/1000 | Loss: 0.00001614
Iteration 185/1000 | Loss: 0.00001614
Iteration 186/1000 | Loss: 0.00001614
Iteration 187/1000 | Loss: 0.00001614
Iteration 188/1000 | Loss: 0.00001614
Iteration 189/1000 | Loss: 0.00001614
Iteration 190/1000 | Loss: 0.00001614
Iteration 191/1000 | Loss: 0.00001614
Iteration 192/1000 | Loss: 0.00001614
Iteration 193/1000 | Loss: 0.00001614
Iteration 194/1000 | Loss: 0.00001614
Iteration 195/1000 | Loss: 0.00001614
Iteration 196/1000 | Loss: 0.00001613
Iteration 197/1000 | Loss: 0.00001613
Iteration 198/1000 | Loss: 0.00001613
Iteration 199/1000 | Loss: 0.00001613
Iteration 200/1000 | Loss: 0.00001613
Iteration 201/1000 | Loss: 0.00001613
Iteration 202/1000 | Loss: 0.00001613
Iteration 203/1000 | Loss: 0.00001613
Iteration 204/1000 | Loss: 0.00001613
Iteration 205/1000 | Loss: 0.00001613
Iteration 206/1000 | Loss: 0.00001613
Iteration 207/1000 | Loss: 0.00001613
Iteration 208/1000 | Loss: 0.00001613
Iteration 209/1000 | Loss: 0.00001613
Iteration 210/1000 | Loss: 0.00001613
Iteration 211/1000 | Loss: 0.00001613
Iteration 212/1000 | Loss: 0.00001613
Iteration 213/1000 | Loss: 0.00001613
Iteration 214/1000 | Loss: 0.00001613
Iteration 215/1000 | Loss: 0.00001613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.612678352103103e-05, 1.612678352103103e-05, 1.612678352103103e-05, 1.612678352103103e-05, 1.612678352103103e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.612678352103103e-05

Optimization complete. Final v2v error: 3.3274407386779785 mm

Highest mean error: 6.8950114250183105 mm for frame 32

Lowest mean error: 2.794154405593872 mm for frame 19

Saving results

Total time: 175.85841464996338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00636154
Iteration 2/25 | Loss: 0.00096076
Iteration 3/25 | Loss: 0.00086207
Iteration 4/25 | Loss: 0.00084180
Iteration 5/25 | Loss: 0.00083429
Iteration 6/25 | Loss: 0.00083231
Iteration 7/25 | Loss: 0.00083146
Iteration 8/25 | Loss: 0.00083142
Iteration 9/25 | Loss: 0.00083142
Iteration 10/25 | Loss: 0.00083142
Iteration 11/25 | Loss: 0.00083142
Iteration 12/25 | Loss: 0.00083142
Iteration 13/25 | Loss: 0.00083142
Iteration 14/25 | Loss: 0.00083142
Iteration 15/25 | Loss: 0.00083142
Iteration 16/25 | Loss: 0.00083142
Iteration 17/25 | Loss: 0.00083142
Iteration 18/25 | Loss: 0.00083142
Iteration 19/25 | Loss: 0.00083142
Iteration 20/25 | Loss: 0.00083142
Iteration 21/25 | Loss: 0.00083142
Iteration 22/25 | Loss: 0.00083142
Iteration 23/25 | Loss: 0.00083142
Iteration 24/25 | Loss: 0.00083142
Iteration 25/25 | Loss: 0.00083142

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.23898554
Iteration 2/25 | Loss: 0.00329448
Iteration 3/25 | Loss: 0.00329448
Iteration 4/25 | Loss: 0.00329448
Iteration 5/25 | Loss: 0.00329447
Iteration 6/25 | Loss: 0.00329447
Iteration 7/25 | Loss: 0.00329447
Iteration 8/25 | Loss: 0.00329447
Iteration 9/25 | Loss: 0.00329447
Iteration 10/25 | Loss: 0.00329447
Iteration 11/25 | Loss: 0.00329447
Iteration 12/25 | Loss: 0.00329447
Iteration 13/25 | Loss: 0.00329447
Iteration 14/25 | Loss: 0.00329447
Iteration 15/25 | Loss: 0.00329447
Iteration 16/25 | Loss: 0.00329447
Iteration 17/25 | Loss: 0.00329447
Iteration 18/25 | Loss: 0.00329447
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0032944725826382637, 0.0032944725826382637, 0.0032944725826382637, 0.0032944725826382637, 0.0032944725826382637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032944725826382637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00329447
Iteration 2/1000 | Loss: 0.00002275
Iteration 3/1000 | Loss: 0.00001553
Iteration 4/1000 | Loss: 0.00001346
Iteration 5/1000 | Loss: 0.00001282
Iteration 6/1000 | Loss: 0.00001242
Iteration 7/1000 | Loss: 0.00001219
Iteration 8/1000 | Loss: 0.00001201
Iteration 9/1000 | Loss: 0.00001200
Iteration 10/1000 | Loss: 0.00001197
Iteration 11/1000 | Loss: 0.00001191
Iteration 12/1000 | Loss: 0.00001190
Iteration 13/1000 | Loss: 0.00001188
Iteration 14/1000 | Loss: 0.00001188
Iteration 15/1000 | Loss: 0.00001187
Iteration 16/1000 | Loss: 0.00001186
Iteration 17/1000 | Loss: 0.00001186
Iteration 18/1000 | Loss: 0.00001186
Iteration 19/1000 | Loss: 0.00001186
Iteration 20/1000 | Loss: 0.00001186
Iteration 21/1000 | Loss: 0.00001186
Iteration 22/1000 | Loss: 0.00001186
Iteration 23/1000 | Loss: 0.00001185
Iteration 24/1000 | Loss: 0.00001185
Iteration 25/1000 | Loss: 0.00001185
Iteration 26/1000 | Loss: 0.00001185
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001185
Iteration 29/1000 | Loss: 0.00001185
Iteration 30/1000 | Loss: 0.00001185
Iteration 31/1000 | Loss: 0.00001184
Iteration 32/1000 | Loss: 0.00001184
Iteration 33/1000 | Loss: 0.00001183
Iteration 34/1000 | Loss: 0.00001182
Iteration 35/1000 | Loss: 0.00001182
Iteration 36/1000 | Loss: 0.00001182
Iteration 37/1000 | Loss: 0.00001181
Iteration 38/1000 | Loss: 0.00001181
Iteration 39/1000 | Loss: 0.00001179
Iteration 40/1000 | Loss: 0.00001178
Iteration 41/1000 | Loss: 0.00001178
Iteration 42/1000 | Loss: 0.00001176
Iteration 43/1000 | Loss: 0.00001176
Iteration 44/1000 | Loss: 0.00001175
Iteration 45/1000 | Loss: 0.00001175
Iteration 46/1000 | Loss: 0.00001174
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001172
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001171
Iteration 52/1000 | Loss: 0.00001171
Iteration 53/1000 | Loss: 0.00001171
Iteration 54/1000 | Loss: 0.00001170
Iteration 55/1000 | Loss: 0.00001170
Iteration 56/1000 | Loss: 0.00001170
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001169
Iteration 59/1000 | Loss: 0.00001169
Iteration 60/1000 | Loss: 0.00001169
Iteration 61/1000 | Loss: 0.00001168
Iteration 62/1000 | Loss: 0.00001168
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001168
Iteration 65/1000 | Loss: 0.00001168
Iteration 66/1000 | Loss: 0.00001168
Iteration 67/1000 | Loss: 0.00001168
Iteration 68/1000 | Loss: 0.00001168
Iteration 69/1000 | Loss: 0.00001168
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001167
Iteration 72/1000 | Loss: 0.00001167
Iteration 73/1000 | Loss: 0.00001167
Iteration 74/1000 | Loss: 0.00001167
Iteration 75/1000 | Loss: 0.00001167
Iteration 76/1000 | Loss: 0.00001167
Iteration 77/1000 | Loss: 0.00001167
Iteration 78/1000 | Loss: 0.00001167
Iteration 79/1000 | Loss: 0.00001167
Iteration 80/1000 | Loss: 0.00001166
Iteration 81/1000 | Loss: 0.00001166
Iteration 82/1000 | Loss: 0.00001166
Iteration 83/1000 | Loss: 0.00001166
Iteration 84/1000 | Loss: 0.00001166
Iteration 85/1000 | Loss: 0.00001166
Iteration 86/1000 | Loss: 0.00001166
Iteration 87/1000 | Loss: 0.00001166
Iteration 88/1000 | Loss: 0.00001166
Iteration 89/1000 | Loss: 0.00001166
Iteration 90/1000 | Loss: 0.00001166
Iteration 91/1000 | Loss: 0.00001166
Iteration 92/1000 | Loss: 0.00001166
Iteration 93/1000 | Loss: 0.00001166
Iteration 94/1000 | Loss: 0.00001166
Iteration 95/1000 | Loss: 0.00001166
Iteration 96/1000 | Loss: 0.00001166
Iteration 97/1000 | Loss: 0.00001166
Iteration 98/1000 | Loss: 0.00001166
Iteration 99/1000 | Loss: 0.00001166
Iteration 100/1000 | Loss: 0.00001166
Iteration 101/1000 | Loss: 0.00001166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.1660013115033507e-05, 1.1660013115033507e-05, 1.1660013115033507e-05, 1.1660013115033507e-05, 1.1660013115033507e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1660013115033507e-05

Optimization complete. Final v2v error: 2.9222443103790283 mm

Highest mean error: 3.2832438945770264 mm for frame 110

Lowest mean error: 2.330322027206421 mm for frame 2

Saving results

Total time: 30.512789487838745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01156056
Iteration 2/25 | Loss: 0.00539723
Iteration 3/25 | Loss: 0.00163546
Iteration 4/25 | Loss: 0.00126779
Iteration 5/25 | Loss: 0.00117136
Iteration 6/25 | Loss: 0.00116016
Iteration 7/25 | Loss: 0.00106178
Iteration 8/25 | Loss: 0.00098980
Iteration 9/25 | Loss: 0.00097335
Iteration 10/25 | Loss: 0.00096253
Iteration 11/25 | Loss: 0.00095973
Iteration 12/25 | Loss: 0.00095682
Iteration 13/25 | Loss: 0.00095394
Iteration 14/25 | Loss: 0.00096139
Iteration 15/25 | Loss: 0.00095672
Iteration 16/25 | Loss: 0.00095036
Iteration 17/25 | Loss: 0.00095351
Iteration 18/25 | Loss: 0.00094971
Iteration 19/25 | Loss: 0.00094352
Iteration 20/25 | Loss: 0.00093934
Iteration 21/25 | Loss: 0.00093503
Iteration 22/25 | Loss: 0.00093160
Iteration 23/25 | Loss: 0.00092550
Iteration 24/25 | Loss: 0.00092613
Iteration 25/25 | Loss: 0.00092066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65852320
Iteration 2/25 | Loss: 0.00309380
Iteration 3/25 | Loss: 0.00309380
Iteration 4/25 | Loss: 0.00309380
Iteration 5/25 | Loss: 0.00309379
Iteration 6/25 | Loss: 0.00309379
Iteration 7/25 | Loss: 0.00309379
Iteration 8/25 | Loss: 0.00309379
Iteration 9/25 | Loss: 0.00309379
Iteration 10/25 | Loss: 0.00309379
Iteration 11/25 | Loss: 0.00309379
Iteration 12/25 | Loss: 0.00309379
Iteration 13/25 | Loss: 0.00309379
Iteration 14/25 | Loss: 0.00309379
Iteration 15/25 | Loss: 0.00309379
Iteration 16/25 | Loss: 0.00309379
Iteration 17/25 | Loss: 0.00309379
Iteration 18/25 | Loss: 0.00309379
Iteration 19/25 | Loss: 0.00309379
Iteration 20/25 | Loss: 0.00309379
Iteration 21/25 | Loss: 0.00309379
Iteration 22/25 | Loss: 0.00309379
Iteration 23/25 | Loss: 0.00309379
Iteration 24/25 | Loss: 0.00309379
Iteration 25/25 | Loss: 0.00309379

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00309379
Iteration 2/1000 | Loss: 0.00004303
Iteration 3/1000 | Loss: 0.00003437
Iteration 4/1000 | Loss: 0.00003040
Iteration 5/1000 | Loss: 0.00002873
Iteration 6/1000 | Loss: 0.00002791
Iteration 7/1000 | Loss: 0.00002730
Iteration 8/1000 | Loss: 0.00002678
Iteration 9/1000 | Loss: 0.00002631
Iteration 10/1000 | Loss: 0.00002607
Iteration 11/1000 | Loss: 0.00002581
Iteration 12/1000 | Loss: 0.00002556
Iteration 13/1000 | Loss: 0.00002544
Iteration 14/1000 | Loss: 0.00002532
Iteration 15/1000 | Loss: 0.00002478
Iteration 16/1000 | Loss: 0.00002551
Iteration 17/1000 | Loss: 0.00002415
Iteration 18/1000 | Loss: 0.00002376
Iteration 19/1000 | Loss: 0.00002484
Iteration 20/1000 | Loss: 0.00002326
Iteration 21/1000 | Loss: 0.00002286
Iteration 22/1000 | Loss: 0.00002265
Iteration 23/1000 | Loss: 0.00002251
Iteration 24/1000 | Loss: 0.00002246
Iteration 25/1000 | Loss: 0.00002233
Iteration 26/1000 | Loss: 0.00002229
Iteration 27/1000 | Loss: 0.00002229
Iteration 28/1000 | Loss: 0.00002228
Iteration 29/1000 | Loss: 0.00002225
Iteration 30/1000 | Loss: 0.00002225
Iteration 31/1000 | Loss: 0.00002224
Iteration 32/1000 | Loss: 0.00002223
Iteration 33/1000 | Loss: 0.00002223
Iteration 34/1000 | Loss: 0.00002223
Iteration 35/1000 | Loss: 0.00002223
Iteration 36/1000 | Loss: 0.00002223
Iteration 37/1000 | Loss: 0.00002223
Iteration 38/1000 | Loss: 0.00002223
Iteration 39/1000 | Loss: 0.00002223
Iteration 40/1000 | Loss: 0.00002223
Iteration 41/1000 | Loss: 0.00002222
Iteration 42/1000 | Loss: 0.00002222
Iteration 43/1000 | Loss: 0.00002222
Iteration 44/1000 | Loss: 0.00002222
Iteration 45/1000 | Loss: 0.00002222
Iteration 46/1000 | Loss: 0.00002221
Iteration 47/1000 | Loss: 0.00002221
Iteration 48/1000 | Loss: 0.00032031
Iteration 49/1000 | Loss: 0.00027105
Iteration 50/1000 | Loss: 0.00016599
Iteration 51/1000 | Loss: 0.00022446
Iteration 52/1000 | Loss: 0.00027392
Iteration 53/1000 | Loss: 0.00023978
Iteration 54/1000 | Loss: 0.00018728
Iteration 55/1000 | Loss: 0.00023320
Iteration 56/1000 | Loss: 0.00004199
Iteration 57/1000 | Loss: 0.00003213
Iteration 58/1000 | Loss: 0.00002865
Iteration 59/1000 | Loss: 0.00018116
Iteration 60/1000 | Loss: 0.00002530
Iteration 61/1000 | Loss: 0.00002398
Iteration 62/1000 | Loss: 0.00002342
Iteration 63/1000 | Loss: 0.00002281
Iteration 64/1000 | Loss: 0.00002210
Iteration 65/1000 | Loss: 0.00002176
Iteration 66/1000 | Loss: 0.00002152
Iteration 67/1000 | Loss: 0.00002149
Iteration 68/1000 | Loss: 0.00002132
Iteration 69/1000 | Loss: 0.00002131
Iteration 70/1000 | Loss: 0.00002125
Iteration 71/1000 | Loss: 0.00002124
Iteration 72/1000 | Loss: 0.00002123
Iteration 73/1000 | Loss: 0.00002122
Iteration 74/1000 | Loss: 0.00002122
Iteration 75/1000 | Loss: 0.00002122
Iteration 76/1000 | Loss: 0.00002122
Iteration 77/1000 | Loss: 0.00002122
Iteration 78/1000 | Loss: 0.00002121
Iteration 79/1000 | Loss: 0.00002121
Iteration 80/1000 | Loss: 0.00002121
Iteration 81/1000 | Loss: 0.00002121
Iteration 82/1000 | Loss: 0.00002121
Iteration 83/1000 | Loss: 0.00002121
Iteration 84/1000 | Loss: 0.00002121
Iteration 85/1000 | Loss: 0.00002121
Iteration 86/1000 | Loss: 0.00002121
Iteration 87/1000 | Loss: 0.00002121
Iteration 88/1000 | Loss: 0.00002121
Iteration 89/1000 | Loss: 0.00002121
Iteration 90/1000 | Loss: 0.00002120
Iteration 91/1000 | Loss: 0.00002120
Iteration 92/1000 | Loss: 0.00002119
Iteration 93/1000 | Loss: 0.00002119
Iteration 94/1000 | Loss: 0.00002118
Iteration 95/1000 | Loss: 0.00002118
Iteration 96/1000 | Loss: 0.00002118
Iteration 97/1000 | Loss: 0.00002118
Iteration 98/1000 | Loss: 0.00002118
Iteration 99/1000 | Loss: 0.00002118
Iteration 100/1000 | Loss: 0.00002118
Iteration 101/1000 | Loss: 0.00002118
Iteration 102/1000 | Loss: 0.00002118
Iteration 103/1000 | Loss: 0.00002117
Iteration 104/1000 | Loss: 0.00002117
Iteration 105/1000 | Loss: 0.00002117
Iteration 106/1000 | Loss: 0.00002117
Iteration 107/1000 | Loss: 0.00002116
Iteration 108/1000 | Loss: 0.00002116
Iteration 109/1000 | Loss: 0.00002116
Iteration 110/1000 | Loss: 0.00002116
Iteration 111/1000 | Loss: 0.00002116
Iteration 112/1000 | Loss: 0.00002116
Iteration 113/1000 | Loss: 0.00002116
Iteration 114/1000 | Loss: 0.00002116
Iteration 115/1000 | Loss: 0.00002116
Iteration 116/1000 | Loss: 0.00002116
Iteration 117/1000 | Loss: 0.00002116
Iteration 118/1000 | Loss: 0.00002115
Iteration 119/1000 | Loss: 0.00002115
Iteration 120/1000 | Loss: 0.00002115
Iteration 121/1000 | Loss: 0.00002115
Iteration 122/1000 | Loss: 0.00002115
Iteration 123/1000 | Loss: 0.00002115
Iteration 124/1000 | Loss: 0.00002115
Iteration 125/1000 | Loss: 0.00002115
Iteration 126/1000 | Loss: 0.00002115
Iteration 127/1000 | Loss: 0.00002115
Iteration 128/1000 | Loss: 0.00002115
Iteration 129/1000 | Loss: 0.00002115
Iteration 130/1000 | Loss: 0.00002115
Iteration 131/1000 | Loss: 0.00002115
Iteration 132/1000 | Loss: 0.00002115
Iteration 133/1000 | Loss: 0.00002115
Iteration 134/1000 | Loss: 0.00002115
Iteration 135/1000 | Loss: 0.00002115
Iteration 136/1000 | Loss: 0.00002115
Iteration 137/1000 | Loss: 0.00002115
Iteration 138/1000 | Loss: 0.00002114
Iteration 139/1000 | Loss: 0.00002114
Iteration 140/1000 | Loss: 0.00002114
Iteration 141/1000 | Loss: 0.00002114
Iteration 142/1000 | Loss: 0.00002114
Iteration 143/1000 | Loss: 0.00002114
Iteration 144/1000 | Loss: 0.00002114
Iteration 145/1000 | Loss: 0.00002114
Iteration 146/1000 | Loss: 0.00002114
Iteration 147/1000 | Loss: 0.00002114
Iteration 148/1000 | Loss: 0.00002114
Iteration 149/1000 | Loss: 0.00002114
Iteration 150/1000 | Loss: 0.00002114
Iteration 151/1000 | Loss: 0.00002114
Iteration 152/1000 | Loss: 0.00002114
Iteration 153/1000 | Loss: 0.00002114
Iteration 154/1000 | Loss: 0.00002114
Iteration 155/1000 | Loss: 0.00002114
Iteration 156/1000 | Loss: 0.00002114
Iteration 157/1000 | Loss: 0.00002114
Iteration 158/1000 | Loss: 0.00002114
Iteration 159/1000 | Loss: 0.00002114
Iteration 160/1000 | Loss: 0.00002114
Iteration 161/1000 | Loss: 0.00002114
Iteration 162/1000 | Loss: 0.00002114
Iteration 163/1000 | Loss: 0.00002114
Iteration 164/1000 | Loss: 0.00002113
Iteration 165/1000 | Loss: 0.00002113
Iteration 166/1000 | Loss: 0.00002113
Iteration 167/1000 | Loss: 0.00002113
Iteration 168/1000 | Loss: 0.00002113
Iteration 169/1000 | Loss: 0.00002113
Iteration 170/1000 | Loss: 0.00002113
Iteration 171/1000 | Loss: 0.00002113
Iteration 172/1000 | Loss: 0.00002113
Iteration 173/1000 | Loss: 0.00002113
Iteration 174/1000 | Loss: 0.00002113
Iteration 175/1000 | Loss: 0.00002113
Iteration 176/1000 | Loss: 0.00002113
Iteration 177/1000 | Loss: 0.00002113
Iteration 178/1000 | Loss: 0.00002113
Iteration 179/1000 | Loss: 0.00002113
Iteration 180/1000 | Loss: 0.00002113
Iteration 181/1000 | Loss: 0.00002113
Iteration 182/1000 | Loss: 0.00002113
Iteration 183/1000 | Loss: 0.00002113
Iteration 184/1000 | Loss: 0.00002113
Iteration 185/1000 | Loss: 0.00002113
Iteration 186/1000 | Loss: 0.00002113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.112954098265618e-05, 2.112954098265618e-05, 2.112954098265618e-05, 2.112954098265618e-05, 2.112954098265618e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.112954098265618e-05

Optimization complete. Final v2v error: 3.8767499923706055 mm

Highest mean error: 5.558691501617432 mm for frame 89

Lowest mean error: 3.2120563983917236 mm for frame 182

Saving results

Total time: 119.63389945030212
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00663531
Iteration 2/25 | Loss: 0.00125808
Iteration 3/25 | Loss: 0.00102539
Iteration 4/25 | Loss: 0.00098047
Iteration 5/25 | Loss: 0.00094584
Iteration 6/25 | Loss: 0.00094019
Iteration 7/25 | Loss: 0.00092954
Iteration 8/25 | Loss: 0.00093099
Iteration 9/25 | Loss: 0.00092788
Iteration 10/25 | Loss: 0.00092704
Iteration 11/25 | Loss: 0.00092498
Iteration 12/25 | Loss: 0.00092487
Iteration 13/25 | Loss: 0.00092487
Iteration 14/25 | Loss: 0.00092486
Iteration 15/25 | Loss: 0.00092486
Iteration 16/25 | Loss: 0.00092486
Iteration 17/25 | Loss: 0.00092486
Iteration 18/25 | Loss: 0.00092486
Iteration 19/25 | Loss: 0.00092486
Iteration 20/25 | Loss: 0.00092486
Iteration 21/25 | Loss: 0.00092486
Iteration 22/25 | Loss: 0.00092486
Iteration 23/25 | Loss: 0.00092486
Iteration 24/25 | Loss: 0.00092486
Iteration 25/25 | Loss: 0.00092486

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93145299
Iteration 2/25 | Loss: 0.00397243
Iteration 3/25 | Loss: 0.00397243
Iteration 4/25 | Loss: 0.00397242
Iteration 5/25 | Loss: 0.00397242
Iteration 6/25 | Loss: 0.00397242
Iteration 7/25 | Loss: 0.00397242
Iteration 8/25 | Loss: 0.00397242
Iteration 9/25 | Loss: 0.00397242
Iteration 10/25 | Loss: 0.00397242
Iteration 11/25 | Loss: 0.00397242
Iteration 12/25 | Loss: 0.00397242
Iteration 13/25 | Loss: 0.00397242
Iteration 14/25 | Loss: 0.00397242
Iteration 15/25 | Loss: 0.00397242
Iteration 16/25 | Loss: 0.00397242
Iteration 17/25 | Loss: 0.00397242
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00397242046892643, 0.00397242046892643, 0.00397242046892643, 0.00397242046892643, 0.00397242046892643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00397242046892643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00397242
Iteration 2/1000 | Loss: 0.00003869
Iteration 3/1000 | Loss: 0.00002792
Iteration 4/1000 | Loss: 0.00002409
Iteration 5/1000 | Loss: 0.00002273
Iteration 6/1000 | Loss: 0.00002188
Iteration 7/1000 | Loss: 0.00002112
Iteration 8/1000 | Loss: 0.00002056
Iteration 9/1000 | Loss: 0.00002014
Iteration 10/1000 | Loss: 0.00001982
Iteration 11/1000 | Loss: 0.00001956
Iteration 12/1000 | Loss: 0.00001952
Iteration 13/1000 | Loss: 0.00001934
Iteration 14/1000 | Loss: 0.00001934
Iteration 15/1000 | Loss: 0.00001931
Iteration 16/1000 | Loss: 0.00001925
Iteration 17/1000 | Loss: 0.00001918
Iteration 18/1000 | Loss: 0.00001911
Iteration 19/1000 | Loss: 0.00001909
Iteration 20/1000 | Loss: 0.00001908
Iteration 21/1000 | Loss: 0.00001908
Iteration 22/1000 | Loss: 0.00001907
Iteration 23/1000 | Loss: 0.00001906
Iteration 24/1000 | Loss: 0.00001906
Iteration 25/1000 | Loss: 0.00001906
Iteration 26/1000 | Loss: 0.00001906
Iteration 27/1000 | Loss: 0.00001905
Iteration 28/1000 | Loss: 0.00001905
Iteration 29/1000 | Loss: 0.00001904
Iteration 30/1000 | Loss: 0.00001904
Iteration 31/1000 | Loss: 0.00001903
Iteration 32/1000 | Loss: 0.00001903
Iteration 33/1000 | Loss: 0.00001902
Iteration 34/1000 | Loss: 0.00001902
Iteration 35/1000 | Loss: 0.00001901
Iteration 36/1000 | Loss: 0.00001901
Iteration 37/1000 | Loss: 0.00001901
Iteration 38/1000 | Loss: 0.00001901
Iteration 39/1000 | Loss: 0.00001901
Iteration 40/1000 | Loss: 0.00001900
Iteration 41/1000 | Loss: 0.00001900
Iteration 42/1000 | Loss: 0.00001899
Iteration 43/1000 | Loss: 0.00001899
Iteration 44/1000 | Loss: 0.00001899
Iteration 45/1000 | Loss: 0.00001898
Iteration 46/1000 | Loss: 0.00001898
Iteration 47/1000 | Loss: 0.00001898
Iteration 48/1000 | Loss: 0.00001898
Iteration 49/1000 | Loss: 0.00001898
Iteration 50/1000 | Loss: 0.00001898
Iteration 51/1000 | Loss: 0.00001898
Iteration 52/1000 | Loss: 0.00001898
Iteration 53/1000 | Loss: 0.00001897
Iteration 54/1000 | Loss: 0.00001897
Iteration 55/1000 | Loss: 0.00001897
Iteration 56/1000 | Loss: 0.00001897
Iteration 57/1000 | Loss: 0.00001897
Iteration 58/1000 | Loss: 0.00001897
Iteration 59/1000 | Loss: 0.00001897
Iteration 60/1000 | Loss: 0.00001897
Iteration 61/1000 | Loss: 0.00001897
Iteration 62/1000 | Loss: 0.00001897
Iteration 63/1000 | Loss: 0.00001897
Iteration 64/1000 | Loss: 0.00001897
Iteration 65/1000 | Loss: 0.00001897
Iteration 66/1000 | Loss: 0.00001896
Iteration 67/1000 | Loss: 0.00001896
Iteration 68/1000 | Loss: 0.00001896
Iteration 69/1000 | Loss: 0.00001896
Iteration 70/1000 | Loss: 0.00001895
Iteration 71/1000 | Loss: 0.00001895
Iteration 72/1000 | Loss: 0.00001894
Iteration 73/1000 | Loss: 0.00001894
Iteration 74/1000 | Loss: 0.00001893
Iteration 75/1000 | Loss: 0.00001893
Iteration 76/1000 | Loss: 0.00001892
Iteration 77/1000 | Loss: 0.00001892
Iteration 78/1000 | Loss: 0.00001892
Iteration 79/1000 | Loss: 0.00001891
Iteration 80/1000 | Loss: 0.00001890
Iteration 81/1000 | Loss: 0.00001890
Iteration 82/1000 | Loss: 0.00001890
Iteration 83/1000 | Loss: 0.00001890
Iteration 84/1000 | Loss: 0.00001890
Iteration 85/1000 | Loss: 0.00001890
Iteration 86/1000 | Loss: 0.00001890
Iteration 87/1000 | Loss: 0.00001890
Iteration 88/1000 | Loss: 0.00001890
Iteration 89/1000 | Loss: 0.00001890
Iteration 90/1000 | Loss: 0.00001890
Iteration 91/1000 | Loss: 0.00001889
Iteration 92/1000 | Loss: 0.00001889
Iteration 93/1000 | Loss: 0.00001889
Iteration 94/1000 | Loss: 0.00001889
Iteration 95/1000 | Loss: 0.00001889
Iteration 96/1000 | Loss: 0.00001889
Iteration 97/1000 | Loss: 0.00001888
Iteration 98/1000 | Loss: 0.00001888
Iteration 99/1000 | Loss: 0.00001888
Iteration 100/1000 | Loss: 0.00001887
Iteration 101/1000 | Loss: 0.00001887
Iteration 102/1000 | Loss: 0.00001887
Iteration 103/1000 | Loss: 0.00001886
Iteration 104/1000 | Loss: 0.00001886
Iteration 105/1000 | Loss: 0.00001886
Iteration 106/1000 | Loss: 0.00001886
Iteration 107/1000 | Loss: 0.00001886
Iteration 108/1000 | Loss: 0.00001886
Iteration 109/1000 | Loss: 0.00001886
Iteration 110/1000 | Loss: 0.00001885
Iteration 111/1000 | Loss: 0.00001885
Iteration 112/1000 | Loss: 0.00001885
Iteration 113/1000 | Loss: 0.00001885
Iteration 114/1000 | Loss: 0.00001884
Iteration 115/1000 | Loss: 0.00001884
Iteration 116/1000 | Loss: 0.00001884
Iteration 117/1000 | Loss: 0.00001884
Iteration 118/1000 | Loss: 0.00001883
Iteration 119/1000 | Loss: 0.00001883
Iteration 120/1000 | Loss: 0.00001883
Iteration 121/1000 | Loss: 0.00001883
Iteration 122/1000 | Loss: 0.00001883
Iteration 123/1000 | Loss: 0.00001883
Iteration 124/1000 | Loss: 0.00001883
Iteration 125/1000 | Loss: 0.00001883
Iteration 126/1000 | Loss: 0.00001883
Iteration 127/1000 | Loss: 0.00001883
Iteration 128/1000 | Loss: 0.00001883
Iteration 129/1000 | Loss: 0.00001883
Iteration 130/1000 | Loss: 0.00001882
Iteration 131/1000 | Loss: 0.00001882
Iteration 132/1000 | Loss: 0.00001882
Iteration 133/1000 | Loss: 0.00001882
Iteration 134/1000 | Loss: 0.00001882
Iteration 135/1000 | Loss: 0.00001882
Iteration 136/1000 | Loss: 0.00001882
Iteration 137/1000 | Loss: 0.00001882
Iteration 138/1000 | Loss: 0.00001882
Iteration 139/1000 | Loss: 0.00001882
Iteration 140/1000 | Loss: 0.00001882
Iteration 141/1000 | Loss: 0.00001882
Iteration 142/1000 | Loss: 0.00001882
Iteration 143/1000 | Loss: 0.00001882
Iteration 144/1000 | Loss: 0.00001882
Iteration 145/1000 | Loss: 0.00001881
Iteration 146/1000 | Loss: 0.00001881
Iteration 147/1000 | Loss: 0.00001881
Iteration 148/1000 | Loss: 0.00001881
Iteration 149/1000 | Loss: 0.00001881
Iteration 150/1000 | Loss: 0.00001881
Iteration 151/1000 | Loss: 0.00001881
Iteration 152/1000 | Loss: 0.00001881
Iteration 153/1000 | Loss: 0.00001881
Iteration 154/1000 | Loss: 0.00001881
Iteration 155/1000 | Loss: 0.00001881
Iteration 156/1000 | Loss: 0.00001880
Iteration 157/1000 | Loss: 0.00001880
Iteration 158/1000 | Loss: 0.00001880
Iteration 159/1000 | Loss: 0.00001880
Iteration 160/1000 | Loss: 0.00001880
Iteration 161/1000 | Loss: 0.00001880
Iteration 162/1000 | Loss: 0.00001880
Iteration 163/1000 | Loss: 0.00001880
Iteration 164/1000 | Loss: 0.00001880
Iteration 165/1000 | Loss: 0.00001880
Iteration 166/1000 | Loss: 0.00001880
Iteration 167/1000 | Loss: 0.00001879
Iteration 168/1000 | Loss: 0.00001879
Iteration 169/1000 | Loss: 0.00001879
Iteration 170/1000 | Loss: 0.00001879
Iteration 171/1000 | Loss: 0.00001879
Iteration 172/1000 | Loss: 0.00001879
Iteration 173/1000 | Loss: 0.00001879
Iteration 174/1000 | Loss: 0.00001879
Iteration 175/1000 | Loss: 0.00001879
Iteration 176/1000 | Loss: 0.00001879
Iteration 177/1000 | Loss: 0.00001879
Iteration 178/1000 | Loss: 0.00001879
Iteration 179/1000 | Loss: 0.00001879
Iteration 180/1000 | Loss: 0.00001879
Iteration 181/1000 | Loss: 0.00001879
Iteration 182/1000 | Loss: 0.00001879
Iteration 183/1000 | Loss: 0.00001879
Iteration 184/1000 | Loss: 0.00001879
Iteration 185/1000 | Loss: 0.00001879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.878813600342255e-05, 1.878813600342255e-05, 1.878813600342255e-05, 1.878813600342255e-05, 1.878813600342255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.878813600342255e-05

Optimization complete. Final v2v error: 3.6206815242767334 mm

Highest mean error: 4.588498115539551 mm for frame 114

Lowest mean error: 2.817863702774048 mm for frame 69

Saving results

Total time: 58.446815967559814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00354723
Iteration 2/25 | Loss: 0.00091083
Iteration 3/25 | Loss: 0.00081199
Iteration 4/25 | Loss: 0.00079703
Iteration 5/25 | Loss: 0.00079320
Iteration 6/25 | Loss: 0.00079137
Iteration 7/25 | Loss: 0.00079105
Iteration 8/25 | Loss: 0.00079105
Iteration 9/25 | Loss: 0.00079105
Iteration 10/25 | Loss: 0.00079105
Iteration 11/25 | Loss: 0.00079105
Iteration 12/25 | Loss: 0.00079105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007910459535196424, 0.0007910459535196424, 0.0007910459535196424, 0.0007910459535196424, 0.0007910459535196424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007910459535196424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70281804
Iteration 2/25 | Loss: 0.00334046
Iteration 3/25 | Loss: 0.00334046
Iteration 4/25 | Loss: 0.00334046
Iteration 5/25 | Loss: 0.00334046
Iteration 6/25 | Loss: 0.00334046
Iteration 7/25 | Loss: 0.00334045
Iteration 8/25 | Loss: 0.00334045
Iteration 9/25 | Loss: 0.00334045
Iteration 10/25 | Loss: 0.00334045
Iteration 11/25 | Loss: 0.00334045
Iteration 12/25 | Loss: 0.00334045
Iteration 13/25 | Loss: 0.00334045
Iteration 14/25 | Loss: 0.00334045
Iteration 15/25 | Loss: 0.00334045
Iteration 16/25 | Loss: 0.00334045
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0033404540736228228, 0.0033404540736228228, 0.0033404540736228228, 0.0033404540736228228, 0.0033404540736228228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033404540736228228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00334045
Iteration 2/1000 | Loss: 0.00002410
Iteration 3/1000 | Loss: 0.00001703
Iteration 4/1000 | Loss: 0.00001485
Iteration 5/1000 | Loss: 0.00001383
Iteration 6/1000 | Loss: 0.00001325
Iteration 7/1000 | Loss: 0.00001292
Iteration 8/1000 | Loss: 0.00001265
Iteration 9/1000 | Loss: 0.00001252
Iteration 10/1000 | Loss: 0.00001246
Iteration 11/1000 | Loss: 0.00001233
Iteration 12/1000 | Loss: 0.00001233
Iteration 13/1000 | Loss: 0.00001231
Iteration 14/1000 | Loss: 0.00001231
Iteration 15/1000 | Loss: 0.00001231
Iteration 16/1000 | Loss: 0.00001230
Iteration 17/1000 | Loss: 0.00001228
Iteration 18/1000 | Loss: 0.00001227
Iteration 19/1000 | Loss: 0.00001227
Iteration 20/1000 | Loss: 0.00001226
Iteration 21/1000 | Loss: 0.00001226
Iteration 22/1000 | Loss: 0.00001226
Iteration 23/1000 | Loss: 0.00001226
Iteration 24/1000 | Loss: 0.00001226
Iteration 25/1000 | Loss: 0.00001226
Iteration 26/1000 | Loss: 0.00001226
Iteration 27/1000 | Loss: 0.00001226
Iteration 28/1000 | Loss: 0.00001226
Iteration 29/1000 | Loss: 0.00001225
Iteration 30/1000 | Loss: 0.00001225
Iteration 31/1000 | Loss: 0.00001224
Iteration 32/1000 | Loss: 0.00001223
Iteration 33/1000 | Loss: 0.00001223
Iteration 34/1000 | Loss: 0.00001223
Iteration 35/1000 | Loss: 0.00001222
Iteration 36/1000 | Loss: 0.00001214
Iteration 37/1000 | Loss: 0.00001214
Iteration 38/1000 | Loss: 0.00001213
Iteration 39/1000 | Loss: 0.00001212
Iteration 40/1000 | Loss: 0.00001211
Iteration 41/1000 | Loss: 0.00001210
Iteration 42/1000 | Loss: 0.00001210
Iteration 43/1000 | Loss: 0.00001209
Iteration 44/1000 | Loss: 0.00001209
Iteration 45/1000 | Loss: 0.00001209
Iteration 46/1000 | Loss: 0.00001209
Iteration 47/1000 | Loss: 0.00001208
Iteration 48/1000 | Loss: 0.00001208
Iteration 49/1000 | Loss: 0.00001207
Iteration 50/1000 | Loss: 0.00001207
Iteration 51/1000 | Loss: 0.00001207
Iteration 52/1000 | Loss: 0.00001206
Iteration 53/1000 | Loss: 0.00001206
Iteration 54/1000 | Loss: 0.00001206
Iteration 55/1000 | Loss: 0.00001206
Iteration 56/1000 | Loss: 0.00001205
Iteration 57/1000 | Loss: 0.00001205
Iteration 58/1000 | Loss: 0.00001205
Iteration 59/1000 | Loss: 0.00001205
Iteration 60/1000 | Loss: 0.00001205
Iteration 61/1000 | Loss: 0.00001205
Iteration 62/1000 | Loss: 0.00001205
Iteration 63/1000 | Loss: 0.00001204
Iteration 64/1000 | Loss: 0.00001204
Iteration 65/1000 | Loss: 0.00001204
Iteration 66/1000 | Loss: 0.00001204
Iteration 67/1000 | Loss: 0.00001204
Iteration 68/1000 | Loss: 0.00001204
Iteration 69/1000 | Loss: 0.00001204
Iteration 70/1000 | Loss: 0.00001204
Iteration 71/1000 | Loss: 0.00001204
Iteration 72/1000 | Loss: 0.00001203
Iteration 73/1000 | Loss: 0.00001203
Iteration 74/1000 | Loss: 0.00001203
Iteration 75/1000 | Loss: 0.00001203
Iteration 76/1000 | Loss: 0.00001203
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001202
Iteration 81/1000 | Loss: 0.00001202
Iteration 82/1000 | Loss: 0.00001202
Iteration 83/1000 | Loss: 0.00001202
Iteration 84/1000 | Loss: 0.00001202
Iteration 85/1000 | Loss: 0.00001202
Iteration 86/1000 | Loss: 0.00001202
Iteration 87/1000 | Loss: 0.00001202
Iteration 88/1000 | Loss: 0.00001202
Iteration 89/1000 | Loss: 0.00001202
Iteration 90/1000 | Loss: 0.00001202
Iteration 91/1000 | Loss: 0.00001201
Iteration 92/1000 | Loss: 0.00001201
Iteration 93/1000 | Loss: 0.00001201
Iteration 94/1000 | Loss: 0.00001201
Iteration 95/1000 | Loss: 0.00001201
Iteration 96/1000 | Loss: 0.00001201
Iteration 97/1000 | Loss: 0.00001201
Iteration 98/1000 | Loss: 0.00001201
Iteration 99/1000 | Loss: 0.00001201
Iteration 100/1000 | Loss: 0.00001200
Iteration 101/1000 | Loss: 0.00001200
Iteration 102/1000 | Loss: 0.00001200
Iteration 103/1000 | Loss: 0.00001200
Iteration 104/1000 | Loss: 0.00001200
Iteration 105/1000 | Loss: 0.00001200
Iteration 106/1000 | Loss: 0.00001200
Iteration 107/1000 | Loss: 0.00001200
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001200
Iteration 111/1000 | Loss: 0.00001200
Iteration 112/1000 | Loss: 0.00001200
Iteration 113/1000 | Loss: 0.00001200
Iteration 114/1000 | Loss: 0.00001200
Iteration 115/1000 | Loss: 0.00001199
Iteration 116/1000 | Loss: 0.00001199
Iteration 117/1000 | Loss: 0.00001199
Iteration 118/1000 | Loss: 0.00001199
Iteration 119/1000 | Loss: 0.00001199
Iteration 120/1000 | Loss: 0.00001199
Iteration 121/1000 | Loss: 0.00001199
Iteration 122/1000 | Loss: 0.00001198
Iteration 123/1000 | Loss: 0.00001198
Iteration 124/1000 | Loss: 0.00001198
Iteration 125/1000 | Loss: 0.00001198
Iteration 126/1000 | Loss: 0.00001198
Iteration 127/1000 | Loss: 0.00001198
Iteration 128/1000 | Loss: 0.00001198
Iteration 129/1000 | Loss: 0.00001198
Iteration 130/1000 | Loss: 0.00001198
Iteration 131/1000 | Loss: 0.00001198
Iteration 132/1000 | Loss: 0.00001198
Iteration 133/1000 | Loss: 0.00001198
Iteration 134/1000 | Loss: 0.00001198
Iteration 135/1000 | Loss: 0.00001198
Iteration 136/1000 | Loss: 0.00001198
Iteration 137/1000 | Loss: 0.00001198
Iteration 138/1000 | Loss: 0.00001198
Iteration 139/1000 | Loss: 0.00001198
Iteration 140/1000 | Loss: 0.00001198
Iteration 141/1000 | Loss: 0.00001198
Iteration 142/1000 | Loss: 0.00001198
Iteration 143/1000 | Loss: 0.00001198
Iteration 144/1000 | Loss: 0.00001198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.1982173418800812e-05, 1.1982173418800812e-05, 1.1982173418800812e-05, 1.1982173418800812e-05, 1.1982173418800812e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1982173418800812e-05

Optimization complete. Final v2v error: 2.9908225536346436 mm

Highest mean error: 3.6461782455444336 mm for frame 9

Lowest mean error: 2.628202199935913 mm for frame 36

Saving results

Total time: 32.810919523239136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860946
Iteration 2/25 | Loss: 0.00099779
Iteration 3/25 | Loss: 0.00086041
Iteration 4/25 | Loss: 0.00083355
Iteration 5/25 | Loss: 0.00082608
Iteration 6/25 | Loss: 0.00082403
Iteration 7/25 | Loss: 0.00082388
Iteration 8/25 | Loss: 0.00082388
Iteration 9/25 | Loss: 0.00082388
Iteration 10/25 | Loss: 0.00082388
Iteration 11/25 | Loss: 0.00082388
Iteration 12/25 | Loss: 0.00082388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008238815353251994, 0.0008238815353251994, 0.0008238815353251994, 0.0008238815353251994, 0.0008238815353251994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008238815353251994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72293437
Iteration 2/25 | Loss: 0.00356995
Iteration 3/25 | Loss: 0.00356995
Iteration 4/25 | Loss: 0.00356995
Iteration 5/25 | Loss: 0.00356994
Iteration 6/25 | Loss: 0.00356994
Iteration 7/25 | Loss: 0.00356994
Iteration 8/25 | Loss: 0.00356994
Iteration 9/25 | Loss: 0.00356994
Iteration 10/25 | Loss: 0.00356994
Iteration 11/25 | Loss: 0.00356994
Iteration 12/25 | Loss: 0.00356994
Iteration 13/25 | Loss: 0.00356994
Iteration 14/25 | Loss: 0.00356994
Iteration 15/25 | Loss: 0.00356994
Iteration 16/25 | Loss: 0.00356994
Iteration 17/25 | Loss: 0.00356994
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0035699428990483284, 0.0035699428990483284, 0.0035699428990483284, 0.0035699428990483284, 0.0035699428990483284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035699428990483284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00356994
Iteration 2/1000 | Loss: 0.00004410
Iteration 3/1000 | Loss: 0.00002415
Iteration 4/1000 | Loss: 0.00001888
Iteration 5/1000 | Loss: 0.00001648
Iteration 6/1000 | Loss: 0.00001529
Iteration 7/1000 | Loss: 0.00001415
Iteration 8/1000 | Loss: 0.00001331
Iteration 9/1000 | Loss: 0.00001262
Iteration 10/1000 | Loss: 0.00001221
Iteration 11/1000 | Loss: 0.00001192
Iteration 12/1000 | Loss: 0.00001170
Iteration 13/1000 | Loss: 0.00001152
Iteration 14/1000 | Loss: 0.00001136
Iteration 15/1000 | Loss: 0.00001135
Iteration 16/1000 | Loss: 0.00001132
Iteration 17/1000 | Loss: 0.00001130
Iteration 18/1000 | Loss: 0.00001129
Iteration 19/1000 | Loss: 0.00001128
Iteration 20/1000 | Loss: 0.00001127
Iteration 21/1000 | Loss: 0.00001126
Iteration 22/1000 | Loss: 0.00001124
Iteration 23/1000 | Loss: 0.00001120
Iteration 24/1000 | Loss: 0.00001119
Iteration 25/1000 | Loss: 0.00001118
Iteration 26/1000 | Loss: 0.00001118
Iteration 27/1000 | Loss: 0.00001110
Iteration 28/1000 | Loss: 0.00001109
Iteration 29/1000 | Loss: 0.00001109
Iteration 30/1000 | Loss: 0.00001108
Iteration 31/1000 | Loss: 0.00001108
Iteration 32/1000 | Loss: 0.00001107
Iteration 33/1000 | Loss: 0.00001107
Iteration 34/1000 | Loss: 0.00001107
Iteration 35/1000 | Loss: 0.00001107
Iteration 36/1000 | Loss: 0.00001106
Iteration 37/1000 | Loss: 0.00001106
Iteration 38/1000 | Loss: 0.00001105
Iteration 39/1000 | Loss: 0.00001105
Iteration 40/1000 | Loss: 0.00001105
Iteration 41/1000 | Loss: 0.00001103
Iteration 42/1000 | Loss: 0.00001103
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001102
Iteration 45/1000 | Loss: 0.00001101
Iteration 46/1000 | Loss: 0.00001101
Iteration 47/1000 | Loss: 0.00001100
Iteration 48/1000 | Loss: 0.00001100
Iteration 49/1000 | Loss: 0.00001100
Iteration 50/1000 | Loss: 0.00001099
Iteration 51/1000 | Loss: 0.00001099
Iteration 52/1000 | Loss: 0.00001099
Iteration 53/1000 | Loss: 0.00001098
Iteration 54/1000 | Loss: 0.00001098
Iteration 55/1000 | Loss: 0.00001097
Iteration 56/1000 | Loss: 0.00001097
Iteration 57/1000 | Loss: 0.00001097
Iteration 58/1000 | Loss: 0.00001097
Iteration 59/1000 | Loss: 0.00001096
Iteration 60/1000 | Loss: 0.00001096
Iteration 61/1000 | Loss: 0.00001096
Iteration 62/1000 | Loss: 0.00001096
Iteration 63/1000 | Loss: 0.00001095
Iteration 64/1000 | Loss: 0.00001095
Iteration 65/1000 | Loss: 0.00001095
Iteration 66/1000 | Loss: 0.00001095
Iteration 67/1000 | Loss: 0.00001095
Iteration 68/1000 | Loss: 0.00001095
Iteration 69/1000 | Loss: 0.00001094
Iteration 70/1000 | Loss: 0.00001094
Iteration 71/1000 | Loss: 0.00001094
Iteration 72/1000 | Loss: 0.00001093
Iteration 73/1000 | Loss: 0.00001093
Iteration 74/1000 | Loss: 0.00001092
Iteration 75/1000 | Loss: 0.00001092
Iteration 76/1000 | Loss: 0.00001092
Iteration 77/1000 | Loss: 0.00001092
Iteration 78/1000 | Loss: 0.00001092
Iteration 79/1000 | Loss: 0.00001092
Iteration 80/1000 | Loss: 0.00001091
Iteration 81/1000 | Loss: 0.00001091
Iteration 82/1000 | Loss: 0.00001091
Iteration 83/1000 | Loss: 0.00001091
Iteration 84/1000 | Loss: 0.00001091
Iteration 85/1000 | Loss: 0.00001090
Iteration 86/1000 | Loss: 0.00001090
Iteration 87/1000 | Loss: 0.00001090
Iteration 88/1000 | Loss: 0.00001090
Iteration 89/1000 | Loss: 0.00001090
Iteration 90/1000 | Loss: 0.00001090
Iteration 91/1000 | Loss: 0.00001090
Iteration 92/1000 | Loss: 0.00001090
Iteration 93/1000 | Loss: 0.00001090
Iteration 94/1000 | Loss: 0.00001089
Iteration 95/1000 | Loss: 0.00001089
Iteration 96/1000 | Loss: 0.00001089
Iteration 97/1000 | Loss: 0.00001089
Iteration 98/1000 | Loss: 0.00001089
Iteration 99/1000 | Loss: 0.00001088
Iteration 100/1000 | Loss: 0.00001088
Iteration 101/1000 | Loss: 0.00001088
Iteration 102/1000 | Loss: 0.00001088
Iteration 103/1000 | Loss: 0.00001088
Iteration 104/1000 | Loss: 0.00001088
Iteration 105/1000 | Loss: 0.00001088
Iteration 106/1000 | Loss: 0.00001088
Iteration 107/1000 | Loss: 0.00001087
Iteration 108/1000 | Loss: 0.00001087
Iteration 109/1000 | Loss: 0.00001087
Iteration 110/1000 | Loss: 0.00001087
Iteration 111/1000 | Loss: 0.00001087
Iteration 112/1000 | Loss: 0.00001087
Iteration 113/1000 | Loss: 0.00001087
Iteration 114/1000 | Loss: 0.00001087
Iteration 115/1000 | Loss: 0.00001087
Iteration 116/1000 | Loss: 0.00001087
Iteration 117/1000 | Loss: 0.00001086
Iteration 118/1000 | Loss: 0.00001086
Iteration 119/1000 | Loss: 0.00001086
Iteration 120/1000 | Loss: 0.00001086
Iteration 121/1000 | Loss: 0.00001086
Iteration 122/1000 | Loss: 0.00001086
Iteration 123/1000 | Loss: 0.00001086
Iteration 124/1000 | Loss: 0.00001086
Iteration 125/1000 | Loss: 0.00001086
Iteration 126/1000 | Loss: 0.00001085
Iteration 127/1000 | Loss: 0.00001085
Iteration 128/1000 | Loss: 0.00001085
Iteration 129/1000 | Loss: 0.00001085
Iteration 130/1000 | Loss: 0.00001085
Iteration 131/1000 | Loss: 0.00001085
Iteration 132/1000 | Loss: 0.00001085
Iteration 133/1000 | Loss: 0.00001085
Iteration 134/1000 | Loss: 0.00001085
Iteration 135/1000 | Loss: 0.00001085
Iteration 136/1000 | Loss: 0.00001085
Iteration 137/1000 | Loss: 0.00001085
Iteration 138/1000 | Loss: 0.00001085
Iteration 139/1000 | Loss: 0.00001084
Iteration 140/1000 | Loss: 0.00001084
Iteration 141/1000 | Loss: 0.00001084
Iteration 142/1000 | Loss: 0.00001084
Iteration 143/1000 | Loss: 0.00001084
Iteration 144/1000 | Loss: 0.00001084
Iteration 145/1000 | Loss: 0.00001084
Iteration 146/1000 | Loss: 0.00001084
Iteration 147/1000 | Loss: 0.00001084
Iteration 148/1000 | Loss: 0.00001084
Iteration 149/1000 | Loss: 0.00001084
Iteration 150/1000 | Loss: 0.00001084
Iteration 151/1000 | Loss: 0.00001084
Iteration 152/1000 | Loss: 0.00001084
Iteration 153/1000 | Loss: 0.00001084
Iteration 154/1000 | Loss: 0.00001084
Iteration 155/1000 | Loss: 0.00001084
Iteration 156/1000 | Loss: 0.00001084
Iteration 157/1000 | Loss: 0.00001084
Iteration 158/1000 | Loss: 0.00001084
Iteration 159/1000 | Loss: 0.00001084
Iteration 160/1000 | Loss: 0.00001084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.0836037290573586e-05, 1.0836037290573586e-05, 1.0836037290573586e-05, 1.0836037290573586e-05, 1.0836037290573586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0836037290573586e-05

Optimization complete. Final v2v error: 2.8038136959075928 mm

Highest mean error: 3.214185953140259 mm for frame 190

Lowest mean error: 2.4367265701293945 mm for frame 232

Saving results

Total time: 46.84841513633728
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879138
Iteration 2/25 | Loss: 0.00112431
Iteration 3/25 | Loss: 0.00095793
Iteration 4/25 | Loss: 0.00093485
Iteration 5/25 | Loss: 0.00092703
Iteration 6/25 | Loss: 0.00092535
Iteration 7/25 | Loss: 0.00092485
Iteration 8/25 | Loss: 0.00092459
Iteration 9/25 | Loss: 0.00092440
Iteration 10/25 | Loss: 0.00092426
Iteration 11/25 | Loss: 0.00092412
Iteration 12/25 | Loss: 0.00092389
Iteration 13/25 | Loss: 0.00092624
Iteration 14/25 | Loss: 0.00092540
Iteration 15/25 | Loss: 0.00092499
Iteration 16/25 | Loss: 0.00092511
Iteration 17/25 | Loss: 0.00092525
Iteration 18/25 | Loss: 0.00092391
Iteration 19/25 | Loss: 0.00092160
Iteration 20/25 | Loss: 0.00092111
Iteration 21/25 | Loss: 0.00092084
Iteration 22/25 | Loss: 0.00092072
Iteration 23/25 | Loss: 0.00092063
Iteration 24/25 | Loss: 0.00092052
Iteration 25/25 | Loss: 0.00092042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70275259
Iteration 2/25 | Loss: 0.00381798
Iteration 3/25 | Loss: 0.00381797
Iteration 4/25 | Loss: 0.00381796
Iteration 5/25 | Loss: 0.00381796
Iteration 6/25 | Loss: 0.00381796
Iteration 7/25 | Loss: 0.00381796
Iteration 8/25 | Loss: 0.00381796
Iteration 9/25 | Loss: 0.00381796
Iteration 10/25 | Loss: 0.00381796
Iteration 11/25 | Loss: 0.00381796
Iteration 12/25 | Loss: 0.00381796
Iteration 13/25 | Loss: 0.00381796
Iteration 14/25 | Loss: 0.00381796
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.003817962482571602, 0.003817962482571602, 0.003817962482571602, 0.003817962482571602, 0.003817962482571602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003817962482571602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00381796
Iteration 2/1000 | Loss: 0.00015629
Iteration 3/1000 | Loss: 0.00010938
Iteration 4/1000 | Loss: 0.00009197
Iteration 5/1000 | Loss: 0.00008222
Iteration 6/1000 | Loss: 0.00007758
Iteration 7/1000 | Loss: 0.00007448
Iteration 8/1000 | Loss: 0.00007193
Iteration 9/1000 | Loss: 0.00006936
Iteration 10/1000 | Loss: 0.00006801
Iteration 11/1000 | Loss: 0.00006728
Iteration 12/1000 | Loss: 0.00006665
Iteration 13/1000 | Loss: 0.00006599
Iteration 14/1000 | Loss: 0.00006540
Iteration 15/1000 | Loss: 0.00006490
Iteration 16/1000 | Loss: 0.00078956
Iteration 17/1000 | Loss: 0.00086967
Iteration 18/1000 | Loss: 0.00008236
Iteration 19/1000 | Loss: 0.00006984
Iteration 20/1000 | Loss: 0.00006592
Iteration 21/1000 | Loss: 0.00006393
Iteration 22/1000 | Loss: 0.00006231
Iteration 23/1000 | Loss: 0.00060860
Iteration 24/1000 | Loss: 0.00030803
Iteration 25/1000 | Loss: 0.00006836
Iteration 26/1000 | Loss: 0.00006310
Iteration 27/1000 | Loss: 0.00006148
Iteration 28/1000 | Loss: 0.00072673
Iteration 29/1000 | Loss: 0.00046913
Iteration 30/1000 | Loss: 0.00006160
Iteration 31/1000 | Loss: 0.00006054
Iteration 32/1000 | Loss: 0.00006000
Iteration 33/1000 | Loss: 0.00212708
Iteration 34/1000 | Loss: 0.00135788
Iteration 35/1000 | Loss: 0.00073274
Iteration 36/1000 | Loss: 0.00006039
Iteration 37/1000 | Loss: 0.00005705
Iteration 38/1000 | Loss: 0.00005554
Iteration 39/1000 | Loss: 0.00039715
Iteration 40/1000 | Loss: 0.00033270
Iteration 41/1000 | Loss: 0.00005455
Iteration 42/1000 | Loss: 0.00122717
Iteration 43/1000 | Loss: 0.00052023
Iteration 44/1000 | Loss: 0.00034150
Iteration 45/1000 | Loss: 0.00037640
Iteration 46/1000 | Loss: 0.00033432
Iteration 47/1000 | Loss: 0.00266533
Iteration 48/1000 | Loss: 0.00199715
Iteration 49/1000 | Loss: 0.00108483
Iteration 50/1000 | Loss: 0.00006484
Iteration 51/1000 | Loss: 0.00005328
Iteration 52/1000 | Loss: 0.00004890
Iteration 53/1000 | Loss: 0.00004670
Iteration 54/1000 | Loss: 0.00004448
Iteration 55/1000 | Loss: 0.00124331
Iteration 56/1000 | Loss: 0.00384220
Iteration 57/1000 | Loss: 0.00015861
Iteration 58/1000 | Loss: 0.00008086
Iteration 59/1000 | Loss: 0.00005423
Iteration 60/1000 | Loss: 0.00004381
Iteration 61/1000 | Loss: 0.00003755
Iteration 62/1000 | Loss: 0.00070195
Iteration 63/1000 | Loss: 0.00066879
Iteration 64/1000 | Loss: 0.00004589
Iteration 65/1000 | Loss: 0.00022792
Iteration 66/1000 | Loss: 0.00046782
Iteration 67/1000 | Loss: 0.00044686
Iteration 68/1000 | Loss: 0.00074147
Iteration 69/1000 | Loss: 0.00015273
Iteration 70/1000 | Loss: 0.00067155
Iteration 71/1000 | Loss: 0.00047934
Iteration 72/1000 | Loss: 0.00004625
Iteration 73/1000 | Loss: 0.00024566
Iteration 74/1000 | Loss: 0.00022244
Iteration 75/1000 | Loss: 0.00011421
Iteration 76/1000 | Loss: 0.00029869
Iteration 77/1000 | Loss: 0.00030084
Iteration 78/1000 | Loss: 0.00050450
Iteration 79/1000 | Loss: 0.00047887
Iteration 80/1000 | Loss: 0.00072007
Iteration 81/1000 | Loss: 0.00008228
Iteration 82/1000 | Loss: 0.00032278
Iteration 83/1000 | Loss: 0.00072517
Iteration 84/1000 | Loss: 0.00085727
Iteration 85/1000 | Loss: 0.00055999
Iteration 86/1000 | Loss: 0.00047209
Iteration 87/1000 | Loss: 0.00108376
Iteration 88/1000 | Loss: 0.00078281
Iteration 89/1000 | Loss: 0.00108323
Iteration 90/1000 | Loss: 0.00076944
Iteration 91/1000 | Loss: 0.00023769
Iteration 92/1000 | Loss: 0.00026086
Iteration 93/1000 | Loss: 0.00057612
Iteration 94/1000 | Loss: 0.00004533
Iteration 95/1000 | Loss: 0.00003643
Iteration 96/1000 | Loss: 0.00003280
Iteration 97/1000 | Loss: 0.00003539
Iteration 98/1000 | Loss: 0.00002974
Iteration 99/1000 | Loss: 0.00002863
Iteration 100/1000 | Loss: 0.00002778
Iteration 101/1000 | Loss: 0.00002672
Iteration 102/1000 | Loss: 0.00002543
Iteration 103/1000 | Loss: 0.00002429
Iteration 104/1000 | Loss: 0.00002343
Iteration 105/1000 | Loss: 0.00002289
Iteration 106/1000 | Loss: 0.00060600
Iteration 107/1000 | Loss: 0.00037427
Iteration 108/1000 | Loss: 0.00059379
Iteration 109/1000 | Loss: 0.00026096
Iteration 110/1000 | Loss: 0.00023890
Iteration 111/1000 | Loss: 0.00072736
Iteration 112/1000 | Loss: 0.00020305
Iteration 113/1000 | Loss: 0.00047078
Iteration 114/1000 | Loss: 0.00004567
Iteration 115/1000 | Loss: 0.00033716
Iteration 116/1000 | Loss: 0.00047867
Iteration 117/1000 | Loss: 0.00010015
Iteration 118/1000 | Loss: 0.00007836
Iteration 119/1000 | Loss: 0.00005606
Iteration 120/1000 | Loss: 0.00005054
Iteration 121/1000 | Loss: 0.00005610
Iteration 122/1000 | Loss: 0.00003188
Iteration 123/1000 | Loss: 0.00071200
Iteration 124/1000 | Loss: 0.00009233
Iteration 125/1000 | Loss: 0.00006463
Iteration 126/1000 | Loss: 0.00003163
Iteration 127/1000 | Loss: 0.00007054
Iteration 128/1000 | Loss: 0.00002468
Iteration 129/1000 | Loss: 0.00002136
Iteration 130/1000 | Loss: 0.00002048
Iteration 131/1000 | Loss: 0.00001984
Iteration 132/1000 | Loss: 0.00001894
Iteration 133/1000 | Loss: 0.00001828
Iteration 134/1000 | Loss: 0.00001782
Iteration 135/1000 | Loss: 0.00001751
Iteration 136/1000 | Loss: 0.00001724
Iteration 137/1000 | Loss: 0.00001704
Iteration 138/1000 | Loss: 0.00001702
Iteration 139/1000 | Loss: 0.00001690
Iteration 140/1000 | Loss: 0.00001688
Iteration 141/1000 | Loss: 0.00001688
Iteration 142/1000 | Loss: 0.00001687
Iteration 143/1000 | Loss: 0.00001687
Iteration 144/1000 | Loss: 0.00001684
Iteration 145/1000 | Loss: 0.00001683
Iteration 146/1000 | Loss: 0.00001682
Iteration 147/1000 | Loss: 0.00001681
Iteration 148/1000 | Loss: 0.00001681
Iteration 149/1000 | Loss: 0.00001677
Iteration 150/1000 | Loss: 0.00001676
Iteration 151/1000 | Loss: 0.00001676
Iteration 152/1000 | Loss: 0.00001675
Iteration 153/1000 | Loss: 0.00001675
Iteration 154/1000 | Loss: 0.00001675
Iteration 155/1000 | Loss: 0.00001675
Iteration 156/1000 | Loss: 0.00001675
Iteration 157/1000 | Loss: 0.00001675
Iteration 158/1000 | Loss: 0.00001675
Iteration 159/1000 | Loss: 0.00001675
Iteration 160/1000 | Loss: 0.00001675
Iteration 161/1000 | Loss: 0.00001675
Iteration 162/1000 | Loss: 0.00001675
Iteration 163/1000 | Loss: 0.00001675
Iteration 164/1000 | Loss: 0.00001675
Iteration 165/1000 | Loss: 0.00001674
Iteration 166/1000 | Loss: 0.00001674
Iteration 167/1000 | Loss: 0.00001674
Iteration 168/1000 | Loss: 0.00001674
Iteration 169/1000 | Loss: 0.00001674
Iteration 170/1000 | Loss: 0.00001674
Iteration 171/1000 | Loss: 0.00001674
Iteration 172/1000 | Loss: 0.00001673
Iteration 173/1000 | Loss: 0.00001672
Iteration 174/1000 | Loss: 0.00001672
Iteration 175/1000 | Loss: 0.00001672
Iteration 176/1000 | Loss: 0.00001671
Iteration 177/1000 | Loss: 0.00001671
Iteration 178/1000 | Loss: 0.00001670
Iteration 179/1000 | Loss: 0.00001670
Iteration 180/1000 | Loss: 0.00001670
Iteration 181/1000 | Loss: 0.00001670
Iteration 182/1000 | Loss: 0.00001670
Iteration 183/1000 | Loss: 0.00001670
Iteration 184/1000 | Loss: 0.00001670
Iteration 185/1000 | Loss: 0.00001670
Iteration 186/1000 | Loss: 0.00001669
Iteration 187/1000 | Loss: 0.00001669
Iteration 188/1000 | Loss: 0.00001669
Iteration 189/1000 | Loss: 0.00001669
Iteration 190/1000 | Loss: 0.00001669
Iteration 191/1000 | Loss: 0.00001669
Iteration 192/1000 | Loss: 0.00001669
Iteration 193/1000 | Loss: 0.00001669
Iteration 194/1000 | Loss: 0.00001669
Iteration 195/1000 | Loss: 0.00001669
Iteration 196/1000 | Loss: 0.00001669
Iteration 197/1000 | Loss: 0.00001669
Iteration 198/1000 | Loss: 0.00001669
Iteration 199/1000 | Loss: 0.00001669
Iteration 200/1000 | Loss: 0.00001669
Iteration 201/1000 | Loss: 0.00001669
Iteration 202/1000 | Loss: 0.00001668
Iteration 203/1000 | Loss: 0.00001668
Iteration 204/1000 | Loss: 0.00001668
Iteration 205/1000 | Loss: 0.00001668
Iteration 206/1000 | Loss: 0.00001668
Iteration 207/1000 | Loss: 0.00001668
Iteration 208/1000 | Loss: 0.00001668
Iteration 209/1000 | Loss: 0.00001668
Iteration 210/1000 | Loss: 0.00001668
Iteration 211/1000 | Loss: 0.00001668
Iteration 212/1000 | Loss: 0.00001668
Iteration 213/1000 | Loss: 0.00001668
Iteration 214/1000 | Loss: 0.00001668
Iteration 215/1000 | Loss: 0.00001668
Iteration 216/1000 | Loss: 0.00001668
Iteration 217/1000 | Loss: 0.00001668
Iteration 218/1000 | Loss: 0.00001668
Iteration 219/1000 | Loss: 0.00001668
Iteration 220/1000 | Loss: 0.00001668
Iteration 221/1000 | Loss: 0.00001668
Iteration 222/1000 | Loss: 0.00001667
Iteration 223/1000 | Loss: 0.00001667
Iteration 224/1000 | Loss: 0.00001667
Iteration 225/1000 | Loss: 0.00001667
Iteration 226/1000 | Loss: 0.00001667
Iteration 227/1000 | Loss: 0.00001667
Iteration 228/1000 | Loss: 0.00001667
Iteration 229/1000 | Loss: 0.00001667
Iteration 230/1000 | Loss: 0.00001667
Iteration 231/1000 | Loss: 0.00001667
Iteration 232/1000 | Loss: 0.00001667
Iteration 233/1000 | Loss: 0.00001667
Iteration 234/1000 | Loss: 0.00001667
Iteration 235/1000 | Loss: 0.00001667
Iteration 236/1000 | Loss: 0.00001667
Iteration 237/1000 | Loss: 0.00001666
Iteration 238/1000 | Loss: 0.00001666
Iteration 239/1000 | Loss: 0.00001666
Iteration 240/1000 | Loss: 0.00001666
Iteration 241/1000 | Loss: 0.00001666
Iteration 242/1000 | Loss: 0.00001666
Iteration 243/1000 | Loss: 0.00001666
Iteration 244/1000 | Loss: 0.00001666
Iteration 245/1000 | Loss: 0.00001666
Iteration 246/1000 | Loss: 0.00001666
Iteration 247/1000 | Loss: 0.00001666
Iteration 248/1000 | Loss: 0.00001666
Iteration 249/1000 | Loss: 0.00001666
Iteration 250/1000 | Loss: 0.00001666
Iteration 251/1000 | Loss: 0.00001666
Iteration 252/1000 | Loss: 0.00001666
Iteration 253/1000 | Loss: 0.00001666
Iteration 254/1000 | Loss: 0.00001666
Iteration 255/1000 | Loss: 0.00001666
Iteration 256/1000 | Loss: 0.00001666
Iteration 257/1000 | Loss: 0.00001666
Iteration 258/1000 | Loss: 0.00001665
Iteration 259/1000 | Loss: 0.00001665
Iteration 260/1000 | Loss: 0.00001665
Iteration 261/1000 | Loss: 0.00001665
Iteration 262/1000 | Loss: 0.00001665
Iteration 263/1000 | Loss: 0.00001665
Iteration 264/1000 | Loss: 0.00001665
Iteration 265/1000 | Loss: 0.00001665
Iteration 266/1000 | Loss: 0.00001665
Iteration 267/1000 | Loss: 0.00001665
Iteration 268/1000 | Loss: 0.00001665
Iteration 269/1000 | Loss: 0.00001665
Iteration 270/1000 | Loss: 0.00001665
Iteration 271/1000 | Loss: 0.00001665
Iteration 272/1000 | Loss: 0.00001665
Iteration 273/1000 | Loss: 0.00001665
Iteration 274/1000 | Loss: 0.00001664
Iteration 275/1000 | Loss: 0.00001664
Iteration 276/1000 | Loss: 0.00001664
Iteration 277/1000 | Loss: 0.00001664
Iteration 278/1000 | Loss: 0.00001664
Iteration 279/1000 | Loss: 0.00001664
Iteration 280/1000 | Loss: 0.00001664
Iteration 281/1000 | Loss: 0.00001664
Iteration 282/1000 | Loss: 0.00001664
Iteration 283/1000 | Loss: 0.00001664
Iteration 284/1000 | Loss: 0.00001664
Iteration 285/1000 | Loss: 0.00001664
Iteration 286/1000 | Loss: 0.00001664
Iteration 287/1000 | Loss: 0.00001664
Iteration 288/1000 | Loss: 0.00001663
Iteration 289/1000 | Loss: 0.00001663
Iteration 290/1000 | Loss: 0.00001663
Iteration 291/1000 | Loss: 0.00001663
Iteration 292/1000 | Loss: 0.00001663
Iteration 293/1000 | Loss: 0.00001663
Iteration 294/1000 | Loss: 0.00001663
Iteration 295/1000 | Loss: 0.00001663
Iteration 296/1000 | Loss: 0.00001663
Iteration 297/1000 | Loss: 0.00001663
Iteration 298/1000 | Loss: 0.00001663
Iteration 299/1000 | Loss: 0.00001663
Iteration 300/1000 | Loss: 0.00001663
Iteration 301/1000 | Loss: 0.00001663
Iteration 302/1000 | Loss: 0.00001663
Iteration 303/1000 | Loss: 0.00001663
Iteration 304/1000 | Loss: 0.00001663
Iteration 305/1000 | Loss: 0.00001663
Iteration 306/1000 | Loss: 0.00001663
Iteration 307/1000 | Loss: 0.00001663
Iteration 308/1000 | Loss: 0.00001663
Iteration 309/1000 | Loss: 0.00001663
Iteration 310/1000 | Loss: 0.00001663
Iteration 311/1000 | Loss: 0.00001663
Iteration 312/1000 | Loss: 0.00001663
Iteration 313/1000 | Loss: 0.00001663
Iteration 314/1000 | Loss: 0.00001663
Iteration 315/1000 | Loss: 0.00001663
Iteration 316/1000 | Loss: 0.00001663
Iteration 317/1000 | Loss: 0.00001663
Iteration 318/1000 | Loss: 0.00001663
Iteration 319/1000 | Loss: 0.00001663
Iteration 320/1000 | Loss: 0.00001663
Iteration 321/1000 | Loss: 0.00001663
Iteration 322/1000 | Loss: 0.00001663
Iteration 323/1000 | Loss: 0.00001663
Iteration 324/1000 | Loss: 0.00001663
Iteration 325/1000 | Loss: 0.00001663
Iteration 326/1000 | Loss: 0.00001663
Iteration 327/1000 | Loss: 0.00001663
Iteration 328/1000 | Loss: 0.00001663
Iteration 329/1000 | Loss: 0.00001663
Iteration 330/1000 | Loss: 0.00001663
Iteration 331/1000 | Loss: 0.00001663
Iteration 332/1000 | Loss: 0.00001663
Iteration 333/1000 | Loss: 0.00001663
Iteration 334/1000 | Loss: 0.00001663
Iteration 335/1000 | Loss: 0.00001663
Iteration 336/1000 | Loss: 0.00001663
Iteration 337/1000 | Loss: 0.00001663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 337. Stopping optimization.
Last 5 losses: [1.662995600781869e-05, 1.662995600781869e-05, 1.662995600781869e-05, 1.662995600781869e-05, 1.662995600781869e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.662995600781869e-05

Optimization complete. Final v2v error: 3.1585206985473633 mm

Highest mean error: 11.346231460571289 mm for frame 83

Lowest mean error: 2.5471789836883545 mm for frame 39

Saving results

Total time: 262.0873453617096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971853
Iteration 2/25 | Loss: 0.00118804
Iteration 3/25 | Loss: 0.00100270
Iteration 4/25 | Loss: 0.00096788
Iteration 5/25 | Loss: 0.00095473
Iteration 6/25 | Loss: 0.00095083
Iteration 7/25 | Loss: 0.00094919
Iteration 8/25 | Loss: 0.00094880
Iteration 9/25 | Loss: 0.00094880
Iteration 10/25 | Loss: 0.00094880
Iteration 11/25 | Loss: 0.00094880
Iteration 12/25 | Loss: 0.00094880
Iteration 13/25 | Loss: 0.00094880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009487970964983106, 0.0009487970964983106, 0.0009487970964983106, 0.0009487970964983106, 0.0009487970964983106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009487970964983106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61592376
Iteration 2/25 | Loss: 0.00259742
Iteration 3/25 | Loss: 0.00259723
Iteration 4/25 | Loss: 0.00259723
Iteration 5/25 | Loss: 0.00259723
Iteration 6/25 | Loss: 0.00259723
Iteration 7/25 | Loss: 0.00259723
Iteration 8/25 | Loss: 0.00259723
Iteration 9/25 | Loss: 0.00259723
Iteration 10/25 | Loss: 0.00259723
Iteration 11/25 | Loss: 0.00259723
Iteration 12/25 | Loss: 0.00259723
Iteration 13/25 | Loss: 0.00259723
Iteration 14/25 | Loss: 0.00259723
Iteration 15/25 | Loss: 0.00259723
Iteration 16/25 | Loss: 0.00259723
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0025972253642976284, 0.0025972253642976284, 0.0025972253642976284, 0.0025972253642976284, 0.0025972253642976284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025972253642976284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259723
Iteration 2/1000 | Loss: 0.00006642
Iteration 3/1000 | Loss: 0.00004151
Iteration 4/1000 | Loss: 0.00003630
Iteration 5/1000 | Loss: 0.00003441
Iteration 6/1000 | Loss: 0.00003331
Iteration 7/1000 | Loss: 0.00003260
Iteration 8/1000 | Loss: 0.00003198
Iteration 9/1000 | Loss: 0.00003150
Iteration 10/1000 | Loss: 0.00003108
Iteration 11/1000 | Loss: 0.00003057
Iteration 12/1000 | Loss: 0.00003023
Iteration 13/1000 | Loss: 0.00002988
Iteration 14/1000 | Loss: 0.00002961
Iteration 15/1000 | Loss: 0.00002936
Iteration 16/1000 | Loss: 0.00002919
Iteration 17/1000 | Loss: 0.00002903
Iteration 18/1000 | Loss: 0.00002896
Iteration 19/1000 | Loss: 0.00002896
Iteration 20/1000 | Loss: 0.00002892
Iteration 21/1000 | Loss: 0.00002891
Iteration 22/1000 | Loss: 0.00002890
Iteration 23/1000 | Loss: 0.00002889
Iteration 24/1000 | Loss: 0.00002889
Iteration 25/1000 | Loss: 0.00002886
Iteration 26/1000 | Loss: 0.00002886
Iteration 27/1000 | Loss: 0.00002886
Iteration 28/1000 | Loss: 0.00002885
Iteration 29/1000 | Loss: 0.00002885
Iteration 30/1000 | Loss: 0.00002884
Iteration 31/1000 | Loss: 0.00002883
Iteration 32/1000 | Loss: 0.00002883
Iteration 33/1000 | Loss: 0.00002881
Iteration 34/1000 | Loss: 0.00002878
Iteration 35/1000 | Loss: 0.00002878
Iteration 36/1000 | Loss: 0.00002877
Iteration 37/1000 | Loss: 0.00002875
Iteration 38/1000 | Loss: 0.00002875
Iteration 39/1000 | Loss: 0.00002874
Iteration 40/1000 | Loss: 0.00002874
Iteration 41/1000 | Loss: 0.00002874
Iteration 42/1000 | Loss: 0.00002873
Iteration 43/1000 | Loss: 0.00002873
Iteration 44/1000 | Loss: 0.00002873
Iteration 45/1000 | Loss: 0.00002872
Iteration 46/1000 | Loss: 0.00002872
Iteration 47/1000 | Loss: 0.00002872
Iteration 48/1000 | Loss: 0.00002872
Iteration 49/1000 | Loss: 0.00002871
Iteration 50/1000 | Loss: 0.00002871
Iteration 51/1000 | Loss: 0.00002871
Iteration 52/1000 | Loss: 0.00002870
Iteration 53/1000 | Loss: 0.00002870
Iteration 54/1000 | Loss: 0.00002870
Iteration 55/1000 | Loss: 0.00002870
Iteration 56/1000 | Loss: 0.00002870
Iteration 57/1000 | Loss: 0.00002869
Iteration 58/1000 | Loss: 0.00002869
Iteration 59/1000 | Loss: 0.00002869
Iteration 60/1000 | Loss: 0.00002869
Iteration 61/1000 | Loss: 0.00002869
Iteration 62/1000 | Loss: 0.00002869
Iteration 63/1000 | Loss: 0.00002868
Iteration 64/1000 | Loss: 0.00002868
Iteration 65/1000 | Loss: 0.00002868
Iteration 66/1000 | Loss: 0.00002868
Iteration 67/1000 | Loss: 0.00002868
Iteration 68/1000 | Loss: 0.00002868
Iteration 69/1000 | Loss: 0.00002868
Iteration 70/1000 | Loss: 0.00002868
Iteration 71/1000 | Loss: 0.00002868
Iteration 72/1000 | Loss: 0.00002868
Iteration 73/1000 | Loss: 0.00002867
Iteration 74/1000 | Loss: 0.00002867
Iteration 75/1000 | Loss: 0.00002867
Iteration 76/1000 | Loss: 0.00002866
Iteration 77/1000 | Loss: 0.00002866
Iteration 78/1000 | Loss: 0.00002866
Iteration 79/1000 | Loss: 0.00002865
Iteration 80/1000 | Loss: 0.00002864
Iteration 81/1000 | Loss: 0.00002864
Iteration 82/1000 | Loss: 0.00002864
Iteration 83/1000 | Loss: 0.00002864
Iteration 84/1000 | Loss: 0.00002863
Iteration 85/1000 | Loss: 0.00002862
Iteration 86/1000 | Loss: 0.00002862
Iteration 87/1000 | Loss: 0.00002861
Iteration 88/1000 | Loss: 0.00002861
Iteration 89/1000 | Loss: 0.00002861
Iteration 90/1000 | Loss: 0.00002861
Iteration 91/1000 | Loss: 0.00002860
Iteration 92/1000 | Loss: 0.00002860
Iteration 93/1000 | Loss: 0.00002860
Iteration 94/1000 | Loss: 0.00002860
Iteration 95/1000 | Loss: 0.00002860
Iteration 96/1000 | Loss: 0.00002860
Iteration 97/1000 | Loss: 0.00002860
Iteration 98/1000 | Loss: 0.00002860
Iteration 99/1000 | Loss: 0.00002860
Iteration 100/1000 | Loss: 0.00002860
Iteration 101/1000 | Loss: 0.00002860
Iteration 102/1000 | Loss: 0.00002860
Iteration 103/1000 | Loss: 0.00002859
Iteration 104/1000 | Loss: 0.00002859
Iteration 105/1000 | Loss: 0.00002859
Iteration 106/1000 | Loss: 0.00002859
Iteration 107/1000 | Loss: 0.00002859
Iteration 108/1000 | Loss: 0.00002859
Iteration 109/1000 | Loss: 0.00002859
Iteration 110/1000 | Loss: 0.00002859
Iteration 111/1000 | Loss: 0.00002859
Iteration 112/1000 | Loss: 0.00002859
Iteration 113/1000 | Loss: 0.00002859
Iteration 114/1000 | Loss: 0.00002859
Iteration 115/1000 | Loss: 0.00002859
Iteration 116/1000 | Loss: 0.00002859
Iteration 117/1000 | Loss: 0.00002858
Iteration 118/1000 | Loss: 0.00002858
Iteration 119/1000 | Loss: 0.00002858
Iteration 120/1000 | Loss: 0.00002858
Iteration 121/1000 | Loss: 0.00002858
Iteration 122/1000 | Loss: 0.00002858
Iteration 123/1000 | Loss: 0.00002857
Iteration 124/1000 | Loss: 0.00002857
Iteration 125/1000 | Loss: 0.00002857
Iteration 126/1000 | Loss: 0.00002857
Iteration 127/1000 | Loss: 0.00002856
Iteration 128/1000 | Loss: 0.00002856
Iteration 129/1000 | Loss: 0.00002855
Iteration 130/1000 | Loss: 0.00002855
Iteration 131/1000 | Loss: 0.00002855
Iteration 132/1000 | Loss: 0.00002855
Iteration 133/1000 | Loss: 0.00002855
Iteration 134/1000 | Loss: 0.00002855
Iteration 135/1000 | Loss: 0.00002855
Iteration 136/1000 | Loss: 0.00002854
Iteration 137/1000 | Loss: 0.00002854
Iteration 138/1000 | Loss: 0.00002854
Iteration 139/1000 | Loss: 0.00002853
Iteration 140/1000 | Loss: 0.00002853
Iteration 141/1000 | Loss: 0.00002853
Iteration 142/1000 | Loss: 0.00002853
Iteration 143/1000 | Loss: 0.00002853
Iteration 144/1000 | Loss: 0.00002852
Iteration 145/1000 | Loss: 0.00002852
Iteration 146/1000 | Loss: 0.00002852
Iteration 147/1000 | Loss: 0.00002852
Iteration 148/1000 | Loss: 0.00002852
Iteration 149/1000 | Loss: 0.00002852
Iteration 150/1000 | Loss: 0.00002852
Iteration 151/1000 | Loss: 0.00002852
Iteration 152/1000 | Loss: 0.00002851
Iteration 153/1000 | Loss: 0.00002851
Iteration 154/1000 | Loss: 0.00002851
Iteration 155/1000 | Loss: 0.00002851
Iteration 156/1000 | Loss: 0.00002850
Iteration 157/1000 | Loss: 0.00002850
Iteration 158/1000 | Loss: 0.00002850
Iteration 159/1000 | Loss: 0.00002850
Iteration 160/1000 | Loss: 0.00002850
Iteration 161/1000 | Loss: 0.00002850
Iteration 162/1000 | Loss: 0.00002849
Iteration 163/1000 | Loss: 0.00002849
Iteration 164/1000 | Loss: 0.00002849
Iteration 165/1000 | Loss: 0.00002849
Iteration 166/1000 | Loss: 0.00002849
Iteration 167/1000 | Loss: 0.00002849
Iteration 168/1000 | Loss: 0.00002849
Iteration 169/1000 | Loss: 0.00002849
Iteration 170/1000 | Loss: 0.00002849
Iteration 171/1000 | Loss: 0.00002849
Iteration 172/1000 | Loss: 0.00002848
Iteration 173/1000 | Loss: 0.00002848
Iteration 174/1000 | Loss: 0.00002848
Iteration 175/1000 | Loss: 0.00002848
Iteration 176/1000 | Loss: 0.00002848
Iteration 177/1000 | Loss: 0.00002848
Iteration 178/1000 | Loss: 0.00002848
Iteration 179/1000 | Loss: 0.00002848
Iteration 180/1000 | Loss: 0.00002848
Iteration 181/1000 | Loss: 0.00002848
Iteration 182/1000 | Loss: 0.00002848
Iteration 183/1000 | Loss: 0.00002848
Iteration 184/1000 | Loss: 0.00002848
Iteration 185/1000 | Loss: 0.00002848
Iteration 186/1000 | Loss: 0.00002848
Iteration 187/1000 | Loss: 0.00002848
Iteration 188/1000 | Loss: 0.00002847
Iteration 189/1000 | Loss: 0.00002847
Iteration 190/1000 | Loss: 0.00002847
Iteration 191/1000 | Loss: 0.00002847
Iteration 192/1000 | Loss: 0.00002847
Iteration 193/1000 | Loss: 0.00002847
Iteration 194/1000 | Loss: 0.00002847
Iteration 195/1000 | Loss: 0.00002847
Iteration 196/1000 | Loss: 0.00002847
Iteration 197/1000 | Loss: 0.00002846
Iteration 198/1000 | Loss: 0.00002846
Iteration 199/1000 | Loss: 0.00002846
Iteration 200/1000 | Loss: 0.00002846
Iteration 201/1000 | Loss: 0.00002845
Iteration 202/1000 | Loss: 0.00002845
Iteration 203/1000 | Loss: 0.00002845
Iteration 204/1000 | Loss: 0.00002845
Iteration 205/1000 | Loss: 0.00002845
Iteration 206/1000 | Loss: 0.00002844
Iteration 207/1000 | Loss: 0.00002844
Iteration 208/1000 | Loss: 0.00002844
Iteration 209/1000 | Loss: 0.00002844
Iteration 210/1000 | Loss: 0.00002844
Iteration 211/1000 | Loss: 0.00002844
Iteration 212/1000 | Loss: 0.00002844
Iteration 213/1000 | Loss: 0.00002844
Iteration 214/1000 | Loss: 0.00002844
Iteration 215/1000 | Loss: 0.00002844
Iteration 216/1000 | Loss: 0.00002844
Iteration 217/1000 | Loss: 0.00002844
Iteration 218/1000 | Loss: 0.00002843
Iteration 219/1000 | Loss: 0.00002843
Iteration 220/1000 | Loss: 0.00002843
Iteration 221/1000 | Loss: 0.00002843
Iteration 222/1000 | Loss: 0.00002843
Iteration 223/1000 | Loss: 0.00002843
Iteration 224/1000 | Loss: 0.00002842
Iteration 225/1000 | Loss: 0.00002842
Iteration 226/1000 | Loss: 0.00002842
Iteration 227/1000 | Loss: 0.00002842
Iteration 228/1000 | Loss: 0.00002842
Iteration 229/1000 | Loss: 0.00002842
Iteration 230/1000 | Loss: 0.00002841
Iteration 231/1000 | Loss: 0.00002841
Iteration 232/1000 | Loss: 0.00002841
Iteration 233/1000 | Loss: 0.00002841
Iteration 234/1000 | Loss: 0.00002841
Iteration 235/1000 | Loss: 0.00002841
Iteration 236/1000 | Loss: 0.00002841
Iteration 237/1000 | Loss: 0.00002841
Iteration 238/1000 | Loss: 0.00002841
Iteration 239/1000 | Loss: 0.00002840
Iteration 240/1000 | Loss: 0.00002840
Iteration 241/1000 | Loss: 0.00002840
Iteration 242/1000 | Loss: 0.00002840
Iteration 243/1000 | Loss: 0.00002840
Iteration 244/1000 | Loss: 0.00002840
Iteration 245/1000 | Loss: 0.00002840
Iteration 246/1000 | Loss: 0.00002840
Iteration 247/1000 | Loss: 0.00002840
Iteration 248/1000 | Loss: 0.00002840
Iteration 249/1000 | Loss: 0.00002840
Iteration 250/1000 | Loss: 0.00002840
Iteration 251/1000 | Loss: 0.00002840
Iteration 252/1000 | Loss: 0.00002840
Iteration 253/1000 | Loss: 0.00002840
Iteration 254/1000 | Loss: 0.00002840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [2.840484921762254e-05, 2.840484921762254e-05, 2.840484921762254e-05, 2.840484921762254e-05, 2.840484921762254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.840484921762254e-05

Optimization complete. Final v2v error: 3.991081953048706 mm

Highest mean error: 5.909906387329102 mm for frame 86

Lowest mean error: 2.865752696990967 mm for frame 147

Saving results

Total time: 54.88939452171326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824897
Iteration 2/25 | Loss: 0.00144016
Iteration 3/25 | Loss: 0.00104871
Iteration 4/25 | Loss: 0.00100454
Iteration 5/25 | Loss: 0.00099399
Iteration 6/25 | Loss: 0.00099230
Iteration 7/25 | Loss: 0.00099229
Iteration 8/25 | Loss: 0.00099229
Iteration 9/25 | Loss: 0.00099229
Iteration 10/25 | Loss: 0.00099229
Iteration 11/25 | Loss: 0.00099229
Iteration 12/25 | Loss: 0.00099229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009922904428094625, 0.0009922904428094625, 0.0009922904428094625, 0.0009922904428094625, 0.0009922904428094625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009922904428094625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49718475
Iteration 2/25 | Loss: 0.00320481
Iteration 3/25 | Loss: 0.00320477
Iteration 4/25 | Loss: 0.00320477
Iteration 5/25 | Loss: 0.00320477
Iteration 6/25 | Loss: 0.00320477
Iteration 7/25 | Loss: 0.00320477
Iteration 8/25 | Loss: 0.00320477
Iteration 9/25 | Loss: 0.00320477
Iteration 10/25 | Loss: 0.00320477
Iteration 11/25 | Loss: 0.00320477
Iteration 12/25 | Loss: 0.00320477
Iteration 13/25 | Loss: 0.00320477
Iteration 14/25 | Loss: 0.00320477
Iteration 15/25 | Loss: 0.00320477
Iteration 16/25 | Loss: 0.00320477
Iteration 17/25 | Loss: 0.00320477
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003204766195267439, 0.003204766195267439, 0.003204766195267439, 0.003204766195267439, 0.003204766195267439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003204766195267439

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00320477
Iteration 2/1000 | Loss: 0.00003871
Iteration 3/1000 | Loss: 0.00002840
Iteration 4/1000 | Loss: 0.00002525
Iteration 5/1000 | Loss: 0.00002418
Iteration 6/1000 | Loss: 0.00002355
Iteration 7/1000 | Loss: 0.00002307
Iteration 8/1000 | Loss: 0.00002261
Iteration 9/1000 | Loss: 0.00002218
Iteration 10/1000 | Loss: 0.00002188
Iteration 11/1000 | Loss: 0.00002166
Iteration 12/1000 | Loss: 0.00002153
Iteration 13/1000 | Loss: 0.00002145
Iteration 14/1000 | Loss: 0.00002142
Iteration 15/1000 | Loss: 0.00002139
Iteration 16/1000 | Loss: 0.00002138
Iteration 17/1000 | Loss: 0.00002138
Iteration 18/1000 | Loss: 0.00002138
Iteration 19/1000 | Loss: 0.00002138
Iteration 20/1000 | Loss: 0.00002137
Iteration 21/1000 | Loss: 0.00002137
Iteration 22/1000 | Loss: 0.00002137
Iteration 23/1000 | Loss: 0.00002137
Iteration 24/1000 | Loss: 0.00002137
Iteration 25/1000 | Loss: 0.00002137
Iteration 26/1000 | Loss: 0.00002137
Iteration 27/1000 | Loss: 0.00002137
Iteration 28/1000 | Loss: 0.00002137
Iteration 29/1000 | Loss: 0.00002136
Iteration 30/1000 | Loss: 0.00002136
Iteration 31/1000 | Loss: 0.00002136
Iteration 32/1000 | Loss: 0.00002136
Iteration 33/1000 | Loss: 0.00002136
Iteration 34/1000 | Loss: 0.00002136
Iteration 35/1000 | Loss: 0.00002136
Iteration 36/1000 | Loss: 0.00002136
Iteration 37/1000 | Loss: 0.00002136
Iteration 38/1000 | Loss: 0.00002135
Iteration 39/1000 | Loss: 0.00002135
Iteration 40/1000 | Loss: 0.00002134
Iteration 41/1000 | Loss: 0.00002134
Iteration 42/1000 | Loss: 0.00002134
Iteration 43/1000 | Loss: 0.00002134
Iteration 44/1000 | Loss: 0.00002133
Iteration 45/1000 | Loss: 0.00002133
Iteration 46/1000 | Loss: 0.00002133
Iteration 47/1000 | Loss: 0.00002133
Iteration 48/1000 | Loss: 0.00002133
Iteration 49/1000 | Loss: 0.00002133
Iteration 50/1000 | Loss: 0.00002132
Iteration 51/1000 | Loss: 0.00002132
Iteration 52/1000 | Loss: 0.00002132
Iteration 53/1000 | Loss: 0.00002132
Iteration 54/1000 | Loss: 0.00002131
Iteration 55/1000 | Loss: 0.00002131
Iteration 56/1000 | Loss: 0.00002131
Iteration 57/1000 | Loss: 0.00002131
Iteration 58/1000 | Loss: 0.00002130
Iteration 59/1000 | Loss: 0.00002130
Iteration 60/1000 | Loss: 0.00002130
Iteration 61/1000 | Loss: 0.00002129
Iteration 62/1000 | Loss: 0.00002129
Iteration 63/1000 | Loss: 0.00002129
Iteration 64/1000 | Loss: 0.00002129
Iteration 65/1000 | Loss: 0.00002128
Iteration 66/1000 | Loss: 0.00002128
Iteration 67/1000 | Loss: 0.00002128
Iteration 68/1000 | Loss: 0.00002128
Iteration 69/1000 | Loss: 0.00002127
Iteration 70/1000 | Loss: 0.00002127
Iteration 71/1000 | Loss: 0.00002127
Iteration 72/1000 | Loss: 0.00002127
Iteration 73/1000 | Loss: 0.00002127
Iteration 74/1000 | Loss: 0.00002127
Iteration 75/1000 | Loss: 0.00002126
Iteration 76/1000 | Loss: 0.00002126
Iteration 77/1000 | Loss: 0.00002126
Iteration 78/1000 | Loss: 0.00002126
Iteration 79/1000 | Loss: 0.00002126
Iteration 80/1000 | Loss: 0.00002126
Iteration 81/1000 | Loss: 0.00002126
Iteration 82/1000 | Loss: 0.00002126
Iteration 83/1000 | Loss: 0.00002125
Iteration 84/1000 | Loss: 0.00002125
Iteration 85/1000 | Loss: 0.00002125
Iteration 86/1000 | Loss: 0.00002125
Iteration 87/1000 | Loss: 0.00002125
Iteration 88/1000 | Loss: 0.00002125
Iteration 89/1000 | Loss: 0.00002125
Iteration 90/1000 | Loss: 0.00002125
Iteration 91/1000 | Loss: 0.00002125
Iteration 92/1000 | Loss: 0.00002125
Iteration 93/1000 | Loss: 0.00002125
Iteration 94/1000 | Loss: 0.00002124
Iteration 95/1000 | Loss: 0.00002124
Iteration 96/1000 | Loss: 0.00002124
Iteration 97/1000 | Loss: 0.00002124
Iteration 98/1000 | Loss: 0.00002124
Iteration 99/1000 | Loss: 0.00002124
Iteration 100/1000 | Loss: 0.00002124
Iteration 101/1000 | Loss: 0.00002124
Iteration 102/1000 | Loss: 0.00002123
Iteration 103/1000 | Loss: 0.00002123
Iteration 104/1000 | Loss: 0.00002123
Iteration 105/1000 | Loss: 0.00002123
Iteration 106/1000 | Loss: 0.00002123
Iteration 107/1000 | Loss: 0.00002123
Iteration 108/1000 | Loss: 0.00002123
Iteration 109/1000 | Loss: 0.00002123
Iteration 110/1000 | Loss: 0.00002123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.12321992876241e-05, 2.12321992876241e-05, 2.12321992876241e-05, 2.12321992876241e-05, 2.12321992876241e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.12321992876241e-05

Optimization complete. Final v2v error: 3.9178030490875244 mm

Highest mean error: 4.776923656463623 mm for frame 223

Lowest mean error: 3.324434518814087 mm for frame 15

Saving results

Total time: 38.7939190864563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844789
Iteration 2/25 | Loss: 0.00121604
Iteration 3/25 | Loss: 0.00102219
Iteration 4/25 | Loss: 0.00097938
Iteration 5/25 | Loss: 0.00096465
Iteration 6/25 | Loss: 0.00096055
Iteration 7/25 | Loss: 0.00095916
Iteration 8/25 | Loss: 0.00095897
Iteration 9/25 | Loss: 0.00095897
Iteration 10/25 | Loss: 0.00095897
Iteration 11/25 | Loss: 0.00095897
Iteration 12/25 | Loss: 0.00095897
Iteration 13/25 | Loss: 0.00095897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009589724941179156, 0.0009589724941179156, 0.0009589724941179156, 0.0009589724941179156, 0.0009589724941179156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009589724941179156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74262047
Iteration 2/25 | Loss: 0.00409285
Iteration 3/25 | Loss: 0.00409283
Iteration 4/25 | Loss: 0.00409283
Iteration 5/25 | Loss: 0.00409283
Iteration 6/25 | Loss: 0.00409283
Iteration 7/25 | Loss: 0.00409283
Iteration 8/25 | Loss: 0.00409283
Iteration 9/25 | Loss: 0.00409283
Iteration 10/25 | Loss: 0.00409283
Iteration 11/25 | Loss: 0.00409283
Iteration 12/25 | Loss: 0.00409283
Iteration 13/25 | Loss: 0.00409283
Iteration 14/25 | Loss: 0.00409283
Iteration 15/25 | Loss: 0.00409283
Iteration 16/25 | Loss: 0.00409283
Iteration 17/25 | Loss: 0.00409283
Iteration 18/25 | Loss: 0.00409283
Iteration 19/25 | Loss: 0.00409283
Iteration 20/25 | Loss: 0.00409283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004092828836292028, 0.004092828836292028, 0.004092828836292028, 0.004092828836292028, 0.004092828836292028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004092828836292028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00409283
Iteration 2/1000 | Loss: 0.00004626
Iteration 3/1000 | Loss: 0.00003601
Iteration 4/1000 | Loss: 0.00003083
Iteration 5/1000 | Loss: 0.00002901
Iteration 6/1000 | Loss: 0.00002784
Iteration 7/1000 | Loss: 0.00002701
Iteration 8/1000 | Loss: 0.00002631
Iteration 9/1000 | Loss: 0.00002573
Iteration 10/1000 | Loss: 0.00002545
Iteration 11/1000 | Loss: 0.00002527
Iteration 12/1000 | Loss: 0.00002506
Iteration 13/1000 | Loss: 0.00002502
Iteration 14/1000 | Loss: 0.00002495
Iteration 15/1000 | Loss: 0.00002494
Iteration 16/1000 | Loss: 0.00002491
Iteration 17/1000 | Loss: 0.00002487
Iteration 18/1000 | Loss: 0.00002486
Iteration 19/1000 | Loss: 0.00002486
Iteration 20/1000 | Loss: 0.00002485
Iteration 21/1000 | Loss: 0.00002484
Iteration 22/1000 | Loss: 0.00002483
Iteration 23/1000 | Loss: 0.00002482
Iteration 24/1000 | Loss: 0.00002481
Iteration 25/1000 | Loss: 0.00002478
Iteration 26/1000 | Loss: 0.00002478
Iteration 27/1000 | Loss: 0.00002477
Iteration 28/1000 | Loss: 0.00002476
Iteration 29/1000 | Loss: 0.00002476
Iteration 30/1000 | Loss: 0.00002476
Iteration 31/1000 | Loss: 0.00002475
Iteration 32/1000 | Loss: 0.00002475
Iteration 33/1000 | Loss: 0.00002474
Iteration 34/1000 | Loss: 0.00002474
Iteration 35/1000 | Loss: 0.00002474
Iteration 36/1000 | Loss: 0.00002474
Iteration 37/1000 | Loss: 0.00002474
Iteration 38/1000 | Loss: 0.00002474
Iteration 39/1000 | Loss: 0.00002474
Iteration 40/1000 | Loss: 0.00002473
Iteration 41/1000 | Loss: 0.00002473
Iteration 42/1000 | Loss: 0.00002473
Iteration 43/1000 | Loss: 0.00002472
Iteration 44/1000 | Loss: 0.00002472
Iteration 45/1000 | Loss: 0.00002472
Iteration 46/1000 | Loss: 0.00002471
Iteration 47/1000 | Loss: 0.00002471
Iteration 48/1000 | Loss: 0.00002471
Iteration 49/1000 | Loss: 0.00002471
Iteration 50/1000 | Loss: 0.00002471
Iteration 51/1000 | Loss: 0.00002471
Iteration 52/1000 | Loss: 0.00002471
Iteration 53/1000 | Loss: 0.00002471
Iteration 54/1000 | Loss: 0.00002471
Iteration 55/1000 | Loss: 0.00002471
Iteration 56/1000 | Loss: 0.00002471
Iteration 57/1000 | Loss: 0.00002471
Iteration 58/1000 | Loss: 0.00002470
Iteration 59/1000 | Loss: 0.00002470
Iteration 60/1000 | Loss: 0.00002470
Iteration 61/1000 | Loss: 0.00002470
Iteration 62/1000 | Loss: 0.00002470
Iteration 63/1000 | Loss: 0.00002470
Iteration 64/1000 | Loss: 0.00002470
Iteration 65/1000 | Loss: 0.00002469
Iteration 66/1000 | Loss: 0.00002469
Iteration 67/1000 | Loss: 0.00002469
Iteration 68/1000 | Loss: 0.00002468
Iteration 69/1000 | Loss: 0.00002468
Iteration 70/1000 | Loss: 0.00002468
Iteration 71/1000 | Loss: 0.00002468
Iteration 72/1000 | Loss: 0.00002468
Iteration 73/1000 | Loss: 0.00002468
Iteration 74/1000 | Loss: 0.00002467
Iteration 75/1000 | Loss: 0.00002467
Iteration 76/1000 | Loss: 0.00002467
Iteration 77/1000 | Loss: 0.00002467
Iteration 78/1000 | Loss: 0.00002467
Iteration 79/1000 | Loss: 0.00002466
Iteration 80/1000 | Loss: 0.00002466
Iteration 81/1000 | Loss: 0.00002466
Iteration 82/1000 | Loss: 0.00002466
Iteration 83/1000 | Loss: 0.00002466
Iteration 84/1000 | Loss: 0.00002466
Iteration 85/1000 | Loss: 0.00002465
Iteration 86/1000 | Loss: 0.00002465
Iteration 87/1000 | Loss: 0.00002465
Iteration 88/1000 | Loss: 0.00002465
Iteration 89/1000 | Loss: 0.00002465
Iteration 90/1000 | Loss: 0.00002465
Iteration 91/1000 | Loss: 0.00002465
Iteration 92/1000 | Loss: 0.00002465
Iteration 93/1000 | Loss: 0.00002465
Iteration 94/1000 | Loss: 0.00002465
Iteration 95/1000 | Loss: 0.00002465
Iteration 96/1000 | Loss: 0.00002464
Iteration 97/1000 | Loss: 0.00002464
Iteration 98/1000 | Loss: 0.00002464
Iteration 99/1000 | Loss: 0.00002464
Iteration 100/1000 | Loss: 0.00002464
Iteration 101/1000 | Loss: 0.00002464
Iteration 102/1000 | Loss: 0.00002464
Iteration 103/1000 | Loss: 0.00002464
Iteration 104/1000 | Loss: 0.00002464
Iteration 105/1000 | Loss: 0.00002463
Iteration 106/1000 | Loss: 0.00002463
Iteration 107/1000 | Loss: 0.00002463
Iteration 108/1000 | Loss: 0.00002463
Iteration 109/1000 | Loss: 0.00002463
Iteration 110/1000 | Loss: 0.00002463
Iteration 111/1000 | Loss: 0.00002463
Iteration 112/1000 | Loss: 0.00002463
Iteration 113/1000 | Loss: 0.00002463
Iteration 114/1000 | Loss: 0.00002463
Iteration 115/1000 | Loss: 0.00002462
Iteration 116/1000 | Loss: 0.00002462
Iteration 117/1000 | Loss: 0.00002462
Iteration 118/1000 | Loss: 0.00002462
Iteration 119/1000 | Loss: 0.00002462
Iteration 120/1000 | Loss: 0.00002462
Iteration 121/1000 | Loss: 0.00002462
Iteration 122/1000 | Loss: 0.00002462
Iteration 123/1000 | Loss: 0.00002462
Iteration 124/1000 | Loss: 0.00002462
Iteration 125/1000 | Loss: 0.00002461
Iteration 126/1000 | Loss: 0.00002461
Iteration 127/1000 | Loss: 0.00002461
Iteration 128/1000 | Loss: 0.00002461
Iteration 129/1000 | Loss: 0.00002461
Iteration 130/1000 | Loss: 0.00002461
Iteration 131/1000 | Loss: 0.00002461
Iteration 132/1000 | Loss: 0.00002461
Iteration 133/1000 | Loss: 0.00002460
Iteration 134/1000 | Loss: 0.00002460
Iteration 135/1000 | Loss: 0.00002460
Iteration 136/1000 | Loss: 0.00002460
Iteration 137/1000 | Loss: 0.00002460
Iteration 138/1000 | Loss: 0.00002459
Iteration 139/1000 | Loss: 0.00002459
Iteration 140/1000 | Loss: 0.00002459
Iteration 141/1000 | Loss: 0.00002459
Iteration 142/1000 | Loss: 0.00002459
Iteration 143/1000 | Loss: 0.00002459
Iteration 144/1000 | Loss: 0.00002459
Iteration 145/1000 | Loss: 0.00002459
Iteration 146/1000 | Loss: 0.00002459
Iteration 147/1000 | Loss: 0.00002459
Iteration 148/1000 | Loss: 0.00002459
Iteration 149/1000 | Loss: 0.00002459
Iteration 150/1000 | Loss: 0.00002459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.459285497025121e-05, 2.459285497025121e-05, 2.459285497025121e-05, 2.459285497025121e-05, 2.459285497025121e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.459285497025121e-05

Optimization complete. Final v2v error: 4.069464206695557 mm

Highest mean error: 4.7551679611206055 mm for frame 152

Lowest mean error: 3.5989608764648438 mm for frame 128

Saving results

Total time: 40.59046387672424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00637862
Iteration 2/25 | Loss: 0.00135751
Iteration 3/25 | Loss: 0.00107542
Iteration 4/25 | Loss: 0.00103632
Iteration 5/25 | Loss: 0.00100554
Iteration 6/25 | Loss: 0.00098919
Iteration 7/25 | Loss: 0.00097586
Iteration 8/25 | Loss: 0.00096556
Iteration 9/25 | Loss: 0.00097251
Iteration 10/25 | Loss: 0.00095833
Iteration 11/25 | Loss: 0.00095327
Iteration 12/25 | Loss: 0.00095245
Iteration 13/25 | Loss: 0.00095222
Iteration 14/25 | Loss: 0.00095212
Iteration 15/25 | Loss: 0.00095208
Iteration 16/25 | Loss: 0.00095207
Iteration 17/25 | Loss: 0.00095207
Iteration 18/25 | Loss: 0.00095207
Iteration 19/25 | Loss: 0.00095207
Iteration 20/25 | Loss: 0.00095207
Iteration 21/25 | Loss: 0.00095207
Iteration 22/25 | Loss: 0.00095207
Iteration 23/25 | Loss: 0.00095207
Iteration 24/25 | Loss: 0.00095206
Iteration 25/25 | Loss: 0.00095206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.19869423
Iteration 2/25 | Loss: 0.00386493
Iteration 3/25 | Loss: 0.00374347
Iteration 4/25 | Loss: 0.00374347
Iteration 5/25 | Loss: 0.00374347
Iteration 6/25 | Loss: 0.00374347
Iteration 7/25 | Loss: 0.00374347
Iteration 8/25 | Loss: 0.00374347
Iteration 9/25 | Loss: 0.00374347
Iteration 10/25 | Loss: 0.00374347
Iteration 11/25 | Loss: 0.00374347
Iteration 12/25 | Loss: 0.00374347
Iteration 13/25 | Loss: 0.00374347
Iteration 14/25 | Loss: 0.00374347
Iteration 15/25 | Loss: 0.00374347
Iteration 16/25 | Loss: 0.00374347
Iteration 17/25 | Loss: 0.00374347
Iteration 18/25 | Loss: 0.00374347
Iteration 19/25 | Loss: 0.00374347
Iteration 20/25 | Loss: 0.00374347
Iteration 21/25 | Loss: 0.00374347
Iteration 22/25 | Loss: 0.00374347
Iteration 23/25 | Loss: 0.00374347
Iteration 24/25 | Loss: 0.00374347
Iteration 25/25 | Loss: 0.00374347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00374347
Iteration 2/1000 | Loss: 0.00014998
Iteration 3/1000 | Loss: 0.00005378
Iteration 4/1000 | Loss: 0.00004177
Iteration 5/1000 | Loss: 0.00003612
Iteration 6/1000 | Loss: 0.00035996
Iteration 7/1000 | Loss: 0.00003217
Iteration 8/1000 | Loss: 0.00003076
Iteration 9/1000 | Loss: 0.00002974
Iteration 10/1000 | Loss: 0.00059072
Iteration 11/1000 | Loss: 0.00039405
Iteration 12/1000 | Loss: 0.00004319
Iteration 13/1000 | Loss: 0.00003404
Iteration 14/1000 | Loss: 0.00003061
Iteration 15/1000 | Loss: 0.00002786
Iteration 16/1000 | Loss: 0.00002614
Iteration 17/1000 | Loss: 0.00002511
Iteration 18/1000 | Loss: 0.00002437
Iteration 19/1000 | Loss: 0.00002394
Iteration 20/1000 | Loss: 0.00058909
Iteration 21/1000 | Loss: 0.00015848
Iteration 22/1000 | Loss: 0.00002535
Iteration 23/1000 | Loss: 0.00036230
Iteration 24/1000 | Loss: 0.00010600
Iteration 25/1000 | Loss: 0.00027108
Iteration 26/1000 | Loss: 0.00010208
Iteration 27/1000 | Loss: 0.00002486
Iteration 28/1000 | Loss: 0.00010198
Iteration 29/1000 | Loss: 0.00002538
Iteration 30/1000 | Loss: 0.00008380
Iteration 31/1000 | Loss: 0.00081662
Iteration 32/1000 | Loss: 0.00036026
Iteration 33/1000 | Loss: 0.00002732
Iteration 34/1000 | Loss: 0.00006515
Iteration 35/1000 | Loss: 0.00012012
Iteration 36/1000 | Loss: 0.00007555
Iteration 37/1000 | Loss: 0.00002296
Iteration 38/1000 | Loss: 0.00002215
Iteration 39/1000 | Loss: 0.00002178
Iteration 40/1000 | Loss: 0.00060673
Iteration 41/1000 | Loss: 0.00021759
Iteration 42/1000 | Loss: 0.00011008
Iteration 43/1000 | Loss: 0.00003133
Iteration 44/1000 | Loss: 0.00002148
Iteration 45/1000 | Loss: 0.00063474
Iteration 46/1000 | Loss: 0.00004224
Iteration 47/1000 | Loss: 0.00003202
Iteration 48/1000 | Loss: 0.00002879
Iteration 49/1000 | Loss: 0.00002534
Iteration 50/1000 | Loss: 0.00002366
Iteration 51/1000 | Loss: 0.00002223
Iteration 52/1000 | Loss: 0.00016564
Iteration 53/1000 | Loss: 0.00006104
Iteration 54/1000 | Loss: 0.00021989
Iteration 55/1000 | Loss: 0.00016998
Iteration 56/1000 | Loss: 0.00034259
Iteration 57/1000 | Loss: 0.00045555
Iteration 58/1000 | Loss: 0.00040299
Iteration 59/1000 | Loss: 0.00005782
Iteration 60/1000 | Loss: 0.00003079
Iteration 61/1000 | Loss: 0.00002635
Iteration 62/1000 | Loss: 0.00002350
Iteration 63/1000 | Loss: 0.00002149
Iteration 64/1000 | Loss: 0.00002006
Iteration 65/1000 | Loss: 0.00001943
Iteration 66/1000 | Loss: 0.00001900
Iteration 67/1000 | Loss: 0.00001880
Iteration 68/1000 | Loss: 0.00001863
Iteration 69/1000 | Loss: 0.00001843
Iteration 70/1000 | Loss: 0.00001837
Iteration 71/1000 | Loss: 0.00001836
Iteration 72/1000 | Loss: 0.00001833
Iteration 73/1000 | Loss: 0.00001826
Iteration 74/1000 | Loss: 0.00001826
Iteration 75/1000 | Loss: 0.00001825
Iteration 76/1000 | Loss: 0.00001825
Iteration 77/1000 | Loss: 0.00001824
Iteration 78/1000 | Loss: 0.00001824
Iteration 79/1000 | Loss: 0.00001823
Iteration 80/1000 | Loss: 0.00001823
Iteration 81/1000 | Loss: 0.00001823
Iteration 82/1000 | Loss: 0.00001822
Iteration 83/1000 | Loss: 0.00001822
Iteration 84/1000 | Loss: 0.00001821
Iteration 85/1000 | Loss: 0.00001821
Iteration 86/1000 | Loss: 0.00001821
Iteration 87/1000 | Loss: 0.00001821
Iteration 88/1000 | Loss: 0.00001820
Iteration 89/1000 | Loss: 0.00001820
Iteration 90/1000 | Loss: 0.00001820
Iteration 91/1000 | Loss: 0.00001820
Iteration 92/1000 | Loss: 0.00001820
Iteration 93/1000 | Loss: 0.00001820
Iteration 94/1000 | Loss: 0.00001820
Iteration 95/1000 | Loss: 0.00001820
Iteration 96/1000 | Loss: 0.00001820
Iteration 97/1000 | Loss: 0.00001820
Iteration 98/1000 | Loss: 0.00001820
Iteration 99/1000 | Loss: 0.00001820
Iteration 100/1000 | Loss: 0.00001819
Iteration 101/1000 | Loss: 0.00001819
Iteration 102/1000 | Loss: 0.00001819
Iteration 103/1000 | Loss: 0.00001819
Iteration 104/1000 | Loss: 0.00001819
Iteration 105/1000 | Loss: 0.00001819
Iteration 106/1000 | Loss: 0.00001819
Iteration 107/1000 | Loss: 0.00001818
Iteration 108/1000 | Loss: 0.00001818
Iteration 109/1000 | Loss: 0.00001818
Iteration 110/1000 | Loss: 0.00001818
Iteration 111/1000 | Loss: 0.00001818
Iteration 112/1000 | Loss: 0.00001818
Iteration 113/1000 | Loss: 0.00001818
Iteration 114/1000 | Loss: 0.00001818
Iteration 115/1000 | Loss: 0.00001818
Iteration 116/1000 | Loss: 0.00001818
Iteration 117/1000 | Loss: 0.00001818
Iteration 118/1000 | Loss: 0.00001818
Iteration 119/1000 | Loss: 0.00001818
Iteration 120/1000 | Loss: 0.00001818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.818462260416709e-05, 1.818462260416709e-05, 1.818462260416709e-05, 1.818462260416709e-05, 1.818462260416709e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.818462260416709e-05

Optimization complete. Final v2v error: 3.5895674228668213 mm

Highest mean error: 4.973875999450684 mm for frame 105

Lowest mean error: 2.970527410507202 mm for frame 233

Saving results

Total time: 145.1700723171234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890135
Iteration 2/25 | Loss: 0.00099012
Iteration 3/25 | Loss: 0.00084903
Iteration 4/25 | Loss: 0.00083488
Iteration 5/25 | Loss: 0.00083265
Iteration 6/25 | Loss: 0.00083265
Iteration 7/25 | Loss: 0.00083265
Iteration 8/25 | Loss: 0.00083265
Iteration 9/25 | Loss: 0.00083265
Iteration 10/25 | Loss: 0.00083265
Iteration 11/25 | Loss: 0.00083265
Iteration 12/25 | Loss: 0.00083265
Iteration 13/25 | Loss: 0.00083265
Iteration 14/25 | Loss: 0.00083265
Iteration 15/25 | Loss: 0.00083265
Iteration 16/25 | Loss: 0.00083265
Iteration 17/25 | Loss: 0.00083265
Iteration 18/25 | Loss: 0.00083265
Iteration 19/25 | Loss: 0.00083265
Iteration 20/25 | Loss: 0.00083265
Iteration 21/25 | Loss: 0.00083265
Iteration 22/25 | Loss: 0.00083265
Iteration 23/25 | Loss: 0.00083265
Iteration 24/25 | Loss: 0.00083265
Iteration 25/25 | Loss: 0.00083265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69544017
Iteration 2/25 | Loss: 0.00309521
Iteration 3/25 | Loss: 0.00309521
Iteration 4/25 | Loss: 0.00309521
Iteration 5/25 | Loss: 0.00309521
Iteration 6/25 | Loss: 0.00309521
Iteration 7/25 | Loss: 0.00309521
Iteration 8/25 | Loss: 0.00309521
Iteration 9/25 | Loss: 0.00309520
Iteration 10/25 | Loss: 0.00309520
Iteration 11/25 | Loss: 0.00309520
Iteration 12/25 | Loss: 0.00309520
Iteration 13/25 | Loss: 0.00309520
Iteration 14/25 | Loss: 0.00309520
Iteration 15/25 | Loss: 0.00309520
Iteration 16/25 | Loss: 0.00309520
Iteration 17/25 | Loss: 0.00309520
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003095204709097743, 0.003095204709097743, 0.003095204709097743, 0.003095204709097743, 0.003095204709097743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003095204709097743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00309520
Iteration 2/1000 | Loss: 0.00002654
Iteration 3/1000 | Loss: 0.00001702
Iteration 4/1000 | Loss: 0.00001426
Iteration 5/1000 | Loss: 0.00001337
Iteration 6/1000 | Loss: 0.00001279
Iteration 7/1000 | Loss: 0.00001258
Iteration 8/1000 | Loss: 0.00001234
Iteration 9/1000 | Loss: 0.00001218
Iteration 10/1000 | Loss: 0.00001214
Iteration 11/1000 | Loss: 0.00001213
Iteration 12/1000 | Loss: 0.00001213
Iteration 13/1000 | Loss: 0.00001212
Iteration 14/1000 | Loss: 0.00001210
Iteration 15/1000 | Loss: 0.00001209
Iteration 16/1000 | Loss: 0.00001208
Iteration 17/1000 | Loss: 0.00001207
Iteration 18/1000 | Loss: 0.00001205
Iteration 19/1000 | Loss: 0.00001203
Iteration 20/1000 | Loss: 0.00001191
Iteration 21/1000 | Loss: 0.00001190
Iteration 22/1000 | Loss: 0.00001190
Iteration 23/1000 | Loss: 0.00001188
Iteration 24/1000 | Loss: 0.00001186
Iteration 25/1000 | Loss: 0.00001186
Iteration 26/1000 | Loss: 0.00001184
Iteration 27/1000 | Loss: 0.00001184
Iteration 28/1000 | Loss: 0.00001184
Iteration 29/1000 | Loss: 0.00001180
Iteration 30/1000 | Loss: 0.00001179
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001176
Iteration 38/1000 | Loss: 0.00001176
Iteration 39/1000 | Loss: 0.00001176
Iteration 40/1000 | Loss: 0.00001176
Iteration 41/1000 | Loss: 0.00001176
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001175
Iteration 44/1000 | Loss: 0.00001175
Iteration 45/1000 | Loss: 0.00001175
Iteration 46/1000 | Loss: 0.00001175
Iteration 47/1000 | Loss: 0.00001174
Iteration 48/1000 | Loss: 0.00001174
Iteration 49/1000 | Loss: 0.00001173
Iteration 50/1000 | Loss: 0.00001173
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001172
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001171
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001171
Iteration 57/1000 | Loss: 0.00001171
Iteration 58/1000 | Loss: 0.00001171
Iteration 59/1000 | Loss: 0.00001171
Iteration 60/1000 | Loss: 0.00001170
Iteration 61/1000 | Loss: 0.00001170
Iteration 62/1000 | Loss: 0.00001170
Iteration 63/1000 | Loss: 0.00001169
Iteration 64/1000 | Loss: 0.00001169
Iteration 65/1000 | Loss: 0.00001169
Iteration 66/1000 | Loss: 0.00001169
Iteration 67/1000 | Loss: 0.00001169
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001168
Iteration 70/1000 | Loss: 0.00001168
Iteration 71/1000 | Loss: 0.00001168
Iteration 72/1000 | Loss: 0.00001167
Iteration 73/1000 | Loss: 0.00001167
Iteration 74/1000 | Loss: 0.00001167
Iteration 75/1000 | Loss: 0.00001167
Iteration 76/1000 | Loss: 0.00001167
Iteration 77/1000 | Loss: 0.00001167
Iteration 78/1000 | Loss: 0.00001166
Iteration 79/1000 | Loss: 0.00001166
Iteration 80/1000 | Loss: 0.00001166
Iteration 81/1000 | Loss: 0.00001166
Iteration 82/1000 | Loss: 0.00001166
Iteration 83/1000 | Loss: 0.00001166
Iteration 84/1000 | Loss: 0.00001166
Iteration 85/1000 | Loss: 0.00001166
Iteration 86/1000 | Loss: 0.00001165
Iteration 87/1000 | Loss: 0.00001165
Iteration 88/1000 | Loss: 0.00001165
Iteration 89/1000 | Loss: 0.00001165
Iteration 90/1000 | Loss: 0.00001165
Iteration 91/1000 | Loss: 0.00001164
Iteration 92/1000 | Loss: 0.00001164
Iteration 93/1000 | Loss: 0.00001164
Iteration 94/1000 | Loss: 0.00001164
Iteration 95/1000 | Loss: 0.00001164
Iteration 96/1000 | Loss: 0.00001164
Iteration 97/1000 | Loss: 0.00001164
Iteration 98/1000 | Loss: 0.00001164
Iteration 99/1000 | Loss: 0.00001164
Iteration 100/1000 | Loss: 0.00001164
Iteration 101/1000 | Loss: 0.00001164
Iteration 102/1000 | Loss: 0.00001164
Iteration 103/1000 | Loss: 0.00001164
Iteration 104/1000 | Loss: 0.00001164
Iteration 105/1000 | Loss: 0.00001163
Iteration 106/1000 | Loss: 0.00001163
Iteration 107/1000 | Loss: 0.00001163
Iteration 108/1000 | Loss: 0.00001163
Iteration 109/1000 | Loss: 0.00001163
Iteration 110/1000 | Loss: 0.00001163
Iteration 111/1000 | Loss: 0.00001163
Iteration 112/1000 | Loss: 0.00001163
Iteration 113/1000 | Loss: 0.00001163
Iteration 114/1000 | Loss: 0.00001163
Iteration 115/1000 | Loss: 0.00001163
Iteration 116/1000 | Loss: 0.00001163
Iteration 117/1000 | Loss: 0.00001163
Iteration 118/1000 | Loss: 0.00001163
Iteration 119/1000 | Loss: 0.00001163
Iteration 120/1000 | Loss: 0.00001163
Iteration 121/1000 | Loss: 0.00001163
Iteration 122/1000 | Loss: 0.00001163
Iteration 123/1000 | Loss: 0.00001163
Iteration 124/1000 | Loss: 0.00001163
Iteration 125/1000 | Loss: 0.00001163
Iteration 126/1000 | Loss: 0.00001163
Iteration 127/1000 | Loss: 0.00001163
Iteration 128/1000 | Loss: 0.00001163
Iteration 129/1000 | Loss: 0.00001163
Iteration 130/1000 | Loss: 0.00001163
Iteration 131/1000 | Loss: 0.00001163
Iteration 132/1000 | Loss: 0.00001163
Iteration 133/1000 | Loss: 0.00001163
Iteration 134/1000 | Loss: 0.00001163
Iteration 135/1000 | Loss: 0.00001163
Iteration 136/1000 | Loss: 0.00001163
Iteration 137/1000 | Loss: 0.00001163
Iteration 138/1000 | Loss: 0.00001163
Iteration 139/1000 | Loss: 0.00001163
Iteration 140/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.162724493042333e-05, 1.162724493042333e-05, 1.162724493042333e-05, 1.162724493042333e-05, 1.162724493042333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.162724493042333e-05

Optimization complete. Final v2v error: 2.893155097961426 mm

Highest mean error: 3.281074047088623 mm for frame 126

Lowest mean error: 2.359822988510132 mm for frame 17

Saving results

Total time: 36.10015845298767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_1682/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_1682/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813971
Iteration 2/25 | Loss: 0.00110730
Iteration 3/25 | Loss: 0.00094365
Iteration 4/25 | Loss: 0.00091169
Iteration 5/25 | Loss: 0.00090134
Iteration 6/25 | Loss: 0.00089990
Iteration 7/25 | Loss: 0.00089990
Iteration 8/25 | Loss: 0.00089990
Iteration 9/25 | Loss: 0.00089990
Iteration 10/25 | Loss: 0.00089990
Iteration 11/25 | Loss: 0.00089990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008998964913189411, 0.0008998964913189411, 0.0008998964913189411, 0.0008998964913189411, 0.0008998964913189411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008998964913189411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66412520
Iteration 2/25 | Loss: 0.00346085
Iteration 3/25 | Loss: 0.00346082
Iteration 4/25 | Loss: 0.00346082
Iteration 5/25 | Loss: 0.00346082
Iteration 6/25 | Loss: 0.00346081
Iteration 7/25 | Loss: 0.00346081
Iteration 8/25 | Loss: 0.00346081
Iteration 9/25 | Loss: 0.00346081
Iteration 10/25 | Loss: 0.00346081
Iteration 11/25 | Loss: 0.00346081
Iteration 12/25 | Loss: 0.00346081
Iteration 13/25 | Loss: 0.00346081
Iteration 14/25 | Loss: 0.00346081
Iteration 15/25 | Loss: 0.00346081
Iteration 16/25 | Loss: 0.00346081
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003460813080891967, 0.003460813080891967, 0.003460813080891967, 0.003460813080891967, 0.003460813080891967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003460813080891967

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00346081
Iteration 2/1000 | Loss: 0.00003101
Iteration 3/1000 | Loss: 0.00002570
Iteration 4/1000 | Loss: 0.00002326
Iteration 5/1000 | Loss: 0.00002233
Iteration 6/1000 | Loss: 0.00002146
Iteration 7/1000 | Loss: 0.00002106
Iteration 8/1000 | Loss: 0.00002055
Iteration 9/1000 | Loss: 0.00002046
Iteration 10/1000 | Loss: 0.00002025
Iteration 11/1000 | Loss: 0.00002022
Iteration 12/1000 | Loss: 0.00002004
Iteration 13/1000 | Loss: 0.00001997
Iteration 14/1000 | Loss: 0.00001980
Iteration 15/1000 | Loss: 0.00001975
Iteration 16/1000 | Loss: 0.00001968
Iteration 17/1000 | Loss: 0.00001967
Iteration 18/1000 | Loss: 0.00001962
Iteration 19/1000 | Loss: 0.00001962
Iteration 20/1000 | Loss: 0.00001961
Iteration 21/1000 | Loss: 0.00001961
Iteration 22/1000 | Loss: 0.00001954
Iteration 23/1000 | Loss: 0.00001954
Iteration 24/1000 | Loss: 0.00001954
Iteration 25/1000 | Loss: 0.00001954
Iteration 26/1000 | Loss: 0.00001954
Iteration 27/1000 | Loss: 0.00001954
Iteration 28/1000 | Loss: 0.00001954
Iteration 29/1000 | Loss: 0.00001953
Iteration 30/1000 | Loss: 0.00001953
Iteration 31/1000 | Loss: 0.00001953
Iteration 32/1000 | Loss: 0.00001951
Iteration 33/1000 | Loss: 0.00001950
Iteration 34/1000 | Loss: 0.00001950
Iteration 35/1000 | Loss: 0.00001950
Iteration 36/1000 | Loss: 0.00001950
Iteration 37/1000 | Loss: 0.00001949
Iteration 38/1000 | Loss: 0.00001949
Iteration 39/1000 | Loss: 0.00001949
Iteration 40/1000 | Loss: 0.00001949
Iteration 41/1000 | Loss: 0.00001948
Iteration 42/1000 | Loss: 0.00001948
Iteration 43/1000 | Loss: 0.00001948
Iteration 44/1000 | Loss: 0.00001948
Iteration 45/1000 | Loss: 0.00001948
Iteration 46/1000 | Loss: 0.00001948
Iteration 47/1000 | Loss: 0.00001947
Iteration 48/1000 | Loss: 0.00001947
Iteration 49/1000 | Loss: 0.00001946
Iteration 50/1000 | Loss: 0.00001946
Iteration 51/1000 | Loss: 0.00001946
Iteration 52/1000 | Loss: 0.00001945
Iteration 53/1000 | Loss: 0.00001945
Iteration 54/1000 | Loss: 0.00001945
Iteration 55/1000 | Loss: 0.00001944
Iteration 56/1000 | Loss: 0.00001944
Iteration 57/1000 | Loss: 0.00001944
Iteration 58/1000 | Loss: 0.00001944
Iteration 59/1000 | Loss: 0.00001944
Iteration 60/1000 | Loss: 0.00001943
Iteration 61/1000 | Loss: 0.00001943
Iteration 62/1000 | Loss: 0.00001943
Iteration 63/1000 | Loss: 0.00001942
Iteration 64/1000 | Loss: 0.00001942
Iteration 65/1000 | Loss: 0.00001941
Iteration 66/1000 | Loss: 0.00001941
Iteration 67/1000 | Loss: 0.00001939
Iteration 68/1000 | Loss: 0.00001938
Iteration 69/1000 | Loss: 0.00001937
Iteration 70/1000 | Loss: 0.00001937
Iteration 71/1000 | Loss: 0.00001937
Iteration 72/1000 | Loss: 0.00001936
Iteration 73/1000 | Loss: 0.00001936
Iteration 74/1000 | Loss: 0.00001936
Iteration 75/1000 | Loss: 0.00001936
Iteration 76/1000 | Loss: 0.00001935
Iteration 77/1000 | Loss: 0.00001935
Iteration 78/1000 | Loss: 0.00001935
Iteration 79/1000 | Loss: 0.00001934
Iteration 80/1000 | Loss: 0.00001934
Iteration 81/1000 | Loss: 0.00001934
Iteration 82/1000 | Loss: 0.00001933
Iteration 83/1000 | Loss: 0.00001933
Iteration 84/1000 | Loss: 0.00001933
Iteration 85/1000 | Loss: 0.00001932
Iteration 86/1000 | Loss: 0.00001932
Iteration 87/1000 | Loss: 0.00001932
Iteration 88/1000 | Loss: 0.00001932
Iteration 89/1000 | Loss: 0.00001932
Iteration 90/1000 | Loss: 0.00001932
Iteration 91/1000 | Loss: 0.00001932
Iteration 92/1000 | Loss: 0.00001932
Iteration 93/1000 | Loss: 0.00001932
Iteration 94/1000 | Loss: 0.00001932
Iteration 95/1000 | Loss: 0.00001932
Iteration 96/1000 | Loss: 0.00001932
Iteration 97/1000 | Loss: 0.00001931
Iteration 98/1000 | Loss: 0.00001931
Iteration 99/1000 | Loss: 0.00001931
Iteration 100/1000 | Loss: 0.00001931
Iteration 101/1000 | Loss: 0.00001931
Iteration 102/1000 | Loss: 0.00001931
Iteration 103/1000 | Loss: 0.00001930
Iteration 104/1000 | Loss: 0.00001930
Iteration 105/1000 | Loss: 0.00001930
Iteration 106/1000 | Loss: 0.00001930
Iteration 107/1000 | Loss: 0.00001930
Iteration 108/1000 | Loss: 0.00001930
Iteration 109/1000 | Loss: 0.00001930
Iteration 110/1000 | Loss: 0.00001930
Iteration 111/1000 | Loss: 0.00001930
Iteration 112/1000 | Loss: 0.00001930
Iteration 113/1000 | Loss: 0.00001930
Iteration 114/1000 | Loss: 0.00001930
Iteration 115/1000 | Loss: 0.00001929
Iteration 116/1000 | Loss: 0.00001929
Iteration 117/1000 | Loss: 0.00001929
Iteration 118/1000 | Loss: 0.00001929
Iteration 119/1000 | Loss: 0.00001929
Iteration 120/1000 | Loss: 0.00001929
Iteration 121/1000 | Loss: 0.00001929
Iteration 122/1000 | Loss: 0.00001929
Iteration 123/1000 | Loss: 0.00001929
Iteration 124/1000 | Loss: 0.00001929
Iteration 125/1000 | Loss: 0.00001929
Iteration 126/1000 | Loss: 0.00001929
Iteration 127/1000 | Loss: 0.00001929
Iteration 128/1000 | Loss: 0.00001929
Iteration 129/1000 | Loss: 0.00001929
Iteration 130/1000 | Loss: 0.00001929
Iteration 131/1000 | Loss: 0.00001928
Iteration 132/1000 | Loss: 0.00001928
Iteration 133/1000 | Loss: 0.00001928
Iteration 134/1000 | Loss: 0.00001928
Iteration 135/1000 | Loss: 0.00001928
Iteration 136/1000 | Loss: 0.00001928
Iteration 137/1000 | Loss: 0.00001928
Iteration 138/1000 | Loss: 0.00001928
Iteration 139/1000 | Loss: 0.00001928
Iteration 140/1000 | Loss: 0.00001928
Iteration 141/1000 | Loss: 0.00001928
Iteration 142/1000 | Loss: 0.00001928
Iteration 143/1000 | Loss: 0.00001928
Iteration 144/1000 | Loss: 0.00001927
Iteration 145/1000 | Loss: 0.00001927
Iteration 146/1000 | Loss: 0.00001927
Iteration 147/1000 | Loss: 0.00001927
Iteration 148/1000 | Loss: 0.00001927
Iteration 149/1000 | Loss: 0.00001927
Iteration 150/1000 | Loss: 0.00001927
Iteration 151/1000 | Loss: 0.00001927
Iteration 152/1000 | Loss: 0.00001927
Iteration 153/1000 | Loss: 0.00001927
Iteration 154/1000 | Loss: 0.00001927
Iteration 155/1000 | Loss: 0.00001927
Iteration 156/1000 | Loss: 0.00001927
Iteration 157/1000 | Loss: 0.00001927
Iteration 158/1000 | Loss: 0.00001927
Iteration 159/1000 | Loss: 0.00001927
Iteration 160/1000 | Loss: 0.00001927
Iteration 161/1000 | Loss: 0.00001926
Iteration 162/1000 | Loss: 0.00001926
Iteration 163/1000 | Loss: 0.00001926
Iteration 164/1000 | Loss: 0.00001926
Iteration 165/1000 | Loss: 0.00001926
Iteration 166/1000 | Loss: 0.00001926
Iteration 167/1000 | Loss: 0.00001926
Iteration 168/1000 | Loss: 0.00001926
Iteration 169/1000 | Loss: 0.00001926
Iteration 170/1000 | Loss: 0.00001926
Iteration 171/1000 | Loss: 0.00001926
Iteration 172/1000 | Loss: 0.00001926
Iteration 173/1000 | Loss: 0.00001926
Iteration 174/1000 | Loss: 0.00001926
Iteration 175/1000 | Loss: 0.00001925
Iteration 176/1000 | Loss: 0.00001925
Iteration 177/1000 | Loss: 0.00001925
Iteration 178/1000 | Loss: 0.00001925
Iteration 179/1000 | Loss: 0.00001925
Iteration 180/1000 | Loss: 0.00001925
Iteration 181/1000 | Loss: 0.00001925
Iteration 182/1000 | Loss: 0.00001925
Iteration 183/1000 | Loss: 0.00001925
Iteration 184/1000 | Loss: 0.00001925
Iteration 185/1000 | Loss: 0.00001925
Iteration 186/1000 | Loss: 0.00001925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.9254614016972482e-05, 1.9254614016972482e-05, 1.9254614016972482e-05, 1.9254614016972482e-05, 1.9254614016972482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9254614016972482e-05

Optimization complete. Final v2v error: 3.744835138320923 mm

Highest mean error: 4.147531509399414 mm for frame 183

Lowest mean error: 3.278026819229126 mm for frame 229

Saving results

Total time: 46.108012199401855
