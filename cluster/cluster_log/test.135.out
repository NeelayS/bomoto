Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=135, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7560-7615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808487
Iteration 2/25 | Loss: 0.00151886
Iteration 3/25 | Loss: 0.00137992
Iteration 4/25 | Loss: 0.00136385
Iteration 5/25 | Loss: 0.00136079
Iteration 6/25 | Loss: 0.00136073
Iteration 7/25 | Loss: 0.00136073
Iteration 8/25 | Loss: 0.00136073
Iteration 9/25 | Loss: 0.00136073
Iteration 10/25 | Loss: 0.00136073
Iteration 11/25 | Loss: 0.00136073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013607330620288849, 0.0013607330620288849, 0.0013607330620288849, 0.0013607330620288849, 0.0013607330620288849]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013607330620288849

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35264122
Iteration 2/25 | Loss: 0.00110227
Iteration 3/25 | Loss: 0.00110225
Iteration 4/25 | Loss: 0.00110225
Iteration 5/25 | Loss: 0.00110225
Iteration 6/25 | Loss: 0.00110225
Iteration 7/25 | Loss: 0.00110225
Iteration 8/25 | Loss: 0.00110225
Iteration 9/25 | Loss: 0.00110224
Iteration 10/25 | Loss: 0.00110224
Iteration 11/25 | Loss: 0.00110224
Iteration 12/25 | Loss: 0.00110224
Iteration 13/25 | Loss: 0.00110224
Iteration 14/25 | Loss: 0.00110224
Iteration 15/25 | Loss: 0.00110224
Iteration 16/25 | Loss: 0.00110224
Iteration 17/25 | Loss: 0.00110224
Iteration 18/25 | Loss: 0.00110224
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011022444814443588, 0.0011022444814443588, 0.0011022444814443588, 0.0011022444814443588, 0.0011022444814443588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011022444814443588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110224
Iteration 2/1000 | Loss: 0.00003561
Iteration 3/1000 | Loss: 0.00002400
Iteration 4/1000 | Loss: 0.00002153
Iteration 5/1000 | Loss: 0.00002030
Iteration 6/1000 | Loss: 0.00001943
Iteration 7/1000 | Loss: 0.00001883
Iteration 8/1000 | Loss: 0.00001852
Iteration 9/1000 | Loss: 0.00001823
Iteration 10/1000 | Loss: 0.00001794
Iteration 11/1000 | Loss: 0.00001772
Iteration 12/1000 | Loss: 0.00001757
Iteration 13/1000 | Loss: 0.00001753
Iteration 14/1000 | Loss: 0.00001735
Iteration 15/1000 | Loss: 0.00001733
Iteration 16/1000 | Loss: 0.00001728
Iteration 17/1000 | Loss: 0.00001726
Iteration 18/1000 | Loss: 0.00001720
Iteration 19/1000 | Loss: 0.00001719
Iteration 20/1000 | Loss: 0.00001719
Iteration 21/1000 | Loss: 0.00001718
Iteration 22/1000 | Loss: 0.00001718
Iteration 23/1000 | Loss: 0.00001718
Iteration 24/1000 | Loss: 0.00001717
Iteration 25/1000 | Loss: 0.00001717
Iteration 26/1000 | Loss: 0.00001717
Iteration 27/1000 | Loss: 0.00001716
Iteration 28/1000 | Loss: 0.00001715
Iteration 29/1000 | Loss: 0.00001715
Iteration 30/1000 | Loss: 0.00001715
Iteration 31/1000 | Loss: 0.00001714
Iteration 32/1000 | Loss: 0.00001714
Iteration 33/1000 | Loss: 0.00001714
Iteration 34/1000 | Loss: 0.00001714
Iteration 35/1000 | Loss: 0.00001714
Iteration 36/1000 | Loss: 0.00001714
Iteration 37/1000 | Loss: 0.00001714
Iteration 38/1000 | Loss: 0.00001712
Iteration 39/1000 | Loss: 0.00001712
Iteration 40/1000 | Loss: 0.00001711
Iteration 41/1000 | Loss: 0.00001711
Iteration 42/1000 | Loss: 0.00001711
Iteration 43/1000 | Loss: 0.00001711
Iteration 44/1000 | Loss: 0.00001711
Iteration 45/1000 | Loss: 0.00001711
Iteration 46/1000 | Loss: 0.00001711
Iteration 47/1000 | Loss: 0.00001711
Iteration 48/1000 | Loss: 0.00001711
Iteration 49/1000 | Loss: 0.00001711
Iteration 50/1000 | Loss: 0.00001711
Iteration 51/1000 | Loss: 0.00001710
Iteration 52/1000 | Loss: 0.00001710
Iteration 53/1000 | Loss: 0.00001709
Iteration 54/1000 | Loss: 0.00001709
Iteration 55/1000 | Loss: 0.00001709
Iteration 56/1000 | Loss: 0.00001709
Iteration 57/1000 | Loss: 0.00001708
Iteration 58/1000 | Loss: 0.00001708
Iteration 59/1000 | Loss: 0.00001708
Iteration 60/1000 | Loss: 0.00001707
Iteration 61/1000 | Loss: 0.00001707
Iteration 62/1000 | Loss: 0.00001707
Iteration 63/1000 | Loss: 0.00001706
Iteration 64/1000 | Loss: 0.00001706
Iteration 65/1000 | Loss: 0.00001706
Iteration 66/1000 | Loss: 0.00001706
Iteration 67/1000 | Loss: 0.00001706
Iteration 68/1000 | Loss: 0.00001706
Iteration 69/1000 | Loss: 0.00001706
Iteration 70/1000 | Loss: 0.00001706
Iteration 71/1000 | Loss: 0.00001706
Iteration 72/1000 | Loss: 0.00001706
Iteration 73/1000 | Loss: 0.00001706
Iteration 74/1000 | Loss: 0.00001706
Iteration 75/1000 | Loss: 0.00001706
Iteration 76/1000 | Loss: 0.00001706
Iteration 77/1000 | Loss: 0.00001705
Iteration 78/1000 | Loss: 0.00001705
Iteration 79/1000 | Loss: 0.00001705
Iteration 80/1000 | Loss: 0.00001705
Iteration 81/1000 | Loss: 0.00001705
Iteration 82/1000 | Loss: 0.00001704
Iteration 83/1000 | Loss: 0.00001704
Iteration 84/1000 | Loss: 0.00001704
Iteration 85/1000 | Loss: 0.00001704
Iteration 86/1000 | Loss: 0.00001704
Iteration 87/1000 | Loss: 0.00001704
Iteration 88/1000 | Loss: 0.00001703
Iteration 89/1000 | Loss: 0.00001703
Iteration 90/1000 | Loss: 0.00001703
Iteration 91/1000 | Loss: 0.00001703
Iteration 92/1000 | Loss: 0.00001703
Iteration 93/1000 | Loss: 0.00001703
Iteration 94/1000 | Loss: 0.00001703
Iteration 95/1000 | Loss: 0.00001702
Iteration 96/1000 | Loss: 0.00001702
Iteration 97/1000 | Loss: 0.00001702
Iteration 98/1000 | Loss: 0.00001702
Iteration 99/1000 | Loss: 0.00001702
Iteration 100/1000 | Loss: 0.00001701
Iteration 101/1000 | Loss: 0.00001701
Iteration 102/1000 | Loss: 0.00001700
Iteration 103/1000 | Loss: 0.00001700
Iteration 104/1000 | Loss: 0.00001700
Iteration 105/1000 | Loss: 0.00001700
Iteration 106/1000 | Loss: 0.00001700
Iteration 107/1000 | Loss: 0.00001700
Iteration 108/1000 | Loss: 0.00001700
Iteration 109/1000 | Loss: 0.00001699
Iteration 110/1000 | Loss: 0.00001698
Iteration 111/1000 | Loss: 0.00001698
Iteration 112/1000 | Loss: 0.00001698
Iteration 113/1000 | Loss: 0.00001698
Iteration 114/1000 | Loss: 0.00001698
Iteration 115/1000 | Loss: 0.00001698
Iteration 116/1000 | Loss: 0.00001698
Iteration 117/1000 | Loss: 0.00001698
Iteration 118/1000 | Loss: 0.00001698
Iteration 119/1000 | Loss: 0.00001698
Iteration 120/1000 | Loss: 0.00001698
Iteration 121/1000 | Loss: 0.00001698
Iteration 122/1000 | Loss: 0.00001697
Iteration 123/1000 | Loss: 0.00001697
Iteration 124/1000 | Loss: 0.00001696
Iteration 125/1000 | Loss: 0.00001696
Iteration 126/1000 | Loss: 0.00001696
Iteration 127/1000 | Loss: 0.00001695
Iteration 128/1000 | Loss: 0.00001695
Iteration 129/1000 | Loss: 0.00001695
Iteration 130/1000 | Loss: 0.00001694
Iteration 131/1000 | Loss: 0.00001694
Iteration 132/1000 | Loss: 0.00001694
Iteration 133/1000 | Loss: 0.00001694
Iteration 134/1000 | Loss: 0.00001694
Iteration 135/1000 | Loss: 0.00001693
Iteration 136/1000 | Loss: 0.00001693
Iteration 137/1000 | Loss: 0.00001693
Iteration 138/1000 | Loss: 0.00001693
Iteration 139/1000 | Loss: 0.00001693
Iteration 140/1000 | Loss: 0.00001693
Iteration 141/1000 | Loss: 0.00001693
Iteration 142/1000 | Loss: 0.00001693
Iteration 143/1000 | Loss: 0.00001693
Iteration 144/1000 | Loss: 0.00001693
Iteration 145/1000 | Loss: 0.00001693
Iteration 146/1000 | Loss: 0.00001692
Iteration 147/1000 | Loss: 0.00001692
Iteration 148/1000 | Loss: 0.00001692
Iteration 149/1000 | Loss: 0.00001692
Iteration 150/1000 | Loss: 0.00001692
Iteration 151/1000 | Loss: 0.00001692
Iteration 152/1000 | Loss: 0.00001692
Iteration 153/1000 | Loss: 0.00001692
Iteration 154/1000 | Loss: 0.00001692
Iteration 155/1000 | Loss: 0.00001692
Iteration 156/1000 | Loss: 0.00001692
Iteration 157/1000 | Loss: 0.00001692
Iteration 158/1000 | Loss: 0.00001692
Iteration 159/1000 | Loss: 0.00001692
Iteration 160/1000 | Loss: 0.00001692
Iteration 161/1000 | Loss: 0.00001692
Iteration 162/1000 | Loss: 0.00001692
Iteration 163/1000 | Loss: 0.00001692
Iteration 164/1000 | Loss: 0.00001692
Iteration 165/1000 | Loss: 0.00001692
Iteration 166/1000 | Loss: 0.00001692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.6918856999836862e-05, 1.6918856999836862e-05, 1.6918856999836862e-05, 1.6918856999836862e-05, 1.6918856999836862e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6918856999836862e-05

Optimization complete. Final v2v error: 3.44820237159729 mm

Highest mean error: 4.238455295562744 mm for frame 2

Lowest mean error: 3.253890037536621 mm for frame 42

Saving results

Total time: 40.90096473693848
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00732274
Iteration 2/25 | Loss: 0.00141427
Iteration 3/25 | Loss: 0.00130314
Iteration 4/25 | Loss: 0.00128946
Iteration 5/25 | Loss: 0.00127776
Iteration 6/25 | Loss: 0.00127629
Iteration 7/25 | Loss: 0.00127601
Iteration 8/25 | Loss: 0.00127589
Iteration 9/25 | Loss: 0.00127589
Iteration 10/25 | Loss: 0.00127589
Iteration 11/25 | Loss: 0.00127589
Iteration 12/25 | Loss: 0.00127589
Iteration 13/25 | Loss: 0.00127589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012758870143443346, 0.0012758870143443346, 0.0012758870143443346, 0.0012758870143443346, 0.0012758870143443346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012758870143443346

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81824791
Iteration 2/25 | Loss: 0.00093917
Iteration 3/25 | Loss: 0.00093917
Iteration 4/25 | Loss: 0.00093917
Iteration 5/25 | Loss: 0.00093917
Iteration 6/25 | Loss: 0.00093916
Iteration 7/25 | Loss: 0.00093916
Iteration 8/25 | Loss: 0.00093916
Iteration 9/25 | Loss: 0.00093916
Iteration 10/25 | Loss: 0.00093916
Iteration 11/25 | Loss: 0.00093916
Iteration 12/25 | Loss: 0.00093916
Iteration 13/25 | Loss: 0.00093916
Iteration 14/25 | Loss: 0.00093916
Iteration 15/25 | Loss: 0.00093916
Iteration 16/25 | Loss: 0.00093916
Iteration 17/25 | Loss: 0.00093916
Iteration 18/25 | Loss: 0.00093916
Iteration 19/25 | Loss: 0.00093916
Iteration 20/25 | Loss: 0.00093916
Iteration 21/25 | Loss: 0.00093916
Iteration 22/25 | Loss: 0.00093916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009391630301252007, 0.0009391630301252007, 0.0009391630301252007, 0.0009391630301252007, 0.0009391630301252007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009391630301252007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093916
Iteration 2/1000 | Loss: 0.00002273
Iteration 3/1000 | Loss: 0.00001789
Iteration 4/1000 | Loss: 0.00001603
Iteration 5/1000 | Loss: 0.00001539
Iteration 6/1000 | Loss: 0.00001476
Iteration 7/1000 | Loss: 0.00001438
Iteration 8/1000 | Loss: 0.00001418
Iteration 9/1000 | Loss: 0.00001388
Iteration 10/1000 | Loss: 0.00001364
Iteration 11/1000 | Loss: 0.00001361
Iteration 12/1000 | Loss: 0.00001344
Iteration 13/1000 | Loss: 0.00001338
Iteration 14/1000 | Loss: 0.00001332
Iteration 15/1000 | Loss: 0.00001326
Iteration 16/1000 | Loss: 0.00001325
Iteration 17/1000 | Loss: 0.00001325
Iteration 18/1000 | Loss: 0.00001324
Iteration 19/1000 | Loss: 0.00001318
Iteration 20/1000 | Loss: 0.00001318
Iteration 21/1000 | Loss: 0.00001318
Iteration 22/1000 | Loss: 0.00001318
Iteration 23/1000 | Loss: 0.00001318
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001318
Iteration 26/1000 | Loss: 0.00001318
Iteration 27/1000 | Loss: 0.00001318
Iteration 28/1000 | Loss: 0.00001318
Iteration 29/1000 | Loss: 0.00001317
Iteration 30/1000 | Loss: 0.00001317
Iteration 31/1000 | Loss: 0.00001317
Iteration 32/1000 | Loss: 0.00001313
Iteration 33/1000 | Loss: 0.00001312
Iteration 34/1000 | Loss: 0.00001312
Iteration 35/1000 | Loss: 0.00001312
Iteration 36/1000 | Loss: 0.00001312
Iteration 37/1000 | Loss: 0.00001311
Iteration 38/1000 | Loss: 0.00001311
Iteration 39/1000 | Loss: 0.00001311
Iteration 40/1000 | Loss: 0.00001311
Iteration 41/1000 | Loss: 0.00001310
Iteration 42/1000 | Loss: 0.00001308
Iteration 43/1000 | Loss: 0.00001308
Iteration 44/1000 | Loss: 0.00001307
Iteration 45/1000 | Loss: 0.00001307
Iteration 46/1000 | Loss: 0.00001307
Iteration 47/1000 | Loss: 0.00001306
Iteration 48/1000 | Loss: 0.00001306
Iteration 49/1000 | Loss: 0.00001305
Iteration 50/1000 | Loss: 0.00001301
Iteration 51/1000 | Loss: 0.00001301
Iteration 52/1000 | Loss: 0.00001300
Iteration 53/1000 | Loss: 0.00001300
Iteration 54/1000 | Loss: 0.00001298
Iteration 55/1000 | Loss: 0.00001292
Iteration 56/1000 | Loss: 0.00001292
Iteration 57/1000 | Loss: 0.00001292
Iteration 58/1000 | Loss: 0.00001290
Iteration 59/1000 | Loss: 0.00001290
Iteration 60/1000 | Loss: 0.00001290
Iteration 61/1000 | Loss: 0.00001289
Iteration 62/1000 | Loss: 0.00001289
Iteration 63/1000 | Loss: 0.00001289
Iteration 64/1000 | Loss: 0.00001289
Iteration 65/1000 | Loss: 0.00001288
Iteration 66/1000 | Loss: 0.00001288
Iteration 67/1000 | Loss: 0.00001288
Iteration 68/1000 | Loss: 0.00001288
Iteration 69/1000 | Loss: 0.00001287
Iteration 70/1000 | Loss: 0.00001287
Iteration 71/1000 | Loss: 0.00001287
Iteration 72/1000 | Loss: 0.00001287
Iteration 73/1000 | Loss: 0.00001286
Iteration 74/1000 | Loss: 0.00001286
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001284
Iteration 82/1000 | Loss: 0.00001284
Iteration 83/1000 | Loss: 0.00001283
Iteration 84/1000 | Loss: 0.00001283
Iteration 85/1000 | Loss: 0.00001283
Iteration 86/1000 | Loss: 0.00001283
Iteration 87/1000 | Loss: 0.00001282
Iteration 88/1000 | Loss: 0.00001282
Iteration 89/1000 | Loss: 0.00001282
Iteration 90/1000 | Loss: 0.00001281
Iteration 91/1000 | Loss: 0.00001281
Iteration 92/1000 | Loss: 0.00001281
Iteration 93/1000 | Loss: 0.00001280
Iteration 94/1000 | Loss: 0.00001280
Iteration 95/1000 | Loss: 0.00001279
Iteration 96/1000 | Loss: 0.00001279
Iteration 97/1000 | Loss: 0.00001279
Iteration 98/1000 | Loss: 0.00001279
Iteration 99/1000 | Loss: 0.00001278
Iteration 100/1000 | Loss: 0.00001277
Iteration 101/1000 | Loss: 0.00001277
Iteration 102/1000 | Loss: 0.00001277
Iteration 103/1000 | Loss: 0.00001277
Iteration 104/1000 | Loss: 0.00001276
Iteration 105/1000 | Loss: 0.00001276
Iteration 106/1000 | Loss: 0.00001276
Iteration 107/1000 | Loss: 0.00001275
Iteration 108/1000 | Loss: 0.00001275
Iteration 109/1000 | Loss: 0.00001275
Iteration 110/1000 | Loss: 0.00001274
Iteration 111/1000 | Loss: 0.00001274
Iteration 112/1000 | Loss: 0.00001274
Iteration 113/1000 | Loss: 0.00001273
Iteration 114/1000 | Loss: 0.00001273
Iteration 115/1000 | Loss: 0.00001273
Iteration 116/1000 | Loss: 0.00001273
Iteration 117/1000 | Loss: 0.00001273
Iteration 118/1000 | Loss: 0.00001272
Iteration 119/1000 | Loss: 0.00001272
Iteration 120/1000 | Loss: 0.00001272
Iteration 121/1000 | Loss: 0.00001272
Iteration 122/1000 | Loss: 0.00001271
Iteration 123/1000 | Loss: 0.00001271
Iteration 124/1000 | Loss: 0.00001271
Iteration 125/1000 | Loss: 0.00001270
Iteration 126/1000 | Loss: 0.00001270
Iteration 127/1000 | Loss: 0.00001270
Iteration 128/1000 | Loss: 0.00001270
Iteration 129/1000 | Loss: 0.00001270
Iteration 130/1000 | Loss: 0.00001270
Iteration 131/1000 | Loss: 0.00001270
Iteration 132/1000 | Loss: 0.00001270
Iteration 133/1000 | Loss: 0.00001270
Iteration 134/1000 | Loss: 0.00001269
Iteration 135/1000 | Loss: 0.00001269
Iteration 136/1000 | Loss: 0.00001269
Iteration 137/1000 | Loss: 0.00001269
Iteration 138/1000 | Loss: 0.00001269
Iteration 139/1000 | Loss: 0.00001268
Iteration 140/1000 | Loss: 0.00001268
Iteration 141/1000 | Loss: 0.00001268
Iteration 142/1000 | Loss: 0.00001268
Iteration 143/1000 | Loss: 0.00001268
Iteration 144/1000 | Loss: 0.00001268
Iteration 145/1000 | Loss: 0.00001268
Iteration 146/1000 | Loss: 0.00001268
Iteration 147/1000 | Loss: 0.00001268
Iteration 148/1000 | Loss: 0.00001268
Iteration 149/1000 | Loss: 0.00001268
Iteration 150/1000 | Loss: 0.00001268
Iteration 151/1000 | Loss: 0.00001268
Iteration 152/1000 | Loss: 0.00001267
Iteration 153/1000 | Loss: 0.00001267
Iteration 154/1000 | Loss: 0.00001267
Iteration 155/1000 | Loss: 0.00001267
Iteration 156/1000 | Loss: 0.00001267
Iteration 157/1000 | Loss: 0.00001267
Iteration 158/1000 | Loss: 0.00001267
Iteration 159/1000 | Loss: 0.00001267
Iteration 160/1000 | Loss: 0.00001267
Iteration 161/1000 | Loss: 0.00001267
Iteration 162/1000 | Loss: 0.00001267
Iteration 163/1000 | Loss: 0.00001267
Iteration 164/1000 | Loss: 0.00001266
Iteration 165/1000 | Loss: 0.00001266
Iteration 166/1000 | Loss: 0.00001266
Iteration 167/1000 | Loss: 0.00001266
Iteration 168/1000 | Loss: 0.00001266
Iteration 169/1000 | Loss: 0.00001266
Iteration 170/1000 | Loss: 0.00001266
Iteration 171/1000 | Loss: 0.00001266
Iteration 172/1000 | Loss: 0.00001265
Iteration 173/1000 | Loss: 0.00001265
Iteration 174/1000 | Loss: 0.00001265
Iteration 175/1000 | Loss: 0.00001265
Iteration 176/1000 | Loss: 0.00001265
Iteration 177/1000 | Loss: 0.00001265
Iteration 178/1000 | Loss: 0.00001265
Iteration 179/1000 | Loss: 0.00001265
Iteration 180/1000 | Loss: 0.00001265
Iteration 181/1000 | Loss: 0.00001265
Iteration 182/1000 | Loss: 0.00001265
Iteration 183/1000 | Loss: 0.00001265
Iteration 184/1000 | Loss: 0.00001265
Iteration 185/1000 | Loss: 0.00001265
Iteration 186/1000 | Loss: 0.00001264
Iteration 187/1000 | Loss: 0.00001264
Iteration 188/1000 | Loss: 0.00001264
Iteration 189/1000 | Loss: 0.00001264
Iteration 190/1000 | Loss: 0.00001264
Iteration 191/1000 | Loss: 0.00001264
Iteration 192/1000 | Loss: 0.00001264
Iteration 193/1000 | Loss: 0.00001264
Iteration 194/1000 | Loss: 0.00001264
Iteration 195/1000 | Loss: 0.00001264
Iteration 196/1000 | Loss: 0.00001264
Iteration 197/1000 | Loss: 0.00001264
Iteration 198/1000 | Loss: 0.00001264
Iteration 199/1000 | Loss: 0.00001264
Iteration 200/1000 | Loss: 0.00001264
Iteration 201/1000 | Loss: 0.00001264
Iteration 202/1000 | Loss: 0.00001264
Iteration 203/1000 | Loss: 0.00001264
Iteration 204/1000 | Loss: 0.00001264
Iteration 205/1000 | Loss: 0.00001264
Iteration 206/1000 | Loss: 0.00001264
Iteration 207/1000 | Loss: 0.00001264
Iteration 208/1000 | Loss: 0.00001264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.2641802641155664e-05, 1.2641802641155664e-05, 1.2641802641155664e-05, 1.2641802641155664e-05, 1.2641802641155664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2641802641155664e-05

Optimization complete. Final v2v error: 3.0567827224731445 mm

Highest mean error: 3.3068697452545166 mm for frame 105

Lowest mean error: 2.9131181240081787 mm for frame 15

Saving results

Total time: 48.204376220703125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486725
Iteration 2/25 | Loss: 0.00140345
Iteration 3/25 | Loss: 0.00132631
Iteration 4/25 | Loss: 0.00131642
Iteration 5/25 | Loss: 0.00131478
Iteration 6/25 | Loss: 0.00131478
Iteration 7/25 | Loss: 0.00131478
Iteration 8/25 | Loss: 0.00131478
Iteration 9/25 | Loss: 0.00131478
Iteration 10/25 | Loss: 0.00131478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013147807912901044, 0.0013147807912901044, 0.0013147807912901044, 0.0013147807912901044, 0.0013147807912901044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013147807912901044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.70587301
Iteration 2/25 | Loss: 0.00090923
Iteration 3/25 | Loss: 0.00090922
Iteration 4/25 | Loss: 0.00090922
Iteration 5/25 | Loss: 0.00090922
Iteration 6/25 | Loss: 0.00090922
Iteration 7/25 | Loss: 0.00090922
Iteration 8/25 | Loss: 0.00090922
Iteration 9/25 | Loss: 0.00090922
Iteration 10/25 | Loss: 0.00090922
Iteration 11/25 | Loss: 0.00090922
Iteration 12/25 | Loss: 0.00090922
Iteration 13/25 | Loss: 0.00090922
Iteration 14/25 | Loss: 0.00090922
Iteration 15/25 | Loss: 0.00090922
Iteration 16/25 | Loss: 0.00090922
Iteration 17/25 | Loss: 0.00090922
Iteration 18/25 | Loss: 0.00090922
Iteration 19/25 | Loss: 0.00090922
Iteration 20/25 | Loss: 0.00090922
Iteration 21/25 | Loss: 0.00090922
Iteration 22/25 | Loss: 0.00090922
Iteration 23/25 | Loss: 0.00090922
Iteration 24/25 | Loss: 0.00090922
Iteration 25/25 | Loss: 0.00090922

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090922
Iteration 2/1000 | Loss: 0.00002513
Iteration 3/1000 | Loss: 0.00002044
Iteration 4/1000 | Loss: 0.00001925
Iteration 5/1000 | Loss: 0.00001824
Iteration 6/1000 | Loss: 0.00001762
Iteration 7/1000 | Loss: 0.00001715
Iteration 8/1000 | Loss: 0.00001674
Iteration 9/1000 | Loss: 0.00001646
Iteration 10/1000 | Loss: 0.00001645
Iteration 11/1000 | Loss: 0.00001616
Iteration 12/1000 | Loss: 0.00001603
Iteration 13/1000 | Loss: 0.00001582
Iteration 14/1000 | Loss: 0.00001573
Iteration 15/1000 | Loss: 0.00001565
Iteration 16/1000 | Loss: 0.00001563
Iteration 17/1000 | Loss: 0.00001555
Iteration 18/1000 | Loss: 0.00001552
Iteration 19/1000 | Loss: 0.00001547
Iteration 20/1000 | Loss: 0.00001545
Iteration 21/1000 | Loss: 0.00001543
Iteration 22/1000 | Loss: 0.00001543
Iteration 23/1000 | Loss: 0.00001542
Iteration 24/1000 | Loss: 0.00001536
Iteration 25/1000 | Loss: 0.00001536
Iteration 26/1000 | Loss: 0.00001536
Iteration 27/1000 | Loss: 0.00001536
Iteration 28/1000 | Loss: 0.00001536
Iteration 29/1000 | Loss: 0.00001536
Iteration 30/1000 | Loss: 0.00001534
Iteration 31/1000 | Loss: 0.00001533
Iteration 32/1000 | Loss: 0.00001533
Iteration 33/1000 | Loss: 0.00001530
Iteration 34/1000 | Loss: 0.00001530
Iteration 35/1000 | Loss: 0.00001530
Iteration 36/1000 | Loss: 0.00001530
Iteration 37/1000 | Loss: 0.00001530
Iteration 38/1000 | Loss: 0.00001530
Iteration 39/1000 | Loss: 0.00001529
Iteration 40/1000 | Loss: 0.00001529
Iteration 41/1000 | Loss: 0.00001529
Iteration 42/1000 | Loss: 0.00001528
Iteration 43/1000 | Loss: 0.00001528
Iteration 44/1000 | Loss: 0.00001528
Iteration 45/1000 | Loss: 0.00001524
Iteration 46/1000 | Loss: 0.00001520
Iteration 47/1000 | Loss: 0.00001517
Iteration 48/1000 | Loss: 0.00001516
Iteration 49/1000 | Loss: 0.00001512
Iteration 50/1000 | Loss: 0.00001510
Iteration 51/1000 | Loss: 0.00001507
Iteration 52/1000 | Loss: 0.00001507
Iteration 53/1000 | Loss: 0.00001507
Iteration 54/1000 | Loss: 0.00001506
Iteration 55/1000 | Loss: 0.00001505
Iteration 56/1000 | Loss: 0.00001505
Iteration 57/1000 | Loss: 0.00001504
Iteration 58/1000 | Loss: 0.00001504
Iteration 59/1000 | Loss: 0.00001503
Iteration 60/1000 | Loss: 0.00001503
Iteration 61/1000 | Loss: 0.00001503
Iteration 62/1000 | Loss: 0.00001502
Iteration 63/1000 | Loss: 0.00001501
Iteration 64/1000 | Loss: 0.00001501
Iteration 65/1000 | Loss: 0.00001500
Iteration 66/1000 | Loss: 0.00001500
Iteration 67/1000 | Loss: 0.00001500
Iteration 68/1000 | Loss: 0.00001500
Iteration 69/1000 | Loss: 0.00001500
Iteration 70/1000 | Loss: 0.00001500
Iteration 71/1000 | Loss: 0.00001500
Iteration 72/1000 | Loss: 0.00001500
Iteration 73/1000 | Loss: 0.00001500
Iteration 74/1000 | Loss: 0.00001500
Iteration 75/1000 | Loss: 0.00001499
Iteration 76/1000 | Loss: 0.00001498
Iteration 77/1000 | Loss: 0.00001498
Iteration 78/1000 | Loss: 0.00001498
Iteration 79/1000 | Loss: 0.00001497
Iteration 80/1000 | Loss: 0.00001497
Iteration 81/1000 | Loss: 0.00001497
Iteration 82/1000 | Loss: 0.00001497
Iteration 83/1000 | Loss: 0.00001497
Iteration 84/1000 | Loss: 0.00001497
Iteration 85/1000 | Loss: 0.00001497
Iteration 86/1000 | Loss: 0.00001497
Iteration 87/1000 | Loss: 0.00001497
Iteration 88/1000 | Loss: 0.00001497
Iteration 89/1000 | Loss: 0.00001497
Iteration 90/1000 | Loss: 0.00001497
Iteration 91/1000 | Loss: 0.00001497
Iteration 92/1000 | Loss: 0.00001497
Iteration 93/1000 | Loss: 0.00001497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.4972146345826332e-05, 1.4972146345826332e-05, 1.4972146345826332e-05, 1.4972146345826332e-05, 1.4972146345826332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4972146345826332e-05

Optimization complete. Final v2v error: 3.307121992111206 mm

Highest mean error: 3.49306058883667 mm for frame 38

Lowest mean error: 3.123624801635742 mm for frame 265

Saving results

Total time: 40.5846312046051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808724
Iteration 2/25 | Loss: 0.00149472
Iteration 3/25 | Loss: 0.00133001
Iteration 4/25 | Loss: 0.00131401
Iteration 5/25 | Loss: 0.00131115
Iteration 6/25 | Loss: 0.00131115
Iteration 7/25 | Loss: 0.00131115
Iteration 8/25 | Loss: 0.00131115
Iteration 9/25 | Loss: 0.00131115
Iteration 10/25 | Loss: 0.00131115
Iteration 11/25 | Loss: 0.00131115
Iteration 12/25 | Loss: 0.00131115
Iteration 13/25 | Loss: 0.00131115
Iteration 14/25 | Loss: 0.00131115
Iteration 15/25 | Loss: 0.00131115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013111467706039548, 0.0013111467706039548, 0.0013111467706039548, 0.0013111467706039548, 0.0013111467706039548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013111467706039548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38067520
Iteration 2/25 | Loss: 0.00068751
Iteration 3/25 | Loss: 0.00068747
Iteration 4/25 | Loss: 0.00068747
Iteration 5/25 | Loss: 0.00068747
Iteration 6/25 | Loss: 0.00068747
Iteration 7/25 | Loss: 0.00068747
Iteration 8/25 | Loss: 0.00068747
Iteration 9/25 | Loss: 0.00068747
Iteration 10/25 | Loss: 0.00068747
Iteration 11/25 | Loss: 0.00068747
Iteration 12/25 | Loss: 0.00068747
Iteration 13/25 | Loss: 0.00068747
Iteration 14/25 | Loss: 0.00068747
Iteration 15/25 | Loss: 0.00068747
Iteration 16/25 | Loss: 0.00068747
Iteration 17/25 | Loss: 0.00068747
Iteration 18/25 | Loss: 0.00068747
Iteration 19/25 | Loss: 0.00068747
Iteration 20/25 | Loss: 0.00068747
Iteration 21/25 | Loss: 0.00068747
Iteration 22/25 | Loss: 0.00068747
Iteration 23/25 | Loss: 0.00068747
Iteration 24/25 | Loss: 0.00068747
Iteration 25/25 | Loss: 0.00068747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068747
Iteration 2/1000 | Loss: 0.00003290
Iteration 3/1000 | Loss: 0.00002351
Iteration 4/1000 | Loss: 0.00002163
Iteration 5/1000 | Loss: 0.00002026
Iteration 6/1000 | Loss: 0.00001938
Iteration 7/1000 | Loss: 0.00001869
Iteration 8/1000 | Loss: 0.00001823
Iteration 9/1000 | Loss: 0.00001782
Iteration 10/1000 | Loss: 0.00001741
Iteration 11/1000 | Loss: 0.00001716
Iteration 12/1000 | Loss: 0.00001716
Iteration 13/1000 | Loss: 0.00001715
Iteration 14/1000 | Loss: 0.00001689
Iteration 15/1000 | Loss: 0.00001667
Iteration 16/1000 | Loss: 0.00001654
Iteration 17/1000 | Loss: 0.00001651
Iteration 18/1000 | Loss: 0.00001649
Iteration 19/1000 | Loss: 0.00001646
Iteration 20/1000 | Loss: 0.00001632
Iteration 21/1000 | Loss: 0.00001631
Iteration 22/1000 | Loss: 0.00001626
Iteration 23/1000 | Loss: 0.00001616
Iteration 24/1000 | Loss: 0.00001613
Iteration 25/1000 | Loss: 0.00001609
Iteration 26/1000 | Loss: 0.00001608
Iteration 27/1000 | Loss: 0.00001604
Iteration 28/1000 | Loss: 0.00001602
Iteration 29/1000 | Loss: 0.00001601
Iteration 30/1000 | Loss: 0.00001601
Iteration 31/1000 | Loss: 0.00001596
Iteration 32/1000 | Loss: 0.00001596
Iteration 33/1000 | Loss: 0.00001593
Iteration 34/1000 | Loss: 0.00001593
Iteration 35/1000 | Loss: 0.00001593
Iteration 36/1000 | Loss: 0.00001593
Iteration 37/1000 | Loss: 0.00001593
Iteration 38/1000 | Loss: 0.00001593
Iteration 39/1000 | Loss: 0.00001592
Iteration 40/1000 | Loss: 0.00001592
Iteration 41/1000 | Loss: 0.00001592
Iteration 42/1000 | Loss: 0.00001592
Iteration 43/1000 | Loss: 0.00001592
Iteration 44/1000 | Loss: 0.00001592
Iteration 45/1000 | Loss: 0.00001592
Iteration 46/1000 | Loss: 0.00001591
Iteration 47/1000 | Loss: 0.00001591
Iteration 48/1000 | Loss: 0.00001590
Iteration 49/1000 | Loss: 0.00001590
Iteration 50/1000 | Loss: 0.00001589
Iteration 51/1000 | Loss: 0.00001588
Iteration 52/1000 | Loss: 0.00001588
Iteration 53/1000 | Loss: 0.00001588
Iteration 54/1000 | Loss: 0.00001587
Iteration 55/1000 | Loss: 0.00001587
Iteration 56/1000 | Loss: 0.00001587
Iteration 57/1000 | Loss: 0.00001586
Iteration 58/1000 | Loss: 0.00001586
Iteration 59/1000 | Loss: 0.00001586
Iteration 60/1000 | Loss: 0.00001585
Iteration 61/1000 | Loss: 0.00001585
Iteration 62/1000 | Loss: 0.00001584
Iteration 63/1000 | Loss: 0.00001584
Iteration 64/1000 | Loss: 0.00001584
Iteration 65/1000 | Loss: 0.00001583
Iteration 66/1000 | Loss: 0.00001583
Iteration 67/1000 | Loss: 0.00001583
Iteration 68/1000 | Loss: 0.00001583
Iteration 69/1000 | Loss: 0.00001582
Iteration 70/1000 | Loss: 0.00001582
Iteration 71/1000 | Loss: 0.00001582
Iteration 72/1000 | Loss: 0.00001582
Iteration 73/1000 | Loss: 0.00001582
Iteration 74/1000 | Loss: 0.00001582
Iteration 75/1000 | Loss: 0.00001582
Iteration 76/1000 | Loss: 0.00001582
Iteration 77/1000 | Loss: 0.00001581
Iteration 78/1000 | Loss: 0.00001581
Iteration 79/1000 | Loss: 0.00001581
Iteration 80/1000 | Loss: 0.00001581
Iteration 81/1000 | Loss: 0.00001581
Iteration 82/1000 | Loss: 0.00001581
Iteration 83/1000 | Loss: 0.00001581
Iteration 84/1000 | Loss: 0.00001581
Iteration 85/1000 | Loss: 0.00001581
Iteration 86/1000 | Loss: 0.00001580
Iteration 87/1000 | Loss: 0.00001580
Iteration 88/1000 | Loss: 0.00001580
Iteration 89/1000 | Loss: 0.00001580
Iteration 90/1000 | Loss: 0.00001580
Iteration 91/1000 | Loss: 0.00001580
Iteration 92/1000 | Loss: 0.00001580
Iteration 93/1000 | Loss: 0.00001580
Iteration 94/1000 | Loss: 0.00001580
Iteration 95/1000 | Loss: 0.00001580
Iteration 96/1000 | Loss: 0.00001580
Iteration 97/1000 | Loss: 0.00001580
Iteration 98/1000 | Loss: 0.00001579
Iteration 99/1000 | Loss: 0.00001579
Iteration 100/1000 | Loss: 0.00001579
Iteration 101/1000 | Loss: 0.00001579
Iteration 102/1000 | Loss: 0.00001579
Iteration 103/1000 | Loss: 0.00001579
Iteration 104/1000 | Loss: 0.00001579
Iteration 105/1000 | Loss: 0.00001579
Iteration 106/1000 | Loss: 0.00001579
Iteration 107/1000 | Loss: 0.00001578
Iteration 108/1000 | Loss: 0.00001578
Iteration 109/1000 | Loss: 0.00001578
Iteration 110/1000 | Loss: 0.00001578
Iteration 111/1000 | Loss: 0.00001578
Iteration 112/1000 | Loss: 0.00001578
Iteration 113/1000 | Loss: 0.00001578
Iteration 114/1000 | Loss: 0.00001578
Iteration 115/1000 | Loss: 0.00001578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.5781111869728193e-05, 1.5781111869728193e-05, 1.5781111869728193e-05, 1.5781111869728193e-05, 1.5781111869728193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5781111869728193e-05

Optimization complete. Final v2v error: 3.3428823947906494 mm

Highest mean error: 4.173179626464844 mm for frame 36

Lowest mean error: 3.0779294967651367 mm for frame 18

Saving results

Total time: 41.09849691390991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065990
Iteration 2/25 | Loss: 0.00360997
Iteration 3/25 | Loss: 0.00244176
Iteration 4/25 | Loss: 0.00213503
Iteration 5/25 | Loss: 0.00247931
Iteration 6/25 | Loss: 0.00260069
Iteration 7/25 | Loss: 0.00201973
Iteration 8/25 | Loss: 0.00179318
Iteration 9/25 | Loss: 0.00171642
Iteration 10/25 | Loss: 0.00169968
Iteration 11/25 | Loss: 0.00168868
Iteration 12/25 | Loss: 0.00168357
Iteration 13/25 | Loss: 0.00168430
Iteration 14/25 | Loss: 0.00167943
Iteration 15/25 | Loss: 0.00166767
Iteration 16/25 | Loss: 0.00166922
Iteration 17/25 | Loss: 0.00167193
Iteration 18/25 | Loss: 0.00166990
Iteration 19/25 | Loss: 0.00166732
Iteration 20/25 | Loss: 0.00166829
Iteration 21/25 | Loss: 0.00166600
Iteration 22/25 | Loss: 0.00166290
Iteration 23/25 | Loss: 0.00166231
Iteration 24/25 | Loss: 0.00166232
Iteration 25/25 | Loss: 0.00166093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.63643897
Iteration 2/25 | Loss: 0.00162685
Iteration 3/25 | Loss: 0.00151790
Iteration 4/25 | Loss: 0.00151790
Iteration 5/25 | Loss: 0.00151790
Iteration 6/25 | Loss: 0.00151790
Iteration 7/25 | Loss: 0.00151790
Iteration 8/25 | Loss: 0.00151790
Iteration 9/25 | Loss: 0.00151790
Iteration 10/25 | Loss: 0.00151790
Iteration 11/25 | Loss: 0.00151790
Iteration 12/25 | Loss: 0.00151790
Iteration 13/25 | Loss: 0.00151790
Iteration 14/25 | Loss: 0.00151790
Iteration 15/25 | Loss: 0.00151790
Iteration 16/25 | Loss: 0.00151790
Iteration 17/25 | Loss: 0.00151790
Iteration 18/25 | Loss: 0.00151790
Iteration 19/25 | Loss: 0.00151790
Iteration 20/25 | Loss: 0.00151790
Iteration 21/25 | Loss: 0.00151790
Iteration 22/25 | Loss: 0.00151790
Iteration 23/25 | Loss: 0.00151790
Iteration 24/25 | Loss: 0.00151790
Iteration 25/25 | Loss: 0.00151790

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151790
Iteration 2/1000 | Loss: 0.00020041
Iteration 3/1000 | Loss: 0.00048344
Iteration 4/1000 | Loss: 0.00016924
Iteration 5/1000 | Loss: 0.00015228
Iteration 6/1000 | Loss: 0.00020884
Iteration 7/1000 | Loss: 0.00011635
Iteration 8/1000 | Loss: 0.00016989
Iteration 9/1000 | Loss: 0.00009277
Iteration 10/1000 | Loss: 0.00025709
Iteration 11/1000 | Loss: 0.00011706
Iteration 12/1000 | Loss: 0.00020206
Iteration 13/1000 | Loss: 0.00024645
Iteration 14/1000 | Loss: 0.00020263
Iteration 15/1000 | Loss: 0.00034952
Iteration 16/1000 | Loss: 0.00018030
Iteration 17/1000 | Loss: 0.00021896
Iteration 18/1000 | Loss: 0.00010986
Iteration 19/1000 | Loss: 0.00035211
Iteration 20/1000 | Loss: 0.00039236
Iteration 21/1000 | Loss: 0.00050867
Iteration 22/1000 | Loss: 0.00051349
Iteration 23/1000 | Loss: 0.00103856
Iteration 24/1000 | Loss: 0.00026343
Iteration 25/1000 | Loss: 0.00026923
Iteration 26/1000 | Loss: 0.00020211
Iteration 27/1000 | Loss: 0.00021981
Iteration 28/1000 | Loss: 0.00008159
Iteration 29/1000 | Loss: 0.00034412
Iteration 30/1000 | Loss: 0.00013938
Iteration 31/1000 | Loss: 0.00013500
Iteration 32/1000 | Loss: 0.00020765
Iteration 33/1000 | Loss: 0.00007211
Iteration 34/1000 | Loss: 0.00006920
Iteration 35/1000 | Loss: 0.00006762
Iteration 36/1000 | Loss: 0.00026524
Iteration 37/1000 | Loss: 0.00028271
Iteration 38/1000 | Loss: 0.00068297
Iteration 39/1000 | Loss: 0.00042918
Iteration 40/1000 | Loss: 0.00016369
Iteration 41/1000 | Loss: 0.00006559
Iteration 42/1000 | Loss: 0.00006475
Iteration 43/1000 | Loss: 0.00006382
Iteration 44/1000 | Loss: 0.00006288
Iteration 45/1000 | Loss: 0.00029093
Iteration 46/1000 | Loss: 0.00007079
Iteration 47/1000 | Loss: 0.00006243
Iteration 48/1000 | Loss: 0.00008278
Iteration 49/1000 | Loss: 0.00006159
Iteration 50/1000 | Loss: 0.00006133
Iteration 51/1000 | Loss: 0.00016199
Iteration 52/1000 | Loss: 0.00006304
Iteration 53/1000 | Loss: 0.00006085
Iteration 54/1000 | Loss: 0.00020819
Iteration 55/1000 | Loss: 0.00006372
Iteration 56/1000 | Loss: 0.00006078
Iteration 57/1000 | Loss: 0.00006046
Iteration 58/1000 | Loss: 0.00006024
Iteration 59/1000 | Loss: 0.00006019
Iteration 60/1000 | Loss: 0.00006019
Iteration 61/1000 | Loss: 0.00006008
Iteration 62/1000 | Loss: 0.00005991
Iteration 63/1000 | Loss: 0.00071742
Iteration 64/1000 | Loss: 0.00006351
Iteration 65/1000 | Loss: 0.00024113
Iteration 66/1000 | Loss: 0.00414949
Iteration 67/1000 | Loss: 0.00013642
Iteration 68/1000 | Loss: 0.00025667
Iteration 69/1000 | Loss: 0.00006147
Iteration 70/1000 | Loss: 0.00006032
Iteration 71/1000 | Loss: 0.00005988
Iteration 72/1000 | Loss: 0.00005965
Iteration 73/1000 | Loss: 0.00005956
Iteration 74/1000 | Loss: 0.00005954
Iteration 75/1000 | Loss: 0.00005954
Iteration 76/1000 | Loss: 0.00005954
Iteration 77/1000 | Loss: 0.00005954
Iteration 78/1000 | Loss: 0.00005954
Iteration 79/1000 | Loss: 0.00005954
Iteration 80/1000 | Loss: 0.00005954
Iteration 81/1000 | Loss: 0.00005954
Iteration 82/1000 | Loss: 0.00005954
Iteration 83/1000 | Loss: 0.00005953
Iteration 84/1000 | Loss: 0.00005953
Iteration 85/1000 | Loss: 0.00005953
Iteration 86/1000 | Loss: 0.00005953
Iteration 87/1000 | Loss: 0.00005953
Iteration 88/1000 | Loss: 0.00005953
Iteration 89/1000 | Loss: 0.00005953
Iteration 90/1000 | Loss: 0.00005953
Iteration 91/1000 | Loss: 0.00005953
Iteration 92/1000 | Loss: 0.00005952
Iteration 93/1000 | Loss: 0.00005952
Iteration 94/1000 | Loss: 0.00005952
Iteration 95/1000 | Loss: 0.00005952
Iteration 96/1000 | Loss: 0.00005952
Iteration 97/1000 | Loss: 0.00005952
Iteration 98/1000 | Loss: 0.00005952
Iteration 99/1000 | Loss: 0.00005952
Iteration 100/1000 | Loss: 0.00005952
Iteration 101/1000 | Loss: 0.00005952
Iteration 102/1000 | Loss: 0.00005952
Iteration 103/1000 | Loss: 0.00005952
Iteration 104/1000 | Loss: 0.00005952
Iteration 105/1000 | Loss: 0.00005951
Iteration 106/1000 | Loss: 0.00005951
Iteration 107/1000 | Loss: 0.00005951
Iteration 108/1000 | Loss: 0.00005951
Iteration 109/1000 | Loss: 0.00005951
Iteration 110/1000 | Loss: 0.00005951
Iteration 111/1000 | Loss: 0.00005951
Iteration 112/1000 | Loss: 0.00005951
Iteration 113/1000 | Loss: 0.00005951
Iteration 114/1000 | Loss: 0.00005951
Iteration 115/1000 | Loss: 0.00005951
Iteration 116/1000 | Loss: 0.00005951
Iteration 117/1000 | Loss: 0.00005951
Iteration 118/1000 | Loss: 0.00005951
Iteration 119/1000 | Loss: 0.00005950
Iteration 120/1000 | Loss: 0.00005950
Iteration 121/1000 | Loss: 0.00005950
Iteration 122/1000 | Loss: 0.00005950
Iteration 123/1000 | Loss: 0.00005950
Iteration 124/1000 | Loss: 0.00005949
Iteration 125/1000 | Loss: 0.00005949
Iteration 126/1000 | Loss: 0.00005949
Iteration 127/1000 | Loss: 0.00005949
Iteration 128/1000 | Loss: 0.00005949
Iteration 129/1000 | Loss: 0.00005948
Iteration 130/1000 | Loss: 0.00005948
Iteration 131/1000 | Loss: 0.00005948
Iteration 132/1000 | Loss: 0.00005948
Iteration 133/1000 | Loss: 0.00005948
Iteration 134/1000 | Loss: 0.00005948
Iteration 135/1000 | Loss: 0.00005948
Iteration 136/1000 | Loss: 0.00005947
Iteration 137/1000 | Loss: 0.00005947
Iteration 138/1000 | Loss: 0.00005947
Iteration 139/1000 | Loss: 0.00005947
Iteration 140/1000 | Loss: 0.00005947
Iteration 141/1000 | Loss: 0.00005947
Iteration 142/1000 | Loss: 0.00005947
Iteration 143/1000 | Loss: 0.00005946
Iteration 144/1000 | Loss: 0.00005946
Iteration 145/1000 | Loss: 0.00005946
Iteration 146/1000 | Loss: 0.00005946
Iteration 147/1000 | Loss: 0.00005946
Iteration 148/1000 | Loss: 0.00005946
Iteration 149/1000 | Loss: 0.00005945
Iteration 150/1000 | Loss: 0.00005945
Iteration 151/1000 | Loss: 0.00005945
Iteration 152/1000 | Loss: 0.00005945
Iteration 153/1000 | Loss: 0.00005945
Iteration 154/1000 | Loss: 0.00005945
Iteration 155/1000 | Loss: 0.00005945
Iteration 156/1000 | Loss: 0.00005945
Iteration 157/1000 | Loss: 0.00005945
Iteration 158/1000 | Loss: 0.00005945
Iteration 159/1000 | Loss: 0.00005944
Iteration 160/1000 | Loss: 0.00005944
Iteration 161/1000 | Loss: 0.00005944
Iteration 162/1000 | Loss: 0.00005944
Iteration 163/1000 | Loss: 0.00005943
Iteration 164/1000 | Loss: 0.00005943
Iteration 165/1000 | Loss: 0.00005943
Iteration 166/1000 | Loss: 0.00005942
Iteration 167/1000 | Loss: 0.00005942
Iteration 168/1000 | Loss: 0.00005942
Iteration 169/1000 | Loss: 0.00005942
Iteration 170/1000 | Loss: 0.00005942
Iteration 171/1000 | Loss: 0.00005942
Iteration 172/1000 | Loss: 0.00005941
Iteration 173/1000 | Loss: 0.00005941
Iteration 174/1000 | Loss: 0.00005941
Iteration 175/1000 | Loss: 0.00005941
Iteration 176/1000 | Loss: 0.00005941
Iteration 177/1000 | Loss: 0.00005941
Iteration 178/1000 | Loss: 0.00005941
Iteration 179/1000 | Loss: 0.00005941
Iteration 180/1000 | Loss: 0.00005941
Iteration 181/1000 | Loss: 0.00005941
Iteration 182/1000 | Loss: 0.00005940
Iteration 183/1000 | Loss: 0.00005940
Iteration 184/1000 | Loss: 0.00005940
Iteration 185/1000 | Loss: 0.00005940
Iteration 186/1000 | Loss: 0.00005940
Iteration 187/1000 | Loss: 0.00005940
Iteration 188/1000 | Loss: 0.00005940
Iteration 189/1000 | Loss: 0.00005940
Iteration 190/1000 | Loss: 0.00005939
Iteration 191/1000 | Loss: 0.00005939
Iteration 192/1000 | Loss: 0.00005939
Iteration 193/1000 | Loss: 0.00005939
Iteration 194/1000 | Loss: 0.00005939
Iteration 195/1000 | Loss: 0.00005939
Iteration 196/1000 | Loss: 0.00005939
Iteration 197/1000 | Loss: 0.00005939
Iteration 198/1000 | Loss: 0.00005939
Iteration 199/1000 | Loss: 0.00005939
Iteration 200/1000 | Loss: 0.00005939
Iteration 201/1000 | Loss: 0.00005939
Iteration 202/1000 | Loss: 0.00005938
Iteration 203/1000 | Loss: 0.00005938
Iteration 204/1000 | Loss: 0.00005938
Iteration 205/1000 | Loss: 0.00005938
Iteration 206/1000 | Loss: 0.00005938
Iteration 207/1000 | Loss: 0.00005937
Iteration 208/1000 | Loss: 0.00005937
Iteration 209/1000 | Loss: 0.00005936
Iteration 210/1000 | Loss: 0.00005936
Iteration 211/1000 | Loss: 0.00005936
Iteration 212/1000 | Loss: 0.00005936
Iteration 213/1000 | Loss: 0.00005936
Iteration 214/1000 | Loss: 0.00005936
Iteration 215/1000 | Loss: 0.00005936
Iteration 216/1000 | Loss: 0.00005936
Iteration 217/1000 | Loss: 0.00005936
Iteration 218/1000 | Loss: 0.00005936
Iteration 219/1000 | Loss: 0.00005935
Iteration 220/1000 | Loss: 0.00005935
Iteration 221/1000 | Loss: 0.00005935
Iteration 222/1000 | Loss: 0.00005935
Iteration 223/1000 | Loss: 0.00005935
Iteration 224/1000 | Loss: 0.00005935
Iteration 225/1000 | Loss: 0.00005935
Iteration 226/1000 | Loss: 0.00005935
Iteration 227/1000 | Loss: 0.00005935
Iteration 228/1000 | Loss: 0.00005935
Iteration 229/1000 | Loss: 0.00005935
Iteration 230/1000 | Loss: 0.00005935
Iteration 231/1000 | Loss: 0.00005935
Iteration 232/1000 | Loss: 0.00005935
Iteration 233/1000 | Loss: 0.00005935
Iteration 234/1000 | Loss: 0.00005935
Iteration 235/1000 | Loss: 0.00005935
Iteration 236/1000 | Loss: 0.00005935
Iteration 237/1000 | Loss: 0.00005935
Iteration 238/1000 | Loss: 0.00005935
Iteration 239/1000 | Loss: 0.00005935
Iteration 240/1000 | Loss: 0.00005935
Iteration 241/1000 | Loss: 0.00005935
Iteration 242/1000 | Loss: 0.00005935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [5.934896398684941e-05, 5.934896398684941e-05, 5.934896398684941e-05, 5.934896398684941e-05, 5.934896398684941e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.934896398684941e-05

Optimization complete. Final v2v error: 5.841857433319092 mm

Highest mean error: 6.666029930114746 mm for frame 93

Lowest mean error: 3.9848060607910156 mm for frame 57

Saving results

Total time: 180.27648615837097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011853
Iteration 2/25 | Loss: 0.00336385
Iteration 3/25 | Loss: 0.00401941
Iteration 4/25 | Loss: 0.00219296
Iteration 5/25 | Loss: 0.00184163
Iteration 6/25 | Loss: 0.00176185
Iteration 7/25 | Loss: 0.00166081
Iteration 8/25 | Loss: 0.00150636
Iteration 9/25 | Loss: 0.00145938
Iteration 10/25 | Loss: 0.00140960
Iteration 11/25 | Loss: 0.00141159
Iteration 12/25 | Loss: 0.00139088
Iteration 13/25 | Loss: 0.00139112
Iteration 14/25 | Loss: 0.00138616
Iteration 15/25 | Loss: 0.00138403
Iteration 16/25 | Loss: 0.00138501
Iteration 17/25 | Loss: 0.00137788
Iteration 18/25 | Loss: 0.00137533
Iteration 19/25 | Loss: 0.00137485
Iteration 20/25 | Loss: 0.00137481
Iteration 21/25 | Loss: 0.00137480
Iteration 22/25 | Loss: 0.00137480
Iteration 23/25 | Loss: 0.00137480
Iteration 24/25 | Loss: 0.00137480
Iteration 25/25 | Loss: 0.00137480

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38791966
Iteration 2/25 | Loss: 0.00110071
Iteration 3/25 | Loss: 0.00082824
Iteration 4/25 | Loss: 0.00082824
Iteration 5/25 | Loss: 0.00082824
Iteration 6/25 | Loss: 0.00082824
Iteration 7/25 | Loss: 0.00082824
Iteration 8/25 | Loss: 0.00082823
Iteration 9/25 | Loss: 0.00082823
Iteration 10/25 | Loss: 0.00082823
Iteration 11/25 | Loss: 0.00082823
Iteration 12/25 | Loss: 0.00082823
Iteration 13/25 | Loss: 0.00082823
Iteration 14/25 | Loss: 0.00082823
Iteration 15/25 | Loss: 0.00082823
Iteration 16/25 | Loss: 0.00082823
Iteration 17/25 | Loss: 0.00082823
Iteration 18/25 | Loss: 0.00082823
Iteration 19/25 | Loss: 0.00082823
Iteration 20/25 | Loss: 0.00082823
Iteration 21/25 | Loss: 0.00082823
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008282338967546821, 0.0008282338967546821, 0.0008282338967546821, 0.0008282338967546821, 0.0008282338967546821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008282338967546821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082823
Iteration 2/1000 | Loss: 0.00003973
Iteration 3/1000 | Loss: 0.00003754
Iteration 4/1000 | Loss: 0.00003264
Iteration 5/1000 | Loss: 0.00003424
Iteration 6/1000 | Loss: 0.00005865
Iteration 7/1000 | Loss: 0.00002770
Iteration 8/1000 | Loss: 0.00002653
Iteration 9/1000 | Loss: 0.00002570
Iteration 10/1000 | Loss: 0.00002514
Iteration 11/1000 | Loss: 0.00012443
Iteration 12/1000 | Loss: 0.00071040
Iteration 13/1000 | Loss: 0.00028097
Iteration 14/1000 | Loss: 0.00010988
Iteration 15/1000 | Loss: 0.00002552
Iteration 16/1000 | Loss: 0.00002436
Iteration 17/1000 | Loss: 0.00002392
Iteration 18/1000 | Loss: 0.00020132
Iteration 19/1000 | Loss: 0.00003394
Iteration 20/1000 | Loss: 0.00004063
Iteration 21/1000 | Loss: 0.00002353
Iteration 22/1000 | Loss: 0.00002337
Iteration 23/1000 | Loss: 0.00002325
Iteration 24/1000 | Loss: 0.00002321
Iteration 25/1000 | Loss: 0.00002311
Iteration 26/1000 | Loss: 0.00002309
Iteration 27/1000 | Loss: 0.00002309
Iteration 28/1000 | Loss: 0.00011057
Iteration 29/1000 | Loss: 0.00002315
Iteration 30/1000 | Loss: 0.00002305
Iteration 31/1000 | Loss: 0.00002305
Iteration 32/1000 | Loss: 0.00002305
Iteration 33/1000 | Loss: 0.00002305
Iteration 34/1000 | Loss: 0.00002305
Iteration 35/1000 | Loss: 0.00002305
Iteration 36/1000 | Loss: 0.00002304
Iteration 37/1000 | Loss: 0.00002304
Iteration 38/1000 | Loss: 0.00002304
Iteration 39/1000 | Loss: 0.00002304
Iteration 40/1000 | Loss: 0.00002301
Iteration 41/1000 | Loss: 0.00002301
Iteration 42/1000 | Loss: 0.00002300
Iteration 43/1000 | Loss: 0.00002300
Iteration 44/1000 | Loss: 0.00002299
Iteration 45/1000 | Loss: 0.00002299
Iteration 46/1000 | Loss: 0.00002299
Iteration 47/1000 | Loss: 0.00002299
Iteration 48/1000 | Loss: 0.00002299
Iteration 49/1000 | Loss: 0.00002299
Iteration 50/1000 | Loss: 0.00002299
Iteration 51/1000 | Loss: 0.00002299
Iteration 52/1000 | Loss: 0.00002299
Iteration 53/1000 | Loss: 0.00002298
Iteration 54/1000 | Loss: 0.00002295
Iteration 55/1000 | Loss: 0.00002295
Iteration 56/1000 | Loss: 0.00002294
Iteration 57/1000 | Loss: 0.00002293
Iteration 58/1000 | Loss: 0.00002291
Iteration 59/1000 | Loss: 0.00002291
Iteration 60/1000 | Loss: 0.00002290
Iteration 61/1000 | Loss: 0.00002289
Iteration 62/1000 | Loss: 0.00002289
Iteration 63/1000 | Loss: 0.00002289
Iteration 64/1000 | Loss: 0.00002288
Iteration 65/1000 | Loss: 0.00002288
Iteration 66/1000 | Loss: 0.00002288
Iteration 67/1000 | Loss: 0.00002288
Iteration 68/1000 | Loss: 0.00002287
Iteration 69/1000 | Loss: 0.00002287
Iteration 70/1000 | Loss: 0.00002286
Iteration 71/1000 | Loss: 0.00002285
Iteration 72/1000 | Loss: 0.00002285
Iteration 73/1000 | Loss: 0.00002283
Iteration 74/1000 | Loss: 0.00002283
Iteration 75/1000 | Loss: 0.00002283
Iteration 76/1000 | Loss: 0.00002282
Iteration 77/1000 | Loss: 0.00002282
Iteration 78/1000 | Loss: 0.00002282
Iteration 79/1000 | Loss: 0.00002282
Iteration 80/1000 | Loss: 0.00002282
Iteration 81/1000 | Loss: 0.00002282
Iteration 82/1000 | Loss: 0.00002281
Iteration 83/1000 | Loss: 0.00002281
Iteration 84/1000 | Loss: 0.00002281
Iteration 85/1000 | Loss: 0.00002281
Iteration 86/1000 | Loss: 0.00002280
Iteration 87/1000 | Loss: 0.00002280
Iteration 88/1000 | Loss: 0.00002280
Iteration 89/1000 | Loss: 0.00002280
Iteration 90/1000 | Loss: 0.00002279
Iteration 91/1000 | Loss: 0.00002279
Iteration 92/1000 | Loss: 0.00002279
Iteration 93/1000 | Loss: 0.00002279
Iteration 94/1000 | Loss: 0.00002279
Iteration 95/1000 | Loss: 0.00002279
Iteration 96/1000 | Loss: 0.00002277
Iteration 97/1000 | Loss: 0.00002277
Iteration 98/1000 | Loss: 0.00002276
Iteration 99/1000 | Loss: 0.00002274
Iteration 100/1000 | Loss: 0.00002273
Iteration 101/1000 | Loss: 0.00002273
Iteration 102/1000 | Loss: 0.00002271
Iteration 103/1000 | Loss: 0.00002271
Iteration 104/1000 | Loss: 0.00002271
Iteration 105/1000 | Loss: 0.00002271
Iteration 106/1000 | Loss: 0.00002271
Iteration 107/1000 | Loss: 0.00002270
Iteration 108/1000 | Loss: 0.00002270
Iteration 109/1000 | Loss: 0.00002270
Iteration 110/1000 | Loss: 0.00002270
Iteration 111/1000 | Loss: 0.00002270
Iteration 112/1000 | Loss: 0.00002270
Iteration 113/1000 | Loss: 0.00002270
Iteration 114/1000 | Loss: 0.00002270
Iteration 115/1000 | Loss: 0.00002270
Iteration 116/1000 | Loss: 0.00002270
Iteration 117/1000 | Loss: 0.00002270
Iteration 118/1000 | Loss: 0.00002270
Iteration 119/1000 | Loss: 0.00002270
Iteration 120/1000 | Loss: 0.00002270
Iteration 121/1000 | Loss: 0.00002269
Iteration 122/1000 | Loss: 0.00002269
Iteration 123/1000 | Loss: 0.00002269
Iteration 124/1000 | Loss: 0.00002269
Iteration 125/1000 | Loss: 0.00002269
Iteration 126/1000 | Loss: 0.00002269
Iteration 127/1000 | Loss: 0.00002269
Iteration 128/1000 | Loss: 0.00002269
Iteration 129/1000 | Loss: 0.00002269
Iteration 130/1000 | Loss: 0.00002269
Iteration 131/1000 | Loss: 0.00002269
Iteration 132/1000 | Loss: 0.00002269
Iteration 133/1000 | Loss: 0.00002269
Iteration 134/1000 | Loss: 0.00002269
Iteration 135/1000 | Loss: 0.00002269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.2689337129122578e-05, 2.2689337129122578e-05, 2.2689337129122578e-05, 2.2689337129122578e-05, 2.2689337129122578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2689337129122578e-05

Optimization complete. Final v2v error: 4.068523406982422 mm

Highest mean error: 4.397195339202881 mm for frame 39

Lowest mean error: 3.917314052581787 mm for frame 27

Saving results

Total time: 92.4306571483612
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836673
Iteration 2/25 | Loss: 0.00132922
Iteration 3/25 | Loss: 0.00127676
Iteration 4/25 | Loss: 0.00127007
Iteration 5/25 | Loss: 0.00126827
Iteration 6/25 | Loss: 0.00126827
Iteration 7/25 | Loss: 0.00126827
Iteration 8/25 | Loss: 0.00126827
Iteration 9/25 | Loss: 0.00126827
Iteration 10/25 | Loss: 0.00126827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012682678643614054, 0.0012682678643614054, 0.0012682678643614054, 0.0012682678643614054, 0.0012682678643614054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012682678643614054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69595015
Iteration 2/25 | Loss: 0.00087460
Iteration 3/25 | Loss: 0.00087460
Iteration 4/25 | Loss: 0.00087460
Iteration 5/25 | Loss: 0.00087460
Iteration 6/25 | Loss: 0.00087460
Iteration 7/25 | Loss: 0.00087460
Iteration 8/25 | Loss: 0.00087460
Iteration 9/25 | Loss: 0.00087460
Iteration 10/25 | Loss: 0.00087460
Iteration 11/25 | Loss: 0.00087460
Iteration 12/25 | Loss: 0.00087460
Iteration 13/25 | Loss: 0.00087460
Iteration 14/25 | Loss: 0.00087460
Iteration 15/25 | Loss: 0.00087460
Iteration 16/25 | Loss: 0.00087460
Iteration 17/25 | Loss: 0.00087460
Iteration 18/25 | Loss: 0.00087460
Iteration 19/25 | Loss: 0.00087460
Iteration 20/25 | Loss: 0.00087460
Iteration 21/25 | Loss: 0.00087460
Iteration 22/25 | Loss: 0.00087460
Iteration 23/25 | Loss: 0.00087460
Iteration 24/25 | Loss: 0.00087460
Iteration 25/25 | Loss: 0.00087460

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087460
Iteration 2/1000 | Loss: 0.00002359
Iteration 3/1000 | Loss: 0.00001788
Iteration 4/1000 | Loss: 0.00001590
Iteration 5/1000 | Loss: 0.00001493
Iteration 6/1000 | Loss: 0.00001436
Iteration 7/1000 | Loss: 0.00001385
Iteration 8/1000 | Loss: 0.00001360
Iteration 9/1000 | Loss: 0.00001353
Iteration 10/1000 | Loss: 0.00001330
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001306
Iteration 13/1000 | Loss: 0.00001300
Iteration 14/1000 | Loss: 0.00001299
Iteration 15/1000 | Loss: 0.00001279
Iteration 16/1000 | Loss: 0.00001279
Iteration 17/1000 | Loss: 0.00001278
Iteration 18/1000 | Loss: 0.00001277
Iteration 19/1000 | Loss: 0.00001277
Iteration 20/1000 | Loss: 0.00001277
Iteration 21/1000 | Loss: 0.00001276
Iteration 22/1000 | Loss: 0.00001275
Iteration 23/1000 | Loss: 0.00001270
Iteration 24/1000 | Loss: 0.00001265
Iteration 25/1000 | Loss: 0.00001264
Iteration 26/1000 | Loss: 0.00001264
Iteration 27/1000 | Loss: 0.00001260
Iteration 28/1000 | Loss: 0.00001259
Iteration 29/1000 | Loss: 0.00001259
Iteration 30/1000 | Loss: 0.00001259
Iteration 31/1000 | Loss: 0.00001258
Iteration 32/1000 | Loss: 0.00001258
Iteration 33/1000 | Loss: 0.00001257
Iteration 34/1000 | Loss: 0.00001257
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001255
Iteration 37/1000 | Loss: 0.00001254
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001252
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001251
Iteration 44/1000 | Loss: 0.00001249
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001248
Iteration 47/1000 | Loss: 0.00001248
Iteration 48/1000 | Loss: 0.00001248
Iteration 49/1000 | Loss: 0.00001248
Iteration 50/1000 | Loss: 0.00001248
Iteration 51/1000 | Loss: 0.00001248
Iteration 52/1000 | Loss: 0.00001248
Iteration 53/1000 | Loss: 0.00001248
Iteration 54/1000 | Loss: 0.00001244
Iteration 55/1000 | Loss: 0.00001244
Iteration 56/1000 | Loss: 0.00001243
Iteration 57/1000 | Loss: 0.00001243
Iteration 58/1000 | Loss: 0.00001243
Iteration 59/1000 | Loss: 0.00001241
Iteration 60/1000 | Loss: 0.00001241
Iteration 61/1000 | Loss: 0.00001239
Iteration 62/1000 | Loss: 0.00001239
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001238
Iteration 65/1000 | Loss: 0.00001237
Iteration 66/1000 | Loss: 0.00001237
Iteration 67/1000 | Loss: 0.00001232
Iteration 68/1000 | Loss: 0.00001232
Iteration 69/1000 | Loss: 0.00001232
Iteration 70/1000 | Loss: 0.00001230
Iteration 71/1000 | Loss: 0.00001230
Iteration 72/1000 | Loss: 0.00001230
Iteration 73/1000 | Loss: 0.00001229
Iteration 74/1000 | Loss: 0.00001229
Iteration 75/1000 | Loss: 0.00001229
Iteration 76/1000 | Loss: 0.00001229
Iteration 77/1000 | Loss: 0.00001229
Iteration 78/1000 | Loss: 0.00001229
Iteration 79/1000 | Loss: 0.00001229
Iteration 80/1000 | Loss: 0.00001229
Iteration 81/1000 | Loss: 0.00001229
Iteration 82/1000 | Loss: 0.00001229
Iteration 83/1000 | Loss: 0.00001228
Iteration 84/1000 | Loss: 0.00001228
Iteration 85/1000 | Loss: 0.00001228
Iteration 86/1000 | Loss: 0.00001228
Iteration 87/1000 | Loss: 0.00001228
Iteration 88/1000 | Loss: 0.00001228
Iteration 89/1000 | Loss: 0.00001226
Iteration 90/1000 | Loss: 0.00001226
Iteration 91/1000 | Loss: 0.00001226
Iteration 92/1000 | Loss: 0.00001226
Iteration 93/1000 | Loss: 0.00001226
Iteration 94/1000 | Loss: 0.00001226
Iteration 95/1000 | Loss: 0.00001226
Iteration 96/1000 | Loss: 0.00001225
Iteration 97/1000 | Loss: 0.00001225
Iteration 98/1000 | Loss: 0.00001225
Iteration 99/1000 | Loss: 0.00001224
Iteration 100/1000 | Loss: 0.00001223
Iteration 101/1000 | Loss: 0.00001223
Iteration 102/1000 | Loss: 0.00001223
Iteration 103/1000 | Loss: 0.00001223
Iteration 104/1000 | Loss: 0.00001223
Iteration 105/1000 | Loss: 0.00001223
Iteration 106/1000 | Loss: 0.00001223
Iteration 107/1000 | Loss: 0.00001222
Iteration 108/1000 | Loss: 0.00001222
Iteration 109/1000 | Loss: 0.00001222
Iteration 110/1000 | Loss: 0.00001222
Iteration 111/1000 | Loss: 0.00001222
Iteration 112/1000 | Loss: 0.00001222
Iteration 113/1000 | Loss: 0.00001222
Iteration 114/1000 | Loss: 0.00001222
Iteration 115/1000 | Loss: 0.00001222
Iteration 116/1000 | Loss: 0.00001222
Iteration 117/1000 | Loss: 0.00001222
Iteration 118/1000 | Loss: 0.00001221
Iteration 119/1000 | Loss: 0.00001221
Iteration 120/1000 | Loss: 0.00001221
Iteration 121/1000 | Loss: 0.00001220
Iteration 122/1000 | Loss: 0.00001220
Iteration 123/1000 | Loss: 0.00001220
Iteration 124/1000 | Loss: 0.00001220
Iteration 125/1000 | Loss: 0.00001220
Iteration 126/1000 | Loss: 0.00001220
Iteration 127/1000 | Loss: 0.00001220
Iteration 128/1000 | Loss: 0.00001220
Iteration 129/1000 | Loss: 0.00001220
Iteration 130/1000 | Loss: 0.00001220
Iteration 131/1000 | Loss: 0.00001220
Iteration 132/1000 | Loss: 0.00001220
Iteration 133/1000 | Loss: 0.00001220
Iteration 134/1000 | Loss: 0.00001220
Iteration 135/1000 | Loss: 0.00001220
Iteration 136/1000 | Loss: 0.00001220
Iteration 137/1000 | Loss: 0.00001220
Iteration 138/1000 | Loss: 0.00001220
Iteration 139/1000 | Loss: 0.00001220
Iteration 140/1000 | Loss: 0.00001220
Iteration 141/1000 | Loss: 0.00001220
Iteration 142/1000 | Loss: 0.00001220
Iteration 143/1000 | Loss: 0.00001220
Iteration 144/1000 | Loss: 0.00001220
Iteration 145/1000 | Loss: 0.00001220
Iteration 146/1000 | Loss: 0.00001220
Iteration 147/1000 | Loss: 0.00001220
Iteration 148/1000 | Loss: 0.00001220
Iteration 149/1000 | Loss: 0.00001220
Iteration 150/1000 | Loss: 0.00001220
Iteration 151/1000 | Loss: 0.00001220
Iteration 152/1000 | Loss: 0.00001220
Iteration 153/1000 | Loss: 0.00001220
Iteration 154/1000 | Loss: 0.00001220
Iteration 155/1000 | Loss: 0.00001220
Iteration 156/1000 | Loss: 0.00001220
Iteration 157/1000 | Loss: 0.00001220
Iteration 158/1000 | Loss: 0.00001220
Iteration 159/1000 | Loss: 0.00001220
Iteration 160/1000 | Loss: 0.00001220
Iteration 161/1000 | Loss: 0.00001220
Iteration 162/1000 | Loss: 0.00001220
Iteration 163/1000 | Loss: 0.00001220
Iteration 164/1000 | Loss: 0.00001220
Iteration 165/1000 | Loss: 0.00001220
Iteration 166/1000 | Loss: 0.00001220
Iteration 167/1000 | Loss: 0.00001220
Iteration 168/1000 | Loss: 0.00001220
Iteration 169/1000 | Loss: 0.00001220
Iteration 170/1000 | Loss: 0.00001220
Iteration 171/1000 | Loss: 0.00001220
Iteration 172/1000 | Loss: 0.00001220
Iteration 173/1000 | Loss: 0.00001220
Iteration 174/1000 | Loss: 0.00001220
Iteration 175/1000 | Loss: 0.00001220
Iteration 176/1000 | Loss: 0.00001220
Iteration 177/1000 | Loss: 0.00001220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.2199986485939007e-05, 1.2199986485939007e-05, 1.2199986485939007e-05, 1.2199986485939007e-05, 1.2199986485939007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2199986485939007e-05

Optimization complete. Final v2v error: 2.9635307788848877 mm

Highest mean error: 3.6983249187469482 mm for frame 44

Lowest mean error: 2.790415048599243 mm for frame 97

Saving results

Total time: 36.43167066574097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874192
Iteration 2/25 | Loss: 0.00175919
Iteration 3/25 | Loss: 0.00149345
Iteration 4/25 | Loss: 0.00144517
Iteration 5/25 | Loss: 0.00145076
Iteration 6/25 | Loss: 0.00143495
Iteration 7/25 | Loss: 0.00141725
Iteration 8/25 | Loss: 0.00139801
Iteration 9/25 | Loss: 0.00138963
Iteration 10/25 | Loss: 0.00139136
Iteration 11/25 | Loss: 0.00138906
Iteration 12/25 | Loss: 0.00139700
Iteration 13/25 | Loss: 0.00139163
Iteration 14/25 | Loss: 0.00138492
Iteration 15/25 | Loss: 0.00138179
Iteration 16/25 | Loss: 0.00137821
Iteration 17/25 | Loss: 0.00137769
Iteration 18/25 | Loss: 0.00137725
Iteration 19/25 | Loss: 0.00137833
Iteration 20/25 | Loss: 0.00137748
Iteration 21/25 | Loss: 0.00137747
Iteration 22/25 | Loss: 0.00137786
Iteration 23/25 | Loss: 0.00137664
Iteration 24/25 | Loss: 0.00137715
Iteration 25/25 | Loss: 0.00137639

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.25810814
Iteration 2/25 | Loss: 0.00125740
Iteration 3/25 | Loss: 0.00125740
Iteration 4/25 | Loss: 0.00125740
Iteration 5/25 | Loss: 0.00125740
Iteration 6/25 | Loss: 0.00125740
Iteration 7/25 | Loss: 0.00125740
Iteration 8/25 | Loss: 0.00125740
Iteration 9/25 | Loss: 0.00125740
Iteration 10/25 | Loss: 0.00125740
Iteration 11/25 | Loss: 0.00125740
Iteration 12/25 | Loss: 0.00125740
Iteration 13/25 | Loss: 0.00125740
Iteration 14/25 | Loss: 0.00125740
Iteration 15/25 | Loss: 0.00125740
Iteration 16/25 | Loss: 0.00125740
Iteration 17/25 | Loss: 0.00125740
Iteration 18/25 | Loss: 0.00125740
Iteration 19/25 | Loss: 0.00125740
Iteration 20/25 | Loss: 0.00125740
Iteration 21/25 | Loss: 0.00125740
Iteration 22/25 | Loss: 0.00125740
Iteration 23/25 | Loss: 0.00125740
Iteration 24/25 | Loss: 0.00125740
Iteration 25/25 | Loss: 0.00125740

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125740
Iteration 2/1000 | Loss: 0.00015768
Iteration 3/1000 | Loss: 0.00037415
Iteration 4/1000 | Loss: 0.00004763
Iteration 5/1000 | Loss: 0.00003534
Iteration 6/1000 | Loss: 0.00004309
Iteration 7/1000 | Loss: 0.00003179
Iteration 8/1000 | Loss: 0.00004401
Iteration 9/1000 | Loss: 0.00004254
Iteration 10/1000 | Loss: 0.00004365
Iteration 11/1000 | Loss: 0.00004415
Iteration 12/1000 | Loss: 0.00004221
Iteration 13/1000 | Loss: 0.00004210
Iteration 14/1000 | Loss: 0.00003753
Iteration 15/1000 | Loss: 0.00003472
Iteration 16/1000 | Loss: 0.00003832
Iteration 17/1000 | Loss: 0.00004776
Iteration 18/1000 | Loss: 0.00003535
Iteration 19/1000 | Loss: 0.00002816
Iteration 20/1000 | Loss: 0.00002706
Iteration 21/1000 | Loss: 0.00002621
Iteration 22/1000 | Loss: 0.00002564
Iteration 23/1000 | Loss: 0.00002519
Iteration 24/1000 | Loss: 0.00002492
Iteration 25/1000 | Loss: 0.00002463
Iteration 26/1000 | Loss: 0.00002438
Iteration 27/1000 | Loss: 0.00002422
Iteration 28/1000 | Loss: 0.00002411
Iteration 29/1000 | Loss: 0.00002409
Iteration 30/1000 | Loss: 0.00002407
Iteration 31/1000 | Loss: 0.00002407
Iteration 32/1000 | Loss: 0.00002407
Iteration 33/1000 | Loss: 0.00002407
Iteration 34/1000 | Loss: 0.00002407
Iteration 35/1000 | Loss: 0.00002407
Iteration 36/1000 | Loss: 0.00002407
Iteration 37/1000 | Loss: 0.00002407
Iteration 38/1000 | Loss: 0.00002406
Iteration 39/1000 | Loss: 0.00002406
Iteration 40/1000 | Loss: 0.00002406
Iteration 41/1000 | Loss: 0.00002406
Iteration 42/1000 | Loss: 0.00002406
Iteration 43/1000 | Loss: 0.00002405
Iteration 44/1000 | Loss: 0.00002404
Iteration 45/1000 | Loss: 0.00002404
Iteration 46/1000 | Loss: 0.00002404
Iteration 47/1000 | Loss: 0.00002404
Iteration 48/1000 | Loss: 0.00002404
Iteration 49/1000 | Loss: 0.00002403
Iteration 50/1000 | Loss: 0.00002403
Iteration 51/1000 | Loss: 0.00002403
Iteration 52/1000 | Loss: 0.00002401
Iteration 53/1000 | Loss: 0.00002398
Iteration 54/1000 | Loss: 0.00002396
Iteration 55/1000 | Loss: 0.00002396
Iteration 56/1000 | Loss: 0.00002396
Iteration 57/1000 | Loss: 0.00002395
Iteration 58/1000 | Loss: 0.00002395
Iteration 59/1000 | Loss: 0.00002395
Iteration 60/1000 | Loss: 0.00002395
Iteration 61/1000 | Loss: 0.00002395
Iteration 62/1000 | Loss: 0.00002394
Iteration 63/1000 | Loss: 0.00002394
Iteration 64/1000 | Loss: 0.00002394
Iteration 65/1000 | Loss: 0.00002394
Iteration 66/1000 | Loss: 0.00002394
Iteration 67/1000 | Loss: 0.00002393
Iteration 68/1000 | Loss: 0.00002393
Iteration 69/1000 | Loss: 0.00002386
Iteration 70/1000 | Loss: 0.00002381
Iteration 71/1000 | Loss: 0.00002380
Iteration 72/1000 | Loss: 0.00002380
Iteration 73/1000 | Loss: 0.00002380
Iteration 74/1000 | Loss: 0.00002380
Iteration 75/1000 | Loss: 0.00002380
Iteration 76/1000 | Loss: 0.00002379
Iteration 77/1000 | Loss: 0.00002379
Iteration 78/1000 | Loss: 0.00002379
Iteration 79/1000 | Loss: 0.00002379
Iteration 80/1000 | Loss: 0.00002379
Iteration 81/1000 | Loss: 0.00002379
Iteration 82/1000 | Loss: 0.00002379
Iteration 83/1000 | Loss: 0.00002378
Iteration 84/1000 | Loss: 0.00002378
Iteration 85/1000 | Loss: 0.00002378
Iteration 86/1000 | Loss: 0.00002378
Iteration 87/1000 | Loss: 0.00002377
Iteration 88/1000 | Loss: 0.00002376
Iteration 89/1000 | Loss: 0.00002376
Iteration 90/1000 | Loss: 0.00002375
Iteration 91/1000 | Loss: 0.00002375
Iteration 92/1000 | Loss: 0.00002375
Iteration 93/1000 | Loss: 0.00002374
Iteration 94/1000 | Loss: 0.00002374
Iteration 95/1000 | Loss: 0.00002373
Iteration 96/1000 | Loss: 0.00002373
Iteration 97/1000 | Loss: 0.00002372
Iteration 98/1000 | Loss: 0.00002372
Iteration 99/1000 | Loss: 0.00002372
Iteration 100/1000 | Loss: 0.00002371
Iteration 101/1000 | Loss: 0.00002371
Iteration 102/1000 | Loss: 0.00002370
Iteration 103/1000 | Loss: 0.00002370
Iteration 104/1000 | Loss: 0.00002370
Iteration 105/1000 | Loss: 0.00002369
Iteration 106/1000 | Loss: 0.00002369
Iteration 107/1000 | Loss: 0.00002369
Iteration 108/1000 | Loss: 0.00002368
Iteration 109/1000 | Loss: 0.00002368
Iteration 110/1000 | Loss: 0.00002368
Iteration 111/1000 | Loss: 0.00002368
Iteration 112/1000 | Loss: 0.00002368
Iteration 113/1000 | Loss: 0.00002367
Iteration 114/1000 | Loss: 0.00002367
Iteration 115/1000 | Loss: 0.00002367
Iteration 116/1000 | Loss: 0.00002367
Iteration 117/1000 | Loss: 0.00002367
Iteration 118/1000 | Loss: 0.00002367
Iteration 119/1000 | Loss: 0.00002367
Iteration 120/1000 | Loss: 0.00002367
Iteration 121/1000 | Loss: 0.00002367
Iteration 122/1000 | Loss: 0.00002367
Iteration 123/1000 | Loss: 0.00002367
Iteration 124/1000 | Loss: 0.00002366
Iteration 125/1000 | Loss: 0.00002366
Iteration 126/1000 | Loss: 0.00002366
Iteration 127/1000 | Loss: 0.00002366
Iteration 128/1000 | Loss: 0.00002366
Iteration 129/1000 | Loss: 0.00002366
Iteration 130/1000 | Loss: 0.00002366
Iteration 131/1000 | Loss: 0.00002366
Iteration 132/1000 | Loss: 0.00002366
Iteration 133/1000 | Loss: 0.00002365
Iteration 134/1000 | Loss: 0.00002365
Iteration 135/1000 | Loss: 0.00002365
Iteration 136/1000 | Loss: 0.00002365
Iteration 137/1000 | Loss: 0.00002364
Iteration 138/1000 | Loss: 0.00002364
Iteration 139/1000 | Loss: 0.00002364
Iteration 140/1000 | Loss: 0.00002364
Iteration 141/1000 | Loss: 0.00002364
Iteration 142/1000 | Loss: 0.00002364
Iteration 143/1000 | Loss: 0.00002364
Iteration 144/1000 | Loss: 0.00002364
Iteration 145/1000 | Loss: 0.00002364
Iteration 146/1000 | Loss: 0.00002364
Iteration 147/1000 | Loss: 0.00002364
Iteration 148/1000 | Loss: 0.00002364
Iteration 149/1000 | Loss: 0.00002364
Iteration 150/1000 | Loss: 0.00002364
Iteration 151/1000 | Loss: 0.00002364
Iteration 152/1000 | Loss: 0.00002363
Iteration 153/1000 | Loss: 0.00002363
Iteration 154/1000 | Loss: 0.00002363
Iteration 155/1000 | Loss: 0.00002363
Iteration 156/1000 | Loss: 0.00002363
Iteration 157/1000 | Loss: 0.00002363
Iteration 158/1000 | Loss: 0.00002363
Iteration 159/1000 | Loss: 0.00002363
Iteration 160/1000 | Loss: 0.00002363
Iteration 161/1000 | Loss: 0.00002363
Iteration 162/1000 | Loss: 0.00002363
Iteration 163/1000 | Loss: 0.00002363
Iteration 164/1000 | Loss: 0.00002363
Iteration 165/1000 | Loss: 0.00002363
Iteration 166/1000 | Loss: 0.00002363
Iteration 167/1000 | Loss: 0.00002363
Iteration 168/1000 | Loss: 0.00002362
Iteration 169/1000 | Loss: 0.00002362
Iteration 170/1000 | Loss: 0.00002362
Iteration 171/1000 | Loss: 0.00002362
Iteration 172/1000 | Loss: 0.00002362
Iteration 173/1000 | Loss: 0.00002362
Iteration 174/1000 | Loss: 0.00002362
Iteration 175/1000 | Loss: 0.00002362
Iteration 176/1000 | Loss: 0.00002362
Iteration 177/1000 | Loss: 0.00002362
Iteration 178/1000 | Loss: 0.00002361
Iteration 179/1000 | Loss: 0.00002361
Iteration 180/1000 | Loss: 0.00002361
Iteration 181/1000 | Loss: 0.00002361
Iteration 182/1000 | Loss: 0.00002361
Iteration 183/1000 | Loss: 0.00002361
Iteration 184/1000 | Loss: 0.00002361
Iteration 185/1000 | Loss: 0.00002361
Iteration 186/1000 | Loss: 0.00002361
Iteration 187/1000 | Loss: 0.00002361
Iteration 188/1000 | Loss: 0.00002360
Iteration 189/1000 | Loss: 0.00002360
Iteration 190/1000 | Loss: 0.00002360
Iteration 191/1000 | Loss: 0.00002360
Iteration 192/1000 | Loss: 0.00002360
Iteration 193/1000 | Loss: 0.00002360
Iteration 194/1000 | Loss: 0.00002360
Iteration 195/1000 | Loss: 0.00002360
Iteration 196/1000 | Loss: 0.00002360
Iteration 197/1000 | Loss: 0.00002359
Iteration 198/1000 | Loss: 0.00002359
Iteration 199/1000 | Loss: 0.00002359
Iteration 200/1000 | Loss: 0.00002359
Iteration 201/1000 | Loss: 0.00002359
Iteration 202/1000 | Loss: 0.00002359
Iteration 203/1000 | Loss: 0.00002359
Iteration 204/1000 | Loss: 0.00002359
Iteration 205/1000 | Loss: 0.00002358
Iteration 206/1000 | Loss: 0.00002358
Iteration 207/1000 | Loss: 0.00002358
Iteration 208/1000 | Loss: 0.00002358
Iteration 209/1000 | Loss: 0.00002358
Iteration 210/1000 | Loss: 0.00002358
Iteration 211/1000 | Loss: 0.00002358
Iteration 212/1000 | Loss: 0.00002357
Iteration 213/1000 | Loss: 0.00002357
Iteration 214/1000 | Loss: 0.00002357
Iteration 215/1000 | Loss: 0.00002357
Iteration 216/1000 | Loss: 0.00002356
Iteration 217/1000 | Loss: 0.00002356
Iteration 218/1000 | Loss: 0.00002356
Iteration 219/1000 | Loss: 0.00002356
Iteration 220/1000 | Loss: 0.00002356
Iteration 221/1000 | Loss: 0.00002355
Iteration 222/1000 | Loss: 0.00002355
Iteration 223/1000 | Loss: 0.00002355
Iteration 224/1000 | Loss: 0.00002355
Iteration 225/1000 | Loss: 0.00002355
Iteration 226/1000 | Loss: 0.00002355
Iteration 227/1000 | Loss: 0.00002355
Iteration 228/1000 | Loss: 0.00002355
Iteration 229/1000 | Loss: 0.00002355
Iteration 230/1000 | Loss: 0.00002354
Iteration 231/1000 | Loss: 0.00002354
Iteration 232/1000 | Loss: 0.00002354
Iteration 233/1000 | Loss: 0.00002354
Iteration 234/1000 | Loss: 0.00002354
Iteration 235/1000 | Loss: 0.00002354
Iteration 236/1000 | Loss: 0.00002354
Iteration 237/1000 | Loss: 0.00002354
Iteration 238/1000 | Loss: 0.00002354
Iteration 239/1000 | Loss: 0.00002354
Iteration 240/1000 | Loss: 0.00002354
Iteration 241/1000 | Loss: 0.00002354
Iteration 242/1000 | Loss: 0.00002354
Iteration 243/1000 | Loss: 0.00002354
Iteration 244/1000 | Loss: 0.00002353
Iteration 245/1000 | Loss: 0.00002353
Iteration 246/1000 | Loss: 0.00002353
Iteration 247/1000 | Loss: 0.00002353
Iteration 248/1000 | Loss: 0.00002353
Iteration 249/1000 | Loss: 0.00002352
Iteration 250/1000 | Loss: 0.00002352
Iteration 251/1000 | Loss: 0.00002352
Iteration 252/1000 | Loss: 0.00002352
Iteration 253/1000 | Loss: 0.00002352
Iteration 254/1000 | Loss: 0.00002352
Iteration 255/1000 | Loss: 0.00002352
Iteration 256/1000 | Loss: 0.00002352
Iteration 257/1000 | Loss: 0.00002352
Iteration 258/1000 | Loss: 0.00002352
Iteration 259/1000 | Loss: 0.00002351
Iteration 260/1000 | Loss: 0.00002351
Iteration 261/1000 | Loss: 0.00002351
Iteration 262/1000 | Loss: 0.00002351
Iteration 263/1000 | Loss: 0.00002351
Iteration 264/1000 | Loss: 0.00002351
Iteration 265/1000 | Loss: 0.00002351
Iteration 266/1000 | Loss: 0.00002351
Iteration 267/1000 | Loss: 0.00002351
Iteration 268/1000 | Loss: 0.00002351
Iteration 269/1000 | Loss: 0.00002351
Iteration 270/1000 | Loss: 0.00002351
Iteration 271/1000 | Loss: 0.00002351
Iteration 272/1000 | Loss: 0.00002351
Iteration 273/1000 | Loss: 0.00002351
Iteration 274/1000 | Loss: 0.00002351
Iteration 275/1000 | Loss: 0.00002351
Iteration 276/1000 | Loss: 0.00002351
Iteration 277/1000 | Loss: 0.00002351
Iteration 278/1000 | Loss: 0.00002351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 278. Stopping optimization.
Last 5 losses: [2.351108742004726e-05, 2.351108742004726e-05, 2.351108742004726e-05, 2.351108742004726e-05, 2.351108742004726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.351108742004726e-05

Optimization complete. Final v2v error: 4.053033351898193 mm

Highest mean error: 6.02889347076416 mm for frame 93

Lowest mean error: 3.5079407691955566 mm for frame 67

Saving results

Total time: 108.61805558204651
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813853
Iteration 2/25 | Loss: 0.00133100
Iteration 3/25 | Loss: 0.00128523
Iteration 4/25 | Loss: 0.00127840
Iteration 5/25 | Loss: 0.00127670
Iteration 6/25 | Loss: 0.00127670
Iteration 7/25 | Loss: 0.00127670
Iteration 8/25 | Loss: 0.00127670
Iteration 9/25 | Loss: 0.00127670
Iteration 10/25 | Loss: 0.00127670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012766969157382846, 0.0012766969157382846, 0.0012766969157382846, 0.0012766969157382846, 0.0012766969157382846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012766969157382846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.26969242
Iteration 2/25 | Loss: 0.00088578
Iteration 3/25 | Loss: 0.00088578
Iteration 4/25 | Loss: 0.00088578
Iteration 5/25 | Loss: 0.00088578
Iteration 6/25 | Loss: 0.00088578
Iteration 7/25 | Loss: 0.00088578
Iteration 8/25 | Loss: 0.00088577
Iteration 9/25 | Loss: 0.00088577
Iteration 10/25 | Loss: 0.00088577
Iteration 11/25 | Loss: 0.00088577
Iteration 12/25 | Loss: 0.00088577
Iteration 13/25 | Loss: 0.00088577
Iteration 14/25 | Loss: 0.00088577
Iteration 15/25 | Loss: 0.00088577
Iteration 16/25 | Loss: 0.00088577
Iteration 17/25 | Loss: 0.00088577
Iteration 18/25 | Loss: 0.00088577
Iteration 19/25 | Loss: 0.00088577
Iteration 20/25 | Loss: 0.00088577
Iteration 21/25 | Loss: 0.00088577
Iteration 22/25 | Loss: 0.00088577
Iteration 23/25 | Loss: 0.00088577
Iteration 24/25 | Loss: 0.00088577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008857737411744893, 0.0008857737411744893, 0.0008857737411744893, 0.0008857737411744893, 0.0008857737411744893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008857737411744893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088577
Iteration 2/1000 | Loss: 0.00003066
Iteration 3/1000 | Loss: 0.00002230
Iteration 4/1000 | Loss: 0.00001914
Iteration 5/1000 | Loss: 0.00001836
Iteration 6/1000 | Loss: 0.00001758
Iteration 7/1000 | Loss: 0.00001705
Iteration 8/1000 | Loss: 0.00001655
Iteration 9/1000 | Loss: 0.00001630
Iteration 10/1000 | Loss: 0.00001600
Iteration 11/1000 | Loss: 0.00001579
Iteration 12/1000 | Loss: 0.00001567
Iteration 13/1000 | Loss: 0.00001550
Iteration 14/1000 | Loss: 0.00001549
Iteration 15/1000 | Loss: 0.00001530
Iteration 16/1000 | Loss: 0.00001529
Iteration 17/1000 | Loss: 0.00001526
Iteration 18/1000 | Loss: 0.00001525
Iteration 19/1000 | Loss: 0.00001524
Iteration 20/1000 | Loss: 0.00001518
Iteration 21/1000 | Loss: 0.00001516
Iteration 22/1000 | Loss: 0.00001515
Iteration 23/1000 | Loss: 0.00001514
Iteration 24/1000 | Loss: 0.00001514
Iteration 25/1000 | Loss: 0.00001512
Iteration 26/1000 | Loss: 0.00001509
Iteration 27/1000 | Loss: 0.00001509
Iteration 28/1000 | Loss: 0.00001508
Iteration 29/1000 | Loss: 0.00001507
Iteration 30/1000 | Loss: 0.00001506
Iteration 31/1000 | Loss: 0.00001506
Iteration 32/1000 | Loss: 0.00001504
Iteration 33/1000 | Loss: 0.00001503
Iteration 34/1000 | Loss: 0.00001502
Iteration 35/1000 | Loss: 0.00001502
Iteration 36/1000 | Loss: 0.00001501
Iteration 37/1000 | Loss: 0.00001501
Iteration 38/1000 | Loss: 0.00001500
Iteration 39/1000 | Loss: 0.00001499
Iteration 40/1000 | Loss: 0.00001495
Iteration 41/1000 | Loss: 0.00001495
Iteration 42/1000 | Loss: 0.00001492
Iteration 43/1000 | Loss: 0.00001488
Iteration 44/1000 | Loss: 0.00001487
Iteration 45/1000 | Loss: 0.00001487
Iteration 46/1000 | Loss: 0.00001486
Iteration 47/1000 | Loss: 0.00001485
Iteration 48/1000 | Loss: 0.00001483
Iteration 49/1000 | Loss: 0.00001482
Iteration 50/1000 | Loss: 0.00001480
Iteration 51/1000 | Loss: 0.00001477
Iteration 52/1000 | Loss: 0.00001477
Iteration 53/1000 | Loss: 0.00001474
Iteration 54/1000 | Loss: 0.00001474
Iteration 55/1000 | Loss: 0.00001473
Iteration 56/1000 | Loss: 0.00001473
Iteration 57/1000 | Loss: 0.00001473
Iteration 58/1000 | Loss: 0.00001471
Iteration 59/1000 | Loss: 0.00001471
Iteration 60/1000 | Loss: 0.00001470
Iteration 61/1000 | Loss: 0.00001469
Iteration 62/1000 | Loss: 0.00001469
Iteration 63/1000 | Loss: 0.00001469
Iteration 64/1000 | Loss: 0.00001469
Iteration 65/1000 | Loss: 0.00001469
Iteration 66/1000 | Loss: 0.00001469
Iteration 67/1000 | Loss: 0.00001469
Iteration 68/1000 | Loss: 0.00001469
Iteration 69/1000 | Loss: 0.00001469
Iteration 70/1000 | Loss: 0.00001469
Iteration 71/1000 | Loss: 0.00001469
Iteration 72/1000 | Loss: 0.00001468
Iteration 73/1000 | Loss: 0.00001468
Iteration 74/1000 | Loss: 0.00001468
Iteration 75/1000 | Loss: 0.00001468
Iteration 76/1000 | Loss: 0.00001468
Iteration 77/1000 | Loss: 0.00001468
Iteration 78/1000 | Loss: 0.00001468
Iteration 79/1000 | Loss: 0.00001468
Iteration 80/1000 | Loss: 0.00001468
Iteration 81/1000 | Loss: 0.00001468
Iteration 82/1000 | Loss: 0.00001468
Iteration 83/1000 | Loss: 0.00001467
Iteration 84/1000 | Loss: 0.00001467
Iteration 85/1000 | Loss: 0.00001467
Iteration 86/1000 | Loss: 0.00001466
Iteration 87/1000 | Loss: 0.00001466
Iteration 88/1000 | Loss: 0.00001466
Iteration 89/1000 | Loss: 0.00001466
Iteration 90/1000 | Loss: 0.00001466
Iteration 91/1000 | Loss: 0.00001466
Iteration 92/1000 | Loss: 0.00001466
Iteration 93/1000 | Loss: 0.00001465
Iteration 94/1000 | Loss: 0.00001465
Iteration 95/1000 | Loss: 0.00001465
Iteration 96/1000 | Loss: 0.00001465
Iteration 97/1000 | Loss: 0.00001465
Iteration 98/1000 | Loss: 0.00001465
Iteration 99/1000 | Loss: 0.00001465
Iteration 100/1000 | Loss: 0.00001465
Iteration 101/1000 | Loss: 0.00001464
Iteration 102/1000 | Loss: 0.00001464
Iteration 103/1000 | Loss: 0.00001464
Iteration 104/1000 | Loss: 0.00001464
Iteration 105/1000 | Loss: 0.00001464
Iteration 106/1000 | Loss: 0.00001464
Iteration 107/1000 | Loss: 0.00001464
Iteration 108/1000 | Loss: 0.00001464
Iteration 109/1000 | Loss: 0.00001464
Iteration 110/1000 | Loss: 0.00001464
Iteration 111/1000 | Loss: 0.00001464
Iteration 112/1000 | Loss: 0.00001464
Iteration 113/1000 | Loss: 0.00001464
Iteration 114/1000 | Loss: 0.00001464
Iteration 115/1000 | Loss: 0.00001464
Iteration 116/1000 | Loss: 0.00001464
Iteration 117/1000 | Loss: 0.00001463
Iteration 118/1000 | Loss: 0.00001463
Iteration 119/1000 | Loss: 0.00001463
Iteration 120/1000 | Loss: 0.00001463
Iteration 121/1000 | Loss: 0.00001463
Iteration 122/1000 | Loss: 0.00001463
Iteration 123/1000 | Loss: 0.00001463
Iteration 124/1000 | Loss: 0.00001462
Iteration 125/1000 | Loss: 0.00001462
Iteration 126/1000 | Loss: 0.00001462
Iteration 127/1000 | Loss: 0.00001462
Iteration 128/1000 | Loss: 0.00001462
Iteration 129/1000 | Loss: 0.00001462
Iteration 130/1000 | Loss: 0.00001462
Iteration 131/1000 | Loss: 0.00001462
Iteration 132/1000 | Loss: 0.00001462
Iteration 133/1000 | Loss: 0.00001462
Iteration 134/1000 | Loss: 0.00001461
Iteration 135/1000 | Loss: 0.00001461
Iteration 136/1000 | Loss: 0.00001461
Iteration 137/1000 | Loss: 0.00001461
Iteration 138/1000 | Loss: 0.00001461
Iteration 139/1000 | Loss: 0.00001461
Iteration 140/1000 | Loss: 0.00001461
Iteration 141/1000 | Loss: 0.00001461
Iteration 142/1000 | Loss: 0.00001461
Iteration 143/1000 | Loss: 0.00001461
Iteration 144/1000 | Loss: 0.00001460
Iteration 145/1000 | Loss: 0.00001460
Iteration 146/1000 | Loss: 0.00001460
Iteration 147/1000 | Loss: 0.00001460
Iteration 148/1000 | Loss: 0.00001460
Iteration 149/1000 | Loss: 0.00001460
Iteration 150/1000 | Loss: 0.00001460
Iteration 151/1000 | Loss: 0.00001460
Iteration 152/1000 | Loss: 0.00001460
Iteration 153/1000 | Loss: 0.00001460
Iteration 154/1000 | Loss: 0.00001460
Iteration 155/1000 | Loss: 0.00001460
Iteration 156/1000 | Loss: 0.00001460
Iteration 157/1000 | Loss: 0.00001460
Iteration 158/1000 | Loss: 0.00001460
Iteration 159/1000 | Loss: 0.00001460
Iteration 160/1000 | Loss: 0.00001460
Iteration 161/1000 | Loss: 0.00001460
Iteration 162/1000 | Loss: 0.00001460
Iteration 163/1000 | Loss: 0.00001460
Iteration 164/1000 | Loss: 0.00001460
Iteration 165/1000 | Loss: 0.00001460
Iteration 166/1000 | Loss: 0.00001460
Iteration 167/1000 | Loss: 0.00001460
Iteration 168/1000 | Loss: 0.00001460
Iteration 169/1000 | Loss: 0.00001460
Iteration 170/1000 | Loss: 0.00001460
Iteration 171/1000 | Loss: 0.00001460
Iteration 172/1000 | Loss: 0.00001460
Iteration 173/1000 | Loss: 0.00001460
Iteration 174/1000 | Loss: 0.00001460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.4599433598050382e-05, 1.4599433598050382e-05, 1.4599433598050382e-05, 1.4599433598050382e-05, 1.4599433598050382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4599433598050382e-05

Optimization complete. Final v2v error: 3.253225803375244 mm

Highest mean error: 3.583791494369507 mm for frame 76

Lowest mean error: 2.9259049892425537 mm for frame 32

Saving results

Total time: 39.98335814476013
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490212
Iteration 2/25 | Loss: 0.00143965
Iteration 3/25 | Loss: 0.00136797
Iteration 4/25 | Loss: 0.00135666
Iteration 5/25 | Loss: 0.00135231
Iteration 6/25 | Loss: 0.00135231
Iteration 7/25 | Loss: 0.00135231
Iteration 8/25 | Loss: 0.00135231
Iteration 9/25 | Loss: 0.00135231
Iteration 10/25 | Loss: 0.00135231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013523082016035914, 0.0013523082016035914, 0.0013523082016035914, 0.0013523082016035914, 0.0013523082016035914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013523082016035914

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46376574
Iteration 2/25 | Loss: 0.00089447
Iteration 3/25 | Loss: 0.00089446
Iteration 4/25 | Loss: 0.00089446
Iteration 5/25 | Loss: 0.00089446
Iteration 6/25 | Loss: 0.00089446
Iteration 7/25 | Loss: 0.00089446
Iteration 8/25 | Loss: 0.00089445
Iteration 9/25 | Loss: 0.00089445
Iteration 10/25 | Loss: 0.00089445
Iteration 11/25 | Loss: 0.00089445
Iteration 12/25 | Loss: 0.00089445
Iteration 13/25 | Loss: 0.00089445
Iteration 14/25 | Loss: 0.00089445
Iteration 15/25 | Loss: 0.00089445
Iteration 16/25 | Loss: 0.00089445
Iteration 17/25 | Loss: 0.00089445
Iteration 18/25 | Loss: 0.00089445
Iteration 19/25 | Loss: 0.00089445
Iteration 20/25 | Loss: 0.00089445
Iteration 21/25 | Loss: 0.00089445
Iteration 22/25 | Loss: 0.00089445
Iteration 23/25 | Loss: 0.00089445
Iteration 24/25 | Loss: 0.00089445
Iteration 25/25 | Loss: 0.00089445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089445
Iteration 2/1000 | Loss: 0.00004163
Iteration 3/1000 | Loss: 0.00002292
Iteration 4/1000 | Loss: 0.00002088
Iteration 5/1000 | Loss: 0.00001970
Iteration 6/1000 | Loss: 0.00001902
Iteration 7/1000 | Loss: 0.00001851
Iteration 8/1000 | Loss: 0.00001828
Iteration 9/1000 | Loss: 0.00001795
Iteration 10/1000 | Loss: 0.00001763
Iteration 11/1000 | Loss: 0.00001754
Iteration 12/1000 | Loss: 0.00001741
Iteration 13/1000 | Loss: 0.00001732
Iteration 14/1000 | Loss: 0.00001713
Iteration 15/1000 | Loss: 0.00001712
Iteration 16/1000 | Loss: 0.00001707
Iteration 17/1000 | Loss: 0.00001706
Iteration 18/1000 | Loss: 0.00001705
Iteration 19/1000 | Loss: 0.00001704
Iteration 20/1000 | Loss: 0.00001704
Iteration 21/1000 | Loss: 0.00001700
Iteration 22/1000 | Loss: 0.00001695
Iteration 23/1000 | Loss: 0.00001695
Iteration 24/1000 | Loss: 0.00001693
Iteration 25/1000 | Loss: 0.00001692
Iteration 26/1000 | Loss: 0.00001692
Iteration 27/1000 | Loss: 0.00001691
Iteration 28/1000 | Loss: 0.00001690
Iteration 29/1000 | Loss: 0.00001685
Iteration 30/1000 | Loss: 0.00001685
Iteration 31/1000 | Loss: 0.00001680
Iteration 32/1000 | Loss: 0.00001680
Iteration 33/1000 | Loss: 0.00001671
Iteration 34/1000 | Loss: 0.00001671
Iteration 35/1000 | Loss: 0.00001670
Iteration 36/1000 | Loss: 0.00001669
Iteration 37/1000 | Loss: 0.00001665
Iteration 38/1000 | Loss: 0.00001665
Iteration 39/1000 | Loss: 0.00001663
Iteration 40/1000 | Loss: 0.00001663
Iteration 41/1000 | Loss: 0.00001661
Iteration 42/1000 | Loss: 0.00001660
Iteration 43/1000 | Loss: 0.00001658
Iteration 44/1000 | Loss: 0.00001657
Iteration 45/1000 | Loss: 0.00001656
Iteration 46/1000 | Loss: 0.00001648
Iteration 47/1000 | Loss: 0.00001647
Iteration 48/1000 | Loss: 0.00001646
Iteration 49/1000 | Loss: 0.00001646
Iteration 50/1000 | Loss: 0.00001645
Iteration 51/1000 | Loss: 0.00001644
Iteration 52/1000 | Loss: 0.00001643
Iteration 53/1000 | Loss: 0.00001643
Iteration 54/1000 | Loss: 0.00001642
Iteration 55/1000 | Loss: 0.00001642
Iteration 56/1000 | Loss: 0.00001642
Iteration 57/1000 | Loss: 0.00001642
Iteration 58/1000 | Loss: 0.00001641
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001641
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001641
Iteration 63/1000 | Loss: 0.00001641
Iteration 64/1000 | Loss: 0.00001641
Iteration 65/1000 | Loss: 0.00001640
Iteration 66/1000 | Loss: 0.00001640
Iteration 67/1000 | Loss: 0.00001640
Iteration 68/1000 | Loss: 0.00001640
Iteration 69/1000 | Loss: 0.00001640
Iteration 70/1000 | Loss: 0.00001640
Iteration 71/1000 | Loss: 0.00001640
Iteration 72/1000 | Loss: 0.00001639
Iteration 73/1000 | Loss: 0.00001639
Iteration 74/1000 | Loss: 0.00001639
Iteration 75/1000 | Loss: 0.00001639
Iteration 76/1000 | Loss: 0.00001639
Iteration 77/1000 | Loss: 0.00001639
Iteration 78/1000 | Loss: 0.00001639
Iteration 79/1000 | Loss: 0.00001638
Iteration 80/1000 | Loss: 0.00001638
Iteration 81/1000 | Loss: 0.00001638
Iteration 82/1000 | Loss: 0.00001637
Iteration 83/1000 | Loss: 0.00001637
Iteration 84/1000 | Loss: 0.00001636
Iteration 85/1000 | Loss: 0.00001636
Iteration 86/1000 | Loss: 0.00001636
Iteration 87/1000 | Loss: 0.00001635
Iteration 88/1000 | Loss: 0.00001635
Iteration 89/1000 | Loss: 0.00001635
Iteration 90/1000 | Loss: 0.00001634
Iteration 91/1000 | Loss: 0.00001634
Iteration 92/1000 | Loss: 0.00001634
Iteration 93/1000 | Loss: 0.00001634
Iteration 94/1000 | Loss: 0.00001634
Iteration 95/1000 | Loss: 0.00001633
Iteration 96/1000 | Loss: 0.00001633
Iteration 97/1000 | Loss: 0.00001633
Iteration 98/1000 | Loss: 0.00001632
Iteration 99/1000 | Loss: 0.00001632
Iteration 100/1000 | Loss: 0.00001631
Iteration 101/1000 | Loss: 0.00001631
Iteration 102/1000 | Loss: 0.00001631
Iteration 103/1000 | Loss: 0.00001630
Iteration 104/1000 | Loss: 0.00001630
Iteration 105/1000 | Loss: 0.00001630
Iteration 106/1000 | Loss: 0.00001630
Iteration 107/1000 | Loss: 0.00001630
Iteration 108/1000 | Loss: 0.00001630
Iteration 109/1000 | Loss: 0.00001629
Iteration 110/1000 | Loss: 0.00001629
Iteration 111/1000 | Loss: 0.00001629
Iteration 112/1000 | Loss: 0.00001629
Iteration 113/1000 | Loss: 0.00001628
Iteration 114/1000 | Loss: 0.00001628
Iteration 115/1000 | Loss: 0.00001628
Iteration 116/1000 | Loss: 0.00001628
Iteration 117/1000 | Loss: 0.00001628
Iteration 118/1000 | Loss: 0.00001628
Iteration 119/1000 | Loss: 0.00001628
Iteration 120/1000 | Loss: 0.00001628
Iteration 121/1000 | Loss: 0.00001627
Iteration 122/1000 | Loss: 0.00001627
Iteration 123/1000 | Loss: 0.00001627
Iteration 124/1000 | Loss: 0.00001627
Iteration 125/1000 | Loss: 0.00001627
Iteration 126/1000 | Loss: 0.00001627
Iteration 127/1000 | Loss: 0.00001627
Iteration 128/1000 | Loss: 0.00001626
Iteration 129/1000 | Loss: 0.00001626
Iteration 130/1000 | Loss: 0.00001626
Iteration 131/1000 | Loss: 0.00001626
Iteration 132/1000 | Loss: 0.00001626
Iteration 133/1000 | Loss: 0.00001625
Iteration 134/1000 | Loss: 0.00001625
Iteration 135/1000 | Loss: 0.00001625
Iteration 136/1000 | Loss: 0.00001625
Iteration 137/1000 | Loss: 0.00001625
Iteration 138/1000 | Loss: 0.00001625
Iteration 139/1000 | Loss: 0.00001625
Iteration 140/1000 | Loss: 0.00001625
Iteration 141/1000 | Loss: 0.00001625
Iteration 142/1000 | Loss: 0.00001625
Iteration 143/1000 | Loss: 0.00001625
Iteration 144/1000 | Loss: 0.00001625
Iteration 145/1000 | Loss: 0.00001624
Iteration 146/1000 | Loss: 0.00001624
Iteration 147/1000 | Loss: 0.00001624
Iteration 148/1000 | Loss: 0.00001624
Iteration 149/1000 | Loss: 0.00001624
Iteration 150/1000 | Loss: 0.00001624
Iteration 151/1000 | Loss: 0.00001624
Iteration 152/1000 | Loss: 0.00001624
Iteration 153/1000 | Loss: 0.00001624
Iteration 154/1000 | Loss: 0.00001624
Iteration 155/1000 | Loss: 0.00001624
Iteration 156/1000 | Loss: 0.00001624
Iteration 157/1000 | Loss: 0.00001624
Iteration 158/1000 | Loss: 0.00001624
Iteration 159/1000 | Loss: 0.00001624
Iteration 160/1000 | Loss: 0.00001624
Iteration 161/1000 | Loss: 0.00001624
Iteration 162/1000 | Loss: 0.00001623
Iteration 163/1000 | Loss: 0.00001623
Iteration 164/1000 | Loss: 0.00001623
Iteration 165/1000 | Loss: 0.00001623
Iteration 166/1000 | Loss: 0.00001623
Iteration 167/1000 | Loss: 0.00001623
Iteration 168/1000 | Loss: 0.00001623
Iteration 169/1000 | Loss: 0.00001623
Iteration 170/1000 | Loss: 0.00001623
Iteration 171/1000 | Loss: 0.00001623
Iteration 172/1000 | Loss: 0.00001623
Iteration 173/1000 | Loss: 0.00001623
Iteration 174/1000 | Loss: 0.00001623
Iteration 175/1000 | Loss: 0.00001623
Iteration 176/1000 | Loss: 0.00001623
Iteration 177/1000 | Loss: 0.00001623
Iteration 178/1000 | Loss: 0.00001623
Iteration 179/1000 | Loss: 0.00001622
Iteration 180/1000 | Loss: 0.00001622
Iteration 181/1000 | Loss: 0.00001622
Iteration 182/1000 | Loss: 0.00001622
Iteration 183/1000 | Loss: 0.00001622
Iteration 184/1000 | Loss: 0.00001622
Iteration 185/1000 | Loss: 0.00001622
Iteration 186/1000 | Loss: 0.00001622
Iteration 187/1000 | Loss: 0.00001622
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.622332820261363e-05, 1.622332820261363e-05, 1.622332820261363e-05, 1.622332820261363e-05, 1.622332820261363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.622332820261363e-05

Optimization complete. Final v2v error: 3.3999438285827637 mm

Highest mean error: 3.8373591899871826 mm for frame 105

Lowest mean error: 3.200925827026367 mm for frame 65

Saving results

Total time: 48.73918795585632
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960820
Iteration 2/25 | Loss: 0.00960820
Iteration 3/25 | Loss: 0.00960820
Iteration 4/25 | Loss: 0.00960820
Iteration 5/25 | Loss: 0.00960819
Iteration 6/25 | Loss: 0.00960819
Iteration 7/25 | Loss: 0.00960819
Iteration 8/25 | Loss: 0.00960819
Iteration 9/25 | Loss: 0.00960819
Iteration 10/25 | Loss: 0.00960819
Iteration 11/25 | Loss: 0.00960819
Iteration 12/25 | Loss: 0.00960819
Iteration 13/25 | Loss: 0.00960819
Iteration 14/25 | Loss: 0.00960819
Iteration 15/25 | Loss: 0.00960818
Iteration 16/25 | Loss: 0.00960818
Iteration 17/25 | Loss: 0.00960818
Iteration 18/25 | Loss: 0.00960818
Iteration 19/25 | Loss: 0.00960818
Iteration 20/25 | Loss: 0.00960818
Iteration 21/25 | Loss: 0.00960818
Iteration 22/25 | Loss: 0.00960818
Iteration 23/25 | Loss: 0.00960818
Iteration 24/25 | Loss: 0.00960818
Iteration 25/25 | Loss: 0.00960817

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55674136
Iteration 2/25 | Loss: 0.18963706
Iteration 3/25 | Loss: 0.18960927
Iteration 4/25 | Loss: 0.18960926
Iteration 5/25 | Loss: 0.18960926
Iteration 6/25 | Loss: 0.18960921
Iteration 7/25 | Loss: 0.18960921
Iteration 8/25 | Loss: 0.18960921
Iteration 9/25 | Loss: 0.18960921
Iteration 10/25 | Loss: 0.18960921
Iteration 11/25 | Loss: 0.18960921
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.18960921466350555, 0.18960921466350555, 0.18960921466350555, 0.18960921466350555, 0.18960921466350555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18960921466350555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18960921
Iteration 2/1000 | Loss: 0.01294262
Iteration 3/1000 | Loss: 0.00541435
Iteration 4/1000 | Loss: 0.00222717
Iteration 5/1000 | Loss: 0.00119299
Iteration 6/1000 | Loss: 0.00070147
Iteration 7/1000 | Loss: 0.00077566
Iteration 8/1000 | Loss: 0.00103437
Iteration 9/1000 | Loss: 0.00265915
Iteration 10/1000 | Loss: 0.00022070
Iteration 11/1000 | Loss: 0.00097290
Iteration 12/1000 | Loss: 0.00081410
Iteration 13/1000 | Loss: 0.00169774
Iteration 14/1000 | Loss: 0.00010555
Iteration 15/1000 | Loss: 0.00032307
Iteration 16/1000 | Loss: 0.00033635
Iteration 17/1000 | Loss: 0.00185238
Iteration 18/1000 | Loss: 0.00234748
Iteration 19/1000 | Loss: 0.00055233
Iteration 20/1000 | Loss: 0.00018381
Iteration 21/1000 | Loss: 0.00024718
Iteration 22/1000 | Loss: 0.00037848
Iteration 23/1000 | Loss: 0.00076016
Iteration 24/1000 | Loss: 0.00004821
Iteration 25/1000 | Loss: 0.00113334
Iteration 26/1000 | Loss: 0.00011932
Iteration 27/1000 | Loss: 0.00079374
Iteration 28/1000 | Loss: 0.00014848
Iteration 29/1000 | Loss: 0.00003623
Iteration 30/1000 | Loss: 0.00008523
Iteration 31/1000 | Loss: 0.00034781
Iteration 32/1000 | Loss: 0.00003110
Iteration 33/1000 | Loss: 0.00005657
Iteration 34/1000 | Loss: 0.00002857
Iteration 35/1000 | Loss: 0.00036166
Iteration 36/1000 | Loss: 0.00002724
Iteration 37/1000 | Loss: 0.00008209
Iteration 38/1000 | Loss: 0.00048651
Iteration 39/1000 | Loss: 0.00006086
Iteration 40/1000 | Loss: 0.00014045
Iteration 41/1000 | Loss: 0.00002578
Iteration 42/1000 | Loss: 0.00005249
Iteration 43/1000 | Loss: 0.00002536
Iteration 44/1000 | Loss: 0.00009440
Iteration 45/1000 | Loss: 0.00002507
Iteration 46/1000 | Loss: 0.00002457
Iteration 47/1000 | Loss: 0.00007052
Iteration 48/1000 | Loss: 0.00022086
Iteration 49/1000 | Loss: 0.00003631
Iteration 50/1000 | Loss: 0.00002568
Iteration 51/1000 | Loss: 0.00002387
Iteration 52/1000 | Loss: 0.00002386
Iteration 53/1000 | Loss: 0.00002365
Iteration 54/1000 | Loss: 0.00014592
Iteration 55/1000 | Loss: 0.00002364
Iteration 56/1000 | Loss: 0.00002350
Iteration 57/1000 | Loss: 0.00002347
Iteration 58/1000 | Loss: 0.00002347
Iteration 59/1000 | Loss: 0.00002347
Iteration 60/1000 | Loss: 0.00002346
Iteration 61/1000 | Loss: 0.00002346
Iteration 62/1000 | Loss: 0.00002345
Iteration 63/1000 | Loss: 0.00002345
Iteration 64/1000 | Loss: 0.00002345
Iteration 65/1000 | Loss: 0.00002345
Iteration 66/1000 | Loss: 0.00002344
Iteration 67/1000 | Loss: 0.00002344
Iteration 68/1000 | Loss: 0.00002344
Iteration 69/1000 | Loss: 0.00002344
Iteration 70/1000 | Loss: 0.00002344
Iteration 71/1000 | Loss: 0.00002343
Iteration 72/1000 | Loss: 0.00002343
Iteration 73/1000 | Loss: 0.00002343
Iteration 74/1000 | Loss: 0.00002343
Iteration 75/1000 | Loss: 0.00002343
Iteration 76/1000 | Loss: 0.00002343
Iteration 77/1000 | Loss: 0.00002342
Iteration 78/1000 | Loss: 0.00002342
Iteration 79/1000 | Loss: 0.00002342
Iteration 80/1000 | Loss: 0.00002342
Iteration 81/1000 | Loss: 0.00002342
Iteration 82/1000 | Loss: 0.00002342
Iteration 83/1000 | Loss: 0.00002342
Iteration 84/1000 | Loss: 0.00002342
Iteration 85/1000 | Loss: 0.00002342
Iteration 86/1000 | Loss: 0.00002341
Iteration 87/1000 | Loss: 0.00002341
Iteration 88/1000 | Loss: 0.00002341
Iteration 89/1000 | Loss: 0.00002341
Iteration 90/1000 | Loss: 0.00002340
Iteration 91/1000 | Loss: 0.00002340
Iteration 92/1000 | Loss: 0.00002340
Iteration 93/1000 | Loss: 0.00002340
Iteration 94/1000 | Loss: 0.00002339
Iteration 95/1000 | Loss: 0.00002339
Iteration 96/1000 | Loss: 0.00002339
Iteration 97/1000 | Loss: 0.00002339
Iteration 98/1000 | Loss: 0.00002338
Iteration 99/1000 | Loss: 0.00002338
Iteration 100/1000 | Loss: 0.00002338
Iteration 101/1000 | Loss: 0.00002338
Iteration 102/1000 | Loss: 0.00002337
Iteration 103/1000 | Loss: 0.00002337
Iteration 104/1000 | Loss: 0.00002337
Iteration 105/1000 | Loss: 0.00002337
Iteration 106/1000 | Loss: 0.00002337
Iteration 107/1000 | Loss: 0.00002336
Iteration 108/1000 | Loss: 0.00002336
Iteration 109/1000 | Loss: 0.00002336
Iteration 110/1000 | Loss: 0.00002336
Iteration 111/1000 | Loss: 0.00002336
Iteration 112/1000 | Loss: 0.00002336
Iteration 113/1000 | Loss: 0.00002336
Iteration 114/1000 | Loss: 0.00002336
Iteration 115/1000 | Loss: 0.00002336
Iteration 116/1000 | Loss: 0.00002336
Iteration 117/1000 | Loss: 0.00002336
Iteration 118/1000 | Loss: 0.00002336
Iteration 119/1000 | Loss: 0.00002336
Iteration 120/1000 | Loss: 0.00002336
Iteration 121/1000 | Loss: 0.00002336
Iteration 122/1000 | Loss: 0.00002336
Iteration 123/1000 | Loss: 0.00002336
Iteration 124/1000 | Loss: 0.00002336
Iteration 125/1000 | Loss: 0.00002336
Iteration 126/1000 | Loss: 0.00002335
Iteration 127/1000 | Loss: 0.00002335
Iteration 128/1000 | Loss: 0.00002335
Iteration 129/1000 | Loss: 0.00002335
Iteration 130/1000 | Loss: 0.00002335
Iteration 131/1000 | Loss: 0.00002335
Iteration 132/1000 | Loss: 0.00009024
Iteration 133/1000 | Loss: 0.00004380
Iteration 134/1000 | Loss: 0.00002615
Iteration 135/1000 | Loss: 0.00002338
Iteration 136/1000 | Loss: 0.00002335
Iteration 137/1000 | Loss: 0.00002335
Iteration 138/1000 | Loss: 0.00002335
Iteration 139/1000 | Loss: 0.00002335
Iteration 140/1000 | Loss: 0.00002335
Iteration 141/1000 | Loss: 0.00002335
Iteration 142/1000 | Loss: 0.00002335
Iteration 143/1000 | Loss: 0.00002335
Iteration 144/1000 | Loss: 0.00002335
Iteration 145/1000 | Loss: 0.00002335
Iteration 146/1000 | Loss: 0.00002334
Iteration 147/1000 | Loss: 0.00002334
Iteration 148/1000 | Loss: 0.00002555
Iteration 149/1000 | Loss: 0.00002330
Iteration 150/1000 | Loss: 0.00002330
Iteration 151/1000 | Loss: 0.00002330
Iteration 152/1000 | Loss: 0.00002330
Iteration 153/1000 | Loss: 0.00002329
Iteration 154/1000 | Loss: 0.00002329
Iteration 155/1000 | Loss: 0.00002329
Iteration 156/1000 | Loss: 0.00002329
Iteration 157/1000 | Loss: 0.00002329
Iteration 158/1000 | Loss: 0.00002329
Iteration 159/1000 | Loss: 0.00002329
Iteration 160/1000 | Loss: 0.00002329
Iteration 161/1000 | Loss: 0.00002329
Iteration 162/1000 | Loss: 0.00002329
Iteration 163/1000 | Loss: 0.00002329
Iteration 164/1000 | Loss: 0.00002329
Iteration 165/1000 | Loss: 0.00002329
Iteration 166/1000 | Loss: 0.00002329
Iteration 167/1000 | Loss: 0.00002329
Iteration 168/1000 | Loss: 0.00002328
Iteration 169/1000 | Loss: 0.00002328
Iteration 170/1000 | Loss: 0.00002328
Iteration 171/1000 | Loss: 0.00002328
Iteration 172/1000 | Loss: 0.00002328
Iteration 173/1000 | Loss: 0.00002328
Iteration 174/1000 | Loss: 0.00002328
Iteration 175/1000 | Loss: 0.00002328
Iteration 176/1000 | Loss: 0.00002328
Iteration 177/1000 | Loss: 0.00002328
Iteration 178/1000 | Loss: 0.00002328
Iteration 179/1000 | Loss: 0.00002327
Iteration 180/1000 | Loss: 0.00002327
Iteration 181/1000 | Loss: 0.00002327
Iteration 182/1000 | Loss: 0.00002327
Iteration 183/1000 | Loss: 0.00002327
Iteration 184/1000 | Loss: 0.00002326
Iteration 185/1000 | Loss: 0.00002326
Iteration 186/1000 | Loss: 0.00002326
Iteration 187/1000 | Loss: 0.00002325
Iteration 188/1000 | Loss: 0.00002324
Iteration 189/1000 | Loss: 0.00002324
Iteration 190/1000 | Loss: 0.00002324
Iteration 191/1000 | Loss: 0.00002324
Iteration 192/1000 | Loss: 0.00002324
Iteration 193/1000 | Loss: 0.00002324
Iteration 194/1000 | Loss: 0.00002324
Iteration 195/1000 | Loss: 0.00002324
Iteration 196/1000 | Loss: 0.00002324
Iteration 197/1000 | Loss: 0.00002324
Iteration 198/1000 | Loss: 0.00002324
Iteration 199/1000 | Loss: 0.00002324
Iteration 200/1000 | Loss: 0.00002323
Iteration 201/1000 | Loss: 0.00002323
Iteration 202/1000 | Loss: 0.00002323
Iteration 203/1000 | Loss: 0.00002323
Iteration 204/1000 | Loss: 0.00002323
Iteration 205/1000 | Loss: 0.00002323
Iteration 206/1000 | Loss: 0.00002323
Iteration 207/1000 | Loss: 0.00002323
Iteration 208/1000 | Loss: 0.00002323
Iteration 209/1000 | Loss: 0.00002323
Iteration 210/1000 | Loss: 0.00002323
Iteration 211/1000 | Loss: 0.00002323
Iteration 212/1000 | Loss: 0.00002323
Iteration 213/1000 | Loss: 0.00002323
Iteration 214/1000 | Loss: 0.00002323
Iteration 215/1000 | Loss: 0.00002323
Iteration 216/1000 | Loss: 0.00002323
Iteration 217/1000 | Loss: 0.00002323
Iteration 218/1000 | Loss: 0.00002323
Iteration 219/1000 | Loss: 0.00002323
Iteration 220/1000 | Loss: 0.00002323
Iteration 221/1000 | Loss: 0.00002322
Iteration 222/1000 | Loss: 0.00002322
Iteration 223/1000 | Loss: 0.00002322
Iteration 224/1000 | Loss: 0.00002322
Iteration 225/1000 | Loss: 0.00002322
Iteration 226/1000 | Loss: 0.00002322
Iteration 227/1000 | Loss: 0.00002322
Iteration 228/1000 | Loss: 0.00002322
Iteration 229/1000 | Loss: 0.00002322
Iteration 230/1000 | Loss: 0.00002322
Iteration 231/1000 | Loss: 0.00002322
Iteration 232/1000 | Loss: 0.00002322
Iteration 233/1000 | Loss: 0.00002322
Iteration 234/1000 | Loss: 0.00002322
Iteration 235/1000 | Loss: 0.00002322
Iteration 236/1000 | Loss: 0.00002321
Iteration 237/1000 | Loss: 0.00002321
Iteration 238/1000 | Loss: 0.00002321
Iteration 239/1000 | Loss: 0.00002321
Iteration 240/1000 | Loss: 0.00002321
Iteration 241/1000 | Loss: 0.00002321
Iteration 242/1000 | Loss: 0.00002321
Iteration 243/1000 | Loss: 0.00002321
Iteration 244/1000 | Loss: 0.00002321
Iteration 245/1000 | Loss: 0.00002321
Iteration 246/1000 | Loss: 0.00002321
Iteration 247/1000 | Loss: 0.00002321
Iteration 248/1000 | Loss: 0.00002321
Iteration 249/1000 | Loss: 0.00002321
Iteration 250/1000 | Loss: 0.00002321
Iteration 251/1000 | Loss: 0.00002320
Iteration 252/1000 | Loss: 0.00002320
Iteration 253/1000 | Loss: 0.00002320
Iteration 254/1000 | Loss: 0.00002320
Iteration 255/1000 | Loss: 0.00002320
Iteration 256/1000 | Loss: 0.00002320
Iteration 257/1000 | Loss: 0.00002320
Iteration 258/1000 | Loss: 0.00002320
Iteration 259/1000 | Loss: 0.00002320
Iteration 260/1000 | Loss: 0.00002320
Iteration 261/1000 | Loss: 0.00002320
Iteration 262/1000 | Loss: 0.00002320
Iteration 263/1000 | Loss: 0.00002320
Iteration 264/1000 | Loss: 0.00002320
Iteration 265/1000 | Loss: 0.00002320
Iteration 266/1000 | Loss: 0.00002320
Iteration 267/1000 | Loss: 0.00002320
Iteration 268/1000 | Loss: 0.00002320
Iteration 269/1000 | Loss: 0.00002320
Iteration 270/1000 | Loss: 0.00002320
Iteration 271/1000 | Loss: 0.00002320
Iteration 272/1000 | Loss: 0.00002320
Iteration 273/1000 | Loss: 0.00002320
Iteration 274/1000 | Loss: 0.00002320
Iteration 275/1000 | Loss: 0.00002320
Iteration 276/1000 | Loss: 0.00002320
Iteration 277/1000 | Loss: 0.00002320
Iteration 278/1000 | Loss: 0.00002320
Iteration 279/1000 | Loss: 0.00002320
Iteration 280/1000 | Loss: 0.00002320
Iteration 281/1000 | Loss: 0.00002320
Iteration 282/1000 | Loss: 0.00002320
Iteration 283/1000 | Loss: 0.00002320
Iteration 284/1000 | Loss: 0.00002320
Iteration 285/1000 | Loss: 0.00002320
Iteration 286/1000 | Loss: 0.00002320
Iteration 287/1000 | Loss: 0.00002320
Iteration 288/1000 | Loss: 0.00002320
Iteration 289/1000 | Loss: 0.00002320
Iteration 290/1000 | Loss: 0.00002320
Iteration 291/1000 | Loss: 0.00002320
Iteration 292/1000 | Loss: 0.00002320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [2.319626037206035e-05, 2.319626037206035e-05, 2.319626037206035e-05, 2.319626037206035e-05, 2.319626037206035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.319626037206035e-05

Optimization complete. Final v2v error: 4.141013145446777 mm

Highest mean error: 4.309073448181152 mm for frame 161

Lowest mean error: 3.900285243988037 mm for frame 17

Saving results

Total time: 113.23907709121704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486742
Iteration 2/25 | Loss: 0.00163112
Iteration 3/25 | Loss: 0.00149449
Iteration 4/25 | Loss: 0.00147569
Iteration 5/25 | Loss: 0.00146980
Iteration 6/25 | Loss: 0.00146807
Iteration 7/25 | Loss: 0.00146687
Iteration 8/25 | Loss: 0.00147033
Iteration 9/25 | Loss: 0.00146915
Iteration 10/25 | Loss: 0.00146466
Iteration 11/25 | Loss: 0.00146370
Iteration 12/25 | Loss: 0.00146297
Iteration 13/25 | Loss: 0.00146254
Iteration 14/25 | Loss: 0.00146599
Iteration 15/25 | Loss: 0.00146567
Iteration 16/25 | Loss: 0.00146020
Iteration 17/25 | Loss: 0.00145959
Iteration 18/25 | Loss: 0.00145942
Iteration 19/25 | Loss: 0.00145941
Iteration 20/25 | Loss: 0.00145941
Iteration 21/25 | Loss: 0.00145941
Iteration 22/25 | Loss: 0.00145941
Iteration 23/25 | Loss: 0.00145941
Iteration 24/25 | Loss: 0.00145940
Iteration 25/25 | Loss: 0.00145940

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37071884
Iteration 2/25 | Loss: 0.00248412
Iteration 3/25 | Loss: 0.00248411
Iteration 4/25 | Loss: 0.00248410
Iteration 5/25 | Loss: 0.00248410
Iteration 6/25 | Loss: 0.00248410
Iteration 7/25 | Loss: 0.00248410
Iteration 8/25 | Loss: 0.00248410
Iteration 9/25 | Loss: 0.00248410
Iteration 10/25 | Loss: 0.00248410
Iteration 11/25 | Loss: 0.00248410
Iteration 12/25 | Loss: 0.00248410
Iteration 13/25 | Loss: 0.00248410
Iteration 14/25 | Loss: 0.00248410
Iteration 15/25 | Loss: 0.00248410
Iteration 16/25 | Loss: 0.00248410
Iteration 17/25 | Loss: 0.00248410
Iteration 18/25 | Loss: 0.00248410
Iteration 19/25 | Loss: 0.00248410
Iteration 20/25 | Loss: 0.00248410
Iteration 21/25 | Loss: 0.00248410
Iteration 22/25 | Loss: 0.00248410
Iteration 23/25 | Loss: 0.00248410
Iteration 24/25 | Loss: 0.00248410
Iteration 25/25 | Loss: 0.00248410

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00248410
Iteration 2/1000 | Loss: 0.00020381
Iteration 3/1000 | Loss: 0.00014747
Iteration 4/1000 | Loss: 0.00012748
Iteration 5/1000 | Loss: 0.00011778
Iteration 6/1000 | Loss: 0.00011216
Iteration 7/1000 | Loss: 0.00010733
Iteration 8/1000 | Loss: 0.00010329
Iteration 9/1000 | Loss: 0.00009914
Iteration 10/1000 | Loss: 0.00009682
Iteration 11/1000 | Loss: 0.00009562
Iteration 12/1000 | Loss: 0.00009468
Iteration 13/1000 | Loss: 0.00009382
Iteration 14/1000 | Loss: 0.00009282
Iteration 15/1000 | Loss: 0.00009196
Iteration 16/1000 | Loss: 0.00009124
Iteration 17/1000 | Loss: 0.00009045
Iteration 18/1000 | Loss: 0.00008973
Iteration 19/1000 | Loss: 0.00008910
Iteration 20/1000 | Loss: 0.00047100
Iteration 21/1000 | Loss: 0.00022736
Iteration 22/1000 | Loss: 0.00008880
Iteration 23/1000 | Loss: 0.00117487
Iteration 24/1000 | Loss: 0.00045937
Iteration 25/1000 | Loss: 0.00024340
Iteration 26/1000 | Loss: 0.00009522
Iteration 27/1000 | Loss: 0.00009032
Iteration 28/1000 | Loss: 0.00033288
Iteration 29/1000 | Loss: 0.00372468
Iteration 30/1000 | Loss: 0.00126398
Iteration 31/1000 | Loss: 0.00010194
Iteration 32/1000 | Loss: 0.00035049
Iteration 33/1000 | Loss: 0.00008995
Iteration 34/1000 | Loss: 0.00084127
Iteration 35/1000 | Loss: 0.00445195
Iteration 36/1000 | Loss: 0.00132970
Iteration 37/1000 | Loss: 0.00125131
Iteration 38/1000 | Loss: 0.00079336
Iteration 39/1000 | Loss: 0.00063725
Iteration 40/1000 | Loss: 0.00032263
Iteration 41/1000 | Loss: 0.00008839
Iteration 42/1000 | Loss: 0.00027931
Iteration 43/1000 | Loss: 0.00008110
Iteration 44/1000 | Loss: 0.00007660
Iteration 45/1000 | Loss: 0.00007264
Iteration 46/1000 | Loss: 0.00006858
Iteration 47/1000 | Loss: 0.00068071
Iteration 48/1000 | Loss: 0.00062441
Iteration 49/1000 | Loss: 0.00024134
Iteration 50/1000 | Loss: 0.00039263
Iteration 51/1000 | Loss: 0.00006483
Iteration 52/1000 | Loss: 0.00074495
Iteration 53/1000 | Loss: 0.00113551
Iteration 54/1000 | Loss: 0.00009700
Iteration 55/1000 | Loss: 0.00056992
Iteration 56/1000 | Loss: 0.00052021
Iteration 57/1000 | Loss: 0.00067899
Iteration 58/1000 | Loss: 0.00056299
Iteration 59/1000 | Loss: 0.00009750
Iteration 60/1000 | Loss: 0.00006707
Iteration 61/1000 | Loss: 0.00006166
Iteration 62/1000 | Loss: 0.00005873
Iteration 63/1000 | Loss: 0.00005592
Iteration 64/1000 | Loss: 0.00005481
Iteration 65/1000 | Loss: 0.00005403
Iteration 66/1000 | Loss: 0.00005333
Iteration 67/1000 | Loss: 0.00005276
Iteration 68/1000 | Loss: 0.00005227
Iteration 69/1000 | Loss: 0.00005175
Iteration 70/1000 | Loss: 0.00144896
Iteration 71/1000 | Loss: 0.00171765
Iteration 72/1000 | Loss: 0.00083449
Iteration 73/1000 | Loss: 0.00008728
Iteration 74/1000 | Loss: 0.00006311
Iteration 75/1000 | Loss: 0.00005629
Iteration 76/1000 | Loss: 0.00005337
Iteration 77/1000 | Loss: 0.00005204
Iteration 78/1000 | Loss: 0.00005114
Iteration 79/1000 | Loss: 0.00005049
Iteration 80/1000 | Loss: 0.00004958
Iteration 81/1000 | Loss: 0.00023835
Iteration 82/1000 | Loss: 0.00017714
Iteration 83/1000 | Loss: 0.00022327
Iteration 84/1000 | Loss: 0.00016229
Iteration 85/1000 | Loss: 0.00005690
Iteration 86/1000 | Loss: 0.00004979
Iteration 87/1000 | Loss: 0.00004815
Iteration 88/1000 | Loss: 0.00004774
Iteration 89/1000 | Loss: 0.00004747
Iteration 90/1000 | Loss: 0.00004722
Iteration 91/1000 | Loss: 0.00004694
Iteration 92/1000 | Loss: 0.00028637
Iteration 93/1000 | Loss: 0.00006790
Iteration 94/1000 | Loss: 0.00004688
Iteration 95/1000 | Loss: 0.00004658
Iteration 96/1000 | Loss: 0.00026903
Iteration 97/1000 | Loss: 0.00041936
Iteration 98/1000 | Loss: 0.00034025
Iteration 99/1000 | Loss: 0.00010585
Iteration 100/1000 | Loss: 0.00010620
Iteration 101/1000 | Loss: 0.00007655
Iteration 102/1000 | Loss: 0.00023516
Iteration 103/1000 | Loss: 0.00037278
Iteration 104/1000 | Loss: 0.00021135
Iteration 105/1000 | Loss: 0.00019338
Iteration 106/1000 | Loss: 0.00020959
Iteration 107/1000 | Loss: 0.00014934
Iteration 108/1000 | Loss: 0.00020204
Iteration 109/1000 | Loss: 0.00015587
Iteration 110/1000 | Loss: 0.00017888
Iteration 111/1000 | Loss: 0.00025042
Iteration 112/1000 | Loss: 0.00020626
Iteration 113/1000 | Loss: 0.00005030
Iteration 114/1000 | Loss: 0.00022750
Iteration 115/1000 | Loss: 0.00005362
Iteration 116/1000 | Loss: 0.00004991
Iteration 117/1000 | Loss: 0.00004813
Iteration 118/1000 | Loss: 0.00004738
Iteration 119/1000 | Loss: 0.00004690
Iteration 120/1000 | Loss: 0.00004647
Iteration 121/1000 | Loss: 0.00004645
Iteration 122/1000 | Loss: 0.00004639
Iteration 123/1000 | Loss: 0.00004627
Iteration 124/1000 | Loss: 0.00004626
Iteration 125/1000 | Loss: 0.00004624
Iteration 126/1000 | Loss: 0.00004623
Iteration 127/1000 | Loss: 0.00004623
Iteration 128/1000 | Loss: 0.00004622
Iteration 129/1000 | Loss: 0.00004621
Iteration 130/1000 | Loss: 0.00004621
Iteration 131/1000 | Loss: 0.00004616
Iteration 132/1000 | Loss: 0.00064833
Iteration 133/1000 | Loss: 0.00026632
Iteration 134/1000 | Loss: 0.00005722
Iteration 135/1000 | Loss: 0.00046410
Iteration 136/1000 | Loss: 0.00017412
Iteration 137/1000 | Loss: 0.00027122
Iteration 138/1000 | Loss: 0.00004712
Iteration 139/1000 | Loss: 0.00004635
Iteration 140/1000 | Loss: 0.00004589
Iteration 141/1000 | Loss: 0.00004546
Iteration 142/1000 | Loss: 0.00004513
Iteration 143/1000 | Loss: 0.00065243
Iteration 144/1000 | Loss: 0.00006407
Iteration 145/1000 | Loss: 0.00066540
Iteration 146/1000 | Loss: 0.00029449
Iteration 147/1000 | Loss: 0.00004914
Iteration 148/1000 | Loss: 0.00004472
Iteration 149/1000 | Loss: 0.00004362
Iteration 150/1000 | Loss: 0.00004309
Iteration 151/1000 | Loss: 0.00004275
Iteration 152/1000 | Loss: 0.00004273
Iteration 153/1000 | Loss: 0.00004248
Iteration 154/1000 | Loss: 0.00004247
Iteration 155/1000 | Loss: 0.00004246
Iteration 156/1000 | Loss: 0.00004244
Iteration 157/1000 | Loss: 0.00004243
Iteration 158/1000 | Loss: 0.00004242
Iteration 159/1000 | Loss: 0.00004241
Iteration 160/1000 | Loss: 0.00004239
Iteration 161/1000 | Loss: 0.00004238
Iteration 162/1000 | Loss: 0.00004238
Iteration 163/1000 | Loss: 0.00004237
Iteration 164/1000 | Loss: 0.00004233
Iteration 165/1000 | Loss: 0.00004230
Iteration 166/1000 | Loss: 0.00004228
Iteration 167/1000 | Loss: 0.00004228
Iteration 168/1000 | Loss: 0.00004227
Iteration 169/1000 | Loss: 0.00004226
Iteration 170/1000 | Loss: 0.00004226
Iteration 171/1000 | Loss: 0.00004226
Iteration 172/1000 | Loss: 0.00004225
Iteration 173/1000 | Loss: 0.00004223
Iteration 174/1000 | Loss: 0.00004220
Iteration 175/1000 | Loss: 0.00004220
Iteration 176/1000 | Loss: 0.00004220
Iteration 177/1000 | Loss: 0.00004220
Iteration 178/1000 | Loss: 0.00004219
Iteration 179/1000 | Loss: 0.00004219
Iteration 180/1000 | Loss: 0.00004219
Iteration 181/1000 | Loss: 0.00004219
Iteration 182/1000 | Loss: 0.00004219
Iteration 183/1000 | Loss: 0.00004219
Iteration 184/1000 | Loss: 0.00004219
Iteration 185/1000 | Loss: 0.00004219
Iteration 186/1000 | Loss: 0.00004219
Iteration 187/1000 | Loss: 0.00004219
Iteration 188/1000 | Loss: 0.00004219
Iteration 189/1000 | Loss: 0.00004218
Iteration 190/1000 | Loss: 0.00004217
Iteration 191/1000 | Loss: 0.00004217
Iteration 192/1000 | Loss: 0.00004216
Iteration 193/1000 | Loss: 0.00004216
Iteration 194/1000 | Loss: 0.00004216
Iteration 195/1000 | Loss: 0.00004216
Iteration 196/1000 | Loss: 0.00004216
Iteration 197/1000 | Loss: 0.00004216
Iteration 198/1000 | Loss: 0.00004215
Iteration 199/1000 | Loss: 0.00004215
Iteration 200/1000 | Loss: 0.00004215
Iteration 201/1000 | Loss: 0.00004215
Iteration 202/1000 | Loss: 0.00004215
Iteration 203/1000 | Loss: 0.00004215
Iteration 204/1000 | Loss: 0.00004214
Iteration 205/1000 | Loss: 0.00004214
Iteration 206/1000 | Loss: 0.00004214
Iteration 207/1000 | Loss: 0.00004213
Iteration 208/1000 | Loss: 0.00004213
Iteration 209/1000 | Loss: 0.00004213
Iteration 210/1000 | Loss: 0.00004213
Iteration 211/1000 | Loss: 0.00004213
Iteration 212/1000 | Loss: 0.00004212
Iteration 213/1000 | Loss: 0.00004212
Iteration 214/1000 | Loss: 0.00004212
Iteration 215/1000 | Loss: 0.00004212
Iteration 216/1000 | Loss: 0.00004212
Iteration 217/1000 | Loss: 0.00004212
Iteration 218/1000 | Loss: 0.00004212
Iteration 219/1000 | Loss: 0.00004211
Iteration 220/1000 | Loss: 0.00004211
Iteration 221/1000 | Loss: 0.00004210
Iteration 222/1000 | Loss: 0.00004210
Iteration 223/1000 | Loss: 0.00004210
Iteration 224/1000 | Loss: 0.00004210
Iteration 225/1000 | Loss: 0.00004210
Iteration 226/1000 | Loss: 0.00004210
Iteration 227/1000 | Loss: 0.00004210
Iteration 228/1000 | Loss: 0.00004210
Iteration 229/1000 | Loss: 0.00004210
Iteration 230/1000 | Loss: 0.00004209
Iteration 231/1000 | Loss: 0.00004209
Iteration 232/1000 | Loss: 0.00004209
Iteration 233/1000 | Loss: 0.00004209
Iteration 234/1000 | Loss: 0.00004209
Iteration 235/1000 | Loss: 0.00004209
Iteration 236/1000 | Loss: 0.00004209
Iteration 237/1000 | Loss: 0.00004209
Iteration 238/1000 | Loss: 0.00004209
Iteration 239/1000 | Loss: 0.00004209
Iteration 240/1000 | Loss: 0.00004209
Iteration 241/1000 | Loss: 0.00004209
Iteration 242/1000 | Loss: 0.00004209
Iteration 243/1000 | Loss: 0.00004208
Iteration 244/1000 | Loss: 0.00004208
Iteration 245/1000 | Loss: 0.00004208
Iteration 246/1000 | Loss: 0.00004208
Iteration 247/1000 | Loss: 0.00004208
Iteration 248/1000 | Loss: 0.00004208
Iteration 249/1000 | Loss: 0.00004207
Iteration 250/1000 | Loss: 0.00004207
Iteration 251/1000 | Loss: 0.00004207
Iteration 252/1000 | Loss: 0.00004207
Iteration 253/1000 | Loss: 0.00004207
Iteration 254/1000 | Loss: 0.00004207
Iteration 255/1000 | Loss: 0.00004206
Iteration 256/1000 | Loss: 0.00004206
Iteration 257/1000 | Loss: 0.00004206
Iteration 258/1000 | Loss: 0.00004206
Iteration 259/1000 | Loss: 0.00004205
Iteration 260/1000 | Loss: 0.00004205
Iteration 261/1000 | Loss: 0.00004204
Iteration 262/1000 | Loss: 0.00004204
Iteration 263/1000 | Loss: 0.00004203
Iteration 264/1000 | Loss: 0.00004203
Iteration 265/1000 | Loss: 0.00004203
Iteration 266/1000 | Loss: 0.00004202
Iteration 267/1000 | Loss: 0.00004202
Iteration 268/1000 | Loss: 0.00004202
Iteration 269/1000 | Loss: 0.00004201
Iteration 270/1000 | Loss: 0.00004201
Iteration 271/1000 | Loss: 0.00004201
Iteration 272/1000 | Loss: 0.00004201
Iteration 273/1000 | Loss: 0.00004201
Iteration 274/1000 | Loss: 0.00004201
Iteration 275/1000 | Loss: 0.00004200
Iteration 276/1000 | Loss: 0.00004200
Iteration 277/1000 | Loss: 0.00004200
Iteration 278/1000 | Loss: 0.00004200
Iteration 279/1000 | Loss: 0.00004200
Iteration 280/1000 | Loss: 0.00004199
Iteration 281/1000 | Loss: 0.00004199
Iteration 282/1000 | Loss: 0.00004199
Iteration 283/1000 | Loss: 0.00004199
Iteration 284/1000 | Loss: 0.00004199
Iteration 285/1000 | Loss: 0.00004199
Iteration 286/1000 | Loss: 0.00004199
Iteration 287/1000 | Loss: 0.00004199
Iteration 288/1000 | Loss: 0.00004199
Iteration 289/1000 | Loss: 0.00004199
Iteration 290/1000 | Loss: 0.00004199
Iteration 291/1000 | Loss: 0.00004199
Iteration 292/1000 | Loss: 0.00004199
Iteration 293/1000 | Loss: 0.00004199
Iteration 294/1000 | Loss: 0.00004199
Iteration 295/1000 | Loss: 0.00004198
Iteration 296/1000 | Loss: 0.00004198
Iteration 297/1000 | Loss: 0.00004198
Iteration 298/1000 | Loss: 0.00004198
Iteration 299/1000 | Loss: 0.00004198
Iteration 300/1000 | Loss: 0.00004198
Iteration 301/1000 | Loss: 0.00004198
Iteration 302/1000 | Loss: 0.00004198
Iteration 303/1000 | Loss: 0.00004197
Iteration 304/1000 | Loss: 0.00004197
Iteration 305/1000 | Loss: 0.00004197
Iteration 306/1000 | Loss: 0.00004197
Iteration 307/1000 | Loss: 0.00004197
Iteration 308/1000 | Loss: 0.00004197
Iteration 309/1000 | Loss: 0.00004197
Iteration 310/1000 | Loss: 0.00004197
Iteration 311/1000 | Loss: 0.00004197
Iteration 312/1000 | Loss: 0.00004197
Iteration 313/1000 | Loss: 0.00004197
Iteration 314/1000 | Loss: 0.00004197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 314. Stopping optimization.
Last 5 losses: [4.196734880679287e-05, 4.196734880679287e-05, 4.196734880679287e-05, 4.196734880679287e-05, 4.196734880679287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.196734880679287e-05

Optimization complete. Final v2v error: 4.163650989532471 mm

Highest mean error: 11.601844787597656 mm for frame 42

Lowest mean error: 3.147308588027954 mm for frame 26

Saving results

Total time: 255.2400460243225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763249
Iteration 2/25 | Loss: 0.00165725
Iteration 3/25 | Loss: 0.00147693
Iteration 4/25 | Loss: 0.00144215
Iteration 5/25 | Loss: 0.00144363
Iteration 6/25 | Loss: 0.00142684
Iteration 7/25 | Loss: 0.00142329
Iteration 8/25 | Loss: 0.00140945
Iteration 9/25 | Loss: 0.00137798
Iteration 10/25 | Loss: 0.00137254
Iteration 11/25 | Loss: 0.00137204
Iteration 12/25 | Loss: 0.00137201
Iteration 13/25 | Loss: 0.00137201
Iteration 14/25 | Loss: 0.00137201
Iteration 15/25 | Loss: 0.00137200
Iteration 16/25 | Loss: 0.00137200
Iteration 17/25 | Loss: 0.00137200
Iteration 18/25 | Loss: 0.00137200
Iteration 19/25 | Loss: 0.00137200
Iteration 20/25 | Loss: 0.00137200
Iteration 21/25 | Loss: 0.00137200
Iteration 22/25 | Loss: 0.00137200
Iteration 23/25 | Loss: 0.00137200
Iteration 24/25 | Loss: 0.00137200
Iteration 25/25 | Loss: 0.00137200

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34672487
Iteration 2/25 | Loss: 0.00081056
Iteration 3/25 | Loss: 0.00081052
Iteration 4/25 | Loss: 0.00081052
Iteration 5/25 | Loss: 0.00081052
Iteration 6/25 | Loss: 0.00081052
Iteration 7/25 | Loss: 0.00081052
Iteration 8/25 | Loss: 0.00081052
Iteration 9/25 | Loss: 0.00081052
Iteration 10/25 | Loss: 0.00081052
Iteration 11/25 | Loss: 0.00081052
Iteration 12/25 | Loss: 0.00081052
Iteration 13/25 | Loss: 0.00081052
Iteration 14/25 | Loss: 0.00081052
Iteration 15/25 | Loss: 0.00081052
Iteration 16/25 | Loss: 0.00081052
Iteration 17/25 | Loss: 0.00081052
Iteration 18/25 | Loss: 0.00081052
Iteration 19/25 | Loss: 0.00081052
Iteration 20/25 | Loss: 0.00081052
Iteration 21/25 | Loss: 0.00081052
Iteration 22/25 | Loss: 0.00081052
Iteration 23/25 | Loss: 0.00081052
Iteration 24/25 | Loss: 0.00081052
Iteration 25/25 | Loss: 0.00081052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081052
Iteration 2/1000 | Loss: 0.00005951
Iteration 3/1000 | Loss: 0.00003728
Iteration 4/1000 | Loss: 0.00003067
Iteration 5/1000 | Loss: 0.00002832
Iteration 6/1000 | Loss: 0.00002721
Iteration 7/1000 | Loss: 0.00002648
Iteration 8/1000 | Loss: 0.00002591
Iteration 9/1000 | Loss: 0.00002534
Iteration 10/1000 | Loss: 0.00002498
Iteration 11/1000 | Loss: 0.00002468
Iteration 12/1000 | Loss: 0.00002438
Iteration 13/1000 | Loss: 0.00002414
Iteration 14/1000 | Loss: 0.00002398
Iteration 15/1000 | Loss: 0.00002387
Iteration 16/1000 | Loss: 0.00002377
Iteration 17/1000 | Loss: 0.00002373
Iteration 18/1000 | Loss: 0.00002373
Iteration 19/1000 | Loss: 0.00002373
Iteration 20/1000 | Loss: 0.00002370
Iteration 21/1000 | Loss: 0.00002369
Iteration 22/1000 | Loss: 0.00002369
Iteration 23/1000 | Loss: 0.00002368
Iteration 24/1000 | Loss: 0.00002365
Iteration 25/1000 | Loss: 0.00002365
Iteration 26/1000 | Loss: 0.00002362
Iteration 27/1000 | Loss: 0.00002361
Iteration 28/1000 | Loss: 0.00002361
Iteration 29/1000 | Loss: 0.00002360
Iteration 30/1000 | Loss: 0.00002359
Iteration 31/1000 | Loss: 0.00002358
Iteration 32/1000 | Loss: 0.00002358
Iteration 33/1000 | Loss: 0.00002355
Iteration 34/1000 | Loss: 0.00002355
Iteration 35/1000 | Loss: 0.00002355
Iteration 36/1000 | Loss: 0.00002355
Iteration 37/1000 | Loss: 0.00002354
Iteration 38/1000 | Loss: 0.00002353
Iteration 39/1000 | Loss: 0.00002350
Iteration 40/1000 | Loss: 0.00002350
Iteration 41/1000 | Loss: 0.00002350
Iteration 42/1000 | Loss: 0.00002349
Iteration 43/1000 | Loss: 0.00002349
Iteration 44/1000 | Loss: 0.00002348
Iteration 45/1000 | Loss: 0.00002348
Iteration 46/1000 | Loss: 0.00002347
Iteration 47/1000 | Loss: 0.00002347
Iteration 48/1000 | Loss: 0.00002347
Iteration 49/1000 | Loss: 0.00002346
Iteration 50/1000 | Loss: 0.00002346
Iteration 51/1000 | Loss: 0.00002346
Iteration 52/1000 | Loss: 0.00002345
Iteration 53/1000 | Loss: 0.00002345
Iteration 54/1000 | Loss: 0.00002345
Iteration 55/1000 | Loss: 0.00002345
Iteration 56/1000 | Loss: 0.00002345
Iteration 57/1000 | Loss: 0.00002344
Iteration 58/1000 | Loss: 0.00002344
Iteration 59/1000 | Loss: 0.00002344
Iteration 60/1000 | Loss: 0.00002344
Iteration 61/1000 | Loss: 0.00002343
Iteration 62/1000 | Loss: 0.00002343
Iteration 63/1000 | Loss: 0.00002343
Iteration 64/1000 | Loss: 0.00002343
Iteration 65/1000 | Loss: 0.00002342
Iteration 66/1000 | Loss: 0.00002342
Iteration 67/1000 | Loss: 0.00002342
Iteration 68/1000 | Loss: 0.00002342
Iteration 69/1000 | Loss: 0.00002342
Iteration 70/1000 | Loss: 0.00002341
Iteration 71/1000 | Loss: 0.00002341
Iteration 72/1000 | Loss: 0.00002341
Iteration 73/1000 | Loss: 0.00002341
Iteration 74/1000 | Loss: 0.00002340
Iteration 75/1000 | Loss: 0.00002340
Iteration 76/1000 | Loss: 0.00002340
Iteration 77/1000 | Loss: 0.00002340
Iteration 78/1000 | Loss: 0.00002340
Iteration 79/1000 | Loss: 0.00002340
Iteration 80/1000 | Loss: 0.00002340
Iteration 81/1000 | Loss: 0.00002340
Iteration 82/1000 | Loss: 0.00002340
Iteration 83/1000 | Loss: 0.00002340
Iteration 84/1000 | Loss: 0.00002340
Iteration 85/1000 | Loss: 0.00002340
Iteration 86/1000 | Loss: 0.00002340
Iteration 87/1000 | Loss: 0.00002340
Iteration 88/1000 | Loss: 0.00002339
Iteration 89/1000 | Loss: 0.00002339
Iteration 90/1000 | Loss: 0.00002339
Iteration 91/1000 | Loss: 0.00002339
Iteration 92/1000 | Loss: 0.00002339
Iteration 93/1000 | Loss: 0.00002339
Iteration 94/1000 | Loss: 0.00002339
Iteration 95/1000 | Loss: 0.00002339
Iteration 96/1000 | Loss: 0.00002339
Iteration 97/1000 | Loss: 0.00002339
Iteration 98/1000 | Loss: 0.00002339
Iteration 99/1000 | Loss: 0.00002339
Iteration 100/1000 | Loss: 0.00002339
Iteration 101/1000 | Loss: 0.00002339
Iteration 102/1000 | Loss: 0.00002339
Iteration 103/1000 | Loss: 0.00002339
Iteration 104/1000 | Loss: 0.00002339
Iteration 105/1000 | Loss: 0.00002339
Iteration 106/1000 | Loss: 0.00002339
Iteration 107/1000 | Loss: 0.00002339
Iteration 108/1000 | Loss: 0.00002339
Iteration 109/1000 | Loss: 0.00002339
Iteration 110/1000 | Loss: 0.00002339
Iteration 111/1000 | Loss: 0.00002339
Iteration 112/1000 | Loss: 0.00002339
Iteration 113/1000 | Loss: 0.00002339
Iteration 114/1000 | Loss: 0.00002339
Iteration 115/1000 | Loss: 0.00002339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.3394783056573942e-05, 2.3394783056573942e-05, 2.3394783056573942e-05, 2.3394783056573942e-05, 2.3394783056573942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3394783056573942e-05

Optimization complete. Final v2v error: 3.9613778591156006 mm

Highest mean error: 4.6614670753479 mm for frame 127

Lowest mean error: 3.3833906650543213 mm for frame 12

Saving results

Total time: 48.07685875892639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802579
Iteration 2/25 | Loss: 0.00147918
Iteration 3/25 | Loss: 0.00131311
Iteration 4/25 | Loss: 0.00130613
Iteration 5/25 | Loss: 0.00130454
Iteration 6/25 | Loss: 0.00130454
Iteration 7/25 | Loss: 0.00130454
Iteration 8/25 | Loss: 0.00130454
Iteration 9/25 | Loss: 0.00130454
Iteration 10/25 | Loss: 0.00130454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013045412488281727, 0.0013045412488281727, 0.0013045412488281727, 0.0013045412488281727, 0.0013045412488281727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013045412488281727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40112555
Iteration 2/25 | Loss: 0.00090247
Iteration 3/25 | Loss: 0.00090247
Iteration 4/25 | Loss: 0.00090247
Iteration 5/25 | Loss: 0.00090247
Iteration 6/25 | Loss: 0.00090247
Iteration 7/25 | Loss: 0.00090247
Iteration 8/25 | Loss: 0.00090247
Iteration 9/25 | Loss: 0.00090247
Iteration 10/25 | Loss: 0.00090247
Iteration 11/25 | Loss: 0.00090247
Iteration 12/25 | Loss: 0.00090247
Iteration 13/25 | Loss: 0.00090247
Iteration 14/25 | Loss: 0.00090247
Iteration 15/25 | Loss: 0.00090247
Iteration 16/25 | Loss: 0.00090247
Iteration 17/25 | Loss: 0.00090247
Iteration 18/25 | Loss: 0.00090247
Iteration 19/25 | Loss: 0.00090247
Iteration 20/25 | Loss: 0.00090247
Iteration 21/25 | Loss: 0.00090247
Iteration 22/25 | Loss: 0.00090247
Iteration 23/25 | Loss: 0.00090247
Iteration 24/25 | Loss: 0.00090247
Iteration 25/25 | Loss: 0.00090247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090247
Iteration 2/1000 | Loss: 0.00002874
Iteration 3/1000 | Loss: 0.00001895
Iteration 4/1000 | Loss: 0.00001650
Iteration 5/1000 | Loss: 0.00001554
Iteration 6/1000 | Loss: 0.00001480
Iteration 7/1000 | Loss: 0.00001429
Iteration 8/1000 | Loss: 0.00001398
Iteration 9/1000 | Loss: 0.00001358
Iteration 10/1000 | Loss: 0.00001345
Iteration 11/1000 | Loss: 0.00001326
Iteration 12/1000 | Loss: 0.00001326
Iteration 13/1000 | Loss: 0.00001307
Iteration 14/1000 | Loss: 0.00001301
Iteration 15/1000 | Loss: 0.00001301
Iteration 16/1000 | Loss: 0.00001299
Iteration 17/1000 | Loss: 0.00001292
Iteration 18/1000 | Loss: 0.00001291
Iteration 19/1000 | Loss: 0.00001289
Iteration 20/1000 | Loss: 0.00001288
Iteration 21/1000 | Loss: 0.00001287
Iteration 22/1000 | Loss: 0.00001286
Iteration 23/1000 | Loss: 0.00001282
Iteration 24/1000 | Loss: 0.00001281
Iteration 25/1000 | Loss: 0.00001280
Iteration 26/1000 | Loss: 0.00001279
Iteration 27/1000 | Loss: 0.00001278
Iteration 28/1000 | Loss: 0.00001278
Iteration 29/1000 | Loss: 0.00001278
Iteration 30/1000 | Loss: 0.00001277
Iteration 31/1000 | Loss: 0.00001277
Iteration 32/1000 | Loss: 0.00001277
Iteration 33/1000 | Loss: 0.00001277
Iteration 34/1000 | Loss: 0.00001276
Iteration 35/1000 | Loss: 0.00001276
Iteration 36/1000 | Loss: 0.00001275
Iteration 37/1000 | Loss: 0.00001274
Iteration 38/1000 | Loss: 0.00001273
Iteration 39/1000 | Loss: 0.00001273
Iteration 40/1000 | Loss: 0.00001273
Iteration 41/1000 | Loss: 0.00001273
Iteration 42/1000 | Loss: 0.00001272
Iteration 43/1000 | Loss: 0.00001272
Iteration 44/1000 | Loss: 0.00001272
Iteration 45/1000 | Loss: 0.00001272
Iteration 46/1000 | Loss: 0.00001271
Iteration 47/1000 | Loss: 0.00001270
Iteration 48/1000 | Loss: 0.00001270
Iteration 49/1000 | Loss: 0.00001270
Iteration 50/1000 | Loss: 0.00001269
Iteration 51/1000 | Loss: 0.00001269
Iteration 52/1000 | Loss: 0.00001269
Iteration 53/1000 | Loss: 0.00001269
Iteration 54/1000 | Loss: 0.00001269
Iteration 55/1000 | Loss: 0.00001268
Iteration 56/1000 | Loss: 0.00001268
Iteration 57/1000 | Loss: 0.00001268
Iteration 58/1000 | Loss: 0.00001268
Iteration 59/1000 | Loss: 0.00001268
Iteration 60/1000 | Loss: 0.00001267
Iteration 61/1000 | Loss: 0.00001267
Iteration 62/1000 | Loss: 0.00001266
Iteration 63/1000 | Loss: 0.00001266
Iteration 64/1000 | Loss: 0.00001265
Iteration 65/1000 | Loss: 0.00001265
Iteration 66/1000 | Loss: 0.00001265
Iteration 67/1000 | Loss: 0.00001265
Iteration 68/1000 | Loss: 0.00001265
Iteration 69/1000 | Loss: 0.00001265
Iteration 70/1000 | Loss: 0.00001264
Iteration 71/1000 | Loss: 0.00001264
Iteration 72/1000 | Loss: 0.00001264
Iteration 73/1000 | Loss: 0.00001264
Iteration 74/1000 | Loss: 0.00001263
Iteration 75/1000 | Loss: 0.00001263
Iteration 76/1000 | Loss: 0.00001263
Iteration 77/1000 | Loss: 0.00001263
Iteration 78/1000 | Loss: 0.00001262
Iteration 79/1000 | Loss: 0.00001262
Iteration 80/1000 | Loss: 0.00001262
Iteration 81/1000 | Loss: 0.00001262
Iteration 82/1000 | Loss: 0.00001262
Iteration 83/1000 | Loss: 0.00001262
Iteration 84/1000 | Loss: 0.00001262
Iteration 85/1000 | Loss: 0.00001261
Iteration 86/1000 | Loss: 0.00001261
Iteration 87/1000 | Loss: 0.00001261
Iteration 88/1000 | Loss: 0.00001261
Iteration 89/1000 | Loss: 0.00001261
Iteration 90/1000 | Loss: 0.00001261
Iteration 91/1000 | Loss: 0.00001261
Iteration 92/1000 | Loss: 0.00001261
Iteration 93/1000 | Loss: 0.00001260
Iteration 94/1000 | Loss: 0.00001260
Iteration 95/1000 | Loss: 0.00001260
Iteration 96/1000 | Loss: 0.00001260
Iteration 97/1000 | Loss: 0.00001260
Iteration 98/1000 | Loss: 0.00001260
Iteration 99/1000 | Loss: 0.00001260
Iteration 100/1000 | Loss: 0.00001260
Iteration 101/1000 | Loss: 0.00001259
Iteration 102/1000 | Loss: 0.00001259
Iteration 103/1000 | Loss: 0.00001259
Iteration 104/1000 | Loss: 0.00001259
Iteration 105/1000 | Loss: 0.00001259
Iteration 106/1000 | Loss: 0.00001259
Iteration 107/1000 | Loss: 0.00001258
Iteration 108/1000 | Loss: 0.00001258
Iteration 109/1000 | Loss: 0.00001258
Iteration 110/1000 | Loss: 0.00001258
Iteration 111/1000 | Loss: 0.00001258
Iteration 112/1000 | Loss: 0.00001258
Iteration 113/1000 | Loss: 0.00001258
Iteration 114/1000 | Loss: 0.00001258
Iteration 115/1000 | Loss: 0.00001258
Iteration 116/1000 | Loss: 0.00001258
Iteration 117/1000 | Loss: 0.00001258
Iteration 118/1000 | Loss: 0.00001258
Iteration 119/1000 | Loss: 0.00001258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.2581193914229516e-05, 1.2581193914229516e-05, 1.2581193914229516e-05, 1.2581193914229516e-05, 1.2581193914229516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2581193914229516e-05

Optimization complete. Final v2v error: 3.039867639541626 mm

Highest mean error: 3.215836763381958 mm for frame 150

Lowest mean error: 2.9298369884490967 mm for frame 77

Saving results

Total time: 39.571609020233154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789316
Iteration 2/25 | Loss: 0.00144369
Iteration 3/25 | Loss: 0.00135340
Iteration 4/25 | Loss: 0.00134110
Iteration 5/25 | Loss: 0.00133741
Iteration 6/25 | Loss: 0.00133646
Iteration 7/25 | Loss: 0.00133646
Iteration 8/25 | Loss: 0.00133646
Iteration 9/25 | Loss: 0.00133646
Iteration 10/25 | Loss: 0.00133646
Iteration 11/25 | Loss: 0.00133646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013364619808271527, 0.0013364619808271527, 0.0013364619808271527, 0.0013364619808271527, 0.0013364619808271527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013364619808271527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49481010
Iteration 2/25 | Loss: 0.00084340
Iteration 3/25 | Loss: 0.00084339
Iteration 4/25 | Loss: 0.00084339
Iteration 5/25 | Loss: 0.00084339
Iteration 6/25 | Loss: 0.00084339
Iteration 7/25 | Loss: 0.00084339
Iteration 8/25 | Loss: 0.00084339
Iteration 9/25 | Loss: 0.00084339
Iteration 10/25 | Loss: 0.00084339
Iteration 11/25 | Loss: 0.00084339
Iteration 12/25 | Loss: 0.00084339
Iteration 13/25 | Loss: 0.00084339
Iteration 14/25 | Loss: 0.00084339
Iteration 15/25 | Loss: 0.00084339
Iteration 16/25 | Loss: 0.00084339
Iteration 17/25 | Loss: 0.00084339
Iteration 18/25 | Loss: 0.00084339
Iteration 19/25 | Loss: 0.00084339
Iteration 20/25 | Loss: 0.00084339
Iteration 21/25 | Loss: 0.00084339
Iteration 22/25 | Loss: 0.00084339
Iteration 23/25 | Loss: 0.00084339
Iteration 24/25 | Loss: 0.00084339
Iteration 25/25 | Loss: 0.00084339

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084339
Iteration 2/1000 | Loss: 0.00004673
Iteration 3/1000 | Loss: 0.00002966
Iteration 4/1000 | Loss: 0.00002498
Iteration 5/1000 | Loss: 0.00002357
Iteration 6/1000 | Loss: 0.00002249
Iteration 7/1000 | Loss: 0.00002192
Iteration 8/1000 | Loss: 0.00002144
Iteration 9/1000 | Loss: 0.00002101
Iteration 10/1000 | Loss: 0.00002070
Iteration 11/1000 | Loss: 0.00002042
Iteration 12/1000 | Loss: 0.00002028
Iteration 13/1000 | Loss: 0.00002019
Iteration 14/1000 | Loss: 0.00002017
Iteration 15/1000 | Loss: 0.00002009
Iteration 16/1000 | Loss: 0.00002008
Iteration 17/1000 | Loss: 0.00002006
Iteration 18/1000 | Loss: 0.00002000
Iteration 19/1000 | Loss: 0.00001991
Iteration 20/1000 | Loss: 0.00001989
Iteration 21/1000 | Loss: 0.00001973
Iteration 22/1000 | Loss: 0.00001972
Iteration 23/1000 | Loss: 0.00001970
Iteration 24/1000 | Loss: 0.00001968
Iteration 25/1000 | Loss: 0.00001967
Iteration 26/1000 | Loss: 0.00001966
Iteration 27/1000 | Loss: 0.00001966
Iteration 28/1000 | Loss: 0.00001965
Iteration 29/1000 | Loss: 0.00001965
Iteration 30/1000 | Loss: 0.00001965
Iteration 31/1000 | Loss: 0.00001964
Iteration 32/1000 | Loss: 0.00001964
Iteration 33/1000 | Loss: 0.00001963
Iteration 34/1000 | Loss: 0.00001963
Iteration 35/1000 | Loss: 0.00001962
Iteration 36/1000 | Loss: 0.00001961
Iteration 37/1000 | Loss: 0.00001961
Iteration 38/1000 | Loss: 0.00001960
Iteration 39/1000 | Loss: 0.00001960
Iteration 40/1000 | Loss: 0.00001959
Iteration 41/1000 | Loss: 0.00001959
Iteration 42/1000 | Loss: 0.00001958
Iteration 43/1000 | Loss: 0.00001957
Iteration 44/1000 | Loss: 0.00001957
Iteration 45/1000 | Loss: 0.00001956
Iteration 46/1000 | Loss: 0.00001956
Iteration 47/1000 | Loss: 0.00001956
Iteration 48/1000 | Loss: 0.00001955
Iteration 49/1000 | Loss: 0.00001955
Iteration 50/1000 | Loss: 0.00001954
Iteration 51/1000 | Loss: 0.00001953
Iteration 52/1000 | Loss: 0.00001952
Iteration 53/1000 | Loss: 0.00001952
Iteration 54/1000 | Loss: 0.00001952
Iteration 55/1000 | Loss: 0.00001951
Iteration 56/1000 | Loss: 0.00001951
Iteration 57/1000 | Loss: 0.00001950
Iteration 58/1000 | Loss: 0.00001950
Iteration 59/1000 | Loss: 0.00001949
Iteration 60/1000 | Loss: 0.00001949
Iteration 61/1000 | Loss: 0.00001949
Iteration 62/1000 | Loss: 0.00001948
Iteration 63/1000 | Loss: 0.00001948
Iteration 64/1000 | Loss: 0.00001948
Iteration 65/1000 | Loss: 0.00001948
Iteration 66/1000 | Loss: 0.00001948
Iteration 67/1000 | Loss: 0.00001947
Iteration 68/1000 | Loss: 0.00001947
Iteration 69/1000 | Loss: 0.00001947
Iteration 70/1000 | Loss: 0.00001947
Iteration 71/1000 | Loss: 0.00001947
Iteration 72/1000 | Loss: 0.00001947
Iteration 73/1000 | Loss: 0.00001946
Iteration 74/1000 | Loss: 0.00001946
Iteration 75/1000 | Loss: 0.00001945
Iteration 76/1000 | Loss: 0.00001945
Iteration 77/1000 | Loss: 0.00001945
Iteration 78/1000 | Loss: 0.00001945
Iteration 79/1000 | Loss: 0.00001945
Iteration 80/1000 | Loss: 0.00001945
Iteration 81/1000 | Loss: 0.00001945
Iteration 82/1000 | Loss: 0.00001945
Iteration 83/1000 | Loss: 0.00001944
Iteration 84/1000 | Loss: 0.00001944
Iteration 85/1000 | Loss: 0.00001944
Iteration 86/1000 | Loss: 0.00001944
Iteration 87/1000 | Loss: 0.00001944
Iteration 88/1000 | Loss: 0.00001944
Iteration 89/1000 | Loss: 0.00001944
Iteration 90/1000 | Loss: 0.00001944
Iteration 91/1000 | Loss: 0.00001944
Iteration 92/1000 | Loss: 0.00001943
Iteration 93/1000 | Loss: 0.00001943
Iteration 94/1000 | Loss: 0.00001943
Iteration 95/1000 | Loss: 0.00001943
Iteration 96/1000 | Loss: 0.00001943
Iteration 97/1000 | Loss: 0.00001943
Iteration 98/1000 | Loss: 0.00001943
Iteration 99/1000 | Loss: 0.00001943
Iteration 100/1000 | Loss: 0.00001942
Iteration 101/1000 | Loss: 0.00001942
Iteration 102/1000 | Loss: 0.00001942
Iteration 103/1000 | Loss: 0.00001941
Iteration 104/1000 | Loss: 0.00001941
Iteration 105/1000 | Loss: 0.00001941
Iteration 106/1000 | Loss: 0.00001941
Iteration 107/1000 | Loss: 0.00001941
Iteration 108/1000 | Loss: 0.00001941
Iteration 109/1000 | Loss: 0.00001941
Iteration 110/1000 | Loss: 0.00001940
Iteration 111/1000 | Loss: 0.00001940
Iteration 112/1000 | Loss: 0.00001940
Iteration 113/1000 | Loss: 0.00001940
Iteration 114/1000 | Loss: 0.00001940
Iteration 115/1000 | Loss: 0.00001939
Iteration 116/1000 | Loss: 0.00001939
Iteration 117/1000 | Loss: 0.00001938
Iteration 118/1000 | Loss: 0.00001938
Iteration 119/1000 | Loss: 0.00001938
Iteration 120/1000 | Loss: 0.00001938
Iteration 121/1000 | Loss: 0.00001937
Iteration 122/1000 | Loss: 0.00001937
Iteration 123/1000 | Loss: 0.00001937
Iteration 124/1000 | Loss: 0.00001936
Iteration 125/1000 | Loss: 0.00001936
Iteration 126/1000 | Loss: 0.00001936
Iteration 127/1000 | Loss: 0.00001935
Iteration 128/1000 | Loss: 0.00001935
Iteration 129/1000 | Loss: 0.00001935
Iteration 130/1000 | Loss: 0.00001935
Iteration 131/1000 | Loss: 0.00001935
Iteration 132/1000 | Loss: 0.00001934
Iteration 133/1000 | Loss: 0.00001934
Iteration 134/1000 | Loss: 0.00001934
Iteration 135/1000 | Loss: 0.00001934
Iteration 136/1000 | Loss: 0.00001934
Iteration 137/1000 | Loss: 0.00001934
Iteration 138/1000 | Loss: 0.00001934
Iteration 139/1000 | Loss: 0.00001934
Iteration 140/1000 | Loss: 0.00001934
Iteration 141/1000 | Loss: 0.00001934
Iteration 142/1000 | Loss: 0.00001933
Iteration 143/1000 | Loss: 0.00001932
Iteration 144/1000 | Loss: 0.00001932
Iteration 145/1000 | Loss: 0.00001932
Iteration 146/1000 | Loss: 0.00001931
Iteration 147/1000 | Loss: 0.00001931
Iteration 148/1000 | Loss: 0.00001931
Iteration 149/1000 | Loss: 0.00001931
Iteration 150/1000 | Loss: 0.00001931
Iteration 151/1000 | Loss: 0.00001931
Iteration 152/1000 | Loss: 0.00001931
Iteration 153/1000 | Loss: 0.00001931
Iteration 154/1000 | Loss: 0.00001930
Iteration 155/1000 | Loss: 0.00001930
Iteration 156/1000 | Loss: 0.00001930
Iteration 157/1000 | Loss: 0.00001930
Iteration 158/1000 | Loss: 0.00001930
Iteration 159/1000 | Loss: 0.00001930
Iteration 160/1000 | Loss: 0.00001930
Iteration 161/1000 | Loss: 0.00001930
Iteration 162/1000 | Loss: 0.00001929
Iteration 163/1000 | Loss: 0.00001929
Iteration 164/1000 | Loss: 0.00001929
Iteration 165/1000 | Loss: 0.00001929
Iteration 166/1000 | Loss: 0.00001929
Iteration 167/1000 | Loss: 0.00001928
Iteration 168/1000 | Loss: 0.00001928
Iteration 169/1000 | Loss: 0.00001928
Iteration 170/1000 | Loss: 0.00001928
Iteration 171/1000 | Loss: 0.00001928
Iteration 172/1000 | Loss: 0.00001928
Iteration 173/1000 | Loss: 0.00001928
Iteration 174/1000 | Loss: 0.00001928
Iteration 175/1000 | Loss: 0.00001928
Iteration 176/1000 | Loss: 0.00001928
Iteration 177/1000 | Loss: 0.00001928
Iteration 178/1000 | Loss: 0.00001928
Iteration 179/1000 | Loss: 0.00001927
Iteration 180/1000 | Loss: 0.00001927
Iteration 181/1000 | Loss: 0.00001927
Iteration 182/1000 | Loss: 0.00001927
Iteration 183/1000 | Loss: 0.00001927
Iteration 184/1000 | Loss: 0.00001927
Iteration 185/1000 | Loss: 0.00001927
Iteration 186/1000 | Loss: 0.00001927
Iteration 187/1000 | Loss: 0.00001927
Iteration 188/1000 | Loss: 0.00001927
Iteration 189/1000 | Loss: 0.00001927
Iteration 190/1000 | Loss: 0.00001926
Iteration 191/1000 | Loss: 0.00001926
Iteration 192/1000 | Loss: 0.00001926
Iteration 193/1000 | Loss: 0.00001926
Iteration 194/1000 | Loss: 0.00001926
Iteration 195/1000 | Loss: 0.00001926
Iteration 196/1000 | Loss: 0.00001926
Iteration 197/1000 | Loss: 0.00001926
Iteration 198/1000 | Loss: 0.00001926
Iteration 199/1000 | Loss: 0.00001926
Iteration 200/1000 | Loss: 0.00001926
Iteration 201/1000 | Loss: 0.00001926
Iteration 202/1000 | Loss: 0.00001925
Iteration 203/1000 | Loss: 0.00001925
Iteration 204/1000 | Loss: 0.00001925
Iteration 205/1000 | Loss: 0.00001925
Iteration 206/1000 | Loss: 0.00001925
Iteration 207/1000 | Loss: 0.00001925
Iteration 208/1000 | Loss: 0.00001925
Iteration 209/1000 | Loss: 0.00001924
Iteration 210/1000 | Loss: 0.00001924
Iteration 211/1000 | Loss: 0.00001924
Iteration 212/1000 | Loss: 0.00001924
Iteration 213/1000 | Loss: 0.00001924
Iteration 214/1000 | Loss: 0.00001924
Iteration 215/1000 | Loss: 0.00001924
Iteration 216/1000 | Loss: 0.00001924
Iteration 217/1000 | Loss: 0.00001924
Iteration 218/1000 | Loss: 0.00001924
Iteration 219/1000 | Loss: 0.00001924
Iteration 220/1000 | Loss: 0.00001924
Iteration 221/1000 | Loss: 0.00001924
Iteration 222/1000 | Loss: 0.00001923
Iteration 223/1000 | Loss: 0.00001923
Iteration 224/1000 | Loss: 0.00001923
Iteration 225/1000 | Loss: 0.00001923
Iteration 226/1000 | Loss: 0.00001923
Iteration 227/1000 | Loss: 0.00001923
Iteration 228/1000 | Loss: 0.00001923
Iteration 229/1000 | Loss: 0.00001923
Iteration 230/1000 | Loss: 0.00001923
Iteration 231/1000 | Loss: 0.00001923
Iteration 232/1000 | Loss: 0.00001923
Iteration 233/1000 | Loss: 0.00001923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.923227137012873e-05, 1.923227137012873e-05, 1.923227137012873e-05, 1.923227137012873e-05, 1.923227137012873e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.923227137012873e-05

Optimization complete. Final v2v error: 3.657890558242798 mm

Highest mean error: 4.549323558807373 mm for frame 122

Lowest mean error: 2.957888126373291 mm for frame 145

Saving results

Total time: 44.45941758155823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00514533
Iteration 2/25 | Loss: 0.00143093
Iteration 3/25 | Loss: 0.00132356
Iteration 4/25 | Loss: 0.00130652
Iteration 5/25 | Loss: 0.00130257
Iteration 6/25 | Loss: 0.00130121
Iteration 7/25 | Loss: 0.00129934
Iteration 8/25 | Loss: 0.00130049
Iteration 9/25 | Loss: 0.00129921
Iteration 10/25 | Loss: 0.00129900
Iteration 11/25 | Loss: 0.00129900
Iteration 12/25 | Loss: 0.00129899
Iteration 13/25 | Loss: 0.00129899
Iteration 14/25 | Loss: 0.00129899
Iteration 15/25 | Loss: 0.00129899
Iteration 16/25 | Loss: 0.00129898
Iteration 17/25 | Loss: 0.00129898
Iteration 18/25 | Loss: 0.00129898
Iteration 19/25 | Loss: 0.00129898
Iteration 20/25 | Loss: 0.00129898
Iteration 21/25 | Loss: 0.00129898
Iteration 22/25 | Loss: 0.00129898
Iteration 23/25 | Loss: 0.00129898
Iteration 24/25 | Loss: 0.00129898
Iteration 25/25 | Loss: 0.00129898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.86283970
Iteration 2/25 | Loss: 0.00090076
Iteration 3/25 | Loss: 0.00088240
Iteration 4/25 | Loss: 0.00088240
Iteration 5/25 | Loss: 0.00088240
Iteration 6/25 | Loss: 0.00088240
Iteration 7/25 | Loss: 0.00088240
Iteration 8/25 | Loss: 0.00088240
Iteration 9/25 | Loss: 0.00088240
Iteration 10/25 | Loss: 0.00088240
Iteration 11/25 | Loss: 0.00088240
Iteration 12/25 | Loss: 0.00088240
Iteration 13/25 | Loss: 0.00088239
Iteration 14/25 | Loss: 0.00088239
Iteration 15/25 | Loss: 0.00088239
Iteration 16/25 | Loss: 0.00088239
Iteration 17/25 | Loss: 0.00088239
Iteration 18/25 | Loss: 0.00088239
Iteration 19/25 | Loss: 0.00088239
Iteration 20/25 | Loss: 0.00088239
Iteration 21/25 | Loss: 0.00088239
Iteration 22/25 | Loss: 0.00088239
Iteration 23/25 | Loss: 0.00088239
Iteration 24/25 | Loss: 0.00088239
Iteration 25/25 | Loss: 0.00088239

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088239
Iteration 2/1000 | Loss: 0.00005625
Iteration 3/1000 | Loss: 0.00002625
Iteration 4/1000 | Loss: 0.00007475
Iteration 5/1000 | Loss: 0.00061221
Iteration 6/1000 | Loss: 0.00225557
Iteration 7/1000 | Loss: 0.00015377
Iteration 8/1000 | Loss: 0.00033793
Iteration 9/1000 | Loss: 0.00002713
Iteration 10/1000 | Loss: 0.00012378
Iteration 11/1000 | Loss: 0.00013632
Iteration 12/1000 | Loss: 0.00002960
Iteration 13/1000 | Loss: 0.00001962
Iteration 14/1000 | Loss: 0.00005918
Iteration 15/1000 | Loss: 0.00002320
Iteration 16/1000 | Loss: 0.00001785
Iteration 17/1000 | Loss: 0.00010653
Iteration 18/1000 | Loss: 0.00007039
Iteration 19/1000 | Loss: 0.00007686
Iteration 20/1000 | Loss: 0.00001694
Iteration 21/1000 | Loss: 0.00001665
Iteration 22/1000 | Loss: 0.00001656
Iteration 23/1000 | Loss: 0.00001635
Iteration 24/1000 | Loss: 0.00001627
Iteration 25/1000 | Loss: 0.00001620
Iteration 26/1000 | Loss: 0.00001619
Iteration 27/1000 | Loss: 0.00008230
Iteration 28/1000 | Loss: 0.00001605
Iteration 29/1000 | Loss: 0.00001592
Iteration 30/1000 | Loss: 0.00001591
Iteration 31/1000 | Loss: 0.00001583
Iteration 32/1000 | Loss: 0.00001582
Iteration 33/1000 | Loss: 0.00001581
Iteration 34/1000 | Loss: 0.00001573
Iteration 35/1000 | Loss: 0.00001571
Iteration 36/1000 | Loss: 0.00004308
Iteration 37/1000 | Loss: 0.00009712
Iteration 38/1000 | Loss: 0.00001576
Iteration 39/1000 | Loss: 0.00001553
Iteration 40/1000 | Loss: 0.00001553
Iteration 41/1000 | Loss: 0.00001552
Iteration 42/1000 | Loss: 0.00001551
Iteration 43/1000 | Loss: 0.00001550
Iteration 44/1000 | Loss: 0.00001548
Iteration 45/1000 | Loss: 0.00001548
Iteration 46/1000 | Loss: 0.00001547
Iteration 47/1000 | Loss: 0.00001547
Iteration 48/1000 | Loss: 0.00001547
Iteration 49/1000 | Loss: 0.00001546
Iteration 50/1000 | Loss: 0.00001546
Iteration 51/1000 | Loss: 0.00001545
Iteration 52/1000 | Loss: 0.00001545
Iteration 53/1000 | Loss: 0.00001545
Iteration 54/1000 | Loss: 0.00001543
Iteration 55/1000 | Loss: 0.00001543
Iteration 56/1000 | Loss: 0.00001543
Iteration 57/1000 | Loss: 0.00001543
Iteration 58/1000 | Loss: 0.00001543
Iteration 59/1000 | Loss: 0.00001543
Iteration 60/1000 | Loss: 0.00001542
Iteration 61/1000 | Loss: 0.00001542
Iteration 62/1000 | Loss: 0.00001542
Iteration 63/1000 | Loss: 0.00001542
Iteration 64/1000 | Loss: 0.00001542
Iteration 65/1000 | Loss: 0.00001542
Iteration 66/1000 | Loss: 0.00001542
Iteration 67/1000 | Loss: 0.00001541
Iteration 68/1000 | Loss: 0.00001541
Iteration 69/1000 | Loss: 0.00001541
Iteration 70/1000 | Loss: 0.00001540
Iteration 71/1000 | Loss: 0.00001540
Iteration 72/1000 | Loss: 0.00001538
Iteration 73/1000 | Loss: 0.00001538
Iteration 74/1000 | Loss: 0.00001538
Iteration 75/1000 | Loss: 0.00001538
Iteration 76/1000 | Loss: 0.00001538
Iteration 77/1000 | Loss: 0.00001538
Iteration 78/1000 | Loss: 0.00001538
Iteration 79/1000 | Loss: 0.00001538
Iteration 80/1000 | Loss: 0.00001538
Iteration 81/1000 | Loss: 0.00001537
Iteration 82/1000 | Loss: 0.00001536
Iteration 83/1000 | Loss: 0.00001536
Iteration 84/1000 | Loss: 0.00001536
Iteration 85/1000 | Loss: 0.00001535
Iteration 86/1000 | Loss: 0.00001535
Iteration 87/1000 | Loss: 0.00001535
Iteration 88/1000 | Loss: 0.00001535
Iteration 89/1000 | Loss: 0.00001534
Iteration 90/1000 | Loss: 0.00001534
Iteration 91/1000 | Loss: 0.00001533
Iteration 92/1000 | Loss: 0.00001533
Iteration 93/1000 | Loss: 0.00001533
Iteration 94/1000 | Loss: 0.00001532
Iteration 95/1000 | Loss: 0.00001532
Iteration 96/1000 | Loss: 0.00001532
Iteration 97/1000 | Loss: 0.00001532
Iteration 98/1000 | Loss: 0.00001532
Iteration 99/1000 | Loss: 0.00001531
Iteration 100/1000 | Loss: 0.00001531
Iteration 101/1000 | Loss: 0.00001531
Iteration 102/1000 | Loss: 0.00001530
Iteration 103/1000 | Loss: 0.00001530
Iteration 104/1000 | Loss: 0.00001530
Iteration 105/1000 | Loss: 0.00001530
Iteration 106/1000 | Loss: 0.00001530
Iteration 107/1000 | Loss: 0.00001529
Iteration 108/1000 | Loss: 0.00001529
Iteration 109/1000 | Loss: 0.00001529
Iteration 110/1000 | Loss: 0.00001529
Iteration 111/1000 | Loss: 0.00001528
Iteration 112/1000 | Loss: 0.00001528
Iteration 113/1000 | Loss: 0.00001528
Iteration 114/1000 | Loss: 0.00001528
Iteration 115/1000 | Loss: 0.00001528
Iteration 116/1000 | Loss: 0.00001527
Iteration 117/1000 | Loss: 0.00001527
Iteration 118/1000 | Loss: 0.00001527
Iteration 119/1000 | Loss: 0.00001527
Iteration 120/1000 | Loss: 0.00001527
Iteration 121/1000 | Loss: 0.00001527
Iteration 122/1000 | Loss: 0.00001527
Iteration 123/1000 | Loss: 0.00001526
Iteration 124/1000 | Loss: 0.00001526
Iteration 125/1000 | Loss: 0.00001525
Iteration 126/1000 | Loss: 0.00001525
Iteration 127/1000 | Loss: 0.00001525
Iteration 128/1000 | Loss: 0.00001524
Iteration 129/1000 | Loss: 0.00001524
Iteration 130/1000 | Loss: 0.00001523
Iteration 131/1000 | Loss: 0.00001523
Iteration 132/1000 | Loss: 0.00001522
Iteration 133/1000 | Loss: 0.00001521
Iteration 134/1000 | Loss: 0.00001521
Iteration 135/1000 | Loss: 0.00001521
Iteration 136/1000 | Loss: 0.00001521
Iteration 137/1000 | Loss: 0.00001521
Iteration 138/1000 | Loss: 0.00001521
Iteration 139/1000 | Loss: 0.00001521
Iteration 140/1000 | Loss: 0.00001520
Iteration 141/1000 | Loss: 0.00001520
Iteration 142/1000 | Loss: 0.00001520
Iteration 143/1000 | Loss: 0.00001519
Iteration 144/1000 | Loss: 0.00001519
Iteration 145/1000 | Loss: 0.00001519
Iteration 146/1000 | Loss: 0.00001519
Iteration 147/1000 | Loss: 0.00001519
Iteration 148/1000 | Loss: 0.00001518
Iteration 149/1000 | Loss: 0.00001518
Iteration 150/1000 | Loss: 0.00001518
Iteration 151/1000 | Loss: 0.00001518
Iteration 152/1000 | Loss: 0.00001517
Iteration 153/1000 | Loss: 0.00001517
Iteration 154/1000 | Loss: 0.00001517
Iteration 155/1000 | Loss: 0.00001517
Iteration 156/1000 | Loss: 0.00001517
Iteration 157/1000 | Loss: 0.00001517
Iteration 158/1000 | Loss: 0.00001517
Iteration 159/1000 | Loss: 0.00001517
Iteration 160/1000 | Loss: 0.00001517
Iteration 161/1000 | Loss: 0.00001517
Iteration 162/1000 | Loss: 0.00001517
Iteration 163/1000 | Loss: 0.00001517
Iteration 164/1000 | Loss: 0.00001517
Iteration 165/1000 | Loss: 0.00001516
Iteration 166/1000 | Loss: 0.00001516
Iteration 167/1000 | Loss: 0.00001516
Iteration 168/1000 | Loss: 0.00001516
Iteration 169/1000 | Loss: 0.00001516
Iteration 170/1000 | Loss: 0.00001516
Iteration 171/1000 | Loss: 0.00001515
Iteration 172/1000 | Loss: 0.00001515
Iteration 173/1000 | Loss: 0.00001515
Iteration 174/1000 | Loss: 0.00001515
Iteration 175/1000 | Loss: 0.00001515
Iteration 176/1000 | Loss: 0.00001515
Iteration 177/1000 | Loss: 0.00001515
Iteration 178/1000 | Loss: 0.00001515
Iteration 179/1000 | Loss: 0.00001515
Iteration 180/1000 | Loss: 0.00001515
Iteration 181/1000 | Loss: 0.00001515
Iteration 182/1000 | Loss: 0.00001515
Iteration 183/1000 | Loss: 0.00001515
Iteration 184/1000 | Loss: 0.00001515
Iteration 185/1000 | Loss: 0.00001515
Iteration 186/1000 | Loss: 0.00001514
Iteration 187/1000 | Loss: 0.00001514
Iteration 188/1000 | Loss: 0.00001514
Iteration 189/1000 | Loss: 0.00001514
Iteration 190/1000 | Loss: 0.00001514
Iteration 191/1000 | Loss: 0.00001514
Iteration 192/1000 | Loss: 0.00001514
Iteration 193/1000 | Loss: 0.00001514
Iteration 194/1000 | Loss: 0.00001514
Iteration 195/1000 | Loss: 0.00001514
Iteration 196/1000 | Loss: 0.00001514
Iteration 197/1000 | Loss: 0.00001514
Iteration 198/1000 | Loss: 0.00001514
Iteration 199/1000 | Loss: 0.00001514
Iteration 200/1000 | Loss: 0.00001513
Iteration 201/1000 | Loss: 0.00001513
Iteration 202/1000 | Loss: 0.00001513
Iteration 203/1000 | Loss: 0.00001513
Iteration 204/1000 | Loss: 0.00001513
Iteration 205/1000 | Loss: 0.00001513
Iteration 206/1000 | Loss: 0.00001513
Iteration 207/1000 | Loss: 0.00001513
Iteration 208/1000 | Loss: 0.00001513
Iteration 209/1000 | Loss: 0.00001513
Iteration 210/1000 | Loss: 0.00001513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.5134554814721923e-05, 1.5134554814721923e-05, 1.5134554814721923e-05, 1.5134554814721923e-05, 1.5134554814721923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5134554814721923e-05

Optimization complete. Final v2v error: 3.279775619506836 mm

Highest mean error: 4.035242080688477 mm for frame 200

Lowest mean error: 3.0085442066192627 mm for frame 215

Saving results

Total time: 85.11409497261047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778259
Iteration 2/25 | Loss: 0.00147050
Iteration 3/25 | Loss: 0.00137808
Iteration 4/25 | Loss: 0.00136240
Iteration 5/25 | Loss: 0.00135870
Iteration 6/25 | Loss: 0.00135803
Iteration 7/25 | Loss: 0.00135803
Iteration 8/25 | Loss: 0.00135803
Iteration 9/25 | Loss: 0.00135803
Iteration 10/25 | Loss: 0.00135803
Iteration 11/25 | Loss: 0.00135803
Iteration 12/25 | Loss: 0.00135803
Iteration 13/25 | Loss: 0.00135803
Iteration 14/25 | Loss: 0.00135803
Iteration 15/25 | Loss: 0.00135803
Iteration 16/25 | Loss: 0.00135803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001358028850518167, 0.001358028850518167, 0.001358028850518167, 0.001358028850518167, 0.001358028850518167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001358028850518167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37788367
Iteration 2/25 | Loss: 0.00105180
Iteration 3/25 | Loss: 0.00105179
Iteration 4/25 | Loss: 0.00105179
Iteration 5/25 | Loss: 0.00105179
Iteration 6/25 | Loss: 0.00105179
Iteration 7/25 | Loss: 0.00105179
Iteration 8/25 | Loss: 0.00105179
Iteration 9/25 | Loss: 0.00105179
Iteration 10/25 | Loss: 0.00105179
Iteration 11/25 | Loss: 0.00105179
Iteration 12/25 | Loss: 0.00105179
Iteration 13/25 | Loss: 0.00105179
Iteration 14/25 | Loss: 0.00105179
Iteration 15/25 | Loss: 0.00105179
Iteration 16/25 | Loss: 0.00105179
Iteration 17/25 | Loss: 0.00105179
Iteration 18/25 | Loss: 0.00105179
Iteration 19/25 | Loss: 0.00105179
Iteration 20/25 | Loss: 0.00105179
Iteration 21/25 | Loss: 0.00105179
Iteration 22/25 | Loss: 0.00105179
Iteration 23/25 | Loss: 0.00105179
Iteration 24/25 | Loss: 0.00105179
Iteration 25/25 | Loss: 0.00105179

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105179
Iteration 2/1000 | Loss: 0.00006016
Iteration 3/1000 | Loss: 0.00004040
Iteration 4/1000 | Loss: 0.00003237
Iteration 5/1000 | Loss: 0.00002951
Iteration 6/1000 | Loss: 0.00002813
Iteration 7/1000 | Loss: 0.00002721
Iteration 8/1000 | Loss: 0.00002635
Iteration 9/1000 | Loss: 0.00002595
Iteration 10/1000 | Loss: 0.00002564
Iteration 11/1000 | Loss: 0.00002531
Iteration 12/1000 | Loss: 0.00002515
Iteration 13/1000 | Loss: 0.00002515
Iteration 14/1000 | Loss: 0.00002507
Iteration 15/1000 | Loss: 0.00002501
Iteration 16/1000 | Loss: 0.00002500
Iteration 17/1000 | Loss: 0.00002496
Iteration 18/1000 | Loss: 0.00002492
Iteration 19/1000 | Loss: 0.00002491
Iteration 20/1000 | Loss: 0.00002490
Iteration 21/1000 | Loss: 0.00002488
Iteration 22/1000 | Loss: 0.00002483
Iteration 23/1000 | Loss: 0.00002483
Iteration 24/1000 | Loss: 0.00002482
Iteration 25/1000 | Loss: 0.00002479
Iteration 26/1000 | Loss: 0.00002475
Iteration 27/1000 | Loss: 0.00002474
Iteration 28/1000 | Loss: 0.00002473
Iteration 29/1000 | Loss: 0.00002473
Iteration 30/1000 | Loss: 0.00002472
Iteration 31/1000 | Loss: 0.00002472
Iteration 32/1000 | Loss: 0.00002471
Iteration 33/1000 | Loss: 0.00002466
Iteration 34/1000 | Loss: 0.00002466
Iteration 35/1000 | Loss: 0.00002463
Iteration 36/1000 | Loss: 0.00002462
Iteration 37/1000 | Loss: 0.00002462
Iteration 38/1000 | Loss: 0.00002461
Iteration 39/1000 | Loss: 0.00002461
Iteration 40/1000 | Loss: 0.00002460
Iteration 41/1000 | Loss: 0.00002460
Iteration 42/1000 | Loss: 0.00002459
Iteration 43/1000 | Loss: 0.00002459
Iteration 44/1000 | Loss: 0.00002457
Iteration 45/1000 | Loss: 0.00002456
Iteration 46/1000 | Loss: 0.00002455
Iteration 47/1000 | Loss: 0.00002455
Iteration 48/1000 | Loss: 0.00002455
Iteration 49/1000 | Loss: 0.00002455
Iteration 50/1000 | Loss: 0.00002455
Iteration 51/1000 | Loss: 0.00002455
Iteration 52/1000 | Loss: 0.00002455
Iteration 53/1000 | Loss: 0.00002455
Iteration 54/1000 | Loss: 0.00002454
Iteration 55/1000 | Loss: 0.00002454
Iteration 56/1000 | Loss: 0.00002454
Iteration 57/1000 | Loss: 0.00002454
Iteration 58/1000 | Loss: 0.00002454
Iteration 59/1000 | Loss: 0.00002454
Iteration 60/1000 | Loss: 0.00002454
Iteration 61/1000 | Loss: 0.00002453
Iteration 62/1000 | Loss: 0.00002453
Iteration 63/1000 | Loss: 0.00002453
Iteration 64/1000 | Loss: 0.00002452
Iteration 65/1000 | Loss: 0.00002452
Iteration 66/1000 | Loss: 0.00002451
Iteration 67/1000 | Loss: 0.00002451
Iteration 68/1000 | Loss: 0.00002451
Iteration 69/1000 | Loss: 0.00002450
Iteration 70/1000 | Loss: 0.00002450
Iteration 71/1000 | Loss: 0.00002450
Iteration 72/1000 | Loss: 0.00002449
Iteration 73/1000 | Loss: 0.00002449
Iteration 74/1000 | Loss: 0.00002449
Iteration 75/1000 | Loss: 0.00002449
Iteration 76/1000 | Loss: 0.00002447
Iteration 77/1000 | Loss: 0.00002447
Iteration 78/1000 | Loss: 0.00002447
Iteration 79/1000 | Loss: 0.00002447
Iteration 80/1000 | Loss: 0.00002447
Iteration 81/1000 | Loss: 0.00002447
Iteration 82/1000 | Loss: 0.00002446
Iteration 83/1000 | Loss: 0.00002446
Iteration 84/1000 | Loss: 0.00002446
Iteration 85/1000 | Loss: 0.00002446
Iteration 86/1000 | Loss: 0.00002446
Iteration 87/1000 | Loss: 0.00002446
Iteration 88/1000 | Loss: 0.00002446
Iteration 89/1000 | Loss: 0.00002446
Iteration 90/1000 | Loss: 0.00002446
Iteration 91/1000 | Loss: 0.00002446
Iteration 92/1000 | Loss: 0.00002446
Iteration 93/1000 | Loss: 0.00002446
Iteration 94/1000 | Loss: 0.00002446
Iteration 95/1000 | Loss: 0.00002446
Iteration 96/1000 | Loss: 0.00002446
Iteration 97/1000 | Loss: 0.00002446
Iteration 98/1000 | Loss: 0.00002446
Iteration 99/1000 | Loss: 0.00002446
Iteration 100/1000 | Loss: 0.00002446
Iteration 101/1000 | Loss: 0.00002446
Iteration 102/1000 | Loss: 0.00002446
Iteration 103/1000 | Loss: 0.00002446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.4461125576635823e-05, 2.4461125576635823e-05, 2.4461125576635823e-05, 2.4461125576635823e-05, 2.4461125576635823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4461125576635823e-05

Optimization complete. Final v2v error: 4.0859479904174805 mm

Highest mean error: 4.995631694793701 mm for frame 95

Lowest mean error: 3.305703639984131 mm for frame 2

Saving results

Total time: 35.677541732788086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948412
Iteration 2/25 | Loss: 0.00948411
Iteration 3/25 | Loss: 0.00244973
Iteration 4/25 | Loss: 0.00169944
Iteration 5/25 | Loss: 0.00162339
Iteration 6/25 | Loss: 0.00163808
Iteration 7/25 | Loss: 0.00160773
Iteration 8/25 | Loss: 0.00148790
Iteration 9/25 | Loss: 0.00146515
Iteration 10/25 | Loss: 0.00143571
Iteration 11/25 | Loss: 0.00142485
Iteration 12/25 | Loss: 0.00142898
Iteration 13/25 | Loss: 0.00140278
Iteration 14/25 | Loss: 0.00139707
Iteration 15/25 | Loss: 0.00141384
Iteration 16/25 | Loss: 0.00138948
Iteration 17/25 | Loss: 0.00138772
Iteration 18/25 | Loss: 0.00138243
Iteration 19/25 | Loss: 0.00137649
Iteration 20/25 | Loss: 0.00138188
Iteration 21/25 | Loss: 0.00137734
Iteration 22/25 | Loss: 0.00137664
Iteration 23/25 | Loss: 0.00138197
Iteration 24/25 | Loss: 0.00137544
Iteration 25/25 | Loss: 0.00137649

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38782537
Iteration 2/25 | Loss: 0.00124679
Iteration 3/25 | Loss: 0.00096911
Iteration 4/25 | Loss: 0.00096909
Iteration 5/25 | Loss: 0.00096909
Iteration 6/25 | Loss: 0.00096908
Iteration 7/25 | Loss: 0.00096908
Iteration 8/25 | Loss: 0.00096908
Iteration 9/25 | Loss: 0.00096908
Iteration 10/25 | Loss: 0.00096908
Iteration 11/25 | Loss: 0.00096908
Iteration 12/25 | Loss: 0.00096908
Iteration 13/25 | Loss: 0.00096908
Iteration 14/25 | Loss: 0.00096908
Iteration 15/25 | Loss: 0.00096908
Iteration 16/25 | Loss: 0.00096908
Iteration 17/25 | Loss: 0.00096908
Iteration 18/25 | Loss: 0.00096908
Iteration 19/25 | Loss: 0.00096908
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009690821170806885, 0.0009690821170806885, 0.0009690821170806885, 0.0009690821170806885, 0.0009690821170806885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009690821170806885

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096908
Iteration 2/1000 | Loss: 0.00031472
Iteration 3/1000 | Loss: 0.00039156
Iteration 4/1000 | Loss: 0.00003486
Iteration 5/1000 | Loss: 0.00003038
Iteration 6/1000 | Loss: 0.00002802
Iteration 7/1000 | Loss: 0.00039096
Iteration 8/1000 | Loss: 0.00004084
Iteration 9/1000 | Loss: 0.00003168
Iteration 10/1000 | Loss: 0.00002639
Iteration 11/1000 | Loss: 0.00002538
Iteration 12/1000 | Loss: 0.00002479
Iteration 13/1000 | Loss: 0.00003955
Iteration 14/1000 | Loss: 0.00016297
Iteration 15/1000 | Loss: 0.00041991
Iteration 16/1000 | Loss: 0.00006709
Iteration 17/1000 | Loss: 0.00017547
Iteration 18/1000 | Loss: 0.00021631
Iteration 19/1000 | Loss: 0.00014539
Iteration 20/1000 | Loss: 0.00014448
Iteration 21/1000 | Loss: 0.00018617
Iteration 22/1000 | Loss: 0.00015180
Iteration 23/1000 | Loss: 0.00003630
Iteration 24/1000 | Loss: 0.00002944
Iteration 25/1000 | Loss: 0.00014252
Iteration 26/1000 | Loss: 0.00009431
Iteration 27/1000 | Loss: 0.00014035
Iteration 28/1000 | Loss: 0.00013217
Iteration 29/1000 | Loss: 0.00017704
Iteration 30/1000 | Loss: 0.00002538
Iteration 31/1000 | Loss: 0.00002371
Iteration 32/1000 | Loss: 0.00002329
Iteration 33/1000 | Loss: 0.00002273
Iteration 34/1000 | Loss: 0.00002247
Iteration 35/1000 | Loss: 0.00002224
Iteration 36/1000 | Loss: 0.00002201
Iteration 37/1000 | Loss: 0.00008130
Iteration 38/1000 | Loss: 0.00002190
Iteration 39/1000 | Loss: 0.00002163
Iteration 40/1000 | Loss: 0.00002162
Iteration 41/1000 | Loss: 0.00002161
Iteration 42/1000 | Loss: 0.00002161
Iteration 43/1000 | Loss: 0.00002144
Iteration 44/1000 | Loss: 0.00002140
Iteration 45/1000 | Loss: 0.00078839
Iteration 46/1000 | Loss: 0.00037738
Iteration 47/1000 | Loss: 0.00082026
Iteration 48/1000 | Loss: 0.00027682
Iteration 49/1000 | Loss: 0.00003129
Iteration 50/1000 | Loss: 0.00002670
Iteration 51/1000 | Loss: 0.00002467
Iteration 52/1000 | Loss: 0.00002314
Iteration 53/1000 | Loss: 0.00002239
Iteration 54/1000 | Loss: 0.00002186
Iteration 55/1000 | Loss: 0.00002163
Iteration 56/1000 | Loss: 0.00002148
Iteration 57/1000 | Loss: 0.00002145
Iteration 58/1000 | Loss: 0.00002144
Iteration 59/1000 | Loss: 0.00002141
Iteration 60/1000 | Loss: 0.00002136
Iteration 61/1000 | Loss: 0.00002126
Iteration 62/1000 | Loss: 0.00002126
Iteration 63/1000 | Loss: 0.00002122
Iteration 64/1000 | Loss: 0.00002121
Iteration 65/1000 | Loss: 0.00086159
Iteration 66/1000 | Loss: 0.00019716
Iteration 67/1000 | Loss: 0.00066465
Iteration 68/1000 | Loss: 0.00003225
Iteration 69/1000 | Loss: 0.00014228
Iteration 70/1000 | Loss: 0.00005523
Iteration 71/1000 | Loss: 0.00006693
Iteration 72/1000 | Loss: 0.00002528
Iteration 73/1000 | Loss: 0.00002814
Iteration 74/1000 | Loss: 0.00002317
Iteration 75/1000 | Loss: 0.00002235
Iteration 76/1000 | Loss: 0.00002193
Iteration 77/1000 | Loss: 0.00002175
Iteration 78/1000 | Loss: 0.00002174
Iteration 79/1000 | Loss: 0.00002173
Iteration 80/1000 | Loss: 0.00002155
Iteration 81/1000 | Loss: 0.00008625
Iteration 82/1000 | Loss: 0.00002184
Iteration 83/1000 | Loss: 0.00002128
Iteration 84/1000 | Loss: 0.00002127
Iteration 85/1000 | Loss: 0.00002127
Iteration 86/1000 | Loss: 0.00002127
Iteration 87/1000 | Loss: 0.00002127
Iteration 88/1000 | Loss: 0.00002127
Iteration 89/1000 | Loss: 0.00002127
Iteration 90/1000 | Loss: 0.00002127
Iteration 91/1000 | Loss: 0.00002127
Iteration 92/1000 | Loss: 0.00002127
Iteration 93/1000 | Loss: 0.00002127
Iteration 94/1000 | Loss: 0.00002126
Iteration 95/1000 | Loss: 0.00002126
Iteration 96/1000 | Loss: 0.00002126
Iteration 97/1000 | Loss: 0.00002126
Iteration 98/1000 | Loss: 0.00002126
Iteration 99/1000 | Loss: 0.00002125
Iteration 100/1000 | Loss: 0.00002125
Iteration 101/1000 | Loss: 0.00002124
Iteration 102/1000 | Loss: 0.00002124
Iteration 103/1000 | Loss: 0.00002124
Iteration 104/1000 | Loss: 0.00002123
Iteration 105/1000 | Loss: 0.00002123
Iteration 106/1000 | Loss: 0.00002122
Iteration 107/1000 | Loss: 0.00002120
Iteration 108/1000 | Loss: 0.00002119
Iteration 109/1000 | Loss: 0.00002114
Iteration 110/1000 | Loss: 0.00002113
Iteration 111/1000 | Loss: 0.00002112
Iteration 112/1000 | Loss: 0.00009000
Iteration 113/1000 | Loss: 0.00002121
Iteration 114/1000 | Loss: 0.00003919
Iteration 115/1000 | Loss: 0.00049401
Iteration 116/1000 | Loss: 0.00050895
Iteration 117/1000 | Loss: 0.00002586
Iteration 118/1000 | Loss: 0.00002148
Iteration 119/1000 | Loss: 0.00008104
Iteration 120/1000 | Loss: 0.00002038
Iteration 121/1000 | Loss: 0.00001975
Iteration 122/1000 | Loss: 0.00006876
Iteration 123/1000 | Loss: 0.00002199
Iteration 124/1000 | Loss: 0.00007092
Iteration 125/1000 | Loss: 0.00001893
Iteration 126/1000 | Loss: 0.00001892
Iteration 127/1000 | Loss: 0.00003537
Iteration 128/1000 | Loss: 0.00002455
Iteration 129/1000 | Loss: 0.00010174
Iteration 130/1000 | Loss: 0.00001878
Iteration 131/1000 | Loss: 0.00001877
Iteration 132/1000 | Loss: 0.00001877
Iteration 133/1000 | Loss: 0.00001877
Iteration 134/1000 | Loss: 0.00001877
Iteration 135/1000 | Loss: 0.00001868
Iteration 136/1000 | Loss: 0.00001866
Iteration 137/1000 | Loss: 0.00001864
Iteration 138/1000 | Loss: 0.00001863
Iteration 139/1000 | Loss: 0.00001846
Iteration 140/1000 | Loss: 0.00001843
Iteration 141/1000 | Loss: 0.00001842
Iteration 142/1000 | Loss: 0.00001841
Iteration 143/1000 | Loss: 0.00001841
Iteration 144/1000 | Loss: 0.00001840
Iteration 145/1000 | Loss: 0.00001840
Iteration 146/1000 | Loss: 0.00001840
Iteration 147/1000 | Loss: 0.00001834
Iteration 148/1000 | Loss: 0.00001834
Iteration 149/1000 | Loss: 0.00001834
Iteration 150/1000 | Loss: 0.00001834
Iteration 151/1000 | Loss: 0.00001833
Iteration 152/1000 | Loss: 0.00001833
Iteration 153/1000 | Loss: 0.00001829
Iteration 154/1000 | Loss: 0.00001828
Iteration 155/1000 | Loss: 0.00001827
Iteration 156/1000 | Loss: 0.00001824
Iteration 157/1000 | Loss: 0.00001822
Iteration 158/1000 | Loss: 0.00001821
Iteration 159/1000 | Loss: 0.00001821
Iteration 160/1000 | Loss: 0.00001820
Iteration 161/1000 | Loss: 0.00001819
Iteration 162/1000 | Loss: 0.00001819
Iteration 163/1000 | Loss: 0.00001818
Iteration 164/1000 | Loss: 0.00001818
Iteration 165/1000 | Loss: 0.00001818
Iteration 166/1000 | Loss: 0.00001817
Iteration 167/1000 | Loss: 0.00001817
Iteration 168/1000 | Loss: 0.00001816
Iteration 169/1000 | Loss: 0.00001816
Iteration 170/1000 | Loss: 0.00001816
Iteration 171/1000 | Loss: 0.00001816
Iteration 172/1000 | Loss: 0.00001816
Iteration 173/1000 | Loss: 0.00001816
Iteration 174/1000 | Loss: 0.00001815
Iteration 175/1000 | Loss: 0.00001815
Iteration 176/1000 | Loss: 0.00001815
Iteration 177/1000 | Loss: 0.00001815
Iteration 178/1000 | Loss: 0.00001815
Iteration 179/1000 | Loss: 0.00001815
Iteration 180/1000 | Loss: 0.00001815
Iteration 181/1000 | Loss: 0.00001815
Iteration 182/1000 | Loss: 0.00001815
Iteration 183/1000 | Loss: 0.00001814
Iteration 184/1000 | Loss: 0.00001814
Iteration 185/1000 | Loss: 0.00001814
Iteration 186/1000 | Loss: 0.00001814
Iteration 187/1000 | Loss: 0.00001814
Iteration 188/1000 | Loss: 0.00001814
Iteration 189/1000 | Loss: 0.00001814
Iteration 190/1000 | Loss: 0.00001814
Iteration 191/1000 | Loss: 0.00001814
Iteration 192/1000 | Loss: 0.00001814
Iteration 193/1000 | Loss: 0.00010041
Iteration 194/1000 | Loss: 0.00002045
Iteration 195/1000 | Loss: 0.00001899
Iteration 196/1000 | Loss: 0.00001838
Iteration 197/1000 | Loss: 0.00001815
Iteration 198/1000 | Loss: 0.00001814
Iteration 199/1000 | Loss: 0.00001814
Iteration 200/1000 | Loss: 0.00001814
Iteration 201/1000 | Loss: 0.00001814
Iteration 202/1000 | Loss: 0.00001814
Iteration 203/1000 | Loss: 0.00001814
Iteration 204/1000 | Loss: 0.00001814
Iteration 205/1000 | Loss: 0.00001814
Iteration 206/1000 | Loss: 0.00001814
Iteration 207/1000 | Loss: 0.00001813
Iteration 208/1000 | Loss: 0.00001813
Iteration 209/1000 | Loss: 0.00001813
Iteration 210/1000 | Loss: 0.00001813
Iteration 211/1000 | Loss: 0.00001812
Iteration 212/1000 | Loss: 0.00001812
Iteration 213/1000 | Loss: 0.00001812
Iteration 214/1000 | Loss: 0.00001812
Iteration 215/1000 | Loss: 0.00001812
Iteration 216/1000 | Loss: 0.00001812
Iteration 217/1000 | Loss: 0.00001812
Iteration 218/1000 | Loss: 0.00001812
Iteration 219/1000 | Loss: 0.00001812
Iteration 220/1000 | Loss: 0.00001812
Iteration 221/1000 | Loss: 0.00001811
Iteration 222/1000 | Loss: 0.00001811
Iteration 223/1000 | Loss: 0.00001811
Iteration 224/1000 | Loss: 0.00001811
Iteration 225/1000 | Loss: 0.00001811
Iteration 226/1000 | Loss: 0.00001811
Iteration 227/1000 | Loss: 0.00001811
Iteration 228/1000 | Loss: 0.00001811
Iteration 229/1000 | Loss: 0.00001811
Iteration 230/1000 | Loss: 0.00001810
Iteration 231/1000 | Loss: 0.00001810
Iteration 232/1000 | Loss: 0.00001810
Iteration 233/1000 | Loss: 0.00001810
Iteration 234/1000 | Loss: 0.00001810
Iteration 235/1000 | Loss: 0.00001810
Iteration 236/1000 | Loss: 0.00001810
Iteration 237/1000 | Loss: 0.00001810
Iteration 238/1000 | Loss: 0.00001810
Iteration 239/1000 | Loss: 0.00001810
Iteration 240/1000 | Loss: 0.00001809
Iteration 241/1000 | Loss: 0.00001809
Iteration 242/1000 | Loss: 0.00001809
Iteration 243/1000 | Loss: 0.00001809
Iteration 244/1000 | Loss: 0.00001809
Iteration 245/1000 | Loss: 0.00001809
Iteration 246/1000 | Loss: 0.00001809
Iteration 247/1000 | Loss: 0.00001809
Iteration 248/1000 | Loss: 0.00001809
Iteration 249/1000 | Loss: 0.00001809
Iteration 250/1000 | Loss: 0.00001809
Iteration 251/1000 | Loss: 0.00001809
Iteration 252/1000 | Loss: 0.00001809
Iteration 253/1000 | Loss: 0.00001809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.8094186089001596e-05, 1.8094186089001596e-05, 1.8094186089001596e-05, 1.8094186089001596e-05, 1.8094186089001596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8094186089001596e-05

Optimization complete. Final v2v error: 3.541638135910034 mm

Highest mean error: 4.913139820098877 mm for frame 27

Lowest mean error: 3.0705466270446777 mm for frame 242

Saving results

Total time: 218.52192282676697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493773
Iteration 2/25 | Loss: 0.00139855
Iteration 3/25 | Loss: 0.00133198
Iteration 4/25 | Loss: 0.00131702
Iteration 5/25 | Loss: 0.00131412
Iteration 6/25 | Loss: 0.00131412
Iteration 7/25 | Loss: 0.00131412
Iteration 8/25 | Loss: 0.00131412
Iteration 9/25 | Loss: 0.00131412
Iteration 10/25 | Loss: 0.00131412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013141173403710127, 0.0013141173403710127, 0.0013141173403710127, 0.0013141173403710127, 0.0013141173403710127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013141173403710127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.64958692
Iteration 2/25 | Loss: 0.00093566
Iteration 3/25 | Loss: 0.00093565
Iteration 4/25 | Loss: 0.00093565
Iteration 5/25 | Loss: 0.00093565
Iteration 6/25 | Loss: 0.00093565
Iteration 7/25 | Loss: 0.00093565
Iteration 8/25 | Loss: 0.00093564
Iteration 9/25 | Loss: 0.00093564
Iteration 10/25 | Loss: 0.00093564
Iteration 11/25 | Loss: 0.00093564
Iteration 12/25 | Loss: 0.00093564
Iteration 13/25 | Loss: 0.00093564
Iteration 14/25 | Loss: 0.00093564
Iteration 15/25 | Loss: 0.00093564
Iteration 16/25 | Loss: 0.00093564
Iteration 17/25 | Loss: 0.00093564
Iteration 18/25 | Loss: 0.00093564
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009356437949463725, 0.0009356437949463725, 0.0009356437949463725, 0.0009356437949463725, 0.0009356437949463725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009356437949463725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093564
Iteration 2/1000 | Loss: 0.00003557
Iteration 3/1000 | Loss: 0.00002381
Iteration 4/1000 | Loss: 0.00002143
Iteration 5/1000 | Loss: 0.00001995
Iteration 6/1000 | Loss: 0.00001904
Iteration 7/1000 | Loss: 0.00001846
Iteration 8/1000 | Loss: 0.00001796
Iteration 9/1000 | Loss: 0.00001751
Iteration 10/1000 | Loss: 0.00001723
Iteration 11/1000 | Loss: 0.00001695
Iteration 12/1000 | Loss: 0.00001678
Iteration 13/1000 | Loss: 0.00001672
Iteration 14/1000 | Loss: 0.00001670
Iteration 15/1000 | Loss: 0.00001657
Iteration 16/1000 | Loss: 0.00001654
Iteration 17/1000 | Loss: 0.00001652
Iteration 18/1000 | Loss: 0.00001651
Iteration 19/1000 | Loss: 0.00001651
Iteration 20/1000 | Loss: 0.00001649
Iteration 21/1000 | Loss: 0.00001648
Iteration 22/1000 | Loss: 0.00001648
Iteration 23/1000 | Loss: 0.00001648
Iteration 24/1000 | Loss: 0.00001644
Iteration 25/1000 | Loss: 0.00001644
Iteration 26/1000 | Loss: 0.00001642
Iteration 27/1000 | Loss: 0.00001641
Iteration 28/1000 | Loss: 0.00001641
Iteration 29/1000 | Loss: 0.00001640
Iteration 30/1000 | Loss: 0.00001639
Iteration 31/1000 | Loss: 0.00001639
Iteration 32/1000 | Loss: 0.00001639
Iteration 33/1000 | Loss: 0.00001637
Iteration 34/1000 | Loss: 0.00001636
Iteration 35/1000 | Loss: 0.00001636
Iteration 36/1000 | Loss: 0.00001635
Iteration 37/1000 | Loss: 0.00001635
Iteration 38/1000 | Loss: 0.00001634
Iteration 39/1000 | Loss: 0.00001634
Iteration 40/1000 | Loss: 0.00001634
Iteration 41/1000 | Loss: 0.00001634
Iteration 42/1000 | Loss: 0.00001633
Iteration 43/1000 | Loss: 0.00001633
Iteration 44/1000 | Loss: 0.00001633
Iteration 45/1000 | Loss: 0.00001632
Iteration 46/1000 | Loss: 0.00001632
Iteration 47/1000 | Loss: 0.00001632
Iteration 48/1000 | Loss: 0.00001631
Iteration 49/1000 | Loss: 0.00001631
Iteration 50/1000 | Loss: 0.00001631
Iteration 51/1000 | Loss: 0.00001630
Iteration 52/1000 | Loss: 0.00001630
Iteration 53/1000 | Loss: 0.00001630
Iteration 54/1000 | Loss: 0.00001630
Iteration 55/1000 | Loss: 0.00001630
Iteration 56/1000 | Loss: 0.00001629
Iteration 57/1000 | Loss: 0.00001629
Iteration 58/1000 | Loss: 0.00001628
Iteration 59/1000 | Loss: 0.00001627
Iteration 60/1000 | Loss: 0.00001627
Iteration 61/1000 | Loss: 0.00001627
Iteration 62/1000 | Loss: 0.00001627
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001626
Iteration 65/1000 | Loss: 0.00001626
Iteration 66/1000 | Loss: 0.00001625
Iteration 67/1000 | Loss: 0.00001625
Iteration 68/1000 | Loss: 0.00001624
Iteration 69/1000 | Loss: 0.00001624
Iteration 70/1000 | Loss: 0.00001624
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001623
Iteration 73/1000 | Loss: 0.00001623
Iteration 74/1000 | Loss: 0.00001622
Iteration 75/1000 | Loss: 0.00001622
Iteration 76/1000 | Loss: 0.00001622
Iteration 77/1000 | Loss: 0.00001622
Iteration 78/1000 | Loss: 0.00001621
Iteration 79/1000 | Loss: 0.00001621
Iteration 80/1000 | Loss: 0.00001620
Iteration 81/1000 | Loss: 0.00001620
Iteration 82/1000 | Loss: 0.00001620
Iteration 83/1000 | Loss: 0.00001620
Iteration 84/1000 | Loss: 0.00001620
Iteration 85/1000 | Loss: 0.00001620
Iteration 86/1000 | Loss: 0.00001619
Iteration 87/1000 | Loss: 0.00001619
Iteration 88/1000 | Loss: 0.00001619
Iteration 89/1000 | Loss: 0.00001618
Iteration 90/1000 | Loss: 0.00001618
Iteration 91/1000 | Loss: 0.00001617
Iteration 92/1000 | Loss: 0.00001616
Iteration 93/1000 | Loss: 0.00001616
Iteration 94/1000 | Loss: 0.00001616
Iteration 95/1000 | Loss: 0.00001616
Iteration 96/1000 | Loss: 0.00001616
Iteration 97/1000 | Loss: 0.00001615
Iteration 98/1000 | Loss: 0.00001615
Iteration 99/1000 | Loss: 0.00001615
Iteration 100/1000 | Loss: 0.00001615
Iteration 101/1000 | Loss: 0.00001614
Iteration 102/1000 | Loss: 0.00001614
Iteration 103/1000 | Loss: 0.00001614
Iteration 104/1000 | Loss: 0.00001613
Iteration 105/1000 | Loss: 0.00001613
Iteration 106/1000 | Loss: 0.00001613
Iteration 107/1000 | Loss: 0.00001613
Iteration 108/1000 | Loss: 0.00001613
Iteration 109/1000 | Loss: 0.00001613
Iteration 110/1000 | Loss: 0.00001613
Iteration 111/1000 | Loss: 0.00001613
Iteration 112/1000 | Loss: 0.00001613
Iteration 113/1000 | Loss: 0.00001613
Iteration 114/1000 | Loss: 0.00001612
Iteration 115/1000 | Loss: 0.00001612
Iteration 116/1000 | Loss: 0.00001612
Iteration 117/1000 | Loss: 0.00001612
Iteration 118/1000 | Loss: 0.00001612
Iteration 119/1000 | Loss: 0.00001612
Iteration 120/1000 | Loss: 0.00001612
Iteration 121/1000 | Loss: 0.00001612
Iteration 122/1000 | Loss: 0.00001612
Iteration 123/1000 | Loss: 0.00001611
Iteration 124/1000 | Loss: 0.00001611
Iteration 125/1000 | Loss: 0.00001611
Iteration 126/1000 | Loss: 0.00001611
Iteration 127/1000 | Loss: 0.00001611
Iteration 128/1000 | Loss: 0.00001611
Iteration 129/1000 | Loss: 0.00001611
Iteration 130/1000 | Loss: 0.00001611
Iteration 131/1000 | Loss: 0.00001611
Iteration 132/1000 | Loss: 0.00001611
Iteration 133/1000 | Loss: 0.00001611
Iteration 134/1000 | Loss: 0.00001611
Iteration 135/1000 | Loss: 0.00001611
Iteration 136/1000 | Loss: 0.00001610
Iteration 137/1000 | Loss: 0.00001610
Iteration 138/1000 | Loss: 0.00001610
Iteration 139/1000 | Loss: 0.00001610
Iteration 140/1000 | Loss: 0.00001610
Iteration 141/1000 | Loss: 0.00001610
Iteration 142/1000 | Loss: 0.00001610
Iteration 143/1000 | Loss: 0.00001610
Iteration 144/1000 | Loss: 0.00001610
Iteration 145/1000 | Loss: 0.00001610
Iteration 146/1000 | Loss: 0.00001609
Iteration 147/1000 | Loss: 0.00001609
Iteration 148/1000 | Loss: 0.00001609
Iteration 149/1000 | Loss: 0.00001609
Iteration 150/1000 | Loss: 0.00001609
Iteration 151/1000 | Loss: 0.00001609
Iteration 152/1000 | Loss: 0.00001609
Iteration 153/1000 | Loss: 0.00001609
Iteration 154/1000 | Loss: 0.00001609
Iteration 155/1000 | Loss: 0.00001608
Iteration 156/1000 | Loss: 0.00001608
Iteration 157/1000 | Loss: 0.00001608
Iteration 158/1000 | Loss: 0.00001608
Iteration 159/1000 | Loss: 0.00001608
Iteration 160/1000 | Loss: 0.00001608
Iteration 161/1000 | Loss: 0.00001608
Iteration 162/1000 | Loss: 0.00001608
Iteration 163/1000 | Loss: 0.00001608
Iteration 164/1000 | Loss: 0.00001608
Iteration 165/1000 | Loss: 0.00001608
Iteration 166/1000 | Loss: 0.00001608
Iteration 167/1000 | Loss: 0.00001608
Iteration 168/1000 | Loss: 0.00001608
Iteration 169/1000 | Loss: 0.00001607
Iteration 170/1000 | Loss: 0.00001607
Iteration 171/1000 | Loss: 0.00001607
Iteration 172/1000 | Loss: 0.00001607
Iteration 173/1000 | Loss: 0.00001607
Iteration 174/1000 | Loss: 0.00001607
Iteration 175/1000 | Loss: 0.00001607
Iteration 176/1000 | Loss: 0.00001607
Iteration 177/1000 | Loss: 0.00001607
Iteration 178/1000 | Loss: 0.00001607
Iteration 179/1000 | Loss: 0.00001607
Iteration 180/1000 | Loss: 0.00001607
Iteration 181/1000 | Loss: 0.00001607
Iteration 182/1000 | Loss: 0.00001607
Iteration 183/1000 | Loss: 0.00001607
Iteration 184/1000 | Loss: 0.00001607
Iteration 185/1000 | Loss: 0.00001607
Iteration 186/1000 | Loss: 0.00001607
Iteration 187/1000 | Loss: 0.00001607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.6070793208200485e-05, 1.6070793208200485e-05, 1.6070793208200485e-05, 1.6070793208200485e-05, 1.6070793208200485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6070793208200485e-05

Optimization complete. Final v2v error: 3.3762218952178955 mm

Highest mean error: 3.5937325954437256 mm for frame 266

Lowest mean error: 3.1841375827789307 mm for frame 8

Saving results

Total time: 46.56003260612488
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004468
Iteration 2/25 | Loss: 0.00266652
Iteration 3/25 | Loss: 0.00210949
Iteration 4/25 | Loss: 0.00182304
Iteration 5/25 | Loss: 0.00164261
Iteration 6/25 | Loss: 0.00151977
Iteration 7/25 | Loss: 0.00144964
Iteration 8/25 | Loss: 0.00140155
Iteration 9/25 | Loss: 0.00137570
Iteration 10/25 | Loss: 0.00136671
Iteration 11/25 | Loss: 0.00136054
Iteration 12/25 | Loss: 0.00135905
Iteration 13/25 | Loss: 0.00135850
Iteration 14/25 | Loss: 0.00135722
Iteration 15/25 | Loss: 0.00135607
Iteration 16/25 | Loss: 0.00135534
Iteration 17/25 | Loss: 0.00135526
Iteration 18/25 | Loss: 0.00135526
Iteration 19/25 | Loss: 0.00135526
Iteration 20/25 | Loss: 0.00135525
Iteration 21/25 | Loss: 0.00135525
Iteration 22/25 | Loss: 0.00135525
Iteration 23/25 | Loss: 0.00135525
Iteration 24/25 | Loss: 0.00135525
Iteration 25/25 | Loss: 0.00135525

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46659410
Iteration 2/25 | Loss: 0.00099779
Iteration 3/25 | Loss: 0.00099779
Iteration 4/25 | Loss: 0.00099779
Iteration 5/25 | Loss: 0.00099779
Iteration 6/25 | Loss: 0.00099779
Iteration 7/25 | Loss: 0.00099779
Iteration 8/25 | Loss: 0.00099779
Iteration 9/25 | Loss: 0.00099779
Iteration 10/25 | Loss: 0.00099779
Iteration 11/25 | Loss: 0.00099779
Iteration 12/25 | Loss: 0.00099779
Iteration 13/25 | Loss: 0.00099779
Iteration 14/25 | Loss: 0.00099779
Iteration 15/25 | Loss: 0.00099779
Iteration 16/25 | Loss: 0.00099779
Iteration 17/25 | Loss: 0.00099779
Iteration 18/25 | Loss: 0.00099779
Iteration 19/25 | Loss: 0.00099779
Iteration 20/25 | Loss: 0.00099779
Iteration 21/25 | Loss: 0.00099779
Iteration 22/25 | Loss: 0.00099779
Iteration 23/25 | Loss: 0.00099779
Iteration 24/25 | Loss: 0.00099779
Iteration 25/25 | Loss: 0.00099779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099779
Iteration 2/1000 | Loss: 0.00006789
Iteration 3/1000 | Loss: 0.00008706
Iteration 4/1000 | Loss: 0.00014896
Iteration 5/1000 | Loss: 0.00007617
Iteration 6/1000 | Loss: 0.00014850
Iteration 7/1000 | Loss: 0.00012944
Iteration 8/1000 | Loss: 0.00005556
Iteration 9/1000 | Loss: 0.00004893
Iteration 10/1000 | Loss: 0.00004483
Iteration 11/1000 | Loss: 0.00009088
Iteration 12/1000 | Loss: 0.00008760
Iteration 13/1000 | Loss: 0.00003415
Iteration 14/1000 | Loss: 0.00010771
Iteration 15/1000 | Loss: 0.00101037
Iteration 16/1000 | Loss: 0.00153280
Iteration 17/1000 | Loss: 0.00072598
Iteration 18/1000 | Loss: 0.00225246
Iteration 19/1000 | Loss: 0.00229721
Iteration 20/1000 | Loss: 0.00015033
Iteration 21/1000 | Loss: 0.00007717
Iteration 22/1000 | Loss: 0.00009212
Iteration 23/1000 | Loss: 0.00005523
Iteration 24/1000 | Loss: 0.00005286
Iteration 25/1000 | Loss: 0.00003436
Iteration 26/1000 | Loss: 0.00048602
Iteration 27/1000 | Loss: 0.00016282
Iteration 28/1000 | Loss: 0.00042141
Iteration 29/1000 | Loss: 0.00012913
Iteration 30/1000 | Loss: 0.00060292
Iteration 31/1000 | Loss: 0.00030693
Iteration 32/1000 | Loss: 0.00036647
Iteration 33/1000 | Loss: 0.00004092
Iteration 34/1000 | Loss: 0.00009977
Iteration 35/1000 | Loss: 0.00003555
Iteration 36/1000 | Loss: 0.00003034
Iteration 37/1000 | Loss: 0.00005750
Iteration 38/1000 | Loss: 0.00002882
Iteration 39/1000 | Loss: 0.00019884
Iteration 40/1000 | Loss: 0.00010662
Iteration 41/1000 | Loss: 0.00062837
Iteration 42/1000 | Loss: 0.00027107
Iteration 43/1000 | Loss: 0.00002932
Iteration 44/1000 | Loss: 0.00002774
Iteration 45/1000 | Loss: 0.00002702
Iteration 46/1000 | Loss: 0.00048946
Iteration 47/1000 | Loss: 0.00067681
Iteration 48/1000 | Loss: 0.00048787
Iteration 49/1000 | Loss: 0.00041196
Iteration 50/1000 | Loss: 0.00012665
Iteration 51/1000 | Loss: 0.00026913
Iteration 52/1000 | Loss: 0.00002864
Iteration 53/1000 | Loss: 0.00002551
Iteration 54/1000 | Loss: 0.00002337
Iteration 55/1000 | Loss: 0.00007867
Iteration 56/1000 | Loss: 0.00002890
Iteration 57/1000 | Loss: 0.00002125
Iteration 58/1000 | Loss: 0.00002084
Iteration 59/1000 | Loss: 0.00002035
Iteration 60/1000 | Loss: 0.00007315
Iteration 61/1000 | Loss: 0.00076046
Iteration 62/1000 | Loss: 0.00006081
Iteration 63/1000 | Loss: 0.00004811
Iteration 64/1000 | Loss: 0.00001971
Iteration 65/1000 | Loss: 0.00001947
Iteration 66/1000 | Loss: 0.00005715
Iteration 67/1000 | Loss: 0.00006836
Iteration 68/1000 | Loss: 0.00029825
Iteration 69/1000 | Loss: 0.00002317
Iteration 70/1000 | Loss: 0.00001925
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00006130
Iteration 73/1000 | Loss: 0.00002029
Iteration 74/1000 | Loss: 0.00001901
Iteration 75/1000 | Loss: 0.00001899
Iteration 76/1000 | Loss: 0.00001898
Iteration 77/1000 | Loss: 0.00001892
Iteration 78/1000 | Loss: 0.00001891
Iteration 79/1000 | Loss: 0.00001891
Iteration 80/1000 | Loss: 0.00001890
Iteration 81/1000 | Loss: 0.00007599
Iteration 82/1000 | Loss: 0.00001891
Iteration 83/1000 | Loss: 0.00001888
Iteration 84/1000 | Loss: 0.00001887
Iteration 85/1000 | Loss: 0.00001887
Iteration 86/1000 | Loss: 0.00001886
Iteration 87/1000 | Loss: 0.00001886
Iteration 88/1000 | Loss: 0.00001886
Iteration 89/1000 | Loss: 0.00001885
Iteration 90/1000 | Loss: 0.00001885
Iteration 91/1000 | Loss: 0.00001885
Iteration 92/1000 | Loss: 0.00001884
Iteration 93/1000 | Loss: 0.00001884
Iteration 94/1000 | Loss: 0.00001884
Iteration 95/1000 | Loss: 0.00001884
Iteration 96/1000 | Loss: 0.00001884
Iteration 97/1000 | Loss: 0.00001884
Iteration 98/1000 | Loss: 0.00001883
Iteration 99/1000 | Loss: 0.00001883
Iteration 100/1000 | Loss: 0.00001883
Iteration 101/1000 | Loss: 0.00001882
Iteration 102/1000 | Loss: 0.00001882
Iteration 103/1000 | Loss: 0.00001882
Iteration 104/1000 | Loss: 0.00001882
Iteration 105/1000 | Loss: 0.00001882
Iteration 106/1000 | Loss: 0.00001882
Iteration 107/1000 | Loss: 0.00001882
Iteration 108/1000 | Loss: 0.00001881
Iteration 109/1000 | Loss: 0.00001881
Iteration 110/1000 | Loss: 0.00001881
Iteration 111/1000 | Loss: 0.00001881
Iteration 112/1000 | Loss: 0.00001881
Iteration 113/1000 | Loss: 0.00001881
Iteration 114/1000 | Loss: 0.00001881
Iteration 115/1000 | Loss: 0.00001881
Iteration 116/1000 | Loss: 0.00001881
Iteration 117/1000 | Loss: 0.00001881
Iteration 118/1000 | Loss: 0.00001881
Iteration 119/1000 | Loss: 0.00001881
Iteration 120/1000 | Loss: 0.00001881
Iteration 121/1000 | Loss: 0.00001881
Iteration 122/1000 | Loss: 0.00001881
Iteration 123/1000 | Loss: 0.00001881
Iteration 124/1000 | Loss: 0.00001880
Iteration 125/1000 | Loss: 0.00001880
Iteration 126/1000 | Loss: 0.00001880
Iteration 127/1000 | Loss: 0.00001880
Iteration 128/1000 | Loss: 0.00001880
Iteration 129/1000 | Loss: 0.00001880
Iteration 130/1000 | Loss: 0.00001880
Iteration 131/1000 | Loss: 0.00001880
Iteration 132/1000 | Loss: 0.00001880
Iteration 133/1000 | Loss: 0.00001880
Iteration 134/1000 | Loss: 0.00001880
Iteration 135/1000 | Loss: 0.00001880
Iteration 136/1000 | Loss: 0.00001880
Iteration 137/1000 | Loss: 0.00001880
Iteration 138/1000 | Loss: 0.00001880
Iteration 139/1000 | Loss: 0.00001880
Iteration 140/1000 | Loss: 0.00001880
Iteration 141/1000 | Loss: 0.00001879
Iteration 142/1000 | Loss: 0.00001879
Iteration 143/1000 | Loss: 0.00001879
Iteration 144/1000 | Loss: 0.00001879
Iteration 145/1000 | Loss: 0.00001879
Iteration 146/1000 | Loss: 0.00001879
Iteration 147/1000 | Loss: 0.00001879
Iteration 148/1000 | Loss: 0.00001878
Iteration 149/1000 | Loss: 0.00001878
Iteration 150/1000 | Loss: 0.00001878
Iteration 151/1000 | Loss: 0.00001878
Iteration 152/1000 | Loss: 0.00001878
Iteration 153/1000 | Loss: 0.00001878
Iteration 154/1000 | Loss: 0.00001878
Iteration 155/1000 | Loss: 0.00001878
Iteration 156/1000 | Loss: 0.00001878
Iteration 157/1000 | Loss: 0.00001878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.87784662557533e-05, 1.87784662557533e-05, 1.87784662557533e-05, 1.87784662557533e-05, 1.87784662557533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.87784662557533e-05

Optimization complete. Final v2v error: 3.6804022789001465 mm

Highest mean error: 4.400022029876709 mm for frame 234

Lowest mean error: 3.3763463497161865 mm for frame 129

Saving results

Total time: 159.837322473526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781548
Iteration 2/25 | Loss: 0.00176416
Iteration 3/25 | Loss: 0.00139747
Iteration 4/25 | Loss: 0.00134293
Iteration 5/25 | Loss: 0.00133353
Iteration 6/25 | Loss: 0.00133144
Iteration 7/25 | Loss: 0.00133093
Iteration 8/25 | Loss: 0.00133093
Iteration 9/25 | Loss: 0.00133093
Iteration 10/25 | Loss: 0.00133093
Iteration 11/25 | Loss: 0.00133093
Iteration 12/25 | Loss: 0.00133093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001330929808318615, 0.001330929808318615, 0.001330929808318615, 0.001330929808318615, 0.001330929808318615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001330929808318615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37055027
Iteration 2/25 | Loss: 0.00080534
Iteration 3/25 | Loss: 0.00080534
Iteration 4/25 | Loss: 0.00080534
Iteration 5/25 | Loss: 0.00080534
Iteration 6/25 | Loss: 0.00080533
Iteration 7/25 | Loss: 0.00080533
Iteration 8/25 | Loss: 0.00080533
Iteration 9/25 | Loss: 0.00080533
Iteration 10/25 | Loss: 0.00080533
Iteration 11/25 | Loss: 0.00080533
Iteration 12/25 | Loss: 0.00080533
Iteration 13/25 | Loss: 0.00080533
Iteration 14/25 | Loss: 0.00080533
Iteration 15/25 | Loss: 0.00080533
Iteration 16/25 | Loss: 0.00080533
Iteration 17/25 | Loss: 0.00080533
Iteration 18/25 | Loss: 0.00080533
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008053336059674621, 0.0008053336059674621, 0.0008053336059674621, 0.0008053336059674621, 0.0008053336059674621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008053336059674621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080533
Iteration 2/1000 | Loss: 0.00004222
Iteration 3/1000 | Loss: 0.00002787
Iteration 4/1000 | Loss: 0.00002361
Iteration 5/1000 | Loss: 0.00002220
Iteration 6/1000 | Loss: 0.00002840
Iteration 7/1000 | Loss: 0.00002126
Iteration 8/1000 | Loss: 0.00002037
Iteration 9/1000 | Loss: 0.00001962
Iteration 10/1000 | Loss: 0.00001918
Iteration 11/1000 | Loss: 0.00001889
Iteration 12/1000 | Loss: 0.00001863
Iteration 13/1000 | Loss: 0.00001830
Iteration 14/1000 | Loss: 0.00001811
Iteration 15/1000 | Loss: 0.00001808
Iteration 16/1000 | Loss: 0.00001802
Iteration 17/1000 | Loss: 0.00001801
Iteration 18/1000 | Loss: 0.00001800
Iteration 19/1000 | Loss: 0.00001798
Iteration 20/1000 | Loss: 0.00001798
Iteration 21/1000 | Loss: 0.00001785
Iteration 22/1000 | Loss: 0.00001781
Iteration 23/1000 | Loss: 0.00001762
Iteration 24/1000 | Loss: 0.00001741
Iteration 25/1000 | Loss: 0.00001738
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001722
Iteration 28/1000 | Loss: 0.00001714
Iteration 29/1000 | Loss: 0.00001714
Iteration 30/1000 | Loss: 0.00001713
Iteration 31/1000 | Loss: 0.00001712
Iteration 32/1000 | Loss: 0.00001712
Iteration 33/1000 | Loss: 0.00001712
Iteration 34/1000 | Loss: 0.00001710
Iteration 35/1000 | Loss: 0.00001710
Iteration 36/1000 | Loss: 0.00001709
Iteration 37/1000 | Loss: 0.00001708
Iteration 38/1000 | Loss: 0.00001708
Iteration 39/1000 | Loss: 0.00001707
Iteration 40/1000 | Loss: 0.00001707
Iteration 41/1000 | Loss: 0.00001705
Iteration 42/1000 | Loss: 0.00001705
Iteration 43/1000 | Loss: 0.00001705
Iteration 44/1000 | Loss: 0.00001704
Iteration 45/1000 | Loss: 0.00001704
Iteration 46/1000 | Loss: 0.00001703
Iteration 47/1000 | Loss: 0.00001702
Iteration 48/1000 | Loss: 0.00001702
Iteration 49/1000 | Loss: 0.00001702
Iteration 50/1000 | Loss: 0.00001701
Iteration 51/1000 | Loss: 0.00001701
Iteration 52/1000 | Loss: 0.00001701
Iteration 53/1000 | Loss: 0.00001700
Iteration 54/1000 | Loss: 0.00001700
Iteration 55/1000 | Loss: 0.00001700
Iteration 56/1000 | Loss: 0.00001700
Iteration 57/1000 | Loss: 0.00001700
Iteration 58/1000 | Loss: 0.00001699
Iteration 59/1000 | Loss: 0.00001699
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001698
Iteration 62/1000 | Loss: 0.00001698
Iteration 63/1000 | Loss: 0.00001697
Iteration 64/1000 | Loss: 0.00001695
Iteration 65/1000 | Loss: 0.00001695
Iteration 66/1000 | Loss: 0.00001695
Iteration 67/1000 | Loss: 0.00001694
Iteration 68/1000 | Loss: 0.00001693
Iteration 69/1000 | Loss: 0.00001693
Iteration 70/1000 | Loss: 0.00001693
Iteration 71/1000 | Loss: 0.00001692
Iteration 72/1000 | Loss: 0.00001692
Iteration 73/1000 | Loss: 0.00001692
Iteration 74/1000 | Loss: 0.00001692
Iteration 75/1000 | Loss: 0.00001692
Iteration 76/1000 | Loss: 0.00001692
Iteration 77/1000 | Loss: 0.00001692
Iteration 78/1000 | Loss: 0.00001691
Iteration 79/1000 | Loss: 0.00001691
Iteration 80/1000 | Loss: 0.00001691
Iteration 81/1000 | Loss: 0.00001691
Iteration 82/1000 | Loss: 0.00001691
Iteration 83/1000 | Loss: 0.00001691
Iteration 84/1000 | Loss: 0.00001691
Iteration 85/1000 | Loss: 0.00001691
Iteration 86/1000 | Loss: 0.00001691
Iteration 87/1000 | Loss: 0.00001690
Iteration 88/1000 | Loss: 0.00001690
Iteration 89/1000 | Loss: 0.00001690
Iteration 90/1000 | Loss: 0.00001690
Iteration 91/1000 | Loss: 0.00001690
Iteration 92/1000 | Loss: 0.00001690
Iteration 93/1000 | Loss: 0.00001690
Iteration 94/1000 | Loss: 0.00001690
Iteration 95/1000 | Loss: 0.00001690
Iteration 96/1000 | Loss: 0.00001690
Iteration 97/1000 | Loss: 0.00001690
Iteration 98/1000 | Loss: 0.00001690
Iteration 99/1000 | Loss: 0.00001689
Iteration 100/1000 | Loss: 0.00001689
Iteration 101/1000 | Loss: 0.00001689
Iteration 102/1000 | Loss: 0.00001689
Iteration 103/1000 | Loss: 0.00001689
Iteration 104/1000 | Loss: 0.00001689
Iteration 105/1000 | Loss: 0.00001688
Iteration 106/1000 | Loss: 0.00001688
Iteration 107/1000 | Loss: 0.00001688
Iteration 108/1000 | Loss: 0.00001688
Iteration 109/1000 | Loss: 0.00001688
Iteration 110/1000 | Loss: 0.00001687
Iteration 111/1000 | Loss: 0.00001687
Iteration 112/1000 | Loss: 0.00001687
Iteration 113/1000 | Loss: 0.00001687
Iteration 114/1000 | Loss: 0.00001687
Iteration 115/1000 | Loss: 0.00001687
Iteration 116/1000 | Loss: 0.00001687
Iteration 117/1000 | Loss: 0.00001687
Iteration 118/1000 | Loss: 0.00001687
Iteration 119/1000 | Loss: 0.00001687
Iteration 120/1000 | Loss: 0.00001686
Iteration 121/1000 | Loss: 0.00001686
Iteration 122/1000 | Loss: 0.00001686
Iteration 123/1000 | Loss: 0.00001686
Iteration 124/1000 | Loss: 0.00001686
Iteration 125/1000 | Loss: 0.00001686
Iteration 126/1000 | Loss: 0.00001685
Iteration 127/1000 | Loss: 0.00001685
Iteration 128/1000 | Loss: 0.00001685
Iteration 129/1000 | Loss: 0.00001685
Iteration 130/1000 | Loss: 0.00001685
Iteration 131/1000 | Loss: 0.00001685
Iteration 132/1000 | Loss: 0.00001685
Iteration 133/1000 | Loss: 0.00001685
Iteration 134/1000 | Loss: 0.00001685
Iteration 135/1000 | Loss: 0.00001685
Iteration 136/1000 | Loss: 0.00001685
Iteration 137/1000 | Loss: 0.00001685
Iteration 138/1000 | Loss: 0.00001685
Iteration 139/1000 | Loss: 0.00001685
Iteration 140/1000 | Loss: 0.00001684
Iteration 141/1000 | Loss: 0.00001684
Iteration 142/1000 | Loss: 0.00001684
Iteration 143/1000 | Loss: 0.00001684
Iteration 144/1000 | Loss: 0.00001684
Iteration 145/1000 | Loss: 0.00001684
Iteration 146/1000 | Loss: 0.00001684
Iteration 147/1000 | Loss: 0.00001684
Iteration 148/1000 | Loss: 0.00001684
Iteration 149/1000 | Loss: 0.00001684
Iteration 150/1000 | Loss: 0.00001684
Iteration 151/1000 | Loss: 0.00001684
Iteration 152/1000 | Loss: 0.00001684
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001683
Iteration 155/1000 | Loss: 0.00001683
Iteration 156/1000 | Loss: 0.00001683
Iteration 157/1000 | Loss: 0.00001683
Iteration 158/1000 | Loss: 0.00001683
Iteration 159/1000 | Loss: 0.00001683
Iteration 160/1000 | Loss: 0.00001683
Iteration 161/1000 | Loss: 0.00001683
Iteration 162/1000 | Loss: 0.00001683
Iteration 163/1000 | Loss: 0.00001683
Iteration 164/1000 | Loss: 0.00001683
Iteration 165/1000 | Loss: 0.00001683
Iteration 166/1000 | Loss: 0.00001683
Iteration 167/1000 | Loss: 0.00001683
Iteration 168/1000 | Loss: 0.00001683
Iteration 169/1000 | Loss: 0.00001683
Iteration 170/1000 | Loss: 0.00001683
Iteration 171/1000 | Loss: 0.00001683
Iteration 172/1000 | Loss: 0.00001683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.682924084889237e-05, 1.682924084889237e-05, 1.682924084889237e-05, 1.682924084889237e-05, 1.682924084889237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.682924084889237e-05

Optimization complete. Final v2v error: 3.498053789138794 mm

Highest mean error: 4.250382423400879 mm for frame 56

Lowest mean error: 3.019479513168335 mm for frame 3

Saving results

Total time: 54.93989038467407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00716489
Iteration 2/25 | Loss: 0.00168465
Iteration 3/25 | Loss: 0.00149657
Iteration 4/25 | Loss: 0.00146997
Iteration 5/25 | Loss: 0.00146340
Iteration 6/25 | Loss: 0.00146187
Iteration 7/25 | Loss: 0.00146182
Iteration 8/25 | Loss: 0.00146182
Iteration 9/25 | Loss: 0.00146182
Iteration 10/25 | Loss: 0.00146182
Iteration 11/25 | Loss: 0.00146182
Iteration 12/25 | Loss: 0.00146182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014618202112615108, 0.0014618202112615108, 0.0014618202112615108, 0.0014618202112615108, 0.0014618202112615108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014618202112615108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25364506
Iteration 2/25 | Loss: 0.00117479
Iteration 3/25 | Loss: 0.00117471
Iteration 4/25 | Loss: 0.00117471
Iteration 5/25 | Loss: 0.00117470
Iteration 6/25 | Loss: 0.00117470
Iteration 7/25 | Loss: 0.00117470
Iteration 8/25 | Loss: 0.00117470
Iteration 9/25 | Loss: 0.00117470
Iteration 10/25 | Loss: 0.00117470
Iteration 11/25 | Loss: 0.00117470
Iteration 12/25 | Loss: 0.00117470
Iteration 13/25 | Loss: 0.00117470
Iteration 14/25 | Loss: 0.00117470
Iteration 15/25 | Loss: 0.00117470
Iteration 16/25 | Loss: 0.00117470
Iteration 17/25 | Loss: 0.00117470
Iteration 18/25 | Loss: 0.00117470
Iteration 19/25 | Loss: 0.00117470
Iteration 20/25 | Loss: 0.00117470
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011747026583179832, 0.0011747026583179832, 0.0011747026583179832, 0.0011747026583179832, 0.0011747026583179832]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011747026583179832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117470
Iteration 2/1000 | Loss: 0.00009721
Iteration 3/1000 | Loss: 0.00005774
Iteration 4/1000 | Loss: 0.00004533
Iteration 5/1000 | Loss: 0.00004185
Iteration 6/1000 | Loss: 0.00004047
Iteration 7/1000 | Loss: 0.00003943
Iteration 8/1000 | Loss: 0.00003862
Iteration 9/1000 | Loss: 0.00003801
Iteration 10/1000 | Loss: 0.00003764
Iteration 11/1000 | Loss: 0.00003730
Iteration 12/1000 | Loss: 0.00003703
Iteration 13/1000 | Loss: 0.00003677
Iteration 14/1000 | Loss: 0.00003654
Iteration 15/1000 | Loss: 0.00003634
Iteration 16/1000 | Loss: 0.00003630
Iteration 17/1000 | Loss: 0.00003625
Iteration 18/1000 | Loss: 0.00003622
Iteration 19/1000 | Loss: 0.00003614
Iteration 20/1000 | Loss: 0.00003608
Iteration 21/1000 | Loss: 0.00003605
Iteration 22/1000 | Loss: 0.00003603
Iteration 23/1000 | Loss: 0.00003599
Iteration 24/1000 | Loss: 0.00003599
Iteration 25/1000 | Loss: 0.00003597
Iteration 26/1000 | Loss: 0.00003595
Iteration 27/1000 | Loss: 0.00003595
Iteration 28/1000 | Loss: 0.00003595
Iteration 29/1000 | Loss: 0.00003594
Iteration 30/1000 | Loss: 0.00003594
Iteration 31/1000 | Loss: 0.00003593
Iteration 32/1000 | Loss: 0.00003593
Iteration 33/1000 | Loss: 0.00003593
Iteration 34/1000 | Loss: 0.00003592
Iteration 35/1000 | Loss: 0.00003592
Iteration 36/1000 | Loss: 0.00003592
Iteration 37/1000 | Loss: 0.00003592
Iteration 38/1000 | Loss: 0.00003591
Iteration 39/1000 | Loss: 0.00003591
Iteration 40/1000 | Loss: 0.00003591
Iteration 41/1000 | Loss: 0.00003590
Iteration 42/1000 | Loss: 0.00003590
Iteration 43/1000 | Loss: 0.00003590
Iteration 44/1000 | Loss: 0.00003590
Iteration 45/1000 | Loss: 0.00003589
Iteration 46/1000 | Loss: 0.00003589
Iteration 47/1000 | Loss: 0.00003589
Iteration 48/1000 | Loss: 0.00003589
Iteration 49/1000 | Loss: 0.00003588
Iteration 50/1000 | Loss: 0.00003588
Iteration 51/1000 | Loss: 0.00003588
Iteration 52/1000 | Loss: 0.00003587
Iteration 53/1000 | Loss: 0.00003587
Iteration 54/1000 | Loss: 0.00003587
Iteration 55/1000 | Loss: 0.00003587
Iteration 56/1000 | Loss: 0.00003587
Iteration 57/1000 | Loss: 0.00003586
Iteration 58/1000 | Loss: 0.00003586
Iteration 59/1000 | Loss: 0.00003586
Iteration 60/1000 | Loss: 0.00003585
Iteration 61/1000 | Loss: 0.00003585
Iteration 62/1000 | Loss: 0.00003585
Iteration 63/1000 | Loss: 0.00003584
Iteration 64/1000 | Loss: 0.00003584
Iteration 65/1000 | Loss: 0.00003584
Iteration 66/1000 | Loss: 0.00003583
Iteration 67/1000 | Loss: 0.00003583
Iteration 68/1000 | Loss: 0.00003583
Iteration 69/1000 | Loss: 0.00003582
Iteration 70/1000 | Loss: 0.00003582
Iteration 71/1000 | Loss: 0.00003582
Iteration 72/1000 | Loss: 0.00003582
Iteration 73/1000 | Loss: 0.00003582
Iteration 74/1000 | Loss: 0.00003582
Iteration 75/1000 | Loss: 0.00003582
Iteration 76/1000 | Loss: 0.00003581
Iteration 77/1000 | Loss: 0.00003581
Iteration 78/1000 | Loss: 0.00003581
Iteration 79/1000 | Loss: 0.00003581
Iteration 80/1000 | Loss: 0.00003581
Iteration 81/1000 | Loss: 0.00003581
Iteration 82/1000 | Loss: 0.00003581
Iteration 83/1000 | Loss: 0.00003580
Iteration 84/1000 | Loss: 0.00003580
Iteration 85/1000 | Loss: 0.00003580
Iteration 86/1000 | Loss: 0.00003580
Iteration 87/1000 | Loss: 0.00003580
Iteration 88/1000 | Loss: 0.00003580
Iteration 89/1000 | Loss: 0.00003580
Iteration 90/1000 | Loss: 0.00003579
Iteration 91/1000 | Loss: 0.00003579
Iteration 92/1000 | Loss: 0.00003579
Iteration 93/1000 | Loss: 0.00003578
Iteration 94/1000 | Loss: 0.00003578
Iteration 95/1000 | Loss: 0.00003578
Iteration 96/1000 | Loss: 0.00003578
Iteration 97/1000 | Loss: 0.00003578
Iteration 98/1000 | Loss: 0.00003578
Iteration 99/1000 | Loss: 0.00003578
Iteration 100/1000 | Loss: 0.00003578
Iteration 101/1000 | Loss: 0.00003578
Iteration 102/1000 | Loss: 0.00003578
Iteration 103/1000 | Loss: 0.00003578
Iteration 104/1000 | Loss: 0.00003578
Iteration 105/1000 | Loss: 0.00003578
Iteration 106/1000 | Loss: 0.00003578
Iteration 107/1000 | Loss: 0.00003578
Iteration 108/1000 | Loss: 0.00003577
Iteration 109/1000 | Loss: 0.00003577
Iteration 110/1000 | Loss: 0.00003577
Iteration 111/1000 | Loss: 0.00003577
Iteration 112/1000 | Loss: 0.00003577
Iteration 113/1000 | Loss: 0.00003577
Iteration 114/1000 | Loss: 0.00003577
Iteration 115/1000 | Loss: 0.00003577
Iteration 116/1000 | Loss: 0.00003577
Iteration 117/1000 | Loss: 0.00003577
Iteration 118/1000 | Loss: 0.00003576
Iteration 119/1000 | Loss: 0.00003576
Iteration 120/1000 | Loss: 0.00003576
Iteration 121/1000 | Loss: 0.00003576
Iteration 122/1000 | Loss: 0.00003576
Iteration 123/1000 | Loss: 0.00003576
Iteration 124/1000 | Loss: 0.00003576
Iteration 125/1000 | Loss: 0.00003575
Iteration 126/1000 | Loss: 0.00003575
Iteration 127/1000 | Loss: 0.00003575
Iteration 128/1000 | Loss: 0.00003575
Iteration 129/1000 | Loss: 0.00003575
Iteration 130/1000 | Loss: 0.00003574
Iteration 131/1000 | Loss: 0.00003574
Iteration 132/1000 | Loss: 0.00003574
Iteration 133/1000 | Loss: 0.00003574
Iteration 134/1000 | Loss: 0.00003574
Iteration 135/1000 | Loss: 0.00003573
Iteration 136/1000 | Loss: 0.00003573
Iteration 137/1000 | Loss: 0.00003573
Iteration 138/1000 | Loss: 0.00003573
Iteration 139/1000 | Loss: 0.00003573
Iteration 140/1000 | Loss: 0.00003572
Iteration 141/1000 | Loss: 0.00003572
Iteration 142/1000 | Loss: 0.00003572
Iteration 143/1000 | Loss: 0.00003572
Iteration 144/1000 | Loss: 0.00003572
Iteration 145/1000 | Loss: 0.00003572
Iteration 146/1000 | Loss: 0.00003571
Iteration 147/1000 | Loss: 0.00003571
Iteration 148/1000 | Loss: 0.00003571
Iteration 149/1000 | Loss: 0.00003571
Iteration 150/1000 | Loss: 0.00003571
Iteration 151/1000 | Loss: 0.00003571
Iteration 152/1000 | Loss: 0.00003571
Iteration 153/1000 | Loss: 0.00003571
Iteration 154/1000 | Loss: 0.00003570
Iteration 155/1000 | Loss: 0.00003570
Iteration 156/1000 | Loss: 0.00003570
Iteration 157/1000 | Loss: 0.00003570
Iteration 158/1000 | Loss: 0.00003570
Iteration 159/1000 | Loss: 0.00003570
Iteration 160/1000 | Loss: 0.00003569
Iteration 161/1000 | Loss: 0.00003569
Iteration 162/1000 | Loss: 0.00003569
Iteration 163/1000 | Loss: 0.00003569
Iteration 164/1000 | Loss: 0.00003569
Iteration 165/1000 | Loss: 0.00003569
Iteration 166/1000 | Loss: 0.00003569
Iteration 167/1000 | Loss: 0.00003569
Iteration 168/1000 | Loss: 0.00003569
Iteration 169/1000 | Loss: 0.00003569
Iteration 170/1000 | Loss: 0.00003569
Iteration 171/1000 | Loss: 0.00003569
Iteration 172/1000 | Loss: 0.00003569
Iteration 173/1000 | Loss: 0.00003569
Iteration 174/1000 | Loss: 0.00003569
Iteration 175/1000 | Loss: 0.00003568
Iteration 176/1000 | Loss: 0.00003568
Iteration 177/1000 | Loss: 0.00003568
Iteration 178/1000 | Loss: 0.00003568
Iteration 179/1000 | Loss: 0.00003568
Iteration 180/1000 | Loss: 0.00003568
Iteration 181/1000 | Loss: 0.00003568
Iteration 182/1000 | Loss: 0.00003568
Iteration 183/1000 | Loss: 0.00003568
Iteration 184/1000 | Loss: 0.00003568
Iteration 185/1000 | Loss: 0.00003568
Iteration 186/1000 | Loss: 0.00003568
Iteration 187/1000 | Loss: 0.00003568
Iteration 188/1000 | Loss: 0.00003568
Iteration 189/1000 | Loss: 0.00003568
Iteration 190/1000 | Loss: 0.00003568
Iteration 191/1000 | Loss: 0.00003568
Iteration 192/1000 | Loss: 0.00003568
Iteration 193/1000 | Loss: 0.00003567
Iteration 194/1000 | Loss: 0.00003567
Iteration 195/1000 | Loss: 0.00003567
Iteration 196/1000 | Loss: 0.00003567
Iteration 197/1000 | Loss: 0.00003567
Iteration 198/1000 | Loss: 0.00003567
Iteration 199/1000 | Loss: 0.00003567
Iteration 200/1000 | Loss: 0.00003567
Iteration 201/1000 | Loss: 0.00003567
Iteration 202/1000 | Loss: 0.00003567
Iteration 203/1000 | Loss: 0.00003567
Iteration 204/1000 | Loss: 0.00003567
Iteration 205/1000 | Loss: 0.00003567
Iteration 206/1000 | Loss: 0.00003567
Iteration 207/1000 | Loss: 0.00003567
Iteration 208/1000 | Loss: 0.00003567
Iteration 209/1000 | Loss: 0.00003567
Iteration 210/1000 | Loss: 0.00003567
Iteration 211/1000 | Loss: 0.00003567
Iteration 212/1000 | Loss: 0.00003567
Iteration 213/1000 | Loss: 0.00003566
Iteration 214/1000 | Loss: 0.00003566
Iteration 215/1000 | Loss: 0.00003566
Iteration 216/1000 | Loss: 0.00003566
Iteration 217/1000 | Loss: 0.00003566
Iteration 218/1000 | Loss: 0.00003566
Iteration 219/1000 | Loss: 0.00003566
Iteration 220/1000 | Loss: 0.00003566
Iteration 221/1000 | Loss: 0.00003566
Iteration 222/1000 | Loss: 0.00003566
Iteration 223/1000 | Loss: 0.00003566
Iteration 224/1000 | Loss: 0.00003566
Iteration 225/1000 | Loss: 0.00003566
Iteration 226/1000 | Loss: 0.00003566
Iteration 227/1000 | Loss: 0.00003566
Iteration 228/1000 | Loss: 0.00003566
Iteration 229/1000 | Loss: 0.00003566
Iteration 230/1000 | Loss: 0.00003566
Iteration 231/1000 | Loss: 0.00003565
Iteration 232/1000 | Loss: 0.00003565
Iteration 233/1000 | Loss: 0.00003565
Iteration 234/1000 | Loss: 0.00003565
Iteration 235/1000 | Loss: 0.00003565
Iteration 236/1000 | Loss: 0.00003565
Iteration 237/1000 | Loss: 0.00003565
Iteration 238/1000 | Loss: 0.00003565
Iteration 239/1000 | Loss: 0.00003565
Iteration 240/1000 | Loss: 0.00003565
Iteration 241/1000 | Loss: 0.00003565
Iteration 242/1000 | Loss: 0.00003565
Iteration 243/1000 | Loss: 0.00003565
Iteration 244/1000 | Loss: 0.00003565
Iteration 245/1000 | Loss: 0.00003565
Iteration 246/1000 | Loss: 0.00003565
Iteration 247/1000 | Loss: 0.00003565
Iteration 248/1000 | Loss: 0.00003565
Iteration 249/1000 | Loss: 0.00003565
Iteration 250/1000 | Loss: 0.00003565
Iteration 251/1000 | Loss: 0.00003565
Iteration 252/1000 | Loss: 0.00003565
Iteration 253/1000 | Loss: 0.00003564
Iteration 254/1000 | Loss: 0.00003564
Iteration 255/1000 | Loss: 0.00003564
Iteration 256/1000 | Loss: 0.00003564
Iteration 257/1000 | Loss: 0.00003564
Iteration 258/1000 | Loss: 0.00003564
Iteration 259/1000 | Loss: 0.00003564
Iteration 260/1000 | Loss: 0.00003564
Iteration 261/1000 | Loss: 0.00003564
Iteration 262/1000 | Loss: 0.00003564
Iteration 263/1000 | Loss: 0.00003564
Iteration 264/1000 | Loss: 0.00003564
Iteration 265/1000 | Loss: 0.00003564
Iteration 266/1000 | Loss: 0.00003564
Iteration 267/1000 | Loss: 0.00003564
Iteration 268/1000 | Loss: 0.00003564
Iteration 269/1000 | Loss: 0.00003564
Iteration 270/1000 | Loss: 0.00003564
Iteration 271/1000 | Loss: 0.00003564
Iteration 272/1000 | Loss: 0.00003564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [3.564463258953765e-05, 3.564463258953765e-05, 3.564463258953765e-05, 3.564463258953765e-05, 3.564463258953765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.564463258953765e-05

Optimization complete. Final v2v error: 4.814659118652344 mm

Highest mean error: 5.772461414337158 mm for frame 108

Lowest mean error: 3.6634044647216797 mm for frame 16

Saving results

Total time: 49.12679481506348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801774
Iteration 2/25 | Loss: 0.00152526
Iteration 3/25 | Loss: 0.00139023
Iteration 4/25 | Loss: 0.00137364
Iteration 5/25 | Loss: 0.00136926
Iteration 6/25 | Loss: 0.00136810
Iteration 7/25 | Loss: 0.00136793
Iteration 8/25 | Loss: 0.00136793
Iteration 9/25 | Loss: 0.00136793
Iteration 10/25 | Loss: 0.00136793
Iteration 11/25 | Loss: 0.00136793
Iteration 12/25 | Loss: 0.00136793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013679260155186057, 0.0013679260155186057, 0.0013679260155186057, 0.0013679260155186057, 0.0013679260155186057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013679260155186057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39465749
Iteration 2/25 | Loss: 0.00190588
Iteration 3/25 | Loss: 0.00190586
Iteration 4/25 | Loss: 0.00190586
Iteration 5/25 | Loss: 0.00190586
Iteration 6/25 | Loss: 0.00190586
Iteration 7/25 | Loss: 0.00190586
Iteration 8/25 | Loss: 0.00190586
Iteration 9/25 | Loss: 0.00190586
Iteration 10/25 | Loss: 0.00190586
Iteration 11/25 | Loss: 0.00190586
Iteration 12/25 | Loss: 0.00190586
Iteration 13/25 | Loss: 0.00190586
Iteration 14/25 | Loss: 0.00190586
Iteration 15/25 | Loss: 0.00190586
Iteration 16/25 | Loss: 0.00190586
Iteration 17/25 | Loss: 0.00190586
Iteration 18/25 | Loss: 0.00190586
Iteration 19/25 | Loss: 0.00190586
Iteration 20/25 | Loss: 0.00190586
Iteration 21/25 | Loss: 0.00190586
Iteration 22/25 | Loss: 0.00190586
Iteration 23/25 | Loss: 0.00190586
Iteration 24/25 | Loss: 0.00190586
Iteration 25/25 | Loss: 0.00190586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190586
Iteration 2/1000 | Loss: 0.00012650
Iteration 3/1000 | Loss: 0.00008696
Iteration 4/1000 | Loss: 0.00014922
Iteration 5/1000 | Loss: 0.00012239
Iteration 6/1000 | Loss: 0.00010799
Iteration 7/1000 | Loss: 0.00022265
Iteration 8/1000 | Loss: 0.00014317
Iteration 9/1000 | Loss: 0.00011972
Iteration 10/1000 | Loss: 0.00006254
Iteration 11/1000 | Loss: 0.00018472
Iteration 12/1000 | Loss: 0.00019940
Iteration 13/1000 | Loss: 0.00008603
Iteration 14/1000 | Loss: 0.00007480
Iteration 15/1000 | Loss: 0.00013401
Iteration 16/1000 | Loss: 0.00006412
Iteration 17/1000 | Loss: 0.00013772
Iteration 18/1000 | Loss: 0.00015182
Iteration 19/1000 | Loss: 0.00019416
Iteration 20/1000 | Loss: 0.00040549
Iteration 21/1000 | Loss: 0.00007246
Iteration 22/1000 | Loss: 0.00008332
Iteration 23/1000 | Loss: 0.00013511
Iteration 24/1000 | Loss: 0.00012469
Iteration 25/1000 | Loss: 0.00017070
Iteration 26/1000 | Loss: 0.00017596
Iteration 27/1000 | Loss: 0.00019453
Iteration 28/1000 | Loss: 0.00016599
Iteration 29/1000 | Loss: 0.00020934
Iteration 30/1000 | Loss: 0.00008654
Iteration 31/1000 | Loss: 0.00009024
Iteration 32/1000 | Loss: 0.00021315
Iteration 33/1000 | Loss: 0.00016455
Iteration 34/1000 | Loss: 0.00028573
Iteration 35/1000 | Loss: 0.00012413
Iteration 36/1000 | Loss: 0.00010487
Iteration 37/1000 | Loss: 0.00008996
Iteration 38/1000 | Loss: 0.00005846
Iteration 39/1000 | Loss: 0.00010980
Iteration 40/1000 | Loss: 0.00010820
Iteration 41/1000 | Loss: 0.00005743
Iteration 42/1000 | Loss: 0.00020154
Iteration 43/1000 | Loss: 0.00007330
Iteration 44/1000 | Loss: 0.00015224
Iteration 45/1000 | Loss: 0.00006412
Iteration 46/1000 | Loss: 0.00010628
Iteration 47/1000 | Loss: 0.00006348
Iteration 48/1000 | Loss: 0.00018464
Iteration 49/1000 | Loss: 0.00017230
Iteration 50/1000 | Loss: 0.00023995
Iteration 51/1000 | Loss: 0.00015916
Iteration 52/1000 | Loss: 0.00021607
Iteration 53/1000 | Loss: 0.00012349
Iteration 54/1000 | Loss: 0.00020607
Iteration 55/1000 | Loss: 0.00012414
Iteration 56/1000 | Loss: 0.00009275
Iteration 57/1000 | Loss: 0.00009755
Iteration 58/1000 | Loss: 0.00014845
Iteration 59/1000 | Loss: 0.00008753
Iteration 60/1000 | Loss: 0.00007547
Iteration 61/1000 | Loss: 0.00020100
Iteration 62/1000 | Loss: 0.00019560
Iteration 63/1000 | Loss: 0.00006272
Iteration 64/1000 | Loss: 0.00005891
Iteration 65/1000 | Loss: 0.00063822
Iteration 66/1000 | Loss: 0.00011406
Iteration 67/1000 | Loss: 0.00005834
Iteration 68/1000 | Loss: 0.00020329
Iteration 69/1000 | Loss: 0.00005950
Iteration 70/1000 | Loss: 0.00006240
Iteration 71/1000 | Loss: 0.00005614
Iteration 72/1000 | Loss: 0.00005899
Iteration 73/1000 | Loss: 0.00005575
Iteration 74/1000 | Loss: 0.00013996
Iteration 75/1000 | Loss: 0.00013130
Iteration 76/1000 | Loss: 0.00014351
Iteration 77/1000 | Loss: 0.00013673
Iteration 78/1000 | Loss: 0.00012016
Iteration 79/1000 | Loss: 0.00010982
Iteration 80/1000 | Loss: 0.00013613
Iteration 81/1000 | Loss: 0.00023662
Iteration 82/1000 | Loss: 0.00014031
Iteration 83/1000 | Loss: 0.00016581
Iteration 84/1000 | Loss: 0.00014078
Iteration 85/1000 | Loss: 0.00012953
Iteration 86/1000 | Loss: 0.00014574
Iteration 87/1000 | Loss: 0.00026373
Iteration 88/1000 | Loss: 0.00007262
Iteration 89/1000 | Loss: 0.00014499
Iteration 90/1000 | Loss: 0.00022452
Iteration 91/1000 | Loss: 0.00049108
Iteration 92/1000 | Loss: 0.00012113
Iteration 93/1000 | Loss: 0.00014443
Iteration 94/1000 | Loss: 0.00016835
Iteration 95/1000 | Loss: 0.00014938
Iteration 96/1000 | Loss: 0.00012790
Iteration 97/1000 | Loss: 0.00015989
Iteration 98/1000 | Loss: 0.00025398
Iteration 99/1000 | Loss: 0.00015558
Iteration 100/1000 | Loss: 0.00031004
Iteration 101/1000 | Loss: 0.00015913
Iteration 102/1000 | Loss: 0.00015627
Iteration 103/1000 | Loss: 0.00006571
Iteration 104/1000 | Loss: 0.00024918
Iteration 105/1000 | Loss: 0.00006117
Iteration 106/1000 | Loss: 0.00005482
Iteration 107/1000 | Loss: 0.00005371
Iteration 108/1000 | Loss: 0.00005311
Iteration 109/1000 | Loss: 0.00005253
Iteration 110/1000 | Loss: 0.00005228
Iteration 111/1000 | Loss: 0.00005197
Iteration 112/1000 | Loss: 0.00005148
Iteration 113/1000 | Loss: 0.00005058
Iteration 114/1000 | Loss: 0.00005004
Iteration 115/1000 | Loss: 0.00004951
Iteration 116/1000 | Loss: 0.00004917
Iteration 117/1000 | Loss: 0.00004891
Iteration 118/1000 | Loss: 0.00004866
Iteration 119/1000 | Loss: 0.00004836
Iteration 120/1000 | Loss: 0.00004809
Iteration 121/1000 | Loss: 0.00004778
Iteration 122/1000 | Loss: 0.00004766
Iteration 123/1000 | Loss: 0.00004754
Iteration 124/1000 | Loss: 0.00004747
Iteration 125/1000 | Loss: 0.00004740
Iteration 126/1000 | Loss: 0.00004732
Iteration 127/1000 | Loss: 0.00004721
Iteration 128/1000 | Loss: 0.00004720
Iteration 129/1000 | Loss: 0.00004719
Iteration 130/1000 | Loss: 0.00004718
Iteration 131/1000 | Loss: 0.00004718
Iteration 132/1000 | Loss: 0.00004717
Iteration 133/1000 | Loss: 0.00004716
Iteration 134/1000 | Loss: 0.00004716
Iteration 135/1000 | Loss: 0.00004714
Iteration 136/1000 | Loss: 0.00004713
Iteration 137/1000 | Loss: 0.00004708
Iteration 138/1000 | Loss: 0.00004703
Iteration 139/1000 | Loss: 0.00004702
Iteration 140/1000 | Loss: 0.00004702
Iteration 141/1000 | Loss: 0.00004701
Iteration 142/1000 | Loss: 0.00004700
Iteration 143/1000 | Loss: 0.00004699
Iteration 144/1000 | Loss: 0.00004699
Iteration 145/1000 | Loss: 0.00004699
Iteration 146/1000 | Loss: 0.00004699
Iteration 147/1000 | Loss: 0.00004698
Iteration 148/1000 | Loss: 0.00004697
Iteration 149/1000 | Loss: 0.00004697
Iteration 150/1000 | Loss: 0.00004696
Iteration 151/1000 | Loss: 0.00004696
Iteration 152/1000 | Loss: 0.00004696
Iteration 153/1000 | Loss: 0.00004695
Iteration 154/1000 | Loss: 0.00004695
Iteration 155/1000 | Loss: 0.00004695
Iteration 156/1000 | Loss: 0.00004694
Iteration 157/1000 | Loss: 0.00004694
Iteration 158/1000 | Loss: 0.00004694
Iteration 159/1000 | Loss: 0.00004693
Iteration 160/1000 | Loss: 0.00004693
Iteration 161/1000 | Loss: 0.00004692
Iteration 162/1000 | Loss: 0.00004692
Iteration 163/1000 | Loss: 0.00004692
Iteration 164/1000 | Loss: 0.00004691
Iteration 165/1000 | Loss: 0.00004691
Iteration 166/1000 | Loss: 0.00004691
Iteration 167/1000 | Loss: 0.00004690
Iteration 168/1000 | Loss: 0.00004690
Iteration 169/1000 | Loss: 0.00004690
Iteration 170/1000 | Loss: 0.00004689
Iteration 171/1000 | Loss: 0.00004689
Iteration 172/1000 | Loss: 0.00004688
Iteration 173/1000 | Loss: 0.00004688
Iteration 174/1000 | Loss: 0.00004687
Iteration 175/1000 | Loss: 0.00004687
Iteration 176/1000 | Loss: 0.00004685
Iteration 177/1000 | Loss: 0.00004684
Iteration 178/1000 | Loss: 0.00004684
Iteration 179/1000 | Loss: 0.00004684
Iteration 180/1000 | Loss: 0.00004681
Iteration 181/1000 | Loss: 0.00004681
Iteration 182/1000 | Loss: 0.00004680
Iteration 183/1000 | Loss: 0.00004680
Iteration 184/1000 | Loss: 0.00004679
Iteration 185/1000 | Loss: 0.00004679
Iteration 186/1000 | Loss: 0.00004678
Iteration 187/1000 | Loss: 0.00004678
Iteration 188/1000 | Loss: 0.00004678
Iteration 189/1000 | Loss: 0.00004678
Iteration 190/1000 | Loss: 0.00004677
Iteration 191/1000 | Loss: 0.00004677
Iteration 192/1000 | Loss: 0.00004676
Iteration 193/1000 | Loss: 0.00004676
Iteration 194/1000 | Loss: 0.00004676
Iteration 195/1000 | Loss: 0.00004676
Iteration 196/1000 | Loss: 0.00004675
Iteration 197/1000 | Loss: 0.00004675
Iteration 198/1000 | Loss: 0.00004675
Iteration 199/1000 | Loss: 0.00004674
Iteration 200/1000 | Loss: 0.00004673
Iteration 201/1000 | Loss: 0.00004673
Iteration 202/1000 | Loss: 0.00004673
Iteration 203/1000 | Loss: 0.00004673
Iteration 204/1000 | Loss: 0.00004673
Iteration 205/1000 | Loss: 0.00004672
Iteration 206/1000 | Loss: 0.00004672
Iteration 207/1000 | Loss: 0.00004672
Iteration 208/1000 | Loss: 0.00004672
Iteration 209/1000 | Loss: 0.00004672
Iteration 210/1000 | Loss: 0.00004672
Iteration 211/1000 | Loss: 0.00004671
Iteration 212/1000 | Loss: 0.00004671
Iteration 213/1000 | Loss: 0.00004671
Iteration 214/1000 | Loss: 0.00004670
Iteration 215/1000 | Loss: 0.00004670
Iteration 216/1000 | Loss: 0.00004670
Iteration 217/1000 | Loss: 0.00004669
Iteration 218/1000 | Loss: 0.00004669
Iteration 219/1000 | Loss: 0.00004668
Iteration 220/1000 | Loss: 0.00004668
Iteration 221/1000 | Loss: 0.00004668
Iteration 222/1000 | Loss: 0.00004668
Iteration 223/1000 | Loss: 0.00004668
Iteration 224/1000 | Loss: 0.00004668
Iteration 225/1000 | Loss: 0.00004668
Iteration 226/1000 | Loss: 0.00004668
Iteration 227/1000 | Loss: 0.00004668
Iteration 228/1000 | Loss: 0.00004668
Iteration 229/1000 | Loss: 0.00004668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [4.667695247917436e-05, 4.667695247917436e-05, 4.667695247917436e-05, 4.667695247917436e-05, 4.667695247917436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.667695247917436e-05

Optimization complete. Final v2v error: 3.7258923053741455 mm

Highest mean error: 11.24425220489502 mm for frame 84

Lowest mean error: 2.7275311946868896 mm for frame 31

Saving results

Total time: 207.55278062820435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466475
Iteration 2/25 | Loss: 0.00154618
Iteration 3/25 | Loss: 0.00136542
Iteration 4/25 | Loss: 0.00134831
Iteration 5/25 | Loss: 0.00134466
Iteration 6/25 | Loss: 0.00134370
Iteration 7/25 | Loss: 0.00134370
Iteration 8/25 | Loss: 0.00134370
Iteration 9/25 | Loss: 0.00134370
Iteration 10/25 | Loss: 0.00134370
Iteration 11/25 | Loss: 0.00134370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013437004527077079, 0.0013437004527077079, 0.0013437004527077079, 0.0013437004527077079, 0.0013437004527077079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013437004527077079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48212397
Iteration 2/25 | Loss: 0.00079301
Iteration 3/25 | Loss: 0.00079301
Iteration 4/25 | Loss: 0.00079301
Iteration 5/25 | Loss: 0.00079301
Iteration 6/25 | Loss: 0.00079301
Iteration 7/25 | Loss: 0.00079301
Iteration 8/25 | Loss: 0.00079301
Iteration 9/25 | Loss: 0.00079301
Iteration 10/25 | Loss: 0.00079301
Iteration 11/25 | Loss: 0.00079301
Iteration 12/25 | Loss: 0.00079301
Iteration 13/25 | Loss: 0.00079301
Iteration 14/25 | Loss: 0.00079301
Iteration 15/25 | Loss: 0.00079301
Iteration 16/25 | Loss: 0.00079301
Iteration 17/25 | Loss: 0.00079301
Iteration 18/25 | Loss: 0.00079301
Iteration 19/25 | Loss: 0.00079301
Iteration 20/25 | Loss: 0.00079301
Iteration 21/25 | Loss: 0.00079301
Iteration 22/25 | Loss: 0.00079301
Iteration 23/25 | Loss: 0.00079301
Iteration 24/25 | Loss: 0.00079301
Iteration 25/25 | Loss: 0.00079301

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079301
Iteration 2/1000 | Loss: 0.00004026
Iteration 3/1000 | Loss: 0.00002600
Iteration 4/1000 | Loss: 0.00002228
Iteration 5/1000 | Loss: 0.00002087
Iteration 6/1000 | Loss: 0.00001989
Iteration 7/1000 | Loss: 0.00001933
Iteration 8/1000 | Loss: 0.00001880
Iteration 9/1000 | Loss: 0.00001854
Iteration 10/1000 | Loss: 0.00001829
Iteration 11/1000 | Loss: 0.00001806
Iteration 12/1000 | Loss: 0.00001795
Iteration 13/1000 | Loss: 0.00001779
Iteration 14/1000 | Loss: 0.00001763
Iteration 15/1000 | Loss: 0.00001762
Iteration 16/1000 | Loss: 0.00001748
Iteration 17/1000 | Loss: 0.00001747
Iteration 18/1000 | Loss: 0.00001745
Iteration 19/1000 | Loss: 0.00001741
Iteration 20/1000 | Loss: 0.00001741
Iteration 21/1000 | Loss: 0.00001739
Iteration 22/1000 | Loss: 0.00001734
Iteration 23/1000 | Loss: 0.00001732
Iteration 24/1000 | Loss: 0.00001731
Iteration 25/1000 | Loss: 0.00001730
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001729
Iteration 28/1000 | Loss: 0.00001728
Iteration 29/1000 | Loss: 0.00001728
Iteration 30/1000 | Loss: 0.00001727
Iteration 31/1000 | Loss: 0.00001727
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001727
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001726
Iteration 37/1000 | Loss: 0.00001726
Iteration 38/1000 | Loss: 0.00001725
Iteration 39/1000 | Loss: 0.00001725
Iteration 40/1000 | Loss: 0.00001724
Iteration 41/1000 | Loss: 0.00001724
Iteration 42/1000 | Loss: 0.00001724
Iteration 43/1000 | Loss: 0.00001724
Iteration 44/1000 | Loss: 0.00001724
Iteration 45/1000 | Loss: 0.00001724
Iteration 46/1000 | Loss: 0.00001723
Iteration 47/1000 | Loss: 0.00001723
Iteration 48/1000 | Loss: 0.00001723
Iteration 49/1000 | Loss: 0.00001723
Iteration 50/1000 | Loss: 0.00001722
Iteration 51/1000 | Loss: 0.00001722
Iteration 52/1000 | Loss: 0.00001722
Iteration 53/1000 | Loss: 0.00001721
Iteration 54/1000 | Loss: 0.00001721
Iteration 55/1000 | Loss: 0.00001719
Iteration 56/1000 | Loss: 0.00001715
Iteration 57/1000 | Loss: 0.00001715
Iteration 58/1000 | Loss: 0.00001713
Iteration 59/1000 | Loss: 0.00001713
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001712
Iteration 62/1000 | Loss: 0.00001709
Iteration 63/1000 | Loss: 0.00001709
Iteration 64/1000 | Loss: 0.00001708
Iteration 65/1000 | Loss: 0.00001708
Iteration 66/1000 | Loss: 0.00001708
Iteration 67/1000 | Loss: 0.00001708
Iteration 68/1000 | Loss: 0.00001708
Iteration 69/1000 | Loss: 0.00001708
Iteration 70/1000 | Loss: 0.00001708
Iteration 71/1000 | Loss: 0.00001708
Iteration 72/1000 | Loss: 0.00001708
Iteration 73/1000 | Loss: 0.00001708
Iteration 74/1000 | Loss: 0.00001708
Iteration 75/1000 | Loss: 0.00001707
Iteration 76/1000 | Loss: 0.00001707
Iteration 77/1000 | Loss: 0.00001707
Iteration 78/1000 | Loss: 0.00001707
Iteration 79/1000 | Loss: 0.00001707
Iteration 80/1000 | Loss: 0.00001706
Iteration 81/1000 | Loss: 0.00001705
Iteration 82/1000 | Loss: 0.00001705
Iteration 83/1000 | Loss: 0.00001704
Iteration 84/1000 | Loss: 0.00001704
Iteration 85/1000 | Loss: 0.00001704
Iteration 86/1000 | Loss: 0.00001703
Iteration 87/1000 | Loss: 0.00001703
Iteration 88/1000 | Loss: 0.00001703
Iteration 89/1000 | Loss: 0.00001702
Iteration 90/1000 | Loss: 0.00001702
Iteration 91/1000 | Loss: 0.00001702
Iteration 92/1000 | Loss: 0.00001701
Iteration 93/1000 | Loss: 0.00001701
Iteration 94/1000 | Loss: 0.00001701
Iteration 95/1000 | Loss: 0.00001701
Iteration 96/1000 | Loss: 0.00001701
Iteration 97/1000 | Loss: 0.00001701
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001701
Iteration 100/1000 | Loss: 0.00001700
Iteration 101/1000 | Loss: 0.00001700
Iteration 102/1000 | Loss: 0.00001699
Iteration 103/1000 | Loss: 0.00001699
Iteration 104/1000 | Loss: 0.00001698
Iteration 105/1000 | Loss: 0.00001698
Iteration 106/1000 | Loss: 0.00001698
Iteration 107/1000 | Loss: 0.00001698
Iteration 108/1000 | Loss: 0.00001698
Iteration 109/1000 | Loss: 0.00001698
Iteration 110/1000 | Loss: 0.00001698
Iteration 111/1000 | Loss: 0.00001698
Iteration 112/1000 | Loss: 0.00001698
Iteration 113/1000 | Loss: 0.00001698
Iteration 114/1000 | Loss: 0.00001698
Iteration 115/1000 | Loss: 0.00001697
Iteration 116/1000 | Loss: 0.00001697
Iteration 117/1000 | Loss: 0.00001697
Iteration 118/1000 | Loss: 0.00001697
Iteration 119/1000 | Loss: 0.00001697
Iteration 120/1000 | Loss: 0.00001697
Iteration 121/1000 | Loss: 0.00001697
Iteration 122/1000 | Loss: 0.00001697
Iteration 123/1000 | Loss: 0.00001697
Iteration 124/1000 | Loss: 0.00001697
Iteration 125/1000 | Loss: 0.00001697
Iteration 126/1000 | Loss: 0.00001696
Iteration 127/1000 | Loss: 0.00001696
Iteration 128/1000 | Loss: 0.00001696
Iteration 129/1000 | Loss: 0.00001696
Iteration 130/1000 | Loss: 0.00001696
Iteration 131/1000 | Loss: 0.00001696
Iteration 132/1000 | Loss: 0.00001695
Iteration 133/1000 | Loss: 0.00001695
Iteration 134/1000 | Loss: 0.00001695
Iteration 135/1000 | Loss: 0.00001695
Iteration 136/1000 | Loss: 0.00001695
Iteration 137/1000 | Loss: 0.00001695
Iteration 138/1000 | Loss: 0.00001695
Iteration 139/1000 | Loss: 0.00001695
Iteration 140/1000 | Loss: 0.00001695
Iteration 141/1000 | Loss: 0.00001695
Iteration 142/1000 | Loss: 0.00001695
Iteration 143/1000 | Loss: 0.00001694
Iteration 144/1000 | Loss: 0.00001694
Iteration 145/1000 | Loss: 0.00001694
Iteration 146/1000 | Loss: 0.00001694
Iteration 147/1000 | Loss: 0.00001694
Iteration 148/1000 | Loss: 0.00001694
Iteration 149/1000 | Loss: 0.00001694
Iteration 150/1000 | Loss: 0.00001694
Iteration 151/1000 | Loss: 0.00001694
Iteration 152/1000 | Loss: 0.00001693
Iteration 153/1000 | Loss: 0.00001693
Iteration 154/1000 | Loss: 0.00001693
Iteration 155/1000 | Loss: 0.00001693
Iteration 156/1000 | Loss: 0.00001693
Iteration 157/1000 | Loss: 0.00001693
Iteration 158/1000 | Loss: 0.00001693
Iteration 159/1000 | Loss: 0.00001693
Iteration 160/1000 | Loss: 0.00001693
Iteration 161/1000 | Loss: 0.00001692
Iteration 162/1000 | Loss: 0.00001692
Iteration 163/1000 | Loss: 0.00001692
Iteration 164/1000 | Loss: 0.00001692
Iteration 165/1000 | Loss: 0.00001692
Iteration 166/1000 | Loss: 0.00001692
Iteration 167/1000 | Loss: 0.00001692
Iteration 168/1000 | Loss: 0.00001692
Iteration 169/1000 | Loss: 0.00001692
Iteration 170/1000 | Loss: 0.00001692
Iteration 171/1000 | Loss: 0.00001692
Iteration 172/1000 | Loss: 0.00001691
Iteration 173/1000 | Loss: 0.00001691
Iteration 174/1000 | Loss: 0.00001691
Iteration 175/1000 | Loss: 0.00001691
Iteration 176/1000 | Loss: 0.00001691
Iteration 177/1000 | Loss: 0.00001691
Iteration 178/1000 | Loss: 0.00001691
Iteration 179/1000 | Loss: 0.00001691
Iteration 180/1000 | Loss: 0.00001691
Iteration 181/1000 | Loss: 0.00001691
Iteration 182/1000 | Loss: 0.00001691
Iteration 183/1000 | Loss: 0.00001691
Iteration 184/1000 | Loss: 0.00001691
Iteration 185/1000 | Loss: 0.00001690
Iteration 186/1000 | Loss: 0.00001690
Iteration 187/1000 | Loss: 0.00001690
Iteration 188/1000 | Loss: 0.00001690
Iteration 189/1000 | Loss: 0.00001690
Iteration 190/1000 | Loss: 0.00001690
Iteration 191/1000 | Loss: 0.00001690
Iteration 192/1000 | Loss: 0.00001690
Iteration 193/1000 | Loss: 0.00001690
Iteration 194/1000 | Loss: 0.00001690
Iteration 195/1000 | Loss: 0.00001690
Iteration 196/1000 | Loss: 0.00001690
Iteration 197/1000 | Loss: 0.00001690
Iteration 198/1000 | Loss: 0.00001690
Iteration 199/1000 | Loss: 0.00001689
Iteration 200/1000 | Loss: 0.00001689
Iteration 201/1000 | Loss: 0.00001689
Iteration 202/1000 | Loss: 0.00001689
Iteration 203/1000 | Loss: 0.00001689
Iteration 204/1000 | Loss: 0.00001689
Iteration 205/1000 | Loss: 0.00001689
Iteration 206/1000 | Loss: 0.00001689
Iteration 207/1000 | Loss: 0.00001689
Iteration 208/1000 | Loss: 0.00001689
Iteration 209/1000 | Loss: 0.00001689
Iteration 210/1000 | Loss: 0.00001689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.6891213817871176e-05, 1.6891213817871176e-05, 1.6891213817871176e-05, 1.6891213817871176e-05, 1.6891213817871176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6891213817871176e-05

Optimization complete. Final v2v error: 3.445617914199829 mm

Highest mean error: 4.798239707946777 mm for frame 67

Lowest mean error: 2.9634368419647217 mm for frame 29

Saving results

Total time: 44.037270307540894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795007
Iteration 2/25 | Loss: 0.00205801
Iteration 3/25 | Loss: 0.00160174
Iteration 4/25 | Loss: 0.00170894
Iteration 5/25 | Loss: 0.00147449
Iteration 6/25 | Loss: 0.00153037
Iteration 7/25 | Loss: 0.00143148
Iteration 8/25 | Loss: 0.00142535
Iteration 9/25 | Loss: 0.00142784
Iteration 10/25 | Loss: 0.00142403
Iteration 11/25 | Loss: 0.00142559
Iteration 12/25 | Loss: 0.00142473
Iteration 13/25 | Loss: 0.00142444
Iteration 14/25 | Loss: 0.00142196
Iteration 15/25 | Loss: 0.00141955
Iteration 16/25 | Loss: 0.00142077
Iteration 17/25 | Loss: 0.00142344
Iteration 18/25 | Loss: 0.00143057
Iteration 19/25 | Loss: 0.00141667
Iteration 20/25 | Loss: 0.00141009
Iteration 21/25 | Loss: 0.00140888
Iteration 22/25 | Loss: 0.00140877
Iteration 23/25 | Loss: 0.00140876
Iteration 24/25 | Loss: 0.00140875
Iteration 25/25 | Loss: 0.00140875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55128539
Iteration 2/25 | Loss: 0.00104034
Iteration 3/25 | Loss: 0.00104034
Iteration 4/25 | Loss: 0.00104034
Iteration 5/25 | Loss: 0.00104033
Iteration 6/25 | Loss: 0.00104033
Iteration 7/25 | Loss: 0.00104033
Iteration 8/25 | Loss: 0.00104033
Iteration 9/25 | Loss: 0.00104033
Iteration 10/25 | Loss: 0.00104033
Iteration 11/25 | Loss: 0.00104033
Iteration 12/25 | Loss: 0.00104033
Iteration 13/25 | Loss: 0.00104033
Iteration 14/25 | Loss: 0.00104033
Iteration 15/25 | Loss: 0.00104033
Iteration 16/25 | Loss: 0.00104033
Iteration 17/25 | Loss: 0.00104033
Iteration 18/25 | Loss: 0.00104033
Iteration 19/25 | Loss: 0.00104033
Iteration 20/25 | Loss: 0.00104033
Iteration 21/25 | Loss: 0.00104033
Iteration 22/25 | Loss: 0.00104033
Iteration 23/25 | Loss: 0.00104033
Iteration 24/25 | Loss: 0.00104033
Iteration 25/25 | Loss: 0.00104033

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104033
Iteration 2/1000 | Loss: 0.00005536
Iteration 3/1000 | Loss: 0.00003777
Iteration 4/1000 | Loss: 0.00003135
Iteration 5/1000 | Loss: 0.00002859
Iteration 6/1000 | Loss: 0.00002687
Iteration 7/1000 | Loss: 0.00002592
Iteration 8/1000 | Loss: 0.00002525
Iteration 9/1000 | Loss: 0.00002482
Iteration 10/1000 | Loss: 0.00002458
Iteration 11/1000 | Loss: 0.00002438
Iteration 12/1000 | Loss: 0.00002413
Iteration 13/1000 | Loss: 0.00002397
Iteration 14/1000 | Loss: 0.00002389
Iteration 15/1000 | Loss: 0.00002387
Iteration 16/1000 | Loss: 0.00002381
Iteration 17/1000 | Loss: 0.00002378
Iteration 18/1000 | Loss: 0.00002378
Iteration 19/1000 | Loss: 0.00002377
Iteration 20/1000 | Loss: 0.00002376
Iteration 21/1000 | Loss: 0.00002376
Iteration 22/1000 | Loss: 0.00002375
Iteration 23/1000 | Loss: 0.00002375
Iteration 24/1000 | Loss: 0.00002375
Iteration 25/1000 | Loss: 0.00002374
Iteration 26/1000 | Loss: 0.00002373
Iteration 27/1000 | Loss: 0.00002373
Iteration 28/1000 | Loss: 0.00002371
Iteration 29/1000 | Loss: 0.00002370
Iteration 30/1000 | Loss: 0.00002369
Iteration 31/1000 | Loss: 0.00002368
Iteration 32/1000 | Loss: 0.00002368
Iteration 33/1000 | Loss: 0.00002367
Iteration 34/1000 | Loss: 0.00002366
Iteration 35/1000 | Loss: 0.00002366
Iteration 36/1000 | Loss: 0.00002365
Iteration 37/1000 | Loss: 0.00002361
Iteration 38/1000 | Loss: 0.00002361
Iteration 39/1000 | Loss: 0.00002361
Iteration 40/1000 | Loss: 0.00002361
Iteration 41/1000 | Loss: 0.00002360
Iteration 42/1000 | Loss: 0.00002360
Iteration 43/1000 | Loss: 0.00002359
Iteration 44/1000 | Loss: 0.00002359
Iteration 45/1000 | Loss: 0.00002358
Iteration 46/1000 | Loss: 0.00002358
Iteration 47/1000 | Loss: 0.00002358
Iteration 48/1000 | Loss: 0.00002357
Iteration 49/1000 | Loss: 0.00002357
Iteration 50/1000 | Loss: 0.00002356
Iteration 51/1000 | Loss: 0.00002356
Iteration 52/1000 | Loss: 0.00002356
Iteration 53/1000 | Loss: 0.00002355
Iteration 54/1000 | Loss: 0.00002355
Iteration 55/1000 | Loss: 0.00002355
Iteration 56/1000 | Loss: 0.00002355
Iteration 57/1000 | Loss: 0.00002354
Iteration 58/1000 | Loss: 0.00002354
Iteration 59/1000 | Loss: 0.00002354
Iteration 60/1000 | Loss: 0.00002354
Iteration 61/1000 | Loss: 0.00002354
Iteration 62/1000 | Loss: 0.00002354
Iteration 63/1000 | Loss: 0.00002354
Iteration 64/1000 | Loss: 0.00002354
Iteration 65/1000 | Loss: 0.00002353
Iteration 66/1000 | Loss: 0.00002353
Iteration 67/1000 | Loss: 0.00002352
Iteration 68/1000 | Loss: 0.00002352
Iteration 69/1000 | Loss: 0.00002352
Iteration 70/1000 | Loss: 0.00002352
Iteration 71/1000 | Loss: 0.00002352
Iteration 72/1000 | Loss: 0.00002352
Iteration 73/1000 | Loss: 0.00002352
Iteration 74/1000 | Loss: 0.00002351
Iteration 75/1000 | Loss: 0.00002351
Iteration 76/1000 | Loss: 0.00002351
Iteration 77/1000 | Loss: 0.00002350
Iteration 78/1000 | Loss: 0.00002350
Iteration 79/1000 | Loss: 0.00002350
Iteration 80/1000 | Loss: 0.00002350
Iteration 81/1000 | Loss: 0.00002349
Iteration 82/1000 | Loss: 0.00002349
Iteration 83/1000 | Loss: 0.00002349
Iteration 84/1000 | Loss: 0.00002349
Iteration 85/1000 | Loss: 0.00002349
Iteration 86/1000 | Loss: 0.00002349
Iteration 87/1000 | Loss: 0.00002349
Iteration 88/1000 | Loss: 0.00002348
Iteration 89/1000 | Loss: 0.00002348
Iteration 90/1000 | Loss: 0.00002348
Iteration 91/1000 | Loss: 0.00002348
Iteration 92/1000 | Loss: 0.00002348
Iteration 93/1000 | Loss: 0.00002347
Iteration 94/1000 | Loss: 0.00002347
Iteration 95/1000 | Loss: 0.00002347
Iteration 96/1000 | Loss: 0.00002347
Iteration 97/1000 | Loss: 0.00002346
Iteration 98/1000 | Loss: 0.00002346
Iteration 99/1000 | Loss: 0.00002346
Iteration 100/1000 | Loss: 0.00002346
Iteration 101/1000 | Loss: 0.00002346
Iteration 102/1000 | Loss: 0.00002346
Iteration 103/1000 | Loss: 0.00002346
Iteration 104/1000 | Loss: 0.00002346
Iteration 105/1000 | Loss: 0.00002345
Iteration 106/1000 | Loss: 0.00002345
Iteration 107/1000 | Loss: 0.00002345
Iteration 108/1000 | Loss: 0.00002345
Iteration 109/1000 | Loss: 0.00002344
Iteration 110/1000 | Loss: 0.00002344
Iteration 111/1000 | Loss: 0.00002344
Iteration 112/1000 | Loss: 0.00002344
Iteration 113/1000 | Loss: 0.00002343
Iteration 114/1000 | Loss: 0.00002343
Iteration 115/1000 | Loss: 0.00002343
Iteration 116/1000 | Loss: 0.00002343
Iteration 117/1000 | Loss: 0.00002342
Iteration 118/1000 | Loss: 0.00002342
Iteration 119/1000 | Loss: 0.00002342
Iteration 120/1000 | Loss: 0.00002342
Iteration 121/1000 | Loss: 0.00002342
Iteration 122/1000 | Loss: 0.00002342
Iteration 123/1000 | Loss: 0.00002342
Iteration 124/1000 | Loss: 0.00002342
Iteration 125/1000 | Loss: 0.00002342
Iteration 126/1000 | Loss: 0.00002341
Iteration 127/1000 | Loss: 0.00002341
Iteration 128/1000 | Loss: 0.00002341
Iteration 129/1000 | Loss: 0.00002341
Iteration 130/1000 | Loss: 0.00002341
Iteration 131/1000 | Loss: 0.00002341
Iteration 132/1000 | Loss: 0.00002341
Iteration 133/1000 | Loss: 0.00002340
Iteration 134/1000 | Loss: 0.00002340
Iteration 135/1000 | Loss: 0.00002340
Iteration 136/1000 | Loss: 0.00002340
Iteration 137/1000 | Loss: 0.00002340
Iteration 138/1000 | Loss: 0.00002340
Iteration 139/1000 | Loss: 0.00002340
Iteration 140/1000 | Loss: 0.00002340
Iteration 141/1000 | Loss: 0.00002340
Iteration 142/1000 | Loss: 0.00002340
Iteration 143/1000 | Loss: 0.00002340
Iteration 144/1000 | Loss: 0.00002339
Iteration 145/1000 | Loss: 0.00002339
Iteration 146/1000 | Loss: 0.00002339
Iteration 147/1000 | Loss: 0.00002339
Iteration 148/1000 | Loss: 0.00002339
Iteration 149/1000 | Loss: 0.00002339
Iteration 150/1000 | Loss: 0.00002339
Iteration 151/1000 | Loss: 0.00002338
Iteration 152/1000 | Loss: 0.00002338
Iteration 153/1000 | Loss: 0.00002338
Iteration 154/1000 | Loss: 0.00002338
Iteration 155/1000 | Loss: 0.00002338
Iteration 156/1000 | Loss: 0.00002338
Iteration 157/1000 | Loss: 0.00002338
Iteration 158/1000 | Loss: 0.00002338
Iteration 159/1000 | Loss: 0.00002337
Iteration 160/1000 | Loss: 0.00002337
Iteration 161/1000 | Loss: 0.00002337
Iteration 162/1000 | Loss: 0.00002337
Iteration 163/1000 | Loss: 0.00002337
Iteration 164/1000 | Loss: 0.00002337
Iteration 165/1000 | Loss: 0.00002337
Iteration 166/1000 | Loss: 0.00002337
Iteration 167/1000 | Loss: 0.00002337
Iteration 168/1000 | Loss: 0.00002336
Iteration 169/1000 | Loss: 0.00002336
Iteration 170/1000 | Loss: 0.00002336
Iteration 171/1000 | Loss: 0.00002336
Iteration 172/1000 | Loss: 0.00002336
Iteration 173/1000 | Loss: 0.00002336
Iteration 174/1000 | Loss: 0.00002336
Iteration 175/1000 | Loss: 0.00002336
Iteration 176/1000 | Loss: 0.00002335
Iteration 177/1000 | Loss: 0.00002335
Iteration 178/1000 | Loss: 0.00002335
Iteration 179/1000 | Loss: 0.00002335
Iteration 180/1000 | Loss: 0.00002335
Iteration 181/1000 | Loss: 0.00002335
Iteration 182/1000 | Loss: 0.00002335
Iteration 183/1000 | Loss: 0.00002335
Iteration 184/1000 | Loss: 0.00002335
Iteration 185/1000 | Loss: 0.00002335
Iteration 186/1000 | Loss: 0.00002335
Iteration 187/1000 | Loss: 0.00002335
Iteration 188/1000 | Loss: 0.00002335
Iteration 189/1000 | Loss: 0.00002335
Iteration 190/1000 | Loss: 0.00002335
Iteration 191/1000 | Loss: 0.00002335
Iteration 192/1000 | Loss: 0.00002335
Iteration 193/1000 | Loss: 0.00002335
Iteration 194/1000 | Loss: 0.00002335
Iteration 195/1000 | Loss: 0.00002335
Iteration 196/1000 | Loss: 0.00002335
Iteration 197/1000 | Loss: 0.00002335
Iteration 198/1000 | Loss: 0.00002335
Iteration 199/1000 | Loss: 0.00002335
Iteration 200/1000 | Loss: 0.00002335
Iteration 201/1000 | Loss: 0.00002335
Iteration 202/1000 | Loss: 0.00002335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [2.3349562980001792e-05, 2.3349562980001792e-05, 2.3349562980001792e-05, 2.3349562980001792e-05, 2.3349562980001792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3349562980001792e-05

Optimization complete. Final v2v error: 3.964477777481079 mm

Highest mean error: 4.657398223876953 mm for frame 42

Lowest mean error: 3.095247507095337 mm for frame 3

Saving results

Total time: 71.78473329544067
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00647486
Iteration 2/25 | Loss: 0.00156156
Iteration 3/25 | Loss: 0.00138642
Iteration 4/25 | Loss: 0.00136789
Iteration 5/25 | Loss: 0.00136616
Iteration 6/25 | Loss: 0.00136616
Iteration 7/25 | Loss: 0.00136616
Iteration 8/25 | Loss: 0.00136616
Iteration 9/25 | Loss: 0.00136616
Iteration 10/25 | Loss: 0.00136616
Iteration 11/25 | Loss: 0.00136616
Iteration 12/25 | Loss: 0.00136616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013661595294252038, 0.0013661595294252038, 0.0013661595294252038, 0.0013661595294252038, 0.0013661595294252038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013661595294252038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33544791
Iteration 2/25 | Loss: 0.00100887
Iteration 3/25 | Loss: 0.00100887
Iteration 4/25 | Loss: 0.00100887
Iteration 5/25 | Loss: 0.00100887
Iteration 6/25 | Loss: 0.00100887
Iteration 7/25 | Loss: 0.00100887
Iteration 8/25 | Loss: 0.00100887
Iteration 9/25 | Loss: 0.00100887
Iteration 10/25 | Loss: 0.00100887
Iteration 11/25 | Loss: 0.00100887
Iteration 12/25 | Loss: 0.00100887
Iteration 13/25 | Loss: 0.00100887
Iteration 14/25 | Loss: 0.00100887
Iteration 15/25 | Loss: 0.00100887
Iteration 16/25 | Loss: 0.00100887
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010088682174682617, 0.0010088682174682617, 0.0010088682174682617, 0.0010088682174682617, 0.0010088682174682617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010088682174682617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100887
Iteration 2/1000 | Loss: 0.00004646
Iteration 3/1000 | Loss: 0.00003185
Iteration 4/1000 | Loss: 0.00002385
Iteration 5/1000 | Loss: 0.00002170
Iteration 6/1000 | Loss: 0.00002072
Iteration 7/1000 | Loss: 0.00002005
Iteration 8/1000 | Loss: 0.00001954
Iteration 9/1000 | Loss: 0.00001920
Iteration 10/1000 | Loss: 0.00001896
Iteration 11/1000 | Loss: 0.00001873
Iteration 12/1000 | Loss: 0.00001853
Iteration 13/1000 | Loss: 0.00001852
Iteration 14/1000 | Loss: 0.00001831
Iteration 15/1000 | Loss: 0.00001820
Iteration 16/1000 | Loss: 0.00001818
Iteration 17/1000 | Loss: 0.00001809
Iteration 18/1000 | Loss: 0.00001798
Iteration 19/1000 | Loss: 0.00001790
Iteration 20/1000 | Loss: 0.00001784
Iteration 21/1000 | Loss: 0.00001776
Iteration 22/1000 | Loss: 0.00001773
Iteration 23/1000 | Loss: 0.00001767
Iteration 24/1000 | Loss: 0.00001767
Iteration 25/1000 | Loss: 0.00001765
Iteration 26/1000 | Loss: 0.00001765
Iteration 27/1000 | Loss: 0.00001765
Iteration 28/1000 | Loss: 0.00001765
Iteration 29/1000 | Loss: 0.00001764
Iteration 30/1000 | Loss: 0.00001764
Iteration 31/1000 | Loss: 0.00001763
Iteration 32/1000 | Loss: 0.00001763
Iteration 33/1000 | Loss: 0.00001762
Iteration 34/1000 | Loss: 0.00001762
Iteration 35/1000 | Loss: 0.00001762
Iteration 36/1000 | Loss: 0.00001762
Iteration 37/1000 | Loss: 0.00001759
Iteration 38/1000 | Loss: 0.00001757
Iteration 39/1000 | Loss: 0.00001756
Iteration 40/1000 | Loss: 0.00001756
Iteration 41/1000 | Loss: 0.00001755
Iteration 42/1000 | Loss: 0.00001755
Iteration 43/1000 | Loss: 0.00001755
Iteration 44/1000 | Loss: 0.00001754
Iteration 45/1000 | Loss: 0.00001754
Iteration 46/1000 | Loss: 0.00001754
Iteration 47/1000 | Loss: 0.00001754
Iteration 48/1000 | Loss: 0.00001754
Iteration 49/1000 | Loss: 0.00001754
Iteration 50/1000 | Loss: 0.00001754
Iteration 51/1000 | Loss: 0.00001754
Iteration 52/1000 | Loss: 0.00001754
Iteration 53/1000 | Loss: 0.00001754
Iteration 54/1000 | Loss: 0.00001753
Iteration 55/1000 | Loss: 0.00001753
Iteration 56/1000 | Loss: 0.00001753
Iteration 57/1000 | Loss: 0.00001753
Iteration 58/1000 | Loss: 0.00001753
Iteration 59/1000 | Loss: 0.00001753
Iteration 60/1000 | Loss: 0.00001753
Iteration 61/1000 | Loss: 0.00001752
Iteration 62/1000 | Loss: 0.00001752
Iteration 63/1000 | Loss: 0.00001751
Iteration 64/1000 | Loss: 0.00001751
Iteration 65/1000 | Loss: 0.00001751
Iteration 66/1000 | Loss: 0.00001751
Iteration 67/1000 | Loss: 0.00001751
Iteration 68/1000 | Loss: 0.00001751
Iteration 69/1000 | Loss: 0.00001751
Iteration 70/1000 | Loss: 0.00001751
Iteration 71/1000 | Loss: 0.00001751
Iteration 72/1000 | Loss: 0.00001750
Iteration 73/1000 | Loss: 0.00001750
Iteration 74/1000 | Loss: 0.00001750
Iteration 75/1000 | Loss: 0.00001750
Iteration 76/1000 | Loss: 0.00001750
Iteration 77/1000 | Loss: 0.00001749
Iteration 78/1000 | Loss: 0.00001749
Iteration 79/1000 | Loss: 0.00001749
Iteration 80/1000 | Loss: 0.00001749
Iteration 81/1000 | Loss: 0.00001749
Iteration 82/1000 | Loss: 0.00001749
Iteration 83/1000 | Loss: 0.00001748
Iteration 84/1000 | Loss: 0.00001748
Iteration 85/1000 | Loss: 0.00001748
Iteration 86/1000 | Loss: 0.00001748
Iteration 87/1000 | Loss: 0.00001748
Iteration 88/1000 | Loss: 0.00001748
Iteration 89/1000 | Loss: 0.00001748
Iteration 90/1000 | Loss: 0.00001747
Iteration 91/1000 | Loss: 0.00001747
Iteration 92/1000 | Loss: 0.00001747
Iteration 93/1000 | Loss: 0.00001747
Iteration 94/1000 | Loss: 0.00001747
Iteration 95/1000 | Loss: 0.00001747
Iteration 96/1000 | Loss: 0.00001747
Iteration 97/1000 | Loss: 0.00001747
Iteration 98/1000 | Loss: 0.00001747
Iteration 99/1000 | Loss: 0.00001746
Iteration 100/1000 | Loss: 0.00001746
Iteration 101/1000 | Loss: 0.00001746
Iteration 102/1000 | Loss: 0.00001745
Iteration 103/1000 | Loss: 0.00001745
Iteration 104/1000 | Loss: 0.00001745
Iteration 105/1000 | Loss: 0.00001744
Iteration 106/1000 | Loss: 0.00001744
Iteration 107/1000 | Loss: 0.00001744
Iteration 108/1000 | Loss: 0.00001744
Iteration 109/1000 | Loss: 0.00001744
Iteration 110/1000 | Loss: 0.00001744
Iteration 111/1000 | Loss: 0.00001743
Iteration 112/1000 | Loss: 0.00001743
Iteration 113/1000 | Loss: 0.00001743
Iteration 114/1000 | Loss: 0.00001743
Iteration 115/1000 | Loss: 0.00001743
Iteration 116/1000 | Loss: 0.00001742
Iteration 117/1000 | Loss: 0.00001742
Iteration 118/1000 | Loss: 0.00001742
Iteration 119/1000 | Loss: 0.00001742
Iteration 120/1000 | Loss: 0.00001741
Iteration 121/1000 | Loss: 0.00001741
Iteration 122/1000 | Loss: 0.00001741
Iteration 123/1000 | Loss: 0.00001741
Iteration 124/1000 | Loss: 0.00001741
Iteration 125/1000 | Loss: 0.00001741
Iteration 126/1000 | Loss: 0.00001741
Iteration 127/1000 | Loss: 0.00001741
Iteration 128/1000 | Loss: 0.00001740
Iteration 129/1000 | Loss: 0.00001740
Iteration 130/1000 | Loss: 0.00001740
Iteration 131/1000 | Loss: 0.00001740
Iteration 132/1000 | Loss: 0.00001740
Iteration 133/1000 | Loss: 0.00001740
Iteration 134/1000 | Loss: 0.00001740
Iteration 135/1000 | Loss: 0.00001740
Iteration 136/1000 | Loss: 0.00001740
Iteration 137/1000 | Loss: 0.00001740
Iteration 138/1000 | Loss: 0.00001740
Iteration 139/1000 | Loss: 0.00001739
Iteration 140/1000 | Loss: 0.00001739
Iteration 141/1000 | Loss: 0.00001739
Iteration 142/1000 | Loss: 0.00001739
Iteration 143/1000 | Loss: 0.00001739
Iteration 144/1000 | Loss: 0.00001739
Iteration 145/1000 | Loss: 0.00001739
Iteration 146/1000 | Loss: 0.00001739
Iteration 147/1000 | Loss: 0.00001739
Iteration 148/1000 | Loss: 0.00001739
Iteration 149/1000 | Loss: 0.00001739
Iteration 150/1000 | Loss: 0.00001738
Iteration 151/1000 | Loss: 0.00001738
Iteration 152/1000 | Loss: 0.00001738
Iteration 153/1000 | Loss: 0.00001738
Iteration 154/1000 | Loss: 0.00001738
Iteration 155/1000 | Loss: 0.00001738
Iteration 156/1000 | Loss: 0.00001738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.73846201505512e-05, 1.73846201505512e-05, 1.73846201505512e-05, 1.73846201505512e-05, 1.73846201505512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.73846201505512e-05

Optimization complete. Final v2v error: 3.5105233192443848 mm

Highest mean error: 3.869218349456787 mm for frame 108

Lowest mean error: 3.042541027069092 mm for frame 92

Saving results

Total time: 40.94818663597107
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412033
Iteration 2/25 | Loss: 0.00134739
Iteration 3/25 | Loss: 0.00127173
Iteration 4/25 | Loss: 0.00125830
Iteration 5/25 | Loss: 0.00125414
Iteration 6/25 | Loss: 0.00125390
Iteration 7/25 | Loss: 0.00125390
Iteration 8/25 | Loss: 0.00125390
Iteration 9/25 | Loss: 0.00125390
Iteration 10/25 | Loss: 0.00125390
Iteration 11/25 | Loss: 0.00125390
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012538955779746175, 0.0012538955779746175, 0.0012538955779746175, 0.0012538955779746175, 0.0012538955779746175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012538955779746175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80508661
Iteration 2/25 | Loss: 0.00079026
Iteration 3/25 | Loss: 0.00079026
Iteration 4/25 | Loss: 0.00079026
Iteration 5/25 | Loss: 0.00079026
Iteration 6/25 | Loss: 0.00079026
Iteration 7/25 | Loss: 0.00079025
Iteration 8/25 | Loss: 0.00079025
Iteration 9/25 | Loss: 0.00079025
Iteration 10/25 | Loss: 0.00079025
Iteration 11/25 | Loss: 0.00079025
Iteration 12/25 | Loss: 0.00079025
Iteration 13/25 | Loss: 0.00079025
Iteration 14/25 | Loss: 0.00079025
Iteration 15/25 | Loss: 0.00079025
Iteration 16/25 | Loss: 0.00079025
Iteration 17/25 | Loss: 0.00079025
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007902539218775928, 0.0007902539218775928, 0.0007902539218775928, 0.0007902539218775928, 0.0007902539218775928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007902539218775928

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079025
Iteration 2/1000 | Loss: 0.00002297
Iteration 3/1000 | Loss: 0.00001849
Iteration 4/1000 | Loss: 0.00001727
Iteration 5/1000 | Loss: 0.00001647
Iteration 6/1000 | Loss: 0.00001595
Iteration 7/1000 | Loss: 0.00001552
Iteration 8/1000 | Loss: 0.00001538
Iteration 9/1000 | Loss: 0.00001513
Iteration 10/1000 | Loss: 0.00001486
Iteration 11/1000 | Loss: 0.00001478
Iteration 12/1000 | Loss: 0.00001469
Iteration 13/1000 | Loss: 0.00001469
Iteration 14/1000 | Loss: 0.00001463
Iteration 15/1000 | Loss: 0.00001451
Iteration 16/1000 | Loss: 0.00001451
Iteration 17/1000 | Loss: 0.00001450
Iteration 18/1000 | Loss: 0.00001439
Iteration 19/1000 | Loss: 0.00001439
Iteration 20/1000 | Loss: 0.00001438
Iteration 21/1000 | Loss: 0.00001436
Iteration 22/1000 | Loss: 0.00001432
Iteration 23/1000 | Loss: 0.00001429
Iteration 24/1000 | Loss: 0.00001425
Iteration 25/1000 | Loss: 0.00001425
Iteration 26/1000 | Loss: 0.00001423
Iteration 27/1000 | Loss: 0.00001423
Iteration 28/1000 | Loss: 0.00001422
Iteration 29/1000 | Loss: 0.00001422
Iteration 30/1000 | Loss: 0.00001422
Iteration 31/1000 | Loss: 0.00001422
Iteration 32/1000 | Loss: 0.00001422
Iteration 33/1000 | Loss: 0.00001422
Iteration 34/1000 | Loss: 0.00001422
Iteration 35/1000 | Loss: 0.00001422
Iteration 36/1000 | Loss: 0.00001422
Iteration 37/1000 | Loss: 0.00001418
Iteration 38/1000 | Loss: 0.00001417
Iteration 39/1000 | Loss: 0.00001417
Iteration 40/1000 | Loss: 0.00001416
Iteration 41/1000 | Loss: 0.00001416
Iteration 42/1000 | Loss: 0.00001412
Iteration 43/1000 | Loss: 0.00001411
Iteration 44/1000 | Loss: 0.00001409
Iteration 45/1000 | Loss: 0.00001409
Iteration 46/1000 | Loss: 0.00001408
Iteration 47/1000 | Loss: 0.00001406
Iteration 48/1000 | Loss: 0.00001406
Iteration 49/1000 | Loss: 0.00001405
Iteration 50/1000 | Loss: 0.00001405
Iteration 51/1000 | Loss: 0.00001405
Iteration 52/1000 | Loss: 0.00001404
Iteration 53/1000 | Loss: 0.00001404
Iteration 54/1000 | Loss: 0.00001404
Iteration 55/1000 | Loss: 0.00001404
Iteration 56/1000 | Loss: 0.00001404
Iteration 57/1000 | Loss: 0.00001403
Iteration 58/1000 | Loss: 0.00001403
Iteration 59/1000 | Loss: 0.00001403
Iteration 60/1000 | Loss: 0.00001403
Iteration 61/1000 | Loss: 0.00001403
Iteration 62/1000 | Loss: 0.00001402
Iteration 63/1000 | Loss: 0.00001402
Iteration 64/1000 | Loss: 0.00001402
Iteration 65/1000 | Loss: 0.00001402
Iteration 66/1000 | Loss: 0.00001402
Iteration 67/1000 | Loss: 0.00001402
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001402
Iteration 71/1000 | Loss: 0.00001401
Iteration 72/1000 | Loss: 0.00001401
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001401
Iteration 75/1000 | Loss: 0.00001401
Iteration 76/1000 | Loss: 0.00001401
Iteration 77/1000 | Loss: 0.00001401
Iteration 78/1000 | Loss: 0.00001401
Iteration 79/1000 | Loss: 0.00001401
Iteration 80/1000 | Loss: 0.00001401
Iteration 81/1000 | Loss: 0.00001400
Iteration 82/1000 | Loss: 0.00001400
Iteration 83/1000 | Loss: 0.00001399
Iteration 84/1000 | Loss: 0.00001399
Iteration 85/1000 | Loss: 0.00001399
Iteration 86/1000 | Loss: 0.00001398
Iteration 87/1000 | Loss: 0.00001398
Iteration 88/1000 | Loss: 0.00001398
Iteration 89/1000 | Loss: 0.00001398
Iteration 90/1000 | Loss: 0.00001397
Iteration 91/1000 | Loss: 0.00001397
Iteration 92/1000 | Loss: 0.00001397
Iteration 93/1000 | Loss: 0.00001397
Iteration 94/1000 | Loss: 0.00001396
Iteration 95/1000 | Loss: 0.00001396
Iteration 96/1000 | Loss: 0.00001396
Iteration 97/1000 | Loss: 0.00001396
Iteration 98/1000 | Loss: 0.00001396
Iteration 99/1000 | Loss: 0.00001395
Iteration 100/1000 | Loss: 0.00001395
Iteration 101/1000 | Loss: 0.00001395
Iteration 102/1000 | Loss: 0.00001394
Iteration 103/1000 | Loss: 0.00001394
Iteration 104/1000 | Loss: 0.00001393
Iteration 105/1000 | Loss: 0.00001393
Iteration 106/1000 | Loss: 0.00001393
Iteration 107/1000 | Loss: 0.00001392
Iteration 108/1000 | Loss: 0.00001392
Iteration 109/1000 | Loss: 0.00001392
Iteration 110/1000 | Loss: 0.00001391
Iteration 111/1000 | Loss: 0.00001391
Iteration 112/1000 | Loss: 0.00001391
Iteration 113/1000 | Loss: 0.00001390
Iteration 114/1000 | Loss: 0.00001390
Iteration 115/1000 | Loss: 0.00001390
Iteration 116/1000 | Loss: 0.00001389
Iteration 117/1000 | Loss: 0.00001389
Iteration 118/1000 | Loss: 0.00001389
Iteration 119/1000 | Loss: 0.00001389
Iteration 120/1000 | Loss: 0.00001389
Iteration 121/1000 | Loss: 0.00001389
Iteration 122/1000 | Loss: 0.00001388
Iteration 123/1000 | Loss: 0.00001388
Iteration 124/1000 | Loss: 0.00001388
Iteration 125/1000 | Loss: 0.00001387
Iteration 126/1000 | Loss: 0.00001387
Iteration 127/1000 | Loss: 0.00001387
Iteration 128/1000 | Loss: 0.00001387
Iteration 129/1000 | Loss: 0.00001386
Iteration 130/1000 | Loss: 0.00001386
Iteration 131/1000 | Loss: 0.00001386
Iteration 132/1000 | Loss: 0.00001386
Iteration 133/1000 | Loss: 0.00001386
Iteration 134/1000 | Loss: 0.00001386
Iteration 135/1000 | Loss: 0.00001386
Iteration 136/1000 | Loss: 0.00001386
Iteration 137/1000 | Loss: 0.00001386
Iteration 138/1000 | Loss: 0.00001386
Iteration 139/1000 | Loss: 0.00001386
Iteration 140/1000 | Loss: 0.00001386
Iteration 141/1000 | Loss: 0.00001385
Iteration 142/1000 | Loss: 0.00001385
Iteration 143/1000 | Loss: 0.00001385
Iteration 144/1000 | Loss: 0.00001385
Iteration 145/1000 | Loss: 0.00001385
Iteration 146/1000 | Loss: 0.00001385
Iteration 147/1000 | Loss: 0.00001385
Iteration 148/1000 | Loss: 0.00001385
Iteration 149/1000 | Loss: 0.00001385
Iteration 150/1000 | Loss: 0.00001385
Iteration 151/1000 | Loss: 0.00001385
Iteration 152/1000 | Loss: 0.00001385
Iteration 153/1000 | Loss: 0.00001385
Iteration 154/1000 | Loss: 0.00001385
Iteration 155/1000 | Loss: 0.00001385
Iteration 156/1000 | Loss: 0.00001385
Iteration 157/1000 | Loss: 0.00001385
Iteration 158/1000 | Loss: 0.00001385
Iteration 159/1000 | Loss: 0.00001385
Iteration 160/1000 | Loss: 0.00001385
Iteration 161/1000 | Loss: 0.00001385
Iteration 162/1000 | Loss: 0.00001385
Iteration 163/1000 | Loss: 0.00001385
Iteration 164/1000 | Loss: 0.00001385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.3846130059391726e-05, 1.3846130059391726e-05, 1.3846130059391726e-05, 1.3846130059391726e-05, 1.3846130059391726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3846130059391726e-05

Optimization complete. Final v2v error: 3.175475597381592 mm

Highest mean error: 3.510406255722046 mm for frame 130

Lowest mean error: 3.0281691551208496 mm for frame 114

Saving results

Total time: 40.08929967880249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873234
Iteration 2/25 | Loss: 0.00194428
Iteration 3/25 | Loss: 0.00156995
Iteration 4/25 | Loss: 0.00154857
Iteration 5/25 | Loss: 0.00154249
Iteration 6/25 | Loss: 0.00154121
Iteration 7/25 | Loss: 0.00154121
Iteration 8/25 | Loss: 0.00154121
Iteration 9/25 | Loss: 0.00154121
Iteration 10/25 | Loss: 0.00154121
Iteration 11/25 | Loss: 0.00154121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015412127831950784, 0.0015412127831950784, 0.0015412127831950784, 0.0015412127831950784, 0.0015412127831950784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015412127831950784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91413158
Iteration 2/25 | Loss: 0.00100365
Iteration 3/25 | Loss: 0.00100365
Iteration 4/25 | Loss: 0.00100365
Iteration 5/25 | Loss: 0.00100365
Iteration 6/25 | Loss: 0.00100365
Iteration 7/25 | Loss: 0.00100365
Iteration 8/25 | Loss: 0.00100365
Iteration 9/25 | Loss: 0.00100365
Iteration 10/25 | Loss: 0.00100365
Iteration 11/25 | Loss: 0.00100365
Iteration 12/25 | Loss: 0.00100365
Iteration 13/25 | Loss: 0.00100365
Iteration 14/25 | Loss: 0.00100365
Iteration 15/25 | Loss: 0.00100365
Iteration 16/25 | Loss: 0.00100365
Iteration 17/25 | Loss: 0.00100365
Iteration 18/25 | Loss: 0.00100365
Iteration 19/25 | Loss: 0.00100365
Iteration 20/25 | Loss: 0.00100365
Iteration 21/25 | Loss: 0.00100365
Iteration 22/25 | Loss: 0.00100365
Iteration 23/25 | Loss: 0.00100365
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010036489693447948, 0.0010036489693447948, 0.0010036489693447948, 0.0010036489693447948, 0.0010036489693447948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010036489693447948

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100365
Iteration 2/1000 | Loss: 0.00009641
Iteration 3/1000 | Loss: 0.00006348
Iteration 4/1000 | Loss: 0.00005516
Iteration 5/1000 | Loss: 0.00005246
Iteration 6/1000 | Loss: 0.00005054
Iteration 7/1000 | Loss: 0.00004931
Iteration 8/1000 | Loss: 0.00004799
Iteration 9/1000 | Loss: 0.00004692
Iteration 10/1000 | Loss: 0.00004598
Iteration 11/1000 | Loss: 0.00004517
Iteration 12/1000 | Loss: 0.00004473
Iteration 13/1000 | Loss: 0.00004416
Iteration 14/1000 | Loss: 0.00004359
Iteration 15/1000 | Loss: 0.00004306
Iteration 16/1000 | Loss: 0.00004266
Iteration 17/1000 | Loss: 0.00004231
Iteration 18/1000 | Loss: 0.00004191
Iteration 19/1000 | Loss: 0.00004162
Iteration 20/1000 | Loss: 0.00004142
Iteration 21/1000 | Loss: 0.00004122
Iteration 22/1000 | Loss: 0.00004101
Iteration 23/1000 | Loss: 0.00004077
Iteration 24/1000 | Loss: 0.00004059
Iteration 25/1000 | Loss: 0.00004044
Iteration 26/1000 | Loss: 0.00004033
Iteration 27/1000 | Loss: 0.00004028
Iteration 28/1000 | Loss: 0.00004027
Iteration 29/1000 | Loss: 0.00004022
Iteration 30/1000 | Loss: 0.00004019
Iteration 31/1000 | Loss: 0.00004019
Iteration 32/1000 | Loss: 0.00004017
Iteration 33/1000 | Loss: 0.00004015
Iteration 34/1000 | Loss: 0.00004015
Iteration 35/1000 | Loss: 0.00004014
Iteration 36/1000 | Loss: 0.00004014
Iteration 37/1000 | Loss: 0.00004013
Iteration 38/1000 | Loss: 0.00004012
Iteration 39/1000 | Loss: 0.00004011
Iteration 40/1000 | Loss: 0.00004010
Iteration 41/1000 | Loss: 0.00004010
Iteration 42/1000 | Loss: 0.00004010
Iteration 43/1000 | Loss: 0.00004010
Iteration 44/1000 | Loss: 0.00004009
Iteration 45/1000 | Loss: 0.00004008
Iteration 46/1000 | Loss: 0.00004008
Iteration 47/1000 | Loss: 0.00004008
Iteration 48/1000 | Loss: 0.00004006
Iteration 49/1000 | Loss: 0.00004005
Iteration 50/1000 | Loss: 0.00004004
Iteration 51/1000 | Loss: 0.00004003
Iteration 52/1000 | Loss: 0.00004001
Iteration 53/1000 | Loss: 0.00004000
Iteration 54/1000 | Loss: 0.00003999
Iteration 55/1000 | Loss: 0.00003999
Iteration 56/1000 | Loss: 0.00003999
Iteration 57/1000 | Loss: 0.00003998
Iteration 58/1000 | Loss: 0.00003997
Iteration 59/1000 | Loss: 0.00003997
Iteration 60/1000 | Loss: 0.00003996
Iteration 61/1000 | Loss: 0.00003996
Iteration 62/1000 | Loss: 0.00003996
Iteration 63/1000 | Loss: 0.00003996
Iteration 64/1000 | Loss: 0.00003996
Iteration 65/1000 | Loss: 0.00003995
Iteration 66/1000 | Loss: 0.00003995
Iteration 67/1000 | Loss: 0.00003995
Iteration 68/1000 | Loss: 0.00003994
Iteration 69/1000 | Loss: 0.00003993
Iteration 70/1000 | Loss: 0.00003992
Iteration 71/1000 | Loss: 0.00003992
Iteration 72/1000 | Loss: 0.00003992
Iteration 73/1000 | Loss: 0.00003992
Iteration 74/1000 | Loss: 0.00003992
Iteration 75/1000 | Loss: 0.00003992
Iteration 76/1000 | Loss: 0.00003992
Iteration 77/1000 | Loss: 0.00003992
Iteration 78/1000 | Loss: 0.00003992
Iteration 79/1000 | Loss: 0.00003992
Iteration 80/1000 | Loss: 0.00003992
Iteration 81/1000 | Loss: 0.00003991
Iteration 82/1000 | Loss: 0.00003991
Iteration 83/1000 | Loss: 0.00003991
Iteration 84/1000 | Loss: 0.00003991
Iteration 85/1000 | Loss: 0.00003990
Iteration 86/1000 | Loss: 0.00003990
Iteration 87/1000 | Loss: 0.00003990
Iteration 88/1000 | Loss: 0.00003989
Iteration 89/1000 | Loss: 0.00003989
Iteration 90/1000 | Loss: 0.00003989
Iteration 91/1000 | Loss: 0.00003989
Iteration 92/1000 | Loss: 0.00003989
Iteration 93/1000 | Loss: 0.00003989
Iteration 94/1000 | Loss: 0.00003988
Iteration 95/1000 | Loss: 0.00003988
Iteration 96/1000 | Loss: 0.00003988
Iteration 97/1000 | Loss: 0.00003988
Iteration 98/1000 | Loss: 0.00003988
Iteration 99/1000 | Loss: 0.00003988
Iteration 100/1000 | Loss: 0.00003988
Iteration 101/1000 | Loss: 0.00003988
Iteration 102/1000 | Loss: 0.00003987
Iteration 103/1000 | Loss: 0.00003987
Iteration 104/1000 | Loss: 0.00003987
Iteration 105/1000 | Loss: 0.00003987
Iteration 106/1000 | Loss: 0.00003987
Iteration 107/1000 | Loss: 0.00003987
Iteration 108/1000 | Loss: 0.00003987
Iteration 109/1000 | Loss: 0.00003987
Iteration 110/1000 | Loss: 0.00003986
Iteration 111/1000 | Loss: 0.00003986
Iteration 112/1000 | Loss: 0.00003986
Iteration 113/1000 | Loss: 0.00003986
Iteration 114/1000 | Loss: 0.00003986
Iteration 115/1000 | Loss: 0.00003986
Iteration 116/1000 | Loss: 0.00003986
Iteration 117/1000 | Loss: 0.00003986
Iteration 118/1000 | Loss: 0.00003986
Iteration 119/1000 | Loss: 0.00003986
Iteration 120/1000 | Loss: 0.00003985
Iteration 121/1000 | Loss: 0.00003985
Iteration 122/1000 | Loss: 0.00003985
Iteration 123/1000 | Loss: 0.00003985
Iteration 124/1000 | Loss: 0.00003985
Iteration 125/1000 | Loss: 0.00003985
Iteration 126/1000 | Loss: 0.00003985
Iteration 127/1000 | Loss: 0.00003985
Iteration 128/1000 | Loss: 0.00003985
Iteration 129/1000 | Loss: 0.00003985
Iteration 130/1000 | Loss: 0.00003985
Iteration 131/1000 | Loss: 0.00003984
Iteration 132/1000 | Loss: 0.00003984
Iteration 133/1000 | Loss: 0.00003984
Iteration 134/1000 | Loss: 0.00003984
Iteration 135/1000 | Loss: 0.00003984
Iteration 136/1000 | Loss: 0.00003984
Iteration 137/1000 | Loss: 0.00003984
Iteration 138/1000 | Loss: 0.00003984
Iteration 139/1000 | Loss: 0.00003984
Iteration 140/1000 | Loss: 0.00003983
Iteration 141/1000 | Loss: 0.00003983
Iteration 142/1000 | Loss: 0.00003983
Iteration 143/1000 | Loss: 0.00003983
Iteration 144/1000 | Loss: 0.00003983
Iteration 145/1000 | Loss: 0.00003983
Iteration 146/1000 | Loss: 0.00003983
Iteration 147/1000 | Loss: 0.00003983
Iteration 148/1000 | Loss: 0.00003983
Iteration 149/1000 | Loss: 0.00003983
Iteration 150/1000 | Loss: 0.00003983
Iteration 151/1000 | Loss: 0.00003983
Iteration 152/1000 | Loss: 0.00003982
Iteration 153/1000 | Loss: 0.00003982
Iteration 154/1000 | Loss: 0.00003982
Iteration 155/1000 | Loss: 0.00003982
Iteration 156/1000 | Loss: 0.00003982
Iteration 157/1000 | Loss: 0.00003982
Iteration 158/1000 | Loss: 0.00003982
Iteration 159/1000 | Loss: 0.00003982
Iteration 160/1000 | Loss: 0.00003982
Iteration 161/1000 | Loss: 0.00003982
Iteration 162/1000 | Loss: 0.00003982
Iteration 163/1000 | Loss: 0.00003981
Iteration 164/1000 | Loss: 0.00003981
Iteration 165/1000 | Loss: 0.00003981
Iteration 166/1000 | Loss: 0.00003981
Iteration 167/1000 | Loss: 0.00003981
Iteration 168/1000 | Loss: 0.00003981
Iteration 169/1000 | Loss: 0.00003981
Iteration 170/1000 | Loss: 0.00003981
Iteration 171/1000 | Loss: 0.00003981
Iteration 172/1000 | Loss: 0.00003981
Iteration 173/1000 | Loss: 0.00003981
Iteration 174/1000 | Loss: 0.00003981
Iteration 175/1000 | Loss: 0.00003981
Iteration 176/1000 | Loss: 0.00003981
Iteration 177/1000 | Loss: 0.00003981
Iteration 178/1000 | Loss: 0.00003980
Iteration 179/1000 | Loss: 0.00003980
Iteration 180/1000 | Loss: 0.00003980
Iteration 181/1000 | Loss: 0.00003980
Iteration 182/1000 | Loss: 0.00003980
Iteration 183/1000 | Loss: 0.00003980
Iteration 184/1000 | Loss: 0.00003980
Iteration 185/1000 | Loss: 0.00003980
Iteration 186/1000 | Loss: 0.00003979
Iteration 187/1000 | Loss: 0.00003979
Iteration 188/1000 | Loss: 0.00003979
Iteration 189/1000 | Loss: 0.00003979
Iteration 190/1000 | Loss: 0.00003979
Iteration 191/1000 | Loss: 0.00003979
Iteration 192/1000 | Loss: 0.00003979
Iteration 193/1000 | Loss: 0.00003979
Iteration 194/1000 | Loss: 0.00003979
Iteration 195/1000 | Loss: 0.00003979
Iteration 196/1000 | Loss: 0.00003979
Iteration 197/1000 | Loss: 0.00003979
Iteration 198/1000 | Loss: 0.00003979
Iteration 199/1000 | Loss: 0.00003978
Iteration 200/1000 | Loss: 0.00003978
Iteration 201/1000 | Loss: 0.00003978
Iteration 202/1000 | Loss: 0.00003978
Iteration 203/1000 | Loss: 0.00003978
Iteration 204/1000 | Loss: 0.00003978
Iteration 205/1000 | Loss: 0.00003978
Iteration 206/1000 | Loss: 0.00003978
Iteration 207/1000 | Loss: 0.00003978
Iteration 208/1000 | Loss: 0.00003977
Iteration 209/1000 | Loss: 0.00003977
Iteration 210/1000 | Loss: 0.00003977
Iteration 211/1000 | Loss: 0.00003977
Iteration 212/1000 | Loss: 0.00003977
Iteration 213/1000 | Loss: 0.00003977
Iteration 214/1000 | Loss: 0.00003977
Iteration 215/1000 | Loss: 0.00003977
Iteration 216/1000 | Loss: 0.00003977
Iteration 217/1000 | Loss: 0.00003977
Iteration 218/1000 | Loss: 0.00003977
Iteration 219/1000 | Loss: 0.00003977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [3.9771435695001855e-05, 3.9771435695001855e-05, 3.9771435695001855e-05, 3.9771435695001855e-05, 3.9771435695001855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9771435695001855e-05

Optimization complete. Final v2v error: 5.270681858062744 mm

Highest mean error: 5.802400588989258 mm for frame 19

Lowest mean error: 4.797607421875 mm for frame 157

Saving results

Total time: 70.27133321762085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746852
Iteration 2/25 | Loss: 0.00178735
Iteration 3/25 | Loss: 0.00156206
Iteration 4/25 | Loss: 0.00154441
Iteration 5/25 | Loss: 0.00153979
Iteration 6/25 | Loss: 0.00153964
Iteration 7/25 | Loss: 0.00153964
Iteration 8/25 | Loss: 0.00153964
Iteration 9/25 | Loss: 0.00153964
Iteration 10/25 | Loss: 0.00153964
Iteration 11/25 | Loss: 0.00153964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015396422240883112, 0.0015396422240883112, 0.0015396422240883112, 0.0015396422240883112, 0.0015396422240883112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015396422240883112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.25396770
Iteration 2/25 | Loss: 0.00121718
Iteration 3/25 | Loss: 0.00121718
Iteration 4/25 | Loss: 0.00121717
Iteration 5/25 | Loss: 0.00121717
Iteration 6/25 | Loss: 0.00121717
Iteration 7/25 | Loss: 0.00121717
Iteration 8/25 | Loss: 0.00121717
Iteration 9/25 | Loss: 0.00121717
Iteration 10/25 | Loss: 0.00121717
Iteration 11/25 | Loss: 0.00121717
Iteration 12/25 | Loss: 0.00121717
Iteration 13/25 | Loss: 0.00121717
Iteration 14/25 | Loss: 0.00121717
Iteration 15/25 | Loss: 0.00121717
Iteration 16/25 | Loss: 0.00121717
Iteration 17/25 | Loss: 0.00121717
Iteration 18/25 | Loss: 0.00121717
Iteration 19/25 | Loss: 0.00121717
Iteration 20/25 | Loss: 0.00121717
Iteration 21/25 | Loss: 0.00121717
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012171735288575292, 0.0012171735288575292, 0.0012171735288575292, 0.0012171735288575292, 0.0012171735288575292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012171735288575292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121717
Iteration 2/1000 | Loss: 0.00007197
Iteration 3/1000 | Loss: 0.00004829
Iteration 4/1000 | Loss: 0.00004359
Iteration 5/1000 | Loss: 0.00004159
Iteration 6/1000 | Loss: 0.00004030
Iteration 7/1000 | Loss: 0.00003927
Iteration 8/1000 | Loss: 0.00003862
Iteration 9/1000 | Loss: 0.00003817
Iteration 10/1000 | Loss: 0.00003780
Iteration 11/1000 | Loss: 0.00003751
Iteration 12/1000 | Loss: 0.00003727
Iteration 13/1000 | Loss: 0.00003707
Iteration 14/1000 | Loss: 0.00003702
Iteration 15/1000 | Loss: 0.00003687
Iteration 16/1000 | Loss: 0.00003682
Iteration 17/1000 | Loss: 0.00003668
Iteration 18/1000 | Loss: 0.00003668
Iteration 19/1000 | Loss: 0.00003667
Iteration 20/1000 | Loss: 0.00003655
Iteration 21/1000 | Loss: 0.00003654
Iteration 22/1000 | Loss: 0.00003646
Iteration 23/1000 | Loss: 0.00003643
Iteration 24/1000 | Loss: 0.00003642
Iteration 25/1000 | Loss: 0.00003636
Iteration 26/1000 | Loss: 0.00003630
Iteration 27/1000 | Loss: 0.00003626
Iteration 28/1000 | Loss: 0.00003626
Iteration 29/1000 | Loss: 0.00003626
Iteration 30/1000 | Loss: 0.00003626
Iteration 31/1000 | Loss: 0.00003626
Iteration 32/1000 | Loss: 0.00003626
Iteration 33/1000 | Loss: 0.00003626
Iteration 34/1000 | Loss: 0.00003625
Iteration 35/1000 | Loss: 0.00003625
Iteration 36/1000 | Loss: 0.00003625
Iteration 37/1000 | Loss: 0.00003625
Iteration 38/1000 | Loss: 0.00003625
Iteration 39/1000 | Loss: 0.00003623
Iteration 40/1000 | Loss: 0.00003623
Iteration 41/1000 | Loss: 0.00003623
Iteration 42/1000 | Loss: 0.00003622
Iteration 43/1000 | Loss: 0.00003622
Iteration 44/1000 | Loss: 0.00003621
Iteration 45/1000 | Loss: 0.00003621
Iteration 46/1000 | Loss: 0.00003621
Iteration 47/1000 | Loss: 0.00003621
Iteration 48/1000 | Loss: 0.00003621
Iteration 49/1000 | Loss: 0.00003621
Iteration 50/1000 | Loss: 0.00003621
Iteration 51/1000 | Loss: 0.00003621
Iteration 52/1000 | Loss: 0.00003621
Iteration 53/1000 | Loss: 0.00003621
Iteration 54/1000 | Loss: 0.00003620
Iteration 55/1000 | Loss: 0.00003620
Iteration 56/1000 | Loss: 0.00003620
Iteration 57/1000 | Loss: 0.00003620
Iteration 58/1000 | Loss: 0.00003620
Iteration 59/1000 | Loss: 0.00003620
Iteration 60/1000 | Loss: 0.00003620
Iteration 61/1000 | Loss: 0.00003619
Iteration 62/1000 | Loss: 0.00003619
Iteration 63/1000 | Loss: 0.00003619
Iteration 64/1000 | Loss: 0.00003619
Iteration 65/1000 | Loss: 0.00003619
Iteration 66/1000 | Loss: 0.00003619
Iteration 67/1000 | Loss: 0.00003619
Iteration 68/1000 | Loss: 0.00003619
Iteration 69/1000 | Loss: 0.00003618
Iteration 70/1000 | Loss: 0.00003618
Iteration 71/1000 | Loss: 0.00003618
Iteration 72/1000 | Loss: 0.00003618
Iteration 73/1000 | Loss: 0.00003617
Iteration 74/1000 | Loss: 0.00003617
Iteration 75/1000 | Loss: 0.00003617
Iteration 76/1000 | Loss: 0.00003617
Iteration 77/1000 | Loss: 0.00003617
Iteration 78/1000 | Loss: 0.00003616
Iteration 79/1000 | Loss: 0.00003616
Iteration 80/1000 | Loss: 0.00003616
Iteration 81/1000 | Loss: 0.00003615
Iteration 82/1000 | Loss: 0.00003615
Iteration 83/1000 | Loss: 0.00003615
Iteration 84/1000 | Loss: 0.00003615
Iteration 85/1000 | Loss: 0.00003615
Iteration 86/1000 | Loss: 0.00003615
Iteration 87/1000 | Loss: 0.00003615
Iteration 88/1000 | Loss: 0.00003614
Iteration 89/1000 | Loss: 0.00003614
Iteration 90/1000 | Loss: 0.00003614
Iteration 91/1000 | Loss: 0.00003614
Iteration 92/1000 | Loss: 0.00003614
Iteration 93/1000 | Loss: 0.00003614
Iteration 94/1000 | Loss: 0.00003613
Iteration 95/1000 | Loss: 0.00003613
Iteration 96/1000 | Loss: 0.00003613
Iteration 97/1000 | Loss: 0.00003613
Iteration 98/1000 | Loss: 0.00003612
Iteration 99/1000 | Loss: 0.00003612
Iteration 100/1000 | Loss: 0.00003612
Iteration 101/1000 | Loss: 0.00003612
Iteration 102/1000 | Loss: 0.00003612
Iteration 103/1000 | Loss: 0.00003612
Iteration 104/1000 | Loss: 0.00003612
Iteration 105/1000 | Loss: 0.00003612
Iteration 106/1000 | Loss: 0.00003611
Iteration 107/1000 | Loss: 0.00003611
Iteration 108/1000 | Loss: 0.00003611
Iteration 109/1000 | Loss: 0.00003611
Iteration 110/1000 | Loss: 0.00003611
Iteration 111/1000 | Loss: 0.00003611
Iteration 112/1000 | Loss: 0.00003611
Iteration 113/1000 | Loss: 0.00003611
Iteration 114/1000 | Loss: 0.00003611
Iteration 115/1000 | Loss: 0.00003611
Iteration 116/1000 | Loss: 0.00003611
Iteration 117/1000 | Loss: 0.00003610
Iteration 118/1000 | Loss: 0.00003610
Iteration 119/1000 | Loss: 0.00003610
Iteration 120/1000 | Loss: 0.00003610
Iteration 121/1000 | Loss: 0.00003609
Iteration 122/1000 | Loss: 0.00003609
Iteration 123/1000 | Loss: 0.00003609
Iteration 124/1000 | Loss: 0.00003609
Iteration 125/1000 | Loss: 0.00003609
Iteration 126/1000 | Loss: 0.00003609
Iteration 127/1000 | Loss: 0.00003609
Iteration 128/1000 | Loss: 0.00003609
Iteration 129/1000 | Loss: 0.00003609
Iteration 130/1000 | Loss: 0.00003609
Iteration 131/1000 | Loss: 0.00003609
Iteration 132/1000 | Loss: 0.00003609
Iteration 133/1000 | Loss: 0.00003608
Iteration 134/1000 | Loss: 0.00003608
Iteration 135/1000 | Loss: 0.00003608
Iteration 136/1000 | Loss: 0.00003608
Iteration 137/1000 | Loss: 0.00003608
Iteration 138/1000 | Loss: 0.00003608
Iteration 139/1000 | Loss: 0.00003608
Iteration 140/1000 | Loss: 0.00003608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [3.6079851270187646e-05, 3.6079851270187646e-05, 3.6079851270187646e-05, 3.6079851270187646e-05, 3.6079851270187646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6079851270187646e-05

Optimization complete. Final v2v error: 4.931121826171875 mm

Highest mean error: 5.994956970214844 mm for frame 0

Lowest mean error: 4.426233291625977 mm for frame 73

Saving results

Total time: 49.141987562179565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00529113
Iteration 2/25 | Loss: 0.00151138
Iteration 3/25 | Loss: 0.00137995
Iteration 4/25 | Loss: 0.00137145
Iteration 5/25 | Loss: 0.00137042
Iteration 6/25 | Loss: 0.00137042
Iteration 7/25 | Loss: 0.00137042
Iteration 8/25 | Loss: 0.00137042
Iteration 9/25 | Loss: 0.00137042
Iteration 10/25 | Loss: 0.00137042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013704183511435986, 0.0013704183511435986, 0.0013704183511435986, 0.0013704183511435986, 0.0013704183511435986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013704183511435986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.14493084
Iteration 2/25 | Loss: 0.00062678
Iteration 3/25 | Loss: 0.00062662
Iteration 4/25 | Loss: 0.00062662
Iteration 5/25 | Loss: 0.00062661
Iteration 6/25 | Loss: 0.00062661
Iteration 7/25 | Loss: 0.00062661
Iteration 8/25 | Loss: 0.00062661
Iteration 9/25 | Loss: 0.00062661
Iteration 10/25 | Loss: 0.00062661
Iteration 11/25 | Loss: 0.00062661
Iteration 12/25 | Loss: 0.00062661
Iteration 13/25 | Loss: 0.00062661
Iteration 14/25 | Loss: 0.00062661
Iteration 15/25 | Loss: 0.00062661
Iteration 16/25 | Loss: 0.00062661
Iteration 17/25 | Loss: 0.00062661
Iteration 18/25 | Loss: 0.00062661
Iteration 19/25 | Loss: 0.00062661
Iteration 20/25 | Loss: 0.00062661
Iteration 21/25 | Loss: 0.00062661
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006266129203140736, 0.0006266129203140736, 0.0006266129203140736, 0.0006266129203140736, 0.0006266129203140736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006266129203140736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062661
Iteration 2/1000 | Loss: 0.00004442
Iteration 3/1000 | Loss: 0.00002838
Iteration 4/1000 | Loss: 0.00002535
Iteration 5/1000 | Loss: 0.00002461
Iteration 6/1000 | Loss: 0.00002389
Iteration 7/1000 | Loss: 0.00002338
Iteration 8/1000 | Loss: 0.00002300
Iteration 9/1000 | Loss: 0.00002267
Iteration 10/1000 | Loss: 0.00002240
Iteration 11/1000 | Loss: 0.00002214
Iteration 12/1000 | Loss: 0.00002208
Iteration 13/1000 | Loss: 0.00002203
Iteration 14/1000 | Loss: 0.00002190
Iteration 15/1000 | Loss: 0.00002172
Iteration 16/1000 | Loss: 0.00002172
Iteration 17/1000 | Loss: 0.00002170
Iteration 18/1000 | Loss: 0.00002169
Iteration 19/1000 | Loss: 0.00002169
Iteration 20/1000 | Loss: 0.00002169
Iteration 21/1000 | Loss: 0.00002168
Iteration 22/1000 | Loss: 0.00002168
Iteration 23/1000 | Loss: 0.00002166
Iteration 24/1000 | Loss: 0.00002166
Iteration 25/1000 | Loss: 0.00002166
Iteration 26/1000 | Loss: 0.00002166
Iteration 27/1000 | Loss: 0.00002165
Iteration 28/1000 | Loss: 0.00002164
Iteration 29/1000 | Loss: 0.00002162
Iteration 30/1000 | Loss: 0.00002159
Iteration 31/1000 | Loss: 0.00002157
Iteration 32/1000 | Loss: 0.00002157
Iteration 33/1000 | Loss: 0.00002156
Iteration 34/1000 | Loss: 0.00002153
Iteration 35/1000 | Loss: 0.00002153
Iteration 36/1000 | Loss: 0.00002153
Iteration 37/1000 | Loss: 0.00002153
Iteration 38/1000 | Loss: 0.00002152
Iteration 39/1000 | Loss: 0.00002152
Iteration 40/1000 | Loss: 0.00002152
Iteration 41/1000 | Loss: 0.00002151
Iteration 42/1000 | Loss: 0.00002147
Iteration 43/1000 | Loss: 0.00002147
Iteration 44/1000 | Loss: 0.00002147
Iteration 45/1000 | Loss: 0.00002147
Iteration 46/1000 | Loss: 0.00002147
Iteration 47/1000 | Loss: 0.00002146
Iteration 48/1000 | Loss: 0.00002145
Iteration 49/1000 | Loss: 0.00002145
Iteration 50/1000 | Loss: 0.00002145
Iteration 51/1000 | Loss: 0.00002144
Iteration 52/1000 | Loss: 0.00002144
Iteration 53/1000 | Loss: 0.00002143
Iteration 54/1000 | Loss: 0.00002143
Iteration 55/1000 | Loss: 0.00002142
Iteration 56/1000 | Loss: 0.00002142
Iteration 57/1000 | Loss: 0.00002141
Iteration 58/1000 | Loss: 0.00002141
Iteration 59/1000 | Loss: 0.00002141
Iteration 60/1000 | Loss: 0.00002140
Iteration 61/1000 | Loss: 0.00002140
Iteration 62/1000 | Loss: 0.00002139
Iteration 63/1000 | Loss: 0.00002139
Iteration 64/1000 | Loss: 0.00002139
Iteration 65/1000 | Loss: 0.00002139
Iteration 66/1000 | Loss: 0.00002139
Iteration 67/1000 | Loss: 0.00002138
Iteration 68/1000 | Loss: 0.00002138
Iteration 69/1000 | Loss: 0.00002138
Iteration 70/1000 | Loss: 0.00002138
Iteration 71/1000 | Loss: 0.00002138
Iteration 72/1000 | Loss: 0.00002138
Iteration 73/1000 | Loss: 0.00002137
Iteration 74/1000 | Loss: 0.00002137
Iteration 75/1000 | Loss: 0.00002137
Iteration 76/1000 | Loss: 0.00002136
Iteration 77/1000 | Loss: 0.00002136
Iteration 78/1000 | Loss: 0.00002135
Iteration 79/1000 | Loss: 0.00002135
Iteration 80/1000 | Loss: 0.00002135
Iteration 81/1000 | Loss: 0.00002134
Iteration 82/1000 | Loss: 0.00002134
Iteration 83/1000 | Loss: 0.00002134
Iteration 84/1000 | Loss: 0.00002134
Iteration 85/1000 | Loss: 0.00002134
Iteration 86/1000 | Loss: 0.00002133
Iteration 87/1000 | Loss: 0.00002133
Iteration 88/1000 | Loss: 0.00002133
Iteration 89/1000 | Loss: 0.00002133
Iteration 90/1000 | Loss: 0.00002133
Iteration 91/1000 | Loss: 0.00002133
Iteration 92/1000 | Loss: 0.00002133
Iteration 93/1000 | Loss: 0.00002133
Iteration 94/1000 | Loss: 0.00002133
Iteration 95/1000 | Loss: 0.00002133
Iteration 96/1000 | Loss: 0.00002133
Iteration 97/1000 | Loss: 0.00002133
Iteration 98/1000 | Loss: 0.00002133
Iteration 99/1000 | Loss: 0.00002133
Iteration 100/1000 | Loss: 0.00002133
Iteration 101/1000 | Loss: 0.00002133
Iteration 102/1000 | Loss: 0.00002133
Iteration 103/1000 | Loss: 0.00002133
Iteration 104/1000 | Loss: 0.00002133
Iteration 105/1000 | Loss: 0.00002133
Iteration 106/1000 | Loss: 0.00002133
Iteration 107/1000 | Loss: 0.00002133
Iteration 108/1000 | Loss: 0.00002133
Iteration 109/1000 | Loss: 0.00002133
Iteration 110/1000 | Loss: 0.00002133
Iteration 111/1000 | Loss: 0.00002133
Iteration 112/1000 | Loss: 0.00002133
Iteration 113/1000 | Loss: 0.00002133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.132595909642987e-05, 2.132595909642987e-05, 2.132595909642987e-05, 2.132595909642987e-05, 2.132595909642987e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.132595909642987e-05

Optimization complete. Final v2v error: 3.8745927810668945 mm

Highest mean error: 4.287037372589111 mm for frame 83

Lowest mean error: 3.5352137088775635 mm for frame 51

Saving results

Total time: 34.84162211418152
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00709510
Iteration 2/25 | Loss: 0.00156963
Iteration 3/25 | Loss: 0.00143539
Iteration 4/25 | Loss: 0.00142071
Iteration 5/25 | Loss: 0.00141731
Iteration 6/25 | Loss: 0.00141731
Iteration 7/25 | Loss: 0.00141731
Iteration 8/25 | Loss: 0.00141731
Iteration 9/25 | Loss: 0.00141731
Iteration 10/25 | Loss: 0.00141731
Iteration 11/25 | Loss: 0.00141731
Iteration 12/25 | Loss: 0.00141731
Iteration 13/25 | Loss: 0.00141731
Iteration 14/25 | Loss: 0.00141731
Iteration 15/25 | Loss: 0.00141731
Iteration 16/25 | Loss: 0.00141731
Iteration 17/25 | Loss: 0.00141731
Iteration 18/25 | Loss: 0.00141731
Iteration 19/25 | Loss: 0.00141731
Iteration 20/25 | Loss: 0.00141731
Iteration 21/25 | Loss: 0.00141731
Iteration 22/25 | Loss: 0.00141731
Iteration 23/25 | Loss: 0.00141731
Iteration 24/25 | Loss: 0.00141731
Iteration 25/25 | Loss: 0.00141731

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.16774225
Iteration 2/25 | Loss: 0.00117767
Iteration 3/25 | Loss: 0.00117765
Iteration 4/25 | Loss: 0.00117765
Iteration 5/25 | Loss: 0.00117765
Iteration 6/25 | Loss: 0.00117765
Iteration 7/25 | Loss: 0.00117765
Iteration 8/25 | Loss: 0.00117765
Iteration 9/25 | Loss: 0.00117765
Iteration 10/25 | Loss: 0.00117765
Iteration 11/25 | Loss: 0.00117765
Iteration 12/25 | Loss: 0.00117765
Iteration 13/25 | Loss: 0.00117765
Iteration 14/25 | Loss: 0.00117765
Iteration 15/25 | Loss: 0.00117765
Iteration 16/25 | Loss: 0.00117765
Iteration 17/25 | Loss: 0.00117765
Iteration 18/25 | Loss: 0.00117765
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011776451719924808, 0.0011776451719924808, 0.0011776451719924808, 0.0011776451719924808, 0.0011776451719924808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011776451719924808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117765
Iteration 2/1000 | Loss: 0.00004898
Iteration 3/1000 | Loss: 0.00003393
Iteration 4/1000 | Loss: 0.00003058
Iteration 5/1000 | Loss: 0.00002872
Iteration 6/1000 | Loss: 0.00002753
Iteration 7/1000 | Loss: 0.00002694
Iteration 8/1000 | Loss: 0.00002663
Iteration 9/1000 | Loss: 0.00002627
Iteration 10/1000 | Loss: 0.00002600
Iteration 11/1000 | Loss: 0.00002591
Iteration 12/1000 | Loss: 0.00002588
Iteration 13/1000 | Loss: 0.00002579
Iteration 14/1000 | Loss: 0.00002564
Iteration 15/1000 | Loss: 0.00002557
Iteration 16/1000 | Loss: 0.00002556
Iteration 17/1000 | Loss: 0.00002555
Iteration 18/1000 | Loss: 0.00002554
Iteration 19/1000 | Loss: 0.00002552
Iteration 20/1000 | Loss: 0.00002548
Iteration 21/1000 | Loss: 0.00002548
Iteration 22/1000 | Loss: 0.00002546
Iteration 23/1000 | Loss: 0.00002545
Iteration 24/1000 | Loss: 0.00002544
Iteration 25/1000 | Loss: 0.00002544
Iteration 26/1000 | Loss: 0.00002544
Iteration 27/1000 | Loss: 0.00002544
Iteration 28/1000 | Loss: 0.00002544
Iteration 29/1000 | Loss: 0.00002544
Iteration 30/1000 | Loss: 0.00002544
Iteration 31/1000 | Loss: 0.00002544
Iteration 32/1000 | Loss: 0.00002543
Iteration 33/1000 | Loss: 0.00002542
Iteration 34/1000 | Loss: 0.00002542
Iteration 35/1000 | Loss: 0.00002541
Iteration 36/1000 | Loss: 0.00002541
Iteration 37/1000 | Loss: 0.00002541
Iteration 38/1000 | Loss: 0.00002541
Iteration 39/1000 | Loss: 0.00002541
Iteration 40/1000 | Loss: 0.00002541
Iteration 41/1000 | Loss: 0.00002541
Iteration 42/1000 | Loss: 0.00002540
Iteration 43/1000 | Loss: 0.00002540
Iteration 44/1000 | Loss: 0.00002540
Iteration 45/1000 | Loss: 0.00002539
Iteration 46/1000 | Loss: 0.00002539
Iteration 47/1000 | Loss: 0.00002537
Iteration 48/1000 | Loss: 0.00002537
Iteration 49/1000 | Loss: 0.00002536
Iteration 50/1000 | Loss: 0.00002536
Iteration 51/1000 | Loss: 0.00002536
Iteration 52/1000 | Loss: 0.00002536
Iteration 53/1000 | Loss: 0.00002536
Iteration 54/1000 | Loss: 0.00002536
Iteration 55/1000 | Loss: 0.00002536
Iteration 56/1000 | Loss: 0.00002536
Iteration 57/1000 | Loss: 0.00002536
Iteration 58/1000 | Loss: 0.00002536
Iteration 59/1000 | Loss: 0.00002536
Iteration 60/1000 | Loss: 0.00002536
Iteration 61/1000 | Loss: 0.00002536
Iteration 62/1000 | Loss: 0.00002534
Iteration 63/1000 | Loss: 0.00002533
Iteration 64/1000 | Loss: 0.00002533
Iteration 65/1000 | Loss: 0.00002533
Iteration 66/1000 | Loss: 0.00002533
Iteration 67/1000 | Loss: 0.00002533
Iteration 68/1000 | Loss: 0.00002533
Iteration 69/1000 | Loss: 0.00002533
Iteration 70/1000 | Loss: 0.00002533
Iteration 71/1000 | Loss: 0.00002533
Iteration 72/1000 | Loss: 0.00002533
Iteration 73/1000 | Loss: 0.00002533
Iteration 74/1000 | Loss: 0.00002533
Iteration 75/1000 | Loss: 0.00002532
Iteration 76/1000 | Loss: 0.00002532
Iteration 77/1000 | Loss: 0.00002531
Iteration 78/1000 | Loss: 0.00002531
Iteration 79/1000 | Loss: 0.00002530
Iteration 80/1000 | Loss: 0.00002526
Iteration 81/1000 | Loss: 0.00002521
Iteration 82/1000 | Loss: 0.00002518
Iteration 83/1000 | Loss: 0.00002517
Iteration 84/1000 | Loss: 0.00002515
Iteration 85/1000 | Loss: 0.00002515
Iteration 86/1000 | Loss: 0.00002513
Iteration 87/1000 | Loss: 0.00002512
Iteration 88/1000 | Loss: 0.00002510
Iteration 89/1000 | Loss: 0.00002510
Iteration 90/1000 | Loss: 0.00002509
Iteration 91/1000 | Loss: 0.00002509
Iteration 92/1000 | Loss: 0.00002509
Iteration 93/1000 | Loss: 0.00002509
Iteration 94/1000 | Loss: 0.00002507
Iteration 95/1000 | Loss: 0.00002506
Iteration 96/1000 | Loss: 0.00002506
Iteration 97/1000 | Loss: 0.00002505
Iteration 98/1000 | Loss: 0.00002505
Iteration 99/1000 | Loss: 0.00002505
Iteration 100/1000 | Loss: 0.00002504
Iteration 101/1000 | Loss: 0.00002504
Iteration 102/1000 | Loss: 0.00002503
Iteration 103/1000 | Loss: 0.00002503
Iteration 104/1000 | Loss: 0.00002503
Iteration 105/1000 | Loss: 0.00002503
Iteration 106/1000 | Loss: 0.00002503
Iteration 107/1000 | Loss: 0.00002503
Iteration 108/1000 | Loss: 0.00002502
Iteration 109/1000 | Loss: 0.00002502
Iteration 110/1000 | Loss: 0.00002502
Iteration 111/1000 | Loss: 0.00002502
Iteration 112/1000 | Loss: 0.00002502
Iteration 113/1000 | Loss: 0.00002501
Iteration 114/1000 | Loss: 0.00002501
Iteration 115/1000 | Loss: 0.00002501
Iteration 116/1000 | Loss: 0.00002500
Iteration 117/1000 | Loss: 0.00002500
Iteration 118/1000 | Loss: 0.00002500
Iteration 119/1000 | Loss: 0.00002500
Iteration 120/1000 | Loss: 0.00002500
Iteration 121/1000 | Loss: 0.00002500
Iteration 122/1000 | Loss: 0.00002500
Iteration 123/1000 | Loss: 0.00002499
Iteration 124/1000 | Loss: 0.00002499
Iteration 125/1000 | Loss: 0.00002499
Iteration 126/1000 | Loss: 0.00002498
Iteration 127/1000 | Loss: 0.00002498
Iteration 128/1000 | Loss: 0.00002498
Iteration 129/1000 | Loss: 0.00002497
Iteration 130/1000 | Loss: 0.00002497
Iteration 131/1000 | Loss: 0.00002497
Iteration 132/1000 | Loss: 0.00002497
Iteration 133/1000 | Loss: 0.00002496
Iteration 134/1000 | Loss: 0.00002496
Iteration 135/1000 | Loss: 0.00002496
Iteration 136/1000 | Loss: 0.00002496
Iteration 137/1000 | Loss: 0.00002496
Iteration 138/1000 | Loss: 0.00002496
Iteration 139/1000 | Loss: 0.00002495
Iteration 140/1000 | Loss: 0.00002495
Iteration 141/1000 | Loss: 0.00002495
Iteration 142/1000 | Loss: 0.00002495
Iteration 143/1000 | Loss: 0.00002495
Iteration 144/1000 | Loss: 0.00002495
Iteration 145/1000 | Loss: 0.00002495
Iteration 146/1000 | Loss: 0.00002495
Iteration 147/1000 | Loss: 0.00002495
Iteration 148/1000 | Loss: 0.00002494
Iteration 149/1000 | Loss: 0.00002494
Iteration 150/1000 | Loss: 0.00002494
Iteration 151/1000 | Loss: 0.00002494
Iteration 152/1000 | Loss: 0.00002494
Iteration 153/1000 | Loss: 0.00002494
Iteration 154/1000 | Loss: 0.00002494
Iteration 155/1000 | Loss: 0.00002494
Iteration 156/1000 | Loss: 0.00002494
Iteration 157/1000 | Loss: 0.00002494
Iteration 158/1000 | Loss: 0.00002494
Iteration 159/1000 | Loss: 0.00002494
Iteration 160/1000 | Loss: 0.00002494
Iteration 161/1000 | Loss: 0.00002494
Iteration 162/1000 | Loss: 0.00002494
Iteration 163/1000 | Loss: 0.00002494
Iteration 164/1000 | Loss: 0.00002494
Iteration 165/1000 | Loss: 0.00002494
Iteration 166/1000 | Loss: 0.00002494
Iteration 167/1000 | Loss: 0.00002494
Iteration 168/1000 | Loss: 0.00002494
Iteration 169/1000 | Loss: 0.00002494
Iteration 170/1000 | Loss: 0.00002494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.493715874152258e-05, 2.493715874152258e-05, 2.493715874152258e-05, 2.493715874152258e-05, 2.493715874152258e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.493715874152258e-05

Optimization complete. Final v2v error: 4.165959358215332 mm

Highest mean error: 4.545539855957031 mm for frame 59

Lowest mean error: 3.6780834197998047 mm for frame 238

Saving results

Total time: 45.8387610912323
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818162
Iteration 2/25 | Loss: 0.00186169
Iteration 3/25 | Loss: 0.00166562
Iteration 4/25 | Loss: 0.00160668
Iteration 5/25 | Loss: 0.00160211
Iteration 6/25 | Loss: 0.00157054
Iteration 7/25 | Loss: 0.00155228
Iteration 8/25 | Loss: 0.00158823
Iteration 9/25 | Loss: 0.00153932
Iteration 10/25 | Loss: 0.00151520
Iteration 11/25 | Loss: 0.00150796
Iteration 12/25 | Loss: 0.00152316
Iteration 13/25 | Loss: 0.00150437
Iteration 14/25 | Loss: 0.00150033
Iteration 15/25 | Loss: 0.00148664
Iteration 16/25 | Loss: 0.00146111
Iteration 17/25 | Loss: 0.00143331
Iteration 18/25 | Loss: 0.00142284
Iteration 19/25 | Loss: 0.00142167
Iteration 20/25 | Loss: 0.00142058
Iteration 21/25 | Loss: 0.00141463
Iteration 22/25 | Loss: 0.00140991
Iteration 23/25 | Loss: 0.00140729
Iteration 24/25 | Loss: 0.00140596
Iteration 25/25 | Loss: 0.00140958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35611832
Iteration 2/25 | Loss: 0.00095873
Iteration 3/25 | Loss: 0.00095870
Iteration 4/25 | Loss: 0.00095870
Iteration 5/25 | Loss: 0.00095870
Iteration 6/25 | Loss: 0.00095870
Iteration 7/25 | Loss: 0.00095870
Iteration 8/25 | Loss: 0.00095870
Iteration 9/25 | Loss: 0.00095870
Iteration 10/25 | Loss: 0.00095870
Iteration 11/25 | Loss: 0.00095870
Iteration 12/25 | Loss: 0.00095870
Iteration 13/25 | Loss: 0.00095870
Iteration 14/25 | Loss: 0.00095870
Iteration 15/25 | Loss: 0.00095870
Iteration 16/25 | Loss: 0.00095870
Iteration 17/25 | Loss: 0.00095870
Iteration 18/25 | Loss: 0.00095870
Iteration 19/25 | Loss: 0.00095870
Iteration 20/25 | Loss: 0.00095870
Iteration 21/25 | Loss: 0.00095870
Iteration 22/25 | Loss: 0.00095870
Iteration 23/25 | Loss: 0.00095870
Iteration 24/25 | Loss: 0.00095870
Iteration 25/25 | Loss: 0.00095870

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095870
Iteration 2/1000 | Loss: 0.00033642
Iteration 3/1000 | Loss: 0.00022173
Iteration 4/1000 | Loss: 0.00013171
Iteration 5/1000 | Loss: 0.00023115
Iteration 6/1000 | Loss: 0.00009599
Iteration 7/1000 | Loss: 0.00014310
Iteration 8/1000 | Loss: 0.00013810
Iteration 9/1000 | Loss: 0.00015048
Iteration 10/1000 | Loss: 0.00013453
Iteration 11/1000 | Loss: 0.00015139
Iteration 12/1000 | Loss: 0.00018316
Iteration 13/1000 | Loss: 0.00079515
Iteration 14/1000 | Loss: 0.00012837
Iteration 15/1000 | Loss: 0.00012478
Iteration 16/1000 | Loss: 0.00010383
Iteration 17/1000 | Loss: 0.00012927
Iteration 18/1000 | Loss: 0.00009680
Iteration 19/1000 | Loss: 0.00015432
Iteration 20/1000 | Loss: 0.00012748
Iteration 21/1000 | Loss: 0.00019246
Iteration 22/1000 | Loss: 0.00014294
Iteration 23/1000 | Loss: 0.00013553
Iteration 24/1000 | Loss: 0.00015503
Iteration 25/1000 | Loss: 0.00014417
Iteration 26/1000 | Loss: 0.00013540
Iteration 27/1000 | Loss: 0.00014550
Iteration 28/1000 | Loss: 0.00013438
Iteration 29/1000 | Loss: 0.00064694
Iteration 30/1000 | Loss: 0.00028554
Iteration 31/1000 | Loss: 0.00053559
Iteration 32/1000 | Loss: 0.00028481
Iteration 33/1000 | Loss: 0.00010637
Iteration 34/1000 | Loss: 0.00079345
Iteration 35/1000 | Loss: 0.00031154
Iteration 36/1000 | Loss: 0.00009795
Iteration 37/1000 | Loss: 0.00006698
Iteration 38/1000 | Loss: 0.00009618
Iteration 39/1000 | Loss: 0.00054940
Iteration 40/1000 | Loss: 0.00027920
Iteration 41/1000 | Loss: 0.00007343
Iteration 42/1000 | Loss: 0.00015419
Iteration 43/1000 | Loss: 0.00012849
Iteration 44/1000 | Loss: 0.00010790
Iteration 45/1000 | Loss: 0.00004236
Iteration 46/1000 | Loss: 0.00037703
Iteration 47/1000 | Loss: 0.00017315
Iteration 48/1000 | Loss: 0.00036758
Iteration 49/1000 | Loss: 0.00041795
Iteration 50/1000 | Loss: 0.00032141
Iteration 51/1000 | Loss: 0.00011161
Iteration 52/1000 | Loss: 0.00009095
Iteration 53/1000 | Loss: 0.00011740
Iteration 54/1000 | Loss: 0.00012403
Iteration 55/1000 | Loss: 0.00096715
Iteration 56/1000 | Loss: 0.00010602
Iteration 57/1000 | Loss: 0.00017336
Iteration 58/1000 | Loss: 0.00003927
Iteration 59/1000 | Loss: 0.00003371
Iteration 60/1000 | Loss: 0.00003214
Iteration 61/1000 | Loss: 0.00003076
Iteration 62/1000 | Loss: 0.00002964
Iteration 63/1000 | Loss: 0.00002893
Iteration 64/1000 | Loss: 0.00002843
Iteration 65/1000 | Loss: 0.00002792
Iteration 66/1000 | Loss: 0.00002752
Iteration 67/1000 | Loss: 0.00002740
Iteration 68/1000 | Loss: 0.00042122
Iteration 69/1000 | Loss: 0.00002712
Iteration 70/1000 | Loss: 0.00002622
Iteration 71/1000 | Loss: 0.00002586
Iteration 72/1000 | Loss: 0.00002562
Iteration 73/1000 | Loss: 0.00002558
Iteration 74/1000 | Loss: 0.00002535
Iteration 75/1000 | Loss: 0.00002524
Iteration 76/1000 | Loss: 0.00002518
Iteration 77/1000 | Loss: 0.00002517
Iteration 78/1000 | Loss: 0.00002517
Iteration 79/1000 | Loss: 0.00002517
Iteration 80/1000 | Loss: 0.00002517
Iteration 81/1000 | Loss: 0.00002516
Iteration 82/1000 | Loss: 0.00002516
Iteration 83/1000 | Loss: 0.00002516
Iteration 84/1000 | Loss: 0.00002515
Iteration 85/1000 | Loss: 0.00002515
Iteration 86/1000 | Loss: 0.00002515
Iteration 87/1000 | Loss: 0.00002514
Iteration 88/1000 | Loss: 0.00002514
Iteration 89/1000 | Loss: 0.00002514
Iteration 90/1000 | Loss: 0.00002513
Iteration 91/1000 | Loss: 0.00002513
Iteration 92/1000 | Loss: 0.00002512
Iteration 93/1000 | Loss: 0.00002512
Iteration 94/1000 | Loss: 0.00002512
Iteration 95/1000 | Loss: 0.00002511
Iteration 96/1000 | Loss: 0.00002510
Iteration 97/1000 | Loss: 0.00002510
Iteration 98/1000 | Loss: 0.00002510
Iteration 99/1000 | Loss: 0.00002509
Iteration 100/1000 | Loss: 0.00002509
Iteration 101/1000 | Loss: 0.00002509
Iteration 102/1000 | Loss: 0.00002509
Iteration 103/1000 | Loss: 0.00002509
Iteration 104/1000 | Loss: 0.00002508
Iteration 105/1000 | Loss: 0.00002508
Iteration 106/1000 | Loss: 0.00002508
Iteration 107/1000 | Loss: 0.00002507
Iteration 108/1000 | Loss: 0.00002507
Iteration 109/1000 | Loss: 0.00002506
Iteration 110/1000 | Loss: 0.00002506
Iteration 111/1000 | Loss: 0.00002505
Iteration 112/1000 | Loss: 0.00002504
Iteration 113/1000 | Loss: 0.00002502
Iteration 114/1000 | Loss: 0.00002501
Iteration 115/1000 | Loss: 0.00002501
Iteration 116/1000 | Loss: 0.00002501
Iteration 117/1000 | Loss: 0.00002501
Iteration 118/1000 | Loss: 0.00002501
Iteration 119/1000 | Loss: 0.00002501
Iteration 120/1000 | Loss: 0.00002501
Iteration 121/1000 | Loss: 0.00002501
Iteration 122/1000 | Loss: 0.00002501
Iteration 123/1000 | Loss: 0.00002501
Iteration 124/1000 | Loss: 0.00002500
Iteration 125/1000 | Loss: 0.00002500
Iteration 126/1000 | Loss: 0.00002500
Iteration 127/1000 | Loss: 0.00002500
Iteration 128/1000 | Loss: 0.00002499
Iteration 129/1000 | Loss: 0.00002499
Iteration 130/1000 | Loss: 0.00002499
Iteration 131/1000 | Loss: 0.00002499
Iteration 132/1000 | Loss: 0.00002498
Iteration 133/1000 | Loss: 0.00002498
Iteration 134/1000 | Loss: 0.00002498
Iteration 135/1000 | Loss: 0.00002498
Iteration 136/1000 | Loss: 0.00002498
Iteration 137/1000 | Loss: 0.00002498
Iteration 138/1000 | Loss: 0.00002498
Iteration 139/1000 | Loss: 0.00002497
Iteration 140/1000 | Loss: 0.00002497
Iteration 141/1000 | Loss: 0.00002497
Iteration 142/1000 | Loss: 0.00002497
Iteration 143/1000 | Loss: 0.00002497
Iteration 144/1000 | Loss: 0.00002497
Iteration 145/1000 | Loss: 0.00002497
Iteration 146/1000 | Loss: 0.00002497
Iteration 147/1000 | Loss: 0.00002497
Iteration 148/1000 | Loss: 0.00002497
Iteration 149/1000 | Loss: 0.00002497
Iteration 150/1000 | Loss: 0.00002496
Iteration 151/1000 | Loss: 0.00002496
Iteration 152/1000 | Loss: 0.00002496
Iteration 153/1000 | Loss: 0.00002496
Iteration 154/1000 | Loss: 0.00002496
Iteration 155/1000 | Loss: 0.00002496
Iteration 156/1000 | Loss: 0.00002496
Iteration 157/1000 | Loss: 0.00002496
Iteration 158/1000 | Loss: 0.00002496
Iteration 159/1000 | Loss: 0.00002495
Iteration 160/1000 | Loss: 0.00002495
Iteration 161/1000 | Loss: 0.00002495
Iteration 162/1000 | Loss: 0.00002495
Iteration 163/1000 | Loss: 0.00002495
Iteration 164/1000 | Loss: 0.00002495
Iteration 165/1000 | Loss: 0.00002495
Iteration 166/1000 | Loss: 0.00002495
Iteration 167/1000 | Loss: 0.00002495
Iteration 168/1000 | Loss: 0.00002495
Iteration 169/1000 | Loss: 0.00002495
Iteration 170/1000 | Loss: 0.00002495
Iteration 171/1000 | Loss: 0.00002495
Iteration 172/1000 | Loss: 0.00002495
Iteration 173/1000 | Loss: 0.00002495
Iteration 174/1000 | Loss: 0.00002495
Iteration 175/1000 | Loss: 0.00002495
Iteration 176/1000 | Loss: 0.00002494
Iteration 177/1000 | Loss: 0.00002494
Iteration 178/1000 | Loss: 0.00002494
Iteration 179/1000 | Loss: 0.00002494
Iteration 180/1000 | Loss: 0.00002494
Iteration 181/1000 | Loss: 0.00002494
Iteration 182/1000 | Loss: 0.00002494
Iteration 183/1000 | Loss: 0.00002493
Iteration 184/1000 | Loss: 0.00002493
Iteration 185/1000 | Loss: 0.00002493
Iteration 186/1000 | Loss: 0.00002493
Iteration 187/1000 | Loss: 0.00002493
Iteration 188/1000 | Loss: 0.00002493
Iteration 189/1000 | Loss: 0.00002493
Iteration 190/1000 | Loss: 0.00002493
Iteration 191/1000 | Loss: 0.00002493
Iteration 192/1000 | Loss: 0.00002492
Iteration 193/1000 | Loss: 0.00002492
Iteration 194/1000 | Loss: 0.00002492
Iteration 195/1000 | Loss: 0.00002492
Iteration 196/1000 | Loss: 0.00002492
Iteration 197/1000 | Loss: 0.00002492
Iteration 198/1000 | Loss: 0.00002492
Iteration 199/1000 | Loss: 0.00002492
Iteration 200/1000 | Loss: 0.00002492
Iteration 201/1000 | Loss: 0.00002492
Iteration 202/1000 | Loss: 0.00002492
Iteration 203/1000 | Loss: 0.00002492
Iteration 204/1000 | Loss: 0.00002492
Iteration 205/1000 | Loss: 0.00002491
Iteration 206/1000 | Loss: 0.00002491
Iteration 207/1000 | Loss: 0.00002491
Iteration 208/1000 | Loss: 0.00002491
Iteration 209/1000 | Loss: 0.00002491
Iteration 210/1000 | Loss: 0.00002490
Iteration 211/1000 | Loss: 0.00002490
Iteration 212/1000 | Loss: 0.00002490
Iteration 213/1000 | Loss: 0.00002490
Iteration 214/1000 | Loss: 0.00002490
Iteration 215/1000 | Loss: 0.00002490
Iteration 216/1000 | Loss: 0.00002490
Iteration 217/1000 | Loss: 0.00002490
Iteration 218/1000 | Loss: 0.00002490
Iteration 219/1000 | Loss: 0.00002490
Iteration 220/1000 | Loss: 0.00002490
Iteration 221/1000 | Loss: 0.00002490
Iteration 222/1000 | Loss: 0.00002490
Iteration 223/1000 | Loss: 0.00002490
Iteration 224/1000 | Loss: 0.00002490
Iteration 225/1000 | Loss: 0.00002490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.4897548428270966e-05, 2.4897548428270966e-05, 2.4897548428270966e-05, 2.4897548428270966e-05, 2.4897548428270966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4897548428270966e-05

Optimization complete. Final v2v error: 4.160218715667725 mm

Highest mean error: 5.779116153717041 mm for frame 25

Lowest mean error: 3.83249831199646 mm for frame 31

Saving results

Total time: 180.0993251800537
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848817
Iteration 2/25 | Loss: 0.00173870
Iteration 3/25 | Loss: 0.00144348
Iteration 4/25 | Loss: 0.00140451
Iteration 5/25 | Loss: 0.00140002
Iteration 6/25 | Loss: 0.00140002
Iteration 7/25 | Loss: 0.00140002
Iteration 8/25 | Loss: 0.00140002
Iteration 9/25 | Loss: 0.00140002
Iteration 10/25 | Loss: 0.00140002
Iteration 11/25 | Loss: 0.00140002
Iteration 12/25 | Loss: 0.00140002
Iteration 13/25 | Loss: 0.00140002
Iteration 14/25 | Loss: 0.00140002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001400023465976119, 0.001400023465976119, 0.001400023465976119, 0.001400023465976119, 0.001400023465976119]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001400023465976119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.15772830
Iteration 2/25 | Loss: 0.00072776
Iteration 3/25 | Loss: 0.00072776
Iteration 4/25 | Loss: 0.00072776
Iteration 5/25 | Loss: 0.00072776
Iteration 6/25 | Loss: 0.00072776
Iteration 7/25 | Loss: 0.00072776
Iteration 8/25 | Loss: 0.00072776
Iteration 9/25 | Loss: 0.00072776
Iteration 10/25 | Loss: 0.00072776
Iteration 11/25 | Loss: 0.00072776
Iteration 12/25 | Loss: 0.00072776
Iteration 13/25 | Loss: 0.00072776
Iteration 14/25 | Loss: 0.00072776
Iteration 15/25 | Loss: 0.00072776
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007277574623003602, 0.0007277574623003602, 0.0007277574623003602, 0.0007277574623003602, 0.0007277574623003602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007277574623003602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072776
Iteration 2/1000 | Loss: 0.00006528
Iteration 3/1000 | Loss: 0.00003778
Iteration 4/1000 | Loss: 0.00003239
Iteration 5/1000 | Loss: 0.00003086
Iteration 6/1000 | Loss: 0.00002969
Iteration 7/1000 | Loss: 0.00002899
Iteration 8/1000 | Loss: 0.00002851
Iteration 9/1000 | Loss: 0.00002817
Iteration 10/1000 | Loss: 0.00002791
Iteration 11/1000 | Loss: 0.00002766
Iteration 12/1000 | Loss: 0.00002749
Iteration 13/1000 | Loss: 0.00002749
Iteration 14/1000 | Loss: 0.00002737
Iteration 15/1000 | Loss: 0.00002727
Iteration 16/1000 | Loss: 0.00002726
Iteration 17/1000 | Loss: 0.00002725
Iteration 18/1000 | Loss: 0.00002725
Iteration 19/1000 | Loss: 0.00002725
Iteration 20/1000 | Loss: 0.00002725
Iteration 21/1000 | Loss: 0.00002725
Iteration 22/1000 | Loss: 0.00002725
Iteration 23/1000 | Loss: 0.00002724
Iteration 24/1000 | Loss: 0.00002724
Iteration 25/1000 | Loss: 0.00002724
Iteration 26/1000 | Loss: 0.00002724
Iteration 27/1000 | Loss: 0.00002724
Iteration 28/1000 | Loss: 0.00002723
Iteration 29/1000 | Loss: 0.00002723
Iteration 30/1000 | Loss: 0.00002723
Iteration 31/1000 | Loss: 0.00002723
Iteration 32/1000 | Loss: 0.00002723
Iteration 33/1000 | Loss: 0.00002723
Iteration 34/1000 | Loss: 0.00002723
Iteration 35/1000 | Loss: 0.00002723
Iteration 36/1000 | Loss: 0.00002723
Iteration 37/1000 | Loss: 0.00002723
Iteration 38/1000 | Loss: 0.00002722
Iteration 39/1000 | Loss: 0.00002722
Iteration 40/1000 | Loss: 0.00002722
Iteration 41/1000 | Loss: 0.00002721
Iteration 42/1000 | Loss: 0.00002721
Iteration 43/1000 | Loss: 0.00002721
Iteration 44/1000 | Loss: 0.00002720
Iteration 45/1000 | Loss: 0.00002717
Iteration 46/1000 | Loss: 0.00002717
Iteration 47/1000 | Loss: 0.00002716
Iteration 48/1000 | Loss: 0.00002715
Iteration 49/1000 | Loss: 0.00002715
Iteration 50/1000 | Loss: 0.00002715
Iteration 51/1000 | Loss: 0.00002715
Iteration 52/1000 | Loss: 0.00002715
Iteration 53/1000 | Loss: 0.00002715
Iteration 54/1000 | Loss: 0.00002714
Iteration 55/1000 | Loss: 0.00002714
Iteration 56/1000 | Loss: 0.00002714
Iteration 57/1000 | Loss: 0.00002713
Iteration 58/1000 | Loss: 0.00002713
Iteration 59/1000 | Loss: 0.00002712
Iteration 60/1000 | Loss: 0.00002711
Iteration 61/1000 | Loss: 0.00002711
Iteration 62/1000 | Loss: 0.00002711
Iteration 63/1000 | Loss: 0.00002710
Iteration 64/1000 | Loss: 0.00002710
Iteration 65/1000 | Loss: 0.00002710
Iteration 66/1000 | Loss: 0.00002710
Iteration 67/1000 | Loss: 0.00002710
Iteration 68/1000 | Loss: 0.00002710
Iteration 69/1000 | Loss: 0.00002710
Iteration 70/1000 | Loss: 0.00002710
Iteration 71/1000 | Loss: 0.00002710
Iteration 72/1000 | Loss: 0.00002710
Iteration 73/1000 | Loss: 0.00002709
Iteration 74/1000 | Loss: 0.00002709
Iteration 75/1000 | Loss: 0.00002709
Iteration 76/1000 | Loss: 0.00002709
Iteration 77/1000 | Loss: 0.00002709
Iteration 78/1000 | Loss: 0.00002709
Iteration 79/1000 | Loss: 0.00002709
Iteration 80/1000 | Loss: 0.00002709
Iteration 81/1000 | Loss: 0.00002709
Iteration 82/1000 | Loss: 0.00002709
Iteration 83/1000 | Loss: 0.00002709
Iteration 84/1000 | Loss: 0.00002709
Iteration 85/1000 | Loss: 0.00002709
Iteration 86/1000 | Loss: 0.00002709
Iteration 87/1000 | Loss: 0.00002709
Iteration 88/1000 | Loss: 0.00002709
Iteration 89/1000 | Loss: 0.00002709
Iteration 90/1000 | Loss: 0.00002709
Iteration 91/1000 | Loss: 0.00002709
Iteration 92/1000 | Loss: 0.00002709
Iteration 93/1000 | Loss: 0.00002709
Iteration 94/1000 | Loss: 0.00002709
Iteration 95/1000 | Loss: 0.00002709
Iteration 96/1000 | Loss: 0.00002709
Iteration 97/1000 | Loss: 0.00002709
Iteration 98/1000 | Loss: 0.00002709
Iteration 99/1000 | Loss: 0.00002709
Iteration 100/1000 | Loss: 0.00002709
Iteration 101/1000 | Loss: 0.00002709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.7091589799965732e-05, 2.7091589799965732e-05, 2.7091589799965732e-05, 2.7091589799965732e-05, 2.7091589799965732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7091589799965732e-05

Optimization complete. Final v2v error: 4.421087265014648 mm

Highest mean error: 4.589935779571533 mm for frame 95

Lowest mean error: 4.030111312866211 mm for frame 102

Saving results

Total time: 36.31321144104004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465802
Iteration 2/25 | Loss: 0.00139876
Iteration 3/25 | Loss: 0.00131853
Iteration 4/25 | Loss: 0.00130914
Iteration 5/25 | Loss: 0.00130606
Iteration 6/25 | Loss: 0.00130556
Iteration 7/25 | Loss: 0.00130556
Iteration 8/25 | Loss: 0.00130556
Iteration 9/25 | Loss: 0.00130556
Iteration 10/25 | Loss: 0.00130556
Iteration 11/25 | Loss: 0.00130556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013055568560957909, 0.0013055568560957909, 0.0013055568560957909, 0.0013055568560957909, 0.0013055568560957909]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013055568560957909

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.06462383
Iteration 2/25 | Loss: 0.00092484
Iteration 3/25 | Loss: 0.00092484
Iteration 4/25 | Loss: 0.00092484
Iteration 5/25 | Loss: 0.00092484
Iteration 6/25 | Loss: 0.00092484
Iteration 7/25 | Loss: 0.00092484
Iteration 8/25 | Loss: 0.00092484
Iteration 9/25 | Loss: 0.00092484
Iteration 10/25 | Loss: 0.00092484
Iteration 11/25 | Loss: 0.00092484
Iteration 12/25 | Loss: 0.00092484
Iteration 13/25 | Loss: 0.00092484
Iteration 14/25 | Loss: 0.00092484
Iteration 15/25 | Loss: 0.00092484
Iteration 16/25 | Loss: 0.00092484
Iteration 17/25 | Loss: 0.00092484
Iteration 18/25 | Loss: 0.00092484
Iteration 19/25 | Loss: 0.00092484
Iteration 20/25 | Loss: 0.00092484
Iteration 21/25 | Loss: 0.00092484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009248365531675518, 0.0009248365531675518, 0.0009248365531675518, 0.0009248365531675518, 0.0009248365531675518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009248365531675518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092484
Iteration 2/1000 | Loss: 0.00003278
Iteration 3/1000 | Loss: 0.00002317
Iteration 4/1000 | Loss: 0.00001937
Iteration 5/1000 | Loss: 0.00001819
Iteration 6/1000 | Loss: 0.00001753
Iteration 7/1000 | Loss: 0.00001699
Iteration 8/1000 | Loss: 0.00001657
Iteration 9/1000 | Loss: 0.00001634
Iteration 10/1000 | Loss: 0.00001609
Iteration 11/1000 | Loss: 0.00001586
Iteration 12/1000 | Loss: 0.00001571
Iteration 13/1000 | Loss: 0.00001569
Iteration 14/1000 | Loss: 0.00001562
Iteration 15/1000 | Loss: 0.00001559
Iteration 16/1000 | Loss: 0.00001559
Iteration 17/1000 | Loss: 0.00001555
Iteration 18/1000 | Loss: 0.00001554
Iteration 19/1000 | Loss: 0.00001552
Iteration 20/1000 | Loss: 0.00001549
Iteration 21/1000 | Loss: 0.00001549
Iteration 22/1000 | Loss: 0.00001548
Iteration 23/1000 | Loss: 0.00001544
Iteration 24/1000 | Loss: 0.00001544
Iteration 25/1000 | Loss: 0.00001540
Iteration 26/1000 | Loss: 0.00001540
Iteration 27/1000 | Loss: 0.00001540
Iteration 28/1000 | Loss: 0.00001539
Iteration 29/1000 | Loss: 0.00001539
Iteration 30/1000 | Loss: 0.00001539
Iteration 31/1000 | Loss: 0.00001538
Iteration 32/1000 | Loss: 0.00001538
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001536
Iteration 35/1000 | Loss: 0.00001536
Iteration 36/1000 | Loss: 0.00001535
Iteration 37/1000 | Loss: 0.00001535
Iteration 38/1000 | Loss: 0.00001535
Iteration 39/1000 | Loss: 0.00001535
Iteration 40/1000 | Loss: 0.00001535
Iteration 41/1000 | Loss: 0.00001534
Iteration 42/1000 | Loss: 0.00001534
Iteration 43/1000 | Loss: 0.00001534
Iteration 44/1000 | Loss: 0.00001534
Iteration 45/1000 | Loss: 0.00001534
Iteration 46/1000 | Loss: 0.00001534
Iteration 47/1000 | Loss: 0.00001534
Iteration 48/1000 | Loss: 0.00001534
Iteration 49/1000 | Loss: 0.00001533
Iteration 50/1000 | Loss: 0.00001530
Iteration 51/1000 | Loss: 0.00001530
Iteration 52/1000 | Loss: 0.00001530
Iteration 53/1000 | Loss: 0.00001529
Iteration 54/1000 | Loss: 0.00001528
Iteration 55/1000 | Loss: 0.00001528
Iteration 56/1000 | Loss: 0.00001527
Iteration 57/1000 | Loss: 0.00001527
Iteration 58/1000 | Loss: 0.00001527
Iteration 59/1000 | Loss: 0.00001526
Iteration 60/1000 | Loss: 0.00001526
Iteration 61/1000 | Loss: 0.00001526
Iteration 62/1000 | Loss: 0.00001526
Iteration 63/1000 | Loss: 0.00001526
Iteration 64/1000 | Loss: 0.00001525
Iteration 65/1000 | Loss: 0.00001525
Iteration 66/1000 | Loss: 0.00001525
Iteration 67/1000 | Loss: 0.00001525
Iteration 68/1000 | Loss: 0.00001525
Iteration 69/1000 | Loss: 0.00001525
Iteration 70/1000 | Loss: 0.00001525
Iteration 71/1000 | Loss: 0.00001524
Iteration 72/1000 | Loss: 0.00001524
Iteration 73/1000 | Loss: 0.00001523
Iteration 74/1000 | Loss: 0.00001522
Iteration 75/1000 | Loss: 0.00001522
Iteration 76/1000 | Loss: 0.00001521
Iteration 77/1000 | Loss: 0.00001521
Iteration 78/1000 | Loss: 0.00001521
Iteration 79/1000 | Loss: 0.00001519
Iteration 80/1000 | Loss: 0.00001518
Iteration 81/1000 | Loss: 0.00001516
Iteration 82/1000 | Loss: 0.00001516
Iteration 83/1000 | Loss: 0.00001516
Iteration 84/1000 | Loss: 0.00001514
Iteration 85/1000 | Loss: 0.00001514
Iteration 86/1000 | Loss: 0.00001514
Iteration 87/1000 | Loss: 0.00001514
Iteration 88/1000 | Loss: 0.00001514
Iteration 89/1000 | Loss: 0.00001514
Iteration 90/1000 | Loss: 0.00001513
Iteration 91/1000 | Loss: 0.00001513
Iteration 92/1000 | Loss: 0.00001513
Iteration 93/1000 | Loss: 0.00001513
Iteration 94/1000 | Loss: 0.00001513
Iteration 95/1000 | Loss: 0.00001513
Iteration 96/1000 | Loss: 0.00001513
Iteration 97/1000 | Loss: 0.00001513
Iteration 98/1000 | Loss: 0.00001513
Iteration 99/1000 | Loss: 0.00001513
Iteration 100/1000 | Loss: 0.00001512
Iteration 101/1000 | Loss: 0.00001512
Iteration 102/1000 | Loss: 0.00001512
Iteration 103/1000 | Loss: 0.00001511
Iteration 104/1000 | Loss: 0.00001511
Iteration 105/1000 | Loss: 0.00001511
Iteration 106/1000 | Loss: 0.00001510
Iteration 107/1000 | Loss: 0.00001510
Iteration 108/1000 | Loss: 0.00001510
Iteration 109/1000 | Loss: 0.00001510
Iteration 110/1000 | Loss: 0.00001510
Iteration 111/1000 | Loss: 0.00001509
Iteration 112/1000 | Loss: 0.00001509
Iteration 113/1000 | Loss: 0.00001509
Iteration 114/1000 | Loss: 0.00001509
Iteration 115/1000 | Loss: 0.00001508
Iteration 116/1000 | Loss: 0.00001508
Iteration 117/1000 | Loss: 0.00001508
Iteration 118/1000 | Loss: 0.00001507
Iteration 119/1000 | Loss: 0.00001507
Iteration 120/1000 | Loss: 0.00001507
Iteration 121/1000 | Loss: 0.00001507
Iteration 122/1000 | Loss: 0.00001507
Iteration 123/1000 | Loss: 0.00001507
Iteration 124/1000 | Loss: 0.00001507
Iteration 125/1000 | Loss: 0.00001507
Iteration 126/1000 | Loss: 0.00001507
Iteration 127/1000 | Loss: 0.00001507
Iteration 128/1000 | Loss: 0.00001506
Iteration 129/1000 | Loss: 0.00001506
Iteration 130/1000 | Loss: 0.00001506
Iteration 131/1000 | Loss: 0.00001506
Iteration 132/1000 | Loss: 0.00001506
Iteration 133/1000 | Loss: 0.00001505
Iteration 134/1000 | Loss: 0.00001505
Iteration 135/1000 | Loss: 0.00001505
Iteration 136/1000 | Loss: 0.00001505
Iteration 137/1000 | Loss: 0.00001505
Iteration 138/1000 | Loss: 0.00001505
Iteration 139/1000 | Loss: 0.00001505
Iteration 140/1000 | Loss: 0.00001504
Iteration 141/1000 | Loss: 0.00001504
Iteration 142/1000 | Loss: 0.00001504
Iteration 143/1000 | Loss: 0.00001504
Iteration 144/1000 | Loss: 0.00001504
Iteration 145/1000 | Loss: 0.00001504
Iteration 146/1000 | Loss: 0.00001503
Iteration 147/1000 | Loss: 0.00001503
Iteration 148/1000 | Loss: 0.00001503
Iteration 149/1000 | Loss: 0.00001503
Iteration 150/1000 | Loss: 0.00001502
Iteration 151/1000 | Loss: 0.00001502
Iteration 152/1000 | Loss: 0.00001502
Iteration 153/1000 | Loss: 0.00001502
Iteration 154/1000 | Loss: 0.00001502
Iteration 155/1000 | Loss: 0.00001502
Iteration 156/1000 | Loss: 0.00001501
Iteration 157/1000 | Loss: 0.00001501
Iteration 158/1000 | Loss: 0.00001501
Iteration 159/1000 | Loss: 0.00001501
Iteration 160/1000 | Loss: 0.00001501
Iteration 161/1000 | Loss: 0.00001501
Iteration 162/1000 | Loss: 0.00001501
Iteration 163/1000 | Loss: 0.00001501
Iteration 164/1000 | Loss: 0.00001501
Iteration 165/1000 | Loss: 0.00001501
Iteration 166/1000 | Loss: 0.00001501
Iteration 167/1000 | Loss: 0.00001501
Iteration 168/1000 | Loss: 0.00001501
Iteration 169/1000 | Loss: 0.00001500
Iteration 170/1000 | Loss: 0.00001500
Iteration 171/1000 | Loss: 0.00001500
Iteration 172/1000 | Loss: 0.00001500
Iteration 173/1000 | Loss: 0.00001500
Iteration 174/1000 | Loss: 0.00001500
Iteration 175/1000 | Loss: 0.00001500
Iteration 176/1000 | Loss: 0.00001500
Iteration 177/1000 | Loss: 0.00001500
Iteration 178/1000 | Loss: 0.00001500
Iteration 179/1000 | Loss: 0.00001499
Iteration 180/1000 | Loss: 0.00001499
Iteration 181/1000 | Loss: 0.00001499
Iteration 182/1000 | Loss: 0.00001499
Iteration 183/1000 | Loss: 0.00001499
Iteration 184/1000 | Loss: 0.00001499
Iteration 185/1000 | Loss: 0.00001499
Iteration 186/1000 | Loss: 0.00001499
Iteration 187/1000 | Loss: 0.00001499
Iteration 188/1000 | Loss: 0.00001499
Iteration 189/1000 | Loss: 0.00001499
Iteration 190/1000 | Loss: 0.00001499
Iteration 191/1000 | Loss: 0.00001499
Iteration 192/1000 | Loss: 0.00001499
Iteration 193/1000 | Loss: 0.00001499
Iteration 194/1000 | Loss: 0.00001499
Iteration 195/1000 | Loss: 0.00001499
Iteration 196/1000 | Loss: 0.00001498
Iteration 197/1000 | Loss: 0.00001498
Iteration 198/1000 | Loss: 0.00001498
Iteration 199/1000 | Loss: 0.00001498
Iteration 200/1000 | Loss: 0.00001498
Iteration 201/1000 | Loss: 0.00001498
Iteration 202/1000 | Loss: 0.00001498
Iteration 203/1000 | Loss: 0.00001498
Iteration 204/1000 | Loss: 0.00001498
Iteration 205/1000 | Loss: 0.00001498
Iteration 206/1000 | Loss: 0.00001498
Iteration 207/1000 | Loss: 0.00001498
Iteration 208/1000 | Loss: 0.00001498
Iteration 209/1000 | Loss: 0.00001498
Iteration 210/1000 | Loss: 0.00001498
Iteration 211/1000 | Loss: 0.00001498
Iteration 212/1000 | Loss: 0.00001498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.498328765592305e-05, 1.498328765592305e-05, 1.498328765592305e-05, 1.498328765592305e-05, 1.498328765592305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.498328765592305e-05

Optimization complete. Final v2v error: 3.2717971801757812 mm

Highest mean error: 3.616626024246216 mm for frame 114

Lowest mean error: 2.9979631900787354 mm for frame 126

Saving results

Total time: 41.030102014541626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821414
Iteration 2/25 | Loss: 0.00140380
Iteration 3/25 | Loss: 0.00129413
Iteration 4/25 | Loss: 0.00128318
Iteration 5/25 | Loss: 0.00128165
Iteration 6/25 | Loss: 0.00128165
Iteration 7/25 | Loss: 0.00128165
Iteration 8/25 | Loss: 0.00128165
Iteration 9/25 | Loss: 0.00128165
Iteration 10/25 | Loss: 0.00128165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001281651551835239, 0.001281651551835239, 0.001281651551835239, 0.001281651551835239, 0.001281651551835239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001281651551835239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38747752
Iteration 2/25 | Loss: 0.00076302
Iteration 3/25 | Loss: 0.00076302
Iteration 4/25 | Loss: 0.00076302
Iteration 5/25 | Loss: 0.00076301
Iteration 6/25 | Loss: 0.00076301
Iteration 7/25 | Loss: 0.00076301
Iteration 8/25 | Loss: 0.00076301
Iteration 9/25 | Loss: 0.00076301
Iteration 10/25 | Loss: 0.00076301
Iteration 11/25 | Loss: 0.00076301
Iteration 12/25 | Loss: 0.00076301
Iteration 13/25 | Loss: 0.00076301
Iteration 14/25 | Loss: 0.00076301
Iteration 15/25 | Loss: 0.00076301
Iteration 16/25 | Loss: 0.00076301
Iteration 17/25 | Loss: 0.00076301
Iteration 18/25 | Loss: 0.00076301
Iteration 19/25 | Loss: 0.00076301
Iteration 20/25 | Loss: 0.00076301
Iteration 21/25 | Loss: 0.00076301
Iteration 22/25 | Loss: 0.00076301
Iteration 23/25 | Loss: 0.00076301
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007630123873241246, 0.0007630123873241246, 0.0007630123873241246, 0.0007630123873241246, 0.0007630123873241246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007630123873241246

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076301
Iteration 2/1000 | Loss: 0.00002423
Iteration 3/1000 | Loss: 0.00001900
Iteration 4/1000 | Loss: 0.00001692
Iteration 5/1000 | Loss: 0.00001604
Iteration 6/1000 | Loss: 0.00001521
Iteration 7/1000 | Loss: 0.00001488
Iteration 8/1000 | Loss: 0.00001455
Iteration 9/1000 | Loss: 0.00001416
Iteration 10/1000 | Loss: 0.00001397
Iteration 11/1000 | Loss: 0.00001383
Iteration 12/1000 | Loss: 0.00001374
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001356
Iteration 15/1000 | Loss: 0.00001347
Iteration 16/1000 | Loss: 0.00001347
Iteration 17/1000 | Loss: 0.00001343
Iteration 18/1000 | Loss: 0.00001338
Iteration 19/1000 | Loss: 0.00001330
Iteration 20/1000 | Loss: 0.00001317
Iteration 21/1000 | Loss: 0.00001308
Iteration 22/1000 | Loss: 0.00001308
Iteration 23/1000 | Loss: 0.00001305
Iteration 24/1000 | Loss: 0.00001304
Iteration 25/1000 | Loss: 0.00001303
Iteration 26/1000 | Loss: 0.00001297
Iteration 27/1000 | Loss: 0.00001296
Iteration 28/1000 | Loss: 0.00001293
Iteration 29/1000 | Loss: 0.00001293
Iteration 30/1000 | Loss: 0.00001292
Iteration 31/1000 | Loss: 0.00001292
Iteration 32/1000 | Loss: 0.00001291
Iteration 33/1000 | Loss: 0.00001291
Iteration 34/1000 | Loss: 0.00001291
Iteration 35/1000 | Loss: 0.00001291
Iteration 36/1000 | Loss: 0.00001291
Iteration 37/1000 | Loss: 0.00001290
Iteration 38/1000 | Loss: 0.00001289
Iteration 39/1000 | Loss: 0.00001288
Iteration 40/1000 | Loss: 0.00001287
Iteration 41/1000 | Loss: 0.00001287
Iteration 42/1000 | Loss: 0.00001287
Iteration 43/1000 | Loss: 0.00001286
Iteration 44/1000 | Loss: 0.00001285
Iteration 45/1000 | Loss: 0.00001285
Iteration 46/1000 | Loss: 0.00001284
Iteration 47/1000 | Loss: 0.00001283
Iteration 48/1000 | Loss: 0.00001283
Iteration 49/1000 | Loss: 0.00001282
Iteration 50/1000 | Loss: 0.00001282
Iteration 51/1000 | Loss: 0.00001282
Iteration 52/1000 | Loss: 0.00001281
Iteration 53/1000 | Loss: 0.00001281
Iteration 54/1000 | Loss: 0.00001281
Iteration 55/1000 | Loss: 0.00001280
Iteration 56/1000 | Loss: 0.00001280
Iteration 57/1000 | Loss: 0.00001279
Iteration 58/1000 | Loss: 0.00001279
Iteration 59/1000 | Loss: 0.00001276
Iteration 60/1000 | Loss: 0.00001275
Iteration 61/1000 | Loss: 0.00001274
Iteration 62/1000 | Loss: 0.00001273
Iteration 63/1000 | Loss: 0.00001273
Iteration 64/1000 | Loss: 0.00001273
Iteration 65/1000 | Loss: 0.00001272
Iteration 66/1000 | Loss: 0.00001272
Iteration 67/1000 | Loss: 0.00001271
Iteration 68/1000 | Loss: 0.00001270
Iteration 69/1000 | Loss: 0.00001270
Iteration 70/1000 | Loss: 0.00001269
Iteration 71/1000 | Loss: 0.00001269
Iteration 72/1000 | Loss: 0.00001269
Iteration 73/1000 | Loss: 0.00001269
Iteration 74/1000 | Loss: 0.00001268
Iteration 75/1000 | Loss: 0.00001268
Iteration 76/1000 | Loss: 0.00001268
Iteration 77/1000 | Loss: 0.00001268
Iteration 78/1000 | Loss: 0.00001268
Iteration 79/1000 | Loss: 0.00001268
Iteration 80/1000 | Loss: 0.00001267
Iteration 81/1000 | Loss: 0.00001267
Iteration 82/1000 | Loss: 0.00001267
Iteration 83/1000 | Loss: 0.00001267
Iteration 84/1000 | Loss: 0.00001266
Iteration 85/1000 | Loss: 0.00001266
Iteration 86/1000 | Loss: 0.00001266
Iteration 87/1000 | Loss: 0.00001266
Iteration 88/1000 | Loss: 0.00001266
Iteration 89/1000 | Loss: 0.00001266
Iteration 90/1000 | Loss: 0.00001266
Iteration 91/1000 | Loss: 0.00001266
Iteration 92/1000 | Loss: 0.00001266
Iteration 93/1000 | Loss: 0.00001266
Iteration 94/1000 | Loss: 0.00001266
Iteration 95/1000 | Loss: 0.00001266
Iteration 96/1000 | Loss: 0.00001266
Iteration 97/1000 | Loss: 0.00001266
Iteration 98/1000 | Loss: 0.00001266
Iteration 99/1000 | Loss: 0.00001266
Iteration 100/1000 | Loss: 0.00001266
Iteration 101/1000 | Loss: 0.00001266
Iteration 102/1000 | Loss: 0.00001266
Iteration 103/1000 | Loss: 0.00001266
Iteration 104/1000 | Loss: 0.00001266
Iteration 105/1000 | Loss: 0.00001266
Iteration 106/1000 | Loss: 0.00001266
Iteration 107/1000 | Loss: 0.00001266
Iteration 108/1000 | Loss: 0.00001266
Iteration 109/1000 | Loss: 0.00001266
Iteration 110/1000 | Loss: 0.00001266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.2659089406952262e-05, 1.2659089406952262e-05, 1.2659089406952262e-05, 1.2659089406952262e-05, 1.2659089406952262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2659089406952262e-05

Optimization complete. Final v2v error: 3.046600341796875 mm

Highest mean error: 3.193089246749878 mm for frame 0

Lowest mean error: 2.8545544147491455 mm for frame 225

Saving results

Total time: 44.667534828186035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784243
Iteration 2/25 | Loss: 0.00149058
Iteration 3/25 | Loss: 0.00131214
Iteration 4/25 | Loss: 0.00129819
Iteration 5/25 | Loss: 0.00129314
Iteration 6/25 | Loss: 0.00129193
Iteration 7/25 | Loss: 0.00129193
Iteration 8/25 | Loss: 0.00129193
Iteration 9/25 | Loss: 0.00129193
Iteration 10/25 | Loss: 0.00129193
Iteration 11/25 | Loss: 0.00129193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012919277651235461, 0.0012919277651235461, 0.0012919277651235461, 0.0012919277651235461, 0.0012919277651235461]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012919277651235461

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22059429
Iteration 2/25 | Loss: 0.00096033
Iteration 3/25 | Loss: 0.00096033
Iteration 4/25 | Loss: 0.00096033
Iteration 5/25 | Loss: 0.00096033
Iteration 6/25 | Loss: 0.00096032
Iteration 7/25 | Loss: 0.00096032
Iteration 8/25 | Loss: 0.00096032
Iteration 9/25 | Loss: 0.00096032
Iteration 10/25 | Loss: 0.00096032
Iteration 11/25 | Loss: 0.00096032
Iteration 12/25 | Loss: 0.00096032
Iteration 13/25 | Loss: 0.00096032
Iteration 14/25 | Loss: 0.00096032
Iteration 15/25 | Loss: 0.00096032
Iteration 16/25 | Loss: 0.00096032
Iteration 17/25 | Loss: 0.00096032
Iteration 18/25 | Loss: 0.00096032
Iteration 19/25 | Loss: 0.00096032
Iteration 20/25 | Loss: 0.00096032
Iteration 21/25 | Loss: 0.00096032
Iteration 22/25 | Loss: 0.00096032
Iteration 23/25 | Loss: 0.00096032
Iteration 24/25 | Loss: 0.00096032
Iteration 25/25 | Loss: 0.00096032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096032
Iteration 2/1000 | Loss: 0.00003909
Iteration 3/1000 | Loss: 0.00002708
Iteration 4/1000 | Loss: 0.00002265
Iteration 5/1000 | Loss: 0.00002061
Iteration 6/1000 | Loss: 0.00001931
Iteration 7/1000 | Loss: 0.00001841
Iteration 8/1000 | Loss: 0.00001776
Iteration 9/1000 | Loss: 0.00001716
Iteration 10/1000 | Loss: 0.00001681
Iteration 11/1000 | Loss: 0.00001657
Iteration 12/1000 | Loss: 0.00001625
Iteration 13/1000 | Loss: 0.00001607
Iteration 14/1000 | Loss: 0.00001606
Iteration 15/1000 | Loss: 0.00001601
Iteration 16/1000 | Loss: 0.00001590
Iteration 17/1000 | Loss: 0.00001578
Iteration 18/1000 | Loss: 0.00001578
Iteration 19/1000 | Loss: 0.00001576
Iteration 20/1000 | Loss: 0.00001566
Iteration 21/1000 | Loss: 0.00001561
Iteration 22/1000 | Loss: 0.00001558
Iteration 23/1000 | Loss: 0.00001557
Iteration 24/1000 | Loss: 0.00001554
Iteration 25/1000 | Loss: 0.00001552
Iteration 26/1000 | Loss: 0.00001552
Iteration 27/1000 | Loss: 0.00001551
Iteration 28/1000 | Loss: 0.00001550
Iteration 29/1000 | Loss: 0.00001547
Iteration 30/1000 | Loss: 0.00001546
Iteration 31/1000 | Loss: 0.00001542
Iteration 32/1000 | Loss: 0.00001538
Iteration 33/1000 | Loss: 0.00001538
Iteration 34/1000 | Loss: 0.00001537
Iteration 35/1000 | Loss: 0.00001537
Iteration 36/1000 | Loss: 0.00001536
Iteration 37/1000 | Loss: 0.00001536
Iteration 38/1000 | Loss: 0.00001535
Iteration 39/1000 | Loss: 0.00001535
Iteration 40/1000 | Loss: 0.00001534
Iteration 41/1000 | Loss: 0.00001533
Iteration 42/1000 | Loss: 0.00001533
Iteration 43/1000 | Loss: 0.00001533
Iteration 44/1000 | Loss: 0.00001532
Iteration 45/1000 | Loss: 0.00001532
Iteration 46/1000 | Loss: 0.00001532
Iteration 47/1000 | Loss: 0.00001532
Iteration 48/1000 | Loss: 0.00001532
Iteration 49/1000 | Loss: 0.00001531
Iteration 50/1000 | Loss: 0.00001531
Iteration 51/1000 | Loss: 0.00001531
Iteration 52/1000 | Loss: 0.00001531
Iteration 53/1000 | Loss: 0.00001531
Iteration 54/1000 | Loss: 0.00001531
Iteration 55/1000 | Loss: 0.00001531
Iteration 56/1000 | Loss: 0.00001531
Iteration 57/1000 | Loss: 0.00001531
Iteration 58/1000 | Loss: 0.00001530
Iteration 59/1000 | Loss: 0.00001530
Iteration 60/1000 | Loss: 0.00001530
Iteration 61/1000 | Loss: 0.00001530
Iteration 62/1000 | Loss: 0.00001530
Iteration 63/1000 | Loss: 0.00001530
Iteration 64/1000 | Loss: 0.00001529
Iteration 65/1000 | Loss: 0.00001529
Iteration 66/1000 | Loss: 0.00001529
Iteration 67/1000 | Loss: 0.00001529
Iteration 68/1000 | Loss: 0.00001529
Iteration 69/1000 | Loss: 0.00001529
Iteration 70/1000 | Loss: 0.00001529
Iteration 71/1000 | Loss: 0.00001528
Iteration 72/1000 | Loss: 0.00001528
Iteration 73/1000 | Loss: 0.00001528
Iteration 74/1000 | Loss: 0.00001528
Iteration 75/1000 | Loss: 0.00001527
Iteration 76/1000 | Loss: 0.00001527
Iteration 77/1000 | Loss: 0.00001527
Iteration 78/1000 | Loss: 0.00001526
Iteration 79/1000 | Loss: 0.00001525
Iteration 80/1000 | Loss: 0.00001525
Iteration 81/1000 | Loss: 0.00001524
Iteration 82/1000 | Loss: 0.00001524
Iteration 83/1000 | Loss: 0.00001524
Iteration 84/1000 | Loss: 0.00001524
Iteration 85/1000 | Loss: 0.00001524
Iteration 86/1000 | Loss: 0.00001523
Iteration 87/1000 | Loss: 0.00001523
Iteration 88/1000 | Loss: 0.00001523
Iteration 89/1000 | Loss: 0.00001523
Iteration 90/1000 | Loss: 0.00001523
Iteration 91/1000 | Loss: 0.00001523
Iteration 92/1000 | Loss: 0.00001522
Iteration 93/1000 | Loss: 0.00001522
Iteration 94/1000 | Loss: 0.00001522
Iteration 95/1000 | Loss: 0.00001522
Iteration 96/1000 | Loss: 0.00001522
Iteration 97/1000 | Loss: 0.00001522
Iteration 98/1000 | Loss: 0.00001522
Iteration 99/1000 | Loss: 0.00001522
Iteration 100/1000 | Loss: 0.00001521
Iteration 101/1000 | Loss: 0.00001521
Iteration 102/1000 | Loss: 0.00001521
Iteration 103/1000 | Loss: 0.00001521
Iteration 104/1000 | Loss: 0.00001520
Iteration 105/1000 | Loss: 0.00001520
Iteration 106/1000 | Loss: 0.00001520
Iteration 107/1000 | Loss: 0.00001520
Iteration 108/1000 | Loss: 0.00001519
Iteration 109/1000 | Loss: 0.00001519
Iteration 110/1000 | Loss: 0.00001519
Iteration 111/1000 | Loss: 0.00001519
Iteration 112/1000 | Loss: 0.00001519
Iteration 113/1000 | Loss: 0.00001519
Iteration 114/1000 | Loss: 0.00001518
Iteration 115/1000 | Loss: 0.00001518
Iteration 116/1000 | Loss: 0.00001518
Iteration 117/1000 | Loss: 0.00001518
Iteration 118/1000 | Loss: 0.00001517
Iteration 119/1000 | Loss: 0.00001517
Iteration 120/1000 | Loss: 0.00001517
Iteration 121/1000 | Loss: 0.00001516
Iteration 122/1000 | Loss: 0.00001516
Iteration 123/1000 | Loss: 0.00001516
Iteration 124/1000 | Loss: 0.00001515
Iteration 125/1000 | Loss: 0.00001515
Iteration 126/1000 | Loss: 0.00001515
Iteration 127/1000 | Loss: 0.00001515
Iteration 128/1000 | Loss: 0.00001515
Iteration 129/1000 | Loss: 0.00001515
Iteration 130/1000 | Loss: 0.00001515
Iteration 131/1000 | Loss: 0.00001515
Iteration 132/1000 | Loss: 0.00001514
Iteration 133/1000 | Loss: 0.00001514
Iteration 134/1000 | Loss: 0.00001514
Iteration 135/1000 | Loss: 0.00001514
Iteration 136/1000 | Loss: 0.00001514
Iteration 137/1000 | Loss: 0.00001514
Iteration 138/1000 | Loss: 0.00001514
Iteration 139/1000 | Loss: 0.00001514
Iteration 140/1000 | Loss: 0.00001514
Iteration 141/1000 | Loss: 0.00001514
Iteration 142/1000 | Loss: 0.00001514
Iteration 143/1000 | Loss: 0.00001513
Iteration 144/1000 | Loss: 0.00001513
Iteration 145/1000 | Loss: 0.00001513
Iteration 146/1000 | Loss: 0.00001513
Iteration 147/1000 | Loss: 0.00001513
Iteration 148/1000 | Loss: 0.00001513
Iteration 149/1000 | Loss: 0.00001513
Iteration 150/1000 | Loss: 0.00001513
Iteration 151/1000 | Loss: 0.00001513
Iteration 152/1000 | Loss: 0.00001513
Iteration 153/1000 | Loss: 0.00001513
Iteration 154/1000 | Loss: 0.00001513
Iteration 155/1000 | Loss: 0.00001513
Iteration 156/1000 | Loss: 0.00001512
Iteration 157/1000 | Loss: 0.00001512
Iteration 158/1000 | Loss: 0.00001512
Iteration 159/1000 | Loss: 0.00001512
Iteration 160/1000 | Loss: 0.00001512
Iteration 161/1000 | Loss: 0.00001512
Iteration 162/1000 | Loss: 0.00001512
Iteration 163/1000 | Loss: 0.00001512
Iteration 164/1000 | Loss: 0.00001512
Iteration 165/1000 | Loss: 0.00001512
Iteration 166/1000 | Loss: 0.00001512
Iteration 167/1000 | Loss: 0.00001512
Iteration 168/1000 | Loss: 0.00001511
Iteration 169/1000 | Loss: 0.00001511
Iteration 170/1000 | Loss: 0.00001511
Iteration 171/1000 | Loss: 0.00001511
Iteration 172/1000 | Loss: 0.00001511
Iteration 173/1000 | Loss: 0.00001511
Iteration 174/1000 | Loss: 0.00001511
Iteration 175/1000 | Loss: 0.00001511
Iteration 176/1000 | Loss: 0.00001511
Iteration 177/1000 | Loss: 0.00001511
Iteration 178/1000 | Loss: 0.00001510
Iteration 179/1000 | Loss: 0.00001510
Iteration 180/1000 | Loss: 0.00001510
Iteration 181/1000 | Loss: 0.00001510
Iteration 182/1000 | Loss: 0.00001510
Iteration 183/1000 | Loss: 0.00001510
Iteration 184/1000 | Loss: 0.00001510
Iteration 185/1000 | Loss: 0.00001510
Iteration 186/1000 | Loss: 0.00001510
Iteration 187/1000 | Loss: 0.00001510
Iteration 188/1000 | Loss: 0.00001510
Iteration 189/1000 | Loss: 0.00001510
Iteration 190/1000 | Loss: 0.00001510
Iteration 191/1000 | Loss: 0.00001510
Iteration 192/1000 | Loss: 0.00001510
Iteration 193/1000 | Loss: 0.00001510
Iteration 194/1000 | Loss: 0.00001510
Iteration 195/1000 | Loss: 0.00001510
Iteration 196/1000 | Loss: 0.00001510
Iteration 197/1000 | Loss: 0.00001510
Iteration 198/1000 | Loss: 0.00001510
Iteration 199/1000 | Loss: 0.00001510
Iteration 200/1000 | Loss: 0.00001510
Iteration 201/1000 | Loss: 0.00001510
Iteration 202/1000 | Loss: 0.00001510
Iteration 203/1000 | Loss: 0.00001510
Iteration 204/1000 | Loss: 0.00001510
Iteration 205/1000 | Loss: 0.00001510
Iteration 206/1000 | Loss: 0.00001510
Iteration 207/1000 | Loss: 0.00001510
Iteration 208/1000 | Loss: 0.00001510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.509775665908819e-05, 1.509775665908819e-05, 1.509775665908819e-05, 1.509775665908819e-05, 1.509775665908819e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.509775665908819e-05

Optimization complete. Final v2v error: 3.2461719512939453 mm

Highest mean error: 4.536398887634277 mm for frame 76

Lowest mean error: 2.7939956188201904 mm for frame 31

Saving results

Total time: 46.379274129867554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037370
Iteration 2/25 | Loss: 0.00204802
Iteration 3/25 | Loss: 0.00161712
Iteration 4/25 | Loss: 0.00152299
Iteration 5/25 | Loss: 0.00141312
Iteration 6/25 | Loss: 0.00137865
Iteration 7/25 | Loss: 0.00136860
Iteration 8/25 | Loss: 0.00135225
Iteration 9/25 | Loss: 0.00134806
Iteration 10/25 | Loss: 0.00134182
Iteration 11/25 | Loss: 0.00133404
Iteration 12/25 | Loss: 0.00133333
Iteration 13/25 | Loss: 0.00133318
Iteration 14/25 | Loss: 0.00133311
Iteration 15/25 | Loss: 0.00133309
Iteration 16/25 | Loss: 0.00133309
Iteration 17/25 | Loss: 0.00133309
Iteration 18/25 | Loss: 0.00133309
Iteration 19/25 | Loss: 0.00133309
Iteration 20/25 | Loss: 0.00133309
Iteration 21/25 | Loss: 0.00133309
Iteration 22/25 | Loss: 0.00133308
Iteration 23/25 | Loss: 0.00133308
Iteration 24/25 | Loss: 0.00133307
Iteration 25/25 | Loss: 0.00133307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48695719
Iteration 2/25 | Loss: 0.00107859
Iteration 3/25 | Loss: 0.00107859
Iteration 4/25 | Loss: 0.00107858
Iteration 5/25 | Loss: 0.00101414
Iteration 6/25 | Loss: 0.00101414
Iteration 7/25 | Loss: 0.00101414
Iteration 8/25 | Loss: 0.00101414
Iteration 9/25 | Loss: 0.00101414
Iteration 10/25 | Loss: 0.00101414
Iteration 11/25 | Loss: 0.00101414
Iteration 12/25 | Loss: 0.00101414
Iteration 13/25 | Loss: 0.00101414
Iteration 14/25 | Loss: 0.00101414
Iteration 15/25 | Loss: 0.00101414
Iteration 16/25 | Loss: 0.00101414
Iteration 17/25 | Loss: 0.00101414
Iteration 18/25 | Loss: 0.00101414
Iteration 19/25 | Loss: 0.00101414
Iteration 20/25 | Loss: 0.00101414
Iteration 21/25 | Loss: 0.00101414
Iteration 22/25 | Loss: 0.00101414
Iteration 23/25 | Loss: 0.00101414
Iteration 24/25 | Loss: 0.00101414
Iteration 25/25 | Loss: 0.00101414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101414
Iteration 2/1000 | Loss: 0.00019142
Iteration 3/1000 | Loss: 0.00002247
Iteration 4/1000 | Loss: 0.00012425
Iteration 5/1000 | Loss: 0.00001890
Iteration 6/1000 | Loss: 0.00012789
Iteration 7/1000 | Loss: 0.00001840
Iteration 8/1000 | Loss: 0.00001803
Iteration 9/1000 | Loss: 0.00012956
Iteration 10/1000 | Loss: 0.00001768
Iteration 11/1000 | Loss: 0.00001717
Iteration 12/1000 | Loss: 0.00001694
Iteration 13/1000 | Loss: 0.00001677
Iteration 14/1000 | Loss: 0.00001674
Iteration 15/1000 | Loss: 0.00001666
Iteration 16/1000 | Loss: 0.00001663
Iteration 17/1000 | Loss: 0.00001662
Iteration 18/1000 | Loss: 0.00001646
Iteration 19/1000 | Loss: 0.00001633
Iteration 20/1000 | Loss: 0.00001627
Iteration 21/1000 | Loss: 0.00015457
Iteration 22/1000 | Loss: 0.00001640
Iteration 23/1000 | Loss: 0.00001616
Iteration 24/1000 | Loss: 0.00001614
Iteration 25/1000 | Loss: 0.00001613
Iteration 26/1000 | Loss: 0.00001612
Iteration 27/1000 | Loss: 0.00001612
Iteration 28/1000 | Loss: 0.00001612
Iteration 29/1000 | Loss: 0.00001612
Iteration 30/1000 | Loss: 0.00001612
Iteration 31/1000 | Loss: 0.00001612
Iteration 32/1000 | Loss: 0.00001611
Iteration 33/1000 | Loss: 0.00001611
Iteration 34/1000 | Loss: 0.00001611
Iteration 35/1000 | Loss: 0.00001611
Iteration 36/1000 | Loss: 0.00001611
Iteration 37/1000 | Loss: 0.00001611
Iteration 38/1000 | Loss: 0.00001611
Iteration 39/1000 | Loss: 0.00001611
Iteration 40/1000 | Loss: 0.00001610
Iteration 41/1000 | Loss: 0.00001610
Iteration 42/1000 | Loss: 0.00001610
Iteration 43/1000 | Loss: 0.00001609
Iteration 44/1000 | Loss: 0.00001607
Iteration 45/1000 | Loss: 0.00001603
Iteration 46/1000 | Loss: 0.00001603
Iteration 47/1000 | Loss: 0.00001603
Iteration 48/1000 | Loss: 0.00001603
Iteration 49/1000 | Loss: 0.00001603
Iteration 50/1000 | Loss: 0.00001603
Iteration 51/1000 | Loss: 0.00001603
Iteration 52/1000 | Loss: 0.00001603
Iteration 53/1000 | Loss: 0.00001602
Iteration 54/1000 | Loss: 0.00001602
Iteration 55/1000 | Loss: 0.00001602
Iteration 56/1000 | Loss: 0.00001601
Iteration 57/1000 | Loss: 0.00001598
Iteration 58/1000 | Loss: 0.00001598
Iteration 59/1000 | Loss: 0.00001598
Iteration 60/1000 | Loss: 0.00001598
Iteration 61/1000 | Loss: 0.00001597
Iteration 62/1000 | Loss: 0.00001597
Iteration 63/1000 | Loss: 0.00001597
Iteration 64/1000 | Loss: 0.00001597
Iteration 65/1000 | Loss: 0.00001597
Iteration 66/1000 | Loss: 0.00001597
Iteration 67/1000 | Loss: 0.00001596
Iteration 68/1000 | Loss: 0.00001596
Iteration 69/1000 | Loss: 0.00001596
Iteration 70/1000 | Loss: 0.00001596
Iteration 71/1000 | Loss: 0.00001596
Iteration 72/1000 | Loss: 0.00001595
Iteration 73/1000 | Loss: 0.00001595
Iteration 74/1000 | Loss: 0.00001595
Iteration 75/1000 | Loss: 0.00001595
Iteration 76/1000 | Loss: 0.00001595
Iteration 77/1000 | Loss: 0.00001594
Iteration 78/1000 | Loss: 0.00001594
Iteration 79/1000 | Loss: 0.00001594
Iteration 80/1000 | Loss: 0.00001593
Iteration 81/1000 | Loss: 0.00001593
Iteration 82/1000 | Loss: 0.00001593
Iteration 83/1000 | Loss: 0.00001593
Iteration 84/1000 | Loss: 0.00001593
Iteration 85/1000 | Loss: 0.00001593
Iteration 86/1000 | Loss: 0.00001593
Iteration 87/1000 | Loss: 0.00001593
Iteration 88/1000 | Loss: 0.00001592
Iteration 89/1000 | Loss: 0.00001592
Iteration 90/1000 | Loss: 0.00001591
Iteration 91/1000 | Loss: 0.00001591
Iteration 92/1000 | Loss: 0.00001591
Iteration 93/1000 | Loss: 0.00001591
Iteration 94/1000 | Loss: 0.00001591
Iteration 95/1000 | Loss: 0.00001591
Iteration 96/1000 | Loss: 0.00001591
Iteration 97/1000 | Loss: 0.00001591
Iteration 98/1000 | Loss: 0.00001591
Iteration 99/1000 | Loss: 0.00001591
Iteration 100/1000 | Loss: 0.00001591
Iteration 101/1000 | Loss: 0.00001591
Iteration 102/1000 | Loss: 0.00001590
Iteration 103/1000 | Loss: 0.00001590
Iteration 104/1000 | Loss: 0.00001590
Iteration 105/1000 | Loss: 0.00001589
Iteration 106/1000 | Loss: 0.00001589
Iteration 107/1000 | Loss: 0.00001589
Iteration 108/1000 | Loss: 0.00001589
Iteration 109/1000 | Loss: 0.00001589
Iteration 110/1000 | Loss: 0.00001589
Iteration 111/1000 | Loss: 0.00001589
Iteration 112/1000 | Loss: 0.00001588
Iteration 113/1000 | Loss: 0.00001588
Iteration 114/1000 | Loss: 0.00001588
Iteration 115/1000 | Loss: 0.00001588
Iteration 116/1000 | Loss: 0.00001587
Iteration 117/1000 | Loss: 0.00001587
Iteration 118/1000 | Loss: 0.00001587
Iteration 119/1000 | Loss: 0.00001587
Iteration 120/1000 | Loss: 0.00001586
Iteration 121/1000 | Loss: 0.00001586
Iteration 122/1000 | Loss: 0.00001586
Iteration 123/1000 | Loss: 0.00001586
Iteration 124/1000 | Loss: 0.00001585
Iteration 125/1000 | Loss: 0.00001585
Iteration 126/1000 | Loss: 0.00001585
Iteration 127/1000 | Loss: 0.00001585
Iteration 128/1000 | Loss: 0.00001585
Iteration 129/1000 | Loss: 0.00001585
Iteration 130/1000 | Loss: 0.00001585
Iteration 131/1000 | Loss: 0.00001584
Iteration 132/1000 | Loss: 0.00001584
Iteration 133/1000 | Loss: 0.00001584
Iteration 134/1000 | Loss: 0.00001584
Iteration 135/1000 | Loss: 0.00001584
Iteration 136/1000 | Loss: 0.00001584
Iteration 137/1000 | Loss: 0.00001583
Iteration 138/1000 | Loss: 0.00001583
Iteration 139/1000 | Loss: 0.00001583
Iteration 140/1000 | Loss: 0.00001583
Iteration 141/1000 | Loss: 0.00001583
Iteration 142/1000 | Loss: 0.00001583
Iteration 143/1000 | Loss: 0.00001583
Iteration 144/1000 | Loss: 0.00001583
Iteration 145/1000 | Loss: 0.00001583
Iteration 146/1000 | Loss: 0.00001583
Iteration 147/1000 | Loss: 0.00001583
Iteration 148/1000 | Loss: 0.00001583
Iteration 149/1000 | Loss: 0.00001583
Iteration 150/1000 | Loss: 0.00001582
Iteration 151/1000 | Loss: 0.00001582
Iteration 152/1000 | Loss: 0.00001582
Iteration 153/1000 | Loss: 0.00001582
Iteration 154/1000 | Loss: 0.00001582
Iteration 155/1000 | Loss: 0.00001582
Iteration 156/1000 | Loss: 0.00001582
Iteration 157/1000 | Loss: 0.00001582
Iteration 158/1000 | Loss: 0.00001582
Iteration 159/1000 | Loss: 0.00001582
Iteration 160/1000 | Loss: 0.00001581
Iteration 161/1000 | Loss: 0.00001581
Iteration 162/1000 | Loss: 0.00001581
Iteration 163/1000 | Loss: 0.00001581
Iteration 164/1000 | Loss: 0.00001581
Iteration 165/1000 | Loss: 0.00001581
Iteration 166/1000 | Loss: 0.00001581
Iteration 167/1000 | Loss: 0.00001581
Iteration 168/1000 | Loss: 0.00001581
Iteration 169/1000 | Loss: 0.00001581
Iteration 170/1000 | Loss: 0.00001581
Iteration 171/1000 | Loss: 0.00001581
Iteration 172/1000 | Loss: 0.00001581
Iteration 173/1000 | Loss: 0.00001580
Iteration 174/1000 | Loss: 0.00001580
Iteration 175/1000 | Loss: 0.00001580
Iteration 176/1000 | Loss: 0.00001580
Iteration 177/1000 | Loss: 0.00001580
Iteration 178/1000 | Loss: 0.00001580
Iteration 179/1000 | Loss: 0.00001580
Iteration 180/1000 | Loss: 0.00001580
Iteration 181/1000 | Loss: 0.00001580
Iteration 182/1000 | Loss: 0.00001580
Iteration 183/1000 | Loss: 0.00001580
Iteration 184/1000 | Loss: 0.00001580
Iteration 185/1000 | Loss: 0.00001580
Iteration 186/1000 | Loss: 0.00001580
Iteration 187/1000 | Loss: 0.00001580
Iteration 188/1000 | Loss: 0.00001580
Iteration 189/1000 | Loss: 0.00001580
Iteration 190/1000 | Loss: 0.00001580
Iteration 191/1000 | Loss: 0.00001580
Iteration 192/1000 | Loss: 0.00001580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.5800191249581985e-05, 1.5800191249581985e-05, 1.5800191249581985e-05, 1.5800191249581985e-05, 1.5800191249581985e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5800191249581985e-05

Optimization complete. Final v2v error: 3.3650667667388916 mm

Highest mean error: 3.643134117126465 mm for frame 108

Lowest mean error: 3.1550614833831787 mm for frame 46

Saving results

Total time: 61.52584671974182
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948983
Iteration 2/25 | Loss: 0.00229935
Iteration 3/25 | Loss: 0.00173626
Iteration 4/25 | Loss: 0.00164247
Iteration 5/25 | Loss: 0.00154588
Iteration 6/25 | Loss: 0.00151513
Iteration 7/25 | Loss: 0.00147889
Iteration 8/25 | Loss: 0.00144374
Iteration 9/25 | Loss: 0.00143412
Iteration 10/25 | Loss: 0.00142690
Iteration 11/25 | Loss: 0.00142170
Iteration 12/25 | Loss: 0.00141736
Iteration 13/25 | Loss: 0.00142304
Iteration 14/25 | Loss: 0.00141515
Iteration 15/25 | Loss: 0.00141288
Iteration 16/25 | Loss: 0.00141244
Iteration 17/25 | Loss: 0.00141232
Iteration 18/25 | Loss: 0.00141232
Iteration 19/25 | Loss: 0.00141232
Iteration 20/25 | Loss: 0.00141232
Iteration 21/25 | Loss: 0.00141229
Iteration 22/25 | Loss: 0.00141228
Iteration 23/25 | Loss: 0.00141228
Iteration 24/25 | Loss: 0.00141228
Iteration 25/25 | Loss: 0.00141227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34862840
Iteration 2/25 | Loss: 0.00087064
Iteration 3/25 | Loss: 0.00087064
Iteration 4/25 | Loss: 0.00087064
Iteration 5/25 | Loss: 0.00087063
Iteration 6/25 | Loss: 0.00087063
Iteration 7/25 | Loss: 0.00087063
Iteration 8/25 | Loss: 0.00087063
Iteration 9/25 | Loss: 0.00087063
Iteration 10/25 | Loss: 0.00087063
Iteration 11/25 | Loss: 0.00087063
Iteration 12/25 | Loss: 0.00087063
Iteration 13/25 | Loss: 0.00087063
Iteration 14/25 | Loss: 0.00087063
Iteration 15/25 | Loss: 0.00087063
Iteration 16/25 | Loss: 0.00087063
Iteration 17/25 | Loss: 0.00087063
Iteration 18/25 | Loss: 0.00087063
Iteration 19/25 | Loss: 0.00087063
Iteration 20/25 | Loss: 0.00087063
Iteration 21/25 | Loss: 0.00087063
Iteration 22/25 | Loss: 0.00087063
Iteration 23/25 | Loss: 0.00087063
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008706333464942873, 0.0008706333464942873, 0.0008706333464942873, 0.0008706333464942873, 0.0008706333464942873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008706333464942873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087063
Iteration 2/1000 | Loss: 0.00004448
Iteration 3/1000 | Loss: 0.00003344
Iteration 4/1000 | Loss: 0.00002839
Iteration 5/1000 | Loss: 0.00002672
Iteration 6/1000 | Loss: 0.00002583
Iteration 7/1000 | Loss: 0.00002539
Iteration 8/1000 | Loss: 0.00002501
Iteration 9/1000 | Loss: 0.00002471
Iteration 10/1000 | Loss: 0.00002444
Iteration 11/1000 | Loss: 0.00002442
Iteration 12/1000 | Loss: 0.00002420
Iteration 13/1000 | Loss: 0.00002405
Iteration 14/1000 | Loss: 0.00002402
Iteration 15/1000 | Loss: 0.00002399
Iteration 16/1000 | Loss: 0.00002399
Iteration 17/1000 | Loss: 0.00002391
Iteration 18/1000 | Loss: 0.00002391
Iteration 19/1000 | Loss: 0.00002382
Iteration 20/1000 | Loss: 0.00002373
Iteration 21/1000 | Loss: 0.00002370
Iteration 22/1000 | Loss: 0.00002370
Iteration 23/1000 | Loss: 0.00002370
Iteration 24/1000 | Loss: 0.00002369
Iteration 25/1000 | Loss: 0.00002369
Iteration 26/1000 | Loss: 0.00002368
Iteration 27/1000 | Loss: 0.00002368
Iteration 28/1000 | Loss: 0.00002363
Iteration 29/1000 | Loss: 0.00002361
Iteration 30/1000 | Loss: 0.00002361
Iteration 31/1000 | Loss: 0.00002361
Iteration 32/1000 | Loss: 0.00002361
Iteration 33/1000 | Loss: 0.00002361
Iteration 34/1000 | Loss: 0.00002360
Iteration 35/1000 | Loss: 0.00002359
Iteration 36/1000 | Loss: 0.00002359
Iteration 37/1000 | Loss: 0.00002359
Iteration 38/1000 | Loss: 0.00002358
Iteration 39/1000 | Loss: 0.00002358
Iteration 40/1000 | Loss: 0.00002357
Iteration 41/1000 | Loss: 0.00002357
Iteration 42/1000 | Loss: 0.00002357
Iteration 43/1000 | Loss: 0.00002357
Iteration 44/1000 | Loss: 0.00002357
Iteration 45/1000 | Loss: 0.00002356
Iteration 46/1000 | Loss: 0.00002356
Iteration 47/1000 | Loss: 0.00002355
Iteration 48/1000 | Loss: 0.00002355
Iteration 49/1000 | Loss: 0.00002355
Iteration 50/1000 | Loss: 0.00002354
Iteration 51/1000 | Loss: 0.00002354
Iteration 52/1000 | Loss: 0.00002354
Iteration 53/1000 | Loss: 0.00002353
Iteration 54/1000 | Loss: 0.00002353
Iteration 55/1000 | Loss: 0.00002352
Iteration 56/1000 | Loss: 0.00002352
Iteration 57/1000 | Loss: 0.00002352
Iteration 58/1000 | Loss: 0.00002351
Iteration 59/1000 | Loss: 0.00002351
Iteration 60/1000 | Loss: 0.00002351
Iteration 61/1000 | Loss: 0.00002351
Iteration 62/1000 | Loss: 0.00002351
Iteration 63/1000 | Loss: 0.00002351
Iteration 64/1000 | Loss: 0.00002351
Iteration 65/1000 | Loss: 0.00002351
Iteration 66/1000 | Loss: 0.00002351
Iteration 67/1000 | Loss: 0.00002351
Iteration 68/1000 | Loss: 0.00002351
Iteration 69/1000 | Loss: 0.00002350
Iteration 70/1000 | Loss: 0.00002350
Iteration 71/1000 | Loss: 0.00002350
Iteration 72/1000 | Loss: 0.00002349
Iteration 73/1000 | Loss: 0.00002348
Iteration 74/1000 | Loss: 0.00002347
Iteration 75/1000 | Loss: 0.00002347
Iteration 76/1000 | Loss: 0.00002346
Iteration 77/1000 | Loss: 0.00002346
Iteration 78/1000 | Loss: 0.00002346
Iteration 79/1000 | Loss: 0.00002346
Iteration 80/1000 | Loss: 0.00002346
Iteration 81/1000 | Loss: 0.00002346
Iteration 82/1000 | Loss: 0.00002345
Iteration 83/1000 | Loss: 0.00002345
Iteration 84/1000 | Loss: 0.00002345
Iteration 85/1000 | Loss: 0.00002345
Iteration 86/1000 | Loss: 0.00002344
Iteration 87/1000 | Loss: 0.00002344
Iteration 88/1000 | Loss: 0.00002344
Iteration 89/1000 | Loss: 0.00002344
Iteration 90/1000 | Loss: 0.00002344
Iteration 91/1000 | Loss: 0.00002344
Iteration 92/1000 | Loss: 0.00002344
Iteration 93/1000 | Loss: 0.00002344
Iteration 94/1000 | Loss: 0.00002344
Iteration 95/1000 | Loss: 0.00002344
Iteration 96/1000 | Loss: 0.00002344
Iteration 97/1000 | Loss: 0.00002344
Iteration 98/1000 | Loss: 0.00002344
Iteration 99/1000 | Loss: 0.00002344
Iteration 100/1000 | Loss: 0.00002343
Iteration 101/1000 | Loss: 0.00002343
Iteration 102/1000 | Loss: 0.00002343
Iteration 103/1000 | Loss: 0.00002343
Iteration 104/1000 | Loss: 0.00002343
Iteration 105/1000 | Loss: 0.00002343
Iteration 106/1000 | Loss: 0.00002343
Iteration 107/1000 | Loss: 0.00002342
Iteration 108/1000 | Loss: 0.00002341
Iteration 109/1000 | Loss: 0.00002341
Iteration 110/1000 | Loss: 0.00002341
Iteration 111/1000 | Loss: 0.00002340
Iteration 112/1000 | Loss: 0.00002340
Iteration 113/1000 | Loss: 0.00002340
Iteration 114/1000 | Loss: 0.00002340
Iteration 115/1000 | Loss: 0.00002340
Iteration 116/1000 | Loss: 0.00002340
Iteration 117/1000 | Loss: 0.00002339
Iteration 118/1000 | Loss: 0.00002339
Iteration 119/1000 | Loss: 0.00002339
Iteration 120/1000 | Loss: 0.00002338
Iteration 121/1000 | Loss: 0.00002337
Iteration 122/1000 | Loss: 0.00002337
Iteration 123/1000 | Loss: 0.00002337
Iteration 124/1000 | Loss: 0.00002337
Iteration 125/1000 | Loss: 0.00002337
Iteration 126/1000 | Loss: 0.00002337
Iteration 127/1000 | Loss: 0.00002337
Iteration 128/1000 | Loss: 0.00002337
Iteration 129/1000 | Loss: 0.00002336
Iteration 130/1000 | Loss: 0.00002336
Iteration 131/1000 | Loss: 0.00002336
Iteration 132/1000 | Loss: 0.00002336
Iteration 133/1000 | Loss: 0.00002336
Iteration 134/1000 | Loss: 0.00002336
Iteration 135/1000 | Loss: 0.00002336
Iteration 136/1000 | Loss: 0.00002335
Iteration 137/1000 | Loss: 0.00002335
Iteration 138/1000 | Loss: 0.00002335
Iteration 139/1000 | Loss: 0.00002335
Iteration 140/1000 | Loss: 0.00002335
Iteration 141/1000 | Loss: 0.00002335
Iteration 142/1000 | Loss: 0.00002335
Iteration 143/1000 | Loss: 0.00002335
Iteration 144/1000 | Loss: 0.00002335
Iteration 145/1000 | Loss: 0.00002335
Iteration 146/1000 | Loss: 0.00002335
Iteration 147/1000 | Loss: 0.00002335
Iteration 148/1000 | Loss: 0.00002335
Iteration 149/1000 | Loss: 0.00002335
Iteration 150/1000 | Loss: 0.00002335
Iteration 151/1000 | Loss: 0.00002335
Iteration 152/1000 | Loss: 0.00002335
Iteration 153/1000 | Loss: 0.00002335
Iteration 154/1000 | Loss: 0.00002335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.3345322915702127e-05, 2.3345322915702127e-05, 2.3345322915702127e-05, 2.3345322915702127e-05, 2.3345322915702127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3345322915702127e-05

Optimization complete. Final v2v error: 4.0398712158203125 mm

Highest mean error: 4.438680648803711 mm for frame 31

Lowest mean error: 3.768867015838623 mm for frame 55

Saving results

Total time: 60.32141613960266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969615
Iteration 2/25 | Loss: 0.00179644
Iteration 3/25 | Loss: 0.00148300
Iteration 4/25 | Loss: 0.00146477
Iteration 5/25 | Loss: 0.00145796
Iteration 6/25 | Loss: 0.00145662
Iteration 7/25 | Loss: 0.00145662
Iteration 8/25 | Loss: 0.00145662
Iteration 9/25 | Loss: 0.00145662
Iteration 10/25 | Loss: 0.00145662
Iteration 11/25 | Loss: 0.00145662
Iteration 12/25 | Loss: 0.00145662
Iteration 13/25 | Loss: 0.00145662
Iteration 14/25 | Loss: 0.00145662
Iteration 15/25 | Loss: 0.00145662
Iteration 16/25 | Loss: 0.00145662
Iteration 17/25 | Loss: 0.00145662
Iteration 18/25 | Loss: 0.00145662
Iteration 19/25 | Loss: 0.00145662
Iteration 20/25 | Loss: 0.00145662
Iteration 21/25 | Loss: 0.00145662
Iteration 22/25 | Loss: 0.00145662
Iteration 23/25 | Loss: 0.00145662
Iteration 24/25 | Loss: 0.00145662
Iteration 25/25 | Loss: 0.00145662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014566222671419382, 0.0014566222671419382, 0.0014566222671419382, 0.0014566222671419382, 0.0014566222671419382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014566222671419382

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89174765
Iteration 2/25 | Loss: 0.00111318
Iteration 3/25 | Loss: 0.00111317
Iteration 4/25 | Loss: 0.00111317
Iteration 5/25 | Loss: 0.00111317
Iteration 6/25 | Loss: 0.00111317
Iteration 7/25 | Loss: 0.00111317
Iteration 8/25 | Loss: 0.00111317
Iteration 9/25 | Loss: 0.00111317
Iteration 10/25 | Loss: 0.00111317
Iteration 11/25 | Loss: 0.00111317
Iteration 12/25 | Loss: 0.00111317
Iteration 13/25 | Loss: 0.00111317
Iteration 14/25 | Loss: 0.00111317
Iteration 15/25 | Loss: 0.00111317
Iteration 16/25 | Loss: 0.00111317
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011131686624139547, 0.0011131686624139547, 0.0011131686624139547, 0.0011131686624139547, 0.0011131686624139547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011131686624139547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111317
Iteration 2/1000 | Loss: 0.00005735
Iteration 3/1000 | Loss: 0.00003827
Iteration 4/1000 | Loss: 0.00003462
Iteration 5/1000 | Loss: 0.00003267
Iteration 6/1000 | Loss: 0.00003161
Iteration 7/1000 | Loss: 0.00003054
Iteration 8/1000 | Loss: 0.00002985
Iteration 9/1000 | Loss: 0.00002940
Iteration 10/1000 | Loss: 0.00002901
Iteration 11/1000 | Loss: 0.00002868
Iteration 12/1000 | Loss: 0.00002843
Iteration 13/1000 | Loss: 0.00002817
Iteration 14/1000 | Loss: 0.00002792
Iteration 15/1000 | Loss: 0.00002769
Iteration 16/1000 | Loss: 0.00002750
Iteration 17/1000 | Loss: 0.00002732
Iteration 18/1000 | Loss: 0.00002719
Iteration 19/1000 | Loss: 0.00002711
Iteration 20/1000 | Loss: 0.00002707
Iteration 21/1000 | Loss: 0.00002706
Iteration 22/1000 | Loss: 0.00002706
Iteration 23/1000 | Loss: 0.00002705
Iteration 24/1000 | Loss: 0.00002705
Iteration 25/1000 | Loss: 0.00002704
Iteration 26/1000 | Loss: 0.00002701
Iteration 27/1000 | Loss: 0.00002700
Iteration 28/1000 | Loss: 0.00002695
Iteration 29/1000 | Loss: 0.00002688
Iteration 30/1000 | Loss: 0.00002688
Iteration 31/1000 | Loss: 0.00002685
Iteration 32/1000 | Loss: 0.00002685
Iteration 33/1000 | Loss: 0.00002683
Iteration 34/1000 | Loss: 0.00002683
Iteration 35/1000 | Loss: 0.00002682
Iteration 36/1000 | Loss: 0.00002682
Iteration 37/1000 | Loss: 0.00002682
Iteration 38/1000 | Loss: 0.00002682
Iteration 39/1000 | Loss: 0.00002681
Iteration 40/1000 | Loss: 0.00002681
Iteration 41/1000 | Loss: 0.00002681
Iteration 42/1000 | Loss: 0.00002681
Iteration 43/1000 | Loss: 0.00002681
Iteration 44/1000 | Loss: 0.00002681
Iteration 45/1000 | Loss: 0.00002681
Iteration 46/1000 | Loss: 0.00002680
Iteration 47/1000 | Loss: 0.00002680
Iteration 48/1000 | Loss: 0.00002680
Iteration 49/1000 | Loss: 0.00002679
Iteration 50/1000 | Loss: 0.00002679
Iteration 51/1000 | Loss: 0.00002679
Iteration 52/1000 | Loss: 0.00002679
Iteration 53/1000 | Loss: 0.00002679
Iteration 54/1000 | Loss: 0.00002679
Iteration 55/1000 | Loss: 0.00002679
Iteration 56/1000 | Loss: 0.00002678
Iteration 57/1000 | Loss: 0.00002678
Iteration 58/1000 | Loss: 0.00002678
Iteration 59/1000 | Loss: 0.00002678
Iteration 60/1000 | Loss: 0.00002677
Iteration 61/1000 | Loss: 0.00002677
Iteration 62/1000 | Loss: 0.00002677
Iteration 63/1000 | Loss: 0.00002677
Iteration 64/1000 | Loss: 0.00002677
Iteration 65/1000 | Loss: 0.00002677
Iteration 66/1000 | Loss: 0.00002677
Iteration 67/1000 | Loss: 0.00002677
Iteration 68/1000 | Loss: 0.00002677
Iteration 69/1000 | Loss: 0.00002676
Iteration 70/1000 | Loss: 0.00002676
Iteration 71/1000 | Loss: 0.00002676
Iteration 72/1000 | Loss: 0.00002676
Iteration 73/1000 | Loss: 0.00002676
Iteration 74/1000 | Loss: 0.00002676
Iteration 75/1000 | Loss: 0.00002676
Iteration 76/1000 | Loss: 0.00002675
Iteration 77/1000 | Loss: 0.00002675
Iteration 78/1000 | Loss: 0.00002675
Iteration 79/1000 | Loss: 0.00002675
Iteration 80/1000 | Loss: 0.00002674
Iteration 81/1000 | Loss: 0.00002674
Iteration 82/1000 | Loss: 0.00002674
Iteration 83/1000 | Loss: 0.00002674
Iteration 84/1000 | Loss: 0.00002674
Iteration 85/1000 | Loss: 0.00002674
Iteration 86/1000 | Loss: 0.00002674
Iteration 87/1000 | Loss: 0.00002674
Iteration 88/1000 | Loss: 0.00002674
Iteration 89/1000 | Loss: 0.00002673
Iteration 90/1000 | Loss: 0.00002673
Iteration 91/1000 | Loss: 0.00002673
Iteration 92/1000 | Loss: 0.00002673
Iteration 93/1000 | Loss: 0.00002673
Iteration 94/1000 | Loss: 0.00002673
Iteration 95/1000 | Loss: 0.00002673
Iteration 96/1000 | Loss: 0.00002673
Iteration 97/1000 | Loss: 0.00002672
Iteration 98/1000 | Loss: 0.00002672
Iteration 99/1000 | Loss: 0.00002672
Iteration 100/1000 | Loss: 0.00002672
Iteration 101/1000 | Loss: 0.00002672
Iteration 102/1000 | Loss: 0.00002672
Iteration 103/1000 | Loss: 0.00002672
Iteration 104/1000 | Loss: 0.00002672
Iteration 105/1000 | Loss: 0.00002672
Iteration 106/1000 | Loss: 0.00002672
Iteration 107/1000 | Loss: 0.00002672
Iteration 108/1000 | Loss: 0.00002672
Iteration 109/1000 | Loss: 0.00002671
Iteration 110/1000 | Loss: 0.00002671
Iteration 111/1000 | Loss: 0.00002671
Iteration 112/1000 | Loss: 0.00002671
Iteration 113/1000 | Loss: 0.00002671
Iteration 114/1000 | Loss: 0.00002671
Iteration 115/1000 | Loss: 0.00002671
Iteration 116/1000 | Loss: 0.00002670
Iteration 117/1000 | Loss: 0.00002670
Iteration 118/1000 | Loss: 0.00002670
Iteration 119/1000 | Loss: 0.00002670
Iteration 120/1000 | Loss: 0.00002670
Iteration 121/1000 | Loss: 0.00002670
Iteration 122/1000 | Loss: 0.00002669
Iteration 123/1000 | Loss: 0.00002669
Iteration 124/1000 | Loss: 0.00002669
Iteration 125/1000 | Loss: 0.00002669
Iteration 126/1000 | Loss: 0.00002669
Iteration 127/1000 | Loss: 0.00002669
Iteration 128/1000 | Loss: 0.00002669
Iteration 129/1000 | Loss: 0.00002669
Iteration 130/1000 | Loss: 0.00002669
Iteration 131/1000 | Loss: 0.00002669
Iteration 132/1000 | Loss: 0.00002669
Iteration 133/1000 | Loss: 0.00002668
Iteration 134/1000 | Loss: 0.00002668
Iteration 135/1000 | Loss: 0.00002668
Iteration 136/1000 | Loss: 0.00002668
Iteration 137/1000 | Loss: 0.00002668
Iteration 138/1000 | Loss: 0.00002668
Iteration 139/1000 | Loss: 0.00002668
Iteration 140/1000 | Loss: 0.00002668
Iteration 141/1000 | Loss: 0.00002668
Iteration 142/1000 | Loss: 0.00002668
Iteration 143/1000 | Loss: 0.00002668
Iteration 144/1000 | Loss: 0.00002668
Iteration 145/1000 | Loss: 0.00002667
Iteration 146/1000 | Loss: 0.00002667
Iteration 147/1000 | Loss: 0.00002667
Iteration 148/1000 | Loss: 0.00002667
Iteration 149/1000 | Loss: 0.00002667
Iteration 150/1000 | Loss: 0.00002667
Iteration 151/1000 | Loss: 0.00002667
Iteration 152/1000 | Loss: 0.00002667
Iteration 153/1000 | Loss: 0.00002667
Iteration 154/1000 | Loss: 0.00002667
Iteration 155/1000 | Loss: 0.00002667
Iteration 156/1000 | Loss: 0.00002667
Iteration 157/1000 | Loss: 0.00002666
Iteration 158/1000 | Loss: 0.00002666
Iteration 159/1000 | Loss: 0.00002666
Iteration 160/1000 | Loss: 0.00002666
Iteration 161/1000 | Loss: 0.00002666
Iteration 162/1000 | Loss: 0.00002666
Iteration 163/1000 | Loss: 0.00002666
Iteration 164/1000 | Loss: 0.00002666
Iteration 165/1000 | Loss: 0.00002666
Iteration 166/1000 | Loss: 0.00002666
Iteration 167/1000 | Loss: 0.00002666
Iteration 168/1000 | Loss: 0.00002666
Iteration 169/1000 | Loss: 0.00002666
Iteration 170/1000 | Loss: 0.00002666
Iteration 171/1000 | Loss: 0.00002666
Iteration 172/1000 | Loss: 0.00002665
Iteration 173/1000 | Loss: 0.00002665
Iteration 174/1000 | Loss: 0.00002665
Iteration 175/1000 | Loss: 0.00002665
Iteration 176/1000 | Loss: 0.00002665
Iteration 177/1000 | Loss: 0.00002665
Iteration 178/1000 | Loss: 0.00002665
Iteration 179/1000 | Loss: 0.00002665
Iteration 180/1000 | Loss: 0.00002665
Iteration 181/1000 | Loss: 0.00002665
Iteration 182/1000 | Loss: 0.00002665
Iteration 183/1000 | Loss: 0.00002665
Iteration 184/1000 | Loss: 0.00002665
Iteration 185/1000 | Loss: 0.00002665
Iteration 186/1000 | Loss: 0.00002665
Iteration 187/1000 | Loss: 0.00002665
Iteration 188/1000 | Loss: 0.00002665
Iteration 189/1000 | Loss: 0.00002665
Iteration 190/1000 | Loss: 0.00002665
Iteration 191/1000 | Loss: 0.00002665
Iteration 192/1000 | Loss: 0.00002665
Iteration 193/1000 | Loss: 0.00002665
Iteration 194/1000 | Loss: 0.00002665
Iteration 195/1000 | Loss: 0.00002665
Iteration 196/1000 | Loss: 0.00002665
Iteration 197/1000 | Loss: 0.00002665
Iteration 198/1000 | Loss: 0.00002665
Iteration 199/1000 | Loss: 0.00002665
Iteration 200/1000 | Loss: 0.00002665
Iteration 201/1000 | Loss: 0.00002665
Iteration 202/1000 | Loss: 0.00002665
Iteration 203/1000 | Loss: 0.00002665
Iteration 204/1000 | Loss: 0.00002665
Iteration 205/1000 | Loss: 0.00002665
Iteration 206/1000 | Loss: 0.00002665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.6652065571397543e-05, 2.6652065571397543e-05, 2.6652065571397543e-05, 2.6652065571397543e-05, 2.6652065571397543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6652065571397543e-05

Optimization complete. Final v2v error: 4.311460494995117 mm

Highest mean error: 5.229992389678955 mm for frame 111

Lowest mean error: 3.495818853378296 mm for frame 52

Saving results

Total time: 56.10831689834595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053592
Iteration 2/25 | Loss: 0.01053592
Iteration 3/25 | Loss: 0.01053591
Iteration 4/25 | Loss: 0.01053591
Iteration 5/25 | Loss: 0.00211023
Iteration 6/25 | Loss: 0.00160861
Iteration 7/25 | Loss: 0.00147016
Iteration 8/25 | Loss: 0.00142372
Iteration 9/25 | Loss: 0.00138542
Iteration 10/25 | Loss: 0.00137311
Iteration 11/25 | Loss: 0.00135694
Iteration 12/25 | Loss: 0.00134402
Iteration 13/25 | Loss: 0.00133693
Iteration 14/25 | Loss: 0.00133369
Iteration 15/25 | Loss: 0.00133262
Iteration 16/25 | Loss: 0.00133131
Iteration 17/25 | Loss: 0.00132960
Iteration 18/25 | Loss: 0.00132868
Iteration 19/25 | Loss: 0.00132854
Iteration 20/25 | Loss: 0.00132852
Iteration 21/25 | Loss: 0.00132852
Iteration 22/25 | Loss: 0.00132852
Iteration 23/25 | Loss: 0.00132852
Iteration 24/25 | Loss: 0.00132851
Iteration 25/25 | Loss: 0.00132851

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.54877281
Iteration 2/25 | Loss: 0.00095766
Iteration 3/25 | Loss: 0.00090494
Iteration 4/25 | Loss: 0.00090494
Iteration 5/25 | Loss: 0.00090494
Iteration 6/25 | Loss: 0.00090494
Iteration 7/25 | Loss: 0.00090494
Iteration 8/25 | Loss: 0.00090494
Iteration 9/25 | Loss: 0.00090494
Iteration 10/25 | Loss: 0.00090494
Iteration 11/25 | Loss: 0.00090494
Iteration 12/25 | Loss: 0.00090494
Iteration 13/25 | Loss: 0.00090494
Iteration 14/25 | Loss: 0.00090494
Iteration 15/25 | Loss: 0.00090494
Iteration 16/25 | Loss: 0.00090494
Iteration 17/25 | Loss: 0.00090494
Iteration 18/25 | Loss: 0.00090494
Iteration 19/25 | Loss: 0.00090494
Iteration 20/25 | Loss: 0.00090494
Iteration 21/25 | Loss: 0.00090494
Iteration 22/25 | Loss: 0.00090494
Iteration 23/25 | Loss: 0.00090494
Iteration 24/25 | Loss: 0.00090494
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009049352956935763, 0.0009049352956935763, 0.0009049352956935763, 0.0009049352956935763, 0.0009049352956935763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009049352956935763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090494
Iteration 2/1000 | Loss: 0.00005235
Iteration 3/1000 | Loss: 0.00002357
Iteration 4/1000 | Loss: 0.00008588
Iteration 5/1000 | Loss: 0.00002150
Iteration 6/1000 | Loss: 0.00002773
Iteration 7/1000 | Loss: 0.00008176
Iteration 8/1000 | Loss: 0.00017984
Iteration 9/1000 | Loss: 0.00005482
Iteration 10/1000 | Loss: 0.00007218
Iteration 11/1000 | Loss: 0.00002444
Iteration 12/1000 | Loss: 0.00009057
Iteration 13/1000 | Loss: 0.00001948
Iteration 14/1000 | Loss: 0.00005176
Iteration 15/1000 | Loss: 0.00001956
Iteration 16/1000 | Loss: 0.00004590
Iteration 17/1000 | Loss: 0.00001881
Iteration 18/1000 | Loss: 0.00001863
Iteration 19/1000 | Loss: 0.00001851
Iteration 20/1000 | Loss: 0.00002933
Iteration 21/1000 | Loss: 0.00001800
Iteration 22/1000 | Loss: 0.00001800
Iteration 23/1000 | Loss: 0.00014788
Iteration 24/1000 | Loss: 0.00003862
Iteration 25/1000 | Loss: 0.00004096
Iteration 26/1000 | Loss: 0.00001852
Iteration 27/1000 | Loss: 0.00010789
Iteration 28/1000 | Loss: 0.00001794
Iteration 29/1000 | Loss: 0.00001766
Iteration 30/1000 | Loss: 0.00001762
Iteration 31/1000 | Loss: 0.00001762
Iteration 32/1000 | Loss: 0.00001761
Iteration 33/1000 | Loss: 0.00001758
Iteration 34/1000 | Loss: 0.00001758
Iteration 35/1000 | Loss: 0.00001757
Iteration 36/1000 | Loss: 0.00001756
Iteration 37/1000 | Loss: 0.00001756
Iteration 38/1000 | Loss: 0.00001756
Iteration 39/1000 | Loss: 0.00001755
Iteration 40/1000 | Loss: 0.00001755
Iteration 41/1000 | Loss: 0.00001754
Iteration 42/1000 | Loss: 0.00001754
Iteration 43/1000 | Loss: 0.00001754
Iteration 44/1000 | Loss: 0.00001753
Iteration 45/1000 | Loss: 0.00001753
Iteration 46/1000 | Loss: 0.00001752
Iteration 47/1000 | Loss: 0.00002638
Iteration 48/1000 | Loss: 0.00001747
Iteration 49/1000 | Loss: 0.00001747
Iteration 50/1000 | Loss: 0.00001746
Iteration 51/1000 | Loss: 0.00001746
Iteration 52/1000 | Loss: 0.00001746
Iteration 53/1000 | Loss: 0.00001745
Iteration 54/1000 | Loss: 0.00001745
Iteration 55/1000 | Loss: 0.00001740
Iteration 56/1000 | Loss: 0.00001738
Iteration 57/1000 | Loss: 0.00001738
Iteration 58/1000 | Loss: 0.00001737
Iteration 59/1000 | Loss: 0.00001737
Iteration 60/1000 | Loss: 0.00001737
Iteration 61/1000 | Loss: 0.00001737
Iteration 62/1000 | Loss: 0.00001737
Iteration 63/1000 | Loss: 0.00001736
Iteration 64/1000 | Loss: 0.00001736
Iteration 65/1000 | Loss: 0.00001736
Iteration 66/1000 | Loss: 0.00001735
Iteration 67/1000 | Loss: 0.00001734
Iteration 68/1000 | Loss: 0.00001734
Iteration 69/1000 | Loss: 0.00001733
Iteration 70/1000 | Loss: 0.00001733
Iteration 71/1000 | Loss: 0.00001733
Iteration 72/1000 | Loss: 0.00001733
Iteration 73/1000 | Loss: 0.00001732
Iteration 74/1000 | Loss: 0.00001732
Iteration 75/1000 | Loss: 0.00001730
Iteration 76/1000 | Loss: 0.00001730
Iteration 77/1000 | Loss: 0.00001730
Iteration 78/1000 | Loss: 0.00001730
Iteration 79/1000 | Loss: 0.00001728
Iteration 80/1000 | Loss: 0.00001727
Iteration 81/1000 | Loss: 0.00008628
Iteration 82/1000 | Loss: 0.00003109
Iteration 83/1000 | Loss: 0.00002272
Iteration 84/1000 | Loss: 0.00001717
Iteration 85/1000 | Loss: 0.00001716
Iteration 86/1000 | Loss: 0.00001715
Iteration 87/1000 | Loss: 0.00001715
Iteration 88/1000 | Loss: 0.00001715
Iteration 89/1000 | Loss: 0.00001715
Iteration 90/1000 | Loss: 0.00001715
Iteration 91/1000 | Loss: 0.00001715
Iteration 92/1000 | Loss: 0.00001715
Iteration 93/1000 | Loss: 0.00001715
Iteration 94/1000 | Loss: 0.00001714
Iteration 95/1000 | Loss: 0.00001714
Iteration 96/1000 | Loss: 0.00001714
Iteration 97/1000 | Loss: 0.00001714
Iteration 98/1000 | Loss: 0.00001714
Iteration 99/1000 | Loss: 0.00001714
Iteration 100/1000 | Loss: 0.00001714
Iteration 101/1000 | Loss: 0.00001714
Iteration 102/1000 | Loss: 0.00001713
Iteration 103/1000 | Loss: 0.00001713
Iteration 104/1000 | Loss: 0.00001713
Iteration 105/1000 | Loss: 0.00001712
Iteration 106/1000 | Loss: 0.00001712
Iteration 107/1000 | Loss: 0.00001712
Iteration 108/1000 | Loss: 0.00001712
Iteration 109/1000 | Loss: 0.00001712
Iteration 110/1000 | Loss: 0.00001712
Iteration 111/1000 | Loss: 0.00001712
Iteration 112/1000 | Loss: 0.00001712
Iteration 113/1000 | Loss: 0.00001712
Iteration 114/1000 | Loss: 0.00001712
Iteration 115/1000 | Loss: 0.00001712
Iteration 116/1000 | Loss: 0.00001712
Iteration 117/1000 | Loss: 0.00001711
Iteration 118/1000 | Loss: 0.00001711
Iteration 119/1000 | Loss: 0.00001711
Iteration 120/1000 | Loss: 0.00001711
Iteration 121/1000 | Loss: 0.00001711
Iteration 122/1000 | Loss: 0.00001711
Iteration 123/1000 | Loss: 0.00001711
Iteration 124/1000 | Loss: 0.00001711
Iteration 125/1000 | Loss: 0.00001711
Iteration 126/1000 | Loss: 0.00001711
Iteration 127/1000 | Loss: 0.00001711
Iteration 128/1000 | Loss: 0.00001711
Iteration 129/1000 | Loss: 0.00001711
Iteration 130/1000 | Loss: 0.00001711
Iteration 131/1000 | Loss: 0.00001711
Iteration 132/1000 | Loss: 0.00001711
Iteration 133/1000 | Loss: 0.00001711
Iteration 134/1000 | Loss: 0.00001711
Iteration 135/1000 | Loss: 0.00001711
Iteration 136/1000 | Loss: 0.00001711
Iteration 137/1000 | Loss: 0.00001711
Iteration 138/1000 | Loss: 0.00001711
Iteration 139/1000 | Loss: 0.00001711
Iteration 140/1000 | Loss: 0.00001711
Iteration 141/1000 | Loss: 0.00001711
Iteration 142/1000 | Loss: 0.00001711
Iteration 143/1000 | Loss: 0.00001711
Iteration 144/1000 | Loss: 0.00001711
Iteration 145/1000 | Loss: 0.00001711
Iteration 146/1000 | Loss: 0.00001711
Iteration 147/1000 | Loss: 0.00001711
Iteration 148/1000 | Loss: 0.00001711
Iteration 149/1000 | Loss: 0.00001711
Iteration 150/1000 | Loss: 0.00001711
Iteration 151/1000 | Loss: 0.00001711
Iteration 152/1000 | Loss: 0.00001711
Iteration 153/1000 | Loss: 0.00001711
Iteration 154/1000 | Loss: 0.00001711
Iteration 155/1000 | Loss: 0.00001711
Iteration 156/1000 | Loss: 0.00001711
Iteration 157/1000 | Loss: 0.00001711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.7107136955019087e-05, 1.7107136955019087e-05, 1.7107136955019087e-05, 1.7107136955019087e-05, 1.7107136955019087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7107136955019087e-05

Optimization complete. Final v2v error: 3.4862208366394043 mm

Highest mean error: 8.676490783691406 mm for frame 155

Lowest mean error: 3.1581270694732666 mm for frame 242

Saving results

Total time: 97.13068795204163
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00927357
Iteration 2/25 | Loss: 0.00465339
Iteration 3/25 | Loss: 0.00366602
Iteration 4/25 | Loss: 0.00302926
Iteration 5/25 | Loss: 0.00263921
Iteration 6/25 | Loss: 0.00227421
Iteration 7/25 | Loss: 0.00212668
Iteration 8/25 | Loss: 0.00194606
Iteration 9/25 | Loss: 0.00187667
Iteration 10/25 | Loss: 0.00188645
Iteration 11/25 | Loss: 0.00186002
Iteration 12/25 | Loss: 0.00176047
Iteration 13/25 | Loss: 0.00168024
Iteration 14/25 | Loss: 0.00163884
Iteration 15/25 | Loss: 0.00161938
Iteration 16/25 | Loss: 0.00164199
Iteration 17/25 | Loss: 0.00161352
Iteration 18/25 | Loss: 0.00159355
Iteration 19/25 | Loss: 0.00157756
Iteration 20/25 | Loss: 0.00156941
Iteration 21/25 | Loss: 0.00156321
Iteration 22/25 | Loss: 0.00155809
Iteration 23/25 | Loss: 0.00156346
Iteration 24/25 | Loss: 0.00156144
Iteration 25/25 | Loss: 0.00155744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37626565
Iteration 2/25 | Loss: 0.00137344
Iteration 3/25 | Loss: 0.00137344
Iteration 4/25 | Loss: 0.00137344
Iteration 5/25 | Loss: 0.00137344
Iteration 6/25 | Loss: 0.00137344
Iteration 7/25 | Loss: 0.00137344
Iteration 8/25 | Loss: 0.00137344
Iteration 9/25 | Loss: 0.00137344
Iteration 10/25 | Loss: 0.00137344
Iteration 11/25 | Loss: 0.00137344
Iteration 12/25 | Loss: 0.00137344
Iteration 13/25 | Loss: 0.00137344
Iteration 14/25 | Loss: 0.00137344
Iteration 15/25 | Loss: 0.00137344
Iteration 16/25 | Loss: 0.00137344
Iteration 17/25 | Loss: 0.00137344
Iteration 18/25 | Loss: 0.00137344
Iteration 19/25 | Loss: 0.00137344
Iteration 20/25 | Loss: 0.00137344
Iteration 21/25 | Loss: 0.00137344
Iteration 22/25 | Loss: 0.00137344
Iteration 23/25 | Loss: 0.00137344
Iteration 24/25 | Loss: 0.00137344
Iteration 25/25 | Loss: 0.00137344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137344
Iteration 2/1000 | Loss: 0.00030393
Iteration 3/1000 | Loss: 0.00028678
Iteration 4/1000 | Loss: 0.00078559
Iteration 5/1000 | Loss: 0.00033865
Iteration 6/1000 | Loss: 0.00016885
Iteration 7/1000 | Loss: 0.00024421
Iteration 8/1000 | Loss: 0.00017528
Iteration 9/1000 | Loss: 0.00028303
Iteration 10/1000 | Loss: 0.00397638
Iteration 11/1000 | Loss: 0.00080096
Iteration 12/1000 | Loss: 0.00065779
Iteration 13/1000 | Loss: 0.00048800
Iteration 14/1000 | Loss: 0.00045818
Iteration 15/1000 | Loss: 0.00033743
Iteration 16/1000 | Loss: 0.00008570
Iteration 17/1000 | Loss: 0.00010232
Iteration 18/1000 | Loss: 0.00025911
Iteration 19/1000 | Loss: 0.00018507
Iteration 20/1000 | Loss: 0.00009495
Iteration 21/1000 | Loss: 0.00006754
Iteration 22/1000 | Loss: 0.00006946
Iteration 23/1000 | Loss: 0.00024276
Iteration 24/1000 | Loss: 0.00015299
Iteration 25/1000 | Loss: 0.00004768
Iteration 26/1000 | Loss: 0.00005946
Iteration 27/1000 | Loss: 0.00004262
Iteration 28/1000 | Loss: 0.00003661
Iteration 29/1000 | Loss: 0.00003466
Iteration 30/1000 | Loss: 0.00003312
Iteration 31/1000 | Loss: 0.00020448
Iteration 32/1000 | Loss: 0.00015925
Iteration 33/1000 | Loss: 0.00021109
Iteration 34/1000 | Loss: 0.00020135
Iteration 35/1000 | Loss: 0.00004627
Iteration 36/1000 | Loss: 0.00003883
Iteration 37/1000 | Loss: 0.00024336
Iteration 38/1000 | Loss: 0.00005770
Iteration 39/1000 | Loss: 0.00045436
Iteration 40/1000 | Loss: 0.00004172
Iteration 41/1000 | Loss: 0.00003640
Iteration 42/1000 | Loss: 0.00022357
Iteration 43/1000 | Loss: 0.00003500
Iteration 44/1000 | Loss: 0.00003183
Iteration 45/1000 | Loss: 0.00005889
Iteration 46/1000 | Loss: 0.00004182
Iteration 47/1000 | Loss: 0.00008460
Iteration 48/1000 | Loss: 0.00003708
Iteration 49/1000 | Loss: 0.00003264
Iteration 50/1000 | Loss: 0.00003915
Iteration 51/1000 | Loss: 0.00004164
Iteration 52/1000 | Loss: 0.00003986
Iteration 53/1000 | Loss: 0.00003991
Iteration 54/1000 | Loss: 0.00004126
Iteration 55/1000 | Loss: 0.00004491
Iteration 56/1000 | Loss: 0.00004004
Iteration 57/1000 | Loss: 0.00003072
Iteration 58/1000 | Loss: 0.00003807
Iteration 59/1000 | Loss: 0.00002993
Iteration 60/1000 | Loss: 0.00003419
Iteration 61/1000 | Loss: 0.00002946
Iteration 62/1000 | Loss: 0.00003448
Iteration 63/1000 | Loss: 0.00003827
Iteration 64/1000 | Loss: 0.00003729
Iteration 65/1000 | Loss: 0.00003569
Iteration 66/1000 | Loss: 0.00003948
Iteration 67/1000 | Loss: 0.00004006
Iteration 68/1000 | Loss: 0.00002924
Iteration 69/1000 | Loss: 0.00003585
Iteration 70/1000 | Loss: 0.00003649
Iteration 71/1000 | Loss: 0.00003423
Iteration 72/1000 | Loss: 0.00002890
Iteration 73/1000 | Loss: 0.00003651
Iteration 74/1000 | Loss: 0.00003023
Iteration 75/1000 | Loss: 0.00003220
Iteration 76/1000 | Loss: 0.00003555
Iteration 77/1000 | Loss: 0.00003456
Iteration 78/1000 | Loss: 0.00003528
Iteration 79/1000 | Loss: 0.00003557
Iteration 80/1000 | Loss: 0.00002918
Iteration 81/1000 | Loss: 0.00003021
Iteration 82/1000 | Loss: 0.00003497
Iteration 83/1000 | Loss: 0.00003599
Iteration 84/1000 | Loss: 0.00003285
Iteration 85/1000 | Loss: 0.00003495
Iteration 86/1000 | Loss: 0.00003004
Iteration 87/1000 | Loss: 0.00003137
Iteration 88/1000 | Loss: 0.00003287
Iteration 89/1000 | Loss: 0.00003236
Iteration 90/1000 | Loss: 0.00003601
Iteration 91/1000 | Loss: 0.00003064
Iteration 92/1000 | Loss: 0.00002939
Iteration 93/1000 | Loss: 0.00003481
Iteration 94/1000 | Loss: 0.00003955
Iteration 95/1000 | Loss: 0.00002809
Iteration 96/1000 | Loss: 0.00002706
Iteration 97/1000 | Loss: 0.00002637
Iteration 98/1000 | Loss: 0.00002628
Iteration 99/1000 | Loss: 0.00002627
Iteration 100/1000 | Loss: 0.00002620
Iteration 101/1000 | Loss: 0.00002599
Iteration 102/1000 | Loss: 0.00002597
Iteration 103/1000 | Loss: 0.00002596
Iteration 104/1000 | Loss: 0.00002595
Iteration 105/1000 | Loss: 0.00002592
Iteration 106/1000 | Loss: 0.00002591
Iteration 107/1000 | Loss: 0.00002582
Iteration 108/1000 | Loss: 0.00002562
Iteration 109/1000 | Loss: 0.00002562
Iteration 110/1000 | Loss: 0.00002547
Iteration 111/1000 | Loss: 0.00002545
Iteration 112/1000 | Loss: 0.00002540
Iteration 113/1000 | Loss: 0.00002537
Iteration 114/1000 | Loss: 0.00002537
Iteration 115/1000 | Loss: 0.00002537
Iteration 116/1000 | Loss: 0.00002537
Iteration 117/1000 | Loss: 0.00002536
Iteration 118/1000 | Loss: 0.00002535
Iteration 119/1000 | Loss: 0.00002535
Iteration 120/1000 | Loss: 0.00002532
Iteration 121/1000 | Loss: 0.00002530
Iteration 122/1000 | Loss: 0.00002530
Iteration 123/1000 | Loss: 0.00002529
Iteration 124/1000 | Loss: 0.00002529
Iteration 125/1000 | Loss: 0.00002529
Iteration 126/1000 | Loss: 0.00002528
Iteration 127/1000 | Loss: 0.00002528
Iteration 128/1000 | Loss: 0.00002527
Iteration 129/1000 | Loss: 0.00002526
Iteration 130/1000 | Loss: 0.00002526
Iteration 131/1000 | Loss: 0.00002525
Iteration 132/1000 | Loss: 0.00002525
Iteration 133/1000 | Loss: 0.00002525
Iteration 134/1000 | Loss: 0.00002525
Iteration 135/1000 | Loss: 0.00002524
Iteration 136/1000 | Loss: 0.00002524
Iteration 137/1000 | Loss: 0.00002524
Iteration 138/1000 | Loss: 0.00002523
Iteration 139/1000 | Loss: 0.00002523
Iteration 140/1000 | Loss: 0.00002523
Iteration 141/1000 | Loss: 0.00002523
Iteration 142/1000 | Loss: 0.00002522
Iteration 143/1000 | Loss: 0.00002522
Iteration 144/1000 | Loss: 0.00002521
Iteration 145/1000 | Loss: 0.00002521
Iteration 146/1000 | Loss: 0.00002520
Iteration 147/1000 | Loss: 0.00002520
Iteration 148/1000 | Loss: 0.00002519
Iteration 149/1000 | Loss: 0.00002517
Iteration 150/1000 | Loss: 0.00002516
Iteration 151/1000 | Loss: 0.00002511
Iteration 152/1000 | Loss: 0.00002507
Iteration 153/1000 | Loss: 0.00002507
Iteration 154/1000 | Loss: 0.00002506
Iteration 155/1000 | Loss: 0.00002506
Iteration 156/1000 | Loss: 0.00002506
Iteration 157/1000 | Loss: 0.00002506
Iteration 158/1000 | Loss: 0.00002505
Iteration 159/1000 | Loss: 0.00002505
Iteration 160/1000 | Loss: 0.00002505
Iteration 161/1000 | Loss: 0.00002505
Iteration 162/1000 | Loss: 0.00002505
Iteration 163/1000 | Loss: 0.00002504
Iteration 164/1000 | Loss: 0.00002504
Iteration 165/1000 | Loss: 0.00002504
Iteration 166/1000 | Loss: 0.00002502
Iteration 167/1000 | Loss: 0.00002501
Iteration 168/1000 | Loss: 0.00002501
Iteration 169/1000 | Loss: 0.00002501
Iteration 170/1000 | Loss: 0.00002501
Iteration 171/1000 | Loss: 0.00002501
Iteration 172/1000 | Loss: 0.00002501
Iteration 173/1000 | Loss: 0.00002501
Iteration 174/1000 | Loss: 0.00002500
Iteration 175/1000 | Loss: 0.00002500
Iteration 176/1000 | Loss: 0.00002500
Iteration 177/1000 | Loss: 0.00002500
Iteration 178/1000 | Loss: 0.00002500
Iteration 179/1000 | Loss: 0.00002500
Iteration 180/1000 | Loss: 0.00002500
Iteration 181/1000 | Loss: 0.00002500
Iteration 182/1000 | Loss: 0.00002498
Iteration 183/1000 | Loss: 0.00002498
Iteration 184/1000 | Loss: 0.00002498
Iteration 185/1000 | Loss: 0.00002498
Iteration 186/1000 | Loss: 0.00002497
Iteration 187/1000 | Loss: 0.00002497
Iteration 188/1000 | Loss: 0.00002496
Iteration 189/1000 | Loss: 0.00002496
Iteration 190/1000 | Loss: 0.00002495
Iteration 191/1000 | Loss: 0.00002494
Iteration 192/1000 | Loss: 0.00002494
Iteration 193/1000 | Loss: 0.00002494
Iteration 194/1000 | Loss: 0.00002494
Iteration 195/1000 | Loss: 0.00002494
Iteration 196/1000 | Loss: 0.00002494
Iteration 197/1000 | Loss: 0.00002494
Iteration 198/1000 | Loss: 0.00002494
Iteration 199/1000 | Loss: 0.00002494
Iteration 200/1000 | Loss: 0.00002494
Iteration 201/1000 | Loss: 0.00002494
Iteration 202/1000 | Loss: 0.00002494
Iteration 203/1000 | Loss: 0.00002494
Iteration 204/1000 | Loss: 0.00002493
Iteration 205/1000 | Loss: 0.00002493
Iteration 206/1000 | Loss: 0.00002493
Iteration 207/1000 | Loss: 0.00002492
Iteration 208/1000 | Loss: 0.00002492
Iteration 209/1000 | Loss: 0.00002492
Iteration 210/1000 | Loss: 0.00002492
Iteration 211/1000 | Loss: 0.00002491
Iteration 212/1000 | Loss: 0.00002491
Iteration 213/1000 | Loss: 0.00002490
Iteration 214/1000 | Loss: 0.00002490
Iteration 215/1000 | Loss: 0.00002490
Iteration 216/1000 | Loss: 0.00002490
Iteration 217/1000 | Loss: 0.00002489
Iteration 218/1000 | Loss: 0.00002489
Iteration 219/1000 | Loss: 0.00002489
Iteration 220/1000 | Loss: 0.00002489
Iteration 221/1000 | Loss: 0.00002489
Iteration 222/1000 | Loss: 0.00002489
Iteration 223/1000 | Loss: 0.00002489
Iteration 224/1000 | Loss: 0.00002488
Iteration 225/1000 | Loss: 0.00002488
Iteration 226/1000 | Loss: 0.00002488
Iteration 227/1000 | Loss: 0.00002488
Iteration 228/1000 | Loss: 0.00002488
Iteration 229/1000 | Loss: 0.00002488
Iteration 230/1000 | Loss: 0.00002487
Iteration 231/1000 | Loss: 0.00002487
Iteration 232/1000 | Loss: 0.00002487
Iteration 233/1000 | Loss: 0.00002487
Iteration 234/1000 | Loss: 0.00002485
Iteration 235/1000 | Loss: 0.00002485
Iteration 236/1000 | Loss: 0.00002485
Iteration 237/1000 | Loss: 0.00002485
Iteration 238/1000 | Loss: 0.00002485
Iteration 239/1000 | Loss: 0.00002484
Iteration 240/1000 | Loss: 0.00002484
Iteration 241/1000 | Loss: 0.00002484
Iteration 242/1000 | Loss: 0.00002484
Iteration 243/1000 | Loss: 0.00002484
Iteration 244/1000 | Loss: 0.00002483
Iteration 245/1000 | Loss: 0.00002483
Iteration 246/1000 | Loss: 0.00002483
Iteration 247/1000 | Loss: 0.00002482
Iteration 248/1000 | Loss: 0.00002482
Iteration 249/1000 | Loss: 0.00002482
Iteration 250/1000 | Loss: 0.00002482
Iteration 251/1000 | Loss: 0.00002482
Iteration 252/1000 | Loss: 0.00002482
Iteration 253/1000 | Loss: 0.00002482
Iteration 254/1000 | Loss: 0.00002482
Iteration 255/1000 | Loss: 0.00002481
Iteration 256/1000 | Loss: 0.00002481
Iteration 257/1000 | Loss: 0.00002481
Iteration 258/1000 | Loss: 0.00002481
Iteration 259/1000 | Loss: 0.00002481
Iteration 260/1000 | Loss: 0.00002481
Iteration 261/1000 | Loss: 0.00002480
Iteration 262/1000 | Loss: 0.00002480
Iteration 263/1000 | Loss: 0.00002480
Iteration 264/1000 | Loss: 0.00002480
Iteration 265/1000 | Loss: 0.00002480
Iteration 266/1000 | Loss: 0.00002480
Iteration 267/1000 | Loss: 0.00002480
Iteration 268/1000 | Loss: 0.00002479
Iteration 269/1000 | Loss: 0.00002479
Iteration 270/1000 | Loss: 0.00002479
Iteration 271/1000 | Loss: 0.00002478
Iteration 272/1000 | Loss: 0.00002478
Iteration 273/1000 | Loss: 0.00002478
Iteration 274/1000 | Loss: 0.00002478
Iteration 275/1000 | Loss: 0.00002478
Iteration 276/1000 | Loss: 0.00002478
Iteration 277/1000 | Loss: 0.00002478
Iteration 278/1000 | Loss: 0.00002478
Iteration 279/1000 | Loss: 0.00002478
Iteration 280/1000 | Loss: 0.00002478
Iteration 281/1000 | Loss: 0.00002478
Iteration 282/1000 | Loss: 0.00002478
Iteration 283/1000 | Loss: 0.00002478
Iteration 284/1000 | Loss: 0.00002478
Iteration 285/1000 | Loss: 0.00002478
Iteration 286/1000 | Loss: 0.00002478
Iteration 287/1000 | Loss: 0.00002478
Iteration 288/1000 | Loss: 0.00002478
Iteration 289/1000 | Loss: 0.00002478
Iteration 290/1000 | Loss: 0.00002478
Iteration 291/1000 | Loss: 0.00002478
Iteration 292/1000 | Loss: 0.00002478
Iteration 293/1000 | Loss: 0.00002478
Iteration 294/1000 | Loss: 0.00002478
Iteration 295/1000 | Loss: 0.00002478
Iteration 296/1000 | Loss: 0.00002478
Iteration 297/1000 | Loss: 0.00002478
Iteration 298/1000 | Loss: 0.00002478
Iteration 299/1000 | Loss: 0.00002478
Iteration 300/1000 | Loss: 0.00002478
Iteration 301/1000 | Loss: 0.00002478
Iteration 302/1000 | Loss: 0.00002478
Iteration 303/1000 | Loss: 0.00002478
Iteration 304/1000 | Loss: 0.00002478
Iteration 305/1000 | Loss: 0.00002478
Iteration 306/1000 | Loss: 0.00002478
Iteration 307/1000 | Loss: 0.00002478
Iteration 308/1000 | Loss: 0.00002478
Iteration 309/1000 | Loss: 0.00002478
Iteration 310/1000 | Loss: 0.00002478
Iteration 311/1000 | Loss: 0.00002478
Iteration 312/1000 | Loss: 0.00002478
Iteration 313/1000 | Loss: 0.00002478
Iteration 314/1000 | Loss: 0.00002478
Iteration 315/1000 | Loss: 0.00002478
Iteration 316/1000 | Loss: 0.00002478
Iteration 317/1000 | Loss: 0.00002478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 317. Stopping optimization.
Last 5 losses: [2.478432907082606e-05, 2.478432907082606e-05, 2.478432907082606e-05, 2.478432907082606e-05, 2.478432907082606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.478432907082606e-05

Optimization complete. Final v2v error: 4.186611175537109 mm

Highest mean error: 6.345146656036377 mm for frame 164

Lowest mean error: 3.6126339435577393 mm for frame 19

Saving results

Total time: 218.4325795173645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475332
Iteration 2/25 | Loss: 0.00152758
Iteration 3/25 | Loss: 0.00133444
Iteration 4/25 | Loss: 0.00131555
Iteration 5/25 | Loss: 0.00131283
Iteration 6/25 | Loss: 0.00131224
Iteration 7/25 | Loss: 0.00131224
Iteration 8/25 | Loss: 0.00131224
Iteration 9/25 | Loss: 0.00131224
Iteration 10/25 | Loss: 0.00131224
Iteration 11/25 | Loss: 0.00131224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013122442178428173, 0.0013122442178428173, 0.0013122442178428173, 0.0013122442178428173, 0.0013122442178428173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013122442178428173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40590227
Iteration 2/25 | Loss: 0.00083110
Iteration 3/25 | Loss: 0.00083110
Iteration 4/25 | Loss: 0.00083109
Iteration 5/25 | Loss: 0.00083109
Iteration 6/25 | Loss: 0.00083109
Iteration 7/25 | Loss: 0.00083109
Iteration 8/25 | Loss: 0.00083109
Iteration 9/25 | Loss: 0.00083109
Iteration 10/25 | Loss: 0.00083109
Iteration 11/25 | Loss: 0.00083109
Iteration 12/25 | Loss: 0.00083109
Iteration 13/25 | Loss: 0.00083109
Iteration 14/25 | Loss: 0.00083109
Iteration 15/25 | Loss: 0.00083109
Iteration 16/25 | Loss: 0.00083109
Iteration 17/25 | Loss: 0.00083109
Iteration 18/25 | Loss: 0.00083109
Iteration 19/25 | Loss: 0.00083109
Iteration 20/25 | Loss: 0.00083109
Iteration 21/25 | Loss: 0.00083109
Iteration 22/25 | Loss: 0.00083109
Iteration 23/25 | Loss: 0.00083109
Iteration 24/25 | Loss: 0.00083109
Iteration 25/25 | Loss: 0.00083109

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083109
Iteration 2/1000 | Loss: 0.00003520
Iteration 3/1000 | Loss: 0.00002107
Iteration 4/1000 | Loss: 0.00001855
Iteration 5/1000 | Loss: 0.00001736
Iteration 6/1000 | Loss: 0.00001670
Iteration 7/1000 | Loss: 0.00001601
Iteration 8/1000 | Loss: 0.00001552
Iteration 9/1000 | Loss: 0.00001526
Iteration 10/1000 | Loss: 0.00001493
Iteration 11/1000 | Loss: 0.00001477
Iteration 12/1000 | Loss: 0.00001471
Iteration 13/1000 | Loss: 0.00001465
Iteration 14/1000 | Loss: 0.00001453
Iteration 15/1000 | Loss: 0.00001452
Iteration 16/1000 | Loss: 0.00001448
Iteration 17/1000 | Loss: 0.00001447
Iteration 18/1000 | Loss: 0.00001447
Iteration 19/1000 | Loss: 0.00001444
Iteration 20/1000 | Loss: 0.00001439
Iteration 21/1000 | Loss: 0.00001438
Iteration 22/1000 | Loss: 0.00001436
Iteration 23/1000 | Loss: 0.00001434
Iteration 24/1000 | Loss: 0.00001434
Iteration 25/1000 | Loss: 0.00001434
Iteration 26/1000 | Loss: 0.00001434
Iteration 27/1000 | Loss: 0.00001433
Iteration 28/1000 | Loss: 0.00001433
Iteration 29/1000 | Loss: 0.00001432
Iteration 30/1000 | Loss: 0.00001431
Iteration 31/1000 | Loss: 0.00001431
Iteration 32/1000 | Loss: 0.00001430
Iteration 33/1000 | Loss: 0.00001428
Iteration 34/1000 | Loss: 0.00001426
Iteration 35/1000 | Loss: 0.00001425
Iteration 36/1000 | Loss: 0.00001425
Iteration 37/1000 | Loss: 0.00001425
Iteration 38/1000 | Loss: 0.00001425
Iteration 39/1000 | Loss: 0.00001425
Iteration 40/1000 | Loss: 0.00001425
Iteration 41/1000 | Loss: 0.00001424
Iteration 42/1000 | Loss: 0.00001424
Iteration 43/1000 | Loss: 0.00001424
Iteration 44/1000 | Loss: 0.00001423
Iteration 45/1000 | Loss: 0.00001423
Iteration 46/1000 | Loss: 0.00001422
Iteration 47/1000 | Loss: 0.00001422
Iteration 48/1000 | Loss: 0.00001421
Iteration 49/1000 | Loss: 0.00001420
Iteration 50/1000 | Loss: 0.00001417
Iteration 51/1000 | Loss: 0.00001417
Iteration 52/1000 | Loss: 0.00001417
Iteration 53/1000 | Loss: 0.00001417
Iteration 54/1000 | Loss: 0.00001417
Iteration 55/1000 | Loss: 0.00001417
Iteration 56/1000 | Loss: 0.00001417
Iteration 57/1000 | Loss: 0.00001417
Iteration 58/1000 | Loss: 0.00001417
Iteration 59/1000 | Loss: 0.00001417
Iteration 60/1000 | Loss: 0.00001416
Iteration 61/1000 | Loss: 0.00001416
Iteration 62/1000 | Loss: 0.00001416
Iteration 63/1000 | Loss: 0.00001412
Iteration 64/1000 | Loss: 0.00001412
Iteration 65/1000 | Loss: 0.00001412
Iteration 66/1000 | Loss: 0.00001412
Iteration 67/1000 | Loss: 0.00001412
Iteration 68/1000 | Loss: 0.00001411
Iteration 69/1000 | Loss: 0.00001410
Iteration 70/1000 | Loss: 0.00001410
Iteration 71/1000 | Loss: 0.00001409
Iteration 72/1000 | Loss: 0.00001409
Iteration 73/1000 | Loss: 0.00001409
Iteration 74/1000 | Loss: 0.00001408
Iteration 75/1000 | Loss: 0.00001408
Iteration 76/1000 | Loss: 0.00001408
Iteration 77/1000 | Loss: 0.00001408
Iteration 78/1000 | Loss: 0.00001407
Iteration 79/1000 | Loss: 0.00001407
Iteration 80/1000 | Loss: 0.00001406
Iteration 81/1000 | Loss: 0.00001405
Iteration 82/1000 | Loss: 0.00001405
Iteration 83/1000 | Loss: 0.00001404
Iteration 84/1000 | Loss: 0.00001404
Iteration 85/1000 | Loss: 0.00001404
Iteration 86/1000 | Loss: 0.00001403
Iteration 87/1000 | Loss: 0.00001400
Iteration 88/1000 | Loss: 0.00001400
Iteration 89/1000 | Loss: 0.00001399
Iteration 90/1000 | Loss: 0.00001399
Iteration 91/1000 | Loss: 0.00001398
Iteration 92/1000 | Loss: 0.00001398
Iteration 93/1000 | Loss: 0.00001398
Iteration 94/1000 | Loss: 0.00001398
Iteration 95/1000 | Loss: 0.00001398
Iteration 96/1000 | Loss: 0.00001398
Iteration 97/1000 | Loss: 0.00001398
Iteration 98/1000 | Loss: 0.00001398
Iteration 99/1000 | Loss: 0.00001398
Iteration 100/1000 | Loss: 0.00001398
Iteration 101/1000 | Loss: 0.00001397
Iteration 102/1000 | Loss: 0.00001397
Iteration 103/1000 | Loss: 0.00001397
Iteration 104/1000 | Loss: 0.00001397
Iteration 105/1000 | Loss: 0.00001397
Iteration 106/1000 | Loss: 0.00001397
Iteration 107/1000 | Loss: 0.00001397
Iteration 108/1000 | Loss: 0.00001397
Iteration 109/1000 | Loss: 0.00001397
Iteration 110/1000 | Loss: 0.00001397
Iteration 111/1000 | Loss: 0.00001397
Iteration 112/1000 | Loss: 0.00001396
Iteration 113/1000 | Loss: 0.00001395
Iteration 114/1000 | Loss: 0.00001395
Iteration 115/1000 | Loss: 0.00001395
Iteration 116/1000 | Loss: 0.00001395
Iteration 117/1000 | Loss: 0.00001394
Iteration 118/1000 | Loss: 0.00001394
Iteration 119/1000 | Loss: 0.00001394
Iteration 120/1000 | Loss: 0.00001393
Iteration 121/1000 | Loss: 0.00001393
Iteration 122/1000 | Loss: 0.00001393
Iteration 123/1000 | Loss: 0.00001392
Iteration 124/1000 | Loss: 0.00001392
Iteration 125/1000 | Loss: 0.00001392
Iteration 126/1000 | Loss: 0.00001392
Iteration 127/1000 | Loss: 0.00001392
Iteration 128/1000 | Loss: 0.00001391
Iteration 129/1000 | Loss: 0.00001391
Iteration 130/1000 | Loss: 0.00001391
Iteration 131/1000 | Loss: 0.00001391
Iteration 132/1000 | Loss: 0.00001391
Iteration 133/1000 | Loss: 0.00001391
Iteration 134/1000 | Loss: 0.00001391
Iteration 135/1000 | Loss: 0.00001391
Iteration 136/1000 | Loss: 0.00001390
Iteration 137/1000 | Loss: 0.00001390
Iteration 138/1000 | Loss: 0.00001390
Iteration 139/1000 | Loss: 0.00001390
Iteration 140/1000 | Loss: 0.00001389
Iteration 141/1000 | Loss: 0.00001389
Iteration 142/1000 | Loss: 0.00001389
Iteration 143/1000 | Loss: 0.00001389
Iteration 144/1000 | Loss: 0.00001388
Iteration 145/1000 | Loss: 0.00001388
Iteration 146/1000 | Loss: 0.00001388
Iteration 147/1000 | Loss: 0.00001388
Iteration 148/1000 | Loss: 0.00001388
Iteration 149/1000 | Loss: 0.00001388
Iteration 150/1000 | Loss: 0.00001388
Iteration 151/1000 | Loss: 0.00001387
Iteration 152/1000 | Loss: 0.00001387
Iteration 153/1000 | Loss: 0.00001387
Iteration 154/1000 | Loss: 0.00001387
Iteration 155/1000 | Loss: 0.00001386
Iteration 156/1000 | Loss: 0.00001386
Iteration 157/1000 | Loss: 0.00001386
Iteration 158/1000 | Loss: 0.00001386
Iteration 159/1000 | Loss: 0.00001386
Iteration 160/1000 | Loss: 0.00001386
Iteration 161/1000 | Loss: 0.00001386
Iteration 162/1000 | Loss: 0.00001386
Iteration 163/1000 | Loss: 0.00001386
Iteration 164/1000 | Loss: 0.00001386
Iteration 165/1000 | Loss: 0.00001386
Iteration 166/1000 | Loss: 0.00001386
Iteration 167/1000 | Loss: 0.00001386
Iteration 168/1000 | Loss: 0.00001386
Iteration 169/1000 | Loss: 0.00001385
Iteration 170/1000 | Loss: 0.00001385
Iteration 171/1000 | Loss: 0.00001385
Iteration 172/1000 | Loss: 0.00001385
Iteration 173/1000 | Loss: 0.00001385
Iteration 174/1000 | Loss: 0.00001385
Iteration 175/1000 | Loss: 0.00001385
Iteration 176/1000 | Loss: 0.00001385
Iteration 177/1000 | Loss: 0.00001385
Iteration 178/1000 | Loss: 0.00001384
Iteration 179/1000 | Loss: 0.00001384
Iteration 180/1000 | Loss: 0.00001384
Iteration 181/1000 | Loss: 0.00001384
Iteration 182/1000 | Loss: 0.00001383
Iteration 183/1000 | Loss: 0.00001383
Iteration 184/1000 | Loss: 0.00001383
Iteration 185/1000 | Loss: 0.00001383
Iteration 186/1000 | Loss: 0.00001383
Iteration 187/1000 | Loss: 0.00001383
Iteration 188/1000 | Loss: 0.00001383
Iteration 189/1000 | Loss: 0.00001382
Iteration 190/1000 | Loss: 0.00001382
Iteration 191/1000 | Loss: 0.00001382
Iteration 192/1000 | Loss: 0.00001382
Iteration 193/1000 | Loss: 0.00001382
Iteration 194/1000 | Loss: 0.00001381
Iteration 195/1000 | Loss: 0.00001381
Iteration 196/1000 | Loss: 0.00001381
Iteration 197/1000 | Loss: 0.00001380
Iteration 198/1000 | Loss: 0.00001380
Iteration 199/1000 | Loss: 0.00001380
Iteration 200/1000 | Loss: 0.00001380
Iteration 201/1000 | Loss: 0.00001380
Iteration 202/1000 | Loss: 0.00001380
Iteration 203/1000 | Loss: 0.00001380
Iteration 204/1000 | Loss: 0.00001380
Iteration 205/1000 | Loss: 0.00001379
Iteration 206/1000 | Loss: 0.00001379
Iteration 207/1000 | Loss: 0.00001379
Iteration 208/1000 | Loss: 0.00001379
Iteration 209/1000 | Loss: 0.00001379
Iteration 210/1000 | Loss: 0.00001379
Iteration 211/1000 | Loss: 0.00001378
Iteration 212/1000 | Loss: 0.00001378
Iteration 213/1000 | Loss: 0.00001378
Iteration 214/1000 | Loss: 0.00001378
Iteration 215/1000 | Loss: 0.00001378
Iteration 216/1000 | Loss: 0.00001378
Iteration 217/1000 | Loss: 0.00001378
Iteration 218/1000 | Loss: 0.00001378
Iteration 219/1000 | Loss: 0.00001378
Iteration 220/1000 | Loss: 0.00001378
Iteration 221/1000 | Loss: 0.00001378
Iteration 222/1000 | Loss: 0.00001378
Iteration 223/1000 | Loss: 0.00001378
Iteration 224/1000 | Loss: 0.00001378
Iteration 225/1000 | Loss: 0.00001378
Iteration 226/1000 | Loss: 0.00001378
Iteration 227/1000 | Loss: 0.00001378
Iteration 228/1000 | Loss: 0.00001378
Iteration 229/1000 | Loss: 0.00001378
Iteration 230/1000 | Loss: 0.00001378
Iteration 231/1000 | Loss: 0.00001378
Iteration 232/1000 | Loss: 0.00001378
Iteration 233/1000 | Loss: 0.00001378
Iteration 234/1000 | Loss: 0.00001378
Iteration 235/1000 | Loss: 0.00001378
Iteration 236/1000 | Loss: 0.00001378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.3775656952930149e-05, 1.3775656952930149e-05, 1.3775656952930149e-05, 1.3775656952930149e-05, 1.3775656952930149e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3775656952930149e-05

Optimization complete. Final v2v error: 3.1245875358581543 mm

Highest mean error: 3.78090500831604 mm for frame 76

Lowest mean error: 2.8514456748962402 mm for frame 38

Saving results

Total time: 45.56379199028015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736540
Iteration 2/25 | Loss: 0.00173414
Iteration 3/25 | Loss: 0.00146267
Iteration 4/25 | Loss: 0.00141327
Iteration 5/25 | Loss: 0.00140215
Iteration 6/25 | Loss: 0.00139870
Iteration 7/25 | Loss: 0.00139789
Iteration 8/25 | Loss: 0.00139765
Iteration 9/25 | Loss: 0.00139765
Iteration 10/25 | Loss: 0.00139765
Iteration 11/25 | Loss: 0.00139765
Iteration 12/25 | Loss: 0.00139765
Iteration 13/25 | Loss: 0.00139765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013976509217172861, 0.0013976509217172861, 0.0013976509217172861, 0.0013976509217172861, 0.0013976509217172861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013976509217172861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33101058
Iteration 2/25 | Loss: 0.00122961
Iteration 3/25 | Loss: 0.00122961
Iteration 4/25 | Loss: 0.00121566
Iteration 5/25 | Loss: 0.00121566
Iteration 6/25 | Loss: 0.00121566
Iteration 7/25 | Loss: 0.00121566
Iteration 8/25 | Loss: 0.00121566
Iteration 9/25 | Loss: 0.00121566
Iteration 10/25 | Loss: 0.00121566
Iteration 11/25 | Loss: 0.00121566
Iteration 12/25 | Loss: 0.00121566
Iteration 13/25 | Loss: 0.00121566
Iteration 14/25 | Loss: 0.00121566
Iteration 15/25 | Loss: 0.00121566
Iteration 16/25 | Loss: 0.00121566
Iteration 17/25 | Loss: 0.00121566
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012156551238149405, 0.0012156551238149405, 0.0012156551238149405, 0.0012156551238149405, 0.0012156551238149405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012156551238149405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121566
Iteration 2/1000 | Loss: 0.00006205
Iteration 3/1000 | Loss: 0.00003787
Iteration 4/1000 | Loss: 0.00003460
Iteration 5/1000 | Loss: 0.00003007
Iteration 6/1000 | Loss: 0.00002563
Iteration 7/1000 | Loss: 0.00002515
Iteration 8/1000 | Loss: 0.00002496
Iteration 9/1000 | Loss: 0.00002466
Iteration 10/1000 | Loss: 0.00002444
Iteration 11/1000 | Loss: 0.00002444
Iteration 12/1000 | Loss: 0.00002426
Iteration 13/1000 | Loss: 0.00002412
Iteration 14/1000 | Loss: 0.00002409
Iteration 15/1000 | Loss: 0.00002400
Iteration 16/1000 | Loss: 0.00002391
Iteration 17/1000 | Loss: 0.00002388
Iteration 18/1000 | Loss: 0.00002387
Iteration 19/1000 | Loss: 0.00002386
Iteration 20/1000 | Loss: 0.00002384
Iteration 21/1000 | Loss: 0.00002381
Iteration 22/1000 | Loss: 0.00002381
Iteration 23/1000 | Loss: 0.00002380
Iteration 24/1000 | Loss: 0.00002378
Iteration 25/1000 | Loss: 0.00002377
Iteration 26/1000 | Loss: 0.00002375
Iteration 27/1000 | Loss: 0.00002369
Iteration 28/1000 | Loss: 0.00002367
Iteration 29/1000 | Loss: 0.00002366
Iteration 30/1000 | Loss: 0.00002363
Iteration 31/1000 | Loss: 0.00002362
Iteration 32/1000 | Loss: 0.00002362
Iteration 33/1000 | Loss: 0.00002362
Iteration 34/1000 | Loss: 0.00002360
Iteration 35/1000 | Loss: 0.00002357
Iteration 36/1000 | Loss: 0.00002357
Iteration 37/1000 | Loss: 0.00002354
Iteration 38/1000 | Loss: 0.00002353
Iteration 39/1000 | Loss: 0.00002353
Iteration 40/1000 | Loss: 0.00002352
Iteration 41/1000 | Loss: 0.00002352
Iteration 42/1000 | Loss: 0.00002350
Iteration 43/1000 | Loss: 0.00002350
Iteration 44/1000 | Loss: 0.00002350
Iteration 45/1000 | Loss: 0.00002350
Iteration 46/1000 | Loss: 0.00002350
Iteration 47/1000 | Loss: 0.00002350
Iteration 48/1000 | Loss: 0.00002349
Iteration 49/1000 | Loss: 0.00002348
Iteration 50/1000 | Loss: 0.00002344
Iteration 51/1000 | Loss: 0.00002344
Iteration 52/1000 | Loss: 0.00002344
Iteration 53/1000 | Loss: 0.00002343
Iteration 54/1000 | Loss: 0.00002343
Iteration 55/1000 | Loss: 0.00002343
Iteration 56/1000 | Loss: 0.00002343
Iteration 57/1000 | Loss: 0.00002343
Iteration 58/1000 | Loss: 0.00002343
Iteration 59/1000 | Loss: 0.00002343
Iteration 60/1000 | Loss: 0.00002343
Iteration 61/1000 | Loss: 0.00002342
Iteration 62/1000 | Loss: 0.00002342
Iteration 63/1000 | Loss: 0.00002342
Iteration 64/1000 | Loss: 0.00002340
Iteration 65/1000 | Loss: 0.00002339
Iteration 66/1000 | Loss: 0.00002339
Iteration 67/1000 | Loss: 0.00002339
Iteration 68/1000 | Loss: 0.00002338
Iteration 69/1000 | Loss: 0.00002338
Iteration 70/1000 | Loss: 0.00002338
Iteration 71/1000 | Loss: 0.00002338
Iteration 72/1000 | Loss: 0.00002338
Iteration 73/1000 | Loss: 0.00002338
Iteration 74/1000 | Loss: 0.00002337
Iteration 75/1000 | Loss: 0.00002337
Iteration 76/1000 | Loss: 0.00002337
Iteration 77/1000 | Loss: 0.00002337
Iteration 78/1000 | Loss: 0.00002336
Iteration 79/1000 | Loss: 0.00002336
Iteration 80/1000 | Loss: 0.00002335
Iteration 81/1000 | Loss: 0.00002334
Iteration 82/1000 | Loss: 0.00002334
Iteration 83/1000 | Loss: 0.00002334
Iteration 84/1000 | Loss: 0.00002333
Iteration 85/1000 | Loss: 0.00002333
Iteration 86/1000 | Loss: 0.00002333
Iteration 87/1000 | Loss: 0.00002333
Iteration 88/1000 | Loss: 0.00002333
Iteration 89/1000 | Loss: 0.00002333
Iteration 90/1000 | Loss: 0.00002333
Iteration 91/1000 | Loss: 0.00002332
Iteration 92/1000 | Loss: 0.00002332
Iteration 93/1000 | Loss: 0.00002332
Iteration 94/1000 | Loss: 0.00002331
Iteration 95/1000 | Loss: 0.00002331
Iteration 96/1000 | Loss: 0.00002331
Iteration 97/1000 | Loss: 0.00002331
Iteration 98/1000 | Loss: 0.00002331
Iteration 99/1000 | Loss: 0.00002330
Iteration 100/1000 | Loss: 0.00002330
Iteration 101/1000 | Loss: 0.00002330
Iteration 102/1000 | Loss: 0.00002330
Iteration 103/1000 | Loss: 0.00002330
Iteration 104/1000 | Loss: 0.00002330
Iteration 105/1000 | Loss: 0.00002330
Iteration 106/1000 | Loss: 0.00002330
Iteration 107/1000 | Loss: 0.00002330
Iteration 108/1000 | Loss: 0.00002329
Iteration 109/1000 | Loss: 0.00002329
Iteration 110/1000 | Loss: 0.00002329
Iteration 111/1000 | Loss: 0.00002329
Iteration 112/1000 | Loss: 0.00002328
Iteration 113/1000 | Loss: 0.00002328
Iteration 114/1000 | Loss: 0.00002328
Iteration 115/1000 | Loss: 0.00002328
Iteration 116/1000 | Loss: 0.00002328
Iteration 117/1000 | Loss: 0.00002328
Iteration 118/1000 | Loss: 0.00002328
Iteration 119/1000 | Loss: 0.00002328
Iteration 120/1000 | Loss: 0.00002328
Iteration 121/1000 | Loss: 0.00002328
Iteration 122/1000 | Loss: 0.00002328
Iteration 123/1000 | Loss: 0.00002328
Iteration 124/1000 | Loss: 0.00002328
Iteration 125/1000 | Loss: 0.00002328
Iteration 126/1000 | Loss: 0.00002328
Iteration 127/1000 | Loss: 0.00002328
Iteration 128/1000 | Loss: 0.00002328
Iteration 129/1000 | Loss: 0.00002327
Iteration 130/1000 | Loss: 0.00002327
Iteration 131/1000 | Loss: 0.00002327
Iteration 132/1000 | Loss: 0.00002327
Iteration 133/1000 | Loss: 0.00002327
Iteration 134/1000 | Loss: 0.00002327
Iteration 135/1000 | Loss: 0.00002327
Iteration 136/1000 | Loss: 0.00002327
Iteration 137/1000 | Loss: 0.00002327
Iteration 138/1000 | Loss: 0.00002327
Iteration 139/1000 | Loss: 0.00002327
Iteration 140/1000 | Loss: 0.00002327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.327028414583765e-05, 2.327028414583765e-05, 2.327028414583765e-05, 2.327028414583765e-05, 2.327028414583765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.327028414583765e-05

Optimization complete. Final v2v error: 4.030692100524902 mm

Highest mean error: 4.432613849639893 mm for frame 212

Lowest mean error: 3.3818397521972656 mm for frame 144

Saving results

Total time: 49.8770911693573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832235
Iteration 2/25 | Loss: 0.00191972
Iteration 3/25 | Loss: 0.00163664
Iteration 4/25 | Loss: 0.00160242
Iteration 5/25 | Loss: 0.00159786
Iteration 6/25 | Loss: 0.00159483
Iteration 7/25 | Loss: 0.00159366
Iteration 8/25 | Loss: 0.00159161
Iteration 9/25 | Loss: 0.00158005
Iteration 10/25 | Loss: 0.00157473
Iteration 11/25 | Loss: 0.00157689
Iteration 12/25 | Loss: 0.00157646
Iteration 13/25 | Loss: 0.00157649
Iteration 14/25 | Loss: 0.00157376
Iteration 15/25 | Loss: 0.00156978
Iteration 16/25 | Loss: 0.00156058
Iteration 17/25 | Loss: 0.00155885
Iteration 18/25 | Loss: 0.00155819
Iteration 19/25 | Loss: 0.00155542
Iteration 20/25 | Loss: 0.00155238
Iteration 21/25 | Loss: 0.00155112
Iteration 22/25 | Loss: 0.00155092
Iteration 23/25 | Loss: 0.00155082
Iteration 24/25 | Loss: 0.00155082
Iteration 25/25 | Loss: 0.00155081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31818497
Iteration 2/25 | Loss: 0.00203908
Iteration 3/25 | Loss: 0.00203902
Iteration 4/25 | Loss: 0.00203902
Iteration 5/25 | Loss: 0.00203902
Iteration 6/25 | Loss: 0.00203902
Iteration 7/25 | Loss: 0.00203902
Iteration 8/25 | Loss: 0.00203902
Iteration 9/25 | Loss: 0.00203902
Iteration 10/25 | Loss: 0.00203902
Iteration 11/25 | Loss: 0.00203902
Iteration 12/25 | Loss: 0.00203902
Iteration 13/25 | Loss: 0.00203902
Iteration 14/25 | Loss: 0.00203902
Iteration 15/25 | Loss: 0.00203902
Iteration 16/25 | Loss: 0.00203902
Iteration 17/25 | Loss: 0.00203902
Iteration 18/25 | Loss: 0.00203902
Iteration 19/25 | Loss: 0.00203902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002039018552750349, 0.002039018552750349, 0.002039018552750349, 0.002039018552750349, 0.002039018552750349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002039018552750349

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203902
Iteration 2/1000 | Loss: 0.00036896
Iteration 3/1000 | Loss: 0.00023311
Iteration 4/1000 | Loss: 0.00020766
Iteration 5/1000 | Loss: 0.00017483
Iteration 6/1000 | Loss: 0.00016114
Iteration 7/1000 | Loss: 0.00078811
Iteration 8/1000 | Loss: 0.00034681
Iteration 9/1000 | Loss: 0.00044867
Iteration 10/1000 | Loss: 0.00014242
Iteration 11/1000 | Loss: 0.00069532
Iteration 12/1000 | Loss: 0.00014137
Iteration 13/1000 | Loss: 0.00012895
Iteration 14/1000 | Loss: 0.00012184
Iteration 15/1000 | Loss: 0.00011721
Iteration 16/1000 | Loss: 0.00011383
Iteration 17/1000 | Loss: 0.00077054
Iteration 18/1000 | Loss: 0.00045533
Iteration 19/1000 | Loss: 0.00045787
Iteration 20/1000 | Loss: 0.00011245
Iteration 21/1000 | Loss: 0.00010989
Iteration 22/1000 | Loss: 0.00010719
Iteration 23/1000 | Loss: 0.00010505
Iteration 24/1000 | Loss: 0.00010328
Iteration 25/1000 | Loss: 0.00010211
Iteration 26/1000 | Loss: 0.00010138
Iteration 27/1000 | Loss: 0.00010068
Iteration 28/1000 | Loss: 0.00010024
Iteration 29/1000 | Loss: 0.00009972
Iteration 30/1000 | Loss: 0.00009933
Iteration 31/1000 | Loss: 0.00009884
Iteration 32/1000 | Loss: 0.00009840
Iteration 33/1000 | Loss: 0.00009807
Iteration 34/1000 | Loss: 0.00009785
Iteration 35/1000 | Loss: 0.00080107
Iteration 36/1000 | Loss: 0.00305534
Iteration 37/1000 | Loss: 0.00376113
Iteration 38/1000 | Loss: 0.00212341
Iteration 39/1000 | Loss: 0.00094024
Iteration 40/1000 | Loss: 0.00069005
Iteration 41/1000 | Loss: 0.00070643
Iteration 42/1000 | Loss: 0.00010217
Iteration 43/1000 | Loss: 0.00009030
Iteration 44/1000 | Loss: 0.00008382
Iteration 45/1000 | Loss: 0.00007835
Iteration 46/1000 | Loss: 0.00156837
Iteration 47/1000 | Loss: 0.00041924
Iteration 48/1000 | Loss: 0.00007449
Iteration 49/1000 | Loss: 0.00007048
Iteration 50/1000 | Loss: 0.00068595
Iteration 51/1000 | Loss: 0.00097750
Iteration 52/1000 | Loss: 0.00044600
Iteration 53/1000 | Loss: 0.00014758
Iteration 54/1000 | Loss: 0.00007093
Iteration 55/1000 | Loss: 0.00051471
Iteration 56/1000 | Loss: 0.00009617
Iteration 57/1000 | Loss: 0.00027805
Iteration 58/1000 | Loss: 0.00006569
Iteration 59/1000 | Loss: 0.00039409
Iteration 60/1000 | Loss: 0.00018195
Iteration 61/1000 | Loss: 0.00049599
Iteration 62/1000 | Loss: 0.00019572
Iteration 63/1000 | Loss: 0.00007286
Iteration 64/1000 | Loss: 0.00006331
Iteration 65/1000 | Loss: 0.00006058
Iteration 66/1000 | Loss: 0.00005933
Iteration 67/1000 | Loss: 0.00005852
Iteration 68/1000 | Loss: 0.00061704
Iteration 69/1000 | Loss: 0.00006280
Iteration 70/1000 | Loss: 0.00005851
Iteration 71/1000 | Loss: 0.00005661
Iteration 72/1000 | Loss: 0.00005557
Iteration 73/1000 | Loss: 0.00005452
Iteration 74/1000 | Loss: 0.00064350
Iteration 75/1000 | Loss: 0.00005910
Iteration 76/1000 | Loss: 0.00005470
Iteration 77/1000 | Loss: 0.00005341
Iteration 78/1000 | Loss: 0.00005255
Iteration 79/1000 | Loss: 0.00005149
Iteration 80/1000 | Loss: 0.00005056
Iteration 81/1000 | Loss: 0.00005013
Iteration 82/1000 | Loss: 0.00004978
Iteration 83/1000 | Loss: 0.00004946
Iteration 84/1000 | Loss: 0.00004936
Iteration 85/1000 | Loss: 0.00004926
Iteration 86/1000 | Loss: 0.00004913
Iteration 87/1000 | Loss: 0.00004913
Iteration 88/1000 | Loss: 0.00004913
Iteration 89/1000 | Loss: 0.00004913
Iteration 90/1000 | Loss: 0.00004913
Iteration 91/1000 | Loss: 0.00004913
Iteration 92/1000 | Loss: 0.00004912
Iteration 93/1000 | Loss: 0.00004906
Iteration 94/1000 | Loss: 0.00004901
Iteration 95/1000 | Loss: 0.00004899
Iteration 96/1000 | Loss: 0.00004899
Iteration 97/1000 | Loss: 0.00004898
Iteration 98/1000 | Loss: 0.00004898
Iteration 99/1000 | Loss: 0.00004893
Iteration 100/1000 | Loss: 0.00004889
Iteration 101/1000 | Loss: 0.00004888
Iteration 102/1000 | Loss: 0.00004886
Iteration 103/1000 | Loss: 0.00004885
Iteration 104/1000 | Loss: 0.00004885
Iteration 105/1000 | Loss: 0.00004884
Iteration 106/1000 | Loss: 0.00004884
Iteration 107/1000 | Loss: 0.00004884
Iteration 108/1000 | Loss: 0.00004884
Iteration 109/1000 | Loss: 0.00004883
Iteration 110/1000 | Loss: 0.00004882
Iteration 111/1000 | Loss: 0.00004882
Iteration 112/1000 | Loss: 0.00004882
Iteration 113/1000 | Loss: 0.00004882
Iteration 114/1000 | Loss: 0.00004882
Iteration 115/1000 | Loss: 0.00004882
Iteration 116/1000 | Loss: 0.00004881
Iteration 117/1000 | Loss: 0.00004881
Iteration 118/1000 | Loss: 0.00004880
Iteration 119/1000 | Loss: 0.00004880
Iteration 120/1000 | Loss: 0.00004880
Iteration 121/1000 | Loss: 0.00004880
Iteration 122/1000 | Loss: 0.00004879
Iteration 123/1000 | Loss: 0.00004879
Iteration 124/1000 | Loss: 0.00004878
Iteration 125/1000 | Loss: 0.00004878
Iteration 126/1000 | Loss: 0.00004878
Iteration 127/1000 | Loss: 0.00004877
Iteration 128/1000 | Loss: 0.00004877
Iteration 129/1000 | Loss: 0.00004877
Iteration 130/1000 | Loss: 0.00004877
Iteration 131/1000 | Loss: 0.00004876
Iteration 132/1000 | Loss: 0.00004876
Iteration 133/1000 | Loss: 0.00004876
Iteration 134/1000 | Loss: 0.00004876
Iteration 135/1000 | Loss: 0.00004875
Iteration 136/1000 | Loss: 0.00004875
Iteration 137/1000 | Loss: 0.00004875
Iteration 138/1000 | Loss: 0.00004875
Iteration 139/1000 | Loss: 0.00004875
Iteration 140/1000 | Loss: 0.00004874
Iteration 141/1000 | Loss: 0.00004874
Iteration 142/1000 | Loss: 0.00004873
Iteration 143/1000 | Loss: 0.00004873
Iteration 144/1000 | Loss: 0.00004873
Iteration 145/1000 | Loss: 0.00004873
Iteration 146/1000 | Loss: 0.00004872
Iteration 147/1000 | Loss: 0.00004871
Iteration 148/1000 | Loss: 0.00004870
Iteration 149/1000 | Loss: 0.00004870
Iteration 150/1000 | Loss: 0.00004869
Iteration 151/1000 | Loss: 0.00004869
Iteration 152/1000 | Loss: 0.00004869
Iteration 153/1000 | Loss: 0.00004868
Iteration 154/1000 | Loss: 0.00004868
Iteration 155/1000 | Loss: 0.00004868
Iteration 156/1000 | Loss: 0.00004868
Iteration 157/1000 | Loss: 0.00004868
Iteration 158/1000 | Loss: 0.00004867
Iteration 159/1000 | Loss: 0.00004867
Iteration 160/1000 | Loss: 0.00004867
Iteration 161/1000 | Loss: 0.00004867
Iteration 162/1000 | Loss: 0.00004867
Iteration 163/1000 | Loss: 0.00004866
Iteration 164/1000 | Loss: 0.00004866
Iteration 165/1000 | Loss: 0.00004866
Iteration 166/1000 | Loss: 0.00004866
Iteration 167/1000 | Loss: 0.00004866
Iteration 168/1000 | Loss: 0.00004866
Iteration 169/1000 | Loss: 0.00004866
Iteration 170/1000 | Loss: 0.00004866
Iteration 171/1000 | Loss: 0.00004866
Iteration 172/1000 | Loss: 0.00004866
Iteration 173/1000 | Loss: 0.00004866
Iteration 174/1000 | Loss: 0.00004865
Iteration 175/1000 | Loss: 0.00004865
Iteration 176/1000 | Loss: 0.00004865
Iteration 177/1000 | Loss: 0.00004865
Iteration 178/1000 | Loss: 0.00004864
Iteration 179/1000 | Loss: 0.00004864
Iteration 180/1000 | Loss: 0.00004864
Iteration 181/1000 | Loss: 0.00004863
Iteration 182/1000 | Loss: 0.00004862
Iteration 183/1000 | Loss: 0.00004862
Iteration 184/1000 | Loss: 0.00004862
Iteration 185/1000 | Loss: 0.00004862
Iteration 186/1000 | Loss: 0.00004861
Iteration 187/1000 | Loss: 0.00004861
Iteration 188/1000 | Loss: 0.00004861
Iteration 189/1000 | Loss: 0.00004861
Iteration 190/1000 | Loss: 0.00004860
Iteration 191/1000 | Loss: 0.00004860
Iteration 192/1000 | Loss: 0.00004860
Iteration 193/1000 | Loss: 0.00004860
Iteration 194/1000 | Loss: 0.00004860
Iteration 195/1000 | Loss: 0.00004860
Iteration 196/1000 | Loss: 0.00004860
Iteration 197/1000 | Loss: 0.00004860
Iteration 198/1000 | Loss: 0.00004860
Iteration 199/1000 | Loss: 0.00004860
Iteration 200/1000 | Loss: 0.00004860
Iteration 201/1000 | Loss: 0.00004859
Iteration 202/1000 | Loss: 0.00004859
Iteration 203/1000 | Loss: 0.00004859
Iteration 204/1000 | Loss: 0.00004859
Iteration 205/1000 | Loss: 0.00004859
Iteration 206/1000 | Loss: 0.00004859
Iteration 207/1000 | Loss: 0.00004859
Iteration 208/1000 | Loss: 0.00004859
Iteration 209/1000 | Loss: 0.00004859
Iteration 210/1000 | Loss: 0.00004859
Iteration 211/1000 | Loss: 0.00004859
Iteration 212/1000 | Loss: 0.00004858
Iteration 213/1000 | Loss: 0.00004858
Iteration 214/1000 | Loss: 0.00004858
Iteration 215/1000 | Loss: 0.00004858
Iteration 216/1000 | Loss: 0.00004858
Iteration 217/1000 | Loss: 0.00004858
Iteration 218/1000 | Loss: 0.00004858
Iteration 219/1000 | Loss: 0.00004858
Iteration 220/1000 | Loss: 0.00004858
Iteration 221/1000 | Loss: 0.00004858
Iteration 222/1000 | Loss: 0.00004858
Iteration 223/1000 | Loss: 0.00004858
Iteration 224/1000 | Loss: 0.00004858
Iteration 225/1000 | Loss: 0.00004858
Iteration 226/1000 | Loss: 0.00004858
Iteration 227/1000 | Loss: 0.00004858
Iteration 228/1000 | Loss: 0.00004858
Iteration 229/1000 | Loss: 0.00004858
Iteration 230/1000 | Loss: 0.00004858
Iteration 231/1000 | Loss: 0.00004858
Iteration 232/1000 | Loss: 0.00004858
Iteration 233/1000 | Loss: 0.00004858
Iteration 234/1000 | Loss: 0.00004858
Iteration 235/1000 | Loss: 0.00004858
Iteration 236/1000 | Loss: 0.00004858
Iteration 237/1000 | Loss: 0.00004858
Iteration 238/1000 | Loss: 0.00004858
Iteration 239/1000 | Loss: 0.00004858
Iteration 240/1000 | Loss: 0.00004858
Iteration 241/1000 | Loss: 0.00004858
Iteration 242/1000 | Loss: 0.00004858
Iteration 243/1000 | Loss: 0.00004858
Iteration 244/1000 | Loss: 0.00004858
Iteration 245/1000 | Loss: 0.00004858
Iteration 246/1000 | Loss: 0.00004858
Iteration 247/1000 | Loss: 0.00004858
Iteration 248/1000 | Loss: 0.00004858
Iteration 249/1000 | Loss: 0.00004858
Iteration 250/1000 | Loss: 0.00004858
Iteration 251/1000 | Loss: 0.00004858
Iteration 252/1000 | Loss: 0.00004858
Iteration 253/1000 | Loss: 0.00004858
Iteration 254/1000 | Loss: 0.00004858
Iteration 255/1000 | Loss: 0.00004858
Iteration 256/1000 | Loss: 0.00004858
Iteration 257/1000 | Loss: 0.00004858
Iteration 258/1000 | Loss: 0.00004858
Iteration 259/1000 | Loss: 0.00004858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [4.857776366407052e-05, 4.857776366407052e-05, 4.857776366407052e-05, 4.857776366407052e-05, 4.857776366407052e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.857776366407052e-05

Optimization complete. Final v2v error: 4.8822245597839355 mm

Highest mean error: 10.87697696685791 mm for frame 103

Lowest mean error: 3.399468183517456 mm for frame 2

Saving results

Total time: 170.77106618881226
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453114
Iteration 2/25 | Loss: 0.00131556
Iteration 3/25 | Loss: 0.00126371
Iteration 4/25 | Loss: 0.00125558
Iteration 5/25 | Loss: 0.00125361
Iteration 6/25 | Loss: 0.00125319
Iteration 7/25 | Loss: 0.00125319
Iteration 8/25 | Loss: 0.00125319
Iteration 9/25 | Loss: 0.00125319
Iteration 10/25 | Loss: 0.00125319
Iteration 11/25 | Loss: 0.00125319
Iteration 12/25 | Loss: 0.00125319
Iteration 13/25 | Loss: 0.00125319
Iteration 14/25 | Loss: 0.00125319
Iteration 15/25 | Loss: 0.00125319
Iteration 16/25 | Loss: 0.00125319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001253193593584001, 0.001253193593584001, 0.001253193593584001, 0.001253193593584001, 0.001253193593584001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001253193593584001

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53623283
Iteration 2/25 | Loss: 0.00081079
Iteration 3/25 | Loss: 0.00081079
Iteration 4/25 | Loss: 0.00081079
Iteration 5/25 | Loss: 0.00081079
Iteration 6/25 | Loss: 0.00081079
Iteration 7/25 | Loss: 0.00081079
Iteration 8/25 | Loss: 0.00081079
Iteration 9/25 | Loss: 0.00081079
Iteration 10/25 | Loss: 0.00081079
Iteration 11/25 | Loss: 0.00081079
Iteration 12/25 | Loss: 0.00081079
Iteration 13/25 | Loss: 0.00081079
Iteration 14/25 | Loss: 0.00081079
Iteration 15/25 | Loss: 0.00081079
Iteration 16/25 | Loss: 0.00081079
Iteration 17/25 | Loss: 0.00081079
Iteration 18/25 | Loss: 0.00081079
Iteration 19/25 | Loss: 0.00081079
Iteration 20/25 | Loss: 0.00081079
Iteration 21/25 | Loss: 0.00081079
Iteration 22/25 | Loss: 0.00081079
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008107863832265139, 0.0008107863832265139, 0.0008107863832265139, 0.0008107863832265139, 0.0008107863832265139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008107863832265139

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081079
Iteration 2/1000 | Loss: 0.00002788
Iteration 3/1000 | Loss: 0.00001880
Iteration 4/1000 | Loss: 0.00001605
Iteration 5/1000 | Loss: 0.00001483
Iteration 6/1000 | Loss: 0.00001421
Iteration 7/1000 | Loss: 0.00001374
Iteration 8/1000 | Loss: 0.00001327
Iteration 9/1000 | Loss: 0.00001306
Iteration 10/1000 | Loss: 0.00001304
Iteration 11/1000 | Loss: 0.00001283
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001250
Iteration 14/1000 | Loss: 0.00001250
Iteration 15/1000 | Loss: 0.00001247
Iteration 16/1000 | Loss: 0.00001236
Iteration 17/1000 | Loss: 0.00001234
Iteration 18/1000 | Loss: 0.00001231
Iteration 19/1000 | Loss: 0.00001226
Iteration 20/1000 | Loss: 0.00001224
Iteration 21/1000 | Loss: 0.00001223
Iteration 22/1000 | Loss: 0.00001217
Iteration 23/1000 | Loss: 0.00001217
Iteration 24/1000 | Loss: 0.00001216
Iteration 25/1000 | Loss: 0.00001212
Iteration 26/1000 | Loss: 0.00001206
Iteration 27/1000 | Loss: 0.00001205
Iteration 28/1000 | Loss: 0.00001205
Iteration 29/1000 | Loss: 0.00001204
Iteration 30/1000 | Loss: 0.00001204
Iteration 31/1000 | Loss: 0.00001204
Iteration 32/1000 | Loss: 0.00001204
Iteration 33/1000 | Loss: 0.00001204
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001203
Iteration 36/1000 | Loss: 0.00001203
Iteration 37/1000 | Loss: 0.00001202
Iteration 38/1000 | Loss: 0.00001202
Iteration 39/1000 | Loss: 0.00001201
Iteration 40/1000 | Loss: 0.00001200
Iteration 41/1000 | Loss: 0.00001199
Iteration 42/1000 | Loss: 0.00001199
Iteration 43/1000 | Loss: 0.00001199
Iteration 44/1000 | Loss: 0.00001198
Iteration 45/1000 | Loss: 0.00001198
Iteration 46/1000 | Loss: 0.00001198
Iteration 47/1000 | Loss: 0.00001198
Iteration 48/1000 | Loss: 0.00001197
Iteration 49/1000 | Loss: 0.00001197
Iteration 50/1000 | Loss: 0.00001196
Iteration 51/1000 | Loss: 0.00001196
Iteration 52/1000 | Loss: 0.00001195
Iteration 53/1000 | Loss: 0.00001195
Iteration 54/1000 | Loss: 0.00001195
Iteration 55/1000 | Loss: 0.00001195
Iteration 56/1000 | Loss: 0.00001195
Iteration 57/1000 | Loss: 0.00001195
Iteration 58/1000 | Loss: 0.00001195
Iteration 59/1000 | Loss: 0.00001195
Iteration 60/1000 | Loss: 0.00001195
Iteration 61/1000 | Loss: 0.00001194
Iteration 62/1000 | Loss: 0.00001194
Iteration 63/1000 | Loss: 0.00001194
Iteration 64/1000 | Loss: 0.00001193
Iteration 65/1000 | Loss: 0.00001192
Iteration 66/1000 | Loss: 0.00001192
Iteration 67/1000 | Loss: 0.00001192
Iteration 68/1000 | Loss: 0.00001192
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001191
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001189
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001189
Iteration 76/1000 | Loss: 0.00001188
Iteration 77/1000 | Loss: 0.00001187
Iteration 78/1000 | Loss: 0.00001187
Iteration 79/1000 | Loss: 0.00001187
Iteration 80/1000 | Loss: 0.00001186
Iteration 81/1000 | Loss: 0.00001186
Iteration 82/1000 | Loss: 0.00001186
Iteration 83/1000 | Loss: 0.00001186
Iteration 84/1000 | Loss: 0.00001186
Iteration 85/1000 | Loss: 0.00001184
Iteration 86/1000 | Loss: 0.00001184
Iteration 87/1000 | Loss: 0.00001184
Iteration 88/1000 | Loss: 0.00001184
Iteration 89/1000 | Loss: 0.00001184
Iteration 90/1000 | Loss: 0.00001184
Iteration 91/1000 | Loss: 0.00001183
Iteration 92/1000 | Loss: 0.00001183
Iteration 93/1000 | Loss: 0.00001183
Iteration 94/1000 | Loss: 0.00001183
Iteration 95/1000 | Loss: 0.00001183
Iteration 96/1000 | Loss: 0.00001183
Iteration 97/1000 | Loss: 0.00001183
Iteration 98/1000 | Loss: 0.00001183
Iteration 99/1000 | Loss: 0.00001183
Iteration 100/1000 | Loss: 0.00001183
Iteration 101/1000 | Loss: 0.00001182
Iteration 102/1000 | Loss: 0.00001182
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001181
Iteration 105/1000 | Loss: 0.00001181
Iteration 106/1000 | Loss: 0.00001180
Iteration 107/1000 | Loss: 0.00001180
Iteration 108/1000 | Loss: 0.00001180
Iteration 109/1000 | Loss: 0.00001180
Iteration 110/1000 | Loss: 0.00001180
Iteration 111/1000 | Loss: 0.00001180
Iteration 112/1000 | Loss: 0.00001180
Iteration 113/1000 | Loss: 0.00001180
Iteration 114/1000 | Loss: 0.00001180
Iteration 115/1000 | Loss: 0.00001180
Iteration 116/1000 | Loss: 0.00001179
Iteration 117/1000 | Loss: 0.00001179
Iteration 118/1000 | Loss: 0.00001179
Iteration 119/1000 | Loss: 0.00001179
Iteration 120/1000 | Loss: 0.00001178
Iteration 121/1000 | Loss: 0.00001178
Iteration 122/1000 | Loss: 0.00001178
Iteration 123/1000 | Loss: 0.00001177
Iteration 124/1000 | Loss: 0.00001177
Iteration 125/1000 | Loss: 0.00001177
Iteration 126/1000 | Loss: 0.00001177
Iteration 127/1000 | Loss: 0.00001177
Iteration 128/1000 | Loss: 0.00001177
Iteration 129/1000 | Loss: 0.00001176
Iteration 130/1000 | Loss: 0.00001176
Iteration 131/1000 | Loss: 0.00001176
Iteration 132/1000 | Loss: 0.00001176
Iteration 133/1000 | Loss: 0.00001176
Iteration 134/1000 | Loss: 0.00001176
Iteration 135/1000 | Loss: 0.00001176
Iteration 136/1000 | Loss: 0.00001176
Iteration 137/1000 | Loss: 0.00001175
Iteration 138/1000 | Loss: 0.00001175
Iteration 139/1000 | Loss: 0.00001175
Iteration 140/1000 | Loss: 0.00001175
Iteration 141/1000 | Loss: 0.00001175
Iteration 142/1000 | Loss: 0.00001175
Iteration 143/1000 | Loss: 0.00001175
Iteration 144/1000 | Loss: 0.00001175
Iteration 145/1000 | Loss: 0.00001174
Iteration 146/1000 | Loss: 0.00001174
Iteration 147/1000 | Loss: 0.00001174
Iteration 148/1000 | Loss: 0.00001174
Iteration 149/1000 | Loss: 0.00001174
Iteration 150/1000 | Loss: 0.00001174
Iteration 151/1000 | Loss: 0.00001174
Iteration 152/1000 | Loss: 0.00001174
Iteration 153/1000 | Loss: 0.00001174
Iteration 154/1000 | Loss: 0.00001174
Iteration 155/1000 | Loss: 0.00001174
Iteration 156/1000 | Loss: 0.00001174
Iteration 157/1000 | Loss: 0.00001174
Iteration 158/1000 | Loss: 0.00001174
Iteration 159/1000 | Loss: 0.00001174
Iteration 160/1000 | Loss: 0.00001174
Iteration 161/1000 | Loss: 0.00001174
Iteration 162/1000 | Loss: 0.00001174
Iteration 163/1000 | Loss: 0.00001174
Iteration 164/1000 | Loss: 0.00001174
Iteration 165/1000 | Loss: 0.00001174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.1741752132365946e-05, 1.1741752132365946e-05, 1.1741752132365946e-05, 1.1741752132365946e-05, 1.1741752132365946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1741752132365946e-05

Optimization complete. Final v2v error: 2.9554200172424316 mm

Highest mean error: 3.1237261295318604 mm for frame 40

Lowest mean error: 2.8277108669281006 mm for frame 105

Saving results

Total time: 38.23864960670471
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00766712
Iteration 2/25 | Loss: 0.00145088
Iteration 3/25 | Loss: 0.00130860
Iteration 4/25 | Loss: 0.00129641
Iteration 5/25 | Loss: 0.00129414
Iteration 6/25 | Loss: 0.00129391
Iteration 7/25 | Loss: 0.00129391
Iteration 8/25 | Loss: 0.00129391
Iteration 9/25 | Loss: 0.00129391
Iteration 10/25 | Loss: 0.00129391
Iteration 11/25 | Loss: 0.00129391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012939113657921553, 0.0012939113657921553, 0.0012939113657921553, 0.0012939113657921553, 0.0012939113657921553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012939113657921553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40302777
Iteration 2/25 | Loss: 0.00089484
Iteration 3/25 | Loss: 0.00089484
Iteration 4/25 | Loss: 0.00089484
Iteration 5/25 | Loss: 0.00089484
Iteration 6/25 | Loss: 0.00089484
Iteration 7/25 | Loss: 0.00089484
Iteration 8/25 | Loss: 0.00089484
Iteration 9/25 | Loss: 0.00089484
Iteration 10/25 | Loss: 0.00089484
Iteration 11/25 | Loss: 0.00089484
Iteration 12/25 | Loss: 0.00089484
Iteration 13/25 | Loss: 0.00089484
Iteration 14/25 | Loss: 0.00089484
Iteration 15/25 | Loss: 0.00089484
Iteration 16/25 | Loss: 0.00089484
Iteration 17/25 | Loss: 0.00089484
Iteration 18/25 | Loss: 0.00089484
Iteration 19/25 | Loss: 0.00089484
Iteration 20/25 | Loss: 0.00089484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000894836091902107, 0.000894836091902107, 0.000894836091902107, 0.000894836091902107, 0.000894836091902107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000894836091902107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089484
Iteration 2/1000 | Loss: 0.00003218
Iteration 3/1000 | Loss: 0.00002142
Iteration 4/1000 | Loss: 0.00001689
Iteration 5/1000 | Loss: 0.00001562
Iteration 6/1000 | Loss: 0.00001483
Iteration 7/1000 | Loss: 0.00001428
Iteration 8/1000 | Loss: 0.00001377
Iteration 9/1000 | Loss: 0.00001350
Iteration 10/1000 | Loss: 0.00001326
Iteration 11/1000 | Loss: 0.00001299
Iteration 12/1000 | Loss: 0.00001292
Iteration 13/1000 | Loss: 0.00001277
Iteration 14/1000 | Loss: 0.00001277
Iteration 15/1000 | Loss: 0.00001271
Iteration 16/1000 | Loss: 0.00001268
Iteration 17/1000 | Loss: 0.00001268
Iteration 18/1000 | Loss: 0.00001267
Iteration 19/1000 | Loss: 0.00001266
Iteration 20/1000 | Loss: 0.00001266
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001261
Iteration 24/1000 | Loss: 0.00001260
Iteration 25/1000 | Loss: 0.00001260
Iteration 26/1000 | Loss: 0.00001259
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001259
Iteration 29/1000 | Loss: 0.00001259
Iteration 30/1000 | Loss: 0.00001257
Iteration 31/1000 | Loss: 0.00001257
Iteration 32/1000 | Loss: 0.00001256
Iteration 33/1000 | Loss: 0.00001256
Iteration 34/1000 | Loss: 0.00001256
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001256
Iteration 37/1000 | Loss: 0.00001255
Iteration 38/1000 | Loss: 0.00001255
Iteration 39/1000 | Loss: 0.00001255
Iteration 40/1000 | Loss: 0.00001255
Iteration 41/1000 | Loss: 0.00001254
Iteration 42/1000 | Loss: 0.00001254
Iteration 43/1000 | Loss: 0.00001252
Iteration 44/1000 | Loss: 0.00001252
Iteration 45/1000 | Loss: 0.00001252
Iteration 46/1000 | Loss: 0.00001252
Iteration 47/1000 | Loss: 0.00001252
Iteration 48/1000 | Loss: 0.00001251
Iteration 49/1000 | Loss: 0.00001251
Iteration 50/1000 | Loss: 0.00001251
Iteration 51/1000 | Loss: 0.00001251
Iteration 52/1000 | Loss: 0.00001251
Iteration 53/1000 | Loss: 0.00001251
Iteration 54/1000 | Loss: 0.00001251
Iteration 55/1000 | Loss: 0.00001251
Iteration 56/1000 | Loss: 0.00001251
Iteration 57/1000 | Loss: 0.00001250
Iteration 58/1000 | Loss: 0.00001250
Iteration 59/1000 | Loss: 0.00001250
Iteration 60/1000 | Loss: 0.00001250
Iteration 61/1000 | Loss: 0.00001249
Iteration 62/1000 | Loss: 0.00001249
Iteration 63/1000 | Loss: 0.00001249
Iteration 64/1000 | Loss: 0.00001249
Iteration 65/1000 | Loss: 0.00001249
Iteration 66/1000 | Loss: 0.00001249
Iteration 67/1000 | Loss: 0.00001249
Iteration 68/1000 | Loss: 0.00001249
Iteration 69/1000 | Loss: 0.00001248
Iteration 70/1000 | Loss: 0.00001248
Iteration 71/1000 | Loss: 0.00001248
Iteration 72/1000 | Loss: 0.00001248
Iteration 73/1000 | Loss: 0.00001247
Iteration 74/1000 | Loss: 0.00001247
Iteration 75/1000 | Loss: 0.00001247
Iteration 76/1000 | Loss: 0.00001247
Iteration 77/1000 | Loss: 0.00001247
Iteration 78/1000 | Loss: 0.00001246
Iteration 79/1000 | Loss: 0.00001246
Iteration 80/1000 | Loss: 0.00001246
Iteration 81/1000 | Loss: 0.00001246
Iteration 82/1000 | Loss: 0.00001246
Iteration 83/1000 | Loss: 0.00001245
Iteration 84/1000 | Loss: 0.00001245
Iteration 85/1000 | Loss: 0.00001245
Iteration 86/1000 | Loss: 0.00001244
Iteration 87/1000 | Loss: 0.00001244
Iteration 88/1000 | Loss: 0.00001244
Iteration 89/1000 | Loss: 0.00001244
Iteration 90/1000 | Loss: 0.00001244
Iteration 91/1000 | Loss: 0.00001243
Iteration 92/1000 | Loss: 0.00001243
Iteration 93/1000 | Loss: 0.00001243
Iteration 94/1000 | Loss: 0.00001242
Iteration 95/1000 | Loss: 0.00001242
Iteration 96/1000 | Loss: 0.00001242
Iteration 97/1000 | Loss: 0.00001241
Iteration 98/1000 | Loss: 0.00001241
Iteration 99/1000 | Loss: 0.00001241
Iteration 100/1000 | Loss: 0.00001241
Iteration 101/1000 | Loss: 0.00001241
Iteration 102/1000 | Loss: 0.00001241
Iteration 103/1000 | Loss: 0.00001241
Iteration 104/1000 | Loss: 0.00001241
Iteration 105/1000 | Loss: 0.00001241
Iteration 106/1000 | Loss: 0.00001241
Iteration 107/1000 | Loss: 0.00001240
Iteration 108/1000 | Loss: 0.00001240
Iteration 109/1000 | Loss: 0.00001240
Iteration 110/1000 | Loss: 0.00001240
Iteration 111/1000 | Loss: 0.00001240
Iteration 112/1000 | Loss: 0.00001240
Iteration 113/1000 | Loss: 0.00001240
Iteration 114/1000 | Loss: 0.00001240
Iteration 115/1000 | Loss: 0.00001239
Iteration 116/1000 | Loss: 0.00001239
Iteration 117/1000 | Loss: 0.00001239
Iteration 118/1000 | Loss: 0.00001239
Iteration 119/1000 | Loss: 0.00001239
Iteration 120/1000 | Loss: 0.00001239
Iteration 121/1000 | Loss: 0.00001238
Iteration 122/1000 | Loss: 0.00001238
Iteration 123/1000 | Loss: 0.00001238
Iteration 124/1000 | Loss: 0.00001237
Iteration 125/1000 | Loss: 0.00001237
Iteration 126/1000 | Loss: 0.00001237
Iteration 127/1000 | Loss: 0.00001237
Iteration 128/1000 | Loss: 0.00001237
Iteration 129/1000 | Loss: 0.00001236
Iteration 130/1000 | Loss: 0.00001236
Iteration 131/1000 | Loss: 0.00001235
Iteration 132/1000 | Loss: 0.00001235
Iteration 133/1000 | Loss: 0.00001234
Iteration 134/1000 | Loss: 0.00001234
Iteration 135/1000 | Loss: 0.00001234
Iteration 136/1000 | Loss: 0.00001233
Iteration 137/1000 | Loss: 0.00001233
Iteration 138/1000 | Loss: 0.00001233
Iteration 139/1000 | Loss: 0.00001233
Iteration 140/1000 | Loss: 0.00001233
Iteration 141/1000 | Loss: 0.00001233
Iteration 142/1000 | Loss: 0.00001233
Iteration 143/1000 | Loss: 0.00001233
Iteration 144/1000 | Loss: 0.00001233
Iteration 145/1000 | Loss: 0.00001233
Iteration 146/1000 | Loss: 0.00001233
Iteration 147/1000 | Loss: 0.00001233
Iteration 148/1000 | Loss: 0.00001233
Iteration 149/1000 | Loss: 0.00001233
Iteration 150/1000 | Loss: 0.00001233
Iteration 151/1000 | Loss: 0.00001233
Iteration 152/1000 | Loss: 0.00001233
Iteration 153/1000 | Loss: 0.00001233
Iteration 154/1000 | Loss: 0.00001233
Iteration 155/1000 | Loss: 0.00001233
Iteration 156/1000 | Loss: 0.00001232
Iteration 157/1000 | Loss: 0.00001232
Iteration 158/1000 | Loss: 0.00001231
Iteration 159/1000 | Loss: 0.00001231
Iteration 160/1000 | Loss: 0.00001231
Iteration 161/1000 | Loss: 0.00001231
Iteration 162/1000 | Loss: 0.00001230
Iteration 163/1000 | Loss: 0.00001230
Iteration 164/1000 | Loss: 0.00001230
Iteration 165/1000 | Loss: 0.00001230
Iteration 166/1000 | Loss: 0.00001230
Iteration 167/1000 | Loss: 0.00001230
Iteration 168/1000 | Loss: 0.00001230
Iteration 169/1000 | Loss: 0.00001229
Iteration 170/1000 | Loss: 0.00001229
Iteration 171/1000 | Loss: 0.00001229
Iteration 172/1000 | Loss: 0.00001229
Iteration 173/1000 | Loss: 0.00001229
Iteration 174/1000 | Loss: 0.00001228
Iteration 175/1000 | Loss: 0.00001228
Iteration 176/1000 | Loss: 0.00001228
Iteration 177/1000 | Loss: 0.00001228
Iteration 178/1000 | Loss: 0.00001228
Iteration 179/1000 | Loss: 0.00001228
Iteration 180/1000 | Loss: 0.00001228
Iteration 181/1000 | Loss: 0.00001228
Iteration 182/1000 | Loss: 0.00001228
Iteration 183/1000 | Loss: 0.00001227
Iteration 184/1000 | Loss: 0.00001227
Iteration 185/1000 | Loss: 0.00001227
Iteration 186/1000 | Loss: 0.00001227
Iteration 187/1000 | Loss: 0.00001227
Iteration 188/1000 | Loss: 0.00001227
Iteration 189/1000 | Loss: 0.00001227
Iteration 190/1000 | Loss: 0.00001227
Iteration 191/1000 | Loss: 0.00001227
Iteration 192/1000 | Loss: 0.00001227
Iteration 193/1000 | Loss: 0.00001227
Iteration 194/1000 | Loss: 0.00001227
Iteration 195/1000 | Loss: 0.00001227
Iteration 196/1000 | Loss: 0.00001227
Iteration 197/1000 | Loss: 0.00001227
Iteration 198/1000 | Loss: 0.00001227
Iteration 199/1000 | Loss: 0.00001227
Iteration 200/1000 | Loss: 0.00001226
Iteration 201/1000 | Loss: 0.00001226
Iteration 202/1000 | Loss: 0.00001226
Iteration 203/1000 | Loss: 0.00001226
Iteration 204/1000 | Loss: 0.00001226
Iteration 205/1000 | Loss: 0.00001226
Iteration 206/1000 | Loss: 0.00001226
Iteration 207/1000 | Loss: 0.00001226
Iteration 208/1000 | Loss: 0.00001226
Iteration 209/1000 | Loss: 0.00001226
Iteration 210/1000 | Loss: 0.00001226
Iteration 211/1000 | Loss: 0.00001226
Iteration 212/1000 | Loss: 0.00001225
Iteration 213/1000 | Loss: 0.00001225
Iteration 214/1000 | Loss: 0.00001225
Iteration 215/1000 | Loss: 0.00001225
Iteration 216/1000 | Loss: 0.00001225
Iteration 217/1000 | Loss: 0.00001225
Iteration 218/1000 | Loss: 0.00001225
Iteration 219/1000 | Loss: 0.00001225
Iteration 220/1000 | Loss: 0.00001225
Iteration 221/1000 | Loss: 0.00001225
Iteration 222/1000 | Loss: 0.00001225
Iteration 223/1000 | Loss: 0.00001225
Iteration 224/1000 | Loss: 0.00001225
Iteration 225/1000 | Loss: 0.00001225
Iteration 226/1000 | Loss: 0.00001225
Iteration 227/1000 | Loss: 0.00001225
Iteration 228/1000 | Loss: 0.00001225
Iteration 229/1000 | Loss: 0.00001225
Iteration 230/1000 | Loss: 0.00001225
Iteration 231/1000 | Loss: 0.00001224
Iteration 232/1000 | Loss: 0.00001224
Iteration 233/1000 | Loss: 0.00001224
Iteration 234/1000 | Loss: 0.00001223
Iteration 235/1000 | Loss: 0.00001223
Iteration 236/1000 | Loss: 0.00001223
Iteration 237/1000 | Loss: 0.00001223
Iteration 238/1000 | Loss: 0.00001223
Iteration 239/1000 | Loss: 0.00001223
Iteration 240/1000 | Loss: 0.00001223
Iteration 241/1000 | Loss: 0.00001222
Iteration 242/1000 | Loss: 0.00001222
Iteration 243/1000 | Loss: 0.00001222
Iteration 244/1000 | Loss: 0.00001222
Iteration 245/1000 | Loss: 0.00001222
Iteration 246/1000 | Loss: 0.00001222
Iteration 247/1000 | Loss: 0.00001222
Iteration 248/1000 | Loss: 0.00001222
Iteration 249/1000 | Loss: 0.00001222
Iteration 250/1000 | Loss: 0.00001222
Iteration 251/1000 | Loss: 0.00001222
Iteration 252/1000 | Loss: 0.00001222
Iteration 253/1000 | Loss: 0.00001222
Iteration 254/1000 | Loss: 0.00001222
Iteration 255/1000 | Loss: 0.00001222
Iteration 256/1000 | Loss: 0.00001222
Iteration 257/1000 | Loss: 0.00001222
Iteration 258/1000 | Loss: 0.00001222
Iteration 259/1000 | Loss: 0.00001222
Iteration 260/1000 | Loss: 0.00001222
Iteration 261/1000 | Loss: 0.00001222
Iteration 262/1000 | Loss: 0.00001222
Iteration 263/1000 | Loss: 0.00001222
Iteration 264/1000 | Loss: 0.00001222
Iteration 265/1000 | Loss: 0.00001222
Iteration 266/1000 | Loss: 0.00001222
Iteration 267/1000 | Loss: 0.00001222
Iteration 268/1000 | Loss: 0.00001222
Iteration 269/1000 | Loss: 0.00001222
Iteration 270/1000 | Loss: 0.00001222
Iteration 271/1000 | Loss: 0.00001222
Iteration 272/1000 | Loss: 0.00001222
Iteration 273/1000 | Loss: 0.00001222
Iteration 274/1000 | Loss: 0.00001222
Iteration 275/1000 | Loss: 0.00001222
Iteration 276/1000 | Loss: 0.00001222
Iteration 277/1000 | Loss: 0.00001222
Iteration 278/1000 | Loss: 0.00001222
Iteration 279/1000 | Loss: 0.00001222
Iteration 280/1000 | Loss: 0.00001222
Iteration 281/1000 | Loss: 0.00001222
Iteration 282/1000 | Loss: 0.00001222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 282. Stopping optimization.
Last 5 losses: [1.2224155398143921e-05, 1.2224155398143921e-05, 1.2224155398143921e-05, 1.2224155398143921e-05, 1.2224155398143921e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2224155398143921e-05

Optimization complete. Final v2v error: 2.9925107955932617 mm

Highest mean error: 3.348724842071533 mm for frame 84

Lowest mean error: 2.8969624042510986 mm for frame 164

Saving results

Total time: 44.04935956001282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794931
Iteration 2/25 | Loss: 0.00217145
Iteration 3/25 | Loss: 0.00171860
Iteration 4/25 | Loss: 0.00156090
Iteration 5/25 | Loss: 0.00149228
Iteration 6/25 | Loss: 0.00145623
Iteration 7/25 | Loss: 0.00144194
Iteration 8/25 | Loss: 0.00143170
Iteration 9/25 | Loss: 0.00143010
Iteration 10/25 | Loss: 0.00142984
Iteration 11/25 | Loss: 0.00142976
Iteration 12/25 | Loss: 0.00142976
Iteration 13/25 | Loss: 0.00142976
Iteration 14/25 | Loss: 0.00142975
Iteration 15/25 | Loss: 0.00142975
Iteration 16/25 | Loss: 0.00142975
Iteration 17/25 | Loss: 0.00142975
Iteration 18/25 | Loss: 0.00142975
Iteration 19/25 | Loss: 0.00142975
Iteration 20/25 | Loss: 0.00142975
Iteration 21/25 | Loss: 0.00142975
Iteration 22/25 | Loss: 0.00142975
Iteration 23/25 | Loss: 0.00142975
Iteration 24/25 | Loss: 0.00142975
Iteration 25/25 | Loss: 0.00142974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33153653
Iteration 2/25 | Loss: 0.00083990
Iteration 3/25 | Loss: 0.00083988
Iteration 4/25 | Loss: 0.00083988
Iteration 5/25 | Loss: 0.00083988
Iteration 6/25 | Loss: 0.00083988
Iteration 7/25 | Loss: 0.00083988
Iteration 8/25 | Loss: 0.00083988
Iteration 9/25 | Loss: 0.00083987
Iteration 10/25 | Loss: 0.00083987
Iteration 11/25 | Loss: 0.00083987
Iteration 12/25 | Loss: 0.00083987
Iteration 13/25 | Loss: 0.00083987
Iteration 14/25 | Loss: 0.00083987
Iteration 15/25 | Loss: 0.00083987
Iteration 16/25 | Loss: 0.00083987
Iteration 17/25 | Loss: 0.00083987
Iteration 18/25 | Loss: 0.00083987
Iteration 19/25 | Loss: 0.00083987
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008398740901611745, 0.0008398740901611745, 0.0008398740901611745, 0.0008398740901611745, 0.0008398740901611745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008398740901611745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083987
Iteration 2/1000 | Loss: 0.00004673
Iteration 3/1000 | Loss: 0.00002860
Iteration 4/1000 | Loss: 0.00002421
Iteration 5/1000 | Loss: 0.00002310
Iteration 6/1000 | Loss: 0.00002245
Iteration 7/1000 | Loss: 0.00002208
Iteration 8/1000 | Loss: 0.00002176
Iteration 9/1000 | Loss: 0.00002154
Iteration 10/1000 | Loss: 0.00002144
Iteration 11/1000 | Loss: 0.00002142
Iteration 12/1000 | Loss: 0.00002138
Iteration 13/1000 | Loss: 0.00002138
Iteration 14/1000 | Loss: 0.00002137
Iteration 15/1000 | Loss: 0.00002128
Iteration 16/1000 | Loss: 0.00002123
Iteration 17/1000 | Loss: 0.00002122
Iteration 18/1000 | Loss: 0.00002110
Iteration 19/1000 | Loss: 0.00002109
Iteration 20/1000 | Loss: 0.00002107
Iteration 21/1000 | Loss: 0.00002107
Iteration 22/1000 | Loss: 0.00002105
Iteration 23/1000 | Loss: 0.00002104
Iteration 24/1000 | Loss: 0.00002104
Iteration 25/1000 | Loss: 0.00002104
Iteration 26/1000 | Loss: 0.00002096
Iteration 27/1000 | Loss: 0.00002091
Iteration 28/1000 | Loss: 0.00002090
Iteration 29/1000 | Loss: 0.00002090
Iteration 30/1000 | Loss: 0.00002078
Iteration 31/1000 | Loss: 0.00002078
Iteration 32/1000 | Loss: 0.00002078
Iteration 33/1000 | Loss: 0.00002078
Iteration 34/1000 | Loss: 0.00002078
Iteration 35/1000 | Loss: 0.00002078
Iteration 36/1000 | Loss: 0.00002078
Iteration 37/1000 | Loss: 0.00002078
Iteration 38/1000 | Loss: 0.00002078
Iteration 39/1000 | Loss: 0.00002078
Iteration 40/1000 | Loss: 0.00002078
Iteration 41/1000 | Loss: 0.00002078
Iteration 42/1000 | Loss: 0.00002077
Iteration 43/1000 | Loss: 0.00002077
Iteration 44/1000 | Loss: 0.00002077
Iteration 45/1000 | Loss: 0.00002076
Iteration 46/1000 | Loss: 0.00002076
Iteration 47/1000 | Loss: 0.00002076
Iteration 48/1000 | Loss: 0.00002076
Iteration 49/1000 | Loss: 0.00002076
Iteration 50/1000 | Loss: 0.00002076
Iteration 51/1000 | Loss: 0.00002076
Iteration 52/1000 | Loss: 0.00002075
Iteration 53/1000 | Loss: 0.00002075
Iteration 54/1000 | Loss: 0.00002075
Iteration 55/1000 | Loss: 0.00002075
Iteration 56/1000 | Loss: 0.00002075
Iteration 57/1000 | Loss: 0.00002074
Iteration 58/1000 | Loss: 0.00002074
Iteration 59/1000 | Loss: 0.00002074
Iteration 60/1000 | Loss: 0.00002074
Iteration 61/1000 | Loss: 0.00002074
Iteration 62/1000 | Loss: 0.00002074
Iteration 63/1000 | Loss: 0.00002074
Iteration 64/1000 | Loss: 0.00002073
Iteration 65/1000 | Loss: 0.00002073
Iteration 66/1000 | Loss: 0.00002073
Iteration 67/1000 | Loss: 0.00002073
Iteration 68/1000 | Loss: 0.00002073
Iteration 69/1000 | Loss: 0.00002072
Iteration 70/1000 | Loss: 0.00002072
Iteration 71/1000 | Loss: 0.00002072
Iteration 72/1000 | Loss: 0.00002072
Iteration 73/1000 | Loss: 0.00002072
Iteration 74/1000 | Loss: 0.00002071
Iteration 75/1000 | Loss: 0.00002071
Iteration 76/1000 | Loss: 0.00002071
Iteration 77/1000 | Loss: 0.00002071
Iteration 78/1000 | Loss: 0.00002071
Iteration 79/1000 | Loss: 0.00002071
Iteration 80/1000 | Loss: 0.00002071
Iteration 81/1000 | Loss: 0.00002071
Iteration 82/1000 | Loss: 0.00002070
Iteration 83/1000 | Loss: 0.00002070
Iteration 84/1000 | Loss: 0.00002070
Iteration 85/1000 | Loss: 0.00002069
Iteration 86/1000 | Loss: 0.00002069
Iteration 87/1000 | Loss: 0.00002068
Iteration 88/1000 | Loss: 0.00002068
Iteration 89/1000 | Loss: 0.00002068
Iteration 90/1000 | Loss: 0.00002068
Iteration 91/1000 | Loss: 0.00002068
Iteration 92/1000 | Loss: 0.00002068
Iteration 93/1000 | Loss: 0.00002068
Iteration 94/1000 | Loss: 0.00002068
Iteration 95/1000 | Loss: 0.00002068
Iteration 96/1000 | Loss: 0.00002068
Iteration 97/1000 | Loss: 0.00002068
Iteration 98/1000 | Loss: 0.00002067
Iteration 99/1000 | Loss: 0.00002067
Iteration 100/1000 | Loss: 0.00002067
Iteration 101/1000 | Loss: 0.00002067
Iteration 102/1000 | Loss: 0.00002067
Iteration 103/1000 | Loss: 0.00002067
Iteration 104/1000 | Loss: 0.00002067
Iteration 105/1000 | Loss: 0.00002067
Iteration 106/1000 | Loss: 0.00002067
Iteration 107/1000 | Loss: 0.00002067
Iteration 108/1000 | Loss: 0.00002067
Iteration 109/1000 | Loss: 0.00002067
Iteration 110/1000 | Loss: 0.00002067
Iteration 111/1000 | Loss: 0.00002067
Iteration 112/1000 | Loss: 0.00002066
Iteration 113/1000 | Loss: 0.00002066
Iteration 114/1000 | Loss: 0.00002066
Iteration 115/1000 | Loss: 0.00002066
Iteration 116/1000 | Loss: 0.00002066
Iteration 117/1000 | Loss: 0.00002066
Iteration 118/1000 | Loss: 0.00002066
Iteration 119/1000 | Loss: 0.00002066
Iteration 120/1000 | Loss: 0.00002066
Iteration 121/1000 | Loss: 0.00002066
Iteration 122/1000 | Loss: 0.00002066
Iteration 123/1000 | Loss: 0.00002066
Iteration 124/1000 | Loss: 0.00002066
Iteration 125/1000 | Loss: 0.00002065
Iteration 126/1000 | Loss: 0.00002065
Iteration 127/1000 | Loss: 0.00002065
Iteration 128/1000 | Loss: 0.00002065
Iteration 129/1000 | Loss: 0.00002065
Iteration 130/1000 | Loss: 0.00002065
Iteration 131/1000 | Loss: 0.00002065
Iteration 132/1000 | Loss: 0.00002065
Iteration 133/1000 | Loss: 0.00002065
Iteration 134/1000 | Loss: 0.00002065
Iteration 135/1000 | Loss: 0.00002065
Iteration 136/1000 | Loss: 0.00002064
Iteration 137/1000 | Loss: 0.00002064
Iteration 138/1000 | Loss: 0.00002063
Iteration 139/1000 | Loss: 0.00002063
Iteration 140/1000 | Loss: 0.00002063
Iteration 141/1000 | Loss: 0.00002063
Iteration 142/1000 | Loss: 0.00002063
Iteration 143/1000 | Loss: 0.00002063
Iteration 144/1000 | Loss: 0.00002063
Iteration 145/1000 | Loss: 0.00002063
Iteration 146/1000 | Loss: 0.00002063
Iteration 147/1000 | Loss: 0.00002063
Iteration 148/1000 | Loss: 0.00002063
Iteration 149/1000 | Loss: 0.00002063
Iteration 150/1000 | Loss: 0.00002063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.0627316189347766e-05, 2.0627316189347766e-05, 2.0627316189347766e-05, 2.0627316189347766e-05, 2.0627316189347766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0627316189347766e-05

Optimization complete. Final v2v error: 3.7653911113739014 mm

Highest mean error: 4.103087902069092 mm for frame 9

Lowest mean error: 3.6366384029388428 mm for frame 71

Saving results

Total time: 45.54522085189819
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796231
Iteration 2/25 | Loss: 0.00143507
Iteration 3/25 | Loss: 0.00133299
Iteration 4/25 | Loss: 0.00130575
Iteration 5/25 | Loss: 0.00129702
Iteration 6/25 | Loss: 0.00129487
Iteration 7/25 | Loss: 0.00129391
Iteration 8/25 | Loss: 0.00129391
Iteration 9/25 | Loss: 0.00129391
Iteration 10/25 | Loss: 0.00129391
Iteration 11/25 | Loss: 0.00129391
Iteration 12/25 | Loss: 0.00129391
Iteration 13/25 | Loss: 0.00129391
Iteration 14/25 | Loss: 0.00129391
Iteration 15/25 | Loss: 0.00129391
Iteration 16/25 | Loss: 0.00129391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012939060106873512, 0.0012939060106873512, 0.0012939060106873512, 0.0012939060106873512, 0.0012939060106873512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012939060106873512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52156878
Iteration 2/25 | Loss: 0.00089405
Iteration 3/25 | Loss: 0.00089405
Iteration 4/25 | Loss: 0.00089405
Iteration 5/25 | Loss: 0.00089405
Iteration 6/25 | Loss: 0.00089405
Iteration 7/25 | Loss: 0.00089405
Iteration 8/25 | Loss: 0.00089405
Iteration 9/25 | Loss: 0.00089405
Iteration 10/25 | Loss: 0.00089405
Iteration 11/25 | Loss: 0.00089405
Iteration 12/25 | Loss: 0.00089405
Iteration 13/25 | Loss: 0.00089405
Iteration 14/25 | Loss: 0.00089405
Iteration 15/25 | Loss: 0.00089405
Iteration 16/25 | Loss: 0.00089405
Iteration 17/25 | Loss: 0.00089405
Iteration 18/25 | Loss: 0.00089405
Iteration 19/25 | Loss: 0.00089405
Iteration 20/25 | Loss: 0.00089405
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000894047087058425, 0.000894047087058425, 0.000894047087058425, 0.000894047087058425, 0.000894047087058425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000894047087058425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089405
Iteration 2/1000 | Loss: 0.00005822
Iteration 3/1000 | Loss: 0.00004255
Iteration 4/1000 | Loss: 0.00003359
Iteration 5/1000 | Loss: 0.00003052
Iteration 6/1000 | Loss: 0.00002900
Iteration 7/1000 | Loss: 0.00002768
Iteration 8/1000 | Loss: 0.00002696
Iteration 9/1000 | Loss: 0.00002636
Iteration 10/1000 | Loss: 0.00002588
Iteration 11/1000 | Loss: 0.00002556
Iteration 12/1000 | Loss: 0.00002530
Iteration 13/1000 | Loss: 0.00002512
Iteration 14/1000 | Loss: 0.00002511
Iteration 15/1000 | Loss: 0.00002493
Iteration 16/1000 | Loss: 0.00002491
Iteration 17/1000 | Loss: 0.00002490
Iteration 18/1000 | Loss: 0.00002482
Iteration 19/1000 | Loss: 0.00002478
Iteration 20/1000 | Loss: 0.00002477
Iteration 21/1000 | Loss: 0.00002475
Iteration 22/1000 | Loss: 0.00002474
Iteration 23/1000 | Loss: 0.00002469
Iteration 24/1000 | Loss: 0.00002467
Iteration 25/1000 | Loss: 0.00002466
Iteration 26/1000 | Loss: 0.00002465
Iteration 27/1000 | Loss: 0.00002464
Iteration 28/1000 | Loss: 0.00002457
Iteration 29/1000 | Loss: 0.00002457
Iteration 30/1000 | Loss: 0.00002453
Iteration 31/1000 | Loss: 0.00002452
Iteration 32/1000 | Loss: 0.00002447
Iteration 33/1000 | Loss: 0.00002447
Iteration 34/1000 | Loss: 0.00002445
Iteration 35/1000 | Loss: 0.00002445
Iteration 36/1000 | Loss: 0.00002443
Iteration 37/1000 | Loss: 0.00002442
Iteration 38/1000 | Loss: 0.00002442
Iteration 39/1000 | Loss: 0.00002442
Iteration 40/1000 | Loss: 0.00002441
Iteration 41/1000 | Loss: 0.00002440
Iteration 42/1000 | Loss: 0.00002439
Iteration 43/1000 | Loss: 0.00002439
Iteration 44/1000 | Loss: 0.00002438
Iteration 45/1000 | Loss: 0.00002438
Iteration 46/1000 | Loss: 0.00002438
Iteration 47/1000 | Loss: 0.00002437
Iteration 48/1000 | Loss: 0.00002437
Iteration 49/1000 | Loss: 0.00002436
Iteration 50/1000 | Loss: 0.00002436
Iteration 51/1000 | Loss: 0.00002436
Iteration 52/1000 | Loss: 0.00002435
Iteration 53/1000 | Loss: 0.00002435
Iteration 54/1000 | Loss: 0.00002434
Iteration 55/1000 | Loss: 0.00002434
Iteration 56/1000 | Loss: 0.00002434
Iteration 57/1000 | Loss: 0.00002433
Iteration 58/1000 | Loss: 0.00002433
Iteration 59/1000 | Loss: 0.00002433
Iteration 60/1000 | Loss: 0.00002432
Iteration 61/1000 | Loss: 0.00002432
Iteration 62/1000 | Loss: 0.00002432
Iteration 63/1000 | Loss: 0.00002431
Iteration 64/1000 | Loss: 0.00002431
Iteration 65/1000 | Loss: 0.00002431
Iteration 66/1000 | Loss: 0.00002430
Iteration 67/1000 | Loss: 0.00002430
Iteration 68/1000 | Loss: 0.00002430
Iteration 69/1000 | Loss: 0.00002429
Iteration 70/1000 | Loss: 0.00002429
Iteration 71/1000 | Loss: 0.00002429
Iteration 72/1000 | Loss: 0.00002428
Iteration 73/1000 | Loss: 0.00002428
Iteration 74/1000 | Loss: 0.00002428
Iteration 75/1000 | Loss: 0.00002428
Iteration 76/1000 | Loss: 0.00002427
Iteration 77/1000 | Loss: 0.00002427
Iteration 78/1000 | Loss: 0.00002427
Iteration 79/1000 | Loss: 0.00002426
Iteration 80/1000 | Loss: 0.00002426
Iteration 81/1000 | Loss: 0.00002426
Iteration 82/1000 | Loss: 0.00002425
Iteration 83/1000 | Loss: 0.00002425
Iteration 84/1000 | Loss: 0.00002425
Iteration 85/1000 | Loss: 0.00002425
Iteration 86/1000 | Loss: 0.00002425
Iteration 87/1000 | Loss: 0.00002425
Iteration 88/1000 | Loss: 0.00002425
Iteration 89/1000 | Loss: 0.00002425
Iteration 90/1000 | Loss: 0.00002424
Iteration 91/1000 | Loss: 0.00002424
Iteration 92/1000 | Loss: 0.00002424
Iteration 93/1000 | Loss: 0.00002424
Iteration 94/1000 | Loss: 0.00002423
Iteration 95/1000 | Loss: 0.00002423
Iteration 96/1000 | Loss: 0.00002423
Iteration 97/1000 | Loss: 0.00002423
Iteration 98/1000 | Loss: 0.00002422
Iteration 99/1000 | Loss: 0.00002422
Iteration 100/1000 | Loss: 0.00002422
Iteration 101/1000 | Loss: 0.00002422
Iteration 102/1000 | Loss: 0.00002421
Iteration 103/1000 | Loss: 0.00002421
Iteration 104/1000 | Loss: 0.00002421
Iteration 105/1000 | Loss: 0.00002421
Iteration 106/1000 | Loss: 0.00002421
Iteration 107/1000 | Loss: 0.00002421
Iteration 108/1000 | Loss: 0.00002421
Iteration 109/1000 | Loss: 0.00002421
Iteration 110/1000 | Loss: 0.00002421
Iteration 111/1000 | Loss: 0.00002420
Iteration 112/1000 | Loss: 0.00002420
Iteration 113/1000 | Loss: 0.00002420
Iteration 114/1000 | Loss: 0.00002420
Iteration 115/1000 | Loss: 0.00002420
Iteration 116/1000 | Loss: 0.00002420
Iteration 117/1000 | Loss: 0.00002420
Iteration 118/1000 | Loss: 0.00002420
Iteration 119/1000 | Loss: 0.00002420
Iteration 120/1000 | Loss: 0.00002420
Iteration 121/1000 | Loss: 0.00002420
Iteration 122/1000 | Loss: 0.00002420
Iteration 123/1000 | Loss: 0.00002420
Iteration 124/1000 | Loss: 0.00002420
Iteration 125/1000 | Loss: 0.00002420
Iteration 126/1000 | Loss: 0.00002419
Iteration 127/1000 | Loss: 0.00002419
Iteration 128/1000 | Loss: 0.00002419
Iteration 129/1000 | Loss: 0.00002419
Iteration 130/1000 | Loss: 0.00002419
Iteration 131/1000 | Loss: 0.00002419
Iteration 132/1000 | Loss: 0.00002419
Iteration 133/1000 | Loss: 0.00002419
Iteration 134/1000 | Loss: 0.00002419
Iteration 135/1000 | Loss: 0.00002419
Iteration 136/1000 | Loss: 0.00002419
Iteration 137/1000 | Loss: 0.00002419
Iteration 138/1000 | Loss: 0.00002418
Iteration 139/1000 | Loss: 0.00002418
Iteration 140/1000 | Loss: 0.00002418
Iteration 141/1000 | Loss: 0.00002418
Iteration 142/1000 | Loss: 0.00002418
Iteration 143/1000 | Loss: 0.00002418
Iteration 144/1000 | Loss: 0.00002418
Iteration 145/1000 | Loss: 0.00002418
Iteration 146/1000 | Loss: 0.00002418
Iteration 147/1000 | Loss: 0.00002418
Iteration 148/1000 | Loss: 0.00002418
Iteration 149/1000 | Loss: 0.00002418
Iteration 150/1000 | Loss: 0.00002418
Iteration 151/1000 | Loss: 0.00002418
Iteration 152/1000 | Loss: 0.00002418
Iteration 153/1000 | Loss: 0.00002418
Iteration 154/1000 | Loss: 0.00002418
Iteration 155/1000 | Loss: 0.00002418
Iteration 156/1000 | Loss: 0.00002417
Iteration 157/1000 | Loss: 0.00002417
Iteration 158/1000 | Loss: 0.00002417
Iteration 159/1000 | Loss: 0.00002417
Iteration 160/1000 | Loss: 0.00002417
Iteration 161/1000 | Loss: 0.00002417
Iteration 162/1000 | Loss: 0.00002417
Iteration 163/1000 | Loss: 0.00002417
Iteration 164/1000 | Loss: 0.00002417
Iteration 165/1000 | Loss: 0.00002417
Iteration 166/1000 | Loss: 0.00002417
Iteration 167/1000 | Loss: 0.00002417
Iteration 168/1000 | Loss: 0.00002417
Iteration 169/1000 | Loss: 0.00002417
Iteration 170/1000 | Loss: 0.00002417
Iteration 171/1000 | Loss: 0.00002417
Iteration 172/1000 | Loss: 0.00002417
Iteration 173/1000 | Loss: 0.00002417
Iteration 174/1000 | Loss: 0.00002417
Iteration 175/1000 | Loss: 0.00002417
Iteration 176/1000 | Loss: 0.00002417
Iteration 177/1000 | Loss: 0.00002417
Iteration 178/1000 | Loss: 0.00002417
Iteration 179/1000 | Loss: 0.00002417
Iteration 180/1000 | Loss: 0.00002417
Iteration 181/1000 | Loss: 0.00002417
Iteration 182/1000 | Loss: 0.00002417
Iteration 183/1000 | Loss: 0.00002417
Iteration 184/1000 | Loss: 0.00002417
Iteration 185/1000 | Loss: 0.00002417
Iteration 186/1000 | Loss: 0.00002417
Iteration 187/1000 | Loss: 0.00002417
Iteration 188/1000 | Loss: 0.00002417
Iteration 189/1000 | Loss: 0.00002417
Iteration 190/1000 | Loss: 0.00002417
Iteration 191/1000 | Loss: 0.00002417
Iteration 192/1000 | Loss: 0.00002417
Iteration 193/1000 | Loss: 0.00002417
Iteration 194/1000 | Loss: 0.00002417
Iteration 195/1000 | Loss: 0.00002417
Iteration 196/1000 | Loss: 0.00002417
Iteration 197/1000 | Loss: 0.00002417
Iteration 198/1000 | Loss: 0.00002417
Iteration 199/1000 | Loss: 0.00002417
Iteration 200/1000 | Loss: 0.00002417
Iteration 201/1000 | Loss: 0.00002417
Iteration 202/1000 | Loss: 0.00002417
Iteration 203/1000 | Loss: 0.00002417
Iteration 204/1000 | Loss: 0.00002417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.416682218608912e-05, 2.416682218608912e-05, 2.416682218608912e-05, 2.416682218608912e-05, 2.416682218608912e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.416682218608912e-05

Optimization complete. Final v2v error: 4.083356857299805 mm

Highest mean error: 5.730312824249268 mm for frame 93

Lowest mean error: 3.1209299564361572 mm for frame 79

Saving results

Total time: 45.331315755844116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841957
Iteration 2/25 | Loss: 0.00142951
Iteration 3/25 | Loss: 0.00135624
Iteration 4/25 | Loss: 0.00134812
Iteration 5/25 | Loss: 0.00134787
Iteration 6/25 | Loss: 0.00134787
Iteration 7/25 | Loss: 0.00134787
Iteration 8/25 | Loss: 0.00134787
Iteration 9/25 | Loss: 0.00134787
Iteration 10/25 | Loss: 0.00134787
Iteration 11/25 | Loss: 0.00134787
Iteration 12/25 | Loss: 0.00134787
Iteration 13/25 | Loss: 0.00134787
Iteration 14/25 | Loss: 0.00134787
Iteration 15/25 | Loss: 0.00134787
Iteration 16/25 | Loss: 0.00134787
Iteration 17/25 | Loss: 0.00134787
Iteration 18/25 | Loss: 0.00134787
Iteration 19/25 | Loss: 0.00134787
Iteration 20/25 | Loss: 0.00134787
Iteration 21/25 | Loss: 0.00134787
Iteration 22/25 | Loss: 0.00134787
Iteration 23/25 | Loss: 0.00134787
Iteration 24/25 | Loss: 0.00134787
Iteration 25/25 | Loss: 0.00134787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32630980
Iteration 2/25 | Loss: 0.00078058
Iteration 3/25 | Loss: 0.00078049
Iteration 4/25 | Loss: 0.00078049
Iteration 5/25 | Loss: 0.00078049
Iteration 6/25 | Loss: 0.00078049
Iteration 7/25 | Loss: 0.00078049
Iteration 8/25 | Loss: 0.00078049
Iteration 9/25 | Loss: 0.00078049
Iteration 10/25 | Loss: 0.00078049
Iteration 11/25 | Loss: 0.00078049
Iteration 12/25 | Loss: 0.00078049
Iteration 13/25 | Loss: 0.00078049
Iteration 14/25 | Loss: 0.00078049
Iteration 15/25 | Loss: 0.00078049
Iteration 16/25 | Loss: 0.00078049
Iteration 17/25 | Loss: 0.00078049
Iteration 18/25 | Loss: 0.00078049
Iteration 19/25 | Loss: 0.00078049
Iteration 20/25 | Loss: 0.00078049
Iteration 21/25 | Loss: 0.00078049
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007804850465618074, 0.0007804850465618074, 0.0007804850465618074, 0.0007804850465618074, 0.0007804850465618074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007804850465618074

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078049
Iteration 2/1000 | Loss: 0.00003418
Iteration 3/1000 | Loss: 0.00002362
Iteration 4/1000 | Loss: 0.00002121
Iteration 5/1000 | Loss: 0.00001961
Iteration 6/1000 | Loss: 0.00001875
Iteration 7/1000 | Loss: 0.00001825
Iteration 8/1000 | Loss: 0.00001783
Iteration 9/1000 | Loss: 0.00001738
Iteration 10/1000 | Loss: 0.00001707
Iteration 11/1000 | Loss: 0.00001705
Iteration 12/1000 | Loss: 0.00001701
Iteration 13/1000 | Loss: 0.00001679
Iteration 14/1000 | Loss: 0.00001658
Iteration 15/1000 | Loss: 0.00001638
Iteration 16/1000 | Loss: 0.00001625
Iteration 17/1000 | Loss: 0.00001624
Iteration 18/1000 | Loss: 0.00001620
Iteration 19/1000 | Loss: 0.00001617
Iteration 20/1000 | Loss: 0.00001611
Iteration 21/1000 | Loss: 0.00001610
Iteration 22/1000 | Loss: 0.00001610
Iteration 23/1000 | Loss: 0.00001600
Iteration 24/1000 | Loss: 0.00001595
Iteration 25/1000 | Loss: 0.00001592
Iteration 26/1000 | Loss: 0.00001592
Iteration 27/1000 | Loss: 0.00001591
Iteration 28/1000 | Loss: 0.00001591
Iteration 29/1000 | Loss: 0.00001590
Iteration 30/1000 | Loss: 0.00001587
Iteration 31/1000 | Loss: 0.00001586
Iteration 32/1000 | Loss: 0.00001586
Iteration 33/1000 | Loss: 0.00001585
Iteration 34/1000 | Loss: 0.00001584
Iteration 35/1000 | Loss: 0.00001584
Iteration 36/1000 | Loss: 0.00001584
Iteration 37/1000 | Loss: 0.00001583
Iteration 38/1000 | Loss: 0.00001583
Iteration 39/1000 | Loss: 0.00001582
Iteration 40/1000 | Loss: 0.00001582
Iteration 41/1000 | Loss: 0.00001582
Iteration 42/1000 | Loss: 0.00001582
Iteration 43/1000 | Loss: 0.00001582
Iteration 44/1000 | Loss: 0.00001582
Iteration 45/1000 | Loss: 0.00001582
Iteration 46/1000 | Loss: 0.00001582
Iteration 47/1000 | Loss: 0.00001581
Iteration 48/1000 | Loss: 0.00001581
Iteration 49/1000 | Loss: 0.00001581
Iteration 50/1000 | Loss: 0.00001581
Iteration 51/1000 | Loss: 0.00001580
Iteration 52/1000 | Loss: 0.00001580
Iteration 53/1000 | Loss: 0.00001580
Iteration 54/1000 | Loss: 0.00001580
Iteration 55/1000 | Loss: 0.00001579
Iteration 56/1000 | Loss: 0.00001579
Iteration 57/1000 | Loss: 0.00001578
Iteration 58/1000 | Loss: 0.00001578
Iteration 59/1000 | Loss: 0.00001578
Iteration 60/1000 | Loss: 0.00001577
Iteration 61/1000 | Loss: 0.00001577
Iteration 62/1000 | Loss: 0.00001576
Iteration 63/1000 | Loss: 0.00001576
Iteration 64/1000 | Loss: 0.00001575
Iteration 65/1000 | Loss: 0.00001575
Iteration 66/1000 | Loss: 0.00001575
Iteration 67/1000 | Loss: 0.00001575
Iteration 68/1000 | Loss: 0.00001575
Iteration 69/1000 | Loss: 0.00001575
Iteration 70/1000 | Loss: 0.00001575
Iteration 71/1000 | Loss: 0.00001575
Iteration 72/1000 | Loss: 0.00001575
Iteration 73/1000 | Loss: 0.00001575
Iteration 74/1000 | Loss: 0.00001575
Iteration 75/1000 | Loss: 0.00001574
Iteration 76/1000 | Loss: 0.00001574
Iteration 77/1000 | Loss: 0.00001574
Iteration 78/1000 | Loss: 0.00001573
Iteration 79/1000 | Loss: 0.00001573
Iteration 80/1000 | Loss: 0.00001573
Iteration 81/1000 | Loss: 0.00001573
Iteration 82/1000 | Loss: 0.00001572
Iteration 83/1000 | Loss: 0.00001571
Iteration 84/1000 | Loss: 0.00001571
Iteration 85/1000 | Loss: 0.00001571
Iteration 86/1000 | Loss: 0.00001570
Iteration 87/1000 | Loss: 0.00001570
Iteration 88/1000 | Loss: 0.00001570
Iteration 89/1000 | Loss: 0.00001570
Iteration 90/1000 | Loss: 0.00001570
Iteration 91/1000 | Loss: 0.00001570
Iteration 92/1000 | Loss: 0.00001570
Iteration 93/1000 | Loss: 0.00001569
Iteration 94/1000 | Loss: 0.00001569
Iteration 95/1000 | Loss: 0.00001569
Iteration 96/1000 | Loss: 0.00001569
Iteration 97/1000 | Loss: 0.00001568
Iteration 98/1000 | Loss: 0.00001567
Iteration 99/1000 | Loss: 0.00001567
Iteration 100/1000 | Loss: 0.00001567
Iteration 101/1000 | Loss: 0.00001567
Iteration 102/1000 | Loss: 0.00001567
Iteration 103/1000 | Loss: 0.00001567
Iteration 104/1000 | Loss: 0.00001567
Iteration 105/1000 | Loss: 0.00001567
Iteration 106/1000 | Loss: 0.00001566
Iteration 107/1000 | Loss: 0.00001565
Iteration 108/1000 | Loss: 0.00001565
Iteration 109/1000 | Loss: 0.00001565
Iteration 110/1000 | Loss: 0.00001565
Iteration 111/1000 | Loss: 0.00001565
Iteration 112/1000 | Loss: 0.00001565
Iteration 113/1000 | Loss: 0.00001564
Iteration 114/1000 | Loss: 0.00001564
Iteration 115/1000 | Loss: 0.00001564
Iteration 116/1000 | Loss: 0.00001564
Iteration 117/1000 | Loss: 0.00001563
Iteration 118/1000 | Loss: 0.00001563
Iteration 119/1000 | Loss: 0.00001563
Iteration 120/1000 | Loss: 0.00001563
Iteration 121/1000 | Loss: 0.00001563
Iteration 122/1000 | Loss: 0.00001563
Iteration 123/1000 | Loss: 0.00001563
Iteration 124/1000 | Loss: 0.00001563
Iteration 125/1000 | Loss: 0.00001563
Iteration 126/1000 | Loss: 0.00001563
Iteration 127/1000 | Loss: 0.00001563
Iteration 128/1000 | Loss: 0.00001563
Iteration 129/1000 | Loss: 0.00001563
Iteration 130/1000 | Loss: 0.00001562
Iteration 131/1000 | Loss: 0.00001562
Iteration 132/1000 | Loss: 0.00001562
Iteration 133/1000 | Loss: 0.00001562
Iteration 134/1000 | Loss: 0.00001562
Iteration 135/1000 | Loss: 0.00001562
Iteration 136/1000 | Loss: 0.00001562
Iteration 137/1000 | Loss: 0.00001562
Iteration 138/1000 | Loss: 0.00001562
Iteration 139/1000 | Loss: 0.00001562
Iteration 140/1000 | Loss: 0.00001562
Iteration 141/1000 | Loss: 0.00001562
Iteration 142/1000 | Loss: 0.00001562
Iteration 143/1000 | Loss: 0.00001561
Iteration 144/1000 | Loss: 0.00001561
Iteration 145/1000 | Loss: 0.00001561
Iteration 146/1000 | Loss: 0.00001561
Iteration 147/1000 | Loss: 0.00001560
Iteration 148/1000 | Loss: 0.00001560
Iteration 149/1000 | Loss: 0.00001560
Iteration 150/1000 | Loss: 0.00001560
Iteration 151/1000 | Loss: 0.00001560
Iteration 152/1000 | Loss: 0.00001559
Iteration 153/1000 | Loss: 0.00001559
Iteration 154/1000 | Loss: 0.00001559
Iteration 155/1000 | Loss: 0.00001559
Iteration 156/1000 | Loss: 0.00001559
Iteration 157/1000 | Loss: 0.00001559
Iteration 158/1000 | Loss: 0.00001559
Iteration 159/1000 | Loss: 0.00001559
Iteration 160/1000 | Loss: 0.00001559
Iteration 161/1000 | Loss: 0.00001559
Iteration 162/1000 | Loss: 0.00001559
Iteration 163/1000 | Loss: 0.00001559
Iteration 164/1000 | Loss: 0.00001559
Iteration 165/1000 | Loss: 0.00001559
Iteration 166/1000 | Loss: 0.00001559
Iteration 167/1000 | Loss: 0.00001559
Iteration 168/1000 | Loss: 0.00001559
Iteration 169/1000 | Loss: 0.00001559
Iteration 170/1000 | Loss: 0.00001559
Iteration 171/1000 | Loss: 0.00001559
Iteration 172/1000 | Loss: 0.00001559
Iteration 173/1000 | Loss: 0.00001559
Iteration 174/1000 | Loss: 0.00001558
Iteration 175/1000 | Loss: 0.00001558
Iteration 176/1000 | Loss: 0.00001558
Iteration 177/1000 | Loss: 0.00001558
Iteration 178/1000 | Loss: 0.00001558
Iteration 179/1000 | Loss: 0.00001558
Iteration 180/1000 | Loss: 0.00001558
Iteration 181/1000 | Loss: 0.00001558
Iteration 182/1000 | Loss: 0.00001558
Iteration 183/1000 | Loss: 0.00001558
Iteration 184/1000 | Loss: 0.00001558
Iteration 185/1000 | Loss: 0.00001558
Iteration 186/1000 | Loss: 0.00001558
Iteration 187/1000 | Loss: 0.00001558
Iteration 188/1000 | Loss: 0.00001558
Iteration 189/1000 | Loss: 0.00001558
Iteration 190/1000 | Loss: 0.00001558
Iteration 191/1000 | Loss: 0.00001558
Iteration 192/1000 | Loss: 0.00001558
Iteration 193/1000 | Loss: 0.00001558
Iteration 194/1000 | Loss: 0.00001558
Iteration 195/1000 | Loss: 0.00001558
Iteration 196/1000 | Loss: 0.00001558
Iteration 197/1000 | Loss: 0.00001558
Iteration 198/1000 | Loss: 0.00001558
Iteration 199/1000 | Loss: 0.00001558
Iteration 200/1000 | Loss: 0.00001558
Iteration 201/1000 | Loss: 0.00001558
Iteration 202/1000 | Loss: 0.00001558
Iteration 203/1000 | Loss: 0.00001558
Iteration 204/1000 | Loss: 0.00001558
Iteration 205/1000 | Loss: 0.00001558
Iteration 206/1000 | Loss: 0.00001558
Iteration 207/1000 | Loss: 0.00001558
Iteration 208/1000 | Loss: 0.00001558
Iteration 209/1000 | Loss: 0.00001558
Iteration 210/1000 | Loss: 0.00001558
Iteration 211/1000 | Loss: 0.00001558
Iteration 212/1000 | Loss: 0.00001558
Iteration 213/1000 | Loss: 0.00001558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.557866926304996e-05, 1.557866926304996e-05, 1.557866926304996e-05, 1.557866926304996e-05, 1.557866926304996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.557866926304996e-05

Optimization complete. Final v2v error: 3.3622424602508545 mm

Highest mean error: 3.546888828277588 mm for frame 69

Lowest mean error: 3.224107265472412 mm for frame 160

Saving results

Total time: 49.58087754249573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394504
Iteration 2/25 | Loss: 0.00140897
Iteration 3/25 | Loss: 0.00130289
Iteration 4/25 | Loss: 0.00129396
Iteration 5/25 | Loss: 0.00129254
Iteration 6/25 | Loss: 0.00129254
Iteration 7/25 | Loss: 0.00129254
Iteration 8/25 | Loss: 0.00129254
Iteration 9/25 | Loss: 0.00129254
Iteration 10/25 | Loss: 0.00129254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012925381306558847, 0.0012925381306558847, 0.0012925381306558847, 0.0012925381306558847, 0.0012925381306558847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012925381306558847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52679527
Iteration 2/25 | Loss: 0.00082594
Iteration 3/25 | Loss: 0.00082593
Iteration 4/25 | Loss: 0.00082593
Iteration 5/25 | Loss: 0.00082593
Iteration 6/25 | Loss: 0.00082593
Iteration 7/25 | Loss: 0.00082593
Iteration 8/25 | Loss: 0.00082593
Iteration 9/25 | Loss: 0.00082593
Iteration 10/25 | Loss: 0.00082593
Iteration 11/25 | Loss: 0.00082593
Iteration 12/25 | Loss: 0.00082593
Iteration 13/25 | Loss: 0.00082593
Iteration 14/25 | Loss: 0.00082593
Iteration 15/25 | Loss: 0.00082593
Iteration 16/25 | Loss: 0.00082593
Iteration 17/25 | Loss: 0.00082593
Iteration 18/25 | Loss: 0.00082593
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000825932074803859, 0.000825932074803859, 0.000825932074803859, 0.000825932074803859, 0.000825932074803859]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000825932074803859

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082593
Iteration 2/1000 | Loss: 0.00003350
Iteration 3/1000 | Loss: 0.00001895
Iteration 4/1000 | Loss: 0.00001677
Iteration 5/1000 | Loss: 0.00001550
Iteration 6/1000 | Loss: 0.00001473
Iteration 7/1000 | Loss: 0.00001419
Iteration 8/1000 | Loss: 0.00001414
Iteration 9/1000 | Loss: 0.00001387
Iteration 10/1000 | Loss: 0.00001353
Iteration 11/1000 | Loss: 0.00001321
Iteration 12/1000 | Loss: 0.00001319
Iteration 13/1000 | Loss: 0.00001302
Iteration 14/1000 | Loss: 0.00001297
Iteration 15/1000 | Loss: 0.00001286
Iteration 16/1000 | Loss: 0.00001284
Iteration 17/1000 | Loss: 0.00001283
Iteration 18/1000 | Loss: 0.00001283
Iteration 19/1000 | Loss: 0.00001282
Iteration 20/1000 | Loss: 0.00001280
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001278
Iteration 23/1000 | Loss: 0.00001278
Iteration 24/1000 | Loss: 0.00001278
Iteration 25/1000 | Loss: 0.00001273
Iteration 26/1000 | Loss: 0.00001273
Iteration 27/1000 | Loss: 0.00001273
Iteration 28/1000 | Loss: 0.00001273
Iteration 29/1000 | Loss: 0.00001271
Iteration 30/1000 | Loss: 0.00001270
Iteration 31/1000 | Loss: 0.00001269
Iteration 32/1000 | Loss: 0.00001268
Iteration 33/1000 | Loss: 0.00001267
Iteration 34/1000 | Loss: 0.00001267
Iteration 35/1000 | Loss: 0.00001263
Iteration 36/1000 | Loss: 0.00001262
Iteration 37/1000 | Loss: 0.00001261
Iteration 38/1000 | Loss: 0.00001261
Iteration 39/1000 | Loss: 0.00001261
Iteration 40/1000 | Loss: 0.00001259
Iteration 41/1000 | Loss: 0.00001259
Iteration 42/1000 | Loss: 0.00001258
Iteration 43/1000 | Loss: 0.00001255
Iteration 44/1000 | Loss: 0.00001255
Iteration 45/1000 | Loss: 0.00001255
Iteration 46/1000 | Loss: 0.00001255
Iteration 47/1000 | Loss: 0.00001255
Iteration 48/1000 | Loss: 0.00001254
Iteration 49/1000 | Loss: 0.00001254
Iteration 50/1000 | Loss: 0.00001254
Iteration 51/1000 | Loss: 0.00001251
Iteration 52/1000 | Loss: 0.00001251
Iteration 53/1000 | Loss: 0.00001250
Iteration 54/1000 | Loss: 0.00001250
Iteration 55/1000 | Loss: 0.00001249
Iteration 56/1000 | Loss: 0.00001249
Iteration 57/1000 | Loss: 0.00001248
Iteration 58/1000 | Loss: 0.00001247
Iteration 59/1000 | Loss: 0.00001247
Iteration 60/1000 | Loss: 0.00001245
Iteration 61/1000 | Loss: 0.00001245
Iteration 62/1000 | Loss: 0.00001245
Iteration 63/1000 | Loss: 0.00001244
Iteration 64/1000 | Loss: 0.00001244
Iteration 65/1000 | Loss: 0.00001243
Iteration 66/1000 | Loss: 0.00001243
Iteration 67/1000 | Loss: 0.00001242
Iteration 68/1000 | Loss: 0.00001242
Iteration 69/1000 | Loss: 0.00001241
Iteration 70/1000 | Loss: 0.00001241
Iteration 71/1000 | Loss: 0.00001240
Iteration 72/1000 | Loss: 0.00001240
Iteration 73/1000 | Loss: 0.00001237
Iteration 74/1000 | Loss: 0.00001237
Iteration 75/1000 | Loss: 0.00001237
Iteration 76/1000 | Loss: 0.00001236
Iteration 77/1000 | Loss: 0.00001236
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001235
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001235
Iteration 82/1000 | Loss: 0.00001230
Iteration 83/1000 | Loss: 0.00001230
Iteration 84/1000 | Loss: 0.00001228
Iteration 85/1000 | Loss: 0.00001227
Iteration 86/1000 | Loss: 0.00001227
Iteration 87/1000 | Loss: 0.00001226
Iteration 88/1000 | Loss: 0.00001226
Iteration 89/1000 | Loss: 0.00001225
Iteration 90/1000 | Loss: 0.00001225
Iteration 91/1000 | Loss: 0.00001225
Iteration 92/1000 | Loss: 0.00001224
Iteration 93/1000 | Loss: 0.00001224
Iteration 94/1000 | Loss: 0.00001224
Iteration 95/1000 | Loss: 0.00001224
Iteration 96/1000 | Loss: 0.00001224
Iteration 97/1000 | Loss: 0.00001224
Iteration 98/1000 | Loss: 0.00001223
Iteration 99/1000 | Loss: 0.00001223
Iteration 100/1000 | Loss: 0.00001223
Iteration 101/1000 | Loss: 0.00001223
Iteration 102/1000 | Loss: 0.00001223
Iteration 103/1000 | Loss: 0.00001222
Iteration 104/1000 | Loss: 0.00001222
Iteration 105/1000 | Loss: 0.00001222
Iteration 106/1000 | Loss: 0.00001221
Iteration 107/1000 | Loss: 0.00001221
Iteration 108/1000 | Loss: 0.00001220
Iteration 109/1000 | Loss: 0.00001220
Iteration 110/1000 | Loss: 0.00001220
Iteration 111/1000 | Loss: 0.00001220
Iteration 112/1000 | Loss: 0.00001219
Iteration 113/1000 | Loss: 0.00001219
Iteration 114/1000 | Loss: 0.00001219
Iteration 115/1000 | Loss: 0.00001219
Iteration 116/1000 | Loss: 0.00001219
Iteration 117/1000 | Loss: 0.00001218
Iteration 118/1000 | Loss: 0.00001218
Iteration 119/1000 | Loss: 0.00001217
Iteration 120/1000 | Loss: 0.00001217
Iteration 121/1000 | Loss: 0.00001217
Iteration 122/1000 | Loss: 0.00001217
Iteration 123/1000 | Loss: 0.00001216
Iteration 124/1000 | Loss: 0.00001216
Iteration 125/1000 | Loss: 0.00001216
Iteration 126/1000 | Loss: 0.00001216
Iteration 127/1000 | Loss: 0.00001216
Iteration 128/1000 | Loss: 0.00001216
Iteration 129/1000 | Loss: 0.00001216
Iteration 130/1000 | Loss: 0.00001216
Iteration 131/1000 | Loss: 0.00001216
Iteration 132/1000 | Loss: 0.00001215
Iteration 133/1000 | Loss: 0.00001215
Iteration 134/1000 | Loss: 0.00001214
Iteration 135/1000 | Loss: 0.00001214
Iteration 136/1000 | Loss: 0.00001214
Iteration 137/1000 | Loss: 0.00001213
Iteration 138/1000 | Loss: 0.00001213
Iteration 139/1000 | Loss: 0.00001213
Iteration 140/1000 | Loss: 0.00001213
Iteration 141/1000 | Loss: 0.00001213
Iteration 142/1000 | Loss: 0.00001213
Iteration 143/1000 | Loss: 0.00001213
Iteration 144/1000 | Loss: 0.00001213
Iteration 145/1000 | Loss: 0.00001213
Iteration 146/1000 | Loss: 0.00001213
Iteration 147/1000 | Loss: 0.00001213
Iteration 148/1000 | Loss: 0.00001213
Iteration 149/1000 | Loss: 0.00001213
Iteration 150/1000 | Loss: 0.00001212
Iteration 151/1000 | Loss: 0.00001212
Iteration 152/1000 | Loss: 0.00001212
Iteration 153/1000 | Loss: 0.00001212
Iteration 154/1000 | Loss: 0.00001212
Iteration 155/1000 | Loss: 0.00001212
Iteration 156/1000 | Loss: 0.00001212
Iteration 157/1000 | Loss: 0.00001211
Iteration 158/1000 | Loss: 0.00001211
Iteration 159/1000 | Loss: 0.00001211
Iteration 160/1000 | Loss: 0.00001210
Iteration 161/1000 | Loss: 0.00001210
Iteration 162/1000 | Loss: 0.00001210
Iteration 163/1000 | Loss: 0.00001210
Iteration 164/1000 | Loss: 0.00001210
Iteration 165/1000 | Loss: 0.00001210
Iteration 166/1000 | Loss: 0.00001210
Iteration 167/1000 | Loss: 0.00001210
Iteration 168/1000 | Loss: 0.00001210
Iteration 169/1000 | Loss: 0.00001209
Iteration 170/1000 | Loss: 0.00001209
Iteration 171/1000 | Loss: 0.00001209
Iteration 172/1000 | Loss: 0.00001209
Iteration 173/1000 | Loss: 0.00001209
Iteration 174/1000 | Loss: 0.00001208
Iteration 175/1000 | Loss: 0.00001208
Iteration 176/1000 | Loss: 0.00001208
Iteration 177/1000 | Loss: 0.00001208
Iteration 178/1000 | Loss: 0.00001207
Iteration 179/1000 | Loss: 0.00001207
Iteration 180/1000 | Loss: 0.00001207
Iteration 181/1000 | Loss: 0.00001207
Iteration 182/1000 | Loss: 0.00001207
Iteration 183/1000 | Loss: 0.00001207
Iteration 184/1000 | Loss: 0.00001207
Iteration 185/1000 | Loss: 0.00001207
Iteration 186/1000 | Loss: 0.00001207
Iteration 187/1000 | Loss: 0.00001206
Iteration 188/1000 | Loss: 0.00001206
Iteration 189/1000 | Loss: 0.00001206
Iteration 190/1000 | Loss: 0.00001206
Iteration 191/1000 | Loss: 0.00001206
Iteration 192/1000 | Loss: 0.00001206
Iteration 193/1000 | Loss: 0.00001206
Iteration 194/1000 | Loss: 0.00001206
Iteration 195/1000 | Loss: 0.00001206
Iteration 196/1000 | Loss: 0.00001206
Iteration 197/1000 | Loss: 0.00001206
Iteration 198/1000 | Loss: 0.00001206
Iteration 199/1000 | Loss: 0.00001206
Iteration 200/1000 | Loss: 0.00001206
Iteration 201/1000 | Loss: 0.00001205
Iteration 202/1000 | Loss: 0.00001205
Iteration 203/1000 | Loss: 0.00001205
Iteration 204/1000 | Loss: 0.00001205
Iteration 205/1000 | Loss: 0.00001205
Iteration 206/1000 | Loss: 0.00001205
Iteration 207/1000 | Loss: 0.00001205
Iteration 208/1000 | Loss: 0.00001205
Iteration 209/1000 | Loss: 0.00001205
Iteration 210/1000 | Loss: 0.00001205
Iteration 211/1000 | Loss: 0.00001205
Iteration 212/1000 | Loss: 0.00001205
Iteration 213/1000 | Loss: 0.00001205
Iteration 214/1000 | Loss: 0.00001205
Iteration 215/1000 | Loss: 0.00001204
Iteration 216/1000 | Loss: 0.00001204
Iteration 217/1000 | Loss: 0.00001204
Iteration 218/1000 | Loss: 0.00001204
Iteration 219/1000 | Loss: 0.00001204
Iteration 220/1000 | Loss: 0.00001204
Iteration 221/1000 | Loss: 0.00001204
Iteration 222/1000 | Loss: 0.00001204
Iteration 223/1000 | Loss: 0.00001204
Iteration 224/1000 | Loss: 0.00001204
Iteration 225/1000 | Loss: 0.00001204
Iteration 226/1000 | Loss: 0.00001204
Iteration 227/1000 | Loss: 0.00001204
Iteration 228/1000 | Loss: 0.00001204
Iteration 229/1000 | Loss: 0.00001204
Iteration 230/1000 | Loss: 0.00001204
Iteration 231/1000 | Loss: 0.00001203
Iteration 232/1000 | Loss: 0.00001203
Iteration 233/1000 | Loss: 0.00001203
Iteration 234/1000 | Loss: 0.00001203
Iteration 235/1000 | Loss: 0.00001203
Iteration 236/1000 | Loss: 0.00001203
Iteration 237/1000 | Loss: 0.00001203
Iteration 238/1000 | Loss: 0.00001203
Iteration 239/1000 | Loss: 0.00001203
Iteration 240/1000 | Loss: 0.00001203
Iteration 241/1000 | Loss: 0.00001203
Iteration 242/1000 | Loss: 0.00001203
Iteration 243/1000 | Loss: 0.00001203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.2029653589706868e-05, 1.2029653589706868e-05, 1.2029653589706868e-05, 1.2029653589706868e-05, 1.2029653589706868e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2029653589706868e-05

Optimization complete. Final v2v error: 2.982398509979248 mm

Highest mean error: 3.3217334747314453 mm for frame 145

Lowest mean error: 2.8115174770355225 mm for frame 63

Saving results

Total time: 51.233057737350464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825046
Iteration 2/25 | Loss: 0.00171247
Iteration 3/25 | Loss: 0.00140623
Iteration 4/25 | Loss: 0.00136924
Iteration 5/25 | Loss: 0.00136004
Iteration 6/25 | Loss: 0.00135824
Iteration 7/25 | Loss: 0.00135824
Iteration 8/25 | Loss: 0.00135824
Iteration 9/25 | Loss: 0.00135824
Iteration 10/25 | Loss: 0.00135824
Iteration 11/25 | Loss: 0.00135824
Iteration 12/25 | Loss: 0.00135824
Iteration 13/25 | Loss: 0.00135824
Iteration 14/25 | Loss: 0.00135824
Iteration 15/25 | Loss: 0.00135824
Iteration 16/25 | Loss: 0.00135824
Iteration 17/25 | Loss: 0.00135824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013582431711256504, 0.0013582431711256504, 0.0013582431711256504, 0.0013582431711256504, 0.0013582431711256504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013582431711256504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.70445085
Iteration 2/25 | Loss: 0.00084736
Iteration 3/25 | Loss: 0.00084735
Iteration 4/25 | Loss: 0.00084735
Iteration 5/25 | Loss: 0.00084735
Iteration 6/25 | Loss: 0.00084735
Iteration 7/25 | Loss: 0.00084735
Iteration 8/25 | Loss: 0.00084735
Iteration 9/25 | Loss: 0.00084735
Iteration 10/25 | Loss: 0.00084735
Iteration 11/25 | Loss: 0.00084735
Iteration 12/25 | Loss: 0.00084735
Iteration 13/25 | Loss: 0.00084735
Iteration 14/25 | Loss: 0.00084735
Iteration 15/25 | Loss: 0.00084735
Iteration 16/25 | Loss: 0.00084735
Iteration 17/25 | Loss: 0.00084735
Iteration 18/25 | Loss: 0.00084735
Iteration 19/25 | Loss: 0.00084735
Iteration 20/25 | Loss: 0.00084735
Iteration 21/25 | Loss: 0.00084735
Iteration 22/25 | Loss: 0.00084735
Iteration 23/25 | Loss: 0.00084735
Iteration 24/25 | Loss: 0.00084735
Iteration 25/25 | Loss: 0.00084735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084735
Iteration 2/1000 | Loss: 0.00003822
Iteration 3/1000 | Loss: 0.00002986
Iteration 4/1000 | Loss: 0.00002672
Iteration 5/1000 | Loss: 0.00002555
Iteration 6/1000 | Loss: 0.00002464
Iteration 7/1000 | Loss: 0.00002412
Iteration 8/1000 | Loss: 0.00002360
Iteration 9/1000 | Loss: 0.00002319
Iteration 10/1000 | Loss: 0.00002289
Iteration 11/1000 | Loss: 0.00002263
Iteration 12/1000 | Loss: 0.00002247
Iteration 13/1000 | Loss: 0.00002231
Iteration 14/1000 | Loss: 0.00002212
Iteration 15/1000 | Loss: 0.00002201
Iteration 16/1000 | Loss: 0.00002199
Iteration 17/1000 | Loss: 0.00002199
Iteration 18/1000 | Loss: 0.00002199
Iteration 19/1000 | Loss: 0.00002199
Iteration 20/1000 | Loss: 0.00002199
Iteration 21/1000 | Loss: 0.00002199
Iteration 22/1000 | Loss: 0.00002198
Iteration 23/1000 | Loss: 0.00002197
Iteration 24/1000 | Loss: 0.00002197
Iteration 25/1000 | Loss: 0.00002196
Iteration 26/1000 | Loss: 0.00002196
Iteration 27/1000 | Loss: 0.00002195
Iteration 28/1000 | Loss: 0.00002195
Iteration 29/1000 | Loss: 0.00002194
Iteration 30/1000 | Loss: 0.00002194
Iteration 31/1000 | Loss: 0.00002194
Iteration 32/1000 | Loss: 0.00002194
Iteration 33/1000 | Loss: 0.00002193
Iteration 34/1000 | Loss: 0.00002193
Iteration 35/1000 | Loss: 0.00002193
Iteration 36/1000 | Loss: 0.00002193
Iteration 37/1000 | Loss: 0.00002192
Iteration 38/1000 | Loss: 0.00002191
Iteration 39/1000 | Loss: 0.00002190
Iteration 40/1000 | Loss: 0.00002190
Iteration 41/1000 | Loss: 0.00002190
Iteration 42/1000 | Loss: 0.00002190
Iteration 43/1000 | Loss: 0.00002190
Iteration 44/1000 | Loss: 0.00002190
Iteration 45/1000 | Loss: 0.00002190
Iteration 46/1000 | Loss: 0.00002190
Iteration 47/1000 | Loss: 0.00002190
Iteration 48/1000 | Loss: 0.00002190
Iteration 49/1000 | Loss: 0.00002190
Iteration 50/1000 | Loss: 0.00002189
Iteration 51/1000 | Loss: 0.00002189
Iteration 52/1000 | Loss: 0.00002188
Iteration 53/1000 | Loss: 0.00002187
Iteration 54/1000 | Loss: 0.00002187
Iteration 55/1000 | Loss: 0.00002187
Iteration 56/1000 | Loss: 0.00002186
Iteration 57/1000 | Loss: 0.00002186
Iteration 58/1000 | Loss: 0.00002186
Iteration 59/1000 | Loss: 0.00002186
Iteration 60/1000 | Loss: 0.00002186
Iteration 61/1000 | Loss: 0.00002186
Iteration 62/1000 | Loss: 0.00002186
Iteration 63/1000 | Loss: 0.00002186
Iteration 64/1000 | Loss: 0.00002185
Iteration 65/1000 | Loss: 0.00002185
Iteration 66/1000 | Loss: 0.00002185
Iteration 67/1000 | Loss: 0.00002184
Iteration 68/1000 | Loss: 0.00002184
Iteration 69/1000 | Loss: 0.00002184
Iteration 70/1000 | Loss: 0.00002184
Iteration 71/1000 | Loss: 0.00002183
Iteration 72/1000 | Loss: 0.00002183
Iteration 73/1000 | Loss: 0.00002183
Iteration 74/1000 | Loss: 0.00002183
Iteration 75/1000 | Loss: 0.00002182
Iteration 76/1000 | Loss: 0.00002182
Iteration 77/1000 | Loss: 0.00002182
Iteration 78/1000 | Loss: 0.00002182
Iteration 79/1000 | Loss: 0.00002182
Iteration 80/1000 | Loss: 0.00002182
Iteration 81/1000 | Loss: 0.00002182
Iteration 82/1000 | Loss: 0.00002182
Iteration 83/1000 | Loss: 0.00002182
Iteration 84/1000 | Loss: 0.00002180
Iteration 85/1000 | Loss: 0.00002180
Iteration 86/1000 | Loss: 0.00002180
Iteration 87/1000 | Loss: 0.00002180
Iteration 88/1000 | Loss: 0.00002180
Iteration 89/1000 | Loss: 0.00002180
Iteration 90/1000 | Loss: 0.00002179
Iteration 91/1000 | Loss: 0.00002179
Iteration 92/1000 | Loss: 0.00002179
Iteration 93/1000 | Loss: 0.00002179
Iteration 94/1000 | Loss: 0.00002179
Iteration 95/1000 | Loss: 0.00002179
Iteration 96/1000 | Loss: 0.00002179
Iteration 97/1000 | Loss: 0.00002178
Iteration 98/1000 | Loss: 0.00002178
Iteration 99/1000 | Loss: 0.00002178
Iteration 100/1000 | Loss: 0.00002178
Iteration 101/1000 | Loss: 0.00002177
Iteration 102/1000 | Loss: 0.00002176
Iteration 103/1000 | Loss: 0.00002176
Iteration 104/1000 | Loss: 0.00002175
Iteration 105/1000 | Loss: 0.00002175
Iteration 106/1000 | Loss: 0.00002175
Iteration 107/1000 | Loss: 0.00002175
Iteration 108/1000 | Loss: 0.00002175
Iteration 109/1000 | Loss: 0.00002175
Iteration 110/1000 | Loss: 0.00002175
Iteration 111/1000 | Loss: 0.00002175
Iteration 112/1000 | Loss: 0.00002175
Iteration 113/1000 | Loss: 0.00002174
Iteration 114/1000 | Loss: 0.00002174
Iteration 115/1000 | Loss: 0.00002173
Iteration 116/1000 | Loss: 0.00002173
Iteration 117/1000 | Loss: 0.00002172
Iteration 118/1000 | Loss: 0.00002171
Iteration 119/1000 | Loss: 0.00002171
Iteration 120/1000 | Loss: 0.00002171
Iteration 121/1000 | Loss: 0.00002170
Iteration 122/1000 | Loss: 0.00002170
Iteration 123/1000 | Loss: 0.00002170
Iteration 124/1000 | Loss: 0.00002169
Iteration 125/1000 | Loss: 0.00002169
Iteration 126/1000 | Loss: 0.00002168
Iteration 127/1000 | Loss: 0.00002168
Iteration 128/1000 | Loss: 0.00002168
Iteration 129/1000 | Loss: 0.00002168
Iteration 130/1000 | Loss: 0.00002168
Iteration 131/1000 | Loss: 0.00002168
Iteration 132/1000 | Loss: 0.00002167
Iteration 133/1000 | Loss: 0.00002167
Iteration 134/1000 | Loss: 0.00002167
Iteration 135/1000 | Loss: 0.00002167
Iteration 136/1000 | Loss: 0.00002167
Iteration 137/1000 | Loss: 0.00002167
Iteration 138/1000 | Loss: 0.00002167
Iteration 139/1000 | Loss: 0.00002167
Iteration 140/1000 | Loss: 0.00002167
Iteration 141/1000 | Loss: 0.00002167
Iteration 142/1000 | Loss: 0.00002167
Iteration 143/1000 | Loss: 0.00002167
Iteration 144/1000 | Loss: 0.00002167
Iteration 145/1000 | Loss: 0.00002166
Iteration 146/1000 | Loss: 0.00002166
Iteration 147/1000 | Loss: 0.00002166
Iteration 148/1000 | Loss: 0.00002166
Iteration 149/1000 | Loss: 0.00002165
Iteration 150/1000 | Loss: 0.00002165
Iteration 151/1000 | Loss: 0.00002165
Iteration 152/1000 | Loss: 0.00002165
Iteration 153/1000 | Loss: 0.00002165
Iteration 154/1000 | Loss: 0.00002165
Iteration 155/1000 | Loss: 0.00002164
Iteration 156/1000 | Loss: 0.00002164
Iteration 157/1000 | Loss: 0.00002164
Iteration 158/1000 | Loss: 0.00002164
Iteration 159/1000 | Loss: 0.00002164
Iteration 160/1000 | Loss: 0.00002164
Iteration 161/1000 | Loss: 0.00002164
Iteration 162/1000 | Loss: 0.00002164
Iteration 163/1000 | Loss: 0.00002164
Iteration 164/1000 | Loss: 0.00002164
Iteration 165/1000 | Loss: 0.00002164
Iteration 166/1000 | Loss: 0.00002164
Iteration 167/1000 | Loss: 0.00002163
Iteration 168/1000 | Loss: 0.00002163
Iteration 169/1000 | Loss: 0.00002163
Iteration 170/1000 | Loss: 0.00002163
Iteration 171/1000 | Loss: 0.00002163
Iteration 172/1000 | Loss: 0.00002163
Iteration 173/1000 | Loss: 0.00002163
Iteration 174/1000 | Loss: 0.00002162
Iteration 175/1000 | Loss: 0.00002162
Iteration 176/1000 | Loss: 0.00002162
Iteration 177/1000 | Loss: 0.00002162
Iteration 178/1000 | Loss: 0.00002162
Iteration 179/1000 | Loss: 0.00002162
Iteration 180/1000 | Loss: 0.00002162
Iteration 181/1000 | Loss: 0.00002162
Iteration 182/1000 | Loss: 0.00002162
Iteration 183/1000 | Loss: 0.00002162
Iteration 184/1000 | Loss: 0.00002162
Iteration 185/1000 | Loss: 0.00002162
Iteration 186/1000 | Loss: 0.00002162
Iteration 187/1000 | Loss: 0.00002162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.1622696294798516e-05, 2.1622696294798516e-05, 2.1622696294798516e-05, 2.1622696294798516e-05, 2.1622696294798516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1622696294798516e-05

Optimization complete. Final v2v error: 3.874704360961914 mm

Highest mean error: 4.242624759674072 mm for frame 93

Lowest mean error: 3.465763807296753 mm for frame 137

Saving results

Total time: 42.30039358139038
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990679
Iteration 2/25 | Loss: 0.00284426
Iteration 3/25 | Loss: 0.00227371
Iteration 4/25 | Loss: 0.00203760
Iteration 5/25 | Loss: 0.00195540
Iteration 6/25 | Loss: 0.00182487
Iteration 7/25 | Loss: 0.00168139
Iteration 8/25 | Loss: 0.00161383
Iteration 9/25 | Loss: 0.00157647
Iteration 10/25 | Loss: 0.00154848
Iteration 11/25 | Loss: 0.00154716
Iteration 12/25 | Loss: 0.00154142
Iteration 13/25 | Loss: 0.00152878
Iteration 14/25 | Loss: 0.00152851
Iteration 15/25 | Loss: 0.00152086
Iteration 16/25 | Loss: 0.00151030
Iteration 17/25 | Loss: 0.00151009
Iteration 18/25 | Loss: 0.00150954
Iteration 19/25 | Loss: 0.00151533
Iteration 20/25 | Loss: 0.00150246
Iteration 21/25 | Loss: 0.00149946
Iteration 22/25 | Loss: 0.00149861
Iteration 23/25 | Loss: 0.00149990
Iteration 24/25 | Loss: 0.00149813
Iteration 25/25 | Loss: 0.00149954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58506465
Iteration 2/25 | Loss: 0.00259813
Iteration 3/25 | Loss: 0.00202247
Iteration 4/25 | Loss: 0.00198746
Iteration 5/25 | Loss: 0.00198746
Iteration 6/25 | Loss: 0.00198745
Iteration 7/25 | Loss: 0.00198745
Iteration 8/25 | Loss: 0.00198745
Iteration 9/25 | Loss: 0.00198745
Iteration 10/25 | Loss: 0.00198745
Iteration 11/25 | Loss: 0.00198745
Iteration 12/25 | Loss: 0.00198745
Iteration 13/25 | Loss: 0.00198745
Iteration 14/25 | Loss: 0.00198745
Iteration 15/25 | Loss: 0.00198745
Iteration 16/25 | Loss: 0.00198745
Iteration 17/25 | Loss: 0.00198745
Iteration 18/25 | Loss: 0.00198745
Iteration 19/25 | Loss: 0.00198745
Iteration 20/25 | Loss: 0.00198745
Iteration 21/25 | Loss: 0.00198745
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001987452618777752, 0.001987452618777752, 0.001987452618777752, 0.001987452618777752, 0.001987452618777752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001987452618777752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198745
Iteration 2/1000 | Loss: 0.00149368
Iteration 3/1000 | Loss: 0.00195626
Iteration 4/1000 | Loss: 0.00087942
Iteration 5/1000 | Loss: 0.00071898
Iteration 6/1000 | Loss: 0.00032634
Iteration 7/1000 | Loss: 0.00023822
Iteration 8/1000 | Loss: 0.00145723
Iteration 9/1000 | Loss: 0.00015955
Iteration 10/1000 | Loss: 0.00058273
Iteration 11/1000 | Loss: 0.00026275
Iteration 12/1000 | Loss: 0.00082511
Iteration 13/1000 | Loss: 0.00015915
Iteration 14/1000 | Loss: 0.00051477
Iteration 15/1000 | Loss: 0.00020649
Iteration 16/1000 | Loss: 0.00055507
Iteration 17/1000 | Loss: 0.00030128
Iteration 18/1000 | Loss: 0.00033250
Iteration 19/1000 | Loss: 0.00016939
Iteration 20/1000 | Loss: 0.00017677
Iteration 21/1000 | Loss: 0.00062782
Iteration 22/1000 | Loss: 0.00017132
Iteration 23/1000 | Loss: 0.00016433
Iteration 24/1000 | Loss: 0.00010688
Iteration 25/1000 | Loss: 0.00011573
Iteration 26/1000 | Loss: 0.00016893
Iteration 27/1000 | Loss: 0.00009665
Iteration 28/1000 | Loss: 0.00014867
Iteration 29/1000 | Loss: 0.00011975
Iteration 30/1000 | Loss: 0.00012583
Iteration 31/1000 | Loss: 0.00028965
Iteration 32/1000 | Loss: 0.00083915
Iteration 33/1000 | Loss: 0.00018389
Iteration 34/1000 | Loss: 0.00007737
Iteration 35/1000 | Loss: 0.00010217
Iteration 36/1000 | Loss: 0.00010448
Iteration 37/1000 | Loss: 0.00007118
Iteration 38/1000 | Loss: 0.00010652
Iteration 39/1000 | Loss: 0.00022545
Iteration 40/1000 | Loss: 0.00033048
Iteration 41/1000 | Loss: 0.00031259
Iteration 42/1000 | Loss: 0.00033317
Iteration 43/1000 | Loss: 0.00011345
Iteration 44/1000 | Loss: 0.00057398
Iteration 45/1000 | Loss: 0.00081898
Iteration 46/1000 | Loss: 0.00251516
Iteration 47/1000 | Loss: 0.00429607
Iteration 48/1000 | Loss: 0.00067654
Iteration 49/1000 | Loss: 0.00046746
Iteration 50/1000 | Loss: 0.00024662
Iteration 51/1000 | Loss: 0.00009978
Iteration 52/1000 | Loss: 0.00008605
Iteration 53/1000 | Loss: 0.00034876
Iteration 54/1000 | Loss: 0.00008837
Iteration 55/1000 | Loss: 0.00008473
Iteration 56/1000 | Loss: 0.00006198
Iteration 57/1000 | Loss: 0.00006991
Iteration 58/1000 | Loss: 0.00004509
Iteration 59/1000 | Loss: 0.00005213
Iteration 60/1000 | Loss: 0.00004622
Iteration 61/1000 | Loss: 0.00012326
Iteration 62/1000 | Loss: 0.00004158
Iteration 63/1000 | Loss: 0.00057992
Iteration 64/1000 | Loss: 0.00051365
Iteration 65/1000 | Loss: 0.00010036
Iteration 66/1000 | Loss: 0.00008507
Iteration 67/1000 | Loss: 0.00008103
Iteration 68/1000 | Loss: 0.00004049
Iteration 69/1000 | Loss: 0.00007162
Iteration 70/1000 | Loss: 0.00007549
Iteration 71/1000 | Loss: 0.00003911
Iteration 72/1000 | Loss: 0.00009072
Iteration 73/1000 | Loss: 0.00003696
Iteration 74/1000 | Loss: 0.00003546
Iteration 75/1000 | Loss: 0.00003455
Iteration 76/1000 | Loss: 0.00003405
Iteration 77/1000 | Loss: 0.00013250
Iteration 78/1000 | Loss: 0.00015500
Iteration 79/1000 | Loss: 0.00009314
Iteration 80/1000 | Loss: 0.00003358
Iteration 81/1000 | Loss: 0.00005940
Iteration 82/1000 | Loss: 0.00009537
Iteration 83/1000 | Loss: 0.00004882
Iteration 84/1000 | Loss: 0.00003302
Iteration 85/1000 | Loss: 0.00003283
Iteration 86/1000 | Loss: 0.00003283
Iteration 87/1000 | Loss: 0.00010412
Iteration 88/1000 | Loss: 0.00003268
Iteration 89/1000 | Loss: 0.00004744
Iteration 90/1000 | Loss: 0.00003238
Iteration 91/1000 | Loss: 0.00003224
Iteration 92/1000 | Loss: 0.00003219
Iteration 93/1000 | Loss: 0.00005040
Iteration 94/1000 | Loss: 0.00003198
Iteration 95/1000 | Loss: 0.00003196
Iteration 96/1000 | Loss: 0.00003920
Iteration 97/1000 | Loss: 0.00003186
Iteration 98/1000 | Loss: 0.00003871
Iteration 99/1000 | Loss: 0.00003451
Iteration 100/1000 | Loss: 0.00003752
Iteration 101/1000 | Loss: 0.00003148
Iteration 102/1000 | Loss: 0.00006830
Iteration 103/1000 | Loss: 0.00003144
Iteration 104/1000 | Loss: 0.00008076
Iteration 105/1000 | Loss: 0.00004155
Iteration 106/1000 | Loss: 0.00003130
Iteration 107/1000 | Loss: 0.00003124
Iteration 108/1000 | Loss: 0.00003124
Iteration 109/1000 | Loss: 0.00003213
Iteration 110/1000 | Loss: 0.00003118
Iteration 111/1000 | Loss: 0.00003110
Iteration 112/1000 | Loss: 0.00003105
Iteration 113/1000 | Loss: 0.00003103
Iteration 114/1000 | Loss: 0.00003102
Iteration 115/1000 | Loss: 0.00003101
Iteration 116/1000 | Loss: 0.00003100
Iteration 117/1000 | Loss: 0.00003099
Iteration 118/1000 | Loss: 0.00003099
Iteration 119/1000 | Loss: 0.00003099
Iteration 120/1000 | Loss: 0.00003098
Iteration 121/1000 | Loss: 0.00003494
Iteration 122/1000 | Loss: 0.00003210
Iteration 123/1000 | Loss: 0.00003317
Iteration 124/1000 | Loss: 0.00007826
Iteration 125/1000 | Loss: 0.00003737
Iteration 126/1000 | Loss: 0.00003088
Iteration 127/1000 | Loss: 0.00003084
Iteration 128/1000 | Loss: 0.00003084
Iteration 129/1000 | Loss: 0.00005696
Iteration 130/1000 | Loss: 0.00003087
Iteration 131/1000 | Loss: 0.00003083
Iteration 132/1000 | Loss: 0.00003079
Iteration 133/1000 | Loss: 0.00003078
Iteration 134/1000 | Loss: 0.00003078
Iteration 135/1000 | Loss: 0.00003077
Iteration 136/1000 | Loss: 0.00003077
Iteration 137/1000 | Loss: 0.00003077
Iteration 138/1000 | Loss: 0.00003076
Iteration 139/1000 | Loss: 0.00003076
Iteration 140/1000 | Loss: 0.00003076
Iteration 141/1000 | Loss: 0.00003075
Iteration 142/1000 | Loss: 0.00003075
Iteration 143/1000 | Loss: 0.00007054
Iteration 144/1000 | Loss: 0.00003076
Iteration 145/1000 | Loss: 0.00003075
Iteration 146/1000 | Loss: 0.00003074
Iteration 147/1000 | Loss: 0.00003074
Iteration 148/1000 | Loss: 0.00003074
Iteration 149/1000 | Loss: 0.00004518
Iteration 150/1000 | Loss: 0.00003076
Iteration 151/1000 | Loss: 0.00003074
Iteration 152/1000 | Loss: 0.00003074
Iteration 153/1000 | Loss: 0.00003073
Iteration 154/1000 | Loss: 0.00003073
Iteration 155/1000 | Loss: 0.00003073
Iteration 156/1000 | Loss: 0.00003073
Iteration 157/1000 | Loss: 0.00003073
Iteration 158/1000 | Loss: 0.00003073
Iteration 159/1000 | Loss: 0.00003073
Iteration 160/1000 | Loss: 0.00003072
Iteration 161/1000 | Loss: 0.00003072
Iteration 162/1000 | Loss: 0.00003072
Iteration 163/1000 | Loss: 0.00003071
Iteration 164/1000 | Loss: 0.00003070
Iteration 165/1000 | Loss: 0.00003070
Iteration 166/1000 | Loss: 0.00003070
Iteration 167/1000 | Loss: 0.00003070
Iteration 168/1000 | Loss: 0.00003070
Iteration 169/1000 | Loss: 0.00003070
Iteration 170/1000 | Loss: 0.00003069
Iteration 171/1000 | Loss: 0.00003069
Iteration 172/1000 | Loss: 0.00003069
Iteration 173/1000 | Loss: 0.00007532
Iteration 174/1000 | Loss: 0.00003077
Iteration 175/1000 | Loss: 0.00006220
Iteration 176/1000 | Loss: 0.00004972
Iteration 177/1000 | Loss: 0.00003783
Iteration 178/1000 | Loss: 0.00004309
Iteration 179/1000 | Loss: 0.00004414
Iteration 180/1000 | Loss: 0.00004320
Iteration 181/1000 | Loss: 0.00003143
Iteration 182/1000 | Loss: 0.00003070
Iteration 183/1000 | Loss: 0.00003070
Iteration 184/1000 | Loss: 0.00003070
Iteration 185/1000 | Loss: 0.00003069
Iteration 186/1000 | Loss: 0.00003069
Iteration 187/1000 | Loss: 0.00003069
Iteration 188/1000 | Loss: 0.00003069
Iteration 189/1000 | Loss: 0.00003068
Iteration 190/1000 | Loss: 0.00003068
Iteration 191/1000 | Loss: 0.00003068
Iteration 192/1000 | Loss: 0.00003068
Iteration 193/1000 | Loss: 0.00003068
Iteration 194/1000 | Loss: 0.00003068
Iteration 195/1000 | Loss: 0.00003068
Iteration 196/1000 | Loss: 0.00003068
Iteration 197/1000 | Loss: 0.00003068
Iteration 198/1000 | Loss: 0.00003068
Iteration 199/1000 | Loss: 0.00003067
Iteration 200/1000 | Loss: 0.00003067
Iteration 201/1000 | Loss: 0.00003067
Iteration 202/1000 | Loss: 0.00003067
Iteration 203/1000 | Loss: 0.00003067
Iteration 204/1000 | Loss: 0.00003067
Iteration 205/1000 | Loss: 0.00003067
Iteration 206/1000 | Loss: 0.00003067
Iteration 207/1000 | Loss: 0.00003067
Iteration 208/1000 | Loss: 0.00003066
Iteration 209/1000 | Loss: 0.00003066
Iteration 210/1000 | Loss: 0.00003066
Iteration 211/1000 | Loss: 0.00004468
Iteration 212/1000 | Loss: 0.00003067
Iteration 213/1000 | Loss: 0.00003065
Iteration 214/1000 | Loss: 0.00003065
Iteration 215/1000 | Loss: 0.00003064
Iteration 216/1000 | Loss: 0.00003064
Iteration 217/1000 | Loss: 0.00003064
Iteration 218/1000 | Loss: 0.00003064
Iteration 219/1000 | Loss: 0.00003063
Iteration 220/1000 | Loss: 0.00003063
Iteration 221/1000 | Loss: 0.00003063
Iteration 222/1000 | Loss: 0.00003063
Iteration 223/1000 | Loss: 0.00003063
Iteration 224/1000 | Loss: 0.00003063
Iteration 225/1000 | Loss: 0.00003063
Iteration 226/1000 | Loss: 0.00003063
Iteration 227/1000 | Loss: 0.00003063
Iteration 228/1000 | Loss: 0.00003063
Iteration 229/1000 | Loss: 0.00003063
Iteration 230/1000 | Loss: 0.00003063
Iteration 231/1000 | Loss: 0.00003063
Iteration 232/1000 | Loss: 0.00003063
Iteration 233/1000 | Loss: 0.00003063
Iteration 234/1000 | Loss: 0.00003063
Iteration 235/1000 | Loss: 0.00003063
Iteration 236/1000 | Loss: 0.00003063
Iteration 237/1000 | Loss: 0.00003063
Iteration 238/1000 | Loss: 0.00003063
Iteration 239/1000 | Loss: 0.00003063
Iteration 240/1000 | Loss: 0.00003063
Iteration 241/1000 | Loss: 0.00003063
Iteration 242/1000 | Loss: 0.00003063
Iteration 243/1000 | Loss: 0.00003063
Iteration 244/1000 | Loss: 0.00003063
Iteration 245/1000 | Loss: 0.00003063
Iteration 246/1000 | Loss: 0.00003063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [3.0628401873400435e-05, 3.0628401873400435e-05, 3.0628401873400435e-05, 3.0628401873400435e-05, 3.0628401873400435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0628401873400435e-05

Optimization complete. Final v2v error: 3.7681286334991455 mm

Highest mean error: 10.830273628234863 mm for frame 119

Lowest mean error: 2.9176385402679443 mm for frame 22

Saving results

Total time: 254.11947989463806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00547870
Iteration 2/25 | Loss: 0.00132243
Iteration 3/25 | Loss: 0.00127096
Iteration 4/25 | Loss: 0.00126402
Iteration 5/25 | Loss: 0.00126182
Iteration 6/25 | Loss: 0.00126181
Iteration 7/25 | Loss: 0.00126181
Iteration 8/25 | Loss: 0.00126181
Iteration 9/25 | Loss: 0.00126181
Iteration 10/25 | Loss: 0.00126181
Iteration 11/25 | Loss: 0.00126181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012618083273991942, 0.0012618083273991942, 0.0012618083273991942, 0.0012618083273991942, 0.0012618083273991942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012618083273991942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.22635627
Iteration 2/25 | Loss: 0.00084897
Iteration 3/25 | Loss: 0.00084897
Iteration 4/25 | Loss: 0.00084897
Iteration 5/25 | Loss: 0.00084896
Iteration 6/25 | Loss: 0.00084896
Iteration 7/25 | Loss: 0.00084896
Iteration 8/25 | Loss: 0.00084896
Iteration 9/25 | Loss: 0.00084896
Iteration 10/25 | Loss: 0.00084896
Iteration 11/25 | Loss: 0.00084896
Iteration 12/25 | Loss: 0.00084896
Iteration 13/25 | Loss: 0.00084896
Iteration 14/25 | Loss: 0.00084896
Iteration 15/25 | Loss: 0.00084896
Iteration 16/25 | Loss: 0.00084896
Iteration 17/25 | Loss: 0.00084896
Iteration 18/25 | Loss: 0.00084896
Iteration 19/25 | Loss: 0.00084896
Iteration 20/25 | Loss: 0.00084896
Iteration 21/25 | Loss: 0.00084896
Iteration 22/25 | Loss: 0.00084896
Iteration 23/25 | Loss: 0.00084896
Iteration 24/25 | Loss: 0.00084896
Iteration 25/25 | Loss: 0.00084896

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084896
Iteration 2/1000 | Loss: 0.00002699
Iteration 3/1000 | Loss: 0.00001854
Iteration 4/1000 | Loss: 0.00001564
Iteration 5/1000 | Loss: 0.00001456
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001346
Iteration 8/1000 | Loss: 0.00001320
Iteration 9/1000 | Loss: 0.00001295
Iteration 10/1000 | Loss: 0.00001274
Iteration 11/1000 | Loss: 0.00001270
Iteration 12/1000 | Loss: 0.00001261
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001258
Iteration 15/1000 | Loss: 0.00001257
Iteration 16/1000 | Loss: 0.00001257
Iteration 17/1000 | Loss: 0.00001243
Iteration 18/1000 | Loss: 0.00001242
Iteration 19/1000 | Loss: 0.00001232
Iteration 20/1000 | Loss: 0.00001228
Iteration 21/1000 | Loss: 0.00001228
Iteration 22/1000 | Loss: 0.00001227
Iteration 23/1000 | Loss: 0.00001227
Iteration 24/1000 | Loss: 0.00001226
Iteration 25/1000 | Loss: 0.00001224
Iteration 26/1000 | Loss: 0.00001222
Iteration 27/1000 | Loss: 0.00001221
Iteration 28/1000 | Loss: 0.00001221
Iteration 29/1000 | Loss: 0.00001221
Iteration 30/1000 | Loss: 0.00001220
Iteration 31/1000 | Loss: 0.00001220
Iteration 32/1000 | Loss: 0.00001219
Iteration 33/1000 | Loss: 0.00001219
Iteration 34/1000 | Loss: 0.00001218
Iteration 35/1000 | Loss: 0.00001218
Iteration 36/1000 | Loss: 0.00001217
Iteration 37/1000 | Loss: 0.00001217
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001217
Iteration 40/1000 | Loss: 0.00001217
Iteration 41/1000 | Loss: 0.00001217
Iteration 42/1000 | Loss: 0.00001217
Iteration 43/1000 | Loss: 0.00001217
Iteration 44/1000 | Loss: 0.00001217
Iteration 45/1000 | Loss: 0.00001216
Iteration 46/1000 | Loss: 0.00001215
Iteration 47/1000 | Loss: 0.00001215
Iteration 48/1000 | Loss: 0.00001214
Iteration 49/1000 | Loss: 0.00001214
Iteration 50/1000 | Loss: 0.00001214
Iteration 51/1000 | Loss: 0.00001214
Iteration 52/1000 | Loss: 0.00001212
Iteration 53/1000 | Loss: 0.00001211
Iteration 54/1000 | Loss: 0.00001210
Iteration 55/1000 | Loss: 0.00001210
Iteration 56/1000 | Loss: 0.00001209
Iteration 57/1000 | Loss: 0.00001205
Iteration 58/1000 | Loss: 0.00001205
Iteration 59/1000 | Loss: 0.00001204
Iteration 60/1000 | Loss: 0.00001203
Iteration 61/1000 | Loss: 0.00001203
Iteration 62/1000 | Loss: 0.00001201
Iteration 63/1000 | Loss: 0.00001201
Iteration 64/1000 | Loss: 0.00001200
Iteration 65/1000 | Loss: 0.00001200
Iteration 66/1000 | Loss: 0.00001200
Iteration 67/1000 | Loss: 0.00001200
Iteration 68/1000 | Loss: 0.00001200
Iteration 69/1000 | Loss: 0.00001200
Iteration 70/1000 | Loss: 0.00001200
Iteration 71/1000 | Loss: 0.00001199
Iteration 72/1000 | Loss: 0.00001199
Iteration 73/1000 | Loss: 0.00001198
Iteration 74/1000 | Loss: 0.00001198
Iteration 75/1000 | Loss: 0.00001198
Iteration 76/1000 | Loss: 0.00001198
Iteration 77/1000 | Loss: 0.00001197
Iteration 78/1000 | Loss: 0.00001197
Iteration 79/1000 | Loss: 0.00001197
Iteration 80/1000 | Loss: 0.00001197
Iteration 81/1000 | Loss: 0.00001196
Iteration 82/1000 | Loss: 0.00001196
Iteration 83/1000 | Loss: 0.00001195
Iteration 84/1000 | Loss: 0.00001195
Iteration 85/1000 | Loss: 0.00001193
Iteration 86/1000 | Loss: 0.00001193
Iteration 87/1000 | Loss: 0.00001193
Iteration 88/1000 | Loss: 0.00001193
Iteration 89/1000 | Loss: 0.00001193
Iteration 90/1000 | Loss: 0.00001193
Iteration 91/1000 | Loss: 0.00001193
Iteration 92/1000 | Loss: 0.00001193
Iteration 93/1000 | Loss: 0.00001193
Iteration 94/1000 | Loss: 0.00001193
Iteration 95/1000 | Loss: 0.00001193
Iteration 96/1000 | Loss: 0.00001193
Iteration 97/1000 | Loss: 0.00001192
Iteration 98/1000 | Loss: 0.00001192
Iteration 99/1000 | Loss: 0.00001192
Iteration 100/1000 | Loss: 0.00001192
Iteration 101/1000 | Loss: 0.00001191
Iteration 102/1000 | Loss: 0.00001191
Iteration 103/1000 | Loss: 0.00001191
Iteration 104/1000 | Loss: 0.00001191
Iteration 105/1000 | Loss: 0.00001190
Iteration 106/1000 | Loss: 0.00001190
Iteration 107/1000 | Loss: 0.00001190
Iteration 108/1000 | Loss: 0.00001190
Iteration 109/1000 | Loss: 0.00001189
Iteration 110/1000 | Loss: 0.00001189
Iteration 111/1000 | Loss: 0.00001189
Iteration 112/1000 | Loss: 0.00001188
Iteration 113/1000 | Loss: 0.00001188
Iteration 114/1000 | Loss: 0.00001188
Iteration 115/1000 | Loss: 0.00001187
Iteration 116/1000 | Loss: 0.00001187
Iteration 117/1000 | Loss: 0.00001187
Iteration 118/1000 | Loss: 0.00001186
Iteration 119/1000 | Loss: 0.00001186
Iteration 120/1000 | Loss: 0.00001186
Iteration 121/1000 | Loss: 0.00001185
Iteration 122/1000 | Loss: 0.00001185
Iteration 123/1000 | Loss: 0.00001185
Iteration 124/1000 | Loss: 0.00001185
Iteration 125/1000 | Loss: 0.00001185
Iteration 126/1000 | Loss: 0.00001185
Iteration 127/1000 | Loss: 0.00001184
Iteration 128/1000 | Loss: 0.00001184
Iteration 129/1000 | Loss: 0.00001184
Iteration 130/1000 | Loss: 0.00001184
Iteration 131/1000 | Loss: 0.00001184
Iteration 132/1000 | Loss: 0.00001184
Iteration 133/1000 | Loss: 0.00001184
Iteration 134/1000 | Loss: 0.00001184
Iteration 135/1000 | Loss: 0.00001184
Iteration 136/1000 | Loss: 0.00001184
Iteration 137/1000 | Loss: 0.00001183
Iteration 138/1000 | Loss: 0.00001183
Iteration 139/1000 | Loss: 0.00001183
Iteration 140/1000 | Loss: 0.00001183
Iteration 141/1000 | Loss: 0.00001183
Iteration 142/1000 | Loss: 0.00001183
Iteration 143/1000 | Loss: 0.00001183
Iteration 144/1000 | Loss: 0.00001183
Iteration 145/1000 | Loss: 0.00001183
Iteration 146/1000 | Loss: 0.00001183
Iteration 147/1000 | Loss: 0.00001183
Iteration 148/1000 | Loss: 0.00001183
Iteration 149/1000 | Loss: 0.00001183
Iteration 150/1000 | Loss: 0.00001183
Iteration 151/1000 | Loss: 0.00001183
Iteration 152/1000 | Loss: 0.00001183
Iteration 153/1000 | Loss: 0.00001183
Iteration 154/1000 | Loss: 0.00001183
Iteration 155/1000 | Loss: 0.00001183
Iteration 156/1000 | Loss: 0.00001183
Iteration 157/1000 | Loss: 0.00001183
Iteration 158/1000 | Loss: 0.00001183
Iteration 159/1000 | Loss: 0.00001183
Iteration 160/1000 | Loss: 0.00001183
Iteration 161/1000 | Loss: 0.00001183
Iteration 162/1000 | Loss: 0.00001183
Iteration 163/1000 | Loss: 0.00001183
Iteration 164/1000 | Loss: 0.00001183
Iteration 165/1000 | Loss: 0.00001183
Iteration 166/1000 | Loss: 0.00001183
Iteration 167/1000 | Loss: 0.00001183
Iteration 168/1000 | Loss: 0.00001183
Iteration 169/1000 | Loss: 0.00001183
Iteration 170/1000 | Loss: 0.00001183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.1832204108941369e-05, 1.1832204108941369e-05, 1.1832204108941369e-05, 1.1832204108941369e-05, 1.1832204108941369e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1832204108941369e-05

Optimization complete. Final v2v error: 2.946157932281494 mm

Highest mean error: 3.1837973594665527 mm for frame 76

Lowest mean error: 2.8429415225982666 mm for frame 141

Saving results

Total time: 37.01833367347717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00552100
Iteration 2/25 | Loss: 0.00148306
Iteration 3/25 | Loss: 0.00138295
Iteration 4/25 | Loss: 0.00136590
Iteration 5/25 | Loss: 0.00136079
Iteration 6/25 | Loss: 0.00136022
Iteration 7/25 | Loss: 0.00136022
Iteration 8/25 | Loss: 0.00136022
Iteration 9/25 | Loss: 0.00136022
Iteration 10/25 | Loss: 0.00136022
Iteration 11/25 | Loss: 0.00136022
Iteration 12/25 | Loss: 0.00136022
Iteration 13/25 | Loss: 0.00136022
Iteration 14/25 | Loss: 0.00136022
Iteration 15/25 | Loss: 0.00136022
Iteration 16/25 | Loss: 0.00136022
Iteration 17/25 | Loss: 0.00136022
Iteration 18/25 | Loss: 0.00136022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013602154795080423, 0.0013602154795080423, 0.0013602154795080423, 0.0013602154795080423, 0.0013602154795080423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013602154795080423

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36347079
Iteration 2/25 | Loss: 0.00111361
Iteration 3/25 | Loss: 0.00111361
Iteration 4/25 | Loss: 0.00111361
Iteration 5/25 | Loss: 0.00111361
Iteration 6/25 | Loss: 0.00111361
Iteration 7/25 | Loss: 0.00111361
Iteration 8/25 | Loss: 0.00111361
Iteration 9/25 | Loss: 0.00111360
Iteration 10/25 | Loss: 0.00111360
Iteration 11/25 | Loss: 0.00111361
Iteration 12/25 | Loss: 0.00111361
Iteration 13/25 | Loss: 0.00111360
Iteration 14/25 | Loss: 0.00111360
Iteration 15/25 | Loss: 0.00111360
Iteration 16/25 | Loss: 0.00111360
Iteration 17/25 | Loss: 0.00111360
Iteration 18/25 | Loss: 0.00111360
Iteration 19/25 | Loss: 0.00111360
Iteration 20/25 | Loss: 0.00111360
Iteration 21/25 | Loss: 0.00111360
Iteration 22/25 | Loss: 0.00111360
Iteration 23/25 | Loss: 0.00111360
Iteration 24/25 | Loss: 0.00111360
Iteration 25/25 | Loss: 0.00111360

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111360
Iteration 2/1000 | Loss: 0.00005817
Iteration 3/1000 | Loss: 0.00003520
Iteration 4/1000 | Loss: 0.00002895
Iteration 5/1000 | Loss: 0.00002637
Iteration 6/1000 | Loss: 0.00002524
Iteration 7/1000 | Loss: 0.00002421
Iteration 8/1000 | Loss: 0.00002339
Iteration 9/1000 | Loss: 0.00002277
Iteration 10/1000 | Loss: 0.00002227
Iteration 11/1000 | Loss: 0.00002191
Iteration 12/1000 | Loss: 0.00002171
Iteration 13/1000 | Loss: 0.00002149
Iteration 14/1000 | Loss: 0.00002148
Iteration 15/1000 | Loss: 0.00002133
Iteration 16/1000 | Loss: 0.00002132
Iteration 17/1000 | Loss: 0.00002127
Iteration 18/1000 | Loss: 0.00002127
Iteration 19/1000 | Loss: 0.00002121
Iteration 20/1000 | Loss: 0.00002118
Iteration 21/1000 | Loss: 0.00002118
Iteration 22/1000 | Loss: 0.00002118
Iteration 23/1000 | Loss: 0.00002117
Iteration 24/1000 | Loss: 0.00002117
Iteration 25/1000 | Loss: 0.00002116
Iteration 26/1000 | Loss: 0.00002114
Iteration 27/1000 | Loss: 0.00002114
Iteration 28/1000 | Loss: 0.00002113
Iteration 29/1000 | Loss: 0.00002113
Iteration 30/1000 | Loss: 0.00002111
Iteration 31/1000 | Loss: 0.00002107
Iteration 32/1000 | Loss: 0.00002107
Iteration 33/1000 | Loss: 0.00002106
Iteration 34/1000 | Loss: 0.00002106
Iteration 35/1000 | Loss: 0.00002106
Iteration 36/1000 | Loss: 0.00002103
Iteration 37/1000 | Loss: 0.00002103
Iteration 38/1000 | Loss: 0.00002102
Iteration 39/1000 | Loss: 0.00002102
Iteration 40/1000 | Loss: 0.00002102
Iteration 41/1000 | Loss: 0.00002102
Iteration 42/1000 | Loss: 0.00002101
Iteration 43/1000 | Loss: 0.00002101
Iteration 44/1000 | Loss: 0.00002101
Iteration 45/1000 | Loss: 0.00002101
Iteration 46/1000 | Loss: 0.00002100
Iteration 47/1000 | Loss: 0.00002100
Iteration 48/1000 | Loss: 0.00002097
Iteration 49/1000 | Loss: 0.00002097
Iteration 50/1000 | Loss: 0.00002096
Iteration 51/1000 | Loss: 0.00002095
Iteration 52/1000 | Loss: 0.00002095
Iteration 53/1000 | Loss: 0.00002094
Iteration 54/1000 | Loss: 0.00002094
Iteration 55/1000 | Loss: 0.00002093
Iteration 56/1000 | Loss: 0.00002093
Iteration 57/1000 | Loss: 0.00002093
Iteration 58/1000 | Loss: 0.00002092
Iteration 59/1000 | Loss: 0.00002091
Iteration 60/1000 | Loss: 0.00002090
Iteration 61/1000 | Loss: 0.00002089
Iteration 62/1000 | Loss: 0.00002089
Iteration 63/1000 | Loss: 0.00002088
Iteration 64/1000 | Loss: 0.00002088
Iteration 65/1000 | Loss: 0.00002088
Iteration 66/1000 | Loss: 0.00002085
Iteration 67/1000 | Loss: 0.00002085
Iteration 68/1000 | Loss: 0.00002085
Iteration 69/1000 | Loss: 0.00002084
Iteration 70/1000 | Loss: 0.00002084
Iteration 71/1000 | Loss: 0.00002083
Iteration 72/1000 | Loss: 0.00002082
Iteration 73/1000 | Loss: 0.00002082
Iteration 74/1000 | Loss: 0.00002081
Iteration 75/1000 | Loss: 0.00002081
Iteration 76/1000 | Loss: 0.00002081
Iteration 77/1000 | Loss: 0.00002080
Iteration 78/1000 | Loss: 0.00002080
Iteration 79/1000 | Loss: 0.00002080
Iteration 80/1000 | Loss: 0.00002080
Iteration 81/1000 | Loss: 0.00002079
Iteration 82/1000 | Loss: 0.00002078
Iteration 83/1000 | Loss: 0.00002077
Iteration 84/1000 | Loss: 0.00002077
Iteration 85/1000 | Loss: 0.00002077
Iteration 86/1000 | Loss: 0.00002076
Iteration 87/1000 | Loss: 0.00002076
Iteration 88/1000 | Loss: 0.00002076
Iteration 89/1000 | Loss: 0.00002076
Iteration 90/1000 | Loss: 0.00002075
Iteration 91/1000 | Loss: 0.00002075
Iteration 92/1000 | Loss: 0.00002074
Iteration 93/1000 | Loss: 0.00002074
Iteration 94/1000 | Loss: 0.00002074
Iteration 95/1000 | Loss: 0.00002074
Iteration 96/1000 | Loss: 0.00002073
Iteration 97/1000 | Loss: 0.00002073
Iteration 98/1000 | Loss: 0.00002073
Iteration 99/1000 | Loss: 0.00002073
Iteration 100/1000 | Loss: 0.00002072
Iteration 101/1000 | Loss: 0.00002071
Iteration 102/1000 | Loss: 0.00002071
Iteration 103/1000 | Loss: 0.00002071
Iteration 104/1000 | Loss: 0.00002071
Iteration 105/1000 | Loss: 0.00002071
Iteration 106/1000 | Loss: 0.00002071
Iteration 107/1000 | Loss: 0.00002071
Iteration 108/1000 | Loss: 0.00002071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [2.0712495825137012e-05, 2.0712495825137012e-05, 2.0712495825137012e-05, 2.0712495825137012e-05, 2.0712495825137012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0712495825137012e-05

Optimization complete. Final v2v error: 3.835547685623169 mm

Highest mean error: 4.405545711517334 mm for frame 67

Lowest mean error: 3.3835372924804688 mm for frame 224

Saving results

Total time: 43.927913427352905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00526628
Iteration 2/25 | Loss: 0.00149654
Iteration 3/25 | Loss: 0.00133977
Iteration 4/25 | Loss: 0.00132552
Iteration 5/25 | Loss: 0.00132245
Iteration 6/25 | Loss: 0.00132203
Iteration 7/25 | Loss: 0.00132203
Iteration 8/25 | Loss: 0.00132203
Iteration 9/25 | Loss: 0.00132203
Iteration 10/25 | Loss: 0.00132203
Iteration 11/25 | Loss: 0.00132203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013220307882875204, 0.0013220307882875204, 0.0013220307882875204, 0.0013220307882875204, 0.0013220307882875204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013220307882875204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38168430
Iteration 2/25 | Loss: 0.00052872
Iteration 3/25 | Loss: 0.00052870
Iteration 4/25 | Loss: 0.00052870
Iteration 5/25 | Loss: 0.00052870
Iteration 6/25 | Loss: 0.00052870
Iteration 7/25 | Loss: 0.00052870
Iteration 8/25 | Loss: 0.00052870
Iteration 9/25 | Loss: 0.00052870
Iteration 10/25 | Loss: 0.00052870
Iteration 11/25 | Loss: 0.00052870
Iteration 12/25 | Loss: 0.00052870
Iteration 13/25 | Loss: 0.00052870
Iteration 14/25 | Loss: 0.00052870
Iteration 15/25 | Loss: 0.00052870
Iteration 16/25 | Loss: 0.00052870
Iteration 17/25 | Loss: 0.00052870
Iteration 18/25 | Loss: 0.00052870
Iteration 19/25 | Loss: 0.00052870
Iteration 20/25 | Loss: 0.00052870
Iteration 21/25 | Loss: 0.00052870
Iteration 22/25 | Loss: 0.00052870
Iteration 23/25 | Loss: 0.00052870
Iteration 24/25 | Loss: 0.00052870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005287002422846854, 0.0005287002422846854, 0.0005287002422846854, 0.0005287002422846854, 0.0005287002422846854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005287002422846854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052870
Iteration 2/1000 | Loss: 0.00004292
Iteration 3/1000 | Loss: 0.00003347
Iteration 4/1000 | Loss: 0.00002805
Iteration 5/1000 | Loss: 0.00002651
Iteration 6/1000 | Loss: 0.00002596
Iteration 7/1000 | Loss: 0.00002508
Iteration 8/1000 | Loss: 0.00002434
Iteration 9/1000 | Loss: 0.00002369
Iteration 10/1000 | Loss: 0.00002304
Iteration 11/1000 | Loss: 0.00002269
Iteration 12/1000 | Loss: 0.00002251
Iteration 13/1000 | Loss: 0.00002232
Iteration 14/1000 | Loss: 0.00002207
Iteration 15/1000 | Loss: 0.00002194
Iteration 16/1000 | Loss: 0.00002194
Iteration 17/1000 | Loss: 0.00002181
Iteration 18/1000 | Loss: 0.00002176
Iteration 19/1000 | Loss: 0.00002169
Iteration 20/1000 | Loss: 0.00002169
Iteration 21/1000 | Loss: 0.00002169
Iteration 22/1000 | Loss: 0.00002169
Iteration 23/1000 | Loss: 0.00002169
Iteration 24/1000 | Loss: 0.00002169
Iteration 25/1000 | Loss: 0.00002169
Iteration 26/1000 | Loss: 0.00002168
Iteration 27/1000 | Loss: 0.00002168
Iteration 28/1000 | Loss: 0.00002167
Iteration 29/1000 | Loss: 0.00002167
Iteration 30/1000 | Loss: 0.00002166
Iteration 31/1000 | Loss: 0.00002166
Iteration 32/1000 | Loss: 0.00002166
Iteration 33/1000 | Loss: 0.00002166
Iteration 34/1000 | Loss: 0.00002166
Iteration 35/1000 | Loss: 0.00002166
Iteration 36/1000 | Loss: 0.00002166
Iteration 37/1000 | Loss: 0.00002165
Iteration 38/1000 | Loss: 0.00002159
Iteration 39/1000 | Loss: 0.00002157
Iteration 40/1000 | Loss: 0.00002156
Iteration 41/1000 | Loss: 0.00002156
Iteration 42/1000 | Loss: 0.00002155
Iteration 43/1000 | Loss: 0.00002155
Iteration 44/1000 | Loss: 0.00002155
Iteration 45/1000 | Loss: 0.00002154
Iteration 46/1000 | Loss: 0.00002153
Iteration 47/1000 | Loss: 0.00002152
Iteration 48/1000 | Loss: 0.00002152
Iteration 49/1000 | Loss: 0.00002152
Iteration 50/1000 | Loss: 0.00002151
Iteration 51/1000 | Loss: 0.00002151
Iteration 52/1000 | Loss: 0.00002151
Iteration 53/1000 | Loss: 0.00002150
Iteration 54/1000 | Loss: 0.00002150
Iteration 55/1000 | Loss: 0.00002149
Iteration 56/1000 | Loss: 0.00002149
Iteration 57/1000 | Loss: 0.00002149
Iteration 58/1000 | Loss: 0.00002149
Iteration 59/1000 | Loss: 0.00002149
Iteration 60/1000 | Loss: 0.00002149
Iteration 61/1000 | Loss: 0.00002149
Iteration 62/1000 | Loss: 0.00002149
Iteration 63/1000 | Loss: 0.00002149
Iteration 64/1000 | Loss: 0.00002149
Iteration 65/1000 | Loss: 0.00002148
Iteration 66/1000 | Loss: 0.00002148
Iteration 67/1000 | Loss: 0.00002148
Iteration 68/1000 | Loss: 0.00002148
Iteration 69/1000 | Loss: 0.00002148
Iteration 70/1000 | Loss: 0.00002148
Iteration 71/1000 | Loss: 0.00002148
Iteration 72/1000 | Loss: 0.00002148
Iteration 73/1000 | Loss: 0.00002148
Iteration 74/1000 | Loss: 0.00002148
Iteration 75/1000 | Loss: 0.00002148
Iteration 76/1000 | Loss: 0.00002148
Iteration 77/1000 | Loss: 0.00002148
Iteration 78/1000 | Loss: 0.00002148
Iteration 79/1000 | Loss: 0.00002148
Iteration 80/1000 | Loss: 0.00002148
Iteration 81/1000 | Loss: 0.00002148
Iteration 82/1000 | Loss: 0.00002148
Iteration 83/1000 | Loss: 0.00002148
Iteration 84/1000 | Loss: 0.00002148
Iteration 85/1000 | Loss: 0.00002148
Iteration 86/1000 | Loss: 0.00002148
Iteration 87/1000 | Loss: 0.00002148
Iteration 88/1000 | Loss: 0.00002148
Iteration 89/1000 | Loss: 0.00002148
Iteration 90/1000 | Loss: 0.00002148
Iteration 91/1000 | Loss: 0.00002148
Iteration 92/1000 | Loss: 0.00002148
Iteration 93/1000 | Loss: 0.00002148
Iteration 94/1000 | Loss: 0.00002148
Iteration 95/1000 | Loss: 0.00002148
Iteration 96/1000 | Loss: 0.00002148
Iteration 97/1000 | Loss: 0.00002148
Iteration 98/1000 | Loss: 0.00002148
Iteration 99/1000 | Loss: 0.00002148
Iteration 100/1000 | Loss: 0.00002148
Iteration 101/1000 | Loss: 0.00002148
Iteration 102/1000 | Loss: 0.00002148
Iteration 103/1000 | Loss: 0.00002148
Iteration 104/1000 | Loss: 0.00002148
Iteration 105/1000 | Loss: 0.00002148
Iteration 106/1000 | Loss: 0.00002148
Iteration 107/1000 | Loss: 0.00002148
Iteration 108/1000 | Loss: 0.00002148
Iteration 109/1000 | Loss: 0.00002148
Iteration 110/1000 | Loss: 0.00002148
Iteration 111/1000 | Loss: 0.00002148
Iteration 112/1000 | Loss: 0.00002148
Iteration 113/1000 | Loss: 0.00002148
Iteration 114/1000 | Loss: 0.00002148
Iteration 115/1000 | Loss: 0.00002148
Iteration 116/1000 | Loss: 0.00002148
Iteration 117/1000 | Loss: 0.00002148
Iteration 118/1000 | Loss: 0.00002148
Iteration 119/1000 | Loss: 0.00002148
Iteration 120/1000 | Loss: 0.00002148
Iteration 121/1000 | Loss: 0.00002148
Iteration 122/1000 | Loss: 0.00002148
Iteration 123/1000 | Loss: 0.00002148
Iteration 124/1000 | Loss: 0.00002148
Iteration 125/1000 | Loss: 0.00002148
Iteration 126/1000 | Loss: 0.00002148
Iteration 127/1000 | Loss: 0.00002148
Iteration 128/1000 | Loss: 0.00002148
Iteration 129/1000 | Loss: 0.00002148
Iteration 130/1000 | Loss: 0.00002148
Iteration 131/1000 | Loss: 0.00002148
Iteration 132/1000 | Loss: 0.00002148
Iteration 133/1000 | Loss: 0.00002148
Iteration 134/1000 | Loss: 0.00002148
Iteration 135/1000 | Loss: 0.00002148
Iteration 136/1000 | Loss: 0.00002148
Iteration 137/1000 | Loss: 0.00002148
Iteration 138/1000 | Loss: 0.00002148
Iteration 139/1000 | Loss: 0.00002148
Iteration 140/1000 | Loss: 0.00002148
Iteration 141/1000 | Loss: 0.00002148
Iteration 142/1000 | Loss: 0.00002148
Iteration 143/1000 | Loss: 0.00002148
Iteration 144/1000 | Loss: 0.00002148
Iteration 145/1000 | Loss: 0.00002148
Iteration 146/1000 | Loss: 0.00002148
Iteration 147/1000 | Loss: 0.00002148
Iteration 148/1000 | Loss: 0.00002148
Iteration 149/1000 | Loss: 0.00002148
Iteration 150/1000 | Loss: 0.00002148
Iteration 151/1000 | Loss: 0.00002148
Iteration 152/1000 | Loss: 0.00002148
Iteration 153/1000 | Loss: 0.00002148
Iteration 154/1000 | Loss: 0.00002148
Iteration 155/1000 | Loss: 0.00002148
Iteration 156/1000 | Loss: 0.00002148
Iteration 157/1000 | Loss: 0.00002148
Iteration 158/1000 | Loss: 0.00002148
Iteration 159/1000 | Loss: 0.00002148
Iteration 160/1000 | Loss: 0.00002148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.1475550965988077e-05, 2.1475550965988077e-05, 2.1475550965988077e-05, 2.1475550965988077e-05, 2.1475550965988077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1475550965988077e-05

Optimization complete. Final v2v error: 3.953826665878296 mm

Highest mean error: 4.148752689361572 mm for frame 49

Lowest mean error: 3.732283115386963 mm for frame 148

Saving results

Total time: 38.690789222717285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_003/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_003/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00586172
Iteration 2/25 | Loss: 0.00142696
Iteration 3/25 | Loss: 0.00135508
Iteration 4/25 | Loss: 0.00134874
Iteration 5/25 | Loss: 0.00134770
Iteration 6/25 | Loss: 0.00134770
Iteration 7/25 | Loss: 0.00134770
Iteration 8/25 | Loss: 0.00134770
Iteration 9/25 | Loss: 0.00134770
Iteration 10/25 | Loss: 0.00134770
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013476975727826357, 0.0013476975727826357, 0.0013476975727826357, 0.0013476975727826357, 0.0013476975727826357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013476975727826357

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.77124071
Iteration 2/25 | Loss: 0.00093496
Iteration 3/25 | Loss: 0.00093492
Iteration 4/25 | Loss: 0.00093492
Iteration 5/25 | Loss: 0.00093492
Iteration 6/25 | Loss: 0.00093492
Iteration 7/25 | Loss: 0.00093492
Iteration 8/25 | Loss: 0.00093492
Iteration 9/25 | Loss: 0.00093492
Iteration 10/25 | Loss: 0.00093492
Iteration 11/25 | Loss: 0.00093492
Iteration 12/25 | Loss: 0.00093492
Iteration 13/25 | Loss: 0.00093492
Iteration 14/25 | Loss: 0.00093492
Iteration 15/25 | Loss: 0.00093492
Iteration 16/25 | Loss: 0.00093492
Iteration 17/25 | Loss: 0.00093492
Iteration 18/25 | Loss: 0.00093492
Iteration 19/25 | Loss: 0.00093492
Iteration 20/25 | Loss: 0.00093492
Iteration 21/25 | Loss: 0.00093492
Iteration 22/25 | Loss: 0.00093492
Iteration 23/25 | Loss: 0.00093492
Iteration 24/25 | Loss: 0.00093492
Iteration 25/25 | Loss: 0.00093492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093492
Iteration 2/1000 | Loss: 0.00003073
Iteration 3/1000 | Loss: 0.00002230
Iteration 4/1000 | Loss: 0.00002057
Iteration 5/1000 | Loss: 0.00001977
Iteration 6/1000 | Loss: 0.00001930
Iteration 7/1000 | Loss: 0.00001906
Iteration 8/1000 | Loss: 0.00001871
Iteration 9/1000 | Loss: 0.00001839
Iteration 10/1000 | Loss: 0.00001812
Iteration 11/1000 | Loss: 0.00001811
Iteration 12/1000 | Loss: 0.00001803
Iteration 13/1000 | Loss: 0.00001784
Iteration 14/1000 | Loss: 0.00001781
Iteration 15/1000 | Loss: 0.00001780
Iteration 16/1000 | Loss: 0.00001771
Iteration 17/1000 | Loss: 0.00001767
Iteration 18/1000 | Loss: 0.00001766
Iteration 19/1000 | Loss: 0.00001766
Iteration 20/1000 | Loss: 0.00001765
Iteration 21/1000 | Loss: 0.00001759
Iteration 22/1000 | Loss: 0.00001754
Iteration 23/1000 | Loss: 0.00001753
Iteration 24/1000 | Loss: 0.00001753
Iteration 25/1000 | Loss: 0.00001752
Iteration 26/1000 | Loss: 0.00001751
Iteration 27/1000 | Loss: 0.00001751
Iteration 28/1000 | Loss: 0.00001749
Iteration 29/1000 | Loss: 0.00001749
Iteration 30/1000 | Loss: 0.00001748
Iteration 31/1000 | Loss: 0.00001745
Iteration 32/1000 | Loss: 0.00001743
Iteration 33/1000 | Loss: 0.00001743
Iteration 34/1000 | Loss: 0.00001743
Iteration 35/1000 | Loss: 0.00001743
Iteration 36/1000 | Loss: 0.00001743
Iteration 37/1000 | Loss: 0.00001743
Iteration 38/1000 | Loss: 0.00001742
Iteration 39/1000 | Loss: 0.00001742
Iteration 40/1000 | Loss: 0.00001741
Iteration 41/1000 | Loss: 0.00001741
Iteration 42/1000 | Loss: 0.00001738
Iteration 43/1000 | Loss: 0.00001738
Iteration 44/1000 | Loss: 0.00001738
Iteration 45/1000 | Loss: 0.00001738
Iteration 46/1000 | Loss: 0.00001738
Iteration 47/1000 | Loss: 0.00001738
Iteration 48/1000 | Loss: 0.00001737
Iteration 49/1000 | Loss: 0.00001737
Iteration 50/1000 | Loss: 0.00001736
Iteration 51/1000 | Loss: 0.00001734
Iteration 52/1000 | Loss: 0.00001734
Iteration 53/1000 | Loss: 0.00001734
Iteration 54/1000 | Loss: 0.00001734
Iteration 55/1000 | Loss: 0.00001734
Iteration 56/1000 | Loss: 0.00001734
Iteration 57/1000 | Loss: 0.00001734
Iteration 58/1000 | Loss: 0.00001734
Iteration 59/1000 | Loss: 0.00001734
Iteration 60/1000 | Loss: 0.00001734
Iteration 61/1000 | Loss: 0.00001734
Iteration 62/1000 | Loss: 0.00001730
Iteration 63/1000 | Loss: 0.00001730
Iteration 64/1000 | Loss: 0.00001729
Iteration 65/1000 | Loss: 0.00001729
Iteration 66/1000 | Loss: 0.00001729
Iteration 67/1000 | Loss: 0.00001729
Iteration 68/1000 | Loss: 0.00001729
Iteration 69/1000 | Loss: 0.00001729
Iteration 70/1000 | Loss: 0.00001729
Iteration 71/1000 | Loss: 0.00001729
Iteration 72/1000 | Loss: 0.00001729
Iteration 73/1000 | Loss: 0.00001728
Iteration 74/1000 | Loss: 0.00001727
Iteration 75/1000 | Loss: 0.00001726
Iteration 76/1000 | Loss: 0.00001726
Iteration 77/1000 | Loss: 0.00001725
Iteration 78/1000 | Loss: 0.00001725
Iteration 79/1000 | Loss: 0.00001725
Iteration 80/1000 | Loss: 0.00001724
Iteration 81/1000 | Loss: 0.00001724
Iteration 82/1000 | Loss: 0.00001724
Iteration 83/1000 | Loss: 0.00001721
Iteration 84/1000 | Loss: 0.00001721
Iteration 85/1000 | Loss: 0.00001721
Iteration 86/1000 | Loss: 0.00001721
Iteration 87/1000 | Loss: 0.00001721
Iteration 88/1000 | Loss: 0.00001721
Iteration 89/1000 | Loss: 0.00001720
Iteration 90/1000 | Loss: 0.00001720
Iteration 91/1000 | Loss: 0.00001720
Iteration 92/1000 | Loss: 0.00001720
Iteration 93/1000 | Loss: 0.00001719
Iteration 94/1000 | Loss: 0.00001719
Iteration 95/1000 | Loss: 0.00001718
Iteration 96/1000 | Loss: 0.00001717
Iteration 97/1000 | Loss: 0.00001717
Iteration 98/1000 | Loss: 0.00001717
Iteration 99/1000 | Loss: 0.00001716
Iteration 100/1000 | Loss: 0.00001716
Iteration 101/1000 | Loss: 0.00001716
Iteration 102/1000 | Loss: 0.00001715
Iteration 103/1000 | Loss: 0.00001715
Iteration 104/1000 | Loss: 0.00001715
Iteration 105/1000 | Loss: 0.00001715
Iteration 106/1000 | Loss: 0.00001715
Iteration 107/1000 | Loss: 0.00001715
Iteration 108/1000 | Loss: 0.00001714
Iteration 109/1000 | Loss: 0.00001714
Iteration 110/1000 | Loss: 0.00001714
Iteration 111/1000 | Loss: 0.00001714
Iteration 112/1000 | Loss: 0.00001714
Iteration 113/1000 | Loss: 0.00001714
Iteration 114/1000 | Loss: 0.00001714
Iteration 115/1000 | Loss: 0.00001714
Iteration 116/1000 | Loss: 0.00001713
Iteration 117/1000 | Loss: 0.00001713
Iteration 118/1000 | Loss: 0.00001713
Iteration 119/1000 | Loss: 0.00001713
Iteration 120/1000 | Loss: 0.00001713
Iteration 121/1000 | Loss: 0.00001713
Iteration 122/1000 | Loss: 0.00001712
Iteration 123/1000 | Loss: 0.00001712
Iteration 124/1000 | Loss: 0.00001712
Iteration 125/1000 | Loss: 0.00001712
Iteration 126/1000 | Loss: 0.00001712
Iteration 127/1000 | Loss: 0.00001712
Iteration 128/1000 | Loss: 0.00001712
Iteration 129/1000 | Loss: 0.00001711
Iteration 130/1000 | Loss: 0.00001711
Iteration 131/1000 | Loss: 0.00001711
Iteration 132/1000 | Loss: 0.00001711
Iteration 133/1000 | Loss: 0.00001711
Iteration 134/1000 | Loss: 0.00001711
Iteration 135/1000 | Loss: 0.00001711
Iteration 136/1000 | Loss: 0.00001711
Iteration 137/1000 | Loss: 0.00001710
Iteration 138/1000 | Loss: 0.00001710
Iteration 139/1000 | Loss: 0.00001710
Iteration 140/1000 | Loss: 0.00001710
Iteration 141/1000 | Loss: 0.00001710
Iteration 142/1000 | Loss: 0.00001709
Iteration 143/1000 | Loss: 0.00001709
Iteration 144/1000 | Loss: 0.00001709
Iteration 145/1000 | Loss: 0.00001709
Iteration 146/1000 | Loss: 0.00001709
Iteration 147/1000 | Loss: 0.00001709
Iteration 148/1000 | Loss: 0.00001709
Iteration 149/1000 | Loss: 0.00001709
Iteration 150/1000 | Loss: 0.00001709
Iteration 151/1000 | Loss: 0.00001709
Iteration 152/1000 | Loss: 0.00001709
Iteration 153/1000 | Loss: 0.00001708
Iteration 154/1000 | Loss: 0.00001708
Iteration 155/1000 | Loss: 0.00001708
Iteration 156/1000 | Loss: 0.00001708
Iteration 157/1000 | Loss: 0.00001708
Iteration 158/1000 | Loss: 0.00001708
Iteration 159/1000 | Loss: 0.00001708
Iteration 160/1000 | Loss: 0.00001708
Iteration 161/1000 | Loss: 0.00001708
Iteration 162/1000 | Loss: 0.00001708
Iteration 163/1000 | Loss: 0.00001707
Iteration 164/1000 | Loss: 0.00001707
Iteration 165/1000 | Loss: 0.00001707
Iteration 166/1000 | Loss: 0.00001707
Iteration 167/1000 | Loss: 0.00001707
Iteration 168/1000 | Loss: 0.00001707
Iteration 169/1000 | Loss: 0.00001707
Iteration 170/1000 | Loss: 0.00001707
Iteration 171/1000 | Loss: 0.00001707
Iteration 172/1000 | Loss: 0.00001707
Iteration 173/1000 | Loss: 0.00001707
Iteration 174/1000 | Loss: 0.00001707
Iteration 175/1000 | Loss: 0.00001707
Iteration 176/1000 | Loss: 0.00001707
Iteration 177/1000 | Loss: 0.00001707
Iteration 178/1000 | Loss: 0.00001707
Iteration 179/1000 | Loss: 0.00001706
Iteration 180/1000 | Loss: 0.00001706
Iteration 181/1000 | Loss: 0.00001706
Iteration 182/1000 | Loss: 0.00001706
Iteration 183/1000 | Loss: 0.00001706
Iteration 184/1000 | Loss: 0.00001705
Iteration 185/1000 | Loss: 0.00001705
Iteration 186/1000 | Loss: 0.00001705
Iteration 187/1000 | Loss: 0.00001705
Iteration 188/1000 | Loss: 0.00001705
Iteration 189/1000 | Loss: 0.00001705
Iteration 190/1000 | Loss: 0.00001705
Iteration 191/1000 | Loss: 0.00001704
Iteration 192/1000 | Loss: 0.00001704
Iteration 193/1000 | Loss: 0.00001704
Iteration 194/1000 | Loss: 0.00001704
Iteration 195/1000 | Loss: 0.00001704
Iteration 196/1000 | Loss: 0.00001704
Iteration 197/1000 | Loss: 0.00001704
Iteration 198/1000 | Loss: 0.00001704
Iteration 199/1000 | Loss: 0.00001704
Iteration 200/1000 | Loss: 0.00001704
Iteration 201/1000 | Loss: 0.00001703
Iteration 202/1000 | Loss: 0.00001703
Iteration 203/1000 | Loss: 0.00001703
Iteration 204/1000 | Loss: 0.00001703
Iteration 205/1000 | Loss: 0.00001703
Iteration 206/1000 | Loss: 0.00001703
Iteration 207/1000 | Loss: 0.00001703
Iteration 208/1000 | Loss: 0.00001703
Iteration 209/1000 | Loss: 0.00001703
Iteration 210/1000 | Loss: 0.00001703
Iteration 211/1000 | Loss: 0.00001703
Iteration 212/1000 | Loss: 0.00001703
Iteration 213/1000 | Loss: 0.00001703
Iteration 214/1000 | Loss: 0.00001703
Iteration 215/1000 | Loss: 0.00001703
Iteration 216/1000 | Loss: 0.00001703
Iteration 217/1000 | Loss: 0.00001703
Iteration 218/1000 | Loss: 0.00001702
Iteration 219/1000 | Loss: 0.00001702
Iteration 220/1000 | Loss: 0.00001702
Iteration 221/1000 | Loss: 0.00001702
Iteration 222/1000 | Loss: 0.00001702
Iteration 223/1000 | Loss: 0.00001702
Iteration 224/1000 | Loss: 0.00001702
Iteration 225/1000 | Loss: 0.00001702
Iteration 226/1000 | Loss: 0.00001702
Iteration 227/1000 | Loss: 0.00001702
Iteration 228/1000 | Loss: 0.00001702
Iteration 229/1000 | Loss: 0.00001702
Iteration 230/1000 | Loss: 0.00001702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.702203371678479e-05, 1.702203371678479e-05, 1.702203371678479e-05, 1.702203371678479e-05, 1.702203371678479e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.702203371678479e-05

Optimization complete. Final v2v error: 3.491645097732544 mm

Highest mean error: 3.836552143096924 mm for frame 179

Lowest mean error: 3.2573983669281006 mm for frame 201

Saving results

Total time: 47.7263822555542
