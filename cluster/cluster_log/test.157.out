Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=157, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8792-8847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003403
Iteration 2/25 | Loss: 0.00185070
Iteration 3/25 | Loss: 0.00168484
Iteration 4/25 | Loss: 0.00144002
Iteration 5/25 | Loss: 0.00136363
Iteration 6/25 | Loss: 0.00132652
Iteration 7/25 | Loss: 0.00126674
Iteration 8/25 | Loss: 0.00131525
Iteration 9/25 | Loss: 0.00125937
Iteration 10/25 | Loss: 0.00124610
Iteration 11/25 | Loss: 0.00124220
Iteration 12/25 | Loss: 0.00124369
Iteration 13/25 | Loss: 0.00124431
Iteration 14/25 | Loss: 0.00123109
Iteration 15/25 | Loss: 0.00122833
Iteration 16/25 | Loss: 0.00122811
Iteration 17/25 | Loss: 0.00122810
Iteration 18/25 | Loss: 0.00122810
Iteration 19/25 | Loss: 0.00122810
Iteration 20/25 | Loss: 0.00122810
Iteration 21/25 | Loss: 0.00122810
Iteration 22/25 | Loss: 0.00122809
Iteration 23/25 | Loss: 0.00122809
Iteration 24/25 | Loss: 0.00122809
Iteration 25/25 | Loss: 0.00122809

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85635960
Iteration 2/25 | Loss: 0.00143866
Iteration 3/25 | Loss: 0.00143863
Iteration 4/25 | Loss: 0.00143863
Iteration 5/25 | Loss: 0.00143863
Iteration 6/25 | Loss: 0.00143863
Iteration 7/25 | Loss: 0.00143863
Iteration 8/25 | Loss: 0.00143863
Iteration 9/25 | Loss: 0.00143863
Iteration 10/25 | Loss: 0.00143862
Iteration 11/25 | Loss: 0.00143862
Iteration 12/25 | Loss: 0.00143862
Iteration 13/25 | Loss: 0.00143862
Iteration 14/25 | Loss: 0.00143862
Iteration 15/25 | Loss: 0.00143862
Iteration 16/25 | Loss: 0.00143862
Iteration 17/25 | Loss: 0.00143862
Iteration 18/25 | Loss: 0.00143862
Iteration 19/25 | Loss: 0.00143862
Iteration 20/25 | Loss: 0.00143862
Iteration 21/25 | Loss: 0.00143862
Iteration 22/25 | Loss: 0.00143862
Iteration 23/25 | Loss: 0.00143862
Iteration 24/25 | Loss: 0.00143862
Iteration 25/25 | Loss: 0.00143862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143862
Iteration 2/1000 | Loss: 0.00019064
Iteration 3/1000 | Loss: 0.00013764
Iteration 4/1000 | Loss: 0.00011131
Iteration 5/1000 | Loss: 0.00010205
Iteration 6/1000 | Loss: 0.00009356
Iteration 7/1000 | Loss: 0.00056573
Iteration 8/1000 | Loss: 0.00169042
Iteration 9/1000 | Loss: 0.00155118
Iteration 10/1000 | Loss: 0.00043286
Iteration 11/1000 | Loss: 0.00049252
Iteration 12/1000 | Loss: 0.00032791
Iteration 13/1000 | Loss: 0.00025586
Iteration 14/1000 | Loss: 0.00091775
Iteration 15/1000 | Loss: 0.00015226
Iteration 16/1000 | Loss: 0.00010745
Iteration 17/1000 | Loss: 0.00007452
Iteration 18/1000 | Loss: 0.00006447
Iteration 19/1000 | Loss: 0.00005891
Iteration 20/1000 | Loss: 0.00005513
Iteration 21/1000 | Loss: 0.00005230
Iteration 22/1000 | Loss: 0.00005063
Iteration 23/1000 | Loss: 0.00004885
Iteration 24/1000 | Loss: 0.00004746
Iteration 25/1000 | Loss: 0.00004631
Iteration 26/1000 | Loss: 0.00004566
Iteration 27/1000 | Loss: 0.00042581
Iteration 28/1000 | Loss: 0.00092615
Iteration 29/1000 | Loss: 0.00189971
Iteration 30/1000 | Loss: 0.00112139
Iteration 31/1000 | Loss: 0.00011577
Iteration 32/1000 | Loss: 0.00007440
Iteration 33/1000 | Loss: 0.00005657
Iteration 34/1000 | Loss: 0.00004137
Iteration 35/1000 | Loss: 0.00003283
Iteration 36/1000 | Loss: 0.00002821
Iteration 37/1000 | Loss: 0.00002588
Iteration 38/1000 | Loss: 0.00002427
Iteration 39/1000 | Loss: 0.00002313
Iteration 40/1000 | Loss: 0.00002211
Iteration 41/1000 | Loss: 0.00002135
Iteration 42/1000 | Loss: 0.00002076
Iteration 43/1000 | Loss: 0.00002028
Iteration 44/1000 | Loss: 0.00001996
Iteration 45/1000 | Loss: 0.00001972
Iteration 46/1000 | Loss: 0.00001965
Iteration 47/1000 | Loss: 0.00001963
Iteration 48/1000 | Loss: 0.00001955
Iteration 49/1000 | Loss: 0.00001938
Iteration 50/1000 | Loss: 0.00001932
Iteration 51/1000 | Loss: 0.00001932
Iteration 52/1000 | Loss: 0.00001931
Iteration 53/1000 | Loss: 0.00001927
Iteration 54/1000 | Loss: 0.00001926
Iteration 55/1000 | Loss: 0.00001923
Iteration 56/1000 | Loss: 0.00001922
Iteration 57/1000 | Loss: 0.00001921
Iteration 58/1000 | Loss: 0.00001921
Iteration 59/1000 | Loss: 0.00001920
Iteration 60/1000 | Loss: 0.00001920
Iteration 61/1000 | Loss: 0.00001920
Iteration 62/1000 | Loss: 0.00001920
Iteration 63/1000 | Loss: 0.00001919
Iteration 64/1000 | Loss: 0.00001919
Iteration 65/1000 | Loss: 0.00001919
Iteration 66/1000 | Loss: 0.00001918
Iteration 67/1000 | Loss: 0.00001918
Iteration 68/1000 | Loss: 0.00001918
Iteration 69/1000 | Loss: 0.00001917
Iteration 70/1000 | Loss: 0.00001916
Iteration 71/1000 | Loss: 0.00001916
Iteration 72/1000 | Loss: 0.00001916
Iteration 73/1000 | Loss: 0.00001916
Iteration 74/1000 | Loss: 0.00001916
Iteration 75/1000 | Loss: 0.00001916
Iteration 76/1000 | Loss: 0.00001916
Iteration 77/1000 | Loss: 0.00001915
Iteration 78/1000 | Loss: 0.00001914
Iteration 79/1000 | Loss: 0.00001914
Iteration 80/1000 | Loss: 0.00001914
Iteration 81/1000 | Loss: 0.00001913
Iteration 82/1000 | Loss: 0.00001913
Iteration 83/1000 | Loss: 0.00001913
Iteration 84/1000 | Loss: 0.00001912
Iteration 85/1000 | Loss: 0.00001912
Iteration 86/1000 | Loss: 0.00001912
Iteration 87/1000 | Loss: 0.00001912
Iteration 88/1000 | Loss: 0.00001912
Iteration 89/1000 | Loss: 0.00001912
Iteration 90/1000 | Loss: 0.00001912
Iteration 91/1000 | Loss: 0.00001912
Iteration 92/1000 | Loss: 0.00001911
Iteration 93/1000 | Loss: 0.00001911
Iteration 94/1000 | Loss: 0.00001911
Iteration 95/1000 | Loss: 0.00001911
Iteration 96/1000 | Loss: 0.00001911
Iteration 97/1000 | Loss: 0.00001911
Iteration 98/1000 | Loss: 0.00001911
Iteration 99/1000 | Loss: 0.00001911
Iteration 100/1000 | Loss: 0.00001911
Iteration 101/1000 | Loss: 0.00001911
Iteration 102/1000 | Loss: 0.00001911
Iteration 103/1000 | Loss: 0.00001911
Iteration 104/1000 | Loss: 0.00001911
Iteration 105/1000 | Loss: 0.00001911
Iteration 106/1000 | Loss: 0.00001911
Iteration 107/1000 | Loss: 0.00001911
Iteration 108/1000 | Loss: 0.00001911
Iteration 109/1000 | Loss: 0.00001911
Iteration 110/1000 | Loss: 0.00001911
Iteration 111/1000 | Loss: 0.00001911
Iteration 112/1000 | Loss: 0.00001911
Iteration 113/1000 | Loss: 0.00001911
Iteration 114/1000 | Loss: 0.00001911
Iteration 115/1000 | Loss: 0.00001911
Iteration 116/1000 | Loss: 0.00001911
Iteration 117/1000 | Loss: 0.00001911
Iteration 118/1000 | Loss: 0.00001911
Iteration 119/1000 | Loss: 0.00001911
Iteration 120/1000 | Loss: 0.00001911
Iteration 121/1000 | Loss: 0.00001911
Iteration 122/1000 | Loss: 0.00001911
Iteration 123/1000 | Loss: 0.00001911
Iteration 124/1000 | Loss: 0.00001911
Iteration 125/1000 | Loss: 0.00001911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.9105878891423345e-05, 1.9105878891423345e-05, 1.9105878891423345e-05, 1.9105878891423345e-05, 1.9105878891423345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9105878891423345e-05

Optimization complete. Final v2v error: 3.3078653812408447 mm

Highest mean error: 11.580455780029297 mm for frame 45

Lowest mean error: 2.558701515197754 mm for frame 31

Saving results

Total time: 100.72680497169495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017230
Iteration 2/25 | Loss: 0.00218655
Iteration 3/25 | Loss: 0.00172624
Iteration 4/25 | Loss: 0.00194139
Iteration 5/25 | Loss: 0.00132628
Iteration 6/25 | Loss: 0.00127938
Iteration 7/25 | Loss: 0.00127531
Iteration 8/25 | Loss: 0.00127530
Iteration 9/25 | Loss: 0.00127530
Iteration 10/25 | Loss: 0.00127530
Iteration 11/25 | Loss: 0.00127530
Iteration 12/25 | Loss: 0.00127530
Iteration 13/25 | Loss: 0.00127530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012753049377352, 0.0012753049377352, 0.0012753049377352, 0.0012753049377352, 0.0012753049377352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012753049377352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36617064
Iteration 2/25 | Loss: 0.00054042
Iteration 3/25 | Loss: 0.00054041
Iteration 4/25 | Loss: 0.00054041
Iteration 5/25 | Loss: 0.00054041
Iteration 6/25 | Loss: 0.00054041
Iteration 7/25 | Loss: 0.00054041
Iteration 8/25 | Loss: 0.00054041
Iteration 9/25 | Loss: 0.00054041
Iteration 10/25 | Loss: 0.00054041
Iteration 11/25 | Loss: 0.00054041
Iteration 12/25 | Loss: 0.00054040
Iteration 13/25 | Loss: 0.00054040
Iteration 14/25 | Loss: 0.00054040
Iteration 15/25 | Loss: 0.00054040
Iteration 16/25 | Loss: 0.00054040
Iteration 17/25 | Loss: 0.00054040
Iteration 18/25 | Loss: 0.00054040
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000540404871571809, 0.000540404871571809, 0.000540404871571809, 0.000540404871571809, 0.000540404871571809]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000540404871571809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054040
Iteration 2/1000 | Loss: 0.00004517
Iteration 3/1000 | Loss: 0.00003374
Iteration 4/1000 | Loss: 0.00003222
Iteration 5/1000 | Loss: 0.00003151
Iteration 6/1000 | Loss: 0.00003096
Iteration 7/1000 | Loss: 0.00003057
Iteration 8/1000 | Loss: 0.00003025
Iteration 9/1000 | Loss: 0.00002995
Iteration 10/1000 | Loss: 0.00002976
Iteration 11/1000 | Loss: 0.00002956
Iteration 12/1000 | Loss: 0.00002945
Iteration 13/1000 | Loss: 0.00002931
Iteration 14/1000 | Loss: 0.00002925
Iteration 15/1000 | Loss: 0.00002922
Iteration 16/1000 | Loss: 0.00002921
Iteration 17/1000 | Loss: 0.00002921
Iteration 18/1000 | Loss: 0.00002919
Iteration 19/1000 | Loss: 0.00002919
Iteration 20/1000 | Loss: 0.00002919
Iteration 21/1000 | Loss: 0.00002919
Iteration 22/1000 | Loss: 0.00002919
Iteration 23/1000 | Loss: 0.00002918
Iteration 24/1000 | Loss: 0.00002918
Iteration 25/1000 | Loss: 0.00002918
Iteration 26/1000 | Loss: 0.00002917
Iteration 27/1000 | Loss: 0.00002917
Iteration 28/1000 | Loss: 0.00002917
Iteration 29/1000 | Loss: 0.00002916
Iteration 30/1000 | Loss: 0.00002916
Iteration 31/1000 | Loss: 0.00002916
Iteration 32/1000 | Loss: 0.00002916
Iteration 33/1000 | Loss: 0.00002915
Iteration 34/1000 | Loss: 0.00002915
Iteration 35/1000 | Loss: 0.00002915
Iteration 36/1000 | Loss: 0.00002915
Iteration 37/1000 | Loss: 0.00002915
Iteration 38/1000 | Loss: 0.00002914
Iteration 39/1000 | Loss: 0.00002914
Iteration 40/1000 | Loss: 0.00002914
Iteration 41/1000 | Loss: 0.00002914
Iteration 42/1000 | Loss: 0.00002913
Iteration 43/1000 | Loss: 0.00002913
Iteration 44/1000 | Loss: 0.00002913
Iteration 45/1000 | Loss: 0.00002913
Iteration 46/1000 | Loss: 0.00002913
Iteration 47/1000 | Loss: 0.00002912
Iteration 48/1000 | Loss: 0.00002912
Iteration 49/1000 | Loss: 0.00002912
Iteration 50/1000 | Loss: 0.00002912
Iteration 51/1000 | Loss: 0.00002912
Iteration 52/1000 | Loss: 0.00002912
Iteration 53/1000 | Loss: 0.00002912
Iteration 54/1000 | Loss: 0.00002911
Iteration 55/1000 | Loss: 0.00002911
Iteration 56/1000 | Loss: 0.00002911
Iteration 57/1000 | Loss: 0.00002911
Iteration 58/1000 | Loss: 0.00002911
Iteration 59/1000 | Loss: 0.00002911
Iteration 60/1000 | Loss: 0.00002911
Iteration 61/1000 | Loss: 0.00002911
Iteration 62/1000 | Loss: 0.00002910
Iteration 63/1000 | Loss: 0.00002910
Iteration 64/1000 | Loss: 0.00002910
Iteration 65/1000 | Loss: 0.00002910
Iteration 66/1000 | Loss: 0.00002910
Iteration 67/1000 | Loss: 0.00002909
Iteration 68/1000 | Loss: 0.00002909
Iteration 69/1000 | Loss: 0.00002909
Iteration 70/1000 | Loss: 0.00002908
Iteration 71/1000 | Loss: 0.00002908
Iteration 72/1000 | Loss: 0.00002908
Iteration 73/1000 | Loss: 0.00002908
Iteration 74/1000 | Loss: 0.00002908
Iteration 75/1000 | Loss: 0.00002908
Iteration 76/1000 | Loss: 0.00002908
Iteration 77/1000 | Loss: 0.00002908
Iteration 78/1000 | Loss: 0.00002908
Iteration 79/1000 | Loss: 0.00002908
Iteration 80/1000 | Loss: 0.00002908
Iteration 81/1000 | Loss: 0.00002908
Iteration 82/1000 | Loss: 0.00002908
Iteration 83/1000 | Loss: 0.00002908
Iteration 84/1000 | Loss: 0.00002908
Iteration 85/1000 | Loss: 0.00002907
Iteration 86/1000 | Loss: 0.00002907
Iteration 87/1000 | Loss: 0.00002907
Iteration 88/1000 | Loss: 0.00002907
Iteration 89/1000 | Loss: 0.00002907
Iteration 90/1000 | Loss: 0.00002907
Iteration 91/1000 | Loss: 0.00002907
Iteration 92/1000 | Loss: 0.00002907
Iteration 93/1000 | Loss: 0.00002906
Iteration 94/1000 | Loss: 0.00002906
Iteration 95/1000 | Loss: 0.00002906
Iteration 96/1000 | Loss: 0.00002906
Iteration 97/1000 | Loss: 0.00002906
Iteration 98/1000 | Loss: 0.00002906
Iteration 99/1000 | Loss: 0.00002906
Iteration 100/1000 | Loss: 0.00002906
Iteration 101/1000 | Loss: 0.00002906
Iteration 102/1000 | Loss: 0.00002906
Iteration 103/1000 | Loss: 0.00002906
Iteration 104/1000 | Loss: 0.00002905
Iteration 105/1000 | Loss: 0.00002905
Iteration 106/1000 | Loss: 0.00002905
Iteration 107/1000 | Loss: 0.00002905
Iteration 108/1000 | Loss: 0.00002905
Iteration 109/1000 | Loss: 0.00002905
Iteration 110/1000 | Loss: 0.00002905
Iteration 111/1000 | Loss: 0.00002905
Iteration 112/1000 | Loss: 0.00002905
Iteration 113/1000 | Loss: 0.00002905
Iteration 114/1000 | Loss: 0.00002905
Iteration 115/1000 | Loss: 0.00002905
Iteration 116/1000 | Loss: 0.00002905
Iteration 117/1000 | Loss: 0.00002905
Iteration 118/1000 | Loss: 0.00002905
Iteration 119/1000 | Loss: 0.00002905
Iteration 120/1000 | Loss: 0.00002905
Iteration 121/1000 | Loss: 0.00002905
Iteration 122/1000 | Loss: 0.00002905
Iteration 123/1000 | Loss: 0.00002905
Iteration 124/1000 | Loss: 0.00002905
Iteration 125/1000 | Loss: 0.00002905
Iteration 126/1000 | Loss: 0.00002905
Iteration 127/1000 | Loss: 0.00002905
Iteration 128/1000 | Loss: 0.00002905
Iteration 129/1000 | Loss: 0.00002905
Iteration 130/1000 | Loss: 0.00002905
Iteration 131/1000 | Loss: 0.00002905
Iteration 132/1000 | Loss: 0.00002905
Iteration 133/1000 | Loss: 0.00002905
Iteration 134/1000 | Loss: 0.00002905
Iteration 135/1000 | Loss: 0.00002905
Iteration 136/1000 | Loss: 0.00002905
Iteration 137/1000 | Loss: 0.00002905
Iteration 138/1000 | Loss: 0.00002905
Iteration 139/1000 | Loss: 0.00002905
Iteration 140/1000 | Loss: 0.00002905
Iteration 141/1000 | Loss: 0.00002905
Iteration 142/1000 | Loss: 0.00002905
Iteration 143/1000 | Loss: 0.00002905
Iteration 144/1000 | Loss: 0.00002905
Iteration 145/1000 | Loss: 0.00002905
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.9050577722955495e-05, 2.9050577722955495e-05, 2.9050577722955495e-05, 2.9050577722955495e-05, 2.9050577722955495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9050577722955495e-05

Optimization complete. Final v2v error: 4.582683563232422 mm

Highest mean error: 4.8456621170043945 mm for frame 28

Lowest mean error: 4.406843662261963 mm for frame 9

Saving results

Total time: 43.74925637245178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476110
Iteration 2/25 | Loss: 0.00121700
Iteration 3/25 | Loss: 0.00112116
Iteration 4/25 | Loss: 0.00110567
Iteration 5/25 | Loss: 0.00109903
Iteration 6/25 | Loss: 0.00109788
Iteration 7/25 | Loss: 0.00109788
Iteration 8/25 | Loss: 0.00109788
Iteration 9/25 | Loss: 0.00109788
Iteration 10/25 | Loss: 0.00109788
Iteration 11/25 | Loss: 0.00109788
Iteration 12/25 | Loss: 0.00109788
Iteration 13/25 | Loss: 0.00109788
Iteration 14/25 | Loss: 0.00109788
Iteration 15/25 | Loss: 0.00109788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010978813515976071, 0.0010978813515976071, 0.0010978813515976071, 0.0010978813515976071, 0.0010978813515976071]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010978813515976071

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80657345
Iteration 2/25 | Loss: 0.00071911
Iteration 3/25 | Loss: 0.00071910
Iteration 4/25 | Loss: 0.00071910
Iteration 5/25 | Loss: 0.00071910
Iteration 6/25 | Loss: 0.00071910
Iteration 7/25 | Loss: 0.00071910
Iteration 8/25 | Loss: 0.00071910
Iteration 9/25 | Loss: 0.00071910
Iteration 10/25 | Loss: 0.00071910
Iteration 11/25 | Loss: 0.00071910
Iteration 12/25 | Loss: 0.00071910
Iteration 13/25 | Loss: 0.00071910
Iteration 14/25 | Loss: 0.00071910
Iteration 15/25 | Loss: 0.00071910
Iteration 16/25 | Loss: 0.00071910
Iteration 17/25 | Loss: 0.00071910
Iteration 18/25 | Loss: 0.00071910
Iteration 19/25 | Loss: 0.00071910
Iteration 20/25 | Loss: 0.00071910
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007191019831225276, 0.0007191019831225276, 0.0007191019831225276, 0.0007191019831225276, 0.0007191019831225276]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007191019831225276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071910
Iteration 2/1000 | Loss: 0.00003741
Iteration 3/1000 | Loss: 0.00002363
Iteration 4/1000 | Loss: 0.00002139
Iteration 5/1000 | Loss: 0.00002011
Iteration 6/1000 | Loss: 0.00001956
Iteration 7/1000 | Loss: 0.00001900
Iteration 8/1000 | Loss: 0.00001857
Iteration 9/1000 | Loss: 0.00001820
Iteration 10/1000 | Loss: 0.00001787
Iteration 11/1000 | Loss: 0.00001766
Iteration 12/1000 | Loss: 0.00001746
Iteration 13/1000 | Loss: 0.00001729
Iteration 14/1000 | Loss: 0.00001728
Iteration 15/1000 | Loss: 0.00001728
Iteration 16/1000 | Loss: 0.00001723
Iteration 17/1000 | Loss: 0.00001710
Iteration 18/1000 | Loss: 0.00001699
Iteration 19/1000 | Loss: 0.00001698
Iteration 20/1000 | Loss: 0.00001697
Iteration 21/1000 | Loss: 0.00001697
Iteration 22/1000 | Loss: 0.00001696
Iteration 23/1000 | Loss: 0.00001695
Iteration 24/1000 | Loss: 0.00001695
Iteration 25/1000 | Loss: 0.00001695
Iteration 26/1000 | Loss: 0.00001694
Iteration 27/1000 | Loss: 0.00001694
Iteration 28/1000 | Loss: 0.00001694
Iteration 29/1000 | Loss: 0.00001694
Iteration 30/1000 | Loss: 0.00001693
Iteration 31/1000 | Loss: 0.00001693
Iteration 32/1000 | Loss: 0.00001692
Iteration 33/1000 | Loss: 0.00001690
Iteration 34/1000 | Loss: 0.00001690
Iteration 35/1000 | Loss: 0.00001690
Iteration 36/1000 | Loss: 0.00001689
Iteration 37/1000 | Loss: 0.00001689
Iteration 38/1000 | Loss: 0.00001688
Iteration 39/1000 | Loss: 0.00001688
Iteration 40/1000 | Loss: 0.00001688
Iteration 41/1000 | Loss: 0.00001685
Iteration 42/1000 | Loss: 0.00001680
Iteration 43/1000 | Loss: 0.00001679
Iteration 44/1000 | Loss: 0.00001679
Iteration 45/1000 | Loss: 0.00001677
Iteration 46/1000 | Loss: 0.00001676
Iteration 47/1000 | Loss: 0.00001675
Iteration 48/1000 | Loss: 0.00001674
Iteration 49/1000 | Loss: 0.00001674
Iteration 50/1000 | Loss: 0.00001672
Iteration 51/1000 | Loss: 0.00001672
Iteration 52/1000 | Loss: 0.00001671
Iteration 53/1000 | Loss: 0.00001669
Iteration 54/1000 | Loss: 0.00001666
Iteration 55/1000 | Loss: 0.00001666
Iteration 56/1000 | Loss: 0.00001665
Iteration 57/1000 | Loss: 0.00001665
Iteration 58/1000 | Loss: 0.00001665
Iteration 59/1000 | Loss: 0.00001664
Iteration 60/1000 | Loss: 0.00001663
Iteration 61/1000 | Loss: 0.00001663
Iteration 62/1000 | Loss: 0.00001662
Iteration 63/1000 | Loss: 0.00001662
Iteration 64/1000 | Loss: 0.00001662
Iteration 65/1000 | Loss: 0.00001662
Iteration 66/1000 | Loss: 0.00001662
Iteration 67/1000 | Loss: 0.00001662
Iteration 68/1000 | Loss: 0.00001661
Iteration 69/1000 | Loss: 0.00001661
Iteration 70/1000 | Loss: 0.00001661
Iteration 71/1000 | Loss: 0.00001661
Iteration 72/1000 | Loss: 0.00001661
Iteration 73/1000 | Loss: 0.00001661
Iteration 74/1000 | Loss: 0.00001660
Iteration 75/1000 | Loss: 0.00001660
Iteration 76/1000 | Loss: 0.00001660
Iteration 77/1000 | Loss: 0.00001660
Iteration 78/1000 | Loss: 0.00001660
Iteration 79/1000 | Loss: 0.00001660
Iteration 80/1000 | Loss: 0.00001660
Iteration 81/1000 | Loss: 0.00001660
Iteration 82/1000 | Loss: 0.00001660
Iteration 83/1000 | Loss: 0.00001660
Iteration 84/1000 | Loss: 0.00001660
Iteration 85/1000 | Loss: 0.00001660
Iteration 86/1000 | Loss: 0.00001659
Iteration 87/1000 | Loss: 0.00001659
Iteration 88/1000 | Loss: 0.00001659
Iteration 89/1000 | Loss: 0.00001659
Iteration 90/1000 | Loss: 0.00001659
Iteration 91/1000 | Loss: 0.00001659
Iteration 92/1000 | Loss: 0.00001659
Iteration 93/1000 | Loss: 0.00001659
Iteration 94/1000 | Loss: 0.00001659
Iteration 95/1000 | Loss: 0.00001659
Iteration 96/1000 | Loss: 0.00001659
Iteration 97/1000 | Loss: 0.00001658
Iteration 98/1000 | Loss: 0.00001658
Iteration 99/1000 | Loss: 0.00001658
Iteration 100/1000 | Loss: 0.00001658
Iteration 101/1000 | Loss: 0.00001658
Iteration 102/1000 | Loss: 0.00001658
Iteration 103/1000 | Loss: 0.00001658
Iteration 104/1000 | Loss: 0.00001657
Iteration 105/1000 | Loss: 0.00001657
Iteration 106/1000 | Loss: 0.00001657
Iteration 107/1000 | Loss: 0.00001657
Iteration 108/1000 | Loss: 0.00001657
Iteration 109/1000 | Loss: 0.00001657
Iteration 110/1000 | Loss: 0.00001657
Iteration 111/1000 | Loss: 0.00001657
Iteration 112/1000 | Loss: 0.00001656
Iteration 113/1000 | Loss: 0.00001656
Iteration 114/1000 | Loss: 0.00001656
Iteration 115/1000 | Loss: 0.00001656
Iteration 116/1000 | Loss: 0.00001656
Iteration 117/1000 | Loss: 0.00001656
Iteration 118/1000 | Loss: 0.00001656
Iteration 119/1000 | Loss: 0.00001656
Iteration 120/1000 | Loss: 0.00001656
Iteration 121/1000 | Loss: 0.00001656
Iteration 122/1000 | Loss: 0.00001656
Iteration 123/1000 | Loss: 0.00001656
Iteration 124/1000 | Loss: 0.00001656
Iteration 125/1000 | Loss: 0.00001656
Iteration 126/1000 | Loss: 0.00001656
Iteration 127/1000 | Loss: 0.00001656
Iteration 128/1000 | Loss: 0.00001656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.6560914446017705e-05, 1.6560914446017705e-05, 1.6560914446017705e-05, 1.6560914446017705e-05, 1.6560914446017705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6560914446017705e-05

Optimization complete. Final v2v error: 3.4104323387145996 mm

Highest mean error: 4.0421953201293945 mm for frame 255

Lowest mean error: 3.189894199371338 mm for frame 221

Saving results

Total time: 46.78703808784485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783136
Iteration 2/25 | Loss: 0.00143366
Iteration 3/25 | Loss: 0.00115876
Iteration 4/25 | Loss: 0.00111729
Iteration 5/25 | Loss: 0.00111013
Iteration 6/25 | Loss: 0.00110851
Iteration 7/25 | Loss: 0.00110851
Iteration 8/25 | Loss: 0.00110851
Iteration 9/25 | Loss: 0.00110851
Iteration 10/25 | Loss: 0.00110851
Iteration 11/25 | Loss: 0.00110851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011085097212344408, 0.0011085097212344408, 0.0011085097212344408, 0.0011085097212344408, 0.0011085097212344408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011085097212344408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36099005
Iteration 2/25 | Loss: 0.00056918
Iteration 3/25 | Loss: 0.00056917
Iteration 4/25 | Loss: 0.00056917
Iteration 5/25 | Loss: 0.00056917
Iteration 6/25 | Loss: 0.00056917
Iteration 7/25 | Loss: 0.00056916
Iteration 8/25 | Loss: 0.00056916
Iteration 9/25 | Loss: 0.00056916
Iteration 10/25 | Loss: 0.00056916
Iteration 11/25 | Loss: 0.00056916
Iteration 12/25 | Loss: 0.00056916
Iteration 13/25 | Loss: 0.00056916
Iteration 14/25 | Loss: 0.00056916
Iteration 15/25 | Loss: 0.00056916
Iteration 16/25 | Loss: 0.00056916
Iteration 17/25 | Loss: 0.00056916
Iteration 18/25 | Loss: 0.00056916
Iteration 19/25 | Loss: 0.00056916
Iteration 20/25 | Loss: 0.00056916
Iteration 21/25 | Loss: 0.00056916
Iteration 22/25 | Loss: 0.00056916
Iteration 23/25 | Loss: 0.00056916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005691638798452914, 0.0005691638798452914, 0.0005691638798452914, 0.0005691638798452914, 0.0005691638798452914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005691638798452914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056916
Iteration 2/1000 | Loss: 0.00003210
Iteration 3/1000 | Loss: 0.00002221
Iteration 4/1000 | Loss: 0.00001951
Iteration 5/1000 | Loss: 0.00001847
Iteration 6/1000 | Loss: 0.00001759
Iteration 7/1000 | Loss: 0.00001710
Iteration 8/1000 | Loss: 0.00001683
Iteration 9/1000 | Loss: 0.00001647
Iteration 10/1000 | Loss: 0.00001614
Iteration 11/1000 | Loss: 0.00001596
Iteration 12/1000 | Loss: 0.00001580
Iteration 13/1000 | Loss: 0.00001579
Iteration 14/1000 | Loss: 0.00001576
Iteration 15/1000 | Loss: 0.00001575
Iteration 16/1000 | Loss: 0.00001574
Iteration 17/1000 | Loss: 0.00001572
Iteration 18/1000 | Loss: 0.00001566
Iteration 19/1000 | Loss: 0.00001566
Iteration 20/1000 | Loss: 0.00001564
Iteration 21/1000 | Loss: 0.00001563
Iteration 22/1000 | Loss: 0.00001562
Iteration 23/1000 | Loss: 0.00001562
Iteration 24/1000 | Loss: 0.00001558
Iteration 25/1000 | Loss: 0.00001558
Iteration 26/1000 | Loss: 0.00001558
Iteration 27/1000 | Loss: 0.00001557
Iteration 28/1000 | Loss: 0.00001557
Iteration 29/1000 | Loss: 0.00001557
Iteration 30/1000 | Loss: 0.00001554
Iteration 31/1000 | Loss: 0.00001554
Iteration 32/1000 | Loss: 0.00001554
Iteration 33/1000 | Loss: 0.00001554
Iteration 34/1000 | Loss: 0.00001553
Iteration 35/1000 | Loss: 0.00001553
Iteration 36/1000 | Loss: 0.00001552
Iteration 37/1000 | Loss: 0.00001552
Iteration 38/1000 | Loss: 0.00001552
Iteration 39/1000 | Loss: 0.00001552
Iteration 40/1000 | Loss: 0.00001552
Iteration 41/1000 | Loss: 0.00001552
Iteration 42/1000 | Loss: 0.00001552
Iteration 43/1000 | Loss: 0.00001552
Iteration 44/1000 | Loss: 0.00001552
Iteration 45/1000 | Loss: 0.00001552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 45. Stopping optimization.
Last 5 losses: [1.5516552593908273e-05, 1.5516552593908273e-05, 1.5516552593908273e-05, 1.5516552593908273e-05, 1.5516552593908273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5516552593908273e-05

Optimization complete. Final v2v error: 3.3432905673980713 mm

Highest mean error: 3.7960140705108643 mm for frame 156

Lowest mean error: 3.0464437007904053 mm for frame 58

Saving results

Total time: 33.935283184051514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405331
Iteration 2/25 | Loss: 0.00113541
Iteration 3/25 | Loss: 0.00105723
Iteration 4/25 | Loss: 0.00104351
Iteration 5/25 | Loss: 0.00103905
Iteration 6/25 | Loss: 0.00103801
Iteration 7/25 | Loss: 0.00103801
Iteration 8/25 | Loss: 0.00103801
Iteration 9/25 | Loss: 0.00103801
Iteration 10/25 | Loss: 0.00103801
Iteration 11/25 | Loss: 0.00103801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001038009999319911, 0.001038009999319911, 0.001038009999319911, 0.001038009999319911, 0.001038009999319911]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001038009999319911

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.13528299
Iteration 2/25 | Loss: 0.00065429
Iteration 3/25 | Loss: 0.00065429
Iteration 4/25 | Loss: 0.00065429
Iteration 5/25 | Loss: 0.00065429
Iteration 6/25 | Loss: 0.00065429
Iteration 7/25 | Loss: 0.00065429
Iteration 8/25 | Loss: 0.00065429
Iteration 9/25 | Loss: 0.00065429
Iteration 10/25 | Loss: 0.00065429
Iteration 11/25 | Loss: 0.00065429
Iteration 12/25 | Loss: 0.00065429
Iteration 13/25 | Loss: 0.00065429
Iteration 14/25 | Loss: 0.00065429
Iteration 15/25 | Loss: 0.00065429
Iteration 16/25 | Loss: 0.00065429
Iteration 17/25 | Loss: 0.00065429
Iteration 18/25 | Loss: 0.00065429
Iteration 19/25 | Loss: 0.00065429
Iteration 20/25 | Loss: 0.00065429
Iteration 21/25 | Loss: 0.00065429
Iteration 22/25 | Loss: 0.00065429
Iteration 23/25 | Loss: 0.00065429
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006542879273183644, 0.0006542879273183644, 0.0006542879273183644, 0.0006542879273183644, 0.0006542879273183644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006542879273183644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065429
Iteration 2/1000 | Loss: 0.00002272
Iteration 3/1000 | Loss: 0.00001516
Iteration 4/1000 | Loss: 0.00001274
Iteration 5/1000 | Loss: 0.00001182
Iteration 6/1000 | Loss: 0.00001130
Iteration 7/1000 | Loss: 0.00001099
Iteration 8/1000 | Loss: 0.00001073
Iteration 9/1000 | Loss: 0.00001053
Iteration 10/1000 | Loss: 0.00001050
Iteration 11/1000 | Loss: 0.00001041
Iteration 12/1000 | Loss: 0.00001025
Iteration 13/1000 | Loss: 0.00001025
Iteration 14/1000 | Loss: 0.00001014
Iteration 15/1000 | Loss: 0.00001009
Iteration 16/1000 | Loss: 0.00001007
Iteration 17/1000 | Loss: 0.00001007
Iteration 18/1000 | Loss: 0.00001004
Iteration 19/1000 | Loss: 0.00001004
Iteration 20/1000 | Loss: 0.00001003
Iteration 21/1000 | Loss: 0.00001003
Iteration 22/1000 | Loss: 0.00001002
Iteration 23/1000 | Loss: 0.00000998
Iteration 24/1000 | Loss: 0.00000998
Iteration 25/1000 | Loss: 0.00000996
Iteration 26/1000 | Loss: 0.00000995
Iteration 27/1000 | Loss: 0.00000995
Iteration 28/1000 | Loss: 0.00000994
Iteration 29/1000 | Loss: 0.00000993
Iteration 30/1000 | Loss: 0.00000993
Iteration 31/1000 | Loss: 0.00000992
Iteration 32/1000 | Loss: 0.00000991
Iteration 33/1000 | Loss: 0.00000991
Iteration 34/1000 | Loss: 0.00000990
Iteration 35/1000 | Loss: 0.00000990
Iteration 36/1000 | Loss: 0.00000988
Iteration 37/1000 | Loss: 0.00000988
Iteration 38/1000 | Loss: 0.00000987
Iteration 39/1000 | Loss: 0.00000987
Iteration 40/1000 | Loss: 0.00000987
Iteration 41/1000 | Loss: 0.00000986
Iteration 42/1000 | Loss: 0.00000986
Iteration 43/1000 | Loss: 0.00000986
Iteration 44/1000 | Loss: 0.00000986
Iteration 45/1000 | Loss: 0.00000985
Iteration 46/1000 | Loss: 0.00000985
Iteration 47/1000 | Loss: 0.00000985
Iteration 48/1000 | Loss: 0.00000985
Iteration 49/1000 | Loss: 0.00000984
Iteration 50/1000 | Loss: 0.00000983
Iteration 51/1000 | Loss: 0.00000983
Iteration 52/1000 | Loss: 0.00000982
Iteration 53/1000 | Loss: 0.00000982
Iteration 54/1000 | Loss: 0.00000981
Iteration 55/1000 | Loss: 0.00000981
Iteration 56/1000 | Loss: 0.00000981
Iteration 57/1000 | Loss: 0.00000981
Iteration 58/1000 | Loss: 0.00000981
Iteration 59/1000 | Loss: 0.00000981
Iteration 60/1000 | Loss: 0.00000980
Iteration 61/1000 | Loss: 0.00000980
Iteration 62/1000 | Loss: 0.00000980
Iteration 63/1000 | Loss: 0.00000979
Iteration 64/1000 | Loss: 0.00000979
Iteration 65/1000 | Loss: 0.00000979
Iteration 66/1000 | Loss: 0.00000978
Iteration 67/1000 | Loss: 0.00000978
Iteration 68/1000 | Loss: 0.00000978
Iteration 69/1000 | Loss: 0.00000978
Iteration 70/1000 | Loss: 0.00000978
Iteration 71/1000 | Loss: 0.00000978
Iteration 72/1000 | Loss: 0.00000978
Iteration 73/1000 | Loss: 0.00000977
Iteration 74/1000 | Loss: 0.00000977
Iteration 75/1000 | Loss: 0.00000977
Iteration 76/1000 | Loss: 0.00000977
Iteration 77/1000 | Loss: 0.00000977
Iteration 78/1000 | Loss: 0.00000977
Iteration 79/1000 | Loss: 0.00000976
Iteration 80/1000 | Loss: 0.00000976
Iteration 81/1000 | Loss: 0.00000976
Iteration 82/1000 | Loss: 0.00000975
Iteration 83/1000 | Loss: 0.00000975
Iteration 84/1000 | Loss: 0.00000975
Iteration 85/1000 | Loss: 0.00000975
Iteration 86/1000 | Loss: 0.00000974
Iteration 87/1000 | Loss: 0.00000974
Iteration 88/1000 | Loss: 0.00000974
Iteration 89/1000 | Loss: 0.00000974
Iteration 90/1000 | Loss: 0.00000973
Iteration 91/1000 | Loss: 0.00000973
Iteration 92/1000 | Loss: 0.00000973
Iteration 93/1000 | Loss: 0.00000973
Iteration 94/1000 | Loss: 0.00000973
Iteration 95/1000 | Loss: 0.00000972
Iteration 96/1000 | Loss: 0.00000972
Iteration 97/1000 | Loss: 0.00000972
Iteration 98/1000 | Loss: 0.00000972
Iteration 99/1000 | Loss: 0.00000972
Iteration 100/1000 | Loss: 0.00000971
Iteration 101/1000 | Loss: 0.00000971
Iteration 102/1000 | Loss: 0.00000970
Iteration 103/1000 | Loss: 0.00000970
Iteration 104/1000 | Loss: 0.00000970
Iteration 105/1000 | Loss: 0.00000970
Iteration 106/1000 | Loss: 0.00000970
Iteration 107/1000 | Loss: 0.00000969
Iteration 108/1000 | Loss: 0.00000969
Iteration 109/1000 | Loss: 0.00000969
Iteration 110/1000 | Loss: 0.00000969
Iteration 111/1000 | Loss: 0.00000969
Iteration 112/1000 | Loss: 0.00000968
Iteration 113/1000 | Loss: 0.00000968
Iteration 114/1000 | Loss: 0.00000968
Iteration 115/1000 | Loss: 0.00000967
Iteration 116/1000 | Loss: 0.00000967
Iteration 117/1000 | Loss: 0.00000967
Iteration 118/1000 | Loss: 0.00000966
Iteration 119/1000 | Loss: 0.00000966
Iteration 120/1000 | Loss: 0.00000966
Iteration 121/1000 | Loss: 0.00000966
Iteration 122/1000 | Loss: 0.00000966
Iteration 123/1000 | Loss: 0.00000966
Iteration 124/1000 | Loss: 0.00000965
Iteration 125/1000 | Loss: 0.00000965
Iteration 126/1000 | Loss: 0.00000965
Iteration 127/1000 | Loss: 0.00000965
Iteration 128/1000 | Loss: 0.00000964
Iteration 129/1000 | Loss: 0.00000964
Iteration 130/1000 | Loss: 0.00000964
Iteration 131/1000 | Loss: 0.00000964
Iteration 132/1000 | Loss: 0.00000964
Iteration 133/1000 | Loss: 0.00000964
Iteration 134/1000 | Loss: 0.00000964
Iteration 135/1000 | Loss: 0.00000963
Iteration 136/1000 | Loss: 0.00000963
Iteration 137/1000 | Loss: 0.00000963
Iteration 138/1000 | Loss: 0.00000963
Iteration 139/1000 | Loss: 0.00000962
Iteration 140/1000 | Loss: 0.00000962
Iteration 141/1000 | Loss: 0.00000962
Iteration 142/1000 | Loss: 0.00000962
Iteration 143/1000 | Loss: 0.00000962
Iteration 144/1000 | Loss: 0.00000962
Iteration 145/1000 | Loss: 0.00000961
Iteration 146/1000 | Loss: 0.00000961
Iteration 147/1000 | Loss: 0.00000961
Iteration 148/1000 | Loss: 0.00000961
Iteration 149/1000 | Loss: 0.00000961
Iteration 150/1000 | Loss: 0.00000961
Iteration 151/1000 | Loss: 0.00000961
Iteration 152/1000 | Loss: 0.00000960
Iteration 153/1000 | Loss: 0.00000960
Iteration 154/1000 | Loss: 0.00000960
Iteration 155/1000 | Loss: 0.00000960
Iteration 156/1000 | Loss: 0.00000960
Iteration 157/1000 | Loss: 0.00000960
Iteration 158/1000 | Loss: 0.00000960
Iteration 159/1000 | Loss: 0.00000959
Iteration 160/1000 | Loss: 0.00000959
Iteration 161/1000 | Loss: 0.00000959
Iteration 162/1000 | Loss: 0.00000959
Iteration 163/1000 | Loss: 0.00000959
Iteration 164/1000 | Loss: 0.00000959
Iteration 165/1000 | Loss: 0.00000959
Iteration 166/1000 | Loss: 0.00000959
Iteration 167/1000 | Loss: 0.00000959
Iteration 168/1000 | Loss: 0.00000959
Iteration 169/1000 | Loss: 0.00000959
Iteration 170/1000 | Loss: 0.00000958
Iteration 171/1000 | Loss: 0.00000958
Iteration 172/1000 | Loss: 0.00000958
Iteration 173/1000 | Loss: 0.00000958
Iteration 174/1000 | Loss: 0.00000958
Iteration 175/1000 | Loss: 0.00000958
Iteration 176/1000 | Loss: 0.00000958
Iteration 177/1000 | Loss: 0.00000958
Iteration 178/1000 | Loss: 0.00000958
Iteration 179/1000 | Loss: 0.00000958
Iteration 180/1000 | Loss: 0.00000957
Iteration 181/1000 | Loss: 0.00000957
Iteration 182/1000 | Loss: 0.00000957
Iteration 183/1000 | Loss: 0.00000957
Iteration 184/1000 | Loss: 0.00000957
Iteration 185/1000 | Loss: 0.00000957
Iteration 186/1000 | Loss: 0.00000957
Iteration 187/1000 | Loss: 0.00000957
Iteration 188/1000 | Loss: 0.00000957
Iteration 189/1000 | Loss: 0.00000957
Iteration 190/1000 | Loss: 0.00000957
Iteration 191/1000 | Loss: 0.00000957
Iteration 192/1000 | Loss: 0.00000957
Iteration 193/1000 | Loss: 0.00000957
Iteration 194/1000 | Loss: 0.00000957
Iteration 195/1000 | Loss: 0.00000956
Iteration 196/1000 | Loss: 0.00000956
Iteration 197/1000 | Loss: 0.00000956
Iteration 198/1000 | Loss: 0.00000956
Iteration 199/1000 | Loss: 0.00000956
Iteration 200/1000 | Loss: 0.00000956
Iteration 201/1000 | Loss: 0.00000956
Iteration 202/1000 | Loss: 0.00000956
Iteration 203/1000 | Loss: 0.00000956
Iteration 204/1000 | Loss: 0.00000956
Iteration 205/1000 | Loss: 0.00000956
Iteration 206/1000 | Loss: 0.00000956
Iteration 207/1000 | Loss: 0.00000956
Iteration 208/1000 | Loss: 0.00000956
Iteration 209/1000 | Loss: 0.00000956
Iteration 210/1000 | Loss: 0.00000956
Iteration 211/1000 | Loss: 0.00000956
Iteration 212/1000 | Loss: 0.00000956
Iteration 213/1000 | Loss: 0.00000956
Iteration 214/1000 | Loss: 0.00000956
Iteration 215/1000 | Loss: 0.00000956
Iteration 216/1000 | Loss: 0.00000956
Iteration 217/1000 | Loss: 0.00000956
Iteration 218/1000 | Loss: 0.00000956
Iteration 219/1000 | Loss: 0.00000956
Iteration 220/1000 | Loss: 0.00000956
Iteration 221/1000 | Loss: 0.00000956
Iteration 222/1000 | Loss: 0.00000956
Iteration 223/1000 | Loss: 0.00000956
Iteration 224/1000 | Loss: 0.00000956
Iteration 225/1000 | Loss: 0.00000956
Iteration 226/1000 | Loss: 0.00000956
Iteration 227/1000 | Loss: 0.00000956
Iteration 228/1000 | Loss: 0.00000956
Iteration 229/1000 | Loss: 0.00000956
Iteration 230/1000 | Loss: 0.00000956
Iteration 231/1000 | Loss: 0.00000956
Iteration 232/1000 | Loss: 0.00000956
Iteration 233/1000 | Loss: 0.00000956
Iteration 234/1000 | Loss: 0.00000956
Iteration 235/1000 | Loss: 0.00000956
Iteration 236/1000 | Loss: 0.00000956
Iteration 237/1000 | Loss: 0.00000956
Iteration 238/1000 | Loss: 0.00000956
Iteration 239/1000 | Loss: 0.00000956
Iteration 240/1000 | Loss: 0.00000956
Iteration 241/1000 | Loss: 0.00000956
Iteration 242/1000 | Loss: 0.00000956
Iteration 243/1000 | Loss: 0.00000956
Iteration 244/1000 | Loss: 0.00000956
Iteration 245/1000 | Loss: 0.00000956
Iteration 246/1000 | Loss: 0.00000956
Iteration 247/1000 | Loss: 0.00000956
Iteration 248/1000 | Loss: 0.00000956
Iteration 249/1000 | Loss: 0.00000956
Iteration 250/1000 | Loss: 0.00000956
Iteration 251/1000 | Loss: 0.00000956
Iteration 252/1000 | Loss: 0.00000956
Iteration 253/1000 | Loss: 0.00000956
Iteration 254/1000 | Loss: 0.00000956
Iteration 255/1000 | Loss: 0.00000956
Iteration 256/1000 | Loss: 0.00000956
Iteration 257/1000 | Loss: 0.00000956
Iteration 258/1000 | Loss: 0.00000956
Iteration 259/1000 | Loss: 0.00000956
Iteration 260/1000 | Loss: 0.00000956
Iteration 261/1000 | Loss: 0.00000956
Iteration 262/1000 | Loss: 0.00000956
Iteration 263/1000 | Loss: 0.00000956
Iteration 264/1000 | Loss: 0.00000956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [9.56472649704665e-06, 9.56472649704665e-06, 9.56472649704665e-06, 9.56472649704665e-06, 9.56472649704665e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.56472649704665e-06

Optimization complete. Final v2v error: 2.6540188789367676 mm

Highest mean error: 2.941453218460083 mm for frame 102

Lowest mean error: 2.441754102706909 mm for frame 35

Saving results

Total time: 42.723851919174194
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00743080
Iteration 2/25 | Loss: 0.00117672
Iteration 3/25 | Loss: 0.00102561
Iteration 4/25 | Loss: 0.00101234
Iteration 5/25 | Loss: 0.00100928
Iteration 6/25 | Loss: 0.00100908
Iteration 7/25 | Loss: 0.00100908
Iteration 8/25 | Loss: 0.00100908
Iteration 9/25 | Loss: 0.00100908
Iteration 10/25 | Loss: 0.00100908
Iteration 11/25 | Loss: 0.00100908
Iteration 12/25 | Loss: 0.00100908
Iteration 13/25 | Loss: 0.00100908
Iteration 14/25 | Loss: 0.00100908
Iteration 15/25 | Loss: 0.00100908
Iteration 16/25 | Loss: 0.00100908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010090797441080213, 0.0010090797441080213, 0.0010090797441080213, 0.0010090797441080213, 0.0010090797441080213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010090797441080213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34140480
Iteration 2/25 | Loss: 0.00046690
Iteration 3/25 | Loss: 0.00046687
Iteration 4/25 | Loss: 0.00046687
Iteration 5/25 | Loss: 0.00046687
Iteration 6/25 | Loss: 0.00046687
Iteration 7/25 | Loss: 0.00046687
Iteration 8/25 | Loss: 0.00046687
Iteration 9/25 | Loss: 0.00046687
Iteration 10/25 | Loss: 0.00046687
Iteration 11/25 | Loss: 0.00046687
Iteration 12/25 | Loss: 0.00046687
Iteration 13/25 | Loss: 0.00046687
Iteration 14/25 | Loss: 0.00046687
Iteration 15/25 | Loss: 0.00046687
Iteration 16/25 | Loss: 0.00046687
Iteration 17/25 | Loss: 0.00046687
Iteration 18/25 | Loss: 0.00046687
Iteration 19/25 | Loss: 0.00046687
Iteration 20/25 | Loss: 0.00046687
Iteration 21/25 | Loss: 0.00046687
Iteration 22/25 | Loss: 0.00046687
Iteration 23/25 | Loss: 0.00046687
Iteration 24/25 | Loss: 0.00046687
Iteration 25/25 | Loss: 0.00046687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046687
Iteration 2/1000 | Loss: 0.00002620
Iteration 3/1000 | Loss: 0.00001986
Iteration 4/1000 | Loss: 0.00001668
Iteration 5/1000 | Loss: 0.00001530
Iteration 6/1000 | Loss: 0.00001426
Iteration 7/1000 | Loss: 0.00001357
Iteration 8/1000 | Loss: 0.00001293
Iteration 9/1000 | Loss: 0.00001253
Iteration 10/1000 | Loss: 0.00001223
Iteration 11/1000 | Loss: 0.00001197
Iteration 12/1000 | Loss: 0.00001186
Iteration 13/1000 | Loss: 0.00001159
Iteration 14/1000 | Loss: 0.00001141
Iteration 15/1000 | Loss: 0.00001138
Iteration 16/1000 | Loss: 0.00001115
Iteration 17/1000 | Loss: 0.00001107
Iteration 18/1000 | Loss: 0.00001101
Iteration 19/1000 | Loss: 0.00001097
Iteration 20/1000 | Loss: 0.00001097
Iteration 21/1000 | Loss: 0.00001096
Iteration 22/1000 | Loss: 0.00001091
Iteration 23/1000 | Loss: 0.00001090
Iteration 24/1000 | Loss: 0.00001089
Iteration 25/1000 | Loss: 0.00001088
Iteration 26/1000 | Loss: 0.00001088
Iteration 27/1000 | Loss: 0.00001088
Iteration 28/1000 | Loss: 0.00001088
Iteration 29/1000 | Loss: 0.00001088
Iteration 30/1000 | Loss: 0.00001087
Iteration 31/1000 | Loss: 0.00001087
Iteration 32/1000 | Loss: 0.00001087
Iteration 33/1000 | Loss: 0.00001086
Iteration 34/1000 | Loss: 0.00001086
Iteration 35/1000 | Loss: 0.00001086
Iteration 36/1000 | Loss: 0.00001086
Iteration 37/1000 | Loss: 0.00001086
Iteration 38/1000 | Loss: 0.00001086
Iteration 39/1000 | Loss: 0.00001086
Iteration 40/1000 | Loss: 0.00001086
Iteration 41/1000 | Loss: 0.00001086
Iteration 42/1000 | Loss: 0.00001086
Iteration 43/1000 | Loss: 0.00001086
Iteration 44/1000 | Loss: 0.00001085
Iteration 45/1000 | Loss: 0.00001084
Iteration 46/1000 | Loss: 0.00001084
Iteration 47/1000 | Loss: 0.00001084
Iteration 48/1000 | Loss: 0.00001083
Iteration 49/1000 | Loss: 0.00001083
Iteration 50/1000 | Loss: 0.00001083
Iteration 51/1000 | Loss: 0.00001083
Iteration 52/1000 | Loss: 0.00001083
Iteration 53/1000 | Loss: 0.00001082
Iteration 54/1000 | Loss: 0.00001082
Iteration 55/1000 | Loss: 0.00001082
Iteration 56/1000 | Loss: 0.00001082
Iteration 57/1000 | Loss: 0.00001082
Iteration 58/1000 | Loss: 0.00001082
Iteration 59/1000 | Loss: 0.00001082
Iteration 60/1000 | Loss: 0.00001082
Iteration 61/1000 | Loss: 0.00001082
Iteration 62/1000 | Loss: 0.00001081
Iteration 63/1000 | Loss: 0.00001081
Iteration 64/1000 | Loss: 0.00001081
Iteration 65/1000 | Loss: 0.00001081
Iteration 66/1000 | Loss: 0.00001081
Iteration 67/1000 | Loss: 0.00001080
Iteration 68/1000 | Loss: 0.00001080
Iteration 69/1000 | Loss: 0.00001080
Iteration 70/1000 | Loss: 0.00001080
Iteration 71/1000 | Loss: 0.00001080
Iteration 72/1000 | Loss: 0.00001079
Iteration 73/1000 | Loss: 0.00001079
Iteration 74/1000 | Loss: 0.00001079
Iteration 75/1000 | Loss: 0.00001079
Iteration 76/1000 | Loss: 0.00001079
Iteration 77/1000 | Loss: 0.00001079
Iteration 78/1000 | Loss: 0.00001079
Iteration 79/1000 | Loss: 0.00001079
Iteration 80/1000 | Loss: 0.00001079
Iteration 81/1000 | Loss: 0.00001079
Iteration 82/1000 | Loss: 0.00001078
Iteration 83/1000 | Loss: 0.00001078
Iteration 84/1000 | Loss: 0.00001078
Iteration 85/1000 | Loss: 0.00001078
Iteration 86/1000 | Loss: 0.00001077
Iteration 87/1000 | Loss: 0.00001077
Iteration 88/1000 | Loss: 0.00001077
Iteration 89/1000 | Loss: 0.00001077
Iteration 90/1000 | Loss: 0.00001077
Iteration 91/1000 | Loss: 0.00001077
Iteration 92/1000 | Loss: 0.00001076
Iteration 93/1000 | Loss: 0.00001076
Iteration 94/1000 | Loss: 0.00001076
Iteration 95/1000 | Loss: 0.00001075
Iteration 96/1000 | Loss: 0.00001075
Iteration 97/1000 | Loss: 0.00001075
Iteration 98/1000 | Loss: 0.00001075
Iteration 99/1000 | Loss: 0.00001074
Iteration 100/1000 | Loss: 0.00001074
Iteration 101/1000 | Loss: 0.00001074
Iteration 102/1000 | Loss: 0.00001074
Iteration 103/1000 | Loss: 0.00001074
Iteration 104/1000 | Loss: 0.00001074
Iteration 105/1000 | Loss: 0.00001074
Iteration 106/1000 | Loss: 0.00001073
Iteration 107/1000 | Loss: 0.00001073
Iteration 108/1000 | Loss: 0.00001073
Iteration 109/1000 | Loss: 0.00001073
Iteration 110/1000 | Loss: 0.00001073
Iteration 111/1000 | Loss: 0.00001072
Iteration 112/1000 | Loss: 0.00001072
Iteration 113/1000 | Loss: 0.00001072
Iteration 114/1000 | Loss: 0.00001072
Iteration 115/1000 | Loss: 0.00001072
Iteration 116/1000 | Loss: 0.00001072
Iteration 117/1000 | Loss: 0.00001072
Iteration 118/1000 | Loss: 0.00001072
Iteration 119/1000 | Loss: 0.00001072
Iteration 120/1000 | Loss: 0.00001072
Iteration 121/1000 | Loss: 0.00001072
Iteration 122/1000 | Loss: 0.00001071
Iteration 123/1000 | Loss: 0.00001071
Iteration 124/1000 | Loss: 0.00001071
Iteration 125/1000 | Loss: 0.00001071
Iteration 126/1000 | Loss: 0.00001070
Iteration 127/1000 | Loss: 0.00001070
Iteration 128/1000 | Loss: 0.00001070
Iteration 129/1000 | Loss: 0.00001070
Iteration 130/1000 | Loss: 0.00001070
Iteration 131/1000 | Loss: 0.00001070
Iteration 132/1000 | Loss: 0.00001070
Iteration 133/1000 | Loss: 0.00001069
Iteration 134/1000 | Loss: 0.00001069
Iteration 135/1000 | Loss: 0.00001069
Iteration 136/1000 | Loss: 0.00001069
Iteration 137/1000 | Loss: 0.00001068
Iteration 138/1000 | Loss: 0.00001068
Iteration 139/1000 | Loss: 0.00001068
Iteration 140/1000 | Loss: 0.00001068
Iteration 141/1000 | Loss: 0.00001068
Iteration 142/1000 | Loss: 0.00001067
Iteration 143/1000 | Loss: 0.00001067
Iteration 144/1000 | Loss: 0.00001067
Iteration 145/1000 | Loss: 0.00001067
Iteration 146/1000 | Loss: 0.00001067
Iteration 147/1000 | Loss: 0.00001067
Iteration 148/1000 | Loss: 0.00001067
Iteration 149/1000 | Loss: 0.00001066
Iteration 150/1000 | Loss: 0.00001066
Iteration 151/1000 | Loss: 0.00001066
Iteration 152/1000 | Loss: 0.00001066
Iteration 153/1000 | Loss: 0.00001066
Iteration 154/1000 | Loss: 0.00001066
Iteration 155/1000 | Loss: 0.00001066
Iteration 156/1000 | Loss: 0.00001066
Iteration 157/1000 | Loss: 0.00001065
Iteration 158/1000 | Loss: 0.00001065
Iteration 159/1000 | Loss: 0.00001065
Iteration 160/1000 | Loss: 0.00001065
Iteration 161/1000 | Loss: 0.00001065
Iteration 162/1000 | Loss: 0.00001064
Iteration 163/1000 | Loss: 0.00001064
Iteration 164/1000 | Loss: 0.00001064
Iteration 165/1000 | Loss: 0.00001064
Iteration 166/1000 | Loss: 0.00001063
Iteration 167/1000 | Loss: 0.00001063
Iteration 168/1000 | Loss: 0.00001063
Iteration 169/1000 | Loss: 0.00001063
Iteration 170/1000 | Loss: 0.00001063
Iteration 171/1000 | Loss: 0.00001063
Iteration 172/1000 | Loss: 0.00001063
Iteration 173/1000 | Loss: 0.00001063
Iteration 174/1000 | Loss: 0.00001063
Iteration 175/1000 | Loss: 0.00001062
Iteration 176/1000 | Loss: 0.00001062
Iteration 177/1000 | Loss: 0.00001062
Iteration 178/1000 | Loss: 0.00001062
Iteration 179/1000 | Loss: 0.00001062
Iteration 180/1000 | Loss: 0.00001062
Iteration 181/1000 | Loss: 0.00001062
Iteration 182/1000 | Loss: 0.00001062
Iteration 183/1000 | Loss: 0.00001062
Iteration 184/1000 | Loss: 0.00001062
Iteration 185/1000 | Loss: 0.00001062
Iteration 186/1000 | Loss: 0.00001062
Iteration 187/1000 | Loss: 0.00001061
Iteration 188/1000 | Loss: 0.00001061
Iteration 189/1000 | Loss: 0.00001061
Iteration 190/1000 | Loss: 0.00001061
Iteration 191/1000 | Loss: 0.00001061
Iteration 192/1000 | Loss: 0.00001061
Iteration 193/1000 | Loss: 0.00001061
Iteration 194/1000 | Loss: 0.00001061
Iteration 195/1000 | Loss: 0.00001061
Iteration 196/1000 | Loss: 0.00001061
Iteration 197/1000 | Loss: 0.00001061
Iteration 198/1000 | Loss: 0.00001061
Iteration 199/1000 | Loss: 0.00001061
Iteration 200/1000 | Loss: 0.00001061
Iteration 201/1000 | Loss: 0.00001061
Iteration 202/1000 | Loss: 0.00001061
Iteration 203/1000 | Loss: 0.00001061
Iteration 204/1000 | Loss: 0.00001061
Iteration 205/1000 | Loss: 0.00001061
Iteration 206/1000 | Loss: 0.00001061
Iteration 207/1000 | Loss: 0.00001061
Iteration 208/1000 | Loss: 0.00001061
Iteration 209/1000 | Loss: 0.00001061
Iteration 210/1000 | Loss: 0.00001061
Iteration 211/1000 | Loss: 0.00001061
Iteration 212/1000 | Loss: 0.00001061
Iteration 213/1000 | Loss: 0.00001061
Iteration 214/1000 | Loss: 0.00001061
Iteration 215/1000 | Loss: 0.00001061
Iteration 216/1000 | Loss: 0.00001061
Iteration 217/1000 | Loss: 0.00001061
Iteration 218/1000 | Loss: 0.00001061
Iteration 219/1000 | Loss: 0.00001061
Iteration 220/1000 | Loss: 0.00001061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.0606943760649301e-05, 1.0606943760649301e-05, 1.0606943760649301e-05, 1.0606943760649301e-05, 1.0606943760649301e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0606943760649301e-05

Optimization complete. Final v2v error: 2.778679847717285 mm

Highest mean error: 3.160348415374756 mm for frame 21

Lowest mean error: 2.5444376468658447 mm for frame 223

Saving results

Total time: 50.99059200286865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432980
Iteration 2/25 | Loss: 0.00129737
Iteration 3/25 | Loss: 0.00117286
Iteration 4/25 | Loss: 0.00115465
Iteration 5/25 | Loss: 0.00115037
Iteration 6/25 | Loss: 0.00114919
Iteration 7/25 | Loss: 0.00114897
Iteration 8/25 | Loss: 0.00114897
Iteration 9/25 | Loss: 0.00114897
Iteration 10/25 | Loss: 0.00114897
Iteration 11/25 | Loss: 0.00114897
Iteration 12/25 | Loss: 0.00114897
Iteration 13/25 | Loss: 0.00114897
Iteration 14/25 | Loss: 0.00114897
Iteration 15/25 | Loss: 0.00114897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011489728931337595, 0.0011489728931337595, 0.0011489728931337595, 0.0011489728931337595, 0.0011489728931337595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011489728931337595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.99700546
Iteration 2/25 | Loss: 0.00063543
Iteration 3/25 | Loss: 0.00063540
Iteration 4/25 | Loss: 0.00063540
Iteration 5/25 | Loss: 0.00063540
Iteration 6/25 | Loss: 0.00063540
Iteration 7/25 | Loss: 0.00063540
Iteration 8/25 | Loss: 0.00063540
Iteration 9/25 | Loss: 0.00063540
Iteration 10/25 | Loss: 0.00063540
Iteration 11/25 | Loss: 0.00063540
Iteration 12/25 | Loss: 0.00063540
Iteration 13/25 | Loss: 0.00063540
Iteration 14/25 | Loss: 0.00063540
Iteration 15/25 | Loss: 0.00063540
Iteration 16/25 | Loss: 0.00063540
Iteration 17/25 | Loss: 0.00063540
Iteration 18/25 | Loss: 0.00063540
Iteration 19/25 | Loss: 0.00063540
Iteration 20/25 | Loss: 0.00063540
Iteration 21/25 | Loss: 0.00063540
Iteration 22/25 | Loss: 0.00063540
Iteration 23/25 | Loss: 0.00063540
Iteration 24/25 | Loss: 0.00063540
Iteration 25/25 | Loss: 0.00063540

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063540
Iteration 2/1000 | Loss: 0.00004125
Iteration 3/1000 | Loss: 0.00002598
Iteration 4/1000 | Loss: 0.00002351
Iteration 5/1000 | Loss: 0.00002258
Iteration 6/1000 | Loss: 0.00002199
Iteration 7/1000 | Loss: 0.00002140
Iteration 8/1000 | Loss: 0.00002102
Iteration 9/1000 | Loss: 0.00002072
Iteration 10/1000 | Loss: 0.00002046
Iteration 11/1000 | Loss: 0.00002027
Iteration 12/1000 | Loss: 0.00002020
Iteration 13/1000 | Loss: 0.00002015
Iteration 14/1000 | Loss: 0.00002012
Iteration 15/1000 | Loss: 0.00002010
Iteration 16/1000 | Loss: 0.00002009
Iteration 17/1000 | Loss: 0.00002005
Iteration 18/1000 | Loss: 0.00002005
Iteration 19/1000 | Loss: 0.00002004
Iteration 20/1000 | Loss: 0.00002003
Iteration 21/1000 | Loss: 0.00002003
Iteration 22/1000 | Loss: 0.00002002
Iteration 23/1000 | Loss: 0.00002000
Iteration 24/1000 | Loss: 0.00002000
Iteration 25/1000 | Loss: 0.00001999
Iteration 26/1000 | Loss: 0.00001999
Iteration 27/1000 | Loss: 0.00001999
Iteration 28/1000 | Loss: 0.00001999
Iteration 29/1000 | Loss: 0.00001999
Iteration 30/1000 | Loss: 0.00001999
Iteration 31/1000 | Loss: 0.00001996
Iteration 32/1000 | Loss: 0.00001995
Iteration 33/1000 | Loss: 0.00001995
Iteration 34/1000 | Loss: 0.00001995
Iteration 35/1000 | Loss: 0.00001994
Iteration 36/1000 | Loss: 0.00001994
Iteration 37/1000 | Loss: 0.00001994
Iteration 38/1000 | Loss: 0.00001993
Iteration 39/1000 | Loss: 0.00001992
Iteration 40/1000 | Loss: 0.00001992
Iteration 41/1000 | Loss: 0.00001992
Iteration 42/1000 | Loss: 0.00001992
Iteration 43/1000 | Loss: 0.00001991
Iteration 44/1000 | Loss: 0.00001991
Iteration 45/1000 | Loss: 0.00001991
Iteration 46/1000 | Loss: 0.00001991
Iteration 47/1000 | Loss: 0.00001990
Iteration 48/1000 | Loss: 0.00001989
Iteration 49/1000 | Loss: 0.00001989
Iteration 50/1000 | Loss: 0.00001988
Iteration 51/1000 | Loss: 0.00001988
Iteration 52/1000 | Loss: 0.00001987
Iteration 53/1000 | Loss: 0.00001987
Iteration 54/1000 | Loss: 0.00001987
Iteration 55/1000 | Loss: 0.00001987
Iteration 56/1000 | Loss: 0.00001986
Iteration 57/1000 | Loss: 0.00001986
Iteration 58/1000 | Loss: 0.00001985
Iteration 59/1000 | Loss: 0.00001985
Iteration 60/1000 | Loss: 0.00001985
Iteration 61/1000 | Loss: 0.00001985
Iteration 62/1000 | Loss: 0.00001984
Iteration 63/1000 | Loss: 0.00001984
Iteration 64/1000 | Loss: 0.00001984
Iteration 65/1000 | Loss: 0.00001984
Iteration 66/1000 | Loss: 0.00001984
Iteration 67/1000 | Loss: 0.00001984
Iteration 68/1000 | Loss: 0.00001984
Iteration 69/1000 | Loss: 0.00001984
Iteration 70/1000 | Loss: 0.00001984
Iteration 71/1000 | Loss: 0.00001983
Iteration 72/1000 | Loss: 0.00001983
Iteration 73/1000 | Loss: 0.00001983
Iteration 74/1000 | Loss: 0.00001982
Iteration 75/1000 | Loss: 0.00001982
Iteration 76/1000 | Loss: 0.00001982
Iteration 77/1000 | Loss: 0.00001982
Iteration 78/1000 | Loss: 0.00001981
Iteration 79/1000 | Loss: 0.00001981
Iteration 80/1000 | Loss: 0.00001981
Iteration 81/1000 | Loss: 0.00001981
Iteration 82/1000 | Loss: 0.00001981
Iteration 83/1000 | Loss: 0.00001980
Iteration 84/1000 | Loss: 0.00001980
Iteration 85/1000 | Loss: 0.00001980
Iteration 86/1000 | Loss: 0.00001980
Iteration 87/1000 | Loss: 0.00001980
Iteration 88/1000 | Loss: 0.00001979
Iteration 89/1000 | Loss: 0.00001979
Iteration 90/1000 | Loss: 0.00001979
Iteration 91/1000 | Loss: 0.00001979
Iteration 92/1000 | Loss: 0.00001978
Iteration 93/1000 | Loss: 0.00001978
Iteration 94/1000 | Loss: 0.00001978
Iteration 95/1000 | Loss: 0.00001978
Iteration 96/1000 | Loss: 0.00001978
Iteration 97/1000 | Loss: 0.00001978
Iteration 98/1000 | Loss: 0.00001977
Iteration 99/1000 | Loss: 0.00001977
Iteration 100/1000 | Loss: 0.00001977
Iteration 101/1000 | Loss: 0.00001977
Iteration 102/1000 | Loss: 0.00001977
Iteration 103/1000 | Loss: 0.00001977
Iteration 104/1000 | Loss: 0.00001977
Iteration 105/1000 | Loss: 0.00001977
Iteration 106/1000 | Loss: 0.00001977
Iteration 107/1000 | Loss: 0.00001977
Iteration 108/1000 | Loss: 0.00001977
Iteration 109/1000 | Loss: 0.00001976
Iteration 110/1000 | Loss: 0.00001976
Iteration 111/1000 | Loss: 0.00001976
Iteration 112/1000 | Loss: 0.00001976
Iteration 113/1000 | Loss: 0.00001975
Iteration 114/1000 | Loss: 0.00001975
Iteration 115/1000 | Loss: 0.00001975
Iteration 116/1000 | Loss: 0.00001975
Iteration 117/1000 | Loss: 0.00001975
Iteration 118/1000 | Loss: 0.00001974
Iteration 119/1000 | Loss: 0.00001974
Iteration 120/1000 | Loss: 0.00001974
Iteration 121/1000 | Loss: 0.00001974
Iteration 122/1000 | Loss: 0.00001974
Iteration 123/1000 | Loss: 0.00001974
Iteration 124/1000 | Loss: 0.00001974
Iteration 125/1000 | Loss: 0.00001974
Iteration 126/1000 | Loss: 0.00001974
Iteration 127/1000 | Loss: 0.00001974
Iteration 128/1000 | Loss: 0.00001974
Iteration 129/1000 | Loss: 0.00001974
Iteration 130/1000 | Loss: 0.00001974
Iteration 131/1000 | Loss: 0.00001974
Iteration 132/1000 | Loss: 0.00001974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.9738277842407115e-05, 1.9738277842407115e-05, 1.9738277842407115e-05, 1.9738277842407115e-05, 1.9738277842407115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9738277842407115e-05

Optimization complete. Final v2v error: 3.755519151687622 mm

Highest mean error: 4.2503132820129395 mm for frame 59

Lowest mean error: 3.299129009246826 mm for frame 5

Saving results

Total time: 36.741095542907715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413571
Iteration 2/25 | Loss: 0.00112931
Iteration 3/25 | Loss: 0.00107749
Iteration 4/25 | Loss: 0.00107121
Iteration 5/25 | Loss: 0.00106932
Iteration 6/25 | Loss: 0.00106914
Iteration 7/25 | Loss: 0.00106914
Iteration 8/25 | Loss: 0.00106914
Iteration 9/25 | Loss: 0.00106914
Iteration 10/25 | Loss: 0.00106914
Iteration 11/25 | Loss: 0.00106914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010691351490095258, 0.0010691351490095258, 0.0010691351490095258, 0.0010691351490095258, 0.0010691351490095258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010691351490095258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39162850
Iteration 2/25 | Loss: 0.00068489
Iteration 3/25 | Loss: 0.00068489
Iteration 4/25 | Loss: 0.00068489
Iteration 5/25 | Loss: 0.00068489
Iteration 6/25 | Loss: 0.00068489
Iteration 7/25 | Loss: 0.00068489
Iteration 8/25 | Loss: 0.00068489
Iteration 9/25 | Loss: 0.00068489
Iteration 10/25 | Loss: 0.00068489
Iteration 11/25 | Loss: 0.00068489
Iteration 12/25 | Loss: 0.00068489
Iteration 13/25 | Loss: 0.00068489
Iteration 14/25 | Loss: 0.00068489
Iteration 15/25 | Loss: 0.00068489
Iteration 16/25 | Loss: 0.00068489
Iteration 17/25 | Loss: 0.00068489
Iteration 18/25 | Loss: 0.00068489
Iteration 19/25 | Loss: 0.00068489
Iteration 20/25 | Loss: 0.00068489
Iteration 21/25 | Loss: 0.00068489
Iteration 22/25 | Loss: 0.00068489
Iteration 23/25 | Loss: 0.00068489
Iteration 24/25 | Loss: 0.00068489
Iteration 25/25 | Loss: 0.00068489

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068489
Iteration 2/1000 | Loss: 0.00002091
Iteration 3/1000 | Loss: 0.00001303
Iteration 4/1000 | Loss: 0.00001176
Iteration 5/1000 | Loss: 0.00001119
Iteration 6/1000 | Loss: 0.00001089
Iteration 7/1000 | Loss: 0.00001065
Iteration 8/1000 | Loss: 0.00001064
Iteration 9/1000 | Loss: 0.00001050
Iteration 10/1000 | Loss: 0.00001040
Iteration 11/1000 | Loss: 0.00001040
Iteration 12/1000 | Loss: 0.00001022
Iteration 13/1000 | Loss: 0.00001016
Iteration 14/1000 | Loss: 0.00001005
Iteration 15/1000 | Loss: 0.00001004
Iteration 16/1000 | Loss: 0.00001004
Iteration 17/1000 | Loss: 0.00001003
Iteration 18/1000 | Loss: 0.00001002
Iteration 19/1000 | Loss: 0.00001002
Iteration 20/1000 | Loss: 0.00001001
Iteration 21/1000 | Loss: 0.00001000
Iteration 22/1000 | Loss: 0.00000999
Iteration 23/1000 | Loss: 0.00000999
Iteration 24/1000 | Loss: 0.00000998
Iteration 25/1000 | Loss: 0.00000997
Iteration 26/1000 | Loss: 0.00000995
Iteration 27/1000 | Loss: 0.00000994
Iteration 28/1000 | Loss: 0.00000994
Iteration 29/1000 | Loss: 0.00000994
Iteration 30/1000 | Loss: 0.00000991
Iteration 31/1000 | Loss: 0.00000991
Iteration 32/1000 | Loss: 0.00000990
Iteration 33/1000 | Loss: 0.00000990
Iteration 34/1000 | Loss: 0.00000990
Iteration 35/1000 | Loss: 0.00000989
Iteration 36/1000 | Loss: 0.00000989
Iteration 37/1000 | Loss: 0.00000988
Iteration 38/1000 | Loss: 0.00000988
Iteration 39/1000 | Loss: 0.00000987
Iteration 40/1000 | Loss: 0.00000987
Iteration 41/1000 | Loss: 0.00000986
Iteration 42/1000 | Loss: 0.00000986
Iteration 43/1000 | Loss: 0.00000986
Iteration 44/1000 | Loss: 0.00000986
Iteration 45/1000 | Loss: 0.00000986
Iteration 46/1000 | Loss: 0.00000985
Iteration 47/1000 | Loss: 0.00000985
Iteration 48/1000 | Loss: 0.00000985
Iteration 49/1000 | Loss: 0.00000985
Iteration 50/1000 | Loss: 0.00000985
Iteration 51/1000 | Loss: 0.00000984
Iteration 52/1000 | Loss: 0.00000984
Iteration 53/1000 | Loss: 0.00000984
Iteration 54/1000 | Loss: 0.00000984
Iteration 55/1000 | Loss: 0.00000983
Iteration 56/1000 | Loss: 0.00000983
Iteration 57/1000 | Loss: 0.00000983
Iteration 58/1000 | Loss: 0.00000983
Iteration 59/1000 | Loss: 0.00000982
Iteration 60/1000 | Loss: 0.00000982
Iteration 61/1000 | Loss: 0.00000981
Iteration 62/1000 | Loss: 0.00000981
Iteration 63/1000 | Loss: 0.00000981
Iteration 64/1000 | Loss: 0.00000981
Iteration 65/1000 | Loss: 0.00000980
Iteration 66/1000 | Loss: 0.00000980
Iteration 67/1000 | Loss: 0.00000980
Iteration 68/1000 | Loss: 0.00000980
Iteration 69/1000 | Loss: 0.00000980
Iteration 70/1000 | Loss: 0.00000979
Iteration 71/1000 | Loss: 0.00000979
Iteration 72/1000 | Loss: 0.00000979
Iteration 73/1000 | Loss: 0.00000979
Iteration 74/1000 | Loss: 0.00000979
Iteration 75/1000 | Loss: 0.00000978
Iteration 76/1000 | Loss: 0.00000978
Iteration 77/1000 | Loss: 0.00000978
Iteration 78/1000 | Loss: 0.00000978
Iteration 79/1000 | Loss: 0.00000978
Iteration 80/1000 | Loss: 0.00000978
Iteration 81/1000 | Loss: 0.00000978
Iteration 82/1000 | Loss: 0.00000978
Iteration 83/1000 | Loss: 0.00000978
Iteration 84/1000 | Loss: 0.00000977
Iteration 85/1000 | Loss: 0.00000977
Iteration 86/1000 | Loss: 0.00000977
Iteration 87/1000 | Loss: 0.00000977
Iteration 88/1000 | Loss: 0.00000977
Iteration 89/1000 | Loss: 0.00000977
Iteration 90/1000 | Loss: 0.00000977
Iteration 91/1000 | Loss: 0.00000977
Iteration 92/1000 | Loss: 0.00000977
Iteration 93/1000 | Loss: 0.00000977
Iteration 94/1000 | Loss: 0.00000977
Iteration 95/1000 | Loss: 0.00000977
Iteration 96/1000 | Loss: 0.00000977
Iteration 97/1000 | Loss: 0.00000977
Iteration 98/1000 | Loss: 0.00000977
Iteration 99/1000 | Loss: 0.00000977
Iteration 100/1000 | Loss: 0.00000977
Iteration 101/1000 | Loss: 0.00000977
Iteration 102/1000 | Loss: 0.00000977
Iteration 103/1000 | Loss: 0.00000977
Iteration 104/1000 | Loss: 0.00000977
Iteration 105/1000 | Loss: 0.00000977
Iteration 106/1000 | Loss: 0.00000977
Iteration 107/1000 | Loss: 0.00000977
Iteration 108/1000 | Loss: 0.00000977
Iteration 109/1000 | Loss: 0.00000977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [9.774487807590049e-06, 9.774487807590049e-06, 9.774487807590049e-06, 9.774487807590049e-06, 9.774487807590049e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.774487807590049e-06

Optimization complete. Final v2v error: 2.7014973163604736 mm

Highest mean error: 2.781059741973877 mm for frame 40

Lowest mean error: 2.5905098915100098 mm for frame 28

Saving results

Total time: 29.14539670944214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066813
Iteration 2/25 | Loss: 0.00507419
Iteration 3/25 | Loss: 0.00338296
Iteration 4/25 | Loss: 0.00310386
Iteration 5/25 | Loss: 0.00274436
Iteration 6/25 | Loss: 0.00261693
Iteration 7/25 | Loss: 0.00233815
Iteration 8/25 | Loss: 0.00210433
Iteration 9/25 | Loss: 0.00210166
Iteration 10/25 | Loss: 0.00212499
Iteration 11/25 | Loss: 0.00187311
Iteration 12/25 | Loss: 0.00184880
Iteration 13/25 | Loss: 0.00184987
Iteration 14/25 | Loss: 0.00181059
Iteration 15/25 | Loss: 0.00178114
Iteration 16/25 | Loss: 0.00173846
Iteration 17/25 | Loss: 0.00171822
Iteration 18/25 | Loss: 0.00171071
Iteration 19/25 | Loss: 0.00171261
Iteration 20/25 | Loss: 0.00169895
Iteration 21/25 | Loss: 0.00168737
Iteration 22/25 | Loss: 0.00168597
Iteration 23/25 | Loss: 0.00169339
Iteration 24/25 | Loss: 0.00169071
Iteration 25/25 | Loss: 0.00168148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.40047607
Iteration 2/25 | Loss: 0.00594885
Iteration 3/25 | Loss: 0.00348636
Iteration 4/25 | Loss: 0.00305318
Iteration 5/25 | Loss: 0.00214470
Iteration 6/25 | Loss: 0.00214470
Iteration 7/25 | Loss: 0.00214469
Iteration 8/25 | Loss: 0.00214469
Iteration 9/25 | Loss: 0.00214469
Iteration 10/25 | Loss: 0.00214469
Iteration 11/25 | Loss: 0.00214469
Iteration 12/25 | Loss: 0.00214469
Iteration 13/25 | Loss: 0.00214469
Iteration 14/25 | Loss: 0.00214469
Iteration 15/25 | Loss: 0.00214469
Iteration 16/25 | Loss: 0.00214469
Iteration 17/25 | Loss: 0.00214469
Iteration 18/25 | Loss: 0.00214469
Iteration 19/25 | Loss: 0.00214469
Iteration 20/25 | Loss: 0.00214469
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002144687110558152, 0.002144687110558152, 0.002144687110558152, 0.002144687110558152, 0.002144687110558152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002144687110558152

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214469
Iteration 2/1000 | Loss: 0.00063509
Iteration 3/1000 | Loss: 0.00096005
Iteration 4/1000 | Loss: 0.00060737
Iteration 5/1000 | Loss: 0.00051168
Iteration 6/1000 | Loss: 0.00096638
Iteration 7/1000 | Loss: 0.00070283
Iteration 8/1000 | Loss: 0.00073525
Iteration 9/1000 | Loss: 0.00044815
Iteration 10/1000 | Loss: 0.00037897
Iteration 11/1000 | Loss: 0.00090438
Iteration 12/1000 | Loss: 0.00033771
Iteration 13/1000 | Loss: 0.00038144
Iteration 14/1000 | Loss: 0.00041864
Iteration 15/1000 | Loss: 0.00045023
Iteration 16/1000 | Loss: 0.00100860
Iteration 17/1000 | Loss: 0.00050188
Iteration 18/1000 | Loss: 0.00057013
Iteration 19/1000 | Loss: 0.00068540
Iteration 20/1000 | Loss: 0.00062650
Iteration 21/1000 | Loss: 0.00032515
Iteration 22/1000 | Loss: 0.00055792
Iteration 23/1000 | Loss: 0.00066644
Iteration 24/1000 | Loss: 0.00045367
Iteration 25/1000 | Loss: 0.00041097
Iteration 26/1000 | Loss: 0.00053477
Iteration 27/1000 | Loss: 0.00054737
Iteration 28/1000 | Loss: 0.00048694
Iteration 29/1000 | Loss: 0.00032517
Iteration 30/1000 | Loss: 0.00041423
Iteration 31/1000 | Loss: 0.00033126
Iteration 32/1000 | Loss: 0.00034381
Iteration 33/1000 | Loss: 0.00041207
Iteration 34/1000 | Loss: 0.00044595
Iteration 35/1000 | Loss: 0.00048530
Iteration 36/1000 | Loss: 0.00045980
Iteration 37/1000 | Loss: 0.00043834
Iteration 38/1000 | Loss: 0.00043476
Iteration 39/1000 | Loss: 0.00039198
Iteration 40/1000 | Loss: 0.00037627
Iteration 41/1000 | Loss: 0.00043724
Iteration 42/1000 | Loss: 0.00039328
Iteration 43/1000 | Loss: 0.00037062
Iteration 44/1000 | Loss: 0.00052269
Iteration 45/1000 | Loss: 0.00047419
Iteration 46/1000 | Loss: 0.00052869
Iteration 47/1000 | Loss: 0.00037094
Iteration 48/1000 | Loss: 0.00043115
Iteration 49/1000 | Loss: 0.00060669
Iteration 50/1000 | Loss: 0.00040304
Iteration 51/1000 | Loss: 0.00054265
Iteration 52/1000 | Loss: 0.00087798
Iteration 53/1000 | Loss: 0.00056140
Iteration 54/1000 | Loss: 0.00053981
Iteration 55/1000 | Loss: 0.00052655
Iteration 56/1000 | Loss: 0.00046353
Iteration 57/1000 | Loss: 0.00049399
Iteration 58/1000 | Loss: 0.00056571
Iteration 59/1000 | Loss: 0.00063567
Iteration 60/1000 | Loss: 0.00056268
Iteration 61/1000 | Loss: 0.00038152
Iteration 62/1000 | Loss: 0.00037274
Iteration 63/1000 | Loss: 0.00066707
Iteration 64/1000 | Loss: 0.00030599
Iteration 65/1000 | Loss: 0.00037464
Iteration 66/1000 | Loss: 0.00042858
Iteration 67/1000 | Loss: 0.00059065
Iteration 68/1000 | Loss: 0.00059547
Iteration 69/1000 | Loss: 0.00046685
Iteration 70/1000 | Loss: 0.00052284
Iteration 71/1000 | Loss: 0.00075888
Iteration 72/1000 | Loss: 0.00077086
Iteration 73/1000 | Loss: 0.00040768
Iteration 74/1000 | Loss: 0.00042134
Iteration 75/1000 | Loss: 0.00050092
Iteration 76/1000 | Loss: 0.00045048
Iteration 77/1000 | Loss: 0.00049294
Iteration 78/1000 | Loss: 0.00064435
Iteration 79/1000 | Loss: 0.00051316
Iteration 80/1000 | Loss: 0.00041856
Iteration 81/1000 | Loss: 0.00037194
Iteration 82/1000 | Loss: 0.00038665
Iteration 83/1000 | Loss: 0.00036423
Iteration 84/1000 | Loss: 0.00034374
Iteration 85/1000 | Loss: 0.00034555
Iteration 86/1000 | Loss: 0.00025122
Iteration 87/1000 | Loss: 0.00022256
Iteration 88/1000 | Loss: 0.00036697
Iteration 89/1000 | Loss: 0.00063378
Iteration 90/1000 | Loss: 0.00069718
Iteration 91/1000 | Loss: 0.00032936
Iteration 92/1000 | Loss: 0.00039635
Iteration 93/1000 | Loss: 0.00036645
Iteration 94/1000 | Loss: 0.00039971
Iteration 95/1000 | Loss: 0.00042231
Iteration 96/1000 | Loss: 0.00114662
Iteration 97/1000 | Loss: 0.00047663
Iteration 98/1000 | Loss: 0.00034643
Iteration 99/1000 | Loss: 0.00054276
Iteration 100/1000 | Loss: 0.00068206
Iteration 101/1000 | Loss: 0.00076021
Iteration 102/1000 | Loss: 0.00046829
Iteration 103/1000 | Loss: 0.00121370
Iteration 104/1000 | Loss: 0.00035309
Iteration 105/1000 | Loss: 0.00041108
Iteration 106/1000 | Loss: 0.00036058
Iteration 107/1000 | Loss: 0.00031665
Iteration 108/1000 | Loss: 0.00037832
Iteration 109/1000 | Loss: 0.00056963
Iteration 110/1000 | Loss: 0.00055746
Iteration 111/1000 | Loss: 0.00035658
Iteration 112/1000 | Loss: 0.00036430
Iteration 113/1000 | Loss: 0.00040826
Iteration 114/1000 | Loss: 0.00038685
Iteration 115/1000 | Loss: 0.00030540
Iteration 116/1000 | Loss: 0.00032397
Iteration 117/1000 | Loss: 0.00025469
Iteration 118/1000 | Loss: 0.00026068
Iteration 119/1000 | Loss: 0.00039432
Iteration 120/1000 | Loss: 0.00039416
Iteration 121/1000 | Loss: 0.00042204
Iteration 122/1000 | Loss: 0.00039550
Iteration 123/1000 | Loss: 0.00033552
Iteration 124/1000 | Loss: 0.00021064
Iteration 125/1000 | Loss: 0.00035048
Iteration 126/1000 | Loss: 0.00033032
Iteration 127/1000 | Loss: 0.00034123
Iteration 128/1000 | Loss: 0.00030514
Iteration 129/1000 | Loss: 0.00032472
Iteration 130/1000 | Loss: 0.00029227
Iteration 131/1000 | Loss: 0.00034099
Iteration 132/1000 | Loss: 0.00036047
Iteration 133/1000 | Loss: 0.00036043
Iteration 134/1000 | Loss: 0.00039516
Iteration 135/1000 | Loss: 0.00031348
Iteration 136/1000 | Loss: 0.00034202
Iteration 137/1000 | Loss: 0.00075603
Iteration 138/1000 | Loss: 0.00026345
Iteration 139/1000 | Loss: 0.00043695
Iteration 140/1000 | Loss: 0.00026104
Iteration 141/1000 | Loss: 0.00025281
Iteration 142/1000 | Loss: 0.00025140
Iteration 143/1000 | Loss: 0.00025530
Iteration 144/1000 | Loss: 0.00025284
Iteration 145/1000 | Loss: 0.00023550
Iteration 146/1000 | Loss: 0.00025052
Iteration 147/1000 | Loss: 0.00017408
Iteration 148/1000 | Loss: 0.00021859
Iteration 149/1000 | Loss: 0.00028928
Iteration 150/1000 | Loss: 0.00079397
Iteration 151/1000 | Loss: 0.00031115
Iteration 152/1000 | Loss: 0.00026756
Iteration 153/1000 | Loss: 0.00036362
Iteration 154/1000 | Loss: 0.00025029
Iteration 155/1000 | Loss: 0.00043466
Iteration 156/1000 | Loss: 0.00041257
Iteration 157/1000 | Loss: 0.00061496
Iteration 158/1000 | Loss: 0.00041402
Iteration 159/1000 | Loss: 0.00082641
Iteration 160/1000 | Loss: 0.00024760
Iteration 161/1000 | Loss: 0.00043078
Iteration 162/1000 | Loss: 0.00040836
Iteration 163/1000 | Loss: 0.00036946
Iteration 164/1000 | Loss: 0.00025702
Iteration 165/1000 | Loss: 0.00042417
Iteration 166/1000 | Loss: 0.00035147
Iteration 167/1000 | Loss: 0.00040968
Iteration 168/1000 | Loss: 0.00032340
Iteration 169/1000 | Loss: 0.00036875
Iteration 170/1000 | Loss: 0.00021842
Iteration 171/1000 | Loss: 0.00026226
Iteration 172/1000 | Loss: 0.00029863
Iteration 173/1000 | Loss: 0.00022275
Iteration 174/1000 | Loss: 0.00021355
Iteration 175/1000 | Loss: 0.00023773
Iteration 176/1000 | Loss: 0.00026189
Iteration 177/1000 | Loss: 0.00025710
Iteration 178/1000 | Loss: 0.00025608
Iteration 179/1000 | Loss: 0.00023671
Iteration 180/1000 | Loss: 0.00024282
Iteration 181/1000 | Loss: 0.00026350
Iteration 182/1000 | Loss: 0.00028472
Iteration 183/1000 | Loss: 0.00026288
Iteration 184/1000 | Loss: 0.00027477
Iteration 185/1000 | Loss: 0.00025187
Iteration 186/1000 | Loss: 0.00025589
Iteration 187/1000 | Loss: 0.00023336
Iteration 188/1000 | Loss: 0.00022424
Iteration 189/1000 | Loss: 0.00018018
Iteration 190/1000 | Loss: 0.00022144
Iteration 191/1000 | Loss: 0.00021976
Iteration 192/1000 | Loss: 0.00024147
Iteration 193/1000 | Loss: 0.00024686
Iteration 194/1000 | Loss: 0.00024161
Iteration 195/1000 | Loss: 0.00026150
Iteration 196/1000 | Loss: 0.00024478
Iteration 197/1000 | Loss: 0.00026130
Iteration 198/1000 | Loss: 0.00024688
Iteration 199/1000 | Loss: 0.00025949
Iteration 200/1000 | Loss: 0.00023907
Iteration 201/1000 | Loss: 0.00027752
Iteration 202/1000 | Loss: 0.00024482
Iteration 203/1000 | Loss: 0.00025054
Iteration 204/1000 | Loss: 0.00023844
Iteration 205/1000 | Loss: 0.00025067
Iteration 206/1000 | Loss: 0.00024171
Iteration 207/1000 | Loss: 0.00016571
Iteration 208/1000 | Loss: 0.00044036
Iteration 209/1000 | Loss: 0.00061556
Iteration 210/1000 | Loss: 0.00027408
Iteration 211/1000 | Loss: 0.00052215
Iteration 212/1000 | Loss: 0.00025461
Iteration 213/1000 | Loss: 0.00024946
Iteration 214/1000 | Loss: 0.00023104
Iteration 215/1000 | Loss: 0.00021909
Iteration 216/1000 | Loss: 0.00023701
Iteration 217/1000 | Loss: 0.00023720
Iteration 218/1000 | Loss: 0.00024388
Iteration 219/1000 | Loss: 0.00023862
Iteration 220/1000 | Loss: 0.00024422
Iteration 221/1000 | Loss: 0.00022891
Iteration 222/1000 | Loss: 0.00022170
Iteration 223/1000 | Loss: 0.00024352
Iteration 224/1000 | Loss: 0.00024962
Iteration 225/1000 | Loss: 0.00024676
Iteration 226/1000 | Loss: 0.00026061
Iteration 227/1000 | Loss: 0.00025858
Iteration 228/1000 | Loss: 0.00038070
Iteration 229/1000 | Loss: 0.00028580
Iteration 230/1000 | Loss: 0.00014157
Iteration 231/1000 | Loss: 0.00022118
Iteration 232/1000 | Loss: 0.00010686
Iteration 233/1000 | Loss: 0.00014983
Iteration 234/1000 | Loss: 0.00025776
Iteration 235/1000 | Loss: 0.00024269
Iteration 236/1000 | Loss: 0.00022653
Iteration 237/1000 | Loss: 0.00021165
Iteration 238/1000 | Loss: 0.00015430
Iteration 239/1000 | Loss: 0.00018192
Iteration 240/1000 | Loss: 0.00017526
Iteration 241/1000 | Loss: 0.00018504
Iteration 242/1000 | Loss: 0.00012566
Iteration 243/1000 | Loss: 0.00011135
Iteration 244/1000 | Loss: 0.00008909
Iteration 245/1000 | Loss: 0.00025977
Iteration 246/1000 | Loss: 0.00008386
Iteration 247/1000 | Loss: 0.00015478
Iteration 248/1000 | Loss: 0.00018169
Iteration 249/1000 | Loss: 0.00013806
Iteration 250/1000 | Loss: 0.00009905
Iteration 251/1000 | Loss: 0.00017843
Iteration 252/1000 | Loss: 0.00013927
Iteration 253/1000 | Loss: 0.00020122
Iteration 254/1000 | Loss: 0.00031133
Iteration 255/1000 | Loss: 0.00026312
Iteration 256/1000 | Loss: 0.00036711
Iteration 257/1000 | Loss: 0.00025378
Iteration 258/1000 | Loss: 0.00023351
Iteration 259/1000 | Loss: 0.00008349
Iteration 260/1000 | Loss: 0.00008130
Iteration 261/1000 | Loss: 0.00007929
Iteration 262/1000 | Loss: 0.00019334
Iteration 263/1000 | Loss: 0.00015166
Iteration 264/1000 | Loss: 0.00012858
Iteration 265/1000 | Loss: 0.00008183
Iteration 266/1000 | Loss: 0.00013089
Iteration 267/1000 | Loss: 0.00013264
Iteration 268/1000 | Loss: 0.00013048
Iteration 269/1000 | Loss: 0.00012096
Iteration 270/1000 | Loss: 0.00016851
Iteration 271/1000 | Loss: 0.00016352
Iteration 272/1000 | Loss: 0.00014877
Iteration 273/1000 | Loss: 0.00014845
Iteration 274/1000 | Loss: 0.00014368
Iteration 275/1000 | Loss: 0.00017306
Iteration 276/1000 | Loss: 0.00008937
Iteration 277/1000 | Loss: 0.00008118
Iteration 278/1000 | Loss: 0.00013396
Iteration 279/1000 | Loss: 0.00012263
Iteration 280/1000 | Loss: 0.00013042
Iteration 281/1000 | Loss: 0.00061061
Iteration 282/1000 | Loss: 0.00025862
Iteration 283/1000 | Loss: 0.00028892
Iteration 284/1000 | Loss: 0.00022173
Iteration 285/1000 | Loss: 0.00026182
Iteration 286/1000 | Loss: 0.00017663
Iteration 287/1000 | Loss: 0.00015140
Iteration 288/1000 | Loss: 0.00014778
Iteration 289/1000 | Loss: 0.00014330
Iteration 290/1000 | Loss: 0.00013498
Iteration 291/1000 | Loss: 0.00017162
Iteration 292/1000 | Loss: 0.00016609
Iteration 293/1000 | Loss: 0.00017843
Iteration 294/1000 | Loss: 0.00022173
Iteration 295/1000 | Loss: 0.00011548
Iteration 296/1000 | Loss: 0.00014126
Iteration 297/1000 | Loss: 0.00018980
Iteration 298/1000 | Loss: 0.00012914
Iteration 299/1000 | Loss: 0.00012031
Iteration 300/1000 | Loss: 0.00007699
Iteration 301/1000 | Loss: 0.00007541
Iteration 302/1000 | Loss: 0.00010521
Iteration 303/1000 | Loss: 0.00009935
Iteration 304/1000 | Loss: 0.00009637
Iteration 305/1000 | Loss: 0.00010112
Iteration 306/1000 | Loss: 0.00007513
Iteration 307/1000 | Loss: 0.00007448
Iteration 308/1000 | Loss: 0.00026483
Iteration 309/1000 | Loss: 0.00018004
Iteration 310/1000 | Loss: 0.00007461
Iteration 311/1000 | Loss: 0.00007389
Iteration 312/1000 | Loss: 0.00007349
Iteration 313/1000 | Loss: 0.00028440
Iteration 314/1000 | Loss: 0.00018554
Iteration 315/1000 | Loss: 0.00008774
Iteration 316/1000 | Loss: 0.00007376
Iteration 317/1000 | Loss: 0.00007309
Iteration 318/1000 | Loss: 0.00007285
Iteration 319/1000 | Loss: 0.00028278
Iteration 320/1000 | Loss: 0.00012790
Iteration 321/1000 | Loss: 0.00007374
Iteration 322/1000 | Loss: 0.00007285
Iteration 323/1000 | Loss: 0.00027314
Iteration 324/1000 | Loss: 0.00014597
Iteration 325/1000 | Loss: 0.00008651
Iteration 326/1000 | Loss: 0.00025356
Iteration 327/1000 | Loss: 0.00011459
Iteration 328/1000 | Loss: 0.00008034
Iteration 329/1000 | Loss: 0.00007618
Iteration 330/1000 | Loss: 0.00007338
Iteration 331/1000 | Loss: 0.00020273
Iteration 332/1000 | Loss: 0.00009788
Iteration 333/1000 | Loss: 0.00007372
Iteration 334/1000 | Loss: 0.00015009
Iteration 335/1000 | Loss: 0.00009596
Iteration 336/1000 | Loss: 0.00014291
Iteration 337/1000 | Loss: 0.00007372
Iteration 338/1000 | Loss: 0.00007252
Iteration 339/1000 | Loss: 0.00007182
Iteration 340/1000 | Loss: 0.00007135
Iteration 341/1000 | Loss: 0.00007129
Iteration 342/1000 | Loss: 0.00017583
Iteration 343/1000 | Loss: 0.00015659
Iteration 344/1000 | Loss: 0.00020208
Iteration 345/1000 | Loss: 0.00018139
Iteration 346/1000 | Loss: 0.00018149
Iteration 347/1000 | Loss: 0.00015202
Iteration 348/1000 | Loss: 0.00023554
Iteration 349/1000 | Loss: 0.00016125
Iteration 350/1000 | Loss: 0.00015108
Iteration 351/1000 | Loss: 0.00017240
Iteration 352/1000 | Loss: 0.00008760
Iteration 353/1000 | Loss: 0.00008951
Iteration 354/1000 | Loss: 0.00012752
Iteration 355/1000 | Loss: 0.00013012
Iteration 356/1000 | Loss: 0.00008060
Iteration 357/1000 | Loss: 0.00007158
Iteration 358/1000 | Loss: 0.00007122
Iteration 359/1000 | Loss: 0.00007093
Iteration 360/1000 | Loss: 0.00007071
Iteration 361/1000 | Loss: 0.00007043
Iteration 362/1000 | Loss: 0.00007041
Iteration 363/1000 | Loss: 0.00007029
Iteration 364/1000 | Loss: 0.00007023
Iteration 365/1000 | Loss: 0.00007014
Iteration 366/1000 | Loss: 0.00007014
Iteration 367/1000 | Loss: 0.00007013
Iteration 368/1000 | Loss: 0.00007013
Iteration 369/1000 | Loss: 0.00007013
Iteration 370/1000 | Loss: 0.00007013
Iteration 371/1000 | Loss: 0.00007013
Iteration 372/1000 | Loss: 0.00007013
Iteration 373/1000 | Loss: 0.00007012
Iteration 374/1000 | Loss: 0.00007012
Iteration 375/1000 | Loss: 0.00007012
Iteration 376/1000 | Loss: 0.00007012
Iteration 377/1000 | Loss: 0.00007012
Iteration 378/1000 | Loss: 0.00007012
Iteration 379/1000 | Loss: 0.00007012
Iteration 380/1000 | Loss: 0.00007010
Iteration 381/1000 | Loss: 0.00007010
Iteration 382/1000 | Loss: 0.00007010
Iteration 383/1000 | Loss: 0.00007009
Iteration 384/1000 | Loss: 0.00007007
Iteration 385/1000 | Loss: 0.00007005
Iteration 386/1000 | Loss: 0.00007005
Iteration 387/1000 | Loss: 0.00007005
Iteration 388/1000 | Loss: 0.00007005
Iteration 389/1000 | Loss: 0.00007005
Iteration 390/1000 | Loss: 0.00007004
Iteration 391/1000 | Loss: 0.00007004
Iteration 392/1000 | Loss: 0.00007004
Iteration 393/1000 | Loss: 0.00007004
Iteration 394/1000 | Loss: 0.00007004
Iteration 395/1000 | Loss: 0.00007004
Iteration 396/1000 | Loss: 0.00007004
Iteration 397/1000 | Loss: 0.00007002
Iteration 398/1000 | Loss: 0.00007002
Iteration 399/1000 | Loss: 0.00007001
Iteration 400/1000 | Loss: 0.00007001
Iteration 401/1000 | Loss: 0.00007000
Iteration 402/1000 | Loss: 0.00007000
Iteration 403/1000 | Loss: 0.00007000
Iteration 404/1000 | Loss: 0.00007000
Iteration 405/1000 | Loss: 0.00007000
Iteration 406/1000 | Loss: 0.00007000
Iteration 407/1000 | Loss: 0.00006999
Iteration 408/1000 | Loss: 0.00006999
Iteration 409/1000 | Loss: 0.00006999
Iteration 410/1000 | Loss: 0.00006998
Iteration 411/1000 | Loss: 0.00006998
Iteration 412/1000 | Loss: 0.00006998
Iteration 413/1000 | Loss: 0.00006998
Iteration 414/1000 | Loss: 0.00006998
Iteration 415/1000 | Loss: 0.00006997
Iteration 416/1000 | Loss: 0.00006997
Iteration 417/1000 | Loss: 0.00006997
Iteration 418/1000 | Loss: 0.00006997
Iteration 419/1000 | Loss: 0.00006997
Iteration 420/1000 | Loss: 0.00006997
Iteration 421/1000 | Loss: 0.00006997
Iteration 422/1000 | Loss: 0.00006997
Iteration 423/1000 | Loss: 0.00006997
Iteration 424/1000 | Loss: 0.00006996
Iteration 425/1000 | Loss: 0.00006996
Iteration 426/1000 | Loss: 0.00006996
Iteration 427/1000 | Loss: 0.00006996
Iteration 428/1000 | Loss: 0.00006995
Iteration 429/1000 | Loss: 0.00006995
Iteration 430/1000 | Loss: 0.00006994
Iteration 431/1000 | Loss: 0.00006994
Iteration 432/1000 | Loss: 0.00006994
Iteration 433/1000 | Loss: 0.00006994
Iteration 434/1000 | Loss: 0.00006993
Iteration 435/1000 | Loss: 0.00006993
Iteration 436/1000 | Loss: 0.00006992
Iteration 437/1000 | Loss: 0.00006991
Iteration 438/1000 | Loss: 0.00006990
Iteration 439/1000 | Loss: 0.00006990
Iteration 440/1000 | Loss: 0.00006990
Iteration 441/1000 | Loss: 0.00006990
Iteration 442/1000 | Loss: 0.00006990
Iteration 443/1000 | Loss: 0.00006989
Iteration 444/1000 | Loss: 0.00006989
Iteration 445/1000 | Loss: 0.00006989
Iteration 446/1000 | Loss: 0.00006989
Iteration 447/1000 | Loss: 0.00006989
Iteration 448/1000 | Loss: 0.00006989
Iteration 449/1000 | Loss: 0.00006988
Iteration 450/1000 | Loss: 0.00006988
Iteration 451/1000 | Loss: 0.00006988
Iteration 452/1000 | Loss: 0.00006988
Iteration 453/1000 | Loss: 0.00006988
Iteration 454/1000 | Loss: 0.00006987
Iteration 455/1000 | Loss: 0.00006987
Iteration 456/1000 | Loss: 0.00006987
Iteration 457/1000 | Loss: 0.00006987
Iteration 458/1000 | Loss: 0.00006987
Iteration 459/1000 | Loss: 0.00006987
Iteration 460/1000 | Loss: 0.00006987
Iteration 461/1000 | Loss: 0.00006987
Iteration 462/1000 | Loss: 0.00006987
Iteration 463/1000 | Loss: 0.00006987
Iteration 464/1000 | Loss: 0.00006987
Iteration 465/1000 | Loss: 0.00006987
Iteration 466/1000 | Loss: 0.00006987
Iteration 467/1000 | Loss: 0.00006987
Iteration 468/1000 | Loss: 0.00006986
Iteration 469/1000 | Loss: 0.00006986
Iteration 470/1000 | Loss: 0.00006986
Iteration 471/1000 | Loss: 0.00006986
Iteration 472/1000 | Loss: 0.00006986
Iteration 473/1000 | Loss: 0.00006986
Iteration 474/1000 | Loss: 0.00006986
Iteration 475/1000 | Loss: 0.00006985
Iteration 476/1000 | Loss: 0.00006985
Iteration 477/1000 | Loss: 0.00006985
Iteration 478/1000 | Loss: 0.00006985
Iteration 479/1000 | Loss: 0.00006985
Iteration 480/1000 | Loss: 0.00006985
Iteration 481/1000 | Loss: 0.00006985
Iteration 482/1000 | Loss: 0.00006985
Iteration 483/1000 | Loss: 0.00006985
Iteration 484/1000 | Loss: 0.00006984
Iteration 485/1000 | Loss: 0.00006984
Iteration 486/1000 | Loss: 0.00006984
Iteration 487/1000 | Loss: 0.00006984
Iteration 488/1000 | Loss: 0.00006984
Iteration 489/1000 | Loss: 0.00006984
Iteration 490/1000 | Loss: 0.00006984
Iteration 491/1000 | Loss: 0.00006984
Iteration 492/1000 | Loss: 0.00006984
Iteration 493/1000 | Loss: 0.00006984
Iteration 494/1000 | Loss: 0.00006984
Iteration 495/1000 | Loss: 0.00006984
Iteration 496/1000 | Loss: 0.00006984
Iteration 497/1000 | Loss: 0.00006983
Iteration 498/1000 | Loss: 0.00006983
Iteration 499/1000 | Loss: 0.00006983
Iteration 500/1000 | Loss: 0.00006983
Iteration 501/1000 | Loss: 0.00006983
Iteration 502/1000 | Loss: 0.00006983
Iteration 503/1000 | Loss: 0.00006983
Iteration 504/1000 | Loss: 0.00006983
Iteration 505/1000 | Loss: 0.00006983
Iteration 506/1000 | Loss: 0.00006983
Iteration 507/1000 | Loss: 0.00006983
Iteration 508/1000 | Loss: 0.00006982
Iteration 509/1000 | Loss: 0.00006982
Iteration 510/1000 | Loss: 0.00006982
Iteration 511/1000 | Loss: 0.00006982
Iteration 512/1000 | Loss: 0.00006982
Iteration 513/1000 | Loss: 0.00006982
Iteration 514/1000 | Loss: 0.00006982
Iteration 515/1000 | Loss: 0.00006982
Iteration 516/1000 | Loss: 0.00006982
Iteration 517/1000 | Loss: 0.00006982
Iteration 518/1000 | Loss: 0.00006982
Iteration 519/1000 | Loss: 0.00006982
Iteration 520/1000 | Loss: 0.00017384
Iteration 521/1000 | Loss: 0.00017104
Iteration 522/1000 | Loss: 0.00007696
Iteration 523/1000 | Loss: 0.00015720
Iteration 524/1000 | Loss: 0.00009833
Iteration 525/1000 | Loss: 0.00103373
Iteration 526/1000 | Loss: 0.00086724
Iteration 527/1000 | Loss: 0.00045381
Iteration 528/1000 | Loss: 0.00022853
Iteration 529/1000 | Loss: 0.00046706
Iteration 530/1000 | Loss: 0.00039508
Iteration 531/1000 | Loss: 0.00009720
Iteration 532/1000 | Loss: 0.00008368
Iteration 533/1000 | Loss: 0.00008383
Iteration 534/1000 | Loss: 0.00008435
Iteration 535/1000 | Loss: 0.00006813
Iteration 536/1000 | Loss: 0.00006374
Iteration 537/1000 | Loss: 0.00006118
Iteration 538/1000 | Loss: 0.00005973
Iteration 539/1000 | Loss: 0.00005861
Iteration 540/1000 | Loss: 0.00005744
Iteration 541/1000 | Loss: 0.00005690
Iteration 542/1000 | Loss: 0.00005655
Iteration 543/1000 | Loss: 0.00005623
Iteration 544/1000 | Loss: 0.00005592
Iteration 545/1000 | Loss: 0.00005570
Iteration 546/1000 | Loss: 0.00005552
Iteration 547/1000 | Loss: 0.00005549
Iteration 548/1000 | Loss: 0.00005546
Iteration 549/1000 | Loss: 0.00005546
Iteration 550/1000 | Loss: 0.00005545
Iteration 551/1000 | Loss: 0.00005545
Iteration 552/1000 | Loss: 0.00005545
Iteration 553/1000 | Loss: 0.00005545
Iteration 554/1000 | Loss: 0.00005545
Iteration 555/1000 | Loss: 0.00005545
Iteration 556/1000 | Loss: 0.00005545
Iteration 557/1000 | Loss: 0.00005544
Iteration 558/1000 | Loss: 0.00005544
Iteration 559/1000 | Loss: 0.00005544
Iteration 560/1000 | Loss: 0.00005543
Iteration 561/1000 | Loss: 0.00005543
Iteration 562/1000 | Loss: 0.00005543
Iteration 563/1000 | Loss: 0.00005541
Iteration 564/1000 | Loss: 0.00005541
Iteration 565/1000 | Loss: 0.00005540
Iteration 566/1000 | Loss: 0.00005539
Iteration 567/1000 | Loss: 0.00005539
Iteration 568/1000 | Loss: 0.00005539
Iteration 569/1000 | Loss: 0.00005538
Iteration 570/1000 | Loss: 0.00005538
Iteration 571/1000 | Loss: 0.00005538
Iteration 572/1000 | Loss: 0.00005538
Iteration 573/1000 | Loss: 0.00005538
Iteration 574/1000 | Loss: 0.00005537
Iteration 575/1000 | Loss: 0.00005537
Iteration 576/1000 | Loss: 0.00005537
Iteration 577/1000 | Loss: 0.00005537
Iteration 578/1000 | Loss: 0.00005537
Iteration 579/1000 | Loss: 0.00005537
Iteration 580/1000 | Loss: 0.00005537
Iteration 581/1000 | Loss: 0.00005537
Iteration 582/1000 | Loss: 0.00005537
Iteration 583/1000 | Loss: 0.00005536
Iteration 584/1000 | Loss: 0.00005536
Iteration 585/1000 | Loss: 0.00005536
Iteration 586/1000 | Loss: 0.00005536
Iteration 587/1000 | Loss: 0.00005536
Iteration 588/1000 | Loss: 0.00005536
Iteration 589/1000 | Loss: 0.00005536
Iteration 590/1000 | Loss: 0.00005536
Iteration 591/1000 | Loss: 0.00005535
Iteration 592/1000 | Loss: 0.00005535
Iteration 593/1000 | Loss: 0.00005535
Iteration 594/1000 | Loss: 0.00005535
Iteration 595/1000 | Loss: 0.00005535
Iteration 596/1000 | Loss: 0.00005535
Iteration 597/1000 | Loss: 0.00005535
Iteration 598/1000 | Loss: 0.00005535
Iteration 599/1000 | Loss: 0.00005535
Iteration 600/1000 | Loss: 0.00005535
Iteration 601/1000 | Loss: 0.00005535
Iteration 602/1000 | Loss: 0.00005535
Iteration 603/1000 | Loss: 0.00005535
Iteration 604/1000 | Loss: 0.00005535
Iteration 605/1000 | Loss: 0.00005535
Iteration 606/1000 | Loss: 0.00005535
Iteration 607/1000 | Loss: 0.00005535
Iteration 608/1000 | Loss: 0.00005535
Iteration 609/1000 | Loss: 0.00005535
Iteration 610/1000 | Loss: 0.00005535
Iteration 611/1000 | Loss: 0.00005535
Iteration 612/1000 | Loss: 0.00005535
Iteration 613/1000 | Loss: 0.00005535
Iteration 614/1000 | Loss: 0.00005535
Iteration 615/1000 | Loss: 0.00005535
Iteration 616/1000 | Loss: 0.00005535
Iteration 617/1000 | Loss: 0.00005535
Iteration 618/1000 | Loss: 0.00005535
Iteration 619/1000 | Loss: 0.00005535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 619. Stopping optimization.
Last 5 losses: [5.53495847270824e-05, 5.53495847270824e-05, 5.53495847270824e-05, 5.53495847270824e-05, 5.53495847270824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.53495847270824e-05

Optimization complete. Final v2v error: 5.887749195098877 mm

Highest mean error: 7.0095062255859375 mm for frame 152

Lowest mean error: 4.418389320373535 mm for frame 10

Saving results

Total time: 703.6306834220886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01087058
Iteration 2/25 | Loss: 0.00220095
Iteration 3/25 | Loss: 0.00160171
Iteration 4/25 | Loss: 0.00154480
Iteration 5/25 | Loss: 0.00169361
Iteration 6/25 | Loss: 0.00161245
Iteration 7/25 | Loss: 0.00159588
Iteration 8/25 | Loss: 0.00162852
Iteration 9/25 | Loss: 0.00167101
Iteration 10/25 | Loss: 0.00159515
Iteration 11/25 | Loss: 0.00150649
Iteration 12/25 | Loss: 0.00155551
Iteration 13/25 | Loss: 0.00150437
Iteration 14/25 | Loss: 0.00153010
Iteration 15/25 | Loss: 0.00151136
Iteration 16/25 | Loss: 0.00146149
Iteration 17/25 | Loss: 0.00147300
Iteration 18/25 | Loss: 0.00143487
Iteration 19/25 | Loss: 0.00139115
Iteration 20/25 | Loss: 0.00140010
Iteration 21/25 | Loss: 0.00138327
Iteration 22/25 | Loss: 0.00135094
Iteration 23/25 | Loss: 0.00132997
Iteration 24/25 | Loss: 0.00131589
Iteration 25/25 | Loss: 0.00131206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24416840
Iteration 2/25 | Loss: 0.00335436
Iteration 3/25 | Loss: 0.00292175
Iteration 4/25 | Loss: 0.00292174
Iteration 5/25 | Loss: 0.00292174
Iteration 6/25 | Loss: 0.00292174
Iteration 7/25 | Loss: 0.00292174
Iteration 8/25 | Loss: 0.00292174
Iteration 9/25 | Loss: 0.00292174
Iteration 10/25 | Loss: 0.00292174
Iteration 11/25 | Loss: 0.00292174
Iteration 12/25 | Loss: 0.00292174
Iteration 13/25 | Loss: 0.00292174
Iteration 14/25 | Loss: 0.00292174
Iteration 15/25 | Loss: 0.00292174
Iteration 16/25 | Loss: 0.00292174
Iteration 17/25 | Loss: 0.00292174
Iteration 18/25 | Loss: 0.00292174
Iteration 19/25 | Loss: 0.00292174
Iteration 20/25 | Loss: 0.00292174
Iteration 21/25 | Loss: 0.00292174
Iteration 22/25 | Loss: 0.00292174
Iteration 23/25 | Loss: 0.00292174
Iteration 24/25 | Loss: 0.00292174
Iteration 25/25 | Loss: 0.00292174
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.002921742619946599, 0.002921742619946599, 0.002921742619946599, 0.002921742619946599, 0.002921742619946599]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002921742619946599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00292174
Iteration 2/1000 | Loss: 0.00201001
Iteration 3/1000 | Loss: 0.00097931
Iteration 4/1000 | Loss: 0.00154234
Iteration 5/1000 | Loss: 0.00169006
Iteration 6/1000 | Loss: 0.00256385
Iteration 7/1000 | Loss: 0.00169292
Iteration 8/1000 | Loss: 0.00138974
Iteration 9/1000 | Loss: 0.00137526
Iteration 10/1000 | Loss: 0.00091640
Iteration 11/1000 | Loss: 0.00104613
Iteration 12/1000 | Loss: 0.00135389
Iteration 13/1000 | Loss: 0.00067990
Iteration 14/1000 | Loss: 0.00065283
Iteration 15/1000 | Loss: 0.00050258
Iteration 16/1000 | Loss: 0.00120518
Iteration 17/1000 | Loss: 0.00087313
Iteration 18/1000 | Loss: 0.00078157
Iteration 19/1000 | Loss: 0.00078098
Iteration 20/1000 | Loss: 0.00039800
Iteration 21/1000 | Loss: 0.00179671
Iteration 22/1000 | Loss: 0.00067912
Iteration 23/1000 | Loss: 0.00082116
Iteration 24/1000 | Loss: 0.00099133
Iteration 25/1000 | Loss: 0.00132692
Iteration 26/1000 | Loss: 0.00097243
Iteration 27/1000 | Loss: 0.00068799
Iteration 28/1000 | Loss: 0.00079383
Iteration 29/1000 | Loss: 0.00070391
Iteration 30/1000 | Loss: 0.00118936
Iteration 31/1000 | Loss: 0.00057374
Iteration 32/1000 | Loss: 0.00046757
Iteration 33/1000 | Loss: 0.00030533
Iteration 34/1000 | Loss: 0.00026573
Iteration 35/1000 | Loss: 0.00031911
Iteration 36/1000 | Loss: 0.00027889
Iteration 37/1000 | Loss: 0.00080114
Iteration 38/1000 | Loss: 0.00037305
Iteration 39/1000 | Loss: 0.00157006
Iteration 40/1000 | Loss: 0.00115710
Iteration 41/1000 | Loss: 0.00083115
Iteration 42/1000 | Loss: 0.00053062
Iteration 43/1000 | Loss: 0.00058826
Iteration 44/1000 | Loss: 0.00212256
Iteration 45/1000 | Loss: 0.00081550
Iteration 46/1000 | Loss: 0.00101954
Iteration 47/1000 | Loss: 0.00142268
Iteration 48/1000 | Loss: 0.00121773
Iteration 49/1000 | Loss: 0.00067256
Iteration 50/1000 | Loss: 0.00047073
Iteration 51/1000 | Loss: 0.00115280
Iteration 52/1000 | Loss: 0.00112221
Iteration 53/1000 | Loss: 0.00100238
Iteration 54/1000 | Loss: 0.00098118
Iteration 55/1000 | Loss: 0.00037776
Iteration 56/1000 | Loss: 0.00033858
Iteration 57/1000 | Loss: 0.00055878
Iteration 58/1000 | Loss: 0.00097873
Iteration 59/1000 | Loss: 0.00027620
Iteration 60/1000 | Loss: 0.00050092
Iteration 61/1000 | Loss: 0.00016981
Iteration 62/1000 | Loss: 0.00014103
Iteration 63/1000 | Loss: 0.00018186
Iteration 64/1000 | Loss: 0.00022324
Iteration 65/1000 | Loss: 0.00062143
Iteration 66/1000 | Loss: 0.00039790
Iteration 67/1000 | Loss: 0.00069199
Iteration 68/1000 | Loss: 0.00059915
Iteration 69/1000 | Loss: 0.00033668
Iteration 70/1000 | Loss: 0.00074341
Iteration 71/1000 | Loss: 0.00044891
Iteration 72/1000 | Loss: 0.00016312
Iteration 73/1000 | Loss: 0.00009644
Iteration 74/1000 | Loss: 0.00014453
Iteration 75/1000 | Loss: 0.00040169
Iteration 76/1000 | Loss: 0.00042266
Iteration 77/1000 | Loss: 0.00035692
Iteration 78/1000 | Loss: 0.00008894
Iteration 79/1000 | Loss: 0.00010530
Iteration 80/1000 | Loss: 0.00008161
Iteration 81/1000 | Loss: 0.00070075
Iteration 82/1000 | Loss: 0.00041965
Iteration 83/1000 | Loss: 0.00046782
Iteration 84/1000 | Loss: 0.00038181
Iteration 85/1000 | Loss: 0.00123989
Iteration 86/1000 | Loss: 0.00072102
Iteration 87/1000 | Loss: 0.00009298
Iteration 88/1000 | Loss: 0.00037966
Iteration 89/1000 | Loss: 0.00087347
Iteration 90/1000 | Loss: 0.00042147
Iteration 91/1000 | Loss: 0.00039569
Iteration 92/1000 | Loss: 0.00008284
Iteration 93/1000 | Loss: 0.00007934
Iteration 94/1000 | Loss: 0.00040473
Iteration 95/1000 | Loss: 0.00062097
Iteration 96/1000 | Loss: 0.00029365
Iteration 97/1000 | Loss: 0.00027244
Iteration 98/1000 | Loss: 0.00058840
Iteration 99/1000 | Loss: 0.00047675
Iteration 100/1000 | Loss: 0.00040368
Iteration 101/1000 | Loss: 0.00023807
Iteration 102/1000 | Loss: 0.00028615
Iteration 103/1000 | Loss: 0.00006886
Iteration 104/1000 | Loss: 0.00028084
Iteration 105/1000 | Loss: 0.00023115
Iteration 106/1000 | Loss: 0.00025430
Iteration 107/1000 | Loss: 0.00011256
Iteration 108/1000 | Loss: 0.00010610
Iteration 109/1000 | Loss: 0.00007342
Iteration 110/1000 | Loss: 0.00011073
Iteration 111/1000 | Loss: 0.00018643
Iteration 112/1000 | Loss: 0.00014854
Iteration 113/1000 | Loss: 0.00014507
Iteration 114/1000 | Loss: 0.00007866
Iteration 115/1000 | Loss: 0.00037912
Iteration 116/1000 | Loss: 0.00058146
Iteration 117/1000 | Loss: 0.00024577
Iteration 118/1000 | Loss: 0.00034597
Iteration 119/1000 | Loss: 0.00028991
Iteration 120/1000 | Loss: 0.00008547
Iteration 121/1000 | Loss: 0.00008682
Iteration 122/1000 | Loss: 0.00007169
Iteration 123/1000 | Loss: 0.00006487
Iteration 124/1000 | Loss: 0.00008546
Iteration 125/1000 | Loss: 0.00006100
Iteration 126/1000 | Loss: 0.00009225
Iteration 127/1000 | Loss: 0.00043891
Iteration 128/1000 | Loss: 0.00035895
Iteration 129/1000 | Loss: 0.00008890
Iteration 130/1000 | Loss: 0.00007534
Iteration 131/1000 | Loss: 0.00007694
Iteration 132/1000 | Loss: 0.00005485
Iteration 133/1000 | Loss: 0.00008653
Iteration 134/1000 | Loss: 0.00009308
Iteration 135/1000 | Loss: 0.00008534
Iteration 136/1000 | Loss: 0.00008985
Iteration 137/1000 | Loss: 0.00045504
Iteration 138/1000 | Loss: 0.00098908
Iteration 139/1000 | Loss: 0.00065594
Iteration 140/1000 | Loss: 0.00052772
Iteration 141/1000 | Loss: 0.00059735
Iteration 142/1000 | Loss: 0.00037492
Iteration 143/1000 | Loss: 0.00059078
Iteration 144/1000 | Loss: 0.00032101
Iteration 145/1000 | Loss: 0.00042849
Iteration 146/1000 | Loss: 0.00152484
Iteration 147/1000 | Loss: 0.00011616
Iteration 148/1000 | Loss: 0.00008587
Iteration 149/1000 | Loss: 0.00008832
Iteration 150/1000 | Loss: 0.00009084
Iteration 151/1000 | Loss: 0.00057810
Iteration 152/1000 | Loss: 0.00030941
Iteration 153/1000 | Loss: 0.00007746
Iteration 154/1000 | Loss: 0.00006333
Iteration 155/1000 | Loss: 0.00008053
Iteration 156/1000 | Loss: 0.00009915
Iteration 157/1000 | Loss: 0.00006048
Iteration 158/1000 | Loss: 0.00003900
Iteration 159/1000 | Loss: 0.00050501
Iteration 160/1000 | Loss: 0.00035311
Iteration 161/1000 | Loss: 0.00008510
Iteration 162/1000 | Loss: 0.00005856
Iteration 163/1000 | Loss: 0.00008540
Iteration 164/1000 | Loss: 0.00003367
Iteration 165/1000 | Loss: 0.00030506
Iteration 166/1000 | Loss: 0.00034913
Iteration 167/1000 | Loss: 0.00002561
Iteration 168/1000 | Loss: 0.00002379
Iteration 169/1000 | Loss: 0.00002230
Iteration 170/1000 | Loss: 0.00042765
Iteration 171/1000 | Loss: 0.00024180
Iteration 172/1000 | Loss: 0.00019518
Iteration 173/1000 | Loss: 0.00002167
Iteration 174/1000 | Loss: 0.00039887
Iteration 175/1000 | Loss: 0.00029066
Iteration 176/1000 | Loss: 0.00016576
Iteration 177/1000 | Loss: 0.00019620
Iteration 178/1000 | Loss: 0.00003192
Iteration 179/1000 | Loss: 0.00002798
Iteration 180/1000 | Loss: 0.00002565
Iteration 181/1000 | Loss: 0.00002371
Iteration 182/1000 | Loss: 0.00002254
Iteration 183/1000 | Loss: 0.00002206
Iteration 184/1000 | Loss: 0.00040073
Iteration 185/1000 | Loss: 0.00017977
Iteration 186/1000 | Loss: 0.00033043
Iteration 187/1000 | Loss: 0.00025277
Iteration 188/1000 | Loss: 0.00002700
Iteration 189/1000 | Loss: 0.00002187
Iteration 190/1000 | Loss: 0.00038749
Iteration 191/1000 | Loss: 0.00002463
Iteration 192/1000 | Loss: 0.00002126
Iteration 193/1000 | Loss: 0.00001955
Iteration 194/1000 | Loss: 0.00001907
Iteration 195/1000 | Loss: 0.00001863
Iteration 196/1000 | Loss: 0.00001811
Iteration 197/1000 | Loss: 0.00001781
Iteration 198/1000 | Loss: 0.00001759
Iteration 199/1000 | Loss: 0.00001735
Iteration 200/1000 | Loss: 0.00001722
Iteration 201/1000 | Loss: 0.00001720
Iteration 202/1000 | Loss: 0.00001714
Iteration 203/1000 | Loss: 0.00001714
Iteration 204/1000 | Loss: 0.00001710
Iteration 205/1000 | Loss: 0.00001707
Iteration 206/1000 | Loss: 0.00001706
Iteration 207/1000 | Loss: 0.00001706
Iteration 208/1000 | Loss: 0.00001704
Iteration 209/1000 | Loss: 0.00001703
Iteration 210/1000 | Loss: 0.00001697
Iteration 211/1000 | Loss: 0.00001696
Iteration 212/1000 | Loss: 0.00001694
Iteration 213/1000 | Loss: 0.00001693
Iteration 214/1000 | Loss: 0.00001685
Iteration 215/1000 | Loss: 0.00001684
Iteration 216/1000 | Loss: 0.00001678
Iteration 217/1000 | Loss: 0.00001677
Iteration 218/1000 | Loss: 0.00001677
Iteration 219/1000 | Loss: 0.00001676
Iteration 220/1000 | Loss: 0.00001662
Iteration 221/1000 | Loss: 0.00001661
Iteration 222/1000 | Loss: 0.00002891
Iteration 223/1000 | Loss: 0.00001782
Iteration 224/1000 | Loss: 0.00001699
Iteration 225/1000 | Loss: 0.00001639
Iteration 226/1000 | Loss: 0.00001610
Iteration 227/1000 | Loss: 0.00001602
Iteration 228/1000 | Loss: 0.00001594
Iteration 229/1000 | Loss: 0.00001582
Iteration 230/1000 | Loss: 0.00001582
Iteration 231/1000 | Loss: 0.00001578
Iteration 232/1000 | Loss: 0.00001578
Iteration 233/1000 | Loss: 0.00001576
Iteration 234/1000 | Loss: 0.00001575
Iteration 235/1000 | Loss: 0.00001574
Iteration 236/1000 | Loss: 0.00001574
Iteration 237/1000 | Loss: 0.00001573
Iteration 238/1000 | Loss: 0.00001573
Iteration 239/1000 | Loss: 0.00001572
Iteration 240/1000 | Loss: 0.00001572
Iteration 241/1000 | Loss: 0.00001572
Iteration 242/1000 | Loss: 0.00001572
Iteration 243/1000 | Loss: 0.00001571
Iteration 244/1000 | Loss: 0.00001571
Iteration 245/1000 | Loss: 0.00001571
Iteration 246/1000 | Loss: 0.00001570
Iteration 247/1000 | Loss: 0.00001570
Iteration 248/1000 | Loss: 0.00001570
Iteration 249/1000 | Loss: 0.00001570
Iteration 250/1000 | Loss: 0.00001570
Iteration 251/1000 | Loss: 0.00001570
Iteration 252/1000 | Loss: 0.00001569
Iteration 253/1000 | Loss: 0.00001569
Iteration 254/1000 | Loss: 0.00001569
Iteration 255/1000 | Loss: 0.00001569
Iteration 256/1000 | Loss: 0.00001569
Iteration 257/1000 | Loss: 0.00001568
Iteration 258/1000 | Loss: 0.00001568
Iteration 259/1000 | Loss: 0.00001568
Iteration 260/1000 | Loss: 0.00001567
Iteration 261/1000 | Loss: 0.00001567
Iteration 262/1000 | Loss: 0.00001567
Iteration 263/1000 | Loss: 0.00001566
Iteration 264/1000 | Loss: 0.00001566
Iteration 265/1000 | Loss: 0.00001566
Iteration 266/1000 | Loss: 0.00001565
Iteration 267/1000 | Loss: 0.00001565
Iteration 268/1000 | Loss: 0.00001565
Iteration 269/1000 | Loss: 0.00001565
Iteration 270/1000 | Loss: 0.00001564
Iteration 271/1000 | Loss: 0.00001564
Iteration 272/1000 | Loss: 0.00001564
Iteration 273/1000 | Loss: 0.00001564
Iteration 274/1000 | Loss: 0.00001563
Iteration 275/1000 | Loss: 0.00001563
Iteration 276/1000 | Loss: 0.00001563
Iteration 277/1000 | Loss: 0.00001563
Iteration 278/1000 | Loss: 0.00001563
Iteration 279/1000 | Loss: 0.00001563
Iteration 280/1000 | Loss: 0.00001563
Iteration 281/1000 | Loss: 0.00001563
Iteration 282/1000 | Loss: 0.00001563
Iteration 283/1000 | Loss: 0.00001563
Iteration 284/1000 | Loss: 0.00001563
Iteration 285/1000 | Loss: 0.00001563
Iteration 286/1000 | Loss: 0.00001562
Iteration 287/1000 | Loss: 0.00001562
Iteration 288/1000 | Loss: 0.00001562
Iteration 289/1000 | Loss: 0.00001562
Iteration 290/1000 | Loss: 0.00001562
Iteration 291/1000 | Loss: 0.00001562
Iteration 292/1000 | Loss: 0.00001562
Iteration 293/1000 | Loss: 0.00001562
Iteration 294/1000 | Loss: 0.00001562
Iteration 295/1000 | Loss: 0.00001562
Iteration 296/1000 | Loss: 0.00001562
Iteration 297/1000 | Loss: 0.00001562
Iteration 298/1000 | Loss: 0.00001562
Iteration 299/1000 | Loss: 0.00001562
Iteration 300/1000 | Loss: 0.00001562
Iteration 301/1000 | Loss: 0.00001562
Iteration 302/1000 | Loss: 0.00001562
Iteration 303/1000 | Loss: 0.00001562
Iteration 304/1000 | Loss: 0.00001562
Iteration 305/1000 | Loss: 0.00001562
Iteration 306/1000 | Loss: 0.00001562
Iteration 307/1000 | Loss: 0.00001562
Iteration 308/1000 | Loss: 0.00001562
Iteration 309/1000 | Loss: 0.00001562
Iteration 310/1000 | Loss: 0.00001562
Iteration 311/1000 | Loss: 0.00001562
Iteration 312/1000 | Loss: 0.00001562
Iteration 313/1000 | Loss: 0.00001562
Iteration 314/1000 | Loss: 0.00001562
Iteration 315/1000 | Loss: 0.00001562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [1.56162404891802e-05, 1.56162404891802e-05, 1.56162404891802e-05, 1.56162404891802e-05, 1.56162404891802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.56162404891802e-05

Optimization complete. Final v2v error: 3.3530426025390625 mm

Highest mean error: 4.097420692443848 mm for frame 40

Lowest mean error: 3.2549831867218018 mm for frame 17

Saving results

Total time: 340.3721785545349
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000135
Iteration 2/25 | Loss: 0.01000135
Iteration 3/25 | Loss: 0.00266200
Iteration 4/25 | Loss: 0.00175409
Iteration 5/25 | Loss: 0.00160868
Iteration 6/25 | Loss: 0.00161887
Iteration 7/25 | Loss: 0.00164599
Iteration 8/25 | Loss: 0.00164244
Iteration 9/25 | Loss: 0.00150498
Iteration 10/25 | Loss: 0.00138768
Iteration 11/25 | Loss: 0.00133898
Iteration 12/25 | Loss: 0.00131887
Iteration 13/25 | Loss: 0.00131855
Iteration 14/25 | Loss: 0.00130944
Iteration 15/25 | Loss: 0.00130429
Iteration 16/25 | Loss: 0.00130953
Iteration 17/25 | Loss: 0.00130365
Iteration 18/25 | Loss: 0.00129407
Iteration 19/25 | Loss: 0.00127221
Iteration 20/25 | Loss: 0.00126338
Iteration 21/25 | Loss: 0.00126607
Iteration 22/25 | Loss: 0.00126543
Iteration 23/25 | Loss: 0.00124866
Iteration 24/25 | Loss: 0.00124504
Iteration 25/25 | Loss: 0.00124895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39099240
Iteration 2/25 | Loss: 0.00256880
Iteration 3/25 | Loss: 0.00233783
Iteration 4/25 | Loss: 0.00233783
Iteration 5/25 | Loss: 0.00233783
Iteration 6/25 | Loss: 0.00233783
Iteration 7/25 | Loss: 0.00233783
Iteration 8/25 | Loss: 0.00233783
Iteration 9/25 | Loss: 0.00233783
Iteration 10/25 | Loss: 0.00233783
Iteration 11/25 | Loss: 0.00233783
Iteration 12/25 | Loss: 0.00233783
Iteration 13/25 | Loss: 0.00233783
Iteration 14/25 | Loss: 0.00233783
Iteration 15/25 | Loss: 0.00233783
Iteration 16/25 | Loss: 0.00233783
Iteration 17/25 | Loss: 0.00233783
Iteration 18/25 | Loss: 0.00233783
Iteration 19/25 | Loss: 0.00233783
Iteration 20/25 | Loss: 0.00233783
Iteration 21/25 | Loss: 0.00233783
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0023378299083560705, 0.0023378299083560705, 0.0023378299083560705, 0.0023378299083560705, 0.0023378299083560705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023378299083560705

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233783
Iteration 2/1000 | Loss: 0.00150624
Iteration 3/1000 | Loss: 0.00131828
Iteration 4/1000 | Loss: 0.00063295
Iteration 5/1000 | Loss: 0.00090242
Iteration 6/1000 | Loss: 0.00041245
Iteration 7/1000 | Loss: 0.00060174
Iteration 8/1000 | Loss: 0.00283469
Iteration 9/1000 | Loss: 0.00215945
Iteration 10/1000 | Loss: 0.00150851
Iteration 11/1000 | Loss: 0.00044022
Iteration 12/1000 | Loss: 0.00109837
Iteration 13/1000 | Loss: 0.00109907
Iteration 14/1000 | Loss: 0.00027727
Iteration 15/1000 | Loss: 0.00094533
Iteration 16/1000 | Loss: 0.00101909
Iteration 17/1000 | Loss: 0.00024392
Iteration 18/1000 | Loss: 0.00017121
Iteration 19/1000 | Loss: 0.00034465
Iteration 20/1000 | Loss: 0.00025682
Iteration 21/1000 | Loss: 0.00037397
Iteration 22/1000 | Loss: 0.00051201
Iteration 23/1000 | Loss: 0.00050103
Iteration 24/1000 | Loss: 0.00024865
Iteration 25/1000 | Loss: 0.00020065
Iteration 26/1000 | Loss: 0.00019330
Iteration 27/1000 | Loss: 0.00036127
Iteration 28/1000 | Loss: 0.00042385
Iteration 29/1000 | Loss: 0.00022195
Iteration 30/1000 | Loss: 0.00036344
Iteration 31/1000 | Loss: 0.00015869
Iteration 32/1000 | Loss: 0.00017413
Iteration 33/1000 | Loss: 0.00019205
Iteration 34/1000 | Loss: 0.00016194
Iteration 35/1000 | Loss: 0.00017383
Iteration 36/1000 | Loss: 0.00018215
Iteration 37/1000 | Loss: 0.00015858
Iteration 38/1000 | Loss: 0.00021532
Iteration 39/1000 | Loss: 0.00033118
Iteration 40/1000 | Loss: 0.00022948
Iteration 41/1000 | Loss: 0.00014384
Iteration 42/1000 | Loss: 0.00015235
Iteration 43/1000 | Loss: 0.00007595
Iteration 44/1000 | Loss: 0.00009926
Iteration 45/1000 | Loss: 0.00010232
Iteration 46/1000 | Loss: 0.00012149
Iteration 47/1000 | Loss: 0.00011755
Iteration 48/1000 | Loss: 0.00013857
Iteration 49/1000 | Loss: 0.00025495
Iteration 50/1000 | Loss: 0.00013909
Iteration 51/1000 | Loss: 0.00009686
Iteration 52/1000 | Loss: 0.00010949
Iteration 53/1000 | Loss: 0.00010937
Iteration 54/1000 | Loss: 0.00011677
Iteration 55/1000 | Loss: 0.00010952
Iteration 56/1000 | Loss: 0.00065464
Iteration 57/1000 | Loss: 0.00129172
Iteration 58/1000 | Loss: 0.00035642
Iteration 59/1000 | Loss: 0.00014098
Iteration 60/1000 | Loss: 0.00007559
Iteration 61/1000 | Loss: 0.00007301
Iteration 62/1000 | Loss: 0.00012153
Iteration 63/1000 | Loss: 0.00015198
Iteration 64/1000 | Loss: 0.00011420
Iteration 65/1000 | Loss: 0.00011136
Iteration 66/1000 | Loss: 0.00007545
Iteration 67/1000 | Loss: 0.00009958
Iteration 68/1000 | Loss: 0.00005007
Iteration 69/1000 | Loss: 0.00005330
Iteration 70/1000 | Loss: 0.00004641
Iteration 71/1000 | Loss: 0.00004232
Iteration 72/1000 | Loss: 0.00016842
Iteration 73/1000 | Loss: 0.00068464
Iteration 74/1000 | Loss: 0.00031409
Iteration 75/1000 | Loss: 0.00005931
Iteration 76/1000 | Loss: 0.00056372
Iteration 77/1000 | Loss: 0.00015285
Iteration 78/1000 | Loss: 0.00005564
Iteration 79/1000 | Loss: 0.00045406
Iteration 80/1000 | Loss: 0.00066757
Iteration 81/1000 | Loss: 0.00014464
Iteration 82/1000 | Loss: 0.00014225
Iteration 83/1000 | Loss: 0.00013472
Iteration 84/1000 | Loss: 0.00013969
Iteration 85/1000 | Loss: 0.00013673
Iteration 86/1000 | Loss: 0.00013741
Iteration 87/1000 | Loss: 0.00009184
Iteration 88/1000 | Loss: 0.00005257
Iteration 89/1000 | Loss: 0.00003437
Iteration 90/1000 | Loss: 0.00003744
Iteration 91/1000 | Loss: 0.00003233
Iteration 92/1000 | Loss: 0.00003018
Iteration 93/1000 | Loss: 0.00019006
Iteration 94/1000 | Loss: 0.00003423
Iteration 95/1000 | Loss: 0.00047000
Iteration 96/1000 | Loss: 0.00114515
Iteration 97/1000 | Loss: 0.00029066
Iteration 98/1000 | Loss: 0.00074380
Iteration 99/1000 | Loss: 0.00040144
Iteration 100/1000 | Loss: 0.00020225
Iteration 101/1000 | Loss: 0.00039816
Iteration 102/1000 | Loss: 0.00005235
Iteration 103/1000 | Loss: 0.00004524
Iteration 104/1000 | Loss: 0.00002947
Iteration 105/1000 | Loss: 0.00053114
Iteration 106/1000 | Loss: 0.00081431
Iteration 107/1000 | Loss: 0.00036617
Iteration 108/1000 | Loss: 0.00005022
Iteration 109/1000 | Loss: 0.00037868
Iteration 110/1000 | Loss: 0.00025893
Iteration 111/1000 | Loss: 0.00011557
Iteration 112/1000 | Loss: 0.00011146
Iteration 113/1000 | Loss: 0.00018886
Iteration 114/1000 | Loss: 0.00014118
Iteration 115/1000 | Loss: 0.00023922
Iteration 116/1000 | Loss: 0.00011694
Iteration 117/1000 | Loss: 0.00024409
Iteration 118/1000 | Loss: 0.00012742
Iteration 119/1000 | Loss: 0.00002930
Iteration 120/1000 | Loss: 0.00002479
Iteration 121/1000 | Loss: 0.00008887
Iteration 122/1000 | Loss: 0.00002194
Iteration 123/1000 | Loss: 0.00001901
Iteration 124/1000 | Loss: 0.00001848
Iteration 125/1000 | Loss: 0.00001809
Iteration 126/1000 | Loss: 0.00001575
Iteration 127/1000 | Loss: 0.00004016
Iteration 128/1000 | Loss: 0.00001554
Iteration 129/1000 | Loss: 0.00001425
Iteration 130/1000 | Loss: 0.00001907
Iteration 131/1000 | Loss: 0.00001368
Iteration 132/1000 | Loss: 0.00045741
Iteration 133/1000 | Loss: 0.00001689
Iteration 134/1000 | Loss: 0.00001755
Iteration 135/1000 | Loss: 0.00001301
Iteration 136/1000 | Loss: 0.00001237
Iteration 137/1000 | Loss: 0.00001172
Iteration 138/1000 | Loss: 0.00003189
Iteration 139/1000 | Loss: 0.00001301
Iteration 140/1000 | Loss: 0.00001115
Iteration 141/1000 | Loss: 0.00001110
Iteration 142/1000 | Loss: 0.00001109
Iteration 143/1000 | Loss: 0.00001108
Iteration 144/1000 | Loss: 0.00001492
Iteration 145/1000 | Loss: 0.00001100
Iteration 146/1000 | Loss: 0.00001089
Iteration 147/1000 | Loss: 0.00001089
Iteration 148/1000 | Loss: 0.00001089
Iteration 149/1000 | Loss: 0.00001089
Iteration 150/1000 | Loss: 0.00001089
Iteration 151/1000 | Loss: 0.00001089
Iteration 152/1000 | Loss: 0.00001089
Iteration 153/1000 | Loss: 0.00001089
Iteration 154/1000 | Loss: 0.00001089
Iteration 155/1000 | Loss: 0.00001088
Iteration 156/1000 | Loss: 0.00001088
Iteration 157/1000 | Loss: 0.00001088
Iteration 158/1000 | Loss: 0.00001088
Iteration 159/1000 | Loss: 0.00001088
Iteration 160/1000 | Loss: 0.00001088
Iteration 161/1000 | Loss: 0.00001086
Iteration 162/1000 | Loss: 0.00001084
Iteration 163/1000 | Loss: 0.00001083
Iteration 164/1000 | Loss: 0.00001082
Iteration 165/1000 | Loss: 0.00001081
Iteration 166/1000 | Loss: 0.00001081
Iteration 167/1000 | Loss: 0.00001081
Iteration 168/1000 | Loss: 0.00001081
Iteration 169/1000 | Loss: 0.00001081
Iteration 170/1000 | Loss: 0.00001080
Iteration 171/1000 | Loss: 0.00001080
Iteration 172/1000 | Loss: 0.00001080
Iteration 173/1000 | Loss: 0.00001080
Iteration 174/1000 | Loss: 0.00001079
Iteration 175/1000 | Loss: 0.00001079
Iteration 176/1000 | Loss: 0.00001078
Iteration 177/1000 | Loss: 0.00001078
Iteration 178/1000 | Loss: 0.00001077
Iteration 179/1000 | Loss: 0.00001077
Iteration 180/1000 | Loss: 0.00001076
Iteration 181/1000 | Loss: 0.00001076
Iteration 182/1000 | Loss: 0.00001076
Iteration 183/1000 | Loss: 0.00001076
Iteration 184/1000 | Loss: 0.00001075
Iteration 185/1000 | Loss: 0.00001075
Iteration 186/1000 | Loss: 0.00001075
Iteration 187/1000 | Loss: 0.00001075
Iteration 188/1000 | Loss: 0.00001212
Iteration 189/1000 | Loss: 0.00001073
Iteration 190/1000 | Loss: 0.00001073
Iteration 191/1000 | Loss: 0.00001073
Iteration 192/1000 | Loss: 0.00001073
Iteration 193/1000 | Loss: 0.00001073
Iteration 194/1000 | Loss: 0.00001072
Iteration 195/1000 | Loss: 0.00001072
Iteration 196/1000 | Loss: 0.00001072
Iteration 197/1000 | Loss: 0.00001072
Iteration 198/1000 | Loss: 0.00001072
Iteration 199/1000 | Loss: 0.00001072
Iteration 200/1000 | Loss: 0.00001072
Iteration 201/1000 | Loss: 0.00001072
Iteration 202/1000 | Loss: 0.00001072
Iteration 203/1000 | Loss: 0.00001071
Iteration 204/1000 | Loss: 0.00001071
Iteration 205/1000 | Loss: 0.00001071
Iteration 206/1000 | Loss: 0.00001071
Iteration 207/1000 | Loss: 0.00001071
Iteration 208/1000 | Loss: 0.00001071
Iteration 209/1000 | Loss: 0.00001071
Iteration 210/1000 | Loss: 0.00001071
Iteration 211/1000 | Loss: 0.00001071
Iteration 212/1000 | Loss: 0.00001071
Iteration 213/1000 | Loss: 0.00001071
Iteration 214/1000 | Loss: 0.00001070
Iteration 215/1000 | Loss: 0.00001070
Iteration 216/1000 | Loss: 0.00001070
Iteration 217/1000 | Loss: 0.00001070
Iteration 218/1000 | Loss: 0.00001070
Iteration 219/1000 | Loss: 0.00001070
Iteration 220/1000 | Loss: 0.00001070
Iteration 221/1000 | Loss: 0.00001070
Iteration 222/1000 | Loss: 0.00001070
Iteration 223/1000 | Loss: 0.00001069
Iteration 224/1000 | Loss: 0.00001069
Iteration 225/1000 | Loss: 0.00001069
Iteration 226/1000 | Loss: 0.00001069
Iteration 227/1000 | Loss: 0.00001069
Iteration 228/1000 | Loss: 0.00001069
Iteration 229/1000 | Loss: 0.00001069
Iteration 230/1000 | Loss: 0.00001069
Iteration 231/1000 | Loss: 0.00001069
Iteration 232/1000 | Loss: 0.00001069
Iteration 233/1000 | Loss: 0.00001069
Iteration 234/1000 | Loss: 0.00001069
Iteration 235/1000 | Loss: 0.00001069
Iteration 236/1000 | Loss: 0.00001069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.0688328075048048e-05, 1.0688328075048048e-05, 1.0688328075048048e-05, 1.0688328075048048e-05, 1.0688328075048048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0688328075048048e-05

Optimization complete. Final v2v error: 2.746948719024658 mm

Highest mean error: 4.877024173736572 mm for frame 155

Lowest mean error: 2.2709641456604004 mm for frame 11

Saving results

Total time: 286.5586943626404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422983
Iteration 2/25 | Loss: 0.00120406
Iteration 3/25 | Loss: 0.00112021
Iteration 4/25 | Loss: 0.00110029
Iteration 5/25 | Loss: 0.00109461
Iteration 6/25 | Loss: 0.00109328
Iteration 7/25 | Loss: 0.00109306
Iteration 8/25 | Loss: 0.00109306
Iteration 9/25 | Loss: 0.00109306
Iteration 10/25 | Loss: 0.00109306
Iteration 11/25 | Loss: 0.00109306
Iteration 12/25 | Loss: 0.00109306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010930623393505812, 0.0010930623393505812, 0.0010930623393505812, 0.0010930623393505812, 0.0010930623393505812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010930623393505812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38977432
Iteration 2/25 | Loss: 0.00072302
Iteration 3/25 | Loss: 0.00072302
Iteration 4/25 | Loss: 0.00072302
Iteration 5/25 | Loss: 0.00072302
Iteration 6/25 | Loss: 0.00072302
Iteration 7/25 | Loss: 0.00072302
Iteration 8/25 | Loss: 0.00072302
Iteration 9/25 | Loss: 0.00072302
Iteration 10/25 | Loss: 0.00072302
Iteration 11/25 | Loss: 0.00072302
Iteration 12/25 | Loss: 0.00072302
Iteration 13/25 | Loss: 0.00072302
Iteration 14/25 | Loss: 0.00072302
Iteration 15/25 | Loss: 0.00072302
Iteration 16/25 | Loss: 0.00072302
Iteration 17/25 | Loss: 0.00072302
Iteration 18/25 | Loss: 0.00072302
Iteration 19/25 | Loss: 0.00072302
Iteration 20/25 | Loss: 0.00072302
Iteration 21/25 | Loss: 0.00072302
Iteration 22/25 | Loss: 0.00072302
Iteration 23/25 | Loss: 0.00072302
Iteration 24/25 | Loss: 0.00072302
Iteration 25/25 | Loss: 0.00072302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072302
Iteration 2/1000 | Loss: 0.00002726
Iteration 3/1000 | Loss: 0.00002056
Iteration 4/1000 | Loss: 0.00001778
Iteration 5/1000 | Loss: 0.00001686
Iteration 6/1000 | Loss: 0.00001637
Iteration 7/1000 | Loss: 0.00001602
Iteration 8/1000 | Loss: 0.00001571
Iteration 9/1000 | Loss: 0.00001550
Iteration 10/1000 | Loss: 0.00001547
Iteration 11/1000 | Loss: 0.00001536
Iteration 12/1000 | Loss: 0.00001533
Iteration 13/1000 | Loss: 0.00001527
Iteration 14/1000 | Loss: 0.00001526
Iteration 15/1000 | Loss: 0.00001524
Iteration 16/1000 | Loss: 0.00001504
Iteration 17/1000 | Loss: 0.00001498
Iteration 18/1000 | Loss: 0.00001498
Iteration 19/1000 | Loss: 0.00001491
Iteration 20/1000 | Loss: 0.00001491
Iteration 21/1000 | Loss: 0.00001491
Iteration 22/1000 | Loss: 0.00001491
Iteration 23/1000 | Loss: 0.00001491
Iteration 24/1000 | Loss: 0.00001491
Iteration 25/1000 | Loss: 0.00001491
Iteration 26/1000 | Loss: 0.00001491
Iteration 27/1000 | Loss: 0.00001491
Iteration 28/1000 | Loss: 0.00001491
Iteration 29/1000 | Loss: 0.00001491
Iteration 30/1000 | Loss: 0.00001491
Iteration 31/1000 | Loss: 0.00001491
Iteration 32/1000 | Loss: 0.00001491
Iteration 33/1000 | Loss: 0.00001491
Iteration 34/1000 | Loss: 0.00001491
Iteration 35/1000 | Loss: 0.00001491
Iteration 36/1000 | Loss: 0.00001491
Iteration 37/1000 | Loss: 0.00001491
Iteration 38/1000 | Loss: 0.00001491
Iteration 39/1000 | Loss: 0.00001491
Iteration 40/1000 | Loss: 0.00001491
Iteration 41/1000 | Loss: 0.00001491
Iteration 42/1000 | Loss: 0.00001491
Iteration 43/1000 | Loss: 0.00001490
Iteration 44/1000 | Loss: 0.00001490
Iteration 45/1000 | Loss: 0.00001486
Iteration 46/1000 | Loss: 0.00001483
Iteration 47/1000 | Loss: 0.00001482
Iteration 48/1000 | Loss: 0.00001478
Iteration 49/1000 | Loss: 0.00001475
Iteration 50/1000 | Loss: 0.00001471
Iteration 51/1000 | Loss: 0.00001471
Iteration 52/1000 | Loss: 0.00001471
Iteration 53/1000 | Loss: 0.00001470
Iteration 54/1000 | Loss: 0.00001469
Iteration 55/1000 | Loss: 0.00001469
Iteration 56/1000 | Loss: 0.00001468
Iteration 57/1000 | Loss: 0.00001467
Iteration 58/1000 | Loss: 0.00001467
Iteration 59/1000 | Loss: 0.00001467
Iteration 60/1000 | Loss: 0.00001466
Iteration 61/1000 | Loss: 0.00001466
Iteration 62/1000 | Loss: 0.00001466
Iteration 63/1000 | Loss: 0.00001465
Iteration 64/1000 | Loss: 0.00001465
Iteration 65/1000 | Loss: 0.00001464
Iteration 66/1000 | Loss: 0.00001464
Iteration 67/1000 | Loss: 0.00001463
Iteration 68/1000 | Loss: 0.00001462
Iteration 69/1000 | Loss: 0.00001462
Iteration 70/1000 | Loss: 0.00001462
Iteration 71/1000 | Loss: 0.00001461
Iteration 72/1000 | Loss: 0.00001460
Iteration 73/1000 | Loss: 0.00001460
Iteration 74/1000 | Loss: 0.00001460
Iteration 75/1000 | Loss: 0.00001459
Iteration 76/1000 | Loss: 0.00001458
Iteration 77/1000 | Loss: 0.00001458
Iteration 78/1000 | Loss: 0.00001457
Iteration 79/1000 | Loss: 0.00001457
Iteration 80/1000 | Loss: 0.00001456
Iteration 81/1000 | Loss: 0.00001456
Iteration 82/1000 | Loss: 0.00001455
Iteration 83/1000 | Loss: 0.00001454
Iteration 84/1000 | Loss: 0.00001453
Iteration 85/1000 | Loss: 0.00001453
Iteration 86/1000 | Loss: 0.00001453
Iteration 87/1000 | Loss: 0.00001453
Iteration 88/1000 | Loss: 0.00001453
Iteration 89/1000 | Loss: 0.00001453
Iteration 90/1000 | Loss: 0.00001453
Iteration 91/1000 | Loss: 0.00001453
Iteration 92/1000 | Loss: 0.00001453
Iteration 93/1000 | Loss: 0.00001453
Iteration 94/1000 | Loss: 0.00001453
Iteration 95/1000 | Loss: 0.00001452
Iteration 96/1000 | Loss: 0.00001451
Iteration 97/1000 | Loss: 0.00001451
Iteration 98/1000 | Loss: 0.00001450
Iteration 99/1000 | Loss: 0.00001450
Iteration 100/1000 | Loss: 0.00001450
Iteration 101/1000 | Loss: 0.00001450
Iteration 102/1000 | Loss: 0.00001450
Iteration 103/1000 | Loss: 0.00001450
Iteration 104/1000 | Loss: 0.00001449
Iteration 105/1000 | Loss: 0.00001449
Iteration 106/1000 | Loss: 0.00001448
Iteration 107/1000 | Loss: 0.00001448
Iteration 108/1000 | Loss: 0.00001448
Iteration 109/1000 | Loss: 0.00001448
Iteration 110/1000 | Loss: 0.00001447
Iteration 111/1000 | Loss: 0.00001447
Iteration 112/1000 | Loss: 0.00001446
Iteration 113/1000 | Loss: 0.00001446
Iteration 114/1000 | Loss: 0.00001446
Iteration 115/1000 | Loss: 0.00001446
Iteration 116/1000 | Loss: 0.00001446
Iteration 117/1000 | Loss: 0.00001445
Iteration 118/1000 | Loss: 0.00001445
Iteration 119/1000 | Loss: 0.00001445
Iteration 120/1000 | Loss: 0.00001445
Iteration 121/1000 | Loss: 0.00001445
Iteration 122/1000 | Loss: 0.00001445
Iteration 123/1000 | Loss: 0.00001445
Iteration 124/1000 | Loss: 0.00001445
Iteration 125/1000 | Loss: 0.00001445
Iteration 126/1000 | Loss: 0.00001444
Iteration 127/1000 | Loss: 0.00001444
Iteration 128/1000 | Loss: 0.00001444
Iteration 129/1000 | Loss: 0.00001444
Iteration 130/1000 | Loss: 0.00001444
Iteration 131/1000 | Loss: 0.00001444
Iteration 132/1000 | Loss: 0.00001444
Iteration 133/1000 | Loss: 0.00001443
Iteration 134/1000 | Loss: 0.00001443
Iteration 135/1000 | Loss: 0.00001443
Iteration 136/1000 | Loss: 0.00001443
Iteration 137/1000 | Loss: 0.00001443
Iteration 138/1000 | Loss: 0.00001443
Iteration 139/1000 | Loss: 0.00001443
Iteration 140/1000 | Loss: 0.00001443
Iteration 141/1000 | Loss: 0.00001443
Iteration 142/1000 | Loss: 0.00001443
Iteration 143/1000 | Loss: 0.00001442
Iteration 144/1000 | Loss: 0.00001442
Iteration 145/1000 | Loss: 0.00001442
Iteration 146/1000 | Loss: 0.00001442
Iteration 147/1000 | Loss: 0.00001442
Iteration 148/1000 | Loss: 0.00001442
Iteration 149/1000 | Loss: 0.00001442
Iteration 150/1000 | Loss: 0.00001442
Iteration 151/1000 | Loss: 0.00001441
Iteration 152/1000 | Loss: 0.00001441
Iteration 153/1000 | Loss: 0.00001441
Iteration 154/1000 | Loss: 0.00001441
Iteration 155/1000 | Loss: 0.00001441
Iteration 156/1000 | Loss: 0.00001441
Iteration 157/1000 | Loss: 0.00001441
Iteration 158/1000 | Loss: 0.00001441
Iteration 159/1000 | Loss: 0.00001440
Iteration 160/1000 | Loss: 0.00001440
Iteration 161/1000 | Loss: 0.00001440
Iteration 162/1000 | Loss: 0.00001440
Iteration 163/1000 | Loss: 0.00001440
Iteration 164/1000 | Loss: 0.00001440
Iteration 165/1000 | Loss: 0.00001439
Iteration 166/1000 | Loss: 0.00001439
Iteration 167/1000 | Loss: 0.00001439
Iteration 168/1000 | Loss: 0.00001439
Iteration 169/1000 | Loss: 0.00001439
Iteration 170/1000 | Loss: 0.00001439
Iteration 171/1000 | Loss: 0.00001439
Iteration 172/1000 | Loss: 0.00001439
Iteration 173/1000 | Loss: 0.00001439
Iteration 174/1000 | Loss: 0.00001438
Iteration 175/1000 | Loss: 0.00001438
Iteration 176/1000 | Loss: 0.00001438
Iteration 177/1000 | Loss: 0.00001438
Iteration 178/1000 | Loss: 0.00001438
Iteration 179/1000 | Loss: 0.00001438
Iteration 180/1000 | Loss: 0.00001438
Iteration 181/1000 | Loss: 0.00001438
Iteration 182/1000 | Loss: 0.00001438
Iteration 183/1000 | Loss: 0.00001438
Iteration 184/1000 | Loss: 0.00001438
Iteration 185/1000 | Loss: 0.00001438
Iteration 186/1000 | Loss: 0.00001438
Iteration 187/1000 | Loss: 0.00001438
Iteration 188/1000 | Loss: 0.00001438
Iteration 189/1000 | Loss: 0.00001438
Iteration 190/1000 | Loss: 0.00001438
Iteration 191/1000 | Loss: 0.00001438
Iteration 192/1000 | Loss: 0.00001438
Iteration 193/1000 | Loss: 0.00001438
Iteration 194/1000 | Loss: 0.00001438
Iteration 195/1000 | Loss: 0.00001438
Iteration 196/1000 | Loss: 0.00001438
Iteration 197/1000 | Loss: 0.00001438
Iteration 198/1000 | Loss: 0.00001438
Iteration 199/1000 | Loss: 0.00001438
Iteration 200/1000 | Loss: 0.00001438
Iteration 201/1000 | Loss: 0.00001438
Iteration 202/1000 | Loss: 0.00001438
Iteration 203/1000 | Loss: 0.00001438
Iteration 204/1000 | Loss: 0.00001438
Iteration 205/1000 | Loss: 0.00001438
Iteration 206/1000 | Loss: 0.00001438
Iteration 207/1000 | Loss: 0.00001438
Iteration 208/1000 | Loss: 0.00001438
Iteration 209/1000 | Loss: 0.00001438
Iteration 210/1000 | Loss: 0.00001438
Iteration 211/1000 | Loss: 0.00001438
Iteration 212/1000 | Loss: 0.00001438
Iteration 213/1000 | Loss: 0.00001438
Iteration 214/1000 | Loss: 0.00001438
Iteration 215/1000 | Loss: 0.00001438
Iteration 216/1000 | Loss: 0.00001438
Iteration 217/1000 | Loss: 0.00001438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.4383598681888543e-05, 1.4383598681888543e-05, 1.4383598681888543e-05, 1.4383598681888543e-05, 1.4383598681888543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4383598681888543e-05

Optimization complete. Final v2v error: 3.182971954345703 mm

Highest mean error: 3.9321231842041016 mm for frame 70

Lowest mean error: 2.850842237472534 mm for frame 45

Saving results

Total time: 41.5359742641449
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017503
Iteration 2/25 | Loss: 0.00193786
Iteration 3/25 | Loss: 0.00157690
Iteration 4/25 | Loss: 0.00155062
Iteration 5/25 | Loss: 0.00136239
Iteration 6/25 | Loss: 0.00143994
Iteration 7/25 | Loss: 0.00131063
Iteration 8/25 | Loss: 0.00125997
Iteration 9/25 | Loss: 0.00119672
Iteration 10/25 | Loss: 0.00113840
Iteration 11/25 | Loss: 0.00112602
Iteration 12/25 | Loss: 0.00111467
Iteration 13/25 | Loss: 0.00111746
Iteration 14/25 | Loss: 0.00111351
Iteration 15/25 | Loss: 0.00111178
Iteration 16/25 | Loss: 0.00111304
Iteration 17/25 | Loss: 0.00111560
Iteration 18/25 | Loss: 0.00111345
Iteration 19/25 | Loss: 0.00111178
Iteration 20/25 | Loss: 0.00111160
Iteration 21/25 | Loss: 0.00110940
Iteration 22/25 | Loss: 0.00110925
Iteration 23/25 | Loss: 0.00110921
Iteration 24/25 | Loss: 0.00110921
Iteration 25/25 | Loss: 0.00110921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56613839
Iteration 2/25 | Loss: 0.00124376
Iteration 3/25 | Loss: 0.00063574
Iteration 4/25 | Loss: 0.00063574
Iteration 5/25 | Loss: 0.00063574
Iteration 6/25 | Loss: 0.00063573
Iteration 7/25 | Loss: 0.00063573
Iteration 8/25 | Loss: 0.00063573
Iteration 9/25 | Loss: 0.00063573
Iteration 10/25 | Loss: 0.00063573
Iteration 11/25 | Loss: 0.00063573
Iteration 12/25 | Loss: 0.00063573
Iteration 13/25 | Loss: 0.00063573
Iteration 14/25 | Loss: 0.00063573
Iteration 15/25 | Loss: 0.00063573
Iteration 16/25 | Loss: 0.00063573
Iteration 17/25 | Loss: 0.00063573
Iteration 18/25 | Loss: 0.00063573
Iteration 19/25 | Loss: 0.00063573
Iteration 20/25 | Loss: 0.00063573
Iteration 21/25 | Loss: 0.00063573
Iteration 22/25 | Loss: 0.00063573
Iteration 23/25 | Loss: 0.00063573
Iteration 24/25 | Loss: 0.00063573
Iteration 25/25 | Loss: 0.00063573

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063573
Iteration 2/1000 | Loss: 0.00034156
Iteration 3/1000 | Loss: 0.00008880
Iteration 4/1000 | Loss: 0.00002432
Iteration 5/1000 | Loss: 0.00007904
Iteration 6/1000 | Loss: 0.00078119
Iteration 7/1000 | Loss: 0.00006365
Iteration 8/1000 | Loss: 0.00001782
Iteration 9/1000 | Loss: 0.00008128
Iteration 10/1000 | Loss: 0.00001685
Iteration 11/1000 | Loss: 0.00001654
Iteration 12/1000 | Loss: 0.00001625
Iteration 13/1000 | Loss: 0.00005116
Iteration 14/1000 | Loss: 0.00050146
Iteration 15/1000 | Loss: 0.00009991
Iteration 16/1000 | Loss: 0.00007903
Iteration 17/1000 | Loss: 0.00021578
Iteration 18/1000 | Loss: 0.00002401
Iteration 19/1000 | Loss: 0.00002524
Iteration 20/1000 | Loss: 0.00002328
Iteration 21/1000 | Loss: 0.00001572
Iteration 22/1000 | Loss: 0.00008974
Iteration 23/1000 | Loss: 0.00001898
Iteration 24/1000 | Loss: 0.00005640
Iteration 25/1000 | Loss: 0.00001482
Iteration 26/1000 | Loss: 0.00005518
Iteration 27/1000 | Loss: 0.00008429
Iteration 28/1000 | Loss: 0.00001595
Iteration 29/1000 | Loss: 0.00003145
Iteration 30/1000 | Loss: 0.00001489
Iteration 31/1000 | Loss: 0.00002087
Iteration 32/1000 | Loss: 0.00001485
Iteration 33/1000 | Loss: 0.00001432
Iteration 34/1000 | Loss: 0.00001432
Iteration 35/1000 | Loss: 0.00001432
Iteration 36/1000 | Loss: 0.00001432
Iteration 37/1000 | Loss: 0.00001432
Iteration 38/1000 | Loss: 0.00001432
Iteration 39/1000 | Loss: 0.00001432
Iteration 40/1000 | Loss: 0.00001432
Iteration 41/1000 | Loss: 0.00001431
Iteration 42/1000 | Loss: 0.00001431
Iteration 43/1000 | Loss: 0.00001506
Iteration 44/1000 | Loss: 0.00001560
Iteration 45/1000 | Loss: 0.00001503
Iteration 46/1000 | Loss: 0.00007890
Iteration 47/1000 | Loss: 0.00001720
Iteration 48/1000 | Loss: 0.00001538
Iteration 49/1000 | Loss: 0.00001425
Iteration 50/1000 | Loss: 0.00001425
Iteration 51/1000 | Loss: 0.00001425
Iteration 52/1000 | Loss: 0.00001425
Iteration 53/1000 | Loss: 0.00001425
Iteration 54/1000 | Loss: 0.00001425
Iteration 55/1000 | Loss: 0.00001425
Iteration 56/1000 | Loss: 0.00001425
Iteration 57/1000 | Loss: 0.00001425
Iteration 58/1000 | Loss: 0.00001424
Iteration 59/1000 | Loss: 0.00001424
Iteration 60/1000 | Loss: 0.00001424
Iteration 61/1000 | Loss: 0.00001424
Iteration 62/1000 | Loss: 0.00002941
Iteration 63/1000 | Loss: 0.00001853
Iteration 64/1000 | Loss: 0.00001421
Iteration 65/1000 | Loss: 0.00001421
Iteration 66/1000 | Loss: 0.00001421
Iteration 67/1000 | Loss: 0.00001421
Iteration 68/1000 | Loss: 0.00001887
Iteration 69/1000 | Loss: 0.00001795
Iteration 70/1000 | Loss: 0.00001425
Iteration 71/1000 | Loss: 0.00001418
Iteration 72/1000 | Loss: 0.00001418
Iteration 73/1000 | Loss: 0.00001417
Iteration 74/1000 | Loss: 0.00001417
Iteration 75/1000 | Loss: 0.00001417
Iteration 76/1000 | Loss: 0.00001417
Iteration 77/1000 | Loss: 0.00001417
Iteration 78/1000 | Loss: 0.00001417
Iteration 79/1000 | Loss: 0.00001417
Iteration 80/1000 | Loss: 0.00001416
Iteration 81/1000 | Loss: 0.00001416
Iteration 82/1000 | Loss: 0.00001416
Iteration 83/1000 | Loss: 0.00001416
Iteration 84/1000 | Loss: 0.00001416
Iteration 85/1000 | Loss: 0.00001416
Iteration 86/1000 | Loss: 0.00001416
Iteration 87/1000 | Loss: 0.00001416
Iteration 88/1000 | Loss: 0.00001416
Iteration 89/1000 | Loss: 0.00001416
Iteration 90/1000 | Loss: 0.00001416
Iteration 91/1000 | Loss: 0.00001416
Iteration 92/1000 | Loss: 0.00001416
Iteration 93/1000 | Loss: 0.00001416
Iteration 94/1000 | Loss: 0.00001416
Iteration 95/1000 | Loss: 0.00001416
Iteration 96/1000 | Loss: 0.00001416
Iteration 97/1000 | Loss: 0.00001415
Iteration 98/1000 | Loss: 0.00001415
Iteration 99/1000 | Loss: 0.00002102
Iteration 100/1000 | Loss: 0.00001417
Iteration 101/1000 | Loss: 0.00001416
Iteration 102/1000 | Loss: 0.00001956
Iteration 103/1000 | Loss: 0.00001421
Iteration 104/1000 | Loss: 0.00001413
Iteration 105/1000 | Loss: 0.00001413
Iteration 106/1000 | Loss: 0.00001413
Iteration 107/1000 | Loss: 0.00001413
Iteration 108/1000 | Loss: 0.00001413
Iteration 109/1000 | Loss: 0.00001412
Iteration 110/1000 | Loss: 0.00001412
Iteration 111/1000 | Loss: 0.00003036
Iteration 112/1000 | Loss: 0.00003419
Iteration 113/1000 | Loss: 0.00002202
Iteration 114/1000 | Loss: 0.00001411
Iteration 115/1000 | Loss: 0.00001408
Iteration 116/1000 | Loss: 0.00001407
Iteration 117/1000 | Loss: 0.00001407
Iteration 118/1000 | Loss: 0.00001407
Iteration 119/1000 | Loss: 0.00001407
Iteration 120/1000 | Loss: 0.00001407
Iteration 121/1000 | Loss: 0.00001407
Iteration 122/1000 | Loss: 0.00001407
Iteration 123/1000 | Loss: 0.00001407
Iteration 124/1000 | Loss: 0.00001407
Iteration 125/1000 | Loss: 0.00001407
Iteration 126/1000 | Loss: 0.00001407
Iteration 127/1000 | Loss: 0.00001407
Iteration 128/1000 | Loss: 0.00001407
Iteration 129/1000 | Loss: 0.00001407
Iteration 130/1000 | Loss: 0.00001407
Iteration 131/1000 | Loss: 0.00001407
Iteration 132/1000 | Loss: 0.00001407
Iteration 133/1000 | Loss: 0.00001407
Iteration 134/1000 | Loss: 0.00001407
Iteration 135/1000 | Loss: 0.00001407
Iteration 136/1000 | Loss: 0.00001407
Iteration 137/1000 | Loss: 0.00001407
Iteration 138/1000 | Loss: 0.00001407
Iteration 139/1000 | Loss: 0.00001407
Iteration 140/1000 | Loss: 0.00001407
Iteration 141/1000 | Loss: 0.00001407
Iteration 142/1000 | Loss: 0.00001407
Iteration 143/1000 | Loss: 0.00001407
Iteration 144/1000 | Loss: 0.00001407
Iteration 145/1000 | Loss: 0.00001407
Iteration 146/1000 | Loss: 0.00001407
Iteration 147/1000 | Loss: 0.00001407
Iteration 148/1000 | Loss: 0.00001407
Iteration 149/1000 | Loss: 0.00001407
Iteration 150/1000 | Loss: 0.00001407
Iteration 151/1000 | Loss: 0.00001407
Iteration 152/1000 | Loss: 0.00001407
Iteration 153/1000 | Loss: 0.00001407
Iteration 154/1000 | Loss: 0.00001407
Iteration 155/1000 | Loss: 0.00001407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.4067342817725148e-05, 1.4067342817725148e-05, 1.4067342817725148e-05, 1.4067342817725148e-05, 1.4067342817725148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4067342817725148e-05

Optimization complete. Final v2v error: 3.141946315765381 mm

Highest mean error: 3.935917854309082 mm for frame 35

Lowest mean error: 2.8654184341430664 mm for frame 247

Saving results

Total time: 124.4484658241272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824331
Iteration 2/25 | Loss: 0.00110842
Iteration 3/25 | Loss: 0.00103725
Iteration 4/25 | Loss: 0.00101723
Iteration 5/25 | Loss: 0.00101069
Iteration 6/25 | Loss: 0.00100963
Iteration 7/25 | Loss: 0.00100963
Iteration 8/25 | Loss: 0.00100963
Iteration 9/25 | Loss: 0.00100963
Iteration 10/25 | Loss: 0.00100961
Iteration 11/25 | Loss: 0.00100961
Iteration 12/25 | Loss: 0.00100961
Iteration 13/25 | Loss: 0.00100961
Iteration 14/25 | Loss: 0.00100961
Iteration 15/25 | Loss: 0.00100961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010096128098666668, 0.0010096128098666668, 0.0010096128098666668, 0.0010096128098666668, 0.0010096128098666668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010096128098666668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27790666
Iteration 2/25 | Loss: 0.00095490
Iteration 3/25 | Loss: 0.00095490
Iteration 4/25 | Loss: 0.00095490
Iteration 5/25 | Loss: 0.00095490
Iteration 6/25 | Loss: 0.00095490
Iteration 7/25 | Loss: 0.00095490
Iteration 8/25 | Loss: 0.00095490
Iteration 9/25 | Loss: 0.00095490
Iteration 10/25 | Loss: 0.00095490
Iteration 11/25 | Loss: 0.00095490
Iteration 12/25 | Loss: 0.00095490
Iteration 13/25 | Loss: 0.00095490
Iteration 14/25 | Loss: 0.00095490
Iteration 15/25 | Loss: 0.00095490
Iteration 16/25 | Loss: 0.00095490
Iteration 17/25 | Loss: 0.00095490
Iteration 18/25 | Loss: 0.00095490
Iteration 19/25 | Loss: 0.00095490
Iteration 20/25 | Loss: 0.00095490
Iteration 21/25 | Loss: 0.00095490
Iteration 22/25 | Loss: 0.00095490
Iteration 23/25 | Loss: 0.00095490
Iteration 24/25 | Loss: 0.00095490
Iteration 25/25 | Loss: 0.00095490

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095490
Iteration 2/1000 | Loss: 0.00002787
Iteration 3/1000 | Loss: 0.00001750
Iteration 4/1000 | Loss: 0.00001413
Iteration 5/1000 | Loss: 0.00001284
Iteration 6/1000 | Loss: 0.00001199
Iteration 7/1000 | Loss: 0.00001147
Iteration 8/1000 | Loss: 0.00001107
Iteration 9/1000 | Loss: 0.00001080
Iteration 10/1000 | Loss: 0.00001050
Iteration 11/1000 | Loss: 0.00001047
Iteration 12/1000 | Loss: 0.00001033
Iteration 13/1000 | Loss: 0.00001027
Iteration 14/1000 | Loss: 0.00001022
Iteration 15/1000 | Loss: 0.00001012
Iteration 16/1000 | Loss: 0.00001012
Iteration 17/1000 | Loss: 0.00001011
Iteration 18/1000 | Loss: 0.00001006
Iteration 19/1000 | Loss: 0.00001004
Iteration 20/1000 | Loss: 0.00001003
Iteration 21/1000 | Loss: 0.00001003
Iteration 22/1000 | Loss: 0.00001003
Iteration 23/1000 | Loss: 0.00001003
Iteration 24/1000 | Loss: 0.00001003
Iteration 25/1000 | Loss: 0.00001002
Iteration 26/1000 | Loss: 0.00001002
Iteration 27/1000 | Loss: 0.00001002
Iteration 28/1000 | Loss: 0.00001001
Iteration 29/1000 | Loss: 0.00001001
Iteration 30/1000 | Loss: 0.00001001
Iteration 31/1000 | Loss: 0.00001001
Iteration 32/1000 | Loss: 0.00001000
Iteration 33/1000 | Loss: 0.00001000
Iteration 34/1000 | Loss: 0.00001000
Iteration 35/1000 | Loss: 0.00001000
Iteration 36/1000 | Loss: 0.00000999
Iteration 37/1000 | Loss: 0.00000999
Iteration 38/1000 | Loss: 0.00000999
Iteration 39/1000 | Loss: 0.00000999
Iteration 40/1000 | Loss: 0.00000998
Iteration 41/1000 | Loss: 0.00000998
Iteration 42/1000 | Loss: 0.00000998
Iteration 43/1000 | Loss: 0.00000997
Iteration 44/1000 | Loss: 0.00000997
Iteration 45/1000 | Loss: 0.00000997
Iteration 46/1000 | Loss: 0.00000997
Iteration 47/1000 | Loss: 0.00000996
Iteration 48/1000 | Loss: 0.00000996
Iteration 49/1000 | Loss: 0.00000995
Iteration 50/1000 | Loss: 0.00000995
Iteration 51/1000 | Loss: 0.00000995
Iteration 52/1000 | Loss: 0.00000994
Iteration 53/1000 | Loss: 0.00000994
Iteration 54/1000 | Loss: 0.00000994
Iteration 55/1000 | Loss: 0.00000993
Iteration 56/1000 | Loss: 0.00000992
Iteration 57/1000 | Loss: 0.00000992
Iteration 58/1000 | Loss: 0.00000992
Iteration 59/1000 | Loss: 0.00000991
Iteration 60/1000 | Loss: 0.00000991
Iteration 61/1000 | Loss: 0.00000991
Iteration 62/1000 | Loss: 0.00000990
Iteration 63/1000 | Loss: 0.00000990
Iteration 64/1000 | Loss: 0.00000989
Iteration 65/1000 | Loss: 0.00000989
Iteration 66/1000 | Loss: 0.00000989
Iteration 67/1000 | Loss: 0.00000988
Iteration 68/1000 | Loss: 0.00000988
Iteration 69/1000 | Loss: 0.00000988
Iteration 70/1000 | Loss: 0.00000988
Iteration 71/1000 | Loss: 0.00000987
Iteration 72/1000 | Loss: 0.00000987
Iteration 73/1000 | Loss: 0.00000987
Iteration 74/1000 | Loss: 0.00000987
Iteration 75/1000 | Loss: 0.00000987
Iteration 76/1000 | Loss: 0.00000987
Iteration 77/1000 | Loss: 0.00000986
Iteration 78/1000 | Loss: 0.00000986
Iteration 79/1000 | Loss: 0.00000986
Iteration 80/1000 | Loss: 0.00000986
Iteration 81/1000 | Loss: 0.00000986
Iteration 82/1000 | Loss: 0.00000985
Iteration 83/1000 | Loss: 0.00000985
Iteration 84/1000 | Loss: 0.00000985
Iteration 85/1000 | Loss: 0.00000985
Iteration 86/1000 | Loss: 0.00000985
Iteration 87/1000 | Loss: 0.00000985
Iteration 88/1000 | Loss: 0.00000985
Iteration 89/1000 | Loss: 0.00000985
Iteration 90/1000 | Loss: 0.00000985
Iteration 91/1000 | Loss: 0.00000985
Iteration 92/1000 | Loss: 0.00000985
Iteration 93/1000 | Loss: 0.00000984
Iteration 94/1000 | Loss: 0.00000984
Iteration 95/1000 | Loss: 0.00000984
Iteration 96/1000 | Loss: 0.00000984
Iteration 97/1000 | Loss: 0.00000984
Iteration 98/1000 | Loss: 0.00000984
Iteration 99/1000 | Loss: 0.00000984
Iteration 100/1000 | Loss: 0.00000984
Iteration 101/1000 | Loss: 0.00000984
Iteration 102/1000 | Loss: 0.00000984
Iteration 103/1000 | Loss: 0.00000984
Iteration 104/1000 | Loss: 0.00000984
Iteration 105/1000 | Loss: 0.00000984
Iteration 106/1000 | Loss: 0.00000984
Iteration 107/1000 | Loss: 0.00000984
Iteration 108/1000 | Loss: 0.00000983
Iteration 109/1000 | Loss: 0.00000983
Iteration 110/1000 | Loss: 0.00000983
Iteration 111/1000 | Loss: 0.00000983
Iteration 112/1000 | Loss: 0.00000983
Iteration 113/1000 | Loss: 0.00000983
Iteration 114/1000 | Loss: 0.00000983
Iteration 115/1000 | Loss: 0.00000982
Iteration 116/1000 | Loss: 0.00000982
Iteration 117/1000 | Loss: 0.00000982
Iteration 118/1000 | Loss: 0.00000982
Iteration 119/1000 | Loss: 0.00000982
Iteration 120/1000 | Loss: 0.00000982
Iteration 121/1000 | Loss: 0.00000982
Iteration 122/1000 | Loss: 0.00000982
Iteration 123/1000 | Loss: 0.00000982
Iteration 124/1000 | Loss: 0.00000982
Iteration 125/1000 | Loss: 0.00000982
Iteration 126/1000 | Loss: 0.00000982
Iteration 127/1000 | Loss: 0.00000982
Iteration 128/1000 | Loss: 0.00000982
Iteration 129/1000 | Loss: 0.00000982
Iteration 130/1000 | Loss: 0.00000981
Iteration 131/1000 | Loss: 0.00000981
Iteration 132/1000 | Loss: 0.00000981
Iteration 133/1000 | Loss: 0.00000981
Iteration 134/1000 | Loss: 0.00000981
Iteration 135/1000 | Loss: 0.00000981
Iteration 136/1000 | Loss: 0.00000981
Iteration 137/1000 | Loss: 0.00000981
Iteration 138/1000 | Loss: 0.00000981
Iteration 139/1000 | Loss: 0.00000981
Iteration 140/1000 | Loss: 0.00000981
Iteration 141/1000 | Loss: 0.00000981
Iteration 142/1000 | Loss: 0.00000981
Iteration 143/1000 | Loss: 0.00000981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [9.809590665099677e-06, 9.809590665099677e-06, 9.809590665099677e-06, 9.809590665099677e-06, 9.809590665099677e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.809590665099677e-06

Optimization complete. Final v2v error: 2.6815860271453857 mm

Highest mean error: 2.9440572261810303 mm for frame 239

Lowest mean error: 2.4354705810546875 mm for frame 44

Saving results

Total time: 40.49238872528076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01091907
Iteration 2/25 | Loss: 0.00331262
Iteration 3/25 | Loss: 0.00217681
Iteration 4/25 | Loss: 0.00201036
Iteration 5/25 | Loss: 0.00190231
Iteration 6/25 | Loss: 0.00159559
Iteration 7/25 | Loss: 0.00155612
Iteration 8/25 | Loss: 0.00154363
Iteration 9/25 | Loss: 0.00154052
Iteration 10/25 | Loss: 0.00153878
Iteration 11/25 | Loss: 0.00153830
Iteration 12/25 | Loss: 0.00153797
Iteration 13/25 | Loss: 0.00153772
Iteration 14/25 | Loss: 0.00153760
Iteration 15/25 | Loss: 0.00153750
Iteration 16/25 | Loss: 0.00153741
Iteration 17/25 | Loss: 0.00153741
Iteration 18/25 | Loss: 0.00153741
Iteration 19/25 | Loss: 0.00153740
Iteration 20/25 | Loss: 0.00153740
Iteration 21/25 | Loss: 0.00153740
Iteration 22/25 | Loss: 0.00153740
Iteration 23/25 | Loss: 0.00153740
Iteration 24/25 | Loss: 0.00153740
Iteration 25/25 | Loss: 0.00153740

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22829187
Iteration 2/25 | Loss: 0.00228986
Iteration 3/25 | Loss: 0.00228985
Iteration 4/25 | Loss: 0.00228985
Iteration 5/25 | Loss: 0.00228985
Iteration 6/25 | Loss: 0.00228985
Iteration 7/25 | Loss: 0.00228985
Iteration 8/25 | Loss: 0.00228985
Iteration 9/25 | Loss: 0.00228985
Iteration 10/25 | Loss: 0.00228985
Iteration 11/25 | Loss: 0.00228985
Iteration 12/25 | Loss: 0.00228985
Iteration 13/25 | Loss: 0.00228985
Iteration 14/25 | Loss: 0.00228985
Iteration 15/25 | Loss: 0.00228985
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0022898497991263866, 0.0022898497991263866, 0.0022898497991263866, 0.0022898497991263866, 0.0022898497991263866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022898497991263866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228985
Iteration 2/1000 | Loss: 0.00030074
Iteration 3/1000 | Loss: 0.00023456
Iteration 4/1000 | Loss: 0.00020940
Iteration 5/1000 | Loss: 0.00019750
Iteration 6/1000 | Loss: 0.00018619
Iteration 7/1000 | Loss: 0.00017869
Iteration 8/1000 | Loss: 0.00017397
Iteration 9/1000 | Loss: 0.00016770
Iteration 10/1000 | Loss: 0.00016290
Iteration 11/1000 | Loss: 0.00015866
Iteration 12/1000 | Loss: 0.00015601
Iteration 13/1000 | Loss: 0.00015431
Iteration 14/1000 | Loss: 0.00015324
Iteration 15/1000 | Loss: 0.00015205
Iteration 16/1000 | Loss: 0.00015068
Iteration 17/1000 | Loss: 0.00015004
Iteration 18/1000 | Loss: 0.00014950
Iteration 19/1000 | Loss: 0.00014906
Iteration 20/1000 | Loss: 0.00014875
Iteration 21/1000 | Loss: 0.00014842
Iteration 22/1000 | Loss: 0.00014817
Iteration 23/1000 | Loss: 0.00014805
Iteration 24/1000 | Loss: 0.00014802
Iteration 25/1000 | Loss: 0.00014791
Iteration 26/1000 | Loss: 0.00014780
Iteration 27/1000 | Loss: 0.00014771
Iteration 28/1000 | Loss: 0.00014766
Iteration 29/1000 | Loss: 0.00014763
Iteration 30/1000 | Loss: 0.00014755
Iteration 31/1000 | Loss: 0.00014751
Iteration 32/1000 | Loss: 0.00014751
Iteration 33/1000 | Loss: 0.00014744
Iteration 34/1000 | Loss: 0.00014733
Iteration 35/1000 | Loss: 0.00014732
Iteration 36/1000 | Loss: 0.00014729
Iteration 37/1000 | Loss: 0.00014727
Iteration 38/1000 | Loss: 0.00014727
Iteration 39/1000 | Loss: 0.00014726
Iteration 40/1000 | Loss: 0.00014724
Iteration 41/1000 | Loss: 0.00014724
Iteration 42/1000 | Loss: 0.00014724
Iteration 43/1000 | Loss: 0.00014723
Iteration 44/1000 | Loss: 0.00014722
Iteration 45/1000 | Loss: 0.00014718
Iteration 46/1000 | Loss: 0.00014718
Iteration 47/1000 | Loss: 0.00014717
Iteration 48/1000 | Loss: 0.00014717
Iteration 49/1000 | Loss: 0.00014716
Iteration 50/1000 | Loss: 0.00014716
Iteration 51/1000 | Loss: 0.00014715
Iteration 52/1000 | Loss: 0.00014715
Iteration 53/1000 | Loss: 0.00014714
Iteration 54/1000 | Loss: 0.00014714
Iteration 55/1000 | Loss: 0.00014713
Iteration 56/1000 | Loss: 0.00014713
Iteration 57/1000 | Loss: 0.00014713
Iteration 58/1000 | Loss: 0.00014712
Iteration 59/1000 | Loss: 0.00014712
Iteration 60/1000 | Loss: 0.00014712
Iteration 61/1000 | Loss: 0.00014711
Iteration 62/1000 | Loss: 0.00014711
Iteration 63/1000 | Loss: 0.00014711
Iteration 64/1000 | Loss: 0.00014711
Iteration 65/1000 | Loss: 0.00014711
Iteration 66/1000 | Loss: 0.00014711
Iteration 67/1000 | Loss: 0.00014711
Iteration 68/1000 | Loss: 0.00014711
Iteration 69/1000 | Loss: 0.00014710
Iteration 70/1000 | Loss: 0.00014710
Iteration 71/1000 | Loss: 0.00014710
Iteration 72/1000 | Loss: 0.00014710
Iteration 73/1000 | Loss: 0.00014710
Iteration 74/1000 | Loss: 0.00014710
Iteration 75/1000 | Loss: 0.00014710
Iteration 76/1000 | Loss: 0.00014710
Iteration 77/1000 | Loss: 0.00014710
Iteration 78/1000 | Loss: 0.00014710
Iteration 79/1000 | Loss: 0.00014710
Iteration 80/1000 | Loss: 0.00014710
Iteration 81/1000 | Loss: 0.00014710
Iteration 82/1000 | Loss: 0.00014710
Iteration 83/1000 | Loss: 0.00014710
Iteration 84/1000 | Loss: 0.00014709
Iteration 85/1000 | Loss: 0.00014709
Iteration 86/1000 | Loss: 0.00014709
Iteration 87/1000 | Loss: 0.00014709
Iteration 88/1000 | Loss: 0.00014708
Iteration 89/1000 | Loss: 0.00014708
Iteration 90/1000 | Loss: 0.00014708
Iteration 91/1000 | Loss: 0.00014708
Iteration 92/1000 | Loss: 0.00014707
Iteration 93/1000 | Loss: 0.00014707
Iteration 94/1000 | Loss: 0.00014706
Iteration 95/1000 | Loss: 0.00014706
Iteration 96/1000 | Loss: 0.00014706
Iteration 97/1000 | Loss: 0.00014706
Iteration 98/1000 | Loss: 0.00014706
Iteration 99/1000 | Loss: 0.00014706
Iteration 100/1000 | Loss: 0.00014706
Iteration 101/1000 | Loss: 0.00014706
Iteration 102/1000 | Loss: 0.00014706
Iteration 103/1000 | Loss: 0.00014706
Iteration 104/1000 | Loss: 0.00014706
Iteration 105/1000 | Loss: 0.00014706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [0.00014705670764669776, 0.00014705670764669776, 0.00014705670764669776, 0.00014705670764669776, 0.00014705670764669776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014705670764669776

Optimization complete. Final v2v error: 7.033981800079346 mm

Highest mean error: 10.953923225402832 mm for frame 80

Lowest mean error: 4.803708076477051 mm for frame 63

Saving results

Total time: 84.41730618476868
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054869
Iteration 2/25 | Loss: 0.00214308
Iteration 3/25 | Loss: 0.00144646
Iteration 4/25 | Loss: 0.00138606
Iteration 5/25 | Loss: 0.00127702
Iteration 6/25 | Loss: 0.00130217
Iteration 7/25 | Loss: 0.00121903
Iteration 8/25 | Loss: 0.00117831
Iteration 9/25 | Loss: 0.00115375
Iteration 10/25 | Loss: 0.00116011
Iteration 11/25 | Loss: 0.00113192
Iteration 12/25 | Loss: 0.00111108
Iteration 13/25 | Loss: 0.00111722
Iteration 14/25 | Loss: 0.00110320
Iteration 15/25 | Loss: 0.00111314
Iteration 16/25 | Loss: 0.00110455
Iteration 17/25 | Loss: 0.00110621
Iteration 18/25 | Loss: 0.00110380
Iteration 19/25 | Loss: 0.00110328
Iteration 20/25 | Loss: 0.00110183
Iteration 21/25 | Loss: 0.00110594
Iteration 22/25 | Loss: 0.00110175
Iteration 23/25 | Loss: 0.00110171
Iteration 24/25 | Loss: 0.00110171
Iteration 25/25 | Loss: 0.00110171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32573009
Iteration 2/25 | Loss: 0.00097392
Iteration 3/25 | Loss: 0.00097392
Iteration 4/25 | Loss: 0.00097392
Iteration 5/25 | Loss: 0.00097392
Iteration 6/25 | Loss: 0.00097392
Iteration 7/25 | Loss: 0.00097392
Iteration 8/25 | Loss: 0.00097392
Iteration 9/25 | Loss: 0.00097392
Iteration 10/25 | Loss: 0.00097392
Iteration 11/25 | Loss: 0.00097392
Iteration 12/25 | Loss: 0.00097392
Iteration 13/25 | Loss: 0.00097392
Iteration 14/25 | Loss: 0.00097392
Iteration 15/25 | Loss: 0.00097392
Iteration 16/25 | Loss: 0.00097392
Iteration 17/25 | Loss: 0.00097392
Iteration 18/25 | Loss: 0.00097392
Iteration 19/25 | Loss: 0.00097392
Iteration 20/25 | Loss: 0.00097392
Iteration 21/25 | Loss: 0.00097392
Iteration 22/25 | Loss: 0.00097392
Iteration 23/25 | Loss: 0.00097392
Iteration 24/25 | Loss: 0.00097392
Iteration 25/25 | Loss: 0.00097392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097392
Iteration 2/1000 | Loss: 0.00008945
Iteration 3/1000 | Loss: 0.00006630
Iteration 4/1000 | Loss: 0.00005741
Iteration 5/1000 | Loss: 0.00005287
Iteration 6/1000 | Loss: 0.00005056
Iteration 7/1000 | Loss: 0.00004801
Iteration 8/1000 | Loss: 0.00004654
Iteration 9/1000 | Loss: 0.00004494
Iteration 10/1000 | Loss: 0.00004386
Iteration 11/1000 | Loss: 0.00004244
Iteration 12/1000 | Loss: 0.00004154
Iteration 13/1000 | Loss: 0.00061729
Iteration 14/1000 | Loss: 0.00756198
Iteration 15/1000 | Loss: 0.00039642
Iteration 16/1000 | Loss: 0.00073443
Iteration 17/1000 | Loss: 0.00006856
Iteration 18/1000 | Loss: 0.00010193
Iteration 19/1000 | Loss: 0.00020606
Iteration 20/1000 | Loss: 0.00027016
Iteration 21/1000 | Loss: 0.00091857
Iteration 22/1000 | Loss: 0.00002888
Iteration 23/1000 | Loss: 0.00007466
Iteration 24/1000 | Loss: 0.00001951
Iteration 25/1000 | Loss: 0.00018953
Iteration 26/1000 | Loss: 0.00016663
Iteration 27/1000 | Loss: 0.00002470
Iteration 28/1000 | Loss: 0.00016530
Iteration 29/1000 | Loss: 0.00002170
Iteration 30/1000 | Loss: 0.00001938
Iteration 31/1000 | Loss: 0.00001257
Iteration 32/1000 | Loss: 0.00016133
Iteration 33/1000 | Loss: 0.00001176
Iteration 34/1000 | Loss: 0.00001026
Iteration 35/1000 | Loss: 0.00000966
Iteration 36/1000 | Loss: 0.00005486
Iteration 37/1000 | Loss: 0.00031576
Iteration 38/1000 | Loss: 0.00006513
Iteration 39/1000 | Loss: 0.00001588
Iteration 40/1000 | Loss: 0.00001102
Iteration 41/1000 | Loss: 0.00001676
Iteration 42/1000 | Loss: 0.00000910
Iteration 43/1000 | Loss: 0.00000876
Iteration 44/1000 | Loss: 0.00000856
Iteration 45/1000 | Loss: 0.00001771
Iteration 46/1000 | Loss: 0.00000889
Iteration 47/1000 | Loss: 0.00000841
Iteration 48/1000 | Loss: 0.00000839
Iteration 49/1000 | Loss: 0.00000838
Iteration 50/1000 | Loss: 0.00000838
Iteration 51/1000 | Loss: 0.00000838
Iteration 52/1000 | Loss: 0.00000837
Iteration 53/1000 | Loss: 0.00000837
Iteration 54/1000 | Loss: 0.00000837
Iteration 55/1000 | Loss: 0.00000837
Iteration 56/1000 | Loss: 0.00000836
Iteration 57/1000 | Loss: 0.00000836
Iteration 58/1000 | Loss: 0.00000836
Iteration 59/1000 | Loss: 0.00000836
Iteration 60/1000 | Loss: 0.00000835
Iteration 61/1000 | Loss: 0.00000835
Iteration 62/1000 | Loss: 0.00000835
Iteration 63/1000 | Loss: 0.00000834
Iteration 64/1000 | Loss: 0.00000834
Iteration 65/1000 | Loss: 0.00000834
Iteration 66/1000 | Loss: 0.00000834
Iteration 67/1000 | Loss: 0.00000834
Iteration 68/1000 | Loss: 0.00000833
Iteration 69/1000 | Loss: 0.00000833
Iteration 70/1000 | Loss: 0.00000833
Iteration 71/1000 | Loss: 0.00000833
Iteration 72/1000 | Loss: 0.00000833
Iteration 73/1000 | Loss: 0.00000832
Iteration 74/1000 | Loss: 0.00000831
Iteration 75/1000 | Loss: 0.00000831
Iteration 76/1000 | Loss: 0.00000830
Iteration 77/1000 | Loss: 0.00000829
Iteration 78/1000 | Loss: 0.00000829
Iteration 79/1000 | Loss: 0.00000828
Iteration 80/1000 | Loss: 0.00000828
Iteration 81/1000 | Loss: 0.00000828
Iteration 82/1000 | Loss: 0.00000828
Iteration 83/1000 | Loss: 0.00000828
Iteration 84/1000 | Loss: 0.00000828
Iteration 85/1000 | Loss: 0.00000827
Iteration 86/1000 | Loss: 0.00000827
Iteration 87/1000 | Loss: 0.00000827
Iteration 88/1000 | Loss: 0.00000827
Iteration 89/1000 | Loss: 0.00000826
Iteration 90/1000 | Loss: 0.00000826
Iteration 91/1000 | Loss: 0.00000826
Iteration 92/1000 | Loss: 0.00000826
Iteration 93/1000 | Loss: 0.00000826
Iteration 94/1000 | Loss: 0.00000826
Iteration 95/1000 | Loss: 0.00000825
Iteration 96/1000 | Loss: 0.00000825
Iteration 97/1000 | Loss: 0.00000825
Iteration 98/1000 | Loss: 0.00000825
Iteration 99/1000 | Loss: 0.00000825
Iteration 100/1000 | Loss: 0.00000825
Iteration 101/1000 | Loss: 0.00000824
Iteration 102/1000 | Loss: 0.00000824
Iteration 103/1000 | Loss: 0.00000824
Iteration 104/1000 | Loss: 0.00000824
Iteration 105/1000 | Loss: 0.00000824
Iteration 106/1000 | Loss: 0.00000824
Iteration 107/1000 | Loss: 0.00000824
Iteration 108/1000 | Loss: 0.00000824
Iteration 109/1000 | Loss: 0.00000824
Iteration 110/1000 | Loss: 0.00000824
Iteration 111/1000 | Loss: 0.00000823
Iteration 112/1000 | Loss: 0.00000823
Iteration 113/1000 | Loss: 0.00000823
Iteration 114/1000 | Loss: 0.00000823
Iteration 115/1000 | Loss: 0.00000823
Iteration 116/1000 | Loss: 0.00000823
Iteration 117/1000 | Loss: 0.00000823
Iteration 118/1000 | Loss: 0.00000823
Iteration 119/1000 | Loss: 0.00000823
Iteration 120/1000 | Loss: 0.00000823
Iteration 121/1000 | Loss: 0.00000823
Iteration 122/1000 | Loss: 0.00000823
Iteration 123/1000 | Loss: 0.00000823
Iteration 124/1000 | Loss: 0.00000822
Iteration 125/1000 | Loss: 0.00000822
Iteration 126/1000 | Loss: 0.00000822
Iteration 127/1000 | Loss: 0.00000822
Iteration 128/1000 | Loss: 0.00000822
Iteration 129/1000 | Loss: 0.00000822
Iteration 130/1000 | Loss: 0.00000822
Iteration 131/1000 | Loss: 0.00000822
Iteration 132/1000 | Loss: 0.00000822
Iteration 133/1000 | Loss: 0.00000822
Iteration 134/1000 | Loss: 0.00000822
Iteration 135/1000 | Loss: 0.00000822
Iteration 136/1000 | Loss: 0.00000822
Iteration 137/1000 | Loss: 0.00000821
Iteration 138/1000 | Loss: 0.00000821
Iteration 139/1000 | Loss: 0.00000821
Iteration 140/1000 | Loss: 0.00000821
Iteration 141/1000 | Loss: 0.00000821
Iteration 142/1000 | Loss: 0.00000821
Iteration 143/1000 | Loss: 0.00000821
Iteration 144/1000 | Loss: 0.00000820
Iteration 145/1000 | Loss: 0.00000820
Iteration 146/1000 | Loss: 0.00000820
Iteration 147/1000 | Loss: 0.00000820
Iteration 148/1000 | Loss: 0.00000820
Iteration 149/1000 | Loss: 0.00000820
Iteration 150/1000 | Loss: 0.00000820
Iteration 151/1000 | Loss: 0.00000820
Iteration 152/1000 | Loss: 0.00000820
Iteration 153/1000 | Loss: 0.00000819
Iteration 154/1000 | Loss: 0.00000819
Iteration 155/1000 | Loss: 0.00000819
Iteration 156/1000 | Loss: 0.00000819
Iteration 157/1000 | Loss: 0.00000819
Iteration 158/1000 | Loss: 0.00000819
Iteration 159/1000 | Loss: 0.00000819
Iteration 160/1000 | Loss: 0.00000819
Iteration 161/1000 | Loss: 0.00000819
Iteration 162/1000 | Loss: 0.00000819
Iteration 163/1000 | Loss: 0.00000819
Iteration 164/1000 | Loss: 0.00000819
Iteration 165/1000 | Loss: 0.00000819
Iteration 166/1000 | Loss: 0.00000819
Iteration 167/1000 | Loss: 0.00000819
Iteration 168/1000 | Loss: 0.00000819
Iteration 169/1000 | Loss: 0.00000818
Iteration 170/1000 | Loss: 0.00000818
Iteration 171/1000 | Loss: 0.00000818
Iteration 172/1000 | Loss: 0.00000818
Iteration 173/1000 | Loss: 0.00000818
Iteration 174/1000 | Loss: 0.00000818
Iteration 175/1000 | Loss: 0.00000818
Iteration 176/1000 | Loss: 0.00000818
Iteration 177/1000 | Loss: 0.00000818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [8.184272701328155e-06, 8.184272701328155e-06, 8.184272701328155e-06, 8.184272701328155e-06, 8.184272701328155e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.184272701328155e-06

Optimization complete. Final v2v error: 2.4461069107055664 mm

Highest mean error: 2.7333946228027344 mm for frame 82

Lowest mean error: 2.3570642471313477 mm for frame 1

Saving results

Total time: 110.96206474304199
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402458
Iteration 2/25 | Loss: 0.00114759
Iteration 3/25 | Loss: 0.00106333
Iteration 4/25 | Loss: 0.00105227
Iteration 5/25 | Loss: 0.00104904
Iteration 6/25 | Loss: 0.00104822
Iteration 7/25 | Loss: 0.00104822
Iteration 8/25 | Loss: 0.00104822
Iteration 9/25 | Loss: 0.00104822
Iteration 10/25 | Loss: 0.00104822
Iteration 11/25 | Loss: 0.00104822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010482155485078692, 0.0010482155485078692, 0.0010482155485078692, 0.0010482155485078692, 0.0010482155485078692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010482155485078692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46572769
Iteration 2/25 | Loss: 0.00073172
Iteration 3/25 | Loss: 0.00073172
Iteration 4/25 | Loss: 0.00073172
Iteration 5/25 | Loss: 0.00073172
Iteration 6/25 | Loss: 0.00073171
Iteration 7/25 | Loss: 0.00073171
Iteration 8/25 | Loss: 0.00073171
Iteration 9/25 | Loss: 0.00073171
Iteration 10/25 | Loss: 0.00073171
Iteration 11/25 | Loss: 0.00073171
Iteration 12/25 | Loss: 0.00073171
Iteration 13/25 | Loss: 0.00073171
Iteration 14/25 | Loss: 0.00073171
Iteration 15/25 | Loss: 0.00073171
Iteration 16/25 | Loss: 0.00073171
Iteration 17/25 | Loss: 0.00073171
Iteration 18/25 | Loss: 0.00073171
Iteration 19/25 | Loss: 0.00073171
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007317132549360394, 0.0007317132549360394, 0.0007317132549360394, 0.0007317132549360394, 0.0007317132549360394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007317132549360394

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073171
Iteration 2/1000 | Loss: 0.00001872
Iteration 3/1000 | Loss: 0.00001157
Iteration 4/1000 | Loss: 0.00001018
Iteration 5/1000 | Loss: 0.00000958
Iteration 6/1000 | Loss: 0.00000936
Iteration 7/1000 | Loss: 0.00000910
Iteration 8/1000 | Loss: 0.00000901
Iteration 9/1000 | Loss: 0.00000900
Iteration 10/1000 | Loss: 0.00000900
Iteration 11/1000 | Loss: 0.00000893
Iteration 12/1000 | Loss: 0.00000887
Iteration 13/1000 | Loss: 0.00000887
Iteration 14/1000 | Loss: 0.00000883
Iteration 15/1000 | Loss: 0.00000877
Iteration 16/1000 | Loss: 0.00000876
Iteration 17/1000 | Loss: 0.00000873
Iteration 18/1000 | Loss: 0.00000872
Iteration 19/1000 | Loss: 0.00000870
Iteration 20/1000 | Loss: 0.00000870
Iteration 21/1000 | Loss: 0.00000868
Iteration 22/1000 | Loss: 0.00000868
Iteration 23/1000 | Loss: 0.00000867
Iteration 24/1000 | Loss: 0.00000867
Iteration 25/1000 | Loss: 0.00000862
Iteration 26/1000 | Loss: 0.00000859
Iteration 27/1000 | Loss: 0.00000859
Iteration 28/1000 | Loss: 0.00000858
Iteration 29/1000 | Loss: 0.00000856
Iteration 30/1000 | Loss: 0.00000854
Iteration 31/1000 | Loss: 0.00000853
Iteration 32/1000 | Loss: 0.00000852
Iteration 33/1000 | Loss: 0.00000852
Iteration 34/1000 | Loss: 0.00000851
Iteration 35/1000 | Loss: 0.00000849
Iteration 36/1000 | Loss: 0.00000848
Iteration 37/1000 | Loss: 0.00000847
Iteration 38/1000 | Loss: 0.00000847
Iteration 39/1000 | Loss: 0.00000847
Iteration 40/1000 | Loss: 0.00000846
Iteration 41/1000 | Loss: 0.00000846
Iteration 42/1000 | Loss: 0.00000846
Iteration 43/1000 | Loss: 0.00000845
Iteration 44/1000 | Loss: 0.00000845
Iteration 45/1000 | Loss: 0.00000844
Iteration 46/1000 | Loss: 0.00000843
Iteration 47/1000 | Loss: 0.00000843
Iteration 48/1000 | Loss: 0.00000843
Iteration 49/1000 | Loss: 0.00000842
Iteration 50/1000 | Loss: 0.00000842
Iteration 51/1000 | Loss: 0.00000842
Iteration 52/1000 | Loss: 0.00000842
Iteration 53/1000 | Loss: 0.00000842
Iteration 54/1000 | Loss: 0.00000841
Iteration 55/1000 | Loss: 0.00000841
Iteration 56/1000 | Loss: 0.00000841
Iteration 57/1000 | Loss: 0.00000840
Iteration 58/1000 | Loss: 0.00000839
Iteration 59/1000 | Loss: 0.00000838
Iteration 60/1000 | Loss: 0.00000838
Iteration 61/1000 | Loss: 0.00000837
Iteration 62/1000 | Loss: 0.00000837
Iteration 63/1000 | Loss: 0.00000837
Iteration 64/1000 | Loss: 0.00000837
Iteration 65/1000 | Loss: 0.00000837
Iteration 66/1000 | Loss: 0.00000837
Iteration 67/1000 | Loss: 0.00000836
Iteration 68/1000 | Loss: 0.00000836
Iteration 69/1000 | Loss: 0.00000836
Iteration 70/1000 | Loss: 0.00000836
Iteration 71/1000 | Loss: 0.00000835
Iteration 72/1000 | Loss: 0.00000835
Iteration 73/1000 | Loss: 0.00000835
Iteration 74/1000 | Loss: 0.00000834
Iteration 75/1000 | Loss: 0.00000834
Iteration 76/1000 | Loss: 0.00000834
Iteration 77/1000 | Loss: 0.00000834
Iteration 78/1000 | Loss: 0.00000834
Iteration 79/1000 | Loss: 0.00000834
Iteration 80/1000 | Loss: 0.00000833
Iteration 81/1000 | Loss: 0.00000833
Iteration 82/1000 | Loss: 0.00000833
Iteration 83/1000 | Loss: 0.00000833
Iteration 84/1000 | Loss: 0.00000833
Iteration 85/1000 | Loss: 0.00000833
Iteration 86/1000 | Loss: 0.00000833
Iteration 87/1000 | Loss: 0.00000833
Iteration 88/1000 | Loss: 0.00000832
Iteration 89/1000 | Loss: 0.00000832
Iteration 90/1000 | Loss: 0.00000832
Iteration 91/1000 | Loss: 0.00000832
Iteration 92/1000 | Loss: 0.00000832
Iteration 93/1000 | Loss: 0.00000832
Iteration 94/1000 | Loss: 0.00000832
Iteration 95/1000 | Loss: 0.00000832
Iteration 96/1000 | Loss: 0.00000832
Iteration 97/1000 | Loss: 0.00000832
Iteration 98/1000 | Loss: 0.00000832
Iteration 99/1000 | Loss: 0.00000832
Iteration 100/1000 | Loss: 0.00000832
Iteration 101/1000 | Loss: 0.00000832
Iteration 102/1000 | Loss: 0.00000832
Iteration 103/1000 | Loss: 0.00000832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [8.319979315274395e-06, 8.319979315274395e-06, 8.319979315274395e-06, 8.319979315274395e-06, 8.319979315274395e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.319979315274395e-06

Optimization complete. Final v2v error: 2.4927661418914795 mm

Highest mean error: 3.241889238357544 mm for frame 59

Lowest mean error: 2.197589874267578 mm for frame 41

Saving results

Total time: 30.30478262901306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00699428
Iteration 2/25 | Loss: 0.00149286
Iteration 3/25 | Loss: 0.00117339
Iteration 4/25 | Loss: 0.00114254
Iteration 5/25 | Loss: 0.00112677
Iteration 6/25 | Loss: 0.00113688
Iteration 7/25 | Loss: 0.00113428
Iteration 8/25 | Loss: 0.00113008
Iteration 9/25 | Loss: 0.00112068
Iteration 10/25 | Loss: 0.00111319
Iteration 11/25 | Loss: 0.00111166
Iteration 12/25 | Loss: 0.00110592
Iteration 13/25 | Loss: 0.00110246
Iteration 14/25 | Loss: 0.00110156
Iteration 15/25 | Loss: 0.00109851
Iteration 16/25 | Loss: 0.00109736
Iteration 17/25 | Loss: 0.00109702
Iteration 18/25 | Loss: 0.00109690
Iteration 19/25 | Loss: 0.00109676
Iteration 20/25 | Loss: 0.00109668
Iteration 21/25 | Loss: 0.00109668
Iteration 22/25 | Loss: 0.00109667
Iteration 23/25 | Loss: 0.00109667
Iteration 24/25 | Loss: 0.00109667
Iteration 25/25 | Loss: 0.00109667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.04463959
Iteration 2/25 | Loss: 0.00072368
Iteration 3/25 | Loss: 0.00069621
Iteration 4/25 | Loss: 0.00069621
Iteration 5/25 | Loss: 0.00069621
Iteration 6/25 | Loss: 0.00069621
Iteration 7/25 | Loss: 0.00069621
Iteration 8/25 | Loss: 0.00069621
Iteration 9/25 | Loss: 0.00069621
Iteration 10/25 | Loss: 0.00069621
Iteration 11/25 | Loss: 0.00069621
Iteration 12/25 | Loss: 0.00069621
Iteration 13/25 | Loss: 0.00069621
Iteration 14/25 | Loss: 0.00069621
Iteration 15/25 | Loss: 0.00069621
Iteration 16/25 | Loss: 0.00069621
Iteration 17/25 | Loss: 0.00069621
Iteration 18/25 | Loss: 0.00069621
Iteration 19/25 | Loss: 0.00069621
Iteration 20/25 | Loss: 0.00069621
Iteration 21/25 | Loss: 0.00069621
Iteration 22/25 | Loss: 0.00069621
Iteration 23/25 | Loss: 0.00069621
Iteration 24/25 | Loss: 0.00069621
Iteration 25/25 | Loss: 0.00069621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069621
Iteration 2/1000 | Loss: 0.00023634
Iteration 3/1000 | Loss: 0.00013969
Iteration 4/1000 | Loss: 0.00002150
Iteration 5/1000 | Loss: 0.00012648
Iteration 6/1000 | Loss: 0.00016779
Iteration 7/1000 | Loss: 0.00001841
Iteration 8/1000 | Loss: 0.00001594
Iteration 9/1000 | Loss: 0.00001501
Iteration 10/1000 | Loss: 0.00001423
Iteration 11/1000 | Loss: 0.00001358
Iteration 12/1000 | Loss: 0.00001329
Iteration 13/1000 | Loss: 0.00001303
Iteration 14/1000 | Loss: 0.00001286
Iteration 15/1000 | Loss: 0.00001286
Iteration 16/1000 | Loss: 0.00001285
Iteration 17/1000 | Loss: 0.00001284
Iteration 18/1000 | Loss: 0.00001276
Iteration 19/1000 | Loss: 0.00001272
Iteration 20/1000 | Loss: 0.00001271
Iteration 21/1000 | Loss: 0.00001267
Iteration 22/1000 | Loss: 0.00001267
Iteration 23/1000 | Loss: 0.00001266
Iteration 24/1000 | Loss: 0.00001259
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001253
Iteration 27/1000 | Loss: 0.00001248
Iteration 28/1000 | Loss: 0.00001240
Iteration 29/1000 | Loss: 0.00001239
Iteration 30/1000 | Loss: 0.00001238
Iteration 31/1000 | Loss: 0.00001238
Iteration 32/1000 | Loss: 0.00001238
Iteration 33/1000 | Loss: 0.00001235
Iteration 34/1000 | Loss: 0.00001235
Iteration 35/1000 | Loss: 0.00001235
Iteration 36/1000 | Loss: 0.00001235
Iteration 37/1000 | Loss: 0.00001235
Iteration 38/1000 | Loss: 0.00001234
Iteration 39/1000 | Loss: 0.00001234
Iteration 40/1000 | Loss: 0.00001234
Iteration 41/1000 | Loss: 0.00001233
Iteration 42/1000 | Loss: 0.00001233
Iteration 43/1000 | Loss: 0.00001232
Iteration 44/1000 | Loss: 0.00001232
Iteration 45/1000 | Loss: 0.00001232
Iteration 46/1000 | Loss: 0.00001232
Iteration 47/1000 | Loss: 0.00001232
Iteration 48/1000 | Loss: 0.00001231
Iteration 49/1000 | Loss: 0.00001231
Iteration 50/1000 | Loss: 0.00001231
Iteration 51/1000 | Loss: 0.00001231
Iteration 52/1000 | Loss: 0.00001231
Iteration 53/1000 | Loss: 0.00001231
Iteration 54/1000 | Loss: 0.00001231
Iteration 55/1000 | Loss: 0.00001231
Iteration 56/1000 | Loss: 0.00001231
Iteration 57/1000 | Loss: 0.00001231
Iteration 58/1000 | Loss: 0.00001230
Iteration 59/1000 | Loss: 0.00001230
Iteration 60/1000 | Loss: 0.00001229
Iteration 61/1000 | Loss: 0.00001229
Iteration 62/1000 | Loss: 0.00001229
Iteration 63/1000 | Loss: 0.00001228
Iteration 64/1000 | Loss: 0.00001228
Iteration 65/1000 | Loss: 0.00001228
Iteration 66/1000 | Loss: 0.00001227
Iteration 67/1000 | Loss: 0.00001227
Iteration 68/1000 | Loss: 0.00001227
Iteration 69/1000 | Loss: 0.00001227
Iteration 70/1000 | Loss: 0.00001226
Iteration 71/1000 | Loss: 0.00001226
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001226
Iteration 74/1000 | Loss: 0.00001226
Iteration 75/1000 | Loss: 0.00001225
Iteration 76/1000 | Loss: 0.00001225
Iteration 77/1000 | Loss: 0.00001225
Iteration 78/1000 | Loss: 0.00001225
Iteration 79/1000 | Loss: 0.00001225
Iteration 80/1000 | Loss: 0.00001224
Iteration 81/1000 | Loss: 0.00001224
Iteration 82/1000 | Loss: 0.00001224
Iteration 83/1000 | Loss: 0.00001224
Iteration 84/1000 | Loss: 0.00001224
Iteration 85/1000 | Loss: 0.00001224
Iteration 86/1000 | Loss: 0.00001224
Iteration 87/1000 | Loss: 0.00001224
Iteration 88/1000 | Loss: 0.00001224
Iteration 89/1000 | Loss: 0.00001224
Iteration 90/1000 | Loss: 0.00001223
Iteration 91/1000 | Loss: 0.00001223
Iteration 92/1000 | Loss: 0.00001223
Iteration 93/1000 | Loss: 0.00001223
Iteration 94/1000 | Loss: 0.00001223
Iteration 95/1000 | Loss: 0.00001223
Iteration 96/1000 | Loss: 0.00001223
Iteration 97/1000 | Loss: 0.00001222
Iteration 98/1000 | Loss: 0.00001222
Iteration 99/1000 | Loss: 0.00001222
Iteration 100/1000 | Loss: 0.00001222
Iteration 101/1000 | Loss: 0.00001222
Iteration 102/1000 | Loss: 0.00001221
Iteration 103/1000 | Loss: 0.00001221
Iteration 104/1000 | Loss: 0.00001221
Iteration 105/1000 | Loss: 0.00001220
Iteration 106/1000 | Loss: 0.00001220
Iteration 107/1000 | Loss: 0.00001220
Iteration 108/1000 | Loss: 0.00001220
Iteration 109/1000 | Loss: 0.00001220
Iteration 110/1000 | Loss: 0.00001220
Iteration 111/1000 | Loss: 0.00001220
Iteration 112/1000 | Loss: 0.00001219
Iteration 113/1000 | Loss: 0.00001219
Iteration 114/1000 | Loss: 0.00001219
Iteration 115/1000 | Loss: 0.00001219
Iteration 116/1000 | Loss: 0.00001219
Iteration 117/1000 | Loss: 0.00001218
Iteration 118/1000 | Loss: 0.00001218
Iteration 119/1000 | Loss: 0.00001218
Iteration 120/1000 | Loss: 0.00001218
Iteration 121/1000 | Loss: 0.00001217
Iteration 122/1000 | Loss: 0.00001217
Iteration 123/1000 | Loss: 0.00001217
Iteration 124/1000 | Loss: 0.00001217
Iteration 125/1000 | Loss: 0.00001217
Iteration 126/1000 | Loss: 0.00001216
Iteration 127/1000 | Loss: 0.00001216
Iteration 128/1000 | Loss: 0.00001216
Iteration 129/1000 | Loss: 0.00001215
Iteration 130/1000 | Loss: 0.00001215
Iteration 131/1000 | Loss: 0.00001215
Iteration 132/1000 | Loss: 0.00001215
Iteration 133/1000 | Loss: 0.00001215
Iteration 134/1000 | Loss: 0.00001215
Iteration 135/1000 | Loss: 0.00001215
Iteration 136/1000 | Loss: 0.00001215
Iteration 137/1000 | Loss: 0.00001215
Iteration 138/1000 | Loss: 0.00001215
Iteration 139/1000 | Loss: 0.00001215
Iteration 140/1000 | Loss: 0.00001214
Iteration 141/1000 | Loss: 0.00001214
Iteration 142/1000 | Loss: 0.00001214
Iteration 143/1000 | Loss: 0.00001214
Iteration 144/1000 | Loss: 0.00001214
Iteration 145/1000 | Loss: 0.00001214
Iteration 146/1000 | Loss: 0.00001214
Iteration 147/1000 | Loss: 0.00001214
Iteration 148/1000 | Loss: 0.00001214
Iteration 149/1000 | Loss: 0.00001214
Iteration 150/1000 | Loss: 0.00001214
Iteration 151/1000 | Loss: 0.00001214
Iteration 152/1000 | Loss: 0.00001214
Iteration 153/1000 | Loss: 0.00001214
Iteration 154/1000 | Loss: 0.00001213
Iteration 155/1000 | Loss: 0.00001213
Iteration 156/1000 | Loss: 0.00001213
Iteration 157/1000 | Loss: 0.00001213
Iteration 158/1000 | Loss: 0.00001213
Iteration 159/1000 | Loss: 0.00001213
Iteration 160/1000 | Loss: 0.00001213
Iteration 161/1000 | Loss: 0.00001213
Iteration 162/1000 | Loss: 0.00001213
Iteration 163/1000 | Loss: 0.00001213
Iteration 164/1000 | Loss: 0.00001213
Iteration 165/1000 | Loss: 0.00001213
Iteration 166/1000 | Loss: 0.00001213
Iteration 167/1000 | Loss: 0.00001213
Iteration 168/1000 | Loss: 0.00001213
Iteration 169/1000 | Loss: 0.00001213
Iteration 170/1000 | Loss: 0.00001213
Iteration 171/1000 | Loss: 0.00001213
Iteration 172/1000 | Loss: 0.00001212
Iteration 173/1000 | Loss: 0.00001212
Iteration 174/1000 | Loss: 0.00001212
Iteration 175/1000 | Loss: 0.00001212
Iteration 176/1000 | Loss: 0.00001212
Iteration 177/1000 | Loss: 0.00001212
Iteration 178/1000 | Loss: 0.00001212
Iteration 179/1000 | Loss: 0.00001211
Iteration 180/1000 | Loss: 0.00001211
Iteration 181/1000 | Loss: 0.00001211
Iteration 182/1000 | Loss: 0.00001211
Iteration 183/1000 | Loss: 0.00001211
Iteration 184/1000 | Loss: 0.00001211
Iteration 185/1000 | Loss: 0.00001211
Iteration 186/1000 | Loss: 0.00001211
Iteration 187/1000 | Loss: 0.00001211
Iteration 188/1000 | Loss: 0.00001210
Iteration 189/1000 | Loss: 0.00001210
Iteration 190/1000 | Loss: 0.00001210
Iteration 191/1000 | Loss: 0.00001210
Iteration 192/1000 | Loss: 0.00001210
Iteration 193/1000 | Loss: 0.00001210
Iteration 194/1000 | Loss: 0.00001210
Iteration 195/1000 | Loss: 0.00001210
Iteration 196/1000 | Loss: 0.00001210
Iteration 197/1000 | Loss: 0.00001210
Iteration 198/1000 | Loss: 0.00001210
Iteration 199/1000 | Loss: 0.00001210
Iteration 200/1000 | Loss: 0.00001210
Iteration 201/1000 | Loss: 0.00001210
Iteration 202/1000 | Loss: 0.00001210
Iteration 203/1000 | Loss: 0.00001209
Iteration 204/1000 | Loss: 0.00001209
Iteration 205/1000 | Loss: 0.00001209
Iteration 206/1000 | Loss: 0.00001209
Iteration 207/1000 | Loss: 0.00001209
Iteration 208/1000 | Loss: 0.00001209
Iteration 209/1000 | Loss: 0.00001209
Iteration 210/1000 | Loss: 0.00001209
Iteration 211/1000 | Loss: 0.00001209
Iteration 212/1000 | Loss: 0.00001209
Iteration 213/1000 | Loss: 0.00001209
Iteration 214/1000 | Loss: 0.00001209
Iteration 215/1000 | Loss: 0.00001209
Iteration 216/1000 | Loss: 0.00001209
Iteration 217/1000 | Loss: 0.00001209
Iteration 218/1000 | Loss: 0.00001209
Iteration 219/1000 | Loss: 0.00001209
Iteration 220/1000 | Loss: 0.00001208
Iteration 221/1000 | Loss: 0.00001208
Iteration 222/1000 | Loss: 0.00001208
Iteration 223/1000 | Loss: 0.00001208
Iteration 224/1000 | Loss: 0.00001208
Iteration 225/1000 | Loss: 0.00001208
Iteration 226/1000 | Loss: 0.00001208
Iteration 227/1000 | Loss: 0.00001208
Iteration 228/1000 | Loss: 0.00001208
Iteration 229/1000 | Loss: 0.00001208
Iteration 230/1000 | Loss: 0.00001208
Iteration 231/1000 | Loss: 0.00001208
Iteration 232/1000 | Loss: 0.00001208
Iteration 233/1000 | Loss: 0.00001208
Iteration 234/1000 | Loss: 0.00001208
Iteration 235/1000 | Loss: 0.00001208
Iteration 236/1000 | Loss: 0.00001208
Iteration 237/1000 | Loss: 0.00001208
Iteration 238/1000 | Loss: 0.00001208
Iteration 239/1000 | Loss: 0.00001207
Iteration 240/1000 | Loss: 0.00001207
Iteration 241/1000 | Loss: 0.00001207
Iteration 242/1000 | Loss: 0.00001207
Iteration 243/1000 | Loss: 0.00001207
Iteration 244/1000 | Loss: 0.00001207
Iteration 245/1000 | Loss: 0.00001207
Iteration 246/1000 | Loss: 0.00001207
Iteration 247/1000 | Loss: 0.00001207
Iteration 248/1000 | Loss: 0.00001207
Iteration 249/1000 | Loss: 0.00001207
Iteration 250/1000 | Loss: 0.00001207
Iteration 251/1000 | Loss: 0.00001207
Iteration 252/1000 | Loss: 0.00001207
Iteration 253/1000 | Loss: 0.00001207
Iteration 254/1000 | Loss: 0.00001207
Iteration 255/1000 | Loss: 0.00001207
Iteration 256/1000 | Loss: 0.00001207
Iteration 257/1000 | Loss: 0.00001207
Iteration 258/1000 | Loss: 0.00001207
Iteration 259/1000 | Loss: 0.00001207
Iteration 260/1000 | Loss: 0.00001207
Iteration 261/1000 | Loss: 0.00001207
Iteration 262/1000 | Loss: 0.00001207
Iteration 263/1000 | Loss: 0.00001207
Iteration 264/1000 | Loss: 0.00001207
Iteration 265/1000 | Loss: 0.00001207
Iteration 266/1000 | Loss: 0.00001207
Iteration 267/1000 | Loss: 0.00001207
Iteration 268/1000 | Loss: 0.00001207
Iteration 269/1000 | Loss: 0.00001207
Iteration 270/1000 | Loss: 0.00001207
Iteration 271/1000 | Loss: 0.00001207
Iteration 272/1000 | Loss: 0.00001207
Iteration 273/1000 | Loss: 0.00001207
Iteration 274/1000 | Loss: 0.00001207
Iteration 275/1000 | Loss: 0.00001207
Iteration 276/1000 | Loss: 0.00001207
Iteration 277/1000 | Loss: 0.00001207
Iteration 278/1000 | Loss: 0.00001207
Iteration 279/1000 | Loss: 0.00001207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [1.20655795399216e-05, 1.20655795399216e-05, 1.20655795399216e-05, 1.20655795399216e-05, 1.20655795399216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.20655795399216e-05

Optimization complete. Final v2v error: 2.917253017425537 mm

Highest mean error: 3.760389804840088 mm for frame 158

Lowest mean error: 2.401130437850952 mm for frame 219

Saving results

Total time: 87.75804257392883
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478000
Iteration 2/25 | Loss: 0.00118821
Iteration 3/25 | Loss: 0.00111424
Iteration 4/25 | Loss: 0.00110432
Iteration 5/25 | Loss: 0.00110028
Iteration 6/25 | Loss: 0.00109941
Iteration 7/25 | Loss: 0.00109941
Iteration 8/25 | Loss: 0.00109941
Iteration 9/25 | Loss: 0.00109941
Iteration 10/25 | Loss: 0.00109941
Iteration 11/25 | Loss: 0.00109941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010994072072207928, 0.0010994072072207928, 0.0010994072072207928, 0.0010994072072207928, 0.0010994072072207928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010994072072207928

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86395985
Iteration 2/25 | Loss: 0.00063578
Iteration 3/25 | Loss: 0.00063578
Iteration 4/25 | Loss: 0.00063578
Iteration 5/25 | Loss: 0.00063578
Iteration 6/25 | Loss: 0.00063578
Iteration 7/25 | Loss: 0.00063578
Iteration 8/25 | Loss: 0.00063577
Iteration 9/25 | Loss: 0.00063577
Iteration 10/25 | Loss: 0.00063577
Iteration 11/25 | Loss: 0.00063577
Iteration 12/25 | Loss: 0.00063577
Iteration 13/25 | Loss: 0.00063577
Iteration 14/25 | Loss: 0.00063577
Iteration 15/25 | Loss: 0.00063577
Iteration 16/25 | Loss: 0.00063577
Iteration 17/25 | Loss: 0.00063577
Iteration 18/25 | Loss: 0.00063577
Iteration 19/25 | Loss: 0.00063577
Iteration 20/25 | Loss: 0.00063577
Iteration 21/25 | Loss: 0.00063577
Iteration 22/25 | Loss: 0.00063577
Iteration 23/25 | Loss: 0.00063577
Iteration 24/25 | Loss: 0.00063577
Iteration 25/25 | Loss: 0.00063577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063577
Iteration 2/1000 | Loss: 0.00002964
Iteration 3/1000 | Loss: 0.00001986
Iteration 4/1000 | Loss: 0.00001769
Iteration 5/1000 | Loss: 0.00001665
Iteration 6/1000 | Loss: 0.00001569
Iteration 7/1000 | Loss: 0.00001519
Iteration 8/1000 | Loss: 0.00001484
Iteration 9/1000 | Loss: 0.00001456
Iteration 10/1000 | Loss: 0.00001431
Iteration 11/1000 | Loss: 0.00001409
Iteration 12/1000 | Loss: 0.00001403
Iteration 13/1000 | Loss: 0.00001396
Iteration 14/1000 | Loss: 0.00001388
Iteration 15/1000 | Loss: 0.00001384
Iteration 16/1000 | Loss: 0.00001383
Iteration 17/1000 | Loss: 0.00001380
Iteration 18/1000 | Loss: 0.00001372
Iteration 19/1000 | Loss: 0.00001368
Iteration 20/1000 | Loss: 0.00001368
Iteration 21/1000 | Loss: 0.00001367
Iteration 22/1000 | Loss: 0.00001364
Iteration 23/1000 | Loss: 0.00001360
Iteration 24/1000 | Loss: 0.00001358
Iteration 25/1000 | Loss: 0.00001357
Iteration 26/1000 | Loss: 0.00001354
Iteration 27/1000 | Loss: 0.00001351
Iteration 28/1000 | Loss: 0.00001351
Iteration 29/1000 | Loss: 0.00001350
Iteration 30/1000 | Loss: 0.00001349
Iteration 31/1000 | Loss: 0.00001347
Iteration 32/1000 | Loss: 0.00001347
Iteration 33/1000 | Loss: 0.00001346
Iteration 34/1000 | Loss: 0.00001346
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001342
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001342
Iteration 40/1000 | Loss: 0.00001342
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001337
Iteration 47/1000 | Loss: 0.00001337
Iteration 48/1000 | Loss: 0.00001337
Iteration 49/1000 | Loss: 0.00001337
Iteration 50/1000 | Loss: 0.00001337
Iteration 51/1000 | Loss: 0.00001336
Iteration 52/1000 | Loss: 0.00001336
Iteration 53/1000 | Loss: 0.00001335
Iteration 54/1000 | Loss: 0.00001334
Iteration 55/1000 | Loss: 0.00001334
Iteration 56/1000 | Loss: 0.00001333
Iteration 57/1000 | Loss: 0.00001333
Iteration 58/1000 | Loss: 0.00001333
Iteration 59/1000 | Loss: 0.00001333
Iteration 60/1000 | Loss: 0.00001333
Iteration 61/1000 | Loss: 0.00001333
Iteration 62/1000 | Loss: 0.00001333
Iteration 63/1000 | Loss: 0.00001333
Iteration 64/1000 | Loss: 0.00001333
Iteration 65/1000 | Loss: 0.00001333
Iteration 66/1000 | Loss: 0.00001333
Iteration 67/1000 | Loss: 0.00001332
Iteration 68/1000 | Loss: 0.00001332
Iteration 69/1000 | Loss: 0.00001331
Iteration 70/1000 | Loss: 0.00001331
Iteration 71/1000 | Loss: 0.00001331
Iteration 72/1000 | Loss: 0.00001330
Iteration 73/1000 | Loss: 0.00001330
Iteration 74/1000 | Loss: 0.00001330
Iteration 75/1000 | Loss: 0.00001330
Iteration 76/1000 | Loss: 0.00001330
Iteration 77/1000 | Loss: 0.00001329
Iteration 78/1000 | Loss: 0.00001329
Iteration 79/1000 | Loss: 0.00001329
Iteration 80/1000 | Loss: 0.00001329
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001328
Iteration 84/1000 | Loss: 0.00001328
Iteration 85/1000 | Loss: 0.00001327
Iteration 86/1000 | Loss: 0.00001327
Iteration 87/1000 | Loss: 0.00001326
Iteration 88/1000 | Loss: 0.00001326
Iteration 89/1000 | Loss: 0.00001326
Iteration 90/1000 | Loss: 0.00001326
Iteration 91/1000 | Loss: 0.00001325
Iteration 92/1000 | Loss: 0.00001325
Iteration 93/1000 | Loss: 0.00001325
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001324
Iteration 96/1000 | Loss: 0.00001324
Iteration 97/1000 | Loss: 0.00001324
Iteration 98/1000 | Loss: 0.00001324
Iteration 99/1000 | Loss: 0.00001324
Iteration 100/1000 | Loss: 0.00001324
Iteration 101/1000 | Loss: 0.00001324
Iteration 102/1000 | Loss: 0.00001323
Iteration 103/1000 | Loss: 0.00001323
Iteration 104/1000 | Loss: 0.00001323
Iteration 105/1000 | Loss: 0.00001323
Iteration 106/1000 | Loss: 0.00001323
Iteration 107/1000 | Loss: 0.00001322
Iteration 108/1000 | Loss: 0.00001322
Iteration 109/1000 | Loss: 0.00001322
Iteration 110/1000 | Loss: 0.00001322
Iteration 111/1000 | Loss: 0.00001322
Iteration 112/1000 | Loss: 0.00001322
Iteration 113/1000 | Loss: 0.00001322
Iteration 114/1000 | Loss: 0.00001322
Iteration 115/1000 | Loss: 0.00001322
Iteration 116/1000 | Loss: 0.00001321
Iteration 117/1000 | Loss: 0.00001321
Iteration 118/1000 | Loss: 0.00001321
Iteration 119/1000 | Loss: 0.00001321
Iteration 120/1000 | Loss: 0.00001321
Iteration 121/1000 | Loss: 0.00001321
Iteration 122/1000 | Loss: 0.00001321
Iteration 123/1000 | Loss: 0.00001321
Iteration 124/1000 | Loss: 0.00001320
Iteration 125/1000 | Loss: 0.00001320
Iteration 126/1000 | Loss: 0.00001320
Iteration 127/1000 | Loss: 0.00001320
Iteration 128/1000 | Loss: 0.00001320
Iteration 129/1000 | Loss: 0.00001320
Iteration 130/1000 | Loss: 0.00001320
Iteration 131/1000 | Loss: 0.00001320
Iteration 132/1000 | Loss: 0.00001319
Iteration 133/1000 | Loss: 0.00001319
Iteration 134/1000 | Loss: 0.00001319
Iteration 135/1000 | Loss: 0.00001319
Iteration 136/1000 | Loss: 0.00001319
Iteration 137/1000 | Loss: 0.00001319
Iteration 138/1000 | Loss: 0.00001319
Iteration 139/1000 | Loss: 0.00001319
Iteration 140/1000 | Loss: 0.00001319
Iteration 141/1000 | Loss: 0.00001319
Iteration 142/1000 | Loss: 0.00001318
Iteration 143/1000 | Loss: 0.00001318
Iteration 144/1000 | Loss: 0.00001318
Iteration 145/1000 | Loss: 0.00001318
Iteration 146/1000 | Loss: 0.00001318
Iteration 147/1000 | Loss: 0.00001318
Iteration 148/1000 | Loss: 0.00001318
Iteration 149/1000 | Loss: 0.00001317
Iteration 150/1000 | Loss: 0.00001317
Iteration 151/1000 | Loss: 0.00001317
Iteration 152/1000 | Loss: 0.00001317
Iteration 153/1000 | Loss: 0.00001317
Iteration 154/1000 | Loss: 0.00001317
Iteration 155/1000 | Loss: 0.00001317
Iteration 156/1000 | Loss: 0.00001317
Iteration 157/1000 | Loss: 0.00001317
Iteration 158/1000 | Loss: 0.00001317
Iteration 159/1000 | Loss: 0.00001317
Iteration 160/1000 | Loss: 0.00001317
Iteration 161/1000 | Loss: 0.00001317
Iteration 162/1000 | Loss: 0.00001317
Iteration 163/1000 | Loss: 0.00001317
Iteration 164/1000 | Loss: 0.00001317
Iteration 165/1000 | Loss: 0.00001317
Iteration 166/1000 | Loss: 0.00001317
Iteration 167/1000 | Loss: 0.00001317
Iteration 168/1000 | Loss: 0.00001317
Iteration 169/1000 | Loss: 0.00001317
Iteration 170/1000 | Loss: 0.00001317
Iteration 171/1000 | Loss: 0.00001317
Iteration 172/1000 | Loss: 0.00001317
Iteration 173/1000 | Loss: 0.00001317
Iteration 174/1000 | Loss: 0.00001317
Iteration 175/1000 | Loss: 0.00001317
Iteration 176/1000 | Loss: 0.00001317
Iteration 177/1000 | Loss: 0.00001317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.3168427358323243e-05, 1.3168427358323243e-05, 1.3168427358323243e-05, 1.3168427358323243e-05, 1.3168427358323243e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3168427358323243e-05

Optimization complete. Final v2v error: 3.0724098682403564 mm

Highest mean error: 3.41776180267334 mm for frame 0

Lowest mean error: 2.9137439727783203 mm for frame 32

Saving results

Total time: 48.06671094894409
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803316
Iteration 2/25 | Loss: 0.00178622
Iteration 3/25 | Loss: 0.00156020
Iteration 4/25 | Loss: 0.00156925
Iteration 5/25 | Loss: 0.00170472
Iteration 6/25 | Loss: 0.00151726
Iteration 7/25 | Loss: 0.00124171
Iteration 8/25 | Loss: 0.00117147
Iteration 9/25 | Loss: 0.00112047
Iteration 10/25 | Loss: 0.00113338
Iteration 11/25 | Loss: 0.00108094
Iteration 12/25 | Loss: 0.00109720
Iteration 13/25 | Loss: 0.00107349
Iteration 14/25 | Loss: 0.00107219
Iteration 15/25 | Loss: 0.00107208
Iteration 16/25 | Loss: 0.00107208
Iteration 17/25 | Loss: 0.00107208
Iteration 18/25 | Loss: 0.00107208
Iteration 19/25 | Loss: 0.00107208
Iteration 20/25 | Loss: 0.00107208
Iteration 21/25 | Loss: 0.00107208
Iteration 22/25 | Loss: 0.00107208
Iteration 23/25 | Loss: 0.00107208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010720780119299889, 0.0010720780119299889, 0.0010720780119299889, 0.0010720780119299889, 0.0010720780119299889]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010720780119299889

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36697173
Iteration 2/25 | Loss: 0.00081075
Iteration 3/25 | Loss: 0.00081075
Iteration 4/25 | Loss: 0.00081075
Iteration 5/25 | Loss: 0.00081075
Iteration 6/25 | Loss: 0.00081075
Iteration 7/25 | Loss: 0.00081075
Iteration 8/25 | Loss: 0.00081075
Iteration 9/25 | Loss: 0.00081075
Iteration 10/25 | Loss: 0.00081075
Iteration 11/25 | Loss: 0.00081074
Iteration 12/25 | Loss: 0.00081074
Iteration 13/25 | Loss: 0.00081074
Iteration 14/25 | Loss: 0.00081074
Iteration 15/25 | Loss: 0.00081074
Iteration 16/25 | Loss: 0.00081074
Iteration 17/25 | Loss: 0.00081074
Iteration 18/25 | Loss: 0.00081074
Iteration 19/25 | Loss: 0.00081074
Iteration 20/25 | Loss: 0.00081074
Iteration 21/25 | Loss: 0.00081074
Iteration 22/25 | Loss: 0.00081074
Iteration 23/25 | Loss: 0.00081074
Iteration 24/25 | Loss: 0.00081074
Iteration 25/25 | Loss: 0.00081074

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081074
Iteration 2/1000 | Loss: 0.00002338
Iteration 3/1000 | Loss: 0.00001631
Iteration 4/1000 | Loss: 0.00001533
Iteration 5/1000 | Loss: 0.00001446
Iteration 6/1000 | Loss: 0.00001387
Iteration 7/1000 | Loss: 0.00001354
Iteration 8/1000 | Loss: 0.00001341
Iteration 9/1000 | Loss: 0.00001317
Iteration 10/1000 | Loss: 0.00001313
Iteration 11/1000 | Loss: 0.00001300
Iteration 12/1000 | Loss: 0.00001288
Iteration 13/1000 | Loss: 0.00001288
Iteration 14/1000 | Loss: 0.00001287
Iteration 15/1000 | Loss: 0.00001286
Iteration 16/1000 | Loss: 0.00001285
Iteration 17/1000 | Loss: 0.00001281
Iteration 18/1000 | Loss: 0.00001281
Iteration 19/1000 | Loss: 0.00001281
Iteration 20/1000 | Loss: 0.00001279
Iteration 21/1000 | Loss: 0.00001278
Iteration 22/1000 | Loss: 0.00001276
Iteration 23/1000 | Loss: 0.00001276
Iteration 24/1000 | Loss: 0.00001275
Iteration 25/1000 | Loss: 0.00001275
Iteration 26/1000 | Loss: 0.00001275
Iteration 27/1000 | Loss: 0.00001275
Iteration 28/1000 | Loss: 0.00001275
Iteration 29/1000 | Loss: 0.00001275
Iteration 30/1000 | Loss: 0.00001275
Iteration 31/1000 | Loss: 0.00001275
Iteration 32/1000 | Loss: 0.00001275
Iteration 33/1000 | Loss: 0.00001275
Iteration 34/1000 | Loss: 0.00001274
Iteration 35/1000 | Loss: 0.00001274
Iteration 36/1000 | Loss: 0.00001274
Iteration 37/1000 | Loss: 0.00001274
Iteration 38/1000 | Loss: 0.00001274
Iteration 39/1000 | Loss: 0.00001273
Iteration 40/1000 | Loss: 0.00001273
Iteration 41/1000 | Loss: 0.00001271
Iteration 42/1000 | Loss: 0.00001271
Iteration 43/1000 | Loss: 0.00001271
Iteration 44/1000 | Loss: 0.00001270
Iteration 45/1000 | Loss: 0.00001270
Iteration 46/1000 | Loss: 0.00001270
Iteration 47/1000 | Loss: 0.00001270
Iteration 48/1000 | Loss: 0.00001269
Iteration 49/1000 | Loss: 0.00001269
Iteration 50/1000 | Loss: 0.00001269
Iteration 51/1000 | Loss: 0.00001269
Iteration 52/1000 | Loss: 0.00001269
Iteration 53/1000 | Loss: 0.00001268
Iteration 54/1000 | Loss: 0.00001268
Iteration 55/1000 | Loss: 0.00001268
Iteration 56/1000 | Loss: 0.00001268
Iteration 57/1000 | Loss: 0.00001267
Iteration 58/1000 | Loss: 0.00001267
Iteration 59/1000 | Loss: 0.00001267
Iteration 60/1000 | Loss: 0.00001267
Iteration 61/1000 | Loss: 0.00001267
Iteration 62/1000 | Loss: 0.00001267
Iteration 63/1000 | Loss: 0.00001266
Iteration 64/1000 | Loss: 0.00001266
Iteration 65/1000 | Loss: 0.00001266
Iteration 66/1000 | Loss: 0.00001266
Iteration 67/1000 | Loss: 0.00001266
Iteration 68/1000 | Loss: 0.00001266
Iteration 69/1000 | Loss: 0.00001266
Iteration 70/1000 | Loss: 0.00001266
Iteration 71/1000 | Loss: 0.00001266
Iteration 72/1000 | Loss: 0.00001266
Iteration 73/1000 | Loss: 0.00001265
Iteration 74/1000 | Loss: 0.00001265
Iteration 75/1000 | Loss: 0.00001265
Iteration 76/1000 | Loss: 0.00001265
Iteration 77/1000 | Loss: 0.00001265
Iteration 78/1000 | Loss: 0.00001265
Iteration 79/1000 | Loss: 0.00001265
Iteration 80/1000 | Loss: 0.00001265
Iteration 81/1000 | Loss: 0.00001264
Iteration 82/1000 | Loss: 0.00001264
Iteration 83/1000 | Loss: 0.00001264
Iteration 84/1000 | Loss: 0.00001264
Iteration 85/1000 | Loss: 0.00001264
Iteration 86/1000 | Loss: 0.00001264
Iteration 87/1000 | Loss: 0.00001264
Iteration 88/1000 | Loss: 0.00001263
Iteration 89/1000 | Loss: 0.00001263
Iteration 90/1000 | Loss: 0.00001263
Iteration 91/1000 | Loss: 0.00001263
Iteration 92/1000 | Loss: 0.00001263
Iteration 93/1000 | Loss: 0.00001263
Iteration 94/1000 | Loss: 0.00001263
Iteration 95/1000 | Loss: 0.00001263
Iteration 96/1000 | Loss: 0.00001263
Iteration 97/1000 | Loss: 0.00001263
Iteration 98/1000 | Loss: 0.00001262
Iteration 99/1000 | Loss: 0.00001262
Iteration 100/1000 | Loss: 0.00001262
Iteration 101/1000 | Loss: 0.00001262
Iteration 102/1000 | Loss: 0.00001262
Iteration 103/1000 | Loss: 0.00001261
Iteration 104/1000 | Loss: 0.00001261
Iteration 105/1000 | Loss: 0.00001261
Iteration 106/1000 | Loss: 0.00001261
Iteration 107/1000 | Loss: 0.00001260
Iteration 108/1000 | Loss: 0.00001260
Iteration 109/1000 | Loss: 0.00001260
Iteration 110/1000 | Loss: 0.00001260
Iteration 111/1000 | Loss: 0.00001260
Iteration 112/1000 | Loss: 0.00001260
Iteration 113/1000 | Loss: 0.00001260
Iteration 114/1000 | Loss: 0.00001260
Iteration 115/1000 | Loss: 0.00001259
Iteration 116/1000 | Loss: 0.00001259
Iteration 117/1000 | Loss: 0.00001259
Iteration 118/1000 | Loss: 0.00001259
Iteration 119/1000 | Loss: 0.00001259
Iteration 120/1000 | Loss: 0.00001259
Iteration 121/1000 | Loss: 0.00001259
Iteration 122/1000 | Loss: 0.00001259
Iteration 123/1000 | Loss: 0.00001259
Iteration 124/1000 | Loss: 0.00001259
Iteration 125/1000 | Loss: 0.00001259
Iteration 126/1000 | Loss: 0.00001259
Iteration 127/1000 | Loss: 0.00001259
Iteration 128/1000 | Loss: 0.00001259
Iteration 129/1000 | Loss: 0.00001259
Iteration 130/1000 | Loss: 0.00001259
Iteration 131/1000 | Loss: 0.00001259
Iteration 132/1000 | Loss: 0.00001259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.2594529835041612e-05, 1.2594529835041612e-05, 1.2594529835041612e-05, 1.2594529835041612e-05, 1.2594529835041612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2594529835041612e-05

Optimization complete. Final v2v error: 2.928399085998535 mm

Highest mean error: 3.9732465744018555 mm for frame 50

Lowest mean error: 2.3310062885284424 mm for frame 0

Saving results

Total time: 54.64449715614319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051141
Iteration 2/25 | Loss: 0.01051141
Iteration 3/25 | Loss: 0.01051141
Iteration 4/25 | Loss: 0.01051141
Iteration 5/25 | Loss: 0.01051141
Iteration 6/25 | Loss: 0.01051141
Iteration 7/25 | Loss: 0.01051141
Iteration 8/25 | Loss: 0.01051141
Iteration 9/25 | Loss: 0.01051141
Iteration 10/25 | Loss: 0.01051141
Iteration 11/25 | Loss: 0.01051141
Iteration 12/25 | Loss: 0.01051141
Iteration 13/25 | Loss: 0.01051141
Iteration 14/25 | Loss: 0.01051141
Iteration 15/25 | Loss: 0.01051141
Iteration 16/25 | Loss: 0.01051141
Iteration 17/25 | Loss: 0.01051141
Iteration 18/25 | Loss: 0.01051140
Iteration 19/25 | Loss: 0.01051140
Iteration 20/25 | Loss: 0.01051140
Iteration 21/25 | Loss: 0.01051140
Iteration 22/25 | Loss: 0.01051140
Iteration 23/25 | Loss: 0.01051140
Iteration 24/25 | Loss: 0.01051140
Iteration 25/25 | Loss: 0.01051140

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48356104
Iteration 2/25 | Loss: 0.12676208
Iteration 3/25 | Loss: 0.12655908
Iteration 4/25 | Loss: 0.12626280
Iteration 5/25 | Loss: 0.12625492
Iteration 6/25 | Loss: 0.12623781
Iteration 7/25 | Loss: 0.12623779
Iteration 8/25 | Loss: 0.12623776
Iteration 9/25 | Loss: 0.12623776
Iteration 10/25 | Loss: 0.12623776
Iteration 11/25 | Loss: 0.12623776
Iteration 12/25 | Loss: 0.12623776
Iteration 13/25 | Loss: 0.12623775
Iteration 14/25 | Loss: 0.12623775
Iteration 15/25 | Loss: 0.12623775
Iteration 16/25 | Loss: 0.12623775
Iteration 17/25 | Loss: 0.12623775
Iteration 18/25 | Loss: 0.12623775
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.12623775005340576, 0.12623775005340576, 0.12623775005340576, 0.12623775005340576, 0.12623775005340576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12623775005340576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12623775
Iteration 2/1000 | Loss: 0.00088663
Iteration 3/1000 | Loss: 0.00025506
Iteration 4/1000 | Loss: 0.00031632
Iteration 5/1000 | Loss: 0.00314824
Iteration 6/1000 | Loss: 0.00009862
Iteration 7/1000 | Loss: 0.00020834
Iteration 8/1000 | Loss: 0.00006141
Iteration 9/1000 | Loss: 0.00007155
Iteration 10/1000 | Loss: 0.00002114
Iteration 11/1000 | Loss: 0.00006630
Iteration 12/1000 | Loss: 0.00001685
Iteration 13/1000 | Loss: 0.00004624
Iteration 14/1000 | Loss: 0.00020474
Iteration 15/1000 | Loss: 0.00002466
Iteration 16/1000 | Loss: 0.00002636
Iteration 17/1000 | Loss: 0.00002655
Iteration 18/1000 | Loss: 0.00002163
Iteration 19/1000 | Loss: 0.00002932
Iteration 20/1000 | Loss: 0.00001544
Iteration 21/1000 | Loss: 0.00001178
Iteration 22/1000 | Loss: 0.00001352
Iteration 23/1000 | Loss: 0.00001469
Iteration 24/1000 | Loss: 0.00003809
Iteration 25/1000 | Loss: 0.00001219
Iteration 26/1000 | Loss: 0.00004535
Iteration 27/1000 | Loss: 0.00001263
Iteration 28/1000 | Loss: 0.00001029
Iteration 29/1000 | Loss: 0.00001436
Iteration 30/1000 | Loss: 0.00013182
Iteration 31/1000 | Loss: 0.00003832
Iteration 32/1000 | Loss: 0.00001318
Iteration 33/1000 | Loss: 0.00000915
Iteration 34/1000 | Loss: 0.00002998
Iteration 35/1000 | Loss: 0.00000903
Iteration 36/1000 | Loss: 0.00001272
Iteration 37/1000 | Loss: 0.00000999
Iteration 38/1000 | Loss: 0.00000999
Iteration 39/1000 | Loss: 0.00001123
Iteration 40/1000 | Loss: 0.00000875
Iteration 41/1000 | Loss: 0.00000875
Iteration 42/1000 | Loss: 0.00000874
Iteration 43/1000 | Loss: 0.00000874
Iteration 44/1000 | Loss: 0.00000874
Iteration 45/1000 | Loss: 0.00000874
Iteration 46/1000 | Loss: 0.00001003
Iteration 47/1000 | Loss: 0.00000866
Iteration 48/1000 | Loss: 0.00000865
Iteration 49/1000 | Loss: 0.00001026
Iteration 50/1000 | Loss: 0.00002369
Iteration 51/1000 | Loss: 0.00000870
Iteration 52/1000 | Loss: 0.00001185
Iteration 53/1000 | Loss: 0.00001237
Iteration 54/1000 | Loss: 0.00000861
Iteration 55/1000 | Loss: 0.00000861
Iteration 56/1000 | Loss: 0.00003841
Iteration 57/1000 | Loss: 0.00003662
Iteration 58/1000 | Loss: 0.00002272
Iteration 59/1000 | Loss: 0.00010356
Iteration 60/1000 | Loss: 0.00002440
Iteration 61/1000 | Loss: 0.00023338
Iteration 62/1000 | Loss: 0.00002100
Iteration 63/1000 | Loss: 0.00001482
Iteration 64/1000 | Loss: 0.00000943
Iteration 65/1000 | Loss: 0.00001231
Iteration 66/1000 | Loss: 0.00004878
Iteration 67/1000 | Loss: 0.00000985
Iteration 68/1000 | Loss: 0.00001014
Iteration 69/1000 | Loss: 0.00002134
Iteration 70/1000 | Loss: 0.00002107
Iteration 71/1000 | Loss: 0.00001070
Iteration 72/1000 | Loss: 0.00001571
Iteration 73/1000 | Loss: 0.00000973
Iteration 74/1000 | Loss: 0.00000836
Iteration 75/1000 | Loss: 0.00000836
Iteration 76/1000 | Loss: 0.00000836
Iteration 77/1000 | Loss: 0.00000836
Iteration 78/1000 | Loss: 0.00000903
Iteration 79/1000 | Loss: 0.00000903
Iteration 80/1000 | Loss: 0.00003143
Iteration 81/1000 | Loss: 0.00000894
Iteration 82/1000 | Loss: 0.00001239
Iteration 83/1000 | Loss: 0.00001569
Iteration 84/1000 | Loss: 0.00000868
Iteration 85/1000 | Loss: 0.00001222
Iteration 86/1000 | Loss: 0.00003327
Iteration 87/1000 | Loss: 0.00000990
Iteration 88/1000 | Loss: 0.00000983
Iteration 89/1000 | Loss: 0.00000812
Iteration 90/1000 | Loss: 0.00000811
Iteration 91/1000 | Loss: 0.00000811
Iteration 92/1000 | Loss: 0.00000811
Iteration 93/1000 | Loss: 0.00000811
Iteration 94/1000 | Loss: 0.00000811
Iteration 95/1000 | Loss: 0.00000811
Iteration 96/1000 | Loss: 0.00000811
Iteration 97/1000 | Loss: 0.00000811
Iteration 98/1000 | Loss: 0.00000810
Iteration 99/1000 | Loss: 0.00000810
Iteration 100/1000 | Loss: 0.00000810
Iteration 101/1000 | Loss: 0.00000810
Iteration 102/1000 | Loss: 0.00000810
Iteration 103/1000 | Loss: 0.00000810
Iteration 104/1000 | Loss: 0.00000809
Iteration 105/1000 | Loss: 0.00000809
Iteration 106/1000 | Loss: 0.00000857
Iteration 107/1000 | Loss: 0.00001281
Iteration 108/1000 | Loss: 0.00000812
Iteration 109/1000 | Loss: 0.00000809
Iteration 110/1000 | Loss: 0.00000807
Iteration 111/1000 | Loss: 0.00000807
Iteration 112/1000 | Loss: 0.00000807
Iteration 113/1000 | Loss: 0.00000807
Iteration 114/1000 | Loss: 0.00000806
Iteration 115/1000 | Loss: 0.00000806
Iteration 116/1000 | Loss: 0.00000806
Iteration 117/1000 | Loss: 0.00000806
Iteration 118/1000 | Loss: 0.00000806
Iteration 119/1000 | Loss: 0.00000806
Iteration 120/1000 | Loss: 0.00000806
Iteration 121/1000 | Loss: 0.00000806
Iteration 122/1000 | Loss: 0.00000806
Iteration 123/1000 | Loss: 0.00000806
Iteration 124/1000 | Loss: 0.00000806
Iteration 125/1000 | Loss: 0.00000806
Iteration 126/1000 | Loss: 0.00000806
Iteration 127/1000 | Loss: 0.00000806
Iteration 128/1000 | Loss: 0.00000806
Iteration 129/1000 | Loss: 0.00000866
Iteration 130/1000 | Loss: 0.00000805
Iteration 131/1000 | Loss: 0.00000805
Iteration 132/1000 | Loss: 0.00000805
Iteration 133/1000 | Loss: 0.00000805
Iteration 134/1000 | Loss: 0.00000805
Iteration 135/1000 | Loss: 0.00000805
Iteration 136/1000 | Loss: 0.00000805
Iteration 137/1000 | Loss: 0.00000805
Iteration 138/1000 | Loss: 0.00000805
Iteration 139/1000 | Loss: 0.00000805
Iteration 140/1000 | Loss: 0.00000805
Iteration 141/1000 | Loss: 0.00000805
Iteration 142/1000 | Loss: 0.00000805
Iteration 143/1000 | Loss: 0.00000805
Iteration 144/1000 | Loss: 0.00000805
Iteration 145/1000 | Loss: 0.00000805
Iteration 146/1000 | Loss: 0.00000805
Iteration 147/1000 | Loss: 0.00000805
Iteration 148/1000 | Loss: 0.00000805
Iteration 149/1000 | Loss: 0.00000805
Iteration 150/1000 | Loss: 0.00000805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [8.047875780903269e-06, 8.047875780903269e-06, 8.047875780903269e-06, 8.047875780903269e-06, 8.047875780903269e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.047875780903269e-06

Optimization complete. Final v2v error: 2.4465107917785645 mm

Highest mean error: 2.7828104496002197 mm for frame 28

Lowest mean error: 2.159360885620117 mm for frame 142

Saving results

Total time: 122.02232885360718
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444753
Iteration 2/25 | Loss: 0.00116567
Iteration 3/25 | Loss: 0.00106389
Iteration 4/25 | Loss: 0.00104445
Iteration 5/25 | Loss: 0.00103798
Iteration 6/25 | Loss: 0.00103616
Iteration 7/25 | Loss: 0.00103562
Iteration 8/25 | Loss: 0.00103562
Iteration 9/25 | Loss: 0.00103562
Iteration 10/25 | Loss: 0.00103562
Iteration 11/25 | Loss: 0.00103562
Iteration 12/25 | Loss: 0.00103562
Iteration 13/25 | Loss: 0.00103562
Iteration 14/25 | Loss: 0.00103562
Iteration 15/25 | Loss: 0.00103562
Iteration 16/25 | Loss: 0.00103562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010356184793636203, 0.0010356184793636203, 0.0010356184793636203, 0.0010356184793636203, 0.0010356184793636203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010356184793636203

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25891328
Iteration 2/25 | Loss: 0.00066419
Iteration 3/25 | Loss: 0.00066415
Iteration 4/25 | Loss: 0.00066415
Iteration 5/25 | Loss: 0.00066415
Iteration 6/25 | Loss: 0.00066415
Iteration 7/25 | Loss: 0.00066415
Iteration 8/25 | Loss: 0.00066415
Iteration 9/25 | Loss: 0.00066415
Iteration 10/25 | Loss: 0.00066415
Iteration 11/25 | Loss: 0.00066415
Iteration 12/25 | Loss: 0.00066415
Iteration 13/25 | Loss: 0.00066415
Iteration 14/25 | Loss: 0.00066415
Iteration 15/25 | Loss: 0.00066415
Iteration 16/25 | Loss: 0.00066415
Iteration 17/25 | Loss: 0.00066415
Iteration 18/25 | Loss: 0.00066415
Iteration 19/25 | Loss: 0.00066415
Iteration 20/25 | Loss: 0.00066415
Iteration 21/25 | Loss: 0.00066415
Iteration 22/25 | Loss: 0.00066415
Iteration 23/25 | Loss: 0.00066415
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006641471991315484, 0.0006641471991315484, 0.0006641471991315484, 0.0006641471991315484, 0.0006641471991315484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006641471991315484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066415
Iteration 2/1000 | Loss: 0.00002928
Iteration 3/1000 | Loss: 0.00001838
Iteration 4/1000 | Loss: 0.00001488
Iteration 5/1000 | Loss: 0.00001382
Iteration 6/1000 | Loss: 0.00001288
Iteration 7/1000 | Loss: 0.00001233
Iteration 8/1000 | Loss: 0.00001187
Iteration 9/1000 | Loss: 0.00001162
Iteration 10/1000 | Loss: 0.00001153
Iteration 11/1000 | Loss: 0.00001151
Iteration 12/1000 | Loss: 0.00001148
Iteration 13/1000 | Loss: 0.00001133
Iteration 14/1000 | Loss: 0.00001130
Iteration 15/1000 | Loss: 0.00001122
Iteration 16/1000 | Loss: 0.00001119
Iteration 17/1000 | Loss: 0.00001118
Iteration 18/1000 | Loss: 0.00001117
Iteration 19/1000 | Loss: 0.00001117
Iteration 20/1000 | Loss: 0.00001116
Iteration 21/1000 | Loss: 0.00001115
Iteration 22/1000 | Loss: 0.00001115
Iteration 23/1000 | Loss: 0.00001115
Iteration 24/1000 | Loss: 0.00001114
Iteration 25/1000 | Loss: 0.00001113
Iteration 26/1000 | Loss: 0.00001113
Iteration 27/1000 | Loss: 0.00001112
Iteration 28/1000 | Loss: 0.00001112
Iteration 29/1000 | Loss: 0.00001111
Iteration 30/1000 | Loss: 0.00001111
Iteration 31/1000 | Loss: 0.00001110
Iteration 32/1000 | Loss: 0.00001110
Iteration 33/1000 | Loss: 0.00001110
Iteration 34/1000 | Loss: 0.00001110
Iteration 35/1000 | Loss: 0.00001109
Iteration 36/1000 | Loss: 0.00001109
Iteration 37/1000 | Loss: 0.00001108
Iteration 38/1000 | Loss: 0.00001107
Iteration 39/1000 | Loss: 0.00001104
Iteration 40/1000 | Loss: 0.00001102
Iteration 41/1000 | Loss: 0.00001102
Iteration 42/1000 | Loss: 0.00001100
Iteration 43/1000 | Loss: 0.00001098
Iteration 44/1000 | Loss: 0.00001097
Iteration 45/1000 | Loss: 0.00001097
Iteration 46/1000 | Loss: 0.00001097
Iteration 47/1000 | Loss: 0.00001096
Iteration 48/1000 | Loss: 0.00001096
Iteration 49/1000 | Loss: 0.00001095
Iteration 50/1000 | Loss: 0.00001095
Iteration 51/1000 | Loss: 0.00001094
Iteration 52/1000 | Loss: 0.00001093
Iteration 53/1000 | Loss: 0.00001091
Iteration 54/1000 | Loss: 0.00001091
Iteration 55/1000 | Loss: 0.00001091
Iteration 56/1000 | Loss: 0.00001091
Iteration 57/1000 | Loss: 0.00001091
Iteration 58/1000 | Loss: 0.00001091
Iteration 59/1000 | Loss: 0.00001091
Iteration 60/1000 | Loss: 0.00001090
Iteration 61/1000 | Loss: 0.00001090
Iteration 62/1000 | Loss: 0.00001090
Iteration 63/1000 | Loss: 0.00001090
Iteration 64/1000 | Loss: 0.00001090
Iteration 65/1000 | Loss: 0.00001090
Iteration 66/1000 | Loss: 0.00001090
Iteration 67/1000 | Loss: 0.00001090
Iteration 68/1000 | Loss: 0.00001090
Iteration 69/1000 | Loss: 0.00001089
Iteration 70/1000 | Loss: 0.00001089
Iteration 71/1000 | Loss: 0.00001088
Iteration 72/1000 | Loss: 0.00001088
Iteration 73/1000 | Loss: 0.00001087
Iteration 74/1000 | Loss: 0.00001087
Iteration 75/1000 | Loss: 0.00001086
Iteration 76/1000 | Loss: 0.00001086
Iteration 77/1000 | Loss: 0.00001086
Iteration 78/1000 | Loss: 0.00001086
Iteration 79/1000 | Loss: 0.00001085
Iteration 80/1000 | Loss: 0.00001085
Iteration 81/1000 | Loss: 0.00001085
Iteration 82/1000 | Loss: 0.00001085
Iteration 83/1000 | Loss: 0.00001085
Iteration 84/1000 | Loss: 0.00001085
Iteration 85/1000 | Loss: 0.00001085
Iteration 86/1000 | Loss: 0.00001084
Iteration 87/1000 | Loss: 0.00001084
Iteration 88/1000 | Loss: 0.00001083
Iteration 89/1000 | Loss: 0.00001083
Iteration 90/1000 | Loss: 0.00001083
Iteration 91/1000 | Loss: 0.00001083
Iteration 92/1000 | Loss: 0.00001083
Iteration 93/1000 | Loss: 0.00001082
Iteration 94/1000 | Loss: 0.00001082
Iteration 95/1000 | Loss: 0.00001082
Iteration 96/1000 | Loss: 0.00001082
Iteration 97/1000 | Loss: 0.00001082
Iteration 98/1000 | Loss: 0.00001082
Iteration 99/1000 | Loss: 0.00001082
Iteration 100/1000 | Loss: 0.00001082
Iteration 101/1000 | Loss: 0.00001082
Iteration 102/1000 | Loss: 0.00001082
Iteration 103/1000 | Loss: 0.00001082
Iteration 104/1000 | Loss: 0.00001082
Iteration 105/1000 | Loss: 0.00001082
Iteration 106/1000 | Loss: 0.00001082
Iteration 107/1000 | Loss: 0.00001082
Iteration 108/1000 | Loss: 0.00001081
Iteration 109/1000 | Loss: 0.00001081
Iteration 110/1000 | Loss: 0.00001081
Iteration 111/1000 | Loss: 0.00001081
Iteration 112/1000 | Loss: 0.00001081
Iteration 113/1000 | Loss: 0.00001081
Iteration 114/1000 | Loss: 0.00001081
Iteration 115/1000 | Loss: 0.00001081
Iteration 116/1000 | Loss: 0.00001081
Iteration 117/1000 | Loss: 0.00001081
Iteration 118/1000 | Loss: 0.00001081
Iteration 119/1000 | Loss: 0.00001081
Iteration 120/1000 | Loss: 0.00001081
Iteration 121/1000 | Loss: 0.00001081
Iteration 122/1000 | Loss: 0.00001080
Iteration 123/1000 | Loss: 0.00001080
Iteration 124/1000 | Loss: 0.00001080
Iteration 125/1000 | Loss: 0.00001080
Iteration 126/1000 | Loss: 0.00001080
Iteration 127/1000 | Loss: 0.00001080
Iteration 128/1000 | Loss: 0.00001080
Iteration 129/1000 | Loss: 0.00001080
Iteration 130/1000 | Loss: 0.00001080
Iteration 131/1000 | Loss: 0.00001080
Iteration 132/1000 | Loss: 0.00001080
Iteration 133/1000 | Loss: 0.00001080
Iteration 134/1000 | Loss: 0.00001080
Iteration 135/1000 | Loss: 0.00001080
Iteration 136/1000 | Loss: 0.00001080
Iteration 137/1000 | Loss: 0.00001080
Iteration 138/1000 | Loss: 0.00001079
Iteration 139/1000 | Loss: 0.00001079
Iteration 140/1000 | Loss: 0.00001079
Iteration 141/1000 | Loss: 0.00001079
Iteration 142/1000 | Loss: 0.00001079
Iteration 143/1000 | Loss: 0.00001079
Iteration 144/1000 | Loss: 0.00001079
Iteration 145/1000 | Loss: 0.00001079
Iteration 146/1000 | Loss: 0.00001079
Iteration 147/1000 | Loss: 0.00001079
Iteration 148/1000 | Loss: 0.00001078
Iteration 149/1000 | Loss: 0.00001078
Iteration 150/1000 | Loss: 0.00001078
Iteration 151/1000 | Loss: 0.00001078
Iteration 152/1000 | Loss: 0.00001077
Iteration 153/1000 | Loss: 0.00001077
Iteration 154/1000 | Loss: 0.00001077
Iteration 155/1000 | Loss: 0.00001077
Iteration 156/1000 | Loss: 0.00001077
Iteration 157/1000 | Loss: 0.00001077
Iteration 158/1000 | Loss: 0.00001077
Iteration 159/1000 | Loss: 0.00001076
Iteration 160/1000 | Loss: 0.00001076
Iteration 161/1000 | Loss: 0.00001076
Iteration 162/1000 | Loss: 0.00001076
Iteration 163/1000 | Loss: 0.00001076
Iteration 164/1000 | Loss: 0.00001076
Iteration 165/1000 | Loss: 0.00001076
Iteration 166/1000 | Loss: 0.00001075
Iteration 167/1000 | Loss: 0.00001075
Iteration 168/1000 | Loss: 0.00001075
Iteration 169/1000 | Loss: 0.00001075
Iteration 170/1000 | Loss: 0.00001075
Iteration 171/1000 | Loss: 0.00001075
Iteration 172/1000 | Loss: 0.00001075
Iteration 173/1000 | Loss: 0.00001075
Iteration 174/1000 | Loss: 0.00001075
Iteration 175/1000 | Loss: 0.00001075
Iteration 176/1000 | Loss: 0.00001075
Iteration 177/1000 | Loss: 0.00001074
Iteration 178/1000 | Loss: 0.00001074
Iteration 179/1000 | Loss: 0.00001074
Iteration 180/1000 | Loss: 0.00001074
Iteration 181/1000 | Loss: 0.00001074
Iteration 182/1000 | Loss: 0.00001074
Iteration 183/1000 | Loss: 0.00001074
Iteration 184/1000 | Loss: 0.00001074
Iteration 185/1000 | Loss: 0.00001074
Iteration 186/1000 | Loss: 0.00001074
Iteration 187/1000 | Loss: 0.00001074
Iteration 188/1000 | Loss: 0.00001073
Iteration 189/1000 | Loss: 0.00001073
Iteration 190/1000 | Loss: 0.00001073
Iteration 191/1000 | Loss: 0.00001073
Iteration 192/1000 | Loss: 0.00001073
Iteration 193/1000 | Loss: 0.00001073
Iteration 194/1000 | Loss: 0.00001073
Iteration 195/1000 | Loss: 0.00001073
Iteration 196/1000 | Loss: 0.00001073
Iteration 197/1000 | Loss: 0.00001073
Iteration 198/1000 | Loss: 0.00001073
Iteration 199/1000 | Loss: 0.00001073
Iteration 200/1000 | Loss: 0.00001073
Iteration 201/1000 | Loss: 0.00001073
Iteration 202/1000 | Loss: 0.00001073
Iteration 203/1000 | Loss: 0.00001073
Iteration 204/1000 | Loss: 0.00001073
Iteration 205/1000 | Loss: 0.00001073
Iteration 206/1000 | Loss: 0.00001073
Iteration 207/1000 | Loss: 0.00001073
Iteration 208/1000 | Loss: 0.00001073
Iteration 209/1000 | Loss: 0.00001073
Iteration 210/1000 | Loss: 0.00001073
Iteration 211/1000 | Loss: 0.00001073
Iteration 212/1000 | Loss: 0.00001073
Iteration 213/1000 | Loss: 0.00001073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.072833674697904e-05, 1.072833674697904e-05, 1.072833674697904e-05, 1.072833674697904e-05, 1.072833674697904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.072833674697904e-05

Optimization complete. Final v2v error: 2.8119773864746094 mm

Highest mean error: 3.2978355884552 mm for frame 57

Lowest mean error: 2.5094709396362305 mm for frame 16

Saving results

Total time: 41.43992257118225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402255
Iteration 2/25 | Loss: 0.00118310
Iteration 3/25 | Loss: 0.00107640
Iteration 4/25 | Loss: 0.00106745
Iteration 5/25 | Loss: 0.00106493
Iteration 6/25 | Loss: 0.00106420
Iteration 7/25 | Loss: 0.00106420
Iteration 8/25 | Loss: 0.00106420
Iteration 9/25 | Loss: 0.00106420
Iteration 10/25 | Loss: 0.00106420
Iteration 11/25 | Loss: 0.00106420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010641971603035927, 0.0010641971603035927, 0.0010641971603035927, 0.0010641971603035927, 0.0010641971603035927]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010641971603035927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34970009
Iteration 2/25 | Loss: 0.00088718
Iteration 3/25 | Loss: 0.00088715
Iteration 4/25 | Loss: 0.00088715
Iteration 5/25 | Loss: 0.00088715
Iteration 6/25 | Loss: 0.00088715
Iteration 7/25 | Loss: 0.00088715
Iteration 8/25 | Loss: 0.00088715
Iteration 9/25 | Loss: 0.00088715
Iteration 10/25 | Loss: 0.00088715
Iteration 11/25 | Loss: 0.00088715
Iteration 12/25 | Loss: 0.00088715
Iteration 13/25 | Loss: 0.00088715
Iteration 14/25 | Loss: 0.00088715
Iteration 15/25 | Loss: 0.00088715
Iteration 16/25 | Loss: 0.00088715
Iteration 17/25 | Loss: 0.00088715
Iteration 18/25 | Loss: 0.00088715
Iteration 19/25 | Loss: 0.00088715
Iteration 20/25 | Loss: 0.00088715
Iteration 21/25 | Loss: 0.00088715
Iteration 22/25 | Loss: 0.00088715
Iteration 23/25 | Loss: 0.00088715
Iteration 24/25 | Loss: 0.00088715
Iteration 25/25 | Loss: 0.00088715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088715
Iteration 2/1000 | Loss: 0.00002439
Iteration 3/1000 | Loss: 0.00001548
Iteration 4/1000 | Loss: 0.00001244
Iteration 5/1000 | Loss: 0.00001145
Iteration 6/1000 | Loss: 0.00001075
Iteration 7/1000 | Loss: 0.00001034
Iteration 8/1000 | Loss: 0.00001024
Iteration 9/1000 | Loss: 0.00001002
Iteration 10/1000 | Loss: 0.00000995
Iteration 11/1000 | Loss: 0.00000993
Iteration 12/1000 | Loss: 0.00000988
Iteration 13/1000 | Loss: 0.00000988
Iteration 14/1000 | Loss: 0.00000988
Iteration 15/1000 | Loss: 0.00000987
Iteration 16/1000 | Loss: 0.00000981
Iteration 17/1000 | Loss: 0.00000977
Iteration 18/1000 | Loss: 0.00000977
Iteration 19/1000 | Loss: 0.00000977
Iteration 20/1000 | Loss: 0.00000975
Iteration 21/1000 | Loss: 0.00000973
Iteration 22/1000 | Loss: 0.00000972
Iteration 23/1000 | Loss: 0.00000972
Iteration 24/1000 | Loss: 0.00000972
Iteration 25/1000 | Loss: 0.00000972
Iteration 26/1000 | Loss: 0.00000971
Iteration 27/1000 | Loss: 0.00000971
Iteration 28/1000 | Loss: 0.00000971
Iteration 29/1000 | Loss: 0.00000971
Iteration 30/1000 | Loss: 0.00000971
Iteration 31/1000 | Loss: 0.00000970
Iteration 32/1000 | Loss: 0.00000969
Iteration 33/1000 | Loss: 0.00000969
Iteration 34/1000 | Loss: 0.00000968
Iteration 35/1000 | Loss: 0.00000967
Iteration 36/1000 | Loss: 0.00000967
Iteration 37/1000 | Loss: 0.00000965
Iteration 38/1000 | Loss: 0.00000964
Iteration 39/1000 | Loss: 0.00000964
Iteration 40/1000 | Loss: 0.00000963
Iteration 41/1000 | Loss: 0.00000962
Iteration 42/1000 | Loss: 0.00000962
Iteration 43/1000 | Loss: 0.00000962
Iteration 44/1000 | Loss: 0.00000961
Iteration 45/1000 | Loss: 0.00000961
Iteration 46/1000 | Loss: 0.00000960
Iteration 47/1000 | Loss: 0.00000960
Iteration 48/1000 | Loss: 0.00000959
Iteration 49/1000 | Loss: 0.00000959
Iteration 50/1000 | Loss: 0.00000959
Iteration 51/1000 | Loss: 0.00000958
Iteration 52/1000 | Loss: 0.00000958
Iteration 53/1000 | Loss: 0.00000957
Iteration 54/1000 | Loss: 0.00000957
Iteration 55/1000 | Loss: 0.00000957
Iteration 56/1000 | Loss: 0.00000956
Iteration 57/1000 | Loss: 0.00000956
Iteration 58/1000 | Loss: 0.00000955
Iteration 59/1000 | Loss: 0.00000955
Iteration 60/1000 | Loss: 0.00000955
Iteration 61/1000 | Loss: 0.00000954
Iteration 62/1000 | Loss: 0.00000954
Iteration 63/1000 | Loss: 0.00000952
Iteration 64/1000 | Loss: 0.00000947
Iteration 65/1000 | Loss: 0.00000946
Iteration 66/1000 | Loss: 0.00000946
Iteration 67/1000 | Loss: 0.00000945
Iteration 68/1000 | Loss: 0.00000945
Iteration 69/1000 | Loss: 0.00000945
Iteration 70/1000 | Loss: 0.00000945
Iteration 71/1000 | Loss: 0.00000944
Iteration 72/1000 | Loss: 0.00000944
Iteration 73/1000 | Loss: 0.00000944
Iteration 74/1000 | Loss: 0.00000943
Iteration 75/1000 | Loss: 0.00000942
Iteration 76/1000 | Loss: 0.00000942
Iteration 77/1000 | Loss: 0.00000942
Iteration 78/1000 | Loss: 0.00000941
Iteration 79/1000 | Loss: 0.00000941
Iteration 80/1000 | Loss: 0.00000941
Iteration 81/1000 | Loss: 0.00000940
Iteration 82/1000 | Loss: 0.00000939
Iteration 83/1000 | Loss: 0.00000939
Iteration 84/1000 | Loss: 0.00000939
Iteration 85/1000 | Loss: 0.00000939
Iteration 86/1000 | Loss: 0.00000939
Iteration 87/1000 | Loss: 0.00000939
Iteration 88/1000 | Loss: 0.00000939
Iteration 89/1000 | Loss: 0.00000938
Iteration 90/1000 | Loss: 0.00000938
Iteration 91/1000 | Loss: 0.00000937
Iteration 92/1000 | Loss: 0.00000937
Iteration 93/1000 | Loss: 0.00000937
Iteration 94/1000 | Loss: 0.00000937
Iteration 95/1000 | Loss: 0.00000937
Iteration 96/1000 | Loss: 0.00000937
Iteration 97/1000 | Loss: 0.00000937
Iteration 98/1000 | Loss: 0.00000936
Iteration 99/1000 | Loss: 0.00000936
Iteration 100/1000 | Loss: 0.00000936
Iteration 101/1000 | Loss: 0.00000936
Iteration 102/1000 | Loss: 0.00000936
Iteration 103/1000 | Loss: 0.00000936
Iteration 104/1000 | Loss: 0.00000936
Iteration 105/1000 | Loss: 0.00000936
Iteration 106/1000 | Loss: 0.00000936
Iteration 107/1000 | Loss: 0.00000935
Iteration 108/1000 | Loss: 0.00000935
Iteration 109/1000 | Loss: 0.00000935
Iteration 110/1000 | Loss: 0.00000935
Iteration 111/1000 | Loss: 0.00000935
Iteration 112/1000 | Loss: 0.00000935
Iteration 113/1000 | Loss: 0.00000935
Iteration 114/1000 | Loss: 0.00000935
Iteration 115/1000 | Loss: 0.00000934
Iteration 116/1000 | Loss: 0.00000934
Iteration 117/1000 | Loss: 0.00000934
Iteration 118/1000 | Loss: 0.00000933
Iteration 119/1000 | Loss: 0.00000933
Iteration 120/1000 | Loss: 0.00000933
Iteration 121/1000 | Loss: 0.00000933
Iteration 122/1000 | Loss: 0.00000933
Iteration 123/1000 | Loss: 0.00000933
Iteration 124/1000 | Loss: 0.00000933
Iteration 125/1000 | Loss: 0.00000933
Iteration 126/1000 | Loss: 0.00000933
Iteration 127/1000 | Loss: 0.00000933
Iteration 128/1000 | Loss: 0.00000933
Iteration 129/1000 | Loss: 0.00000933
Iteration 130/1000 | Loss: 0.00000932
Iteration 131/1000 | Loss: 0.00000932
Iteration 132/1000 | Loss: 0.00000932
Iteration 133/1000 | Loss: 0.00000932
Iteration 134/1000 | Loss: 0.00000932
Iteration 135/1000 | Loss: 0.00000932
Iteration 136/1000 | Loss: 0.00000932
Iteration 137/1000 | Loss: 0.00000932
Iteration 138/1000 | Loss: 0.00000932
Iteration 139/1000 | Loss: 0.00000932
Iteration 140/1000 | Loss: 0.00000931
Iteration 141/1000 | Loss: 0.00000931
Iteration 142/1000 | Loss: 0.00000931
Iteration 143/1000 | Loss: 0.00000931
Iteration 144/1000 | Loss: 0.00000931
Iteration 145/1000 | Loss: 0.00000930
Iteration 146/1000 | Loss: 0.00000930
Iteration 147/1000 | Loss: 0.00000930
Iteration 148/1000 | Loss: 0.00000930
Iteration 149/1000 | Loss: 0.00000930
Iteration 150/1000 | Loss: 0.00000930
Iteration 151/1000 | Loss: 0.00000930
Iteration 152/1000 | Loss: 0.00000930
Iteration 153/1000 | Loss: 0.00000930
Iteration 154/1000 | Loss: 0.00000930
Iteration 155/1000 | Loss: 0.00000930
Iteration 156/1000 | Loss: 0.00000930
Iteration 157/1000 | Loss: 0.00000929
Iteration 158/1000 | Loss: 0.00000929
Iteration 159/1000 | Loss: 0.00000929
Iteration 160/1000 | Loss: 0.00000929
Iteration 161/1000 | Loss: 0.00000929
Iteration 162/1000 | Loss: 0.00000929
Iteration 163/1000 | Loss: 0.00000929
Iteration 164/1000 | Loss: 0.00000929
Iteration 165/1000 | Loss: 0.00000928
Iteration 166/1000 | Loss: 0.00000928
Iteration 167/1000 | Loss: 0.00000928
Iteration 168/1000 | Loss: 0.00000928
Iteration 169/1000 | Loss: 0.00000928
Iteration 170/1000 | Loss: 0.00000928
Iteration 171/1000 | Loss: 0.00000928
Iteration 172/1000 | Loss: 0.00000928
Iteration 173/1000 | Loss: 0.00000927
Iteration 174/1000 | Loss: 0.00000927
Iteration 175/1000 | Loss: 0.00000927
Iteration 176/1000 | Loss: 0.00000927
Iteration 177/1000 | Loss: 0.00000927
Iteration 178/1000 | Loss: 0.00000927
Iteration 179/1000 | Loss: 0.00000926
Iteration 180/1000 | Loss: 0.00000926
Iteration 181/1000 | Loss: 0.00000926
Iteration 182/1000 | Loss: 0.00000926
Iteration 183/1000 | Loss: 0.00000926
Iteration 184/1000 | Loss: 0.00000926
Iteration 185/1000 | Loss: 0.00000926
Iteration 186/1000 | Loss: 0.00000926
Iteration 187/1000 | Loss: 0.00000926
Iteration 188/1000 | Loss: 0.00000926
Iteration 189/1000 | Loss: 0.00000925
Iteration 190/1000 | Loss: 0.00000925
Iteration 191/1000 | Loss: 0.00000925
Iteration 192/1000 | Loss: 0.00000925
Iteration 193/1000 | Loss: 0.00000925
Iteration 194/1000 | Loss: 0.00000925
Iteration 195/1000 | Loss: 0.00000925
Iteration 196/1000 | Loss: 0.00000925
Iteration 197/1000 | Loss: 0.00000925
Iteration 198/1000 | Loss: 0.00000924
Iteration 199/1000 | Loss: 0.00000924
Iteration 200/1000 | Loss: 0.00000924
Iteration 201/1000 | Loss: 0.00000924
Iteration 202/1000 | Loss: 0.00000924
Iteration 203/1000 | Loss: 0.00000924
Iteration 204/1000 | Loss: 0.00000924
Iteration 205/1000 | Loss: 0.00000924
Iteration 206/1000 | Loss: 0.00000924
Iteration 207/1000 | Loss: 0.00000924
Iteration 208/1000 | Loss: 0.00000923
Iteration 209/1000 | Loss: 0.00000923
Iteration 210/1000 | Loss: 0.00000923
Iteration 211/1000 | Loss: 0.00000922
Iteration 212/1000 | Loss: 0.00000922
Iteration 213/1000 | Loss: 0.00000921
Iteration 214/1000 | Loss: 0.00000921
Iteration 215/1000 | Loss: 0.00000921
Iteration 216/1000 | Loss: 0.00000920
Iteration 217/1000 | Loss: 0.00000920
Iteration 218/1000 | Loss: 0.00000920
Iteration 219/1000 | Loss: 0.00000920
Iteration 220/1000 | Loss: 0.00000920
Iteration 221/1000 | Loss: 0.00000919
Iteration 222/1000 | Loss: 0.00000919
Iteration 223/1000 | Loss: 0.00000919
Iteration 224/1000 | Loss: 0.00000919
Iteration 225/1000 | Loss: 0.00000918
Iteration 226/1000 | Loss: 0.00000918
Iteration 227/1000 | Loss: 0.00000918
Iteration 228/1000 | Loss: 0.00000918
Iteration 229/1000 | Loss: 0.00000918
Iteration 230/1000 | Loss: 0.00000918
Iteration 231/1000 | Loss: 0.00000918
Iteration 232/1000 | Loss: 0.00000918
Iteration 233/1000 | Loss: 0.00000918
Iteration 234/1000 | Loss: 0.00000918
Iteration 235/1000 | Loss: 0.00000918
Iteration 236/1000 | Loss: 0.00000918
Iteration 237/1000 | Loss: 0.00000918
Iteration 238/1000 | Loss: 0.00000917
Iteration 239/1000 | Loss: 0.00000917
Iteration 240/1000 | Loss: 0.00000917
Iteration 241/1000 | Loss: 0.00000917
Iteration 242/1000 | Loss: 0.00000917
Iteration 243/1000 | Loss: 0.00000917
Iteration 244/1000 | Loss: 0.00000917
Iteration 245/1000 | Loss: 0.00000917
Iteration 246/1000 | Loss: 0.00000917
Iteration 247/1000 | Loss: 0.00000917
Iteration 248/1000 | Loss: 0.00000917
Iteration 249/1000 | Loss: 0.00000916
Iteration 250/1000 | Loss: 0.00000916
Iteration 251/1000 | Loss: 0.00000916
Iteration 252/1000 | Loss: 0.00000916
Iteration 253/1000 | Loss: 0.00000916
Iteration 254/1000 | Loss: 0.00000916
Iteration 255/1000 | Loss: 0.00000916
Iteration 256/1000 | Loss: 0.00000916
Iteration 257/1000 | Loss: 0.00000915
Iteration 258/1000 | Loss: 0.00000915
Iteration 259/1000 | Loss: 0.00000915
Iteration 260/1000 | Loss: 0.00000915
Iteration 261/1000 | Loss: 0.00000915
Iteration 262/1000 | Loss: 0.00000915
Iteration 263/1000 | Loss: 0.00000915
Iteration 264/1000 | Loss: 0.00000915
Iteration 265/1000 | Loss: 0.00000915
Iteration 266/1000 | Loss: 0.00000915
Iteration 267/1000 | Loss: 0.00000915
Iteration 268/1000 | Loss: 0.00000915
Iteration 269/1000 | Loss: 0.00000915
Iteration 270/1000 | Loss: 0.00000915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [9.154916369880084e-06, 9.154916369880084e-06, 9.154916369880084e-06, 9.154916369880084e-06, 9.154916369880084e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.154916369880084e-06

Optimization complete. Final v2v error: 2.5804271697998047 mm

Highest mean error: 2.852754592895508 mm for frame 70

Lowest mean error: 2.3835840225219727 mm for frame 12

Saving results

Total time: 41.37483549118042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814997
Iteration 2/25 | Loss: 0.00139771
Iteration 3/25 | Loss: 0.00119782
Iteration 4/25 | Loss: 0.00118039
Iteration 5/25 | Loss: 0.00117562
Iteration 6/25 | Loss: 0.00117391
Iteration 7/25 | Loss: 0.00117341
Iteration 8/25 | Loss: 0.00117341
Iteration 9/25 | Loss: 0.00117341
Iteration 10/25 | Loss: 0.00117341
Iteration 11/25 | Loss: 0.00117341
Iteration 12/25 | Loss: 0.00117341
Iteration 13/25 | Loss: 0.00117341
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011734088184311986, 0.0011734088184311986, 0.0011734088184311986, 0.0011734088184311986, 0.0011734088184311986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011734088184311986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26598537
Iteration 2/25 | Loss: 0.00128165
Iteration 3/25 | Loss: 0.00128165
Iteration 4/25 | Loss: 0.00128165
Iteration 5/25 | Loss: 0.00128165
Iteration 6/25 | Loss: 0.00128165
Iteration 7/25 | Loss: 0.00128165
Iteration 8/25 | Loss: 0.00128165
Iteration 9/25 | Loss: 0.00128165
Iteration 10/25 | Loss: 0.00128165
Iteration 11/25 | Loss: 0.00128165
Iteration 12/25 | Loss: 0.00128165
Iteration 13/25 | Loss: 0.00128165
Iteration 14/25 | Loss: 0.00128165
Iteration 15/25 | Loss: 0.00128165
Iteration 16/25 | Loss: 0.00128165
Iteration 17/25 | Loss: 0.00128165
Iteration 18/25 | Loss: 0.00128165
Iteration 19/25 | Loss: 0.00128165
Iteration 20/25 | Loss: 0.00128165
Iteration 21/25 | Loss: 0.00128165
Iteration 22/25 | Loss: 0.00128165
Iteration 23/25 | Loss: 0.00128165
Iteration 24/25 | Loss: 0.00128165
Iteration 25/25 | Loss: 0.00128165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128165
Iteration 2/1000 | Loss: 0.00006563
Iteration 3/1000 | Loss: 0.00004283
Iteration 4/1000 | Loss: 0.00003861
Iteration 5/1000 | Loss: 0.00003702
Iteration 6/1000 | Loss: 0.00003605
Iteration 7/1000 | Loss: 0.00003505
Iteration 8/1000 | Loss: 0.00003412
Iteration 9/1000 | Loss: 0.00003344
Iteration 10/1000 | Loss: 0.00003303
Iteration 11/1000 | Loss: 0.00003260
Iteration 12/1000 | Loss: 0.00003238
Iteration 13/1000 | Loss: 0.00003217
Iteration 14/1000 | Loss: 0.00003208
Iteration 15/1000 | Loss: 0.00003196
Iteration 16/1000 | Loss: 0.00003194
Iteration 17/1000 | Loss: 0.00003190
Iteration 18/1000 | Loss: 0.00003190
Iteration 19/1000 | Loss: 0.00003187
Iteration 20/1000 | Loss: 0.00003178
Iteration 21/1000 | Loss: 0.00003175
Iteration 22/1000 | Loss: 0.00003175
Iteration 23/1000 | Loss: 0.00003175
Iteration 24/1000 | Loss: 0.00003175
Iteration 25/1000 | Loss: 0.00003175
Iteration 26/1000 | Loss: 0.00003175
Iteration 27/1000 | Loss: 0.00003174
Iteration 28/1000 | Loss: 0.00003174
Iteration 29/1000 | Loss: 0.00003173
Iteration 30/1000 | Loss: 0.00003173
Iteration 31/1000 | Loss: 0.00003173
Iteration 32/1000 | Loss: 0.00003173
Iteration 33/1000 | Loss: 0.00003173
Iteration 34/1000 | Loss: 0.00003172
Iteration 35/1000 | Loss: 0.00003172
Iteration 36/1000 | Loss: 0.00003172
Iteration 37/1000 | Loss: 0.00003172
Iteration 38/1000 | Loss: 0.00003172
Iteration 39/1000 | Loss: 0.00003172
Iteration 40/1000 | Loss: 0.00003172
Iteration 41/1000 | Loss: 0.00003172
Iteration 42/1000 | Loss: 0.00003171
Iteration 43/1000 | Loss: 0.00003171
Iteration 44/1000 | Loss: 0.00003169
Iteration 45/1000 | Loss: 0.00003168
Iteration 46/1000 | Loss: 0.00003168
Iteration 47/1000 | Loss: 0.00003167
Iteration 48/1000 | Loss: 0.00003166
Iteration 49/1000 | Loss: 0.00003162
Iteration 50/1000 | Loss: 0.00003162
Iteration 51/1000 | Loss: 0.00003162
Iteration 52/1000 | Loss: 0.00003162
Iteration 53/1000 | Loss: 0.00003162
Iteration 54/1000 | Loss: 0.00003161
Iteration 55/1000 | Loss: 0.00003161
Iteration 56/1000 | Loss: 0.00003161
Iteration 57/1000 | Loss: 0.00003161
Iteration 58/1000 | Loss: 0.00003161
Iteration 59/1000 | Loss: 0.00003161
Iteration 60/1000 | Loss: 0.00003161
Iteration 61/1000 | Loss: 0.00003161
Iteration 62/1000 | Loss: 0.00003161
Iteration 63/1000 | Loss: 0.00003160
Iteration 64/1000 | Loss: 0.00003160
Iteration 65/1000 | Loss: 0.00003160
Iteration 66/1000 | Loss: 0.00003160
Iteration 67/1000 | Loss: 0.00003160
Iteration 68/1000 | Loss: 0.00003160
Iteration 69/1000 | Loss: 0.00003160
Iteration 70/1000 | Loss: 0.00003159
Iteration 71/1000 | Loss: 0.00003159
Iteration 72/1000 | Loss: 0.00003159
Iteration 73/1000 | Loss: 0.00003159
Iteration 74/1000 | Loss: 0.00003159
Iteration 75/1000 | Loss: 0.00003159
Iteration 76/1000 | Loss: 0.00003159
Iteration 77/1000 | Loss: 0.00003159
Iteration 78/1000 | Loss: 0.00003158
Iteration 79/1000 | Loss: 0.00003158
Iteration 80/1000 | Loss: 0.00003158
Iteration 81/1000 | Loss: 0.00003158
Iteration 82/1000 | Loss: 0.00003158
Iteration 83/1000 | Loss: 0.00003158
Iteration 84/1000 | Loss: 0.00003158
Iteration 85/1000 | Loss: 0.00003158
Iteration 86/1000 | Loss: 0.00003158
Iteration 87/1000 | Loss: 0.00003158
Iteration 88/1000 | Loss: 0.00003158
Iteration 89/1000 | Loss: 0.00003157
Iteration 90/1000 | Loss: 0.00003157
Iteration 91/1000 | Loss: 0.00003157
Iteration 92/1000 | Loss: 0.00003157
Iteration 93/1000 | Loss: 0.00003157
Iteration 94/1000 | Loss: 0.00003157
Iteration 95/1000 | Loss: 0.00003157
Iteration 96/1000 | Loss: 0.00003157
Iteration 97/1000 | Loss: 0.00003157
Iteration 98/1000 | Loss: 0.00003156
Iteration 99/1000 | Loss: 0.00003156
Iteration 100/1000 | Loss: 0.00003156
Iteration 101/1000 | Loss: 0.00003156
Iteration 102/1000 | Loss: 0.00003156
Iteration 103/1000 | Loss: 0.00003156
Iteration 104/1000 | Loss: 0.00003156
Iteration 105/1000 | Loss: 0.00003156
Iteration 106/1000 | Loss: 0.00003156
Iteration 107/1000 | Loss: 0.00003155
Iteration 108/1000 | Loss: 0.00003155
Iteration 109/1000 | Loss: 0.00003155
Iteration 110/1000 | Loss: 0.00003155
Iteration 111/1000 | Loss: 0.00003155
Iteration 112/1000 | Loss: 0.00003155
Iteration 113/1000 | Loss: 0.00003155
Iteration 114/1000 | Loss: 0.00003155
Iteration 115/1000 | Loss: 0.00003155
Iteration 116/1000 | Loss: 0.00003155
Iteration 117/1000 | Loss: 0.00003155
Iteration 118/1000 | Loss: 0.00003155
Iteration 119/1000 | Loss: 0.00003155
Iteration 120/1000 | Loss: 0.00003155
Iteration 121/1000 | Loss: 0.00003155
Iteration 122/1000 | Loss: 0.00003155
Iteration 123/1000 | Loss: 0.00003155
Iteration 124/1000 | Loss: 0.00003155
Iteration 125/1000 | Loss: 0.00003155
Iteration 126/1000 | Loss: 0.00003155
Iteration 127/1000 | Loss: 0.00003155
Iteration 128/1000 | Loss: 0.00003155
Iteration 129/1000 | Loss: 0.00003155
Iteration 130/1000 | Loss: 0.00003155
Iteration 131/1000 | Loss: 0.00003155
Iteration 132/1000 | Loss: 0.00003155
Iteration 133/1000 | Loss: 0.00003155
Iteration 134/1000 | Loss: 0.00003155
Iteration 135/1000 | Loss: 0.00003155
Iteration 136/1000 | Loss: 0.00003155
Iteration 137/1000 | Loss: 0.00003155
Iteration 138/1000 | Loss: 0.00003155
Iteration 139/1000 | Loss: 0.00003155
Iteration 140/1000 | Loss: 0.00003155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [3.155257218168117e-05, 3.155257218168117e-05, 3.155257218168117e-05, 3.155257218168117e-05, 3.155257218168117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.155257218168117e-05

Optimization complete. Final v2v error: 4.45808744430542 mm

Highest mean error: 4.532848358154297 mm for frame 23

Lowest mean error: 4.34832763671875 mm for frame 66

Saving results

Total time: 38.84874153137207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475954
Iteration 2/25 | Loss: 0.00119417
Iteration 3/25 | Loss: 0.00111496
Iteration 4/25 | Loss: 0.00110329
Iteration 5/25 | Loss: 0.00110050
Iteration 6/25 | Loss: 0.00110010
Iteration 7/25 | Loss: 0.00110010
Iteration 8/25 | Loss: 0.00110010
Iteration 9/25 | Loss: 0.00110010
Iteration 10/25 | Loss: 0.00110010
Iteration 11/25 | Loss: 0.00110010
Iteration 12/25 | Loss: 0.00110010
Iteration 13/25 | Loss: 0.00110010
Iteration 14/25 | Loss: 0.00110010
Iteration 15/25 | Loss: 0.00110010
Iteration 16/25 | Loss: 0.00110010
Iteration 17/25 | Loss: 0.00110010
Iteration 18/25 | Loss: 0.00110010
Iteration 19/25 | Loss: 0.00110010
Iteration 20/25 | Loss: 0.00110010
Iteration 21/25 | Loss: 0.00110010
Iteration 22/25 | Loss: 0.00110010
Iteration 23/25 | Loss: 0.00110010
Iteration 24/25 | Loss: 0.00110010
Iteration 25/25 | Loss: 0.00110010

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41313982
Iteration 2/25 | Loss: 0.00079608
Iteration 3/25 | Loss: 0.00079606
Iteration 4/25 | Loss: 0.00079606
Iteration 5/25 | Loss: 0.00079606
Iteration 6/25 | Loss: 0.00079606
Iteration 7/25 | Loss: 0.00079606
Iteration 8/25 | Loss: 0.00079606
Iteration 9/25 | Loss: 0.00079606
Iteration 10/25 | Loss: 0.00079606
Iteration 11/25 | Loss: 0.00079606
Iteration 12/25 | Loss: 0.00079606
Iteration 13/25 | Loss: 0.00079606
Iteration 14/25 | Loss: 0.00079606
Iteration 15/25 | Loss: 0.00079606
Iteration 16/25 | Loss: 0.00079606
Iteration 17/25 | Loss: 0.00079606
Iteration 18/25 | Loss: 0.00079606
Iteration 19/25 | Loss: 0.00079606
Iteration 20/25 | Loss: 0.00079606
Iteration 21/25 | Loss: 0.00079606
Iteration 22/25 | Loss: 0.00079606
Iteration 23/25 | Loss: 0.00079606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000796060892753303, 0.000796060892753303, 0.000796060892753303, 0.000796060892753303, 0.000796060892753303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000796060892753303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079606
Iteration 2/1000 | Loss: 0.00002498
Iteration 3/1000 | Loss: 0.00001777
Iteration 4/1000 | Loss: 0.00001599
Iteration 5/1000 | Loss: 0.00001526
Iteration 6/1000 | Loss: 0.00001471
Iteration 7/1000 | Loss: 0.00001440
Iteration 8/1000 | Loss: 0.00001417
Iteration 9/1000 | Loss: 0.00001393
Iteration 10/1000 | Loss: 0.00001383
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001370
Iteration 13/1000 | Loss: 0.00001363
Iteration 14/1000 | Loss: 0.00001360
Iteration 15/1000 | Loss: 0.00001359
Iteration 16/1000 | Loss: 0.00001359
Iteration 17/1000 | Loss: 0.00001358
Iteration 18/1000 | Loss: 0.00001357
Iteration 19/1000 | Loss: 0.00001354
Iteration 20/1000 | Loss: 0.00001353
Iteration 21/1000 | Loss: 0.00001352
Iteration 22/1000 | Loss: 0.00001351
Iteration 23/1000 | Loss: 0.00001350
Iteration 24/1000 | Loss: 0.00001350
Iteration 25/1000 | Loss: 0.00001350
Iteration 26/1000 | Loss: 0.00001350
Iteration 27/1000 | Loss: 0.00001349
Iteration 28/1000 | Loss: 0.00001349
Iteration 29/1000 | Loss: 0.00001348
Iteration 30/1000 | Loss: 0.00001347
Iteration 31/1000 | Loss: 0.00001346
Iteration 32/1000 | Loss: 0.00001346
Iteration 33/1000 | Loss: 0.00001346
Iteration 34/1000 | Loss: 0.00001343
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001342
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001342
Iteration 40/1000 | Loss: 0.00001341
Iteration 41/1000 | Loss: 0.00001341
Iteration 42/1000 | Loss: 0.00001341
Iteration 43/1000 | Loss: 0.00001340
Iteration 44/1000 | Loss: 0.00001340
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001339
Iteration 47/1000 | Loss: 0.00001339
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001338
Iteration 50/1000 | Loss: 0.00001338
Iteration 51/1000 | Loss: 0.00001338
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001337
Iteration 54/1000 | Loss: 0.00001337
Iteration 55/1000 | Loss: 0.00001336
Iteration 56/1000 | Loss: 0.00001336
Iteration 57/1000 | Loss: 0.00001336
Iteration 58/1000 | Loss: 0.00001335
Iteration 59/1000 | Loss: 0.00001335
Iteration 60/1000 | Loss: 0.00001335
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001333
Iteration 72/1000 | Loss: 0.00001333
Iteration 73/1000 | Loss: 0.00001333
Iteration 74/1000 | Loss: 0.00001333
Iteration 75/1000 | Loss: 0.00001333
Iteration 76/1000 | Loss: 0.00001333
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001332
Iteration 81/1000 | Loss: 0.00001332
Iteration 82/1000 | Loss: 0.00001332
Iteration 83/1000 | Loss: 0.00001332
Iteration 84/1000 | Loss: 0.00001331
Iteration 85/1000 | Loss: 0.00001331
Iteration 86/1000 | Loss: 0.00001331
Iteration 87/1000 | Loss: 0.00001330
Iteration 88/1000 | Loss: 0.00001330
Iteration 89/1000 | Loss: 0.00001330
Iteration 90/1000 | Loss: 0.00001329
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001329
Iteration 93/1000 | Loss: 0.00001329
Iteration 94/1000 | Loss: 0.00001329
Iteration 95/1000 | Loss: 0.00001329
Iteration 96/1000 | Loss: 0.00001328
Iteration 97/1000 | Loss: 0.00001328
Iteration 98/1000 | Loss: 0.00001328
Iteration 99/1000 | Loss: 0.00001328
Iteration 100/1000 | Loss: 0.00001327
Iteration 101/1000 | Loss: 0.00001327
Iteration 102/1000 | Loss: 0.00001327
Iteration 103/1000 | Loss: 0.00001327
Iteration 104/1000 | Loss: 0.00001327
Iteration 105/1000 | Loss: 0.00001327
Iteration 106/1000 | Loss: 0.00001327
Iteration 107/1000 | Loss: 0.00001327
Iteration 108/1000 | Loss: 0.00001327
Iteration 109/1000 | Loss: 0.00001327
Iteration 110/1000 | Loss: 0.00001327
Iteration 111/1000 | Loss: 0.00001326
Iteration 112/1000 | Loss: 0.00001326
Iteration 113/1000 | Loss: 0.00001326
Iteration 114/1000 | Loss: 0.00001326
Iteration 115/1000 | Loss: 0.00001326
Iteration 116/1000 | Loss: 0.00001326
Iteration 117/1000 | Loss: 0.00001326
Iteration 118/1000 | Loss: 0.00001326
Iteration 119/1000 | Loss: 0.00001325
Iteration 120/1000 | Loss: 0.00001325
Iteration 121/1000 | Loss: 0.00001325
Iteration 122/1000 | Loss: 0.00001325
Iteration 123/1000 | Loss: 0.00001325
Iteration 124/1000 | Loss: 0.00001325
Iteration 125/1000 | Loss: 0.00001324
Iteration 126/1000 | Loss: 0.00001324
Iteration 127/1000 | Loss: 0.00001324
Iteration 128/1000 | Loss: 0.00001324
Iteration 129/1000 | Loss: 0.00001324
Iteration 130/1000 | Loss: 0.00001324
Iteration 131/1000 | Loss: 0.00001324
Iteration 132/1000 | Loss: 0.00001323
Iteration 133/1000 | Loss: 0.00001323
Iteration 134/1000 | Loss: 0.00001323
Iteration 135/1000 | Loss: 0.00001323
Iteration 136/1000 | Loss: 0.00001323
Iteration 137/1000 | Loss: 0.00001323
Iteration 138/1000 | Loss: 0.00001323
Iteration 139/1000 | Loss: 0.00001323
Iteration 140/1000 | Loss: 0.00001323
Iteration 141/1000 | Loss: 0.00001322
Iteration 142/1000 | Loss: 0.00001322
Iteration 143/1000 | Loss: 0.00001322
Iteration 144/1000 | Loss: 0.00001322
Iteration 145/1000 | Loss: 0.00001322
Iteration 146/1000 | Loss: 0.00001322
Iteration 147/1000 | Loss: 0.00001321
Iteration 148/1000 | Loss: 0.00001321
Iteration 149/1000 | Loss: 0.00001321
Iteration 150/1000 | Loss: 0.00001321
Iteration 151/1000 | Loss: 0.00001321
Iteration 152/1000 | Loss: 0.00001321
Iteration 153/1000 | Loss: 0.00001321
Iteration 154/1000 | Loss: 0.00001321
Iteration 155/1000 | Loss: 0.00001321
Iteration 156/1000 | Loss: 0.00001321
Iteration 157/1000 | Loss: 0.00001321
Iteration 158/1000 | Loss: 0.00001321
Iteration 159/1000 | Loss: 0.00001320
Iteration 160/1000 | Loss: 0.00001320
Iteration 161/1000 | Loss: 0.00001320
Iteration 162/1000 | Loss: 0.00001320
Iteration 163/1000 | Loss: 0.00001320
Iteration 164/1000 | Loss: 0.00001320
Iteration 165/1000 | Loss: 0.00001320
Iteration 166/1000 | Loss: 0.00001320
Iteration 167/1000 | Loss: 0.00001320
Iteration 168/1000 | Loss: 0.00001319
Iteration 169/1000 | Loss: 0.00001319
Iteration 170/1000 | Loss: 0.00001319
Iteration 171/1000 | Loss: 0.00001319
Iteration 172/1000 | Loss: 0.00001319
Iteration 173/1000 | Loss: 0.00001319
Iteration 174/1000 | Loss: 0.00001319
Iteration 175/1000 | Loss: 0.00001319
Iteration 176/1000 | Loss: 0.00001319
Iteration 177/1000 | Loss: 0.00001319
Iteration 178/1000 | Loss: 0.00001318
Iteration 179/1000 | Loss: 0.00001318
Iteration 180/1000 | Loss: 0.00001318
Iteration 181/1000 | Loss: 0.00001318
Iteration 182/1000 | Loss: 0.00001318
Iteration 183/1000 | Loss: 0.00001318
Iteration 184/1000 | Loss: 0.00001318
Iteration 185/1000 | Loss: 0.00001318
Iteration 186/1000 | Loss: 0.00001318
Iteration 187/1000 | Loss: 0.00001318
Iteration 188/1000 | Loss: 0.00001318
Iteration 189/1000 | Loss: 0.00001318
Iteration 190/1000 | Loss: 0.00001318
Iteration 191/1000 | Loss: 0.00001318
Iteration 192/1000 | Loss: 0.00001317
Iteration 193/1000 | Loss: 0.00001317
Iteration 194/1000 | Loss: 0.00001317
Iteration 195/1000 | Loss: 0.00001317
Iteration 196/1000 | Loss: 0.00001317
Iteration 197/1000 | Loss: 0.00001317
Iteration 198/1000 | Loss: 0.00001317
Iteration 199/1000 | Loss: 0.00001317
Iteration 200/1000 | Loss: 0.00001317
Iteration 201/1000 | Loss: 0.00001317
Iteration 202/1000 | Loss: 0.00001317
Iteration 203/1000 | Loss: 0.00001317
Iteration 204/1000 | Loss: 0.00001316
Iteration 205/1000 | Loss: 0.00001316
Iteration 206/1000 | Loss: 0.00001316
Iteration 207/1000 | Loss: 0.00001316
Iteration 208/1000 | Loss: 0.00001316
Iteration 209/1000 | Loss: 0.00001316
Iteration 210/1000 | Loss: 0.00001316
Iteration 211/1000 | Loss: 0.00001315
Iteration 212/1000 | Loss: 0.00001315
Iteration 213/1000 | Loss: 0.00001315
Iteration 214/1000 | Loss: 0.00001315
Iteration 215/1000 | Loss: 0.00001315
Iteration 216/1000 | Loss: 0.00001315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.3153678082744591e-05, 1.3153678082744591e-05, 1.3153678082744591e-05, 1.3153678082744591e-05, 1.3153678082744591e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3153678082744591e-05

Optimization complete. Final v2v error: 3.0422980785369873 mm

Highest mean error: 3.558946371078491 mm for frame 60

Lowest mean error: 2.5574138164520264 mm for frame 232

Saving results

Total time: 45.16381335258484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022329
Iteration 2/25 | Loss: 0.00174557
Iteration 3/25 | Loss: 0.00140446
Iteration 4/25 | Loss: 0.00131949
Iteration 5/25 | Loss: 0.00136677
Iteration 6/25 | Loss: 0.00128083
Iteration 7/25 | Loss: 0.00129216
Iteration 8/25 | Loss: 0.00131030
Iteration 9/25 | Loss: 0.00124743
Iteration 10/25 | Loss: 0.00122085
Iteration 11/25 | Loss: 0.00119754
Iteration 12/25 | Loss: 0.00121474
Iteration 13/25 | Loss: 0.00118078
Iteration 14/25 | Loss: 0.00116086
Iteration 15/25 | Loss: 0.00114550
Iteration 16/25 | Loss: 0.00113842
Iteration 17/25 | Loss: 0.00112482
Iteration 18/25 | Loss: 0.00111791
Iteration 19/25 | Loss: 0.00111515
Iteration 20/25 | Loss: 0.00110688
Iteration 21/25 | Loss: 0.00110221
Iteration 22/25 | Loss: 0.00110162
Iteration 23/25 | Loss: 0.00110216
Iteration 24/25 | Loss: 0.00110010
Iteration 25/25 | Loss: 0.00109582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43445182
Iteration 2/25 | Loss: 0.00081469
Iteration 3/25 | Loss: 0.00081468
Iteration 4/25 | Loss: 0.00081468
Iteration 5/25 | Loss: 0.00081468
Iteration 6/25 | Loss: 0.00081468
Iteration 7/25 | Loss: 0.00081468
Iteration 8/25 | Loss: 0.00081468
Iteration 9/25 | Loss: 0.00081468
Iteration 10/25 | Loss: 0.00081468
Iteration 11/25 | Loss: 0.00081468
Iteration 12/25 | Loss: 0.00081468
Iteration 13/25 | Loss: 0.00081468
Iteration 14/25 | Loss: 0.00081468
Iteration 15/25 | Loss: 0.00081468
Iteration 16/25 | Loss: 0.00081468
Iteration 17/25 | Loss: 0.00081468
Iteration 18/25 | Loss: 0.00081468
Iteration 19/25 | Loss: 0.00081468
Iteration 20/25 | Loss: 0.00081468
Iteration 21/25 | Loss: 0.00081468
Iteration 22/25 | Loss: 0.00081468
Iteration 23/25 | Loss: 0.00081468
Iteration 24/25 | Loss: 0.00081468
Iteration 25/25 | Loss: 0.00081468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081468
Iteration 2/1000 | Loss: 0.00002435
Iteration 3/1000 | Loss: 0.00001627
Iteration 4/1000 | Loss: 0.00001429
Iteration 5/1000 | Loss: 0.00001330
Iteration 6/1000 | Loss: 0.00007390
Iteration 7/1000 | Loss: 0.00001264
Iteration 8/1000 | Loss: 0.00001220
Iteration 9/1000 | Loss: 0.00001215
Iteration 10/1000 | Loss: 0.00001194
Iteration 11/1000 | Loss: 0.00001188
Iteration 12/1000 | Loss: 0.00001183
Iteration 13/1000 | Loss: 0.00001182
Iteration 14/1000 | Loss: 0.00001176
Iteration 15/1000 | Loss: 0.00001173
Iteration 16/1000 | Loss: 0.00001158
Iteration 17/1000 | Loss: 0.00337776
Iteration 18/1000 | Loss: 0.00013456
Iteration 19/1000 | Loss: 0.00002102
Iteration 20/1000 | Loss: 0.00001288
Iteration 21/1000 | Loss: 0.00001540
Iteration 22/1000 | Loss: 0.00001160
Iteration 23/1000 | Loss: 0.00001438
Iteration 24/1000 | Loss: 0.00001133
Iteration 25/1000 | Loss: 0.00001133
Iteration 26/1000 | Loss: 0.00001132
Iteration 27/1000 | Loss: 0.00001131
Iteration 28/1000 | Loss: 0.00001131
Iteration 29/1000 | Loss: 0.00001131
Iteration 30/1000 | Loss: 0.00001130
Iteration 31/1000 | Loss: 0.00001130
Iteration 32/1000 | Loss: 0.00001130
Iteration 33/1000 | Loss: 0.00001129
Iteration 34/1000 | Loss: 0.00001128
Iteration 35/1000 | Loss: 0.00001124
Iteration 36/1000 | Loss: 0.00001123
Iteration 37/1000 | Loss: 0.00001123
Iteration 38/1000 | Loss: 0.00001122
Iteration 39/1000 | Loss: 0.00001122
Iteration 40/1000 | Loss: 0.00001121
Iteration 41/1000 | Loss: 0.00001120
Iteration 42/1000 | Loss: 0.00001120
Iteration 43/1000 | Loss: 0.00001119
Iteration 44/1000 | Loss: 0.00001118
Iteration 45/1000 | Loss: 0.00001118
Iteration 46/1000 | Loss: 0.00001117
Iteration 47/1000 | Loss: 0.00001117
Iteration 48/1000 | Loss: 0.00001116
Iteration 49/1000 | Loss: 0.00001116
Iteration 50/1000 | Loss: 0.00001116
Iteration 51/1000 | Loss: 0.00001116
Iteration 52/1000 | Loss: 0.00001116
Iteration 53/1000 | Loss: 0.00001116
Iteration 54/1000 | Loss: 0.00001116
Iteration 55/1000 | Loss: 0.00001116
Iteration 56/1000 | Loss: 0.00001116
Iteration 57/1000 | Loss: 0.00001115
Iteration 58/1000 | Loss: 0.00001115
Iteration 59/1000 | Loss: 0.00001115
Iteration 60/1000 | Loss: 0.00001115
Iteration 61/1000 | Loss: 0.00001115
Iteration 62/1000 | Loss: 0.00001115
Iteration 63/1000 | Loss: 0.00001115
Iteration 64/1000 | Loss: 0.00001115
Iteration 65/1000 | Loss: 0.00001115
Iteration 66/1000 | Loss: 0.00001115
Iteration 67/1000 | Loss: 0.00001115
Iteration 68/1000 | Loss: 0.00001115
Iteration 69/1000 | Loss: 0.00001115
Iteration 70/1000 | Loss: 0.00001115
Iteration 71/1000 | Loss: 0.00001115
Iteration 72/1000 | Loss: 0.00001115
Iteration 73/1000 | Loss: 0.00001115
Iteration 74/1000 | Loss: 0.00001115
Iteration 75/1000 | Loss: 0.00001114
Iteration 76/1000 | Loss: 0.00001114
Iteration 77/1000 | Loss: 0.00001114
Iteration 78/1000 | Loss: 0.00001114
Iteration 79/1000 | Loss: 0.00001114
Iteration 80/1000 | Loss: 0.00001114
Iteration 81/1000 | Loss: 0.00001114
Iteration 82/1000 | Loss: 0.00001114
Iteration 83/1000 | Loss: 0.00001114
Iteration 84/1000 | Loss: 0.00001114
Iteration 85/1000 | Loss: 0.00001114
Iteration 86/1000 | Loss: 0.00001114
Iteration 87/1000 | Loss: 0.00001114
Iteration 88/1000 | Loss: 0.00001114
Iteration 89/1000 | Loss: 0.00001114
Iteration 90/1000 | Loss: 0.00001114
Iteration 91/1000 | Loss: 0.00001113
Iteration 92/1000 | Loss: 0.00001113
Iteration 93/1000 | Loss: 0.00001113
Iteration 94/1000 | Loss: 0.00001113
Iteration 95/1000 | Loss: 0.00001113
Iteration 96/1000 | Loss: 0.00001113
Iteration 97/1000 | Loss: 0.00001113
Iteration 98/1000 | Loss: 0.00001113
Iteration 99/1000 | Loss: 0.00001113
Iteration 100/1000 | Loss: 0.00001113
Iteration 101/1000 | Loss: 0.00001113
Iteration 102/1000 | Loss: 0.00001113
Iteration 103/1000 | Loss: 0.00001112
Iteration 104/1000 | Loss: 0.00001112
Iteration 105/1000 | Loss: 0.00001112
Iteration 106/1000 | Loss: 0.00001112
Iteration 107/1000 | Loss: 0.00001112
Iteration 108/1000 | Loss: 0.00001112
Iteration 109/1000 | Loss: 0.00001111
Iteration 110/1000 | Loss: 0.00001111
Iteration 111/1000 | Loss: 0.00001111
Iteration 112/1000 | Loss: 0.00001111
Iteration 113/1000 | Loss: 0.00001110
Iteration 114/1000 | Loss: 0.00001110
Iteration 115/1000 | Loss: 0.00001109
Iteration 116/1000 | Loss: 0.00001109
Iteration 117/1000 | Loss: 0.00001109
Iteration 118/1000 | Loss: 0.00001109
Iteration 119/1000 | Loss: 0.00001109
Iteration 120/1000 | Loss: 0.00001109
Iteration 121/1000 | Loss: 0.00010076
Iteration 122/1000 | Loss: 0.00001113
Iteration 123/1000 | Loss: 0.00001103
Iteration 124/1000 | Loss: 0.00001103
Iteration 125/1000 | Loss: 0.00001102
Iteration 126/1000 | Loss: 0.00001102
Iteration 127/1000 | Loss: 0.00001102
Iteration 128/1000 | Loss: 0.00001102
Iteration 129/1000 | Loss: 0.00001101
Iteration 130/1000 | Loss: 0.00001101
Iteration 131/1000 | Loss: 0.00001101
Iteration 132/1000 | Loss: 0.00001101
Iteration 133/1000 | Loss: 0.00001101
Iteration 134/1000 | Loss: 0.00001100
Iteration 135/1000 | Loss: 0.00001100
Iteration 136/1000 | Loss: 0.00001100
Iteration 137/1000 | Loss: 0.00001100
Iteration 138/1000 | Loss: 0.00001100
Iteration 139/1000 | Loss: 0.00001100
Iteration 140/1000 | Loss: 0.00001100
Iteration 141/1000 | Loss: 0.00001100
Iteration 142/1000 | Loss: 0.00001100
Iteration 143/1000 | Loss: 0.00001100
Iteration 144/1000 | Loss: 0.00001100
Iteration 145/1000 | Loss: 0.00001100
Iteration 146/1000 | Loss: 0.00001100
Iteration 147/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.0997996469086502e-05, 1.0997996469086502e-05, 1.0997996469086502e-05, 1.0997996469086502e-05, 1.0997996469086502e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0997996469086502e-05

Optimization complete. Final v2v error: 2.7683770656585693 mm

Highest mean error: 4.034030914306641 mm for frame 62

Lowest mean error: 2.3685693740844727 mm for frame 134

Saving results

Total time: 82.1154716014862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837566
Iteration 2/25 | Loss: 0.00126488
Iteration 3/25 | Loss: 0.00111177
Iteration 4/25 | Loss: 0.00110404
Iteration 5/25 | Loss: 0.00110192
Iteration 6/25 | Loss: 0.00110192
Iteration 7/25 | Loss: 0.00110192
Iteration 8/25 | Loss: 0.00110192
Iteration 9/25 | Loss: 0.00110192
Iteration 10/25 | Loss: 0.00110192
Iteration 11/25 | Loss: 0.00110192
Iteration 12/25 | Loss: 0.00110192
Iteration 13/25 | Loss: 0.00110192
Iteration 14/25 | Loss: 0.00110192
Iteration 15/25 | Loss: 0.00110192
Iteration 16/25 | Loss: 0.00110192
Iteration 17/25 | Loss: 0.00110192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011019195662811399, 0.0011019195662811399, 0.0011019195662811399, 0.0011019195662811399, 0.0011019195662811399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011019195662811399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47442257
Iteration 2/25 | Loss: 0.00071266
Iteration 3/25 | Loss: 0.00071266
Iteration 4/25 | Loss: 0.00071266
Iteration 5/25 | Loss: 0.00071266
Iteration 6/25 | Loss: 0.00071266
Iteration 7/25 | Loss: 0.00071266
Iteration 8/25 | Loss: 0.00071266
Iteration 9/25 | Loss: 0.00071266
Iteration 10/25 | Loss: 0.00071266
Iteration 11/25 | Loss: 0.00071266
Iteration 12/25 | Loss: 0.00071266
Iteration 13/25 | Loss: 0.00071266
Iteration 14/25 | Loss: 0.00071266
Iteration 15/25 | Loss: 0.00071266
Iteration 16/25 | Loss: 0.00071266
Iteration 17/25 | Loss: 0.00071266
Iteration 18/25 | Loss: 0.00071266
Iteration 19/25 | Loss: 0.00071266
Iteration 20/25 | Loss: 0.00071266
Iteration 21/25 | Loss: 0.00071266
Iteration 22/25 | Loss: 0.00071266
Iteration 23/25 | Loss: 0.00071266
Iteration 24/25 | Loss: 0.00071266
Iteration 25/25 | Loss: 0.00071266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071266
Iteration 2/1000 | Loss: 0.00003416
Iteration 3/1000 | Loss: 0.00001872
Iteration 4/1000 | Loss: 0.00001451
Iteration 5/1000 | Loss: 0.00001336
Iteration 6/1000 | Loss: 0.00001277
Iteration 7/1000 | Loss: 0.00001243
Iteration 8/1000 | Loss: 0.00001211
Iteration 9/1000 | Loss: 0.00001193
Iteration 10/1000 | Loss: 0.00001183
Iteration 11/1000 | Loss: 0.00001167
Iteration 12/1000 | Loss: 0.00001163
Iteration 13/1000 | Loss: 0.00001154
Iteration 14/1000 | Loss: 0.00001153
Iteration 15/1000 | Loss: 0.00001148
Iteration 16/1000 | Loss: 0.00001147
Iteration 17/1000 | Loss: 0.00001146
Iteration 18/1000 | Loss: 0.00001146
Iteration 19/1000 | Loss: 0.00001144
Iteration 20/1000 | Loss: 0.00001143
Iteration 21/1000 | Loss: 0.00001142
Iteration 22/1000 | Loss: 0.00001141
Iteration 23/1000 | Loss: 0.00001141
Iteration 24/1000 | Loss: 0.00001141
Iteration 25/1000 | Loss: 0.00001137
Iteration 26/1000 | Loss: 0.00001135
Iteration 27/1000 | Loss: 0.00001135
Iteration 28/1000 | Loss: 0.00001134
Iteration 29/1000 | Loss: 0.00001134
Iteration 30/1000 | Loss: 0.00001133
Iteration 31/1000 | Loss: 0.00001132
Iteration 32/1000 | Loss: 0.00001132
Iteration 33/1000 | Loss: 0.00001131
Iteration 34/1000 | Loss: 0.00001131
Iteration 35/1000 | Loss: 0.00001130
Iteration 36/1000 | Loss: 0.00001130
Iteration 37/1000 | Loss: 0.00001130
Iteration 38/1000 | Loss: 0.00001130
Iteration 39/1000 | Loss: 0.00001129
Iteration 40/1000 | Loss: 0.00001129
Iteration 41/1000 | Loss: 0.00001128
Iteration 42/1000 | Loss: 0.00001127
Iteration 43/1000 | Loss: 0.00001127
Iteration 44/1000 | Loss: 0.00001127
Iteration 45/1000 | Loss: 0.00001127
Iteration 46/1000 | Loss: 0.00001126
Iteration 47/1000 | Loss: 0.00001125
Iteration 48/1000 | Loss: 0.00001125
Iteration 49/1000 | Loss: 0.00001124
Iteration 50/1000 | Loss: 0.00001124
Iteration 51/1000 | Loss: 0.00001123
Iteration 52/1000 | Loss: 0.00001123
Iteration 53/1000 | Loss: 0.00001123
Iteration 54/1000 | Loss: 0.00001123
Iteration 55/1000 | Loss: 0.00001123
Iteration 56/1000 | Loss: 0.00001122
Iteration 57/1000 | Loss: 0.00001122
Iteration 58/1000 | Loss: 0.00001121
Iteration 59/1000 | Loss: 0.00001121
Iteration 60/1000 | Loss: 0.00001121
Iteration 61/1000 | Loss: 0.00001121
Iteration 62/1000 | Loss: 0.00001121
Iteration 63/1000 | Loss: 0.00001121
Iteration 64/1000 | Loss: 0.00001121
Iteration 65/1000 | Loss: 0.00001121
Iteration 66/1000 | Loss: 0.00001121
Iteration 67/1000 | Loss: 0.00001121
Iteration 68/1000 | Loss: 0.00001121
Iteration 69/1000 | Loss: 0.00001121
Iteration 70/1000 | Loss: 0.00001121
Iteration 71/1000 | Loss: 0.00001121
Iteration 72/1000 | Loss: 0.00001121
Iteration 73/1000 | Loss: 0.00001121
Iteration 74/1000 | Loss: 0.00001121
Iteration 75/1000 | Loss: 0.00001121
Iteration 76/1000 | Loss: 0.00001121
Iteration 77/1000 | Loss: 0.00001121
Iteration 78/1000 | Loss: 0.00001121
Iteration 79/1000 | Loss: 0.00001121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.1206395356566645e-05, 1.1206395356566645e-05, 1.1206395356566645e-05, 1.1206395356566645e-05, 1.1206395356566645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1206395356566645e-05

Optimization complete. Final v2v error: 2.8262500762939453 mm

Highest mean error: 3.6891300678253174 mm for frame 119

Lowest mean error: 2.53197979927063 mm for frame 31

Saving results

Total time: 32.378227949142456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973856
Iteration 2/25 | Loss: 0.00973856
Iteration 3/25 | Loss: 0.00267569
Iteration 4/25 | Loss: 0.00209666
Iteration 5/25 | Loss: 0.00201108
Iteration 6/25 | Loss: 0.00199494
Iteration 7/25 | Loss: 0.00167214
Iteration 8/25 | Loss: 0.00150484
Iteration 9/25 | Loss: 0.00139368
Iteration 10/25 | Loss: 0.00136791
Iteration 11/25 | Loss: 0.00134142
Iteration 12/25 | Loss: 0.00131035
Iteration 13/25 | Loss: 0.00130274
Iteration 14/25 | Loss: 0.00127180
Iteration 15/25 | Loss: 0.00125021
Iteration 16/25 | Loss: 0.00124431
Iteration 17/25 | Loss: 0.00124229
Iteration 18/25 | Loss: 0.00124224
Iteration 19/25 | Loss: 0.00123860
Iteration 20/25 | Loss: 0.00122825
Iteration 21/25 | Loss: 0.00122311
Iteration 22/25 | Loss: 0.00122461
Iteration 23/25 | Loss: 0.00123107
Iteration 24/25 | Loss: 0.00121721
Iteration 25/25 | Loss: 0.00121118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35753047
Iteration 2/25 | Loss: 0.00184057
Iteration 3/25 | Loss: 0.00183177
Iteration 4/25 | Loss: 0.00163417
Iteration 5/25 | Loss: 0.00163416
Iteration 6/25 | Loss: 0.00163416
Iteration 7/25 | Loss: 0.00163416
Iteration 8/25 | Loss: 0.00163416
Iteration 9/25 | Loss: 0.00163416
Iteration 10/25 | Loss: 0.00163416
Iteration 11/25 | Loss: 0.00163416
Iteration 12/25 | Loss: 0.00163416
Iteration 13/25 | Loss: 0.00163416
Iteration 14/25 | Loss: 0.00163416
Iteration 15/25 | Loss: 0.00163416
Iteration 16/25 | Loss: 0.00163416
Iteration 17/25 | Loss: 0.00163416
Iteration 18/25 | Loss: 0.00163416
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016341570299118757, 0.0016341570299118757, 0.0016341570299118757, 0.0016341570299118757, 0.0016341570299118757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016341570299118757

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163416
Iteration 2/1000 | Loss: 0.00142515
Iteration 3/1000 | Loss: 0.00038723
Iteration 4/1000 | Loss: 0.00012660
Iteration 5/1000 | Loss: 0.00008775
Iteration 6/1000 | Loss: 0.00006137
Iteration 7/1000 | Loss: 0.00014403
Iteration 8/1000 | Loss: 0.00004611
Iteration 9/1000 | Loss: 0.00039403
Iteration 10/1000 | Loss: 0.00005068
Iteration 11/1000 | Loss: 0.00013572
Iteration 12/1000 | Loss: 0.00004394
Iteration 13/1000 | Loss: 0.00003835
Iteration 14/1000 | Loss: 0.00003460
Iteration 15/1000 | Loss: 0.00003188
Iteration 16/1000 | Loss: 0.00016591
Iteration 17/1000 | Loss: 0.00007690
Iteration 18/1000 | Loss: 0.00014733
Iteration 19/1000 | Loss: 0.00018552
Iteration 20/1000 | Loss: 0.00003204
Iteration 21/1000 | Loss: 0.00002750
Iteration 22/1000 | Loss: 0.00002545
Iteration 23/1000 | Loss: 0.00002429
Iteration 24/1000 | Loss: 0.00002235
Iteration 25/1000 | Loss: 0.00003700
Iteration 26/1000 | Loss: 0.00002924
Iteration 27/1000 | Loss: 0.00002579
Iteration 28/1000 | Loss: 0.00002360
Iteration 29/1000 | Loss: 0.00002257
Iteration 30/1000 | Loss: 0.00002168
Iteration 31/1000 | Loss: 0.00002430
Iteration 32/1000 | Loss: 0.00002516
Iteration 33/1000 | Loss: 0.00002239
Iteration 34/1000 | Loss: 0.00003163
Iteration 35/1000 | Loss: 0.00002608
Iteration 36/1000 | Loss: 0.00002300
Iteration 37/1000 | Loss: 0.00001923
Iteration 38/1000 | Loss: 0.00001794
Iteration 39/1000 | Loss: 0.00001735
Iteration 40/1000 | Loss: 0.00002132
Iteration 41/1000 | Loss: 0.00003089
Iteration 42/1000 | Loss: 0.00002379
Iteration 43/1000 | Loss: 0.00001866
Iteration 44/1000 | Loss: 0.00001718
Iteration 45/1000 | Loss: 0.00002052
Iteration 46/1000 | Loss: 0.00002290
Iteration 47/1000 | Loss: 0.00002052
Iteration 48/1000 | Loss: 0.00002529
Iteration 49/1000 | Loss: 0.00002862
Iteration 50/1000 | Loss: 0.00002181
Iteration 51/1000 | Loss: 0.00001803
Iteration 52/1000 | Loss: 0.00001595
Iteration 53/1000 | Loss: 0.00001505
Iteration 54/1000 | Loss: 0.00001492
Iteration 55/1000 | Loss: 0.00001474
Iteration 56/1000 | Loss: 0.00001472
Iteration 57/1000 | Loss: 0.00001457
Iteration 58/1000 | Loss: 0.00001456
Iteration 59/1000 | Loss: 0.00001453
Iteration 60/1000 | Loss: 0.00001448
Iteration 61/1000 | Loss: 0.00001447
Iteration 62/1000 | Loss: 0.00001447
Iteration 63/1000 | Loss: 0.00001446
Iteration 64/1000 | Loss: 0.00001446
Iteration 65/1000 | Loss: 0.00001446
Iteration 66/1000 | Loss: 0.00001446
Iteration 67/1000 | Loss: 0.00001446
Iteration 68/1000 | Loss: 0.00001446
Iteration 69/1000 | Loss: 0.00001445
Iteration 70/1000 | Loss: 0.00001445
Iteration 71/1000 | Loss: 0.00001445
Iteration 72/1000 | Loss: 0.00001445
Iteration 73/1000 | Loss: 0.00001444
Iteration 74/1000 | Loss: 0.00001444
Iteration 75/1000 | Loss: 0.00001444
Iteration 76/1000 | Loss: 0.00001444
Iteration 77/1000 | Loss: 0.00001444
Iteration 78/1000 | Loss: 0.00001444
Iteration 79/1000 | Loss: 0.00001444
Iteration 80/1000 | Loss: 0.00001444
Iteration 81/1000 | Loss: 0.00001444
Iteration 82/1000 | Loss: 0.00001443
Iteration 83/1000 | Loss: 0.00001443
Iteration 84/1000 | Loss: 0.00001443
Iteration 85/1000 | Loss: 0.00001443
Iteration 86/1000 | Loss: 0.00001443
Iteration 87/1000 | Loss: 0.00001443
Iteration 88/1000 | Loss: 0.00001442
Iteration 89/1000 | Loss: 0.00001442
Iteration 90/1000 | Loss: 0.00001442
Iteration 91/1000 | Loss: 0.00001442
Iteration 92/1000 | Loss: 0.00001442
Iteration 93/1000 | Loss: 0.00001442
Iteration 94/1000 | Loss: 0.00001442
Iteration 95/1000 | Loss: 0.00001442
Iteration 96/1000 | Loss: 0.00001441
Iteration 97/1000 | Loss: 0.00001441
Iteration 98/1000 | Loss: 0.00001441
Iteration 99/1000 | Loss: 0.00001441
Iteration 100/1000 | Loss: 0.00001441
Iteration 101/1000 | Loss: 0.00001440
Iteration 102/1000 | Loss: 0.00001440
Iteration 103/1000 | Loss: 0.00001440
Iteration 104/1000 | Loss: 0.00001440
Iteration 105/1000 | Loss: 0.00001440
Iteration 106/1000 | Loss: 0.00001440
Iteration 107/1000 | Loss: 0.00001440
Iteration 108/1000 | Loss: 0.00001440
Iteration 109/1000 | Loss: 0.00001440
Iteration 110/1000 | Loss: 0.00001440
Iteration 111/1000 | Loss: 0.00001440
Iteration 112/1000 | Loss: 0.00001440
Iteration 113/1000 | Loss: 0.00001440
Iteration 114/1000 | Loss: 0.00001440
Iteration 115/1000 | Loss: 0.00001439
Iteration 116/1000 | Loss: 0.00001439
Iteration 117/1000 | Loss: 0.00001439
Iteration 118/1000 | Loss: 0.00001439
Iteration 119/1000 | Loss: 0.00001439
Iteration 120/1000 | Loss: 0.00001439
Iteration 121/1000 | Loss: 0.00001439
Iteration 122/1000 | Loss: 0.00001439
Iteration 123/1000 | Loss: 0.00001439
Iteration 124/1000 | Loss: 0.00001439
Iteration 125/1000 | Loss: 0.00001439
Iteration 126/1000 | Loss: 0.00001439
Iteration 127/1000 | Loss: 0.00001439
Iteration 128/1000 | Loss: 0.00001439
Iteration 129/1000 | Loss: 0.00001439
Iteration 130/1000 | Loss: 0.00001439
Iteration 131/1000 | Loss: 0.00001439
Iteration 132/1000 | Loss: 0.00001439
Iteration 133/1000 | Loss: 0.00001439
Iteration 134/1000 | Loss: 0.00001439
Iteration 135/1000 | Loss: 0.00001439
Iteration 136/1000 | Loss: 0.00001439
Iteration 137/1000 | Loss: 0.00001439
Iteration 138/1000 | Loss: 0.00001438
Iteration 139/1000 | Loss: 0.00001438
Iteration 140/1000 | Loss: 0.00001438
Iteration 141/1000 | Loss: 0.00001438
Iteration 142/1000 | Loss: 0.00001438
Iteration 143/1000 | Loss: 0.00001438
Iteration 144/1000 | Loss: 0.00001438
Iteration 145/1000 | Loss: 0.00001438
Iteration 146/1000 | Loss: 0.00001438
Iteration 147/1000 | Loss: 0.00001438
Iteration 148/1000 | Loss: 0.00001438
Iteration 149/1000 | Loss: 0.00001438
Iteration 150/1000 | Loss: 0.00001438
Iteration 151/1000 | Loss: 0.00001438
Iteration 152/1000 | Loss: 0.00001438
Iteration 153/1000 | Loss: 0.00001438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.4384047972271219e-05, 1.4384047972271219e-05, 1.4384047972271219e-05, 1.4384047972271219e-05, 1.4384047972271219e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4384047972271219e-05

Optimization complete. Final v2v error: 3.1856603622436523 mm

Highest mean error: 4.299020290374756 mm for frame 24

Lowest mean error: 2.7269272804260254 mm for frame 216

Saving results

Total time: 150.40287232398987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052265
Iteration 2/25 | Loss: 0.00204060
Iteration 3/25 | Loss: 0.00155449
Iteration 4/25 | Loss: 0.00158704
Iteration 5/25 | Loss: 0.00156267
Iteration 6/25 | Loss: 0.00148669
Iteration 7/25 | Loss: 0.00138250
Iteration 8/25 | Loss: 0.00133013
Iteration 9/25 | Loss: 0.00122769
Iteration 10/25 | Loss: 0.00125991
Iteration 11/25 | Loss: 0.00122896
Iteration 12/25 | Loss: 0.00116629
Iteration 13/25 | Loss: 0.00122954
Iteration 14/25 | Loss: 0.00110645
Iteration 15/25 | Loss: 0.00111017
Iteration 16/25 | Loss: 0.00109511
Iteration 17/25 | Loss: 0.00108653
Iteration 18/25 | Loss: 0.00108803
Iteration 19/25 | Loss: 0.00108974
Iteration 20/25 | Loss: 0.00108655
Iteration 21/25 | Loss: 0.00109310
Iteration 22/25 | Loss: 0.00108869
Iteration 23/25 | Loss: 0.00108812
Iteration 24/25 | Loss: 0.00109276
Iteration 25/25 | Loss: 0.00109030

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50848448
Iteration 2/25 | Loss: 0.00248600
Iteration 3/25 | Loss: 0.00108408
Iteration 4/25 | Loss: 0.00108408
Iteration 5/25 | Loss: 0.00108408
Iteration 6/25 | Loss: 0.00108408
Iteration 7/25 | Loss: 0.00108408
Iteration 8/25 | Loss: 0.00108408
Iteration 9/25 | Loss: 0.00108408
Iteration 10/25 | Loss: 0.00108408
Iteration 11/25 | Loss: 0.00108408
Iteration 12/25 | Loss: 0.00108408
Iteration 13/25 | Loss: 0.00108408
Iteration 14/25 | Loss: 0.00108408
Iteration 15/25 | Loss: 0.00108408
Iteration 16/25 | Loss: 0.00108408
Iteration 17/25 | Loss: 0.00108408
Iteration 18/25 | Loss: 0.00108408
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010840775212273002, 0.0010840775212273002, 0.0010840775212273002, 0.0010840775212273002, 0.0010840775212273002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010840775212273002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108408
Iteration 2/1000 | Loss: 0.00071982
Iteration 3/1000 | Loss: 0.00024834
Iteration 4/1000 | Loss: 0.00363321
Iteration 5/1000 | Loss: 0.00022879
Iteration 6/1000 | Loss: 0.00089039
Iteration 7/1000 | Loss: 0.00006692
Iteration 8/1000 | Loss: 0.00060689
Iteration 9/1000 | Loss: 0.00100159
Iteration 10/1000 | Loss: 0.00074401
Iteration 11/1000 | Loss: 0.00085344
Iteration 12/1000 | Loss: 0.00060361
Iteration 13/1000 | Loss: 0.00024543
Iteration 14/1000 | Loss: 0.00041542
Iteration 15/1000 | Loss: 0.00043352
Iteration 16/1000 | Loss: 0.00005912
Iteration 17/1000 | Loss: 0.00079580
Iteration 18/1000 | Loss: 0.00054419
Iteration 19/1000 | Loss: 0.00025972
Iteration 20/1000 | Loss: 0.00072744
Iteration 21/1000 | Loss: 0.00046913
Iteration 22/1000 | Loss: 0.00089438
Iteration 23/1000 | Loss: 0.00042765
Iteration 24/1000 | Loss: 0.00071282
Iteration 25/1000 | Loss: 0.00049804
Iteration 26/1000 | Loss: 0.00057249
Iteration 27/1000 | Loss: 0.00054336
Iteration 28/1000 | Loss: 0.00069591
Iteration 29/1000 | Loss: 0.00040709
Iteration 30/1000 | Loss: 0.00022313
Iteration 31/1000 | Loss: 0.00004493
Iteration 32/1000 | Loss: 0.00004169
Iteration 33/1000 | Loss: 0.00003963
Iteration 34/1000 | Loss: 0.00010571
Iteration 35/1000 | Loss: 0.00006740
Iteration 36/1000 | Loss: 0.00006996
Iteration 37/1000 | Loss: 0.00006376
Iteration 38/1000 | Loss: 0.00006013
Iteration 39/1000 | Loss: 0.00005336
Iteration 40/1000 | Loss: 0.00005302
Iteration 41/1000 | Loss: 0.00007694
Iteration 42/1000 | Loss: 0.00005710
Iteration 43/1000 | Loss: 0.00006394
Iteration 44/1000 | Loss: 0.00007445
Iteration 45/1000 | Loss: 0.00004177
Iteration 46/1000 | Loss: 0.00004694
Iteration 47/1000 | Loss: 0.00004655
Iteration 48/1000 | Loss: 0.00005691
Iteration 49/1000 | Loss: 0.00005834
Iteration 50/1000 | Loss: 0.00005495
Iteration 51/1000 | Loss: 0.00007338
Iteration 52/1000 | Loss: 0.00003535
Iteration 53/1000 | Loss: 0.00005245
Iteration 54/1000 | Loss: 0.00005496
Iteration 55/1000 | Loss: 0.00006502
Iteration 56/1000 | Loss: 0.00006117
Iteration 57/1000 | Loss: 0.00005785
Iteration 58/1000 | Loss: 0.00004386
Iteration 59/1000 | Loss: 0.00006790
Iteration 60/1000 | Loss: 0.00005121
Iteration 61/1000 | Loss: 0.00006261
Iteration 62/1000 | Loss: 0.00006917
Iteration 63/1000 | Loss: 0.00005997
Iteration 64/1000 | Loss: 0.00004483
Iteration 65/1000 | Loss: 0.00004955
Iteration 66/1000 | Loss: 0.00005779
Iteration 67/1000 | Loss: 0.00005822
Iteration 68/1000 | Loss: 0.00005715
Iteration 69/1000 | Loss: 0.00007034
Iteration 70/1000 | Loss: 0.00005766
Iteration 71/1000 | Loss: 0.00005677
Iteration 72/1000 | Loss: 0.00006126
Iteration 73/1000 | Loss: 0.00005954
Iteration 74/1000 | Loss: 0.00005910
Iteration 75/1000 | Loss: 0.00006375
Iteration 76/1000 | Loss: 0.00004301
Iteration 77/1000 | Loss: 0.00001668
Iteration 78/1000 | Loss: 0.00005770
Iteration 79/1000 | Loss: 0.00004571
Iteration 80/1000 | Loss: 0.00004391
Iteration 81/1000 | Loss: 0.00005189
Iteration 82/1000 | Loss: 0.00005992
Iteration 83/1000 | Loss: 0.00006035
Iteration 84/1000 | Loss: 0.00005449
Iteration 85/1000 | Loss: 0.00005653
Iteration 86/1000 | Loss: 0.00005541
Iteration 87/1000 | Loss: 0.00006043
Iteration 88/1000 | Loss: 0.00005394
Iteration 89/1000 | Loss: 0.00006008
Iteration 90/1000 | Loss: 0.00005364
Iteration 91/1000 | Loss: 0.00005573
Iteration 92/1000 | Loss: 0.00005529
Iteration 93/1000 | Loss: 0.00005800
Iteration 94/1000 | Loss: 0.00005748
Iteration 95/1000 | Loss: 0.00005888
Iteration 96/1000 | Loss: 0.00005758
Iteration 97/1000 | Loss: 0.00005681
Iteration 98/1000 | Loss: 0.00005470
Iteration 99/1000 | Loss: 0.00005669
Iteration 100/1000 | Loss: 0.00005412
Iteration 101/1000 | Loss: 0.00007435
Iteration 102/1000 | Loss: 0.00005779
Iteration 103/1000 | Loss: 0.00005660
Iteration 104/1000 | Loss: 0.00005482
Iteration 105/1000 | Loss: 0.00005759
Iteration 106/1000 | Loss: 0.00005979
Iteration 107/1000 | Loss: 0.00008407
Iteration 108/1000 | Loss: 0.00006217
Iteration 109/1000 | Loss: 0.00006083
Iteration 110/1000 | Loss: 0.00006224
Iteration 111/1000 | Loss: 0.00003346
Iteration 112/1000 | Loss: 0.00004305
Iteration 113/1000 | Loss: 0.00004946
Iteration 114/1000 | Loss: 0.00005831
Iteration 115/1000 | Loss: 0.00006003
Iteration 116/1000 | Loss: 0.00005513
Iteration 117/1000 | Loss: 0.00006095
Iteration 118/1000 | Loss: 0.00005322
Iteration 119/1000 | Loss: 0.00005777
Iteration 120/1000 | Loss: 0.00005137
Iteration 121/1000 | Loss: 0.00005681
Iteration 122/1000 | Loss: 0.00005127
Iteration 123/1000 | Loss: 0.00004198
Iteration 124/1000 | Loss: 0.00005140
Iteration 125/1000 | Loss: 0.00005518
Iteration 126/1000 | Loss: 0.00005219
Iteration 127/1000 | Loss: 0.00003894
Iteration 128/1000 | Loss: 0.00009005
Iteration 129/1000 | Loss: 0.00005737
Iteration 130/1000 | Loss: 0.00005418
Iteration 131/1000 | Loss: 0.00005865
Iteration 132/1000 | Loss: 0.00005843
Iteration 133/1000 | Loss: 0.00006103
Iteration 134/1000 | Loss: 0.00007048
Iteration 135/1000 | Loss: 0.00011769
Iteration 136/1000 | Loss: 0.00004010
Iteration 137/1000 | Loss: 0.00004921
Iteration 138/1000 | Loss: 0.00004153
Iteration 139/1000 | Loss: 0.00005595
Iteration 140/1000 | Loss: 0.00005569
Iteration 141/1000 | Loss: 0.00006199
Iteration 142/1000 | Loss: 0.00006471
Iteration 143/1000 | Loss: 0.00005656
Iteration 144/1000 | Loss: 0.00003809
Iteration 145/1000 | Loss: 0.00004604
Iteration 146/1000 | Loss: 0.00003428
Iteration 147/1000 | Loss: 0.00003619
Iteration 148/1000 | Loss: 0.00003428
Iteration 149/1000 | Loss: 0.00003984
Iteration 150/1000 | Loss: 0.00002283
Iteration 151/1000 | Loss: 0.00002596
Iteration 152/1000 | Loss: 0.00001839
Iteration 153/1000 | Loss: 0.00003500
Iteration 154/1000 | Loss: 0.00002542
Iteration 155/1000 | Loss: 0.00003181
Iteration 156/1000 | Loss: 0.00003324
Iteration 157/1000 | Loss: 0.00003132
Iteration 158/1000 | Loss: 0.00003728
Iteration 159/1000 | Loss: 0.00003765
Iteration 160/1000 | Loss: 0.00003236
Iteration 161/1000 | Loss: 0.00004407
Iteration 162/1000 | Loss: 0.00003609
Iteration 163/1000 | Loss: 0.00003196
Iteration 164/1000 | Loss: 0.00003803
Iteration 165/1000 | Loss: 0.00003094
Iteration 166/1000 | Loss: 0.00003855
Iteration 167/1000 | Loss: 0.00001611
Iteration 168/1000 | Loss: 0.00004092
Iteration 169/1000 | Loss: 0.00003373
Iteration 170/1000 | Loss: 0.00004583
Iteration 171/1000 | Loss: 0.00006604
Iteration 172/1000 | Loss: 0.00005082
Iteration 173/1000 | Loss: 0.00002322
Iteration 174/1000 | Loss: 0.00005146
Iteration 175/1000 | Loss: 0.00003341
Iteration 176/1000 | Loss: 0.00003470
Iteration 177/1000 | Loss: 0.00003529
Iteration 178/1000 | Loss: 0.00003525
Iteration 179/1000 | Loss: 0.00003038
Iteration 180/1000 | Loss: 0.00004733
Iteration 181/1000 | Loss: 0.00001712
Iteration 182/1000 | Loss: 0.00001325
Iteration 183/1000 | Loss: 0.00001247
Iteration 184/1000 | Loss: 0.00001178
Iteration 185/1000 | Loss: 0.00001133
Iteration 186/1000 | Loss: 0.00001100
Iteration 187/1000 | Loss: 0.00001064
Iteration 188/1000 | Loss: 0.00001045
Iteration 189/1000 | Loss: 0.00004870
Iteration 190/1000 | Loss: 0.00001043
Iteration 191/1000 | Loss: 0.00001041
Iteration 192/1000 | Loss: 0.00001034
Iteration 193/1000 | Loss: 0.00001034
Iteration 194/1000 | Loss: 0.00001034
Iteration 195/1000 | Loss: 0.00001033
Iteration 196/1000 | Loss: 0.00001033
Iteration 197/1000 | Loss: 0.00001033
Iteration 198/1000 | Loss: 0.00001032
Iteration 199/1000 | Loss: 0.00001032
Iteration 200/1000 | Loss: 0.00001032
Iteration 201/1000 | Loss: 0.00001031
Iteration 202/1000 | Loss: 0.00001031
Iteration 203/1000 | Loss: 0.00001031
Iteration 204/1000 | Loss: 0.00001031
Iteration 205/1000 | Loss: 0.00001031
Iteration 206/1000 | Loss: 0.00001031
Iteration 207/1000 | Loss: 0.00001031
Iteration 208/1000 | Loss: 0.00001031
Iteration 209/1000 | Loss: 0.00001031
Iteration 210/1000 | Loss: 0.00001030
Iteration 211/1000 | Loss: 0.00001030
Iteration 212/1000 | Loss: 0.00001030
Iteration 213/1000 | Loss: 0.00001030
Iteration 214/1000 | Loss: 0.00001030
Iteration 215/1000 | Loss: 0.00001030
Iteration 216/1000 | Loss: 0.00001030
Iteration 217/1000 | Loss: 0.00001030
Iteration 218/1000 | Loss: 0.00001030
Iteration 219/1000 | Loss: 0.00001030
Iteration 220/1000 | Loss: 0.00001030
Iteration 221/1000 | Loss: 0.00001030
Iteration 222/1000 | Loss: 0.00001030
Iteration 223/1000 | Loss: 0.00001030
Iteration 224/1000 | Loss: 0.00001030
Iteration 225/1000 | Loss: 0.00001030
Iteration 226/1000 | Loss: 0.00001030
Iteration 227/1000 | Loss: 0.00001030
Iteration 228/1000 | Loss: 0.00001030
Iteration 229/1000 | Loss: 0.00001030
Iteration 230/1000 | Loss: 0.00001030
Iteration 231/1000 | Loss: 0.00001030
Iteration 232/1000 | Loss: 0.00001030
Iteration 233/1000 | Loss: 0.00001030
Iteration 234/1000 | Loss: 0.00001030
Iteration 235/1000 | Loss: 0.00001030
Iteration 236/1000 | Loss: 0.00001030
Iteration 237/1000 | Loss: 0.00001030
Iteration 238/1000 | Loss: 0.00001030
Iteration 239/1000 | Loss: 0.00001030
Iteration 240/1000 | Loss: 0.00001030
Iteration 241/1000 | Loss: 0.00001030
Iteration 242/1000 | Loss: 0.00001030
Iteration 243/1000 | Loss: 0.00001030
Iteration 244/1000 | Loss: 0.00001030
Iteration 245/1000 | Loss: 0.00001030
Iteration 246/1000 | Loss: 0.00001030
Iteration 247/1000 | Loss: 0.00001030
Iteration 248/1000 | Loss: 0.00001030
Iteration 249/1000 | Loss: 0.00001030
Iteration 250/1000 | Loss: 0.00001030
Iteration 251/1000 | Loss: 0.00001030
Iteration 252/1000 | Loss: 0.00001030
Iteration 253/1000 | Loss: 0.00001030
Iteration 254/1000 | Loss: 0.00001030
Iteration 255/1000 | Loss: 0.00001030
Iteration 256/1000 | Loss: 0.00001030
Iteration 257/1000 | Loss: 0.00001030
Iteration 258/1000 | Loss: 0.00001030
Iteration 259/1000 | Loss: 0.00001030
Iteration 260/1000 | Loss: 0.00001030
Iteration 261/1000 | Loss: 0.00001030
Iteration 262/1000 | Loss: 0.00001030
Iteration 263/1000 | Loss: 0.00001030
Iteration 264/1000 | Loss: 0.00001030
Iteration 265/1000 | Loss: 0.00001030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [1.0304000170435756e-05, 1.0304000170435756e-05, 1.0304000170435756e-05, 1.0304000170435756e-05, 1.0304000170435756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0304000170435756e-05

Optimization complete. Final v2v error: 2.7088639736175537 mm

Highest mean error: 4.096022129058838 mm for frame 40

Lowest mean error: 2.3575823307037354 mm for frame 44

Saving results

Total time: 308.3435447216034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846431
Iteration 2/25 | Loss: 0.00131249
Iteration 3/25 | Loss: 0.00107712
Iteration 4/25 | Loss: 0.00105286
Iteration 5/25 | Loss: 0.00104661
Iteration 6/25 | Loss: 0.00104569
Iteration 7/25 | Loss: 0.00104569
Iteration 8/25 | Loss: 0.00104569
Iteration 9/25 | Loss: 0.00104569
Iteration 10/25 | Loss: 0.00104569
Iteration 11/25 | Loss: 0.00104569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010456888703629375, 0.0010456888703629375, 0.0010456888703629375, 0.0010456888703629375, 0.0010456888703629375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010456888703629375

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.70901442
Iteration 2/25 | Loss: 0.00071187
Iteration 3/25 | Loss: 0.00071187
Iteration 4/25 | Loss: 0.00071187
Iteration 5/25 | Loss: 0.00071187
Iteration 6/25 | Loss: 0.00071187
Iteration 7/25 | Loss: 0.00071187
Iteration 8/25 | Loss: 0.00071187
Iteration 9/25 | Loss: 0.00071187
Iteration 10/25 | Loss: 0.00071187
Iteration 11/25 | Loss: 0.00071187
Iteration 12/25 | Loss: 0.00071187
Iteration 13/25 | Loss: 0.00071187
Iteration 14/25 | Loss: 0.00071187
Iteration 15/25 | Loss: 0.00071187
Iteration 16/25 | Loss: 0.00071187
Iteration 17/25 | Loss: 0.00071187
Iteration 18/25 | Loss: 0.00071187
Iteration 19/25 | Loss: 0.00071187
Iteration 20/25 | Loss: 0.00071187
Iteration 21/25 | Loss: 0.00071187
Iteration 22/25 | Loss: 0.00071187
Iteration 23/25 | Loss: 0.00071187
Iteration 24/25 | Loss: 0.00071187
Iteration 25/25 | Loss: 0.00071187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071187
Iteration 2/1000 | Loss: 0.00002065
Iteration 3/1000 | Loss: 0.00001439
Iteration 4/1000 | Loss: 0.00001305
Iteration 5/1000 | Loss: 0.00001212
Iteration 6/1000 | Loss: 0.00001169
Iteration 7/1000 | Loss: 0.00001135
Iteration 8/1000 | Loss: 0.00001117
Iteration 9/1000 | Loss: 0.00001114
Iteration 10/1000 | Loss: 0.00001087
Iteration 11/1000 | Loss: 0.00001079
Iteration 12/1000 | Loss: 0.00001078
Iteration 13/1000 | Loss: 0.00001070
Iteration 14/1000 | Loss: 0.00001069
Iteration 15/1000 | Loss: 0.00001069
Iteration 16/1000 | Loss: 0.00001067
Iteration 17/1000 | Loss: 0.00001061
Iteration 18/1000 | Loss: 0.00001061
Iteration 19/1000 | Loss: 0.00001057
Iteration 20/1000 | Loss: 0.00001051
Iteration 21/1000 | Loss: 0.00001046
Iteration 22/1000 | Loss: 0.00001037
Iteration 23/1000 | Loss: 0.00001037
Iteration 24/1000 | Loss: 0.00001036
Iteration 25/1000 | Loss: 0.00001036
Iteration 26/1000 | Loss: 0.00001035
Iteration 27/1000 | Loss: 0.00001035
Iteration 28/1000 | Loss: 0.00001035
Iteration 29/1000 | Loss: 0.00001035
Iteration 30/1000 | Loss: 0.00001034
Iteration 31/1000 | Loss: 0.00001034
Iteration 32/1000 | Loss: 0.00001031
Iteration 33/1000 | Loss: 0.00001031
Iteration 34/1000 | Loss: 0.00001030
Iteration 35/1000 | Loss: 0.00001029
Iteration 36/1000 | Loss: 0.00001029
Iteration 37/1000 | Loss: 0.00001029
Iteration 38/1000 | Loss: 0.00001029
Iteration 39/1000 | Loss: 0.00001028
Iteration 40/1000 | Loss: 0.00001028
Iteration 41/1000 | Loss: 0.00001027
Iteration 42/1000 | Loss: 0.00001026
Iteration 43/1000 | Loss: 0.00001026
Iteration 44/1000 | Loss: 0.00001026
Iteration 45/1000 | Loss: 0.00001025
Iteration 46/1000 | Loss: 0.00001025
Iteration 47/1000 | Loss: 0.00001021
Iteration 48/1000 | Loss: 0.00001020
Iteration 49/1000 | Loss: 0.00001020
Iteration 50/1000 | Loss: 0.00001019
Iteration 51/1000 | Loss: 0.00001018
Iteration 52/1000 | Loss: 0.00001016
Iteration 53/1000 | Loss: 0.00001016
Iteration 54/1000 | Loss: 0.00001016
Iteration 55/1000 | Loss: 0.00001016
Iteration 56/1000 | Loss: 0.00001016
Iteration 57/1000 | Loss: 0.00001015
Iteration 58/1000 | Loss: 0.00001015
Iteration 59/1000 | Loss: 0.00001015
Iteration 60/1000 | Loss: 0.00001014
Iteration 61/1000 | Loss: 0.00001014
Iteration 62/1000 | Loss: 0.00001013
Iteration 63/1000 | Loss: 0.00001013
Iteration 64/1000 | Loss: 0.00001013
Iteration 65/1000 | Loss: 0.00001012
Iteration 66/1000 | Loss: 0.00001012
Iteration 67/1000 | Loss: 0.00001012
Iteration 68/1000 | Loss: 0.00001011
Iteration 69/1000 | Loss: 0.00001010
Iteration 70/1000 | Loss: 0.00001010
Iteration 71/1000 | Loss: 0.00001010
Iteration 72/1000 | Loss: 0.00001010
Iteration 73/1000 | Loss: 0.00001010
Iteration 74/1000 | Loss: 0.00001010
Iteration 75/1000 | Loss: 0.00001010
Iteration 76/1000 | Loss: 0.00001010
Iteration 77/1000 | Loss: 0.00001010
Iteration 78/1000 | Loss: 0.00001010
Iteration 79/1000 | Loss: 0.00001010
Iteration 80/1000 | Loss: 0.00001010
Iteration 81/1000 | Loss: 0.00001010
Iteration 82/1000 | Loss: 0.00001010
Iteration 83/1000 | Loss: 0.00001010
Iteration 84/1000 | Loss: 0.00001010
Iteration 85/1000 | Loss: 0.00001010
Iteration 86/1000 | Loss: 0.00001010
Iteration 87/1000 | Loss: 0.00001010
Iteration 88/1000 | Loss: 0.00001010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.0101756743097212e-05, 1.0101756743097212e-05, 1.0101756743097212e-05, 1.0101756743097212e-05, 1.0101756743097212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0101756743097212e-05

Optimization complete. Final v2v error: 2.7384963035583496 mm

Highest mean error: 3.0647964477539062 mm for frame 191

Lowest mean error: 2.4167330265045166 mm for frame 2

Saving results

Total time: 34.61062002182007
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018296
Iteration 2/25 | Loss: 0.00129603
Iteration 3/25 | Loss: 0.00110943
Iteration 4/25 | Loss: 0.00108419
Iteration 5/25 | Loss: 0.00108031
Iteration 6/25 | Loss: 0.00107975
Iteration 7/25 | Loss: 0.00107975
Iteration 8/25 | Loss: 0.00107975
Iteration 9/25 | Loss: 0.00107975
Iteration 10/25 | Loss: 0.00107975
Iteration 11/25 | Loss: 0.00107975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010797488503158092, 0.0010797488503158092, 0.0010797488503158092, 0.0010797488503158092, 0.0010797488503158092]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010797488503158092

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66988707
Iteration 2/25 | Loss: 0.00068473
Iteration 3/25 | Loss: 0.00068473
Iteration 4/25 | Loss: 0.00068473
Iteration 5/25 | Loss: 0.00068473
Iteration 6/25 | Loss: 0.00068473
Iteration 7/25 | Loss: 0.00068473
Iteration 8/25 | Loss: 0.00068473
Iteration 9/25 | Loss: 0.00068473
Iteration 10/25 | Loss: 0.00068473
Iteration 11/25 | Loss: 0.00068473
Iteration 12/25 | Loss: 0.00068473
Iteration 13/25 | Loss: 0.00068473
Iteration 14/25 | Loss: 0.00068473
Iteration 15/25 | Loss: 0.00068473
Iteration 16/25 | Loss: 0.00068473
Iteration 17/25 | Loss: 0.00068473
Iteration 18/25 | Loss: 0.00068473
Iteration 19/25 | Loss: 0.00068473
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006847305921837687, 0.0006847305921837687, 0.0006847305921837687, 0.0006847305921837687, 0.0006847305921837687]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006847305921837687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068473
Iteration 2/1000 | Loss: 0.00002115
Iteration 3/1000 | Loss: 0.00001527
Iteration 4/1000 | Loss: 0.00001413
Iteration 5/1000 | Loss: 0.00001312
Iteration 6/1000 | Loss: 0.00001255
Iteration 7/1000 | Loss: 0.00001214
Iteration 8/1000 | Loss: 0.00001183
Iteration 9/1000 | Loss: 0.00001150
Iteration 10/1000 | Loss: 0.00001141
Iteration 11/1000 | Loss: 0.00001122
Iteration 12/1000 | Loss: 0.00001121
Iteration 13/1000 | Loss: 0.00001119
Iteration 14/1000 | Loss: 0.00001113
Iteration 15/1000 | Loss: 0.00001111
Iteration 16/1000 | Loss: 0.00001110
Iteration 17/1000 | Loss: 0.00001109
Iteration 18/1000 | Loss: 0.00001109
Iteration 19/1000 | Loss: 0.00001108
Iteration 20/1000 | Loss: 0.00001108
Iteration 21/1000 | Loss: 0.00001108
Iteration 22/1000 | Loss: 0.00001107
Iteration 23/1000 | Loss: 0.00001107
Iteration 24/1000 | Loss: 0.00001106
Iteration 25/1000 | Loss: 0.00001106
Iteration 26/1000 | Loss: 0.00001105
Iteration 27/1000 | Loss: 0.00001105
Iteration 28/1000 | Loss: 0.00001102
Iteration 29/1000 | Loss: 0.00001102
Iteration 30/1000 | Loss: 0.00001102
Iteration 31/1000 | Loss: 0.00001102
Iteration 32/1000 | Loss: 0.00001102
Iteration 33/1000 | Loss: 0.00001102
Iteration 34/1000 | Loss: 0.00001101
Iteration 35/1000 | Loss: 0.00001101
Iteration 36/1000 | Loss: 0.00001101
Iteration 37/1000 | Loss: 0.00001101
Iteration 38/1000 | Loss: 0.00001101
Iteration 39/1000 | Loss: 0.00001101
Iteration 40/1000 | Loss: 0.00001101
Iteration 41/1000 | Loss: 0.00001101
Iteration 42/1000 | Loss: 0.00001101
Iteration 43/1000 | Loss: 0.00001100
Iteration 44/1000 | Loss: 0.00001100
Iteration 45/1000 | Loss: 0.00001099
Iteration 46/1000 | Loss: 0.00001099
Iteration 47/1000 | Loss: 0.00001099
Iteration 48/1000 | Loss: 0.00001099
Iteration 49/1000 | Loss: 0.00001099
Iteration 50/1000 | Loss: 0.00001098
Iteration 51/1000 | Loss: 0.00001098
Iteration 52/1000 | Loss: 0.00001098
Iteration 53/1000 | Loss: 0.00001098
Iteration 54/1000 | Loss: 0.00001097
Iteration 55/1000 | Loss: 0.00001097
Iteration 56/1000 | Loss: 0.00001097
Iteration 57/1000 | Loss: 0.00001097
Iteration 58/1000 | Loss: 0.00001096
Iteration 59/1000 | Loss: 0.00001096
Iteration 60/1000 | Loss: 0.00001095
Iteration 61/1000 | Loss: 0.00001095
Iteration 62/1000 | Loss: 0.00001095
Iteration 63/1000 | Loss: 0.00001094
Iteration 64/1000 | Loss: 0.00001094
Iteration 65/1000 | Loss: 0.00001094
Iteration 66/1000 | Loss: 0.00001092
Iteration 67/1000 | Loss: 0.00001092
Iteration 68/1000 | Loss: 0.00001091
Iteration 69/1000 | Loss: 0.00001091
Iteration 70/1000 | Loss: 0.00001091
Iteration 71/1000 | Loss: 0.00001091
Iteration 72/1000 | Loss: 0.00001091
Iteration 73/1000 | Loss: 0.00001091
Iteration 74/1000 | Loss: 0.00001091
Iteration 75/1000 | Loss: 0.00001091
Iteration 76/1000 | Loss: 0.00001090
Iteration 77/1000 | Loss: 0.00001090
Iteration 78/1000 | Loss: 0.00001090
Iteration 79/1000 | Loss: 0.00001090
Iteration 80/1000 | Loss: 0.00001089
Iteration 81/1000 | Loss: 0.00001089
Iteration 82/1000 | Loss: 0.00001089
Iteration 83/1000 | Loss: 0.00001089
Iteration 84/1000 | Loss: 0.00001088
Iteration 85/1000 | Loss: 0.00001088
Iteration 86/1000 | Loss: 0.00001088
Iteration 87/1000 | Loss: 0.00001087
Iteration 88/1000 | Loss: 0.00001087
Iteration 89/1000 | Loss: 0.00001087
Iteration 90/1000 | Loss: 0.00001087
Iteration 91/1000 | Loss: 0.00001087
Iteration 92/1000 | Loss: 0.00001087
Iteration 93/1000 | Loss: 0.00001087
Iteration 94/1000 | Loss: 0.00001087
Iteration 95/1000 | Loss: 0.00001087
Iteration 96/1000 | Loss: 0.00001087
Iteration 97/1000 | Loss: 0.00001087
Iteration 98/1000 | Loss: 0.00001087
Iteration 99/1000 | Loss: 0.00001087
Iteration 100/1000 | Loss: 0.00001087
Iteration 101/1000 | Loss: 0.00001087
Iteration 102/1000 | Loss: 0.00001087
Iteration 103/1000 | Loss: 0.00001087
Iteration 104/1000 | Loss: 0.00001087
Iteration 105/1000 | Loss: 0.00001087
Iteration 106/1000 | Loss: 0.00001087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.0868823665077798e-05, 1.0868823665077798e-05, 1.0868823665077798e-05, 1.0868823665077798e-05, 1.0868823665077798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0868823665077798e-05

Optimization complete. Final v2v error: 2.813446283340454 mm

Highest mean error: 3.210374355316162 mm for frame 95

Lowest mean error: 2.6166412830352783 mm for frame 246

Saving results

Total time: 36.066304445266724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009185
Iteration 2/25 | Loss: 0.00412871
Iteration 3/25 | Loss: 0.00244721
Iteration 4/25 | Loss: 0.00206595
Iteration 5/25 | Loss: 0.00201572
Iteration 6/25 | Loss: 0.00160947
Iteration 7/25 | Loss: 0.00143005
Iteration 8/25 | Loss: 0.00127456
Iteration 9/25 | Loss: 0.00120884
Iteration 10/25 | Loss: 0.00120047
Iteration 11/25 | Loss: 0.00114670
Iteration 12/25 | Loss: 0.00113746
Iteration 13/25 | Loss: 0.00112612
Iteration 14/25 | Loss: 0.00111929
Iteration 15/25 | Loss: 0.00111668
Iteration 16/25 | Loss: 0.00111691
Iteration 17/25 | Loss: 0.00111392
Iteration 18/25 | Loss: 0.00111164
Iteration 19/25 | Loss: 0.00111940
Iteration 20/25 | Loss: 0.00111021
Iteration 21/25 | Loss: 0.00110934
Iteration 22/25 | Loss: 0.00110721
Iteration 23/25 | Loss: 0.00111010
Iteration 24/25 | Loss: 0.00110452
Iteration 25/25 | Loss: 0.00110337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36041403
Iteration 2/25 | Loss: 0.00105720
Iteration 3/25 | Loss: 0.00105720
Iteration 4/25 | Loss: 0.00104003
Iteration 5/25 | Loss: 0.00104003
Iteration 6/25 | Loss: 0.00104003
Iteration 7/25 | Loss: 0.00104003
Iteration 8/25 | Loss: 0.00104003
Iteration 9/25 | Loss: 0.00104003
Iteration 10/25 | Loss: 0.00104003
Iteration 11/25 | Loss: 0.00104003
Iteration 12/25 | Loss: 0.00104003
Iteration 13/25 | Loss: 0.00104003
Iteration 14/25 | Loss: 0.00104003
Iteration 15/25 | Loss: 0.00104003
Iteration 16/25 | Loss: 0.00104003
Iteration 17/25 | Loss: 0.00104003
Iteration 18/25 | Loss: 0.00104003
Iteration 19/25 | Loss: 0.00104003
Iteration 20/25 | Loss: 0.00104003
Iteration 21/25 | Loss: 0.00104003
Iteration 22/25 | Loss: 0.00104003
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00104002864100039, 0.00104002864100039, 0.00104002864100039, 0.00104002864100039, 0.00104002864100039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00104002864100039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104003
Iteration 2/1000 | Loss: 0.00009835
Iteration 3/1000 | Loss: 0.00006090
Iteration 4/1000 | Loss: 0.00005330
Iteration 5/1000 | Loss: 0.00004851
Iteration 6/1000 | Loss: 0.00010772
Iteration 7/1000 | Loss: 0.00004580
Iteration 8/1000 | Loss: 0.00004295
Iteration 9/1000 | Loss: 0.00005203
Iteration 10/1000 | Loss: 0.00004202
Iteration 11/1000 | Loss: 0.00004184
Iteration 12/1000 | Loss: 0.00007550
Iteration 13/1000 | Loss: 0.00011945
Iteration 14/1000 | Loss: 0.00008781
Iteration 15/1000 | Loss: 0.00003958
Iteration 16/1000 | Loss: 0.00003909
Iteration 17/1000 | Loss: 0.00005319
Iteration 18/1000 | Loss: 0.00143939
Iteration 19/1000 | Loss: 0.00136624
Iteration 20/1000 | Loss: 0.00011281
Iteration 21/1000 | Loss: 0.00007242
Iteration 22/1000 | Loss: 0.00004692
Iteration 23/1000 | Loss: 0.00039160
Iteration 24/1000 | Loss: 0.00046476
Iteration 25/1000 | Loss: 0.00059422
Iteration 26/1000 | Loss: 0.00005635
Iteration 27/1000 | Loss: 0.00134769
Iteration 28/1000 | Loss: 0.00067213
Iteration 29/1000 | Loss: 0.00084364
Iteration 30/1000 | Loss: 0.00005953
Iteration 31/1000 | Loss: 0.00005619
Iteration 32/1000 | Loss: 0.00004722
Iteration 33/1000 | Loss: 0.00054648
Iteration 34/1000 | Loss: 0.00011387
Iteration 35/1000 | Loss: 0.00005177
Iteration 36/1000 | Loss: 0.00028712
Iteration 37/1000 | Loss: 0.00008117
Iteration 38/1000 | Loss: 0.00009469
Iteration 39/1000 | Loss: 0.00023379
Iteration 40/1000 | Loss: 0.00009571
Iteration 41/1000 | Loss: 0.00004677
Iteration 42/1000 | Loss: 0.00020031
Iteration 43/1000 | Loss: 0.00003868
Iteration 44/1000 | Loss: 0.00003862
Iteration 45/1000 | Loss: 0.00005432
Iteration 46/1000 | Loss: 0.00003567
Iteration 47/1000 | Loss: 0.00004135
Iteration 48/1000 | Loss: 0.00005318
Iteration 49/1000 | Loss: 0.00003327
Iteration 50/1000 | Loss: 0.00003380
Iteration 51/1000 | Loss: 0.00005364
Iteration 52/1000 | Loss: 0.00015215
Iteration 53/1000 | Loss: 0.00003322
Iteration 54/1000 | Loss: 0.00003349
Iteration 55/1000 | Loss: 0.00003316
Iteration 56/1000 | Loss: 0.00003313
Iteration 57/1000 | Loss: 0.00003155
Iteration 58/1000 | Loss: 0.00003431
Iteration 59/1000 | Loss: 0.00003431
Iteration 60/1000 | Loss: 0.00004003
Iteration 61/1000 | Loss: 0.00005012
Iteration 62/1000 | Loss: 0.00003347
Iteration 63/1000 | Loss: 0.00003383
Iteration 64/1000 | Loss: 0.00005002
Iteration 65/1000 | Loss: 0.00003361
Iteration 66/1000 | Loss: 0.00003180
Iteration 67/1000 | Loss: 0.00003115
Iteration 68/1000 | Loss: 0.00003115
Iteration 69/1000 | Loss: 0.00003321
Iteration 70/1000 | Loss: 0.00003847
Iteration 71/1000 | Loss: 0.00007295
Iteration 72/1000 | Loss: 0.00003203
Iteration 73/1000 | Loss: 0.00003317
Iteration 74/1000 | Loss: 0.00003108
Iteration 75/1000 | Loss: 0.00003107
Iteration 76/1000 | Loss: 0.00003104
Iteration 77/1000 | Loss: 0.00003103
Iteration 78/1000 | Loss: 0.00003103
Iteration 79/1000 | Loss: 0.00003103
Iteration 80/1000 | Loss: 0.00003103
Iteration 81/1000 | Loss: 0.00003103
Iteration 82/1000 | Loss: 0.00003103
Iteration 83/1000 | Loss: 0.00003103
Iteration 84/1000 | Loss: 0.00003103
Iteration 85/1000 | Loss: 0.00003103
Iteration 86/1000 | Loss: 0.00003103
Iteration 87/1000 | Loss: 0.00003103
Iteration 88/1000 | Loss: 0.00003103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [3.103119524894282e-05, 3.103119524894282e-05, 3.103119524894282e-05, 3.103119524894282e-05, 3.103119524894282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.103119524894282e-05

Optimization complete. Final v2v error: 3.117823362350464 mm

Highest mean error: 9.944653511047363 mm for frame 39

Lowest mean error: 2.1907613277435303 mm for frame 50

Saving results

Total time: 140.34792041778564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891518
Iteration 2/25 | Loss: 0.00146589
Iteration 3/25 | Loss: 0.00120201
Iteration 4/25 | Loss: 0.00116167
Iteration 5/25 | Loss: 0.00115058
Iteration 6/25 | Loss: 0.00114805
Iteration 7/25 | Loss: 0.00114759
Iteration 8/25 | Loss: 0.00114759
Iteration 9/25 | Loss: 0.00114759
Iteration 10/25 | Loss: 0.00114759
Iteration 11/25 | Loss: 0.00114759
Iteration 12/25 | Loss: 0.00114759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011475860374048352, 0.0011475860374048352, 0.0011475860374048352, 0.0011475860374048352, 0.0011475860374048352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011475860374048352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35964131
Iteration 2/25 | Loss: 0.00091317
Iteration 3/25 | Loss: 0.00091317
Iteration 4/25 | Loss: 0.00091317
Iteration 5/25 | Loss: 0.00091317
Iteration 6/25 | Loss: 0.00091317
Iteration 7/25 | Loss: 0.00091317
Iteration 8/25 | Loss: 0.00091317
Iteration 9/25 | Loss: 0.00091317
Iteration 10/25 | Loss: 0.00091317
Iteration 11/25 | Loss: 0.00091317
Iteration 12/25 | Loss: 0.00091317
Iteration 13/25 | Loss: 0.00091317
Iteration 14/25 | Loss: 0.00091317
Iteration 15/25 | Loss: 0.00091317
Iteration 16/25 | Loss: 0.00091317
Iteration 17/25 | Loss: 0.00091317
Iteration 18/25 | Loss: 0.00091317
Iteration 19/25 | Loss: 0.00091317
Iteration 20/25 | Loss: 0.00091317
Iteration 21/25 | Loss: 0.00091317
Iteration 22/25 | Loss: 0.00091317
Iteration 23/25 | Loss: 0.00091317
Iteration 24/25 | Loss: 0.00091317
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009131678380072117, 0.0009131678380072117, 0.0009131678380072117, 0.0009131678380072117, 0.0009131678380072117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009131678380072117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091317
Iteration 2/1000 | Loss: 0.00007765
Iteration 3/1000 | Loss: 0.00004942
Iteration 4/1000 | Loss: 0.00003510
Iteration 5/1000 | Loss: 0.00003230
Iteration 6/1000 | Loss: 0.00003045
Iteration 7/1000 | Loss: 0.00002928
Iteration 8/1000 | Loss: 0.00002827
Iteration 9/1000 | Loss: 0.00002762
Iteration 10/1000 | Loss: 0.00002715
Iteration 11/1000 | Loss: 0.00002679
Iteration 12/1000 | Loss: 0.00002650
Iteration 13/1000 | Loss: 0.00002629
Iteration 14/1000 | Loss: 0.00002608
Iteration 15/1000 | Loss: 0.00002604
Iteration 16/1000 | Loss: 0.00002594
Iteration 17/1000 | Loss: 0.00002582
Iteration 18/1000 | Loss: 0.00002580
Iteration 19/1000 | Loss: 0.00002578
Iteration 20/1000 | Loss: 0.00002573
Iteration 21/1000 | Loss: 0.00002569
Iteration 22/1000 | Loss: 0.00002569
Iteration 23/1000 | Loss: 0.00002568
Iteration 24/1000 | Loss: 0.00002567
Iteration 25/1000 | Loss: 0.00002566
Iteration 26/1000 | Loss: 0.00002566
Iteration 27/1000 | Loss: 0.00002565
Iteration 28/1000 | Loss: 0.00002564
Iteration 29/1000 | Loss: 0.00002562
Iteration 30/1000 | Loss: 0.00002562
Iteration 31/1000 | Loss: 0.00002559
Iteration 32/1000 | Loss: 0.00002558
Iteration 33/1000 | Loss: 0.00002557
Iteration 34/1000 | Loss: 0.00002556
Iteration 35/1000 | Loss: 0.00002556
Iteration 36/1000 | Loss: 0.00002555
Iteration 37/1000 | Loss: 0.00002555
Iteration 38/1000 | Loss: 0.00002555
Iteration 39/1000 | Loss: 0.00002555
Iteration 40/1000 | Loss: 0.00002555
Iteration 41/1000 | Loss: 0.00002555
Iteration 42/1000 | Loss: 0.00002555
Iteration 43/1000 | Loss: 0.00002555
Iteration 44/1000 | Loss: 0.00002555
Iteration 45/1000 | Loss: 0.00002554
Iteration 46/1000 | Loss: 0.00002553
Iteration 47/1000 | Loss: 0.00002552
Iteration 48/1000 | Loss: 0.00002552
Iteration 49/1000 | Loss: 0.00002551
Iteration 50/1000 | Loss: 0.00002551
Iteration 51/1000 | Loss: 0.00002551
Iteration 52/1000 | Loss: 0.00002551
Iteration 53/1000 | Loss: 0.00002550
Iteration 54/1000 | Loss: 0.00002550
Iteration 55/1000 | Loss: 0.00002550
Iteration 56/1000 | Loss: 0.00002549
Iteration 57/1000 | Loss: 0.00002549
Iteration 58/1000 | Loss: 0.00002549
Iteration 59/1000 | Loss: 0.00002548
Iteration 60/1000 | Loss: 0.00002548
Iteration 61/1000 | Loss: 0.00002548
Iteration 62/1000 | Loss: 0.00002547
Iteration 63/1000 | Loss: 0.00002547
Iteration 64/1000 | Loss: 0.00002547
Iteration 65/1000 | Loss: 0.00002546
Iteration 66/1000 | Loss: 0.00002546
Iteration 67/1000 | Loss: 0.00002546
Iteration 68/1000 | Loss: 0.00002545
Iteration 69/1000 | Loss: 0.00002545
Iteration 70/1000 | Loss: 0.00002545
Iteration 71/1000 | Loss: 0.00002545
Iteration 72/1000 | Loss: 0.00002545
Iteration 73/1000 | Loss: 0.00002545
Iteration 74/1000 | Loss: 0.00002545
Iteration 75/1000 | Loss: 0.00002545
Iteration 76/1000 | Loss: 0.00002545
Iteration 77/1000 | Loss: 0.00002544
Iteration 78/1000 | Loss: 0.00002544
Iteration 79/1000 | Loss: 0.00002544
Iteration 80/1000 | Loss: 0.00002544
Iteration 81/1000 | Loss: 0.00002543
Iteration 82/1000 | Loss: 0.00002543
Iteration 83/1000 | Loss: 0.00002543
Iteration 84/1000 | Loss: 0.00002543
Iteration 85/1000 | Loss: 0.00002543
Iteration 86/1000 | Loss: 0.00002543
Iteration 87/1000 | Loss: 0.00002543
Iteration 88/1000 | Loss: 0.00002543
Iteration 89/1000 | Loss: 0.00002543
Iteration 90/1000 | Loss: 0.00002542
Iteration 91/1000 | Loss: 0.00002542
Iteration 92/1000 | Loss: 0.00002542
Iteration 93/1000 | Loss: 0.00002542
Iteration 94/1000 | Loss: 0.00002542
Iteration 95/1000 | Loss: 0.00002541
Iteration 96/1000 | Loss: 0.00002541
Iteration 97/1000 | Loss: 0.00002541
Iteration 98/1000 | Loss: 0.00002541
Iteration 99/1000 | Loss: 0.00002541
Iteration 100/1000 | Loss: 0.00002540
Iteration 101/1000 | Loss: 0.00002540
Iteration 102/1000 | Loss: 0.00002540
Iteration 103/1000 | Loss: 0.00002540
Iteration 104/1000 | Loss: 0.00002539
Iteration 105/1000 | Loss: 0.00002539
Iteration 106/1000 | Loss: 0.00002539
Iteration 107/1000 | Loss: 0.00002539
Iteration 108/1000 | Loss: 0.00002539
Iteration 109/1000 | Loss: 0.00002539
Iteration 110/1000 | Loss: 0.00002539
Iteration 111/1000 | Loss: 0.00002538
Iteration 112/1000 | Loss: 0.00002538
Iteration 113/1000 | Loss: 0.00002538
Iteration 114/1000 | Loss: 0.00002538
Iteration 115/1000 | Loss: 0.00002538
Iteration 116/1000 | Loss: 0.00002538
Iteration 117/1000 | Loss: 0.00002538
Iteration 118/1000 | Loss: 0.00002538
Iteration 119/1000 | Loss: 0.00002538
Iteration 120/1000 | Loss: 0.00002538
Iteration 121/1000 | Loss: 0.00002538
Iteration 122/1000 | Loss: 0.00002538
Iteration 123/1000 | Loss: 0.00002538
Iteration 124/1000 | Loss: 0.00002538
Iteration 125/1000 | Loss: 0.00002538
Iteration 126/1000 | Loss: 0.00002537
Iteration 127/1000 | Loss: 0.00002537
Iteration 128/1000 | Loss: 0.00002537
Iteration 129/1000 | Loss: 0.00002537
Iteration 130/1000 | Loss: 0.00002537
Iteration 131/1000 | Loss: 0.00002537
Iteration 132/1000 | Loss: 0.00002537
Iteration 133/1000 | Loss: 0.00002537
Iteration 134/1000 | Loss: 0.00002537
Iteration 135/1000 | Loss: 0.00002537
Iteration 136/1000 | Loss: 0.00002537
Iteration 137/1000 | Loss: 0.00002537
Iteration 138/1000 | Loss: 0.00002537
Iteration 139/1000 | Loss: 0.00002537
Iteration 140/1000 | Loss: 0.00002537
Iteration 141/1000 | Loss: 0.00002537
Iteration 142/1000 | Loss: 0.00002537
Iteration 143/1000 | Loss: 0.00002537
Iteration 144/1000 | Loss: 0.00002537
Iteration 145/1000 | Loss: 0.00002537
Iteration 146/1000 | Loss: 0.00002537
Iteration 147/1000 | Loss: 0.00002537
Iteration 148/1000 | Loss: 0.00002537
Iteration 149/1000 | Loss: 0.00002537
Iteration 150/1000 | Loss: 0.00002537
Iteration 151/1000 | Loss: 0.00002537
Iteration 152/1000 | Loss: 0.00002537
Iteration 153/1000 | Loss: 0.00002537
Iteration 154/1000 | Loss: 0.00002537
Iteration 155/1000 | Loss: 0.00002537
Iteration 156/1000 | Loss: 0.00002537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.5366420231875964e-05, 2.5366420231875964e-05, 2.5366420231875964e-05, 2.5366420231875964e-05, 2.5366420231875964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5366420231875964e-05

Optimization complete. Final v2v error: 4.056853294372559 mm

Highest mean error: 7.0634260177612305 mm for frame 116

Lowest mean error: 2.7009716033935547 mm for frame 76

Saving results

Total time: 43.15353298187256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00678133
Iteration 2/25 | Loss: 0.00113946
Iteration 3/25 | Loss: 0.00105866
Iteration 4/25 | Loss: 0.00104983
Iteration 5/25 | Loss: 0.00104690
Iteration 6/25 | Loss: 0.00104629
Iteration 7/25 | Loss: 0.00104625
Iteration 8/25 | Loss: 0.00104625
Iteration 9/25 | Loss: 0.00104625
Iteration 10/25 | Loss: 0.00104625
Iteration 11/25 | Loss: 0.00104625
Iteration 12/25 | Loss: 0.00104625
Iteration 13/25 | Loss: 0.00104625
Iteration 14/25 | Loss: 0.00104625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001046246849000454, 0.001046246849000454, 0.001046246849000454, 0.001046246849000454, 0.001046246849000454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001046246849000454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46031177
Iteration 2/25 | Loss: 0.00070791
Iteration 3/25 | Loss: 0.00070791
Iteration 4/25 | Loss: 0.00070791
Iteration 5/25 | Loss: 0.00070791
Iteration 6/25 | Loss: 0.00070791
Iteration 7/25 | Loss: 0.00070791
Iteration 8/25 | Loss: 0.00070791
Iteration 9/25 | Loss: 0.00070791
Iteration 10/25 | Loss: 0.00070791
Iteration 11/25 | Loss: 0.00070791
Iteration 12/25 | Loss: 0.00070791
Iteration 13/25 | Loss: 0.00070791
Iteration 14/25 | Loss: 0.00070791
Iteration 15/25 | Loss: 0.00070791
Iteration 16/25 | Loss: 0.00070791
Iteration 17/25 | Loss: 0.00070791
Iteration 18/25 | Loss: 0.00070791
Iteration 19/25 | Loss: 0.00070791
Iteration 20/25 | Loss: 0.00070791
Iteration 21/25 | Loss: 0.00070791
Iteration 22/25 | Loss: 0.00070791
Iteration 23/25 | Loss: 0.00070791
Iteration 24/25 | Loss: 0.00070791
Iteration 25/25 | Loss: 0.00070791

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070791
Iteration 2/1000 | Loss: 0.00002070
Iteration 3/1000 | Loss: 0.00001273
Iteration 4/1000 | Loss: 0.00001102
Iteration 5/1000 | Loss: 0.00001054
Iteration 6/1000 | Loss: 0.00001016
Iteration 7/1000 | Loss: 0.00001016
Iteration 8/1000 | Loss: 0.00000988
Iteration 9/1000 | Loss: 0.00000974
Iteration 10/1000 | Loss: 0.00000973
Iteration 11/1000 | Loss: 0.00000972
Iteration 12/1000 | Loss: 0.00000969
Iteration 13/1000 | Loss: 0.00000964
Iteration 14/1000 | Loss: 0.00000959
Iteration 15/1000 | Loss: 0.00000951
Iteration 16/1000 | Loss: 0.00000948
Iteration 17/1000 | Loss: 0.00000942
Iteration 18/1000 | Loss: 0.00000941
Iteration 19/1000 | Loss: 0.00000938
Iteration 20/1000 | Loss: 0.00000937
Iteration 21/1000 | Loss: 0.00000936
Iteration 22/1000 | Loss: 0.00000936
Iteration 23/1000 | Loss: 0.00000935
Iteration 24/1000 | Loss: 0.00000932
Iteration 25/1000 | Loss: 0.00000932
Iteration 26/1000 | Loss: 0.00000932
Iteration 27/1000 | Loss: 0.00000932
Iteration 28/1000 | Loss: 0.00000932
Iteration 29/1000 | Loss: 0.00000932
Iteration 30/1000 | Loss: 0.00000932
Iteration 31/1000 | Loss: 0.00000932
Iteration 32/1000 | Loss: 0.00000932
Iteration 33/1000 | Loss: 0.00000931
Iteration 34/1000 | Loss: 0.00000928
Iteration 35/1000 | Loss: 0.00000928
Iteration 36/1000 | Loss: 0.00000928
Iteration 37/1000 | Loss: 0.00000927
Iteration 38/1000 | Loss: 0.00000927
Iteration 39/1000 | Loss: 0.00000926
Iteration 40/1000 | Loss: 0.00000926
Iteration 41/1000 | Loss: 0.00000925
Iteration 42/1000 | Loss: 0.00000925
Iteration 43/1000 | Loss: 0.00000925
Iteration 44/1000 | Loss: 0.00000924
Iteration 45/1000 | Loss: 0.00000923
Iteration 46/1000 | Loss: 0.00000923
Iteration 47/1000 | Loss: 0.00000923
Iteration 48/1000 | Loss: 0.00000923
Iteration 49/1000 | Loss: 0.00000923
Iteration 50/1000 | Loss: 0.00000922
Iteration 51/1000 | Loss: 0.00000922
Iteration 52/1000 | Loss: 0.00000922
Iteration 53/1000 | Loss: 0.00000921
Iteration 54/1000 | Loss: 0.00000921
Iteration 55/1000 | Loss: 0.00000920
Iteration 56/1000 | Loss: 0.00000920
Iteration 57/1000 | Loss: 0.00000920
Iteration 58/1000 | Loss: 0.00000919
Iteration 59/1000 | Loss: 0.00000919
Iteration 60/1000 | Loss: 0.00000919
Iteration 61/1000 | Loss: 0.00000919
Iteration 62/1000 | Loss: 0.00000919
Iteration 63/1000 | Loss: 0.00000918
Iteration 64/1000 | Loss: 0.00000917
Iteration 65/1000 | Loss: 0.00000917
Iteration 66/1000 | Loss: 0.00000916
Iteration 67/1000 | Loss: 0.00000916
Iteration 68/1000 | Loss: 0.00000915
Iteration 69/1000 | Loss: 0.00000915
Iteration 70/1000 | Loss: 0.00000915
Iteration 71/1000 | Loss: 0.00000915
Iteration 72/1000 | Loss: 0.00000915
Iteration 73/1000 | Loss: 0.00000915
Iteration 74/1000 | Loss: 0.00000915
Iteration 75/1000 | Loss: 0.00000914
Iteration 76/1000 | Loss: 0.00000914
Iteration 77/1000 | Loss: 0.00000914
Iteration 78/1000 | Loss: 0.00000914
Iteration 79/1000 | Loss: 0.00000913
Iteration 80/1000 | Loss: 0.00000913
Iteration 81/1000 | Loss: 0.00000912
Iteration 82/1000 | Loss: 0.00000912
Iteration 83/1000 | Loss: 0.00000912
Iteration 84/1000 | Loss: 0.00000912
Iteration 85/1000 | Loss: 0.00000912
Iteration 86/1000 | Loss: 0.00000912
Iteration 87/1000 | Loss: 0.00000912
Iteration 88/1000 | Loss: 0.00000912
Iteration 89/1000 | Loss: 0.00000912
Iteration 90/1000 | Loss: 0.00000912
Iteration 91/1000 | Loss: 0.00000912
Iteration 92/1000 | Loss: 0.00000911
Iteration 93/1000 | Loss: 0.00000911
Iteration 94/1000 | Loss: 0.00000911
Iteration 95/1000 | Loss: 0.00000911
Iteration 96/1000 | Loss: 0.00000911
Iteration 97/1000 | Loss: 0.00000911
Iteration 98/1000 | Loss: 0.00000911
Iteration 99/1000 | Loss: 0.00000911
Iteration 100/1000 | Loss: 0.00000911
Iteration 101/1000 | Loss: 0.00000911
Iteration 102/1000 | Loss: 0.00000911
Iteration 103/1000 | Loss: 0.00000910
Iteration 104/1000 | Loss: 0.00000910
Iteration 105/1000 | Loss: 0.00000910
Iteration 106/1000 | Loss: 0.00000910
Iteration 107/1000 | Loss: 0.00000910
Iteration 108/1000 | Loss: 0.00000910
Iteration 109/1000 | Loss: 0.00000910
Iteration 110/1000 | Loss: 0.00000910
Iteration 111/1000 | Loss: 0.00000910
Iteration 112/1000 | Loss: 0.00000910
Iteration 113/1000 | Loss: 0.00000909
Iteration 114/1000 | Loss: 0.00000909
Iteration 115/1000 | Loss: 0.00000909
Iteration 116/1000 | Loss: 0.00000909
Iteration 117/1000 | Loss: 0.00000909
Iteration 118/1000 | Loss: 0.00000909
Iteration 119/1000 | Loss: 0.00000909
Iteration 120/1000 | Loss: 0.00000909
Iteration 121/1000 | Loss: 0.00000909
Iteration 122/1000 | Loss: 0.00000909
Iteration 123/1000 | Loss: 0.00000909
Iteration 124/1000 | Loss: 0.00000909
Iteration 125/1000 | Loss: 0.00000909
Iteration 126/1000 | Loss: 0.00000909
Iteration 127/1000 | Loss: 0.00000909
Iteration 128/1000 | Loss: 0.00000908
Iteration 129/1000 | Loss: 0.00000908
Iteration 130/1000 | Loss: 0.00000908
Iteration 131/1000 | Loss: 0.00000908
Iteration 132/1000 | Loss: 0.00000908
Iteration 133/1000 | Loss: 0.00000908
Iteration 134/1000 | Loss: 0.00000908
Iteration 135/1000 | Loss: 0.00000908
Iteration 136/1000 | Loss: 0.00000908
Iteration 137/1000 | Loss: 0.00000908
Iteration 138/1000 | Loss: 0.00000907
Iteration 139/1000 | Loss: 0.00000907
Iteration 140/1000 | Loss: 0.00000907
Iteration 141/1000 | Loss: 0.00000907
Iteration 142/1000 | Loss: 0.00000907
Iteration 143/1000 | Loss: 0.00000907
Iteration 144/1000 | Loss: 0.00000907
Iteration 145/1000 | Loss: 0.00000907
Iteration 146/1000 | Loss: 0.00000907
Iteration 147/1000 | Loss: 0.00000907
Iteration 148/1000 | Loss: 0.00000907
Iteration 149/1000 | Loss: 0.00000907
Iteration 150/1000 | Loss: 0.00000906
Iteration 151/1000 | Loss: 0.00000906
Iteration 152/1000 | Loss: 0.00000906
Iteration 153/1000 | Loss: 0.00000906
Iteration 154/1000 | Loss: 0.00000906
Iteration 155/1000 | Loss: 0.00000906
Iteration 156/1000 | Loss: 0.00000906
Iteration 157/1000 | Loss: 0.00000906
Iteration 158/1000 | Loss: 0.00000906
Iteration 159/1000 | Loss: 0.00000906
Iteration 160/1000 | Loss: 0.00000906
Iteration 161/1000 | Loss: 0.00000905
Iteration 162/1000 | Loss: 0.00000905
Iteration 163/1000 | Loss: 0.00000905
Iteration 164/1000 | Loss: 0.00000905
Iteration 165/1000 | Loss: 0.00000905
Iteration 166/1000 | Loss: 0.00000905
Iteration 167/1000 | Loss: 0.00000905
Iteration 168/1000 | Loss: 0.00000905
Iteration 169/1000 | Loss: 0.00000905
Iteration 170/1000 | Loss: 0.00000905
Iteration 171/1000 | Loss: 0.00000904
Iteration 172/1000 | Loss: 0.00000904
Iteration 173/1000 | Loss: 0.00000904
Iteration 174/1000 | Loss: 0.00000904
Iteration 175/1000 | Loss: 0.00000904
Iteration 176/1000 | Loss: 0.00000904
Iteration 177/1000 | Loss: 0.00000904
Iteration 178/1000 | Loss: 0.00000903
Iteration 179/1000 | Loss: 0.00000903
Iteration 180/1000 | Loss: 0.00000903
Iteration 181/1000 | Loss: 0.00000903
Iteration 182/1000 | Loss: 0.00000903
Iteration 183/1000 | Loss: 0.00000903
Iteration 184/1000 | Loss: 0.00000902
Iteration 185/1000 | Loss: 0.00000902
Iteration 186/1000 | Loss: 0.00000902
Iteration 187/1000 | Loss: 0.00000902
Iteration 188/1000 | Loss: 0.00000902
Iteration 189/1000 | Loss: 0.00000902
Iteration 190/1000 | Loss: 0.00000902
Iteration 191/1000 | Loss: 0.00000902
Iteration 192/1000 | Loss: 0.00000901
Iteration 193/1000 | Loss: 0.00000901
Iteration 194/1000 | Loss: 0.00000901
Iteration 195/1000 | Loss: 0.00000901
Iteration 196/1000 | Loss: 0.00000901
Iteration 197/1000 | Loss: 0.00000901
Iteration 198/1000 | Loss: 0.00000901
Iteration 199/1000 | Loss: 0.00000901
Iteration 200/1000 | Loss: 0.00000901
Iteration 201/1000 | Loss: 0.00000901
Iteration 202/1000 | Loss: 0.00000901
Iteration 203/1000 | Loss: 0.00000901
Iteration 204/1000 | Loss: 0.00000901
Iteration 205/1000 | Loss: 0.00000901
Iteration 206/1000 | Loss: 0.00000900
Iteration 207/1000 | Loss: 0.00000900
Iteration 208/1000 | Loss: 0.00000900
Iteration 209/1000 | Loss: 0.00000900
Iteration 210/1000 | Loss: 0.00000900
Iteration 211/1000 | Loss: 0.00000900
Iteration 212/1000 | Loss: 0.00000900
Iteration 213/1000 | Loss: 0.00000900
Iteration 214/1000 | Loss: 0.00000900
Iteration 215/1000 | Loss: 0.00000900
Iteration 216/1000 | Loss: 0.00000899
Iteration 217/1000 | Loss: 0.00000899
Iteration 218/1000 | Loss: 0.00000899
Iteration 219/1000 | Loss: 0.00000899
Iteration 220/1000 | Loss: 0.00000899
Iteration 221/1000 | Loss: 0.00000899
Iteration 222/1000 | Loss: 0.00000898
Iteration 223/1000 | Loss: 0.00000898
Iteration 224/1000 | Loss: 0.00000898
Iteration 225/1000 | Loss: 0.00000898
Iteration 226/1000 | Loss: 0.00000898
Iteration 227/1000 | Loss: 0.00000898
Iteration 228/1000 | Loss: 0.00000898
Iteration 229/1000 | Loss: 0.00000898
Iteration 230/1000 | Loss: 0.00000898
Iteration 231/1000 | Loss: 0.00000898
Iteration 232/1000 | Loss: 0.00000898
Iteration 233/1000 | Loss: 0.00000898
Iteration 234/1000 | Loss: 0.00000898
Iteration 235/1000 | Loss: 0.00000898
Iteration 236/1000 | Loss: 0.00000898
Iteration 237/1000 | Loss: 0.00000898
Iteration 238/1000 | Loss: 0.00000898
Iteration 239/1000 | Loss: 0.00000898
Iteration 240/1000 | Loss: 0.00000898
Iteration 241/1000 | Loss: 0.00000898
Iteration 242/1000 | Loss: 0.00000898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [8.981799510365818e-06, 8.981799510365818e-06, 8.981799510365818e-06, 8.981799510365818e-06, 8.981799510365818e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.981799510365818e-06

Optimization complete. Final v2v error: 2.5375139713287354 mm

Highest mean error: 3.408348560333252 mm for frame 73

Lowest mean error: 2.2887139320373535 mm for frame 11

Saving results

Total time: 39.14415669441223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832676
Iteration 2/25 | Loss: 0.00118525
Iteration 3/25 | Loss: 0.00105904
Iteration 4/25 | Loss: 0.00104683
Iteration 5/25 | Loss: 0.00104522
Iteration 6/25 | Loss: 0.00104522
Iteration 7/25 | Loss: 0.00104522
Iteration 8/25 | Loss: 0.00104522
Iteration 9/25 | Loss: 0.00104522
Iteration 10/25 | Loss: 0.00104522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010452230926603079, 0.0010452230926603079, 0.0010452230926603079, 0.0010452230926603079, 0.0010452230926603079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010452230926603079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37064087
Iteration 2/25 | Loss: 0.00059115
Iteration 3/25 | Loss: 0.00059115
Iteration 4/25 | Loss: 0.00059115
Iteration 5/25 | Loss: 0.00059115
Iteration 6/25 | Loss: 0.00059115
Iteration 7/25 | Loss: 0.00059115
Iteration 8/25 | Loss: 0.00059115
Iteration 9/25 | Loss: 0.00059115
Iteration 10/25 | Loss: 0.00059115
Iteration 11/25 | Loss: 0.00059115
Iteration 12/25 | Loss: 0.00059115
Iteration 13/25 | Loss: 0.00059115
Iteration 14/25 | Loss: 0.00059115
Iteration 15/25 | Loss: 0.00059115
Iteration 16/25 | Loss: 0.00059115
Iteration 17/25 | Loss: 0.00059115
Iteration 18/25 | Loss: 0.00059115
Iteration 19/25 | Loss: 0.00059115
Iteration 20/25 | Loss: 0.00059115
Iteration 21/25 | Loss: 0.00059115
Iteration 22/25 | Loss: 0.00059115
Iteration 23/25 | Loss: 0.00059115
Iteration 24/25 | Loss: 0.00059115
Iteration 25/25 | Loss: 0.00059115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059115
Iteration 2/1000 | Loss: 0.00001990
Iteration 3/1000 | Loss: 0.00001382
Iteration 4/1000 | Loss: 0.00001219
Iteration 5/1000 | Loss: 0.00001133
Iteration 6/1000 | Loss: 0.00001077
Iteration 7/1000 | Loss: 0.00001045
Iteration 8/1000 | Loss: 0.00001022
Iteration 9/1000 | Loss: 0.00000993
Iteration 10/1000 | Loss: 0.00000990
Iteration 11/1000 | Loss: 0.00000986
Iteration 12/1000 | Loss: 0.00000978
Iteration 13/1000 | Loss: 0.00000977
Iteration 14/1000 | Loss: 0.00000976
Iteration 15/1000 | Loss: 0.00000976
Iteration 16/1000 | Loss: 0.00000974
Iteration 17/1000 | Loss: 0.00000973
Iteration 18/1000 | Loss: 0.00000972
Iteration 19/1000 | Loss: 0.00000972
Iteration 20/1000 | Loss: 0.00000972
Iteration 21/1000 | Loss: 0.00000971
Iteration 22/1000 | Loss: 0.00000970
Iteration 23/1000 | Loss: 0.00000964
Iteration 24/1000 | Loss: 0.00000963
Iteration 25/1000 | Loss: 0.00000963
Iteration 26/1000 | Loss: 0.00000962
Iteration 27/1000 | Loss: 0.00000962
Iteration 28/1000 | Loss: 0.00000954
Iteration 29/1000 | Loss: 0.00000954
Iteration 30/1000 | Loss: 0.00000947
Iteration 31/1000 | Loss: 0.00000947
Iteration 32/1000 | Loss: 0.00000941
Iteration 33/1000 | Loss: 0.00000941
Iteration 34/1000 | Loss: 0.00000940
Iteration 35/1000 | Loss: 0.00000939
Iteration 36/1000 | Loss: 0.00000939
Iteration 37/1000 | Loss: 0.00000933
Iteration 38/1000 | Loss: 0.00000932
Iteration 39/1000 | Loss: 0.00000928
Iteration 40/1000 | Loss: 0.00000928
Iteration 41/1000 | Loss: 0.00000927
Iteration 42/1000 | Loss: 0.00000927
Iteration 43/1000 | Loss: 0.00000926
Iteration 44/1000 | Loss: 0.00000925
Iteration 45/1000 | Loss: 0.00000924
Iteration 46/1000 | Loss: 0.00000924
Iteration 47/1000 | Loss: 0.00000923
Iteration 48/1000 | Loss: 0.00000923
Iteration 49/1000 | Loss: 0.00000922
Iteration 50/1000 | Loss: 0.00000921
Iteration 51/1000 | Loss: 0.00000921
Iteration 52/1000 | Loss: 0.00000920
Iteration 53/1000 | Loss: 0.00000920
Iteration 54/1000 | Loss: 0.00000919
Iteration 55/1000 | Loss: 0.00000919
Iteration 56/1000 | Loss: 0.00000919
Iteration 57/1000 | Loss: 0.00000919
Iteration 58/1000 | Loss: 0.00000919
Iteration 59/1000 | Loss: 0.00000918
Iteration 60/1000 | Loss: 0.00000918
Iteration 61/1000 | Loss: 0.00000918
Iteration 62/1000 | Loss: 0.00000918
Iteration 63/1000 | Loss: 0.00000917
Iteration 64/1000 | Loss: 0.00000917
Iteration 65/1000 | Loss: 0.00000917
Iteration 66/1000 | Loss: 0.00000917
Iteration 67/1000 | Loss: 0.00000916
Iteration 68/1000 | Loss: 0.00000916
Iteration 69/1000 | Loss: 0.00000916
Iteration 70/1000 | Loss: 0.00000916
Iteration 71/1000 | Loss: 0.00000915
Iteration 72/1000 | Loss: 0.00000915
Iteration 73/1000 | Loss: 0.00000914
Iteration 74/1000 | Loss: 0.00000914
Iteration 75/1000 | Loss: 0.00000913
Iteration 76/1000 | Loss: 0.00000913
Iteration 77/1000 | Loss: 0.00000913
Iteration 78/1000 | Loss: 0.00000912
Iteration 79/1000 | Loss: 0.00000912
Iteration 80/1000 | Loss: 0.00000912
Iteration 81/1000 | Loss: 0.00000911
Iteration 82/1000 | Loss: 0.00000911
Iteration 83/1000 | Loss: 0.00000910
Iteration 84/1000 | Loss: 0.00000910
Iteration 85/1000 | Loss: 0.00000910
Iteration 86/1000 | Loss: 0.00000909
Iteration 87/1000 | Loss: 0.00000908
Iteration 88/1000 | Loss: 0.00000908
Iteration 89/1000 | Loss: 0.00000908
Iteration 90/1000 | Loss: 0.00000908
Iteration 91/1000 | Loss: 0.00000908
Iteration 92/1000 | Loss: 0.00000908
Iteration 93/1000 | Loss: 0.00000908
Iteration 94/1000 | Loss: 0.00000908
Iteration 95/1000 | Loss: 0.00000908
Iteration 96/1000 | Loss: 0.00000907
Iteration 97/1000 | Loss: 0.00000907
Iteration 98/1000 | Loss: 0.00000907
Iteration 99/1000 | Loss: 0.00000907
Iteration 100/1000 | Loss: 0.00000907
Iteration 101/1000 | Loss: 0.00000907
Iteration 102/1000 | Loss: 0.00000907
Iteration 103/1000 | Loss: 0.00000907
Iteration 104/1000 | Loss: 0.00000906
Iteration 105/1000 | Loss: 0.00000906
Iteration 106/1000 | Loss: 0.00000906
Iteration 107/1000 | Loss: 0.00000906
Iteration 108/1000 | Loss: 0.00000905
Iteration 109/1000 | Loss: 0.00000905
Iteration 110/1000 | Loss: 0.00000905
Iteration 111/1000 | Loss: 0.00000905
Iteration 112/1000 | Loss: 0.00000904
Iteration 113/1000 | Loss: 0.00000904
Iteration 114/1000 | Loss: 0.00000904
Iteration 115/1000 | Loss: 0.00000903
Iteration 116/1000 | Loss: 0.00000903
Iteration 117/1000 | Loss: 0.00000903
Iteration 118/1000 | Loss: 0.00000902
Iteration 119/1000 | Loss: 0.00000902
Iteration 120/1000 | Loss: 0.00000901
Iteration 121/1000 | Loss: 0.00000901
Iteration 122/1000 | Loss: 0.00000900
Iteration 123/1000 | Loss: 0.00000900
Iteration 124/1000 | Loss: 0.00000900
Iteration 125/1000 | Loss: 0.00000899
Iteration 126/1000 | Loss: 0.00000899
Iteration 127/1000 | Loss: 0.00000899
Iteration 128/1000 | Loss: 0.00000898
Iteration 129/1000 | Loss: 0.00000898
Iteration 130/1000 | Loss: 0.00000898
Iteration 131/1000 | Loss: 0.00000898
Iteration 132/1000 | Loss: 0.00000898
Iteration 133/1000 | Loss: 0.00000898
Iteration 134/1000 | Loss: 0.00000897
Iteration 135/1000 | Loss: 0.00000897
Iteration 136/1000 | Loss: 0.00000897
Iteration 137/1000 | Loss: 0.00000897
Iteration 138/1000 | Loss: 0.00000897
Iteration 139/1000 | Loss: 0.00000897
Iteration 140/1000 | Loss: 0.00000896
Iteration 141/1000 | Loss: 0.00000896
Iteration 142/1000 | Loss: 0.00000896
Iteration 143/1000 | Loss: 0.00000896
Iteration 144/1000 | Loss: 0.00000895
Iteration 145/1000 | Loss: 0.00000895
Iteration 146/1000 | Loss: 0.00000895
Iteration 147/1000 | Loss: 0.00000895
Iteration 148/1000 | Loss: 0.00000895
Iteration 149/1000 | Loss: 0.00000894
Iteration 150/1000 | Loss: 0.00000894
Iteration 151/1000 | Loss: 0.00000894
Iteration 152/1000 | Loss: 0.00000894
Iteration 153/1000 | Loss: 0.00000894
Iteration 154/1000 | Loss: 0.00000894
Iteration 155/1000 | Loss: 0.00000894
Iteration 156/1000 | Loss: 0.00000894
Iteration 157/1000 | Loss: 0.00000894
Iteration 158/1000 | Loss: 0.00000894
Iteration 159/1000 | Loss: 0.00000894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [8.940276529756375e-06, 8.940276529756375e-06, 8.940276529756375e-06, 8.940276529756375e-06, 8.940276529756375e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.940276529756375e-06

Optimization complete. Final v2v error: 2.5518364906311035 mm

Highest mean error: 2.706217050552368 mm for frame 0

Lowest mean error: 2.3518624305725098 mm for frame 240

Saving results

Total time: 43.46386671066284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398968
Iteration 2/25 | Loss: 0.00116377
Iteration 3/25 | Loss: 0.00105068
Iteration 4/25 | Loss: 0.00103911
Iteration 5/25 | Loss: 0.00103653
Iteration 6/25 | Loss: 0.00103580
Iteration 7/25 | Loss: 0.00103580
Iteration 8/25 | Loss: 0.00103580
Iteration 9/25 | Loss: 0.00103580
Iteration 10/25 | Loss: 0.00103580
Iteration 11/25 | Loss: 0.00103580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010357999708503485, 0.0010357999708503485, 0.0010357999708503485, 0.0010357999708503485, 0.0010357999708503485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010357999708503485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38499701
Iteration 2/25 | Loss: 0.00077686
Iteration 3/25 | Loss: 0.00077686
Iteration 4/25 | Loss: 0.00077686
Iteration 5/25 | Loss: 0.00077686
Iteration 6/25 | Loss: 0.00077686
Iteration 7/25 | Loss: 0.00077686
Iteration 8/25 | Loss: 0.00077685
Iteration 9/25 | Loss: 0.00077685
Iteration 10/25 | Loss: 0.00077685
Iteration 11/25 | Loss: 0.00077685
Iteration 12/25 | Loss: 0.00077685
Iteration 13/25 | Loss: 0.00077685
Iteration 14/25 | Loss: 0.00077685
Iteration 15/25 | Loss: 0.00077685
Iteration 16/25 | Loss: 0.00077685
Iteration 17/25 | Loss: 0.00077685
Iteration 18/25 | Loss: 0.00077685
Iteration 19/25 | Loss: 0.00077685
Iteration 20/25 | Loss: 0.00077685
Iteration 21/25 | Loss: 0.00077685
Iteration 22/25 | Loss: 0.00077685
Iteration 23/25 | Loss: 0.00077685
Iteration 24/25 | Loss: 0.00077685
Iteration 25/25 | Loss: 0.00077685

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077685
Iteration 2/1000 | Loss: 0.00002513
Iteration 3/1000 | Loss: 0.00001485
Iteration 4/1000 | Loss: 0.00001171
Iteration 5/1000 | Loss: 0.00001039
Iteration 6/1000 | Loss: 0.00000965
Iteration 7/1000 | Loss: 0.00000918
Iteration 8/1000 | Loss: 0.00000889
Iteration 9/1000 | Loss: 0.00000871
Iteration 10/1000 | Loss: 0.00000870
Iteration 11/1000 | Loss: 0.00000869
Iteration 12/1000 | Loss: 0.00000869
Iteration 13/1000 | Loss: 0.00000865
Iteration 14/1000 | Loss: 0.00000863
Iteration 15/1000 | Loss: 0.00000862
Iteration 16/1000 | Loss: 0.00000859
Iteration 17/1000 | Loss: 0.00000852
Iteration 18/1000 | Loss: 0.00000849
Iteration 19/1000 | Loss: 0.00000846
Iteration 20/1000 | Loss: 0.00000846
Iteration 21/1000 | Loss: 0.00000845
Iteration 22/1000 | Loss: 0.00000845
Iteration 23/1000 | Loss: 0.00000845
Iteration 24/1000 | Loss: 0.00000844
Iteration 25/1000 | Loss: 0.00000843
Iteration 26/1000 | Loss: 0.00000843
Iteration 27/1000 | Loss: 0.00000843
Iteration 28/1000 | Loss: 0.00000842
Iteration 29/1000 | Loss: 0.00000842
Iteration 30/1000 | Loss: 0.00000840
Iteration 31/1000 | Loss: 0.00000840
Iteration 32/1000 | Loss: 0.00000840
Iteration 33/1000 | Loss: 0.00000840
Iteration 34/1000 | Loss: 0.00000840
Iteration 35/1000 | Loss: 0.00000840
Iteration 36/1000 | Loss: 0.00000840
Iteration 37/1000 | Loss: 0.00000840
Iteration 38/1000 | Loss: 0.00000840
Iteration 39/1000 | Loss: 0.00000839
Iteration 40/1000 | Loss: 0.00000839
Iteration 41/1000 | Loss: 0.00000839
Iteration 42/1000 | Loss: 0.00000838
Iteration 43/1000 | Loss: 0.00000837
Iteration 44/1000 | Loss: 0.00000837
Iteration 45/1000 | Loss: 0.00000836
Iteration 46/1000 | Loss: 0.00000836
Iteration 47/1000 | Loss: 0.00000835
Iteration 48/1000 | Loss: 0.00000834
Iteration 49/1000 | Loss: 0.00000834
Iteration 50/1000 | Loss: 0.00000834
Iteration 51/1000 | Loss: 0.00000832
Iteration 52/1000 | Loss: 0.00000831
Iteration 53/1000 | Loss: 0.00000831
Iteration 54/1000 | Loss: 0.00000831
Iteration 55/1000 | Loss: 0.00000830
Iteration 56/1000 | Loss: 0.00000830
Iteration 57/1000 | Loss: 0.00000830
Iteration 58/1000 | Loss: 0.00000829
Iteration 59/1000 | Loss: 0.00000829
Iteration 60/1000 | Loss: 0.00000828
Iteration 61/1000 | Loss: 0.00000828
Iteration 62/1000 | Loss: 0.00000827
Iteration 63/1000 | Loss: 0.00000827
Iteration 64/1000 | Loss: 0.00000825
Iteration 65/1000 | Loss: 0.00000824
Iteration 66/1000 | Loss: 0.00000824
Iteration 67/1000 | Loss: 0.00000823
Iteration 68/1000 | Loss: 0.00000823
Iteration 69/1000 | Loss: 0.00000822
Iteration 70/1000 | Loss: 0.00000822
Iteration 71/1000 | Loss: 0.00000822
Iteration 72/1000 | Loss: 0.00000822
Iteration 73/1000 | Loss: 0.00000821
Iteration 74/1000 | Loss: 0.00000821
Iteration 75/1000 | Loss: 0.00000821
Iteration 76/1000 | Loss: 0.00000821
Iteration 77/1000 | Loss: 0.00000821
Iteration 78/1000 | Loss: 0.00000821
Iteration 79/1000 | Loss: 0.00000821
Iteration 80/1000 | Loss: 0.00000821
Iteration 81/1000 | Loss: 0.00000820
Iteration 82/1000 | Loss: 0.00000820
Iteration 83/1000 | Loss: 0.00000820
Iteration 84/1000 | Loss: 0.00000820
Iteration 85/1000 | Loss: 0.00000820
Iteration 86/1000 | Loss: 0.00000819
Iteration 87/1000 | Loss: 0.00000819
Iteration 88/1000 | Loss: 0.00000819
Iteration 89/1000 | Loss: 0.00000818
Iteration 90/1000 | Loss: 0.00000818
Iteration 91/1000 | Loss: 0.00000818
Iteration 92/1000 | Loss: 0.00000817
Iteration 93/1000 | Loss: 0.00000817
Iteration 94/1000 | Loss: 0.00000817
Iteration 95/1000 | Loss: 0.00000817
Iteration 96/1000 | Loss: 0.00000816
Iteration 97/1000 | Loss: 0.00000816
Iteration 98/1000 | Loss: 0.00000816
Iteration 99/1000 | Loss: 0.00000816
Iteration 100/1000 | Loss: 0.00000816
Iteration 101/1000 | Loss: 0.00000816
Iteration 102/1000 | Loss: 0.00000815
Iteration 103/1000 | Loss: 0.00000815
Iteration 104/1000 | Loss: 0.00000815
Iteration 105/1000 | Loss: 0.00000815
Iteration 106/1000 | Loss: 0.00000815
Iteration 107/1000 | Loss: 0.00000814
Iteration 108/1000 | Loss: 0.00000814
Iteration 109/1000 | Loss: 0.00000814
Iteration 110/1000 | Loss: 0.00000814
Iteration 111/1000 | Loss: 0.00000814
Iteration 112/1000 | Loss: 0.00000814
Iteration 113/1000 | Loss: 0.00000814
Iteration 114/1000 | Loss: 0.00000814
Iteration 115/1000 | Loss: 0.00000814
Iteration 116/1000 | Loss: 0.00000814
Iteration 117/1000 | Loss: 0.00000813
Iteration 118/1000 | Loss: 0.00000813
Iteration 119/1000 | Loss: 0.00000813
Iteration 120/1000 | Loss: 0.00000813
Iteration 121/1000 | Loss: 0.00000813
Iteration 122/1000 | Loss: 0.00000813
Iteration 123/1000 | Loss: 0.00000813
Iteration 124/1000 | Loss: 0.00000813
Iteration 125/1000 | Loss: 0.00000813
Iteration 126/1000 | Loss: 0.00000813
Iteration 127/1000 | Loss: 0.00000812
Iteration 128/1000 | Loss: 0.00000812
Iteration 129/1000 | Loss: 0.00000812
Iteration 130/1000 | Loss: 0.00000812
Iteration 131/1000 | Loss: 0.00000812
Iteration 132/1000 | Loss: 0.00000812
Iteration 133/1000 | Loss: 0.00000812
Iteration 134/1000 | Loss: 0.00000812
Iteration 135/1000 | Loss: 0.00000812
Iteration 136/1000 | Loss: 0.00000812
Iteration 137/1000 | Loss: 0.00000812
Iteration 138/1000 | Loss: 0.00000812
Iteration 139/1000 | Loss: 0.00000812
Iteration 140/1000 | Loss: 0.00000811
Iteration 141/1000 | Loss: 0.00000811
Iteration 142/1000 | Loss: 0.00000811
Iteration 143/1000 | Loss: 0.00000811
Iteration 144/1000 | Loss: 0.00000811
Iteration 145/1000 | Loss: 0.00000811
Iteration 146/1000 | Loss: 0.00000811
Iteration 147/1000 | Loss: 0.00000811
Iteration 148/1000 | Loss: 0.00000811
Iteration 149/1000 | Loss: 0.00000811
Iteration 150/1000 | Loss: 0.00000811
Iteration 151/1000 | Loss: 0.00000810
Iteration 152/1000 | Loss: 0.00000810
Iteration 153/1000 | Loss: 0.00000810
Iteration 154/1000 | Loss: 0.00000810
Iteration 155/1000 | Loss: 0.00000810
Iteration 156/1000 | Loss: 0.00000810
Iteration 157/1000 | Loss: 0.00000810
Iteration 158/1000 | Loss: 0.00000809
Iteration 159/1000 | Loss: 0.00000809
Iteration 160/1000 | Loss: 0.00000809
Iteration 161/1000 | Loss: 0.00000808
Iteration 162/1000 | Loss: 0.00000808
Iteration 163/1000 | Loss: 0.00000808
Iteration 164/1000 | Loss: 0.00000808
Iteration 165/1000 | Loss: 0.00000808
Iteration 166/1000 | Loss: 0.00000808
Iteration 167/1000 | Loss: 0.00000808
Iteration 168/1000 | Loss: 0.00000808
Iteration 169/1000 | Loss: 0.00000808
Iteration 170/1000 | Loss: 0.00000807
Iteration 171/1000 | Loss: 0.00000807
Iteration 172/1000 | Loss: 0.00000807
Iteration 173/1000 | Loss: 0.00000807
Iteration 174/1000 | Loss: 0.00000807
Iteration 175/1000 | Loss: 0.00000806
Iteration 176/1000 | Loss: 0.00000806
Iteration 177/1000 | Loss: 0.00000806
Iteration 178/1000 | Loss: 0.00000806
Iteration 179/1000 | Loss: 0.00000806
Iteration 180/1000 | Loss: 0.00000806
Iteration 181/1000 | Loss: 0.00000806
Iteration 182/1000 | Loss: 0.00000806
Iteration 183/1000 | Loss: 0.00000806
Iteration 184/1000 | Loss: 0.00000806
Iteration 185/1000 | Loss: 0.00000806
Iteration 186/1000 | Loss: 0.00000806
Iteration 187/1000 | Loss: 0.00000806
Iteration 188/1000 | Loss: 0.00000806
Iteration 189/1000 | Loss: 0.00000806
Iteration 190/1000 | Loss: 0.00000806
Iteration 191/1000 | Loss: 0.00000806
Iteration 192/1000 | Loss: 0.00000805
Iteration 193/1000 | Loss: 0.00000805
Iteration 194/1000 | Loss: 0.00000805
Iteration 195/1000 | Loss: 0.00000805
Iteration 196/1000 | Loss: 0.00000805
Iteration 197/1000 | Loss: 0.00000805
Iteration 198/1000 | Loss: 0.00000805
Iteration 199/1000 | Loss: 0.00000805
Iteration 200/1000 | Loss: 0.00000805
Iteration 201/1000 | Loss: 0.00000804
Iteration 202/1000 | Loss: 0.00000804
Iteration 203/1000 | Loss: 0.00000804
Iteration 204/1000 | Loss: 0.00000804
Iteration 205/1000 | Loss: 0.00000804
Iteration 206/1000 | Loss: 0.00000804
Iteration 207/1000 | Loss: 0.00000804
Iteration 208/1000 | Loss: 0.00000804
Iteration 209/1000 | Loss: 0.00000803
Iteration 210/1000 | Loss: 0.00000803
Iteration 211/1000 | Loss: 0.00000803
Iteration 212/1000 | Loss: 0.00000803
Iteration 213/1000 | Loss: 0.00000803
Iteration 214/1000 | Loss: 0.00000803
Iteration 215/1000 | Loss: 0.00000803
Iteration 216/1000 | Loss: 0.00000803
Iteration 217/1000 | Loss: 0.00000803
Iteration 218/1000 | Loss: 0.00000803
Iteration 219/1000 | Loss: 0.00000803
Iteration 220/1000 | Loss: 0.00000803
Iteration 221/1000 | Loss: 0.00000803
Iteration 222/1000 | Loss: 0.00000803
Iteration 223/1000 | Loss: 0.00000803
Iteration 224/1000 | Loss: 0.00000803
Iteration 225/1000 | Loss: 0.00000803
Iteration 226/1000 | Loss: 0.00000803
Iteration 227/1000 | Loss: 0.00000802
Iteration 228/1000 | Loss: 0.00000802
Iteration 229/1000 | Loss: 0.00000802
Iteration 230/1000 | Loss: 0.00000802
Iteration 231/1000 | Loss: 0.00000802
Iteration 232/1000 | Loss: 0.00000802
Iteration 233/1000 | Loss: 0.00000802
Iteration 234/1000 | Loss: 0.00000802
Iteration 235/1000 | Loss: 0.00000802
Iteration 236/1000 | Loss: 0.00000802
Iteration 237/1000 | Loss: 0.00000802
Iteration 238/1000 | Loss: 0.00000802
Iteration 239/1000 | Loss: 0.00000802
Iteration 240/1000 | Loss: 0.00000802
Iteration 241/1000 | Loss: 0.00000802
Iteration 242/1000 | Loss: 0.00000802
Iteration 243/1000 | Loss: 0.00000802
Iteration 244/1000 | Loss: 0.00000802
Iteration 245/1000 | Loss: 0.00000802
Iteration 246/1000 | Loss: 0.00000801
Iteration 247/1000 | Loss: 0.00000801
Iteration 248/1000 | Loss: 0.00000801
Iteration 249/1000 | Loss: 0.00000801
Iteration 250/1000 | Loss: 0.00000801
Iteration 251/1000 | Loss: 0.00000801
Iteration 252/1000 | Loss: 0.00000801
Iteration 253/1000 | Loss: 0.00000801
Iteration 254/1000 | Loss: 0.00000801
Iteration 255/1000 | Loss: 0.00000801
Iteration 256/1000 | Loss: 0.00000801
Iteration 257/1000 | Loss: 0.00000801
Iteration 258/1000 | Loss: 0.00000801
Iteration 259/1000 | Loss: 0.00000801
Iteration 260/1000 | Loss: 0.00000801
Iteration 261/1000 | Loss: 0.00000801
Iteration 262/1000 | Loss: 0.00000801
Iteration 263/1000 | Loss: 0.00000801
Iteration 264/1000 | Loss: 0.00000801
Iteration 265/1000 | Loss: 0.00000801
Iteration 266/1000 | Loss: 0.00000801
Iteration 267/1000 | Loss: 0.00000801
Iteration 268/1000 | Loss: 0.00000801
Iteration 269/1000 | Loss: 0.00000801
Iteration 270/1000 | Loss: 0.00000801
Iteration 271/1000 | Loss: 0.00000801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [8.013324986677617e-06, 8.013324986677617e-06, 8.013324986677617e-06, 8.013324986677617e-06, 8.013324986677617e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.013324986677617e-06

Optimization complete. Final v2v error: 2.4083075523376465 mm

Highest mean error: 3.328730821609497 mm for frame 61

Lowest mean error: 2.2066121101379395 mm for frame 85

Saving results

Total time: 40.98328900337219
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867556
Iteration 2/25 | Loss: 0.00206367
Iteration 3/25 | Loss: 0.00187660
Iteration 4/25 | Loss: 0.00144465
Iteration 5/25 | Loss: 0.00145436
Iteration 6/25 | Loss: 0.00144386
Iteration 7/25 | Loss: 0.00140908
Iteration 8/25 | Loss: 0.00137535
Iteration 9/25 | Loss: 0.00136523
Iteration 10/25 | Loss: 0.00134509
Iteration 11/25 | Loss: 0.00134413
Iteration 12/25 | Loss: 0.00134401
Iteration 13/25 | Loss: 0.00134561
Iteration 14/25 | Loss: 0.00134297
Iteration 15/25 | Loss: 0.00133932
Iteration 16/25 | Loss: 0.00133926
Iteration 17/25 | Loss: 0.00133892
Iteration 18/25 | Loss: 0.00133146
Iteration 19/25 | Loss: 0.00132850
Iteration 20/25 | Loss: 0.00132961
Iteration 21/25 | Loss: 0.00132751
Iteration 22/25 | Loss: 0.00132387
Iteration 23/25 | Loss: 0.00132590
Iteration 24/25 | Loss: 0.00132418
Iteration 25/25 | Loss: 0.00132353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.79825592
Iteration 2/25 | Loss: 0.00299942
Iteration 3/25 | Loss: 0.00299942
Iteration 4/25 | Loss: 0.00299942
Iteration 5/25 | Loss: 0.00299942
Iteration 6/25 | Loss: 0.00299942
Iteration 7/25 | Loss: 0.00299942
Iteration 8/25 | Loss: 0.00299942
Iteration 9/25 | Loss: 0.00299942
Iteration 10/25 | Loss: 0.00299942
Iteration 11/25 | Loss: 0.00299942
Iteration 12/25 | Loss: 0.00299942
Iteration 13/25 | Loss: 0.00299942
Iteration 14/25 | Loss: 0.00299942
Iteration 15/25 | Loss: 0.00299942
Iteration 16/25 | Loss: 0.00299942
Iteration 17/25 | Loss: 0.00299942
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0029994212090969086, 0.0029994212090969086, 0.0029994212090969086, 0.0029994212090969086, 0.0029994212090969086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029994212090969086

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00299942
Iteration 2/1000 | Loss: 0.00041554
Iteration 3/1000 | Loss: 0.00129918
Iteration 4/1000 | Loss: 0.00360027
Iteration 5/1000 | Loss: 0.00374420
Iteration 6/1000 | Loss: 0.00327933
Iteration 7/1000 | Loss: 0.00277186
Iteration 8/1000 | Loss: 0.00119427
Iteration 9/1000 | Loss: 0.00064475
Iteration 10/1000 | Loss: 0.00019802
Iteration 11/1000 | Loss: 0.00104664
Iteration 12/1000 | Loss: 0.00070997
Iteration 13/1000 | Loss: 0.00008498
Iteration 14/1000 | Loss: 0.00057170
Iteration 15/1000 | Loss: 0.00005869
Iteration 16/1000 | Loss: 0.00021742
Iteration 17/1000 | Loss: 0.00017540
Iteration 18/1000 | Loss: 0.00023268
Iteration 19/1000 | Loss: 0.00022074
Iteration 20/1000 | Loss: 0.00021796
Iteration 21/1000 | Loss: 0.00017147
Iteration 22/1000 | Loss: 0.00089818
Iteration 23/1000 | Loss: 0.00036908
Iteration 24/1000 | Loss: 0.00020467
Iteration 25/1000 | Loss: 0.00078523
Iteration 26/1000 | Loss: 0.00014473
Iteration 27/1000 | Loss: 0.00021232
Iteration 28/1000 | Loss: 0.00005304
Iteration 29/1000 | Loss: 0.00013752
Iteration 30/1000 | Loss: 0.00010591
Iteration 31/1000 | Loss: 0.00011725
Iteration 32/1000 | Loss: 0.00004775
Iteration 33/1000 | Loss: 0.00003137
Iteration 34/1000 | Loss: 0.00005440
Iteration 35/1000 | Loss: 0.00002592
Iteration 36/1000 | Loss: 0.00003157
Iteration 37/1000 | Loss: 0.00003188
Iteration 38/1000 | Loss: 0.00003244
Iteration 39/1000 | Loss: 0.00002356
Iteration 40/1000 | Loss: 0.00003885
Iteration 41/1000 | Loss: 0.00002953
Iteration 42/1000 | Loss: 0.00002438
Iteration 43/1000 | Loss: 0.00002214
Iteration 44/1000 | Loss: 0.00002111
Iteration 45/1000 | Loss: 0.00044810
Iteration 46/1000 | Loss: 0.00049378
Iteration 47/1000 | Loss: 0.00032835
Iteration 48/1000 | Loss: 0.00016590
Iteration 49/1000 | Loss: 0.00040437
Iteration 50/1000 | Loss: 0.00031524
Iteration 51/1000 | Loss: 0.00004065
Iteration 52/1000 | Loss: 0.00005791
Iteration 53/1000 | Loss: 0.00017749
Iteration 54/1000 | Loss: 0.00018205
Iteration 55/1000 | Loss: 0.00029664
Iteration 56/1000 | Loss: 0.00029113
Iteration 57/1000 | Loss: 0.00027717
Iteration 58/1000 | Loss: 0.00014160
Iteration 59/1000 | Loss: 0.00014327
Iteration 60/1000 | Loss: 0.00013895
Iteration 61/1000 | Loss: 0.00023008
Iteration 62/1000 | Loss: 0.00003453
Iteration 63/1000 | Loss: 0.00005622
Iteration 64/1000 | Loss: 0.00009505
Iteration 65/1000 | Loss: 0.00011107
Iteration 66/1000 | Loss: 0.00002943
Iteration 67/1000 | Loss: 0.00002626
Iteration 68/1000 | Loss: 0.00002371
Iteration 69/1000 | Loss: 0.00026323
Iteration 70/1000 | Loss: 0.00013145
Iteration 71/1000 | Loss: 0.00007313
Iteration 72/1000 | Loss: 0.00013142
Iteration 73/1000 | Loss: 0.00009028
Iteration 74/1000 | Loss: 0.00010953
Iteration 75/1000 | Loss: 0.00009571
Iteration 76/1000 | Loss: 0.00012302
Iteration 77/1000 | Loss: 0.00002683
Iteration 78/1000 | Loss: 0.00018440
Iteration 79/1000 | Loss: 0.00011783
Iteration 80/1000 | Loss: 0.00018203
Iteration 81/1000 | Loss: 0.00017563
Iteration 82/1000 | Loss: 0.00018437
Iteration 83/1000 | Loss: 0.00011587
Iteration 84/1000 | Loss: 0.00015147
Iteration 85/1000 | Loss: 0.00021277
Iteration 86/1000 | Loss: 0.00003460
Iteration 87/1000 | Loss: 0.00019156
Iteration 88/1000 | Loss: 0.00022246
Iteration 89/1000 | Loss: 0.00019285
Iteration 90/1000 | Loss: 0.00020863
Iteration 91/1000 | Loss: 0.00018126
Iteration 92/1000 | Loss: 0.00010689
Iteration 93/1000 | Loss: 0.00001768
Iteration 94/1000 | Loss: 0.00001686
Iteration 95/1000 | Loss: 0.00030358
Iteration 96/1000 | Loss: 0.00031641
Iteration 97/1000 | Loss: 0.00002611
Iteration 98/1000 | Loss: 0.00001926
Iteration 99/1000 | Loss: 0.00001810
Iteration 100/1000 | Loss: 0.00001754
Iteration 101/1000 | Loss: 0.00020514
Iteration 102/1000 | Loss: 0.00044168
Iteration 103/1000 | Loss: 0.00024746
Iteration 104/1000 | Loss: 0.00030564
Iteration 105/1000 | Loss: 0.00047071
Iteration 106/1000 | Loss: 0.00002851
Iteration 107/1000 | Loss: 0.00002027
Iteration 108/1000 | Loss: 0.00001894
Iteration 109/1000 | Loss: 0.00001680
Iteration 110/1000 | Loss: 0.00001575
Iteration 111/1000 | Loss: 0.00001502
Iteration 112/1000 | Loss: 0.00001451
Iteration 113/1000 | Loss: 0.00001426
Iteration 114/1000 | Loss: 0.00001410
Iteration 115/1000 | Loss: 0.00001396
Iteration 116/1000 | Loss: 0.00001385
Iteration 117/1000 | Loss: 0.00001385
Iteration 118/1000 | Loss: 0.00001385
Iteration 119/1000 | Loss: 0.00001384
Iteration 120/1000 | Loss: 0.00001382
Iteration 121/1000 | Loss: 0.00001381
Iteration 122/1000 | Loss: 0.00001380
Iteration 123/1000 | Loss: 0.00001379
Iteration 124/1000 | Loss: 0.00001378
Iteration 125/1000 | Loss: 0.00001378
Iteration 126/1000 | Loss: 0.00001375
Iteration 127/1000 | Loss: 0.00001372
Iteration 128/1000 | Loss: 0.00001371
Iteration 129/1000 | Loss: 0.00001371
Iteration 130/1000 | Loss: 0.00001370
Iteration 131/1000 | Loss: 0.00001369
Iteration 132/1000 | Loss: 0.00001368
Iteration 133/1000 | Loss: 0.00001367
Iteration 134/1000 | Loss: 0.00001367
Iteration 135/1000 | Loss: 0.00001367
Iteration 136/1000 | Loss: 0.00001366
Iteration 137/1000 | Loss: 0.00001365
Iteration 138/1000 | Loss: 0.00001365
Iteration 139/1000 | Loss: 0.00001364
Iteration 140/1000 | Loss: 0.00001364
Iteration 141/1000 | Loss: 0.00001363
Iteration 142/1000 | Loss: 0.00001363
Iteration 143/1000 | Loss: 0.00001363
Iteration 144/1000 | Loss: 0.00001362
Iteration 145/1000 | Loss: 0.00001362
Iteration 146/1000 | Loss: 0.00001361
Iteration 147/1000 | Loss: 0.00001360
Iteration 148/1000 | Loss: 0.00001359
Iteration 149/1000 | Loss: 0.00001359
Iteration 150/1000 | Loss: 0.00001358
Iteration 151/1000 | Loss: 0.00001358
Iteration 152/1000 | Loss: 0.00001358
Iteration 153/1000 | Loss: 0.00001358
Iteration 154/1000 | Loss: 0.00001358
Iteration 155/1000 | Loss: 0.00001358
Iteration 156/1000 | Loss: 0.00001358
Iteration 157/1000 | Loss: 0.00001358
Iteration 158/1000 | Loss: 0.00001358
Iteration 159/1000 | Loss: 0.00001357
Iteration 160/1000 | Loss: 0.00001357
Iteration 161/1000 | Loss: 0.00001357
Iteration 162/1000 | Loss: 0.00001356
Iteration 163/1000 | Loss: 0.00001356
Iteration 164/1000 | Loss: 0.00001356
Iteration 165/1000 | Loss: 0.00001355
Iteration 166/1000 | Loss: 0.00001355
Iteration 167/1000 | Loss: 0.00001355
Iteration 168/1000 | Loss: 0.00001355
Iteration 169/1000 | Loss: 0.00001354
Iteration 170/1000 | Loss: 0.00001354
Iteration 171/1000 | Loss: 0.00001354
Iteration 172/1000 | Loss: 0.00001354
Iteration 173/1000 | Loss: 0.00001354
Iteration 174/1000 | Loss: 0.00001354
Iteration 175/1000 | Loss: 0.00001354
Iteration 176/1000 | Loss: 0.00001354
Iteration 177/1000 | Loss: 0.00001354
Iteration 178/1000 | Loss: 0.00001354
Iteration 179/1000 | Loss: 0.00001354
Iteration 180/1000 | Loss: 0.00001354
Iteration 181/1000 | Loss: 0.00001353
Iteration 182/1000 | Loss: 0.00001353
Iteration 183/1000 | Loss: 0.00001353
Iteration 184/1000 | Loss: 0.00001353
Iteration 185/1000 | Loss: 0.00001353
Iteration 186/1000 | Loss: 0.00001353
Iteration 187/1000 | Loss: 0.00001353
Iteration 188/1000 | Loss: 0.00001352
Iteration 189/1000 | Loss: 0.00001352
Iteration 190/1000 | Loss: 0.00001352
Iteration 191/1000 | Loss: 0.00001352
Iteration 192/1000 | Loss: 0.00001352
Iteration 193/1000 | Loss: 0.00001352
Iteration 194/1000 | Loss: 0.00001351
Iteration 195/1000 | Loss: 0.00001351
Iteration 196/1000 | Loss: 0.00001351
Iteration 197/1000 | Loss: 0.00001351
Iteration 198/1000 | Loss: 0.00001351
Iteration 199/1000 | Loss: 0.00001351
Iteration 200/1000 | Loss: 0.00001351
Iteration 201/1000 | Loss: 0.00001350
Iteration 202/1000 | Loss: 0.00001350
Iteration 203/1000 | Loss: 0.00001350
Iteration 204/1000 | Loss: 0.00001350
Iteration 205/1000 | Loss: 0.00001350
Iteration 206/1000 | Loss: 0.00001350
Iteration 207/1000 | Loss: 0.00001350
Iteration 208/1000 | Loss: 0.00001350
Iteration 209/1000 | Loss: 0.00001350
Iteration 210/1000 | Loss: 0.00001350
Iteration 211/1000 | Loss: 0.00001350
Iteration 212/1000 | Loss: 0.00001350
Iteration 213/1000 | Loss: 0.00001350
Iteration 214/1000 | Loss: 0.00001349
Iteration 215/1000 | Loss: 0.00001349
Iteration 216/1000 | Loss: 0.00001349
Iteration 217/1000 | Loss: 0.00001349
Iteration 218/1000 | Loss: 0.00001349
Iteration 219/1000 | Loss: 0.00001349
Iteration 220/1000 | Loss: 0.00001349
Iteration 221/1000 | Loss: 0.00001349
Iteration 222/1000 | Loss: 0.00001349
Iteration 223/1000 | Loss: 0.00001349
Iteration 224/1000 | Loss: 0.00001349
Iteration 225/1000 | Loss: 0.00001348
Iteration 226/1000 | Loss: 0.00001348
Iteration 227/1000 | Loss: 0.00001348
Iteration 228/1000 | Loss: 0.00001348
Iteration 229/1000 | Loss: 0.00001348
Iteration 230/1000 | Loss: 0.00001348
Iteration 231/1000 | Loss: 0.00001348
Iteration 232/1000 | Loss: 0.00001348
Iteration 233/1000 | Loss: 0.00001348
Iteration 234/1000 | Loss: 0.00001348
Iteration 235/1000 | Loss: 0.00001348
Iteration 236/1000 | Loss: 0.00001348
Iteration 237/1000 | Loss: 0.00001348
Iteration 238/1000 | Loss: 0.00001347
Iteration 239/1000 | Loss: 0.00001347
Iteration 240/1000 | Loss: 0.00001347
Iteration 241/1000 | Loss: 0.00001347
Iteration 242/1000 | Loss: 0.00001347
Iteration 243/1000 | Loss: 0.00001347
Iteration 244/1000 | Loss: 0.00001347
Iteration 245/1000 | Loss: 0.00001347
Iteration 246/1000 | Loss: 0.00001347
Iteration 247/1000 | Loss: 0.00001347
Iteration 248/1000 | Loss: 0.00001347
Iteration 249/1000 | Loss: 0.00001347
Iteration 250/1000 | Loss: 0.00001347
Iteration 251/1000 | Loss: 0.00001347
Iteration 252/1000 | Loss: 0.00001347
Iteration 253/1000 | Loss: 0.00001347
Iteration 254/1000 | Loss: 0.00001347
Iteration 255/1000 | Loss: 0.00001347
Iteration 256/1000 | Loss: 0.00001347
Iteration 257/1000 | Loss: 0.00001347
Iteration 258/1000 | Loss: 0.00001347
Iteration 259/1000 | Loss: 0.00001347
Iteration 260/1000 | Loss: 0.00001347
Iteration 261/1000 | Loss: 0.00001347
Iteration 262/1000 | Loss: 0.00001347
Iteration 263/1000 | Loss: 0.00001347
Iteration 264/1000 | Loss: 0.00001347
Iteration 265/1000 | Loss: 0.00001347
Iteration 266/1000 | Loss: 0.00001347
Iteration 267/1000 | Loss: 0.00001347
Iteration 268/1000 | Loss: 0.00001347
Iteration 269/1000 | Loss: 0.00001347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [1.3465260053635575e-05, 1.3465260053635575e-05, 1.3465260053635575e-05, 1.3465260053635575e-05, 1.3465260053635575e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3465260053635575e-05

Optimization complete. Final v2v error: 3.0241854190826416 mm

Highest mean error: 4.918522357940674 mm for frame 63

Lowest mean error: 2.344494104385376 mm for frame 115

Saving results

Total time: 229.1871852874756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435413
Iteration 2/25 | Loss: 0.00114962
Iteration 3/25 | Loss: 0.00108812
Iteration 4/25 | Loss: 0.00108292
Iteration 5/25 | Loss: 0.00108147
Iteration 6/25 | Loss: 0.00108147
Iteration 7/25 | Loss: 0.00108147
Iteration 8/25 | Loss: 0.00108147
Iteration 9/25 | Loss: 0.00108147
Iteration 10/25 | Loss: 0.00108147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010814687702804804, 0.0010814687702804804, 0.0010814687702804804, 0.0010814687702804804, 0.0010814687702804804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010814687702804804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01714015
Iteration 2/25 | Loss: 0.00049160
Iteration 3/25 | Loss: 0.00049160
Iteration 4/25 | Loss: 0.00049160
Iteration 5/25 | Loss: 0.00049160
Iteration 6/25 | Loss: 0.00049160
Iteration 7/25 | Loss: 0.00049160
Iteration 8/25 | Loss: 0.00049160
Iteration 9/25 | Loss: 0.00049160
Iteration 10/25 | Loss: 0.00049160
Iteration 11/25 | Loss: 0.00049160
Iteration 12/25 | Loss: 0.00049160
Iteration 13/25 | Loss: 0.00049160
Iteration 14/25 | Loss: 0.00049160
Iteration 15/25 | Loss: 0.00049160
Iteration 16/25 | Loss: 0.00049160
Iteration 17/25 | Loss: 0.00049160
Iteration 18/25 | Loss: 0.00049160
Iteration 19/25 | Loss: 0.00049160
Iteration 20/25 | Loss: 0.00049160
Iteration 21/25 | Loss: 0.00049160
Iteration 22/25 | Loss: 0.00049160
Iteration 23/25 | Loss: 0.00049160
Iteration 24/25 | Loss: 0.00049160
Iteration 25/25 | Loss: 0.00049160

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049160
Iteration 2/1000 | Loss: 0.00003559
Iteration 3/1000 | Loss: 0.00002101
Iteration 4/1000 | Loss: 0.00001731
Iteration 5/1000 | Loss: 0.00001608
Iteration 6/1000 | Loss: 0.00001527
Iteration 7/1000 | Loss: 0.00001456
Iteration 8/1000 | Loss: 0.00001423
Iteration 9/1000 | Loss: 0.00001386
Iteration 10/1000 | Loss: 0.00001377
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001375
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001344
Iteration 15/1000 | Loss: 0.00001322
Iteration 16/1000 | Loss: 0.00001317
Iteration 17/1000 | Loss: 0.00001301
Iteration 18/1000 | Loss: 0.00001288
Iteration 19/1000 | Loss: 0.00001287
Iteration 20/1000 | Loss: 0.00001287
Iteration 21/1000 | Loss: 0.00001286
Iteration 22/1000 | Loss: 0.00001286
Iteration 23/1000 | Loss: 0.00001285
Iteration 24/1000 | Loss: 0.00001285
Iteration 25/1000 | Loss: 0.00001285
Iteration 26/1000 | Loss: 0.00001284
Iteration 27/1000 | Loss: 0.00001284
Iteration 28/1000 | Loss: 0.00001284
Iteration 29/1000 | Loss: 0.00001283
Iteration 30/1000 | Loss: 0.00001283
Iteration 31/1000 | Loss: 0.00001283
Iteration 32/1000 | Loss: 0.00001282
Iteration 33/1000 | Loss: 0.00001282
Iteration 34/1000 | Loss: 0.00001282
Iteration 35/1000 | Loss: 0.00001282
Iteration 36/1000 | Loss: 0.00001282
Iteration 37/1000 | Loss: 0.00001279
Iteration 38/1000 | Loss: 0.00001278
Iteration 39/1000 | Loss: 0.00001278
Iteration 40/1000 | Loss: 0.00001278
Iteration 41/1000 | Loss: 0.00001278
Iteration 42/1000 | Loss: 0.00001278
Iteration 43/1000 | Loss: 0.00001278
Iteration 44/1000 | Loss: 0.00001278
Iteration 45/1000 | Loss: 0.00001278
Iteration 46/1000 | Loss: 0.00001278
Iteration 47/1000 | Loss: 0.00001278
Iteration 48/1000 | Loss: 0.00001278
Iteration 49/1000 | Loss: 0.00001277
Iteration 50/1000 | Loss: 0.00001277
Iteration 51/1000 | Loss: 0.00001277
Iteration 52/1000 | Loss: 0.00001277
Iteration 53/1000 | Loss: 0.00001277
Iteration 54/1000 | Loss: 0.00001277
Iteration 55/1000 | Loss: 0.00001277
Iteration 56/1000 | Loss: 0.00001277
Iteration 57/1000 | Loss: 0.00001277
Iteration 58/1000 | Loss: 0.00001277
Iteration 59/1000 | Loss: 0.00001277
Iteration 60/1000 | Loss: 0.00001277
Iteration 61/1000 | Loss: 0.00001277
Iteration 62/1000 | Loss: 0.00001277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.2774771676049568e-05, 1.2774771676049568e-05, 1.2774771676049568e-05, 1.2774771676049568e-05, 1.2774771676049568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2774771676049568e-05

Optimization complete. Final v2v error: 3.0627777576446533 mm

Highest mean error: 3.097334146499634 mm for frame 41

Lowest mean error: 3.0222620964050293 mm for frame 89

Saving results

Total time: 27.963716983795166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00340541
Iteration 2/25 | Loss: 0.00121756
Iteration 3/25 | Loss: 0.00105889
Iteration 4/25 | Loss: 0.00104753
Iteration 5/25 | Loss: 0.00104312
Iteration 6/25 | Loss: 0.00104170
Iteration 7/25 | Loss: 0.00104170
Iteration 8/25 | Loss: 0.00104170
Iteration 9/25 | Loss: 0.00104170
Iteration 10/25 | Loss: 0.00104170
Iteration 11/25 | Loss: 0.00104170
Iteration 12/25 | Loss: 0.00104170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010416969889774919, 0.0010416969889774919, 0.0010416969889774919, 0.0010416969889774919, 0.0010416969889774919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010416969889774919

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33796275
Iteration 2/25 | Loss: 0.00077530
Iteration 3/25 | Loss: 0.00077529
Iteration 4/25 | Loss: 0.00077529
Iteration 5/25 | Loss: 0.00077529
Iteration 6/25 | Loss: 0.00077529
Iteration 7/25 | Loss: 0.00077529
Iteration 8/25 | Loss: 0.00077529
Iteration 9/25 | Loss: 0.00077529
Iteration 10/25 | Loss: 0.00077529
Iteration 11/25 | Loss: 0.00077529
Iteration 12/25 | Loss: 0.00077529
Iteration 13/25 | Loss: 0.00077529
Iteration 14/25 | Loss: 0.00077529
Iteration 15/25 | Loss: 0.00077529
Iteration 16/25 | Loss: 0.00077529
Iteration 17/25 | Loss: 0.00077529
Iteration 18/25 | Loss: 0.00077529
Iteration 19/25 | Loss: 0.00077529
Iteration 20/25 | Loss: 0.00077529
Iteration 21/25 | Loss: 0.00077529
Iteration 22/25 | Loss: 0.00077529
Iteration 23/25 | Loss: 0.00077529
Iteration 24/25 | Loss: 0.00077529
Iteration 25/25 | Loss: 0.00077529

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077529
Iteration 2/1000 | Loss: 0.00003430
Iteration 3/1000 | Loss: 0.00002130
Iteration 4/1000 | Loss: 0.00001674
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001402
Iteration 7/1000 | Loss: 0.00001327
Iteration 8/1000 | Loss: 0.00001273
Iteration 9/1000 | Loss: 0.00001247
Iteration 10/1000 | Loss: 0.00001229
Iteration 11/1000 | Loss: 0.00001214
Iteration 12/1000 | Loss: 0.00001200
Iteration 13/1000 | Loss: 0.00001197
Iteration 14/1000 | Loss: 0.00001195
Iteration 15/1000 | Loss: 0.00001194
Iteration 16/1000 | Loss: 0.00001192
Iteration 17/1000 | Loss: 0.00001191
Iteration 18/1000 | Loss: 0.00001191
Iteration 19/1000 | Loss: 0.00001190
Iteration 20/1000 | Loss: 0.00001190
Iteration 21/1000 | Loss: 0.00001189
Iteration 22/1000 | Loss: 0.00001189
Iteration 23/1000 | Loss: 0.00001186
Iteration 24/1000 | Loss: 0.00001185
Iteration 25/1000 | Loss: 0.00001185
Iteration 26/1000 | Loss: 0.00001185
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001184
Iteration 29/1000 | Loss: 0.00001183
Iteration 30/1000 | Loss: 0.00001183
Iteration 31/1000 | Loss: 0.00001182
Iteration 32/1000 | Loss: 0.00001182
Iteration 33/1000 | Loss: 0.00001179
Iteration 34/1000 | Loss: 0.00001179
Iteration 35/1000 | Loss: 0.00001179
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001178
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00001177
Iteration 41/1000 | Loss: 0.00001177
Iteration 42/1000 | Loss: 0.00001174
Iteration 43/1000 | Loss: 0.00001173
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001172
Iteration 46/1000 | Loss: 0.00001172
Iteration 47/1000 | Loss: 0.00001172
Iteration 48/1000 | Loss: 0.00001172
Iteration 49/1000 | Loss: 0.00001171
Iteration 50/1000 | Loss: 0.00001171
Iteration 51/1000 | Loss: 0.00001170
Iteration 52/1000 | Loss: 0.00001170
Iteration 53/1000 | Loss: 0.00001169
Iteration 54/1000 | Loss: 0.00001169
Iteration 55/1000 | Loss: 0.00001168
Iteration 56/1000 | Loss: 0.00001167
Iteration 57/1000 | Loss: 0.00001167
Iteration 58/1000 | Loss: 0.00001167
Iteration 59/1000 | Loss: 0.00001166
Iteration 60/1000 | Loss: 0.00001166
Iteration 61/1000 | Loss: 0.00001166
Iteration 62/1000 | Loss: 0.00001165
Iteration 63/1000 | Loss: 0.00001165
Iteration 64/1000 | Loss: 0.00001165
Iteration 65/1000 | Loss: 0.00001164
Iteration 66/1000 | Loss: 0.00001164
Iteration 67/1000 | Loss: 0.00001164
Iteration 68/1000 | Loss: 0.00001163
Iteration 69/1000 | Loss: 0.00001162
Iteration 70/1000 | Loss: 0.00001162
Iteration 71/1000 | Loss: 0.00001161
Iteration 72/1000 | Loss: 0.00001161
Iteration 73/1000 | Loss: 0.00001161
Iteration 74/1000 | Loss: 0.00001161
Iteration 75/1000 | Loss: 0.00001161
Iteration 76/1000 | Loss: 0.00001160
Iteration 77/1000 | Loss: 0.00001160
Iteration 78/1000 | Loss: 0.00001160
Iteration 79/1000 | Loss: 0.00001160
Iteration 80/1000 | Loss: 0.00001160
Iteration 81/1000 | Loss: 0.00001159
Iteration 82/1000 | Loss: 0.00001159
Iteration 83/1000 | Loss: 0.00001159
Iteration 84/1000 | Loss: 0.00001158
Iteration 85/1000 | Loss: 0.00001158
Iteration 86/1000 | Loss: 0.00001158
Iteration 87/1000 | Loss: 0.00001157
Iteration 88/1000 | Loss: 0.00001157
Iteration 89/1000 | Loss: 0.00001157
Iteration 90/1000 | Loss: 0.00001157
Iteration 91/1000 | Loss: 0.00001157
Iteration 92/1000 | Loss: 0.00001157
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001156
Iteration 95/1000 | Loss: 0.00001156
Iteration 96/1000 | Loss: 0.00001156
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001156
Iteration 99/1000 | Loss: 0.00001156
Iteration 100/1000 | Loss: 0.00001156
Iteration 101/1000 | Loss: 0.00001156
Iteration 102/1000 | Loss: 0.00001156
Iteration 103/1000 | Loss: 0.00001156
Iteration 104/1000 | Loss: 0.00001156
Iteration 105/1000 | Loss: 0.00001155
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001155
Iteration 109/1000 | Loss: 0.00001154
Iteration 110/1000 | Loss: 0.00001154
Iteration 111/1000 | Loss: 0.00001154
Iteration 112/1000 | Loss: 0.00001152
Iteration 113/1000 | Loss: 0.00001151
Iteration 114/1000 | Loss: 0.00001151
Iteration 115/1000 | Loss: 0.00001151
Iteration 116/1000 | Loss: 0.00001150
Iteration 117/1000 | Loss: 0.00001150
Iteration 118/1000 | Loss: 0.00001149
Iteration 119/1000 | Loss: 0.00001149
Iteration 120/1000 | Loss: 0.00001149
Iteration 121/1000 | Loss: 0.00001148
Iteration 122/1000 | Loss: 0.00001148
Iteration 123/1000 | Loss: 0.00001148
Iteration 124/1000 | Loss: 0.00001148
Iteration 125/1000 | Loss: 0.00001147
Iteration 126/1000 | Loss: 0.00001147
Iteration 127/1000 | Loss: 0.00001147
Iteration 128/1000 | Loss: 0.00001147
Iteration 129/1000 | Loss: 0.00001147
Iteration 130/1000 | Loss: 0.00001147
Iteration 131/1000 | Loss: 0.00001147
Iteration 132/1000 | Loss: 0.00001147
Iteration 133/1000 | Loss: 0.00001147
Iteration 134/1000 | Loss: 0.00001147
Iteration 135/1000 | Loss: 0.00001147
Iteration 136/1000 | Loss: 0.00001147
Iteration 137/1000 | Loss: 0.00001147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.1473201084299944e-05, 1.1473201084299944e-05, 1.1473201084299944e-05, 1.1473201084299944e-05, 1.1473201084299944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1473201084299944e-05

Optimization complete. Final v2v error: 2.9016270637512207 mm

Highest mean error: 3.2817800045013428 mm for frame 18

Lowest mean error: 2.6020872592926025 mm for frame 182

Saving results

Total time: 38.85089302062988
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00927600
Iteration 2/25 | Loss: 0.00166800
Iteration 3/25 | Loss: 0.00127870
Iteration 4/25 | Loss: 0.00119774
Iteration 5/25 | Loss: 0.00117815
Iteration 6/25 | Loss: 0.00117294
Iteration 7/25 | Loss: 0.00116854
Iteration 8/25 | Loss: 0.00116680
Iteration 9/25 | Loss: 0.00116606
Iteration 10/25 | Loss: 0.00116565
Iteration 11/25 | Loss: 0.00116549
Iteration 12/25 | Loss: 0.00116537
Iteration 13/25 | Loss: 0.00116519
Iteration 14/25 | Loss: 0.00116508
Iteration 15/25 | Loss: 0.00116500
Iteration 16/25 | Loss: 0.00116498
Iteration 17/25 | Loss: 0.00116498
Iteration 18/25 | Loss: 0.00116498
Iteration 19/25 | Loss: 0.00116498
Iteration 20/25 | Loss: 0.00116498
Iteration 21/25 | Loss: 0.00116498
Iteration 22/25 | Loss: 0.00116498
Iteration 23/25 | Loss: 0.00116498
Iteration 24/25 | Loss: 0.00116498
Iteration 25/25 | Loss: 0.00116498

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34169340
Iteration 2/25 | Loss: 0.00179967
Iteration 3/25 | Loss: 0.00179965
Iteration 4/25 | Loss: 0.00179965
Iteration 5/25 | Loss: 0.00179965
Iteration 6/25 | Loss: 0.00179965
Iteration 7/25 | Loss: 0.00179965
Iteration 8/25 | Loss: 0.00179965
Iteration 9/25 | Loss: 0.00179965
Iteration 10/25 | Loss: 0.00179965
Iteration 11/25 | Loss: 0.00179965
Iteration 12/25 | Loss: 0.00179965
Iteration 13/25 | Loss: 0.00179965
Iteration 14/25 | Loss: 0.00179965
Iteration 15/25 | Loss: 0.00179965
Iteration 16/25 | Loss: 0.00179965
Iteration 17/25 | Loss: 0.00179965
Iteration 18/25 | Loss: 0.00179965
Iteration 19/25 | Loss: 0.00179965
Iteration 20/25 | Loss: 0.00179965
Iteration 21/25 | Loss: 0.00179965
Iteration 22/25 | Loss: 0.00179965
Iteration 23/25 | Loss: 0.00179965
Iteration 24/25 | Loss: 0.00179965
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0017996495589613914, 0.0017996495589613914, 0.0017996495589613914, 0.0017996495589613914, 0.0017996495589613914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017996495589613914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179965
Iteration 2/1000 | Loss: 0.00014320
Iteration 3/1000 | Loss: 0.00010469
Iteration 4/1000 | Loss: 0.00008821
Iteration 5/1000 | Loss: 0.00008293
Iteration 6/1000 | Loss: 0.00007885
Iteration 7/1000 | Loss: 0.00007660
Iteration 8/1000 | Loss: 0.00007488
Iteration 9/1000 | Loss: 0.00007313
Iteration 10/1000 | Loss: 0.00007204
Iteration 11/1000 | Loss: 0.00007111
Iteration 12/1000 | Loss: 0.00007034
Iteration 13/1000 | Loss: 0.00070546
Iteration 14/1000 | Loss: 0.00696167
Iteration 15/1000 | Loss: 0.00643750
Iteration 16/1000 | Loss: 0.00012402
Iteration 17/1000 | Loss: 0.00009526
Iteration 18/1000 | Loss: 0.00007818
Iteration 19/1000 | Loss: 0.00007241
Iteration 20/1000 | Loss: 0.00007041
Iteration 21/1000 | Loss: 0.00055289
Iteration 22/1000 | Loss: 0.00006987
Iteration 23/1000 | Loss: 0.00006876
Iteration 24/1000 | Loss: 0.00188921
Iteration 25/1000 | Loss: 0.00447299
Iteration 26/1000 | Loss: 0.00402687
Iteration 27/1000 | Loss: 0.00316349
Iteration 28/1000 | Loss: 0.00271248
Iteration 29/1000 | Loss: 0.00120462
Iteration 30/1000 | Loss: 0.00060058
Iteration 31/1000 | Loss: 0.00010168
Iteration 32/1000 | Loss: 0.00098254
Iteration 33/1000 | Loss: 0.00064915
Iteration 34/1000 | Loss: 0.00081742
Iteration 35/1000 | Loss: 0.00043687
Iteration 36/1000 | Loss: 0.00090864
Iteration 37/1000 | Loss: 0.00112156
Iteration 38/1000 | Loss: 0.00082094
Iteration 39/1000 | Loss: 0.00006289
Iteration 40/1000 | Loss: 0.00006589
Iteration 41/1000 | Loss: 0.00004809
Iteration 42/1000 | Loss: 0.00004211
Iteration 43/1000 | Loss: 0.00003856
Iteration 44/1000 | Loss: 0.00003542
Iteration 45/1000 | Loss: 0.00003294
Iteration 46/1000 | Loss: 0.00003053
Iteration 47/1000 | Loss: 0.00002833
Iteration 48/1000 | Loss: 0.00002657
Iteration 49/1000 | Loss: 0.00002545
Iteration 50/1000 | Loss: 0.00002490
Iteration 51/1000 | Loss: 0.00002448
Iteration 52/1000 | Loss: 0.00002410
Iteration 53/1000 | Loss: 0.00002379
Iteration 54/1000 | Loss: 0.00002356
Iteration 55/1000 | Loss: 0.00002354
Iteration 56/1000 | Loss: 0.00002352
Iteration 57/1000 | Loss: 0.00002338
Iteration 58/1000 | Loss: 0.00002332
Iteration 59/1000 | Loss: 0.00002330
Iteration 60/1000 | Loss: 0.00002329
Iteration 61/1000 | Loss: 0.00002321
Iteration 62/1000 | Loss: 0.00002312
Iteration 63/1000 | Loss: 0.00002307
Iteration 64/1000 | Loss: 0.00002305
Iteration 65/1000 | Loss: 0.00002303
Iteration 66/1000 | Loss: 0.00002303
Iteration 67/1000 | Loss: 0.00002296
Iteration 68/1000 | Loss: 0.00002290
Iteration 69/1000 | Loss: 0.00002289
Iteration 70/1000 | Loss: 0.00002289
Iteration 71/1000 | Loss: 0.00002289
Iteration 72/1000 | Loss: 0.00002289
Iteration 73/1000 | Loss: 0.00002289
Iteration 74/1000 | Loss: 0.00002287
Iteration 75/1000 | Loss: 0.00002287
Iteration 76/1000 | Loss: 0.00002286
Iteration 77/1000 | Loss: 0.00002285
Iteration 78/1000 | Loss: 0.00002283
Iteration 79/1000 | Loss: 0.00002282
Iteration 80/1000 | Loss: 0.00002282
Iteration 81/1000 | Loss: 0.00002282
Iteration 82/1000 | Loss: 0.00002282
Iteration 83/1000 | Loss: 0.00002281
Iteration 84/1000 | Loss: 0.00002281
Iteration 85/1000 | Loss: 0.00002281
Iteration 86/1000 | Loss: 0.00002280
Iteration 87/1000 | Loss: 0.00002280
Iteration 88/1000 | Loss: 0.00002280
Iteration 89/1000 | Loss: 0.00002279
Iteration 90/1000 | Loss: 0.00002279
Iteration 91/1000 | Loss: 0.00002278
Iteration 92/1000 | Loss: 0.00002277
Iteration 93/1000 | Loss: 0.00002277
Iteration 94/1000 | Loss: 0.00002277
Iteration 95/1000 | Loss: 0.00002276
Iteration 96/1000 | Loss: 0.00002276
Iteration 97/1000 | Loss: 0.00002276
Iteration 98/1000 | Loss: 0.00002276
Iteration 99/1000 | Loss: 0.00002276
Iteration 100/1000 | Loss: 0.00002276
Iteration 101/1000 | Loss: 0.00002275
Iteration 102/1000 | Loss: 0.00002275
Iteration 103/1000 | Loss: 0.00002275
Iteration 104/1000 | Loss: 0.00002275
Iteration 105/1000 | Loss: 0.00002275
Iteration 106/1000 | Loss: 0.00002275
Iteration 107/1000 | Loss: 0.00002274
Iteration 108/1000 | Loss: 0.00002274
Iteration 109/1000 | Loss: 0.00002274
Iteration 110/1000 | Loss: 0.00002273
Iteration 111/1000 | Loss: 0.00002273
Iteration 112/1000 | Loss: 0.00002273
Iteration 113/1000 | Loss: 0.00002273
Iteration 114/1000 | Loss: 0.00002273
Iteration 115/1000 | Loss: 0.00002272
Iteration 116/1000 | Loss: 0.00002272
Iteration 117/1000 | Loss: 0.00002272
Iteration 118/1000 | Loss: 0.00002271
Iteration 119/1000 | Loss: 0.00002271
Iteration 120/1000 | Loss: 0.00002271
Iteration 121/1000 | Loss: 0.00002270
Iteration 122/1000 | Loss: 0.00002270
Iteration 123/1000 | Loss: 0.00002270
Iteration 124/1000 | Loss: 0.00002270
Iteration 125/1000 | Loss: 0.00002270
Iteration 126/1000 | Loss: 0.00002270
Iteration 127/1000 | Loss: 0.00002270
Iteration 128/1000 | Loss: 0.00002269
Iteration 129/1000 | Loss: 0.00002269
Iteration 130/1000 | Loss: 0.00002268
Iteration 131/1000 | Loss: 0.00002268
Iteration 132/1000 | Loss: 0.00002268
Iteration 133/1000 | Loss: 0.00002268
Iteration 134/1000 | Loss: 0.00002268
Iteration 135/1000 | Loss: 0.00002268
Iteration 136/1000 | Loss: 0.00002267
Iteration 137/1000 | Loss: 0.00002267
Iteration 138/1000 | Loss: 0.00002267
Iteration 139/1000 | Loss: 0.00002267
Iteration 140/1000 | Loss: 0.00002267
Iteration 141/1000 | Loss: 0.00002267
Iteration 142/1000 | Loss: 0.00002267
Iteration 143/1000 | Loss: 0.00002267
Iteration 144/1000 | Loss: 0.00002266
Iteration 145/1000 | Loss: 0.00002266
Iteration 146/1000 | Loss: 0.00002266
Iteration 147/1000 | Loss: 0.00002265
Iteration 148/1000 | Loss: 0.00002265
Iteration 149/1000 | Loss: 0.00002265
Iteration 150/1000 | Loss: 0.00002265
Iteration 151/1000 | Loss: 0.00002265
Iteration 152/1000 | Loss: 0.00002265
Iteration 153/1000 | Loss: 0.00002265
Iteration 154/1000 | Loss: 0.00002265
Iteration 155/1000 | Loss: 0.00002264
Iteration 156/1000 | Loss: 0.00002264
Iteration 157/1000 | Loss: 0.00002264
Iteration 158/1000 | Loss: 0.00002264
Iteration 159/1000 | Loss: 0.00002264
Iteration 160/1000 | Loss: 0.00002264
Iteration 161/1000 | Loss: 0.00002264
Iteration 162/1000 | Loss: 0.00002264
Iteration 163/1000 | Loss: 0.00002264
Iteration 164/1000 | Loss: 0.00002264
Iteration 165/1000 | Loss: 0.00002264
Iteration 166/1000 | Loss: 0.00002264
Iteration 167/1000 | Loss: 0.00002264
Iteration 168/1000 | Loss: 0.00002264
Iteration 169/1000 | Loss: 0.00002264
Iteration 170/1000 | Loss: 0.00002264
Iteration 171/1000 | Loss: 0.00002264
Iteration 172/1000 | Loss: 0.00002264
Iteration 173/1000 | Loss: 0.00002263
Iteration 174/1000 | Loss: 0.00002263
Iteration 175/1000 | Loss: 0.00002263
Iteration 176/1000 | Loss: 0.00002263
Iteration 177/1000 | Loss: 0.00002263
Iteration 178/1000 | Loss: 0.00002263
Iteration 179/1000 | Loss: 0.00002263
Iteration 180/1000 | Loss: 0.00002263
Iteration 181/1000 | Loss: 0.00002263
Iteration 182/1000 | Loss: 0.00002263
Iteration 183/1000 | Loss: 0.00002263
Iteration 184/1000 | Loss: 0.00002263
Iteration 185/1000 | Loss: 0.00002263
Iteration 186/1000 | Loss: 0.00002262
Iteration 187/1000 | Loss: 0.00002262
Iteration 188/1000 | Loss: 0.00002262
Iteration 189/1000 | Loss: 0.00002262
Iteration 190/1000 | Loss: 0.00002262
Iteration 191/1000 | Loss: 0.00002262
Iteration 192/1000 | Loss: 0.00002262
Iteration 193/1000 | Loss: 0.00002262
Iteration 194/1000 | Loss: 0.00002262
Iteration 195/1000 | Loss: 0.00002262
Iteration 196/1000 | Loss: 0.00002262
Iteration 197/1000 | Loss: 0.00002262
Iteration 198/1000 | Loss: 0.00002262
Iteration 199/1000 | Loss: 0.00002262
Iteration 200/1000 | Loss: 0.00002262
Iteration 201/1000 | Loss: 0.00002262
Iteration 202/1000 | Loss: 0.00002262
Iteration 203/1000 | Loss: 0.00002262
Iteration 204/1000 | Loss: 0.00002262
Iteration 205/1000 | Loss: 0.00002261
Iteration 206/1000 | Loss: 0.00002261
Iteration 207/1000 | Loss: 0.00002261
Iteration 208/1000 | Loss: 0.00002261
Iteration 209/1000 | Loss: 0.00002261
Iteration 210/1000 | Loss: 0.00002261
Iteration 211/1000 | Loss: 0.00002261
Iteration 212/1000 | Loss: 0.00002261
Iteration 213/1000 | Loss: 0.00002261
Iteration 214/1000 | Loss: 0.00002261
Iteration 215/1000 | Loss: 0.00002261
Iteration 216/1000 | Loss: 0.00002260
Iteration 217/1000 | Loss: 0.00002260
Iteration 218/1000 | Loss: 0.00002260
Iteration 219/1000 | Loss: 0.00002260
Iteration 220/1000 | Loss: 0.00002260
Iteration 221/1000 | Loss: 0.00002260
Iteration 222/1000 | Loss: 0.00002260
Iteration 223/1000 | Loss: 0.00002260
Iteration 224/1000 | Loss: 0.00002260
Iteration 225/1000 | Loss: 0.00002259
Iteration 226/1000 | Loss: 0.00002259
Iteration 227/1000 | Loss: 0.00002259
Iteration 228/1000 | Loss: 0.00002259
Iteration 229/1000 | Loss: 0.00002259
Iteration 230/1000 | Loss: 0.00002259
Iteration 231/1000 | Loss: 0.00002259
Iteration 232/1000 | Loss: 0.00002259
Iteration 233/1000 | Loss: 0.00002259
Iteration 234/1000 | Loss: 0.00002258
Iteration 235/1000 | Loss: 0.00002258
Iteration 236/1000 | Loss: 0.00002258
Iteration 237/1000 | Loss: 0.00002258
Iteration 238/1000 | Loss: 0.00002258
Iteration 239/1000 | Loss: 0.00002258
Iteration 240/1000 | Loss: 0.00002258
Iteration 241/1000 | Loss: 0.00002258
Iteration 242/1000 | Loss: 0.00002258
Iteration 243/1000 | Loss: 0.00002258
Iteration 244/1000 | Loss: 0.00002258
Iteration 245/1000 | Loss: 0.00002258
Iteration 246/1000 | Loss: 0.00002257
Iteration 247/1000 | Loss: 0.00002257
Iteration 248/1000 | Loss: 0.00002257
Iteration 249/1000 | Loss: 0.00002257
Iteration 250/1000 | Loss: 0.00002257
Iteration 251/1000 | Loss: 0.00002257
Iteration 252/1000 | Loss: 0.00002257
Iteration 253/1000 | Loss: 0.00002257
Iteration 254/1000 | Loss: 0.00002257
Iteration 255/1000 | Loss: 0.00002257
Iteration 256/1000 | Loss: 0.00002257
Iteration 257/1000 | Loss: 0.00002257
Iteration 258/1000 | Loss: 0.00002257
Iteration 259/1000 | Loss: 0.00002256
Iteration 260/1000 | Loss: 0.00002256
Iteration 261/1000 | Loss: 0.00002256
Iteration 262/1000 | Loss: 0.00002256
Iteration 263/1000 | Loss: 0.00002256
Iteration 264/1000 | Loss: 0.00002256
Iteration 265/1000 | Loss: 0.00002256
Iteration 266/1000 | Loss: 0.00002256
Iteration 267/1000 | Loss: 0.00002256
Iteration 268/1000 | Loss: 0.00002256
Iteration 269/1000 | Loss: 0.00002256
Iteration 270/1000 | Loss: 0.00002256
Iteration 271/1000 | Loss: 0.00002256
Iteration 272/1000 | Loss: 0.00002256
Iteration 273/1000 | Loss: 0.00002256
Iteration 274/1000 | Loss: 0.00002256
Iteration 275/1000 | Loss: 0.00002255
Iteration 276/1000 | Loss: 0.00002255
Iteration 277/1000 | Loss: 0.00002255
Iteration 278/1000 | Loss: 0.00002255
Iteration 279/1000 | Loss: 0.00002255
Iteration 280/1000 | Loss: 0.00002255
Iteration 281/1000 | Loss: 0.00002255
Iteration 282/1000 | Loss: 0.00002255
Iteration 283/1000 | Loss: 0.00002255
Iteration 284/1000 | Loss: 0.00002255
Iteration 285/1000 | Loss: 0.00002254
Iteration 286/1000 | Loss: 0.00002254
Iteration 287/1000 | Loss: 0.00002254
Iteration 288/1000 | Loss: 0.00002254
Iteration 289/1000 | Loss: 0.00002254
Iteration 290/1000 | Loss: 0.00002254
Iteration 291/1000 | Loss: 0.00002254
Iteration 292/1000 | Loss: 0.00002254
Iteration 293/1000 | Loss: 0.00002254
Iteration 294/1000 | Loss: 0.00002254
Iteration 295/1000 | Loss: 0.00002254
Iteration 296/1000 | Loss: 0.00002254
Iteration 297/1000 | Loss: 0.00002254
Iteration 298/1000 | Loss: 0.00002253
Iteration 299/1000 | Loss: 0.00002253
Iteration 300/1000 | Loss: 0.00002253
Iteration 301/1000 | Loss: 0.00002253
Iteration 302/1000 | Loss: 0.00002253
Iteration 303/1000 | Loss: 0.00002253
Iteration 304/1000 | Loss: 0.00002253
Iteration 305/1000 | Loss: 0.00002253
Iteration 306/1000 | Loss: 0.00002253
Iteration 307/1000 | Loss: 0.00002253
Iteration 308/1000 | Loss: 0.00002253
Iteration 309/1000 | Loss: 0.00002253
Iteration 310/1000 | Loss: 0.00002253
Iteration 311/1000 | Loss: 0.00002253
Iteration 312/1000 | Loss: 0.00002253
Iteration 313/1000 | Loss: 0.00002253
Iteration 314/1000 | Loss: 0.00002253
Iteration 315/1000 | Loss: 0.00002253
Iteration 316/1000 | Loss: 0.00002253
Iteration 317/1000 | Loss: 0.00002252
Iteration 318/1000 | Loss: 0.00002252
Iteration 319/1000 | Loss: 0.00002252
Iteration 320/1000 | Loss: 0.00002252
Iteration 321/1000 | Loss: 0.00002252
Iteration 322/1000 | Loss: 0.00002252
Iteration 323/1000 | Loss: 0.00002252
Iteration 324/1000 | Loss: 0.00002252
Iteration 325/1000 | Loss: 0.00002252
Iteration 326/1000 | Loss: 0.00002252
Iteration 327/1000 | Loss: 0.00002252
Iteration 328/1000 | Loss: 0.00002252
Iteration 329/1000 | Loss: 0.00002252
Iteration 330/1000 | Loss: 0.00002252
Iteration 331/1000 | Loss: 0.00002252
Iteration 332/1000 | Loss: 0.00002252
Iteration 333/1000 | Loss: 0.00002252
Iteration 334/1000 | Loss: 0.00002252
Iteration 335/1000 | Loss: 0.00002252
Iteration 336/1000 | Loss: 0.00002252
Iteration 337/1000 | Loss: 0.00002252
Iteration 338/1000 | Loss: 0.00002252
Iteration 339/1000 | Loss: 0.00002252
Iteration 340/1000 | Loss: 0.00002252
Iteration 341/1000 | Loss: 0.00002251
Iteration 342/1000 | Loss: 0.00002251
Iteration 343/1000 | Loss: 0.00002251
Iteration 344/1000 | Loss: 0.00002251
Iteration 345/1000 | Loss: 0.00002251
Iteration 346/1000 | Loss: 0.00002251
Iteration 347/1000 | Loss: 0.00002251
Iteration 348/1000 | Loss: 0.00002251
Iteration 349/1000 | Loss: 0.00002251
Iteration 350/1000 | Loss: 0.00002251
Iteration 351/1000 | Loss: 0.00002251
Iteration 352/1000 | Loss: 0.00002251
Iteration 353/1000 | Loss: 0.00002251
Iteration 354/1000 | Loss: 0.00002251
Iteration 355/1000 | Loss: 0.00002251
Iteration 356/1000 | Loss: 0.00002251
Iteration 357/1000 | Loss: 0.00002251
Iteration 358/1000 | Loss: 0.00002251
Iteration 359/1000 | Loss: 0.00002251
Iteration 360/1000 | Loss: 0.00002251
Iteration 361/1000 | Loss: 0.00002251
Iteration 362/1000 | Loss: 0.00002251
Iteration 363/1000 | Loss: 0.00002251
Iteration 364/1000 | Loss: 0.00002251
Iteration 365/1000 | Loss: 0.00002251
Iteration 366/1000 | Loss: 0.00002251
Iteration 367/1000 | Loss: 0.00002251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 367. Stopping optimization.
Last 5 losses: [2.2512580471811816e-05, 2.2512580471811816e-05, 2.2512580471811816e-05, 2.2512580471811816e-05, 2.2512580471811816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2512580471811816e-05

Optimization complete. Final v2v error: 3.7822418212890625 mm

Highest mean error: 11.412504196166992 mm for frame 26

Lowest mean error: 3.496332883834839 mm for frame 151

Saving results

Total time: 128.52847599983215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00712241
Iteration 2/25 | Loss: 0.00163269
Iteration 3/25 | Loss: 0.00130047
Iteration 4/25 | Loss: 0.00126121
Iteration 5/25 | Loss: 0.00125412
Iteration 6/25 | Loss: 0.00125335
Iteration 7/25 | Loss: 0.00125335
Iteration 8/25 | Loss: 0.00125335
Iteration 9/25 | Loss: 0.00125335
Iteration 10/25 | Loss: 0.00125335
Iteration 11/25 | Loss: 0.00125335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012533464469015598, 0.0012533464469015598, 0.0012533464469015598, 0.0012533464469015598, 0.0012533464469015598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012533464469015598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46570373
Iteration 2/25 | Loss: 0.00092007
Iteration 3/25 | Loss: 0.00092007
Iteration 4/25 | Loss: 0.00092007
Iteration 5/25 | Loss: 0.00092007
Iteration 6/25 | Loss: 0.00092007
Iteration 7/25 | Loss: 0.00092007
Iteration 8/25 | Loss: 0.00092007
Iteration 9/25 | Loss: 0.00092007
Iteration 10/25 | Loss: 0.00092007
Iteration 11/25 | Loss: 0.00092007
Iteration 12/25 | Loss: 0.00092007
Iteration 13/25 | Loss: 0.00092007
Iteration 14/25 | Loss: 0.00092007
Iteration 15/25 | Loss: 0.00092007
Iteration 16/25 | Loss: 0.00092007
Iteration 17/25 | Loss: 0.00092007
Iteration 18/25 | Loss: 0.00092007
Iteration 19/25 | Loss: 0.00092007
Iteration 20/25 | Loss: 0.00092007
Iteration 21/25 | Loss: 0.00092007
Iteration 22/25 | Loss: 0.00092007
Iteration 23/25 | Loss: 0.00092007
Iteration 24/25 | Loss: 0.00092007
Iteration 25/25 | Loss: 0.00092007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092007
Iteration 2/1000 | Loss: 0.00005877
Iteration 3/1000 | Loss: 0.00003718
Iteration 4/1000 | Loss: 0.00002974
Iteration 5/1000 | Loss: 0.00002736
Iteration 6/1000 | Loss: 0.00002589
Iteration 7/1000 | Loss: 0.00002503
Iteration 8/1000 | Loss: 0.00002442
Iteration 9/1000 | Loss: 0.00002405
Iteration 10/1000 | Loss: 0.00002377
Iteration 11/1000 | Loss: 0.00002351
Iteration 12/1000 | Loss: 0.00002330
Iteration 13/1000 | Loss: 0.00002314
Iteration 14/1000 | Loss: 0.00002313
Iteration 15/1000 | Loss: 0.00002311
Iteration 16/1000 | Loss: 0.00002310
Iteration 17/1000 | Loss: 0.00002308
Iteration 18/1000 | Loss: 0.00002307
Iteration 19/1000 | Loss: 0.00002305
Iteration 20/1000 | Loss: 0.00002304
Iteration 21/1000 | Loss: 0.00002304
Iteration 22/1000 | Loss: 0.00002303
Iteration 23/1000 | Loss: 0.00002303
Iteration 24/1000 | Loss: 0.00002302
Iteration 25/1000 | Loss: 0.00002301
Iteration 26/1000 | Loss: 0.00002300
Iteration 27/1000 | Loss: 0.00002300
Iteration 28/1000 | Loss: 0.00002299
Iteration 29/1000 | Loss: 0.00002294
Iteration 30/1000 | Loss: 0.00002294
Iteration 31/1000 | Loss: 0.00002293
Iteration 32/1000 | Loss: 0.00002293
Iteration 33/1000 | Loss: 0.00002293
Iteration 34/1000 | Loss: 0.00002293
Iteration 35/1000 | Loss: 0.00002293
Iteration 36/1000 | Loss: 0.00002292
Iteration 37/1000 | Loss: 0.00002292
Iteration 38/1000 | Loss: 0.00002292
Iteration 39/1000 | Loss: 0.00002291
Iteration 40/1000 | Loss: 0.00002291
Iteration 41/1000 | Loss: 0.00002291
Iteration 42/1000 | Loss: 0.00002291
Iteration 43/1000 | Loss: 0.00002291
Iteration 44/1000 | Loss: 0.00002290
Iteration 45/1000 | Loss: 0.00002290
Iteration 46/1000 | Loss: 0.00002290
Iteration 47/1000 | Loss: 0.00002289
Iteration 48/1000 | Loss: 0.00002289
Iteration 49/1000 | Loss: 0.00002289
Iteration 50/1000 | Loss: 0.00002289
Iteration 51/1000 | Loss: 0.00002288
Iteration 52/1000 | Loss: 0.00002288
Iteration 53/1000 | Loss: 0.00002288
Iteration 54/1000 | Loss: 0.00002288
Iteration 55/1000 | Loss: 0.00002288
Iteration 56/1000 | Loss: 0.00002288
Iteration 57/1000 | Loss: 0.00002288
Iteration 58/1000 | Loss: 0.00002288
Iteration 59/1000 | Loss: 0.00002288
Iteration 60/1000 | Loss: 0.00002288
Iteration 61/1000 | Loss: 0.00002287
Iteration 62/1000 | Loss: 0.00002287
Iteration 63/1000 | Loss: 0.00002287
Iteration 64/1000 | Loss: 0.00002287
Iteration 65/1000 | Loss: 0.00002287
Iteration 66/1000 | Loss: 0.00002287
Iteration 67/1000 | Loss: 0.00002287
Iteration 68/1000 | Loss: 0.00002287
Iteration 69/1000 | Loss: 0.00002287
Iteration 70/1000 | Loss: 0.00002287
Iteration 71/1000 | Loss: 0.00002286
Iteration 72/1000 | Loss: 0.00002286
Iteration 73/1000 | Loss: 0.00002286
Iteration 74/1000 | Loss: 0.00002286
Iteration 75/1000 | Loss: 0.00002286
Iteration 76/1000 | Loss: 0.00002286
Iteration 77/1000 | Loss: 0.00002286
Iteration 78/1000 | Loss: 0.00002286
Iteration 79/1000 | Loss: 0.00002286
Iteration 80/1000 | Loss: 0.00002286
Iteration 81/1000 | Loss: 0.00002286
Iteration 82/1000 | Loss: 0.00002286
Iteration 83/1000 | Loss: 0.00002286
Iteration 84/1000 | Loss: 0.00002286
Iteration 85/1000 | Loss: 0.00002286
Iteration 86/1000 | Loss: 0.00002286
Iteration 87/1000 | Loss: 0.00002286
Iteration 88/1000 | Loss: 0.00002286
Iteration 89/1000 | Loss: 0.00002286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [2.285573646076955e-05, 2.285573646076955e-05, 2.285573646076955e-05, 2.285573646076955e-05, 2.285573646076955e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.285573646076955e-05

Optimization complete. Final v2v error: 3.990795373916626 mm

Highest mean error: 4.61753511428833 mm for frame 51

Lowest mean error: 3.53210711479187 mm for frame 0

Saving results

Total time: 37.237685441970825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020804
Iteration 2/25 | Loss: 0.00201303
Iteration 3/25 | Loss: 0.00151918
Iteration 4/25 | Loss: 0.00141401
Iteration 5/25 | Loss: 0.00142423
Iteration 6/25 | Loss: 0.00133459
Iteration 7/25 | Loss: 0.00131836
Iteration 8/25 | Loss: 0.00123266
Iteration 9/25 | Loss: 0.00125646
Iteration 10/25 | Loss: 0.00118384
Iteration 11/25 | Loss: 0.00118615
Iteration 12/25 | Loss: 0.00123164
Iteration 13/25 | Loss: 0.00115865
Iteration 14/25 | Loss: 0.00115533
Iteration 15/25 | Loss: 0.00115333
Iteration 16/25 | Loss: 0.00114369
Iteration 17/25 | Loss: 0.00114103
Iteration 18/25 | Loss: 0.00114068
Iteration 19/25 | Loss: 0.00112994
Iteration 20/25 | Loss: 0.00113227
Iteration 21/25 | Loss: 0.00113132
Iteration 22/25 | Loss: 0.00113684
Iteration 23/25 | Loss: 0.00114216
Iteration 24/25 | Loss: 0.00114340
Iteration 25/25 | Loss: 0.00114183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45993972
Iteration 2/25 | Loss: 0.00172921
Iteration 3/25 | Loss: 0.00172921
Iteration 4/25 | Loss: 0.00172921
Iteration 5/25 | Loss: 0.00172921
Iteration 6/25 | Loss: 0.00172921
Iteration 7/25 | Loss: 0.00172921
Iteration 8/25 | Loss: 0.00172921
Iteration 9/25 | Loss: 0.00172921
Iteration 10/25 | Loss: 0.00172921
Iteration 11/25 | Loss: 0.00172921
Iteration 12/25 | Loss: 0.00172921
Iteration 13/25 | Loss: 0.00172921
Iteration 14/25 | Loss: 0.00172921
Iteration 15/25 | Loss: 0.00172921
Iteration 16/25 | Loss: 0.00172921
Iteration 17/25 | Loss: 0.00172921
Iteration 18/25 | Loss: 0.00172921
Iteration 19/25 | Loss: 0.00172921
Iteration 20/25 | Loss: 0.00172921
Iteration 21/25 | Loss: 0.00172921
Iteration 22/25 | Loss: 0.00172921
Iteration 23/25 | Loss: 0.00172921
Iteration 24/25 | Loss: 0.00172921
Iteration 25/25 | Loss: 0.00172921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172921
Iteration 2/1000 | Loss: 0.00073907
Iteration 3/1000 | Loss: 0.00059429
Iteration 4/1000 | Loss: 0.00055689
Iteration 5/1000 | Loss: 0.00313581
Iteration 6/1000 | Loss: 0.00058726
Iteration 7/1000 | Loss: 0.00055994
Iteration 8/1000 | Loss: 0.00058731
Iteration 9/1000 | Loss: 0.00043813
Iteration 10/1000 | Loss: 0.00051879
Iteration 11/1000 | Loss: 0.00058728
Iteration 12/1000 | Loss: 0.00064829
Iteration 13/1000 | Loss: 0.00027850
Iteration 14/1000 | Loss: 0.00027088
Iteration 15/1000 | Loss: 0.00053863
Iteration 16/1000 | Loss: 0.00047470
Iteration 17/1000 | Loss: 0.00059611
Iteration 18/1000 | Loss: 0.00042621
Iteration 19/1000 | Loss: 0.00035837
Iteration 20/1000 | Loss: 0.00054106
Iteration 21/1000 | Loss: 0.00052689
Iteration 22/1000 | Loss: 0.00034476
Iteration 23/1000 | Loss: 0.00037271
Iteration 24/1000 | Loss: 0.00038189
Iteration 25/1000 | Loss: 0.00049552
Iteration 26/1000 | Loss: 0.00050550
Iteration 27/1000 | Loss: 0.00054770
Iteration 28/1000 | Loss: 0.00037446
Iteration 29/1000 | Loss: 0.00043485
Iteration 30/1000 | Loss: 0.00064773
Iteration 31/1000 | Loss: 0.00045318
Iteration 32/1000 | Loss: 0.00034454
Iteration 33/1000 | Loss: 0.00074384
Iteration 34/1000 | Loss: 0.00117494
Iteration 35/1000 | Loss: 0.00187483
Iteration 36/1000 | Loss: 0.00211889
Iteration 37/1000 | Loss: 0.00055240
Iteration 38/1000 | Loss: 0.00061274
Iteration 39/1000 | Loss: 0.00039171
Iteration 40/1000 | Loss: 0.00042704
Iteration 41/1000 | Loss: 0.00043442
Iteration 42/1000 | Loss: 0.00058980
Iteration 43/1000 | Loss: 0.00032833
Iteration 44/1000 | Loss: 0.00048088
Iteration 45/1000 | Loss: 0.00037730
Iteration 46/1000 | Loss: 0.00031609
Iteration 47/1000 | Loss: 0.00026983
Iteration 48/1000 | Loss: 0.00036304
Iteration 49/1000 | Loss: 0.00115897
Iteration 50/1000 | Loss: 0.00105698
Iteration 51/1000 | Loss: 0.00058143
Iteration 52/1000 | Loss: 0.00029971
Iteration 53/1000 | Loss: 0.00034591
Iteration 54/1000 | Loss: 0.00053312
Iteration 55/1000 | Loss: 0.00015937
Iteration 56/1000 | Loss: 0.00029757
Iteration 57/1000 | Loss: 0.00026946
Iteration 58/1000 | Loss: 0.00029632
Iteration 59/1000 | Loss: 0.00022316
Iteration 60/1000 | Loss: 0.00022749
Iteration 61/1000 | Loss: 0.00045698
Iteration 62/1000 | Loss: 0.00019636
Iteration 63/1000 | Loss: 0.00024087
Iteration 64/1000 | Loss: 0.00040389
Iteration 65/1000 | Loss: 0.00049783
Iteration 66/1000 | Loss: 0.00022433
Iteration 67/1000 | Loss: 0.00019685
Iteration 68/1000 | Loss: 0.00029172
Iteration 69/1000 | Loss: 0.00040047
Iteration 70/1000 | Loss: 0.00050002
Iteration 71/1000 | Loss: 0.00035174
Iteration 72/1000 | Loss: 0.00023432
Iteration 73/1000 | Loss: 0.00013541
Iteration 74/1000 | Loss: 0.00026126
Iteration 75/1000 | Loss: 0.00034713
Iteration 76/1000 | Loss: 0.00031366
Iteration 77/1000 | Loss: 0.00023821
Iteration 78/1000 | Loss: 0.00022859
Iteration 79/1000 | Loss: 0.00024895
Iteration 80/1000 | Loss: 0.00041020
Iteration 81/1000 | Loss: 0.00024426
Iteration 82/1000 | Loss: 0.00034489
Iteration 83/1000 | Loss: 0.00028948
Iteration 84/1000 | Loss: 0.00019160
Iteration 85/1000 | Loss: 0.00018579
Iteration 86/1000 | Loss: 0.00022820
Iteration 87/1000 | Loss: 0.00027376
Iteration 88/1000 | Loss: 0.00037669
Iteration 89/1000 | Loss: 0.00032339
Iteration 90/1000 | Loss: 0.00036983
Iteration 91/1000 | Loss: 0.00056215
Iteration 92/1000 | Loss: 0.00064168
Iteration 93/1000 | Loss: 0.00037200
Iteration 94/1000 | Loss: 0.00035430
Iteration 95/1000 | Loss: 0.00035442
Iteration 96/1000 | Loss: 0.00026977
Iteration 97/1000 | Loss: 0.00021145
Iteration 98/1000 | Loss: 0.00011922
Iteration 99/1000 | Loss: 0.00049991
Iteration 100/1000 | Loss: 0.00023419
Iteration 101/1000 | Loss: 0.00033658
Iteration 102/1000 | Loss: 0.00025545
Iteration 103/1000 | Loss: 0.00028736
Iteration 104/1000 | Loss: 0.00019620
Iteration 105/1000 | Loss: 0.00008626
Iteration 106/1000 | Loss: 0.00009353
Iteration 107/1000 | Loss: 0.00013684
Iteration 108/1000 | Loss: 0.00083288
Iteration 109/1000 | Loss: 0.00042076
Iteration 110/1000 | Loss: 0.00008131
Iteration 111/1000 | Loss: 0.00041237
Iteration 112/1000 | Loss: 0.00030033
Iteration 113/1000 | Loss: 0.00013948
Iteration 114/1000 | Loss: 0.00105304
Iteration 115/1000 | Loss: 0.00111968
Iteration 116/1000 | Loss: 0.00131582
Iteration 117/1000 | Loss: 0.00029775
Iteration 118/1000 | Loss: 0.00085014
Iteration 119/1000 | Loss: 0.00064386
Iteration 120/1000 | Loss: 0.00114989
Iteration 121/1000 | Loss: 0.00105787
Iteration 122/1000 | Loss: 0.00056837
Iteration 123/1000 | Loss: 0.00068283
Iteration 124/1000 | Loss: 0.00061673
Iteration 125/1000 | Loss: 0.00075285
Iteration 126/1000 | Loss: 0.00066606
Iteration 127/1000 | Loss: 0.00021767
Iteration 128/1000 | Loss: 0.00019250
Iteration 129/1000 | Loss: 0.00034858
Iteration 130/1000 | Loss: 0.00047681
Iteration 131/1000 | Loss: 0.00024711
Iteration 132/1000 | Loss: 0.00051909
Iteration 133/1000 | Loss: 0.00071299
Iteration 134/1000 | Loss: 0.00063588
Iteration 135/1000 | Loss: 0.00087243
Iteration 136/1000 | Loss: 0.00079260
Iteration 137/1000 | Loss: 0.00061616
Iteration 138/1000 | Loss: 0.00042573
Iteration 139/1000 | Loss: 0.00019403
Iteration 140/1000 | Loss: 0.00016176
Iteration 141/1000 | Loss: 0.00014877
Iteration 142/1000 | Loss: 0.00024356
Iteration 143/1000 | Loss: 0.00033312
Iteration 144/1000 | Loss: 0.00017310
Iteration 145/1000 | Loss: 0.00017052
Iteration 146/1000 | Loss: 0.00029144
Iteration 147/1000 | Loss: 0.00016166
Iteration 148/1000 | Loss: 0.00019247
Iteration 149/1000 | Loss: 0.00037812
Iteration 150/1000 | Loss: 0.00029874
Iteration 151/1000 | Loss: 0.00035990
Iteration 152/1000 | Loss: 0.00032507
Iteration 153/1000 | Loss: 0.00023697
Iteration 154/1000 | Loss: 0.00024404
Iteration 155/1000 | Loss: 0.00033410
Iteration 156/1000 | Loss: 0.00040091
Iteration 157/1000 | Loss: 0.00036818
Iteration 158/1000 | Loss: 0.00023360
Iteration 159/1000 | Loss: 0.00103845
Iteration 160/1000 | Loss: 0.00039266
Iteration 161/1000 | Loss: 0.00054111
Iteration 162/1000 | Loss: 0.00032888
Iteration 163/1000 | Loss: 0.00037795
Iteration 164/1000 | Loss: 0.00035592
Iteration 165/1000 | Loss: 0.00042638
Iteration 166/1000 | Loss: 0.00032163
Iteration 167/1000 | Loss: 0.00012189
Iteration 168/1000 | Loss: 0.00018160
Iteration 169/1000 | Loss: 0.00029355
Iteration 170/1000 | Loss: 0.00028602
Iteration 171/1000 | Loss: 0.00041353
Iteration 172/1000 | Loss: 0.00035808
Iteration 173/1000 | Loss: 0.00033115
Iteration 174/1000 | Loss: 0.00033403
Iteration 175/1000 | Loss: 0.00026534
Iteration 176/1000 | Loss: 0.00036928
Iteration 177/1000 | Loss: 0.00042896
Iteration 178/1000 | Loss: 0.00028751
Iteration 179/1000 | Loss: 0.00027618
Iteration 180/1000 | Loss: 0.00018822
Iteration 181/1000 | Loss: 0.00032522
Iteration 182/1000 | Loss: 0.00037387
Iteration 183/1000 | Loss: 0.00040013
Iteration 184/1000 | Loss: 0.00028059
Iteration 185/1000 | Loss: 0.00025196
Iteration 186/1000 | Loss: 0.00013472
Iteration 187/1000 | Loss: 0.00014507
Iteration 188/1000 | Loss: 0.00049528
Iteration 189/1000 | Loss: 0.00021550
Iteration 190/1000 | Loss: 0.00031174
Iteration 191/1000 | Loss: 0.00039102
Iteration 192/1000 | Loss: 0.00041627
Iteration 193/1000 | Loss: 0.00033010
Iteration 194/1000 | Loss: 0.00045514
Iteration 195/1000 | Loss: 0.00028004
Iteration 196/1000 | Loss: 0.00029932
Iteration 197/1000 | Loss: 0.00023575
Iteration 198/1000 | Loss: 0.00020469
Iteration 199/1000 | Loss: 0.00012196
Iteration 200/1000 | Loss: 0.00009735
Iteration 201/1000 | Loss: 0.00028044
Iteration 202/1000 | Loss: 0.00130721
Iteration 203/1000 | Loss: 0.00050570
Iteration 204/1000 | Loss: 0.00025314
Iteration 205/1000 | Loss: 0.00012969
Iteration 206/1000 | Loss: 0.00011577
Iteration 207/1000 | Loss: 0.00047234
Iteration 208/1000 | Loss: 0.00051609
Iteration 209/1000 | Loss: 0.00025430
Iteration 210/1000 | Loss: 0.00093795
Iteration 211/1000 | Loss: 0.00030649
Iteration 212/1000 | Loss: 0.00019164
Iteration 213/1000 | Loss: 0.00016629
Iteration 214/1000 | Loss: 0.00013701
Iteration 215/1000 | Loss: 0.00043419
Iteration 216/1000 | Loss: 0.00108261
Iteration 217/1000 | Loss: 0.00053633
Iteration 218/1000 | Loss: 0.00041456
Iteration 219/1000 | Loss: 0.00062722
Iteration 220/1000 | Loss: 0.00051002
Iteration 221/1000 | Loss: 0.00076874
Iteration 222/1000 | Loss: 0.00030840
Iteration 223/1000 | Loss: 0.00031850
Iteration 224/1000 | Loss: 0.00027369
Iteration 225/1000 | Loss: 0.00021992
Iteration 226/1000 | Loss: 0.00036014
Iteration 227/1000 | Loss: 0.00035177
Iteration 228/1000 | Loss: 0.00023704
Iteration 229/1000 | Loss: 0.00021061
Iteration 230/1000 | Loss: 0.00043818
Iteration 231/1000 | Loss: 0.00042506
Iteration 232/1000 | Loss: 0.00033956
Iteration 233/1000 | Loss: 0.00053791
Iteration 234/1000 | Loss: 0.00016486
Iteration 235/1000 | Loss: 0.00034245
Iteration 236/1000 | Loss: 0.00031617
Iteration 237/1000 | Loss: 0.00040051
Iteration 238/1000 | Loss: 0.00034147
Iteration 239/1000 | Loss: 0.00038669
Iteration 240/1000 | Loss: 0.00041372
Iteration 241/1000 | Loss: 0.00038100
Iteration 242/1000 | Loss: 0.00020653
Iteration 243/1000 | Loss: 0.00012218
Iteration 244/1000 | Loss: 0.00053569
Iteration 245/1000 | Loss: 0.00023966
Iteration 246/1000 | Loss: 0.00029971
Iteration 247/1000 | Loss: 0.00063029
Iteration 248/1000 | Loss: 0.00042970
Iteration 249/1000 | Loss: 0.00018300
Iteration 250/1000 | Loss: 0.00024960
Iteration 251/1000 | Loss: 0.00030272
Iteration 252/1000 | Loss: 0.00025252
Iteration 253/1000 | Loss: 0.00032558
Iteration 254/1000 | Loss: 0.00040758
Iteration 255/1000 | Loss: 0.00027014
Iteration 256/1000 | Loss: 0.00014294
Iteration 257/1000 | Loss: 0.00009318
Iteration 258/1000 | Loss: 0.00025242
Iteration 259/1000 | Loss: 0.00010887
Iteration 260/1000 | Loss: 0.00052659
Iteration 261/1000 | Loss: 0.00008047
Iteration 262/1000 | Loss: 0.00047432
Iteration 263/1000 | Loss: 0.00028748
Iteration 264/1000 | Loss: 0.00022677
Iteration 265/1000 | Loss: 0.00013719
Iteration 266/1000 | Loss: 0.00020021
Iteration 267/1000 | Loss: 0.00037735
Iteration 268/1000 | Loss: 0.00045184
Iteration 269/1000 | Loss: 0.00067406
Iteration 270/1000 | Loss: 0.00029152
Iteration 271/1000 | Loss: 0.00022414
Iteration 272/1000 | Loss: 0.00021626
Iteration 273/1000 | Loss: 0.00004084
Iteration 274/1000 | Loss: 0.00008104
Iteration 275/1000 | Loss: 0.00016109
Iteration 276/1000 | Loss: 0.00015449
Iteration 277/1000 | Loss: 0.00082693
Iteration 278/1000 | Loss: 0.00021930
Iteration 279/1000 | Loss: 0.00008991
Iteration 280/1000 | Loss: 0.00003964
Iteration 281/1000 | Loss: 0.00009571
Iteration 282/1000 | Loss: 0.00004401
Iteration 283/1000 | Loss: 0.00033470
Iteration 284/1000 | Loss: 0.00035836
Iteration 285/1000 | Loss: 0.00028554
Iteration 286/1000 | Loss: 0.00026849
Iteration 287/1000 | Loss: 0.00069278
Iteration 288/1000 | Loss: 0.00008254
Iteration 289/1000 | Loss: 0.00017976
Iteration 290/1000 | Loss: 0.00054039
Iteration 291/1000 | Loss: 0.00075129
Iteration 292/1000 | Loss: 0.00096300
Iteration 293/1000 | Loss: 0.00029940
Iteration 294/1000 | Loss: 0.00053688
Iteration 295/1000 | Loss: 0.00027355
Iteration 296/1000 | Loss: 0.00017996
Iteration 297/1000 | Loss: 0.00023334
Iteration 298/1000 | Loss: 0.00004821
Iteration 299/1000 | Loss: 0.00008005
Iteration 300/1000 | Loss: 0.00004692
Iteration 301/1000 | Loss: 0.00003983
Iteration 302/1000 | Loss: 0.00011702
Iteration 303/1000 | Loss: 0.00012220
Iteration 304/1000 | Loss: 0.00020357
Iteration 305/1000 | Loss: 0.00008704
Iteration 306/1000 | Loss: 0.00021615
Iteration 307/1000 | Loss: 0.00022290
Iteration 308/1000 | Loss: 0.00005136
Iteration 309/1000 | Loss: 0.00016184
Iteration 310/1000 | Loss: 0.00022328
Iteration 311/1000 | Loss: 0.00023992
Iteration 312/1000 | Loss: 0.00005656
Iteration 313/1000 | Loss: 0.00014675
Iteration 314/1000 | Loss: 0.00014851
Iteration 315/1000 | Loss: 0.00036466
Iteration 316/1000 | Loss: 0.00040808
Iteration 317/1000 | Loss: 0.00057207
Iteration 318/1000 | Loss: 0.00007826
Iteration 319/1000 | Loss: 0.00035636
Iteration 320/1000 | Loss: 0.00066136
Iteration 321/1000 | Loss: 0.00009541
Iteration 322/1000 | Loss: 0.00049936
Iteration 323/1000 | Loss: 0.00060826
Iteration 324/1000 | Loss: 0.00067481
Iteration 325/1000 | Loss: 0.00058219
Iteration 326/1000 | Loss: 0.00063181
Iteration 327/1000 | Loss: 0.00003660
Iteration 328/1000 | Loss: 0.00003553
Iteration 329/1000 | Loss: 0.00003272
Iteration 330/1000 | Loss: 0.00003681
Iteration 331/1000 | Loss: 0.00002797
Iteration 332/1000 | Loss: 0.00006308
Iteration 333/1000 | Loss: 0.00058004
Iteration 334/1000 | Loss: 0.00005091
Iteration 335/1000 | Loss: 0.00005598
Iteration 336/1000 | Loss: 0.00074306
Iteration 337/1000 | Loss: 0.00068478
Iteration 338/1000 | Loss: 0.00042844
Iteration 339/1000 | Loss: 0.00003475
Iteration 340/1000 | Loss: 0.00002605
Iteration 341/1000 | Loss: 0.00002295
Iteration 342/1000 | Loss: 0.00002083
Iteration 343/1000 | Loss: 0.00002098
Iteration 344/1000 | Loss: 0.00002065
Iteration 345/1000 | Loss: 0.00001939
Iteration 346/1000 | Loss: 0.00001906
Iteration 347/1000 | Loss: 0.00002263
Iteration 348/1000 | Loss: 0.00001842
Iteration 349/1000 | Loss: 0.00002074
Iteration 350/1000 | Loss: 0.00001812
Iteration 351/1000 | Loss: 0.00001846
Iteration 352/1000 | Loss: 0.00001804
Iteration 353/1000 | Loss: 0.00001804
Iteration 354/1000 | Loss: 0.00001803
Iteration 355/1000 | Loss: 0.00001803
Iteration 356/1000 | Loss: 0.00001803
Iteration 357/1000 | Loss: 0.00053292
Iteration 358/1000 | Loss: 0.00013331
Iteration 359/1000 | Loss: 0.00035748
Iteration 360/1000 | Loss: 0.00002597
Iteration 361/1000 | Loss: 0.00002424
Iteration 362/1000 | Loss: 0.00002315
Iteration 363/1000 | Loss: 0.00006066
Iteration 364/1000 | Loss: 0.00001838
Iteration 365/1000 | Loss: 0.00001690
Iteration 366/1000 | Loss: 0.00001647
Iteration 367/1000 | Loss: 0.00001628
Iteration 368/1000 | Loss: 0.00002520
Iteration 369/1000 | Loss: 0.00001718
Iteration 370/1000 | Loss: 0.00001861
Iteration 371/1000 | Loss: 0.00001604
Iteration 372/1000 | Loss: 0.00001602
Iteration 373/1000 | Loss: 0.00001600
Iteration 374/1000 | Loss: 0.00001598
Iteration 375/1000 | Loss: 0.00001597
Iteration 376/1000 | Loss: 0.00001593
Iteration 377/1000 | Loss: 0.00001592
Iteration 378/1000 | Loss: 0.00001592
Iteration 379/1000 | Loss: 0.00001591
Iteration 380/1000 | Loss: 0.00001591
Iteration 381/1000 | Loss: 0.00001591
Iteration 382/1000 | Loss: 0.00001588
Iteration 383/1000 | Loss: 0.00001587
Iteration 384/1000 | Loss: 0.00001995
Iteration 385/1000 | Loss: 0.00001597
Iteration 386/1000 | Loss: 0.00001579
Iteration 387/1000 | Loss: 0.00001579
Iteration 388/1000 | Loss: 0.00001579
Iteration 389/1000 | Loss: 0.00001579
Iteration 390/1000 | Loss: 0.00001579
Iteration 391/1000 | Loss: 0.00001579
Iteration 392/1000 | Loss: 0.00001578
Iteration 393/1000 | Loss: 0.00001578
Iteration 394/1000 | Loss: 0.00001578
Iteration 395/1000 | Loss: 0.00001578
Iteration 396/1000 | Loss: 0.00001577
Iteration 397/1000 | Loss: 0.00001577
Iteration 398/1000 | Loss: 0.00001577
Iteration 399/1000 | Loss: 0.00001577
Iteration 400/1000 | Loss: 0.00001577
Iteration 401/1000 | Loss: 0.00001577
Iteration 402/1000 | Loss: 0.00001577
Iteration 403/1000 | Loss: 0.00001577
Iteration 404/1000 | Loss: 0.00001577
Iteration 405/1000 | Loss: 0.00001577
Iteration 406/1000 | Loss: 0.00001577
Iteration 407/1000 | Loss: 0.00001576
Iteration 408/1000 | Loss: 0.00001576
Iteration 409/1000 | Loss: 0.00001576
Iteration 410/1000 | Loss: 0.00001576
Iteration 411/1000 | Loss: 0.00001576
Iteration 412/1000 | Loss: 0.00001575
Iteration 413/1000 | Loss: 0.00001575
Iteration 414/1000 | Loss: 0.00001575
Iteration 415/1000 | Loss: 0.00001575
Iteration 416/1000 | Loss: 0.00001575
Iteration 417/1000 | Loss: 0.00001575
Iteration 418/1000 | Loss: 0.00001575
Iteration 419/1000 | Loss: 0.00001574
Iteration 420/1000 | Loss: 0.00001574
Iteration 421/1000 | Loss: 0.00001574
Iteration 422/1000 | Loss: 0.00001574
Iteration 423/1000 | Loss: 0.00001574
Iteration 424/1000 | Loss: 0.00002017
Iteration 425/1000 | Loss: 0.00001635
Iteration 426/1000 | Loss: 0.00001832
Iteration 427/1000 | Loss: 0.00001575
Iteration 428/1000 | Loss: 0.00001575
Iteration 429/1000 | Loss: 0.00001575
Iteration 430/1000 | Loss: 0.00001575
Iteration 431/1000 | Loss: 0.00001574
Iteration 432/1000 | Loss: 0.00001574
Iteration 433/1000 | Loss: 0.00001574
Iteration 434/1000 | Loss: 0.00001574
Iteration 435/1000 | Loss: 0.00001572
Iteration 436/1000 | Loss: 0.00001572
Iteration 437/1000 | Loss: 0.00001572
Iteration 438/1000 | Loss: 0.00001572
Iteration 439/1000 | Loss: 0.00001572
Iteration 440/1000 | Loss: 0.00001572
Iteration 441/1000 | Loss: 0.00001572
Iteration 442/1000 | Loss: 0.00001572
Iteration 443/1000 | Loss: 0.00001572
Iteration 444/1000 | Loss: 0.00001572
Iteration 445/1000 | Loss: 0.00001571
Iteration 446/1000 | Loss: 0.00001571
Iteration 447/1000 | Loss: 0.00001571
Iteration 448/1000 | Loss: 0.00001571
Iteration 449/1000 | Loss: 0.00001571
Iteration 450/1000 | Loss: 0.00001571
Iteration 451/1000 | Loss: 0.00001571
Iteration 452/1000 | Loss: 0.00001571
Iteration 453/1000 | Loss: 0.00001571
Iteration 454/1000 | Loss: 0.00001571
Iteration 455/1000 | Loss: 0.00001571
Iteration 456/1000 | Loss: 0.00001571
Iteration 457/1000 | Loss: 0.00001571
Iteration 458/1000 | Loss: 0.00001571
Iteration 459/1000 | Loss: 0.00001571
Iteration 460/1000 | Loss: 0.00001571
Iteration 461/1000 | Loss: 0.00001571
Iteration 462/1000 | Loss: 0.00001571
Iteration 463/1000 | Loss: 0.00001571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 463. Stopping optimization.
Last 5 losses: [1.5711904779891483e-05, 1.5711904779891483e-05, 1.5711904779891483e-05, 1.5711904779891483e-05, 1.5711904779891483e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5711904779891483e-05

Optimization complete. Final v2v error: 2.881864547729492 mm

Highest mean error: 11.050751686096191 mm for frame 95

Lowest mean error: 2.381178379058838 mm for frame 162

Saving results

Total time: 564.464506149292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00762332
Iteration 2/25 | Loss: 0.00121110
Iteration 3/25 | Loss: 0.00110915
Iteration 4/25 | Loss: 0.00108670
Iteration 5/25 | Loss: 0.00109829
Iteration 6/25 | Loss: 0.00104564
Iteration 7/25 | Loss: 0.00103968
Iteration 8/25 | Loss: 0.00103893
Iteration 9/25 | Loss: 0.00103876
Iteration 10/25 | Loss: 0.00103876
Iteration 11/25 | Loss: 0.00103876
Iteration 12/25 | Loss: 0.00103876
Iteration 13/25 | Loss: 0.00103876
Iteration 14/25 | Loss: 0.00103876
Iteration 15/25 | Loss: 0.00103876
Iteration 16/25 | Loss: 0.00103876
Iteration 17/25 | Loss: 0.00103876
Iteration 18/25 | Loss: 0.00103876
Iteration 19/25 | Loss: 0.00103876
Iteration 20/25 | Loss: 0.00103876
Iteration 21/25 | Loss: 0.00103876
Iteration 22/25 | Loss: 0.00103876
Iteration 23/25 | Loss: 0.00103876
Iteration 24/25 | Loss: 0.00103876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001038755988702178, 0.001038755988702178, 0.001038755988702178, 0.001038755988702178, 0.001038755988702178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001038755988702178

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36932361
Iteration 2/25 | Loss: 0.00070786
Iteration 3/25 | Loss: 0.00070785
Iteration 4/25 | Loss: 0.00070785
Iteration 5/25 | Loss: 0.00070785
Iteration 6/25 | Loss: 0.00070785
Iteration 7/25 | Loss: 0.00070785
Iteration 8/25 | Loss: 0.00070785
Iteration 9/25 | Loss: 0.00070785
Iteration 10/25 | Loss: 0.00070785
Iteration 11/25 | Loss: 0.00070785
Iteration 12/25 | Loss: 0.00070785
Iteration 13/25 | Loss: 0.00070785
Iteration 14/25 | Loss: 0.00070785
Iteration 15/25 | Loss: 0.00070785
Iteration 16/25 | Loss: 0.00070785
Iteration 17/25 | Loss: 0.00070785
Iteration 18/25 | Loss: 0.00070785
Iteration 19/25 | Loss: 0.00070785
Iteration 20/25 | Loss: 0.00070785
Iteration 21/25 | Loss: 0.00070785
Iteration 22/25 | Loss: 0.00070785
Iteration 23/25 | Loss: 0.00070785
Iteration 24/25 | Loss: 0.00070785
Iteration 25/25 | Loss: 0.00070785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070785
Iteration 2/1000 | Loss: 0.00002434
Iteration 3/1000 | Loss: 0.00001640
Iteration 4/1000 | Loss: 0.00001479
Iteration 5/1000 | Loss: 0.00001405
Iteration 6/1000 | Loss: 0.00001342
Iteration 7/1000 | Loss: 0.00001300
Iteration 8/1000 | Loss: 0.00001273
Iteration 9/1000 | Loss: 0.00001254
Iteration 10/1000 | Loss: 0.00001229
Iteration 11/1000 | Loss: 0.00001222
Iteration 12/1000 | Loss: 0.00001219
Iteration 13/1000 | Loss: 0.00001213
Iteration 14/1000 | Loss: 0.00001203
Iteration 15/1000 | Loss: 0.00001201
Iteration 16/1000 | Loss: 0.00001194
Iteration 17/1000 | Loss: 0.00001193
Iteration 18/1000 | Loss: 0.00001189
Iteration 19/1000 | Loss: 0.00001187
Iteration 20/1000 | Loss: 0.00001187
Iteration 21/1000 | Loss: 0.00001183
Iteration 22/1000 | Loss: 0.00001182
Iteration 23/1000 | Loss: 0.00001175
Iteration 24/1000 | Loss: 0.00001175
Iteration 25/1000 | Loss: 0.00001174
Iteration 26/1000 | Loss: 0.00001174
Iteration 27/1000 | Loss: 0.00001173
Iteration 28/1000 | Loss: 0.00001173
Iteration 29/1000 | Loss: 0.00001172
Iteration 30/1000 | Loss: 0.00001172
Iteration 31/1000 | Loss: 0.00001172
Iteration 32/1000 | Loss: 0.00001172
Iteration 33/1000 | Loss: 0.00001172
Iteration 34/1000 | Loss: 0.00001171
Iteration 35/1000 | Loss: 0.00001171
Iteration 36/1000 | Loss: 0.00001171
Iteration 37/1000 | Loss: 0.00001170
Iteration 38/1000 | Loss: 0.00001169
Iteration 39/1000 | Loss: 0.00001169
Iteration 40/1000 | Loss: 0.00001169
Iteration 41/1000 | Loss: 0.00001169
Iteration 42/1000 | Loss: 0.00001169
Iteration 43/1000 | Loss: 0.00001169
Iteration 44/1000 | Loss: 0.00001169
Iteration 45/1000 | Loss: 0.00001168
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001168
Iteration 49/1000 | Loss: 0.00001168
Iteration 50/1000 | Loss: 0.00001168
Iteration 51/1000 | Loss: 0.00001166
Iteration 52/1000 | Loss: 0.00001166
Iteration 53/1000 | Loss: 0.00001165
Iteration 54/1000 | Loss: 0.00001164
Iteration 55/1000 | Loss: 0.00001164
Iteration 56/1000 | Loss: 0.00001164
Iteration 57/1000 | Loss: 0.00001163
Iteration 58/1000 | Loss: 0.00001163
Iteration 59/1000 | Loss: 0.00001163
Iteration 60/1000 | Loss: 0.00001163
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001162
Iteration 64/1000 | Loss: 0.00001162
Iteration 65/1000 | Loss: 0.00001161
Iteration 66/1000 | Loss: 0.00001161
Iteration 67/1000 | Loss: 0.00001161
Iteration 68/1000 | Loss: 0.00001161
Iteration 69/1000 | Loss: 0.00001161
Iteration 70/1000 | Loss: 0.00001161
Iteration 71/1000 | Loss: 0.00001161
Iteration 72/1000 | Loss: 0.00001161
Iteration 73/1000 | Loss: 0.00001161
Iteration 74/1000 | Loss: 0.00001161
Iteration 75/1000 | Loss: 0.00001161
Iteration 76/1000 | Loss: 0.00001161
Iteration 77/1000 | Loss: 0.00001161
Iteration 78/1000 | Loss: 0.00001160
Iteration 79/1000 | Loss: 0.00001160
Iteration 80/1000 | Loss: 0.00001160
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001159
Iteration 85/1000 | Loss: 0.00001159
Iteration 86/1000 | Loss: 0.00001159
Iteration 87/1000 | Loss: 0.00001159
Iteration 88/1000 | Loss: 0.00001159
Iteration 89/1000 | Loss: 0.00001159
Iteration 90/1000 | Loss: 0.00001159
Iteration 91/1000 | Loss: 0.00001159
Iteration 92/1000 | Loss: 0.00001159
Iteration 93/1000 | Loss: 0.00001158
Iteration 94/1000 | Loss: 0.00001158
Iteration 95/1000 | Loss: 0.00001158
Iteration 96/1000 | Loss: 0.00001158
Iteration 97/1000 | Loss: 0.00001158
Iteration 98/1000 | Loss: 0.00001157
Iteration 99/1000 | Loss: 0.00001157
Iteration 100/1000 | Loss: 0.00001157
Iteration 101/1000 | Loss: 0.00001157
Iteration 102/1000 | Loss: 0.00001157
Iteration 103/1000 | Loss: 0.00001157
Iteration 104/1000 | Loss: 0.00001157
Iteration 105/1000 | Loss: 0.00001157
Iteration 106/1000 | Loss: 0.00001156
Iteration 107/1000 | Loss: 0.00001156
Iteration 108/1000 | Loss: 0.00001156
Iteration 109/1000 | Loss: 0.00001156
Iteration 110/1000 | Loss: 0.00001156
Iteration 111/1000 | Loss: 0.00001156
Iteration 112/1000 | Loss: 0.00001156
Iteration 113/1000 | Loss: 0.00001155
Iteration 114/1000 | Loss: 0.00001155
Iteration 115/1000 | Loss: 0.00001155
Iteration 116/1000 | Loss: 0.00001155
Iteration 117/1000 | Loss: 0.00001155
Iteration 118/1000 | Loss: 0.00001154
Iteration 119/1000 | Loss: 0.00001154
Iteration 120/1000 | Loss: 0.00001154
Iteration 121/1000 | Loss: 0.00001154
Iteration 122/1000 | Loss: 0.00001154
Iteration 123/1000 | Loss: 0.00001154
Iteration 124/1000 | Loss: 0.00001154
Iteration 125/1000 | Loss: 0.00001153
Iteration 126/1000 | Loss: 0.00001153
Iteration 127/1000 | Loss: 0.00001153
Iteration 128/1000 | Loss: 0.00001152
Iteration 129/1000 | Loss: 0.00001152
Iteration 130/1000 | Loss: 0.00001152
Iteration 131/1000 | Loss: 0.00001151
Iteration 132/1000 | Loss: 0.00001151
Iteration 133/1000 | Loss: 0.00001151
Iteration 134/1000 | Loss: 0.00001150
Iteration 135/1000 | Loss: 0.00001150
Iteration 136/1000 | Loss: 0.00001150
Iteration 137/1000 | Loss: 0.00001149
Iteration 138/1000 | Loss: 0.00001149
Iteration 139/1000 | Loss: 0.00001149
Iteration 140/1000 | Loss: 0.00001149
Iteration 141/1000 | Loss: 0.00001149
Iteration 142/1000 | Loss: 0.00001149
Iteration 143/1000 | Loss: 0.00001149
Iteration 144/1000 | Loss: 0.00001149
Iteration 145/1000 | Loss: 0.00001149
Iteration 146/1000 | Loss: 0.00001149
Iteration 147/1000 | Loss: 0.00001148
Iteration 148/1000 | Loss: 0.00001148
Iteration 149/1000 | Loss: 0.00001148
Iteration 150/1000 | Loss: 0.00001148
Iteration 151/1000 | Loss: 0.00001148
Iteration 152/1000 | Loss: 0.00001148
Iteration 153/1000 | Loss: 0.00001148
Iteration 154/1000 | Loss: 0.00001148
Iteration 155/1000 | Loss: 0.00001148
Iteration 156/1000 | Loss: 0.00001148
Iteration 157/1000 | Loss: 0.00001148
Iteration 158/1000 | Loss: 0.00001147
Iteration 159/1000 | Loss: 0.00001147
Iteration 160/1000 | Loss: 0.00001147
Iteration 161/1000 | Loss: 0.00001147
Iteration 162/1000 | Loss: 0.00001147
Iteration 163/1000 | Loss: 0.00001147
Iteration 164/1000 | Loss: 0.00001147
Iteration 165/1000 | Loss: 0.00001147
Iteration 166/1000 | Loss: 0.00001147
Iteration 167/1000 | Loss: 0.00001147
Iteration 168/1000 | Loss: 0.00001147
Iteration 169/1000 | Loss: 0.00001147
Iteration 170/1000 | Loss: 0.00001147
Iteration 171/1000 | Loss: 0.00001147
Iteration 172/1000 | Loss: 0.00001146
Iteration 173/1000 | Loss: 0.00001146
Iteration 174/1000 | Loss: 0.00001146
Iteration 175/1000 | Loss: 0.00001146
Iteration 176/1000 | Loss: 0.00001146
Iteration 177/1000 | Loss: 0.00001146
Iteration 178/1000 | Loss: 0.00001146
Iteration 179/1000 | Loss: 0.00001146
Iteration 180/1000 | Loss: 0.00001146
Iteration 181/1000 | Loss: 0.00001146
Iteration 182/1000 | Loss: 0.00001146
Iteration 183/1000 | Loss: 0.00001146
Iteration 184/1000 | Loss: 0.00001145
Iteration 185/1000 | Loss: 0.00001145
Iteration 186/1000 | Loss: 0.00001145
Iteration 187/1000 | Loss: 0.00001145
Iteration 188/1000 | Loss: 0.00001145
Iteration 189/1000 | Loss: 0.00001145
Iteration 190/1000 | Loss: 0.00001145
Iteration 191/1000 | Loss: 0.00001145
Iteration 192/1000 | Loss: 0.00001145
Iteration 193/1000 | Loss: 0.00001144
Iteration 194/1000 | Loss: 0.00001144
Iteration 195/1000 | Loss: 0.00001144
Iteration 196/1000 | Loss: 0.00001144
Iteration 197/1000 | Loss: 0.00001144
Iteration 198/1000 | Loss: 0.00001144
Iteration 199/1000 | Loss: 0.00001144
Iteration 200/1000 | Loss: 0.00001143
Iteration 201/1000 | Loss: 0.00001143
Iteration 202/1000 | Loss: 0.00001143
Iteration 203/1000 | Loss: 0.00001143
Iteration 204/1000 | Loss: 0.00001143
Iteration 205/1000 | Loss: 0.00001143
Iteration 206/1000 | Loss: 0.00001143
Iteration 207/1000 | Loss: 0.00001143
Iteration 208/1000 | Loss: 0.00001142
Iteration 209/1000 | Loss: 0.00001142
Iteration 210/1000 | Loss: 0.00001142
Iteration 211/1000 | Loss: 0.00001142
Iteration 212/1000 | Loss: 0.00001142
Iteration 213/1000 | Loss: 0.00001142
Iteration 214/1000 | Loss: 0.00001142
Iteration 215/1000 | Loss: 0.00001141
Iteration 216/1000 | Loss: 0.00001141
Iteration 217/1000 | Loss: 0.00001141
Iteration 218/1000 | Loss: 0.00001141
Iteration 219/1000 | Loss: 0.00001141
Iteration 220/1000 | Loss: 0.00001141
Iteration 221/1000 | Loss: 0.00001141
Iteration 222/1000 | Loss: 0.00001141
Iteration 223/1000 | Loss: 0.00001141
Iteration 224/1000 | Loss: 0.00001141
Iteration 225/1000 | Loss: 0.00001141
Iteration 226/1000 | Loss: 0.00001141
Iteration 227/1000 | Loss: 0.00001141
Iteration 228/1000 | Loss: 0.00001141
Iteration 229/1000 | Loss: 0.00001141
Iteration 230/1000 | Loss: 0.00001141
Iteration 231/1000 | Loss: 0.00001141
Iteration 232/1000 | Loss: 0.00001141
Iteration 233/1000 | Loss: 0.00001140
Iteration 234/1000 | Loss: 0.00001140
Iteration 235/1000 | Loss: 0.00001140
Iteration 236/1000 | Loss: 0.00001140
Iteration 237/1000 | Loss: 0.00001140
Iteration 238/1000 | Loss: 0.00001140
Iteration 239/1000 | Loss: 0.00001140
Iteration 240/1000 | Loss: 0.00001140
Iteration 241/1000 | Loss: 0.00001140
Iteration 242/1000 | Loss: 0.00001140
Iteration 243/1000 | Loss: 0.00001140
Iteration 244/1000 | Loss: 0.00001140
Iteration 245/1000 | Loss: 0.00001140
Iteration 246/1000 | Loss: 0.00001140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.14032063720515e-05, 1.14032063720515e-05, 1.14032063720515e-05, 1.14032063720515e-05, 1.14032063720515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.14032063720515e-05

Optimization complete. Final v2v error: 2.8733155727386475 mm

Highest mean error: 3.3213393688201904 mm for frame 110

Lowest mean error: 2.3582146167755127 mm for frame 203

Saving results

Total time: 57.749168395996094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045890
Iteration 2/25 | Loss: 0.01045890
Iteration 3/25 | Loss: 0.01045889
Iteration 4/25 | Loss: 0.01045889
Iteration 5/25 | Loss: 0.01045889
Iteration 6/25 | Loss: 0.00267695
Iteration 7/25 | Loss: 0.00175142
Iteration 8/25 | Loss: 0.00176558
Iteration 9/25 | Loss: 0.00164854
Iteration 10/25 | Loss: 0.00156104
Iteration 11/25 | Loss: 0.00145456
Iteration 12/25 | Loss: 0.00142885
Iteration 13/25 | Loss: 0.00142127
Iteration 14/25 | Loss: 0.00139275
Iteration 15/25 | Loss: 0.00139003
Iteration 16/25 | Loss: 0.00136727
Iteration 17/25 | Loss: 0.00134738
Iteration 18/25 | Loss: 0.00133809
Iteration 19/25 | Loss: 0.00133525
Iteration 20/25 | Loss: 0.00133082
Iteration 21/25 | Loss: 0.00132907
Iteration 22/25 | Loss: 0.00132591
Iteration 23/25 | Loss: 0.00132461
Iteration 24/25 | Loss: 0.00132409
Iteration 25/25 | Loss: 0.00132396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33729768
Iteration 2/25 | Loss: 0.00272371
Iteration 3/25 | Loss: 0.00265142
Iteration 4/25 | Loss: 0.00265142
Iteration 5/25 | Loss: 0.00265142
Iteration 6/25 | Loss: 0.00265142
Iteration 7/25 | Loss: 0.00265142
Iteration 8/25 | Loss: 0.00265142
Iteration 9/25 | Loss: 0.00265141
Iteration 10/25 | Loss: 0.00265141
Iteration 11/25 | Loss: 0.00265141
Iteration 12/25 | Loss: 0.00265141
Iteration 13/25 | Loss: 0.00265141
Iteration 14/25 | Loss: 0.00265141
Iteration 15/25 | Loss: 0.00265141
Iteration 16/25 | Loss: 0.00265141
Iteration 17/25 | Loss: 0.00265141
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0026514139026403427, 0.0026514139026403427, 0.0026514139026403427, 0.0026514139026403427, 0.0026514139026403427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026514139026403427

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00265141
Iteration 2/1000 | Loss: 0.00325681
Iteration 3/1000 | Loss: 0.00443336
Iteration 4/1000 | Loss: 0.00303372
Iteration 5/1000 | Loss: 0.00111065
Iteration 6/1000 | Loss: 0.00184603
Iteration 7/1000 | Loss: 0.00316936
Iteration 8/1000 | Loss: 0.00191884
Iteration 9/1000 | Loss: 0.00104262
Iteration 10/1000 | Loss: 0.00161515
Iteration 11/1000 | Loss: 0.00046323
Iteration 12/1000 | Loss: 0.00100810
Iteration 13/1000 | Loss: 0.00047069
Iteration 14/1000 | Loss: 0.00015067
Iteration 15/1000 | Loss: 0.00053822
Iteration 16/1000 | Loss: 0.00017736
Iteration 17/1000 | Loss: 0.00015063
Iteration 18/1000 | Loss: 0.00108400
Iteration 19/1000 | Loss: 0.00076771
Iteration 20/1000 | Loss: 0.00065543
Iteration 21/1000 | Loss: 0.00156891
Iteration 22/1000 | Loss: 0.00093745
Iteration 23/1000 | Loss: 0.00216227
Iteration 24/1000 | Loss: 0.00256411
Iteration 25/1000 | Loss: 0.00740294
Iteration 26/1000 | Loss: 0.00354288
Iteration 27/1000 | Loss: 0.00563922
Iteration 28/1000 | Loss: 0.00191604
Iteration 29/1000 | Loss: 0.00126222
Iteration 30/1000 | Loss: 0.00074578
Iteration 31/1000 | Loss: 0.00041766
Iteration 32/1000 | Loss: 0.00101209
Iteration 33/1000 | Loss: 0.00127673
Iteration 34/1000 | Loss: 0.00160827
Iteration 35/1000 | Loss: 0.00044434
Iteration 36/1000 | Loss: 0.00041219
Iteration 37/1000 | Loss: 0.00082957
Iteration 38/1000 | Loss: 0.00130578
Iteration 39/1000 | Loss: 0.00023445
Iteration 40/1000 | Loss: 0.00039356
Iteration 41/1000 | Loss: 0.00031448
Iteration 42/1000 | Loss: 0.00005784
Iteration 43/1000 | Loss: 0.00029056
Iteration 44/1000 | Loss: 0.00020176
Iteration 45/1000 | Loss: 0.00056141
Iteration 46/1000 | Loss: 0.00025957
Iteration 47/1000 | Loss: 0.00091264
Iteration 48/1000 | Loss: 0.00088504
Iteration 49/1000 | Loss: 0.00082904
Iteration 50/1000 | Loss: 0.00086851
Iteration 51/1000 | Loss: 0.00009497
Iteration 52/1000 | Loss: 0.00050746
Iteration 53/1000 | Loss: 0.00013643
Iteration 54/1000 | Loss: 0.00014626
Iteration 55/1000 | Loss: 0.00046507
Iteration 56/1000 | Loss: 0.00025434
Iteration 57/1000 | Loss: 0.00080353
Iteration 58/1000 | Loss: 0.00060017
Iteration 59/1000 | Loss: 0.00038434
Iteration 60/1000 | Loss: 0.00017337
Iteration 61/1000 | Loss: 0.00016172
Iteration 62/1000 | Loss: 0.00014502
Iteration 63/1000 | Loss: 0.00012214
Iteration 64/1000 | Loss: 0.00010256
Iteration 65/1000 | Loss: 0.00010515
Iteration 66/1000 | Loss: 0.00009149
Iteration 67/1000 | Loss: 0.00007025
Iteration 68/1000 | Loss: 0.00007347
Iteration 69/1000 | Loss: 0.00011122
Iteration 70/1000 | Loss: 0.00015217
Iteration 71/1000 | Loss: 0.00017287
Iteration 72/1000 | Loss: 0.00016547
Iteration 73/1000 | Loss: 0.00017706
Iteration 74/1000 | Loss: 0.00017722
Iteration 75/1000 | Loss: 0.00018313
Iteration 76/1000 | Loss: 0.00050443
Iteration 77/1000 | Loss: 0.00036909
Iteration 78/1000 | Loss: 0.00048406
Iteration 79/1000 | Loss: 0.00058824
Iteration 80/1000 | Loss: 0.00031932
Iteration 81/1000 | Loss: 0.00027996
Iteration 82/1000 | Loss: 0.00010191
Iteration 83/1000 | Loss: 0.00002928
Iteration 84/1000 | Loss: 0.00002342
Iteration 85/1000 | Loss: 0.00003927
Iteration 86/1000 | Loss: 0.00003303
Iteration 87/1000 | Loss: 0.00001783
Iteration 88/1000 | Loss: 0.00003974
Iteration 89/1000 | Loss: 0.00001850
Iteration 90/1000 | Loss: 0.00002047
Iteration 91/1000 | Loss: 0.00001546
Iteration 92/1000 | Loss: 0.00002322
Iteration 93/1000 | Loss: 0.00003329
Iteration 94/1000 | Loss: 0.00001716
Iteration 95/1000 | Loss: 0.00002008
Iteration 96/1000 | Loss: 0.00001487
Iteration 97/1000 | Loss: 0.00001714
Iteration 98/1000 | Loss: 0.00001451
Iteration 99/1000 | Loss: 0.00001421
Iteration 100/1000 | Loss: 0.00001421
Iteration 101/1000 | Loss: 0.00001420
Iteration 102/1000 | Loss: 0.00001420
Iteration 103/1000 | Loss: 0.00001420
Iteration 104/1000 | Loss: 0.00001420
Iteration 105/1000 | Loss: 0.00001420
Iteration 106/1000 | Loss: 0.00001420
Iteration 107/1000 | Loss: 0.00001420
Iteration 108/1000 | Loss: 0.00001420
Iteration 109/1000 | Loss: 0.00001420
Iteration 110/1000 | Loss: 0.00001420
Iteration 111/1000 | Loss: 0.00001419
Iteration 112/1000 | Loss: 0.00001419
Iteration 113/1000 | Loss: 0.00001419
Iteration 114/1000 | Loss: 0.00001419
Iteration 115/1000 | Loss: 0.00001419
Iteration 116/1000 | Loss: 0.00001419
Iteration 117/1000 | Loss: 0.00001419
Iteration 118/1000 | Loss: 0.00001419
Iteration 119/1000 | Loss: 0.00001419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.4194916730048135e-05, 1.4194916730048135e-05, 1.4194916730048135e-05, 1.4194916730048135e-05, 1.4194916730048135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4194916730048135e-05

Optimization complete. Final v2v error: 3.0807342529296875 mm

Highest mean error: 6.010106086730957 mm for frame 33

Lowest mean error: 2.525660514831543 mm for frame 80

Saving results

Total time: 198.98992466926575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979430
Iteration 2/25 | Loss: 0.00462442
Iteration 3/25 | Loss: 0.00231273
Iteration 4/25 | Loss: 0.00194299
Iteration 5/25 | Loss: 0.00176876
Iteration 6/25 | Loss: 0.00170986
Iteration 7/25 | Loss: 0.00169850
Iteration 8/25 | Loss: 0.00166682
Iteration 9/25 | Loss: 0.00166376
Iteration 10/25 | Loss: 0.00164902
Iteration 11/25 | Loss: 0.00164136
Iteration 12/25 | Loss: 0.00163887
Iteration 13/25 | Loss: 0.00165787
Iteration 14/25 | Loss: 0.00162767
Iteration 15/25 | Loss: 0.00162393
Iteration 16/25 | Loss: 0.00163311
Iteration 17/25 | Loss: 0.00162347
Iteration 18/25 | Loss: 0.00161515
Iteration 19/25 | Loss: 0.00161356
Iteration 20/25 | Loss: 0.00161440
Iteration 21/25 | Loss: 0.00161142
Iteration 22/25 | Loss: 0.00161249
Iteration 23/25 | Loss: 0.00166693
Iteration 24/25 | Loss: 0.00158631
Iteration 25/25 | Loss: 0.00149393

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36748123
Iteration 2/25 | Loss: 0.00446570
Iteration 3/25 | Loss: 0.00431002
Iteration 4/25 | Loss: 0.00431002
Iteration 5/25 | Loss: 0.00431002
Iteration 6/25 | Loss: 0.00431002
Iteration 7/25 | Loss: 0.00431002
Iteration 8/25 | Loss: 0.00431002
Iteration 9/25 | Loss: 0.00431002
Iteration 10/25 | Loss: 0.00431002
Iteration 11/25 | Loss: 0.00431002
Iteration 12/25 | Loss: 0.00431002
Iteration 13/25 | Loss: 0.00431002
Iteration 14/25 | Loss: 0.00431002
Iteration 15/25 | Loss: 0.00431002
Iteration 16/25 | Loss: 0.00431002
Iteration 17/25 | Loss: 0.00431002
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0043100169859826565, 0.0043100169859826565, 0.0043100169859826565, 0.0043100169859826565, 0.0043100169859826565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0043100169859826565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00431002
Iteration 2/1000 | Loss: 0.00141932
Iteration 3/1000 | Loss: 0.00124439
Iteration 4/1000 | Loss: 0.00084468
Iteration 5/1000 | Loss: 0.00081953
Iteration 6/1000 | Loss: 0.00043518
Iteration 7/1000 | Loss: 0.00133486
Iteration 8/1000 | Loss: 0.00034297
Iteration 9/1000 | Loss: 0.00076575
Iteration 10/1000 | Loss: 0.00115772
Iteration 11/1000 | Loss: 0.00080581
Iteration 12/1000 | Loss: 0.00046931
Iteration 13/1000 | Loss: 0.00131122
Iteration 14/1000 | Loss: 0.00065530
Iteration 15/1000 | Loss: 0.00102546
Iteration 16/1000 | Loss: 0.00043059
Iteration 17/1000 | Loss: 0.00050959
Iteration 18/1000 | Loss: 0.00072201
Iteration 19/1000 | Loss: 0.00044734
Iteration 20/1000 | Loss: 0.00140111
Iteration 21/1000 | Loss: 0.00046544
Iteration 22/1000 | Loss: 0.00061610
Iteration 23/1000 | Loss: 0.00118790
Iteration 24/1000 | Loss: 0.00053728
Iteration 25/1000 | Loss: 0.00065619
Iteration 26/1000 | Loss: 0.00152062
Iteration 27/1000 | Loss: 0.00185315
Iteration 28/1000 | Loss: 0.00036811
Iteration 29/1000 | Loss: 0.00021801
Iteration 30/1000 | Loss: 0.00052066
Iteration 31/1000 | Loss: 0.00019867
Iteration 32/1000 | Loss: 0.00104676
Iteration 33/1000 | Loss: 0.00112665
Iteration 34/1000 | Loss: 0.00086942
Iteration 35/1000 | Loss: 0.00020466
Iteration 36/1000 | Loss: 0.00018844
Iteration 37/1000 | Loss: 0.00035719
Iteration 38/1000 | Loss: 0.00031131
Iteration 39/1000 | Loss: 0.00039369
Iteration 40/1000 | Loss: 0.00031344
Iteration 41/1000 | Loss: 0.00091394
Iteration 42/1000 | Loss: 0.00049069
Iteration 43/1000 | Loss: 0.00135586
Iteration 44/1000 | Loss: 0.00079517
Iteration 45/1000 | Loss: 0.00054348
Iteration 46/1000 | Loss: 0.00065273
Iteration 47/1000 | Loss: 0.00030356
Iteration 48/1000 | Loss: 0.00027421
Iteration 49/1000 | Loss: 0.00051649
Iteration 50/1000 | Loss: 0.00080961
Iteration 51/1000 | Loss: 0.00019773
Iteration 52/1000 | Loss: 0.00016897
Iteration 53/1000 | Loss: 0.00015635
Iteration 54/1000 | Loss: 0.00014078
Iteration 55/1000 | Loss: 0.00017194
Iteration 56/1000 | Loss: 0.00014098
Iteration 57/1000 | Loss: 0.00024834
Iteration 58/1000 | Loss: 0.00016156
Iteration 59/1000 | Loss: 0.00014155
Iteration 60/1000 | Loss: 0.00041251
Iteration 61/1000 | Loss: 0.00015101
Iteration 62/1000 | Loss: 0.00025929
Iteration 63/1000 | Loss: 0.00021952
Iteration 64/1000 | Loss: 0.00013449
Iteration 65/1000 | Loss: 0.00012995
Iteration 66/1000 | Loss: 0.00072462
Iteration 67/1000 | Loss: 0.00138029
Iteration 68/1000 | Loss: 0.00071795
Iteration 69/1000 | Loss: 0.00042886
Iteration 70/1000 | Loss: 0.00068428
Iteration 71/1000 | Loss: 0.00014748
Iteration 72/1000 | Loss: 0.00013761
Iteration 73/1000 | Loss: 0.00014355
Iteration 74/1000 | Loss: 0.00013290
Iteration 75/1000 | Loss: 0.00021116
Iteration 76/1000 | Loss: 0.00042500
Iteration 77/1000 | Loss: 0.00078816
Iteration 78/1000 | Loss: 0.00117654
Iteration 79/1000 | Loss: 0.00083229
Iteration 80/1000 | Loss: 0.00051619
Iteration 81/1000 | Loss: 0.00015849
Iteration 82/1000 | Loss: 0.00012668
Iteration 83/1000 | Loss: 0.00013779
Iteration 84/1000 | Loss: 0.00013501
Iteration 85/1000 | Loss: 0.00032022
Iteration 86/1000 | Loss: 0.00012379
Iteration 87/1000 | Loss: 0.00012393
Iteration 88/1000 | Loss: 0.00014607
Iteration 89/1000 | Loss: 0.00014243
Iteration 90/1000 | Loss: 0.00011458
Iteration 91/1000 | Loss: 0.00010887
Iteration 92/1000 | Loss: 0.00011678
Iteration 93/1000 | Loss: 0.00012793
Iteration 94/1000 | Loss: 0.00010602
Iteration 95/1000 | Loss: 0.00010650
Iteration 96/1000 | Loss: 0.00010448
Iteration 97/1000 | Loss: 0.00011844
Iteration 98/1000 | Loss: 0.00010403
Iteration 99/1000 | Loss: 0.00050689
Iteration 100/1000 | Loss: 0.00011132
Iteration 101/1000 | Loss: 0.00011915
Iteration 102/1000 | Loss: 0.00011460
Iteration 103/1000 | Loss: 0.00010565
Iteration 104/1000 | Loss: 0.00011746
Iteration 105/1000 | Loss: 0.00010851
Iteration 106/1000 | Loss: 0.00011714
Iteration 107/1000 | Loss: 0.00010681
Iteration 108/1000 | Loss: 0.00012313
Iteration 109/1000 | Loss: 0.00012092
Iteration 110/1000 | Loss: 0.00011596
Iteration 111/1000 | Loss: 0.00010885
Iteration 112/1000 | Loss: 0.00012025
Iteration 113/1000 | Loss: 0.00010899
Iteration 114/1000 | Loss: 0.00034432
Iteration 115/1000 | Loss: 0.00070766
Iteration 116/1000 | Loss: 0.00055815
Iteration 117/1000 | Loss: 0.00011329
Iteration 118/1000 | Loss: 0.00024203
Iteration 119/1000 | Loss: 0.00012169
Iteration 120/1000 | Loss: 0.00045156
Iteration 121/1000 | Loss: 0.00011627
Iteration 122/1000 | Loss: 0.00010753
Iteration 123/1000 | Loss: 0.00010429
Iteration 124/1000 | Loss: 0.00010312
Iteration 125/1000 | Loss: 0.00011927
Iteration 126/1000 | Loss: 0.00009571
Iteration 127/1000 | Loss: 0.00010267
Iteration 128/1000 | Loss: 0.00009603
Iteration 129/1000 | Loss: 0.00029078
Iteration 130/1000 | Loss: 0.00009839
Iteration 131/1000 | Loss: 0.00010875
Iteration 132/1000 | Loss: 0.00011737
Iteration 133/1000 | Loss: 0.00009716
Iteration 134/1000 | Loss: 0.00009262
Iteration 135/1000 | Loss: 0.00009275
Iteration 136/1000 | Loss: 0.00009011
Iteration 137/1000 | Loss: 0.00009393
Iteration 138/1000 | Loss: 0.00008952
Iteration 139/1000 | Loss: 0.00009327
Iteration 140/1000 | Loss: 0.00009283
Iteration 141/1000 | Loss: 0.00010449
Iteration 142/1000 | Loss: 0.00008885
Iteration 143/1000 | Loss: 0.00009011
Iteration 144/1000 | Loss: 0.00008957
Iteration 145/1000 | Loss: 0.00038159
Iteration 146/1000 | Loss: 0.00019172
Iteration 147/1000 | Loss: 0.00024992
Iteration 148/1000 | Loss: 0.00020208
Iteration 149/1000 | Loss: 0.00023532
Iteration 150/1000 | Loss: 0.00012052
Iteration 151/1000 | Loss: 0.00009578
Iteration 152/1000 | Loss: 0.00012035
Iteration 153/1000 | Loss: 0.00009021
Iteration 154/1000 | Loss: 0.00010190
Iteration 155/1000 | Loss: 0.00009143
Iteration 156/1000 | Loss: 0.00009654
Iteration 157/1000 | Loss: 0.00008674
Iteration 158/1000 | Loss: 0.00035150
Iteration 159/1000 | Loss: 0.00020985
Iteration 160/1000 | Loss: 0.00030378
Iteration 161/1000 | Loss: 0.00014196
Iteration 162/1000 | Loss: 0.00009814
Iteration 163/1000 | Loss: 0.00009025
Iteration 164/1000 | Loss: 0.00008913
Iteration 165/1000 | Loss: 0.00012474
Iteration 166/1000 | Loss: 0.00008753
Iteration 167/1000 | Loss: 0.00010263
Iteration 168/1000 | Loss: 0.00008515
Iteration 169/1000 | Loss: 0.00009205
Iteration 170/1000 | Loss: 0.00013291
Iteration 171/1000 | Loss: 0.00008488
Iteration 172/1000 | Loss: 0.00008529
Iteration 173/1000 | Loss: 0.00008559
Iteration 174/1000 | Loss: 0.00008849
Iteration 175/1000 | Loss: 0.00008469
Iteration 176/1000 | Loss: 0.00008463
Iteration 177/1000 | Loss: 0.00008433
Iteration 178/1000 | Loss: 0.00008433
Iteration 179/1000 | Loss: 0.00008433
Iteration 180/1000 | Loss: 0.00008433
Iteration 181/1000 | Loss: 0.00008433
Iteration 182/1000 | Loss: 0.00008433
Iteration 183/1000 | Loss: 0.00008433
Iteration 184/1000 | Loss: 0.00008433
Iteration 185/1000 | Loss: 0.00008432
Iteration 186/1000 | Loss: 0.00008432
Iteration 187/1000 | Loss: 0.00008449
Iteration 188/1000 | Loss: 0.00008424
Iteration 189/1000 | Loss: 0.00008423
Iteration 190/1000 | Loss: 0.00008423
Iteration 191/1000 | Loss: 0.00008423
Iteration 192/1000 | Loss: 0.00008422
Iteration 193/1000 | Loss: 0.00008422
Iteration 194/1000 | Loss: 0.00008422
Iteration 195/1000 | Loss: 0.00008422
Iteration 196/1000 | Loss: 0.00008422
Iteration 197/1000 | Loss: 0.00008422
Iteration 198/1000 | Loss: 0.00008422
Iteration 199/1000 | Loss: 0.00008421
Iteration 200/1000 | Loss: 0.00066688
Iteration 201/1000 | Loss: 0.00129490
Iteration 202/1000 | Loss: 0.00087728
Iteration 203/1000 | Loss: 0.00057661
Iteration 204/1000 | Loss: 0.00032722
Iteration 205/1000 | Loss: 0.00018793
Iteration 206/1000 | Loss: 0.00027405
Iteration 207/1000 | Loss: 0.00011517
Iteration 208/1000 | Loss: 0.00010658
Iteration 209/1000 | Loss: 0.00014198
Iteration 210/1000 | Loss: 0.00009400
Iteration 211/1000 | Loss: 0.00008557
Iteration 212/1000 | Loss: 0.00007813
Iteration 213/1000 | Loss: 0.00008477
Iteration 214/1000 | Loss: 0.00007630
Iteration 215/1000 | Loss: 0.00007789
Iteration 216/1000 | Loss: 0.00009151
Iteration 217/1000 | Loss: 0.00007538
Iteration 218/1000 | Loss: 0.00008850
Iteration 219/1000 | Loss: 0.00035521
Iteration 220/1000 | Loss: 0.00014340
Iteration 221/1000 | Loss: 0.00039640
Iteration 222/1000 | Loss: 0.00014363
Iteration 223/1000 | Loss: 0.00016442
Iteration 224/1000 | Loss: 0.00010017
Iteration 225/1000 | Loss: 0.00008995
Iteration 226/1000 | Loss: 0.00008366
Iteration 227/1000 | Loss: 0.00007772
Iteration 228/1000 | Loss: 0.00008240
Iteration 229/1000 | Loss: 0.00007409
Iteration 230/1000 | Loss: 0.00007567
Iteration 231/1000 | Loss: 0.00009971
Iteration 232/1000 | Loss: 0.00007458
Iteration 233/1000 | Loss: 0.00007260
Iteration 234/1000 | Loss: 0.00007259
Iteration 235/1000 | Loss: 0.00009845
Iteration 236/1000 | Loss: 0.00035676
Iteration 237/1000 | Loss: 0.00035696
Iteration 238/1000 | Loss: 0.00107305
Iteration 239/1000 | Loss: 0.00057582
Iteration 240/1000 | Loss: 0.00105205
Iteration 241/1000 | Loss: 0.00049258
Iteration 242/1000 | Loss: 0.00030642
Iteration 243/1000 | Loss: 0.00040197
Iteration 244/1000 | Loss: 0.00008180
Iteration 245/1000 | Loss: 0.00008003
Iteration 246/1000 | Loss: 0.00008898
Iteration 247/1000 | Loss: 0.00007679
Iteration 248/1000 | Loss: 0.00007324
Iteration 249/1000 | Loss: 0.00011160
Iteration 250/1000 | Loss: 0.00007871
Iteration 251/1000 | Loss: 0.00007136
Iteration 252/1000 | Loss: 0.00036823
Iteration 253/1000 | Loss: 0.00008639
Iteration 254/1000 | Loss: 0.00024573
Iteration 255/1000 | Loss: 0.00009444
Iteration 256/1000 | Loss: 0.00007197
Iteration 257/1000 | Loss: 0.00027262
Iteration 258/1000 | Loss: 0.00007664
Iteration 259/1000 | Loss: 0.00026509
Iteration 260/1000 | Loss: 0.00008470
Iteration 261/1000 | Loss: 0.00018330
Iteration 262/1000 | Loss: 0.00009055
Iteration 263/1000 | Loss: 0.00012916
Iteration 264/1000 | Loss: 0.00009156
Iteration 265/1000 | Loss: 0.00041459
Iteration 266/1000 | Loss: 0.00044278
Iteration 267/1000 | Loss: 0.00025206
Iteration 268/1000 | Loss: 0.00038388
Iteration 269/1000 | Loss: 0.00022995
Iteration 270/1000 | Loss: 0.00030085
Iteration 271/1000 | Loss: 0.00012038
Iteration 272/1000 | Loss: 0.00024681
Iteration 273/1000 | Loss: 0.00025636
Iteration 274/1000 | Loss: 0.00010412
Iteration 275/1000 | Loss: 0.00007342
Iteration 276/1000 | Loss: 0.00007205
Iteration 277/1000 | Loss: 0.00022113
Iteration 278/1000 | Loss: 0.00008699
Iteration 279/1000 | Loss: 0.00017240
Iteration 280/1000 | Loss: 0.00008124
Iteration 281/1000 | Loss: 0.00011344
Iteration 282/1000 | Loss: 0.00007335
Iteration 283/1000 | Loss: 0.00007333
Iteration 284/1000 | Loss: 0.00007311
Iteration 285/1000 | Loss: 0.00007504
Iteration 286/1000 | Loss: 0.00006819
Iteration 287/1000 | Loss: 0.00007092
Iteration 288/1000 | Loss: 0.00018145
Iteration 289/1000 | Loss: 0.00012085
Iteration 290/1000 | Loss: 0.00006679
Iteration 291/1000 | Loss: 0.00006637
Iteration 292/1000 | Loss: 0.00006623
Iteration 293/1000 | Loss: 0.00006622
Iteration 294/1000 | Loss: 0.00006617
Iteration 295/1000 | Loss: 0.00006617
Iteration 296/1000 | Loss: 0.00006617
Iteration 297/1000 | Loss: 0.00006720
Iteration 298/1000 | Loss: 0.00036098
Iteration 299/1000 | Loss: 0.00036097
Iteration 300/1000 | Loss: 0.00037171
Iteration 301/1000 | Loss: 0.00013093
Iteration 302/1000 | Loss: 0.00007760
Iteration 303/1000 | Loss: 0.00007052
Iteration 304/1000 | Loss: 0.00007301
Iteration 305/1000 | Loss: 0.00006903
Iteration 306/1000 | Loss: 0.00006846
Iteration 307/1000 | Loss: 0.00006904
Iteration 308/1000 | Loss: 0.00006700
Iteration 309/1000 | Loss: 0.00006725
Iteration 310/1000 | Loss: 0.00006696
Iteration 311/1000 | Loss: 0.00006695
Iteration 312/1000 | Loss: 0.00006716
Iteration 313/1000 | Loss: 0.00007238
Iteration 314/1000 | Loss: 0.00006752
Iteration 315/1000 | Loss: 0.00006820
Iteration 316/1000 | Loss: 0.00006683
Iteration 317/1000 | Loss: 0.00006682
Iteration 318/1000 | Loss: 0.00006745
Iteration 319/1000 | Loss: 0.00006680
Iteration 320/1000 | Loss: 0.00006670
Iteration 321/1000 | Loss: 0.00006670
Iteration 322/1000 | Loss: 0.00006670
Iteration 323/1000 | Loss: 0.00006670
Iteration 324/1000 | Loss: 0.00006669
Iteration 325/1000 | Loss: 0.00006669
Iteration 326/1000 | Loss: 0.00006669
Iteration 327/1000 | Loss: 0.00006669
Iteration 328/1000 | Loss: 0.00006669
Iteration 329/1000 | Loss: 0.00006669
Iteration 330/1000 | Loss: 0.00006662
Iteration 331/1000 | Loss: 0.00006966
Iteration 332/1000 | Loss: 0.00006768
Iteration 333/1000 | Loss: 0.00006642
Iteration 334/1000 | Loss: 0.00006644
Iteration 335/1000 | Loss: 0.00006644
Iteration 336/1000 | Loss: 0.00006642
Iteration 337/1000 | Loss: 0.00006638
Iteration 338/1000 | Loss: 0.00006637
Iteration 339/1000 | Loss: 0.00006640
Iteration 340/1000 | Loss: 0.00006698
Iteration 341/1000 | Loss: 0.00006631
Iteration 342/1000 | Loss: 0.00006632
Iteration 343/1000 | Loss: 0.00006629
Iteration 344/1000 | Loss: 0.00006629
Iteration 345/1000 | Loss: 0.00006629
Iteration 346/1000 | Loss: 0.00006629
Iteration 347/1000 | Loss: 0.00006629
Iteration 348/1000 | Loss: 0.00006629
Iteration 349/1000 | Loss: 0.00006629
Iteration 350/1000 | Loss: 0.00006629
Iteration 351/1000 | Loss: 0.00006629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 351. Stopping optimization.
Last 5 losses: [6.628607661696151e-05, 6.628607661696151e-05, 6.628607661696151e-05, 6.628607661696151e-05, 6.628607661696151e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.628607661696151e-05

Optimization complete. Final v2v error: 4.176989555358887 mm

Highest mean error: 12.228652954101562 mm for frame 1

Lowest mean error: 2.7547433376312256 mm for frame 100

Saving results

Total time: 510.70887875556946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769792
Iteration 2/25 | Loss: 0.00164358
Iteration 3/25 | Loss: 0.00114838
Iteration 4/25 | Loss: 0.00107970
Iteration 5/25 | Loss: 0.00106774
Iteration 6/25 | Loss: 0.00106662
Iteration 7/25 | Loss: 0.00106775
Iteration 8/25 | Loss: 0.00106630
Iteration 9/25 | Loss: 0.00106439
Iteration 10/25 | Loss: 0.00106151
Iteration 11/25 | Loss: 0.00105972
Iteration 12/25 | Loss: 0.00105845
Iteration 13/25 | Loss: 0.00105794
Iteration 14/25 | Loss: 0.00105776
Iteration 15/25 | Loss: 0.00105773
Iteration 16/25 | Loss: 0.00105772
Iteration 17/25 | Loss: 0.00105772
Iteration 18/25 | Loss: 0.00105771
Iteration 19/25 | Loss: 0.00105771
Iteration 20/25 | Loss: 0.00105771
Iteration 21/25 | Loss: 0.00105771
Iteration 22/25 | Loss: 0.00105771
Iteration 23/25 | Loss: 0.00105771
Iteration 24/25 | Loss: 0.00105771
Iteration 25/25 | Loss: 0.00105771

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91176200
Iteration 2/25 | Loss: 0.00067157
Iteration 3/25 | Loss: 0.00067157
Iteration 4/25 | Loss: 0.00067157
Iteration 5/25 | Loss: 0.00067157
Iteration 6/25 | Loss: 0.00067157
Iteration 7/25 | Loss: 0.00067157
Iteration 8/25 | Loss: 0.00067157
Iteration 9/25 | Loss: 0.00067157
Iteration 10/25 | Loss: 0.00067157
Iteration 11/25 | Loss: 0.00067157
Iteration 12/25 | Loss: 0.00067157
Iteration 13/25 | Loss: 0.00067157
Iteration 14/25 | Loss: 0.00067157
Iteration 15/25 | Loss: 0.00067157
Iteration 16/25 | Loss: 0.00067157
Iteration 17/25 | Loss: 0.00067157
Iteration 18/25 | Loss: 0.00067157
Iteration 19/25 | Loss: 0.00067157
Iteration 20/25 | Loss: 0.00067157
Iteration 21/25 | Loss: 0.00067157
Iteration 22/25 | Loss: 0.00067157
Iteration 23/25 | Loss: 0.00067157
Iteration 24/25 | Loss: 0.00067157
Iteration 25/25 | Loss: 0.00067157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067157
Iteration 2/1000 | Loss: 0.00002022
Iteration 3/1000 | Loss: 0.00001657
Iteration 4/1000 | Loss: 0.00001516
Iteration 5/1000 | Loss: 0.00001425
Iteration 6/1000 | Loss: 0.00001382
Iteration 7/1000 | Loss: 0.00001342
Iteration 8/1000 | Loss: 0.00001336
Iteration 9/1000 | Loss: 0.00001309
Iteration 10/1000 | Loss: 0.00001286
Iteration 11/1000 | Loss: 0.00001267
Iteration 12/1000 | Loss: 0.00001263
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001258
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001255
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001246
Iteration 19/1000 | Loss: 0.00001245
Iteration 20/1000 | Loss: 0.00001244
Iteration 21/1000 | Loss: 0.00001244
Iteration 22/1000 | Loss: 0.00001243
Iteration 23/1000 | Loss: 0.00001243
Iteration 24/1000 | Loss: 0.00001242
Iteration 25/1000 | Loss: 0.00001241
Iteration 26/1000 | Loss: 0.00001240
Iteration 27/1000 | Loss: 0.00001239
Iteration 28/1000 | Loss: 0.00001239
Iteration 29/1000 | Loss: 0.00001238
Iteration 30/1000 | Loss: 0.00001238
Iteration 31/1000 | Loss: 0.00001237
Iteration 32/1000 | Loss: 0.00001237
Iteration 33/1000 | Loss: 0.00001236
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001235
Iteration 36/1000 | Loss: 0.00001234
Iteration 37/1000 | Loss: 0.00001234
Iteration 38/1000 | Loss: 0.00001233
Iteration 39/1000 | Loss: 0.00001232
Iteration 40/1000 | Loss: 0.00001232
Iteration 41/1000 | Loss: 0.00001232
Iteration 42/1000 | Loss: 0.00001227
Iteration 43/1000 | Loss: 0.00001227
Iteration 44/1000 | Loss: 0.00001226
Iteration 45/1000 | Loss: 0.00001226
Iteration 46/1000 | Loss: 0.00001218
Iteration 47/1000 | Loss: 0.00001218
Iteration 48/1000 | Loss: 0.00001217
Iteration 49/1000 | Loss: 0.00001217
Iteration 50/1000 | Loss: 0.00001216
Iteration 51/1000 | Loss: 0.00001216
Iteration 52/1000 | Loss: 0.00001215
Iteration 53/1000 | Loss: 0.00001215
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001213
Iteration 56/1000 | Loss: 0.00001213
Iteration 57/1000 | Loss: 0.00001212
Iteration 58/1000 | Loss: 0.00001212
Iteration 59/1000 | Loss: 0.00001211
Iteration 60/1000 | Loss: 0.00001211
Iteration 61/1000 | Loss: 0.00001210
Iteration 62/1000 | Loss: 0.00001210
Iteration 63/1000 | Loss: 0.00001210
Iteration 64/1000 | Loss: 0.00001209
Iteration 65/1000 | Loss: 0.00001209
Iteration 66/1000 | Loss: 0.00001208
Iteration 67/1000 | Loss: 0.00001208
Iteration 68/1000 | Loss: 0.00001207
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001206
Iteration 71/1000 | Loss: 0.00001206
Iteration 72/1000 | Loss: 0.00001205
Iteration 73/1000 | Loss: 0.00001205
Iteration 74/1000 | Loss: 0.00001205
Iteration 75/1000 | Loss: 0.00001205
Iteration 76/1000 | Loss: 0.00001205
Iteration 77/1000 | Loss: 0.00001205
Iteration 78/1000 | Loss: 0.00001205
Iteration 79/1000 | Loss: 0.00001205
Iteration 80/1000 | Loss: 0.00001204
Iteration 81/1000 | Loss: 0.00001204
Iteration 82/1000 | Loss: 0.00001204
Iteration 83/1000 | Loss: 0.00001203
Iteration 84/1000 | Loss: 0.00001203
Iteration 85/1000 | Loss: 0.00001203
Iteration 86/1000 | Loss: 0.00001203
Iteration 87/1000 | Loss: 0.00001203
Iteration 88/1000 | Loss: 0.00001203
Iteration 89/1000 | Loss: 0.00001203
Iteration 90/1000 | Loss: 0.00001202
Iteration 91/1000 | Loss: 0.00001202
Iteration 92/1000 | Loss: 0.00001202
Iteration 93/1000 | Loss: 0.00001201
Iteration 94/1000 | Loss: 0.00001201
Iteration 95/1000 | Loss: 0.00001201
Iteration 96/1000 | Loss: 0.00001201
Iteration 97/1000 | Loss: 0.00001201
Iteration 98/1000 | Loss: 0.00001200
Iteration 99/1000 | Loss: 0.00001200
Iteration 100/1000 | Loss: 0.00001200
Iteration 101/1000 | Loss: 0.00001200
Iteration 102/1000 | Loss: 0.00001200
Iteration 103/1000 | Loss: 0.00001199
Iteration 104/1000 | Loss: 0.00001199
Iteration 105/1000 | Loss: 0.00001199
Iteration 106/1000 | Loss: 0.00001199
Iteration 107/1000 | Loss: 0.00001198
Iteration 108/1000 | Loss: 0.00001198
Iteration 109/1000 | Loss: 0.00001198
Iteration 110/1000 | Loss: 0.00001197
Iteration 111/1000 | Loss: 0.00001197
Iteration 112/1000 | Loss: 0.00001197
Iteration 113/1000 | Loss: 0.00001196
Iteration 114/1000 | Loss: 0.00001196
Iteration 115/1000 | Loss: 0.00001196
Iteration 116/1000 | Loss: 0.00001196
Iteration 117/1000 | Loss: 0.00001196
Iteration 118/1000 | Loss: 0.00001195
Iteration 119/1000 | Loss: 0.00001195
Iteration 120/1000 | Loss: 0.00001195
Iteration 121/1000 | Loss: 0.00001195
Iteration 122/1000 | Loss: 0.00001195
Iteration 123/1000 | Loss: 0.00001195
Iteration 124/1000 | Loss: 0.00001195
Iteration 125/1000 | Loss: 0.00001194
Iteration 126/1000 | Loss: 0.00001194
Iteration 127/1000 | Loss: 0.00001194
Iteration 128/1000 | Loss: 0.00001194
Iteration 129/1000 | Loss: 0.00001194
Iteration 130/1000 | Loss: 0.00001194
Iteration 131/1000 | Loss: 0.00001194
Iteration 132/1000 | Loss: 0.00001194
Iteration 133/1000 | Loss: 0.00001193
Iteration 134/1000 | Loss: 0.00001193
Iteration 135/1000 | Loss: 0.00001193
Iteration 136/1000 | Loss: 0.00001193
Iteration 137/1000 | Loss: 0.00001193
Iteration 138/1000 | Loss: 0.00001193
Iteration 139/1000 | Loss: 0.00001193
Iteration 140/1000 | Loss: 0.00001192
Iteration 141/1000 | Loss: 0.00001192
Iteration 142/1000 | Loss: 0.00001192
Iteration 143/1000 | Loss: 0.00001192
Iteration 144/1000 | Loss: 0.00001192
Iteration 145/1000 | Loss: 0.00001192
Iteration 146/1000 | Loss: 0.00001192
Iteration 147/1000 | Loss: 0.00001192
Iteration 148/1000 | Loss: 0.00001191
Iteration 149/1000 | Loss: 0.00001191
Iteration 150/1000 | Loss: 0.00001191
Iteration 151/1000 | Loss: 0.00001191
Iteration 152/1000 | Loss: 0.00001191
Iteration 153/1000 | Loss: 0.00001191
Iteration 154/1000 | Loss: 0.00001191
Iteration 155/1000 | Loss: 0.00001191
Iteration 156/1000 | Loss: 0.00001191
Iteration 157/1000 | Loss: 0.00001191
Iteration 158/1000 | Loss: 0.00001191
Iteration 159/1000 | Loss: 0.00001191
Iteration 160/1000 | Loss: 0.00001190
Iteration 161/1000 | Loss: 0.00001190
Iteration 162/1000 | Loss: 0.00001190
Iteration 163/1000 | Loss: 0.00001190
Iteration 164/1000 | Loss: 0.00001190
Iteration 165/1000 | Loss: 0.00001190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.1904391612915788e-05, 1.1904391612915788e-05, 1.1904391612915788e-05, 1.1904391612915788e-05, 1.1904391612915788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1904391612915788e-05

Optimization complete. Final v2v error: 2.960540294647217 mm

Highest mean error: 3.3929836750030518 mm for frame 82

Lowest mean error: 2.7316648960113525 mm for frame 9

Saving results

Total time: 55.66429567337036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00368631
Iteration 2/25 | Loss: 0.00113218
Iteration 3/25 | Loss: 0.00104930
Iteration 4/25 | Loss: 0.00103934
Iteration 5/25 | Loss: 0.00103638
Iteration 6/25 | Loss: 0.00103557
Iteration 7/25 | Loss: 0.00103557
Iteration 8/25 | Loss: 0.00103557
Iteration 9/25 | Loss: 0.00103557
Iteration 10/25 | Loss: 0.00103557
Iteration 11/25 | Loss: 0.00103557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010355653939768672, 0.0010355653939768672, 0.0010355653939768672, 0.0010355653939768672, 0.0010355653939768672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010355653939768672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76731634
Iteration 2/25 | Loss: 0.00074648
Iteration 3/25 | Loss: 0.00074648
Iteration 4/25 | Loss: 0.00074648
Iteration 5/25 | Loss: 0.00074647
Iteration 6/25 | Loss: 0.00074647
Iteration 7/25 | Loss: 0.00074647
Iteration 8/25 | Loss: 0.00074647
Iteration 9/25 | Loss: 0.00074647
Iteration 10/25 | Loss: 0.00074647
Iteration 11/25 | Loss: 0.00074647
Iteration 12/25 | Loss: 0.00074647
Iteration 13/25 | Loss: 0.00074647
Iteration 14/25 | Loss: 0.00074647
Iteration 15/25 | Loss: 0.00074647
Iteration 16/25 | Loss: 0.00074647
Iteration 17/25 | Loss: 0.00074647
Iteration 18/25 | Loss: 0.00074647
Iteration 19/25 | Loss: 0.00074647
Iteration 20/25 | Loss: 0.00074647
Iteration 21/25 | Loss: 0.00074647
Iteration 22/25 | Loss: 0.00074647
Iteration 23/25 | Loss: 0.00074647
Iteration 24/25 | Loss: 0.00074647
Iteration 25/25 | Loss: 0.00074647

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074647
Iteration 2/1000 | Loss: 0.00002452
Iteration 3/1000 | Loss: 0.00001479
Iteration 4/1000 | Loss: 0.00001143
Iteration 5/1000 | Loss: 0.00001032
Iteration 6/1000 | Loss: 0.00000950
Iteration 7/1000 | Loss: 0.00000906
Iteration 8/1000 | Loss: 0.00000870
Iteration 9/1000 | Loss: 0.00000860
Iteration 10/1000 | Loss: 0.00000853
Iteration 11/1000 | Loss: 0.00000850
Iteration 12/1000 | Loss: 0.00000850
Iteration 13/1000 | Loss: 0.00000849
Iteration 14/1000 | Loss: 0.00000848
Iteration 15/1000 | Loss: 0.00000839
Iteration 16/1000 | Loss: 0.00000837
Iteration 17/1000 | Loss: 0.00000830
Iteration 18/1000 | Loss: 0.00000829
Iteration 19/1000 | Loss: 0.00000824
Iteration 20/1000 | Loss: 0.00000823
Iteration 21/1000 | Loss: 0.00000821
Iteration 22/1000 | Loss: 0.00000821
Iteration 23/1000 | Loss: 0.00000820
Iteration 24/1000 | Loss: 0.00000820
Iteration 25/1000 | Loss: 0.00000819
Iteration 26/1000 | Loss: 0.00000815
Iteration 27/1000 | Loss: 0.00000812
Iteration 28/1000 | Loss: 0.00000810
Iteration 29/1000 | Loss: 0.00000810
Iteration 30/1000 | Loss: 0.00000809
Iteration 31/1000 | Loss: 0.00000808
Iteration 32/1000 | Loss: 0.00000807
Iteration 33/1000 | Loss: 0.00000807
Iteration 34/1000 | Loss: 0.00000806
Iteration 35/1000 | Loss: 0.00000806
Iteration 36/1000 | Loss: 0.00000806
Iteration 37/1000 | Loss: 0.00000805
Iteration 38/1000 | Loss: 0.00000805
Iteration 39/1000 | Loss: 0.00000804
Iteration 40/1000 | Loss: 0.00000803
Iteration 41/1000 | Loss: 0.00000803
Iteration 42/1000 | Loss: 0.00000802
Iteration 43/1000 | Loss: 0.00000802
Iteration 44/1000 | Loss: 0.00000801
Iteration 45/1000 | Loss: 0.00000801
Iteration 46/1000 | Loss: 0.00000801
Iteration 47/1000 | Loss: 0.00000801
Iteration 48/1000 | Loss: 0.00000800
Iteration 49/1000 | Loss: 0.00000800
Iteration 50/1000 | Loss: 0.00000800
Iteration 51/1000 | Loss: 0.00000800
Iteration 52/1000 | Loss: 0.00000800
Iteration 53/1000 | Loss: 0.00000800
Iteration 54/1000 | Loss: 0.00000800
Iteration 55/1000 | Loss: 0.00000799
Iteration 56/1000 | Loss: 0.00000799
Iteration 57/1000 | Loss: 0.00000799
Iteration 58/1000 | Loss: 0.00000798
Iteration 59/1000 | Loss: 0.00000798
Iteration 60/1000 | Loss: 0.00000797
Iteration 61/1000 | Loss: 0.00000797
Iteration 62/1000 | Loss: 0.00000797
Iteration 63/1000 | Loss: 0.00000797
Iteration 64/1000 | Loss: 0.00000796
Iteration 65/1000 | Loss: 0.00000796
Iteration 66/1000 | Loss: 0.00000796
Iteration 67/1000 | Loss: 0.00000796
Iteration 68/1000 | Loss: 0.00000796
Iteration 69/1000 | Loss: 0.00000795
Iteration 70/1000 | Loss: 0.00000795
Iteration 71/1000 | Loss: 0.00000795
Iteration 72/1000 | Loss: 0.00000795
Iteration 73/1000 | Loss: 0.00000794
Iteration 74/1000 | Loss: 0.00000794
Iteration 75/1000 | Loss: 0.00000794
Iteration 76/1000 | Loss: 0.00000794
Iteration 77/1000 | Loss: 0.00000794
Iteration 78/1000 | Loss: 0.00000793
Iteration 79/1000 | Loss: 0.00000793
Iteration 80/1000 | Loss: 0.00000793
Iteration 81/1000 | Loss: 0.00000793
Iteration 82/1000 | Loss: 0.00000793
Iteration 83/1000 | Loss: 0.00000793
Iteration 84/1000 | Loss: 0.00000793
Iteration 85/1000 | Loss: 0.00000793
Iteration 86/1000 | Loss: 0.00000793
Iteration 87/1000 | Loss: 0.00000792
Iteration 88/1000 | Loss: 0.00000792
Iteration 89/1000 | Loss: 0.00000792
Iteration 90/1000 | Loss: 0.00000792
Iteration 91/1000 | Loss: 0.00000792
Iteration 92/1000 | Loss: 0.00000792
Iteration 93/1000 | Loss: 0.00000792
Iteration 94/1000 | Loss: 0.00000791
Iteration 95/1000 | Loss: 0.00000791
Iteration 96/1000 | Loss: 0.00000791
Iteration 97/1000 | Loss: 0.00000791
Iteration 98/1000 | Loss: 0.00000791
Iteration 99/1000 | Loss: 0.00000791
Iteration 100/1000 | Loss: 0.00000791
Iteration 101/1000 | Loss: 0.00000791
Iteration 102/1000 | Loss: 0.00000791
Iteration 103/1000 | Loss: 0.00000790
Iteration 104/1000 | Loss: 0.00000790
Iteration 105/1000 | Loss: 0.00000790
Iteration 106/1000 | Loss: 0.00000789
Iteration 107/1000 | Loss: 0.00000789
Iteration 108/1000 | Loss: 0.00000789
Iteration 109/1000 | Loss: 0.00000789
Iteration 110/1000 | Loss: 0.00000789
Iteration 111/1000 | Loss: 0.00000789
Iteration 112/1000 | Loss: 0.00000789
Iteration 113/1000 | Loss: 0.00000789
Iteration 114/1000 | Loss: 0.00000789
Iteration 115/1000 | Loss: 0.00000788
Iteration 116/1000 | Loss: 0.00000788
Iteration 117/1000 | Loss: 0.00000788
Iteration 118/1000 | Loss: 0.00000788
Iteration 119/1000 | Loss: 0.00000788
Iteration 120/1000 | Loss: 0.00000788
Iteration 121/1000 | Loss: 0.00000788
Iteration 122/1000 | Loss: 0.00000787
Iteration 123/1000 | Loss: 0.00000787
Iteration 124/1000 | Loss: 0.00000787
Iteration 125/1000 | Loss: 0.00000786
Iteration 126/1000 | Loss: 0.00000786
Iteration 127/1000 | Loss: 0.00000786
Iteration 128/1000 | Loss: 0.00000786
Iteration 129/1000 | Loss: 0.00000786
Iteration 130/1000 | Loss: 0.00000786
Iteration 131/1000 | Loss: 0.00000786
Iteration 132/1000 | Loss: 0.00000786
Iteration 133/1000 | Loss: 0.00000786
Iteration 134/1000 | Loss: 0.00000786
Iteration 135/1000 | Loss: 0.00000786
Iteration 136/1000 | Loss: 0.00000786
Iteration 137/1000 | Loss: 0.00000786
Iteration 138/1000 | Loss: 0.00000786
Iteration 139/1000 | Loss: 0.00000786
Iteration 140/1000 | Loss: 0.00000786
Iteration 141/1000 | Loss: 0.00000786
Iteration 142/1000 | Loss: 0.00000785
Iteration 143/1000 | Loss: 0.00000785
Iteration 144/1000 | Loss: 0.00000785
Iteration 145/1000 | Loss: 0.00000785
Iteration 146/1000 | Loss: 0.00000785
Iteration 147/1000 | Loss: 0.00000785
Iteration 148/1000 | Loss: 0.00000785
Iteration 149/1000 | Loss: 0.00000785
Iteration 150/1000 | Loss: 0.00000785
Iteration 151/1000 | Loss: 0.00000785
Iteration 152/1000 | Loss: 0.00000785
Iteration 153/1000 | Loss: 0.00000785
Iteration 154/1000 | Loss: 0.00000785
Iteration 155/1000 | Loss: 0.00000785
Iteration 156/1000 | Loss: 0.00000784
Iteration 157/1000 | Loss: 0.00000784
Iteration 158/1000 | Loss: 0.00000784
Iteration 159/1000 | Loss: 0.00000784
Iteration 160/1000 | Loss: 0.00000784
Iteration 161/1000 | Loss: 0.00000784
Iteration 162/1000 | Loss: 0.00000784
Iteration 163/1000 | Loss: 0.00000784
Iteration 164/1000 | Loss: 0.00000784
Iteration 165/1000 | Loss: 0.00000784
Iteration 166/1000 | Loss: 0.00000784
Iteration 167/1000 | Loss: 0.00000783
Iteration 168/1000 | Loss: 0.00000783
Iteration 169/1000 | Loss: 0.00000783
Iteration 170/1000 | Loss: 0.00000783
Iteration 171/1000 | Loss: 0.00000783
Iteration 172/1000 | Loss: 0.00000783
Iteration 173/1000 | Loss: 0.00000783
Iteration 174/1000 | Loss: 0.00000783
Iteration 175/1000 | Loss: 0.00000783
Iteration 176/1000 | Loss: 0.00000783
Iteration 177/1000 | Loss: 0.00000783
Iteration 178/1000 | Loss: 0.00000783
Iteration 179/1000 | Loss: 0.00000783
Iteration 180/1000 | Loss: 0.00000783
Iteration 181/1000 | Loss: 0.00000782
Iteration 182/1000 | Loss: 0.00000782
Iteration 183/1000 | Loss: 0.00000782
Iteration 184/1000 | Loss: 0.00000782
Iteration 185/1000 | Loss: 0.00000782
Iteration 186/1000 | Loss: 0.00000782
Iteration 187/1000 | Loss: 0.00000782
Iteration 188/1000 | Loss: 0.00000782
Iteration 189/1000 | Loss: 0.00000781
Iteration 190/1000 | Loss: 0.00000781
Iteration 191/1000 | Loss: 0.00000781
Iteration 192/1000 | Loss: 0.00000781
Iteration 193/1000 | Loss: 0.00000781
Iteration 194/1000 | Loss: 0.00000781
Iteration 195/1000 | Loss: 0.00000781
Iteration 196/1000 | Loss: 0.00000780
Iteration 197/1000 | Loss: 0.00000780
Iteration 198/1000 | Loss: 0.00000780
Iteration 199/1000 | Loss: 0.00000780
Iteration 200/1000 | Loss: 0.00000780
Iteration 201/1000 | Loss: 0.00000780
Iteration 202/1000 | Loss: 0.00000780
Iteration 203/1000 | Loss: 0.00000780
Iteration 204/1000 | Loss: 0.00000780
Iteration 205/1000 | Loss: 0.00000780
Iteration 206/1000 | Loss: 0.00000780
Iteration 207/1000 | Loss: 0.00000780
Iteration 208/1000 | Loss: 0.00000780
Iteration 209/1000 | Loss: 0.00000779
Iteration 210/1000 | Loss: 0.00000779
Iteration 211/1000 | Loss: 0.00000779
Iteration 212/1000 | Loss: 0.00000778
Iteration 213/1000 | Loss: 0.00000778
Iteration 214/1000 | Loss: 0.00000778
Iteration 215/1000 | Loss: 0.00000778
Iteration 216/1000 | Loss: 0.00000778
Iteration 217/1000 | Loss: 0.00000778
Iteration 218/1000 | Loss: 0.00000778
Iteration 219/1000 | Loss: 0.00000778
Iteration 220/1000 | Loss: 0.00000778
Iteration 221/1000 | Loss: 0.00000778
Iteration 222/1000 | Loss: 0.00000778
Iteration 223/1000 | Loss: 0.00000778
Iteration 224/1000 | Loss: 0.00000778
Iteration 225/1000 | Loss: 0.00000778
Iteration 226/1000 | Loss: 0.00000778
Iteration 227/1000 | Loss: 0.00000778
Iteration 228/1000 | Loss: 0.00000778
Iteration 229/1000 | Loss: 0.00000778
Iteration 230/1000 | Loss: 0.00000778
Iteration 231/1000 | Loss: 0.00000778
Iteration 232/1000 | Loss: 0.00000778
Iteration 233/1000 | Loss: 0.00000778
Iteration 234/1000 | Loss: 0.00000778
Iteration 235/1000 | Loss: 0.00000778
Iteration 236/1000 | Loss: 0.00000778
Iteration 237/1000 | Loss: 0.00000778
Iteration 238/1000 | Loss: 0.00000778
Iteration 239/1000 | Loss: 0.00000778
Iteration 240/1000 | Loss: 0.00000778
Iteration 241/1000 | Loss: 0.00000778
Iteration 242/1000 | Loss: 0.00000778
Iteration 243/1000 | Loss: 0.00000778
Iteration 244/1000 | Loss: 0.00000778
Iteration 245/1000 | Loss: 0.00000778
Iteration 246/1000 | Loss: 0.00000778
Iteration 247/1000 | Loss: 0.00000778
Iteration 248/1000 | Loss: 0.00000778
Iteration 249/1000 | Loss: 0.00000778
Iteration 250/1000 | Loss: 0.00000778
Iteration 251/1000 | Loss: 0.00000778
Iteration 252/1000 | Loss: 0.00000778
Iteration 253/1000 | Loss: 0.00000778
Iteration 254/1000 | Loss: 0.00000778
Iteration 255/1000 | Loss: 0.00000778
Iteration 256/1000 | Loss: 0.00000778
Iteration 257/1000 | Loss: 0.00000778
Iteration 258/1000 | Loss: 0.00000778
Iteration 259/1000 | Loss: 0.00000778
Iteration 260/1000 | Loss: 0.00000778
Iteration 261/1000 | Loss: 0.00000778
Iteration 262/1000 | Loss: 0.00000778
Iteration 263/1000 | Loss: 0.00000778
Iteration 264/1000 | Loss: 0.00000778
Iteration 265/1000 | Loss: 0.00000778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [7.780653504596557e-06, 7.780653504596557e-06, 7.780653504596557e-06, 7.780653504596557e-06, 7.780653504596557e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.780653504596557e-06

Optimization complete. Final v2v error: 2.3962388038635254 mm

Highest mean error: 2.9090940952301025 mm for frame 76

Lowest mean error: 2.276512384414673 mm for frame 114

Saving results

Total time: 41.11615252494812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794694
Iteration 2/25 | Loss: 0.00128817
Iteration 3/25 | Loss: 0.00107740
Iteration 4/25 | Loss: 0.00106236
Iteration 5/25 | Loss: 0.00105707
Iteration 6/25 | Loss: 0.00105542
Iteration 7/25 | Loss: 0.00105532
Iteration 8/25 | Loss: 0.00105532
Iteration 9/25 | Loss: 0.00105532
Iteration 10/25 | Loss: 0.00105532
Iteration 11/25 | Loss: 0.00105532
Iteration 12/25 | Loss: 0.00105532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010553160682320595, 0.0010553160682320595, 0.0010553160682320595, 0.0010553160682320595, 0.0010553160682320595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010553160682320595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20699263
Iteration 2/25 | Loss: 0.00078825
Iteration 3/25 | Loss: 0.00078825
Iteration 4/25 | Loss: 0.00078824
Iteration 5/25 | Loss: 0.00078824
Iteration 6/25 | Loss: 0.00078824
Iteration 7/25 | Loss: 0.00078824
Iteration 8/25 | Loss: 0.00078824
Iteration 9/25 | Loss: 0.00078824
Iteration 10/25 | Loss: 0.00078824
Iteration 11/25 | Loss: 0.00078824
Iteration 12/25 | Loss: 0.00078824
Iteration 13/25 | Loss: 0.00078824
Iteration 14/25 | Loss: 0.00078824
Iteration 15/25 | Loss: 0.00078824
Iteration 16/25 | Loss: 0.00078824
Iteration 17/25 | Loss: 0.00078824
Iteration 18/25 | Loss: 0.00078824
Iteration 19/25 | Loss: 0.00078824
Iteration 20/25 | Loss: 0.00078824
Iteration 21/25 | Loss: 0.00078824
Iteration 22/25 | Loss: 0.00078824
Iteration 23/25 | Loss: 0.00078824
Iteration 24/25 | Loss: 0.00078824
Iteration 25/25 | Loss: 0.00078824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078824
Iteration 2/1000 | Loss: 0.00004090
Iteration 3/1000 | Loss: 0.00002406
Iteration 4/1000 | Loss: 0.00001835
Iteration 5/1000 | Loss: 0.00001625
Iteration 6/1000 | Loss: 0.00001508
Iteration 7/1000 | Loss: 0.00001430
Iteration 8/1000 | Loss: 0.00001371
Iteration 9/1000 | Loss: 0.00001326
Iteration 10/1000 | Loss: 0.00001298
Iteration 11/1000 | Loss: 0.00001274
Iteration 12/1000 | Loss: 0.00001259
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001257
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001250
Iteration 17/1000 | Loss: 0.00001248
Iteration 18/1000 | Loss: 0.00001246
Iteration 19/1000 | Loss: 0.00001242
Iteration 20/1000 | Loss: 0.00001238
Iteration 21/1000 | Loss: 0.00001238
Iteration 22/1000 | Loss: 0.00001236
Iteration 23/1000 | Loss: 0.00001235
Iteration 24/1000 | Loss: 0.00001235
Iteration 25/1000 | Loss: 0.00001234
Iteration 26/1000 | Loss: 0.00001233
Iteration 27/1000 | Loss: 0.00001233
Iteration 28/1000 | Loss: 0.00001226
Iteration 29/1000 | Loss: 0.00001223
Iteration 30/1000 | Loss: 0.00001221
Iteration 31/1000 | Loss: 0.00001221
Iteration 32/1000 | Loss: 0.00001219
Iteration 33/1000 | Loss: 0.00001218
Iteration 34/1000 | Loss: 0.00001217
Iteration 35/1000 | Loss: 0.00001216
Iteration 36/1000 | Loss: 0.00001216
Iteration 37/1000 | Loss: 0.00001215
Iteration 38/1000 | Loss: 0.00001214
Iteration 39/1000 | Loss: 0.00001214
Iteration 40/1000 | Loss: 0.00001213
Iteration 41/1000 | Loss: 0.00001213
Iteration 42/1000 | Loss: 0.00001212
Iteration 43/1000 | Loss: 0.00001212
Iteration 44/1000 | Loss: 0.00001212
Iteration 45/1000 | Loss: 0.00001211
Iteration 46/1000 | Loss: 0.00001211
Iteration 47/1000 | Loss: 0.00001211
Iteration 48/1000 | Loss: 0.00001210
Iteration 49/1000 | Loss: 0.00001210
Iteration 50/1000 | Loss: 0.00001209
Iteration 51/1000 | Loss: 0.00001209
Iteration 52/1000 | Loss: 0.00001208
Iteration 53/1000 | Loss: 0.00001208
Iteration 54/1000 | Loss: 0.00001208
Iteration 55/1000 | Loss: 0.00001207
Iteration 56/1000 | Loss: 0.00001207
Iteration 57/1000 | Loss: 0.00001204
Iteration 58/1000 | Loss: 0.00001204
Iteration 59/1000 | Loss: 0.00001200
Iteration 60/1000 | Loss: 0.00001200
Iteration 61/1000 | Loss: 0.00001199
Iteration 62/1000 | Loss: 0.00001199
Iteration 63/1000 | Loss: 0.00001199
Iteration 64/1000 | Loss: 0.00001198
Iteration 65/1000 | Loss: 0.00001197
Iteration 66/1000 | Loss: 0.00001197
Iteration 67/1000 | Loss: 0.00001196
Iteration 68/1000 | Loss: 0.00001196
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001195
Iteration 72/1000 | Loss: 0.00001195
Iteration 73/1000 | Loss: 0.00001195
Iteration 74/1000 | Loss: 0.00001194
Iteration 75/1000 | Loss: 0.00001194
Iteration 76/1000 | Loss: 0.00001194
Iteration 77/1000 | Loss: 0.00001194
Iteration 78/1000 | Loss: 0.00001194
Iteration 79/1000 | Loss: 0.00001194
Iteration 80/1000 | Loss: 0.00001194
Iteration 81/1000 | Loss: 0.00001194
Iteration 82/1000 | Loss: 0.00001194
Iteration 83/1000 | Loss: 0.00001193
Iteration 84/1000 | Loss: 0.00001193
Iteration 85/1000 | Loss: 0.00001193
Iteration 86/1000 | Loss: 0.00001192
Iteration 87/1000 | Loss: 0.00001192
Iteration 88/1000 | Loss: 0.00001192
Iteration 89/1000 | Loss: 0.00001192
Iteration 90/1000 | Loss: 0.00001192
Iteration 91/1000 | Loss: 0.00001192
Iteration 92/1000 | Loss: 0.00001192
Iteration 93/1000 | Loss: 0.00001192
Iteration 94/1000 | Loss: 0.00001191
Iteration 95/1000 | Loss: 0.00001191
Iteration 96/1000 | Loss: 0.00001191
Iteration 97/1000 | Loss: 0.00001191
Iteration 98/1000 | Loss: 0.00001191
Iteration 99/1000 | Loss: 0.00001191
Iteration 100/1000 | Loss: 0.00001191
Iteration 101/1000 | Loss: 0.00001191
Iteration 102/1000 | Loss: 0.00001190
Iteration 103/1000 | Loss: 0.00001190
Iteration 104/1000 | Loss: 0.00001190
Iteration 105/1000 | Loss: 0.00001190
Iteration 106/1000 | Loss: 0.00001190
Iteration 107/1000 | Loss: 0.00001190
Iteration 108/1000 | Loss: 0.00001189
Iteration 109/1000 | Loss: 0.00001189
Iteration 110/1000 | Loss: 0.00001189
Iteration 111/1000 | Loss: 0.00001189
Iteration 112/1000 | Loss: 0.00001189
Iteration 113/1000 | Loss: 0.00001189
Iteration 114/1000 | Loss: 0.00001189
Iteration 115/1000 | Loss: 0.00001189
Iteration 116/1000 | Loss: 0.00001188
Iteration 117/1000 | Loss: 0.00001188
Iteration 118/1000 | Loss: 0.00001188
Iteration 119/1000 | Loss: 0.00001188
Iteration 120/1000 | Loss: 0.00001188
Iteration 121/1000 | Loss: 0.00001187
Iteration 122/1000 | Loss: 0.00001187
Iteration 123/1000 | Loss: 0.00001187
Iteration 124/1000 | Loss: 0.00001187
Iteration 125/1000 | Loss: 0.00001187
Iteration 126/1000 | Loss: 0.00001186
Iteration 127/1000 | Loss: 0.00001186
Iteration 128/1000 | Loss: 0.00001186
Iteration 129/1000 | Loss: 0.00001185
Iteration 130/1000 | Loss: 0.00001185
Iteration 131/1000 | Loss: 0.00001185
Iteration 132/1000 | Loss: 0.00001185
Iteration 133/1000 | Loss: 0.00001185
Iteration 134/1000 | Loss: 0.00001185
Iteration 135/1000 | Loss: 0.00001185
Iteration 136/1000 | Loss: 0.00001185
Iteration 137/1000 | Loss: 0.00001185
Iteration 138/1000 | Loss: 0.00001185
Iteration 139/1000 | Loss: 0.00001185
Iteration 140/1000 | Loss: 0.00001185
Iteration 141/1000 | Loss: 0.00001185
Iteration 142/1000 | Loss: 0.00001185
Iteration 143/1000 | Loss: 0.00001185
Iteration 144/1000 | Loss: 0.00001185
Iteration 145/1000 | Loss: 0.00001185
Iteration 146/1000 | Loss: 0.00001185
Iteration 147/1000 | Loss: 0.00001184
Iteration 148/1000 | Loss: 0.00001184
Iteration 149/1000 | Loss: 0.00001184
Iteration 150/1000 | Loss: 0.00001184
Iteration 151/1000 | Loss: 0.00001184
Iteration 152/1000 | Loss: 0.00001183
Iteration 153/1000 | Loss: 0.00001183
Iteration 154/1000 | Loss: 0.00001183
Iteration 155/1000 | Loss: 0.00001183
Iteration 156/1000 | Loss: 0.00001182
Iteration 157/1000 | Loss: 0.00001182
Iteration 158/1000 | Loss: 0.00001182
Iteration 159/1000 | Loss: 0.00001182
Iteration 160/1000 | Loss: 0.00001182
Iteration 161/1000 | Loss: 0.00001182
Iteration 162/1000 | Loss: 0.00001182
Iteration 163/1000 | Loss: 0.00001182
Iteration 164/1000 | Loss: 0.00001182
Iteration 165/1000 | Loss: 0.00001182
Iteration 166/1000 | Loss: 0.00001182
Iteration 167/1000 | Loss: 0.00001182
Iteration 168/1000 | Loss: 0.00001182
Iteration 169/1000 | Loss: 0.00001182
Iteration 170/1000 | Loss: 0.00001182
Iteration 171/1000 | Loss: 0.00001182
Iteration 172/1000 | Loss: 0.00001182
Iteration 173/1000 | Loss: 0.00001182
Iteration 174/1000 | Loss: 0.00001182
Iteration 175/1000 | Loss: 0.00001182
Iteration 176/1000 | Loss: 0.00001182
Iteration 177/1000 | Loss: 0.00001182
Iteration 178/1000 | Loss: 0.00001182
Iteration 179/1000 | Loss: 0.00001182
Iteration 180/1000 | Loss: 0.00001182
Iteration 181/1000 | Loss: 0.00001182
Iteration 182/1000 | Loss: 0.00001182
Iteration 183/1000 | Loss: 0.00001182
Iteration 184/1000 | Loss: 0.00001182
Iteration 185/1000 | Loss: 0.00001182
Iteration 186/1000 | Loss: 0.00001182
Iteration 187/1000 | Loss: 0.00001182
Iteration 188/1000 | Loss: 0.00001182
Iteration 189/1000 | Loss: 0.00001182
Iteration 190/1000 | Loss: 0.00001182
Iteration 191/1000 | Loss: 0.00001182
Iteration 192/1000 | Loss: 0.00001182
Iteration 193/1000 | Loss: 0.00001182
Iteration 194/1000 | Loss: 0.00001182
Iteration 195/1000 | Loss: 0.00001182
Iteration 196/1000 | Loss: 0.00001182
Iteration 197/1000 | Loss: 0.00001182
Iteration 198/1000 | Loss: 0.00001182
Iteration 199/1000 | Loss: 0.00001182
Iteration 200/1000 | Loss: 0.00001182
Iteration 201/1000 | Loss: 0.00001182
Iteration 202/1000 | Loss: 0.00001182
Iteration 203/1000 | Loss: 0.00001182
Iteration 204/1000 | Loss: 0.00001182
Iteration 205/1000 | Loss: 0.00001182
Iteration 206/1000 | Loss: 0.00001182
Iteration 207/1000 | Loss: 0.00001182
Iteration 208/1000 | Loss: 0.00001182
Iteration 209/1000 | Loss: 0.00001182
Iteration 210/1000 | Loss: 0.00001182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.1819422070402652e-05, 1.1819422070402652e-05, 1.1819422070402652e-05, 1.1819422070402652e-05, 1.1819422070402652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1819422070402652e-05

Optimization complete. Final v2v error: 2.8035888671875 mm

Highest mean error: 4.214213848114014 mm for frame 75

Lowest mean error: 2.2971067428588867 mm for frame 107

Saving results

Total time: 43.85179042816162
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457588
Iteration 2/25 | Loss: 0.00122054
Iteration 3/25 | Loss: 0.00112538
Iteration 4/25 | Loss: 0.00110902
Iteration 5/25 | Loss: 0.00110375
Iteration 6/25 | Loss: 0.00110317
Iteration 7/25 | Loss: 0.00110317
Iteration 8/25 | Loss: 0.00110317
Iteration 9/25 | Loss: 0.00110317
Iteration 10/25 | Loss: 0.00110317
Iteration 11/25 | Loss: 0.00110317
Iteration 12/25 | Loss: 0.00110317
Iteration 13/25 | Loss: 0.00110317
Iteration 14/25 | Loss: 0.00110317
Iteration 15/25 | Loss: 0.00110317
Iteration 16/25 | Loss: 0.00110317
Iteration 17/25 | Loss: 0.00110317
Iteration 18/25 | Loss: 0.00110317
Iteration 19/25 | Loss: 0.00110317
Iteration 20/25 | Loss: 0.00110317
Iteration 21/25 | Loss: 0.00110317
Iteration 22/25 | Loss: 0.00110317
Iteration 23/25 | Loss: 0.00110317
Iteration 24/25 | Loss: 0.00110317
Iteration 25/25 | Loss: 0.00110317

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43854296
Iteration 2/25 | Loss: 0.00070709
Iteration 3/25 | Loss: 0.00070709
Iteration 4/25 | Loss: 0.00070709
Iteration 5/25 | Loss: 0.00070709
Iteration 6/25 | Loss: 0.00070709
Iteration 7/25 | Loss: 0.00070709
Iteration 8/25 | Loss: 0.00070709
Iteration 9/25 | Loss: 0.00070709
Iteration 10/25 | Loss: 0.00070709
Iteration 11/25 | Loss: 0.00070709
Iteration 12/25 | Loss: 0.00070709
Iteration 13/25 | Loss: 0.00070709
Iteration 14/25 | Loss: 0.00070709
Iteration 15/25 | Loss: 0.00070709
Iteration 16/25 | Loss: 0.00070709
Iteration 17/25 | Loss: 0.00070709
Iteration 18/25 | Loss: 0.00070709
Iteration 19/25 | Loss: 0.00070709
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007070885039865971, 0.0007070885039865971, 0.0007070885039865971, 0.0007070885039865971, 0.0007070885039865971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007070885039865971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070709
Iteration 2/1000 | Loss: 0.00003196
Iteration 3/1000 | Loss: 0.00002221
Iteration 4/1000 | Loss: 0.00002056
Iteration 5/1000 | Loss: 0.00001975
Iteration 6/1000 | Loss: 0.00001923
Iteration 7/1000 | Loss: 0.00001868
Iteration 8/1000 | Loss: 0.00001828
Iteration 9/1000 | Loss: 0.00001798
Iteration 10/1000 | Loss: 0.00001771
Iteration 11/1000 | Loss: 0.00001761
Iteration 12/1000 | Loss: 0.00001759
Iteration 13/1000 | Loss: 0.00001742
Iteration 14/1000 | Loss: 0.00001731
Iteration 15/1000 | Loss: 0.00001726
Iteration 16/1000 | Loss: 0.00001716
Iteration 17/1000 | Loss: 0.00001715
Iteration 18/1000 | Loss: 0.00001715
Iteration 19/1000 | Loss: 0.00001713
Iteration 20/1000 | Loss: 0.00001712
Iteration 21/1000 | Loss: 0.00001712
Iteration 22/1000 | Loss: 0.00001711
Iteration 23/1000 | Loss: 0.00001711
Iteration 24/1000 | Loss: 0.00001710
Iteration 25/1000 | Loss: 0.00001709
Iteration 26/1000 | Loss: 0.00001708
Iteration 27/1000 | Loss: 0.00001701
Iteration 28/1000 | Loss: 0.00001696
Iteration 29/1000 | Loss: 0.00001692
Iteration 30/1000 | Loss: 0.00001691
Iteration 31/1000 | Loss: 0.00001690
Iteration 32/1000 | Loss: 0.00001689
Iteration 33/1000 | Loss: 0.00001689
Iteration 34/1000 | Loss: 0.00001688
Iteration 35/1000 | Loss: 0.00001688
Iteration 36/1000 | Loss: 0.00001687
Iteration 37/1000 | Loss: 0.00001686
Iteration 38/1000 | Loss: 0.00001686
Iteration 39/1000 | Loss: 0.00001685
Iteration 40/1000 | Loss: 0.00001685
Iteration 41/1000 | Loss: 0.00001685
Iteration 42/1000 | Loss: 0.00001685
Iteration 43/1000 | Loss: 0.00001684
Iteration 44/1000 | Loss: 0.00001684
Iteration 45/1000 | Loss: 0.00001684
Iteration 46/1000 | Loss: 0.00001683
Iteration 47/1000 | Loss: 0.00001683
Iteration 48/1000 | Loss: 0.00001682
Iteration 49/1000 | Loss: 0.00001682
Iteration 50/1000 | Loss: 0.00001681
Iteration 51/1000 | Loss: 0.00001681
Iteration 52/1000 | Loss: 0.00001681
Iteration 53/1000 | Loss: 0.00001680
Iteration 54/1000 | Loss: 0.00001679
Iteration 55/1000 | Loss: 0.00001679
Iteration 56/1000 | Loss: 0.00001679
Iteration 57/1000 | Loss: 0.00001678
Iteration 58/1000 | Loss: 0.00001678
Iteration 59/1000 | Loss: 0.00001678
Iteration 60/1000 | Loss: 0.00001678
Iteration 61/1000 | Loss: 0.00001677
Iteration 62/1000 | Loss: 0.00001677
Iteration 63/1000 | Loss: 0.00001677
Iteration 64/1000 | Loss: 0.00001677
Iteration 65/1000 | Loss: 0.00001676
Iteration 66/1000 | Loss: 0.00001676
Iteration 67/1000 | Loss: 0.00001676
Iteration 68/1000 | Loss: 0.00001676
Iteration 69/1000 | Loss: 0.00001676
Iteration 70/1000 | Loss: 0.00001675
Iteration 71/1000 | Loss: 0.00001675
Iteration 72/1000 | Loss: 0.00001675
Iteration 73/1000 | Loss: 0.00001674
Iteration 74/1000 | Loss: 0.00001674
Iteration 75/1000 | Loss: 0.00001674
Iteration 76/1000 | Loss: 0.00001673
Iteration 77/1000 | Loss: 0.00001673
Iteration 78/1000 | Loss: 0.00001673
Iteration 79/1000 | Loss: 0.00001673
Iteration 80/1000 | Loss: 0.00001673
Iteration 81/1000 | Loss: 0.00001672
Iteration 82/1000 | Loss: 0.00001672
Iteration 83/1000 | Loss: 0.00001672
Iteration 84/1000 | Loss: 0.00001672
Iteration 85/1000 | Loss: 0.00001672
Iteration 86/1000 | Loss: 0.00001672
Iteration 87/1000 | Loss: 0.00001672
Iteration 88/1000 | Loss: 0.00001672
Iteration 89/1000 | Loss: 0.00001671
Iteration 90/1000 | Loss: 0.00001671
Iteration 91/1000 | Loss: 0.00001671
Iteration 92/1000 | Loss: 0.00001670
Iteration 93/1000 | Loss: 0.00001670
Iteration 94/1000 | Loss: 0.00001670
Iteration 95/1000 | Loss: 0.00001670
Iteration 96/1000 | Loss: 0.00001670
Iteration 97/1000 | Loss: 0.00001670
Iteration 98/1000 | Loss: 0.00001670
Iteration 99/1000 | Loss: 0.00001670
Iteration 100/1000 | Loss: 0.00001670
Iteration 101/1000 | Loss: 0.00001670
Iteration 102/1000 | Loss: 0.00001670
Iteration 103/1000 | Loss: 0.00001669
Iteration 104/1000 | Loss: 0.00001669
Iteration 105/1000 | Loss: 0.00001669
Iteration 106/1000 | Loss: 0.00001669
Iteration 107/1000 | Loss: 0.00001669
Iteration 108/1000 | Loss: 0.00001669
Iteration 109/1000 | Loss: 0.00001669
Iteration 110/1000 | Loss: 0.00001669
Iteration 111/1000 | Loss: 0.00001669
Iteration 112/1000 | Loss: 0.00001669
Iteration 113/1000 | Loss: 0.00001669
Iteration 114/1000 | Loss: 0.00001669
Iteration 115/1000 | Loss: 0.00001669
Iteration 116/1000 | Loss: 0.00001669
Iteration 117/1000 | Loss: 0.00001669
Iteration 118/1000 | Loss: 0.00001669
Iteration 119/1000 | Loss: 0.00001669
Iteration 120/1000 | Loss: 0.00001668
Iteration 121/1000 | Loss: 0.00001668
Iteration 122/1000 | Loss: 0.00001668
Iteration 123/1000 | Loss: 0.00001668
Iteration 124/1000 | Loss: 0.00001668
Iteration 125/1000 | Loss: 0.00001668
Iteration 126/1000 | Loss: 0.00001667
Iteration 127/1000 | Loss: 0.00001667
Iteration 128/1000 | Loss: 0.00001667
Iteration 129/1000 | Loss: 0.00001667
Iteration 130/1000 | Loss: 0.00001667
Iteration 131/1000 | Loss: 0.00001667
Iteration 132/1000 | Loss: 0.00001667
Iteration 133/1000 | Loss: 0.00001667
Iteration 134/1000 | Loss: 0.00001667
Iteration 135/1000 | Loss: 0.00001667
Iteration 136/1000 | Loss: 0.00001667
Iteration 137/1000 | Loss: 0.00001667
Iteration 138/1000 | Loss: 0.00001667
Iteration 139/1000 | Loss: 0.00001667
Iteration 140/1000 | Loss: 0.00001667
Iteration 141/1000 | Loss: 0.00001667
Iteration 142/1000 | Loss: 0.00001667
Iteration 143/1000 | Loss: 0.00001667
Iteration 144/1000 | Loss: 0.00001667
Iteration 145/1000 | Loss: 0.00001667
Iteration 146/1000 | Loss: 0.00001667
Iteration 147/1000 | Loss: 0.00001667
Iteration 148/1000 | Loss: 0.00001667
Iteration 149/1000 | Loss: 0.00001667
Iteration 150/1000 | Loss: 0.00001667
Iteration 151/1000 | Loss: 0.00001667
Iteration 152/1000 | Loss: 0.00001667
Iteration 153/1000 | Loss: 0.00001667
Iteration 154/1000 | Loss: 0.00001667
Iteration 155/1000 | Loss: 0.00001667
Iteration 156/1000 | Loss: 0.00001667
Iteration 157/1000 | Loss: 0.00001667
Iteration 158/1000 | Loss: 0.00001667
Iteration 159/1000 | Loss: 0.00001667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.6666050214553252e-05, 1.6666050214553252e-05, 1.6666050214553252e-05, 1.6666050214553252e-05, 1.6666050214553252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6666050214553252e-05

Optimization complete. Final v2v error: 3.4615182876586914 mm

Highest mean error: 3.8123841285705566 mm for frame 32

Lowest mean error: 3.188732624053955 mm for frame 89

Saving results

Total time: 40.37201380729675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787857
Iteration 2/25 | Loss: 0.00129101
Iteration 3/25 | Loss: 0.00119321
Iteration 4/25 | Loss: 0.00118934
Iteration 5/25 | Loss: 0.00118822
Iteration 6/25 | Loss: 0.00118822
Iteration 7/25 | Loss: 0.00118822
Iteration 8/25 | Loss: 0.00118822
Iteration 9/25 | Loss: 0.00118822
Iteration 10/25 | Loss: 0.00118822
Iteration 11/25 | Loss: 0.00118822
Iteration 12/25 | Loss: 0.00118822
Iteration 13/25 | Loss: 0.00118822
Iteration 14/25 | Loss: 0.00118822
Iteration 15/25 | Loss: 0.00118822
Iteration 16/25 | Loss: 0.00118822
Iteration 17/25 | Loss: 0.00118822
Iteration 18/25 | Loss: 0.00118822
Iteration 19/25 | Loss: 0.00118822
Iteration 20/25 | Loss: 0.00118822
Iteration 21/25 | Loss: 0.00118822
Iteration 22/25 | Loss: 0.00118822
Iteration 23/25 | Loss: 0.00118822
Iteration 24/25 | Loss: 0.00118822
Iteration 25/25 | Loss: 0.00118822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96581888
Iteration 2/25 | Loss: 0.00066792
Iteration 3/25 | Loss: 0.00066789
Iteration 4/25 | Loss: 0.00066789
Iteration 5/25 | Loss: 0.00066789
Iteration 6/25 | Loss: 0.00066789
Iteration 7/25 | Loss: 0.00066789
Iteration 8/25 | Loss: 0.00066789
Iteration 9/25 | Loss: 0.00066789
Iteration 10/25 | Loss: 0.00066789
Iteration 11/25 | Loss: 0.00066789
Iteration 12/25 | Loss: 0.00066789
Iteration 13/25 | Loss: 0.00066789
Iteration 14/25 | Loss: 0.00066789
Iteration 15/25 | Loss: 0.00066789
Iteration 16/25 | Loss: 0.00066789
Iteration 17/25 | Loss: 0.00066789
Iteration 18/25 | Loss: 0.00066789
Iteration 19/25 | Loss: 0.00066789
Iteration 20/25 | Loss: 0.00066789
Iteration 21/25 | Loss: 0.00066789
Iteration 22/25 | Loss: 0.00066789
Iteration 23/25 | Loss: 0.00066789
Iteration 24/25 | Loss: 0.00066789
Iteration 25/25 | Loss: 0.00066789

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066789
Iteration 2/1000 | Loss: 0.00002717
Iteration 3/1000 | Loss: 0.00002077
Iteration 4/1000 | Loss: 0.00001879
Iteration 5/1000 | Loss: 0.00001816
Iteration 6/1000 | Loss: 0.00001779
Iteration 7/1000 | Loss: 0.00001752
Iteration 8/1000 | Loss: 0.00001729
Iteration 9/1000 | Loss: 0.00001705
Iteration 10/1000 | Loss: 0.00001687
Iteration 11/1000 | Loss: 0.00001684
Iteration 12/1000 | Loss: 0.00001679
Iteration 13/1000 | Loss: 0.00001673
Iteration 14/1000 | Loss: 0.00001666
Iteration 15/1000 | Loss: 0.00001663
Iteration 16/1000 | Loss: 0.00001662
Iteration 17/1000 | Loss: 0.00001662
Iteration 18/1000 | Loss: 0.00001662
Iteration 19/1000 | Loss: 0.00001662
Iteration 20/1000 | Loss: 0.00001660
Iteration 21/1000 | Loss: 0.00001660
Iteration 22/1000 | Loss: 0.00001659
Iteration 23/1000 | Loss: 0.00001658
Iteration 24/1000 | Loss: 0.00001658
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001658
Iteration 27/1000 | Loss: 0.00001657
Iteration 28/1000 | Loss: 0.00001657
Iteration 29/1000 | Loss: 0.00001657
Iteration 30/1000 | Loss: 0.00001656
Iteration 31/1000 | Loss: 0.00001656
Iteration 32/1000 | Loss: 0.00001656
Iteration 33/1000 | Loss: 0.00001656
Iteration 34/1000 | Loss: 0.00001655
Iteration 35/1000 | Loss: 0.00001655
Iteration 36/1000 | Loss: 0.00001655
Iteration 37/1000 | Loss: 0.00001654
Iteration 38/1000 | Loss: 0.00001654
Iteration 39/1000 | Loss: 0.00001653
Iteration 40/1000 | Loss: 0.00001652
Iteration 41/1000 | Loss: 0.00001651
Iteration 42/1000 | Loss: 0.00001651
Iteration 43/1000 | Loss: 0.00001651
Iteration 44/1000 | Loss: 0.00001648
Iteration 45/1000 | Loss: 0.00001648
Iteration 46/1000 | Loss: 0.00001647
Iteration 47/1000 | Loss: 0.00001647
Iteration 48/1000 | Loss: 0.00001647
Iteration 49/1000 | Loss: 0.00001646
Iteration 50/1000 | Loss: 0.00001646
Iteration 51/1000 | Loss: 0.00001646
Iteration 52/1000 | Loss: 0.00001645
Iteration 53/1000 | Loss: 0.00001645
Iteration 54/1000 | Loss: 0.00001644
Iteration 55/1000 | Loss: 0.00001644
Iteration 56/1000 | Loss: 0.00001643
Iteration 57/1000 | Loss: 0.00001643
Iteration 58/1000 | Loss: 0.00001643
Iteration 59/1000 | Loss: 0.00001643
Iteration 60/1000 | Loss: 0.00001643
Iteration 61/1000 | Loss: 0.00001643
Iteration 62/1000 | Loss: 0.00001643
Iteration 63/1000 | Loss: 0.00001642
Iteration 64/1000 | Loss: 0.00001641
Iteration 65/1000 | Loss: 0.00001641
Iteration 66/1000 | Loss: 0.00001640
Iteration 67/1000 | Loss: 0.00001640
Iteration 68/1000 | Loss: 0.00001639
Iteration 69/1000 | Loss: 0.00001639
Iteration 70/1000 | Loss: 0.00001639
Iteration 71/1000 | Loss: 0.00001637
Iteration 72/1000 | Loss: 0.00001637
Iteration 73/1000 | Loss: 0.00001636
Iteration 74/1000 | Loss: 0.00001636
Iteration 75/1000 | Loss: 0.00001636
Iteration 76/1000 | Loss: 0.00001636
Iteration 77/1000 | Loss: 0.00001635
Iteration 78/1000 | Loss: 0.00001635
Iteration 79/1000 | Loss: 0.00001635
Iteration 80/1000 | Loss: 0.00001634
Iteration 81/1000 | Loss: 0.00001634
Iteration 82/1000 | Loss: 0.00001634
Iteration 83/1000 | Loss: 0.00001634
Iteration 84/1000 | Loss: 0.00001634
Iteration 85/1000 | Loss: 0.00001634
Iteration 86/1000 | Loss: 0.00001633
Iteration 87/1000 | Loss: 0.00001633
Iteration 88/1000 | Loss: 0.00001633
Iteration 89/1000 | Loss: 0.00001633
Iteration 90/1000 | Loss: 0.00001633
Iteration 91/1000 | Loss: 0.00001633
Iteration 92/1000 | Loss: 0.00001633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.6333917301381007e-05, 1.6333917301381007e-05, 1.6333917301381007e-05, 1.6333917301381007e-05, 1.6333917301381007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6333917301381007e-05

Optimization complete. Final v2v error: 3.332275152206421 mm

Highest mean error: 3.8333096504211426 mm for frame 172

Lowest mean error: 3.0289204120635986 mm for frame 58

Saving results

Total time: 36.22288274765015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999210
Iteration 2/25 | Loss: 0.00156024
Iteration 3/25 | Loss: 0.00121323
Iteration 4/25 | Loss: 0.00116082
Iteration 5/25 | Loss: 0.00114634
Iteration 6/25 | Loss: 0.00115119
Iteration 7/25 | Loss: 0.00114226
Iteration 8/25 | Loss: 0.00113083
Iteration 9/25 | Loss: 0.00112424
Iteration 10/25 | Loss: 0.00112856
Iteration 11/25 | Loss: 0.00112839
Iteration 12/25 | Loss: 0.00112329
Iteration 13/25 | Loss: 0.00111955
Iteration 14/25 | Loss: 0.00111910
Iteration 15/25 | Loss: 0.00111988
Iteration 16/25 | Loss: 0.00111952
Iteration 17/25 | Loss: 0.00111836
Iteration 18/25 | Loss: 0.00111899
Iteration 19/25 | Loss: 0.00111687
Iteration 20/25 | Loss: 0.00111491
Iteration 21/25 | Loss: 0.00111436
Iteration 22/25 | Loss: 0.00111453
Iteration 23/25 | Loss: 0.00111481
Iteration 24/25 | Loss: 0.00111269
Iteration 25/25 | Loss: 0.00111254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45281124
Iteration 2/25 | Loss: 0.00090100
Iteration 3/25 | Loss: 0.00090100
Iteration 4/25 | Loss: 0.00090100
Iteration 5/25 | Loss: 0.00090100
Iteration 6/25 | Loss: 0.00090100
Iteration 7/25 | Loss: 0.00090100
Iteration 8/25 | Loss: 0.00090100
Iteration 9/25 | Loss: 0.00090100
Iteration 10/25 | Loss: 0.00090100
Iteration 11/25 | Loss: 0.00090100
Iteration 12/25 | Loss: 0.00090100
Iteration 13/25 | Loss: 0.00090100
Iteration 14/25 | Loss: 0.00090100
Iteration 15/25 | Loss: 0.00090100
Iteration 16/25 | Loss: 0.00090100
Iteration 17/25 | Loss: 0.00090100
Iteration 18/25 | Loss: 0.00090100
Iteration 19/25 | Loss: 0.00090100
Iteration 20/25 | Loss: 0.00090100
Iteration 21/25 | Loss: 0.00090100
Iteration 22/25 | Loss: 0.00090100
Iteration 23/25 | Loss: 0.00090100
Iteration 24/25 | Loss: 0.00090100
Iteration 25/25 | Loss: 0.00090100

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090100
Iteration 2/1000 | Loss: 0.00008021
Iteration 3/1000 | Loss: 0.00006083
Iteration 4/1000 | Loss: 0.00007926
Iteration 5/1000 | Loss: 0.00002616
Iteration 6/1000 | Loss: 0.00009138
Iteration 7/1000 | Loss: 0.00005938
Iteration 8/1000 | Loss: 0.00007349
Iteration 9/1000 | Loss: 0.00005460
Iteration 10/1000 | Loss: 0.00003667
Iteration 11/1000 | Loss: 0.00011936
Iteration 12/1000 | Loss: 0.00002209
Iteration 13/1000 | Loss: 0.00001970
Iteration 14/1000 | Loss: 0.00001878
Iteration 15/1000 | Loss: 0.00012994
Iteration 16/1000 | Loss: 0.00019894
Iteration 17/1000 | Loss: 0.00003327
Iteration 18/1000 | Loss: 0.00002166
Iteration 19/1000 | Loss: 0.00018928
Iteration 20/1000 | Loss: 0.00004757
Iteration 21/1000 | Loss: 0.00002394
Iteration 22/1000 | Loss: 0.00002128
Iteration 23/1000 | Loss: 0.00001978
Iteration 24/1000 | Loss: 0.00001839
Iteration 25/1000 | Loss: 0.00001721
Iteration 26/1000 | Loss: 0.00001644
Iteration 27/1000 | Loss: 0.00001576
Iteration 28/1000 | Loss: 0.00001510
Iteration 29/1000 | Loss: 0.00001464
Iteration 30/1000 | Loss: 0.00001436
Iteration 31/1000 | Loss: 0.00001415
Iteration 32/1000 | Loss: 0.00001393
Iteration 33/1000 | Loss: 0.00001385
Iteration 34/1000 | Loss: 0.00001378
Iteration 35/1000 | Loss: 0.00001378
Iteration 36/1000 | Loss: 0.00001377
Iteration 37/1000 | Loss: 0.00001376
Iteration 38/1000 | Loss: 0.00001375
Iteration 39/1000 | Loss: 0.00001375
Iteration 40/1000 | Loss: 0.00001375
Iteration 41/1000 | Loss: 0.00001373
Iteration 42/1000 | Loss: 0.00001372
Iteration 43/1000 | Loss: 0.00001371
Iteration 44/1000 | Loss: 0.00001370
Iteration 45/1000 | Loss: 0.00001368
Iteration 46/1000 | Loss: 0.00001368
Iteration 47/1000 | Loss: 0.00001367
Iteration 48/1000 | Loss: 0.00001367
Iteration 49/1000 | Loss: 0.00001367
Iteration 50/1000 | Loss: 0.00001367
Iteration 51/1000 | Loss: 0.00001366
Iteration 52/1000 | Loss: 0.00001366
Iteration 53/1000 | Loss: 0.00001366
Iteration 54/1000 | Loss: 0.00001366
Iteration 55/1000 | Loss: 0.00001366
Iteration 56/1000 | Loss: 0.00001365
Iteration 57/1000 | Loss: 0.00001365
Iteration 58/1000 | Loss: 0.00001365
Iteration 59/1000 | Loss: 0.00001364
Iteration 60/1000 | Loss: 0.00001364
Iteration 61/1000 | Loss: 0.00001364
Iteration 62/1000 | Loss: 0.00001363
Iteration 63/1000 | Loss: 0.00001363
Iteration 64/1000 | Loss: 0.00001363
Iteration 65/1000 | Loss: 0.00001362
Iteration 66/1000 | Loss: 0.00001362
Iteration 67/1000 | Loss: 0.00001362
Iteration 68/1000 | Loss: 0.00001361
Iteration 69/1000 | Loss: 0.00001361
Iteration 70/1000 | Loss: 0.00001361
Iteration 71/1000 | Loss: 0.00001360
Iteration 72/1000 | Loss: 0.00001360
Iteration 73/1000 | Loss: 0.00001360
Iteration 74/1000 | Loss: 0.00001359
Iteration 75/1000 | Loss: 0.00001359
Iteration 76/1000 | Loss: 0.00001359
Iteration 77/1000 | Loss: 0.00001358
Iteration 78/1000 | Loss: 0.00001358
Iteration 79/1000 | Loss: 0.00001357
Iteration 80/1000 | Loss: 0.00001357
Iteration 81/1000 | Loss: 0.00001357
Iteration 82/1000 | Loss: 0.00001356
Iteration 83/1000 | Loss: 0.00001356
Iteration 84/1000 | Loss: 0.00001356
Iteration 85/1000 | Loss: 0.00001356
Iteration 86/1000 | Loss: 0.00001356
Iteration 87/1000 | Loss: 0.00001356
Iteration 88/1000 | Loss: 0.00001355
Iteration 89/1000 | Loss: 0.00001355
Iteration 90/1000 | Loss: 0.00001355
Iteration 91/1000 | Loss: 0.00001355
Iteration 92/1000 | Loss: 0.00001355
Iteration 93/1000 | Loss: 0.00001354
Iteration 94/1000 | Loss: 0.00001354
Iteration 95/1000 | Loss: 0.00001354
Iteration 96/1000 | Loss: 0.00001354
Iteration 97/1000 | Loss: 0.00001353
Iteration 98/1000 | Loss: 0.00001353
Iteration 99/1000 | Loss: 0.00001353
Iteration 100/1000 | Loss: 0.00001353
Iteration 101/1000 | Loss: 0.00001352
Iteration 102/1000 | Loss: 0.00001352
Iteration 103/1000 | Loss: 0.00001351
Iteration 104/1000 | Loss: 0.00001351
Iteration 105/1000 | Loss: 0.00001351
Iteration 106/1000 | Loss: 0.00001350
Iteration 107/1000 | Loss: 0.00001350
Iteration 108/1000 | Loss: 0.00001350
Iteration 109/1000 | Loss: 0.00001350
Iteration 110/1000 | Loss: 0.00001350
Iteration 111/1000 | Loss: 0.00001349
Iteration 112/1000 | Loss: 0.00001348
Iteration 113/1000 | Loss: 0.00001348
Iteration 114/1000 | Loss: 0.00001348
Iteration 115/1000 | Loss: 0.00001348
Iteration 116/1000 | Loss: 0.00001347
Iteration 117/1000 | Loss: 0.00001347
Iteration 118/1000 | Loss: 0.00001347
Iteration 119/1000 | Loss: 0.00001347
Iteration 120/1000 | Loss: 0.00001347
Iteration 121/1000 | Loss: 0.00001347
Iteration 122/1000 | Loss: 0.00001347
Iteration 123/1000 | Loss: 0.00001346
Iteration 124/1000 | Loss: 0.00001346
Iteration 125/1000 | Loss: 0.00001346
Iteration 126/1000 | Loss: 0.00001346
Iteration 127/1000 | Loss: 0.00001346
Iteration 128/1000 | Loss: 0.00001346
Iteration 129/1000 | Loss: 0.00001346
Iteration 130/1000 | Loss: 0.00001346
Iteration 131/1000 | Loss: 0.00001345
Iteration 132/1000 | Loss: 0.00001345
Iteration 133/1000 | Loss: 0.00001345
Iteration 134/1000 | Loss: 0.00001344
Iteration 135/1000 | Loss: 0.00001344
Iteration 136/1000 | Loss: 0.00001344
Iteration 137/1000 | Loss: 0.00001344
Iteration 138/1000 | Loss: 0.00001344
Iteration 139/1000 | Loss: 0.00001344
Iteration 140/1000 | Loss: 0.00001344
Iteration 141/1000 | Loss: 0.00001344
Iteration 142/1000 | Loss: 0.00001344
Iteration 143/1000 | Loss: 0.00001343
Iteration 144/1000 | Loss: 0.00001343
Iteration 145/1000 | Loss: 0.00001343
Iteration 146/1000 | Loss: 0.00001343
Iteration 147/1000 | Loss: 0.00001343
Iteration 148/1000 | Loss: 0.00001343
Iteration 149/1000 | Loss: 0.00001343
Iteration 150/1000 | Loss: 0.00001342
Iteration 151/1000 | Loss: 0.00001342
Iteration 152/1000 | Loss: 0.00001342
Iteration 153/1000 | Loss: 0.00001342
Iteration 154/1000 | Loss: 0.00001342
Iteration 155/1000 | Loss: 0.00001342
Iteration 156/1000 | Loss: 0.00001342
Iteration 157/1000 | Loss: 0.00001342
Iteration 158/1000 | Loss: 0.00001342
Iteration 159/1000 | Loss: 0.00001342
Iteration 160/1000 | Loss: 0.00001341
Iteration 161/1000 | Loss: 0.00001341
Iteration 162/1000 | Loss: 0.00001341
Iteration 163/1000 | Loss: 0.00001341
Iteration 164/1000 | Loss: 0.00001341
Iteration 165/1000 | Loss: 0.00001340
Iteration 166/1000 | Loss: 0.00001340
Iteration 167/1000 | Loss: 0.00001340
Iteration 168/1000 | Loss: 0.00001340
Iteration 169/1000 | Loss: 0.00001340
Iteration 170/1000 | Loss: 0.00001340
Iteration 171/1000 | Loss: 0.00001340
Iteration 172/1000 | Loss: 0.00001340
Iteration 173/1000 | Loss: 0.00001340
Iteration 174/1000 | Loss: 0.00001340
Iteration 175/1000 | Loss: 0.00001340
Iteration 176/1000 | Loss: 0.00001340
Iteration 177/1000 | Loss: 0.00001339
Iteration 178/1000 | Loss: 0.00001339
Iteration 179/1000 | Loss: 0.00001339
Iteration 180/1000 | Loss: 0.00001339
Iteration 181/1000 | Loss: 0.00001339
Iteration 182/1000 | Loss: 0.00001339
Iteration 183/1000 | Loss: 0.00001339
Iteration 184/1000 | Loss: 0.00001339
Iteration 185/1000 | Loss: 0.00001339
Iteration 186/1000 | Loss: 0.00001339
Iteration 187/1000 | Loss: 0.00001339
Iteration 188/1000 | Loss: 0.00001339
Iteration 189/1000 | Loss: 0.00001339
Iteration 190/1000 | Loss: 0.00001339
Iteration 191/1000 | Loss: 0.00001339
Iteration 192/1000 | Loss: 0.00001339
Iteration 193/1000 | Loss: 0.00001339
Iteration 194/1000 | Loss: 0.00001339
Iteration 195/1000 | Loss: 0.00001339
Iteration 196/1000 | Loss: 0.00001339
Iteration 197/1000 | Loss: 0.00001339
Iteration 198/1000 | Loss: 0.00001339
Iteration 199/1000 | Loss: 0.00001339
Iteration 200/1000 | Loss: 0.00001338
Iteration 201/1000 | Loss: 0.00001338
Iteration 202/1000 | Loss: 0.00001338
Iteration 203/1000 | Loss: 0.00001338
Iteration 204/1000 | Loss: 0.00001338
Iteration 205/1000 | Loss: 0.00001338
Iteration 206/1000 | Loss: 0.00001338
Iteration 207/1000 | Loss: 0.00001338
Iteration 208/1000 | Loss: 0.00001338
Iteration 209/1000 | Loss: 0.00001338
Iteration 210/1000 | Loss: 0.00001338
Iteration 211/1000 | Loss: 0.00001338
Iteration 212/1000 | Loss: 0.00001338
Iteration 213/1000 | Loss: 0.00001338
Iteration 214/1000 | Loss: 0.00001338
Iteration 215/1000 | Loss: 0.00001338
Iteration 216/1000 | Loss: 0.00001338
Iteration 217/1000 | Loss: 0.00001338
Iteration 218/1000 | Loss: 0.00001338
Iteration 219/1000 | Loss: 0.00001338
Iteration 220/1000 | Loss: 0.00001338
Iteration 221/1000 | Loss: 0.00001338
Iteration 222/1000 | Loss: 0.00001337
Iteration 223/1000 | Loss: 0.00001337
Iteration 224/1000 | Loss: 0.00001337
Iteration 225/1000 | Loss: 0.00001337
Iteration 226/1000 | Loss: 0.00001337
Iteration 227/1000 | Loss: 0.00001337
Iteration 228/1000 | Loss: 0.00001337
Iteration 229/1000 | Loss: 0.00001337
Iteration 230/1000 | Loss: 0.00001337
Iteration 231/1000 | Loss: 0.00001337
Iteration 232/1000 | Loss: 0.00001337
Iteration 233/1000 | Loss: 0.00001337
Iteration 234/1000 | Loss: 0.00001337
Iteration 235/1000 | Loss: 0.00001337
Iteration 236/1000 | Loss: 0.00001336
Iteration 237/1000 | Loss: 0.00001336
Iteration 238/1000 | Loss: 0.00001336
Iteration 239/1000 | Loss: 0.00001336
Iteration 240/1000 | Loss: 0.00001336
Iteration 241/1000 | Loss: 0.00001336
Iteration 242/1000 | Loss: 0.00001336
Iteration 243/1000 | Loss: 0.00001336
Iteration 244/1000 | Loss: 0.00001336
Iteration 245/1000 | Loss: 0.00001336
Iteration 246/1000 | Loss: 0.00001336
Iteration 247/1000 | Loss: 0.00001336
Iteration 248/1000 | Loss: 0.00001336
Iteration 249/1000 | Loss: 0.00001336
Iteration 250/1000 | Loss: 0.00001336
Iteration 251/1000 | Loss: 0.00001336
Iteration 252/1000 | Loss: 0.00001336
Iteration 253/1000 | Loss: 0.00001336
Iteration 254/1000 | Loss: 0.00001336
Iteration 255/1000 | Loss: 0.00001336
Iteration 256/1000 | Loss: 0.00001336
Iteration 257/1000 | Loss: 0.00001336
Iteration 258/1000 | Loss: 0.00001335
Iteration 259/1000 | Loss: 0.00001335
Iteration 260/1000 | Loss: 0.00001335
Iteration 261/1000 | Loss: 0.00001335
Iteration 262/1000 | Loss: 0.00001335
Iteration 263/1000 | Loss: 0.00001335
Iteration 264/1000 | Loss: 0.00001335
Iteration 265/1000 | Loss: 0.00001335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [1.3354476323002018e-05, 1.3354476323002018e-05, 1.3354476323002018e-05, 1.3354476323002018e-05, 1.3354476323002018e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3354476323002018e-05

Optimization complete. Final v2v error: 3.0749619007110596 mm

Highest mean error: 3.553417205810547 mm for frame 92

Lowest mean error: 2.591257095336914 mm for frame 16

Saving results

Total time: 123.56924390792847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975269
Iteration 2/25 | Loss: 0.00143392
Iteration 3/25 | Loss: 0.00118903
Iteration 4/25 | Loss: 0.00115900
Iteration 5/25 | Loss: 0.00115553
Iteration 6/25 | Loss: 0.00115547
Iteration 7/25 | Loss: 0.00115547
Iteration 8/25 | Loss: 0.00115547
Iteration 9/25 | Loss: 0.00115547
Iteration 10/25 | Loss: 0.00115547
Iteration 11/25 | Loss: 0.00115547
Iteration 12/25 | Loss: 0.00115547
Iteration 13/25 | Loss: 0.00115547
Iteration 14/25 | Loss: 0.00115547
Iteration 15/25 | Loss: 0.00115547
Iteration 16/25 | Loss: 0.00115547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011554677039384842, 0.0011554677039384842, 0.0011554677039384842, 0.0011554677039384842, 0.0011554677039384842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011554677039384842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37647605
Iteration 2/25 | Loss: 0.00054151
Iteration 3/25 | Loss: 0.00054151
Iteration 4/25 | Loss: 0.00054151
Iteration 5/25 | Loss: 0.00054151
Iteration 6/25 | Loss: 0.00054151
Iteration 7/25 | Loss: 0.00054151
Iteration 8/25 | Loss: 0.00054151
Iteration 9/25 | Loss: 0.00054151
Iteration 10/25 | Loss: 0.00054151
Iteration 11/25 | Loss: 0.00054151
Iteration 12/25 | Loss: 0.00054151
Iteration 13/25 | Loss: 0.00054151
Iteration 14/25 | Loss: 0.00054151
Iteration 15/25 | Loss: 0.00054151
Iteration 16/25 | Loss: 0.00054151
Iteration 17/25 | Loss: 0.00054151
Iteration 18/25 | Loss: 0.00054151
Iteration 19/25 | Loss: 0.00054151
Iteration 20/25 | Loss: 0.00054151
Iteration 21/25 | Loss: 0.00054151
Iteration 22/25 | Loss: 0.00054151
Iteration 23/25 | Loss: 0.00054151
Iteration 24/25 | Loss: 0.00054151
Iteration 25/25 | Loss: 0.00054151

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054151
Iteration 2/1000 | Loss: 0.00002823
Iteration 3/1000 | Loss: 0.00002035
Iteration 4/1000 | Loss: 0.00001899
Iteration 5/1000 | Loss: 0.00001835
Iteration 6/1000 | Loss: 0.00001790
Iteration 7/1000 | Loss: 0.00001756
Iteration 8/1000 | Loss: 0.00001727
Iteration 9/1000 | Loss: 0.00001714
Iteration 10/1000 | Loss: 0.00001710
Iteration 11/1000 | Loss: 0.00001686
Iteration 12/1000 | Loss: 0.00001667
Iteration 13/1000 | Loss: 0.00001660
Iteration 14/1000 | Loss: 0.00001660
Iteration 15/1000 | Loss: 0.00001654
Iteration 16/1000 | Loss: 0.00001650
Iteration 17/1000 | Loss: 0.00001647
Iteration 18/1000 | Loss: 0.00001647
Iteration 19/1000 | Loss: 0.00001646
Iteration 20/1000 | Loss: 0.00001646
Iteration 21/1000 | Loss: 0.00001646
Iteration 22/1000 | Loss: 0.00001645
Iteration 23/1000 | Loss: 0.00001645
Iteration 24/1000 | Loss: 0.00001644
Iteration 25/1000 | Loss: 0.00001644
Iteration 26/1000 | Loss: 0.00001644
Iteration 27/1000 | Loss: 0.00001644
Iteration 28/1000 | Loss: 0.00001644
Iteration 29/1000 | Loss: 0.00001643
Iteration 30/1000 | Loss: 0.00001640
Iteration 31/1000 | Loss: 0.00001640
Iteration 32/1000 | Loss: 0.00001640
Iteration 33/1000 | Loss: 0.00001640
Iteration 34/1000 | Loss: 0.00001639
Iteration 35/1000 | Loss: 0.00001639
Iteration 36/1000 | Loss: 0.00001637
Iteration 37/1000 | Loss: 0.00001637
Iteration 38/1000 | Loss: 0.00001636
Iteration 39/1000 | Loss: 0.00001635
Iteration 40/1000 | Loss: 0.00001633
Iteration 41/1000 | Loss: 0.00001633
Iteration 42/1000 | Loss: 0.00001633
Iteration 43/1000 | Loss: 0.00001633
Iteration 44/1000 | Loss: 0.00001633
Iteration 45/1000 | Loss: 0.00001633
Iteration 46/1000 | Loss: 0.00001633
Iteration 47/1000 | Loss: 0.00001633
Iteration 48/1000 | Loss: 0.00001632
Iteration 49/1000 | Loss: 0.00001632
Iteration 50/1000 | Loss: 0.00001632
Iteration 51/1000 | Loss: 0.00001632
Iteration 52/1000 | Loss: 0.00001632
Iteration 53/1000 | Loss: 0.00001632
Iteration 54/1000 | Loss: 0.00001632
Iteration 55/1000 | Loss: 0.00001632
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00001631
Iteration 58/1000 | Loss: 0.00001631
Iteration 59/1000 | Loss: 0.00001630
Iteration 60/1000 | Loss: 0.00001630
Iteration 61/1000 | Loss: 0.00001630
Iteration 62/1000 | Loss: 0.00001630
Iteration 63/1000 | Loss: 0.00001629
Iteration 64/1000 | Loss: 0.00001629
Iteration 65/1000 | Loss: 0.00001629
Iteration 66/1000 | Loss: 0.00001629
Iteration 67/1000 | Loss: 0.00001629
Iteration 68/1000 | Loss: 0.00001628
Iteration 69/1000 | Loss: 0.00001628
Iteration 70/1000 | Loss: 0.00001628
Iteration 71/1000 | Loss: 0.00001628
Iteration 72/1000 | Loss: 0.00001628
Iteration 73/1000 | Loss: 0.00001628
Iteration 74/1000 | Loss: 0.00001628
Iteration 75/1000 | Loss: 0.00001627
Iteration 76/1000 | Loss: 0.00001627
Iteration 77/1000 | Loss: 0.00001627
Iteration 78/1000 | Loss: 0.00001627
Iteration 79/1000 | Loss: 0.00001627
Iteration 80/1000 | Loss: 0.00001626
Iteration 81/1000 | Loss: 0.00001626
Iteration 82/1000 | Loss: 0.00001626
Iteration 83/1000 | Loss: 0.00001626
Iteration 84/1000 | Loss: 0.00001626
Iteration 85/1000 | Loss: 0.00001626
Iteration 86/1000 | Loss: 0.00001626
Iteration 87/1000 | Loss: 0.00001626
Iteration 88/1000 | Loss: 0.00001626
Iteration 89/1000 | Loss: 0.00001626
Iteration 90/1000 | Loss: 0.00001625
Iteration 91/1000 | Loss: 0.00001625
Iteration 92/1000 | Loss: 0.00001625
Iteration 93/1000 | Loss: 0.00001625
Iteration 94/1000 | Loss: 0.00001625
Iteration 95/1000 | Loss: 0.00001625
Iteration 96/1000 | Loss: 0.00001625
Iteration 97/1000 | Loss: 0.00001625
Iteration 98/1000 | Loss: 0.00001625
Iteration 99/1000 | Loss: 0.00001625
Iteration 100/1000 | Loss: 0.00001625
Iteration 101/1000 | Loss: 0.00001625
Iteration 102/1000 | Loss: 0.00001625
Iteration 103/1000 | Loss: 0.00001624
Iteration 104/1000 | Loss: 0.00001624
Iteration 105/1000 | Loss: 0.00001624
Iteration 106/1000 | Loss: 0.00001624
Iteration 107/1000 | Loss: 0.00001624
Iteration 108/1000 | Loss: 0.00001624
Iteration 109/1000 | Loss: 0.00001624
Iteration 110/1000 | Loss: 0.00001624
Iteration 111/1000 | Loss: 0.00001624
Iteration 112/1000 | Loss: 0.00001624
Iteration 113/1000 | Loss: 0.00001624
Iteration 114/1000 | Loss: 0.00001624
Iteration 115/1000 | Loss: 0.00001624
Iteration 116/1000 | Loss: 0.00001624
Iteration 117/1000 | Loss: 0.00001623
Iteration 118/1000 | Loss: 0.00001623
Iteration 119/1000 | Loss: 0.00001623
Iteration 120/1000 | Loss: 0.00001623
Iteration 121/1000 | Loss: 0.00001623
Iteration 122/1000 | Loss: 0.00001623
Iteration 123/1000 | Loss: 0.00001623
Iteration 124/1000 | Loss: 0.00001622
Iteration 125/1000 | Loss: 0.00001622
Iteration 126/1000 | Loss: 0.00001622
Iteration 127/1000 | Loss: 0.00001622
Iteration 128/1000 | Loss: 0.00001622
Iteration 129/1000 | Loss: 0.00001622
Iteration 130/1000 | Loss: 0.00001622
Iteration 131/1000 | Loss: 0.00001622
Iteration 132/1000 | Loss: 0.00001622
Iteration 133/1000 | Loss: 0.00001622
Iteration 134/1000 | Loss: 0.00001622
Iteration 135/1000 | Loss: 0.00001622
Iteration 136/1000 | Loss: 0.00001622
Iteration 137/1000 | Loss: 0.00001622
Iteration 138/1000 | Loss: 0.00001622
Iteration 139/1000 | Loss: 0.00001622
Iteration 140/1000 | Loss: 0.00001622
Iteration 141/1000 | Loss: 0.00001621
Iteration 142/1000 | Loss: 0.00001621
Iteration 143/1000 | Loss: 0.00001621
Iteration 144/1000 | Loss: 0.00001621
Iteration 145/1000 | Loss: 0.00001621
Iteration 146/1000 | Loss: 0.00001621
Iteration 147/1000 | Loss: 0.00001621
Iteration 148/1000 | Loss: 0.00001621
Iteration 149/1000 | Loss: 0.00001621
Iteration 150/1000 | Loss: 0.00001621
Iteration 151/1000 | Loss: 0.00001621
Iteration 152/1000 | Loss: 0.00001621
Iteration 153/1000 | Loss: 0.00001621
Iteration 154/1000 | Loss: 0.00001621
Iteration 155/1000 | Loss: 0.00001621
Iteration 156/1000 | Loss: 0.00001621
Iteration 157/1000 | Loss: 0.00001621
Iteration 158/1000 | Loss: 0.00001621
Iteration 159/1000 | Loss: 0.00001621
Iteration 160/1000 | Loss: 0.00001621
Iteration 161/1000 | Loss: 0.00001621
Iteration 162/1000 | Loss: 0.00001621
Iteration 163/1000 | Loss: 0.00001621
Iteration 164/1000 | Loss: 0.00001621
Iteration 165/1000 | Loss: 0.00001621
Iteration 166/1000 | Loss: 0.00001621
Iteration 167/1000 | Loss: 0.00001621
Iteration 168/1000 | Loss: 0.00001621
Iteration 169/1000 | Loss: 0.00001621
Iteration 170/1000 | Loss: 0.00001621
Iteration 171/1000 | Loss: 0.00001621
Iteration 172/1000 | Loss: 0.00001621
Iteration 173/1000 | Loss: 0.00001621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.620597868168261e-05, 1.620597868168261e-05, 1.620597868168261e-05, 1.620597868168261e-05, 1.620597868168261e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.620597868168261e-05

Optimization complete. Final v2v error: 3.3914477825164795 mm

Highest mean error: 3.546830654144287 mm for frame 116

Lowest mean error: 2.6438708305358887 mm for frame 0

Saving results

Total time: 34.25555372238159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462115
Iteration 2/25 | Loss: 0.00120394
Iteration 3/25 | Loss: 0.00109667
Iteration 4/25 | Loss: 0.00108220
Iteration 5/25 | Loss: 0.00107913
Iteration 6/25 | Loss: 0.00107913
Iteration 7/25 | Loss: 0.00107913
Iteration 8/25 | Loss: 0.00107913
Iteration 9/25 | Loss: 0.00107913
Iteration 10/25 | Loss: 0.00107913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010791298700496554, 0.0010791298700496554, 0.0010791298700496554, 0.0010791298700496554, 0.0010791298700496554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010791298700496554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.89983010
Iteration 2/25 | Loss: 0.00066785
Iteration 3/25 | Loss: 0.00066785
Iteration 4/25 | Loss: 0.00066785
Iteration 5/25 | Loss: 0.00066785
Iteration 6/25 | Loss: 0.00066785
Iteration 7/25 | Loss: 0.00066785
Iteration 8/25 | Loss: 0.00066785
Iteration 9/25 | Loss: 0.00066785
Iteration 10/25 | Loss: 0.00066785
Iteration 11/25 | Loss: 0.00066785
Iteration 12/25 | Loss: 0.00066785
Iteration 13/25 | Loss: 0.00066785
Iteration 14/25 | Loss: 0.00066785
Iteration 15/25 | Loss: 0.00066785
Iteration 16/25 | Loss: 0.00066785
Iteration 17/25 | Loss: 0.00066785
Iteration 18/25 | Loss: 0.00066785
Iteration 19/25 | Loss: 0.00066785
Iteration 20/25 | Loss: 0.00066785
Iteration 21/25 | Loss: 0.00066785
Iteration 22/25 | Loss: 0.00066785
Iteration 23/25 | Loss: 0.00066785
Iteration 24/25 | Loss: 0.00066785
Iteration 25/25 | Loss: 0.00066785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066785
Iteration 2/1000 | Loss: 0.00001871
Iteration 3/1000 | Loss: 0.00001488
Iteration 4/1000 | Loss: 0.00001379
Iteration 5/1000 | Loss: 0.00001311
Iteration 6/1000 | Loss: 0.00001276
Iteration 7/1000 | Loss: 0.00001246
Iteration 8/1000 | Loss: 0.00001217
Iteration 9/1000 | Loss: 0.00001217
Iteration 10/1000 | Loss: 0.00001199
Iteration 11/1000 | Loss: 0.00001184
Iteration 12/1000 | Loss: 0.00001179
Iteration 13/1000 | Loss: 0.00001174
Iteration 14/1000 | Loss: 0.00001173
Iteration 15/1000 | Loss: 0.00001173
Iteration 16/1000 | Loss: 0.00001165
Iteration 17/1000 | Loss: 0.00001159
Iteration 18/1000 | Loss: 0.00001159
Iteration 19/1000 | Loss: 0.00001158
Iteration 20/1000 | Loss: 0.00001157
Iteration 21/1000 | Loss: 0.00001155
Iteration 22/1000 | Loss: 0.00001155
Iteration 23/1000 | Loss: 0.00001153
Iteration 24/1000 | Loss: 0.00001151
Iteration 25/1000 | Loss: 0.00001147
Iteration 26/1000 | Loss: 0.00001144
Iteration 27/1000 | Loss: 0.00001143
Iteration 28/1000 | Loss: 0.00001143
Iteration 29/1000 | Loss: 0.00001143
Iteration 30/1000 | Loss: 0.00001143
Iteration 31/1000 | Loss: 0.00001143
Iteration 32/1000 | Loss: 0.00001142
Iteration 33/1000 | Loss: 0.00001142
Iteration 34/1000 | Loss: 0.00001142
Iteration 35/1000 | Loss: 0.00001141
Iteration 36/1000 | Loss: 0.00001141
Iteration 37/1000 | Loss: 0.00001140
Iteration 38/1000 | Loss: 0.00001139
Iteration 39/1000 | Loss: 0.00001139
Iteration 40/1000 | Loss: 0.00001138
Iteration 41/1000 | Loss: 0.00001137
Iteration 42/1000 | Loss: 0.00001137
Iteration 43/1000 | Loss: 0.00001137
Iteration 44/1000 | Loss: 0.00001137
Iteration 45/1000 | Loss: 0.00001137
Iteration 46/1000 | Loss: 0.00001136
Iteration 47/1000 | Loss: 0.00001136
Iteration 48/1000 | Loss: 0.00001136
Iteration 49/1000 | Loss: 0.00001135
Iteration 50/1000 | Loss: 0.00001135
Iteration 51/1000 | Loss: 0.00001135
Iteration 52/1000 | Loss: 0.00001133
Iteration 53/1000 | Loss: 0.00001133
Iteration 54/1000 | Loss: 0.00001131
Iteration 55/1000 | Loss: 0.00001131
Iteration 56/1000 | Loss: 0.00001131
Iteration 57/1000 | Loss: 0.00001131
Iteration 58/1000 | Loss: 0.00001131
Iteration 59/1000 | Loss: 0.00001131
Iteration 60/1000 | Loss: 0.00001131
Iteration 61/1000 | Loss: 0.00001131
Iteration 62/1000 | Loss: 0.00001131
Iteration 63/1000 | Loss: 0.00001131
Iteration 64/1000 | Loss: 0.00001131
Iteration 65/1000 | Loss: 0.00001131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.1308577086310834e-05, 1.1308577086310834e-05, 1.1308577086310834e-05, 1.1308577086310834e-05, 1.1308577086310834e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1308577086310834e-05

Optimization complete. Final v2v error: 2.8718860149383545 mm

Highest mean error: 3.1097257137298584 mm for frame 123

Lowest mean error: 2.7110238075256348 mm for frame 161

Saving results

Total time: 32.92526149749756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497441
Iteration 2/25 | Loss: 0.00131239
Iteration 3/25 | Loss: 0.00116327
Iteration 4/25 | Loss: 0.00113417
Iteration 5/25 | Loss: 0.00112482
Iteration 6/25 | Loss: 0.00112246
Iteration 7/25 | Loss: 0.00112163
Iteration 8/25 | Loss: 0.00112121
Iteration 9/25 | Loss: 0.00112105
Iteration 10/25 | Loss: 0.00112094
Iteration 11/25 | Loss: 0.00112461
Iteration 12/25 | Loss: 0.00112220
Iteration 13/25 | Loss: 0.00111999
Iteration 14/25 | Loss: 0.00111861
Iteration 15/25 | Loss: 0.00111747
Iteration 16/25 | Loss: 0.00111727
Iteration 17/25 | Loss: 0.00111725
Iteration 18/25 | Loss: 0.00111725
Iteration 19/25 | Loss: 0.00111725
Iteration 20/25 | Loss: 0.00111725
Iteration 21/25 | Loss: 0.00111725
Iteration 22/25 | Loss: 0.00111724
Iteration 23/25 | Loss: 0.00111724
Iteration 24/25 | Loss: 0.00111724
Iteration 25/25 | Loss: 0.00111724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41396952
Iteration 2/25 | Loss: 0.00094119
Iteration 3/25 | Loss: 0.00094119
Iteration 4/25 | Loss: 0.00094119
Iteration 5/25 | Loss: 0.00094119
Iteration 6/25 | Loss: 0.00094119
Iteration 7/25 | Loss: 0.00094119
Iteration 8/25 | Loss: 0.00094118
Iteration 9/25 | Loss: 0.00094118
Iteration 10/25 | Loss: 0.00094118
Iteration 11/25 | Loss: 0.00094118
Iteration 12/25 | Loss: 0.00094118
Iteration 13/25 | Loss: 0.00094118
Iteration 14/25 | Loss: 0.00094118
Iteration 15/25 | Loss: 0.00094118
Iteration 16/25 | Loss: 0.00094118
Iteration 17/25 | Loss: 0.00094118
Iteration 18/25 | Loss: 0.00094118
Iteration 19/25 | Loss: 0.00094118
Iteration 20/25 | Loss: 0.00094118
Iteration 21/25 | Loss: 0.00094118
Iteration 22/25 | Loss: 0.00094118
Iteration 23/25 | Loss: 0.00094118
Iteration 24/25 | Loss: 0.00094118
Iteration 25/25 | Loss: 0.00094118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094118
Iteration 2/1000 | Loss: 0.00005033
Iteration 3/1000 | Loss: 0.00003133
Iteration 4/1000 | Loss: 0.00002458
Iteration 5/1000 | Loss: 0.00002264
Iteration 6/1000 | Loss: 0.00002156
Iteration 7/1000 | Loss: 0.00002083
Iteration 8/1000 | Loss: 0.00002033
Iteration 9/1000 | Loss: 0.00001994
Iteration 10/1000 | Loss: 0.00001972
Iteration 11/1000 | Loss: 0.00001970
Iteration 12/1000 | Loss: 0.00001962
Iteration 13/1000 | Loss: 0.00001957
Iteration 14/1000 | Loss: 0.00001955
Iteration 15/1000 | Loss: 0.00001954
Iteration 16/1000 | Loss: 0.00001950
Iteration 17/1000 | Loss: 0.00001949
Iteration 18/1000 | Loss: 0.00001948
Iteration 19/1000 | Loss: 0.00001947
Iteration 20/1000 | Loss: 0.00001946
Iteration 21/1000 | Loss: 0.00001942
Iteration 22/1000 | Loss: 0.00001938
Iteration 23/1000 | Loss: 0.00001932
Iteration 24/1000 | Loss: 0.00001928
Iteration 25/1000 | Loss: 0.00001927
Iteration 26/1000 | Loss: 0.00001927
Iteration 27/1000 | Loss: 0.00001925
Iteration 28/1000 | Loss: 0.00001923
Iteration 29/1000 | Loss: 0.00001923
Iteration 30/1000 | Loss: 0.00001923
Iteration 31/1000 | Loss: 0.00001921
Iteration 32/1000 | Loss: 0.00001921
Iteration 33/1000 | Loss: 0.00001920
Iteration 34/1000 | Loss: 0.00001920
Iteration 35/1000 | Loss: 0.00001920
Iteration 36/1000 | Loss: 0.00001919
Iteration 37/1000 | Loss: 0.00001919
Iteration 38/1000 | Loss: 0.00001919
Iteration 39/1000 | Loss: 0.00001919
Iteration 40/1000 | Loss: 0.00001919
Iteration 41/1000 | Loss: 0.00001919
Iteration 42/1000 | Loss: 0.00001918
Iteration 43/1000 | Loss: 0.00001918
Iteration 44/1000 | Loss: 0.00001918
Iteration 45/1000 | Loss: 0.00001917
Iteration 46/1000 | Loss: 0.00001916
Iteration 47/1000 | Loss: 0.00001915
Iteration 48/1000 | Loss: 0.00001915
Iteration 49/1000 | Loss: 0.00001915
Iteration 50/1000 | Loss: 0.00001915
Iteration 51/1000 | Loss: 0.00001914
Iteration 52/1000 | Loss: 0.00001914
Iteration 53/1000 | Loss: 0.00001914
Iteration 54/1000 | Loss: 0.00001913
Iteration 55/1000 | Loss: 0.00001913
Iteration 56/1000 | Loss: 0.00001913
Iteration 57/1000 | Loss: 0.00001912
Iteration 58/1000 | Loss: 0.00001912
Iteration 59/1000 | Loss: 0.00001912
Iteration 60/1000 | Loss: 0.00001911
Iteration 61/1000 | Loss: 0.00001911
Iteration 62/1000 | Loss: 0.00001911
Iteration 63/1000 | Loss: 0.00001910
Iteration 64/1000 | Loss: 0.00001910
Iteration 65/1000 | Loss: 0.00001910
Iteration 66/1000 | Loss: 0.00001909
Iteration 67/1000 | Loss: 0.00001909
Iteration 68/1000 | Loss: 0.00001909
Iteration 69/1000 | Loss: 0.00001909
Iteration 70/1000 | Loss: 0.00001909
Iteration 71/1000 | Loss: 0.00001909
Iteration 72/1000 | Loss: 0.00001909
Iteration 73/1000 | Loss: 0.00001908
Iteration 74/1000 | Loss: 0.00001908
Iteration 75/1000 | Loss: 0.00001908
Iteration 76/1000 | Loss: 0.00001908
Iteration 77/1000 | Loss: 0.00001908
Iteration 78/1000 | Loss: 0.00001908
Iteration 79/1000 | Loss: 0.00001907
Iteration 80/1000 | Loss: 0.00001907
Iteration 81/1000 | Loss: 0.00001907
Iteration 82/1000 | Loss: 0.00001907
Iteration 83/1000 | Loss: 0.00001906
Iteration 84/1000 | Loss: 0.00001906
Iteration 85/1000 | Loss: 0.00001906
Iteration 86/1000 | Loss: 0.00001906
Iteration 87/1000 | Loss: 0.00001906
Iteration 88/1000 | Loss: 0.00001906
Iteration 89/1000 | Loss: 0.00001906
Iteration 90/1000 | Loss: 0.00001905
Iteration 91/1000 | Loss: 0.00001905
Iteration 92/1000 | Loss: 0.00001905
Iteration 93/1000 | Loss: 0.00001905
Iteration 94/1000 | Loss: 0.00001904
Iteration 95/1000 | Loss: 0.00001904
Iteration 96/1000 | Loss: 0.00001904
Iteration 97/1000 | Loss: 0.00001904
Iteration 98/1000 | Loss: 0.00001904
Iteration 99/1000 | Loss: 0.00001904
Iteration 100/1000 | Loss: 0.00001904
Iteration 101/1000 | Loss: 0.00001903
Iteration 102/1000 | Loss: 0.00001903
Iteration 103/1000 | Loss: 0.00001903
Iteration 104/1000 | Loss: 0.00001903
Iteration 105/1000 | Loss: 0.00001903
Iteration 106/1000 | Loss: 0.00001903
Iteration 107/1000 | Loss: 0.00001903
Iteration 108/1000 | Loss: 0.00001903
Iteration 109/1000 | Loss: 0.00001903
Iteration 110/1000 | Loss: 0.00001903
Iteration 111/1000 | Loss: 0.00001903
Iteration 112/1000 | Loss: 0.00001903
Iteration 113/1000 | Loss: 0.00001902
Iteration 114/1000 | Loss: 0.00001902
Iteration 115/1000 | Loss: 0.00001902
Iteration 116/1000 | Loss: 0.00001902
Iteration 117/1000 | Loss: 0.00001902
Iteration 118/1000 | Loss: 0.00001902
Iteration 119/1000 | Loss: 0.00001902
Iteration 120/1000 | Loss: 0.00001902
Iteration 121/1000 | Loss: 0.00001901
Iteration 122/1000 | Loss: 0.00001901
Iteration 123/1000 | Loss: 0.00001901
Iteration 124/1000 | Loss: 0.00001901
Iteration 125/1000 | Loss: 0.00001901
Iteration 126/1000 | Loss: 0.00001901
Iteration 127/1000 | Loss: 0.00001900
Iteration 128/1000 | Loss: 0.00001900
Iteration 129/1000 | Loss: 0.00001900
Iteration 130/1000 | Loss: 0.00001900
Iteration 131/1000 | Loss: 0.00001900
Iteration 132/1000 | Loss: 0.00001900
Iteration 133/1000 | Loss: 0.00001900
Iteration 134/1000 | Loss: 0.00001900
Iteration 135/1000 | Loss: 0.00001899
Iteration 136/1000 | Loss: 0.00001899
Iteration 137/1000 | Loss: 0.00001899
Iteration 138/1000 | Loss: 0.00001899
Iteration 139/1000 | Loss: 0.00001899
Iteration 140/1000 | Loss: 0.00001899
Iteration 141/1000 | Loss: 0.00001899
Iteration 142/1000 | Loss: 0.00001899
Iteration 143/1000 | Loss: 0.00001899
Iteration 144/1000 | Loss: 0.00001898
Iteration 145/1000 | Loss: 0.00001898
Iteration 146/1000 | Loss: 0.00001898
Iteration 147/1000 | Loss: 0.00001898
Iteration 148/1000 | Loss: 0.00001898
Iteration 149/1000 | Loss: 0.00001898
Iteration 150/1000 | Loss: 0.00001898
Iteration 151/1000 | Loss: 0.00001898
Iteration 152/1000 | Loss: 0.00001898
Iteration 153/1000 | Loss: 0.00001898
Iteration 154/1000 | Loss: 0.00001898
Iteration 155/1000 | Loss: 0.00001898
Iteration 156/1000 | Loss: 0.00001898
Iteration 157/1000 | Loss: 0.00001898
Iteration 158/1000 | Loss: 0.00001898
Iteration 159/1000 | Loss: 0.00001898
Iteration 160/1000 | Loss: 0.00001898
Iteration 161/1000 | Loss: 0.00001898
Iteration 162/1000 | Loss: 0.00001898
Iteration 163/1000 | Loss: 0.00001898
Iteration 164/1000 | Loss: 0.00001898
Iteration 165/1000 | Loss: 0.00001898
Iteration 166/1000 | Loss: 0.00001898
Iteration 167/1000 | Loss: 0.00001898
Iteration 168/1000 | Loss: 0.00001898
Iteration 169/1000 | Loss: 0.00001898
Iteration 170/1000 | Loss: 0.00001898
Iteration 171/1000 | Loss: 0.00001898
Iteration 172/1000 | Loss: 0.00001898
Iteration 173/1000 | Loss: 0.00001898
Iteration 174/1000 | Loss: 0.00001898
Iteration 175/1000 | Loss: 0.00001898
Iteration 176/1000 | Loss: 0.00001898
Iteration 177/1000 | Loss: 0.00001898
Iteration 178/1000 | Loss: 0.00001898
Iteration 179/1000 | Loss: 0.00001898
Iteration 180/1000 | Loss: 0.00001898
Iteration 181/1000 | Loss: 0.00001898
Iteration 182/1000 | Loss: 0.00001898
Iteration 183/1000 | Loss: 0.00001898
Iteration 184/1000 | Loss: 0.00001898
Iteration 185/1000 | Loss: 0.00001898
Iteration 186/1000 | Loss: 0.00001898
Iteration 187/1000 | Loss: 0.00001898
Iteration 188/1000 | Loss: 0.00001898
Iteration 189/1000 | Loss: 0.00001898
Iteration 190/1000 | Loss: 0.00001898
Iteration 191/1000 | Loss: 0.00001898
Iteration 192/1000 | Loss: 0.00001898
Iteration 193/1000 | Loss: 0.00001898
Iteration 194/1000 | Loss: 0.00001898
Iteration 195/1000 | Loss: 0.00001898
Iteration 196/1000 | Loss: 0.00001898
Iteration 197/1000 | Loss: 0.00001898
Iteration 198/1000 | Loss: 0.00001898
Iteration 199/1000 | Loss: 0.00001898
Iteration 200/1000 | Loss: 0.00001898
Iteration 201/1000 | Loss: 0.00001898
Iteration 202/1000 | Loss: 0.00001898
Iteration 203/1000 | Loss: 0.00001898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.8982094843522646e-05, 1.8982094843522646e-05, 1.8982094843522646e-05, 1.8982094843522646e-05, 1.8982094843522646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8982094843522646e-05

Optimization complete. Final v2v error: 3.516176700592041 mm

Highest mean error: 4.760612964630127 mm for frame 94

Lowest mean error: 2.522240400314331 mm for frame 49

Saving results

Total time: 55.83160996437073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425231
Iteration 2/25 | Loss: 0.00120849
Iteration 3/25 | Loss: 0.00110718
Iteration 4/25 | Loss: 0.00109572
Iteration 5/25 | Loss: 0.00109308
Iteration 6/25 | Loss: 0.00109257
Iteration 7/25 | Loss: 0.00109257
Iteration 8/25 | Loss: 0.00109257
Iteration 9/25 | Loss: 0.00109257
Iteration 10/25 | Loss: 0.00109257
Iteration 11/25 | Loss: 0.00109257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010925658280029893, 0.0010925658280029893, 0.0010925658280029893, 0.0010925658280029893, 0.0010925658280029893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010925658280029893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39759922
Iteration 2/25 | Loss: 0.00080127
Iteration 3/25 | Loss: 0.00080127
Iteration 4/25 | Loss: 0.00080126
Iteration 5/25 | Loss: 0.00080126
Iteration 6/25 | Loss: 0.00080126
Iteration 7/25 | Loss: 0.00080126
Iteration 8/25 | Loss: 0.00080126
Iteration 9/25 | Loss: 0.00080126
Iteration 10/25 | Loss: 0.00080126
Iteration 11/25 | Loss: 0.00080126
Iteration 12/25 | Loss: 0.00080126
Iteration 13/25 | Loss: 0.00080126
Iteration 14/25 | Loss: 0.00080126
Iteration 15/25 | Loss: 0.00080126
Iteration 16/25 | Loss: 0.00080126
Iteration 17/25 | Loss: 0.00080126
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008012625039555132, 0.0008012625039555132, 0.0008012625039555132, 0.0008012625039555132, 0.0008012625039555132]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008012625039555132

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080126
Iteration 2/1000 | Loss: 0.00002587
Iteration 3/1000 | Loss: 0.00001618
Iteration 4/1000 | Loss: 0.00001463
Iteration 5/1000 | Loss: 0.00001406
Iteration 6/1000 | Loss: 0.00001369
Iteration 7/1000 | Loss: 0.00001336
Iteration 8/1000 | Loss: 0.00001310
Iteration 9/1000 | Loss: 0.00001309
Iteration 10/1000 | Loss: 0.00001292
Iteration 11/1000 | Loss: 0.00001289
Iteration 12/1000 | Loss: 0.00001276
Iteration 13/1000 | Loss: 0.00001268
Iteration 14/1000 | Loss: 0.00001265
Iteration 15/1000 | Loss: 0.00001254
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001248
Iteration 18/1000 | Loss: 0.00001247
Iteration 19/1000 | Loss: 0.00001245
Iteration 20/1000 | Loss: 0.00001244
Iteration 21/1000 | Loss: 0.00001244
Iteration 22/1000 | Loss: 0.00001241
Iteration 23/1000 | Loss: 0.00001241
Iteration 24/1000 | Loss: 0.00001241
Iteration 25/1000 | Loss: 0.00001241
Iteration 26/1000 | Loss: 0.00001240
Iteration 27/1000 | Loss: 0.00001240
Iteration 28/1000 | Loss: 0.00001235
Iteration 29/1000 | Loss: 0.00001235
Iteration 30/1000 | Loss: 0.00001235
Iteration 31/1000 | Loss: 0.00001234
Iteration 32/1000 | Loss: 0.00001232
Iteration 33/1000 | Loss: 0.00001232
Iteration 34/1000 | Loss: 0.00001232
Iteration 35/1000 | Loss: 0.00001232
Iteration 36/1000 | Loss: 0.00001232
Iteration 37/1000 | Loss: 0.00001232
Iteration 38/1000 | Loss: 0.00001231
Iteration 39/1000 | Loss: 0.00001231
Iteration 40/1000 | Loss: 0.00001230
Iteration 41/1000 | Loss: 0.00001229
Iteration 42/1000 | Loss: 0.00001229
Iteration 43/1000 | Loss: 0.00001229
Iteration 44/1000 | Loss: 0.00001229
Iteration 45/1000 | Loss: 0.00001229
Iteration 46/1000 | Loss: 0.00001228
Iteration 47/1000 | Loss: 0.00001228
Iteration 48/1000 | Loss: 0.00001228
Iteration 49/1000 | Loss: 0.00001228
Iteration 50/1000 | Loss: 0.00001228
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001227
Iteration 53/1000 | Loss: 0.00001227
Iteration 54/1000 | Loss: 0.00001227
Iteration 55/1000 | Loss: 0.00001227
Iteration 56/1000 | Loss: 0.00001226
Iteration 57/1000 | Loss: 0.00001226
Iteration 58/1000 | Loss: 0.00001226
Iteration 59/1000 | Loss: 0.00001226
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001225
Iteration 62/1000 | Loss: 0.00001225
Iteration 63/1000 | Loss: 0.00001225
Iteration 64/1000 | Loss: 0.00001224
Iteration 65/1000 | Loss: 0.00001224
Iteration 66/1000 | Loss: 0.00001224
Iteration 67/1000 | Loss: 0.00001224
Iteration 68/1000 | Loss: 0.00001224
Iteration 69/1000 | Loss: 0.00001223
Iteration 70/1000 | Loss: 0.00001223
Iteration 71/1000 | Loss: 0.00001223
Iteration 72/1000 | Loss: 0.00001222
Iteration 73/1000 | Loss: 0.00001222
Iteration 74/1000 | Loss: 0.00001222
Iteration 75/1000 | Loss: 0.00001222
Iteration 76/1000 | Loss: 0.00001222
Iteration 77/1000 | Loss: 0.00001222
Iteration 78/1000 | Loss: 0.00001222
Iteration 79/1000 | Loss: 0.00001222
Iteration 80/1000 | Loss: 0.00001222
Iteration 81/1000 | Loss: 0.00001222
Iteration 82/1000 | Loss: 0.00001222
Iteration 83/1000 | Loss: 0.00001221
Iteration 84/1000 | Loss: 0.00001221
Iteration 85/1000 | Loss: 0.00001221
Iteration 86/1000 | Loss: 0.00001220
Iteration 87/1000 | Loss: 0.00001220
Iteration 88/1000 | Loss: 0.00001220
Iteration 89/1000 | Loss: 0.00001220
Iteration 90/1000 | Loss: 0.00001219
Iteration 91/1000 | Loss: 0.00001219
Iteration 92/1000 | Loss: 0.00001219
Iteration 93/1000 | Loss: 0.00001219
Iteration 94/1000 | Loss: 0.00001219
Iteration 95/1000 | Loss: 0.00001219
Iteration 96/1000 | Loss: 0.00001219
Iteration 97/1000 | Loss: 0.00001218
Iteration 98/1000 | Loss: 0.00001218
Iteration 99/1000 | Loss: 0.00001218
Iteration 100/1000 | Loss: 0.00001218
Iteration 101/1000 | Loss: 0.00001217
Iteration 102/1000 | Loss: 0.00001217
Iteration 103/1000 | Loss: 0.00001217
Iteration 104/1000 | Loss: 0.00001217
Iteration 105/1000 | Loss: 0.00001217
Iteration 106/1000 | Loss: 0.00001217
Iteration 107/1000 | Loss: 0.00001217
Iteration 108/1000 | Loss: 0.00001216
Iteration 109/1000 | Loss: 0.00001216
Iteration 110/1000 | Loss: 0.00001216
Iteration 111/1000 | Loss: 0.00001216
Iteration 112/1000 | Loss: 0.00001216
Iteration 113/1000 | Loss: 0.00001216
Iteration 114/1000 | Loss: 0.00001216
Iteration 115/1000 | Loss: 0.00001216
Iteration 116/1000 | Loss: 0.00001216
Iteration 117/1000 | Loss: 0.00001216
Iteration 118/1000 | Loss: 0.00001216
Iteration 119/1000 | Loss: 0.00001216
Iteration 120/1000 | Loss: 0.00001215
Iteration 121/1000 | Loss: 0.00001215
Iteration 122/1000 | Loss: 0.00001215
Iteration 123/1000 | Loss: 0.00001215
Iteration 124/1000 | Loss: 0.00001215
Iteration 125/1000 | Loss: 0.00001215
Iteration 126/1000 | Loss: 0.00001215
Iteration 127/1000 | Loss: 0.00001215
Iteration 128/1000 | Loss: 0.00001214
Iteration 129/1000 | Loss: 0.00001214
Iteration 130/1000 | Loss: 0.00001214
Iteration 131/1000 | Loss: 0.00001214
Iteration 132/1000 | Loss: 0.00001214
Iteration 133/1000 | Loss: 0.00001214
Iteration 134/1000 | Loss: 0.00001214
Iteration 135/1000 | Loss: 0.00001214
Iteration 136/1000 | Loss: 0.00001214
Iteration 137/1000 | Loss: 0.00001214
Iteration 138/1000 | Loss: 0.00001214
Iteration 139/1000 | Loss: 0.00001214
Iteration 140/1000 | Loss: 0.00001214
Iteration 141/1000 | Loss: 0.00001214
Iteration 142/1000 | Loss: 0.00001214
Iteration 143/1000 | Loss: 0.00001214
Iteration 144/1000 | Loss: 0.00001213
Iteration 145/1000 | Loss: 0.00001213
Iteration 146/1000 | Loss: 0.00001213
Iteration 147/1000 | Loss: 0.00001213
Iteration 148/1000 | Loss: 0.00001213
Iteration 149/1000 | Loss: 0.00001213
Iteration 150/1000 | Loss: 0.00001213
Iteration 151/1000 | Loss: 0.00001213
Iteration 152/1000 | Loss: 0.00001213
Iteration 153/1000 | Loss: 0.00001212
Iteration 154/1000 | Loss: 0.00001212
Iteration 155/1000 | Loss: 0.00001212
Iteration 156/1000 | Loss: 0.00001212
Iteration 157/1000 | Loss: 0.00001212
Iteration 158/1000 | Loss: 0.00001212
Iteration 159/1000 | Loss: 0.00001212
Iteration 160/1000 | Loss: 0.00001212
Iteration 161/1000 | Loss: 0.00001212
Iteration 162/1000 | Loss: 0.00001212
Iteration 163/1000 | Loss: 0.00001212
Iteration 164/1000 | Loss: 0.00001211
Iteration 165/1000 | Loss: 0.00001211
Iteration 166/1000 | Loss: 0.00001211
Iteration 167/1000 | Loss: 0.00001211
Iteration 168/1000 | Loss: 0.00001211
Iteration 169/1000 | Loss: 0.00001211
Iteration 170/1000 | Loss: 0.00001211
Iteration 171/1000 | Loss: 0.00001211
Iteration 172/1000 | Loss: 0.00001211
Iteration 173/1000 | Loss: 0.00001211
Iteration 174/1000 | Loss: 0.00001211
Iteration 175/1000 | Loss: 0.00001210
Iteration 176/1000 | Loss: 0.00001210
Iteration 177/1000 | Loss: 0.00001210
Iteration 178/1000 | Loss: 0.00001210
Iteration 179/1000 | Loss: 0.00001210
Iteration 180/1000 | Loss: 0.00001209
Iteration 181/1000 | Loss: 0.00001209
Iteration 182/1000 | Loss: 0.00001209
Iteration 183/1000 | Loss: 0.00001209
Iteration 184/1000 | Loss: 0.00001209
Iteration 185/1000 | Loss: 0.00001209
Iteration 186/1000 | Loss: 0.00001209
Iteration 187/1000 | Loss: 0.00001209
Iteration 188/1000 | Loss: 0.00001209
Iteration 189/1000 | Loss: 0.00001209
Iteration 190/1000 | Loss: 0.00001209
Iteration 191/1000 | Loss: 0.00001209
Iteration 192/1000 | Loss: 0.00001209
Iteration 193/1000 | Loss: 0.00001208
Iteration 194/1000 | Loss: 0.00001208
Iteration 195/1000 | Loss: 0.00001208
Iteration 196/1000 | Loss: 0.00001208
Iteration 197/1000 | Loss: 0.00001208
Iteration 198/1000 | Loss: 0.00001208
Iteration 199/1000 | Loss: 0.00001208
Iteration 200/1000 | Loss: 0.00001207
Iteration 201/1000 | Loss: 0.00001207
Iteration 202/1000 | Loss: 0.00001207
Iteration 203/1000 | Loss: 0.00001207
Iteration 204/1000 | Loss: 0.00001207
Iteration 205/1000 | Loss: 0.00001207
Iteration 206/1000 | Loss: 0.00001207
Iteration 207/1000 | Loss: 0.00001207
Iteration 208/1000 | Loss: 0.00001207
Iteration 209/1000 | Loss: 0.00001207
Iteration 210/1000 | Loss: 0.00001207
Iteration 211/1000 | Loss: 0.00001207
Iteration 212/1000 | Loss: 0.00001206
Iteration 213/1000 | Loss: 0.00001206
Iteration 214/1000 | Loss: 0.00001206
Iteration 215/1000 | Loss: 0.00001206
Iteration 216/1000 | Loss: 0.00001206
Iteration 217/1000 | Loss: 0.00001206
Iteration 218/1000 | Loss: 0.00001206
Iteration 219/1000 | Loss: 0.00001206
Iteration 220/1000 | Loss: 0.00001206
Iteration 221/1000 | Loss: 0.00001206
Iteration 222/1000 | Loss: 0.00001206
Iteration 223/1000 | Loss: 0.00001206
Iteration 224/1000 | Loss: 0.00001206
Iteration 225/1000 | Loss: 0.00001206
Iteration 226/1000 | Loss: 0.00001206
Iteration 227/1000 | Loss: 0.00001206
Iteration 228/1000 | Loss: 0.00001206
Iteration 229/1000 | Loss: 0.00001206
Iteration 230/1000 | Loss: 0.00001206
Iteration 231/1000 | Loss: 0.00001206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.2058883839927148e-05, 1.2058883839927148e-05, 1.2058883839927148e-05, 1.2058883839927148e-05, 1.2058883839927148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2058883839927148e-05

Optimization complete. Final v2v error: 2.905978202819824 mm

Highest mean error: 3.2884609699249268 mm for frame 19

Lowest mean error: 2.494837999343872 mm for frame 79

Saving results

Total time: 39.757696866989136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037159
Iteration 2/25 | Loss: 0.01037159
Iteration 3/25 | Loss: 0.01037159
Iteration 4/25 | Loss: 0.01037159
Iteration 5/25 | Loss: 0.01037158
Iteration 6/25 | Loss: 0.01037158
Iteration 7/25 | Loss: 0.01037158
Iteration 8/25 | Loss: 0.01037158
Iteration 9/25 | Loss: 0.01037158
Iteration 10/25 | Loss: 0.01037158
Iteration 11/25 | Loss: 0.01037158
Iteration 12/25 | Loss: 0.01037158
Iteration 13/25 | Loss: 0.01037157
Iteration 14/25 | Loss: 0.01037157
Iteration 15/25 | Loss: 0.01037157
Iteration 16/25 | Loss: 0.01037157
Iteration 17/25 | Loss: 0.01037157
Iteration 18/25 | Loss: 0.01037157
Iteration 19/25 | Loss: 0.01037157
Iteration 20/25 | Loss: 0.01037156
Iteration 21/25 | Loss: 0.01037156
Iteration 22/25 | Loss: 0.01037156
Iteration 23/25 | Loss: 0.01037156
Iteration 24/25 | Loss: 0.01037156
Iteration 25/25 | Loss: 0.01037156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86511743
Iteration 2/25 | Loss: 0.09172744
Iteration 3/25 | Loss: 0.08822580
Iteration 4/25 | Loss: 0.08763453
Iteration 5/25 | Loss: 0.08763452
Iteration 6/25 | Loss: 0.08763452
Iteration 7/25 | Loss: 0.08763450
Iteration 8/25 | Loss: 0.08763450
Iteration 9/25 | Loss: 0.08763450
Iteration 10/25 | Loss: 0.08763450
Iteration 11/25 | Loss: 0.08763450
Iteration 12/25 | Loss: 0.08763450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.08763450384140015, 0.08763450384140015, 0.08763450384140015, 0.08763450384140015, 0.08763450384140015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08763450384140015

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08763450
Iteration 2/1000 | Loss: 0.00330800
Iteration 3/1000 | Loss: 0.00236211
Iteration 4/1000 | Loss: 0.00037906
Iteration 5/1000 | Loss: 0.00100134
Iteration 6/1000 | Loss: 0.00230583
Iteration 7/1000 | Loss: 0.00046060
Iteration 8/1000 | Loss: 0.00008970
Iteration 9/1000 | Loss: 0.00008175
Iteration 10/1000 | Loss: 0.00007102
Iteration 11/1000 | Loss: 0.00005501
Iteration 12/1000 | Loss: 0.00003668
Iteration 13/1000 | Loss: 0.00003206
Iteration 14/1000 | Loss: 0.00002935
Iteration 15/1000 | Loss: 0.00002701
Iteration 16/1000 | Loss: 0.00002546
Iteration 17/1000 | Loss: 0.00002422
Iteration 18/1000 | Loss: 0.00009137
Iteration 19/1000 | Loss: 0.00004523
Iteration 20/1000 | Loss: 0.00006589
Iteration 21/1000 | Loss: 0.00002981
Iteration 22/1000 | Loss: 0.00002193
Iteration 23/1000 | Loss: 0.00003631
Iteration 24/1000 | Loss: 0.00003943
Iteration 25/1000 | Loss: 0.00002055
Iteration 26/1000 | Loss: 0.00004592
Iteration 27/1000 | Loss: 0.00002425
Iteration 28/1000 | Loss: 0.00002420
Iteration 29/1000 | Loss: 0.00003845
Iteration 30/1000 | Loss: 0.00004050
Iteration 31/1000 | Loss: 0.00001793
Iteration 32/1000 | Loss: 0.00001757
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00003851
Iteration 35/1000 | Loss: 0.00013066
Iteration 36/1000 | Loss: 0.00006499
Iteration 37/1000 | Loss: 0.00006471
Iteration 38/1000 | Loss: 0.00001703
Iteration 39/1000 | Loss: 0.00007631
Iteration 40/1000 | Loss: 0.00001618
Iteration 41/1000 | Loss: 0.00001582
Iteration 42/1000 | Loss: 0.00001559
Iteration 43/1000 | Loss: 0.00001545
Iteration 44/1000 | Loss: 0.00001542
Iteration 45/1000 | Loss: 0.00001541
Iteration 46/1000 | Loss: 0.00001541
Iteration 47/1000 | Loss: 0.00001540
Iteration 48/1000 | Loss: 0.00001540
Iteration 49/1000 | Loss: 0.00001540
Iteration 50/1000 | Loss: 0.00001539
Iteration 51/1000 | Loss: 0.00001537
Iteration 52/1000 | Loss: 0.00001535
Iteration 53/1000 | Loss: 0.00001527
Iteration 54/1000 | Loss: 0.00001525
Iteration 55/1000 | Loss: 0.00001524
Iteration 56/1000 | Loss: 0.00001524
Iteration 57/1000 | Loss: 0.00001524
Iteration 58/1000 | Loss: 0.00001523
Iteration 59/1000 | Loss: 0.00001523
Iteration 60/1000 | Loss: 0.00001521
Iteration 61/1000 | Loss: 0.00001519
Iteration 62/1000 | Loss: 0.00001518
Iteration 63/1000 | Loss: 0.00001517
Iteration 64/1000 | Loss: 0.00001515
Iteration 65/1000 | Loss: 0.00001512
Iteration 66/1000 | Loss: 0.00001512
Iteration 67/1000 | Loss: 0.00001510
Iteration 68/1000 | Loss: 0.00001510
Iteration 69/1000 | Loss: 0.00001509
Iteration 70/1000 | Loss: 0.00001509
Iteration 71/1000 | Loss: 0.00001508
Iteration 72/1000 | Loss: 0.00001508
Iteration 73/1000 | Loss: 0.00004360
Iteration 74/1000 | Loss: 0.00001852
Iteration 75/1000 | Loss: 0.00001508
Iteration 76/1000 | Loss: 0.00001552
Iteration 77/1000 | Loss: 0.00001512
Iteration 78/1000 | Loss: 0.00001512
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001512
Iteration 81/1000 | Loss: 0.00001523
Iteration 82/1000 | Loss: 0.00001771
Iteration 83/1000 | Loss: 0.00001794
Iteration 84/1000 | Loss: 0.00001564
Iteration 85/1000 | Loss: 0.00001499
Iteration 86/1000 | Loss: 0.00001498
Iteration 87/1000 | Loss: 0.00001498
Iteration 88/1000 | Loss: 0.00001498
Iteration 89/1000 | Loss: 0.00001498
Iteration 90/1000 | Loss: 0.00001498
Iteration 91/1000 | Loss: 0.00001498
Iteration 92/1000 | Loss: 0.00001521
Iteration 93/1000 | Loss: 0.00001500
Iteration 94/1000 | Loss: 0.00001497
Iteration 95/1000 | Loss: 0.00001496
Iteration 96/1000 | Loss: 0.00001496
Iteration 97/1000 | Loss: 0.00001496
Iteration 98/1000 | Loss: 0.00001496
Iteration 99/1000 | Loss: 0.00001496
Iteration 100/1000 | Loss: 0.00001496
Iteration 101/1000 | Loss: 0.00001496
Iteration 102/1000 | Loss: 0.00001496
Iteration 103/1000 | Loss: 0.00001496
Iteration 104/1000 | Loss: 0.00001495
Iteration 105/1000 | Loss: 0.00001495
Iteration 106/1000 | Loss: 0.00001495
Iteration 107/1000 | Loss: 0.00001495
Iteration 108/1000 | Loss: 0.00001495
Iteration 109/1000 | Loss: 0.00001495
Iteration 110/1000 | Loss: 0.00001495
Iteration 111/1000 | Loss: 0.00001495
Iteration 112/1000 | Loss: 0.00001495
Iteration 113/1000 | Loss: 0.00001621
Iteration 114/1000 | Loss: 0.00001535
Iteration 115/1000 | Loss: 0.00001495
Iteration 116/1000 | Loss: 0.00001604
Iteration 117/1000 | Loss: 0.00001494
Iteration 118/1000 | Loss: 0.00001493
Iteration 119/1000 | Loss: 0.00001493
Iteration 120/1000 | Loss: 0.00001493
Iteration 121/1000 | Loss: 0.00001493
Iteration 122/1000 | Loss: 0.00001493
Iteration 123/1000 | Loss: 0.00001493
Iteration 124/1000 | Loss: 0.00001493
Iteration 125/1000 | Loss: 0.00001493
Iteration 126/1000 | Loss: 0.00001493
Iteration 127/1000 | Loss: 0.00001493
Iteration 128/1000 | Loss: 0.00001493
Iteration 129/1000 | Loss: 0.00001493
Iteration 130/1000 | Loss: 0.00001493
Iteration 131/1000 | Loss: 0.00001493
Iteration 132/1000 | Loss: 0.00001492
Iteration 133/1000 | Loss: 0.00001492
Iteration 134/1000 | Loss: 0.00001500
Iteration 135/1000 | Loss: 0.00001494
Iteration 136/1000 | Loss: 0.00001492
Iteration 137/1000 | Loss: 0.00001498
Iteration 138/1000 | Loss: 0.00001494
Iteration 139/1000 | Loss: 0.00001492
Iteration 140/1000 | Loss: 0.00001491
Iteration 141/1000 | Loss: 0.00001490
Iteration 142/1000 | Loss: 0.00001490
Iteration 143/1000 | Loss: 0.00001490
Iteration 144/1000 | Loss: 0.00001490
Iteration 145/1000 | Loss: 0.00001490
Iteration 146/1000 | Loss: 0.00001490
Iteration 147/1000 | Loss: 0.00001490
Iteration 148/1000 | Loss: 0.00001490
Iteration 149/1000 | Loss: 0.00001490
Iteration 150/1000 | Loss: 0.00001490
Iteration 151/1000 | Loss: 0.00001490
Iteration 152/1000 | Loss: 0.00001490
Iteration 153/1000 | Loss: 0.00001490
Iteration 154/1000 | Loss: 0.00001490
Iteration 155/1000 | Loss: 0.00001490
Iteration 156/1000 | Loss: 0.00001490
Iteration 157/1000 | Loss: 0.00001490
Iteration 158/1000 | Loss: 0.00001490
Iteration 159/1000 | Loss: 0.00001490
Iteration 160/1000 | Loss: 0.00001490
Iteration 161/1000 | Loss: 0.00001490
Iteration 162/1000 | Loss: 0.00001490
Iteration 163/1000 | Loss: 0.00001490
Iteration 164/1000 | Loss: 0.00001490
Iteration 165/1000 | Loss: 0.00001490
Iteration 166/1000 | Loss: 0.00001490
Iteration 167/1000 | Loss: 0.00001490
Iteration 168/1000 | Loss: 0.00001490
Iteration 169/1000 | Loss: 0.00001490
Iteration 170/1000 | Loss: 0.00001490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.4900851965649053e-05, 1.4900851965649053e-05, 1.4900851965649053e-05, 1.4900851965649053e-05, 1.4900851965649053e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4900851965649053e-05

Optimization complete. Final v2v error: 3.3047938346862793 mm

Highest mean error: 4.12262487411499 mm for frame 122

Lowest mean error: 2.93058705329895 mm for frame 105

Saving results

Total time: 99.37304544448853
