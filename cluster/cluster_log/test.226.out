Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=226, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12656-12711
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824864
Iteration 2/25 | Loss: 0.00128446
Iteration 3/25 | Loss: 0.00120184
Iteration 4/25 | Loss: 0.00119127
Iteration 5/25 | Loss: 0.00118806
Iteration 6/25 | Loss: 0.00118723
Iteration 7/25 | Loss: 0.00118723
Iteration 8/25 | Loss: 0.00118723
Iteration 9/25 | Loss: 0.00118723
Iteration 10/25 | Loss: 0.00118723
Iteration 11/25 | Loss: 0.00118723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001187233137898147, 0.001187233137898147, 0.001187233137898147, 0.001187233137898147, 0.001187233137898147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001187233137898147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37426162
Iteration 2/25 | Loss: 0.00144897
Iteration 3/25 | Loss: 0.00144897
Iteration 4/25 | Loss: 0.00144897
Iteration 5/25 | Loss: 0.00144897
Iteration 6/25 | Loss: 0.00144897
Iteration 7/25 | Loss: 0.00144897
Iteration 8/25 | Loss: 0.00144897
Iteration 9/25 | Loss: 0.00144897
Iteration 10/25 | Loss: 0.00144896
Iteration 11/25 | Loss: 0.00144896
Iteration 12/25 | Loss: 0.00144896
Iteration 13/25 | Loss: 0.00144896
Iteration 14/25 | Loss: 0.00144896
Iteration 15/25 | Loss: 0.00144896
Iteration 16/25 | Loss: 0.00144896
Iteration 17/25 | Loss: 0.00144896
Iteration 18/25 | Loss: 0.00144896
Iteration 19/25 | Loss: 0.00144896
Iteration 20/25 | Loss: 0.00144896
Iteration 21/25 | Loss: 0.00144896
Iteration 22/25 | Loss: 0.00144896
Iteration 23/25 | Loss: 0.00144896
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001448964118026197, 0.001448964118026197, 0.001448964118026197, 0.001448964118026197, 0.001448964118026197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001448964118026197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144896
Iteration 2/1000 | Loss: 0.00001883
Iteration 3/1000 | Loss: 0.00001323
Iteration 4/1000 | Loss: 0.00001194
Iteration 5/1000 | Loss: 0.00001138
Iteration 6/1000 | Loss: 0.00001090
Iteration 7/1000 | Loss: 0.00001044
Iteration 8/1000 | Loss: 0.00001014
Iteration 9/1000 | Loss: 0.00000997
Iteration 10/1000 | Loss: 0.00000994
Iteration 11/1000 | Loss: 0.00000967
Iteration 12/1000 | Loss: 0.00000954
Iteration 13/1000 | Loss: 0.00000954
Iteration 14/1000 | Loss: 0.00000952
Iteration 15/1000 | Loss: 0.00000937
Iteration 16/1000 | Loss: 0.00000931
Iteration 17/1000 | Loss: 0.00000931
Iteration 18/1000 | Loss: 0.00000931
Iteration 19/1000 | Loss: 0.00000929
Iteration 20/1000 | Loss: 0.00000928
Iteration 21/1000 | Loss: 0.00000924
Iteration 22/1000 | Loss: 0.00000923
Iteration 23/1000 | Loss: 0.00000922
Iteration 24/1000 | Loss: 0.00000921
Iteration 25/1000 | Loss: 0.00000921
Iteration 26/1000 | Loss: 0.00000920
Iteration 27/1000 | Loss: 0.00000920
Iteration 28/1000 | Loss: 0.00000917
Iteration 29/1000 | Loss: 0.00000916
Iteration 30/1000 | Loss: 0.00000915
Iteration 31/1000 | Loss: 0.00000915
Iteration 32/1000 | Loss: 0.00000914
Iteration 33/1000 | Loss: 0.00000909
Iteration 34/1000 | Loss: 0.00000909
Iteration 35/1000 | Loss: 0.00000908
Iteration 36/1000 | Loss: 0.00000908
Iteration 37/1000 | Loss: 0.00000908
Iteration 38/1000 | Loss: 0.00000908
Iteration 39/1000 | Loss: 0.00000907
Iteration 40/1000 | Loss: 0.00000907
Iteration 41/1000 | Loss: 0.00000907
Iteration 42/1000 | Loss: 0.00000907
Iteration 43/1000 | Loss: 0.00000907
Iteration 44/1000 | Loss: 0.00000905
Iteration 45/1000 | Loss: 0.00000904
Iteration 46/1000 | Loss: 0.00000903
Iteration 47/1000 | Loss: 0.00000903
Iteration 48/1000 | Loss: 0.00000903
Iteration 49/1000 | Loss: 0.00000903
Iteration 50/1000 | Loss: 0.00000903
Iteration 51/1000 | Loss: 0.00000903
Iteration 52/1000 | Loss: 0.00000903
Iteration 53/1000 | Loss: 0.00000903
Iteration 54/1000 | Loss: 0.00000903
Iteration 55/1000 | Loss: 0.00000903
Iteration 56/1000 | Loss: 0.00000903
Iteration 57/1000 | Loss: 0.00000903
Iteration 58/1000 | Loss: 0.00000903
Iteration 59/1000 | Loss: 0.00000903
Iteration 60/1000 | Loss: 0.00000903
Iteration 61/1000 | Loss: 0.00000903
Iteration 62/1000 | Loss: 0.00000903
Iteration 63/1000 | Loss: 0.00000903
Iteration 64/1000 | Loss: 0.00000903
Iteration 65/1000 | Loss: 0.00000903
Iteration 66/1000 | Loss: 0.00000903
Iteration 67/1000 | Loss: 0.00000903
Iteration 68/1000 | Loss: 0.00000903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [9.030727596837096e-06, 9.030727596837096e-06, 9.030727596837096e-06, 9.030727596837096e-06, 9.030727596837096e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.030727596837096e-06

Optimization complete. Final v2v error: 2.62290096282959 mm

Highest mean error: 3.0926191806793213 mm for frame 89

Lowest mean error: 2.469583749771118 mm for frame 2

Saving results

Total time: 31.75617480278015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00588764
Iteration 2/25 | Loss: 0.00133508
Iteration 3/25 | Loss: 0.00125771
Iteration 4/25 | Loss: 0.00124636
Iteration 5/25 | Loss: 0.00124246
Iteration 6/25 | Loss: 0.00124123
Iteration 7/25 | Loss: 0.00124123
Iteration 8/25 | Loss: 0.00124123
Iteration 9/25 | Loss: 0.00124123
Iteration 10/25 | Loss: 0.00124123
Iteration 11/25 | Loss: 0.00124123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012412316864356399, 0.0012412316864356399, 0.0012412316864356399, 0.0012412316864356399, 0.0012412316864356399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012412316864356399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.40769672
Iteration 2/25 | Loss: 0.00149687
Iteration 3/25 | Loss: 0.00149686
Iteration 4/25 | Loss: 0.00149686
Iteration 5/25 | Loss: 0.00149685
Iteration 6/25 | Loss: 0.00149685
Iteration 7/25 | Loss: 0.00149685
Iteration 8/25 | Loss: 0.00149685
Iteration 9/25 | Loss: 0.00149685
Iteration 10/25 | Loss: 0.00149685
Iteration 11/25 | Loss: 0.00149685
Iteration 12/25 | Loss: 0.00149685
Iteration 13/25 | Loss: 0.00149685
Iteration 14/25 | Loss: 0.00149685
Iteration 15/25 | Loss: 0.00149685
Iteration 16/25 | Loss: 0.00149685
Iteration 17/25 | Loss: 0.00149685
Iteration 18/25 | Loss: 0.00149685
Iteration 19/25 | Loss: 0.00149685
Iteration 20/25 | Loss: 0.00149685
Iteration 21/25 | Loss: 0.00149685
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001496851909905672, 0.001496851909905672, 0.001496851909905672, 0.001496851909905672, 0.001496851909905672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001496851909905672

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149685
Iteration 2/1000 | Loss: 0.00002977
Iteration 3/1000 | Loss: 0.00001916
Iteration 4/1000 | Loss: 0.00001720
Iteration 5/1000 | Loss: 0.00001625
Iteration 6/1000 | Loss: 0.00001570
Iteration 7/1000 | Loss: 0.00001528
Iteration 8/1000 | Loss: 0.00001496
Iteration 9/1000 | Loss: 0.00001472
Iteration 10/1000 | Loss: 0.00001441
Iteration 11/1000 | Loss: 0.00001416
Iteration 12/1000 | Loss: 0.00001405
Iteration 13/1000 | Loss: 0.00001392
Iteration 14/1000 | Loss: 0.00001391
Iteration 15/1000 | Loss: 0.00001383
Iteration 16/1000 | Loss: 0.00001375
Iteration 17/1000 | Loss: 0.00001371
Iteration 18/1000 | Loss: 0.00001369
Iteration 19/1000 | Loss: 0.00001368
Iteration 20/1000 | Loss: 0.00001367
Iteration 21/1000 | Loss: 0.00001367
Iteration 22/1000 | Loss: 0.00001366
Iteration 23/1000 | Loss: 0.00001366
Iteration 24/1000 | Loss: 0.00001362
Iteration 25/1000 | Loss: 0.00001361
Iteration 26/1000 | Loss: 0.00001360
Iteration 27/1000 | Loss: 0.00001359
Iteration 28/1000 | Loss: 0.00001351
Iteration 29/1000 | Loss: 0.00001351
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001349
Iteration 32/1000 | Loss: 0.00001348
Iteration 33/1000 | Loss: 0.00001348
Iteration 34/1000 | Loss: 0.00001347
Iteration 35/1000 | Loss: 0.00001347
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001344
Iteration 38/1000 | Loss: 0.00001343
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001339
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001338
Iteration 50/1000 | Loss: 0.00001338
Iteration 51/1000 | Loss: 0.00001338
Iteration 52/1000 | Loss: 0.00001337
Iteration 53/1000 | Loss: 0.00001336
Iteration 54/1000 | Loss: 0.00001336
Iteration 55/1000 | Loss: 0.00001336
Iteration 56/1000 | Loss: 0.00001335
Iteration 57/1000 | Loss: 0.00001335
Iteration 58/1000 | Loss: 0.00001335
Iteration 59/1000 | Loss: 0.00001335
Iteration 60/1000 | Loss: 0.00001335
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001335
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001335
Iteration 71/1000 | Loss: 0.00001335
Iteration 72/1000 | Loss: 0.00001335
Iteration 73/1000 | Loss: 0.00001335
Iteration 74/1000 | Loss: 0.00001335
Iteration 75/1000 | Loss: 0.00001335
Iteration 76/1000 | Loss: 0.00001335
Iteration 77/1000 | Loss: 0.00001335
Iteration 78/1000 | Loss: 0.00001335
Iteration 79/1000 | Loss: 0.00001335
Iteration 80/1000 | Loss: 0.00001335
Iteration 81/1000 | Loss: 0.00001335
Iteration 82/1000 | Loss: 0.00001335
Iteration 83/1000 | Loss: 0.00001335
Iteration 84/1000 | Loss: 0.00001335
Iteration 85/1000 | Loss: 0.00001335
Iteration 86/1000 | Loss: 0.00001335
Iteration 87/1000 | Loss: 0.00001335
Iteration 88/1000 | Loss: 0.00001335
Iteration 89/1000 | Loss: 0.00001335
Iteration 90/1000 | Loss: 0.00001335
Iteration 91/1000 | Loss: 0.00001335
Iteration 92/1000 | Loss: 0.00001335
Iteration 93/1000 | Loss: 0.00001335
Iteration 94/1000 | Loss: 0.00001335
Iteration 95/1000 | Loss: 0.00001335
Iteration 96/1000 | Loss: 0.00001335
Iteration 97/1000 | Loss: 0.00001335
Iteration 98/1000 | Loss: 0.00001335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.334516127826646e-05, 1.334516127826646e-05, 1.334516127826646e-05, 1.334516127826646e-05, 1.334516127826646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.334516127826646e-05

Optimization complete. Final v2v error: 3.111250162124634 mm

Highest mean error: 4.275302886962891 mm for frame 152

Lowest mean error: 2.769331216812134 mm for frame 30

Saving results

Total time: 36.411492109298706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776388
Iteration 2/25 | Loss: 0.00139589
Iteration 3/25 | Loss: 0.00127589
Iteration 4/25 | Loss: 0.00126794
Iteration 5/25 | Loss: 0.00127475
Iteration 6/25 | Loss: 0.00126053
Iteration 7/25 | Loss: 0.00125825
Iteration 8/25 | Loss: 0.00125447
Iteration 9/25 | Loss: 0.00125208
Iteration 10/25 | Loss: 0.00125180
Iteration 11/25 | Loss: 0.00125173
Iteration 12/25 | Loss: 0.00125173
Iteration 13/25 | Loss: 0.00125173
Iteration 14/25 | Loss: 0.00125173
Iteration 15/25 | Loss: 0.00125173
Iteration 16/25 | Loss: 0.00125173
Iteration 17/25 | Loss: 0.00125173
Iteration 18/25 | Loss: 0.00125173
Iteration 19/25 | Loss: 0.00125173
Iteration 20/25 | Loss: 0.00125172
Iteration 21/25 | Loss: 0.00125172
Iteration 22/25 | Loss: 0.00125172
Iteration 23/25 | Loss: 0.00125172
Iteration 24/25 | Loss: 0.00125172
Iteration 25/25 | Loss: 0.00125172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.48551750
Iteration 2/25 | Loss: 0.00122594
Iteration 3/25 | Loss: 0.00122588
Iteration 4/25 | Loss: 0.00122588
Iteration 5/25 | Loss: 0.00122587
Iteration 6/25 | Loss: 0.00122587
Iteration 7/25 | Loss: 0.00122587
Iteration 8/25 | Loss: 0.00122587
Iteration 9/25 | Loss: 0.00122587
Iteration 10/25 | Loss: 0.00122587
Iteration 11/25 | Loss: 0.00122587
Iteration 12/25 | Loss: 0.00122587
Iteration 13/25 | Loss: 0.00122587
Iteration 14/25 | Loss: 0.00122587
Iteration 15/25 | Loss: 0.00122587
Iteration 16/25 | Loss: 0.00122587
Iteration 17/25 | Loss: 0.00122587
Iteration 18/25 | Loss: 0.00122587
Iteration 19/25 | Loss: 0.00122587
Iteration 20/25 | Loss: 0.00122587
Iteration 21/25 | Loss: 0.00122587
Iteration 22/25 | Loss: 0.00122587
Iteration 23/25 | Loss: 0.00122587
Iteration 24/25 | Loss: 0.00122587
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012258721981197596, 0.0012258721981197596, 0.0012258721981197596, 0.0012258721981197596, 0.0012258721981197596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012258721981197596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122587
Iteration 2/1000 | Loss: 0.00002277
Iteration 3/1000 | Loss: 0.00001799
Iteration 4/1000 | Loss: 0.00036900
Iteration 5/1000 | Loss: 0.00003553
Iteration 6/1000 | Loss: 0.00002317
Iteration 7/1000 | Loss: 0.00001680
Iteration 8/1000 | Loss: 0.00001566
Iteration 9/1000 | Loss: 0.00001524
Iteration 10/1000 | Loss: 0.00001489
Iteration 11/1000 | Loss: 0.00001464
Iteration 12/1000 | Loss: 0.00001441
Iteration 13/1000 | Loss: 0.00001426
Iteration 14/1000 | Loss: 0.00001422
Iteration 15/1000 | Loss: 0.00001409
Iteration 16/1000 | Loss: 0.00001408
Iteration 17/1000 | Loss: 0.00001407
Iteration 18/1000 | Loss: 0.00001407
Iteration 19/1000 | Loss: 0.00001405
Iteration 20/1000 | Loss: 0.00001402
Iteration 21/1000 | Loss: 0.00001402
Iteration 22/1000 | Loss: 0.00001401
Iteration 23/1000 | Loss: 0.00001401
Iteration 24/1000 | Loss: 0.00001401
Iteration 25/1000 | Loss: 0.00001401
Iteration 26/1000 | Loss: 0.00001400
Iteration 27/1000 | Loss: 0.00001400
Iteration 28/1000 | Loss: 0.00001399
Iteration 29/1000 | Loss: 0.00001396
Iteration 30/1000 | Loss: 0.00001395
Iteration 31/1000 | Loss: 0.00001389
Iteration 32/1000 | Loss: 0.00001389
Iteration 33/1000 | Loss: 0.00001388
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001386
Iteration 36/1000 | Loss: 0.00001385
Iteration 37/1000 | Loss: 0.00001385
Iteration 38/1000 | Loss: 0.00001385
Iteration 39/1000 | Loss: 0.00001385
Iteration 40/1000 | Loss: 0.00001382
Iteration 41/1000 | Loss: 0.00001380
Iteration 42/1000 | Loss: 0.00001379
Iteration 43/1000 | Loss: 0.00001379
Iteration 44/1000 | Loss: 0.00001379
Iteration 45/1000 | Loss: 0.00001379
Iteration 46/1000 | Loss: 0.00001379
Iteration 47/1000 | Loss: 0.00001379
Iteration 48/1000 | Loss: 0.00001379
Iteration 49/1000 | Loss: 0.00001379
Iteration 50/1000 | Loss: 0.00001379
Iteration 51/1000 | Loss: 0.00001378
Iteration 52/1000 | Loss: 0.00001378
Iteration 53/1000 | Loss: 0.00001376
Iteration 54/1000 | Loss: 0.00001375
Iteration 55/1000 | Loss: 0.00001375
Iteration 56/1000 | Loss: 0.00001374
Iteration 57/1000 | Loss: 0.00001371
Iteration 58/1000 | Loss: 0.00001371
Iteration 59/1000 | Loss: 0.00001369
Iteration 60/1000 | Loss: 0.00001367
Iteration 61/1000 | Loss: 0.00001367
Iteration 62/1000 | Loss: 0.00001367
Iteration 63/1000 | Loss: 0.00001367
Iteration 64/1000 | Loss: 0.00001367
Iteration 65/1000 | Loss: 0.00001367
Iteration 66/1000 | Loss: 0.00001366
Iteration 67/1000 | Loss: 0.00001366
Iteration 68/1000 | Loss: 0.00001366
Iteration 69/1000 | Loss: 0.00001365
Iteration 70/1000 | Loss: 0.00001365
Iteration 71/1000 | Loss: 0.00001364
Iteration 72/1000 | Loss: 0.00001364
Iteration 73/1000 | Loss: 0.00001364
Iteration 74/1000 | Loss: 0.00001363
Iteration 75/1000 | Loss: 0.00001363
Iteration 76/1000 | Loss: 0.00001363
Iteration 77/1000 | Loss: 0.00001362
Iteration 78/1000 | Loss: 0.00001362
Iteration 79/1000 | Loss: 0.00001362
Iteration 80/1000 | Loss: 0.00001362
Iteration 81/1000 | Loss: 0.00001361
Iteration 82/1000 | Loss: 0.00001361
Iteration 83/1000 | Loss: 0.00001361
Iteration 84/1000 | Loss: 0.00001361
Iteration 85/1000 | Loss: 0.00001361
Iteration 86/1000 | Loss: 0.00001361
Iteration 87/1000 | Loss: 0.00001361
Iteration 88/1000 | Loss: 0.00001361
Iteration 89/1000 | Loss: 0.00001360
Iteration 90/1000 | Loss: 0.00001359
Iteration 91/1000 | Loss: 0.00001359
Iteration 92/1000 | Loss: 0.00001359
Iteration 93/1000 | Loss: 0.00001359
Iteration 94/1000 | Loss: 0.00001358
Iteration 95/1000 | Loss: 0.00001358
Iteration 96/1000 | Loss: 0.00001358
Iteration 97/1000 | Loss: 0.00001358
Iteration 98/1000 | Loss: 0.00001358
Iteration 99/1000 | Loss: 0.00001358
Iteration 100/1000 | Loss: 0.00001356
Iteration 101/1000 | Loss: 0.00001356
Iteration 102/1000 | Loss: 0.00001355
Iteration 103/1000 | Loss: 0.00001355
Iteration 104/1000 | Loss: 0.00001355
Iteration 105/1000 | Loss: 0.00001354
Iteration 106/1000 | Loss: 0.00001354
Iteration 107/1000 | Loss: 0.00001353
Iteration 108/1000 | Loss: 0.00001353
Iteration 109/1000 | Loss: 0.00001353
Iteration 110/1000 | Loss: 0.00001352
Iteration 111/1000 | Loss: 0.00001352
Iteration 112/1000 | Loss: 0.00001352
Iteration 113/1000 | Loss: 0.00001352
Iteration 114/1000 | Loss: 0.00001352
Iteration 115/1000 | Loss: 0.00001352
Iteration 116/1000 | Loss: 0.00001352
Iteration 117/1000 | Loss: 0.00001351
Iteration 118/1000 | Loss: 0.00001351
Iteration 119/1000 | Loss: 0.00001351
Iteration 120/1000 | Loss: 0.00001351
Iteration 121/1000 | Loss: 0.00001351
Iteration 122/1000 | Loss: 0.00001351
Iteration 123/1000 | Loss: 0.00001351
Iteration 124/1000 | Loss: 0.00001351
Iteration 125/1000 | Loss: 0.00001350
Iteration 126/1000 | Loss: 0.00001350
Iteration 127/1000 | Loss: 0.00001350
Iteration 128/1000 | Loss: 0.00001350
Iteration 129/1000 | Loss: 0.00001350
Iteration 130/1000 | Loss: 0.00001350
Iteration 131/1000 | Loss: 0.00001350
Iteration 132/1000 | Loss: 0.00001350
Iteration 133/1000 | Loss: 0.00001349
Iteration 134/1000 | Loss: 0.00001349
Iteration 135/1000 | Loss: 0.00001349
Iteration 136/1000 | Loss: 0.00001349
Iteration 137/1000 | Loss: 0.00001349
Iteration 138/1000 | Loss: 0.00001349
Iteration 139/1000 | Loss: 0.00001349
Iteration 140/1000 | Loss: 0.00001349
Iteration 141/1000 | Loss: 0.00001349
Iteration 142/1000 | Loss: 0.00001349
Iteration 143/1000 | Loss: 0.00001349
Iteration 144/1000 | Loss: 0.00001349
Iteration 145/1000 | Loss: 0.00001348
Iteration 146/1000 | Loss: 0.00001348
Iteration 147/1000 | Loss: 0.00001348
Iteration 148/1000 | Loss: 0.00001348
Iteration 149/1000 | Loss: 0.00001348
Iteration 150/1000 | Loss: 0.00001347
Iteration 151/1000 | Loss: 0.00001347
Iteration 152/1000 | Loss: 0.00001347
Iteration 153/1000 | Loss: 0.00001347
Iteration 154/1000 | Loss: 0.00001346
Iteration 155/1000 | Loss: 0.00001346
Iteration 156/1000 | Loss: 0.00001346
Iteration 157/1000 | Loss: 0.00001346
Iteration 158/1000 | Loss: 0.00001346
Iteration 159/1000 | Loss: 0.00001346
Iteration 160/1000 | Loss: 0.00001345
Iteration 161/1000 | Loss: 0.00001345
Iteration 162/1000 | Loss: 0.00001345
Iteration 163/1000 | Loss: 0.00001345
Iteration 164/1000 | Loss: 0.00001345
Iteration 165/1000 | Loss: 0.00001345
Iteration 166/1000 | Loss: 0.00001345
Iteration 167/1000 | Loss: 0.00001345
Iteration 168/1000 | Loss: 0.00001345
Iteration 169/1000 | Loss: 0.00001345
Iteration 170/1000 | Loss: 0.00001345
Iteration 171/1000 | Loss: 0.00001345
Iteration 172/1000 | Loss: 0.00001345
Iteration 173/1000 | Loss: 0.00001345
Iteration 174/1000 | Loss: 0.00001345
Iteration 175/1000 | Loss: 0.00001345
Iteration 176/1000 | Loss: 0.00001344
Iteration 177/1000 | Loss: 0.00001344
Iteration 178/1000 | Loss: 0.00001344
Iteration 179/1000 | Loss: 0.00001344
Iteration 180/1000 | Loss: 0.00001344
Iteration 181/1000 | Loss: 0.00001344
Iteration 182/1000 | Loss: 0.00001344
Iteration 183/1000 | Loss: 0.00001344
Iteration 184/1000 | Loss: 0.00001344
Iteration 185/1000 | Loss: 0.00001344
Iteration 186/1000 | Loss: 0.00001344
Iteration 187/1000 | Loss: 0.00001344
Iteration 188/1000 | Loss: 0.00001344
Iteration 189/1000 | Loss: 0.00001344
Iteration 190/1000 | Loss: 0.00001344
Iteration 191/1000 | Loss: 0.00001344
Iteration 192/1000 | Loss: 0.00001344
Iteration 193/1000 | Loss: 0.00001344
Iteration 194/1000 | Loss: 0.00001344
Iteration 195/1000 | Loss: 0.00001344
Iteration 196/1000 | Loss: 0.00001344
Iteration 197/1000 | Loss: 0.00001344
Iteration 198/1000 | Loss: 0.00001344
Iteration 199/1000 | Loss: 0.00001344
Iteration 200/1000 | Loss: 0.00001344
Iteration 201/1000 | Loss: 0.00001344
Iteration 202/1000 | Loss: 0.00001344
Iteration 203/1000 | Loss: 0.00001344
Iteration 204/1000 | Loss: 0.00001344
Iteration 205/1000 | Loss: 0.00001344
Iteration 206/1000 | Loss: 0.00001344
Iteration 207/1000 | Loss: 0.00001344
Iteration 208/1000 | Loss: 0.00001344
Iteration 209/1000 | Loss: 0.00001344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.3442433555610478e-05, 1.3442433555610478e-05, 1.3442433555610478e-05, 1.3442433555610478e-05, 1.3442433555610478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3442433555610478e-05

Optimization complete. Final v2v error: 3.0931625366210938 mm

Highest mean error: 3.559342861175537 mm for frame 174

Lowest mean error: 2.672799825668335 mm for frame 107

Saving results

Total time: 61.84666728973389
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507208
Iteration 2/25 | Loss: 0.00132161
Iteration 3/25 | Loss: 0.00124764
Iteration 4/25 | Loss: 0.00123195
Iteration 5/25 | Loss: 0.00122679
Iteration 6/25 | Loss: 0.00122544
Iteration 7/25 | Loss: 0.00122544
Iteration 8/25 | Loss: 0.00122544
Iteration 9/25 | Loss: 0.00122544
Iteration 10/25 | Loss: 0.00122544
Iteration 11/25 | Loss: 0.00122544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012254392495378852, 0.0012254392495378852, 0.0012254392495378852, 0.0012254392495378852, 0.0012254392495378852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012254392495378852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.95665169
Iteration 2/25 | Loss: 0.00153534
Iteration 3/25 | Loss: 0.00153534
Iteration 4/25 | Loss: 0.00153534
Iteration 5/25 | Loss: 0.00153534
Iteration 6/25 | Loss: 0.00153534
Iteration 7/25 | Loss: 0.00153534
Iteration 8/25 | Loss: 0.00153534
Iteration 9/25 | Loss: 0.00153534
Iteration 10/25 | Loss: 0.00153534
Iteration 11/25 | Loss: 0.00153534
Iteration 12/25 | Loss: 0.00153534
Iteration 13/25 | Loss: 0.00153534
Iteration 14/25 | Loss: 0.00153534
Iteration 15/25 | Loss: 0.00153534
Iteration 16/25 | Loss: 0.00153534
Iteration 17/25 | Loss: 0.00153534
Iteration 18/25 | Loss: 0.00153534
Iteration 19/25 | Loss: 0.00153534
Iteration 20/25 | Loss: 0.00153534
Iteration 21/25 | Loss: 0.00153534
Iteration 22/25 | Loss: 0.00153534
Iteration 23/25 | Loss: 0.00153534
Iteration 24/25 | Loss: 0.00153534
Iteration 25/25 | Loss: 0.00153534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153534
Iteration 2/1000 | Loss: 0.00003980
Iteration 3/1000 | Loss: 0.00002675
Iteration 4/1000 | Loss: 0.00002178
Iteration 5/1000 | Loss: 0.00002053
Iteration 6/1000 | Loss: 0.00001976
Iteration 7/1000 | Loss: 0.00001937
Iteration 8/1000 | Loss: 0.00001893
Iteration 9/1000 | Loss: 0.00001855
Iteration 10/1000 | Loss: 0.00001834
Iteration 11/1000 | Loss: 0.00001798
Iteration 12/1000 | Loss: 0.00001776
Iteration 13/1000 | Loss: 0.00001760
Iteration 14/1000 | Loss: 0.00001751
Iteration 15/1000 | Loss: 0.00001732
Iteration 16/1000 | Loss: 0.00001724
Iteration 17/1000 | Loss: 0.00001723
Iteration 18/1000 | Loss: 0.00001723
Iteration 19/1000 | Loss: 0.00001722
Iteration 20/1000 | Loss: 0.00001716
Iteration 21/1000 | Loss: 0.00001714
Iteration 22/1000 | Loss: 0.00001714
Iteration 23/1000 | Loss: 0.00001709
Iteration 24/1000 | Loss: 0.00001709
Iteration 25/1000 | Loss: 0.00001707
Iteration 26/1000 | Loss: 0.00001706
Iteration 27/1000 | Loss: 0.00001705
Iteration 28/1000 | Loss: 0.00001705
Iteration 29/1000 | Loss: 0.00001704
Iteration 30/1000 | Loss: 0.00001704
Iteration 31/1000 | Loss: 0.00001703
Iteration 32/1000 | Loss: 0.00001702
Iteration 33/1000 | Loss: 0.00001701
Iteration 34/1000 | Loss: 0.00001701
Iteration 35/1000 | Loss: 0.00001701
Iteration 36/1000 | Loss: 0.00001700
Iteration 37/1000 | Loss: 0.00001700
Iteration 38/1000 | Loss: 0.00001700
Iteration 39/1000 | Loss: 0.00001699
Iteration 40/1000 | Loss: 0.00001699
Iteration 41/1000 | Loss: 0.00001699
Iteration 42/1000 | Loss: 0.00001698
Iteration 43/1000 | Loss: 0.00001698
Iteration 44/1000 | Loss: 0.00001698
Iteration 45/1000 | Loss: 0.00001698
Iteration 46/1000 | Loss: 0.00001697
Iteration 47/1000 | Loss: 0.00001697
Iteration 48/1000 | Loss: 0.00001696
Iteration 49/1000 | Loss: 0.00001696
Iteration 50/1000 | Loss: 0.00001696
Iteration 51/1000 | Loss: 0.00001695
Iteration 52/1000 | Loss: 0.00001695
Iteration 53/1000 | Loss: 0.00001694
Iteration 54/1000 | Loss: 0.00001694
Iteration 55/1000 | Loss: 0.00001694
Iteration 56/1000 | Loss: 0.00001693
Iteration 57/1000 | Loss: 0.00001693
Iteration 58/1000 | Loss: 0.00001692
Iteration 59/1000 | Loss: 0.00001692
Iteration 60/1000 | Loss: 0.00001692
Iteration 61/1000 | Loss: 0.00001691
Iteration 62/1000 | Loss: 0.00001691
Iteration 63/1000 | Loss: 0.00001690
Iteration 64/1000 | Loss: 0.00001690
Iteration 65/1000 | Loss: 0.00001690
Iteration 66/1000 | Loss: 0.00001689
Iteration 67/1000 | Loss: 0.00001689
Iteration 68/1000 | Loss: 0.00001689
Iteration 69/1000 | Loss: 0.00001688
Iteration 70/1000 | Loss: 0.00001688
Iteration 71/1000 | Loss: 0.00001688
Iteration 72/1000 | Loss: 0.00001687
Iteration 73/1000 | Loss: 0.00001687
Iteration 74/1000 | Loss: 0.00001686
Iteration 75/1000 | Loss: 0.00001686
Iteration 76/1000 | Loss: 0.00001686
Iteration 77/1000 | Loss: 0.00001686
Iteration 78/1000 | Loss: 0.00001685
Iteration 79/1000 | Loss: 0.00001685
Iteration 80/1000 | Loss: 0.00001685
Iteration 81/1000 | Loss: 0.00001683
Iteration 82/1000 | Loss: 0.00001682
Iteration 83/1000 | Loss: 0.00001682
Iteration 84/1000 | Loss: 0.00001682
Iteration 85/1000 | Loss: 0.00001681
Iteration 86/1000 | Loss: 0.00001681
Iteration 87/1000 | Loss: 0.00001680
Iteration 88/1000 | Loss: 0.00001680
Iteration 89/1000 | Loss: 0.00001680
Iteration 90/1000 | Loss: 0.00001680
Iteration 91/1000 | Loss: 0.00001680
Iteration 92/1000 | Loss: 0.00001679
Iteration 93/1000 | Loss: 0.00001679
Iteration 94/1000 | Loss: 0.00001678
Iteration 95/1000 | Loss: 0.00001678
Iteration 96/1000 | Loss: 0.00001678
Iteration 97/1000 | Loss: 0.00001677
Iteration 98/1000 | Loss: 0.00001677
Iteration 99/1000 | Loss: 0.00001677
Iteration 100/1000 | Loss: 0.00001677
Iteration 101/1000 | Loss: 0.00001677
Iteration 102/1000 | Loss: 0.00001676
Iteration 103/1000 | Loss: 0.00001676
Iteration 104/1000 | Loss: 0.00001675
Iteration 105/1000 | Loss: 0.00001675
Iteration 106/1000 | Loss: 0.00001675
Iteration 107/1000 | Loss: 0.00001675
Iteration 108/1000 | Loss: 0.00001675
Iteration 109/1000 | Loss: 0.00001675
Iteration 110/1000 | Loss: 0.00001675
Iteration 111/1000 | Loss: 0.00001674
Iteration 112/1000 | Loss: 0.00001674
Iteration 113/1000 | Loss: 0.00001674
Iteration 114/1000 | Loss: 0.00001673
Iteration 115/1000 | Loss: 0.00001673
Iteration 116/1000 | Loss: 0.00001673
Iteration 117/1000 | Loss: 0.00001672
Iteration 118/1000 | Loss: 0.00001672
Iteration 119/1000 | Loss: 0.00001672
Iteration 120/1000 | Loss: 0.00001672
Iteration 121/1000 | Loss: 0.00001672
Iteration 122/1000 | Loss: 0.00001672
Iteration 123/1000 | Loss: 0.00001672
Iteration 124/1000 | Loss: 0.00001672
Iteration 125/1000 | Loss: 0.00001671
Iteration 126/1000 | Loss: 0.00001671
Iteration 127/1000 | Loss: 0.00001671
Iteration 128/1000 | Loss: 0.00001671
Iteration 129/1000 | Loss: 0.00001670
Iteration 130/1000 | Loss: 0.00001670
Iteration 131/1000 | Loss: 0.00001670
Iteration 132/1000 | Loss: 0.00001669
Iteration 133/1000 | Loss: 0.00001669
Iteration 134/1000 | Loss: 0.00001669
Iteration 135/1000 | Loss: 0.00001668
Iteration 136/1000 | Loss: 0.00001668
Iteration 137/1000 | Loss: 0.00001668
Iteration 138/1000 | Loss: 0.00001668
Iteration 139/1000 | Loss: 0.00001668
Iteration 140/1000 | Loss: 0.00001668
Iteration 141/1000 | Loss: 0.00001668
Iteration 142/1000 | Loss: 0.00001668
Iteration 143/1000 | Loss: 0.00001668
Iteration 144/1000 | Loss: 0.00001667
Iteration 145/1000 | Loss: 0.00001667
Iteration 146/1000 | Loss: 0.00001667
Iteration 147/1000 | Loss: 0.00001667
Iteration 148/1000 | Loss: 0.00001667
Iteration 149/1000 | Loss: 0.00001667
Iteration 150/1000 | Loss: 0.00001667
Iteration 151/1000 | Loss: 0.00001667
Iteration 152/1000 | Loss: 0.00001667
Iteration 153/1000 | Loss: 0.00001666
Iteration 154/1000 | Loss: 0.00001666
Iteration 155/1000 | Loss: 0.00001666
Iteration 156/1000 | Loss: 0.00001666
Iteration 157/1000 | Loss: 0.00001666
Iteration 158/1000 | Loss: 0.00001666
Iteration 159/1000 | Loss: 0.00001666
Iteration 160/1000 | Loss: 0.00001666
Iteration 161/1000 | Loss: 0.00001666
Iteration 162/1000 | Loss: 0.00001666
Iteration 163/1000 | Loss: 0.00001666
Iteration 164/1000 | Loss: 0.00001666
Iteration 165/1000 | Loss: 0.00001666
Iteration 166/1000 | Loss: 0.00001666
Iteration 167/1000 | Loss: 0.00001665
Iteration 168/1000 | Loss: 0.00001665
Iteration 169/1000 | Loss: 0.00001665
Iteration 170/1000 | Loss: 0.00001665
Iteration 171/1000 | Loss: 0.00001665
Iteration 172/1000 | Loss: 0.00001665
Iteration 173/1000 | Loss: 0.00001665
Iteration 174/1000 | Loss: 0.00001665
Iteration 175/1000 | Loss: 0.00001665
Iteration 176/1000 | Loss: 0.00001665
Iteration 177/1000 | Loss: 0.00001665
Iteration 178/1000 | Loss: 0.00001665
Iteration 179/1000 | Loss: 0.00001665
Iteration 180/1000 | Loss: 0.00001665
Iteration 181/1000 | Loss: 0.00001664
Iteration 182/1000 | Loss: 0.00001664
Iteration 183/1000 | Loss: 0.00001664
Iteration 184/1000 | Loss: 0.00001664
Iteration 185/1000 | Loss: 0.00001664
Iteration 186/1000 | Loss: 0.00001664
Iteration 187/1000 | Loss: 0.00001664
Iteration 188/1000 | Loss: 0.00001664
Iteration 189/1000 | Loss: 0.00001664
Iteration 190/1000 | Loss: 0.00001663
Iteration 191/1000 | Loss: 0.00001663
Iteration 192/1000 | Loss: 0.00001663
Iteration 193/1000 | Loss: 0.00001663
Iteration 194/1000 | Loss: 0.00001663
Iteration 195/1000 | Loss: 0.00001663
Iteration 196/1000 | Loss: 0.00001663
Iteration 197/1000 | Loss: 0.00001663
Iteration 198/1000 | Loss: 0.00001663
Iteration 199/1000 | Loss: 0.00001663
Iteration 200/1000 | Loss: 0.00001663
Iteration 201/1000 | Loss: 0.00001663
Iteration 202/1000 | Loss: 0.00001663
Iteration 203/1000 | Loss: 0.00001663
Iteration 204/1000 | Loss: 0.00001663
Iteration 205/1000 | Loss: 0.00001663
Iteration 206/1000 | Loss: 0.00001662
Iteration 207/1000 | Loss: 0.00001662
Iteration 208/1000 | Loss: 0.00001662
Iteration 209/1000 | Loss: 0.00001662
Iteration 210/1000 | Loss: 0.00001662
Iteration 211/1000 | Loss: 0.00001662
Iteration 212/1000 | Loss: 0.00001662
Iteration 213/1000 | Loss: 0.00001662
Iteration 214/1000 | Loss: 0.00001662
Iteration 215/1000 | Loss: 0.00001662
Iteration 216/1000 | Loss: 0.00001662
Iteration 217/1000 | Loss: 0.00001661
Iteration 218/1000 | Loss: 0.00001661
Iteration 219/1000 | Loss: 0.00001661
Iteration 220/1000 | Loss: 0.00001661
Iteration 221/1000 | Loss: 0.00001661
Iteration 222/1000 | Loss: 0.00001661
Iteration 223/1000 | Loss: 0.00001661
Iteration 224/1000 | Loss: 0.00001661
Iteration 225/1000 | Loss: 0.00001661
Iteration 226/1000 | Loss: 0.00001660
Iteration 227/1000 | Loss: 0.00001660
Iteration 228/1000 | Loss: 0.00001660
Iteration 229/1000 | Loss: 0.00001660
Iteration 230/1000 | Loss: 0.00001660
Iteration 231/1000 | Loss: 0.00001660
Iteration 232/1000 | Loss: 0.00001660
Iteration 233/1000 | Loss: 0.00001660
Iteration 234/1000 | Loss: 0.00001660
Iteration 235/1000 | Loss: 0.00001660
Iteration 236/1000 | Loss: 0.00001660
Iteration 237/1000 | Loss: 0.00001660
Iteration 238/1000 | Loss: 0.00001660
Iteration 239/1000 | Loss: 0.00001660
Iteration 240/1000 | Loss: 0.00001660
Iteration 241/1000 | Loss: 0.00001660
Iteration 242/1000 | Loss: 0.00001660
Iteration 243/1000 | Loss: 0.00001660
Iteration 244/1000 | Loss: 0.00001660
Iteration 245/1000 | Loss: 0.00001660
Iteration 246/1000 | Loss: 0.00001660
Iteration 247/1000 | Loss: 0.00001660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [1.6604237316641957e-05, 1.6604237316641957e-05, 1.6604237316641957e-05, 1.6604237316641957e-05, 1.6604237316641957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6604237316641957e-05

Optimization complete. Final v2v error: 3.4367449283599854 mm

Highest mean error: 4.571484088897705 mm for frame 67

Lowest mean error: 2.930217981338501 mm for frame 18

Saving results

Total time: 48.71419811248779
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418061
Iteration 2/25 | Loss: 0.00132076
Iteration 3/25 | Loss: 0.00124024
Iteration 4/25 | Loss: 0.00122878
Iteration 5/25 | Loss: 0.00122560
Iteration 6/25 | Loss: 0.00122560
Iteration 7/25 | Loss: 0.00122560
Iteration 8/25 | Loss: 0.00122560
Iteration 9/25 | Loss: 0.00122560
Iteration 10/25 | Loss: 0.00122560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001225602230988443, 0.001225602230988443, 0.001225602230988443, 0.001225602230988443, 0.001225602230988443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001225602230988443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32873809
Iteration 2/25 | Loss: 0.00162575
Iteration 3/25 | Loss: 0.00162575
Iteration 4/25 | Loss: 0.00162575
Iteration 5/25 | Loss: 0.00162575
Iteration 6/25 | Loss: 0.00162575
Iteration 7/25 | Loss: 0.00162575
Iteration 8/25 | Loss: 0.00162575
Iteration 9/25 | Loss: 0.00162575
Iteration 10/25 | Loss: 0.00162575
Iteration 11/25 | Loss: 0.00162575
Iteration 12/25 | Loss: 0.00162575
Iteration 13/25 | Loss: 0.00162575
Iteration 14/25 | Loss: 0.00162575
Iteration 15/25 | Loss: 0.00162575
Iteration 16/25 | Loss: 0.00162575
Iteration 17/25 | Loss: 0.00162575
Iteration 18/25 | Loss: 0.00162575
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016257498646155, 0.0016257498646155, 0.0016257498646155, 0.0016257498646155, 0.0016257498646155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016257498646155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162575
Iteration 2/1000 | Loss: 0.00002491
Iteration 3/1000 | Loss: 0.00001742
Iteration 4/1000 | Loss: 0.00001515
Iteration 5/1000 | Loss: 0.00001431
Iteration 6/1000 | Loss: 0.00001389
Iteration 7/1000 | Loss: 0.00001357
Iteration 8/1000 | Loss: 0.00001337
Iteration 9/1000 | Loss: 0.00001311
Iteration 10/1000 | Loss: 0.00001270
Iteration 11/1000 | Loss: 0.00001264
Iteration 12/1000 | Loss: 0.00001242
Iteration 13/1000 | Loss: 0.00001232
Iteration 14/1000 | Loss: 0.00001231
Iteration 15/1000 | Loss: 0.00001231
Iteration 16/1000 | Loss: 0.00001230
Iteration 17/1000 | Loss: 0.00001229
Iteration 18/1000 | Loss: 0.00001223
Iteration 19/1000 | Loss: 0.00001218
Iteration 20/1000 | Loss: 0.00001214
Iteration 21/1000 | Loss: 0.00001212
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001209
Iteration 24/1000 | Loss: 0.00001209
Iteration 25/1000 | Loss: 0.00001208
Iteration 26/1000 | Loss: 0.00001207
Iteration 27/1000 | Loss: 0.00001206
Iteration 28/1000 | Loss: 0.00001206
Iteration 29/1000 | Loss: 0.00001205
Iteration 30/1000 | Loss: 0.00001205
Iteration 31/1000 | Loss: 0.00001204
Iteration 32/1000 | Loss: 0.00001204
Iteration 33/1000 | Loss: 0.00001203
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001203
Iteration 36/1000 | Loss: 0.00001201
Iteration 37/1000 | Loss: 0.00001200
Iteration 38/1000 | Loss: 0.00001196
Iteration 39/1000 | Loss: 0.00001195
Iteration 40/1000 | Loss: 0.00001195
Iteration 41/1000 | Loss: 0.00001192
Iteration 42/1000 | Loss: 0.00001191
Iteration 43/1000 | Loss: 0.00001190
Iteration 44/1000 | Loss: 0.00001190
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001189
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001188
Iteration 49/1000 | Loss: 0.00001187
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001186
Iteration 52/1000 | Loss: 0.00001186
Iteration 53/1000 | Loss: 0.00001186
Iteration 54/1000 | Loss: 0.00001186
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001185
Iteration 57/1000 | Loss: 0.00001184
Iteration 58/1000 | Loss: 0.00001183
Iteration 59/1000 | Loss: 0.00001182
Iteration 60/1000 | Loss: 0.00001182
Iteration 61/1000 | Loss: 0.00001181
Iteration 62/1000 | Loss: 0.00001181
Iteration 63/1000 | Loss: 0.00001181
Iteration 64/1000 | Loss: 0.00001181
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001180
Iteration 67/1000 | Loss: 0.00001180
Iteration 68/1000 | Loss: 0.00001180
Iteration 69/1000 | Loss: 0.00001180
Iteration 70/1000 | Loss: 0.00001180
Iteration 71/1000 | Loss: 0.00001180
Iteration 72/1000 | Loss: 0.00001179
Iteration 73/1000 | Loss: 0.00001179
Iteration 74/1000 | Loss: 0.00001179
Iteration 75/1000 | Loss: 0.00001179
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001178
Iteration 78/1000 | Loss: 0.00001178
Iteration 79/1000 | Loss: 0.00001178
Iteration 80/1000 | Loss: 0.00001178
Iteration 81/1000 | Loss: 0.00001178
Iteration 82/1000 | Loss: 0.00001178
Iteration 83/1000 | Loss: 0.00001178
Iteration 84/1000 | Loss: 0.00001178
Iteration 85/1000 | Loss: 0.00001177
Iteration 86/1000 | Loss: 0.00001177
Iteration 87/1000 | Loss: 0.00001177
Iteration 88/1000 | Loss: 0.00001177
Iteration 89/1000 | Loss: 0.00001176
Iteration 90/1000 | Loss: 0.00001176
Iteration 91/1000 | Loss: 0.00001176
Iteration 92/1000 | Loss: 0.00001176
Iteration 93/1000 | Loss: 0.00001175
Iteration 94/1000 | Loss: 0.00001175
Iteration 95/1000 | Loss: 0.00001175
Iteration 96/1000 | Loss: 0.00001175
Iteration 97/1000 | Loss: 0.00001175
Iteration 98/1000 | Loss: 0.00001175
Iteration 99/1000 | Loss: 0.00001175
Iteration 100/1000 | Loss: 0.00001175
Iteration 101/1000 | Loss: 0.00001175
Iteration 102/1000 | Loss: 0.00001175
Iteration 103/1000 | Loss: 0.00001175
Iteration 104/1000 | Loss: 0.00001174
Iteration 105/1000 | Loss: 0.00001174
Iteration 106/1000 | Loss: 0.00001174
Iteration 107/1000 | Loss: 0.00001174
Iteration 108/1000 | Loss: 0.00001174
Iteration 109/1000 | Loss: 0.00001174
Iteration 110/1000 | Loss: 0.00001173
Iteration 111/1000 | Loss: 0.00001173
Iteration 112/1000 | Loss: 0.00001173
Iteration 113/1000 | Loss: 0.00001173
Iteration 114/1000 | Loss: 0.00001173
Iteration 115/1000 | Loss: 0.00001173
Iteration 116/1000 | Loss: 0.00001173
Iteration 117/1000 | Loss: 0.00001173
Iteration 118/1000 | Loss: 0.00001173
Iteration 119/1000 | Loss: 0.00001173
Iteration 120/1000 | Loss: 0.00001173
Iteration 121/1000 | Loss: 0.00001173
Iteration 122/1000 | Loss: 0.00001173
Iteration 123/1000 | Loss: 0.00001173
Iteration 124/1000 | Loss: 0.00001173
Iteration 125/1000 | Loss: 0.00001172
Iteration 126/1000 | Loss: 0.00001172
Iteration 127/1000 | Loss: 0.00001172
Iteration 128/1000 | Loss: 0.00001172
Iteration 129/1000 | Loss: 0.00001172
Iteration 130/1000 | Loss: 0.00001172
Iteration 131/1000 | Loss: 0.00001172
Iteration 132/1000 | Loss: 0.00001172
Iteration 133/1000 | Loss: 0.00001172
Iteration 134/1000 | Loss: 0.00001172
Iteration 135/1000 | Loss: 0.00001172
Iteration 136/1000 | Loss: 0.00001172
Iteration 137/1000 | Loss: 0.00001172
Iteration 138/1000 | Loss: 0.00001172
Iteration 139/1000 | Loss: 0.00001172
Iteration 140/1000 | Loss: 0.00001172
Iteration 141/1000 | Loss: 0.00001172
Iteration 142/1000 | Loss: 0.00001172
Iteration 143/1000 | Loss: 0.00001171
Iteration 144/1000 | Loss: 0.00001171
Iteration 145/1000 | Loss: 0.00001171
Iteration 146/1000 | Loss: 0.00001171
Iteration 147/1000 | Loss: 0.00001171
Iteration 148/1000 | Loss: 0.00001171
Iteration 149/1000 | Loss: 0.00001171
Iteration 150/1000 | Loss: 0.00001171
Iteration 151/1000 | Loss: 0.00001171
Iteration 152/1000 | Loss: 0.00001171
Iteration 153/1000 | Loss: 0.00001171
Iteration 154/1000 | Loss: 0.00001171
Iteration 155/1000 | Loss: 0.00001171
Iteration 156/1000 | Loss: 0.00001171
Iteration 157/1000 | Loss: 0.00001171
Iteration 158/1000 | Loss: 0.00001171
Iteration 159/1000 | Loss: 0.00001171
Iteration 160/1000 | Loss: 0.00001171
Iteration 161/1000 | Loss: 0.00001171
Iteration 162/1000 | Loss: 0.00001170
Iteration 163/1000 | Loss: 0.00001170
Iteration 164/1000 | Loss: 0.00001170
Iteration 165/1000 | Loss: 0.00001170
Iteration 166/1000 | Loss: 0.00001170
Iteration 167/1000 | Loss: 0.00001170
Iteration 168/1000 | Loss: 0.00001170
Iteration 169/1000 | Loss: 0.00001170
Iteration 170/1000 | Loss: 0.00001170
Iteration 171/1000 | Loss: 0.00001170
Iteration 172/1000 | Loss: 0.00001170
Iteration 173/1000 | Loss: 0.00001170
Iteration 174/1000 | Loss: 0.00001170
Iteration 175/1000 | Loss: 0.00001170
Iteration 176/1000 | Loss: 0.00001170
Iteration 177/1000 | Loss: 0.00001170
Iteration 178/1000 | Loss: 0.00001169
Iteration 179/1000 | Loss: 0.00001169
Iteration 180/1000 | Loss: 0.00001169
Iteration 181/1000 | Loss: 0.00001169
Iteration 182/1000 | Loss: 0.00001169
Iteration 183/1000 | Loss: 0.00001169
Iteration 184/1000 | Loss: 0.00001169
Iteration 185/1000 | Loss: 0.00001169
Iteration 186/1000 | Loss: 0.00001169
Iteration 187/1000 | Loss: 0.00001169
Iteration 188/1000 | Loss: 0.00001169
Iteration 189/1000 | Loss: 0.00001169
Iteration 190/1000 | Loss: 0.00001169
Iteration 191/1000 | Loss: 0.00001169
Iteration 192/1000 | Loss: 0.00001169
Iteration 193/1000 | Loss: 0.00001169
Iteration 194/1000 | Loss: 0.00001169
Iteration 195/1000 | Loss: 0.00001169
Iteration 196/1000 | Loss: 0.00001169
Iteration 197/1000 | Loss: 0.00001169
Iteration 198/1000 | Loss: 0.00001169
Iteration 199/1000 | Loss: 0.00001169
Iteration 200/1000 | Loss: 0.00001169
Iteration 201/1000 | Loss: 0.00001169
Iteration 202/1000 | Loss: 0.00001169
Iteration 203/1000 | Loss: 0.00001169
Iteration 204/1000 | Loss: 0.00001169
Iteration 205/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.1685651770676486e-05, 1.1685651770676486e-05, 1.1685651770676486e-05, 1.1685651770676486e-05, 1.1685651770676486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1685651770676486e-05

Optimization complete. Final v2v error: 2.890589475631714 mm

Highest mean error: 3.04860520362854 mm for frame 167

Lowest mean error: 2.7335078716278076 mm for frame 49

Saving results

Total time: 41.67651152610779
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065638
Iteration 2/25 | Loss: 0.00168499
Iteration 3/25 | Loss: 0.00138608
Iteration 4/25 | Loss: 0.00136453
Iteration 5/25 | Loss: 0.00135874
Iteration 6/25 | Loss: 0.00135740
Iteration 7/25 | Loss: 0.00135740
Iteration 8/25 | Loss: 0.00135740
Iteration 9/25 | Loss: 0.00135740
Iteration 10/25 | Loss: 0.00135740
Iteration 11/25 | Loss: 0.00135740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013573960168287158, 0.0013573960168287158, 0.0013573960168287158, 0.0013573960168287158, 0.0013573960168287158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013573960168287158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92006338
Iteration 2/25 | Loss: 0.00154179
Iteration 3/25 | Loss: 0.00154179
Iteration 4/25 | Loss: 0.00154179
Iteration 5/25 | Loss: 0.00154179
Iteration 6/25 | Loss: 0.00154179
Iteration 7/25 | Loss: 0.00154179
Iteration 8/25 | Loss: 0.00154179
Iteration 9/25 | Loss: 0.00154179
Iteration 10/25 | Loss: 0.00154179
Iteration 11/25 | Loss: 0.00154179
Iteration 12/25 | Loss: 0.00154179
Iteration 13/25 | Loss: 0.00154179
Iteration 14/25 | Loss: 0.00154179
Iteration 15/25 | Loss: 0.00154179
Iteration 16/25 | Loss: 0.00154179
Iteration 17/25 | Loss: 0.00154179
Iteration 18/25 | Loss: 0.00154179
Iteration 19/25 | Loss: 0.00154179
Iteration 20/25 | Loss: 0.00154179
Iteration 21/25 | Loss: 0.00154179
Iteration 22/25 | Loss: 0.00154179
Iteration 23/25 | Loss: 0.00154179
Iteration 24/25 | Loss: 0.00154179
Iteration 25/25 | Loss: 0.00154179

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154179
Iteration 2/1000 | Loss: 0.00005553
Iteration 3/1000 | Loss: 0.00003724
Iteration 4/1000 | Loss: 0.00003066
Iteration 5/1000 | Loss: 0.00002837
Iteration 6/1000 | Loss: 0.00002725
Iteration 7/1000 | Loss: 0.00002660
Iteration 8/1000 | Loss: 0.00002606
Iteration 9/1000 | Loss: 0.00002560
Iteration 10/1000 | Loss: 0.00002534
Iteration 11/1000 | Loss: 0.00002509
Iteration 12/1000 | Loss: 0.00002483
Iteration 13/1000 | Loss: 0.00002467
Iteration 14/1000 | Loss: 0.00002447
Iteration 15/1000 | Loss: 0.00002441
Iteration 16/1000 | Loss: 0.00002430
Iteration 17/1000 | Loss: 0.00002425
Iteration 18/1000 | Loss: 0.00002424
Iteration 19/1000 | Loss: 0.00002419
Iteration 20/1000 | Loss: 0.00002412
Iteration 21/1000 | Loss: 0.00002406
Iteration 22/1000 | Loss: 0.00002400
Iteration 23/1000 | Loss: 0.00002397
Iteration 24/1000 | Loss: 0.00002396
Iteration 25/1000 | Loss: 0.00002395
Iteration 26/1000 | Loss: 0.00002395
Iteration 27/1000 | Loss: 0.00002393
Iteration 28/1000 | Loss: 0.00002392
Iteration 29/1000 | Loss: 0.00002392
Iteration 30/1000 | Loss: 0.00002392
Iteration 31/1000 | Loss: 0.00002391
Iteration 32/1000 | Loss: 0.00002390
Iteration 33/1000 | Loss: 0.00002390
Iteration 34/1000 | Loss: 0.00002390
Iteration 35/1000 | Loss: 0.00002389
Iteration 36/1000 | Loss: 0.00002389
Iteration 37/1000 | Loss: 0.00002388
Iteration 38/1000 | Loss: 0.00002386
Iteration 39/1000 | Loss: 0.00002386
Iteration 40/1000 | Loss: 0.00002386
Iteration 41/1000 | Loss: 0.00002386
Iteration 42/1000 | Loss: 0.00002386
Iteration 43/1000 | Loss: 0.00002385
Iteration 44/1000 | Loss: 0.00002385
Iteration 45/1000 | Loss: 0.00002385
Iteration 46/1000 | Loss: 0.00002384
Iteration 47/1000 | Loss: 0.00002382
Iteration 48/1000 | Loss: 0.00002382
Iteration 49/1000 | Loss: 0.00002379
Iteration 50/1000 | Loss: 0.00002379
Iteration 51/1000 | Loss: 0.00002379
Iteration 52/1000 | Loss: 0.00002376
Iteration 53/1000 | Loss: 0.00002376
Iteration 54/1000 | Loss: 0.00002375
Iteration 55/1000 | Loss: 0.00002375
Iteration 56/1000 | Loss: 0.00002374
Iteration 57/1000 | Loss: 0.00002373
Iteration 58/1000 | Loss: 0.00002373
Iteration 59/1000 | Loss: 0.00002373
Iteration 60/1000 | Loss: 0.00002372
Iteration 61/1000 | Loss: 0.00002372
Iteration 62/1000 | Loss: 0.00002371
Iteration 63/1000 | Loss: 0.00002371
Iteration 64/1000 | Loss: 0.00002371
Iteration 65/1000 | Loss: 0.00002370
Iteration 66/1000 | Loss: 0.00002370
Iteration 67/1000 | Loss: 0.00002370
Iteration 68/1000 | Loss: 0.00002370
Iteration 69/1000 | Loss: 0.00002370
Iteration 70/1000 | Loss: 0.00002370
Iteration 71/1000 | Loss: 0.00002370
Iteration 72/1000 | Loss: 0.00002370
Iteration 73/1000 | Loss: 0.00002370
Iteration 74/1000 | Loss: 0.00002370
Iteration 75/1000 | Loss: 0.00002370
Iteration 76/1000 | Loss: 0.00002369
Iteration 77/1000 | Loss: 0.00002369
Iteration 78/1000 | Loss: 0.00002368
Iteration 79/1000 | Loss: 0.00002368
Iteration 80/1000 | Loss: 0.00002368
Iteration 81/1000 | Loss: 0.00002368
Iteration 82/1000 | Loss: 0.00002368
Iteration 83/1000 | Loss: 0.00002367
Iteration 84/1000 | Loss: 0.00002367
Iteration 85/1000 | Loss: 0.00002367
Iteration 86/1000 | Loss: 0.00002367
Iteration 87/1000 | Loss: 0.00002366
Iteration 88/1000 | Loss: 0.00002366
Iteration 89/1000 | Loss: 0.00002366
Iteration 90/1000 | Loss: 0.00002366
Iteration 91/1000 | Loss: 0.00002365
Iteration 92/1000 | Loss: 0.00002365
Iteration 93/1000 | Loss: 0.00002365
Iteration 94/1000 | Loss: 0.00002365
Iteration 95/1000 | Loss: 0.00002365
Iteration 96/1000 | Loss: 0.00002365
Iteration 97/1000 | Loss: 0.00002365
Iteration 98/1000 | Loss: 0.00002365
Iteration 99/1000 | Loss: 0.00002364
Iteration 100/1000 | Loss: 0.00002364
Iteration 101/1000 | Loss: 0.00002364
Iteration 102/1000 | Loss: 0.00002364
Iteration 103/1000 | Loss: 0.00002364
Iteration 104/1000 | Loss: 0.00002364
Iteration 105/1000 | Loss: 0.00002364
Iteration 106/1000 | Loss: 0.00002364
Iteration 107/1000 | Loss: 0.00002364
Iteration 108/1000 | Loss: 0.00002364
Iteration 109/1000 | Loss: 0.00002364
Iteration 110/1000 | Loss: 0.00002364
Iteration 111/1000 | Loss: 0.00002364
Iteration 112/1000 | Loss: 0.00002364
Iteration 113/1000 | Loss: 0.00002364
Iteration 114/1000 | Loss: 0.00002364
Iteration 115/1000 | Loss: 0.00002364
Iteration 116/1000 | Loss: 0.00002364
Iteration 117/1000 | Loss: 0.00002364
Iteration 118/1000 | Loss: 0.00002364
Iteration 119/1000 | Loss: 0.00002364
Iteration 120/1000 | Loss: 0.00002364
Iteration 121/1000 | Loss: 0.00002364
Iteration 122/1000 | Loss: 0.00002364
Iteration 123/1000 | Loss: 0.00002364
Iteration 124/1000 | Loss: 0.00002364
Iteration 125/1000 | Loss: 0.00002364
Iteration 126/1000 | Loss: 0.00002364
Iteration 127/1000 | Loss: 0.00002364
Iteration 128/1000 | Loss: 0.00002364
Iteration 129/1000 | Loss: 0.00002364
Iteration 130/1000 | Loss: 0.00002364
Iteration 131/1000 | Loss: 0.00002364
Iteration 132/1000 | Loss: 0.00002364
Iteration 133/1000 | Loss: 0.00002364
Iteration 134/1000 | Loss: 0.00002364
Iteration 135/1000 | Loss: 0.00002364
Iteration 136/1000 | Loss: 0.00002364
Iteration 137/1000 | Loss: 0.00002364
Iteration 138/1000 | Loss: 0.00002364
Iteration 139/1000 | Loss: 0.00002364
Iteration 140/1000 | Loss: 0.00002364
Iteration 141/1000 | Loss: 0.00002364
Iteration 142/1000 | Loss: 0.00002364
Iteration 143/1000 | Loss: 0.00002364
Iteration 144/1000 | Loss: 0.00002364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.3641727239009924e-05, 2.3641727239009924e-05, 2.3641727239009924e-05, 2.3641727239009924e-05, 2.3641727239009924e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3641727239009924e-05

Optimization complete. Final v2v error: 3.998063087463379 mm

Highest mean error: 4.776765823364258 mm for frame 120

Lowest mean error: 3.245896339416504 mm for frame 27

Saving results

Total time: 44.11664080619812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00550179
Iteration 2/25 | Loss: 0.00165908
Iteration 3/25 | Loss: 0.00137372
Iteration 4/25 | Loss: 0.00135516
Iteration 5/25 | Loss: 0.00135252
Iteration 6/25 | Loss: 0.00135195
Iteration 7/25 | Loss: 0.00135195
Iteration 8/25 | Loss: 0.00135195
Iteration 9/25 | Loss: 0.00135195
Iteration 10/25 | Loss: 0.00135195
Iteration 11/25 | Loss: 0.00135195
Iteration 12/25 | Loss: 0.00135195
Iteration 13/25 | Loss: 0.00135195
Iteration 14/25 | Loss: 0.00135195
Iteration 15/25 | Loss: 0.00135195
Iteration 16/25 | Loss: 0.00135195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013519470812752843, 0.0013519470812752843, 0.0013519470812752843, 0.0013519470812752843, 0.0013519470812752843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013519470812752843

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97727013
Iteration 2/25 | Loss: 0.00129448
Iteration 3/25 | Loss: 0.00129447
Iteration 4/25 | Loss: 0.00129447
Iteration 5/25 | Loss: 0.00129447
Iteration 6/25 | Loss: 0.00129447
Iteration 7/25 | Loss: 0.00129447
Iteration 8/25 | Loss: 0.00129447
Iteration 9/25 | Loss: 0.00129447
Iteration 10/25 | Loss: 0.00129447
Iteration 11/25 | Loss: 0.00129447
Iteration 12/25 | Loss: 0.00129447
Iteration 13/25 | Loss: 0.00129447
Iteration 14/25 | Loss: 0.00129447
Iteration 15/25 | Loss: 0.00129447
Iteration 16/25 | Loss: 0.00129447
Iteration 17/25 | Loss: 0.00129447
Iteration 18/25 | Loss: 0.00129447
Iteration 19/25 | Loss: 0.00129447
Iteration 20/25 | Loss: 0.00129447
Iteration 21/25 | Loss: 0.00129447
Iteration 22/25 | Loss: 0.00129447
Iteration 23/25 | Loss: 0.00129447
Iteration 24/25 | Loss: 0.00129447
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001294465851970017, 0.001294465851970017, 0.001294465851970017, 0.001294465851970017, 0.001294465851970017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001294465851970017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129447
Iteration 2/1000 | Loss: 0.00004751
Iteration 3/1000 | Loss: 0.00003359
Iteration 4/1000 | Loss: 0.00002647
Iteration 5/1000 | Loss: 0.00002491
Iteration 6/1000 | Loss: 0.00002410
Iteration 7/1000 | Loss: 0.00002367
Iteration 8/1000 | Loss: 0.00002312
Iteration 9/1000 | Loss: 0.00002270
Iteration 10/1000 | Loss: 0.00002230
Iteration 11/1000 | Loss: 0.00002200
Iteration 12/1000 | Loss: 0.00002170
Iteration 13/1000 | Loss: 0.00002152
Iteration 14/1000 | Loss: 0.00002131
Iteration 15/1000 | Loss: 0.00002111
Iteration 16/1000 | Loss: 0.00002100
Iteration 17/1000 | Loss: 0.00002091
Iteration 18/1000 | Loss: 0.00002085
Iteration 19/1000 | Loss: 0.00002076
Iteration 20/1000 | Loss: 0.00002073
Iteration 21/1000 | Loss: 0.00002072
Iteration 22/1000 | Loss: 0.00002071
Iteration 23/1000 | Loss: 0.00002067
Iteration 24/1000 | Loss: 0.00002067
Iteration 25/1000 | Loss: 0.00002065
Iteration 26/1000 | Loss: 0.00002065
Iteration 27/1000 | Loss: 0.00002065
Iteration 28/1000 | Loss: 0.00002064
Iteration 29/1000 | Loss: 0.00002061
Iteration 30/1000 | Loss: 0.00002059
Iteration 31/1000 | Loss: 0.00002057
Iteration 32/1000 | Loss: 0.00002057
Iteration 33/1000 | Loss: 0.00002055
Iteration 34/1000 | Loss: 0.00002055
Iteration 35/1000 | Loss: 0.00002051
Iteration 36/1000 | Loss: 0.00002051
Iteration 37/1000 | Loss: 0.00002051
Iteration 38/1000 | Loss: 0.00002051
Iteration 39/1000 | Loss: 0.00002051
Iteration 40/1000 | Loss: 0.00002051
Iteration 41/1000 | Loss: 0.00002051
Iteration 42/1000 | Loss: 0.00002050
Iteration 43/1000 | Loss: 0.00002050
Iteration 44/1000 | Loss: 0.00002050
Iteration 45/1000 | Loss: 0.00002049
Iteration 46/1000 | Loss: 0.00002049
Iteration 47/1000 | Loss: 0.00002048
Iteration 48/1000 | Loss: 0.00002048
Iteration 49/1000 | Loss: 0.00002047
Iteration 50/1000 | Loss: 0.00002047
Iteration 51/1000 | Loss: 0.00002047
Iteration 52/1000 | Loss: 0.00002047
Iteration 53/1000 | Loss: 0.00002047
Iteration 54/1000 | Loss: 0.00002047
Iteration 55/1000 | Loss: 0.00002047
Iteration 56/1000 | Loss: 0.00002047
Iteration 57/1000 | Loss: 0.00002047
Iteration 58/1000 | Loss: 0.00002047
Iteration 59/1000 | Loss: 0.00002047
Iteration 60/1000 | Loss: 0.00002047
Iteration 61/1000 | Loss: 0.00002047
Iteration 62/1000 | Loss: 0.00002046
Iteration 63/1000 | Loss: 0.00002046
Iteration 64/1000 | Loss: 0.00002046
Iteration 65/1000 | Loss: 0.00002046
Iteration 66/1000 | Loss: 0.00002045
Iteration 67/1000 | Loss: 0.00002045
Iteration 68/1000 | Loss: 0.00002045
Iteration 69/1000 | Loss: 0.00002045
Iteration 70/1000 | Loss: 0.00002045
Iteration 71/1000 | Loss: 0.00002045
Iteration 72/1000 | Loss: 0.00002044
Iteration 73/1000 | Loss: 0.00002044
Iteration 74/1000 | Loss: 0.00002044
Iteration 75/1000 | Loss: 0.00002044
Iteration 76/1000 | Loss: 0.00002043
Iteration 77/1000 | Loss: 0.00002043
Iteration 78/1000 | Loss: 0.00002043
Iteration 79/1000 | Loss: 0.00002042
Iteration 80/1000 | Loss: 0.00002042
Iteration 81/1000 | Loss: 0.00002042
Iteration 82/1000 | Loss: 0.00002041
Iteration 83/1000 | Loss: 0.00002041
Iteration 84/1000 | Loss: 0.00002041
Iteration 85/1000 | Loss: 0.00002040
Iteration 86/1000 | Loss: 0.00002040
Iteration 87/1000 | Loss: 0.00002040
Iteration 88/1000 | Loss: 0.00002040
Iteration 89/1000 | Loss: 0.00002040
Iteration 90/1000 | Loss: 0.00002040
Iteration 91/1000 | Loss: 0.00002040
Iteration 92/1000 | Loss: 0.00002040
Iteration 93/1000 | Loss: 0.00002039
Iteration 94/1000 | Loss: 0.00002039
Iteration 95/1000 | Loss: 0.00002039
Iteration 96/1000 | Loss: 0.00002039
Iteration 97/1000 | Loss: 0.00002039
Iteration 98/1000 | Loss: 0.00002039
Iteration 99/1000 | Loss: 0.00002038
Iteration 100/1000 | Loss: 0.00002038
Iteration 101/1000 | Loss: 0.00002038
Iteration 102/1000 | Loss: 0.00002038
Iteration 103/1000 | Loss: 0.00002038
Iteration 104/1000 | Loss: 0.00002038
Iteration 105/1000 | Loss: 0.00002038
Iteration 106/1000 | Loss: 0.00002038
Iteration 107/1000 | Loss: 0.00002038
Iteration 108/1000 | Loss: 0.00002038
Iteration 109/1000 | Loss: 0.00002038
Iteration 110/1000 | Loss: 0.00002038
Iteration 111/1000 | Loss: 0.00002038
Iteration 112/1000 | Loss: 0.00002037
Iteration 113/1000 | Loss: 0.00002037
Iteration 114/1000 | Loss: 0.00002037
Iteration 115/1000 | Loss: 0.00002037
Iteration 116/1000 | Loss: 0.00002037
Iteration 117/1000 | Loss: 0.00002037
Iteration 118/1000 | Loss: 0.00002037
Iteration 119/1000 | Loss: 0.00002037
Iteration 120/1000 | Loss: 0.00002037
Iteration 121/1000 | Loss: 0.00002036
Iteration 122/1000 | Loss: 0.00002036
Iteration 123/1000 | Loss: 0.00002036
Iteration 124/1000 | Loss: 0.00002036
Iteration 125/1000 | Loss: 0.00002035
Iteration 126/1000 | Loss: 0.00002035
Iteration 127/1000 | Loss: 0.00002035
Iteration 128/1000 | Loss: 0.00002035
Iteration 129/1000 | Loss: 0.00002035
Iteration 130/1000 | Loss: 0.00002035
Iteration 131/1000 | Loss: 0.00002035
Iteration 132/1000 | Loss: 0.00002034
Iteration 133/1000 | Loss: 0.00002034
Iteration 134/1000 | Loss: 0.00002034
Iteration 135/1000 | Loss: 0.00002034
Iteration 136/1000 | Loss: 0.00002033
Iteration 137/1000 | Loss: 0.00002033
Iteration 138/1000 | Loss: 0.00002033
Iteration 139/1000 | Loss: 0.00002033
Iteration 140/1000 | Loss: 0.00002033
Iteration 141/1000 | Loss: 0.00002033
Iteration 142/1000 | Loss: 0.00002032
Iteration 143/1000 | Loss: 0.00002032
Iteration 144/1000 | Loss: 0.00002032
Iteration 145/1000 | Loss: 0.00002032
Iteration 146/1000 | Loss: 0.00002032
Iteration 147/1000 | Loss: 0.00002032
Iteration 148/1000 | Loss: 0.00002032
Iteration 149/1000 | Loss: 0.00002031
Iteration 150/1000 | Loss: 0.00002031
Iteration 151/1000 | Loss: 0.00002031
Iteration 152/1000 | Loss: 0.00002031
Iteration 153/1000 | Loss: 0.00002031
Iteration 154/1000 | Loss: 0.00002031
Iteration 155/1000 | Loss: 0.00002031
Iteration 156/1000 | Loss: 0.00002031
Iteration 157/1000 | Loss: 0.00002031
Iteration 158/1000 | Loss: 0.00002031
Iteration 159/1000 | Loss: 0.00002031
Iteration 160/1000 | Loss: 0.00002031
Iteration 161/1000 | Loss: 0.00002031
Iteration 162/1000 | Loss: 0.00002031
Iteration 163/1000 | Loss: 0.00002031
Iteration 164/1000 | Loss: 0.00002031
Iteration 165/1000 | Loss: 0.00002031
Iteration 166/1000 | Loss: 0.00002031
Iteration 167/1000 | Loss: 0.00002031
Iteration 168/1000 | Loss: 0.00002031
Iteration 169/1000 | Loss: 0.00002031
Iteration 170/1000 | Loss: 0.00002031
Iteration 171/1000 | Loss: 0.00002031
Iteration 172/1000 | Loss: 0.00002031
Iteration 173/1000 | Loss: 0.00002031
Iteration 174/1000 | Loss: 0.00002031
Iteration 175/1000 | Loss: 0.00002031
Iteration 176/1000 | Loss: 0.00002031
Iteration 177/1000 | Loss: 0.00002031
Iteration 178/1000 | Loss: 0.00002031
Iteration 179/1000 | Loss: 0.00002031
Iteration 180/1000 | Loss: 0.00002031
Iteration 181/1000 | Loss: 0.00002031
Iteration 182/1000 | Loss: 0.00002031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.0305547877796926e-05, 2.0305547877796926e-05, 2.0305547877796926e-05, 2.0305547877796926e-05, 2.0305547877796926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0305547877796926e-05

Optimization complete. Final v2v error: 3.602867603302002 mm

Highest mean error: 4.651363849639893 mm for frame 59

Lowest mean error: 2.7563276290893555 mm for frame 136

Saving results

Total time: 49.90092158317566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894589
Iteration 2/25 | Loss: 0.00164797
Iteration 3/25 | Loss: 0.00134680
Iteration 4/25 | Loss: 0.00132132
Iteration 5/25 | Loss: 0.00131374
Iteration 6/25 | Loss: 0.00131142
Iteration 7/25 | Loss: 0.00131125
Iteration 8/25 | Loss: 0.00131125
Iteration 9/25 | Loss: 0.00131125
Iteration 10/25 | Loss: 0.00131125
Iteration 11/25 | Loss: 0.00131125
Iteration 12/25 | Loss: 0.00131125
Iteration 13/25 | Loss: 0.00131125
Iteration 14/25 | Loss: 0.00131125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013112472370266914, 0.0013112472370266914, 0.0013112472370266914, 0.0013112472370266914, 0.0013112472370266914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013112472370266914

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04637635
Iteration 2/25 | Loss: 0.00122026
Iteration 3/25 | Loss: 0.00122026
Iteration 4/25 | Loss: 0.00122026
Iteration 5/25 | Loss: 0.00122026
Iteration 6/25 | Loss: 0.00122026
Iteration 7/25 | Loss: 0.00122026
Iteration 8/25 | Loss: 0.00122026
Iteration 9/25 | Loss: 0.00122026
Iteration 10/25 | Loss: 0.00122026
Iteration 11/25 | Loss: 0.00122026
Iteration 12/25 | Loss: 0.00122026
Iteration 13/25 | Loss: 0.00122026
Iteration 14/25 | Loss: 0.00122026
Iteration 15/25 | Loss: 0.00122026
Iteration 16/25 | Loss: 0.00122026
Iteration 17/25 | Loss: 0.00122026
Iteration 18/25 | Loss: 0.00122026
Iteration 19/25 | Loss: 0.00122026
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012202567886561155, 0.0012202567886561155, 0.0012202567886561155, 0.0012202567886561155, 0.0012202567886561155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012202567886561155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122026
Iteration 2/1000 | Loss: 0.00005634
Iteration 3/1000 | Loss: 0.00003836
Iteration 4/1000 | Loss: 0.00003069
Iteration 5/1000 | Loss: 0.00002794
Iteration 6/1000 | Loss: 0.00002670
Iteration 7/1000 | Loss: 0.00002606
Iteration 8/1000 | Loss: 0.00002532
Iteration 9/1000 | Loss: 0.00002473
Iteration 10/1000 | Loss: 0.00002438
Iteration 11/1000 | Loss: 0.00002412
Iteration 12/1000 | Loss: 0.00002390
Iteration 13/1000 | Loss: 0.00002370
Iteration 14/1000 | Loss: 0.00002345
Iteration 15/1000 | Loss: 0.00002324
Iteration 16/1000 | Loss: 0.00002305
Iteration 17/1000 | Loss: 0.00002288
Iteration 18/1000 | Loss: 0.00002274
Iteration 19/1000 | Loss: 0.00002273
Iteration 20/1000 | Loss: 0.00002263
Iteration 21/1000 | Loss: 0.00002260
Iteration 22/1000 | Loss: 0.00002260
Iteration 23/1000 | Loss: 0.00002259
Iteration 24/1000 | Loss: 0.00002257
Iteration 25/1000 | Loss: 0.00002255
Iteration 26/1000 | Loss: 0.00002254
Iteration 27/1000 | Loss: 0.00002253
Iteration 28/1000 | Loss: 0.00002251
Iteration 29/1000 | Loss: 0.00002250
Iteration 30/1000 | Loss: 0.00002241
Iteration 31/1000 | Loss: 0.00002240
Iteration 32/1000 | Loss: 0.00002240
Iteration 33/1000 | Loss: 0.00002237
Iteration 34/1000 | Loss: 0.00002237
Iteration 35/1000 | Loss: 0.00002236
Iteration 36/1000 | Loss: 0.00002235
Iteration 37/1000 | Loss: 0.00002235
Iteration 38/1000 | Loss: 0.00002234
Iteration 39/1000 | Loss: 0.00002233
Iteration 40/1000 | Loss: 0.00002233
Iteration 41/1000 | Loss: 0.00002233
Iteration 42/1000 | Loss: 0.00002233
Iteration 43/1000 | Loss: 0.00002233
Iteration 44/1000 | Loss: 0.00002233
Iteration 45/1000 | Loss: 0.00002233
Iteration 46/1000 | Loss: 0.00002233
Iteration 47/1000 | Loss: 0.00002232
Iteration 48/1000 | Loss: 0.00002231
Iteration 49/1000 | Loss: 0.00002231
Iteration 50/1000 | Loss: 0.00002231
Iteration 51/1000 | Loss: 0.00002230
Iteration 52/1000 | Loss: 0.00002230
Iteration 53/1000 | Loss: 0.00002230
Iteration 54/1000 | Loss: 0.00002230
Iteration 55/1000 | Loss: 0.00002229
Iteration 56/1000 | Loss: 0.00002229
Iteration 57/1000 | Loss: 0.00002229
Iteration 58/1000 | Loss: 0.00002229
Iteration 59/1000 | Loss: 0.00002229
Iteration 60/1000 | Loss: 0.00002228
Iteration 61/1000 | Loss: 0.00002228
Iteration 62/1000 | Loss: 0.00002228
Iteration 63/1000 | Loss: 0.00002227
Iteration 64/1000 | Loss: 0.00002227
Iteration 65/1000 | Loss: 0.00002227
Iteration 66/1000 | Loss: 0.00002227
Iteration 67/1000 | Loss: 0.00002227
Iteration 68/1000 | Loss: 0.00002227
Iteration 69/1000 | Loss: 0.00002226
Iteration 70/1000 | Loss: 0.00002226
Iteration 71/1000 | Loss: 0.00002226
Iteration 72/1000 | Loss: 0.00002226
Iteration 73/1000 | Loss: 0.00002226
Iteration 74/1000 | Loss: 0.00002226
Iteration 75/1000 | Loss: 0.00002226
Iteration 76/1000 | Loss: 0.00002225
Iteration 77/1000 | Loss: 0.00002225
Iteration 78/1000 | Loss: 0.00002224
Iteration 79/1000 | Loss: 0.00002224
Iteration 80/1000 | Loss: 0.00002224
Iteration 81/1000 | Loss: 0.00002224
Iteration 82/1000 | Loss: 0.00002224
Iteration 83/1000 | Loss: 0.00002224
Iteration 84/1000 | Loss: 0.00002224
Iteration 85/1000 | Loss: 0.00002224
Iteration 86/1000 | Loss: 0.00002224
Iteration 87/1000 | Loss: 0.00002224
Iteration 88/1000 | Loss: 0.00002224
Iteration 89/1000 | Loss: 0.00002224
Iteration 90/1000 | Loss: 0.00002224
Iteration 91/1000 | Loss: 0.00002224
Iteration 92/1000 | Loss: 0.00002223
Iteration 93/1000 | Loss: 0.00002223
Iteration 94/1000 | Loss: 0.00002223
Iteration 95/1000 | Loss: 0.00002223
Iteration 96/1000 | Loss: 0.00002223
Iteration 97/1000 | Loss: 0.00002223
Iteration 98/1000 | Loss: 0.00002223
Iteration 99/1000 | Loss: 0.00002223
Iteration 100/1000 | Loss: 0.00002223
Iteration 101/1000 | Loss: 0.00002223
Iteration 102/1000 | Loss: 0.00002223
Iteration 103/1000 | Loss: 0.00002223
Iteration 104/1000 | Loss: 0.00002222
Iteration 105/1000 | Loss: 0.00002222
Iteration 106/1000 | Loss: 0.00002222
Iteration 107/1000 | Loss: 0.00002222
Iteration 108/1000 | Loss: 0.00002222
Iteration 109/1000 | Loss: 0.00002222
Iteration 110/1000 | Loss: 0.00002222
Iteration 111/1000 | Loss: 0.00002222
Iteration 112/1000 | Loss: 0.00002222
Iteration 113/1000 | Loss: 0.00002222
Iteration 114/1000 | Loss: 0.00002222
Iteration 115/1000 | Loss: 0.00002222
Iteration 116/1000 | Loss: 0.00002222
Iteration 117/1000 | Loss: 0.00002222
Iteration 118/1000 | Loss: 0.00002222
Iteration 119/1000 | Loss: 0.00002222
Iteration 120/1000 | Loss: 0.00002222
Iteration 121/1000 | Loss: 0.00002221
Iteration 122/1000 | Loss: 0.00002221
Iteration 123/1000 | Loss: 0.00002221
Iteration 124/1000 | Loss: 0.00002221
Iteration 125/1000 | Loss: 0.00002221
Iteration 126/1000 | Loss: 0.00002221
Iteration 127/1000 | Loss: 0.00002221
Iteration 128/1000 | Loss: 0.00002221
Iteration 129/1000 | Loss: 0.00002221
Iteration 130/1000 | Loss: 0.00002221
Iteration 131/1000 | Loss: 0.00002220
Iteration 132/1000 | Loss: 0.00002220
Iteration 133/1000 | Loss: 0.00002220
Iteration 134/1000 | Loss: 0.00002220
Iteration 135/1000 | Loss: 0.00002220
Iteration 136/1000 | Loss: 0.00002220
Iteration 137/1000 | Loss: 0.00002220
Iteration 138/1000 | Loss: 0.00002220
Iteration 139/1000 | Loss: 0.00002220
Iteration 140/1000 | Loss: 0.00002220
Iteration 141/1000 | Loss: 0.00002220
Iteration 142/1000 | Loss: 0.00002220
Iteration 143/1000 | Loss: 0.00002220
Iteration 144/1000 | Loss: 0.00002220
Iteration 145/1000 | Loss: 0.00002220
Iteration 146/1000 | Loss: 0.00002220
Iteration 147/1000 | Loss: 0.00002220
Iteration 148/1000 | Loss: 0.00002220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.219911402789876e-05, 2.219911402789876e-05, 2.219911402789876e-05, 2.219911402789876e-05, 2.219911402789876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.219911402789876e-05

Optimization complete. Final v2v error: 3.932854652404785 mm

Highest mean error: 5.264250755310059 mm for frame 103

Lowest mean error: 3.1578807830810547 mm for frame 120

Saving results

Total time: 48.347689628601074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893688
Iteration 2/25 | Loss: 0.00183352
Iteration 3/25 | Loss: 0.00147980
Iteration 4/25 | Loss: 0.00143159
Iteration 5/25 | Loss: 0.00141176
Iteration 6/25 | Loss: 0.00140083
Iteration 7/25 | Loss: 0.00140445
Iteration 8/25 | Loss: 0.00140577
Iteration 9/25 | Loss: 0.00139011
Iteration 10/25 | Loss: 0.00139563
Iteration 11/25 | Loss: 0.00138839
Iteration 12/25 | Loss: 0.00137910
Iteration 13/25 | Loss: 0.00137424
Iteration 14/25 | Loss: 0.00137129
Iteration 15/25 | Loss: 0.00136939
Iteration 16/25 | Loss: 0.00136897
Iteration 17/25 | Loss: 0.00136328
Iteration 18/25 | Loss: 0.00136622
Iteration 19/25 | Loss: 0.00135847
Iteration 20/25 | Loss: 0.00135382
Iteration 21/25 | Loss: 0.00136428
Iteration 22/25 | Loss: 0.00136769
Iteration 23/25 | Loss: 0.00136289
Iteration 24/25 | Loss: 0.00136085
Iteration 25/25 | Loss: 0.00136861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27150011
Iteration 2/25 | Loss: 0.00184805
Iteration 3/25 | Loss: 0.00184796
Iteration 4/25 | Loss: 0.00184796
Iteration 5/25 | Loss: 0.00184796
Iteration 6/25 | Loss: 0.00184796
Iteration 7/25 | Loss: 0.00184796
Iteration 8/25 | Loss: 0.00184796
Iteration 9/25 | Loss: 0.00184796
Iteration 10/25 | Loss: 0.00184796
Iteration 11/25 | Loss: 0.00184796
Iteration 12/25 | Loss: 0.00184796
Iteration 13/25 | Loss: 0.00184795
Iteration 14/25 | Loss: 0.00184796
Iteration 15/25 | Loss: 0.00184796
Iteration 16/25 | Loss: 0.00184796
Iteration 17/25 | Loss: 0.00184796
Iteration 18/25 | Loss: 0.00184796
Iteration 19/25 | Loss: 0.00184796
Iteration 20/25 | Loss: 0.00184795
Iteration 21/25 | Loss: 0.00184795
Iteration 22/25 | Loss: 0.00184795
Iteration 23/25 | Loss: 0.00184795
Iteration 24/25 | Loss: 0.00184795
Iteration 25/25 | Loss: 0.00184795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184795
Iteration 2/1000 | Loss: 0.00168861
Iteration 3/1000 | Loss: 0.00665117
Iteration 4/1000 | Loss: 0.00398504
Iteration 5/1000 | Loss: 0.00337621
Iteration 6/1000 | Loss: 0.00248357
Iteration 7/1000 | Loss: 0.00237334
Iteration 8/1000 | Loss: 0.00161753
Iteration 9/1000 | Loss: 0.00099601
Iteration 10/1000 | Loss: 0.00388612
Iteration 11/1000 | Loss: 0.00076629
Iteration 12/1000 | Loss: 0.00166340
Iteration 13/1000 | Loss: 0.00118227
Iteration 14/1000 | Loss: 0.00203837
Iteration 15/1000 | Loss: 0.00106593
Iteration 16/1000 | Loss: 0.00152764
Iteration 17/1000 | Loss: 0.00361897
Iteration 18/1000 | Loss: 0.00039619
Iteration 19/1000 | Loss: 0.00356350
Iteration 20/1000 | Loss: 0.00029574
Iteration 21/1000 | Loss: 0.00030587
Iteration 22/1000 | Loss: 0.00032177
Iteration 23/1000 | Loss: 0.00029690
Iteration 24/1000 | Loss: 0.00025113
Iteration 25/1000 | Loss: 0.00012857
Iteration 26/1000 | Loss: 0.00015392
Iteration 27/1000 | Loss: 0.00013045
Iteration 28/1000 | Loss: 0.00042271
Iteration 29/1000 | Loss: 0.00026937
Iteration 30/1000 | Loss: 0.00019649
Iteration 31/1000 | Loss: 0.00034407
Iteration 32/1000 | Loss: 0.00032603
Iteration 33/1000 | Loss: 0.00033469
Iteration 34/1000 | Loss: 0.00024729
Iteration 35/1000 | Loss: 0.00041560
Iteration 36/1000 | Loss: 0.00029381
Iteration 37/1000 | Loss: 0.00025484
Iteration 38/1000 | Loss: 0.00027509
Iteration 39/1000 | Loss: 0.00033449
Iteration 40/1000 | Loss: 0.00041804
Iteration 41/1000 | Loss: 0.00046056
Iteration 42/1000 | Loss: 0.00017998
Iteration 43/1000 | Loss: 0.00020772
Iteration 44/1000 | Loss: 0.00027582
Iteration 45/1000 | Loss: 0.00022263
Iteration 46/1000 | Loss: 0.00021816
Iteration 47/1000 | Loss: 0.00020513
Iteration 48/1000 | Loss: 0.00016146
Iteration 49/1000 | Loss: 0.00021809
Iteration 50/1000 | Loss: 0.00021986
Iteration 51/1000 | Loss: 0.00016439
Iteration 52/1000 | Loss: 0.00023291
Iteration 53/1000 | Loss: 0.00016205
Iteration 54/1000 | Loss: 0.00017912
Iteration 55/1000 | Loss: 0.00010566
Iteration 56/1000 | Loss: 0.00009545
Iteration 57/1000 | Loss: 0.00023656
Iteration 58/1000 | Loss: 0.00015253
Iteration 59/1000 | Loss: 0.00012165
Iteration 60/1000 | Loss: 0.00004419
Iteration 61/1000 | Loss: 0.00007874
Iteration 62/1000 | Loss: 0.00008574
Iteration 63/1000 | Loss: 0.00024177
Iteration 64/1000 | Loss: 0.00051287
Iteration 65/1000 | Loss: 0.00030478
Iteration 66/1000 | Loss: 0.00004127
Iteration 67/1000 | Loss: 0.00003531
Iteration 68/1000 | Loss: 0.00003264
Iteration 69/1000 | Loss: 0.00028795
Iteration 70/1000 | Loss: 0.00018923
Iteration 71/1000 | Loss: 0.00009602
Iteration 72/1000 | Loss: 0.00007968
Iteration 73/1000 | Loss: 0.00006986
Iteration 74/1000 | Loss: 0.00006983
Iteration 75/1000 | Loss: 0.00007412
Iteration 76/1000 | Loss: 0.00003430
Iteration 77/1000 | Loss: 0.00003197
Iteration 78/1000 | Loss: 0.00028636
Iteration 79/1000 | Loss: 0.00007213
Iteration 80/1000 | Loss: 0.00011901
Iteration 81/1000 | Loss: 0.00008142
Iteration 82/1000 | Loss: 0.00030453
Iteration 83/1000 | Loss: 0.00005941
Iteration 84/1000 | Loss: 0.00029669
Iteration 85/1000 | Loss: 0.00021705
Iteration 86/1000 | Loss: 0.00004418
Iteration 87/1000 | Loss: 0.00018437
Iteration 88/1000 | Loss: 0.00023109
Iteration 89/1000 | Loss: 0.00017970
Iteration 90/1000 | Loss: 0.00023725
Iteration 91/1000 | Loss: 0.00016545
Iteration 92/1000 | Loss: 0.00015519
Iteration 93/1000 | Loss: 0.00032605
Iteration 94/1000 | Loss: 0.00010758
Iteration 95/1000 | Loss: 0.00003388
Iteration 96/1000 | Loss: 0.00003103
Iteration 97/1000 | Loss: 0.00004240
Iteration 98/1000 | Loss: 0.00008450
Iteration 99/1000 | Loss: 0.00006894
Iteration 100/1000 | Loss: 0.00002965
Iteration 101/1000 | Loss: 0.00002840
Iteration 102/1000 | Loss: 0.00009381
Iteration 103/1000 | Loss: 0.00007700
Iteration 104/1000 | Loss: 0.00007681
Iteration 105/1000 | Loss: 0.00008783
Iteration 106/1000 | Loss: 0.00006701
Iteration 107/1000 | Loss: 0.00005843
Iteration 108/1000 | Loss: 0.00004543
Iteration 109/1000 | Loss: 0.00009831
Iteration 110/1000 | Loss: 0.00018098
Iteration 111/1000 | Loss: 0.00017174
Iteration 112/1000 | Loss: 0.00013275
Iteration 113/1000 | Loss: 0.00017543
Iteration 114/1000 | Loss: 0.00010994
Iteration 115/1000 | Loss: 0.00023946
Iteration 116/1000 | Loss: 0.00008115
Iteration 117/1000 | Loss: 0.00011294
Iteration 118/1000 | Loss: 0.00002966
Iteration 119/1000 | Loss: 0.00002746
Iteration 120/1000 | Loss: 0.00002660
Iteration 121/1000 | Loss: 0.00025844
Iteration 122/1000 | Loss: 0.00006122
Iteration 123/1000 | Loss: 0.00022371
Iteration 124/1000 | Loss: 0.00031293
Iteration 125/1000 | Loss: 0.00021432
Iteration 126/1000 | Loss: 0.00028496
Iteration 127/1000 | Loss: 0.00004192
Iteration 128/1000 | Loss: 0.00003345
Iteration 129/1000 | Loss: 0.00002970
Iteration 130/1000 | Loss: 0.00002806
Iteration 131/1000 | Loss: 0.00002732
Iteration 132/1000 | Loss: 0.00002672
Iteration 133/1000 | Loss: 0.00002631
Iteration 134/1000 | Loss: 0.00002595
Iteration 135/1000 | Loss: 0.00002562
Iteration 136/1000 | Loss: 0.00002517
Iteration 137/1000 | Loss: 0.00002473
Iteration 138/1000 | Loss: 0.00002445
Iteration 139/1000 | Loss: 0.00002427
Iteration 140/1000 | Loss: 0.00002424
Iteration 141/1000 | Loss: 0.00002417
Iteration 142/1000 | Loss: 0.00002417
Iteration 143/1000 | Loss: 0.00002414
Iteration 144/1000 | Loss: 0.00002413
Iteration 145/1000 | Loss: 0.00002413
Iteration 146/1000 | Loss: 0.00002410
Iteration 147/1000 | Loss: 0.00002407
Iteration 148/1000 | Loss: 0.00002407
Iteration 149/1000 | Loss: 0.00002406
Iteration 150/1000 | Loss: 0.00002406
Iteration 151/1000 | Loss: 0.00002405
Iteration 152/1000 | Loss: 0.00002405
Iteration 153/1000 | Loss: 0.00002405
Iteration 154/1000 | Loss: 0.00002405
Iteration 155/1000 | Loss: 0.00002405
Iteration 156/1000 | Loss: 0.00002405
Iteration 157/1000 | Loss: 0.00002405
Iteration 158/1000 | Loss: 0.00002405
Iteration 159/1000 | Loss: 0.00002405
Iteration 160/1000 | Loss: 0.00002405
Iteration 161/1000 | Loss: 0.00002404
Iteration 162/1000 | Loss: 0.00002404
Iteration 163/1000 | Loss: 0.00002404
Iteration 164/1000 | Loss: 0.00002403
Iteration 165/1000 | Loss: 0.00002402
Iteration 166/1000 | Loss: 0.00002402
Iteration 167/1000 | Loss: 0.00002401
Iteration 168/1000 | Loss: 0.00002401
Iteration 169/1000 | Loss: 0.00002401
Iteration 170/1000 | Loss: 0.00002401
Iteration 171/1000 | Loss: 0.00002401
Iteration 172/1000 | Loss: 0.00002401
Iteration 173/1000 | Loss: 0.00002401
Iteration 174/1000 | Loss: 0.00002401
Iteration 175/1000 | Loss: 0.00002400
Iteration 176/1000 | Loss: 0.00002400
Iteration 177/1000 | Loss: 0.00002400
Iteration 178/1000 | Loss: 0.00002400
Iteration 179/1000 | Loss: 0.00002400
Iteration 180/1000 | Loss: 0.00002400
Iteration 181/1000 | Loss: 0.00002400
Iteration 182/1000 | Loss: 0.00002399
Iteration 183/1000 | Loss: 0.00002399
Iteration 184/1000 | Loss: 0.00002399
Iteration 185/1000 | Loss: 0.00002399
Iteration 186/1000 | Loss: 0.00002399
Iteration 187/1000 | Loss: 0.00002399
Iteration 188/1000 | Loss: 0.00002399
Iteration 189/1000 | Loss: 0.00002399
Iteration 190/1000 | Loss: 0.00002399
Iteration 191/1000 | Loss: 0.00002399
Iteration 192/1000 | Loss: 0.00002398
Iteration 193/1000 | Loss: 0.00002398
Iteration 194/1000 | Loss: 0.00002398
Iteration 195/1000 | Loss: 0.00002398
Iteration 196/1000 | Loss: 0.00002398
Iteration 197/1000 | Loss: 0.00002398
Iteration 198/1000 | Loss: 0.00002398
Iteration 199/1000 | Loss: 0.00002397
Iteration 200/1000 | Loss: 0.00002397
Iteration 201/1000 | Loss: 0.00002397
Iteration 202/1000 | Loss: 0.00002397
Iteration 203/1000 | Loss: 0.00002396
Iteration 204/1000 | Loss: 0.00002396
Iteration 205/1000 | Loss: 0.00002396
Iteration 206/1000 | Loss: 0.00002395
Iteration 207/1000 | Loss: 0.00002395
Iteration 208/1000 | Loss: 0.00002395
Iteration 209/1000 | Loss: 0.00002395
Iteration 210/1000 | Loss: 0.00002394
Iteration 211/1000 | Loss: 0.00002394
Iteration 212/1000 | Loss: 0.00002394
Iteration 213/1000 | Loss: 0.00002394
Iteration 214/1000 | Loss: 0.00002394
Iteration 215/1000 | Loss: 0.00002394
Iteration 216/1000 | Loss: 0.00002394
Iteration 217/1000 | Loss: 0.00002394
Iteration 218/1000 | Loss: 0.00002394
Iteration 219/1000 | Loss: 0.00002393
Iteration 220/1000 | Loss: 0.00002393
Iteration 221/1000 | Loss: 0.00002393
Iteration 222/1000 | Loss: 0.00002392
Iteration 223/1000 | Loss: 0.00002392
Iteration 224/1000 | Loss: 0.00002391
Iteration 225/1000 | Loss: 0.00002391
Iteration 226/1000 | Loss: 0.00002391
Iteration 227/1000 | Loss: 0.00002390
Iteration 228/1000 | Loss: 0.00002390
Iteration 229/1000 | Loss: 0.00002389
Iteration 230/1000 | Loss: 0.00002389
Iteration 231/1000 | Loss: 0.00002389
Iteration 232/1000 | Loss: 0.00002388
Iteration 233/1000 | Loss: 0.00002388
Iteration 234/1000 | Loss: 0.00002387
Iteration 235/1000 | Loss: 0.00002387
Iteration 236/1000 | Loss: 0.00002387
Iteration 237/1000 | Loss: 0.00002386
Iteration 238/1000 | Loss: 0.00002386
Iteration 239/1000 | Loss: 0.00002386
Iteration 240/1000 | Loss: 0.00002385
Iteration 241/1000 | Loss: 0.00002385
Iteration 242/1000 | Loss: 0.00002384
Iteration 243/1000 | Loss: 0.00002384
Iteration 244/1000 | Loss: 0.00002382
Iteration 245/1000 | Loss: 0.00002380
Iteration 246/1000 | Loss: 0.00002380
Iteration 247/1000 | Loss: 0.00002379
Iteration 248/1000 | Loss: 0.00002379
Iteration 249/1000 | Loss: 0.00002378
Iteration 250/1000 | Loss: 0.00002375
Iteration 251/1000 | Loss: 0.00002375
Iteration 252/1000 | Loss: 0.00002374
Iteration 253/1000 | Loss: 0.00002373
Iteration 254/1000 | Loss: 0.00002373
Iteration 255/1000 | Loss: 0.00002373
Iteration 256/1000 | Loss: 0.00002372
Iteration 257/1000 | Loss: 0.00002372
Iteration 258/1000 | Loss: 0.00002372
Iteration 259/1000 | Loss: 0.00002372
Iteration 260/1000 | Loss: 0.00002372
Iteration 261/1000 | Loss: 0.00002371
Iteration 262/1000 | Loss: 0.00002371
Iteration 263/1000 | Loss: 0.00002371
Iteration 264/1000 | Loss: 0.00002371
Iteration 265/1000 | Loss: 0.00002371
Iteration 266/1000 | Loss: 0.00002370
Iteration 267/1000 | Loss: 0.00002370
Iteration 268/1000 | Loss: 0.00002370
Iteration 269/1000 | Loss: 0.00002370
Iteration 270/1000 | Loss: 0.00002370
Iteration 271/1000 | Loss: 0.00002370
Iteration 272/1000 | Loss: 0.00002369
Iteration 273/1000 | Loss: 0.00002369
Iteration 274/1000 | Loss: 0.00002369
Iteration 275/1000 | Loss: 0.00002369
Iteration 276/1000 | Loss: 0.00002369
Iteration 277/1000 | Loss: 0.00002368
Iteration 278/1000 | Loss: 0.00002368
Iteration 279/1000 | Loss: 0.00002368
Iteration 280/1000 | Loss: 0.00002368
Iteration 281/1000 | Loss: 0.00002368
Iteration 282/1000 | Loss: 0.00002367
Iteration 283/1000 | Loss: 0.00002367
Iteration 284/1000 | Loss: 0.00002367
Iteration 285/1000 | Loss: 0.00002367
Iteration 286/1000 | Loss: 0.00002366
Iteration 287/1000 | Loss: 0.00002366
Iteration 288/1000 | Loss: 0.00002366
Iteration 289/1000 | Loss: 0.00002366
Iteration 290/1000 | Loss: 0.00002365
Iteration 291/1000 | Loss: 0.00002365
Iteration 292/1000 | Loss: 0.00002365
Iteration 293/1000 | Loss: 0.00002365
Iteration 294/1000 | Loss: 0.00002365
Iteration 295/1000 | Loss: 0.00002365
Iteration 296/1000 | Loss: 0.00002365
Iteration 297/1000 | Loss: 0.00002365
Iteration 298/1000 | Loss: 0.00002365
Iteration 299/1000 | Loss: 0.00002365
Iteration 300/1000 | Loss: 0.00002364
Iteration 301/1000 | Loss: 0.00002364
Iteration 302/1000 | Loss: 0.00002364
Iteration 303/1000 | Loss: 0.00002363
Iteration 304/1000 | Loss: 0.00002363
Iteration 305/1000 | Loss: 0.00002363
Iteration 306/1000 | Loss: 0.00002363
Iteration 307/1000 | Loss: 0.00002362
Iteration 308/1000 | Loss: 0.00002362
Iteration 309/1000 | Loss: 0.00002362
Iteration 310/1000 | Loss: 0.00002362
Iteration 311/1000 | Loss: 0.00002361
Iteration 312/1000 | Loss: 0.00002361
Iteration 313/1000 | Loss: 0.00002361
Iteration 314/1000 | Loss: 0.00002360
Iteration 315/1000 | Loss: 0.00002360
Iteration 316/1000 | Loss: 0.00002360
Iteration 317/1000 | Loss: 0.00002360
Iteration 318/1000 | Loss: 0.00002360
Iteration 319/1000 | Loss: 0.00002360
Iteration 320/1000 | Loss: 0.00002360
Iteration 321/1000 | Loss: 0.00002360
Iteration 322/1000 | Loss: 0.00002360
Iteration 323/1000 | Loss: 0.00002360
Iteration 324/1000 | Loss: 0.00002359
Iteration 325/1000 | Loss: 0.00002359
Iteration 326/1000 | Loss: 0.00002359
Iteration 327/1000 | Loss: 0.00002359
Iteration 328/1000 | Loss: 0.00002359
Iteration 329/1000 | Loss: 0.00002359
Iteration 330/1000 | Loss: 0.00002359
Iteration 331/1000 | Loss: 0.00002359
Iteration 332/1000 | Loss: 0.00002359
Iteration 333/1000 | Loss: 0.00002359
Iteration 334/1000 | Loss: 0.00002359
Iteration 335/1000 | Loss: 0.00002359
Iteration 336/1000 | Loss: 0.00002359
Iteration 337/1000 | Loss: 0.00002359
Iteration 338/1000 | Loss: 0.00002359
Iteration 339/1000 | Loss: 0.00002358
Iteration 340/1000 | Loss: 0.00002358
Iteration 341/1000 | Loss: 0.00002358
Iteration 342/1000 | Loss: 0.00002358
Iteration 343/1000 | Loss: 0.00002358
Iteration 344/1000 | Loss: 0.00002358
Iteration 345/1000 | Loss: 0.00002358
Iteration 346/1000 | Loss: 0.00002358
Iteration 347/1000 | Loss: 0.00002358
Iteration 348/1000 | Loss: 0.00002358
Iteration 349/1000 | Loss: 0.00002358
Iteration 350/1000 | Loss: 0.00002358
Iteration 351/1000 | Loss: 0.00002358
Iteration 352/1000 | Loss: 0.00002358
Iteration 353/1000 | Loss: 0.00002358
Iteration 354/1000 | Loss: 0.00002358
Iteration 355/1000 | Loss: 0.00002358
Iteration 356/1000 | Loss: 0.00002358
Iteration 357/1000 | Loss: 0.00002358
Iteration 358/1000 | Loss: 0.00002358
Iteration 359/1000 | Loss: 0.00002358
Iteration 360/1000 | Loss: 0.00002358
Iteration 361/1000 | Loss: 0.00002358
Iteration 362/1000 | Loss: 0.00002358
Iteration 363/1000 | Loss: 0.00002358
Iteration 364/1000 | Loss: 0.00002358
Iteration 365/1000 | Loss: 0.00002358
Iteration 366/1000 | Loss: 0.00002358
Iteration 367/1000 | Loss: 0.00002358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 367. Stopping optimization.
Last 5 losses: [2.3583805159432814e-05, 2.3583805159432814e-05, 2.3583805159432814e-05, 2.3583805159432814e-05, 2.3583805159432814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3583805159432814e-05

Optimization complete. Final v2v error: 3.9673471450805664 mm

Highest mean error: 7.3186798095703125 mm for frame 76

Lowest mean error: 2.9631826877593994 mm for frame 144

Saving results

Total time: 260.5948886871338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956522
Iteration 2/25 | Loss: 0.00956522
Iteration 3/25 | Loss: 0.00390117
Iteration 4/25 | Loss: 0.00221689
Iteration 5/25 | Loss: 0.00199529
Iteration 6/25 | Loss: 0.00179532
Iteration 7/25 | Loss: 0.00182027
Iteration 8/25 | Loss: 0.00185706
Iteration 9/25 | Loss: 0.00181815
Iteration 10/25 | Loss: 0.00169053
Iteration 11/25 | Loss: 0.00159265
Iteration 12/25 | Loss: 0.00154681
Iteration 13/25 | Loss: 0.00153016
Iteration 14/25 | Loss: 0.00152798
Iteration 15/25 | Loss: 0.00151703
Iteration 16/25 | Loss: 0.00150516
Iteration 17/25 | Loss: 0.00150465
Iteration 18/25 | Loss: 0.00150034
Iteration 19/25 | Loss: 0.00149621
Iteration 20/25 | Loss: 0.00149830
Iteration 21/25 | Loss: 0.00149878
Iteration 22/25 | Loss: 0.00149347
Iteration 23/25 | Loss: 0.00149251
Iteration 24/25 | Loss: 0.00149090
Iteration 25/25 | Loss: 0.00149064

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27560151
Iteration 2/25 | Loss: 0.00325344
Iteration 3/25 | Loss: 0.00315589
Iteration 4/25 | Loss: 0.00315589
Iteration 5/25 | Loss: 0.00315589
Iteration 6/25 | Loss: 0.00315588
Iteration 7/25 | Loss: 0.00315588
Iteration 8/25 | Loss: 0.00315588
Iteration 9/25 | Loss: 0.00315588
Iteration 10/25 | Loss: 0.00315588
Iteration 11/25 | Loss: 0.00315588
Iteration 12/25 | Loss: 0.00315588
Iteration 13/25 | Loss: 0.00315588
Iteration 14/25 | Loss: 0.00315588
Iteration 15/25 | Loss: 0.00315588
Iteration 16/25 | Loss: 0.00315588
Iteration 17/25 | Loss: 0.00315588
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0031558831688016653, 0.0031558831688016653, 0.0031558831688016653, 0.0031558831688016653, 0.0031558831688016653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031558831688016653

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00315588
Iteration 2/1000 | Loss: 0.00073598
Iteration 3/1000 | Loss: 0.00054095
Iteration 4/1000 | Loss: 0.00029603
Iteration 5/1000 | Loss: 0.00040537
Iteration 6/1000 | Loss: 0.00059648
Iteration 7/1000 | Loss: 0.00087447
Iteration 8/1000 | Loss: 0.00049836
Iteration 9/1000 | Loss: 0.00079393
Iteration 10/1000 | Loss: 0.00078986
Iteration 11/1000 | Loss: 0.00036732
Iteration 12/1000 | Loss: 0.00023715
Iteration 13/1000 | Loss: 0.00022775
Iteration 14/1000 | Loss: 0.00020989
Iteration 15/1000 | Loss: 0.00014678
Iteration 16/1000 | Loss: 0.00041637
Iteration 17/1000 | Loss: 0.00023509
Iteration 18/1000 | Loss: 0.00022023
Iteration 19/1000 | Loss: 0.00042448
Iteration 20/1000 | Loss: 0.00065737
Iteration 21/1000 | Loss: 0.00041060
Iteration 22/1000 | Loss: 0.00028603
Iteration 23/1000 | Loss: 0.00042423
Iteration 24/1000 | Loss: 0.00106532
Iteration 25/1000 | Loss: 0.00292075
Iteration 26/1000 | Loss: 0.00366120
Iteration 27/1000 | Loss: 0.00102181
Iteration 28/1000 | Loss: 0.00034678
Iteration 29/1000 | Loss: 0.00217392
Iteration 30/1000 | Loss: 0.00013355
Iteration 31/1000 | Loss: 0.00025100
Iteration 32/1000 | Loss: 0.00018883
Iteration 33/1000 | Loss: 0.00031274
Iteration 34/1000 | Loss: 0.00013871
Iteration 35/1000 | Loss: 0.00010248
Iteration 36/1000 | Loss: 0.00020030
Iteration 37/1000 | Loss: 0.00004583
Iteration 38/1000 | Loss: 0.00011622
Iteration 39/1000 | Loss: 0.00005150
Iteration 40/1000 | Loss: 0.00003295
Iteration 41/1000 | Loss: 0.00003046
Iteration 42/1000 | Loss: 0.00003419
Iteration 43/1000 | Loss: 0.00004572
Iteration 44/1000 | Loss: 0.00002618
Iteration 45/1000 | Loss: 0.00010497
Iteration 46/1000 | Loss: 0.00002419
Iteration 47/1000 | Loss: 0.00027397
Iteration 48/1000 | Loss: 0.00015129
Iteration 49/1000 | Loss: 0.00004929
Iteration 50/1000 | Loss: 0.00022196
Iteration 51/1000 | Loss: 0.00018588
Iteration 52/1000 | Loss: 0.00023805
Iteration 53/1000 | Loss: 0.00016973
Iteration 54/1000 | Loss: 0.00030354
Iteration 55/1000 | Loss: 0.00006008
Iteration 56/1000 | Loss: 0.00002667
Iteration 57/1000 | Loss: 0.00002603
Iteration 58/1000 | Loss: 0.00002619
Iteration 59/1000 | Loss: 0.00002170
Iteration 60/1000 | Loss: 0.00003182
Iteration 61/1000 | Loss: 0.00002162
Iteration 62/1000 | Loss: 0.00002038
Iteration 63/1000 | Loss: 0.00002000
Iteration 64/1000 | Loss: 0.00002384
Iteration 65/1000 | Loss: 0.00003556
Iteration 66/1000 | Loss: 0.00001961
Iteration 67/1000 | Loss: 0.00001997
Iteration 68/1000 | Loss: 0.00001938
Iteration 69/1000 | Loss: 0.00001936
Iteration 70/1000 | Loss: 0.00001935
Iteration 71/1000 | Loss: 0.00001935
Iteration 72/1000 | Loss: 0.00002425
Iteration 73/1000 | Loss: 0.00002425
Iteration 74/1000 | Loss: 0.00004122
Iteration 75/1000 | Loss: 0.00002140
Iteration 76/1000 | Loss: 0.00002188
Iteration 77/1000 | Loss: 0.00001977
Iteration 78/1000 | Loss: 0.00001908
Iteration 79/1000 | Loss: 0.00001908
Iteration 80/1000 | Loss: 0.00001908
Iteration 81/1000 | Loss: 0.00001908
Iteration 82/1000 | Loss: 0.00001908
Iteration 83/1000 | Loss: 0.00001908
Iteration 84/1000 | Loss: 0.00001908
Iteration 85/1000 | Loss: 0.00001908
Iteration 86/1000 | Loss: 0.00001908
Iteration 87/1000 | Loss: 0.00001907
Iteration 88/1000 | Loss: 0.00001907
Iteration 89/1000 | Loss: 0.00001906
Iteration 90/1000 | Loss: 0.00001906
Iteration 91/1000 | Loss: 0.00001906
Iteration 92/1000 | Loss: 0.00001906
Iteration 93/1000 | Loss: 0.00001905
Iteration 94/1000 | Loss: 0.00001905
Iteration 95/1000 | Loss: 0.00001905
Iteration 96/1000 | Loss: 0.00001902
Iteration 97/1000 | Loss: 0.00001902
Iteration 98/1000 | Loss: 0.00001900
Iteration 99/1000 | Loss: 0.00001900
Iteration 100/1000 | Loss: 0.00001900
Iteration 101/1000 | Loss: 0.00001900
Iteration 102/1000 | Loss: 0.00001900
Iteration 103/1000 | Loss: 0.00001899
Iteration 104/1000 | Loss: 0.00002334
Iteration 105/1000 | Loss: 0.00001903
Iteration 106/1000 | Loss: 0.00001901
Iteration 107/1000 | Loss: 0.00001900
Iteration 108/1000 | Loss: 0.00001899
Iteration 109/1000 | Loss: 0.00001898
Iteration 110/1000 | Loss: 0.00001897
Iteration 111/1000 | Loss: 0.00001897
Iteration 112/1000 | Loss: 0.00001896
Iteration 113/1000 | Loss: 0.00001896
Iteration 114/1000 | Loss: 0.00001896
Iteration 115/1000 | Loss: 0.00001895
Iteration 116/1000 | Loss: 0.00001895
Iteration 117/1000 | Loss: 0.00001894
Iteration 118/1000 | Loss: 0.00001894
Iteration 119/1000 | Loss: 0.00001894
Iteration 120/1000 | Loss: 0.00001894
Iteration 121/1000 | Loss: 0.00001894
Iteration 122/1000 | Loss: 0.00001893
Iteration 123/1000 | Loss: 0.00001893
Iteration 124/1000 | Loss: 0.00001893
Iteration 125/1000 | Loss: 0.00001893
Iteration 126/1000 | Loss: 0.00001893
Iteration 127/1000 | Loss: 0.00001893
Iteration 128/1000 | Loss: 0.00001893
Iteration 129/1000 | Loss: 0.00001893
Iteration 130/1000 | Loss: 0.00001917
Iteration 131/1000 | Loss: 0.00001893
Iteration 132/1000 | Loss: 0.00001893
Iteration 133/1000 | Loss: 0.00001893
Iteration 134/1000 | Loss: 0.00001893
Iteration 135/1000 | Loss: 0.00001892
Iteration 136/1000 | Loss: 0.00001892
Iteration 137/1000 | Loss: 0.00001892
Iteration 138/1000 | Loss: 0.00001892
Iteration 139/1000 | Loss: 0.00001892
Iteration 140/1000 | Loss: 0.00001892
Iteration 141/1000 | Loss: 0.00001892
Iteration 142/1000 | Loss: 0.00001892
Iteration 143/1000 | Loss: 0.00001891
Iteration 144/1000 | Loss: 0.00001891
Iteration 145/1000 | Loss: 0.00001891
Iteration 146/1000 | Loss: 0.00001891
Iteration 147/1000 | Loss: 0.00001890
Iteration 148/1000 | Loss: 0.00001890
Iteration 149/1000 | Loss: 0.00001890
Iteration 150/1000 | Loss: 0.00001890
Iteration 151/1000 | Loss: 0.00001890
Iteration 152/1000 | Loss: 0.00001889
Iteration 153/1000 | Loss: 0.00001889
Iteration 154/1000 | Loss: 0.00001889
Iteration 155/1000 | Loss: 0.00001889
Iteration 156/1000 | Loss: 0.00001889
Iteration 157/1000 | Loss: 0.00001889
Iteration 158/1000 | Loss: 0.00001889
Iteration 159/1000 | Loss: 0.00001889
Iteration 160/1000 | Loss: 0.00001889
Iteration 161/1000 | Loss: 0.00001888
Iteration 162/1000 | Loss: 0.00001888
Iteration 163/1000 | Loss: 0.00001888
Iteration 164/1000 | Loss: 0.00001888
Iteration 165/1000 | Loss: 0.00001888
Iteration 166/1000 | Loss: 0.00001888
Iteration 167/1000 | Loss: 0.00001888
Iteration 168/1000 | Loss: 0.00001888
Iteration 169/1000 | Loss: 0.00001888
Iteration 170/1000 | Loss: 0.00001888
Iteration 171/1000 | Loss: 0.00001888
Iteration 172/1000 | Loss: 0.00001888
Iteration 173/1000 | Loss: 0.00001888
Iteration 174/1000 | Loss: 0.00001888
Iteration 175/1000 | Loss: 0.00001887
Iteration 176/1000 | Loss: 0.00001887
Iteration 177/1000 | Loss: 0.00001887
Iteration 178/1000 | Loss: 0.00001887
Iteration 179/1000 | Loss: 0.00001887
Iteration 180/1000 | Loss: 0.00001887
Iteration 181/1000 | Loss: 0.00001887
Iteration 182/1000 | Loss: 0.00001887
Iteration 183/1000 | Loss: 0.00001887
Iteration 184/1000 | Loss: 0.00001887
Iteration 185/1000 | Loss: 0.00001887
Iteration 186/1000 | Loss: 0.00001887
Iteration 187/1000 | Loss: 0.00001887
Iteration 188/1000 | Loss: 0.00001887
Iteration 189/1000 | Loss: 0.00001887
Iteration 190/1000 | Loss: 0.00001887
Iteration 191/1000 | Loss: 0.00001887
Iteration 192/1000 | Loss: 0.00001887
Iteration 193/1000 | Loss: 0.00001887
Iteration 194/1000 | Loss: 0.00001887
Iteration 195/1000 | Loss: 0.00001887
Iteration 196/1000 | Loss: 0.00001887
Iteration 197/1000 | Loss: 0.00001887
Iteration 198/1000 | Loss: 0.00001887
Iteration 199/1000 | Loss: 0.00001887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.8867302060243674e-05, 1.8867302060243674e-05, 1.8867302060243674e-05, 1.8867302060243674e-05, 1.8867302060243674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8867302060243674e-05

Optimization complete. Final v2v error: 3.189682960510254 mm

Highest mean error: 11.375258445739746 mm for frame 229

Lowest mean error: 2.6707746982574463 mm for frame 233

Saving results

Total time: 177.37530660629272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012816
Iteration 2/25 | Loss: 0.00177037
Iteration 3/25 | Loss: 0.00136330
Iteration 4/25 | Loss: 0.00130274
Iteration 5/25 | Loss: 0.00134375
Iteration 6/25 | Loss: 0.00128943
Iteration 7/25 | Loss: 0.00125502
Iteration 8/25 | Loss: 0.00125876
Iteration 9/25 | Loss: 0.00120371
Iteration 10/25 | Loss: 0.00120258
Iteration 11/25 | Loss: 0.00118694
Iteration 12/25 | Loss: 0.00118487
Iteration 13/25 | Loss: 0.00118090
Iteration 14/25 | Loss: 0.00118041
Iteration 15/25 | Loss: 0.00118040
Iteration 16/25 | Loss: 0.00118040
Iteration 17/25 | Loss: 0.00118040
Iteration 18/25 | Loss: 0.00118040
Iteration 19/25 | Loss: 0.00118040
Iteration 20/25 | Loss: 0.00118040
Iteration 21/25 | Loss: 0.00118040
Iteration 22/25 | Loss: 0.00118040
Iteration 23/25 | Loss: 0.00118040
Iteration 24/25 | Loss: 0.00118040
Iteration 25/25 | Loss: 0.00118040

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27630639
Iteration 2/25 | Loss: 0.00145655
Iteration 3/25 | Loss: 0.00131714
Iteration 4/25 | Loss: 0.00131714
Iteration 5/25 | Loss: 0.00131714
Iteration 6/25 | Loss: 0.00131714
Iteration 7/25 | Loss: 0.00131714
Iteration 8/25 | Loss: 0.00131714
Iteration 9/25 | Loss: 0.00131714
Iteration 10/25 | Loss: 0.00131714
Iteration 11/25 | Loss: 0.00131714
Iteration 12/25 | Loss: 0.00131714
Iteration 13/25 | Loss: 0.00131714
Iteration 14/25 | Loss: 0.00131714
Iteration 15/25 | Loss: 0.00131714
Iteration 16/25 | Loss: 0.00131714
Iteration 17/25 | Loss: 0.00131714
Iteration 18/25 | Loss: 0.00131714
Iteration 19/25 | Loss: 0.00131714
Iteration 20/25 | Loss: 0.00131714
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001317136106081307, 0.001317136106081307, 0.001317136106081307, 0.001317136106081307, 0.001317136106081307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001317136106081307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131714
Iteration 2/1000 | Loss: 0.00024254
Iteration 3/1000 | Loss: 0.00043424
Iteration 4/1000 | Loss: 0.00001420
Iteration 5/1000 | Loss: 0.00006449
Iteration 6/1000 | Loss: 0.00015293
Iteration 7/1000 | Loss: 0.00002212
Iteration 8/1000 | Loss: 0.00004808
Iteration 9/1000 | Loss: 0.00001213
Iteration 10/1000 | Loss: 0.00003606
Iteration 11/1000 | Loss: 0.00001152
Iteration 12/1000 | Loss: 0.00001605
Iteration 13/1000 | Loss: 0.00001109
Iteration 14/1000 | Loss: 0.00001461
Iteration 15/1000 | Loss: 0.00002509
Iteration 16/1000 | Loss: 0.00001084
Iteration 17/1000 | Loss: 0.00001544
Iteration 18/1000 | Loss: 0.00004876
Iteration 19/1000 | Loss: 0.00002902
Iteration 20/1000 | Loss: 0.00004602
Iteration 21/1000 | Loss: 0.00001218
Iteration 22/1000 | Loss: 0.00001049
Iteration 23/1000 | Loss: 0.00001079
Iteration 24/1000 | Loss: 0.00001140
Iteration 25/1000 | Loss: 0.00001913
Iteration 26/1000 | Loss: 0.00001157
Iteration 27/1000 | Loss: 0.00001207
Iteration 28/1000 | Loss: 0.00001120
Iteration 29/1000 | Loss: 0.00001062
Iteration 30/1000 | Loss: 0.00001026
Iteration 31/1000 | Loss: 0.00001026
Iteration 32/1000 | Loss: 0.00001024
Iteration 33/1000 | Loss: 0.00001024
Iteration 34/1000 | Loss: 0.00001024
Iteration 35/1000 | Loss: 0.00001024
Iteration 36/1000 | Loss: 0.00001024
Iteration 37/1000 | Loss: 0.00001024
Iteration 38/1000 | Loss: 0.00001024
Iteration 39/1000 | Loss: 0.00001024
Iteration 40/1000 | Loss: 0.00001024
Iteration 41/1000 | Loss: 0.00001023
Iteration 42/1000 | Loss: 0.00001023
Iteration 43/1000 | Loss: 0.00001023
Iteration 44/1000 | Loss: 0.00001023
Iteration 45/1000 | Loss: 0.00001023
Iteration 46/1000 | Loss: 0.00001020
Iteration 47/1000 | Loss: 0.00001020
Iteration 48/1000 | Loss: 0.00001020
Iteration 49/1000 | Loss: 0.00001019
Iteration 50/1000 | Loss: 0.00001019
Iteration 51/1000 | Loss: 0.00001015
Iteration 52/1000 | Loss: 0.00001015
Iteration 53/1000 | Loss: 0.00001014
Iteration 54/1000 | Loss: 0.00001014
Iteration 55/1000 | Loss: 0.00001014
Iteration 56/1000 | Loss: 0.00001014
Iteration 57/1000 | Loss: 0.00001014
Iteration 58/1000 | Loss: 0.00001014
Iteration 59/1000 | Loss: 0.00001012
Iteration 60/1000 | Loss: 0.00001011
Iteration 61/1000 | Loss: 0.00001010
Iteration 62/1000 | Loss: 0.00001010
Iteration 63/1000 | Loss: 0.00001010
Iteration 64/1000 | Loss: 0.00001010
Iteration 65/1000 | Loss: 0.00001010
Iteration 66/1000 | Loss: 0.00001010
Iteration 67/1000 | Loss: 0.00001010
Iteration 68/1000 | Loss: 0.00001010
Iteration 69/1000 | Loss: 0.00001010
Iteration 70/1000 | Loss: 0.00001009
Iteration 71/1000 | Loss: 0.00001009
Iteration 72/1000 | Loss: 0.00001009
Iteration 73/1000 | Loss: 0.00001009
Iteration 74/1000 | Loss: 0.00001009
Iteration 75/1000 | Loss: 0.00001009
Iteration 76/1000 | Loss: 0.00001918
Iteration 77/1000 | Loss: 0.00018438
Iteration 78/1000 | Loss: 0.00003318
Iteration 79/1000 | Loss: 0.00001099
Iteration 80/1000 | Loss: 0.00001005
Iteration 81/1000 | Loss: 0.00001000
Iteration 82/1000 | Loss: 0.00000999
Iteration 83/1000 | Loss: 0.00000998
Iteration 84/1000 | Loss: 0.00000998
Iteration 85/1000 | Loss: 0.00000998
Iteration 86/1000 | Loss: 0.00001253
Iteration 87/1000 | Loss: 0.00000994
Iteration 88/1000 | Loss: 0.00000994
Iteration 89/1000 | Loss: 0.00000994
Iteration 90/1000 | Loss: 0.00000994
Iteration 91/1000 | Loss: 0.00000994
Iteration 92/1000 | Loss: 0.00000994
Iteration 93/1000 | Loss: 0.00000994
Iteration 94/1000 | Loss: 0.00000994
Iteration 95/1000 | Loss: 0.00000993
Iteration 96/1000 | Loss: 0.00001111
Iteration 97/1000 | Loss: 0.00000995
Iteration 98/1000 | Loss: 0.00000995
Iteration 99/1000 | Loss: 0.00000995
Iteration 100/1000 | Loss: 0.00000995
Iteration 101/1000 | Loss: 0.00000995
Iteration 102/1000 | Loss: 0.00000995
Iteration 103/1000 | Loss: 0.00000995
Iteration 104/1000 | Loss: 0.00000995
Iteration 105/1000 | Loss: 0.00000995
Iteration 106/1000 | Loss: 0.00000995
Iteration 107/1000 | Loss: 0.00000994
Iteration 108/1000 | Loss: 0.00000994
Iteration 109/1000 | Loss: 0.00000994
Iteration 110/1000 | Loss: 0.00000994
Iteration 111/1000 | Loss: 0.00000994
Iteration 112/1000 | Loss: 0.00000993
Iteration 113/1000 | Loss: 0.00000993
Iteration 114/1000 | Loss: 0.00000993
Iteration 115/1000 | Loss: 0.00000993
Iteration 116/1000 | Loss: 0.00000993
Iteration 117/1000 | Loss: 0.00000993
Iteration 118/1000 | Loss: 0.00000993
Iteration 119/1000 | Loss: 0.00000993
Iteration 120/1000 | Loss: 0.00000993
Iteration 121/1000 | Loss: 0.00000993
Iteration 122/1000 | Loss: 0.00000993
Iteration 123/1000 | Loss: 0.00000993
Iteration 124/1000 | Loss: 0.00000993
Iteration 125/1000 | Loss: 0.00000993
Iteration 126/1000 | Loss: 0.00000993
Iteration 127/1000 | Loss: 0.00000993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [9.927824066835456e-06, 9.927824066835456e-06, 9.927824066835456e-06, 9.927824066835456e-06, 9.927824066835456e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.927824066835456e-06

Optimization complete. Final v2v error: 2.745144844055176 mm

Highest mean error: 2.8973069190979004 mm for frame 101

Lowest mean error: 2.6442971229553223 mm for frame 114

Saving results

Total time: 77.50297689437866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472019
Iteration 2/25 | Loss: 0.00129104
Iteration 3/25 | Loss: 0.00120919
Iteration 4/25 | Loss: 0.00119923
Iteration 5/25 | Loss: 0.00119684
Iteration 6/25 | Loss: 0.00119681
Iteration 7/25 | Loss: 0.00119681
Iteration 8/25 | Loss: 0.00119681
Iteration 9/25 | Loss: 0.00119681
Iteration 10/25 | Loss: 0.00119681
Iteration 11/25 | Loss: 0.00119681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011968062026426196, 0.0011968062026426196, 0.0011968062026426196, 0.0011968062026426196, 0.0011968062026426196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011968062026426196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.03544998
Iteration 2/25 | Loss: 0.00137025
Iteration 3/25 | Loss: 0.00137024
Iteration 4/25 | Loss: 0.00137024
Iteration 5/25 | Loss: 0.00137024
Iteration 6/25 | Loss: 0.00137024
Iteration 7/25 | Loss: 0.00137024
Iteration 8/25 | Loss: 0.00137024
Iteration 9/25 | Loss: 0.00137024
Iteration 10/25 | Loss: 0.00137024
Iteration 11/25 | Loss: 0.00137023
Iteration 12/25 | Loss: 0.00137023
Iteration 13/25 | Loss: 0.00137023
Iteration 14/25 | Loss: 0.00137023
Iteration 15/25 | Loss: 0.00137023
Iteration 16/25 | Loss: 0.00137023
Iteration 17/25 | Loss: 0.00137023
Iteration 18/25 | Loss: 0.00137023
Iteration 19/25 | Loss: 0.00137023
Iteration 20/25 | Loss: 0.00137023
Iteration 21/25 | Loss: 0.00137023
Iteration 22/25 | Loss: 0.00137023
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001370234414935112, 0.001370234414935112, 0.001370234414935112, 0.001370234414935112, 0.001370234414935112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001370234414935112

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137023
Iteration 2/1000 | Loss: 0.00002560
Iteration 3/1000 | Loss: 0.00001846
Iteration 4/1000 | Loss: 0.00001552
Iteration 5/1000 | Loss: 0.00001422
Iteration 6/1000 | Loss: 0.00001345
Iteration 7/1000 | Loss: 0.00001284
Iteration 8/1000 | Loss: 0.00001225
Iteration 9/1000 | Loss: 0.00001193
Iteration 10/1000 | Loss: 0.00001161
Iteration 11/1000 | Loss: 0.00001132
Iteration 12/1000 | Loss: 0.00001120
Iteration 13/1000 | Loss: 0.00001116
Iteration 14/1000 | Loss: 0.00001112
Iteration 15/1000 | Loss: 0.00001108
Iteration 16/1000 | Loss: 0.00001093
Iteration 17/1000 | Loss: 0.00001091
Iteration 18/1000 | Loss: 0.00001083
Iteration 19/1000 | Loss: 0.00001077
Iteration 20/1000 | Loss: 0.00001068
Iteration 21/1000 | Loss: 0.00001065
Iteration 22/1000 | Loss: 0.00001064
Iteration 23/1000 | Loss: 0.00001064
Iteration 24/1000 | Loss: 0.00001063
Iteration 25/1000 | Loss: 0.00001061
Iteration 26/1000 | Loss: 0.00001059
Iteration 27/1000 | Loss: 0.00001059
Iteration 28/1000 | Loss: 0.00001058
Iteration 29/1000 | Loss: 0.00001057
Iteration 30/1000 | Loss: 0.00001056
Iteration 31/1000 | Loss: 0.00001055
Iteration 32/1000 | Loss: 0.00001053
Iteration 33/1000 | Loss: 0.00001052
Iteration 34/1000 | Loss: 0.00001051
Iteration 35/1000 | Loss: 0.00001050
Iteration 36/1000 | Loss: 0.00001049
Iteration 37/1000 | Loss: 0.00001048
Iteration 38/1000 | Loss: 0.00001044
Iteration 39/1000 | Loss: 0.00001043
Iteration 40/1000 | Loss: 0.00001041
Iteration 41/1000 | Loss: 0.00001041
Iteration 42/1000 | Loss: 0.00001040
Iteration 43/1000 | Loss: 0.00001040
Iteration 44/1000 | Loss: 0.00001039
Iteration 45/1000 | Loss: 0.00001039
Iteration 46/1000 | Loss: 0.00001039
Iteration 47/1000 | Loss: 0.00001038
Iteration 48/1000 | Loss: 0.00001038
Iteration 49/1000 | Loss: 0.00001037
Iteration 50/1000 | Loss: 0.00001035
Iteration 51/1000 | Loss: 0.00001035
Iteration 52/1000 | Loss: 0.00001034
Iteration 53/1000 | Loss: 0.00001033
Iteration 54/1000 | Loss: 0.00001033
Iteration 55/1000 | Loss: 0.00001032
Iteration 56/1000 | Loss: 0.00001031
Iteration 57/1000 | Loss: 0.00001031
Iteration 58/1000 | Loss: 0.00001031
Iteration 59/1000 | Loss: 0.00001030
Iteration 60/1000 | Loss: 0.00001030
Iteration 61/1000 | Loss: 0.00001030
Iteration 62/1000 | Loss: 0.00001029
Iteration 63/1000 | Loss: 0.00001029
Iteration 64/1000 | Loss: 0.00001029
Iteration 65/1000 | Loss: 0.00001028
Iteration 66/1000 | Loss: 0.00001028
Iteration 67/1000 | Loss: 0.00001027
Iteration 68/1000 | Loss: 0.00001027
Iteration 69/1000 | Loss: 0.00001027
Iteration 70/1000 | Loss: 0.00001026
Iteration 71/1000 | Loss: 0.00001026
Iteration 72/1000 | Loss: 0.00001026
Iteration 73/1000 | Loss: 0.00001026
Iteration 74/1000 | Loss: 0.00001026
Iteration 75/1000 | Loss: 0.00001025
Iteration 76/1000 | Loss: 0.00001025
Iteration 77/1000 | Loss: 0.00001025
Iteration 78/1000 | Loss: 0.00001025
Iteration 79/1000 | Loss: 0.00001025
Iteration 80/1000 | Loss: 0.00001025
Iteration 81/1000 | Loss: 0.00001025
Iteration 82/1000 | Loss: 0.00001025
Iteration 83/1000 | Loss: 0.00001025
Iteration 84/1000 | Loss: 0.00001025
Iteration 85/1000 | Loss: 0.00001024
Iteration 86/1000 | Loss: 0.00001024
Iteration 87/1000 | Loss: 0.00001024
Iteration 88/1000 | Loss: 0.00001024
Iteration 89/1000 | Loss: 0.00001024
Iteration 90/1000 | Loss: 0.00001023
Iteration 91/1000 | Loss: 0.00001023
Iteration 92/1000 | Loss: 0.00001023
Iteration 93/1000 | Loss: 0.00001023
Iteration 94/1000 | Loss: 0.00001022
Iteration 95/1000 | Loss: 0.00001022
Iteration 96/1000 | Loss: 0.00001022
Iteration 97/1000 | Loss: 0.00001022
Iteration 98/1000 | Loss: 0.00001021
Iteration 99/1000 | Loss: 0.00001021
Iteration 100/1000 | Loss: 0.00001021
Iteration 101/1000 | Loss: 0.00001021
Iteration 102/1000 | Loss: 0.00001020
Iteration 103/1000 | Loss: 0.00001020
Iteration 104/1000 | Loss: 0.00001020
Iteration 105/1000 | Loss: 0.00001020
Iteration 106/1000 | Loss: 0.00001020
Iteration 107/1000 | Loss: 0.00001020
Iteration 108/1000 | Loss: 0.00001020
Iteration 109/1000 | Loss: 0.00001020
Iteration 110/1000 | Loss: 0.00001019
Iteration 111/1000 | Loss: 0.00001019
Iteration 112/1000 | Loss: 0.00001019
Iteration 113/1000 | Loss: 0.00001019
Iteration 114/1000 | Loss: 0.00001019
Iteration 115/1000 | Loss: 0.00001019
Iteration 116/1000 | Loss: 0.00001019
Iteration 117/1000 | Loss: 0.00001018
Iteration 118/1000 | Loss: 0.00001018
Iteration 119/1000 | Loss: 0.00001018
Iteration 120/1000 | Loss: 0.00001018
Iteration 121/1000 | Loss: 0.00001018
Iteration 122/1000 | Loss: 0.00001018
Iteration 123/1000 | Loss: 0.00001018
Iteration 124/1000 | Loss: 0.00001018
Iteration 125/1000 | Loss: 0.00001018
Iteration 126/1000 | Loss: 0.00001018
Iteration 127/1000 | Loss: 0.00001018
Iteration 128/1000 | Loss: 0.00001018
Iteration 129/1000 | Loss: 0.00001018
Iteration 130/1000 | Loss: 0.00001017
Iteration 131/1000 | Loss: 0.00001017
Iteration 132/1000 | Loss: 0.00001017
Iteration 133/1000 | Loss: 0.00001017
Iteration 134/1000 | Loss: 0.00001017
Iteration 135/1000 | Loss: 0.00001017
Iteration 136/1000 | Loss: 0.00001017
Iteration 137/1000 | Loss: 0.00001017
Iteration 138/1000 | Loss: 0.00001016
Iteration 139/1000 | Loss: 0.00001016
Iteration 140/1000 | Loss: 0.00001016
Iteration 141/1000 | Loss: 0.00001016
Iteration 142/1000 | Loss: 0.00001016
Iteration 143/1000 | Loss: 0.00001016
Iteration 144/1000 | Loss: 0.00001016
Iteration 145/1000 | Loss: 0.00001016
Iteration 146/1000 | Loss: 0.00001016
Iteration 147/1000 | Loss: 0.00001016
Iteration 148/1000 | Loss: 0.00001016
Iteration 149/1000 | Loss: 0.00001016
Iteration 150/1000 | Loss: 0.00001016
Iteration 151/1000 | Loss: 0.00001015
Iteration 152/1000 | Loss: 0.00001015
Iteration 153/1000 | Loss: 0.00001015
Iteration 154/1000 | Loss: 0.00001015
Iteration 155/1000 | Loss: 0.00001015
Iteration 156/1000 | Loss: 0.00001015
Iteration 157/1000 | Loss: 0.00001015
Iteration 158/1000 | Loss: 0.00001015
Iteration 159/1000 | Loss: 0.00001015
Iteration 160/1000 | Loss: 0.00001015
Iteration 161/1000 | Loss: 0.00001015
Iteration 162/1000 | Loss: 0.00001015
Iteration 163/1000 | Loss: 0.00001015
Iteration 164/1000 | Loss: 0.00001015
Iteration 165/1000 | Loss: 0.00001015
Iteration 166/1000 | Loss: 0.00001015
Iteration 167/1000 | Loss: 0.00001015
Iteration 168/1000 | Loss: 0.00001015
Iteration 169/1000 | Loss: 0.00001014
Iteration 170/1000 | Loss: 0.00001014
Iteration 171/1000 | Loss: 0.00001014
Iteration 172/1000 | Loss: 0.00001014
Iteration 173/1000 | Loss: 0.00001014
Iteration 174/1000 | Loss: 0.00001014
Iteration 175/1000 | Loss: 0.00001014
Iteration 176/1000 | Loss: 0.00001014
Iteration 177/1000 | Loss: 0.00001014
Iteration 178/1000 | Loss: 0.00001014
Iteration 179/1000 | Loss: 0.00001014
Iteration 180/1000 | Loss: 0.00001014
Iteration 181/1000 | Loss: 0.00001014
Iteration 182/1000 | Loss: 0.00001014
Iteration 183/1000 | Loss: 0.00001014
Iteration 184/1000 | Loss: 0.00001014
Iteration 185/1000 | Loss: 0.00001014
Iteration 186/1000 | Loss: 0.00001014
Iteration 187/1000 | Loss: 0.00001014
Iteration 188/1000 | Loss: 0.00001013
Iteration 189/1000 | Loss: 0.00001013
Iteration 190/1000 | Loss: 0.00001013
Iteration 191/1000 | Loss: 0.00001013
Iteration 192/1000 | Loss: 0.00001013
Iteration 193/1000 | Loss: 0.00001013
Iteration 194/1000 | Loss: 0.00001013
Iteration 195/1000 | Loss: 0.00001013
Iteration 196/1000 | Loss: 0.00001013
Iteration 197/1000 | Loss: 0.00001013
Iteration 198/1000 | Loss: 0.00001013
Iteration 199/1000 | Loss: 0.00001013
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.0134845979337115e-05, 1.0134845979337115e-05, 1.0134845979337115e-05, 1.0134845979337115e-05, 1.0134845979337115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0134845979337115e-05

Optimization complete. Final v2v error: 2.7591402530670166 mm

Highest mean error: 3.145526885986328 mm for frame 76

Lowest mean error: 2.554941177368164 mm for frame 30

Saving results

Total time: 44.98672318458557
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384660
Iteration 2/25 | Loss: 0.00125186
Iteration 3/25 | Loss: 0.00118682
Iteration 4/25 | Loss: 0.00117826
Iteration 5/25 | Loss: 0.00117557
Iteration 6/25 | Loss: 0.00117527
Iteration 7/25 | Loss: 0.00117527
Iteration 8/25 | Loss: 0.00117527
Iteration 9/25 | Loss: 0.00117527
Iteration 10/25 | Loss: 0.00117527
Iteration 11/25 | Loss: 0.00117527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011752721620723605, 0.0011752721620723605, 0.0011752721620723605, 0.0011752721620723605, 0.0011752721620723605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011752721620723605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.97674179
Iteration 2/25 | Loss: 0.00138271
Iteration 3/25 | Loss: 0.00138270
Iteration 4/25 | Loss: 0.00138270
Iteration 5/25 | Loss: 0.00138270
Iteration 6/25 | Loss: 0.00138270
Iteration 7/25 | Loss: 0.00138270
Iteration 8/25 | Loss: 0.00138270
Iteration 9/25 | Loss: 0.00138270
Iteration 10/25 | Loss: 0.00138270
Iteration 11/25 | Loss: 0.00138270
Iteration 12/25 | Loss: 0.00138270
Iteration 13/25 | Loss: 0.00138270
Iteration 14/25 | Loss: 0.00138270
Iteration 15/25 | Loss: 0.00138270
Iteration 16/25 | Loss: 0.00138270
Iteration 17/25 | Loss: 0.00138270
Iteration 18/25 | Loss: 0.00138270
Iteration 19/25 | Loss: 0.00138270
Iteration 20/25 | Loss: 0.00138270
Iteration 21/25 | Loss: 0.00138270
Iteration 22/25 | Loss: 0.00138270
Iteration 23/25 | Loss: 0.00138270
Iteration 24/25 | Loss: 0.00138270
Iteration 25/25 | Loss: 0.00138270

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138270
Iteration 2/1000 | Loss: 0.00001965
Iteration 3/1000 | Loss: 0.00001366
Iteration 4/1000 | Loss: 0.00001232
Iteration 5/1000 | Loss: 0.00001155
Iteration 6/1000 | Loss: 0.00001079
Iteration 7/1000 | Loss: 0.00001034
Iteration 8/1000 | Loss: 0.00001003
Iteration 9/1000 | Loss: 0.00000973
Iteration 10/1000 | Loss: 0.00000959
Iteration 11/1000 | Loss: 0.00000947
Iteration 12/1000 | Loss: 0.00000936
Iteration 13/1000 | Loss: 0.00000934
Iteration 14/1000 | Loss: 0.00000927
Iteration 15/1000 | Loss: 0.00000921
Iteration 16/1000 | Loss: 0.00000920
Iteration 17/1000 | Loss: 0.00000920
Iteration 18/1000 | Loss: 0.00000919
Iteration 19/1000 | Loss: 0.00000919
Iteration 20/1000 | Loss: 0.00000910
Iteration 21/1000 | Loss: 0.00000906
Iteration 22/1000 | Loss: 0.00000906
Iteration 23/1000 | Loss: 0.00000906
Iteration 24/1000 | Loss: 0.00000905
Iteration 25/1000 | Loss: 0.00000905
Iteration 26/1000 | Loss: 0.00000904
Iteration 27/1000 | Loss: 0.00000904
Iteration 28/1000 | Loss: 0.00000903
Iteration 29/1000 | Loss: 0.00000901
Iteration 30/1000 | Loss: 0.00000898
Iteration 31/1000 | Loss: 0.00000898
Iteration 32/1000 | Loss: 0.00000897
Iteration 33/1000 | Loss: 0.00000897
Iteration 34/1000 | Loss: 0.00000896
Iteration 35/1000 | Loss: 0.00000895
Iteration 36/1000 | Loss: 0.00000893
Iteration 37/1000 | Loss: 0.00000892
Iteration 38/1000 | Loss: 0.00000891
Iteration 39/1000 | Loss: 0.00000891
Iteration 40/1000 | Loss: 0.00000890
Iteration 41/1000 | Loss: 0.00000890
Iteration 42/1000 | Loss: 0.00000890
Iteration 43/1000 | Loss: 0.00000890
Iteration 44/1000 | Loss: 0.00000890
Iteration 45/1000 | Loss: 0.00000890
Iteration 46/1000 | Loss: 0.00000890
Iteration 47/1000 | Loss: 0.00000890
Iteration 48/1000 | Loss: 0.00000890
Iteration 49/1000 | Loss: 0.00000890
Iteration 50/1000 | Loss: 0.00000889
Iteration 51/1000 | Loss: 0.00000889
Iteration 52/1000 | Loss: 0.00000887
Iteration 53/1000 | Loss: 0.00000887
Iteration 54/1000 | Loss: 0.00000885
Iteration 55/1000 | Loss: 0.00000885
Iteration 56/1000 | Loss: 0.00000885
Iteration 57/1000 | Loss: 0.00000884
Iteration 58/1000 | Loss: 0.00000884
Iteration 59/1000 | Loss: 0.00000884
Iteration 60/1000 | Loss: 0.00000884
Iteration 61/1000 | Loss: 0.00000884
Iteration 62/1000 | Loss: 0.00000883
Iteration 63/1000 | Loss: 0.00000883
Iteration 64/1000 | Loss: 0.00000883
Iteration 65/1000 | Loss: 0.00000883
Iteration 66/1000 | Loss: 0.00000882
Iteration 67/1000 | Loss: 0.00000882
Iteration 68/1000 | Loss: 0.00000882
Iteration 69/1000 | Loss: 0.00000882
Iteration 70/1000 | Loss: 0.00000881
Iteration 71/1000 | Loss: 0.00000881
Iteration 72/1000 | Loss: 0.00000880
Iteration 73/1000 | Loss: 0.00000880
Iteration 74/1000 | Loss: 0.00000879
Iteration 75/1000 | Loss: 0.00000879
Iteration 76/1000 | Loss: 0.00000879
Iteration 77/1000 | Loss: 0.00000879
Iteration 78/1000 | Loss: 0.00000879
Iteration 79/1000 | Loss: 0.00000878
Iteration 80/1000 | Loss: 0.00000878
Iteration 81/1000 | Loss: 0.00000878
Iteration 82/1000 | Loss: 0.00000878
Iteration 83/1000 | Loss: 0.00000878
Iteration 84/1000 | Loss: 0.00000878
Iteration 85/1000 | Loss: 0.00000878
Iteration 86/1000 | Loss: 0.00000877
Iteration 87/1000 | Loss: 0.00000877
Iteration 88/1000 | Loss: 0.00000877
Iteration 89/1000 | Loss: 0.00000877
Iteration 90/1000 | Loss: 0.00000877
Iteration 91/1000 | Loss: 0.00000876
Iteration 92/1000 | Loss: 0.00000875
Iteration 93/1000 | Loss: 0.00000875
Iteration 94/1000 | Loss: 0.00000875
Iteration 95/1000 | Loss: 0.00000875
Iteration 96/1000 | Loss: 0.00000875
Iteration 97/1000 | Loss: 0.00000875
Iteration 98/1000 | Loss: 0.00000875
Iteration 99/1000 | Loss: 0.00000875
Iteration 100/1000 | Loss: 0.00000875
Iteration 101/1000 | Loss: 0.00000874
Iteration 102/1000 | Loss: 0.00000874
Iteration 103/1000 | Loss: 0.00000874
Iteration 104/1000 | Loss: 0.00000873
Iteration 105/1000 | Loss: 0.00000873
Iteration 106/1000 | Loss: 0.00000873
Iteration 107/1000 | Loss: 0.00000872
Iteration 108/1000 | Loss: 0.00000872
Iteration 109/1000 | Loss: 0.00000872
Iteration 110/1000 | Loss: 0.00000872
Iteration 111/1000 | Loss: 0.00000871
Iteration 112/1000 | Loss: 0.00000871
Iteration 113/1000 | Loss: 0.00000871
Iteration 114/1000 | Loss: 0.00000871
Iteration 115/1000 | Loss: 0.00000871
Iteration 116/1000 | Loss: 0.00000871
Iteration 117/1000 | Loss: 0.00000871
Iteration 118/1000 | Loss: 0.00000871
Iteration 119/1000 | Loss: 0.00000871
Iteration 120/1000 | Loss: 0.00000871
Iteration 121/1000 | Loss: 0.00000871
Iteration 122/1000 | Loss: 0.00000871
Iteration 123/1000 | Loss: 0.00000871
Iteration 124/1000 | Loss: 0.00000871
Iteration 125/1000 | Loss: 0.00000871
Iteration 126/1000 | Loss: 0.00000871
Iteration 127/1000 | Loss: 0.00000871
Iteration 128/1000 | Loss: 0.00000871
Iteration 129/1000 | Loss: 0.00000871
Iteration 130/1000 | Loss: 0.00000871
Iteration 131/1000 | Loss: 0.00000871
Iteration 132/1000 | Loss: 0.00000871
Iteration 133/1000 | Loss: 0.00000871
Iteration 134/1000 | Loss: 0.00000871
Iteration 135/1000 | Loss: 0.00000871
Iteration 136/1000 | Loss: 0.00000871
Iteration 137/1000 | Loss: 0.00000871
Iteration 138/1000 | Loss: 0.00000871
Iteration 139/1000 | Loss: 0.00000871
Iteration 140/1000 | Loss: 0.00000871
Iteration 141/1000 | Loss: 0.00000871
Iteration 142/1000 | Loss: 0.00000871
Iteration 143/1000 | Loss: 0.00000871
Iteration 144/1000 | Loss: 0.00000871
Iteration 145/1000 | Loss: 0.00000871
Iteration 146/1000 | Loss: 0.00000871
Iteration 147/1000 | Loss: 0.00000871
Iteration 148/1000 | Loss: 0.00000871
Iteration 149/1000 | Loss: 0.00000871
Iteration 150/1000 | Loss: 0.00000871
Iteration 151/1000 | Loss: 0.00000871
Iteration 152/1000 | Loss: 0.00000871
Iteration 153/1000 | Loss: 0.00000871
Iteration 154/1000 | Loss: 0.00000871
Iteration 155/1000 | Loss: 0.00000871
Iteration 156/1000 | Loss: 0.00000871
Iteration 157/1000 | Loss: 0.00000871
Iteration 158/1000 | Loss: 0.00000871
Iteration 159/1000 | Loss: 0.00000871
Iteration 160/1000 | Loss: 0.00000871
Iteration 161/1000 | Loss: 0.00000871
Iteration 162/1000 | Loss: 0.00000871
Iteration 163/1000 | Loss: 0.00000871
Iteration 164/1000 | Loss: 0.00000871
Iteration 165/1000 | Loss: 0.00000871
Iteration 166/1000 | Loss: 0.00000871
Iteration 167/1000 | Loss: 0.00000871
Iteration 168/1000 | Loss: 0.00000871
Iteration 169/1000 | Loss: 0.00000871
Iteration 170/1000 | Loss: 0.00000871
Iteration 171/1000 | Loss: 0.00000871
Iteration 172/1000 | Loss: 0.00000871
Iteration 173/1000 | Loss: 0.00000871
Iteration 174/1000 | Loss: 0.00000871
Iteration 175/1000 | Loss: 0.00000871
Iteration 176/1000 | Loss: 0.00000871
Iteration 177/1000 | Loss: 0.00000871
Iteration 178/1000 | Loss: 0.00000871
Iteration 179/1000 | Loss: 0.00000871
Iteration 180/1000 | Loss: 0.00000871
Iteration 181/1000 | Loss: 0.00000871
Iteration 182/1000 | Loss: 0.00000871
Iteration 183/1000 | Loss: 0.00000871
Iteration 184/1000 | Loss: 0.00000871
Iteration 185/1000 | Loss: 0.00000871
Iteration 186/1000 | Loss: 0.00000871
Iteration 187/1000 | Loss: 0.00000871
Iteration 188/1000 | Loss: 0.00000871
Iteration 189/1000 | Loss: 0.00000871
Iteration 190/1000 | Loss: 0.00000871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [8.708642781130038e-06, 8.708642781130038e-06, 8.708642781130038e-06, 8.708642781130038e-06, 8.708642781130038e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.708642781130038e-06

Optimization complete. Final v2v error: 2.5837104320526123 mm

Highest mean error: 2.7390823364257812 mm for frame 117

Lowest mean error: 2.4970648288726807 mm for frame 0

Saving results

Total time: 39.50789189338684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804593
Iteration 2/25 | Loss: 0.00129009
Iteration 3/25 | Loss: 0.00119232
Iteration 4/25 | Loss: 0.00118357
Iteration 5/25 | Loss: 0.00118178
Iteration 6/25 | Loss: 0.00118178
Iteration 7/25 | Loss: 0.00118178
Iteration 8/25 | Loss: 0.00118178
Iteration 9/25 | Loss: 0.00118178
Iteration 10/25 | Loss: 0.00118178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00118178129196167, 0.00118178129196167, 0.00118178129196167, 0.00118178129196167, 0.00118178129196167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00118178129196167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29178905
Iteration 2/25 | Loss: 0.00143406
Iteration 3/25 | Loss: 0.00143406
Iteration 4/25 | Loss: 0.00143406
Iteration 5/25 | Loss: 0.00143406
Iteration 6/25 | Loss: 0.00143406
Iteration 7/25 | Loss: 0.00143406
Iteration 8/25 | Loss: 0.00143406
Iteration 9/25 | Loss: 0.00143406
Iteration 10/25 | Loss: 0.00143406
Iteration 11/25 | Loss: 0.00143406
Iteration 12/25 | Loss: 0.00143406
Iteration 13/25 | Loss: 0.00143406
Iteration 14/25 | Loss: 0.00143405
Iteration 15/25 | Loss: 0.00143405
Iteration 16/25 | Loss: 0.00143405
Iteration 17/25 | Loss: 0.00143405
Iteration 18/25 | Loss: 0.00143405
Iteration 19/25 | Loss: 0.00143405
Iteration 20/25 | Loss: 0.00143405
Iteration 21/25 | Loss: 0.00143405
Iteration 22/25 | Loss: 0.00143405
Iteration 23/25 | Loss: 0.00143405
Iteration 24/25 | Loss: 0.00143405
Iteration 25/25 | Loss: 0.00143405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143405
Iteration 2/1000 | Loss: 0.00002335
Iteration 3/1000 | Loss: 0.00001491
Iteration 4/1000 | Loss: 0.00001300
Iteration 5/1000 | Loss: 0.00001200
Iteration 6/1000 | Loss: 0.00001144
Iteration 7/1000 | Loss: 0.00001084
Iteration 8/1000 | Loss: 0.00001058
Iteration 9/1000 | Loss: 0.00001031
Iteration 10/1000 | Loss: 0.00001009
Iteration 11/1000 | Loss: 0.00000996
Iteration 12/1000 | Loss: 0.00000995
Iteration 13/1000 | Loss: 0.00000993
Iteration 14/1000 | Loss: 0.00000992
Iteration 15/1000 | Loss: 0.00000992
Iteration 16/1000 | Loss: 0.00000991
Iteration 17/1000 | Loss: 0.00000990
Iteration 18/1000 | Loss: 0.00000989
Iteration 19/1000 | Loss: 0.00000985
Iteration 20/1000 | Loss: 0.00000978
Iteration 21/1000 | Loss: 0.00000967
Iteration 22/1000 | Loss: 0.00000959
Iteration 23/1000 | Loss: 0.00000958
Iteration 24/1000 | Loss: 0.00000955
Iteration 25/1000 | Loss: 0.00000955
Iteration 26/1000 | Loss: 0.00000954
Iteration 27/1000 | Loss: 0.00000954
Iteration 28/1000 | Loss: 0.00000954
Iteration 29/1000 | Loss: 0.00000954
Iteration 30/1000 | Loss: 0.00000954
Iteration 31/1000 | Loss: 0.00000954
Iteration 32/1000 | Loss: 0.00000951
Iteration 33/1000 | Loss: 0.00000946
Iteration 34/1000 | Loss: 0.00000945
Iteration 35/1000 | Loss: 0.00000941
Iteration 36/1000 | Loss: 0.00000940
Iteration 37/1000 | Loss: 0.00000940
Iteration 38/1000 | Loss: 0.00000933
Iteration 39/1000 | Loss: 0.00000932
Iteration 40/1000 | Loss: 0.00000931
Iteration 41/1000 | Loss: 0.00000930
Iteration 42/1000 | Loss: 0.00000929
Iteration 43/1000 | Loss: 0.00000928
Iteration 44/1000 | Loss: 0.00000928
Iteration 45/1000 | Loss: 0.00000928
Iteration 46/1000 | Loss: 0.00000928
Iteration 47/1000 | Loss: 0.00000927
Iteration 48/1000 | Loss: 0.00000927
Iteration 49/1000 | Loss: 0.00000927
Iteration 50/1000 | Loss: 0.00000926
Iteration 51/1000 | Loss: 0.00000926
Iteration 52/1000 | Loss: 0.00000925
Iteration 53/1000 | Loss: 0.00000924
Iteration 54/1000 | Loss: 0.00000923
Iteration 55/1000 | Loss: 0.00000922
Iteration 56/1000 | Loss: 0.00000922
Iteration 57/1000 | Loss: 0.00000921
Iteration 58/1000 | Loss: 0.00000919
Iteration 59/1000 | Loss: 0.00000919
Iteration 60/1000 | Loss: 0.00000919
Iteration 61/1000 | Loss: 0.00000918
Iteration 62/1000 | Loss: 0.00000917
Iteration 63/1000 | Loss: 0.00000917
Iteration 64/1000 | Loss: 0.00000913
Iteration 65/1000 | Loss: 0.00000913
Iteration 66/1000 | Loss: 0.00000913
Iteration 67/1000 | Loss: 0.00000913
Iteration 68/1000 | Loss: 0.00000913
Iteration 69/1000 | Loss: 0.00000913
Iteration 70/1000 | Loss: 0.00000913
Iteration 71/1000 | Loss: 0.00000913
Iteration 72/1000 | Loss: 0.00000913
Iteration 73/1000 | Loss: 0.00000913
Iteration 74/1000 | Loss: 0.00000912
Iteration 75/1000 | Loss: 0.00000912
Iteration 76/1000 | Loss: 0.00000911
Iteration 77/1000 | Loss: 0.00000910
Iteration 78/1000 | Loss: 0.00000910
Iteration 79/1000 | Loss: 0.00000910
Iteration 80/1000 | Loss: 0.00000909
Iteration 81/1000 | Loss: 0.00000909
Iteration 82/1000 | Loss: 0.00000909
Iteration 83/1000 | Loss: 0.00000909
Iteration 84/1000 | Loss: 0.00000909
Iteration 85/1000 | Loss: 0.00000908
Iteration 86/1000 | Loss: 0.00000908
Iteration 87/1000 | Loss: 0.00000907
Iteration 88/1000 | Loss: 0.00000907
Iteration 89/1000 | Loss: 0.00000907
Iteration 90/1000 | Loss: 0.00000907
Iteration 91/1000 | Loss: 0.00000906
Iteration 92/1000 | Loss: 0.00000906
Iteration 93/1000 | Loss: 0.00000906
Iteration 94/1000 | Loss: 0.00000906
Iteration 95/1000 | Loss: 0.00000906
Iteration 96/1000 | Loss: 0.00000906
Iteration 97/1000 | Loss: 0.00000906
Iteration 98/1000 | Loss: 0.00000906
Iteration 99/1000 | Loss: 0.00000906
Iteration 100/1000 | Loss: 0.00000906
Iteration 101/1000 | Loss: 0.00000906
Iteration 102/1000 | Loss: 0.00000906
Iteration 103/1000 | Loss: 0.00000906
Iteration 104/1000 | Loss: 0.00000905
Iteration 105/1000 | Loss: 0.00000905
Iteration 106/1000 | Loss: 0.00000905
Iteration 107/1000 | Loss: 0.00000905
Iteration 108/1000 | Loss: 0.00000905
Iteration 109/1000 | Loss: 0.00000904
Iteration 110/1000 | Loss: 0.00000904
Iteration 111/1000 | Loss: 0.00000904
Iteration 112/1000 | Loss: 0.00000904
Iteration 113/1000 | Loss: 0.00000904
Iteration 114/1000 | Loss: 0.00000904
Iteration 115/1000 | Loss: 0.00000903
Iteration 116/1000 | Loss: 0.00000903
Iteration 117/1000 | Loss: 0.00000903
Iteration 118/1000 | Loss: 0.00000903
Iteration 119/1000 | Loss: 0.00000903
Iteration 120/1000 | Loss: 0.00000903
Iteration 121/1000 | Loss: 0.00000903
Iteration 122/1000 | Loss: 0.00000902
Iteration 123/1000 | Loss: 0.00000902
Iteration 124/1000 | Loss: 0.00000902
Iteration 125/1000 | Loss: 0.00000901
Iteration 126/1000 | Loss: 0.00000901
Iteration 127/1000 | Loss: 0.00000901
Iteration 128/1000 | Loss: 0.00000901
Iteration 129/1000 | Loss: 0.00000900
Iteration 130/1000 | Loss: 0.00000900
Iteration 131/1000 | Loss: 0.00000899
Iteration 132/1000 | Loss: 0.00000899
Iteration 133/1000 | Loss: 0.00000899
Iteration 134/1000 | Loss: 0.00000899
Iteration 135/1000 | Loss: 0.00000899
Iteration 136/1000 | Loss: 0.00000898
Iteration 137/1000 | Loss: 0.00000898
Iteration 138/1000 | Loss: 0.00000898
Iteration 139/1000 | Loss: 0.00000898
Iteration 140/1000 | Loss: 0.00000898
Iteration 141/1000 | Loss: 0.00000898
Iteration 142/1000 | Loss: 0.00000898
Iteration 143/1000 | Loss: 0.00000898
Iteration 144/1000 | Loss: 0.00000898
Iteration 145/1000 | Loss: 0.00000898
Iteration 146/1000 | Loss: 0.00000897
Iteration 147/1000 | Loss: 0.00000897
Iteration 148/1000 | Loss: 0.00000897
Iteration 149/1000 | Loss: 0.00000897
Iteration 150/1000 | Loss: 0.00000897
Iteration 151/1000 | Loss: 0.00000897
Iteration 152/1000 | Loss: 0.00000897
Iteration 153/1000 | Loss: 0.00000896
Iteration 154/1000 | Loss: 0.00000896
Iteration 155/1000 | Loss: 0.00000896
Iteration 156/1000 | Loss: 0.00000896
Iteration 157/1000 | Loss: 0.00000896
Iteration 158/1000 | Loss: 0.00000895
Iteration 159/1000 | Loss: 0.00000895
Iteration 160/1000 | Loss: 0.00000895
Iteration 161/1000 | Loss: 0.00000895
Iteration 162/1000 | Loss: 0.00000895
Iteration 163/1000 | Loss: 0.00000895
Iteration 164/1000 | Loss: 0.00000895
Iteration 165/1000 | Loss: 0.00000895
Iteration 166/1000 | Loss: 0.00000895
Iteration 167/1000 | Loss: 0.00000895
Iteration 168/1000 | Loss: 0.00000895
Iteration 169/1000 | Loss: 0.00000895
Iteration 170/1000 | Loss: 0.00000894
Iteration 171/1000 | Loss: 0.00000894
Iteration 172/1000 | Loss: 0.00000893
Iteration 173/1000 | Loss: 0.00000893
Iteration 174/1000 | Loss: 0.00000893
Iteration 175/1000 | Loss: 0.00000893
Iteration 176/1000 | Loss: 0.00000893
Iteration 177/1000 | Loss: 0.00000892
Iteration 178/1000 | Loss: 0.00000892
Iteration 179/1000 | Loss: 0.00000892
Iteration 180/1000 | Loss: 0.00000892
Iteration 181/1000 | Loss: 0.00000892
Iteration 182/1000 | Loss: 0.00000892
Iteration 183/1000 | Loss: 0.00000892
Iteration 184/1000 | Loss: 0.00000892
Iteration 185/1000 | Loss: 0.00000892
Iteration 186/1000 | Loss: 0.00000892
Iteration 187/1000 | Loss: 0.00000892
Iteration 188/1000 | Loss: 0.00000892
Iteration 189/1000 | Loss: 0.00000892
Iteration 190/1000 | Loss: 0.00000892
Iteration 191/1000 | Loss: 0.00000892
Iteration 192/1000 | Loss: 0.00000892
Iteration 193/1000 | Loss: 0.00000891
Iteration 194/1000 | Loss: 0.00000891
Iteration 195/1000 | Loss: 0.00000891
Iteration 196/1000 | Loss: 0.00000891
Iteration 197/1000 | Loss: 0.00000891
Iteration 198/1000 | Loss: 0.00000891
Iteration 199/1000 | Loss: 0.00000891
Iteration 200/1000 | Loss: 0.00000891
Iteration 201/1000 | Loss: 0.00000891
Iteration 202/1000 | Loss: 0.00000891
Iteration 203/1000 | Loss: 0.00000891
Iteration 204/1000 | Loss: 0.00000891
Iteration 205/1000 | Loss: 0.00000891
Iteration 206/1000 | Loss: 0.00000891
Iteration 207/1000 | Loss: 0.00000891
Iteration 208/1000 | Loss: 0.00000891
Iteration 209/1000 | Loss: 0.00000891
Iteration 210/1000 | Loss: 0.00000891
Iteration 211/1000 | Loss: 0.00000891
Iteration 212/1000 | Loss: 0.00000891
Iteration 213/1000 | Loss: 0.00000891
Iteration 214/1000 | Loss: 0.00000891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [8.914786121749785e-06, 8.914786121749785e-06, 8.914786121749785e-06, 8.914786121749785e-06, 8.914786121749785e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.914786121749785e-06

Optimization complete. Final v2v error: 2.5532379150390625 mm

Highest mean error: 2.769697427749634 mm for frame 44

Lowest mean error: 2.390285015106201 mm for frame 160

Saving results

Total time: 43.49239492416382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461220
Iteration 2/25 | Loss: 0.00136667
Iteration 3/25 | Loss: 0.00124883
Iteration 4/25 | Loss: 0.00123688
Iteration 5/25 | Loss: 0.00123392
Iteration 6/25 | Loss: 0.00123309
Iteration 7/25 | Loss: 0.00123309
Iteration 8/25 | Loss: 0.00123309
Iteration 9/25 | Loss: 0.00123309
Iteration 10/25 | Loss: 0.00123309
Iteration 11/25 | Loss: 0.00123309
Iteration 12/25 | Loss: 0.00123309
Iteration 13/25 | Loss: 0.00123309
Iteration 14/25 | Loss: 0.00123309
Iteration 15/25 | Loss: 0.00123309
Iteration 16/25 | Loss: 0.00123309
Iteration 17/25 | Loss: 0.00123309
Iteration 18/25 | Loss: 0.00123309
Iteration 19/25 | Loss: 0.00123309
Iteration 20/25 | Loss: 0.00123309
Iteration 21/25 | Loss: 0.00123309
Iteration 22/25 | Loss: 0.00123309
Iteration 23/25 | Loss: 0.00123309
Iteration 24/25 | Loss: 0.00123309
Iteration 25/25 | Loss: 0.00123309

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39211345
Iteration 2/25 | Loss: 0.00159240
Iteration 3/25 | Loss: 0.00159240
Iteration 4/25 | Loss: 0.00159240
Iteration 5/25 | Loss: 0.00159240
Iteration 6/25 | Loss: 0.00159240
Iteration 7/25 | Loss: 0.00159240
Iteration 8/25 | Loss: 0.00159240
Iteration 9/25 | Loss: 0.00159240
Iteration 10/25 | Loss: 0.00159240
Iteration 11/25 | Loss: 0.00159240
Iteration 12/25 | Loss: 0.00159240
Iteration 13/25 | Loss: 0.00159240
Iteration 14/25 | Loss: 0.00159240
Iteration 15/25 | Loss: 0.00159240
Iteration 16/25 | Loss: 0.00159240
Iteration 17/25 | Loss: 0.00159240
Iteration 18/25 | Loss: 0.00159240
Iteration 19/25 | Loss: 0.00159240
Iteration 20/25 | Loss: 0.00159240
Iteration 21/25 | Loss: 0.00159240
Iteration 22/25 | Loss: 0.00159240
Iteration 23/25 | Loss: 0.00159240
Iteration 24/25 | Loss: 0.00159240
Iteration 25/25 | Loss: 0.00159240

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159240
Iteration 2/1000 | Loss: 0.00002733
Iteration 3/1000 | Loss: 0.00001969
Iteration 4/1000 | Loss: 0.00001742
Iteration 5/1000 | Loss: 0.00001646
Iteration 6/1000 | Loss: 0.00001594
Iteration 7/1000 | Loss: 0.00001538
Iteration 8/1000 | Loss: 0.00001506
Iteration 9/1000 | Loss: 0.00001477
Iteration 10/1000 | Loss: 0.00001446
Iteration 11/1000 | Loss: 0.00001429
Iteration 12/1000 | Loss: 0.00001423
Iteration 13/1000 | Loss: 0.00001409
Iteration 14/1000 | Loss: 0.00001394
Iteration 15/1000 | Loss: 0.00001387
Iteration 16/1000 | Loss: 0.00001383
Iteration 17/1000 | Loss: 0.00001381
Iteration 18/1000 | Loss: 0.00001375
Iteration 19/1000 | Loss: 0.00001372
Iteration 20/1000 | Loss: 0.00001372
Iteration 21/1000 | Loss: 0.00001372
Iteration 22/1000 | Loss: 0.00001369
Iteration 23/1000 | Loss: 0.00001367
Iteration 24/1000 | Loss: 0.00001360
Iteration 25/1000 | Loss: 0.00001353
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001351
Iteration 28/1000 | Loss: 0.00001350
Iteration 29/1000 | Loss: 0.00001348
Iteration 30/1000 | Loss: 0.00001348
Iteration 31/1000 | Loss: 0.00001347
Iteration 32/1000 | Loss: 0.00001345
Iteration 33/1000 | Loss: 0.00001344
Iteration 34/1000 | Loss: 0.00001343
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001341
Iteration 38/1000 | Loss: 0.00001341
Iteration 39/1000 | Loss: 0.00001339
Iteration 40/1000 | Loss: 0.00001339
Iteration 41/1000 | Loss: 0.00001338
Iteration 42/1000 | Loss: 0.00001338
Iteration 43/1000 | Loss: 0.00001338
Iteration 44/1000 | Loss: 0.00001337
Iteration 45/1000 | Loss: 0.00001336
Iteration 46/1000 | Loss: 0.00001336
Iteration 47/1000 | Loss: 0.00001335
Iteration 48/1000 | Loss: 0.00001335
Iteration 49/1000 | Loss: 0.00001335
Iteration 50/1000 | Loss: 0.00001335
Iteration 51/1000 | Loss: 0.00001335
Iteration 52/1000 | Loss: 0.00001334
Iteration 53/1000 | Loss: 0.00001333
Iteration 54/1000 | Loss: 0.00001333
Iteration 55/1000 | Loss: 0.00001333
Iteration 56/1000 | Loss: 0.00001333
Iteration 57/1000 | Loss: 0.00001332
Iteration 58/1000 | Loss: 0.00001332
Iteration 59/1000 | Loss: 0.00001332
Iteration 60/1000 | Loss: 0.00001332
Iteration 61/1000 | Loss: 0.00001332
Iteration 62/1000 | Loss: 0.00001332
Iteration 63/1000 | Loss: 0.00001332
Iteration 64/1000 | Loss: 0.00001332
Iteration 65/1000 | Loss: 0.00001332
Iteration 66/1000 | Loss: 0.00001332
Iteration 67/1000 | Loss: 0.00001332
Iteration 68/1000 | Loss: 0.00001331
Iteration 69/1000 | Loss: 0.00001331
Iteration 70/1000 | Loss: 0.00001331
Iteration 71/1000 | Loss: 0.00001331
Iteration 72/1000 | Loss: 0.00001331
Iteration 73/1000 | Loss: 0.00001330
Iteration 74/1000 | Loss: 0.00001330
Iteration 75/1000 | Loss: 0.00001330
Iteration 76/1000 | Loss: 0.00001330
Iteration 77/1000 | Loss: 0.00001330
Iteration 78/1000 | Loss: 0.00001330
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00001329
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001328
Iteration 83/1000 | Loss: 0.00001328
Iteration 84/1000 | Loss: 0.00001328
Iteration 85/1000 | Loss: 0.00001328
Iteration 86/1000 | Loss: 0.00001328
Iteration 87/1000 | Loss: 0.00001327
Iteration 88/1000 | Loss: 0.00001327
Iteration 89/1000 | Loss: 0.00001327
Iteration 90/1000 | Loss: 0.00001327
Iteration 91/1000 | Loss: 0.00001327
Iteration 92/1000 | Loss: 0.00001327
Iteration 93/1000 | Loss: 0.00001327
Iteration 94/1000 | Loss: 0.00001326
Iteration 95/1000 | Loss: 0.00001326
Iteration 96/1000 | Loss: 0.00001326
Iteration 97/1000 | Loss: 0.00001326
Iteration 98/1000 | Loss: 0.00001325
Iteration 99/1000 | Loss: 0.00001325
Iteration 100/1000 | Loss: 0.00001325
Iteration 101/1000 | Loss: 0.00001325
Iteration 102/1000 | Loss: 0.00001325
Iteration 103/1000 | Loss: 0.00001325
Iteration 104/1000 | Loss: 0.00001325
Iteration 105/1000 | Loss: 0.00001325
Iteration 106/1000 | Loss: 0.00001325
Iteration 107/1000 | Loss: 0.00001325
Iteration 108/1000 | Loss: 0.00001325
Iteration 109/1000 | Loss: 0.00001324
Iteration 110/1000 | Loss: 0.00001324
Iteration 111/1000 | Loss: 0.00001324
Iteration 112/1000 | Loss: 0.00001324
Iteration 113/1000 | Loss: 0.00001324
Iteration 114/1000 | Loss: 0.00001324
Iteration 115/1000 | Loss: 0.00001323
Iteration 116/1000 | Loss: 0.00001323
Iteration 117/1000 | Loss: 0.00001323
Iteration 118/1000 | Loss: 0.00001323
Iteration 119/1000 | Loss: 0.00001323
Iteration 120/1000 | Loss: 0.00001323
Iteration 121/1000 | Loss: 0.00001322
Iteration 122/1000 | Loss: 0.00001322
Iteration 123/1000 | Loss: 0.00001322
Iteration 124/1000 | Loss: 0.00001322
Iteration 125/1000 | Loss: 0.00001322
Iteration 126/1000 | Loss: 0.00001322
Iteration 127/1000 | Loss: 0.00001322
Iteration 128/1000 | Loss: 0.00001322
Iteration 129/1000 | Loss: 0.00001322
Iteration 130/1000 | Loss: 0.00001322
Iteration 131/1000 | Loss: 0.00001322
Iteration 132/1000 | Loss: 0.00001322
Iteration 133/1000 | Loss: 0.00001321
Iteration 134/1000 | Loss: 0.00001321
Iteration 135/1000 | Loss: 0.00001321
Iteration 136/1000 | Loss: 0.00001321
Iteration 137/1000 | Loss: 0.00001321
Iteration 138/1000 | Loss: 0.00001321
Iteration 139/1000 | Loss: 0.00001321
Iteration 140/1000 | Loss: 0.00001321
Iteration 141/1000 | Loss: 0.00001321
Iteration 142/1000 | Loss: 0.00001320
Iteration 143/1000 | Loss: 0.00001320
Iteration 144/1000 | Loss: 0.00001320
Iteration 145/1000 | Loss: 0.00001320
Iteration 146/1000 | Loss: 0.00001320
Iteration 147/1000 | Loss: 0.00001320
Iteration 148/1000 | Loss: 0.00001320
Iteration 149/1000 | Loss: 0.00001320
Iteration 150/1000 | Loss: 0.00001320
Iteration 151/1000 | Loss: 0.00001320
Iteration 152/1000 | Loss: 0.00001320
Iteration 153/1000 | Loss: 0.00001320
Iteration 154/1000 | Loss: 0.00001320
Iteration 155/1000 | Loss: 0.00001319
Iteration 156/1000 | Loss: 0.00001319
Iteration 157/1000 | Loss: 0.00001319
Iteration 158/1000 | Loss: 0.00001319
Iteration 159/1000 | Loss: 0.00001319
Iteration 160/1000 | Loss: 0.00001319
Iteration 161/1000 | Loss: 0.00001318
Iteration 162/1000 | Loss: 0.00001318
Iteration 163/1000 | Loss: 0.00001318
Iteration 164/1000 | Loss: 0.00001318
Iteration 165/1000 | Loss: 0.00001318
Iteration 166/1000 | Loss: 0.00001318
Iteration 167/1000 | Loss: 0.00001318
Iteration 168/1000 | Loss: 0.00001318
Iteration 169/1000 | Loss: 0.00001318
Iteration 170/1000 | Loss: 0.00001318
Iteration 171/1000 | Loss: 0.00001318
Iteration 172/1000 | Loss: 0.00001318
Iteration 173/1000 | Loss: 0.00001318
Iteration 174/1000 | Loss: 0.00001318
Iteration 175/1000 | Loss: 0.00001318
Iteration 176/1000 | Loss: 0.00001318
Iteration 177/1000 | Loss: 0.00001317
Iteration 178/1000 | Loss: 0.00001317
Iteration 179/1000 | Loss: 0.00001317
Iteration 180/1000 | Loss: 0.00001317
Iteration 181/1000 | Loss: 0.00001317
Iteration 182/1000 | Loss: 0.00001317
Iteration 183/1000 | Loss: 0.00001317
Iteration 184/1000 | Loss: 0.00001317
Iteration 185/1000 | Loss: 0.00001317
Iteration 186/1000 | Loss: 0.00001317
Iteration 187/1000 | Loss: 0.00001317
Iteration 188/1000 | Loss: 0.00001317
Iteration 189/1000 | Loss: 0.00001317
Iteration 190/1000 | Loss: 0.00001317
Iteration 191/1000 | Loss: 0.00001317
Iteration 192/1000 | Loss: 0.00001317
Iteration 193/1000 | Loss: 0.00001316
Iteration 194/1000 | Loss: 0.00001316
Iteration 195/1000 | Loss: 0.00001316
Iteration 196/1000 | Loss: 0.00001316
Iteration 197/1000 | Loss: 0.00001316
Iteration 198/1000 | Loss: 0.00001316
Iteration 199/1000 | Loss: 0.00001316
Iteration 200/1000 | Loss: 0.00001316
Iteration 201/1000 | Loss: 0.00001316
Iteration 202/1000 | Loss: 0.00001316
Iteration 203/1000 | Loss: 0.00001316
Iteration 204/1000 | Loss: 0.00001316
Iteration 205/1000 | Loss: 0.00001316
Iteration 206/1000 | Loss: 0.00001316
Iteration 207/1000 | Loss: 0.00001316
Iteration 208/1000 | Loss: 0.00001315
Iteration 209/1000 | Loss: 0.00001315
Iteration 210/1000 | Loss: 0.00001315
Iteration 211/1000 | Loss: 0.00001315
Iteration 212/1000 | Loss: 0.00001315
Iteration 213/1000 | Loss: 0.00001315
Iteration 214/1000 | Loss: 0.00001315
Iteration 215/1000 | Loss: 0.00001315
Iteration 216/1000 | Loss: 0.00001315
Iteration 217/1000 | Loss: 0.00001315
Iteration 218/1000 | Loss: 0.00001315
Iteration 219/1000 | Loss: 0.00001315
Iteration 220/1000 | Loss: 0.00001315
Iteration 221/1000 | Loss: 0.00001315
Iteration 222/1000 | Loss: 0.00001315
Iteration 223/1000 | Loss: 0.00001315
Iteration 224/1000 | Loss: 0.00001315
Iteration 225/1000 | Loss: 0.00001315
Iteration 226/1000 | Loss: 0.00001315
Iteration 227/1000 | Loss: 0.00001315
Iteration 228/1000 | Loss: 0.00001315
Iteration 229/1000 | Loss: 0.00001315
Iteration 230/1000 | Loss: 0.00001315
Iteration 231/1000 | Loss: 0.00001315
Iteration 232/1000 | Loss: 0.00001315
Iteration 233/1000 | Loss: 0.00001315
Iteration 234/1000 | Loss: 0.00001315
Iteration 235/1000 | Loss: 0.00001315
Iteration 236/1000 | Loss: 0.00001315
Iteration 237/1000 | Loss: 0.00001315
Iteration 238/1000 | Loss: 0.00001315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.3153071449778508e-05, 1.3153071449778508e-05, 1.3153071449778508e-05, 1.3153071449778508e-05, 1.3153071449778508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3153071449778508e-05

Optimization complete. Final v2v error: 2.9984405040740967 mm

Highest mean error: 3.5265183448791504 mm for frame 15

Lowest mean error: 2.423701524734497 mm for frame 109

Saving results

Total time: 45.985475301742554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449757
Iteration 2/25 | Loss: 0.00129739
Iteration 3/25 | Loss: 0.00122789
Iteration 4/25 | Loss: 0.00121866
Iteration 5/25 | Loss: 0.00121644
Iteration 6/25 | Loss: 0.00121644
Iteration 7/25 | Loss: 0.00121644
Iteration 8/25 | Loss: 0.00121644
Iteration 9/25 | Loss: 0.00121644
Iteration 10/25 | Loss: 0.00121644
Iteration 11/25 | Loss: 0.00121644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012164380168542266, 0.0012164380168542266, 0.0012164380168542266, 0.0012164380168542266, 0.0012164380168542266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012164380168542266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.19684076
Iteration 2/25 | Loss: 0.00142612
Iteration 3/25 | Loss: 0.00142611
Iteration 4/25 | Loss: 0.00142611
Iteration 5/25 | Loss: 0.00142611
Iteration 6/25 | Loss: 0.00142611
Iteration 7/25 | Loss: 0.00142611
Iteration 8/25 | Loss: 0.00142611
Iteration 9/25 | Loss: 0.00142611
Iteration 10/25 | Loss: 0.00142611
Iteration 11/25 | Loss: 0.00142611
Iteration 12/25 | Loss: 0.00142611
Iteration 13/25 | Loss: 0.00142611
Iteration 14/25 | Loss: 0.00142611
Iteration 15/25 | Loss: 0.00142611
Iteration 16/25 | Loss: 0.00142611
Iteration 17/25 | Loss: 0.00142611
Iteration 18/25 | Loss: 0.00142611
Iteration 19/25 | Loss: 0.00142611
Iteration 20/25 | Loss: 0.00142611
Iteration 21/25 | Loss: 0.00142611
Iteration 22/25 | Loss: 0.00142611
Iteration 23/25 | Loss: 0.00142611
Iteration 24/25 | Loss: 0.00142611
Iteration 25/25 | Loss: 0.00142611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142611
Iteration 2/1000 | Loss: 0.00001947
Iteration 3/1000 | Loss: 0.00001538
Iteration 4/1000 | Loss: 0.00001429
Iteration 5/1000 | Loss: 0.00001354
Iteration 6/1000 | Loss: 0.00001299
Iteration 7/1000 | Loss: 0.00001268
Iteration 8/1000 | Loss: 0.00001233
Iteration 9/1000 | Loss: 0.00001198
Iteration 10/1000 | Loss: 0.00001171
Iteration 11/1000 | Loss: 0.00001160
Iteration 12/1000 | Loss: 0.00001151
Iteration 13/1000 | Loss: 0.00001140
Iteration 14/1000 | Loss: 0.00001137
Iteration 15/1000 | Loss: 0.00001134
Iteration 16/1000 | Loss: 0.00001130
Iteration 17/1000 | Loss: 0.00001128
Iteration 18/1000 | Loss: 0.00001123
Iteration 19/1000 | Loss: 0.00001122
Iteration 20/1000 | Loss: 0.00001122
Iteration 21/1000 | Loss: 0.00001121
Iteration 22/1000 | Loss: 0.00001114
Iteration 23/1000 | Loss: 0.00001113
Iteration 24/1000 | Loss: 0.00001113
Iteration 25/1000 | Loss: 0.00001111
Iteration 26/1000 | Loss: 0.00001111
Iteration 27/1000 | Loss: 0.00001110
Iteration 28/1000 | Loss: 0.00001110
Iteration 29/1000 | Loss: 0.00001109
Iteration 30/1000 | Loss: 0.00001109
Iteration 31/1000 | Loss: 0.00001107
Iteration 32/1000 | Loss: 0.00001106
Iteration 33/1000 | Loss: 0.00001104
Iteration 34/1000 | Loss: 0.00001103
Iteration 35/1000 | Loss: 0.00001103
Iteration 36/1000 | Loss: 0.00001102
Iteration 37/1000 | Loss: 0.00001102
Iteration 38/1000 | Loss: 0.00001102
Iteration 39/1000 | Loss: 0.00001101
Iteration 40/1000 | Loss: 0.00001100
Iteration 41/1000 | Loss: 0.00001100
Iteration 42/1000 | Loss: 0.00001099
Iteration 43/1000 | Loss: 0.00001098
Iteration 44/1000 | Loss: 0.00001097
Iteration 45/1000 | Loss: 0.00001096
Iteration 46/1000 | Loss: 0.00001096
Iteration 47/1000 | Loss: 0.00001096
Iteration 48/1000 | Loss: 0.00001096
Iteration 49/1000 | Loss: 0.00001095
Iteration 50/1000 | Loss: 0.00001095
Iteration 51/1000 | Loss: 0.00001095
Iteration 52/1000 | Loss: 0.00001094
Iteration 53/1000 | Loss: 0.00001094
Iteration 54/1000 | Loss: 0.00001094
Iteration 55/1000 | Loss: 0.00001094
Iteration 56/1000 | Loss: 0.00001093
Iteration 57/1000 | Loss: 0.00001093
Iteration 58/1000 | Loss: 0.00001093
Iteration 59/1000 | Loss: 0.00001092
Iteration 60/1000 | Loss: 0.00001092
Iteration 61/1000 | Loss: 0.00001092
Iteration 62/1000 | Loss: 0.00001091
Iteration 63/1000 | Loss: 0.00001090
Iteration 64/1000 | Loss: 0.00001089
Iteration 65/1000 | Loss: 0.00001089
Iteration 66/1000 | Loss: 0.00001089
Iteration 67/1000 | Loss: 0.00001088
Iteration 68/1000 | Loss: 0.00001088
Iteration 69/1000 | Loss: 0.00001088
Iteration 70/1000 | Loss: 0.00001088
Iteration 71/1000 | Loss: 0.00001088
Iteration 72/1000 | Loss: 0.00001087
Iteration 73/1000 | Loss: 0.00001087
Iteration 74/1000 | Loss: 0.00001087
Iteration 75/1000 | Loss: 0.00001087
Iteration 76/1000 | Loss: 0.00001087
Iteration 77/1000 | Loss: 0.00001086
Iteration 78/1000 | Loss: 0.00001086
Iteration 79/1000 | Loss: 0.00001086
Iteration 80/1000 | Loss: 0.00001086
Iteration 81/1000 | Loss: 0.00001086
Iteration 82/1000 | Loss: 0.00001085
Iteration 83/1000 | Loss: 0.00001085
Iteration 84/1000 | Loss: 0.00001084
Iteration 85/1000 | Loss: 0.00001084
Iteration 86/1000 | Loss: 0.00001084
Iteration 87/1000 | Loss: 0.00001084
Iteration 88/1000 | Loss: 0.00001083
Iteration 89/1000 | Loss: 0.00001082
Iteration 90/1000 | Loss: 0.00001082
Iteration 91/1000 | Loss: 0.00001081
Iteration 92/1000 | Loss: 0.00001081
Iteration 93/1000 | Loss: 0.00001081
Iteration 94/1000 | Loss: 0.00001081
Iteration 95/1000 | Loss: 0.00001081
Iteration 96/1000 | Loss: 0.00001080
Iteration 97/1000 | Loss: 0.00001080
Iteration 98/1000 | Loss: 0.00001080
Iteration 99/1000 | Loss: 0.00001079
Iteration 100/1000 | Loss: 0.00001079
Iteration 101/1000 | Loss: 0.00001078
Iteration 102/1000 | Loss: 0.00001078
Iteration 103/1000 | Loss: 0.00001077
Iteration 104/1000 | Loss: 0.00001077
Iteration 105/1000 | Loss: 0.00001077
Iteration 106/1000 | Loss: 0.00001077
Iteration 107/1000 | Loss: 0.00001076
Iteration 108/1000 | Loss: 0.00001076
Iteration 109/1000 | Loss: 0.00001075
Iteration 110/1000 | Loss: 0.00001075
Iteration 111/1000 | Loss: 0.00001074
Iteration 112/1000 | Loss: 0.00001074
Iteration 113/1000 | Loss: 0.00001074
Iteration 114/1000 | Loss: 0.00001074
Iteration 115/1000 | Loss: 0.00001074
Iteration 116/1000 | Loss: 0.00001074
Iteration 117/1000 | Loss: 0.00001074
Iteration 118/1000 | Loss: 0.00001073
Iteration 119/1000 | Loss: 0.00001073
Iteration 120/1000 | Loss: 0.00001073
Iteration 121/1000 | Loss: 0.00001073
Iteration 122/1000 | Loss: 0.00001073
Iteration 123/1000 | Loss: 0.00001072
Iteration 124/1000 | Loss: 0.00001072
Iteration 125/1000 | Loss: 0.00001072
Iteration 126/1000 | Loss: 0.00001071
Iteration 127/1000 | Loss: 0.00001071
Iteration 128/1000 | Loss: 0.00001070
Iteration 129/1000 | Loss: 0.00001070
Iteration 130/1000 | Loss: 0.00001070
Iteration 131/1000 | Loss: 0.00001070
Iteration 132/1000 | Loss: 0.00001070
Iteration 133/1000 | Loss: 0.00001069
Iteration 134/1000 | Loss: 0.00001069
Iteration 135/1000 | Loss: 0.00001068
Iteration 136/1000 | Loss: 0.00001068
Iteration 137/1000 | Loss: 0.00001068
Iteration 138/1000 | Loss: 0.00001068
Iteration 139/1000 | Loss: 0.00001068
Iteration 140/1000 | Loss: 0.00001068
Iteration 141/1000 | Loss: 0.00001068
Iteration 142/1000 | Loss: 0.00001068
Iteration 143/1000 | Loss: 0.00001068
Iteration 144/1000 | Loss: 0.00001068
Iteration 145/1000 | Loss: 0.00001068
Iteration 146/1000 | Loss: 0.00001068
Iteration 147/1000 | Loss: 0.00001068
Iteration 148/1000 | Loss: 0.00001067
Iteration 149/1000 | Loss: 0.00001067
Iteration 150/1000 | Loss: 0.00001067
Iteration 151/1000 | Loss: 0.00001067
Iteration 152/1000 | Loss: 0.00001067
Iteration 153/1000 | Loss: 0.00001067
Iteration 154/1000 | Loss: 0.00001067
Iteration 155/1000 | Loss: 0.00001067
Iteration 156/1000 | Loss: 0.00001067
Iteration 157/1000 | Loss: 0.00001067
Iteration 158/1000 | Loss: 0.00001067
Iteration 159/1000 | Loss: 0.00001067
Iteration 160/1000 | Loss: 0.00001067
Iteration 161/1000 | Loss: 0.00001067
Iteration 162/1000 | Loss: 0.00001067
Iteration 163/1000 | Loss: 0.00001067
Iteration 164/1000 | Loss: 0.00001067
Iteration 165/1000 | Loss: 0.00001067
Iteration 166/1000 | Loss: 0.00001066
Iteration 167/1000 | Loss: 0.00001066
Iteration 168/1000 | Loss: 0.00001066
Iteration 169/1000 | Loss: 0.00001066
Iteration 170/1000 | Loss: 0.00001066
Iteration 171/1000 | Loss: 0.00001066
Iteration 172/1000 | Loss: 0.00001066
Iteration 173/1000 | Loss: 0.00001066
Iteration 174/1000 | Loss: 0.00001066
Iteration 175/1000 | Loss: 0.00001066
Iteration 176/1000 | Loss: 0.00001066
Iteration 177/1000 | Loss: 0.00001066
Iteration 178/1000 | Loss: 0.00001066
Iteration 179/1000 | Loss: 0.00001066
Iteration 180/1000 | Loss: 0.00001066
Iteration 181/1000 | Loss: 0.00001066
Iteration 182/1000 | Loss: 0.00001066
Iteration 183/1000 | Loss: 0.00001066
Iteration 184/1000 | Loss: 0.00001066
Iteration 185/1000 | Loss: 0.00001066
Iteration 186/1000 | Loss: 0.00001066
Iteration 187/1000 | Loss: 0.00001066
Iteration 188/1000 | Loss: 0.00001066
Iteration 189/1000 | Loss: 0.00001066
Iteration 190/1000 | Loss: 0.00001066
Iteration 191/1000 | Loss: 0.00001066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.0664065484888852e-05, 1.0664065484888852e-05, 1.0664065484888852e-05, 1.0664065484888852e-05, 1.0664065484888852e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0664065484888852e-05

Optimization complete. Final v2v error: 2.814974308013916 mm

Highest mean error: 3.122032403945923 mm for frame 229

Lowest mean error: 2.633312940597534 mm for frame 51

Saving results

Total time: 45.86392140388489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00555333
Iteration 2/25 | Loss: 0.00145144
Iteration 3/25 | Loss: 0.00135071
Iteration 4/25 | Loss: 0.00134042
Iteration 5/25 | Loss: 0.00133851
Iteration 6/25 | Loss: 0.00133851
Iteration 7/25 | Loss: 0.00133851
Iteration 8/25 | Loss: 0.00133851
Iteration 9/25 | Loss: 0.00133851
Iteration 10/25 | Loss: 0.00133851
Iteration 11/25 | Loss: 0.00133851
Iteration 12/25 | Loss: 0.00133851
Iteration 13/25 | Loss: 0.00133851
Iteration 14/25 | Loss: 0.00133851
Iteration 15/25 | Loss: 0.00133851
Iteration 16/25 | Loss: 0.00133851
Iteration 17/25 | Loss: 0.00133851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013385056518018246, 0.0013385056518018246, 0.0013385056518018246, 0.0013385056518018246, 0.0013385056518018246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013385056518018246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.32768559
Iteration 2/25 | Loss: 0.00150390
Iteration 3/25 | Loss: 0.00150388
Iteration 4/25 | Loss: 0.00150388
Iteration 5/25 | Loss: 0.00150388
Iteration 6/25 | Loss: 0.00150388
Iteration 7/25 | Loss: 0.00150388
Iteration 8/25 | Loss: 0.00150388
Iteration 9/25 | Loss: 0.00150388
Iteration 10/25 | Loss: 0.00150388
Iteration 11/25 | Loss: 0.00150388
Iteration 12/25 | Loss: 0.00150388
Iteration 13/25 | Loss: 0.00150388
Iteration 14/25 | Loss: 0.00150388
Iteration 15/25 | Loss: 0.00150388
Iteration 16/25 | Loss: 0.00150388
Iteration 17/25 | Loss: 0.00150388
Iteration 18/25 | Loss: 0.00150388
Iteration 19/25 | Loss: 0.00150388
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001503875246271491, 0.001503875246271491, 0.001503875246271491, 0.001503875246271491, 0.001503875246271491]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001503875246271491

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150388
Iteration 2/1000 | Loss: 0.00003805
Iteration 3/1000 | Loss: 0.00002786
Iteration 4/1000 | Loss: 0.00002510
Iteration 5/1000 | Loss: 0.00002408
Iteration 6/1000 | Loss: 0.00002339
Iteration 7/1000 | Loss: 0.00002298
Iteration 8/1000 | Loss: 0.00002255
Iteration 9/1000 | Loss: 0.00002227
Iteration 10/1000 | Loss: 0.00002202
Iteration 11/1000 | Loss: 0.00002202
Iteration 12/1000 | Loss: 0.00002199
Iteration 13/1000 | Loss: 0.00002192
Iteration 14/1000 | Loss: 0.00002190
Iteration 15/1000 | Loss: 0.00002188
Iteration 16/1000 | Loss: 0.00002175
Iteration 17/1000 | Loss: 0.00002155
Iteration 18/1000 | Loss: 0.00002134
Iteration 19/1000 | Loss: 0.00002117
Iteration 20/1000 | Loss: 0.00002117
Iteration 21/1000 | Loss: 0.00002116
Iteration 22/1000 | Loss: 0.00002110
Iteration 23/1000 | Loss: 0.00002104
Iteration 24/1000 | Loss: 0.00002102
Iteration 25/1000 | Loss: 0.00002101
Iteration 26/1000 | Loss: 0.00002098
Iteration 27/1000 | Loss: 0.00002097
Iteration 28/1000 | Loss: 0.00002096
Iteration 29/1000 | Loss: 0.00002096
Iteration 30/1000 | Loss: 0.00002095
Iteration 31/1000 | Loss: 0.00002095
Iteration 32/1000 | Loss: 0.00002095
Iteration 33/1000 | Loss: 0.00002091
Iteration 34/1000 | Loss: 0.00002090
Iteration 35/1000 | Loss: 0.00002088
Iteration 36/1000 | Loss: 0.00002087
Iteration 37/1000 | Loss: 0.00002086
Iteration 38/1000 | Loss: 0.00002083
Iteration 39/1000 | Loss: 0.00002083
Iteration 40/1000 | Loss: 0.00002083
Iteration 41/1000 | Loss: 0.00002082
Iteration 42/1000 | Loss: 0.00002082
Iteration 43/1000 | Loss: 0.00002082
Iteration 44/1000 | Loss: 0.00002082
Iteration 45/1000 | Loss: 0.00002076
Iteration 46/1000 | Loss: 0.00002076
Iteration 47/1000 | Loss: 0.00002072
Iteration 48/1000 | Loss: 0.00002070
Iteration 49/1000 | Loss: 0.00002069
Iteration 50/1000 | Loss: 0.00002069
Iteration 51/1000 | Loss: 0.00002068
Iteration 52/1000 | Loss: 0.00002064
Iteration 53/1000 | Loss: 0.00002063
Iteration 54/1000 | Loss: 0.00002063
Iteration 55/1000 | Loss: 0.00002062
Iteration 56/1000 | Loss: 0.00002062
Iteration 57/1000 | Loss: 0.00002062
Iteration 58/1000 | Loss: 0.00002061
Iteration 59/1000 | Loss: 0.00002061
Iteration 60/1000 | Loss: 0.00002061
Iteration 61/1000 | Loss: 0.00002060
Iteration 62/1000 | Loss: 0.00002060
Iteration 63/1000 | Loss: 0.00002060
Iteration 64/1000 | Loss: 0.00002060
Iteration 65/1000 | Loss: 0.00002059
Iteration 66/1000 | Loss: 0.00002059
Iteration 67/1000 | Loss: 0.00002059
Iteration 68/1000 | Loss: 0.00002059
Iteration 69/1000 | Loss: 0.00002059
Iteration 70/1000 | Loss: 0.00002059
Iteration 71/1000 | Loss: 0.00002059
Iteration 72/1000 | Loss: 0.00002059
Iteration 73/1000 | Loss: 0.00002059
Iteration 74/1000 | Loss: 0.00002059
Iteration 75/1000 | Loss: 0.00002059
Iteration 76/1000 | Loss: 0.00002059
Iteration 77/1000 | Loss: 0.00002059
Iteration 78/1000 | Loss: 0.00002059
Iteration 79/1000 | Loss: 0.00002058
Iteration 80/1000 | Loss: 0.00002058
Iteration 81/1000 | Loss: 0.00002058
Iteration 82/1000 | Loss: 0.00002057
Iteration 83/1000 | Loss: 0.00002057
Iteration 84/1000 | Loss: 0.00002057
Iteration 85/1000 | Loss: 0.00002056
Iteration 86/1000 | Loss: 0.00002056
Iteration 87/1000 | Loss: 0.00002056
Iteration 88/1000 | Loss: 0.00002055
Iteration 89/1000 | Loss: 0.00002055
Iteration 90/1000 | Loss: 0.00002055
Iteration 91/1000 | Loss: 0.00002055
Iteration 92/1000 | Loss: 0.00002055
Iteration 93/1000 | Loss: 0.00002055
Iteration 94/1000 | Loss: 0.00002055
Iteration 95/1000 | Loss: 0.00002054
Iteration 96/1000 | Loss: 0.00002054
Iteration 97/1000 | Loss: 0.00002054
Iteration 98/1000 | Loss: 0.00002053
Iteration 99/1000 | Loss: 0.00002053
Iteration 100/1000 | Loss: 0.00002052
Iteration 101/1000 | Loss: 0.00002052
Iteration 102/1000 | Loss: 0.00002052
Iteration 103/1000 | Loss: 0.00002052
Iteration 104/1000 | Loss: 0.00002051
Iteration 105/1000 | Loss: 0.00002051
Iteration 106/1000 | Loss: 0.00002051
Iteration 107/1000 | Loss: 0.00002051
Iteration 108/1000 | Loss: 0.00002051
Iteration 109/1000 | Loss: 0.00002050
Iteration 110/1000 | Loss: 0.00002050
Iteration 111/1000 | Loss: 0.00002050
Iteration 112/1000 | Loss: 0.00002050
Iteration 113/1000 | Loss: 0.00002050
Iteration 114/1000 | Loss: 0.00002050
Iteration 115/1000 | Loss: 0.00002050
Iteration 116/1000 | Loss: 0.00002050
Iteration 117/1000 | Loss: 0.00002050
Iteration 118/1000 | Loss: 0.00002050
Iteration 119/1000 | Loss: 0.00002050
Iteration 120/1000 | Loss: 0.00002050
Iteration 121/1000 | Loss: 0.00002050
Iteration 122/1000 | Loss: 0.00002050
Iteration 123/1000 | Loss: 0.00002050
Iteration 124/1000 | Loss: 0.00002050
Iteration 125/1000 | Loss: 0.00002050
Iteration 126/1000 | Loss: 0.00002050
Iteration 127/1000 | Loss: 0.00002050
Iteration 128/1000 | Loss: 0.00002050
Iteration 129/1000 | Loss: 0.00002050
Iteration 130/1000 | Loss: 0.00002050
Iteration 131/1000 | Loss: 0.00002050
Iteration 132/1000 | Loss: 0.00002050
Iteration 133/1000 | Loss: 0.00002050
Iteration 134/1000 | Loss: 0.00002050
Iteration 135/1000 | Loss: 0.00002050
Iteration 136/1000 | Loss: 0.00002050
Iteration 137/1000 | Loss: 0.00002050
Iteration 138/1000 | Loss: 0.00002050
Iteration 139/1000 | Loss: 0.00002050
Iteration 140/1000 | Loss: 0.00002050
Iteration 141/1000 | Loss: 0.00002050
Iteration 142/1000 | Loss: 0.00002050
Iteration 143/1000 | Loss: 0.00002050
Iteration 144/1000 | Loss: 0.00002050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.0499621314229444e-05, 2.0499621314229444e-05, 2.0499621314229444e-05, 2.0499621314229444e-05, 2.0499621314229444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0499621314229444e-05

Optimization complete. Final v2v error: 3.7423527240753174 mm

Highest mean error: 3.9570024013519287 mm for frame 83

Lowest mean error: 3.4462335109710693 mm for frame 15

Saving results

Total time: 47.345627784729004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00529974
Iteration 2/25 | Loss: 0.00128258
Iteration 3/25 | Loss: 0.00122003
Iteration 4/25 | Loss: 0.00121039
Iteration 5/25 | Loss: 0.00120756
Iteration 6/25 | Loss: 0.00120731
Iteration 7/25 | Loss: 0.00120731
Iteration 8/25 | Loss: 0.00120731
Iteration 9/25 | Loss: 0.00120731
Iteration 10/25 | Loss: 0.00120731
Iteration 11/25 | Loss: 0.00120731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012073065154254436, 0.0012073065154254436, 0.0012073065154254436, 0.0012073065154254436, 0.0012073065154254436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012073065154254436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.55501556
Iteration 2/25 | Loss: 0.00136734
Iteration 3/25 | Loss: 0.00136732
Iteration 4/25 | Loss: 0.00136732
Iteration 5/25 | Loss: 0.00136732
Iteration 6/25 | Loss: 0.00136732
Iteration 7/25 | Loss: 0.00136732
Iteration 8/25 | Loss: 0.00136732
Iteration 9/25 | Loss: 0.00136732
Iteration 10/25 | Loss: 0.00136732
Iteration 11/25 | Loss: 0.00136732
Iteration 12/25 | Loss: 0.00136732
Iteration 13/25 | Loss: 0.00136732
Iteration 14/25 | Loss: 0.00136732
Iteration 15/25 | Loss: 0.00136732
Iteration 16/25 | Loss: 0.00136732
Iteration 17/25 | Loss: 0.00136732
Iteration 18/25 | Loss: 0.00136732
Iteration 19/25 | Loss: 0.00136732
Iteration 20/25 | Loss: 0.00136732
Iteration 21/25 | Loss: 0.00136732
Iteration 22/25 | Loss: 0.00136732
Iteration 23/25 | Loss: 0.00136732
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013673183275386691, 0.0013673183275386691, 0.0013673183275386691, 0.0013673183275386691, 0.0013673183275386691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013673183275386691

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136732
Iteration 2/1000 | Loss: 0.00002562
Iteration 3/1000 | Loss: 0.00001900
Iteration 4/1000 | Loss: 0.00001574
Iteration 5/1000 | Loss: 0.00001486
Iteration 6/1000 | Loss: 0.00001421
Iteration 7/1000 | Loss: 0.00001363
Iteration 8/1000 | Loss: 0.00001326
Iteration 9/1000 | Loss: 0.00001286
Iteration 10/1000 | Loss: 0.00001250
Iteration 11/1000 | Loss: 0.00001229
Iteration 12/1000 | Loss: 0.00001213
Iteration 13/1000 | Loss: 0.00001212
Iteration 14/1000 | Loss: 0.00001208
Iteration 15/1000 | Loss: 0.00001200
Iteration 16/1000 | Loss: 0.00001191
Iteration 17/1000 | Loss: 0.00001187
Iteration 18/1000 | Loss: 0.00001187
Iteration 19/1000 | Loss: 0.00001187
Iteration 20/1000 | Loss: 0.00001186
Iteration 21/1000 | Loss: 0.00001184
Iteration 22/1000 | Loss: 0.00001176
Iteration 23/1000 | Loss: 0.00001173
Iteration 24/1000 | Loss: 0.00001168
Iteration 25/1000 | Loss: 0.00001167
Iteration 26/1000 | Loss: 0.00001165
Iteration 27/1000 | Loss: 0.00001163
Iteration 28/1000 | Loss: 0.00001161
Iteration 29/1000 | Loss: 0.00001160
Iteration 30/1000 | Loss: 0.00001159
Iteration 31/1000 | Loss: 0.00001158
Iteration 32/1000 | Loss: 0.00001155
Iteration 33/1000 | Loss: 0.00001155
Iteration 34/1000 | Loss: 0.00001154
Iteration 35/1000 | Loss: 0.00001154
Iteration 36/1000 | Loss: 0.00001154
Iteration 37/1000 | Loss: 0.00001151
Iteration 38/1000 | Loss: 0.00001149
Iteration 39/1000 | Loss: 0.00001149
Iteration 40/1000 | Loss: 0.00001146
Iteration 41/1000 | Loss: 0.00001145
Iteration 42/1000 | Loss: 0.00001145
Iteration 43/1000 | Loss: 0.00001145
Iteration 44/1000 | Loss: 0.00001144
Iteration 45/1000 | Loss: 0.00001143
Iteration 46/1000 | Loss: 0.00001142
Iteration 47/1000 | Loss: 0.00001142
Iteration 48/1000 | Loss: 0.00001141
Iteration 49/1000 | Loss: 0.00001141
Iteration 50/1000 | Loss: 0.00001140
Iteration 51/1000 | Loss: 0.00001140
Iteration 52/1000 | Loss: 0.00001140
Iteration 53/1000 | Loss: 0.00001140
Iteration 54/1000 | Loss: 0.00001140
Iteration 55/1000 | Loss: 0.00001140
Iteration 56/1000 | Loss: 0.00001139
Iteration 57/1000 | Loss: 0.00001139
Iteration 58/1000 | Loss: 0.00001139
Iteration 59/1000 | Loss: 0.00001138
Iteration 60/1000 | Loss: 0.00001138
Iteration 61/1000 | Loss: 0.00001138
Iteration 62/1000 | Loss: 0.00001138
Iteration 63/1000 | Loss: 0.00001138
Iteration 64/1000 | Loss: 0.00001137
Iteration 65/1000 | Loss: 0.00001137
Iteration 66/1000 | Loss: 0.00001137
Iteration 67/1000 | Loss: 0.00001136
Iteration 68/1000 | Loss: 0.00001136
Iteration 69/1000 | Loss: 0.00001136
Iteration 70/1000 | Loss: 0.00001136
Iteration 71/1000 | Loss: 0.00001135
Iteration 72/1000 | Loss: 0.00001135
Iteration 73/1000 | Loss: 0.00001135
Iteration 74/1000 | Loss: 0.00001134
Iteration 75/1000 | Loss: 0.00001134
Iteration 76/1000 | Loss: 0.00001134
Iteration 77/1000 | Loss: 0.00001133
Iteration 78/1000 | Loss: 0.00001133
Iteration 79/1000 | Loss: 0.00001132
Iteration 80/1000 | Loss: 0.00001132
Iteration 81/1000 | Loss: 0.00001132
Iteration 82/1000 | Loss: 0.00001132
Iteration 83/1000 | Loss: 0.00001132
Iteration 84/1000 | Loss: 0.00001132
Iteration 85/1000 | Loss: 0.00001132
Iteration 86/1000 | Loss: 0.00001132
Iteration 87/1000 | Loss: 0.00001131
Iteration 88/1000 | Loss: 0.00001131
Iteration 89/1000 | Loss: 0.00001131
Iteration 90/1000 | Loss: 0.00001131
Iteration 91/1000 | Loss: 0.00001131
Iteration 92/1000 | Loss: 0.00001131
Iteration 93/1000 | Loss: 0.00001131
Iteration 94/1000 | Loss: 0.00001131
Iteration 95/1000 | Loss: 0.00001130
Iteration 96/1000 | Loss: 0.00001130
Iteration 97/1000 | Loss: 0.00001130
Iteration 98/1000 | Loss: 0.00001130
Iteration 99/1000 | Loss: 0.00001130
Iteration 100/1000 | Loss: 0.00001129
Iteration 101/1000 | Loss: 0.00001129
Iteration 102/1000 | Loss: 0.00001129
Iteration 103/1000 | Loss: 0.00001129
Iteration 104/1000 | Loss: 0.00001129
Iteration 105/1000 | Loss: 0.00001129
Iteration 106/1000 | Loss: 0.00001129
Iteration 107/1000 | Loss: 0.00001129
Iteration 108/1000 | Loss: 0.00001129
Iteration 109/1000 | Loss: 0.00001129
Iteration 110/1000 | Loss: 0.00001128
Iteration 111/1000 | Loss: 0.00001128
Iteration 112/1000 | Loss: 0.00001128
Iteration 113/1000 | Loss: 0.00001128
Iteration 114/1000 | Loss: 0.00001128
Iteration 115/1000 | Loss: 0.00001128
Iteration 116/1000 | Loss: 0.00001127
Iteration 117/1000 | Loss: 0.00001127
Iteration 118/1000 | Loss: 0.00001127
Iteration 119/1000 | Loss: 0.00001127
Iteration 120/1000 | Loss: 0.00001126
Iteration 121/1000 | Loss: 0.00001126
Iteration 122/1000 | Loss: 0.00001126
Iteration 123/1000 | Loss: 0.00001126
Iteration 124/1000 | Loss: 0.00001126
Iteration 125/1000 | Loss: 0.00001126
Iteration 126/1000 | Loss: 0.00001125
Iteration 127/1000 | Loss: 0.00001125
Iteration 128/1000 | Loss: 0.00001125
Iteration 129/1000 | Loss: 0.00001125
Iteration 130/1000 | Loss: 0.00001124
Iteration 131/1000 | Loss: 0.00001124
Iteration 132/1000 | Loss: 0.00001124
Iteration 133/1000 | Loss: 0.00001124
Iteration 134/1000 | Loss: 0.00001124
Iteration 135/1000 | Loss: 0.00001124
Iteration 136/1000 | Loss: 0.00001124
Iteration 137/1000 | Loss: 0.00001124
Iteration 138/1000 | Loss: 0.00001124
Iteration 139/1000 | Loss: 0.00001124
Iteration 140/1000 | Loss: 0.00001123
Iteration 141/1000 | Loss: 0.00001123
Iteration 142/1000 | Loss: 0.00001123
Iteration 143/1000 | Loss: 0.00001123
Iteration 144/1000 | Loss: 0.00001123
Iteration 145/1000 | Loss: 0.00001123
Iteration 146/1000 | Loss: 0.00001123
Iteration 147/1000 | Loss: 0.00001123
Iteration 148/1000 | Loss: 0.00001122
Iteration 149/1000 | Loss: 0.00001122
Iteration 150/1000 | Loss: 0.00001122
Iteration 151/1000 | Loss: 0.00001122
Iteration 152/1000 | Loss: 0.00001122
Iteration 153/1000 | Loss: 0.00001122
Iteration 154/1000 | Loss: 0.00001122
Iteration 155/1000 | Loss: 0.00001122
Iteration 156/1000 | Loss: 0.00001122
Iteration 157/1000 | Loss: 0.00001122
Iteration 158/1000 | Loss: 0.00001122
Iteration 159/1000 | Loss: 0.00001122
Iteration 160/1000 | Loss: 0.00001122
Iteration 161/1000 | Loss: 0.00001122
Iteration 162/1000 | Loss: 0.00001122
Iteration 163/1000 | Loss: 0.00001122
Iteration 164/1000 | Loss: 0.00001122
Iteration 165/1000 | Loss: 0.00001122
Iteration 166/1000 | Loss: 0.00001122
Iteration 167/1000 | Loss: 0.00001122
Iteration 168/1000 | Loss: 0.00001122
Iteration 169/1000 | Loss: 0.00001122
Iteration 170/1000 | Loss: 0.00001122
Iteration 171/1000 | Loss: 0.00001122
Iteration 172/1000 | Loss: 0.00001122
Iteration 173/1000 | Loss: 0.00001122
Iteration 174/1000 | Loss: 0.00001122
Iteration 175/1000 | Loss: 0.00001122
Iteration 176/1000 | Loss: 0.00001122
Iteration 177/1000 | Loss: 0.00001122
Iteration 178/1000 | Loss: 0.00001122
Iteration 179/1000 | Loss: 0.00001122
Iteration 180/1000 | Loss: 0.00001122
Iteration 181/1000 | Loss: 0.00001122
Iteration 182/1000 | Loss: 0.00001122
Iteration 183/1000 | Loss: 0.00001122
Iteration 184/1000 | Loss: 0.00001122
Iteration 185/1000 | Loss: 0.00001122
Iteration 186/1000 | Loss: 0.00001122
Iteration 187/1000 | Loss: 0.00001122
Iteration 188/1000 | Loss: 0.00001122
Iteration 189/1000 | Loss: 0.00001122
Iteration 190/1000 | Loss: 0.00001122
Iteration 191/1000 | Loss: 0.00001122
Iteration 192/1000 | Loss: 0.00001122
Iteration 193/1000 | Loss: 0.00001122
Iteration 194/1000 | Loss: 0.00001122
Iteration 195/1000 | Loss: 0.00001122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.1216935490665492e-05, 1.1216935490665492e-05, 1.1216935490665492e-05, 1.1216935490665492e-05, 1.1216935490665492e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1216935490665492e-05

Optimization complete. Final v2v error: 2.8788630962371826 mm

Highest mean error: 3.5619876384735107 mm for frame 66

Lowest mean error: 2.6125643253326416 mm for frame 168

Saving results

Total time: 43.84208679199219
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00740573
Iteration 2/25 | Loss: 0.00158883
Iteration 3/25 | Loss: 0.00132437
Iteration 4/25 | Loss: 0.00127851
Iteration 5/25 | Loss: 0.00127301
Iteration 6/25 | Loss: 0.00126907
Iteration 7/25 | Loss: 0.00126547
Iteration 8/25 | Loss: 0.00126369
Iteration 9/25 | Loss: 0.00126224
Iteration 10/25 | Loss: 0.00126532
Iteration 11/25 | Loss: 0.00126394
Iteration 12/25 | Loss: 0.00126352
Iteration 13/25 | Loss: 0.00126231
Iteration 14/25 | Loss: 0.00126347
Iteration 15/25 | Loss: 0.00126119
Iteration 16/25 | Loss: 0.00125806
Iteration 17/25 | Loss: 0.00125749
Iteration 18/25 | Loss: 0.00125728
Iteration 19/25 | Loss: 0.00125727
Iteration 20/25 | Loss: 0.00125726
Iteration 21/25 | Loss: 0.00125726
Iteration 22/25 | Loss: 0.00125726
Iteration 23/25 | Loss: 0.00125726
Iteration 24/25 | Loss: 0.00125726
Iteration 25/25 | Loss: 0.00125726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.59067631
Iteration 2/25 | Loss: 0.00160237
Iteration 3/25 | Loss: 0.00160209
Iteration 4/25 | Loss: 0.00160209
Iteration 5/25 | Loss: 0.00160209
Iteration 6/25 | Loss: 0.00160209
Iteration 7/25 | Loss: 0.00160209
Iteration 8/25 | Loss: 0.00160209
Iteration 9/25 | Loss: 0.00160209
Iteration 10/25 | Loss: 0.00160209
Iteration 11/25 | Loss: 0.00160209
Iteration 12/25 | Loss: 0.00160209
Iteration 13/25 | Loss: 0.00160209
Iteration 14/25 | Loss: 0.00160209
Iteration 15/25 | Loss: 0.00160209
Iteration 16/25 | Loss: 0.00160209
Iteration 17/25 | Loss: 0.00160209
Iteration 18/25 | Loss: 0.00160209
Iteration 19/25 | Loss: 0.00160209
Iteration 20/25 | Loss: 0.00160209
Iteration 21/25 | Loss: 0.00160209
Iteration 22/25 | Loss: 0.00160209
Iteration 23/25 | Loss: 0.00160209
Iteration 24/25 | Loss: 0.00160209
Iteration 25/25 | Loss: 0.00160209
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0016020863549783826, 0.0016020863549783826, 0.0016020863549783826, 0.0016020863549783826, 0.0016020863549783826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016020863549783826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160209
Iteration 2/1000 | Loss: 0.00012946
Iteration 3/1000 | Loss: 0.00004039
Iteration 4/1000 | Loss: 0.00002962
Iteration 5/1000 | Loss: 0.00002564
Iteration 6/1000 | Loss: 0.00011394
Iteration 7/1000 | Loss: 0.00003059
Iteration 8/1000 | Loss: 0.00007367
Iteration 9/1000 | Loss: 0.00004962
Iteration 10/1000 | Loss: 0.00002640
Iteration 11/1000 | Loss: 0.00006235
Iteration 12/1000 | Loss: 0.00002532
Iteration 13/1000 | Loss: 0.00011352
Iteration 14/1000 | Loss: 0.00003828
Iteration 15/1000 | Loss: 0.00002526
Iteration 16/1000 | Loss: 0.00002212
Iteration 17/1000 | Loss: 0.00002119
Iteration 18/1000 | Loss: 0.00002057
Iteration 19/1000 | Loss: 0.00002004
Iteration 20/1000 | Loss: 0.00001974
Iteration 21/1000 | Loss: 0.00001931
Iteration 22/1000 | Loss: 0.00001897
Iteration 23/1000 | Loss: 0.00001866
Iteration 24/1000 | Loss: 0.00001847
Iteration 25/1000 | Loss: 0.00001839
Iteration 26/1000 | Loss: 0.00001827
Iteration 27/1000 | Loss: 0.00001812
Iteration 28/1000 | Loss: 0.00001789
Iteration 29/1000 | Loss: 0.00001775
Iteration 30/1000 | Loss: 0.00001761
Iteration 31/1000 | Loss: 0.00001744
Iteration 32/1000 | Loss: 0.00001744
Iteration 33/1000 | Loss: 0.00001741
Iteration 34/1000 | Loss: 0.00001741
Iteration 35/1000 | Loss: 0.00001740
Iteration 36/1000 | Loss: 0.00001736
Iteration 37/1000 | Loss: 0.00001735
Iteration 38/1000 | Loss: 0.00001734
Iteration 39/1000 | Loss: 0.00001731
Iteration 40/1000 | Loss: 0.00001729
Iteration 41/1000 | Loss: 0.00001726
Iteration 42/1000 | Loss: 0.00001725
Iteration 43/1000 | Loss: 0.00001725
Iteration 44/1000 | Loss: 0.00001724
Iteration 45/1000 | Loss: 0.00001724
Iteration 46/1000 | Loss: 0.00001724
Iteration 47/1000 | Loss: 0.00001723
Iteration 48/1000 | Loss: 0.00001723
Iteration 49/1000 | Loss: 0.00001723
Iteration 50/1000 | Loss: 0.00001723
Iteration 51/1000 | Loss: 0.00001722
Iteration 52/1000 | Loss: 0.00001722
Iteration 53/1000 | Loss: 0.00001722
Iteration 54/1000 | Loss: 0.00001722
Iteration 55/1000 | Loss: 0.00001722
Iteration 56/1000 | Loss: 0.00001721
Iteration 57/1000 | Loss: 0.00001721
Iteration 58/1000 | Loss: 0.00001721
Iteration 59/1000 | Loss: 0.00001721
Iteration 60/1000 | Loss: 0.00001721
Iteration 61/1000 | Loss: 0.00001721
Iteration 62/1000 | Loss: 0.00001721
Iteration 63/1000 | Loss: 0.00001721
Iteration 64/1000 | Loss: 0.00001721
Iteration 65/1000 | Loss: 0.00001720
Iteration 66/1000 | Loss: 0.00001720
Iteration 67/1000 | Loss: 0.00001720
Iteration 68/1000 | Loss: 0.00001720
Iteration 69/1000 | Loss: 0.00001720
Iteration 70/1000 | Loss: 0.00001720
Iteration 71/1000 | Loss: 0.00001720
Iteration 72/1000 | Loss: 0.00001720
Iteration 73/1000 | Loss: 0.00001720
Iteration 74/1000 | Loss: 0.00001720
Iteration 75/1000 | Loss: 0.00001720
Iteration 76/1000 | Loss: 0.00001720
Iteration 77/1000 | Loss: 0.00001720
Iteration 78/1000 | Loss: 0.00001720
Iteration 79/1000 | Loss: 0.00001719
Iteration 80/1000 | Loss: 0.00001719
Iteration 81/1000 | Loss: 0.00001719
Iteration 82/1000 | Loss: 0.00001719
Iteration 83/1000 | Loss: 0.00001719
Iteration 84/1000 | Loss: 0.00001719
Iteration 85/1000 | Loss: 0.00001719
Iteration 86/1000 | Loss: 0.00001719
Iteration 87/1000 | Loss: 0.00001719
Iteration 88/1000 | Loss: 0.00001719
Iteration 89/1000 | Loss: 0.00001719
Iteration 90/1000 | Loss: 0.00001719
Iteration 91/1000 | Loss: 0.00001719
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.7192112863995135e-05, 1.7192112863995135e-05, 1.7192112863995135e-05, 1.7192112863995135e-05, 1.7192112863995135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7192112863995135e-05

Optimization complete. Final v2v error: 3.4548094272613525 mm

Highest mean error: 5.464260578155518 mm for frame 149

Lowest mean error: 2.6460084915161133 mm for frame 94

Saving results

Total time: 90.73492956161499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900806
Iteration 2/25 | Loss: 0.00155259
Iteration 3/25 | Loss: 0.00128484
Iteration 4/25 | Loss: 0.00126093
Iteration 5/25 | Loss: 0.00125625
Iteration 6/25 | Loss: 0.00125570
Iteration 7/25 | Loss: 0.00125570
Iteration 8/25 | Loss: 0.00125570
Iteration 9/25 | Loss: 0.00125570
Iteration 10/25 | Loss: 0.00125570
Iteration 11/25 | Loss: 0.00125570
Iteration 12/25 | Loss: 0.00125570
Iteration 13/25 | Loss: 0.00125570
Iteration 14/25 | Loss: 0.00125570
Iteration 15/25 | Loss: 0.00125570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012556983856484294, 0.0012556983856484294, 0.0012556983856484294, 0.0012556983856484294, 0.0012556983856484294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012556983856484294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78056329
Iteration 2/25 | Loss: 0.00157937
Iteration 3/25 | Loss: 0.00157937
Iteration 4/25 | Loss: 0.00157937
Iteration 5/25 | Loss: 0.00157937
Iteration 6/25 | Loss: 0.00157937
Iteration 7/25 | Loss: 0.00157937
Iteration 8/25 | Loss: 0.00157937
Iteration 9/25 | Loss: 0.00157937
Iteration 10/25 | Loss: 0.00157937
Iteration 11/25 | Loss: 0.00157937
Iteration 12/25 | Loss: 0.00157937
Iteration 13/25 | Loss: 0.00157937
Iteration 14/25 | Loss: 0.00157937
Iteration 15/25 | Loss: 0.00157937
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0015793672064319253, 0.0015793672064319253, 0.0015793672064319253, 0.0015793672064319253, 0.0015793672064319253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015793672064319253

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157937
Iteration 2/1000 | Loss: 0.00005146
Iteration 3/1000 | Loss: 0.00003927
Iteration 4/1000 | Loss: 0.00003397
Iteration 5/1000 | Loss: 0.00003111
Iteration 6/1000 | Loss: 0.00002978
Iteration 7/1000 | Loss: 0.00002898
Iteration 8/1000 | Loss: 0.00002863
Iteration 9/1000 | Loss: 0.00002838
Iteration 10/1000 | Loss: 0.00002820
Iteration 11/1000 | Loss: 0.00002807
Iteration 12/1000 | Loss: 0.00002803
Iteration 13/1000 | Loss: 0.00002796
Iteration 14/1000 | Loss: 0.00002781
Iteration 15/1000 | Loss: 0.00002781
Iteration 16/1000 | Loss: 0.00002776
Iteration 17/1000 | Loss: 0.00002775
Iteration 18/1000 | Loss: 0.00002774
Iteration 19/1000 | Loss: 0.00002768
Iteration 20/1000 | Loss: 0.00002768
Iteration 21/1000 | Loss: 0.00002768
Iteration 22/1000 | Loss: 0.00002768
Iteration 23/1000 | Loss: 0.00002768
Iteration 24/1000 | Loss: 0.00002768
Iteration 25/1000 | Loss: 0.00002768
Iteration 26/1000 | Loss: 0.00002767
Iteration 27/1000 | Loss: 0.00002767
Iteration 28/1000 | Loss: 0.00002766
Iteration 29/1000 | Loss: 0.00002766
Iteration 30/1000 | Loss: 0.00002766
Iteration 31/1000 | Loss: 0.00002766
Iteration 32/1000 | Loss: 0.00002766
Iteration 33/1000 | Loss: 0.00002765
Iteration 34/1000 | Loss: 0.00002762
Iteration 35/1000 | Loss: 0.00002761
Iteration 36/1000 | Loss: 0.00002761
Iteration 37/1000 | Loss: 0.00002761
Iteration 38/1000 | Loss: 0.00002760
Iteration 39/1000 | Loss: 0.00002760
Iteration 40/1000 | Loss: 0.00002760
Iteration 41/1000 | Loss: 0.00002760
Iteration 42/1000 | Loss: 0.00002760
Iteration 43/1000 | Loss: 0.00002760
Iteration 44/1000 | Loss: 0.00002760
Iteration 45/1000 | Loss: 0.00002760
Iteration 46/1000 | Loss: 0.00002760
Iteration 47/1000 | Loss: 0.00002759
Iteration 48/1000 | Loss: 0.00002759
Iteration 49/1000 | Loss: 0.00002759
Iteration 50/1000 | Loss: 0.00002759
Iteration 51/1000 | Loss: 0.00002759
Iteration 52/1000 | Loss: 0.00002759
Iteration 53/1000 | Loss: 0.00002759
Iteration 54/1000 | Loss: 0.00002759
Iteration 55/1000 | Loss: 0.00002759
Iteration 56/1000 | Loss: 0.00002759
Iteration 57/1000 | Loss: 0.00002758
Iteration 58/1000 | Loss: 0.00002758
Iteration 59/1000 | Loss: 0.00002758
Iteration 60/1000 | Loss: 0.00002758
Iteration 61/1000 | Loss: 0.00002758
Iteration 62/1000 | Loss: 0.00002758
Iteration 63/1000 | Loss: 0.00002756
Iteration 64/1000 | Loss: 0.00002756
Iteration 65/1000 | Loss: 0.00002756
Iteration 66/1000 | Loss: 0.00002755
Iteration 67/1000 | Loss: 0.00002755
Iteration 68/1000 | Loss: 0.00002755
Iteration 69/1000 | Loss: 0.00002755
Iteration 70/1000 | Loss: 0.00002755
Iteration 71/1000 | Loss: 0.00002755
Iteration 72/1000 | Loss: 0.00002755
Iteration 73/1000 | Loss: 0.00002755
Iteration 74/1000 | Loss: 0.00002755
Iteration 75/1000 | Loss: 0.00002755
Iteration 76/1000 | Loss: 0.00002754
Iteration 77/1000 | Loss: 0.00002753
Iteration 78/1000 | Loss: 0.00002753
Iteration 79/1000 | Loss: 0.00002753
Iteration 80/1000 | Loss: 0.00002753
Iteration 81/1000 | Loss: 0.00002753
Iteration 82/1000 | Loss: 0.00002753
Iteration 83/1000 | Loss: 0.00002753
Iteration 84/1000 | Loss: 0.00002752
Iteration 85/1000 | Loss: 0.00002752
Iteration 86/1000 | Loss: 0.00002752
Iteration 87/1000 | Loss: 0.00002752
Iteration 88/1000 | Loss: 0.00002752
Iteration 89/1000 | Loss: 0.00002752
Iteration 90/1000 | Loss: 0.00002752
Iteration 91/1000 | Loss: 0.00002752
Iteration 92/1000 | Loss: 0.00002752
Iteration 93/1000 | Loss: 0.00002752
Iteration 94/1000 | Loss: 0.00002752
Iteration 95/1000 | Loss: 0.00002752
Iteration 96/1000 | Loss: 0.00002752
Iteration 97/1000 | Loss: 0.00002752
Iteration 98/1000 | Loss: 0.00002752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.752250111370813e-05, 2.752250111370813e-05, 2.752250111370813e-05, 2.752250111370813e-05, 2.752250111370813e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.752250111370813e-05

Optimization complete. Final v2v error: 4.417656421661377 mm

Highest mean error: 4.794602870941162 mm for frame 111

Lowest mean error: 4.130004405975342 mm for frame 46

Saving results

Total time: 32.55428171157837
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507260
Iteration 2/25 | Loss: 0.00152119
Iteration 3/25 | Loss: 0.00135017
Iteration 4/25 | Loss: 0.00133330
Iteration 5/25 | Loss: 0.00132970
Iteration 6/25 | Loss: 0.00132963
Iteration 7/25 | Loss: 0.00132963
Iteration 8/25 | Loss: 0.00132963
Iteration 9/25 | Loss: 0.00132963
Iteration 10/25 | Loss: 0.00132963
Iteration 11/25 | Loss: 0.00132963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013296348042786121, 0.0013296348042786121, 0.0013296348042786121, 0.0013296348042786121, 0.0013296348042786121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013296348042786121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18520772
Iteration 2/25 | Loss: 0.00300088
Iteration 3/25 | Loss: 0.00300088
Iteration 4/25 | Loss: 0.00300088
Iteration 5/25 | Loss: 0.00300088
Iteration 6/25 | Loss: 0.00300088
Iteration 7/25 | Loss: 0.00300088
Iteration 8/25 | Loss: 0.00300088
Iteration 9/25 | Loss: 0.00300088
Iteration 10/25 | Loss: 0.00300088
Iteration 11/25 | Loss: 0.00300088
Iteration 12/25 | Loss: 0.00300088
Iteration 13/25 | Loss: 0.00300088
Iteration 14/25 | Loss: 0.00300088
Iteration 15/25 | Loss: 0.00300088
Iteration 16/25 | Loss: 0.00300088
Iteration 17/25 | Loss: 0.00300088
Iteration 18/25 | Loss: 0.00300088
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0030008782632648945, 0.0030008782632648945, 0.0030008782632648945, 0.0030008782632648945, 0.0030008782632648945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030008782632648945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00300088
Iteration 2/1000 | Loss: 0.00006158
Iteration 3/1000 | Loss: 0.00004395
Iteration 4/1000 | Loss: 0.00003909
Iteration 5/1000 | Loss: 0.00003690
Iteration 6/1000 | Loss: 0.00003568
Iteration 7/1000 | Loss: 0.00003490
Iteration 8/1000 | Loss: 0.00003434
Iteration 9/1000 | Loss: 0.00003388
Iteration 10/1000 | Loss: 0.00003347
Iteration 11/1000 | Loss: 0.00003316
Iteration 12/1000 | Loss: 0.00003292
Iteration 13/1000 | Loss: 0.00003274
Iteration 14/1000 | Loss: 0.00003265
Iteration 15/1000 | Loss: 0.00003259
Iteration 16/1000 | Loss: 0.00003258
Iteration 17/1000 | Loss: 0.00003257
Iteration 18/1000 | Loss: 0.00003257
Iteration 19/1000 | Loss: 0.00003257
Iteration 20/1000 | Loss: 0.00003257
Iteration 21/1000 | Loss: 0.00003256
Iteration 22/1000 | Loss: 0.00003255
Iteration 23/1000 | Loss: 0.00003254
Iteration 24/1000 | Loss: 0.00003251
Iteration 25/1000 | Loss: 0.00003251
Iteration 26/1000 | Loss: 0.00003251
Iteration 27/1000 | Loss: 0.00003251
Iteration 28/1000 | Loss: 0.00003251
Iteration 29/1000 | Loss: 0.00003250
Iteration 30/1000 | Loss: 0.00003250
Iteration 31/1000 | Loss: 0.00003250
Iteration 32/1000 | Loss: 0.00003249
Iteration 33/1000 | Loss: 0.00003249
Iteration 34/1000 | Loss: 0.00003249
Iteration 35/1000 | Loss: 0.00003249
Iteration 36/1000 | Loss: 0.00003249
Iteration 37/1000 | Loss: 0.00003249
Iteration 38/1000 | Loss: 0.00003248
Iteration 39/1000 | Loss: 0.00003248
Iteration 40/1000 | Loss: 0.00003248
Iteration 41/1000 | Loss: 0.00003248
Iteration 42/1000 | Loss: 0.00003248
Iteration 43/1000 | Loss: 0.00003247
Iteration 44/1000 | Loss: 0.00003246
Iteration 45/1000 | Loss: 0.00003246
Iteration 46/1000 | Loss: 0.00003246
Iteration 47/1000 | Loss: 0.00003245
Iteration 48/1000 | Loss: 0.00003245
Iteration 49/1000 | Loss: 0.00003245
Iteration 50/1000 | Loss: 0.00003245
Iteration 51/1000 | Loss: 0.00003244
Iteration 52/1000 | Loss: 0.00003244
Iteration 53/1000 | Loss: 0.00003244
Iteration 54/1000 | Loss: 0.00003244
Iteration 55/1000 | Loss: 0.00003244
Iteration 56/1000 | Loss: 0.00003244
Iteration 57/1000 | Loss: 0.00003243
Iteration 58/1000 | Loss: 0.00003243
Iteration 59/1000 | Loss: 0.00003243
Iteration 60/1000 | Loss: 0.00003243
Iteration 61/1000 | Loss: 0.00003243
Iteration 62/1000 | Loss: 0.00003243
Iteration 63/1000 | Loss: 0.00003243
Iteration 64/1000 | Loss: 0.00003243
Iteration 65/1000 | Loss: 0.00003243
Iteration 66/1000 | Loss: 0.00003242
Iteration 67/1000 | Loss: 0.00003242
Iteration 68/1000 | Loss: 0.00003242
Iteration 69/1000 | Loss: 0.00003242
Iteration 70/1000 | Loss: 0.00003242
Iteration 71/1000 | Loss: 0.00003242
Iteration 72/1000 | Loss: 0.00003242
Iteration 73/1000 | Loss: 0.00003242
Iteration 74/1000 | Loss: 0.00003241
Iteration 75/1000 | Loss: 0.00003241
Iteration 76/1000 | Loss: 0.00003241
Iteration 77/1000 | Loss: 0.00003241
Iteration 78/1000 | Loss: 0.00003240
Iteration 79/1000 | Loss: 0.00003240
Iteration 80/1000 | Loss: 0.00003240
Iteration 81/1000 | Loss: 0.00003240
Iteration 82/1000 | Loss: 0.00003240
Iteration 83/1000 | Loss: 0.00003240
Iteration 84/1000 | Loss: 0.00003240
Iteration 85/1000 | Loss: 0.00003240
Iteration 86/1000 | Loss: 0.00003240
Iteration 87/1000 | Loss: 0.00003240
Iteration 88/1000 | Loss: 0.00003239
Iteration 89/1000 | Loss: 0.00003239
Iteration 90/1000 | Loss: 0.00003239
Iteration 91/1000 | Loss: 0.00003239
Iteration 92/1000 | Loss: 0.00003239
Iteration 93/1000 | Loss: 0.00003239
Iteration 94/1000 | Loss: 0.00003239
Iteration 95/1000 | Loss: 0.00003238
Iteration 96/1000 | Loss: 0.00003238
Iteration 97/1000 | Loss: 0.00003238
Iteration 98/1000 | Loss: 0.00003238
Iteration 99/1000 | Loss: 0.00003238
Iteration 100/1000 | Loss: 0.00003238
Iteration 101/1000 | Loss: 0.00003238
Iteration 102/1000 | Loss: 0.00003238
Iteration 103/1000 | Loss: 0.00003237
Iteration 104/1000 | Loss: 0.00003237
Iteration 105/1000 | Loss: 0.00003237
Iteration 106/1000 | Loss: 0.00003237
Iteration 107/1000 | Loss: 0.00003237
Iteration 108/1000 | Loss: 0.00003237
Iteration 109/1000 | Loss: 0.00003237
Iteration 110/1000 | Loss: 0.00003236
Iteration 111/1000 | Loss: 0.00003236
Iteration 112/1000 | Loss: 0.00003236
Iteration 113/1000 | Loss: 0.00003236
Iteration 114/1000 | Loss: 0.00003236
Iteration 115/1000 | Loss: 0.00003236
Iteration 116/1000 | Loss: 0.00003236
Iteration 117/1000 | Loss: 0.00003236
Iteration 118/1000 | Loss: 0.00003236
Iteration 119/1000 | Loss: 0.00003236
Iteration 120/1000 | Loss: 0.00003236
Iteration 121/1000 | Loss: 0.00003235
Iteration 122/1000 | Loss: 0.00003235
Iteration 123/1000 | Loss: 0.00003235
Iteration 124/1000 | Loss: 0.00003235
Iteration 125/1000 | Loss: 0.00003235
Iteration 126/1000 | Loss: 0.00003235
Iteration 127/1000 | Loss: 0.00003235
Iteration 128/1000 | Loss: 0.00003235
Iteration 129/1000 | Loss: 0.00003235
Iteration 130/1000 | Loss: 0.00003234
Iteration 131/1000 | Loss: 0.00003234
Iteration 132/1000 | Loss: 0.00003234
Iteration 133/1000 | Loss: 0.00003234
Iteration 134/1000 | Loss: 0.00003234
Iteration 135/1000 | Loss: 0.00003234
Iteration 136/1000 | Loss: 0.00003233
Iteration 137/1000 | Loss: 0.00003233
Iteration 138/1000 | Loss: 0.00003233
Iteration 139/1000 | Loss: 0.00003233
Iteration 140/1000 | Loss: 0.00003233
Iteration 141/1000 | Loss: 0.00003233
Iteration 142/1000 | Loss: 0.00003233
Iteration 143/1000 | Loss: 0.00003233
Iteration 144/1000 | Loss: 0.00003232
Iteration 145/1000 | Loss: 0.00003232
Iteration 146/1000 | Loss: 0.00003232
Iteration 147/1000 | Loss: 0.00003232
Iteration 148/1000 | Loss: 0.00003232
Iteration 149/1000 | Loss: 0.00003231
Iteration 150/1000 | Loss: 0.00003231
Iteration 151/1000 | Loss: 0.00003231
Iteration 152/1000 | Loss: 0.00003231
Iteration 153/1000 | Loss: 0.00003231
Iteration 154/1000 | Loss: 0.00003231
Iteration 155/1000 | Loss: 0.00003231
Iteration 156/1000 | Loss: 0.00003231
Iteration 157/1000 | Loss: 0.00003231
Iteration 158/1000 | Loss: 0.00003231
Iteration 159/1000 | Loss: 0.00003230
Iteration 160/1000 | Loss: 0.00003230
Iteration 161/1000 | Loss: 0.00003230
Iteration 162/1000 | Loss: 0.00003230
Iteration 163/1000 | Loss: 0.00003230
Iteration 164/1000 | Loss: 0.00003230
Iteration 165/1000 | Loss: 0.00003230
Iteration 166/1000 | Loss: 0.00003229
Iteration 167/1000 | Loss: 0.00003229
Iteration 168/1000 | Loss: 0.00003229
Iteration 169/1000 | Loss: 0.00003229
Iteration 170/1000 | Loss: 0.00003229
Iteration 171/1000 | Loss: 0.00003229
Iteration 172/1000 | Loss: 0.00003229
Iteration 173/1000 | Loss: 0.00003229
Iteration 174/1000 | Loss: 0.00003229
Iteration 175/1000 | Loss: 0.00003229
Iteration 176/1000 | Loss: 0.00003228
Iteration 177/1000 | Loss: 0.00003228
Iteration 178/1000 | Loss: 0.00003228
Iteration 179/1000 | Loss: 0.00003228
Iteration 180/1000 | Loss: 0.00003228
Iteration 181/1000 | Loss: 0.00003228
Iteration 182/1000 | Loss: 0.00003228
Iteration 183/1000 | Loss: 0.00003228
Iteration 184/1000 | Loss: 0.00003228
Iteration 185/1000 | Loss: 0.00003228
Iteration 186/1000 | Loss: 0.00003228
Iteration 187/1000 | Loss: 0.00003228
Iteration 188/1000 | Loss: 0.00003228
Iteration 189/1000 | Loss: 0.00003228
Iteration 190/1000 | Loss: 0.00003228
Iteration 191/1000 | Loss: 0.00003228
Iteration 192/1000 | Loss: 0.00003228
Iteration 193/1000 | Loss: 0.00003228
Iteration 194/1000 | Loss: 0.00003228
Iteration 195/1000 | Loss: 0.00003228
Iteration 196/1000 | Loss: 0.00003228
Iteration 197/1000 | Loss: 0.00003228
Iteration 198/1000 | Loss: 0.00003228
Iteration 199/1000 | Loss: 0.00003228
Iteration 200/1000 | Loss: 0.00003228
Iteration 201/1000 | Loss: 0.00003228
Iteration 202/1000 | Loss: 0.00003228
Iteration 203/1000 | Loss: 0.00003228
Iteration 204/1000 | Loss: 0.00003228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [3.22808264172636e-05, 3.22808264172636e-05, 3.22808264172636e-05, 3.22808264172636e-05, 3.22808264172636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.22808264172636e-05

Optimization complete. Final v2v error: 4.384756565093994 mm

Highest mean error: 5.187981128692627 mm for frame 176

Lowest mean error: 3.839574098587036 mm for frame 70

Saving results

Total time: 46.12583923339844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990422
Iteration 2/25 | Loss: 0.00182205
Iteration 3/25 | Loss: 0.00145201
Iteration 4/25 | Loss: 0.00143404
Iteration 5/25 | Loss: 0.00142830
Iteration 6/25 | Loss: 0.00142677
Iteration 7/25 | Loss: 0.00142677
Iteration 8/25 | Loss: 0.00142677
Iteration 9/25 | Loss: 0.00142677
Iteration 10/25 | Loss: 0.00142677
Iteration 11/25 | Loss: 0.00142677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014267711667343974, 0.0014267711667343974, 0.0014267711667343974, 0.0014267711667343974, 0.0014267711667343974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014267711667343974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.47217387
Iteration 2/25 | Loss: 0.00270216
Iteration 3/25 | Loss: 0.00270216
Iteration 4/25 | Loss: 0.00270216
Iteration 5/25 | Loss: 0.00270216
Iteration 6/25 | Loss: 0.00270216
Iteration 7/25 | Loss: 0.00270216
Iteration 8/25 | Loss: 0.00270216
Iteration 9/25 | Loss: 0.00270216
Iteration 10/25 | Loss: 0.00270216
Iteration 11/25 | Loss: 0.00270216
Iteration 12/25 | Loss: 0.00270216
Iteration 13/25 | Loss: 0.00270216
Iteration 14/25 | Loss: 0.00270216
Iteration 15/25 | Loss: 0.00270216
Iteration 16/25 | Loss: 0.00270216
Iteration 17/25 | Loss: 0.00270216
Iteration 18/25 | Loss: 0.00270216
Iteration 19/25 | Loss: 0.00270216
Iteration 20/25 | Loss: 0.00270216
Iteration 21/25 | Loss: 0.00270216
Iteration 22/25 | Loss: 0.00270216
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0027021565474569798, 0.0027021565474569798, 0.0027021565474569798, 0.0027021565474569798, 0.0027021565474569798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027021565474569798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00270216
Iteration 2/1000 | Loss: 0.00007472
Iteration 3/1000 | Loss: 0.00005554
Iteration 4/1000 | Loss: 0.00004737
Iteration 5/1000 | Loss: 0.00004364
Iteration 6/1000 | Loss: 0.00004102
Iteration 7/1000 | Loss: 0.00003937
Iteration 8/1000 | Loss: 0.00003797
Iteration 9/1000 | Loss: 0.00003731
Iteration 10/1000 | Loss: 0.00003690
Iteration 11/1000 | Loss: 0.00003641
Iteration 12/1000 | Loss: 0.00003601
Iteration 13/1000 | Loss: 0.00003574
Iteration 14/1000 | Loss: 0.00003537
Iteration 15/1000 | Loss: 0.00003498
Iteration 16/1000 | Loss: 0.00003457
Iteration 17/1000 | Loss: 0.00003424
Iteration 18/1000 | Loss: 0.00003384
Iteration 19/1000 | Loss: 0.00003363
Iteration 20/1000 | Loss: 0.00003344
Iteration 21/1000 | Loss: 0.00003328
Iteration 22/1000 | Loss: 0.00003311
Iteration 23/1000 | Loss: 0.00003291
Iteration 24/1000 | Loss: 0.00003279
Iteration 25/1000 | Loss: 0.00003273
Iteration 26/1000 | Loss: 0.00003273
Iteration 27/1000 | Loss: 0.00003271
Iteration 28/1000 | Loss: 0.00003270
Iteration 29/1000 | Loss: 0.00003270
Iteration 30/1000 | Loss: 0.00003269
Iteration 31/1000 | Loss: 0.00003268
Iteration 32/1000 | Loss: 0.00003267
Iteration 33/1000 | Loss: 0.00003267
Iteration 34/1000 | Loss: 0.00003267
Iteration 35/1000 | Loss: 0.00003266
Iteration 36/1000 | Loss: 0.00003266
Iteration 37/1000 | Loss: 0.00003265
Iteration 38/1000 | Loss: 0.00003264
Iteration 39/1000 | Loss: 0.00003263
Iteration 40/1000 | Loss: 0.00003262
Iteration 41/1000 | Loss: 0.00003262
Iteration 42/1000 | Loss: 0.00003262
Iteration 43/1000 | Loss: 0.00003262
Iteration 44/1000 | Loss: 0.00003259
Iteration 45/1000 | Loss: 0.00003259
Iteration 46/1000 | Loss: 0.00003259
Iteration 47/1000 | Loss: 0.00003259
Iteration 48/1000 | Loss: 0.00003259
Iteration 49/1000 | Loss: 0.00003259
Iteration 50/1000 | Loss: 0.00003258
Iteration 51/1000 | Loss: 0.00003258
Iteration 52/1000 | Loss: 0.00003258
Iteration 53/1000 | Loss: 0.00003257
Iteration 54/1000 | Loss: 0.00003257
Iteration 55/1000 | Loss: 0.00003257
Iteration 56/1000 | Loss: 0.00003256
Iteration 57/1000 | Loss: 0.00003256
Iteration 58/1000 | Loss: 0.00003256
Iteration 59/1000 | Loss: 0.00003255
Iteration 60/1000 | Loss: 0.00003255
Iteration 61/1000 | Loss: 0.00003255
Iteration 62/1000 | Loss: 0.00003255
Iteration 63/1000 | Loss: 0.00003255
Iteration 64/1000 | Loss: 0.00003254
Iteration 65/1000 | Loss: 0.00003254
Iteration 66/1000 | Loss: 0.00003254
Iteration 67/1000 | Loss: 0.00003254
Iteration 68/1000 | Loss: 0.00003254
Iteration 69/1000 | Loss: 0.00003254
Iteration 70/1000 | Loss: 0.00003254
Iteration 71/1000 | Loss: 0.00003254
Iteration 72/1000 | Loss: 0.00003254
Iteration 73/1000 | Loss: 0.00003254
Iteration 74/1000 | Loss: 0.00003253
Iteration 75/1000 | Loss: 0.00003253
Iteration 76/1000 | Loss: 0.00003253
Iteration 77/1000 | Loss: 0.00003253
Iteration 78/1000 | Loss: 0.00003253
Iteration 79/1000 | Loss: 0.00003253
Iteration 80/1000 | Loss: 0.00003253
Iteration 81/1000 | Loss: 0.00003253
Iteration 82/1000 | Loss: 0.00003253
Iteration 83/1000 | Loss: 0.00003253
Iteration 84/1000 | Loss: 0.00003253
Iteration 85/1000 | Loss: 0.00003252
Iteration 86/1000 | Loss: 0.00003252
Iteration 87/1000 | Loss: 0.00003252
Iteration 88/1000 | Loss: 0.00003252
Iteration 89/1000 | Loss: 0.00003252
Iteration 90/1000 | Loss: 0.00003252
Iteration 91/1000 | Loss: 0.00003252
Iteration 92/1000 | Loss: 0.00003252
Iteration 93/1000 | Loss: 0.00003251
Iteration 94/1000 | Loss: 0.00003251
Iteration 95/1000 | Loss: 0.00003251
Iteration 96/1000 | Loss: 0.00003251
Iteration 97/1000 | Loss: 0.00003251
Iteration 98/1000 | Loss: 0.00003251
Iteration 99/1000 | Loss: 0.00003251
Iteration 100/1000 | Loss: 0.00003251
Iteration 101/1000 | Loss: 0.00003251
Iteration 102/1000 | Loss: 0.00003250
Iteration 103/1000 | Loss: 0.00003250
Iteration 104/1000 | Loss: 0.00003250
Iteration 105/1000 | Loss: 0.00003250
Iteration 106/1000 | Loss: 0.00003250
Iteration 107/1000 | Loss: 0.00003249
Iteration 108/1000 | Loss: 0.00003249
Iteration 109/1000 | Loss: 0.00003249
Iteration 110/1000 | Loss: 0.00003249
Iteration 111/1000 | Loss: 0.00003249
Iteration 112/1000 | Loss: 0.00003249
Iteration 113/1000 | Loss: 0.00003249
Iteration 114/1000 | Loss: 0.00003248
Iteration 115/1000 | Loss: 0.00003248
Iteration 116/1000 | Loss: 0.00003248
Iteration 117/1000 | Loss: 0.00003248
Iteration 118/1000 | Loss: 0.00003248
Iteration 119/1000 | Loss: 0.00003248
Iteration 120/1000 | Loss: 0.00003248
Iteration 121/1000 | Loss: 0.00003248
Iteration 122/1000 | Loss: 0.00003248
Iteration 123/1000 | Loss: 0.00003248
Iteration 124/1000 | Loss: 0.00003248
Iteration 125/1000 | Loss: 0.00003248
Iteration 126/1000 | Loss: 0.00003247
Iteration 127/1000 | Loss: 0.00003247
Iteration 128/1000 | Loss: 0.00003247
Iteration 129/1000 | Loss: 0.00003247
Iteration 130/1000 | Loss: 0.00003247
Iteration 131/1000 | Loss: 0.00003247
Iteration 132/1000 | Loss: 0.00003247
Iteration 133/1000 | Loss: 0.00003247
Iteration 134/1000 | Loss: 0.00003247
Iteration 135/1000 | Loss: 0.00003247
Iteration 136/1000 | Loss: 0.00003247
Iteration 137/1000 | Loss: 0.00003246
Iteration 138/1000 | Loss: 0.00003246
Iteration 139/1000 | Loss: 0.00003246
Iteration 140/1000 | Loss: 0.00003246
Iteration 141/1000 | Loss: 0.00003246
Iteration 142/1000 | Loss: 0.00003246
Iteration 143/1000 | Loss: 0.00003246
Iteration 144/1000 | Loss: 0.00003246
Iteration 145/1000 | Loss: 0.00003246
Iteration 146/1000 | Loss: 0.00003246
Iteration 147/1000 | Loss: 0.00003246
Iteration 148/1000 | Loss: 0.00003246
Iteration 149/1000 | Loss: 0.00003246
Iteration 150/1000 | Loss: 0.00003246
Iteration 151/1000 | Loss: 0.00003246
Iteration 152/1000 | Loss: 0.00003246
Iteration 153/1000 | Loss: 0.00003246
Iteration 154/1000 | Loss: 0.00003246
Iteration 155/1000 | Loss: 0.00003246
Iteration 156/1000 | Loss: 0.00003246
Iteration 157/1000 | Loss: 0.00003246
Iteration 158/1000 | Loss: 0.00003246
Iteration 159/1000 | Loss: 0.00003246
Iteration 160/1000 | Loss: 0.00003246
Iteration 161/1000 | Loss: 0.00003246
Iteration 162/1000 | Loss: 0.00003246
Iteration 163/1000 | Loss: 0.00003246
Iteration 164/1000 | Loss: 0.00003246
Iteration 165/1000 | Loss: 0.00003246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [3.2455893233418465e-05, 3.2455893233418465e-05, 3.2455893233418465e-05, 3.2455893233418465e-05, 3.2455893233418465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2455893233418465e-05

Optimization complete. Final v2v error: 4.65500020980835 mm

Highest mean error: 5.0248847007751465 mm for frame 18

Lowest mean error: 4.445804119110107 mm for frame 153

Saving results

Total time: 52.61640119552612
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797204
Iteration 2/25 | Loss: 0.00164640
Iteration 3/25 | Loss: 0.00140263
Iteration 4/25 | Loss: 0.00139298
Iteration 5/25 | Loss: 0.00139035
Iteration 6/25 | Loss: 0.00138997
Iteration 7/25 | Loss: 0.00138997
Iteration 8/25 | Loss: 0.00138997
Iteration 9/25 | Loss: 0.00138997
Iteration 10/25 | Loss: 0.00138997
Iteration 11/25 | Loss: 0.00138997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013899714685976505, 0.0013899714685976505, 0.0013899714685976505, 0.0013899714685976505, 0.0013899714685976505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013899714685976505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.23122044
Iteration 2/25 | Loss: 0.00277506
Iteration 3/25 | Loss: 0.00277505
Iteration 4/25 | Loss: 0.00277505
Iteration 5/25 | Loss: 0.00277505
Iteration 6/25 | Loss: 0.00277505
Iteration 7/25 | Loss: 0.00277505
Iteration 8/25 | Loss: 0.00277505
Iteration 9/25 | Loss: 0.00277505
Iteration 10/25 | Loss: 0.00277505
Iteration 11/25 | Loss: 0.00277505
Iteration 12/25 | Loss: 0.00277505
Iteration 13/25 | Loss: 0.00277505
Iteration 14/25 | Loss: 0.00277505
Iteration 15/25 | Loss: 0.00277505
Iteration 16/25 | Loss: 0.00277505
Iteration 17/25 | Loss: 0.00277505
Iteration 18/25 | Loss: 0.00277505
Iteration 19/25 | Loss: 0.00277505
Iteration 20/25 | Loss: 0.00277505
Iteration 21/25 | Loss: 0.00277505
Iteration 22/25 | Loss: 0.00277505
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0027750511653721333, 0.0027750511653721333, 0.0027750511653721333, 0.0027750511653721333, 0.0027750511653721333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027750511653721333

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00277505
Iteration 2/1000 | Loss: 0.00007412
Iteration 3/1000 | Loss: 0.00005675
Iteration 4/1000 | Loss: 0.00005056
Iteration 5/1000 | Loss: 0.00004731
Iteration 6/1000 | Loss: 0.00004536
Iteration 7/1000 | Loss: 0.00004399
Iteration 8/1000 | Loss: 0.00004318
Iteration 9/1000 | Loss: 0.00004251
Iteration 10/1000 | Loss: 0.00004201
Iteration 11/1000 | Loss: 0.00004126
Iteration 12/1000 | Loss: 0.00004074
Iteration 13/1000 | Loss: 0.00004032
Iteration 14/1000 | Loss: 0.00003993
Iteration 15/1000 | Loss: 0.00003962
Iteration 16/1000 | Loss: 0.00003933
Iteration 17/1000 | Loss: 0.00003902
Iteration 18/1000 | Loss: 0.00003872
Iteration 19/1000 | Loss: 0.00003846
Iteration 20/1000 | Loss: 0.00003826
Iteration 21/1000 | Loss: 0.00003811
Iteration 22/1000 | Loss: 0.00003804
Iteration 23/1000 | Loss: 0.00003804
Iteration 24/1000 | Loss: 0.00003799
Iteration 25/1000 | Loss: 0.00003797
Iteration 26/1000 | Loss: 0.00003795
Iteration 27/1000 | Loss: 0.00003795
Iteration 28/1000 | Loss: 0.00003793
Iteration 29/1000 | Loss: 0.00003792
Iteration 30/1000 | Loss: 0.00003791
Iteration 31/1000 | Loss: 0.00003789
Iteration 32/1000 | Loss: 0.00003789
Iteration 33/1000 | Loss: 0.00003789
Iteration 34/1000 | Loss: 0.00003789
Iteration 35/1000 | Loss: 0.00003789
Iteration 36/1000 | Loss: 0.00003789
Iteration 37/1000 | Loss: 0.00003789
Iteration 38/1000 | Loss: 0.00003789
Iteration 39/1000 | Loss: 0.00003789
Iteration 40/1000 | Loss: 0.00003789
Iteration 41/1000 | Loss: 0.00003788
Iteration 42/1000 | Loss: 0.00003788
Iteration 43/1000 | Loss: 0.00003788
Iteration 44/1000 | Loss: 0.00003788
Iteration 45/1000 | Loss: 0.00003788
Iteration 46/1000 | Loss: 0.00003787
Iteration 47/1000 | Loss: 0.00003787
Iteration 48/1000 | Loss: 0.00003787
Iteration 49/1000 | Loss: 0.00003786
Iteration 50/1000 | Loss: 0.00003786
Iteration 51/1000 | Loss: 0.00003785
Iteration 52/1000 | Loss: 0.00003785
Iteration 53/1000 | Loss: 0.00003785
Iteration 54/1000 | Loss: 0.00003785
Iteration 55/1000 | Loss: 0.00003784
Iteration 56/1000 | Loss: 0.00003784
Iteration 57/1000 | Loss: 0.00003784
Iteration 58/1000 | Loss: 0.00003784
Iteration 59/1000 | Loss: 0.00003784
Iteration 60/1000 | Loss: 0.00003784
Iteration 61/1000 | Loss: 0.00003783
Iteration 62/1000 | Loss: 0.00003783
Iteration 63/1000 | Loss: 0.00003783
Iteration 64/1000 | Loss: 0.00003783
Iteration 65/1000 | Loss: 0.00003782
Iteration 66/1000 | Loss: 0.00003782
Iteration 67/1000 | Loss: 0.00003782
Iteration 68/1000 | Loss: 0.00003782
Iteration 69/1000 | Loss: 0.00003782
Iteration 70/1000 | Loss: 0.00003781
Iteration 71/1000 | Loss: 0.00003781
Iteration 72/1000 | Loss: 0.00003781
Iteration 73/1000 | Loss: 0.00003781
Iteration 74/1000 | Loss: 0.00003781
Iteration 75/1000 | Loss: 0.00003781
Iteration 76/1000 | Loss: 0.00003781
Iteration 77/1000 | Loss: 0.00003781
Iteration 78/1000 | Loss: 0.00003780
Iteration 79/1000 | Loss: 0.00003780
Iteration 80/1000 | Loss: 0.00003780
Iteration 81/1000 | Loss: 0.00003778
Iteration 82/1000 | Loss: 0.00003778
Iteration 83/1000 | Loss: 0.00003778
Iteration 84/1000 | Loss: 0.00003778
Iteration 85/1000 | Loss: 0.00003778
Iteration 86/1000 | Loss: 0.00003778
Iteration 87/1000 | Loss: 0.00003777
Iteration 88/1000 | Loss: 0.00003777
Iteration 89/1000 | Loss: 0.00003777
Iteration 90/1000 | Loss: 0.00003776
Iteration 91/1000 | Loss: 0.00003776
Iteration 92/1000 | Loss: 0.00003776
Iteration 93/1000 | Loss: 0.00003776
Iteration 94/1000 | Loss: 0.00003776
Iteration 95/1000 | Loss: 0.00003776
Iteration 96/1000 | Loss: 0.00003776
Iteration 97/1000 | Loss: 0.00003776
Iteration 98/1000 | Loss: 0.00003776
Iteration 99/1000 | Loss: 0.00003776
Iteration 100/1000 | Loss: 0.00003775
Iteration 101/1000 | Loss: 0.00003775
Iteration 102/1000 | Loss: 0.00003775
Iteration 103/1000 | Loss: 0.00003775
Iteration 104/1000 | Loss: 0.00003775
Iteration 105/1000 | Loss: 0.00003774
Iteration 106/1000 | Loss: 0.00003774
Iteration 107/1000 | Loss: 0.00003774
Iteration 108/1000 | Loss: 0.00003774
Iteration 109/1000 | Loss: 0.00003774
Iteration 110/1000 | Loss: 0.00003773
Iteration 111/1000 | Loss: 0.00003773
Iteration 112/1000 | Loss: 0.00003773
Iteration 113/1000 | Loss: 0.00003773
Iteration 114/1000 | Loss: 0.00003773
Iteration 115/1000 | Loss: 0.00003773
Iteration 116/1000 | Loss: 0.00003772
Iteration 117/1000 | Loss: 0.00003772
Iteration 118/1000 | Loss: 0.00003772
Iteration 119/1000 | Loss: 0.00003772
Iteration 120/1000 | Loss: 0.00003772
Iteration 121/1000 | Loss: 0.00003772
Iteration 122/1000 | Loss: 0.00003772
Iteration 123/1000 | Loss: 0.00003771
Iteration 124/1000 | Loss: 0.00003771
Iteration 125/1000 | Loss: 0.00003771
Iteration 126/1000 | Loss: 0.00003771
Iteration 127/1000 | Loss: 0.00003771
Iteration 128/1000 | Loss: 0.00003771
Iteration 129/1000 | Loss: 0.00003771
Iteration 130/1000 | Loss: 0.00003771
Iteration 131/1000 | Loss: 0.00003771
Iteration 132/1000 | Loss: 0.00003771
Iteration 133/1000 | Loss: 0.00003771
Iteration 134/1000 | Loss: 0.00003771
Iteration 135/1000 | Loss: 0.00003771
Iteration 136/1000 | Loss: 0.00003770
Iteration 137/1000 | Loss: 0.00003770
Iteration 138/1000 | Loss: 0.00003770
Iteration 139/1000 | Loss: 0.00003770
Iteration 140/1000 | Loss: 0.00003770
Iteration 141/1000 | Loss: 0.00003769
Iteration 142/1000 | Loss: 0.00003769
Iteration 143/1000 | Loss: 0.00003769
Iteration 144/1000 | Loss: 0.00003769
Iteration 145/1000 | Loss: 0.00003769
Iteration 146/1000 | Loss: 0.00003769
Iteration 147/1000 | Loss: 0.00003769
Iteration 148/1000 | Loss: 0.00003769
Iteration 149/1000 | Loss: 0.00003769
Iteration 150/1000 | Loss: 0.00003769
Iteration 151/1000 | Loss: 0.00003768
Iteration 152/1000 | Loss: 0.00003768
Iteration 153/1000 | Loss: 0.00003768
Iteration 154/1000 | Loss: 0.00003768
Iteration 155/1000 | Loss: 0.00003768
Iteration 156/1000 | Loss: 0.00003768
Iteration 157/1000 | Loss: 0.00003768
Iteration 158/1000 | Loss: 0.00003768
Iteration 159/1000 | Loss: 0.00003768
Iteration 160/1000 | Loss: 0.00003767
Iteration 161/1000 | Loss: 0.00003767
Iteration 162/1000 | Loss: 0.00003767
Iteration 163/1000 | Loss: 0.00003767
Iteration 164/1000 | Loss: 0.00003767
Iteration 165/1000 | Loss: 0.00003767
Iteration 166/1000 | Loss: 0.00003767
Iteration 167/1000 | Loss: 0.00003767
Iteration 168/1000 | Loss: 0.00003767
Iteration 169/1000 | Loss: 0.00003767
Iteration 170/1000 | Loss: 0.00003767
Iteration 171/1000 | Loss: 0.00003766
Iteration 172/1000 | Loss: 0.00003766
Iteration 173/1000 | Loss: 0.00003766
Iteration 174/1000 | Loss: 0.00003766
Iteration 175/1000 | Loss: 0.00003766
Iteration 176/1000 | Loss: 0.00003766
Iteration 177/1000 | Loss: 0.00003766
Iteration 178/1000 | Loss: 0.00003766
Iteration 179/1000 | Loss: 0.00003766
Iteration 180/1000 | Loss: 0.00003766
Iteration 181/1000 | Loss: 0.00003766
Iteration 182/1000 | Loss: 0.00003766
Iteration 183/1000 | Loss: 0.00003765
Iteration 184/1000 | Loss: 0.00003765
Iteration 185/1000 | Loss: 0.00003765
Iteration 186/1000 | Loss: 0.00003765
Iteration 187/1000 | Loss: 0.00003765
Iteration 188/1000 | Loss: 0.00003765
Iteration 189/1000 | Loss: 0.00003765
Iteration 190/1000 | Loss: 0.00003765
Iteration 191/1000 | Loss: 0.00003765
Iteration 192/1000 | Loss: 0.00003765
Iteration 193/1000 | Loss: 0.00003765
Iteration 194/1000 | Loss: 0.00003765
Iteration 195/1000 | Loss: 0.00003765
Iteration 196/1000 | Loss: 0.00003765
Iteration 197/1000 | Loss: 0.00003765
Iteration 198/1000 | Loss: 0.00003765
Iteration 199/1000 | Loss: 0.00003765
Iteration 200/1000 | Loss: 0.00003765
Iteration 201/1000 | Loss: 0.00003765
Iteration 202/1000 | Loss: 0.00003765
Iteration 203/1000 | Loss: 0.00003765
Iteration 204/1000 | Loss: 0.00003765
Iteration 205/1000 | Loss: 0.00003764
Iteration 206/1000 | Loss: 0.00003764
Iteration 207/1000 | Loss: 0.00003764
Iteration 208/1000 | Loss: 0.00003764
Iteration 209/1000 | Loss: 0.00003764
Iteration 210/1000 | Loss: 0.00003764
Iteration 211/1000 | Loss: 0.00003764
Iteration 212/1000 | Loss: 0.00003764
Iteration 213/1000 | Loss: 0.00003764
Iteration 214/1000 | Loss: 0.00003764
Iteration 215/1000 | Loss: 0.00003764
Iteration 216/1000 | Loss: 0.00003764
Iteration 217/1000 | Loss: 0.00003764
Iteration 218/1000 | Loss: 0.00003764
Iteration 219/1000 | Loss: 0.00003764
Iteration 220/1000 | Loss: 0.00003764
Iteration 221/1000 | Loss: 0.00003763
Iteration 222/1000 | Loss: 0.00003763
Iteration 223/1000 | Loss: 0.00003763
Iteration 224/1000 | Loss: 0.00003763
Iteration 225/1000 | Loss: 0.00003763
Iteration 226/1000 | Loss: 0.00003763
Iteration 227/1000 | Loss: 0.00003763
Iteration 228/1000 | Loss: 0.00003763
Iteration 229/1000 | Loss: 0.00003763
Iteration 230/1000 | Loss: 0.00003763
Iteration 231/1000 | Loss: 0.00003762
Iteration 232/1000 | Loss: 0.00003762
Iteration 233/1000 | Loss: 0.00003762
Iteration 234/1000 | Loss: 0.00003762
Iteration 235/1000 | Loss: 0.00003762
Iteration 236/1000 | Loss: 0.00003762
Iteration 237/1000 | Loss: 0.00003762
Iteration 238/1000 | Loss: 0.00003762
Iteration 239/1000 | Loss: 0.00003762
Iteration 240/1000 | Loss: 0.00003762
Iteration 241/1000 | Loss: 0.00003762
Iteration 242/1000 | Loss: 0.00003762
Iteration 243/1000 | Loss: 0.00003762
Iteration 244/1000 | Loss: 0.00003762
Iteration 245/1000 | Loss: 0.00003762
Iteration 246/1000 | Loss: 0.00003762
Iteration 247/1000 | Loss: 0.00003762
Iteration 248/1000 | Loss: 0.00003761
Iteration 249/1000 | Loss: 0.00003761
Iteration 250/1000 | Loss: 0.00003761
Iteration 251/1000 | Loss: 0.00003761
Iteration 252/1000 | Loss: 0.00003761
Iteration 253/1000 | Loss: 0.00003761
Iteration 254/1000 | Loss: 0.00003761
Iteration 255/1000 | Loss: 0.00003761
Iteration 256/1000 | Loss: 0.00003761
Iteration 257/1000 | Loss: 0.00003761
Iteration 258/1000 | Loss: 0.00003761
Iteration 259/1000 | Loss: 0.00003761
Iteration 260/1000 | Loss: 0.00003761
Iteration 261/1000 | Loss: 0.00003761
Iteration 262/1000 | Loss: 0.00003761
Iteration 263/1000 | Loss: 0.00003761
Iteration 264/1000 | Loss: 0.00003761
Iteration 265/1000 | Loss: 0.00003761
Iteration 266/1000 | Loss: 0.00003760
Iteration 267/1000 | Loss: 0.00003760
Iteration 268/1000 | Loss: 0.00003760
Iteration 269/1000 | Loss: 0.00003760
Iteration 270/1000 | Loss: 0.00003760
Iteration 271/1000 | Loss: 0.00003760
Iteration 272/1000 | Loss: 0.00003760
Iteration 273/1000 | Loss: 0.00003760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [3.76037678506691e-05, 3.76037678506691e-05, 3.76037678506691e-05, 3.76037678506691e-05, 3.76037678506691e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.76037678506691e-05

Optimization complete. Final v2v error: 4.735269069671631 mm

Highest mean error: 6.76810884475708 mm for frame 153

Lowest mean error: 3.4023401737213135 mm for frame 20

Saving results

Total time: 56.45957159996033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002676
Iteration 2/25 | Loss: 0.00219267
Iteration 3/25 | Loss: 0.00180566
Iteration 4/25 | Loss: 0.00173146
Iteration 5/25 | Loss: 0.00176187
Iteration 6/25 | Loss: 0.00171316
Iteration 7/25 | Loss: 0.00164958
Iteration 8/25 | Loss: 0.00162697
Iteration 9/25 | Loss: 0.00160079
Iteration 10/25 | Loss: 0.00158643
Iteration 11/25 | Loss: 0.00158305
Iteration 12/25 | Loss: 0.00158747
Iteration 13/25 | Loss: 0.00158031
Iteration 14/25 | Loss: 0.00158769
Iteration 15/25 | Loss: 0.00158048
Iteration 16/25 | Loss: 0.00157411
Iteration 17/25 | Loss: 0.00157996
Iteration 18/25 | Loss: 0.00158381
Iteration 19/25 | Loss: 0.00158164
Iteration 20/25 | Loss: 0.00158057
Iteration 21/25 | Loss: 0.00156887
Iteration 22/25 | Loss: 0.00157070
Iteration 23/25 | Loss: 0.00156481
Iteration 24/25 | Loss: 0.00157860
Iteration 25/25 | Loss: 0.00157228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17984045
Iteration 2/25 | Loss: 0.00609780
Iteration 3/25 | Loss: 0.00570080
Iteration 4/25 | Loss: 0.00570080
Iteration 5/25 | Loss: 0.00570080
Iteration 6/25 | Loss: 0.00570079
Iteration 7/25 | Loss: 0.00570079
Iteration 8/25 | Loss: 0.00570079
Iteration 9/25 | Loss: 0.00570079
Iteration 10/25 | Loss: 0.00570079
Iteration 11/25 | Loss: 0.00570079
Iteration 12/25 | Loss: 0.00570079
Iteration 13/25 | Loss: 0.00570079
Iteration 14/25 | Loss: 0.00570079
Iteration 15/25 | Loss: 0.00570079
Iteration 16/25 | Loss: 0.00570079
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.005700793117284775, 0.005700793117284775, 0.005700793117284775, 0.005700793117284775, 0.005700793117284775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005700793117284775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00570079
Iteration 2/1000 | Loss: 0.00090218
Iteration 3/1000 | Loss: 0.00090732
Iteration 4/1000 | Loss: 0.00073034
Iteration 5/1000 | Loss: 0.00048611
Iteration 6/1000 | Loss: 0.00054677
Iteration 7/1000 | Loss: 0.00045085
Iteration 8/1000 | Loss: 0.00043065
Iteration 9/1000 | Loss: 0.00045455
Iteration 10/1000 | Loss: 0.00072068
Iteration 11/1000 | Loss: 0.00068203
Iteration 12/1000 | Loss: 0.00033368
Iteration 13/1000 | Loss: 0.00048742
Iteration 14/1000 | Loss: 0.00037806
Iteration 15/1000 | Loss: 0.00029029
Iteration 16/1000 | Loss: 0.00036343
Iteration 17/1000 | Loss: 0.00042343
Iteration 18/1000 | Loss: 0.00046388
Iteration 19/1000 | Loss: 0.00048033
Iteration 20/1000 | Loss: 0.00072619
Iteration 21/1000 | Loss: 0.00034095
Iteration 22/1000 | Loss: 0.00047336
Iteration 23/1000 | Loss: 0.00040556
Iteration 24/1000 | Loss: 0.00052909
Iteration 25/1000 | Loss: 0.00039529
Iteration 26/1000 | Loss: 0.00049160
Iteration 27/1000 | Loss: 0.00036311
Iteration 28/1000 | Loss: 0.00031040
Iteration 29/1000 | Loss: 0.00049476
Iteration 30/1000 | Loss: 0.00109656
Iteration 31/1000 | Loss: 0.00043827
Iteration 32/1000 | Loss: 0.00044965
Iteration 33/1000 | Loss: 0.00067633
Iteration 34/1000 | Loss: 0.00046409
Iteration 35/1000 | Loss: 0.00024183
Iteration 36/1000 | Loss: 0.00032444
Iteration 37/1000 | Loss: 0.00019912
Iteration 38/1000 | Loss: 0.00039838
Iteration 39/1000 | Loss: 0.00018514
Iteration 40/1000 | Loss: 0.00021732
Iteration 41/1000 | Loss: 0.00020694
Iteration 42/1000 | Loss: 0.00035992
Iteration 43/1000 | Loss: 0.00018274
Iteration 44/1000 | Loss: 0.00028466
Iteration 45/1000 | Loss: 0.00032733
Iteration 46/1000 | Loss: 0.00022722
Iteration 47/1000 | Loss: 0.00026404
Iteration 48/1000 | Loss: 0.00017119
Iteration 49/1000 | Loss: 0.00074700
Iteration 50/1000 | Loss: 0.00058442
Iteration 51/1000 | Loss: 0.00065666
Iteration 52/1000 | Loss: 0.00057379
Iteration 53/1000 | Loss: 0.00021433
Iteration 54/1000 | Loss: 0.00021258
Iteration 55/1000 | Loss: 0.00067711
Iteration 56/1000 | Loss: 0.00024593
Iteration 57/1000 | Loss: 0.00026684
Iteration 58/1000 | Loss: 0.00020911
Iteration 59/1000 | Loss: 0.00019376
Iteration 60/1000 | Loss: 0.00025665
Iteration 61/1000 | Loss: 0.00027683
Iteration 62/1000 | Loss: 0.00025802
Iteration 63/1000 | Loss: 0.00023284
Iteration 64/1000 | Loss: 0.00025012
Iteration 65/1000 | Loss: 0.00019814
Iteration 66/1000 | Loss: 0.00020109
Iteration 67/1000 | Loss: 0.00031127
Iteration 68/1000 | Loss: 0.00055964
Iteration 69/1000 | Loss: 0.00053964
Iteration 70/1000 | Loss: 0.00018483
Iteration 71/1000 | Loss: 0.00044234
Iteration 72/1000 | Loss: 0.00107047
Iteration 73/1000 | Loss: 0.00633627
Iteration 74/1000 | Loss: 0.00553435
Iteration 75/1000 | Loss: 0.00615425
Iteration 76/1000 | Loss: 0.00433245
Iteration 77/1000 | Loss: 0.00112591
Iteration 78/1000 | Loss: 0.00179265
Iteration 79/1000 | Loss: 0.00329511
Iteration 80/1000 | Loss: 0.00058768
Iteration 81/1000 | Loss: 0.00028732
Iteration 82/1000 | Loss: 0.00103630
Iteration 83/1000 | Loss: 0.00037538
Iteration 84/1000 | Loss: 0.00046428
Iteration 85/1000 | Loss: 0.00012115
Iteration 86/1000 | Loss: 0.00029306
Iteration 87/1000 | Loss: 0.00009950
Iteration 88/1000 | Loss: 0.00039248
Iteration 89/1000 | Loss: 0.00027218
Iteration 90/1000 | Loss: 0.00007224
Iteration 91/1000 | Loss: 0.00006844
Iteration 92/1000 | Loss: 0.00056641
Iteration 93/1000 | Loss: 0.00011818
Iteration 94/1000 | Loss: 0.00011226
Iteration 95/1000 | Loss: 0.00006339
Iteration 96/1000 | Loss: 0.00035767
Iteration 97/1000 | Loss: 0.00007046
Iteration 98/1000 | Loss: 0.00005596
Iteration 99/1000 | Loss: 0.00065712
Iteration 100/1000 | Loss: 0.00066560
Iteration 101/1000 | Loss: 0.00037422
Iteration 102/1000 | Loss: 0.00036820
Iteration 103/1000 | Loss: 0.00038592
Iteration 104/1000 | Loss: 0.00069280
Iteration 105/1000 | Loss: 0.00009409
Iteration 106/1000 | Loss: 0.00006076
Iteration 107/1000 | Loss: 0.00063297
Iteration 108/1000 | Loss: 0.00006610
Iteration 109/1000 | Loss: 0.00004766
Iteration 110/1000 | Loss: 0.00030029
Iteration 111/1000 | Loss: 0.00004679
Iteration 112/1000 | Loss: 0.00003609
Iteration 113/1000 | Loss: 0.00003007
Iteration 114/1000 | Loss: 0.00002407
Iteration 115/1000 | Loss: 0.00002161
Iteration 116/1000 | Loss: 0.00002009
Iteration 117/1000 | Loss: 0.00001879
Iteration 118/1000 | Loss: 0.00001791
Iteration 119/1000 | Loss: 0.00001735
Iteration 120/1000 | Loss: 0.00001683
Iteration 121/1000 | Loss: 0.00001659
Iteration 122/1000 | Loss: 0.00001633
Iteration 123/1000 | Loss: 0.00001610
Iteration 124/1000 | Loss: 0.00001609
Iteration 125/1000 | Loss: 0.00001605
Iteration 126/1000 | Loss: 0.00001602
Iteration 127/1000 | Loss: 0.00001598
Iteration 128/1000 | Loss: 0.00001597
Iteration 129/1000 | Loss: 0.00001595
Iteration 130/1000 | Loss: 0.00001594
Iteration 131/1000 | Loss: 0.00001593
Iteration 132/1000 | Loss: 0.00001592
Iteration 133/1000 | Loss: 0.00001592
Iteration 134/1000 | Loss: 0.00001591
Iteration 135/1000 | Loss: 0.00001591
Iteration 136/1000 | Loss: 0.00001590
Iteration 137/1000 | Loss: 0.00001590
Iteration 138/1000 | Loss: 0.00001589
Iteration 139/1000 | Loss: 0.00001589
Iteration 140/1000 | Loss: 0.00001589
Iteration 141/1000 | Loss: 0.00001588
Iteration 142/1000 | Loss: 0.00001588
Iteration 143/1000 | Loss: 0.00001588
Iteration 144/1000 | Loss: 0.00001587
Iteration 145/1000 | Loss: 0.00001587
Iteration 146/1000 | Loss: 0.00001586
Iteration 147/1000 | Loss: 0.00001586
Iteration 148/1000 | Loss: 0.00001585
Iteration 149/1000 | Loss: 0.00001582
Iteration 150/1000 | Loss: 0.00001582
Iteration 151/1000 | Loss: 0.00001580
Iteration 152/1000 | Loss: 0.00001579
Iteration 153/1000 | Loss: 0.00001579
Iteration 154/1000 | Loss: 0.00001579
Iteration 155/1000 | Loss: 0.00001579
Iteration 156/1000 | Loss: 0.00001579
Iteration 157/1000 | Loss: 0.00001579
Iteration 158/1000 | Loss: 0.00001579
Iteration 159/1000 | Loss: 0.00001579
Iteration 160/1000 | Loss: 0.00001579
Iteration 161/1000 | Loss: 0.00001579
Iteration 162/1000 | Loss: 0.00001579
Iteration 163/1000 | Loss: 0.00001578
Iteration 164/1000 | Loss: 0.00001578
Iteration 165/1000 | Loss: 0.00001578
Iteration 166/1000 | Loss: 0.00001578
Iteration 167/1000 | Loss: 0.00001578
Iteration 168/1000 | Loss: 0.00001578
Iteration 169/1000 | Loss: 0.00001578
Iteration 170/1000 | Loss: 0.00001578
Iteration 171/1000 | Loss: 0.00001578
Iteration 172/1000 | Loss: 0.00001577
Iteration 173/1000 | Loss: 0.00001577
Iteration 174/1000 | Loss: 0.00001576
Iteration 175/1000 | Loss: 0.00001576
Iteration 176/1000 | Loss: 0.00001576
Iteration 177/1000 | Loss: 0.00001576
Iteration 178/1000 | Loss: 0.00001575
Iteration 179/1000 | Loss: 0.00001575
Iteration 180/1000 | Loss: 0.00001575
Iteration 181/1000 | Loss: 0.00001575
Iteration 182/1000 | Loss: 0.00001574
Iteration 183/1000 | Loss: 0.00001574
Iteration 184/1000 | Loss: 0.00001574
Iteration 185/1000 | Loss: 0.00001573
Iteration 186/1000 | Loss: 0.00001573
Iteration 187/1000 | Loss: 0.00001573
Iteration 188/1000 | Loss: 0.00001572
Iteration 189/1000 | Loss: 0.00001572
Iteration 190/1000 | Loss: 0.00001572
Iteration 191/1000 | Loss: 0.00001572
Iteration 192/1000 | Loss: 0.00001572
Iteration 193/1000 | Loss: 0.00001572
Iteration 194/1000 | Loss: 0.00001572
Iteration 195/1000 | Loss: 0.00001572
Iteration 196/1000 | Loss: 0.00001572
Iteration 197/1000 | Loss: 0.00001572
Iteration 198/1000 | Loss: 0.00001572
Iteration 199/1000 | Loss: 0.00001572
Iteration 200/1000 | Loss: 0.00001572
Iteration 201/1000 | Loss: 0.00001572
Iteration 202/1000 | Loss: 0.00001572
Iteration 203/1000 | Loss: 0.00001572
Iteration 204/1000 | Loss: 0.00001571
Iteration 205/1000 | Loss: 0.00001571
Iteration 206/1000 | Loss: 0.00001571
Iteration 207/1000 | Loss: 0.00001571
Iteration 208/1000 | Loss: 0.00001571
Iteration 209/1000 | Loss: 0.00001571
Iteration 210/1000 | Loss: 0.00001570
Iteration 211/1000 | Loss: 0.00001570
Iteration 212/1000 | Loss: 0.00001570
Iteration 213/1000 | Loss: 0.00001570
Iteration 214/1000 | Loss: 0.00001570
Iteration 215/1000 | Loss: 0.00001570
Iteration 216/1000 | Loss: 0.00001570
Iteration 217/1000 | Loss: 0.00001570
Iteration 218/1000 | Loss: 0.00001570
Iteration 219/1000 | Loss: 0.00001569
Iteration 220/1000 | Loss: 0.00001569
Iteration 221/1000 | Loss: 0.00001569
Iteration 222/1000 | Loss: 0.00001569
Iteration 223/1000 | Loss: 0.00001569
Iteration 224/1000 | Loss: 0.00001569
Iteration 225/1000 | Loss: 0.00001569
Iteration 226/1000 | Loss: 0.00001569
Iteration 227/1000 | Loss: 0.00001569
Iteration 228/1000 | Loss: 0.00001569
Iteration 229/1000 | Loss: 0.00001569
Iteration 230/1000 | Loss: 0.00001569
Iteration 231/1000 | Loss: 0.00001569
Iteration 232/1000 | Loss: 0.00001569
Iteration 233/1000 | Loss: 0.00001569
Iteration 234/1000 | Loss: 0.00001569
Iteration 235/1000 | Loss: 0.00001569
Iteration 236/1000 | Loss: 0.00001569
Iteration 237/1000 | Loss: 0.00001569
Iteration 238/1000 | Loss: 0.00001569
Iteration 239/1000 | Loss: 0.00001569
Iteration 240/1000 | Loss: 0.00001569
Iteration 241/1000 | Loss: 0.00001569
Iteration 242/1000 | Loss: 0.00001569
Iteration 243/1000 | Loss: 0.00001569
Iteration 244/1000 | Loss: 0.00001569
Iteration 245/1000 | Loss: 0.00001569
Iteration 246/1000 | Loss: 0.00001569
Iteration 247/1000 | Loss: 0.00001569
Iteration 248/1000 | Loss: 0.00001569
Iteration 249/1000 | Loss: 0.00001569
Iteration 250/1000 | Loss: 0.00001569
Iteration 251/1000 | Loss: 0.00001569
Iteration 252/1000 | Loss: 0.00001569
Iteration 253/1000 | Loss: 0.00001569
Iteration 254/1000 | Loss: 0.00001569
Iteration 255/1000 | Loss: 0.00001569
Iteration 256/1000 | Loss: 0.00001569
Iteration 257/1000 | Loss: 0.00001569
Iteration 258/1000 | Loss: 0.00001569
Iteration 259/1000 | Loss: 0.00001569
Iteration 260/1000 | Loss: 0.00001569
Iteration 261/1000 | Loss: 0.00001569
Iteration 262/1000 | Loss: 0.00001569
Iteration 263/1000 | Loss: 0.00001569
Iteration 264/1000 | Loss: 0.00001569
Iteration 265/1000 | Loss: 0.00001569
Iteration 266/1000 | Loss: 0.00001569
Iteration 267/1000 | Loss: 0.00001569
Iteration 268/1000 | Loss: 0.00001569
Iteration 269/1000 | Loss: 0.00001569
Iteration 270/1000 | Loss: 0.00001569
Iteration 271/1000 | Loss: 0.00001569
Iteration 272/1000 | Loss: 0.00001569
Iteration 273/1000 | Loss: 0.00001569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [1.5690766304032877e-05, 1.5690766304032877e-05, 1.5690766304032877e-05, 1.5690766304032877e-05, 1.5690766304032877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5690766304032877e-05

Optimization complete. Final v2v error: 3.230565309524536 mm

Highest mean error: 5.07914924621582 mm for frame 6

Lowest mean error: 2.839052677154541 mm for frame 77

Saving results

Total time: 232.90640568733215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839425
Iteration 2/25 | Loss: 0.00147214
Iteration 3/25 | Loss: 0.00126061
Iteration 4/25 | Loss: 0.00124516
Iteration 5/25 | Loss: 0.00124326
Iteration 6/25 | Loss: 0.00124317
Iteration 7/25 | Loss: 0.00124317
Iteration 8/25 | Loss: 0.00124317
Iteration 9/25 | Loss: 0.00124317
Iteration 10/25 | Loss: 0.00124317
Iteration 11/25 | Loss: 0.00124317
Iteration 12/25 | Loss: 0.00124317
Iteration 13/25 | Loss: 0.00124317
Iteration 14/25 | Loss: 0.00124317
Iteration 15/25 | Loss: 0.00124317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001243172213435173, 0.001243172213435173, 0.001243172213435173, 0.001243172213435173, 0.001243172213435173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001243172213435173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77620149
Iteration 2/25 | Loss: 0.00162484
Iteration 3/25 | Loss: 0.00162484
Iteration 4/25 | Loss: 0.00162484
Iteration 5/25 | Loss: 0.00162484
Iteration 6/25 | Loss: 0.00162484
Iteration 7/25 | Loss: 0.00162484
Iteration 8/25 | Loss: 0.00162484
Iteration 9/25 | Loss: 0.00162484
Iteration 10/25 | Loss: 0.00162484
Iteration 11/25 | Loss: 0.00162484
Iteration 12/25 | Loss: 0.00162484
Iteration 13/25 | Loss: 0.00162484
Iteration 14/25 | Loss: 0.00162484
Iteration 15/25 | Loss: 0.00162484
Iteration 16/25 | Loss: 0.00162484
Iteration 17/25 | Loss: 0.00162484
Iteration 18/25 | Loss: 0.00162484
Iteration 19/25 | Loss: 0.00162484
Iteration 20/25 | Loss: 0.00162484
Iteration 21/25 | Loss: 0.00162484
Iteration 22/25 | Loss: 0.00162484
Iteration 23/25 | Loss: 0.00162484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016248379833996296, 0.0016248379833996296, 0.0016248379833996296, 0.0016248379833996296, 0.0016248379833996296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016248379833996296

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162484
Iteration 2/1000 | Loss: 0.00003330
Iteration 3/1000 | Loss: 0.00002314
Iteration 4/1000 | Loss: 0.00001988
Iteration 5/1000 | Loss: 0.00001842
Iteration 6/1000 | Loss: 0.00001764
Iteration 7/1000 | Loss: 0.00001716
Iteration 8/1000 | Loss: 0.00001697
Iteration 9/1000 | Loss: 0.00001677
Iteration 10/1000 | Loss: 0.00001676
Iteration 11/1000 | Loss: 0.00001658
Iteration 12/1000 | Loss: 0.00001653
Iteration 13/1000 | Loss: 0.00001652
Iteration 14/1000 | Loss: 0.00001652
Iteration 15/1000 | Loss: 0.00001652
Iteration 16/1000 | Loss: 0.00001652
Iteration 17/1000 | Loss: 0.00001652
Iteration 18/1000 | Loss: 0.00001651
Iteration 19/1000 | Loss: 0.00001651
Iteration 20/1000 | Loss: 0.00001651
Iteration 21/1000 | Loss: 0.00001649
Iteration 22/1000 | Loss: 0.00001648
Iteration 23/1000 | Loss: 0.00001648
Iteration 24/1000 | Loss: 0.00001648
Iteration 25/1000 | Loss: 0.00001648
Iteration 26/1000 | Loss: 0.00001648
Iteration 27/1000 | Loss: 0.00001648
Iteration 28/1000 | Loss: 0.00001648
Iteration 29/1000 | Loss: 0.00001648
Iteration 30/1000 | Loss: 0.00001648
Iteration 31/1000 | Loss: 0.00001646
Iteration 32/1000 | Loss: 0.00001645
Iteration 33/1000 | Loss: 0.00001645
Iteration 34/1000 | Loss: 0.00001645
Iteration 35/1000 | Loss: 0.00001642
Iteration 36/1000 | Loss: 0.00001641
Iteration 37/1000 | Loss: 0.00001640
Iteration 38/1000 | Loss: 0.00001640
Iteration 39/1000 | Loss: 0.00001639
Iteration 40/1000 | Loss: 0.00001639
Iteration 41/1000 | Loss: 0.00001639
Iteration 42/1000 | Loss: 0.00001639
Iteration 43/1000 | Loss: 0.00001639
Iteration 44/1000 | Loss: 0.00001639
Iteration 45/1000 | Loss: 0.00001639
Iteration 46/1000 | Loss: 0.00001639
Iteration 47/1000 | Loss: 0.00001639
Iteration 48/1000 | Loss: 0.00001639
Iteration 49/1000 | Loss: 0.00001639
Iteration 50/1000 | Loss: 0.00001639
Iteration 51/1000 | Loss: 0.00001638
Iteration 52/1000 | Loss: 0.00001638
Iteration 53/1000 | Loss: 0.00001638
Iteration 54/1000 | Loss: 0.00001637
Iteration 55/1000 | Loss: 0.00001637
Iteration 56/1000 | Loss: 0.00001636
Iteration 57/1000 | Loss: 0.00001636
Iteration 58/1000 | Loss: 0.00001636
Iteration 59/1000 | Loss: 0.00001636
Iteration 60/1000 | Loss: 0.00001636
Iteration 61/1000 | Loss: 0.00001636
Iteration 62/1000 | Loss: 0.00001635
Iteration 63/1000 | Loss: 0.00001634
Iteration 64/1000 | Loss: 0.00001634
Iteration 65/1000 | Loss: 0.00001633
Iteration 66/1000 | Loss: 0.00001633
Iteration 67/1000 | Loss: 0.00001633
Iteration 68/1000 | Loss: 0.00001633
Iteration 69/1000 | Loss: 0.00001633
Iteration 70/1000 | Loss: 0.00001633
Iteration 71/1000 | Loss: 0.00001632
Iteration 72/1000 | Loss: 0.00001632
Iteration 73/1000 | Loss: 0.00001632
Iteration 74/1000 | Loss: 0.00001632
Iteration 75/1000 | Loss: 0.00001632
Iteration 76/1000 | Loss: 0.00001632
Iteration 77/1000 | Loss: 0.00001632
Iteration 78/1000 | Loss: 0.00001632
Iteration 79/1000 | Loss: 0.00001631
Iteration 80/1000 | Loss: 0.00001631
Iteration 81/1000 | Loss: 0.00001631
Iteration 82/1000 | Loss: 0.00001631
Iteration 83/1000 | Loss: 0.00001631
Iteration 84/1000 | Loss: 0.00001631
Iteration 85/1000 | Loss: 0.00001631
Iteration 86/1000 | Loss: 0.00001631
Iteration 87/1000 | Loss: 0.00001630
Iteration 88/1000 | Loss: 0.00001630
Iteration 89/1000 | Loss: 0.00001630
Iteration 90/1000 | Loss: 0.00001630
Iteration 91/1000 | Loss: 0.00001630
Iteration 92/1000 | Loss: 0.00001630
Iteration 93/1000 | Loss: 0.00001630
Iteration 94/1000 | Loss: 0.00001630
Iteration 95/1000 | Loss: 0.00001630
Iteration 96/1000 | Loss: 0.00001630
Iteration 97/1000 | Loss: 0.00001629
Iteration 98/1000 | Loss: 0.00001629
Iteration 99/1000 | Loss: 0.00001629
Iteration 100/1000 | Loss: 0.00001629
Iteration 101/1000 | Loss: 0.00001629
Iteration 102/1000 | Loss: 0.00001629
Iteration 103/1000 | Loss: 0.00001629
Iteration 104/1000 | Loss: 0.00001629
Iteration 105/1000 | Loss: 0.00001629
Iteration 106/1000 | Loss: 0.00001629
Iteration 107/1000 | Loss: 0.00001629
Iteration 108/1000 | Loss: 0.00001629
Iteration 109/1000 | Loss: 0.00001629
Iteration 110/1000 | Loss: 0.00001629
Iteration 111/1000 | Loss: 0.00001629
Iteration 112/1000 | Loss: 0.00001629
Iteration 113/1000 | Loss: 0.00001629
Iteration 114/1000 | Loss: 0.00001629
Iteration 115/1000 | Loss: 0.00001629
Iteration 116/1000 | Loss: 0.00001629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.6291180145344697e-05, 1.6291180145344697e-05, 1.6291180145344697e-05, 1.6291180145344697e-05, 1.6291180145344697e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6291180145344697e-05

Optimization complete. Final v2v error: 3.353790283203125 mm

Highest mean error: 3.4612436294555664 mm for frame 116

Lowest mean error: 3.2290303707122803 mm for frame 132

Saving results

Total time: 28.972422122955322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860253
Iteration 2/25 | Loss: 0.00147681
Iteration 3/25 | Loss: 0.00127347
Iteration 4/25 | Loss: 0.00125367
Iteration 5/25 | Loss: 0.00125031
Iteration 6/25 | Loss: 0.00125028
Iteration 7/25 | Loss: 0.00125028
Iteration 8/25 | Loss: 0.00125028
Iteration 9/25 | Loss: 0.00125028
Iteration 10/25 | Loss: 0.00125028
Iteration 11/25 | Loss: 0.00125028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012502778554335237, 0.0012502778554335237, 0.0012502778554335237, 0.0012502778554335237, 0.0012502778554335237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012502778554335237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78680682
Iteration 2/25 | Loss: 0.00166897
Iteration 3/25 | Loss: 0.00166896
Iteration 4/25 | Loss: 0.00166896
Iteration 5/25 | Loss: 0.00166896
Iteration 6/25 | Loss: 0.00166896
Iteration 7/25 | Loss: 0.00166896
Iteration 8/25 | Loss: 0.00166896
Iteration 9/25 | Loss: 0.00166896
Iteration 10/25 | Loss: 0.00166896
Iteration 11/25 | Loss: 0.00166896
Iteration 12/25 | Loss: 0.00166896
Iteration 13/25 | Loss: 0.00166896
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0016689598560333252, 0.0016689598560333252, 0.0016689598560333252, 0.0016689598560333252, 0.0016689598560333252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016689598560333252

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166896
Iteration 2/1000 | Loss: 0.00003620
Iteration 3/1000 | Loss: 0.00002545
Iteration 4/1000 | Loss: 0.00002110
Iteration 5/1000 | Loss: 0.00001862
Iteration 6/1000 | Loss: 0.00001773
Iteration 7/1000 | Loss: 0.00001734
Iteration 8/1000 | Loss: 0.00001705
Iteration 9/1000 | Loss: 0.00001673
Iteration 10/1000 | Loss: 0.00001650
Iteration 11/1000 | Loss: 0.00001646
Iteration 12/1000 | Loss: 0.00001640
Iteration 13/1000 | Loss: 0.00001634
Iteration 14/1000 | Loss: 0.00001633
Iteration 15/1000 | Loss: 0.00001633
Iteration 16/1000 | Loss: 0.00001632
Iteration 17/1000 | Loss: 0.00001632
Iteration 18/1000 | Loss: 0.00001631
Iteration 19/1000 | Loss: 0.00001629
Iteration 20/1000 | Loss: 0.00001629
Iteration 21/1000 | Loss: 0.00001629
Iteration 22/1000 | Loss: 0.00001629
Iteration 23/1000 | Loss: 0.00001629
Iteration 24/1000 | Loss: 0.00001629
Iteration 25/1000 | Loss: 0.00001629
Iteration 26/1000 | Loss: 0.00001629
Iteration 27/1000 | Loss: 0.00001629
Iteration 28/1000 | Loss: 0.00001629
Iteration 29/1000 | Loss: 0.00001628
Iteration 30/1000 | Loss: 0.00001626
Iteration 31/1000 | Loss: 0.00001625
Iteration 32/1000 | Loss: 0.00001625
Iteration 33/1000 | Loss: 0.00001624
Iteration 34/1000 | Loss: 0.00001621
Iteration 35/1000 | Loss: 0.00001618
Iteration 36/1000 | Loss: 0.00001617
Iteration 37/1000 | Loss: 0.00001615
Iteration 38/1000 | Loss: 0.00001614
Iteration 39/1000 | Loss: 0.00001614
Iteration 40/1000 | Loss: 0.00001614
Iteration 41/1000 | Loss: 0.00001613
Iteration 42/1000 | Loss: 0.00001612
Iteration 43/1000 | Loss: 0.00001610
Iteration 44/1000 | Loss: 0.00001610
Iteration 45/1000 | Loss: 0.00001609
Iteration 46/1000 | Loss: 0.00001609
Iteration 47/1000 | Loss: 0.00001609
Iteration 48/1000 | Loss: 0.00001609
Iteration 49/1000 | Loss: 0.00001609
Iteration 50/1000 | Loss: 0.00001609
Iteration 51/1000 | Loss: 0.00001609
Iteration 52/1000 | Loss: 0.00001609
Iteration 53/1000 | Loss: 0.00001609
Iteration 54/1000 | Loss: 0.00001609
Iteration 55/1000 | Loss: 0.00001609
Iteration 56/1000 | Loss: 0.00001608
Iteration 57/1000 | Loss: 0.00001608
Iteration 58/1000 | Loss: 0.00001608
Iteration 59/1000 | Loss: 0.00001608
Iteration 60/1000 | Loss: 0.00001607
Iteration 61/1000 | Loss: 0.00001607
Iteration 62/1000 | Loss: 0.00001607
Iteration 63/1000 | Loss: 0.00001607
Iteration 64/1000 | Loss: 0.00001606
Iteration 65/1000 | Loss: 0.00001606
Iteration 66/1000 | Loss: 0.00001606
Iteration 67/1000 | Loss: 0.00001605
Iteration 68/1000 | Loss: 0.00001604
Iteration 69/1000 | Loss: 0.00001603
Iteration 70/1000 | Loss: 0.00001603
Iteration 71/1000 | Loss: 0.00001603
Iteration 72/1000 | Loss: 0.00001603
Iteration 73/1000 | Loss: 0.00001603
Iteration 74/1000 | Loss: 0.00001603
Iteration 75/1000 | Loss: 0.00001603
Iteration 76/1000 | Loss: 0.00001603
Iteration 77/1000 | Loss: 0.00001603
Iteration 78/1000 | Loss: 0.00001603
Iteration 79/1000 | Loss: 0.00001603
Iteration 80/1000 | Loss: 0.00001603
Iteration 81/1000 | Loss: 0.00001603
Iteration 82/1000 | Loss: 0.00001603
Iteration 83/1000 | Loss: 0.00001603
Iteration 84/1000 | Loss: 0.00001603
Iteration 85/1000 | Loss: 0.00001603
Iteration 86/1000 | Loss: 0.00001603
Iteration 87/1000 | Loss: 0.00001603
Iteration 88/1000 | Loss: 0.00001603
Iteration 89/1000 | Loss: 0.00001603
Iteration 90/1000 | Loss: 0.00001603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.6029140169848688e-05, 1.6029140169848688e-05, 1.6029140169848688e-05, 1.6029140169848688e-05, 1.6029140169848688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6029140169848688e-05

Optimization complete. Final v2v error: 3.3404247760772705 mm

Highest mean error: 3.4613864421844482 mm for frame 212

Lowest mean error: 3.243417739868164 mm for frame 154

Saving results

Total time: 33.77246832847595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078902
Iteration 2/25 | Loss: 0.01078902
Iteration 3/25 | Loss: 0.01078902
Iteration 4/25 | Loss: 0.00297509
Iteration 5/25 | Loss: 0.00192340
Iteration 6/25 | Loss: 0.00226987
Iteration 7/25 | Loss: 0.00174903
Iteration 8/25 | Loss: 0.00148218
Iteration 9/25 | Loss: 0.00141757
Iteration 10/25 | Loss: 0.00139960
Iteration 11/25 | Loss: 0.00139591
Iteration 12/25 | Loss: 0.00139837
Iteration 13/25 | Loss: 0.00138643
Iteration 14/25 | Loss: 0.00138641
Iteration 15/25 | Loss: 0.00138577
Iteration 16/25 | Loss: 0.00138663
Iteration 17/25 | Loss: 0.00138717
Iteration 18/25 | Loss: 0.00138370
Iteration 19/25 | Loss: 0.00138241
Iteration 20/25 | Loss: 0.00138337
Iteration 21/25 | Loss: 0.00138171
Iteration 22/25 | Loss: 0.00138181
Iteration 23/25 | Loss: 0.00138162
Iteration 24/25 | Loss: 0.00138162
Iteration 25/25 | Loss: 0.00138162

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.48920891
Iteration 2/25 | Loss: 0.00252016
Iteration 3/25 | Loss: 0.00251184
Iteration 4/25 | Loss: 0.00251184
Iteration 5/25 | Loss: 0.00251184
Iteration 6/25 | Loss: 0.00251184
Iteration 7/25 | Loss: 0.00251184
Iteration 8/25 | Loss: 0.00251184
Iteration 9/25 | Loss: 0.00251184
Iteration 10/25 | Loss: 0.00251184
Iteration 11/25 | Loss: 0.00251184
Iteration 12/25 | Loss: 0.00251184
Iteration 13/25 | Loss: 0.00251184
Iteration 14/25 | Loss: 0.00251184
Iteration 15/25 | Loss: 0.00251184
Iteration 16/25 | Loss: 0.00251184
Iteration 17/25 | Loss: 0.00251184
Iteration 18/25 | Loss: 0.00251184
Iteration 19/25 | Loss: 0.00251184
Iteration 20/25 | Loss: 0.00251184
Iteration 21/25 | Loss: 0.00251184
Iteration 22/25 | Loss: 0.00251184
Iteration 23/25 | Loss: 0.00251184
Iteration 24/25 | Loss: 0.00251184
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00251183845102787, 0.00251183845102787, 0.00251183845102787, 0.00251183845102787, 0.00251183845102787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00251183845102787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251184
Iteration 2/1000 | Loss: 0.00028515
Iteration 3/1000 | Loss: 0.00319012
Iteration 4/1000 | Loss: 0.00003043
Iteration 5/1000 | Loss: 0.00004472
Iteration 6/1000 | Loss: 0.00009814
Iteration 7/1000 | Loss: 0.00002151
Iteration 8/1000 | Loss: 0.00002018
Iteration 9/1000 | Loss: 0.00001948
Iteration 10/1000 | Loss: 0.00001894
Iteration 11/1000 | Loss: 0.00001836
Iteration 12/1000 | Loss: 0.00001802
Iteration 13/1000 | Loss: 0.00001777
Iteration 14/1000 | Loss: 0.00001769
Iteration 15/1000 | Loss: 0.00001767
Iteration 16/1000 | Loss: 0.00001767
Iteration 17/1000 | Loss: 0.00001767
Iteration 18/1000 | Loss: 0.00001767
Iteration 19/1000 | Loss: 0.00001767
Iteration 20/1000 | Loss: 0.00001767
Iteration 21/1000 | Loss: 0.00001766
Iteration 22/1000 | Loss: 0.00001766
Iteration 23/1000 | Loss: 0.00001764
Iteration 24/1000 | Loss: 0.00001763
Iteration 25/1000 | Loss: 0.00001763
Iteration 26/1000 | Loss: 0.00001762
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001760
Iteration 29/1000 | Loss: 0.00001760
Iteration 30/1000 | Loss: 0.00001760
Iteration 31/1000 | Loss: 0.00001760
Iteration 32/1000 | Loss: 0.00001760
Iteration 33/1000 | Loss: 0.00001760
Iteration 34/1000 | Loss: 0.00001759
Iteration 35/1000 | Loss: 0.00001756
Iteration 36/1000 | Loss: 0.00001755
Iteration 37/1000 | Loss: 0.00001754
Iteration 38/1000 | Loss: 0.00001754
Iteration 39/1000 | Loss: 0.00001753
Iteration 40/1000 | Loss: 0.00001753
Iteration 41/1000 | Loss: 0.00001753
Iteration 42/1000 | Loss: 0.00001753
Iteration 43/1000 | Loss: 0.00001752
Iteration 44/1000 | Loss: 0.00001752
Iteration 45/1000 | Loss: 0.00001751
Iteration 46/1000 | Loss: 0.00001751
Iteration 47/1000 | Loss: 0.00001751
Iteration 48/1000 | Loss: 0.00001751
Iteration 49/1000 | Loss: 0.00001751
Iteration 50/1000 | Loss: 0.00001750
Iteration 51/1000 | Loss: 0.00001750
Iteration 52/1000 | Loss: 0.00001750
Iteration 53/1000 | Loss: 0.00001749
Iteration 54/1000 | Loss: 0.00001749
Iteration 55/1000 | Loss: 0.00001748
Iteration 56/1000 | Loss: 0.00001747
Iteration 57/1000 | Loss: 0.00001747
Iteration 58/1000 | Loss: 0.00001745
Iteration 59/1000 | Loss: 0.00001745
Iteration 60/1000 | Loss: 0.00001745
Iteration 61/1000 | Loss: 0.00001744
Iteration 62/1000 | Loss: 0.00001744
Iteration 63/1000 | Loss: 0.00001744
Iteration 64/1000 | Loss: 0.00001744
Iteration 65/1000 | Loss: 0.00001744
Iteration 66/1000 | Loss: 0.00001744
Iteration 67/1000 | Loss: 0.00001744
Iteration 68/1000 | Loss: 0.00001744
Iteration 69/1000 | Loss: 0.00001744
Iteration 70/1000 | Loss: 0.00001744
Iteration 71/1000 | Loss: 0.00001744
Iteration 72/1000 | Loss: 0.00001744
Iteration 73/1000 | Loss: 0.00001744
Iteration 74/1000 | Loss: 0.00001744
Iteration 75/1000 | Loss: 0.00001744
Iteration 76/1000 | Loss: 0.00001744
Iteration 77/1000 | Loss: 0.00001744
Iteration 78/1000 | Loss: 0.00001744
Iteration 79/1000 | Loss: 0.00001744
Iteration 80/1000 | Loss: 0.00001744
Iteration 81/1000 | Loss: 0.00001744
Iteration 82/1000 | Loss: 0.00001744
Iteration 83/1000 | Loss: 0.00001744
Iteration 84/1000 | Loss: 0.00001744
Iteration 85/1000 | Loss: 0.00001744
Iteration 86/1000 | Loss: 0.00001744
Iteration 87/1000 | Loss: 0.00001744
Iteration 88/1000 | Loss: 0.00001744
Iteration 89/1000 | Loss: 0.00001744
Iteration 90/1000 | Loss: 0.00001744
Iteration 91/1000 | Loss: 0.00001744
Iteration 92/1000 | Loss: 0.00001744
Iteration 93/1000 | Loss: 0.00001744
Iteration 94/1000 | Loss: 0.00001744
Iteration 95/1000 | Loss: 0.00001744
Iteration 96/1000 | Loss: 0.00001744
Iteration 97/1000 | Loss: 0.00001744
Iteration 98/1000 | Loss: 0.00001744
Iteration 99/1000 | Loss: 0.00001744
Iteration 100/1000 | Loss: 0.00001744
Iteration 101/1000 | Loss: 0.00001744
Iteration 102/1000 | Loss: 0.00001744
Iteration 103/1000 | Loss: 0.00001744
Iteration 104/1000 | Loss: 0.00001744
Iteration 105/1000 | Loss: 0.00001744
Iteration 106/1000 | Loss: 0.00001744
Iteration 107/1000 | Loss: 0.00001744
Iteration 108/1000 | Loss: 0.00001744
Iteration 109/1000 | Loss: 0.00001744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.7439691873732954e-05, 1.7439691873732954e-05, 1.7439691873732954e-05, 1.7439691873732954e-05, 1.7439691873732954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7439691873732954e-05

Optimization complete. Final v2v error: 3.3739352226257324 mm

Highest mean error: 8.186541557312012 mm for frame 228

Lowest mean error: 2.942553997039795 mm for frame 0

Saving results

Total time: 68.4823203086853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00612994
Iteration 2/25 | Loss: 0.00142917
Iteration 3/25 | Loss: 0.00132984
Iteration 4/25 | Loss: 0.00131431
Iteration 5/25 | Loss: 0.00130827
Iteration 6/25 | Loss: 0.00130611
Iteration 7/25 | Loss: 0.00130593
Iteration 8/25 | Loss: 0.00130593
Iteration 9/25 | Loss: 0.00130589
Iteration 10/25 | Loss: 0.00130589
Iteration 11/25 | Loss: 0.00130589
Iteration 12/25 | Loss: 0.00130589
Iteration 13/25 | Loss: 0.00130589
Iteration 14/25 | Loss: 0.00130589
Iteration 15/25 | Loss: 0.00130589
Iteration 16/25 | Loss: 0.00130589
Iteration 17/25 | Loss: 0.00130589
Iteration 18/25 | Loss: 0.00130589
Iteration 19/25 | Loss: 0.00130589
Iteration 20/25 | Loss: 0.00130589
Iteration 21/25 | Loss: 0.00130589
Iteration 22/25 | Loss: 0.00130589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013058948097750545, 0.0013058948097750545, 0.0013058948097750545, 0.0013058948097750545, 0.0013058948097750545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013058948097750545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14104807
Iteration 2/25 | Loss: 0.00556395
Iteration 3/25 | Loss: 0.00556395
Iteration 4/25 | Loss: 0.00556395
Iteration 5/25 | Loss: 0.00556395
Iteration 6/25 | Loss: 0.00556394
Iteration 7/25 | Loss: 0.00556394
Iteration 8/25 | Loss: 0.00556394
Iteration 9/25 | Loss: 0.00556394
Iteration 10/25 | Loss: 0.00556394
Iteration 11/25 | Loss: 0.00556394
Iteration 12/25 | Loss: 0.00556394
Iteration 13/25 | Loss: 0.00556394
Iteration 14/25 | Loss: 0.00556394
Iteration 15/25 | Loss: 0.00556394
Iteration 16/25 | Loss: 0.00556394
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0055639417842030525, 0.0055639417842030525, 0.0055639417842030525, 0.0055639417842030525, 0.0055639417842030525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0055639417842030525

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00556394
Iteration 2/1000 | Loss: 0.00006842
Iteration 3/1000 | Loss: 0.00003836
Iteration 4/1000 | Loss: 0.00002795
Iteration 5/1000 | Loss: 0.00002341
Iteration 6/1000 | Loss: 0.00002058
Iteration 7/1000 | Loss: 0.00001966
Iteration 8/1000 | Loss: 0.00001911
Iteration 9/1000 | Loss: 0.00001865
Iteration 10/1000 | Loss: 0.00001830
Iteration 11/1000 | Loss: 0.00001797
Iteration 12/1000 | Loss: 0.00001768
Iteration 13/1000 | Loss: 0.00001757
Iteration 14/1000 | Loss: 0.00001740
Iteration 15/1000 | Loss: 0.00001725
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001718
Iteration 18/1000 | Loss: 0.00001718
Iteration 19/1000 | Loss: 0.00001717
Iteration 20/1000 | Loss: 0.00001712
Iteration 21/1000 | Loss: 0.00001711
Iteration 22/1000 | Loss: 0.00001710
Iteration 23/1000 | Loss: 0.00001709
Iteration 24/1000 | Loss: 0.00001708
Iteration 25/1000 | Loss: 0.00001707
Iteration 26/1000 | Loss: 0.00001705
Iteration 27/1000 | Loss: 0.00001704
Iteration 28/1000 | Loss: 0.00001703
Iteration 29/1000 | Loss: 0.00001703
Iteration 30/1000 | Loss: 0.00001702
Iteration 31/1000 | Loss: 0.00001702
Iteration 32/1000 | Loss: 0.00001702
Iteration 33/1000 | Loss: 0.00001702
Iteration 34/1000 | Loss: 0.00001702
Iteration 35/1000 | Loss: 0.00001702
Iteration 36/1000 | Loss: 0.00001702
Iteration 37/1000 | Loss: 0.00001702
Iteration 38/1000 | Loss: 0.00001702
Iteration 39/1000 | Loss: 0.00001701
Iteration 40/1000 | Loss: 0.00001701
Iteration 41/1000 | Loss: 0.00001701
Iteration 42/1000 | Loss: 0.00001701
Iteration 43/1000 | Loss: 0.00001701
Iteration 44/1000 | Loss: 0.00001701
Iteration 45/1000 | Loss: 0.00001700
Iteration 46/1000 | Loss: 0.00001699
Iteration 47/1000 | Loss: 0.00001699
Iteration 48/1000 | Loss: 0.00001698
Iteration 49/1000 | Loss: 0.00001698
Iteration 50/1000 | Loss: 0.00001698
Iteration 51/1000 | Loss: 0.00001695
Iteration 52/1000 | Loss: 0.00001695
Iteration 53/1000 | Loss: 0.00001693
Iteration 54/1000 | Loss: 0.00001693
Iteration 55/1000 | Loss: 0.00001692
Iteration 56/1000 | Loss: 0.00001692
Iteration 57/1000 | Loss: 0.00001692
Iteration 58/1000 | Loss: 0.00001692
Iteration 59/1000 | Loss: 0.00001691
Iteration 60/1000 | Loss: 0.00001691
Iteration 61/1000 | Loss: 0.00001691
Iteration 62/1000 | Loss: 0.00001690
Iteration 63/1000 | Loss: 0.00001690
Iteration 64/1000 | Loss: 0.00001690
Iteration 65/1000 | Loss: 0.00001689
Iteration 66/1000 | Loss: 0.00001689
Iteration 67/1000 | Loss: 0.00001689
Iteration 68/1000 | Loss: 0.00001688
Iteration 69/1000 | Loss: 0.00001688
Iteration 70/1000 | Loss: 0.00001688
Iteration 71/1000 | Loss: 0.00001688
Iteration 72/1000 | Loss: 0.00001688
Iteration 73/1000 | Loss: 0.00001688
Iteration 74/1000 | Loss: 0.00001688
Iteration 75/1000 | Loss: 0.00001687
Iteration 76/1000 | Loss: 0.00001687
Iteration 77/1000 | Loss: 0.00001687
Iteration 78/1000 | Loss: 0.00001686
Iteration 79/1000 | Loss: 0.00001686
Iteration 80/1000 | Loss: 0.00001686
Iteration 81/1000 | Loss: 0.00001686
Iteration 82/1000 | Loss: 0.00001686
Iteration 83/1000 | Loss: 0.00001686
Iteration 84/1000 | Loss: 0.00001685
Iteration 85/1000 | Loss: 0.00001685
Iteration 86/1000 | Loss: 0.00001685
Iteration 87/1000 | Loss: 0.00001685
Iteration 88/1000 | Loss: 0.00001684
Iteration 89/1000 | Loss: 0.00001684
Iteration 90/1000 | Loss: 0.00001684
Iteration 91/1000 | Loss: 0.00001684
Iteration 92/1000 | Loss: 0.00001684
Iteration 93/1000 | Loss: 0.00001684
Iteration 94/1000 | Loss: 0.00001683
Iteration 95/1000 | Loss: 0.00001683
Iteration 96/1000 | Loss: 0.00001683
Iteration 97/1000 | Loss: 0.00001682
Iteration 98/1000 | Loss: 0.00001682
Iteration 99/1000 | Loss: 0.00001682
Iteration 100/1000 | Loss: 0.00001682
Iteration 101/1000 | Loss: 0.00001682
Iteration 102/1000 | Loss: 0.00001682
Iteration 103/1000 | Loss: 0.00001682
Iteration 104/1000 | Loss: 0.00001682
Iteration 105/1000 | Loss: 0.00001682
Iteration 106/1000 | Loss: 0.00001681
Iteration 107/1000 | Loss: 0.00001681
Iteration 108/1000 | Loss: 0.00001681
Iteration 109/1000 | Loss: 0.00001681
Iteration 110/1000 | Loss: 0.00001681
Iteration 111/1000 | Loss: 0.00001680
Iteration 112/1000 | Loss: 0.00001680
Iteration 113/1000 | Loss: 0.00001680
Iteration 114/1000 | Loss: 0.00001679
Iteration 115/1000 | Loss: 0.00001679
Iteration 116/1000 | Loss: 0.00001679
Iteration 117/1000 | Loss: 0.00001679
Iteration 118/1000 | Loss: 0.00001679
Iteration 119/1000 | Loss: 0.00001679
Iteration 120/1000 | Loss: 0.00001679
Iteration 121/1000 | Loss: 0.00001679
Iteration 122/1000 | Loss: 0.00001679
Iteration 123/1000 | Loss: 0.00001679
Iteration 124/1000 | Loss: 0.00001678
Iteration 125/1000 | Loss: 0.00001678
Iteration 126/1000 | Loss: 0.00001678
Iteration 127/1000 | Loss: 0.00001678
Iteration 128/1000 | Loss: 0.00001677
Iteration 129/1000 | Loss: 0.00001677
Iteration 130/1000 | Loss: 0.00001677
Iteration 131/1000 | Loss: 0.00001677
Iteration 132/1000 | Loss: 0.00001677
Iteration 133/1000 | Loss: 0.00001677
Iteration 134/1000 | Loss: 0.00001677
Iteration 135/1000 | Loss: 0.00001677
Iteration 136/1000 | Loss: 0.00001677
Iteration 137/1000 | Loss: 0.00001677
Iteration 138/1000 | Loss: 0.00001677
Iteration 139/1000 | Loss: 0.00001676
Iteration 140/1000 | Loss: 0.00001676
Iteration 141/1000 | Loss: 0.00001676
Iteration 142/1000 | Loss: 0.00001676
Iteration 143/1000 | Loss: 0.00001676
Iteration 144/1000 | Loss: 0.00001675
Iteration 145/1000 | Loss: 0.00001675
Iteration 146/1000 | Loss: 0.00001675
Iteration 147/1000 | Loss: 0.00001675
Iteration 148/1000 | Loss: 0.00001675
Iteration 149/1000 | Loss: 0.00001675
Iteration 150/1000 | Loss: 0.00001675
Iteration 151/1000 | Loss: 0.00001675
Iteration 152/1000 | Loss: 0.00001674
Iteration 153/1000 | Loss: 0.00001674
Iteration 154/1000 | Loss: 0.00001674
Iteration 155/1000 | Loss: 0.00001674
Iteration 156/1000 | Loss: 0.00001674
Iteration 157/1000 | Loss: 0.00001673
Iteration 158/1000 | Loss: 0.00001673
Iteration 159/1000 | Loss: 0.00001673
Iteration 160/1000 | Loss: 0.00001673
Iteration 161/1000 | Loss: 0.00001672
Iteration 162/1000 | Loss: 0.00001672
Iteration 163/1000 | Loss: 0.00001672
Iteration 164/1000 | Loss: 0.00001671
Iteration 165/1000 | Loss: 0.00001671
Iteration 166/1000 | Loss: 0.00001671
Iteration 167/1000 | Loss: 0.00001671
Iteration 168/1000 | Loss: 0.00001670
Iteration 169/1000 | Loss: 0.00001670
Iteration 170/1000 | Loss: 0.00001670
Iteration 171/1000 | Loss: 0.00001670
Iteration 172/1000 | Loss: 0.00001670
Iteration 173/1000 | Loss: 0.00001670
Iteration 174/1000 | Loss: 0.00001670
Iteration 175/1000 | Loss: 0.00001670
Iteration 176/1000 | Loss: 0.00001670
Iteration 177/1000 | Loss: 0.00001669
Iteration 178/1000 | Loss: 0.00001669
Iteration 179/1000 | Loss: 0.00001669
Iteration 180/1000 | Loss: 0.00001669
Iteration 181/1000 | Loss: 0.00001669
Iteration 182/1000 | Loss: 0.00001669
Iteration 183/1000 | Loss: 0.00001668
Iteration 184/1000 | Loss: 0.00001668
Iteration 185/1000 | Loss: 0.00001668
Iteration 186/1000 | Loss: 0.00001668
Iteration 187/1000 | Loss: 0.00001668
Iteration 188/1000 | Loss: 0.00001668
Iteration 189/1000 | Loss: 0.00001668
Iteration 190/1000 | Loss: 0.00001667
Iteration 191/1000 | Loss: 0.00001667
Iteration 192/1000 | Loss: 0.00001667
Iteration 193/1000 | Loss: 0.00001667
Iteration 194/1000 | Loss: 0.00001667
Iteration 195/1000 | Loss: 0.00001667
Iteration 196/1000 | Loss: 0.00001667
Iteration 197/1000 | Loss: 0.00001667
Iteration 198/1000 | Loss: 0.00001667
Iteration 199/1000 | Loss: 0.00001667
Iteration 200/1000 | Loss: 0.00001666
Iteration 201/1000 | Loss: 0.00001666
Iteration 202/1000 | Loss: 0.00001666
Iteration 203/1000 | Loss: 0.00001666
Iteration 204/1000 | Loss: 0.00001666
Iteration 205/1000 | Loss: 0.00001666
Iteration 206/1000 | Loss: 0.00001666
Iteration 207/1000 | Loss: 0.00001665
Iteration 208/1000 | Loss: 0.00001665
Iteration 209/1000 | Loss: 0.00001665
Iteration 210/1000 | Loss: 0.00001664
Iteration 211/1000 | Loss: 0.00001664
Iteration 212/1000 | Loss: 0.00001664
Iteration 213/1000 | Loss: 0.00001664
Iteration 214/1000 | Loss: 0.00001664
Iteration 215/1000 | Loss: 0.00001663
Iteration 216/1000 | Loss: 0.00001663
Iteration 217/1000 | Loss: 0.00001663
Iteration 218/1000 | Loss: 0.00001663
Iteration 219/1000 | Loss: 0.00001663
Iteration 220/1000 | Loss: 0.00001663
Iteration 221/1000 | Loss: 0.00001662
Iteration 222/1000 | Loss: 0.00001662
Iteration 223/1000 | Loss: 0.00001662
Iteration 224/1000 | Loss: 0.00001662
Iteration 225/1000 | Loss: 0.00001662
Iteration 226/1000 | Loss: 0.00001662
Iteration 227/1000 | Loss: 0.00001662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.6624444469925947e-05, 1.6624444469925947e-05, 1.6624444469925947e-05, 1.6624444469925947e-05, 1.6624444469925947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6624444469925947e-05

Optimization complete. Final v2v error: 3.4481284618377686 mm

Highest mean error: 3.7764711380004883 mm for frame 24

Lowest mean error: 3.129512310028076 mm for frame 58

Saving results

Total time: 46.50143074989319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473609
Iteration 2/25 | Loss: 0.00141925
Iteration 3/25 | Loss: 0.00126684
Iteration 4/25 | Loss: 0.00125251
Iteration 5/25 | Loss: 0.00124870
Iteration 6/25 | Loss: 0.00124870
Iteration 7/25 | Loss: 0.00124870
Iteration 8/25 | Loss: 0.00124870
Iteration 9/25 | Loss: 0.00124870
Iteration 10/25 | Loss: 0.00124870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012487014755606651, 0.0012487014755606651, 0.0012487014755606651, 0.0012487014755606651, 0.0012487014755606651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012487014755606651

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.69484878
Iteration 2/25 | Loss: 0.00261688
Iteration 3/25 | Loss: 0.00261688
Iteration 4/25 | Loss: 0.00261688
Iteration 5/25 | Loss: 0.00261688
Iteration 6/25 | Loss: 0.00261687
Iteration 7/25 | Loss: 0.00261687
Iteration 8/25 | Loss: 0.00261687
Iteration 9/25 | Loss: 0.00261687
Iteration 10/25 | Loss: 0.00261687
Iteration 11/25 | Loss: 0.00261687
Iteration 12/25 | Loss: 0.00261687
Iteration 13/25 | Loss: 0.00261687
Iteration 14/25 | Loss: 0.00261687
Iteration 15/25 | Loss: 0.00261687
Iteration 16/25 | Loss: 0.00261687
Iteration 17/25 | Loss: 0.00261687
Iteration 18/25 | Loss: 0.00261687
Iteration 19/25 | Loss: 0.00261687
Iteration 20/25 | Loss: 0.00261687
Iteration 21/25 | Loss: 0.00261687
Iteration 22/25 | Loss: 0.00261687
Iteration 23/25 | Loss: 0.00261687
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002616873010993004, 0.002616873010993004, 0.002616873010993004, 0.002616873010993004, 0.002616873010993004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002616873010993004

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00261687
Iteration 2/1000 | Loss: 0.00004717
Iteration 3/1000 | Loss: 0.00003251
Iteration 4/1000 | Loss: 0.00002759
Iteration 5/1000 | Loss: 0.00002489
Iteration 6/1000 | Loss: 0.00002351
Iteration 7/1000 | Loss: 0.00002289
Iteration 8/1000 | Loss: 0.00002234
Iteration 9/1000 | Loss: 0.00002181
Iteration 10/1000 | Loss: 0.00002146
Iteration 11/1000 | Loss: 0.00002110
Iteration 12/1000 | Loss: 0.00002083
Iteration 13/1000 | Loss: 0.00002063
Iteration 14/1000 | Loss: 0.00002043
Iteration 15/1000 | Loss: 0.00002024
Iteration 16/1000 | Loss: 0.00002010
Iteration 17/1000 | Loss: 0.00001998
Iteration 18/1000 | Loss: 0.00001981
Iteration 19/1000 | Loss: 0.00001966
Iteration 20/1000 | Loss: 0.00001948
Iteration 21/1000 | Loss: 0.00001933
Iteration 22/1000 | Loss: 0.00001928
Iteration 23/1000 | Loss: 0.00001919
Iteration 24/1000 | Loss: 0.00001914
Iteration 25/1000 | Loss: 0.00001913
Iteration 26/1000 | Loss: 0.00001910
Iteration 27/1000 | Loss: 0.00001909
Iteration 28/1000 | Loss: 0.00001907
Iteration 29/1000 | Loss: 0.00001906
Iteration 30/1000 | Loss: 0.00001906
Iteration 31/1000 | Loss: 0.00001905
Iteration 32/1000 | Loss: 0.00001904
Iteration 33/1000 | Loss: 0.00001904
Iteration 34/1000 | Loss: 0.00001903
Iteration 35/1000 | Loss: 0.00001900
Iteration 36/1000 | Loss: 0.00001899
Iteration 37/1000 | Loss: 0.00001897
Iteration 38/1000 | Loss: 0.00001896
Iteration 39/1000 | Loss: 0.00001894
Iteration 40/1000 | Loss: 0.00001894
Iteration 41/1000 | Loss: 0.00001894
Iteration 42/1000 | Loss: 0.00001894
Iteration 43/1000 | Loss: 0.00001894
Iteration 44/1000 | Loss: 0.00001894
Iteration 45/1000 | Loss: 0.00001894
Iteration 46/1000 | Loss: 0.00001893
Iteration 47/1000 | Loss: 0.00001890
Iteration 48/1000 | Loss: 0.00001889
Iteration 49/1000 | Loss: 0.00001888
Iteration 50/1000 | Loss: 0.00001888
Iteration 51/1000 | Loss: 0.00001888
Iteration 52/1000 | Loss: 0.00001887
Iteration 53/1000 | Loss: 0.00001887
Iteration 54/1000 | Loss: 0.00001887
Iteration 55/1000 | Loss: 0.00001887
Iteration 56/1000 | Loss: 0.00001887
Iteration 57/1000 | Loss: 0.00001886
Iteration 58/1000 | Loss: 0.00001886
Iteration 59/1000 | Loss: 0.00001886
Iteration 60/1000 | Loss: 0.00001886
Iteration 61/1000 | Loss: 0.00001886
Iteration 62/1000 | Loss: 0.00001885
Iteration 63/1000 | Loss: 0.00001885
Iteration 64/1000 | Loss: 0.00001885
Iteration 65/1000 | Loss: 0.00001885
Iteration 66/1000 | Loss: 0.00001885
Iteration 67/1000 | Loss: 0.00001885
Iteration 68/1000 | Loss: 0.00001885
Iteration 69/1000 | Loss: 0.00001884
Iteration 70/1000 | Loss: 0.00001884
Iteration 71/1000 | Loss: 0.00001884
Iteration 72/1000 | Loss: 0.00001884
Iteration 73/1000 | Loss: 0.00001884
Iteration 74/1000 | Loss: 0.00001884
Iteration 75/1000 | Loss: 0.00001884
Iteration 76/1000 | Loss: 0.00001884
Iteration 77/1000 | Loss: 0.00001884
Iteration 78/1000 | Loss: 0.00001884
Iteration 79/1000 | Loss: 0.00001884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.8841581550077535e-05, 1.8841581550077535e-05, 1.8841581550077535e-05, 1.8841581550077535e-05, 1.8841581550077535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8841581550077535e-05

Optimization complete. Final v2v error: 3.622791290283203 mm

Highest mean error: 4.239675521850586 mm for frame 9

Lowest mean error: 3.2642273902893066 mm for frame 62

Saving results

Total time: 52.461299657821655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00370361
Iteration 2/25 | Loss: 0.00148885
Iteration 3/25 | Loss: 0.00128541
Iteration 4/25 | Loss: 0.00126044
Iteration 5/25 | Loss: 0.00125585
Iteration 6/25 | Loss: 0.00125486
Iteration 7/25 | Loss: 0.00125486
Iteration 8/25 | Loss: 0.00125486
Iteration 9/25 | Loss: 0.00125486
Iteration 10/25 | Loss: 0.00125486
Iteration 11/25 | Loss: 0.00125486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001254861825145781, 0.001254861825145781, 0.001254861825145781, 0.001254861825145781, 0.001254861825145781]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001254861825145781

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21414590
Iteration 2/25 | Loss: 0.00395348
Iteration 3/25 | Loss: 0.00395347
Iteration 4/25 | Loss: 0.00395347
Iteration 5/25 | Loss: 0.00395347
Iteration 6/25 | Loss: 0.00395347
Iteration 7/25 | Loss: 0.00395347
Iteration 8/25 | Loss: 0.00395347
Iteration 9/25 | Loss: 0.00395347
Iteration 10/25 | Loss: 0.00395347
Iteration 11/25 | Loss: 0.00395347
Iteration 12/25 | Loss: 0.00395347
Iteration 13/25 | Loss: 0.00395347
Iteration 14/25 | Loss: 0.00395347
Iteration 15/25 | Loss: 0.00395347
Iteration 16/25 | Loss: 0.00395347
Iteration 17/25 | Loss: 0.00395347
Iteration 18/25 | Loss: 0.00395347
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003953471779823303, 0.003953471779823303, 0.003953471779823303, 0.003953471779823303, 0.003953471779823303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003953471779823303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00395347
Iteration 2/1000 | Loss: 0.00006592
Iteration 3/1000 | Loss: 0.00003664
Iteration 4/1000 | Loss: 0.00002407
Iteration 5/1000 | Loss: 0.00001941
Iteration 6/1000 | Loss: 0.00001724
Iteration 7/1000 | Loss: 0.00001580
Iteration 8/1000 | Loss: 0.00001513
Iteration 9/1000 | Loss: 0.00001448
Iteration 10/1000 | Loss: 0.00001413
Iteration 11/1000 | Loss: 0.00001379
Iteration 12/1000 | Loss: 0.00001354
Iteration 13/1000 | Loss: 0.00001331
Iteration 14/1000 | Loss: 0.00001314
Iteration 15/1000 | Loss: 0.00001313
Iteration 16/1000 | Loss: 0.00001312
Iteration 17/1000 | Loss: 0.00001312
Iteration 18/1000 | Loss: 0.00001310
Iteration 19/1000 | Loss: 0.00001309
Iteration 20/1000 | Loss: 0.00001303
Iteration 21/1000 | Loss: 0.00001302
Iteration 22/1000 | Loss: 0.00001300
Iteration 23/1000 | Loss: 0.00001300
Iteration 24/1000 | Loss: 0.00001300
Iteration 25/1000 | Loss: 0.00001300
Iteration 26/1000 | Loss: 0.00001300
Iteration 27/1000 | Loss: 0.00001300
Iteration 28/1000 | Loss: 0.00001299
Iteration 29/1000 | Loss: 0.00001299
Iteration 30/1000 | Loss: 0.00001299
Iteration 31/1000 | Loss: 0.00001299
Iteration 32/1000 | Loss: 0.00001299
Iteration 33/1000 | Loss: 0.00001294
Iteration 34/1000 | Loss: 0.00001293
Iteration 35/1000 | Loss: 0.00001293
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001292
Iteration 38/1000 | Loss: 0.00001292
Iteration 39/1000 | Loss: 0.00001291
Iteration 40/1000 | Loss: 0.00001291
Iteration 41/1000 | Loss: 0.00001291
Iteration 42/1000 | Loss: 0.00001291
Iteration 43/1000 | Loss: 0.00001290
Iteration 44/1000 | Loss: 0.00001290
Iteration 45/1000 | Loss: 0.00001290
Iteration 46/1000 | Loss: 0.00001290
Iteration 47/1000 | Loss: 0.00001290
Iteration 48/1000 | Loss: 0.00001289
Iteration 49/1000 | Loss: 0.00001289
Iteration 50/1000 | Loss: 0.00001289
Iteration 51/1000 | Loss: 0.00001288
Iteration 52/1000 | Loss: 0.00001288
Iteration 53/1000 | Loss: 0.00001288
Iteration 54/1000 | Loss: 0.00001288
Iteration 55/1000 | Loss: 0.00001288
Iteration 56/1000 | Loss: 0.00001288
Iteration 57/1000 | Loss: 0.00001288
Iteration 58/1000 | Loss: 0.00001287
Iteration 59/1000 | Loss: 0.00001287
Iteration 60/1000 | Loss: 0.00001287
Iteration 61/1000 | Loss: 0.00001287
Iteration 62/1000 | Loss: 0.00001287
Iteration 63/1000 | Loss: 0.00001287
Iteration 64/1000 | Loss: 0.00001287
Iteration 65/1000 | Loss: 0.00001287
Iteration 66/1000 | Loss: 0.00001287
Iteration 67/1000 | Loss: 0.00001287
Iteration 68/1000 | Loss: 0.00001286
Iteration 69/1000 | Loss: 0.00001286
Iteration 70/1000 | Loss: 0.00001286
Iteration 71/1000 | Loss: 0.00001286
Iteration 72/1000 | Loss: 0.00001286
Iteration 73/1000 | Loss: 0.00001286
Iteration 74/1000 | Loss: 0.00001285
Iteration 75/1000 | Loss: 0.00001285
Iteration 76/1000 | Loss: 0.00001285
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001285
Iteration 82/1000 | Loss: 0.00001285
Iteration 83/1000 | Loss: 0.00001285
Iteration 84/1000 | Loss: 0.00001285
Iteration 85/1000 | Loss: 0.00001285
Iteration 86/1000 | Loss: 0.00001284
Iteration 87/1000 | Loss: 0.00001284
Iteration 88/1000 | Loss: 0.00001284
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001284
Iteration 92/1000 | Loss: 0.00001284
Iteration 93/1000 | Loss: 0.00001284
Iteration 94/1000 | Loss: 0.00001284
Iteration 95/1000 | Loss: 0.00001284
Iteration 96/1000 | Loss: 0.00001284
Iteration 97/1000 | Loss: 0.00001284
Iteration 98/1000 | Loss: 0.00001284
Iteration 99/1000 | Loss: 0.00001284
Iteration 100/1000 | Loss: 0.00001284
Iteration 101/1000 | Loss: 0.00001284
Iteration 102/1000 | Loss: 0.00001283
Iteration 103/1000 | Loss: 0.00001283
Iteration 104/1000 | Loss: 0.00001283
Iteration 105/1000 | Loss: 0.00001283
Iteration 106/1000 | Loss: 0.00001283
Iteration 107/1000 | Loss: 0.00001283
Iteration 108/1000 | Loss: 0.00001283
Iteration 109/1000 | Loss: 0.00001283
Iteration 110/1000 | Loss: 0.00001283
Iteration 111/1000 | Loss: 0.00001283
Iteration 112/1000 | Loss: 0.00001283
Iteration 113/1000 | Loss: 0.00001283
Iteration 114/1000 | Loss: 0.00001283
Iteration 115/1000 | Loss: 0.00001283
Iteration 116/1000 | Loss: 0.00001283
Iteration 117/1000 | Loss: 0.00001283
Iteration 118/1000 | Loss: 0.00001283
Iteration 119/1000 | Loss: 0.00001283
Iteration 120/1000 | Loss: 0.00001283
Iteration 121/1000 | Loss: 0.00001283
Iteration 122/1000 | Loss: 0.00001282
Iteration 123/1000 | Loss: 0.00001282
Iteration 124/1000 | Loss: 0.00001282
Iteration 125/1000 | Loss: 0.00001282
Iteration 126/1000 | Loss: 0.00001282
Iteration 127/1000 | Loss: 0.00001282
Iteration 128/1000 | Loss: 0.00001282
Iteration 129/1000 | Loss: 0.00001282
Iteration 130/1000 | Loss: 0.00001282
Iteration 131/1000 | Loss: 0.00001282
Iteration 132/1000 | Loss: 0.00001282
Iteration 133/1000 | Loss: 0.00001282
Iteration 134/1000 | Loss: 0.00001282
Iteration 135/1000 | Loss: 0.00001282
Iteration 136/1000 | Loss: 0.00001282
Iteration 137/1000 | Loss: 0.00001282
Iteration 138/1000 | Loss: 0.00001281
Iteration 139/1000 | Loss: 0.00001281
Iteration 140/1000 | Loss: 0.00001281
Iteration 141/1000 | Loss: 0.00001281
Iteration 142/1000 | Loss: 0.00001281
Iteration 143/1000 | Loss: 0.00001281
Iteration 144/1000 | Loss: 0.00001281
Iteration 145/1000 | Loss: 0.00001281
Iteration 146/1000 | Loss: 0.00001281
Iteration 147/1000 | Loss: 0.00001281
Iteration 148/1000 | Loss: 0.00001281
Iteration 149/1000 | Loss: 0.00001281
Iteration 150/1000 | Loss: 0.00001281
Iteration 151/1000 | Loss: 0.00001280
Iteration 152/1000 | Loss: 0.00001280
Iteration 153/1000 | Loss: 0.00001280
Iteration 154/1000 | Loss: 0.00001280
Iteration 155/1000 | Loss: 0.00001280
Iteration 156/1000 | Loss: 0.00001280
Iteration 157/1000 | Loss: 0.00001279
Iteration 158/1000 | Loss: 0.00001279
Iteration 159/1000 | Loss: 0.00001279
Iteration 160/1000 | Loss: 0.00001279
Iteration 161/1000 | Loss: 0.00001279
Iteration 162/1000 | Loss: 0.00001279
Iteration 163/1000 | Loss: 0.00001279
Iteration 164/1000 | Loss: 0.00001279
Iteration 165/1000 | Loss: 0.00001279
Iteration 166/1000 | Loss: 0.00001279
Iteration 167/1000 | Loss: 0.00001279
Iteration 168/1000 | Loss: 0.00001279
Iteration 169/1000 | Loss: 0.00001279
Iteration 170/1000 | Loss: 0.00001279
Iteration 171/1000 | Loss: 0.00001279
Iteration 172/1000 | Loss: 0.00001279
Iteration 173/1000 | Loss: 0.00001279
Iteration 174/1000 | Loss: 0.00001278
Iteration 175/1000 | Loss: 0.00001278
Iteration 176/1000 | Loss: 0.00001278
Iteration 177/1000 | Loss: 0.00001278
Iteration 178/1000 | Loss: 0.00001278
Iteration 179/1000 | Loss: 0.00001278
Iteration 180/1000 | Loss: 0.00001278
Iteration 181/1000 | Loss: 0.00001277
Iteration 182/1000 | Loss: 0.00001277
Iteration 183/1000 | Loss: 0.00001277
Iteration 184/1000 | Loss: 0.00001277
Iteration 185/1000 | Loss: 0.00001277
Iteration 186/1000 | Loss: 0.00001277
Iteration 187/1000 | Loss: 0.00001277
Iteration 188/1000 | Loss: 0.00001276
Iteration 189/1000 | Loss: 0.00001276
Iteration 190/1000 | Loss: 0.00001276
Iteration 191/1000 | Loss: 0.00001276
Iteration 192/1000 | Loss: 0.00001276
Iteration 193/1000 | Loss: 0.00001276
Iteration 194/1000 | Loss: 0.00001276
Iteration 195/1000 | Loss: 0.00001276
Iteration 196/1000 | Loss: 0.00001276
Iteration 197/1000 | Loss: 0.00001276
Iteration 198/1000 | Loss: 0.00001276
Iteration 199/1000 | Loss: 0.00001276
Iteration 200/1000 | Loss: 0.00001276
Iteration 201/1000 | Loss: 0.00001276
Iteration 202/1000 | Loss: 0.00001276
Iteration 203/1000 | Loss: 0.00001276
Iteration 204/1000 | Loss: 0.00001276
Iteration 205/1000 | Loss: 0.00001276
Iteration 206/1000 | Loss: 0.00001276
Iteration 207/1000 | Loss: 0.00001276
Iteration 208/1000 | Loss: 0.00001275
Iteration 209/1000 | Loss: 0.00001275
Iteration 210/1000 | Loss: 0.00001275
Iteration 211/1000 | Loss: 0.00001275
Iteration 212/1000 | Loss: 0.00001275
Iteration 213/1000 | Loss: 0.00001275
Iteration 214/1000 | Loss: 0.00001275
Iteration 215/1000 | Loss: 0.00001275
Iteration 216/1000 | Loss: 0.00001275
Iteration 217/1000 | Loss: 0.00001275
Iteration 218/1000 | Loss: 0.00001275
Iteration 219/1000 | Loss: 0.00001275
Iteration 220/1000 | Loss: 0.00001275
Iteration 221/1000 | Loss: 0.00001275
Iteration 222/1000 | Loss: 0.00001275
Iteration 223/1000 | Loss: 0.00001275
Iteration 224/1000 | Loss: 0.00001275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.2748106200888287e-05, 1.2748106200888287e-05, 1.2748106200888287e-05, 1.2748106200888287e-05, 1.2748106200888287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2748106200888287e-05

Optimization complete. Final v2v error: 3.007936477661133 mm

Highest mean error: 3.2945406436920166 mm for frame 43

Lowest mean error: 2.4899282455444336 mm for frame 140

Saving results

Total time: 41.68402647972107
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998542
Iteration 2/25 | Loss: 0.00258754
Iteration 3/25 | Loss: 0.00206280
Iteration 4/25 | Loss: 0.00199325
Iteration 5/25 | Loss: 0.00202496
Iteration 6/25 | Loss: 0.00196521
Iteration 7/25 | Loss: 0.00192167
Iteration 8/25 | Loss: 0.00189757
Iteration 9/25 | Loss: 0.00188769
Iteration 10/25 | Loss: 0.00188551
Iteration 11/25 | Loss: 0.00188129
Iteration 12/25 | Loss: 0.00188084
Iteration 13/25 | Loss: 0.00187482
Iteration 14/25 | Loss: 0.00187238
Iteration 15/25 | Loss: 0.00187056
Iteration 16/25 | Loss: 0.00186998
Iteration 17/25 | Loss: 0.00186947
Iteration 18/25 | Loss: 0.00186904
Iteration 19/25 | Loss: 0.00187235
Iteration 20/25 | Loss: 0.00186709
Iteration 21/25 | Loss: 0.00186684
Iteration 22/25 | Loss: 0.00186674
Iteration 23/25 | Loss: 0.00186674
Iteration 24/25 | Loss: 0.00186674
Iteration 25/25 | Loss: 0.00186674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14612544
Iteration 2/25 | Loss: 0.00559041
Iteration 3/25 | Loss: 0.00559041
Iteration 4/25 | Loss: 0.00559041
Iteration 5/25 | Loss: 0.00559041
Iteration 6/25 | Loss: 0.00559041
Iteration 7/25 | Loss: 0.00559041
Iteration 8/25 | Loss: 0.00559041
Iteration 9/25 | Loss: 0.00559041
Iteration 10/25 | Loss: 0.00559041
Iteration 11/25 | Loss: 0.00559041
Iteration 12/25 | Loss: 0.00559041
Iteration 13/25 | Loss: 0.00559041
Iteration 14/25 | Loss: 0.00559041
Iteration 15/25 | Loss: 0.00559041
Iteration 16/25 | Loss: 0.00559041
Iteration 17/25 | Loss: 0.00559040
Iteration 18/25 | Loss: 0.00559040
Iteration 19/25 | Loss: 0.00559040
Iteration 20/25 | Loss: 0.00559040
Iteration 21/25 | Loss: 0.00559040
Iteration 22/25 | Loss: 0.00559040
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.005590404849499464, 0.005590404849499464, 0.005590404849499464, 0.005590404849499464, 0.005590404849499464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005590404849499464

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00559040
Iteration 2/1000 | Loss: 0.00056786
Iteration 3/1000 | Loss: 0.00043362
Iteration 4/1000 | Loss: 0.00037397
Iteration 5/1000 | Loss: 0.00034362
Iteration 6/1000 | Loss: 0.00032461
Iteration 7/1000 | Loss: 0.00031214
Iteration 8/1000 | Loss: 0.00029896
Iteration 9/1000 | Loss: 0.00049079
Iteration 10/1000 | Loss: 0.00027977
Iteration 11/1000 | Loss: 0.00401065
Iteration 12/1000 | Loss: 0.01553125
Iteration 13/1000 | Loss: 0.00069279
Iteration 14/1000 | Loss: 0.00036037
Iteration 15/1000 | Loss: 0.00021224
Iteration 16/1000 | Loss: 0.00015951
Iteration 17/1000 | Loss: 0.00020756
Iteration 18/1000 | Loss: 0.00008168
Iteration 19/1000 | Loss: 0.00006057
Iteration 20/1000 | Loss: 0.00009917
Iteration 21/1000 | Loss: 0.00004008
Iteration 22/1000 | Loss: 0.00009138
Iteration 23/1000 | Loss: 0.00003103
Iteration 24/1000 | Loss: 0.00002767
Iteration 25/1000 | Loss: 0.00011020
Iteration 26/1000 | Loss: 0.00003403
Iteration 27/1000 | Loss: 0.00002525
Iteration 28/1000 | Loss: 0.00002107
Iteration 29/1000 | Loss: 0.00001909
Iteration 30/1000 | Loss: 0.00001728
Iteration 31/1000 | Loss: 0.00001567
Iteration 32/1000 | Loss: 0.00001445
Iteration 33/1000 | Loss: 0.00001351
Iteration 34/1000 | Loss: 0.00001295
Iteration 35/1000 | Loss: 0.00001260
Iteration 36/1000 | Loss: 0.00001227
Iteration 37/1000 | Loss: 0.00001213
Iteration 38/1000 | Loss: 0.00001201
Iteration 39/1000 | Loss: 0.00001201
Iteration 40/1000 | Loss: 0.00001197
Iteration 41/1000 | Loss: 0.00001197
Iteration 42/1000 | Loss: 0.00001196
Iteration 43/1000 | Loss: 0.00001196
Iteration 44/1000 | Loss: 0.00001196
Iteration 45/1000 | Loss: 0.00001193
Iteration 46/1000 | Loss: 0.00001192
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001177
Iteration 49/1000 | Loss: 0.00001176
Iteration 50/1000 | Loss: 0.00001176
Iteration 51/1000 | Loss: 0.00001176
Iteration 52/1000 | Loss: 0.00001176
Iteration 53/1000 | Loss: 0.00001176
Iteration 54/1000 | Loss: 0.00001176
Iteration 55/1000 | Loss: 0.00001175
Iteration 56/1000 | Loss: 0.00001175
Iteration 57/1000 | Loss: 0.00001175
Iteration 58/1000 | Loss: 0.00001175
Iteration 59/1000 | Loss: 0.00001175
Iteration 60/1000 | Loss: 0.00001175
Iteration 61/1000 | Loss: 0.00001175
Iteration 62/1000 | Loss: 0.00001175
Iteration 63/1000 | Loss: 0.00001175
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001175
Iteration 66/1000 | Loss: 0.00001174
Iteration 67/1000 | Loss: 0.00001174
Iteration 68/1000 | Loss: 0.00001174
Iteration 69/1000 | Loss: 0.00001173
Iteration 70/1000 | Loss: 0.00001173
Iteration 71/1000 | Loss: 0.00001173
Iteration 72/1000 | Loss: 0.00001173
Iteration 73/1000 | Loss: 0.00001173
Iteration 74/1000 | Loss: 0.00001173
Iteration 75/1000 | Loss: 0.00001173
Iteration 76/1000 | Loss: 0.00001172
Iteration 77/1000 | Loss: 0.00001172
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001171
Iteration 80/1000 | Loss: 0.00001171
Iteration 81/1000 | Loss: 0.00001171
Iteration 82/1000 | Loss: 0.00001171
Iteration 83/1000 | Loss: 0.00001171
Iteration 84/1000 | Loss: 0.00001171
Iteration 85/1000 | Loss: 0.00001171
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001171
Iteration 88/1000 | Loss: 0.00001170
Iteration 89/1000 | Loss: 0.00001170
Iteration 90/1000 | Loss: 0.00001170
Iteration 91/1000 | Loss: 0.00001170
Iteration 92/1000 | Loss: 0.00001170
Iteration 93/1000 | Loss: 0.00001170
Iteration 94/1000 | Loss: 0.00001170
Iteration 95/1000 | Loss: 0.00001170
Iteration 96/1000 | Loss: 0.00001170
Iteration 97/1000 | Loss: 0.00001170
Iteration 98/1000 | Loss: 0.00001170
Iteration 99/1000 | Loss: 0.00001170
Iteration 100/1000 | Loss: 0.00001169
Iteration 101/1000 | Loss: 0.00001169
Iteration 102/1000 | Loss: 0.00001169
Iteration 103/1000 | Loss: 0.00001169
Iteration 104/1000 | Loss: 0.00001169
Iteration 105/1000 | Loss: 0.00001169
Iteration 106/1000 | Loss: 0.00001169
Iteration 107/1000 | Loss: 0.00001169
Iteration 108/1000 | Loss: 0.00001169
Iteration 109/1000 | Loss: 0.00001169
Iteration 110/1000 | Loss: 0.00001169
Iteration 111/1000 | Loss: 0.00001169
Iteration 112/1000 | Loss: 0.00001169
Iteration 113/1000 | Loss: 0.00001169
Iteration 114/1000 | Loss: 0.00001168
Iteration 115/1000 | Loss: 0.00001168
Iteration 116/1000 | Loss: 0.00001168
Iteration 117/1000 | Loss: 0.00001168
Iteration 118/1000 | Loss: 0.00001168
Iteration 119/1000 | Loss: 0.00001168
Iteration 120/1000 | Loss: 0.00001168
Iteration 121/1000 | Loss: 0.00001168
Iteration 122/1000 | Loss: 0.00001168
Iteration 123/1000 | Loss: 0.00001168
Iteration 124/1000 | Loss: 0.00001168
Iteration 125/1000 | Loss: 0.00001168
Iteration 126/1000 | Loss: 0.00001167
Iteration 127/1000 | Loss: 0.00001167
Iteration 128/1000 | Loss: 0.00001167
Iteration 129/1000 | Loss: 0.00001167
Iteration 130/1000 | Loss: 0.00001166
Iteration 131/1000 | Loss: 0.00001166
Iteration 132/1000 | Loss: 0.00001166
Iteration 133/1000 | Loss: 0.00001166
Iteration 134/1000 | Loss: 0.00001166
Iteration 135/1000 | Loss: 0.00001166
Iteration 136/1000 | Loss: 0.00001166
Iteration 137/1000 | Loss: 0.00001166
Iteration 138/1000 | Loss: 0.00001165
Iteration 139/1000 | Loss: 0.00001165
Iteration 140/1000 | Loss: 0.00001165
Iteration 141/1000 | Loss: 0.00001165
Iteration 142/1000 | Loss: 0.00001165
Iteration 143/1000 | Loss: 0.00001165
Iteration 144/1000 | Loss: 0.00001165
Iteration 145/1000 | Loss: 0.00001165
Iteration 146/1000 | Loss: 0.00001164
Iteration 147/1000 | Loss: 0.00001164
Iteration 148/1000 | Loss: 0.00001164
Iteration 149/1000 | Loss: 0.00001164
Iteration 150/1000 | Loss: 0.00001164
Iteration 151/1000 | Loss: 0.00001164
Iteration 152/1000 | Loss: 0.00001164
Iteration 153/1000 | Loss: 0.00001164
Iteration 154/1000 | Loss: 0.00001164
Iteration 155/1000 | Loss: 0.00001164
Iteration 156/1000 | Loss: 0.00001164
Iteration 157/1000 | Loss: 0.00001164
Iteration 158/1000 | Loss: 0.00001164
Iteration 159/1000 | Loss: 0.00001163
Iteration 160/1000 | Loss: 0.00001163
Iteration 161/1000 | Loss: 0.00001163
Iteration 162/1000 | Loss: 0.00001163
Iteration 163/1000 | Loss: 0.00001163
Iteration 164/1000 | Loss: 0.00001163
Iteration 165/1000 | Loss: 0.00001163
Iteration 166/1000 | Loss: 0.00001163
Iteration 167/1000 | Loss: 0.00001163
Iteration 168/1000 | Loss: 0.00001163
Iteration 169/1000 | Loss: 0.00001163
Iteration 170/1000 | Loss: 0.00001163
Iteration 171/1000 | Loss: 0.00001163
Iteration 172/1000 | Loss: 0.00001163
Iteration 173/1000 | Loss: 0.00001163
Iteration 174/1000 | Loss: 0.00001162
Iteration 175/1000 | Loss: 0.00001162
Iteration 176/1000 | Loss: 0.00001162
Iteration 177/1000 | Loss: 0.00001162
Iteration 178/1000 | Loss: 0.00001162
Iteration 179/1000 | Loss: 0.00001162
Iteration 180/1000 | Loss: 0.00001162
Iteration 181/1000 | Loss: 0.00001162
Iteration 182/1000 | Loss: 0.00001162
Iteration 183/1000 | Loss: 0.00001162
Iteration 184/1000 | Loss: 0.00001162
Iteration 185/1000 | Loss: 0.00001162
Iteration 186/1000 | Loss: 0.00001162
Iteration 187/1000 | Loss: 0.00001162
Iteration 188/1000 | Loss: 0.00001162
Iteration 189/1000 | Loss: 0.00001162
Iteration 190/1000 | Loss: 0.00001162
Iteration 191/1000 | Loss: 0.00001162
Iteration 192/1000 | Loss: 0.00001162
Iteration 193/1000 | Loss: 0.00001162
Iteration 194/1000 | Loss: 0.00001162
Iteration 195/1000 | Loss: 0.00001162
Iteration 196/1000 | Loss: 0.00001162
Iteration 197/1000 | Loss: 0.00001162
Iteration 198/1000 | Loss: 0.00001162
Iteration 199/1000 | Loss: 0.00001162
Iteration 200/1000 | Loss: 0.00001162
Iteration 201/1000 | Loss: 0.00001162
Iteration 202/1000 | Loss: 0.00001162
Iteration 203/1000 | Loss: 0.00001162
Iteration 204/1000 | Loss: 0.00001162
Iteration 205/1000 | Loss: 0.00001162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.1616315532592125e-05, 1.1616315532592125e-05, 1.1616315532592125e-05, 1.1616315532592125e-05, 1.1616315532592125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1616315532592125e-05

Optimization complete. Final v2v error: 2.866554021835327 mm

Highest mean error: 3.1065282821655273 mm for frame 228

Lowest mean error: 2.726418972015381 mm for frame 16

Saving results

Total time: 116.66781044006348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394157
Iteration 2/25 | Loss: 0.00132074
Iteration 3/25 | Loss: 0.00123418
Iteration 4/25 | Loss: 0.00122881
Iteration 5/25 | Loss: 0.00122797
Iteration 6/25 | Loss: 0.00122797
Iteration 7/25 | Loss: 0.00122797
Iteration 8/25 | Loss: 0.00122797
Iteration 9/25 | Loss: 0.00122797
Iteration 10/25 | Loss: 0.00122797
Iteration 11/25 | Loss: 0.00122797
Iteration 12/25 | Loss: 0.00122797
Iteration 13/25 | Loss: 0.00122797
Iteration 14/25 | Loss: 0.00122797
Iteration 15/25 | Loss: 0.00122797
Iteration 16/25 | Loss: 0.00122797
Iteration 17/25 | Loss: 0.00122797
Iteration 18/25 | Loss: 0.00122797
Iteration 19/25 | Loss: 0.00122797
Iteration 20/25 | Loss: 0.00122797
Iteration 21/25 | Loss: 0.00122797
Iteration 22/25 | Loss: 0.00122797
Iteration 23/25 | Loss: 0.00122797
Iteration 24/25 | Loss: 0.00122797
Iteration 25/25 | Loss: 0.00122797

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18282723
Iteration 2/25 | Loss: 0.00285234
Iteration 3/25 | Loss: 0.00285234
Iteration 4/25 | Loss: 0.00285234
Iteration 5/25 | Loss: 0.00285234
Iteration 6/25 | Loss: 0.00285234
Iteration 7/25 | Loss: 0.00285234
Iteration 8/25 | Loss: 0.00285234
Iteration 9/25 | Loss: 0.00285234
Iteration 10/25 | Loss: 0.00285234
Iteration 11/25 | Loss: 0.00285234
Iteration 12/25 | Loss: 0.00285234
Iteration 13/25 | Loss: 0.00285234
Iteration 14/25 | Loss: 0.00285234
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0028523392975330353, 0.0028523392975330353, 0.0028523392975330353, 0.0028523392975330353, 0.0028523392975330353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028523392975330353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285234
Iteration 2/1000 | Loss: 0.00003463
Iteration 3/1000 | Loss: 0.00001996
Iteration 4/1000 | Loss: 0.00001610
Iteration 5/1000 | Loss: 0.00001446
Iteration 6/1000 | Loss: 0.00001346
Iteration 7/1000 | Loss: 0.00001304
Iteration 8/1000 | Loss: 0.00001253
Iteration 9/1000 | Loss: 0.00001223
Iteration 10/1000 | Loss: 0.00001201
Iteration 11/1000 | Loss: 0.00001193
Iteration 12/1000 | Loss: 0.00001192
Iteration 13/1000 | Loss: 0.00001192
Iteration 14/1000 | Loss: 0.00001192
Iteration 15/1000 | Loss: 0.00001192
Iteration 16/1000 | Loss: 0.00001192
Iteration 17/1000 | Loss: 0.00001192
Iteration 18/1000 | Loss: 0.00001190
Iteration 19/1000 | Loss: 0.00001189
Iteration 20/1000 | Loss: 0.00001188
Iteration 21/1000 | Loss: 0.00001188
Iteration 22/1000 | Loss: 0.00001187
Iteration 23/1000 | Loss: 0.00001186
Iteration 24/1000 | Loss: 0.00001185
Iteration 25/1000 | Loss: 0.00001184
Iteration 26/1000 | Loss: 0.00001184
Iteration 27/1000 | Loss: 0.00001183
Iteration 28/1000 | Loss: 0.00001183
Iteration 29/1000 | Loss: 0.00001183
Iteration 30/1000 | Loss: 0.00001182
Iteration 31/1000 | Loss: 0.00001182
Iteration 32/1000 | Loss: 0.00001182
Iteration 33/1000 | Loss: 0.00001181
Iteration 34/1000 | Loss: 0.00001181
Iteration 35/1000 | Loss: 0.00001181
Iteration 36/1000 | Loss: 0.00001181
Iteration 37/1000 | Loss: 0.00001181
Iteration 38/1000 | Loss: 0.00001181
Iteration 39/1000 | Loss: 0.00001180
Iteration 40/1000 | Loss: 0.00001180
Iteration 41/1000 | Loss: 0.00001180
Iteration 42/1000 | Loss: 0.00001180
Iteration 43/1000 | Loss: 0.00001180
Iteration 44/1000 | Loss: 0.00001180
Iteration 45/1000 | Loss: 0.00001180
Iteration 46/1000 | Loss: 0.00001179
Iteration 47/1000 | Loss: 0.00001179
Iteration 48/1000 | Loss: 0.00001179
Iteration 49/1000 | Loss: 0.00001179
Iteration 50/1000 | Loss: 0.00001178
Iteration 51/1000 | Loss: 0.00001178
Iteration 52/1000 | Loss: 0.00001177
Iteration 53/1000 | Loss: 0.00001177
Iteration 54/1000 | Loss: 0.00001177
Iteration 55/1000 | Loss: 0.00001176
Iteration 56/1000 | Loss: 0.00001176
Iteration 57/1000 | Loss: 0.00001175
Iteration 58/1000 | Loss: 0.00001175
Iteration 59/1000 | Loss: 0.00001175
Iteration 60/1000 | Loss: 0.00001174
Iteration 61/1000 | Loss: 0.00001173
Iteration 62/1000 | Loss: 0.00001173
Iteration 63/1000 | Loss: 0.00001173
Iteration 64/1000 | Loss: 0.00001173
Iteration 65/1000 | Loss: 0.00001173
Iteration 66/1000 | Loss: 0.00001173
Iteration 67/1000 | Loss: 0.00001172
Iteration 68/1000 | Loss: 0.00001172
Iteration 69/1000 | Loss: 0.00001172
Iteration 70/1000 | Loss: 0.00001172
Iteration 71/1000 | Loss: 0.00001172
Iteration 72/1000 | Loss: 0.00001172
Iteration 73/1000 | Loss: 0.00001172
Iteration 74/1000 | Loss: 0.00001172
Iteration 75/1000 | Loss: 0.00001172
Iteration 76/1000 | Loss: 0.00001172
Iteration 77/1000 | Loss: 0.00001172
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001172
Iteration 81/1000 | Loss: 0.00001172
Iteration 82/1000 | Loss: 0.00001172
Iteration 83/1000 | Loss: 0.00001172
Iteration 84/1000 | Loss: 0.00001172
Iteration 85/1000 | Loss: 0.00001172
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001172
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001172
Iteration 95/1000 | Loss: 0.00001172
Iteration 96/1000 | Loss: 0.00001172
Iteration 97/1000 | Loss: 0.00001172
Iteration 98/1000 | Loss: 0.00001172
Iteration 99/1000 | Loss: 0.00001172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.1716311746567953e-05, 1.1716311746567953e-05, 1.1716311746567953e-05, 1.1716311746567953e-05, 1.1716311746567953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1716311746567953e-05

Optimization complete. Final v2v error: 2.826333999633789 mm

Highest mean error: 3.072190523147583 mm for frame 179

Lowest mean error: 2.6701536178588867 mm for frame 64

Saving results

Total time: 29.121928215026855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00942359
Iteration 2/25 | Loss: 0.00255772
Iteration 3/25 | Loss: 0.00179154
Iteration 4/25 | Loss: 0.00158360
Iteration 5/25 | Loss: 0.00162114
Iteration 6/25 | Loss: 0.00158846
Iteration 7/25 | Loss: 0.00160081
Iteration 8/25 | Loss: 0.00157405
Iteration 9/25 | Loss: 0.00158860
Iteration 10/25 | Loss: 0.00156040
Iteration 11/25 | Loss: 0.00153886
Iteration 12/25 | Loss: 0.00152966
Iteration 13/25 | Loss: 0.00153356
Iteration 14/25 | Loss: 0.00153050
Iteration 15/25 | Loss: 0.00152790
Iteration 16/25 | Loss: 0.00151726
Iteration 17/25 | Loss: 0.00151518
Iteration 18/25 | Loss: 0.00151328
Iteration 19/25 | Loss: 0.00150902
Iteration 20/25 | Loss: 0.00150082
Iteration 21/25 | Loss: 0.00150506
Iteration 22/25 | Loss: 0.00151250
Iteration 23/25 | Loss: 0.00152311
Iteration 24/25 | Loss: 0.00150719
Iteration 25/25 | Loss: 0.00150412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.57064748
Iteration 2/25 | Loss: 0.00542773
Iteration 3/25 | Loss: 0.00537326
Iteration 4/25 | Loss: 0.00537321
Iteration 5/25 | Loss: 0.00537321
Iteration 6/25 | Loss: 0.00537321
Iteration 7/25 | Loss: 0.00537321
Iteration 8/25 | Loss: 0.00537321
Iteration 9/25 | Loss: 0.00537320
Iteration 10/25 | Loss: 0.00537320
Iteration 11/25 | Loss: 0.00537320
Iteration 12/25 | Loss: 0.00537320
Iteration 13/25 | Loss: 0.00537320
Iteration 14/25 | Loss: 0.00537320
Iteration 15/25 | Loss: 0.00537320
Iteration 16/25 | Loss: 0.00537320
Iteration 17/25 | Loss: 0.00537320
Iteration 18/25 | Loss: 0.00537320
Iteration 19/25 | Loss: 0.00537320
Iteration 20/25 | Loss: 0.00537320
Iteration 21/25 | Loss: 0.00537320
Iteration 22/25 | Loss: 0.00537320
Iteration 23/25 | Loss: 0.00537320
Iteration 24/25 | Loss: 0.00537320
Iteration 25/25 | Loss: 0.00537320
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.005373201332986355, 0.005373201332986355, 0.005373201332986355, 0.005373201332986355, 0.005373201332986355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005373201332986355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00537320
Iteration 2/1000 | Loss: 0.00073997
Iteration 3/1000 | Loss: 0.00058584
Iteration 4/1000 | Loss: 0.00087349
Iteration 5/1000 | Loss: 0.00074736
Iteration 6/1000 | Loss: 0.00037098
Iteration 7/1000 | Loss: 0.00051162
Iteration 8/1000 | Loss: 0.00043427
Iteration 9/1000 | Loss: 0.00039944
Iteration 10/1000 | Loss: 0.00062460
Iteration 11/1000 | Loss: 0.00047904
Iteration 12/1000 | Loss: 0.00036505
Iteration 13/1000 | Loss: 0.00052677
Iteration 14/1000 | Loss: 0.00076978
Iteration 15/1000 | Loss: 0.00040601
Iteration 16/1000 | Loss: 0.00057800
Iteration 17/1000 | Loss: 0.00077623
Iteration 18/1000 | Loss: 0.00080707
Iteration 19/1000 | Loss: 0.00035458
Iteration 20/1000 | Loss: 0.00027404
Iteration 21/1000 | Loss: 0.00043100
Iteration 22/1000 | Loss: 0.00039483
Iteration 23/1000 | Loss: 0.00048822
Iteration 24/1000 | Loss: 0.00051270
Iteration 25/1000 | Loss: 0.00036940
Iteration 26/1000 | Loss: 0.00029527
Iteration 27/1000 | Loss: 0.00146611
Iteration 28/1000 | Loss: 0.00177457
Iteration 29/1000 | Loss: 0.00078970
Iteration 30/1000 | Loss: 0.00054238
Iteration 31/1000 | Loss: 0.00034343
Iteration 32/1000 | Loss: 0.00044776
Iteration 33/1000 | Loss: 0.00035963
Iteration 34/1000 | Loss: 0.00038466
Iteration 35/1000 | Loss: 0.00056352
Iteration 36/1000 | Loss: 0.00052321
Iteration 37/1000 | Loss: 0.00024170
Iteration 38/1000 | Loss: 0.00029495
Iteration 39/1000 | Loss: 0.00026881
Iteration 40/1000 | Loss: 0.00019917
Iteration 41/1000 | Loss: 0.00023265
Iteration 42/1000 | Loss: 0.00022018
Iteration 43/1000 | Loss: 0.00054190
Iteration 44/1000 | Loss: 0.00025440
Iteration 45/1000 | Loss: 0.00022445
Iteration 46/1000 | Loss: 0.00020449
Iteration 47/1000 | Loss: 0.00017246
Iteration 48/1000 | Loss: 0.00023129
Iteration 49/1000 | Loss: 0.00026139
Iteration 50/1000 | Loss: 0.00063053
Iteration 51/1000 | Loss: 0.00026026
Iteration 52/1000 | Loss: 0.00039358
Iteration 53/1000 | Loss: 0.00039827
Iteration 54/1000 | Loss: 0.00029064
Iteration 55/1000 | Loss: 0.00022193
Iteration 56/1000 | Loss: 0.00037146
Iteration 57/1000 | Loss: 0.00032078
Iteration 58/1000 | Loss: 0.00043818
Iteration 59/1000 | Loss: 0.00018139
Iteration 60/1000 | Loss: 0.00028016
Iteration 61/1000 | Loss: 0.00015632
Iteration 62/1000 | Loss: 0.00022581
Iteration 63/1000 | Loss: 0.00039064
Iteration 64/1000 | Loss: 0.00018372
Iteration 65/1000 | Loss: 0.00015473
Iteration 66/1000 | Loss: 0.00031256
Iteration 67/1000 | Loss: 0.00021570
Iteration 68/1000 | Loss: 0.00062578
Iteration 69/1000 | Loss: 0.00053247
Iteration 70/1000 | Loss: 0.00032862
Iteration 71/1000 | Loss: 0.00041171
Iteration 72/1000 | Loss: 0.00096125
Iteration 73/1000 | Loss: 0.00045777
Iteration 74/1000 | Loss: 0.00017919
Iteration 75/1000 | Loss: 0.00013945
Iteration 76/1000 | Loss: 0.00027016
Iteration 77/1000 | Loss: 0.00018350
Iteration 78/1000 | Loss: 0.00014241
Iteration 79/1000 | Loss: 0.00032227
Iteration 80/1000 | Loss: 0.00028435
Iteration 81/1000 | Loss: 0.00015457
Iteration 82/1000 | Loss: 0.00013379
Iteration 83/1000 | Loss: 0.00012762
Iteration 84/1000 | Loss: 0.00047656
Iteration 85/1000 | Loss: 0.00015001
Iteration 86/1000 | Loss: 0.00013384
Iteration 87/1000 | Loss: 0.00029583
Iteration 88/1000 | Loss: 0.00048076
Iteration 89/1000 | Loss: 0.00019830
Iteration 90/1000 | Loss: 0.00023212
Iteration 91/1000 | Loss: 0.00023068
Iteration 92/1000 | Loss: 0.00023299
Iteration 93/1000 | Loss: 0.00012022
Iteration 94/1000 | Loss: 0.00011198
Iteration 95/1000 | Loss: 0.00010859
Iteration 96/1000 | Loss: 0.00068449
Iteration 97/1000 | Loss: 0.00043803
Iteration 98/1000 | Loss: 0.00057278
Iteration 99/1000 | Loss: 0.00024316
Iteration 100/1000 | Loss: 0.00068374
Iteration 101/1000 | Loss: 0.00049319
Iteration 102/1000 | Loss: 0.00053967
Iteration 103/1000 | Loss: 0.00021169
Iteration 104/1000 | Loss: 0.00015066
Iteration 105/1000 | Loss: 0.00010597
Iteration 106/1000 | Loss: 0.00011099
Iteration 107/1000 | Loss: 0.00017517
Iteration 108/1000 | Loss: 0.00066277
Iteration 109/1000 | Loss: 0.00029220
Iteration 110/1000 | Loss: 0.00109541
Iteration 111/1000 | Loss: 0.00148610
Iteration 112/1000 | Loss: 0.00032732
Iteration 113/1000 | Loss: 0.00089428
Iteration 114/1000 | Loss: 0.00038484
Iteration 115/1000 | Loss: 0.00077586
Iteration 116/1000 | Loss: 0.00116408
Iteration 117/1000 | Loss: 0.00018830
Iteration 118/1000 | Loss: 0.00067405
Iteration 119/1000 | Loss: 0.00011145
Iteration 120/1000 | Loss: 0.00009776
Iteration 121/1000 | Loss: 0.00009115
Iteration 122/1000 | Loss: 0.00008704
Iteration 123/1000 | Loss: 0.00008483
Iteration 124/1000 | Loss: 0.00042311
Iteration 125/1000 | Loss: 0.00019452
Iteration 126/1000 | Loss: 0.00031447
Iteration 127/1000 | Loss: 0.00025260
Iteration 128/1000 | Loss: 0.00008216
Iteration 129/1000 | Loss: 0.00033477
Iteration 130/1000 | Loss: 0.00008688
Iteration 131/1000 | Loss: 0.00008153
Iteration 132/1000 | Loss: 0.00078197
Iteration 133/1000 | Loss: 0.00090660
Iteration 134/1000 | Loss: 0.00040312
Iteration 135/1000 | Loss: 0.00013832
Iteration 136/1000 | Loss: 0.00030805
Iteration 137/1000 | Loss: 0.00011888
Iteration 138/1000 | Loss: 0.00008401
Iteration 139/1000 | Loss: 0.00008003
Iteration 140/1000 | Loss: 0.00040353
Iteration 141/1000 | Loss: 0.00082704
Iteration 142/1000 | Loss: 0.00045464
Iteration 143/1000 | Loss: 0.00024904
Iteration 144/1000 | Loss: 0.00033642
Iteration 145/1000 | Loss: 0.00016355
Iteration 146/1000 | Loss: 0.00037290
Iteration 147/1000 | Loss: 0.00025016
Iteration 148/1000 | Loss: 0.00027245
Iteration 149/1000 | Loss: 0.00007897
Iteration 150/1000 | Loss: 0.00007524
Iteration 151/1000 | Loss: 0.00035266
Iteration 152/1000 | Loss: 0.00042603
Iteration 153/1000 | Loss: 0.00007984
Iteration 154/1000 | Loss: 0.00007293
Iteration 155/1000 | Loss: 0.00058741
Iteration 156/1000 | Loss: 0.00009071
Iteration 157/1000 | Loss: 0.00007669
Iteration 158/1000 | Loss: 0.00007164
Iteration 159/1000 | Loss: 0.00006968
Iteration 160/1000 | Loss: 0.00006836
Iteration 161/1000 | Loss: 0.00045303
Iteration 162/1000 | Loss: 0.00015306
Iteration 163/1000 | Loss: 0.00006916
Iteration 164/1000 | Loss: 0.00027014
Iteration 165/1000 | Loss: 0.00008755
Iteration 166/1000 | Loss: 0.00007688
Iteration 167/1000 | Loss: 0.00011992
Iteration 168/1000 | Loss: 0.00010142
Iteration 169/1000 | Loss: 0.00010192
Iteration 170/1000 | Loss: 0.00006973
Iteration 171/1000 | Loss: 0.00006841
Iteration 172/1000 | Loss: 0.00112788
Iteration 173/1000 | Loss: 0.00103986
Iteration 174/1000 | Loss: 0.00096578
Iteration 175/1000 | Loss: 0.00082285
Iteration 176/1000 | Loss: 0.00009980
Iteration 177/1000 | Loss: 0.00008783
Iteration 178/1000 | Loss: 0.00007034
Iteration 179/1000 | Loss: 0.00006691
Iteration 180/1000 | Loss: 0.00006479
Iteration 181/1000 | Loss: 0.00006325
Iteration 182/1000 | Loss: 0.00006204
Iteration 183/1000 | Loss: 0.00021559
Iteration 184/1000 | Loss: 0.00010526
Iteration 185/1000 | Loss: 0.00006215
Iteration 186/1000 | Loss: 0.00020269
Iteration 187/1000 | Loss: 0.00009587
Iteration 188/1000 | Loss: 0.00015336
Iteration 189/1000 | Loss: 0.00006704
Iteration 190/1000 | Loss: 0.00006329
Iteration 191/1000 | Loss: 0.00029190
Iteration 192/1000 | Loss: 0.00006730
Iteration 193/1000 | Loss: 0.00032582
Iteration 194/1000 | Loss: 0.00020286
Iteration 195/1000 | Loss: 0.00006910
Iteration 196/1000 | Loss: 0.00006506
Iteration 197/1000 | Loss: 0.00006143
Iteration 198/1000 | Loss: 0.00005933
Iteration 199/1000 | Loss: 0.00005795
Iteration 200/1000 | Loss: 0.00005730
Iteration 201/1000 | Loss: 0.00005693
Iteration 202/1000 | Loss: 0.00005668
Iteration 203/1000 | Loss: 0.00005652
Iteration 204/1000 | Loss: 0.00005648
Iteration 205/1000 | Loss: 0.00005638
Iteration 206/1000 | Loss: 0.00005634
Iteration 207/1000 | Loss: 0.00005630
Iteration 208/1000 | Loss: 0.00005629
Iteration 209/1000 | Loss: 0.00005629
Iteration 210/1000 | Loss: 0.00005629
Iteration 211/1000 | Loss: 0.00005628
Iteration 212/1000 | Loss: 0.00005628
Iteration 213/1000 | Loss: 0.00005626
Iteration 214/1000 | Loss: 0.00005625
Iteration 215/1000 | Loss: 0.00005625
Iteration 216/1000 | Loss: 0.00005625
Iteration 217/1000 | Loss: 0.00005625
Iteration 218/1000 | Loss: 0.00005625
Iteration 219/1000 | Loss: 0.00005625
Iteration 220/1000 | Loss: 0.00005625
Iteration 221/1000 | Loss: 0.00005623
Iteration 222/1000 | Loss: 0.00005623
Iteration 223/1000 | Loss: 0.00005622
Iteration 224/1000 | Loss: 0.00005618
Iteration 225/1000 | Loss: 0.00005617
Iteration 226/1000 | Loss: 0.00005614
Iteration 227/1000 | Loss: 0.00005614
Iteration 228/1000 | Loss: 0.00005614
Iteration 229/1000 | Loss: 0.00005613
Iteration 230/1000 | Loss: 0.00005613
Iteration 231/1000 | Loss: 0.00005613
Iteration 232/1000 | Loss: 0.00005613
Iteration 233/1000 | Loss: 0.00005612
Iteration 234/1000 | Loss: 0.00005611
Iteration 235/1000 | Loss: 0.00005610
Iteration 236/1000 | Loss: 0.00005610
Iteration 237/1000 | Loss: 0.00005609
Iteration 238/1000 | Loss: 0.00005605
Iteration 239/1000 | Loss: 0.00005605
Iteration 240/1000 | Loss: 0.00005604
Iteration 241/1000 | Loss: 0.00005604
Iteration 242/1000 | Loss: 0.00005603
Iteration 243/1000 | Loss: 0.00005603
Iteration 244/1000 | Loss: 0.00005603
Iteration 245/1000 | Loss: 0.00005602
Iteration 246/1000 | Loss: 0.00005602
Iteration 247/1000 | Loss: 0.00005602
Iteration 248/1000 | Loss: 0.00005602
Iteration 249/1000 | Loss: 0.00005601
Iteration 250/1000 | Loss: 0.00005601
Iteration 251/1000 | Loss: 0.00005601
Iteration 252/1000 | Loss: 0.00005601
Iteration 253/1000 | Loss: 0.00005601
Iteration 254/1000 | Loss: 0.00005601
Iteration 255/1000 | Loss: 0.00005601
Iteration 256/1000 | Loss: 0.00005601
Iteration 257/1000 | Loss: 0.00005601
Iteration 258/1000 | Loss: 0.00005601
Iteration 259/1000 | Loss: 0.00005599
Iteration 260/1000 | Loss: 0.00005599
Iteration 261/1000 | Loss: 0.00005599
Iteration 262/1000 | Loss: 0.00005599
Iteration 263/1000 | Loss: 0.00005599
Iteration 264/1000 | Loss: 0.00005599
Iteration 265/1000 | Loss: 0.00005599
Iteration 266/1000 | Loss: 0.00005599
Iteration 267/1000 | Loss: 0.00005599
Iteration 268/1000 | Loss: 0.00005599
Iteration 269/1000 | Loss: 0.00005599
Iteration 270/1000 | Loss: 0.00005598
Iteration 271/1000 | Loss: 0.00005598
Iteration 272/1000 | Loss: 0.00005598
Iteration 273/1000 | Loss: 0.00005598
Iteration 274/1000 | Loss: 0.00005598
Iteration 275/1000 | Loss: 0.00005597
Iteration 276/1000 | Loss: 0.00005597
Iteration 277/1000 | Loss: 0.00005597
Iteration 278/1000 | Loss: 0.00005596
Iteration 279/1000 | Loss: 0.00005596
Iteration 280/1000 | Loss: 0.00005596
Iteration 281/1000 | Loss: 0.00005596
Iteration 282/1000 | Loss: 0.00005595
Iteration 283/1000 | Loss: 0.00005595
Iteration 284/1000 | Loss: 0.00005595
Iteration 285/1000 | Loss: 0.00005595
Iteration 286/1000 | Loss: 0.00005595
Iteration 287/1000 | Loss: 0.00005594
Iteration 288/1000 | Loss: 0.00005594
Iteration 289/1000 | Loss: 0.00005594
Iteration 290/1000 | Loss: 0.00005594
Iteration 291/1000 | Loss: 0.00005594
Iteration 292/1000 | Loss: 0.00005594
Iteration 293/1000 | Loss: 0.00005594
Iteration 294/1000 | Loss: 0.00005593
Iteration 295/1000 | Loss: 0.00005593
Iteration 296/1000 | Loss: 0.00005593
Iteration 297/1000 | Loss: 0.00005593
Iteration 298/1000 | Loss: 0.00005593
Iteration 299/1000 | Loss: 0.00005593
Iteration 300/1000 | Loss: 0.00005593
Iteration 301/1000 | Loss: 0.00005592
Iteration 302/1000 | Loss: 0.00005592
Iteration 303/1000 | Loss: 0.00005592
Iteration 304/1000 | Loss: 0.00005592
Iteration 305/1000 | Loss: 0.00005592
Iteration 306/1000 | Loss: 0.00005592
Iteration 307/1000 | Loss: 0.00005592
Iteration 308/1000 | Loss: 0.00005592
Iteration 309/1000 | Loss: 0.00005592
Iteration 310/1000 | Loss: 0.00005592
Iteration 311/1000 | Loss: 0.00005591
Iteration 312/1000 | Loss: 0.00005591
Iteration 313/1000 | Loss: 0.00005591
Iteration 314/1000 | Loss: 0.00005591
Iteration 315/1000 | Loss: 0.00005591
Iteration 316/1000 | Loss: 0.00005591
Iteration 317/1000 | Loss: 0.00005590
Iteration 318/1000 | Loss: 0.00005590
Iteration 319/1000 | Loss: 0.00005590
Iteration 320/1000 | Loss: 0.00005590
Iteration 321/1000 | Loss: 0.00005590
Iteration 322/1000 | Loss: 0.00005590
Iteration 323/1000 | Loss: 0.00005590
Iteration 324/1000 | Loss: 0.00005590
Iteration 325/1000 | Loss: 0.00005590
Iteration 326/1000 | Loss: 0.00005590
Iteration 327/1000 | Loss: 0.00005590
Iteration 328/1000 | Loss: 0.00005590
Iteration 329/1000 | Loss: 0.00005590
Iteration 330/1000 | Loss: 0.00005590
Iteration 331/1000 | Loss: 0.00005590
Iteration 332/1000 | Loss: 0.00005590
Iteration 333/1000 | Loss: 0.00005590
Iteration 334/1000 | Loss: 0.00005590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 334. Stopping optimization.
Last 5 losses: [5.5903252359712496e-05, 5.5903252359712496e-05, 5.5903252359712496e-05, 5.5903252359712496e-05, 5.5903252359712496e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.5903252359712496e-05

Optimization complete. Final v2v error: 4.141347408294678 mm

Highest mean error: 14.04228401184082 mm for frame 52

Lowest mean error: 2.659787893295288 mm for frame 151

Saving results

Total time: 357.85471391677856
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420432
Iteration 2/25 | Loss: 0.00133272
Iteration 3/25 | Loss: 0.00125841
Iteration 4/25 | Loss: 0.00124270
Iteration 5/25 | Loss: 0.00123749
Iteration 6/25 | Loss: 0.00123585
Iteration 7/25 | Loss: 0.00123585
Iteration 8/25 | Loss: 0.00123585
Iteration 9/25 | Loss: 0.00123585
Iteration 10/25 | Loss: 0.00123585
Iteration 11/25 | Loss: 0.00123585
Iteration 12/25 | Loss: 0.00123585
Iteration 13/25 | Loss: 0.00123585
Iteration 14/25 | Loss: 0.00123585
Iteration 15/25 | Loss: 0.00123585
Iteration 16/25 | Loss: 0.00123585
Iteration 17/25 | Loss: 0.00123585
Iteration 18/25 | Loss: 0.00123585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001235853647813201, 0.001235853647813201, 0.001235853647813201, 0.001235853647813201, 0.001235853647813201]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001235853647813201

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23127007
Iteration 2/25 | Loss: 0.00300021
Iteration 3/25 | Loss: 0.00300021
Iteration 4/25 | Loss: 0.00300021
Iteration 5/25 | Loss: 0.00300020
Iteration 6/25 | Loss: 0.00300020
Iteration 7/25 | Loss: 0.00300020
Iteration 8/25 | Loss: 0.00300020
Iteration 9/25 | Loss: 0.00300020
Iteration 10/25 | Loss: 0.00300020
Iteration 11/25 | Loss: 0.00300020
Iteration 12/25 | Loss: 0.00300020
Iteration 13/25 | Loss: 0.00300020
Iteration 14/25 | Loss: 0.00300020
Iteration 15/25 | Loss: 0.00300020
Iteration 16/25 | Loss: 0.00300020
Iteration 17/25 | Loss: 0.00300020
Iteration 18/25 | Loss: 0.00300020
Iteration 19/25 | Loss: 0.00300020
Iteration 20/25 | Loss: 0.00300020
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0030002030543982983, 0.0030002030543982983, 0.0030002030543982983, 0.0030002030543982983, 0.0030002030543982983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030002030543982983

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00300020
Iteration 2/1000 | Loss: 0.00004577
Iteration 3/1000 | Loss: 0.00002550
Iteration 4/1000 | Loss: 0.00002097
Iteration 5/1000 | Loss: 0.00001849
Iteration 6/1000 | Loss: 0.00001692
Iteration 7/1000 | Loss: 0.00001611
Iteration 8/1000 | Loss: 0.00001562
Iteration 9/1000 | Loss: 0.00001505
Iteration 10/1000 | Loss: 0.00001472
Iteration 11/1000 | Loss: 0.00001467
Iteration 12/1000 | Loss: 0.00001452
Iteration 13/1000 | Loss: 0.00001449
Iteration 14/1000 | Loss: 0.00001446
Iteration 15/1000 | Loss: 0.00001438
Iteration 16/1000 | Loss: 0.00001438
Iteration 17/1000 | Loss: 0.00001434
Iteration 18/1000 | Loss: 0.00001434
Iteration 19/1000 | Loss: 0.00001434
Iteration 20/1000 | Loss: 0.00001432
Iteration 21/1000 | Loss: 0.00001431
Iteration 22/1000 | Loss: 0.00001431
Iteration 23/1000 | Loss: 0.00001429
Iteration 24/1000 | Loss: 0.00001429
Iteration 25/1000 | Loss: 0.00001428
Iteration 26/1000 | Loss: 0.00001427
Iteration 27/1000 | Loss: 0.00001426
Iteration 28/1000 | Loss: 0.00001426
Iteration 29/1000 | Loss: 0.00001426
Iteration 30/1000 | Loss: 0.00001426
Iteration 31/1000 | Loss: 0.00001425
Iteration 32/1000 | Loss: 0.00001425
Iteration 33/1000 | Loss: 0.00001424
Iteration 34/1000 | Loss: 0.00001423
Iteration 35/1000 | Loss: 0.00001423
Iteration 36/1000 | Loss: 0.00001423
Iteration 37/1000 | Loss: 0.00001423
Iteration 38/1000 | Loss: 0.00001422
Iteration 39/1000 | Loss: 0.00001422
Iteration 40/1000 | Loss: 0.00001422
Iteration 41/1000 | Loss: 0.00001422
Iteration 42/1000 | Loss: 0.00001422
Iteration 43/1000 | Loss: 0.00001422
Iteration 44/1000 | Loss: 0.00001422
Iteration 45/1000 | Loss: 0.00001422
Iteration 46/1000 | Loss: 0.00001422
Iteration 47/1000 | Loss: 0.00001422
Iteration 48/1000 | Loss: 0.00001422
Iteration 49/1000 | Loss: 0.00001422
Iteration 50/1000 | Loss: 0.00001422
Iteration 51/1000 | Loss: 0.00001422
Iteration 52/1000 | Loss: 0.00001422
Iteration 53/1000 | Loss: 0.00001422
Iteration 54/1000 | Loss: 0.00001422
Iteration 55/1000 | Loss: 0.00001422
Iteration 56/1000 | Loss: 0.00001422
Iteration 57/1000 | Loss: 0.00001422
Iteration 58/1000 | Loss: 0.00001422
Iteration 59/1000 | Loss: 0.00001422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [1.4217175703379326e-05, 1.4217175703379326e-05, 1.4217175703379326e-05, 1.4217175703379326e-05, 1.4217175703379326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4217175703379326e-05

Optimization complete. Final v2v error: 3.209315061569214 mm

Highest mean error: 3.777686595916748 mm for frame 93

Lowest mean error: 2.9428462982177734 mm for frame 31

Saving results

Total time: 29.71325397491455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012991
Iteration 2/25 | Loss: 0.00233251
Iteration 3/25 | Loss: 0.00212491
Iteration 4/25 | Loss: 0.00163969
Iteration 5/25 | Loss: 0.00151279
Iteration 6/25 | Loss: 0.00150334
Iteration 7/25 | Loss: 0.00139418
Iteration 8/25 | Loss: 0.00137181
Iteration 9/25 | Loss: 0.00133662
Iteration 10/25 | Loss: 0.00135810
Iteration 11/25 | Loss: 0.00130052
Iteration 12/25 | Loss: 0.00132459
Iteration 13/25 | Loss: 0.00128528
Iteration 14/25 | Loss: 0.00128442
Iteration 15/25 | Loss: 0.00129257
Iteration 16/25 | Loss: 0.00128885
Iteration 17/25 | Loss: 0.00128910
Iteration 18/25 | Loss: 0.00128537
Iteration 19/25 | Loss: 0.00128376
Iteration 20/25 | Loss: 0.00128347
Iteration 21/25 | Loss: 0.00128473
Iteration 22/25 | Loss: 0.00128326
Iteration 23/25 | Loss: 0.00128341
Iteration 24/25 | Loss: 0.00128273
Iteration 25/25 | Loss: 0.00128275

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22949624
Iteration 2/25 | Loss: 0.00336842
Iteration 3/25 | Loss: 0.00336842
Iteration 4/25 | Loss: 0.00336842
Iteration 5/25 | Loss: 0.00336842
Iteration 6/25 | Loss: 0.00336842
Iteration 7/25 | Loss: 0.00336842
Iteration 8/25 | Loss: 0.00336842
Iteration 9/25 | Loss: 0.00336842
Iteration 10/25 | Loss: 0.00336842
Iteration 11/25 | Loss: 0.00336842
Iteration 12/25 | Loss: 0.00336842
Iteration 13/25 | Loss: 0.00336842
Iteration 14/25 | Loss: 0.00336842
Iteration 15/25 | Loss: 0.00336842
Iteration 16/25 | Loss: 0.00336842
Iteration 17/25 | Loss: 0.00336842
Iteration 18/25 | Loss: 0.00336842
Iteration 19/25 | Loss: 0.00336842
Iteration 20/25 | Loss: 0.00336842
Iteration 21/25 | Loss: 0.00336842
Iteration 22/25 | Loss: 0.00336842
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0033684170339256525, 0.0033684170339256525, 0.0033684170339256525, 0.0033684170339256525, 0.0033684170339256525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033684170339256525

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00336842
Iteration 2/1000 | Loss: 0.00009157
Iteration 3/1000 | Loss: 0.00008863
Iteration 4/1000 | Loss: 0.00009323
Iteration 5/1000 | Loss: 0.00077416
Iteration 6/1000 | Loss: 0.00006123
Iteration 7/1000 | Loss: 0.00003628
Iteration 8/1000 | Loss: 0.00023860
Iteration 9/1000 | Loss: 0.00003224
Iteration 10/1000 | Loss: 0.00002691
Iteration 11/1000 | Loss: 0.00002502
Iteration 12/1000 | Loss: 0.00002374
Iteration 13/1000 | Loss: 0.00002296
Iteration 14/1000 | Loss: 0.00002208
Iteration 15/1000 | Loss: 0.00164556
Iteration 16/1000 | Loss: 0.00023960
Iteration 17/1000 | Loss: 0.00025001
Iteration 18/1000 | Loss: 0.00020990
Iteration 19/1000 | Loss: 0.00019681
Iteration 20/1000 | Loss: 0.00018596
Iteration 21/1000 | Loss: 0.00011793
Iteration 22/1000 | Loss: 0.00002551
Iteration 23/1000 | Loss: 0.00002143
Iteration 24/1000 | Loss: 0.00001825
Iteration 25/1000 | Loss: 0.00001687
Iteration 26/1000 | Loss: 0.00001598
Iteration 27/1000 | Loss: 0.00001548
Iteration 28/1000 | Loss: 0.00001486
Iteration 29/1000 | Loss: 0.00001452
Iteration 30/1000 | Loss: 0.00001429
Iteration 31/1000 | Loss: 0.00001416
Iteration 32/1000 | Loss: 0.00001403
Iteration 33/1000 | Loss: 0.00001401
Iteration 34/1000 | Loss: 0.00001400
Iteration 35/1000 | Loss: 0.00001394
Iteration 36/1000 | Loss: 0.00001394
Iteration 37/1000 | Loss: 0.00001391
Iteration 38/1000 | Loss: 0.00001390
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001390
Iteration 41/1000 | Loss: 0.00001390
Iteration 42/1000 | Loss: 0.00001389
Iteration 43/1000 | Loss: 0.00001389
Iteration 44/1000 | Loss: 0.00001389
Iteration 45/1000 | Loss: 0.00001389
Iteration 46/1000 | Loss: 0.00001387
Iteration 47/1000 | Loss: 0.00001387
Iteration 48/1000 | Loss: 0.00001387
Iteration 49/1000 | Loss: 0.00001387
Iteration 50/1000 | Loss: 0.00001386
Iteration 51/1000 | Loss: 0.00001385
Iteration 52/1000 | Loss: 0.00001384
Iteration 53/1000 | Loss: 0.00001384
Iteration 54/1000 | Loss: 0.00001383
Iteration 55/1000 | Loss: 0.00001383
Iteration 56/1000 | Loss: 0.00001381
Iteration 57/1000 | Loss: 0.00001380
Iteration 58/1000 | Loss: 0.00001378
Iteration 59/1000 | Loss: 0.00001377
Iteration 60/1000 | Loss: 0.00001373
Iteration 61/1000 | Loss: 0.00001372
Iteration 62/1000 | Loss: 0.00001372
Iteration 63/1000 | Loss: 0.00001367
Iteration 64/1000 | Loss: 0.00001367
Iteration 65/1000 | Loss: 0.00001365
Iteration 66/1000 | Loss: 0.00001363
Iteration 67/1000 | Loss: 0.00001363
Iteration 68/1000 | Loss: 0.00001362
Iteration 69/1000 | Loss: 0.00001362
Iteration 70/1000 | Loss: 0.00001362
Iteration 71/1000 | Loss: 0.00001362
Iteration 72/1000 | Loss: 0.00001362
Iteration 73/1000 | Loss: 0.00001362
Iteration 74/1000 | Loss: 0.00001362
Iteration 75/1000 | Loss: 0.00001362
Iteration 76/1000 | Loss: 0.00001360
Iteration 77/1000 | Loss: 0.00001359
Iteration 78/1000 | Loss: 0.00001359
Iteration 79/1000 | Loss: 0.00001357
Iteration 80/1000 | Loss: 0.00001357
Iteration 81/1000 | Loss: 0.00001357
Iteration 82/1000 | Loss: 0.00001357
Iteration 83/1000 | Loss: 0.00001357
Iteration 84/1000 | Loss: 0.00001357
Iteration 85/1000 | Loss: 0.00001357
Iteration 86/1000 | Loss: 0.00001357
Iteration 87/1000 | Loss: 0.00001357
Iteration 88/1000 | Loss: 0.00001357
Iteration 89/1000 | Loss: 0.00001356
Iteration 90/1000 | Loss: 0.00001354
Iteration 91/1000 | Loss: 0.00001354
Iteration 92/1000 | Loss: 0.00001353
Iteration 93/1000 | Loss: 0.00001353
Iteration 94/1000 | Loss: 0.00001353
Iteration 95/1000 | Loss: 0.00001353
Iteration 96/1000 | Loss: 0.00001353
Iteration 97/1000 | Loss: 0.00001352
Iteration 98/1000 | Loss: 0.00001352
Iteration 99/1000 | Loss: 0.00001352
Iteration 100/1000 | Loss: 0.00001352
Iteration 101/1000 | Loss: 0.00001352
Iteration 102/1000 | Loss: 0.00001352
Iteration 103/1000 | Loss: 0.00001352
Iteration 104/1000 | Loss: 0.00001352
Iteration 105/1000 | Loss: 0.00001352
Iteration 106/1000 | Loss: 0.00001351
Iteration 107/1000 | Loss: 0.00001351
Iteration 108/1000 | Loss: 0.00001351
Iteration 109/1000 | Loss: 0.00001351
Iteration 110/1000 | Loss: 0.00001351
Iteration 111/1000 | Loss: 0.00001351
Iteration 112/1000 | Loss: 0.00001351
Iteration 113/1000 | Loss: 0.00001351
Iteration 114/1000 | Loss: 0.00001351
Iteration 115/1000 | Loss: 0.00001351
Iteration 116/1000 | Loss: 0.00001350
Iteration 117/1000 | Loss: 0.00001350
Iteration 118/1000 | Loss: 0.00001350
Iteration 119/1000 | Loss: 0.00001350
Iteration 120/1000 | Loss: 0.00001350
Iteration 121/1000 | Loss: 0.00001350
Iteration 122/1000 | Loss: 0.00001350
Iteration 123/1000 | Loss: 0.00001349
Iteration 124/1000 | Loss: 0.00001349
Iteration 125/1000 | Loss: 0.00001349
Iteration 126/1000 | Loss: 0.00001349
Iteration 127/1000 | Loss: 0.00001348
Iteration 128/1000 | Loss: 0.00001348
Iteration 129/1000 | Loss: 0.00001348
Iteration 130/1000 | Loss: 0.00001347
Iteration 131/1000 | Loss: 0.00001347
Iteration 132/1000 | Loss: 0.00001347
Iteration 133/1000 | Loss: 0.00001346
Iteration 134/1000 | Loss: 0.00001346
Iteration 135/1000 | Loss: 0.00001346
Iteration 136/1000 | Loss: 0.00001346
Iteration 137/1000 | Loss: 0.00001345
Iteration 138/1000 | Loss: 0.00001345
Iteration 139/1000 | Loss: 0.00001345
Iteration 140/1000 | Loss: 0.00001345
Iteration 141/1000 | Loss: 0.00001345
Iteration 142/1000 | Loss: 0.00001345
Iteration 143/1000 | Loss: 0.00001345
Iteration 144/1000 | Loss: 0.00001345
Iteration 145/1000 | Loss: 0.00001344
Iteration 146/1000 | Loss: 0.00001344
Iteration 147/1000 | Loss: 0.00001344
Iteration 148/1000 | Loss: 0.00001344
Iteration 149/1000 | Loss: 0.00001344
Iteration 150/1000 | Loss: 0.00001344
Iteration 151/1000 | Loss: 0.00001344
Iteration 152/1000 | Loss: 0.00001344
Iteration 153/1000 | Loss: 0.00001343
Iteration 154/1000 | Loss: 0.00001343
Iteration 155/1000 | Loss: 0.00001343
Iteration 156/1000 | Loss: 0.00001343
Iteration 157/1000 | Loss: 0.00001343
Iteration 158/1000 | Loss: 0.00001343
Iteration 159/1000 | Loss: 0.00001343
Iteration 160/1000 | Loss: 0.00001343
Iteration 161/1000 | Loss: 0.00001343
Iteration 162/1000 | Loss: 0.00001342
Iteration 163/1000 | Loss: 0.00001342
Iteration 164/1000 | Loss: 0.00001342
Iteration 165/1000 | Loss: 0.00001342
Iteration 166/1000 | Loss: 0.00001342
Iteration 167/1000 | Loss: 0.00001342
Iteration 168/1000 | Loss: 0.00001342
Iteration 169/1000 | Loss: 0.00001342
Iteration 170/1000 | Loss: 0.00001342
Iteration 171/1000 | Loss: 0.00001342
Iteration 172/1000 | Loss: 0.00001341
Iteration 173/1000 | Loss: 0.00001341
Iteration 174/1000 | Loss: 0.00001341
Iteration 175/1000 | Loss: 0.00001341
Iteration 176/1000 | Loss: 0.00001341
Iteration 177/1000 | Loss: 0.00001341
Iteration 178/1000 | Loss: 0.00001341
Iteration 179/1000 | Loss: 0.00001341
Iteration 180/1000 | Loss: 0.00001341
Iteration 181/1000 | Loss: 0.00001340
Iteration 182/1000 | Loss: 0.00001340
Iteration 183/1000 | Loss: 0.00001340
Iteration 184/1000 | Loss: 0.00001340
Iteration 185/1000 | Loss: 0.00001340
Iteration 186/1000 | Loss: 0.00001340
Iteration 187/1000 | Loss: 0.00001340
Iteration 188/1000 | Loss: 0.00001340
Iteration 189/1000 | Loss: 0.00001340
Iteration 190/1000 | Loss: 0.00001340
Iteration 191/1000 | Loss: 0.00001340
Iteration 192/1000 | Loss: 0.00001340
Iteration 193/1000 | Loss: 0.00001340
Iteration 194/1000 | Loss: 0.00001340
Iteration 195/1000 | Loss: 0.00001340
Iteration 196/1000 | Loss: 0.00001340
Iteration 197/1000 | Loss: 0.00001340
Iteration 198/1000 | Loss: 0.00001340
Iteration 199/1000 | Loss: 0.00001339
Iteration 200/1000 | Loss: 0.00001339
Iteration 201/1000 | Loss: 0.00001339
Iteration 202/1000 | Loss: 0.00001339
Iteration 203/1000 | Loss: 0.00001339
Iteration 204/1000 | Loss: 0.00001339
Iteration 205/1000 | Loss: 0.00001339
Iteration 206/1000 | Loss: 0.00001339
Iteration 207/1000 | Loss: 0.00001339
Iteration 208/1000 | Loss: 0.00001339
Iteration 209/1000 | Loss: 0.00001339
Iteration 210/1000 | Loss: 0.00001339
Iteration 211/1000 | Loss: 0.00001339
Iteration 212/1000 | Loss: 0.00001338
Iteration 213/1000 | Loss: 0.00001338
Iteration 214/1000 | Loss: 0.00001338
Iteration 215/1000 | Loss: 0.00001338
Iteration 216/1000 | Loss: 0.00001338
Iteration 217/1000 | Loss: 0.00001338
Iteration 218/1000 | Loss: 0.00001338
Iteration 219/1000 | Loss: 0.00001338
Iteration 220/1000 | Loss: 0.00001338
Iteration 221/1000 | Loss: 0.00001338
Iteration 222/1000 | Loss: 0.00001338
Iteration 223/1000 | Loss: 0.00001338
Iteration 224/1000 | Loss: 0.00001338
Iteration 225/1000 | Loss: 0.00001338
Iteration 226/1000 | Loss: 0.00001338
Iteration 227/1000 | Loss: 0.00001338
Iteration 228/1000 | Loss: 0.00001338
Iteration 229/1000 | Loss: 0.00001338
Iteration 230/1000 | Loss: 0.00001338
Iteration 231/1000 | Loss: 0.00001338
Iteration 232/1000 | Loss: 0.00001337
Iteration 233/1000 | Loss: 0.00001337
Iteration 234/1000 | Loss: 0.00001337
Iteration 235/1000 | Loss: 0.00001337
Iteration 236/1000 | Loss: 0.00001337
Iteration 237/1000 | Loss: 0.00001337
Iteration 238/1000 | Loss: 0.00001337
Iteration 239/1000 | Loss: 0.00001337
Iteration 240/1000 | Loss: 0.00001337
Iteration 241/1000 | Loss: 0.00001337
Iteration 242/1000 | Loss: 0.00001337
Iteration 243/1000 | Loss: 0.00001337
Iteration 244/1000 | Loss: 0.00001336
Iteration 245/1000 | Loss: 0.00001336
Iteration 246/1000 | Loss: 0.00001336
Iteration 247/1000 | Loss: 0.00001336
Iteration 248/1000 | Loss: 0.00001336
Iteration 249/1000 | Loss: 0.00001336
Iteration 250/1000 | Loss: 0.00001336
Iteration 251/1000 | Loss: 0.00001336
Iteration 252/1000 | Loss: 0.00001336
Iteration 253/1000 | Loss: 0.00001336
Iteration 254/1000 | Loss: 0.00001336
Iteration 255/1000 | Loss: 0.00001336
Iteration 256/1000 | Loss: 0.00001336
Iteration 257/1000 | Loss: 0.00001336
Iteration 258/1000 | Loss: 0.00001336
Iteration 259/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.3363910511543509e-05, 1.3363910511543509e-05, 1.3363910511543509e-05, 1.3363910511543509e-05, 1.3363910511543509e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3363910511543509e-05

Optimization complete. Final v2v error: 2.997119903564453 mm

Highest mean error: 5.700321197509766 mm for frame 56

Lowest mean error: 2.6458542346954346 mm for frame 101

Saving results

Total time: 105.95853042602539
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00696723
Iteration 2/25 | Loss: 0.00180647
Iteration 3/25 | Loss: 0.00142305
Iteration 4/25 | Loss: 0.00134271
Iteration 5/25 | Loss: 0.00131706
Iteration 6/25 | Loss: 0.00131319
Iteration 7/25 | Loss: 0.00130916
Iteration 8/25 | Loss: 0.00130344
Iteration 9/25 | Loss: 0.00130309
Iteration 10/25 | Loss: 0.00131208
Iteration 11/25 | Loss: 0.00130424
Iteration 12/25 | Loss: 0.00129795
Iteration 13/25 | Loss: 0.00129658
Iteration 14/25 | Loss: 0.00129998
Iteration 15/25 | Loss: 0.00129461
Iteration 16/25 | Loss: 0.00129345
Iteration 17/25 | Loss: 0.00129311
Iteration 18/25 | Loss: 0.00129303
Iteration 19/25 | Loss: 0.00129303
Iteration 20/25 | Loss: 0.00129302
Iteration 21/25 | Loss: 0.00129302
Iteration 22/25 | Loss: 0.00129302
Iteration 23/25 | Loss: 0.00129302
Iteration 24/25 | Loss: 0.00129302
Iteration 25/25 | Loss: 0.00129302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07774401
Iteration 2/25 | Loss: 0.00241588
Iteration 3/25 | Loss: 0.00241585
Iteration 4/25 | Loss: 0.00241585
Iteration 5/25 | Loss: 0.00241585
Iteration 6/25 | Loss: 0.00241585
Iteration 7/25 | Loss: 0.00241585
Iteration 8/25 | Loss: 0.00241585
Iteration 9/25 | Loss: 0.00241585
Iteration 10/25 | Loss: 0.00241585
Iteration 11/25 | Loss: 0.00241585
Iteration 12/25 | Loss: 0.00241585
Iteration 13/25 | Loss: 0.00241585
Iteration 14/25 | Loss: 0.00241585
Iteration 15/25 | Loss: 0.00241585
Iteration 16/25 | Loss: 0.00241585
Iteration 17/25 | Loss: 0.00241585
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002415850292891264, 0.002415850292891264, 0.002415850292891264, 0.002415850292891264, 0.002415850292891264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002415850292891264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00241585
Iteration 2/1000 | Loss: 0.00004239
Iteration 3/1000 | Loss: 0.00002714
Iteration 4/1000 | Loss: 0.00002325
Iteration 5/1000 | Loss: 0.00002115
Iteration 6/1000 | Loss: 0.00001989
Iteration 7/1000 | Loss: 0.00001898
Iteration 8/1000 | Loss: 0.00001847
Iteration 9/1000 | Loss: 0.00001809
Iteration 10/1000 | Loss: 0.00001766
Iteration 11/1000 | Loss: 0.00012321
Iteration 12/1000 | Loss: 0.00001743
Iteration 13/1000 | Loss: 0.00001719
Iteration 14/1000 | Loss: 0.00001711
Iteration 15/1000 | Loss: 0.00001704
Iteration 16/1000 | Loss: 0.00018763
Iteration 17/1000 | Loss: 0.00002009
Iteration 18/1000 | Loss: 0.00001798
Iteration 19/1000 | Loss: 0.00001684
Iteration 20/1000 | Loss: 0.00001612
Iteration 21/1000 | Loss: 0.00001555
Iteration 22/1000 | Loss: 0.00001537
Iteration 23/1000 | Loss: 0.00001535
Iteration 24/1000 | Loss: 0.00001534
Iteration 25/1000 | Loss: 0.00001533
Iteration 26/1000 | Loss: 0.00001532
Iteration 27/1000 | Loss: 0.00001514
Iteration 28/1000 | Loss: 0.00001513
Iteration 29/1000 | Loss: 0.00001512
Iteration 30/1000 | Loss: 0.00001510
Iteration 31/1000 | Loss: 0.00001505
Iteration 32/1000 | Loss: 0.00001505
Iteration 33/1000 | Loss: 0.00001505
Iteration 34/1000 | Loss: 0.00001505
Iteration 35/1000 | Loss: 0.00001503
Iteration 36/1000 | Loss: 0.00001503
Iteration 37/1000 | Loss: 0.00001502
Iteration 38/1000 | Loss: 0.00001502
Iteration 39/1000 | Loss: 0.00001501
Iteration 40/1000 | Loss: 0.00001501
Iteration 41/1000 | Loss: 0.00001501
Iteration 42/1000 | Loss: 0.00001500
Iteration 43/1000 | Loss: 0.00001500
Iteration 44/1000 | Loss: 0.00001500
Iteration 45/1000 | Loss: 0.00001500
Iteration 46/1000 | Loss: 0.00001500
Iteration 47/1000 | Loss: 0.00001499
Iteration 48/1000 | Loss: 0.00001497
Iteration 49/1000 | Loss: 0.00001497
Iteration 50/1000 | Loss: 0.00001496
Iteration 51/1000 | Loss: 0.00001496
Iteration 52/1000 | Loss: 0.00001496
Iteration 53/1000 | Loss: 0.00001495
Iteration 54/1000 | Loss: 0.00001494
Iteration 55/1000 | Loss: 0.00001494
Iteration 56/1000 | Loss: 0.00001494
Iteration 57/1000 | Loss: 0.00001494
Iteration 58/1000 | Loss: 0.00001493
Iteration 59/1000 | Loss: 0.00001493
Iteration 60/1000 | Loss: 0.00001493
Iteration 61/1000 | Loss: 0.00001493
Iteration 62/1000 | Loss: 0.00001493
Iteration 63/1000 | Loss: 0.00001493
Iteration 64/1000 | Loss: 0.00001492
Iteration 65/1000 | Loss: 0.00001492
Iteration 66/1000 | Loss: 0.00001492
Iteration 67/1000 | Loss: 0.00001491
Iteration 68/1000 | Loss: 0.00001491
Iteration 69/1000 | Loss: 0.00001490
Iteration 70/1000 | Loss: 0.00001490
Iteration 71/1000 | Loss: 0.00001490
Iteration 72/1000 | Loss: 0.00001488
Iteration 73/1000 | Loss: 0.00001488
Iteration 74/1000 | Loss: 0.00001487
Iteration 75/1000 | Loss: 0.00001487
Iteration 76/1000 | Loss: 0.00001487
Iteration 77/1000 | Loss: 0.00001485
Iteration 78/1000 | Loss: 0.00001485
Iteration 79/1000 | Loss: 0.00001484
Iteration 80/1000 | Loss: 0.00001484
Iteration 81/1000 | Loss: 0.00001484
Iteration 82/1000 | Loss: 0.00001483
Iteration 83/1000 | Loss: 0.00001482
Iteration 84/1000 | Loss: 0.00001482
Iteration 85/1000 | Loss: 0.00001481
Iteration 86/1000 | Loss: 0.00001481
Iteration 87/1000 | Loss: 0.00001480
Iteration 88/1000 | Loss: 0.00001480
Iteration 89/1000 | Loss: 0.00001480
Iteration 90/1000 | Loss: 0.00001479
Iteration 91/1000 | Loss: 0.00001479
Iteration 92/1000 | Loss: 0.00001479
Iteration 93/1000 | Loss: 0.00001479
Iteration 94/1000 | Loss: 0.00001479
Iteration 95/1000 | Loss: 0.00001479
Iteration 96/1000 | Loss: 0.00001478
Iteration 97/1000 | Loss: 0.00001478
Iteration 98/1000 | Loss: 0.00001478
Iteration 99/1000 | Loss: 0.00001478
Iteration 100/1000 | Loss: 0.00001478
Iteration 101/1000 | Loss: 0.00001478
Iteration 102/1000 | Loss: 0.00001478
Iteration 103/1000 | Loss: 0.00001478
Iteration 104/1000 | Loss: 0.00001478
Iteration 105/1000 | Loss: 0.00001477
Iteration 106/1000 | Loss: 0.00001477
Iteration 107/1000 | Loss: 0.00001475
Iteration 108/1000 | Loss: 0.00001475
Iteration 109/1000 | Loss: 0.00001475
Iteration 110/1000 | Loss: 0.00001475
Iteration 111/1000 | Loss: 0.00001475
Iteration 112/1000 | Loss: 0.00001475
Iteration 113/1000 | Loss: 0.00001475
Iteration 114/1000 | Loss: 0.00001474
Iteration 115/1000 | Loss: 0.00001474
Iteration 116/1000 | Loss: 0.00001474
Iteration 117/1000 | Loss: 0.00001474
Iteration 118/1000 | Loss: 0.00001474
Iteration 119/1000 | Loss: 0.00001473
Iteration 120/1000 | Loss: 0.00001473
Iteration 121/1000 | Loss: 0.00001473
Iteration 122/1000 | Loss: 0.00001473
Iteration 123/1000 | Loss: 0.00001473
Iteration 124/1000 | Loss: 0.00001473
Iteration 125/1000 | Loss: 0.00001473
Iteration 126/1000 | Loss: 0.00001473
Iteration 127/1000 | Loss: 0.00001473
Iteration 128/1000 | Loss: 0.00001472
Iteration 129/1000 | Loss: 0.00001472
Iteration 130/1000 | Loss: 0.00001472
Iteration 131/1000 | Loss: 0.00001472
Iteration 132/1000 | Loss: 0.00001472
Iteration 133/1000 | Loss: 0.00001471
Iteration 134/1000 | Loss: 0.00001471
Iteration 135/1000 | Loss: 0.00001471
Iteration 136/1000 | Loss: 0.00001471
Iteration 137/1000 | Loss: 0.00001470
Iteration 138/1000 | Loss: 0.00001470
Iteration 139/1000 | Loss: 0.00001470
Iteration 140/1000 | Loss: 0.00001470
Iteration 141/1000 | Loss: 0.00001470
Iteration 142/1000 | Loss: 0.00001470
Iteration 143/1000 | Loss: 0.00001470
Iteration 144/1000 | Loss: 0.00001470
Iteration 145/1000 | Loss: 0.00001470
Iteration 146/1000 | Loss: 0.00001470
Iteration 147/1000 | Loss: 0.00001470
Iteration 148/1000 | Loss: 0.00001470
Iteration 149/1000 | Loss: 0.00001469
Iteration 150/1000 | Loss: 0.00001469
Iteration 151/1000 | Loss: 0.00001469
Iteration 152/1000 | Loss: 0.00001469
Iteration 153/1000 | Loss: 0.00001469
Iteration 154/1000 | Loss: 0.00001469
Iteration 155/1000 | Loss: 0.00001469
Iteration 156/1000 | Loss: 0.00001469
Iteration 157/1000 | Loss: 0.00001469
Iteration 158/1000 | Loss: 0.00001469
Iteration 159/1000 | Loss: 0.00001469
Iteration 160/1000 | Loss: 0.00001469
Iteration 161/1000 | Loss: 0.00001469
Iteration 162/1000 | Loss: 0.00001468
Iteration 163/1000 | Loss: 0.00001468
Iteration 164/1000 | Loss: 0.00001468
Iteration 165/1000 | Loss: 0.00001468
Iteration 166/1000 | Loss: 0.00001468
Iteration 167/1000 | Loss: 0.00001468
Iteration 168/1000 | Loss: 0.00001468
Iteration 169/1000 | Loss: 0.00001468
Iteration 170/1000 | Loss: 0.00001468
Iteration 171/1000 | Loss: 0.00001468
Iteration 172/1000 | Loss: 0.00001468
Iteration 173/1000 | Loss: 0.00001468
Iteration 174/1000 | Loss: 0.00001468
Iteration 175/1000 | Loss: 0.00001468
Iteration 176/1000 | Loss: 0.00001468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.4681284483231138e-05, 1.4681284483231138e-05, 1.4681284483231138e-05, 1.4681284483231138e-05, 1.4681284483231138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4681284483231138e-05

Optimization complete. Final v2v error: 3.1857917308807373 mm

Highest mean error: 4.177692413330078 mm for frame 19

Lowest mean error: 2.6658096313476562 mm for frame 132

Saving results

Total time: 74.4717185497284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00570008
Iteration 2/25 | Loss: 0.00128347
Iteration 3/25 | Loss: 0.00120254
Iteration 4/25 | Loss: 0.00119805
Iteration 5/25 | Loss: 0.00119733
Iteration 6/25 | Loss: 0.00119733
Iteration 7/25 | Loss: 0.00119733
Iteration 8/25 | Loss: 0.00119733
Iteration 9/25 | Loss: 0.00119733
Iteration 10/25 | Loss: 0.00119733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011973320506513119, 0.0011973320506513119, 0.0011973320506513119, 0.0011973320506513119, 0.0011973320506513119]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011973320506513119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.70616627
Iteration 2/25 | Loss: 0.00288361
Iteration 3/25 | Loss: 0.00288360
Iteration 4/25 | Loss: 0.00288360
Iteration 5/25 | Loss: 0.00288360
Iteration 6/25 | Loss: 0.00288360
Iteration 7/25 | Loss: 0.00288360
Iteration 8/25 | Loss: 0.00288360
Iteration 9/25 | Loss: 0.00288360
Iteration 10/25 | Loss: 0.00288360
Iteration 11/25 | Loss: 0.00288360
Iteration 12/25 | Loss: 0.00288360
Iteration 13/25 | Loss: 0.00288360
Iteration 14/25 | Loss: 0.00288360
Iteration 15/25 | Loss: 0.00288360
Iteration 16/25 | Loss: 0.00288360
Iteration 17/25 | Loss: 0.00288360
Iteration 18/25 | Loss: 0.00288360
Iteration 19/25 | Loss: 0.00288360
Iteration 20/25 | Loss: 0.00288360
Iteration 21/25 | Loss: 0.00288360
Iteration 22/25 | Loss: 0.00288360
Iteration 23/25 | Loss: 0.00288360
Iteration 24/25 | Loss: 0.00288360
Iteration 25/25 | Loss: 0.00288360

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288360
Iteration 2/1000 | Loss: 0.00002792
Iteration 3/1000 | Loss: 0.00001707
Iteration 4/1000 | Loss: 0.00001360
Iteration 5/1000 | Loss: 0.00001225
Iteration 6/1000 | Loss: 0.00001145
Iteration 7/1000 | Loss: 0.00001105
Iteration 8/1000 | Loss: 0.00001055
Iteration 9/1000 | Loss: 0.00001036
Iteration 10/1000 | Loss: 0.00001033
Iteration 11/1000 | Loss: 0.00001031
Iteration 12/1000 | Loss: 0.00001030
Iteration 13/1000 | Loss: 0.00001029
Iteration 14/1000 | Loss: 0.00001024
Iteration 15/1000 | Loss: 0.00001023
Iteration 16/1000 | Loss: 0.00001016
Iteration 17/1000 | Loss: 0.00001016
Iteration 18/1000 | Loss: 0.00001016
Iteration 19/1000 | Loss: 0.00001016
Iteration 20/1000 | Loss: 0.00001016
Iteration 21/1000 | Loss: 0.00001016
Iteration 22/1000 | Loss: 0.00001016
Iteration 23/1000 | Loss: 0.00001016
Iteration 24/1000 | Loss: 0.00001015
Iteration 25/1000 | Loss: 0.00001015
Iteration 26/1000 | Loss: 0.00001015
Iteration 27/1000 | Loss: 0.00001015
Iteration 28/1000 | Loss: 0.00001015
Iteration 29/1000 | Loss: 0.00001015
Iteration 30/1000 | Loss: 0.00001013
Iteration 31/1000 | Loss: 0.00001013
Iteration 32/1000 | Loss: 0.00001012
Iteration 33/1000 | Loss: 0.00001012
Iteration 34/1000 | Loss: 0.00001012
Iteration 35/1000 | Loss: 0.00001011
Iteration 36/1000 | Loss: 0.00001011
Iteration 37/1000 | Loss: 0.00001011
Iteration 38/1000 | Loss: 0.00001010
Iteration 39/1000 | Loss: 0.00001010
Iteration 40/1000 | Loss: 0.00001010
Iteration 41/1000 | Loss: 0.00001009
Iteration 42/1000 | Loss: 0.00001009
Iteration 43/1000 | Loss: 0.00001009
Iteration 44/1000 | Loss: 0.00001009
Iteration 45/1000 | Loss: 0.00001009
Iteration 46/1000 | Loss: 0.00001009
Iteration 47/1000 | Loss: 0.00001009
Iteration 48/1000 | Loss: 0.00001009
Iteration 49/1000 | Loss: 0.00001009
Iteration 50/1000 | Loss: 0.00001008
Iteration 51/1000 | Loss: 0.00001008
Iteration 52/1000 | Loss: 0.00001008
Iteration 53/1000 | Loss: 0.00001008
Iteration 54/1000 | Loss: 0.00001008
Iteration 55/1000 | Loss: 0.00001008
Iteration 56/1000 | Loss: 0.00001008
Iteration 57/1000 | Loss: 0.00001008
Iteration 58/1000 | Loss: 0.00001007
Iteration 59/1000 | Loss: 0.00001007
Iteration 60/1000 | Loss: 0.00001007
Iteration 61/1000 | Loss: 0.00001007
Iteration 62/1000 | Loss: 0.00001006
Iteration 63/1000 | Loss: 0.00001006
Iteration 64/1000 | Loss: 0.00001005
Iteration 65/1000 | Loss: 0.00001005
Iteration 66/1000 | Loss: 0.00001005
Iteration 67/1000 | Loss: 0.00001005
Iteration 68/1000 | Loss: 0.00001005
Iteration 69/1000 | Loss: 0.00001005
Iteration 70/1000 | Loss: 0.00001005
Iteration 71/1000 | Loss: 0.00001005
Iteration 72/1000 | Loss: 0.00001005
Iteration 73/1000 | Loss: 0.00001005
Iteration 74/1000 | Loss: 0.00001005
Iteration 75/1000 | Loss: 0.00001005
Iteration 76/1000 | Loss: 0.00001004
Iteration 77/1000 | Loss: 0.00001004
Iteration 78/1000 | Loss: 0.00001004
Iteration 79/1000 | Loss: 0.00001004
Iteration 80/1000 | Loss: 0.00001004
Iteration 81/1000 | Loss: 0.00001004
Iteration 82/1000 | Loss: 0.00001004
Iteration 83/1000 | Loss: 0.00001004
Iteration 84/1000 | Loss: 0.00001004
Iteration 85/1000 | Loss: 0.00001004
Iteration 86/1000 | Loss: 0.00001004
Iteration 87/1000 | Loss: 0.00001004
Iteration 88/1000 | Loss: 0.00001004
Iteration 89/1000 | Loss: 0.00001004
Iteration 90/1000 | Loss: 0.00001004
Iteration 91/1000 | Loss: 0.00001004
Iteration 92/1000 | Loss: 0.00001004
Iteration 93/1000 | Loss: 0.00001003
Iteration 94/1000 | Loss: 0.00001003
Iteration 95/1000 | Loss: 0.00001003
Iteration 96/1000 | Loss: 0.00001003
Iteration 97/1000 | Loss: 0.00001003
Iteration 98/1000 | Loss: 0.00001003
Iteration 99/1000 | Loss: 0.00001003
Iteration 100/1000 | Loss: 0.00001003
Iteration 101/1000 | Loss: 0.00001003
Iteration 102/1000 | Loss: 0.00001003
Iteration 103/1000 | Loss: 0.00001003
Iteration 104/1000 | Loss: 0.00001003
Iteration 105/1000 | Loss: 0.00001003
Iteration 106/1000 | Loss: 0.00001003
Iteration 107/1000 | Loss: 0.00001003
Iteration 108/1000 | Loss: 0.00001003
Iteration 109/1000 | Loss: 0.00001003
Iteration 110/1000 | Loss: 0.00001003
Iteration 111/1000 | Loss: 0.00001003
Iteration 112/1000 | Loss: 0.00001003
Iteration 113/1000 | Loss: 0.00001003
Iteration 114/1000 | Loss: 0.00001003
Iteration 115/1000 | Loss: 0.00001003
Iteration 116/1000 | Loss: 0.00001003
Iteration 117/1000 | Loss: 0.00001003
Iteration 118/1000 | Loss: 0.00001003
Iteration 119/1000 | Loss: 0.00001003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.0030549674411304e-05, 1.0030549674411304e-05, 1.0030549674411304e-05, 1.0030549674411304e-05, 1.0030549674411304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0030549674411304e-05

Optimization complete. Final v2v error: 2.6362106800079346 mm

Highest mean error: 3.0525295734405518 mm for frame 30

Lowest mean error: 2.2843258380889893 mm for frame 67

Saving results

Total time: 27.922210216522217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973430
Iteration 2/25 | Loss: 0.00271607
Iteration 3/25 | Loss: 0.00199550
Iteration 4/25 | Loss: 0.00180599
Iteration 5/25 | Loss: 0.00158779
Iteration 6/25 | Loss: 0.00147996
Iteration 7/25 | Loss: 0.00143326
Iteration 8/25 | Loss: 0.00141577
Iteration 9/25 | Loss: 0.00140730
Iteration 10/25 | Loss: 0.00139095
Iteration 11/25 | Loss: 0.00139135
Iteration 12/25 | Loss: 0.00138686
Iteration 13/25 | Loss: 0.00139219
Iteration 14/25 | Loss: 0.00138436
Iteration 15/25 | Loss: 0.00138704
Iteration 16/25 | Loss: 0.00138376
Iteration 17/25 | Loss: 0.00138142
Iteration 18/25 | Loss: 0.00138041
Iteration 19/25 | Loss: 0.00138003
Iteration 20/25 | Loss: 0.00137932
Iteration 21/25 | Loss: 0.00137868
Iteration 22/25 | Loss: 0.00137861
Iteration 23/25 | Loss: 0.00137861
Iteration 24/25 | Loss: 0.00137856
Iteration 25/25 | Loss: 0.00137939

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15565050
Iteration 2/25 | Loss: 0.00444349
Iteration 3/25 | Loss: 0.00378241
Iteration 4/25 | Loss: 0.00378238
Iteration 5/25 | Loss: 0.00378237
Iteration 6/25 | Loss: 0.00378237
Iteration 7/25 | Loss: 0.00378237
Iteration 8/25 | Loss: 0.00378237
Iteration 9/25 | Loss: 0.00378237
Iteration 10/25 | Loss: 0.00378237
Iteration 11/25 | Loss: 0.00378237
Iteration 12/25 | Loss: 0.00378237
Iteration 13/25 | Loss: 0.00378237
Iteration 14/25 | Loss: 0.00378237
Iteration 15/25 | Loss: 0.00378237
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0037823719903826714, 0.0037823719903826714, 0.0037823719903826714, 0.0037823719903826714, 0.0037823719903826714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037823719903826714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00378237
Iteration 2/1000 | Loss: 0.00059964
Iteration 3/1000 | Loss: 0.00016664
Iteration 4/1000 | Loss: 0.00035415
Iteration 5/1000 | Loss: 0.00018568
Iteration 6/1000 | Loss: 0.00113561
Iteration 7/1000 | Loss: 0.00010567
Iteration 8/1000 | Loss: 0.00008514
Iteration 9/1000 | Loss: 0.00025872
Iteration 10/1000 | Loss: 0.00012938
Iteration 11/1000 | Loss: 0.00024860
Iteration 12/1000 | Loss: 0.00060801
Iteration 13/1000 | Loss: 0.00042813
Iteration 14/1000 | Loss: 0.00041691
Iteration 15/1000 | Loss: 0.00028885
Iteration 16/1000 | Loss: 0.00007445
Iteration 17/1000 | Loss: 0.00041743
Iteration 18/1000 | Loss: 0.00007052
Iteration 19/1000 | Loss: 0.00014983
Iteration 20/1000 | Loss: 0.00049129
Iteration 21/1000 | Loss: 0.00016794
Iteration 22/1000 | Loss: 0.00011991
Iteration 23/1000 | Loss: 0.00035890
Iteration 24/1000 | Loss: 0.00006284
Iteration 25/1000 | Loss: 0.00013504
Iteration 26/1000 | Loss: 0.00045036
Iteration 27/1000 | Loss: 0.00006326
Iteration 28/1000 | Loss: 0.00035683
Iteration 29/1000 | Loss: 0.00063830
Iteration 30/1000 | Loss: 0.00117724
Iteration 31/1000 | Loss: 0.00034079
Iteration 32/1000 | Loss: 0.00012957
Iteration 33/1000 | Loss: 0.00041340
Iteration 34/1000 | Loss: 0.00277780
Iteration 35/1000 | Loss: 0.00295416
Iteration 36/1000 | Loss: 0.00041378
Iteration 37/1000 | Loss: 0.00011492
Iteration 38/1000 | Loss: 0.00008566
Iteration 39/1000 | Loss: 0.00005680
Iteration 40/1000 | Loss: 0.00069598
Iteration 41/1000 | Loss: 0.00020920
Iteration 42/1000 | Loss: 0.00053806
Iteration 43/1000 | Loss: 0.00047645
Iteration 44/1000 | Loss: 0.00044508
Iteration 45/1000 | Loss: 0.00024658
Iteration 46/1000 | Loss: 0.00007229
Iteration 47/1000 | Loss: 0.00014450
Iteration 48/1000 | Loss: 0.00004719
Iteration 49/1000 | Loss: 0.00004570
Iteration 50/1000 | Loss: 0.00016248
Iteration 51/1000 | Loss: 0.00004619
Iteration 52/1000 | Loss: 0.00004384
Iteration 53/1000 | Loss: 0.00004307
Iteration 54/1000 | Loss: 0.00004245
Iteration 55/1000 | Loss: 0.00024706
Iteration 56/1000 | Loss: 0.00004194
Iteration 57/1000 | Loss: 0.00004151
Iteration 58/1000 | Loss: 0.00018399
Iteration 59/1000 | Loss: 0.00004211
Iteration 60/1000 | Loss: 0.00004108
Iteration 61/1000 | Loss: 0.00004081
Iteration 62/1000 | Loss: 0.00029863
Iteration 63/1000 | Loss: 0.00004753
Iteration 64/1000 | Loss: 0.00007138
Iteration 65/1000 | Loss: 0.00004141
Iteration 66/1000 | Loss: 0.00006845
Iteration 67/1000 | Loss: 0.00004032
Iteration 68/1000 | Loss: 0.00017293
Iteration 69/1000 | Loss: 0.00003949
Iteration 70/1000 | Loss: 0.00003905
Iteration 71/1000 | Loss: 0.00014828
Iteration 72/1000 | Loss: 0.00004498
Iteration 73/1000 | Loss: 0.00005849
Iteration 74/1000 | Loss: 0.00005012
Iteration 75/1000 | Loss: 0.00003911
Iteration 76/1000 | Loss: 0.00003815
Iteration 77/1000 | Loss: 0.00003753
Iteration 78/1000 | Loss: 0.00006847
Iteration 79/1000 | Loss: 0.00003959
Iteration 80/1000 | Loss: 0.00003775
Iteration 81/1000 | Loss: 0.00005754
Iteration 82/1000 | Loss: 0.00003692
Iteration 83/1000 | Loss: 0.00005174
Iteration 84/1000 | Loss: 0.00003884
Iteration 85/1000 | Loss: 0.00006051
Iteration 86/1000 | Loss: 0.00003686
Iteration 87/1000 | Loss: 0.00003677
Iteration 88/1000 | Loss: 0.00004319
Iteration 89/1000 | Loss: 0.00003667
Iteration 90/1000 | Loss: 0.00003666
Iteration 91/1000 | Loss: 0.00003666
Iteration 92/1000 | Loss: 0.00003666
Iteration 93/1000 | Loss: 0.00003666
Iteration 94/1000 | Loss: 0.00003665
Iteration 95/1000 | Loss: 0.00003665
Iteration 96/1000 | Loss: 0.00003665
Iteration 97/1000 | Loss: 0.00003665
Iteration 98/1000 | Loss: 0.00003664
Iteration 99/1000 | Loss: 0.00003664
Iteration 100/1000 | Loss: 0.00003664
Iteration 101/1000 | Loss: 0.00003663
Iteration 102/1000 | Loss: 0.00003661
Iteration 103/1000 | Loss: 0.00004902
Iteration 104/1000 | Loss: 0.00003665
Iteration 105/1000 | Loss: 0.00003664
Iteration 106/1000 | Loss: 0.00003664
Iteration 107/1000 | Loss: 0.00003657
Iteration 108/1000 | Loss: 0.00003657
Iteration 109/1000 | Loss: 0.00003656
Iteration 110/1000 | Loss: 0.00003656
Iteration 111/1000 | Loss: 0.00003656
Iteration 112/1000 | Loss: 0.00003655
Iteration 113/1000 | Loss: 0.00003655
Iteration 114/1000 | Loss: 0.00003654
Iteration 115/1000 | Loss: 0.00003654
Iteration 116/1000 | Loss: 0.00003654
Iteration 117/1000 | Loss: 0.00003654
Iteration 118/1000 | Loss: 0.00003654
Iteration 119/1000 | Loss: 0.00003654
Iteration 120/1000 | Loss: 0.00003653
Iteration 121/1000 | Loss: 0.00003653
Iteration 122/1000 | Loss: 0.00003653
Iteration 123/1000 | Loss: 0.00003653
Iteration 124/1000 | Loss: 0.00003652
Iteration 125/1000 | Loss: 0.00003652
Iteration 126/1000 | Loss: 0.00003652
Iteration 127/1000 | Loss: 0.00003651
Iteration 128/1000 | Loss: 0.00003651
Iteration 129/1000 | Loss: 0.00003651
Iteration 130/1000 | Loss: 0.00003651
Iteration 131/1000 | Loss: 0.00003651
Iteration 132/1000 | Loss: 0.00003651
Iteration 133/1000 | Loss: 0.00003651
Iteration 134/1000 | Loss: 0.00003651
Iteration 135/1000 | Loss: 0.00003650
Iteration 136/1000 | Loss: 0.00003650
Iteration 137/1000 | Loss: 0.00003650
Iteration 138/1000 | Loss: 0.00003650
Iteration 139/1000 | Loss: 0.00003649
Iteration 140/1000 | Loss: 0.00003649
Iteration 141/1000 | Loss: 0.00003649
Iteration 142/1000 | Loss: 0.00003649
Iteration 143/1000 | Loss: 0.00003649
Iteration 144/1000 | Loss: 0.00003649
Iteration 145/1000 | Loss: 0.00003649
Iteration 146/1000 | Loss: 0.00003649
Iteration 147/1000 | Loss: 0.00003649
Iteration 148/1000 | Loss: 0.00003648
Iteration 149/1000 | Loss: 0.00003648
Iteration 150/1000 | Loss: 0.00003648
Iteration 151/1000 | Loss: 0.00003648
Iteration 152/1000 | Loss: 0.00003648
Iteration 153/1000 | Loss: 0.00003648
Iteration 154/1000 | Loss: 0.00003648
Iteration 155/1000 | Loss: 0.00003648
Iteration 156/1000 | Loss: 0.00003648
Iteration 157/1000 | Loss: 0.00003647
Iteration 158/1000 | Loss: 0.00003647
Iteration 159/1000 | Loss: 0.00003647
Iteration 160/1000 | Loss: 0.00003647
Iteration 161/1000 | Loss: 0.00003646
Iteration 162/1000 | Loss: 0.00003646
Iteration 163/1000 | Loss: 0.00003646
Iteration 164/1000 | Loss: 0.00003646
Iteration 165/1000 | Loss: 0.00003646
Iteration 166/1000 | Loss: 0.00003646
Iteration 167/1000 | Loss: 0.00003646
Iteration 168/1000 | Loss: 0.00003645
Iteration 169/1000 | Loss: 0.00003645
Iteration 170/1000 | Loss: 0.00003645
Iteration 171/1000 | Loss: 0.00003644
Iteration 172/1000 | Loss: 0.00003644
Iteration 173/1000 | Loss: 0.00003644
Iteration 174/1000 | Loss: 0.00003644
Iteration 175/1000 | Loss: 0.00003643
Iteration 176/1000 | Loss: 0.00003643
Iteration 177/1000 | Loss: 0.00003643
Iteration 178/1000 | Loss: 0.00003643
Iteration 179/1000 | Loss: 0.00003642
Iteration 180/1000 | Loss: 0.00003642
Iteration 181/1000 | Loss: 0.00003642
Iteration 182/1000 | Loss: 0.00003642
Iteration 183/1000 | Loss: 0.00003642
Iteration 184/1000 | Loss: 0.00003642
Iteration 185/1000 | Loss: 0.00003641
Iteration 186/1000 | Loss: 0.00003641
Iteration 187/1000 | Loss: 0.00003641
Iteration 188/1000 | Loss: 0.00003641
Iteration 189/1000 | Loss: 0.00003641
Iteration 190/1000 | Loss: 0.00003641
Iteration 191/1000 | Loss: 0.00003641
Iteration 192/1000 | Loss: 0.00003641
Iteration 193/1000 | Loss: 0.00003641
Iteration 194/1000 | Loss: 0.00003641
Iteration 195/1000 | Loss: 0.00003641
Iteration 196/1000 | Loss: 0.00003641
Iteration 197/1000 | Loss: 0.00003641
Iteration 198/1000 | Loss: 0.00003641
Iteration 199/1000 | Loss: 0.00003641
Iteration 200/1000 | Loss: 0.00003641
Iteration 201/1000 | Loss: 0.00003641
Iteration 202/1000 | Loss: 0.00003641
Iteration 203/1000 | Loss: 0.00003641
Iteration 204/1000 | Loss: 0.00003641
Iteration 205/1000 | Loss: 0.00003641
Iteration 206/1000 | Loss: 0.00003641
Iteration 207/1000 | Loss: 0.00003641
Iteration 208/1000 | Loss: 0.00003641
Iteration 209/1000 | Loss: 0.00003641
Iteration 210/1000 | Loss: 0.00003641
Iteration 211/1000 | Loss: 0.00003641
Iteration 212/1000 | Loss: 0.00003641
Iteration 213/1000 | Loss: 0.00003641
Iteration 214/1000 | Loss: 0.00003641
Iteration 215/1000 | Loss: 0.00003641
Iteration 216/1000 | Loss: 0.00003641
Iteration 217/1000 | Loss: 0.00003641
Iteration 218/1000 | Loss: 0.00003641
Iteration 219/1000 | Loss: 0.00003641
Iteration 220/1000 | Loss: 0.00003641
Iteration 221/1000 | Loss: 0.00003641
Iteration 222/1000 | Loss: 0.00003641
Iteration 223/1000 | Loss: 0.00003641
Iteration 224/1000 | Loss: 0.00003641
Iteration 225/1000 | Loss: 0.00003641
Iteration 226/1000 | Loss: 0.00003641
Iteration 227/1000 | Loss: 0.00003641
Iteration 228/1000 | Loss: 0.00003641
Iteration 229/1000 | Loss: 0.00003641
Iteration 230/1000 | Loss: 0.00003641
Iteration 231/1000 | Loss: 0.00003641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [3.6409026506589726e-05, 3.6409026506589726e-05, 3.6409026506589726e-05, 3.6409026506589726e-05, 3.6409026506589726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6409026506589726e-05

Optimization complete. Final v2v error: 3.396495819091797 mm

Highest mean error: 11.9927396774292 mm for frame 116

Lowest mean error: 2.6863467693328857 mm for frame 233

Saving results

Total time: 202.08407187461853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00914511
Iteration 2/25 | Loss: 0.00136572
Iteration 3/25 | Loss: 0.00124159
Iteration 4/25 | Loss: 0.00122001
Iteration 5/25 | Loss: 0.00121269
Iteration 6/25 | Loss: 0.00120964
Iteration 7/25 | Loss: 0.00120911
Iteration 8/25 | Loss: 0.00120911
Iteration 9/25 | Loss: 0.00120911
Iteration 10/25 | Loss: 0.00120911
Iteration 11/25 | Loss: 0.00120911
Iteration 12/25 | Loss: 0.00120911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012091145617887378, 0.0012091145617887378, 0.0012091145617887378, 0.0012091145617887378, 0.0012091145617887378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012091145617887378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25721478
Iteration 2/25 | Loss: 0.00282667
Iteration 3/25 | Loss: 0.00282666
Iteration 4/25 | Loss: 0.00282666
Iteration 5/25 | Loss: 0.00282666
Iteration 6/25 | Loss: 0.00282666
Iteration 7/25 | Loss: 0.00282666
Iteration 8/25 | Loss: 0.00282666
Iteration 9/25 | Loss: 0.00282666
Iteration 10/25 | Loss: 0.00282666
Iteration 11/25 | Loss: 0.00282666
Iteration 12/25 | Loss: 0.00282666
Iteration 13/25 | Loss: 0.00282666
Iteration 14/25 | Loss: 0.00282666
Iteration 15/25 | Loss: 0.00282666
Iteration 16/25 | Loss: 0.00282666
Iteration 17/25 | Loss: 0.00282666
Iteration 18/25 | Loss: 0.00282666
Iteration 19/25 | Loss: 0.00282666
Iteration 20/25 | Loss: 0.00282666
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0028266562148928642, 0.0028266562148928642, 0.0028266562148928642, 0.0028266562148928642, 0.0028266562148928642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028266562148928642

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00282666
Iteration 2/1000 | Loss: 0.00005590
Iteration 3/1000 | Loss: 0.00003123
Iteration 4/1000 | Loss: 0.00002510
Iteration 5/1000 | Loss: 0.00002180
Iteration 6/1000 | Loss: 0.00002013
Iteration 7/1000 | Loss: 0.00001847
Iteration 8/1000 | Loss: 0.00001773
Iteration 9/1000 | Loss: 0.00001730
Iteration 10/1000 | Loss: 0.00001698
Iteration 11/1000 | Loss: 0.00001676
Iteration 12/1000 | Loss: 0.00001656
Iteration 13/1000 | Loss: 0.00001641
Iteration 14/1000 | Loss: 0.00001637
Iteration 15/1000 | Loss: 0.00001632
Iteration 16/1000 | Loss: 0.00001631
Iteration 17/1000 | Loss: 0.00001631
Iteration 18/1000 | Loss: 0.00001630
Iteration 19/1000 | Loss: 0.00001630
Iteration 20/1000 | Loss: 0.00001629
Iteration 21/1000 | Loss: 0.00001628
Iteration 22/1000 | Loss: 0.00001626
Iteration 23/1000 | Loss: 0.00001625
Iteration 24/1000 | Loss: 0.00001625
Iteration 25/1000 | Loss: 0.00001625
Iteration 26/1000 | Loss: 0.00001624
Iteration 27/1000 | Loss: 0.00001623
Iteration 28/1000 | Loss: 0.00001622
Iteration 29/1000 | Loss: 0.00001621
Iteration 30/1000 | Loss: 0.00001620
Iteration 31/1000 | Loss: 0.00001620
Iteration 32/1000 | Loss: 0.00001620
Iteration 33/1000 | Loss: 0.00001620
Iteration 34/1000 | Loss: 0.00001620
Iteration 35/1000 | Loss: 0.00001619
Iteration 36/1000 | Loss: 0.00001616
Iteration 37/1000 | Loss: 0.00001616
Iteration 38/1000 | Loss: 0.00001615
Iteration 39/1000 | Loss: 0.00001615
Iteration 40/1000 | Loss: 0.00001614
Iteration 41/1000 | Loss: 0.00001614
Iteration 42/1000 | Loss: 0.00001613
Iteration 43/1000 | Loss: 0.00001613
Iteration 44/1000 | Loss: 0.00001613
Iteration 45/1000 | Loss: 0.00001612
Iteration 46/1000 | Loss: 0.00001612
Iteration 47/1000 | Loss: 0.00001612
Iteration 48/1000 | Loss: 0.00001612
Iteration 49/1000 | Loss: 0.00001612
Iteration 50/1000 | Loss: 0.00001612
Iteration 51/1000 | Loss: 0.00001612
Iteration 52/1000 | Loss: 0.00001612
Iteration 53/1000 | Loss: 0.00001612
Iteration 54/1000 | Loss: 0.00001611
Iteration 55/1000 | Loss: 0.00001611
Iteration 56/1000 | Loss: 0.00001611
Iteration 57/1000 | Loss: 0.00001611
Iteration 58/1000 | Loss: 0.00001611
Iteration 59/1000 | Loss: 0.00001611
Iteration 60/1000 | Loss: 0.00001611
Iteration 61/1000 | Loss: 0.00001611
Iteration 62/1000 | Loss: 0.00001610
Iteration 63/1000 | Loss: 0.00001610
Iteration 64/1000 | Loss: 0.00001610
Iteration 65/1000 | Loss: 0.00001610
Iteration 66/1000 | Loss: 0.00001609
Iteration 67/1000 | Loss: 0.00001609
Iteration 68/1000 | Loss: 0.00001609
Iteration 69/1000 | Loss: 0.00001609
Iteration 70/1000 | Loss: 0.00001609
Iteration 71/1000 | Loss: 0.00001609
Iteration 72/1000 | Loss: 0.00001609
Iteration 73/1000 | Loss: 0.00001609
Iteration 74/1000 | Loss: 0.00001609
Iteration 75/1000 | Loss: 0.00001609
Iteration 76/1000 | Loss: 0.00001609
Iteration 77/1000 | Loss: 0.00001608
Iteration 78/1000 | Loss: 0.00001608
Iteration 79/1000 | Loss: 0.00001608
Iteration 80/1000 | Loss: 0.00001608
Iteration 81/1000 | Loss: 0.00001608
Iteration 82/1000 | Loss: 0.00001608
Iteration 83/1000 | Loss: 0.00001608
Iteration 84/1000 | Loss: 0.00001608
Iteration 85/1000 | Loss: 0.00001607
Iteration 86/1000 | Loss: 0.00001607
Iteration 87/1000 | Loss: 0.00001607
Iteration 88/1000 | Loss: 0.00001607
Iteration 89/1000 | Loss: 0.00001607
Iteration 90/1000 | Loss: 0.00001606
Iteration 91/1000 | Loss: 0.00001606
Iteration 92/1000 | Loss: 0.00001606
Iteration 93/1000 | Loss: 0.00001606
Iteration 94/1000 | Loss: 0.00001606
Iteration 95/1000 | Loss: 0.00001606
Iteration 96/1000 | Loss: 0.00001606
Iteration 97/1000 | Loss: 0.00001606
Iteration 98/1000 | Loss: 0.00001605
Iteration 99/1000 | Loss: 0.00001605
Iteration 100/1000 | Loss: 0.00001605
Iteration 101/1000 | Loss: 0.00001605
Iteration 102/1000 | Loss: 0.00001605
Iteration 103/1000 | Loss: 0.00001604
Iteration 104/1000 | Loss: 0.00001604
Iteration 105/1000 | Loss: 0.00001604
Iteration 106/1000 | Loss: 0.00001604
Iteration 107/1000 | Loss: 0.00001604
Iteration 108/1000 | Loss: 0.00001604
Iteration 109/1000 | Loss: 0.00001604
Iteration 110/1000 | Loss: 0.00001604
Iteration 111/1000 | Loss: 0.00001604
Iteration 112/1000 | Loss: 0.00001604
Iteration 113/1000 | Loss: 0.00001604
Iteration 114/1000 | Loss: 0.00001603
Iteration 115/1000 | Loss: 0.00001603
Iteration 116/1000 | Loss: 0.00001603
Iteration 117/1000 | Loss: 0.00001603
Iteration 118/1000 | Loss: 0.00001603
Iteration 119/1000 | Loss: 0.00001603
Iteration 120/1000 | Loss: 0.00001603
Iteration 121/1000 | Loss: 0.00001603
Iteration 122/1000 | Loss: 0.00001602
Iteration 123/1000 | Loss: 0.00001602
Iteration 124/1000 | Loss: 0.00001602
Iteration 125/1000 | Loss: 0.00001602
Iteration 126/1000 | Loss: 0.00001602
Iteration 127/1000 | Loss: 0.00001602
Iteration 128/1000 | Loss: 0.00001602
Iteration 129/1000 | Loss: 0.00001602
Iteration 130/1000 | Loss: 0.00001602
Iteration 131/1000 | Loss: 0.00001602
Iteration 132/1000 | Loss: 0.00001602
Iteration 133/1000 | Loss: 0.00001602
Iteration 134/1000 | Loss: 0.00001602
Iteration 135/1000 | Loss: 0.00001602
Iteration 136/1000 | Loss: 0.00001602
Iteration 137/1000 | Loss: 0.00001602
Iteration 138/1000 | Loss: 0.00001602
Iteration 139/1000 | Loss: 0.00001602
Iteration 140/1000 | Loss: 0.00001602
Iteration 141/1000 | Loss: 0.00001601
Iteration 142/1000 | Loss: 0.00001601
Iteration 143/1000 | Loss: 0.00001601
Iteration 144/1000 | Loss: 0.00001601
Iteration 145/1000 | Loss: 0.00001601
Iteration 146/1000 | Loss: 0.00001601
Iteration 147/1000 | Loss: 0.00001601
Iteration 148/1000 | Loss: 0.00001601
Iteration 149/1000 | Loss: 0.00001601
Iteration 150/1000 | Loss: 0.00001601
Iteration 151/1000 | Loss: 0.00001600
Iteration 152/1000 | Loss: 0.00001600
Iteration 153/1000 | Loss: 0.00001600
Iteration 154/1000 | Loss: 0.00001600
Iteration 155/1000 | Loss: 0.00001600
Iteration 156/1000 | Loss: 0.00001600
Iteration 157/1000 | Loss: 0.00001600
Iteration 158/1000 | Loss: 0.00001600
Iteration 159/1000 | Loss: 0.00001600
Iteration 160/1000 | Loss: 0.00001600
Iteration 161/1000 | Loss: 0.00001600
Iteration 162/1000 | Loss: 0.00001600
Iteration 163/1000 | Loss: 0.00001599
Iteration 164/1000 | Loss: 0.00001599
Iteration 165/1000 | Loss: 0.00001599
Iteration 166/1000 | Loss: 0.00001599
Iteration 167/1000 | Loss: 0.00001599
Iteration 168/1000 | Loss: 0.00001599
Iteration 169/1000 | Loss: 0.00001599
Iteration 170/1000 | Loss: 0.00001599
Iteration 171/1000 | Loss: 0.00001599
Iteration 172/1000 | Loss: 0.00001599
Iteration 173/1000 | Loss: 0.00001599
Iteration 174/1000 | Loss: 0.00001599
Iteration 175/1000 | Loss: 0.00001598
Iteration 176/1000 | Loss: 0.00001598
Iteration 177/1000 | Loss: 0.00001598
Iteration 178/1000 | Loss: 0.00001598
Iteration 179/1000 | Loss: 0.00001598
Iteration 180/1000 | Loss: 0.00001598
Iteration 181/1000 | Loss: 0.00001598
Iteration 182/1000 | Loss: 0.00001598
Iteration 183/1000 | Loss: 0.00001598
Iteration 184/1000 | Loss: 0.00001598
Iteration 185/1000 | Loss: 0.00001598
Iteration 186/1000 | Loss: 0.00001598
Iteration 187/1000 | Loss: 0.00001598
Iteration 188/1000 | Loss: 0.00001598
Iteration 189/1000 | Loss: 0.00001598
Iteration 190/1000 | Loss: 0.00001598
Iteration 191/1000 | Loss: 0.00001598
Iteration 192/1000 | Loss: 0.00001598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.597780646989122e-05, 1.597780646989122e-05, 1.597780646989122e-05, 1.597780646989122e-05, 1.597780646989122e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.597780646989122e-05

Optimization complete. Final v2v error: 3.2224929332733154 mm

Highest mean error: 5.2596659660339355 mm for frame 68

Lowest mean error: 2.372131109237671 mm for frame 137

Saving results

Total time: 42.641653060913086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886609
Iteration 2/25 | Loss: 0.00234485
Iteration 3/25 | Loss: 0.00188258
Iteration 4/25 | Loss: 0.00169739
Iteration 5/25 | Loss: 0.00176408
Iteration 6/25 | Loss: 0.00158253
Iteration 7/25 | Loss: 0.00155084
Iteration 8/25 | Loss: 0.00153697
Iteration 9/25 | Loss: 0.00151579
Iteration 10/25 | Loss: 0.00149355
Iteration 11/25 | Loss: 0.00148657
Iteration 12/25 | Loss: 0.00148020
Iteration 13/25 | Loss: 0.00147902
Iteration 14/25 | Loss: 0.00147853
Iteration 15/25 | Loss: 0.00147800
Iteration 16/25 | Loss: 0.00147726
Iteration 17/25 | Loss: 0.00148076
Iteration 18/25 | Loss: 0.00147977
Iteration 19/25 | Loss: 0.00147589
Iteration 20/25 | Loss: 0.00147483
Iteration 21/25 | Loss: 0.00147427
Iteration 22/25 | Loss: 0.00147420
Iteration 23/25 | Loss: 0.00147420
Iteration 24/25 | Loss: 0.00147420
Iteration 25/25 | Loss: 0.00147420

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.08385992
Iteration 2/25 | Loss: 0.00471409
Iteration 3/25 | Loss: 0.00471408
Iteration 4/25 | Loss: 0.00471408
Iteration 5/25 | Loss: 0.00471408
Iteration 6/25 | Loss: 0.00471408
Iteration 7/25 | Loss: 0.00471408
Iteration 8/25 | Loss: 0.00471408
Iteration 9/25 | Loss: 0.00471408
Iteration 10/25 | Loss: 0.00471408
Iteration 11/25 | Loss: 0.00471408
Iteration 12/25 | Loss: 0.00471408
Iteration 13/25 | Loss: 0.00471408
Iteration 14/25 | Loss: 0.00471408
Iteration 15/25 | Loss: 0.00471408
Iteration 16/25 | Loss: 0.00471408
Iteration 17/25 | Loss: 0.00471408
Iteration 18/25 | Loss: 0.00471408
Iteration 19/25 | Loss: 0.00471408
Iteration 20/25 | Loss: 0.00471408
Iteration 21/25 | Loss: 0.00471408
Iteration 22/25 | Loss: 0.00471408
Iteration 23/25 | Loss: 0.00471408
Iteration 24/25 | Loss: 0.00471408
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.004714080598205328, 0.004714080598205328, 0.004714080598205328, 0.004714080598205328, 0.004714080598205328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004714080598205328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00471408
Iteration 2/1000 | Loss: 0.00042835
Iteration 3/1000 | Loss: 0.00105442
Iteration 4/1000 | Loss: 0.00084251
Iteration 5/1000 | Loss: 0.00060086
Iteration 6/1000 | Loss: 0.00023351
Iteration 7/1000 | Loss: 0.00014311
Iteration 8/1000 | Loss: 0.00011398
Iteration 9/1000 | Loss: 0.00006950
Iteration 10/1000 | Loss: 0.00055281
Iteration 11/1000 | Loss: 0.00006218
Iteration 12/1000 | Loss: 0.00005738
Iteration 13/1000 | Loss: 0.00005380
Iteration 14/1000 | Loss: 0.00005255
Iteration 15/1000 | Loss: 0.00003993
Iteration 16/1000 | Loss: 0.00004140
Iteration 17/1000 | Loss: 0.00003626
Iteration 18/1000 | Loss: 0.00008620
Iteration 19/1000 | Loss: 0.00003578
Iteration 20/1000 | Loss: 0.00004328
Iteration 21/1000 | Loss: 0.00004600
Iteration 22/1000 | Loss: 0.00003614
Iteration 23/1000 | Loss: 0.00004777
Iteration 24/1000 | Loss: 0.00004457
Iteration 25/1000 | Loss: 0.00004131
Iteration 26/1000 | Loss: 0.00003348
Iteration 27/1000 | Loss: 0.00002935
Iteration 28/1000 | Loss: 0.00004143
Iteration 29/1000 | Loss: 0.00004095
Iteration 30/1000 | Loss: 0.00004091
Iteration 31/1000 | Loss: 0.00003984
Iteration 32/1000 | Loss: 0.00003549
Iteration 33/1000 | Loss: 0.00004125
Iteration 34/1000 | Loss: 0.00003428
Iteration 35/1000 | Loss: 0.00004257
Iteration 36/1000 | Loss: 0.00002995
Iteration 37/1000 | Loss: 0.00002895
Iteration 38/1000 | Loss: 0.00002707
Iteration 39/1000 | Loss: 0.00002656
Iteration 40/1000 | Loss: 0.00002617
Iteration 41/1000 | Loss: 0.00002566
Iteration 42/1000 | Loss: 0.00002527
Iteration 43/1000 | Loss: 0.00002499
Iteration 44/1000 | Loss: 0.00002476
Iteration 45/1000 | Loss: 0.00002448
Iteration 46/1000 | Loss: 0.00002430
Iteration 47/1000 | Loss: 0.00002415
Iteration 48/1000 | Loss: 0.00002407
Iteration 49/1000 | Loss: 0.00002403
Iteration 50/1000 | Loss: 0.00002402
Iteration 51/1000 | Loss: 0.00002402
Iteration 52/1000 | Loss: 0.00002401
Iteration 53/1000 | Loss: 0.00002401
Iteration 54/1000 | Loss: 0.00002401
Iteration 55/1000 | Loss: 0.00002400
Iteration 56/1000 | Loss: 0.00002400
Iteration 57/1000 | Loss: 0.00002399
Iteration 58/1000 | Loss: 0.00002395
Iteration 59/1000 | Loss: 0.00002395
Iteration 60/1000 | Loss: 0.00002393
Iteration 61/1000 | Loss: 0.00002392
Iteration 62/1000 | Loss: 0.00031666
Iteration 63/1000 | Loss: 0.00008742
Iteration 64/1000 | Loss: 0.00002427
Iteration 65/1000 | Loss: 0.00002395
Iteration 66/1000 | Loss: 0.00032611
Iteration 67/1000 | Loss: 0.00005950
Iteration 68/1000 | Loss: 0.00002860
Iteration 69/1000 | Loss: 0.00002594
Iteration 70/1000 | Loss: 0.00002470
Iteration 71/1000 | Loss: 0.00002425
Iteration 72/1000 | Loss: 0.00002418
Iteration 73/1000 | Loss: 0.00002414
Iteration 74/1000 | Loss: 0.00002396
Iteration 75/1000 | Loss: 0.00002395
Iteration 76/1000 | Loss: 0.00002388
Iteration 77/1000 | Loss: 0.00002388
Iteration 78/1000 | Loss: 0.00002387
Iteration 79/1000 | Loss: 0.00002384
Iteration 80/1000 | Loss: 0.00037967
Iteration 81/1000 | Loss: 0.00002565
Iteration 82/1000 | Loss: 0.00002428
Iteration 83/1000 | Loss: 0.00002331
Iteration 84/1000 | Loss: 0.00002269
Iteration 85/1000 | Loss: 0.00002230
Iteration 86/1000 | Loss: 0.00002220
Iteration 87/1000 | Loss: 0.00002219
Iteration 88/1000 | Loss: 0.00002217
Iteration 89/1000 | Loss: 0.00002214
Iteration 90/1000 | Loss: 0.00002212
Iteration 91/1000 | Loss: 0.00002211
Iteration 92/1000 | Loss: 0.00002211
Iteration 93/1000 | Loss: 0.00002210
Iteration 94/1000 | Loss: 0.00002210
Iteration 95/1000 | Loss: 0.00002209
Iteration 96/1000 | Loss: 0.00002207
Iteration 97/1000 | Loss: 0.00002191
Iteration 98/1000 | Loss: 0.00002188
Iteration 99/1000 | Loss: 0.00002180
Iteration 100/1000 | Loss: 0.00002178
Iteration 101/1000 | Loss: 0.00002177
Iteration 102/1000 | Loss: 0.00002177
Iteration 103/1000 | Loss: 0.00002176
Iteration 104/1000 | Loss: 0.00002175
Iteration 105/1000 | Loss: 0.00002174
Iteration 106/1000 | Loss: 0.00002174
Iteration 107/1000 | Loss: 0.00002173
Iteration 108/1000 | Loss: 0.00002173
Iteration 109/1000 | Loss: 0.00002172
Iteration 110/1000 | Loss: 0.00002172
Iteration 111/1000 | Loss: 0.00002171
Iteration 112/1000 | Loss: 0.00002171
Iteration 113/1000 | Loss: 0.00002171
Iteration 114/1000 | Loss: 0.00002171
Iteration 115/1000 | Loss: 0.00002171
Iteration 116/1000 | Loss: 0.00002171
Iteration 117/1000 | Loss: 0.00002171
Iteration 118/1000 | Loss: 0.00002171
Iteration 119/1000 | Loss: 0.00002171
Iteration 120/1000 | Loss: 0.00002171
Iteration 121/1000 | Loss: 0.00002170
Iteration 122/1000 | Loss: 0.00002170
Iteration 123/1000 | Loss: 0.00002170
Iteration 124/1000 | Loss: 0.00002170
Iteration 125/1000 | Loss: 0.00002170
Iteration 126/1000 | Loss: 0.00002170
Iteration 127/1000 | Loss: 0.00002170
Iteration 128/1000 | Loss: 0.00002168
Iteration 129/1000 | Loss: 0.00002165
Iteration 130/1000 | Loss: 0.00002158
Iteration 131/1000 | Loss: 0.00002157
Iteration 132/1000 | Loss: 0.00002156
Iteration 133/1000 | Loss: 0.00002156
Iteration 134/1000 | Loss: 0.00002155
Iteration 135/1000 | Loss: 0.00002155
Iteration 136/1000 | Loss: 0.00002155
Iteration 137/1000 | Loss: 0.00002154
Iteration 138/1000 | Loss: 0.00002154
Iteration 139/1000 | Loss: 0.00002154
Iteration 140/1000 | Loss: 0.00002153
Iteration 141/1000 | Loss: 0.00002153
Iteration 142/1000 | Loss: 0.00002153
Iteration 143/1000 | Loss: 0.00002153
Iteration 144/1000 | Loss: 0.00002153
Iteration 145/1000 | Loss: 0.00002153
Iteration 146/1000 | Loss: 0.00002153
Iteration 147/1000 | Loss: 0.00002153
Iteration 148/1000 | Loss: 0.00002153
Iteration 149/1000 | Loss: 0.00002153
Iteration 150/1000 | Loss: 0.00002153
Iteration 151/1000 | Loss: 0.00002153
Iteration 152/1000 | Loss: 0.00002153
Iteration 153/1000 | Loss: 0.00002152
Iteration 154/1000 | Loss: 0.00002152
Iteration 155/1000 | Loss: 0.00002152
Iteration 156/1000 | Loss: 0.00002152
Iteration 157/1000 | Loss: 0.00002152
Iteration 158/1000 | Loss: 0.00002152
Iteration 159/1000 | Loss: 0.00002152
Iteration 160/1000 | Loss: 0.00002152
Iteration 161/1000 | Loss: 0.00002152
Iteration 162/1000 | Loss: 0.00002152
Iteration 163/1000 | Loss: 0.00002152
Iteration 164/1000 | Loss: 0.00002152
Iteration 165/1000 | Loss: 0.00002151
Iteration 166/1000 | Loss: 0.00002151
Iteration 167/1000 | Loss: 0.00002151
Iteration 168/1000 | Loss: 0.00002151
Iteration 169/1000 | Loss: 0.00002151
Iteration 170/1000 | Loss: 0.00002151
Iteration 171/1000 | Loss: 0.00002151
Iteration 172/1000 | Loss: 0.00002151
Iteration 173/1000 | Loss: 0.00002150
Iteration 174/1000 | Loss: 0.00002150
Iteration 175/1000 | Loss: 0.00002150
Iteration 176/1000 | Loss: 0.00002150
Iteration 177/1000 | Loss: 0.00002150
Iteration 178/1000 | Loss: 0.00002150
Iteration 179/1000 | Loss: 0.00002150
Iteration 180/1000 | Loss: 0.00002150
Iteration 181/1000 | Loss: 0.00002150
Iteration 182/1000 | Loss: 0.00002150
Iteration 183/1000 | Loss: 0.00002150
Iteration 184/1000 | Loss: 0.00002150
Iteration 185/1000 | Loss: 0.00002150
Iteration 186/1000 | Loss: 0.00002150
Iteration 187/1000 | Loss: 0.00002150
Iteration 188/1000 | Loss: 0.00002150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.150424916180782e-05, 2.150424916180782e-05, 2.150424916180782e-05, 2.150424916180782e-05, 2.150424916180782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.150424916180782e-05

Optimization complete. Final v2v error: 3.4549107551574707 mm

Highest mean error: 12.593842506408691 mm for frame 56

Lowest mean error: 2.690502643585205 mm for frame 152

Saving results

Total time: 155.27664732933044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046733
Iteration 2/25 | Loss: 0.01046733
Iteration 3/25 | Loss: 0.01046733
Iteration 4/25 | Loss: 0.01046733
Iteration 5/25 | Loss: 0.01046733
Iteration 6/25 | Loss: 0.01046733
Iteration 7/25 | Loss: 0.01046732
Iteration 8/25 | Loss: 0.01046732
Iteration 9/25 | Loss: 0.01046732
Iteration 10/25 | Loss: 0.01046732
Iteration 11/25 | Loss: 0.01046732
Iteration 12/25 | Loss: 0.00233493
Iteration 13/25 | Loss: 0.00167413
Iteration 14/25 | Loss: 0.00152504
Iteration 15/25 | Loss: 0.00167724
Iteration 16/25 | Loss: 0.00141794
Iteration 17/25 | Loss: 0.00129650
Iteration 18/25 | Loss: 0.00128054
Iteration 19/25 | Loss: 0.00127119
Iteration 20/25 | Loss: 0.00126860
Iteration 21/25 | Loss: 0.00126342
Iteration 22/25 | Loss: 0.00128330
Iteration 23/25 | Loss: 0.00127881
Iteration 24/25 | Loss: 0.00125723
Iteration 25/25 | Loss: 0.00125696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37133873
Iteration 2/25 | Loss: 0.00299838
Iteration 3/25 | Loss: 0.00296620
Iteration 4/25 | Loss: 0.00296619
Iteration 5/25 | Loss: 0.00296619
Iteration 6/25 | Loss: 0.00296619
Iteration 7/25 | Loss: 0.00296619
Iteration 8/25 | Loss: 0.00296619
Iteration 9/25 | Loss: 0.00296619
Iteration 10/25 | Loss: 0.00296619
Iteration 11/25 | Loss: 0.00296619
Iteration 12/25 | Loss: 0.00296619
Iteration 13/25 | Loss: 0.00296619
Iteration 14/25 | Loss: 0.00296619
Iteration 15/25 | Loss: 0.00296619
Iteration 16/25 | Loss: 0.00296619
Iteration 17/25 | Loss: 0.00296619
Iteration 18/25 | Loss: 0.00296619
Iteration 19/25 | Loss: 0.00296619
Iteration 20/25 | Loss: 0.00296619
Iteration 21/25 | Loss: 0.00296619
Iteration 22/25 | Loss: 0.00296619
Iteration 23/25 | Loss: 0.00296619
Iteration 24/25 | Loss: 0.00296619
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002966191153973341, 0.002966191153973341, 0.002966191153973341, 0.002966191153973341, 0.002966191153973341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002966191153973341

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00296619
Iteration 2/1000 | Loss: 0.00095887
Iteration 3/1000 | Loss: 0.00109569
Iteration 4/1000 | Loss: 0.00094630
Iteration 5/1000 | Loss: 0.00016894
Iteration 6/1000 | Loss: 0.00014071
Iteration 7/1000 | Loss: 0.00005218
Iteration 8/1000 | Loss: 0.00006441
Iteration 9/1000 | Loss: 0.00001981
Iteration 10/1000 | Loss: 0.00012073
Iteration 11/1000 | Loss: 0.00001934
Iteration 12/1000 | Loss: 0.00002346
Iteration 13/1000 | Loss: 0.00009179
Iteration 14/1000 | Loss: 0.00028549
Iteration 15/1000 | Loss: 0.00017031
Iteration 16/1000 | Loss: 0.00009903
Iteration 17/1000 | Loss: 0.00002402
Iteration 18/1000 | Loss: 0.00003832
Iteration 19/1000 | Loss: 0.00002387
Iteration 20/1000 | Loss: 0.00001775
Iteration 21/1000 | Loss: 0.00003064
Iteration 22/1000 | Loss: 0.00001783
Iteration 23/1000 | Loss: 0.00002762
Iteration 24/1000 | Loss: 0.00002514
Iteration 25/1000 | Loss: 0.00001324
Iteration 26/1000 | Loss: 0.00002135
Iteration 27/1000 | Loss: 0.00001889
Iteration 28/1000 | Loss: 0.00001186
Iteration 29/1000 | Loss: 0.00001185
Iteration 30/1000 | Loss: 0.00001701
Iteration 31/1000 | Loss: 0.00007239
Iteration 32/1000 | Loss: 0.00004348
Iteration 33/1000 | Loss: 0.00003395
Iteration 34/1000 | Loss: 0.00001458
Iteration 35/1000 | Loss: 0.00001166
Iteration 36/1000 | Loss: 0.00002966
Iteration 37/1000 | Loss: 0.00001230
Iteration 38/1000 | Loss: 0.00001160
Iteration 39/1000 | Loss: 0.00001154
Iteration 40/1000 | Loss: 0.00001445
Iteration 41/1000 | Loss: 0.00001149
Iteration 42/1000 | Loss: 0.00001148
Iteration 43/1000 | Loss: 0.00001148
Iteration 44/1000 | Loss: 0.00001147
Iteration 45/1000 | Loss: 0.00001147
Iteration 46/1000 | Loss: 0.00001147
Iteration 47/1000 | Loss: 0.00001147
Iteration 48/1000 | Loss: 0.00001147
Iteration 49/1000 | Loss: 0.00001146
Iteration 50/1000 | Loss: 0.00001146
Iteration 51/1000 | Loss: 0.00001146
Iteration 52/1000 | Loss: 0.00001146
Iteration 53/1000 | Loss: 0.00001146
Iteration 54/1000 | Loss: 0.00001146
Iteration 55/1000 | Loss: 0.00001146
Iteration 56/1000 | Loss: 0.00001146
Iteration 57/1000 | Loss: 0.00001146
Iteration 58/1000 | Loss: 0.00001146
Iteration 59/1000 | Loss: 0.00001146
Iteration 60/1000 | Loss: 0.00001146
Iteration 61/1000 | Loss: 0.00001145
Iteration 62/1000 | Loss: 0.00001145
Iteration 63/1000 | Loss: 0.00002486
Iteration 64/1000 | Loss: 0.00001409
Iteration 65/1000 | Loss: 0.00001272
Iteration 66/1000 | Loss: 0.00001142
Iteration 67/1000 | Loss: 0.00001142
Iteration 68/1000 | Loss: 0.00001142
Iteration 69/1000 | Loss: 0.00001142
Iteration 70/1000 | Loss: 0.00001142
Iteration 71/1000 | Loss: 0.00001142
Iteration 72/1000 | Loss: 0.00001142
Iteration 73/1000 | Loss: 0.00001142
Iteration 74/1000 | Loss: 0.00001142
Iteration 75/1000 | Loss: 0.00001141
Iteration 76/1000 | Loss: 0.00001140
Iteration 77/1000 | Loss: 0.00001604
Iteration 78/1000 | Loss: 0.00001141
Iteration 79/1000 | Loss: 0.00001141
Iteration 80/1000 | Loss: 0.00001141
Iteration 81/1000 | Loss: 0.00001141
Iteration 82/1000 | Loss: 0.00001141
Iteration 83/1000 | Loss: 0.00001141
Iteration 84/1000 | Loss: 0.00001141
Iteration 85/1000 | Loss: 0.00001823
Iteration 86/1000 | Loss: 0.00001823
Iteration 87/1000 | Loss: 0.00001143
Iteration 88/1000 | Loss: 0.00001217
Iteration 89/1000 | Loss: 0.00001139
Iteration 90/1000 | Loss: 0.00001139
Iteration 91/1000 | Loss: 0.00001140
Iteration 92/1000 | Loss: 0.00001139
Iteration 93/1000 | Loss: 0.00001139
Iteration 94/1000 | Loss: 0.00001139
Iteration 95/1000 | Loss: 0.00001138
Iteration 96/1000 | Loss: 0.00001138
Iteration 97/1000 | Loss: 0.00001138
Iteration 98/1000 | Loss: 0.00001138
Iteration 99/1000 | Loss: 0.00001138
Iteration 100/1000 | Loss: 0.00001138
Iteration 101/1000 | Loss: 0.00001138
Iteration 102/1000 | Loss: 0.00001138
Iteration 103/1000 | Loss: 0.00001138
Iteration 104/1000 | Loss: 0.00001138
Iteration 105/1000 | Loss: 0.00001138
Iteration 106/1000 | Loss: 0.00001138
Iteration 107/1000 | Loss: 0.00001137
Iteration 108/1000 | Loss: 0.00001137
Iteration 109/1000 | Loss: 0.00001137
Iteration 110/1000 | Loss: 0.00001137
Iteration 111/1000 | Loss: 0.00001137
Iteration 112/1000 | Loss: 0.00001137
Iteration 113/1000 | Loss: 0.00001137
Iteration 114/1000 | Loss: 0.00001137
Iteration 115/1000 | Loss: 0.00001137
Iteration 116/1000 | Loss: 0.00001137
Iteration 117/1000 | Loss: 0.00001137
Iteration 118/1000 | Loss: 0.00001137
Iteration 119/1000 | Loss: 0.00001137
Iteration 120/1000 | Loss: 0.00001220
Iteration 121/1000 | Loss: 0.00001215
Iteration 122/1000 | Loss: 0.00001185
Iteration 123/1000 | Loss: 0.00001162
Iteration 124/1000 | Loss: 0.00001137
Iteration 125/1000 | Loss: 0.00001137
Iteration 126/1000 | Loss: 0.00001137
Iteration 127/1000 | Loss: 0.00001137
Iteration 128/1000 | Loss: 0.00001137
Iteration 129/1000 | Loss: 0.00001137
Iteration 130/1000 | Loss: 0.00001137
Iteration 131/1000 | Loss: 0.00001137
Iteration 132/1000 | Loss: 0.00001137
Iteration 133/1000 | Loss: 0.00001137
Iteration 134/1000 | Loss: 0.00001137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.1365428690623958e-05, 1.1365428690623958e-05, 1.1365428690623958e-05, 1.1365428690623958e-05, 1.1365428690623958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1365428690623958e-05

Optimization complete. Final v2v error: 2.6924922466278076 mm

Highest mean error: 5.8663787841796875 mm for frame 34

Lowest mean error: 2.456983804702759 mm for frame 0

Saving results

Total time: 111.72833299636841
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075894
Iteration 2/25 | Loss: 0.01075894
Iteration 3/25 | Loss: 0.01075894
Iteration 4/25 | Loss: 0.01075893
Iteration 5/25 | Loss: 0.01075893
Iteration 6/25 | Loss: 0.01075893
Iteration 7/25 | Loss: 0.01075893
Iteration 8/25 | Loss: 0.01075893
Iteration 9/25 | Loss: 0.01075892
Iteration 10/25 | Loss: 0.01075892
Iteration 11/25 | Loss: 0.01075892
Iteration 12/25 | Loss: 0.01075892
Iteration 13/25 | Loss: 0.01075892
Iteration 14/25 | Loss: 0.01075892
Iteration 15/25 | Loss: 0.01075891
Iteration 16/25 | Loss: 0.01075891
Iteration 17/25 | Loss: 0.01075891
Iteration 18/25 | Loss: 0.01075891
Iteration 19/25 | Loss: 0.01075890
Iteration 20/25 | Loss: 0.01075890
Iteration 21/25 | Loss: 0.01075890
Iteration 22/25 | Loss: 0.01075890
Iteration 23/25 | Loss: 0.01075890
Iteration 24/25 | Loss: 0.01075890
Iteration 25/25 | Loss: 0.01075889

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52051806
Iteration 2/25 | Loss: 0.08368811
Iteration 3/25 | Loss: 0.08361388
Iteration 4/25 | Loss: 0.08350531
Iteration 5/25 | Loss: 0.08350529
Iteration 6/25 | Loss: 0.08350529
Iteration 7/25 | Loss: 0.08350529
Iteration 8/25 | Loss: 0.08350529
Iteration 9/25 | Loss: 0.08350529
Iteration 10/25 | Loss: 0.08350529
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0835052877664566, 0.0835052877664566, 0.0835052877664566, 0.0835052877664566, 0.0835052877664566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0835052877664566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08350529
Iteration 2/1000 | Loss: 0.00074851
Iteration 3/1000 | Loss: 0.00017611
Iteration 4/1000 | Loss: 0.00010029
Iteration 5/1000 | Loss: 0.00005280
Iteration 6/1000 | Loss: 0.00012213
Iteration 7/1000 | Loss: 0.00003347
Iteration 8/1000 | Loss: 0.00002931
Iteration 9/1000 | Loss: 0.00002501
Iteration 10/1000 | Loss: 0.00012189
Iteration 11/1000 | Loss: 0.00002315
Iteration 12/1000 | Loss: 0.00021052
Iteration 13/1000 | Loss: 0.00001950
Iteration 14/1000 | Loss: 0.00002180
Iteration 15/1000 | Loss: 0.00004241
Iteration 16/1000 | Loss: 0.00001804
Iteration 17/1000 | Loss: 0.00002925
Iteration 18/1000 | Loss: 0.00001650
Iteration 19/1000 | Loss: 0.00002221
Iteration 20/1000 | Loss: 0.00001458
Iteration 21/1000 | Loss: 0.00001382
Iteration 22/1000 | Loss: 0.00001306
Iteration 23/1000 | Loss: 0.00003085
Iteration 24/1000 | Loss: 0.00004330
Iteration 25/1000 | Loss: 0.00001953
Iteration 26/1000 | Loss: 0.00001208
Iteration 27/1000 | Loss: 0.00001347
Iteration 28/1000 | Loss: 0.00001149
Iteration 29/1000 | Loss: 0.00001394
Iteration 30/1000 | Loss: 0.00003501
Iteration 31/1000 | Loss: 0.00005150
Iteration 32/1000 | Loss: 0.00001439
Iteration 33/1000 | Loss: 0.00003416
Iteration 34/1000 | Loss: 0.00001956
Iteration 35/1000 | Loss: 0.00006370
Iteration 36/1000 | Loss: 0.00002782
Iteration 37/1000 | Loss: 0.00003600
Iteration 38/1000 | Loss: 0.00003781
Iteration 39/1000 | Loss: 0.00003548
Iteration 40/1000 | Loss: 0.00001479
Iteration 41/1000 | Loss: 0.00003297
Iteration 42/1000 | Loss: 0.00008734
Iteration 43/1000 | Loss: 0.00003479
Iteration 44/1000 | Loss: 0.00002382
Iteration 45/1000 | Loss: 0.00006454
Iteration 46/1000 | Loss: 0.00006211
Iteration 47/1000 | Loss: 0.00026886
Iteration 48/1000 | Loss: 0.00035639
Iteration 49/1000 | Loss: 0.00028089
Iteration 50/1000 | Loss: 0.00003542
Iteration 51/1000 | Loss: 0.00005134
Iteration 52/1000 | Loss: 0.00001262
Iteration 53/1000 | Loss: 0.00004944
Iteration 54/1000 | Loss: 0.00003891
Iteration 55/1000 | Loss: 0.00001489
Iteration 56/1000 | Loss: 0.00013830
Iteration 57/1000 | Loss: 0.00002615
Iteration 58/1000 | Loss: 0.00003969
Iteration 59/1000 | Loss: 0.00014820
Iteration 60/1000 | Loss: 0.00003726
Iteration 61/1000 | Loss: 0.00002277
Iteration 62/1000 | Loss: 0.00003361
Iteration 63/1000 | Loss: 0.00001388
Iteration 64/1000 | Loss: 0.00003381
Iteration 65/1000 | Loss: 0.00015470
Iteration 66/1000 | Loss: 0.00003510
Iteration 67/1000 | Loss: 0.00001715
Iteration 68/1000 | Loss: 0.00010656
Iteration 69/1000 | Loss: 0.00005731
Iteration 70/1000 | Loss: 0.00010160
Iteration 71/1000 | Loss: 0.00002549
Iteration 72/1000 | Loss: 0.00003400
Iteration 73/1000 | Loss: 0.00001654
Iteration 74/1000 | Loss: 0.00003327
Iteration 75/1000 | Loss: 0.00002012
Iteration 76/1000 | Loss: 0.00003321
Iteration 77/1000 | Loss: 0.00002113
Iteration 78/1000 | Loss: 0.00003388
Iteration 79/1000 | Loss: 0.00003127
Iteration 80/1000 | Loss: 0.00003314
Iteration 81/1000 | Loss: 0.00002434
Iteration 82/1000 | Loss: 0.00003435
Iteration 83/1000 | Loss: 0.00072997
Iteration 84/1000 | Loss: 0.00007671
Iteration 85/1000 | Loss: 0.00002845
Iteration 86/1000 | Loss: 0.00006142
Iteration 87/1000 | Loss: 0.00002357
Iteration 88/1000 | Loss: 0.00003109
Iteration 89/1000 | Loss: 0.00002672
Iteration 90/1000 | Loss: 0.00009659
Iteration 91/1000 | Loss: 0.00002642
Iteration 92/1000 | Loss: 0.00001087
Iteration 93/1000 | Loss: 0.00001071
Iteration 94/1000 | Loss: 0.00001068
Iteration 95/1000 | Loss: 0.00001068
Iteration 96/1000 | Loss: 0.00001067
Iteration 97/1000 | Loss: 0.00001066
Iteration 98/1000 | Loss: 0.00001065
Iteration 99/1000 | Loss: 0.00001057
Iteration 100/1000 | Loss: 0.00001055
Iteration 101/1000 | Loss: 0.00001054
Iteration 102/1000 | Loss: 0.00001053
Iteration 103/1000 | Loss: 0.00001127
Iteration 104/1000 | Loss: 0.00001047
Iteration 105/1000 | Loss: 0.00001042
Iteration 106/1000 | Loss: 0.00001042
Iteration 107/1000 | Loss: 0.00001041
Iteration 108/1000 | Loss: 0.00001041
Iteration 109/1000 | Loss: 0.00001041
Iteration 110/1000 | Loss: 0.00001039
Iteration 111/1000 | Loss: 0.00001038
Iteration 112/1000 | Loss: 0.00001050
Iteration 113/1000 | Loss: 0.00001050
Iteration 114/1000 | Loss: 0.00001026
Iteration 115/1000 | Loss: 0.00001025
Iteration 116/1000 | Loss: 0.00001025
Iteration 117/1000 | Loss: 0.00001025
Iteration 118/1000 | Loss: 0.00001025
Iteration 119/1000 | Loss: 0.00001025
Iteration 120/1000 | Loss: 0.00001025
Iteration 121/1000 | Loss: 0.00001025
Iteration 122/1000 | Loss: 0.00001025
Iteration 123/1000 | Loss: 0.00001024
Iteration 124/1000 | Loss: 0.00001024
Iteration 125/1000 | Loss: 0.00001024
Iteration 126/1000 | Loss: 0.00001023
Iteration 127/1000 | Loss: 0.00001023
Iteration 128/1000 | Loss: 0.00001023
Iteration 129/1000 | Loss: 0.00001023
Iteration 130/1000 | Loss: 0.00001016
Iteration 131/1000 | Loss: 0.00001015
Iteration 132/1000 | Loss: 0.00001014
Iteration 133/1000 | Loss: 0.00001013
Iteration 134/1000 | Loss: 0.00001009
Iteration 135/1000 | Loss: 0.00001006
Iteration 136/1000 | Loss: 0.00001006
Iteration 137/1000 | Loss: 0.00001005
Iteration 138/1000 | Loss: 0.00001005
Iteration 139/1000 | Loss: 0.00001005
Iteration 140/1000 | Loss: 0.00001005
Iteration 141/1000 | Loss: 0.00001004
Iteration 142/1000 | Loss: 0.00001004
Iteration 143/1000 | Loss: 0.00001004
Iteration 144/1000 | Loss: 0.00001003
Iteration 145/1000 | Loss: 0.00001003
Iteration 146/1000 | Loss: 0.00001003
Iteration 147/1000 | Loss: 0.00001003
Iteration 148/1000 | Loss: 0.00001003
Iteration 149/1000 | Loss: 0.00001003
Iteration 150/1000 | Loss: 0.00001003
Iteration 151/1000 | Loss: 0.00001002
Iteration 152/1000 | Loss: 0.00001002
Iteration 153/1000 | Loss: 0.00001002
Iteration 154/1000 | Loss: 0.00001002
Iteration 155/1000 | Loss: 0.00001002
Iteration 156/1000 | Loss: 0.00001002
Iteration 157/1000 | Loss: 0.00001002
Iteration 158/1000 | Loss: 0.00001002
Iteration 159/1000 | Loss: 0.00001001
Iteration 160/1000 | Loss: 0.00001001
Iteration 161/1000 | Loss: 0.00001001
Iteration 162/1000 | Loss: 0.00001001
Iteration 163/1000 | Loss: 0.00001001
Iteration 164/1000 | Loss: 0.00001001
Iteration 165/1000 | Loss: 0.00001001
Iteration 166/1000 | Loss: 0.00001001
Iteration 167/1000 | Loss: 0.00001001
Iteration 168/1000 | Loss: 0.00001001
Iteration 169/1000 | Loss: 0.00001001
Iteration 170/1000 | Loss: 0.00001001
Iteration 171/1000 | Loss: 0.00001001
Iteration 172/1000 | Loss: 0.00001001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.00115667009959e-05, 1.00115667009959e-05, 1.00115667009959e-05, 1.00115667009959e-05, 1.00115667009959e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.00115667009959e-05

Optimization complete. Final v2v error: 2.5913095474243164 mm

Highest mean error: 9.13803482055664 mm for frame 151

Lowest mean error: 2.221450090408325 mm for frame 0

Saving results

Total time: 152.58161664009094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419750
Iteration 2/25 | Loss: 0.00132348
Iteration 3/25 | Loss: 0.00124728
Iteration 4/25 | Loss: 0.00123534
Iteration 5/25 | Loss: 0.00123241
Iteration 6/25 | Loss: 0.00123196
Iteration 7/25 | Loss: 0.00123196
Iteration 8/25 | Loss: 0.00123196
Iteration 9/25 | Loss: 0.00123196
Iteration 10/25 | Loss: 0.00123196
Iteration 11/25 | Loss: 0.00123196
Iteration 12/25 | Loss: 0.00123196
Iteration 13/25 | Loss: 0.00123196
Iteration 14/25 | Loss: 0.00123196
Iteration 15/25 | Loss: 0.00123196
Iteration 16/25 | Loss: 0.00123196
Iteration 17/25 | Loss: 0.00123196
Iteration 18/25 | Loss: 0.00123196
Iteration 19/25 | Loss: 0.00123196
Iteration 20/25 | Loss: 0.00123196
Iteration 21/25 | Loss: 0.00123196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012319562956690788, 0.0012319562956690788, 0.0012319562956690788, 0.0012319562956690788, 0.0012319562956690788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012319562956690788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.96083879
Iteration 2/25 | Loss: 0.00306404
Iteration 3/25 | Loss: 0.00306403
Iteration 4/25 | Loss: 0.00306402
Iteration 5/25 | Loss: 0.00306402
Iteration 6/25 | Loss: 0.00306402
Iteration 7/25 | Loss: 0.00306402
Iteration 8/25 | Loss: 0.00306402
Iteration 9/25 | Loss: 0.00306402
Iteration 10/25 | Loss: 0.00306402
Iteration 11/25 | Loss: 0.00306402
Iteration 12/25 | Loss: 0.00306402
Iteration 13/25 | Loss: 0.00306402
Iteration 14/25 | Loss: 0.00306402
Iteration 15/25 | Loss: 0.00306402
Iteration 16/25 | Loss: 0.00306402
Iteration 17/25 | Loss: 0.00306402
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0030640221666544676, 0.0030640221666544676, 0.0030640221666544676, 0.0030640221666544676, 0.0030640221666544676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030640221666544676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00306402
Iteration 2/1000 | Loss: 0.00002920
Iteration 3/1000 | Loss: 0.00001925
Iteration 4/1000 | Loss: 0.00001530
Iteration 5/1000 | Loss: 0.00001350
Iteration 6/1000 | Loss: 0.00001271
Iteration 7/1000 | Loss: 0.00001215
Iteration 8/1000 | Loss: 0.00001168
Iteration 9/1000 | Loss: 0.00001138
Iteration 10/1000 | Loss: 0.00001118
Iteration 11/1000 | Loss: 0.00001112
Iteration 12/1000 | Loss: 0.00001109
Iteration 13/1000 | Loss: 0.00001108
Iteration 14/1000 | Loss: 0.00001106
Iteration 15/1000 | Loss: 0.00001106
Iteration 16/1000 | Loss: 0.00001105
Iteration 17/1000 | Loss: 0.00001105
Iteration 18/1000 | Loss: 0.00001104
Iteration 19/1000 | Loss: 0.00001103
Iteration 20/1000 | Loss: 0.00001103
Iteration 21/1000 | Loss: 0.00001103
Iteration 22/1000 | Loss: 0.00001102
Iteration 23/1000 | Loss: 0.00001102
Iteration 24/1000 | Loss: 0.00001101
Iteration 25/1000 | Loss: 0.00001100
Iteration 26/1000 | Loss: 0.00001099
Iteration 27/1000 | Loss: 0.00001097
Iteration 28/1000 | Loss: 0.00001097
Iteration 29/1000 | Loss: 0.00001097
Iteration 30/1000 | Loss: 0.00001097
Iteration 31/1000 | Loss: 0.00001097
Iteration 32/1000 | Loss: 0.00001097
Iteration 33/1000 | Loss: 0.00001097
Iteration 34/1000 | Loss: 0.00001097
Iteration 35/1000 | Loss: 0.00001097
Iteration 36/1000 | Loss: 0.00001096
Iteration 37/1000 | Loss: 0.00001093
Iteration 38/1000 | Loss: 0.00001093
Iteration 39/1000 | Loss: 0.00001093
Iteration 40/1000 | Loss: 0.00001093
Iteration 41/1000 | Loss: 0.00001091
Iteration 42/1000 | Loss: 0.00001091
Iteration 43/1000 | Loss: 0.00001090
Iteration 44/1000 | Loss: 0.00001088
Iteration 45/1000 | Loss: 0.00001088
Iteration 46/1000 | Loss: 0.00001088
Iteration 47/1000 | Loss: 0.00001088
Iteration 48/1000 | Loss: 0.00001088
Iteration 49/1000 | Loss: 0.00001088
Iteration 50/1000 | Loss: 0.00001088
Iteration 51/1000 | Loss: 0.00001088
Iteration 52/1000 | Loss: 0.00001088
Iteration 53/1000 | Loss: 0.00001087
Iteration 54/1000 | Loss: 0.00001087
Iteration 55/1000 | Loss: 0.00001087
Iteration 56/1000 | Loss: 0.00001087
Iteration 57/1000 | Loss: 0.00001087
Iteration 58/1000 | Loss: 0.00001086
Iteration 59/1000 | Loss: 0.00001086
Iteration 60/1000 | Loss: 0.00001086
Iteration 61/1000 | Loss: 0.00001085
Iteration 62/1000 | Loss: 0.00001084
Iteration 63/1000 | Loss: 0.00001084
Iteration 64/1000 | Loss: 0.00001084
Iteration 65/1000 | Loss: 0.00001084
Iteration 66/1000 | Loss: 0.00001083
Iteration 67/1000 | Loss: 0.00001083
Iteration 68/1000 | Loss: 0.00001083
Iteration 69/1000 | Loss: 0.00001083
Iteration 70/1000 | Loss: 0.00001083
Iteration 71/1000 | Loss: 0.00001083
Iteration 72/1000 | Loss: 0.00001083
Iteration 73/1000 | Loss: 0.00001083
Iteration 74/1000 | Loss: 0.00001082
Iteration 75/1000 | Loss: 0.00001082
Iteration 76/1000 | Loss: 0.00001081
Iteration 77/1000 | Loss: 0.00001081
Iteration 78/1000 | Loss: 0.00001081
Iteration 79/1000 | Loss: 0.00001081
Iteration 80/1000 | Loss: 0.00001081
Iteration 81/1000 | Loss: 0.00001081
Iteration 82/1000 | Loss: 0.00001081
Iteration 83/1000 | Loss: 0.00001081
Iteration 84/1000 | Loss: 0.00001081
Iteration 85/1000 | Loss: 0.00001080
Iteration 86/1000 | Loss: 0.00001080
Iteration 87/1000 | Loss: 0.00001080
Iteration 88/1000 | Loss: 0.00001080
Iteration 89/1000 | Loss: 0.00001080
Iteration 90/1000 | Loss: 0.00001079
Iteration 91/1000 | Loss: 0.00001079
Iteration 92/1000 | Loss: 0.00001078
Iteration 93/1000 | Loss: 0.00001078
Iteration 94/1000 | Loss: 0.00001078
Iteration 95/1000 | Loss: 0.00001078
Iteration 96/1000 | Loss: 0.00001078
Iteration 97/1000 | Loss: 0.00001078
Iteration 98/1000 | Loss: 0.00001078
Iteration 99/1000 | Loss: 0.00001077
Iteration 100/1000 | Loss: 0.00001077
Iteration 101/1000 | Loss: 0.00001077
Iteration 102/1000 | Loss: 0.00001077
Iteration 103/1000 | Loss: 0.00001077
Iteration 104/1000 | Loss: 0.00001077
Iteration 105/1000 | Loss: 0.00001077
Iteration 106/1000 | Loss: 0.00001077
Iteration 107/1000 | Loss: 0.00001077
Iteration 108/1000 | Loss: 0.00001077
Iteration 109/1000 | Loss: 0.00001077
Iteration 110/1000 | Loss: 0.00001077
Iteration 111/1000 | Loss: 0.00001077
Iteration 112/1000 | Loss: 0.00001077
Iteration 113/1000 | Loss: 0.00001077
Iteration 114/1000 | Loss: 0.00001077
Iteration 115/1000 | Loss: 0.00001077
Iteration 116/1000 | Loss: 0.00001077
Iteration 117/1000 | Loss: 0.00001077
Iteration 118/1000 | Loss: 0.00001077
Iteration 119/1000 | Loss: 0.00001077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.0770353583211545e-05, 1.0770353583211545e-05, 1.0770353583211545e-05, 1.0770353583211545e-05, 1.0770353583211545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0770353583211545e-05

Optimization complete. Final v2v error: 2.750164270401001 mm

Highest mean error: 3.084423542022705 mm for frame 25

Lowest mean error: 2.4932475090026855 mm for frame 213

Saving results

Total time: 37.02360248565674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_1202/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_1202/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752612
Iteration 2/25 | Loss: 0.00176790
Iteration 3/25 | Loss: 0.00133411
Iteration 4/25 | Loss: 0.00127885
Iteration 5/25 | Loss: 0.00126062
Iteration 6/25 | Loss: 0.00125204
Iteration 7/25 | Loss: 0.00124552
Iteration 8/25 | Loss: 0.00124031
Iteration 9/25 | Loss: 0.00123850
Iteration 10/25 | Loss: 0.00123799
Iteration 11/25 | Loss: 0.00123789
Iteration 12/25 | Loss: 0.00123786
Iteration 13/25 | Loss: 0.00123786
Iteration 14/25 | Loss: 0.00123786
Iteration 15/25 | Loss: 0.00123786
Iteration 16/25 | Loss: 0.00123786
Iteration 17/25 | Loss: 0.00123786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012378641404211521, 0.0012378641404211521, 0.0012378641404211521, 0.0012378641404211521, 0.0012378641404211521]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012378641404211521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91155303
Iteration 2/25 | Loss: 0.00170732
Iteration 3/25 | Loss: 0.00170728
Iteration 4/25 | Loss: 0.00170728
Iteration 5/25 | Loss: 0.00170728
Iteration 6/25 | Loss: 0.00170728
Iteration 7/25 | Loss: 0.00170728
Iteration 8/25 | Loss: 0.00170728
Iteration 9/25 | Loss: 0.00170728
Iteration 10/25 | Loss: 0.00170728
Iteration 11/25 | Loss: 0.00170728
Iteration 12/25 | Loss: 0.00170728
Iteration 13/25 | Loss: 0.00170728
Iteration 14/25 | Loss: 0.00170727
Iteration 15/25 | Loss: 0.00170728
Iteration 16/25 | Loss: 0.00170728
Iteration 17/25 | Loss: 0.00170728
Iteration 18/25 | Loss: 0.00170728
Iteration 19/25 | Loss: 0.00170727
Iteration 20/25 | Loss: 0.00170728
Iteration 21/25 | Loss: 0.00170727
Iteration 22/25 | Loss: 0.00170728
Iteration 23/25 | Loss: 0.00170727
Iteration 24/25 | Loss: 0.00170727
Iteration 25/25 | Loss: 0.00170727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170727
Iteration 2/1000 | Loss: 0.00004118
Iteration 3/1000 | Loss: 0.00002424
Iteration 4/1000 | Loss: 0.00002057
Iteration 5/1000 | Loss: 0.00001868
Iteration 6/1000 | Loss: 0.00001762
Iteration 7/1000 | Loss: 0.00001709
Iteration 8/1000 | Loss: 0.00001665
Iteration 9/1000 | Loss: 0.00001625
Iteration 10/1000 | Loss: 0.00001594
Iteration 11/1000 | Loss: 0.00001568
Iteration 12/1000 | Loss: 0.00001554
Iteration 13/1000 | Loss: 0.00001542
Iteration 14/1000 | Loss: 0.00001539
Iteration 15/1000 | Loss: 0.00001539
Iteration 16/1000 | Loss: 0.00001538
Iteration 17/1000 | Loss: 0.00001538
Iteration 18/1000 | Loss: 0.00001537
Iteration 19/1000 | Loss: 0.00001536
Iteration 20/1000 | Loss: 0.00001536
Iteration 21/1000 | Loss: 0.00001536
Iteration 22/1000 | Loss: 0.00001536
Iteration 23/1000 | Loss: 0.00001536
Iteration 24/1000 | Loss: 0.00001536
Iteration 25/1000 | Loss: 0.00001536
Iteration 26/1000 | Loss: 0.00001535
Iteration 27/1000 | Loss: 0.00001534
Iteration 28/1000 | Loss: 0.00001534
Iteration 29/1000 | Loss: 0.00001534
Iteration 30/1000 | Loss: 0.00001533
Iteration 31/1000 | Loss: 0.00001533
Iteration 32/1000 | Loss: 0.00001533
Iteration 33/1000 | Loss: 0.00001533
Iteration 34/1000 | Loss: 0.00001533
Iteration 35/1000 | Loss: 0.00001533
Iteration 36/1000 | Loss: 0.00001533
Iteration 37/1000 | Loss: 0.00001533
Iteration 38/1000 | Loss: 0.00001532
Iteration 39/1000 | Loss: 0.00001532
Iteration 40/1000 | Loss: 0.00001532
Iteration 41/1000 | Loss: 0.00001532
Iteration 42/1000 | Loss: 0.00001531
Iteration 43/1000 | Loss: 0.00001531
Iteration 44/1000 | Loss: 0.00001530
Iteration 45/1000 | Loss: 0.00001530
Iteration 46/1000 | Loss: 0.00001530
Iteration 47/1000 | Loss: 0.00001530
Iteration 48/1000 | Loss: 0.00001530
Iteration 49/1000 | Loss: 0.00001530
Iteration 50/1000 | Loss: 0.00001529
Iteration 51/1000 | Loss: 0.00001529
Iteration 52/1000 | Loss: 0.00001529
Iteration 53/1000 | Loss: 0.00001529
Iteration 54/1000 | Loss: 0.00001529
Iteration 55/1000 | Loss: 0.00001528
Iteration 56/1000 | Loss: 0.00001528
Iteration 57/1000 | Loss: 0.00001528
Iteration 58/1000 | Loss: 0.00001528
Iteration 59/1000 | Loss: 0.00001527
Iteration 60/1000 | Loss: 0.00001527
Iteration 61/1000 | Loss: 0.00001527
Iteration 62/1000 | Loss: 0.00001527
Iteration 63/1000 | Loss: 0.00001527
Iteration 64/1000 | Loss: 0.00001527
Iteration 65/1000 | Loss: 0.00001527
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001526
Iteration 68/1000 | Loss: 0.00001526
Iteration 69/1000 | Loss: 0.00001526
Iteration 70/1000 | Loss: 0.00001526
Iteration 71/1000 | Loss: 0.00001526
Iteration 72/1000 | Loss: 0.00001526
Iteration 73/1000 | Loss: 0.00001525
Iteration 74/1000 | Loss: 0.00001525
Iteration 75/1000 | Loss: 0.00001525
Iteration 76/1000 | Loss: 0.00001525
Iteration 77/1000 | Loss: 0.00001525
Iteration 78/1000 | Loss: 0.00001525
Iteration 79/1000 | Loss: 0.00001525
Iteration 80/1000 | Loss: 0.00001525
Iteration 81/1000 | Loss: 0.00001525
Iteration 82/1000 | Loss: 0.00001525
Iteration 83/1000 | Loss: 0.00001525
Iteration 84/1000 | Loss: 0.00001525
Iteration 85/1000 | Loss: 0.00001525
Iteration 86/1000 | Loss: 0.00001525
Iteration 87/1000 | Loss: 0.00001525
Iteration 88/1000 | Loss: 0.00001525
Iteration 89/1000 | Loss: 0.00001525
Iteration 90/1000 | Loss: 0.00001525
Iteration 91/1000 | Loss: 0.00001525
Iteration 92/1000 | Loss: 0.00001525
Iteration 93/1000 | Loss: 0.00001525
Iteration 94/1000 | Loss: 0.00001525
Iteration 95/1000 | Loss: 0.00001525
Iteration 96/1000 | Loss: 0.00001525
Iteration 97/1000 | Loss: 0.00001525
Iteration 98/1000 | Loss: 0.00001525
Iteration 99/1000 | Loss: 0.00001525
Iteration 100/1000 | Loss: 0.00001525
Iteration 101/1000 | Loss: 0.00001525
Iteration 102/1000 | Loss: 0.00001525
Iteration 103/1000 | Loss: 0.00001525
Iteration 104/1000 | Loss: 0.00001525
Iteration 105/1000 | Loss: 0.00001525
Iteration 106/1000 | Loss: 0.00001525
Iteration 107/1000 | Loss: 0.00001525
Iteration 108/1000 | Loss: 0.00001525
Iteration 109/1000 | Loss: 0.00001525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.5247787814587355e-05, 1.5247787814587355e-05, 1.5247787814587355e-05, 1.5247787814587355e-05, 1.5247787814587355e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5247787814587355e-05

Optimization complete. Final v2v error: 3.221438407897949 mm

Highest mean error: 4.015997409820557 mm for frame 0

Lowest mean error: 3.0103302001953125 mm for frame 27

Saving results

Total time: 48.72510480880737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01155212
Iteration 2/25 | Loss: 0.00640088
Iteration 3/25 | Loss: 0.00405137
Iteration 4/25 | Loss: 0.00345210
Iteration 5/25 | Loss: 0.00335724
Iteration 6/25 | Loss: 0.00307800
Iteration 7/25 | Loss: 0.00278301
Iteration 8/25 | Loss: 0.00272724
Iteration 9/25 | Loss: 0.00268763
Iteration 10/25 | Loss: 0.00264121
Iteration 11/25 | Loss: 0.00261924
Iteration 12/25 | Loss: 0.00262943
Iteration 13/25 | Loss: 0.00259418
Iteration 14/25 | Loss: 0.00258812
Iteration 15/25 | Loss: 0.00257401
Iteration 16/25 | Loss: 0.00257242
Iteration 17/25 | Loss: 0.00258692
Iteration 18/25 | Loss: 0.00257564
Iteration 19/25 | Loss: 0.00255718
Iteration 20/25 | Loss: 0.00256017
Iteration 21/25 | Loss: 0.00255466
Iteration 22/25 | Loss: 0.00256360
Iteration 23/25 | Loss: 0.00254416
Iteration 24/25 | Loss: 0.00254539
Iteration 25/25 | Loss: 0.00253487

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.39463824
Iteration 2/25 | Loss: 0.01246591
Iteration 3/25 | Loss: 0.01116981
Iteration 4/25 | Loss: 0.01034589
Iteration 5/25 | Loss: 0.01102998
Iteration 6/25 | Loss: 0.00953890
Iteration 7/25 | Loss: 0.00953880
Iteration 8/25 | Loss: 0.00953880
Iteration 9/25 | Loss: 0.00953879
Iteration 10/25 | Loss: 0.00953879
Iteration 11/25 | Loss: 0.00953879
Iteration 12/25 | Loss: 0.00953879
Iteration 13/25 | Loss: 0.00953879
Iteration 14/25 | Loss: 0.00953879
Iteration 15/25 | Loss: 0.00953879
Iteration 16/25 | Loss: 0.00953879
Iteration 17/25 | Loss: 0.00953879
Iteration 18/25 | Loss: 0.00953879
Iteration 19/25 | Loss: 0.00953879
Iteration 20/25 | Loss: 0.00953879
Iteration 21/25 | Loss: 0.00953879
Iteration 22/25 | Loss: 0.00953879
Iteration 23/25 | Loss: 0.00953879
Iteration 24/25 | Loss: 0.00953879
Iteration 25/25 | Loss: 0.00953879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00953879
Iteration 2/1000 | Loss: 0.00154272
Iteration 3/1000 | Loss: 0.00131762
Iteration 4/1000 | Loss: 0.00123069
Iteration 5/1000 | Loss: 0.00093151
Iteration 6/1000 | Loss: 0.00155614
Iteration 7/1000 | Loss: 0.00253959
Iteration 8/1000 | Loss: 0.00155550
Iteration 9/1000 | Loss: 0.00260485
Iteration 10/1000 | Loss: 0.00101163
Iteration 11/1000 | Loss: 0.00201679
Iteration 12/1000 | Loss: 0.00233045
Iteration 13/1000 | Loss: 0.00174960
Iteration 14/1000 | Loss: 0.00188623
Iteration 15/1000 | Loss: 0.00163689
Iteration 16/1000 | Loss: 0.00116939
Iteration 17/1000 | Loss: 0.00217960
Iteration 18/1000 | Loss: 0.00108925
Iteration 19/1000 | Loss: 0.00115369
Iteration 20/1000 | Loss: 0.00147960
Iteration 21/1000 | Loss: 0.00188290
Iteration 22/1000 | Loss: 0.00170694
Iteration 23/1000 | Loss: 0.00061181
Iteration 24/1000 | Loss: 0.00068553
Iteration 25/1000 | Loss: 0.00045731
Iteration 26/1000 | Loss: 0.00152969
Iteration 27/1000 | Loss: 0.00139172
Iteration 28/1000 | Loss: 0.00071330
Iteration 29/1000 | Loss: 0.00081515
Iteration 30/1000 | Loss: 0.00127931
Iteration 31/1000 | Loss: 0.00101288
Iteration 32/1000 | Loss: 0.00176083
Iteration 33/1000 | Loss: 0.00152466
Iteration 34/1000 | Loss: 0.00154304
Iteration 35/1000 | Loss: 0.00167607
Iteration 36/1000 | Loss: 0.00131136
Iteration 37/1000 | Loss: 0.00102046
Iteration 38/1000 | Loss: 0.00066941
Iteration 39/1000 | Loss: 0.00041997
Iteration 40/1000 | Loss: 0.00098732
Iteration 41/1000 | Loss: 0.00120963
Iteration 42/1000 | Loss: 0.00122897
Iteration 43/1000 | Loss: 0.00055786
Iteration 44/1000 | Loss: 0.00130398
Iteration 45/1000 | Loss: 0.00180869
Iteration 46/1000 | Loss: 0.00168301
Iteration 47/1000 | Loss: 0.00141771
Iteration 48/1000 | Loss: 0.00125748
Iteration 49/1000 | Loss: 0.00114437
Iteration 50/1000 | Loss: 0.00094768
Iteration 51/1000 | Loss: 0.00100453
Iteration 52/1000 | Loss: 0.00126960
Iteration 53/1000 | Loss: 0.00125897
Iteration 54/1000 | Loss: 0.00107126
Iteration 55/1000 | Loss: 0.00183967
Iteration 56/1000 | Loss: 0.00210684
Iteration 57/1000 | Loss: 0.00180830
Iteration 58/1000 | Loss: 0.00112024
Iteration 59/1000 | Loss: 0.00062220
Iteration 60/1000 | Loss: 0.00052378
Iteration 61/1000 | Loss: 0.00062121
Iteration 62/1000 | Loss: 0.00050795
Iteration 63/1000 | Loss: 0.00076306
Iteration 64/1000 | Loss: 0.00050554
Iteration 65/1000 | Loss: 0.00056731
Iteration 66/1000 | Loss: 0.00057512
Iteration 67/1000 | Loss: 0.00047612
Iteration 68/1000 | Loss: 0.00050198
Iteration 69/1000 | Loss: 0.00052037
Iteration 70/1000 | Loss: 0.00054551
Iteration 71/1000 | Loss: 0.00052680
Iteration 72/1000 | Loss: 0.00057930
Iteration 73/1000 | Loss: 0.00038264
Iteration 74/1000 | Loss: 0.00040366
Iteration 75/1000 | Loss: 0.00037623
Iteration 76/1000 | Loss: 0.00059753
Iteration 77/1000 | Loss: 0.00047570
Iteration 78/1000 | Loss: 0.00054154
Iteration 79/1000 | Loss: 0.00055503
Iteration 80/1000 | Loss: 0.00037870
Iteration 81/1000 | Loss: 0.00036495
Iteration 82/1000 | Loss: 0.00039645
Iteration 83/1000 | Loss: 0.00036403
Iteration 84/1000 | Loss: 0.00063088
Iteration 85/1000 | Loss: 0.00040231
Iteration 86/1000 | Loss: 0.00037850
Iteration 87/1000 | Loss: 0.00036359
Iteration 88/1000 | Loss: 0.00035841
Iteration 89/1000 | Loss: 0.00035501
Iteration 90/1000 | Loss: 0.00059960
Iteration 91/1000 | Loss: 0.00036751
Iteration 92/1000 | Loss: 0.00087567
Iteration 93/1000 | Loss: 0.00037395
Iteration 94/1000 | Loss: 0.00063189
Iteration 95/1000 | Loss: 0.00035960
Iteration 96/1000 | Loss: 0.00035187
Iteration 97/1000 | Loss: 0.00034627
Iteration 98/1000 | Loss: 0.00062339
Iteration 99/1000 | Loss: 0.00035314
Iteration 100/1000 | Loss: 0.00034399
Iteration 101/1000 | Loss: 0.00063441
Iteration 102/1000 | Loss: 0.00035073
Iteration 103/1000 | Loss: 0.00034137
Iteration 104/1000 | Loss: 0.00035011
Iteration 105/1000 | Loss: 0.00063639
Iteration 106/1000 | Loss: 0.00035471
Iteration 107/1000 | Loss: 0.00033791
Iteration 108/1000 | Loss: 0.00033385
Iteration 109/1000 | Loss: 0.00064171
Iteration 110/1000 | Loss: 0.00034725
Iteration 111/1000 | Loss: 0.00037380
Iteration 112/1000 | Loss: 0.00040945
Iteration 113/1000 | Loss: 0.00034121
Iteration 114/1000 | Loss: 0.00033192
Iteration 115/1000 | Loss: 0.00064191
Iteration 116/1000 | Loss: 0.00033729
Iteration 117/1000 | Loss: 0.00032919
Iteration 118/1000 | Loss: 0.00033670
Iteration 119/1000 | Loss: 0.00039922
Iteration 120/1000 | Loss: 0.00065738
Iteration 121/1000 | Loss: 0.00060704
Iteration 122/1000 | Loss: 0.00039488
Iteration 123/1000 | Loss: 0.00033897
Iteration 124/1000 | Loss: 0.00032720
Iteration 125/1000 | Loss: 0.00039338
Iteration 126/1000 | Loss: 0.00036473
Iteration 127/1000 | Loss: 0.00035130
Iteration 128/1000 | Loss: 0.00036307
Iteration 129/1000 | Loss: 0.00036142
Iteration 130/1000 | Loss: 0.00042022
Iteration 131/1000 | Loss: 0.00054059
Iteration 132/1000 | Loss: 0.00037758
Iteration 133/1000 | Loss: 0.00068785
Iteration 134/1000 | Loss: 0.00037862
Iteration 135/1000 | Loss: 0.00032681
Iteration 136/1000 | Loss: 0.00031791
Iteration 137/1000 | Loss: 0.00031726
Iteration 138/1000 | Loss: 0.00036620
Iteration 139/1000 | Loss: 0.00074156
Iteration 140/1000 | Loss: 0.00071959
Iteration 141/1000 | Loss: 0.00102664
Iteration 142/1000 | Loss: 0.00080286
Iteration 143/1000 | Loss: 0.00053207
Iteration 144/1000 | Loss: 0.00046446
Iteration 145/1000 | Loss: 0.00052160
Iteration 146/1000 | Loss: 0.00051295
Iteration 147/1000 | Loss: 0.00033153
Iteration 148/1000 | Loss: 0.00073983
Iteration 149/1000 | Loss: 0.00059912
Iteration 150/1000 | Loss: 0.00029871
Iteration 151/1000 | Loss: 0.00031589
Iteration 152/1000 | Loss: 0.00047819
Iteration 153/1000 | Loss: 0.00028854
Iteration 154/1000 | Loss: 0.00030454
Iteration 155/1000 | Loss: 0.00039355
Iteration 156/1000 | Loss: 0.00103972
Iteration 157/1000 | Loss: 0.00072785
Iteration 158/1000 | Loss: 0.00111299
Iteration 159/1000 | Loss: 0.00062602
Iteration 160/1000 | Loss: 0.00027057
Iteration 161/1000 | Loss: 0.00065478
Iteration 162/1000 | Loss: 0.00110832
Iteration 163/1000 | Loss: 0.00026105
Iteration 164/1000 | Loss: 0.00061683
Iteration 165/1000 | Loss: 0.00025774
Iteration 166/1000 | Loss: 0.00103226
Iteration 167/1000 | Loss: 0.00135622
Iteration 168/1000 | Loss: 0.00145484
Iteration 169/1000 | Loss: 0.00110695
Iteration 170/1000 | Loss: 0.00026503
Iteration 171/1000 | Loss: 0.00045063
Iteration 172/1000 | Loss: 0.00027492
Iteration 173/1000 | Loss: 0.00023468
Iteration 174/1000 | Loss: 0.00021579
Iteration 175/1000 | Loss: 0.00021787
Iteration 176/1000 | Loss: 0.00097198
Iteration 177/1000 | Loss: 0.00068698
Iteration 178/1000 | Loss: 0.00034463
Iteration 179/1000 | Loss: 0.00024589
Iteration 180/1000 | Loss: 0.00065494
Iteration 181/1000 | Loss: 0.00022049
Iteration 182/1000 | Loss: 0.00059830
Iteration 183/1000 | Loss: 0.00022665
Iteration 184/1000 | Loss: 0.00082787
Iteration 185/1000 | Loss: 0.00029650
Iteration 186/1000 | Loss: 0.00021640
Iteration 187/1000 | Loss: 0.00020118
Iteration 188/1000 | Loss: 0.00019525
Iteration 189/1000 | Loss: 0.00150808
Iteration 190/1000 | Loss: 0.00025319
Iteration 191/1000 | Loss: 0.00020254
Iteration 192/1000 | Loss: 0.00019101
Iteration 193/1000 | Loss: 0.00080547
Iteration 194/1000 | Loss: 0.00174375
Iteration 195/1000 | Loss: 0.00217345
Iteration 196/1000 | Loss: 0.00061248
Iteration 197/1000 | Loss: 0.00079932
Iteration 198/1000 | Loss: 0.00027636
Iteration 199/1000 | Loss: 0.00046954
Iteration 200/1000 | Loss: 0.00028286
Iteration 201/1000 | Loss: 0.00063632
Iteration 202/1000 | Loss: 0.00026412
Iteration 203/1000 | Loss: 0.00045915
Iteration 204/1000 | Loss: 0.00021027
Iteration 205/1000 | Loss: 0.00017125
Iteration 206/1000 | Loss: 0.00016542
Iteration 207/1000 | Loss: 0.00025172
Iteration 208/1000 | Loss: 0.00025979
Iteration 209/1000 | Loss: 0.00075759
Iteration 210/1000 | Loss: 0.00020899
Iteration 211/1000 | Loss: 0.00015657
Iteration 212/1000 | Loss: 0.00015406
Iteration 213/1000 | Loss: 0.00026869
Iteration 214/1000 | Loss: 0.00018987
Iteration 215/1000 | Loss: 0.00016109
Iteration 216/1000 | Loss: 0.00040763
Iteration 217/1000 | Loss: 0.00068170
Iteration 218/1000 | Loss: 0.00064864
Iteration 219/1000 | Loss: 0.00016944
Iteration 220/1000 | Loss: 0.00016035
Iteration 221/1000 | Loss: 0.00015449
Iteration 222/1000 | Loss: 0.00032895
Iteration 223/1000 | Loss: 0.00015676
Iteration 224/1000 | Loss: 0.00015187
Iteration 225/1000 | Loss: 0.00014908
Iteration 226/1000 | Loss: 0.00014706
Iteration 227/1000 | Loss: 0.00014602
Iteration 228/1000 | Loss: 0.00014485
Iteration 229/1000 | Loss: 0.00015281
Iteration 230/1000 | Loss: 0.00014434
Iteration 231/1000 | Loss: 0.00014375
Iteration 232/1000 | Loss: 0.00014347
Iteration 233/1000 | Loss: 0.00046154
Iteration 234/1000 | Loss: 0.00047683
Iteration 235/1000 | Loss: 0.00093618
Iteration 236/1000 | Loss: 0.00031774
Iteration 237/1000 | Loss: 0.00028892
Iteration 238/1000 | Loss: 0.00030702
Iteration 239/1000 | Loss: 0.00022679
Iteration 240/1000 | Loss: 0.00016752
Iteration 241/1000 | Loss: 0.00015779
Iteration 242/1000 | Loss: 0.00015387
Iteration 243/1000 | Loss: 0.00015014
Iteration 244/1000 | Loss: 0.00014586
Iteration 245/1000 | Loss: 0.00014316
Iteration 246/1000 | Loss: 0.00014149
Iteration 247/1000 | Loss: 0.00014042
Iteration 248/1000 | Loss: 0.00013962
Iteration 249/1000 | Loss: 0.00013896
Iteration 250/1000 | Loss: 0.00013865
Iteration 251/1000 | Loss: 0.00013835
Iteration 252/1000 | Loss: 0.00013803
Iteration 253/1000 | Loss: 0.00013782
Iteration 254/1000 | Loss: 0.00013771
Iteration 255/1000 | Loss: 0.00013761
Iteration 256/1000 | Loss: 0.00013748
Iteration 257/1000 | Loss: 0.00013747
Iteration 258/1000 | Loss: 0.00013746
Iteration 259/1000 | Loss: 0.00013743
Iteration 260/1000 | Loss: 0.00013738
Iteration 261/1000 | Loss: 0.00013738
Iteration 262/1000 | Loss: 0.00013738
Iteration 263/1000 | Loss: 0.00013738
Iteration 264/1000 | Loss: 0.00013738
Iteration 265/1000 | Loss: 0.00013737
Iteration 266/1000 | Loss: 0.00013737
Iteration 267/1000 | Loss: 0.00013737
Iteration 268/1000 | Loss: 0.00013737
Iteration 269/1000 | Loss: 0.00013737
Iteration 270/1000 | Loss: 0.00013737
Iteration 271/1000 | Loss: 0.00013737
Iteration 272/1000 | Loss: 0.00013736
Iteration 273/1000 | Loss: 0.00013736
Iteration 274/1000 | Loss: 0.00013736
Iteration 275/1000 | Loss: 0.00013736
Iteration 276/1000 | Loss: 0.00013735
Iteration 277/1000 | Loss: 0.00013735
Iteration 278/1000 | Loss: 0.00013735
Iteration 279/1000 | Loss: 0.00013735
Iteration 280/1000 | Loss: 0.00013734
Iteration 281/1000 | Loss: 0.00013734
Iteration 282/1000 | Loss: 0.00013734
Iteration 283/1000 | Loss: 0.00013734
Iteration 284/1000 | Loss: 0.00013734
Iteration 285/1000 | Loss: 0.00013733
Iteration 286/1000 | Loss: 0.00013733
Iteration 287/1000 | Loss: 0.00013732
Iteration 288/1000 | Loss: 0.00013731
Iteration 289/1000 | Loss: 0.00013731
Iteration 290/1000 | Loss: 0.00013731
Iteration 291/1000 | Loss: 0.00013730
Iteration 292/1000 | Loss: 0.00064720
Iteration 293/1000 | Loss: 0.00029945
Iteration 294/1000 | Loss: 0.00016777
Iteration 295/1000 | Loss: 0.00014424
Iteration 296/1000 | Loss: 0.00013995
Iteration 297/1000 | Loss: 0.00013878
Iteration 298/1000 | Loss: 0.00013830
Iteration 299/1000 | Loss: 0.00013801
Iteration 300/1000 | Loss: 0.00013792
Iteration 301/1000 | Loss: 0.00029260
Iteration 302/1000 | Loss: 0.00020569
Iteration 303/1000 | Loss: 0.00018762
Iteration 304/1000 | Loss: 0.00022451
Iteration 305/1000 | Loss: 0.00018798
Iteration 306/1000 | Loss: 0.00028092
Iteration 307/1000 | Loss: 0.00014493
Iteration 308/1000 | Loss: 0.00065061
Iteration 309/1000 | Loss: 0.00033698
Iteration 310/1000 | Loss: 0.00014634
Iteration 311/1000 | Loss: 0.00014226
Iteration 312/1000 | Loss: 0.00014064
Iteration 313/1000 | Loss: 0.00013966
Iteration 314/1000 | Loss: 0.00013913
Iteration 315/1000 | Loss: 0.00013861
Iteration 316/1000 | Loss: 0.00013831
Iteration 317/1000 | Loss: 0.00013807
Iteration 318/1000 | Loss: 0.00013787
Iteration 319/1000 | Loss: 0.00013771
Iteration 320/1000 | Loss: 0.00014953
Iteration 321/1000 | Loss: 0.00014705
Iteration 322/1000 | Loss: 0.00014928
Iteration 323/1000 | Loss: 0.00014928
Iteration 324/1000 | Loss: 0.00071147
Iteration 325/1000 | Loss: 0.00018529
Iteration 326/1000 | Loss: 0.00014886
Iteration 327/1000 | Loss: 0.00014078
Iteration 328/1000 | Loss: 0.00013758
Iteration 329/1000 | Loss: 0.00013587
Iteration 330/1000 | Loss: 0.00013507
Iteration 331/1000 | Loss: 0.00013467
Iteration 332/1000 | Loss: 0.00013446
Iteration 333/1000 | Loss: 0.00013440
Iteration 334/1000 | Loss: 0.00013438
Iteration 335/1000 | Loss: 0.00013438
Iteration 336/1000 | Loss: 0.00013438
Iteration 337/1000 | Loss: 0.00013437
Iteration 338/1000 | Loss: 0.00013437
Iteration 339/1000 | Loss: 0.00013437
Iteration 340/1000 | Loss: 0.00013436
Iteration 341/1000 | Loss: 0.00013435
Iteration 342/1000 | Loss: 0.00013435
Iteration 343/1000 | Loss: 0.00013435
Iteration 344/1000 | Loss: 0.00013435
Iteration 345/1000 | Loss: 0.00013435
Iteration 346/1000 | Loss: 0.00013435
Iteration 347/1000 | Loss: 0.00013435
Iteration 348/1000 | Loss: 0.00013435
Iteration 349/1000 | Loss: 0.00013435
Iteration 350/1000 | Loss: 0.00013435
Iteration 351/1000 | Loss: 0.00013435
Iteration 352/1000 | Loss: 0.00013435
Iteration 353/1000 | Loss: 0.00013435
Iteration 354/1000 | Loss: 0.00013435
Iteration 355/1000 | Loss: 0.00013435
Iteration 356/1000 | Loss: 0.00013435
Iteration 357/1000 | Loss: 0.00013435
Iteration 358/1000 | Loss: 0.00013435
Iteration 359/1000 | Loss: 0.00013435
Iteration 360/1000 | Loss: 0.00013435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 360. Stopping optimization.
Last 5 losses: [0.00013434735592454672, 0.00013434735592454672, 0.00013434735592454672, 0.00013434735592454672, 0.00013434735592454672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013434735592454672

Optimization complete. Final v2v error: 7.748732566833496 mm

Highest mean error: 15.473264694213867 mm for frame 140

Lowest mean error: 5.267342567443848 mm for frame 12

Saving results

Total time: 505.3292076587677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850180
Iteration 2/25 | Loss: 0.00181053
Iteration 3/25 | Loss: 0.00160134
Iteration 4/25 | Loss: 0.00158368
Iteration 5/25 | Loss: 0.00158008
Iteration 6/25 | Loss: 0.00157993
Iteration 7/25 | Loss: 0.00157990
Iteration 8/25 | Loss: 0.00157990
Iteration 9/25 | Loss: 0.00157990
Iteration 10/25 | Loss: 0.00157990
Iteration 11/25 | Loss: 0.00157990
Iteration 12/25 | Loss: 0.00157990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015798950335010886, 0.0015798950335010886, 0.0015798950335010886, 0.0015798950335010886, 0.0015798950335010886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015798950335010886

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25187147
Iteration 2/25 | Loss: 0.00163406
Iteration 3/25 | Loss: 0.00163406
Iteration 4/25 | Loss: 0.00163406
Iteration 5/25 | Loss: 0.00163406
Iteration 6/25 | Loss: 0.00163406
Iteration 7/25 | Loss: 0.00163406
Iteration 8/25 | Loss: 0.00163406
Iteration 9/25 | Loss: 0.00163406
Iteration 10/25 | Loss: 0.00163406
Iteration 11/25 | Loss: 0.00163406
Iteration 12/25 | Loss: 0.00163406
Iteration 13/25 | Loss: 0.00163406
Iteration 14/25 | Loss: 0.00163406
Iteration 15/25 | Loss: 0.00163406
Iteration 16/25 | Loss: 0.00163406
Iteration 17/25 | Loss: 0.00163406
Iteration 18/25 | Loss: 0.00163406
Iteration 19/25 | Loss: 0.00163406
Iteration 20/25 | Loss: 0.00163406
Iteration 21/25 | Loss: 0.00163406
Iteration 22/25 | Loss: 0.00163406
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0016340611036866903, 0.0016340611036866903, 0.0016340611036866903, 0.0016340611036866903, 0.0016340611036866903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016340611036866903

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163406
Iteration 2/1000 | Loss: 0.00007857
Iteration 3/1000 | Loss: 0.00004228
Iteration 4/1000 | Loss: 0.00003720
Iteration 5/1000 | Loss: 0.00003086
Iteration 6/1000 | Loss: 0.00002828
Iteration 7/1000 | Loss: 0.00002697
Iteration 8/1000 | Loss: 0.00002610
Iteration 9/1000 | Loss: 0.00002562
Iteration 10/1000 | Loss: 0.00002523
Iteration 11/1000 | Loss: 0.00002499
Iteration 12/1000 | Loss: 0.00002474
Iteration 13/1000 | Loss: 0.00002455
Iteration 14/1000 | Loss: 0.00002446
Iteration 15/1000 | Loss: 0.00002446
Iteration 16/1000 | Loss: 0.00002445
Iteration 17/1000 | Loss: 0.00002445
Iteration 18/1000 | Loss: 0.00002444
Iteration 19/1000 | Loss: 0.00002444
Iteration 20/1000 | Loss: 0.00002442
Iteration 21/1000 | Loss: 0.00002442
Iteration 22/1000 | Loss: 0.00002438
Iteration 23/1000 | Loss: 0.00002438
Iteration 24/1000 | Loss: 0.00002438
Iteration 25/1000 | Loss: 0.00002438
Iteration 26/1000 | Loss: 0.00002435
Iteration 27/1000 | Loss: 0.00002435
Iteration 28/1000 | Loss: 0.00002435
Iteration 29/1000 | Loss: 0.00002434
Iteration 30/1000 | Loss: 0.00002434
Iteration 31/1000 | Loss: 0.00002433
Iteration 32/1000 | Loss: 0.00002432
Iteration 33/1000 | Loss: 0.00002429
Iteration 34/1000 | Loss: 0.00002427
Iteration 35/1000 | Loss: 0.00002426
Iteration 36/1000 | Loss: 0.00002426
Iteration 37/1000 | Loss: 0.00002425
Iteration 38/1000 | Loss: 0.00002425
Iteration 39/1000 | Loss: 0.00002424
Iteration 40/1000 | Loss: 0.00002424
Iteration 41/1000 | Loss: 0.00002423
Iteration 42/1000 | Loss: 0.00002423
Iteration 43/1000 | Loss: 0.00002423
Iteration 44/1000 | Loss: 0.00002423
Iteration 45/1000 | Loss: 0.00002422
Iteration 46/1000 | Loss: 0.00002422
Iteration 47/1000 | Loss: 0.00002421
Iteration 48/1000 | Loss: 0.00002421
Iteration 49/1000 | Loss: 0.00002421
Iteration 50/1000 | Loss: 0.00002420
Iteration 51/1000 | Loss: 0.00002420
Iteration 52/1000 | Loss: 0.00002420
Iteration 53/1000 | Loss: 0.00002419
Iteration 54/1000 | Loss: 0.00002419
Iteration 55/1000 | Loss: 0.00002419
Iteration 56/1000 | Loss: 0.00002418
Iteration 57/1000 | Loss: 0.00002418
Iteration 58/1000 | Loss: 0.00002418
Iteration 59/1000 | Loss: 0.00002418
Iteration 60/1000 | Loss: 0.00002417
Iteration 61/1000 | Loss: 0.00002417
Iteration 62/1000 | Loss: 0.00002417
Iteration 63/1000 | Loss: 0.00002417
Iteration 64/1000 | Loss: 0.00002416
Iteration 65/1000 | Loss: 0.00002416
Iteration 66/1000 | Loss: 0.00002416
Iteration 67/1000 | Loss: 0.00002416
Iteration 68/1000 | Loss: 0.00002416
Iteration 69/1000 | Loss: 0.00002416
Iteration 70/1000 | Loss: 0.00002415
Iteration 71/1000 | Loss: 0.00002415
Iteration 72/1000 | Loss: 0.00002415
Iteration 73/1000 | Loss: 0.00002414
Iteration 74/1000 | Loss: 0.00002414
Iteration 75/1000 | Loss: 0.00002414
Iteration 76/1000 | Loss: 0.00002414
Iteration 77/1000 | Loss: 0.00002414
Iteration 78/1000 | Loss: 0.00002414
Iteration 79/1000 | Loss: 0.00002414
Iteration 80/1000 | Loss: 0.00002414
Iteration 81/1000 | Loss: 0.00002414
Iteration 82/1000 | Loss: 0.00002414
Iteration 83/1000 | Loss: 0.00002414
Iteration 84/1000 | Loss: 0.00002414
Iteration 85/1000 | Loss: 0.00002414
Iteration 86/1000 | Loss: 0.00002413
Iteration 87/1000 | Loss: 0.00002413
Iteration 88/1000 | Loss: 0.00002413
Iteration 89/1000 | Loss: 0.00002413
Iteration 90/1000 | Loss: 0.00002413
Iteration 91/1000 | Loss: 0.00002413
Iteration 92/1000 | Loss: 0.00002413
Iteration 93/1000 | Loss: 0.00002413
Iteration 94/1000 | Loss: 0.00002412
Iteration 95/1000 | Loss: 0.00002412
Iteration 96/1000 | Loss: 0.00002412
Iteration 97/1000 | Loss: 0.00002412
Iteration 98/1000 | Loss: 0.00002412
Iteration 99/1000 | Loss: 0.00002412
Iteration 100/1000 | Loss: 0.00002412
Iteration 101/1000 | Loss: 0.00002412
Iteration 102/1000 | Loss: 0.00002412
Iteration 103/1000 | Loss: 0.00002412
Iteration 104/1000 | Loss: 0.00002411
Iteration 105/1000 | Loss: 0.00002411
Iteration 106/1000 | Loss: 0.00002410
Iteration 107/1000 | Loss: 0.00002410
Iteration 108/1000 | Loss: 0.00002409
Iteration 109/1000 | Loss: 0.00002409
Iteration 110/1000 | Loss: 0.00002409
Iteration 111/1000 | Loss: 0.00002409
Iteration 112/1000 | Loss: 0.00002408
Iteration 113/1000 | Loss: 0.00002408
Iteration 114/1000 | Loss: 0.00002408
Iteration 115/1000 | Loss: 0.00002408
Iteration 116/1000 | Loss: 0.00002408
Iteration 117/1000 | Loss: 0.00002408
Iteration 118/1000 | Loss: 0.00002408
Iteration 119/1000 | Loss: 0.00002408
Iteration 120/1000 | Loss: 0.00002408
Iteration 121/1000 | Loss: 0.00002407
Iteration 122/1000 | Loss: 0.00002407
Iteration 123/1000 | Loss: 0.00002407
Iteration 124/1000 | Loss: 0.00002407
Iteration 125/1000 | Loss: 0.00002407
Iteration 126/1000 | Loss: 0.00002407
Iteration 127/1000 | Loss: 0.00002407
Iteration 128/1000 | Loss: 0.00002406
Iteration 129/1000 | Loss: 0.00002406
Iteration 130/1000 | Loss: 0.00002406
Iteration 131/1000 | Loss: 0.00002406
Iteration 132/1000 | Loss: 0.00002406
Iteration 133/1000 | Loss: 0.00002406
Iteration 134/1000 | Loss: 0.00002406
Iteration 135/1000 | Loss: 0.00002406
Iteration 136/1000 | Loss: 0.00002406
Iteration 137/1000 | Loss: 0.00002406
Iteration 138/1000 | Loss: 0.00002406
Iteration 139/1000 | Loss: 0.00002405
Iteration 140/1000 | Loss: 0.00002405
Iteration 141/1000 | Loss: 0.00002405
Iteration 142/1000 | Loss: 0.00002405
Iteration 143/1000 | Loss: 0.00002405
Iteration 144/1000 | Loss: 0.00002405
Iteration 145/1000 | Loss: 0.00002405
Iteration 146/1000 | Loss: 0.00002405
Iteration 147/1000 | Loss: 0.00002405
Iteration 148/1000 | Loss: 0.00002405
Iteration 149/1000 | Loss: 0.00002405
Iteration 150/1000 | Loss: 0.00002405
Iteration 151/1000 | Loss: 0.00002405
Iteration 152/1000 | Loss: 0.00002405
Iteration 153/1000 | Loss: 0.00002404
Iteration 154/1000 | Loss: 0.00002404
Iteration 155/1000 | Loss: 0.00002404
Iteration 156/1000 | Loss: 0.00002404
Iteration 157/1000 | Loss: 0.00002404
Iteration 158/1000 | Loss: 0.00002404
Iteration 159/1000 | Loss: 0.00002404
Iteration 160/1000 | Loss: 0.00002404
Iteration 161/1000 | Loss: 0.00002404
Iteration 162/1000 | Loss: 0.00002404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.404437145742122e-05, 2.404437145742122e-05, 2.404437145742122e-05, 2.404437145742122e-05, 2.404437145742122e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.404437145742122e-05

Optimization complete. Final v2v error: 4.206978797912598 mm

Highest mean error: 4.64520263671875 mm for frame 0

Lowest mean error: 3.9018378257751465 mm for frame 34

Saving results

Total time: 43.278639793395996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00912729
Iteration 2/25 | Loss: 0.00165454
Iteration 3/25 | Loss: 0.00153045
Iteration 4/25 | Loss: 0.00152070
Iteration 5/25 | Loss: 0.00151970
Iteration 6/25 | Loss: 0.00151970
Iteration 7/25 | Loss: 0.00151970
Iteration 8/25 | Loss: 0.00151970
Iteration 9/25 | Loss: 0.00151970
Iteration 10/25 | Loss: 0.00151970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0015196976019069552, 0.0015196976019069552, 0.0015196976019069552, 0.0015196976019069552, 0.0015196976019069552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015196976019069552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27136064
Iteration 2/25 | Loss: 0.00180738
Iteration 3/25 | Loss: 0.00180738
Iteration 4/25 | Loss: 0.00180738
Iteration 5/25 | Loss: 0.00180738
Iteration 6/25 | Loss: 0.00180738
Iteration 7/25 | Loss: 0.00180738
Iteration 8/25 | Loss: 0.00180738
Iteration 9/25 | Loss: 0.00180738
Iteration 10/25 | Loss: 0.00180738
Iteration 11/25 | Loss: 0.00180738
Iteration 12/25 | Loss: 0.00180738
Iteration 13/25 | Loss: 0.00180738
Iteration 14/25 | Loss: 0.00180738
Iteration 15/25 | Loss: 0.00180738
Iteration 16/25 | Loss: 0.00180738
Iteration 17/25 | Loss: 0.00180738
Iteration 18/25 | Loss: 0.00180738
Iteration 19/25 | Loss: 0.00180738
Iteration 20/25 | Loss: 0.00180738
Iteration 21/25 | Loss: 0.00180738
Iteration 22/25 | Loss: 0.00180738
Iteration 23/25 | Loss: 0.00180738
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0018073790706694126, 0.0018073790706694126, 0.0018073790706694126, 0.0018073790706694126, 0.0018073790706694126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018073790706694126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180738
Iteration 2/1000 | Loss: 0.00003545
Iteration 3/1000 | Loss: 0.00002793
Iteration 4/1000 | Loss: 0.00002506
Iteration 5/1000 | Loss: 0.00002385
Iteration 6/1000 | Loss: 0.00002308
Iteration 7/1000 | Loss: 0.00002267
Iteration 8/1000 | Loss: 0.00002229
Iteration 9/1000 | Loss: 0.00002196
Iteration 10/1000 | Loss: 0.00002196
Iteration 11/1000 | Loss: 0.00002174
Iteration 12/1000 | Loss: 0.00002170
Iteration 13/1000 | Loss: 0.00002169
Iteration 14/1000 | Loss: 0.00002169
Iteration 15/1000 | Loss: 0.00002168
Iteration 16/1000 | Loss: 0.00002167
Iteration 17/1000 | Loss: 0.00002166
Iteration 18/1000 | Loss: 0.00002166
Iteration 19/1000 | Loss: 0.00002166
Iteration 20/1000 | Loss: 0.00002166
Iteration 21/1000 | Loss: 0.00002165
Iteration 22/1000 | Loss: 0.00002165
Iteration 23/1000 | Loss: 0.00002164
Iteration 24/1000 | Loss: 0.00002164
Iteration 25/1000 | Loss: 0.00002164
Iteration 26/1000 | Loss: 0.00002163
Iteration 27/1000 | Loss: 0.00002163
Iteration 28/1000 | Loss: 0.00002163
Iteration 29/1000 | Loss: 0.00002162
Iteration 30/1000 | Loss: 0.00002162
Iteration 31/1000 | Loss: 0.00002158
Iteration 32/1000 | Loss: 0.00002158
Iteration 33/1000 | Loss: 0.00002158
Iteration 34/1000 | Loss: 0.00002158
Iteration 35/1000 | Loss: 0.00002158
Iteration 36/1000 | Loss: 0.00002157
Iteration 37/1000 | Loss: 0.00002157
Iteration 38/1000 | Loss: 0.00002153
Iteration 39/1000 | Loss: 0.00002153
Iteration 40/1000 | Loss: 0.00002151
Iteration 41/1000 | Loss: 0.00002151
Iteration 42/1000 | Loss: 0.00002149
Iteration 43/1000 | Loss: 0.00002148
Iteration 44/1000 | Loss: 0.00002147
Iteration 45/1000 | Loss: 0.00002146
Iteration 46/1000 | Loss: 0.00002146
Iteration 47/1000 | Loss: 0.00002145
Iteration 48/1000 | Loss: 0.00002144
Iteration 49/1000 | Loss: 0.00002143
Iteration 50/1000 | Loss: 0.00002142
Iteration 51/1000 | Loss: 0.00002142
Iteration 52/1000 | Loss: 0.00002141
Iteration 53/1000 | Loss: 0.00002139
Iteration 54/1000 | Loss: 0.00002139
Iteration 55/1000 | Loss: 0.00002138
Iteration 56/1000 | Loss: 0.00002138
Iteration 57/1000 | Loss: 0.00002137
Iteration 58/1000 | Loss: 0.00002137
Iteration 59/1000 | Loss: 0.00002136
Iteration 60/1000 | Loss: 0.00002136
Iteration 61/1000 | Loss: 0.00002134
Iteration 62/1000 | Loss: 0.00002134
Iteration 63/1000 | Loss: 0.00002134
Iteration 64/1000 | Loss: 0.00002134
Iteration 65/1000 | Loss: 0.00002134
Iteration 66/1000 | Loss: 0.00002134
Iteration 67/1000 | Loss: 0.00002134
Iteration 68/1000 | Loss: 0.00002134
Iteration 69/1000 | Loss: 0.00002134
Iteration 70/1000 | Loss: 0.00002134
Iteration 71/1000 | Loss: 0.00002133
Iteration 72/1000 | Loss: 0.00002133
Iteration 73/1000 | Loss: 0.00002133
Iteration 74/1000 | Loss: 0.00002133
Iteration 75/1000 | Loss: 0.00002132
Iteration 76/1000 | Loss: 0.00002132
Iteration 77/1000 | Loss: 0.00002131
Iteration 78/1000 | Loss: 0.00002130
Iteration 79/1000 | Loss: 0.00002130
Iteration 80/1000 | Loss: 0.00002130
Iteration 81/1000 | Loss: 0.00002130
Iteration 82/1000 | Loss: 0.00002130
Iteration 83/1000 | Loss: 0.00002130
Iteration 84/1000 | Loss: 0.00002130
Iteration 85/1000 | Loss: 0.00002130
Iteration 86/1000 | Loss: 0.00002130
Iteration 87/1000 | Loss: 0.00002130
Iteration 88/1000 | Loss: 0.00002130
Iteration 89/1000 | Loss: 0.00002130
Iteration 90/1000 | Loss: 0.00002130
Iteration 91/1000 | Loss: 0.00002129
Iteration 92/1000 | Loss: 0.00002129
Iteration 93/1000 | Loss: 0.00002128
Iteration 94/1000 | Loss: 0.00002128
Iteration 95/1000 | Loss: 0.00002128
Iteration 96/1000 | Loss: 0.00002128
Iteration 97/1000 | Loss: 0.00002128
Iteration 98/1000 | Loss: 0.00002128
Iteration 99/1000 | Loss: 0.00002128
Iteration 100/1000 | Loss: 0.00002128
Iteration 101/1000 | Loss: 0.00002128
Iteration 102/1000 | Loss: 0.00002128
Iteration 103/1000 | Loss: 0.00002127
Iteration 104/1000 | Loss: 0.00002127
Iteration 105/1000 | Loss: 0.00002127
Iteration 106/1000 | Loss: 0.00002127
Iteration 107/1000 | Loss: 0.00002127
Iteration 108/1000 | Loss: 0.00002127
Iteration 109/1000 | Loss: 0.00002127
Iteration 110/1000 | Loss: 0.00002127
Iteration 111/1000 | Loss: 0.00002127
Iteration 112/1000 | Loss: 0.00002127
Iteration 113/1000 | Loss: 0.00002127
Iteration 114/1000 | Loss: 0.00002127
Iteration 115/1000 | Loss: 0.00002127
Iteration 116/1000 | Loss: 0.00002127
Iteration 117/1000 | Loss: 0.00002127
Iteration 118/1000 | Loss: 0.00002127
Iteration 119/1000 | Loss: 0.00002127
Iteration 120/1000 | Loss: 0.00002126
Iteration 121/1000 | Loss: 0.00002126
Iteration 122/1000 | Loss: 0.00002126
Iteration 123/1000 | Loss: 0.00002126
Iteration 124/1000 | Loss: 0.00002126
Iteration 125/1000 | Loss: 0.00002126
Iteration 126/1000 | Loss: 0.00002126
Iteration 127/1000 | Loss: 0.00002126
Iteration 128/1000 | Loss: 0.00002126
Iteration 129/1000 | Loss: 0.00002126
Iteration 130/1000 | Loss: 0.00002126
Iteration 131/1000 | Loss: 0.00002126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.126411891367752e-05, 2.126411891367752e-05, 2.126411891367752e-05, 2.126411891367752e-05, 2.126411891367752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.126411891367752e-05

Optimization complete. Final v2v error: 3.8809471130371094 mm

Highest mean error: 4.070409297943115 mm for frame 99

Lowest mean error: 3.677934408187866 mm for frame 0

Saving results

Total time: 36.672905683517456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455975
Iteration 2/25 | Loss: 0.00166544
Iteration 3/25 | Loss: 0.00158004
Iteration 4/25 | Loss: 0.00157046
Iteration 5/25 | Loss: 0.00156738
Iteration 6/25 | Loss: 0.00156673
Iteration 7/25 | Loss: 0.00156673
Iteration 8/25 | Loss: 0.00156673
Iteration 9/25 | Loss: 0.00156673
Iteration 10/25 | Loss: 0.00156673
Iteration 11/25 | Loss: 0.00156673
Iteration 12/25 | Loss: 0.00156673
Iteration 13/25 | Loss: 0.00156673
Iteration 14/25 | Loss: 0.00156673
Iteration 15/25 | Loss: 0.00156673
Iteration 16/25 | Loss: 0.00156673
Iteration 17/25 | Loss: 0.00156673
Iteration 18/25 | Loss: 0.00156673
Iteration 19/25 | Loss: 0.00156673
Iteration 20/25 | Loss: 0.00156673
Iteration 21/25 | Loss: 0.00156673
Iteration 22/25 | Loss: 0.00156673
Iteration 23/25 | Loss: 0.00156673
Iteration 24/25 | Loss: 0.00156673
Iteration 25/25 | Loss: 0.00156673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31761777
Iteration 2/25 | Loss: 0.00181188
Iteration 3/25 | Loss: 0.00181188
Iteration 4/25 | Loss: 0.00181188
Iteration 5/25 | Loss: 0.00181188
Iteration 6/25 | Loss: 0.00181188
Iteration 7/25 | Loss: 0.00181188
Iteration 8/25 | Loss: 0.00181188
Iteration 9/25 | Loss: 0.00181188
Iteration 10/25 | Loss: 0.00181188
Iteration 11/25 | Loss: 0.00181188
Iteration 12/25 | Loss: 0.00181188
Iteration 13/25 | Loss: 0.00181188
Iteration 14/25 | Loss: 0.00181188
Iteration 15/25 | Loss: 0.00181188
Iteration 16/25 | Loss: 0.00181188
Iteration 17/25 | Loss: 0.00181188
Iteration 18/25 | Loss: 0.00181188
Iteration 19/25 | Loss: 0.00181188
Iteration 20/25 | Loss: 0.00181188
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018118813168257475, 0.0018118813168257475, 0.0018118813168257475, 0.0018118813168257475, 0.0018118813168257475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018118813168257475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181188
Iteration 2/1000 | Loss: 0.00004773
Iteration 3/1000 | Loss: 0.00003353
Iteration 4/1000 | Loss: 0.00003003
Iteration 5/1000 | Loss: 0.00002856
Iteration 6/1000 | Loss: 0.00002739
Iteration 7/1000 | Loss: 0.00002679
Iteration 8/1000 | Loss: 0.00002652
Iteration 9/1000 | Loss: 0.00002624
Iteration 10/1000 | Loss: 0.00002610
Iteration 11/1000 | Loss: 0.00002604
Iteration 12/1000 | Loss: 0.00002601
Iteration 13/1000 | Loss: 0.00002600
Iteration 14/1000 | Loss: 0.00002600
Iteration 15/1000 | Loss: 0.00002600
Iteration 16/1000 | Loss: 0.00002599
Iteration 17/1000 | Loss: 0.00002599
Iteration 18/1000 | Loss: 0.00002598
Iteration 19/1000 | Loss: 0.00002598
Iteration 20/1000 | Loss: 0.00002597
Iteration 21/1000 | Loss: 0.00002596
Iteration 22/1000 | Loss: 0.00002596
Iteration 23/1000 | Loss: 0.00002596
Iteration 24/1000 | Loss: 0.00002595
Iteration 25/1000 | Loss: 0.00002595
Iteration 26/1000 | Loss: 0.00002595
Iteration 27/1000 | Loss: 0.00002594
Iteration 28/1000 | Loss: 0.00002594
Iteration 29/1000 | Loss: 0.00002594
Iteration 30/1000 | Loss: 0.00002593
Iteration 31/1000 | Loss: 0.00002593
Iteration 32/1000 | Loss: 0.00002592
Iteration 33/1000 | Loss: 0.00002592
Iteration 34/1000 | Loss: 0.00002592
Iteration 35/1000 | Loss: 0.00002592
Iteration 36/1000 | Loss: 0.00002592
Iteration 37/1000 | Loss: 0.00002592
Iteration 38/1000 | Loss: 0.00002591
Iteration 39/1000 | Loss: 0.00002591
Iteration 40/1000 | Loss: 0.00002591
Iteration 41/1000 | Loss: 0.00002590
Iteration 42/1000 | Loss: 0.00002590
Iteration 43/1000 | Loss: 0.00002589
Iteration 44/1000 | Loss: 0.00002589
Iteration 45/1000 | Loss: 0.00002589
Iteration 46/1000 | Loss: 0.00002589
Iteration 47/1000 | Loss: 0.00002589
Iteration 48/1000 | Loss: 0.00002589
Iteration 49/1000 | Loss: 0.00002589
Iteration 50/1000 | Loss: 0.00002588
Iteration 51/1000 | Loss: 0.00002588
Iteration 52/1000 | Loss: 0.00002588
Iteration 53/1000 | Loss: 0.00002588
Iteration 54/1000 | Loss: 0.00002588
Iteration 55/1000 | Loss: 0.00002588
Iteration 56/1000 | Loss: 0.00002588
Iteration 57/1000 | Loss: 0.00002587
Iteration 58/1000 | Loss: 0.00002587
Iteration 59/1000 | Loss: 0.00002587
Iteration 60/1000 | Loss: 0.00002586
Iteration 61/1000 | Loss: 0.00002586
Iteration 62/1000 | Loss: 0.00002586
Iteration 63/1000 | Loss: 0.00002586
Iteration 64/1000 | Loss: 0.00002586
Iteration 65/1000 | Loss: 0.00002586
Iteration 66/1000 | Loss: 0.00002585
Iteration 67/1000 | Loss: 0.00002585
Iteration 68/1000 | Loss: 0.00002585
Iteration 69/1000 | Loss: 0.00002585
Iteration 70/1000 | Loss: 0.00002585
Iteration 71/1000 | Loss: 0.00002585
Iteration 72/1000 | Loss: 0.00002584
Iteration 73/1000 | Loss: 0.00002584
Iteration 74/1000 | Loss: 0.00002584
Iteration 75/1000 | Loss: 0.00002584
Iteration 76/1000 | Loss: 0.00002584
Iteration 77/1000 | Loss: 0.00002584
Iteration 78/1000 | Loss: 0.00002583
Iteration 79/1000 | Loss: 0.00002583
Iteration 80/1000 | Loss: 0.00002583
Iteration 81/1000 | Loss: 0.00002583
Iteration 82/1000 | Loss: 0.00002583
Iteration 83/1000 | Loss: 0.00002583
Iteration 84/1000 | Loss: 0.00002583
Iteration 85/1000 | Loss: 0.00002583
Iteration 86/1000 | Loss: 0.00002583
Iteration 87/1000 | Loss: 0.00002582
Iteration 88/1000 | Loss: 0.00002582
Iteration 89/1000 | Loss: 0.00002582
Iteration 90/1000 | Loss: 0.00002582
Iteration 91/1000 | Loss: 0.00002582
Iteration 92/1000 | Loss: 0.00002582
Iteration 93/1000 | Loss: 0.00002582
Iteration 94/1000 | Loss: 0.00002582
Iteration 95/1000 | Loss: 0.00002582
Iteration 96/1000 | Loss: 0.00002582
Iteration 97/1000 | Loss: 0.00002582
Iteration 98/1000 | Loss: 0.00002582
Iteration 99/1000 | Loss: 0.00002582
Iteration 100/1000 | Loss: 0.00002582
Iteration 101/1000 | Loss: 0.00002582
Iteration 102/1000 | Loss: 0.00002582
Iteration 103/1000 | Loss: 0.00002582
Iteration 104/1000 | Loss: 0.00002582
Iteration 105/1000 | Loss: 0.00002582
Iteration 106/1000 | Loss: 0.00002582
Iteration 107/1000 | Loss: 0.00002582
Iteration 108/1000 | Loss: 0.00002582
Iteration 109/1000 | Loss: 0.00002582
Iteration 110/1000 | Loss: 0.00002582
Iteration 111/1000 | Loss: 0.00002582
Iteration 112/1000 | Loss: 0.00002582
Iteration 113/1000 | Loss: 0.00002582
Iteration 114/1000 | Loss: 0.00002582
Iteration 115/1000 | Loss: 0.00002582
Iteration 116/1000 | Loss: 0.00002582
Iteration 117/1000 | Loss: 0.00002582
Iteration 118/1000 | Loss: 0.00002582
Iteration 119/1000 | Loss: 0.00002582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.5816616471274756e-05, 2.5816616471274756e-05, 2.5816616471274756e-05, 2.5816616471274756e-05, 2.5816616471274756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5816616471274756e-05

Optimization complete. Final v2v error: 4.34658670425415 mm

Highest mean error: 4.61402702331543 mm for frame 24

Lowest mean error: 4.0567803382873535 mm for frame 45

Saving results

Total time: 29.528222799301147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00720192
Iteration 2/25 | Loss: 0.00195179
Iteration 3/25 | Loss: 0.00156428
Iteration 4/25 | Loss: 0.00151332
Iteration 5/25 | Loss: 0.00147338
Iteration 6/25 | Loss: 0.00145178
Iteration 7/25 | Loss: 0.00145150
Iteration 8/25 | Loss: 0.00144285
Iteration 9/25 | Loss: 0.00143701
Iteration 10/25 | Loss: 0.00143270
Iteration 11/25 | Loss: 0.00143104
Iteration 12/25 | Loss: 0.00143018
Iteration 13/25 | Loss: 0.00143001
Iteration 14/25 | Loss: 0.00142992
Iteration 15/25 | Loss: 0.00142984
Iteration 16/25 | Loss: 0.00142953
Iteration 17/25 | Loss: 0.00143266
Iteration 18/25 | Loss: 0.00143056
Iteration 19/25 | Loss: 0.00142977
Iteration 20/25 | Loss: 0.00142967
Iteration 21/25 | Loss: 0.00142965
Iteration 22/25 | Loss: 0.00142965
Iteration 23/25 | Loss: 0.00142965
Iteration 24/25 | Loss: 0.00142965
Iteration 25/25 | Loss: 0.00142965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14709234
Iteration 2/25 | Loss: 0.00149382
Iteration 3/25 | Loss: 0.00149378
Iteration 4/25 | Loss: 0.00149378
Iteration 5/25 | Loss: 0.00149378
Iteration 6/25 | Loss: 0.00149378
Iteration 7/25 | Loss: 0.00149378
Iteration 8/25 | Loss: 0.00149378
Iteration 9/25 | Loss: 0.00149378
Iteration 10/25 | Loss: 0.00149378
Iteration 11/25 | Loss: 0.00149378
Iteration 12/25 | Loss: 0.00149378
Iteration 13/25 | Loss: 0.00149378
Iteration 14/25 | Loss: 0.00149378
Iteration 15/25 | Loss: 0.00149378
Iteration 16/25 | Loss: 0.00149378
Iteration 17/25 | Loss: 0.00149378
Iteration 18/25 | Loss: 0.00149378
Iteration 19/25 | Loss: 0.00149378
Iteration 20/25 | Loss: 0.00149378
Iteration 21/25 | Loss: 0.00149378
Iteration 22/25 | Loss: 0.00149378
Iteration 23/25 | Loss: 0.00149378
Iteration 24/25 | Loss: 0.00149378
Iteration 25/25 | Loss: 0.00149378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149378
Iteration 2/1000 | Loss: 0.00007133
Iteration 3/1000 | Loss: 0.00006533
Iteration 4/1000 | Loss: 0.00005895
Iteration 5/1000 | Loss: 0.00004173
Iteration 6/1000 | Loss: 0.00003804
Iteration 7/1000 | Loss: 0.00003557
Iteration 8/1000 | Loss: 0.00003596
Iteration 9/1000 | Loss: 0.00003694
Iteration 10/1000 | Loss: 0.00003445
Iteration 11/1000 | Loss: 0.00003487
Iteration 12/1000 | Loss: 0.00003308
Iteration 13/1000 | Loss: 0.00003250
Iteration 14/1000 | Loss: 0.00003204
Iteration 15/1000 | Loss: 0.00021559
Iteration 16/1000 | Loss: 0.00009786
Iteration 17/1000 | Loss: 0.00005099
Iteration 18/1000 | Loss: 0.00004778
Iteration 19/1000 | Loss: 0.00003412
Iteration 20/1000 | Loss: 0.00003437
Iteration 21/1000 | Loss: 0.00003053
Iteration 22/1000 | Loss: 0.00004624
Iteration 23/1000 | Loss: 0.00003176
Iteration 24/1000 | Loss: 0.00003593
Iteration 25/1000 | Loss: 0.00002817
Iteration 26/1000 | Loss: 0.00002780
Iteration 27/1000 | Loss: 0.00002747
Iteration 28/1000 | Loss: 0.00002712
Iteration 29/1000 | Loss: 0.00002684
Iteration 30/1000 | Loss: 0.00004310
Iteration 31/1000 | Loss: 0.00003790
Iteration 32/1000 | Loss: 0.00002789
Iteration 33/1000 | Loss: 0.00002665
Iteration 34/1000 | Loss: 0.00002649
Iteration 35/1000 | Loss: 0.00003876
Iteration 36/1000 | Loss: 0.00002856
Iteration 37/1000 | Loss: 0.00003940
Iteration 38/1000 | Loss: 0.00004824
Iteration 39/1000 | Loss: 0.00004076
Iteration 40/1000 | Loss: 0.00003697
Iteration 41/1000 | Loss: 0.00004028
Iteration 42/1000 | Loss: 0.00004982
Iteration 43/1000 | Loss: 0.00006048
Iteration 44/1000 | Loss: 0.00005692
Iteration 45/1000 | Loss: 0.00005442
Iteration 46/1000 | Loss: 0.00007607
Iteration 47/1000 | Loss: 0.00006745
Iteration 48/1000 | Loss: 0.00004281
Iteration 49/1000 | Loss: 0.00003680
Iteration 50/1000 | Loss: 0.00002976
Iteration 51/1000 | Loss: 0.00002751
Iteration 52/1000 | Loss: 0.00002701
Iteration 53/1000 | Loss: 0.00002666
Iteration 54/1000 | Loss: 0.00002646
Iteration 55/1000 | Loss: 0.00002646
Iteration 56/1000 | Loss: 0.00002643
Iteration 57/1000 | Loss: 0.00002643
Iteration 58/1000 | Loss: 0.00002643
Iteration 59/1000 | Loss: 0.00002643
Iteration 60/1000 | Loss: 0.00002641
Iteration 61/1000 | Loss: 0.00002641
Iteration 62/1000 | Loss: 0.00003801
Iteration 63/1000 | Loss: 0.00003103
Iteration 64/1000 | Loss: 0.00003701
Iteration 65/1000 | Loss: 0.00002792
Iteration 66/1000 | Loss: 0.00003894
Iteration 67/1000 | Loss: 0.00002912
Iteration 68/1000 | Loss: 0.00003734
Iteration 69/1000 | Loss: 0.00002937
Iteration 70/1000 | Loss: 0.00005282
Iteration 71/1000 | Loss: 0.00003619
Iteration 72/1000 | Loss: 0.00004158
Iteration 73/1000 | Loss: 0.00003378
Iteration 74/1000 | Loss: 0.00005202
Iteration 75/1000 | Loss: 0.00003145
Iteration 76/1000 | Loss: 0.00004625
Iteration 77/1000 | Loss: 0.00004227
Iteration 78/1000 | Loss: 0.00003266
Iteration 79/1000 | Loss: 0.00002892
Iteration 80/1000 | Loss: 0.00004579
Iteration 81/1000 | Loss: 0.00004823
Iteration 82/1000 | Loss: 0.00003636
Iteration 83/1000 | Loss: 0.00004354
Iteration 84/1000 | Loss: 0.00002624
Iteration 85/1000 | Loss: 0.00003729
Iteration 86/1000 | Loss: 0.00003887
Iteration 87/1000 | Loss: 0.00004904
Iteration 88/1000 | Loss: 0.00003053
Iteration 89/1000 | Loss: 0.00003877
Iteration 90/1000 | Loss: 0.00005065
Iteration 91/1000 | Loss: 0.00003159
Iteration 92/1000 | Loss: 0.00002780
Iteration 93/1000 | Loss: 0.00005218
Iteration 94/1000 | Loss: 0.00005149
Iteration 95/1000 | Loss: 0.00003056
Iteration 96/1000 | Loss: 0.00004576
Iteration 97/1000 | Loss: 0.00007358
Iteration 98/1000 | Loss: 0.00005890
Iteration 99/1000 | Loss: 0.00002875
Iteration 100/1000 | Loss: 0.00003705
Iteration 101/1000 | Loss: 0.00002703
Iteration 102/1000 | Loss: 0.00003204
Iteration 103/1000 | Loss: 0.00004450
Iteration 104/1000 | Loss: 0.00002881
Iteration 105/1000 | Loss: 0.00004987
Iteration 106/1000 | Loss: 0.00004459
Iteration 107/1000 | Loss: 0.00004521
Iteration 108/1000 | Loss: 0.00003180
Iteration 109/1000 | Loss: 0.00004472
Iteration 110/1000 | Loss: 0.00003391
Iteration 111/1000 | Loss: 0.00004942
Iteration 112/1000 | Loss: 0.00004549
Iteration 113/1000 | Loss: 0.00004822
Iteration 114/1000 | Loss: 0.00003354
Iteration 115/1000 | Loss: 0.00003688
Iteration 116/1000 | Loss: 0.00002648
Iteration 117/1000 | Loss: 0.00003972
Iteration 118/1000 | Loss: 0.00004983
Iteration 119/1000 | Loss: 0.00003335
Iteration 120/1000 | Loss: 0.00004880
Iteration 121/1000 | Loss: 0.00004073
Iteration 122/1000 | Loss: 0.00003910
Iteration 123/1000 | Loss: 0.00005056
Iteration 124/1000 | Loss: 0.00003934
Iteration 125/1000 | Loss: 0.00004076
Iteration 126/1000 | Loss: 0.00003474
Iteration 127/1000 | Loss: 0.00002990
Iteration 128/1000 | Loss: 0.00002716
Iteration 129/1000 | Loss: 0.00005125
Iteration 130/1000 | Loss: 0.00003109
Iteration 131/1000 | Loss: 0.00005031
Iteration 132/1000 | Loss: 0.00002788
Iteration 133/1000 | Loss: 0.00003473
Iteration 134/1000 | Loss: 0.00007189
Iteration 135/1000 | Loss: 0.00005311
Iteration 136/1000 | Loss: 0.00003957
Iteration 137/1000 | Loss: 0.00004344
Iteration 138/1000 | Loss: 0.00002737
Iteration 139/1000 | Loss: 0.00003574
Iteration 140/1000 | Loss: 0.00003762
Iteration 141/1000 | Loss: 0.00004340
Iteration 142/1000 | Loss: 0.00003826
Iteration 143/1000 | Loss: 0.00004824
Iteration 144/1000 | Loss: 0.00003734
Iteration 145/1000 | Loss: 0.00004618
Iteration 146/1000 | Loss: 0.00003855
Iteration 147/1000 | Loss: 0.00004435
Iteration 148/1000 | Loss: 0.00003995
Iteration 149/1000 | Loss: 0.00005193
Iteration 150/1000 | Loss: 0.00003744
Iteration 151/1000 | Loss: 0.00005747
Iteration 152/1000 | Loss: 0.00003585
Iteration 153/1000 | Loss: 0.00005498
Iteration 154/1000 | Loss: 0.00003572
Iteration 155/1000 | Loss: 0.00005266
Iteration 156/1000 | Loss: 0.00003934
Iteration 157/1000 | Loss: 0.00005258
Iteration 158/1000 | Loss: 0.00003838
Iteration 159/1000 | Loss: 0.00006821
Iteration 160/1000 | Loss: 0.00004417
Iteration 161/1000 | Loss: 0.00005524
Iteration 162/1000 | Loss: 0.00003574
Iteration 163/1000 | Loss: 0.00003021
Iteration 164/1000 | Loss: 0.00002751
Iteration 165/1000 | Loss: 0.00004352
Iteration 166/1000 | Loss: 0.00003112
Iteration 167/1000 | Loss: 0.00003967
Iteration 168/1000 | Loss: 0.00003531
Iteration 169/1000 | Loss: 0.00004200
Iteration 170/1000 | Loss: 0.00003415
Iteration 171/1000 | Loss: 0.00004201
Iteration 172/1000 | Loss: 0.00003534
Iteration 173/1000 | Loss: 0.00004571
Iteration 174/1000 | Loss: 0.00003899
Iteration 175/1000 | Loss: 0.00003828
Iteration 176/1000 | Loss: 0.00003733
Iteration 177/1000 | Loss: 0.00003861
Iteration 178/1000 | Loss: 0.00003597
Iteration 179/1000 | Loss: 0.00003685
Iteration 180/1000 | Loss: 0.00003762
Iteration 181/1000 | Loss: 0.00003955
Iteration 182/1000 | Loss: 0.00003627
Iteration 183/1000 | Loss: 0.00003906
Iteration 184/1000 | Loss: 0.00004350
Iteration 185/1000 | Loss: 0.00005292
Iteration 186/1000 | Loss: 0.00004410
Iteration 187/1000 | Loss: 0.00005259
Iteration 188/1000 | Loss: 0.00004230
Iteration 189/1000 | Loss: 0.00003211
Iteration 190/1000 | Loss: 0.00003412
Iteration 191/1000 | Loss: 0.00005229
Iteration 192/1000 | Loss: 0.00003039
Iteration 193/1000 | Loss: 0.00003375
Iteration 194/1000 | Loss: 0.00003010
Iteration 195/1000 | Loss: 0.00002836
Iteration 196/1000 | Loss: 0.00003251
Iteration 197/1000 | Loss: 0.00004966
Iteration 198/1000 | Loss: 0.00003346
Iteration 199/1000 | Loss: 0.00004036
Iteration 200/1000 | Loss: 0.00003780
Iteration 201/1000 | Loss: 0.00003857
Iteration 202/1000 | Loss: 0.00004243
Iteration 203/1000 | Loss: 0.00003885
Iteration 204/1000 | Loss: 0.00005476
Iteration 205/1000 | Loss: 0.00004712
Iteration 206/1000 | Loss: 0.00005267
Iteration 207/1000 | Loss: 0.00004782
Iteration 208/1000 | Loss: 0.00004259
Iteration 209/1000 | Loss: 0.00004961
Iteration 210/1000 | Loss: 0.00004508
Iteration 211/1000 | Loss: 0.00004017
Iteration 212/1000 | Loss: 0.00003856
Iteration 213/1000 | Loss: 0.00005357
Iteration 214/1000 | Loss: 0.00004064
Iteration 215/1000 | Loss: 0.00004838
Iteration 216/1000 | Loss: 0.00004584
Iteration 217/1000 | Loss: 0.00003449
Iteration 218/1000 | Loss: 0.00003839
Iteration 219/1000 | Loss: 0.00003500
Iteration 220/1000 | Loss: 0.00003695
Iteration 221/1000 | Loss: 0.00003460
Iteration 222/1000 | Loss: 0.00003727
Iteration 223/1000 | Loss: 0.00003451
Iteration 224/1000 | Loss: 0.00003832
Iteration 225/1000 | Loss: 0.00003383
Iteration 226/1000 | Loss: 0.00003581
Iteration 227/1000 | Loss: 0.00003348
Iteration 228/1000 | Loss: 0.00003757
Iteration 229/1000 | Loss: 0.00003347
Iteration 230/1000 | Loss: 0.00003346
Iteration 231/1000 | Loss: 0.00003346
Iteration 232/1000 | Loss: 0.00003346
Iteration 233/1000 | Loss: 0.00003652
Iteration 234/1000 | Loss: 0.00003338
Iteration 235/1000 | Loss: 0.00003337
Iteration 236/1000 | Loss: 0.00003372
Iteration 237/1000 | Loss: 0.00003228
Iteration 238/1000 | Loss: 0.00004258
Iteration 239/1000 | Loss: 0.00003202
Iteration 240/1000 | Loss: 0.00003798
Iteration 241/1000 | Loss: 0.00003146
Iteration 242/1000 | Loss: 0.00003617
Iteration 243/1000 | Loss: 0.00003280
Iteration 244/1000 | Loss: 0.00004048
Iteration 245/1000 | Loss: 0.00003325
Iteration 246/1000 | Loss: 0.00003984
Iteration 247/1000 | Loss: 0.00003589
Iteration 248/1000 | Loss: 0.00004033
Iteration 249/1000 | Loss: 0.00003473
Iteration 250/1000 | Loss: 0.00004080
Iteration 251/1000 | Loss: 0.00003792
Iteration 252/1000 | Loss: 0.00003724
Iteration 253/1000 | Loss: 0.00003536
Iteration 254/1000 | Loss: 0.00003659
Iteration 255/1000 | Loss: 0.00003471
Iteration 256/1000 | Loss: 0.00004111
Iteration 257/1000 | Loss: 0.00003414
Iteration 258/1000 | Loss: 0.00004363
Iteration 259/1000 | Loss: 0.00003384
Iteration 260/1000 | Loss: 0.00005170
Iteration 261/1000 | Loss: 0.00003043
Iteration 262/1000 | Loss: 0.00004543
Iteration 263/1000 | Loss: 0.00003045
Iteration 264/1000 | Loss: 0.00004021
Iteration 265/1000 | Loss: 0.00003366
Iteration 266/1000 | Loss: 0.00003471
Iteration 267/1000 | Loss: 0.00004937
Iteration 268/1000 | Loss: 0.00004164
Iteration 269/1000 | Loss: 0.00003349
Iteration 270/1000 | Loss: 0.00004240
Iteration 271/1000 | Loss: 0.00002898
Iteration 272/1000 | Loss: 0.00003767
Iteration 273/1000 | Loss: 0.00002914
Iteration 274/1000 | Loss: 0.00002915
Iteration 275/1000 | Loss: 0.00004165
Iteration 276/1000 | Loss: 0.00003020
Iteration 277/1000 | Loss: 0.00003844
Iteration 278/1000 | Loss: 0.00002812
Iteration 279/1000 | Loss: 0.00003053
Iteration 280/1000 | Loss: 0.00002887
Iteration 281/1000 | Loss: 0.00002789
Iteration 282/1000 | Loss: 0.00004444
Iteration 283/1000 | Loss: 0.00004893
Iteration 284/1000 | Loss: 0.00005056
Iteration 285/1000 | Loss: 0.00004505
Iteration 286/1000 | Loss: 0.00004754
Iteration 287/1000 | Loss: 0.00004520
Iteration 288/1000 | Loss: 0.00005088
Iteration 289/1000 | Loss: 0.00004303
Iteration 290/1000 | Loss: 0.00004951
Iteration 291/1000 | Loss: 0.00004086
Iteration 292/1000 | Loss: 0.00004610
Iteration 293/1000 | Loss: 0.00003762
Iteration 294/1000 | Loss: 0.00005035
Iteration 295/1000 | Loss: 0.00003870
Iteration 296/1000 | Loss: 0.00004350
Iteration 297/1000 | Loss: 0.00004081
Iteration 298/1000 | Loss: 0.00003713
Iteration 299/1000 | Loss: 0.00006294
Iteration 300/1000 | Loss: 0.00002867
Iteration 301/1000 | Loss: 0.00002948
Iteration 302/1000 | Loss: 0.00003064
Iteration 303/1000 | Loss: 0.00002649
Iteration 304/1000 | Loss: 0.00002625
Iteration 305/1000 | Loss: 0.00002611
Iteration 306/1000 | Loss: 0.00002604
Iteration 307/1000 | Loss: 0.00002601
Iteration 308/1000 | Loss: 0.00002599
Iteration 309/1000 | Loss: 0.00002598
Iteration 310/1000 | Loss: 0.00002598
Iteration 311/1000 | Loss: 0.00002595
Iteration 312/1000 | Loss: 0.00004147
Iteration 313/1000 | Loss: 0.00003388
Iteration 314/1000 | Loss: 0.00002723
Iteration 315/1000 | Loss: 0.00002663
Iteration 316/1000 | Loss: 0.00004245
Iteration 317/1000 | Loss: 0.00003423
Iteration 318/1000 | Loss: 0.00004176
Iteration 319/1000 | Loss: 0.00003434
Iteration 320/1000 | Loss: 0.00003977
Iteration 321/1000 | Loss: 0.00004285
Iteration 322/1000 | Loss: 0.00002605
Iteration 323/1000 | Loss: 0.00002586
Iteration 324/1000 | Loss: 0.00002586
Iteration 325/1000 | Loss: 0.00002585
Iteration 326/1000 | Loss: 0.00004048
Iteration 327/1000 | Loss: 0.00004203
Iteration 328/1000 | Loss: 0.00004032
Iteration 329/1000 | Loss: 0.00002893
Iteration 330/1000 | Loss: 0.00004945
Iteration 331/1000 | Loss: 0.00002897
Iteration 332/1000 | Loss: 0.00002703
Iteration 333/1000 | Loss: 0.00003814
Iteration 334/1000 | Loss: 0.00002685
Iteration 335/1000 | Loss: 0.00002823
Iteration 336/1000 | Loss: 0.00004027
Iteration 337/1000 | Loss: 0.00003746
Iteration 338/1000 | Loss: 0.00003842
Iteration 339/1000 | Loss: 0.00004822
Iteration 340/1000 | Loss: 0.00002975
Iteration 341/1000 | Loss: 0.00002701
Iteration 342/1000 | Loss: 0.00003764
Iteration 343/1000 | Loss: 0.00002717
Iteration 344/1000 | Loss: 0.00003796
Iteration 345/1000 | Loss: 0.00004829
Iteration 346/1000 | Loss: 0.00002665
Iteration 347/1000 | Loss: 0.00003758
Iteration 348/1000 | Loss: 0.00005076
Iteration 349/1000 | Loss: 0.00003209
Iteration 350/1000 | Loss: 0.00003009
Iteration 351/1000 | Loss: 0.00003375
Iteration 352/1000 | Loss: 0.00002881
Iteration 353/1000 | Loss: 0.00003427
Iteration 354/1000 | Loss: 0.00002614
Iteration 355/1000 | Loss: 0.00004607
Iteration 356/1000 | Loss: 0.00003247
Iteration 357/1000 | Loss: 0.00002688
Iteration 358/1000 | Loss: 0.00003689
Iteration 359/1000 | Loss: 0.00005088
Iteration 360/1000 | Loss: 0.00005313
Iteration 361/1000 | Loss: 0.00003526
Iteration 362/1000 | Loss: 0.00003310
Iteration 363/1000 | Loss: 0.00004047
Iteration 364/1000 | Loss: 0.00003935
Iteration 365/1000 | Loss: 0.00006294
Iteration 366/1000 | Loss: 0.00007749
Iteration 367/1000 | Loss: 0.00006425
Iteration 368/1000 | Loss: 0.00003081
Iteration 369/1000 | Loss: 0.00005180
Iteration 370/1000 | Loss: 0.00003569
Iteration 371/1000 | Loss: 0.00007429
Iteration 372/1000 | Loss: 0.00004291
Iteration 373/1000 | Loss: 0.00005435
Iteration 374/1000 | Loss: 0.00004172
Iteration 375/1000 | Loss: 0.00003083
Iteration 376/1000 | Loss: 0.00002873
Iteration 377/1000 | Loss: 0.00002753
Iteration 378/1000 | Loss: 0.00002697
Iteration 379/1000 | Loss: 0.00004416
Iteration 380/1000 | Loss: 0.00003464
Iteration 381/1000 | Loss: 0.00003835
Iteration 382/1000 | Loss: 0.00004314
Iteration 383/1000 | Loss: 0.00003694
Iteration 384/1000 | Loss: 0.00004312
Iteration 385/1000 | Loss: 0.00003470
Iteration 386/1000 | Loss: 0.00004346
Iteration 387/1000 | Loss: 0.00003810
Iteration 388/1000 | Loss: 0.00002635
Iteration 389/1000 | Loss: 0.00002622
Iteration 390/1000 | Loss: 0.00002600
Iteration 391/1000 | Loss: 0.00002591
Iteration 392/1000 | Loss: 0.00002590
Iteration 393/1000 | Loss: 0.00002589
Iteration 394/1000 | Loss: 0.00002589
Iteration 395/1000 | Loss: 0.00002588
Iteration 396/1000 | Loss: 0.00002587
Iteration 397/1000 | Loss: 0.00002587
Iteration 398/1000 | Loss: 0.00002587
Iteration 399/1000 | Loss: 0.00002587
Iteration 400/1000 | Loss: 0.00002587
Iteration 401/1000 | Loss: 0.00002587
Iteration 402/1000 | Loss: 0.00002587
Iteration 403/1000 | Loss: 0.00002587
Iteration 404/1000 | Loss: 0.00002586
Iteration 405/1000 | Loss: 0.00002586
Iteration 406/1000 | Loss: 0.00002586
Iteration 407/1000 | Loss: 0.00002586
Iteration 408/1000 | Loss: 0.00002586
Iteration 409/1000 | Loss: 0.00002586
Iteration 410/1000 | Loss: 0.00002586
Iteration 411/1000 | Loss: 0.00002586
Iteration 412/1000 | Loss: 0.00002586
Iteration 413/1000 | Loss: 0.00002586
Iteration 414/1000 | Loss: 0.00002586
Iteration 415/1000 | Loss: 0.00002585
Iteration 416/1000 | Loss: 0.00002585
Iteration 417/1000 | Loss: 0.00002585
Iteration 418/1000 | Loss: 0.00002585
Iteration 419/1000 | Loss: 0.00002585
Iteration 420/1000 | Loss: 0.00002585
Iteration 421/1000 | Loss: 0.00002584
Iteration 422/1000 | Loss: 0.00002584
Iteration 423/1000 | Loss: 0.00002584
Iteration 424/1000 | Loss: 0.00002584
Iteration 425/1000 | Loss: 0.00002584
Iteration 426/1000 | Loss: 0.00002584
Iteration 427/1000 | Loss: 0.00002584
Iteration 428/1000 | Loss: 0.00002584
Iteration 429/1000 | Loss: 0.00002584
Iteration 430/1000 | Loss: 0.00002584
Iteration 431/1000 | Loss: 0.00002584
Iteration 432/1000 | Loss: 0.00002584
Iteration 433/1000 | Loss: 0.00002584
Iteration 434/1000 | Loss: 0.00002584
Iteration 435/1000 | Loss: 0.00002584
Iteration 436/1000 | Loss: 0.00002583
Iteration 437/1000 | Loss: 0.00002583
Iteration 438/1000 | Loss: 0.00002583
Iteration 439/1000 | Loss: 0.00002582
Iteration 440/1000 | Loss: 0.00002582
Iteration 441/1000 | Loss: 0.00002582
Iteration 442/1000 | Loss: 0.00002582
Iteration 443/1000 | Loss: 0.00002582
Iteration 444/1000 | Loss: 0.00002582
Iteration 445/1000 | Loss: 0.00002582
Iteration 446/1000 | Loss: 0.00002581
Iteration 447/1000 | Loss: 0.00002581
Iteration 448/1000 | Loss: 0.00002581
Iteration 449/1000 | Loss: 0.00002581
Iteration 450/1000 | Loss: 0.00002581
Iteration 451/1000 | Loss: 0.00002581
Iteration 452/1000 | Loss: 0.00002581
Iteration 453/1000 | Loss: 0.00002581
Iteration 454/1000 | Loss: 0.00002581
Iteration 455/1000 | Loss: 0.00002581
Iteration 456/1000 | Loss: 0.00002581
Iteration 457/1000 | Loss: 0.00002581
Iteration 458/1000 | Loss: 0.00002581
Iteration 459/1000 | Loss: 0.00002581
Iteration 460/1000 | Loss: 0.00002581
Iteration 461/1000 | Loss: 0.00002580
Iteration 462/1000 | Loss: 0.00002580
Iteration 463/1000 | Loss: 0.00002580
Iteration 464/1000 | Loss: 0.00002580
Iteration 465/1000 | Loss: 0.00002580
Iteration 466/1000 | Loss: 0.00002580
Iteration 467/1000 | Loss: 0.00002580
Iteration 468/1000 | Loss: 0.00002580
Iteration 469/1000 | Loss: 0.00002580
Iteration 470/1000 | Loss: 0.00002580
Iteration 471/1000 | Loss: 0.00002579
Iteration 472/1000 | Loss: 0.00002579
Iteration 473/1000 | Loss: 0.00002579
Iteration 474/1000 | Loss: 0.00002579
Iteration 475/1000 | Loss: 0.00002579
Iteration 476/1000 | Loss: 0.00002579
Iteration 477/1000 | Loss: 0.00002579
Iteration 478/1000 | Loss: 0.00002578
Iteration 479/1000 | Loss: 0.00002578
Iteration 480/1000 | Loss: 0.00002578
Iteration 481/1000 | Loss: 0.00002578
Iteration 482/1000 | Loss: 0.00002578
Iteration 483/1000 | Loss: 0.00002578
Iteration 484/1000 | Loss: 0.00002578
Iteration 485/1000 | Loss: 0.00002578
Iteration 486/1000 | Loss: 0.00002578
Iteration 487/1000 | Loss: 0.00002578
Iteration 488/1000 | Loss: 0.00002577
Iteration 489/1000 | Loss: 0.00002577
Iteration 490/1000 | Loss: 0.00002577
Iteration 491/1000 | Loss: 0.00002577
Iteration 492/1000 | Loss: 0.00002577
Iteration 493/1000 | Loss: 0.00002577
Iteration 494/1000 | Loss: 0.00002577
Iteration 495/1000 | Loss: 0.00002577
Iteration 496/1000 | Loss: 0.00002577
Iteration 497/1000 | Loss: 0.00002577
Iteration 498/1000 | Loss: 0.00002577
Iteration 499/1000 | Loss: 0.00002577
Iteration 500/1000 | Loss: 0.00002577
Iteration 501/1000 | Loss: 0.00002577
Iteration 502/1000 | Loss: 0.00002577
Iteration 503/1000 | Loss: 0.00002577
Iteration 504/1000 | Loss: 0.00002577
Iteration 505/1000 | Loss: 0.00002577
Iteration 506/1000 | Loss: 0.00002577
Iteration 507/1000 | Loss: 0.00002577
Iteration 508/1000 | Loss: 0.00002576
Iteration 509/1000 | Loss: 0.00002576
Iteration 510/1000 | Loss: 0.00002576
Iteration 511/1000 | Loss: 0.00002576
Iteration 512/1000 | Loss: 0.00002576
Iteration 513/1000 | Loss: 0.00002576
Iteration 514/1000 | Loss: 0.00002576
Iteration 515/1000 | Loss: 0.00002576
Iteration 516/1000 | Loss: 0.00002576
Iteration 517/1000 | Loss: 0.00002576
Iteration 518/1000 | Loss: 0.00002576
Iteration 519/1000 | Loss: 0.00002576
Iteration 520/1000 | Loss: 0.00002576
Iteration 521/1000 | Loss: 0.00002576
Iteration 522/1000 | Loss: 0.00002576
Iteration 523/1000 | Loss: 0.00002576
Iteration 524/1000 | Loss: 0.00002576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 524. Stopping optimization.
Last 5 losses: [2.576286715338938e-05, 2.576286715338938e-05, 2.576286715338938e-05, 2.576286715338938e-05, 2.576286715338938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.576286715338938e-05

Optimization complete. Final v2v error: 4.213634014129639 mm

Highest mean error: 10.498659133911133 mm for frame 23

Lowest mean error: 3.4440271854400635 mm for frame 1

Saving results

Total time: 571.6987764835358
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039122
Iteration 2/25 | Loss: 0.01039122
Iteration 3/25 | Loss: 0.01039121
Iteration 4/25 | Loss: 0.00311084
Iteration 5/25 | Loss: 0.00204117
Iteration 6/25 | Loss: 0.00181481
Iteration 7/25 | Loss: 0.00157138
Iteration 8/25 | Loss: 0.00151957
Iteration 9/25 | Loss: 0.00146072
Iteration 10/25 | Loss: 0.00148372
Iteration 11/25 | Loss: 0.00138869
Iteration 12/25 | Loss: 0.00136855
Iteration 13/25 | Loss: 0.00135052
Iteration 14/25 | Loss: 0.00134865
Iteration 15/25 | Loss: 0.00134534
Iteration 16/25 | Loss: 0.00133881
Iteration 17/25 | Loss: 0.00134226
Iteration 18/25 | Loss: 0.00133727
Iteration 19/25 | Loss: 0.00133762
Iteration 20/25 | Loss: 0.00133721
Iteration 21/25 | Loss: 0.00133786
Iteration 22/25 | Loss: 0.00133758
Iteration 23/25 | Loss: 0.00133803
Iteration 24/25 | Loss: 0.00133757
Iteration 25/25 | Loss: 0.00133638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.78193593
Iteration 2/25 | Loss: 0.00296154
Iteration 3/25 | Loss: 0.00284727
Iteration 4/25 | Loss: 0.00284726
Iteration 5/25 | Loss: 0.00284726
Iteration 6/25 | Loss: 0.00284726
Iteration 7/25 | Loss: 0.00284726
Iteration 8/25 | Loss: 0.00284726
Iteration 9/25 | Loss: 0.00284726
Iteration 10/25 | Loss: 0.00284726
Iteration 11/25 | Loss: 0.00284726
Iteration 12/25 | Loss: 0.00284726
Iteration 13/25 | Loss: 0.00284726
Iteration 14/25 | Loss: 0.00284726
Iteration 15/25 | Loss: 0.00284726
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0028472617268562317, 0.0028472617268562317, 0.0028472617268562317, 0.0028472617268562317, 0.0028472617268562317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028472617268562317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00284726
Iteration 2/1000 | Loss: 0.00166022
Iteration 3/1000 | Loss: 0.00008913
Iteration 4/1000 | Loss: 0.00007122
Iteration 5/1000 | Loss: 0.00089912
Iteration 6/1000 | Loss: 0.00006400
Iteration 7/1000 | Loss: 0.00009369
Iteration 8/1000 | Loss: 0.00064462
Iteration 9/1000 | Loss: 0.00060334
Iteration 10/1000 | Loss: 0.00010669
Iteration 11/1000 | Loss: 0.00067000
Iteration 12/1000 | Loss: 0.00005368
Iteration 13/1000 | Loss: 0.00007053
Iteration 14/1000 | Loss: 0.00005395
Iteration 15/1000 | Loss: 0.00005066
Iteration 16/1000 | Loss: 0.00005061
Iteration 17/1000 | Loss: 0.00004945
Iteration 18/1000 | Loss: 0.00122552
Iteration 19/1000 | Loss: 0.00093190
Iteration 20/1000 | Loss: 0.00113955
Iteration 21/1000 | Loss: 0.00009640
Iteration 22/1000 | Loss: 0.00005351
Iteration 23/1000 | Loss: 0.00004912
Iteration 24/1000 | Loss: 0.00004835
Iteration 25/1000 | Loss: 0.00097945
Iteration 26/1000 | Loss: 0.00005707
Iteration 27/1000 | Loss: 0.00006075
Iteration 28/1000 | Loss: 0.00005307
Iteration 29/1000 | Loss: 0.00005058
Iteration 30/1000 | Loss: 0.00005008
Iteration 31/1000 | Loss: 0.00004972
Iteration 32/1000 | Loss: 0.00006797
Iteration 33/1000 | Loss: 0.00005053
Iteration 34/1000 | Loss: 0.00004926
Iteration 35/1000 | Loss: 0.00004912
Iteration 36/1000 | Loss: 0.00004912
Iteration 37/1000 | Loss: 0.00004912
Iteration 38/1000 | Loss: 0.00004912
Iteration 39/1000 | Loss: 0.00004911
Iteration 40/1000 | Loss: 0.00004911
Iteration 41/1000 | Loss: 0.00004910
Iteration 42/1000 | Loss: 0.00004910
Iteration 43/1000 | Loss: 0.00004910
Iteration 44/1000 | Loss: 0.00004909
Iteration 45/1000 | Loss: 0.00004908
Iteration 46/1000 | Loss: 0.00004903
Iteration 47/1000 | Loss: 0.00004895
Iteration 48/1000 | Loss: 0.00004895
Iteration 49/1000 | Loss: 0.00004894
Iteration 50/1000 | Loss: 0.00004894
Iteration 51/1000 | Loss: 0.00004892
Iteration 52/1000 | Loss: 0.00004891
Iteration 53/1000 | Loss: 0.00004891
Iteration 54/1000 | Loss: 0.00004890
Iteration 55/1000 | Loss: 0.00004890
Iteration 56/1000 | Loss: 0.00004890
Iteration 57/1000 | Loss: 0.00004890
Iteration 58/1000 | Loss: 0.00004890
Iteration 59/1000 | Loss: 0.00004889
Iteration 60/1000 | Loss: 0.00007925
Iteration 61/1000 | Loss: 0.00004887
Iteration 62/1000 | Loss: 0.00004883
Iteration 63/1000 | Loss: 0.00004882
Iteration 64/1000 | Loss: 0.00004881
Iteration 65/1000 | Loss: 0.00004881
Iteration 66/1000 | Loss: 0.00004880
Iteration 67/1000 | Loss: 0.00004874
Iteration 68/1000 | Loss: 0.00004874
Iteration 69/1000 | Loss: 0.00004874
Iteration 70/1000 | Loss: 0.00004873
Iteration 71/1000 | Loss: 0.00004873
Iteration 72/1000 | Loss: 0.00004873
Iteration 73/1000 | Loss: 0.00030501
Iteration 74/1000 | Loss: 0.00191158
Iteration 75/1000 | Loss: 0.00143013
Iteration 76/1000 | Loss: 0.00190028
Iteration 77/1000 | Loss: 0.00035070
Iteration 78/1000 | Loss: 0.00027444
Iteration 79/1000 | Loss: 0.00043356
Iteration 80/1000 | Loss: 0.00067143
Iteration 81/1000 | Loss: 0.00009328
Iteration 82/1000 | Loss: 0.00006066
Iteration 83/1000 | Loss: 0.00016114
Iteration 84/1000 | Loss: 0.00006933
Iteration 85/1000 | Loss: 0.00005079
Iteration 86/1000 | Loss: 0.00082925
Iteration 87/1000 | Loss: 0.00020180
Iteration 88/1000 | Loss: 0.00102677
Iteration 89/1000 | Loss: 0.00012956
Iteration 90/1000 | Loss: 0.00006424
Iteration 91/1000 | Loss: 0.00005438
Iteration 92/1000 | Loss: 0.00004858
Iteration 93/1000 | Loss: 0.00008963
Iteration 94/1000 | Loss: 0.00005657
Iteration 95/1000 | Loss: 0.00004381
Iteration 96/1000 | Loss: 0.00072331
Iteration 97/1000 | Loss: 0.00005485
Iteration 98/1000 | Loss: 0.00010898
Iteration 99/1000 | Loss: 0.00004682
Iteration 100/1000 | Loss: 0.00003960
Iteration 101/1000 | Loss: 0.00006215
Iteration 102/1000 | Loss: 0.00008691
Iteration 103/1000 | Loss: 0.00010086
Iteration 104/1000 | Loss: 0.00005907
Iteration 105/1000 | Loss: 0.00004242
Iteration 106/1000 | Loss: 0.00003932
Iteration 107/1000 | Loss: 0.00003862
Iteration 108/1000 | Loss: 0.00003824
Iteration 109/1000 | Loss: 0.00003795
Iteration 110/1000 | Loss: 0.00003771
Iteration 111/1000 | Loss: 0.00006344
Iteration 112/1000 | Loss: 0.00003781
Iteration 113/1000 | Loss: 0.00003744
Iteration 114/1000 | Loss: 0.00003741
Iteration 115/1000 | Loss: 0.00003741
Iteration 116/1000 | Loss: 0.00003736
Iteration 117/1000 | Loss: 0.00003735
Iteration 118/1000 | Loss: 0.00006664
Iteration 119/1000 | Loss: 0.00011052
Iteration 120/1000 | Loss: 0.00005472
Iteration 121/1000 | Loss: 0.00003725
Iteration 122/1000 | Loss: 0.00008026
Iteration 123/1000 | Loss: 0.00031950
Iteration 124/1000 | Loss: 0.00004916
Iteration 125/1000 | Loss: 0.00003908
Iteration 126/1000 | Loss: 0.00003736
Iteration 127/1000 | Loss: 0.00006931
Iteration 128/1000 | Loss: 0.00004567
Iteration 129/1000 | Loss: 0.00003711
Iteration 130/1000 | Loss: 0.00003711
Iteration 131/1000 | Loss: 0.00003710
Iteration 132/1000 | Loss: 0.00003710
Iteration 133/1000 | Loss: 0.00003710
Iteration 134/1000 | Loss: 0.00003791
Iteration 135/1000 | Loss: 0.00003770
Iteration 136/1000 | Loss: 0.00005764
Iteration 137/1000 | Loss: 0.00004119
Iteration 138/1000 | Loss: 0.00003787
Iteration 139/1000 | Loss: 0.00003711
Iteration 140/1000 | Loss: 0.00003711
Iteration 141/1000 | Loss: 0.00003711
Iteration 142/1000 | Loss: 0.00003711
Iteration 143/1000 | Loss: 0.00003711
Iteration 144/1000 | Loss: 0.00003711
Iteration 145/1000 | Loss: 0.00003711
Iteration 146/1000 | Loss: 0.00003711
Iteration 147/1000 | Loss: 0.00003711
Iteration 148/1000 | Loss: 0.00003711
Iteration 149/1000 | Loss: 0.00003710
Iteration 150/1000 | Loss: 0.00003710
Iteration 151/1000 | Loss: 0.00003709
Iteration 152/1000 | Loss: 0.00003708
Iteration 153/1000 | Loss: 0.00003708
Iteration 154/1000 | Loss: 0.00003708
Iteration 155/1000 | Loss: 0.00003708
Iteration 156/1000 | Loss: 0.00003707
Iteration 157/1000 | Loss: 0.00003707
Iteration 158/1000 | Loss: 0.00003707
Iteration 159/1000 | Loss: 0.00003707
Iteration 160/1000 | Loss: 0.00003707
Iteration 161/1000 | Loss: 0.00003707
Iteration 162/1000 | Loss: 0.00003707
Iteration 163/1000 | Loss: 0.00003707
Iteration 164/1000 | Loss: 0.00003707
Iteration 165/1000 | Loss: 0.00003707
Iteration 166/1000 | Loss: 0.00003707
Iteration 167/1000 | Loss: 0.00003707
Iteration 168/1000 | Loss: 0.00003707
Iteration 169/1000 | Loss: 0.00003707
Iteration 170/1000 | Loss: 0.00003707
Iteration 171/1000 | Loss: 0.00003707
Iteration 172/1000 | Loss: 0.00003707
Iteration 173/1000 | Loss: 0.00003707
Iteration 174/1000 | Loss: 0.00003707
Iteration 175/1000 | Loss: 0.00003707
Iteration 176/1000 | Loss: 0.00003707
Iteration 177/1000 | Loss: 0.00003707
Iteration 178/1000 | Loss: 0.00003707
Iteration 179/1000 | Loss: 0.00003707
Iteration 180/1000 | Loss: 0.00003707
Iteration 181/1000 | Loss: 0.00003707
Iteration 182/1000 | Loss: 0.00003707
Iteration 183/1000 | Loss: 0.00003707
Iteration 184/1000 | Loss: 0.00003707
Iteration 185/1000 | Loss: 0.00003707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [3.707052019308321e-05, 3.707052019308321e-05, 3.707052019308321e-05, 3.707052019308321e-05, 3.707052019308321e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.707052019308321e-05

Optimization complete. Final v2v error: 4.551677703857422 mm

Highest mean error: 10.893940925598145 mm for frame 153

Lowest mean error: 3.705824851989746 mm for frame 83

Saving results

Total time: 181.58938336372375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869543
Iteration 2/25 | Loss: 0.00183021
Iteration 3/25 | Loss: 0.00163030
Iteration 4/25 | Loss: 0.00160286
Iteration 5/25 | Loss: 0.00159829
Iteration 6/25 | Loss: 0.00159755
Iteration 7/25 | Loss: 0.00159755
Iteration 8/25 | Loss: 0.00159755
Iteration 9/25 | Loss: 0.00159755
Iteration 10/25 | Loss: 0.00159755
Iteration 11/25 | Loss: 0.00159755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015975452261045575, 0.0015975452261045575, 0.0015975452261045575, 0.0015975452261045575, 0.0015975452261045575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015975452261045575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25126648
Iteration 2/25 | Loss: 0.00178297
Iteration 3/25 | Loss: 0.00178296
Iteration 4/25 | Loss: 0.00178296
Iteration 5/25 | Loss: 0.00178296
Iteration 6/25 | Loss: 0.00178296
Iteration 7/25 | Loss: 0.00178296
Iteration 8/25 | Loss: 0.00178296
Iteration 9/25 | Loss: 0.00178296
Iteration 10/25 | Loss: 0.00178296
Iteration 11/25 | Loss: 0.00178296
Iteration 12/25 | Loss: 0.00178296
Iteration 13/25 | Loss: 0.00178296
Iteration 14/25 | Loss: 0.00178296
Iteration 15/25 | Loss: 0.00178296
Iteration 16/25 | Loss: 0.00178296
Iteration 17/25 | Loss: 0.00178296
Iteration 18/25 | Loss: 0.00178296
Iteration 19/25 | Loss: 0.00178296
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0017829594435170293, 0.0017829594435170293, 0.0017829594435170293, 0.0017829594435170293, 0.0017829594435170293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017829594435170293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178296
Iteration 2/1000 | Loss: 0.00004990
Iteration 3/1000 | Loss: 0.00003761
Iteration 4/1000 | Loss: 0.00003166
Iteration 5/1000 | Loss: 0.00002896
Iteration 6/1000 | Loss: 0.00002746
Iteration 7/1000 | Loss: 0.00002670
Iteration 8/1000 | Loss: 0.00002624
Iteration 9/1000 | Loss: 0.00002598
Iteration 10/1000 | Loss: 0.00002569
Iteration 11/1000 | Loss: 0.00002564
Iteration 12/1000 | Loss: 0.00002551
Iteration 13/1000 | Loss: 0.00002546
Iteration 14/1000 | Loss: 0.00002544
Iteration 15/1000 | Loss: 0.00002538
Iteration 16/1000 | Loss: 0.00002529
Iteration 17/1000 | Loss: 0.00002528
Iteration 18/1000 | Loss: 0.00002527
Iteration 19/1000 | Loss: 0.00002527
Iteration 20/1000 | Loss: 0.00002527
Iteration 21/1000 | Loss: 0.00002527
Iteration 22/1000 | Loss: 0.00002526
Iteration 23/1000 | Loss: 0.00002524
Iteration 24/1000 | Loss: 0.00002524
Iteration 25/1000 | Loss: 0.00002524
Iteration 26/1000 | Loss: 0.00002524
Iteration 27/1000 | Loss: 0.00002524
Iteration 28/1000 | Loss: 0.00002524
Iteration 29/1000 | Loss: 0.00002523
Iteration 30/1000 | Loss: 0.00002523
Iteration 31/1000 | Loss: 0.00002523
Iteration 32/1000 | Loss: 0.00002523
Iteration 33/1000 | Loss: 0.00002523
Iteration 34/1000 | Loss: 0.00002523
Iteration 35/1000 | Loss: 0.00002522
Iteration 36/1000 | Loss: 0.00002519
Iteration 37/1000 | Loss: 0.00002519
Iteration 38/1000 | Loss: 0.00002519
Iteration 39/1000 | Loss: 0.00002519
Iteration 40/1000 | Loss: 0.00002519
Iteration 41/1000 | Loss: 0.00002519
Iteration 42/1000 | Loss: 0.00002519
Iteration 43/1000 | Loss: 0.00002518
Iteration 44/1000 | Loss: 0.00002518
Iteration 45/1000 | Loss: 0.00002518
Iteration 46/1000 | Loss: 0.00002515
Iteration 47/1000 | Loss: 0.00002515
Iteration 48/1000 | Loss: 0.00002514
Iteration 49/1000 | Loss: 0.00002514
Iteration 50/1000 | Loss: 0.00002514
Iteration 51/1000 | Loss: 0.00002514
Iteration 52/1000 | Loss: 0.00002514
Iteration 53/1000 | Loss: 0.00002514
Iteration 54/1000 | Loss: 0.00002514
Iteration 55/1000 | Loss: 0.00002514
Iteration 56/1000 | Loss: 0.00002513
Iteration 57/1000 | Loss: 0.00002513
Iteration 58/1000 | Loss: 0.00002512
Iteration 59/1000 | Loss: 0.00002512
Iteration 60/1000 | Loss: 0.00002512
Iteration 61/1000 | Loss: 0.00002512
Iteration 62/1000 | Loss: 0.00002512
Iteration 63/1000 | Loss: 0.00002512
Iteration 64/1000 | Loss: 0.00002512
Iteration 65/1000 | Loss: 0.00002512
Iteration 66/1000 | Loss: 0.00002512
Iteration 67/1000 | Loss: 0.00002512
Iteration 68/1000 | Loss: 0.00002512
Iteration 69/1000 | Loss: 0.00002512
Iteration 70/1000 | Loss: 0.00002512
Iteration 71/1000 | Loss: 0.00002512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [2.5115245080087334e-05, 2.5115245080087334e-05, 2.5115245080087334e-05, 2.5115245080087334e-05, 2.5115245080087334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5115245080087334e-05

Optimization complete. Final v2v error: 4.2969183921813965 mm

Highest mean error: 4.807444095611572 mm for frame 155

Lowest mean error: 3.701190233230591 mm for frame 239

Saving results

Total time: 34.1999785900116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463397
Iteration 2/25 | Loss: 0.00178559
Iteration 3/25 | Loss: 0.00157233
Iteration 4/25 | Loss: 0.00155613
Iteration 5/25 | Loss: 0.00155412
Iteration 6/25 | Loss: 0.00155362
Iteration 7/25 | Loss: 0.00155362
Iteration 8/25 | Loss: 0.00155362
Iteration 9/25 | Loss: 0.00155362
Iteration 10/25 | Loss: 0.00155362
Iteration 11/25 | Loss: 0.00155362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015536234714090824, 0.0015536234714090824, 0.0015536234714090824, 0.0015536234714090824, 0.0015536234714090824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015536234714090824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.20823812
Iteration 2/25 | Loss: 0.00162755
Iteration 3/25 | Loss: 0.00162750
Iteration 4/25 | Loss: 0.00162750
Iteration 5/25 | Loss: 0.00162749
Iteration 6/25 | Loss: 0.00162749
Iteration 7/25 | Loss: 0.00162749
Iteration 8/25 | Loss: 0.00162749
Iteration 9/25 | Loss: 0.00162749
Iteration 10/25 | Loss: 0.00162749
Iteration 11/25 | Loss: 0.00162749
Iteration 12/25 | Loss: 0.00162749
Iteration 13/25 | Loss: 0.00162749
Iteration 14/25 | Loss: 0.00162749
Iteration 15/25 | Loss: 0.00162749
Iteration 16/25 | Loss: 0.00162749
Iteration 17/25 | Loss: 0.00162749
Iteration 18/25 | Loss: 0.00162749
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016274936497211456, 0.0016274936497211456, 0.0016274936497211456, 0.0016274936497211456, 0.0016274936497211456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016274936497211456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162749
Iteration 2/1000 | Loss: 0.00004791
Iteration 3/1000 | Loss: 0.00003213
Iteration 4/1000 | Loss: 0.00002903
Iteration 5/1000 | Loss: 0.00002639
Iteration 6/1000 | Loss: 0.00002507
Iteration 7/1000 | Loss: 0.00002443
Iteration 8/1000 | Loss: 0.00002402
Iteration 9/1000 | Loss: 0.00002374
Iteration 10/1000 | Loss: 0.00002346
Iteration 11/1000 | Loss: 0.00002344
Iteration 12/1000 | Loss: 0.00002324
Iteration 13/1000 | Loss: 0.00002304
Iteration 14/1000 | Loss: 0.00002299
Iteration 15/1000 | Loss: 0.00002291
Iteration 16/1000 | Loss: 0.00002290
Iteration 17/1000 | Loss: 0.00002290
Iteration 18/1000 | Loss: 0.00002290
Iteration 19/1000 | Loss: 0.00002286
Iteration 20/1000 | Loss: 0.00002285
Iteration 21/1000 | Loss: 0.00002284
Iteration 22/1000 | Loss: 0.00002283
Iteration 23/1000 | Loss: 0.00002283
Iteration 24/1000 | Loss: 0.00002283
Iteration 25/1000 | Loss: 0.00002282
Iteration 26/1000 | Loss: 0.00002281
Iteration 27/1000 | Loss: 0.00002281
Iteration 28/1000 | Loss: 0.00002281
Iteration 29/1000 | Loss: 0.00002281
Iteration 30/1000 | Loss: 0.00002281
Iteration 31/1000 | Loss: 0.00002281
Iteration 32/1000 | Loss: 0.00002281
Iteration 33/1000 | Loss: 0.00002280
Iteration 34/1000 | Loss: 0.00002280
Iteration 35/1000 | Loss: 0.00002280
Iteration 36/1000 | Loss: 0.00002280
Iteration 37/1000 | Loss: 0.00002280
Iteration 38/1000 | Loss: 0.00002280
Iteration 39/1000 | Loss: 0.00002280
Iteration 40/1000 | Loss: 0.00002280
Iteration 41/1000 | Loss: 0.00002280
Iteration 42/1000 | Loss: 0.00002280
Iteration 43/1000 | Loss: 0.00002278
Iteration 44/1000 | Loss: 0.00002278
Iteration 45/1000 | Loss: 0.00002278
Iteration 46/1000 | Loss: 0.00002278
Iteration 47/1000 | Loss: 0.00002278
Iteration 48/1000 | Loss: 0.00002278
Iteration 49/1000 | Loss: 0.00002278
Iteration 50/1000 | Loss: 0.00002277
Iteration 51/1000 | Loss: 0.00002277
Iteration 52/1000 | Loss: 0.00002277
Iteration 53/1000 | Loss: 0.00002277
Iteration 54/1000 | Loss: 0.00002276
Iteration 55/1000 | Loss: 0.00002276
Iteration 56/1000 | Loss: 0.00002276
Iteration 57/1000 | Loss: 0.00002276
Iteration 58/1000 | Loss: 0.00002276
Iteration 59/1000 | Loss: 0.00002275
Iteration 60/1000 | Loss: 0.00002275
Iteration 61/1000 | Loss: 0.00002275
Iteration 62/1000 | Loss: 0.00002274
Iteration 63/1000 | Loss: 0.00002274
Iteration 64/1000 | Loss: 0.00002274
Iteration 65/1000 | Loss: 0.00002274
Iteration 66/1000 | Loss: 0.00002273
Iteration 67/1000 | Loss: 0.00002273
Iteration 68/1000 | Loss: 0.00002273
Iteration 69/1000 | Loss: 0.00002273
Iteration 70/1000 | Loss: 0.00002273
Iteration 71/1000 | Loss: 0.00002273
Iteration 72/1000 | Loss: 0.00002272
Iteration 73/1000 | Loss: 0.00002272
Iteration 74/1000 | Loss: 0.00002272
Iteration 75/1000 | Loss: 0.00002271
Iteration 76/1000 | Loss: 0.00002271
Iteration 77/1000 | Loss: 0.00002270
Iteration 78/1000 | Loss: 0.00002270
Iteration 79/1000 | Loss: 0.00002270
Iteration 80/1000 | Loss: 0.00002270
Iteration 81/1000 | Loss: 0.00002270
Iteration 82/1000 | Loss: 0.00002270
Iteration 83/1000 | Loss: 0.00002270
Iteration 84/1000 | Loss: 0.00002270
Iteration 85/1000 | Loss: 0.00002270
Iteration 86/1000 | Loss: 0.00002270
Iteration 87/1000 | Loss: 0.00002270
Iteration 88/1000 | Loss: 0.00002269
Iteration 89/1000 | Loss: 0.00002269
Iteration 90/1000 | Loss: 0.00002269
Iteration 91/1000 | Loss: 0.00002268
Iteration 92/1000 | Loss: 0.00002268
Iteration 93/1000 | Loss: 0.00002268
Iteration 94/1000 | Loss: 0.00002268
Iteration 95/1000 | Loss: 0.00002268
Iteration 96/1000 | Loss: 0.00002267
Iteration 97/1000 | Loss: 0.00002267
Iteration 98/1000 | Loss: 0.00002267
Iteration 99/1000 | Loss: 0.00002267
Iteration 100/1000 | Loss: 0.00002267
Iteration 101/1000 | Loss: 0.00002266
Iteration 102/1000 | Loss: 0.00002266
Iteration 103/1000 | Loss: 0.00002266
Iteration 104/1000 | Loss: 0.00002265
Iteration 105/1000 | Loss: 0.00002265
Iteration 106/1000 | Loss: 0.00002265
Iteration 107/1000 | Loss: 0.00002265
Iteration 108/1000 | Loss: 0.00002265
Iteration 109/1000 | Loss: 0.00002265
Iteration 110/1000 | Loss: 0.00002265
Iteration 111/1000 | Loss: 0.00002265
Iteration 112/1000 | Loss: 0.00002265
Iteration 113/1000 | Loss: 0.00002265
Iteration 114/1000 | Loss: 0.00002265
Iteration 115/1000 | Loss: 0.00002265
Iteration 116/1000 | Loss: 0.00002265
Iteration 117/1000 | Loss: 0.00002265
Iteration 118/1000 | Loss: 0.00002265
Iteration 119/1000 | Loss: 0.00002264
Iteration 120/1000 | Loss: 0.00002264
Iteration 121/1000 | Loss: 0.00002264
Iteration 122/1000 | Loss: 0.00002264
Iteration 123/1000 | Loss: 0.00002264
Iteration 124/1000 | Loss: 0.00002264
Iteration 125/1000 | Loss: 0.00002264
Iteration 126/1000 | Loss: 0.00002264
Iteration 127/1000 | Loss: 0.00002264
Iteration 128/1000 | Loss: 0.00002264
Iteration 129/1000 | Loss: 0.00002264
Iteration 130/1000 | Loss: 0.00002264
Iteration 131/1000 | Loss: 0.00002264
Iteration 132/1000 | Loss: 0.00002264
Iteration 133/1000 | Loss: 0.00002264
Iteration 134/1000 | Loss: 0.00002264
Iteration 135/1000 | Loss: 0.00002264
Iteration 136/1000 | Loss: 0.00002263
Iteration 137/1000 | Loss: 0.00002263
Iteration 138/1000 | Loss: 0.00002263
Iteration 139/1000 | Loss: 0.00002263
Iteration 140/1000 | Loss: 0.00002263
Iteration 141/1000 | Loss: 0.00002263
Iteration 142/1000 | Loss: 0.00002263
Iteration 143/1000 | Loss: 0.00002263
Iteration 144/1000 | Loss: 0.00002263
Iteration 145/1000 | Loss: 0.00002263
Iteration 146/1000 | Loss: 0.00002263
Iteration 147/1000 | Loss: 0.00002263
Iteration 148/1000 | Loss: 0.00002263
Iteration 149/1000 | Loss: 0.00002263
Iteration 150/1000 | Loss: 0.00002263
Iteration 151/1000 | Loss: 0.00002263
Iteration 152/1000 | Loss: 0.00002263
Iteration 153/1000 | Loss: 0.00002263
Iteration 154/1000 | Loss: 0.00002263
Iteration 155/1000 | Loss: 0.00002263
Iteration 156/1000 | Loss: 0.00002263
Iteration 157/1000 | Loss: 0.00002263
Iteration 158/1000 | Loss: 0.00002263
Iteration 159/1000 | Loss: 0.00002263
Iteration 160/1000 | Loss: 0.00002263
Iteration 161/1000 | Loss: 0.00002263
Iteration 162/1000 | Loss: 0.00002263
Iteration 163/1000 | Loss: 0.00002263
Iteration 164/1000 | Loss: 0.00002263
Iteration 165/1000 | Loss: 0.00002263
Iteration 166/1000 | Loss: 0.00002263
Iteration 167/1000 | Loss: 0.00002263
Iteration 168/1000 | Loss: 0.00002263
Iteration 169/1000 | Loss: 0.00002263
Iteration 170/1000 | Loss: 0.00002263
Iteration 171/1000 | Loss: 0.00002263
Iteration 172/1000 | Loss: 0.00002263
Iteration 173/1000 | Loss: 0.00002263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.2629950763075612e-05, 2.2629950763075612e-05, 2.2629950763075612e-05, 2.2629950763075612e-05, 2.2629950763075612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2629950763075612e-05

Optimization complete. Final v2v error: 4.045929908752441 mm

Highest mean error: 4.6586713790893555 mm for frame 37

Lowest mean error: 3.7771589756011963 mm for frame 93

Saving results

Total time: 36.84902787208557
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885478
Iteration 2/25 | Loss: 0.00184507
Iteration 3/25 | Loss: 0.00160739
Iteration 4/25 | Loss: 0.00159279
Iteration 5/25 | Loss: 0.00159169
Iteration 6/25 | Loss: 0.00159169
Iteration 7/25 | Loss: 0.00159169
Iteration 8/25 | Loss: 0.00159169
Iteration 9/25 | Loss: 0.00159169
Iteration 10/25 | Loss: 0.00159169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0015916855772957206, 0.0015916855772957206, 0.0015916855772957206, 0.0015916855772957206, 0.0015916855772957206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015916855772957206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82156026
Iteration 2/25 | Loss: 0.00104450
Iteration 3/25 | Loss: 0.00104450
Iteration 4/25 | Loss: 0.00104450
Iteration 5/25 | Loss: 0.00104450
Iteration 6/25 | Loss: 0.00104450
Iteration 7/25 | Loss: 0.00104450
Iteration 8/25 | Loss: 0.00104450
Iteration 9/25 | Loss: 0.00104450
Iteration 10/25 | Loss: 0.00104450
Iteration 11/25 | Loss: 0.00104450
Iteration 12/25 | Loss: 0.00104450
Iteration 13/25 | Loss: 0.00104450
Iteration 14/25 | Loss: 0.00104450
Iteration 15/25 | Loss: 0.00104450
Iteration 16/25 | Loss: 0.00104450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001044495846144855, 0.001044495846144855, 0.001044495846144855, 0.001044495846144855, 0.001044495846144855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001044495846144855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104450
Iteration 2/1000 | Loss: 0.00004789
Iteration 3/1000 | Loss: 0.00003374
Iteration 4/1000 | Loss: 0.00002989
Iteration 5/1000 | Loss: 0.00002749
Iteration 6/1000 | Loss: 0.00002621
Iteration 7/1000 | Loss: 0.00002572
Iteration 8/1000 | Loss: 0.00002540
Iteration 9/1000 | Loss: 0.00002521
Iteration 10/1000 | Loss: 0.00002503
Iteration 11/1000 | Loss: 0.00002499
Iteration 12/1000 | Loss: 0.00002499
Iteration 13/1000 | Loss: 0.00002497
Iteration 14/1000 | Loss: 0.00002496
Iteration 15/1000 | Loss: 0.00002495
Iteration 16/1000 | Loss: 0.00002490
Iteration 17/1000 | Loss: 0.00002490
Iteration 18/1000 | Loss: 0.00002490
Iteration 19/1000 | Loss: 0.00002488
Iteration 20/1000 | Loss: 0.00002488
Iteration 21/1000 | Loss: 0.00002488
Iteration 22/1000 | Loss: 0.00002488
Iteration 23/1000 | Loss: 0.00002488
Iteration 24/1000 | Loss: 0.00002488
Iteration 25/1000 | Loss: 0.00002488
Iteration 26/1000 | Loss: 0.00002488
Iteration 27/1000 | Loss: 0.00002488
Iteration 28/1000 | Loss: 0.00002488
Iteration 29/1000 | Loss: 0.00002487
Iteration 30/1000 | Loss: 0.00002487
Iteration 31/1000 | Loss: 0.00002487
Iteration 32/1000 | Loss: 0.00002487
Iteration 33/1000 | Loss: 0.00002487
Iteration 34/1000 | Loss: 0.00002487
Iteration 35/1000 | Loss: 0.00002487
Iteration 36/1000 | Loss: 0.00002487
Iteration 37/1000 | Loss: 0.00002487
Iteration 38/1000 | Loss: 0.00002487
Iteration 39/1000 | Loss: 0.00002487
Iteration 40/1000 | Loss: 0.00002486
Iteration 41/1000 | Loss: 0.00002486
Iteration 42/1000 | Loss: 0.00002486
Iteration 43/1000 | Loss: 0.00002486
Iteration 44/1000 | Loss: 0.00002486
Iteration 45/1000 | Loss: 0.00002486
Iteration 46/1000 | Loss: 0.00002486
Iteration 47/1000 | Loss: 0.00002486
Iteration 48/1000 | Loss: 0.00002485
Iteration 49/1000 | Loss: 0.00002485
Iteration 50/1000 | Loss: 0.00002485
Iteration 51/1000 | Loss: 0.00002485
Iteration 52/1000 | Loss: 0.00002485
Iteration 53/1000 | Loss: 0.00002485
Iteration 54/1000 | Loss: 0.00002485
Iteration 55/1000 | Loss: 0.00002484
Iteration 56/1000 | Loss: 0.00002484
Iteration 57/1000 | Loss: 0.00002483
Iteration 58/1000 | Loss: 0.00002483
Iteration 59/1000 | Loss: 0.00002483
Iteration 60/1000 | Loss: 0.00002483
Iteration 61/1000 | Loss: 0.00002483
Iteration 62/1000 | Loss: 0.00002482
Iteration 63/1000 | Loss: 0.00002482
Iteration 64/1000 | Loss: 0.00002482
Iteration 65/1000 | Loss: 0.00002481
Iteration 66/1000 | Loss: 0.00002481
Iteration 67/1000 | Loss: 0.00002481
Iteration 68/1000 | Loss: 0.00002481
Iteration 69/1000 | Loss: 0.00002480
Iteration 70/1000 | Loss: 0.00002480
Iteration 71/1000 | Loss: 0.00002480
Iteration 72/1000 | Loss: 0.00002479
Iteration 73/1000 | Loss: 0.00002479
Iteration 74/1000 | Loss: 0.00002479
Iteration 75/1000 | Loss: 0.00002479
Iteration 76/1000 | Loss: 0.00002479
Iteration 77/1000 | Loss: 0.00002479
Iteration 78/1000 | Loss: 0.00002479
Iteration 79/1000 | Loss: 0.00002479
Iteration 80/1000 | Loss: 0.00002479
Iteration 81/1000 | Loss: 0.00002479
Iteration 82/1000 | Loss: 0.00002479
Iteration 83/1000 | Loss: 0.00002479
Iteration 84/1000 | Loss: 0.00002479
Iteration 85/1000 | Loss: 0.00002479
Iteration 86/1000 | Loss: 0.00002478
Iteration 87/1000 | Loss: 0.00002478
Iteration 88/1000 | Loss: 0.00002478
Iteration 89/1000 | Loss: 0.00002478
Iteration 90/1000 | Loss: 0.00002478
Iteration 91/1000 | Loss: 0.00002478
Iteration 92/1000 | Loss: 0.00002478
Iteration 93/1000 | Loss: 0.00002478
Iteration 94/1000 | Loss: 0.00002478
Iteration 95/1000 | Loss: 0.00002478
Iteration 96/1000 | Loss: 0.00002478
Iteration 97/1000 | Loss: 0.00002478
Iteration 98/1000 | Loss: 0.00002478
Iteration 99/1000 | Loss: 0.00002478
Iteration 100/1000 | Loss: 0.00002478
Iteration 101/1000 | Loss: 0.00002477
Iteration 102/1000 | Loss: 0.00002477
Iteration 103/1000 | Loss: 0.00002477
Iteration 104/1000 | Loss: 0.00002477
Iteration 105/1000 | Loss: 0.00002477
Iteration 106/1000 | Loss: 0.00002477
Iteration 107/1000 | Loss: 0.00002477
Iteration 108/1000 | Loss: 0.00002477
Iteration 109/1000 | Loss: 0.00002477
Iteration 110/1000 | Loss: 0.00002477
Iteration 111/1000 | Loss: 0.00002477
Iteration 112/1000 | Loss: 0.00002477
Iteration 113/1000 | Loss: 0.00002477
Iteration 114/1000 | Loss: 0.00002477
Iteration 115/1000 | Loss: 0.00002476
Iteration 116/1000 | Loss: 0.00002476
Iteration 117/1000 | Loss: 0.00002476
Iteration 118/1000 | Loss: 0.00002476
Iteration 119/1000 | Loss: 0.00002476
Iteration 120/1000 | Loss: 0.00002476
Iteration 121/1000 | Loss: 0.00002476
Iteration 122/1000 | Loss: 0.00002476
Iteration 123/1000 | Loss: 0.00002476
Iteration 124/1000 | Loss: 0.00002475
Iteration 125/1000 | Loss: 0.00002475
Iteration 126/1000 | Loss: 0.00002475
Iteration 127/1000 | Loss: 0.00002475
Iteration 128/1000 | Loss: 0.00002475
Iteration 129/1000 | Loss: 0.00002475
Iteration 130/1000 | Loss: 0.00002475
Iteration 131/1000 | Loss: 0.00002475
Iteration 132/1000 | Loss: 0.00002475
Iteration 133/1000 | Loss: 0.00002475
Iteration 134/1000 | Loss: 0.00002475
Iteration 135/1000 | Loss: 0.00002475
Iteration 136/1000 | Loss: 0.00002475
Iteration 137/1000 | Loss: 0.00002475
Iteration 138/1000 | Loss: 0.00002475
Iteration 139/1000 | Loss: 0.00002475
Iteration 140/1000 | Loss: 0.00002475
Iteration 141/1000 | Loss: 0.00002475
Iteration 142/1000 | Loss: 0.00002475
Iteration 143/1000 | Loss: 0.00002475
Iteration 144/1000 | Loss: 0.00002475
Iteration 145/1000 | Loss: 0.00002475
Iteration 146/1000 | Loss: 0.00002475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.4753746401984245e-05, 2.4753746401984245e-05, 2.4753746401984245e-05, 2.4753746401984245e-05, 2.4753746401984245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4753746401984245e-05

Optimization complete. Final v2v error: 4.260188579559326 mm

Highest mean error: 4.400518417358398 mm for frame 121

Lowest mean error: 4.171897888183594 mm for frame 140

Saving results

Total time: 29.168925046920776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01164840
Iteration 2/25 | Loss: 0.00239169
Iteration 3/25 | Loss: 0.00181115
Iteration 4/25 | Loss: 0.00169884
Iteration 5/25 | Loss: 0.00171705
Iteration 6/25 | Loss: 0.00164897
Iteration 7/25 | Loss: 0.00162596
Iteration 8/25 | Loss: 0.00161866
Iteration 9/25 | Loss: 0.00160147
Iteration 10/25 | Loss: 0.00158952
Iteration 11/25 | Loss: 0.00159103
Iteration 12/25 | Loss: 0.00159105
Iteration 13/25 | Loss: 0.00159422
Iteration 14/25 | Loss: 0.00159134
Iteration 15/25 | Loss: 0.00158960
Iteration 16/25 | Loss: 0.00158457
Iteration 17/25 | Loss: 0.00157636
Iteration 18/25 | Loss: 0.00157117
Iteration 19/25 | Loss: 0.00156751
Iteration 20/25 | Loss: 0.00156654
Iteration 21/25 | Loss: 0.00156567
Iteration 22/25 | Loss: 0.00156093
Iteration 23/25 | Loss: 0.00155962
Iteration 24/25 | Loss: 0.00156632
Iteration 25/25 | Loss: 0.00156571

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26257539
Iteration 2/25 | Loss: 0.00256243
Iteration 3/25 | Loss: 0.00248566
Iteration 4/25 | Loss: 0.00248566
Iteration 5/25 | Loss: 0.00248566
Iteration 6/25 | Loss: 0.00248566
Iteration 7/25 | Loss: 0.00248566
Iteration 8/25 | Loss: 0.00248566
Iteration 9/25 | Loss: 0.00248566
Iteration 10/25 | Loss: 0.00248566
Iteration 11/25 | Loss: 0.00248566
Iteration 12/25 | Loss: 0.00248565
Iteration 13/25 | Loss: 0.00248565
Iteration 14/25 | Loss: 0.00248565
Iteration 15/25 | Loss: 0.00248565
Iteration 16/25 | Loss: 0.00248565
Iteration 17/25 | Loss: 0.00248566
Iteration 18/25 | Loss: 0.00248565
Iteration 19/25 | Loss: 0.00248565
Iteration 20/25 | Loss: 0.00248565
Iteration 21/25 | Loss: 0.00248565
Iteration 22/25 | Loss: 0.00248565
Iteration 23/25 | Loss: 0.00248565
Iteration 24/25 | Loss: 0.00248565
Iteration 25/25 | Loss: 0.00248565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00248565
Iteration 2/1000 | Loss: 0.00024065
Iteration 3/1000 | Loss: 0.00029205
Iteration 4/1000 | Loss: 0.00043861
Iteration 5/1000 | Loss: 0.00027721
Iteration 6/1000 | Loss: 0.00021828
Iteration 7/1000 | Loss: 0.00018983
Iteration 8/1000 | Loss: 0.00016690
Iteration 9/1000 | Loss: 0.00015467
Iteration 10/1000 | Loss: 0.00017947
Iteration 11/1000 | Loss: 0.00020201
Iteration 12/1000 | Loss: 0.00024068
Iteration 13/1000 | Loss: 0.00020580
Iteration 14/1000 | Loss: 0.00024479
Iteration 15/1000 | Loss: 0.00023401
Iteration 16/1000 | Loss: 0.00023929
Iteration 17/1000 | Loss: 0.00028764
Iteration 18/1000 | Loss: 0.00018649
Iteration 19/1000 | Loss: 0.00004595
Iteration 20/1000 | Loss: 0.00020728
Iteration 21/1000 | Loss: 0.00017101
Iteration 22/1000 | Loss: 0.00020248
Iteration 23/1000 | Loss: 0.00016379
Iteration 24/1000 | Loss: 0.00016831
Iteration 25/1000 | Loss: 0.00026190
Iteration 26/1000 | Loss: 0.00016618
Iteration 27/1000 | Loss: 0.00024041
Iteration 28/1000 | Loss: 0.00015741
Iteration 29/1000 | Loss: 0.00006414
Iteration 30/1000 | Loss: 0.00012880
Iteration 31/1000 | Loss: 0.00014977
Iteration 32/1000 | Loss: 0.00026722
Iteration 33/1000 | Loss: 0.00032433
Iteration 34/1000 | Loss: 0.00026204
Iteration 35/1000 | Loss: 0.00004817
Iteration 36/1000 | Loss: 0.00004124
Iteration 37/1000 | Loss: 0.00003761
Iteration 38/1000 | Loss: 0.00054789
Iteration 39/1000 | Loss: 0.00003436
Iteration 40/1000 | Loss: 0.00003227
Iteration 41/1000 | Loss: 0.00010373
Iteration 42/1000 | Loss: 0.00003096
Iteration 43/1000 | Loss: 0.00002990
Iteration 44/1000 | Loss: 0.00002941
Iteration 45/1000 | Loss: 0.00002897
Iteration 46/1000 | Loss: 0.00002863
Iteration 47/1000 | Loss: 0.00002849
Iteration 48/1000 | Loss: 0.00002844
Iteration 49/1000 | Loss: 0.00002832
Iteration 50/1000 | Loss: 0.00002832
Iteration 51/1000 | Loss: 0.00002823
Iteration 52/1000 | Loss: 0.00002817
Iteration 53/1000 | Loss: 0.00002817
Iteration 54/1000 | Loss: 0.00002817
Iteration 55/1000 | Loss: 0.00002816
Iteration 56/1000 | Loss: 0.00002816
Iteration 57/1000 | Loss: 0.00002811
Iteration 58/1000 | Loss: 0.00002811
Iteration 59/1000 | Loss: 0.00002810
Iteration 60/1000 | Loss: 0.00002809
Iteration 61/1000 | Loss: 0.00002809
Iteration 62/1000 | Loss: 0.00002808
Iteration 63/1000 | Loss: 0.00002808
Iteration 64/1000 | Loss: 0.00002808
Iteration 65/1000 | Loss: 0.00002807
Iteration 66/1000 | Loss: 0.00002807
Iteration 67/1000 | Loss: 0.00002807
Iteration 68/1000 | Loss: 0.00002806
Iteration 69/1000 | Loss: 0.00002805
Iteration 70/1000 | Loss: 0.00002804
Iteration 71/1000 | Loss: 0.00002804
Iteration 72/1000 | Loss: 0.00002804
Iteration 73/1000 | Loss: 0.00002803
Iteration 74/1000 | Loss: 0.00002801
Iteration 75/1000 | Loss: 0.00002801
Iteration 76/1000 | Loss: 0.00002801
Iteration 77/1000 | Loss: 0.00002801
Iteration 78/1000 | Loss: 0.00002800
Iteration 79/1000 | Loss: 0.00002800
Iteration 80/1000 | Loss: 0.00002800
Iteration 81/1000 | Loss: 0.00002799
Iteration 82/1000 | Loss: 0.00002799
Iteration 83/1000 | Loss: 0.00002799
Iteration 84/1000 | Loss: 0.00002799
Iteration 85/1000 | Loss: 0.00002798
Iteration 86/1000 | Loss: 0.00002798
Iteration 87/1000 | Loss: 0.00002798
Iteration 88/1000 | Loss: 0.00002798
Iteration 89/1000 | Loss: 0.00002798
Iteration 90/1000 | Loss: 0.00002798
Iteration 91/1000 | Loss: 0.00002798
Iteration 92/1000 | Loss: 0.00002798
Iteration 93/1000 | Loss: 0.00002798
Iteration 94/1000 | Loss: 0.00002798
Iteration 95/1000 | Loss: 0.00002798
Iteration 96/1000 | Loss: 0.00002798
Iteration 97/1000 | Loss: 0.00002798
Iteration 98/1000 | Loss: 0.00002798
Iteration 99/1000 | Loss: 0.00002798
Iteration 100/1000 | Loss: 0.00002798
Iteration 101/1000 | Loss: 0.00002798
Iteration 102/1000 | Loss: 0.00002798
Iteration 103/1000 | Loss: 0.00002798
Iteration 104/1000 | Loss: 0.00002798
Iteration 105/1000 | Loss: 0.00002798
Iteration 106/1000 | Loss: 0.00002798
Iteration 107/1000 | Loss: 0.00002798
Iteration 108/1000 | Loss: 0.00002798
Iteration 109/1000 | Loss: 0.00002798
Iteration 110/1000 | Loss: 0.00002798
Iteration 111/1000 | Loss: 0.00002798
Iteration 112/1000 | Loss: 0.00002798
Iteration 113/1000 | Loss: 0.00002798
Iteration 114/1000 | Loss: 0.00002798
Iteration 115/1000 | Loss: 0.00002798
Iteration 116/1000 | Loss: 0.00002798
Iteration 117/1000 | Loss: 0.00002798
Iteration 118/1000 | Loss: 0.00002798
Iteration 119/1000 | Loss: 0.00002798
Iteration 120/1000 | Loss: 0.00002798
Iteration 121/1000 | Loss: 0.00002798
Iteration 122/1000 | Loss: 0.00002798
Iteration 123/1000 | Loss: 0.00002798
Iteration 124/1000 | Loss: 0.00002798
Iteration 125/1000 | Loss: 0.00002798
Iteration 126/1000 | Loss: 0.00002798
Iteration 127/1000 | Loss: 0.00002798
Iteration 128/1000 | Loss: 0.00002798
Iteration 129/1000 | Loss: 0.00002798
Iteration 130/1000 | Loss: 0.00002798
Iteration 131/1000 | Loss: 0.00002798
Iteration 132/1000 | Loss: 0.00002798
Iteration 133/1000 | Loss: 0.00002798
Iteration 134/1000 | Loss: 0.00002798
Iteration 135/1000 | Loss: 0.00002798
Iteration 136/1000 | Loss: 0.00002798
Iteration 137/1000 | Loss: 0.00002798
Iteration 138/1000 | Loss: 0.00002798
Iteration 139/1000 | Loss: 0.00002798
Iteration 140/1000 | Loss: 0.00002798
Iteration 141/1000 | Loss: 0.00002798
Iteration 142/1000 | Loss: 0.00002798
Iteration 143/1000 | Loss: 0.00002798
Iteration 144/1000 | Loss: 0.00002798
Iteration 145/1000 | Loss: 0.00002798
Iteration 146/1000 | Loss: 0.00002798
Iteration 147/1000 | Loss: 0.00002798
Iteration 148/1000 | Loss: 0.00002798
Iteration 149/1000 | Loss: 0.00002798
Iteration 150/1000 | Loss: 0.00002798
Iteration 151/1000 | Loss: 0.00002798
Iteration 152/1000 | Loss: 0.00002798
Iteration 153/1000 | Loss: 0.00002798
Iteration 154/1000 | Loss: 0.00002798
Iteration 155/1000 | Loss: 0.00002798
Iteration 156/1000 | Loss: 0.00002798
Iteration 157/1000 | Loss: 0.00002798
Iteration 158/1000 | Loss: 0.00002798
Iteration 159/1000 | Loss: 0.00002798
Iteration 160/1000 | Loss: 0.00002798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.7977212084806524e-05, 2.7977212084806524e-05, 2.7977212084806524e-05, 2.7977212084806524e-05, 2.7977212084806524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7977212084806524e-05

Optimization complete. Final v2v error: 4.486372470855713 mm

Highest mean error: 9.57516860961914 mm for frame 12

Lowest mean error: 4.163917541503906 mm for frame 136

Saving results

Total time: 120.84706139564514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843515
Iteration 2/25 | Loss: 0.00208377
Iteration 3/25 | Loss: 0.00176889
Iteration 4/25 | Loss: 0.00170886
Iteration 5/25 | Loss: 0.00174870
Iteration 6/25 | Loss: 0.00175352
Iteration 7/25 | Loss: 0.00168257
Iteration 8/25 | Loss: 0.00166737
Iteration 9/25 | Loss: 0.00165434
Iteration 10/25 | Loss: 0.00164846
Iteration 11/25 | Loss: 0.00164751
Iteration 12/25 | Loss: 0.00164726
Iteration 13/25 | Loss: 0.00164830
Iteration 14/25 | Loss: 0.00164735
Iteration 15/25 | Loss: 0.00164739
Iteration 16/25 | Loss: 0.00164802
Iteration 17/25 | Loss: 0.00164750
Iteration 18/25 | Loss: 0.00164772
Iteration 19/25 | Loss: 0.00164705
Iteration 20/25 | Loss: 0.00164587
Iteration 21/25 | Loss: 0.00164669
Iteration 22/25 | Loss: 0.00164676
Iteration 23/25 | Loss: 0.00164752
Iteration 24/25 | Loss: 0.00164638
Iteration 25/25 | Loss: 0.00164671

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23229313
Iteration 2/25 | Loss: 0.00201918
Iteration 3/25 | Loss: 0.00201917
Iteration 4/25 | Loss: 0.00201917
Iteration 5/25 | Loss: 0.00201917
Iteration 6/25 | Loss: 0.00201917
Iteration 7/25 | Loss: 0.00201917
Iteration 8/25 | Loss: 0.00201917
Iteration 9/25 | Loss: 0.00201917
Iteration 10/25 | Loss: 0.00201917
Iteration 11/25 | Loss: 0.00201917
Iteration 12/25 | Loss: 0.00201917
Iteration 13/25 | Loss: 0.00201917
Iteration 14/25 | Loss: 0.00201917
Iteration 15/25 | Loss: 0.00201917
Iteration 16/25 | Loss: 0.00201917
Iteration 17/25 | Loss: 0.00201917
Iteration 18/25 | Loss: 0.00201917
Iteration 19/25 | Loss: 0.00201917
Iteration 20/25 | Loss: 0.00201917
Iteration 21/25 | Loss: 0.00201917
Iteration 22/25 | Loss: 0.00201917
Iteration 23/25 | Loss: 0.00201917
Iteration 24/25 | Loss: 0.00201917
Iteration 25/25 | Loss: 0.00201917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201917
Iteration 2/1000 | Loss: 0.00013405
Iteration 3/1000 | Loss: 0.00007764
Iteration 4/1000 | Loss: 0.00007499
Iteration 5/1000 | Loss: 0.00008240
Iteration 6/1000 | Loss: 0.00007088
Iteration 7/1000 | Loss: 0.00006306
Iteration 8/1000 | Loss: 0.00005783
Iteration 9/1000 | Loss: 0.00005515
Iteration 10/1000 | Loss: 0.00005296
Iteration 11/1000 | Loss: 0.00006040
Iteration 12/1000 | Loss: 0.00005325
Iteration 13/1000 | Loss: 0.00005770
Iteration 14/1000 | Loss: 0.00005355
Iteration 15/1000 | Loss: 0.00006110
Iteration 16/1000 | Loss: 0.00006370
Iteration 17/1000 | Loss: 0.00006058
Iteration 18/1000 | Loss: 0.00005085
Iteration 19/1000 | Loss: 0.00005684
Iteration 20/1000 | Loss: 0.00005071
Iteration 21/1000 | Loss: 0.00005101
Iteration 22/1000 | Loss: 0.00005766
Iteration 23/1000 | Loss: 0.00005200
Iteration 24/1000 | Loss: 0.00004984
Iteration 25/1000 | Loss: 0.00004817
Iteration 26/1000 | Loss: 0.00004682
Iteration 27/1000 | Loss: 0.00004631
Iteration 28/1000 | Loss: 0.00004589
Iteration 29/1000 | Loss: 0.00004554
Iteration 30/1000 | Loss: 0.00004512
Iteration 31/1000 | Loss: 0.00004481
Iteration 32/1000 | Loss: 0.00004470
Iteration 33/1000 | Loss: 0.00004458
Iteration 34/1000 | Loss: 0.00004457
Iteration 35/1000 | Loss: 0.00004446
Iteration 36/1000 | Loss: 0.00004442
Iteration 37/1000 | Loss: 0.00004442
Iteration 38/1000 | Loss: 0.00004438
Iteration 39/1000 | Loss: 0.00004428
Iteration 40/1000 | Loss: 0.00004424
Iteration 41/1000 | Loss: 0.00004423
Iteration 42/1000 | Loss: 0.00004423
Iteration 43/1000 | Loss: 0.00004410
Iteration 44/1000 | Loss: 0.00004405
Iteration 45/1000 | Loss: 0.00004401
Iteration 46/1000 | Loss: 0.00004399
Iteration 47/1000 | Loss: 0.00004399
Iteration 48/1000 | Loss: 0.00004398
Iteration 49/1000 | Loss: 0.00004398
Iteration 50/1000 | Loss: 0.00004397
Iteration 51/1000 | Loss: 0.00004397
Iteration 52/1000 | Loss: 0.00004396
Iteration 53/1000 | Loss: 0.00004395
Iteration 54/1000 | Loss: 0.00004394
Iteration 55/1000 | Loss: 0.00004390
Iteration 56/1000 | Loss: 0.00004389
Iteration 57/1000 | Loss: 0.00004385
Iteration 58/1000 | Loss: 0.00004384
Iteration 59/1000 | Loss: 0.00004383
Iteration 60/1000 | Loss: 0.00004382
Iteration 61/1000 | Loss: 0.00004380
Iteration 62/1000 | Loss: 0.00004380
Iteration 63/1000 | Loss: 0.00004380
Iteration 64/1000 | Loss: 0.00004378
Iteration 65/1000 | Loss: 0.00004377
Iteration 66/1000 | Loss: 0.00004377
Iteration 67/1000 | Loss: 0.00004376
Iteration 68/1000 | Loss: 0.00004376
Iteration 69/1000 | Loss: 0.00004375
Iteration 70/1000 | Loss: 0.00004375
Iteration 71/1000 | Loss: 0.00004373
Iteration 72/1000 | Loss: 0.00004369
Iteration 73/1000 | Loss: 0.00004367
Iteration 74/1000 | Loss: 0.00004366
Iteration 75/1000 | Loss: 0.00004365
Iteration 76/1000 | Loss: 0.00004364
Iteration 77/1000 | Loss: 0.00004358
Iteration 78/1000 | Loss: 0.00004355
Iteration 79/1000 | Loss: 0.00004354
Iteration 80/1000 | Loss: 0.00004353
Iteration 81/1000 | Loss: 0.00004352
Iteration 82/1000 | Loss: 0.00004352
Iteration 83/1000 | Loss: 0.00004351
Iteration 84/1000 | Loss: 0.00004350
Iteration 85/1000 | Loss: 0.00004350
Iteration 86/1000 | Loss: 0.00004347
Iteration 87/1000 | Loss: 0.00004344
Iteration 88/1000 | Loss: 0.00004341
Iteration 89/1000 | Loss: 0.00004341
Iteration 90/1000 | Loss: 0.00004340
Iteration 91/1000 | Loss: 0.00004340
Iteration 92/1000 | Loss: 0.00004337
Iteration 93/1000 | Loss: 0.00004337
Iteration 94/1000 | Loss: 0.00004337
Iteration 95/1000 | Loss: 0.00004336
Iteration 96/1000 | Loss: 0.00004334
Iteration 97/1000 | Loss: 0.00004333
Iteration 98/1000 | Loss: 0.00004333
Iteration 99/1000 | Loss: 0.00004332
Iteration 100/1000 | Loss: 0.00004331
Iteration 101/1000 | Loss: 0.00004331
Iteration 102/1000 | Loss: 0.00004330
Iteration 103/1000 | Loss: 0.00004330
Iteration 104/1000 | Loss: 0.00004329
Iteration 105/1000 | Loss: 0.00004328
Iteration 106/1000 | Loss: 0.00004328
Iteration 107/1000 | Loss: 0.00004325
Iteration 108/1000 | Loss: 0.00004323
Iteration 109/1000 | Loss: 0.00004323
Iteration 110/1000 | Loss: 0.00004323
Iteration 111/1000 | Loss: 0.00004323
Iteration 112/1000 | Loss: 0.00004319
Iteration 113/1000 | Loss: 0.00004319
Iteration 114/1000 | Loss: 0.00004319
Iteration 115/1000 | Loss: 0.00004318
Iteration 116/1000 | Loss: 0.00004318
Iteration 117/1000 | Loss: 0.00004318
Iteration 118/1000 | Loss: 0.00004317
Iteration 119/1000 | Loss: 0.00004317
Iteration 120/1000 | Loss: 0.00004317
Iteration 121/1000 | Loss: 0.00004317
Iteration 122/1000 | Loss: 0.00004317
Iteration 123/1000 | Loss: 0.00004317
Iteration 124/1000 | Loss: 0.00004317
Iteration 125/1000 | Loss: 0.00004316
Iteration 126/1000 | Loss: 0.00004316
Iteration 127/1000 | Loss: 0.00004316
Iteration 128/1000 | Loss: 0.00004316
Iteration 129/1000 | Loss: 0.00004316
Iteration 130/1000 | Loss: 0.00004316
Iteration 131/1000 | Loss: 0.00004316
Iteration 132/1000 | Loss: 0.00004316
Iteration 133/1000 | Loss: 0.00004316
Iteration 134/1000 | Loss: 0.00004316
Iteration 135/1000 | Loss: 0.00004316
Iteration 136/1000 | Loss: 0.00004316
Iteration 137/1000 | Loss: 0.00004316
Iteration 138/1000 | Loss: 0.00004316
Iteration 139/1000 | Loss: 0.00004316
Iteration 140/1000 | Loss: 0.00004316
Iteration 141/1000 | Loss: 0.00004316
Iteration 142/1000 | Loss: 0.00004316
Iteration 143/1000 | Loss: 0.00004316
Iteration 144/1000 | Loss: 0.00004316
Iteration 145/1000 | Loss: 0.00004316
Iteration 146/1000 | Loss: 0.00004316
Iteration 147/1000 | Loss: 0.00004316
Iteration 148/1000 | Loss: 0.00004316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [4.316178092267364e-05, 4.316178092267364e-05, 4.316178092267364e-05, 4.316178092267364e-05, 4.316178092267364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.316178092267364e-05

Optimization complete. Final v2v error: 4.760224342346191 mm

Highest mean error: 12.395744323730469 mm for frame 13

Lowest mean error: 4.019795894622803 mm for frame 106

Saving results

Total time: 129.00484538078308
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00743529
Iteration 2/25 | Loss: 0.00187365
Iteration 3/25 | Loss: 0.00168842
Iteration 4/25 | Loss: 0.00165972
Iteration 5/25 | Loss: 0.00165391
Iteration 6/25 | Loss: 0.00165277
Iteration 7/25 | Loss: 0.00165277
Iteration 8/25 | Loss: 0.00165277
Iteration 9/25 | Loss: 0.00165277
Iteration 10/25 | Loss: 0.00165277
Iteration 11/25 | Loss: 0.00165277
Iteration 12/25 | Loss: 0.00165277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001652765553444624, 0.001652765553444624, 0.001652765553444624, 0.001652765553444624, 0.001652765553444624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001652765553444624

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.58532047
Iteration 2/25 | Loss: 0.00204761
Iteration 3/25 | Loss: 0.00204760
Iteration 4/25 | Loss: 0.00204760
Iteration 5/25 | Loss: 0.00204760
Iteration 6/25 | Loss: 0.00204760
Iteration 7/25 | Loss: 0.00204760
Iteration 8/25 | Loss: 0.00204760
Iteration 9/25 | Loss: 0.00204760
Iteration 10/25 | Loss: 0.00204760
Iteration 11/25 | Loss: 0.00204760
Iteration 12/25 | Loss: 0.00204760
Iteration 13/25 | Loss: 0.00204760
Iteration 14/25 | Loss: 0.00204760
Iteration 15/25 | Loss: 0.00204760
Iteration 16/25 | Loss: 0.00204760
Iteration 17/25 | Loss: 0.00204760
Iteration 18/25 | Loss: 0.00204760
Iteration 19/25 | Loss: 0.00204760
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020476016215980053, 0.0020476016215980053, 0.0020476016215980053, 0.0020476016215980053, 0.0020476016215980053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020476016215980053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204760
Iteration 2/1000 | Loss: 0.00010384
Iteration 3/1000 | Loss: 0.00007359
Iteration 4/1000 | Loss: 0.00006590
Iteration 5/1000 | Loss: 0.00006080
Iteration 6/1000 | Loss: 0.00005792
Iteration 7/1000 | Loss: 0.00005584
Iteration 8/1000 | Loss: 0.00005428
Iteration 9/1000 | Loss: 0.00005311
Iteration 10/1000 | Loss: 0.00005229
Iteration 11/1000 | Loss: 0.00005159
Iteration 12/1000 | Loss: 0.00005109
Iteration 13/1000 | Loss: 0.00005067
Iteration 14/1000 | Loss: 0.00005032
Iteration 15/1000 | Loss: 0.00005000
Iteration 16/1000 | Loss: 0.00004977
Iteration 17/1000 | Loss: 0.00004956
Iteration 18/1000 | Loss: 0.00004937
Iteration 19/1000 | Loss: 0.00004920
Iteration 20/1000 | Loss: 0.00004907
Iteration 21/1000 | Loss: 0.00004905
Iteration 22/1000 | Loss: 0.00004893
Iteration 23/1000 | Loss: 0.00004889
Iteration 24/1000 | Loss: 0.00004884
Iteration 25/1000 | Loss: 0.00004883
Iteration 26/1000 | Loss: 0.00004881
Iteration 27/1000 | Loss: 0.00004878
Iteration 28/1000 | Loss: 0.00004877
Iteration 29/1000 | Loss: 0.00004875
Iteration 30/1000 | Loss: 0.00004874
Iteration 31/1000 | Loss: 0.00004873
Iteration 32/1000 | Loss: 0.00004873
Iteration 33/1000 | Loss: 0.00004872
Iteration 34/1000 | Loss: 0.00004872
Iteration 35/1000 | Loss: 0.00004871
Iteration 36/1000 | Loss: 0.00004871
Iteration 37/1000 | Loss: 0.00004870
Iteration 38/1000 | Loss: 0.00004870
Iteration 39/1000 | Loss: 0.00004869
Iteration 40/1000 | Loss: 0.00004869
Iteration 41/1000 | Loss: 0.00004863
Iteration 42/1000 | Loss: 0.00004863
Iteration 43/1000 | Loss: 0.00004863
Iteration 44/1000 | Loss: 0.00004863
Iteration 45/1000 | Loss: 0.00004863
Iteration 46/1000 | Loss: 0.00004862
Iteration 47/1000 | Loss: 0.00004862
Iteration 48/1000 | Loss: 0.00004862
Iteration 49/1000 | Loss: 0.00004862
Iteration 50/1000 | Loss: 0.00004862
Iteration 51/1000 | Loss: 0.00004861
Iteration 52/1000 | Loss: 0.00004861
Iteration 53/1000 | Loss: 0.00004861
Iteration 54/1000 | Loss: 0.00004861
Iteration 55/1000 | Loss: 0.00004861
Iteration 56/1000 | Loss: 0.00004860
Iteration 57/1000 | Loss: 0.00004860
Iteration 58/1000 | Loss: 0.00004859
Iteration 59/1000 | Loss: 0.00004859
Iteration 60/1000 | Loss: 0.00004858
Iteration 61/1000 | Loss: 0.00004858
Iteration 62/1000 | Loss: 0.00004858
Iteration 63/1000 | Loss: 0.00004858
Iteration 64/1000 | Loss: 0.00004857
Iteration 65/1000 | Loss: 0.00004857
Iteration 66/1000 | Loss: 0.00004857
Iteration 67/1000 | Loss: 0.00004857
Iteration 68/1000 | Loss: 0.00004856
Iteration 69/1000 | Loss: 0.00004856
Iteration 70/1000 | Loss: 0.00004855
Iteration 71/1000 | Loss: 0.00004855
Iteration 72/1000 | Loss: 0.00004855
Iteration 73/1000 | Loss: 0.00004855
Iteration 74/1000 | Loss: 0.00004855
Iteration 75/1000 | Loss: 0.00004855
Iteration 76/1000 | Loss: 0.00004855
Iteration 77/1000 | Loss: 0.00004855
Iteration 78/1000 | Loss: 0.00004855
Iteration 79/1000 | Loss: 0.00004854
Iteration 80/1000 | Loss: 0.00004854
Iteration 81/1000 | Loss: 0.00004854
Iteration 82/1000 | Loss: 0.00004854
Iteration 83/1000 | Loss: 0.00004853
Iteration 84/1000 | Loss: 0.00004853
Iteration 85/1000 | Loss: 0.00004853
Iteration 86/1000 | Loss: 0.00004853
Iteration 87/1000 | Loss: 0.00004853
Iteration 88/1000 | Loss: 0.00004853
Iteration 89/1000 | Loss: 0.00004853
Iteration 90/1000 | Loss: 0.00004853
Iteration 91/1000 | Loss: 0.00004853
Iteration 92/1000 | Loss: 0.00004853
Iteration 93/1000 | Loss: 0.00004853
Iteration 94/1000 | Loss: 0.00004852
Iteration 95/1000 | Loss: 0.00004851
Iteration 96/1000 | Loss: 0.00004851
Iteration 97/1000 | Loss: 0.00004851
Iteration 98/1000 | Loss: 0.00004851
Iteration 99/1000 | Loss: 0.00004851
Iteration 100/1000 | Loss: 0.00004851
Iteration 101/1000 | Loss: 0.00004851
Iteration 102/1000 | Loss: 0.00004851
Iteration 103/1000 | Loss: 0.00004850
Iteration 104/1000 | Loss: 0.00004850
Iteration 105/1000 | Loss: 0.00004850
Iteration 106/1000 | Loss: 0.00004850
Iteration 107/1000 | Loss: 0.00004850
Iteration 108/1000 | Loss: 0.00004849
Iteration 109/1000 | Loss: 0.00004849
Iteration 110/1000 | Loss: 0.00004849
Iteration 111/1000 | Loss: 0.00004849
Iteration 112/1000 | Loss: 0.00004848
Iteration 113/1000 | Loss: 0.00004848
Iteration 114/1000 | Loss: 0.00004848
Iteration 115/1000 | Loss: 0.00004848
Iteration 116/1000 | Loss: 0.00004848
Iteration 117/1000 | Loss: 0.00004848
Iteration 118/1000 | Loss: 0.00004848
Iteration 119/1000 | Loss: 0.00004848
Iteration 120/1000 | Loss: 0.00004847
Iteration 121/1000 | Loss: 0.00004847
Iteration 122/1000 | Loss: 0.00004847
Iteration 123/1000 | Loss: 0.00004847
Iteration 124/1000 | Loss: 0.00004846
Iteration 125/1000 | Loss: 0.00004846
Iteration 126/1000 | Loss: 0.00004846
Iteration 127/1000 | Loss: 0.00004846
Iteration 128/1000 | Loss: 0.00004846
Iteration 129/1000 | Loss: 0.00004846
Iteration 130/1000 | Loss: 0.00004846
Iteration 131/1000 | Loss: 0.00004846
Iteration 132/1000 | Loss: 0.00004845
Iteration 133/1000 | Loss: 0.00004845
Iteration 134/1000 | Loss: 0.00004845
Iteration 135/1000 | Loss: 0.00004845
Iteration 136/1000 | Loss: 0.00004844
Iteration 137/1000 | Loss: 0.00004844
Iteration 138/1000 | Loss: 0.00004844
Iteration 139/1000 | Loss: 0.00004844
Iteration 140/1000 | Loss: 0.00004844
Iteration 141/1000 | Loss: 0.00004844
Iteration 142/1000 | Loss: 0.00004844
Iteration 143/1000 | Loss: 0.00004844
Iteration 144/1000 | Loss: 0.00004844
Iteration 145/1000 | Loss: 0.00004844
Iteration 146/1000 | Loss: 0.00004844
Iteration 147/1000 | Loss: 0.00004844
Iteration 148/1000 | Loss: 0.00004844
Iteration 149/1000 | Loss: 0.00004844
Iteration 150/1000 | Loss: 0.00004844
Iteration 151/1000 | Loss: 0.00004844
Iteration 152/1000 | Loss: 0.00004844
Iteration 153/1000 | Loss: 0.00004844
Iteration 154/1000 | Loss: 0.00004844
Iteration 155/1000 | Loss: 0.00004844
Iteration 156/1000 | Loss: 0.00004844
Iteration 157/1000 | Loss: 0.00004844
Iteration 158/1000 | Loss: 0.00004844
Iteration 159/1000 | Loss: 0.00004844
Iteration 160/1000 | Loss: 0.00004844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [4.843657006858848e-05, 4.843657006858848e-05, 4.843657006858848e-05, 4.843657006858848e-05, 4.843657006858848e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.843657006858848e-05

Optimization complete. Final v2v error: 5.746896266937256 mm

Highest mean error: 7.927272319793701 mm for frame 160

Lowest mean error: 4.36529541015625 mm for frame 234

Saving results

Total time: 59.863935470581055
