Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=140, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7840-7895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776510
Iteration 2/25 | Loss: 0.00141118
Iteration 3/25 | Loss: 0.00106150
Iteration 4/25 | Loss: 0.00102750
Iteration 5/25 | Loss: 0.00102833
Iteration 6/25 | Loss: 0.00102413
Iteration 7/25 | Loss: 0.00102309
Iteration 8/25 | Loss: 0.00102300
Iteration 9/25 | Loss: 0.00102300
Iteration 10/25 | Loss: 0.00102300
Iteration 11/25 | Loss: 0.00102300
Iteration 12/25 | Loss: 0.00102299
Iteration 13/25 | Loss: 0.00102299
Iteration 14/25 | Loss: 0.00102299
Iteration 15/25 | Loss: 0.00102299
Iteration 16/25 | Loss: 0.00102299
Iteration 17/25 | Loss: 0.00102299
Iteration 18/25 | Loss: 0.00102299
Iteration 19/25 | Loss: 0.00102299
Iteration 20/25 | Loss: 0.00102299
Iteration 21/25 | Loss: 0.00102299
Iteration 22/25 | Loss: 0.00102299
Iteration 23/25 | Loss: 0.00102299
Iteration 24/25 | Loss: 0.00102298
Iteration 25/25 | Loss: 0.00102298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.91074109
Iteration 2/25 | Loss: 0.00037661
Iteration 3/25 | Loss: 0.00037659
Iteration 4/25 | Loss: 0.00037658
Iteration 5/25 | Loss: 0.00037658
Iteration 6/25 | Loss: 0.00037658
Iteration 7/25 | Loss: 0.00037658
Iteration 8/25 | Loss: 0.00037658
Iteration 9/25 | Loss: 0.00037658
Iteration 10/25 | Loss: 0.00037658
Iteration 11/25 | Loss: 0.00037658
Iteration 12/25 | Loss: 0.00037658
Iteration 13/25 | Loss: 0.00037658
Iteration 14/25 | Loss: 0.00037658
Iteration 15/25 | Loss: 0.00037658
Iteration 16/25 | Loss: 0.00037658
Iteration 17/25 | Loss: 0.00037658
Iteration 18/25 | Loss: 0.00037658
Iteration 19/25 | Loss: 0.00037658
Iteration 20/25 | Loss: 0.00037658
Iteration 21/25 | Loss: 0.00037658
Iteration 22/25 | Loss: 0.00037658
Iteration 23/25 | Loss: 0.00037658
Iteration 24/25 | Loss: 0.00037658
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003765822621062398, 0.0003765822621062398, 0.0003765822621062398, 0.0003765822621062398, 0.0003765822621062398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003765822621062398

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037658
Iteration 2/1000 | Loss: 0.00002812
Iteration 3/1000 | Loss: 0.00002087
Iteration 4/1000 | Loss: 0.00001855
Iteration 5/1000 | Loss: 0.00001788
Iteration 6/1000 | Loss: 0.00001738
Iteration 7/1000 | Loss: 0.00001693
Iteration 8/1000 | Loss: 0.00001664
Iteration 9/1000 | Loss: 0.00001641
Iteration 10/1000 | Loss: 0.00001638
Iteration 11/1000 | Loss: 0.00001628
Iteration 12/1000 | Loss: 0.00001627
Iteration 13/1000 | Loss: 0.00001627
Iteration 14/1000 | Loss: 0.00001623
Iteration 15/1000 | Loss: 0.00001622
Iteration 16/1000 | Loss: 0.00001619
Iteration 17/1000 | Loss: 0.00001619
Iteration 18/1000 | Loss: 0.00001619
Iteration 19/1000 | Loss: 0.00001618
Iteration 20/1000 | Loss: 0.00001614
Iteration 21/1000 | Loss: 0.00001613
Iteration 22/1000 | Loss: 0.00001613
Iteration 23/1000 | Loss: 0.00001612
Iteration 24/1000 | Loss: 0.00001612
Iteration 25/1000 | Loss: 0.00001611
Iteration 26/1000 | Loss: 0.00001611
Iteration 27/1000 | Loss: 0.00001611
Iteration 28/1000 | Loss: 0.00001610
Iteration 29/1000 | Loss: 0.00001610
Iteration 30/1000 | Loss: 0.00001610
Iteration 31/1000 | Loss: 0.00001609
Iteration 32/1000 | Loss: 0.00001609
Iteration 33/1000 | Loss: 0.00001609
Iteration 34/1000 | Loss: 0.00001609
Iteration 35/1000 | Loss: 0.00001608
Iteration 36/1000 | Loss: 0.00001608
Iteration 37/1000 | Loss: 0.00001608
Iteration 38/1000 | Loss: 0.00001607
Iteration 39/1000 | Loss: 0.00001607
Iteration 40/1000 | Loss: 0.00001607
Iteration 41/1000 | Loss: 0.00001607
Iteration 42/1000 | Loss: 0.00001607
Iteration 43/1000 | Loss: 0.00001607
Iteration 44/1000 | Loss: 0.00001607
Iteration 45/1000 | Loss: 0.00001607
Iteration 46/1000 | Loss: 0.00001607
Iteration 47/1000 | Loss: 0.00001607
Iteration 48/1000 | Loss: 0.00001607
Iteration 49/1000 | Loss: 0.00001606
Iteration 50/1000 | Loss: 0.00001606
Iteration 51/1000 | Loss: 0.00001606
Iteration 52/1000 | Loss: 0.00001606
Iteration 53/1000 | Loss: 0.00001605
Iteration 54/1000 | Loss: 0.00001605
Iteration 55/1000 | Loss: 0.00001605
Iteration 56/1000 | Loss: 0.00001605
Iteration 57/1000 | Loss: 0.00001605
Iteration 58/1000 | Loss: 0.00001605
Iteration 59/1000 | Loss: 0.00001605
Iteration 60/1000 | Loss: 0.00001604
Iteration 61/1000 | Loss: 0.00001604
Iteration 62/1000 | Loss: 0.00001604
Iteration 63/1000 | Loss: 0.00001604
Iteration 64/1000 | Loss: 0.00001603
Iteration 65/1000 | Loss: 0.00001603
Iteration 66/1000 | Loss: 0.00001603
Iteration 67/1000 | Loss: 0.00001603
Iteration 68/1000 | Loss: 0.00001602
Iteration 69/1000 | Loss: 0.00001602
Iteration 70/1000 | Loss: 0.00001602
Iteration 71/1000 | Loss: 0.00001602
Iteration 72/1000 | Loss: 0.00001602
Iteration 73/1000 | Loss: 0.00001602
Iteration 74/1000 | Loss: 0.00001602
Iteration 75/1000 | Loss: 0.00001602
Iteration 76/1000 | Loss: 0.00001602
Iteration 77/1000 | Loss: 0.00001602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.6023070202209055e-05, 1.6023070202209055e-05, 1.6023070202209055e-05, 1.6023070202209055e-05, 1.6023070202209055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6023070202209055e-05

Optimization complete. Final v2v error: 3.3701114654541016 mm

Highest mean error: 5.069253444671631 mm for frame 202

Lowest mean error: 2.9359049797058105 mm for frame 217

Saving results

Total time: 40.224945306777954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798921
Iteration 2/25 | Loss: 0.00166964
Iteration 3/25 | Loss: 0.00115519
Iteration 4/25 | Loss: 0.00118560
Iteration 5/25 | Loss: 0.00107891
Iteration 6/25 | Loss: 0.00105445
Iteration 7/25 | Loss: 0.00105261
Iteration 8/25 | Loss: 0.00107428
Iteration 9/25 | Loss: 0.00106425
Iteration 10/25 | Loss: 0.00105792
Iteration 11/25 | Loss: 0.00104902
Iteration 12/25 | Loss: 0.00104414
Iteration 13/25 | Loss: 0.00104310
Iteration 14/25 | Loss: 0.00104259
Iteration 15/25 | Loss: 0.00104237
Iteration 16/25 | Loss: 0.00104215
Iteration 17/25 | Loss: 0.00104212
Iteration 18/25 | Loss: 0.00104211
Iteration 19/25 | Loss: 0.00104211
Iteration 20/25 | Loss: 0.00104211
Iteration 21/25 | Loss: 0.00104211
Iteration 22/25 | Loss: 0.00104211
Iteration 23/25 | Loss: 0.00104211
Iteration 24/25 | Loss: 0.00104211
Iteration 25/25 | Loss: 0.00104211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.48726082
Iteration 2/25 | Loss: 0.00046774
Iteration 3/25 | Loss: 0.00046772
Iteration 4/25 | Loss: 0.00046772
Iteration 5/25 | Loss: 0.00046772
Iteration 6/25 | Loss: 0.00046772
Iteration 7/25 | Loss: 0.00046772
Iteration 8/25 | Loss: 0.00046772
Iteration 9/25 | Loss: 0.00046772
Iteration 10/25 | Loss: 0.00046772
Iteration 11/25 | Loss: 0.00046771
Iteration 12/25 | Loss: 0.00046771
Iteration 13/25 | Loss: 0.00046771
Iteration 14/25 | Loss: 0.00046771
Iteration 15/25 | Loss: 0.00046771
Iteration 16/25 | Loss: 0.00046771
Iteration 17/25 | Loss: 0.00046771
Iteration 18/25 | Loss: 0.00046771
Iteration 19/25 | Loss: 0.00046771
Iteration 20/25 | Loss: 0.00046771
Iteration 21/25 | Loss: 0.00046771
Iteration 22/25 | Loss: 0.00046771
Iteration 23/25 | Loss: 0.00046771
Iteration 24/25 | Loss: 0.00046771
Iteration 25/25 | Loss: 0.00046771

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046771
Iteration 2/1000 | Loss: 0.00003033
Iteration 3/1000 | Loss: 0.00002292
Iteration 4/1000 | Loss: 0.00002129
Iteration 5/1000 | Loss: 0.00002042
Iteration 6/1000 | Loss: 0.00002004
Iteration 7/1000 | Loss: 0.00001978
Iteration 8/1000 | Loss: 0.00001956
Iteration 9/1000 | Loss: 0.00001947
Iteration 10/1000 | Loss: 0.00001938
Iteration 11/1000 | Loss: 0.00001934
Iteration 12/1000 | Loss: 0.00001933
Iteration 13/1000 | Loss: 0.00001933
Iteration 14/1000 | Loss: 0.00001928
Iteration 15/1000 | Loss: 0.00001928
Iteration 16/1000 | Loss: 0.00001927
Iteration 17/1000 | Loss: 0.00001921
Iteration 18/1000 | Loss: 0.00001916
Iteration 19/1000 | Loss: 0.00001914
Iteration 20/1000 | Loss: 0.00001914
Iteration 21/1000 | Loss: 0.00001909
Iteration 22/1000 | Loss: 0.00001908
Iteration 23/1000 | Loss: 0.00001904
Iteration 24/1000 | Loss: 0.00001904
Iteration 25/1000 | Loss: 0.00001903
Iteration 26/1000 | Loss: 0.00001903
Iteration 27/1000 | Loss: 0.00001902
Iteration 28/1000 | Loss: 0.00001901
Iteration 29/1000 | Loss: 0.00001901
Iteration 30/1000 | Loss: 0.00001901
Iteration 31/1000 | Loss: 0.00001901
Iteration 32/1000 | Loss: 0.00001901
Iteration 33/1000 | Loss: 0.00001901
Iteration 34/1000 | Loss: 0.00001900
Iteration 35/1000 | Loss: 0.00001900
Iteration 36/1000 | Loss: 0.00001900
Iteration 37/1000 | Loss: 0.00001899
Iteration 38/1000 | Loss: 0.00001899
Iteration 39/1000 | Loss: 0.00001899
Iteration 40/1000 | Loss: 0.00001899
Iteration 41/1000 | Loss: 0.00001899
Iteration 42/1000 | Loss: 0.00001899
Iteration 43/1000 | Loss: 0.00001899
Iteration 44/1000 | Loss: 0.00001898
Iteration 45/1000 | Loss: 0.00001898
Iteration 46/1000 | Loss: 0.00001898
Iteration 47/1000 | Loss: 0.00001897
Iteration 48/1000 | Loss: 0.00001897
Iteration 49/1000 | Loss: 0.00001896
Iteration 50/1000 | Loss: 0.00001896
Iteration 51/1000 | Loss: 0.00001896
Iteration 52/1000 | Loss: 0.00001896
Iteration 53/1000 | Loss: 0.00001896
Iteration 54/1000 | Loss: 0.00001896
Iteration 55/1000 | Loss: 0.00001896
Iteration 56/1000 | Loss: 0.00001896
Iteration 57/1000 | Loss: 0.00001896
Iteration 58/1000 | Loss: 0.00001895
Iteration 59/1000 | Loss: 0.00001895
Iteration 60/1000 | Loss: 0.00001895
Iteration 61/1000 | Loss: 0.00001895
Iteration 62/1000 | Loss: 0.00001895
Iteration 63/1000 | Loss: 0.00001894
Iteration 64/1000 | Loss: 0.00001893
Iteration 65/1000 | Loss: 0.00001893
Iteration 66/1000 | Loss: 0.00001892
Iteration 67/1000 | Loss: 0.00001892
Iteration 68/1000 | Loss: 0.00001892
Iteration 69/1000 | Loss: 0.00001891
Iteration 70/1000 | Loss: 0.00001891
Iteration 71/1000 | Loss: 0.00001891
Iteration 72/1000 | Loss: 0.00001891
Iteration 73/1000 | Loss: 0.00001891
Iteration 74/1000 | Loss: 0.00001889
Iteration 75/1000 | Loss: 0.00001889
Iteration 76/1000 | Loss: 0.00001889
Iteration 77/1000 | Loss: 0.00001889
Iteration 78/1000 | Loss: 0.00001889
Iteration 79/1000 | Loss: 0.00001888
Iteration 80/1000 | Loss: 0.00001888
Iteration 81/1000 | Loss: 0.00001888
Iteration 82/1000 | Loss: 0.00001888
Iteration 83/1000 | Loss: 0.00001888
Iteration 84/1000 | Loss: 0.00001888
Iteration 85/1000 | Loss: 0.00001887
Iteration 86/1000 | Loss: 0.00001887
Iteration 87/1000 | Loss: 0.00001886
Iteration 88/1000 | Loss: 0.00001886
Iteration 89/1000 | Loss: 0.00001883
Iteration 90/1000 | Loss: 0.00001883
Iteration 91/1000 | Loss: 0.00001883
Iteration 92/1000 | Loss: 0.00001882
Iteration 93/1000 | Loss: 0.00001882
Iteration 94/1000 | Loss: 0.00001882
Iteration 95/1000 | Loss: 0.00001880
Iteration 96/1000 | Loss: 0.00001879
Iteration 97/1000 | Loss: 0.00001879
Iteration 98/1000 | Loss: 0.00001878
Iteration 99/1000 | Loss: 0.00001878
Iteration 100/1000 | Loss: 0.00001878
Iteration 101/1000 | Loss: 0.00001878
Iteration 102/1000 | Loss: 0.00001877
Iteration 103/1000 | Loss: 0.00001876
Iteration 104/1000 | Loss: 0.00001876
Iteration 105/1000 | Loss: 0.00001876
Iteration 106/1000 | Loss: 0.00001874
Iteration 107/1000 | Loss: 0.00001874
Iteration 108/1000 | Loss: 0.00001874
Iteration 109/1000 | Loss: 0.00001874
Iteration 110/1000 | Loss: 0.00001873
Iteration 111/1000 | Loss: 0.00019383
Iteration 112/1000 | Loss: 0.00018420
Iteration 113/1000 | Loss: 0.00002682
Iteration 114/1000 | Loss: 0.00002432
Iteration 115/1000 | Loss: 0.00002299
Iteration 116/1000 | Loss: 0.00002204
Iteration 117/1000 | Loss: 0.00002155
Iteration 118/1000 | Loss: 0.00002103
Iteration 119/1000 | Loss: 0.00002070
Iteration 120/1000 | Loss: 0.00002037
Iteration 121/1000 | Loss: 0.00002007
Iteration 122/1000 | Loss: 0.00001985
Iteration 123/1000 | Loss: 0.00001966
Iteration 124/1000 | Loss: 0.00001949
Iteration 125/1000 | Loss: 0.00001932
Iteration 126/1000 | Loss: 0.00001899
Iteration 127/1000 | Loss: 0.00022319
Iteration 128/1000 | Loss: 0.00009980
Iteration 129/1000 | Loss: 0.00002034
Iteration 130/1000 | Loss: 0.00001883
Iteration 131/1000 | Loss: 0.00022632
Iteration 132/1000 | Loss: 0.00011235
Iteration 133/1000 | Loss: 0.00021635
Iteration 134/1000 | Loss: 0.00008898
Iteration 135/1000 | Loss: 0.00002532
Iteration 136/1000 | Loss: 0.00002271
Iteration 137/1000 | Loss: 0.00035839
Iteration 138/1000 | Loss: 0.00003541
Iteration 139/1000 | Loss: 0.00002417
Iteration 140/1000 | Loss: 0.00001960
Iteration 141/1000 | Loss: 0.00001913
Iteration 142/1000 | Loss: 0.00001865
Iteration 143/1000 | Loss: 0.00001838
Iteration 144/1000 | Loss: 0.00001820
Iteration 145/1000 | Loss: 0.00002743
Iteration 146/1000 | Loss: 0.00002431
Iteration 147/1000 | Loss: 0.00002728
Iteration 148/1000 | Loss: 0.00001975
Iteration 149/1000 | Loss: 0.00001884
Iteration 150/1000 | Loss: 0.00001839
Iteration 151/1000 | Loss: 0.00001802
Iteration 152/1000 | Loss: 0.00001779
Iteration 153/1000 | Loss: 0.00001761
Iteration 154/1000 | Loss: 0.00001754
Iteration 155/1000 | Loss: 0.00001751
Iteration 156/1000 | Loss: 0.00001751
Iteration 157/1000 | Loss: 0.00001751
Iteration 158/1000 | Loss: 0.00001751
Iteration 159/1000 | Loss: 0.00001750
Iteration 160/1000 | Loss: 0.00001750
Iteration 161/1000 | Loss: 0.00001750
Iteration 162/1000 | Loss: 0.00001750
Iteration 163/1000 | Loss: 0.00001749
Iteration 164/1000 | Loss: 0.00001749
Iteration 165/1000 | Loss: 0.00001748
Iteration 166/1000 | Loss: 0.00001748
Iteration 167/1000 | Loss: 0.00001748
Iteration 168/1000 | Loss: 0.00001747
Iteration 169/1000 | Loss: 0.00001747
Iteration 170/1000 | Loss: 0.00001747
Iteration 171/1000 | Loss: 0.00001746
Iteration 172/1000 | Loss: 0.00001746
Iteration 173/1000 | Loss: 0.00001746
Iteration 174/1000 | Loss: 0.00001746
Iteration 175/1000 | Loss: 0.00001746
Iteration 176/1000 | Loss: 0.00001746
Iteration 177/1000 | Loss: 0.00001746
Iteration 178/1000 | Loss: 0.00001746
Iteration 179/1000 | Loss: 0.00001746
Iteration 180/1000 | Loss: 0.00001746
Iteration 181/1000 | Loss: 0.00001746
Iteration 182/1000 | Loss: 0.00001746
Iteration 183/1000 | Loss: 0.00001746
Iteration 184/1000 | Loss: 0.00001746
Iteration 185/1000 | Loss: 0.00001746
Iteration 186/1000 | Loss: 0.00001746
Iteration 187/1000 | Loss: 0.00001746
Iteration 188/1000 | Loss: 0.00001746
Iteration 189/1000 | Loss: 0.00001746
Iteration 190/1000 | Loss: 0.00001746
Iteration 191/1000 | Loss: 0.00001746
Iteration 192/1000 | Loss: 0.00001746
Iteration 193/1000 | Loss: 0.00001746
Iteration 194/1000 | Loss: 0.00001746
Iteration 195/1000 | Loss: 0.00001746
Iteration 196/1000 | Loss: 0.00001746
Iteration 197/1000 | Loss: 0.00001746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.7455076886108145e-05, 1.7455076886108145e-05, 1.7455076886108145e-05, 1.7455076886108145e-05, 1.7455076886108145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7455076886108145e-05

Optimization complete. Final v2v error: 3.413700819015503 mm

Highest mean error: 5.2923102378845215 mm for frame 231

Lowest mean error: 2.9506337642669678 mm for frame 101

Saving results

Total time: 132.5400149822235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00535113
Iteration 2/25 | Loss: 0.00135398
Iteration 3/25 | Loss: 0.00107876
Iteration 4/25 | Loss: 0.00103890
Iteration 5/25 | Loss: 0.00103260
Iteration 6/25 | Loss: 0.00103040
Iteration 7/25 | Loss: 0.00103025
Iteration 8/25 | Loss: 0.00103025
Iteration 9/25 | Loss: 0.00103025
Iteration 10/25 | Loss: 0.00103025
Iteration 11/25 | Loss: 0.00103025
Iteration 12/25 | Loss: 0.00103025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010302523151040077, 0.0010302523151040077, 0.0010302523151040077, 0.0010302523151040077, 0.0010302523151040077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010302523151040077

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.70665956
Iteration 2/25 | Loss: 0.00037895
Iteration 3/25 | Loss: 0.00037895
Iteration 4/25 | Loss: 0.00037895
Iteration 5/25 | Loss: 0.00037895
Iteration 6/25 | Loss: 0.00037895
Iteration 7/25 | Loss: 0.00037895
Iteration 8/25 | Loss: 0.00037895
Iteration 9/25 | Loss: 0.00037895
Iteration 10/25 | Loss: 0.00037895
Iteration 11/25 | Loss: 0.00037895
Iteration 12/25 | Loss: 0.00037895
Iteration 13/25 | Loss: 0.00037895
Iteration 14/25 | Loss: 0.00037895
Iteration 15/25 | Loss: 0.00037895
Iteration 16/25 | Loss: 0.00037895
Iteration 17/25 | Loss: 0.00037895
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003789513430092484, 0.0003789513430092484, 0.0003789513430092484, 0.0003789513430092484, 0.0003789513430092484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003789513430092484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037895
Iteration 2/1000 | Loss: 0.00003667
Iteration 3/1000 | Loss: 0.00002063
Iteration 4/1000 | Loss: 0.00001772
Iteration 5/1000 | Loss: 0.00001635
Iteration 6/1000 | Loss: 0.00001560
Iteration 7/1000 | Loss: 0.00001515
Iteration 8/1000 | Loss: 0.00001483
Iteration 9/1000 | Loss: 0.00001458
Iteration 10/1000 | Loss: 0.00001451
Iteration 11/1000 | Loss: 0.00001451
Iteration 12/1000 | Loss: 0.00001450
Iteration 13/1000 | Loss: 0.00001439
Iteration 14/1000 | Loss: 0.00001438
Iteration 15/1000 | Loss: 0.00001432
Iteration 16/1000 | Loss: 0.00001423
Iteration 17/1000 | Loss: 0.00001423
Iteration 18/1000 | Loss: 0.00001422
Iteration 19/1000 | Loss: 0.00001421
Iteration 20/1000 | Loss: 0.00001420
Iteration 21/1000 | Loss: 0.00001420
Iteration 22/1000 | Loss: 0.00001419
Iteration 23/1000 | Loss: 0.00001419
Iteration 24/1000 | Loss: 0.00001417
Iteration 25/1000 | Loss: 0.00001417
Iteration 26/1000 | Loss: 0.00001417
Iteration 27/1000 | Loss: 0.00001417
Iteration 28/1000 | Loss: 0.00001417
Iteration 29/1000 | Loss: 0.00001416
Iteration 30/1000 | Loss: 0.00001416
Iteration 31/1000 | Loss: 0.00001416
Iteration 32/1000 | Loss: 0.00001415
Iteration 33/1000 | Loss: 0.00001415
Iteration 34/1000 | Loss: 0.00001415
Iteration 35/1000 | Loss: 0.00001415
Iteration 36/1000 | Loss: 0.00001415
Iteration 37/1000 | Loss: 0.00001414
Iteration 38/1000 | Loss: 0.00001414
Iteration 39/1000 | Loss: 0.00001414
Iteration 40/1000 | Loss: 0.00001413
Iteration 41/1000 | Loss: 0.00001413
Iteration 42/1000 | Loss: 0.00001413
Iteration 43/1000 | Loss: 0.00001413
Iteration 44/1000 | Loss: 0.00001413
Iteration 45/1000 | Loss: 0.00001413
Iteration 46/1000 | Loss: 0.00001413
Iteration 47/1000 | Loss: 0.00001413
Iteration 48/1000 | Loss: 0.00001412
Iteration 49/1000 | Loss: 0.00001412
Iteration 50/1000 | Loss: 0.00001412
Iteration 51/1000 | Loss: 0.00001412
Iteration 52/1000 | Loss: 0.00001412
Iteration 53/1000 | Loss: 0.00001412
Iteration 54/1000 | Loss: 0.00001412
Iteration 55/1000 | Loss: 0.00001412
Iteration 56/1000 | Loss: 0.00001411
Iteration 57/1000 | Loss: 0.00001411
Iteration 58/1000 | Loss: 0.00001411
Iteration 59/1000 | Loss: 0.00001411
Iteration 60/1000 | Loss: 0.00001411
Iteration 61/1000 | Loss: 0.00001411
Iteration 62/1000 | Loss: 0.00001411
Iteration 63/1000 | Loss: 0.00001411
Iteration 64/1000 | Loss: 0.00001411
Iteration 65/1000 | Loss: 0.00001410
Iteration 66/1000 | Loss: 0.00001410
Iteration 67/1000 | Loss: 0.00001410
Iteration 68/1000 | Loss: 0.00001410
Iteration 69/1000 | Loss: 0.00001409
Iteration 70/1000 | Loss: 0.00001409
Iteration 71/1000 | Loss: 0.00001409
Iteration 72/1000 | Loss: 0.00001409
Iteration 73/1000 | Loss: 0.00001409
Iteration 74/1000 | Loss: 0.00001409
Iteration 75/1000 | Loss: 0.00001409
Iteration 76/1000 | Loss: 0.00001409
Iteration 77/1000 | Loss: 0.00001409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.4089298019825947e-05, 1.4089298019825947e-05, 1.4089298019825947e-05, 1.4089298019825947e-05, 1.4089298019825947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4089298019825947e-05

Optimization complete. Final v2v error: 3.218574047088623 mm

Highest mean error: 3.43056321144104 mm for frame 87

Lowest mean error: 3.019312858581543 mm for frame 67

Saving results

Total time: 29.08941102027893
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01137547
Iteration 2/25 | Loss: 0.00195226
Iteration 3/25 | Loss: 0.00139002
Iteration 4/25 | Loss: 0.00111710
Iteration 5/25 | Loss: 0.00105771
Iteration 6/25 | Loss: 0.00101694
Iteration 7/25 | Loss: 0.00102038
Iteration 8/25 | Loss: 0.00101701
Iteration 9/25 | Loss: 0.00100696
Iteration 10/25 | Loss: 0.00099577
Iteration 11/25 | Loss: 0.00098874
Iteration 12/25 | Loss: 0.00096789
Iteration 13/25 | Loss: 0.00096503
Iteration 14/25 | Loss: 0.00095893
Iteration 15/25 | Loss: 0.00096293
Iteration 16/25 | Loss: 0.00096123
Iteration 17/25 | Loss: 0.00096066
Iteration 18/25 | Loss: 0.00095569
Iteration 19/25 | Loss: 0.00095356
Iteration 20/25 | Loss: 0.00095228
Iteration 21/25 | Loss: 0.00095208
Iteration 22/25 | Loss: 0.00095202
Iteration 23/25 | Loss: 0.00095202
Iteration 24/25 | Loss: 0.00095202
Iteration 25/25 | Loss: 0.00095202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37834811
Iteration 2/25 | Loss: 0.00079283
Iteration 3/25 | Loss: 0.00079283
Iteration 4/25 | Loss: 0.00079283
Iteration 5/25 | Loss: 0.00079282
Iteration 6/25 | Loss: 0.00079282
Iteration 7/25 | Loss: 0.00079282
Iteration 8/25 | Loss: 0.00079282
Iteration 9/25 | Loss: 0.00079282
Iteration 10/25 | Loss: 0.00079282
Iteration 11/25 | Loss: 0.00079282
Iteration 12/25 | Loss: 0.00079282
Iteration 13/25 | Loss: 0.00079282
Iteration 14/25 | Loss: 0.00079282
Iteration 15/25 | Loss: 0.00079282
Iteration 16/25 | Loss: 0.00079282
Iteration 17/25 | Loss: 0.00079282
Iteration 18/25 | Loss: 0.00079282
Iteration 19/25 | Loss: 0.00079282
Iteration 20/25 | Loss: 0.00079282
Iteration 21/25 | Loss: 0.00079282
Iteration 22/25 | Loss: 0.00079282
Iteration 23/25 | Loss: 0.00079282
Iteration 24/25 | Loss: 0.00079282
Iteration 25/25 | Loss: 0.00079282

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079282
Iteration 2/1000 | Loss: 0.00038079
Iteration 3/1000 | Loss: 0.00005584
Iteration 4/1000 | Loss: 0.00003145
Iteration 5/1000 | Loss: 0.00003612
Iteration 6/1000 | Loss: 0.00002266
Iteration 7/1000 | Loss: 0.00008259
Iteration 8/1000 | Loss: 0.00054899
Iteration 9/1000 | Loss: 0.00010456
Iteration 10/1000 | Loss: 0.00135073
Iteration 11/1000 | Loss: 0.00004287
Iteration 12/1000 | Loss: 0.00002415
Iteration 13/1000 | Loss: 0.00003478
Iteration 14/1000 | Loss: 0.00002649
Iteration 15/1000 | Loss: 0.00003378
Iteration 16/1000 | Loss: 0.00006104
Iteration 17/1000 | Loss: 0.00002358
Iteration 18/1000 | Loss: 0.00001954
Iteration 19/1000 | Loss: 0.00004537
Iteration 20/1000 | Loss: 0.00002350
Iteration 21/1000 | Loss: 0.00006001
Iteration 22/1000 | Loss: 0.00008641
Iteration 23/1000 | Loss: 0.00007973
Iteration 24/1000 | Loss: 0.00007882
Iteration 25/1000 | Loss: 0.00005463
Iteration 26/1000 | Loss: 0.00007735
Iteration 27/1000 | Loss: 0.00005248
Iteration 28/1000 | Loss: 0.00007915
Iteration 29/1000 | Loss: 0.00008471
Iteration 30/1000 | Loss: 0.00007790
Iteration 31/1000 | Loss: 0.00004441
Iteration 32/1000 | Loss: 0.00007478
Iteration 33/1000 | Loss: 0.00005414
Iteration 34/1000 | Loss: 0.00007205
Iteration 35/1000 | Loss: 0.00004217
Iteration 36/1000 | Loss: 0.00006993
Iteration 37/1000 | Loss: 0.00002960
Iteration 38/1000 | Loss: 0.00024748
Iteration 39/1000 | Loss: 0.00011400
Iteration 40/1000 | Loss: 0.00007034
Iteration 41/1000 | Loss: 0.00024896
Iteration 42/1000 | Loss: 0.00010390
Iteration 43/1000 | Loss: 0.00019405
Iteration 44/1000 | Loss: 0.00022861
Iteration 45/1000 | Loss: 0.00003333
Iteration 46/1000 | Loss: 0.00006004
Iteration 47/1000 | Loss: 0.00002749
Iteration 48/1000 | Loss: 0.00022090
Iteration 49/1000 | Loss: 0.00008748
Iteration 50/1000 | Loss: 0.00013188
Iteration 51/1000 | Loss: 0.00014926
Iteration 52/1000 | Loss: 0.00003235
Iteration 53/1000 | Loss: 0.00002705
Iteration 54/1000 | Loss: 0.00002727
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00003161
Iteration 57/1000 | Loss: 0.00001918
Iteration 58/1000 | Loss: 0.00003375
Iteration 59/1000 | Loss: 0.00023244
Iteration 60/1000 | Loss: 0.00007952
Iteration 61/1000 | Loss: 0.00002612
Iteration 62/1000 | Loss: 0.00001883
Iteration 63/1000 | Loss: 0.00005373
Iteration 64/1000 | Loss: 0.00003041
Iteration 65/1000 | Loss: 0.00004527
Iteration 66/1000 | Loss: 0.00003645
Iteration 67/1000 | Loss: 0.00002653
Iteration 68/1000 | Loss: 0.00004439
Iteration 69/1000 | Loss: 0.00003016
Iteration 70/1000 | Loss: 0.00002173
Iteration 71/1000 | Loss: 0.00023356
Iteration 72/1000 | Loss: 0.00026263
Iteration 73/1000 | Loss: 0.00019893
Iteration 74/1000 | Loss: 0.00004759
Iteration 75/1000 | Loss: 0.00002774
Iteration 76/1000 | Loss: 0.00023367
Iteration 77/1000 | Loss: 0.00019106
Iteration 78/1000 | Loss: 0.00016978
Iteration 79/1000 | Loss: 0.00005660
Iteration 80/1000 | Loss: 0.00003131
Iteration 81/1000 | Loss: 0.00002958
Iteration 82/1000 | Loss: 0.00002322
Iteration 83/1000 | Loss: 0.00001859
Iteration 84/1000 | Loss: 0.00001855
Iteration 85/1000 | Loss: 0.00002945
Iteration 86/1000 | Loss: 0.00001811
Iteration 87/1000 | Loss: 0.00020492
Iteration 88/1000 | Loss: 0.00006325
Iteration 89/1000 | Loss: 0.00002092
Iteration 90/1000 | Loss: 0.00001795
Iteration 91/1000 | Loss: 0.00001794
Iteration 92/1000 | Loss: 0.00001791
Iteration 93/1000 | Loss: 0.00026919
Iteration 94/1000 | Loss: 0.00021664
Iteration 95/1000 | Loss: 0.00015474
Iteration 96/1000 | Loss: 0.00001872
Iteration 97/1000 | Loss: 0.00019199
Iteration 98/1000 | Loss: 0.00008623
Iteration 99/1000 | Loss: 0.00025773
Iteration 100/1000 | Loss: 0.00054196
Iteration 101/1000 | Loss: 0.00010207
Iteration 102/1000 | Loss: 0.00001940
Iteration 103/1000 | Loss: 0.00001906
Iteration 104/1000 | Loss: 0.00005542
Iteration 105/1000 | Loss: 0.00002813
Iteration 106/1000 | Loss: 0.00021540
Iteration 107/1000 | Loss: 0.00005676
Iteration 108/1000 | Loss: 0.00005523
Iteration 109/1000 | Loss: 0.00002578
Iteration 110/1000 | Loss: 0.00005455
Iteration 111/1000 | Loss: 0.00002361
Iteration 112/1000 | Loss: 0.00005778
Iteration 113/1000 | Loss: 0.00002278
Iteration 114/1000 | Loss: 0.00003828
Iteration 115/1000 | Loss: 0.00001997
Iteration 116/1000 | Loss: 0.00005034
Iteration 117/1000 | Loss: 0.00002148
Iteration 118/1000 | Loss: 0.00005559
Iteration 119/1000 | Loss: 0.00003681
Iteration 120/1000 | Loss: 0.00005267
Iteration 121/1000 | Loss: 0.00002640
Iteration 122/1000 | Loss: 0.00004886
Iteration 123/1000 | Loss: 0.00002397
Iteration 124/1000 | Loss: 0.00004232
Iteration 125/1000 | Loss: 0.00002161
Iteration 126/1000 | Loss: 0.00004078
Iteration 127/1000 | Loss: 0.00002917
Iteration 128/1000 | Loss: 0.00027645
Iteration 129/1000 | Loss: 0.00004837
Iteration 130/1000 | Loss: 0.00003526
Iteration 131/1000 | Loss: 0.00002751
Iteration 132/1000 | Loss: 0.00004579
Iteration 133/1000 | Loss: 0.00002119
Iteration 134/1000 | Loss: 0.00004495
Iteration 135/1000 | Loss: 0.00001995
Iteration 136/1000 | Loss: 0.00002837
Iteration 137/1000 | Loss: 0.00001902
Iteration 138/1000 | Loss: 0.00002254
Iteration 139/1000 | Loss: 0.00001892
Iteration 140/1000 | Loss: 0.00001800
Iteration 141/1000 | Loss: 0.00001800
Iteration 142/1000 | Loss: 0.00001800
Iteration 143/1000 | Loss: 0.00001800
Iteration 144/1000 | Loss: 0.00001800
Iteration 145/1000 | Loss: 0.00001800
Iteration 146/1000 | Loss: 0.00001800
Iteration 147/1000 | Loss: 0.00001800
Iteration 148/1000 | Loss: 0.00001800
Iteration 149/1000 | Loss: 0.00001799
Iteration 150/1000 | Loss: 0.00001799
Iteration 151/1000 | Loss: 0.00001799
Iteration 152/1000 | Loss: 0.00001799
Iteration 153/1000 | Loss: 0.00001799
Iteration 154/1000 | Loss: 0.00001799
Iteration 155/1000 | Loss: 0.00001798
Iteration 156/1000 | Loss: 0.00001777
Iteration 157/1000 | Loss: 0.00001770
Iteration 158/1000 | Loss: 0.00001764
Iteration 159/1000 | Loss: 0.00001764
Iteration 160/1000 | Loss: 0.00001763
Iteration 161/1000 | Loss: 0.00001761
Iteration 162/1000 | Loss: 0.00021239
Iteration 163/1000 | Loss: 0.00005963
Iteration 164/1000 | Loss: 0.00002038
Iteration 165/1000 | Loss: 0.00002002
Iteration 166/1000 | Loss: 0.00027164
Iteration 167/1000 | Loss: 0.00005435
Iteration 168/1000 | Loss: 0.00001904
Iteration 169/1000 | Loss: 0.00002473
Iteration 170/1000 | Loss: 0.00001762
Iteration 171/1000 | Loss: 0.00002066
Iteration 172/1000 | Loss: 0.00001925
Iteration 173/1000 | Loss: 0.00001776
Iteration 174/1000 | Loss: 0.00020188
Iteration 175/1000 | Loss: 0.00005469
Iteration 176/1000 | Loss: 0.00011574
Iteration 177/1000 | Loss: 0.00002502
Iteration 178/1000 | Loss: 0.00002066
Iteration 179/1000 | Loss: 0.00001751
Iteration 180/1000 | Loss: 0.00020072
Iteration 181/1000 | Loss: 0.00013213
Iteration 182/1000 | Loss: 0.00001909
Iteration 183/1000 | Loss: 0.00002121
Iteration 184/1000 | Loss: 0.00001841
Iteration 185/1000 | Loss: 0.00001755
Iteration 186/1000 | Loss: 0.00001753
Iteration 187/1000 | Loss: 0.00001753
Iteration 188/1000 | Loss: 0.00001753
Iteration 189/1000 | Loss: 0.00001752
Iteration 190/1000 | Loss: 0.00001752
Iteration 191/1000 | Loss: 0.00001752
Iteration 192/1000 | Loss: 0.00001751
Iteration 193/1000 | Loss: 0.00001751
Iteration 194/1000 | Loss: 0.00001751
Iteration 195/1000 | Loss: 0.00001750
Iteration 196/1000 | Loss: 0.00001750
Iteration 197/1000 | Loss: 0.00001749
Iteration 198/1000 | Loss: 0.00001748
Iteration 199/1000 | Loss: 0.00002068
Iteration 200/1000 | Loss: 0.00002020
Iteration 201/1000 | Loss: 0.00001745
Iteration 202/1000 | Loss: 0.00001745
Iteration 203/1000 | Loss: 0.00001745
Iteration 204/1000 | Loss: 0.00001745
Iteration 205/1000 | Loss: 0.00001745
Iteration 206/1000 | Loss: 0.00001745
Iteration 207/1000 | Loss: 0.00001745
Iteration 208/1000 | Loss: 0.00001745
Iteration 209/1000 | Loss: 0.00001745
Iteration 210/1000 | Loss: 0.00001745
Iteration 211/1000 | Loss: 0.00001744
Iteration 212/1000 | Loss: 0.00019329
Iteration 213/1000 | Loss: 0.00006948
Iteration 214/1000 | Loss: 0.00001773
Iteration 215/1000 | Loss: 0.00001999
Iteration 216/1000 | Loss: 0.00001743
Iteration 217/1000 | Loss: 0.00001743
Iteration 218/1000 | Loss: 0.00001743
Iteration 219/1000 | Loss: 0.00001742
Iteration 220/1000 | Loss: 0.00001742
Iteration 221/1000 | Loss: 0.00001742
Iteration 222/1000 | Loss: 0.00001742
Iteration 223/1000 | Loss: 0.00019485
Iteration 224/1000 | Loss: 0.00005554
Iteration 225/1000 | Loss: 0.00001940
Iteration 226/1000 | Loss: 0.00001903
Iteration 227/1000 | Loss: 0.00001778
Iteration 228/1000 | Loss: 0.00001749
Iteration 229/1000 | Loss: 0.00001749
Iteration 230/1000 | Loss: 0.00001749
Iteration 231/1000 | Loss: 0.00001749
Iteration 232/1000 | Loss: 0.00001749
Iteration 233/1000 | Loss: 0.00001749
Iteration 234/1000 | Loss: 0.00001749
Iteration 235/1000 | Loss: 0.00001749
Iteration 236/1000 | Loss: 0.00001748
Iteration 237/1000 | Loss: 0.00001748
Iteration 238/1000 | Loss: 0.00001748
Iteration 239/1000 | Loss: 0.00001748
Iteration 240/1000 | Loss: 0.00001748
Iteration 241/1000 | Loss: 0.00001748
Iteration 242/1000 | Loss: 0.00001746
Iteration 243/1000 | Loss: 0.00001746
Iteration 244/1000 | Loss: 0.00001745
Iteration 245/1000 | Loss: 0.00001748
Iteration 246/1000 | Loss: 0.00001745
Iteration 247/1000 | Loss: 0.00001745
Iteration 248/1000 | Loss: 0.00001745
Iteration 249/1000 | Loss: 0.00001744
Iteration 250/1000 | Loss: 0.00001744
Iteration 251/1000 | Loss: 0.00001744
Iteration 252/1000 | Loss: 0.00001745
Iteration 253/1000 | Loss: 0.00001744
Iteration 254/1000 | Loss: 0.00001744
Iteration 255/1000 | Loss: 0.00001744
Iteration 256/1000 | Loss: 0.00001744
Iteration 257/1000 | Loss: 0.00001744
Iteration 258/1000 | Loss: 0.00001744
Iteration 259/1000 | Loss: 0.00001744
Iteration 260/1000 | Loss: 0.00001744
Iteration 261/1000 | Loss: 0.00001744
Iteration 262/1000 | Loss: 0.00001743
Iteration 263/1000 | Loss: 0.00001743
Iteration 264/1000 | Loss: 0.00001743
Iteration 265/1000 | Loss: 0.00001743
Iteration 266/1000 | Loss: 0.00001743
Iteration 267/1000 | Loss: 0.00001743
Iteration 268/1000 | Loss: 0.00001742
Iteration 269/1000 | Loss: 0.00001742
Iteration 270/1000 | Loss: 0.00001742
Iteration 271/1000 | Loss: 0.00001741
Iteration 272/1000 | Loss: 0.00001741
Iteration 273/1000 | Loss: 0.00001741
Iteration 274/1000 | Loss: 0.00001740
Iteration 275/1000 | Loss: 0.00001740
Iteration 276/1000 | Loss: 0.00001740
Iteration 277/1000 | Loss: 0.00001739
Iteration 278/1000 | Loss: 0.00019803
Iteration 279/1000 | Loss: 0.00021056
Iteration 280/1000 | Loss: 0.00056791
Iteration 281/1000 | Loss: 0.00028565
Iteration 282/1000 | Loss: 0.00005396
Iteration 283/1000 | Loss: 0.00004111
Iteration 284/1000 | Loss: 0.00002302
Iteration 285/1000 | Loss: 0.00002141
Iteration 286/1000 | Loss: 0.00001981
Iteration 287/1000 | Loss: 0.00019764
Iteration 288/1000 | Loss: 0.00006379
Iteration 289/1000 | Loss: 0.00021944
Iteration 290/1000 | Loss: 0.00016953
Iteration 291/1000 | Loss: 0.00007508
Iteration 292/1000 | Loss: 0.00002061
Iteration 293/1000 | Loss: 0.00020011
Iteration 294/1000 | Loss: 0.00005293
Iteration 295/1000 | Loss: 0.00004232
Iteration 296/1000 | Loss: 0.00002959
Iteration 297/1000 | Loss: 0.00001953
Iteration 298/1000 | Loss: 0.00020823
Iteration 299/1000 | Loss: 0.00005513
Iteration 300/1000 | Loss: 0.00001780
Iteration 301/1000 | Loss: 0.00002002
Iteration 302/1000 | Loss: 0.00001824
Iteration 303/1000 | Loss: 0.00001738
Iteration 304/1000 | Loss: 0.00001738
Iteration 305/1000 | Loss: 0.00001738
Iteration 306/1000 | Loss: 0.00001738
Iteration 307/1000 | Loss: 0.00001737
Iteration 308/1000 | Loss: 0.00001737
Iteration 309/1000 | Loss: 0.00001737
Iteration 310/1000 | Loss: 0.00001737
Iteration 311/1000 | Loss: 0.00001737
Iteration 312/1000 | Loss: 0.00001736
Iteration 313/1000 | Loss: 0.00019679
Iteration 314/1000 | Loss: 0.00004607
Iteration 315/1000 | Loss: 0.00024174
Iteration 316/1000 | Loss: 0.00022059
Iteration 317/1000 | Loss: 0.00003083
Iteration 318/1000 | Loss: 0.00021457
Iteration 319/1000 | Loss: 0.00027252
Iteration 320/1000 | Loss: 0.00012900
Iteration 321/1000 | Loss: 0.00002329
Iteration 322/1000 | Loss: 0.00025813
Iteration 323/1000 | Loss: 0.00012720
Iteration 324/1000 | Loss: 0.00003328
Iteration 325/1000 | Loss: 0.00002258
Iteration 326/1000 | Loss: 0.00001768
Iteration 327/1000 | Loss: 0.00001760
Iteration 328/1000 | Loss: 0.00001752
Iteration 329/1000 | Loss: 0.00001748
Iteration 330/1000 | Loss: 0.00003032
Iteration 331/1000 | Loss: 0.00001888
Iteration 332/1000 | Loss: 0.00001740
Iteration 333/1000 | Loss: 0.00001740
Iteration 334/1000 | Loss: 0.00001740
Iteration 335/1000 | Loss: 0.00001740
Iteration 336/1000 | Loss: 0.00001740
Iteration 337/1000 | Loss: 0.00001740
Iteration 338/1000 | Loss: 0.00001740
Iteration 339/1000 | Loss: 0.00001740
Iteration 340/1000 | Loss: 0.00001740
Iteration 341/1000 | Loss: 0.00001740
Iteration 342/1000 | Loss: 0.00001740
Iteration 343/1000 | Loss: 0.00001740
Iteration 344/1000 | Loss: 0.00001740
Iteration 345/1000 | Loss: 0.00001740
Iteration 346/1000 | Loss: 0.00001740
Iteration 347/1000 | Loss: 0.00001739
Iteration 348/1000 | Loss: 0.00001739
Iteration 349/1000 | Loss: 0.00001739
Iteration 350/1000 | Loss: 0.00001738
Iteration 351/1000 | Loss: 0.00001738
Iteration 352/1000 | Loss: 0.00001738
Iteration 353/1000 | Loss: 0.00001738
Iteration 354/1000 | Loss: 0.00001738
Iteration 355/1000 | Loss: 0.00001738
Iteration 356/1000 | Loss: 0.00001738
Iteration 357/1000 | Loss: 0.00001737
Iteration 358/1000 | Loss: 0.00001737
Iteration 359/1000 | Loss: 0.00001737
Iteration 360/1000 | Loss: 0.00001737
Iteration 361/1000 | Loss: 0.00001737
Iteration 362/1000 | Loss: 0.00001737
Iteration 363/1000 | Loss: 0.00001737
Iteration 364/1000 | Loss: 0.00001737
Iteration 365/1000 | Loss: 0.00001737
Iteration 366/1000 | Loss: 0.00001737
Iteration 367/1000 | Loss: 0.00001737
Iteration 368/1000 | Loss: 0.00001737
Iteration 369/1000 | Loss: 0.00001737
Iteration 370/1000 | Loss: 0.00001737
Iteration 371/1000 | Loss: 0.00001737
Iteration 372/1000 | Loss: 0.00001736
Iteration 373/1000 | Loss: 0.00001736
Iteration 374/1000 | Loss: 0.00001736
Iteration 375/1000 | Loss: 0.00001736
Iteration 376/1000 | Loss: 0.00001736
Iteration 377/1000 | Loss: 0.00001736
Iteration 378/1000 | Loss: 0.00001736
Iteration 379/1000 | Loss: 0.00001736
Iteration 380/1000 | Loss: 0.00001736
Iteration 381/1000 | Loss: 0.00001736
Iteration 382/1000 | Loss: 0.00001736
Iteration 383/1000 | Loss: 0.00001736
Iteration 384/1000 | Loss: 0.00001736
Iteration 385/1000 | Loss: 0.00001736
Iteration 386/1000 | Loss: 0.00001735
Iteration 387/1000 | Loss: 0.00001735
Iteration 388/1000 | Loss: 0.00001735
Iteration 389/1000 | Loss: 0.00001735
Iteration 390/1000 | Loss: 0.00001735
Iteration 391/1000 | Loss: 0.00001735
Iteration 392/1000 | Loss: 0.00001735
Iteration 393/1000 | Loss: 0.00001735
Iteration 394/1000 | Loss: 0.00001735
Iteration 395/1000 | Loss: 0.00001735
Iteration 396/1000 | Loss: 0.00001735
Iteration 397/1000 | Loss: 0.00001735
Iteration 398/1000 | Loss: 0.00001735
Iteration 399/1000 | Loss: 0.00001735
Iteration 400/1000 | Loss: 0.00001735
Iteration 401/1000 | Loss: 0.00001735
Iteration 402/1000 | Loss: 0.00001734
Iteration 403/1000 | Loss: 0.00001734
Iteration 404/1000 | Loss: 0.00001734
Iteration 405/1000 | Loss: 0.00001734
Iteration 406/1000 | Loss: 0.00001734
Iteration 407/1000 | Loss: 0.00001734
Iteration 408/1000 | Loss: 0.00001734
Iteration 409/1000 | Loss: 0.00001734
Iteration 410/1000 | Loss: 0.00001734
Iteration 411/1000 | Loss: 0.00001734
Iteration 412/1000 | Loss: 0.00001734
Iteration 413/1000 | Loss: 0.00001734
Iteration 414/1000 | Loss: 0.00001734
Iteration 415/1000 | Loss: 0.00001734
Iteration 416/1000 | Loss: 0.00001734
Iteration 417/1000 | Loss: 0.00001734
Iteration 418/1000 | Loss: 0.00001734
Iteration 419/1000 | Loss: 0.00001734
Iteration 420/1000 | Loss: 0.00001734
Iteration 421/1000 | Loss: 0.00001733
Iteration 422/1000 | Loss: 0.00001733
Iteration 423/1000 | Loss: 0.00001733
Iteration 424/1000 | Loss: 0.00001733
Iteration 425/1000 | Loss: 0.00001732
Iteration 426/1000 | Loss: 0.00001732
Iteration 427/1000 | Loss: 0.00001732
Iteration 428/1000 | Loss: 0.00001732
Iteration 429/1000 | Loss: 0.00001732
Iteration 430/1000 | Loss: 0.00001732
Iteration 431/1000 | Loss: 0.00001732
Iteration 432/1000 | Loss: 0.00001732
Iteration 433/1000 | Loss: 0.00001732
Iteration 434/1000 | Loss: 0.00001732
Iteration 435/1000 | Loss: 0.00001732
Iteration 436/1000 | Loss: 0.00001732
Iteration 437/1000 | Loss: 0.00001732
Iteration 438/1000 | Loss: 0.00001732
Iteration 439/1000 | Loss: 0.00001732
Iteration 440/1000 | Loss: 0.00001732
Iteration 441/1000 | Loss: 0.00001732
Iteration 442/1000 | Loss: 0.00001732
Iteration 443/1000 | Loss: 0.00001732
Iteration 444/1000 | Loss: 0.00001732
Iteration 445/1000 | Loss: 0.00001732
Iteration 446/1000 | Loss: 0.00001732
Iteration 447/1000 | Loss: 0.00001732
Iteration 448/1000 | Loss: 0.00001732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 448. Stopping optimization.
Last 5 losses: [1.7315793229499832e-05, 1.7315793229499832e-05, 1.7315793229499832e-05, 1.7315793229499832e-05, 1.7315793229499832e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7315793229499832e-05

Optimization complete. Final v2v error: 3.2695865631103516 mm

Highest mean error: 8.786168098449707 mm for frame 98

Lowest mean error: 2.722480058670044 mm for frame 65

Saving results

Total time: 340.3919425010681
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01083248
Iteration 2/25 | Loss: 0.00239688
Iteration 3/25 | Loss: 0.00320196
Iteration 4/25 | Loss: 0.00187887
Iteration 5/25 | Loss: 0.00148176
Iteration 6/25 | Loss: 0.00136119
Iteration 7/25 | Loss: 0.00120956
Iteration 8/25 | Loss: 0.00109653
Iteration 9/25 | Loss: 0.00107945
Iteration 10/25 | Loss: 0.00105330
Iteration 11/25 | Loss: 0.00106364
Iteration 12/25 | Loss: 0.00103379
Iteration 13/25 | Loss: 0.00102235
Iteration 14/25 | Loss: 0.00101981
Iteration 15/25 | Loss: 0.00101304
Iteration 16/25 | Loss: 0.00100849
Iteration 17/25 | Loss: 0.00100798
Iteration 18/25 | Loss: 0.00100662
Iteration 19/25 | Loss: 0.00103168
Iteration 20/25 | Loss: 0.00100794
Iteration 21/25 | Loss: 0.00102989
Iteration 22/25 | Loss: 0.00101252
Iteration 23/25 | Loss: 0.00102101
Iteration 24/25 | Loss: 0.00100249
Iteration 25/25 | Loss: 0.00099996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39815116
Iteration 2/25 | Loss: 0.00063057
Iteration 3/25 | Loss: 0.00063056
Iteration 4/25 | Loss: 0.00063056
Iteration 5/25 | Loss: 0.00063056
Iteration 6/25 | Loss: 0.00063056
Iteration 7/25 | Loss: 0.00063056
Iteration 8/25 | Loss: 0.00063056
Iteration 9/25 | Loss: 0.00063056
Iteration 10/25 | Loss: 0.00063056
Iteration 11/25 | Loss: 0.00063056
Iteration 12/25 | Loss: 0.00063056
Iteration 13/25 | Loss: 0.00063056
Iteration 14/25 | Loss: 0.00063056
Iteration 15/25 | Loss: 0.00063056
Iteration 16/25 | Loss: 0.00063056
Iteration 17/25 | Loss: 0.00063056
Iteration 18/25 | Loss: 0.00063056
Iteration 19/25 | Loss: 0.00063056
Iteration 20/25 | Loss: 0.00063056
Iteration 21/25 | Loss: 0.00063056
Iteration 22/25 | Loss: 0.00063056
Iteration 23/25 | Loss: 0.00063056
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006305623683147132, 0.0006305623683147132, 0.0006305623683147132, 0.0006305623683147132, 0.0006305623683147132]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006305623683147132

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063056
Iteration 2/1000 | Loss: 0.00003957
Iteration 3/1000 | Loss: 0.00071917
Iteration 4/1000 | Loss: 0.00004202
Iteration 5/1000 | Loss: 0.00003145
Iteration 6/1000 | Loss: 0.00002473
Iteration 7/1000 | Loss: 0.00002351
Iteration 8/1000 | Loss: 0.00002251
Iteration 9/1000 | Loss: 0.00021087
Iteration 10/1000 | Loss: 0.00024656
Iteration 11/1000 | Loss: 0.00019341
Iteration 12/1000 | Loss: 0.00002643
Iteration 13/1000 | Loss: 0.00002186
Iteration 14/1000 | Loss: 0.00002082
Iteration 15/1000 | Loss: 0.00001991
Iteration 16/1000 | Loss: 0.00001945
Iteration 17/1000 | Loss: 0.00001903
Iteration 18/1000 | Loss: 0.00001893
Iteration 19/1000 | Loss: 0.00001879
Iteration 20/1000 | Loss: 0.00001878
Iteration 21/1000 | Loss: 0.00001875
Iteration 22/1000 | Loss: 0.00001875
Iteration 23/1000 | Loss: 0.00001869
Iteration 24/1000 | Loss: 0.00001868
Iteration 25/1000 | Loss: 0.00001868
Iteration 26/1000 | Loss: 0.00001868
Iteration 27/1000 | Loss: 0.00001867
Iteration 28/1000 | Loss: 0.00001867
Iteration 29/1000 | Loss: 0.00001866
Iteration 30/1000 | Loss: 0.00001866
Iteration 31/1000 | Loss: 0.00001865
Iteration 32/1000 | Loss: 0.00001865
Iteration 33/1000 | Loss: 0.00001865
Iteration 34/1000 | Loss: 0.00001863
Iteration 35/1000 | Loss: 0.00001863
Iteration 36/1000 | Loss: 0.00001863
Iteration 37/1000 | Loss: 0.00001863
Iteration 38/1000 | Loss: 0.00001862
Iteration 39/1000 | Loss: 0.00001862
Iteration 40/1000 | Loss: 0.00001862
Iteration 41/1000 | Loss: 0.00001862
Iteration 42/1000 | Loss: 0.00001861
Iteration 43/1000 | Loss: 0.00001861
Iteration 44/1000 | Loss: 0.00001859
Iteration 45/1000 | Loss: 0.00001858
Iteration 46/1000 | Loss: 0.00001858
Iteration 47/1000 | Loss: 0.00001858
Iteration 48/1000 | Loss: 0.00001857
Iteration 49/1000 | Loss: 0.00001857
Iteration 50/1000 | Loss: 0.00001857
Iteration 51/1000 | Loss: 0.00001857
Iteration 52/1000 | Loss: 0.00001857
Iteration 53/1000 | Loss: 0.00001857
Iteration 54/1000 | Loss: 0.00001857
Iteration 55/1000 | Loss: 0.00001857
Iteration 56/1000 | Loss: 0.00001857
Iteration 57/1000 | Loss: 0.00001856
Iteration 58/1000 | Loss: 0.00001856
Iteration 59/1000 | Loss: 0.00001856
Iteration 60/1000 | Loss: 0.00001856
Iteration 61/1000 | Loss: 0.00001856
Iteration 62/1000 | Loss: 0.00001856
Iteration 63/1000 | Loss: 0.00001856
Iteration 64/1000 | Loss: 0.00001856
Iteration 65/1000 | Loss: 0.00001856
Iteration 66/1000 | Loss: 0.00001854
Iteration 67/1000 | Loss: 0.00001854
Iteration 68/1000 | Loss: 0.00001853
Iteration 69/1000 | Loss: 0.00001853
Iteration 70/1000 | Loss: 0.00001852
Iteration 71/1000 | Loss: 0.00001852
Iteration 72/1000 | Loss: 0.00001852
Iteration 73/1000 | Loss: 0.00001852
Iteration 74/1000 | Loss: 0.00001852
Iteration 75/1000 | Loss: 0.00001852
Iteration 76/1000 | Loss: 0.00001852
Iteration 77/1000 | Loss: 0.00001852
Iteration 78/1000 | Loss: 0.00001852
Iteration 79/1000 | Loss: 0.00001852
Iteration 80/1000 | Loss: 0.00001852
Iteration 81/1000 | Loss: 0.00001851
Iteration 82/1000 | Loss: 0.00001851
Iteration 83/1000 | Loss: 0.00001850
Iteration 84/1000 | Loss: 0.00001850
Iteration 85/1000 | Loss: 0.00001849
Iteration 86/1000 | Loss: 0.00001847
Iteration 87/1000 | Loss: 0.00001847
Iteration 88/1000 | Loss: 0.00001847
Iteration 89/1000 | Loss: 0.00001846
Iteration 90/1000 | Loss: 0.00001845
Iteration 91/1000 | Loss: 0.00001845
Iteration 92/1000 | Loss: 0.00001845
Iteration 93/1000 | Loss: 0.00001845
Iteration 94/1000 | Loss: 0.00001844
Iteration 95/1000 | Loss: 0.00001842
Iteration 96/1000 | Loss: 0.00001841
Iteration 97/1000 | Loss: 0.00001841
Iteration 98/1000 | Loss: 0.00001841
Iteration 99/1000 | Loss: 0.00001841
Iteration 100/1000 | Loss: 0.00001841
Iteration 101/1000 | Loss: 0.00001841
Iteration 102/1000 | Loss: 0.00001841
Iteration 103/1000 | Loss: 0.00001840
Iteration 104/1000 | Loss: 0.00001840
Iteration 105/1000 | Loss: 0.00001839
Iteration 106/1000 | Loss: 0.00001839
Iteration 107/1000 | Loss: 0.00001839
Iteration 108/1000 | Loss: 0.00001839
Iteration 109/1000 | Loss: 0.00001838
Iteration 110/1000 | Loss: 0.00001838
Iteration 111/1000 | Loss: 0.00001838
Iteration 112/1000 | Loss: 0.00001838
Iteration 113/1000 | Loss: 0.00001838
Iteration 114/1000 | Loss: 0.00001838
Iteration 115/1000 | Loss: 0.00001838
Iteration 116/1000 | Loss: 0.00001837
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001837
Iteration 121/1000 | Loss: 0.00001837
Iteration 122/1000 | Loss: 0.00001836
Iteration 123/1000 | Loss: 0.00001836
Iteration 124/1000 | Loss: 0.00001836
Iteration 125/1000 | Loss: 0.00001836
Iteration 126/1000 | Loss: 0.00001836
Iteration 127/1000 | Loss: 0.00001836
Iteration 128/1000 | Loss: 0.00001836
Iteration 129/1000 | Loss: 0.00001836
Iteration 130/1000 | Loss: 0.00001836
Iteration 131/1000 | Loss: 0.00001835
Iteration 132/1000 | Loss: 0.00001835
Iteration 133/1000 | Loss: 0.00001835
Iteration 134/1000 | Loss: 0.00001835
Iteration 135/1000 | Loss: 0.00001834
Iteration 136/1000 | Loss: 0.00001834
Iteration 137/1000 | Loss: 0.00001834
Iteration 138/1000 | Loss: 0.00001834
Iteration 139/1000 | Loss: 0.00001834
Iteration 140/1000 | Loss: 0.00001834
Iteration 141/1000 | Loss: 0.00001833
Iteration 142/1000 | Loss: 0.00001833
Iteration 143/1000 | Loss: 0.00001833
Iteration 144/1000 | Loss: 0.00001833
Iteration 145/1000 | Loss: 0.00001833
Iteration 146/1000 | Loss: 0.00001833
Iteration 147/1000 | Loss: 0.00001833
Iteration 148/1000 | Loss: 0.00001833
Iteration 149/1000 | Loss: 0.00001833
Iteration 150/1000 | Loss: 0.00001833
Iteration 151/1000 | Loss: 0.00001833
Iteration 152/1000 | Loss: 0.00001833
Iteration 153/1000 | Loss: 0.00001832
Iteration 154/1000 | Loss: 0.00001832
Iteration 155/1000 | Loss: 0.00001832
Iteration 156/1000 | Loss: 0.00001832
Iteration 157/1000 | Loss: 0.00001832
Iteration 158/1000 | Loss: 0.00001832
Iteration 159/1000 | Loss: 0.00001832
Iteration 160/1000 | Loss: 0.00001832
Iteration 161/1000 | Loss: 0.00001832
Iteration 162/1000 | Loss: 0.00001832
Iteration 163/1000 | Loss: 0.00001832
Iteration 164/1000 | Loss: 0.00001832
Iteration 165/1000 | Loss: 0.00001832
Iteration 166/1000 | Loss: 0.00001832
Iteration 167/1000 | Loss: 0.00001832
Iteration 168/1000 | Loss: 0.00001832
Iteration 169/1000 | Loss: 0.00001832
Iteration 170/1000 | Loss: 0.00001831
Iteration 171/1000 | Loss: 0.00001831
Iteration 172/1000 | Loss: 0.00001831
Iteration 173/1000 | Loss: 0.00001831
Iteration 174/1000 | Loss: 0.00001831
Iteration 175/1000 | Loss: 0.00001831
Iteration 176/1000 | Loss: 0.00001831
Iteration 177/1000 | Loss: 0.00001831
Iteration 178/1000 | Loss: 0.00001831
Iteration 179/1000 | Loss: 0.00001831
Iteration 180/1000 | Loss: 0.00001831
Iteration 181/1000 | Loss: 0.00001831
Iteration 182/1000 | Loss: 0.00001831
Iteration 183/1000 | Loss: 0.00001831
Iteration 184/1000 | Loss: 0.00001831
Iteration 185/1000 | Loss: 0.00001831
Iteration 186/1000 | Loss: 0.00001831
Iteration 187/1000 | Loss: 0.00001831
Iteration 188/1000 | Loss: 0.00001831
Iteration 189/1000 | Loss: 0.00001831
Iteration 190/1000 | Loss: 0.00001831
Iteration 191/1000 | Loss: 0.00001831
Iteration 192/1000 | Loss: 0.00001831
Iteration 193/1000 | Loss: 0.00001830
Iteration 194/1000 | Loss: 0.00001830
Iteration 195/1000 | Loss: 0.00001830
Iteration 196/1000 | Loss: 0.00001830
Iteration 197/1000 | Loss: 0.00001830
Iteration 198/1000 | Loss: 0.00001830
Iteration 199/1000 | Loss: 0.00001830
Iteration 200/1000 | Loss: 0.00001830
Iteration 201/1000 | Loss: 0.00001830
Iteration 202/1000 | Loss: 0.00001830
Iteration 203/1000 | Loss: 0.00001830
Iteration 204/1000 | Loss: 0.00001830
Iteration 205/1000 | Loss: 0.00001830
Iteration 206/1000 | Loss: 0.00001830
Iteration 207/1000 | Loss: 0.00001830
Iteration 208/1000 | Loss: 0.00001830
Iteration 209/1000 | Loss: 0.00001830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.830354995036032e-05, 1.830354995036032e-05, 1.830354995036032e-05, 1.830354995036032e-05, 1.830354995036032e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.830354995036032e-05

Optimization complete. Final v2v error: 3.4109294414520264 mm

Highest mean error: 4.922094345092773 mm for frame 108

Lowest mean error: 2.8136913776397705 mm for frame 168

Saving results

Total time: 97.06948685646057
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967862
Iteration 2/25 | Loss: 0.00216095
Iteration 3/25 | Loss: 0.00119287
Iteration 4/25 | Loss: 0.00116588
Iteration 5/25 | Loss: 0.00115928
Iteration 6/25 | Loss: 0.00115613
Iteration 7/25 | Loss: 0.00115610
Iteration 8/25 | Loss: 0.00115610
Iteration 9/25 | Loss: 0.00115610
Iteration 10/25 | Loss: 0.00115610
Iteration 11/25 | Loss: 0.00115610
Iteration 12/25 | Loss: 0.00115610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001156104844994843, 0.001156104844994843, 0.001156104844994843, 0.001156104844994843, 0.001156104844994843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001156104844994843

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52364564
Iteration 2/25 | Loss: 0.00068197
Iteration 3/25 | Loss: 0.00068197
Iteration 4/25 | Loss: 0.00068197
Iteration 5/25 | Loss: 0.00068197
Iteration 6/25 | Loss: 0.00068197
Iteration 7/25 | Loss: 0.00068197
Iteration 8/25 | Loss: 0.00068197
Iteration 9/25 | Loss: 0.00068197
Iteration 10/25 | Loss: 0.00068197
Iteration 11/25 | Loss: 0.00068197
Iteration 12/25 | Loss: 0.00068197
Iteration 13/25 | Loss: 0.00068197
Iteration 14/25 | Loss: 0.00068197
Iteration 15/25 | Loss: 0.00068197
Iteration 16/25 | Loss: 0.00068197
Iteration 17/25 | Loss: 0.00068197
Iteration 18/25 | Loss: 0.00068197
Iteration 19/25 | Loss: 0.00068197
Iteration 20/25 | Loss: 0.00068197
Iteration 21/25 | Loss: 0.00068197
Iteration 22/25 | Loss: 0.00068197
Iteration 23/25 | Loss: 0.00068197
Iteration 24/25 | Loss: 0.00068197
Iteration 25/25 | Loss: 0.00068197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068197
Iteration 2/1000 | Loss: 0.00008190
Iteration 3/1000 | Loss: 0.00006629
Iteration 4/1000 | Loss: 0.00005912
Iteration 5/1000 | Loss: 0.00005648
Iteration 6/1000 | Loss: 0.00005497
Iteration 7/1000 | Loss: 0.00005379
Iteration 8/1000 | Loss: 0.00005258
Iteration 9/1000 | Loss: 0.00005157
Iteration 10/1000 | Loss: 0.00005046
Iteration 11/1000 | Loss: 0.00004978
Iteration 12/1000 | Loss: 0.00004897
Iteration 13/1000 | Loss: 0.00004846
Iteration 14/1000 | Loss: 0.00004795
Iteration 15/1000 | Loss: 0.00004736
Iteration 16/1000 | Loss: 0.00004680
Iteration 17/1000 | Loss: 0.00004639
Iteration 18/1000 | Loss: 0.00004608
Iteration 19/1000 | Loss: 0.00004574
Iteration 20/1000 | Loss: 0.00004541
Iteration 21/1000 | Loss: 0.00004516
Iteration 22/1000 | Loss: 0.00004499
Iteration 23/1000 | Loss: 0.00004487
Iteration 24/1000 | Loss: 0.00004485
Iteration 25/1000 | Loss: 0.00004478
Iteration 26/1000 | Loss: 0.00004478
Iteration 27/1000 | Loss: 0.00004472
Iteration 28/1000 | Loss: 0.00004462
Iteration 29/1000 | Loss: 0.00004462
Iteration 30/1000 | Loss: 0.00004454
Iteration 31/1000 | Loss: 0.00004450
Iteration 32/1000 | Loss: 0.00004450
Iteration 33/1000 | Loss: 0.00004450
Iteration 34/1000 | Loss: 0.00004450
Iteration 35/1000 | Loss: 0.00004450
Iteration 36/1000 | Loss: 0.00004450
Iteration 37/1000 | Loss: 0.00004450
Iteration 38/1000 | Loss: 0.00004450
Iteration 39/1000 | Loss: 0.00004450
Iteration 40/1000 | Loss: 0.00004449
Iteration 41/1000 | Loss: 0.00004449
Iteration 42/1000 | Loss: 0.00004449
Iteration 43/1000 | Loss: 0.00004449
Iteration 44/1000 | Loss: 0.00004449
Iteration 45/1000 | Loss: 0.00004449
Iteration 46/1000 | Loss: 0.00004449
Iteration 47/1000 | Loss: 0.00004449
Iteration 48/1000 | Loss: 0.00004449
Iteration 49/1000 | Loss: 0.00004449
Iteration 50/1000 | Loss: 0.00004448
Iteration 51/1000 | Loss: 0.00004448
Iteration 52/1000 | Loss: 0.00004448
Iteration 53/1000 | Loss: 0.00004448
Iteration 54/1000 | Loss: 0.00004448
Iteration 55/1000 | Loss: 0.00004448
Iteration 56/1000 | Loss: 0.00004448
Iteration 57/1000 | Loss: 0.00004448
Iteration 58/1000 | Loss: 0.00004447
Iteration 59/1000 | Loss: 0.00004447
Iteration 60/1000 | Loss: 0.00004447
Iteration 61/1000 | Loss: 0.00004447
Iteration 62/1000 | Loss: 0.00004446
Iteration 63/1000 | Loss: 0.00004446
Iteration 64/1000 | Loss: 0.00004446
Iteration 65/1000 | Loss: 0.00004446
Iteration 66/1000 | Loss: 0.00004446
Iteration 67/1000 | Loss: 0.00004445
Iteration 68/1000 | Loss: 0.00004445
Iteration 69/1000 | Loss: 0.00004445
Iteration 70/1000 | Loss: 0.00004445
Iteration 71/1000 | Loss: 0.00004445
Iteration 72/1000 | Loss: 0.00004445
Iteration 73/1000 | Loss: 0.00004444
Iteration 74/1000 | Loss: 0.00004444
Iteration 75/1000 | Loss: 0.00004444
Iteration 76/1000 | Loss: 0.00004444
Iteration 77/1000 | Loss: 0.00004444
Iteration 78/1000 | Loss: 0.00004444
Iteration 79/1000 | Loss: 0.00004444
Iteration 80/1000 | Loss: 0.00004444
Iteration 81/1000 | Loss: 0.00004444
Iteration 82/1000 | Loss: 0.00004444
Iteration 83/1000 | Loss: 0.00004444
Iteration 84/1000 | Loss: 0.00004444
Iteration 85/1000 | Loss: 0.00004443
Iteration 86/1000 | Loss: 0.00004443
Iteration 87/1000 | Loss: 0.00004443
Iteration 88/1000 | Loss: 0.00004443
Iteration 89/1000 | Loss: 0.00004443
Iteration 90/1000 | Loss: 0.00004443
Iteration 91/1000 | Loss: 0.00004443
Iteration 92/1000 | Loss: 0.00004443
Iteration 93/1000 | Loss: 0.00004443
Iteration 94/1000 | Loss: 0.00004443
Iteration 95/1000 | Loss: 0.00004443
Iteration 96/1000 | Loss: 0.00004443
Iteration 97/1000 | Loss: 0.00004443
Iteration 98/1000 | Loss: 0.00004443
Iteration 99/1000 | Loss: 0.00004442
Iteration 100/1000 | Loss: 0.00004442
Iteration 101/1000 | Loss: 0.00004442
Iteration 102/1000 | Loss: 0.00004442
Iteration 103/1000 | Loss: 0.00004442
Iteration 104/1000 | Loss: 0.00004442
Iteration 105/1000 | Loss: 0.00004442
Iteration 106/1000 | Loss: 0.00004442
Iteration 107/1000 | Loss: 0.00004442
Iteration 108/1000 | Loss: 0.00004442
Iteration 109/1000 | Loss: 0.00004442
Iteration 110/1000 | Loss: 0.00004442
Iteration 111/1000 | Loss: 0.00004442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [4.4422522478271276e-05, 4.4422522478271276e-05, 4.4422522478271276e-05, 4.4422522478271276e-05, 4.4422522478271276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.4422522478271276e-05

Optimization complete. Final v2v error: 5.226693630218506 mm

Highest mean error: 5.645541667938232 mm for frame 70

Lowest mean error: 4.834987640380859 mm for frame 229

Saving results

Total time: 59.86658525466919
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471199
Iteration 2/25 | Loss: 0.00125092
Iteration 3/25 | Loss: 0.00104836
Iteration 4/25 | Loss: 0.00101556
Iteration 5/25 | Loss: 0.00100530
Iteration 6/25 | Loss: 0.00100232
Iteration 7/25 | Loss: 0.00100207
Iteration 8/25 | Loss: 0.00100207
Iteration 9/25 | Loss: 0.00100207
Iteration 10/25 | Loss: 0.00100207
Iteration 11/25 | Loss: 0.00100207
Iteration 12/25 | Loss: 0.00100207
Iteration 13/25 | Loss: 0.00100207
Iteration 14/25 | Loss: 0.00100207
Iteration 15/25 | Loss: 0.00100207
Iteration 16/25 | Loss: 0.00100207
Iteration 17/25 | Loss: 0.00100207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010020722402259707, 0.0010020722402259707, 0.0010020722402259707, 0.0010020722402259707, 0.0010020722402259707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010020722402259707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.22335887
Iteration 2/25 | Loss: 0.00080723
Iteration 3/25 | Loss: 0.00080719
Iteration 4/25 | Loss: 0.00080719
Iteration 5/25 | Loss: 0.00080719
Iteration 6/25 | Loss: 0.00080719
Iteration 7/25 | Loss: 0.00080719
Iteration 8/25 | Loss: 0.00080719
Iteration 9/25 | Loss: 0.00080719
Iteration 10/25 | Loss: 0.00080719
Iteration 11/25 | Loss: 0.00080719
Iteration 12/25 | Loss: 0.00080719
Iteration 13/25 | Loss: 0.00080719
Iteration 14/25 | Loss: 0.00080719
Iteration 15/25 | Loss: 0.00080719
Iteration 16/25 | Loss: 0.00080719
Iteration 17/25 | Loss: 0.00080719
Iteration 18/25 | Loss: 0.00080719
Iteration 19/25 | Loss: 0.00080719
Iteration 20/25 | Loss: 0.00080719
Iteration 21/25 | Loss: 0.00080719
Iteration 22/25 | Loss: 0.00080719
Iteration 23/25 | Loss: 0.00080719
Iteration 24/25 | Loss: 0.00080719
Iteration 25/25 | Loss: 0.00080719

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080719
Iteration 2/1000 | Loss: 0.00003915
Iteration 3/1000 | Loss: 0.00002498
Iteration 4/1000 | Loss: 0.00001864
Iteration 5/1000 | Loss: 0.00001711
Iteration 6/1000 | Loss: 0.00001614
Iteration 7/1000 | Loss: 0.00001557
Iteration 8/1000 | Loss: 0.00001525
Iteration 9/1000 | Loss: 0.00001498
Iteration 10/1000 | Loss: 0.00001488
Iteration 11/1000 | Loss: 0.00001485
Iteration 12/1000 | Loss: 0.00001475
Iteration 13/1000 | Loss: 0.00001474
Iteration 14/1000 | Loss: 0.00001463
Iteration 15/1000 | Loss: 0.00001456
Iteration 16/1000 | Loss: 0.00001450
Iteration 17/1000 | Loss: 0.00001448
Iteration 18/1000 | Loss: 0.00001447
Iteration 19/1000 | Loss: 0.00001447
Iteration 20/1000 | Loss: 0.00001446
Iteration 21/1000 | Loss: 0.00001446
Iteration 22/1000 | Loss: 0.00001445
Iteration 23/1000 | Loss: 0.00001444
Iteration 24/1000 | Loss: 0.00001442
Iteration 25/1000 | Loss: 0.00001442
Iteration 26/1000 | Loss: 0.00001440
Iteration 27/1000 | Loss: 0.00001439
Iteration 28/1000 | Loss: 0.00001439
Iteration 29/1000 | Loss: 0.00001438
Iteration 30/1000 | Loss: 0.00001438
Iteration 31/1000 | Loss: 0.00001438
Iteration 32/1000 | Loss: 0.00001437
Iteration 33/1000 | Loss: 0.00001437
Iteration 34/1000 | Loss: 0.00001435
Iteration 35/1000 | Loss: 0.00001435
Iteration 36/1000 | Loss: 0.00001435
Iteration 37/1000 | Loss: 0.00001435
Iteration 38/1000 | Loss: 0.00001434
Iteration 39/1000 | Loss: 0.00001433
Iteration 40/1000 | Loss: 0.00001432
Iteration 41/1000 | Loss: 0.00001431
Iteration 42/1000 | Loss: 0.00001431
Iteration 43/1000 | Loss: 0.00001431
Iteration 44/1000 | Loss: 0.00001430
Iteration 45/1000 | Loss: 0.00001430
Iteration 46/1000 | Loss: 0.00001427
Iteration 47/1000 | Loss: 0.00001427
Iteration 48/1000 | Loss: 0.00001427
Iteration 49/1000 | Loss: 0.00001426
Iteration 50/1000 | Loss: 0.00001426
Iteration 51/1000 | Loss: 0.00001426
Iteration 52/1000 | Loss: 0.00001425
Iteration 53/1000 | Loss: 0.00001425
Iteration 54/1000 | Loss: 0.00001425
Iteration 55/1000 | Loss: 0.00001424
Iteration 56/1000 | Loss: 0.00001424
Iteration 57/1000 | Loss: 0.00001424
Iteration 58/1000 | Loss: 0.00001424
Iteration 59/1000 | Loss: 0.00001424
Iteration 60/1000 | Loss: 0.00001423
Iteration 61/1000 | Loss: 0.00001423
Iteration 62/1000 | Loss: 0.00001423
Iteration 63/1000 | Loss: 0.00001423
Iteration 64/1000 | Loss: 0.00001423
Iteration 65/1000 | Loss: 0.00001423
Iteration 66/1000 | Loss: 0.00001423
Iteration 67/1000 | Loss: 0.00001423
Iteration 68/1000 | Loss: 0.00001422
Iteration 69/1000 | Loss: 0.00001422
Iteration 70/1000 | Loss: 0.00001422
Iteration 71/1000 | Loss: 0.00001422
Iteration 72/1000 | Loss: 0.00001422
Iteration 73/1000 | Loss: 0.00001422
Iteration 74/1000 | Loss: 0.00001422
Iteration 75/1000 | Loss: 0.00001422
Iteration 76/1000 | Loss: 0.00001422
Iteration 77/1000 | Loss: 0.00001422
Iteration 78/1000 | Loss: 0.00001421
Iteration 79/1000 | Loss: 0.00001421
Iteration 80/1000 | Loss: 0.00001421
Iteration 81/1000 | Loss: 0.00001421
Iteration 82/1000 | Loss: 0.00001421
Iteration 83/1000 | Loss: 0.00001421
Iteration 84/1000 | Loss: 0.00001421
Iteration 85/1000 | Loss: 0.00001421
Iteration 86/1000 | Loss: 0.00001421
Iteration 87/1000 | Loss: 0.00001421
Iteration 88/1000 | Loss: 0.00001421
Iteration 89/1000 | Loss: 0.00001421
Iteration 90/1000 | Loss: 0.00001420
Iteration 91/1000 | Loss: 0.00001420
Iteration 92/1000 | Loss: 0.00001420
Iteration 93/1000 | Loss: 0.00001420
Iteration 94/1000 | Loss: 0.00001420
Iteration 95/1000 | Loss: 0.00001420
Iteration 96/1000 | Loss: 0.00001420
Iteration 97/1000 | Loss: 0.00001420
Iteration 98/1000 | Loss: 0.00001419
Iteration 99/1000 | Loss: 0.00001419
Iteration 100/1000 | Loss: 0.00001419
Iteration 101/1000 | Loss: 0.00001419
Iteration 102/1000 | Loss: 0.00001419
Iteration 103/1000 | Loss: 0.00001419
Iteration 104/1000 | Loss: 0.00001419
Iteration 105/1000 | Loss: 0.00001419
Iteration 106/1000 | Loss: 0.00001419
Iteration 107/1000 | Loss: 0.00001419
Iteration 108/1000 | Loss: 0.00001419
Iteration 109/1000 | Loss: 0.00001419
Iteration 110/1000 | Loss: 0.00001419
Iteration 111/1000 | Loss: 0.00001418
Iteration 112/1000 | Loss: 0.00001418
Iteration 113/1000 | Loss: 0.00001418
Iteration 114/1000 | Loss: 0.00001418
Iteration 115/1000 | Loss: 0.00001418
Iteration 116/1000 | Loss: 0.00001418
Iteration 117/1000 | Loss: 0.00001418
Iteration 118/1000 | Loss: 0.00001418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.4182790437189396e-05, 1.4182790437189396e-05, 1.4182790437189396e-05, 1.4182790437189396e-05, 1.4182790437189396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4182790437189396e-05

Optimization complete. Final v2v error: 3.1265203952789307 mm

Highest mean error: 4.0636773109436035 mm for frame 90

Lowest mean error: 2.4944798946380615 mm for frame 136

Saving results

Total time: 40.39301824569702
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108084
Iteration 2/25 | Loss: 0.00169609
Iteration 3/25 | Loss: 0.00164348
Iteration 4/25 | Loss: 0.00098412
Iteration 5/25 | Loss: 0.00093387
Iteration 6/25 | Loss: 0.00092472
Iteration 7/25 | Loss: 0.00091936
Iteration 8/25 | Loss: 0.00091357
Iteration 9/25 | Loss: 0.00090959
Iteration 10/25 | Loss: 0.00091292
Iteration 11/25 | Loss: 0.00090962
Iteration 12/25 | Loss: 0.00090410
Iteration 13/25 | Loss: 0.00089780
Iteration 14/25 | Loss: 0.00089387
Iteration 15/25 | Loss: 0.00089331
Iteration 16/25 | Loss: 0.00089318
Iteration 17/25 | Loss: 0.00089314
Iteration 18/25 | Loss: 0.00089314
Iteration 19/25 | Loss: 0.00089314
Iteration 20/25 | Loss: 0.00089313
Iteration 21/25 | Loss: 0.00089313
Iteration 22/25 | Loss: 0.00089313
Iteration 23/25 | Loss: 0.00089313
Iteration 24/25 | Loss: 0.00089313
Iteration 25/25 | Loss: 0.00089313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.50597858
Iteration 2/25 | Loss: 0.00065039
Iteration 3/25 | Loss: 0.00065039
Iteration 4/25 | Loss: 0.00065039
Iteration 5/25 | Loss: 0.00065039
Iteration 6/25 | Loss: 0.00065039
Iteration 7/25 | Loss: 0.00065039
Iteration 8/25 | Loss: 0.00065039
Iteration 9/25 | Loss: 0.00065039
Iteration 10/25 | Loss: 0.00065039
Iteration 11/25 | Loss: 0.00065039
Iteration 12/25 | Loss: 0.00065039
Iteration 13/25 | Loss: 0.00065039
Iteration 14/25 | Loss: 0.00065039
Iteration 15/25 | Loss: 0.00065039
Iteration 16/25 | Loss: 0.00065039
Iteration 17/25 | Loss: 0.00065039
Iteration 18/25 | Loss: 0.00065039
Iteration 19/25 | Loss: 0.00065039
Iteration 20/25 | Loss: 0.00065039
Iteration 21/25 | Loss: 0.00065039
Iteration 22/25 | Loss: 0.00065039
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006503891199827194, 0.0006503891199827194, 0.0006503891199827194, 0.0006503891199827194, 0.0006503891199827194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006503891199827194

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065039
Iteration 2/1000 | Loss: 0.00009413
Iteration 3/1000 | Loss: 0.00005194
Iteration 4/1000 | Loss: 0.00001418
Iteration 5/1000 | Loss: 0.00007371
Iteration 6/1000 | Loss: 0.00008141
Iteration 7/1000 | Loss: 0.00001293
Iteration 8/1000 | Loss: 0.00004167
Iteration 9/1000 | Loss: 0.00001174
Iteration 10/1000 | Loss: 0.00001127
Iteration 11/1000 | Loss: 0.00016530
Iteration 12/1000 | Loss: 0.00009528
Iteration 13/1000 | Loss: 0.00009194
Iteration 14/1000 | Loss: 0.00001105
Iteration 15/1000 | Loss: 0.00001081
Iteration 16/1000 | Loss: 0.00001070
Iteration 17/1000 | Loss: 0.00001069
Iteration 18/1000 | Loss: 0.00001069
Iteration 19/1000 | Loss: 0.00001069
Iteration 20/1000 | Loss: 0.00001069
Iteration 21/1000 | Loss: 0.00001068
Iteration 22/1000 | Loss: 0.00001056
Iteration 23/1000 | Loss: 0.00001055
Iteration 24/1000 | Loss: 0.00001053
Iteration 25/1000 | Loss: 0.00001052
Iteration 26/1000 | Loss: 0.00001052
Iteration 27/1000 | Loss: 0.00001052
Iteration 28/1000 | Loss: 0.00001052
Iteration 29/1000 | Loss: 0.00001052
Iteration 30/1000 | Loss: 0.00001052
Iteration 31/1000 | Loss: 0.00001052
Iteration 32/1000 | Loss: 0.00001050
Iteration 33/1000 | Loss: 0.00001049
Iteration 34/1000 | Loss: 0.00001048
Iteration 35/1000 | Loss: 0.00001048
Iteration 36/1000 | Loss: 0.00001048
Iteration 37/1000 | Loss: 0.00001047
Iteration 38/1000 | Loss: 0.00001046
Iteration 39/1000 | Loss: 0.00001046
Iteration 40/1000 | Loss: 0.00001046
Iteration 41/1000 | Loss: 0.00001046
Iteration 42/1000 | Loss: 0.00001045
Iteration 43/1000 | Loss: 0.00001045
Iteration 44/1000 | Loss: 0.00001044
Iteration 45/1000 | Loss: 0.00001044
Iteration 46/1000 | Loss: 0.00001044
Iteration 47/1000 | Loss: 0.00001044
Iteration 48/1000 | Loss: 0.00001043
Iteration 49/1000 | Loss: 0.00001043
Iteration 50/1000 | Loss: 0.00001042
Iteration 51/1000 | Loss: 0.00001040
Iteration 52/1000 | Loss: 0.00001039
Iteration 53/1000 | Loss: 0.00001039
Iteration 54/1000 | Loss: 0.00001039
Iteration 55/1000 | Loss: 0.00001038
Iteration 56/1000 | Loss: 0.00010370
Iteration 57/1000 | Loss: 0.00004711
Iteration 58/1000 | Loss: 0.00013385
Iteration 59/1000 | Loss: 0.00005577
Iteration 60/1000 | Loss: 0.00002055
Iteration 61/1000 | Loss: 0.00001048
Iteration 62/1000 | Loss: 0.00001041
Iteration 63/1000 | Loss: 0.00001041
Iteration 64/1000 | Loss: 0.00001041
Iteration 65/1000 | Loss: 0.00001041
Iteration 66/1000 | Loss: 0.00001035
Iteration 67/1000 | Loss: 0.00001034
Iteration 68/1000 | Loss: 0.00002583
Iteration 69/1000 | Loss: 0.00001064
Iteration 70/1000 | Loss: 0.00001213
Iteration 71/1000 | Loss: 0.00001034
Iteration 72/1000 | Loss: 0.00001033
Iteration 73/1000 | Loss: 0.00001032
Iteration 74/1000 | Loss: 0.00001032
Iteration 75/1000 | Loss: 0.00001032
Iteration 76/1000 | Loss: 0.00001031
Iteration 77/1000 | Loss: 0.00001031
Iteration 78/1000 | Loss: 0.00001030
Iteration 79/1000 | Loss: 0.00001029
Iteration 80/1000 | Loss: 0.00001028
Iteration 81/1000 | Loss: 0.00001028
Iteration 82/1000 | Loss: 0.00001028
Iteration 83/1000 | Loss: 0.00001027
Iteration 84/1000 | Loss: 0.00001027
Iteration 85/1000 | Loss: 0.00001027
Iteration 86/1000 | Loss: 0.00001027
Iteration 87/1000 | Loss: 0.00001026
Iteration 88/1000 | Loss: 0.00001026
Iteration 89/1000 | Loss: 0.00001025
Iteration 90/1000 | Loss: 0.00001025
Iteration 91/1000 | Loss: 0.00001025
Iteration 92/1000 | Loss: 0.00001025
Iteration 93/1000 | Loss: 0.00001025
Iteration 94/1000 | Loss: 0.00001025
Iteration 95/1000 | Loss: 0.00001025
Iteration 96/1000 | Loss: 0.00001025
Iteration 97/1000 | Loss: 0.00001025
Iteration 98/1000 | Loss: 0.00001025
Iteration 99/1000 | Loss: 0.00001025
Iteration 100/1000 | Loss: 0.00001025
Iteration 101/1000 | Loss: 0.00001025
Iteration 102/1000 | Loss: 0.00001025
Iteration 103/1000 | Loss: 0.00001025
Iteration 104/1000 | Loss: 0.00001025
Iteration 105/1000 | Loss: 0.00001025
Iteration 106/1000 | Loss: 0.00001025
Iteration 107/1000 | Loss: 0.00001025
Iteration 108/1000 | Loss: 0.00001025
Iteration 109/1000 | Loss: 0.00001025
Iteration 110/1000 | Loss: 0.00001025
Iteration 111/1000 | Loss: 0.00001025
Iteration 112/1000 | Loss: 0.00001025
Iteration 113/1000 | Loss: 0.00001025
Iteration 114/1000 | Loss: 0.00001025
Iteration 115/1000 | Loss: 0.00001025
Iteration 116/1000 | Loss: 0.00001025
Iteration 117/1000 | Loss: 0.00001025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.0246969395666383e-05, 1.0246969395666383e-05, 1.0246969395666383e-05, 1.0246969395666383e-05, 1.0246969395666383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0246969395666383e-05

Optimization complete. Final v2v error: 2.6853485107421875 mm

Highest mean error: 8.233455657958984 mm for frame 34

Lowest mean error: 2.2709708213806152 mm for frame 137

Saving results

Total time: 70.57240509986877
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850117
Iteration 2/25 | Loss: 0.00157584
Iteration 3/25 | Loss: 0.00122677
Iteration 4/25 | Loss: 0.00119284
Iteration 5/25 | Loss: 0.00136791
Iteration 6/25 | Loss: 0.00107322
Iteration 7/25 | Loss: 0.00104542
Iteration 8/25 | Loss: 0.00103244
Iteration 9/25 | Loss: 0.00102006
Iteration 10/25 | Loss: 0.00101696
Iteration 11/25 | Loss: 0.00105628
Iteration 12/25 | Loss: 0.00101129
Iteration 13/25 | Loss: 0.00100415
Iteration 14/25 | Loss: 0.00100360
Iteration 15/25 | Loss: 0.00100349
Iteration 16/25 | Loss: 0.00100339
Iteration 17/25 | Loss: 0.00100329
Iteration 18/25 | Loss: 0.00101248
Iteration 19/25 | Loss: 0.00100098
Iteration 20/25 | Loss: 0.00099908
Iteration 21/25 | Loss: 0.00099860
Iteration 22/25 | Loss: 0.00099839
Iteration 23/25 | Loss: 0.00099820
Iteration 24/25 | Loss: 0.00099799
Iteration 25/25 | Loss: 0.00100981

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26059949
Iteration 2/25 | Loss: 0.00084203
Iteration 3/25 | Loss: 0.00084201
Iteration 4/25 | Loss: 0.00084201
Iteration 5/25 | Loss: 0.00084201
Iteration 6/25 | Loss: 0.00084201
Iteration 7/25 | Loss: 0.00084201
Iteration 8/25 | Loss: 0.00084201
Iteration 9/25 | Loss: 0.00084201
Iteration 10/25 | Loss: 0.00084201
Iteration 11/25 | Loss: 0.00084201
Iteration 12/25 | Loss: 0.00084201
Iteration 13/25 | Loss: 0.00084201
Iteration 14/25 | Loss: 0.00084201
Iteration 15/25 | Loss: 0.00084201
Iteration 16/25 | Loss: 0.00084201
Iteration 17/25 | Loss: 0.00084201
Iteration 18/25 | Loss: 0.00084201
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008420090307481587, 0.0008420090307481587, 0.0008420090307481587, 0.0008420090307481587, 0.0008420090307481587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008420090307481587

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084201
Iteration 2/1000 | Loss: 0.00004744
Iteration 3/1000 | Loss: 0.00002814
Iteration 4/1000 | Loss: 0.00002116
Iteration 5/1000 | Loss: 0.00001947
Iteration 6/1000 | Loss: 0.00001770
Iteration 7/1000 | Loss: 0.00001644
Iteration 8/1000 | Loss: 0.00001571
Iteration 9/1000 | Loss: 0.00001510
Iteration 10/1000 | Loss: 0.00001472
Iteration 11/1000 | Loss: 0.00001450
Iteration 12/1000 | Loss: 0.00001440
Iteration 13/1000 | Loss: 0.00001438
Iteration 14/1000 | Loss: 0.00001431
Iteration 15/1000 | Loss: 0.00001427
Iteration 16/1000 | Loss: 0.00001421
Iteration 17/1000 | Loss: 0.00001418
Iteration 18/1000 | Loss: 0.00001417
Iteration 19/1000 | Loss: 0.00001416
Iteration 20/1000 | Loss: 0.00001415
Iteration 21/1000 | Loss: 0.00001415
Iteration 22/1000 | Loss: 0.00001415
Iteration 23/1000 | Loss: 0.00001412
Iteration 24/1000 | Loss: 0.00001412
Iteration 25/1000 | Loss: 0.00001411
Iteration 26/1000 | Loss: 0.00001411
Iteration 27/1000 | Loss: 0.00001410
Iteration 28/1000 | Loss: 0.00001410
Iteration 29/1000 | Loss: 0.00001409
Iteration 30/1000 | Loss: 0.00001409
Iteration 31/1000 | Loss: 0.00001409
Iteration 32/1000 | Loss: 0.00001408
Iteration 33/1000 | Loss: 0.00001408
Iteration 34/1000 | Loss: 0.00001408
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001407
Iteration 37/1000 | Loss: 0.00001407
Iteration 38/1000 | Loss: 0.00001406
Iteration 39/1000 | Loss: 0.00001406
Iteration 40/1000 | Loss: 0.00001405
Iteration 41/1000 | Loss: 0.00001405
Iteration 42/1000 | Loss: 0.00001405
Iteration 43/1000 | Loss: 0.00001405
Iteration 44/1000 | Loss: 0.00001405
Iteration 45/1000 | Loss: 0.00001405
Iteration 46/1000 | Loss: 0.00001404
Iteration 47/1000 | Loss: 0.00001404
Iteration 48/1000 | Loss: 0.00001404
Iteration 49/1000 | Loss: 0.00001403
Iteration 50/1000 | Loss: 0.00001403
Iteration 51/1000 | Loss: 0.00001403
Iteration 52/1000 | Loss: 0.00001402
Iteration 53/1000 | Loss: 0.00001402
Iteration 54/1000 | Loss: 0.00001402
Iteration 55/1000 | Loss: 0.00001402
Iteration 56/1000 | Loss: 0.00001401
Iteration 57/1000 | Loss: 0.00001401
Iteration 58/1000 | Loss: 0.00001401
Iteration 59/1000 | Loss: 0.00001400
Iteration 60/1000 | Loss: 0.00001400
Iteration 61/1000 | Loss: 0.00001400
Iteration 62/1000 | Loss: 0.00001399
Iteration 63/1000 | Loss: 0.00001399
Iteration 64/1000 | Loss: 0.00001399
Iteration 65/1000 | Loss: 0.00001399
Iteration 66/1000 | Loss: 0.00001399
Iteration 67/1000 | Loss: 0.00001399
Iteration 68/1000 | Loss: 0.00001399
Iteration 69/1000 | Loss: 0.00001399
Iteration 70/1000 | Loss: 0.00001398
Iteration 71/1000 | Loss: 0.00001397
Iteration 72/1000 | Loss: 0.00001397
Iteration 73/1000 | Loss: 0.00001397
Iteration 74/1000 | Loss: 0.00001396
Iteration 75/1000 | Loss: 0.00001396
Iteration 76/1000 | Loss: 0.00001396
Iteration 77/1000 | Loss: 0.00001396
Iteration 78/1000 | Loss: 0.00001396
Iteration 79/1000 | Loss: 0.00001396
Iteration 80/1000 | Loss: 0.00001396
Iteration 81/1000 | Loss: 0.00001395
Iteration 82/1000 | Loss: 0.00001395
Iteration 83/1000 | Loss: 0.00001395
Iteration 84/1000 | Loss: 0.00001395
Iteration 85/1000 | Loss: 0.00001394
Iteration 86/1000 | Loss: 0.00001394
Iteration 87/1000 | Loss: 0.00001394
Iteration 88/1000 | Loss: 0.00001394
Iteration 89/1000 | Loss: 0.00001394
Iteration 90/1000 | Loss: 0.00001394
Iteration 91/1000 | Loss: 0.00001394
Iteration 92/1000 | Loss: 0.00001394
Iteration 93/1000 | Loss: 0.00001394
Iteration 94/1000 | Loss: 0.00001394
Iteration 95/1000 | Loss: 0.00001394
Iteration 96/1000 | Loss: 0.00001394
Iteration 97/1000 | Loss: 0.00001394
Iteration 98/1000 | Loss: 0.00001394
Iteration 99/1000 | Loss: 0.00001393
Iteration 100/1000 | Loss: 0.00001393
Iteration 101/1000 | Loss: 0.00001393
Iteration 102/1000 | Loss: 0.00001393
Iteration 103/1000 | Loss: 0.00001393
Iteration 104/1000 | Loss: 0.00001393
Iteration 105/1000 | Loss: 0.00001393
Iteration 106/1000 | Loss: 0.00001393
Iteration 107/1000 | Loss: 0.00001393
Iteration 108/1000 | Loss: 0.00001393
Iteration 109/1000 | Loss: 0.00001392
Iteration 110/1000 | Loss: 0.00001392
Iteration 111/1000 | Loss: 0.00001392
Iteration 112/1000 | Loss: 0.00001392
Iteration 113/1000 | Loss: 0.00001392
Iteration 114/1000 | Loss: 0.00001392
Iteration 115/1000 | Loss: 0.00001392
Iteration 116/1000 | Loss: 0.00001392
Iteration 117/1000 | Loss: 0.00001392
Iteration 118/1000 | Loss: 0.00001392
Iteration 119/1000 | Loss: 0.00001391
Iteration 120/1000 | Loss: 0.00001391
Iteration 121/1000 | Loss: 0.00001391
Iteration 122/1000 | Loss: 0.00001391
Iteration 123/1000 | Loss: 0.00001391
Iteration 124/1000 | Loss: 0.00001391
Iteration 125/1000 | Loss: 0.00001391
Iteration 126/1000 | Loss: 0.00001391
Iteration 127/1000 | Loss: 0.00001391
Iteration 128/1000 | Loss: 0.00001391
Iteration 129/1000 | Loss: 0.00001391
Iteration 130/1000 | Loss: 0.00001391
Iteration 131/1000 | Loss: 0.00001391
Iteration 132/1000 | Loss: 0.00001391
Iteration 133/1000 | Loss: 0.00001391
Iteration 134/1000 | Loss: 0.00001391
Iteration 135/1000 | Loss: 0.00001391
Iteration 136/1000 | Loss: 0.00001391
Iteration 137/1000 | Loss: 0.00001391
Iteration 138/1000 | Loss: 0.00001390
Iteration 139/1000 | Loss: 0.00001390
Iteration 140/1000 | Loss: 0.00001390
Iteration 141/1000 | Loss: 0.00001390
Iteration 142/1000 | Loss: 0.00001390
Iteration 143/1000 | Loss: 0.00001390
Iteration 144/1000 | Loss: 0.00001390
Iteration 145/1000 | Loss: 0.00001390
Iteration 146/1000 | Loss: 0.00001390
Iteration 147/1000 | Loss: 0.00001390
Iteration 148/1000 | Loss: 0.00001390
Iteration 149/1000 | Loss: 0.00001390
Iteration 150/1000 | Loss: 0.00001390
Iteration 151/1000 | Loss: 0.00001390
Iteration 152/1000 | Loss: 0.00001390
Iteration 153/1000 | Loss: 0.00001390
Iteration 154/1000 | Loss: 0.00001390
Iteration 155/1000 | Loss: 0.00001390
Iteration 156/1000 | Loss: 0.00001390
Iteration 157/1000 | Loss: 0.00001390
Iteration 158/1000 | Loss: 0.00001390
Iteration 159/1000 | Loss: 0.00001390
Iteration 160/1000 | Loss: 0.00001390
Iteration 161/1000 | Loss: 0.00001390
Iteration 162/1000 | Loss: 0.00001390
Iteration 163/1000 | Loss: 0.00001390
Iteration 164/1000 | Loss: 0.00001390
Iteration 165/1000 | Loss: 0.00001390
Iteration 166/1000 | Loss: 0.00001390
Iteration 167/1000 | Loss: 0.00001390
Iteration 168/1000 | Loss: 0.00001390
Iteration 169/1000 | Loss: 0.00001390
Iteration 170/1000 | Loss: 0.00001390
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.3902108548791148e-05, 1.3902108548791148e-05, 1.3902108548791148e-05, 1.3902108548791148e-05, 1.3902108548791148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3902108548791148e-05

Optimization complete. Final v2v error: 3.161602258682251 mm

Highest mean error: 3.935420036315918 mm for frame 36

Lowest mean error: 2.6538374423980713 mm for frame 154

Saving results

Total time: 72.44746136665344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395078
Iteration 2/25 | Loss: 0.00100634
Iteration 3/25 | Loss: 0.00092138
Iteration 4/25 | Loss: 0.00090904
Iteration 5/25 | Loss: 0.00090475
Iteration 6/25 | Loss: 0.00090371
Iteration 7/25 | Loss: 0.00090371
Iteration 8/25 | Loss: 0.00090371
Iteration 9/25 | Loss: 0.00090371
Iteration 10/25 | Loss: 0.00090371
Iteration 11/25 | Loss: 0.00090371
Iteration 12/25 | Loss: 0.00090371
Iteration 13/25 | Loss: 0.00090371
Iteration 14/25 | Loss: 0.00090371
Iteration 15/25 | Loss: 0.00090371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009037097916007042, 0.0009037097916007042, 0.0009037097916007042, 0.0009037097916007042, 0.0009037097916007042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009037097916007042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94951487
Iteration 2/25 | Loss: 0.00060365
Iteration 3/25 | Loss: 0.00060365
Iteration 4/25 | Loss: 0.00060364
Iteration 5/25 | Loss: 0.00060364
Iteration 6/25 | Loss: 0.00060364
Iteration 7/25 | Loss: 0.00060364
Iteration 8/25 | Loss: 0.00060364
Iteration 9/25 | Loss: 0.00060364
Iteration 10/25 | Loss: 0.00060364
Iteration 11/25 | Loss: 0.00060364
Iteration 12/25 | Loss: 0.00060364
Iteration 13/25 | Loss: 0.00060364
Iteration 14/25 | Loss: 0.00060364
Iteration 15/25 | Loss: 0.00060364
Iteration 16/25 | Loss: 0.00060364
Iteration 17/25 | Loss: 0.00060364
Iteration 18/25 | Loss: 0.00060364
Iteration 19/25 | Loss: 0.00060364
Iteration 20/25 | Loss: 0.00060364
Iteration 21/25 | Loss: 0.00060364
Iteration 22/25 | Loss: 0.00060364
Iteration 23/25 | Loss: 0.00060364
Iteration 24/25 | Loss: 0.00060364
Iteration 25/25 | Loss: 0.00060364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060364
Iteration 2/1000 | Loss: 0.00001778
Iteration 3/1000 | Loss: 0.00001143
Iteration 4/1000 | Loss: 0.00001016
Iteration 5/1000 | Loss: 0.00000957
Iteration 6/1000 | Loss: 0.00000924
Iteration 7/1000 | Loss: 0.00000901
Iteration 8/1000 | Loss: 0.00000900
Iteration 9/1000 | Loss: 0.00000899
Iteration 10/1000 | Loss: 0.00000892
Iteration 11/1000 | Loss: 0.00000891
Iteration 12/1000 | Loss: 0.00000891
Iteration 13/1000 | Loss: 0.00000890
Iteration 14/1000 | Loss: 0.00000889
Iteration 15/1000 | Loss: 0.00000889
Iteration 16/1000 | Loss: 0.00000886
Iteration 17/1000 | Loss: 0.00000883
Iteration 18/1000 | Loss: 0.00000880
Iteration 19/1000 | Loss: 0.00000880
Iteration 20/1000 | Loss: 0.00000880
Iteration 21/1000 | Loss: 0.00000879
Iteration 22/1000 | Loss: 0.00000879
Iteration 23/1000 | Loss: 0.00000878
Iteration 24/1000 | Loss: 0.00000877
Iteration 25/1000 | Loss: 0.00000877
Iteration 26/1000 | Loss: 0.00000877
Iteration 27/1000 | Loss: 0.00000875
Iteration 28/1000 | Loss: 0.00000875
Iteration 29/1000 | Loss: 0.00000875
Iteration 30/1000 | Loss: 0.00000874
Iteration 31/1000 | Loss: 0.00000874
Iteration 32/1000 | Loss: 0.00000874
Iteration 33/1000 | Loss: 0.00000874
Iteration 34/1000 | Loss: 0.00000873
Iteration 35/1000 | Loss: 0.00000873
Iteration 36/1000 | Loss: 0.00000873
Iteration 37/1000 | Loss: 0.00000873
Iteration 38/1000 | Loss: 0.00000872
Iteration 39/1000 | Loss: 0.00000872
Iteration 40/1000 | Loss: 0.00000872
Iteration 41/1000 | Loss: 0.00000871
Iteration 42/1000 | Loss: 0.00000871
Iteration 43/1000 | Loss: 0.00000871
Iteration 44/1000 | Loss: 0.00000871
Iteration 45/1000 | Loss: 0.00000871
Iteration 46/1000 | Loss: 0.00000870
Iteration 47/1000 | Loss: 0.00000870
Iteration 48/1000 | Loss: 0.00000870
Iteration 49/1000 | Loss: 0.00000870
Iteration 50/1000 | Loss: 0.00000870
Iteration 51/1000 | Loss: 0.00000870
Iteration 52/1000 | Loss: 0.00000870
Iteration 53/1000 | Loss: 0.00000870
Iteration 54/1000 | Loss: 0.00000870
Iteration 55/1000 | Loss: 0.00000870
Iteration 56/1000 | Loss: 0.00000869
Iteration 57/1000 | Loss: 0.00000869
Iteration 58/1000 | Loss: 0.00000869
Iteration 59/1000 | Loss: 0.00000869
Iteration 60/1000 | Loss: 0.00000869
Iteration 61/1000 | Loss: 0.00000868
Iteration 62/1000 | Loss: 0.00000868
Iteration 63/1000 | Loss: 0.00000868
Iteration 64/1000 | Loss: 0.00000868
Iteration 65/1000 | Loss: 0.00000868
Iteration 66/1000 | Loss: 0.00000868
Iteration 67/1000 | Loss: 0.00000867
Iteration 68/1000 | Loss: 0.00000867
Iteration 69/1000 | Loss: 0.00000866
Iteration 70/1000 | Loss: 0.00000866
Iteration 71/1000 | Loss: 0.00000866
Iteration 72/1000 | Loss: 0.00000866
Iteration 73/1000 | Loss: 0.00000866
Iteration 74/1000 | Loss: 0.00000865
Iteration 75/1000 | Loss: 0.00000865
Iteration 76/1000 | Loss: 0.00000865
Iteration 77/1000 | Loss: 0.00000865
Iteration 78/1000 | Loss: 0.00000865
Iteration 79/1000 | Loss: 0.00000865
Iteration 80/1000 | Loss: 0.00000865
Iteration 81/1000 | Loss: 0.00000865
Iteration 82/1000 | Loss: 0.00000865
Iteration 83/1000 | Loss: 0.00000865
Iteration 84/1000 | Loss: 0.00000865
Iteration 85/1000 | Loss: 0.00000865
Iteration 86/1000 | Loss: 0.00000864
Iteration 87/1000 | Loss: 0.00000864
Iteration 88/1000 | Loss: 0.00000864
Iteration 89/1000 | Loss: 0.00000864
Iteration 90/1000 | Loss: 0.00000864
Iteration 91/1000 | Loss: 0.00000864
Iteration 92/1000 | Loss: 0.00000864
Iteration 93/1000 | Loss: 0.00000864
Iteration 94/1000 | Loss: 0.00000864
Iteration 95/1000 | Loss: 0.00000864
Iteration 96/1000 | Loss: 0.00000864
Iteration 97/1000 | Loss: 0.00000864
Iteration 98/1000 | Loss: 0.00000864
Iteration 99/1000 | Loss: 0.00000864
Iteration 100/1000 | Loss: 0.00000863
Iteration 101/1000 | Loss: 0.00000863
Iteration 102/1000 | Loss: 0.00000863
Iteration 103/1000 | Loss: 0.00000863
Iteration 104/1000 | Loss: 0.00000863
Iteration 105/1000 | Loss: 0.00000863
Iteration 106/1000 | Loss: 0.00000862
Iteration 107/1000 | Loss: 0.00000862
Iteration 108/1000 | Loss: 0.00000862
Iteration 109/1000 | Loss: 0.00000862
Iteration 110/1000 | Loss: 0.00000862
Iteration 111/1000 | Loss: 0.00000862
Iteration 112/1000 | Loss: 0.00000861
Iteration 113/1000 | Loss: 0.00000861
Iteration 114/1000 | Loss: 0.00000861
Iteration 115/1000 | Loss: 0.00000861
Iteration 116/1000 | Loss: 0.00000861
Iteration 117/1000 | Loss: 0.00000861
Iteration 118/1000 | Loss: 0.00000861
Iteration 119/1000 | Loss: 0.00000861
Iteration 120/1000 | Loss: 0.00000861
Iteration 121/1000 | Loss: 0.00000861
Iteration 122/1000 | Loss: 0.00000861
Iteration 123/1000 | Loss: 0.00000861
Iteration 124/1000 | Loss: 0.00000860
Iteration 125/1000 | Loss: 0.00000860
Iteration 126/1000 | Loss: 0.00000860
Iteration 127/1000 | Loss: 0.00000860
Iteration 128/1000 | Loss: 0.00000860
Iteration 129/1000 | Loss: 0.00000860
Iteration 130/1000 | Loss: 0.00000860
Iteration 131/1000 | Loss: 0.00000860
Iteration 132/1000 | Loss: 0.00000860
Iteration 133/1000 | Loss: 0.00000860
Iteration 134/1000 | Loss: 0.00000860
Iteration 135/1000 | Loss: 0.00000860
Iteration 136/1000 | Loss: 0.00000860
Iteration 137/1000 | Loss: 0.00000860
Iteration 138/1000 | Loss: 0.00000860
Iteration 139/1000 | Loss: 0.00000860
Iteration 140/1000 | Loss: 0.00000860
Iteration 141/1000 | Loss: 0.00000860
Iteration 142/1000 | Loss: 0.00000860
Iteration 143/1000 | Loss: 0.00000860
Iteration 144/1000 | Loss: 0.00000860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [8.602994967077393e-06, 8.602994967077393e-06, 8.602994967077393e-06, 8.602994967077393e-06, 8.602994967077393e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.602994967077393e-06

Optimization complete. Final v2v error: 2.5075652599334717 mm

Highest mean error: 2.772557020187378 mm for frame 80

Lowest mean error: 2.315782070159912 mm for frame 56

Saving results

Total time: 30.60249900817871
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395125
Iteration 2/25 | Loss: 0.00096791
Iteration 3/25 | Loss: 0.00090140
Iteration 4/25 | Loss: 0.00089462
Iteration 5/25 | Loss: 0.00089248
Iteration 6/25 | Loss: 0.00089196
Iteration 7/25 | Loss: 0.00089196
Iteration 8/25 | Loss: 0.00089196
Iteration 9/25 | Loss: 0.00089196
Iteration 10/25 | Loss: 0.00089196
Iteration 11/25 | Loss: 0.00089196
Iteration 12/25 | Loss: 0.00089196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008919569081626832, 0.0008919569081626832, 0.0008919569081626832, 0.0008919569081626832, 0.0008919569081626832]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008919569081626832

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.00923586
Iteration 2/25 | Loss: 0.00058906
Iteration 3/25 | Loss: 0.00058905
Iteration 4/25 | Loss: 0.00058905
Iteration 5/25 | Loss: 0.00058905
Iteration 6/25 | Loss: 0.00058905
Iteration 7/25 | Loss: 0.00058905
Iteration 8/25 | Loss: 0.00058905
Iteration 9/25 | Loss: 0.00058905
Iteration 10/25 | Loss: 0.00058905
Iteration 11/25 | Loss: 0.00058905
Iteration 12/25 | Loss: 0.00058905
Iteration 13/25 | Loss: 0.00058905
Iteration 14/25 | Loss: 0.00058905
Iteration 15/25 | Loss: 0.00058905
Iteration 16/25 | Loss: 0.00058905
Iteration 17/25 | Loss: 0.00058905
Iteration 18/25 | Loss: 0.00058905
Iteration 19/25 | Loss: 0.00058905
Iteration 20/25 | Loss: 0.00058905
Iteration 21/25 | Loss: 0.00058905
Iteration 22/25 | Loss: 0.00058905
Iteration 23/25 | Loss: 0.00058905
Iteration 24/25 | Loss: 0.00058905
Iteration 25/25 | Loss: 0.00058905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058905
Iteration 2/1000 | Loss: 0.00002083
Iteration 3/1000 | Loss: 0.00001002
Iteration 4/1000 | Loss: 0.00000921
Iteration 5/1000 | Loss: 0.00000839
Iteration 6/1000 | Loss: 0.00000809
Iteration 7/1000 | Loss: 0.00000804
Iteration 8/1000 | Loss: 0.00000777
Iteration 9/1000 | Loss: 0.00000767
Iteration 10/1000 | Loss: 0.00000763
Iteration 11/1000 | Loss: 0.00000763
Iteration 12/1000 | Loss: 0.00000762
Iteration 13/1000 | Loss: 0.00000762
Iteration 14/1000 | Loss: 0.00000754
Iteration 15/1000 | Loss: 0.00000749
Iteration 16/1000 | Loss: 0.00000746
Iteration 17/1000 | Loss: 0.00000745
Iteration 18/1000 | Loss: 0.00000745
Iteration 19/1000 | Loss: 0.00000745
Iteration 20/1000 | Loss: 0.00000744
Iteration 21/1000 | Loss: 0.00000744
Iteration 22/1000 | Loss: 0.00000744
Iteration 23/1000 | Loss: 0.00000744
Iteration 24/1000 | Loss: 0.00000744
Iteration 25/1000 | Loss: 0.00000743
Iteration 26/1000 | Loss: 0.00000743
Iteration 27/1000 | Loss: 0.00000741
Iteration 28/1000 | Loss: 0.00000741
Iteration 29/1000 | Loss: 0.00000741
Iteration 30/1000 | Loss: 0.00000741
Iteration 31/1000 | Loss: 0.00000741
Iteration 32/1000 | Loss: 0.00000741
Iteration 33/1000 | Loss: 0.00000740
Iteration 34/1000 | Loss: 0.00000740
Iteration 35/1000 | Loss: 0.00000739
Iteration 36/1000 | Loss: 0.00000739
Iteration 37/1000 | Loss: 0.00000739
Iteration 38/1000 | Loss: 0.00000739
Iteration 39/1000 | Loss: 0.00000739
Iteration 40/1000 | Loss: 0.00000739
Iteration 41/1000 | Loss: 0.00000739
Iteration 42/1000 | Loss: 0.00000738
Iteration 43/1000 | Loss: 0.00000738
Iteration 44/1000 | Loss: 0.00000738
Iteration 45/1000 | Loss: 0.00000737
Iteration 46/1000 | Loss: 0.00000737
Iteration 47/1000 | Loss: 0.00000737
Iteration 48/1000 | Loss: 0.00000736
Iteration 49/1000 | Loss: 0.00000736
Iteration 50/1000 | Loss: 0.00000736
Iteration 51/1000 | Loss: 0.00000735
Iteration 52/1000 | Loss: 0.00000735
Iteration 53/1000 | Loss: 0.00000735
Iteration 54/1000 | Loss: 0.00000735
Iteration 55/1000 | Loss: 0.00000735
Iteration 56/1000 | Loss: 0.00000734
Iteration 57/1000 | Loss: 0.00000734
Iteration 58/1000 | Loss: 0.00000734
Iteration 59/1000 | Loss: 0.00000733
Iteration 60/1000 | Loss: 0.00000733
Iteration 61/1000 | Loss: 0.00000733
Iteration 62/1000 | Loss: 0.00000733
Iteration 63/1000 | Loss: 0.00000733
Iteration 64/1000 | Loss: 0.00000733
Iteration 65/1000 | Loss: 0.00000733
Iteration 66/1000 | Loss: 0.00000732
Iteration 67/1000 | Loss: 0.00000732
Iteration 68/1000 | Loss: 0.00000732
Iteration 69/1000 | Loss: 0.00000732
Iteration 70/1000 | Loss: 0.00000732
Iteration 71/1000 | Loss: 0.00000732
Iteration 72/1000 | Loss: 0.00000732
Iteration 73/1000 | Loss: 0.00000732
Iteration 74/1000 | Loss: 0.00000732
Iteration 75/1000 | Loss: 0.00000732
Iteration 76/1000 | Loss: 0.00000732
Iteration 77/1000 | Loss: 0.00000732
Iteration 78/1000 | Loss: 0.00000731
Iteration 79/1000 | Loss: 0.00000731
Iteration 80/1000 | Loss: 0.00000730
Iteration 81/1000 | Loss: 0.00000730
Iteration 82/1000 | Loss: 0.00000730
Iteration 83/1000 | Loss: 0.00000730
Iteration 84/1000 | Loss: 0.00000730
Iteration 85/1000 | Loss: 0.00000730
Iteration 86/1000 | Loss: 0.00000729
Iteration 87/1000 | Loss: 0.00000729
Iteration 88/1000 | Loss: 0.00000729
Iteration 89/1000 | Loss: 0.00000729
Iteration 90/1000 | Loss: 0.00000729
Iteration 91/1000 | Loss: 0.00000728
Iteration 92/1000 | Loss: 0.00000728
Iteration 93/1000 | Loss: 0.00000728
Iteration 94/1000 | Loss: 0.00000728
Iteration 95/1000 | Loss: 0.00000728
Iteration 96/1000 | Loss: 0.00000728
Iteration 97/1000 | Loss: 0.00000728
Iteration 98/1000 | Loss: 0.00000728
Iteration 99/1000 | Loss: 0.00000728
Iteration 100/1000 | Loss: 0.00000728
Iteration 101/1000 | Loss: 0.00000728
Iteration 102/1000 | Loss: 0.00000728
Iteration 103/1000 | Loss: 0.00000728
Iteration 104/1000 | Loss: 0.00000728
Iteration 105/1000 | Loss: 0.00000728
Iteration 106/1000 | Loss: 0.00000727
Iteration 107/1000 | Loss: 0.00000727
Iteration 108/1000 | Loss: 0.00000727
Iteration 109/1000 | Loss: 0.00000727
Iteration 110/1000 | Loss: 0.00000727
Iteration 111/1000 | Loss: 0.00000727
Iteration 112/1000 | Loss: 0.00000727
Iteration 113/1000 | Loss: 0.00000727
Iteration 114/1000 | Loss: 0.00000727
Iteration 115/1000 | Loss: 0.00000727
Iteration 116/1000 | Loss: 0.00000727
Iteration 117/1000 | Loss: 0.00000727
Iteration 118/1000 | Loss: 0.00000727
Iteration 119/1000 | Loss: 0.00000727
Iteration 120/1000 | Loss: 0.00000727
Iteration 121/1000 | Loss: 0.00000727
Iteration 122/1000 | Loss: 0.00000727
Iteration 123/1000 | Loss: 0.00000727
Iteration 124/1000 | Loss: 0.00000727
Iteration 125/1000 | Loss: 0.00000727
Iteration 126/1000 | Loss: 0.00000727
Iteration 127/1000 | Loss: 0.00000727
Iteration 128/1000 | Loss: 0.00000727
Iteration 129/1000 | Loss: 0.00000727
Iteration 130/1000 | Loss: 0.00000727
Iteration 131/1000 | Loss: 0.00000727
Iteration 132/1000 | Loss: 0.00000727
Iteration 133/1000 | Loss: 0.00000727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [7.2746252044453286e-06, 7.2746252044453286e-06, 7.2746252044453286e-06, 7.2746252044453286e-06, 7.2746252044453286e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.2746252044453286e-06

Optimization complete. Final v2v error: 2.3106281757354736 mm

Highest mean error: 2.4629905223846436 mm for frame 112

Lowest mean error: 2.192427158355713 mm for frame 38

Saving results

Total time: 30.016109943389893
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858333
Iteration 2/25 | Loss: 0.00105219
Iteration 3/25 | Loss: 0.00097795
Iteration 4/25 | Loss: 0.00095253
Iteration 5/25 | Loss: 0.00094368
Iteration 6/25 | Loss: 0.00094155
Iteration 7/25 | Loss: 0.00094155
Iteration 8/25 | Loss: 0.00094155
Iteration 9/25 | Loss: 0.00094155
Iteration 10/25 | Loss: 0.00094155
Iteration 11/25 | Loss: 0.00094155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009415493113920093, 0.0009415493113920093, 0.0009415493113920093, 0.0009415493113920093, 0.0009415493113920093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009415493113920093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.32813978
Iteration 2/25 | Loss: 0.00094847
Iteration 3/25 | Loss: 0.00094847
Iteration 4/25 | Loss: 0.00094846
Iteration 5/25 | Loss: 0.00094846
Iteration 6/25 | Loss: 0.00094846
Iteration 7/25 | Loss: 0.00094846
Iteration 8/25 | Loss: 0.00094846
Iteration 9/25 | Loss: 0.00094846
Iteration 10/25 | Loss: 0.00094846
Iteration 11/25 | Loss: 0.00094846
Iteration 12/25 | Loss: 0.00094846
Iteration 13/25 | Loss: 0.00094846
Iteration 14/25 | Loss: 0.00094846
Iteration 15/25 | Loss: 0.00094846
Iteration 16/25 | Loss: 0.00094846
Iteration 17/25 | Loss: 0.00094846
Iteration 18/25 | Loss: 0.00094846
Iteration 19/25 | Loss: 0.00094846
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00094846315914765, 0.00094846315914765, 0.00094846315914765, 0.00094846315914765, 0.00094846315914765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00094846315914765

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094846
Iteration 2/1000 | Loss: 0.00002675
Iteration 3/1000 | Loss: 0.00001633
Iteration 4/1000 | Loss: 0.00001482
Iteration 5/1000 | Loss: 0.00001434
Iteration 6/1000 | Loss: 0.00001377
Iteration 7/1000 | Loss: 0.00001347
Iteration 8/1000 | Loss: 0.00001318
Iteration 9/1000 | Loss: 0.00001310
Iteration 10/1000 | Loss: 0.00001310
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001301
Iteration 13/1000 | Loss: 0.00001300
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001285
Iteration 16/1000 | Loss: 0.00001284
Iteration 17/1000 | Loss: 0.00001282
Iteration 18/1000 | Loss: 0.00001282
Iteration 19/1000 | Loss: 0.00001281
Iteration 20/1000 | Loss: 0.00001280
Iteration 21/1000 | Loss: 0.00001276
Iteration 22/1000 | Loss: 0.00001275
Iteration 23/1000 | Loss: 0.00001275
Iteration 24/1000 | Loss: 0.00001275
Iteration 25/1000 | Loss: 0.00001275
Iteration 26/1000 | Loss: 0.00001274
Iteration 27/1000 | Loss: 0.00001274
Iteration 28/1000 | Loss: 0.00001274
Iteration 29/1000 | Loss: 0.00001273
Iteration 30/1000 | Loss: 0.00001272
Iteration 31/1000 | Loss: 0.00001266
Iteration 32/1000 | Loss: 0.00001265
Iteration 33/1000 | Loss: 0.00001263
Iteration 34/1000 | Loss: 0.00001263
Iteration 35/1000 | Loss: 0.00001263
Iteration 36/1000 | Loss: 0.00001263
Iteration 37/1000 | Loss: 0.00001263
Iteration 38/1000 | Loss: 0.00001263
Iteration 39/1000 | Loss: 0.00001263
Iteration 40/1000 | Loss: 0.00001263
Iteration 41/1000 | Loss: 0.00001263
Iteration 42/1000 | Loss: 0.00001262
Iteration 43/1000 | Loss: 0.00001262
Iteration 44/1000 | Loss: 0.00001262
Iteration 45/1000 | Loss: 0.00001261
Iteration 46/1000 | Loss: 0.00001260
Iteration 47/1000 | Loss: 0.00001260
Iteration 48/1000 | Loss: 0.00001260
Iteration 49/1000 | Loss: 0.00001260
Iteration 50/1000 | Loss: 0.00001260
Iteration 51/1000 | Loss: 0.00001259
Iteration 52/1000 | Loss: 0.00001259
Iteration 53/1000 | Loss: 0.00001258
Iteration 54/1000 | Loss: 0.00001258
Iteration 55/1000 | Loss: 0.00001258
Iteration 56/1000 | Loss: 0.00001258
Iteration 57/1000 | Loss: 0.00001258
Iteration 58/1000 | Loss: 0.00001258
Iteration 59/1000 | Loss: 0.00001257
Iteration 60/1000 | Loss: 0.00001257
Iteration 61/1000 | Loss: 0.00001257
Iteration 62/1000 | Loss: 0.00001257
Iteration 63/1000 | Loss: 0.00001257
Iteration 64/1000 | Loss: 0.00001257
Iteration 65/1000 | Loss: 0.00001257
Iteration 66/1000 | Loss: 0.00001257
Iteration 67/1000 | Loss: 0.00001257
Iteration 68/1000 | Loss: 0.00001256
Iteration 69/1000 | Loss: 0.00001256
Iteration 70/1000 | Loss: 0.00001256
Iteration 71/1000 | Loss: 0.00001256
Iteration 72/1000 | Loss: 0.00001256
Iteration 73/1000 | Loss: 0.00001256
Iteration 74/1000 | Loss: 0.00001256
Iteration 75/1000 | Loss: 0.00001256
Iteration 76/1000 | Loss: 0.00001256
Iteration 77/1000 | Loss: 0.00001256
Iteration 78/1000 | Loss: 0.00001256
Iteration 79/1000 | Loss: 0.00001256
Iteration 80/1000 | Loss: 0.00001255
Iteration 81/1000 | Loss: 0.00001255
Iteration 82/1000 | Loss: 0.00001255
Iteration 83/1000 | Loss: 0.00001255
Iteration 84/1000 | Loss: 0.00001255
Iteration 85/1000 | Loss: 0.00001255
Iteration 86/1000 | Loss: 0.00001255
Iteration 87/1000 | Loss: 0.00001255
Iteration 88/1000 | Loss: 0.00001255
Iteration 89/1000 | Loss: 0.00001255
Iteration 90/1000 | Loss: 0.00001255
Iteration 91/1000 | Loss: 0.00001255
Iteration 92/1000 | Loss: 0.00001255
Iteration 93/1000 | Loss: 0.00001255
Iteration 94/1000 | Loss: 0.00001254
Iteration 95/1000 | Loss: 0.00001254
Iteration 96/1000 | Loss: 0.00001254
Iteration 97/1000 | Loss: 0.00001254
Iteration 98/1000 | Loss: 0.00001254
Iteration 99/1000 | Loss: 0.00001254
Iteration 100/1000 | Loss: 0.00001254
Iteration 101/1000 | Loss: 0.00001254
Iteration 102/1000 | Loss: 0.00001254
Iteration 103/1000 | Loss: 0.00001254
Iteration 104/1000 | Loss: 0.00001254
Iteration 105/1000 | Loss: 0.00001254
Iteration 106/1000 | Loss: 0.00001254
Iteration 107/1000 | Loss: 0.00001254
Iteration 108/1000 | Loss: 0.00001254
Iteration 109/1000 | Loss: 0.00001254
Iteration 110/1000 | Loss: 0.00001254
Iteration 111/1000 | Loss: 0.00001254
Iteration 112/1000 | Loss: 0.00001254
Iteration 113/1000 | Loss: 0.00001253
Iteration 114/1000 | Loss: 0.00001253
Iteration 115/1000 | Loss: 0.00001253
Iteration 116/1000 | Loss: 0.00001253
Iteration 117/1000 | Loss: 0.00001253
Iteration 118/1000 | Loss: 0.00001253
Iteration 119/1000 | Loss: 0.00001253
Iteration 120/1000 | Loss: 0.00001253
Iteration 121/1000 | Loss: 0.00001253
Iteration 122/1000 | Loss: 0.00001253
Iteration 123/1000 | Loss: 0.00001253
Iteration 124/1000 | Loss: 0.00001253
Iteration 125/1000 | Loss: 0.00001253
Iteration 126/1000 | Loss: 0.00001253
Iteration 127/1000 | Loss: 0.00001253
Iteration 128/1000 | Loss: 0.00001252
Iteration 129/1000 | Loss: 0.00001252
Iteration 130/1000 | Loss: 0.00001252
Iteration 131/1000 | Loss: 0.00001252
Iteration 132/1000 | Loss: 0.00001252
Iteration 133/1000 | Loss: 0.00001252
Iteration 134/1000 | Loss: 0.00001252
Iteration 135/1000 | Loss: 0.00001252
Iteration 136/1000 | Loss: 0.00001252
Iteration 137/1000 | Loss: 0.00001251
Iteration 138/1000 | Loss: 0.00001251
Iteration 139/1000 | Loss: 0.00001251
Iteration 140/1000 | Loss: 0.00001251
Iteration 141/1000 | Loss: 0.00001251
Iteration 142/1000 | Loss: 0.00001251
Iteration 143/1000 | Loss: 0.00001251
Iteration 144/1000 | Loss: 0.00001251
Iteration 145/1000 | Loss: 0.00001251
Iteration 146/1000 | Loss: 0.00001251
Iteration 147/1000 | Loss: 0.00001251
Iteration 148/1000 | Loss: 0.00001251
Iteration 149/1000 | Loss: 0.00001251
Iteration 150/1000 | Loss: 0.00001251
Iteration 151/1000 | Loss: 0.00001251
Iteration 152/1000 | Loss: 0.00001251
Iteration 153/1000 | Loss: 0.00001251
Iteration 154/1000 | Loss: 0.00001251
Iteration 155/1000 | Loss: 0.00001251
Iteration 156/1000 | Loss: 0.00001251
Iteration 157/1000 | Loss: 0.00001250
Iteration 158/1000 | Loss: 0.00001250
Iteration 159/1000 | Loss: 0.00001250
Iteration 160/1000 | Loss: 0.00001250
Iteration 161/1000 | Loss: 0.00001250
Iteration 162/1000 | Loss: 0.00001250
Iteration 163/1000 | Loss: 0.00001250
Iteration 164/1000 | Loss: 0.00001250
Iteration 165/1000 | Loss: 0.00001250
Iteration 166/1000 | Loss: 0.00001250
Iteration 167/1000 | Loss: 0.00001249
Iteration 168/1000 | Loss: 0.00001249
Iteration 169/1000 | Loss: 0.00001249
Iteration 170/1000 | Loss: 0.00001249
Iteration 171/1000 | Loss: 0.00001249
Iteration 172/1000 | Loss: 0.00001249
Iteration 173/1000 | Loss: 0.00001249
Iteration 174/1000 | Loss: 0.00001249
Iteration 175/1000 | Loss: 0.00001249
Iteration 176/1000 | Loss: 0.00001249
Iteration 177/1000 | Loss: 0.00001249
Iteration 178/1000 | Loss: 0.00001249
Iteration 179/1000 | Loss: 0.00001249
Iteration 180/1000 | Loss: 0.00001249
Iteration 181/1000 | Loss: 0.00001249
Iteration 182/1000 | Loss: 0.00001249
Iteration 183/1000 | Loss: 0.00001249
Iteration 184/1000 | Loss: 0.00001249
Iteration 185/1000 | Loss: 0.00001249
Iteration 186/1000 | Loss: 0.00001249
Iteration 187/1000 | Loss: 0.00001249
Iteration 188/1000 | Loss: 0.00001248
Iteration 189/1000 | Loss: 0.00001248
Iteration 190/1000 | Loss: 0.00001248
Iteration 191/1000 | Loss: 0.00001248
Iteration 192/1000 | Loss: 0.00001248
Iteration 193/1000 | Loss: 0.00001248
Iteration 194/1000 | Loss: 0.00001248
Iteration 195/1000 | Loss: 0.00001248
Iteration 196/1000 | Loss: 0.00001248
Iteration 197/1000 | Loss: 0.00001248
Iteration 198/1000 | Loss: 0.00001248
Iteration 199/1000 | Loss: 0.00001248
Iteration 200/1000 | Loss: 0.00001248
Iteration 201/1000 | Loss: 0.00001248
Iteration 202/1000 | Loss: 0.00001248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.2484823855629656e-05, 1.2484823855629656e-05, 1.2484823855629656e-05, 1.2484823855629656e-05, 1.2484823855629656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2484823855629656e-05

Optimization complete. Final v2v error: 2.9657959938049316 mm

Highest mean error: 3.666630506515503 mm for frame 55

Lowest mean error: 2.4681997299194336 mm for frame 221

Saving results

Total time: 40.6947283744812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819820
Iteration 2/25 | Loss: 0.00148957
Iteration 3/25 | Loss: 0.00114402
Iteration 4/25 | Loss: 0.00111817
Iteration 5/25 | Loss: 0.00109984
Iteration 6/25 | Loss: 0.00109088
Iteration 7/25 | Loss: 0.00108732
Iteration 8/25 | Loss: 0.00108644
Iteration 9/25 | Loss: 0.00108870
Iteration 10/25 | Loss: 0.00108571
Iteration 11/25 | Loss: 0.00108457
Iteration 12/25 | Loss: 0.00108440
Iteration 13/25 | Loss: 0.00108438
Iteration 14/25 | Loss: 0.00108438
Iteration 15/25 | Loss: 0.00108438
Iteration 16/25 | Loss: 0.00108437
Iteration 17/25 | Loss: 0.00108437
Iteration 18/25 | Loss: 0.00108437
Iteration 19/25 | Loss: 0.00108437
Iteration 20/25 | Loss: 0.00108437
Iteration 21/25 | Loss: 0.00108437
Iteration 22/25 | Loss: 0.00108434
Iteration 23/25 | Loss: 0.00108434
Iteration 24/25 | Loss: 0.00108434
Iteration 25/25 | Loss: 0.00108433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54037905
Iteration 2/25 | Loss: 0.00046916
Iteration 3/25 | Loss: 0.00046915
Iteration 4/25 | Loss: 0.00046915
Iteration 5/25 | Loss: 0.00046914
Iteration 6/25 | Loss: 0.00046914
Iteration 7/25 | Loss: 0.00046914
Iteration 8/25 | Loss: 0.00046914
Iteration 9/25 | Loss: 0.00046914
Iteration 10/25 | Loss: 0.00046914
Iteration 11/25 | Loss: 0.00046914
Iteration 12/25 | Loss: 0.00046914
Iteration 13/25 | Loss: 0.00046914
Iteration 14/25 | Loss: 0.00046914
Iteration 15/25 | Loss: 0.00046914
Iteration 16/25 | Loss: 0.00046914
Iteration 17/25 | Loss: 0.00046914
Iteration 18/25 | Loss: 0.00046914
Iteration 19/25 | Loss: 0.00046914
Iteration 20/25 | Loss: 0.00046914
Iteration 21/25 | Loss: 0.00046914
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004691426584031433, 0.0004691426584031433, 0.0004691426584031433, 0.0004691426584031433, 0.0004691426584031433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004691426584031433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046914
Iteration 2/1000 | Loss: 0.00004487
Iteration 3/1000 | Loss: 0.00003059
Iteration 4/1000 | Loss: 0.00002511
Iteration 5/1000 | Loss: 0.00002349
Iteration 6/1000 | Loss: 0.00002256
Iteration 7/1000 | Loss: 0.00002208
Iteration 8/1000 | Loss: 0.00002173
Iteration 9/1000 | Loss: 0.00002143
Iteration 10/1000 | Loss: 0.00002128
Iteration 11/1000 | Loss: 0.00002110
Iteration 12/1000 | Loss: 0.00002093
Iteration 13/1000 | Loss: 0.00002089
Iteration 14/1000 | Loss: 0.00002088
Iteration 15/1000 | Loss: 0.00002085
Iteration 16/1000 | Loss: 0.00002078
Iteration 17/1000 | Loss: 0.00002072
Iteration 18/1000 | Loss: 0.00002071
Iteration 19/1000 | Loss: 0.00002071
Iteration 20/1000 | Loss: 0.00002070
Iteration 21/1000 | Loss: 0.00002069
Iteration 22/1000 | Loss: 0.00002069
Iteration 23/1000 | Loss: 0.00002069
Iteration 24/1000 | Loss: 0.00002068
Iteration 25/1000 | Loss: 0.00002067
Iteration 26/1000 | Loss: 0.00002067
Iteration 27/1000 | Loss: 0.00002066
Iteration 28/1000 | Loss: 0.00002066
Iteration 29/1000 | Loss: 0.00002065
Iteration 30/1000 | Loss: 0.00002063
Iteration 31/1000 | Loss: 0.00002062
Iteration 32/1000 | Loss: 0.00002062
Iteration 33/1000 | Loss: 0.00002062
Iteration 34/1000 | Loss: 0.00002062
Iteration 35/1000 | Loss: 0.00002061
Iteration 36/1000 | Loss: 0.00002060
Iteration 37/1000 | Loss: 0.00002060
Iteration 38/1000 | Loss: 0.00002059
Iteration 39/1000 | Loss: 0.00002059
Iteration 40/1000 | Loss: 0.00002059
Iteration 41/1000 | Loss: 0.00002059
Iteration 42/1000 | Loss: 0.00002059
Iteration 43/1000 | Loss: 0.00002059
Iteration 44/1000 | Loss: 0.00002059
Iteration 45/1000 | Loss: 0.00002058
Iteration 46/1000 | Loss: 0.00002058
Iteration 47/1000 | Loss: 0.00002057
Iteration 48/1000 | Loss: 0.00002057
Iteration 49/1000 | Loss: 0.00002056
Iteration 50/1000 | Loss: 0.00002056
Iteration 51/1000 | Loss: 0.00002055
Iteration 52/1000 | Loss: 0.00002055
Iteration 53/1000 | Loss: 0.00002055
Iteration 54/1000 | Loss: 0.00002055
Iteration 55/1000 | Loss: 0.00002054
Iteration 56/1000 | Loss: 0.00002054
Iteration 57/1000 | Loss: 0.00002054
Iteration 58/1000 | Loss: 0.00002054
Iteration 59/1000 | Loss: 0.00002053
Iteration 60/1000 | Loss: 0.00002053
Iteration 61/1000 | Loss: 0.00002052
Iteration 62/1000 | Loss: 0.00002052
Iteration 63/1000 | Loss: 0.00002052
Iteration 64/1000 | Loss: 0.00002052
Iteration 65/1000 | Loss: 0.00002052
Iteration 66/1000 | Loss: 0.00002051
Iteration 67/1000 | Loss: 0.00002051
Iteration 68/1000 | Loss: 0.00002051
Iteration 69/1000 | Loss: 0.00002051
Iteration 70/1000 | Loss: 0.00002051
Iteration 71/1000 | Loss: 0.00002051
Iteration 72/1000 | Loss: 0.00002050
Iteration 73/1000 | Loss: 0.00002050
Iteration 74/1000 | Loss: 0.00002050
Iteration 75/1000 | Loss: 0.00002050
Iteration 76/1000 | Loss: 0.00002050
Iteration 77/1000 | Loss: 0.00002050
Iteration 78/1000 | Loss: 0.00002050
Iteration 79/1000 | Loss: 0.00002050
Iteration 80/1000 | Loss: 0.00002049
Iteration 81/1000 | Loss: 0.00002049
Iteration 82/1000 | Loss: 0.00002049
Iteration 83/1000 | Loss: 0.00002049
Iteration 84/1000 | Loss: 0.00002049
Iteration 85/1000 | Loss: 0.00002049
Iteration 86/1000 | Loss: 0.00002049
Iteration 87/1000 | Loss: 0.00002048
Iteration 88/1000 | Loss: 0.00002048
Iteration 89/1000 | Loss: 0.00002048
Iteration 90/1000 | Loss: 0.00002048
Iteration 91/1000 | Loss: 0.00002048
Iteration 92/1000 | Loss: 0.00002048
Iteration 93/1000 | Loss: 0.00002047
Iteration 94/1000 | Loss: 0.00002047
Iteration 95/1000 | Loss: 0.00002047
Iteration 96/1000 | Loss: 0.00002047
Iteration 97/1000 | Loss: 0.00002046
Iteration 98/1000 | Loss: 0.00002046
Iteration 99/1000 | Loss: 0.00002046
Iteration 100/1000 | Loss: 0.00002046
Iteration 101/1000 | Loss: 0.00002046
Iteration 102/1000 | Loss: 0.00002046
Iteration 103/1000 | Loss: 0.00002046
Iteration 104/1000 | Loss: 0.00002046
Iteration 105/1000 | Loss: 0.00002046
Iteration 106/1000 | Loss: 0.00002046
Iteration 107/1000 | Loss: 0.00002046
Iteration 108/1000 | Loss: 0.00002046
Iteration 109/1000 | Loss: 0.00002046
Iteration 110/1000 | Loss: 0.00002046
Iteration 111/1000 | Loss: 0.00002046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.0457942810025997e-05, 2.0457942810025997e-05, 2.0457942810025997e-05, 2.0457942810025997e-05, 2.0457942810025997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0457942810025997e-05

Optimization complete. Final v2v error: 3.6191656589508057 mm

Highest mean error: 6.082000255584717 mm for frame 41

Lowest mean error: 2.7262766361236572 mm for frame 147

Saving results

Total time: 54.51076102256775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00523725
Iteration 2/25 | Loss: 0.00113746
Iteration 3/25 | Loss: 0.00100187
Iteration 4/25 | Loss: 0.00099172
Iteration 5/25 | Loss: 0.00099044
Iteration 6/25 | Loss: 0.00099044
Iteration 7/25 | Loss: 0.00099044
Iteration 8/25 | Loss: 0.00099044
Iteration 9/25 | Loss: 0.00099044
Iteration 10/25 | Loss: 0.00099044
Iteration 11/25 | Loss: 0.00099044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009904353646561503, 0.0009904353646561503, 0.0009904353646561503, 0.0009904353646561503, 0.0009904353646561503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009904353646561503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79979706
Iteration 2/25 | Loss: 0.00034897
Iteration 3/25 | Loss: 0.00034897
Iteration 4/25 | Loss: 0.00034897
Iteration 5/25 | Loss: 0.00034897
Iteration 6/25 | Loss: 0.00034897
Iteration 7/25 | Loss: 0.00034897
Iteration 8/25 | Loss: 0.00034897
Iteration 9/25 | Loss: 0.00034897
Iteration 10/25 | Loss: 0.00034896
Iteration 11/25 | Loss: 0.00034896
Iteration 12/25 | Loss: 0.00034896
Iteration 13/25 | Loss: 0.00034896
Iteration 14/25 | Loss: 0.00034896
Iteration 15/25 | Loss: 0.00034896
Iteration 16/25 | Loss: 0.00034896
Iteration 17/25 | Loss: 0.00034896
Iteration 18/25 | Loss: 0.00034896
Iteration 19/25 | Loss: 0.00034896
Iteration 20/25 | Loss: 0.00034896
Iteration 21/25 | Loss: 0.00034896
Iteration 22/25 | Loss: 0.00034896
Iteration 23/25 | Loss: 0.00034896
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0003489646769594401, 0.0003489646769594401, 0.0003489646769594401, 0.0003489646769594401, 0.0003489646769594401]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003489646769594401

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034896
Iteration 2/1000 | Loss: 0.00002504
Iteration 3/1000 | Loss: 0.00001758
Iteration 4/1000 | Loss: 0.00001574
Iteration 5/1000 | Loss: 0.00001502
Iteration 6/1000 | Loss: 0.00001465
Iteration 7/1000 | Loss: 0.00001441
Iteration 8/1000 | Loss: 0.00001426
Iteration 9/1000 | Loss: 0.00001426
Iteration 10/1000 | Loss: 0.00001426
Iteration 11/1000 | Loss: 0.00001426
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001414
Iteration 14/1000 | Loss: 0.00001409
Iteration 15/1000 | Loss: 0.00001409
Iteration 16/1000 | Loss: 0.00001408
Iteration 17/1000 | Loss: 0.00001408
Iteration 18/1000 | Loss: 0.00001406
Iteration 19/1000 | Loss: 0.00001405
Iteration 20/1000 | Loss: 0.00001405
Iteration 21/1000 | Loss: 0.00001404
Iteration 22/1000 | Loss: 0.00001404
Iteration 23/1000 | Loss: 0.00001404
Iteration 24/1000 | Loss: 0.00001404
Iteration 25/1000 | Loss: 0.00001404
Iteration 26/1000 | Loss: 0.00001404
Iteration 27/1000 | Loss: 0.00001404
Iteration 28/1000 | Loss: 0.00001404
Iteration 29/1000 | Loss: 0.00001403
Iteration 30/1000 | Loss: 0.00001403
Iteration 31/1000 | Loss: 0.00001403
Iteration 32/1000 | Loss: 0.00001403
Iteration 33/1000 | Loss: 0.00001403
Iteration 34/1000 | Loss: 0.00001403
Iteration 35/1000 | Loss: 0.00001403
Iteration 36/1000 | Loss: 0.00001402
Iteration 37/1000 | Loss: 0.00001402
Iteration 38/1000 | Loss: 0.00001401
Iteration 39/1000 | Loss: 0.00001401
Iteration 40/1000 | Loss: 0.00001400
Iteration 41/1000 | Loss: 0.00001400
Iteration 42/1000 | Loss: 0.00001400
Iteration 43/1000 | Loss: 0.00001399
Iteration 44/1000 | Loss: 0.00001399
Iteration 45/1000 | Loss: 0.00001399
Iteration 46/1000 | Loss: 0.00001399
Iteration 47/1000 | Loss: 0.00001399
Iteration 48/1000 | Loss: 0.00001399
Iteration 49/1000 | Loss: 0.00001399
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001397
Iteration 52/1000 | Loss: 0.00001397
Iteration 53/1000 | Loss: 0.00001396
Iteration 54/1000 | Loss: 0.00001396
Iteration 55/1000 | Loss: 0.00001396
Iteration 56/1000 | Loss: 0.00001396
Iteration 57/1000 | Loss: 0.00001395
Iteration 58/1000 | Loss: 0.00001393
Iteration 59/1000 | Loss: 0.00001389
Iteration 60/1000 | Loss: 0.00001389
Iteration 61/1000 | Loss: 0.00001388
Iteration 62/1000 | Loss: 0.00001388
Iteration 63/1000 | Loss: 0.00001388
Iteration 64/1000 | Loss: 0.00001388
Iteration 65/1000 | Loss: 0.00001388
Iteration 66/1000 | Loss: 0.00001388
Iteration 67/1000 | Loss: 0.00001388
Iteration 68/1000 | Loss: 0.00001387
Iteration 69/1000 | Loss: 0.00001387
Iteration 70/1000 | Loss: 0.00001387
Iteration 71/1000 | Loss: 0.00001387
Iteration 72/1000 | Loss: 0.00001387
Iteration 73/1000 | Loss: 0.00001387
Iteration 74/1000 | Loss: 0.00001387
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001387
Iteration 77/1000 | Loss: 0.00001387
Iteration 78/1000 | Loss: 0.00001386
Iteration 79/1000 | Loss: 0.00001386
Iteration 80/1000 | Loss: 0.00001386
Iteration 81/1000 | Loss: 0.00001386
Iteration 82/1000 | Loss: 0.00001386
Iteration 83/1000 | Loss: 0.00001386
Iteration 84/1000 | Loss: 0.00001385
Iteration 85/1000 | Loss: 0.00001385
Iteration 86/1000 | Loss: 0.00001385
Iteration 87/1000 | Loss: 0.00001385
Iteration 88/1000 | Loss: 0.00001385
Iteration 89/1000 | Loss: 0.00001384
Iteration 90/1000 | Loss: 0.00001384
Iteration 91/1000 | Loss: 0.00001384
Iteration 92/1000 | Loss: 0.00001384
Iteration 93/1000 | Loss: 0.00001384
Iteration 94/1000 | Loss: 0.00001384
Iteration 95/1000 | Loss: 0.00001384
Iteration 96/1000 | Loss: 0.00001384
Iteration 97/1000 | Loss: 0.00001383
Iteration 98/1000 | Loss: 0.00001382
Iteration 99/1000 | Loss: 0.00001382
Iteration 100/1000 | Loss: 0.00001382
Iteration 101/1000 | Loss: 0.00001382
Iteration 102/1000 | Loss: 0.00001382
Iteration 103/1000 | Loss: 0.00001382
Iteration 104/1000 | Loss: 0.00001381
Iteration 105/1000 | Loss: 0.00001381
Iteration 106/1000 | Loss: 0.00001381
Iteration 107/1000 | Loss: 0.00001381
Iteration 108/1000 | Loss: 0.00001381
Iteration 109/1000 | Loss: 0.00001380
Iteration 110/1000 | Loss: 0.00001380
Iteration 111/1000 | Loss: 0.00001380
Iteration 112/1000 | Loss: 0.00001380
Iteration 113/1000 | Loss: 0.00001380
Iteration 114/1000 | Loss: 0.00001380
Iteration 115/1000 | Loss: 0.00001380
Iteration 116/1000 | Loss: 0.00001380
Iteration 117/1000 | Loss: 0.00001379
Iteration 118/1000 | Loss: 0.00001379
Iteration 119/1000 | Loss: 0.00001379
Iteration 120/1000 | Loss: 0.00001379
Iteration 121/1000 | Loss: 0.00001379
Iteration 122/1000 | Loss: 0.00001379
Iteration 123/1000 | Loss: 0.00001379
Iteration 124/1000 | Loss: 0.00001379
Iteration 125/1000 | Loss: 0.00001379
Iteration 126/1000 | Loss: 0.00001378
Iteration 127/1000 | Loss: 0.00001378
Iteration 128/1000 | Loss: 0.00001378
Iteration 129/1000 | Loss: 0.00001377
Iteration 130/1000 | Loss: 0.00001377
Iteration 131/1000 | Loss: 0.00001377
Iteration 132/1000 | Loss: 0.00001377
Iteration 133/1000 | Loss: 0.00001377
Iteration 134/1000 | Loss: 0.00001377
Iteration 135/1000 | Loss: 0.00001377
Iteration 136/1000 | Loss: 0.00001377
Iteration 137/1000 | Loss: 0.00001377
Iteration 138/1000 | Loss: 0.00001377
Iteration 139/1000 | Loss: 0.00001377
Iteration 140/1000 | Loss: 0.00001377
Iteration 141/1000 | Loss: 0.00001377
Iteration 142/1000 | Loss: 0.00001377
Iteration 143/1000 | Loss: 0.00001377
Iteration 144/1000 | Loss: 0.00001377
Iteration 145/1000 | Loss: 0.00001377
Iteration 146/1000 | Loss: 0.00001377
Iteration 147/1000 | Loss: 0.00001377
Iteration 148/1000 | Loss: 0.00001377
Iteration 149/1000 | Loss: 0.00001377
Iteration 150/1000 | Loss: 0.00001377
Iteration 151/1000 | Loss: 0.00001377
Iteration 152/1000 | Loss: 0.00001377
Iteration 153/1000 | Loss: 0.00001377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.3766091797151603e-05, 1.3766091797151603e-05, 1.3766091797151603e-05, 1.3766091797151603e-05, 1.3766091797151603e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3766091797151603e-05

Optimization complete. Final v2v error: 3.113818407058716 mm

Highest mean error: 3.2038800716400146 mm for frame 6

Lowest mean error: 2.999619245529175 mm for frame 168

Saving results

Total time: 29.739261150360107
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847160
Iteration 2/25 | Loss: 0.00154033
Iteration 3/25 | Loss: 0.00118297
Iteration 4/25 | Loss: 0.00104701
Iteration 5/25 | Loss: 0.00103566
Iteration 6/25 | Loss: 0.00103321
Iteration 7/25 | Loss: 0.00103279
Iteration 8/25 | Loss: 0.00103273
Iteration 9/25 | Loss: 0.00103273
Iteration 10/25 | Loss: 0.00103273
Iteration 11/25 | Loss: 0.00103273
Iteration 12/25 | Loss: 0.00103273
Iteration 13/25 | Loss: 0.00103273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001032725558616221, 0.001032725558616221, 0.001032725558616221, 0.001032725558616221, 0.001032725558616221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001032725558616221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.12810326
Iteration 2/25 | Loss: 0.00070953
Iteration 3/25 | Loss: 0.00070952
Iteration 4/25 | Loss: 0.00070952
Iteration 5/25 | Loss: 0.00070952
Iteration 6/25 | Loss: 0.00070952
Iteration 7/25 | Loss: 0.00070952
Iteration 8/25 | Loss: 0.00070952
Iteration 9/25 | Loss: 0.00070952
Iteration 10/25 | Loss: 0.00070952
Iteration 11/25 | Loss: 0.00070952
Iteration 12/25 | Loss: 0.00070952
Iteration 13/25 | Loss: 0.00070952
Iteration 14/25 | Loss: 0.00070952
Iteration 15/25 | Loss: 0.00070952
Iteration 16/25 | Loss: 0.00070952
Iteration 17/25 | Loss: 0.00070952
Iteration 18/25 | Loss: 0.00070952
Iteration 19/25 | Loss: 0.00070952
Iteration 20/25 | Loss: 0.00070952
Iteration 21/25 | Loss: 0.00070952
Iteration 22/25 | Loss: 0.00070952
Iteration 23/25 | Loss: 0.00070952
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007095205364748836, 0.0007095205364748836, 0.0007095205364748836, 0.0007095205364748836, 0.0007095205364748836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007095205364748836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070952
Iteration 2/1000 | Loss: 0.00002797
Iteration 3/1000 | Loss: 0.00002078
Iteration 4/1000 | Loss: 0.00001853
Iteration 5/1000 | Loss: 0.00001792
Iteration 6/1000 | Loss: 0.00001753
Iteration 7/1000 | Loss: 0.00001736
Iteration 8/1000 | Loss: 0.00001723
Iteration 9/1000 | Loss: 0.00001722
Iteration 10/1000 | Loss: 0.00001703
Iteration 11/1000 | Loss: 0.00001684
Iteration 12/1000 | Loss: 0.00001680
Iteration 13/1000 | Loss: 0.00001679
Iteration 14/1000 | Loss: 0.00001677
Iteration 15/1000 | Loss: 0.00001675
Iteration 16/1000 | Loss: 0.00001666
Iteration 17/1000 | Loss: 0.00001665
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001662
Iteration 20/1000 | Loss: 0.00001662
Iteration 21/1000 | Loss: 0.00001662
Iteration 22/1000 | Loss: 0.00001661
Iteration 23/1000 | Loss: 0.00001661
Iteration 24/1000 | Loss: 0.00001661
Iteration 25/1000 | Loss: 0.00001660
Iteration 26/1000 | Loss: 0.00001660
Iteration 27/1000 | Loss: 0.00001660
Iteration 28/1000 | Loss: 0.00001660
Iteration 29/1000 | Loss: 0.00001658
Iteration 30/1000 | Loss: 0.00001658
Iteration 31/1000 | Loss: 0.00001658
Iteration 32/1000 | Loss: 0.00001658
Iteration 33/1000 | Loss: 0.00001658
Iteration 34/1000 | Loss: 0.00001658
Iteration 35/1000 | Loss: 0.00001658
Iteration 36/1000 | Loss: 0.00001658
Iteration 37/1000 | Loss: 0.00001657
Iteration 38/1000 | Loss: 0.00001657
Iteration 39/1000 | Loss: 0.00001657
Iteration 40/1000 | Loss: 0.00001657
Iteration 41/1000 | Loss: 0.00001657
Iteration 42/1000 | Loss: 0.00001657
Iteration 43/1000 | Loss: 0.00001657
Iteration 44/1000 | Loss: 0.00001657
Iteration 45/1000 | Loss: 0.00001657
Iteration 46/1000 | Loss: 0.00001657
Iteration 47/1000 | Loss: 0.00001657
Iteration 48/1000 | Loss: 0.00001657
Iteration 49/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 49. Stopping optimization.
Last 5 losses: [1.6572950698900968e-05, 1.6572950698900968e-05, 1.6572950698900968e-05, 1.6572950698900968e-05, 1.6572950698900968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6572950698900968e-05

Optimization complete. Final v2v error: 3.422220230102539 mm

Highest mean error: 3.8572120666503906 mm for frame 220

Lowest mean error: 2.9175498485565186 mm for frame 177

Saving results

Total time: 33.06814193725586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386600
Iteration 2/25 | Loss: 0.00102730
Iteration 3/25 | Loss: 0.00091201
Iteration 4/25 | Loss: 0.00090546
Iteration 5/25 | Loss: 0.00090367
Iteration 6/25 | Loss: 0.00090367
Iteration 7/25 | Loss: 0.00090367
Iteration 8/25 | Loss: 0.00090367
Iteration 9/25 | Loss: 0.00090367
Iteration 10/25 | Loss: 0.00090367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009036662522703409, 0.0009036662522703409, 0.0009036662522703409, 0.0009036662522703409, 0.0009036662522703409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009036662522703409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32160151
Iteration 2/25 | Loss: 0.00051638
Iteration 3/25 | Loss: 0.00051638
Iteration 4/25 | Loss: 0.00051638
Iteration 5/25 | Loss: 0.00051638
Iteration 6/25 | Loss: 0.00051638
Iteration 7/25 | Loss: 0.00051638
Iteration 8/25 | Loss: 0.00051638
Iteration 9/25 | Loss: 0.00051638
Iteration 10/25 | Loss: 0.00051638
Iteration 11/25 | Loss: 0.00051638
Iteration 12/25 | Loss: 0.00051638
Iteration 13/25 | Loss: 0.00051638
Iteration 14/25 | Loss: 0.00051638
Iteration 15/25 | Loss: 0.00051638
Iteration 16/25 | Loss: 0.00051638
Iteration 17/25 | Loss: 0.00051638
Iteration 18/25 | Loss: 0.00051638
Iteration 19/25 | Loss: 0.00051638
Iteration 20/25 | Loss: 0.00051638
Iteration 21/25 | Loss: 0.00051638
Iteration 22/25 | Loss: 0.00051638
Iteration 23/25 | Loss: 0.00051638
Iteration 24/25 | Loss: 0.00051638
Iteration 25/25 | Loss: 0.00051638

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051638
Iteration 2/1000 | Loss: 0.00001497
Iteration 3/1000 | Loss: 0.00001094
Iteration 4/1000 | Loss: 0.00000998
Iteration 5/1000 | Loss: 0.00000935
Iteration 6/1000 | Loss: 0.00000893
Iteration 7/1000 | Loss: 0.00000868
Iteration 8/1000 | Loss: 0.00000858
Iteration 9/1000 | Loss: 0.00000855
Iteration 10/1000 | Loss: 0.00000853
Iteration 11/1000 | Loss: 0.00000853
Iteration 12/1000 | Loss: 0.00000852
Iteration 13/1000 | Loss: 0.00000851
Iteration 14/1000 | Loss: 0.00000850
Iteration 15/1000 | Loss: 0.00000848
Iteration 16/1000 | Loss: 0.00000847
Iteration 17/1000 | Loss: 0.00000846
Iteration 18/1000 | Loss: 0.00000846
Iteration 19/1000 | Loss: 0.00000845
Iteration 20/1000 | Loss: 0.00000845
Iteration 21/1000 | Loss: 0.00000842
Iteration 22/1000 | Loss: 0.00000842
Iteration 23/1000 | Loss: 0.00000842
Iteration 24/1000 | Loss: 0.00000842
Iteration 25/1000 | Loss: 0.00000841
Iteration 26/1000 | Loss: 0.00000841
Iteration 27/1000 | Loss: 0.00000841
Iteration 28/1000 | Loss: 0.00000841
Iteration 29/1000 | Loss: 0.00000841
Iteration 30/1000 | Loss: 0.00000841
Iteration 31/1000 | Loss: 0.00000841
Iteration 32/1000 | Loss: 0.00000841
Iteration 33/1000 | Loss: 0.00000841
Iteration 34/1000 | Loss: 0.00000841
Iteration 35/1000 | Loss: 0.00000841
Iteration 36/1000 | Loss: 0.00000840
Iteration 37/1000 | Loss: 0.00000840
Iteration 38/1000 | Loss: 0.00000840
Iteration 39/1000 | Loss: 0.00000840
Iteration 40/1000 | Loss: 0.00000839
Iteration 41/1000 | Loss: 0.00000839
Iteration 42/1000 | Loss: 0.00000839
Iteration 43/1000 | Loss: 0.00000839
Iteration 44/1000 | Loss: 0.00000838
Iteration 45/1000 | Loss: 0.00000838
Iteration 46/1000 | Loss: 0.00000838
Iteration 47/1000 | Loss: 0.00000837
Iteration 48/1000 | Loss: 0.00000837
Iteration 49/1000 | Loss: 0.00000837
Iteration 50/1000 | Loss: 0.00000837
Iteration 51/1000 | Loss: 0.00000837
Iteration 52/1000 | Loss: 0.00000837
Iteration 53/1000 | Loss: 0.00000837
Iteration 54/1000 | Loss: 0.00000837
Iteration 55/1000 | Loss: 0.00000836
Iteration 56/1000 | Loss: 0.00000835
Iteration 57/1000 | Loss: 0.00000835
Iteration 58/1000 | Loss: 0.00000835
Iteration 59/1000 | Loss: 0.00000834
Iteration 60/1000 | Loss: 0.00000834
Iteration 61/1000 | Loss: 0.00000834
Iteration 62/1000 | Loss: 0.00000834
Iteration 63/1000 | Loss: 0.00000833
Iteration 64/1000 | Loss: 0.00000833
Iteration 65/1000 | Loss: 0.00000833
Iteration 66/1000 | Loss: 0.00000833
Iteration 67/1000 | Loss: 0.00000832
Iteration 68/1000 | Loss: 0.00000832
Iteration 69/1000 | Loss: 0.00000832
Iteration 70/1000 | Loss: 0.00000832
Iteration 71/1000 | Loss: 0.00000832
Iteration 72/1000 | Loss: 0.00000832
Iteration 73/1000 | Loss: 0.00000832
Iteration 74/1000 | Loss: 0.00000832
Iteration 75/1000 | Loss: 0.00000832
Iteration 76/1000 | Loss: 0.00000832
Iteration 77/1000 | Loss: 0.00000832
Iteration 78/1000 | Loss: 0.00000832
Iteration 79/1000 | Loss: 0.00000832
Iteration 80/1000 | Loss: 0.00000832
Iteration 81/1000 | Loss: 0.00000832
Iteration 82/1000 | Loss: 0.00000832
Iteration 83/1000 | Loss: 0.00000832
Iteration 84/1000 | Loss: 0.00000832
Iteration 85/1000 | Loss: 0.00000832
Iteration 86/1000 | Loss: 0.00000831
Iteration 87/1000 | Loss: 0.00000831
Iteration 88/1000 | Loss: 0.00000831
Iteration 89/1000 | Loss: 0.00000831
Iteration 90/1000 | Loss: 0.00000831
Iteration 91/1000 | Loss: 0.00000831
Iteration 92/1000 | Loss: 0.00000831
Iteration 93/1000 | Loss: 0.00000831
Iteration 94/1000 | Loss: 0.00000831
Iteration 95/1000 | Loss: 0.00000831
Iteration 96/1000 | Loss: 0.00000831
Iteration 97/1000 | Loss: 0.00000831
Iteration 98/1000 | Loss: 0.00000831
Iteration 99/1000 | Loss: 0.00000831
Iteration 100/1000 | Loss: 0.00000831
Iteration 101/1000 | Loss: 0.00000831
Iteration 102/1000 | Loss: 0.00000831
Iteration 103/1000 | Loss: 0.00000831
Iteration 104/1000 | Loss: 0.00000831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [8.314214028359856e-06, 8.314214028359856e-06, 8.314214028359856e-06, 8.314214028359856e-06, 8.314214028359856e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.314214028359856e-06

Optimization complete. Final v2v error: 2.467921018600464 mm

Highest mean error: 2.5734455585479736 mm for frame 140

Lowest mean error: 2.3739640712738037 mm for frame 253

Saving results

Total time: 27.033551692962646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847405
Iteration 2/25 | Loss: 0.00133893
Iteration 3/25 | Loss: 0.00108616
Iteration 4/25 | Loss: 0.00104278
Iteration 5/25 | Loss: 0.00103290
Iteration 6/25 | Loss: 0.00102991
Iteration 7/25 | Loss: 0.00102944
Iteration 8/25 | Loss: 0.00102944
Iteration 9/25 | Loss: 0.00102944
Iteration 10/25 | Loss: 0.00102944
Iteration 11/25 | Loss: 0.00102944
Iteration 12/25 | Loss: 0.00102944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001029437524266541, 0.001029437524266541, 0.001029437524266541, 0.001029437524266541, 0.001029437524266541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001029437524266541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13782728
Iteration 2/25 | Loss: 0.00052949
Iteration 3/25 | Loss: 0.00052949
Iteration 4/25 | Loss: 0.00052949
Iteration 5/25 | Loss: 0.00052948
Iteration 6/25 | Loss: 0.00052948
Iteration 7/25 | Loss: 0.00052948
Iteration 8/25 | Loss: 0.00052948
Iteration 9/25 | Loss: 0.00052948
Iteration 10/25 | Loss: 0.00052948
Iteration 11/25 | Loss: 0.00052948
Iteration 12/25 | Loss: 0.00052948
Iteration 13/25 | Loss: 0.00052948
Iteration 14/25 | Loss: 0.00052948
Iteration 15/25 | Loss: 0.00052948
Iteration 16/25 | Loss: 0.00052948
Iteration 17/25 | Loss: 0.00052948
Iteration 18/25 | Loss: 0.00052948
Iteration 19/25 | Loss: 0.00052948
Iteration 20/25 | Loss: 0.00052948
Iteration 21/25 | Loss: 0.00052948
Iteration 22/25 | Loss: 0.00052948
Iteration 23/25 | Loss: 0.00052948
Iteration 24/25 | Loss: 0.00052948
Iteration 25/25 | Loss: 0.00052948

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052948
Iteration 2/1000 | Loss: 0.00003688
Iteration 3/1000 | Loss: 0.00002647
Iteration 4/1000 | Loss: 0.00002332
Iteration 5/1000 | Loss: 0.00002205
Iteration 6/1000 | Loss: 0.00002109
Iteration 7/1000 | Loss: 0.00002055
Iteration 8/1000 | Loss: 0.00002008
Iteration 9/1000 | Loss: 0.00001967
Iteration 10/1000 | Loss: 0.00001937
Iteration 11/1000 | Loss: 0.00001918
Iteration 12/1000 | Loss: 0.00001902
Iteration 13/1000 | Loss: 0.00001897
Iteration 14/1000 | Loss: 0.00001893
Iteration 15/1000 | Loss: 0.00001891
Iteration 16/1000 | Loss: 0.00001890
Iteration 17/1000 | Loss: 0.00001889
Iteration 18/1000 | Loss: 0.00001888
Iteration 19/1000 | Loss: 0.00001888
Iteration 20/1000 | Loss: 0.00001886
Iteration 21/1000 | Loss: 0.00001881
Iteration 22/1000 | Loss: 0.00001879
Iteration 23/1000 | Loss: 0.00001876
Iteration 24/1000 | Loss: 0.00001872
Iteration 25/1000 | Loss: 0.00001871
Iteration 26/1000 | Loss: 0.00001871
Iteration 27/1000 | Loss: 0.00001869
Iteration 28/1000 | Loss: 0.00001869
Iteration 29/1000 | Loss: 0.00001868
Iteration 30/1000 | Loss: 0.00001868
Iteration 31/1000 | Loss: 0.00001867
Iteration 32/1000 | Loss: 0.00001867
Iteration 33/1000 | Loss: 0.00001867
Iteration 34/1000 | Loss: 0.00001866
Iteration 35/1000 | Loss: 0.00001866
Iteration 36/1000 | Loss: 0.00001866
Iteration 37/1000 | Loss: 0.00001866
Iteration 38/1000 | Loss: 0.00001866
Iteration 39/1000 | Loss: 0.00001865
Iteration 40/1000 | Loss: 0.00001865
Iteration 41/1000 | Loss: 0.00001865
Iteration 42/1000 | Loss: 0.00001865
Iteration 43/1000 | Loss: 0.00001865
Iteration 44/1000 | Loss: 0.00001865
Iteration 45/1000 | Loss: 0.00001865
Iteration 46/1000 | Loss: 0.00001865
Iteration 47/1000 | Loss: 0.00001865
Iteration 48/1000 | Loss: 0.00001865
Iteration 49/1000 | Loss: 0.00001865
Iteration 50/1000 | Loss: 0.00001864
Iteration 51/1000 | Loss: 0.00001864
Iteration 52/1000 | Loss: 0.00001864
Iteration 53/1000 | Loss: 0.00001863
Iteration 54/1000 | Loss: 0.00001863
Iteration 55/1000 | Loss: 0.00001863
Iteration 56/1000 | Loss: 0.00001862
Iteration 57/1000 | Loss: 0.00001862
Iteration 58/1000 | Loss: 0.00001862
Iteration 59/1000 | Loss: 0.00001861
Iteration 60/1000 | Loss: 0.00001861
Iteration 61/1000 | Loss: 0.00001861
Iteration 62/1000 | Loss: 0.00001861
Iteration 63/1000 | Loss: 0.00001860
Iteration 64/1000 | Loss: 0.00001860
Iteration 65/1000 | Loss: 0.00001860
Iteration 66/1000 | Loss: 0.00001860
Iteration 67/1000 | Loss: 0.00001859
Iteration 68/1000 | Loss: 0.00001859
Iteration 69/1000 | Loss: 0.00001859
Iteration 70/1000 | Loss: 0.00001859
Iteration 71/1000 | Loss: 0.00001859
Iteration 72/1000 | Loss: 0.00001859
Iteration 73/1000 | Loss: 0.00001858
Iteration 74/1000 | Loss: 0.00001858
Iteration 75/1000 | Loss: 0.00001858
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001857
Iteration 79/1000 | Loss: 0.00001857
Iteration 80/1000 | Loss: 0.00001857
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001857
Iteration 83/1000 | Loss: 0.00001857
Iteration 84/1000 | Loss: 0.00001857
Iteration 85/1000 | Loss: 0.00001857
Iteration 86/1000 | Loss: 0.00001857
Iteration 87/1000 | Loss: 0.00001857
Iteration 88/1000 | Loss: 0.00001857
Iteration 89/1000 | Loss: 0.00001856
Iteration 90/1000 | Loss: 0.00001856
Iteration 91/1000 | Loss: 0.00001856
Iteration 92/1000 | Loss: 0.00001856
Iteration 93/1000 | Loss: 0.00001856
Iteration 94/1000 | Loss: 0.00001856
Iteration 95/1000 | Loss: 0.00001856
Iteration 96/1000 | Loss: 0.00001856
Iteration 97/1000 | Loss: 0.00001856
Iteration 98/1000 | Loss: 0.00001856
Iteration 99/1000 | Loss: 0.00001856
Iteration 100/1000 | Loss: 0.00001856
Iteration 101/1000 | Loss: 0.00001856
Iteration 102/1000 | Loss: 0.00001856
Iteration 103/1000 | Loss: 0.00001855
Iteration 104/1000 | Loss: 0.00001855
Iteration 105/1000 | Loss: 0.00001855
Iteration 106/1000 | Loss: 0.00001855
Iteration 107/1000 | Loss: 0.00001855
Iteration 108/1000 | Loss: 0.00001855
Iteration 109/1000 | Loss: 0.00001855
Iteration 110/1000 | Loss: 0.00001855
Iteration 111/1000 | Loss: 0.00001855
Iteration 112/1000 | Loss: 0.00001855
Iteration 113/1000 | Loss: 0.00001855
Iteration 114/1000 | Loss: 0.00001855
Iteration 115/1000 | Loss: 0.00001855
Iteration 116/1000 | Loss: 0.00001855
Iteration 117/1000 | Loss: 0.00001855
Iteration 118/1000 | Loss: 0.00001855
Iteration 119/1000 | Loss: 0.00001855
Iteration 120/1000 | Loss: 0.00001855
Iteration 121/1000 | Loss: 0.00001855
Iteration 122/1000 | Loss: 0.00001855
Iteration 123/1000 | Loss: 0.00001855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.8549037122284062e-05, 1.8549037122284062e-05, 1.8549037122284062e-05, 1.8549037122284062e-05, 1.8549037122284062e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8549037122284062e-05

Optimization complete. Final v2v error: 3.5629708766937256 mm

Highest mean error: 4.720301628112793 mm for frame 114

Lowest mean error: 2.546815872192383 mm for frame 218

Saving results

Total time: 42.23232412338257
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063839
Iteration 2/25 | Loss: 0.01063839
Iteration 3/25 | Loss: 0.00249894
Iteration 4/25 | Loss: 0.00152822
Iteration 5/25 | Loss: 0.00133239
Iteration 6/25 | Loss: 0.00122220
Iteration 7/25 | Loss: 0.00119921
Iteration 8/25 | Loss: 0.00115487
Iteration 9/25 | Loss: 0.00113470
Iteration 10/25 | Loss: 0.00111403
Iteration 11/25 | Loss: 0.00110193
Iteration 12/25 | Loss: 0.00109754
Iteration 13/25 | Loss: 0.00108935
Iteration 14/25 | Loss: 0.00108103
Iteration 15/25 | Loss: 0.00108471
Iteration 16/25 | Loss: 0.00107787
Iteration 17/25 | Loss: 0.00106810
Iteration 18/25 | Loss: 0.00107082
Iteration 19/25 | Loss: 0.00106904
Iteration 20/25 | Loss: 0.00106776
Iteration 21/25 | Loss: 0.00106913
Iteration 22/25 | Loss: 0.00106648
Iteration 23/25 | Loss: 0.00106626
Iteration 24/25 | Loss: 0.00107100
Iteration 25/25 | Loss: 0.00106258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32709801
Iteration 2/25 | Loss: 0.00100635
Iteration 3/25 | Loss: 0.00100635
Iteration 4/25 | Loss: 0.00088129
Iteration 5/25 | Loss: 0.00088126
Iteration 6/25 | Loss: 0.00088125
Iteration 7/25 | Loss: 0.00088125
Iteration 8/25 | Loss: 0.00088125
Iteration 9/25 | Loss: 0.00088125
Iteration 10/25 | Loss: 0.00088125
Iteration 11/25 | Loss: 0.00088125
Iteration 12/25 | Loss: 0.00088125
Iteration 13/25 | Loss: 0.00088125
Iteration 14/25 | Loss: 0.00088125
Iteration 15/25 | Loss: 0.00088125
Iteration 16/25 | Loss: 0.00088125
Iteration 17/25 | Loss: 0.00088125
Iteration 18/25 | Loss: 0.00088125
Iteration 19/25 | Loss: 0.00088125
Iteration 20/25 | Loss: 0.00088125
Iteration 21/25 | Loss: 0.00088125
Iteration 22/25 | Loss: 0.00088125
Iteration 23/25 | Loss: 0.00088125
Iteration 24/25 | Loss: 0.00088125
Iteration 25/25 | Loss: 0.00088125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088125
Iteration 2/1000 | Loss: 0.00022829
Iteration 3/1000 | Loss: 0.00005300
Iteration 4/1000 | Loss: 0.00022924
Iteration 5/1000 | Loss: 0.00017014
Iteration 6/1000 | Loss: 0.00023924
Iteration 7/1000 | Loss: 0.00034871
Iteration 8/1000 | Loss: 0.00023463
Iteration 9/1000 | Loss: 0.00022916
Iteration 10/1000 | Loss: 0.00030427
Iteration 11/1000 | Loss: 0.00023480
Iteration 12/1000 | Loss: 0.00019237
Iteration 13/1000 | Loss: 0.00026623
Iteration 14/1000 | Loss: 0.00011906
Iteration 15/1000 | Loss: 0.00040276
Iteration 16/1000 | Loss: 0.00014275
Iteration 17/1000 | Loss: 0.00017256
Iteration 18/1000 | Loss: 0.00018357
Iteration 19/1000 | Loss: 0.00017070
Iteration 20/1000 | Loss: 0.00015161
Iteration 21/1000 | Loss: 0.00021710
Iteration 22/1000 | Loss: 0.00006036
Iteration 23/1000 | Loss: 0.00033731
Iteration 24/1000 | Loss: 0.00006407
Iteration 25/1000 | Loss: 0.00004792
Iteration 26/1000 | Loss: 0.00005534
Iteration 27/1000 | Loss: 0.00004487
Iteration 28/1000 | Loss: 0.00005364
Iteration 29/1000 | Loss: 0.00006977
Iteration 30/1000 | Loss: 0.00007479
Iteration 31/1000 | Loss: 0.00006452
Iteration 32/1000 | Loss: 0.00027014
Iteration 33/1000 | Loss: 0.00011662
Iteration 34/1000 | Loss: 0.00006942
Iteration 35/1000 | Loss: 0.00006364
Iteration 36/1000 | Loss: 0.00006971
Iteration 37/1000 | Loss: 0.00005981
Iteration 38/1000 | Loss: 0.00005210
Iteration 39/1000 | Loss: 0.00005985
Iteration 40/1000 | Loss: 0.00005851
Iteration 41/1000 | Loss: 0.00006742
Iteration 42/1000 | Loss: 0.00005060
Iteration 43/1000 | Loss: 0.00012447
Iteration 44/1000 | Loss: 0.00006277
Iteration 45/1000 | Loss: 0.00005617
Iteration 46/1000 | Loss: 0.00006840
Iteration 47/1000 | Loss: 0.00009821
Iteration 48/1000 | Loss: 0.00006050
Iteration 49/1000 | Loss: 0.00006172
Iteration 50/1000 | Loss: 0.00016409
Iteration 51/1000 | Loss: 0.00007234
Iteration 52/1000 | Loss: 0.00004216
Iteration 53/1000 | Loss: 0.00006094
Iteration 54/1000 | Loss: 0.00003273
Iteration 55/1000 | Loss: 0.00011045
Iteration 56/1000 | Loss: 0.00002904
Iteration 57/1000 | Loss: 0.00002778
Iteration 58/1000 | Loss: 0.00002672
Iteration 59/1000 | Loss: 0.00054493
Iteration 60/1000 | Loss: 0.00022708
Iteration 61/1000 | Loss: 0.00004284
Iteration 62/1000 | Loss: 0.00034863
Iteration 63/1000 | Loss: 0.00003358
Iteration 64/1000 | Loss: 0.00002612
Iteration 65/1000 | Loss: 0.00004635
Iteration 66/1000 | Loss: 0.00002434
Iteration 67/1000 | Loss: 0.00002350
Iteration 68/1000 | Loss: 0.00003805
Iteration 69/1000 | Loss: 0.00008695
Iteration 70/1000 | Loss: 0.00010195
Iteration 71/1000 | Loss: 0.00004547
Iteration 72/1000 | Loss: 0.00002544
Iteration 73/1000 | Loss: 0.00004796
Iteration 74/1000 | Loss: 0.00011139
Iteration 75/1000 | Loss: 0.00007431
Iteration 76/1000 | Loss: 0.00002557
Iteration 77/1000 | Loss: 0.00006685
Iteration 78/1000 | Loss: 0.00023128
Iteration 79/1000 | Loss: 0.00009044
Iteration 80/1000 | Loss: 0.00015270
Iteration 81/1000 | Loss: 0.00008834
Iteration 82/1000 | Loss: 0.00011630
Iteration 83/1000 | Loss: 0.00008670
Iteration 84/1000 | Loss: 0.00023934
Iteration 85/1000 | Loss: 0.00011821
Iteration 86/1000 | Loss: 0.00004430
Iteration 87/1000 | Loss: 0.00006192
Iteration 88/1000 | Loss: 0.00003181
Iteration 89/1000 | Loss: 0.00006330
Iteration 90/1000 | Loss: 0.00011637
Iteration 91/1000 | Loss: 0.00009165
Iteration 92/1000 | Loss: 0.00002726
Iteration 93/1000 | Loss: 0.00004974
Iteration 94/1000 | Loss: 0.00002798
Iteration 95/1000 | Loss: 0.00004973
Iteration 96/1000 | Loss: 0.00003607
Iteration 97/1000 | Loss: 0.00003947
Iteration 98/1000 | Loss: 0.00003080
Iteration 99/1000 | Loss: 0.00003553
Iteration 100/1000 | Loss: 0.00003051
Iteration 101/1000 | Loss: 0.00003429
Iteration 102/1000 | Loss: 0.00003073
Iteration 103/1000 | Loss: 0.00002273
Iteration 104/1000 | Loss: 0.00002258
Iteration 105/1000 | Loss: 0.00002257
Iteration 106/1000 | Loss: 0.00002257
Iteration 107/1000 | Loss: 0.00002256
Iteration 108/1000 | Loss: 0.00002256
Iteration 109/1000 | Loss: 0.00002256
Iteration 110/1000 | Loss: 0.00002256
Iteration 111/1000 | Loss: 0.00002256
Iteration 112/1000 | Loss: 0.00002256
Iteration 113/1000 | Loss: 0.00002256
Iteration 114/1000 | Loss: 0.00002256
Iteration 115/1000 | Loss: 0.00002256
Iteration 116/1000 | Loss: 0.00002255
Iteration 117/1000 | Loss: 0.00002255
Iteration 118/1000 | Loss: 0.00002255
Iteration 119/1000 | Loss: 0.00010465
Iteration 120/1000 | Loss: 0.00002937
Iteration 121/1000 | Loss: 0.00012338
Iteration 122/1000 | Loss: 0.00009521
Iteration 123/1000 | Loss: 0.00017360
Iteration 124/1000 | Loss: 0.00025361
Iteration 125/1000 | Loss: 0.00027207
Iteration 126/1000 | Loss: 0.00004622
Iteration 127/1000 | Loss: 0.00003136
Iteration 128/1000 | Loss: 0.00014967
Iteration 129/1000 | Loss: 0.00006825
Iteration 130/1000 | Loss: 0.00016606
Iteration 131/1000 | Loss: 0.00019418
Iteration 132/1000 | Loss: 0.00008640
Iteration 133/1000 | Loss: 0.00003196
Iteration 134/1000 | Loss: 0.00002720
Iteration 135/1000 | Loss: 0.00002644
Iteration 136/1000 | Loss: 0.00016596
Iteration 137/1000 | Loss: 0.00015146
Iteration 138/1000 | Loss: 0.00002890
Iteration 139/1000 | Loss: 0.00017238
Iteration 140/1000 | Loss: 0.00023740
Iteration 141/1000 | Loss: 0.00009158
Iteration 142/1000 | Loss: 0.00009927
Iteration 143/1000 | Loss: 0.00003287
Iteration 144/1000 | Loss: 0.00004878
Iteration 145/1000 | Loss: 0.00002804
Iteration 146/1000 | Loss: 0.00005361
Iteration 147/1000 | Loss: 0.00005442
Iteration 148/1000 | Loss: 0.00004021
Iteration 149/1000 | Loss: 0.00002620
Iteration 150/1000 | Loss: 0.00028216
Iteration 151/1000 | Loss: 0.00014769
Iteration 152/1000 | Loss: 0.00002641
Iteration 153/1000 | Loss: 0.00027300
Iteration 154/1000 | Loss: 0.00007900
Iteration 155/1000 | Loss: 0.00005297
Iteration 156/1000 | Loss: 0.00008924
Iteration 157/1000 | Loss: 0.00002445
Iteration 158/1000 | Loss: 0.00002354
Iteration 159/1000 | Loss: 0.00006592
Iteration 160/1000 | Loss: 0.00002296
Iteration 161/1000 | Loss: 0.00002284
Iteration 162/1000 | Loss: 0.00002284
Iteration 163/1000 | Loss: 0.00002283
Iteration 164/1000 | Loss: 0.00002282
Iteration 165/1000 | Loss: 0.00002282
Iteration 166/1000 | Loss: 0.00002281
Iteration 167/1000 | Loss: 0.00002270
Iteration 168/1000 | Loss: 0.00002268
Iteration 169/1000 | Loss: 0.00002268
Iteration 170/1000 | Loss: 0.00002268
Iteration 171/1000 | Loss: 0.00002267
Iteration 172/1000 | Loss: 0.00002267
Iteration 173/1000 | Loss: 0.00002267
Iteration 174/1000 | Loss: 0.00002267
Iteration 175/1000 | Loss: 0.00002267
Iteration 176/1000 | Loss: 0.00002267
Iteration 177/1000 | Loss: 0.00002267
Iteration 178/1000 | Loss: 0.00002266
Iteration 179/1000 | Loss: 0.00002265
Iteration 180/1000 | Loss: 0.00002263
Iteration 181/1000 | Loss: 0.00004189
Iteration 182/1000 | Loss: 0.00004189
Iteration 183/1000 | Loss: 0.00004189
Iteration 184/1000 | Loss: 0.00003829
Iteration 185/1000 | Loss: 0.00004139
Iteration 186/1000 | Loss: 0.00007031
Iteration 187/1000 | Loss: 0.00004399
Iteration 188/1000 | Loss: 0.00003675
Iteration 189/1000 | Loss: 0.00004198
Iteration 190/1000 | Loss: 0.00003513
Iteration 191/1000 | Loss: 0.00002613
Iteration 192/1000 | Loss: 0.00003023
Iteration 193/1000 | Loss: 0.00002495
Iteration 194/1000 | Loss: 0.00002383
Iteration 195/1000 | Loss: 0.00003853
Iteration 196/1000 | Loss: 0.00002934
Iteration 197/1000 | Loss: 0.00010153
Iteration 198/1000 | Loss: 0.00003945
Iteration 199/1000 | Loss: 0.00003605
Iteration 200/1000 | Loss: 0.00003853
Iteration 201/1000 | Loss: 0.00003089
Iteration 202/1000 | Loss: 0.00002760
Iteration 203/1000 | Loss: 0.00003948
Iteration 204/1000 | Loss: 0.00007241
Iteration 205/1000 | Loss: 0.00003078
Iteration 206/1000 | Loss: 0.00004078
Iteration 207/1000 | Loss: 0.00003986
Iteration 208/1000 | Loss: 0.00002840
Iteration 209/1000 | Loss: 0.00006703
Iteration 210/1000 | Loss: 0.00004136
Iteration 211/1000 | Loss: 0.00002547
Iteration 212/1000 | Loss: 0.00011705
Iteration 213/1000 | Loss: 0.00002376
Iteration 214/1000 | Loss: 0.00004302
Iteration 215/1000 | Loss: 0.00002292
Iteration 216/1000 | Loss: 0.00002265
Iteration 217/1000 | Loss: 0.00002256
Iteration 218/1000 | Loss: 0.00002245
Iteration 219/1000 | Loss: 0.00002244
Iteration 220/1000 | Loss: 0.00002244
Iteration 221/1000 | Loss: 0.00002243
Iteration 222/1000 | Loss: 0.00002243
Iteration 223/1000 | Loss: 0.00002243
Iteration 224/1000 | Loss: 0.00002243
Iteration 225/1000 | Loss: 0.00002243
Iteration 226/1000 | Loss: 0.00002243
Iteration 227/1000 | Loss: 0.00002243
Iteration 228/1000 | Loss: 0.00002243
Iteration 229/1000 | Loss: 0.00002243
Iteration 230/1000 | Loss: 0.00002236
Iteration 231/1000 | Loss: 0.00002234
Iteration 232/1000 | Loss: 0.00002234
Iteration 233/1000 | Loss: 0.00002234
Iteration 234/1000 | Loss: 0.00002234
Iteration 235/1000 | Loss: 0.00002234
Iteration 236/1000 | Loss: 0.00002233
Iteration 237/1000 | Loss: 0.00002231
Iteration 238/1000 | Loss: 0.00002231
Iteration 239/1000 | Loss: 0.00002224
Iteration 240/1000 | Loss: 0.00002222
Iteration 241/1000 | Loss: 0.00002216
Iteration 242/1000 | Loss: 0.00002216
Iteration 243/1000 | Loss: 0.00002216
Iteration 244/1000 | Loss: 0.00002216
Iteration 245/1000 | Loss: 0.00002216
Iteration 246/1000 | Loss: 0.00002216
Iteration 247/1000 | Loss: 0.00002216
Iteration 248/1000 | Loss: 0.00002216
Iteration 249/1000 | Loss: 0.00002216
Iteration 250/1000 | Loss: 0.00002216
Iteration 251/1000 | Loss: 0.00002215
Iteration 252/1000 | Loss: 0.00010331
Iteration 253/1000 | Loss: 0.00002217
Iteration 254/1000 | Loss: 0.00002209
Iteration 255/1000 | Loss: 0.00002209
Iteration 256/1000 | Loss: 0.00002209
Iteration 257/1000 | Loss: 0.00002209
Iteration 258/1000 | Loss: 0.00002209
Iteration 259/1000 | Loss: 0.00002209
Iteration 260/1000 | Loss: 0.00002209
Iteration 261/1000 | Loss: 0.00002208
Iteration 262/1000 | Loss: 0.00002208
Iteration 263/1000 | Loss: 0.00002208
Iteration 264/1000 | Loss: 0.00002208
Iteration 265/1000 | Loss: 0.00002208
Iteration 266/1000 | Loss: 0.00002208
Iteration 267/1000 | Loss: 0.00002206
Iteration 268/1000 | Loss: 0.00002206
Iteration 269/1000 | Loss: 0.00002206
Iteration 270/1000 | Loss: 0.00002206
Iteration 271/1000 | Loss: 0.00002206
Iteration 272/1000 | Loss: 0.00002206
Iteration 273/1000 | Loss: 0.00002206
Iteration 274/1000 | Loss: 0.00002206
Iteration 275/1000 | Loss: 0.00002206
Iteration 276/1000 | Loss: 0.00002205
Iteration 277/1000 | Loss: 0.00002205
Iteration 278/1000 | Loss: 0.00002205
Iteration 279/1000 | Loss: 0.00002205
Iteration 280/1000 | Loss: 0.00002205
Iteration 281/1000 | Loss: 0.00002205
Iteration 282/1000 | Loss: 0.00002205
Iteration 283/1000 | Loss: 0.00002205
Iteration 284/1000 | Loss: 0.00002203
Iteration 285/1000 | Loss: 0.00002203
Iteration 286/1000 | Loss: 0.00002203
Iteration 287/1000 | Loss: 0.00002203
Iteration 288/1000 | Loss: 0.00002203
Iteration 289/1000 | Loss: 0.00002202
Iteration 290/1000 | Loss: 0.00002202
Iteration 291/1000 | Loss: 0.00002202
Iteration 292/1000 | Loss: 0.00002202
Iteration 293/1000 | Loss: 0.00002202
Iteration 294/1000 | Loss: 0.00002202
Iteration 295/1000 | Loss: 0.00002201
Iteration 296/1000 | Loss: 0.00002201
Iteration 297/1000 | Loss: 0.00002201
Iteration 298/1000 | Loss: 0.00002200
Iteration 299/1000 | Loss: 0.00002199
Iteration 300/1000 | Loss: 0.00002199
Iteration 301/1000 | Loss: 0.00002199
Iteration 302/1000 | Loss: 0.00002199
Iteration 303/1000 | Loss: 0.00002198
Iteration 304/1000 | Loss: 0.00002198
Iteration 305/1000 | Loss: 0.00002198
Iteration 306/1000 | Loss: 0.00002198
Iteration 307/1000 | Loss: 0.00002198
Iteration 308/1000 | Loss: 0.00002198
Iteration 309/1000 | Loss: 0.00002198
Iteration 310/1000 | Loss: 0.00002198
Iteration 311/1000 | Loss: 0.00002198
Iteration 312/1000 | Loss: 0.00002198
Iteration 313/1000 | Loss: 0.00002198
Iteration 314/1000 | Loss: 0.00002198
Iteration 315/1000 | Loss: 0.00002198
Iteration 316/1000 | Loss: 0.00002198
Iteration 317/1000 | Loss: 0.00002198
Iteration 318/1000 | Loss: 0.00002198
Iteration 319/1000 | Loss: 0.00002198
Iteration 320/1000 | Loss: 0.00002198
Iteration 321/1000 | Loss: 0.00002198
Iteration 322/1000 | Loss: 0.00002198
Iteration 323/1000 | Loss: 0.00002198
Iteration 324/1000 | Loss: 0.00002198
Iteration 325/1000 | Loss: 0.00002198
Iteration 326/1000 | Loss: 0.00002198
Iteration 327/1000 | Loss: 0.00002198
Iteration 328/1000 | Loss: 0.00002198
Iteration 329/1000 | Loss: 0.00002198
Iteration 330/1000 | Loss: 0.00002198
Iteration 331/1000 | Loss: 0.00002198
Iteration 332/1000 | Loss: 0.00002198
Iteration 333/1000 | Loss: 0.00002198
Iteration 334/1000 | Loss: 0.00002198
Iteration 335/1000 | Loss: 0.00002198
Iteration 336/1000 | Loss: 0.00002198
Iteration 337/1000 | Loss: 0.00002198
Iteration 338/1000 | Loss: 0.00002198
Iteration 339/1000 | Loss: 0.00002198
Iteration 340/1000 | Loss: 0.00002198
Iteration 341/1000 | Loss: 0.00002198
Iteration 342/1000 | Loss: 0.00002198
Iteration 343/1000 | Loss: 0.00002198
Iteration 344/1000 | Loss: 0.00002198
Iteration 345/1000 | Loss: 0.00002198
Iteration 346/1000 | Loss: 0.00002198
Iteration 347/1000 | Loss: 0.00002198
Iteration 348/1000 | Loss: 0.00002198
Iteration 349/1000 | Loss: 0.00002198
Iteration 350/1000 | Loss: 0.00002198
Iteration 351/1000 | Loss: 0.00002198
Iteration 352/1000 | Loss: 0.00002198
Iteration 353/1000 | Loss: 0.00002198
Iteration 354/1000 | Loss: 0.00002198
Iteration 355/1000 | Loss: 0.00002198
Iteration 356/1000 | Loss: 0.00002198
Iteration 357/1000 | Loss: 0.00002198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 357. Stopping optimization.
Last 5 losses: [2.19780886254739e-05, 2.19780886254739e-05, 2.19780886254739e-05, 2.19780886254739e-05, 2.19780886254739e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.19780886254739e-05

Optimization complete. Final v2v error: 3.4329721927642822 mm

Highest mean error: 22.32558822631836 mm for frame 17

Lowest mean error: 2.9902658462524414 mm for frame 0

Saving results

Total time: 356.1481695175171
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980636
Iteration 2/25 | Loss: 0.00120119
Iteration 3/25 | Loss: 0.00098227
Iteration 4/25 | Loss: 0.00096201
Iteration 5/25 | Loss: 0.00095683
Iteration 6/25 | Loss: 0.00095552
Iteration 7/25 | Loss: 0.00095552
Iteration 8/25 | Loss: 0.00095552
Iteration 9/25 | Loss: 0.00095552
Iteration 10/25 | Loss: 0.00095552
Iteration 11/25 | Loss: 0.00095552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009555204305797815, 0.0009555204305797815, 0.0009555204305797815, 0.0009555204305797815, 0.0009555204305797815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009555204305797815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.43725705
Iteration 2/25 | Loss: 0.00058323
Iteration 3/25 | Loss: 0.00058323
Iteration 4/25 | Loss: 0.00058323
Iteration 5/25 | Loss: 0.00058323
Iteration 6/25 | Loss: 0.00058323
Iteration 7/25 | Loss: 0.00058323
Iteration 8/25 | Loss: 0.00058323
Iteration 9/25 | Loss: 0.00058323
Iteration 10/25 | Loss: 0.00058323
Iteration 11/25 | Loss: 0.00058323
Iteration 12/25 | Loss: 0.00058323
Iteration 13/25 | Loss: 0.00058323
Iteration 14/25 | Loss: 0.00058323
Iteration 15/25 | Loss: 0.00058323
Iteration 16/25 | Loss: 0.00058323
Iteration 17/25 | Loss: 0.00058323
Iteration 18/25 | Loss: 0.00058323
Iteration 19/25 | Loss: 0.00058323
Iteration 20/25 | Loss: 0.00058323
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005832308088429272, 0.0005832308088429272, 0.0005832308088429272, 0.0005832308088429272, 0.0005832308088429272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005832308088429272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058323
Iteration 2/1000 | Loss: 0.00002849
Iteration 3/1000 | Loss: 0.00001579
Iteration 4/1000 | Loss: 0.00001363
Iteration 5/1000 | Loss: 0.00001259
Iteration 6/1000 | Loss: 0.00001202
Iteration 7/1000 | Loss: 0.00001172
Iteration 8/1000 | Loss: 0.00001136
Iteration 9/1000 | Loss: 0.00001122
Iteration 10/1000 | Loss: 0.00001106
Iteration 11/1000 | Loss: 0.00001105
Iteration 12/1000 | Loss: 0.00001096
Iteration 13/1000 | Loss: 0.00001095
Iteration 14/1000 | Loss: 0.00001089
Iteration 15/1000 | Loss: 0.00001088
Iteration 16/1000 | Loss: 0.00001087
Iteration 17/1000 | Loss: 0.00001087
Iteration 18/1000 | Loss: 0.00001087
Iteration 19/1000 | Loss: 0.00001086
Iteration 20/1000 | Loss: 0.00001086
Iteration 21/1000 | Loss: 0.00001085
Iteration 22/1000 | Loss: 0.00001084
Iteration 23/1000 | Loss: 0.00001083
Iteration 24/1000 | Loss: 0.00001083
Iteration 25/1000 | Loss: 0.00001083
Iteration 26/1000 | Loss: 0.00001082
Iteration 27/1000 | Loss: 0.00001082
Iteration 28/1000 | Loss: 0.00001081
Iteration 29/1000 | Loss: 0.00001081
Iteration 30/1000 | Loss: 0.00001080
Iteration 31/1000 | Loss: 0.00001080
Iteration 32/1000 | Loss: 0.00001079
Iteration 33/1000 | Loss: 0.00001079
Iteration 34/1000 | Loss: 0.00001078
Iteration 35/1000 | Loss: 0.00001078
Iteration 36/1000 | Loss: 0.00001078
Iteration 37/1000 | Loss: 0.00001078
Iteration 38/1000 | Loss: 0.00001077
Iteration 39/1000 | Loss: 0.00001077
Iteration 40/1000 | Loss: 0.00001077
Iteration 41/1000 | Loss: 0.00001077
Iteration 42/1000 | Loss: 0.00001077
Iteration 43/1000 | Loss: 0.00001077
Iteration 44/1000 | Loss: 0.00001077
Iteration 45/1000 | Loss: 0.00001077
Iteration 46/1000 | Loss: 0.00001077
Iteration 47/1000 | Loss: 0.00001077
Iteration 48/1000 | Loss: 0.00001077
Iteration 49/1000 | Loss: 0.00001076
Iteration 50/1000 | Loss: 0.00001076
Iteration 51/1000 | Loss: 0.00001076
Iteration 52/1000 | Loss: 0.00001076
Iteration 53/1000 | Loss: 0.00001076
Iteration 54/1000 | Loss: 0.00001076
Iteration 55/1000 | Loss: 0.00001076
Iteration 56/1000 | Loss: 0.00001076
Iteration 57/1000 | Loss: 0.00001076
Iteration 58/1000 | Loss: 0.00001076
Iteration 59/1000 | Loss: 0.00001075
Iteration 60/1000 | Loss: 0.00001075
Iteration 61/1000 | Loss: 0.00001074
Iteration 62/1000 | Loss: 0.00001074
Iteration 63/1000 | Loss: 0.00001074
Iteration 64/1000 | Loss: 0.00001073
Iteration 65/1000 | Loss: 0.00001073
Iteration 66/1000 | Loss: 0.00001072
Iteration 67/1000 | Loss: 0.00001072
Iteration 68/1000 | Loss: 0.00001072
Iteration 69/1000 | Loss: 0.00001072
Iteration 70/1000 | Loss: 0.00001072
Iteration 71/1000 | Loss: 0.00001071
Iteration 72/1000 | Loss: 0.00001071
Iteration 73/1000 | Loss: 0.00001071
Iteration 74/1000 | Loss: 0.00001071
Iteration 75/1000 | Loss: 0.00001071
Iteration 76/1000 | Loss: 0.00001070
Iteration 77/1000 | Loss: 0.00001070
Iteration 78/1000 | Loss: 0.00001070
Iteration 79/1000 | Loss: 0.00001070
Iteration 80/1000 | Loss: 0.00001069
Iteration 81/1000 | Loss: 0.00001069
Iteration 82/1000 | Loss: 0.00001069
Iteration 83/1000 | Loss: 0.00001069
Iteration 84/1000 | Loss: 0.00001069
Iteration 85/1000 | Loss: 0.00001069
Iteration 86/1000 | Loss: 0.00001069
Iteration 87/1000 | Loss: 0.00001069
Iteration 88/1000 | Loss: 0.00001068
Iteration 89/1000 | Loss: 0.00001068
Iteration 90/1000 | Loss: 0.00001068
Iteration 91/1000 | Loss: 0.00001068
Iteration 92/1000 | Loss: 0.00001068
Iteration 93/1000 | Loss: 0.00001068
Iteration 94/1000 | Loss: 0.00001068
Iteration 95/1000 | Loss: 0.00001068
Iteration 96/1000 | Loss: 0.00001068
Iteration 97/1000 | Loss: 0.00001068
Iteration 98/1000 | Loss: 0.00001067
Iteration 99/1000 | Loss: 0.00001067
Iteration 100/1000 | Loss: 0.00001067
Iteration 101/1000 | Loss: 0.00001067
Iteration 102/1000 | Loss: 0.00001067
Iteration 103/1000 | Loss: 0.00001067
Iteration 104/1000 | Loss: 0.00001067
Iteration 105/1000 | Loss: 0.00001067
Iteration 106/1000 | Loss: 0.00001067
Iteration 107/1000 | Loss: 0.00001066
Iteration 108/1000 | Loss: 0.00001066
Iteration 109/1000 | Loss: 0.00001066
Iteration 110/1000 | Loss: 0.00001066
Iteration 111/1000 | Loss: 0.00001066
Iteration 112/1000 | Loss: 0.00001066
Iteration 113/1000 | Loss: 0.00001066
Iteration 114/1000 | Loss: 0.00001066
Iteration 115/1000 | Loss: 0.00001066
Iteration 116/1000 | Loss: 0.00001066
Iteration 117/1000 | Loss: 0.00001066
Iteration 118/1000 | Loss: 0.00001066
Iteration 119/1000 | Loss: 0.00001066
Iteration 120/1000 | Loss: 0.00001066
Iteration 121/1000 | Loss: 0.00001066
Iteration 122/1000 | Loss: 0.00001066
Iteration 123/1000 | Loss: 0.00001066
Iteration 124/1000 | Loss: 0.00001065
Iteration 125/1000 | Loss: 0.00001065
Iteration 126/1000 | Loss: 0.00001065
Iteration 127/1000 | Loss: 0.00001065
Iteration 128/1000 | Loss: 0.00001065
Iteration 129/1000 | Loss: 0.00001065
Iteration 130/1000 | Loss: 0.00001065
Iteration 131/1000 | Loss: 0.00001065
Iteration 132/1000 | Loss: 0.00001065
Iteration 133/1000 | Loss: 0.00001065
Iteration 134/1000 | Loss: 0.00001065
Iteration 135/1000 | Loss: 0.00001065
Iteration 136/1000 | Loss: 0.00001065
Iteration 137/1000 | Loss: 0.00001065
Iteration 138/1000 | Loss: 0.00001065
Iteration 139/1000 | Loss: 0.00001065
Iteration 140/1000 | Loss: 0.00001065
Iteration 141/1000 | Loss: 0.00001065
Iteration 142/1000 | Loss: 0.00001065
Iteration 143/1000 | Loss: 0.00001065
Iteration 144/1000 | Loss: 0.00001065
Iteration 145/1000 | Loss: 0.00001065
Iteration 146/1000 | Loss: 0.00001065
Iteration 147/1000 | Loss: 0.00001065
Iteration 148/1000 | Loss: 0.00001065
Iteration 149/1000 | Loss: 0.00001065
Iteration 150/1000 | Loss: 0.00001065
Iteration 151/1000 | Loss: 0.00001065
Iteration 152/1000 | Loss: 0.00001065
Iteration 153/1000 | Loss: 0.00001065
Iteration 154/1000 | Loss: 0.00001065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.0647247108863667e-05, 1.0647247108863667e-05, 1.0647247108863667e-05, 1.0647247108863667e-05, 1.0647247108863667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0647247108863667e-05

Optimization complete. Final v2v error: 2.7329447269439697 mm

Highest mean error: 3.435410737991333 mm for frame 4

Lowest mean error: 2.2438151836395264 mm for frame 174

Saving results

Total time: 37.164599895477295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01116113
Iteration 2/25 | Loss: 0.00198245
Iteration 3/25 | Loss: 0.00132352
Iteration 4/25 | Loss: 0.00152075
Iteration 5/25 | Loss: 0.00132635
Iteration 6/25 | Loss: 0.00128537
Iteration 7/25 | Loss: 0.00121918
Iteration 8/25 | Loss: 0.00124295
Iteration 9/25 | Loss: 0.00137169
Iteration 10/25 | Loss: 0.00123866
Iteration 11/25 | Loss: 0.00115911
Iteration 12/25 | Loss: 0.00106824
Iteration 13/25 | Loss: 0.00102859
Iteration 14/25 | Loss: 0.00103435
Iteration 15/25 | Loss: 0.00101141
Iteration 16/25 | Loss: 0.00101465
Iteration 17/25 | Loss: 0.00101067
Iteration 18/25 | Loss: 0.00099031
Iteration 19/25 | Loss: 0.00097711
Iteration 20/25 | Loss: 0.00096795
Iteration 21/25 | Loss: 0.00096234
Iteration 22/25 | Loss: 0.00097518
Iteration 23/25 | Loss: 0.00097477
Iteration 24/25 | Loss: 0.00095934
Iteration 25/25 | Loss: 0.00096208

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44707596
Iteration 2/25 | Loss: 0.00145723
Iteration 3/25 | Loss: 0.00137354
Iteration 4/25 | Loss: 0.00137354
Iteration 5/25 | Loss: 0.00137354
Iteration 6/25 | Loss: 0.00137354
Iteration 7/25 | Loss: 0.00137354
Iteration 8/25 | Loss: 0.00137354
Iteration 9/25 | Loss: 0.00137354
Iteration 10/25 | Loss: 0.00137353
Iteration 11/25 | Loss: 0.00137353
Iteration 12/25 | Loss: 0.00137353
Iteration 13/25 | Loss: 0.00137353
Iteration 14/25 | Loss: 0.00137353
Iteration 15/25 | Loss: 0.00137353
Iteration 16/25 | Loss: 0.00137353
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013735340908169746, 0.0013735340908169746, 0.0013735340908169746, 0.0013735340908169746, 0.0013735340908169746]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013735340908169746

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137353
Iteration 2/1000 | Loss: 0.00067615
Iteration 3/1000 | Loss: 0.00059356
Iteration 4/1000 | Loss: 0.00079231
Iteration 5/1000 | Loss: 0.00164247
Iteration 6/1000 | Loss: 0.00103093
Iteration 7/1000 | Loss: 0.00026734
Iteration 8/1000 | Loss: 0.00009012
Iteration 9/1000 | Loss: 0.00005586
Iteration 10/1000 | Loss: 0.00306154
Iteration 11/1000 | Loss: 0.00009471
Iteration 12/1000 | Loss: 0.00019425
Iteration 13/1000 | Loss: 0.00017322
Iteration 14/1000 | Loss: 0.00005662
Iteration 15/1000 | Loss: 0.00005315
Iteration 16/1000 | Loss: 0.00005391
Iteration 17/1000 | Loss: 0.00027205
Iteration 18/1000 | Loss: 0.00021621
Iteration 19/1000 | Loss: 0.00025945
Iteration 20/1000 | Loss: 0.00025885
Iteration 21/1000 | Loss: 0.00021595
Iteration 22/1000 | Loss: 0.00020670
Iteration 23/1000 | Loss: 0.00052048
Iteration 24/1000 | Loss: 0.00022567
Iteration 25/1000 | Loss: 0.00005683
Iteration 26/1000 | Loss: 0.00014394
Iteration 27/1000 | Loss: 0.00004997
Iteration 28/1000 | Loss: 0.00020688
Iteration 29/1000 | Loss: 0.00023284
Iteration 30/1000 | Loss: 0.00044004
Iteration 31/1000 | Loss: 0.00022879
Iteration 32/1000 | Loss: 0.00008857
Iteration 33/1000 | Loss: 0.00013334
Iteration 34/1000 | Loss: 0.00017954
Iteration 35/1000 | Loss: 0.00019948
Iteration 36/1000 | Loss: 0.00025431
Iteration 37/1000 | Loss: 0.00025650
Iteration 38/1000 | Loss: 0.00019581
Iteration 39/1000 | Loss: 0.00010848
Iteration 40/1000 | Loss: 0.00014097
Iteration 41/1000 | Loss: 0.00043504
Iteration 42/1000 | Loss: 0.00017412
Iteration 43/1000 | Loss: 0.00041349
Iteration 44/1000 | Loss: 0.00014855
Iteration 45/1000 | Loss: 0.00019365
Iteration 46/1000 | Loss: 0.00026402
Iteration 47/1000 | Loss: 0.00028966
Iteration 48/1000 | Loss: 0.00013029
Iteration 49/1000 | Loss: 0.00015508
Iteration 50/1000 | Loss: 0.00013664
Iteration 51/1000 | Loss: 0.00016808
Iteration 52/1000 | Loss: 0.00025419
Iteration 53/1000 | Loss: 0.00027824
Iteration 54/1000 | Loss: 0.00021089
Iteration 55/1000 | Loss: 0.00035966
Iteration 56/1000 | Loss: 0.00021500
Iteration 57/1000 | Loss: 0.00005332
Iteration 58/1000 | Loss: 0.00004424
Iteration 59/1000 | Loss: 0.00007650
Iteration 60/1000 | Loss: 0.00006672
Iteration 61/1000 | Loss: 0.00004759
Iteration 62/1000 | Loss: 0.00002749
Iteration 63/1000 | Loss: 0.00005990
Iteration 64/1000 | Loss: 0.00004091
Iteration 65/1000 | Loss: 0.00003496
Iteration 66/1000 | Loss: 0.00006494
Iteration 67/1000 | Loss: 0.00004138
Iteration 68/1000 | Loss: 0.00005241
Iteration 69/1000 | Loss: 0.00003989
Iteration 70/1000 | Loss: 0.00003174
Iteration 71/1000 | Loss: 0.00006264
Iteration 72/1000 | Loss: 0.00002373
Iteration 73/1000 | Loss: 0.00001856
Iteration 74/1000 | Loss: 0.00001630
Iteration 75/1000 | Loss: 0.00037698
Iteration 76/1000 | Loss: 0.00003307
Iteration 77/1000 | Loss: 0.00001937
Iteration 78/1000 | Loss: 0.00001534
Iteration 79/1000 | Loss: 0.00001341
Iteration 80/1000 | Loss: 0.00001208
Iteration 81/1000 | Loss: 0.00001138
Iteration 82/1000 | Loss: 0.00001085
Iteration 83/1000 | Loss: 0.00001075
Iteration 84/1000 | Loss: 0.00001051
Iteration 85/1000 | Loss: 0.00001030
Iteration 86/1000 | Loss: 0.00001013
Iteration 87/1000 | Loss: 0.00001006
Iteration 88/1000 | Loss: 0.00001001
Iteration 89/1000 | Loss: 0.00000991
Iteration 90/1000 | Loss: 0.00000983
Iteration 91/1000 | Loss: 0.00000980
Iteration 92/1000 | Loss: 0.00000977
Iteration 93/1000 | Loss: 0.00000977
Iteration 94/1000 | Loss: 0.00000975
Iteration 95/1000 | Loss: 0.00000974
Iteration 96/1000 | Loss: 0.00000974
Iteration 97/1000 | Loss: 0.00000974
Iteration 98/1000 | Loss: 0.00000971
Iteration 99/1000 | Loss: 0.00000971
Iteration 100/1000 | Loss: 0.00000970
Iteration 101/1000 | Loss: 0.00000970
Iteration 102/1000 | Loss: 0.00000970
Iteration 103/1000 | Loss: 0.00000969
Iteration 104/1000 | Loss: 0.00000969
Iteration 105/1000 | Loss: 0.00000968
Iteration 106/1000 | Loss: 0.00000968
Iteration 107/1000 | Loss: 0.00000968
Iteration 108/1000 | Loss: 0.00000968
Iteration 109/1000 | Loss: 0.00000967
Iteration 110/1000 | Loss: 0.00000967
Iteration 111/1000 | Loss: 0.00000967
Iteration 112/1000 | Loss: 0.00000967
Iteration 113/1000 | Loss: 0.00000967
Iteration 114/1000 | Loss: 0.00000967
Iteration 115/1000 | Loss: 0.00000967
Iteration 116/1000 | Loss: 0.00000967
Iteration 117/1000 | Loss: 0.00000967
Iteration 118/1000 | Loss: 0.00000967
Iteration 119/1000 | Loss: 0.00000967
Iteration 120/1000 | Loss: 0.00000966
Iteration 121/1000 | Loss: 0.00000966
Iteration 122/1000 | Loss: 0.00000966
Iteration 123/1000 | Loss: 0.00000966
Iteration 124/1000 | Loss: 0.00000966
Iteration 125/1000 | Loss: 0.00000966
Iteration 126/1000 | Loss: 0.00000966
Iteration 127/1000 | Loss: 0.00000966
Iteration 128/1000 | Loss: 0.00000966
Iteration 129/1000 | Loss: 0.00000966
Iteration 130/1000 | Loss: 0.00000966
Iteration 131/1000 | Loss: 0.00000966
Iteration 132/1000 | Loss: 0.00000966
Iteration 133/1000 | Loss: 0.00000966
Iteration 134/1000 | Loss: 0.00000966
Iteration 135/1000 | Loss: 0.00000966
Iteration 136/1000 | Loss: 0.00000966
Iteration 137/1000 | Loss: 0.00000966
Iteration 138/1000 | Loss: 0.00000966
Iteration 139/1000 | Loss: 0.00000966
Iteration 140/1000 | Loss: 0.00000966
Iteration 141/1000 | Loss: 0.00000966
Iteration 142/1000 | Loss: 0.00000966
Iteration 143/1000 | Loss: 0.00000966
Iteration 144/1000 | Loss: 0.00000966
Iteration 145/1000 | Loss: 0.00000966
Iteration 146/1000 | Loss: 0.00000966
Iteration 147/1000 | Loss: 0.00000966
Iteration 148/1000 | Loss: 0.00000966
Iteration 149/1000 | Loss: 0.00000966
Iteration 150/1000 | Loss: 0.00000966
Iteration 151/1000 | Loss: 0.00000966
Iteration 152/1000 | Loss: 0.00000966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [9.664733624958899e-06, 9.664733624958899e-06, 9.664733624958899e-06, 9.664733624958899e-06, 9.664733624958899e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.664733624958899e-06

Optimization complete. Final v2v error: 2.58243989944458 mm

Highest mean error: 3.9156689643859863 mm for frame 58

Lowest mean error: 2.214240074157715 mm for frame 27

Saving results

Total time: 169.92160058021545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395673
Iteration 2/25 | Loss: 0.00113571
Iteration 3/25 | Loss: 0.00097251
Iteration 4/25 | Loss: 0.00096292
Iteration 5/25 | Loss: 0.00096144
Iteration 6/25 | Loss: 0.00096144
Iteration 7/25 | Loss: 0.00096144
Iteration 8/25 | Loss: 0.00096144
Iteration 9/25 | Loss: 0.00096144
Iteration 10/25 | Loss: 0.00096144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009614368318580091, 0.0009614368318580091, 0.0009614368318580091, 0.0009614368318580091, 0.0009614368318580091]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009614368318580091

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33180845
Iteration 2/25 | Loss: 0.00055979
Iteration 3/25 | Loss: 0.00055979
Iteration 4/25 | Loss: 0.00055979
Iteration 5/25 | Loss: 0.00055979
Iteration 6/25 | Loss: 0.00055979
Iteration 7/25 | Loss: 0.00055979
Iteration 8/25 | Loss: 0.00055979
Iteration 9/25 | Loss: 0.00055979
Iteration 10/25 | Loss: 0.00055979
Iteration 11/25 | Loss: 0.00055979
Iteration 12/25 | Loss: 0.00055979
Iteration 13/25 | Loss: 0.00055979
Iteration 14/25 | Loss: 0.00055979
Iteration 15/25 | Loss: 0.00055979
Iteration 16/25 | Loss: 0.00055979
Iteration 17/25 | Loss: 0.00055979
Iteration 18/25 | Loss: 0.00055979
Iteration 19/25 | Loss: 0.00055979
Iteration 20/25 | Loss: 0.00055979
Iteration 21/25 | Loss: 0.00055979
Iteration 22/25 | Loss: 0.00055979
Iteration 23/25 | Loss: 0.00055979
Iteration 24/25 | Loss: 0.00055979
Iteration 25/25 | Loss: 0.00055979

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055979
Iteration 2/1000 | Loss: 0.00002420
Iteration 3/1000 | Loss: 0.00001485
Iteration 4/1000 | Loss: 0.00001305
Iteration 5/1000 | Loss: 0.00001220
Iteration 6/1000 | Loss: 0.00001174
Iteration 7/1000 | Loss: 0.00001149
Iteration 8/1000 | Loss: 0.00001129
Iteration 9/1000 | Loss: 0.00001115
Iteration 10/1000 | Loss: 0.00001114
Iteration 11/1000 | Loss: 0.00001103
Iteration 12/1000 | Loss: 0.00001094
Iteration 13/1000 | Loss: 0.00001094
Iteration 14/1000 | Loss: 0.00001090
Iteration 15/1000 | Loss: 0.00001088
Iteration 16/1000 | Loss: 0.00001086
Iteration 17/1000 | Loss: 0.00001086
Iteration 18/1000 | Loss: 0.00001084
Iteration 19/1000 | Loss: 0.00001083
Iteration 20/1000 | Loss: 0.00001082
Iteration 21/1000 | Loss: 0.00001081
Iteration 22/1000 | Loss: 0.00001078
Iteration 23/1000 | Loss: 0.00001077
Iteration 24/1000 | Loss: 0.00001075
Iteration 25/1000 | Loss: 0.00001074
Iteration 26/1000 | Loss: 0.00001074
Iteration 27/1000 | Loss: 0.00001074
Iteration 28/1000 | Loss: 0.00001073
Iteration 29/1000 | Loss: 0.00001073
Iteration 30/1000 | Loss: 0.00001073
Iteration 31/1000 | Loss: 0.00001072
Iteration 32/1000 | Loss: 0.00001072
Iteration 33/1000 | Loss: 0.00001071
Iteration 34/1000 | Loss: 0.00001071
Iteration 35/1000 | Loss: 0.00001071
Iteration 36/1000 | Loss: 0.00001071
Iteration 37/1000 | Loss: 0.00001071
Iteration 38/1000 | Loss: 0.00001071
Iteration 39/1000 | Loss: 0.00001071
Iteration 40/1000 | Loss: 0.00001071
Iteration 41/1000 | Loss: 0.00001071
Iteration 42/1000 | Loss: 0.00001070
Iteration 43/1000 | Loss: 0.00001070
Iteration 44/1000 | Loss: 0.00001070
Iteration 45/1000 | Loss: 0.00001070
Iteration 46/1000 | Loss: 0.00001070
Iteration 47/1000 | Loss: 0.00001070
Iteration 48/1000 | Loss: 0.00001070
Iteration 49/1000 | Loss: 0.00001070
Iteration 50/1000 | Loss: 0.00001070
Iteration 51/1000 | Loss: 0.00001069
Iteration 52/1000 | Loss: 0.00001069
Iteration 53/1000 | Loss: 0.00001069
Iteration 54/1000 | Loss: 0.00001069
Iteration 55/1000 | Loss: 0.00001069
Iteration 56/1000 | Loss: 0.00001069
Iteration 57/1000 | Loss: 0.00001069
Iteration 58/1000 | Loss: 0.00001069
Iteration 59/1000 | Loss: 0.00001069
Iteration 60/1000 | Loss: 0.00001069
Iteration 61/1000 | Loss: 0.00001069
Iteration 62/1000 | Loss: 0.00001069
Iteration 63/1000 | Loss: 0.00001069
Iteration 64/1000 | Loss: 0.00001069
Iteration 65/1000 | Loss: 0.00001069
Iteration 66/1000 | Loss: 0.00001069
Iteration 67/1000 | Loss: 0.00001069
Iteration 68/1000 | Loss: 0.00001069
Iteration 69/1000 | Loss: 0.00001068
Iteration 70/1000 | Loss: 0.00001068
Iteration 71/1000 | Loss: 0.00001068
Iteration 72/1000 | Loss: 0.00001068
Iteration 73/1000 | Loss: 0.00001068
Iteration 74/1000 | Loss: 0.00001068
Iteration 75/1000 | Loss: 0.00001068
Iteration 76/1000 | Loss: 0.00001068
Iteration 77/1000 | Loss: 0.00001068
Iteration 78/1000 | Loss: 0.00001068
Iteration 79/1000 | Loss: 0.00001068
Iteration 80/1000 | Loss: 0.00001068
Iteration 81/1000 | Loss: 0.00001068
Iteration 82/1000 | Loss: 0.00001068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.068221899913624e-05, 1.068221899913624e-05, 1.068221899913624e-05, 1.068221899913624e-05, 1.068221899913624e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.068221899913624e-05

Optimization complete. Final v2v error: 2.6650784015655518 mm

Highest mean error: 2.9837756156921387 mm for frame 131

Lowest mean error: 2.2682769298553467 mm for frame 67

Saving results

Total time: 30.5547935962677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844988
Iteration 2/25 | Loss: 0.00105138
Iteration 3/25 | Loss: 0.00091534
Iteration 4/25 | Loss: 0.00091083
Iteration 5/25 | Loss: 0.00091083
Iteration 6/25 | Loss: 0.00091083
Iteration 7/25 | Loss: 0.00091083
Iteration 8/25 | Loss: 0.00091083
Iteration 9/25 | Loss: 0.00091083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0009108295198529959, 0.0009108295198529959, 0.0009108295198529959, 0.0009108295198529959, 0.0009108295198529959]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009108295198529959

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34238267
Iteration 2/25 | Loss: 0.00059200
Iteration 3/25 | Loss: 0.00059200
Iteration 4/25 | Loss: 0.00059200
Iteration 5/25 | Loss: 0.00059200
Iteration 6/25 | Loss: 0.00059200
Iteration 7/25 | Loss: 0.00059200
Iteration 8/25 | Loss: 0.00059200
Iteration 9/25 | Loss: 0.00059200
Iteration 10/25 | Loss: 0.00059200
Iteration 11/25 | Loss: 0.00059200
Iteration 12/25 | Loss: 0.00059200
Iteration 13/25 | Loss: 0.00059200
Iteration 14/25 | Loss: 0.00059200
Iteration 15/25 | Loss: 0.00059200
Iteration 16/25 | Loss: 0.00059200
Iteration 17/25 | Loss: 0.00059200
Iteration 18/25 | Loss: 0.00059200
Iteration 19/25 | Loss: 0.00059200
Iteration 20/25 | Loss: 0.00059200
Iteration 21/25 | Loss: 0.00059200
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005919952527619898, 0.0005919952527619898, 0.0005919952527619898, 0.0005919952527619898, 0.0005919952527619898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005919952527619898

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059200
Iteration 2/1000 | Loss: 0.00001536
Iteration 3/1000 | Loss: 0.00001063
Iteration 4/1000 | Loss: 0.00000968
Iteration 5/1000 | Loss: 0.00000923
Iteration 6/1000 | Loss: 0.00000898
Iteration 7/1000 | Loss: 0.00000863
Iteration 8/1000 | Loss: 0.00000843
Iteration 9/1000 | Loss: 0.00000826
Iteration 10/1000 | Loss: 0.00000818
Iteration 11/1000 | Loss: 0.00000815
Iteration 12/1000 | Loss: 0.00000814
Iteration 13/1000 | Loss: 0.00000809
Iteration 14/1000 | Loss: 0.00000809
Iteration 15/1000 | Loss: 0.00000808
Iteration 16/1000 | Loss: 0.00000807
Iteration 17/1000 | Loss: 0.00000807
Iteration 18/1000 | Loss: 0.00000807
Iteration 19/1000 | Loss: 0.00000806
Iteration 20/1000 | Loss: 0.00000803
Iteration 21/1000 | Loss: 0.00000803
Iteration 22/1000 | Loss: 0.00000803
Iteration 23/1000 | Loss: 0.00000803
Iteration 24/1000 | Loss: 0.00000803
Iteration 25/1000 | Loss: 0.00000803
Iteration 26/1000 | Loss: 0.00000803
Iteration 27/1000 | Loss: 0.00000802
Iteration 28/1000 | Loss: 0.00000802
Iteration 29/1000 | Loss: 0.00000802
Iteration 30/1000 | Loss: 0.00000802
Iteration 31/1000 | Loss: 0.00000802
Iteration 32/1000 | Loss: 0.00000802
Iteration 33/1000 | Loss: 0.00000802
Iteration 34/1000 | Loss: 0.00000802
Iteration 35/1000 | Loss: 0.00000799
Iteration 36/1000 | Loss: 0.00000799
Iteration 37/1000 | Loss: 0.00000798
Iteration 38/1000 | Loss: 0.00000798
Iteration 39/1000 | Loss: 0.00000798
Iteration 40/1000 | Loss: 0.00000798
Iteration 41/1000 | Loss: 0.00000798
Iteration 42/1000 | Loss: 0.00000798
Iteration 43/1000 | Loss: 0.00000797
Iteration 44/1000 | Loss: 0.00000797
Iteration 45/1000 | Loss: 0.00000797
Iteration 46/1000 | Loss: 0.00000797
Iteration 47/1000 | Loss: 0.00000796
Iteration 48/1000 | Loss: 0.00000795
Iteration 49/1000 | Loss: 0.00000795
Iteration 50/1000 | Loss: 0.00000795
Iteration 51/1000 | Loss: 0.00000795
Iteration 52/1000 | Loss: 0.00000795
Iteration 53/1000 | Loss: 0.00000795
Iteration 54/1000 | Loss: 0.00000795
Iteration 55/1000 | Loss: 0.00000795
Iteration 56/1000 | Loss: 0.00000795
Iteration 57/1000 | Loss: 0.00000795
Iteration 58/1000 | Loss: 0.00000794
Iteration 59/1000 | Loss: 0.00000794
Iteration 60/1000 | Loss: 0.00000794
Iteration 61/1000 | Loss: 0.00000794
Iteration 62/1000 | Loss: 0.00000794
Iteration 63/1000 | Loss: 0.00000794
Iteration 64/1000 | Loss: 0.00000794
Iteration 65/1000 | Loss: 0.00000794
Iteration 66/1000 | Loss: 0.00000794
Iteration 67/1000 | Loss: 0.00000794
Iteration 68/1000 | Loss: 0.00000794
Iteration 69/1000 | Loss: 0.00000794
Iteration 70/1000 | Loss: 0.00000794
Iteration 71/1000 | Loss: 0.00000794
Iteration 72/1000 | Loss: 0.00000794
Iteration 73/1000 | Loss: 0.00000794
Iteration 74/1000 | Loss: 0.00000794
Iteration 75/1000 | Loss: 0.00000794
Iteration 76/1000 | Loss: 0.00000794
Iteration 77/1000 | Loss: 0.00000794
Iteration 78/1000 | Loss: 0.00000794
Iteration 79/1000 | Loss: 0.00000794
Iteration 80/1000 | Loss: 0.00000794
Iteration 81/1000 | Loss: 0.00000794
Iteration 82/1000 | Loss: 0.00000794
Iteration 83/1000 | Loss: 0.00000794
Iteration 84/1000 | Loss: 0.00000794
Iteration 85/1000 | Loss: 0.00000794
Iteration 86/1000 | Loss: 0.00000794
Iteration 87/1000 | Loss: 0.00000794
Iteration 88/1000 | Loss: 0.00000794
Iteration 89/1000 | Loss: 0.00000794
Iteration 90/1000 | Loss: 0.00000794
Iteration 91/1000 | Loss: 0.00000794
Iteration 92/1000 | Loss: 0.00000794
Iteration 93/1000 | Loss: 0.00000794
Iteration 94/1000 | Loss: 0.00000794
Iteration 95/1000 | Loss: 0.00000794
Iteration 96/1000 | Loss: 0.00000794
Iteration 97/1000 | Loss: 0.00000794
Iteration 98/1000 | Loss: 0.00000794
Iteration 99/1000 | Loss: 0.00000794
Iteration 100/1000 | Loss: 0.00000794
Iteration 101/1000 | Loss: 0.00000794
Iteration 102/1000 | Loss: 0.00000794
Iteration 103/1000 | Loss: 0.00000794
Iteration 104/1000 | Loss: 0.00000794
Iteration 105/1000 | Loss: 0.00000794
Iteration 106/1000 | Loss: 0.00000794
Iteration 107/1000 | Loss: 0.00000794
Iteration 108/1000 | Loss: 0.00000794
Iteration 109/1000 | Loss: 0.00000794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [7.935433131933678e-06, 7.935433131933678e-06, 7.935433131933678e-06, 7.935433131933678e-06, 7.935433131933678e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.935433131933678e-06

Optimization complete. Final v2v error: 2.358428716659546 mm

Highest mean error: 2.6471457481384277 mm for frame 124

Lowest mean error: 2.1717286109924316 mm for frame 159

Saving results

Total time: 28.57874846458435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00720623
Iteration 2/25 | Loss: 0.00139453
Iteration 3/25 | Loss: 0.00103626
Iteration 4/25 | Loss: 0.00099281
Iteration 5/25 | Loss: 0.00100302
Iteration 6/25 | Loss: 0.00100546
Iteration 7/25 | Loss: 0.00098392
Iteration 8/25 | Loss: 0.00095532
Iteration 9/25 | Loss: 0.00094653
Iteration 10/25 | Loss: 0.00094316
Iteration 11/25 | Loss: 0.00094195
Iteration 12/25 | Loss: 0.00094171
Iteration 13/25 | Loss: 0.00094155
Iteration 14/25 | Loss: 0.00094143
Iteration 15/25 | Loss: 0.00094140
Iteration 16/25 | Loss: 0.00094135
Iteration 17/25 | Loss: 0.00094135
Iteration 18/25 | Loss: 0.00094134
Iteration 19/25 | Loss: 0.00094134
Iteration 20/25 | Loss: 0.00094134
Iteration 21/25 | Loss: 0.00094134
Iteration 22/25 | Loss: 0.00094133
Iteration 23/25 | Loss: 0.00094133
Iteration 24/25 | Loss: 0.00094133
Iteration 25/25 | Loss: 0.00094133

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.62432575
Iteration 2/25 | Loss: 0.00039003
Iteration 3/25 | Loss: 0.00038995
Iteration 4/25 | Loss: 0.00038995
Iteration 5/25 | Loss: 0.00038995
Iteration 6/25 | Loss: 0.00038995
Iteration 7/25 | Loss: 0.00038995
Iteration 8/25 | Loss: 0.00038995
Iteration 9/25 | Loss: 0.00038995
Iteration 10/25 | Loss: 0.00038995
Iteration 11/25 | Loss: 0.00038995
Iteration 12/25 | Loss: 0.00038995
Iteration 13/25 | Loss: 0.00038995
Iteration 14/25 | Loss: 0.00038995
Iteration 15/25 | Loss: 0.00038995
Iteration 16/25 | Loss: 0.00038995
Iteration 17/25 | Loss: 0.00038995
Iteration 18/25 | Loss: 0.00038995
Iteration 19/25 | Loss: 0.00038995
Iteration 20/25 | Loss: 0.00038995
Iteration 21/25 | Loss: 0.00038995
Iteration 22/25 | Loss: 0.00038995
Iteration 23/25 | Loss: 0.00038995
Iteration 24/25 | Loss: 0.00038995
Iteration 25/25 | Loss: 0.00038995

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038995
Iteration 2/1000 | Loss: 0.00002860
Iteration 3/1000 | Loss: 0.00001532
Iteration 4/1000 | Loss: 0.00001271
Iteration 5/1000 | Loss: 0.00001161
Iteration 6/1000 | Loss: 0.00001121
Iteration 7/1000 | Loss: 0.00001096
Iteration 8/1000 | Loss: 0.00001086
Iteration 9/1000 | Loss: 0.00001076
Iteration 10/1000 | Loss: 0.00001066
Iteration 11/1000 | Loss: 0.00001065
Iteration 12/1000 | Loss: 0.00001055
Iteration 13/1000 | Loss: 0.00001052
Iteration 14/1000 | Loss: 0.00001051
Iteration 15/1000 | Loss: 0.00001051
Iteration 16/1000 | Loss: 0.00001043
Iteration 17/1000 | Loss: 0.00001040
Iteration 18/1000 | Loss: 0.00001040
Iteration 19/1000 | Loss: 0.00001038
Iteration 20/1000 | Loss: 0.00001037
Iteration 21/1000 | Loss: 0.00001036
Iteration 22/1000 | Loss: 0.00001036
Iteration 23/1000 | Loss: 0.00001036
Iteration 24/1000 | Loss: 0.00001035
Iteration 25/1000 | Loss: 0.00001035
Iteration 26/1000 | Loss: 0.00001035
Iteration 27/1000 | Loss: 0.00001035
Iteration 28/1000 | Loss: 0.00001034
Iteration 29/1000 | Loss: 0.00001034
Iteration 30/1000 | Loss: 0.00001033
Iteration 31/1000 | Loss: 0.00001032
Iteration 32/1000 | Loss: 0.00001032
Iteration 33/1000 | Loss: 0.00001032
Iteration 34/1000 | Loss: 0.00001032
Iteration 35/1000 | Loss: 0.00001031
Iteration 36/1000 | Loss: 0.00001031
Iteration 37/1000 | Loss: 0.00001031
Iteration 38/1000 | Loss: 0.00001030
Iteration 39/1000 | Loss: 0.00001030
Iteration 40/1000 | Loss: 0.00001030
Iteration 41/1000 | Loss: 0.00001029
Iteration 42/1000 | Loss: 0.00001029
Iteration 43/1000 | Loss: 0.00001029
Iteration 44/1000 | Loss: 0.00001029
Iteration 45/1000 | Loss: 0.00001029
Iteration 46/1000 | Loss: 0.00001028
Iteration 47/1000 | Loss: 0.00001028
Iteration 48/1000 | Loss: 0.00001028
Iteration 49/1000 | Loss: 0.00001027
Iteration 50/1000 | Loss: 0.00001027
Iteration 51/1000 | Loss: 0.00001027
Iteration 52/1000 | Loss: 0.00001027
Iteration 53/1000 | Loss: 0.00001026
Iteration 54/1000 | Loss: 0.00001026
Iteration 55/1000 | Loss: 0.00001026
Iteration 56/1000 | Loss: 0.00001026
Iteration 57/1000 | Loss: 0.00001026
Iteration 58/1000 | Loss: 0.00001026
Iteration 59/1000 | Loss: 0.00001025
Iteration 60/1000 | Loss: 0.00001025
Iteration 61/1000 | Loss: 0.00001024
Iteration 62/1000 | Loss: 0.00001024
Iteration 63/1000 | Loss: 0.00001024
Iteration 64/1000 | Loss: 0.00001024
Iteration 65/1000 | Loss: 0.00001024
Iteration 66/1000 | Loss: 0.00001024
Iteration 67/1000 | Loss: 0.00001024
Iteration 68/1000 | Loss: 0.00001024
Iteration 69/1000 | Loss: 0.00001023
Iteration 70/1000 | Loss: 0.00001023
Iteration 71/1000 | Loss: 0.00001023
Iteration 72/1000 | Loss: 0.00001023
Iteration 73/1000 | Loss: 0.00001023
Iteration 74/1000 | Loss: 0.00001023
Iteration 75/1000 | Loss: 0.00001023
Iteration 76/1000 | Loss: 0.00001023
Iteration 77/1000 | Loss: 0.00001023
Iteration 78/1000 | Loss: 0.00001022
Iteration 79/1000 | Loss: 0.00001022
Iteration 80/1000 | Loss: 0.00001022
Iteration 81/1000 | Loss: 0.00001022
Iteration 82/1000 | Loss: 0.00001022
Iteration 83/1000 | Loss: 0.00001021
Iteration 84/1000 | Loss: 0.00001021
Iteration 85/1000 | Loss: 0.00001021
Iteration 86/1000 | Loss: 0.00001020
Iteration 87/1000 | Loss: 0.00001020
Iteration 88/1000 | Loss: 0.00001020
Iteration 89/1000 | Loss: 0.00001020
Iteration 90/1000 | Loss: 0.00001020
Iteration 91/1000 | Loss: 0.00001020
Iteration 92/1000 | Loss: 0.00001020
Iteration 93/1000 | Loss: 0.00001020
Iteration 94/1000 | Loss: 0.00001019
Iteration 95/1000 | Loss: 0.00001019
Iteration 96/1000 | Loss: 0.00001019
Iteration 97/1000 | Loss: 0.00001019
Iteration 98/1000 | Loss: 0.00001019
Iteration 99/1000 | Loss: 0.00001019
Iteration 100/1000 | Loss: 0.00001019
Iteration 101/1000 | Loss: 0.00001019
Iteration 102/1000 | Loss: 0.00001019
Iteration 103/1000 | Loss: 0.00001019
Iteration 104/1000 | Loss: 0.00001019
Iteration 105/1000 | Loss: 0.00001018
Iteration 106/1000 | Loss: 0.00001018
Iteration 107/1000 | Loss: 0.00001018
Iteration 108/1000 | Loss: 0.00001018
Iteration 109/1000 | Loss: 0.00001018
Iteration 110/1000 | Loss: 0.00001018
Iteration 111/1000 | Loss: 0.00001018
Iteration 112/1000 | Loss: 0.00001018
Iteration 113/1000 | Loss: 0.00001018
Iteration 114/1000 | Loss: 0.00001018
Iteration 115/1000 | Loss: 0.00001018
Iteration 116/1000 | Loss: 0.00001018
Iteration 117/1000 | Loss: 0.00001018
Iteration 118/1000 | Loss: 0.00001018
Iteration 119/1000 | Loss: 0.00001018
Iteration 120/1000 | Loss: 0.00001017
Iteration 121/1000 | Loss: 0.00001017
Iteration 122/1000 | Loss: 0.00001017
Iteration 123/1000 | Loss: 0.00001017
Iteration 124/1000 | Loss: 0.00001017
Iteration 125/1000 | Loss: 0.00001017
Iteration 126/1000 | Loss: 0.00001017
Iteration 127/1000 | Loss: 0.00001017
Iteration 128/1000 | Loss: 0.00001017
Iteration 129/1000 | Loss: 0.00001017
Iteration 130/1000 | Loss: 0.00001017
Iteration 131/1000 | Loss: 0.00001017
Iteration 132/1000 | Loss: 0.00001017
Iteration 133/1000 | Loss: 0.00001017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.0170395398745313e-05, 1.0170395398745313e-05, 1.0170395398745313e-05, 1.0170395398745313e-05, 1.0170395398745313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0170395398745313e-05

Optimization complete. Final v2v error: 2.627007007598877 mm

Highest mean error: 8.526384353637695 mm for frame 129

Lowest mean error: 2.324868679046631 mm for frame 89

Saving results

Total time: 49.62296462059021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403025
Iteration 2/25 | Loss: 0.00109231
Iteration 3/25 | Loss: 0.00095475
Iteration 4/25 | Loss: 0.00094536
Iteration 5/25 | Loss: 0.00094190
Iteration 6/25 | Loss: 0.00094175
Iteration 7/25 | Loss: 0.00094175
Iteration 8/25 | Loss: 0.00094175
Iteration 9/25 | Loss: 0.00094175
Iteration 10/25 | Loss: 0.00094175
Iteration 11/25 | Loss: 0.00094175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009417535620741546, 0.0009417535620741546, 0.0009417535620741546, 0.0009417535620741546, 0.0009417535620741546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009417535620741546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57266176
Iteration 2/25 | Loss: 0.00053172
Iteration 3/25 | Loss: 0.00053172
Iteration 4/25 | Loss: 0.00053172
Iteration 5/25 | Loss: 0.00053171
Iteration 6/25 | Loss: 0.00053171
Iteration 7/25 | Loss: 0.00053171
Iteration 8/25 | Loss: 0.00053171
Iteration 9/25 | Loss: 0.00053171
Iteration 10/25 | Loss: 0.00053171
Iteration 11/25 | Loss: 0.00053171
Iteration 12/25 | Loss: 0.00053171
Iteration 13/25 | Loss: 0.00053171
Iteration 14/25 | Loss: 0.00053171
Iteration 15/25 | Loss: 0.00053171
Iteration 16/25 | Loss: 0.00053171
Iteration 17/25 | Loss: 0.00053171
Iteration 18/25 | Loss: 0.00053171
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005317134782671928, 0.0005317134782671928, 0.0005317134782671928, 0.0005317134782671928, 0.0005317134782671928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005317134782671928

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053171
Iteration 2/1000 | Loss: 0.00002027
Iteration 3/1000 | Loss: 0.00001203
Iteration 4/1000 | Loss: 0.00001073
Iteration 5/1000 | Loss: 0.00000996
Iteration 6/1000 | Loss: 0.00000957
Iteration 7/1000 | Loss: 0.00000945
Iteration 8/1000 | Loss: 0.00000921
Iteration 9/1000 | Loss: 0.00000914
Iteration 10/1000 | Loss: 0.00000907
Iteration 11/1000 | Loss: 0.00000906
Iteration 12/1000 | Loss: 0.00000906
Iteration 13/1000 | Loss: 0.00000894
Iteration 14/1000 | Loss: 0.00000890
Iteration 15/1000 | Loss: 0.00000890
Iteration 16/1000 | Loss: 0.00000890
Iteration 17/1000 | Loss: 0.00000890
Iteration 18/1000 | Loss: 0.00000889
Iteration 19/1000 | Loss: 0.00000889
Iteration 20/1000 | Loss: 0.00000889
Iteration 21/1000 | Loss: 0.00000888
Iteration 22/1000 | Loss: 0.00000887
Iteration 23/1000 | Loss: 0.00000882
Iteration 24/1000 | Loss: 0.00000882
Iteration 25/1000 | Loss: 0.00000881
Iteration 26/1000 | Loss: 0.00000880
Iteration 27/1000 | Loss: 0.00000879
Iteration 28/1000 | Loss: 0.00000879
Iteration 29/1000 | Loss: 0.00000879
Iteration 30/1000 | Loss: 0.00000878
Iteration 31/1000 | Loss: 0.00000878
Iteration 32/1000 | Loss: 0.00000878
Iteration 33/1000 | Loss: 0.00000878
Iteration 34/1000 | Loss: 0.00000878
Iteration 35/1000 | Loss: 0.00000877
Iteration 36/1000 | Loss: 0.00000877
Iteration 37/1000 | Loss: 0.00000874
Iteration 38/1000 | Loss: 0.00000874
Iteration 39/1000 | Loss: 0.00000873
Iteration 40/1000 | Loss: 0.00000873
Iteration 41/1000 | Loss: 0.00000872
Iteration 42/1000 | Loss: 0.00000872
Iteration 43/1000 | Loss: 0.00000872
Iteration 44/1000 | Loss: 0.00000872
Iteration 45/1000 | Loss: 0.00000872
Iteration 46/1000 | Loss: 0.00000872
Iteration 47/1000 | Loss: 0.00000872
Iteration 48/1000 | Loss: 0.00000871
Iteration 49/1000 | Loss: 0.00000871
Iteration 50/1000 | Loss: 0.00000870
Iteration 51/1000 | Loss: 0.00000870
Iteration 52/1000 | Loss: 0.00000870
Iteration 53/1000 | Loss: 0.00000869
Iteration 54/1000 | Loss: 0.00000869
Iteration 55/1000 | Loss: 0.00000869
Iteration 56/1000 | Loss: 0.00000869
Iteration 57/1000 | Loss: 0.00000868
Iteration 58/1000 | Loss: 0.00000868
Iteration 59/1000 | Loss: 0.00000867
Iteration 60/1000 | Loss: 0.00000867
Iteration 61/1000 | Loss: 0.00000867
Iteration 62/1000 | Loss: 0.00000866
Iteration 63/1000 | Loss: 0.00000866
Iteration 64/1000 | Loss: 0.00000866
Iteration 65/1000 | Loss: 0.00000866
Iteration 66/1000 | Loss: 0.00000866
Iteration 67/1000 | Loss: 0.00000866
Iteration 68/1000 | Loss: 0.00000866
Iteration 69/1000 | Loss: 0.00000866
Iteration 70/1000 | Loss: 0.00000866
Iteration 71/1000 | Loss: 0.00000866
Iteration 72/1000 | Loss: 0.00000865
Iteration 73/1000 | Loss: 0.00000865
Iteration 74/1000 | Loss: 0.00000865
Iteration 75/1000 | Loss: 0.00000864
Iteration 76/1000 | Loss: 0.00000864
Iteration 77/1000 | Loss: 0.00000864
Iteration 78/1000 | Loss: 0.00000863
Iteration 79/1000 | Loss: 0.00000863
Iteration 80/1000 | Loss: 0.00000862
Iteration 81/1000 | Loss: 0.00000862
Iteration 82/1000 | Loss: 0.00000862
Iteration 83/1000 | Loss: 0.00000862
Iteration 84/1000 | Loss: 0.00000861
Iteration 85/1000 | Loss: 0.00000861
Iteration 86/1000 | Loss: 0.00000861
Iteration 87/1000 | Loss: 0.00000860
Iteration 88/1000 | Loss: 0.00000860
Iteration 89/1000 | Loss: 0.00000860
Iteration 90/1000 | Loss: 0.00000860
Iteration 91/1000 | Loss: 0.00000859
Iteration 92/1000 | Loss: 0.00000859
Iteration 93/1000 | Loss: 0.00000859
Iteration 94/1000 | Loss: 0.00000859
Iteration 95/1000 | Loss: 0.00000859
Iteration 96/1000 | Loss: 0.00000858
Iteration 97/1000 | Loss: 0.00000858
Iteration 98/1000 | Loss: 0.00000857
Iteration 99/1000 | Loss: 0.00000857
Iteration 100/1000 | Loss: 0.00000857
Iteration 101/1000 | Loss: 0.00000856
Iteration 102/1000 | Loss: 0.00000856
Iteration 103/1000 | Loss: 0.00000856
Iteration 104/1000 | Loss: 0.00000856
Iteration 105/1000 | Loss: 0.00000856
Iteration 106/1000 | Loss: 0.00000856
Iteration 107/1000 | Loss: 0.00000856
Iteration 108/1000 | Loss: 0.00000856
Iteration 109/1000 | Loss: 0.00000856
Iteration 110/1000 | Loss: 0.00000856
Iteration 111/1000 | Loss: 0.00000856
Iteration 112/1000 | Loss: 0.00000856
Iteration 113/1000 | Loss: 0.00000856
Iteration 114/1000 | Loss: 0.00000856
Iteration 115/1000 | Loss: 0.00000856
Iteration 116/1000 | Loss: 0.00000856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [8.55748567119008e-06, 8.55748567119008e-06, 8.55748567119008e-06, 8.55748567119008e-06, 8.55748567119008e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.55748567119008e-06

Optimization complete. Final v2v error: 2.515068292617798 mm

Highest mean error: 2.850388288497925 mm for frame 43

Lowest mean error: 2.295595169067383 mm for frame 203

Saving results

Total time: 34.778531074523926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841670
Iteration 2/25 | Loss: 0.00104401
Iteration 3/25 | Loss: 0.00092581
Iteration 4/25 | Loss: 0.00091360
Iteration 5/25 | Loss: 0.00091137
Iteration 6/25 | Loss: 0.00091079
Iteration 7/25 | Loss: 0.00091079
Iteration 8/25 | Loss: 0.00091079
Iteration 9/25 | Loss: 0.00091079
Iteration 10/25 | Loss: 0.00091079
Iteration 11/25 | Loss: 0.00091079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009107939549721777, 0.0009107939549721777, 0.0009107939549721777, 0.0009107939549721777, 0.0009107939549721777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009107939549721777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33849096
Iteration 2/25 | Loss: 0.00059666
Iteration 3/25 | Loss: 0.00059666
Iteration 4/25 | Loss: 0.00059665
Iteration 5/25 | Loss: 0.00059665
Iteration 6/25 | Loss: 0.00059665
Iteration 7/25 | Loss: 0.00059665
Iteration 8/25 | Loss: 0.00059665
Iteration 9/25 | Loss: 0.00059665
Iteration 10/25 | Loss: 0.00059665
Iteration 11/25 | Loss: 0.00059665
Iteration 12/25 | Loss: 0.00059665
Iteration 13/25 | Loss: 0.00059665
Iteration 14/25 | Loss: 0.00059665
Iteration 15/25 | Loss: 0.00059665
Iteration 16/25 | Loss: 0.00059665
Iteration 17/25 | Loss: 0.00059665
Iteration 18/25 | Loss: 0.00059665
Iteration 19/25 | Loss: 0.00059665
Iteration 20/25 | Loss: 0.00059665
Iteration 21/25 | Loss: 0.00059665
Iteration 22/25 | Loss: 0.00059665
Iteration 23/25 | Loss: 0.00059665
Iteration 24/25 | Loss: 0.00059665
Iteration 25/25 | Loss: 0.00059665

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059665
Iteration 2/1000 | Loss: 0.00002353
Iteration 3/1000 | Loss: 0.00001132
Iteration 4/1000 | Loss: 0.00000981
Iteration 5/1000 | Loss: 0.00000917
Iteration 6/1000 | Loss: 0.00000891
Iteration 7/1000 | Loss: 0.00000870
Iteration 8/1000 | Loss: 0.00000855
Iteration 9/1000 | Loss: 0.00000847
Iteration 10/1000 | Loss: 0.00000845
Iteration 11/1000 | Loss: 0.00000845
Iteration 12/1000 | Loss: 0.00000845
Iteration 13/1000 | Loss: 0.00000844
Iteration 14/1000 | Loss: 0.00000844
Iteration 15/1000 | Loss: 0.00000844
Iteration 16/1000 | Loss: 0.00000839
Iteration 17/1000 | Loss: 0.00000839
Iteration 18/1000 | Loss: 0.00000839
Iteration 19/1000 | Loss: 0.00000839
Iteration 20/1000 | Loss: 0.00000838
Iteration 21/1000 | Loss: 0.00000838
Iteration 22/1000 | Loss: 0.00000838
Iteration 23/1000 | Loss: 0.00000838
Iteration 24/1000 | Loss: 0.00000838
Iteration 25/1000 | Loss: 0.00000835
Iteration 26/1000 | Loss: 0.00000835
Iteration 27/1000 | Loss: 0.00000835
Iteration 28/1000 | Loss: 0.00000835
Iteration 29/1000 | Loss: 0.00000832
Iteration 30/1000 | Loss: 0.00000830
Iteration 31/1000 | Loss: 0.00000830
Iteration 32/1000 | Loss: 0.00000830
Iteration 33/1000 | Loss: 0.00000829
Iteration 34/1000 | Loss: 0.00000829
Iteration 35/1000 | Loss: 0.00000829
Iteration 36/1000 | Loss: 0.00000828
Iteration 37/1000 | Loss: 0.00000828
Iteration 38/1000 | Loss: 0.00000827
Iteration 39/1000 | Loss: 0.00000827
Iteration 40/1000 | Loss: 0.00000827
Iteration 41/1000 | Loss: 0.00000827
Iteration 42/1000 | Loss: 0.00000827
Iteration 43/1000 | Loss: 0.00000827
Iteration 44/1000 | Loss: 0.00000826
Iteration 45/1000 | Loss: 0.00000826
Iteration 46/1000 | Loss: 0.00000826
Iteration 47/1000 | Loss: 0.00000826
Iteration 48/1000 | Loss: 0.00000826
Iteration 49/1000 | Loss: 0.00000826
Iteration 50/1000 | Loss: 0.00000826
Iteration 51/1000 | Loss: 0.00000825
Iteration 52/1000 | Loss: 0.00000825
Iteration 53/1000 | Loss: 0.00000824
Iteration 54/1000 | Loss: 0.00000824
Iteration 55/1000 | Loss: 0.00000824
Iteration 56/1000 | Loss: 0.00000823
Iteration 57/1000 | Loss: 0.00000823
Iteration 58/1000 | Loss: 0.00000823
Iteration 59/1000 | Loss: 0.00000823
Iteration 60/1000 | Loss: 0.00000823
Iteration 61/1000 | Loss: 0.00000823
Iteration 62/1000 | Loss: 0.00000822
Iteration 63/1000 | Loss: 0.00000822
Iteration 64/1000 | Loss: 0.00000822
Iteration 65/1000 | Loss: 0.00000822
Iteration 66/1000 | Loss: 0.00000822
Iteration 67/1000 | Loss: 0.00000821
Iteration 68/1000 | Loss: 0.00000821
Iteration 69/1000 | Loss: 0.00000821
Iteration 70/1000 | Loss: 0.00000821
Iteration 71/1000 | Loss: 0.00000821
Iteration 72/1000 | Loss: 0.00000821
Iteration 73/1000 | Loss: 0.00000820
Iteration 74/1000 | Loss: 0.00000820
Iteration 75/1000 | Loss: 0.00000820
Iteration 76/1000 | Loss: 0.00000820
Iteration 77/1000 | Loss: 0.00000820
Iteration 78/1000 | Loss: 0.00000819
Iteration 79/1000 | Loss: 0.00000819
Iteration 80/1000 | Loss: 0.00000819
Iteration 81/1000 | Loss: 0.00000818
Iteration 82/1000 | Loss: 0.00000818
Iteration 83/1000 | Loss: 0.00000818
Iteration 84/1000 | Loss: 0.00000817
Iteration 85/1000 | Loss: 0.00000817
Iteration 86/1000 | Loss: 0.00000817
Iteration 87/1000 | Loss: 0.00000817
Iteration 88/1000 | Loss: 0.00000816
Iteration 89/1000 | Loss: 0.00000816
Iteration 90/1000 | Loss: 0.00000816
Iteration 91/1000 | Loss: 0.00000815
Iteration 92/1000 | Loss: 0.00000815
Iteration 93/1000 | Loss: 0.00000815
Iteration 94/1000 | Loss: 0.00000815
Iteration 95/1000 | Loss: 0.00000815
Iteration 96/1000 | Loss: 0.00000815
Iteration 97/1000 | Loss: 0.00000814
Iteration 98/1000 | Loss: 0.00000814
Iteration 99/1000 | Loss: 0.00000814
Iteration 100/1000 | Loss: 0.00000814
Iteration 101/1000 | Loss: 0.00000813
Iteration 102/1000 | Loss: 0.00000813
Iteration 103/1000 | Loss: 0.00000813
Iteration 104/1000 | Loss: 0.00000813
Iteration 105/1000 | Loss: 0.00000813
Iteration 106/1000 | Loss: 0.00000813
Iteration 107/1000 | Loss: 0.00000812
Iteration 108/1000 | Loss: 0.00000812
Iteration 109/1000 | Loss: 0.00000812
Iteration 110/1000 | Loss: 0.00000812
Iteration 111/1000 | Loss: 0.00000812
Iteration 112/1000 | Loss: 0.00000812
Iteration 113/1000 | Loss: 0.00000812
Iteration 114/1000 | Loss: 0.00000811
Iteration 115/1000 | Loss: 0.00000811
Iteration 116/1000 | Loss: 0.00000811
Iteration 117/1000 | Loss: 0.00000811
Iteration 118/1000 | Loss: 0.00000811
Iteration 119/1000 | Loss: 0.00000811
Iteration 120/1000 | Loss: 0.00000811
Iteration 121/1000 | Loss: 0.00000811
Iteration 122/1000 | Loss: 0.00000811
Iteration 123/1000 | Loss: 0.00000810
Iteration 124/1000 | Loss: 0.00000810
Iteration 125/1000 | Loss: 0.00000810
Iteration 126/1000 | Loss: 0.00000810
Iteration 127/1000 | Loss: 0.00000810
Iteration 128/1000 | Loss: 0.00000810
Iteration 129/1000 | Loss: 0.00000810
Iteration 130/1000 | Loss: 0.00000810
Iteration 131/1000 | Loss: 0.00000809
Iteration 132/1000 | Loss: 0.00000809
Iteration 133/1000 | Loss: 0.00000809
Iteration 134/1000 | Loss: 0.00000809
Iteration 135/1000 | Loss: 0.00000808
Iteration 136/1000 | Loss: 0.00000808
Iteration 137/1000 | Loss: 0.00000808
Iteration 138/1000 | Loss: 0.00000808
Iteration 139/1000 | Loss: 0.00000808
Iteration 140/1000 | Loss: 0.00000808
Iteration 141/1000 | Loss: 0.00000808
Iteration 142/1000 | Loss: 0.00000808
Iteration 143/1000 | Loss: 0.00000808
Iteration 144/1000 | Loss: 0.00000808
Iteration 145/1000 | Loss: 0.00000807
Iteration 146/1000 | Loss: 0.00000807
Iteration 147/1000 | Loss: 0.00000807
Iteration 148/1000 | Loss: 0.00000807
Iteration 149/1000 | Loss: 0.00000806
Iteration 150/1000 | Loss: 0.00000806
Iteration 151/1000 | Loss: 0.00000806
Iteration 152/1000 | Loss: 0.00000806
Iteration 153/1000 | Loss: 0.00000806
Iteration 154/1000 | Loss: 0.00000806
Iteration 155/1000 | Loss: 0.00000806
Iteration 156/1000 | Loss: 0.00000806
Iteration 157/1000 | Loss: 0.00000806
Iteration 158/1000 | Loss: 0.00000806
Iteration 159/1000 | Loss: 0.00000806
Iteration 160/1000 | Loss: 0.00000806
Iteration 161/1000 | Loss: 0.00000806
Iteration 162/1000 | Loss: 0.00000806
Iteration 163/1000 | Loss: 0.00000806
Iteration 164/1000 | Loss: 0.00000806
Iteration 165/1000 | Loss: 0.00000806
Iteration 166/1000 | Loss: 0.00000806
Iteration 167/1000 | Loss: 0.00000806
Iteration 168/1000 | Loss: 0.00000806
Iteration 169/1000 | Loss: 0.00000806
Iteration 170/1000 | Loss: 0.00000805
Iteration 171/1000 | Loss: 0.00000805
Iteration 172/1000 | Loss: 0.00000805
Iteration 173/1000 | Loss: 0.00000805
Iteration 174/1000 | Loss: 0.00000805
Iteration 175/1000 | Loss: 0.00000805
Iteration 176/1000 | Loss: 0.00000805
Iteration 177/1000 | Loss: 0.00000805
Iteration 178/1000 | Loss: 0.00000805
Iteration 179/1000 | Loss: 0.00000805
Iteration 180/1000 | Loss: 0.00000805
Iteration 181/1000 | Loss: 0.00000805
Iteration 182/1000 | Loss: 0.00000805
Iteration 183/1000 | Loss: 0.00000805
Iteration 184/1000 | Loss: 0.00000804
Iteration 185/1000 | Loss: 0.00000804
Iteration 186/1000 | Loss: 0.00000804
Iteration 187/1000 | Loss: 0.00000804
Iteration 188/1000 | Loss: 0.00000804
Iteration 189/1000 | Loss: 0.00000804
Iteration 190/1000 | Loss: 0.00000804
Iteration 191/1000 | Loss: 0.00000804
Iteration 192/1000 | Loss: 0.00000804
Iteration 193/1000 | Loss: 0.00000804
Iteration 194/1000 | Loss: 0.00000803
Iteration 195/1000 | Loss: 0.00000803
Iteration 196/1000 | Loss: 0.00000803
Iteration 197/1000 | Loss: 0.00000803
Iteration 198/1000 | Loss: 0.00000803
Iteration 199/1000 | Loss: 0.00000803
Iteration 200/1000 | Loss: 0.00000803
Iteration 201/1000 | Loss: 0.00000803
Iteration 202/1000 | Loss: 0.00000803
Iteration 203/1000 | Loss: 0.00000803
Iteration 204/1000 | Loss: 0.00000803
Iteration 205/1000 | Loss: 0.00000803
Iteration 206/1000 | Loss: 0.00000802
Iteration 207/1000 | Loss: 0.00000802
Iteration 208/1000 | Loss: 0.00000802
Iteration 209/1000 | Loss: 0.00000802
Iteration 210/1000 | Loss: 0.00000802
Iteration 211/1000 | Loss: 0.00000802
Iteration 212/1000 | Loss: 0.00000802
Iteration 213/1000 | Loss: 0.00000802
Iteration 214/1000 | Loss: 0.00000802
Iteration 215/1000 | Loss: 0.00000801
Iteration 216/1000 | Loss: 0.00000801
Iteration 217/1000 | Loss: 0.00000801
Iteration 218/1000 | Loss: 0.00000801
Iteration 219/1000 | Loss: 0.00000801
Iteration 220/1000 | Loss: 0.00000801
Iteration 221/1000 | Loss: 0.00000801
Iteration 222/1000 | Loss: 0.00000801
Iteration 223/1000 | Loss: 0.00000801
Iteration 224/1000 | Loss: 0.00000801
Iteration 225/1000 | Loss: 0.00000801
Iteration 226/1000 | Loss: 0.00000801
Iteration 227/1000 | Loss: 0.00000801
Iteration 228/1000 | Loss: 0.00000801
Iteration 229/1000 | Loss: 0.00000801
Iteration 230/1000 | Loss: 0.00000801
Iteration 231/1000 | Loss: 0.00000801
Iteration 232/1000 | Loss: 0.00000800
Iteration 233/1000 | Loss: 0.00000800
Iteration 234/1000 | Loss: 0.00000800
Iteration 235/1000 | Loss: 0.00000800
Iteration 236/1000 | Loss: 0.00000800
Iteration 237/1000 | Loss: 0.00000800
Iteration 238/1000 | Loss: 0.00000800
Iteration 239/1000 | Loss: 0.00000800
Iteration 240/1000 | Loss: 0.00000800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [8.00371799414279e-06, 8.00371799414279e-06, 8.00371799414279e-06, 8.00371799414279e-06, 8.00371799414279e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.00371799414279e-06

Optimization complete. Final v2v error: 2.3986542224884033 mm

Highest mean error: 2.550976276397705 mm for frame 69

Lowest mean error: 2.298649787902832 mm for frame 160

Saving results

Total time: 36.477022647857666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00621646
Iteration 2/25 | Loss: 0.00122002
Iteration 3/25 | Loss: 0.00100896
Iteration 4/25 | Loss: 0.00097795
Iteration 5/25 | Loss: 0.00096914
Iteration 6/25 | Loss: 0.00096693
Iteration 7/25 | Loss: 0.00096680
Iteration 8/25 | Loss: 0.00096680
Iteration 9/25 | Loss: 0.00096680
Iteration 10/25 | Loss: 0.00096680
Iteration 11/25 | Loss: 0.00096680
Iteration 12/25 | Loss: 0.00096680
Iteration 13/25 | Loss: 0.00096680
Iteration 14/25 | Loss: 0.00096680
Iteration 15/25 | Loss: 0.00096680
Iteration 16/25 | Loss: 0.00096680
Iteration 17/25 | Loss: 0.00096680
Iteration 18/25 | Loss: 0.00096680
Iteration 19/25 | Loss: 0.00096680
Iteration 20/25 | Loss: 0.00096680
Iteration 21/25 | Loss: 0.00096680
Iteration 22/25 | Loss: 0.00096680
Iteration 23/25 | Loss: 0.00096680
Iteration 24/25 | Loss: 0.00096680
Iteration 25/25 | Loss: 0.00096680

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74078071
Iteration 2/25 | Loss: 0.00053652
Iteration 3/25 | Loss: 0.00053652
Iteration 4/25 | Loss: 0.00053652
Iteration 5/25 | Loss: 0.00053651
Iteration 6/25 | Loss: 0.00053651
Iteration 7/25 | Loss: 0.00053651
Iteration 8/25 | Loss: 0.00053651
Iteration 9/25 | Loss: 0.00053651
Iteration 10/25 | Loss: 0.00053651
Iteration 11/25 | Loss: 0.00053651
Iteration 12/25 | Loss: 0.00053651
Iteration 13/25 | Loss: 0.00053651
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005365147953853011, 0.0005365147953853011, 0.0005365147953853011, 0.0005365147953853011, 0.0005365147953853011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005365147953853011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053651
Iteration 2/1000 | Loss: 0.00002513
Iteration 3/1000 | Loss: 0.00001799
Iteration 4/1000 | Loss: 0.00001565
Iteration 5/1000 | Loss: 0.00001416
Iteration 6/1000 | Loss: 0.00001348
Iteration 7/1000 | Loss: 0.00001304
Iteration 8/1000 | Loss: 0.00001251
Iteration 9/1000 | Loss: 0.00001219
Iteration 10/1000 | Loss: 0.00001203
Iteration 11/1000 | Loss: 0.00001198
Iteration 12/1000 | Loss: 0.00001188
Iteration 13/1000 | Loss: 0.00001186
Iteration 14/1000 | Loss: 0.00001186
Iteration 15/1000 | Loss: 0.00001184
Iteration 16/1000 | Loss: 0.00001178
Iteration 17/1000 | Loss: 0.00001177
Iteration 18/1000 | Loss: 0.00001177
Iteration 19/1000 | Loss: 0.00001176
Iteration 20/1000 | Loss: 0.00001176
Iteration 21/1000 | Loss: 0.00001175
Iteration 22/1000 | Loss: 0.00001173
Iteration 23/1000 | Loss: 0.00001173
Iteration 24/1000 | Loss: 0.00001171
Iteration 25/1000 | Loss: 0.00001168
Iteration 26/1000 | Loss: 0.00001166
Iteration 27/1000 | Loss: 0.00001166
Iteration 28/1000 | Loss: 0.00001165
Iteration 29/1000 | Loss: 0.00001165
Iteration 30/1000 | Loss: 0.00001165
Iteration 31/1000 | Loss: 0.00001165
Iteration 32/1000 | Loss: 0.00001165
Iteration 33/1000 | Loss: 0.00001164
Iteration 34/1000 | Loss: 0.00001164
Iteration 35/1000 | Loss: 0.00001164
Iteration 36/1000 | Loss: 0.00001163
Iteration 37/1000 | Loss: 0.00001163
Iteration 38/1000 | Loss: 0.00001163
Iteration 39/1000 | Loss: 0.00001163
Iteration 40/1000 | Loss: 0.00001163
Iteration 41/1000 | Loss: 0.00001163
Iteration 42/1000 | Loss: 0.00001162
Iteration 43/1000 | Loss: 0.00001162
Iteration 44/1000 | Loss: 0.00001162
Iteration 45/1000 | Loss: 0.00001161
Iteration 46/1000 | Loss: 0.00001161
Iteration 47/1000 | Loss: 0.00001161
Iteration 48/1000 | Loss: 0.00001161
Iteration 49/1000 | Loss: 0.00001161
Iteration 50/1000 | Loss: 0.00001160
Iteration 51/1000 | Loss: 0.00001160
Iteration 52/1000 | Loss: 0.00001160
Iteration 53/1000 | Loss: 0.00001160
Iteration 54/1000 | Loss: 0.00001160
Iteration 55/1000 | Loss: 0.00001160
Iteration 56/1000 | Loss: 0.00001160
Iteration 57/1000 | Loss: 0.00001160
Iteration 58/1000 | Loss: 0.00001160
Iteration 59/1000 | Loss: 0.00001160
Iteration 60/1000 | Loss: 0.00001160
Iteration 61/1000 | Loss: 0.00001160
Iteration 62/1000 | Loss: 0.00001159
Iteration 63/1000 | Loss: 0.00001159
Iteration 64/1000 | Loss: 0.00001159
Iteration 65/1000 | Loss: 0.00001158
Iteration 66/1000 | Loss: 0.00001158
Iteration 67/1000 | Loss: 0.00001158
Iteration 68/1000 | Loss: 0.00001158
Iteration 69/1000 | Loss: 0.00001158
Iteration 70/1000 | Loss: 0.00001157
Iteration 71/1000 | Loss: 0.00001157
Iteration 72/1000 | Loss: 0.00001157
Iteration 73/1000 | Loss: 0.00001157
Iteration 74/1000 | Loss: 0.00001157
Iteration 75/1000 | Loss: 0.00001157
Iteration 76/1000 | Loss: 0.00001157
Iteration 77/1000 | Loss: 0.00001157
Iteration 78/1000 | Loss: 0.00001157
Iteration 79/1000 | Loss: 0.00001157
Iteration 80/1000 | Loss: 0.00001157
Iteration 81/1000 | Loss: 0.00001157
Iteration 82/1000 | Loss: 0.00001157
Iteration 83/1000 | Loss: 0.00001157
Iteration 84/1000 | Loss: 0.00001157
Iteration 85/1000 | Loss: 0.00001157
Iteration 86/1000 | Loss: 0.00001157
Iteration 87/1000 | Loss: 0.00001157
Iteration 88/1000 | Loss: 0.00001157
Iteration 89/1000 | Loss: 0.00001157
Iteration 90/1000 | Loss: 0.00001157
Iteration 91/1000 | Loss: 0.00001157
Iteration 92/1000 | Loss: 0.00001157
Iteration 93/1000 | Loss: 0.00001157
Iteration 94/1000 | Loss: 0.00001157
Iteration 95/1000 | Loss: 0.00001157
Iteration 96/1000 | Loss: 0.00001157
Iteration 97/1000 | Loss: 0.00001157
Iteration 98/1000 | Loss: 0.00001157
Iteration 99/1000 | Loss: 0.00001157
Iteration 100/1000 | Loss: 0.00001157
Iteration 101/1000 | Loss: 0.00001157
Iteration 102/1000 | Loss: 0.00001157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.1566317880351562e-05, 1.1566317880351562e-05, 1.1566317880351562e-05, 1.1566317880351562e-05, 1.1566317880351562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1566317880351562e-05

Optimization complete. Final v2v error: 2.840536117553711 mm

Highest mean error: 3.1006007194519043 mm for frame 189

Lowest mean error: 2.6222898960113525 mm for frame 136

Saving results

Total time: 35.620023012161255
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055337
Iteration 2/25 | Loss: 0.00186395
Iteration 3/25 | Loss: 0.00124230
Iteration 4/25 | Loss: 0.00121026
Iteration 5/25 | Loss: 0.00120103
Iteration 6/25 | Loss: 0.00119757
Iteration 7/25 | Loss: 0.00119719
Iteration 8/25 | Loss: 0.00119719
Iteration 9/25 | Loss: 0.00119719
Iteration 10/25 | Loss: 0.00119719
Iteration 11/25 | Loss: 0.00119719
Iteration 12/25 | Loss: 0.00119719
Iteration 13/25 | Loss: 0.00119719
Iteration 14/25 | Loss: 0.00119719
Iteration 15/25 | Loss: 0.00119719
Iteration 16/25 | Loss: 0.00119719
Iteration 17/25 | Loss: 0.00119719
Iteration 18/25 | Loss: 0.00119719
Iteration 19/25 | Loss: 0.00119719
Iteration 20/25 | Loss: 0.00119719
Iteration 21/25 | Loss: 0.00119719
Iteration 22/25 | Loss: 0.00119719
Iteration 23/25 | Loss: 0.00119719
Iteration 24/25 | Loss: 0.00119719
Iteration 25/25 | Loss: 0.00119719

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53974319
Iteration 2/25 | Loss: 0.00073894
Iteration 3/25 | Loss: 0.00073893
Iteration 4/25 | Loss: 0.00073893
Iteration 5/25 | Loss: 0.00073893
Iteration 6/25 | Loss: 0.00073893
Iteration 7/25 | Loss: 0.00073893
Iteration 8/25 | Loss: 0.00073893
Iteration 9/25 | Loss: 0.00073893
Iteration 10/25 | Loss: 0.00073893
Iteration 11/25 | Loss: 0.00073893
Iteration 12/25 | Loss: 0.00073893
Iteration 13/25 | Loss: 0.00073893
Iteration 14/25 | Loss: 0.00073893
Iteration 15/25 | Loss: 0.00073893
Iteration 16/25 | Loss: 0.00073893
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007389289676211774, 0.0007389289676211774, 0.0007389289676211774, 0.0007389289676211774, 0.0007389289676211774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007389289676211774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073893
Iteration 2/1000 | Loss: 0.00007974
Iteration 3/1000 | Loss: 0.00005900
Iteration 4/1000 | Loss: 0.00005099
Iteration 5/1000 | Loss: 0.00004760
Iteration 6/1000 | Loss: 0.00004616
Iteration 7/1000 | Loss: 0.00004514
Iteration 8/1000 | Loss: 0.00004433
Iteration 9/1000 | Loss: 0.00004382
Iteration 10/1000 | Loss: 0.00004334
Iteration 11/1000 | Loss: 0.00004287
Iteration 12/1000 | Loss: 0.00004247
Iteration 13/1000 | Loss: 0.00004217
Iteration 14/1000 | Loss: 0.00004190
Iteration 15/1000 | Loss: 0.00004163
Iteration 16/1000 | Loss: 0.00004137
Iteration 17/1000 | Loss: 0.00004111
Iteration 18/1000 | Loss: 0.00004095
Iteration 19/1000 | Loss: 0.00004081
Iteration 20/1000 | Loss: 0.00004062
Iteration 21/1000 | Loss: 0.00004046
Iteration 22/1000 | Loss: 0.00004046
Iteration 23/1000 | Loss: 0.00004032
Iteration 24/1000 | Loss: 0.00004024
Iteration 25/1000 | Loss: 0.00004024
Iteration 26/1000 | Loss: 0.00004019
Iteration 27/1000 | Loss: 0.00004017
Iteration 28/1000 | Loss: 0.00004015
Iteration 29/1000 | Loss: 0.00004015
Iteration 30/1000 | Loss: 0.00004015
Iteration 31/1000 | Loss: 0.00004015
Iteration 32/1000 | Loss: 0.00004015
Iteration 33/1000 | Loss: 0.00004015
Iteration 34/1000 | Loss: 0.00004014
Iteration 35/1000 | Loss: 0.00004014
Iteration 36/1000 | Loss: 0.00004014
Iteration 37/1000 | Loss: 0.00004013
Iteration 38/1000 | Loss: 0.00004012
Iteration 39/1000 | Loss: 0.00004011
Iteration 40/1000 | Loss: 0.00004011
Iteration 41/1000 | Loss: 0.00004011
Iteration 42/1000 | Loss: 0.00004011
Iteration 43/1000 | Loss: 0.00004011
Iteration 44/1000 | Loss: 0.00004011
Iteration 45/1000 | Loss: 0.00004011
Iteration 46/1000 | Loss: 0.00004011
Iteration 47/1000 | Loss: 0.00004011
Iteration 48/1000 | Loss: 0.00004010
Iteration 49/1000 | Loss: 0.00004010
Iteration 50/1000 | Loss: 0.00004010
Iteration 51/1000 | Loss: 0.00004009
Iteration 52/1000 | Loss: 0.00004009
Iteration 53/1000 | Loss: 0.00004008
Iteration 54/1000 | Loss: 0.00004008
Iteration 55/1000 | Loss: 0.00004008
Iteration 56/1000 | Loss: 0.00004007
Iteration 57/1000 | Loss: 0.00004007
Iteration 58/1000 | Loss: 0.00004007
Iteration 59/1000 | Loss: 0.00004007
Iteration 60/1000 | Loss: 0.00004007
Iteration 61/1000 | Loss: 0.00004007
Iteration 62/1000 | Loss: 0.00004007
Iteration 63/1000 | Loss: 0.00004007
Iteration 64/1000 | Loss: 0.00004007
Iteration 65/1000 | Loss: 0.00004006
Iteration 66/1000 | Loss: 0.00004006
Iteration 67/1000 | Loss: 0.00004006
Iteration 68/1000 | Loss: 0.00004005
Iteration 69/1000 | Loss: 0.00004005
Iteration 70/1000 | Loss: 0.00004005
Iteration 71/1000 | Loss: 0.00004004
Iteration 72/1000 | Loss: 0.00004004
Iteration 73/1000 | Loss: 0.00004004
Iteration 74/1000 | Loss: 0.00004004
Iteration 75/1000 | Loss: 0.00004004
Iteration 76/1000 | Loss: 0.00004003
Iteration 77/1000 | Loss: 0.00004003
Iteration 78/1000 | Loss: 0.00004003
Iteration 79/1000 | Loss: 0.00004003
Iteration 80/1000 | Loss: 0.00004002
Iteration 81/1000 | Loss: 0.00004002
Iteration 82/1000 | Loss: 0.00004002
Iteration 83/1000 | Loss: 0.00004002
Iteration 84/1000 | Loss: 0.00004002
Iteration 85/1000 | Loss: 0.00004002
Iteration 86/1000 | Loss: 0.00004002
Iteration 87/1000 | Loss: 0.00004002
Iteration 88/1000 | Loss: 0.00004002
Iteration 89/1000 | Loss: 0.00004002
Iteration 90/1000 | Loss: 0.00004002
Iteration 91/1000 | Loss: 0.00004002
Iteration 92/1000 | Loss: 0.00004001
Iteration 93/1000 | Loss: 0.00004001
Iteration 94/1000 | Loss: 0.00004001
Iteration 95/1000 | Loss: 0.00004001
Iteration 96/1000 | Loss: 0.00004001
Iteration 97/1000 | Loss: 0.00004001
Iteration 98/1000 | Loss: 0.00004001
Iteration 99/1000 | Loss: 0.00004001
Iteration 100/1000 | Loss: 0.00004000
Iteration 101/1000 | Loss: 0.00004000
Iteration 102/1000 | Loss: 0.00004000
Iteration 103/1000 | Loss: 0.00004000
Iteration 104/1000 | Loss: 0.00004000
Iteration 105/1000 | Loss: 0.00004000
Iteration 106/1000 | Loss: 0.00004000
Iteration 107/1000 | Loss: 0.00003999
Iteration 108/1000 | Loss: 0.00003999
Iteration 109/1000 | Loss: 0.00003999
Iteration 110/1000 | Loss: 0.00003999
Iteration 111/1000 | Loss: 0.00003999
Iteration 112/1000 | Loss: 0.00003998
Iteration 113/1000 | Loss: 0.00003998
Iteration 114/1000 | Loss: 0.00003998
Iteration 115/1000 | Loss: 0.00003998
Iteration 116/1000 | Loss: 0.00003998
Iteration 117/1000 | Loss: 0.00003998
Iteration 118/1000 | Loss: 0.00003997
Iteration 119/1000 | Loss: 0.00003997
Iteration 120/1000 | Loss: 0.00003997
Iteration 121/1000 | Loss: 0.00003997
Iteration 122/1000 | Loss: 0.00003997
Iteration 123/1000 | Loss: 0.00003997
Iteration 124/1000 | Loss: 0.00003997
Iteration 125/1000 | Loss: 0.00003996
Iteration 126/1000 | Loss: 0.00003996
Iteration 127/1000 | Loss: 0.00003996
Iteration 128/1000 | Loss: 0.00003996
Iteration 129/1000 | Loss: 0.00003995
Iteration 130/1000 | Loss: 0.00003995
Iteration 131/1000 | Loss: 0.00003995
Iteration 132/1000 | Loss: 0.00003994
Iteration 133/1000 | Loss: 0.00003994
Iteration 134/1000 | Loss: 0.00003994
Iteration 135/1000 | Loss: 0.00003994
Iteration 136/1000 | Loss: 0.00003994
Iteration 137/1000 | Loss: 0.00003994
Iteration 138/1000 | Loss: 0.00003994
Iteration 139/1000 | Loss: 0.00003994
Iteration 140/1000 | Loss: 0.00003994
Iteration 141/1000 | Loss: 0.00003994
Iteration 142/1000 | Loss: 0.00003994
Iteration 143/1000 | Loss: 0.00003993
Iteration 144/1000 | Loss: 0.00003993
Iteration 145/1000 | Loss: 0.00003993
Iteration 146/1000 | Loss: 0.00003993
Iteration 147/1000 | Loss: 0.00003993
Iteration 148/1000 | Loss: 0.00003993
Iteration 149/1000 | Loss: 0.00003993
Iteration 150/1000 | Loss: 0.00003993
Iteration 151/1000 | Loss: 0.00003993
Iteration 152/1000 | Loss: 0.00003993
Iteration 153/1000 | Loss: 0.00003993
Iteration 154/1000 | Loss: 0.00003993
Iteration 155/1000 | Loss: 0.00003993
Iteration 156/1000 | Loss: 0.00003993
Iteration 157/1000 | Loss: 0.00003993
Iteration 158/1000 | Loss: 0.00003993
Iteration 159/1000 | Loss: 0.00003993
Iteration 160/1000 | Loss: 0.00003993
Iteration 161/1000 | Loss: 0.00003993
Iteration 162/1000 | Loss: 0.00003993
Iteration 163/1000 | Loss: 0.00003993
Iteration 164/1000 | Loss: 0.00003993
Iteration 165/1000 | Loss: 0.00003993
Iteration 166/1000 | Loss: 0.00003993
Iteration 167/1000 | Loss: 0.00003993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [3.992648998973891e-05, 3.992648998973891e-05, 3.992648998973891e-05, 3.992648998973891e-05, 3.992648998973891e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.992648998973891e-05

Optimization complete. Final v2v error: 4.944578647613525 mm

Highest mean error: 6.08245325088501 mm for frame 40

Lowest mean error: 4.306161880493164 mm for frame 62

Saving results

Total time: 53.533952474594116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820818
Iteration 2/25 | Loss: 0.00130821
Iteration 3/25 | Loss: 0.00099768
Iteration 4/25 | Loss: 0.00096860
Iteration 5/25 | Loss: 0.00096426
Iteration 6/25 | Loss: 0.00096355
Iteration 7/25 | Loss: 0.00096355
Iteration 8/25 | Loss: 0.00096355
Iteration 9/25 | Loss: 0.00096355
Iteration 10/25 | Loss: 0.00096355
Iteration 11/25 | Loss: 0.00096355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000963545695412904, 0.000963545695412904, 0.000963545695412904, 0.000963545695412904, 0.000963545695412904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000963545695412904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32747638
Iteration 2/25 | Loss: 0.00062444
Iteration 3/25 | Loss: 0.00062443
Iteration 4/25 | Loss: 0.00062443
Iteration 5/25 | Loss: 0.00062443
Iteration 6/25 | Loss: 0.00062443
Iteration 7/25 | Loss: 0.00062443
Iteration 8/25 | Loss: 0.00062443
Iteration 9/25 | Loss: 0.00062443
Iteration 10/25 | Loss: 0.00062443
Iteration 11/25 | Loss: 0.00062443
Iteration 12/25 | Loss: 0.00062443
Iteration 13/25 | Loss: 0.00062443
Iteration 14/25 | Loss: 0.00062443
Iteration 15/25 | Loss: 0.00062443
Iteration 16/25 | Loss: 0.00062443
Iteration 17/25 | Loss: 0.00062443
Iteration 18/25 | Loss: 0.00062443
Iteration 19/25 | Loss: 0.00062443
Iteration 20/25 | Loss: 0.00062443
Iteration 21/25 | Loss: 0.00062443
Iteration 22/25 | Loss: 0.00062443
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006244315300136805, 0.0006244315300136805, 0.0006244315300136805, 0.0006244315300136805, 0.0006244315300136805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006244315300136805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062443
Iteration 2/1000 | Loss: 0.00002674
Iteration 3/1000 | Loss: 0.00001507
Iteration 4/1000 | Loss: 0.00001068
Iteration 5/1000 | Loss: 0.00000991
Iteration 6/1000 | Loss: 0.00000941
Iteration 7/1000 | Loss: 0.00000912
Iteration 8/1000 | Loss: 0.00000910
Iteration 9/1000 | Loss: 0.00000886
Iteration 10/1000 | Loss: 0.00000866
Iteration 11/1000 | Loss: 0.00000857
Iteration 12/1000 | Loss: 0.00000853
Iteration 13/1000 | Loss: 0.00000852
Iteration 14/1000 | Loss: 0.00000851
Iteration 15/1000 | Loss: 0.00000850
Iteration 16/1000 | Loss: 0.00000850
Iteration 17/1000 | Loss: 0.00000849
Iteration 18/1000 | Loss: 0.00000849
Iteration 19/1000 | Loss: 0.00000848
Iteration 20/1000 | Loss: 0.00000848
Iteration 21/1000 | Loss: 0.00000847
Iteration 22/1000 | Loss: 0.00000847
Iteration 23/1000 | Loss: 0.00000846
Iteration 24/1000 | Loss: 0.00000846
Iteration 25/1000 | Loss: 0.00000845
Iteration 26/1000 | Loss: 0.00000844
Iteration 27/1000 | Loss: 0.00000843
Iteration 28/1000 | Loss: 0.00000843
Iteration 29/1000 | Loss: 0.00000842
Iteration 30/1000 | Loss: 0.00000841
Iteration 31/1000 | Loss: 0.00000839
Iteration 32/1000 | Loss: 0.00000839
Iteration 33/1000 | Loss: 0.00000839
Iteration 34/1000 | Loss: 0.00000838
Iteration 35/1000 | Loss: 0.00000838
Iteration 36/1000 | Loss: 0.00000837
Iteration 37/1000 | Loss: 0.00000837
Iteration 38/1000 | Loss: 0.00000837
Iteration 39/1000 | Loss: 0.00000837
Iteration 40/1000 | Loss: 0.00000837
Iteration 41/1000 | Loss: 0.00000837
Iteration 42/1000 | Loss: 0.00000837
Iteration 43/1000 | Loss: 0.00000835
Iteration 44/1000 | Loss: 0.00000835
Iteration 45/1000 | Loss: 0.00000834
Iteration 46/1000 | Loss: 0.00000834
Iteration 47/1000 | Loss: 0.00000833
Iteration 48/1000 | Loss: 0.00000833
Iteration 49/1000 | Loss: 0.00000833
Iteration 50/1000 | Loss: 0.00000833
Iteration 51/1000 | Loss: 0.00000833
Iteration 52/1000 | Loss: 0.00000833
Iteration 53/1000 | Loss: 0.00000832
Iteration 54/1000 | Loss: 0.00000832
Iteration 55/1000 | Loss: 0.00000831
Iteration 56/1000 | Loss: 0.00000831
Iteration 57/1000 | Loss: 0.00000831
Iteration 58/1000 | Loss: 0.00000831
Iteration 59/1000 | Loss: 0.00000831
Iteration 60/1000 | Loss: 0.00000830
Iteration 61/1000 | Loss: 0.00000830
Iteration 62/1000 | Loss: 0.00000830
Iteration 63/1000 | Loss: 0.00000830
Iteration 64/1000 | Loss: 0.00000830
Iteration 65/1000 | Loss: 0.00000829
Iteration 66/1000 | Loss: 0.00000829
Iteration 67/1000 | Loss: 0.00000829
Iteration 68/1000 | Loss: 0.00000829
Iteration 69/1000 | Loss: 0.00000829
Iteration 70/1000 | Loss: 0.00000829
Iteration 71/1000 | Loss: 0.00000828
Iteration 72/1000 | Loss: 0.00000828
Iteration 73/1000 | Loss: 0.00000828
Iteration 74/1000 | Loss: 0.00000828
Iteration 75/1000 | Loss: 0.00000827
Iteration 76/1000 | Loss: 0.00000827
Iteration 77/1000 | Loss: 0.00000827
Iteration 78/1000 | Loss: 0.00000827
Iteration 79/1000 | Loss: 0.00000826
Iteration 80/1000 | Loss: 0.00000826
Iteration 81/1000 | Loss: 0.00000825
Iteration 82/1000 | Loss: 0.00000825
Iteration 83/1000 | Loss: 0.00000825
Iteration 84/1000 | Loss: 0.00000824
Iteration 85/1000 | Loss: 0.00000824
Iteration 86/1000 | Loss: 0.00000823
Iteration 87/1000 | Loss: 0.00000823
Iteration 88/1000 | Loss: 0.00000823
Iteration 89/1000 | Loss: 0.00000823
Iteration 90/1000 | Loss: 0.00000822
Iteration 91/1000 | Loss: 0.00000822
Iteration 92/1000 | Loss: 0.00000822
Iteration 93/1000 | Loss: 0.00000822
Iteration 94/1000 | Loss: 0.00000822
Iteration 95/1000 | Loss: 0.00000822
Iteration 96/1000 | Loss: 0.00000821
Iteration 97/1000 | Loss: 0.00000821
Iteration 98/1000 | Loss: 0.00000821
Iteration 99/1000 | Loss: 0.00000821
Iteration 100/1000 | Loss: 0.00000821
Iteration 101/1000 | Loss: 0.00000821
Iteration 102/1000 | Loss: 0.00000820
Iteration 103/1000 | Loss: 0.00000820
Iteration 104/1000 | Loss: 0.00000820
Iteration 105/1000 | Loss: 0.00000820
Iteration 106/1000 | Loss: 0.00000820
Iteration 107/1000 | Loss: 0.00000820
Iteration 108/1000 | Loss: 0.00000819
Iteration 109/1000 | Loss: 0.00000819
Iteration 110/1000 | Loss: 0.00000819
Iteration 111/1000 | Loss: 0.00000819
Iteration 112/1000 | Loss: 0.00000819
Iteration 113/1000 | Loss: 0.00000819
Iteration 114/1000 | Loss: 0.00000818
Iteration 115/1000 | Loss: 0.00000818
Iteration 116/1000 | Loss: 0.00000818
Iteration 117/1000 | Loss: 0.00000818
Iteration 118/1000 | Loss: 0.00000818
Iteration 119/1000 | Loss: 0.00000818
Iteration 120/1000 | Loss: 0.00000818
Iteration 121/1000 | Loss: 0.00000818
Iteration 122/1000 | Loss: 0.00000818
Iteration 123/1000 | Loss: 0.00000818
Iteration 124/1000 | Loss: 0.00000817
Iteration 125/1000 | Loss: 0.00000817
Iteration 126/1000 | Loss: 0.00000817
Iteration 127/1000 | Loss: 0.00000817
Iteration 128/1000 | Loss: 0.00000817
Iteration 129/1000 | Loss: 0.00000817
Iteration 130/1000 | Loss: 0.00000817
Iteration 131/1000 | Loss: 0.00000817
Iteration 132/1000 | Loss: 0.00000817
Iteration 133/1000 | Loss: 0.00000817
Iteration 134/1000 | Loss: 0.00000817
Iteration 135/1000 | Loss: 0.00000817
Iteration 136/1000 | Loss: 0.00000817
Iteration 137/1000 | Loss: 0.00000817
Iteration 138/1000 | Loss: 0.00000816
Iteration 139/1000 | Loss: 0.00000816
Iteration 140/1000 | Loss: 0.00000816
Iteration 141/1000 | Loss: 0.00000816
Iteration 142/1000 | Loss: 0.00000816
Iteration 143/1000 | Loss: 0.00000816
Iteration 144/1000 | Loss: 0.00000816
Iteration 145/1000 | Loss: 0.00000816
Iteration 146/1000 | Loss: 0.00000816
Iteration 147/1000 | Loss: 0.00000816
Iteration 148/1000 | Loss: 0.00000816
Iteration 149/1000 | Loss: 0.00000816
Iteration 150/1000 | Loss: 0.00000816
Iteration 151/1000 | Loss: 0.00000816
Iteration 152/1000 | Loss: 0.00000816
Iteration 153/1000 | Loss: 0.00000816
Iteration 154/1000 | Loss: 0.00000816
Iteration 155/1000 | Loss: 0.00000815
Iteration 156/1000 | Loss: 0.00000815
Iteration 157/1000 | Loss: 0.00000815
Iteration 158/1000 | Loss: 0.00000815
Iteration 159/1000 | Loss: 0.00000815
Iteration 160/1000 | Loss: 0.00000815
Iteration 161/1000 | Loss: 0.00000815
Iteration 162/1000 | Loss: 0.00000815
Iteration 163/1000 | Loss: 0.00000815
Iteration 164/1000 | Loss: 0.00000815
Iteration 165/1000 | Loss: 0.00000815
Iteration 166/1000 | Loss: 0.00000815
Iteration 167/1000 | Loss: 0.00000815
Iteration 168/1000 | Loss: 0.00000815
Iteration 169/1000 | Loss: 0.00000815
Iteration 170/1000 | Loss: 0.00000814
Iteration 171/1000 | Loss: 0.00000814
Iteration 172/1000 | Loss: 0.00000814
Iteration 173/1000 | Loss: 0.00000814
Iteration 174/1000 | Loss: 0.00000814
Iteration 175/1000 | Loss: 0.00000814
Iteration 176/1000 | Loss: 0.00000814
Iteration 177/1000 | Loss: 0.00000814
Iteration 178/1000 | Loss: 0.00000814
Iteration 179/1000 | Loss: 0.00000814
Iteration 180/1000 | Loss: 0.00000814
Iteration 181/1000 | Loss: 0.00000814
Iteration 182/1000 | Loss: 0.00000814
Iteration 183/1000 | Loss: 0.00000814
Iteration 184/1000 | Loss: 0.00000814
Iteration 185/1000 | Loss: 0.00000814
Iteration 186/1000 | Loss: 0.00000814
Iteration 187/1000 | Loss: 0.00000814
Iteration 188/1000 | Loss: 0.00000814
Iteration 189/1000 | Loss: 0.00000814
Iteration 190/1000 | Loss: 0.00000814
Iteration 191/1000 | Loss: 0.00000814
Iteration 192/1000 | Loss: 0.00000814
Iteration 193/1000 | Loss: 0.00000814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [8.13725000625709e-06, 8.13725000625709e-06, 8.13725000625709e-06, 8.13725000625709e-06, 8.13725000625709e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.13725000625709e-06

Optimization complete. Final v2v error: 2.4375553131103516 mm

Highest mean error: 2.7628490924835205 mm for frame 98

Lowest mean error: 2.1175787448883057 mm for frame 187

Saving results

Total time: 39.40018653869629
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459961
Iteration 2/25 | Loss: 0.00112465
Iteration 3/25 | Loss: 0.00099858
Iteration 4/25 | Loss: 0.00097342
Iteration 5/25 | Loss: 0.00096510
Iteration 6/25 | Loss: 0.00096320
Iteration 7/25 | Loss: 0.00096301
Iteration 8/25 | Loss: 0.00096301
Iteration 9/25 | Loss: 0.00096301
Iteration 10/25 | Loss: 0.00096301
Iteration 11/25 | Loss: 0.00096301
Iteration 12/25 | Loss: 0.00096301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009630066924728453, 0.0009630066924728453, 0.0009630066924728453, 0.0009630066924728453, 0.0009630066924728453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009630066924728453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32630706
Iteration 2/25 | Loss: 0.00058646
Iteration 3/25 | Loss: 0.00058645
Iteration 4/25 | Loss: 0.00058645
Iteration 5/25 | Loss: 0.00058645
Iteration 6/25 | Loss: 0.00058645
Iteration 7/25 | Loss: 0.00058645
Iteration 8/25 | Loss: 0.00058645
Iteration 9/25 | Loss: 0.00058645
Iteration 10/25 | Loss: 0.00058645
Iteration 11/25 | Loss: 0.00058645
Iteration 12/25 | Loss: 0.00058645
Iteration 13/25 | Loss: 0.00058645
Iteration 14/25 | Loss: 0.00058645
Iteration 15/25 | Loss: 0.00058645
Iteration 16/25 | Loss: 0.00058645
Iteration 17/25 | Loss: 0.00058645
Iteration 18/25 | Loss: 0.00058645
Iteration 19/25 | Loss: 0.00058645
Iteration 20/25 | Loss: 0.00058645
Iteration 21/25 | Loss: 0.00058645
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005864478880539536, 0.0005864478880539536, 0.0005864478880539536, 0.0005864478880539536, 0.0005864478880539536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005864478880539536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058645
Iteration 2/1000 | Loss: 0.00002411
Iteration 3/1000 | Loss: 0.00001927
Iteration 4/1000 | Loss: 0.00001765
Iteration 5/1000 | Loss: 0.00001665
Iteration 6/1000 | Loss: 0.00001572
Iteration 7/1000 | Loss: 0.00001535
Iteration 8/1000 | Loss: 0.00001494
Iteration 9/1000 | Loss: 0.00001482
Iteration 10/1000 | Loss: 0.00001477
Iteration 11/1000 | Loss: 0.00001476
Iteration 12/1000 | Loss: 0.00001475
Iteration 13/1000 | Loss: 0.00001475
Iteration 14/1000 | Loss: 0.00001474
Iteration 15/1000 | Loss: 0.00001473
Iteration 16/1000 | Loss: 0.00001472
Iteration 17/1000 | Loss: 0.00001471
Iteration 18/1000 | Loss: 0.00001470
Iteration 19/1000 | Loss: 0.00001467
Iteration 20/1000 | Loss: 0.00001465
Iteration 21/1000 | Loss: 0.00001464
Iteration 22/1000 | Loss: 0.00001464
Iteration 23/1000 | Loss: 0.00001463
Iteration 24/1000 | Loss: 0.00001463
Iteration 25/1000 | Loss: 0.00001462
Iteration 26/1000 | Loss: 0.00001462
Iteration 27/1000 | Loss: 0.00001462
Iteration 28/1000 | Loss: 0.00001462
Iteration 29/1000 | Loss: 0.00001462
Iteration 30/1000 | Loss: 0.00001461
Iteration 31/1000 | Loss: 0.00001461
Iteration 32/1000 | Loss: 0.00001460
Iteration 33/1000 | Loss: 0.00001460
Iteration 34/1000 | Loss: 0.00001459
Iteration 35/1000 | Loss: 0.00001459
Iteration 36/1000 | Loss: 0.00001458
Iteration 37/1000 | Loss: 0.00001458
Iteration 38/1000 | Loss: 0.00001458
Iteration 39/1000 | Loss: 0.00001458
Iteration 40/1000 | Loss: 0.00001457
Iteration 41/1000 | Loss: 0.00001457
Iteration 42/1000 | Loss: 0.00001456
Iteration 43/1000 | Loss: 0.00001456
Iteration 44/1000 | Loss: 0.00001456
Iteration 45/1000 | Loss: 0.00001456
Iteration 46/1000 | Loss: 0.00001456
Iteration 47/1000 | Loss: 0.00001456
Iteration 48/1000 | Loss: 0.00001456
Iteration 49/1000 | Loss: 0.00001456
Iteration 50/1000 | Loss: 0.00001456
Iteration 51/1000 | Loss: 0.00001456
Iteration 52/1000 | Loss: 0.00001455
Iteration 53/1000 | Loss: 0.00001455
Iteration 54/1000 | Loss: 0.00001454
Iteration 55/1000 | Loss: 0.00001454
Iteration 56/1000 | Loss: 0.00001454
Iteration 57/1000 | Loss: 0.00001454
Iteration 58/1000 | Loss: 0.00001454
Iteration 59/1000 | Loss: 0.00001454
Iteration 60/1000 | Loss: 0.00001454
Iteration 61/1000 | Loss: 0.00001453
Iteration 62/1000 | Loss: 0.00001453
Iteration 63/1000 | Loss: 0.00001453
Iteration 64/1000 | Loss: 0.00001453
Iteration 65/1000 | Loss: 0.00001453
Iteration 66/1000 | Loss: 0.00001453
Iteration 67/1000 | Loss: 0.00001453
Iteration 68/1000 | Loss: 0.00001453
Iteration 69/1000 | Loss: 0.00001452
Iteration 70/1000 | Loss: 0.00001452
Iteration 71/1000 | Loss: 0.00001452
Iteration 72/1000 | Loss: 0.00001452
Iteration 73/1000 | Loss: 0.00001452
Iteration 74/1000 | Loss: 0.00001452
Iteration 75/1000 | Loss: 0.00001452
Iteration 76/1000 | Loss: 0.00001452
Iteration 77/1000 | Loss: 0.00001452
Iteration 78/1000 | Loss: 0.00001452
Iteration 79/1000 | Loss: 0.00001452
Iteration 80/1000 | Loss: 0.00001452
Iteration 81/1000 | Loss: 0.00001452
Iteration 82/1000 | Loss: 0.00001452
Iteration 83/1000 | Loss: 0.00001452
Iteration 84/1000 | Loss: 0.00001452
Iteration 85/1000 | Loss: 0.00001452
Iteration 86/1000 | Loss: 0.00001452
Iteration 87/1000 | Loss: 0.00001451
Iteration 88/1000 | Loss: 0.00001451
Iteration 89/1000 | Loss: 0.00001451
Iteration 90/1000 | Loss: 0.00001451
Iteration 91/1000 | Loss: 0.00001451
Iteration 92/1000 | Loss: 0.00001451
Iteration 93/1000 | Loss: 0.00001451
Iteration 94/1000 | Loss: 0.00001451
Iteration 95/1000 | Loss: 0.00001451
Iteration 96/1000 | Loss: 0.00001451
Iteration 97/1000 | Loss: 0.00001451
Iteration 98/1000 | Loss: 0.00001451
Iteration 99/1000 | Loss: 0.00001451
Iteration 100/1000 | Loss: 0.00001451
Iteration 101/1000 | Loss: 0.00001450
Iteration 102/1000 | Loss: 0.00001450
Iteration 103/1000 | Loss: 0.00001450
Iteration 104/1000 | Loss: 0.00001450
Iteration 105/1000 | Loss: 0.00001450
Iteration 106/1000 | Loss: 0.00001450
Iteration 107/1000 | Loss: 0.00001450
Iteration 108/1000 | Loss: 0.00001450
Iteration 109/1000 | Loss: 0.00001450
Iteration 110/1000 | Loss: 0.00001450
Iteration 111/1000 | Loss: 0.00001450
Iteration 112/1000 | Loss: 0.00001450
Iteration 113/1000 | Loss: 0.00001450
Iteration 114/1000 | Loss: 0.00001450
Iteration 115/1000 | Loss: 0.00001450
Iteration 116/1000 | Loss: 0.00001450
Iteration 117/1000 | Loss: 0.00001450
Iteration 118/1000 | Loss: 0.00001450
Iteration 119/1000 | Loss: 0.00001449
Iteration 120/1000 | Loss: 0.00001449
Iteration 121/1000 | Loss: 0.00001449
Iteration 122/1000 | Loss: 0.00001449
Iteration 123/1000 | Loss: 0.00001449
Iteration 124/1000 | Loss: 0.00001449
Iteration 125/1000 | Loss: 0.00001449
Iteration 126/1000 | Loss: 0.00001449
Iteration 127/1000 | Loss: 0.00001449
Iteration 128/1000 | Loss: 0.00001449
Iteration 129/1000 | Loss: 0.00001449
Iteration 130/1000 | Loss: 0.00001449
Iteration 131/1000 | Loss: 0.00001449
Iteration 132/1000 | Loss: 0.00001449
Iteration 133/1000 | Loss: 0.00001449
Iteration 134/1000 | Loss: 0.00001449
Iteration 135/1000 | Loss: 0.00001449
Iteration 136/1000 | Loss: 0.00001449
Iteration 137/1000 | Loss: 0.00001449
Iteration 138/1000 | Loss: 0.00001449
Iteration 139/1000 | Loss: 0.00001449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.4493309208774008e-05, 1.4493309208774008e-05, 1.4493309208774008e-05, 1.4493309208774008e-05, 1.4493309208774008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4493309208774008e-05

Optimization complete. Final v2v error: 3.1991121768951416 mm

Highest mean error: 3.3325836658477783 mm for frame 116

Lowest mean error: 3.054716110229492 mm for frame 53

Saving results

Total time: 30.215770721435547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440405
Iteration 2/25 | Loss: 0.00106083
Iteration 3/25 | Loss: 0.00095416
Iteration 4/25 | Loss: 0.00093856
Iteration 5/25 | Loss: 0.00093270
Iteration 6/25 | Loss: 0.00093126
Iteration 7/25 | Loss: 0.00093126
Iteration 8/25 | Loss: 0.00093126
Iteration 9/25 | Loss: 0.00093126
Iteration 10/25 | Loss: 0.00093126
Iteration 11/25 | Loss: 0.00093126
Iteration 12/25 | Loss: 0.00093126
Iteration 13/25 | Loss: 0.00093126
Iteration 14/25 | Loss: 0.00093126
Iteration 15/25 | Loss: 0.00093126
Iteration 16/25 | Loss: 0.00093126
Iteration 17/25 | Loss: 0.00093126
Iteration 18/25 | Loss: 0.00093126
Iteration 19/25 | Loss: 0.00093126
Iteration 20/25 | Loss: 0.00093126
Iteration 21/25 | Loss: 0.00093126
Iteration 22/25 | Loss: 0.00093126
Iteration 23/25 | Loss: 0.00093126
Iteration 24/25 | Loss: 0.00093126
Iteration 25/25 | Loss: 0.00093126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.56030703
Iteration 2/25 | Loss: 0.00064845
Iteration 3/25 | Loss: 0.00064844
Iteration 4/25 | Loss: 0.00064844
Iteration 5/25 | Loss: 0.00064844
Iteration 6/25 | Loss: 0.00064844
Iteration 7/25 | Loss: 0.00064844
Iteration 8/25 | Loss: 0.00064844
Iteration 9/25 | Loss: 0.00064844
Iteration 10/25 | Loss: 0.00064844
Iteration 11/25 | Loss: 0.00064844
Iteration 12/25 | Loss: 0.00064844
Iteration 13/25 | Loss: 0.00064844
Iteration 14/25 | Loss: 0.00064844
Iteration 15/25 | Loss: 0.00064844
Iteration 16/25 | Loss: 0.00064844
Iteration 17/25 | Loss: 0.00064844
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006484390469267964, 0.0006484390469267964, 0.0006484390469267964, 0.0006484390469267964, 0.0006484390469267964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006484390469267964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064844
Iteration 2/1000 | Loss: 0.00002113
Iteration 3/1000 | Loss: 0.00001399
Iteration 4/1000 | Loss: 0.00001290
Iteration 5/1000 | Loss: 0.00001209
Iteration 6/1000 | Loss: 0.00001164
Iteration 7/1000 | Loss: 0.00001127
Iteration 8/1000 | Loss: 0.00001098
Iteration 9/1000 | Loss: 0.00001096
Iteration 10/1000 | Loss: 0.00001088
Iteration 11/1000 | Loss: 0.00001087
Iteration 12/1000 | Loss: 0.00001086
Iteration 13/1000 | Loss: 0.00001086
Iteration 14/1000 | Loss: 0.00001085
Iteration 15/1000 | Loss: 0.00001083
Iteration 16/1000 | Loss: 0.00001081
Iteration 17/1000 | Loss: 0.00001079
Iteration 18/1000 | Loss: 0.00001078
Iteration 19/1000 | Loss: 0.00001077
Iteration 20/1000 | Loss: 0.00001075
Iteration 21/1000 | Loss: 0.00001074
Iteration 22/1000 | Loss: 0.00001073
Iteration 23/1000 | Loss: 0.00001073
Iteration 24/1000 | Loss: 0.00001072
Iteration 25/1000 | Loss: 0.00001072
Iteration 26/1000 | Loss: 0.00001071
Iteration 27/1000 | Loss: 0.00001071
Iteration 28/1000 | Loss: 0.00001071
Iteration 29/1000 | Loss: 0.00001070
Iteration 30/1000 | Loss: 0.00001069
Iteration 31/1000 | Loss: 0.00001068
Iteration 32/1000 | Loss: 0.00001068
Iteration 33/1000 | Loss: 0.00001068
Iteration 34/1000 | Loss: 0.00001068
Iteration 35/1000 | Loss: 0.00001067
Iteration 36/1000 | Loss: 0.00001067
Iteration 37/1000 | Loss: 0.00001067
Iteration 38/1000 | Loss: 0.00001066
Iteration 39/1000 | Loss: 0.00001066
Iteration 40/1000 | Loss: 0.00001066
Iteration 41/1000 | Loss: 0.00001065
Iteration 42/1000 | Loss: 0.00001065
Iteration 43/1000 | Loss: 0.00001065
Iteration 44/1000 | Loss: 0.00001065
Iteration 45/1000 | Loss: 0.00001065
Iteration 46/1000 | Loss: 0.00001065
Iteration 47/1000 | Loss: 0.00001065
Iteration 48/1000 | Loss: 0.00001065
Iteration 49/1000 | Loss: 0.00001065
Iteration 50/1000 | Loss: 0.00001065
Iteration 51/1000 | Loss: 0.00001065
Iteration 52/1000 | Loss: 0.00001064
Iteration 53/1000 | Loss: 0.00001064
Iteration 54/1000 | Loss: 0.00001064
Iteration 55/1000 | Loss: 0.00001064
Iteration 56/1000 | Loss: 0.00001063
Iteration 57/1000 | Loss: 0.00001063
Iteration 58/1000 | Loss: 0.00001063
Iteration 59/1000 | Loss: 0.00001062
Iteration 60/1000 | Loss: 0.00001062
Iteration 61/1000 | Loss: 0.00001062
Iteration 62/1000 | Loss: 0.00001061
Iteration 63/1000 | Loss: 0.00001061
Iteration 64/1000 | Loss: 0.00001061
Iteration 65/1000 | Loss: 0.00001060
Iteration 66/1000 | Loss: 0.00001060
Iteration 67/1000 | Loss: 0.00001060
Iteration 68/1000 | Loss: 0.00001060
Iteration 69/1000 | Loss: 0.00001060
Iteration 70/1000 | Loss: 0.00001060
Iteration 71/1000 | Loss: 0.00001059
Iteration 72/1000 | Loss: 0.00001059
Iteration 73/1000 | Loss: 0.00001059
Iteration 74/1000 | Loss: 0.00001059
Iteration 75/1000 | Loss: 0.00001059
Iteration 76/1000 | Loss: 0.00001058
Iteration 77/1000 | Loss: 0.00001058
Iteration 78/1000 | Loss: 0.00001057
Iteration 79/1000 | Loss: 0.00001057
Iteration 80/1000 | Loss: 0.00001057
Iteration 81/1000 | Loss: 0.00001057
Iteration 82/1000 | Loss: 0.00001057
Iteration 83/1000 | Loss: 0.00001057
Iteration 84/1000 | Loss: 0.00001056
Iteration 85/1000 | Loss: 0.00001056
Iteration 86/1000 | Loss: 0.00001056
Iteration 87/1000 | Loss: 0.00001056
Iteration 88/1000 | Loss: 0.00001056
Iteration 89/1000 | Loss: 0.00001056
Iteration 90/1000 | Loss: 0.00001056
Iteration 91/1000 | Loss: 0.00001055
Iteration 92/1000 | Loss: 0.00001055
Iteration 93/1000 | Loss: 0.00001055
Iteration 94/1000 | Loss: 0.00001055
Iteration 95/1000 | Loss: 0.00001054
Iteration 96/1000 | Loss: 0.00001054
Iteration 97/1000 | Loss: 0.00001054
Iteration 98/1000 | Loss: 0.00001054
Iteration 99/1000 | Loss: 0.00001054
Iteration 100/1000 | Loss: 0.00001054
Iteration 101/1000 | Loss: 0.00001054
Iteration 102/1000 | Loss: 0.00001054
Iteration 103/1000 | Loss: 0.00001054
Iteration 104/1000 | Loss: 0.00001053
Iteration 105/1000 | Loss: 0.00001053
Iteration 106/1000 | Loss: 0.00001053
Iteration 107/1000 | Loss: 0.00001053
Iteration 108/1000 | Loss: 0.00001053
Iteration 109/1000 | Loss: 0.00001053
Iteration 110/1000 | Loss: 0.00001053
Iteration 111/1000 | Loss: 0.00001053
Iteration 112/1000 | Loss: 0.00001053
Iteration 113/1000 | Loss: 0.00001053
Iteration 114/1000 | Loss: 0.00001053
Iteration 115/1000 | Loss: 0.00001053
Iteration 116/1000 | Loss: 0.00001053
Iteration 117/1000 | Loss: 0.00001053
Iteration 118/1000 | Loss: 0.00001053
Iteration 119/1000 | Loss: 0.00001053
Iteration 120/1000 | Loss: 0.00001053
Iteration 121/1000 | Loss: 0.00001052
Iteration 122/1000 | Loss: 0.00001052
Iteration 123/1000 | Loss: 0.00001052
Iteration 124/1000 | Loss: 0.00001052
Iteration 125/1000 | Loss: 0.00001052
Iteration 126/1000 | Loss: 0.00001052
Iteration 127/1000 | Loss: 0.00001052
Iteration 128/1000 | Loss: 0.00001052
Iteration 129/1000 | Loss: 0.00001052
Iteration 130/1000 | Loss: 0.00001052
Iteration 131/1000 | Loss: 0.00001052
Iteration 132/1000 | Loss: 0.00001052
Iteration 133/1000 | Loss: 0.00001052
Iteration 134/1000 | Loss: 0.00001052
Iteration 135/1000 | Loss: 0.00001052
Iteration 136/1000 | Loss: 0.00001052
Iteration 137/1000 | Loss: 0.00001052
Iteration 138/1000 | Loss: 0.00001052
Iteration 139/1000 | Loss: 0.00001052
Iteration 140/1000 | Loss: 0.00001052
Iteration 141/1000 | Loss: 0.00001052
Iteration 142/1000 | Loss: 0.00001052
Iteration 143/1000 | Loss: 0.00001052
Iteration 144/1000 | Loss: 0.00001052
Iteration 145/1000 | Loss: 0.00001052
Iteration 146/1000 | Loss: 0.00001052
Iteration 147/1000 | Loss: 0.00001052
Iteration 148/1000 | Loss: 0.00001052
Iteration 149/1000 | Loss: 0.00001052
Iteration 150/1000 | Loss: 0.00001052
Iteration 151/1000 | Loss: 0.00001052
Iteration 152/1000 | Loss: 0.00001052
Iteration 153/1000 | Loss: 0.00001052
Iteration 154/1000 | Loss: 0.00001052
Iteration 155/1000 | Loss: 0.00001052
Iteration 156/1000 | Loss: 0.00001052
Iteration 157/1000 | Loss: 0.00001052
Iteration 158/1000 | Loss: 0.00001052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.052449533744948e-05, 1.052449533744948e-05, 1.052449533744948e-05, 1.052449533744948e-05, 1.052449533744948e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.052449533744948e-05

Optimization complete. Final v2v error: 2.753481864929199 mm

Highest mean error: 3.3181376457214355 mm for frame 91

Lowest mean error: 2.409130096435547 mm for frame 196

Saving results

Total time: 34.65633797645569
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049462
Iteration 2/25 | Loss: 0.01049462
Iteration 3/25 | Loss: 0.01049462
Iteration 4/25 | Loss: 0.01049461
Iteration 5/25 | Loss: 0.01049461
Iteration 6/25 | Loss: 0.01049461
Iteration 7/25 | Loss: 0.01049461
Iteration 8/25 | Loss: 0.01049461
Iteration 9/25 | Loss: 0.01049461
Iteration 10/25 | Loss: 0.01049461
Iteration 11/25 | Loss: 0.01049460
Iteration 12/25 | Loss: 0.01049460
Iteration 13/25 | Loss: 0.01049460
Iteration 14/25 | Loss: 0.01049460
Iteration 15/25 | Loss: 0.01049460
Iteration 16/25 | Loss: 0.01049460
Iteration 17/25 | Loss: 0.01049460
Iteration 18/25 | Loss: 0.01049459
Iteration 19/25 | Loss: 0.01049459
Iteration 20/25 | Loss: 0.01049459
Iteration 21/25 | Loss: 0.01049459
Iteration 22/25 | Loss: 0.01049459
Iteration 23/25 | Loss: 0.01049459
Iteration 24/25 | Loss: 0.01049459
Iteration 25/25 | Loss: 0.01049458

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62010276
Iteration 2/25 | Loss: 0.08295327
Iteration 3/25 | Loss: 0.07457432
Iteration 4/25 | Loss: 0.07430229
Iteration 5/25 | Loss: 0.07430228
Iteration 6/25 | Loss: 0.07430227
Iteration 7/25 | Loss: 0.07430227
Iteration 8/25 | Loss: 0.07430227
Iteration 9/25 | Loss: 0.07430227
Iteration 10/25 | Loss: 0.07430226
Iteration 11/25 | Loss: 0.07430226
Iteration 12/25 | Loss: 0.07430226
Iteration 13/25 | Loss: 0.07430227
Iteration 14/25 | Loss: 0.07430226
Iteration 15/25 | Loss: 0.07430226
Iteration 16/25 | Loss: 0.07430226
Iteration 17/25 | Loss: 0.07430226
Iteration 18/25 | Loss: 0.07430226
Iteration 19/25 | Loss: 0.07430226
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.07430225610733032, 0.07430225610733032, 0.07430225610733032, 0.07430225610733032, 0.07430225610733032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07430225610733032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07430226
Iteration 2/1000 | Loss: 0.01516209
Iteration 3/1000 | Loss: 0.00869231
Iteration 4/1000 | Loss: 0.00478519
Iteration 5/1000 | Loss: 0.00304481
Iteration 6/1000 | Loss: 0.00357026
Iteration 7/1000 | Loss: 0.00169383
Iteration 8/1000 | Loss: 0.00154934
Iteration 9/1000 | Loss: 0.00237353
Iteration 10/1000 | Loss: 0.00063755
Iteration 11/1000 | Loss: 0.00090812
Iteration 12/1000 | Loss: 0.00153704
Iteration 13/1000 | Loss: 0.00098202
Iteration 14/1000 | Loss: 0.00034663
Iteration 15/1000 | Loss: 0.00040015
Iteration 16/1000 | Loss: 0.00027724
Iteration 17/1000 | Loss: 0.00054889
Iteration 18/1000 | Loss: 0.00047170
Iteration 19/1000 | Loss: 0.00113414
Iteration 20/1000 | Loss: 0.00074032
Iteration 21/1000 | Loss: 0.00070140
Iteration 22/1000 | Loss: 0.00066913
Iteration 23/1000 | Loss: 0.00025014
Iteration 24/1000 | Loss: 0.00028217
Iteration 25/1000 | Loss: 0.00044680
Iteration 26/1000 | Loss: 0.00021824
Iteration 27/1000 | Loss: 0.00018165
Iteration 28/1000 | Loss: 0.00150341
Iteration 29/1000 | Loss: 0.00057490
Iteration 30/1000 | Loss: 0.00070033
Iteration 31/1000 | Loss: 0.00069898
Iteration 32/1000 | Loss: 0.00064490
Iteration 33/1000 | Loss: 0.00033121
Iteration 34/1000 | Loss: 0.00025790
Iteration 35/1000 | Loss: 0.00024736
Iteration 36/1000 | Loss: 0.00023315
Iteration 37/1000 | Loss: 0.00015307
Iteration 38/1000 | Loss: 0.00046250
Iteration 39/1000 | Loss: 0.00032788
Iteration 40/1000 | Loss: 0.00062670
Iteration 41/1000 | Loss: 0.00015173
Iteration 42/1000 | Loss: 0.00030535
Iteration 43/1000 | Loss: 0.00056256
Iteration 44/1000 | Loss: 0.00045142
Iteration 45/1000 | Loss: 0.00028371
Iteration 46/1000 | Loss: 0.00018168
Iteration 47/1000 | Loss: 0.00027933
Iteration 48/1000 | Loss: 0.00031593
Iteration 49/1000 | Loss: 0.00025404
Iteration 50/1000 | Loss: 0.00014670
Iteration 51/1000 | Loss: 0.00044998
Iteration 52/1000 | Loss: 0.00019832
Iteration 53/1000 | Loss: 0.00054526
Iteration 54/1000 | Loss: 0.00062966
Iteration 55/1000 | Loss: 0.00040946
Iteration 56/1000 | Loss: 0.00050262
Iteration 57/1000 | Loss: 0.00059886
Iteration 58/1000 | Loss: 0.00025955
Iteration 59/1000 | Loss: 0.00025576
Iteration 60/1000 | Loss: 0.00011827
Iteration 61/1000 | Loss: 0.00013534
Iteration 62/1000 | Loss: 0.00009886
Iteration 63/1000 | Loss: 0.00022832
Iteration 64/1000 | Loss: 0.00035028
Iteration 65/1000 | Loss: 0.00127288
Iteration 66/1000 | Loss: 0.00029519
Iteration 67/1000 | Loss: 0.00036991
Iteration 68/1000 | Loss: 0.00010183
Iteration 69/1000 | Loss: 0.00026348
Iteration 70/1000 | Loss: 0.00016943
Iteration 71/1000 | Loss: 0.00012954
Iteration 72/1000 | Loss: 0.00013529
Iteration 73/1000 | Loss: 0.00008565
Iteration 74/1000 | Loss: 0.00022849
Iteration 75/1000 | Loss: 0.00020941
Iteration 76/1000 | Loss: 0.00032538
Iteration 77/1000 | Loss: 0.00018672
Iteration 78/1000 | Loss: 0.00026552
Iteration 79/1000 | Loss: 0.00020613
Iteration 80/1000 | Loss: 0.00019379
Iteration 81/1000 | Loss: 0.00019786
Iteration 82/1000 | Loss: 0.00019662
Iteration 83/1000 | Loss: 0.00017391
Iteration 84/1000 | Loss: 0.00010776
Iteration 85/1000 | Loss: 0.00009776
Iteration 86/1000 | Loss: 0.00007897
Iteration 87/1000 | Loss: 0.00019307
Iteration 88/1000 | Loss: 0.00015195
Iteration 89/1000 | Loss: 0.00023891
Iteration 90/1000 | Loss: 0.00014295
Iteration 91/1000 | Loss: 0.00020413
Iteration 92/1000 | Loss: 0.00011792
Iteration 93/1000 | Loss: 0.00012740
Iteration 94/1000 | Loss: 0.00010522
Iteration 95/1000 | Loss: 0.00008825
Iteration 96/1000 | Loss: 0.00049194
Iteration 97/1000 | Loss: 0.00058098
Iteration 98/1000 | Loss: 0.00037383
Iteration 99/1000 | Loss: 0.00039946
Iteration 100/1000 | Loss: 0.00020151
Iteration 101/1000 | Loss: 0.00008256
Iteration 102/1000 | Loss: 0.00014213
Iteration 103/1000 | Loss: 0.00027560
Iteration 104/1000 | Loss: 0.00026910
Iteration 105/1000 | Loss: 0.00008115
Iteration 106/1000 | Loss: 0.00011025
Iteration 107/1000 | Loss: 0.00023236
Iteration 108/1000 | Loss: 0.00016978
Iteration 109/1000 | Loss: 0.00035713
Iteration 110/1000 | Loss: 0.00034595
Iteration 111/1000 | Loss: 0.00035128
Iteration 112/1000 | Loss: 0.00020220
Iteration 113/1000 | Loss: 0.00022431
Iteration 114/1000 | Loss: 0.00066867
Iteration 115/1000 | Loss: 0.00052249
Iteration 116/1000 | Loss: 0.00021365
Iteration 117/1000 | Loss: 0.00047927
Iteration 118/1000 | Loss: 0.00032357
Iteration 119/1000 | Loss: 0.00022492
Iteration 120/1000 | Loss: 0.00015236
Iteration 121/1000 | Loss: 0.00033668
Iteration 122/1000 | Loss: 0.00036729
Iteration 123/1000 | Loss: 0.00021167
Iteration 124/1000 | Loss: 0.00008754
Iteration 125/1000 | Loss: 0.00007940
Iteration 126/1000 | Loss: 0.00007622
Iteration 127/1000 | Loss: 0.00027183
Iteration 128/1000 | Loss: 0.00081031
Iteration 129/1000 | Loss: 0.00067150
Iteration 130/1000 | Loss: 0.00025427
Iteration 131/1000 | Loss: 0.00009981
Iteration 132/1000 | Loss: 0.00008150
Iteration 133/1000 | Loss: 0.00007691
Iteration 134/1000 | Loss: 0.00023872
Iteration 135/1000 | Loss: 0.00037949
Iteration 136/1000 | Loss: 0.00027880
Iteration 137/1000 | Loss: 0.00076603
Iteration 138/1000 | Loss: 0.00034205
Iteration 139/1000 | Loss: 0.00007964
Iteration 140/1000 | Loss: 0.00022498
Iteration 141/1000 | Loss: 0.00007773
Iteration 142/1000 | Loss: 0.00007078
Iteration 143/1000 | Loss: 0.00006723
Iteration 144/1000 | Loss: 0.00026167
Iteration 145/1000 | Loss: 0.00057704
Iteration 146/1000 | Loss: 0.00026139
Iteration 147/1000 | Loss: 0.00020433
Iteration 148/1000 | Loss: 0.00009510
Iteration 149/1000 | Loss: 0.00008203
Iteration 150/1000 | Loss: 0.00007412
Iteration 151/1000 | Loss: 0.00006969
Iteration 152/1000 | Loss: 0.00028189
Iteration 153/1000 | Loss: 0.00023074
Iteration 154/1000 | Loss: 0.00026696
Iteration 155/1000 | Loss: 0.00033300
Iteration 156/1000 | Loss: 0.00041953
Iteration 157/1000 | Loss: 0.00028583
Iteration 158/1000 | Loss: 0.00016613
Iteration 159/1000 | Loss: 0.00028307
Iteration 160/1000 | Loss: 0.00015765
Iteration 161/1000 | Loss: 0.00034418
Iteration 162/1000 | Loss: 0.00017613
Iteration 163/1000 | Loss: 0.00009453
Iteration 164/1000 | Loss: 0.00020774
Iteration 165/1000 | Loss: 0.00017987
Iteration 166/1000 | Loss: 0.00008986
Iteration 167/1000 | Loss: 0.00008127
Iteration 168/1000 | Loss: 0.00007569
Iteration 169/1000 | Loss: 0.00007445
Iteration 170/1000 | Loss: 0.00007495
Iteration 171/1000 | Loss: 0.00006553
Iteration 172/1000 | Loss: 0.00019888
Iteration 173/1000 | Loss: 0.00015762
Iteration 174/1000 | Loss: 0.00008748
Iteration 175/1000 | Loss: 0.00039515
Iteration 176/1000 | Loss: 0.00022301
Iteration 177/1000 | Loss: 0.00037702
Iteration 178/1000 | Loss: 0.00020849
Iteration 179/1000 | Loss: 0.00021871
Iteration 180/1000 | Loss: 0.00013487
Iteration 181/1000 | Loss: 0.00008520
Iteration 182/1000 | Loss: 0.00013831
Iteration 183/1000 | Loss: 0.00010517
Iteration 184/1000 | Loss: 0.00023480
Iteration 185/1000 | Loss: 0.00007620
Iteration 186/1000 | Loss: 0.00007499
Iteration 187/1000 | Loss: 0.00007084
Iteration 188/1000 | Loss: 0.00007893
Iteration 189/1000 | Loss: 0.00007113
Iteration 190/1000 | Loss: 0.00006841
Iteration 191/1000 | Loss: 0.00007563
Iteration 192/1000 | Loss: 0.00006395
Iteration 193/1000 | Loss: 0.00007519
Iteration 194/1000 | Loss: 0.00007534
Iteration 195/1000 | Loss: 0.00007650
Iteration 196/1000 | Loss: 0.00006485
Iteration 197/1000 | Loss: 0.00006529
Iteration 198/1000 | Loss: 0.00007366
Iteration 199/1000 | Loss: 0.00007357
Iteration 200/1000 | Loss: 0.00007419
Iteration 201/1000 | Loss: 0.00007423
Iteration 202/1000 | Loss: 0.00007429
Iteration 203/1000 | Loss: 0.00007384
Iteration 204/1000 | Loss: 0.00007283
Iteration 205/1000 | Loss: 0.00008386
Iteration 206/1000 | Loss: 0.00007347
Iteration 207/1000 | Loss: 0.00007881
Iteration 208/1000 | Loss: 0.00050193
Iteration 209/1000 | Loss: 0.00037431
Iteration 210/1000 | Loss: 0.00037865
Iteration 211/1000 | Loss: 0.00026421
Iteration 212/1000 | Loss: 0.00017031
Iteration 213/1000 | Loss: 0.00019294
Iteration 214/1000 | Loss: 0.00021702
Iteration 215/1000 | Loss: 0.00022246
Iteration 216/1000 | Loss: 0.00010467
Iteration 217/1000 | Loss: 0.00023833
Iteration 218/1000 | Loss: 0.00025729
Iteration 219/1000 | Loss: 0.00011340
Iteration 220/1000 | Loss: 0.00035418
Iteration 221/1000 | Loss: 0.00017047
Iteration 222/1000 | Loss: 0.00007589
Iteration 223/1000 | Loss: 0.00007548
Iteration 224/1000 | Loss: 0.00018333
Iteration 225/1000 | Loss: 0.00027146
Iteration 226/1000 | Loss: 0.00019465
Iteration 227/1000 | Loss: 0.00020895
Iteration 228/1000 | Loss: 0.00008753
Iteration 229/1000 | Loss: 0.00006645
Iteration 230/1000 | Loss: 0.00007110
Iteration 231/1000 | Loss: 0.00006378
Iteration 232/1000 | Loss: 0.00008265
Iteration 233/1000 | Loss: 0.00018623
Iteration 234/1000 | Loss: 0.00009450
Iteration 235/1000 | Loss: 0.00019602
Iteration 236/1000 | Loss: 0.00027027
Iteration 237/1000 | Loss: 0.00020483
Iteration 238/1000 | Loss: 0.00043141
Iteration 239/1000 | Loss: 0.00016849
Iteration 240/1000 | Loss: 0.00032285
Iteration 241/1000 | Loss: 0.00030065
Iteration 242/1000 | Loss: 0.00029107
Iteration 243/1000 | Loss: 0.00024150
Iteration 244/1000 | Loss: 0.00023119
Iteration 245/1000 | Loss: 0.00035297
Iteration 246/1000 | Loss: 0.00032034
Iteration 247/1000 | Loss: 0.00026250
Iteration 248/1000 | Loss: 0.00016123
Iteration 249/1000 | Loss: 0.00022572
Iteration 250/1000 | Loss: 0.00018983
Iteration 251/1000 | Loss: 0.00013091
Iteration 252/1000 | Loss: 0.00008612
Iteration 253/1000 | Loss: 0.00013897
Iteration 254/1000 | Loss: 0.00011713
Iteration 255/1000 | Loss: 0.00014323
Iteration 256/1000 | Loss: 0.00008984
Iteration 257/1000 | Loss: 0.00006509
Iteration 258/1000 | Loss: 0.00012662
Iteration 259/1000 | Loss: 0.00009694
Iteration 260/1000 | Loss: 0.00007020
Iteration 261/1000 | Loss: 0.00010833
Iteration 262/1000 | Loss: 0.00011597
Iteration 263/1000 | Loss: 0.00010463
Iteration 264/1000 | Loss: 0.00010016
Iteration 265/1000 | Loss: 0.00014252
Iteration 266/1000 | Loss: 0.00019810
Iteration 267/1000 | Loss: 0.00008023
Iteration 268/1000 | Loss: 0.00006774
Iteration 269/1000 | Loss: 0.00007257
Iteration 270/1000 | Loss: 0.00006025
Iteration 271/1000 | Loss: 0.00008035
Iteration 272/1000 | Loss: 0.00006038
Iteration 273/1000 | Loss: 0.00005935
Iteration 274/1000 | Loss: 0.00008355
Iteration 275/1000 | Loss: 0.00007397
Iteration 276/1000 | Loss: 0.00006585
Iteration 277/1000 | Loss: 0.00008219
Iteration 278/1000 | Loss: 0.00020167
Iteration 279/1000 | Loss: 0.00006558
Iteration 280/1000 | Loss: 0.00005843
Iteration 281/1000 | Loss: 0.00007485
Iteration 282/1000 | Loss: 0.00005767
Iteration 283/1000 | Loss: 0.00005743
Iteration 284/1000 | Loss: 0.00040107
Iteration 285/1000 | Loss: 0.00025737
Iteration 286/1000 | Loss: 0.00011112
Iteration 287/1000 | Loss: 0.00006650
Iteration 288/1000 | Loss: 0.00006399
Iteration 289/1000 | Loss: 0.00006082
Iteration 290/1000 | Loss: 0.00006223
Iteration 291/1000 | Loss: 0.00015345
Iteration 292/1000 | Loss: 0.00009055
Iteration 293/1000 | Loss: 0.00006213
Iteration 294/1000 | Loss: 0.00005948
Iteration 295/1000 | Loss: 0.00005882
Iteration 296/1000 | Loss: 0.00005844
Iteration 297/1000 | Loss: 0.00034820
Iteration 298/1000 | Loss: 0.00022452
Iteration 299/1000 | Loss: 0.00007510
Iteration 300/1000 | Loss: 0.00007222
Iteration 301/1000 | Loss: 0.00007504
Iteration 302/1000 | Loss: 0.00006085
Iteration 303/1000 | Loss: 0.00009071
Iteration 304/1000 | Loss: 0.00008349
Iteration 305/1000 | Loss: 0.00009081
Iteration 306/1000 | Loss: 0.00007636
Iteration 307/1000 | Loss: 0.00006461
Iteration 308/1000 | Loss: 0.00008770
Iteration 309/1000 | Loss: 0.00006103
Iteration 310/1000 | Loss: 0.00017824
Iteration 311/1000 | Loss: 0.00008845
Iteration 312/1000 | Loss: 0.00006521
Iteration 313/1000 | Loss: 0.00008319
Iteration 314/1000 | Loss: 0.00007565
Iteration 315/1000 | Loss: 0.00005983
Iteration 316/1000 | Loss: 0.00006232
Iteration 317/1000 | Loss: 0.00017964
Iteration 318/1000 | Loss: 0.00013576
Iteration 319/1000 | Loss: 0.00017838
Iteration 320/1000 | Loss: 0.00013830
Iteration 321/1000 | Loss: 0.00032511
Iteration 322/1000 | Loss: 0.00022407
Iteration 323/1000 | Loss: 0.00010388
Iteration 324/1000 | Loss: 0.00028481
Iteration 325/1000 | Loss: 0.00006320
Iteration 326/1000 | Loss: 0.00005887
Iteration 327/1000 | Loss: 0.00005797
Iteration 328/1000 | Loss: 0.00005803
Iteration 329/1000 | Loss: 0.00016935
Iteration 330/1000 | Loss: 0.00031919
Iteration 331/1000 | Loss: 0.00018796
Iteration 332/1000 | Loss: 0.00014849
Iteration 333/1000 | Loss: 0.00007039
Iteration 334/1000 | Loss: 0.00006263
Iteration 335/1000 | Loss: 0.00006246
Iteration 336/1000 | Loss: 0.00005933
Iteration 337/1000 | Loss: 0.00006090
Iteration 338/1000 | Loss: 0.00006007
Iteration 339/1000 | Loss: 0.00005810
Iteration 340/1000 | Loss: 0.00005892
Iteration 341/1000 | Loss: 0.00005892
Iteration 342/1000 | Loss: 0.00005786
Iteration 343/1000 | Loss: 0.00005786
Iteration 344/1000 | Loss: 0.00005785
Iteration 345/1000 | Loss: 0.00005785
Iteration 346/1000 | Loss: 0.00005785
Iteration 347/1000 | Loss: 0.00005785
Iteration 348/1000 | Loss: 0.00005848
Iteration 349/1000 | Loss: 0.00005778
Iteration 350/1000 | Loss: 0.00005775
Iteration 351/1000 | Loss: 0.00015670
Iteration 352/1000 | Loss: 0.00015949
Iteration 353/1000 | Loss: 0.00016772
Iteration 354/1000 | Loss: 0.00013512
Iteration 355/1000 | Loss: 0.00024818
Iteration 356/1000 | Loss: 0.00021352
Iteration 357/1000 | Loss: 0.00009828
Iteration 358/1000 | Loss: 0.00006300
Iteration 359/1000 | Loss: 0.00005775
Iteration 360/1000 | Loss: 0.00007441
Iteration 361/1000 | Loss: 0.00006072
Iteration 362/1000 | Loss: 0.00006905
Iteration 363/1000 | Loss: 0.00005954
Iteration 364/1000 | Loss: 0.00016633
Iteration 365/1000 | Loss: 0.00012936
Iteration 366/1000 | Loss: 0.00005962
Iteration 367/1000 | Loss: 0.00007076
Iteration 368/1000 | Loss: 0.00005877
Iteration 369/1000 | Loss: 0.00005832
Iteration 370/1000 | Loss: 0.00005781
Iteration 371/1000 | Loss: 0.00005749
Iteration 372/1000 | Loss: 0.00009723
Iteration 373/1000 | Loss: 0.00007555
Iteration 374/1000 | Loss: 0.00008980
Iteration 375/1000 | Loss: 0.00007284
Iteration 376/1000 | Loss: 0.00007475
Iteration 377/1000 | Loss: 0.00006390
Iteration 378/1000 | Loss: 0.00005729
Iteration 379/1000 | Loss: 0.00007345
Iteration 380/1000 | Loss: 0.00009399
Iteration 381/1000 | Loss: 0.00007278
Iteration 382/1000 | Loss: 0.00006114
Iteration 383/1000 | Loss: 0.00007289
Iteration 384/1000 | Loss: 0.00007288
Iteration 385/1000 | Loss: 0.00027225
Iteration 386/1000 | Loss: 0.00027040
Iteration 387/1000 | Loss: 0.00011718
Iteration 388/1000 | Loss: 0.00009628
Iteration 389/1000 | Loss: 0.00005982
Iteration 390/1000 | Loss: 0.00006842
Iteration 391/1000 | Loss: 0.00007590
Iteration 392/1000 | Loss: 0.00008090
Iteration 393/1000 | Loss: 0.00009411
Iteration 394/1000 | Loss: 0.00007340
Iteration 395/1000 | Loss: 0.00025691
Iteration 396/1000 | Loss: 0.00011241
Iteration 397/1000 | Loss: 0.00006052
Iteration 398/1000 | Loss: 0.00005785
Iteration 399/1000 | Loss: 0.00006261
Iteration 400/1000 | Loss: 0.00006269
Iteration 401/1000 | Loss: 0.00006158
Iteration 402/1000 | Loss: 0.00016478
Iteration 403/1000 | Loss: 0.00011744
Iteration 404/1000 | Loss: 0.00005711
Iteration 405/1000 | Loss: 0.00016361
Iteration 406/1000 | Loss: 0.00013221
Iteration 407/1000 | Loss: 0.00005686
Iteration 408/1000 | Loss: 0.00005680
Iteration 409/1000 | Loss: 0.00005676
Iteration 410/1000 | Loss: 0.00005675
Iteration 411/1000 | Loss: 0.00005672
Iteration 412/1000 | Loss: 0.00005672
Iteration 413/1000 | Loss: 0.00005671
Iteration 414/1000 | Loss: 0.00005670
Iteration 415/1000 | Loss: 0.00015620
Iteration 416/1000 | Loss: 0.00005969
Iteration 417/1000 | Loss: 0.00005823
Iteration 418/1000 | Loss: 0.00005745
Iteration 419/1000 | Loss: 0.00024936
Iteration 420/1000 | Loss: 0.00009340
Iteration 421/1000 | Loss: 0.00011075
Iteration 422/1000 | Loss: 0.00006078
Iteration 423/1000 | Loss: 0.00022711
Iteration 424/1000 | Loss: 0.00016481
Iteration 425/1000 | Loss: 0.00022947
Iteration 426/1000 | Loss: 0.00015831
Iteration 427/1000 | Loss: 0.00025201
Iteration 428/1000 | Loss: 0.00024983
Iteration 429/1000 | Loss: 0.00019685
Iteration 430/1000 | Loss: 0.00010228
Iteration 431/1000 | Loss: 0.00026754
Iteration 432/1000 | Loss: 0.00018360
Iteration 433/1000 | Loss: 0.00015665
Iteration 434/1000 | Loss: 0.00008673
Iteration 435/1000 | Loss: 0.00007448
Iteration 436/1000 | Loss: 0.00006620
Iteration 437/1000 | Loss: 0.00020610
Iteration 438/1000 | Loss: 0.00020414
Iteration 439/1000 | Loss: 0.00021072
Iteration 440/1000 | Loss: 0.00005958
Iteration 441/1000 | Loss: 0.00005757
Iteration 442/1000 | Loss: 0.00005843
Iteration 443/1000 | Loss: 0.00005685
Iteration 444/1000 | Loss: 0.00005732
Iteration 445/1000 | Loss: 0.00005648
Iteration 446/1000 | Loss: 0.00023879
Iteration 447/1000 | Loss: 0.00015508
Iteration 448/1000 | Loss: 0.00005741
Iteration 449/1000 | Loss: 0.00022228
Iteration 450/1000 | Loss: 0.00016630
Iteration 451/1000 | Loss: 0.00028855
Iteration 452/1000 | Loss: 0.00006589
Iteration 453/1000 | Loss: 0.00005908
Iteration 454/1000 | Loss: 0.00006396
Iteration 455/1000 | Loss: 0.00005779
Iteration 456/1000 | Loss: 0.00005763
Iteration 457/1000 | Loss: 0.00007989
Iteration 458/1000 | Loss: 0.00005975
Iteration 459/1000 | Loss: 0.00005725
Iteration 460/1000 | Loss: 0.00005878
Iteration 461/1000 | Loss: 0.00005707
Iteration 462/1000 | Loss: 0.00005705
Iteration 463/1000 | Loss: 0.00005704
Iteration 464/1000 | Loss: 0.00005704
Iteration 465/1000 | Loss: 0.00005703
Iteration 466/1000 | Loss: 0.00005703
Iteration 467/1000 | Loss: 0.00005702
Iteration 468/1000 | Loss: 0.00005701
Iteration 469/1000 | Loss: 0.00005700
Iteration 470/1000 | Loss: 0.00005700
Iteration 471/1000 | Loss: 0.00005697
Iteration 472/1000 | Loss: 0.00006149
Iteration 473/1000 | Loss: 0.00007729
Iteration 474/1000 | Loss: 0.00006974
Iteration 475/1000 | Loss: 0.00005695
Iteration 476/1000 | Loss: 0.00007526
Iteration 477/1000 | Loss: 0.00006566
Iteration 478/1000 | Loss: 0.00007932
Iteration 479/1000 | Loss: 0.00006507
Iteration 480/1000 | Loss: 0.00007751
Iteration 481/1000 | Loss: 0.00006219
Iteration 482/1000 | Loss: 0.00005752
Iteration 483/1000 | Loss: 0.00007522
Iteration 484/1000 | Loss: 0.00005996
Iteration 485/1000 | Loss: 0.00007530
Iteration 486/1000 | Loss: 0.00005866
Iteration 487/1000 | Loss: 0.00005970
Iteration 488/1000 | Loss: 0.00005777
Iteration 489/1000 | Loss: 0.00006728
Iteration 490/1000 | Loss: 0.00010771
Iteration 491/1000 | Loss: 0.00005835
Iteration 492/1000 | Loss: 0.00005678
Iteration 493/1000 | Loss: 0.00005658
Iteration 494/1000 | Loss: 0.00005658
Iteration 495/1000 | Loss: 0.00005658
Iteration 496/1000 | Loss: 0.00005658
Iteration 497/1000 | Loss: 0.00005658
Iteration 498/1000 | Loss: 0.00005657
Iteration 499/1000 | Loss: 0.00005657
Iteration 500/1000 | Loss: 0.00005657
Iteration 501/1000 | Loss: 0.00006213
Iteration 502/1000 | Loss: 0.00005689
Iteration 503/1000 | Loss: 0.00005656
Iteration 504/1000 | Loss: 0.00005656
Iteration 505/1000 | Loss: 0.00005656
Iteration 506/1000 | Loss: 0.00005656
Iteration 507/1000 | Loss: 0.00005656
Iteration 508/1000 | Loss: 0.00005656
Iteration 509/1000 | Loss: 0.00005656
Iteration 510/1000 | Loss: 0.00005656
Iteration 511/1000 | Loss: 0.00005656
Iteration 512/1000 | Loss: 0.00005656
Iteration 513/1000 | Loss: 0.00005655
Iteration 514/1000 | Loss: 0.00005655
Iteration 515/1000 | Loss: 0.00005655
Iteration 516/1000 | Loss: 0.00005655
Iteration 517/1000 | Loss: 0.00006752
Iteration 518/1000 | Loss: 0.00006188
Iteration 519/1000 | Loss: 0.00006098
Iteration 520/1000 | Loss: 0.00006720
Iteration 521/1000 | Loss: 0.00009029
Iteration 522/1000 | Loss: 0.00005834
Iteration 523/1000 | Loss: 0.00008056
Iteration 524/1000 | Loss: 0.00005675
Iteration 525/1000 | Loss: 0.00005724
Iteration 526/1000 | Loss: 0.00007012
Iteration 527/1000 | Loss: 0.00006528
Iteration 528/1000 | Loss: 0.00006749
Iteration 529/1000 | Loss: 0.00006054
Iteration 530/1000 | Loss: 0.00005972
Iteration 531/1000 | Loss: 0.00005739
Iteration 532/1000 | Loss: 0.00006668
Iteration 533/1000 | Loss: 0.00005673
Iteration 534/1000 | Loss: 0.00005778
Iteration 535/1000 | Loss: 0.00005653
Iteration 536/1000 | Loss: 0.00005653
Iteration 537/1000 | Loss: 0.00005653
Iteration 538/1000 | Loss: 0.00005653
Iteration 539/1000 | Loss: 0.00005653
Iteration 540/1000 | Loss: 0.00005652
Iteration 541/1000 | Loss: 0.00005652
Iteration 542/1000 | Loss: 0.00005652
Iteration 543/1000 | Loss: 0.00005652
Iteration 544/1000 | Loss: 0.00005652
Iteration 545/1000 | Loss: 0.00005652
Iteration 546/1000 | Loss: 0.00005651
Iteration 547/1000 | Loss: 0.00005651
Iteration 548/1000 | Loss: 0.00005651
Iteration 549/1000 | Loss: 0.00005651
Iteration 550/1000 | Loss: 0.00005651
Iteration 551/1000 | Loss: 0.00005681
Iteration 552/1000 | Loss: 0.00005648
Iteration 553/1000 | Loss: 0.00005648
Iteration 554/1000 | Loss: 0.00005648
Iteration 555/1000 | Loss: 0.00005647
Iteration 556/1000 | Loss: 0.00005646
Iteration 557/1000 | Loss: 0.00005646
Iteration 558/1000 | Loss: 0.00005699
Iteration 559/1000 | Loss: 0.00005670
Iteration 560/1000 | Loss: 0.00005674
Iteration 561/1000 | Loss: 0.00005644
Iteration 562/1000 | Loss: 0.00005644
Iteration 563/1000 | Loss: 0.00005644
Iteration 564/1000 | Loss: 0.00005644
Iteration 565/1000 | Loss: 0.00005644
Iteration 566/1000 | Loss: 0.00005643
Iteration 567/1000 | Loss: 0.00005643
Iteration 568/1000 | Loss: 0.00005643
Iteration 569/1000 | Loss: 0.00005643
Iteration 570/1000 | Loss: 0.00005643
Iteration 571/1000 | Loss: 0.00005643
Iteration 572/1000 | Loss: 0.00005643
Iteration 573/1000 | Loss: 0.00005643
Iteration 574/1000 | Loss: 0.00005643
Iteration 575/1000 | Loss: 0.00005643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 575. Stopping optimization.
Last 5 losses: [5.6434400903526694e-05, 5.6434400903526694e-05, 5.6434400903526694e-05, 5.6434400903526694e-05, 5.6434400903526694e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.6434400903526694e-05

Optimization complete. Final v2v error: 4.2214813232421875 mm

Highest mean error: 18.853559494018555 mm for frame 45

Lowest mean error: 2.3900105953216553 mm for frame 88

Saving results

Total time: 796.0620229244232
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856090
Iteration 2/25 | Loss: 0.00155462
Iteration 3/25 | Loss: 0.00127902
Iteration 4/25 | Loss: 0.00123617
Iteration 5/25 | Loss: 0.00122928
Iteration 6/25 | Loss: 0.00122182
Iteration 7/25 | Loss: 0.00121933
Iteration 8/25 | Loss: 0.00121810
Iteration 9/25 | Loss: 0.00121717
Iteration 10/25 | Loss: 0.00121618
Iteration 11/25 | Loss: 0.00121527
Iteration 12/25 | Loss: 0.00121387
Iteration 13/25 | Loss: 0.00121288
Iteration 14/25 | Loss: 0.00121259
Iteration 15/25 | Loss: 0.00121246
Iteration 16/25 | Loss: 0.00121246
Iteration 17/25 | Loss: 0.00121246
Iteration 18/25 | Loss: 0.00121246
Iteration 19/25 | Loss: 0.00121246
Iteration 20/25 | Loss: 0.00121246
Iteration 21/25 | Loss: 0.00121246
Iteration 22/25 | Loss: 0.00121246
Iteration 23/25 | Loss: 0.00121246
Iteration 24/25 | Loss: 0.00121246
Iteration 25/25 | Loss: 0.00121246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.40122509
Iteration 2/25 | Loss: 0.00216735
Iteration 3/25 | Loss: 0.00216730
Iteration 4/25 | Loss: 0.00216730
Iteration 5/25 | Loss: 0.00216730
Iteration 6/25 | Loss: 0.00216730
Iteration 7/25 | Loss: 0.00216730
Iteration 8/25 | Loss: 0.00216730
Iteration 9/25 | Loss: 0.00216730
Iteration 10/25 | Loss: 0.00216730
Iteration 11/25 | Loss: 0.00216730
Iteration 12/25 | Loss: 0.00216730
Iteration 13/25 | Loss: 0.00216730
Iteration 14/25 | Loss: 0.00216730
Iteration 15/25 | Loss: 0.00216730
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0021672984585165977, 0.0021672984585165977, 0.0021672984585165977, 0.0021672984585165977, 0.0021672984585165977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021672984585165977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216730
Iteration 2/1000 | Loss: 0.00033741
Iteration 3/1000 | Loss: 0.00025907
Iteration 4/1000 | Loss: 0.00186220
Iteration 5/1000 | Loss: 0.00571318
Iteration 6/1000 | Loss: 0.00057375
Iteration 7/1000 | Loss: 0.00169489
Iteration 8/1000 | Loss: 0.00075502
Iteration 9/1000 | Loss: 0.00199819
Iteration 10/1000 | Loss: 0.00025249
Iteration 11/1000 | Loss: 0.00016542
Iteration 12/1000 | Loss: 0.00020511
Iteration 13/1000 | Loss: 0.00029440
Iteration 14/1000 | Loss: 0.00010177
Iteration 15/1000 | Loss: 0.00006337
Iteration 16/1000 | Loss: 0.00005382
Iteration 17/1000 | Loss: 0.00004729
Iteration 18/1000 | Loss: 0.00004387
Iteration 19/1000 | Loss: 0.00004189
Iteration 20/1000 | Loss: 0.00004033
Iteration 21/1000 | Loss: 0.00003896
Iteration 22/1000 | Loss: 0.00003814
Iteration 23/1000 | Loss: 0.00003726
Iteration 24/1000 | Loss: 0.00003651
Iteration 25/1000 | Loss: 0.00003592
Iteration 26/1000 | Loss: 0.00003547
Iteration 27/1000 | Loss: 0.00003505
Iteration 28/1000 | Loss: 0.00003462
Iteration 29/1000 | Loss: 0.00003433
Iteration 30/1000 | Loss: 0.00003402
Iteration 31/1000 | Loss: 0.00003369
Iteration 32/1000 | Loss: 0.00104011
Iteration 33/1000 | Loss: 0.00074183
Iteration 34/1000 | Loss: 0.00336218
Iteration 35/1000 | Loss: 0.00182843
Iteration 36/1000 | Loss: 0.00296000
Iteration 37/1000 | Loss: 0.00006064
Iteration 38/1000 | Loss: 0.00004420
Iteration 39/1000 | Loss: 0.00003289
Iteration 40/1000 | Loss: 0.00002633
Iteration 41/1000 | Loss: 0.00002067
Iteration 42/1000 | Loss: 0.00001834
Iteration 43/1000 | Loss: 0.00001611
Iteration 44/1000 | Loss: 0.00001484
Iteration 45/1000 | Loss: 0.00001404
Iteration 46/1000 | Loss: 0.00001357
Iteration 47/1000 | Loss: 0.00001321
Iteration 48/1000 | Loss: 0.00001286
Iteration 49/1000 | Loss: 0.00001282
Iteration 50/1000 | Loss: 0.00001260
Iteration 51/1000 | Loss: 0.00001258
Iteration 52/1000 | Loss: 0.00001255
Iteration 53/1000 | Loss: 0.00001252
Iteration 54/1000 | Loss: 0.00001251
Iteration 55/1000 | Loss: 0.00001251
Iteration 56/1000 | Loss: 0.00001251
Iteration 57/1000 | Loss: 0.00001251
Iteration 58/1000 | Loss: 0.00001251
Iteration 59/1000 | Loss: 0.00001246
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001234
Iteration 63/1000 | Loss: 0.00001234
Iteration 64/1000 | Loss: 0.00001233
Iteration 65/1000 | Loss: 0.00001231
Iteration 66/1000 | Loss: 0.00001230
Iteration 67/1000 | Loss: 0.00001230
Iteration 68/1000 | Loss: 0.00001230
Iteration 69/1000 | Loss: 0.00001230
Iteration 70/1000 | Loss: 0.00001230
Iteration 71/1000 | Loss: 0.00001230
Iteration 72/1000 | Loss: 0.00001229
Iteration 73/1000 | Loss: 0.00001227
Iteration 74/1000 | Loss: 0.00001226
Iteration 75/1000 | Loss: 0.00001225
Iteration 76/1000 | Loss: 0.00001224
Iteration 77/1000 | Loss: 0.00001223
Iteration 78/1000 | Loss: 0.00001222
Iteration 79/1000 | Loss: 0.00001222
Iteration 80/1000 | Loss: 0.00001221
Iteration 81/1000 | Loss: 0.00001220
Iteration 82/1000 | Loss: 0.00001219
Iteration 83/1000 | Loss: 0.00001219
Iteration 84/1000 | Loss: 0.00001219
Iteration 85/1000 | Loss: 0.00001219
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001215
Iteration 89/1000 | Loss: 0.00001214
Iteration 90/1000 | Loss: 0.00001214
Iteration 91/1000 | Loss: 0.00001214
Iteration 92/1000 | Loss: 0.00001213
Iteration 93/1000 | Loss: 0.00001213
Iteration 94/1000 | Loss: 0.00001213
Iteration 95/1000 | Loss: 0.00001213
Iteration 96/1000 | Loss: 0.00001213
Iteration 97/1000 | Loss: 0.00001212
Iteration 98/1000 | Loss: 0.00001212
Iteration 99/1000 | Loss: 0.00001212
Iteration 100/1000 | Loss: 0.00001211
Iteration 101/1000 | Loss: 0.00001211
Iteration 102/1000 | Loss: 0.00001211
Iteration 103/1000 | Loss: 0.00001211
Iteration 104/1000 | Loss: 0.00001211
Iteration 105/1000 | Loss: 0.00001211
Iteration 106/1000 | Loss: 0.00001211
Iteration 107/1000 | Loss: 0.00001211
Iteration 108/1000 | Loss: 0.00001211
Iteration 109/1000 | Loss: 0.00001211
Iteration 110/1000 | Loss: 0.00001211
Iteration 111/1000 | Loss: 0.00001210
Iteration 112/1000 | Loss: 0.00001210
Iteration 113/1000 | Loss: 0.00001210
Iteration 114/1000 | Loss: 0.00001210
Iteration 115/1000 | Loss: 0.00001210
Iteration 116/1000 | Loss: 0.00001210
Iteration 117/1000 | Loss: 0.00001209
Iteration 118/1000 | Loss: 0.00001209
Iteration 119/1000 | Loss: 0.00001209
Iteration 120/1000 | Loss: 0.00001209
Iteration 121/1000 | Loss: 0.00001209
Iteration 122/1000 | Loss: 0.00001209
Iteration 123/1000 | Loss: 0.00001209
Iteration 124/1000 | Loss: 0.00001209
Iteration 125/1000 | Loss: 0.00001209
Iteration 126/1000 | Loss: 0.00001209
Iteration 127/1000 | Loss: 0.00001209
Iteration 128/1000 | Loss: 0.00001209
Iteration 129/1000 | Loss: 0.00001208
Iteration 130/1000 | Loss: 0.00001208
Iteration 131/1000 | Loss: 0.00001208
Iteration 132/1000 | Loss: 0.00001208
Iteration 133/1000 | Loss: 0.00001208
Iteration 134/1000 | Loss: 0.00001208
Iteration 135/1000 | Loss: 0.00001208
Iteration 136/1000 | Loss: 0.00001208
Iteration 137/1000 | Loss: 0.00001207
Iteration 138/1000 | Loss: 0.00001207
Iteration 139/1000 | Loss: 0.00001207
Iteration 140/1000 | Loss: 0.00001207
Iteration 141/1000 | Loss: 0.00001207
Iteration 142/1000 | Loss: 0.00001207
Iteration 143/1000 | Loss: 0.00001207
Iteration 144/1000 | Loss: 0.00001207
Iteration 145/1000 | Loss: 0.00001207
Iteration 146/1000 | Loss: 0.00001207
Iteration 147/1000 | Loss: 0.00001207
Iteration 148/1000 | Loss: 0.00001207
Iteration 149/1000 | Loss: 0.00001207
Iteration 150/1000 | Loss: 0.00001207
Iteration 151/1000 | Loss: 0.00001207
Iteration 152/1000 | Loss: 0.00001207
Iteration 153/1000 | Loss: 0.00001207
Iteration 154/1000 | Loss: 0.00001207
Iteration 155/1000 | Loss: 0.00001207
Iteration 156/1000 | Loss: 0.00001207
Iteration 157/1000 | Loss: 0.00001207
Iteration 158/1000 | Loss: 0.00001207
Iteration 159/1000 | Loss: 0.00001206
Iteration 160/1000 | Loss: 0.00001206
Iteration 161/1000 | Loss: 0.00001206
Iteration 162/1000 | Loss: 0.00001206
Iteration 163/1000 | Loss: 0.00001206
Iteration 164/1000 | Loss: 0.00001206
Iteration 165/1000 | Loss: 0.00001206
Iteration 166/1000 | Loss: 0.00001206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.206362503580749e-05, 1.206362503580749e-05, 1.206362503580749e-05, 1.206362503580749e-05, 1.206362503580749e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.206362503580749e-05

Optimization complete. Final v2v error: 2.8855409622192383 mm

Highest mean error: 3.1835596561431885 mm for frame 18

Lowest mean error: 2.593768358230591 mm for frame 53

Saving results

Total time: 119.22204566001892
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412064
Iteration 2/25 | Loss: 0.00108738
Iteration 3/25 | Loss: 0.00096115
Iteration 4/25 | Loss: 0.00095182
Iteration 5/25 | Loss: 0.00094865
Iteration 6/25 | Loss: 0.00094863
Iteration 7/25 | Loss: 0.00094863
Iteration 8/25 | Loss: 0.00094863
Iteration 9/25 | Loss: 0.00094863
Iteration 10/25 | Loss: 0.00094863
Iteration 11/25 | Loss: 0.00094863
Iteration 12/25 | Loss: 0.00094863
Iteration 13/25 | Loss: 0.00094863
Iteration 14/25 | Loss: 0.00094863
Iteration 15/25 | Loss: 0.00094863
Iteration 16/25 | Loss: 0.00094863
Iteration 17/25 | Loss: 0.00094863
Iteration 18/25 | Loss: 0.00094863
Iteration 19/25 | Loss: 0.00094863
Iteration 20/25 | Loss: 0.00094863
Iteration 21/25 | Loss: 0.00094863
Iteration 22/25 | Loss: 0.00094863
Iteration 23/25 | Loss: 0.00094863
Iteration 24/25 | Loss: 0.00094863
Iteration 25/25 | Loss: 0.00094863

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60981321
Iteration 2/25 | Loss: 0.00048879
Iteration 3/25 | Loss: 0.00048879
Iteration 4/25 | Loss: 0.00048879
Iteration 5/25 | Loss: 0.00048879
Iteration 6/25 | Loss: 0.00048879
Iteration 7/25 | Loss: 0.00048879
Iteration 8/25 | Loss: 0.00048879
Iteration 9/25 | Loss: 0.00048879
Iteration 10/25 | Loss: 0.00048879
Iteration 11/25 | Loss: 0.00048879
Iteration 12/25 | Loss: 0.00048879
Iteration 13/25 | Loss: 0.00048879
Iteration 14/25 | Loss: 0.00048879
Iteration 15/25 | Loss: 0.00048879
Iteration 16/25 | Loss: 0.00048879
Iteration 17/25 | Loss: 0.00048879
Iteration 18/25 | Loss: 0.00048879
Iteration 19/25 | Loss: 0.00048879
Iteration 20/25 | Loss: 0.00048879
Iteration 21/25 | Loss: 0.00048879
Iteration 22/25 | Loss: 0.00048879
Iteration 23/25 | Loss: 0.00048879
Iteration 24/25 | Loss: 0.00048879
Iteration 25/25 | Loss: 0.00048879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048879
Iteration 2/1000 | Loss: 0.00002254
Iteration 3/1000 | Loss: 0.00001272
Iteration 4/1000 | Loss: 0.00001003
Iteration 5/1000 | Loss: 0.00000930
Iteration 6/1000 | Loss: 0.00000894
Iteration 7/1000 | Loss: 0.00000878
Iteration 8/1000 | Loss: 0.00000860
Iteration 9/1000 | Loss: 0.00000857
Iteration 10/1000 | Loss: 0.00000856
Iteration 11/1000 | Loss: 0.00000856
Iteration 12/1000 | Loss: 0.00000839
Iteration 13/1000 | Loss: 0.00000833
Iteration 14/1000 | Loss: 0.00000831
Iteration 15/1000 | Loss: 0.00000830
Iteration 16/1000 | Loss: 0.00000830
Iteration 17/1000 | Loss: 0.00000830
Iteration 18/1000 | Loss: 0.00000829
Iteration 19/1000 | Loss: 0.00000829
Iteration 20/1000 | Loss: 0.00000828
Iteration 21/1000 | Loss: 0.00000828
Iteration 22/1000 | Loss: 0.00000827
Iteration 23/1000 | Loss: 0.00000827
Iteration 24/1000 | Loss: 0.00000826
Iteration 25/1000 | Loss: 0.00000826
Iteration 26/1000 | Loss: 0.00000825
Iteration 27/1000 | Loss: 0.00000825
Iteration 28/1000 | Loss: 0.00000824
Iteration 29/1000 | Loss: 0.00000824
Iteration 30/1000 | Loss: 0.00000823
Iteration 31/1000 | Loss: 0.00000823
Iteration 32/1000 | Loss: 0.00000822
Iteration 33/1000 | Loss: 0.00000822
Iteration 34/1000 | Loss: 0.00000822
Iteration 35/1000 | Loss: 0.00000821
Iteration 36/1000 | Loss: 0.00000821
Iteration 37/1000 | Loss: 0.00000821
Iteration 38/1000 | Loss: 0.00000820
Iteration 39/1000 | Loss: 0.00000820
Iteration 40/1000 | Loss: 0.00000820
Iteration 41/1000 | Loss: 0.00000819
Iteration 42/1000 | Loss: 0.00000819
Iteration 43/1000 | Loss: 0.00000819
Iteration 44/1000 | Loss: 0.00000819
Iteration 45/1000 | Loss: 0.00000819
Iteration 46/1000 | Loss: 0.00000818
Iteration 47/1000 | Loss: 0.00000818
Iteration 48/1000 | Loss: 0.00000818
Iteration 49/1000 | Loss: 0.00000818
Iteration 50/1000 | Loss: 0.00000817
Iteration 51/1000 | Loss: 0.00000817
Iteration 52/1000 | Loss: 0.00000817
Iteration 53/1000 | Loss: 0.00000817
Iteration 54/1000 | Loss: 0.00000816
Iteration 55/1000 | Loss: 0.00000816
Iteration 56/1000 | Loss: 0.00000816
Iteration 57/1000 | Loss: 0.00000816
Iteration 58/1000 | Loss: 0.00000816
Iteration 59/1000 | Loss: 0.00000816
Iteration 60/1000 | Loss: 0.00000816
Iteration 61/1000 | Loss: 0.00000816
Iteration 62/1000 | Loss: 0.00000816
Iteration 63/1000 | Loss: 0.00000815
Iteration 64/1000 | Loss: 0.00000815
Iteration 65/1000 | Loss: 0.00000815
Iteration 66/1000 | Loss: 0.00000815
Iteration 67/1000 | Loss: 0.00000815
Iteration 68/1000 | Loss: 0.00000814
Iteration 69/1000 | Loss: 0.00000814
Iteration 70/1000 | Loss: 0.00000814
Iteration 71/1000 | Loss: 0.00000814
Iteration 72/1000 | Loss: 0.00000814
Iteration 73/1000 | Loss: 0.00000814
Iteration 74/1000 | Loss: 0.00000814
Iteration 75/1000 | Loss: 0.00000813
Iteration 76/1000 | Loss: 0.00000813
Iteration 77/1000 | Loss: 0.00000812
Iteration 78/1000 | Loss: 0.00000812
Iteration 79/1000 | Loss: 0.00000811
Iteration 80/1000 | Loss: 0.00000811
Iteration 81/1000 | Loss: 0.00000811
Iteration 82/1000 | Loss: 0.00000811
Iteration 83/1000 | Loss: 0.00000811
Iteration 84/1000 | Loss: 0.00000810
Iteration 85/1000 | Loss: 0.00000810
Iteration 86/1000 | Loss: 0.00000809
Iteration 87/1000 | Loss: 0.00000809
Iteration 88/1000 | Loss: 0.00000809
Iteration 89/1000 | Loss: 0.00000809
Iteration 90/1000 | Loss: 0.00000808
Iteration 91/1000 | Loss: 0.00000808
Iteration 92/1000 | Loss: 0.00000808
Iteration 93/1000 | Loss: 0.00000807
Iteration 94/1000 | Loss: 0.00000807
Iteration 95/1000 | Loss: 0.00000807
Iteration 96/1000 | Loss: 0.00000807
Iteration 97/1000 | Loss: 0.00000806
Iteration 98/1000 | Loss: 0.00000806
Iteration 99/1000 | Loss: 0.00000806
Iteration 100/1000 | Loss: 0.00000806
Iteration 101/1000 | Loss: 0.00000806
Iteration 102/1000 | Loss: 0.00000805
Iteration 103/1000 | Loss: 0.00000805
Iteration 104/1000 | Loss: 0.00000805
Iteration 105/1000 | Loss: 0.00000804
Iteration 106/1000 | Loss: 0.00000804
Iteration 107/1000 | Loss: 0.00000804
Iteration 108/1000 | Loss: 0.00000803
Iteration 109/1000 | Loss: 0.00000803
Iteration 110/1000 | Loss: 0.00000803
Iteration 111/1000 | Loss: 0.00000803
Iteration 112/1000 | Loss: 0.00000803
Iteration 113/1000 | Loss: 0.00000803
Iteration 114/1000 | Loss: 0.00000802
Iteration 115/1000 | Loss: 0.00000802
Iteration 116/1000 | Loss: 0.00000802
Iteration 117/1000 | Loss: 0.00000802
Iteration 118/1000 | Loss: 0.00000802
Iteration 119/1000 | Loss: 0.00000802
Iteration 120/1000 | Loss: 0.00000802
Iteration 121/1000 | Loss: 0.00000802
Iteration 122/1000 | Loss: 0.00000802
Iteration 123/1000 | Loss: 0.00000801
Iteration 124/1000 | Loss: 0.00000801
Iteration 125/1000 | Loss: 0.00000801
Iteration 126/1000 | Loss: 0.00000801
Iteration 127/1000 | Loss: 0.00000801
Iteration 128/1000 | Loss: 0.00000801
Iteration 129/1000 | Loss: 0.00000800
Iteration 130/1000 | Loss: 0.00000800
Iteration 131/1000 | Loss: 0.00000800
Iteration 132/1000 | Loss: 0.00000800
Iteration 133/1000 | Loss: 0.00000800
Iteration 134/1000 | Loss: 0.00000800
Iteration 135/1000 | Loss: 0.00000800
Iteration 136/1000 | Loss: 0.00000800
Iteration 137/1000 | Loss: 0.00000800
Iteration 138/1000 | Loss: 0.00000800
Iteration 139/1000 | Loss: 0.00000800
Iteration 140/1000 | Loss: 0.00000800
Iteration 141/1000 | Loss: 0.00000800
Iteration 142/1000 | Loss: 0.00000800
Iteration 143/1000 | Loss: 0.00000800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [7.998363798833452e-06, 7.998363798833452e-06, 7.998363798833452e-06, 7.998363798833452e-06, 7.998363798833452e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.998363798833452e-06

Optimization complete. Final v2v error: 2.436492443084717 mm

Highest mean error: 2.773108959197998 mm for frame 110

Lowest mean error: 2.2291171550750732 mm for frame 260

Saving results

Total time: 35.51533341407776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945530
Iteration 2/25 | Loss: 0.00403512
Iteration 3/25 | Loss: 0.00262131
Iteration 4/25 | Loss: 0.00216249
Iteration 5/25 | Loss: 0.00197501
Iteration 6/25 | Loss: 0.00188461
Iteration 7/25 | Loss: 0.00182693
Iteration 8/25 | Loss: 0.00180760
Iteration 9/25 | Loss: 0.00178926
Iteration 10/25 | Loss: 0.00180941
Iteration 11/25 | Loss: 0.00177098
Iteration 12/25 | Loss: 0.00168144
Iteration 13/25 | Loss: 0.00166369
Iteration 14/25 | Loss: 0.00167180
Iteration 15/25 | Loss: 0.00158545
Iteration 16/25 | Loss: 0.00145135
Iteration 17/25 | Loss: 0.00136346
Iteration 18/25 | Loss: 0.00132167
Iteration 19/25 | Loss: 0.00130282
Iteration 20/25 | Loss: 0.00124662
Iteration 21/25 | Loss: 0.00121713
Iteration 22/25 | Loss: 0.00119265
Iteration 23/25 | Loss: 0.00118390
Iteration 24/25 | Loss: 0.00119972
Iteration 25/25 | Loss: 0.00118452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31152546
Iteration 2/25 | Loss: 0.00292501
Iteration 3/25 | Loss: 0.00285058
Iteration 4/25 | Loss: 0.00285057
Iteration 5/25 | Loss: 0.00285057
Iteration 6/25 | Loss: 0.00285057
Iteration 7/25 | Loss: 0.00285057
Iteration 8/25 | Loss: 0.00285057
Iteration 9/25 | Loss: 0.00285057
Iteration 10/25 | Loss: 0.00285057
Iteration 11/25 | Loss: 0.00285057
Iteration 12/25 | Loss: 0.00285057
Iteration 13/25 | Loss: 0.00285057
Iteration 14/25 | Loss: 0.00285057
Iteration 15/25 | Loss: 0.00285057
Iteration 16/25 | Loss: 0.00285057
Iteration 17/25 | Loss: 0.00285057
Iteration 18/25 | Loss: 0.00285057
Iteration 19/25 | Loss: 0.00285057
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002850572345778346, 0.002850572345778346, 0.002850572345778346, 0.002850572345778346, 0.002850572345778346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002850572345778346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285057
Iteration 2/1000 | Loss: 0.00023953
Iteration 3/1000 | Loss: 0.00017709
Iteration 4/1000 | Loss: 0.00046800
Iteration 5/1000 | Loss: 0.00012025
Iteration 6/1000 | Loss: 0.00011367
Iteration 7/1000 | Loss: 0.00010881
Iteration 8/1000 | Loss: 0.00010509
Iteration 9/1000 | Loss: 0.00010352
Iteration 10/1000 | Loss: 0.00010196
Iteration 11/1000 | Loss: 0.00010086
Iteration 12/1000 | Loss: 0.00010018
Iteration 13/1000 | Loss: 0.00009948
Iteration 14/1000 | Loss: 0.00009885
Iteration 15/1000 | Loss: 0.00009832
Iteration 16/1000 | Loss: 0.00009778
Iteration 17/1000 | Loss: 0.00009735
Iteration 18/1000 | Loss: 0.00009686
Iteration 19/1000 | Loss: 0.00009645
Iteration 20/1000 | Loss: 0.00029126
Iteration 21/1000 | Loss: 0.00017031
Iteration 22/1000 | Loss: 0.00010012
Iteration 23/1000 | Loss: 0.00009706
Iteration 24/1000 | Loss: 0.00009624
Iteration 25/1000 | Loss: 0.00031169
Iteration 26/1000 | Loss: 0.00012467
Iteration 27/1000 | Loss: 0.00010131
Iteration 28/1000 | Loss: 0.00009681
Iteration 29/1000 | Loss: 0.00009618
Iteration 30/1000 | Loss: 0.00030257
Iteration 31/1000 | Loss: 0.00013542
Iteration 32/1000 | Loss: 0.00009758
Iteration 33/1000 | Loss: 0.00009625
Iteration 34/1000 | Loss: 0.00029954
Iteration 35/1000 | Loss: 0.00013300
Iteration 36/1000 | Loss: 0.00029392
Iteration 37/1000 | Loss: 0.00010147
Iteration 38/1000 | Loss: 0.00021015
Iteration 39/1000 | Loss: 0.00026890
Iteration 40/1000 | Loss: 0.00020268
Iteration 41/1000 | Loss: 0.00025113
Iteration 42/1000 | Loss: 0.00009953
Iteration 43/1000 | Loss: 0.00009893
Iteration 44/1000 | Loss: 0.00009859
Iteration 45/1000 | Loss: 0.00032377
Iteration 46/1000 | Loss: 0.00012194
Iteration 47/1000 | Loss: 0.00037458
Iteration 48/1000 | Loss: 0.00010045
Iteration 49/1000 | Loss: 0.00009748
Iteration 50/1000 | Loss: 0.00009659
Iteration 51/1000 | Loss: 0.00009640
Iteration 52/1000 | Loss: 0.00009623
Iteration 53/1000 | Loss: 0.00009622
Iteration 54/1000 | Loss: 0.00009621
Iteration 55/1000 | Loss: 0.00009610
Iteration 56/1000 | Loss: 0.00009604
Iteration 57/1000 | Loss: 0.00032825
Iteration 58/1000 | Loss: 0.00020702
Iteration 59/1000 | Loss: 0.00028477
Iteration 60/1000 | Loss: 0.00010557
Iteration 61/1000 | Loss: 0.00009941
Iteration 62/1000 | Loss: 0.00037686
Iteration 63/1000 | Loss: 0.00042747
Iteration 64/1000 | Loss: 0.00021538
Iteration 65/1000 | Loss: 0.00009866
Iteration 66/1000 | Loss: 0.00030440
Iteration 67/1000 | Loss: 0.00009860
Iteration 68/1000 | Loss: 0.00009705
Iteration 69/1000 | Loss: 0.00009661
Iteration 70/1000 | Loss: 0.00009635
Iteration 71/1000 | Loss: 0.00009607
Iteration 72/1000 | Loss: 0.00009604
Iteration 73/1000 | Loss: 0.00009602
Iteration 74/1000 | Loss: 0.00009585
Iteration 75/1000 | Loss: 0.00009568
Iteration 76/1000 | Loss: 0.00009557
Iteration 77/1000 | Loss: 0.00009554
Iteration 78/1000 | Loss: 0.00009537
Iteration 79/1000 | Loss: 0.00009521
Iteration 80/1000 | Loss: 0.00009514
Iteration 81/1000 | Loss: 0.00009513
Iteration 82/1000 | Loss: 0.00009509
Iteration 83/1000 | Loss: 0.00009502
Iteration 84/1000 | Loss: 0.00009502
Iteration 85/1000 | Loss: 0.00009502
Iteration 86/1000 | Loss: 0.00009500
Iteration 87/1000 | Loss: 0.00009500
Iteration 88/1000 | Loss: 0.00009500
Iteration 89/1000 | Loss: 0.00009500
Iteration 90/1000 | Loss: 0.00009500
Iteration 91/1000 | Loss: 0.00009500
Iteration 92/1000 | Loss: 0.00009500
Iteration 93/1000 | Loss: 0.00009500
Iteration 94/1000 | Loss: 0.00009499
Iteration 95/1000 | Loss: 0.00009499
Iteration 96/1000 | Loss: 0.00009499
Iteration 97/1000 | Loss: 0.00009498
Iteration 98/1000 | Loss: 0.00009498
Iteration 99/1000 | Loss: 0.00009497
Iteration 100/1000 | Loss: 0.00009497
Iteration 101/1000 | Loss: 0.00009497
Iteration 102/1000 | Loss: 0.00009497
Iteration 103/1000 | Loss: 0.00009497
Iteration 104/1000 | Loss: 0.00009496
Iteration 105/1000 | Loss: 0.00009496
Iteration 106/1000 | Loss: 0.00009496
Iteration 107/1000 | Loss: 0.00009496
Iteration 108/1000 | Loss: 0.00009496
Iteration 109/1000 | Loss: 0.00009496
Iteration 110/1000 | Loss: 0.00009496
Iteration 111/1000 | Loss: 0.00009496
Iteration 112/1000 | Loss: 0.00009496
Iteration 113/1000 | Loss: 0.00009496
Iteration 114/1000 | Loss: 0.00009496
Iteration 115/1000 | Loss: 0.00009496
Iteration 116/1000 | Loss: 0.00009496
Iteration 117/1000 | Loss: 0.00009496
Iteration 118/1000 | Loss: 0.00009496
Iteration 119/1000 | Loss: 0.00009496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [9.495623089605942e-05, 9.495623089605942e-05, 9.495623089605942e-05, 9.495623089605942e-05, 9.495623089605942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.495623089605942e-05

Optimization complete. Final v2v error: 4.569351673126221 mm

Highest mean error: 10.32612133026123 mm for frame 144

Lowest mean error: 2.798412561416626 mm for frame 39

Saving results

Total time: 158.0232870578766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774892
Iteration 2/25 | Loss: 0.00136344
Iteration 3/25 | Loss: 0.00106199
Iteration 4/25 | Loss: 0.00102060
Iteration 5/25 | Loss: 0.00099660
Iteration 6/25 | Loss: 0.00098362
Iteration 7/25 | Loss: 0.00097946
Iteration 8/25 | Loss: 0.00097656
Iteration 9/25 | Loss: 0.00097550
Iteration 10/25 | Loss: 0.00097603
Iteration 11/25 | Loss: 0.00097317
Iteration 12/25 | Loss: 0.00097243
Iteration 13/25 | Loss: 0.00097222
Iteration 14/25 | Loss: 0.00097214
Iteration 15/25 | Loss: 0.00097214
Iteration 16/25 | Loss: 0.00097214
Iteration 17/25 | Loss: 0.00097214
Iteration 18/25 | Loss: 0.00097214
Iteration 19/25 | Loss: 0.00097214
Iteration 20/25 | Loss: 0.00097214
Iteration 21/25 | Loss: 0.00097214
Iteration 22/25 | Loss: 0.00097214
Iteration 23/25 | Loss: 0.00097213
Iteration 24/25 | Loss: 0.00097213
Iteration 25/25 | Loss: 0.00097213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.81148243
Iteration 2/25 | Loss: 0.00073358
Iteration 3/25 | Loss: 0.00073345
Iteration 4/25 | Loss: 0.00073345
Iteration 5/25 | Loss: 0.00073345
Iteration 6/25 | Loss: 0.00073345
Iteration 7/25 | Loss: 0.00073345
Iteration 8/25 | Loss: 0.00073345
Iteration 9/25 | Loss: 0.00073345
Iteration 10/25 | Loss: 0.00073345
Iteration 11/25 | Loss: 0.00073345
Iteration 12/25 | Loss: 0.00073345
Iteration 13/25 | Loss: 0.00073345
Iteration 14/25 | Loss: 0.00073345
Iteration 15/25 | Loss: 0.00073345
Iteration 16/25 | Loss: 0.00073345
Iteration 17/25 | Loss: 0.00073345
Iteration 18/25 | Loss: 0.00073345
Iteration 19/25 | Loss: 0.00073345
Iteration 20/25 | Loss: 0.00073345
Iteration 21/25 | Loss: 0.00073345
Iteration 22/25 | Loss: 0.00073345
Iteration 23/25 | Loss: 0.00073345
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007334469119086862, 0.0007334469119086862, 0.0007334469119086862, 0.0007334469119086862, 0.0007334469119086862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007334469119086862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073345
Iteration 2/1000 | Loss: 0.00009239
Iteration 3/1000 | Loss: 0.00006573
Iteration 4/1000 | Loss: 0.00029481
Iteration 5/1000 | Loss: 0.00004409
Iteration 6/1000 | Loss: 0.00002461
Iteration 7/1000 | Loss: 0.00002121
Iteration 8/1000 | Loss: 0.00001857
Iteration 9/1000 | Loss: 0.00001767
Iteration 10/1000 | Loss: 0.00001695
Iteration 11/1000 | Loss: 0.00001653
Iteration 12/1000 | Loss: 0.00001645
Iteration 13/1000 | Loss: 0.00001637
Iteration 14/1000 | Loss: 0.00001613
Iteration 15/1000 | Loss: 0.00001604
Iteration 16/1000 | Loss: 0.00001599
Iteration 17/1000 | Loss: 0.00001586
Iteration 18/1000 | Loss: 0.00001585
Iteration 19/1000 | Loss: 0.00001584
Iteration 20/1000 | Loss: 0.00001584
Iteration 21/1000 | Loss: 0.00001579
Iteration 22/1000 | Loss: 0.00001573
Iteration 23/1000 | Loss: 0.00001571
Iteration 24/1000 | Loss: 0.00001571
Iteration 25/1000 | Loss: 0.00001569
Iteration 26/1000 | Loss: 0.00001569
Iteration 27/1000 | Loss: 0.00001569
Iteration 28/1000 | Loss: 0.00001569
Iteration 29/1000 | Loss: 0.00001569
Iteration 30/1000 | Loss: 0.00001569
Iteration 31/1000 | Loss: 0.00001569
Iteration 32/1000 | Loss: 0.00001569
Iteration 33/1000 | Loss: 0.00001569
Iteration 34/1000 | Loss: 0.00001568
Iteration 35/1000 | Loss: 0.00001568
Iteration 36/1000 | Loss: 0.00001568
Iteration 37/1000 | Loss: 0.00001567
Iteration 38/1000 | Loss: 0.00001567
Iteration 39/1000 | Loss: 0.00001566
Iteration 40/1000 | Loss: 0.00001566
Iteration 41/1000 | Loss: 0.00001565
Iteration 42/1000 | Loss: 0.00001565
Iteration 43/1000 | Loss: 0.00001565
Iteration 44/1000 | Loss: 0.00001564
Iteration 45/1000 | Loss: 0.00001563
Iteration 46/1000 | Loss: 0.00001563
Iteration 47/1000 | Loss: 0.00001562
Iteration 48/1000 | Loss: 0.00001562
Iteration 49/1000 | Loss: 0.00001562
Iteration 50/1000 | Loss: 0.00001562
Iteration 51/1000 | Loss: 0.00001562
Iteration 52/1000 | Loss: 0.00001562
Iteration 53/1000 | Loss: 0.00001562
Iteration 54/1000 | Loss: 0.00001561
Iteration 55/1000 | Loss: 0.00001561
Iteration 56/1000 | Loss: 0.00001561
Iteration 57/1000 | Loss: 0.00001560
Iteration 58/1000 | Loss: 0.00001560
Iteration 59/1000 | Loss: 0.00001559
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001558
Iteration 62/1000 | Loss: 0.00001558
Iteration 63/1000 | Loss: 0.00001557
Iteration 64/1000 | Loss: 0.00001557
Iteration 65/1000 | Loss: 0.00001557
Iteration 66/1000 | Loss: 0.00001556
Iteration 67/1000 | Loss: 0.00001556
Iteration 68/1000 | Loss: 0.00001556
Iteration 69/1000 | Loss: 0.00001556
Iteration 70/1000 | Loss: 0.00001555
Iteration 71/1000 | Loss: 0.00001555
Iteration 72/1000 | Loss: 0.00001555
Iteration 73/1000 | Loss: 0.00001555
Iteration 74/1000 | Loss: 0.00001555
Iteration 75/1000 | Loss: 0.00001555
Iteration 76/1000 | Loss: 0.00001555
Iteration 77/1000 | Loss: 0.00001555
Iteration 78/1000 | Loss: 0.00001554
Iteration 79/1000 | Loss: 0.00001554
Iteration 80/1000 | Loss: 0.00001554
Iteration 81/1000 | Loss: 0.00001554
Iteration 82/1000 | Loss: 0.00001554
Iteration 83/1000 | Loss: 0.00001554
Iteration 84/1000 | Loss: 0.00001553
Iteration 85/1000 | Loss: 0.00001553
Iteration 86/1000 | Loss: 0.00001553
Iteration 87/1000 | Loss: 0.00001553
Iteration 88/1000 | Loss: 0.00001553
Iteration 89/1000 | Loss: 0.00001553
Iteration 90/1000 | Loss: 0.00001553
Iteration 91/1000 | Loss: 0.00001553
Iteration 92/1000 | Loss: 0.00001553
Iteration 93/1000 | Loss: 0.00001552
Iteration 94/1000 | Loss: 0.00001552
Iteration 95/1000 | Loss: 0.00001552
Iteration 96/1000 | Loss: 0.00001552
Iteration 97/1000 | Loss: 0.00001552
Iteration 98/1000 | Loss: 0.00001552
Iteration 99/1000 | Loss: 0.00001552
Iteration 100/1000 | Loss: 0.00001552
Iteration 101/1000 | Loss: 0.00001552
Iteration 102/1000 | Loss: 0.00001552
Iteration 103/1000 | Loss: 0.00001552
Iteration 104/1000 | Loss: 0.00001552
Iteration 105/1000 | Loss: 0.00001552
Iteration 106/1000 | Loss: 0.00001552
Iteration 107/1000 | Loss: 0.00001552
Iteration 108/1000 | Loss: 0.00001552
Iteration 109/1000 | Loss: 0.00001552
Iteration 110/1000 | Loss: 0.00001551
Iteration 111/1000 | Loss: 0.00001551
Iteration 112/1000 | Loss: 0.00001551
Iteration 113/1000 | Loss: 0.00001551
Iteration 114/1000 | Loss: 0.00001551
Iteration 115/1000 | Loss: 0.00001551
Iteration 116/1000 | Loss: 0.00001551
Iteration 117/1000 | Loss: 0.00001551
Iteration 118/1000 | Loss: 0.00001551
Iteration 119/1000 | Loss: 0.00001551
Iteration 120/1000 | Loss: 0.00001551
Iteration 121/1000 | Loss: 0.00001551
Iteration 122/1000 | Loss: 0.00001550
Iteration 123/1000 | Loss: 0.00001550
Iteration 124/1000 | Loss: 0.00001550
Iteration 125/1000 | Loss: 0.00001550
Iteration 126/1000 | Loss: 0.00001549
Iteration 127/1000 | Loss: 0.00001549
Iteration 128/1000 | Loss: 0.00001549
Iteration 129/1000 | Loss: 0.00001548
Iteration 130/1000 | Loss: 0.00001548
Iteration 131/1000 | Loss: 0.00001548
Iteration 132/1000 | Loss: 0.00001548
Iteration 133/1000 | Loss: 0.00001547
Iteration 134/1000 | Loss: 0.00001547
Iteration 135/1000 | Loss: 0.00001547
Iteration 136/1000 | Loss: 0.00001547
Iteration 137/1000 | Loss: 0.00001546
Iteration 138/1000 | Loss: 0.00001546
Iteration 139/1000 | Loss: 0.00001546
Iteration 140/1000 | Loss: 0.00001546
Iteration 141/1000 | Loss: 0.00001545
Iteration 142/1000 | Loss: 0.00001545
Iteration 143/1000 | Loss: 0.00001545
Iteration 144/1000 | Loss: 0.00001545
Iteration 145/1000 | Loss: 0.00001545
Iteration 146/1000 | Loss: 0.00001544
Iteration 147/1000 | Loss: 0.00001544
Iteration 148/1000 | Loss: 0.00001544
Iteration 149/1000 | Loss: 0.00001543
Iteration 150/1000 | Loss: 0.00001543
Iteration 151/1000 | Loss: 0.00001543
Iteration 152/1000 | Loss: 0.00001543
Iteration 153/1000 | Loss: 0.00001542
Iteration 154/1000 | Loss: 0.00001542
Iteration 155/1000 | Loss: 0.00001542
Iteration 156/1000 | Loss: 0.00001542
Iteration 157/1000 | Loss: 0.00001542
Iteration 158/1000 | Loss: 0.00001542
Iteration 159/1000 | Loss: 0.00001542
Iteration 160/1000 | Loss: 0.00001542
Iteration 161/1000 | Loss: 0.00001542
Iteration 162/1000 | Loss: 0.00001542
Iteration 163/1000 | Loss: 0.00001542
Iteration 164/1000 | Loss: 0.00001542
Iteration 165/1000 | Loss: 0.00001541
Iteration 166/1000 | Loss: 0.00001541
Iteration 167/1000 | Loss: 0.00001541
Iteration 168/1000 | Loss: 0.00001541
Iteration 169/1000 | Loss: 0.00001541
Iteration 170/1000 | Loss: 0.00001541
Iteration 171/1000 | Loss: 0.00001541
Iteration 172/1000 | Loss: 0.00001541
Iteration 173/1000 | Loss: 0.00001541
Iteration 174/1000 | Loss: 0.00001541
Iteration 175/1000 | Loss: 0.00001541
Iteration 176/1000 | Loss: 0.00001540
Iteration 177/1000 | Loss: 0.00001540
Iteration 178/1000 | Loss: 0.00001540
Iteration 179/1000 | Loss: 0.00001540
Iteration 180/1000 | Loss: 0.00001540
Iteration 181/1000 | Loss: 0.00001540
Iteration 182/1000 | Loss: 0.00001540
Iteration 183/1000 | Loss: 0.00001539
Iteration 184/1000 | Loss: 0.00001539
Iteration 185/1000 | Loss: 0.00001539
Iteration 186/1000 | Loss: 0.00001539
Iteration 187/1000 | Loss: 0.00001539
Iteration 188/1000 | Loss: 0.00001539
Iteration 189/1000 | Loss: 0.00001539
Iteration 190/1000 | Loss: 0.00001539
Iteration 191/1000 | Loss: 0.00001539
Iteration 192/1000 | Loss: 0.00001539
Iteration 193/1000 | Loss: 0.00001539
Iteration 194/1000 | Loss: 0.00001539
Iteration 195/1000 | Loss: 0.00001539
Iteration 196/1000 | Loss: 0.00001539
Iteration 197/1000 | Loss: 0.00001539
Iteration 198/1000 | Loss: 0.00001539
Iteration 199/1000 | Loss: 0.00001539
Iteration 200/1000 | Loss: 0.00001539
Iteration 201/1000 | Loss: 0.00001539
Iteration 202/1000 | Loss: 0.00001539
Iteration 203/1000 | Loss: 0.00001539
Iteration 204/1000 | Loss: 0.00001538
Iteration 205/1000 | Loss: 0.00001538
Iteration 206/1000 | Loss: 0.00001538
Iteration 207/1000 | Loss: 0.00001538
Iteration 208/1000 | Loss: 0.00001538
Iteration 209/1000 | Loss: 0.00001538
Iteration 210/1000 | Loss: 0.00001538
Iteration 211/1000 | Loss: 0.00001538
Iteration 212/1000 | Loss: 0.00001538
Iteration 213/1000 | Loss: 0.00001538
Iteration 214/1000 | Loss: 0.00001538
Iteration 215/1000 | Loss: 0.00001538
Iteration 216/1000 | Loss: 0.00001538
Iteration 217/1000 | Loss: 0.00001538
Iteration 218/1000 | Loss: 0.00001538
Iteration 219/1000 | Loss: 0.00001538
Iteration 220/1000 | Loss: 0.00001538
Iteration 221/1000 | Loss: 0.00001538
Iteration 222/1000 | Loss: 0.00001538
Iteration 223/1000 | Loss: 0.00001538
Iteration 224/1000 | Loss: 0.00001538
Iteration 225/1000 | Loss: 0.00001538
Iteration 226/1000 | Loss: 0.00001538
Iteration 227/1000 | Loss: 0.00001538
Iteration 228/1000 | Loss: 0.00001538
Iteration 229/1000 | Loss: 0.00001538
Iteration 230/1000 | Loss: 0.00001537
Iteration 231/1000 | Loss: 0.00001537
Iteration 232/1000 | Loss: 0.00001537
Iteration 233/1000 | Loss: 0.00001537
Iteration 234/1000 | Loss: 0.00001537
Iteration 235/1000 | Loss: 0.00001537
Iteration 236/1000 | Loss: 0.00001537
Iteration 237/1000 | Loss: 0.00001537
Iteration 238/1000 | Loss: 0.00001537
Iteration 239/1000 | Loss: 0.00001537
Iteration 240/1000 | Loss: 0.00001537
Iteration 241/1000 | Loss: 0.00001537
Iteration 242/1000 | Loss: 0.00001537
Iteration 243/1000 | Loss: 0.00001537
Iteration 244/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [1.537173920951318e-05, 1.537173920951318e-05, 1.537173920951318e-05, 1.537173920951318e-05, 1.537173920951318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.537173920951318e-05

Optimization complete. Final v2v error: 3.1615922451019287 mm

Highest mean error: 8.842288970947266 mm for frame 75

Lowest mean error: 2.404121160507202 mm for frame 117

Saving results

Total time: 69.17543435096741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072843
Iteration 2/25 | Loss: 0.00253992
Iteration 3/25 | Loss: 0.00193653
Iteration 4/25 | Loss: 0.00186167
Iteration 5/25 | Loss: 0.00183354
Iteration 6/25 | Loss: 0.00182429
Iteration 7/25 | Loss: 0.00181654
Iteration 8/25 | Loss: 0.00181499
Iteration 9/25 | Loss: 0.00181161
Iteration 10/25 | Loss: 0.00180881
Iteration 11/25 | Loss: 0.00180717
Iteration 12/25 | Loss: 0.00180659
Iteration 13/25 | Loss: 0.00180641
Iteration 14/25 | Loss: 0.00180641
Iteration 15/25 | Loss: 0.00180641
Iteration 16/25 | Loss: 0.00180641
Iteration 17/25 | Loss: 0.00180641
Iteration 18/25 | Loss: 0.00180641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018064120085909963, 0.0018064120085909963, 0.0018064120085909963, 0.0018064120085909963, 0.0018064120085909963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018064120085909963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31311584
Iteration 2/25 | Loss: 0.00532375
Iteration 3/25 | Loss: 0.00532375
Iteration 4/25 | Loss: 0.00532375
Iteration 5/25 | Loss: 0.00532375
Iteration 6/25 | Loss: 0.00532375
Iteration 7/25 | Loss: 0.00532375
Iteration 8/25 | Loss: 0.00532375
Iteration 9/25 | Loss: 0.00532375
Iteration 10/25 | Loss: 0.00532375
Iteration 11/25 | Loss: 0.00532375
Iteration 12/25 | Loss: 0.00532375
Iteration 13/25 | Loss: 0.00532374
Iteration 14/25 | Loss: 0.00532375
Iteration 15/25 | Loss: 0.00532375
Iteration 16/25 | Loss: 0.00532374
Iteration 17/25 | Loss: 0.00532375
Iteration 18/25 | Loss: 0.00532374
Iteration 19/25 | Loss: 0.00532374
Iteration 20/25 | Loss: 0.00532374
Iteration 21/25 | Loss: 0.00532375
Iteration 22/25 | Loss: 0.00532374
Iteration 23/25 | Loss: 0.00532375
Iteration 24/25 | Loss: 0.00532375
Iteration 25/25 | Loss: 0.00532374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00532374
Iteration 2/1000 | Loss: 0.00082640
Iteration 3/1000 | Loss: 0.00060219
Iteration 4/1000 | Loss: 0.00052590
Iteration 5/1000 | Loss: 0.00046883
Iteration 6/1000 | Loss: 0.00042171
Iteration 7/1000 | Loss: 0.00038913
Iteration 8/1000 | Loss: 0.01303389
Iteration 9/1000 | Loss: 0.01222403
Iteration 10/1000 | Loss: 0.00109803
Iteration 11/1000 | Loss: 0.00047655
Iteration 12/1000 | Loss: 0.00028886
Iteration 13/1000 | Loss: 0.00020872
Iteration 14/1000 | Loss: 0.00014668
Iteration 15/1000 | Loss: 0.00009531
Iteration 16/1000 | Loss: 0.00007046
Iteration 17/1000 | Loss: 0.00005202
Iteration 18/1000 | Loss: 0.00004114
Iteration 19/1000 | Loss: 0.00003189
Iteration 20/1000 | Loss: 0.00002624
Iteration 21/1000 | Loss: 0.00002283
Iteration 22/1000 | Loss: 0.00002062
Iteration 23/1000 | Loss: 0.00001848
Iteration 24/1000 | Loss: 0.00001694
Iteration 25/1000 | Loss: 0.00001621
Iteration 26/1000 | Loss: 0.00001521
Iteration 27/1000 | Loss: 0.00001444
Iteration 28/1000 | Loss: 0.00001392
Iteration 29/1000 | Loss: 0.00001346
Iteration 30/1000 | Loss: 0.00001322
Iteration 31/1000 | Loss: 0.00001299
Iteration 32/1000 | Loss: 0.00001287
Iteration 33/1000 | Loss: 0.00001285
Iteration 34/1000 | Loss: 0.00001283
Iteration 35/1000 | Loss: 0.00001283
Iteration 36/1000 | Loss: 0.00001283
Iteration 37/1000 | Loss: 0.00001282
Iteration 38/1000 | Loss: 0.00001280
Iteration 39/1000 | Loss: 0.00001280
Iteration 40/1000 | Loss: 0.00001279
Iteration 41/1000 | Loss: 0.00001274
Iteration 42/1000 | Loss: 0.00001274
Iteration 43/1000 | Loss: 0.00001272
Iteration 44/1000 | Loss: 0.00001272
Iteration 45/1000 | Loss: 0.00001272
Iteration 46/1000 | Loss: 0.00001272
Iteration 47/1000 | Loss: 0.00001272
Iteration 48/1000 | Loss: 0.00001272
Iteration 49/1000 | Loss: 0.00001272
Iteration 50/1000 | Loss: 0.00001272
Iteration 51/1000 | Loss: 0.00001272
Iteration 52/1000 | Loss: 0.00001271
Iteration 53/1000 | Loss: 0.00001271
Iteration 54/1000 | Loss: 0.00001271
Iteration 55/1000 | Loss: 0.00001271
Iteration 56/1000 | Loss: 0.00001271
Iteration 57/1000 | Loss: 0.00001270
Iteration 58/1000 | Loss: 0.00001270
Iteration 59/1000 | Loss: 0.00001270
Iteration 60/1000 | Loss: 0.00001270
Iteration 61/1000 | Loss: 0.00001270
Iteration 62/1000 | Loss: 0.00001269
Iteration 63/1000 | Loss: 0.00001269
Iteration 64/1000 | Loss: 0.00001269
Iteration 65/1000 | Loss: 0.00001268
Iteration 66/1000 | Loss: 0.00001268
Iteration 67/1000 | Loss: 0.00001268
Iteration 68/1000 | Loss: 0.00001268
Iteration 69/1000 | Loss: 0.00001268
Iteration 70/1000 | Loss: 0.00001268
Iteration 71/1000 | Loss: 0.00001268
Iteration 72/1000 | Loss: 0.00001268
Iteration 73/1000 | Loss: 0.00001268
Iteration 74/1000 | Loss: 0.00001268
Iteration 75/1000 | Loss: 0.00001267
Iteration 76/1000 | Loss: 0.00001267
Iteration 77/1000 | Loss: 0.00001267
Iteration 78/1000 | Loss: 0.00001266
Iteration 79/1000 | Loss: 0.00001266
Iteration 80/1000 | Loss: 0.00001266
Iteration 81/1000 | Loss: 0.00001266
Iteration 82/1000 | Loss: 0.00001265
Iteration 83/1000 | Loss: 0.00001265
Iteration 84/1000 | Loss: 0.00001265
Iteration 85/1000 | Loss: 0.00001265
Iteration 86/1000 | Loss: 0.00001265
Iteration 87/1000 | Loss: 0.00001264
Iteration 88/1000 | Loss: 0.00001264
Iteration 89/1000 | Loss: 0.00001264
Iteration 90/1000 | Loss: 0.00001263
Iteration 91/1000 | Loss: 0.00001263
Iteration 92/1000 | Loss: 0.00001263
Iteration 93/1000 | Loss: 0.00001263
Iteration 94/1000 | Loss: 0.00001263
Iteration 95/1000 | Loss: 0.00001263
Iteration 96/1000 | Loss: 0.00001263
Iteration 97/1000 | Loss: 0.00001263
Iteration 98/1000 | Loss: 0.00001263
Iteration 99/1000 | Loss: 0.00001263
Iteration 100/1000 | Loss: 0.00001263
Iteration 101/1000 | Loss: 0.00001263
Iteration 102/1000 | Loss: 0.00001262
Iteration 103/1000 | Loss: 0.00001262
Iteration 104/1000 | Loss: 0.00001262
Iteration 105/1000 | Loss: 0.00001262
Iteration 106/1000 | Loss: 0.00001262
Iteration 107/1000 | Loss: 0.00001262
Iteration 108/1000 | Loss: 0.00001262
Iteration 109/1000 | Loss: 0.00001262
Iteration 110/1000 | Loss: 0.00001262
Iteration 111/1000 | Loss: 0.00001261
Iteration 112/1000 | Loss: 0.00001261
Iteration 113/1000 | Loss: 0.00001261
Iteration 114/1000 | Loss: 0.00001261
Iteration 115/1000 | Loss: 0.00001261
Iteration 116/1000 | Loss: 0.00001261
Iteration 117/1000 | Loss: 0.00001261
Iteration 118/1000 | Loss: 0.00001260
Iteration 119/1000 | Loss: 0.00001260
Iteration 120/1000 | Loss: 0.00001260
Iteration 121/1000 | Loss: 0.00001260
Iteration 122/1000 | Loss: 0.00001260
Iteration 123/1000 | Loss: 0.00001260
Iteration 124/1000 | Loss: 0.00001260
Iteration 125/1000 | Loss: 0.00001260
Iteration 126/1000 | Loss: 0.00001260
Iteration 127/1000 | Loss: 0.00001260
Iteration 128/1000 | Loss: 0.00001259
Iteration 129/1000 | Loss: 0.00001259
Iteration 130/1000 | Loss: 0.00001259
Iteration 131/1000 | Loss: 0.00001259
Iteration 132/1000 | Loss: 0.00001259
Iteration 133/1000 | Loss: 0.00001259
Iteration 134/1000 | Loss: 0.00001259
Iteration 135/1000 | Loss: 0.00001259
Iteration 136/1000 | Loss: 0.00001258
Iteration 137/1000 | Loss: 0.00001258
Iteration 138/1000 | Loss: 0.00001258
Iteration 139/1000 | Loss: 0.00001258
Iteration 140/1000 | Loss: 0.00001258
Iteration 141/1000 | Loss: 0.00001258
Iteration 142/1000 | Loss: 0.00001258
Iteration 143/1000 | Loss: 0.00001258
Iteration 144/1000 | Loss: 0.00001258
Iteration 145/1000 | Loss: 0.00001258
Iteration 146/1000 | Loss: 0.00001258
Iteration 147/1000 | Loss: 0.00001258
Iteration 148/1000 | Loss: 0.00001258
Iteration 149/1000 | Loss: 0.00001257
Iteration 150/1000 | Loss: 0.00001257
Iteration 151/1000 | Loss: 0.00001257
Iteration 152/1000 | Loss: 0.00001257
Iteration 153/1000 | Loss: 0.00001257
Iteration 154/1000 | Loss: 0.00001257
Iteration 155/1000 | Loss: 0.00001257
Iteration 156/1000 | Loss: 0.00001257
Iteration 157/1000 | Loss: 0.00001257
Iteration 158/1000 | Loss: 0.00001257
Iteration 159/1000 | Loss: 0.00001257
Iteration 160/1000 | Loss: 0.00001257
Iteration 161/1000 | Loss: 0.00001257
Iteration 162/1000 | Loss: 0.00001257
Iteration 163/1000 | Loss: 0.00001256
Iteration 164/1000 | Loss: 0.00001256
Iteration 165/1000 | Loss: 0.00001256
Iteration 166/1000 | Loss: 0.00001256
Iteration 167/1000 | Loss: 0.00001256
Iteration 168/1000 | Loss: 0.00001256
Iteration 169/1000 | Loss: 0.00001256
Iteration 170/1000 | Loss: 0.00001256
Iteration 171/1000 | Loss: 0.00001256
Iteration 172/1000 | Loss: 0.00001256
Iteration 173/1000 | Loss: 0.00001256
Iteration 174/1000 | Loss: 0.00001256
Iteration 175/1000 | Loss: 0.00001255
Iteration 176/1000 | Loss: 0.00001255
Iteration 177/1000 | Loss: 0.00001255
Iteration 178/1000 | Loss: 0.00001255
Iteration 179/1000 | Loss: 0.00001255
Iteration 180/1000 | Loss: 0.00001255
Iteration 181/1000 | Loss: 0.00001255
Iteration 182/1000 | Loss: 0.00001255
Iteration 183/1000 | Loss: 0.00001255
Iteration 184/1000 | Loss: 0.00001255
Iteration 185/1000 | Loss: 0.00001255
Iteration 186/1000 | Loss: 0.00001255
Iteration 187/1000 | Loss: 0.00001255
Iteration 188/1000 | Loss: 0.00001255
Iteration 189/1000 | Loss: 0.00001255
Iteration 190/1000 | Loss: 0.00001255
Iteration 191/1000 | Loss: 0.00001255
Iteration 192/1000 | Loss: 0.00001255
Iteration 193/1000 | Loss: 0.00001255
Iteration 194/1000 | Loss: 0.00001255
Iteration 195/1000 | Loss: 0.00001255
Iteration 196/1000 | Loss: 0.00001255
Iteration 197/1000 | Loss: 0.00001255
Iteration 198/1000 | Loss: 0.00001255
Iteration 199/1000 | Loss: 0.00001255
Iteration 200/1000 | Loss: 0.00001255
Iteration 201/1000 | Loss: 0.00001255
Iteration 202/1000 | Loss: 0.00001255
Iteration 203/1000 | Loss: 0.00001255
Iteration 204/1000 | Loss: 0.00001255
Iteration 205/1000 | Loss: 0.00001255
Iteration 206/1000 | Loss: 0.00001255
Iteration 207/1000 | Loss: 0.00001255
Iteration 208/1000 | Loss: 0.00001255
Iteration 209/1000 | Loss: 0.00001255
Iteration 210/1000 | Loss: 0.00001255
Iteration 211/1000 | Loss: 0.00001255
Iteration 212/1000 | Loss: 0.00001255
Iteration 213/1000 | Loss: 0.00001255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.2554060049296822e-05, 1.2554060049296822e-05, 1.2554060049296822e-05, 1.2554060049296822e-05, 1.2554060049296822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2554060049296822e-05

Optimization complete. Final v2v error: 2.9512593746185303 mm

Highest mean error: 2.998610019683838 mm for frame 81

Lowest mean error: 2.8891282081604004 mm for frame 3

Saving results

Total time: 75.42514705657959
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00738371
Iteration 2/25 | Loss: 0.00122767
Iteration 3/25 | Loss: 0.00100163
Iteration 4/25 | Loss: 0.00094630
Iteration 5/25 | Loss: 0.00094150
Iteration 6/25 | Loss: 0.00093073
Iteration 7/25 | Loss: 0.00092986
Iteration 8/25 | Loss: 0.00092965
Iteration 9/25 | Loss: 0.00092952
Iteration 10/25 | Loss: 0.00092951
Iteration 11/25 | Loss: 0.00092951
Iteration 12/25 | Loss: 0.00092951
Iteration 13/25 | Loss: 0.00092951
Iteration 14/25 | Loss: 0.00092951
Iteration 15/25 | Loss: 0.00092951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009295140043832362, 0.0009295140043832362, 0.0009295140043832362, 0.0009295140043832362, 0.0009295140043832362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009295140043832362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85969174
Iteration 2/25 | Loss: 0.00068520
Iteration 3/25 | Loss: 0.00068520
Iteration 4/25 | Loss: 0.00068520
Iteration 5/25 | Loss: 0.00068519
Iteration 6/25 | Loss: 0.00068519
Iteration 7/25 | Loss: 0.00068519
Iteration 8/25 | Loss: 0.00068519
Iteration 9/25 | Loss: 0.00068519
Iteration 10/25 | Loss: 0.00068519
Iteration 11/25 | Loss: 0.00068519
Iteration 12/25 | Loss: 0.00068519
Iteration 13/25 | Loss: 0.00068519
Iteration 14/25 | Loss: 0.00068519
Iteration 15/25 | Loss: 0.00068519
Iteration 16/25 | Loss: 0.00068519
Iteration 17/25 | Loss: 0.00068519
Iteration 18/25 | Loss: 0.00068519
Iteration 19/25 | Loss: 0.00068519
Iteration 20/25 | Loss: 0.00068519
Iteration 21/25 | Loss: 0.00068519
Iteration 22/25 | Loss: 0.00068519
Iteration 23/25 | Loss: 0.00068519
Iteration 24/25 | Loss: 0.00068519
Iteration 25/25 | Loss: 0.00068519

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068519
Iteration 2/1000 | Loss: 0.00001663
Iteration 3/1000 | Loss: 0.00001264
Iteration 4/1000 | Loss: 0.00001172
Iteration 5/1000 | Loss: 0.00013106
Iteration 6/1000 | Loss: 0.00010272
Iteration 7/1000 | Loss: 0.00002131
Iteration 8/1000 | Loss: 0.00006632
Iteration 9/1000 | Loss: 0.00001376
Iteration 10/1000 | Loss: 0.00001125
Iteration 11/1000 | Loss: 0.00001092
Iteration 12/1000 | Loss: 0.00001078
Iteration 13/1000 | Loss: 0.00001076
Iteration 14/1000 | Loss: 0.00001075
Iteration 15/1000 | Loss: 0.00001071
Iteration 16/1000 | Loss: 0.00001071
Iteration 17/1000 | Loss: 0.00001071
Iteration 18/1000 | Loss: 0.00001071
Iteration 19/1000 | Loss: 0.00001070
Iteration 20/1000 | Loss: 0.00001066
Iteration 21/1000 | Loss: 0.00001064
Iteration 22/1000 | Loss: 0.00001064
Iteration 23/1000 | Loss: 0.00001061
Iteration 24/1000 | Loss: 0.00001060
Iteration 25/1000 | Loss: 0.00001058
Iteration 26/1000 | Loss: 0.00001057
Iteration 27/1000 | Loss: 0.00001057
Iteration 28/1000 | Loss: 0.00001057
Iteration 29/1000 | Loss: 0.00001056
Iteration 30/1000 | Loss: 0.00001053
Iteration 31/1000 | Loss: 0.00001048
Iteration 32/1000 | Loss: 0.00001048
Iteration 33/1000 | Loss: 0.00001048
Iteration 34/1000 | Loss: 0.00001047
Iteration 35/1000 | Loss: 0.00001047
Iteration 36/1000 | Loss: 0.00001047
Iteration 37/1000 | Loss: 0.00001047
Iteration 38/1000 | Loss: 0.00001047
Iteration 39/1000 | Loss: 0.00001047
Iteration 40/1000 | Loss: 0.00001047
Iteration 41/1000 | Loss: 0.00001047
Iteration 42/1000 | Loss: 0.00001046
Iteration 43/1000 | Loss: 0.00001046
Iteration 44/1000 | Loss: 0.00001046
Iteration 45/1000 | Loss: 0.00001045
Iteration 46/1000 | Loss: 0.00001044
Iteration 47/1000 | Loss: 0.00001044
Iteration 48/1000 | Loss: 0.00001044
Iteration 49/1000 | Loss: 0.00001044
Iteration 50/1000 | Loss: 0.00001044
Iteration 51/1000 | Loss: 0.00001044
Iteration 52/1000 | Loss: 0.00001043
Iteration 53/1000 | Loss: 0.00001043
Iteration 54/1000 | Loss: 0.00001043
Iteration 55/1000 | Loss: 0.00001043
Iteration 56/1000 | Loss: 0.00001043
Iteration 57/1000 | Loss: 0.00001043
Iteration 58/1000 | Loss: 0.00001043
Iteration 59/1000 | Loss: 0.00001042
Iteration 60/1000 | Loss: 0.00001042
Iteration 61/1000 | Loss: 0.00001042
Iteration 62/1000 | Loss: 0.00001042
Iteration 63/1000 | Loss: 0.00001042
Iteration 64/1000 | Loss: 0.00001042
Iteration 65/1000 | Loss: 0.00001042
Iteration 66/1000 | Loss: 0.00001042
Iteration 67/1000 | Loss: 0.00001042
Iteration 68/1000 | Loss: 0.00001041
Iteration 69/1000 | Loss: 0.00001040
Iteration 70/1000 | Loss: 0.00001040
Iteration 71/1000 | Loss: 0.00001040
Iteration 72/1000 | Loss: 0.00001040
Iteration 73/1000 | Loss: 0.00001040
Iteration 74/1000 | Loss: 0.00001040
Iteration 75/1000 | Loss: 0.00001040
Iteration 76/1000 | Loss: 0.00001040
Iteration 77/1000 | Loss: 0.00001039
Iteration 78/1000 | Loss: 0.00001039
Iteration 79/1000 | Loss: 0.00001039
Iteration 80/1000 | Loss: 0.00001038
Iteration 81/1000 | Loss: 0.00001038
Iteration 82/1000 | Loss: 0.00001037
Iteration 83/1000 | Loss: 0.00001037
Iteration 84/1000 | Loss: 0.00001036
Iteration 85/1000 | Loss: 0.00001036
Iteration 86/1000 | Loss: 0.00001035
Iteration 87/1000 | Loss: 0.00001035
Iteration 88/1000 | Loss: 0.00001035
Iteration 89/1000 | Loss: 0.00001035
Iteration 90/1000 | Loss: 0.00001035
Iteration 91/1000 | Loss: 0.00001035
Iteration 92/1000 | Loss: 0.00001035
Iteration 93/1000 | Loss: 0.00001035
Iteration 94/1000 | Loss: 0.00001035
Iteration 95/1000 | Loss: 0.00001035
Iteration 96/1000 | Loss: 0.00001035
Iteration 97/1000 | Loss: 0.00001035
Iteration 98/1000 | Loss: 0.00001035
Iteration 99/1000 | Loss: 0.00001035
Iteration 100/1000 | Loss: 0.00001035
Iteration 101/1000 | Loss: 0.00001035
Iteration 102/1000 | Loss: 0.00001035
Iteration 103/1000 | Loss: 0.00001035
Iteration 104/1000 | Loss: 0.00001035
Iteration 105/1000 | Loss: 0.00001035
Iteration 106/1000 | Loss: 0.00001035
Iteration 107/1000 | Loss: 0.00001035
Iteration 108/1000 | Loss: 0.00001035
Iteration 109/1000 | Loss: 0.00001035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.0348771866119932e-05, 1.0348771866119932e-05, 1.0348771866119932e-05, 1.0348771866119932e-05, 1.0348771866119932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0348771866119932e-05

Optimization complete. Final v2v error: 2.684330940246582 mm

Highest mean error: 6.412621021270752 mm for frame 141

Lowest mean error: 2.5134496688842773 mm for frame 52

Saving results

Total time: 45.880561113357544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840769
Iteration 2/25 | Loss: 0.00179936
Iteration 3/25 | Loss: 0.00130083
Iteration 4/25 | Loss: 0.00135797
Iteration 5/25 | Loss: 0.00119485
Iteration 6/25 | Loss: 0.00115980
Iteration 7/25 | Loss: 0.00111802
Iteration 8/25 | Loss: 0.00109919
Iteration 9/25 | Loss: 0.00111118
Iteration 10/25 | Loss: 0.00108639
Iteration 11/25 | Loss: 0.00110615
Iteration 12/25 | Loss: 0.00107543
Iteration 13/25 | Loss: 0.00107002
Iteration 14/25 | Loss: 0.00107065
Iteration 15/25 | Loss: 0.00107071
Iteration 16/25 | Loss: 0.00106982
Iteration 17/25 | Loss: 0.00106775
Iteration 18/25 | Loss: 0.00106760
Iteration 19/25 | Loss: 0.00106757
Iteration 20/25 | Loss: 0.00106757
Iteration 21/25 | Loss: 0.00106757
Iteration 22/25 | Loss: 0.00106757
Iteration 23/25 | Loss: 0.00106757
Iteration 24/25 | Loss: 0.00106757
Iteration 25/25 | Loss: 0.00106756

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36592317
Iteration 2/25 | Loss: 0.00080354
Iteration 3/25 | Loss: 0.00080353
Iteration 4/25 | Loss: 0.00080353
Iteration 5/25 | Loss: 0.00080353
Iteration 6/25 | Loss: 0.00080353
Iteration 7/25 | Loss: 0.00080353
Iteration 8/25 | Loss: 0.00080353
Iteration 9/25 | Loss: 0.00080353
Iteration 10/25 | Loss: 0.00080353
Iteration 11/25 | Loss: 0.00080353
Iteration 12/25 | Loss: 0.00080353
Iteration 13/25 | Loss: 0.00080353
Iteration 14/25 | Loss: 0.00080353
Iteration 15/25 | Loss: 0.00080353
Iteration 16/25 | Loss: 0.00080353
Iteration 17/25 | Loss: 0.00080353
Iteration 18/25 | Loss: 0.00080353
Iteration 19/25 | Loss: 0.00080353
Iteration 20/25 | Loss: 0.00080353
Iteration 21/25 | Loss: 0.00080353
Iteration 22/25 | Loss: 0.00080353
Iteration 23/25 | Loss: 0.00080353
Iteration 24/25 | Loss: 0.00080353
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008035300415940583, 0.0008035300415940583, 0.0008035300415940583, 0.0008035300415940583, 0.0008035300415940583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008035300415940583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080353
Iteration 2/1000 | Loss: 0.00009194
Iteration 3/1000 | Loss: 0.00164072
Iteration 4/1000 | Loss: 0.00023660
Iteration 5/1000 | Loss: 0.00087191
Iteration 6/1000 | Loss: 0.00040437
Iteration 7/1000 | Loss: 0.00037718
Iteration 8/1000 | Loss: 0.00021293
Iteration 9/1000 | Loss: 0.00045212
Iteration 10/1000 | Loss: 0.00029307
Iteration 11/1000 | Loss: 0.00069788
Iteration 12/1000 | Loss: 0.00057780
Iteration 13/1000 | Loss: 0.00017011
Iteration 14/1000 | Loss: 0.00050191
Iteration 15/1000 | Loss: 0.00011178
Iteration 16/1000 | Loss: 0.00055109
Iteration 17/1000 | Loss: 0.00090562
Iteration 18/1000 | Loss: 0.00045410
Iteration 19/1000 | Loss: 0.00051017
Iteration 20/1000 | Loss: 0.00006712
Iteration 21/1000 | Loss: 0.00007016
Iteration 22/1000 | Loss: 0.00014388
Iteration 23/1000 | Loss: 0.00005798
Iteration 24/1000 | Loss: 0.00016853
Iteration 25/1000 | Loss: 0.00011281
Iteration 26/1000 | Loss: 0.00026239
Iteration 27/1000 | Loss: 0.00036706
Iteration 28/1000 | Loss: 0.00037747
Iteration 29/1000 | Loss: 0.00021176
Iteration 30/1000 | Loss: 0.00003441
Iteration 31/1000 | Loss: 0.00025419
Iteration 32/1000 | Loss: 0.00033690
Iteration 33/1000 | Loss: 0.00037371
Iteration 34/1000 | Loss: 0.00002567
Iteration 35/1000 | Loss: 0.00032964
Iteration 36/1000 | Loss: 0.00003175
Iteration 37/1000 | Loss: 0.00056386
Iteration 38/1000 | Loss: 0.00003516
Iteration 39/1000 | Loss: 0.00002468
Iteration 40/1000 | Loss: 0.00002216
Iteration 41/1000 | Loss: 0.00002161
Iteration 42/1000 | Loss: 0.00002096
Iteration 43/1000 | Loss: 0.00002048
Iteration 44/1000 | Loss: 0.00002007
Iteration 45/1000 | Loss: 0.00001981
Iteration 46/1000 | Loss: 0.00001972
Iteration 47/1000 | Loss: 0.00001970
Iteration 48/1000 | Loss: 0.00001950
Iteration 49/1000 | Loss: 0.00001940
Iteration 50/1000 | Loss: 0.00001925
Iteration 51/1000 | Loss: 0.00001921
Iteration 52/1000 | Loss: 0.00001920
Iteration 53/1000 | Loss: 0.00001919
Iteration 54/1000 | Loss: 0.00001919
Iteration 55/1000 | Loss: 0.00001919
Iteration 56/1000 | Loss: 0.00001918
Iteration 57/1000 | Loss: 0.00001918
Iteration 58/1000 | Loss: 0.00001918
Iteration 59/1000 | Loss: 0.00001918
Iteration 60/1000 | Loss: 0.00001918
Iteration 61/1000 | Loss: 0.00001917
Iteration 62/1000 | Loss: 0.00001917
Iteration 63/1000 | Loss: 0.00001917
Iteration 64/1000 | Loss: 0.00001917
Iteration 65/1000 | Loss: 0.00001917
Iteration 66/1000 | Loss: 0.00001917
Iteration 67/1000 | Loss: 0.00001916
Iteration 68/1000 | Loss: 0.00001916
Iteration 69/1000 | Loss: 0.00001916
Iteration 70/1000 | Loss: 0.00001916
Iteration 71/1000 | Loss: 0.00001915
Iteration 72/1000 | Loss: 0.00001915
Iteration 73/1000 | Loss: 0.00001914
Iteration 74/1000 | Loss: 0.00001914
Iteration 75/1000 | Loss: 0.00001914
Iteration 76/1000 | Loss: 0.00001914
Iteration 77/1000 | Loss: 0.00001914
Iteration 78/1000 | Loss: 0.00001913
Iteration 79/1000 | Loss: 0.00001913
Iteration 80/1000 | Loss: 0.00001913
Iteration 81/1000 | Loss: 0.00001912
Iteration 82/1000 | Loss: 0.00001912
Iteration 83/1000 | Loss: 0.00001912
Iteration 84/1000 | Loss: 0.00001911
Iteration 85/1000 | Loss: 0.00001911
Iteration 86/1000 | Loss: 0.00001910
Iteration 87/1000 | Loss: 0.00001910
Iteration 88/1000 | Loss: 0.00001910
Iteration 89/1000 | Loss: 0.00001909
Iteration 90/1000 | Loss: 0.00001909
Iteration 91/1000 | Loss: 0.00001909
Iteration 92/1000 | Loss: 0.00001908
Iteration 93/1000 | Loss: 0.00001908
Iteration 94/1000 | Loss: 0.00001908
Iteration 95/1000 | Loss: 0.00001908
Iteration 96/1000 | Loss: 0.00001908
Iteration 97/1000 | Loss: 0.00001908
Iteration 98/1000 | Loss: 0.00001907
Iteration 99/1000 | Loss: 0.00001907
Iteration 100/1000 | Loss: 0.00001907
Iteration 101/1000 | Loss: 0.00001906
Iteration 102/1000 | Loss: 0.00001906
Iteration 103/1000 | Loss: 0.00001906
Iteration 104/1000 | Loss: 0.00001906
Iteration 105/1000 | Loss: 0.00001905
Iteration 106/1000 | Loss: 0.00001905
Iteration 107/1000 | Loss: 0.00001905
Iteration 108/1000 | Loss: 0.00001904
Iteration 109/1000 | Loss: 0.00001904
Iteration 110/1000 | Loss: 0.00001904
Iteration 111/1000 | Loss: 0.00001904
Iteration 112/1000 | Loss: 0.00001903
Iteration 113/1000 | Loss: 0.00001903
Iteration 114/1000 | Loss: 0.00001903
Iteration 115/1000 | Loss: 0.00001903
Iteration 116/1000 | Loss: 0.00001902
Iteration 117/1000 | Loss: 0.00001902
Iteration 118/1000 | Loss: 0.00001902
Iteration 119/1000 | Loss: 0.00001901
Iteration 120/1000 | Loss: 0.00001901
Iteration 121/1000 | Loss: 0.00001901
Iteration 122/1000 | Loss: 0.00001901
Iteration 123/1000 | Loss: 0.00001900
Iteration 124/1000 | Loss: 0.00001900
Iteration 125/1000 | Loss: 0.00001900
Iteration 126/1000 | Loss: 0.00001900
Iteration 127/1000 | Loss: 0.00001900
Iteration 128/1000 | Loss: 0.00001900
Iteration 129/1000 | Loss: 0.00001900
Iteration 130/1000 | Loss: 0.00001900
Iteration 131/1000 | Loss: 0.00001899
Iteration 132/1000 | Loss: 0.00001899
Iteration 133/1000 | Loss: 0.00001899
Iteration 134/1000 | Loss: 0.00001899
Iteration 135/1000 | Loss: 0.00001899
Iteration 136/1000 | Loss: 0.00001899
Iteration 137/1000 | Loss: 0.00001898
Iteration 138/1000 | Loss: 0.00001898
Iteration 139/1000 | Loss: 0.00001898
Iteration 140/1000 | Loss: 0.00001898
Iteration 141/1000 | Loss: 0.00001898
Iteration 142/1000 | Loss: 0.00001898
Iteration 143/1000 | Loss: 0.00001898
Iteration 144/1000 | Loss: 0.00001898
Iteration 145/1000 | Loss: 0.00001897
Iteration 146/1000 | Loss: 0.00001897
Iteration 147/1000 | Loss: 0.00001897
Iteration 148/1000 | Loss: 0.00001897
Iteration 149/1000 | Loss: 0.00001897
Iteration 150/1000 | Loss: 0.00001897
Iteration 151/1000 | Loss: 0.00001897
Iteration 152/1000 | Loss: 0.00001897
Iteration 153/1000 | Loss: 0.00001896
Iteration 154/1000 | Loss: 0.00001896
Iteration 155/1000 | Loss: 0.00001896
Iteration 156/1000 | Loss: 0.00001896
Iteration 157/1000 | Loss: 0.00001896
Iteration 158/1000 | Loss: 0.00001896
Iteration 159/1000 | Loss: 0.00001896
Iteration 160/1000 | Loss: 0.00001896
Iteration 161/1000 | Loss: 0.00001895
Iteration 162/1000 | Loss: 0.00001895
Iteration 163/1000 | Loss: 0.00001895
Iteration 164/1000 | Loss: 0.00001895
Iteration 165/1000 | Loss: 0.00001895
Iteration 166/1000 | Loss: 0.00001895
Iteration 167/1000 | Loss: 0.00001895
Iteration 168/1000 | Loss: 0.00001895
Iteration 169/1000 | Loss: 0.00001895
Iteration 170/1000 | Loss: 0.00001894
Iteration 171/1000 | Loss: 0.00001894
Iteration 172/1000 | Loss: 0.00001894
Iteration 173/1000 | Loss: 0.00001894
Iteration 174/1000 | Loss: 0.00001894
Iteration 175/1000 | Loss: 0.00001894
Iteration 176/1000 | Loss: 0.00001894
Iteration 177/1000 | Loss: 0.00001894
Iteration 178/1000 | Loss: 0.00001894
Iteration 179/1000 | Loss: 0.00001894
Iteration 180/1000 | Loss: 0.00001894
Iteration 181/1000 | Loss: 0.00001894
Iteration 182/1000 | Loss: 0.00001894
Iteration 183/1000 | Loss: 0.00001893
Iteration 184/1000 | Loss: 0.00001893
Iteration 185/1000 | Loss: 0.00001893
Iteration 186/1000 | Loss: 0.00001893
Iteration 187/1000 | Loss: 0.00001893
Iteration 188/1000 | Loss: 0.00001893
Iteration 189/1000 | Loss: 0.00001893
Iteration 190/1000 | Loss: 0.00001893
Iteration 191/1000 | Loss: 0.00001893
Iteration 192/1000 | Loss: 0.00001893
Iteration 193/1000 | Loss: 0.00001893
Iteration 194/1000 | Loss: 0.00001893
Iteration 195/1000 | Loss: 0.00001893
Iteration 196/1000 | Loss: 0.00001893
Iteration 197/1000 | Loss: 0.00001893
Iteration 198/1000 | Loss: 0.00001893
Iteration 199/1000 | Loss: 0.00001893
Iteration 200/1000 | Loss: 0.00001892
Iteration 201/1000 | Loss: 0.00001892
Iteration 202/1000 | Loss: 0.00001892
Iteration 203/1000 | Loss: 0.00001892
Iteration 204/1000 | Loss: 0.00001892
Iteration 205/1000 | Loss: 0.00001892
Iteration 206/1000 | Loss: 0.00001892
Iteration 207/1000 | Loss: 0.00001892
Iteration 208/1000 | Loss: 0.00001892
Iteration 209/1000 | Loss: 0.00001891
Iteration 210/1000 | Loss: 0.00001891
Iteration 211/1000 | Loss: 0.00001891
Iteration 212/1000 | Loss: 0.00001891
Iteration 213/1000 | Loss: 0.00001891
Iteration 214/1000 | Loss: 0.00001891
Iteration 215/1000 | Loss: 0.00001891
Iteration 216/1000 | Loss: 0.00001891
Iteration 217/1000 | Loss: 0.00001891
Iteration 218/1000 | Loss: 0.00001891
Iteration 219/1000 | Loss: 0.00001891
Iteration 220/1000 | Loss: 0.00001891
Iteration 221/1000 | Loss: 0.00001891
Iteration 222/1000 | Loss: 0.00001891
Iteration 223/1000 | Loss: 0.00001891
Iteration 224/1000 | Loss: 0.00001891
Iteration 225/1000 | Loss: 0.00001891
Iteration 226/1000 | Loss: 0.00001891
Iteration 227/1000 | Loss: 0.00001890
Iteration 228/1000 | Loss: 0.00001890
Iteration 229/1000 | Loss: 0.00001890
Iteration 230/1000 | Loss: 0.00001890
Iteration 231/1000 | Loss: 0.00001890
Iteration 232/1000 | Loss: 0.00001890
Iteration 233/1000 | Loss: 0.00001890
Iteration 234/1000 | Loss: 0.00001890
Iteration 235/1000 | Loss: 0.00001890
Iteration 236/1000 | Loss: 0.00001890
Iteration 237/1000 | Loss: 0.00001890
Iteration 238/1000 | Loss: 0.00001890
Iteration 239/1000 | Loss: 0.00001889
Iteration 240/1000 | Loss: 0.00001889
Iteration 241/1000 | Loss: 0.00001889
Iteration 242/1000 | Loss: 0.00001889
Iteration 243/1000 | Loss: 0.00001889
Iteration 244/1000 | Loss: 0.00001889
Iteration 245/1000 | Loss: 0.00001889
Iteration 246/1000 | Loss: 0.00001889
Iteration 247/1000 | Loss: 0.00001889
Iteration 248/1000 | Loss: 0.00001889
Iteration 249/1000 | Loss: 0.00001889
Iteration 250/1000 | Loss: 0.00001889
Iteration 251/1000 | Loss: 0.00001889
Iteration 252/1000 | Loss: 0.00001888
Iteration 253/1000 | Loss: 0.00001888
Iteration 254/1000 | Loss: 0.00001888
Iteration 255/1000 | Loss: 0.00001888
Iteration 256/1000 | Loss: 0.00001888
Iteration 257/1000 | Loss: 0.00001888
Iteration 258/1000 | Loss: 0.00001888
Iteration 259/1000 | Loss: 0.00001888
Iteration 260/1000 | Loss: 0.00001888
Iteration 261/1000 | Loss: 0.00001888
Iteration 262/1000 | Loss: 0.00001888
Iteration 263/1000 | Loss: 0.00001888
Iteration 264/1000 | Loss: 0.00001888
Iteration 265/1000 | Loss: 0.00001888
Iteration 266/1000 | Loss: 0.00001888
Iteration 267/1000 | Loss: 0.00001887
Iteration 268/1000 | Loss: 0.00001887
Iteration 269/1000 | Loss: 0.00001887
Iteration 270/1000 | Loss: 0.00001887
Iteration 271/1000 | Loss: 0.00001887
Iteration 272/1000 | Loss: 0.00001887
Iteration 273/1000 | Loss: 0.00001887
Iteration 274/1000 | Loss: 0.00001887
Iteration 275/1000 | Loss: 0.00001887
Iteration 276/1000 | Loss: 0.00001887
Iteration 277/1000 | Loss: 0.00001887
Iteration 278/1000 | Loss: 0.00001887
Iteration 279/1000 | Loss: 0.00001887
Iteration 280/1000 | Loss: 0.00001887
Iteration 281/1000 | Loss: 0.00001887
Iteration 282/1000 | Loss: 0.00001887
Iteration 283/1000 | Loss: 0.00001887
Iteration 284/1000 | Loss: 0.00001886
Iteration 285/1000 | Loss: 0.00001886
Iteration 286/1000 | Loss: 0.00001886
Iteration 287/1000 | Loss: 0.00001886
Iteration 288/1000 | Loss: 0.00001886
Iteration 289/1000 | Loss: 0.00001886
Iteration 290/1000 | Loss: 0.00001886
Iteration 291/1000 | Loss: 0.00001886
Iteration 292/1000 | Loss: 0.00001886
Iteration 293/1000 | Loss: 0.00001886
Iteration 294/1000 | Loss: 0.00001886
Iteration 295/1000 | Loss: 0.00001886
Iteration 296/1000 | Loss: 0.00001886
Iteration 297/1000 | Loss: 0.00001886
Iteration 298/1000 | Loss: 0.00001886
Iteration 299/1000 | Loss: 0.00001886
Iteration 300/1000 | Loss: 0.00001886
Iteration 301/1000 | Loss: 0.00001886
Iteration 302/1000 | Loss: 0.00001886
Iteration 303/1000 | Loss: 0.00001886
Iteration 304/1000 | Loss: 0.00001886
Iteration 305/1000 | Loss: 0.00001886
Iteration 306/1000 | Loss: 0.00001886
Iteration 307/1000 | Loss: 0.00001886
Iteration 308/1000 | Loss: 0.00001886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 308. Stopping optimization.
Last 5 losses: [1.8861044736695476e-05, 1.8861044736695476e-05, 1.8861044736695476e-05, 1.8861044736695476e-05, 1.8861044736695476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8861044736695476e-05

Optimization complete. Final v2v error: 3.632941722869873 mm

Highest mean error: 4.354597568511963 mm for frame 211

Lowest mean error: 3.1089632511138916 mm for frame 31

Saving results

Total time: 134.0575544834137
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00940971
Iteration 2/25 | Loss: 0.00161608
Iteration 3/25 | Loss: 0.00117727
Iteration 4/25 | Loss: 0.00114081
Iteration 5/25 | Loss: 0.00113585
Iteration 6/25 | Loss: 0.00113477
Iteration 7/25 | Loss: 0.00113477
Iteration 8/25 | Loss: 0.00113477
Iteration 9/25 | Loss: 0.00113477
Iteration 10/25 | Loss: 0.00113477
Iteration 11/25 | Loss: 0.00113477
Iteration 12/25 | Loss: 0.00113477
Iteration 13/25 | Loss: 0.00113477
Iteration 14/25 | Loss: 0.00113477
Iteration 15/25 | Loss: 0.00113477
Iteration 16/25 | Loss: 0.00113477
Iteration 17/25 | Loss: 0.00113477
Iteration 18/25 | Loss: 0.00113477
Iteration 19/25 | Loss: 0.00113477
Iteration 20/25 | Loss: 0.00113477
Iteration 21/25 | Loss: 0.00113477
Iteration 22/25 | Loss: 0.00113477
Iteration 23/25 | Loss: 0.00113477
Iteration 24/25 | Loss: 0.00113477
Iteration 25/25 | Loss: 0.00113477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.55656856
Iteration 2/25 | Loss: 0.00050489
Iteration 3/25 | Loss: 0.00050489
Iteration 4/25 | Loss: 0.00050489
Iteration 5/25 | Loss: 0.00050489
Iteration 6/25 | Loss: 0.00050489
Iteration 7/25 | Loss: 0.00050489
Iteration 8/25 | Loss: 0.00050489
Iteration 9/25 | Loss: 0.00050489
Iteration 10/25 | Loss: 0.00050489
Iteration 11/25 | Loss: 0.00050489
Iteration 12/25 | Loss: 0.00050489
Iteration 13/25 | Loss: 0.00050489
Iteration 14/25 | Loss: 0.00050489
Iteration 15/25 | Loss: 0.00050489
Iteration 16/25 | Loss: 0.00050489
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005048897583037615, 0.0005048897583037615, 0.0005048897583037615, 0.0005048897583037615, 0.0005048897583037615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005048897583037615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050489
Iteration 2/1000 | Loss: 0.00007412
Iteration 3/1000 | Loss: 0.00004435
Iteration 4/1000 | Loss: 0.00003819
Iteration 5/1000 | Loss: 0.00003595
Iteration 6/1000 | Loss: 0.00003476
Iteration 7/1000 | Loss: 0.00003431
Iteration 8/1000 | Loss: 0.00003390
Iteration 9/1000 | Loss: 0.00003362
Iteration 10/1000 | Loss: 0.00003335
Iteration 11/1000 | Loss: 0.00003305
Iteration 12/1000 | Loss: 0.00003281
Iteration 13/1000 | Loss: 0.00003265
Iteration 14/1000 | Loss: 0.00003252
Iteration 15/1000 | Loss: 0.00003248
Iteration 16/1000 | Loss: 0.00003242
Iteration 17/1000 | Loss: 0.00003234
Iteration 18/1000 | Loss: 0.00003233
Iteration 19/1000 | Loss: 0.00003233
Iteration 20/1000 | Loss: 0.00003231
Iteration 21/1000 | Loss: 0.00003231
Iteration 22/1000 | Loss: 0.00003231
Iteration 23/1000 | Loss: 0.00003230
Iteration 24/1000 | Loss: 0.00003230
Iteration 25/1000 | Loss: 0.00003230
Iteration 26/1000 | Loss: 0.00003230
Iteration 27/1000 | Loss: 0.00003230
Iteration 28/1000 | Loss: 0.00003230
Iteration 29/1000 | Loss: 0.00003230
Iteration 30/1000 | Loss: 0.00003230
Iteration 31/1000 | Loss: 0.00003229
Iteration 32/1000 | Loss: 0.00003229
Iteration 33/1000 | Loss: 0.00003227
Iteration 34/1000 | Loss: 0.00003227
Iteration 35/1000 | Loss: 0.00003226
Iteration 36/1000 | Loss: 0.00003226
Iteration 37/1000 | Loss: 0.00003226
Iteration 38/1000 | Loss: 0.00003226
Iteration 39/1000 | Loss: 0.00003225
Iteration 40/1000 | Loss: 0.00003225
Iteration 41/1000 | Loss: 0.00003225
Iteration 42/1000 | Loss: 0.00003225
Iteration 43/1000 | Loss: 0.00003225
Iteration 44/1000 | Loss: 0.00003225
Iteration 45/1000 | Loss: 0.00003225
Iteration 46/1000 | Loss: 0.00003225
Iteration 47/1000 | Loss: 0.00003224
Iteration 48/1000 | Loss: 0.00003223
Iteration 49/1000 | Loss: 0.00003223
Iteration 50/1000 | Loss: 0.00003222
Iteration 51/1000 | Loss: 0.00003222
Iteration 52/1000 | Loss: 0.00003222
Iteration 53/1000 | Loss: 0.00003222
Iteration 54/1000 | Loss: 0.00003222
Iteration 55/1000 | Loss: 0.00003222
Iteration 56/1000 | Loss: 0.00003222
Iteration 57/1000 | Loss: 0.00003222
Iteration 58/1000 | Loss: 0.00003222
Iteration 59/1000 | Loss: 0.00003222
Iteration 60/1000 | Loss: 0.00003221
Iteration 61/1000 | Loss: 0.00003221
Iteration 62/1000 | Loss: 0.00003221
Iteration 63/1000 | Loss: 0.00003221
Iteration 64/1000 | Loss: 0.00003220
Iteration 65/1000 | Loss: 0.00003220
Iteration 66/1000 | Loss: 0.00003220
Iteration 67/1000 | Loss: 0.00003219
Iteration 68/1000 | Loss: 0.00003218
Iteration 69/1000 | Loss: 0.00003218
Iteration 70/1000 | Loss: 0.00003218
Iteration 71/1000 | Loss: 0.00003217
Iteration 72/1000 | Loss: 0.00003217
Iteration 73/1000 | Loss: 0.00003217
Iteration 74/1000 | Loss: 0.00003217
Iteration 75/1000 | Loss: 0.00003217
Iteration 76/1000 | Loss: 0.00003217
Iteration 77/1000 | Loss: 0.00003217
Iteration 78/1000 | Loss: 0.00003217
Iteration 79/1000 | Loss: 0.00003217
Iteration 80/1000 | Loss: 0.00003217
Iteration 81/1000 | Loss: 0.00003216
Iteration 82/1000 | Loss: 0.00003216
Iteration 83/1000 | Loss: 0.00003216
Iteration 84/1000 | Loss: 0.00003216
Iteration 85/1000 | Loss: 0.00003215
Iteration 86/1000 | Loss: 0.00003215
Iteration 87/1000 | Loss: 0.00003215
Iteration 88/1000 | Loss: 0.00003215
Iteration 89/1000 | Loss: 0.00003215
Iteration 90/1000 | Loss: 0.00003215
Iteration 91/1000 | Loss: 0.00003215
Iteration 92/1000 | Loss: 0.00003214
Iteration 93/1000 | Loss: 0.00003214
Iteration 94/1000 | Loss: 0.00003214
Iteration 95/1000 | Loss: 0.00003214
Iteration 96/1000 | Loss: 0.00003214
Iteration 97/1000 | Loss: 0.00003214
Iteration 98/1000 | Loss: 0.00003214
Iteration 99/1000 | Loss: 0.00003213
Iteration 100/1000 | Loss: 0.00003213
Iteration 101/1000 | Loss: 0.00003212
Iteration 102/1000 | Loss: 0.00003212
Iteration 103/1000 | Loss: 0.00003212
Iteration 104/1000 | Loss: 0.00003212
Iteration 105/1000 | Loss: 0.00003212
Iteration 106/1000 | Loss: 0.00003212
Iteration 107/1000 | Loss: 0.00003212
Iteration 108/1000 | Loss: 0.00003212
Iteration 109/1000 | Loss: 0.00003212
Iteration 110/1000 | Loss: 0.00003211
Iteration 111/1000 | Loss: 0.00003211
Iteration 112/1000 | Loss: 0.00003211
Iteration 113/1000 | Loss: 0.00003210
Iteration 114/1000 | Loss: 0.00003210
Iteration 115/1000 | Loss: 0.00003210
Iteration 116/1000 | Loss: 0.00003210
Iteration 117/1000 | Loss: 0.00003210
Iteration 118/1000 | Loss: 0.00003209
Iteration 119/1000 | Loss: 0.00003209
Iteration 120/1000 | Loss: 0.00003209
Iteration 121/1000 | Loss: 0.00003209
Iteration 122/1000 | Loss: 0.00003209
Iteration 123/1000 | Loss: 0.00003209
Iteration 124/1000 | Loss: 0.00003209
Iteration 125/1000 | Loss: 0.00003209
Iteration 126/1000 | Loss: 0.00003209
Iteration 127/1000 | Loss: 0.00003208
Iteration 128/1000 | Loss: 0.00003208
Iteration 129/1000 | Loss: 0.00003208
Iteration 130/1000 | Loss: 0.00003208
Iteration 131/1000 | Loss: 0.00003208
Iteration 132/1000 | Loss: 0.00003208
Iteration 133/1000 | Loss: 0.00003208
Iteration 134/1000 | Loss: 0.00003208
Iteration 135/1000 | Loss: 0.00003208
Iteration 136/1000 | Loss: 0.00003208
Iteration 137/1000 | Loss: 0.00003208
Iteration 138/1000 | Loss: 0.00003207
Iteration 139/1000 | Loss: 0.00003207
Iteration 140/1000 | Loss: 0.00003207
Iteration 141/1000 | Loss: 0.00003207
Iteration 142/1000 | Loss: 0.00003207
Iteration 143/1000 | Loss: 0.00003207
Iteration 144/1000 | Loss: 0.00003207
Iteration 145/1000 | Loss: 0.00003207
Iteration 146/1000 | Loss: 0.00003207
Iteration 147/1000 | Loss: 0.00003207
Iteration 148/1000 | Loss: 0.00003206
Iteration 149/1000 | Loss: 0.00003206
Iteration 150/1000 | Loss: 0.00003206
Iteration 151/1000 | Loss: 0.00003206
Iteration 152/1000 | Loss: 0.00003206
Iteration 153/1000 | Loss: 0.00003206
Iteration 154/1000 | Loss: 0.00003206
Iteration 155/1000 | Loss: 0.00003206
Iteration 156/1000 | Loss: 0.00003206
Iteration 157/1000 | Loss: 0.00003205
Iteration 158/1000 | Loss: 0.00003205
Iteration 159/1000 | Loss: 0.00003205
Iteration 160/1000 | Loss: 0.00003205
Iteration 161/1000 | Loss: 0.00003205
Iteration 162/1000 | Loss: 0.00003205
Iteration 163/1000 | Loss: 0.00003205
Iteration 164/1000 | Loss: 0.00003205
Iteration 165/1000 | Loss: 0.00003204
Iteration 166/1000 | Loss: 0.00003204
Iteration 167/1000 | Loss: 0.00003204
Iteration 168/1000 | Loss: 0.00003204
Iteration 169/1000 | Loss: 0.00003204
Iteration 170/1000 | Loss: 0.00003204
Iteration 171/1000 | Loss: 0.00003204
Iteration 172/1000 | Loss: 0.00003204
Iteration 173/1000 | Loss: 0.00003204
Iteration 174/1000 | Loss: 0.00003204
Iteration 175/1000 | Loss: 0.00003204
Iteration 176/1000 | Loss: 0.00003204
Iteration 177/1000 | Loss: 0.00003204
Iteration 178/1000 | Loss: 0.00003203
Iteration 179/1000 | Loss: 0.00003203
Iteration 180/1000 | Loss: 0.00003203
Iteration 181/1000 | Loss: 0.00003203
Iteration 182/1000 | Loss: 0.00003203
Iteration 183/1000 | Loss: 0.00003203
Iteration 184/1000 | Loss: 0.00003203
Iteration 185/1000 | Loss: 0.00003203
Iteration 186/1000 | Loss: 0.00003203
Iteration 187/1000 | Loss: 0.00003203
Iteration 188/1000 | Loss: 0.00003203
Iteration 189/1000 | Loss: 0.00003203
Iteration 190/1000 | Loss: 0.00003203
Iteration 191/1000 | Loss: 0.00003203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [3.203080268576741e-05, 3.203080268576741e-05, 3.203080268576741e-05, 3.203080268576741e-05, 3.203080268576741e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.203080268576741e-05

Optimization complete. Final v2v error: 4.643980026245117 mm

Highest mean error: 5.338033676147461 mm for frame 17

Lowest mean error: 4.385199546813965 mm for frame 60

Saving results

Total time: 44.03021478652954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844993
Iteration 2/25 | Loss: 0.00132236
Iteration 3/25 | Loss: 0.00109891
Iteration 4/25 | Loss: 0.00106352
Iteration 5/25 | Loss: 0.00105527
Iteration 6/25 | Loss: 0.00105224
Iteration 7/25 | Loss: 0.00105171
Iteration 8/25 | Loss: 0.00105171
Iteration 9/25 | Loss: 0.00105171
Iteration 10/25 | Loss: 0.00105171
Iteration 11/25 | Loss: 0.00105171
Iteration 12/25 | Loss: 0.00105171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010517123155295849, 0.0010517123155295849, 0.0010517123155295849, 0.0010517123155295849, 0.0010517123155295849]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010517123155295849

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08528733
Iteration 2/25 | Loss: 0.00086143
Iteration 3/25 | Loss: 0.00086137
Iteration 4/25 | Loss: 0.00086137
Iteration 5/25 | Loss: 0.00086137
Iteration 6/25 | Loss: 0.00086137
Iteration 7/25 | Loss: 0.00086137
Iteration 8/25 | Loss: 0.00086137
Iteration 9/25 | Loss: 0.00086137
Iteration 10/25 | Loss: 0.00086137
Iteration 11/25 | Loss: 0.00086137
Iteration 12/25 | Loss: 0.00086137
Iteration 13/25 | Loss: 0.00086137
Iteration 14/25 | Loss: 0.00086137
Iteration 15/25 | Loss: 0.00086137
Iteration 16/25 | Loss: 0.00086137
Iteration 17/25 | Loss: 0.00086137
Iteration 18/25 | Loss: 0.00086137
Iteration 19/25 | Loss: 0.00086137
Iteration 20/25 | Loss: 0.00086137
Iteration 21/25 | Loss: 0.00086137
Iteration 22/25 | Loss: 0.00086137
Iteration 23/25 | Loss: 0.00086137
Iteration 24/25 | Loss: 0.00086137
Iteration 25/25 | Loss: 0.00086137

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086137
Iteration 2/1000 | Loss: 0.00004298
Iteration 3/1000 | Loss: 0.00002189
Iteration 4/1000 | Loss: 0.00001934
Iteration 5/1000 | Loss: 0.00001801
Iteration 6/1000 | Loss: 0.00001731
Iteration 7/1000 | Loss: 0.00001693
Iteration 8/1000 | Loss: 0.00001663
Iteration 9/1000 | Loss: 0.00001635
Iteration 10/1000 | Loss: 0.00001621
Iteration 11/1000 | Loss: 0.00001609
Iteration 12/1000 | Loss: 0.00001592
Iteration 13/1000 | Loss: 0.00001584
Iteration 14/1000 | Loss: 0.00001580
Iteration 15/1000 | Loss: 0.00001580
Iteration 16/1000 | Loss: 0.00001578
Iteration 17/1000 | Loss: 0.00001575
Iteration 18/1000 | Loss: 0.00001575
Iteration 19/1000 | Loss: 0.00001574
Iteration 20/1000 | Loss: 0.00001571
Iteration 21/1000 | Loss: 0.00001571
Iteration 22/1000 | Loss: 0.00001571
Iteration 23/1000 | Loss: 0.00001570
Iteration 24/1000 | Loss: 0.00001570
Iteration 25/1000 | Loss: 0.00001567
Iteration 26/1000 | Loss: 0.00001567
Iteration 27/1000 | Loss: 0.00001565
Iteration 28/1000 | Loss: 0.00001565
Iteration 29/1000 | Loss: 0.00001565
Iteration 30/1000 | Loss: 0.00001565
Iteration 31/1000 | Loss: 0.00001565
Iteration 32/1000 | Loss: 0.00001565
Iteration 33/1000 | Loss: 0.00001564
Iteration 34/1000 | Loss: 0.00001564
Iteration 35/1000 | Loss: 0.00001564
Iteration 36/1000 | Loss: 0.00001562
Iteration 37/1000 | Loss: 0.00001562
Iteration 38/1000 | Loss: 0.00001562
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001560
Iteration 41/1000 | Loss: 0.00001560
Iteration 42/1000 | Loss: 0.00001559
Iteration 43/1000 | Loss: 0.00001559
Iteration 44/1000 | Loss: 0.00001559
Iteration 45/1000 | Loss: 0.00001558
Iteration 46/1000 | Loss: 0.00001557
Iteration 47/1000 | Loss: 0.00001557
Iteration 48/1000 | Loss: 0.00001557
Iteration 49/1000 | Loss: 0.00001557
Iteration 50/1000 | Loss: 0.00001556
Iteration 51/1000 | Loss: 0.00001556
Iteration 52/1000 | Loss: 0.00001556
Iteration 53/1000 | Loss: 0.00001555
Iteration 54/1000 | Loss: 0.00001555
Iteration 55/1000 | Loss: 0.00001555
Iteration 56/1000 | Loss: 0.00001555
Iteration 57/1000 | Loss: 0.00001554
Iteration 58/1000 | Loss: 0.00001554
Iteration 59/1000 | Loss: 0.00001554
Iteration 60/1000 | Loss: 0.00001554
Iteration 61/1000 | Loss: 0.00001554
Iteration 62/1000 | Loss: 0.00001554
Iteration 63/1000 | Loss: 0.00001554
Iteration 64/1000 | Loss: 0.00001554
Iteration 65/1000 | Loss: 0.00001553
Iteration 66/1000 | Loss: 0.00001553
Iteration 67/1000 | Loss: 0.00001553
Iteration 68/1000 | Loss: 0.00001553
Iteration 69/1000 | Loss: 0.00001553
Iteration 70/1000 | Loss: 0.00001553
Iteration 71/1000 | Loss: 0.00001553
Iteration 72/1000 | Loss: 0.00001553
Iteration 73/1000 | Loss: 0.00001553
Iteration 74/1000 | Loss: 0.00001552
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001552
Iteration 77/1000 | Loss: 0.00001552
Iteration 78/1000 | Loss: 0.00001552
Iteration 79/1000 | Loss: 0.00001552
Iteration 80/1000 | Loss: 0.00001552
Iteration 81/1000 | Loss: 0.00001551
Iteration 82/1000 | Loss: 0.00001551
Iteration 83/1000 | Loss: 0.00001551
Iteration 84/1000 | Loss: 0.00001551
Iteration 85/1000 | Loss: 0.00001550
Iteration 86/1000 | Loss: 0.00001550
Iteration 87/1000 | Loss: 0.00001550
Iteration 88/1000 | Loss: 0.00001550
Iteration 89/1000 | Loss: 0.00001550
Iteration 90/1000 | Loss: 0.00001550
Iteration 91/1000 | Loss: 0.00001550
Iteration 92/1000 | Loss: 0.00001550
Iteration 93/1000 | Loss: 0.00001550
Iteration 94/1000 | Loss: 0.00001549
Iteration 95/1000 | Loss: 0.00001549
Iteration 96/1000 | Loss: 0.00001549
Iteration 97/1000 | Loss: 0.00001549
Iteration 98/1000 | Loss: 0.00001549
Iteration 99/1000 | Loss: 0.00001549
Iteration 100/1000 | Loss: 0.00001549
Iteration 101/1000 | Loss: 0.00001548
Iteration 102/1000 | Loss: 0.00001548
Iteration 103/1000 | Loss: 0.00001548
Iteration 104/1000 | Loss: 0.00001548
Iteration 105/1000 | Loss: 0.00001548
Iteration 106/1000 | Loss: 0.00001548
Iteration 107/1000 | Loss: 0.00001548
Iteration 108/1000 | Loss: 0.00001548
Iteration 109/1000 | Loss: 0.00001548
Iteration 110/1000 | Loss: 0.00001548
Iteration 111/1000 | Loss: 0.00001548
Iteration 112/1000 | Loss: 0.00001547
Iteration 113/1000 | Loss: 0.00001547
Iteration 114/1000 | Loss: 0.00001547
Iteration 115/1000 | Loss: 0.00001547
Iteration 116/1000 | Loss: 0.00001547
Iteration 117/1000 | Loss: 0.00001547
Iteration 118/1000 | Loss: 0.00001547
Iteration 119/1000 | Loss: 0.00001547
Iteration 120/1000 | Loss: 0.00001547
Iteration 121/1000 | Loss: 0.00001547
Iteration 122/1000 | Loss: 0.00001547
Iteration 123/1000 | Loss: 0.00001547
Iteration 124/1000 | Loss: 0.00001547
Iteration 125/1000 | Loss: 0.00001547
Iteration 126/1000 | Loss: 0.00001547
Iteration 127/1000 | Loss: 0.00001547
Iteration 128/1000 | Loss: 0.00001547
Iteration 129/1000 | Loss: 0.00001547
Iteration 130/1000 | Loss: 0.00001547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.546719613543246e-05, 1.546719613543246e-05, 1.546719613543246e-05, 1.546719613543246e-05, 1.546719613543246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.546719613543246e-05

Optimization complete. Final v2v error: 3.3730523586273193 mm

Highest mean error: 4.08533239364624 mm for frame 127

Lowest mean error: 2.755797863006592 mm for frame 217

Saving results

Total time: 42.10953903198242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941552
Iteration 2/25 | Loss: 0.00202257
Iteration 3/25 | Loss: 0.00137183
Iteration 4/25 | Loss: 0.00131529
Iteration 5/25 | Loss: 0.00127617
Iteration 6/25 | Loss: 0.00122105
Iteration 7/25 | Loss: 0.00121344
Iteration 8/25 | Loss: 0.00120571
Iteration 9/25 | Loss: 0.00119352
Iteration 10/25 | Loss: 0.00118420
Iteration 11/25 | Loss: 0.00118434
Iteration 12/25 | Loss: 0.00114153
Iteration 13/25 | Loss: 0.00112381
Iteration 14/25 | Loss: 0.00112843
Iteration 15/25 | Loss: 0.00112644
Iteration 16/25 | Loss: 0.00112815
Iteration 17/25 | Loss: 0.00112972
Iteration 18/25 | Loss: 0.00113059
Iteration 19/25 | Loss: 0.00112976
Iteration 20/25 | Loss: 0.00113333
Iteration 21/25 | Loss: 0.00112187
Iteration 22/25 | Loss: 0.00115376
Iteration 23/25 | Loss: 0.00114671
Iteration 24/25 | Loss: 0.00110969
Iteration 25/25 | Loss: 0.00109947

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67162347
Iteration 2/25 | Loss: 0.00157248
Iteration 3/25 | Loss: 0.00157214
Iteration 4/25 | Loss: 0.00157214
Iteration 5/25 | Loss: 0.00157214
Iteration 6/25 | Loss: 0.00157214
Iteration 7/25 | Loss: 0.00157214
Iteration 8/25 | Loss: 0.00157213
Iteration 9/25 | Loss: 0.00157213
Iteration 10/25 | Loss: 0.00157213
Iteration 11/25 | Loss: 0.00157213
Iteration 12/25 | Loss: 0.00157213
Iteration 13/25 | Loss: 0.00157213
Iteration 14/25 | Loss: 0.00157213
Iteration 15/25 | Loss: 0.00157213
Iteration 16/25 | Loss: 0.00157213
Iteration 17/25 | Loss: 0.00157213
Iteration 18/25 | Loss: 0.00157213
Iteration 19/25 | Loss: 0.00157213
Iteration 20/25 | Loss: 0.00157213
Iteration 21/25 | Loss: 0.00157213
Iteration 22/25 | Loss: 0.00157213
Iteration 23/25 | Loss: 0.00157213
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015721343224868178, 0.0015721343224868178, 0.0015721343224868178, 0.0015721343224868178, 0.0015721343224868178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015721343224868178

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157213
Iteration 2/1000 | Loss: 0.00450917
Iteration 3/1000 | Loss: 0.00513203
Iteration 4/1000 | Loss: 0.00464696
Iteration 5/1000 | Loss: 0.00610847
Iteration 6/1000 | Loss: 0.00163512
Iteration 7/1000 | Loss: 0.00137483
Iteration 8/1000 | Loss: 0.00126922
Iteration 9/1000 | Loss: 0.00075104
Iteration 10/1000 | Loss: 0.00059023
Iteration 11/1000 | Loss: 0.00042563
Iteration 12/1000 | Loss: 0.00027691
Iteration 13/1000 | Loss: 0.00097347
Iteration 14/1000 | Loss: 0.00062593
Iteration 15/1000 | Loss: 0.00279426
Iteration 16/1000 | Loss: 0.00120913
Iteration 17/1000 | Loss: 0.00088282
Iteration 18/1000 | Loss: 0.00592148
Iteration 19/1000 | Loss: 0.00079024
Iteration 20/1000 | Loss: 0.00053942
Iteration 21/1000 | Loss: 0.00057534
Iteration 22/1000 | Loss: 0.00147323
Iteration 23/1000 | Loss: 0.00442972
Iteration 24/1000 | Loss: 0.00466006
Iteration 25/1000 | Loss: 0.00023646
Iteration 26/1000 | Loss: 0.00025878
Iteration 27/1000 | Loss: 0.00368242
Iteration 28/1000 | Loss: 0.00029281
Iteration 29/1000 | Loss: 0.00044592
Iteration 30/1000 | Loss: 0.00016812
Iteration 31/1000 | Loss: 0.00005332
Iteration 32/1000 | Loss: 0.00005104
Iteration 33/1000 | Loss: 0.00004246
Iteration 34/1000 | Loss: 0.00032294
Iteration 35/1000 | Loss: 0.00047865
Iteration 36/1000 | Loss: 0.00036174
Iteration 37/1000 | Loss: 0.00005658
Iteration 38/1000 | Loss: 0.00004863
Iteration 39/1000 | Loss: 0.00050707
Iteration 40/1000 | Loss: 0.00038167
Iteration 41/1000 | Loss: 0.00029439
Iteration 42/1000 | Loss: 0.00004746
Iteration 43/1000 | Loss: 0.00056276
Iteration 44/1000 | Loss: 0.00117117
Iteration 45/1000 | Loss: 0.00006354
Iteration 46/1000 | Loss: 0.00119257
Iteration 47/1000 | Loss: 0.00003939
Iteration 48/1000 | Loss: 0.00099390
Iteration 49/1000 | Loss: 0.00060294
Iteration 50/1000 | Loss: 0.00057827
Iteration 51/1000 | Loss: 0.00100540
Iteration 52/1000 | Loss: 0.00047428
Iteration 53/1000 | Loss: 0.00004305
Iteration 54/1000 | Loss: 0.00246988
Iteration 55/1000 | Loss: 0.00007791
Iteration 56/1000 | Loss: 0.00005373
Iteration 57/1000 | Loss: 0.00004625
Iteration 58/1000 | Loss: 0.00280179
Iteration 59/1000 | Loss: 0.00005415
Iteration 60/1000 | Loss: 0.00003584
Iteration 61/1000 | Loss: 0.00003372
Iteration 62/1000 | Loss: 0.00252614
Iteration 63/1000 | Loss: 0.00068282
Iteration 64/1000 | Loss: 0.00003673
Iteration 65/1000 | Loss: 0.00003418
Iteration 66/1000 | Loss: 0.00003253
Iteration 67/1000 | Loss: 0.00003436
Iteration 68/1000 | Loss: 0.00003400
Iteration 69/1000 | Loss: 0.00003649
Iteration 70/1000 | Loss: 0.00003684
Iteration 71/1000 | Loss: 0.00003445
Iteration 72/1000 | Loss: 0.00003530
Iteration 73/1000 | Loss: 0.00003664
Iteration 74/1000 | Loss: 0.00003306
Iteration 75/1000 | Loss: 0.00003395
Iteration 76/1000 | Loss: 0.00003313
Iteration 77/1000 | Loss: 0.00003451
Iteration 78/1000 | Loss: 0.00003757
Iteration 79/1000 | Loss: 0.00004243
Iteration 80/1000 | Loss: 0.00004147
Iteration 81/1000 | Loss: 0.00004038
Iteration 82/1000 | Loss: 0.00003442
Iteration 83/1000 | Loss: 0.00003417
Iteration 84/1000 | Loss: 0.00064177
Iteration 85/1000 | Loss: 0.00023199
Iteration 86/1000 | Loss: 0.00003855
Iteration 87/1000 | Loss: 0.00003623
Iteration 88/1000 | Loss: 0.00003419
Iteration 89/1000 | Loss: 0.00003408
Iteration 90/1000 | Loss: 0.00003375
Iteration 91/1000 | Loss: 0.00003428
Iteration 92/1000 | Loss: 0.00003350
Iteration 93/1000 | Loss: 0.00003410
Iteration 94/1000 | Loss: 0.00003419
Iteration 95/1000 | Loss: 0.00003405
Iteration 96/1000 | Loss: 0.00003422
Iteration 97/1000 | Loss: 0.00003590
Iteration 98/1000 | Loss: 0.00003423
Iteration 99/1000 | Loss: 0.00003528
Iteration 100/1000 | Loss: 0.00003473
Iteration 101/1000 | Loss: 0.00003487
Iteration 102/1000 | Loss: 0.00003497
Iteration 103/1000 | Loss: 0.00003497
Iteration 104/1000 | Loss: 0.00003500
Iteration 105/1000 | Loss: 0.00003472
Iteration 106/1000 | Loss: 0.00003521
Iteration 107/1000 | Loss: 0.00003467
Iteration 108/1000 | Loss: 0.00003543
Iteration 109/1000 | Loss: 0.00066414
Iteration 110/1000 | Loss: 0.00164683
Iteration 111/1000 | Loss: 0.00093881
Iteration 112/1000 | Loss: 0.00075112
Iteration 113/1000 | Loss: 0.00085891
Iteration 114/1000 | Loss: 0.00008724
Iteration 115/1000 | Loss: 0.00262015
Iteration 116/1000 | Loss: 0.00039670
Iteration 117/1000 | Loss: 0.00004792
Iteration 118/1000 | Loss: 0.00096569
Iteration 119/1000 | Loss: 0.00097358
Iteration 120/1000 | Loss: 0.00116882
Iteration 121/1000 | Loss: 0.00120512
Iteration 122/1000 | Loss: 0.00075619
Iteration 123/1000 | Loss: 0.00135284
Iteration 124/1000 | Loss: 0.00069390
Iteration 125/1000 | Loss: 0.00143344
Iteration 126/1000 | Loss: 0.00178532
Iteration 127/1000 | Loss: 0.00165347
Iteration 128/1000 | Loss: 0.00175049
Iteration 129/1000 | Loss: 0.00083407
Iteration 130/1000 | Loss: 0.00133889
Iteration 131/1000 | Loss: 0.00140688
Iteration 132/1000 | Loss: 0.00086361
Iteration 133/1000 | Loss: 0.00052500
Iteration 134/1000 | Loss: 0.00102939
Iteration 135/1000 | Loss: 0.00133441
Iteration 136/1000 | Loss: 0.00150561
Iteration 137/1000 | Loss: 0.00038169
Iteration 138/1000 | Loss: 0.00052409
Iteration 139/1000 | Loss: 0.00104754
Iteration 140/1000 | Loss: 0.00012067
Iteration 141/1000 | Loss: 0.00015784
Iteration 142/1000 | Loss: 0.00156341
Iteration 143/1000 | Loss: 0.00007950
Iteration 144/1000 | Loss: 0.00005062
Iteration 145/1000 | Loss: 0.00004325
Iteration 146/1000 | Loss: 0.00004050
Iteration 147/1000 | Loss: 0.00003805
Iteration 148/1000 | Loss: 0.00059251
Iteration 149/1000 | Loss: 0.00029395
Iteration 150/1000 | Loss: 0.00019734
Iteration 151/1000 | Loss: 0.00004041
Iteration 152/1000 | Loss: 0.00003662
Iteration 153/1000 | Loss: 0.00003383
Iteration 154/1000 | Loss: 0.00003241
Iteration 155/1000 | Loss: 0.00003177
Iteration 156/1000 | Loss: 0.00003110
Iteration 157/1000 | Loss: 0.00003067
Iteration 158/1000 | Loss: 0.00003027
Iteration 159/1000 | Loss: 0.00002990
Iteration 160/1000 | Loss: 0.00002967
Iteration 161/1000 | Loss: 0.00002949
Iteration 162/1000 | Loss: 0.00002928
Iteration 163/1000 | Loss: 0.00002904
Iteration 164/1000 | Loss: 0.00002883
Iteration 165/1000 | Loss: 0.00002876
Iteration 166/1000 | Loss: 0.00002875
Iteration 167/1000 | Loss: 0.00002871
Iteration 168/1000 | Loss: 0.00002868
Iteration 169/1000 | Loss: 0.00002867
Iteration 170/1000 | Loss: 0.00002867
Iteration 171/1000 | Loss: 0.00002867
Iteration 172/1000 | Loss: 0.00002863
Iteration 173/1000 | Loss: 0.00002861
Iteration 174/1000 | Loss: 0.00002860
Iteration 175/1000 | Loss: 0.00002860
Iteration 176/1000 | Loss: 0.00002859
Iteration 177/1000 | Loss: 0.00002859
Iteration 178/1000 | Loss: 0.00002859
Iteration 179/1000 | Loss: 0.00002857
Iteration 180/1000 | Loss: 0.00002857
Iteration 181/1000 | Loss: 0.00002857
Iteration 182/1000 | Loss: 0.00002857
Iteration 183/1000 | Loss: 0.00002857
Iteration 184/1000 | Loss: 0.00002856
Iteration 185/1000 | Loss: 0.00002856
Iteration 186/1000 | Loss: 0.00002856
Iteration 187/1000 | Loss: 0.00002856
Iteration 188/1000 | Loss: 0.00002855
Iteration 189/1000 | Loss: 0.00002855
Iteration 190/1000 | Loss: 0.00002854
Iteration 191/1000 | Loss: 0.00002854
Iteration 192/1000 | Loss: 0.00002854
Iteration 193/1000 | Loss: 0.00002854
Iteration 194/1000 | Loss: 0.00002853
Iteration 195/1000 | Loss: 0.00002853
Iteration 196/1000 | Loss: 0.00002853
Iteration 197/1000 | Loss: 0.00002852
Iteration 198/1000 | Loss: 0.00002851
Iteration 199/1000 | Loss: 0.00002847
Iteration 200/1000 | Loss: 0.00002842
Iteration 201/1000 | Loss: 0.00002841
Iteration 202/1000 | Loss: 0.00002841
Iteration 203/1000 | Loss: 0.00002840
Iteration 204/1000 | Loss: 0.00002839
Iteration 205/1000 | Loss: 0.00002838
Iteration 206/1000 | Loss: 0.00002838
Iteration 207/1000 | Loss: 0.00002838
Iteration 208/1000 | Loss: 0.00002837
Iteration 209/1000 | Loss: 0.00002837
Iteration 210/1000 | Loss: 0.00002836
Iteration 211/1000 | Loss: 0.00002836
Iteration 212/1000 | Loss: 0.00002835
Iteration 213/1000 | Loss: 0.00002835
Iteration 214/1000 | Loss: 0.00002834
Iteration 215/1000 | Loss: 0.00002833
Iteration 216/1000 | Loss: 0.00002833
Iteration 217/1000 | Loss: 0.00002833
Iteration 218/1000 | Loss: 0.00002833
Iteration 219/1000 | Loss: 0.00002833
Iteration 220/1000 | Loss: 0.00002833
Iteration 221/1000 | Loss: 0.00002833
Iteration 222/1000 | Loss: 0.00002832
Iteration 223/1000 | Loss: 0.00002829
Iteration 224/1000 | Loss: 0.00002829
Iteration 225/1000 | Loss: 0.00002826
Iteration 226/1000 | Loss: 0.00002826
Iteration 227/1000 | Loss: 0.00002826
Iteration 228/1000 | Loss: 0.00002826
Iteration 229/1000 | Loss: 0.00002826
Iteration 230/1000 | Loss: 0.00002826
Iteration 231/1000 | Loss: 0.00002825
Iteration 232/1000 | Loss: 0.00002825
Iteration 233/1000 | Loss: 0.00002825
Iteration 234/1000 | Loss: 0.00002824
Iteration 235/1000 | Loss: 0.00002824
Iteration 236/1000 | Loss: 0.00002823
Iteration 237/1000 | Loss: 0.00002823
Iteration 238/1000 | Loss: 0.00002823
Iteration 239/1000 | Loss: 0.00002823
Iteration 240/1000 | Loss: 0.00002823
Iteration 241/1000 | Loss: 0.00002823
Iteration 242/1000 | Loss: 0.00002823
Iteration 243/1000 | Loss: 0.00002822
Iteration 244/1000 | Loss: 0.00002822
Iteration 245/1000 | Loss: 0.00002822
Iteration 246/1000 | Loss: 0.00002822
Iteration 247/1000 | Loss: 0.00002822
Iteration 248/1000 | Loss: 0.00002822
Iteration 249/1000 | Loss: 0.00002822
Iteration 250/1000 | Loss: 0.00002821
Iteration 251/1000 | Loss: 0.00002821
Iteration 252/1000 | Loss: 0.00002821
Iteration 253/1000 | Loss: 0.00002821
Iteration 254/1000 | Loss: 0.00002821
Iteration 255/1000 | Loss: 0.00002821
Iteration 256/1000 | Loss: 0.00002821
Iteration 257/1000 | Loss: 0.00002820
Iteration 258/1000 | Loss: 0.00002820
Iteration 259/1000 | Loss: 0.00002820
Iteration 260/1000 | Loss: 0.00002820
Iteration 261/1000 | Loss: 0.00002820
Iteration 262/1000 | Loss: 0.00002820
Iteration 263/1000 | Loss: 0.00002819
Iteration 264/1000 | Loss: 0.00002819
Iteration 265/1000 | Loss: 0.00002819
Iteration 266/1000 | Loss: 0.00002819
Iteration 267/1000 | Loss: 0.00002819
Iteration 268/1000 | Loss: 0.00002819
Iteration 269/1000 | Loss: 0.00002819
Iteration 270/1000 | Loss: 0.00002819
Iteration 271/1000 | Loss: 0.00002819
Iteration 272/1000 | Loss: 0.00002819
Iteration 273/1000 | Loss: 0.00002819
Iteration 274/1000 | Loss: 0.00002819
Iteration 275/1000 | Loss: 0.00002819
Iteration 276/1000 | Loss: 0.00002819
Iteration 277/1000 | Loss: 0.00002819
Iteration 278/1000 | Loss: 0.00002818
Iteration 279/1000 | Loss: 0.00002818
Iteration 280/1000 | Loss: 0.00002818
Iteration 281/1000 | Loss: 0.00002818
Iteration 282/1000 | Loss: 0.00002818
Iteration 283/1000 | Loss: 0.00002818
Iteration 284/1000 | Loss: 0.00002817
Iteration 285/1000 | Loss: 0.00002817
Iteration 286/1000 | Loss: 0.00002817
Iteration 287/1000 | Loss: 0.00002817
Iteration 288/1000 | Loss: 0.00002817
Iteration 289/1000 | Loss: 0.00002817
Iteration 290/1000 | Loss: 0.00002816
Iteration 291/1000 | Loss: 0.00002816
Iteration 292/1000 | Loss: 0.00002816
Iteration 293/1000 | Loss: 0.00002816
Iteration 294/1000 | Loss: 0.00002816
Iteration 295/1000 | Loss: 0.00002816
Iteration 296/1000 | Loss: 0.00002816
Iteration 297/1000 | Loss: 0.00002816
Iteration 298/1000 | Loss: 0.00002816
Iteration 299/1000 | Loss: 0.00002816
Iteration 300/1000 | Loss: 0.00002816
Iteration 301/1000 | Loss: 0.00002816
Iteration 302/1000 | Loss: 0.00002816
Iteration 303/1000 | Loss: 0.00002815
Iteration 304/1000 | Loss: 0.00002815
Iteration 305/1000 | Loss: 0.00002815
Iteration 306/1000 | Loss: 0.00002815
Iteration 307/1000 | Loss: 0.00002815
Iteration 308/1000 | Loss: 0.00002815
Iteration 309/1000 | Loss: 0.00002814
Iteration 310/1000 | Loss: 0.00002814
Iteration 311/1000 | Loss: 0.00002814
Iteration 312/1000 | Loss: 0.00002814
Iteration 313/1000 | Loss: 0.00002814
Iteration 314/1000 | Loss: 0.00002814
Iteration 315/1000 | Loss: 0.00002814
Iteration 316/1000 | Loss: 0.00002814
Iteration 317/1000 | Loss: 0.00002814
Iteration 318/1000 | Loss: 0.00002814
Iteration 319/1000 | Loss: 0.00002814
Iteration 320/1000 | Loss: 0.00002814
Iteration 321/1000 | Loss: 0.00002814
Iteration 322/1000 | Loss: 0.00002814
Iteration 323/1000 | Loss: 0.00002814
Iteration 324/1000 | Loss: 0.00002814
Iteration 325/1000 | Loss: 0.00002814
Iteration 326/1000 | Loss: 0.00002814
Iteration 327/1000 | Loss: 0.00002814
Iteration 328/1000 | Loss: 0.00002813
Iteration 329/1000 | Loss: 0.00002813
Iteration 330/1000 | Loss: 0.00002813
Iteration 331/1000 | Loss: 0.00002813
Iteration 332/1000 | Loss: 0.00002813
Iteration 333/1000 | Loss: 0.00002813
Iteration 334/1000 | Loss: 0.00002813
Iteration 335/1000 | Loss: 0.00002813
Iteration 336/1000 | Loss: 0.00002813
Iteration 337/1000 | Loss: 0.00002813
Iteration 338/1000 | Loss: 0.00002813
Iteration 339/1000 | Loss: 0.00002813
Iteration 340/1000 | Loss: 0.00002813
Iteration 341/1000 | Loss: 0.00002813
Iteration 342/1000 | Loss: 0.00002813
Iteration 343/1000 | Loss: 0.00002813
Iteration 344/1000 | Loss: 0.00002813
Iteration 345/1000 | Loss: 0.00002813
Iteration 346/1000 | Loss: 0.00002813
Iteration 347/1000 | Loss: 0.00002813
Iteration 348/1000 | Loss: 0.00002813
Iteration 349/1000 | Loss: 0.00002813
Iteration 350/1000 | Loss: 0.00002813
Iteration 351/1000 | Loss: 0.00002813
Iteration 352/1000 | Loss: 0.00002813
Iteration 353/1000 | Loss: 0.00002813
Iteration 354/1000 | Loss: 0.00002813
Iteration 355/1000 | Loss: 0.00002813
Iteration 356/1000 | Loss: 0.00002813
Iteration 357/1000 | Loss: 0.00002813
Iteration 358/1000 | Loss: 0.00002813
Iteration 359/1000 | Loss: 0.00002813
Iteration 360/1000 | Loss: 0.00002813
Iteration 361/1000 | Loss: 0.00002813
Iteration 362/1000 | Loss: 0.00002813
Iteration 363/1000 | Loss: 0.00002813
Iteration 364/1000 | Loss: 0.00002813
Iteration 365/1000 | Loss: 0.00002813
Iteration 366/1000 | Loss: 0.00002813
Iteration 367/1000 | Loss: 0.00002813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 367. Stopping optimization.
Last 5 losses: [2.8130893042543903e-05, 2.8130893042543903e-05, 2.8130893042543903e-05, 2.8130893042543903e-05, 2.8130893042543903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8130893042543903e-05

Optimization complete. Final v2v error: 4.107236385345459 mm

Highest mean error: 12.72996997833252 mm for frame 125

Lowest mean error: 2.678133964538574 mm for frame 46

Saving results

Total time: 328.9535346031189
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00627726
Iteration 2/25 | Loss: 0.00144297
Iteration 3/25 | Loss: 0.00124305
Iteration 4/25 | Loss: 0.00118465
Iteration 5/25 | Loss: 0.00121010
Iteration 6/25 | Loss: 0.00117584
Iteration 7/25 | Loss: 0.00111446
Iteration 8/25 | Loss: 0.00107871
Iteration 9/25 | Loss: 0.00107094
Iteration 10/25 | Loss: 0.00106991
Iteration 11/25 | Loss: 0.00107143
Iteration 12/25 | Loss: 0.00107069
Iteration 13/25 | Loss: 0.00107000
Iteration 14/25 | Loss: 0.00106552
Iteration 15/25 | Loss: 0.00106216
Iteration 16/25 | Loss: 0.00105936
Iteration 17/25 | Loss: 0.00105774
Iteration 18/25 | Loss: 0.00105724
Iteration 19/25 | Loss: 0.00105709
Iteration 20/25 | Loss: 0.00105731
Iteration 21/25 | Loss: 0.00105535
Iteration 22/25 | Loss: 0.00105432
Iteration 23/25 | Loss: 0.00105416
Iteration 24/25 | Loss: 0.00105411
Iteration 25/25 | Loss: 0.00105411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25084031
Iteration 2/25 | Loss: 0.00067983
Iteration 3/25 | Loss: 0.00067974
Iteration 4/25 | Loss: 0.00067974
Iteration 5/25 | Loss: 0.00067974
Iteration 6/25 | Loss: 0.00067974
Iteration 7/25 | Loss: 0.00067974
Iteration 8/25 | Loss: 0.00067974
Iteration 9/25 | Loss: 0.00067974
Iteration 10/25 | Loss: 0.00067974
Iteration 11/25 | Loss: 0.00067974
Iteration 12/25 | Loss: 0.00067974
Iteration 13/25 | Loss: 0.00067974
Iteration 14/25 | Loss: 0.00067974
Iteration 15/25 | Loss: 0.00067974
Iteration 16/25 | Loss: 0.00067974
Iteration 17/25 | Loss: 0.00067974
Iteration 18/25 | Loss: 0.00067974
Iteration 19/25 | Loss: 0.00067974
Iteration 20/25 | Loss: 0.00067974
Iteration 21/25 | Loss: 0.00067974
Iteration 22/25 | Loss: 0.00067974
Iteration 23/25 | Loss: 0.00067974
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006797403329983354, 0.0006797403329983354, 0.0006797403329983354, 0.0006797403329983354, 0.0006797403329983354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006797403329983354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067974
Iteration 2/1000 | Loss: 0.00006594
Iteration 3/1000 | Loss: 0.00004040
Iteration 4/1000 | Loss: 0.00003328
Iteration 5/1000 | Loss: 0.00003133
Iteration 6/1000 | Loss: 0.00002997
Iteration 7/1000 | Loss: 0.00002899
Iteration 8/1000 | Loss: 0.00002821
Iteration 9/1000 | Loss: 0.00002765
Iteration 10/1000 | Loss: 0.00002727
Iteration 11/1000 | Loss: 0.00002691
Iteration 12/1000 | Loss: 0.00002660
Iteration 13/1000 | Loss: 0.00002635
Iteration 14/1000 | Loss: 0.00002613
Iteration 15/1000 | Loss: 0.00002592
Iteration 16/1000 | Loss: 0.00002577
Iteration 17/1000 | Loss: 0.00002564
Iteration 18/1000 | Loss: 0.00002552
Iteration 19/1000 | Loss: 0.00002550
Iteration 20/1000 | Loss: 0.00002545
Iteration 21/1000 | Loss: 0.00002542
Iteration 22/1000 | Loss: 0.00002541
Iteration 23/1000 | Loss: 0.00002540
Iteration 24/1000 | Loss: 0.00002540
Iteration 25/1000 | Loss: 0.00002539
Iteration 26/1000 | Loss: 0.00002539
Iteration 27/1000 | Loss: 0.00002538
Iteration 28/1000 | Loss: 0.00002538
Iteration 29/1000 | Loss: 0.00002537
Iteration 30/1000 | Loss: 0.00002536
Iteration 31/1000 | Loss: 0.00002536
Iteration 32/1000 | Loss: 0.00002535
Iteration 33/1000 | Loss: 0.00002535
Iteration 34/1000 | Loss: 0.00002534
Iteration 35/1000 | Loss: 0.00002534
Iteration 36/1000 | Loss: 0.00002534
Iteration 37/1000 | Loss: 0.00002533
Iteration 38/1000 | Loss: 0.00002533
Iteration 39/1000 | Loss: 0.00002532
Iteration 40/1000 | Loss: 0.00002532
Iteration 41/1000 | Loss: 0.00002532
Iteration 42/1000 | Loss: 0.00002531
Iteration 43/1000 | Loss: 0.00002531
Iteration 44/1000 | Loss: 0.00002531
Iteration 45/1000 | Loss: 0.00002530
Iteration 46/1000 | Loss: 0.00002530
Iteration 47/1000 | Loss: 0.00002530
Iteration 48/1000 | Loss: 0.00002529
Iteration 49/1000 | Loss: 0.00002529
Iteration 50/1000 | Loss: 0.00002529
Iteration 51/1000 | Loss: 0.00002529
Iteration 52/1000 | Loss: 0.00002529
Iteration 53/1000 | Loss: 0.00002529
Iteration 54/1000 | Loss: 0.00002528
Iteration 55/1000 | Loss: 0.00002528
Iteration 56/1000 | Loss: 0.00002528
Iteration 57/1000 | Loss: 0.00002528
Iteration 58/1000 | Loss: 0.00002528
Iteration 59/1000 | Loss: 0.00002527
Iteration 60/1000 | Loss: 0.00002527
Iteration 61/1000 | Loss: 0.00002527
Iteration 62/1000 | Loss: 0.00002527
Iteration 63/1000 | Loss: 0.00002527
Iteration 64/1000 | Loss: 0.00002527
Iteration 65/1000 | Loss: 0.00002527
Iteration 66/1000 | Loss: 0.00002527
Iteration 67/1000 | Loss: 0.00002527
Iteration 68/1000 | Loss: 0.00002527
Iteration 69/1000 | Loss: 0.00002527
Iteration 70/1000 | Loss: 0.00002527
Iteration 71/1000 | Loss: 0.00002526
Iteration 72/1000 | Loss: 0.00002526
Iteration 73/1000 | Loss: 0.00002526
Iteration 74/1000 | Loss: 0.00002526
Iteration 75/1000 | Loss: 0.00002526
Iteration 76/1000 | Loss: 0.00002526
Iteration 77/1000 | Loss: 0.00002526
Iteration 78/1000 | Loss: 0.00002526
Iteration 79/1000 | Loss: 0.00002526
Iteration 80/1000 | Loss: 0.00002526
Iteration 81/1000 | Loss: 0.00002526
Iteration 82/1000 | Loss: 0.00002525
Iteration 83/1000 | Loss: 0.00002525
Iteration 84/1000 | Loss: 0.00002525
Iteration 85/1000 | Loss: 0.00002525
Iteration 86/1000 | Loss: 0.00002525
Iteration 87/1000 | Loss: 0.00002525
Iteration 88/1000 | Loss: 0.00002525
Iteration 89/1000 | Loss: 0.00002525
Iteration 90/1000 | Loss: 0.00002525
Iteration 91/1000 | Loss: 0.00002524
Iteration 92/1000 | Loss: 0.00002524
Iteration 93/1000 | Loss: 0.00002524
Iteration 94/1000 | Loss: 0.00002524
Iteration 95/1000 | Loss: 0.00002524
Iteration 96/1000 | Loss: 0.00002523
Iteration 97/1000 | Loss: 0.00002523
Iteration 98/1000 | Loss: 0.00002523
Iteration 99/1000 | Loss: 0.00002523
Iteration 100/1000 | Loss: 0.00002522
Iteration 101/1000 | Loss: 0.00002522
Iteration 102/1000 | Loss: 0.00002522
Iteration 103/1000 | Loss: 0.00002522
Iteration 104/1000 | Loss: 0.00002522
Iteration 105/1000 | Loss: 0.00002522
Iteration 106/1000 | Loss: 0.00002522
Iteration 107/1000 | Loss: 0.00002522
Iteration 108/1000 | Loss: 0.00002522
Iteration 109/1000 | Loss: 0.00002522
Iteration 110/1000 | Loss: 0.00002522
Iteration 111/1000 | Loss: 0.00002522
Iteration 112/1000 | Loss: 0.00002522
Iteration 113/1000 | Loss: 0.00002522
Iteration 114/1000 | Loss: 0.00002522
Iteration 115/1000 | Loss: 0.00002522
Iteration 116/1000 | Loss: 0.00002522
Iteration 117/1000 | Loss: 0.00002522
Iteration 118/1000 | Loss: 0.00002522
Iteration 119/1000 | Loss: 0.00002522
Iteration 120/1000 | Loss: 0.00002522
Iteration 121/1000 | Loss: 0.00002522
Iteration 122/1000 | Loss: 0.00002522
Iteration 123/1000 | Loss: 0.00002522
Iteration 124/1000 | Loss: 0.00002522
Iteration 125/1000 | Loss: 0.00002522
Iteration 126/1000 | Loss: 0.00002522
Iteration 127/1000 | Loss: 0.00002522
Iteration 128/1000 | Loss: 0.00002522
Iteration 129/1000 | Loss: 0.00002522
Iteration 130/1000 | Loss: 0.00002522
Iteration 131/1000 | Loss: 0.00002522
Iteration 132/1000 | Loss: 0.00002522
Iteration 133/1000 | Loss: 0.00002522
Iteration 134/1000 | Loss: 0.00002522
Iteration 135/1000 | Loss: 0.00002522
Iteration 136/1000 | Loss: 0.00002522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.5219274903065525e-05, 2.5219274903065525e-05, 2.5219274903065525e-05, 2.5219274903065525e-05, 2.5219274903065525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5219274903065525e-05

Optimization complete. Final v2v error: 3.9246315956115723 mm

Highest mean error: 5.735409259796143 mm for frame 138

Lowest mean error: 3.1501941680908203 mm for frame 0

Saving results

Total time: 74.1881468296051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064102
Iteration 2/25 | Loss: 0.00213307
Iteration 3/25 | Loss: 0.00174913
Iteration 4/25 | Loss: 0.00146781
Iteration 5/25 | Loss: 0.00146385
Iteration 6/25 | Loss: 0.00125264
Iteration 7/25 | Loss: 0.00121442
Iteration 8/25 | Loss: 0.00123136
Iteration 9/25 | Loss: 0.00121784
Iteration 10/25 | Loss: 0.00127632
Iteration 11/25 | Loss: 0.00114981
Iteration 12/25 | Loss: 0.00113900
Iteration 13/25 | Loss: 0.00113713
Iteration 14/25 | Loss: 0.00114351
Iteration 15/25 | Loss: 0.00113041
Iteration 16/25 | Loss: 0.00112720
Iteration 17/25 | Loss: 0.00112602
Iteration 18/25 | Loss: 0.00112553
Iteration 19/25 | Loss: 0.00112521
Iteration 20/25 | Loss: 0.00112513
Iteration 21/25 | Loss: 0.00112513
Iteration 22/25 | Loss: 0.00112513
Iteration 23/25 | Loss: 0.00112513
Iteration 24/25 | Loss: 0.00112513
Iteration 25/25 | Loss: 0.00112512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79978204
Iteration 2/25 | Loss: 0.00144977
Iteration 3/25 | Loss: 0.00144974
Iteration 4/25 | Loss: 0.00144974
Iteration 5/25 | Loss: 0.00144974
Iteration 6/25 | Loss: 0.00144974
Iteration 7/25 | Loss: 0.00144974
Iteration 8/25 | Loss: 0.00144973
Iteration 9/25 | Loss: 0.00144973
Iteration 10/25 | Loss: 0.00144973
Iteration 11/25 | Loss: 0.00144973
Iteration 12/25 | Loss: 0.00144973
Iteration 13/25 | Loss: 0.00144973
Iteration 14/25 | Loss: 0.00144973
Iteration 15/25 | Loss: 0.00144973
Iteration 16/25 | Loss: 0.00144973
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001449734321795404, 0.001449734321795404, 0.001449734321795404, 0.001449734321795404, 0.001449734321795404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001449734321795404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144973
Iteration 2/1000 | Loss: 0.00023571
Iteration 3/1000 | Loss: 0.00040017
Iteration 4/1000 | Loss: 0.00150358
Iteration 5/1000 | Loss: 0.00077139
Iteration 6/1000 | Loss: 0.00018905
Iteration 7/1000 | Loss: 0.00154273
Iteration 8/1000 | Loss: 0.00065517
Iteration 9/1000 | Loss: 0.00051728
Iteration 10/1000 | Loss: 0.00107439
Iteration 11/1000 | Loss: 0.00077547
Iteration 12/1000 | Loss: 0.00075334
Iteration 13/1000 | Loss: 0.00012504
Iteration 14/1000 | Loss: 0.00010911
Iteration 15/1000 | Loss: 0.00007695
Iteration 16/1000 | Loss: 0.00006735
Iteration 17/1000 | Loss: 0.00006023
Iteration 18/1000 | Loss: 0.00095232
Iteration 19/1000 | Loss: 0.00008680
Iteration 20/1000 | Loss: 0.00005968
Iteration 21/1000 | Loss: 0.00005231
Iteration 22/1000 | Loss: 0.00004703
Iteration 23/1000 | Loss: 0.00004331
Iteration 24/1000 | Loss: 0.00004120
Iteration 25/1000 | Loss: 0.00004025
Iteration 26/1000 | Loss: 0.00003955
Iteration 27/1000 | Loss: 0.00003886
Iteration 28/1000 | Loss: 0.00003816
Iteration 29/1000 | Loss: 0.00003762
Iteration 30/1000 | Loss: 0.00035703
Iteration 31/1000 | Loss: 0.00196723
Iteration 32/1000 | Loss: 0.00012444
Iteration 33/1000 | Loss: 0.00007779
Iteration 34/1000 | Loss: 0.00005437
Iteration 35/1000 | Loss: 0.00003743
Iteration 36/1000 | Loss: 0.00002692
Iteration 37/1000 | Loss: 0.00002220
Iteration 38/1000 | Loss: 0.00001982
Iteration 39/1000 | Loss: 0.00001835
Iteration 40/1000 | Loss: 0.00001734
Iteration 41/1000 | Loss: 0.00001646
Iteration 42/1000 | Loss: 0.00001589
Iteration 43/1000 | Loss: 0.00001539
Iteration 44/1000 | Loss: 0.00001495
Iteration 45/1000 | Loss: 0.00001459
Iteration 46/1000 | Loss: 0.00001433
Iteration 47/1000 | Loss: 0.00001412
Iteration 48/1000 | Loss: 0.00001397
Iteration 49/1000 | Loss: 0.00001390
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001388
Iteration 52/1000 | Loss: 0.00001387
Iteration 53/1000 | Loss: 0.00001386
Iteration 54/1000 | Loss: 0.00001385
Iteration 55/1000 | Loss: 0.00001379
Iteration 56/1000 | Loss: 0.00001379
Iteration 57/1000 | Loss: 0.00001379
Iteration 58/1000 | Loss: 0.00001377
Iteration 59/1000 | Loss: 0.00001377
Iteration 60/1000 | Loss: 0.00001377
Iteration 61/1000 | Loss: 0.00001377
Iteration 62/1000 | Loss: 0.00001377
Iteration 63/1000 | Loss: 0.00001377
Iteration 64/1000 | Loss: 0.00001377
Iteration 65/1000 | Loss: 0.00001377
Iteration 66/1000 | Loss: 0.00001377
Iteration 67/1000 | Loss: 0.00001376
Iteration 68/1000 | Loss: 0.00001376
Iteration 69/1000 | Loss: 0.00001376
Iteration 70/1000 | Loss: 0.00001376
Iteration 71/1000 | Loss: 0.00001376
Iteration 72/1000 | Loss: 0.00001376
Iteration 73/1000 | Loss: 0.00001376
Iteration 74/1000 | Loss: 0.00001376
Iteration 75/1000 | Loss: 0.00001375
Iteration 76/1000 | Loss: 0.00001374
Iteration 77/1000 | Loss: 0.00001374
Iteration 78/1000 | Loss: 0.00001374
Iteration 79/1000 | Loss: 0.00001374
Iteration 80/1000 | Loss: 0.00001374
Iteration 81/1000 | Loss: 0.00001373
Iteration 82/1000 | Loss: 0.00001373
Iteration 83/1000 | Loss: 0.00001373
Iteration 84/1000 | Loss: 0.00001373
Iteration 85/1000 | Loss: 0.00001373
Iteration 86/1000 | Loss: 0.00001373
Iteration 87/1000 | Loss: 0.00001373
Iteration 88/1000 | Loss: 0.00001373
Iteration 89/1000 | Loss: 0.00001373
Iteration 90/1000 | Loss: 0.00001372
Iteration 91/1000 | Loss: 0.00001372
Iteration 92/1000 | Loss: 0.00001372
Iteration 93/1000 | Loss: 0.00001372
Iteration 94/1000 | Loss: 0.00001372
Iteration 95/1000 | Loss: 0.00001372
Iteration 96/1000 | Loss: 0.00001372
Iteration 97/1000 | Loss: 0.00001372
Iteration 98/1000 | Loss: 0.00001372
Iteration 99/1000 | Loss: 0.00001371
Iteration 100/1000 | Loss: 0.00001371
Iteration 101/1000 | Loss: 0.00001371
Iteration 102/1000 | Loss: 0.00001371
Iteration 103/1000 | Loss: 0.00001371
Iteration 104/1000 | Loss: 0.00001371
Iteration 105/1000 | Loss: 0.00001371
Iteration 106/1000 | Loss: 0.00001371
Iteration 107/1000 | Loss: 0.00001371
Iteration 108/1000 | Loss: 0.00001371
Iteration 109/1000 | Loss: 0.00001371
Iteration 110/1000 | Loss: 0.00001371
Iteration 111/1000 | Loss: 0.00001371
Iteration 112/1000 | Loss: 0.00001371
Iteration 113/1000 | Loss: 0.00001371
Iteration 114/1000 | Loss: 0.00001371
Iteration 115/1000 | Loss: 0.00001371
Iteration 116/1000 | Loss: 0.00001371
Iteration 117/1000 | Loss: 0.00001371
Iteration 118/1000 | Loss: 0.00001371
Iteration 119/1000 | Loss: 0.00001371
Iteration 120/1000 | Loss: 0.00001371
Iteration 121/1000 | Loss: 0.00001371
Iteration 122/1000 | Loss: 0.00001371
Iteration 123/1000 | Loss: 0.00001371
Iteration 124/1000 | Loss: 0.00001371
Iteration 125/1000 | Loss: 0.00001371
Iteration 126/1000 | Loss: 0.00001371
Iteration 127/1000 | Loss: 0.00001371
Iteration 128/1000 | Loss: 0.00001371
Iteration 129/1000 | Loss: 0.00001371
Iteration 130/1000 | Loss: 0.00001371
Iteration 131/1000 | Loss: 0.00001371
Iteration 132/1000 | Loss: 0.00001371
Iteration 133/1000 | Loss: 0.00001371
Iteration 134/1000 | Loss: 0.00001371
Iteration 135/1000 | Loss: 0.00001371
Iteration 136/1000 | Loss: 0.00001371
Iteration 137/1000 | Loss: 0.00001371
Iteration 138/1000 | Loss: 0.00001371
Iteration 139/1000 | Loss: 0.00001371
Iteration 140/1000 | Loss: 0.00001371
Iteration 141/1000 | Loss: 0.00001371
Iteration 142/1000 | Loss: 0.00001371
Iteration 143/1000 | Loss: 0.00001371
Iteration 144/1000 | Loss: 0.00001371
Iteration 145/1000 | Loss: 0.00001371
Iteration 146/1000 | Loss: 0.00001371
Iteration 147/1000 | Loss: 0.00001371
Iteration 148/1000 | Loss: 0.00001371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.3708530786971096e-05, 1.3708530786971096e-05, 1.3708530786971096e-05, 1.3708530786971096e-05, 1.3708530786971096e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3708530786971096e-05

Optimization complete. Final v2v error: 3.0475962162017822 mm

Highest mean error: 3.881596088409424 mm for frame 88

Lowest mean error: 2.3946049213409424 mm for frame 28

Saving results

Total time: 106.67732048034668
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468602
Iteration 2/25 | Loss: 0.00110339
Iteration 3/25 | Loss: 0.00095985
Iteration 4/25 | Loss: 0.00094453
Iteration 5/25 | Loss: 0.00094098
Iteration 6/25 | Loss: 0.00093995
Iteration 7/25 | Loss: 0.00093995
Iteration 8/25 | Loss: 0.00093995
Iteration 9/25 | Loss: 0.00093995
Iteration 10/25 | Loss: 0.00093995
Iteration 11/25 | Loss: 0.00093995
Iteration 12/25 | Loss: 0.00093995
Iteration 13/25 | Loss: 0.00093995
Iteration 14/25 | Loss: 0.00093995
Iteration 15/25 | Loss: 0.00093995
Iteration 16/25 | Loss: 0.00093995
Iteration 17/25 | Loss: 0.00093995
Iteration 18/25 | Loss: 0.00093995
Iteration 19/25 | Loss: 0.00093995
Iteration 20/25 | Loss: 0.00093995
Iteration 21/25 | Loss: 0.00093995
Iteration 22/25 | Loss: 0.00093995
Iteration 23/25 | Loss: 0.00093995
Iteration 24/25 | Loss: 0.00093995
Iteration 25/25 | Loss: 0.00093995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.84951043
Iteration 2/25 | Loss: 0.00053791
Iteration 3/25 | Loss: 0.00053791
Iteration 4/25 | Loss: 0.00053790
Iteration 5/25 | Loss: 0.00053790
Iteration 6/25 | Loss: 0.00053790
Iteration 7/25 | Loss: 0.00053790
Iteration 8/25 | Loss: 0.00053790
Iteration 9/25 | Loss: 0.00053790
Iteration 10/25 | Loss: 0.00053790
Iteration 11/25 | Loss: 0.00053790
Iteration 12/25 | Loss: 0.00053790
Iteration 13/25 | Loss: 0.00053790
Iteration 14/25 | Loss: 0.00053790
Iteration 15/25 | Loss: 0.00053790
Iteration 16/25 | Loss: 0.00053790
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005379027570597827, 0.0005379027570597827, 0.0005379027570597827, 0.0005379027570597827, 0.0005379027570597827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005379027570597827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053790
Iteration 2/1000 | Loss: 0.00001625
Iteration 3/1000 | Loss: 0.00001285
Iteration 4/1000 | Loss: 0.00001185
Iteration 5/1000 | Loss: 0.00001135
Iteration 6/1000 | Loss: 0.00001130
Iteration 7/1000 | Loss: 0.00001092
Iteration 8/1000 | Loss: 0.00001081
Iteration 9/1000 | Loss: 0.00001081
Iteration 10/1000 | Loss: 0.00001081
Iteration 11/1000 | Loss: 0.00001078
Iteration 12/1000 | Loss: 0.00001076
Iteration 13/1000 | Loss: 0.00001076
Iteration 14/1000 | Loss: 0.00001075
Iteration 15/1000 | Loss: 0.00001075
Iteration 16/1000 | Loss: 0.00001074
Iteration 17/1000 | Loss: 0.00001070
Iteration 18/1000 | Loss: 0.00001070
Iteration 19/1000 | Loss: 0.00001070
Iteration 20/1000 | Loss: 0.00001070
Iteration 21/1000 | Loss: 0.00001069
Iteration 22/1000 | Loss: 0.00001069
Iteration 23/1000 | Loss: 0.00001069
Iteration 24/1000 | Loss: 0.00001069
Iteration 25/1000 | Loss: 0.00001069
Iteration 26/1000 | Loss: 0.00001069
Iteration 27/1000 | Loss: 0.00001068
Iteration 28/1000 | Loss: 0.00001068
Iteration 29/1000 | Loss: 0.00001066
Iteration 30/1000 | Loss: 0.00001065
Iteration 31/1000 | Loss: 0.00001065
Iteration 32/1000 | Loss: 0.00001065
Iteration 33/1000 | Loss: 0.00001065
Iteration 34/1000 | Loss: 0.00001065
Iteration 35/1000 | Loss: 0.00001065
Iteration 36/1000 | Loss: 0.00001065
Iteration 37/1000 | Loss: 0.00001064
Iteration 38/1000 | Loss: 0.00001064
Iteration 39/1000 | Loss: 0.00001064
Iteration 40/1000 | Loss: 0.00001063
Iteration 41/1000 | Loss: 0.00001063
Iteration 42/1000 | Loss: 0.00001063
Iteration 43/1000 | Loss: 0.00001062
Iteration 44/1000 | Loss: 0.00001062
Iteration 45/1000 | Loss: 0.00001062
Iteration 46/1000 | Loss: 0.00001062
Iteration 47/1000 | Loss: 0.00001062
Iteration 48/1000 | Loss: 0.00001061
Iteration 49/1000 | Loss: 0.00001061
Iteration 50/1000 | Loss: 0.00001061
Iteration 51/1000 | Loss: 0.00001060
Iteration 52/1000 | Loss: 0.00001060
Iteration 53/1000 | Loss: 0.00001060
Iteration 54/1000 | Loss: 0.00001060
Iteration 55/1000 | Loss: 0.00001059
Iteration 56/1000 | Loss: 0.00001059
Iteration 57/1000 | Loss: 0.00001059
Iteration 58/1000 | Loss: 0.00001059
Iteration 59/1000 | Loss: 0.00001059
Iteration 60/1000 | Loss: 0.00001059
Iteration 61/1000 | Loss: 0.00001059
Iteration 62/1000 | Loss: 0.00001059
Iteration 63/1000 | Loss: 0.00001059
Iteration 64/1000 | Loss: 0.00001059
Iteration 65/1000 | Loss: 0.00001058
Iteration 66/1000 | Loss: 0.00001058
Iteration 67/1000 | Loss: 0.00001058
Iteration 68/1000 | Loss: 0.00001058
Iteration 69/1000 | Loss: 0.00001058
Iteration 70/1000 | Loss: 0.00001057
Iteration 71/1000 | Loss: 0.00001057
Iteration 72/1000 | Loss: 0.00001057
Iteration 73/1000 | Loss: 0.00001057
Iteration 74/1000 | Loss: 0.00001057
Iteration 75/1000 | Loss: 0.00001057
Iteration 76/1000 | Loss: 0.00001056
Iteration 77/1000 | Loss: 0.00001056
Iteration 78/1000 | Loss: 0.00001056
Iteration 79/1000 | Loss: 0.00001056
Iteration 80/1000 | Loss: 0.00001056
Iteration 81/1000 | Loss: 0.00001056
Iteration 82/1000 | Loss: 0.00001056
Iteration 83/1000 | Loss: 0.00001056
Iteration 84/1000 | Loss: 0.00001056
Iteration 85/1000 | Loss: 0.00001056
Iteration 86/1000 | Loss: 0.00001056
Iteration 87/1000 | Loss: 0.00001056
Iteration 88/1000 | Loss: 0.00001056
Iteration 89/1000 | Loss: 0.00001056
Iteration 90/1000 | Loss: 0.00001056
Iteration 91/1000 | Loss: 0.00001056
Iteration 92/1000 | Loss: 0.00001056
Iteration 93/1000 | Loss: 0.00001056
Iteration 94/1000 | Loss: 0.00001056
Iteration 95/1000 | Loss: 0.00001056
Iteration 96/1000 | Loss: 0.00001056
Iteration 97/1000 | Loss: 0.00001056
Iteration 98/1000 | Loss: 0.00001056
Iteration 99/1000 | Loss: 0.00001056
Iteration 100/1000 | Loss: 0.00001056
Iteration 101/1000 | Loss: 0.00001056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.0557678251643665e-05, 1.0557678251643665e-05, 1.0557678251643665e-05, 1.0557678251643665e-05, 1.0557678251643665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0557678251643665e-05

Optimization complete. Final v2v error: 2.728278398513794 mm

Highest mean error: 3.2025108337402344 mm for frame 121

Lowest mean error: 2.4013423919677734 mm for frame 31

Saving results

Total time: 28.82075071334839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428838
Iteration 2/25 | Loss: 0.00109062
Iteration 3/25 | Loss: 0.00095893
Iteration 4/25 | Loss: 0.00094364
Iteration 5/25 | Loss: 0.00094003
Iteration 6/25 | Loss: 0.00093913
Iteration 7/25 | Loss: 0.00093913
Iteration 8/25 | Loss: 0.00093913
Iteration 9/25 | Loss: 0.00093913
Iteration 10/25 | Loss: 0.00093913
Iteration 11/25 | Loss: 0.00093913
Iteration 12/25 | Loss: 0.00093913
Iteration 13/25 | Loss: 0.00093913
Iteration 14/25 | Loss: 0.00093913
Iteration 15/25 | Loss: 0.00093913
Iteration 16/25 | Loss: 0.00093913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009391253115609288, 0.0009391253115609288, 0.0009391253115609288, 0.0009391253115609288, 0.0009391253115609288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009391253115609288

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31828070
Iteration 2/25 | Loss: 0.00063425
Iteration 3/25 | Loss: 0.00063425
Iteration 4/25 | Loss: 0.00063425
Iteration 5/25 | Loss: 0.00063425
Iteration 6/25 | Loss: 0.00063425
Iteration 7/25 | Loss: 0.00063425
Iteration 8/25 | Loss: 0.00063425
Iteration 9/25 | Loss: 0.00063425
Iteration 10/25 | Loss: 0.00063425
Iteration 11/25 | Loss: 0.00063425
Iteration 12/25 | Loss: 0.00063425
Iteration 13/25 | Loss: 0.00063425
Iteration 14/25 | Loss: 0.00063425
Iteration 15/25 | Loss: 0.00063425
Iteration 16/25 | Loss: 0.00063425
Iteration 17/25 | Loss: 0.00063425
Iteration 18/25 | Loss: 0.00063425
Iteration 19/25 | Loss: 0.00063425
Iteration 20/25 | Loss: 0.00063425
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006342475535348058, 0.0006342475535348058, 0.0006342475535348058, 0.0006342475535348058, 0.0006342475535348058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006342475535348058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063425
Iteration 2/1000 | Loss: 0.00002849
Iteration 3/1000 | Loss: 0.00001599
Iteration 4/1000 | Loss: 0.00001238
Iteration 5/1000 | Loss: 0.00001168
Iteration 6/1000 | Loss: 0.00001113
Iteration 7/1000 | Loss: 0.00001083
Iteration 8/1000 | Loss: 0.00001057
Iteration 9/1000 | Loss: 0.00001043
Iteration 10/1000 | Loss: 0.00001040
Iteration 11/1000 | Loss: 0.00001033
Iteration 12/1000 | Loss: 0.00001027
Iteration 13/1000 | Loss: 0.00001027
Iteration 14/1000 | Loss: 0.00001025
Iteration 15/1000 | Loss: 0.00001024
Iteration 16/1000 | Loss: 0.00001024
Iteration 17/1000 | Loss: 0.00001023
Iteration 18/1000 | Loss: 0.00001023
Iteration 19/1000 | Loss: 0.00001022
Iteration 20/1000 | Loss: 0.00001020
Iteration 21/1000 | Loss: 0.00001019
Iteration 22/1000 | Loss: 0.00001016
Iteration 23/1000 | Loss: 0.00001015
Iteration 24/1000 | Loss: 0.00001013
Iteration 25/1000 | Loss: 0.00001012
Iteration 26/1000 | Loss: 0.00001011
Iteration 27/1000 | Loss: 0.00001011
Iteration 28/1000 | Loss: 0.00001010
Iteration 29/1000 | Loss: 0.00001010
Iteration 30/1000 | Loss: 0.00001010
Iteration 31/1000 | Loss: 0.00001009
Iteration 32/1000 | Loss: 0.00001009
Iteration 33/1000 | Loss: 0.00001008
Iteration 34/1000 | Loss: 0.00001008
Iteration 35/1000 | Loss: 0.00001007
Iteration 36/1000 | Loss: 0.00001006
Iteration 37/1000 | Loss: 0.00001006
Iteration 38/1000 | Loss: 0.00001006
Iteration 39/1000 | Loss: 0.00001005
Iteration 40/1000 | Loss: 0.00001005
Iteration 41/1000 | Loss: 0.00001005
Iteration 42/1000 | Loss: 0.00001005
Iteration 43/1000 | Loss: 0.00001004
Iteration 44/1000 | Loss: 0.00001004
Iteration 45/1000 | Loss: 0.00001004
Iteration 46/1000 | Loss: 0.00001004
Iteration 47/1000 | Loss: 0.00001004
Iteration 48/1000 | Loss: 0.00001004
Iteration 49/1000 | Loss: 0.00001003
Iteration 50/1000 | Loss: 0.00001003
Iteration 51/1000 | Loss: 0.00001003
Iteration 52/1000 | Loss: 0.00001002
Iteration 53/1000 | Loss: 0.00001002
Iteration 54/1000 | Loss: 0.00001002
Iteration 55/1000 | Loss: 0.00001002
Iteration 56/1000 | Loss: 0.00001001
Iteration 57/1000 | Loss: 0.00001001
Iteration 58/1000 | Loss: 0.00001001
Iteration 59/1000 | Loss: 0.00001001
Iteration 60/1000 | Loss: 0.00001001
Iteration 61/1000 | Loss: 0.00001001
Iteration 62/1000 | Loss: 0.00001001
Iteration 63/1000 | Loss: 0.00001001
Iteration 64/1000 | Loss: 0.00001001
Iteration 65/1000 | Loss: 0.00001000
Iteration 66/1000 | Loss: 0.00000998
Iteration 67/1000 | Loss: 0.00000997
Iteration 68/1000 | Loss: 0.00000997
Iteration 69/1000 | Loss: 0.00000997
Iteration 70/1000 | Loss: 0.00000996
Iteration 71/1000 | Loss: 0.00000996
Iteration 72/1000 | Loss: 0.00000996
Iteration 73/1000 | Loss: 0.00000996
Iteration 74/1000 | Loss: 0.00000995
Iteration 75/1000 | Loss: 0.00000995
Iteration 76/1000 | Loss: 0.00000995
Iteration 77/1000 | Loss: 0.00000994
Iteration 78/1000 | Loss: 0.00000994
Iteration 79/1000 | Loss: 0.00000994
Iteration 80/1000 | Loss: 0.00000994
Iteration 81/1000 | Loss: 0.00000994
Iteration 82/1000 | Loss: 0.00000994
Iteration 83/1000 | Loss: 0.00000994
Iteration 84/1000 | Loss: 0.00000994
Iteration 85/1000 | Loss: 0.00000994
Iteration 86/1000 | Loss: 0.00000994
Iteration 87/1000 | Loss: 0.00000994
Iteration 88/1000 | Loss: 0.00000993
Iteration 89/1000 | Loss: 0.00000993
Iteration 90/1000 | Loss: 0.00000993
Iteration 91/1000 | Loss: 0.00000993
Iteration 92/1000 | Loss: 0.00000993
Iteration 93/1000 | Loss: 0.00000993
Iteration 94/1000 | Loss: 0.00000993
Iteration 95/1000 | Loss: 0.00000993
Iteration 96/1000 | Loss: 0.00000993
Iteration 97/1000 | Loss: 0.00000992
Iteration 98/1000 | Loss: 0.00000992
Iteration 99/1000 | Loss: 0.00000992
Iteration 100/1000 | Loss: 0.00000992
Iteration 101/1000 | Loss: 0.00000992
Iteration 102/1000 | Loss: 0.00000992
Iteration 103/1000 | Loss: 0.00000992
Iteration 104/1000 | Loss: 0.00000992
Iteration 105/1000 | Loss: 0.00000992
Iteration 106/1000 | Loss: 0.00000991
Iteration 107/1000 | Loss: 0.00000991
Iteration 108/1000 | Loss: 0.00000991
Iteration 109/1000 | Loss: 0.00000991
Iteration 110/1000 | Loss: 0.00000991
Iteration 111/1000 | Loss: 0.00000991
Iteration 112/1000 | Loss: 0.00000991
Iteration 113/1000 | Loss: 0.00000991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [9.912057976180222e-06, 9.912057976180222e-06, 9.912057976180222e-06, 9.912057976180222e-06, 9.912057976180222e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.912057976180222e-06

Optimization complete. Final v2v error: 2.6294524669647217 mm

Highest mean error: 3.718461036682129 mm for frame 61

Lowest mean error: 2.3291335105895996 mm for frame 212

Saving results

Total time: 36.79467034339905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017510
Iteration 2/25 | Loss: 0.00136615
Iteration 3/25 | Loss: 0.00110852
Iteration 4/25 | Loss: 0.00104868
Iteration 5/25 | Loss: 0.00104085
Iteration 6/25 | Loss: 0.00103773
Iteration 7/25 | Loss: 0.00103665
Iteration 8/25 | Loss: 0.00103644
Iteration 9/25 | Loss: 0.00103640
Iteration 10/25 | Loss: 0.00103640
Iteration 11/25 | Loss: 0.00103640
Iteration 12/25 | Loss: 0.00103640
Iteration 13/25 | Loss: 0.00103640
Iteration 14/25 | Loss: 0.00103640
Iteration 15/25 | Loss: 0.00103640
Iteration 16/25 | Loss: 0.00103640
Iteration 17/25 | Loss: 0.00103640
Iteration 18/25 | Loss: 0.00103640
Iteration 19/25 | Loss: 0.00103640
Iteration 20/25 | Loss: 0.00103640
Iteration 21/25 | Loss: 0.00103640
Iteration 22/25 | Loss: 0.00103640
Iteration 23/25 | Loss: 0.00103640
Iteration 24/25 | Loss: 0.00103640
Iteration 25/25 | Loss: 0.00103640

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31063080
Iteration 2/25 | Loss: 0.00074240
Iteration 3/25 | Loss: 0.00074240
Iteration 4/25 | Loss: 0.00074240
Iteration 5/25 | Loss: 0.00074240
Iteration 6/25 | Loss: 0.00074240
Iteration 7/25 | Loss: 0.00074240
Iteration 8/25 | Loss: 0.00074240
Iteration 9/25 | Loss: 0.00074240
Iteration 10/25 | Loss: 0.00074240
Iteration 11/25 | Loss: 0.00074240
Iteration 12/25 | Loss: 0.00074240
Iteration 13/25 | Loss: 0.00074240
Iteration 14/25 | Loss: 0.00074240
Iteration 15/25 | Loss: 0.00074240
Iteration 16/25 | Loss: 0.00074240
Iteration 17/25 | Loss: 0.00074240
Iteration 18/25 | Loss: 0.00074240
Iteration 19/25 | Loss: 0.00074240
Iteration 20/25 | Loss: 0.00074240
Iteration 21/25 | Loss: 0.00074240
Iteration 22/25 | Loss: 0.00074240
Iteration 23/25 | Loss: 0.00074240
Iteration 24/25 | Loss: 0.00074240
Iteration 25/25 | Loss: 0.00074240

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074240
Iteration 2/1000 | Loss: 0.00003494
Iteration 3/1000 | Loss: 0.00002529
Iteration 4/1000 | Loss: 0.00002285
Iteration 5/1000 | Loss: 0.00002147
Iteration 6/1000 | Loss: 0.00002071
Iteration 7/1000 | Loss: 0.00002006
Iteration 8/1000 | Loss: 0.00001957
Iteration 9/1000 | Loss: 0.00001920
Iteration 10/1000 | Loss: 0.00001916
Iteration 11/1000 | Loss: 0.00001904
Iteration 12/1000 | Loss: 0.00001899
Iteration 13/1000 | Loss: 0.00001898
Iteration 14/1000 | Loss: 0.00001896
Iteration 15/1000 | Loss: 0.00001886
Iteration 16/1000 | Loss: 0.00001886
Iteration 17/1000 | Loss: 0.00001879
Iteration 18/1000 | Loss: 0.00001875
Iteration 19/1000 | Loss: 0.00001873
Iteration 20/1000 | Loss: 0.00001873
Iteration 21/1000 | Loss: 0.00001872
Iteration 22/1000 | Loss: 0.00001870
Iteration 23/1000 | Loss: 0.00001870
Iteration 24/1000 | Loss: 0.00001869
Iteration 25/1000 | Loss: 0.00001869
Iteration 26/1000 | Loss: 0.00001869
Iteration 27/1000 | Loss: 0.00001868
Iteration 28/1000 | Loss: 0.00001868
Iteration 29/1000 | Loss: 0.00001868
Iteration 30/1000 | Loss: 0.00001867
Iteration 31/1000 | Loss: 0.00001867
Iteration 32/1000 | Loss: 0.00001867
Iteration 33/1000 | Loss: 0.00001867
Iteration 34/1000 | Loss: 0.00001867
Iteration 35/1000 | Loss: 0.00001867
Iteration 36/1000 | Loss: 0.00001867
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001867
Iteration 40/1000 | Loss: 0.00001866
Iteration 41/1000 | Loss: 0.00001866
Iteration 42/1000 | Loss: 0.00001866
Iteration 43/1000 | Loss: 0.00001866
Iteration 44/1000 | Loss: 0.00001866
Iteration 45/1000 | Loss: 0.00001866
Iteration 46/1000 | Loss: 0.00001866
Iteration 47/1000 | Loss: 0.00001866
Iteration 48/1000 | Loss: 0.00001866
Iteration 49/1000 | Loss: 0.00001866
Iteration 50/1000 | Loss: 0.00001866
Iteration 51/1000 | Loss: 0.00001866
Iteration 52/1000 | Loss: 0.00001866
Iteration 53/1000 | Loss: 0.00001866
Iteration 54/1000 | Loss: 0.00001866
Iteration 55/1000 | Loss: 0.00001866
Iteration 56/1000 | Loss: 0.00001866
Iteration 57/1000 | Loss: 0.00001866
Iteration 58/1000 | Loss: 0.00001866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [1.866248385340441e-05, 1.866248385340441e-05, 1.866248385340441e-05, 1.866248385340441e-05, 1.866248385340441e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.866248385340441e-05

Optimization complete. Final v2v error: 3.627732753753662 mm

Highest mean error: 3.7249832153320312 mm for frame 115

Lowest mean error: 3.4339449405670166 mm for frame 151

Saving results

Total time: 33.04655122756958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401401
Iteration 2/25 | Loss: 0.00111153
Iteration 3/25 | Loss: 0.00101625
Iteration 4/25 | Loss: 0.00099516
Iteration 5/25 | Loss: 0.00098867
Iteration 6/25 | Loss: 0.00098673
Iteration 7/25 | Loss: 0.00098594
Iteration 8/25 | Loss: 0.00098594
Iteration 9/25 | Loss: 0.00098594
Iteration 10/25 | Loss: 0.00098594
Iteration 11/25 | Loss: 0.00098594
Iteration 12/25 | Loss: 0.00098594
Iteration 13/25 | Loss: 0.00098594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000985937425866723, 0.000985937425866723, 0.000985937425866723, 0.000985937425866723, 0.000985937425866723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000985937425866723

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33756876
Iteration 2/25 | Loss: 0.00110109
Iteration 3/25 | Loss: 0.00110109
Iteration 4/25 | Loss: 0.00110109
Iteration 5/25 | Loss: 0.00110109
Iteration 6/25 | Loss: 0.00110109
Iteration 7/25 | Loss: 0.00110109
Iteration 8/25 | Loss: 0.00110109
Iteration 9/25 | Loss: 0.00110109
Iteration 10/25 | Loss: 0.00110109
Iteration 11/25 | Loss: 0.00110109
Iteration 12/25 | Loss: 0.00110109
Iteration 13/25 | Loss: 0.00110109
Iteration 14/25 | Loss: 0.00110109
Iteration 15/25 | Loss: 0.00110109
Iteration 16/25 | Loss: 0.00110109
Iteration 17/25 | Loss: 0.00110109
Iteration 18/25 | Loss: 0.00110109
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011010918533429503, 0.0011010918533429503, 0.0011010918533429503, 0.0011010918533429503, 0.0011010918533429503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011010918533429503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110109
Iteration 2/1000 | Loss: 0.00003637
Iteration 3/1000 | Loss: 0.00002111
Iteration 4/1000 | Loss: 0.00001765
Iteration 5/1000 | Loss: 0.00001636
Iteration 6/1000 | Loss: 0.00001556
Iteration 7/1000 | Loss: 0.00001490
Iteration 8/1000 | Loss: 0.00001449
Iteration 9/1000 | Loss: 0.00001425
Iteration 10/1000 | Loss: 0.00001406
Iteration 11/1000 | Loss: 0.00001403
Iteration 12/1000 | Loss: 0.00001401
Iteration 13/1000 | Loss: 0.00001392
Iteration 14/1000 | Loss: 0.00001385
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001378
Iteration 17/1000 | Loss: 0.00001364
Iteration 18/1000 | Loss: 0.00001360
Iteration 19/1000 | Loss: 0.00001346
Iteration 20/1000 | Loss: 0.00001342
Iteration 21/1000 | Loss: 0.00001334
Iteration 22/1000 | Loss: 0.00001333
Iteration 23/1000 | Loss: 0.00001333
Iteration 24/1000 | Loss: 0.00001332
Iteration 25/1000 | Loss: 0.00001331
Iteration 26/1000 | Loss: 0.00001331
Iteration 27/1000 | Loss: 0.00001330
Iteration 28/1000 | Loss: 0.00001330
Iteration 29/1000 | Loss: 0.00001330
Iteration 30/1000 | Loss: 0.00001328
Iteration 31/1000 | Loss: 0.00001328
Iteration 32/1000 | Loss: 0.00001327
Iteration 33/1000 | Loss: 0.00001327
Iteration 34/1000 | Loss: 0.00001326
Iteration 35/1000 | Loss: 0.00001325
Iteration 36/1000 | Loss: 0.00001321
Iteration 37/1000 | Loss: 0.00001321
Iteration 38/1000 | Loss: 0.00001319
Iteration 39/1000 | Loss: 0.00001318
Iteration 40/1000 | Loss: 0.00001317
Iteration 41/1000 | Loss: 0.00001316
Iteration 42/1000 | Loss: 0.00001315
Iteration 43/1000 | Loss: 0.00001315
Iteration 44/1000 | Loss: 0.00001315
Iteration 45/1000 | Loss: 0.00001315
Iteration 46/1000 | Loss: 0.00001314
Iteration 47/1000 | Loss: 0.00001314
Iteration 48/1000 | Loss: 0.00001313
Iteration 49/1000 | Loss: 0.00001313
Iteration 50/1000 | Loss: 0.00001313
Iteration 51/1000 | Loss: 0.00001313
Iteration 52/1000 | Loss: 0.00001313
Iteration 53/1000 | Loss: 0.00001313
Iteration 54/1000 | Loss: 0.00001313
Iteration 55/1000 | Loss: 0.00001313
Iteration 56/1000 | Loss: 0.00001312
Iteration 57/1000 | Loss: 0.00001312
Iteration 58/1000 | Loss: 0.00001312
Iteration 59/1000 | Loss: 0.00001312
Iteration 60/1000 | Loss: 0.00001312
Iteration 61/1000 | Loss: 0.00001311
Iteration 62/1000 | Loss: 0.00001311
Iteration 63/1000 | Loss: 0.00001311
Iteration 64/1000 | Loss: 0.00001311
Iteration 65/1000 | Loss: 0.00001310
Iteration 66/1000 | Loss: 0.00001310
Iteration 67/1000 | Loss: 0.00001310
Iteration 68/1000 | Loss: 0.00001310
Iteration 69/1000 | Loss: 0.00001310
Iteration 70/1000 | Loss: 0.00001310
Iteration 71/1000 | Loss: 0.00001310
Iteration 72/1000 | Loss: 0.00001310
Iteration 73/1000 | Loss: 0.00001310
Iteration 74/1000 | Loss: 0.00001310
Iteration 75/1000 | Loss: 0.00001310
Iteration 76/1000 | Loss: 0.00001309
Iteration 77/1000 | Loss: 0.00001309
Iteration 78/1000 | Loss: 0.00001309
Iteration 79/1000 | Loss: 0.00001309
Iteration 80/1000 | Loss: 0.00001309
Iteration 81/1000 | Loss: 0.00001309
Iteration 82/1000 | Loss: 0.00001309
Iteration 83/1000 | Loss: 0.00001309
Iteration 84/1000 | Loss: 0.00001309
Iteration 85/1000 | Loss: 0.00001308
Iteration 86/1000 | Loss: 0.00001308
Iteration 87/1000 | Loss: 0.00001308
Iteration 88/1000 | Loss: 0.00001308
Iteration 89/1000 | Loss: 0.00001308
Iteration 90/1000 | Loss: 0.00001307
Iteration 91/1000 | Loss: 0.00001307
Iteration 92/1000 | Loss: 0.00001307
Iteration 93/1000 | Loss: 0.00001307
Iteration 94/1000 | Loss: 0.00001307
Iteration 95/1000 | Loss: 0.00001307
Iteration 96/1000 | Loss: 0.00001307
Iteration 97/1000 | Loss: 0.00001307
Iteration 98/1000 | Loss: 0.00001306
Iteration 99/1000 | Loss: 0.00001306
Iteration 100/1000 | Loss: 0.00001306
Iteration 101/1000 | Loss: 0.00001306
Iteration 102/1000 | Loss: 0.00001306
Iteration 103/1000 | Loss: 0.00001306
Iteration 104/1000 | Loss: 0.00001305
Iteration 105/1000 | Loss: 0.00001305
Iteration 106/1000 | Loss: 0.00001305
Iteration 107/1000 | Loss: 0.00001305
Iteration 108/1000 | Loss: 0.00001305
Iteration 109/1000 | Loss: 0.00001305
Iteration 110/1000 | Loss: 0.00001305
Iteration 111/1000 | Loss: 0.00001305
Iteration 112/1000 | Loss: 0.00001305
Iteration 113/1000 | Loss: 0.00001305
Iteration 114/1000 | Loss: 0.00001305
Iteration 115/1000 | Loss: 0.00001305
Iteration 116/1000 | Loss: 0.00001305
Iteration 117/1000 | Loss: 0.00001305
Iteration 118/1000 | Loss: 0.00001305
Iteration 119/1000 | Loss: 0.00001305
Iteration 120/1000 | Loss: 0.00001305
Iteration 121/1000 | Loss: 0.00001304
Iteration 122/1000 | Loss: 0.00001304
Iteration 123/1000 | Loss: 0.00001304
Iteration 124/1000 | Loss: 0.00001304
Iteration 125/1000 | Loss: 0.00001304
Iteration 126/1000 | Loss: 0.00001304
Iteration 127/1000 | Loss: 0.00001304
Iteration 128/1000 | Loss: 0.00001304
Iteration 129/1000 | Loss: 0.00001304
Iteration 130/1000 | Loss: 0.00001303
Iteration 131/1000 | Loss: 0.00001303
Iteration 132/1000 | Loss: 0.00001303
Iteration 133/1000 | Loss: 0.00001303
Iteration 134/1000 | Loss: 0.00001303
Iteration 135/1000 | Loss: 0.00001303
Iteration 136/1000 | Loss: 0.00001303
Iteration 137/1000 | Loss: 0.00001303
Iteration 138/1000 | Loss: 0.00001303
Iteration 139/1000 | Loss: 0.00001303
Iteration 140/1000 | Loss: 0.00001303
Iteration 141/1000 | Loss: 0.00001303
Iteration 142/1000 | Loss: 0.00001303
Iteration 143/1000 | Loss: 0.00001303
Iteration 144/1000 | Loss: 0.00001303
Iteration 145/1000 | Loss: 0.00001303
Iteration 146/1000 | Loss: 0.00001303
Iteration 147/1000 | Loss: 0.00001302
Iteration 148/1000 | Loss: 0.00001302
Iteration 149/1000 | Loss: 0.00001302
Iteration 150/1000 | Loss: 0.00001302
Iteration 151/1000 | Loss: 0.00001302
Iteration 152/1000 | Loss: 0.00001302
Iteration 153/1000 | Loss: 0.00001302
Iteration 154/1000 | Loss: 0.00001302
Iteration 155/1000 | Loss: 0.00001302
Iteration 156/1000 | Loss: 0.00001301
Iteration 157/1000 | Loss: 0.00001301
Iteration 158/1000 | Loss: 0.00001301
Iteration 159/1000 | Loss: 0.00001301
Iteration 160/1000 | Loss: 0.00001301
Iteration 161/1000 | Loss: 0.00001301
Iteration 162/1000 | Loss: 0.00001301
Iteration 163/1000 | Loss: 0.00001301
Iteration 164/1000 | Loss: 0.00001301
Iteration 165/1000 | Loss: 0.00001301
Iteration 166/1000 | Loss: 0.00001300
Iteration 167/1000 | Loss: 0.00001300
Iteration 168/1000 | Loss: 0.00001300
Iteration 169/1000 | Loss: 0.00001300
Iteration 170/1000 | Loss: 0.00001300
Iteration 171/1000 | Loss: 0.00001300
Iteration 172/1000 | Loss: 0.00001300
Iteration 173/1000 | Loss: 0.00001300
Iteration 174/1000 | Loss: 0.00001300
Iteration 175/1000 | Loss: 0.00001300
Iteration 176/1000 | Loss: 0.00001300
Iteration 177/1000 | Loss: 0.00001300
Iteration 178/1000 | Loss: 0.00001300
Iteration 179/1000 | Loss: 0.00001300
Iteration 180/1000 | Loss: 0.00001300
Iteration 181/1000 | Loss: 0.00001300
Iteration 182/1000 | Loss: 0.00001300
Iteration 183/1000 | Loss: 0.00001300
Iteration 184/1000 | Loss: 0.00001299
Iteration 185/1000 | Loss: 0.00001299
Iteration 186/1000 | Loss: 0.00001299
Iteration 187/1000 | Loss: 0.00001299
Iteration 188/1000 | Loss: 0.00001299
Iteration 189/1000 | Loss: 0.00001299
Iteration 190/1000 | Loss: 0.00001299
Iteration 191/1000 | Loss: 0.00001299
Iteration 192/1000 | Loss: 0.00001299
Iteration 193/1000 | Loss: 0.00001299
Iteration 194/1000 | Loss: 0.00001299
Iteration 195/1000 | Loss: 0.00001299
Iteration 196/1000 | Loss: 0.00001299
Iteration 197/1000 | Loss: 0.00001299
Iteration 198/1000 | Loss: 0.00001299
Iteration 199/1000 | Loss: 0.00001299
Iteration 200/1000 | Loss: 0.00001299
Iteration 201/1000 | Loss: 0.00001299
Iteration 202/1000 | Loss: 0.00001299
Iteration 203/1000 | Loss: 0.00001299
Iteration 204/1000 | Loss: 0.00001299
Iteration 205/1000 | Loss: 0.00001298
Iteration 206/1000 | Loss: 0.00001298
Iteration 207/1000 | Loss: 0.00001298
Iteration 208/1000 | Loss: 0.00001298
Iteration 209/1000 | Loss: 0.00001298
Iteration 210/1000 | Loss: 0.00001298
Iteration 211/1000 | Loss: 0.00001298
Iteration 212/1000 | Loss: 0.00001298
Iteration 213/1000 | Loss: 0.00001298
Iteration 214/1000 | Loss: 0.00001298
Iteration 215/1000 | Loss: 0.00001298
Iteration 216/1000 | Loss: 0.00001298
Iteration 217/1000 | Loss: 0.00001298
Iteration 218/1000 | Loss: 0.00001298
Iteration 219/1000 | Loss: 0.00001298
Iteration 220/1000 | Loss: 0.00001297
Iteration 221/1000 | Loss: 0.00001297
Iteration 222/1000 | Loss: 0.00001297
Iteration 223/1000 | Loss: 0.00001297
Iteration 224/1000 | Loss: 0.00001297
Iteration 225/1000 | Loss: 0.00001297
Iteration 226/1000 | Loss: 0.00001297
Iteration 227/1000 | Loss: 0.00001297
Iteration 228/1000 | Loss: 0.00001297
Iteration 229/1000 | Loss: 0.00001297
Iteration 230/1000 | Loss: 0.00001297
Iteration 231/1000 | Loss: 0.00001297
Iteration 232/1000 | Loss: 0.00001297
Iteration 233/1000 | Loss: 0.00001296
Iteration 234/1000 | Loss: 0.00001296
Iteration 235/1000 | Loss: 0.00001296
Iteration 236/1000 | Loss: 0.00001296
Iteration 237/1000 | Loss: 0.00001296
Iteration 238/1000 | Loss: 0.00001296
Iteration 239/1000 | Loss: 0.00001296
Iteration 240/1000 | Loss: 0.00001296
Iteration 241/1000 | Loss: 0.00001296
Iteration 242/1000 | Loss: 0.00001296
Iteration 243/1000 | Loss: 0.00001295
Iteration 244/1000 | Loss: 0.00001295
Iteration 245/1000 | Loss: 0.00001295
Iteration 246/1000 | Loss: 0.00001295
Iteration 247/1000 | Loss: 0.00001295
Iteration 248/1000 | Loss: 0.00001295
Iteration 249/1000 | Loss: 0.00001295
Iteration 250/1000 | Loss: 0.00001295
Iteration 251/1000 | Loss: 0.00001295
Iteration 252/1000 | Loss: 0.00001295
Iteration 253/1000 | Loss: 0.00001294
Iteration 254/1000 | Loss: 0.00001294
Iteration 255/1000 | Loss: 0.00001294
Iteration 256/1000 | Loss: 0.00001294
Iteration 257/1000 | Loss: 0.00001294
Iteration 258/1000 | Loss: 0.00001294
Iteration 259/1000 | Loss: 0.00001294
Iteration 260/1000 | Loss: 0.00001294
Iteration 261/1000 | Loss: 0.00001294
Iteration 262/1000 | Loss: 0.00001294
Iteration 263/1000 | Loss: 0.00001294
Iteration 264/1000 | Loss: 0.00001294
Iteration 265/1000 | Loss: 0.00001294
Iteration 266/1000 | Loss: 0.00001293
Iteration 267/1000 | Loss: 0.00001293
Iteration 268/1000 | Loss: 0.00001293
Iteration 269/1000 | Loss: 0.00001293
Iteration 270/1000 | Loss: 0.00001293
Iteration 271/1000 | Loss: 0.00001293
Iteration 272/1000 | Loss: 0.00001293
Iteration 273/1000 | Loss: 0.00001293
Iteration 274/1000 | Loss: 0.00001293
Iteration 275/1000 | Loss: 0.00001293
Iteration 276/1000 | Loss: 0.00001293
Iteration 277/1000 | Loss: 0.00001293
Iteration 278/1000 | Loss: 0.00001293
Iteration 279/1000 | Loss: 0.00001293
Iteration 280/1000 | Loss: 0.00001293
Iteration 281/1000 | Loss: 0.00001293
Iteration 282/1000 | Loss: 0.00001293
Iteration 283/1000 | Loss: 0.00001293
Iteration 284/1000 | Loss: 0.00001293
Iteration 285/1000 | Loss: 0.00001293
Iteration 286/1000 | Loss: 0.00001293
Iteration 287/1000 | Loss: 0.00001293
Iteration 288/1000 | Loss: 0.00001293
Iteration 289/1000 | Loss: 0.00001293
Iteration 290/1000 | Loss: 0.00001293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [1.2929483091284055e-05, 1.2929483091284055e-05, 1.2929483091284055e-05, 1.2929483091284055e-05, 1.2929483091284055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2929483091284055e-05

Optimization complete. Final v2v error: 2.9330854415893555 mm

Highest mean error: 3.220358371734619 mm for frame 2

Lowest mean error: 2.6744985580444336 mm for frame 47

Saving results

Total time: 47.36538815498352
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01087485
Iteration 2/25 | Loss: 0.00136499
Iteration 3/25 | Loss: 0.00106416
Iteration 4/25 | Loss: 0.00101062
Iteration 5/25 | Loss: 0.00100569
Iteration 6/25 | Loss: 0.00104773
Iteration 7/25 | Loss: 0.00104514
Iteration 8/25 | Loss: 0.00101398
Iteration 9/25 | Loss: 0.00098084
Iteration 10/25 | Loss: 0.00097765
Iteration 11/25 | Loss: 0.00098243
Iteration 12/25 | Loss: 0.00098123
Iteration 13/25 | Loss: 0.00097338
Iteration 14/25 | Loss: 0.00097617
Iteration 15/25 | Loss: 0.00097304
Iteration 16/25 | Loss: 0.00097580
Iteration 17/25 | Loss: 0.00096516
Iteration 18/25 | Loss: 0.00096674
Iteration 19/25 | Loss: 0.00095854
Iteration 20/25 | Loss: 0.00095849
Iteration 21/25 | Loss: 0.00095805
Iteration 22/25 | Loss: 0.00095431
Iteration 23/25 | Loss: 0.00095262
Iteration 24/25 | Loss: 0.00095227
Iteration 25/25 | Loss: 0.00095470

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42220116
Iteration 2/25 | Loss: 0.00078160
Iteration 3/25 | Loss: 0.00078160
Iteration 4/25 | Loss: 0.00078160
Iteration 5/25 | Loss: 0.00078160
Iteration 6/25 | Loss: 0.00078160
Iteration 7/25 | Loss: 0.00078160
Iteration 8/25 | Loss: 0.00078160
Iteration 9/25 | Loss: 0.00078160
Iteration 10/25 | Loss: 0.00078160
Iteration 11/25 | Loss: 0.00078160
Iteration 12/25 | Loss: 0.00078160
Iteration 13/25 | Loss: 0.00078160
Iteration 14/25 | Loss: 0.00078160
Iteration 15/25 | Loss: 0.00078160
Iteration 16/25 | Loss: 0.00078160
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007815976277925074, 0.0007815976277925074, 0.0007815976277925074, 0.0007815976277925074, 0.0007815976277925074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007815976277925074

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078160
Iteration 2/1000 | Loss: 0.00041813
Iteration 3/1000 | Loss: 0.00014510
Iteration 4/1000 | Loss: 0.00022069
Iteration 5/1000 | Loss: 0.00011985
Iteration 6/1000 | Loss: 0.00022393
Iteration 7/1000 | Loss: 0.00022477
Iteration 8/1000 | Loss: 0.00041095
Iteration 9/1000 | Loss: 0.00009426
Iteration 10/1000 | Loss: 0.00008863
Iteration 11/1000 | Loss: 0.00015043
Iteration 12/1000 | Loss: 0.00007919
Iteration 13/1000 | Loss: 0.00007968
Iteration 14/1000 | Loss: 0.00008078
Iteration 15/1000 | Loss: 0.00051423
Iteration 16/1000 | Loss: 0.00056789
Iteration 17/1000 | Loss: 0.00082703
Iteration 18/1000 | Loss: 0.00068398
Iteration 19/1000 | Loss: 0.00038399
Iteration 20/1000 | Loss: 0.00045520
Iteration 21/1000 | Loss: 0.00024021
Iteration 22/1000 | Loss: 0.00019904
Iteration 23/1000 | Loss: 0.00042080
Iteration 24/1000 | Loss: 0.00056583
Iteration 25/1000 | Loss: 0.00023997
Iteration 26/1000 | Loss: 0.00090304
Iteration 27/1000 | Loss: 0.00053396
Iteration 28/1000 | Loss: 0.00073098
Iteration 29/1000 | Loss: 0.00007647
Iteration 30/1000 | Loss: 0.00020313
Iteration 31/1000 | Loss: 0.00022996
Iteration 32/1000 | Loss: 0.00008361
Iteration 33/1000 | Loss: 0.00004760
Iteration 34/1000 | Loss: 0.00025804
Iteration 35/1000 | Loss: 0.00027172
Iteration 36/1000 | Loss: 0.00008148
Iteration 37/1000 | Loss: 0.00023421
Iteration 38/1000 | Loss: 0.00026052
Iteration 39/1000 | Loss: 0.00028765
Iteration 40/1000 | Loss: 0.00021724
Iteration 41/1000 | Loss: 0.00019621
Iteration 42/1000 | Loss: 0.00017337
Iteration 43/1000 | Loss: 0.00008258
Iteration 44/1000 | Loss: 0.00007826
Iteration 45/1000 | Loss: 0.00008119
Iteration 46/1000 | Loss: 0.00008593
Iteration 47/1000 | Loss: 0.00004266
Iteration 48/1000 | Loss: 0.00007893
Iteration 49/1000 | Loss: 0.00006322
Iteration 50/1000 | Loss: 0.00008111
Iteration 51/1000 | Loss: 0.00007954
Iteration 52/1000 | Loss: 0.00007973
Iteration 53/1000 | Loss: 0.00009283
Iteration 54/1000 | Loss: 0.00008545
Iteration 55/1000 | Loss: 0.00007892
Iteration 56/1000 | Loss: 0.00007953
Iteration 57/1000 | Loss: 0.00007254
Iteration 58/1000 | Loss: 0.00007796
Iteration 59/1000 | Loss: 0.00007496
Iteration 60/1000 | Loss: 0.00007916
Iteration 61/1000 | Loss: 0.00007411
Iteration 62/1000 | Loss: 0.00007827
Iteration 63/1000 | Loss: 0.00007250
Iteration 64/1000 | Loss: 0.00034186
Iteration 65/1000 | Loss: 0.00023083
Iteration 66/1000 | Loss: 0.00008842
Iteration 67/1000 | Loss: 0.00004382
Iteration 68/1000 | Loss: 0.00004718
Iteration 69/1000 | Loss: 0.00007312
Iteration 70/1000 | Loss: 0.00009272
Iteration 71/1000 | Loss: 0.00007175
Iteration 72/1000 | Loss: 0.00008478
Iteration 73/1000 | Loss: 0.00004382
Iteration 74/1000 | Loss: 0.00029428
Iteration 75/1000 | Loss: 0.00023285
Iteration 76/1000 | Loss: 0.00007330
Iteration 77/1000 | Loss: 0.00009260
Iteration 78/1000 | Loss: 0.00010159
Iteration 79/1000 | Loss: 0.00009462
Iteration 80/1000 | Loss: 0.00009790
Iteration 81/1000 | Loss: 0.00008776
Iteration 82/1000 | Loss: 0.00009929
Iteration 83/1000 | Loss: 0.00008673
Iteration 84/1000 | Loss: 0.00009688
Iteration 85/1000 | Loss: 0.00008627
Iteration 86/1000 | Loss: 0.00008956
Iteration 87/1000 | Loss: 0.00007761
Iteration 88/1000 | Loss: 0.00008757
Iteration 89/1000 | Loss: 0.00010352
Iteration 90/1000 | Loss: 0.00047078
Iteration 91/1000 | Loss: 0.00027265
Iteration 92/1000 | Loss: 0.00031943
Iteration 93/1000 | Loss: 0.00004128
Iteration 94/1000 | Loss: 0.00037527
Iteration 95/1000 | Loss: 0.00034415
Iteration 96/1000 | Loss: 0.00038255
Iteration 97/1000 | Loss: 0.00031219
Iteration 98/1000 | Loss: 0.00034785
Iteration 99/1000 | Loss: 0.00028280
Iteration 100/1000 | Loss: 0.00030708
Iteration 101/1000 | Loss: 0.00021466
Iteration 102/1000 | Loss: 0.00002052
Iteration 103/1000 | Loss: 0.00001703
Iteration 104/1000 | Loss: 0.00037173
Iteration 105/1000 | Loss: 0.00025470
Iteration 106/1000 | Loss: 0.00022087
Iteration 107/1000 | Loss: 0.00054205
Iteration 108/1000 | Loss: 0.00053749
Iteration 109/1000 | Loss: 0.00015252
Iteration 110/1000 | Loss: 0.00035040
Iteration 111/1000 | Loss: 0.00003393
Iteration 112/1000 | Loss: 0.00002633
Iteration 113/1000 | Loss: 0.00002687
Iteration 114/1000 | Loss: 0.00001862
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00007818
Iteration 117/1000 | Loss: 0.00004904
Iteration 118/1000 | Loss: 0.00018870
Iteration 119/1000 | Loss: 0.00004186
Iteration 120/1000 | Loss: 0.00021464
Iteration 121/1000 | Loss: 0.00001620
Iteration 122/1000 | Loss: 0.00025203
Iteration 123/1000 | Loss: 0.00019260
Iteration 124/1000 | Loss: 0.00008977
Iteration 125/1000 | Loss: 0.00013874
Iteration 126/1000 | Loss: 0.00018798
Iteration 127/1000 | Loss: 0.00013037
Iteration 128/1000 | Loss: 0.00010070
Iteration 129/1000 | Loss: 0.00005766
Iteration 130/1000 | Loss: 0.00011903
Iteration 131/1000 | Loss: 0.00009896
Iteration 132/1000 | Loss: 0.00007025
Iteration 133/1000 | Loss: 0.00001980
Iteration 134/1000 | Loss: 0.00023317
Iteration 135/1000 | Loss: 0.00004004
Iteration 136/1000 | Loss: 0.00003093
Iteration 137/1000 | Loss: 0.00001708
Iteration 138/1000 | Loss: 0.00001394
Iteration 139/1000 | Loss: 0.00001387
Iteration 140/1000 | Loss: 0.00001722
Iteration 141/1000 | Loss: 0.00017623
Iteration 142/1000 | Loss: 0.00001086
Iteration 143/1000 | Loss: 0.00001049
Iteration 144/1000 | Loss: 0.00001026
Iteration 145/1000 | Loss: 0.00001021
Iteration 146/1000 | Loss: 0.00001408
Iteration 147/1000 | Loss: 0.00001075
Iteration 148/1000 | Loss: 0.00001235
Iteration 149/1000 | Loss: 0.00000981
Iteration 150/1000 | Loss: 0.00000977
Iteration 151/1000 | Loss: 0.00000977
Iteration 152/1000 | Loss: 0.00000976
Iteration 153/1000 | Loss: 0.00000974
Iteration 154/1000 | Loss: 0.00000971
Iteration 155/1000 | Loss: 0.00000968
Iteration 156/1000 | Loss: 0.00000965
Iteration 157/1000 | Loss: 0.00000961
Iteration 158/1000 | Loss: 0.00000961
Iteration 159/1000 | Loss: 0.00000960
Iteration 160/1000 | Loss: 0.00000960
Iteration 161/1000 | Loss: 0.00000959
Iteration 162/1000 | Loss: 0.00000959
Iteration 163/1000 | Loss: 0.00000959
Iteration 164/1000 | Loss: 0.00000958
Iteration 165/1000 | Loss: 0.00000958
Iteration 166/1000 | Loss: 0.00001040
Iteration 167/1000 | Loss: 0.00000957
Iteration 168/1000 | Loss: 0.00000956
Iteration 169/1000 | Loss: 0.00000956
Iteration 170/1000 | Loss: 0.00000956
Iteration 171/1000 | Loss: 0.00000956
Iteration 172/1000 | Loss: 0.00000956
Iteration 173/1000 | Loss: 0.00000956
Iteration 174/1000 | Loss: 0.00000956
Iteration 175/1000 | Loss: 0.00000956
Iteration 176/1000 | Loss: 0.00000956
Iteration 177/1000 | Loss: 0.00000956
Iteration 178/1000 | Loss: 0.00000956
Iteration 179/1000 | Loss: 0.00000956
Iteration 180/1000 | Loss: 0.00000956
Iteration 181/1000 | Loss: 0.00000956
Iteration 182/1000 | Loss: 0.00000955
Iteration 183/1000 | Loss: 0.00000955
Iteration 184/1000 | Loss: 0.00000955
Iteration 185/1000 | Loss: 0.00000955
Iteration 186/1000 | Loss: 0.00000955
Iteration 187/1000 | Loss: 0.00000955
Iteration 188/1000 | Loss: 0.00000955
Iteration 189/1000 | Loss: 0.00000955
Iteration 190/1000 | Loss: 0.00000955
Iteration 191/1000 | Loss: 0.00000954
Iteration 192/1000 | Loss: 0.00000954
Iteration 193/1000 | Loss: 0.00000954
Iteration 194/1000 | Loss: 0.00000954
Iteration 195/1000 | Loss: 0.00000954
Iteration 196/1000 | Loss: 0.00000954
Iteration 197/1000 | Loss: 0.00000954
Iteration 198/1000 | Loss: 0.00000954
Iteration 199/1000 | Loss: 0.00000954
Iteration 200/1000 | Loss: 0.00000954
Iteration 201/1000 | Loss: 0.00000954
Iteration 202/1000 | Loss: 0.00000953
Iteration 203/1000 | Loss: 0.00000953
Iteration 204/1000 | Loss: 0.00000953
Iteration 205/1000 | Loss: 0.00000953
Iteration 206/1000 | Loss: 0.00000953
Iteration 207/1000 | Loss: 0.00000953
Iteration 208/1000 | Loss: 0.00000953
Iteration 209/1000 | Loss: 0.00000953
Iteration 210/1000 | Loss: 0.00000953
Iteration 211/1000 | Loss: 0.00000953
Iteration 212/1000 | Loss: 0.00000953
Iteration 213/1000 | Loss: 0.00000953
Iteration 214/1000 | Loss: 0.00000953
Iteration 215/1000 | Loss: 0.00000953
Iteration 216/1000 | Loss: 0.00000952
Iteration 217/1000 | Loss: 0.00000961
Iteration 218/1000 | Loss: 0.00000955
Iteration 219/1000 | Loss: 0.00000955
Iteration 220/1000 | Loss: 0.00000955
Iteration 221/1000 | Loss: 0.00000953
Iteration 222/1000 | Loss: 0.00000952
Iteration 223/1000 | Loss: 0.00000952
Iteration 224/1000 | Loss: 0.00000952
Iteration 225/1000 | Loss: 0.00000952
Iteration 226/1000 | Loss: 0.00000952
Iteration 227/1000 | Loss: 0.00000952
Iteration 228/1000 | Loss: 0.00000952
Iteration 229/1000 | Loss: 0.00000952
Iteration 230/1000 | Loss: 0.00000952
Iteration 231/1000 | Loss: 0.00000952
Iteration 232/1000 | Loss: 0.00000952
Iteration 233/1000 | Loss: 0.00000952
Iteration 234/1000 | Loss: 0.00000952
Iteration 235/1000 | Loss: 0.00000952
Iteration 236/1000 | Loss: 0.00000952
Iteration 237/1000 | Loss: 0.00000952
Iteration 238/1000 | Loss: 0.00000952
Iteration 239/1000 | Loss: 0.00000952
Iteration 240/1000 | Loss: 0.00000952
Iteration 241/1000 | Loss: 0.00000952
Iteration 242/1000 | Loss: 0.00000952
Iteration 243/1000 | Loss: 0.00000952
Iteration 244/1000 | Loss: 0.00000952
Iteration 245/1000 | Loss: 0.00000952
Iteration 246/1000 | Loss: 0.00000952
Iteration 247/1000 | Loss: 0.00000952
Iteration 248/1000 | Loss: 0.00000952
Iteration 249/1000 | Loss: 0.00000952
Iteration 250/1000 | Loss: 0.00000952
Iteration 251/1000 | Loss: 0.00000952
Iteration 252/1000 | Loss: 0.00000952
Iteration 253/1000 | Loss: 0.00000952
Iteration 254/1000 | Loss: 0.00000952
Iteration 255/1000 | Loss: 0.00000952
Iteration 256/1000 | Loss: 0.00000952
Iteration 257/1000 | Loss: 0.00000952
Iteration 258/1000 | Loss: 0.00000952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [9.51620859268587e-06, 9.51620859268587e-06, 9.51620859268587e-06, 9.51620859268587e-06, 9.51620859268587e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.51620859268587e-06

Optimization complete. Final v2v error: 2.577315330505371 mm

Highest mean error: 3.568183660507202 mm for frame 65

Lowest mean error: 2.1893861293792725 mm for frame 21

Saving results

Total time: 255.45354199409485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041524
Iteration 2/25 | Loss: 0.01041524
Iteration 3/25 | Loss: 0.00231544
Iteration 4/25 | Loss: 0.00161177
Iteration 5/25 | Loss: 0.00147409
Iteration 6/25 | Loss: 0.00143359
Iteration 7/25 | Loss: 0.00140111
Iteration 8/25 | Loss: 0.00135103
Iteration 9/25 | Loss: 0.00133010
Iteration 10/25 | Loss: 0.00133452
Iteration 11/25 | Loss: 0.00131179
Iteration 12/25 | Loss: 0.00127969
Iteration 13/25 | Loss: 0.00125984
Iteration 14/25 | Loss: 0.00125720
Iteration 15/25 | Loss: 0.00125502
Iteration 16/25 | Loss: 0.00125703
Iteration 17/25 | Loss: 0.00124790
Iteration 18/25 | Loss: 0.00124551
Iteration 19/25 | Loss: 0.00124504
Iteration 20/25 | Loss: 0.00124960
Iteration 21/25 | Loss: 0.00124128
Iteration 22/25 | Loss: 0.00124105
Iteration 23/25 | Loss: 0.00123986
Iteration 24/25 | Loss: 0.00123989
Iteration 25/25 | Loss: 0.00124008

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28758883
Iteration 2/25 | Loss: 0.00273004
Iteration 3/25 | Loss: 0.00273003
Iteration 4/25 | Loss: 0.00273003
Iteration 5/25 | Loss: 0.00273003
Iteration 6/25 | Loss: 0.00273003
Iteration 7/25 | Loss: 0.00273003
Iteration 8/25 | Loss: 0.00273003
Iteration 9/25 | Loss: 0.00273003
Iteration 10/25 | Loss: 0.00273003
Iteration 11/25 | Loss: 0.00273003
Iteration 12/25 | Loss: 0.00273003
Iteration 13/25 | Loss: 0.00273003
Iteration 14/25 | Loss: 0.00273003
Iteration 15/25 | Loss: 0.00273003
Iteration 16/25 | Loss: 0.00273003
Iteration 17/25 | Loss: 0.00273003
Iteration 18/25 | Loss: 0.00273003
Iteration 19/25 | Loss: 0.00273003
Iteration 20/25 | Loss: 0.00273003
Iteration 21/25 | Loss: 0.00273003
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002730032429099083, 0.002730032429099083, 0.002730032429099083, 0.002730032429099083, 0.002730032429099083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002730032429099083

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273003
Iteration 2/1000 | Loss: 0.00049252
Iteration 3/1000 | Loss: 0.00043303
Iteration 4/1000 | Loss: 0.00031633
Iteration 5/1000 | Loss: 0.00024782
Iteration 6/1000 | Loss: 0.00031049
Iteration 7/1000 | Loss: 0.00024767
Iteration 8/1000 | Loss: 0.00042129
Iteration 9/1000 | Loss: 0.00019176
Iteration 10/1000 | Loss: 0.00018519
Iteration 11/1000 | Loss: 0.00017776
Iteration 12/1000 | Loss: 0.00018449
Iteration 13/1000 | Loss: 0.00016935
Iteration 14/1000 | Loss: 0.00016585
Iteration 15/1000 | Loss: 0.00031862
Iteration 16/1000 | Loss: 0.00035022
Iteration 17/1000 | Loss: 0.00093462
Iteration 18/1000 | Loss: 0.00160010
Iteration 19/1000 | Loss: 0.01014806
Iteration 20/1000 | Loss: 0.00225727
Iteration 21/1000 | Loss: 0.00070247
Iteration 22/1000 | Loss: 0.00049295
Iteration 23/1000 | Loss: 0.00024721
Iteration 24/1000 | Loss: 0.00023833
Iteration 25/1000 | Loss: 0.00015136
Iteration 26/1000 | Loss: 0.00030795
Iteration 27/1000 | Loss: 0.00013906
Iteration 28/1000 | Loss: 0.00033309
Iteration 29/1000 | Loss: 0.00016942
Iteration 30/1000 | Loss: 0.00028456
Iteration 31/1000 | Loss: 0.00018742
Iteration 32/1000 | Loss: 0.00017295
Iteration 33/1000 | Loss: 0.00010251
Iteration 34/1000 | Loss: 0.00004839
Iteration 35/1000 | Loss: 0.00004102
Iteration 36/1000 | Loss: 0.00003519
Iteration 37/1000 | Loss: 0.00003234
Iteration 38/1000 | Loss: 0.00003666
Iteration 39/1000 | Loss: 0.00003874
Iteration 40/1000 | Loss: 0.00003350
Iteration 41/1000 | Loss: 0.00002618
Iteration 42/1000 | Loss: 0.00002343
Iteration 43/1000 | Loss: 0.00002136
Iteration 44/1000 | Loss: 0.00001981
Iteration 45/1000 | Loss: 0.00002888
Iteration 46/1000 | Loss: 0.00001953
Iteration 47/1000 | Loss: 0.00001855
Iteration 48/1000 | Loss: 0.00021293
Iteration 49/1000 | Loss: 0.00002397
Iteration 50/1000 | Loss: 0.00001840
Iteration 51/1000 | Loss: 0.00001710
Iteration 52/1000 | Loss: 0.00001624
Iteration 53/1000 | Loss: 0.00001565
Iteration 54/1000 | Loss: 0.00002056
Iteration 55/1000 | Loss: 0.00001665
Iteration 56/1000 | Loss: 0.00001547
Iteration 57/1000 | Loss: 0.00001531
Iteration 58/1000 | Loss: 0.00001597
Iteration 59/1000 | Loss: 0.00001474
Iteration 60/1000 | Loss: 0.00002528
Iteration 61/1000 | Loss: 0.00001919
Iteration 62/1000 | Loss: 0.00002220
Iteration 63/1000 | Loss: 0.00001867
Iteration 64/1000 | Loss: 0.00002191
Iteration 65/1000 | Loss: 0.00002473
Iteration 66/1000 | Loss: 0.00001594
Iteration 67/1000 | Loss: 0.00002230
Iteration 68/1000 | Loss: 0.00002412
Iteration 69/1000 | Loss: 0.00001867
Iteration 70/1000 | Loss: 0.00002128
Iteration 71/1000 | Loss: 0.00002882
Iteration 72/1000 | Loss: 0.00002662
Iteration 73/1000 | Loss: 0.00001780
Iteration 74/1000 | Loss: 0.00002534
Iteration 75/1000 | Loss: 0.00001499
Iteration 76/1000 | Loss: 0.00001386
Iteration 77/1000 | Loss: 0.00001538
Iteration 78/1000 | Loss: 0.00001436
Iteration 79/1000 | Loss: 0.00001350
Iteration 80/1000 | Loss: 0.00001348
Iteration 81/1000 | Loss: 0.00001347
Iteration 82/1000 | Loss: 0.00001347
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001347
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001347
Iteration 88/1000 | Loss: 0.00001347
Iteration 89/1000 | Loss: 0.00001347
Iteration 90/1000 | Loss: 0.00001347
Iteration 91/1000 | Loss: 0.00001347
Iteration 92/1000 | Loss: 0.00001346
Iteration 93/1000 | Loss: 0.00001345
Iteration 94/1000 | Loss: 0.00001449
Iteration 95/1000 | Loss: 0.00001608
Iteration 96/1000 | Loss: 0.00001523
Iteration 97/1000 | Loss: 0.00001362
Iteration 98/1000 | Loss: 0.00001357
Iteration 99/1000 | Loss: 0.00001465
Iteration 100/1000 | Loss: 0.00001398
Iteration 101/1000 | Loss: 0.00001347
Iteration 102/1000 | Loss: 0.00001360
Iteration 103/1000 | Loss: 0.00001348
Iteration 104/1000 | Loss: 0.00001346
Iteration 105/1000 | Loss: 0.00001339
Iteration 106/1000 | Loss: 0.00001338
Iteration 107/1000 | Loss: 0.00001338
Iteration 108/1000 | Loss: 0.00001337
Iteration 109/1000 | Loss: 0.00001373
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001341
Iteration 112/1000 | Loss: 0.00001341
Iteration 113/1000 | Loss: 0.00001341
Iteration 114/1000 | Loss: 0.00001341
Iteration 115/1000 | Loss: 0.00001339
Iteration 116/1000 | Loss: 0.00001338
Iteration 117/1000 | Loss: 0.00001340
Iteration 118/1000 | Loss: 0.00001337
Iteration 119/1000 | Loss: 0.00001334
Iteration 120/1000 | Loss: 0.00001334
Iteration 121/1000 | Loss: 0.00001333
Iteration 122/1000 | Loss: 0.00001333
Iteration 123/1000 | Loss: 0.00001333
Iteration 124/1000 | Loss: 0.00001333
Iteration 125/1000 | Loss: 0.00001333
Iteration 126/1000 | Loss: 0.00001333
Iteration 127/1000 | Loss: 0.00001333
Iteration 128/1000 | Loss: 0.00001333
Iteration 129/1000 | Loss: 0.00001336
Iteration 130/1000 | Loss: 0.00001336
Iteration 131/1000 | Loss: 0.00001333
Iteration 132/1000 | Loss: 0.00001333
Iteration 133/1000 | Loss: 0.00001333
Iteration 134/1000 | Loss: 0.00001333
Iteration 135/1000 | Loss: 0.00001333
Iteration 136/1000 | Loss: 0.00001333
Iteration 137/1000 | Loss: 0.00001333
Iteration 138/1000 | Loss: 0.00001333
Iteration 139/1000 | Loss: 0.00001333
Iteration 140/1000 | Loss: 0.00001333
Iteration 141/1000 | Loss: 0.00001333
Iteration 142/1000 | Loss: 0.00001333
Iteration 143/1000 | Loss: 0.00001333
Iteration 144/1000 | Loss: 0.00001333
Iteration 145/1000 | Loss: 0.00001333
Iteration 146/1000 | Loss: 0.00001333
Iteration 147/1000 | Loss: 0.00001333
Iteration 148/1000 | Loss: 0.00001333
Iteration 149/1000 | Loss: 0.00001333
Iteration 150/1000 | Loss: 0.00001333
Iteration 151/1000 | Loss: 0.00001333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.3327809028851334e-05, 1.3327809028851334e-05, 1.3327809028851334e-05, 1.3327809028851334e-05, 1.3327809028851334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3327809028851334e-05

Optimization complete. Final v2v error: 2.5885252952575684 mm

Highest mean error: 12.2489652633667 mm for frame 40

Lowest mean error: 2.1899945735931396 mm for frame 197

Saving results

Total time: 192.3543725013733
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01157673
Iteration 2/25 | Loss: 0.00136458
Iteration 3/25 | Loss: 0.00104936
Iteration 4/25 | Loss: 0.00101735
Iteration 5/25 | Loss: 0.00101860
Iteration 6/25 | Loss: 0.00101009
Iteration 7/25 | Loss: 0.00100930
Iteration 8/25 | Loss: 0.00101475
Iteration 9/25 | Loss: 0.00100773
Iteration 10/25 | Loss: 0.00100720
Iteration 11/25 | Loss: 0.00100707
Iteration 12/25 | Loss: 0.00100692
Iteration 13/25 | Loss: 0.00100691
Iteration 14/25 | Loss: 0.00100691
Iteration 15/25 | Loss: 0.00100691
Iteration 16/25 | Loss: 0.00100691
Iteration 17/25 | Loss: 0.00100691
Iteration 18/25 | Loss: 0.00100690
Iteration 19/25 | Loss: 0.00100690
Iteration 20/25 | Loss: 0.00100690
Iteration 21/25 | Loss: 0.00100690
Iteration 22/25 | Loss: 0.00100690
Iteration 23/25 | Loss: 0.00100690
Iteration 24/25 | Loss: 0.00100690
Iteration 25/25 | Loss: 0.00100690

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84190774
Iteration 2/25 | Loss: 0.00051692
Iteration 3/25 | Loss: 0.00051691
Iteration 4/25 | Loss: 0.00051691
Iteration 5/25 | Loss: 0.00051691
Iteration 6/25 | Loss: 0.00051691
Iteration 7/25 | Loss: 0.00051691
Iteration 8/25 | Loss: 0.00051691
Iteration 9/25 | Loss: 0.00051691
Iteration 10/25 | Loss: 0.00051691
Iteration 11/25 | Loss: 0.00051690
Iteration 12/25 | Loss: 0.00051690
Iteration 13/25 | Loss: 0.00051690
Iteration 14/25 | Loss: 0.00051690
Iteration 15/25 | Loss: 0.00051690
Iteration 16/25 | Loss: 0.00051690
Iteration 17/25 | Loss: 0.00051690
Iteration 18/25 | Loss: 0.00051690
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005169047508388758, 0.0005169047508388758, 0.0005169047508388758, 0.0005169047508388758, 0.0005169047508388758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005169047508388758

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051690
Iteration 2/1000 | Loss: 0.00003779
Iteration 3/1000 | Loss: 0.00002910
Iteration 4/1000 | Loss: 0.00002669
Iteration 5/1000 | Loss: 0.00002533
Iteration 6/1000 | Loss: 0.00002477
Iteration 7/1000 | Loss: 0.00002438
Iteration 8/1000 | Loss: 0.00002410
Iteration 9/1000 | Loss: 0.00002409
Iteration 10/1000 | Loss: 0.00002388
Iteration 11/1000 | Loss: 0.00002370
Iteration 12/1000 | Loss: 0.00002362
Iteration 13/1000 | Loss: 0.00002362
Iteration 14/1000 | Loss: 0.00002348
Iteration 15/1000 | Loss: 0.00002339
Iteration 16/1000 | Loss: 0.00002330
Iteration 17/1000 | Loss: 0.00002329
Iteration 18/1000 | Loss: 0.00002329
Iteration 19/1000 | Loss: 0.00002329
Iteration 20/1000 | Loss: 0.00002325
Iteration 21/1000 | Loss: 0.00002324
Iteration 22/1000 | Loss: 0.00002324
Iteration 23/1000 | Loss: 0.00002323
Iteration 24/1000 | Loss: 0.00002322
Iteration 25/1000 | Loss: 0.00002322
Iteration 26/1000 | Loss: 0.00002322
Iteration 27/1000 | Loss: 0.00002321
Iteration 28/1000 | Loss: 0.00002321
Iteration 29/1000 | Loss: 0.00002320
Iteration 30/1000 | Loss: 0.00002318
Iteration 31/1000 | Loss: 0.00002317
Iteration 32/1000 | Loss: 0.00002316
Iteration 33/1000 | Loss: 0.00002315
Iteration 34/1000 | Loss: 0.00002314
Iteration 35/1000 | Loss: 0.00002313
Iteration 36/1000 | Loss: 0.00002313
Iteration 37/1000 | Loss: 0.00002313
Iteration 38/1000 | Loss: 0.00002313
Iteration 39/1000 | Loss: 0.00002313
Iteration 40/1000 | Loss: 0.00002313
Iteration 41/1000 | Loss: 0.00002312
Iteration 42/1000 | Loss: 0.00002312
Iteration 43/1000 | Loss: 0.00002312
Iteration 44/1000 | Loss: 0.00002312
Iteration 45/1000 | Loss: 0.00002310
Iteration 46/1000 | Loss: 0.00002310
Iteration 47/1000 | Loss: 0.00002310
Iteration 48/1000 | Loss: 0.00002310
Iteration 49/1000 | Loss: 0.00002310
Iteration 50/1000 | Loss: 0.00002310
Iteration 51/1000 | Loss: 0.00002309
Iteration 52/1000 | Loss: 0.00002309
Iteration 53/1000 | Loss: 0.00002309
Iteration 54/1000 | Loss: 0.00002308
Iteration 55/1000 | Loss: 0.00002308
Iteration 56/1000 | Loss: 0.00002308
Iteration 57/1000 | Loss: 0.00002308
Iteration 58/1000 | Loss: 0.00002308
Iteration 59/1000 | Loss: 0.00002308
Iteration 60/1000 | Loss: 0.00002307
Iteration 61/1000 | Loss: 0.00002307
Iteration 62/1000 | Loss: 0.00002307
Iteration 63/1000 | Loss: 0.00002307
Iteration 64/1000 | Loss: 0.00002306
Iteration 65/1000 | Loss: 0.00002305
Iteration 66/1000 | Loss: 0.00002305
Iteration 67/1000 | Loss: 0.00002305
Iteration 68/1000 | Loss: 0.00002305
Iteration 69/1000 | Loss: 0.00002305
Iteration 70/1000 | Loss: 0.00002304
Iteration 71/1000 | Loss: 0.00002304
Iteration 72/1000 | Loss: 0.00002304
Iteration 73/1000 | Loss: 0.00002304
Iteration 74/1000 | Loss: 0.00002304
Iteration 75/1000 | Loss: 0.00002304
Iteration 76/1000 | Loss: 0.00002303
Iteration 77/1000 | Loss: 0.00002303
Iteration 78/1000 | Loss: 0.00002302
Iteration 79/1000 | Loss: 0.00002301
Iteration 80/1000 | Loss: 0.00002301
Iteration 81/1000 | Loss: 0.00002300
Iteration 82/1000 | Loss: 0.00002300
Iteration 83/1000 | Loss: 0.00002300
Iteration 84/1000 | Loss: 0.00002300
Iteration 85/1000 | Loss: 0.00002300
Iteration 86/1000 | Loss: 0.00002300
Iteration 87/1000 | Loss: 0.00002300
Iteration 88/1000 | Loss: 0.00002299
Iteration 89/1000 | Loss: 0.00002299
Iteration 90/1000 | Loss: 0.00002299
Iteration 91/1000 | Loss: 0.00002299
Iteration 92/1000 | Loss: 0.00002298
Iteration 93/1000 | Loss: 0.00002298
Iteration 94/1000 | Loss: 0.00002297
Iteration 95/1000 | Loss: 0.00002297
Iteration 96/1000 | Loss: 0.00002297
Iteration 97/1000 | Loss: 0.00002297
Iteration 98/1000 | Loss: 0.00002296
Iteration 99/1000 | Loss: 0.00002296
Iteration 100/1000 | Loss: 0.00002296
Iteration 101/1000 | Loss: 0.00002295
Iteration 102/1000 | Loss: 0.00002295
Iteration 103/1000 | Loss: 0.00002294
Iteration 104/1000 | Loss: 0.00002294
Iteration 105/1000 | Loss: 0.00002293
Iteration 106/1000 | Loss: 0.00002293
Iteration 107/1000 | Loss: 0.00002293
Iteration 108/1000 | Loss: 0.00002292
Iteration 109/1000 | Loss: 0.00002292
Iteration 110/1000 | Loss: 0.00002292
Iteration 111/1000 | Loss: 0.00002291
Iteration 112/1000 | Loss: 0.00002291
Iteration 113/1000 | Loss: 0.00002290
Iteration 114/1000 | Loss: 0.00002290
Iteration 115/1000 | Loss: 0.00002289
Iteration 116/1000 | Loss: 0.00002288
Iteration 117/1000 | Loss: 0.00002288
Iteration 118/1000 | Loss: 0.00002288
Iteration 119/1000 | Loss: 0.00002288
Iteration 120/1000 | Loss: 0.00002288
Iteration 121/1000 | Loss: 0.00002288
Iteration 122/1000 | Loss: 0.00002288
Iteration 123/1000 | Loss: 0.00002288
Iteration 124/1000 | Loss: 0.00002288
Iteration 125/1000 | Loss: 0.00002287
Iteration 126/1000 | Loss: 0.00002287
Iteration 127/1000 | Loss: 0.00002286
Iteration 128/1000 | Loss: 0.00002286
Iteration 129/1000 | Loss: 0.00002286
Iteration 130/1000 | Loss: 0.00002286
Iteration 131/1000 | Loss: 0.00002285
Iteration 132/1000 | Loss: 0.00002285
Iteration 133/1000 | Loss: 0.00002285
Iteration 134/1000 | Loss: 0.00002285
Iteration 135/1000 | Loss: 0.00002285
Iteration 136/1000 | Loss: 0.00002285
Iteration 137/1000 | Loss: 0.00002285
Iteration 138/1000 | Loss: 0.00002285
Iteration 139/1000 | Loss: 0.00002285
Iteration 140/1000 | Loss: 0.00002284
Iteration 141/1000 | Loss: 0.00002283
Iteration 142/1000 | Loss: 0.00002282
Iteration 143/1000 | Loss: 0.00002282
Iteration 144/1000 | Loss: 0.00002282
Iteration 145/1000 | Loss: 0.00002282
Iteration 146/1000 | Loss: 0.00002281
Iteration 147/1000 | Loss: 0.00002281
Iteration 148/1000 | Loss: 0.00002281
Iteration 149/1000 | Loss: 0.00002281
Iteration 150/1000 | Loss: 0.00002281
Iteration 151/1000 | Loss: 0.00002281
Iteration 152/1000 | Loss: 0.00002281
Iteration 153/1000 | Loss: 0.00002281
Iteration 154/1000 | Loss: 0.00002281
Iteration 155/1000 | Loss: 0.00002281
Iteration 156/1000 | Loss: 0.00002281
Iteration 157/1000 | Loss: 0.00002281
Iteration 158/1000 | Loss: 0.00002281
Iteration 159/1000 | Loss: 0.00002281
Iteration 160/1000 | Loss: 0.00002281
Iteration 161/1000 | Loss: 0.00002281
Iteration 162/1000 | Loss: 0.00002281
Iteration 163/1000 | Loss: 0.00002281
Iteration 164/1000 | Loss: 0.00002281
Iteration 165/1000 | Loss: 0.00002281
Iteration 166/1000 | Loss: 0.00002281
Iteration 167/1000 | Loss: 0.00002281
Iteration 168/1000 | Loss: 0.00002281
Iteration 169/1000 | Loss: 0.00002281
Iteration 170/1000 | Loss: 0.00002281
Iteration 171/1000 | Loss: 0.00002281
Iteration 172/1000 | Loss: 0.00002281
Iteration 173/1000 | Loss: 0.00002281
Iteration 174/1000 | Loss: 0.00002281
Iteration 175/1000 | Loss: 0.00002281
Iteration 176/1000 | Loss: 0.00002281
Iteration 177/1000 | Loss: 0.00002281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.2811931557953358e-05, 2.2811931557953358e-05, 2.2811931557953358e-05, 2.2811931557953358e-05, 2.2811931557953358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2811931557953358e-05

Optimization complete. Final v2v error: 3.8252339363098145 mm

Highest mean error: 9.462091445922852 mm for frame 3

Lowest mean error: 3.312978744506836 mm for frame 0

Saving results

Total time: 52.72657322883606
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01115162
Iteration 2/25 | Loss: 0.00565206
Iteration 3/25 | Loss: 0.00376269
Iteration 4/25 | Loss: 0.00270300
Iteration 5/25 | Loss: 0.00243726
Iteration 6/25 | Loss: 0.00228317
Iteration 7/25 | Loss: 0.00212560
Iteration 8/25 | Loss: 0.00201385
Iteration 9/25 | Loss: 0.00192134
Iteration 10/25 | Loss: 0.00182151
Iteration 11/25 | Loss: 0.00173992
Iteration 12/25 | Loss: 0.00173471
Iteration 13/25 | Loss: 0.00170688
Iteration 14/25 | Loss: 0.00162283
Iteration 15/25 | Loss: 0.00153298
Iteration 16/25 | Loss: 0.00149195
Iteration 17/25 | Loss: 0.00148858
Iteration 18/25 | Loss: 0.00146594
Iteration 19/25 | Loss: 0.00144774
Iteration 20/25 | Loss: 0.00138905
Iteration 21/25 | Loss: 0.00136445
Iteration 22/25 | Loss: 0.00136150
Iteration 23/25 | Loss: 0.00136202
Iteration 24/25 | Loss: 0.00135238
Iteration 25/25 | Loss: 0.00135529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.65459812
Iteration 2/25 | Loss: 0.00227612
Iteration 3/25 | Loss: 0.00227612
Iteration 4/25 | Loss: 0.00227612
Iteration 5/25 | Loss: 0.00227612
Iteration 6/25 | Loss: 0.00227612
Iteration 7/25 | Loss: 0.00227612
Iteration 8/25 | Loss: 0.00227612
Iteration 9/25 | Loss: 0.00227612
Iteration 10/25 | Loss: 0.00227612
Iteration 11/25 | Loss: 0.00227612
Iteration 12/25 | Loss: 0.00227612
Iteration 13/25 | Loss: 0.00227612
Iteration 14/25 | Loss: 0.00227612
Iteration 15/25 | Loss: 0.00227612
Iteration 16/25 | Loss: 0.00227612
Iteration 17/25 | Loss: 0.00227612
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0022761174477636814, 0.0022761174477636814, 0.0022761174477636814, 0.0022761174477636814, 0.0022761174477636814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022761174477636814

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227612
Iteration 2/1000 | Loss: 0.00160167
Iteration 3/1000 | Loss: 0.00152651
Iteration 4/1000 | Loss: 0.00120838
Iteration 5/1000 | Loss: 0.00032188
Iteration 6/1000 | Loss: 0.00053418
Iteration 7/1000 | Loss: 0.00090708
Iteration 8/1000 | Loss: 0.00053070
Iteration 9/1000 | Loss: 0.00033707
Iteration 10/1000 | Loss: 0.00014272
Iteration 11/1000 | Loss: 0.00045219
Iteration 12/1000 | Loss: 0.00021857
Iteration 13/1000 | Loss: 0.00011943
Iteration 14/1000 | Loss: 0.00009073
Iteration 15/1000 | Loss: 0.00008115
Iteration 16/1000 | Loss: 0.00007439
Iteration 17/1000 | Loss: 0.00026854
Iteration 18/1000 | Loss: 0.00023709
Iteration 19/1000 | Loss: 0.00010687
Iteration 20/1000 | Loss: 0.00010904
Iteration 21/1000 | Loss: 0.00006871
Iteration 22/1000 | Loss: 0.00015777
Iteration 23/1000 | Loss: 0.00032014
Iteration 24/1000 | Loss: 0.00116384
Iteration 25/1000 | Loss: 0.00160075
Iteration 26/1000 | Loss: 0.00103097
Iteration 27/1000 | Loss: 0.00101661
Iteration 28/1000 | Loss: 0.00064438
Iteration 29/1000 | Loss: 0.00114595
Iteration 30/1000 | Loss: 0.00013777
Iteration 31/1000 | Loss: 0.00012124
Iteration 32/1000 | Loss: 0.00034237
Iteration 33/1000 | Loss: 0.00006700
Iteration 34/1000 | Loss: 0.00069419
Iteration 35/1000 | Loss: 0.00006218
Iteration 36/1000 | Loss: 0.00005640
Iteration 37/1000 | Loss: 0.00004876
Iteration 38/1000 | Loss: 0.00004614
Iteration 39/1000 | Loss: 0.00004230
Iteration 40/1000 | Loss: 0.00004319
Iteration 41/1000 | Loss: 0.00004031
Iteration 42/1000 | Loss: 0.00014639
Iteration 43/1000 | Loss: 0.00009211
Iteration 44/1000 | Loss: 0.00012254
Iteration 45/1000 | Loss: 0.00008740
Iteration 46/1000 | Loss: 0.00010873
Iteration 47/1000 | Loss: 0.00012779
Iteration 48/1000 | Loss: 0.00009224
Iteration 49/1000 | Loss: 0.00005121
Iteration 50/1000 | Loss: 0.00012250
Iteration 51/1000 | Loss: 0.00011745
Iteration 52/1000 | Loss: 0.00008588
Iteration 53/1000 | Loss: 0.00009118
Iteration 54/1000 | Loss: 0.00003974
Iteration 55/1000 | Loss: 0.00003740
Iteration 56/1000 | Loss: 0.00003614
Iteration 57/1000 | Loss: 0.00008157
Iteration 58/1000 | Loss: 0.00003664
Iteration 59/1000 | Loss: 0.00006726
Iteration 60/1000 | Loss: 0.00007164
Iteration 61/1000 | Loss: 0.00008262
Iteration 62/1000 | Loss: 0.00007224
Iteration 63/1000 | Loss: 0.00009674
Iteration 64/1000 | Loss: 0.00017976
Iteration 65/1000 | Loss: 0.00010273
Iteration 66/1000 | Loss: 0.00020691
Iteration 67/1000 | Loss: 0.00010102
Iteration 68/1000 | Loss: 0.00018415
Iteration 69/1000 | Loss: 0.00010791
Iteration 70/1000 | Loss: 0.00011848
Iteration 71/1000 | Loss: 0.00011394
Iteration 72/1000 | Loss: 0.00006668
Iteration 73/1000 | Loss: 0.00011222
Iteration 74/1000 | Loss: 0.00007586
Iteration 75/1000 | Loss: 0.00011075
Iteration 76/1000 | Loss: 0.00006025
Iteration 77/1000 | Loss: 0.00011576
Iteration 78/1000 | Loss: 0.00008648
Iteration 79/1000 | Loss: 0.00009009
Iteration 80/1000 | Loss: 0.00012716
Iteration 81/1000 | Loss: 0.00006917
Iteration 82/1000 | Loss: 0.00009370
Iteration 83/1000 | Loss: 0.00006939
Iteration 84/1000 | Loss: 0.00010854
Iteration 85/1000 | Loss: 0.00007047
Iteration 86/1000 | Loss: 0.00009892
Iteration 87/1000 | Loss: 0.00007337
Iteration 88/1000 | Loss: 0.00004968
Iteration 89/1000 | Loss: 0.00012934
Iteration 90/1000 | Loss: 0.00008179
Iteration 91/1000 | Loss: 0.00008311
Iteration 92/1000 | Loss: 0.00009441
Iteration 93/1000 | Loss: 0.00010270
Iteration 94/1000 | Loss: 0.00006327
Iteration 95/1000 | Loss: 0.00006751
Iteration 96/1000 | Loss: 0.00006115
Iteration 97/1000 | Loss: 0.00006804
Iteration 98/1000 | Loss: 0.00006187
Iteration 99/1000 | Loss: 0.00006524
Iteration 100/1000 | Loss: 0.00006129
Iteration 101/1000 | Loss: 0.00006673
Iteration 102/1000 | Loss: 0.00006069
Iteration 103/1000 | Loss: 0.00016901
Iteration 104/1000 | Loss: 0.00010336
Iteration 105/1000 | Loss: 0.00008950
Iteration 106/1000 | Loss: 0.00011507
Iteration 107/1000 | Loss: 0.00009240
Iteration 108/1000 | Loss: 0.00005948
Iteration 109/1000 | Loss: 0.00017178
Iteration 110/1000 | Loss: 0.00010213
Iteration 111/1000 | Loss: 0.00011462
Iteration 112/1000 | Loss: 0.00010487
Iteration 113/1000 | Loss: 0.00010741
Iteration 114/1000 | Loss: 0.00011298
Iteration 115/1000 | Loss: 0.00011081
Iteration 116/1000 | Loss: 0.00010560
Iteration 117/1000 | Loss: 0.00010612
Iteration 118/1000 | Loss: 0.00009247
Iteration 119/1000 | Loss: 0.00015490
Iteration 120/1000 | Loss: 0.00019605
Iteration 121/1000 | Loss: 0.00005107
Iteration 122/1000 | Loss: 0.00009300
Iteration 123/1000 | Loss: 0.00007372
Iteration 124/1000 | Loss: 0.00010579
Iteration 125/1000 | Loss: 0.00007581
Iteration 126/1000 | Loss: 0.00010323
Iteration 127/1000 | Loss: 0.00008603
Iteration 128/1000 | Loss: 0.00008572
Iteration 129/1000 | Loss: 0.00009970
Iteration 130/1000 | Loss: 0.00009500
Iteration 131/1000 | Loss: 0.00020510
Iteration 132/1000 | Loss: 0.00009320
Iteration 133/1000 | Loss: 0.00011824
Iteration 134/1000 | Loss: 0.00013406
Iteration 135/1000 | Loss: 0.00011617
Iteration 136/1000 | Loss: 0.00013183
Iteration 137/1000 | Loss: 0.00011769
Iteration 138/1000 | Loss: 0.00009943
Iteration 139/1000 | Loss: 0.00010941
Iteration 140/1000 | Loss: 0.00009444
Iteration 141/1000 | Loss: 0.00009320
Iteration 142/1000 | Loss: 0.00009028
Iteration 143/1000 | Loss: 0.00007826
Iteration 144/1000 | Loss: 0.00012042
Iteration 145/1000 | Loss: 0.00007437
Iteration 146/1000 | Loss: 0.00009413
Iteration 147/1000 | Loss: 0.00012332
Iteration 148/1000 | Loss: 0.00020005
Iteration 149/1000 | Loss: 0.00005174
Iteration 150/1000 | Loss: 0.00004079
Iteration 151/1000 | Loss: 0.00003770
Iteration 152/1000 | Loss: 0.00003512
Iteration 153/1000 | Loss: 0.00003373
Iteration 154/1000 | Loss: 0.00003287
Iteration 155/1000 | Loss: 0.00003612
Iteration 156/1000 | Loss: 0.00003368
Iteration 157/1000 | Loss: 0.00003256
Iteration 158/1000 | Loss: 0.00003233
Iteration 159/1000 | Loss: 0.00003192
Iteration 160/1000 | Loss: 0.00003187
Iteration 161/1000 | Loss: 0.00003187
Iteration 162/1000 | Loss: 0.00003472
Iteration 163/1000 | Loss: 0.00003202
Iteration 164/1000 | Loss: 0.00003704
Iteration 165/1000 | Loss: 0.00003194
Iteration 166/1000 | Loss: 0.00003160
Iteration 167/1000 | Loss: 0.00003696
Iteration 168/1000 | Loss: 0.00003187
Iteration 169/1000 | Loss: 0.00003816
Iteration 170/1000 | Loss: 0.00003181
Iteration 171/1000 | Loss: 0.00003717
Iteration 172/1000 | Loss: 0.00003182
Iteration 173/1000 | Loss: 0.00003727
Iteration 174/1000 | Loss: 0.00003185
Iteration 175/1000 | Loss: 0.00003825
Iteration 176/1000 | Loss: 0.00003193
Iteration 177/1000 | Loss: 0.00003175
Iteration 178/1000 | Loss: 0.00003146
Iteration 179/1000 | Loss: 0.00003640
Iteration 180/1000 | Loss: 0.00003234
Iteration 181/1000 | Loss: 0.00003565
Iteration 182/1000 | Loss: 0.00003278
Iteration 183/1000 | Loss: 0.00003395
Iteration 184/1000 | Loss: 0.00003342
Iteration 185/1000 | Loss: 0.00003130
Iteration 186/1000 | Loss: 0.00003647
Iteration 187/1000 | Loss: 0.00003339
Iteration 188/1000 | Loss: 0.00003688
Iteration 189/1000 | Loss: 0.00003471
Iteration 190/1000 | Loss: 0.00003679
Iteration 191/1000 | Loss: 0.00003548
Iteration 192/1000 | Loss: 0.00003602
Iteration 193/1000 | Loss: 0.00003511
Iteration 194/1000 | Loss: 0.00003585
Iteration 195/1000 | Loss: 0.00021939
Iteration 196/1000 | Loss: 0.00020558
Iteration 197/1000 | Loss: 0.00004706
Iteration 198/1000 | Loss: 0.00003777
Iteration 199/1000 | Loss: 0.00003414
Iteration 200/1000 | Loss: 0.00003131
Iteration 201/1000 | Loss: 0.00002993
Iteration 202/1000 | Loss: 0.00002882
Iteration 203/1000 | Loss: 0.00003806
Iteration 204/1000 | Loss: 0.00002943
Iteration 205/1000 | Loss: 0.00002845
Iteration 206/1000 | Loss: 0.00002806
Iteration 207/1000 | Loss: 0.00003000
Iteration 208/1000 | Loss: 0.00002811
Iteration 209/1000 | Loss: 0.00002785
Iteration 210/1000 | Loss: 0.00002785
Iteration 211/1000 | Loss: 0.00002783
Iteration 212/1000 | Loss: 0.00002768
Iteration 213/1000 | Loss: 0.00002764
Iteration 214/1000 | Loss: 0.00002761
Iteration 215/1000 | Loss: 0.00002760
Iteration 216/1000 | Loss: 0.00002760
Iteration 217/1000 | Loss: 0.00002760
Iteration 218/1000 | Loss: 0.00002759
Iteration 219/1000 | Loss: 0.00002756
Iteration 220/1000 | Loss: 0.00002756
Iteration 221/1000 | Loss: 0.00002756
Iteration 222/1000 | Loss: 0.00002756
Iteration 223/1000 | Loss: 0.00002756
Iteration 224/1000 | Loss: 0.00002755
Iteration 225/1000 | Loss: 0.00002755
Iteration 226/1000 | Loss: 0.00002755
Iteration 227/1000 | Loss: 0.00002755
Iteration 228/1000 | Loss: 0.00002754
Iteration 229/1000 | Loss: 0.00002753
Iteration 230/1000 | Loss: 0.00002753
Iteration 231/1000 | Loss: 0.00002753
Iteration 232/1000 | Loss: 0.00002752
Iteration 233/1000 | Loss: 0.00002752
Iteration 234/1000 | Loss: 0.00002752
Iteration 235/1000 | Loss: 0.00002751
Iteration 236/1000 | Loss: 0.00002751
Iteration 237/1000 | Loss: 0.00002751
Iteration 238/1000 | Loss: 0.00002751
Iteration 239/1000 | Loss: 0.00002750
Iteration 240/1000 | Loss: 0.00002750
Iteration 241/1000 | Loss: 0.00002750
Iteration 242/1000 | Loss: 0.00002749
Iteration 243/1000 | Loss: 0.00002749
Iteration 244/1000 | Loss: 0.00002749
Iteration 245/1000 | Loss: 0.00002749
Iteration 246/1000 | Loss: 0.00002749
Iteration 247/1000 | Loss: 0.00002749
Iteration 248/1000 | Loss: 0.00002749
Iteration 249/1000 | Loss: 0.00002749
Iteration 250/1000 | Loss: 0.00002749
Iteration 251/1000 | Loss: 0.00002749
Iteration 252/1000 | Loss: 0.00002749
Iteration 253/1000 | Loss: 0.00002749
Iteration 254/1000 | Loss: 0.00002749
Iteration 255/1000 | Loss: 0.00002749
Iteration 256/1000 | Loss: 0.00002748
Iteration 257/1000 | Loss: 0.00002748
Iteration 258/1000 | Loss: 0.00002748
Iteration 259/1000 | Loss: 0.00002748
Iteration 260/1000 | Loss: 0.00002748
Iteration 261/1000 | Loss: 0.00002748
Iteration 262/1000 | Loss: 0.00002748
Iteration 263/1000 | Loss: 0.00002748
Iteration 264/1000 | Loss: 0.00002748
Iteration 265/1000 | Loss: 0.00002748
Iteration 266/1000 | Loss: 0.00002748
Iteration 267/1000 | Loss: 0.00002748
Iteration 268/1000 | Loss: 0.00002748
Iteration 269/1000 | Loss: 0.00002748
Iteration 270/1000 | Loss: 0.00002748
Iteration 271/1000 | Loss: 0.00002747
Iteration 272/1000 | Loss: 0.00002747
Iteration 273/1000 | Loss: 0.00002747
Iteration 274/1000 | Loss: 0.00002747
Iteration 275/1000 | Loss: 0.00002746
Iteration 276/1000 | Loss: 0.00002746
Iteration 277/1000 | Loss: 0.00002746
Iteration 278/1000 | Loss: 0.00002746
Iteration 279/1000 | Loss: 0.00002746
Iteration 280/1000 | Loss: 0.00002746
Iteration 281/1000 | Loss: 0.00002746
Iteration 282/1000 | Loss: 0.00002746
Iteration 283/1000 | Loss: 0.00002746
Iteration 284/1000 | Loss: 0.00002745
Iteration 285/1000 | Loss: 0.00002745
Iteration 286/1000 | Loss: 0.00002745
Iteration 287/1000 | Loss: 0.00002745
Iteration 288/1000 | Loss: 0.00002745
Iteration 289/1000 | Loss: 0.00002745
Iteration 290/1000 | Loss: 0.00002745
Iteration 291/1000 | Loss: 0.00002744
Iteration 292/1000 | Loss: 0.00002744
Iteration 293/1000 | Loss: 0.00002744
Iteration 294/1000 | Loss: 0.00002744
Iteration 295/1000 | Loss: 0.00002744
Iteration 296/1000 | Loss: 0.00002744
Iteration 297/1000 | Loss: 0.00002744
Iteration 298/1000 | Loss: 0.00002744
Iteration 299/1000 | Loss: 0.00002744
Iteration 300/1000 | Loss: 0.00002744
Iteration 301/1000 | Loss: 0.00002744
Iteration 302/1000 | Loss: 0.00002744
Iteration 303/1000 | Loss: 0.00002744
Iteration 304/1000 | Loss: 0.00002744
Iteration 305/1000 | Loss: 0.00002744
Iteration 306/1000 | Loss: 0.00002744
Iteration 307/1000 | Loss: 0.00002744
Iteration 308/1000 | Loss: 0.00002744
Iteration 309/1000 | Loss: 0.00002744
Iteration 310/1000 | Loss: 0.00002744
Iteration 311/1000 | Loss: 0.00002744
Iteration 312/1000 | Loss: 0.00002744
Iteration 313/1000 | Loss: 0.00002744
Iteration 314/1000 | Loss: 0.00002744
Iteration 315/1000 | Loss: 0.00002744
Iteration 316/1000 | Loss: 0.00002744
Iteration 317/1000 | Loss: 0.00002744
Iteration 318/1000 | Loss: 0.00002744
Iteration 319/1000 | Loss: 0.00002744
Iteration 320/1000 | Loss: 0.00002744
Iteration 321/1000 | Loss: 0.00002744
Iteration 322/1000 | Loss: 0.00002744
Iteration 323/1000 | Loss: 0.00002744
Iteration 324/1000 | Loss: 0.00002744
Iteration 325/1000 | Loss: 0.00002744
Iteration 326/1000 | Loss: 0.00002744
Iteration 327/1000 | Loss: 0.00002744
Iteration 328/1000 | Loss: 0.00002744
Iteration 329/1000 | Loss: 0.00002744
Iteration 330/1000 | Loss: 0.00002744
Iteration 331/1000 | Loss: 0.00002744
Iteration 332/1000 | Loss: 0.00002744
Iteration 333/1000 | Loss: 0.00002744
Iteration 334/1000 | Loss: 0.00002744
Iteration 335/1000 | Loss: 0.00002744
Iteration 336/1000 | Loss: 0.00002744
Iteration 337/1000 | Loss: 0.00002744
Iteration 338/1000 | Loss: 0.00002744
Iteration 339/1000 | Loss: 0.00002744
Iteration 340/1000 | Loss: 0.00002744
Iteration 341/1000 | Loss: 0.00002744
Iteration 342/1000 | Loss: 0.00002744
Iteration 343/1000 | Loss: 0.00002744
Iteration 344/1000 | Loss: 0.00002744
Iteration 345/1000 | Loss: 0.00002744
Iteration 346/1000 | Loss: 0.00002744
Iteration 347/1000 | Loss: 0.00002744
Iteration 348/1000 | Loss: 0.00002744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 348. Stopping optimization.
Last 5 losses: [2.7439045879873447e-05, 2.7439045879873447e-05, 2.7439045879873447e-05, 2.7439045879873447e-05, 2.7439045879873447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7439045879873447e-05

Optimization complete. Final v2v error: 4.157361030578613 mm

Highest mean error: 11.905630111694336 mm for frame 94

Lowest mean error: 3.80207896232605 mm for frame 122

Saving results

Total time: 392.14319467544556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00511468
Iteration 2/25 | Loss: 0.00103312
Iteration 3/25 | Loss: 0.00092886
Iteration 4/25 | Loss: 0.00091755
Iteration 5/25 | Loss: 0.00091344
Iteration 6/25 | Loss: 0.00091247
Iteration 7/25 | Loss: 0.00091247
Iteration 8/25 | Loss: 0.00091247
Iteration 9/25 | Loss: 0.00091247
Iteration 10/25 | Loss: 0.00091247
Iteration 11/25 | Loss: 0.00091247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009124710340984166, 0.0009124710340984166, 0.0009124710340984166, 0.0009124710340984166, 0.0009124710340984166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009124710340984166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47580051
Iteration 2/25 | Loss: 0.00059247
Iteration 3/25 | Loss: 0.00059247
Iteration 4/25 | Loss: 0.00059247
Iteration 5/25 | Loss: 0.00059247
Iteration 6/25 | Loss: 0.00059247
Iteration 7/25 | Loss: 0.00059246
Iteration 8/25 | Loss: 0.00059246
Iteration 9/25 | Loss: 0.00059246
Iteration 10/25 | Loss: 0.00059246
Iteration 11/25 | Loss: 0.00059246
Iteration 12/25 | Loss: 0.00059246
Iteration 13/25 | Loss: 0.00059246
Iteration 14/25 | Loss: 0.00059246
Iteration 15/25 | Loss: 0.00059246
Iteration 16/25 | Loss: 0.00059246
Iteration 17/25 | Loss: 0.00059246
Iteration 18/25 | Loss: 0.00059246
Iteration 19/25 | Loss: 0.00059246
Iteration 20/25 | Loss: 0.00059246
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000592463300563395, 0.000592463300563395, 0.000592463300563395, 0.000592463300563395, 0.000592463300563395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000592463300563395

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059246
Iteration 2/1000 | Loss: 0.00002079
Iteration 3/1000 | Loss: 0.00001218
Iteration 4/1000 | Loss: 0.00001111
Iteration 5/1000 | Loss: 0.00001024
Iteration 6/1000 | Loss: 0.00000990
Iteration 7/1000 | Loss: 0.00000969
Iteration 8/1000 | Loss: 0.00000960
Iteration 9/1000 | Loss: 0.00000960
Iteration 10/1000 | Loss: 0.00000957
Iteration 11/1000 | Loss: 0.00000953
Iteration 12/1000 | Loss: 0.00000953
Iteration 13/1000 | Loss: 0.00000953
Iteration 14/1000 | Loss: 0.00000953
Iteration 15/1000 | Loss: 0.00000953
Iteration 16/1000 | Loss: 0.00000953
Iteration 17/1000 | Loss: 0.00000953
Iteration 18/1000 | Loss: 0.00000950
Iteration 19/1000 | Loss: 0.00000949
Iteration 20/1000 | Loss: 0.00000949
Iteration 21/1000 | Loss: 0.00000948
Iteration 22/1000 | Loss: 0.00000947
Iteration 23/1000 | Loss: 0.00000947
Iteration 24/1000 | Loss: 0.00000945
Iteration 25/1000 | Loss: 0.00000943
Iteration 26/1000 | Loss: 0.00000943
Iteration 27/1000 | Loss: 0.00000943
Iteration 28/1000 | Loss: 0.00000943
Iteration 29/1000 | Loss: 0.00000943
Iteration 30/1000 | Loss: 0.00000943
Iteration 31/1000 | Loss: 0.00000943
Iteration 32/1000 | Loss: 0.00000943
Iteration 33/1000 | Loss: 0.00000943
Iteration 34/1000 | Loss: 0.00000943
Iteration 35/1000 | Loss: 0.00000943
Iteration 36/1000 | Loss: 0.00000943
Iteration 37/1000 | Loss: 0.00000943
Iteration 38/1000 | Loss: 0.00000943
Iteration 39/1000 | Loss: 0.00000943
Iteration 40/1000 | Loss: 0.00000943
Iteration 41/1000 | Loss: 0.00000943
Iteration 42/1000 | Loss: 0.00000943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 42. Stopping optimization.
Last 5 losses: [9.425561074749567e-06, 9.425561074749567e-06, 9.425561074749567e-06, 9.425561074749567e-06, 9.425561074749567e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.425561074749567e-06

Optimization complete. Final v2v error: 2.619157075881958 mm

Highest mean error: 3.0220136642456055 mm for frame 35

Lowest mean error: 2.465712308883667 mm for frame 103

Saving results

Total time: 22.26811385154724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042529
Iteration 2/25 | Loss: 0.00271132
Iteration 3/25 | Loss: 0.00157549
Iteration 4/25 | Loss: 0.00138570
Iteration 5/25 | Loss: 0.00132174
Iteration 6/25 | Loss: 0.00129122
Iteration 7/25 | Loss: 0.00125628
Iteration 8/25 | Loss: 0.00110922
Iteration 9/25 | Loss: 0.00104773
Iteration 10/25 | Loss: 0.00097830
Iteration 11/25 | Loss: 0.00097222
Iteration 12/25 | Loss: 0.00096064
Iteration 13/25 | Loss: 0.00096013
Iteration 14/25 | Loss: 0.00096012
Iteration 15/25 | Loss: 0.00096012
Iteration 16/25 | Loss: 0.00096012
Iteration 17/25 | Loss: 0.00096012
Iteration 18/25 | Loss: 0.00096012
Iteration 19/25 | Loss: 0.00096012
Iteration 20/25 | Loss: 0.00096012
Iteration 21/25 | Loss: 0.00096012
Iteration 22/25 | Loss: 0.00096011
Iteration 23/25 | Loss: 0.00096011
Iteration 24/25 | Loss: 0.00096011
Iteration 25/25 | Loss: 0.00096010

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33568859
Iteration 2/25 | Loss: 0.00063944
Iteration 3/25 | Loss: 0.00051266
Iteration 4/25 | Loss: 0.00050916
Iteration 5/25 | Loss: 0.00050916
Iteration 6/25 | Loss: 0.00050916
Iteration 7/25 | Loss: 0.00050916
Iteration 8/25 | Loss: 0.00050916
Iteration 9/25 | Loss: 0.00050916
Iteration 10/25 | Loss: 0.00050916
Iteration 11/25 | Loss: 0.00050916
Iteration 12/25 | Loss: 0.00050916
Iteration 13/25 | Loss: 0.00050916
Iteration 14/25 | Loss: 0.00050916
Iteration 15/25 | Loss: 0.00050916
Iteration 16/25 | Loss: 0.00050916
Iteration 17/25 | Loss: 0.00050916
Iteration 18/25 | Loss: 0.00050916
Iteration 19/25 | Loss: 0.00050916
Iteration 20/25 | Loss: 0.00050916
Iteration 21/25 | Loss: 0.00050916
Iteration 22/25 | Loss: 0.00050916
Iteration 23/25 | Loss: 0.00050916
Iteration 24/25 | Loss: 0.00050916
Iteration 25/25 | Loss: 0.00050916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050916
Iteration 2/1000 | Loss: 0.00013868
Iteration 3/1000 | Loss: 0.00022665
Iteration 4/1000 | Loss: 0.00006708
Iteration 5/1000 | Loss: 0.00004961
Iteration 6/1000 | Loss: 0.00003104
Iteration 7/1000 | Loss: 0.00005624
Iteration 8/1000 | Loss: 0.00026183
Iteration 9/1000 | Loss: 0.00004315
Iteration 10/1000 | Loss: 0.00008319
Iteration 11/1000 | Loss: 0.00063441
Iteration 12/1000 | Loss: 0.00002226
Iteration 13/1000 | Loss: 0.00001683
Iteration 14/1000 | Loss: 0.00001916
Iteration 15/1000 | Loss: 0.00001345
Iteration 16/1000 | Loss: 0.00001138
Iteration 17/1000 | Loss: 0.00001137
Iteration 18/1000 | Loss: 0.00005855
Iteration 19/1000 | Loss: 0.00055336
Iteration 20/1000 | Loss: 0.00005393
Iteration 21/1000 | Loss: 0.00005883
Iteration 22/1000 | Loss: 0.00001581
Iteration 23/1000 | Loss: 0.00001122
Iteration 24/1000 | Loss: 0.00001276
Iteration 25/1000 | Loss: 0.00002541
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001372
Iteration 28/1000 | Loss: 0.00001400
Iteration 29/1000 | Loss: 0.00001306
Iteration 30/1000 | Loss: 0.00001176
Iteration 31/1000 | Loss: 0.00002026
Iteration 32/1000 | Loss: 0.00001449
Iteration 33/1000 | Loss: 0.00001096
Iteration 34/1000 | Loss: 0.00001096
Iteration 35/1000 | Loss: 0.00001114
Iteration 36/1000 | Loss: 0.00001107
Iteration 37/1000 | Loss: 0.00001094
Iteration 38/1000 | Loss: 0.00001094
Iteration 39/1000 | Loss: 0.00001094
Iteration 40/1000 | Loss: 0.00001093
Iteration 41/1000 | Loss: 0.00001093
Iteration 42/1000 | Loss: 0.00001093
Iteration 43/1000 | Loss: 0.00001093
Iteration 44/1000 | Loss: 0.00001093
Iteration 45/1000 | Loss: 0.00001416
Iteration 46/1000 | Loss: 0.00001197
Iteration 47/1000 | Loss: 0.00001628
Iteration 48/1000 | Loss: 0.00016188
Iteration 49/1000 | Loss: 0.00005149
Iteration 50/1000 | Loss: 0.00002854
Iteration 51/1000 | Loss: 0.00002848
Iteration 52/1000 | Loss: 0.00001149
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001081
Iteration 55/1000 | Loss: 0.00001078
Iteration 56/1000 | Loss: 0.00001076
Iteration 57/1000 | Loss: 0.00001349
Iteration 58/1000 | Loss: 0.00001099
Iteration 59/1000 | Loss: 0.00001943
Iteration 60/1000 | Loss: 0.00004798
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001199
Iteration 63/1000 | Loss: 0.00001072
Iteration 64/1000 | Loss: 0.00001071
Iteration 65/1000 | Loss: 0.00001071
Iteration 66/1000 | Loss: 0.00001071
Iteration 67/1000 | Loss: 0.00001071
Iteration 68/1000 | Loss: 0.00001371
Iteration 69/1000 | Loss: 0.00001071
Iteration 70/1000 | Loss: 0.00001071
Iteration 71/1000 | Loss: 0.00001071
Iteration 72/1000 | Loss: 0.00001071
Iteration 73/1000 | Loss: 0.00001071
Iteration 74/1000 | Loss: 0.00001071
Iteration 75/1000 | Loss: 0.00001071
Iteration 76/1000 | Loss: 0.00001071
Iteration 77/1000 | Loss: 0.00001071
Iteration 78/1000 | Loss: 0.00001071
Iteration 79/1000 | Loss: 0.00001071
Iteration 80/1000 | Loss: 0.00001071
Iteration 81/1000 | Loss: 0.00001071
Iteration 82/1000 | Loss: 0.00001071
Iteration 83/1000 | Loss: 0.00001071
Iteration 84/1000 | Loss: 0.00001071
Iteration 85/1000 | Loss: 0.00001071
Iteration 86/1000 | Loss: 0.00001071
Iteration 87/1000 | Loss: 0.00001071
Iteration 88/1000 | Loss: 0.00001071
Iteration 89/1000 | Loss: 0.00001071
Iteration 90/1000 | Loss: 0.00001071
Iteration 91/1000 | Loss: 0.00001071
Iteration 92/1000 | Loss: 0.00001071
Iteration 93/1000 | Loss: 0.00001071
Iteration 94/1000 | Loss: 0.00001071
Iteration 95/1000 | Loss: 0.00001071
Iteration 96/1000 | Loss: 0.00001071
Iteration 97/1000 | Loss: 0.00001071
Iteration 98/1000 | Loss: 0.00001071
Iteration 99/1000 | Loss: 0.00001071
Iteration 100/1000 | Loss: 0.00001071
Iteration 101/1000 | Loss: 0.00001071
Iteration 102/1000 | Loss: 0.00001071
Iteration 103/1000 | Loss: 0.00001071
Iteration 104/1000 | Loss: 0.00001071
Iteration 105/1000 | Loss: 0.00001071
Iteration 106/1000 | Loss: 0.00001071
Iteration 107/1000 | Loss: 0.00001071
Iteration 108/1000 | Loss: 0.00001071
Iteration 109/1000 | Loss: 0.00001071
Iteration 110/1000 | Loss: 0.00001071
Iteration 111/1000 | Loss: 0.00001071
Iteration 112/1000 | Loss: 0.00001071
Iteration 113/1000 | Loss: 0.00001071
Iteration 114/1000 | Loss: 0.00001071
Iteration 115/1000 | Loss: 0.00001071
Iteration 116/1000 | Loss: 0.00001071
Iteration 117/1000 | Loss: 0.00001071
Iteration 118/1000 | Loss: 0.00001071
Iteration 119/1000 | Loss: 0.00001071
Iteration 120/1000 | Loss: 0.00001071
Iteration 121/1000 | Loss: 0.00001071
Iteration 122/1000 | Loss: 0.00001071
Iteration 123/1000 | Loss: 0.00001071
Iteration 124/1000 | Loss: 0.00001071
Iteration 125/1000 | Loss: 0.00001071
Iteration 126/1000 | Loss: 0.00001071
Iteration 127/1000 | Loss: 0.00001071
Iteration 128/1000 | Loss: 0.00001071
Iteration 129/1000 | Loss: 0.00001071
Iteration 130/1000 | Loss: 0.00001071
Iteration 131/1000 | Loss: 0.00001071
Iteration 132/1000 | Loss: 0.00001071
Iteration 133/1000 | Loss: 0.00001071
Iteration 134/1000 | Loss: 0.00001071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.0705778549890965e-05, 1.0705778549890965e-05, 1.0705778549890965e-05, 1.0705778549890965e-05, 1.0705778549890965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0705778549890965e-05

Optimization complete. Final v2v error: 2.757733106613159 mm

Highest mean error: 3.3173348903656006 mm for frame 90

Lowest mean error: 2.452239513397217 mm for frame 117

Saving results

Total time: 84.50344395637512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476445
Iteration 2/25 | Loss: 0.00108938
Iteration 3/25 | Loss: 0.00098445
Iteration 4/25 | Loss: 0.00096481
Iteration 5/25 | Loss: 0.00095957
Iteration 6/25 | Loss: 0.00095842
Iteration 7/25 | Loss: 0.00095842
Iteration 8/25 | Loss: 0.00095842
Iteration 9/25 | Loss: 0.00095842
Iteration 10/25 | Loss: 0.00095842
Iteration 11/25 | Loss: 0.00095842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009584162617102265, 0.0009584162617102265, 0.0009584162617102265, 0.0009584162617102265, 0.0009584162617102265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009584162617102265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34186602
Iteration 2/25 | Loss: 0.00064153
Iteration 3/25 | Loss: 0.00064152
Iteration 4/25 | Loss: 0.00064152
Iteration 5/25 | Loss: 0.00064152
Iteration 6/25 | Loss: 0.00064152
Iteration 7/25 | Loss: 0.00064152
Iteration 8/25 | Loss: 0.00064152
Iteration 9/25 | Loss: 0.00064152
Iteration 10/25 | Loss: 0.00064152
Iteration 11/25 | Loss: 0.00064152
Iteration 12/25 | Loss: 0.00064152
Iteration 13/25 | Loss: 0.00064152
Iteration 14/25 | Loss: 0.00064152
Iteration 15/25 | Loss: 0.00064152
Iteration 16/25 | Loss: 0.00064152
Iteration 17/25 | Loss: 0.00064152
Iteration 18/25 | Loss: 0.00064152
Iteration 19/25 | Loss: 0.00064152
Iteration 20/25 | Loss: 0.00064152
Iteration 21/25 | Loss: 0.00064152
Iteration 22/25 | Loss: 0.00064152
Iteration 23/25 | Loss: 0.00064152
Iteration 24/25 | Loss: 0.00064152
Iteration 25/25 | Loss: 0.00064152

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064152
Iteration 2/1000 | Loss: 0.00001708
Iteration 3/1000 | Loss: 0.00001378
Iteration 4/1000 | Loss: 0.00001292
Iteration 5/1000 | Loss: 0.00001236
Iteration 6/1000 | Loss: 0.00001205
Iteration 7/1000 | Loss: 0.00001188
Iteration 8/1000 | Loss: 0.00001183
Iteration 9/1000 | Loss: 0.00001183
Iteration 10/1000 | Loss: 0.00001181
Iteration 11/1000 | Loss: 0.00001181
Iteration 12/1000 | Loss: 0.00001181
Iteration 13/1000 | Loss: 0.00001180
Iteration 14/1000 | Loss: 0.00001179
Iteration 15/1000 | Loss: 0.00001179
Iteration 16/1000 | Loss: 0.00001179
Iteration 17/1000 | Loss: 0.00001179
Iteration 18/1000 | Loss: 0.00001179
Iteration 19/1000 | Loss: 0.00001179
Iteration 20/1000 | Loss: 0.00001179
Iteration 21/1000 | Loss: 0.00001178
Iteration 22/1000 | Loss: 0.00001178
Iteration 23/1000 | Loss: 0.00001178
Iteration 24/1000 | Loss: 0.00001178
Iteration 25/1000 | Loss: 0.00001178
Iteration 26/1000 | Loss: 0.00001178
Iteration 27/1000 | Loss: 0.00001177
Iteration 28/1000 | Loss: 0.00001177
Iteration 29/1000 | Loss: 0.00001176
Iteration 30/1000 | Loss: 0.00001173
Iteration 31/1000 | Loss: 0.00001173
Iteration 32/1000 | Loss: 0.00001173
Iteration 33/1000 | Loss: 0.00001173
Iteration 34/1000 | Loss: 0.00001173
Iteration 35/1000 | Loss: 0.00001173
Iteration 36/1000 | Loss: 0.00001173
Iteration 37/1000 | Loss: 0.00001173
Iteration 38/1000 | Loss: 0.00001173
Iteration 39/1000 | Loss: 0.00001173
Iteration 40/1000 | Loss: 0.00001173
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001173
Iteration 43/1000 | Loss: 0.00001173
Iteration 44/1000 | Loss: 0.00001173
Iteration 45/1000 | Loss: 0.00001173
Iteration 46/1000 | Loss: 0.00001173
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001173
Iteration 50/1000 | Loss: 0.00001173
Iteration 51/1000 | Loss: 0.00001173
Iteration 52/1000 | Loss: 0.00001173
Iteration 53/1000 | Loss: 0.00001173
Iteration 54/1000 | Loss: 0.00001173
Iteration 55/1000 | Loss: 0.00001173
Iteration 56/1000 | Loss: 0.00001173
Iteration 57/1000 | Loss: 0.00001173
Iteration 58/1000 | Loss: 0.00001173
Iteration 59/1000 | Loss: 0.00001173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [1.1726339835149702e-05, 1.1726339835149702e-05, 1.1726339835149702e-05, 1.1726339835149702e-05, 1.1726339835149702e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1726339835149702e-05

Optimization complete. Final v2v error: 2.9297478199005127 mm

Highest mean error: 3.2531416416168213 mm for frame 142

Lowest mean error: 2.7181167602539062 mm for frame 35

Saving results

Total time: 23.45861291885376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01092997
Iteration 2/25 | Loss: 0.01092996
Iteration 3/25 | Loss: 0.01092996
Iteration 4/25 | Loss: 0.01092996
Iteration 5/25 | Loss: 0.01092996
Iteration 6/25 | Loss: 0.00152613
Iteration 7/25 | Loss: 0.00102068
Iteration 8/25 | Loss: 0.00097124
Iteration 9/25 | Loss: 0.00095742
Iteration 10/25 | Loss: 0.00095414
Iteration 11/25 | Loss: 0.00095360
Iteration 12/25 | Loss: 0.00095360
Iteration 13/25 | Loss: 0.00095360
Iteration 14/25 | Loss: 0.00095360
Iteration 15/25 | Loss: 0.00095360
Iteration 16/25 | Loss: 0.00095360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009535995195619762, 0.0009535995195619762, 0.0009535995195619762, 0.0009535995195619762, 0.0009535995195619762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009535995195619762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35706532
Iteration 2/25 | Loss: 0.00060942
Iteration 3/25 | Loss: 0.00060942
Iteration 4/25 | Loss: 0.00060942
Iteration 5/25 | Loss: 0.00060942
Iteration 6/25 | Loss: 0.00060942
Iteration 7/25 | Loss: 0.00060942
Iteration 8/25 | Loss: 0.00060942
Iteration 9/25 | Loss: 0.00060942
Iteration 10/25 | Loss: 0.00060942
Iteration 11/25 | Loss: 0.00060942
Iteration 12/25 | Loss: 0.00060942
Iteration 13/25 | Loss: 0.00060942
Iteration 14/25 | Loss: 0.00060942
Iteration 15/25 | Loss: 0.00060942
Iteration 16/25 | Loss: 0.00060942
Iteration 17/25 | Loss: 0.00060942
Iteration 18/25 | Loss: 0.00060942
Iteration 19/25 | Loss: 0.00060942
Iteration 20/25 | Loss: 0.00060942
Iteration 21/25 | Loss: 0.00060942
Iteration 22/25 | Loss: 0.00060942
Iteration 23/25 | Loss: 0.00060942
Iteration 24/25 | Loss: 0.00060942
Iteration 25/25 | Loss: 0.00060942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060942
Iteration 2/1000 | Loss: 0.00002654
Iteration 3/1000 | Loss: 0.00002611
Iteration 4/1000 | Loss: 0.00027005
Iteration 5/1000 | Loss: 0.00001328
Iteration 6/1000 | Loss: 0.00001260
Iteration 7/1000 | Loss: 0.00001757
Iteration 8/1000 | Loss: 0.00001755
Iteration 9/1000 | Loss: 0.00001202
Iteration 10/1000 | Loss: 0.00001172
Iteration 11/1000 | Loss: 0.00001685
Iteration 12/1000 | Loss: 0.00001797
Iteration 13/1000 | Loss: 0.00001191
Iteration 14/1000 | Loss: 0.00001149
Iteration 15/1000 | Loss: 0.00001147
Iteration 16/1000 | Loss: 0.00001143
Iteration 17/1000 | Loss: 0.00001142
Iteration 18/1000 | Loss: 0.00001119
Iteration 19/1000 | Loss: 0.00001104
Iteration 20/1000 | Loss: 0.00001758
Iteration 21/1000 | Loss: 0.00001093
Iteration 22/1000 | Loss: 0.00001090
Iteration 23/1000 | Loss: 0.00001089
Iteration 24/1000 | Loss: 0.00001089
Iteration 25/1000 | Loss: 0.00001089
Iteration 26/1000 | Loss: 0.00001088
Iteration 27/1000 | Loss: 0.00001088
Iteration 28/1000 | Loss: 0.00001088
Iteration 29/1000 | Loss: 0.00001086
Iteration 30/1000 | Loss: 0.00001085
Iteration 31/1000 | Loss: 0.00001082
Iteration 32/1000 | Loss: 0.00001391
Iteration 33/1000 | Loss: 0.00001079
Iteration 34/1000 | Loss: 0.00001079
Iteration 35/1000 | Loss: 0.00001079
Iteration 36/1000 | Loss: 0.00001079
Iteration 37/1000 | Loss: 0.00001079
Iteration 38/1000 | Loss: 0.00001078
Iteration 39/1000 | Loss: 0.00001078
Iteration 40/1000 | Loss: 0.00001078
Iteration 41/1000 | Loss: 0.00001078
Iteration 42/1000 | Loss: 0.00001078
Iteration 43/1000 | Loss: 0.00001078
Iteration 44/1000 | Loss: 0.00001078
Iteration 45/1000 | Loss: 0.00001078
Iteration 46/1000 | Loss: 0.00001078
Iteration 47/1000 | Loss: 0.00001078
Iteration 48/1000 | Loss: 0.00001078
Iteration 49/1000 | Loss: 0.00001078
Iteration 50/1000 | Loss: 0.00001077
Iteration 51/1000 | Loss: 0.00001077
Iteration 52/1000 | Loss: 0.00001077
Iteration 53/1000 | Loss: 0.00001077
Iteration 54/1000 | Loss: 0.00001077
Iteration 55/1000 | Loss: 0.00001077
Iteration 56/1000 | Loss: 0.00001076
Iteration 57/1000 | Loss: 0.00001076
Iteration 58/1000 | Loss: 0.00001076
Iteration 59/1000 | Loss: 0.00001075
Iteration 60/1000 | Loss: 0.00001075
Iteration 61/1000 | Loss: 0.00001075
Iteration 62/1000 | Loss: 0.00001075
Iteration 63/1000 | Loss: 0.00001074
Iteration 64/1000 | Loss: 0.00001074
Iteration 65/1000 | Loss: 0.00001074
Iteration 66/1000 | Loss: 0.00001073
Iteration 67/1000 | Loss: 0.00001073
Iteration 68/1000 | Loss: 0.00001073
Iteration 69/1000 | Loss: 0.00001073
Iteration 70/1000 | Loss: 0.00001072
Iteration 71/1000 | Loss: 0.00001072
Iteration 72/1000 | Loss: 0.00001072
Iteration 73/1000 | Loss: 0.00001072
Iteration 74/1000 | Loss: 0.00001071
Iteration 75/1000 | Loss: 0.00001071
Iteration 76/1000 | Loss: 0.00001071
Iteration 77/1000 | Loss: 0.00001071
Iteration 78/1000 | Loss: 0.00001071
Iteration 79/1000 | Loss: 0.00001071
Iteration 80/1000 | Loss: 0.00001071
Iteration 81/1000 | Loss: 0.00001071
Iteration 82/1000 | Loss: 0.00001071
Iteration 83/1000 | Loss: 0.00001071
Iteration 84/1000 | Loss: 0.00001071
Iteration 85/1000 | Loss: 0.00001071
Iteration 86/1000 | Loss: 0.00001070
Iteration 87/1000 | Loss: 0.00001070
Iteration 88/1000 | Loss: 0.00001070
Iteration 89/1000 | Loss: 0.00001070
Iteration 90/1000 | Loss: 0.00001070
Iteration 91/1000 | Loss: 0.00001070
Iteration 92/1000 | Loss: 0.00001070
Iteration 93/1000 | Loss: 0.00001069
Iteration 94/1000 | Loss: 0.00001069
Iteration 95/1000 | Loss: 0.00001069
Iteration 96/1000 | Loss: 0.00001069
Iteration 97/1000 | Loss: 0.00001069
Iteration 98/1000 | Loss: 0.00001069
Iteration 99/1000 | Loss: 0.00001069
Iteration 100/1000 | Loss: 0.00001069
Iteration 101/1000 | Loss: 0.00001068
Iteration 102/1000 | Loss: 0.00001068
Iteration 103/1000 | Loss: 0.00001068
Iteration 104/1000 | Loss: 0.00001068
Iteration 105/1000 | Loss: 0.00001068
Iteration 106/1000 | Loss: 0.00001068
Iteration 107/1000 | Loss: 0.00001068
Iteration 108/1000 | Loss: 0.00001068
Iteration 109/1000 | Loss: 0.00001068
Iteration 110/1000 | Loss: 0.00001067
Iteration 111/1000 | Loss: 0.00001067
Iteration 112/1000 | Loss: 0.00001067
Iteration 113/1000 | Loss: 0.00001066
Iteration 114/1000 | Loss: 0.00001066
Iteration 115/1000 | Loss: 0.00001066
Iteration 116/1000 | Loss: 0.00001066
Iteration 117/1000 | Loss: 0.00001066
Iteration 118/1000 | Loss: 0.00001066
Iteration 119/1000 | Loss: 0.00001066
Iteration 120/1000 | Loss: 0.00001066
Iteration 121/1000 | Loss: 0.00001066
Iteration 122/1000 | Loss: 0.00001066
Iteration 123/1000 | Loss: 0.00001066
Iteration 124/1000 | Loss: 0.00001065
Iteration 125/1000 | Loss: 0.00001065
Iteration 126/1000 | Loss: 0.00001065
Iteration 127/1000 | Loss: 0.00001065
Iteration 128/1000 | Loss: 0.00001065
Iteration 129/1000 | Loss: 0.00001065
Iteration 130/1000 | Loss: 0.00001065
Iteration 131/1000 | Loss: 0.00001065
Iteration 132/1000 | Loss: 0.00001065
Iteration 133/1000 | Loss: 0.00001065
Iteration 134/1000 | Loss: 0.00001064
Iteration 135/1000 | Loss: 0.00001064
Iteration 136/1000 | Loss: 0.00001064
Iteration 137/1000 | Loss: 0.00001064
Iteration 138/1000 | Loss: 0.00001064
Iteration 139/1000 | Loss: 0.00001064
Iteration 140/1000 | Loss: 0.00001064
Iteration 141/1000 | Loss: 0.00001064
Iteration 142/1000 | Loss: 0.00001064
Iteration 143/1000 | Loss: 0.00001064
Iteration 144/1000 | Loss: 0.00001064
Iteration 145/1000 | Loss: 0.00001064
Iteration 146/1000 | Loss: 0.00001828
Iteration 147/1000 | Loss: 0.00001075
Iteration 148/1000 | Loss: 0.00001063
Iteration 149/1000 | Loss: 0.00001063
Iteration 150/1000 | Loss: 0.00001062
Iteration 151/1000 | Loss: 0.00001061
Iteration 152/1000 | Loss: 0.00001061
Iteration 153/1000 | Loss: 0.00001061
Iteration 154/1000 | Loss: 0.00001061
Iteration 155/1000 | Loss: 0.00001061
Iteration 156/1000 | Loss: 0.00001061
Iteration 157/1000 | Loss: 0.00001061
Iteration 158/1000 | Loss: 0.00001061
Iteration 159/1000 | Loss: 0.00001061
Iteration 160/1000 | Loss: 0.00001060
Iteration 161/1000 | Loss: 0.00001060
Iteration 162/1000 | Loss: 0.00001060
Iteration 163/1000 | Loss: 0.00001060
Iteration 164/1000 | Loss: 0.00001060
Iteration 165/1000 | Loss: 0.00001060
Iteration 166/1000 | Loss: 0.00001060
Iteration 167/1000 | Loss: 0.00001060
Iteration 168/1000 | Loss: 0.00001060
Iteration 169/1000 | Loss: 0.00001060
Iteration 170/1000 | Loss: 0.00001060
Iteration 171/1000 | Loss: 0.00001060
Iteration 172/1000 | Loss: 0.00001060
Iteration 173/1000 | Loss: 0.00001060
Iteration 174/1000 | Loss: 0.00001060
Iteration 175/1000 | Loss: 0.00001060
Iteration 176/1000 | Loss: 0.00001060
Iteration 177/1000 | Loss: 0.00001060
Iteration 178/1000 | Loss: 0.00001060
Iteration 179/1000 | Loss: 0.00001060
Iteration 180/1000 | Loss: 0.00001060
Iteration 181/1000 | Loss: 0.00001060
Iteration 182/1000 | Loss: 0.00001060
Iteration 183/1000 | Loss: 0.00001060
Iteration 184/1000 | Loss: 0.00001060
Iteration 185/1000 | Loss: 0.00001060
Iteration 186/1000 | Loss: 0.00001060
Iteration 187/1000 | Loss: 0.00001060
Iteration 188/1000 | Loss: 0.00001060
Iteration 189/1000 | Loss: 0.00001060
Iteration 190/1000 | Loss: 0.00001060
Iteration 191/1000 | Loss: 0.00001060
Iteration 192/1000 | Loss: 0.00001060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.0595013918646146e-05, 1.0595013918646146e-05, 1.0595013918646146e-05, 1.0595013918646146e-05, 1.0595013918646146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0595013918646146e-05

Optimization complete. Final v2v error: 2.7440810203552246 mm

Highest mean error: 3.094974994659424 mm for frame 26

Lowest mean error: 2.543351650238037 mm for frame 67

Saving results

Total time: 54.253928422927856
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444902
Iteration 2/25 | Loss: 0.00118815
Iteration 3/25 | Loss: 0.00101782
Iteration 4/25 | Loss: 0.00098845
Iteration 5/25 | Loss: 0.00098234
Iteration 6/25 | Loss: 0.00098071
Iteration 7/25 | Loss: 0.00098037
Iteration 8/25 | Loss: 0.00098037
Iteration 9/25 | Loss: 0.00098037
Iteration 10/25 | Loss: 0.00098037
Iteration 11/25 | Loss: 0.00098037
Iteration 12/25 | Loss: 0.00098037
Iteration 13/25 | Loss: 0.00098037
Iteration 14/25 | Loss: 0.00098037
Iteration 15/25 | Loss: 0.00098037
Iteration 16/25 | Loss: 0.00098037
Iteration 17/25 | Loss: 0.00098037
Iteration 18/25 | Loss: 0.00098037
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009803690481930971, 0.0009803690481930971, 0.0009803690481930971, 0.0009803690481930971, 0.0009803690481930971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009803690481930971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29709768
Iteration 2/25 | Loss: 0.00055576
Iteration 3/25 | Loss: 0.00055576
Iteration 4/25 | Loss: 0.00055576
Iteration 5/25 | Loss: 0.00055576
Iteration 6/25 | Loss: 0.00055576
Iteration 7/25 | Loss: 0.00055576
Iteration 8/25 | Loss: 0.00055576
Iteration 9/25 | Loss: 0.00055576
Iteration 10/25 | Loss: 0.00055576
Iteration 11/25 | Loss: 0.00055576
Iteration 12/25 | Loss: 0.00055575
Iteration 13/25 | Loss: 0.00055575
Iteration 14/25 | Loss: 0.00055575
Iteration 15/25 | Loss: 0.00055575
Iteration 16/25 | Loss: 0.00055575
Iteration 17/25 | Loss: 0.00055575
Iteration 18/25 | Loss: 0.00055575
Iteration 19/25 | Loss: 0.00055575
Iteration 20/25 | Loss: 0.00055575
Iteration 21/25 | Loss: 0.00055575
Iteration 22/25 | Loss: 0.00055575
Iteration 23/25 | Loss: 0.00055575
Iteration 24/25 | Loss: 0.00055575
Iteration 25/25 | Loss: 0.00055575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055575
Iteration 2/1000 | Loss: 0.00002240
Iteration 3/1000 | Loss: 0.00001678
Iteration 4/1000 | Loss: 0.00001552
Iteration 5/1000 | Loss: 0.00001415
Iteration 6/1000 | Loss: 0.00001359
Iteration 7/1000 | Loss: 0.00001300
Iteration 8/1000 | Loss: 0.00001261
Iteration 9/1000 | Loss: 0.00001242
Iteration 10/1000 | Loss: 0.00001242
Iteration 11/1000 | Loss: 0.00001233
Iteration 12/1000 | Loss: 0.00001231
Iteration 13/1000 | Loss: 0.00001226
Iteration 14/1000 | Loss: 0.00001223
Iteration 15/1000 | Loss: 0.00001223
Iteration 16/1000 | Loss: 0.00001220
Iteration 17/1000 | Loss: 0.00001219
Iteration 18/1000 | Loss: 0.00001218
Iteration 19/1000 | Loss: 0.00001217
Iteration 20/1000 | Loss: 0.00001217
Iteration 21/1000 | Loss: 0.00001217
Iteration 22/1000 | Loss: 0.00001217
Iteration 23/1000 | Loss: 0.00001216
Iteration 24/1000 | Loss: 0.00001216
Iteration 25/1000 | Loss: 0.00001216
Iteration 26/1000 | Loss: 0.00001215
Iteration 27/1000 | Loss: 0.00001215
Iteration 28/1000 | Loss: 0.00001215
Iteration 29/1000 | Loss: 0.00001214
Iteration 30/1000 | Loss: 0.00001214
Iteration 31/1000 | Loss: 0.00001214
Iteration 32/1000 | Loss: 0.00001214
Iteration 33/1000 | Loss: 0.00001213
Iteration 34/1000 | Loss: 0.00001213
Iteration 35/1000 | Loss: 0.00001212
Iteration 36/1000 | Loss: 0.00001211
Iteration 37/1000 | Loss: 0.00001211
Iteration 38/1000 | Loss: 0.00001210
Iteration 39/1000 | Loss: 0.00001210
Iteration 40/1000 | Loss: 0.00001210
Iteration 41/1000 | Loss: 0.00001210
Iteration 42/1000 | Loss: 0.00001210
Iteration 43/1000 | Loss: 0.00001209
Iteration 44/1000 | Loss: 0.00001209
Iteration 45/1000 | Loss: 0.00001209
Iteration 46/1000 | Loss: 0.00001209
Iteration 47/1000 | Loss: 0.00001209
Iteration 48/1000 | Loss: 0.00001209
Iteration 49/1000 | Loss: 0.00001209
Iteration 50/1000 | Loss: 0.00001209
Iteration 51/1000 | Loss: 0.00001209
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001208
Iteration 54/1000 | Loss: 0.00001208
Iteration 55/1000 | Loss: 0.00001208
Iteration 56/1000 | Loss: 0.00001207
Iteration 57/1000 | Loss: 0.00001207
Iteration 58/1000 | Loss: 0.00001207
Iteration 59/1000 | Loss: 0.00001207
Iteration 60/1000 | Loss: 0.00001207
Iteration 61/1000 | Loss: 0.00001207
Iteration 62/1000 | Loss: 0.00001207
Iteration 63/1000 | Loss: 0.00001207
Iteration 64/1000 | Loss: 0.00001207
Iteration 65/1000 | Loss: 0.00001206
Iteration 66/1000 | Loss: 0.00001206
Iteration 67/1000 | Loss: 0.00001206
Iteration 68/1000 | Loss: 0.00001206
Iteration 69/1000 | Loss: 0.00001205
Iteration 70/1000 | Loss: 0.00001205
Iteration 71/1000 | Loss: 0.00001205
Iteration 72/1000 | Loss: 0.00001205
Iteration 73/1000 | Loss: 0.00001205
Iteration 74/1000 | Loss: 0.00001205
Iteration 75/1000 | Loss: 0.00001205
Iteration 76/1000 | Loss: 0.00001205
Iteration 77/1000 | Loss: 0.00001205
Iteration 78/1000 | Loss: 0.00001205
Iteration 79/1000 | Loss: 0.00001205
Iteration 80/1000 | Loss: 0.00001205
Iteration 81/1000 | Loss: 0.00001205
Iteration 82/1000 | Loss: 0.00001205
Iteration 83/1000 | Loss: 0.00001205
Iteration 84/1000 | Loss: 0.00001205
Iteration 85/1000 | Loss: 0.00001205
Iteration 86/1000 | Loss: 0.00001205
Iteration 87/1000 | Loss: 0.00001205
Iteration 88/1000 | Loss: 0.00001205
Iteration 89/1000 | Loss: 0.00001205
Iteration 90/1000 | Loss: 0.00001205
Iteration 91/1000 | Loss: 0.00001205
Iteration 92/1000 | Loss: 0.00001205
Iteration 93/1000 | Loss: 0.00001205
Iteration 94/1000 | Loss: 0.00001205
Iteration 95/1000 | Loss: 0.00001205
Iteration 96/1000 | Loss: 0.00001205
Iteration 97/1000 | Loss: 0.00001205
Iteration 98/1000 | Loss: 0.00001205
Iteration 99/1000 | Loss: 0.00001205
Iteration 100/1000 | Loss: 0.00001205
Iteration 101/1000 | Loss: 0.00001205
Iteration 102/1000 | Loss: 0.00001205
Iteration 103/1000 | Loss: 0.00001205
Iteration 104/1000 | Loss: 0.00001205
Iteration 105/1000 | Loss: 0.00001205
Iteration 106/1000 | Loss: 0.00001205
Iteration 107/1000 | Loss: 0.00001205
Iteration 108/1000 | Loss: 0.00001205
Iteration 109/1000 | Loss: 0.00001205
Iteration 110/1000 | Loss: 0.00001205
Iteration 111/1000 | Loss: 0.00001205
Iteration 112/1000 | Loss: 0.00001205
Iteration 113/1000 | Loss: 0.00001205
Iteration 114/1000 | Loss: 0.00001205
Iteration 115/1000 | Loss: 0.00001205
Iteration 116/1000 | Loss: 0.00001205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.2045550647599157e-05, 1.2045550647599157e-05, 1.2045550647599157e-05, 1.2045550647599157e-05, 1.2045550647599157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2045550647599157e-05

Optimization complete. Final v2v error: 2.9489758014678955 mm

Highest mean error: 3.166987657546997 mm for frame 203

Lowest mean error: 2.539564371109009 mm for frame 188

Saving results

Total time: 35.01916146278381
