Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=16, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 896-951
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806127
Iteration 2/25 | Loss: 0.00098130
Iteration 3/25 | Loss: 0.00078098
Iteration 4/25 | Loss: 0.00074306
Iteration 5/25 | Loss: 0.00073341
Iteration 6/25 | Loss: 0.00073115
Iteration 7/25 | Loss: 0.00073037
Iteration 8/25 | Loss: 0.00073037
Iteration 9/25 | Loss: 0.00073037
Iteration 10/25 | Loss: 0.00073037
Iteration 11/25 | Loss: 0.00073037
Iteration 12/25 | Loss: 0.00073037
Iteration 13/25 | Loss: 0.00073037
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007303701713681221, 0.0007303701713681221, 0.0007303701713681221, 0.0007303701713681221, 0.0007303701713681221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007303701713681221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47880375
Iteration 2/25 | Loss: 0.00034250
Iteration 3/25 | Loss: 0.00034248
Iteration 4/25 | Loss: 0.00034248
Iteration 5/25 | Loss: 0.00034248
Iteration 6/25 | Loss: 0.00034248
Iteration 7/25 | Loss: 0.00034248
Iteration 8/25 | Loss: 0.00034248
Iteration 9/25 | Loss: 0.00034248
Iteration 10/25 | Loss: 0.00034248
Iteration 11/25 | Loss: 0.00034248
Iteration 12/25 | Loss: 0.00034248
Iteration 13/25 | Loss: 0.00034248
Iteration 14/25 | Loss: 0.00034248
Iteration 15/25 | Loss: 0.00034248
Iteration 16/25 | Loss: 0.00034248
Iteration 17/25 | Loss: 0.00034248
Iteration 18/25 | Loss: 0.00034248
Iteration 19/25 | Loss: 0.00034248
Iteration 20/25 | Loss: 0.00034248
Iteration 21/25 | Loss: 0.00034248
Iteration 22/25 | Loss: 0.00034248
Iteration 23/25 | Loss: 0.00034248
Iteration 24/25 | Loss: 0.00034248
Iteration 25/25 | Loss: 0.00034248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034248
Iteration 2/1000 | Loss: 0.00004719
Iteration 3/1000 | Loss: 0.00003316
Iteration 4/1000 | Loss: 0.00003025
Iteration 5/1000 | Loss: 0.00002858
Iteration 6/1000 | Loss: 0.00002732
Iteration 7/1000 | Loss: 0.00002681
Iteration 8/1000 | Loss: 0.00002629
Iteration 9/1000 | Loss: 0.00002593
Iteration 10/1000 | Loss: 0.00002564
Iteration 11/1000 | Loss: 0.00002540
Iteration 12/1000 | Loss: 0.00002527
Iteration 13/1000 | Loss: 0.00002511
Iteration 14/1000 | Loss: 0.00002510
Iteration 15/1000 | Loss: 0.00002509
Iteration 16/1000 | Loss: 0.00002505
Iteration 17/1000 | Loss: 0.00002503
Iteration 18/1000 | Loss: 0.00002503
Iteration 19/1000 | Loss: 0.00002502
Iteration 20/1000 | Loss: 0.00002502
Iteration 21/1000 | Loss: 0.00002501
Iteration 22/1000 | Loss: 0.00002500
Iteration 23/1000 | Loss: 0.00002500
Iteration 24/1000 | Loss: 0.00002500
Iteration 25/1000 | Loss: 0.00002499
Iteration 26/1000 | Loss: 0.00002499
Iteration 27/1000 | Loss: 0.00002499
Iteration 28/1000 | Loss: 0.00002498
Iteration 29/1000 | Loss: 0.00002498
Iteration 30/1000 | Loss: 0.00002497
Iteration 31/1000 | Loss: 0.00002496
Iteration 32/1000 | Loss: 0.00002496
Iteration 33/1000 | Loss: 0.00002495
Iteration 34/1000 | Loss: 0.00002495
Iteration 35/1000 | Loss: 0.00002495
Iteration 36/1000 | Loss: 0.00002495
Iteration 37/1000 | Loss: 0.00002495
Iteration 38/1000 | Loss: 0.00002494
Iteration 39/1000 | Loss: 0.00002494
Iteration 40/1000 | Loss: 0.00002494
Iteration 41/1000 | Loss: 0.00002494
Iteration 42/1000 | Loss: 0.00002494
Iteration 43/1000 | Loss: 0.00002494
Iteration 44/1000 | Loss: 0.00002494
Iteration 45/1000 | Loss: 0.00002493
Iteration 46/1000 | Loss: 0.00002493
Iteration 47/1000 | Loss: 0.00002492
Iteration 48/1000 | Loss: 0.00002492
Iteration 49/1000 | Loss: 0.00002492
Iteration 50/1000 | Loss: 0.00002492
Iteration 51/1000 | Loss: 0.00002492
Iteration 52/1000 | Loss: 0.00002491
Iteration 53/1000 | Loss: 0.00002491
Iteration 54/1000 | Loss: 0.00002491
Iteration 55/1000 | Loss: 0.00002491
Iteration 56/1000 | Loss: 0.00002491
Iteration 57/1000 | Loss: 0.00002491
Iteration 58/1000 | Loss: 0.00002491
Iteration 59/1000 | Loss: 0.00002490
Iteration 60/1000 | Loss: 0.00002490
Iteration 61/1000 | Loss: 0.00002490
Iteration 62/1000 | Loss: 0.00002490
Iteration 63/1000 | Loss: 0.00002490
Iteration 64/1000 | Loss: 0.00002490
Iteration 65/1000 | Loss: 0.00002490
Iteration 66/1000 | Loss: 0.00002490
Iteration 67/1000 | Loss: 0.00002489
Iteration 68/1000 | Loss: 0.00002489
Iteration 69/1000 | Loss: 0.00002489
Iteration 70/1000 | Loss: 0.00002489
Iteration 71/1000 | Loss: 0.00002489
Iteration 72/1000 | Loss: 0.00002488
Iteration 73/1000 | Loss: 0.00002488
Iteration 74/1000 | Loss: 0.00002488
Iteration 75/1000 | Loss: 0.00002488
Iteration 76/1000 | Loss: 0.00002488
Iteration 77/1000 | Loss: 0.00002488
Iteration 78/1000 | Loss: 0.00002487
Iteration 79/1000 | Loss: 0.00002487
Iteration 80/1000 | Loss: 0.00002487
Iteration 81/1000 | Loss: 0.00002487
Iteration 82/1000 | Loss: 0.00002487
Iteration 83/1000 | Loss: 0.00002487
Iteration 84/1000 | Loss: 0.00002487
Iteration 85/1000 | Loss: 0.00002487
Iteration 86/1000 | Loss: 0.00002487
Iteration 87/1000 | Loss: 0.00002487
Iteration 88/1000 | Loss: 0.00002487
Iteration 89/1000 | Loss: 0.00002487
Iteration 90/1000 | Loss: 0.00002487
Iteration 91/1000 | Loss: 0.00002487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.4874843802535906e-05, 2.4874843802535906e-05, 2.4874843802535906e-05, 2.4874843802535906e-05, 2.4874843802535906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4874843802535906e-05

Optimization complete. Final v2v error: 4.095085144042969 mm

Highest mean error: 4.983407974243164 mm for frame 153

Lowest mean error: 3.6884584426879883 mm for frame 167

Saving results

Total time: 37.46894812583923
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465376
Iteration 2/25 | Loss: 0.00074424
Iteration 3/25 | Loss: 0.00063350
Iteration 4/25 | Loss: 0.00061180
Iteration 5/25 | Loss: 0.00060509
Iteration 6/25 | Loss: 0.00060349
Iteration 7/25 | Loss: 0.00060309
Iteration 8/25 | Loss: 0.00060309
Iteration 9/25 | Loss: 0.00060309
Iteration 10/25 | Loss: 0.00060309
Iteration 11/25 | Loss: 0.00060309
Iteration 12/25 | Loss: 0.00060309
Iteration 13/25 | Loss: 0.00060309
Iteration 14/25 | Loss: 0.00060309
Iteration 15/25 | Loss: 0.00060309
Iteration 16/25 | Loss: 0.00060309
Iteration 17/25 | Loss: 0.00060309
Iteration 18/25 | Loss: 0.00060309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000603091495577246, 0.000603091495577246, 0.000603091495577246, 0.000603091495577246, 0.000603091495577246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000603091495577246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57212687
Iteration 2/25 | Loss: 0.00024073
Iteration 3/25 | Loss: 0.00024072
Iteration 4/25 | Loss: 0.00024072
Iteration 5/25 | Loss: 0.00024072
Iteration 6/25 | Loss: 0.00024072
Iteration 7/25 | Loss: 0.00024072
Iteration 8/25 | Loss: 0.00024072
Iteration 9/25 | Loss: 0.00024072
Iteration 10/25 | Loss: 0.00024072
Iteration 11/25 | Loss: 0.00024072
Iteration 12/25 | Loss: 0.00024072
Iteration 13/25 | Loss: 0.00024072
Iteration 14/25 | Loss: 0.00024072
Iteration 15/25 | Loss: 0.00024072
Iteration 16/25 | Loss: 0.00024072
Iteration 17/25 | Loss: 0.00024072
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00024071888765320182, 0.00024071888765320182, 0.00024071888765320182, 0.00024071888765320182, 0.00024071888765320182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024071888765320182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024072
Iteration 2/1000 | Loss: 0.00002376
Iteration 3/1000 | Loss: 0.00001743
Iteration 4/1000 | Loss: 0.00001618
Iteration 5/1000 | Loss: 0.00001540
Iteration 6/1000 | Loss: 0.00001485
Iteration 7/1000 | Loss: 0.00001462
Iteration 8/1000 | Loss: 0.00001461
Iteration 9/1000 | Loss: 0.00001441
Iteration 10/1000 | Loss: 0.00001429
Iteration 11/1000 | Loss: 0.00001425
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001420
Iteration 14/1000 | Loss: 0.00001419
Iteration 15/1000 | Loss: 0.00001419
Iteration 16/1000 | Loss: 0.00001418
Iteration 17/1000 | Loss: 0.00001417
Iteration 18/1000 | Loss: 0.00001417
Iteration 19/1000 | Loss: 0.00001417
Iteration 20/1000 | Loss: 0.00001416
Iteration 21/1000 | Loss: 0.00001411
Iteration 22/1000 | Loss: 0.00001404
Iteration 23/1000 | Loss: 0.00001402
Iteration 24/1000 | Loss: 0.00001401
Iteration 25/1000 | Loss: 0.00001401
Iteration 26/1000 | Loss: 0.00001398
Iteration 27/1000 | Loss: 0.00001396
Iteration 28/1000 | Loss: 0.00001394
Iteration 29/1000 | Loss: 0.00001394
Iteration 30/1000 | Loss: 0.00001394
Iteration 31/1000 | Loss: 0.00001393
Iteration 32/1000 | Loss: 0.00001393
Iteration 33/1000 | Loss: 0.00001393
Iteration 34/1000 | Loss: 0.00001393
Iteration 35/1000 | Loss: 0.00001393
Iteration 36/1000 | Loss: 0.00001393
Iteration 37/1000 | Loss: 0.00001392
Iteration 38/1000 | Loss: 0.00001392
Iteration 39/1000 | Loss: 0.00001391
Iteration 40/1000 | Loss: 0.00001390
Iteration 41/1000 | Loss: 0.00001390
Iteration 42/1000 | Loss: 0.00001390
Iteration 43/1000 | Loss: 0.00001389
Iteration 44/1000 | Loss: 0.00001389
Iteration 45/1000 | Loss: 0.00001389
Iteration 46/1000 | Loss: 0.00001387
Iteration 47/1000 | Loss: 0.00001387
Iteration 48/1000 | Loss: 0.00001387
Iteration 49/1000 | Loss: 0.00001387
Iteration 50/1000 | Loss: 0.00001387
Iteration 51/1000 | Loss: 0.00001386
Iteration 52/1000 | Loss: 0.00001386
Iteration 53/1000 | Loss: 0.00001385
Iteration 54/1000 | Loss: 0.00001385
Iteration 55/1000 | Loss: 0.00001384
Iteration 56/1000 | Loss: 0.00001384
Iteration 57/1000 | Loss: 0.00001383
Iteration 58/1000 | Loss: 0.00001382
Iteration 59/1000 | Loss: 0.00001382
Iteration 60/1000 | Loss: 0.00001382
Iteration 61/1000 | Loss: 0.00001381
Iteration 62/1000 | Loss: 0.00001381
Iteration 63/1000 | Loss: 0.00001380
Iteration 64/1000 | Loss: 0.00001380
Iteration 65/1000 | Loss: 0.00001379
Iteration 66/1000 | Loss: 0.00001378
Iteration 67/1000 | Loss: 0.00001378
Iteration 68/1000 | Loss: 0.00001378
Iteration 69/1000 | Loss: 0.00001377
Iteration 70/1000 | Loss: 0.00001377
Iteration 71/1000 | Loss: 0.00001376
Iteration 72/1000 | Loss: 0.00001376
Iteration 73/1000 | Loss: 0.00001374
Iteration 74/1000 | Loss: 0.00001373
Iteration 75/1000 | Loss: 0.00001373
Iteration 76/1000 | Loss: 0.00001372
Iteration 77/1000 | Loss: 0.00001372
Iteration 78/1000 | Loss: 0.00001372
Iteration 79/1000 | Loss: 0.00001372
Iteration 80/1000 | Loss: 0.00001372
Iteration 81/1000 | Loss: 0.00001372
Iteration 82/1000 | Loss: 0.00001372
Iteration 83/1000 | Loss: 0.00001372
Iteration 84/1000 | Loss: 0.00001372
Iteration 85/1000 | Loss: 0.00001372
Iteration 86/1000 | Loss: 0.00001371
Iteration 87/1000 | Loss: 0.00001370
Iteration 88/1000 | Loss: 0.00001369
Iteration 89/1000 | Loss: 0.00001369
Iteration 90/1000 | Loss: 0.00001369
Iteration 91/1000 | Loss: 0.00001369
Iteration 92/1000 | Loss: 0.00001369
Iteration 93/1000 | Loss: 0.00001369
Iteration 94/1000 | Loss: 0.00001369
Iteration 95/1000 | Loss: 0.00001369
Iteration 96/1000 | Loss: 0.00001369
Iteration 97/1000 | Loss: 0.00001369
Iteration 98/1000 | Loss: 0.00001369
Iteration 99/1000 | Loss: 0.00001369
Iteration 100/1000 | Loss: 0.00001369
Iteration 101/1000 | Loss: 0.00001369
Iteration 102/1000 | Loss: 0.00001369
Iteration 103/1000 | Loss: 0.00001368
Iteration 104/1000 | Loss: 0.00001368
Iteration 105/1000 | Loss: 0.00001368
Iteration 106/1000 | Loss: 0.00001368
Iteration 107/1000 | Loss: 0.00001368
Iteration 108/1000 | Loss: 0.00001368
Iteration 109/1000 | Loss: 0.00001368
Iteration 110/1000 | Loss: 0.00001368
Iteration 111/1000 | Loss: 0.00001368
Iteration 112/1000 | Loss: 0.00001368
Iteration 113/1000 | Loss: 0.00001368
Iteration 114/1000 | Loss: 0.00001368
Iteration 115/1000 | Loss: 0.00001368
Iteration 116/1000 | Loss: 0.00001367
Iteration 117/1000 | Loss: 0.00001367
Iteration 118/1000 | Loss: 0.00001367
Iteration 119/1000 | Loss: 0.00001367
Iteration 120/1000 | Loss: 0.00001367
Iteration 121/1000 | Loss: 0.00001367
Iteration 122/1000 | Loss: 0.00001367
Iteration 123/1000 | Loss: 0.00001366
Iteration 124/1000 | Loss: 0.00001366
Iteration 125/1000 | Loss: 0.00001366
Iteration 126/1000 | Loss: 0.00001366
Iteration 127/1000 | Loss: 0.00001366
Iteration 128/1000 | Loss: 0.00001366
Iteration 129/1000 | Loss: 0.00001366
Iteration 130/1000 | Loss: 0.00001366
Iteration 131/1000 | Loss: 0.00001366
Iteration 132/1000 | Loss: 0.00001366
Iteration 133/1000 | Loss: 0.00001366
Iteration 134/1000 | Loss: 0.00001366
Iteration 135/1000 | Loss: 0.00001366
Iteration 136/1000 | Loss: 0.00001366
Iteration 137/1000 | Loss: 0.00001366
Iteration 138/1000 | Loss: 0.00001366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.3660056538356002e-05, 1.3660056538356002e-05, 1.3660056538356002e-05, 1.3660056538356002e-05, 1.3660056538356002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3660056538356002e-05

Optimization complete. Final v2v error: 3.1609106063842773 mm

Highest mean error: 3.502706527709961 mm for frame 24

Lowest mean error: 3.017521381378174 mm for frame 12

Saving results

Total time: 35.56849956512451
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00631282
Iteration 2/25 | Loss: 0.00080339
Iteration 3/25 | Loss: 0.00066687
Iteration 4/25 | Loss: 0.00063571
Iteration 5/25 | Loss: 0.00063102
Iteration 6/25 | Loss: 0.00063036
Iteration 7/25 | Loss: 0.00063036
Iteration 8/25 | Loss: 0.00063036
Iteration 9/25 | Loss: 0.00063036
Iteration 10/25 | Loss: 0.00063036
Iteration 11/25 | Loss: 0.00063036
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006303570698946714, 0.0006303570698946714, 0.0006303570698946714, 0.0006303570698946714, 0.0006303570698946714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006303570698946714

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50189865
Iteration 2/25 | Loss: 0.00031211
Iteration 3/25 | Loss: 0.00031211
Iteration 4/25 | Loss: 0.00031211
Iteration 5/25 | Loss: 0.00031211
Iteration 6/25 | Loss: 0.00031211
Iteration 7/25 | Loss: 0.00031210
Iteration 8/25 | Loss: 0.00031210
Iteration 9/25 | Loss: 0.00031210
Iteration 10/25 | Loss: 0.00031210
Iteration 11/25 | Loss: 0.00031210
Iteration 12/25 | Loss: 0.00031210
Iteration 13/25 | Loss: 0.00031210
Iteration 14/25 | Loss: 0.00031210
Iteration 15/25 | Loss: 0.00031210
Iteration 16/25 | Loss: 0.00031210
Iteration 17/25 | Loss: 0.00031210
Iteration 18/25 | Loss: 0.00031210
Iteration 19/25 | Loss: 0.00031210
Iteration 20/25 | Loss: 0.00031210
Iteration 21/25 | Loss: 0.00031210
Iteration 22/25 | Loss: 0.00031210
Iteration 23/25 | Loss: 0.00031210
Iteration 24/25 | Loss: 0.00031210
Iteration 25/25 | Loss: 0.00031210

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031210
Iteration 2/1000 | Loss: 0.00002006
Iteration 3/1000 | Loss: 0.00001458
Iteration 4/1000 | Loss: 0.00001373
Iteration 5/1000 | Loss: 0.00001315
Iteration 6/1000 | Loss: 0.00001289
Iteration 7/1000 | Loss: 0.00001268
Iteration 8/1000 | Loss: 0.00001260
Iteration 9/1000 | Loss: 0.00001259
Iteration 10/1000 | Loss: 0.00001254
Iteration 11/1000 | Loss: 0.00001252
Iteration 12/1000 | Loss: 0.00001251
Iteration 13/1000 | Loss: 0.00001245
Iteration 14/1000 | Loss: 0.00001245
Iteration 15/1000 | Loss: 0.00001244
Iteration 16/1000 | Loss: 0.00001243
Iteration 17/1000 | Loss: 0.00001243
Iteration 18/1000 | Loss: 0.00001241
Iteration 19/1000 | Loss: 0.00001237
Iteration 20/1000 | Loss: 0.00001237
Iteration 21/1000 | Loss: 0.00001235
Iteration 22/1000 | Loss: 0.00001230
Iteration 23/1000 | Loss: 0.00001230
Iteration 24/1000 | Loss: 0.00001230
Iteration 25/1000 | Loss: 0.00001230
Iteration 26/1000 | Loss: 0.00001230
Iteration 27/1000 | Loss: 0.00001230
Iteration 28/1000 | Loss: 0.00001230
Iteration 29/1000 | Loss: 0.00001229
Iteration 30/1000 | Loss: 0.00001229
Iteration 31/1000 | Loss: 0.00001229
Iteration 32/1000 | Loss: 0.00001228
Iteration 33/1000 | Loss: 0.00001226
Iteration 34/1000 | Loss: 0.00001226
Iteration 35/1000 | Loss: 0.00001226
Iteration 36/1000 | Loss: 0.00001226
Iteration 37/1000 | Loss: 0.00001226
Iteration 38/1000 | Loss: 0.00001226
Iteration 39/1000 | Loss: 0.00001225
Iteration 40/1000 | Loss: 0.00001225
Iteration 41/1000 | Loss: 0.00001225
Iteration 42/1000 | Loss: 0.00001224
Iteration 43/1000 | Loss: 0.00001223
Iteration 44/1000 | Loss: 0.00001223
Iteration 45/1000 | Loss: 0.00001222
Iteration 46/1000 | Loss: 0.00001222
Iteration 47/1000 | Loss: 0.00001221
Iteration 48/1000 | Loss: 0.00001221
Iteration 49/1000 | Loss: 0.00001221
Iteration 50/1000 | Loss: 0.00001220
Iteration 51/1000 | Loss: 0.00001220
Iteration 52/1000 | Loss: 0.00001220
Iteration 53/1000 | Loss: 0.00001220
Iteration 54/1000 | Loss: 0.00001219
Iteration 55/1000 | Loss: 0.00001219
Iteration 56/1000 | Loss: 0.00001218
Iteration 57/1000 | Loss: 0.00001218
Iteration 58/1000 | Loss: 0.00001217
Iteration 59/1000 | Loss: 0.00001217
Iteration 60/1000 | Loss: 0.00001217
Iteration 61/1000 | Loss: 0.00001217
Iteration 62/1000 | Loss: 0.00001217
Iteration 63/1000 | Loss: 0.00001217
Iteration 64/1000 | Loss: 0.00001216
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001216
Iteration 68/1000 | Loss: 0.00001216
Iteration 69/1000 | Loss: 0.00001216
Iteration 70/1000 | Loss: 0.00001216
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001214
Iteration 73/1000 | Loss: 0.00001214
Iteration 74/1000 | Loss: 0.00001214
Iteration 75/1000 | Loss: 0.00001214
Iteration 76/1000 | Loss: 0.00001213
Iteration 77/1000 | Loss: 0.00001213
Iteration 78/1000 | Loss: 0.00001213
Iteration 79/1000 | Loss: 0.00001213
Iteration 80/1000 | Loss: 0.00001213
Iteration 81/1000 | Loss: 0.00001213
Iteration 82/1000 | Loss: 0.00001213
Iteration 83/1000 | Loss: 0.00001212
Iteration 84/1000 | Loss: 0.00001212
Iteration 85/1000 | Loss: 0.00001211
Iteration 86/1000 | Loss: 0.00001210
Iteration 87/1000 | Loss: 0.00001209
Iteration 88/1000 | Loss: 0.00001209
Iteration 89/1000 | Loss: 0.00001209
Iteration 90/1000 | Loss: 0.00001209
Iteration 91/1000 | Loss: 0.00001209
Iteration 92/1000 | Loss: 0.00001209
Iteration 93/1000 | Loss: 0.00001208
Iteration 94/1000 | Loss: 0.00001208
Iteration 95/1000 | Loss: 0.00001208
Iteration 96/1000 | Loss: 0.00001208
Iteration 97/1000 | Loss: 0.00001208
Iteration 98/1000 | Loss: 0.00001207
Iteration 99/1000 | Loss: 0.00001206
Iteration 100/1000 | Loss: 0.00001206
Iteration 101/1000 | Loss: 0.00001206
Iteration 102/1000 | Loss: 0.00001205
Iteration 103/1000 | Loss: 0.00001205
Iteration 104/1000 | Loss: 0.00001205
Iteration 105/1000 | Loss: 0.00001205
Iteration 106/1000 | Loss: 0.00001205
Iteration 107/1000 | Loss: 0.00001204
Iteration 108/1000 | Loss: 0.00001204
Iteration 109/1000 | Loss: 0.00001202
Iteration 110/1000 | Loss: 0.00001202
Iteration 111/1000 | Loss: 0.00001202
Iteration 112/1000 | Loss: 0.00001202
Iteration 113/1000 | Loss: 0.00001202
Iteration 114/1000 | Loss: 0.00001202
Iteration 115/1000 | Loss: 0.00001202
Iteration 116/1000 | Loss: 0.00001202
Iteration 117/1000 | Loss: 0.00001202
Iteration 118/1000 | Loss: 0.00001202
Iteration 119/1000 | Loss: 0.00001201
Iteration 120/1000 | Loss: 0.00001201
Iteration 121/1000 | Loss: 0.00001201
Iteration 122/1000 | Loss: 0.00001201
Iteration 123/1000 | Loss: 0.00001201
Iteration 124/1000 | Loss: 0.00001201
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001201
Iteration 127/1000 | Loss: 0.00001201
Iteration 128/1000 | Loss: 0.00001200
Iteration 129/1000 | Loss: 0.00001200
Iteration 130/1000 | Loss: 0.00001200
Iteration 131/1000 | Loss: 0.00001200
Iteration 132/1000 | Loss: 0.00001199
Iteration 133/1000 | Loss: 0.00001199
Iteration 134/1000 | Loss: 0.00001199
Iteration 135/1000 | Loss: 0.00001199
Iteration 136/1000 | Loss: 0.00001199
Iteration 137/1000 | Loss: 0.00001199
Iteration 138/1000 | Loss: 0.00001199
Iteration 139/1000 | Loss: 0.00001199
Iteration 140/1000 | Loss: 0.00001199
Iteration 141/1000 | Loss: 0.00001199
Iteration 142/1000 | Loss: 0.00001199
Iteration 143/1000 | Loss: 0.00001199
Iteration 144/1000 | Loss: 0.00001199
Iteration 145/1000 | Loss: 0.00001199
Iteration 146/1000 | Loss: 0.00001199
Iteration 147/1000 | Loss: 0.00001199
Iteration 148/1000 | Loss: 0.00001199
Iteration 149/1000 | Loss: 0.00001199
Iteration 150/1000 | Loss: 0.00001199
Iteration 151/1000 | Loss: 0.00001199
Iteration 152/1000 | Loss: 0.00001199
Iteration 153/1000 | Loss: 0.00001199
Iteration 154/1000 | Loss: 0.00001199
Iteration 155/1000 | Loss: 0.00001199
Iteration 156/1000 | Loss: 0.00001199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.1988907317572739e-05, 1.1988907317572739e-05, 1.1988907317572739e-05, 1.1988907317572739e-05, 1.1988907317572739e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1988907317572739e-05

Optimization complete. Final v2v error: 2.9562671184539795 mm

Highest mean error: 3.3859503269195557 mm for frame 10

Lowest mean error: 2.6230151653289795 mm for frame 67

Saving results

Total time: 37.154502630233765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853546
Iteration 2/25 | Loss: 0.00079097
Iteration 3/25 | Loss: 0.00067288
Iteration 4/25 | Loss: 0.00063052
Iteration 5/25 | Loss: 0.00061560
Iteration 6/25 | Loss: 0.00061293
Iteration 7/25 | Loss: 0.00061205
Iteration 8/25 | Loss: 0.00061205
Iteration 9/25 | Loss: 0.00061205
Iteration 10/25 | Loss: 0.00061205
Iteration 11/25 | Loss: 0.00061205
Iteration 12/25 | Loss: 0.00061205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006120527978055179, 0.0006120527978055179, 0.0006120527978055179, 0.0006120527978055179, 0.0006120527978055179]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006120527978055179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36562991
Iteration 2/25 | Loss: 0.00027842
Iteration 3/25 | Loss: 0.00027842
Iteration 4/25 | Loss: 0.00027842
Iteration 5/25 | Loss: 0.00027841
Iteration 6/25 | Loss: 0.00027841
Iteration 7/25 | Loss: 0.00027841
Iteration 8/25 | Loss: 0.00027841
Iteration 9/25 | Loss: 0.00027841
Iteration 10/25 | Loss: 0.00027841
Iteration 11/25 | Loss: 0.00027841
Iteration 12/25 | Loss: 0.00027841
Iteration 13/25 | Loss: 0.00027841
Iteration 14/25 | Loss: 0.00027841
Iteration 15/25 | Loss: 0.00027841
Iteration 16/25 | Loss: 0.00027841
Iteration 17/25 | Loss: 0.00027841
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002784133539535105, 0.0002784133539535105, 0.0002784133539535105, 0.0002784133539535105, 0.0002784133539535105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002784133539535105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027841
Iteration 2/1000 | Loss: 0.00002931
Iteration 3/1000 | Loss: 0.00001924
Iteration 4/1000 | Loss: 0.00001752
Iteration 5/1000 | Loss: 0.00001646
Iteration 6/1000 | Loss: 0.00001589
Iteration 7/1000 | Loss: 0.00001540
Iteration 8/1000 | Loss: 0.00001504
Iteration 9/1000 | Loss: 0.00001476
Iteration 10/1000 | Loss: 0.00001455
Iteration 11/1000 | Loss: 0.00001438
Iteration 12/1000 | Loss: 0.00001437
Iteration 13/1000 | Loss: 0.00001431
Iteration 14/1000 | Loss: 0.00001426
Iteration 15/1000 | Loss: 0.00001425
Iteration 16/1000 | Loss: 0.00001423
Iteration 17/1000 | Loss: 0.00001422
Iteration 18/1000 | Loss: 0.00001421
Iteration 19/1000 | Loss: 0.00001421
Iteration 20/1000 | Loss: 0.00001421
Iteration 21/1000 | Loss: 0.00001419
Iteration 22/1000 | Loss: 0.00001419
Iteration 23/1000 | Loss: 0.00001418
Iteration 24/1000 | Loss: 0.00001417
Iteration 25/1000 | Loss: 0.00001417
Iteration 26/1000 | Loss: 0.00001417
Iteration 27/1000 | Loss: 0.00001416
Iteration 28/1000 | Loss: 0.00001415
Iteration 29/1000 | Loss: 0.00001415
Iteration 30/1000 | Loss: 0.00001415
Iteration 31/1000 | Loss: 0.00001414
Iteration 32/1000 | Loss: 0.00001414
Iteration 33/1000 | Loss: 0.00001414
Iteration 34/1000 | Loss: 0.00001414
Iteration 35/1000 | Loss: 0.00001414
Iteration 36/1000 | Loss: 0.00001414
Iteration 37/1000 | Loss: 0.00001413
Iteration 38/1000 | Loss: 0.00001413
Iteration 39/1000 | Loss: 0.00001413
Iteration 40/1000 | Loss: 0.00001413
Iteration 41/1000 | Loss: 0.00001413
Iteration 42/1000 | Loss: 0.00001413
Iteration 43/1000 | Loss: 0.00001412
Iteration 44/1000 | Loss: 0.00001412
Iteration 45/1000 | Loss: 0.00001412
Iteration 46/1000 | Loss: 0.00001411
Iteration 47/1000 | Loss: 0.00001411
Iteration 48/1000 | Loss: 0.00001411
Iteration 49/1000 | Loss: 0.00001411
Iteration 50/1000 | Loss: 0.00001411
Iteration 51/1000 | Loss: 0.00001410
Iteration 52/1000 | Loss: 0.00001410
Iteration 53/1000 | Loss: 0.00001410
Iteration 54/1000 | Loss: 0.00001409
Iteration 55/1000 | Loss: 0.00001409
Iteration 56/1000 | Loss: 0.00001409
Iteration 57/1000 | Loss: 0.00001409
Iteration 58/1000 | Loss: 0.00001409
Iteration 59/1000 | Loss: 0.00001409
Iteration 60/1000 | Loss: 0.00001409
Iteration 61/1000 | Loss: 0.00001409
Iteration 62/1000 | Loss: 0.00001409
Iteration 63/1000 | Loss: 0.00001408
Iteration 64/1000 | Loss: 0.00001408
Iteration 65/1000 | Loss: 0.00001408
Iteration 66/1000 | Loss: 0.00001408
Iteration 67/1000 | Loss: 0.00001408
Iteration 68/1000 | Loss: 0.00001408
Iteration 69/1000 | Loss: 0.00001408
Iteration 70/1000 | Loss: 0.00001408
Iteration 71/1000 | Loss: 0.00001408
Iteration 72/1000 | Loss: 0.00001408
Iteration 73/1000 | Loss: 0.00001408
Iteration 74/1000 | Loss: 0.00001408
Iteration 75/1000 | Loss: 0.00001408
Iteration 76/1000 | Loss: 0.00001408
Iteration 77/1000 | Loss: 0.00001408
Iteration 78/1000 | Loss: 0.00001408
Iteration 79/1000 | Loss: 0.00001408
Iteration 80/1000 | Loss: 0.00001408
Iteration 81/1000 | Loss: 0.00001408
Iteration 82/1000 | Loss: 0.00001408
Iteration 83/1000 | Loss: 0.00001408
Iteration 84/1000 | Loss: 0.00001408
Iteration 85/1000 | Loss: 0.00001408
Iteration 86/1000 | Loss: 0.00001408
Iteration 87/1000 | Loss: 0.00001408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.4079872016736772e-05, 1.4079872016736772e-05, 1.4079872016736772e-05, 1.4079872016736772e-05, 1.4079872016736772e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4079872016736772e-05

Optimization complete. Final v2v error: 3.178783893585205 mm

Highest mean error: 3.3790769577026367 mm for frame 132

Lowest mean error: 2.927963972091675 mm for frame 36

Saving results

Total time: 36.91987752914429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073415
Iteration 2/25 | Loss: 0.01073415
Iteration 3/25 | Loss: 0.01073415
Iteration 4/25 | Loss: 0.01073415
Iteration 5/25 | Loss: 0.01073414
Iteration 6/25 | Loss: 0.01073414
Iteration 7/25 | Loss: 0.01073414
Iteration 8/25 | Loss: 0.01073414
Iteration 9/25 | Loss: 0.01073414
Iteration 10/25 | Loss: 0.01073414
Iteration 11/25 | Loss: 0.01073414
Iteration 12/25 | Loss: 0.01073414
Iteration 13/25 | Loss: 0.01073413
Iteration 14/25 | Loss: 0.01073413
Iteration 15/25 | Loss: 0.01073413
Iteration 16/25 | Loss: 0.01073413
Iteration 17/25 | Loss: 0.01073413
Iteration 18/25 | Loss: 0.01073413
Iteration 19/25 | Loss: 0.01073413
Iteration 20/25 | Loss: 0.01073412
Iteration 21/25 | Loss: 0.01073412
Iteration 22/25 | Loss: 0.01073412
Iteration 23/25 | Loss: 0.01073412
Iteration 24/25 | Loss: 0.01073412
Iteration 25/25 | Loss: 0.01073411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06033444
Iteration 2/25 | Loss: 0.18149263
Iteration 3/25 | Loss: 0.17910543
Iteration 4/25 | Loss: 0.17088713
Iteration 5/25 | Loss: 0.17024232
Iteration 6/25 | Loss: 0.16864115
Iteration 7/25 | Loss: 0.16864076
Iteration 8/25 | Loss: 0.16864073
Iteration 9/25 | Loss: 0.16864073
Iteration 10/25 | Loss: 0.16864069
Iteration 11/25 | Loss: 0.16864069
Iteration 12/25 | Loss: 0.16864069
Iteration 13/25 | Loss: 0.16864069
Iteration 14/25 | Loss: 0.16864069
Iteration 15/25 | Loss: 0.16864069
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.16864068806171417, 0.16864068806171417, 0.16864068806171417, 0.16864068806171417, 0.16864068806171417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.16864068806171417

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16864069
Iteration 2/1000 | Loss: 0.00670350
Iteration 3/1000 | Loss: 0.00641114
Iteration 4/1000 | Loss: 0.01438059
Iteration 5/1000 | Loss: 0.01094841
Iteration 6/1000 | Loss: 0.00201727
Iteration 7/1000 | Loss: 0.00141137
Iteration 8/1000 | Loss: 0.00185158
Iteration 9/1000 | Loss: 0.00158715
Iteration 10/1000 | Loss: 0.00030135
Iteration 11/1000 | Loss: 0.00095324
Iteration 12/1000 | Loss: 0.00029333
Iteration 13/1000 | Loss: 0.00067878
Iteration 14/1000 | Loss: 0.00011514
Iteration 15/1000 | Loss: 0.00045133
Iteration 16/1000 | Loss: 0.00018822
Iteration 17/1000 | Loss: 0.00008528
Iteration 18/1000 | Loss: 0.00011311
Iteration 19/1000 | Loss: 0.00005656
Iteration 20/1000 | Loss: 0.00004879
Iteration 21/1000 | Loss: 0.00030710
Iteration 22/1000 | Loss: 0.00107825
Iteration 23/1000 | Loss: 0.00005856
Iteration 24/1000 | Loss: 0.00032511
Iteration 25/1000 | Loss: 0.00004390
Iteration 26/1000 | Loss: 0.00010815
Iteration 27/1000 | Loss: 0.00014637
Iteration 28/1000 | Loss: 0.00003720
Iteration 29/1000 | Loss: 0.00012465
Iteration 30/1000 | Loss: 0.00006216
Iteration 31/1000 | Loss: 0.00003424
Iteration 32/1000 | Loss: 0.00033919
Iteration 33/1000 | Loss: 0.00015631
Iteration 34/1000 | Loss: 0.00005215
Iteration 35/1000 | Loss: 0.00003298
Iteration 36/1000 | Loss: 0.00003175
Iteration 37/1000 | Loss: 0.00029824
Iteration 38/1000 | Loss: 0.00003936
Iteration 39/1000 | Loss: 0.00002998
Iteration 40/1000 | Loss: 0.00021509
Iteration 41/1000 | Loss: 0.00067529
Iteration 42/1000 | Loss: 0.00004161
Iteration 43/1000 | Loss: 0.00003408
Iteration 44/1000 | Loss: 0.00002950
Iteration 45/1000 | Loss: 0.00017731
Iteration 46/1000 | Loss: 0.00003005
Iteration 47/1000 | Loss: 0.00009059
Iteration 48/1000 | Loss: 0.00003218
Iteration 49/1000 | Loss: 0.00003055
Iteration 50/1000 | Loss: 0.00003004
Iteration 51/1000 | Loss: 0.00002698
Iteration 52/1000 | Loss: 0.00003423
Iteration 53/1000 | Loss: 0.00010699
Iteration 54/1000 | Loss: 0.00003256
Iteration 55/1000 | Loss: 0.00002632
Iteration 56/1000 | Loss: 0.00002634
Iteration 57/1000 | Loss: 0.00002614
Iteration 58/1000 | Loss: 0.00002609
Iteration 59/1000 | Loss: 0.00002608
Iteration 60/1000 | Loss: 0.00004452
Iteration 61/1000 | Loss: 0.00003914
Iteration 62/1000 | Loss: 0.00002609
Iteration 63/1000 | Loss: 0.00002585
Iteration 64/1000 | Loss: 0.00002986
Iteration 65/1000 | Loss: 0.00007734
Iteration 66/1000 | Loss: 0.00010866
Iteration 67/1000 | Loss: 0.00002589
Iteration 68/1000 | Loss: 0.00002568
Iteration 69/1000 | Loss: 0.00006402
Iteration 70/1000 | Loss: 0.00002550
Iteration 71/1000 | Loss: 0.00002528
Iteration 72/1000 | Loss: 0.00002528
Iteration 73/1000 | Loss: 0.00002527
Iteration 74/1000 | Loss: 0.00012121
Iteration 75/1000 | Loss: 0.00002874
Iteration 76/1000 | Loss: 0.00002602
Iteration 77/1000 | Loss: 0.00002540
Iteration 78/1000 | Loss: 0.00002522
Iteration 79/1000 | Loss: 0.00002521
Iteration 80/1000 | Loss: 0.00002520
Iteration 81/1000 | Loss: 0.00002517
Iteration 82/1000 | Loss: 0.00002516
Iteration 83/1000 | Loss: 0.00002515
Iteration 84/1000 | Loss: 0.00010797
Iteration 85/1000 | Loss: 0.00029547
Iteration 86/1000 | Loss: 0.00002640
Iteration 87/1000 | Loss: 0.00010384
Iteration 88/1000 | Loss: 0.00003286
Iteration 89/1000 | Loss: 0.00002621
Iteration 90/1000 | Loss: 0.00043066
Iteration 91/1000 | Loss: 0.00009868
Iteration 92/1000 | Loss: 0.00008283
Iteration 93/1000 | Loss: 0.00011806
Iteration 94/1000 | Loss: 0.00005671
Iteration 95/1000 | Loss: 0.00010111
Iteration 96/1000 | Loss: 0.00003066
Iteration 97/1000 | Loss: 0.00003445
Iteration 98/1000 | Loss: 0.00002847
Iteration 99/1000 | Loss: 0.00002523
Iteration 100/1000 | Loss: 0.00002893
Iteration 101/1000 | Loss: 0.00002432
Iteration 102/1000 | Loss: 0.00002412
Iteration 103/1000 | Loss: 0.00003175
Iteration 104/1000 | Loss: 0.00002315
Iteration 105/1000 | Loss: 0.00002304
Iteration 106/1000 | Loss: 0.00002299
Iteration 107/1000 | Loss: 0.00002298
Iteration 108/1000 | Loss: 0.00002298
Iteration 109/1000 | Loss: 0.00002298
Iteration 110/1000 | Loss: 0.00002297
Iteration 111/1000 | Loss: 0.00002297
Iteration 112/1000 | Loss: 0.00002295
Iteration 113/1000 | Loss: 0.00002295
Iteration 114/1000 | Loss: 0.00002295
Iteration 115/1000 | Loss: 0.00002295
Iteration 116/1000 | Loss: 0.00002294
Iteration 117/1000 | Loss: 0.00002294
Iteration 118/1000 | Loss: 0.00002294
Iteration 119/1000 | Loss: 0.00002294
Iteration 120/1000 | Loss: 0.00002294
Iteration 121/1000 | Loss: 0.00002294
Iteration 122/1000 | Loss: 0.00002294
Iteration 123/1000 | Loss: 0.00009860
Iteration 124/1000 | Loss: 0.00003257
Iteration 125/1000 | Loss: 0.00004972
Iteration 126/1000 | Loss: 0.00002296
Iteration 127/1000 | Loss: 0.00002292
Iteration 128/1000 | Loss: 0.00002290
Iteration 129/1000 | Loss: 0.00002289
Iteration 130/1000 | Loss: 0.00002289
Iteration 131/1000 | Loss: 0.00002289
Iteration 132/1000 | Loss: 0.00002288
Iteration 133/1000 | Loss: 0.00002288
Iteration 134/1000 | Loss: 0.00002288
Iteration 135/1000 | Loss: 0.00002288
Iteration 136/1000 | Loss: 0.00002287
Iteration 137/1000 | Loss: 0.00002287
Iteration 138/1000 | Loss: 0.00002287
Iteration 139/1000 | Loss: 0.00002287
Iteration 140/1000 | Loss: 0.00002287
Iteration 141/1000 | Loss: 0.00002287
Iteration 142/1000 | Loss: 0.00002286
Iteration 143/1000 | Loss: 0.00002286
Iteration 144/1000 | Loss: 0.00002286
Iteration 145/1000 | Loss: 0.00002285
Iteration 146/1000 | Loss: 0.00002285
Iteration 147/1000 | Loss: 0.00002285
Iteration 148/1000 | Loss: 0.00002285
Iteration 149/1000 | Loss: 0.00002285
Iteration 150/1000 | Loss: 0.00002284
Iteration 151/1000 | Loss: 0.00002284
Iteration 152/1000 | Loss: 0.00002284
Iteration 153/1000 | Loss: 0.00002283
Iteration 154/1000 | Loss: 0.00002283
Iteration 155/1000 | Loss: 0.00002283
Iteration 156/1000 | Loss: 0.00002283
Iteration 157/1000 | Loss: 0.00002283
Iteration 158/1000 | Loss: 0.00002282
Iteration 159/1000 | Loss: 0.00002282
Iteration 160/1000 | Loss: 0.00002282
Iteration 161/1000 | Loss: 0.00002282
Iteration 162/1000 | Loss: 0.00002282
Iteration 163/1000 | Loss: 0.00002282
Iteration 164/1000 | Loss: 0.00002282
Iteration 165/1000 | Loss: 0.00002282
Iteration 166/1000 | Loss: 0.00002282
Iteration 167/1000 | Loss: 0.00002282
Iteration 168/1000 | Loss: 0.00002282
Iteration 169/1000 | Loss: 0.00002282
Iteration 170/1000 | Loss: 0.00002282
Iteration 171/1000 | Loss: 0.00002282
Iteration 172/1000 | Loss: 0.00002282
Iteration 173/1000 | Loss: 0.00002282
Iteration 174/1000 | Loss: 0.00002282
Iteration 175/1000 | Loss: 0.00002282
Iteration 176/1000 | Loss: 0.00002282
Iteration 177/1000 | Loss: 0.00002282
Iteration 178/1000 | Loss: 0.00002282
Iteration 179/1000 | Loss: 0.00002282
Iteration 180/1000 | Loss: 0.00002282
Iteration 181/1000 | Loss: 0.00002282
Iteration 182/1000 | Loss: 0.00002282
Iteration 183/1000 | Loss: 0.00002282
Iteration 184/1000 | Loss: 0.00002282
Iteration 185/1000 | Loss: 0.00002282
Iteration 186/1000 | Loss: 0.00002282
Iteration 187/1000 | Loss: 0.00002282
Iteration 188/1000 | Loss: 0.00002282
Iteration 189/1000 | Loss: 0.00002282
Iteration 190/1000 | Loss: 0.00002282
Iteration 191/1000 | Loss: 0.00002282
Iteration 192/1000 | Loss: 0.00002282
Iteration 193/1000 | Loss: 0.00002282
Iteration 194/1000 | Loss: 0.00002282
Iteration 195/1000 | Loss: 0.00002282
Iteration 196/1000 | Loss: 0.00002282
Iteration 197/1000 | Loss: 0.00002282
Iteration 198/1000 | Loss: 0.00002282
Iteration 199/1000 | Loss: 0.00002282
Iteration 200/1000 | Loss: 0.00002282
Iteration 201/1000 | Loss: 0.00002282
Iteration 202/1000 | Loss: 0.00002282
Iteration 203/1000 | Loss: 0.00002282
Iteration 204/1000 | Loss: 0.00002282
Iteration 205/1000 | Loss: 0.00002282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [2.2817552235210314e-05, 2.2817552235210314e-05, 2.2817552235210314e-05, 2.2817552235210314e-05, 2.2817552235210314e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2817552235210314e-05

Optimization complete. Final v2v error: 3.8768928050994873 mm

Highest mean error: 8.588778495788574 mm for frame 51

Lowest mean error: 2.8739662170410156 mm for frame 37

Saving results

Total time: 174.30851793289185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462361
Iteration 2/25 | Loss: 0.00125504
Iteration 3/25 | Loss: 0.00090924
Iteration 4/25 | Loss: 0.00081329
Iteration 5/25 | Loss: 0.00077715
Iteration 6/25 | Loss: 0.00075035
Iteration 7/25 | Loss: 0.00074283
Iteration 8/25 | Loss: 0.00072829
Iteration 9/25 | Loss: 0.00071286
Iteration 10/25 | Loss: 0.00071072
Iteration 11/25 | Loss: 0.00071441
Iteration 12/25 | Loss: 0.00070918
Iteration 13/25 | Loss: 0.00070743
Iteration 14/25 | Loss: 0.00070899
Iteration 15/25 | Loss: 0.00070698
Iteration 16/25 | Loss: 0.00070572
Iteration 17/25 | Loss: 0.00070358
Iteration 18/25 | Loss: 0.00070394
Iteration 19/25 | Loss: 0.00070619
Iteration 20/25 | Loss: 0.00070507
Iteration 21/25 | Loss: 0.00071129
Iteration 22/25 | Loss: 0.00071131
Iteration 23/25 | Loss: 0.00071161
Iteration 24/25 | Loss: 0.00071268
Iteration 25/25 | Loss: 0.00071172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46415091
Iteration 2/25 | Loss: 0.00050829
Iteration 3/25 | Loss: 0.00050827
Iteration 4/25 | Loss: 0.00050827
Iteration 5/25 | Loss: 0.00050827
Iteration 6/25 | Loss: 0.00050827
Iteration 7/25 | Loss: 0.00050827
Iteration 8/25 | Loss: 0.00050827
Iteration 9/25 | Loss: 0.00050827
Iteration 10/25 | Loss: 0.00050827
Iteration 11/25 | Loss: 0.00050827
Iteration 12/25 | Loss: 0.00050827
Iteration 13/25 | Loss: 0.00050827
Iteration 14/25 | Loss: 0.00050827
Iteration 15/25 | Loss: 0.00050827
Iteration 16/25 | Loss: 0.00050827
Iteration 17/25 | Loss: 0.00050827
Iteration 18/25 | Loss: 0.00050827
Iteration 19/25 | Loss: 0.00050827
Iteration 20/25 | Loss: 0.00050827
Iteration 21/25 | Loss: 0.00050827
Iteration 22/25 | Loss: 0.00050827
Iteration 23/25 | Loss: 0.00050827
Iteration 24/25 | Loss: 0.00050827
Iteration 25/25 | Loss: 0.00050827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050827
Iteration 2/1000 | Loss: 0.00007882
Iteration 3/1000 | Loss: 0.00004861
Iteration 4/1000 | Loss: 0.00003367
Iteration 5/1000 | Loss: 0.00003050
Iteration 6/1000 | Loss: 0.00003637
Iteration 7/1000 | Loss: 0.00002829
Iteration 8/1000 | Loss: 0.00002935
Iteration 9/1000 | Loss: 0.00002708
Iteration 10/1000 | Loss: 0.00003050
Iteration 11/1000 | Loss: 0.00003079
Iteration 12/1000 | Loss: 0.00003098
Iteration 13/1000 | Loss: 0.00003096
Iteration 14/1000 | Loss: 0.00002843
Iteration 15/1000 | Loss: 0.00003652
Iteration 16/1000 | Loss: 0.00004310
Iteration 17/1000 | Loss: 0.00005217
Iteration 18/1000 | Loss: 0.00003972
Iteration 19/1000 | Loss: 0.00002628
Iteration 20/1000 | Loss: 0.00002567
Iteration 21/1000 | Loss: 0.00003212
Iteration 22/1000 | Loss: 0.00004854
Iteration 23/1000 | Loss: 0.00003759
Iteration 24/1000 | Loss: 0.00003121
Iteration 25/1000 | Loss: 0.00002997
Iteration 26/1000 | Loss: 0.00004154
Iteration 27/1000 | Loss: 0.00002835
Iteration 28/1000 | Loss: 0.00003130
Iteration 29/1000 | Loss: 0.00003084
Iteration 30/1000 | Loss: 0.00004289
Iteration 31/1000 | Loss: 0.00003231
Iteration 32/1000 | Loss: 0.00002664
Iteration 33/1000 | Loss: 0.00003590
Iteration 34/1000 | Loss: 0.00002956
Iteration 35/1000 | Loss: 0.00002626
Iteration 36/1000 | Loss: 0.00003342
Iteration 37/1000 | Loss: 0.00002803
Iteration 38/1000 | Loss: 0.00003262
Iteration 39/1000 | Loss: 0.00003023
Iteration 40/1000 | Loss: 0.00004015
Iteration 41/1000 | Loss: 0.00003741
Iteration 42/1000 | Loss: 0.00003262
Iteration 43/1000 | Loss: 0.00002793
Iteration 44/1000 | Loss: 0.00003723
Iteration 45/1000 | Loss: 0.00002820
Iteration 46/1000 | Loss: 0.00002595
Iteration 47/1000 | Loss: 0.00003874
Iteration 48/1000 | Loss: 0.00003055
Iteration 49/1000 | Loss: 0.00003744
Iteration 50/1000 | Loss: 0.00003097
Iteration 51/1000 | Loss: 0.00003715
Iteration 52/1000 | Loss: 0.00003330
Iteration 53/1000 | Loss: 0.00004123
Iteration 54/1000 | Loss: 0.00003449
Iteration 55/1000 | Loss: 0.00003675
Iteration 56/1000 | Loss: 0.00003446
Iteration 57/1000 | Loss: 0.00004459
Iteration 58/1000 | Loss: 0.00003654
Iteration 59/1000 | Loss: 0.00005823
Iteration 60/1000 | Loss: 0.00004623
Iteration 61/1000 | Loss: 0.00004437
Iteration 62/1000 | Loss: 0.00003871
Iteration 63/1000 | Loss: 0.00004642
Iteration 64/1000 | Loss: 0.00004036
Iteration 65/1000 | Loss: 0.00003604
Iteration 66/1000 | Loss: 0.00003632
Iteration 67/1000 | Loss: 0.00002932
Iteration 68/1000 | Loss: 0.00004613
Iteration 69/1000 | Loss: 0.00003757
Iteration 70/1000 | Loss: 0.00005573
Iteration 71/1000 | Loss: 0.00004488
Iteration 72/1000 | Loss: 0.00005941
Iteration 73/1000 | Loss: 0.00005577
Iteration 74/1000 | Loss: 0.00004673
Iteration 75/1000 | Loss: 0.00002984
Iteration 76/1000 | Loss: 0.00003201
Iteration 77/1000 | Loss: 0.00002973
Iteration 78/1000 | Loss: 0.00003182
Iteration 79/1000 | Loss: 0.00002933
Iteration 80/1000 | Loss: 0.00002570
Iteration 81/1000 | Loss: 0.00002504
Iteration 82/1000 | Loss: 0.00002924
Iteration 83/1000 | Loss: 0.00002788
Iteration 84/1000 | Loss: 0.00003071
Iteration 85/1000 | Loss: 0.00002939
Iteration 86/1000 | Loss: 0.00004211
Iteration 87/1000 | Loss: 0.00003758
Iteration 88/1000 | Loss: 0.00004874
Iteration 89/1000 | Loss: 0.00004224
Iteration 90/1000 | Loss: 0.00003995
Iteration 91/1000 | Loss: 0.00003627
Iteration 92/1000 | Loss: 0.00004105
Iteration 93/1000 | Loss: 0.00003725
Iteration 94/1000 | Loss: 0.00005178
Iteration 95/1000 | Loss: 0.00004246
Iteration 96/1000 | Loss: 0.00006172
Iteration 97/1000 | Loss: 0.00005037
Iteration 98/1000 | Loss: 0.00007283
Iteration 99/1000 | Loss: 0.00005528
Iteration 100/1000 | Loss: 0.00021543
Iteration 101/1000 | Loss: 0.00015653
Iteration 102/1000 | Loss: 0.00006535
Iteration 103/1000 | Loss: 0.00006806
Iteration 104/1000 | Loss: 0.00003374
Iteration 105/1000 | Loss: 0.00003074
Iteration 106/1000 | Loss: 0.00002583
Iteration 107/1000 | Loss: 0.00002934
Iteration 108/1000 | Loss: 0.00002591
Iteration 109/1000 | Loss: 0.00003259
Iteration 110/1000 | Loss: 0.00002833
Iteration 111/1000 | Loss: 0.00003895
Iteration 112/1000 | Loss: 0.00004220
Iteration 113/1000 | Loss: 0.00003814
Iteration 114/1000 | Loss: 0.00003141
Iteration 115/1000 | Loss: 0.00002680
Iteration 116/1000 | Loss: 0.00003052
Iteration 117/1000 | Loss: 0.00003052
Iteration 118/1000 | Loss: 0.00003466
Iteration 119/1000 | Loss: 0.00003584
Iteration 120/1000 | Loss: 0.00003976
Iteration 121/1000 | Loss: 0.00003750
Iteration 122/1000 | Loss: 0.00004342
Iteration 123/1000 | Loss: 0.00004021
Iteration 124/1000 | Loss: 0.00005124
Iteration 125/1000 | Loss: 0.00004554
Iteration 126/1000 | Loss: 0.00005799
Iteration 127/1000 | Loss: 0.00004937
Iteration 128/1000 | Loss: 0.00005949
Iteration 129/1000 | Loss: 0.00003886
Iteration 130/1000 | Loss: 0.00004932
Iteration 131/1000 | Loss: 0.00005766
Iteration 132/1000 | Loss: 0.00003489
Iteration 133/1000 | Loss: 0.00004074
Iteration 134/1000 | Loss: 0.00003602
Iteration 135/1000 | Loss: 0.00004874
Iteration 136/1000 | Loss: 0.00004343
Iteration 137/1000 | Loss: 0.00006049
Iteration 138/1000 | Loss: 0.00005054
Iteration 139/1000 | Loss: 0.00006863
Iteration 140/1000 | Loss: 0.00003419
Iteration 141/1000 | Loss: 0.00002567
Iteration 142/1000 | Loss: 0.00002307
Iteration 143/1000 | Loss: 0.00002930
Iteration 144/1000 | Loss: 0.00003125
Iteration 145/1000 | Loss: 0.00002897
Iteration 146/1000 | Loss: 0.00002888
Iteration 147/1000 | Loss: 0.00003376
Iteration 148/1000 | Loss: 0.00003274
Iteration 149/1000 | Loss: 0.00003332
Iteration 150/1000 | Loss: 0.00003537
Iteration 151/1000 | Loss: 0.00002623
Iteration 152/1000 | Loss: 0.00003050
Iteration 153/1000 | Loss: 0.00002791
Iteration 154/1000 | Loss: 0.00003026
Iteration 155/1000 | Loss: 0.00003658
Iteration 156/1000 | Loss: 0.00003514
Iteration 157/1000 | Loss: 0.00004202
Iteration 158/1000 | Loss: 0.00004023
Iteration 159/1000 | Loss: 0.00004867
Iteration 160/1000 | Loss: 0.00003544
Iteration 161/1000 | Loss: 0.00005282
Iteration 162/1000 | Loss: 0.00004469
Iteration 163/1000 | Loss: 0.00005214
Iteration 164/1000 | Loss: 0.00004360
Iteration 165/1000 | Loss: 0.00005817
Iteration 166/1000 | Loss: 0.00004726
Iteration 167/1000 | Loss: 0.00006678
Iteration 168/1000 | Loss: 0.00004851
Iteration 169/1000 | Loss: 0.00007157
Iteration 170/1000 | Loss: 0.00005182
Iteration 171/1000 | Loss: 0.00007166
Iteration 172/1000 | Loss: 0.00005271
Iteration 173/1000 | Loss: 0.00004394
Iteration 174/1000 | Loss: 0.00003170
Iteration 175/1000 | Loss: 0.00004817
Iteration 176/1000 | Loss: 0.00003095
Iteration 177/1000 | Loss: 0.00003935
Iteration 178/1000 | Loss: 0.00003148
Iteration 179/1000 | Loss: 0.00006595
Iteration 180/1000 | Loss: 0.00003943
Iteration 181/1000 | Loss: 0.00006047
Iteration 182/1000 | Loss: 0.00003302
Iteration 183/1000 | Loss: 0.00002365
Iteration 184/1000 | Loss: 0.00002730
Iteration 185/1000 | Loss: 0.00002571
Iteration 186/1000 | Loss: 0.00003652
Iteration 187/1000 | Loss: 0.00003850
Iteration 188/1000 | Loss: 0.00005298
Iteration 189/1000 | Loss: 0.00004643
Iteration 190/1000 | Loss: 0.00006455
Iteration 191/1000 | Loss: 0.00003389
Iteration 192/1000 | Loss: 0.00005745
Iteration 193/1000 | Loss: 0.00004251
Iteration 194/1000 | Loss: 0.00002824
Iteration 195/1000 | Loss: 0.00003528
Iteration 196/1000 | Loss: 0.00003753
Iteration 197/1000 | Loss: 0.00003780
Iteration 198/1000 | Loss: 0.00003986
Iteration 199/1000 | Loss: 0.00005985
Iteration 200/1000 | Loss: 0.00004856
Iteration 201/1000 | Loss: 0.00006601
Iteration 202/1000 | Loss: 0.00005591
Iteration 203/1000 | Loss: 0.00006122
Iteration 204/1000 | Loss: 0.00005509
Iteration 205/1000 | Loss: 0.00006777
Iteration 206/1000 | Loss: 0.00006093
Iteration 207/1000 | Loss: 0.00006348
Iteration 208/1000 | Loss: 0.00003224
Iteration 209/1000 | Loss: 0.00002951
Iteration 210/1000 | Loss: 0.00002600
Iteration 211/1000 | Loss: 0.00002360
Iteration 212/1000 | Loss: 0.00003632
Iteration 213/1000 | Loss: 0.00005035
Iteration 214/1000 | Loss: 0.00002594
Iteration 215/1000 | Loss: 0.00002702
Iteration 216/1000 | Loss: 0.00004549
Iteration 217/1000 | Loss: 0.00003291
Iteration 218/1000 | Loss: 0.00003472
Iteration 219/1000 | Loss: 0.00003177
Iteration 220/1000 | Loss: 0.00003583
Iteration 221/1000 | Loss: 0.00003542
Iteration 222/1000 | Loss: 0.00003770
Iteration 223/1000 | Loss: 0.00003795
Iteration 224/1000 | Loss: 0.00003551
Iteration 225/1000 | Loss: 0.00003427
Iteration 226/1000 | Loss: 0.00004546
Iteration 227/1000 | Loss: 0.00004838
Iteration 228/1000 | Loss: 0.00005195
Iteration 229/1000 | Loss: 0.00005182
Iteration 230/1000 | Loss: 0.00005693
Iteration 231/1000 | Loss: 0.00004452
Iteration 232/1000 | Loss: 0.00005124
Iteration 233/1000 | Loss: 0.00005371
Iteration 234/1000 | Loss: 0.00006302
Iteration 235/1000 | Loss: 0.00003863
Iteration 236/1000 | Loss: 0.00004190
Iteration 237/1000 | Loss: 0.00003508
Iteration 238/1000 | Loss: 0.00006497
Iteration 239/1000 | Loss: 0.00005429
Iteration 240/1000 | Loss: 0.00006470
Iteration 241/1000 | Loss: 0.00005563
Iteration 242/1000 | Loss: 0.00007495
Iteration 243/1000 | Loss: 0.00006063
Iteration 244/1000 | Loss: 0.00004611
Iteration 245/1000 | Loss: 0.00003274
Iteration 246/1000 | Loss: 0.00005261
Iteration 247/1000 | Loss: 0.00004410
Iteration 248/1000 | Loss: 0.00006203
Iteration 249/1000 | Loss: 0.00005422
Iteration 250/1000 | Loss: 0.00006454
Iteration 251/1000 | Loss: 0.00006155
Iteration 252/1000 | Loss: 0.00007769
Iteration 253/1000 | Loss: 0.00006557
Iteration 254/1000 | Loss: 0.00008285
Iteration 255/1000 | Loss: 0.00007544
Iteration 256/1000 | Loss: 0.00010037
Iteration 257/1000 | Loss: 0.00006235
Iteration 258/1000 | Loss: 0.00005107
Iteration 259/1000 | Loss: 0.00003847
Iteration 260/1000 | Loss: 0.00004625
Iteration 261/1000 | Loss: 0.00004790
Iteration 262/1000 | Loss: 0.00008385
Iteration 263/1000 | Loss: 0.00005995
Iteration 264/1000 | Loss: 0.00008269
Iteration 265/1000 | Loss: 0.00006065
Iteration 266/1000 | Loss: 0.00013505
Iteration 267/1000 | Loss: 0.00005784
Iteration 268/1000 | Loss: 0.00007055
Iteration 269/1000 | Loss: 0.00005680
Iteration 270/1000 | Loss: 0.00011006
Iteration 271/1000 | Loss: 0.00005747
Iteration 272/1000 | Loss: 0.00006034
Iteration 273/1000 | Loss: 0.00004807
Iteration 274/1000 | Loss: 0.00005465
Iteration 275/1000 | Loss: 0.00006045
Iteration 276/1000 | Loss: 0.00005941
Iteration 277/1000 | Loss: 0.00004700
Iteration 278/1000 | Loss: 0.00007332
Iteration 279/1000 | Loss: 0.00006203
Iteration 280/1000 | Loss: 0.00008339
Iteration 281/1000 | Loss: 0.00006994
Iteration 282/1000 | Loss: 0.00009371
Iteration 283/1000 | Loss: 0.00007016
Iteration 284/1000 | Loss: 0.00006627
Iteration 285/1000 | Loss: 0.00005623
Iteration 286/1000 | Loss: 0.00009826
Iteration 287/1000 | Loss: 0.00005332
Iteration 288/1000 | Loss: 0.00006680
Iteration 289/1000 | Loss: 0.00004091
Iteration 290/1000 | Loss: 0.00002588
Iteration 291/1000 | Loss: 0.00002322
Iteration 292/1000 | Loss: 0.00002351
Iteration 293/1000 | Loss: 0.00002307
Iteration 294/1000 | Loss: 0.00002373
Iteration 295/1000 | Loss: 0.00002685
Iteration 296/1000 | Loss: 0.00002445
Iteration 297/1000 | Loss: 0.00002943
Iteration 298/1000 | Loss: 0.00002527
Iteration 299/1000 | Loss: 0.00002749
Iteration 300/1000 | Loss: 0.00002716
Iteration 301/1000 | Loss: 0.00002523
Iteration 302/1000 | Loss: 0.00003318
Iteration 303/1000 | Loss: 0.00002714
Iteration 304/1000 | Loss: 0.00002587
Iteration 305/1000 | Loss: 0.00003007
Iteration 306/1000 | Loss: 0.00002543
Iteration 307/1000 | Loss: 0.00002703
Iteration 308/1000 | Loss: 0.00002363
Iteration 309/1000 | Loss: 0.00002210
Iteration 310/1000 | Loss: 0.00002765
Iteration 311/1000 | Loss: 0.00002509
Iteration 312/1000 | Loss: 0.00002495
Iteration 313/1000 | Loss: 0.00002511
Iteration 314/1000 | Loss: 0.00002283
Iteration 315/1000 | Loss: 0.00002966
Iteration 316/1000 | Loss: 0.00002567
Iteration 317/1000 | Loss: 0.00002284
Iteration 318/1000 | Loss: 0.00003140
Iteration 319/1000 | Loss: 0.00003269
Iteration 320/1000 | Loss: 0.00003590
Iteration 321/1000 | Loss: 0.00003137
Iteration 322/1000 | Loss: 0.00004070
Iteration 323/1000 | Loss: 0.00003975
Iteration 324/1000 | Loss: 0.00005426
Iteration 325/1000 | Loss: 0.00003360
Iteration 326/1000 | Loss: 0.00004814
Iteration 327/1000 | Loss: 0.00003788
Iteration 328/1000 | Loss: 0.00005679
Iteration 329/1000 | Loss: 0.00002917
Iteration 330/1000 | Loss: 0.00003032
Iteration 331/1000 | Loss: 0.00003024
Iteration 332/1000 | Loss: 0.00004179
Iteration 333/1000 | Loss: 0.00003212
Iteration 334/1000 | Loss: 0.00004964
Iteration 335/1000 | Loss: 0.00003542
Iteration 336/1000 | Loss: 0.00004604
Iteration 337/1000 | Loss: 0.00004201
Iteration 338/1000 | Loss: 0.00005682
Iteration 339/1000 | Loss: 0.00003220
Iteration 340/1000 | Loss: 0.00003162
Iteration 341/1000 | Loss: 0.00002936
Iteration 342/1000 | Loss: 0.00004362
Iteration 343/1000 | Loss: 0.00003522
Iteration 344/1000 | Loss: 0.00003701
Iteration 345/1000 | Loss: 0.00003203
Iteration 346/1000 | Loss: 0.00004991
Iteration 347/1000 | Loss: 0.00003860
Iteration 348/1000 | Loss: 0.00005886
Iteration 349/1000 | Loss: 0.00004139
Iteration 350/1000 | Loss: 0.00005863
Iteration 351/1000 | Loss: 0.00004356
Iteration 352/1000 | Loss: 0.00003769
Iteration 353/1000 | Loss: 0.00002862
Iteration 354/1000 | Loss: 0.00002560
Iteration 355/1000 | Loss: 0.00002579
Iteration 356/1000 | Loss: 0.00003412
Iteration 357/1000 | Loss: 0.00003747
Iteration 358/1000 | Loss: 0.00003808
Iteration 359/1000 | Loss: 0.00003264
Iteration 360/1000 | Loss: 0.00002356
Iteration 361/1000 | Loss: 0.00002987
Iteration 362/1000 | Loss: 0.00002654
Iteration 363/1000 | Loss: 0.00002306
Iteration 364/1000 | Loss: 0.00003568
Iteration 365/1000 | Loss: 0.00002777
Iteration 366/1000 | Loss: 0.00003885
Iteration 367/1000 | Loss: 0.00002802
Iteration 368/1000 | Loss: 0.00003571
Iteration 369/1000 | Loss: 0.00002820
Iteration 370/1000 | Loss: 0.00004399
Iteration 371/1000 | Loss: 0.00003254
Iteration 372/1000 | Loss: 0.00002669
Iteration 373/1000 | Loss: 0.00002353
Iteration 374/1000 | Loss: 0.00003407
Iteration 375/1000 | Loss: 0.00003105
Iteration 376/1000 | Loss: 0.00002919
Iteration 377/1000 | Loss: 0.00002986
Iteration 378/1000 | Loss: 0.00002909
Iteration 379/1000 | Loss: 0.00002546
Iteration 380/1000 | Loss: 0.00003149
Iteration 381/1000 | Loss: 0.00003237
Iteration 382/1000 | Loss: 0.00002711
Iteration 383/1000 | Loss: 0.00002485
Iteration 384/1000 | Loss: 0.00003370
Iteration 385/1000 | Loss: 0.00002689
Iteration 386/1000 | Loss: 0.00002715
Iteration 387/1000 | Loss: 0.00002445
Iteration 388/1000 | Loss: 0.00003042
Iteration 389/1000 | Loss: 0.00002578
Iteration 390/1000 | Loss: 0.00003847
Iteration 391/1000 | Loss: 0.00002762
Iteration 392/1000 | Loss: 0.00003138
Iteration 393/1000 | Loss: 0.00002572
Iteration 394/1000 | Loss: 0.00004058
Iteration 395/1000 | Loss: 0.00002884
Iteration 396/1000 | Loss: 0.00005489
Iteration 397/1000 | Loss: 0.00002964
Iteration 398/1000 | Loss: 0.00005043
Iteration 399/1000 | Loss: 0.00003311
Iteration 400/1000 | Loss: 0.00004060
Iteration 401/1000 | Loss: 0.00003069
Iteration 402/1000 | Loss: 0.00003151
Iteration 403/1000 | Loss: 0.00002642
Iteration 404/1000 | Loss: 0.00003964
Iteration 405/1000 | Loss: 0.00002647
Iteration 406/1000 | Loss: 0.00003408
Iteration 407/1000 | Loss: 0.00002510
Iteration 408/1000 | Loss: 0.00002849
Iteration 409/1000 | Loss: 0.00002847
Iteration 410/1000 | Loss: 0.00003458
Iteration 411/1000 | Loss: 0.00002502
Iteration 412/1000 | Loss: 0.00003884
Iteration 413/1000 | Loss: 0.00002766
Iteration 414/1000 | Loss: 0.00004328
Iteration 415/1000 | Loss: 0.00003084
Iteration 416/1000 | Loss: 0.00002892
Iteration 417/1000 | Loss: 0.00002450
Iteration 418/1000 | Loss: 0.00002311
Iteration 419/1000 | Loss: 0.00004836
Iteration 420/1000 | Loss: 0.00003410
Iteration 421/1000 | Loss: 0.00005623
Iteration 422/1000 | Loss: 0.00003117
Iteration 423/1000 | Loss: 0.00002457
Iteration 424/1000 | Loss: 0.00003217
Iteration 425/1000 | Loss: 0.00002767
Iteration 426/1000 | Loss: 0.00004669
Iteration 427/1000 | Loss: 0.00002936
Iteration 428/1000 | Loss: 0.00003225
Iteration 429/1000 | Loss: 0.00002500
Iteration 430/1000 | Loss: 0.00003584
Iteration 431/1000 | Loss: 0.00002500
Iteration 432/1000 | Loss: 0.00002389
Iteration 433/1000 | Loss: 0.00003012
Iteration 434/1000 | Loss: 0.00002716
Iteration 435/1000 | Loss: 0.00003071
Iteration 436/1000 | Loss: 0.00003566
Iteration 437/1000 | Loss: 0.00003809
Iteration 438/1000 | Loss: 0.00003517
Iteration 439/1000 | Loss: 0.00004136
Iteration 440/1000 | Loss: 0.00003029
Iteration 441/1000 | Loss: 0.00003762
Iteration 442/1000 | Loss: 0.00002694
Iteration 443/1000 | Loss: 0.00004277
Iteration 444/1000 | Loss: 0.00003607
Iteration 445/1000 | Loss: 0.00004643
Iteration 446/1000 | Loss: 0.00004531
Iteration 447/1000 | Loss: 0.00005510
Iteration 448/1000 | Loss: 0.00004250
Iteration 449/1000 | Loss: 0.00005310
Iteration 450/1000 | Loss: 0.00004435
Iteration 451/1000 | Loss: 0.00004299
Iteration 452/1000 | Loss: 0.00004112
Iteration 453/1000 | Loss: 0.00003910
Iteration 454/1000 | Loss: 0.00004376
Iteration 455/1000 | Loss: 0.00003541
Iteration 456/1000 | Loss: 0.00004907
Iteration 457/1000 | Loss: 0.00003974
Iteration 458/1000 | Loss: 0.00005616
Iteration 459/1000 | Loss: 0.00004760
Iteration 460/1000 | Loss: 0.00005764
Iteration 461/1000 | Loss: 0.00004629
Iteration 462/1000 | Loss: 0.00005325
Iteration 463/1000 | Loss: 0.00004152
Iteration 464/1000 | Loss: 0.00005420
Iteration 465/1000 | Loss: 0.00005422
Iteration 466/1000 | Loss: 0.00003294
Iteration 467/1000 | Loss: 0.00002742
Iteration 468/1000 | Loss: 0.00003693
Iteration 469/1000 | Loss: 0.00004175
Iteration 470/1000 | Loss: 0.00004073
Iteration 471/1000 | Loss: 0.00004494
Iteration 472/1000 | Loss: 0.00004612
Iteration 473/1000 | Loss: 0.00003646
Iteration 474/1000 | Loss: 0.00004061
Iteration 475/1000 | Loss: 0.00004258
Iteration 476/1000 | Loss: 0.00004683
Iteration 477/1000 | Loss: 0.00004858
Iteration 478/1000 | Loss: 0.00003949
Iteration 479/1000 | Loss: 0.00003989
Iteration 480/1000 | Loss: 0.00004988
Iteration 481/1000 | Loss: 0.00003777
Iteration 482/1000 | Loss: 0.00002940
Iteration 483/1000 | Loss: 0.00003576
Iteration 484/1000 | Loss: 0.00003801
Iteration 485/1000 | Loss: 0.00004421
Iteration 486/1000 | Loss: 0.00005187
Iteration 487/1000 | Loss: 0.00005054
Iteration 488/1000 | Loss: 0.00005429
Iteration 489/1000 | Loss: 0.00004744
Iteration 490/1000 | Loss: 0.00003640
Iteration 491/1000 | Loss: 0.00004467
Iteration 492/1000 | Loss: 0.00005334
Iteration 493/1000 | Loss: 0.00004913
Iteration 494/1000 | Loss: 0.00006321
Iteration 495/1000 | Loss: 0.00005476
Iteration 496/1000 | Loss: 0.00006485
Iteration 497/1000 | Loss: 0.00005427
Iteration 498/1000 | Loss: 0.00006253
Iteration 499/1000 | Loss: 0.00004149
Iteration 500/1000 | Loss: 0.00002534
Iteration 501/1000 | Loss: 0.00002410
Iteration 502/1000 | Loss: 0.00002497
Iteration 503/1000 | Loss: 0.00003179
Iteration 504/1000 | Loss: 0.00003223
Iteration 505/1000 | Loss: 0.00002909
Iteration 506/1000 | Loss: 0.00002447
Iteration 507/1000 | Loss: 0.00002667
Iteration 508/1000 | Loss: 0.00002902
Iteration 509/1000 | Loss: 0.00002782
Iteration 510/1000 | Loss: 0.00002885
Iteration 511/1000 | Loss: 0.00003093
Iteration 512/1000 | Loss: 0.00002681
Iteration 513/1000 | Loss: 0.00002567
Iteration 514/1000 | Loss: 0.00003880
Iteration 515/1000 | Loss: 0.00003540
Iteration 516/1000 | Loss: 0.00003449
Iteration 517/1000 | Loss: 0.00002500
Iteration 518/1000 | Loss: 0.00002694
Iteration 519/1000 | Loss: 0.00003251
Iteration 520/1000 | Loss: 0.00003128
Iteration 521/1000 | Loss: 0.00003027
Iteration 522/1000 | Loss: 0.00002902
Iteration 523/1000 | Loss: 0.00003536
Iteration 524/1000 | Loss: 0.00003369
Iteration 525/1000 | Loss: 0.00002936
Iteration 526/1000 | Loss: 0.00002560
Iteration 527/1000 | Loss: 0.00002372
Iteration 528/1000 | Loss: 0.00002536
Iteration 529/1000 | Loss: 0.00003184
Iteration 530/1000 | Loss: 0.00003212
Iteration 531/1000 | Loss: 0.00002518
Iteration 532/1000 | Loss: 0.00002619
Iteration 533/1000 | Loss: 0.00003424
Iteration 534/1000 | Loss: 0.00002757
Iteration 535/1000 | Loss: 0.00003078
Iteration 536/1000 | Loss: 0.00002803
Iteration 537/1000 | Loss: 0.00002860
Iteration 538/1000 | Loss: 0.00003284
Iteration 539/1000 | Loss: 0.00002813
Iteration 540/1000 | Loss: 0.00002791
Iteration 541/1000 | Loss: 0.00003105
Iteration 542/1000 | Loss: 0.00002961
Iteration 543/1000 | Loss: 0.00003071
Iteration 544/1000 | Loss: 0.00002890
Iteration 545/1000 | Loss: 0.00003638
Iteration 546/1000 | Loss: 0.00003741
Iteration 547/1000 | Loss: 0.00003028
Iteration 548/1000 | Loss: 0.00003390
Iteration 549/1000 | Loss: 0.00003726
Iteration 550/1000 | Loss: 0.00003532
Iteration 551/1000 | Loss: 0.00004832
Iteration 552/1000 | Loss: 0.00004175
Iteration 553/1000 | Loss: 0.00005517
Iteration 554/1000 | Loss: 0.00004528
Iteration 555/1000 | Loss: 0.00005537
Iteration 556/1000 | Loss: 0.00004838
Iteration 557/1000 | Loss: 0.00006028
Iteration 558/1000 | Loss: 0.00005440
Iteration 559/1000 | Loss: 0.00006392
Iteration 560/1000 | Loss: 0.00004943
Iteration 561/1000 | Loss: 0.00004731
Iteration 562/1000 | Loss: 0.00005124
Iteration 563/1000 | Loss: 0.00006545
Iteration 564/1000 | Loss: 0.00004843
Iteration 565/1000 | Loss: 0.00004248
Iteration 566/1000 | Loss: 0.00003536
Iteration 567/1000 | Loss: 0.00002495
Iteration 568/1000 | Loss: 0.00002883
Iteration 569/1000 | Loss: 0.00003104
Iteration 570/1000 | Loss: 0.00003379
Iteration 571/1000 | Loss: 0.00003752
Iteration 572/1000 | Loss: 0.00003883
Iteration 573/1000 | Loss: 0.00003598
Iteration 574/1000 | Loss: 0.00003171
Iteration 575/1000 | Loss: 0.00004224
Iteration 576/1000 | Loss: 0.00004251
Iteration 577/1000 | Loss: 0.00002643
Iteration 578/1000 | Loss: 0.00002910
Iteration 579/1000 | Loss: 0.00003353
Iteration 580/1000 | Loss: 0.00002563
Iteration 581/1000 | Loss: 0.00003235
Iteration 582/1000 | Loss: 0.00003713
Iteration 583/1000 | Loss: 0.00005226
Iteration 584/1000 | Loss: 0.00005368
Iteration 585/1000 | Loss: 0.00005967
Iteration 586/1000 | Loss: 0.00003909
Iteration 587/1000 | Loss: 0.00002688
Iteration 588/1000 | Loss: 0.00003011
Iteration 589/1000 | Loss: 0.00003964
Iteration 590/1000 | Loss: 0.00004523
Iteration 591/1000 | Loss: 0.00004985
Iteration 592/1000 | Loss: 0.00005257
Iteration 593/1000 | Loss: 0.00004240
Iteration 594/1000 | Loss: 0.00004217
Iteration 595/1000 | Loss: 0.00004022
Iteration 596/1000 | Loss: 0.00002548
Iteration 597/1000 | Loss: 0.00002309
Iteration 598/1000 | Loss: 0.00002761
Iteration 599/1000 | Loss: 0.00002669
Iteration 600/1000 | Loss: 0.00002671
Iteration 601/1000 | Loss: 0.00002994
Iteration 602/1000 | Loss: 0.00002821
Iteration 603/1000 | Loss: 0.00003166
Iteration 604/1000 | Loss: 0.00002479
Iteration 605/1000 | Loss: 0.00002521
Iteration 606/1000 | Loss: 0.00003394
Iteration 607/1000 | Loss: 0.00002785
Iteration 608/1000 | Loss: 0.00003219
Iteration 609/1000 | Loss: 0.00003145
Iteration 610/1000 | Loss: 0.00002730
Iteration 611/1000 | Loss: 0.00002703
Iteration 612/1000 | Loss: 0.00002752
Iteration 613/1000 | Loss: 0.00002381
Iteration 614/1000 | Loss: 0.00002670
Iteration 615/1000 | Loss: 0.00002715
Iteration 616/1000 | Loss: 0.00003670
Iteration 617/1000 | Loss: 0.00003399
Iteration 618/1000 | Loss: 0.00003989
Iteration 619/1000 | Loss: 0.00003863
Iteration 620/1000 | Loss: 0.00004346
Iteration 621/1000 | Loss: 0.00004232
Iteration 622/1000 | Loss: 0.00002861
Iteration 623/1000 | Loss: 0.00002905
Iteration 624/1000 | Loss: 0.00002963
Iteration 625/1000 | Loss: 0.00003488
Iteration 626/1000 | Loss: 0.00003847
Iteration 627/1000 | Loss: 0.00003838
Iteration 628/1000 | Loss: 0.00004250
Iteration 629/1000 | Loss: 0.00004361
Iteration 630/1000 | Loss: 0.00003452
Iteration 631/1000 | Loss: 0.00003493
Iteration 632/1000 | Loss: 0.00003693
Iteration 633/1000 | Loss: 0.00003406
Iteration 634/1000 | Loss: 0.00003737
Iteration 635/1000 | Loss: 0.00004086
Iteration 636/1000 | Loss: 0.00004191
Iteration 637/1000 | Loss: 0.00004367
Iteration 638/1000 | Loss: 0.00004336
Iteration 639/1000 | Loss: 0.00004404
Iteration 640/1000 | Loss: 0.00003728
Iteration 641/1000 | Loss: 0.00003800
Iteration 642/1000 | Loss: 0.00005533
Iteration 643/1000 | Loss: 0.00005254
Iteration 644/1000 | Loss: 0.00004775
Iteration 645/1000 | Loss: 0.00004166
Iteration 646/1000 | Loss: 0.00004713
Iteration 647/1000 | Loss: 0.00004456
Iteration 648/1000 | Loss: 0.00002635
Iteration 649/1000 | Loss: 0.00002479
Iteration 650/1000 | Loss: 0.00002625
Iteration 651/1000 | Loss: 0.00002519
Iteration 652/1000 | Loss: 0.00004365
Iteration 653/1000 | Loss: 0.00004642
Iteration 654/1000 | Loss: 0.00005026
Iteration 655/1000 | Loss: 0.00004628
Iteration 656/1000 | Loss: 0.00004234
Iteration 657/1000 | Loss: 0.00004645
Iteration 658/1000 | Loss: 0.00005157
Iteration 659/1000 | Loss: 0.00004582
Iteration 660/1000 | Loss: 0.00003541
Iteration 661/1000 | Loss: 0.00003646
Iteration 662/1000 | Loss: 0.00003038
Iteration 663/1000 | Loss: 0.00002931
Iteration 664/1000 | Loss: 0.00002816
Iteration 665/1000 | Loss: 0.00004637
Iteration 666/1000 | Loss: 0.00004232
Iteration 667/1000 | Loss: 0.00006342
Iteration 668/1000 | Loss: 0.00004699
Iteration 669/1000 | Loss: 0.00007080
Iteration 670/1000 | Loss: 0.00004241
Iteration 671/1000 | Loss: 0.00005609
Iteration 672/1000 | Loss: 0.00004691
Iteration 673/1000 | Loss: 0.00004544
Iteration 674/1000 | Loss: 0.00004915
Iteration 675/1000 | Loss: 0.00002592
Iteration 676/1000 | Loss: 0.00002587
Iteration 677/1000 | Loss: 0.00003853
Iteration 678/1000 | Loss: 0.00004340
Iteration 679/1000 | Loss: 0.00004831
Iteration 680/1000 | Loss: 0.00004620
Iteration 681/1000 | Loss: 0.00004047
Iteration 682/1000 | Loss: 0.00003595
Iteration 683/1000 | Loss: 0.00004474
Iteration 684/1000 | Loss: 0.00004047
Iteration 685/1000 | Loss: 0.00004707
Iteration 686/1000 | Loss: 0.00003440
Iteration 687/1000 | Loss: 0.00004164
Iteration 688/1000 | Loss: 0.00003161
Iteration 689/1000 | Loss: 0.00003872
Iteration 690/1000 | Loss: 0.00003125
Iteration 691/1000 | Loss: 0.00002688
Iteration 692/1000 | Loss: 0.00002481
Iteration 693/1000 | Loss: 0.00003339
Iteration 694/1000 | Loss: 0.00002902
Iteration 695/1000 | Loss: 0.00003069
Iteration 696/1000 | Loss: 0.00002326
Iteration 697/1000 | Loss: 0.00002544
Iteration 698/1000 | Loss: 0.00002634
Iteration 699/1000 | Loss: 0.00003877
Iteration 700/1000 | Loss: 0.00003314
Iteration 701/1000 | Loss: 0.00004138
Iteration 702/1000 | Loss: 0.00003679
Iteration 703/1000 | Loss: 0.00004491
Iteration 704/1000 | Loss: 0.00003933
Iteration 705/1000 | Loss: 0.00002627
Iteration 706/1000 | Loss: 0.00002432
Iteration 707/1000 | Loss: 0.00003937
Iteration 708/1000 | Loss: 0.00002852
Iteration 709/1000 | Loss: 0.00002478
Iteration 710/1000 | Loss: 0.00002438
Iteration 711/1000 | Loss: 0.00002359
Iteration 712/1000 | Loss: 0.00003172
Iteration 713/1000 | Loss: 0.00002994
Iteration 714/1000 | Loss: 0.00002720
Iteration 715/1000 | Loss: 0.00002855
Iteration 716/1000 | Loss: 0.00002807
Iteration 717/1000 | Loss: 0.00002667
Iteration 718/1000 | Loss: 0.00002966
Iteration 719/1000 | Loss: 0.00002904
Iteration 720/1000 | Loss: 0.00002715
Iteration 721/1000 | Loss: 0.00002900
Iteration 722/1000 | Loss: 0.00003079
Iteration 723/1000 | Loss: 0.00003224
Iteration 724/1000 | Loss: 0.00003703
Iteration 725/1000 | Loss: 0.00003780
Iteration 726/1000 | Loss: 0.00002746
Iteration 727/1000 | Loss: 0.00002733
Iteration 728/1000 | Loss: 0.00003131
Iteration 729/1000 | Loss: 0.00003854
Iteration 730/1000 | Loss: 0.00003959
Iteration 731/1000 | Loss: 0.00004833
Iteration 732/1000 | Loss: 0.00004607
Iteration 733/1000 | Loss: 0.00004696
Iteration 734/1000 | Loss: 0.00004998
Iteration 735/1000 | Loss: 0.00005364
Iteration 736/1000 | Loss: 0.00004711
Iteration 737/1000 | Loss: 0.00005009
Iteration 738/1000 | Loss: 0.00005493
Iteration 739/1000 | Loss: 0.00004679
Iteration 740/1000 | Loss: 0.00005322
Iteration 741/1000 | Loss: 0.00005862
Iteration 742/1000 | Loss: 0.00005175
Iteration 743/1000 | Loss: 0.00003621
Iteration 744/1000 | Loss: 0.00004037
Iteration 745/1000 | Loss: 0.00004422
Iteration 746/1000 | Loss: 0.00003655
Iteration 747/1000 | Loss: 0.00003146
Iteration 748/1000 | Loss: 0.00003202
Iteration 749/1000 | Loss: 0.00003071
Iteration 750/1000 | Loss: 0.00003036
Iteration 751/1000 | Loss: 0.00003698
Iteration 752/1000 | Loss: 0.00003350
Iteration 753/1000 | Loss: 0.00002504
Iteration 754/1000 | Loss: 0.00002325
Iteration 755/1000 | Loss: 0.00002410
Iteration 756/1000 | Loss: 0.00002389
Iteration 757/1000 | Loss: 0.00003309
Iteration 758/1000 | Loss: 0.00002596
Iteration 759/1000 | Loss: 0.00002563
Iteration 760/1000 | Loss: 0.00002389
Iteration 761/1000 | Loss: 0.00003056
Iteration 762/1000 | Loss: 0.00002720
Iteration 763/1000 | Loss: 0.00003294
Iteration 764/1000 | Loss: 0.00002913
Iteration 765/1000 | Loss: 0.00005289
Iteration 766/1000 | Loss: 0.00003651
Iteration 767/1000 | Loss: 0.00005241
Iteration 768/1000 | Loss: 0.00004048
Iteration 769/1000 | Loss: 0.00005147
Iteration 770/1000 | Loss: 0.00004023
Iteration 771/1000 | Loss: 0.00005615
Iteration 772/1000 | Loss: 0.00004916
Iteration 773/1000 | Loss: 0.00007079
Iteration 774/1000 | Loss: 0.00005378
Iteration 775/1000 | Loss: 0.00008110
Iteration 776/1000 | Loss: 0.00003417
Iteration 777/1000 | Loss: 0.00003574
Iteration 778/1000 | Loss: 0.00003287
Iteration 779/1000 | Loss: 0.00002553
Iteration 780/1000 | Loss: 0.00003765
Iteration 781/1000 | Loss: 0.00003261
Iteration 782/1000 | Loss: 0.00002594
Iteration 783/1000 | Loss: 0.00003445
Iteration 784/1000 | Loss: 0.00002973
Iteration 785/1000 | Loss: 0.00004673
Iteration 786/1000 | Loss: 0.00004038
Iteration 787/1000 | Loss: 0.00006682
Iteration 788/1000 | Loss: 0.00004672
Iteration 789/1000 | Loss: 0.00003100
Iteration 790/1000 | Loss: 0.00003044
Iteration 791/1000 | Loss: 0.00002570
Iteration 792/1000 | Loss: 0.00003361
Iteration 793/1000 | Loss: 0.00002612
Iteration 794/1000 | Loss: 0.00002274
Iteration 795/1000 | Loss: 0.00002493
Iteration 796/1000 | Loss: 0.00002561
Iteration 797/1000 | Loss: 0.00002612
Iteration 798/1000 | Loss: 0.00003513
Iteration 799/1000 | Loss: 0.00002649
Iteration 800/1000 | Loss: 0.00002345
Iteration 801/1000 | Loss: 0.00002901
Iteration 802/1000 | Loss: 0.00002595
Iteration 803/1000 | Loss: 0.00003648
Iteration 804/1000 | Loss: 0.00003100
Iteration 805/1000 | Loss: 0.00003228
Iteration 806/1000 | Loss: 0.00002611
Iteration 807/1000 | Loss: 0.00002381
Iteration 808/1000 | Loss: 0.00002993
Iteration 809/1000 | Loss: 0.00002734
Iteration 810/1000 | Loss: 0.00002467
Iteration 811/1000 | Loss: 0.00002591
Iteration 812/1000 | Loss: 0.00002537
Iteration 813/1000 | Loss: 0.00002745
Iteration 814/1000 | Loss: 0.00003041
Iteration 815/1000 | Loss: 0.00003348
Iteration 816/1000 | Loss: 0.00003528
Iteration 817/1000 | Loss: 0.00002694
Iteration 818/1000 | Loss: 0.00002695
Iteration 819/1000 | Loss: 0.00002774
Iteration 820/1000 | Loss: 0.00003146
Iteration 821/1000 | Loss: 0.00002723
Iteration 822/1000 | Loss: 0.00002825
Iteration 823/1000 | Loss: 0.00003265
Iteration 824/1000 | Loss: 0.00003045
Iteration 825/1000 | Loss: 0.00002467
Iteration 826/1000 | Loss: 0.00002207
Iteration 827/1000 | Loss: 0.00002719
Iteration 828/1000 | Loss: 0.00003230
Iteration 829/1000 | Loss: 0.00003387
Iteration 830/1000 | Loss: 0.00003797
Iteration 831/1000 | Loss: 0.00002689
Iteration 832/1000 | Loss: 0.00002338
Iteration 833/1000 | Loss: 0.00002515
Iteration 834/1000 | Loss: 0.00002734
Iteration 835/1000 | Loss: 0.00002911
Iteration 836/1000 | Loss: 0.00003548
Iteration 837/1000 | Loss: 0.00003266
Iteration 838/1000 | Loss: 0.00003941
Iteration 839/1000 | Loss: 0.00003952
Iteration 840/1000 | Loss: 0.00004749
Iteration 841/1000 | Loss: 0.00004297
Iteration 842/1000 | Loss: 0.00004243
Iteration 843/1000 | Loss: 0.00004182
Iteration 844/1000 | Loss: 0.00005074
Iteration 845/1000 | Loss: 0.00003698
Iteration 846/1000 | Loss: 0.00002686
Iteration 847/1000 | Loss: 0.00003764
Iteration 848/1000 | Loss: 0.00003438
Iteration 849/1000 | Loss: 0.00002696
Iteration 850/1000 | Loss: 0.00002288
Iteration 851/1000 | Loss: 0.00002336
Iteration 852/1000 | Loss: 0.00003087
Iteration 853/1000 | Loss: 0.00003487
Iteration 854/1000 | Loss: 0.00004102
Iteration 855/1000 | Loss: 0.00003847
Iteration 856/1000 | Loss: 0.00004314
Iteration 857/1000 | Loss: 0.00004492
Iteration 858/1000 | Loss: 0.00004729
Iteration 859/1000 | Loss: 0.00005025
Iteration 860/1000 | Loss: 0.00003945
Iteration 861/1000 | Loss: 0.00004259
Iteration 862/1000 | Loss: 0.00004300
Iteration 863/1000 | Loss: 0.00003787
Iteration 864/1000 | Loss: 0.00003995
Iteration 865/1000 | Loss: 0.00002970
Iteration 866/1000 | Loss: 0.00002553
Iteration 867/1000 | Loss: 0.00003242
Iteration 868/1000 | Loss: 0.00003901
Iteration 869/1000 | Loss: 0.00003092
Iteration 870/1000 | Loss: 0.00002639
Iteration 871/1000 | Loss: 0.00003085
Iteration 872/1000 | Loss: 0.00003145
Iteration 873/1000 | Loss: 0.00002893
Iteration 874/1000 | Loss: 0.00003000
Iteration 875/1000 | Loss: 0.00003242
Iteration 876/1000 | Loss: 0.00003325
Iteration 877/1000 | Loss: 0.00002935
Iteration 878/1000 | Loss: 0.00002666
Iteration 879/1000 | Loss: 0.00002850
Iteration 880/1000 | Loss: 0.00003330
Iteration 881/1000 | Loss: 0.00003786
Iteration 882/1000 | Loss: 0.00002600
Iteration 883/1000 | Loss: 0.00002761
Iteration 884/1000 | Loss: 0.00003540
Iteration 885/1000 | Loss: 0.00003654
Iteration 886/1000 | Loss: 0.00002696
Iteration 887/1000 | Loss: 0.00003395
Iteration 888/1000 | Loss: 0.00003689
Iteration 889/1000 | Loss: 0.00003300
Iteration 890/1000 | Loss: 0.00003050
Iteration 891/1000 | Loss: 0.00002999
Iteration 892/1000 | Loss: 0.00004099
Iteration 893/1000 | Loss: 0.00003674
Iteration 894/1000 | Loss: 0.00003581
Iteration 895/1000 | Loss: 0.00003217
Iteration 896/1000 | Loss: 0.00002981
Iteration 897/1000 | Loss: 0.00002972
Iteration 898/1000 | Loss: 0.00003041
Iteration 899/1000 | Loss: 0.00003257
Iteration 900/1000 | Loss: 0.00004419
Iteration 901/1000 | Loss: 0.00003422
Iteration 902/1000 | Loss: 0.00003059
Iteration 903/1000 | Loss: 0.00003032
Iteration 904/1000 | Loss: 0.00003771
Iteration 905/1000 | Loss: 0.00003871
Iteration 906/1000 | Loss: 0.00004844
Iteration 907/1000 | Loss: 0.00004733
Iteration 908/1000 | Loss: 0.00004909
Iteration 909/1000 | Loss: 0.00004780
Iteration 910/1000 | Loss: 0.00004997
Iteration 911/1000 | Loss: 0.00005684
Iteration 912/1000 | Loss: 0.00005058
Iteration 913/1000 | Loss: 0.00005796
Iteration 914/1000 | Loss: 0.00004965
Iteration 915/1000 | Loss: 0.00005395
Iteration 916/1000 | Loss: 0.00003729
Iteration 917/1000 | Loss: 0.00003123
Iteration 918/1000 | Loss: 0.00004883
Iteration 919/1000 | Loss: 0.00005843
Iteration 920/1000 | Loss: 0.00005798
Iteration 921/1000 | Loss: 0.00004607
Iteration 922/1000 | Loss: 0.00004585
Iteration 923/1000 | Loss: 0.00002983
Iteration 924/1000 | Loss: 0.00003431
Iteration 925/1000 | Loss: 0.00005040
Iteration 926/1000 | Loss: 0.00005201
Iteration 927/1000 | Loss: 0.00004779
Iteration 928/1000 | Loss: 0.00003696
Iteration 929/1000 | Loss: 0.00002442
Iteration 930/1000 | Loss: 0.00004325
Iteration 931/1000 | Loss: 0.00004488
Iteration 932/1000 | Loss: 0.00005170
Iteration 933/1000 | Loss: 0.00006920
Iteration 934/1000 | Loss: 0.00006222
Iteration 935/1000 | Loss: 0.00004736
Iteration 936/1000 | Loss: 0.00003229
Iteration 937/1000 | Loss: 0.00002539
Iteration 938/1000 | Loss: 0.00002415
Iteration 939/1000 | Loss: 0.00003313
Iteration 940/1000 | Loss: 0.00003987
Iteration 941/1000 | Loss: 0.00003109
Iteration 942/1000 | Loss: 0.00002833
Iteration 943/1000 | Loss: 0.00002625
Iteration 944/1000 | Loss: 0.00002909
Iteration 945/1000 | Loss: 0.00002902
Iteration 946/1000 | Loss: 0.00002481
Iteration 947/1000 | Loss: 0.00003014
Iteration 948/1000 | Loss: 0.00002875
Iteration 949/1000 | Loss: 0.00003501
Iteration 950/1000 | Loss: 0.00003115
Iteration 951/1000 | Loss: 0.00002483
Iteration 952/1000 | Loss: 0.00002481
Iteration 953/1000 | Loss: 0.00002817
Iteration 954/1000 | Loss: 0.00002765
Iteration 955/1000 | Loss: 0.00002822
Iteration 956/1000 | Loss: 0.00003353
Iteration 957/1000 | Loss: 0.00003454
Iteration 958/1000 | Loss: 0.00003964
Iteration 959/1000 | Loss: 0.00003731
Iteration 960/1000 | Loss: 0.00004133
Iteration 961/1000 | Loss: 0.00003785
Iteration 962/1000 | Loss: 0.00004109
Iteration 963/1000 | Loss: 0.00004051
Iteration 964/1000 | Loss: 0.00004834
Iteration 965/1000 | Loss: 0.00004291
Iteration 966/1000 | Loss: 0.00004550
Iteration 967/1000 | Loss: 0.00004481
Iteration 968/1000 | Loss: 0.00005445
Iteration 969/1000 | Loss: 0.00005580
Iteration 970/1000 | Loss: 0.00004242
Iteration 971/1000 | Loss: 0.00002581
Iteration 972/1000 | Loss: 0.00002519
Iteration 973/1000 | Loss: 0.00003147
Iteration 974/1000 | Loss: 0.00003418
Iteration 975/1000 | Loss: 0.00003598
Iteration 976/1000 | Loss: 0.00002841
Iteration 977/1000 | Loss: 0.00002547
Iteration 978/1000 | Loss: 0.00003468
Iteration 979/1000 | Loss: 0.00003921
Iteration 980/1000 | Loss: 0.00003353
Iteration 981/1000 | Loss: 0.00004295
Iteration 982/1000 | Loss: 0.00003809
Iteration 983/1000 | Loss: 0.00002906
Iteration 984/1000 | Loss: 0.00002535
Iteration 985/1000 | Loss: 0.00003318
Iteration 986/1000 | Loss: 0.00003552
Iteration 987/1000 | Loss: 0.00002805
Iteration 988/1000 | Loss: 0.00003383
Iteration 989/1000 | Loss: 0.00002805
Iteration 990/1000 | Loss: 0.00003023
Iteration 991/1000 | Loss: 0.00003329
Iteration 992/1000 | Loss: 0.00003832
Iteration 993/1000 | Loss: 0.00003928
Iteration 994/1000 | Loss: 0.00003502
Iteration 995/1000 | Loss: 0.00002753
Iteration 996/1000 | Loss: 0.00003817
Iteration 997/1000 | Loss: 0.00004355
Iteration 998/1000 | Loss: 0.00005500
Iteration 999/1000 | Loss: 0.00005235
Iteration 1000/1000 | Loss: 0.00006240

Optimization complete. Final v2v error: 4.488229751586914 mm

Highest mean error: 11.444621086120605 mm for frame 128

Lowest mean error: 2.759695291519165 mm for frame 45

Saving results

Total time: 1422.4835000038147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026627
Iteration 2/25 | Loss: 0.00251250
Iteration 3/25 | Loss: 0.00168707
Iteration 4/25 | Loss: 0.00146006
Iteration 5/25 | Loss: 0.00132598
Iteration 6/25 | Loss: 0.00123294
Iteration 7/25 | Loss: 0.00115881
Iteration 8/25 | Loss: 0.00110155
Iteration 9/25 | Loss: 0.00109088
Iteration 10/25 | Loss: 0.00108928
Iteration 11/25 | Loss: 0.00107863
Iteration 12/25 | Loss: 0.00094809
Iteration 13/25 | Loss: 0.00089311
Iteration 14/25 | Loss: 0.00086193
Iteration 15/25 | Loss: 0.00085656
Iteration 16/25 | Loss: 0.00084871
Iteration 17/25 | Loss: 0.00084258
Iteration 18/25 | Loss: 0.00083920
Iteration 19/25 | Loss: 0.00084650
Iteration 20/25 | Loss: 0.00084552
Iteration 21/25 | Loss: 0.00083499
Iteration 22/25 | Loss: 0.00082998
Iteration 23/25 | Loss: 0.00082814
Iteration 24/25 | Loss: 0.00082729
Iteration 25/25 | Loss: 0.00082713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46201909
Iteration 2/25 | Loss: 0.00111460
Iteration 3/25 | Loss: 0.00086393
Iteration 4/25 | Loss: 0.00086393
Iteration 5/25 | Loss: 0.00086393
Iteration 6/25 | Loss: 0.00086393
Iteration 7/25 | Loss: 0.00086393
Iteration 8/25 | Loss: 0.00086393
Iteration 9/25 | Loss: 0.00086393
Iteration 10/25 | Loss: 0.00086393
Iteration 11/25 | Loss: 0.00086393
Iteration 12/25 | Loss: 0.00086393
Iteration 13/25 | Loss: 0.00086393
Iteration 14/25 | Loss: 0.00086393
Iteration 15/25 | Loss: 0.00086393
Iteration 16/25 | Loss: 0.00086393
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008639284060336649, 0.0008639284060336649, 0.0008639284060336649, 0.0008639284060336649, 0.0008639284060336649]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008639284060336649

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086393
Iteration 2/1000 | Loss: 0.00037696
Iteration 3/1000 | Loss: 0.00067078
Iteration 4/1000 | Loss: 0.00007202
Iteration 5/1000 | Loss: 0.00014519
Iteration 6/1000 | Loss: 0.00022604
Iteration 7/1000 | Loss: 0.00067780
Iteration 8/1000 | Loss: 0.00019424
Iteration 9/1000 | Loss: 0.00007129
Iteration 10/1000 | Loss: 0.00005834
Iteration 11/1000 | Loss: 0.00032000
Iteration 12/1000 | Loss: 0.00023337
Iteration 13/1000 | Loss: 0.00038362
Iteration 14/1000 | Loss: 0.00036236
Iteration 15/1000 | Loss: 0.00013918
Iteration 16/1000 | Loss: 0.00009073
Iteration 17/1000 | Loss: 0.00005734
Iteration 18/1000 | Loss: 0.00005175
Iteration 19/1000 | Loss: 0.00029233
Iteration 20/1000 | Loss: 0.00057068
Iteration 21/1000 | Loss: 0.00034669
Iteration 22/1000 | Loss: 0.00013566
Iteration 23/1000 | Loss: 0.00012124
Iteration 24/1000 | Loss: 0.00007212
Iteration 25/1000 | Loss: 0.00005711
Iteration 26/1000 | Loss: 0.00005892
Iteration 27/1000 | Loss: 0.00059375
Iteration 28/1000 | Loss: 0.00008697
Iteration 29/1000 | Loss: 0.00029657
Iteration 30/1000 | Loss: 0.00023050
Iteration 31/1000 | Loss: 0.00014564
Iteration 32/1000 | Loss: 0.00019953
Iteration 33/1000 | Loss: 0.00013213
Iteration 34/1000 | Loss: 0.00006022
Iteration 35/1000 | Loss: 0.00005450
Iteration 36/1000 | Loss: 0.00029421
Iteration 37/1000 | Loss: 0.00007402
Iteration 38/1000 | Loss: 0.00004772
Iteration 39/1000 | Loss: 0.00004552
Iteration 40/1000 | Loss: 0.00004380
Iteration 41/1000 | Loss: 0.00026275
Iteration 42/1000 | Loss: 0.00005076
Iteration 43/1000 | Loss: 0.00004429
Iteration 44/1000 | Loss: 0.00004232
Iteration 45/1000 | Loss: 0.00005151
Iteration 46/1000 | Loss: 0.00004612
Iteration 47/1000 | Loss: 0.00014969
Iteration 48/1000 | Loss: 0.00004107
Iteration 49/1000 | Loss: 0.00003900
Iteration 50/1000 | Loss: 0.00003753
Iteration 51/1000 | Loss: 0.00023102
Iteration 52/1000 | Loss: 0.00004612
Iteration 53/1000 | Loss: 0.00003998
Iteration 54/1000 | Loss: 0.00003746
Iteration 55/1000 | Loss: 0.00003518
Iteration 56/1000 | Loss: 0.00003351
Iteration 57/1000 | Loss: 0.00003291
Iteration 58/1000 | Loss: 0.00003242
Iteration 59/1000 | Loss: 0.00003212
Iteration 60/1000 | Loss: 0.00003204
Iteration 61/1000 | Loss: 0.00003201
Iteration 62/1000 | Loss: 0.00003185
Iteration 63/1000 | Loss: 0.00003181
Iteration 64/1000 | Loss: 0.00003175
Iteration 65/1000 | Loss: 0.00003174
Iteration 66/1000 | Loss: 0.00003165
Iteration 67/1000 | Loss: 0.00003165
Iteration 68/1000 | Loss: 0.00003164
Iteration 69/1000 | Loss: 0.00003164
Iteration 70/1000 | Loss: 0.00003163
Iteration 71/1000 | Loss: 0.00003163
Iteration 72/1000 | Loss: 0.00003157
Iteration 73/1000 | Loss: 0.00003154
Iteration 74/1000 | Loss: 0.00003154
Iteration 75/1000 | Loss: 0.00003154
Iteration 76/1000 | Loss: 0.00003154
Iteration 77/1000 | Loss: 0.00003154
Iteration 78/1000 | Loss: 0.00003153
Iteration 79/1000 | Loss: 0.00003153
Iteration 80/1000 | Loss: 0.00003153
Iteration 81/1000 | Loss: 0.00003152
Iteration 82/1000 | Loss: 0.00003152
Iteration 83/1000 | Loss: 0.00003151
Iteration 84/1000 | Loss: 0.00003149
Iteration 85/1000 | Loss: 0.00003148
Iteration 86/1000 | Loss: 0.00003148
Iteration 87/1000 | Loss: 0.00003148
Iteration 88/1000 | Loss: 0.00003148
Iteration 89/1000 | Loss: 0.00003148
Iteration 90/1000 | Loss: 0.00003147
Iteration 91/1000 | Loss: 0.00003147
Iteration 92/1000 | Loss: 0.00003147
Iteration 93/1000 | Loss: 0.00003146
Iteration 94/1000 | Loss: 0.00003146
Iteration 95/1000 | Loss: 0.00003146
Iteration 96/1000 | Loss: 0.00003146
Iteration 97/1000 | Loss: 0.00003145
Iteration 98/1000 | Loss: 0.00003145
Iteration 99/1000 | Loss: 0.00003145
Iteration 100/1000 | Loss: 0.00003145
Iteration 101/1000 | Loss: 0.00003145
Iteration 102/1000 | Loss: 0.00003145
Iteration 103/1000 | Loss: 0.00003145
Iteration 104/1000 | Loss: 0.00003145
Iteration 105/1000 | Loss: 0.00003145
Iteration 106/1000 | Loss: 0.00003145
Iteration 107/1000 | Loss: 0.00003145
Iteration 108/1000 | Loss: 0.00003145
Iteration 109/1000 | Loss: 0.00003145
Iteration 110/1000 | Loss: 0.00003145
Iteration 111/1000 | Loss: 0.00003145
Iteration 112/1000 | Loss: 0.00003144
Iteration 113/1000 | Loss: 0.00003144
Iteration 114/1000 | Loss: 0.00003144
Iteration 115/1000 | Loss: 0.00003144
Iteration 116/1000 | Loss: 0.00003144
Iteration 117/1000 | Loss: 0.00003143
Iteration 118/1000 | Loss: 0.00003143
Iteration 119/1000 | Loss: 0.00003143
Iteration 120/1000 | Loss: 0.00003143
Iteration 121/1000 | Loss: 0.00003143
Iteration 122/1000 | Loss: 0.00003143
Iteration 123/1000 | Loss: 0.00003143
Iteration 124/1000 | Loss: 0.00003143
Iteration 125/1000 | Loss: 0.00003143
Iteration 126/1000 | Loss: 0.00003143
Iteration 127/1000 | Loss: 0.00003143
Iteration 128/1000 | Loss: 0.00003143
Iteration 129/1000 | Loss: 0.00003143
Iteration 130/1000 | Loss: 0.00003143
Iteration 131/1000 | Loss: 0.00003142
Iteration 132/1000 | Loss: 0.00003142
Iteration 133/1000 | Loss: 0.00003142
Iteration 134/1000 | Loss: 0.00003142
Iteration 135/1000 | Loss: 0.00003142
Iteration 136/1000 | Loss: 0.00003141
Iteration 137/1000 | Loss: 0.00003141
Iteration 138/1000 | Loss: 0.00003141
Iteration 139/1000 | Loss: 0.00003140
Iteration 140/1000 | Loss: 0.00003140
Iteration 141/1000 | Loss: 0.00003140
Iteration 142/1000 | Loss: 0.00003139
Iteration 143/1000 | Loss: 0.00003139
Iteration 144/1000 | Loss: 0.00003139
Iteration 145/1000 | Loss: 0.00003139
Iteration 146/1000 | Loss: 0.00003139
Iteration 147/1000 | Loss: 0.00003139
Iteration 148/1000 | Loss: 0.00003139
Iteration 149/1000 | Loss: 0.00003139
Iteration 150/1000 | Loss: 0.00003139
Iteration 151/1000 | Loss: 0.00003139
Iteration 152/1000 | Loss: 0.00003139
Iteration 153/1000 | Loss: 0.00003139
Iteration 154/1000 | Loss: 0.00003138
Iteration 155/1000 | Loss: 0.00003138
Iteration 156/1000 | Loss: 0.00003138
Iteration 157/1000 | Loss: 0.00003138
Iteration 158/1000 | Loss: 0.00003138
Iteration 159/1000 | Loss: 0.00003138
Iteration 160/1000 | Loss: 0.00003138
Iteration 161/1000 | Loss: 0.00003138
Iteration 162/1000 | Loss: 0.00003138
Iteration 163/1000 | Loss: 0.00003138
Iteration 164/1000 | Loss: 0.00003138
Iteration 165/1000 | Loss: 0.00003138
Iteration 166/1000 | Loss: 0.00003137
Iteration 167/1000 | Loss: 0.00003137
Iteration 168/1000 | Loss: 0.00003137
Iteration 169/1000 | Loss: 0.00003137
Iteration 170/1000 | Loss: 0.00003137
Iteration 171/1000 | Loss: 0.00003137
Iteration 172/1000 | Loss: 0.00003137
Iteration 173/1000 | Loss: 0.00003137
Iteration 174/1000 | Loss: 0.00003137
Iteration 175/1000 | Loss: 0.00003137
Iteration 176/1000 | Loss: 0.00003137
Iteration 177/1000 | Loss: 0.00003137
Iteration 178/1000 | Loss: 0.00003137
Iteration 179/1000 | Loss: 0.00003137
Iteration 180/1000 | Loss: 0.00003137
Iteration 181/1000 | Loss: 0.00003137
Iteration 182/1000 | Loss: 0.00003137
Iteration 183/1000 | Loss: 0.00003137
Iteration 184/1000 | Loss: 0.00003137
Iteration 185/1000 | Loss: 0.00003137
Iteration 186/1000 | Loss: 0.00003137
Iteration 187/1000 | Loss: 0.00003137
Iteration 188/1000 | Loss: 0.00003137
Iteration 189/1000 | Loss: 0.00003137
Iteration 190/1000 | Loss: 0.00003136
Iteration 191/1000 | Loss: 0.00003136
Iteration 192/1000 | Loss: 0.00003136
Iteration 193/1000 | Loss: 0.00003136
Iteration 194/1000 | Loss: 0.00003136
Iteration 195/1000 | Loss: 0.00003136
Iteration 196/1000 | Loss: 0.00003136
Iteration 197/1000 | Loss: 0.00003136
Iteration 198/1000 | Loss: 0.00003136
Iteration 199/1000 | Loss: 0.00003136
Iteration 200/1000 | Loss: 0.00003136
Iteration 201/1000 | Loss: 0.00003136
Iteration 202/1000 | Loss: 0.00003136
Iteration 203/1000 | Loss: 0.00003136
Iteration 204/1000 | Loss: 0.00003136
Iteration 205/1000 | Loss: 0.00003136
Iteration 206/1000 | Loss: 0.00003136
Iteration 207/1000 | Loss: 0.00003136
Iteration 208/1000 | Loss: 0.00003136
Iteration 209/1000 | Loss: 0.00003136
Iteration 210/1000 | Loss: 0.00003136
Iteration 211/1000 | Loss: 0.00003136
Iteration 212/1000 | Loss: 0.00003136
Iteration 213/1000 | Loss: 0.00003136
Iteration 214/1000 | Loss: 0.00003136
Iteration 215/1000 | Loss: 0.00003136
Iteration 216/1000 | Loss: 0.00003136
Iteration 217/1000 | Loss: 0.00003136
Iteration 218/1000 | Loss: 0.00003136
Iteration 219/1000 | Loss: 0.00003136
Iteration 220/1000 | Loss: 0.00003136
Iteration 221/1000 | Loss: 0.00003136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [3.1356590625364333e-05, 3.1356590625364333e-05, 3.1356590625364333e-05, 3.1356590625364333e-05, 3.1356590625364333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1356590625364333e-05

Optimization complete. Final v2v error: 3.88934063911438 mm

Highest mean error: 10.204513549804688 mm for frame 123

Lowest mean error: 2.8984572887420654 mm for frame 149

Saving results

Total time: 141.0964479446411
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491646
Iteration 2/25 | Loss: 0.00088980
Iteration 3/25 | Loss: 0.00074036
Iteration 4/25 | Loss: 0.00070197
Iteration 5/25 | Loss: 0.00069179
Iteration 6/25 | Loss: 0.00069066
Iteration 7/25 | Loss: 0.00069066
Iteration 8/25 | Loss: 0.00069066
Iteration 9/25 | Loss: 0.00069066
Iteration 10/25 | Loss: 0.00069066
Iteration 11/25 | Loss: 0.00069066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006906558410264552, 0.0006906558410264552, 0.0006906558410264552, 0.0006906558410264552, 0.0006906558410264552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006906558410264552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41677284
Iteration 2/25 | Loss: 0.00023181
Iteration 3/25 | Loss: 0.00023178
Iteration 4/25 | Loss: 0.00023178
Iteration 5/25 | Loss: 0.00023178
Iteration 6/25 | Loss: 0.00023178
Iteration 7/25 | Loss: 0.00023178
Iteration 8/25 | Loss: 0.00023178
Iteration 9/25 | Loss: 0.00023178
Iteration 10/25 | Loss: 0.00023178
Iteration 11/25 | Loss: 0.00023178
Iteration 12/25 | Loss: 0.00023178
Iteration 13/25 | Loss: 0.00023178
Iteration 14/25 | Loss: 0.00023178
Iteration 15/25 | Loss: 0.00023178
Iteration 16/25 | Loss: 0.00023178
Iteration 17/25 | Loss: 0.00023178
Iteration 18/25 | Loss: 0.00023178
Iteration 19/25 | Loss: 0.00023178
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0002317759790457785, 0.0002317759790457785, 0.0002317759790457785, 0.0002317759790457785, 0.0002317759790457785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002317759790457785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023178
Iteration 2/1000 | Loss: 0.00004433
Iteration 3/1000 | Loss: 0.00003069
Iteration 4/1000 | Loss: 0.00002746
Iteration 5/1000 | Loss: 0.00002572
Iteration 6/1000 | Loss: 0.00002461
Iteration 7/1000 | Loss: 0.00002389
Iteration 8/1000 | Loss: 0.00002325
Iteration 9/1000 | Loss: 0.00002293
Iteration 10/1000 | Loss: 0.00002274
Iteration 11/1000 | Loss: 0.00002254
Iteration 12/1000 | Loss: 0.00002240
Iteration 13/1000 | Loss: 0.00002239
Iteration 14/1000 | Loss: 0.00002232
Iteration 15/1000 | Loss: 0.00002224
Iteration 16/1000 | Loss: 0.00002224
Iteration 17/1000 | Loss: 0.00002224
Iteration 18/1000 | Loss: 0.00002223
Iteration 19/1000 | Loss: 0.00002223
Iteration 20/1000 | Loss: 0.00002223
Iteration 21/1000 | Loss: 0.00002223
Iteration 22/1000 | Loss: 0.00002223
Iteration 23/1000 | Loss: 0.00002220
Iteration 24/1000 | Loss: 0.00002220
Iteration 25/1000 | Loss: 0.00002219
Iteration 26/1000 | Loss: 0.00002219
Iteration 27/1000 | Loss: 0.00002218
Iteration 28/1000 | Loss: 0.00002218
Iteration 29/1000 | Loss: 0.00002217
Iteration 30/1000 | Loss: 0.00002216
Iteration 31/1000 | Loss: 0.00002215
Iteration 32/1000 | Loss: 0.00002214
Iteration 33/1000 | Loss: 0.00002214
Iteration 34/1000 | Loss: 0.00002214
Iteration 35/1000 | Loss: 0.00002214
Iteration 36/1000 | Loss: 0.00002214
Iteration 37/1000 | Loss: 0.00002214
Iteration 38/1000 | Loss: 0.00002214
Iteration 39/1000 | Loss: 0.00002214
Iteration 40/1000 | Loss: 0.00002213
Iteration 41/1000 | Loss: 0.00002212
Iteration 42/1000 | Loss: 0.00002211
Iteration 43/1000 | Loss: 0.00002211
Iteration 44/1000 | Loss: 0.00002210
Iteration 45/1000 | Loss: 0.00002210
Iteration 46/1000 | Loss: 0.00002209
Iteration 47/1000 | Loss: 0.00002208
Iteration 48/1000 | Loss: 0.00002208
Iteration 49/1000 | Loss: 0.00002208
Iteration 50/1000 | Loss: 0.00002207
Iteration 51/1000 | Loss: 0.00002206
Iteration 52/1000 | Loss: 0.00002206
Iteration 53/1000 | Loss: 0.00002206
Iteration 54/1000 | Loss: 0.00002205
Iteration 55/1000 | Loss: 0.00002205
Iteration 56/1000 | Loss: 0.00002204
Iteration 57/1000 | Loss: 0.00002203
Iteration 58/1000 | Loss: 0.00002202
Iteration 59/1000 | Loss: 0.00002202
Iteration 60/1000 | Loss: 0.00002202
Iteration 61/1000 | Loss: 0.00002202
Iteration 62/1000 | Loss: 0.00002202
Iteration 63/1000 | Loss: 0.00002201
Iteration 64/1000 | Loss: 0.00002201
Iteration 65/1000 | Loss: 0.00002201
Iteration 66/1000 | Loss: 0.00002201
Iteration 67/1000 | Loss: 0.00002201
Iteration 68/1000 | Loss: 0.00002201
Iteration 69/1000 | Loss: 0.00002201
Iteration 70/1000 | Loss: 0.00002200
Iteration 71/1000 | Loss: 0.00002200
Iteration 72/1000 | Loss: 0.00002200
Iteration 73/1000 | Loss: 0.00002200
Iteration 74/1000 | Loss: 0.00002200
Iteration 75/1000 | Loss: 0.00002200
Iteration 76/1000 | Loss: 0.00002199
Iteration 77/1000 | Loss: 0.00002199
Iteration 78/1000 | Loss: 0.00002199
Iteration 79/1000 | Loss: 0.00002199
Iteration 80/1000 | Loss: 0.00002199
Iteration 81/1000 | Loss: 0.00002198
Iteration 82/1000 | Loss: 0.00002198
Iteration 83/1000 | Loss: 0.00002198
Iteration 84/1000 | Loss: 0.00002198
Iteration 85/1000 | Loss: 0.00002198
Iteration 86/1000 | Loss: 0.00002198
Iteration 87/1000 | Loss: 0.00002198
Iteration 88/1000 | Loss: 0.00002198
Iteration 89/1000 | Loss: 0.00002198
Iteration 90/1000 | Loss: 0.00002197
Iteration 91/1000 | Loss: 0.00002197
Iteration 92/1000 | Loss: 0.00002197
Iteration 93/1000 | Loss: 0.00002196
Iteration 94/1000 | Loss: 0.00002196
Iteration 95/1000 | Loss: 0.00002196
Iteration 96/1000 | Loss: 0.00002196
Iteration 97/1000 | Loss: 0.00002196
Iteration 98/1000 | Loss: 0.00002196
Iteration 99/1000 | Loss: 0.00002196
Iteration 100/1000 | Loss: 0.00002196
Iteration 101/1000 | Loss: 0.00002195
Iteration 102/1000 | Loss: 0.00002195
Iteration 103/1000 | Loss: 0.00002195
Iteration 104/1000 | Loss: 0.00002195
Iteration 105/1000 | Loss: 0.00002194
Iteration 106/1000 | Loss: 0.00002194
Iteration 107/1000 | Loss: 0.00002194
Iteration 108/1000 | Loss: 0.00002194
Iteration 109/1000 | Loss: 0.00002194
Iteration 110/1000 | Loss: 0.00002194
Iteration 111/1000 | Loss: 0.00002194
Iteration 112/1000 | Loss: 0.00002194
Iteration 113/1000 | Loss: 0.00002193
Iteration 114/1000 | Loss: 0.00002193
Iteration 115/1000 | Loss: 0.00002193
Iteration 116/1000 | Loss: 0.00002193
Iteration 117/1000 | Loss: 0.00002193
Iteration 118/1000 | Loss: 0.00002192
Iteration 119/1000 | Loss: 0.00002192
Iteration 120/1000 | Loss: 0.00002192
Iteration 121/1000 | Loss: 0.00002192
Iteration 122/1000 | Loss: 0.00002192
Iteration 123/1000 | Loss: 0.00002192
Iteration 124/1000 | Loss: 0.00002192
Iteration 125/1000 | Loss: 0.00002192
Iteration 126/1000 | Loss: 0.00002192
Iteration 127/1000 | Loss: 0.00002191
Iteration 128/1000 | Loss: 0.00002191
Iteration 129/1000 | Loss: 0.00002191
Iteration 130/1000 | Loss: 0.00002191
Iteration 131/1000 | Loss: 0.00002191
Iteration 132/1000 | Loss: 0.00002191
Iteration 133/1000 | Loss: 0.00002191
Iteration 134/1000 | Loss: 0.00002191
Iteration 135/1000 | Loss: 0.00002191
Iteration 136/1000 | Loss: 0.00002190
Iteration 137/1000 | Loss: 0.00002190
Iteration 138/1000 | Loss: 0.00002190
Iteration 139/1000 | Loss: 0.00002189
Iteration 140/1000 | Loss: 0.00002189
Iteration 141/1000 | Loss: 0.00002189
Iteration 142/1000 | Loss: 0.00002189
Iteration 143/1000 | Loss: 0.00002189
Iteration 144/1000 | Loss: 0.00002189
Iteration 145/1000 | Loss: 0.00002188
Iteration 146/1000 | Loss: 0.00002188
Iteration 147/1000 | Loss: 0.00002188
Iteration 148/1000 | Loss: 0.00002188
Iteration 149/1000 | Loss: 0.00002188
Iteration 150/1000 | Loss: 0.00002188
Iteration 151/1000 | Loss: 0.00002188
Iteration 152/1000 | Loss: 0.00002188
Iteration 153/1000 | Loss: 0.00002188
Iteration 154/1000 | Loss: 0.00002188
Iteration 155/1000 | Loss: 0.00002188
Iteration 156/1000 | Loss: 0.00002188
Iteration 157/1000 | Loss: 0.00002187
Iteration 158/1000 | Loss: 0.00002187
Iteration 159/1000 | Loss: 0.00002187
Iteration 160/1000 | Loss: 0.00002187
Iteration 161/1000 | Loss: 0.00002187
Iteration 162/1000 | Loss: 0.00002187
Iteration 163/1000 | Loss: 0.00002187
Iteration 164/1000 | Loss: 0.00002187
Iteration 165/1000 | Loss: 0.00002187
Iteration 166/1000 | Loss: 0.00002187
Iteration 167/1000 | Loss: 0.00002187
Iteration 168/1000 | Loss: 0.00002186
Iteration 169/1000 | Loss: 0.00002186
Iteration 170/1000 | Loss: 0.00002186
Iteration 171/1000 | Loss: 0.00002186
Iteration 172/1000 | Loss: 0.00002186
Iteration 173/1000 | Loss: 0.00002186
Iteration 174/1000 | Loss: 0.00002186
Iteration 175/1000 | Loss: 0.00002186
Iteration 176/1000 | Loss: 0.00002186
Iteration 177/1000 | Loss: 0.00002186
Iteration 178/1000 | Loss: 0.00002186
Iteration 179/1000 | Loss: 0.00002186
Iteration 180/1000 | Loss: 0.00002186
Iteration 181/1000 | Loss: 0.00002186
Iteration 182/1000 | Loss: 0.00002186
Iteration 183/1000 | Loss: 0.00002186
Iteration 184/1000 | Loss: 0.00002186
Iteration 185/1000 | Loss: 0.00002186
Iteration 186/1000 | Loss: 0.00002186
Iteration 187/1000 | Loss: 0.00002186
Iteration 188/1000 | Loss: 0.00002186
Iteration 189/1000 | Loss: 0.00002186
Iteration 190/1000 | Loss: 0.00002186
Iteration 191/1000 | Loss: 0.00002186
Iteration 192/1000 | Loss: 0.00002186
Iteration 193/1000 | Loss: 0.00002186
Iteration 194/1000 | Loss: 0.00002186
Iteration 195/1000 | Loss: 0.00002186
Iteration 196/1000 | Loss: 0.00002186
Iteration 197/1000 | Loss: 0.00002186
Iteration 198/1000 | Loss: 0.00002186
Iteration 199/1000 | Loss: 0.00002186
Iteration 200/1000 | Loss: 0.00002186
Iteration 201/1000 | Loss: 0.00002186
Iteration 202/1000 | Loss: 0.00002186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [2.18563036469277e-05, 2.18563036469277e-05, 2.18563036469277e-05, 2.18563036469277e-05, 2.18563036469277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.18563036469277e-05

Optimization complete. Final v2v error: 3.873755931854248 mm

Highest mean error: 4.298001289367676 mm for frame 103

Lowest mean error: 3.277992010116577 mm for frame 17

Saving results

Total time: 39.50076460838318
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391109
Iteration 2/25 | Loss: 0.00084734
Iteration 3/25 | Loss: 0.00065497
Iteration 4/25 | Loss: 0.00062427
Iteration 5/25 | Loss: 0.00061370
Iteration 6/25 | Loss: 0.00061059
Iteration 7/25 | Loss: 0.00060969
Iteration 8/25 | Loss: 0.00060965
Iteration 9/25 | Loss: 0.00060965
Iteration 10/25 | Loss: 0.00060965
Iteration 11/25 | Loss: 0.00060965
Iteration 12/25 | Loss: 0.00060965
Iteration 13/25 | Loss: 0.00060965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006096547585912049, 0.0006096547585912049, 0.0006096547585912049, 0.0006096547585912049, 0.0006096547585912049]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006096547585912049

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34739769
Iteration 2/25 | Loss: 0.00028350
Iteration 3/25 | Loss: 0.00028347
Iteration 4/25 | Loss: 0.00028347
Iteration 5/25 | Loss: 0.00028347
Iteration 6/25 | Loss: 0.00028347
Iteration 7/25 | Loss: 0.00028347
Iteration 8/25 | Loss: 0.00028347
Iteration 9/25 | Loss: 0.00028347
Iteration 10/25 | Loss: 0.00028347
Iteration 11/25 | Loss: 0.00028347
Iteration 12/25 | Loss: 0.00028347
Iteration 13/25 | Loss: 0.00028347
Iteration 14/25 | Loss: 0.00028347
Iteration 15/25 | Loss: 0.00028347
Iteration 16/25 | Loss: 0.00028347
Iteration 17/25 | Loss: 0.00028347
Iteration 18/25 | Loss: 0.00028347
Iteration 19/25 | Loss: 0.00028347
Iteration 20/25 | Loss: 0.00028347
Iteration 21/25 | Loss: 0.00028347
Iteration 22/25 | Loss: 0.00028347
Iteration 23/25 | Loss: 0.00028347
Iteration 24/25 | Loss: 0.00028347
Iteration 25/25 | Loss: 0.00028347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028347
Iteration 2/1000 | Loss: 0.00002884
Iteration 3/1000 | Loss: 0.00002130
Iteration 4/1000 | Loss: 0.00001659
Iteration 5/1000 | Loss: 0.00001565
Iteration 6/1000 | Loss: 0.00001499
Iteration 7/1000 | Loss: 0.00001448
Iteration 8/1000 | Loss: 0.00001445
Iteration 9/1000 | Loss: 0.00001420
Iteration 10/1000 | Loss: 0.00001394
Iteration 11/1000 | Loss: 0.00001387
Iteration 12/1000 | Loss: 0.00001386
Iteration 13/1000 | Loss: 0.00001385
Iteration 14/1000 | Loss: 0.00001385
Iteration 15/1000 | Loss: 0.00001384
Iteration 16/1000 | Loss: 0.00001381
Iteration 17/1000 | Loss: 0.00001380
Iteration 18/1000 | Loss: 0.00001379
Iteration 19/1000 | Loss: 0.00001379
Iteration 20/1000 | Loss: 0.00001379
Iteration 21/1000 | Loss: 0.00001378
Iteration 22/1000 | Loss: 0.00001378
Iteration 23/1000 | Loss: 0.00001378
Iteration 24/1000 | Loss: 0.00001377
Iteration 25/1000 | Loss: 0.00001377
Iteration 26/1000 | Loss: 0.00001376
Iteration 27/1000 | Loss: 0.00001375
Iteration 28/1000 | Loss: 0.00001371
Iteration 29/1000 | Loss: 0.00001371
Iteration 30/1000 | Loss: 0.00001370
Iteration 31/1000 | Loss: 0.00001370
Iteration 32/1000 | Loss: 0.00001369
Iteration 33/1000 | Loss: 0.00001368
Iteration 34/1000 | Loss: 0.00001368
Iteration 35/1000 | Loss: 0.00001363
Iteration 36/1000 | Loss: 0.00001363
Iteration 37/1000 | Loss: 0.00001361
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001360
Iteration 40/1000 | Loss: 0.00001360
Iteration 41/1000 | Loss: 0.00001359
Iteration 42/1000 | Loss: 0.00001359
Iteration 43/1000 | Loss: 0.00001358
Iteration 44/1000 | Loss: 0.00001356
Iteration 45/1000 | Loss: 0.00001353
Iteration 46/1000 | Loss: 0.00001350
Iteration 47/1000 | Loss: 0.00001350
Iteration 48/1000 | Loss: 0.00001350
Iteration 49/1000 | Loss: 0.00001349
Iteration 50/1000 | Loss: 0.00001348
Iteration 51/1000 | Loss: 0.00001347
Iteration 52/1000 | Loss: 0.00001347
Iteration 53/1000 | Loss: 0.00001346
Iteration 54/1000 | Loss: 0.00001346
Iteration 55/1000 | Loss: 0.00001345
Iteration 56/1000 | Loss: 0.00001345
Iteration 57/1000 | Loss: 0.00001344
Iteration 58/1000 | Loss: 0.00001343
Iteration 59/1000 | Loss: 0.00001343
Iteration 60/1000 | Loss: 0.00001343
Iteration 61/1000 | Loss: 0.00001343
Iteration 62/1000 | Loss: 0.00001343
Iteration 63/1000 | Loss: 0.00001343
Iteration 64/1000 | Loss: 0.00001343
Iteration 65/1000 | Loss: 0.00001343
Iteration 66/1000 | Loss: 0.00001343
Iteration 67/1000 | Loss: 0.00001342
Iteration 68/1000 | Loss: 0.00001342
Iteration 69/1000 | Loss: 0.00001342
Iteration 70/1000 | Loss: 0.00001342
Iteration 71/1000 | Loss: 0.00001342
Iteration 72/1000 | Loss: 0.00001342
Iteration 73/1000 | Loss: 0.00001341
Iteration 74/1000 | Loss: 0.00001341
Iteration 75/1000 | Loss: 0.00001341
Iteration 76/1000 | Loss: 0.00001340
Iteration 77/1000 | Loss: 0.00001340
Iteration 78/1000 | Loss: 0.00001340
Iteration 79/1000 | Loss: 0.00001340
Iteration 80/1000 | Loss: 0.00001339
Iteration 81/1000 | Loss: 0.00001339
Iteration 82/1000 | Loss: 0.00001338
Iteration 83/1000 | Loss: 0.00001338
Iteration 84/1000 | Loss: 0.00001338
Iteration 85/1000 | Loss: 0.00001337
Iteration 86/1000 | Loss: 0.00001337
Iteration 87/1000 | Loss: 0.00001336
Iteration 88/1000 | Loss: 0.00001336
Iteration 89/1000 | Loss: 0.00001336
Iteration 90/1000 | Loss: 0.00001336
Iteration 91/1000 | Loss: 0.00001335
Iteration 92/1000 | Loss: 0.00001335
Iteration 93/1000 | Loss: 0.00001335
Iteration 94/1000 | Loss: 0.00001335
Iteration 95/1000 | Loss: 0.00001335
Iteration 96/1000 | Loss: 0.00001334
Iteration 97/1000 | Loss: 0.00001334
Iteration 98/1000 | Loss: 0.00001334
Iteration 99/1000 | Loss: 0.00001334
Iteration 100/1000 | Loss: 0.00001334
Iteration 101/1000 | Loss: 0.00001334
Iteration 102/1000 | Loss: 0.00001334
Iteration 103/1000 | Loss: 0.00001334
Iteration 104/1000 | Loss: 0.00001334
Iteration 105/1000 | Loss: 0.00001334
Iteration 106/1000 | Loss: 0.00001334
Iteration 107/1000 | Loss: 0.00001334
Iteration 108/1000 | Loss: 0.00001334
Iteration 109/1000 | Loss: 0.00001333
Iteration 110/1000 | Loss: 0.00001333
Iteration 111/1000 | Loss: 0.00001333
Iteration 112/1000 | Loss: 0.00001333
Iteration 113/1000 | Loss: 0.00001332
Iteration 114/1000 | Loss: 0.00001332
Iteration 115/1000 | Loss: 0.00001332
Iteration 116/1000 | Loss: 0.00001331
Iteration 117/1000 | Loss: 0.00001331
Iteration 118/1000 | Loss: 0.00001331
Iteration 119/1000 | Loss: 0.00001331
Iteration 120/1000 | Loss: 0.00001331
Iteration 121/1000 | Loss: 0.00001330
Iteration 122/1000 | Loss: 0.00001330
Iteration 123/1000 | Loss: 0.00001329
Iteration 124/1000 | Loss: 0.00001329
Iteration 125/1000 | Loss: 0.00001329
Iteration 126/1000 | Loss: 0.00001329
Iteration 127/1000 | Loss: 0.00001329
Iteration 128/1000 | Loss: 0.00001329
Iteration 129/1000 | Loss: 0.00001329
Iteration 130/1000 | Loss: 0.00001329
Iteration 131/1000 | Loss: 0.00001328
Iteration 132/1000 | Loss: 0.00001328
Iteration 133/1000 | Loss: 0.00001328
Iteration 134/1000 | Loss: 0.00001328
Iteration 135/1000 | Loss: 0.00001328
Iteration 136/1000 | Loss: 0.00001328
Iteration 137/1000 | Loss: 0.00001328
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001327
Iteration 143/1000 | Loss: 0.00001327
Iteration 144/1000 | Loss: 0.00001327
Iteration 145/1000 | Loss: 0.00001327
Iteration 146/1000 | Loss: 0.00001326
Iteration 147/1000 | Loss: 0.00001326
Iteration 148/1000 | Loss: 0.00001326
Iteration 149/1000 | Loss: 0.00001326
Iteration 150/1000 | Loss: 0.00001326
Iteration 151/1000 | Loss: 0.00001326
Iteration 152/1000 | Loss: 0.00001326
Iteration 153/1000 | Loss: 0.00001326
Iteration 154/1000 | Loss: 0.00001326
Iteration 155/1000 | Loss: 0.00001326
Iteration 156/1000 | Loss: 0.00001325
Iteration 157/1000 | Loss: 0.00001325
Iteration 158/1000 | Loss: 0.00001325
Iteration 159/1000 | Loss: 0.00001325
Iteration 160/1000 | Loss: 0.00001325
Iteration 161/1000 | Loss: 0.00001325
Iteration 162/1000 | Loss: 0.00001325
Iteration 163/1000 | Loss: 0.00001325
Iteration 164/1000 | Loss: 0.00001325
Iteration 165/1000 | Loss: 0.00001324
Iteration 166/1000 | Loss: 0.00001324
Iteration 167/1000 | Loss: 0.00001324
Iteration 168/1000 | Loss: 0.00001324
Iteration 169/1000 | Loss: 0.00001324
Iteration 170/1000 | Loss: 0.00001324
Iteration 171/1000 | Loss: 0.00001324
Iteration 172/1000 | Loss: 0.00001324
Iteration 173/1000 | Loss: 0.00001324
Iteration 174/1000 | Loss: 0.00001324
Iteration 175/1000 | Loss: 0.00001324
Iteration 176/1000 | Loss: 0.00001324
Iteration 177/1000 | Loss: 0.00001324
Iteration 178/1000 | Loss: 0.00001324
Iteration 179/1000 | Loss: 0.00001324
Iteration 180/1000 | Loss: 0.00001324
Iteration 181/1000 | Loss: 0.00001324
Iteration 182/1000 | Loss: 0.00001324
Iteration 183/1000 | Loss: 0.00001324
Iteration 184/1000 | Loss: 0.00001324
Iteration 185/1000 | Loss: 0.00001324
Iteration 186/1000 | Loss: 0.00001324
Iteration 187/1000 | Loss: 0.00001324
Iteration 188/1000 | Loss: 0.00001324
Iteration 189/1000 | Loss: 0.00001324
Iteration 190/1000 | Loss: 0.00001324
Iteration 191/1000 | Loss: 0.00001324
Iteration 192/1000 | Loss: 0.00001324
Iteration 193/1000 | Loss: 0.00001324
Iteration 194/1000 | Loss: 0.00001324
Iteration 195/1000 | Loss: 0.00001324
Iteration 196/1000 | Loss: 0.00001324
Iteration 197/1000 | Loss: 0.00001324
Iteration 198/1000 | Loss: 0.00001324
Iteration 199/1000 | Loss: 0.00001324
Iteration 200/1000 | Loss: 0.00001324
Iteration 201/1000 | Loss: 0.00001324
Iteration 202/1000 | Loss: 0.00001324
Iteration 203/1000 | Loss: 0.00001324
Iteration 204/1000 | Loss: 0.00001324
Iteration 205/1000 | Loss: 0.00001324
Iteration 206/1000 | Loss: 0.00001324
Iteration 207/1000 | Loss: 0.00001324
Iteration 208/1000 | Loss: 0.00001324
Iteration 209/1000 | Loss: 0.00001324
Iteration 210/1000 | Loss: 0.00001324
Iteration 211/1000 | Loss: 0.00001324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.3236931408755481e-05, 1.3236931408755481e-05, 1.3236931408755481e-05, 1.3236931408755481e-05, 1.3236931408755481e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3236931408755481e-05

Optimization complete. Final v2v error: 2.9123129844665527 mm

Highest mean error: 5.016258716583252 mm for frame 87

Lowest mean error: 2.2967188358306885 mm for frame 128

Saving results

Total time: 40.94823384284973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012752
Iteration 2/25 | Loss: 0.00429342
Iteration 3/25 | Loss: 0.00277147
Iteration 4/25 | Loss: 0.00208557
Iteration 5/25 | Loss: 0.00171751
Iteration 6/25 | Loss: 0.00152980
Iteration 7/25 | Loss: 0.00143910
Iteration 8/25 | Loss: 0.00142941
Iteration 9/25 | Loss: 0.00142151
Iteration 10/25 | Loss: 0.00141867
Iteration 11/25 | Loss: 0.00141891
Iteration 12/25 | Loss: 0.00142128
Iteration 13/25 | Loss: 0.00141786
Iteration 14/25 | Loss: 0.00141735
Iteration 15/25 | Loss: 0.00141735
Iteration 16/25 | Loss: 0.00141735
Iteration 17/25 | Loss: 0.00141734
Iteration 18/25 | Loss: 0.00141734
Iteration 19/25 | Loss: 0.00141734
Iteration 20/25 | Loss: 0.00141734
Iteration 21/25 | Loss: 0.00141734
Iteration 22/25 | Loss: 0.00141734
Iteration 23/25 | Loss: 0.00141734
Iteration 24/25 | Loss: 0.00141734
Iteration 25/25 | Loss: 0.00141734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41637921
Iteration 2/25 | Loss: 0.00500513
Iteration 3/25 | Loss: 0.00496352
Iteration 4/25 | Loss: 0.00496352
Iteration 5/25 | Loss: 0.00496352
Iteration 6/25 | Loss: 0.00496352
Iteration 7/25 | Loss: 0.00496352
Iteration 8/25 | Loss: 0.00496352
Iteration 9/25 | Loss: 0.00496352
Iteration 10/25 | Loss: 0.00496352
Iteration 11/25 | Loss: 0.00496352
Iteration 12/25 | Loss: 0.00496352
Iteration 13/25 | Loss: 0.00496352
Iteration 14/25 | Loss: 0.00496352
Iteration 15/25 | Loss: 0.00496352
Iteration 16/25 | Loss: 0.00496352
Iteration 17/25 | Loss: 0.00496352
Iteration 18/25 | Loss: 0.00496352
Iteration 19/25 | Loss: 0.00496352
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004963515792042017, 0.004963515792042017, 0.004963515792042017, 0.004963515792042017, 0.004963515792042017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004963515792042017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00496352
Iteration 2/1000 | Loss: 0.00104438
Iteration 3/1000 | Loss: 0.00077015
Iteration 4/1000 | Loss: 0.00056327
Iteration 5/1000 | Loss: 0.00104632
Iteration 6/1000 | Loss: 0.00059044
Iteration 7/1000 | Loss: 0.00048455
Iteration 8/1000 | Loss: 0.00141494
Iteration 9/1000 | Loss: 0.00043489
Iteration 10/1000 | Loss: 0.00041909
Iteration 11/1000 | Loss: 0.00535977
Iteration 12/1000 | Loss: 0.00044785
Iteration 13/1000 | Loss: 0.00042049
Iteration 14/1000 | Loss: 0.00041051
Iteration 15/1000 | Loss: 0.00202743
Iteration 16/1000 | Loss: 0.04185330
Iteration 17/1000 | Loss: 0.00157237
Iteration 18/1000 | Loss: 0.00126751
Iteration 19/1000 | Loss: 0.00042042
Iteration 20/1000 | Loss: 0.00085149
Iteration 21/1000 | Loss: 0.00020098
Iteration 22/1000 | Loss: 0.00049858
Iteration 23/1000 | Loss: 0.00022183
Iteration 24/1000 | Loss: 0.00008638
Iteration 25/1000 | Loss: 0.00011895
Iteration 26/1000 | Loss: 0.00008228
Iteration 27/1000 | Loss: 0.00004971
Iteration 28/1000 | Loss: 0.00005442
Iteration 29/1000 | Loss: 0.00003165
Iteration 30/1000 | Loss: 0.00005343
Iteration 31/1000 | Loss: 0.00005419
Iteration 32/1000 | Loss: 0.00006715
Iteration 33/1000 | Loss: 0.00012371
Iteration 34/1000 | Loss: 0.00005189
Iteration 35/1000 | Loss: 0.00001844
Iteration 36/1000 | Loss: 0.00009141
Iteration 37/1000 | Loss: 0.00005205
Iteration 38/1000 | Loss: 0.00002890
Iteration 39/1000 | Loss: 0.00002114
Iteration 40/1000 | Loss: 0.00001505
Iteration 41/1000 | Loss: 0.00002454
Iteration 42/1000 | Loss: 0.00002568
Iteration 43/1000 | Loss: 0.00001741
Iteration 44/1000 | Loss: 0.00003382
Iteration 45/1000 | Loss: 0.00001337
Iteration 46/1000 | Loss: 0.00001742
Iteration 47/1000 | Loss: 0.00002101
Iteration 48/1000 | Loss: 0.00002233
Iteration 49/1000 | Loss: 0.00001514
Iteration 50/1000 | Loss: 0.00001307
Iteration 51/1000 | Loss: 0.00001307
Iteration 52/1000 | Loss: 0.00001307
Iteration 53/1000 | Loss: 0.00001306
Iteration 54/1000 | Loss: 0.00001306
Iteration 55/1000 | Loss: 0.00001306
Iteration 56/1000 | Loss: 0.00001306
Iteration 57/1000 | Loss: 0.00001306
Iteration 58/1000 | Loss: 0.00001305
Iteration 59/1000 | Loss: 0.00001305
Iteration 60/1000 | Loss: 0.00001305
Iteration 61/1000 | Loss: 0.00001305
Iteration 62/1000 | Loss: 0.00001305
Iteration 63/1000 | Loss: 0.00001305
Iteration 64/1000 | Loss: 0.00001305
Iteration 65/1000 | Loss: 0.00001305
Iteration 66/1000 | Loss: 0.00001304
Iteration 67/1000 | Loss: 0.00001304
Iteration 68/1000 | Loss: 0.00001304
Iteration 69/1000 | Loss: 0.00001304
Iteration 70/1000 | Loss: 0.00001304
Iteration 71/1000 | Loss: 0.00001304
Iteration 72/1000 | Loss: 0.00001304
Iteration 73/1000 | Loss: 0.00001303
Iteration 74/1000 | Loss: 0.00001303
Iteration 75/1000 | Loss: 0.00001303
Iteration 76/1000 | Loss: 0.00001303
Iteration 77/1000 | Loss: 0.00001303
Iteration 78/1000 | Loss: 0.00001303
Iteration 79/1000 | Loss: 0.00001303
Iteration 80/1000 | Loss: 0.00001302
Iteration 81/1000 | Loss: 0.00001302
Iteration 82/1000 | Loss: 0.00001302
Iteration 83/1000 | Loss: 0.00002011
Iteration 84/1000 | Loss: 0.00002011
Iteration 85/1000 | Loss: 0.00001949
Iteration 86/1000 | Loss: 0.00001345
Iteration 87/1000 | Loss: 0.00001356
Iteration 88/1000 | Loss: 0.00001299
Iteration 89/1000 | Loss: 0.00001299
Iteration 90/1000 | Loss: 0.00001298
Iteration 91/1000 | Loss: 0.00001298
Iteration 92/1000 | Loss: 0.00001298
Iteration 93/1000 | Loss: 0.00001298
Iteration 94/1000 | Loss: 0.00001298
Iteration 95/1000 | Loss: 0.00001298
Iteration 96/1000 | Loss: 0.00001298
Iteration 97/1000 | Loss: 0.00001298
Iteration 98/1000 | Loss: 0.00001298
Iteration 99/1000 | Loss: 0.00001298
Iteration 100/1000 | Loss: 0.00001298
Iteration 101/1000 | Loss: 0.00001298
Iteration 102/1000 | Loss: 0.00001298
Iteration 103/1000 | Loss: 0.00001298
Iteration 104/1000 | Loss: 0.00001297
Iteration 105/1000 | Loss: 0.00001297
Iteration 106/1000 | Loss: 0.00001297
Iteration 107/1000 | Loss: 0.00001297
Iteration 108/1000 | Loss: 0.00001297
Iteration 109/1000 | Loss: 0.00001297
Iteration 110/1000 | Loss: 0.00001297
Iteration 111/1000 | Loss: 0.00001297
Iteration 112/1000 | Loss: 0.00001297
Iteration 113/1000 | Loss: 0.00001297
Iteration 114/1000 | Loss: 0.00001297
Iteration 115/1000 | Loss: 0.00001297
Iteration 116/1000 | Loss: 0.00001297
Iteration 117/1000 | Loss: 0.00001297
Iteration 118/1000 | Loss: 0.00001297
Iteration 119/1000 | Loss: 0.00001297
Iteration 120/1000 | Loss: 0.00001297
Iteration 121/1000 | Loss: 0.00001297
Iteration 122/1000 | Loss: 0.00001297
Iteration 123/1000 | Loss: 0.00001297
Iteration 124/1000 | Loss: 0.00001297
Iteration 125/1000 | Loss: 0.00001297
Iteration 126/1000 | Loss: 0.00001297
Iteration 127/1000 | Loss: 0.00001297
Iteration 128/1000 | Loss: 0.00001297
Iteration 129/1000 | Loss: 0.00001297
Iteration 130/1000 | Loss: 0.00001297
Iteration 131/1000 | Loss: 0.00001297
Iteration 132/1000 | Loss: 0.00001297
Iteration 133/1000 | Loss: 0.00001297
Iteration 134/1000 | Loss: 0.00001297
Iteration 135/1000 | Loss: 0.00001297
Iteration 136/1000 | Loss: 0.00001297
Iteration 137/1000 | Loss: 0.00001297
Iteration 138/1000 | Loss: 0.00001297
Iteration 139/1000 | Loss: 0.00001297
Iteration 140/1000 | Loss: 0.00001297
Iteration 141/1000 | Loss: 0.00001297
Iteration 142/1000 | Loss: 0.00001297
Iteration 143/1000 | Loss: 0.00001297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.2973149750905577e-05, 1.2973149750905577e-05, 1.2973149750905577e-05, 1.2973149750905577e-05, 1.2973149750905577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2973149750905577e-05

Optimization complete. Final v2v error: 3.1147096157073975 mm

Highest mean error: 3.2971761226654053 mm for frame 27

Lowest mean error: 2.895275115966797 mm for frame 120

Saving results

Total time: 97.80386352539062
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839622
Iteration 2/25 | Loss: 0.00108478
Iteration 3/25 | Loss: 0.00069152
Iteration 4/25 | Loss: 0.00062237
Iteration 5/25 | Loss: 0.00060938
Iteration 6/25 | Loss: 0.00060609
Iteration 7/25 | Loss: 0.00060454
Iteration 8/25 | Loss: 0.00060426
Iteration 9/25 | Loss: 0.00060426
Iteration 10/25 | Loss: 0.00060426
Iteration 11/25 | Loss: 0.00060426
Iteration 12/25 | Loss: 0.00060426
Iteration 13/25 | Loss: 0.00060426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006042640889063478, 0.0006042640889063478, 0.0006042640889063478, 0.0006042640889063478, 0.0006042640889063478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006042640889063478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45960128
Iteration 2/25 | Loss: 0.00029557
Iteration 3/25 | Loss: 0.00029557
Iteration 4/25 | Loss: 0.00029557
Iteration 5/25 | Loss: 0.00029557
Iteration 6/25 | Loss: 0.00029557
Iteration 7/25 | Loss: 0.00029557
Iteration 8/25 | Loss: 0.00029557
Iteration 9/25 | Loss: 0.00029557
Iteration 10/25 | Loss: 0.00029557
Iteration 11/25 | Loss: 0.00029557
Iteration 12/25 | Loss: 0.00029557
Iteration 13/25 | Loss: 0.00029557
Iteration 14/25 | Loss: 0.00029557
Iteration 15/25 | Loss: 0.00029557
Iteration 16/25 | Loss: 0.00029557
Iteration 17/25 | Loss: 0.00029557
Iteration 18/25 | Loss: 0.00029557
Iteration 19/25 | Loss: 0.00029557
Iteration 20/25 | Loss: 0.00029557
Iteration 21/25 | Loss: 0.00029557
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0002955688105430454, 0.0002955688105430454, 0.0002955688105430454, 0.0002955688105430454, 0.0002955688105430454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002955688105430454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029557
Iteration 2/1000 | Loss: 0.00002096
Iteration 3/1000 | Loss: 0.00001376
Iteration 4/1000 | Loss: 0.00001279
Iteration 5/1000 | Loss: 0.00001213
Iteration 6/1000 | Loss: 0.00001172
Iteration 7/1000 | Loss: 0.00001142
Iteration 8/1000 | Loss: 0.00001130
Iteration 9/1000 | Loss: 0.00001129
Iteration 10/1000 | Loss: 0.00001125
Iteration 11/1000 | Loss: 0.00001124
Iteration 12/1000 | Loss: 0.00001123
Iteration 13/1000 | Loss: 0.00001122
Iteration 14/1000 | Loss: 0.00001116
Iteration 15/1000 | Loss: 0.00001115
Iteration 16/1000 | Loss: 0.00001114
Iteration 17/1000 | Loss: 0.00001113
Iteration 18/1000 | Loss: 0.00001110
Iteration 19/1000 | Loss: 0.00001106
Iteration 20/1000 | Loss: 0.00001103
Iteration 21/1000 | Loss: 0.00001102
Iteration 22/1000 | Loss: 0.00001102
Iteration 23/1000 | Loss: 0.00001101
Iteration 24/1000 | Loss: 0.00001099
Iteration 25/1000 | Loss: 0.00001099
Iteration 26/1000 | Loss: 0.00001098
Iteration 27/1000 | Loss: 0.00001098
Iteration 28/1000 | Loss: 0.00001097
Iteration 29/1000 | Loss: 0.00001097
Iteration 30/1000 | Loss: 0.00001097
Iteration 31/1000 | Loss: 0.00001097
Iteration 32/1000 | Loss: 0.00001096
Iteration 33/1000 | Loss: 0.00001093
Iteration 34/1000 | Loss: 0.00001093
Iteration 35/1000 | Loss: 0.00001092
Iteration 36/1000 | Loss: 0.00001092
Iteration 37/1000 | Loss: 0.00001092
Iteration 38/1000 | Loss: 0.00001091
Iteration 39/1000 | Loss: 0.00001090
Iteration 40/1000 | Loss: 0.00001090
Iteration 41/1000 | Loss: 0.00001090
Iteration 42/1000 | Loss: 0.00001090
Iteration 43/1000 | Loss: 0.00001089
Iteration 44/1000 | Loss: 0.00001089
Iteration 45/1000 | Loss: 0.00001089
Iteration 46/1000 | Loss: 0.00001089
Iteration 47/1000 | Loss: 0.00001089
Iteration 48/1000 | Loss: 0.00001089
Iteration 49/1000 | Loss: 0.00001088
Iteration 50/1000 | Loss: 0.00001088
Iteration 51/1000 | Loss: 0.00001088
Iteration 52/1000 | Loss: 0.00001088
Iteration 53/1000 | Loss: 0.00001087
Iteration 54/1000 | Loss: 0.00001087
Iteration 55/1000 | Loss: 0.00001087
Iteration 56/1000 | Loss: 0.00001086
Iteration 57/1000 | Loss: 0.00001086
Iteration 58/1000 | Loss: 0.00001086
Iteration 59/1000 | Loss: 0.00001086
Iteration 60/1000 | Loss: 0.00001086
Iteration 61/1000 | Loss: 0.00001086
Iteration 62/1000 | Loss: 0.00001085
Iteration 63/1000 | Loss: 0.00001085
Iteration 64/1000 | Loss: 0.00001084
Iteration 65/1000 | Loss: 0.00001084
Iteration 66/1000 | Loss: 0.00001084
Iteration 67/1000 | Loss: 0.00001084
Iteration 68/1000 | Loss: 0.00001083
Iteration 69/1000 | Loss: 0.00001083
Iteration 70/1000 | Loss: 0.00001082
Iteration 71/1000 | Loss: 0.00001082
Iteration 72/1000 | Loss: 0.00001082
Iteration 73/1000 | Loss: 0.00001081
Iteration 74/1000 | Loss: 0.00001081
Iteration 75/1000 | Loss: 0.00001081
Iteration 76/1000 | Loss: 0.00001081
Iteration 77/1000 | Loss: 0.00001081
Iteration 78/1000 | Loss: 0.00001081
Iteration 79/1000 | Loss: 0.00001081
Iteration 80/1000 | Loss: 0.00001080
Iteration 81/1000 | Loss: 0.00001080
Iteration 82/1000 | Loss: 0.00001080
Iteration 83/1000 | Loss: 0.00001080
Iteration 84/1000 | Loss: 0.00001080
Iteration 85/1000 | Loss: 0.00001080
Iteration 86/1000 | Loss: 0.00001080
Iteration 87/1000 | Loss: 0.00001080
Iteration 88/1000 | Loss: 0.00001080
Iteration 89/1000 | Loss: 0.00001080
Iteration 90/1000 | Loss: 0.00001080
Iteration 91/1000 | Loss: 0.00001080
Iteration 92/1000 | Loss: 0.00001080
Iteration 93/1000 | Loss: 0.00001080
Iteration 94/1000 | Loss: 0.00001080
Iteration 95/1000 | Loss: 0.00001080
Iteration 96/1000 | Loss: 0.00001080
Iteration 97/1000 | Loss: 0.00001080
Iteration 98/1000 | Loss: 0.00001080
Iteration 99/1000 | Loss: 0.00001080
Iteration 100/1000 | Loss: 0.00001080
Iteration 101/1000 | Loss: 0.00001080
Iteration 102/1000 | Loss: 0.00001080
Iteration 103/1000 | Loss: 0.00001080
Iteration 104/1000 | Loss: 0.00001080
Iteration 105/1000 | Loss: 0.00001080
Iteration 106/1000 | Loss: 0.00001080
Iteration 107/1000 | Loss: 0.00001080
Iteration 108/1000 | Loss: 0.00001080
Iteration 109/1000 | Loss: 0.00001080
Iteration 110/1000 | Loss: 0.00001080
Iteration 111/1000 | Loss: 0.00001080
Iteration 112/1000 | Loss: 0.00001080
Iteration 113/1000 | Loss: 0.00001080
Iteration 114/1000 | Loss: 0.00001080
Iteration 115/1000 | Loss: 0.00001080
Iteration 116/1000 | Loss: 0.00001080
Iteration 117/1000 | Loss: 0.00001080
Iteration 118/1000 | Loss: 0.00001080
Iteration 119/1000 | Loss: 0.00001080
Iteration 120/1000 | Loss: 0.00001080
Iteration 121/1000 | Loss: 0.00001080
Iteration 122/1000 | Loss: 0.00001080
Iteration 123/1000 | Loss: 0.00001080
Iteration 124/1000 | Loss: 0.00001080
Iteration 125/1000 | Loss: 0.00001080
Iteration 126/1000 | Loss: 0.00001080
Iteration 127/1000 | Loss: 0.00001080
Iteration 128/1000 | Loss: 0.00001080
Iteration 129/1000 | Loss: 0.00001080
Iteration 130/1000 | Loss: 0.00001080
Iteration 131/1000 | Loss: 0.00001080
Iteration 132/1000 | Loss: 0.00001080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.0797250979521777e-05, 1.0797250979521777e-05, 1.0797250979521777e-05, 1.0797250979521777e-05, 1.0797250979521777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0797250979521777e-05

Optimization complete. Final v2v error: 2.7749438285827637 mm

Highest mean error: 3.1047556400299072 mm for frame 103

Lowest mean error: 2.643627882003784 mm for frame 54

Saving results

Total time: 34.171590089797974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840163
Iteration 2/25 | Loss: 0.00115112
Iteration 3/25 | Loss: 0.00080916
Iteration 4/25 | Loss: 0.00073374
Iteration 5/25 | Loss: 0.00070978
Iteration 6/25 | Loss: 0.00070433
Iteration 7/25 | Loss: 0.00070313
Iteration 8/25 | Loss: 0.00070302
Iteration 9/25 | Loss: 0.00070302
Iteration 10/25 | Loss: 0.00070302
Iteration 11/25 | Loss: 0.00070302
Iteration 12/25 | Loss: 0.00070302
Iteration 13/25 | Loss: 0.00070302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007030222332105041, 0.0007030222332105041, 0.0007030222332105041, 0.0007030222332105041, 0.0007030222332105041]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007030222332105041

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41904104
Iteration 2/25 | Loss: 0.00035085
Iteration 3/25 | Loss: 0.00035085
Iteration 4/25 | Loss: 0.00035085
Iteration 5/25 | Loss: 0.00035084
Iteration 6/25 | Loss: 0.00035084
Iteration 7/25 | Loss: 0.00035084
Iteration 8/25 | Loss: 0.00035084
Iteration 9/25 | Loss: 0.00035084
Iteration 10/25 | Loss: 0.00035084
Iteration 11/25 | Loss: 0.00035084
Iteration 12/25 | Loss: 0.00035084
Iteration 13/25 | Loss: 0.00035084
Iteration 14/25 | Loss: 0.00035084
Iteration 15/25 | Loss: 0.00035084
Iteration 16/25 | Loss: 0.00035084
Iteration 17/25 | Loss: 0.00035084
Iteration 18/25 | Loss: 0.00035084
Iteration 19/25 | Loss: 0.00035084
Iteration 20/25 | Loss: 0.00035084
Iteration 21/25 | Loss: 0.00035084
Iteration 22/25 | Loss: 0.00035084
Iteration 23/25 | Loss: 0.00035084
Iteration 24/25 | Loss: 0.00035084
Iteration 25/25 | Loss: 0.00035084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035084
Iteration 2/1000 | Loss: 0.00003945
Iteration 3/1000 | Loss: 0.00002882
Iteration 4/1000 | Loss: 0.00002562
Iteration 5/1000 | Loss: 0.00002420
Iteration 6/1000 | Loss: 0.00002324
Iteration 7/1000 | Loss: 0.00002249
Iteration 8/1000 | Loss: 0.00002193
Iteration 9/1000 | Loss: 0.00002141
Iteration 10/1000 | Loss: 0.00002114
Iteration 11/1000 | Loss: 0.00002100
Iteration 12/1000 | Loss: 0.00002082
Iteration 13/1000 | Loss: 0.00002067
Iteration 14/1000 | Loss: 0.00002065
Iteration 15/1000 | Loss: 0.00002063
Iteration 16/1000 | Loss: 0.00002059
Iteration 17/1000 | Loss: 0.00002059
Iteration 18/1000 | Loss: 0.00002056
Iteration 19/1000 | Loss: 0.00002055
Iteration 20/1000 | Loss: 0.00002051
Iteration 21/1000 | Loss: 0.00002048
Iteration 22/1000 | Loss: 0.00002047
Iteration 23/1000 | Loss: 0.00002046
Iteration 24/1000 | Loss: 0.00002046
Iteration 25/1000 | Loss: 0.00002046
Iteration 26/1000 | Loss: 0.00002045
Iteration 27/1000 | Loss: 0.00002045
Iteration 28/1000 | Loss: 0.00002044
Iteration 29/1000 | Loss: 0.00002044
Iteration 30/1000 | Loss: 0.00002043
Iteration 31/1000 | Loss: 0.00002043
Iteration 32/1000 | Loss: 0.00002043
Iteration 33/1000 | Loss: 0.00002042
Iteration 34/1000 | Loss: 0.00002041
Iteration 35/1000 | Loss: 0.00002041
Iteration 36/1000 | Loss: 0.00002040
Iteration 37/1000 | Loss: 0.00002040
Iteration 38/1000 | Loss: 0.00002038
Iteration 39/1000 | Loss: 0.00002038
Iteration 40/1000 | Loss: 0.00002037
Iteration 41/1000 | Loss: 0.00002037
Iteration 42/1000 | Loss: 0.00002036
Iteration 43/1000 | Loss: 0.00002036
Iteration 44/1000 | Loss: 0.00002036
Iteration 45/1000 | Loss: 0.00002036
Iteration 46/1000 | Loss: 0.00002035
Iteration 47/1000 | Loss: 0.00002035
Iteration 48/1000 | Loss: 0.00002035
Iteration 49/1000 | Loss: 0.00002035
Iteration 50/1000 | Loss: 0.00002035
Iteration 51/1000 | Loss: 0.00002035
Iteration 52/1000 | Loss: 0.00002035
Iteration 53/1000 | Loss: 0.00002034
Iteration 54/1000 | Loss: 0.00002034
Iteration 55/1000 | Loss: 0.00002034
Iteration 56/1000 | Loss: 0.00002034
Iteration 57/1000 | Loss: 0.00002034
Iteration 58/1000 | Loss: 0.00002034
Iteration 59/1000 | Loss: 0.00002034
Iteration 60/1000 | Loss: 0.00002034
Iteration 61/1000 | Loss: 0.00002033
Iteration 62/1000 | Loss: 0.00002033
Iteration 63/1000 | Loss: 0.00002033
Iteration 64/1000 | Loss: 0.00002033
Iteration 65/1000 | Loss: 0.00002033
Iteration 66/1000 | Loss: 0.00002032
Iteration 67/1000 | Loss: 0.00002032
Iteration 68/1000 | Loss: 0.00002032
Iteration 69/1000 | Loss: 0.00002032
Iteration 70/1000 | Loss: 0.00002031
Iteration 71/1000 | Loss: 0.00002031
Iteration 72/1000 | Loss: 0.00002031
Iteration 73/1000 | Loss: 0.00002031
Iteration 74/1000 | Loss: 0.00002031
Iteration 75/1000 | Loss: 0.00002031
Iteration 76/1000 | Loss: 0.00002031
Iteration 77/1000 | Loss: 0.00002031
Iteration 78/1000 | Loss: 0.00002031
Iteration 79/1000 | Loss: 0.00002030
Iteration 80/1000 | Loss: 0.00002030
Iteration 81/1000 | Loss: 0.00002030
Iteration 82/1000 | Loss: 0.00002030
Iteration 83/1000 | Loss: 0.00002030
Iteration 84/1000 | Loss: 0.00002030
Iteration 85/1000 | Loss: 0.00002030
Iteration 86/1000 | Loss: 0.00002030
Iteration 87/1000 | Loss: 0.00002029
Iteration 88/1000 | Loss: 0.00002029
Iteration 89/1000 | Loss: 0.00002029
Iteration 90/1000 | Loss: 0.00002029
Iteration 91/1000 | Loss: 0.00002029
Iteration 92/1000 | Loss: 0.00002029
Iteration 93/1000 | Loss: 0.00002029
Iteration 94/1000 | Loss: 0.00002029
Iteration 95/1000 | Loss: 0.00002029
Iteration 96/1000 | Loss: 0.00002029
Iteration 97/1000 | Loss: 0.00002029
Iteration 98/1000 | Loss: 0.00002029
Iteration 99/1000 | Loss: 0.00002029
Iteration 100/1000 | Loss: 0.00002029
Iteration 101/1000 | Loss: 0.00002029
Iteration 102/1000 | Loss: 0.00002029
Iteration 103/1000 | Loss: 0.00002029
Iteration 104/1000 | Loss: 0.00002029
Iteration 105/1000 | Loss: 0.00002029
Iteration 106/1000 | Loss: 0.00002029
Iteration 107/1000 | Loss: 0.00002029
Iteration 108/1000 | Loss: 0.00002029
Iteration 109/1000 | Loss: 0.00002029
Iteration 110/1000 | Loss: 0.00002029
Iteration 111/1000 | Loss: 0.00002029
Iteration 112/1000 | Loss: 0.00002029
Iteration 113/1000 | Loss: 0.00002029
Iteration 114/1000 | Loss: 0.00002029
Iteration 115/1000 | Loss: 0.00002029
Iteration 116/1000 | Loss: 0.00002029
Iteration 117/1000 | Loss: 0.00002029
Iteration 118/1000 | Loss: 0.00002029
Iteration 119/1000 | Loss: 0.00002029
Iteration 120/1000 | Loss: 0.00002029
Iteration 121/1000 | Loss: 0.00002029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.028530798270367e-05, 2.028530798270367e-05, 2.028530798270367e-05, 2.028530798270367e-05, 2.028530798270367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.028530798270367e-05

Optimization complete. Final v2v error: 3.792217254638672 mm

Highest mean error: 4.9980998039245605 mm for frame 153

Lowest mean error: 3.247253656387329 mm for frame 35

Saving results

Total time: 42.17511177062988
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445270
Iteration 2/25 | Loss: 0.00098400
Iteration 3/25 | Loss: 0.00069080
Iteration 4/25 | Loss: 0.00065588
Iteration 5/25 | Loss: 0.00064413
Iteration 6/25 | Loss: 0.00064211
Iteration 7/25 | Loss: 0.00064211
Iteration 8/25 | Loss: 0.00064211
Iteration 9/25 | Loss: 0.00064211
Iteration 10/25 | Loss: 0.00064211
Iteration 11/25 | Loss: 0.00064211
Iteration 12/25 | Loss: 0.00064211
Iteration 13/25 | Loss: 0.00064211
Iteration 14/25 | Loss: 0.00064211
Iteration 15/25 | Loss: 0.00064211
Iteration 16/25 | Loss: 0.00064211
Iteration 17/25 | Loss: 0.00064211
Iteration 18/25 | Loss: 0.00064211
Iteration 19/25 | Loss: 0.00064211
Iteration 20/25 | Loss: 0.00064211
Iteration 21/25 | Loss: 0.00064211
Iteration 22/25 | Loss: 0.00064211
Iteration 23/25 | Loss: 0.00064211
Iteration 24/25 | Loss: 0.00064211
Iteration 25/25 | Loss: 0.00064211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89222562
Iteration 2/25 | Loss: 0.00027258
Iteration 3/25 | Loss: 0.00027258
Iteration 4/25 | Loss: 0.00027258
Iteration 5/25 | Loss: 0.00027258
Iteration 6/25 | Loss: 0.00027258
Iteration 7/25 | Loss: 0.00027257
Iteration 8/25 | Loss: 0.00027257
Iteration 9/25 | Loss: 0.00027257
Iteration 10/25 | Loss: 0.00027257
Iteration 11/25 | Loss: 0.00027257
Iteration 12/25 | Loss: 0.00027257
Iteration 13/25 | Loss: 0.00027257
Iteration 14/25 | Loss: 0.00027257
Iteration 15/25 | Loss: 0.00027257
Iteration 16/25 | Loss: 0.00027257
Iteration 17/25 | Loss: 0.00027257
Iteration 18/25 | Loss: 0.00027257
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00027257364126853645, 0.00027257364126853645, 0.00027257364126853645, 0.00027257364126853645, 0.00027257364126853645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027257364126853645

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027257
Iteration 2/1000 | Loss: 0.00002949
Iteration 3/1000 | Loss: 0.00001790
Iteration 4/1000 | Loss: 0.00001658
Iteration 5/1000 | Loss: 0.00001580
Iteration 6/1000 | Loss: 0.00001539
Iteration 7/1000 | Loss: 0.00001500
Iteration 8/1000 | Loss: 0.00001462
Iteration 9/1000 | Loss: 0.00001439
Iteration 10/1000 | Loss: 0.00001439
Iteration 11/1000 | Loss: 0.00001433
Iteration 12/1000 | Loss: 0.00001425
Iteration 13/1000 | Loss: 0.00001418
Iteration 14/1000 | Loss: 0.00001417
Iteration 15/1000 | Loss: 0.00001416
Iteration 16/1000 | Loss: 0.00001411
Iteration 17/1000 | Loss: 0.00001406
Iteration 18/1000 | Loss: 0.00001405
Iteration 19/1000 | Loss: 0.00001404
Iteration 20/1000 | Loss: 0.00001403
Iteration 21/1000 | Loss: 0.00001398
Iteration 22/1000 | Loss: 0.00001398
Iteration 23/1000 | Loss: 0.00001396
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001394
Iteration 26/1000 | Loss: 0.00001393
Iteration 27/1000 | Loss: 0.00001393
Iteration 28/1000 | Loss: 0.00001392
Iteration 29/1000 | Loss: 0.00001392
Iteration 30/1000 | Loss: 0.00001391
Iteration 31/1000 | Loss: 0.00001391
Iteration 32/1000 | Loss: 0.00001391
Iteration 33/1000 | Loss: 0.00001391
Iteration 34/1000 | Loss: 0.00001391
Iteration 35/1000 | Loss: 0.00001390
Iteration 36/1000 | Loss: 0.00001390
Iteration 37/1000 | Loss: 0.00001390
Iteration 38/1000 | Loss: 0.00001390
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001390
Iteration 41/1000 | Loss: 0.00001390
Iteration 42/1000 | Loss: 0.00001390
Iteration 43/1000 | Loss: 0.00001389
Iteration 44/1000 | Loss: 0.00001389
Iteration 45/1000 | Loss: 0.00001389
Iteration 46/1000 | Loss: 0.00001389
Iteration 47/1000 | Loss: 0.00001389
Iteration 48/1000 | Loss: 0.00001388
Iteration 49/1000 | Loss: 0.00001388
Iteration 50/1000 | Loss: 0.00001387
Iteration 51/1000 | Loss: 0.00001387
Iteration 52/1000 | Loss: 0.00001387
Iteration 53/1000 | Loss: 0.00001387
Iteration 54/1000 | Loss: 0.00001386
Iteration 55/1000 | Loss: 0.00001386
Iteration 56/1000 | Loss: 0.00001386
Iteration 57/1000 | Loss: 0.00001386
Iteration 58/1000 | Loss: 0.00001385
Iteration 59/1000 | Loss: 0.00001385
Iteration 60/1000 | Loss: 0.00001385
Iteration 61/1000 | Loss: 0.00001385
Iteration 62/1000 | Loss: 0.00001384
Iteration 63/1000 | Loss: 0.00001384
Iteration 64/1000 | Loss: 0.00001383
Iteration 65/1000 | Loss: 0.00001383
Iteration 66/1000 | Loss: 0.00001383
Iteration 67/1000 | Loss: 0.00001382
Iteration 68/1000 | Loss: 0.00001382
Iteration 69/1000 | Loss: 0.00001382
Iteration 70/1000 | Loss: 0.00001382
Iteration 71/1000 | Loss: 0.00001382
Iteration 72/1000 | Loss: 0.00001382
Iteration 73/1000 | Loss: 0.00001382
Iteration 74/1000 | Loss: 0.00001382
Iteration 75/1000 | Loss: 0.00001381
Iteration 76/1000 | Loss: 0.00001381
Iteration 77/1000 | Loss: 0.00001381
Iteration 78/1000 | Loss: 0.00001381
Iteration 79/1000 | Loss: 0.00001381
Iteration 80/1000 | Loss: 0.00001381
Iteration 81/1000 | Loss: 0.00001381
Iteration 82/1000 | Loss: 0.00001381
Iteration 83/1000 | Loss: 0.00001381
Iteration 84/1000 | Loss: 0.00001380
Iteration 85/1000 | Loss: 0.00001380
Iteration 86/1000 | Loss: 0.00001380
Iteration 87/1000 | Loss: 0.00001379
Iteration 88/1000 | Loss: 0.00001379
Iteration 89/1000 | Loss: 0.00001379
Iteration 90/1000 | Loss: 0.00001379
Iteration 91/1000 | Loss: 0.00001378
Iteration 92/1000 | Loss: 0.00001378
Iteration 93/1000 | Loss: 0.00001378
Iteration 94/1000 | Loss: 0.00001378
Iteration 95/1000 | Loss: 0.00001377
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001376
Iteration 99/1000 | Loss: 0.00001376
Iteration 100/1000 | Loss: 0.00001376
Iteration 101/1000 | Loss: 0.00001376
Iteration 102/1000 | Loss: 0.00001376
Iteration 103/1000 | Loss: 0.00001376
Iteration 104/1000 | Loss: 0.00001376
Iteration 105/1000 | Loss: 0.00001375
Iteration 106/1000 | Loss: 0.00001375
Iteration 107/1000 | Loss: 0.00001375
Iteration 108/1000 | Loss: 0.00001375
Iteration 109/1000 | Loss: 0.00001374
Iteration 110/1000 | Loss: 0.00001374
Iteration 111/1000 | Loss: 0.00001374
Iteration 112/1000 | Loss: 0.00001373
Iteration 113/1000 | Loss: 0.00001373
Iteration 114/1000 | Loss: 0.00001373
Iteration 115/1000 | Loss: 0.00001373
Iteration 116/1000 | Loss: 0.00001373
Iteration 117/1000 | Loss: 0.00001373
Iteration 118/1000 | Loss: 0.00001373
Iteration 119/1000 | Loss: 0.00001373
Iteration 120/1000 | Loss: 0.00001373
Iteration 121/1000 | Loss: 0.00001373
Iteration 122/1000 | Loss: 0.00001373
Iteration 123/1000 | Loss: 0.00001373
Iteration 124/1000 | Loss: 0.00001373
Iteration 125/1000 | Loss: 0.00001373
Iteration 126/1000 | Loss: 0.00001373
Iteration 127/1000 | Loss: 0.00001373
Iteration 128/1000 | Loss: 0.00001373
Iteration 129/1000 | Loss: 0.00001373
Iteration 130/1000 | Loss: 0.00001373
Iteration 131/1000 | Loss: 0.00001373
Iteration 132/1000 | Loss: 0.00001373
Iteration 133/1000 | Loss: 0.00001373
Iteration 134/1000 | Loss: 0.00001373
Iteration 135/1000 | Loss: 0.00001373
Iteration 136/1000 | Loss: 0.00001373
Iteration 137/1000 | Loss: 0.00001373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.3725223652727436e-05, 1.3725223652727436e-05, 1.3725223652727436e-05, 1.3725223652727436e-05, 1.3725223652727436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3725223652727436e-05

Optimization complete. Final v2v error: 3.195429801940918 mm

Highest mean error: 3.4758152961730957 mm for frame 240

Lowest mean error: 3.0531721115112305 mm for frame 188

Saving results

Total time: 40.45078897476196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00366476
Iteration 2/25 | Loss: 0.00075112
Iteration 3/25 | Loss: 0.00060448
Iteration 4/25 | Loss: 0.00058082
Iteration 5/25 | Loss: 0.00057529
Iteration 6/25 | Loss: 0.00057379
Iteration 7/25 | Loss: 0.00057351
Iteration 8/25 | Loss: 0.00057351
Iteration 9/25 | Loss: 0.00057351
Iteration 10/25 | Loss: 0.00057351
Iteration 11/25 | Loss: 0.00057351
Iteration 12/25 | Loss: 0.00057351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005735050071962178, 0.0005735050071962178, 0.0005735050071962178, 0.0005735050071962178, 0.0005735050071962178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005735050071962178

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44614077
Iteration 2/25 | Loss: 0.00028008
Iteration 3/25 | Loss: 0.00028008
Iteration 4/25 | Loss: 0.00028008
Iteration 5/25 | Loss: 0.00028008
Iteration 6/25 | Loss: 0.00028008
Iteration 7/25 | Loss: 0.00028008
Iteration 8/25 | Loss: 0.00028008
Iteration 9/25 | Loss: 0.00028008
Iteration 10/25 | Loss: 0.00028008
Iteration 11/25 | Loss: 0.00028008
Iteration 12/25 | Loss: 0.00028008
Iteration 13/25 | Loss: 0.00028008
Iteration 14/25 | Loss: 0.00028008
Iteration 15/25 | Loss: 0.00028008
Iteration 16/25 | Loss: 0.00028008
Iteration 17/25 | Loss: 0.00028008
Iteration 18/25 | Loss: 0.00028008
Iteration 19/25 | Loss: 0.00028008
Iteration 20/25 | Loss: 0.00028008
Iteration 21/25 | Loss: 0.00028008
Iteration 22/25 | Loss: 0.00028008
Iteration 23/25 | Loss: 0.00028008
Iteration 24/25 | Loss: 0.00028008
Iteration 25/25 | Loss: 0.00028008

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028008
Iteration 2/1000 | Loss: 0.00002010
Iteration 3/1000 | Loss: 0.00001146
Iteration 4/1000 | Loss: 0.00001066
Iteration 5/1000 | Loss: 0.00001014
Iteration 6/1000 | Loss: 0.00000986
Iteration 7/1000 | Loss: 0.00000967
Iteration 8/1000 | Loss: 0.00000955
Iteration 9/1000 | Loss: 0.00000952
Iteration 10/1000 | Loss: 0.00000952
Iteration 11/1000 | Loss: 0.00000949
Iteration 12/1000 | Loss: 0.00000949
Iteration 13/1000 | Loss: 0.00000946
Iteration 14/1000 | Loss: 0.00000945
Iteration 15/1000 | Loss: 0.00000944
Iteration 16/1000 | Loss: 0.00000940
Iteration 17/1000 | Loss: 0.00000939
Iteration 18/1000 | Loss: 0.00000934
Iteration 19/1000 | Loss: 0.00000933
Iteration 20/1000 | Loss: 0.00000933
Iteration 21/1000 | Loss: 0.00000932
Iteration 22/1000 | Loss: 0.00000931
Iteration 23/1000 | Loss: 0.00000929
Iteration 24/1000 | Loss: 0.00000929
Iteration 25/1000 | Loss: 0.00000928
Iteration 26/1000 | Loss: 0.00000928
Iteration 27/1000 | Loss: 0.00000928
Iteration 28/1000 | Loss: 0.00000928
Iteration 29/1000 | Loss: 0.00000928
Iteration 30/1000 | Loss: 0.00000927
Iteration 31/1000 | Loss: 0.00000927
Iteration 32/1000 | Loss: 0.00000926
Iteration 33/1000 | Loss: 0.00000926
Iteration 34/1000 | Loss: 0.00000925
Iteration 35/1000 | Loss: 0.00000925
Iteration 36/1000 | Loss: 0.00000925
Iteration 37/1000 | Loss: 0.00000925
Iteration 38/1000 | Loss: 0.00000925
Iteration 39/1000 | Loss: 0.00000924
Iteration 40/1000 | Loss: 0.00000924
Iteration 41/1000 | Loss: 0.00000924
Iteration 42/1000 | Loss: 0.00000924
Iteration 43/1000 | Loss: 0.00000924
Iteration 44/1000 | Loss: 0.00000924
Iteration 45/1000 | Loss: 0.00000923
Iteration 46/1000 | Loss: 0.00000923
Iteration 47/1000 | Loss: 0.00000922
Iteration 48/1000 | Loss: 0.00000922
Iteration 49/1000 | Loss: 0.00000920
Iteration 50/1000 | Loss: 0.00000920
Iteration 51/1000 | Loss: 0.00000916
Iteration 52/1000 | Loss: 0.00000916
Iteration 53/1000 | Loss: 0.00000916
Iteration 54/1000 | Loss: 0.00000915
Iteration 55/1000 | Loss: 0.00000915
Iteration 56/1000 | Loss: 0.00000915
Iteration 57/1000 | Loss: 0.00000914
Iteration 58/1000 | Loss: 0.00000913
Iteration 59/1000 | Loss: 0.00000913
Iteration 60/1000 | Loss: 0.00000912
Iteration 61/1000 | Loss: 0.00000912
Iteration 62/1000 | Loss: 0.00000912
Iteration 63/1000 | Loss: 0.00000912
Iteration 64/1000 | Loss: 0.00000912
Iteration 65/1000 | Loss: 0.00000912
Iteration 66/1000 | Loss: 0.00000912
Iteration 67/1000 | Loss: 0.00000912
Iteration 68/1000 | Loss: 0.00000912
Iteration 69/1000 | Loss: 0.00000912
Iteration 70/1000 | Loss: 0.00000912
Iteration 71/1000 | Loss: 0.00000912
Iteration 72/1000 | Loss: 0.00000911
Iteration 73/1000 | Loss: 0.00000911
Iteration 74/1000 | Loss: 0.00000911
Iteration 75/1000 | Loss: 0.00000911
Iteration 76/1000 | Loss: 0.00000911
Iteration 77/1000 | Loss: 0.00000911
Iteration 78/1000 | Loss: 0.00000911
Iteration 79/1000 | Loss: 0.00000911
Iteration 80/1000 | Loss: 0.00000910
Iteration 81/1000 | Loss: 0.00000910
Iteration 82/1000 | Loss: 0.00000910
Iteration 83/1000 | Loss: 0.00000910
Iteration 84/1000 | Loss: 0.00000909
Iteration 85/1000 | Loss: 0.00000909
Iteration 86/1000 | Loss: 0.00000909
Iteration 87/1000 | Loss: 0.00000909
Iteration 88/1000 | Loss: 0.00000909
Iteration 89/1000 | Loss: 0.00000909
Iteration 90/1000 | Loss: 0.00000909
Iteration 91/1000 | Loss: 0.00000908
Iteration 92/1000 | Loss: 0.00000908
Iteration 93/1000 | Loss: 0.00000907
Iteration 94/1000 | Loss: 0.00000907
Iteration 95/1000 | Loss: 0.00000906
Iteration 96/1000 | Loss: 0.00000906
Iteration 97/1000 | Loss: 0.00000906
Iteration 98/1000 | Loss: 0.00000906
Iteration 99/1000 | Loss: 0.00000906
Iteration 100/1000 | Loss: 0.00000905
Iteration 101/1000 | Loss: 0.00000905
Iteration 102/1000 | Loss: 0.00000905
Iteration 103/1000 | Loss: 0.00000904
Iteration 104/1000 | Loss: 0.00000904
Iteration 105/1000 | Loss: 0.00000904
Iteration 106/1000 | Loss: 0.00000903
Iteration 107/1000 | Loss: 0.00000903
Iteration 108/1000 | Loss: 0.00000903
Iteration 109/1000 | Loss: 0.00000903
Iteration 110/1000 | Loss: 0.00000902
Iteration 111/1000 | Loss: 0.00000902
Iteration 112/1000 | Loss: 0.00000902
Iteration 113/1000 | Loss: 0.00000901
Iteration 114/1000 | Loss: 0.00000901
Iteration 115/1000 | Loss: 0.00000901
Iteration 116/1000 | Loss: 0.00000901
Iteration 117/1000 | Loss: 0.00000901
Iteration 118/1000 | Loss: 0.00000900
Iteration 119/1000 | Loss: 0.00000900
Iteration 120/1000 | Loss: 0.00000900
Iteration 121/1000 | Loss: 0.00000899
Iteration 122/1000 | Loss: 0.00000899
Iteration 123/1000 | Loss: 0.00000899
Iteration 124/1000 | Loss: 0.00000899
Iteration 125/1000 | Loss: 0.00000899
Iteration 126/1000 | Loss: 0.00000899
Iteration 127/1000 | Loss: 0.00000899
Iteration 128/1000 | Loss: 0.00000899
Iteration 129/1000 | Loss: 0.00000899
Iteration 130/1000 | Loss: 0.00000899
Iteration 131/1000 | Loss: 0.00000899
Iteration 132/1000 | Loss: 0.00000898
Iteration 133/1000 | Loss: 0.00000898
Iteration 134/1000 | Loss: 0.00000898
Iteration 135/1000 | Loss: 0.00000898
Iteration 136/1000 | Loss: 0.00000898
Iteration 137/1000 | Loss: 0.00000898
Iteration 138/1000 | Loss: 0.00000898
Iteration 139/1000 | Loss: 0.00000898
Iteration 140/1000 | Loss: 0.00000898
Iteration 141/1000 | Loss: 0.00000898
Iteration 142/1000 | Loss: 0.00000898
Iteration 143/1000 | Loss: 0.00000898
Iteration 144/1000 | Loss: 0.00000898
Iteration 145/1000 | Loss: 0.00000898
Iteration 146/1000 | Loss: 0.00000898
Iteration 147/1000 | Loss: 0.00000898
Iteration 148/1000 | Loss: 0.00000898
Iteration 149/1000 | Loss: 0.00000897
Iteration 150/1000 | Loss: 0.00000897
Iteration 151/1000 | Loss: 0.00000897
Iteration 152/1000 | Loss: 0.00000897
Iteration 153/1000 | Loss: 0.00000897
Iteration 154/1000 | Loss: 0.00000897
Iteration 155/1000 | Loss: 0.00000897
Iteration 156/1000 | Loss: 0.00000897
Iteration 157/1000 | Loss: 0.00000897
Iteration 158/1000 | Loss: 0.00000897
Iteration 159/1000 | Loss: 0.00000897
Iteration 160/1000 | Loss: 0.00000897
Iteration 161/1000 | Loss: 0.00000897
Iteration 162/1000 | Loss: 0.00000897
Iteration 163/1000 | Loss: 0.00000897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [8.970062481239438e-06, 8.970062481239438e-06, 8.970062481239438e-06, 8.970062481239438e-06, 8.970062481239438e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.970062481239438e-06

Optimization complete. Final v2v error: 2.5550858974456787 mm

Highest mean error: 2.9922468662261963 mm for frame 79

Lowest mean error: 2.445409059524536 mm for frame 102

Saving results

Total time: 35.87159872055054
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000746
Iteration 2/25 | Loss: 0.00140191
Iteration 3/25 | Loss: 0.00092419
Iteration 4/25 | Loss: 0.00084683
Iteration 5/25 | Loss: 0.00083123
Iteration 6/25 | Loss: 0.00082766
Iteration 7/25 | Loss: 0.00082640
Iteration 8/25 | Loss: 0.00082600
Iteration 9/25 | Loss: 0.00082598
Iteration 10/25 | Loss: 0.00082598
Iteration 11/25 | Loss: 0.00082598
Iteration 12/25 | Loss: 0.00082598
Iteration 13/25 | Loss: 0.00082598
Iteration 14/25 | Loss: 0.00082598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008259792812168598, 0.0008259792812168598, 0.0008259792812168598, 0.0008259792812168598, 0.0008259792812168598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008259792812168598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23349142
Iteration 2/25 | Loss: 0.00046167
Iteration 3/25 | Loss: 0.00046164
Iteration 4/25 | Loss: 0.00046164
Iteration 5/25 | Loss: 0.00046164
Iteration 6/25 | Loss: 0.00046164
Iteration 7/25 | Loss: 0.00046164
Iteration 8/25 | Loss: 0.00046164
Iteration 9/25 | Loss: 0.00046164
Iteration 10/25 | Loss: 0.00046164
Iteration 11/25 | Loss: 0.00046164
Iteration 12/25 | Loss: 0.00046164
Iteration 13/25 | Loss: 0.00046164
Iteration 14/25 | Loss: 0.00046164
Iteration 15/25 | Loss: 0.00046164
Iteration 16/25 | Loss: 0.00046164
Iteration 17/25 | Loss: 0.00046164
Iteration 18/25 | Loss: 0.00046164
Iteration 19/25 | Loss: 0.00046164
Iteration 20/25 | Loss: 0.00046164
Iteration 21/25 | Loss: 0.00046164
Iteration 22/25 | Loss: 0.00046164
Iteration 23/25 | Loss: 0.00046164
Iteration 24/25 | Loss: 0.00046164
Iteration 25/25 | Loss: 0.00046164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046164
Iteration 2/1000 | Loss: 0.00005607
Iteration 3/1000 | Loss: 0.00004072
Iteration 4/1000 | Loss: 0.00003653
Iteration 5/1000 | Loss: 0.00003513
Iteration 6/1000 | Loss: 0.00003409
Iteration 7/1000 | Loss: 0.00003343
Iteration 8/1000 | Loss: 0.00003288
Iteration 9/1000 | Loss: 0.00003244
Iteration 10/1000 | Loss: 0.00003209
Iteration 11/1000 | Loss: 0.00003185
Iteration 12/1000 | Loss: 0.00003165
Iteration 13/1000 | Loss: 0.00003148
Iteration 14/1000 | Loss: 0.00003135
Iteration 15/1000 | Loss: 0.00003122
Iteration 16/1000 | Loss: 0.00003120
Iteration 17/1000 | Loss: 0.00003115
Iteration 18/1000 | Loss: 0.00003111
Iteration 19/1000 | Loss: 0.00003104
Iteration 20/1000 | Loss: 0.00003100
Iteration 21/1000 | Loss: 0.00003098
Iteration 22/1000 | Loss: 0.00003098
Iteration 23/1000 | Loss: 0.00003095
Iteration 24/1000 | Loss: 0.00003095
Iteration 25/1000 | Loss: 0.00003095
Iteration 26/1000 | Loss: 0.00003095
Iteration 27/1000 | Loss: 0.00003095
Iteration 28/1000 | Loss: 0.00003094
Iteration 29/1000 | Loss: 0.00003094
Iteration 30/1000 | Loss: 0.00003094
Iteration 31/1000 | Loss: 0.00003094
Iteration 32/1000 | Loss: 0.00003092
Iteration 33/1000 | Loss: 0.00003091
Iteration 34/1000 | Loss: 0.00003090
Iteration 35/1000 | Loss: 0.00003090
Iteration 36/1000 | Loss: 0.00003089
Iteration 37/1000 | Loss: 0.00003088
Iteration 38/1000 | Loss: 0.00003086
Iteration 39/1000 | Loss: 0.00003085
Iteration 40/1000 | Loss: 0.00003084
Iteration 41/1000 | Loss: 0.00003081
Iteration 42/1000 | Loss: 0.00003081
Iteration 43/1000 | Loss: 0.00003080
Iteration 44/1000 | Loss: 0.00003080
Iteration 45/1000 | Loss: 0.00003079
Iteration 46/1000 | Loss: 0.00003079
Iteration 47/1000 | Loss: 0.00003078
Iteration 48/1000 | Loss: 0.00003078
Iteration 49/1000 | Loss: 0.00003078
Iteration 50/1000 | Loss: 0.00003078
Iteration 51/1000 | Loss: 0.00003078
Iteration 52/1000 | Loss: 0.00003078
Iteration 53/1000 | Loss: 0.00003077
Iteration 54/1000 | Loss: 0.00003077
Iteration 55/1000 | Loss: 0.00003075
Iteration 56/1000 | Loss: 0.00003075
Iteration 57/1000 | Loss: 0.00003075
Iteration 58/1000 | Loss: 0.00003074
Iteration 59/1000 | Loss: 0.00003074
Iteration 60/1000 | Loss: 0.00003074
Iteration 61/1000 | Loss: 0.00003073
Iteration 62/1000 | Loss: 0.00003073
Iteration 63/1000 | Loss: 0.00003073
Iteration 64/1000 | Loss: 0.00003073
Iteration 65/1000 | Loss: 0.00003073
Iteration 66/1000 | Loss: 0.00003073
Iteration 67/1000 | Loss: 0.00003073
Iteration 68/1000 | Loss: 0.00003073
Iteration 69/1000 | Loss: 0.00003072
Iteration 70/1000 | Loss: 0.00003072
Iteration 71/1000 | Loss: 0.00003070
Iteration 72/1000 | Loss: 0.00003070
Iteration 73/1000 | Loss: 0.00003070
Iteration 74/1000 | Loss: 0.00003069
Iteration 75/1000 | Loss: 0.00003069
Iteration 76/1000 | Loss: 0.00003069
Iteration 77/1000 | Loss: 0.00003068
Iteration 78/1000 | Loss: 0.00003067
Iteration 79/1000 | Loss: 0.00003065
Iteration 80/1000 | Loss: 0.00003065
Iteration 81/1000 | Loss: 0.00003065
Iteration 82/1000 | Loss: 0.00003064
Iteration 83/1000 | Loss: 0.00003064
Iteration 84/1000 | Loss: 0.00003064
Iteration 85/1000 | Loss: 0.00003064
Iteration 86/1000 | Loss: 0.00003063
Iteration 87/1000 | Loss: 0.00003063
Iteration 88/1000 | Loss: 0.00003063
Iteration 89/1000 | Loss: 0.00003063
Iteration 90/1000 | Loss: 0.00003062
Iteration 91/1000 | Loss: 0.00003062
Iteration 92/1000 | Loss: 0.00003062
Iteration 93/1000 | Loss: 0.00003061
Iteration 94/1000 | Loss: 0.00003061
Iteration 95/1000 | Loss: 0.00003061
Iteration 96/1000 | Loss: 0.00003061
Iteration 97/1000 | Loss: 0.00003060
Iteration 98/1000 | Loss: 0.00003060
Iteration 99/1000 | Loss: 0.00003060
Iteration 100/1000 | Loss: 0.00003059
Iteration 101/1000 | Loss: 0.00003059
Iteration 102/1000 | Loss: 0.00003058
Iteration 103/1000 | Loss: 0.00003058
Iteration 104/1000 | Loss: 0.00003058
Iteration 105/1000 | Loss: 0.00003058
Iteration 106/1000 | Loss: 0.00003057
Iteration 107/1000 | Loss: 0.00003057
Iteration 108/1000 | Loss: 0.00003056
Iteration 109/1000 | Loss: 0.00003056
Iteration 110/1000 | Loss: 0.00003056
Iteration 111/1000 | Loss: 0.00003056
Iteration 112/1000 | Loss: 0.00003056
Iteration 113/1000 | Loss: 0.00003056
Iteration 114/1000 | Loss: 0.00003056
Iteration 115/1000 | Loss: 0.00003056
Iteration 116/1000 | Loss: 0.00003056
Iteration 117/1000 | Loss: 0.00003055
Iteration 118/1000 | Loss: 0.00003055
Iteration 119/1000 | Loss: 0.00003055
Iteration 120/1000 | Loss: 0.00003055
Iteration 121/1000 | Loss: 0.00003055
Iteration 122/1000 | Loss: 0.00003054
Iteration 123/1000 | Loss: 0.00003054
Iteration 124/1000 | Loss: 0.00003054
Iteration 125/1000 | Loss: 0.00003054
Iteration 126/1000 | Loss: 0.00003054
Iteration 127/1000 | Loss: 0.00003054
Iteration 128/1000 | Loss: 0.00003054
Iteration 129/1000 | Loss: 0.00003054
Iteration 130/1000 | Loss: 0.00003054
Iteration 131/1000 | Loss: 0.00003054
Iteration 132/1000 | Loss: 0.00003054
Iteration 133/1000 | Loss: 0.00003054
Iteration 134/1000 | Loss: 0.00003054
Iteration 135/1000 | Loss: 0.00003054
Iteration 136/1000 | Loss: 0.00003054
Iteration 137/1000 | Loss: 0.00003054
Iteration 138/1000 | Loss: 0.00003054
Iteration 139/1000 | Loss: 0.00003054
Iteration 140/1000 | Loss: 0.00003054
Iteration 141/1000 | Loss: 0.00003054
Iteration 142/1000 | Loss: 0.00003054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [3.054398257518187e-05, 3.054398257518187e-05, 3.054398257518187e-05, 3.054398257518187e-05, 3.054398257518187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.054398257518187e-05

Optimization complete. Final v2v error: 4.363647937774658 mm

Highest mean error: 6.245952606201172 mm for frame 123

Lowest mean error: 3.2399351596832275 mm for frame 233

Saving results

Total time: 55.384835720062256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841263
Iteration 2/25 | Loss: 0.00096932
Iteration 3/25 | Loss: 0.00075943
Iteration 4/25 | Loss: 0.00071761
Iteration 5/25 | Loss: 0.00070583
Iteration 6/25 | Loss: 0.00070163
Iteration 7/25 | Loss: 0.00070429
Iteration 8/25 | Loss: 0.00070557
Iteration 9/25 | Loss: 0.00070154
Iteration 10/25 | Loss: 0.00070475
Iteration 11/25 | Loss: 0.00070091
Iteration 12/25 | Loss: 0.00069603
Iteration 13/25 | Loss: 0.00069591
Iteration 14/25 | Loss: 0.00069722
Iteration 15/25 | Loss: 0.00069691
Iteration 16/25 | Loss: 0.00069807
Iteration 17/25 | Loss: 0.00069507
Iteration 18/25 | Loss: 0.00069594
Iteration 19/25 | Loss: 0.00069622
Iteration 20/25 | Loss: 0.00069706
Iteration 21/25 | Loss: 0.00069714
Iteration 22/25 | Loss: 0.00069617
Iteration 23/25 | Loss: 0.00069519
Iteration 24/25 | Loss: 0.00069631
Iteration 25/25 | Loss: 0.00069714

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46272922
Iteration 2/25 | Loss: 0.00143779
Iteration 3/25 | Loss: 0.00143778
Iteration 4/25 | Loss: 0.00143778
Iteration 5/25 | Loss: 0.00143778
Iteration 6/25 | Loss: 0.00143778
Iteration 7/25 | Loss: 0.00143778
Iteration 8/25 | Loss: 0.00143778
Iteration 9/25 | Loss: 0.00143778
Iteration 10/25 | Loss: 0.00143778
Iteration 11/25 | Loss: 0.00143778
Iteration 12/25 | Loss: 0.00143778
Iteration 13/25 | Loss: 0.00143778
Iteration 14/25 | Loss: 0.00143778
Iteration 15/25 | Loss: 0.00143778
Iteration 16/25 | Loss: 0.00143778
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014377806801348925, 0.0014377806801348925, 0.0014377806801348925, 0.0014377806801348925, 0.0014377806801348925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014377806801348925

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143778
Iteration 2/1000 | Loss: 0.00038234
Iteration 3/1000 | Loss: 0.00016969
Iteration 4/1000 | Loss: 0.00014130
Iteration 5/1000 | Loss: 0.00044115
Iteration 6/1000 | Loss: 0.00025645
Iteration 7/1000 | Loss: 0.00042597
Iteration 8/1000 | Loss: 0.00037573
Iteration 9/1000 | Loss: 0.00028448
Iteration 10/1000 | Loss: 0.00024468
Iteration 11/1000 | Loss: 0.00053010
Iteration 12/1000 | Loss: 0.00041523
Iteration 13/1000 | Loss: 0.00042028
Iteration 14/1000 | Loss: 0.00038862
Iteration 15/1000 | Loss: 0.00029698
Iteration 16/1000 | Loss: 0.00013735
Iteration 17/1000 | Loss: 0.00030567
Iteration 18/1000 | Loss: 0.00027977
Iteration 19/1000 | Loss: 0.00026466
Iteration 20/1000 | Loss: 0.00048515
Iteration 21/1000 | Loss: 0.00017843
Iteration 22/1000 | Loss: 0.00024203
Iteration 23/1000 | Loss: 0.00008378
Iteration 24/1000 | Loss: 0.00007528
Iteration 25/1000 | Loss: 0.00022502
Iteration 26/1000 | Loss: 0.00007334
Iteration 27/1000 | Loss: 0.00007010
Iteration 28/1000 | Loss: 0.00020484
Iteration 29/1000 | Loss: 0.00013437
Iteration 30/1000 | Loss: 0.00010057
Iteration 31/1000 | Loss: 0.00006660
Iteration 32/1000 | Loss: 0.00010913
Iteration 33/1000 | Loss: 0.00006427
Iteration 34/1000 | Loss: 0.00012620
Iteration 35/1000 | Loss: 0.00011362
Iteration 36/1000 | Loss: 0.00007448
Iteration 37/1000 | Loss: 0.00009315
Iteration 38/1000 | Loss: 0.00009795
Iteration 39/1000 | Loss: 0.00007330
Iteration 40/1000 | Loss: 0.00010396
Iteration 41/1000 | Loss: 0.00006804
Iteration 42/1000 | Loss: 0.00009458
Iteration 43/1000 | Loss: 0.00014610
Iteration 44/1000 | Loss: 0.00015978
Iteration 45/1000 | Loss: 0.00007149
Iteration 46/1000 | Loss: 0.00016590
Iteration 47/1000 | Loss: 0.00011671
Iteration 48/1000 | Loss: 0.00010452
Iteration 49/1000 | Loss: 0.00012870
Iteration 50/1000 | Loss: 0.00009128
Iteration 51/1000 | Loss: 0.00010225
Iteration 52/1000 | Loss: 0.00008058
Iteration 53/1000 | Loss: 0.00009172
Iteration 54/1000 | Loss: 0.00006257
Iteration 55/1000 | Loss: 0.00012245
Iteration 56/1000 | Loss: 0.00006168
Iteration 57/1000 | Loss: 0.00012383
Iteration 58/1000 | Loss: 0.00017715
Iteration 59/1000 | Loss: 0.00024948
Iteration 60/1000 | Loss: 0.00018517
Iteration 61/1000 | Loss: 0.00012687
Iteration 62/1000 | Loss: 0.00012003
Iteration 63/1000 | Loss: 0.00012217
Iteration 64/1000 | Loss: 0.00010124
Iteration 65/1000 | Loss: 0.00009924
Iteration 66/1000 | Loss: 0.00014571
Iteration 67/1000 | Loss: 0.00014648
Iteration 68/1000 | Loss: 0.00015748
Iteration 69/1000 | Loss: 0.00012063
Iteration 70/1000 | Loss: 0.00022603
Iteration 71/1000 | Loss: 0.00010842
Iteration 72/1000 | Loss: 0.00018293
Iteration 73/1000 | Loss: 0.00017632
Iteration 74/1000 | Loss: 0.00017926
Iteration 75/1000 | Loss: 0.00026834
Iteration 76/1000 | Loss: 0.00012479
Iteration 77/1000 | Loss: 0.00024069
Iteration 78/1000 | Loss: 0.00022779
Iteration 79/1000 | Loss: 0.00017665
Iteration 80/1000 | Loss: 0.00019230
Iteration 81/1000 | Loss: 0.00018094
Iteration 82/1000 | Loss: 0.00016647
Iteration 83/1000 | Loss: 0.00015172
Iteration 84/1000 | Loss: 0.00016084
Iteration 85/1000 | Loss: 0.00067368
Iteration 86/1000 | Loss: 0.00044471
Iteration 87/1000 | Loss: 0.00014813
Iteration 88/1000 | Loss: 0.00014084
Iteration 89/1000 | Loss: 0.00006905
Iteration 90/1000 | Loss: 0.00053767
Iteration 91/1000 | Loss: 0.00032400
Iteration 92/1000 | Loss: 0.00017539
Iteration 93/1000 | Loss: 0.00012742
Iteration 94/1000 | Loss: 0.00017127
Iteration 95/1000 | Loss: 0.00018284
Iteration 96/1000 | Loss: 0.00015325
Iteration 97/1000 | Loss: 0.00026241
Iteration 98/1000 | Loss: 0.00010350
Iteration 99/1000 | Loss: 0.00008123
Iteration 100/1000 | Loss: 0.00006329
Iteration 101/1000 | Loss: 0.00062631
Iteration 102/1000 | Loss: 0.00016478
Iteration 103/1000 | Loss: 0.00009961
Iteration 104/1000 | Loss: 0.00007606
Iteration 105/1000 | Loss: 0.00018492
Iteration 106/1000 | Loss: 0.00017641
Iteration 107/1000 | Loss: 0.00010257
Iteration 108/1000 | Loss: 0.00021425
Iteration 109/1000 | Loss: 0.00015872
Iteration 110/1000 | Loss: 0.00016800
Iteration 111/1000 | Loss: 0.00074958
Iteration 112/1000 | Loss: 0.00007460
Iteration 113/1000 | Loss: 0.00019452
Iteration 114/1000 | Loss: 0.00066845
Iteration 115/1000 | Loss: 0.00006983
Iteration 116/1000 | Loss: 0.00006251
Iteration 117/1000 | Loss: 0.00005836
Iteration 118/1000 | Loss: 0.00005557
Iteration 119/1000 | Loss: 0.00005434
Iteration 120/1000 | Loss: 0.00005348
Iteration 121/1000 | Loss: 0.00005299
Iteration 122/1000 | Loss: 0.00005770
Iteration 123/1000 | Loss: 0.00063492
Iteration 124/1000 | Loss: 0.00033579
Iteration 125/1000 | Loss: 0.00005714
Iteration 126/1000 | Loss: 0.00064012
Iteration 127/1000 | Loss: 0.00006402
Iteration 128/1000 | Loss: 0.00005106
Iteration 129/1000 | Loss: 0.00005020
Iteration 130/1000 | Loss: 0.00004957
Iteration 131/1000 | Loss: 0.00004900
Iteration 132/1000 | Loss: 0.00004843
Iteration 133/1000 | Loss: 0.00004803
Iteration 134/1000 | Loss: 0.00004765
Iteration 135/1000 | Loss: 0.00004735
Iteration 136/1000 | Loss: 0.00004713
Iteration 137/1000 | Loss: 0.00051632
Iteration 138/1000 | Loss: 0.00043471
Iteration 139/1000 | Loss: 0.00006053
Iteration 140/1000 | Loss: 0.00004821
Iteration 141/1000 | Loss: 0.00004459
Iteration 142/1000 | Loss: 0.00004355
Iteration 143/1000 | Loss: 0.00004315
Iteration 144/1000 | Loss: 0.00004283
Iteration 145/1000 | Loss: 0.00004255
Iteration 146/1000 | Loss: 0.00004234
Iteration 147/1000 | Loss: 0.00004221
Iteration 148/1000 | Loss: 0.00055553
Iteration 149/1000 | Loss: 0.00005234
Iteration 150/1000 | Loss: 0.00004199
Iteration 151/1000 | Loss: 0.00004079
Iteration 152/1000 | Loss: 0.00004042
Iteration 153/1000 | Loss: 0.00004021
Iteration 154/1000 | Loss: 0.00004012
Iteration 155/1000 | Loss: 0.00003993
Iteration 156/1000 | Loss: 0.00003990
Iteration 157/1000 | Loss: 0.00003989
Iteration 158/1000 | Loss: 0.00052427
Iteration 159/1000 | Loss: 0.00066630
Iteration 160/1000 | Loss: 0.00032040
Iteration 161/1000 | Loss: 0.00004165
Iteration 162/1000 | Loss: 0.00003904
Iteration 163/1000 | Loss: 0.00003834
Iteration 164/1000 | Loss: 0.00003789
Iteration 165/1000 | Loss: 0.00003785
Iteration 166/1000 | Loss: 0.00003783
Iteration 167/1000 | Loss: 0.00003766
Iteration 168/1000 | Loss: 0.00003766
Iteration 169/1000 | Loss: 0.00003765
Iteration 170/1000 | Loss: 0.00003765
Iteration 171/1000 | Loss: 0.00003765
Iteration 172/1000 | Loss: 0.00003764
Iteration 173/1000 | Loss: 0.00003764
Iteration 174/1000 | Loss: 0.00003763
Iteration 175/1000 | Loss: 0.00003760
Iteration 176/1000 | Loss: 0.00003752
Iteration 177/1000 | Loss: 0.00003747
Iteration 178/1000 | Loss: 0.00003744
Iteration 179/1000 | Loss: 0.00003741
Iteration 180/1000 | Loss: 0.00003740
Iteration 181/1000 | Loss: 0.00003740
Iteration 182/1000 | Loss: 0.00003738
Iteration 183/1000 | Loss: 0.00003738
Iteration 184/1000 | Loss: 0.00003738
Iteration 185/1000 | Loss: 0.00003738
Iteration 186/1000 | Loss: 0.00003738
Iteration 187/1000 | Loss: 0.00003738
Iteration 188/1000 | Loss: 0.00003738
Iteration 189/1000 | Loss: 0.00003737
Iteration 190/1000 | Loss: 0.00003737
Iteration 191/1000 | Loss: 0.00003737
Iteration 192/1000 | Loss: 0.00003737
Iteration 193/1000 | Loss: 0.00003737
Iteration 194/1000 | Loss: 0.00003736
Iteration 195/1000 | Loss: 0.00003735
Iteration 196/1000 | Loss: 0.00003735
Iteration 197/1000 | Loss: 0.00003734
Iteration 198/1000 | Loss: 0.00003734
Iteration 199/1000 | Loss: 0.00003733
Iteration 200/1000 | Loss: 0.00003733
Iteration 201/1000 | Loss: 0.00003733
Iteration 202/1000 | Loss: 0.00003733
Iteration 203/1000 | Loss: 0.00062142
Iteration 204/1000 | Loss: 0.00051258
Iteration 205/1000 | Loss: 0.00016528
Iteration 206/1000 | Loss: 0.00004822
Iteration 207/1000 | Loss: 0.00004256
Iteration 208/1000 | Loss: 0.00004048
Iteration 209/1000 | Loss: 0.00003915
Iteration 210/1000 | Loss: 0.00003870
Iteration 211/1000 | Loss: 0.00062548
Iteration 212/1000 | Loss: 0.00007278
Iteration 213/1000 | Loss: 0.00005132
Iteration 214/1000 | Loss: 0.00003826
Iteration 215/1000 | Loss: 0.00003663
Iteration 216/1000 | Loss: 0.00003599
Iteration 217/1000 | Loss: 0.00003582
Iteration 218/1000 | Loss: 0.00003573
Iteration 219/1000 | Loss: 0.00003567
Iteration 220/1000 | Loss: 0.00003559
Iteration 221/1000 | Loss: 0.00003557
Iteration 222/1000 | Loss: 0.00003550
Iteration 223/1000 | Loss: 0.00003549
Iteration 224/1000 | Loss: 0.00003546
Iteration 225/1000 | Loss: 0.00003543
Iteration 226/1000 | Loss: 0.00003543
Iteration 227/1000 | Loss: 0.00003542
Iteration 228/1000 | Loss: 0.00003541
Iteration 229/1000 | Loss: 0.00003541
Iteration 230/1000 | Loss: 0.00003540
Iteration 231/1000 | Loss: 0.00003540
Iteration 232/1000 | Loss: 0.00003539
Iteration 233/1000 | Loss: 0.00003539
Iteration 234/1000 | Loss: 0.00003539
Iteration 235/1000 | Loss: 0.00003538
Iteration 236/1000 | Loss: 0.00003538
Iteration 237/1000 | Loss: 0.00003538
Iteration 238/1000 | Loss: 0.00003538
Iteration 239/1000 | Loss: 0.00003538
Iteration 240/1000 | Loss: 0.00003538
Iteration 241/1000 | Loss: 0.00003538
Iteration 242/1000 | Loss: 0.00003538
Iteration 243/1000 | Loss: 0.00003537
Iteration 244/1000 | Loss: 0.00003537
Iteration 245/1000 | Loss: 0.00003537
Iteration 246/1000 | Loss: 0.00003537
Iteration 247/1000 | Loss: 0.00003537
Iteration 248/1000 | Loss: 0.00003537
Iteration 249/1000 | Loss: 0.00003537
Iteration 250/1000 | Loss: 0.00003537
Iteration 251/1000 | Loss: 0.00003537
Iteration 252/1000 | Loss: 0.00003537
Iteration 253/1000 | Loss: 0.00003537
Iteration 254/1000 | Loss: 0.00003537
Iteration 255/1000 | Loss: 0.00003537
Iteration 256/1000 | Loss: 0.00003537
Iteration 257/1000 | Loss: 0.00003537
Iteration 258/1000 | Loss: 0.00003537
Iteration 259/1000 | Loss: 0.00003537
Iteration 260/1000 | Loss: 0.00003536
Iteration 261/1000 | Loss: 0.00003536
Iteration 262/1000 | Loss: 0.00003536
Iteration 263/1000 | Loss: 0.00003536
Iteration 264/1000 | Loss: 0.00003536
Iteration 265/1000 | Loss: 0.00003536
Iteration 266/1000 | Loss: 0.00003536
Iteration 267/1000 | Loss: 0.00003536
Iteration 268/1000 | Loss: 0.00003536
Iteration 269/1000 | Loss: 0.00003536
Iteration 270/1000 | Loss: 0.00003536
Iteration 271/1000 | Loss: 0.00003536
Iteration 272/1000 | Loss: 0.00003535
Iteration 273/1000 | Loss: 0.00003535
Iteration 274/1000 | Loss: 0.00003535
Iteration 275/1000 | Loss: 0.00003535
Iteration 276/1000 | Loss: 0.00003535
Iteration 277/1000 | Loss: 0.00003535
Iteration 278/1000 | Loss: 0.00003535
Iteration 279/1000 | Loss: 0.00003535
Iteration 280/1000 | Loss: 0.00003535
Iteration 281/1000 | Loss: 0.00003535
Iteration 282/1000 | Loss: 0.00003535
Iteration 283/1000 | Loss: 0.00003535
Iteration 284/1000 | Loss: 0.00003535
Iteration 285/1000 | Loss: 0.00003535
Iteration 286/1000 | Loss: 0.00003535
Iteration 287/1000 | Loss: 0.00003535
Iteration 288/1000 | Loss: 0.00003535
Iteration 289/1000 | Loss: 0.00003535
Iteration 290/1000 | Loss: 0.00003535
Iteration 291/1000 | Loss: 0.00003535
Iteration 292/1000 | Loss: 0.00003535
Iteration 293/1000 | Loss: 0.00003535
Iteration 294/1000 | Loss: 0.00003535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [3.534873030730523e-05, 3.534873030730523e-05, 3.534873030730523e-05, 3.534873030730523e-05, 3.534873030730523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.534873030730523e-05

Optimization complete. Final v2v error: 3.1795337200164795 mm

Highest mean error: 11.193891525268555 mm for frame 85

Lowest mean error: 2.4712893962860107 mm for frame 52

Saving results

Total time: 323.95680975914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085256
Iteration 2/25 | Loss: 0.01085256
Iteration 3/25 | Loss: 0.00375614
Iteration 4/25 | Loss: 0.00238121
Iteration 5/25 | Loss: 0.00215141
Iteration 6/25 | Loss: 0.00180740
Iteration 7/25 | Loss: 0.00166300
Iteration 8/25 | Loss: 0.00153283
Iteration 9/25 | Loss: 0.00146830
Iteration 10/25 | Loss: 0.00134790
Iteration 11/25 | Loss: 0.00126706
Iteration 12/25 | Loss: 0.00122923
Iteration 13/25 | Loss: 0.00120232
Iteration 14/25 | Loss: 0.00115549
Iteration 15/25 | Loss: 0.00113938
Iteration 16/25 | Loss: 0.00111492
Iteration 17/25 | Loss: 0.00112051
Iteration 18/25 | Loss: 0.00112182
Iteration 19/25 | Loss: 0.00110858
Iteration 20/25 | Loss: 0.00110083
Iteration 21/25 | Loss: 0.00109780
Iteration 22/25 | Loss: 0.00109939
Iteration 23/25 | Loss: 0.00110291
Iteration 24/25 | Loss: 0.00110132
Iteration 25/25 | Loss: 0.00107962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40098190
Iteration 2/25 | Loss: 0.00374291
Iteration 3/25 | Loss: 0.00373459
Iteration 4/25 | Loss: 0.00373459
Iteration 5/25 | Loss: 0.00373459
Iteration 6/25 | Loss: 0.00373459
Iteration 7/25 | Loss: 0.00373459
Iteration 8/25 | Loss: 0.00373459
Iteration 9/25 | Loss: 0.00373459
Iteration 10/25 | Loss: 0.00373459
Iteration 11/25 | Loss: 0.00373459
Iteration 12/25 | Loss: 0.00373459
Iteration 13/25 | Loss: 0.00373459
Iteration 14/25 | Loss: 0.00373459
Iteration 15/25 | Loss: 0.00373459
Iteration 16/25 | Loss: 0.00373459
Iteration 17/25 | Loss: 0.00373459
Iteration 18/25 | Loss: 0.00373459
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003734587226063013, 0.003734587226063013, 0.003734587226063013, 0.003734587226063013, 0.003734587226063013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003734587226063013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00373459
Iteration 2/1000 | Loss: 0.00319163
Iteration 3/1000 | Loss: 0.00272793
Iteration 4/1000 | Loss: 0.00365673
Iteration 5/1000 | Loss: 0.00181250
Iteration 6/1000 | Loss: 0.00367449
Iteration 7/1000 | Loss: 0.00725680
Iteration 8/1000 | Loss: 0.00094530
Iteration 9/1000 | Loss: 0.00177522
Iteration 10/1000 | Loss: 0.00551566
Iteration 11/1000 | Loss: 0.00477382
Iteration 12/1000 | Loss: 0.00160856
Iteration 13/1000 | Loss: 0.00112159
Iteration 14/1000 | Loss: 0.00105192
Iteration 15/1000 | Loss: 0.00063835
Iteration 16/1000 | Loss: 0.00237786
Iteration 17/1000 | Loss: 0.00163013
Iteration 18/1000 | Loss: 0.00184333
Iteration 19/1000 | Loss: 0.00048693
Iteration 20/1000 | Loss: 0.00161515
Iteration 21/1000 | Loss: 0.00083850
Iteration 22/1000 | Loss: 0.00524725
Iteration 23/1000 | Loss: 0.00406683
Iteration 24/1000 | Loss: 0.00532726
Iteration 25/1000 | Loss: 0.00432698
Iteration 26/1000 | Loss: 0.00359472
Iteration 27/1000 | Loss: 0.00504977
Iteration 28/1000 | Loss: 0.00267181
Iteration 29/1000 | Loss: 0.00236797
Iteration 30/1000 | Loss: 0.00287263
Iteration 31/1000 | Loss: 0.00203413
Iteration 32/1000 | Loss: 0.00174729
Iteration 33/1000 | Loss: 0.00081029
Iteration 34/1000 | Loss: 0.00054140
Iteration 35/1000 | Loss: 0.00076799
Iteration 36/1000 | Loss: 0.00063119
Iteration 37/1000 | Loss: 0.00028663
Iteration 38/1000 | Loss: 0.00049787
Iteration 39/1000 | Loss: 0.00072473
Iteration 40/1000 | Loss: 0.00091973
Iteration 41/1000 | Loss: 0.00031479
Iteration 42/1000 | Loss: 0.00040691
Iteration 43/1000 | Loss: 0.00056168
Iteration 44/1000 | Loss: 0.00020829
Iteration 45/1000 | Loss: 0.00010103
Iteration 46/1000 | Loss: 0.00095077
Iteration 47/1000 | Loss: 0.00078053
Iteration 48/1000 | Loss: 0.00079723
Iteration 49/1000 | Loss: 0.00041944
Iteration 50/1000 | Loss: 0.00023152
Iteration 51/1000 | Loss: 0.00007011
Iteration 52/1000 | Loss: 0.00088481
Iteration 53/1000 | Loss: 0.00008936
Iteration 54/1000 | Loss: 0.00006435
Iteration 55/1000 | Loss: 0.00005411
Iteration 56/1000 | Loss: 0.00004870
Iteration 57/1000 | Loss: 0.00067076
Iteration 58/1000 | Loss: 0.00071550
Iteration 59/1000 | Loss: 0.00037296
Iteration 60/1000 | Loss: 0.00018114
Iteration 61/1000 | Loss: 0.00012334
Iteration 62/1000 | Loss: 0.00004287
Iteration 63/1000 | Loss: 0.00035877
Iteration 64/1000 | Loss: 0.00076293
Iteration 65/1000 | Loss: 0.00020602
Iteration 66/1000 | Loss: 0.00059070
Iteration 67/1000 | Loss: 0.00034705
Iteration 68/1000 | Loss: 0.00004014
Iteration 69/1000 | Loss: 0.00003634
Iteration 70/1000 | Loss: 0.00005066
Iteration 71/1000 | Loss: 0.00004779
Iteration 72/1000 | Loss: 0.00002920
Iteration 73/1000 | Loss: 0.00002809
Iteration 74/1000 | Loss: 0.00002751
Iteration 75/1000 | Loss: 0.00002712
Iteration 76/1000 | Loss: 0.00002675
Iteration 77/1000 | Loss: 0.00002666
Iteration 78/1000 | Loss: 0.00002641
Iteration 79/1000 | Loss: 0.00002619
Iteration 80/1000 | Loss: 0.00002600
Iteration 81/1000 | Loss: 0.00002597
Iteration 82/1000 | Loss: 0.00002582
Iteration 83/1000 | Loss: 0.00002579
Iteration 84/1000 | Loss: 0.00002576
Iteration 85/1000 | Loss: 0.00002574
Iteration 86/1000 | Loss: 0.00002574
Iteration 87/1000 | Loss: 0.00002573
Iteration 88/1000 | Loss: 0.00002573
Iteration 89/1000 | Loss: 0.00002572
Iteration 90/1000 | Loss: 0.00002571
Iteration 91/1000 | Loss: 0.00002571
Iteration 92/1000 | Loss: 0.00075777
Iteration 93/1000 | Loss: 0.00005789
Iteration 94/1000 | Loss: 0.00003109
Iteration 95/1000 | Loss: 0.00002588
Iteration 96/1000 | Loss: 0.00002471
Iteration 97/1000 | Loss: 0.00002385
Iteration 98/1000 | Loss: 0.00004849
Iteration 99/1000 | Loss: 0.00002324
Iteration 100/1000 | Loss: 0.00002305
Iteration 101/1000 | Loss: 0.00002300
Iteration 102/1000 | Loss: 0.00002299
Iteration 103/1000 | Loss: 0.00002297
Iteration 104/1000 | Loss: 0.00002297
Iteration 105/1000 | Loss: 0.00002296
Iteration 106/1000 | Loss: 0.00002296
Iteration 107/1000 | Loss: 0.00002294
Iteration 108/1000 | Loss: 0.00002294
Iteration 109/1000 | Loss: 0.00002291
Iteration 110/1000 | Loss: 0.00002290
Iteration 111/1000 | Loss: 0.00002289
Iteration 112/1000 | Loss: 0.00002289
Iteration 113/1000 | Loss: 0.00002288
Iteration 114/1000 | Loss: 0.00002285
Iteration 115/1000 | Loss: 0.00002285
Iteration 116/1000 | Loss: 0.00002285
Iteration 117/1000 | Loss: 0.00002285
Iteration 118/1000 | Loss: 0.00002285
Iteration 119/1000 | Loss: 0.00002285
Iteration 120/1000 | Loss: 0.00002284
Iteration 121/1000 | Loss: 0.00002284
Iteration 122/1000 | Loss: 0.00002283
Iteration 123/1000 | Loss: 0.00002283
Iteration 124/1000 | Loss: 0.00002282
Iteration 125/1000 | Loss: 0.00002282
Iteration 126/1000 | Loss: 0.00002282
Iteration 127/1000 | Loss: 0.00002281
Iteration 128/1000 | Loss: 0.00002281
Iteration 129/1000 | Loss: 0.00002281
Iteration 130/1000 | Loss: 0.00002280
Iteration 131/1000 | Loss: 0.00002280
Iteration 132/1000 | Loss: 0.00002280
Iteration 133/1000 | Loss: 0.00002280
Iteration 134/1000 | Loss: 0.00002280
Iteration 135/1000 | Loss: 0.00002280
Iteration 136/1000 | Loss: 0.00002280
Iteration 137/1000 | Loss: 0.00002280
Iteration 138/1000 | Loss: 0.00002280
Iteration 139/1000 | Loss: 0.00002280
Iteration 140/1000 | Loss: 0.00002280
Iteration 141/1000 | Loss: 0.00002279
Iteration 142/1000 | Loss: 0.00002279
Iteration 143/1000 | Loss: 0.00002279
Iteration 144/1000 | Loss: 0.00002279
Iteration 145/1000 | Loss: 0.00002278
Iteration 146/1000 | Loss: 0.00002278
Iteration 147/1000 | Loss: 0.00002278
Iteration 148/1000 | Loss: 0.00002277
Iteration 149/1000 | Loss: 0.00002277
Iteration 150/1000 | Loss: 0.00002277
Iteration 151/1000 | Loss: 0.00002277
Iteration 152/1000 | Loss: 0.00002277
Iteration 153/1000 | Loss: 0.00002277
Iteration 154/1000 | Loss: 0.00002277
Iteration 155/1000 | Loss: 0.00002277
Iteration 156/1000 | Loss: 0.00002277
Iteration 157/1000 | Loss: 0.00002277
Iteration 158/1000 | Loss: 0.00002277
Iteration 159/1000 | Loss: 0.00002276
Iteration 160/1000 | Loss: 0.00002276
Iteration 161/1000 | Loss: 0.00002276
Iteration 162/1000 | Loss: 0.00002276
Iteration 163/1000 | Loss: 0.00002276
Iteration 164/1000 | Loss: 0.00002276
Iteration 165/1000 | Loss: 0.00002276
Iteration 166/1000 | Loss: 0.00002275
Iteration 167/1000 | Loss: 0.00002275
Iteration 168/1000 | Loss: 0.00002275
Iteration 169/1000 | Loss: 0.00002275
Iteration 170/1000 | Loss: 0.00002275
Iteration 171/1000 | Loss: 0.00002275
Iteration 172/1000 | Loss: 0.00002275
Iteration 173/1000 | Loss: 0.00002275
Iteration 174/1000 | Loss: 0.00002275
Iteration 175/1000 | Loss: 0.00002275
Iteration 176/1000 | Loss: 0.00002275
Iteration 177/1000 | Loss: 0.00002275
Iteration 178/1000 | Loss: 0.00002275
Iteration 179/1000 | Loss: 0.00002275
Iteration 180/1000 | Loss: 0.00002275
Iteration 181/1000 | Loss: 0.00002275
Iteration 182/1000 | Loss: 0.00002275
Iteration 183/1000 | Loss: 0.00002275
Iteration 184/1000 | Loss: 0.00002275
Iteration 185/1000 | Loss: 0.00002275
Iteration 186/1000 | Loss: 0.00002275
Iteration 187/1000 | Loss: 0.00002275
Iteration 188/1000 | Loss: 0.00002275
Iteration 189/1000 | Loss: 0.00002275
Iteration 190/1000 | Loss: 0.00002275
Iteration 191/1000 | Loss: 0.00002275
Iteration 192/1000 | Loss: 0.00002275
Iteration 193/1000 | Loss: 0.00002275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.274606049468275e-05, 2.274606049468275e-05, 2.274606049468275e-05, 2.274606049468275e-05, 2.274606049468275e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.274606049468275e-05

Optimization complete. Final v2v error: 3.9138076305389404 mm

Highest mean error: 5.846292972564697 mm for frame 66

Lowest mean error: 3.3357112407684326 mm for frame 125

Saving results

Total time: 203.44044542312622
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489711
Iteration 2/25 | Loss: 0.00092812
Iteration 3/25 | Loss: 0.00067272
Iteration 4/25 | Loss: 0.00063849
Iteration 5/25 | Loss: 0.00063074
Iteration 6/25 | Loss: 0.00062891
Iteration 7/25 | Loss: 0.00062854
Iteration 8/25 | Loss: 0.00062854
Iteration 9/25 | Loss: 0.00062854
Iteration 10/25 | Loss: 0.00062854
Iteration 11/25 | Loss: 0.00062854
Iteration 12/25 | Loss: 0.00062854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000628538487944752, 0.000628538487944752, 0.000628538487944752, 0.000628538487944752, 0.000628538487944752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000628538487944752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57066965
Iteration 2/25 | Loss: 0.00027065
Iteration 3/25 | Loss: 0.00027063
Iteration 4/25 | Loss: 0.00027063
Iteration 5/25 | Loss: 0.00027063
Iteration 6/25 | Loss: 0.00027063
Iteration 7/25 | Loss: 0.00027063
Iteration 8/25 | Loss: 0.00027063
Iteration 9/25 | Loss: 0.00027063
Iteration 10/25 | Loss: 0.00027063
Iteration 11/25 | Loss: 0.00027063
Iteration 12/25 | Loss: 0.00027063
Iteration 13/25 | Loss: 0.00027063
Iteration 14/25 | Loss: 0.00027063
Iteration 15/25 | Loss: 0.00027063
Iteration 16/25 | Loss: 0.00027063
Iteration 17/25 | Loss: 0.00027063
Iteration 18/25 | Loss: 0.00027063
Iteration 19/25 | Loss: 0.00027063
Iteration 20/25 | Loss: 0.00027063
Iteration 21/25 | Loss: 0.00027063
Iteration 22/25 | Loss: 0.00027063
Iteration 23/25 | Loss: 0.00027063
Iteration 24/25 | Loss: 0.00027063
Iteration 25/25 | Loss: 0.00027063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027063
Iteration 2/1000 | Loss: 0.00002623
Iteration 3/1000 | Loss: 0.00001887
Iteration 4/1000 | Loss: 0.00001670
Iteration 5/1000 | Loss: 0.00001596
Iteration 6/1000 | Loss: 0.00001525
Iteration 7/1000 | Loss: 0.00001483
Iteration 8/1000 | Loss: 0.00001443
Iteration 9/1000 | Loss: 0.00001420
Iteration 10/1000 | Loss: 0.00001403
Iteration 11/1000 | Loss: 0.00001399
Iteration 12/1000 | Loss: 0.00001396
Iteration 13/1000 | Loss: 0.00001395
Iteration 14/1000 | Loss: 0.00001393
Iteration 15/1000 | Loss: 0.00001392
Iteration 16/1000 | Loss: 0.00001392
Iteration 17/1000 | Loss: 0.00001392
Iteration 18/1000 | Loss: 0.00001391
Iteration 19/1000 | Loss: 0.00001391
Iteration 20/1000 | Loss: 0.00001387
Iteration 21/1000 | Loss: 0.00001387
Iteration 22/1000 | Loss: 0.00001387
Iteration 23/1000 | Loss: 0.00001385
Iteration 24/1000 | Loss: 0.00001385
Iteration 25/1000 | Loss: 0.00001384
Iteration 26/1000 | Loss: 0.00001384
Iteration 27/1000 | Loss: 0.00001384
Iteration 28/1000 | Loss: 0.00001384
Iteration 29/1000 | Loss: 0.00001383
Iteration 30/1000 | Loss: 0.00001383
Iteration 31/1000 | Loss: 0.00001383
Iteration 32/1000 | Loss: 0.00001382
Iteration 33/1000 | Loss: 0.00001382
Iteration 34/1000 | Loss: 0.00001381
Iteration 35/1000 | Loss: 0.00001381
Iteration 36/1000 | Loss: 0.00001381
Iteration 37/1000 | Loss: 0.00001381
Iteration 38/1000 | Loss: 0.00001380
Iteration 39/1000 | Loss: 0.00001380
Iteration 40/1000 | Loss: 0.00001379
Iteration 41/1000 | Loss: 0.00001379
Iteration 42/1000 | Loss: 0.00001378
Iteration 43/1000 | Loss: 0.00001378
Iteration 44/1000 | Loss: 0.00001377
Iteration 45/1000 | Loss: 0.00001377
Iteration 46/1000 | Loss: 0.00001377
Iteration 47/1000 | Loss: 0.00001377
Iteration 48/1000 | Loss: 0.00001376
Iteration 49/1000 | Loss: 0.00001376
Iteration 50/1000 | Loss: 0.00001376
Iteration 51/1000 | Loss: 0.00001375
Iteration 52/1000 | Loss: 0.00001375
Iteration 53/1000 | Loss: 0.00001375
Iteration 54/1000 | Loss: 0.00001375
Iteration 55/1000 | Loss: 0.00001375
Iteration 56/1000 | Loss: 0.00001375
Iteration 57/1000 | Loss: 0.00001375
Iteration 58/1000 | Loss: 0.00001374
Iteration 59/1000 | Loss: 0.00001374
Iteration 60/1000 | Loss: 0.00001374
Iteration 61/1000 | Loss: 0.00001374
Iteration 62/1000 | Loss: 0.00001374
Iteration 63/1000 | Loss: 0.00001374
Iteration 64/1000 | Loss: 0.00001373
Iteration 65/1000 | Loss: 0.00001373
Iteration 66/1000 | Loss: 0.00001373
Iteration 67/1000 | Loss: 0.00001373
Iteration 68/1000 | Loss: 0.00001373
Iteration 69/1000 | Loss: 0.00001373
Iteration 70/1000 | Loss: 0.00001373
Iteration 71/1000 | Loss: 0.00001373
Iteration 72/1000 | Loss: 0.00001373
Iteration 73/1000 | Loss: 0.00001373
Iteration 74/1000 | Loss: 0.00001372
Iteration 75/1000 | Loss: 0.00001372
Iteration 76/1000 | Loss: 0.00001372
Iteration 77/1000 | Loss: 0.00001372
Iteration 78/1000 | Loss: 0.00001372
Iteration 79/1000 | Loss: 0.00001372
Iteration 80/1000 | Loss: 0.00001372
Iteration 81/1000 | Loss: 0.00001372
Iteration 82/1000 | Loss: 0.00001372
Iteration 83/1000 | Loss: 0.00001372
Iteration 84/1000 | Loss: 0.00001372
Iteration 85/1000 | Loss: 0.00001372
Iteration 86/1000 | Loss: 0.00001372
Iteration 87/1000 | Loss: 0.00001372
Iteration 88/1000 | Loss: 0.00001372
Iteration 89/1000 | Loss: 0.00001371
Iteration 90/1000 | Loss: 0.00001371
Iteration 91/1000 | Loss: 0.00001371
Iteration 92/1000 | Loss: 0.00001371
Iteration 93/1000 | Loss: 0.00001371
Iteration 94/1000 | Loss: 0.00001371
Iteration 95/1000 | Loss: 0.00001371
Iteration 96/1000 | Loss: 0.00001371
Iteration 97/1000 | Loss: 0.00001371
Iteration 98/1000 | Loss: 0.00001371
Iteration 99/1000 | Loss: 0.00001371
Iteration 100/1000 | Loss: 0.00001371
Iteration 101/1000 | Loss: 0.00001371
Iteration 102/1000 | Loss: 0.00001371
Iteration 103/1000 | Loss: 0.00001371
Iteration 104/1000 | Loss: 0.00001371
Iteration 105/1000 | Loss: 0.00001371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.3708860024053138e-05, 1.3708860024053138e-05, 1.3708860024053138e-05, 1.3708860024053138e-05, 1.3708860024053138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3708860024053138e-05

Optimization complete. Final v2v error: 3.0769267082214355 mm

Highest mean error: 4.205375671386719 mm for frame 110

Lowest mean error: 2.546005964279175 mm for frame 17

Saving results

Total time: 32.5653121471405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842640
Iteration 2/25 | Loss: 0.00102098
Iteration 3/25 | Loss: 0.00087389
Iteration 4/25 | Loss: 0.00081109
Iteration 5/25 | Loss: 0.00078927
Iteration 6/25 | Loss: 0.00078445
Iteration 7/25 | Loss: 0.00078192
Iteration 8/25 | Loss: 0.00078162
Iteration 9/25 | Loss: 0.00078162
Iteration 10/25 | Loss: 0.00078162
Iteration 11/25 | Loss: 0.00078162
Iteration 12/25 | Loss: 0.00078162
Iteration 13/25 | Loss: 0.00078162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007816198049113154, 0.0007816198049113154, 0.0007816198049113154, 0.0007816198049113154, 0.0007816198049113154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007816198049113154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55751419
Iteration 2/25 | Loss: 0.00037334
Iteration 3/25 | Loss: 0.00037333
Iteration 4/25 | Loss: 0.00037333
Iteration 5/25 | Loss: 0.00037333
Iteration 6/25 | Loss: 0.00037333
Iteration 7/25 | Loss: 0.00037333
Iteration 8/25 | Loss: 0.00037333
Iteration 9/25 | Loss: 0.00037333
Iteration 10/25 | Loss: 0.00037333
Iteration 11/25 | Loss: 0.00037333
Iteration 12/25 | Loss: 0.00037333
Iteration 13/25 | Loss: 0.00037333
Iteration 14/25 | Loss: 0.00037333
Iteration 15/25 | Loss: 0.00037333
Iteration 16/25 | Loss: 0.00037333
Iteration 17/25 | Loss: 0.00037333
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00037333116051740944, 0.00037333116051740944, 0.00037333116051740944, 0.00037333116051740944, 0.00037333116051740944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037333116051740944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037333
Iteration 2/1000 | Loss: 0.00006989
Iteration 3/1000 | Loss: 0.00004531
Iteration 4/1000 | Loss: 0.00003640
Iteration 5/1000 | Loss: 0.00003442
Iteration 6/1000 | Loss: 0.00003363
Iteration 7/1000 | Loss: 0.00003294
Iteration 8/1000 | Loss: 0.00003234
Iteration 9/1000 | Loss: 0.00003184
Iteration 10/1000 | Loss: 0.00003144
Iteration 11/1000 | Loss: 0.00003116
Iteration 12/1000 | Loss: 0.00003104
Iteration 13/1000 | Loss: 0.00003103
Iteration 14/1000 | Loss: 0.00003094
Iteration 15/1000 | Loss: 0.00003094
Iteration 16/1000 | Loss: 0.00003082
Iteration 17/1000 | Loss: 0.00003081
Iteration 18/1000 | Loss: 0.00003081
Iteration 19/1000 | Loss: 0.00003080
Iteration 20/1000 | Loss: 0.00003075
Iteration 21/1000 | Loss: 0.00003074
Iteration 22/1000 | Loss: 0.00003071
Iteration 23/1000 | Loss: 0.00003071
Iteration 24/1000 | Loss: 0.00003071
Iteration 25/1000 | Loss: 0.00003070
Iteration 26/1000 | Loss: 0.00003066
Iteration 27/1000 | Loss: 0.00003066
Iteration 28/1000 | Loss: 0.00003066
Iteration 29/1000 | Loss: 0.00003065
Iteration 30/1000 | Loss: 0.00003065
Iteration 31/1000 | Loss: 0.00003065
Iteration 32/1000 | Loss: 0.00003064
Iteration 33/1000 | Loss: 0.00003064
Iteration 34/1000 | Loss: 0.00003063
Iteration 35/1000 | Loss: 0.00003062
Iteration 36/1000 | Loss: 0.00003062
Iteration 37/1000 | Loss: 0.00003062
Iteration 38/1000 | Loss: 0.00003062
Iteration 39/1000 | Loss: 0.00003062
Iteration 40/1000 | Loss: 0.00003062
Iteration 41/1000 | Loss: 0.00003061
Iteration 42/1000 | Loss: 0.00003061
Iteration 43/1000 | Loss: 0.00003061
Iteration 44/1000 | Loss: 0.00003061
Iteration 45/1000 | Loss: 0.00003061
Iteration 46/1000 | Loss: 0.00003061
Iteration 47/1000 | Loss: 0.00003061
Iteration 48/1000 | Loss: 0.00003061
Iteration 49/1000 | Loss: 0.00003060
Iteration 50/1000 | Loss: 0.00003060
Iteration 51/1000 | Loss: 0.00003060
Iteration 52/1000 | Loss: 0.00003059
Iteration 53/1000 | Loss: 0.00003059
Iteration 54/1000 | Loss: 0.00003059
Iteration 55/1000 | Loss: 0.00003059
Iteration 56/1000 | Loss: 0.00003059
Iteration 57/1000 | Loss: 0.00003058
Iteration 58/1000 | Loss: 0.00003058
Iteration 59/1000 | Loss: 0.00003058
Iteration 60/1000 | Loss: 0.00003057
Iteration 61/1000 | Loss: 0.00003057
Iteration 62/1000 | Loss: 0.00003057
Iteration 63/1000 | Loss: 0.00003057
Iteration 64/1000 | Loss: 0.00003057
Iteration 65/1000 | Loss: 0.00003057
Iteration 66/1000 | Loss: 0.00003056
Iteration 67/1000 | Loss: 0.00003056
Iteration 68/1000 | Loss: 0.00003056
Iteration 69/1000 | Loss: 0.00003055
Iteration 70/1000 | Loss: 0.00003055
Iteration 71/1000 | Loss: 0.00003055
Iteration 72/1000 | Loss: 0.00003055
Iteration 73/1000 | Loss: 0.00003054
Iteration 74/1000 | Loss: 0.00003054
Iteration 75/1000 | Loss: 0.00003054
Iteration 76/1000 | Loss: 0.00003054
Iteration 77/1000 | Loss: 0.00003054
Iteration 78/1000 | Loss: 0.00003054
Iteration 79/1000 | Loss: 0.00003054
Iteration 80/1000 | Loss: 0.00003053
Iteration 81/1000 | Loss: 0.00003053
Iteration 82/1000 | Loss: 0.00003053
Iteration 83/1000 | Loss: 0.00003053
Iteration 84/1000 | Loss: 0.00003053
Iteration 85/1000 | Loss: 0.00003052
Iteration 86/1000 | Loss: 0.00003052
Iteration 87/1000 | Loss: 0.00003052
Iteration 88/1000 | Loss: 0.00003052
Iteration 89/1000 | Loss: 0.00003051
Iteration 90/1000 | Loss: 0.00003051
Iteration 91/1000 | Loss: 0.00003051
Iteration 92/1000 | Loss: 0.00003051
Iteration 93/1000 | Loss: 0.00003051
Iteration 94/1000 | Loss: 0.00003051
Iteration 95/1000 | Loss: 0.00003051
Iteration 96/1000 | Loss: 0.00003050
Iteration 97/1000 | Loss: 0.00003050
Iteration 98/1000 | Loss: 0.00003050
Iteration 99/1000 | Loss: 0.00003050
Iteration 100/1000 | Loss: 0.00003049
Iteration 101/1000 | Loss: 0.00003049
Iteration 102/1000 | Loss: 0.00003049
Iteration 103/1000 | Loss: 0.00003049
Iteration 104/1000 | Loss: 0.00003049
Iteration 105/1000 | Loss: 0.00003048
Iteration 106/1000 | Loss: 0.00003048
Iteration 107/1000 | Loss: 0.00003048
Iteration 108/1000 | Loss: 0.00003048
Iteration 109/1000 | Loss: 0.00003048
Iteration 110/1000 | Loss: 0.00003048
Iteration 111/1000 | Loss: 0.00003048
Iteration 112/1000 | Loss: 0.00003048
Iteration 113/1000 | Loss: 0.00003048
Iteration 114/1000 | Loss: 0.00003048
Iteration 115/1000 | Loss: 0.00003048
Iteration 116/1000 | Loss: 0.00003048
Iteration 117/1000 | Loss: 0.00003048
Iteration 118/1000 | Loss: 0.00003048
Iteration 119/1000 | Loss: 0.00003047
Iteration 120/1000 | Loss: 0.00003047
Iteration 121/1000 | Loss: 0.00003047
Iteration 122/1000 | Loss: 0.00003047
Iteration 123/1000 | Loss: 0.00003047
Iteration 124/1000 | Loss: 0.00003047
Iteration 125/1000 | Loss: 0.00003047
Iteration 126/1000 | Loss: 0.00003047
Iteration 127/1000 | Loss: 0.00003047
Iteration 128/1000 | Loss: 0.00003047
Iteration 129/1000 | Loss: 0.00003047
Iteration 130/1000 | Loss: 0.00003047
Iteration 131/1000 | Loss: 0.00003047
Iteration 132/1000 | Loss: 0.00003047
Iteration 133/1000 | Loss: 0.00003046
Iteration 134/1000 | Loss: 0.00003046
Iteration 135/1000 | Loss: 0.00003046
Iteration 136/1000 | Loss: 0.00003046
Iteration 137/1000 | Loss: 0.00003046
Iteration 138/1000 | Loss: 0.00003046
Iteration 139/1000 | Loss: 0.00003046
Iteration 140/1000 | Loss: 0.00003046
Iteration 141/1000 | Loss: 0.00003046
Iteration 142/1000 | Loss: 0.00003046
Iteration 143/1000 | Loss: 0.00003046
Iteration 144/1000 | Loss: 0.00003046
Iteration 145/1000 | Loss: 0.00003046
Iteration 146/1000 | Loss: 0.00003046
Iteration 147/1000 | Loss: 0.00003045
Iteration 148/1000 | Loss: 0.00003045
Iteration 149/1000 | Loss: 0.00003045
Iteration 150/1000 | Loss: 0.00003045
Iteration 151/1000 | Loss: 0.00003045
Iteration 152/1000 | Loss: 0.00003045
Iteration 153/1000 | Loss: 0.00003045
Iteration 154/1000 | Loss: 0.00003045
Iteration 155/1000 | Loss: 0.00003045
Iteration 156/1000 | Loss: 0.00003045
Iteration 157/1000 | Loss: 0.00003045
Iteration 158/1000 | Loss: 0.00003045
Iteration 159/1000 | Loss: 0.00003044
Iteration 160/1000 | Loss: 0.00003044
Iteration 161/1000 | Loss: 0.00003044
Iteration 162/1000 | Loss: 0.00003044
Iteration 163/1000 | Loss: 0.00003044
Iteration 164/1000 | Loss: 0.00003044
Iteration 165/1000 | Loss: 0.00003044
Iteration 166/1000 | Loss: 0.00003043
Iteration 167/1000 | Loss: 0.00003043
Iteration 168/1000 | Loss: 0.00003043
Iteration 169/1000 | Loss: 0.00003043
Iteration 170/1000 | Loss: 0.00003043
Iteration 171/1000 | Loss: 0.00003043
Iteration 172/1000 | Loss: 0.00003043
Iteration 173/1000 | Loss: 0.00003043
Iteration 174/1000 | Loss: 0.00003042
Iteration 175/1000 | Loss: 0.00003042
Iteration 176/1000 | Loss: 0.00003042
Iteration 177/1000 | Loss: 0.00003042
Iteration 178/1000 | Loss: 0.00003042
Iteration 179/1000 | Loss: 0.00003041
Iteration 180/1000 | Loss: 0.00003041
Iteration 181/1000 | Loss: 0.00003041
Iteration 182/1000 | Loss: 0.00003041
Iteration 183/1000 | Loss: 0.00003041
Iteration 184/1000 | Loss: 0.00003041
Iteration 185/1000 | Loss: 0.00003041
Iteration 186/1000 | Loss: 0.00003041
Iteration 187/1000 | Loss: 0.00003041
Iteration 188/1000 | Loss: 0.00003041
Iteration 189/1000 | Loss: 0.00003041
Iteration 190/1000 | Loss: 0.00003041
Iteration 191/1000 | Loss: 0.00003041
Iteration 192/1000 | Loss: 0.00003040
Iteration 193/1000 | Loss: 0.00003040
Iteration 194/1000 | Loss: 0.00003040
Iteration 195/1000 | Loss: 0.00003040
Iteration 196/1000 | Loss: 0.00003040
Iteration 197/1000 | Loss: 0.00003040
Iteration 198/1000 | Loss: 0.00003040
Iteration 199/1000 | Loss: 0.00003040
Iteration 200/1000 | Loss: 0.00003040
Iteration 201/1000 | Loss: 0.00003040
Iteration 202/1000 | Loss: 0.00003040
Iteration 203/1000 | Loss: 0.00003040
Iteration 204/1000 | Loss: 0.00003039
Iteration 205/1000 | Loss: 0.00003039
Iteration 206/1000 | Loss: 0.00003039
Iteration 207/1000 | Loss: 0.00003039
Iteration 208/1000 | Loss: 0.00003039
Iteration 209/1000 | Loss: 0.00003039
Iteration 210/1000 | Loss: 0.00003039
Iteration 211/1000 | Loss: 0.00003039
Iteration 212/1000 | Loss: 0.00003039
Iteration 213/1000 | Loss: 0.00003039
Iteration 214/1000 | Loss: 0.00003039
Iteration 215/1000 | Loss: 0.00003039
Iteration 216/1000 | Loss: 0.00003039
Iteration 217/1000 | Loss: 0.00003039
Iteration 218/1000 | Loss: 0.00003038
Iteration 219/1000 | Loss: 0.00003038
Iteration 220/1000 | Loss: 0.00003038
Iteration 221/1000 | Loss: 0.00003038
Iteration 222/1000 | Loss: 0.00003038
Iteration 223/1000 | Loss: 0.00003038
Iteration 224/1000 | Loss: 0.00003038
Iteration 225/1000 | Loss: 0.00003038
Iteration 226/1000 | Loss: 0.00003038
Iteration 227/1000 | Loss: 0.00003038
Iteration 228/1000 | Loss: 0.00003038
Iteration 229/1000 | Loss: 0.00003038
Iteration 230/1000 | Loss: 0.00003038
Iteration 231/1000 | Loss: 0.00003038
Iteration 232/1000 | Loss: 0.00003038
Iteration 233/1000 | Loss: 0.00003038
Iteration 234/1000 | Loss: 0.00003038
Iteration 235/1000 | Loss: 0.00003038
Iteration 236/1000 | Loss: 0.00003038
Iteration 237/1000 | Loss: 0.00003038
Iteration 238/1000 | Loss: 0.00003038
Iteration 239/1000 | Loss: 0.00003038
Iteration 240/1000 | Loss: 0.00003038
Iteration 241/1000 | Loss: 0.00003038
Iteration 242/1000 | Loss: 0.00003038
Iteration 243/1000 | Loss: 0.00003038
Iteration 244/1000 | Loss: 0.00003038
Iteration 245/1000 | Loss: 0.00003038
Iteration 246/1000 | Loss: 0.00003038
Iteration 247/1000 | Loss: 0.00003038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [3.0380668249563314e-05, 3.0380668249563314e-05, 3.0380668249563314e-05, 3.0380668249563314e-05, 3.0380668249563314e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0380668249563314e-05

Optimization complete. Final v2v error: 4.529791831970215 mm

Highest mean error: 5.131284713745117 mm for frame 76

Lowest mean error: 3.788273334503174 mm for frame 0

Saving results

Total time: 44.03911018371582
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043493
Iteration 2/25 | Loss: 0.01043493
Iteration 3/25 | Loss: 0.00214051
Iteration 4/25 | Loss: 0.00115823
Iteration 5/25 | Loss: 0.00100647
Iteration 6/25 | Loss: 0.00090634
Iteration 7/25 | Loss: 0.00088078
Iteration 8/25 | Loss: 0.00086699
Iteration 9/25 | Loss: 0.00085453
Iteration 10/25 | Loss: 0.00082760
Iteration 11/25 | Loss: 0.00081775
Iteration 12/25 | Loss: 0.00081073
Iteration 13/25 | Loss: 0.00078978
Iteration 14/25 | Loss: 0.00077999
Iteration 15/25 | Loss: 0.00076560
Iteration 16/25 | Loss: 0.00076286
Iteration 17/25 | Loss: 0.00076099
Iteration 18/25 | Loss: 0.00075979
Iteration 19/25 | Loss: 0.00075934
Iteration 20/25 | Loss: 0.00075902
Iteration 21/25 | Loss: 0.00075899
Iteration 22/25 | Loss: 0.00075899
Iteration 23/25 | Loss: 0.00075898
Iteration 24/25 | Loss: 0.00075898
Iteration 25/25 | Loss: 0.00075898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44510341
Iteration 2/25 | Loss: 0.00036712
Iteration 3/25 | Loss: 0.00029072
Iteration 4/25 | Loss: 0.00025565
Iteration 5/25 | Loss: 0.00025565
Iteration 6/25 | Loss: 0.00025565
Iteration 7/25 | Loss: 0.00025565
Iteration 8/25 | Loss: 0.00025565
Iteration 9/25 | Loss: 0.00025565
Iteration 10/25 | Loss: 0.00025565
Iteration 11/25 | Loss: 0.00025565
Iteration 12/25 | Loss: 0.00025565
Iteration 13/25 | Loss: 0.00025565
Iteration 14/25 | Loss: 0.00025565
Iteration 15/25 | Loss: 0.00025565
Iteration 16/25 | Loss: 0.00025565
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00025565040414221585, 0.00025565040414221585, 0.00025565040414221585, 0.00025565040414221585, 0.00025565040414221585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025565040414221585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025565
Iteration 2/1000 | Loss: 0.00007639
Iteration 3/1000 | Loss: 0.00014438
Iteration 4/1000 | Loss: 0.00018465
Iteration 5/1000 | Loss: 0.00003245
Iteration 6/1000 | Loss: 0.00002638
Iteration 7/1000 | Loss: 0.00006268
Iteration 8/1000 | Loss: 0.00002506
Iteration 9/1000 | Loss: 0.00016113
Iteration 10/1000 | Loss: 0.00003301
Iteration 11/1000 | Loss: 0.00002402
Iteration 12/1000 | Loss: 0.00014956
Iteration 13/1000 | Loss: 0.00039007
Iteration 14/1000 | Loss: 0.00003077
Iteration 15/1000 | Loss: 0.00003160
Iteration 16/1000 | Loss: 0.00002283
Iteration 17/1000 | Loss: 0.00005265
Iteration 18/1000 | Loss: 0.00002522
Iteration 19/1000 | Loss: 0.00002233
Iteration 20/1000 | Loss: 0.00003909
Iteration 21/1000 | Loss: 0.00002214
Iteration 22/1000 | Loss: 0.00002211
Iteration 23/1000 | Loss: 0.00004847
Iteration 24/1000 | Loss: 0.00002206
Iteration 25/1000 | Loss: 0.00002200
Iteration 26/1000 | Loss: 0.00002199
Iteration 27/1000 | Loss: 0.00002198
Iteration 28/1000 | Loss: 0.00002197
Iteration 29/1000 | Loss: 0.00002197
Iteration 30/1000 | Loss: 0.00002197
Iteration 31/1000 | Loss: 0.00002197
Iteration 32/1000 | Loss: 0.00002197
Iteration 33/1000 | Loss: 0.00002197
Iteration 34/1000 | Loss: 0.00002197
Iteration 35/1000 | Loss: 0.00002197
Iteration 36/1000 | Loss: 0.00002196
Iteration 37/1000 | Loss: 0.00002196
Iteration 38/1000 | Loss: 0.00002196
Iteration 39/1000 | Loss: 0.00002196
Iteration 40/1000 | Loss: 0.00002196
Iteration 41/1000 | Loss: 0.00007097
Iteration 42/1000 | Loss: 0.00006509
Iteration 43/1000 | Loss: 0.00017150
Iteration 44/1000 | Loss: 0.00002228
Iteration 45/1000 | Loss: 0.00002194
Iteration 46/1000 | Loss: 0.00002194
Iteration 47/1000 | Loss: 0.00006058
Iteration 48/1000 | Loss: 0.00007046
Iteration 49/1000 | Loss: 0.00005925
Iteration 50/1000 | Loss: 0.00005630
Iteration 51/1000 | Loss: 0.00002477
Iteration 52/1000 | Loss: 0.00003959
Iteration 53/1000 | Loss: 0.00002313
Iteration 54/1000 | Loss: 0.00005590
Iteration 55/1000 | Loss: 0.00002281
Iteration 56/1000 | Loss: 0.00002204
Iteration 57/1000 | Loss: 0.00002192
Iteration 58/1000 | Loss: 0.00002192
Iteration 59/1000 | Loss: 0.00002192
Iteration 60/1000 | Loss: 0.00002190
Iteration 61/1000 | Loss: 0.00002189
Iteration 62/1000 | Loss: 0.00002189
Iteration 63/1000 | Loss: 0.00002189
Iteration 64/1000 | Loss: 0.00002188
Iteration 65/1000 | Loss: 0.00002188
Iteration 66/1000 | Loss: 0.00002187
Iteration 67/1000 | Loss: 0.00002186
Iteration 68/1000 | Loss: 0.00002186
Iteration 69/1000 | Loss: 0.00002186
Iteration 70/1000 | Loss: 0.00002186
Iteration 71/1000 | Loss: 0.00002186
Iteration 72/1000 | Loss: 0.00002185
Iteration 73/1000 | Loss: 0.00002185
Iteration 74/1000 | Loss: 0.00002184
Iteration 75/1000 | Loss: 0.00002184
Iteration 76/1000 | Loss: 0.00002184
Iteration 77/1000 | Loss: 0.00002184
Iteration 78/1000 | Loss: 0.00002184
Iteration 79/1000 | Loss: 0.00002184
Iteration 80/1000 | Loss: 0.00002184
Iteration 81/1000 | Loss: 0.00002184
Iteration 82/1000 | Loss: 0.00002184
Iteration 83/1000 | Loss: 0.00002183
Iteration 84/1000 | Loss: 0.00002183
Iteration 85/1000 | Loss: 0.00002183
Iteration 86/1000 | Loss: 0.00002183
Iteration 87/1000 | Loss: 0.00002183
Iteration 88/1000 | Loss: 0.00002183
Iteration 89/1000 | Loss: 0.00002183
Iteration 90/1000 | Loss: 0.00002183
Iteration 91/1000 | Loss: 0.00002183
Iteration 92/1000 | Loss: 0.00002183
Iteration 93/1000 | Loss: 0.00002183
Iteration 94/1000 | Loss: 0.00002183
Iteration 95/1000 | Loss: 0.00002183
Iteration 96/1000 | Loss: 0.00002183
Iteration 97/1000 | Loss: 0.00002183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.1833051505382173e-05, 2.1833051505382173e-05, 2.1833051505382173e-05, 2.1833051505382173e-05, 2.1833051505382173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1833051505382173e-05

Optimization complete. Final v2v error: 3.9307045936584473 mm

Highest mean error: 10.123687744140625 mm for frame 197

Lowest mean error: 3.630557060241699 mm for frame 139

Saving results

Total time: 101.14951205253601
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899523
Iteration 2/25 | Loss: 0.00145653
Iteration 3/25 | Loss: 0.00114227
Iteration 4/25 | Loss: 0.00108592
Iteration 5/25 | Loss: 0.00107041
Iteration 6/25 | Loss: 0.00107227
Iteration 7/25 | Loss: 0.00106848
Iteration 8/25 | Loss: 0.00106245
Iteration 9/25 | Loss: 0.00104728
Iteration 10/25 | Loss: 0.00104877
Iteration 11/25 | Loss: 0.00104557
Iteration 12/25 | Loss: 0.00103623
Iteration 13/25 | Loss: 0.00102195
Iteration 14/25 | Loss: 0.00102164
Iteration 15/25 | Loss: 0.00101806
Iteration 16/25 | Loss: 0.00101560
Iteration 17/25 | Loss: 0.00101831
Iteration 18/25 | Loss: 0.00101870
Iteration 19/25 | Loss: 0.00101688
Iteration 20/25 | Loss: 0.00101088
Iteration 21/25 | Loss: 0.00100500
Iteration 22/25 | Loss: 0.00100273
Iteration 23/25 | Loss: 0.00100150
Iteration 24/25 | Loss: 0.00100104
Iteration 25/25 | Loss: 0.00100088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33644331
Iteration 2/25 | Loss: 0.00245175
Iteration 3/25 | Loss: 0.00245175
Iteration 4/25 | Loss: 0.00245174
Iteration 5/25 | Loss: 0.00245174
Iteration 6/25 | Loss: 0.00245174
Iteration 7/25 | Loss: 0.00245174
Iteration 8/25 | Loss: 0.00245174
Iteration 9/25 | Loss: 0.00245174
Iteration 10/25 | Loss: 0.00245174
Iteration 11/25 | Loss: 0.00245174
Iteration 12/25 | Loss: 0.00245174
Iteration 13/25 | Loss: 0.00245174
Iteration 14/25 | Loss: 0.00245174
Iteration 15/25 | Loss: 0.00245174
Iteration 16/25 | Loss: 0.00245174
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0024517420679330826, 0.0024517420679330826, 0.0024517420679330826, 0.0024517420679330826, 0.0024517420679330826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024517420679330826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245174
Iteration 2/1000 | Loss: 0.00044193
Iteration 3/1000 | Loss: 0.00033500
Iteration 4/1000 | Loss: 0.00027325
Iteration 5/1000 | Loss: 0.00024306
Iteration 6/1000 | Loss: 0.00033395
Iteration 7/1000 | Loss: 0.00025721
Iteration 8/1000 | Loss: 0.00024641
Iteration 9/1000 | Loss: 0.00071154
Iteration 10/1000 | Loss: 0.00019427
Iteration 11/1000 | Loss: 0.00136756
Iteration 12/1000 | Loss: 0.00041954
Iteration 13/1000 | Loss: 0.00017786
Iteration 14/1000 | Loss: 0.00016109
Iteration 15/1000 | Loss: 0.00069065
Iteration 16/1000 | Loss: 0.00015695
Iteration 17/1000 | Loss: 0.00014422
Iteration 18/1000 | Loss: 0.00043220
Iteration 19/1000 | Loss: 0.00013546
Iteration 20/1000 | Loss: 0.00012924
Iteration 21/1000 | Loss: 0.00012558
Iteration 22/1000 | Loss: 0.00012119
Iteration 23/1000 | Loss: 0.00131099
Iteration 24/1000 | Loss: 0.00012125
Iteration 25/1000 | Loss: 0.00011683
Iteration 26/1000 | Loss: 0.00128603
Iteration 27/1000 | Loss: 0.00251778
Iteration 28/1000 | Loss: 0.00298487
Iteration 29/1000 | Loss: 0.00194672
Iteration 30/1000 | Loss: 0.00125238
Iteration 31/1000 | Loss: 0.00130224
Iteration 32/1000 | Loss: 0.00099465
Iteration 33/1000 | Loss: 0.00011412
Iteration 34/1000 | Loss: 0.00010986
Iteration 35/1000 | Loss: 0.00111200
Iteration 36/1000 | Loss: 0.00140753
Iteration 37/1000 | Loss: 0.00082927
Iteration 38/1000 | Loss: 0.00099480
Iteration 39/1000 | Loss: 0.00011433
Iteration 40/1000 | Loss: 0.00010625
Iteration 41/1000 | Loss: 0.00122567
Iteration 42/1000 | Loss: 0.00010607
Iteration 43/1000 | Loss: 0.00588450
Iteration 44/1000 | Loss: 0.00843227
Iteration 45/1000 | Loss: 0.00555632
Iteration 46/1000 | Loss: 0.00214620
Iteration 47/1000 | Loss: 0.00353795
Iteration 48/1000 | Loss: 0.00112105
Iteration 49/1000 | Loss: 0.00016144
Iteration 50/1000 | Loss: 0.00011185
Iteration 51/1000 | Loss: 0.00008681
Iteration 52/1000 | Loss: 0.00061558
Iteration 53/1000 | Loss: 0.00010090
Iteration 54/1000 | Loss: 0.00014095
Iteration 55/1000 | Loss: 0.00059763
Iteration 56/1000 | Loss: 0.00005930
Iteration 57/1000 | Loss: 0.00078927
Iteration 58/1000 | Loss: 0.00032289
Iteration 59/1000 | Loss: 0.00006966
Iteration 60/1000 | Loss: 0.00094383
Iteration 61/1000 | Loss: 0.00084811
Iteration 62/1000 | Loss: 0.00008629
Iteration 63/1000 | Loss: 0.00006555
Iteration 64/1000 | Loss: 0.00038606
Iteration 65/1000 | Loss: 0.00118791
Iteration 66/1000 | Loss: 0.00007558
Iteration 67/1000 | Loss: 0.00034902
Iteration 68/1000 | Loss: 0.00104789
Iteration 69/1000 | Loss: 0.00020298
Iteration 70/1000 | Loss: 0.00011767
Iteration 71/1000 | Loss: 0.00004268
Iteration 72/1000 | Loss: 0.00004138
Iteration 73/1000 | Loss: 0.00106563
Iteration 74/1000 | Loss: 0.00004584
Iteration 75/1000 | Loss: 0.00004122
Iteration 76/1000 | Loss: 0.00003897
Iteration 77/1000 | Loss: 0.00104678
Iteration 78/1000 | Loss: 0.00072052
Iteration 79/1000 | Loss: 0.00098155
Iteration 80/1000 | Loss: 0.00043061
Iteration 81/1000 | Loss: 0.00079712
Iteration 82/1000 | Loss: 0.00036918
Iteration 83/1000 | Loss: 0.00054558
Iteration 84/1000 | Loss: 0.00003735
Iteration 85/1000 | Loss: 0.00105987
Iteration 86/1000 | Loss: 0.00010601
Iteration 87/1000 | Loss: 0.00005072
Iteration 88/1000 | Loss: 0.00003713
Iteration 89/1000 | Loss: 0.00003249
Iteration 90/1000 | Loss: 0.00003103
Iteration 91/1000 | Loss: 0.00002940
Iteration 92/1000 | Loss: 0.00002784
Iteration 93/1000 | Loss: 0.00002612
Iteration 94/1000 | Loss: 0.00002545
Iteration 95/1000 | Loss: 0.00002498
Iteration 96/1000 | Loss: 0.00002458
Iteration 97/1000 | Loss: 0.00002441
Iteration 98/1000 | Loss: 0.00002416
Iteration 99/1000 | Loss: 0.00002390
Iteration 100/1000 | Loss: 0.00002380
Iteration 101/1000 | Loss: 0.00002360
Iteration 102/1000 | Loss: 0.00002351
Iteration 103/1000 | Loss: 0.00002348
Iteration 104/1000 | Loss: 0.00002344
Iteration 105/1000 | Loss: 0.00002333
Iteration 106/1000 | Loss: 0.00002324
Iteration 107/1000 | Loss: 0.00002321
Iteration 108/1000 | Loss: 0.00002320
Iteration 109/1000 | Loss: 0.00002320
Iteration 110/1000 | Loss: 0.00002320
Iteration 111/1000 | Loss: 0.00002319
Iteration 112/1000 | Loss: 0.00002318
Iteration 113/1000 | Loss: 0.00002317
Iteration 114/1000 | Loss: 0.00002316
Iteration 115/1000 | Loss: 0.00002316
Iteration 116/1000 | Loss: 0.00002316
Iteration 117/1000 | Loss: 0.00002313
Iteration 118/1000 | Loss: 0.00002312
Iteration 119/1000 | Loss: 0.00002311
Iteration 120/1000 | Loss: 0.00002311
Iteration 121/1000 | Loss: 0.00002311
Iteration 122/1000 | Loss: 0.00002310
Iteration 123/1000 | Loss: 0.00002309
Iteration 124/1000 | Loss: 0.00002308
Iteration 125/1000 | Loss: 0.00002308
Iteration 126/1000 | Loss: 0.00002308
Iteration 127/1000 | Loss: 0.00002308
Iteration 128/1000 | Loss: 0.00002308
Iteration 129/1000 | Loss: 0.00002308
Iteration 130/1000 | Loss: 0.00002308
Iteration 131/1000 | Loss: 0.00002308
Iteration 132/1000 | Loss: 0.00002308
Iteration 133/1000 | Loss: 0.00002308
Iteration 134/1000 | Loss: 0.00002308
Iteration 135/1000 | Loss: 0.00002307
Iteration 136/1000 | Loss: 0.00002307
Iteration 137/1000 | Loss: 0.00002307
Iteration 138/1000 | Loss: 0.00002306
Iteration 139/1000 | Loss: 0.00002305
Iteration 140/1000 | Loss: 0.00002305
Iteration 141/1000 | Loss: 0.00002305
Iteration 142/1000 | Loss: 0.00002305
Iteration 143/1000 | Loss: 0.00002304
Iteration 144/1000 | Loss: 0.00002304
Iteration 145/1000 | Loss: 0.00002304
Iteration 146/1000 | Loss: 0.00002304
Iteration 147/1000 | Loss: 0.00002304
Iteration 148/1000 | Loss: 0.00002304
Iteration 149/1000 | Loss: 0.00002304
Iteration 150/1000 | Loss: 0.00002304
Iteration 151/1000 | Loss: 0.00002304
Iteration 152/1000 | Loss: 0.00002304
Iteration 153/1000 | Loss: 0.00002304
Iteration 154/1000 | Loss: 0.00002304
Iteration 155/1000 | Loss: 0.00002304
Iteration 156/1000 | Loss: 0.00002304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.3041235181153752e-05, 2.3041235181153752e-05, 2.3041235181153752e-05, 2.3041235181153752e-05, 2.3041235181153752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3041235181153752e-05

Optimization complete. Final v2v error: 3.9707343578338623 mm

Highest mean error: 4.9635515213012695 mm for frame 56

Lowest mean error: 3.7005059719085693 mm for frame 13

Saving results

Total time: 187.2273154258728
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00340364
Iteration 2/25 | Loss: 0.00070602
Iteration 3/25 | Loss: 0.00060744
Iteration 4/25 | Loss: 0.00059072
Iteration 5/25 | Loss: 0.00058399
Iteration 6/25 | Loss: 0.00058272
Iteration 7/25 | Loss: 0.00058264
Iteration 8/25 | Loss: 0.00058264
Iteration 9/25 | Loss: 0.00058264
Iteration 10/25 | Loss: 0.00058264
Iteration 11/25 | Loss: 0.00058264
Iteration 12/25 | Loss: 0.00058264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005826379638165236, 0.0005826379638165236, 0.0005826379638165236, 0.0005826379638165236, 0.0005826379638165236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005826379638165236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69373357
Iteration 2/25 | Loss: 0.00028842
Iteration 3/25 | Loss: 0.00028842
Iteration 4/25 | Loss: 0.00028842
Iteration 5/25 | Loss: 0.00028842
Iteration 6/25 | Loss: 0.00028842
Iteration 7/25 | Loss: 0.00028842
Iteration 8/25 | Loss: 0.00028842
Iteration 9/25 | Loss: 0.00028842
Iteration 10/25 | Loss: 0.00028842
Iteration 11/25 | Loss: 0.00028842
Iteration 12/25 | Loss: 0.00028842
Iteration 13/25 | Loss: 0.00028842
Iteration 14/25 | Loss: 0.00028842
Iteration 15/25 | Loss: 0.00028842
Iteration 16/25 | Loss: 0.00028842
Iteration 17/25 | Loss: 0.00028842
Iteration 18/25 | Loss: 0.00028842
Iteration 19/25 | Loss: 0.00028842
Iteration 20/25 | Loss: 0.00028842
Iteration 21/25 | Loss: 0.00028842
Iteration 22/25 | Loss: 0.00028842
Iteration 23/25 | Loss: 0.00028842
Iteration 24/25 | Loss: 0.00028842
Iteration 25/25 | Loss: 0.00028842

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028842
Iteration 2/1000 | Loss: 0.00001885
Iteration 3/1000 | Loss: 0.00001429
Iteration 4/1000 | Loss: 0.00001327
Iteration 5/1000 | Loss: 0.00001260
Iteration 6/1000 | Loss: 0.00001222
Iteration 7/1000 | Loss: 0.00001195
Iteration 8/1000 | Loss: 0.00001172
Iteration 9/1000 | Loss: 0.00001171
Iteration 10/1000 | Loss: 0.00001170
Iteration 11/1000 | Loss: 0.00001169
Iteration 12/1000 | Loss: 0.00001169
Iteration 13/1000 | Loss: 0.00001163
Iteration 14/1000 | Loss: 0.00001162
Iteration 15/1000 | Loss: 0.00001162
Iteration 16/1000 | Loss: 0.00001161
Iteration 17/1000 | Loss: 0.00001161
Iteration 18/1000 | Loss: 0.00001160
Iteration 19/1000 | Loss: 0.00001158
Iteration 20/1000 | Loss: 0.00001158
Iteration 21/1000 | Loss: 0.00001157
Iteration 22/1000 | Loss: 0.00001157
Iteration 23/1000 | Loss: 0.00001157
Iteration 24/1000 | Loss: 0.00001157
Iteration 25/1000 | Loss: 0.00001156
Iteration 26/1000 | Loss: 0.00001156
Iteration 27/1000 | Loss: 0.00001155
Iteration 28/1000 | Loss: 0.00001155
Iteration 29/1000 | Loss: 0.00001154
Iteration 30/1000 | Loss: 0.00001152
Iteration 31/1000 | Loss: 0.00001152
Iteration 32/1000 | Loss: 0.00001152
Iteration 33/1000 | Loss: 0.00001151
Iteration 34/1000 | Loss: 0.00001150
Iteration 35/1000 | Loss: 0.00001150
Iteration 36/1000 | Loss: 0.00001148
Iteration 37/1000 | Loss: 0.00001148
Iteration 38/1000 | Loss: 0.00001148
Iteration 39/1000 | Loss: 0.00001147
Iteration 40/1000 | Loss: 0.00001147
Iteration 41/1000 | Loss: 0.00001147
Iteration 42/1000 | Loss: 0.00001146
Iteration 43/1000 | Loss: 0.00001146
Iteration 44/1000 | Loss: 0.00001145
Iteration 45/1000 | Loss: 0.00001142
Iteration 46/1000 | Loss: 0.00001142
Iteration 47/1000 | Loss: 0.00001142
Iteration 48/1000 | Loss: 0.00001142
Iteration 49/1000 | Loss: 0.00001142
Iteration 50/1000 | Loss: 0.00001141
Iteration 51/1000 | Loss: 0.00001141
Iteration 52/1000 | Loss: 0.00001141
Iteration 53/1000 | Loss: 0.00001141
Iteration 54/1000 | Loss: 0.00001140
Iteration 55/1000 | Loss: 0.00001140
Iteration 56/1000 | Loss: 0.00001139
Iteration 57/1000 | Loss: 0.00001139
Iteration 58/1000 | Loss: 0.00001139
Iteration 59/1000 | Loss: 0.00001138
Iteration 60/1000 | Loss: 0.00001138
Iteration 61/1000 | Loss: 0.00001138
Iteration 62/1000 | Loss: 0.00001138
Iteration 63/1000 | Loss: 0.00001138
Iteration 64/1000 | Loss: 0.00001137
Iteration 65/1000 | Loss: 0.00001136
Iteration 66/1000 | Loss: 0.00001136
Iteration 67/1000 | Loss: 0.00001136
Iteration 68/1000 | Loss: 0.00001136
Iteration 69/1000 | Loss: 0.00001136
Iteration 70/1000 | Loss: 0.00001135
Iteration 71/1000 | Loss: 0.00001135
Iteration 72/1000 | Loss: 0.00001135
Iteration 73/1000 | Loss: 0.00001134
Iteration 74/1000 | Loss: 0.00001134
Iteration 75/1000 | Loss: 0.00001134
Iteration 76/1000 | Loss: 0.00001134
Iteration 77/1000 | Loss: 0.00001133
Iteration 78/1000 | Loss: 0.00001133
Iteration 79/1000 | Loss: 0.00001132
Iteration 80/1000 | Loss: 0.00001132
Iteration 81/1000 | Loss: 0.00001132
Iteration 82/1000 | Loss: 0.00001132
Iteration 83/1000 | Loss: 0.00001131
Iteration 84/1000 | Loss: 0.00001131
Iteration 85/1000 | Loss: 0.00001131
Iteration 86/1000 | Loss: 0.00001130
Iteration 87/1000 | Loss: 0.00001130
Iteration 88/1000 | Loss: 0.00001130
Iteration 89/1000 | Loss: 0.00001130
Iteration 90/1000 | Loss: 0.00001130
Iteration 91/1000 | Loss: 0.00001130
Iteration 92/1000 | Loss: 0.00001129
Iteration 93/1000 | Loss: 0.00001129
Iteration 94/1000 | Loss: 0.00001129
Iteration 95/1000 | Loss: 0.00001129
Iteration 96/1000 | Loss: 0.00001129
Iteration 97/1000 | Loss: 0.00001129
Iteration 98/1000 | Loss: 0.00001129
Iteration 99/1000 | Loss: 0.00001128
Iteration 100/1000 | Loss: 0.00001128
Iteration 101/1000 | Loss: 0.00001128
Iteration 102/1000 | Loss: 0.00001127
Iteration 103/1000 | Loss: 0.00001127
Iteration 104/1000 | Loss: 0.00001127
Iteration 105/1000 | Loss: 0.00001126
Iteration 106/1000 | Loss: 0.00001126
Iteration 107/1000 | Loss: 0.00001126
Iteration 108/1000 | Loss: 0.00001125
Iteration 109/1000 | Loss: 0.00001125
Iteration 110/1000 | Loss: 0.00001125
Iteration 111/1000 | Loss: 0.00001125
Iteration 112/1000 | Loss: 0.00001125
Iteration 113/1000 | Loss: 0.00001125
Iteration 114/1000 | Loss: 0.00001125
Iteration 115/1000 | Loss: 0.00001125
Iteration 116/1000 | Loss: 0.00001124
Iteration 117/1000 | Loss: 0.00001124
Iteration 118/1000 | Loss: 0.00001124
Iteration 119/1000 | Loss: 0.00001124
Iteration 120/1000 | Loss: 0.00001124
Iteration 121/1000 | Loss: 0.00001124
Iteration 122/1000 | Loss: 0.00001124
Iteration 123/1000 | Loss: 0.00001124
Iteration 124/1000 | Loss: 0.00001123
Iteration 125/1000 | Loss: 0.00001123
Iteration 126/1000 | Loss: 0.00001123
Iteration 127/1000 | Loss: 0.00001123
Iteration 128/1000 | Loss: 0.00001123
Iteration 129/1000 | Loss: 0.00001123
Iteration 130/1000 | Loss: 0.00001122
Iteration 131/1000 | Loss: 0.00001122
Iteration 132/1000 | Loss: 0.00001122
Iteration 133/1000 | Loss: 0.00001121
Iteration 134/1000 | Loss: 0.00001121
Iteration 135/1000 | Loss: 0.00001121
Iteration 136/1000 | Loss: 0.00001121
Iteration 137/1000 | Loss: 0.00001120
Iteration 138/1000 | Loss: 0.00001120
Iteration 139/1000 | Loss: 0.00001120
Iteration 140/1000 | Loss: 0.00001120
Iteration 141/1000 | Loss: 0.00001120
Iteration 142/1000 | Loss: 0.00001120
Iteration 143/1000 | Loss: 0.00001120
Iteration 144/1000 | Loss: 0.00001120
Iteration 145/1000 | Loss: 0.00001120
Iteration 146/1000 | Loss: 0.00001120
Iteration 147/1000 | Loss: 0.00001120
Iteration 148/1000 | Loss: 0.00001120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.1202807399968151e-05, 1.1202807399968151e-05, 1.1202807399968151e-05, 1.1202807399968151e-05, 1.1202807399968151e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1202807399968151e-05

Optimization complete. Final v2v error: 2.8362274169921875 mm

Highest mean error: 3.2337515354156494 mm for frame 139

Lowest mean error: 2.709946870803833 mm for frame 123

Saving results

Total time: 38.36011528968811
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004631
Iteration 2/25 | Loss: 0.00204236
Iteration 3/25 | Loss: 0.00122655
Iteration 4/25 | Loss: 0.00103680
Iteration 5/25 | Loss: 0.00094595
Iteration 6/25 | Loss: 0.00086741
Iteration 7/25 | Loss: 0.00077581
Iteration 8/25 | Loss: 0.00072456
Iteration 9/25 | Loss: 0.00074520
Iteration 10/25 | Loss: 0.00068731
Iteration 11/25 | Loss: 0.00068421
Iteration 12/25 | Loss: 0.00066887
Iteration 13/25 | Loss: 0.00065965
Iteration 14/25 | Loss: 0.00065743
Iteration 15/25 | Loss: 0.00065629
Iteration 16/25 | Loss: 0.00065583
Iteration 17/25 | Loss: 0.00065562
Iteration 18/25 | Loss: 0.00065553
Iteration 19/25 | Loss: 0.00065552
Iteration 20/25 | Loss: 0.00065552
Iteration 21/25 | Loss: 0.00065552
Iteration 22/25 | Loss: 0.00065551
Iteration 23/25 | Loss: 0.00065551
Iteration 24/25 | Loss: 0.00065551
Iteration 25/25 | Loss: 0.00065551

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59116161
Iteration 2/25 | Loss: 0.00051930
Iteration 3/25 | Loss: 0.00047249
Iteration 4/25 | Loss: 0.00047249
Iteration 5/25 | Loss: 0.00047249
Iteration 6/25 | Loss: 0.00047249
Iteration 7/25 | Loss: 0.00047249
Iteration 8/25 | Loss: 0.00047249
Iteration 9/25 | Loss: 0.00047249
Iteration 10/25 | Loss: 0.00047249
Iteration 11/25 | Loss: 0.00047249
Iteration 12/25 | Loss: 0.00047249
Iteration 13/25 | Loss: 0.00047249
Iteration 14/25 | Loss: 0.00047249
Iteration 15/25 | Loss: 0.00047249
Iteration 16/25 | Loss: 0.00047249
Iteration 17/25 | Loss: 0.00047249
Iteration 18/25 | Loss: 0.00047249
Iteration 19/25 | Loss: 0.00047249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00047249210183508694, 0.00047249210183508694, 0.00047249210183508694, 0.00047249210183508694, 0.00047249210183508694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00047249210183508694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047249
Iteration 2/1000 | Loss: 0.00029766
Iteration 3/1000 | Loss: 0.00003087
Iteration 4/1000 | Loss: 0.00002429
Iteration 5/1000 | Loss: 0.00029293
Iteration 6/1000 | Loss: 0.00010613
Iteration 7/1000 | Loss: 0.00004163
Iteration 8/1000 | Loss: 0.00002356
Iteration 9/1000 | Loss: 0.00002037
Iteration 10/1000 | Loss: 0.00027971
Iteration 11/1000 | Loss: 0.00004839
Iteration 12/1000 | Loss: 0.00028975
Iteration 13/1000 | Loss: 0.00032263
Iteration 14/1000 | Loss: 0.00006324
Iteration 15/1000 | Loss: 0.00004038
Iteration 16/1000 | Loss: 0.00003090
Iteration 17/1000 | Loss: 0.00002695
Iteration 18/1000 | Loss: 0.00002379
Iteration 19/1000 | Loss: 0.00041610
Iteration 20/1000 | Loss: 0.00029161
Iteration 21/1000 | Loss: 0.00003663
Iteration 22/1000 | Loss: 0.00002695
Iteration 23/1000 | Loss: 0.00030181
Iteration 24/1000 | Loss: 0.00010435
Iteration 25/1000 | Loss: 0.00033034
Iteration 26/1000 | Loss: 0.00003317
Iteration 27/1000 | Loss: 0.00002815
Iteration 28/1000 | Loss: 0.00002386
Iteration 29/1000 | Loss: 0.00012456
Iteration 30/1000 | Loss: 0.00006780
Iteration 31/1000 | Loss: 0.00020529
Iteration 32/1000 | Loss: 0.00037098
Iteration 33/1000 | Loss: 0.00008166
Iteration 34/1000 | Loss: 0.00016213
Iteration 35/1000 | Loss: 0.00036908
Iteration 36/1000 | Loss: 0.00028451
Iteration 37/1000 | Loss: 0.00001900
Iteration 38/1000 | Loss: 0.00005168
Iteration 39/1000 | Loss: 0.00032118
Iteration 40/1000 | Loss: 0.00021780
Iteration 41/1000 | Loss: 0.00031743
Iteration 42/1000 | Loss: 0.00022496
Iteration 43/1000 | Loss: 0.00008309
Iteration 44/1000 | Loss: 0.00004486
Iteration 45/1000 | Loss: 0.00002391
Iteration 46/1000 | Loss: 0.00040625
Iteration 47/1000 | Loss: 0.00019345
Iteration 48/1000 | Loss: 0.00002304
Iteration 49/1000 | Loss: 0.00002112
Iteration 50/1000 | Loss: 0.00008274
Iteration 51/1000 | Loss: 0.00002018
Iteration 52/1000 | Loss: 0.00005126
Iteration 53/1000 | Loss: 0.00001951
Iteration 54/1000 | Loss: 0.00001924
Iteration 55/1000 | Loss: 0.00003445
Iteration 56/1000 | Loss: 0.00002452
Iteration 57/1000 | Loss: 0.00001863
Iteration 58/1000 | Loss: 0.00002198
Iteration 59/1000 | Loss: 0.00020639
Iteration 60/1000 | Loss: 0.00001843
Iteration 61/1000 | Loss: 0.00001823
Iteration 62/1000 | Loss: 0.00012494
Iteration 63/1000 | Loss: 0.00002110
Iteration 64/1000 | Loss: 0.00001715
Iteration 65/1000 | Loss: 0.00001664
Iteration 66/1000 | Loss: 0.00048607
Iteration 67/1000 | Loss: 0.00002192
Iteration 68/1000 | Loss: 0.00001734
Iteration 69/1000 | Loss: 0.00001634
Iteration 70/1000 | Loss: 0.00002401
Iteration 71/1000 | Loss: 0.00001505
Iteration 72/1000 | Loss: 0.00007041
Iteration 73/1000 | Loss: 0.00002550
Iteration 74/1000 | Loss: 0.00002172
Iteration 75/1000 | Loss: 0.00003077
Iteration 76/1000 | Loss: 0.00001422
Iteration 77/1000 | Loss: 0.00006808
Iteration 78/1000 | Loss: 0.00001432
Iteration 79/1000 | Loss: 0.00001401
Iteration 80/1000 | Loss: 0.00001396
Iteration 81/1000 | Loss: 0.00003064
Iteration 82/1000 | Loss: 0.00001477
Iteration 83/1000 | Loss: 0.00001396
Iteration 84/1000 | Loss: 0.00001392
Iteration 85/1000 | Loss: 0.00001513
Iteration 86/1000 | Loss: 0.00001389
Iteration 87/1000 | Loss: 0.00001389
Iteration 88/1000 | Loss: 0.00001389
Iteration 89/1000 | Loss: 0.00001389
Iteration 90/1000 | Loss: 0.00001388
Iteration 91/1000 | Loss: 0.00001388
Iteration 92/1000 | Loss: 0.00001388
Iteration 93/1000 | Loss: 0.00001388
Iteration 94/1000 | Loss: 0.00001388
Iteration 95/1000 | Loss: 0.00001388
Iteration 96/1000 | Loss: 0.00001387
Iteration 97/1000 | Loss: 0.00001387
Iteration 98/1000 | Loss: 0.00001387
Iteration 99/1000 | Loss: 0.00001387
Iteration 100/1000 | Loss: 0.00001386
Iteration 101/1000 | Loss: 0.00001386
Iteration 102/1000 | Loss: 0.00001385
Iteration 103/1000 | Loss: 0.00001385
Iteration 104/1000 | Loss: 0.00001384
Iteration 105/1000 | Loss: 0.00001384
Iteration 106/1000 | Loss: 0.00001384
Iteration 107/1000 | Loss: 0.00001383
Iteration 108/1000 | Loss: 0.00001383
Iteration 109/1000 | Loss: 0.00001383
Iteration 110/1000 | Loss: 0.00001383
Iteration 111/1000 | Loss: 0.00001383
Iteration 112/1000 | Loss: 0.00001383
Iteration 113/1000 | Loss: 0.00001383
Iteration 114/1000 | Loss: 0.00001383
Iteration 115/1000 | Loss: 0.00001382
Iteration 116/1000 | Loss: 0.00001382
Iteration 117/1000 | Loss: 0.00001382
Iteration 118/1000 | Loss: 0.00001381
Iteration 119/1000 | Loss: 0.00001380
Iteration 120/1000 | Loss: 0.00001380
Iteration 121/1000 | Loss: 0.00001379
Iteration 122/1000 | Loss: 0.00001379
Iteration 123/1000 | Loss: 0.00001379
Iteration 124/1000 | Loss: 0.00001378
Iteration 125/1000 | Loss: 0.00001378
Iteration 126/1000 | Loss: 0.00001378
Iteration 127/1000 | Loss: 0.00001377
Iteration 128/1000 | Loss: 0.00001377
Iteration 129/1000 | Loss: 0.00001377
Iteration 130/1000 | Loss: 0.00001377
Iteration 131/1000 | Loss: 0.00001376
Iteration 132/1000 | Loss: 0.00001376
Iteration 133/1000 | Loss: 0.00001376
Iteration 134/1000 | Loss: 0.00001375
Iteration 135/1000 | Loss: 0.00001375
Iteration 136/1000 | Loss: 0.00001375
Iteration 137/1000 | Loss: 0.00001374
Iteration 138/1000 | Loss: 0.00001374
Iteration 139/1000 | Loss: 0.00001374
Iteration 140/1000 | Loss: 0.00001374
Iteration 141/1000 | Loss: 0.00001374
Iteration 142/1000 | Loss: 0.00001374
Iteration 143/1000 | Loss: 0.00001374
Iteration 144/1000 | Loss: 0.00001374
Iteration 145/1000 | Loss: 0.00001374
Iteration 146/1000 | Loss: 0.00001374
Iteration 147/1000 | Loss: 0.00001374
Iteration 148/1000 | Loss: 0.00001374
Iteration 149/1000 | Loss: 0.00001374
Iteration 150/1000 | Loss: 0.00001374
Iteration 151/1000 | Loss: 0.00001374
Iteration 152/1000 | Loss: 0.00001374
Iteration 153/1000 | Loss: 0.00001374
Iteration 154/1000 | Loss: 0.00001374
Iteration 155/1000 | Loss: 0.00001374
Iteration 156/1000 | Loss: 0.00001374
Iteration 157/1000 | Loss: 0.00001374
Iteration 158/1000 | Loss: 0.00001374
Iteration 159/1000 | Loss: 0.00001374
Iteration 160/1000 | Loss: 0.00001374
Iteration 161/1000 | Loss: 0.00001374
Iteration 162/1000 | Loss: 0.00001374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.3736534128838684e-05, 1.3736534128838684e-05, 1.3736534128838684e-05, 1.3736534128838684e-05, 1.3736534128838684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3736534128838684e-05

Optimization complete. Final v2v error: 3.0599703788757324 mm

Highest mean error: 3.866374969482422 mm for frame 86

Lowest mean error: 2.5281903743743896 mm for frame 123

Saving results

Total time: 146.71090841293335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395591
Iteration 2/25 | Loss: 0.00094805
Iteration 3/25 | Loss: 0.00064771
Iteration 4/25 | Loss: 0.00060819
Iteration 5/25 | Loss: 0.00059883
Iteration 6/25 | Loss: 0.00059636
Iteration 7/25 | Loss: 0.00059578
Iteration 8/25 | Loss: 0.00059571
Iteration 9/25 | Loss: 0.00059571
Iteration 10/25 | Loss: 0.00059571
Iteration 11/25 | Loss: 0.00059571
Iteration 12/25 | Loss: 0.00059571
Iteration 13/25 | Loss: 0.00059571
Iteration 14/25 | Loss: 0.00059571
Iteration 15/25 | Loss: 0.00059571
Iteration 16/25 | Loss: 0.00059571
Iteration 17/25 | Loss: 0.00059571
Iteration 18/25 | Loss: 0.00059571
Iteration 19/25 | Loss: 0.00059571
Iteration 20/25 | Loss: 0.00059571
Iteration 21/25 | Loss: 0.00059571
Iteration 22/25 | Loss: 0.00059571
Iteration 23/25 | Loss: 0.00059571
Iteration 24/25 | Loss: 0.00059571
Iteration 25/25 | Loss: 0.00059571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0005957058747299016, 0.0005957058747299016, 0.0005957058747299016, 0.0005957058747299016, 0.0005957058747299016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005957058747299016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45562327
Iteration 2/25 | Loss: 0.00024857
Iteration 3/25 | Loss: 0.00024856
Iteration 4/25 | Loss: 0.00024856
Iteration 5/25 | Loss: 0.00024856
Iteration 6/25 | Loss: 0.00024856
Iteration 7/25 | Loss: 0.00024856
Iteration 8/25 | Loss: 0.00024856
Iteration 9/25 | Loss: 0.00024856
Iteration 10/25 | Loss: 0.00024856
Iteration 11/25 | Loss: 0.00024856
Iteration 12/25 | Loss: 0.00024856
Iteration 13/25 | Loss: 0.00024856
Iteration 14/25 | Loss: 0.00024856
Iteration 15/25 | Loss: 0.00024856
Iteration 16/25 | Loss: 0.00024856
Iteration 17/25 | Loss: 0.00024856
Iteration 18/25 | Loss: 0.00024856
Iteration 19/25 | Loss: 0.00024856
Iteration 20/25 | Loss: 0.00024856
Iteration 21/25 | Loss: 0.00024856
Iteration 22/25 | Loss: 0.00024856
Iteration 23/25 | Loss: 0.00024856
Iteration 24/25 | Loss: 0.00024856
Iteration 25/25 | Loss: 0.00024856

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024856
Iteration 2/1000 | Loss: 0.00002275
Iteration 3/1000 | Loss: 0.00001674
Iteration 4/1000 | Loss: 0.00001393
Iteration 5/1000 | Loss: 0.00001312
Iteration 6/1000 | Loss: 0.00001262
Iteration 7/1000 | Loss: 0.00001217
Iteration 8/1000 | Loss: 0.00001188
Iteration 9/1000 | Loss: 0.00001173
Iteration 10/1000 | Loss: 0.00001164
Iteration 11/1000 | Loss: 0.00001163
Iteration 12/1000 | Loss: 0.00001160
Iteration 13/1000 | Loss: 0.00001159
Iteration 14/1000 | Loss: 0.00001158
Iteration 15/1000 | Loss: 0.00001157
Iteration 16/1000 | Loss: 0.00001156
Iteration 17/1000 | Loss: 0.00001155
Iteration 18/1000 | Loss: 0.00001155
Iteration 19/1000 | Loss: 0.00001154
Iteration 20/1000 | Loss: 0.00001153
Iteration 21/1000 | Loss: 0.00001153
Iteration 22/1000 | Loss: 0.00001152
Iteration 23/1000 | Loss: 0.00001151
Iteration 24/1000 | Loss: 0.00001151
Iteration 25/1000 | Loss: 0.00001150
Iteration 26/1000 | Loss: 0.00001150
Iteration 27/1000 | Loss: 0.00001150
Iteration 28/1000 | Loss: 0.00001149
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001148
Iteration 31/1000 | Loss: 0.00001148
Iteration 32/1000 | Loss: 0.00001147
Iteration 33/1000 | Loss: 0.00001147
Iteration 34/1000 | Loss: 0.00001147
Iteration 35/1000 | Loss: 0.00001146
Iteration 36/1000 | Loss: 0.00001146
Iteration 37/1000 | Loss: 0.00001142
Iteration 38/1000 | Loss: 0.00001142
Iteration 39/1000 | Loss: 0.00001142
Iteration 40/1000 | Loss: 0.00001142
Iteration 41/1000 | Loss: 0.00001142
Iteration 42/1000 | Loss: 0.00001142
Iteration 43/1000 | Loss: 0.00001141
Iteration 44/1000 | Loss: 0.00001141
Iteration 45/1000 | Loss: 0.00001140
Iteration 46/1000 | Loss: 0.00001140
Iteration 47/1000 | Loss: 0.00001139
Iteration 48/1000 | Loss: 0.00001139
Iteration 49/1000 | Loss: 0.00001139
Iteration 50/1000 | Loss: 0.00001139
Iteration 51/1000 | Loss: 0.00001138
Iteration 52/1000 | Loss: 0.00001138
Iteration 53/1000 | Loss: 0.00001138
Iteration 54/1000 | Loss: 0.00001138
Iteration 55/1000 | Loss: 0.00001137
Iteration 56/1000 | Loss: 0.00001137
Iteration 57/1000 | Loss: 0.00001137
Iteration 58/1000 | Loss: 0.00001137
Iteration 59/1000 | Loss: 0.00001137
Iteration 60/1000 | Loss: 0.00001136
Iteration 61/1000 | Loss: 0.00001136
Iteration 62/1000 | Loss: 0.00001136
Iteration 63/1000 | Loss: 0.00001135
Iteration 64/1000 | Loss: 0.00001135
Iteration 65/1000 | Loss: 0.00001134
Iteration 66/1000 | Loss: 0.00001134
Iteration 67/1000 | Loss: 0.00001133
Iteration 68/1000 | Loss: 0.00001133
Iteration 69/1000 | Loss: 0.00001133
Iteration 70/1000 | Loss: 0.00001132
Iteration 71/1000 | Loss: 0.00001132
Iteration 72/1000 | Loss: 0.00001132
Iteration 73/1000 | Loss: 0.00001132
Iteration 74/1000 | Loss: 0.00001131
Iteration 75/1000 | Loss: 0.00001131
Iteration 76/1000 | Loss: 0.00001131
Iteration 77/1000 | Loss: 0.00001131
Iteration 78/1000 | Loss: 0.00001130
Iteration 79/1000 | Loss: 0.00001130
Iteration 80/1000 | Loss: 0.00001130
Iteration 81/1000 | Loss: 0.00001130
Iteration 82/1000 | Loss: 0.00001130
Iteration 83/1000 | Loss: 0.00001130
Iteration 84/1000 | Loss: 0.00001129
Iteration 85/1000 | Loss: 0.00001129
Iteration 86/1000 | Loss: 0.00001129
Iteration 87/1000 | Loss: 0.00001128
Iteration 88/1000 | Loss: 0.00001128
Iteration 89/1000 | Loss: 0.00001128
Iteration 90/1000 | Loss: 0.00001128
Iteration 91/1000 | Loss: 0.00001128
Iteration 92/1000 | Loss: 0.00001128
Iteration 93/1000 | Loss: 0.00001128
Iteration 94/1000 | Loss: 0.00001128
Iteration 95/1000 | Loss: 0.00001128
Iteration 96/1000 | Loss: 0.00001128
Iteration 97/1000 | Loss: 0.00001127
Iteration 98/1000 | Loss: 0.00001127
Iteration 99/1000 | Loss: 0.00001127
Iteration 100/1000 | Loss: 0.00001127
Iteration 101/1000 | Loss: 0.00001127
Iteration 102/1000 | Loss: 0.00001127
Iteration 103/1000 | Loss: 0.00001127
Iteration 104/1000 | Loss: 0.00001127
Iteration 105/1000 | Loss: 0.00001127
Iteration 106/1000 | Loss: 0.00001127
Iteration 107/1000 | Loss: 0.00001127
Iteration 108/1000 | Loss: 0.00001127
Iteration 109/1000 | Loss: 0.00001126
Iteration 110/1000 | Loss: 0.00001126
Iteration 111/1000 | Loss: 0.00001126
Iteration 112/1000 | Loss: 0.00001126
Iteration 113/1000 | Loss: 0.00001126
Iteration 114/1000 | Loss: 0.00001126
Iteration 115/1000 | Loss: 0.00001126
Iteration 116/1000 | Loss: 0.00001126
Iteration 117/1000 | Loss: 0.00001126
Iteration 118/1000 | Loss: 0.00001126
Iteration 119/1000 | Loss: 0.00001126
Iteration 120/1000 | Loss: 0.00001126
Iteration 121/1000 | Loss: 0.00001126
Iteration 122/1000 | Loss: 0.00001126
Iteration 123/1000 | Loss: 0.00001126
Iteration 124/1000 | Loss: 0.00001126
Iteration 125/1000 | Loss: 0.00001125
Iteration 126/1000 | Loss: 0.00001125
Iteration 127/1000 | Loss: 0.00001125
Iteration 128/1000 | Loss: 0.00001125
Iteration 129/1000 | Loss: 0.00001125
Iteration 130/1000 | Loss: 0.00001124
Iteration 131/1000 | Loss: 0.00001124
Iteration 132/1000 | Loss: 0.00001124
Iteration 133/1000 | Loss: 0.00001124
Iteration 134/1000 | Loss: 0.00001124
Iteration 135/1000 | Loss: 0.00001124
Iteration 136/1000 | Loss: 0.00001123
Iteration 137/1000 | Loss: 0.00001123
Iteration 138/1000 | Loss: 0.00001123
Iteration 139/1000 | Loss: 0.00001123
Iteration 140/1000 | Loss: 0.00001123
Iteration 141/1000 | Loss: 0.00001123
Iteration 142/1000 | Loss: 0.00001123
Iteration 143/1000 | Loss: 0.00001122
Iteration 144/1000 | Loss: 0.00001122
Iteration 145/1000 | Loss: 0.00001122
Iteration 146/1000 | Loss: 0.00001122
Iteration 147/1000 | Loss: 0.00001122
Iteration 148/1000 | Loss: 0.00001122
Iteration 149/1000 | Loss: 0.00001122
Iteration 150/1000 | Loss: 0.00001122
Iteration 151/1000 | Loss: 0.00001122
Iteration 152/1000 | Loss: 0.00001122
Iteration 153/1000 | Loss: 0.00001121
Iteration 154/1000 | Loss: 0.00001121
Iteration 155/1000 | Loss: 0.00001121
Iteration 156/1000 | Loss: 0.00001121
Iteration 157/1000 | Loss: 0.00001121
Iteration 158/1000 | Loss: 0.00001121
Iteration 159/1000 | Loss: 0.00001121
Iteration 160/1000 | Loss: 0.00001121
Iteration 161/1000 | Loss: 0.00001121
Iteration 162/1000 | Loss: 0.00001121
Iteration 163/1000 | Loss: 0.00001120
Iteration 164/1000 | Loss: 0.00001120
Iteration 165/1000 | Loss: 0.00001120
Iteration 166/1000 | Loss: 0.00001120
Iteration 167/1000 | Loss: 0.00001120
Iteration 168/1000 | Loss: 0.00001120
Iteration 169/1000 | Loss: 0.00001120
Iteration 170/1000 | Loss: 0.00001120
Iteration 171/1000 | Loss: 0.00001119
Iteration 172/1000 | Loss: 0.00001119
Iteration 173/1000 | Loss: 0.00001119
Iteration 174/1000 | Loss: 0.00001119
Iteration 175/1000 | Loss: 0.00001119
Iteration 176/1000 | Loss: 0.00001119
Iteration 177/1000 | Loss: 0.00001119
Iteration 178/1000 | Loss: 0.00001119
Iteration 179/1000 | Loss: 0.00001119
Iteration 180/1000 | Loss: 0.00001118
Iteration 181/1000 | Loss: 0.00001118
Iteration 182/1000 | Loss: 0.00001118
Iteration 183/1000 | Loss: 0.00001118
Iteration 184/1000 | Loss: 0.00001118
Iteration 185/1000 | Loss: 0.00001118
Iteration 186/1000 | Loss: 0.00001118
Iteration 187/1000 | Loss: 0.00001118
Iteration 188/1000 | Loss: 0.00001118
Iteration 189/1000 | Loss: 0.00001118
Iteration 190/1000 | Loss: 0.00001118
Iteration 191/1000 | Loss: 0.00001118
Iteration 192/1000 | Loss: 0.00001118
Iteration 193/1000 | Loss: 0.00001117
Iteration 194/1000 | Loss: 0.00001117
Iteration 195/1000 | Loss: 0.00001117
Iteration 196/1000 | Loss: 0.00001117
Iteration 197/1000 | Loss: 0.00001117
Iteration 198/1000 | Loss: 0.00001117
Iteration 199/1000 | Loss: 0.00001117
Iteration 200/1000 | Loss: 0.00001117
Iteration 201/1000 | Loss: 0.00001117
Iteration 202/1000 | Loss: 0.00001117
Iteration 203/1000 | Loss: 0.00001117
Iteration 204/1000 | Loss: 0.00001117
Iteration 205/1000 | Loss: 0.00001117
Iteration 206/1000 | Loss: 0.00001117
Iteration 207/1000 | Loss: 0.00001117
Iteration 208/1000 | Loss: 0.00001117
Iteration 209/1000 | Loss: 0.00001117
Iteration 210/1000 | Loss: 0.00001117
Iteration 211/1000 | Loss: 0.00001117
Iteration 212/1000 | Loss: 0.00001117
Iteration 213/1000 | Loss: 0.00001117
Iteration 214/1000 | Loss: 0.00001117
Iteration 215/1000 | Loss: 0.00001116
Iteration 216/1000 | Loss: 0.00001116
Iteration 217/1000 | Loss: 0.00001116
Iteration 218/1000 | Loss: 0.00001116
Iteration 219/1000 | Loss: 0.00001116
Iteration 220/1000 | Loss: 0.00001116
Iteration 221/1000 | Loss: 0.00001116
Iteration 222/1000 | Loss: 0.00001116
Iteration 223/1000 | Loss: 0.00001116
Iteration 224/1000 | Loss: 0.00001116
Iteration 225/1000 | Loss: 0.00001116
Iteration 226/1000 | Loss: 0.00001116
Iteration 227/1000 | Loss: 0.00001116
Iteration 228/1000 | Loss: 0.00001116
Iteration 229/1000 | Loss: 0.00001116
Iteration 230/1000 | Loss: 0.00001116
Iteration 231/1000 | Loss: 0.00001116
Iteration 232/1000 | Loss: 0.00001116
Iteration 233/1000 | Loss: 0.00001116
Iteration 234/1000 | Loss: 0.00001116
Iteration 235/1000 | Loss: 0.00001116
Iteration 236/1000 | Loss: 0.00001116
Iteration 237/1000 | Loss: 0.00001116
Iteration 238/1000 | Loss: 0.00001116
Iteration 239/1000 | Loss: 0.00001116
Iteration 240/1000 | Loss: 0.00001116
Iteration 241/1000 | Loss: 0.00001116
Iteration 242/1000 | Loss: 0.00001116
Iteration 243/1000 | Loss: 0.00001116
Iteration 244/1000 | Loss: 0.00001116
Iteration 245/1000 | Loss: 0.00001116
Iteration 246/1000 | Loss: 0.00001116
Iteration 247/1000 | Loss: 0.00001116
Iteration 248/1000 | Loss: 0.00001116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.11617937363917e-05, 1.11617937363917e-05, 1.11617937363917e-05, 1.11617937363917e-05, 1.11617937363917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.11617937363917e-05

Optimization complete. Final v2v error: 2.8666272163391113 mm

Highest mean error: 2.946978807449341 mm for frame 86

Lowest mean error: 2.798172950744629 mm for frame 107

Saving results

Total time: 40.30477476119995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979884
Iteration 2/25 | Loss: 0.00165419
Iteration 3/25 | Loss: 0.00104709
Iteration 4/25 | Loss: 0.00091279
Iteration 5/25 | Loss: 0.00086413
Iteration 6/25 | Loss: 0.00083980
Iteration 7/25 | Loss: 0.00081966
Iteration 8/25 | Loss: 0.00081448
Iteration 9/25 | Loss: 0.00082638
Iteration 10/25 | Loss: 0.00080496
Iteration 11/25 | Loss: 0.00078361
Iteration 12/25 | Loss: 0.00076759
Iteration 13/25 | Loss: 0.00076194
Iteration 14/25 | Loss: 0.00075289
Iteration 15/25 | Loss: 0.00074993
Iteration 16/25 | Loss: 0.00074706
Iteration 17/25 | Loss: 0.00074713
Iteration 18/25 | Loss: 0.00074654
Iteration 19/25 | Loss: 0.00074952
Iteration 20/25 | Loss: 0.00074736
Iteration 21/25 | Loss: 0.00074744
Iteration 22/25 | Loss: 0.00075193
Iteration 23/25 | Loss: 0.00074758
Iteration 24/25 | Loss: 0.00074685
Iteration 25/25 | Loss: 0.00074366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50138319
Iteration 2/25 | Loss: 0.00055399
Iteration 3/25 | Loss: 0.00055399
Iteration 4/25 | Loss: 0.00055399
Iteration 5/25 | Loss: 0.00055399
Iteration 6/25 | Loss: 0.00055399
Iteration 7/25 | Loss: 0.00055399
Iteration 8/25 | Loss: 0.00055399
Iteration 9/25 | Loss: 0.00055399
Iteration 10/25 | Loss: 0.00055399
Iteration 11/25 | Loss: 0.00055399
Iteration 12/25 | Loss: 0.00055399
Iteration 13/25 | Loss: 0.00055399
Iteration 14/25 | Loss: 0.00055399
Iteration 15/25 | Loss: 0.00055399
Iteration 16/25 | Loss: 0.00055399
Iteration 17/25 | Loss: 0.00055399
Iteration 18/25 | Loss: 0.00055399
Iteration 19/25 | Loss: 0.00055399
Iteration 20/25 | Loss: 0.00055399
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005539882113225758, 0.0005539882113225758, 0.0005539882113225758, 0.0005539882113225758, 0.0005539882113225758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005539882113225758

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055399
Iteration 2/1000 | Loss: 0.00230129
Iteration 3/1000 | Loss: 0.00017280
Iteration 4/1000 | Loss: 0.00010997
Iteration 5/1000 | Loss: 0.00034073
Iteration 6/1000 | Loss: 0.00008219
Iteration 7/1000 | Loss: 0.00048815
Iteration 8/1000 | Loss: 0.00029715
Iteration 9/1000 | Loss: 0.00007941
Iteration 10/1000 | Loss: 0.00007961
Iteration 11/1000 | Loss: 0.00008004
Iteration 12/1000 | Loss: 0.00035166
Iteration 13/1000 | Loss: 0.00032696
Iteration 14/1000 | Loss: 0.00035289
Iteration 15/1000 | Loss: 0.00038239
Iteration 16/1000 | Loss: 0.00035224
Iteration 17/1000 | Loss: 0.00027440
Iteration 18/1000 | Loss: 0.00025703
Iteration 19/1000 | Loss: 0.00010730
Iteration 20/1000 | Loss: 0.00013657
Iteration 21/1000 | Loss: 0.00011383
Iteration 22/1000 | Loss: 0.00020282
Iteration 23/1000 | Loss: 0.00015349
Iteration 24/1000 | Loss: 0.00022324
Iteration 25/1000 | Loss: 0.00010836
Iteration 26/1000 | Loss: 0.00008442
Iteration 27/1000 | Loss: 0.00008149
Iteration 28/1000 | Loss: 0.00038991
Iteration 29/1000 | Loss: 0.00018224
Iteration 30/1000 | Loss: 0.00037417
Iteration 31/1000 | Loss: 0.00007587
Iteration 32/1000 | Loss: 0.00005230
Iteration 33/1000 | Loss: 0.00006859
Iteration 34/1000 | Loss: 0.00006488
Iteration 35/1000 | Loss: 0.00006806
Iteration 36/1000 | Loss: 0.00007177
Iteration 37/1000 | Loss: 0.00006894
Iteration 38/1000 | Loss: 0.00006725
Iteration 39/1000 | Loss: 0.00006017
Iteration 40/1000 | Loss: 0.00005320
Iteration 41/1000 | Loss: 0.00006411
Iteration 42/1000 | Loss: 0.00004193
Iteration 43/1000 | Loss: 0.00006925
Iteration 44/1000 | Loss: 0.00006533
Iteration 45/1000 | Loss: 0.00006572
Iteration 46/1000 | Loss: 0.00005968
Iteration 47/1000 | Loss: 0.00006076
Iteration 48/1000 | Loss: 0.00005326
Iteration 49/1000 | Loss: 0.00005062
Iteration 50/1000 | Loss: 0.00004279
Iteration 51/1000 | Loss: 0.00004342
Iteration 52/1000 | Loss: 0.00004166
Iteration 53/1000 | Loss: 0.00005257
Iteration 54/1000 | Loss: 0.00005710
Iteration 55/1000 | Loss: 0.00005177
Iteration 56/1000 | Loss: 0.00004881
Iteration 57/1000 | Loss: 0.00004127
Iteration 58/1000 | Loss: 0.00005964
Iteration 59/1000 | Loss: 0.00004869
Iteration 60/1000 | Loss: 0.00003641
Iteration 61/1000 | Loss: 0.00005546
Iteration 62/1000 | Loss: 0.00005010
Iteration 63/1000 | Loss: 0.00004940
Iteration 64/1000 | Loss: 0.00004691
Iteration 65/1000 | Loss: 0.00004869
Iteration 66/1000 | Loss: 0.00004623
Iteration 67/1000 | Loss: 0.00003778
Iteration 68/1000 | Loss: 0.00003705
Iteration 69/1000 | Loss: 0.00005086
Iteration 70/1000 | Loss: 0.00004994
Iteration 71/1000 | Loss: 0.00004925
Iteration 72/1000 | Loss: 0.00004603
Iteration 73/1000 | Loss: 0.00005227
Iteration 74/1000 | Loss: 0.00004726
Iteration 75/1000 | Loss: 0.00006031
Iteration 76/1000 | Loss: 0.00004866
Iteration 77/1000 | Loss: 0.00004717
Iteration 78/1000 | Loss: 0.00005116
Iteration 79/1000 | Loss: 0.00004654
Iteration 80/1000 | Loss: 0.00005133
Iteration 81/1000 | Loss: 0.00005129
Iteration 82/1000 | Loss: 0.00004689
Iteration 83/1000 | Loss: 0.00005107
Iteration 84/1000 | Loss: 0.00002755
Iteration 85/1000 | Loss: 0.00002412
Iteration 86/1000 | Loss: 0.00002269
Iteration 87/1000 | Loss: 0.00002174
Iteration 88/1000 | Loss: 0.00018481
Iteration 89/1000 | Loss: 0.00002585
Iteration 90/1000 | Loss: 0.00002385
Iteration 91/1000 | Loss: 0.00020119
Iteration 92/1000 | Loss: 0.00002497
Iteration 93/1000 | Loss: 0.00002338
Iteration 94/1000 | Loss: 0.00003241
Iteration 95/1000 | Loss: 0.00002652
Iteration 96/1000 | Loss: 0.00002128
Iteration 97/1000 | Loss: 0.00002037
Iteration 98/1000 | Loss: 0.00002003
Iteration 99/1000 | Loss: 0.00003704
Iteration 100/1000 | Loss: 0.00003478
Iteration 101/1000 | Loss: 0.00003155
Iteration 102/1000 | Loss: 0.00003253
Iteration 103/1000 | Loss: 0.00003191
Iteration 104/1000 | Loss: 0.00003006
Iteration 105/1000 | Loss: 0.00002914
Iteration 106/1000 | Loss: 0.00003639
Iteration 107/1000 | Loss: 0.00003458
Iteration 108/1000 | Loss: 0.00003515
Iteration 109/1000 | Loss: 0.00003464
Iteration 110/1000 | Loss: 0.00002981
Iteration 111/1000 | Loss: 0.00003309
Iteration 112/1000 | Loss: 0.00003444
Iteration 113/1000 | Loss: 0.00003609
Iteration 114/1000 | Loss: 0.00004013
Iteration 115/1000 | Loss: 0.00003740
Iteration 116/1000 | Loss: 0.00004164
Iteration 117/1000 | Loss: 0.00003558
Iteration 118/1000 | Loss: 0.00005086
Iteration 119/1000 | Loss: 0.00003342
Iteration 120/1000 | Loss: 0.00003186
Iteration 121/1000 | Loss: 0.00003414
Iteration 122/1000 | Loss: 0.00003088
Iteration 123/1000 | Loss: 0.00003109
Iteration 124/1000 | Loss: 0.00003368
Iteration 125/1000 | Loss: 0.00002892
Iteration 126/1000 | Loss: 0.00003229
Iteration 127/1000 | Loss: 0.00002930
Iteration 128/1000 | Loss: 0.00003347
Iteration 129/1000 | Loss: 0.00002858
Iteration 130/1000 | Loss: 0.00003346
Iteration 131/1000 | Loss: 0.00003322
Iteration 132/1000 | Loss: 0.00002925
Iteration 133/1000 | Loss: 0.00003132
Iteration 134/1000 | Loss: 0.00004068
Iteration 135/1000 | Loss: 0.00004148
Iteration 136/1000 | Loss: 0.00003241
Iteration 137/1000 | Loss: 0.00003191
Iteration 138/1000 | Loss: 0.00003527
Iteration 139/1000 | Loss: 0.00004746
Iteration 140/1000 | Loss: 0.00003016
Iteration 141/1000 | Loss: 0.00003615
Iteration 142/1000 | Loss: 0.00003074
Iteration 143/1000 | Loss: 0.00003849
Iteration 144/1000 | Loss: 0.00003062
Iteration 145/1000 | Loss: 0.00003761
Iteration 146/1000 | Loss: 0.00003159
Iteration 147/1000 | Loss: 0.00003365
Iteration 148/1000 | Loss: 0.00004627
Iteration 149/1000 | Loss: 0.00003405
Iteration 150/1000 | Loss: 0.00003484
Iteration 151/1000 | Loss: 0.00003456
Iteration 152/1000 | Loss: 0.00004860
Iteration 153/1000 | Loss: 0.00002851
Iteration 154/1000 | Loss: 0.00002178
Iteration 155/1000 | Loss: 0.00001966
Iteration 156/1000 | Loss: 0.00001922
Iteration 157/1000 | Loss: 0.00001892
Iteration 158/1000 | Loss: 0.00001889
Iteration 159/1000 | Loss: 0.00001886
Iteration 160/1000 | Loss: 0.00001886
Iteration 161/1000 | Loss: 0.00001885
Iteration 162/1000 | Loss: 0.00001885
Iteration 163/1000 | Loss: 0.00001885
Iteration 164/1000 | Loss: 0.00001885
Iteration 165/1000 | Loss: 0.00001884
Iteration 166/1000 | Loss: 0.00001884
Iteration 167/1000 | Loss: 0.00001883
Iteration 168/1000 | Loss: 0.00001883
Iteration 169/1000 | Loss: 0.00001882
Iteration 170/1000 | Loss: 0.00001882
Iteration 171/1000 | Loss: 0.00001882
Iteration 172/1000 | Loss: 0.00001881
Iteration 173/1000 | Loss: 0.00001881
Iteration 174/1000 | Loss: 0.00001881
Iteration 175/1000 | Loss: 0.00001881
Iteration 176/1000 | Loss: 0.00001880
Iteration 177/1000 | Loss: 0.00001880
Iteration 178/1000 | Loss: 0.00001880
Iteration 179/1000 | Loss: 0.00001880
Iteration 180/1000 | Loss: 0.00001880
Iteration 181/1000 | Loss: 0.00001880
Iteration 182/1000 | Loss: 0.00001880
Iteration 183/1000 | Loss: 0.00001879
Iteration 184/1000 | Loss: 0.00001879
Iteration 185/1000 | Loss: 0.00001879
Iteration 186/1000 | Loss: 0.00001879
Iteration 187/1000 | Loss: 0.00001879
Iteration 188/1000 | Loss: 0.00001879
Iteration 189/1000 | Loss: 0.00001879
Iteration 190/1000 | Loss: 0.00001879
Iteration 191/1000 | Loss: 0.00001879
Iteration 192/1000 | Loss: 0.00001879
Iteration 193/1000 | Loss: 0.00001879
Iteration 194/1000 | Loss: 0.00001879
Iteration 195/1000 | Loss: 0.00001878
Iteration 196/1000 | Loss: 0.00001878
Iteration 197/1000 | Loss: 0.00001878
Iteration 198/1000 | Loss: 0.00001878
Iteration 199/1000 | Loss: 0.00001878
Iteration 200/1000 | Loss: 0.00001878
Iteration 201/1000 | Loss: 0.00001878
Iteration 202/1000 | Loss: 0.00001878
Iteration 203/1000 | Loss: 0.00001878
Iteration 204/1000 | Loss: 0.00001877
Iteration 205/1000 | Loss: 0.00001877
Iteration 206/1000 | Loss: 0.00001877
Iteration 207/1000 | Loss: 0.00001877
Iteration 208/1000 | Loss: 0.00001877
Iteration 209/1000 | Loss: 0.00001877
Iteration 210/1000 | Loss: 0.00001876
Iteration 211/1000 | Loss: 0.00001876
Iteration 212/1000 | Loss: 0.00001876
Iteration 213/1000 | Loss: 0.00001876
Iteration 214/1000 | Loss: 0.00001876
Iteration 215/1000 | Loss: 0.00001876
Iteration 216/1000 | Loss: 0.00001875
Iteration 217/1000 | Loss: 0.00001875
Iteration 218/1000 | Loss: 0.00001875
Iteration 219/1000 | Loss: 0.00001875
Iteration 220/1000 | Loss: 0.00001875
Iteration 221/1000 | Loss: 0.00001875
Iteration 222/1000 | Loss: 0.00001875
Iteration 223/1000 | Loss: 0.00001875
Iteration 224/1000 | Loss: 0.00001875
Iteration 225/1000 | Loss: 0.00001875
Iteration 226/1000 | Loss: 0.00001874
Iteration 227/1000 | Loss: 0.00001874
Iteration 228/1000 | Loss: 0.00001874
Iteration 229/1000 | Loss: 0.00001874
Iteration 230/1000 | Loss: 0.00001874
Iteration 231/1000 | Loss: 0.00001874
Iteration 232/1000 | Loss: 0.00001874
Iteration 233/1000 | Loss: 0.00001874
Iteration 234/1000 | Loss: 0.00001874
Iteration 235/1000 | Loss: 0.00001873
Iteration 236/1000 | Loss: 0.00001873
Iteration 237/1000 | Loss: 0.00001873
Iteration 238/1000 | Loss: 0.00001873
Iteration 239/1000 | Loss: 0.00001873
Iteration 240/1000 | Loss: 0.00001873
Iteration 241/1000 | Loss: 0.00001873
Iteration 242/1000 | Loss: 0.00001872
Iteration 243/1000 | Loss: 0.00001872
Iteration 244/1000 | Loss: 0.00001872
Iteration 245/1000 | Loss: 0.00001872
Iteration 246/1000 | Loss: 0.00001872
Iteration 247/1000 | Loss: 0.00001872
Iteration 248/1000 | Loss: 0.00001872
Iteration 249/1000 | Loss: 0.00001872
Iteration 250/1000 | Loss: 0.00001872
Iteration 251/1000 | Loss: 0.00001872
Iteration 252/1000 | Loss: 0.00001872
Iteration 253/1000 | Loss: 0.00001871
Iteration 254/1000 | Loss: 0.00001871
Iteration 255/1000 | Loss: 0.00001871
Iteration 256/1000 | Loss: 0.00001871
Iteration 257/1000 | Loss: 0.00001871
Iteration 258/1000 | Loss: 0.00001871
Iteration 259/1000 | Loss: 0.00001871
Iteration 260/1000 | Loss: 0.00001871
Iteration 261/1000 | Loss: 0.00001871
Iteration 262/1000 | Loss: 0.00001871
Iteration 263/1000 | Loss: 0.00001871
Iteration 264/1000 | Loss: 0.00001871
Iteration 265/1000 | Loss: 0.00001871
Iteration 266/1000 | Loss: 0.00001871
Iteration 267/1000 | Loss: 0.00001871
Iteration 268/1000 | Loss: 0.00001871
Iteration 269/1000 | Loss: 0.00001871
Iteration 270/1000 | Loss: 0.00001870
Iteration 271/1000 | Loss: 0.00001870
Iteration 272/1000 | Loss: 0.00001870
Iteration 273/1000 | Loss: 0.00001870
Iteration 274/1000 | Loss: 0.00001870
Iteration 275/1000 | Loss: 0.00001870
Iteration 276/1000 | Loss: 0.00001870
Iteration 277/1000 | Loss: 0.00001870
Iteration 278/1000 | Loss: 0.00001870
Iteration 279/1000 | Loss: 0.00001870
Iteration 280/1000 | Loss: 0.00001870
Iteration 281/1000 | Loss: 0.00001870
Iteration 282/1000 | Loss: 0.00001870
Iteration 283/1000 | Loss: 0.00001870
Iteration 284/1000 | Loss: 0.00001870
Iteration 285/1000 | Loss: 0.00001870
Iteration 286/1000 | Loss: 0.00001870
Iteration 287/1000 | Loss: 0.00001870
Iteration 288/1000 | Loss: 0.00001870
Iteration 289/1000 | Loss: 0.00001870
Iteration 290/1000 | Loss: 0.00001870
Iteration 291/1000 | Loss: 0.00001869
Iteration 292/1000 | Loss: 0.00001869
Iteration 293/1000 | Loss: 0.00001869
Iteration 294/1000 | Loss: 0.00001869
Iteration 295/1000 | Loss: 0.00001869
Iteration 296/1000 | Loss: 0.00001869
Iteration 297/1000 | Loss: 0.00001869
Iteration 298/1000 | Loss: 0.00001869
Iteration 299/1000 | Loss: 0.00001869
Iteration 300/1000 | Loss: 0.00001869
Iteration 301/1000 | Loss: 0.00001869
Iteration 302/1000 | Loss: 0.00001869
Iteration 303/1000 | Loss: 0.00001869
Iteration 304/1000 | Loss: 0.00001869
Iteration 305/1000 | Loss: 0.00001869
Iteration 306/1000 | Loss: 0.00001869
Iteration 307/1000 | Loss: 0.00001869
Iteration 308/1000 | Loss: 0.00001869
Iteration 309/1000 | Loss: 0.00001868
Iteration 310/1000 | Loss: 0.00001868
Iteration 311/1000 | Loss: 0.00001868
Iteration 312/1000 | Loss: 0.00001868
Iteration 313/1000 | Loss: 0.00001868
Iteration 314/1000 | Loss: 0.00001868
Iteration 315/1000 | Loss: 0.00001868
Iteration 316/1000 | Loss: 0.00001868
Iteration 317/1000 | Loss: 0.00001868
Iteration 318/1000 | Loss: 0.00001868
Iteration 319/1000 | Loss: 0.00001868
Iteration 320/1000 | Loss: 0.00001868
Iteration 321/1000 | Loss: 0.00001868
Iteration 322/1000 | Loss: 0.00001868
Iteration 323/1000 | Loss: 0.00001868
Iteration 324/1000 | Loss: 0.00001868
Iteration 325/1000 | Loss: 0.00001868
Iteration 326/1000 | Loss: 0.00001868
Iteration 327/1000 | Loss: 0.00001868
Iteration 328/1000 | Loss: 0.00001868
Iteration 329/1000 | Loss: 0.00001868
Iteration 330/1000 | Loss: 0.00001868
Iteration 331/1000 | Loss: 0.00001868
Iteration 332/1000 | Loss: 0.00001868
Iteration 333/1000 | Loss: 0.00001867
Iteration 334/1000 | Loss: 0.00001867
Iteration 335/1000 | Loss: 0.00001867
Iteration 336/1000 | Loss: 0.00001867
Iteration 337/1000 | Loss: 0.00001867
Iteration 338/1000 | Loss: 0.00001867
Iteration 339/1000 | Loss: 0.00001867
Iteration 340/1000 | Loss: 0.00001867
Iteration 341/1000 | Loss: 0.00001867
Iteration 342/1000 | Loss: 0.00001867
Iteration 343/1000 | Loss: 0.00001867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 343. Stopping optimization.
Last 5 losses: [1.8674962120712735e-05, 1.8674962120712735e-05, 1.8674962120712735e-05, 1.8674962120712735e-05, 1.8674962120712735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8674962120712735e-05

Optimization complete. Final v2v error: 3.4335086345672607 mm

Highest mean error: 5.468437194824219 mm for frame 118

Lowest mean error: 2.9212512969970703 mm for frame 37

Saving results

Total time: 314.322368144989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01136918
Iteration 2/25 | Loss: 0.00332002
Iteration 3/25 | Loss: 0.00264900
Iteration 4/25 | Loss: 0.00151447
Iteration 5/25 | Loss: 0.00134326
Iteration 6/25 | Loss: 0.00129881
Iteration 7/25 | Loss: 0.00120758
Iteration 8/25 | Loss: 0.00109791
Iteration 9/25 | Loss: 0.00099848
Iteration 10/25 | Loss: 0.00093777
Iteration 11/25 | Loss: 0.00089825
Iteration 12/25 | Loss: 0.00088179
Iteration 13/25 | Loss: 0.00086885
Iteration 14/25 | Loss: 0.00085653
Iteration 15/25 | Loss: 0.00085096
Iteration 16/25 | Loss: 0.00084577
Iteration 17/25 | Loss: 0.00084865
Iteration 18/25 | Loss: 0.00083342
Iteration 19/25 | Loss: 0.00082906
Iteration 20/25 | Loss: 0.00082655
Iteration 21/25 | Loss: 0.00082255
Iteration 22/25 | Loss: 0.00082355
Iteration 23/25 | Loss: 0.00082299
Iteration 24/25 | Loss: 0.00082248
Iteration 25/25 | Loss: 0.00082114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84717655
Iteration 2/25 | Loss: 0.00359209
Iteration 3/25 | Loss: 0.00039932
Iteration 4/25 | Loss: 0.00039932
Iteration 5/25 | Loss: 0.00039932
Iteration 6/25 | Loss: 0.00039932
Iteration 7/25 | Loss: 0.00039932
Iteration 8/25 | Loss: 0.00039932
Iteration 9/25 | Loss: 0.00039932
Iteration 10/25 | Loss: 0.00039932
Iteration 11/25 | Loss: 0.00039932
Iteration 12/25 | Loss: 0.00039932
Iteration 13/25 | Loss: 0.00039932
Iteration 14/25 | Loss: 0.00039932
Iteration 15/25 | Loss: 0.00039932
Iteration 16/25 | Loss: 0.00039932
Iteration 17/25 | Loss: 0.00039932
Iteration 18/25 | Loss: 0.00039932
Iteration 19/25 | Loss: 0.00039932
Iteration 20/25 | Loss: 0.00039932
Iteration 21/25 | Loss: 0.00039932
Iteration 22/25 | Loss: 0.00039932
Iteration 23/25 | Loss: 0.00039932
Iteration 24/25 | Loss: 0.00039932
Iteration 25/25 | Loss: 0.00039932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039932
Iteration 2/1000 | Loss: 0.00007439
Iteration 3/1000 | Loss: 0.00018063
Iteration 4/1000 | Loss: 0.00071566
Iteration 5/1000 | Loss: 0.00013893
Iteration 6/1000 | Loss: 0.00008183
Iteration 7/1000 | Loss: 0.00006076
Iteration 8/1000 | Loss: 0.00005075
Iteration 9/1000 | Loss: 0.00005892
Iteration 10/1000 | Loss: 0.00005872
Iteration 11/1000 | Loss: 0.00006708
Iteration 12/1000 | Loss: 0.00005702
Iteration 13/1000 | Loss: 0.00005597
Iteration 14/1000 | Loss: 0.00005720
Iteration 15/1000 | Loss: 0.00006500
Iteration 16/1000 | Loss: 0.00006301
Iteration 17/1000 | Loss: 0.00006330
Iteration 18/1000 | Loss: 0.00020542
Iteration 19/1000 | Loss: 0.00021017
Iteration 20/1000 | Loss: 0.00031337
Iteration 21/1000 | Loss: 0.00006121
Iteration 22/1000 | Loss: 0.00005972
Iteration 23/1000 | Loss: 0.00005210
Iteration 24/1000 | Loss: 0.00005702
Iteration 25/1000 | Loss: 0.00004775
Iteration 26/1000 | Loss: 0.00005375
Iteration 27/1000 | Loss: 0.00004548
Iteration 28/1000 | Loss: 0.00004951
Iteration 29/1000 | Loss: 0.00004114
Iteration 30/1000 | Loss: 0.00004428
Iteration 31/1000 | Loss: 0.00003665
Iteration 32/1000 | Loss: 0.00004476
Iteration 33/1000 | Loss: 0.00004495
Iteration 34/1000 | Loss: 0.00003926
Iteration 35/1000 | Loss: 0.00004436
Iteration 36/1000 | Loss: 0.00003681
Iteration 37/1000 | Loss: 0.00005405
Iteration 38/1000 | Loss: 0.00004028
Iteration 39/1000 | Loss: 0.00004152
Iteration 40/1000 | Loss: 0.00003930
Iteration 41/1000 | Loss: 0.00004157
Iteration 42/1000 | Loss: 0.00003204
Iteration 43/1000 | Loss: 0.00003259
Iteration 44/1000 | Loss: 0.00003477
Iteration 45/1000 | Loss: 0.00003701
Iteration 46/1000 | Loss: 0.00004095
Iteration 47/1000 | Loss: 0.00003816
Iteration 48/1000 | Loss: 0.00004101
Iteration 49/1000 | Loss: 0.00004391
Iteration 50/1000 | Loss: 0.00004003
Iteration 51/1000 | Loss: 0.00003634
Iteration 52/1000 | Loss: 0.00004033
Iteration 53/1000 | Loss: 0.00003913
Iteration 54/1000 | Loss: 0.00003964
Iteration 55/1000 | Loss: 0.00003748
Iteration 56/1000 | Loss: 0.00003914
Iteration 57/1000 | Loss: 0.00003783
Iteration 58/1000 | Loss: 0.00005109
Iteration 59/1000 | Loss: 0.00004077
Iteration 60/1000 | Loss: 0.00004199
Iteration 61/1000 | Loss: 0.00004144
Iteration 62/1000 | Loss: 0.00004323
Iteration 63/1000 | Loss: 0.00003734
Iteration 64/1000 | Loss: 0.00003785
Iteration 65/1000 | Loss: 0.00004022
Iteration 66/1000 | Loss: 0.00004681
Iteration 67/1000 | Loss: 0.00003704
Iteration 68/1000 | Loss: 0.00004080
Iteration 69/1000 | Loss: 0.00004109
Iteration 70/1000 | Loss: 0.00003904
Iteration 71/1000 | Loss: 0.00004229
Iteration 72/1000 | Loss: 0.00003555
Iteration 73/1000 | Loss: 0.00004400
Iteration 74/1000 | Loss: 0.00004629
Iteration 75/1000 | Loss: 0.00004168
Iteration 76/1000 | Loss: 0.00004547
Iteration 77/1000 | Loss: 0.00004450
Iteration 78/1000 | Loss: 0.00004668
Iteration 79/1000 | Loss: 0.00003385
Iteration 80/1000 | Loss: 0.00004279
Iteration 81/1000 | Loss: 0.00004557
Iteration 82/1000 | Loss: 0.00002755
Iteration 83/1000 | Loss: 0.00002430
Iteration 84/1000 | Loss: 0.00002340
Iteration 85/1000 | Loss: 0.00002279
Iteration 86/1000 | Loss: 0.00002237
Iteration 87/1000 | Loss: 0.00005042
Iteration 88/1000 | Loss: 0.00002328
Iteration 89/1000 | Loss: 0.00002240
Iteration 90/1000 | Loss: 0.00002670
Iteration 91/1000 | Loss: 0.00002194
Iteration 92/1000 | Loss: 0.00002185
Iteration 93/1000 | Loss: 0.00002185
Iteration 94/1000 | Loss: 0.00002182
Iteration 95/1000 | Loss: 0.00002182
Iteration 96/1000 | Loss: 0.00002182
Iteration 97/1000 | Loss: 0.00002182
Iteration 98/1000 | Loss: 0.00002182
Iteration 99/1000 | Loss: 0.00002181
Iteration 100/1000 | Loss: 0.00002181
Iteration 101/1000 | Loss: 0.00002181
Iteration 102/1000 | Loss: 0.00002180
Iteration 103/1000 | Loss: 0.00002180
Iteration 104/1000 | Loss: 0.00002180
Iteration 105/1000 | Loss: 0.00002179
Iteration 106/1000 | Loss: 0.00002179
Iteration 107/1000 | Loss: 0.00002179
Iteration 108/1000 | Loss: 0.00002178
Iteration 109/1000 | Loss: 0.00002178
Iteration 110/1000 | Loss: 0.00002178
Iteration 111/1000 | Loss: 0.00002178
Iteration 112/1000 | Loss: 0.00002178
Iteration 113/1000 | Loss: 0.00002178
Iteration 114/1000 | Loss: 0.00002178
Iteration 115/1000 | Loss: 0.00002177
Iteration 116/1000 | Loss: 0.00002177
Iteration 117/1000 | Loss: 0.00002177
Iteration 118/1000 | Loss: 0.00002177
Iteration 119/1000 | Loss: 0.00002177
Iteration 120/1000 | Loss: 0.00002176
Iteration 121/1000 | Loss: 0.00002176
Iteration 122/1000 | Loss: 0.00002176
Iteration 123/1000 | Loss: 0.00002176
Iteration 124/1000 | Loss: 0.00002175
Iteration 125/1000 | Loss: 0.00002175
Iteration 126/1000 | Loss: 0.00002175
Iteration 127/1000 | Loss: 0.00002174
Iteration 128/1000 | Loss: 0.00002173
Iteration 129/1000 | Loss: 0.00002173
Iteration 130/1000 | Loss: 0.00002173
Iteration 131/1000 | Loss: 0.00002172
Iteration 132/1000 | Loss: 0.00002172
Iteration 133/1000 | Loss: 0.00002171
Iteration 134/1000 | Loss: 0.00002170
Iteration 135/1000 | Loss: 0.00002170
Iteration 136/1000 | Loss: 0.00002161
Iteration 137/1000 | Loss: 0.00002160
Iteration 138/1000 | Loss: 0.00002159
Iteration 139/1000 | Loss: 0.00002159
Iteration 140/1000 | Loss: 0.00002159
Iteration 141/1000 | Loss: 0.00002158
Iteration 142/1000 | Loss: 0.00002158
Iteration 143/1000 | Loss: 0.00002158
Iteration 144/1000 | Loss: 0.00002158
Iteration 145/1000 | Loss: 0.00002158
Iteration 146/1000 | Loss: 0.00002158
Iteration 147/1000 | Loss: 0.00002157
Iteration 148/1000 | Loss: 0.00002157
Iteration 149/1000 | Loss: 0.00002157
Iteration 150/1000 | Loss: 0.00002157
Iteration 151/1000 | Loss: 0.00002157
Iteration 152/1000 | Loss: 0.00002157
Iteration 153/1000 | Loss: 0.00002156
Iteration 154/1000 | Loss: 0.00002156
Iteration 155/1000 | Loss: 0.00002156
Iteration 156/1000 | Loss: 0.00002156
Iteration 157/1000 | Loss: 0.00002156
Iteration 158/1000 | Loss: 0.00002156
Iteration 159/1000 | Loss: 0.00002156
Iteration 160/1000 | Loss: 0.00002156
Iteration 161/1000 | Loss: 0.00002156
Iteration 162/1000 | Loss: 0.00002155
Iteration 163/1000 | Loss: 0.00002155
Iteration 164/1000 | Loss: 0.00002155
Iteration 165/1000 | Loss: 0.00002155
Iteration 166/1000 | Loss: 0.00002155
Iteration 167/1000 | Loss: 0.00002155
Iteration 168/1000 | Loss: 0.00002155
Iteration 169/1000 | Loss: 0.00002155
Iteration 170/1000 | Loss: 0.00002155
Iteration 171/1000 | Loss: 0.00002155
Iteration 172/1000 | Loss: 0.00002155
Iteration 173/1000 | Loss: 0.00002155
Iteration 174/1000 | Loss: 0.00002155
Iteration 175/1000 | Loss: 0.00002155
Iteration 176/1000 | Loss: 0.00002155
Iteration 177/1000 | Loss: 0.00002155
Iteration 178/1000 | Loss: 0.00002155
Iteration 179/1000 | Loss: 0.00002155
Iteration 180/1000 | Loss: 0.00002155
Iteration 181/1000 | Loss: 0.00002155
Iteration 182/1000 | Loss: 0.00002155
Iteration 183/1000 | Loss: 0.00002155
Iteration 184/1000 | Loss: 0.00002155
Iteration 185/1000 | Loss: 0.00002155
Iteration 186/1000 | Loss: 0.00002155
Iteration 187/1000 | Loss: 0.00002155
Iteration 188/1000 | Loss: 0.00002155
Iteration 189/1000 | Loss: 0.00002155
Iteration 190/1000 | Loss: 0.00002155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.155172660422977e-05, 2.155172660422977e-05, 2.155172660422977e-05, 2.155172660422977e-05, 2.155172660422977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.155172660422977e-05

Optimization complete. Final v2v error: 3.8111753463745117 mm

Highest mean error: 5.991203784942627 mm for frame 125

Lowest mean error: 3.0005953311920166 mm for frame 80

Saving results

Total time: 206.16929602622986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093245
Iteration 2/25 | Loss: 0.00172261
Iteration 3/25 | Loss: 0.00107384
Iteration 4/25 | Loss: 0.00071389
Iteration 5/25 | Loss: 0.00065756
Iteration 6/25 | Loss: 0.00064006
Iteration 7/25 | Loss: 0.00063397
Iteration 8/25 | Loss: 0.00063233
Iteration 9/25 | Loss: 0.00063215
Iteration 10/25 | Loss: 0.00063215
Iteration 11/25 | Loss: 0.00063215
Iteration 12/25 | Loss: 0.00063214
Iteration 13/25 | Loss: 0.00063214
Iteration 14/25 | Loss: 0.00063214
Iteration 15/25 | Loss: 0.00063214
Iteration 16/25 | Loss: 0.00063214
Iteration 17/25 | Loss: 0.00063214
Iteration 18/25 | Loss: 0.00063214
Iteration 19/25 | Loss: 0.00063214
Iteration 20/25 | Loss: 0.00063214
Iteration 21/25 | Loss: 0.00063214
Iteration 22/25 | Loss: 0.00063213
Iteration 23/25 | Loss: 0.00063213
Iteration 24/25 | Loss: 0.00063213
Iteration 25/25 | Loss: 0.00063213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.66623497
Iteration 2/25 | Loss: 0.00032989
Iteration 3/25 | Loss: 0.00032989
Iteration 4/25 | Loss: 0.00032989
Iteration 5/25 | Loss: 0.00032989
Iteration 6/25 | Loss: 0.00032989
Iteration 7/25 | Loss: 0.00032989
Iteration 8/25 | Loss: 0.00032989
Iteration 9/25 | Loss: 0.00032989
Iteration 10/25 | Loss: 0.00032989
Iteration 11/25 | Loss: 0.00032989
Iteration 12/25 | Loss: 0.00032989
Iteration 13/25 | Loss: 0.00032989
Iteration 14/25 | Loss: 0.00032989
Iteration 15/25 | Loss: 0.00032989
Iteration 16/25 | Loss: 0.00032989
Iteration 17/25 | Loss: 0.00032989
Iteration 18/25 | Loss: 0.00032989
Iteration 19/25 | Loss: 0.00032989
Iteration 20/25 | Loss: 0.00032989
Iteration 21/25 | Loss: 0.00032989
Iteration 22/25 | Loss: 0.00032989
Iteration 23/25 | Loss: 0.00032989
Iteration 24/25 | Loss: 0.00032989
Iteration 25/25 | Loss: 0.00032989

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032989
Iteration 2/1000 | Loss: 0.00003331
Iteration 3/1000 | Loss: 0.00002315
Iteration 4/1000 | Loss: 0.00001912
Iteration 5/1000 | Loss: 0.00003892
Iteration 6/1000 | Loss: 0.00008838
Iteration 7/1000 | Loss: 0.00004510
Iteration 8/1000 | Loss: 0.00001704
Iteration 9/1000 | Loss: 0.00001677
Iteration 10/1000 | Loss: 0.00002573
Iteration 11/1000 | Loss: 0.00001644
Iteration 12/1000 | Loss: 0.00002072
Iteration 13/1000 | Loss: 0.00001629
Iteration 14/1000 | Loss: 0.00001629
Iteration 15/1000 | Loss: 0.00001628
Iteration 16/1000 | Loss: 0.00001628
Iteration 17/1000 | Loss: 0.00001621
Iteration 18/1000 | Loss: 0.00001610
Iteration 19/1000 | Loss: 0.00001603
Iteration 20/1000 | Loss: 0.00001602
Iteration 21/1000 | Loss: 0.00001602
Iteration 22/1000 | Loss: 0.00001600
Iteration 23/1000 | Loss: 0.00001598
Iteration 24/1000 | Loss: 0.00001597
Iteration 25/1000 | Loss: 0.00001597
Iteration 26/1000 | Loss: 0.00001596
Iteration 27/1000 | Loss: 0.00001596
Iteration 28/1000 | Loss: 0.00001591
Iteration 29/1000 | Loss: 0.00001590
Iteration 30/1000 | Loss: 0.00001590
Iteration 31/1000 | Loss: 0.00001589
Iteration 32/1000 | Loss: 0.00001588
Iteration 33/1000 | Loss: 0.00001586
Iteration 34/1000 | Loss: 0.00001586
Iteration 35/1000 | Loss: 0.00001585
Iteration 36/1000 | Loss: 0.00001585
Iteration 37/1000 | Loss: 0.00001584
Iteration 38/1000 | Loss: 0.00001584
Iteration 39/1000 | Loss: 0.00001583
Iteration 40/1000 | Loss: 0.00001583
Iteration 41/1000 | Loss: 0.00001582
Iteration 42/1000 | Loss: 0.00001581
Iteration 43/1000 | Loss: 0.00001581
Iteration 44/1000 | Loss: 0.00001581
Iteration 45/1000 | Loss: 0.00001580
Iteration 46/1000 | Loss: 0.00001580
Iteration 47/1000 | Loss: 0.00001579
Iteration 48/1000 | Loss: 0.00001578
Iteration 49/1000 | Loss: 0.00001573
Iteration 50/1000 | Loss: 0.00001569
Iteration 51/1000 | Loss: 0.00001567
Iteration 52/1000 | Loss: 0.00001567
Iteration 53/1000 | Loss: 0.00001567
Iteration 54/1000 | Loss: 0.00001567
Iteration 55/1000 | Loss: 0.00001566
Iteration 56/1000 | Loss: 0.00001566
Iteration 57/1000 | Loss: 0.00001565
Iteration 58/1000 | Loss: 0.00001564
Iteration 59/1000 | Loss: 0.00001564
Iteration 60/1000 | Loss: 0.00001563
Iteration 61/1000 | Loss: 0.00001563
Iteration 62/1000 | Loss: 0.00001563
Iteration 63/1000 | Loss: 0.00001562
Iteration 64/1000 | Loss: 0.00001562
Iteration 65/1000 | Loss: 0.00001562
Iteration 66/1000 | Loss: 0.00001561
Iteration 67/1000 | Loss: 0.00001561
Iteration 68/1000 | Loss: 0.00001560
Iteration 69/1000 | Loss: 0.00001560
Iteration 70/1000 | Loss: 0.00001560
Iteration 71/1000 | Loss: 0.00001560
Iteration 72/1000 | Loss: 0.00001559
Iteration 73/1000 | Loss: 0.00001559
Iteration 74/1000 | Loss: 0.00001559
Iteration 75/1000 | Loss: 0.00001559
Iteration 76/1000 | Loss: 0.00001559
Iteration 77/1000 | Loss: 0.00001559
Iteration 78/1000 | Loss: 0.00001559
Iteration 79/1000 | Loss: 0.00001559
Iteration 80/1000 | Loss: 0.00001559
Iteration 81/1000 | Loss: 0.00001559
Iteration 82/1000 | Loss: 0.00001559
Iteration 83/1000 | Loss: 0.00001558
Iteration 84/1000 | Loss: 0.00001558
Iteration 85/1000 | Loss: 0.00001558
Iteration 86/1000 | Loss: 0.00001558
Iteration 87/1000 | Loss: 0.00001558
Iteration 88/1000 | Loss: 0.00001558
Iteration 89/1000 | Loss: 0.00001558
Iteration 90/1000 | Loss: 0.00001557
Iteration 91/1000 | Loss: 0.00001557
Iteration 92/1000 | Loss: 0.00001557
Iteration 93/1000 | Loss: 0.00001557
Iteration 94/1000 | Loss: 0.00001557
Iteration 95/1000 | Loss: 0.00001557
Iteration 96/1000 | Loss: 0.00001557
Iteration 97/1000 | Loss: 0.00001557
Iteration 98/1000 | Loss: 0.00001557
Iteration 99/1000 | Loss: 0.00001557
Iteration 100/1000 | Loss: 0.00001557
Iteration 101/1000 | Loss: 0.00001556
Iteration 102/1000 | Loss: 0.00001556
Iteration 103/1000 | Loss: 0.00001556
Iteration 104/1000 | Loss: 0.00001556
Iteration 105/1000 | Loss: 0.00001556
Iteration 106/1000 | Loss: 0.00001556
Iteration 107/1000 | Loss: 0.00001556
Iteration 108/1000 | Loss: 0.00001556
Iteration 109/1000 | Loss: 0.00001556
Iteration 110/1000 | Loss: 0.00001556
Iteration 111/1000 | Loss: 0.00001555
Iteration 112/1000 | Loss: 0.00001555
Iteration 113/1000 | Loss: 0.00001555
Iteration 114/1000 | Loss: 0.00001555
Iteration 115/1000 | Loss: 0.00001555
Iteration 116/1000 | Loss: 0.00001555
Iteration 117/1000 | Loss: 0.00001555
Iteration 118/1000 | Loss: 0.00001555
Iteration 119/1000 | Loss: 0.00001555
Iteration 120/1000 | Loss: 0.00001555
Iteration 121/1000 | Loss: 0.00001555
Iteration 122/1000 | Loss: 0.00001555
Iteration 123/1000 | Loss: 0.00001555
Iteration 124/1000 | Loss: 0.00001554
Iteration 125/1000 | Loss: 0.00001554
Iteration 126/1000 | Loss: 0.00001554
Iteration 127/1000 | Loss: 0.00001554
Iteration 128/1000 | Loss: 0.00001554
Iteration 129/1000 | Loss: 0.00001554
Iteration 130/1000 | Loss: 0.00001554
Iteration 131/1000 | Loss: 0.00001553
Iteration 132/1000 | Loss: 0.00001553
Iteration 133/1000 | Loss: 0.00001553
Iteration 134/1000 | Loss: 0.00001553
Iteration 135/1000 | Loss: 0.00001553
Iteration 136/1000 | Loss: 0.00001553
Iteration 137/1000 | Loss: 0.00001552
Iteration 138/1000 | Loss: 0.00001552
Iteration 139/1000 | Loss: 0.00001552
Iteration 140/1000 | Loss: 0.00001552
Iteration 141/1000 | Loss: 0.00001552
Iteration 142/1000 | Loss: 0.00001552
Iteration 143/1000 | Loss: 0.00001552
Iteration 144/1000 | Loss: 0.00001552
Iteration 145/1000 | Loss: 0.00001552
Iteration 146/1000 | Loss: 0.00001552
Iteration 147/1000 | Loss: 0.00001552
Iteration 148/1000 | Loss: 0.00001552
Iteration 149/1000 | Loss: 0.00001552
Iteration 150/1000 | Loss: 0.00001552
Iteration 151/1000 | Loss: 0.00001552
Iteration 152/1000 | Loss: 0.00001552
Iteration 153/1000 | Loss: 0.00001552
Iteration 154/1000 | Loss: 0.00001552
Iteration 155/1000 | Loss: 0.00001552
Iteration 156/1000 | Loss: 0.00001552
Iteration 157/1000 | Loss: 0.00001552
Iteration 158/1000 | Loss: 0.00001552
Iteration 159/1000 | Loss: 0.00001552
Iteration 160/1000 | Loss: 0.00001552
Iteration 161/1000 | Loss: 0.00001552
Iteration 162/1000 | Loss: 0.00001552
Iteration 163/1000 | Loss: 0.00001552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.5520956367254257e-05, 1.5520956367254257e-05, 1.5520956367254257e-05, 1.5520956367254257e-05, 1.5520956367254257e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5520956367254257e-05

Optimization complete. Final v2v error: 3.3373074531555176 mm

Highest mean error: 3.61902117729187 mm for frame 11

Lowest mean error: 3.007486343383789 mm for frame 145

Saving results

Total time: 46.50663733482361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794247
Iteration 2/25 | Loss: 0.00213137
Iteration 3/25 | Loss: 0.00128803
Iteration 4/25 | Loss: 0.00106771
Iteration 5/25 | Loss: 0.00100965
Iteration 6/25 | Loss: 0.00090476
Iteration 7/25 | Loss: 0.00086001
Iteration 8/25 | Loss: 0.00081961
Iteration 9/25 | Loss: 0.00081985
Iteration 10/25 | Loss: 0.00079558
Iteration 11/25 | Loss: 0.00078545
Iteration 12/25 | Loss: 0.00076008
Iteration 13/25 | Loss: 0.00074785
Iteration 14/25 | Loss: 0.00073525
Iteration 15/25 | Loss: 0.00073879
Iteration 16/25 | Loss: 0.00073632
Iteration 17/25 | Loss: 0.00071498
Iteration 18/25 | Loss: 0.00070876
Iteration 19/25 | Loss: 0.00070838
Iteration 20/25 | Loss: 0.00070830
Iteration 21/25 | Loss: 0.00070830
Iteration 22/25 | Loss: 0.00070830
Iteration 23/25 | Loss: 0.00070830
Iteration 24/25 | Loss: 0.00070830
Iteration 25/25 | Loss: 0.00070830

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.21244526
Iteration 2/25 | Loss: 0.00044393
Iteration 3/25 | Loss: 0.00044390
Iteration 4/25 | Loss: 0.00044390
Iteration 5/25 | Loss: 0.00044389
Iteration 6/25 | Loss: 0.00044389
Iteration 7/25 | Loss: 0.00044389
Iteration 8/25 | Loss: 0.00044389
Iteration 9/25 | Loss: 0.00044389
Iteration 10/25 | Loss: 0.00044389
Iteration 11/25 | Loss: 0.00044389
Iteration 12/25 | Loss: 0.00044389
Iteration 13/25 | Loss: 0.00044389
Iteration 14/25 | Loss: 0.00044389
Iteration 15/25 | Loss: 0.00044389
Iteration 16/25 | Loss: 0.00044389
Iteration 17/25 | Loss: 0.00044389
Iteration 18/25 | Loss: 0.00044389
Iteration 19/25 | Loss: 0.00044389
Iteration 20/25 | Loss: 0.00044389
Iteration 21/25 | Loss: 0.00044389
Iteration 22/25 | Loss: 0.00044389
Iteration 23/25 | Loss: 0.00044389
Iteration 24/25 | Loss: 0.00044389
Iteration 25/25 | Loss: 0.00044389

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044389
Iteration 2/1000 | Loss: 0.00003570
Iteration 3/1000 | Loss: 0.00002780
Iteration 4/1000 | Loss: 0.00002475
Iteration 5/1000 | Loss: 0.00002333
Iteration 6/1000 | Loss: 0.00040217
Iteration 7/1000 | Loss: 0.00023717
Iteration 8/1000 | Loss: 0.00006984
Iteration 9/1000 | Loss: 0.00013241
Iteration 10/1000 | Loss: 0.00002070
Iteration 11/1000 | Loss: 0.00014161
Iteration 12/1000 | Loss: 0.00001862
Iteration 13/1000 | Loss: 0.00001786
Iteration 14/1000 | Loss: 0.00021728
Iteration 15/1000 | Loss: 0.00012725
Iteration 16/1000 | Loss: 0.00034369
Iteration 17/1000 | Loss: 0.00001774
Iteration 18/1000 | Loss: 0.00001742
Iteration 19/1000 | Loss: 0.00001736
Iteration 20/1000 | Loss: 0.00001735
Iteration 21/1000 | Loss: 0.00001719
Iteration 22/1000 | Loss: 0.00001706
Iteration 23/1000 | Loss: 0.00001699
Iteration 24/1000 | Loss: 0.00001698
Iteration 25/1000 | Loss: 0.00001697
Iteration 26/1000 | Loss: 0.00001697
Iteration 27/1000 | Loss: 0.00001696
Iteration 28/1000 | Loss: 0.00001695
Iteration 29/1000 | Loss: 0.00001694
Iteration 30/1000 | Loss: 0.00001693
Iteration 31/1000 | Loss: 0.00001693
Iteration 32/1000 | Loss: 0.00001692
Iteration 33/1000 | Loss: 0.00001692
Iteration 34/1000 | Loss: 0.00001690
Iteration 35/1000 | Loss: 0.00001688
Iteration 36/1000 | Loss: 0.00001687
Iteration 37/1000 | Loss: 0.00001687
Iteration 38/1000 | Loss: 0.00001687
Iteration 39/1000 | Loss: 0.00001686
Iteration 40/1000 | Loss: 0.00001682
Iteration 41/1000 | Loss: 0.00001682
Iteration 42/1000 | Loss: 0.00001681
Iteration 43/1000 | Loss: 0.00001681
Iteration 44/1000 | Loss: 0.00001680
Iteration 45/1000 | Loss: 0.00001680
Iteration 46/1000 | Loss: 0.00001680
Iteration 47/1000 | Loss: 0.00001679
Iteration 48/1000 | Loss: 0.00001679
Iteration 49/1000 | Loss: 0.00001679
Iteration 50/1000 | Loss: 0.00001679
Iteration 51/1000 | Loss: 0.00001679
Iteration 52/1000 | Loss: 0.00001679
Iteration 53/1000 | Loss: 0.00001679
Iteration 54/1000 | Loss: 0.00001678
Iteration 55/1000 | Loss: 0.00001678
Iteration 56/1000 | Loss: 0.00001678
Iteration 57/1000 | Loss: 0.00001678
Iteration 58/1000 | Loss: 0.00001678
Iteration 59/1000 | Loss: 0.00001678
Iteration 60/1000 | Loss: 0.00001678
Iteration 61/1000 | Loss: 0.00001678
Iteration 62/1000 | Loss: 0.00001678
Iteration 63/1000 | Loss: 0.00001678
Iteration 64/1000 | Loss: 0.00001678
Iteration 65/1000 | Loss: 0.00001677
Iteration 66/1000 | Loss: 0.00001677
Iteration 67/1000 | Loss: 0.00001676
Iteration 68/1000 | Loss: 0.00001676
Iteration 69/1000 | Loss: 0.00001676
Iteration 70/1000 | Loss: 0.00001675
Iteration 71/1000 | Loss: 0.00001675
Iteration 72/1000 | Loss: 0.00001675
Iteration 73/1000 | Loss: 0.00001675
Iteration 74/1000 | Loss: 0.00001675
Iteration 75/1000 | Loss: 0.00001674
Iteration 76/1000 | Loss: 0.00001674
Iteration 77/1000 | Loss: 0.00001674
Iteration 78/1000 | Loss: 0.00001673
Iteration 79/1000 | Loss: 0.00001673
Iteration 80/1000 | Loss: 0.00001672
Iteration 81/1000 | Loss: 0.00001672
Iteration 82/1000 | Loss: 0.00001672
Iteration 83/1000 | Loss: 0.00001671
Iteration 84/1000 | Loss: 0.00001671
Iteration 85/1000 | Loss: 0.00001671
Iteration 86/1000 | Loss: 0.00001670
Iteration 87/1000 | Loss: 0.00001670
Iteration 88/1000 | Loss: 0.00001670
Iteration 89/1000 | Loss: 0.00001670
Iteration 90/1000 | Loss: 0.00001670
Iteration 91/1000 | Loss: 0.00001669
Iteration 92/1000 | Loss: 0.00001669
Iteration 93/1000 | Loss: 0.00001669
Iteration 94/1000 | Loss: 0.00001669
Iteration 95/1000 | Loss: 0.00001669
Iteration 96/1000 | Loss: 0.00001669
Iteration 97/1000 | Loss: 0.00001669
Iteration 98/1000 | Loss: 0.00001669
Iteration 99/1000 | Loss: 0.00001669
Iteration 100/1000 | Loss: 0.00001669
Iteration 101/1000 | Loss: 0.00001669
Iteration 102/1000 | Loss: 0.00001669
Iteration 103/1000 | Loss: 0.00001669
Iteration 104/1000 | Loss: 0.00001669
Iteration 105/1000 | Loss: 0.00001669
Iteration 106/1000 | Loss: 0.00001669
Iteration 107/1000 | Loss: 0.00001669
Iteration 108/1000 | Loss: 0.00001669
Iteration 109/1000 | Loss: 0.00001669
Iteration 110/1000 | Loss: 0.00001669
Iteration 111/1000 | Loss: 0.00001669
Iteration 112/1000 | Loss: 0.00001669
Iteration 113/1000 | Loss: 0.00001669
Iteration 114/1000 | Loss: 0.00001669
Iteration 115/1000 | Loss: 0.00001669
Iteration 116/1000 | Loss: 0.00001669
Iteration 117/1000 | Loss: 0.00001669
Iteration 118/1000 | Loss: 0.00001668
Iteration 119/1000 | Loss: 0.00001668
Iteration 120/1000 | Loss: 0.00001668
Iteration 121/1000 | Loss: 0.00001668
Iteration 122/1000 | Loss: 0.00001668
Iteration 123/1000 | Loss: 0.00001668
Iteration 124/1000 | Loss: 0.00001668
Iteration 125/1000 | Loss: 0.00001668
Iteration 126/1000 | Loss: 0.00001668
Iteration 127/1000 | Loss: 0.00001668
Iteration 128/1000 | Loss: 0.00001668
Iteration 129/1000 | Loss: 0.00001668
Iteration 130/1000 | Loss: 0.00001668
Iteration 131/1000 | Loss: 0.00001668
Iteration 132/1000 | Loss: 0.00001668
Iteration 133/1000 | Loss: 0.00001668
Iteration 134/1000 | Loss: 0.00001668
Iteration 135/1000 | Loss: 0.00001668
Iteration 136/1000 | Loss: 0.00001668
Iteration 137/1000 | Loss: 0.00001668
Iteration 138/1000 | Loss: 0.00001668
Iteration 139/1000 | Loss: 0.00001668
Iteration 140/1000 | Loss: 0.00001668
Iteration 141/1000 | Loss: 0.00001668
Iteration 142/1000 | Loss: 0.00001668
Iteration 143/1000 | Loss: 0.00001668
Iteration 144/1000 | Loss: 0.00001668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.668329605308827e-05, 1.668329605308827e-05, 1.668329605308827e-05, 1.668329605308827e-05, 1.668329605308827e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.668329605308827e-05

Optimization complete. Final v2v error: 3.41794753074646 mm

Highest mean error: 4.036680221557617 mm for frame 7

Lowest mean error: 2.9778642654418945 mm for frame 41

Saving results

Total time: 74.52972507476807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049162
Iteration 2/25 | Loss: 0.00266867
Iteration 3/25 | Loss: 0.00141647
Iteration 4/25 | Loss: 0.00122691
Iteration 5/25 | Loss: 0.00120499
Iteration 6/25 | Loss: 0.00106077
Iteration 7/25 | Loss: 0.00096203
Iteration 8/25 | Loss: 0.00101547
Iteration 9/25 | Loss: 0.00103239
Iteration 10/25 | Loss: 0.00097270
Iteration 11/25 | Loss: 0.00091041
Iteration 12/25 | Loss: 0.00083890
Iteration 13/25 | Loss: 0.00086067
Iteration 14/25 | Loss: 0.00078860
Iteration 15/25 | Loss: 0.00078912
Iteration 16/25 | Loss: 0.00079840
Iteration 17/25 | Loss: 0.00077538
Iteration 18/25 | Loss: 0.00073156
Iteration 19/25 | Loss: 0.00070259
Iteration 20/25 | Loss: 0.00069133
Iteration 21/25 | Loss: 0.00068235
Iteration 22/25 | Loss: 0.00067972
Iteration 23/25 | Loss: 0.00067517
Iteration 24/25 | Loss: 0.00067159
Iteration 25/25 | Loss: 0.00067125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67578602
Iteration 2/25 | Loss: 0.00031966
Iteration 3/25 | Loss: 0.00031966
Iteration 4/25 | Loss: 0.00031966
Iteration 5/25 | Loss: 0.00031966
Iteration 6/25 | Loss: 0.00031966
Iteration 7/25 | Loss: 0.00031966
Iteration 8/25 | Loss: 0.00031966
Iteration 9/25 | Loss: 0.00031966
Iteration 10/25 | Loss: 0.00031966
Iteration 11/25 | Loss: 0.00031966
Iteration 12/25 | Loss: 0.00031966
Iteration 13/25 | Loss: 0.00031966
Iteration 14/25 | Loss: 0.00031966
Iteration 15/25 | Loss: 0.00031966
Iteration 16/25 | Loss: 0.00031966
Iteration 17/25 | Loss: 0.00031966
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00031965834205038846, 0.00031965834205038846, 0.00031965834205038846, 0.00031965834205038846, 0.00031965834205038846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031965834205038846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031966
Iteration 2/1000 | Loss: 0.00002489
Iteration 3/1000 | Loss: 0.00022046
Iteration 4/1000 | Loss: 0.00002862
Iteration 5/1000 | Loss: 0.00001814
Iteration 6/1000 | Loss: 0.00001545
Iteration 7/1000 | Loss: 0.00001473
Iteration 8/1000 | Loss: 0.00002464
Iteration 9/1000 | Loss: 0.00015042
Iteration 10/1000 | Loss: 0.00001523
Iteration 11/1000 | Loss: 0.00001396
Iteration 12/1000 | Loss: 0.00011765
Iteration 13/1000 | Loss: 0.00001661
Iteration 14/1000 | Loss: 0.00001339
Iteration 15/1000 | Loss: 0.00007846
Iteration 16/1000 | Loss: 0.00008395
Iteration 17/1000 | Loss: 0.00001305
Iteration 18/1000 | Loss: 0.00001304
Iteration 19/1000 | Loss: 0.00013228
Iteration 20/1000 | Loss: 0.00002113
Iteration 21/1000 | Loss: 0.00001622
Iteration 22/1000 | Loss: 0.00001294
Iteration 23/1000 | Loss: 0.00001266
Iteration 24/1000 | Loss: 0.00001261
Iteration 25/1000 | Loss: 0.00001247
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001245
Iteration 28/1000 | Loss: 0.00001244
Iteration 29/1000 | Loss: 0.00001243
Iteration 30/1000 | Loss: 0.00001243
Iteration 31/1000 | Loss: 0.00001242
Iteration 32/1000 | Loss: 0.00001242
Iteration 33/1000 | Loss: 0.00001242
Iteration 34/1000 | Loss: 0.00001241
Iteration 35/1000 | Loss: 0.00001241
Iteration 36/1000 | Loss: 0.00001241
Iteration 37/1000 | Loss: 0.00001241
Iteration 38/1000 | Loss: 0.00001240
Iteration 39/1000 | Loss: 0.00001240
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001239
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001238
Iteration 44/1000 | Loss: 0.00001238
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001237
Iteration 48/1000 | Loss: 0.00001237
Iteration 49/1000 | Loss: 0.00001237
Iteration 50/1000 | Loss: 0.00001237
Iteration 51/1000 | Loss: 0.00001237
Iteration 52/1000 | Loss: 0.00001237
Iteration 53/1000 | Loss: 0.00001237
Iteration 54/1000 | Loss: 0.00001237
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001237
Iteration 57/1000 | Loss: 0.00001237
Iteration 58/1000 | Loss: 0.00001237
Iteration 59/1000 | Loss: 0.00001237
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001236
Iteration 63/1000 | Loss: 0.00001236
Iteration 64/1000 | Loss: 0.00001236
Iteration 65/1000 | Loss: 0.00001236
Iteration 66/1000 | Loss: 0.00001236
Iteration 67/1000 | Loss: 0.00001236
Iteration 68/1000 | Loss: 0.00001236
Iteration 69/1000 | Loss: 0.00001236
Iteration 70/1000 | Loss: 0.00001236
Iteration 71/1000 | Loss: 0.00001236
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001236
Iteration 74/1000 | Loss: 0.00001235
Iteration 75/1000 | Loss: 0.00001235
Iteration 76/1000 | Loss: 0.00001235
Iteration 77/1000 | Loss: 0.00001235
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001235
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001235
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001234
Iteration 84/1000 | Loss: 0.00001233
Iteration 85/1000 | Loss: 0.00001233
Iteration 86/1000 | Loss: 0.00001232
Iteration 87/1000 | Loss: 0.00001232
Iteration 88/1000 | Loss: 0.00001232
Iteration 89/1000 | Loss: 0.00001232
Iteration 90/1000 | Loss: 0.00001232
Iteration 91/1000 | Loss: 0.00001231
Iteration 92/1000 | Loss: 0.00001231
Iteration 93/1000 | Loss: 0.00001231
Iteration 94/1000 | Loss: 0.00001231
Iteration 95/1000 | Loss: 0.00001231
Iteration 96/1000 | Loss: 0.00001231
Iteration 97/1000 | Loss: 0.00001231
Iteration 98/1000 | Loss: 0.00001230
Iteration 99/1000 | Loss: 0.00001230
Iteration 100/1000 | Loss: 0.00001229
Iteration 101/1000 | Loss: 0.00001228
Iteration 102/1000 | Loss: 0.00001228
Iteration 103/1000 | Loss: 0.00001228
Iteration 104/1000 | Loss: 0.00001228
Iteration 105/1000 | Loss: 0.00001228
Iteration 106/1000 | Loss: 0.00001228
Iteration 107/1000 | Loss: 0.00001227
Iteration 108/1000 | Loss: 0.00001227
Iteration 109/1000 | Loss: 0.00001227
Iteration 110/1000 | Loss: 0.00001227
Iteration 111/1000 | Loss: 0.00001227
Iteration 112/1000 | Loss: 0.00001414
Iteration 113/1000 | Loss: 0.00001302
Iteration 114/1000 | Loss: 0.00001226
Iteration 115/1000 | Loss: 0.00001224
Iteration 116/1000 | Loss: 0.00001224
Iteration 117/1000 | Loss: 0.00001224
Iteration 118/1000 | Loss: 0.00001224
Iteration 119/1000 | Loss: 0.00001224
Iteration 120/1000 | Loss: 0.00001224
Iteration 121/1000 | Loss: 0.00001224
Iteration 122/1000 | Loss: 0.00001224
Iteration 123/1000 | Loss: 0.00001224
Iteration 124/1000 | Loss: 0.00001223
Iteration 125/1000 | Loss: 0.00001223
Iteration 126/1000 | Loss: 0.00001223
Iteration 127/1000 | Loss: 0.00001223
Iteration 128/1000 | Loss: 0.00001223
Iteration 129/1000 | Loss: 0.00001222
Iteration 130/1000 | Loss: 0.00001222
Iteration 131/1000 | Loss: 0.00001222
Iteration 132/1000 | Loss: 0.00001222
Iteration 133/1000 | Loss: 0.00001222
Iteration 134/1000 | Loss: 0.00001222
Iteration 135/1000 | Loss: 0.00001222
Iteration 136/1000 | Loss: 0.00001222
Iteration 137/1000 | Loss: 0.00001222
Iteration 138/1000 | Loss: 0.00001222
Iteration 139/1000 | Loss: 0.00001222
Iteration 140/1000 | Loss: 0.00001222
Iteration 141/1000 | Loss: 0.00001221
Iteration 142/1000 | Loss: 0.00001221
Iteration 143/1000 | Loss: 0.00001221
Iteration 144/1000 | Loss: 0.00001221
Iteration 145/1000 | Loss: 0.00001221
Iteration 146/1000 | Loss: 0.00001221
Iteration 147/1000 | Loss: 0.00001221
Iteration 148/1000 | Loss: 0.00001221
Iteration 149/1000 | Loss: 0.00001221
Iteration 150/1000 | Loss: 0.00001221
Iteration 151/1000 | Loss: 0.00001221
Iteration 152/1000 | Loss: 0.00001221
Iteration 153/1000 | Loss: 0.00001221
Iteration 154/1000 | Loss: 0.00001221
Iteration 155/1000 | Loss: 0.00001221
Iteration 156/1000 | Loss: 0.00001221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.2213872651045676e-05, 1.2213872651045676e-05, 1.2213872651045676e-05, 1.2213872651045676e-05, 1.2213872651045676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2213872651045676e-05

Optimization complete. Final v2v error: 2.8975672721862793 mm

Highest mean error: 8.847381591796875 mm for frame 47

Lowest mean error: 2.416598320007324 mm for frame 38

Saving results

Total time: 91.1190435886383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838141
Iteration 2/25 | Loss: 0.00118017
Iteration 3/25 | Loss: 0.00092182
Iteration 4/25 | Loss: 0.00087309
Iteration 5/25 | Loss: 0.00086516
Iteration 6/25 | Loss: 0.00086271
Iteration 7/25 | Loss: 0.00086224
Iteration 8/25 | Loss: 0.00086224
Iteration 9/25 | Loss: 0.00086224
Iteration 10/25 | Loss: 0.00086224
Iteration 11/25 | Loss: 0.00086224
Iteration 12/25 | Loss: 0.00086224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008622431778348982, 0.0008622431778348982, 0.0008622431778348982, 0.0008622431778348982, 0.0008622431778348982]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008622431778348982

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40628576
Iteration 2/25 | Loss: 0.00046065
Iteration 3/25 | Loss: 0.00046064
Iteration 4/25 | Loss: 0.00046064
Iteration 5/25 | Loss: 0.00046064
Iteration 6/25 | Loss: 0.00046064
Iteration 7/25 | Loss: 0.00046063
Iteration 8/25 | Loss: 0.00046063
Iteration 9/25 | Loss: 0.00046063
Iteration 10/25 | Loss: 0.00046063
Iteration 11/25 | Loss: 0.00046063
Iteration 12/25 | Loss: 0.00046063
Iteration 13/25 | Loss: 0.00046063
Iteration 14/25 | Loss: 0.00046063
Iteration 15/25 | Loss: 0.00046063
Iteration 16/25 | Loss: 0.00046063
Iteration 17/25 | Loss: 0.00046063
Iteration 18/25 | Loss: 0.00046063
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004606336879078299, 0.0004606336879078299, 0.0004606336879078299, 0.0004606336879078299, 0.0004606336879078299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004606336879078299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046063
Iteration 2/1000 | Loss: 0.00003735
Iteration 3/1000 | Loss: 0.00002780
Iteration 4/1000 | Loss: 0.00002466
Iteration 5/1000 | Loss: 0.00002326
Iteration 6/1000 | Loss: 0.00002229
Iteration 7/1000 | Loss: 0.00002168
Iteration 8/1000 | Loss: 0.00002124
Iteration 9/1000 | Loss: 0.00002095
Iteration 10/1000 | Loss: 0.00002070
Iteration 11/1000 | Loss: 0.00002053
Iteration 12/1000 | Loss: 0.00002049
Iteration 13/1000 | Loss: 0.00002045
Iteration 14/1000 | Loss: 0.00002044
Iteration 15/1000 | Loss: 0.00002043
Iteration 16/1000 | Loss: 0.00002040
Iteration 17/1000 | Loss: 0.00002039
Iteration 18/1000 | Loss: 0.00002039
Iteration 19/1000 | Loss: 0.00002038
Iteration 20/1000 | Loss: 0.00002038
Iteration 21/1000 | Loss: 0.00002038
Iteration 22/1000 | Loss: 0.00002037
Iteration 23/1000 | Loss: 0.00002037
Iteration 24/1000 | Loss: 0.00002037
Iteration 25/1000 | Loss: 0.00002035
Iteration 26/1000 | Loss: 0.00002035
Iteration 27/1000 | Loss: 0.00002035
Iteration 28/1000 | Loss: 0.00002035
Iteration 29/1000 | Loss: 0.00002035
Iteration 30/1000 | Loss: 0.00002035
Iteration 31/1000 | Loss: 0.00002035
Iteration 32/1000 | Loss: 0.00002035
Iteration 33/1000 | Loss: 0.00002035
Iteration 34/1000 | Loss: 0.00002035
Iteration 35/1000 | Loss: 0.00002034
Iteration 36/1000 | Loss: 0.00002034
Iteration 37/1000 | Loss: 0.00002033
Iteration 38/1000 | Loss: 0.00002032
Iteration 39/1000 | Loss: 0.00002032
Iteration 40/1000 | Loss: 0.00002032
Iteration 41/1000 | Loss: 0.00002031
Iteration 42/1000 | Loss: 0.00002031
Iteration 43/1000 | Loss: 0.00002031
Iteration 44/1000 | Loss: 0.00002030
Iteration 45/1000 | Loss: 0.00002030
Iteration 46/1000 | Loss: 0.00002030
Iteration 47/1000 | Loss: 0.00002030
Iteration 48/1000 | Loss: 0.00002030
Iteration 49/1000 | Loss: 0.00002030
Iteration 50/1000 | Loss: 0.00002029
Iteration 51/1000 | Loss: 0.00002029
Iteration 52/1000 | Loss: 0.00002029
Iteration 53/1000 | Loss: 0.00002029
Iteration 54/1000 | Loss: 0.00002029
Iteration 55/1000 | Loss: 0.00002028
Iteration 56/1000 | Loss: 0.00002028
Iteration 57/1000 | Loss: 0.00002028
Iteration 58/1000 | Loss: 0.00002028
Iteration 59/1000 | Loss: 0.00002028
Iteration 60/1000 | Loss: 0.00002027
Iteration 61/1000 | Loss: 0.00002027
Iteration 62/1000 | Loss: 0.00002027
Iteration 63/1000 | Loss: 0.00002027
Iteration 64/1000 | Loss: 0.00002027
Iteration 65/1000 | Loss: 0.00002027
Iteration 66/1000 | Loss: 0.00002027
Iteration 67/1000 | Loss: 0.00002026
Iteration 68/1000 | Loss: 0.00002026
Iteration 69/1000 | Loss: 0.00002026
Iteration 70/1000 | Loss: 0.00002026
Iteration 71/1000 | Loss: 0.00002026
Iteration 72/1000 | Loss: 0.00002026
Iteration 73/1000 | Loss: 0.00002024
Iteration 74/1000 | Loss: 0.00002024
Iteration 75/1000 | Loss: 0.00002024
Iteration 76/1000 | Loss: 0.00002024
Iteration 77/1000 | Loss: 0.00002024
Iteration 78/1000 | Loss: 0.00002024
Iteration 79/1000 | Loss: 0.00002024
Iteration 80/1000 | Loss: 0.00002024
Iteration 81/1000 | Loss: 0.00002023
Iteration 82/1000 | Loss: 0.00002023
Iteration 83/1000 | Loss: 0.00002023
Iteration 84/1000 | Loss: 0.00002023
Iteration 85/1000 | Loss: 0.00002023
Iteration 86/1000 | Loss: 0.00002023
Iteration 87/1000 | Loss: 0.00002023
Iteration 88/1000 | Loss: 0.00002023
Iteration 89/1000 | Loss: 0.00002023
Iteration 90/1000 | Loss: 0.00002022
Iteration 91/1000 | Loss: 0.00002022
Iteration 92/1000 | Loss: 0.00002022
Iteration 93/1000 | Loss: 0.00002022
Iteration 94/1000 | Loss: 0.00002021
Iteration 95/1000 | Loss: 0.00002021
Iteration 96/1000 | Loss: 0.00002021
Iteration 97/1000 | Loss: 0.00002021
Iteration 98/1000 | Loss: 0.00002021
Iteration 99/1000 | Loss: 0.00002021
Iteration 100/1000 | Loss: 0.00002021
Iteration 101/1000 | Loss: 0.00002021
Iteration 102/1000 | Loss: 0.00002021
Iteration 103/1000 | Loss: 0.00002021
Iteration 104/1000 | Loss: 0.00002020
Iteration 105/1000 | Loss: 0.00002020
Iteration 106/1000 | Loss: 0.00002020
Iteration 107/1000 | Loss: 0.00002019
Iteration 108/1000 | Loss: 0.00002019
Iteration 109/1000 | Loss: 0.00002018
Iteration 110/1000 | Loss: 0.00002018
Iteration 111/1000 | Loss: 0.00002018
Iteration 112/1000 | Loss: 0.00002018
Iteration 113/1000 | Loss: 0.00002018
Iteration 114/1000 | Loss: 0.00002018
Iteration 115/1000 | Loss: 0.00002018
Iteration 116/1000 | Loss: 0.00002018
Iteration 117/1000 | Loss: 0.00002018
Iteration 118/1000 | Loss: 0.00002017
Iteration 119/1000 | Loss: 0.00002017
Iteration 120/1000 | Loss: 0.00002016
Iteration 121/1000 | Loss: 0.00002016
Iteration 122/1000 | Loss: 0.00002016
Iteration 123/1000 | Loss: 0.00002016
Iteration 124/1000 | Loss: 0.00002016
Iteration 125/1000 | Loss: 0.00002016
Iteration 126/1000 | Loss: 0.00002016
Iteration 127/1000 | Loss: 0.00002016
Iteration 128/1000 | Loss: 0.00002016
Iteration 129/1000 | Loss: 0.00002016
Iteration 130/1000 | Loss: 0.00002016
Iteration 131/1000 | Loss: 0.00002015
Iteration 132/1000 | Loss: 0.00002015
Iteration 133/1000 | Loss: 0.00002015
Iteration 134/1000 | Loss: 0.00002015
Iteration 135/1000 | Loss: 0.00002015
Iteration 136/1000 | Loss: 0.00002015
Iteration 137/1000 | Loss: 0.00002015
Iteration 138/1000 | Loss: 0.00002015
Iteration 139/1000 | Loss: 0.00002015
Iteration 140/1000 | Loss: 0.00002014
Iteration 141/1000 | Loss: 0.00002014
Iteration 142/1000 | Loss: 0.00002014
Iteration 143/1000 | Loss: 0.00002014
Iteration 144/1000 | Loss: 0.00002014
Iteration 145/1000 | Loss: 0.00002014
Iteration 146/1000 | Loss: 0.00002014
Iteration 147/1000 | Loss: 0.00002014
Iteration 148/1000 | Loss: 0.00002014
Iteration 149/1000 | Loss: 0.00002014
Iteration 150/1000 | Loss: 0.00002014
Iteration 151/1000 | Loss: 0.00002014
Iteration 152/1000 | Loss: 0.00002014
Iteration 153/1000 | Loss: 0.00002014
Iteration 154/1000 | Loss: 0.00002013
Iteration 155/1000 | Loss: 0.00002013
Iteration 156/1000 | Loss: 0.00002013
Iteration 157/1000 | Loss: 0.00002013
Iteration 158/1000 | Loss: 0.00002013
Iteration 159/1000 | Loss: 0.00002013
Iteration 160/1000 | Loss: 0.00002013
Iteration 161/1000 | Loss: 0.00002013
Iteration 162/1000 | Loss: 0.00002013
Iteration 163/1000 | Loss: 0.00002013
Iteration 164/1000 | Loss: 0.00002013
Iteration 165/1000 | Loss: 0.00002013
Iteration 166/1000 | Loss: 0.00002013
Iteration 167/1000 | Loss: 0.00002013
Iteration 168/1000 | Loss: 0.00002013
Iteration 169/1000 | Loss: 0.00002013
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.0127754396526143e-05, 2.0127754396526143e-05, 2.0127754396526143e-05, 2.0127754396526143e-05, 2.0127754396526143e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0127754396526143e-05

Optimization complete. Final v2v error: 3.858164072036743 mm

Highest mean error: 4.297202110290527 mm for frame 233

Lowest mean error: 3.4888131618499756 mm for frame 81

Saving results

Total time: 42.96572136878967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820149
Iteration 2/25 | Loss: 0.00109518
Iteration 3/25 | Loss: 0.00091965
Iteration 4/25 | Loss: 0.00089678
Iteration 5/25 | Loss: 0.00089138
Iteration 6/25 | Loss: 0.00089028
Iteration 7/25 | Loss: 0.00089028
Iteration 8/25 | Loss: 0.00089028
Iteration 9/25 | Loss: 0.00089028
Iteration 10/25 | Loss: 0.00089028
Iteration 11/25 | Loss: 0.00089028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008902819245122373, 0.0008902819245122373, 0.0008902819245122373, 0.0008902819245122373, 0.0008902819245122373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008902819245122373

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.69009256
Iteration 2/25 | Loss: 0.00046015
Iteration 3/25 | Loss: 0.00046012
Iteration 4/25 | Loss: 0.00046012
Iteration 5/25 | Loss: 0.00046012
Iteration 6/25 | Loss: 0.00046012
Iteration 7/25 | Loss: 0.00046012
Iteration 8/25 | Loss: 0.00046012
Iteration 9/25 | Loss: 0.00046012
Iteration 10/25 | Loss: 0.00046012
Iteration 11/25 | Loss: 0.00046012
Iteration 12/25 | Loss: 0.00046012
Iteration 13/25 | Loss: 0.00046012
Iteration 14/25 | Loss: 0.00046012
Iteration 15/25 | Loss: 0.00046012
Iteration 16/25 | Loss: 0.00046012
Iteration 17/25 | Loss: 0.00046012
Iteration 18/25 | Loss: 0.00046012
Iteration 19/25 | Loss: 0.00046012
Iteration 20/25 | Loss: 0.00046012
Iteration 21/25 | Loss: 0.00046012
Iteration 22/25 | Loss: 0.00046012
Iteration 23/25 | Loss: 0.00046012
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004601177352014929, 0.0004601177352014929, 0.0004601177352014929, 0.0004601177352014929, 0.0004601177352014929]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004601177352014929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046012
Iteration 2/1000 | Loss: 0.00003763
Iteration 3/1000 | Loss: 0.00002700
Iteration 4/1000 | Loss: 0.00002422
Iteration 5/1000 | Loss: 0.00002306
Iteration 6/1000 | Loss: 0.00002209
Iteration 7/1000 | Loss: 0.00002157
Iteration 8/1000 | Loss: 0.00002125
Iteration 9/1000 | Loss: 0.00002106
Iteration 10/1000 | Loss: 0.00002096
Iteration 11/1000 | Loss: 0.00002096
Iteration 12/1000 | Loss: 0.00002087
Iteration 13/1000 | Loss: 0.00002087
Iteration 14/1000 | Loss: 0.00002087
Iteration 15/1000 | Loss: 0.00002084
Iteration 16/1000 | Loss: 0.00002084
Iteration 17/1000 | Loss: 0.00002084
Iteration 18/1000 | Loss: 0.00002084
Iteration 19/1000 | Loss: 0.00002084
Iteration 20/1000 | Loss: 0.00002083
Iteration 21/1000 | Loss: 0.00002083
Iteration 22/1000 | Loss: 0.00002083
Iteration 23/1000 | Loss: 0.00002083
Iteration 24/1000 | Loss: 0.00002083
Iteration 25/1000 | Loss: 0.00002083
Iteration 26/1000 | Loss: 0.00002083
Iteration 27/1000 | Loss: 0.00002082
Iteration 28/1000 | Loss: 0.00002082
Iteration 29/1000 | Loss: 0.00002082
Iteration 30/1000 | Loss: 0.00002081
Iteration 31/1000 | Loss: 0.00002081
Iteration 32/1000 | Loss: 0.00002081
Iteration 33/1000 | Loss: 0.00002081
Iteration 34/1000 | Loss: 0.00002081
Iteration 35/1000 | Loss: 0.00002081
Iteration 36/1000 | Loss: 0.00002081
Iteration 37/1000 | Loss: 0.00002081
Iteration 38/1000 | Loss: 0.00002081
Iteration 39/1000 | Loss: 0.00002081
Iteration 40/1000 | Loss: 0.00002081
Iteration 41/1000 | Loss: 0.00002080
Iteration 42/1000 | Loss: 0.00002080
Iteration 43/1000 | Loss: 0.00002080
Iteration 44/1000 | Loss: 0.00002080
Iteration 45/1000 | Loss: 0.00002080
Iteration 46/1000 | Loss: 0.00002080
Iteration 47/1000 | Loss: 0.00002080
Iteration 48/1000 | Loss: 0.00002080
Iteration 49/1000 | Loss: 0.00002080
Iteration 50/1000 | Loss: 0.00002079
Iteration 51/1000 | Loss: 0.00002079
Iteration 52/1000 | Loss: 0.00002079
Iteration 53/1000 | Loss: 0.00002079
Iteration 54/1000 | Loss: 0.00002079
Iteration 55/1000 | Loss: 0.00002079
Iteration 56/1000 | Loss: 0.00002079
Iteration 57/1000 | Loss: 0.00002079
Iteration 58/1000 | Loss: 0.00002079
Iteration 59/1000 | Loss: 0.00002079
Iteration 60/1000 | Loss: 0.00002079
Iteration 61/1000 | Loss: 0.00002078
Iteration 62/1000 | Loss: 0.00002078
Iteration 63/1000 | Loss: 0.00002078
Iteration 64/1000 | Loss: 0.00002078
Iteration 65/1000 | Loss: 0.00002078
Iteration 66/1000 | Loss: 0.00002078
Iteration 67/1000 | Loss: 0.00002078
Iteration 68/1000 | Loss: 0.00002078
Iteration 69/1000 | Loss: 0.00002078
Iteration 70/1000 | Loss: 0.00002078
Iteration 71/1000 | Loss: 0.00002078
Iteration 72/1000 | Loss: 0.00002078
Iteration 73/1000 | Loss: 0.00002078
Iteration 74/1000 | Loss: 0.00002077
Iteration 75/1000 | Loss: 0.00002077
Iteration 76/1000 | Loss: 0.00002077
Iteration 77/1000 | Loss: 0.00002077
Iteration 78/1000 | Loss: 0.00002077
Iteration 79/1000 | Loss: 0.00002077
Iteration 80/1000 | Loss: 0.00002077
Iteration 81/1000 | Loss: 0.00002077
Iteration 82/1000 | Loss: 0.00002077
Iteration 83/1000 | Loss: 0.00002077
Iteration 84/1000 | Loss: 0.00002077
Iteration 85/1000 | Loss: 0.00002077
Iteration 86/1000 | Loss: 0.00002076
Iteration 87/1000 | Loss: 0.00002076
Iteration 88/1000 | Loss: 0.00002076
Iteration 89/1000 | Loss: 0.00002076
Iteration 90/1000 | Loss: 0.00002076
Iteration 91/1000 | Loss: 0.00002076
Iteration 92/1000 | Loss: 0.00002076
Iteration 93/1000 | Loss: 0.00002076
Iteration 94/1000 | Loss: 0.00002075
Iteration 95/1000 | Loss: 0.00002075
Iteration 96/1000 | Loss: 0.00002075
Iteration 97/1000 | Loss: 0.00002075
Iteration 98/1000 | Loss: 0.00002075
Iteration 99/1000 | Loss: 0.00002075
Iteration 100/1000 | Loss: 0.00002075
Iteration 101/1000 | Loss: 0.00002075
Iteration 102/1000 | Loss: 0.00002075
Iteration 103/1000 | Loss: 0.00002075
Iteration 104/1000 | Loss: 0.00002075
Iteration 105/1000 | Loss: 0.00002075
Iteration 106/1000 | Loss: 0.00002075
Iteration 107/1000 | Loss: 0.00002075
Iteration 108/1000 | Loss: 0.00002075
Iteration 109/1000 | Loss: 0.00002075
Iteration 110/1000 | Loss: 0.00002075
Iteration 111/1000 | Loss: 0.00002074
Iteration 112/1000 | Loss: 0.00002074
Iteration 113/1000 | Loss: 0.00002074
Iteration 114/1000 | Loss: 0.00002074
Iteration 115/1000 | Loss: 0.00002074
Iteration 116/1000 | Loss: 0.00002074
Iteration 117/1000 | Loss: 0.00002074
Iteration 118/1000 | Loss: 0.00002074
Iteration 119/1000 | Loss: 0.00002074
Iteration 120/1000 | Loss: 0.00002074
Iteration 121/1000 | Loss: 0.00002074
Iteration 122/1000 | Loss: 0.00002074
Iteration 123/1000 | Loss: 0.00002074
Iteration 124/1000 | Loss: 0.00002074
Iteration 125/1000 | Loss: 0.00002074
Iteration 126/1000 | Loss: 0.00002074
Iteration 127/1000 | Loss: 0.00002074
Iteration 128/1000 | Loss: 0.00002074
Iteration 129/1000 | Loss: 0.00002073
Iteration 130/1000 | Loss: 0.00002073
Iteration 131/1000 | Loss: 0.00002073
Iteration 132/1000 | Loss: 0.00002073
Iteration 133/1000 | Loss: 0.00002073
Iteration 134/1000 | Loss: 0.00002073
Iteration 135/1000 | Loss: 0.00002073
Iteration 136/1000 | Loss: 0.00002073
Iteration 137/1000 | Loss: 0.00002073
Iteration 138/1000 | Loss: 0.00002073
Iteration 139/1000 | Loss: 0.00002073
Iteration 140/1000 | Loss: 0.00002073
Iteration 141/1000 | Loss: 0.00002073
Iteration 142/1000 | Loss: 0.00002073
Iteration 143/1000 | Loss: 0.00002073
Iteration 144/1000 | Loss: 0.00002072
Iteration 145/1000 | Loss: 0.00002072
Iteration 146/1000 | Loss: 0.00002072
Iteration 147/1000 | Loss: 0.00002072
Iteration 148/1000 | Loss: 0.00002072
Iteration 149/1000 | Loss: 0.00002072
Iteration 150/1000 | Loss: 0.00002072
Iteration 151/1000 | Loss: 0.00002072
Iteration 152/1000 | Loss: 0.00002072
Iteration 153/1000 | Loss: 0.00002072
Iteration 154/1000 | Loss: 0.00002072
Iteration 155/1000 | Loss: 0.00002072
Iteration 156/1000 | Loss: 0.00002072
Iteration 157/1000 | Loss: 0.00002072
Iteration 158/1000 | Loss: 0.00002071
Iteration 159/1000 | Loss: 0.00002071
Iteration 160/1000 | Loss: 0.00002071
Iteration 161/1000 | Loss: 0.00002071
Iteration 162/1000 | Loss: 0.00002071
Iteration 163/1000 | Loss: 0.00002071
Iteration 164/1000 | Loss: 0.00002071
Iteration 165/1000 | Loss: 0.00002071
Iteration 166/1000 | Loss: 0.00002071
Iteration 167/1000 | Loss: 0.00002071
Iteration 168/1000 | Loss: 0.00002071
Iteration 169/1000 | Loss: 0.00002071
Iteration 170/1000 | Loss: 0.00002071
Iteration 171/1000 | Loss: 0.00002071
Iteration 172/1000 | Loss: 0.00002071
Iteration 173/1000 | Loss: 0.00002070
Iteration 174/1000 | Loss: 0.00002070
Iteration 175/1000 | Loss: 0.00002070
Iteration 176/1000 | Loss: 0.00002070
Iteration 177/1000 | Loss: 0.00002070
Iteration 178/1000 | Loss: 0.00002070
Iteration 179/1000 | Loss: 0.00002070
Iteration 180/1000 | Loss: 0.00002070
Iteration 181/1000 | Loss: 0.00002070
Iteration 182/1000 | Loss: 0.00002070
Iteration 183/1000 | Loss: 0.00002070
Iteration 184/1000 | Loss: 0.00002070
Iteration 185/1000 | Loss: 0.00002070
Iteration 186/1000 | Loss: 0.00002069
Iteration 187/1000 | Loss: 0.00002069
Iteration 188/1000 | Loss: 0.00002069
Iteration 189/1000 | Loss: 0.00002069
Iteration 190/1000 | Loss: 0.00002069
Iteration 191/1000 | Loss: 0.00002069
Iteration 192/1000 | Loss: 0.00002069
Iteration 193/1000 | Loss: 0.00002069
Iteration 194/1000 | Loss: 0.00002069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.0689883967861533e-05, 2.0689883967861533e-05, 2.0689883967861533e-05, 2.0689883967861533e-05, 2.0689883967861533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0689883967861533e-05

Optimization complete. Final v2v error: 3.9264466762542725 mm

Highest mean error: 4.289461135864258 mm for frame 144

Lowest mean error: 3.4448039531707764 mm for frame 208

Saving results

Total time: 38.818581104278564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871368
Iteration 2/25 | Loss: 0.00125728
Iteration 3/25 | Loss: 0.00089835
Iteration 4/25 | Loss: 0.00087404
Iteration 5/25 | Loss: 0.00085704
Iteration 6/25 | Loss: 0.00084125
Iteration 7/25 | Loss: 0.00084817
Iteration 8/25 | Loss: 0.00083612
Iteration 9/25 | Loss: 0.00083698
Iteration 10/25 | Loss: 0.00083476
Iteration 11/25 | Loss: 0.00083452
Iteration 12/25 | Loss: 0.00083444
Iteration 13/25 | Loss: 0.00083444
Iteration 14/25 | Loss: 0.00083444
Iteration 15/25 | Loss: 0.00083444
Iteration 16/25 | Loss: 0.00083444
Iteration 17/25 | Loss: 0.00083444
Iteration 18/25 | Loss: 0.00083444
Iteration 19/25 | Loss: 0.00083444
Iteration 20/25 | Loss: 0.00083444
Iteration 21/25 | Loss: 0.00083444
Iteration 22/25 | Loss: 0.00083444
Iteration 23/25 | Loss: 0.00083444
Iteration 24/25 | Loss: 0.00083444
Iteration 25/25 | Loss: 0.00083444

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.49917245
Iteration 2/25 | Loss: 0.00037285
Iteration 3/25 | Loss: 0.00037285
Iteration 4/25 | Loss: 0.00037285
Iteration 5/25 | Loss: 0.00037285
Iteration 6/25 | Loss: 0.00037285
Iteration 7/25 | Loss: 0.00037285
Iteration 8/25 | Loss: 0.00037285
Iteration 9/25 | Loss: 0.00037285
Iteration 10/25 | Loss: 0.00037285
Iteration 11/25 | Loss: 0.00037285
Iteration 12/25 | Loss: 0.00037285
Iteration 13/25 | Loss: 0.00037285
Iteration 14/25 | Loss: 0.00037285
Iteration 15/25 | Loss: 0.00037285
Iteration 16/25 | Loss: 0.00037285
Iteration 17/25 | Loss: 0.00037285
Iteration 18/25 | Loss: 0.00037285
Iteration 19/25 | Loss: 0.00037285
Iteration 20/25 | Loss: 0.00037285
Iteration 21/25 | Loss: 0.00037285
Iteration 22/25 | Loss: 0.00037285
Iteration 23/25 | Loss: 0.00037285
Iteration 24/25 | Loss: 0.00037285
Iteration 25/25 | Loss: 0.00037285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037285
Iteration 2/1000 | Loss: 0.00003604
Iteration 3/1000 | Loss: 0.00002470
Iteration 4/1000 | Loss: 0.00002206
Iteration 5/1000 | Loss: 0.00002069
Iteration 6/1000 | Loss: 0.00002001
Iteration 7/1000 | Loss: 0.00001961
Iteration 8/1000 | Loss: 0.00001950
Iteration 9/1000 | Loss: 0.00001950
Iteration 10/1000 | Loss: 0.00001930
Iteration 11/1000 | Loss: 0.00001927
Iteration 12/1000 | Loss: 0.00001916
Iteration 13/1000 | Loss: 0.00001914
Iteration 14/1000 | Loss: 0.00001913
Iteration 15/1000 | Loss: 0.00001913
Iteration 16/1000 | Loss: 0.00001908
Iteration 17/1000 | Loss: 0.00001907
Iteration 18/1000 | Loss: 0.00001905
Iteration 19/1000 | Loss: 0.00001904
Iteration 20/1000 | Loss: 0.00001904
Iteration 21/1000 | Loss: 0.00001904
Iteration 22/1000 | Loss: 0.00001904
Iteration 23/1000 | Loss: 0.00001904
Iteration 24/1000 | Loss: 0.00001904
Iteration 25/1000 | Loss: 0.00001904
Iteration 26/1000 | Loss: 0.00001904
Iteration 27/1000 | Loss: 0.00001903
Iteration 28/1000 | Loss: 0.00001903
Iteration 29/1000 | Loss: 0.00001902
Iteration 30/1000 | Loss: 0.00001902
Iteration 31/1000 | Loss: 0.00001901
Iteration 32/1000 | Loss: 0.00001901
Iteration 33/1000 | Loss: 0.00001900
Iteration 34/1000 | Loss: 0.00001900
Iteration 35/1000 | Loss: 0.00001900
Iteration 36/1000 | Loss: 0.00001900
Iteration 37/1000 | Loss: 0.00001900
Iteration 38/1000 | Loss: 0.00001900
Iteration 39/1000 | Loss: 0.00001899
Iteration 40/1000 | Loss: 0.00001899
Iteration 41/1000 | Loss: 0.00001899
Iteration 42/1000 | Loss: 0.00001899
Iteration 43/1000 | Loss: 0.00001899
Iteration 44/1000 | Loss: 0.00001899
Iteration 45/1000 | Loss: 0.00001899
Iteration 46/1000 | Loss: 0.00001898
Iteration 47/1000 | Loss: 0.00001898
Iteration 48/1000 | Loss: 0.00001898
Iteration 49/1000 | Loss: 0.00001898
Iteration 50/1000 | Loss: 0.00001898
Iteration 51/1000 | Loss: 0.00001898
Iteration 52/1000 | Loss: 0.00001898
Iteration 53/1000 | Loss: 0.00001898
Iteration 54/1000 | Loss: 0.00001898
Iteration 55/1000 | Loss: 0.00001898
Iteration 56/1000 | Loss: 0.00001897
Iteration 57/1000 | Loss: 0.00001897
Iteration 58/1000 | Loss: 0.00001897
Iteration 59/1000 | Loss: 0.00001897
Iteration 60/1000 | Loss: 0.00001897
Iteration 61/1000 | Loss: 0.00001897
Iteration 62/1000 | Loss: 0.00001897
Iteration 63/1000 | Loss: 0.00001897
Iteration 64/1000 | Loss: 0.00001896
Iteration 65/1000 | Loss: 0.00001896
Iteration 66/1000 | Loss: 0.00001896
Iteration 67/1000 | Loss: 0.00001896
Iteration 68/1000 | Loss: 0.00001896
Iteration 69/1000 | Loss: 0.00001896
Iteration 70/1000 | Loss: 0.00001896
Iteration 71/1000 | Loss: 0.00001895
Iteration 72/1000 | Loss: 0.00001895
Iteration 73/1000 | Loss: 0.00001895
Iteration 74/1000 | Loss: 0.00001895
Iteration 75/1000 | Loss: 0.00001895
Iteration 76/1000 | Loss: 0.00001895
Iteration 77/1000 | Loss: 0.00001895
Iteration 78/1000 | Loss: 0.00001895
Iteration 79/1000 | Loss: 0.00001895
Iteration 80/1000 | Loss: 0.00001895
Iteration 81/1000 | Loss: 0.00001895
Iteration 82/1000 | Loss: 0.00001895
Iteration 83/1000 | Loss: 0.00001895
Iteration 84/1000 | Loss: 0.00001895
Iteration 85/1000 | Loss: 0.00001895
Iteration 86/1000 | Loss: 0.00001895
Iteration 87/1000 | Loss: 0.00001895
Iteration 88/1000 | Loss: 0.00001895
Iteration 89/1000 | Loss: 0.00001895
Iteration 90/1000 | Loss: 0.00001895
Iteration 91/1000 | Loss: 0.00001895
Iteration 92/1000 | Loss: 0.00001895
Iteration 93/1000 | Loss: 0.00001895
Iteration 94/1000 | Loss: 0.00001895
Iteration 95/1000 | Loss: 0.00001895
Iteration 96/1000 | Loss: 0.00001895
Iteration 97/1000 | Loss: 0.00001895
Iteration 98/1000 | Loss: 0.00001895
Iteration 99/1000 | Loss: 0.00001895
Iteration 100/1000 | Loss: 0.00001895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.894649540190585e-05, 1.894649540190585e-05, 1.894649540190585e-05, 1.894649540190585e-05, 1.894649540190585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.894649540190585e-05

Optimization complete. Final v2v error: 3.675917387008667 mm

Highest mean error: 4.292333126068115 mm for frame 47

Lowest mean error: 3.2533769607543945 mm for frame 34

Saving results

Total time: 40.47046375274658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067262
Iteration 2/25 | Loss: 0.00395257
Iteration 3/25 | Loss: 0.00223734
Iteration 4/25 | Loss: 0.00193213
Iteration 5/25 | Loss: 0.00171905
Iteration 6/25 | Loss: 0.00160609
Iteration 7/25 | Loss: 0.00155471
Iteration 8/25 | Loss: 0.00147046
Iteration 9/25 | Loss: 0.00145358
Iteration 10/25 | Loss: 0.00145188
Iteration 11/25 | Loss: 0.00143245
Iteration 12/25 | Loss: 0.00142521
Iteration 13/25 | Loss: 0.00139839
Iteration 14/25 | Loss: 0.00139650
Iteration 15/25 | Loss: 0.00138896
Iteration 16/25 | Loss: 0.00138657
Iteration 17/25 | Loss: 0.00136941
Iteration 18/25 | Loss: 0.00136836
Iteration 19/25 | Loss: 0.00136746
Iteration 20/25 | Loss: 0.00137070
Iteration 21/25 | Loss: 0.00136050
Iteration 22/25 | Loss: 0.00135313
Iteration 23/25 | Loss: 0.00134647
Iteration 24/25 | Loss: 0.00134067
Iteration 25/25 | Loss: 0.00133496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52937758
Iteration 2/25 | Loss: 0.02225381
Iteration 3/25 | Loss: 0.03058128
Iteration 4/25 | Loss: 0.06622256
Iteration 5/25 | Loss: 0.00936506
Iteration 6/25 | Loss: 0.00460667
Iteration 7/25 | Loss: 0.00389894
Iteration 8/25 | Loss: 0.00389894
Iteration 9/25 | Loss: 0.00389894
Iteration 10/25 | Loss: 0.00389894
Iteration 11/25 | Loss: 0.00389894
Iteration 12/25 | Loss: 0.00389894
Iteration 13/25 | Loss: 0.00389894
Iteration 14/25 | Loss: 0.00389894
Iteration 15/25 | Loss: 0.00389894
Iteration 16/25 | Loss: 0.00389894
Iteration 17/25 | Loss: 0.00389894
Iteration 18/25 | Loss: 0.00389894
Iteration 19/25 | Loss: 0.00389894
Iteration 20/25 | Loss: 0.00389894
Iteration 21/25 | Loss: 0.00389894
Iteration 22/25 | Loss: 0.00389894
Iteration 23/25 | Loss: 0.00389894
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0038989370223134756, 0.0038989370223134756, 0.0038989370223134756, 0.0038989370223134756, 0.0038989370223134756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038989370223134756

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00389894
Iteration 2/1000 | Loss: 0.00568114
Iteration 3/1000 | Loss: 0.00935523
Iteration 4/1000 | Loss: 0.00221173
Iteration 5/1000 | Loss: 0.00456693
Iteration 6/1000 | Loss: 0.00333706
Iteration 7/1000 | Loss: 0.00136696
Iteration 8/1000 | Loss: 0.00135372
Iteration 9/1000 | Loss: 0.00286283
Iteration 10/1000 | Loss: 0.00184930
Iteration 11/1000 | Loss: 0.00218073
Iteration 12/1000 | Loss: 0.00167966
Iteration 13/1000 | Loss: 0.00236801
Iteration 14/1000 | Loss: 0.00224915
Iteration 15/1000 | Loss: 0.00160150
Iteration 16/1000 | Loss: 0.00174497
Iteration 17/1000 | Loss: 0.00136835
Iteration 18/1000 | Loss: 0.00102396
Iteration 19/1000 | Loss: 0.00107455
Iteration 20/1000 | Loss: 0.00097662
Iteration 21/1000 | Loss: 0.00034519
Iteration 22/1000 | Loss: 0.00043709
Iteration 23/1000 | Loss: 0.00052206
Iteration 24/1000 | Loss: 0.00036700
Iteration 25/1000 | Loss: 0.00050735
Iteration 26/1000 | Loss: 0.00073015
Iteration 27/1000 | Loss: 0.00093402
Iteration 28/1000 | Loss: 0.00149257
Iteration 29/1000 | Loss: 0.00299060
Iteration 30/1000 | Loss: 0.00233113
Iteration 31/1000 | Loss: 0.00038982
Iteration 32/1000 | Loss: 0.00031149
Iteration 33/1000 | Loss: 0.00075331
Iteration 34/1000 | Loss: 0.00087716
Iteration 35/1000 | Loss: 0.00047402
Iteration 36/1000 | Loss: 0.00039957
Iteration 37/1000 | Loss: 0.00027556
Iteration 38/1000 | Loss: 0.00025834
Iteration 39/1000 | Loss: 0.00083782
Iteration 40/1000 | Loss: 0.00319211
Iteration 41/1000 | Loss: 0.00128046
Iteration 42/1000 | Loss: 0.00040860
Iteration 43/1000 | Loss: 0.00051252
Iteration 44/1000 | Loss: 0.00049801
Iteration 45/1000 | Loss: 0.00114481
Iteration 46/1000 | Loss: 0.00207766
Iteration 47/1000 | Loss: 0.00146311
Iteration 48/1000 | Loss: 0.00067555
Iteration 49/1000 | Loss: 0.00025416
Iteration 50/1000 | Loss: 0.00112314
Iteration 51/1000 | Loss: 0.00111486
Iteration 52/1000 | Loss: 0.00098668
Iteration 53/1000 | Loss: 0.00175684
Iteration 54/1000 | Loss: 0.00148591
Iteration 55/1000 | Loss: 0.00090389
Iteration 56/1000 | Loss: 0.00351129
Iteration 57/1000 | Loss: 0.00159944
Iteration 58/1000 | Loss: 0.00028877
Iteration 59/1000 | Loss: 0.00053770
Iteration 60/1000 | Loss: 0.00103622
Iteration 61/1000 | Loss: 0.00023599
Iteration 62/1000 | Loss: 0.00035579
Iteration 63/1000 | Loss: 0.00113355
Iteration 64/1000 | Loss: 0.00086647
Iteration 65/1000 | Loss: 0.00056421
Iteration 66/1000 | Loss: 0.00143072
Iteration 67/1000 | Loss: 0.00019187
Iteration 68/1000 | Loss: 0.00068268
Iteration 69/1000 | Loss: 0.00038018
Iteration 70/1000 | Loss: 0.00016211
Iteration 71/1000 | Loss: 0.00275242
Iteration 72/1000 | Loss: 0.00327677
Iteration 73/1000 | Loss: 0.00426143
Iteration 74/1000 | Loss: 0.00074031
Iteration 75/1000 | Loss: 0.00059486
Iteration 76/1000 | Loss: 0.00214405
Iteration 77/1000 | Loss: 0.00305590
Iteration 78/1000 | Loss: 0.00102197
Iteration 79/1000 | Loss: 0.00093815
Iteration 80/1000 | Loss: 0.00101979
Iteration 81/1000 | Loss: 0.00129324
Iteration 82/1000 | Loss: 0.00038601
Iteration 83/1000 | Loss: 0.00020180
Iteration 84/1000 | Loss: 0.00318209
Iteration 85/1000 | Loss: 0.00286264
Iteration 86/1000 | Loss: 0.00340026
Iteration 87/1000 | Loss: 0.00093644
Iteration 88/1000 | Loss: 0.00403339
Iteration 89/1000 | Loss: 0.00118190
Iteration 90/1000 | Loss: 0.00069602
Iteration 91/1000 | Loss: 0.00016422
Iteration 92/1000 | Loss: 0.00014587
Iteration 93/1000 | Loss: 0.00025382
Iteration 94/1000 | Loss: 0.00024732
Iteration 95/1000 | Loss: 0.00069270
Iteration 96/1000 | Loss: 0.00179109
Iteration 97/1000 | Loss: 0.00101136
Iteration 98/1000 | Loss: 0.00028068
Iteration 99/1000 | Loss: 0.00116840
Iteration 100/1000 | Loss: 0.00089731
Iteration 101/1000 | Loss: 0.00021006
Iteration 102/1000 | Loss: 0.00112487
Iteration 103/1000 | Loss: 0.00016625
Iteration 104/1000 | Loss: 0.00026051
Iteration 105/1000 | Loss: 0.00016768
Iteration 106/1000 | Loss: 0.00061739
Iteration 107/1000 | Loss: 0.00009376
Iteration 108/1000 | Loss: 0.00131921
Iteration 109/1000 | Loss: 0.00202766
Iteration 110/1000 | Loss: 0.00039660
Iteration 111/1000 | Loss: 0.00009414
Iteration 112/1000 | Loss: 0.00128010
Iteration 113/1000 | Loss: 0.00115856
Iteration 114/1000 | Loss: 0.00021321
Iteration 115/1000 | Loss: 0.00035975
Iteration 116/1000 | Loss: 0.00121578
Iteration 117/1000 | Loss: 0.00079349
Iteration 118/1000 | Loss: 0.00024979
Iteration 119/1000 | Loss: 0.00012503
Iteration 120/1000 | Loss: 0.00009306
Iteration 121/1000 | Loss: 0.00008449
Iteration 122/1000 | Loss: 0.00021378
Iteration 123/1000 | Loss: 0.00007464
Iteration 124/1000 | Loss: 0.00018056
Iteration 125/1000 | Loss: 0.00008572
Iteration 126/1000 | Loss: 0.00028571
Iteration 127/1000 | Loss: 0.00020147
Iteration 128/1000 | Loss: 0.00024413
Iteration 129/1000 | Loss: 0.00024607
Iteration 130/1000 | Loss: 0.00012934
Iteration 131/1000 | Loss: 0.00011374
Iteration 132/1000 | Loss: 0.00007304
Iteration 133/1000 | Loss: 0.00008119
Iteration 134/1000 | Loss: 0.00016895
Iteration 135/1000 | Loss: 0.00008495
Iteration 136/1000 | Loss: 0.00082162
Iteration 137/1000 | Loss: 0.00009578
Iteration 138/1000 | Loss: 0.00022239
Iteration 139/1000 | Loss: 0.00007167
Iteration 140/1000 | Loss: 0.00006536
Iteration 141/1000 | Loss: 0.00007397
Iteration 142/1000 | Loss: 0.00007341
Iteration 143/1000 | Loss: 0.00008623
Iteration 144/1000 | Loss: 0.00005910
Iteration 145/1000 | Loss: 0.00007235
Iteration 146/1000 | Loss: 0.00007129
Iteration 147/1000 | Loss: 0.00007356
Iteration 148/1000 | Loss: 0.00008717
Iteration 149/1000 | Loss: 0.00006432
Iteration 150/1000 | Loss: 0.00006876
Iteration 151/1000 | Loss: 0.00006821
Iteration 152/1000 | Loss: 0.00006787
Iteration 153/1000 | Loss: 0.00007165
Iteration 154/1000 | Loss: 0.00006043
Iteration 155/1000 | Loss: 0.00102111
Iteration 156/1000 | Loss: 0.00035544
Iteration 157/1000 | Loss: 0.00013439
Iteration 158/1000 | Loss: 0.00013556
Iteration 159/1000 | Loss: 0.00011786
Iteration 160/1000 | Loss: 0.00007982
Iteration 161/1000 | Loss: 0.00005613
Iteration 162/1000 | Loss: 0.00005579
Iteration 163/1000 | Loss: 0.00005549
Iteration 164/1000 | Loss: 0.00005534
Iteration 165/1000 | Loss: 0.00007116
Iteration 166/1000 | Loss: 0.00019710
Iteration 167/1000 | Loss: 0.00009261
Iteration 168/1000 | Loss: 0.00005868
Iteration 169/1000 | Loss: 0.00007478
Iteration 170/1000 | Loss: 0.00010153
Iteration 171/1000 | Loss: 0.00010062
Iteration 172/1000 | Loss: 0.00005516
Iteration 173/1000 | Loss: 0.00005457
Iteration 174/1000 | Loss: 0.00005390
Iteration 175/1000 | Loss: 0.00006967
Iteration 176/1000 | Loss: 0.00098430
Iteration 177/1000 | Loss: 0.00032286
Iteration 178/1000 | Loss: 0.00006507
Iteration 179/1000 | Loss: 0.00005635
Iteration 180/1000 | Loss: 0.00005441
Iteration 181/1000 | Loss: 0.00005324
Iteration 182/1000 | Loss: 0.00005274
Iteration 183/1000 | Loss: 0.00005248
Iteration 184/1000 | Loss: 0.00007067
Iteration 185/1000 | Loss: 0.00005230
Iteration 186/1000 | Loss: 0.00005221
Iteration 187/1000 | Loss: 0.00005220
Iteration 188/1000 | Loss: 0.00005218
Iteration 189/1000 | Loss: 0.00005217
Iteration 190/1000 | Loss: 0.00006607
Iteration 191/1000 | Loss: 0.00005208
Iteration 192/1000 | Loss: 0.00005204
Iteration 193/1000 | Loss: 0.00005183
Iteration 194/1000 | Loss: 0.00017863
Iteration 195/1000 | Loss: 0.00009616
Iteration 196/1000 | Loss: 0.00005192
Iteration 197/1000 | Loss: 0.00106754
Iteration 198/1000 | Loss: 0.00074720
Iteration 199/1000 | Loss: 0.00048792
Iteration 200/1000 | Loss: 0.00068764
Iteration 201/1000 | Loss: 0.00161510
Iteration 202/1000 | Loss: 0.00014239
Iteration 203/1000 | Loss: 0.00052182
Iteration 204/1000 | Loss: 0.00040795
Iteration 205/1000 | Loss: 0.00015605
Iteration 206/1000 | Loss: 0.00005704
Iteration 207/1000 | Loss: 0.00059278
Iteration 208/1000 | Loss: 0.00021841
Iteration 209/1000 | Loss: 0.00011310
Iteration 210/1000 | Loss: 0.00005702
Iteration 211/1000 | Loss: 0.00006231
Iteration 212/1000 | Loss: 0.00005258
Iteration 213/1000 | Loss: 0.00113054
Iteration 214/1000 | Loss: 0.00007595
Iteration 215/1000 | Loss: 0.00005421
Iteration 216/1000 | Loss: 0.00005156
Iteration 217/1000 | Loss: 0.00006704
Iteration 218/1000 | Loss: 0.00005326
Iteration 219/1000 | Loss: 0.00113781
Iteration 220/1000 | Loss: 0.00007803
Iteration 221/1000 | Loss: 0.00005557
Iteration 222/1000 | Loss: 0.00012559
Iteration 223/1000 | Loss: 0.00005778
Iteration 224/1000 | Loss: 0.00005056
Iteration 225/1000 | Loss: 0.00007721
Iteration 226/1000 | Loss: 0.00006007
Iteration 227/1000 | Loss: 0.00027714
Iteration 228/1000 | Loss: 0.00005959
Iteration 229/1000 | Loss: 0.00004820
Iteration 230/1000 | Loss: 0.00004792
Iteration 231/1000 | Loss: 0.00005779
Iteration 232/1000 | Loss: 0.00004761
Iteration 233/1000 | Loss: 0.00004759
Iteration 234/1000 | Loss: 0.00004759
Iteration 235/1000 | Loss: 0.00004758
Iteration 236/1000 | Loss: 0.00004757
Iteration 237/1000 | Loss: 0.00004757
Iteration 238/1000 | Loss: 0.00004757
Iteration 239/1000 | Loss: 0.00004757
Iteration 240/1000 | Loss: 0.00004755
Iteration 241/1000 | Loss: 0.00004755
Iteration 242/1000 | Loss: 0.00004754
Iteration 243/1000 | Loss: 0.00004754
Iteration 244/1000 | Loss: 0.00004754
Iteration 245/1000 | Loss: 0.00004753
Iteration 246/1000 | Loss: 0.00004753
Iteration 247/1000 | Loss: 0.00004753
Iteration 248/1000 | Loss: 0.00005852
Iteration 249/1000 | Loss: 0.00004751
Iteration 250/1000 | Loss: 0.00004744
Iteration 251/1000 | Loss: 0.00004744
Iteration 252/1000 | Loss: 0.00004743
Iteration 253/1000 | Loss: 0.00004743
Iteration 254/1000 | Loss: 0.00004742
Iteration 255/1000 | Loss: 0.00004742
Iteration 256/1000 | Loss: 0.00004742
Iteration 257/1000 | Loss: 0.00004741
Iteration 258/1000 | Loss: 0.00004741
Iteration 259/1000 | Loss: 0.00004741
Iteration 260/1000 | Loss: 0.00004741
Iteration 261/1000 | Loss: 0.00004740
Iteration 262/1000 | Loss: 0.00005965
Iteration 263/1000 | Loss: 0.00004852
Iteration 264/1000 | Loss: 0.00006620
Iteration 265/1000 | Loss: 0.00004741
Iteration 266/1000 | Loss: 0.00005127
Iteration 267/1000 | Loss: 0.00004734
Iteration 268/1000 | Loss: 0.00004734
Iteration 269/1000 | Loss: 0.00004732
Iteration 270/1000 | Loss: 0.00004732
Iteration 271/1000 | Loss: 0.00004732
Iteration 272/1000 | Loss: 0.00004732
Iteration 273/1000 | Loss: 0.00004731
Iteration 274/1000 | Loss: 0.00004731
Iteration 275/1000 | Loss: 0.00004731
Iteration 276/1000 | Loss: 0.00004731
Iteration 277/1000 | Loss: 0.00004731
Iteration 278/1000 | Loss: 0.00004928
Iteration 279/1000 | Loss: 0.00004729
Iteration 280/1000 | Loss: 0.00004729
Iteration 281/1000 | Loss: 0.00004729
Iteration 282/1000 | Loss: 0.00004729
Iteration 283/1000 | Loss: 0.00004729
Iteration 284/1000 | Loss: 0.00004729
Iteration 285/1000 | Loss: 0.00004729
Iteration 286/1000 | Loss: 0.00004729
Iteration 287/1000 | Loss: 0.00004728
Iteration 288/1000 | Loss: 0.00004728
Iteration 289/1000 | Loss: 0.00004728
Iteration 290/1000 | Loss: 0.00004728
Iteration 291/1000 | Loss: 0.00004728
Iteration 292/1000 | Loss: 0.00004728
Iteration 293/1000 | Loss: 0.00004727
Iteration 294/1000 | Loss: 0.00004727
Iteration 295/1000 | Loss: 0.00004727
Iteration 296/1000 | Loss: 0.00004727
Iteration 297/1000 | Loss: 0.00004727
Iteration 298/1000 | Loss: 0.00004727
Iteration 299/1000 | Loss: 0.00004727
Iteration 300/1000 | Loss: 0.00004727
Iteration 301/1000 | Loss: 0.00004727
Iteration 302/1000 | Loss: 0.00004727
Iteration 303/1000 | Loss: 0.00004727
Iteration 304/1000 | Loss: 0.00004726
Iteration 305/1000 | Loss: 0.00004726
Iteration 306/1000 | Loss: 0.00004726
Iteration 307/1000 | Loss: 0.00004726
Iteration 308/1000 | Loss: 0.00004726
Iteration 309/1000 | Loss: 0.00004726
Iteration 310/1000 | Loss: 0.00004726
Iteration 311/1000 | Loss: 0.00004726
Iteration 312/1000 | Loss: 0.00004726
Iteration 313/1000 | Loss: 0.00004726
Iteration 314/1000 | Loss: 0.00004726
Iteration 315/1000 | Loss: 0.00004726
Iteration 316/1000 | Loss: 0.00004726
Iteration 317/1000 | Loss: 0.00004726
Iteration 318/1000 | Loss: 0.00004726
Iteration 319/1000 | Loss: 0.00004726
Iteration 320/1000 | Loss: 0.00004726
Iteration 321/1000 | Loss: 0.00004726
Iteration 322/1000 | Loss: 0.00004726
Iteration 323/1000 | Loss: 0.00004726
Iteration 324/1000 | Loss: 0.00004726
Iteration 325/1000 | Loss: 0.00004726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 325. Stopping optimization.
Last 5 losses: [4.725684630102478e-05, 4.725684630102478e-05, 4.725684630102478e-05, 4.725684630102478e-05, 4.725684630102478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.725684630102478e-05

Optimization complete. Final v2v error: 5.150018215179443 mm

Highest mean error: 15.770740509033203 mm for frame 21

Lowest mean error: 3.0955896377563477 mm for frame 14

Saving results

Total time: 444.6561915874481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823801
Iteration 2/25 | Loss: 0.00126455
Iteration 3/25 | Loss: 0.00102267
Iteration 4/25 | Loss: 0.00096881
Iteration 5/25 | Loss: 0.00093757
Iteration 6/25 | Loss: 0.00094353
Iteration 7/25 | Loss: 0.00092581
Iteration 8/25 | Loss: 0.00091145
Iteration 9/25 | Loss: 0.00089934
Iteration 10/25 | Loss: 0.00089293
Iteration 11/25 | Loss: 0.00088632
Iteration 12/25 | Loss: 0.00088340
Iteration 13/25 | Loss: 0.00088183
Iteration 14/25 | Loss: 0.00088117
Iteration 15/25 | Loss: 0.00088100
Iteration 16/25 | Loss: 0.00088098
Iteration 17/25 | Loss: 0.00088098
Iteration 18/25 | Loss: 0.00088097
Iteration 19/25 | Loss: 0.00088097
Iteration 20/25 | Loss: 0.00088096
Iteration 21/25 | Loss: 0.00088096
Iteration 22/25 | Loss: 0.00088096
Iteration 23/25 | Loss: 0.00088095
Iteration 24/25 | Loss: 0.00088095
Iteration 25/25 | Loss: 0.00088095

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86071324
Iteration 2/25 | Loss: 0.00051920
Iteration 3/25 | Loss: 0.00051919
Iteration 4/25 | Loss: 0.00051919
Iteration 5/25 | Loss: 0.00051919
Iteration 6/25 | Loss: 0.00051919
Iteration 7/25 | Loss: 0.00051919
Iteration 8/25 | Loss: 0.00051919
Iteration 9/25 | Loss: 0.00051919
Iteration 10/25 | Loss: 0.00051919
Iteration 11/25 | Loss: 0.00051919
Iteration 12/25 | Loss: 0.00051919
Iteration 13/25 | Loss: 0.00051919
Iteration 14/25 | Loss: 0.00051919
Iteration 15/25 | Loss: 0.00051919
Iteration 16/25 | Loss: 0.00051919
Iteration 17/25 | Loss: 0.00051919
Iteration 18/25 | Loss: 0.00051919
Iteration 19/25 | Loss: 0.00051919
Iteration 20/25 | Loss: 0.00051919
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005191891686990857, 0.0005191891686990857, 0.0005191891686990857, 0.0005191891686990857, 0.0005191891686990857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005191891686990857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051919
Iteration 2/1000 | Loss: 0.00004260
Iteration 3/1000 | Loss: 0.00003193
Iteration 4/1000 | Loss: 0.00002851
Iteration 5/1000 | Loss: 0.00002674
Iteration 6/1000 | Loss: 0.00002577
Iteration 7/1000 | Loss: 0.00073198
Iteration 8/1000 | Loss: 0.00002937
Iteration 9/1000 | Loss: 0.00002496
Iteration 10/1000 | Loss: 0.00002355
Iteration 11/1000 | Loss: 0.00002266
Iteration 12/1000 | Loss: 0.00002225
Iteration 13/1000 | Loss: 0.00002212
Iteration 14/1000 | Loss: 0.00002211
Iteration 15/1000 | Loss: 0.00002201
Iteration 16/1000 | Loss: 0.00002201
Iteration 17/1000 | Loss: 0.00002196
Iteration 18/1000 | Loss: 0.00002196
Iteration 19/1000 | Loss: 0.00002196
Iteration 20/1000 | Loss: 0.00002196
Iteration 21/1000 | Loss: 0.00002196
Iteration 22/1000 | Loss: 0.00002196
Iteration 23/1000 | Loss: 0.00002195
Iteration 24/1000 | Loss: 0.00002195
Iteration 25/1000 | Loss: 0.00002195
Iteration 26/1000 | Loss: 0.00002195
Iteration 27/1000 | Loss: 0.00002195
Iteration 28/1000 | Loss: 0.00002195
Iteration 29/1000 | Loss: 0.00002195
Iteration 30/1000 | Loss: 0.00002195
Iteration 31/1000 | Loss: 0.00002193
Iteration 32/1000 | Loss: 0.00002193
Iteration 33/1000 | Loss: 0.00002185
Iteration 34/1000 | Loss: 0.00002185
Iteration 35/1000 | Loss: 0.00002185
Iteration 36/1000 | Loss: 0.00002181
Iteration 37/1000 | Loss: 0.00002181
Iteration 38/1000 | Loss: 0.00002181
Iteration 39/1000 | Loss: 0.00002180
Iteration 40/1000 | Loss: 0.00002180
Iteration 41/1000 | Loss: 0.00002180
Iteration 42/1000 | Loss: 0.00002180
Iteration 43/1000 | Loss: 0.00002180
Iteration 44/1000 | Loss: 0.00002179
Iteration 45/1000 | Loss: 0.00002179
Iteration 46/1000 | Loss: 0.00002179
Iteration 47/1000 | Loss: 0.00002178
Iteration 48/1000 | Loss: 0.00002178
Iteration 49/1000 | Loss: 0.00002178
Iteration 50/1000 | Loss: 0.00002177
Iteration 51/1000 | Loss: 0.00002177
Iteration 52/1000 | Loss: 0.00002177
Iteration 53/1000 | Loss: 0.00002175
Iteration 54/1000 | Loss: 0.00002174
Iteration 55/1000 | Loss: 0.00002174
Iteration 56/1000 | Loss: 0.00002174
Iteration 57/1000 | Loss: 0.00002173
Iteration 58/1000 | Loss: 0.00002173
Iteration 59/1000 | Loss: 0.00002173
Iteration 60/1000 | Loss: 0.00002172
Iteration 61/1000 | Loss: 0.00002172
Iteration 62/1000 | Loss: 0.00002171
Iteration 63/1000 | Loss: 0.00002169
Iteration 64/1000 | Loss: 0.00002169
Iteration 65/1000 | Loss: 0.00002169
Iteration 66/1000 | Loss: 0.00002169
Iteration 67/1000 | Loss: 0.00002167
Iteration 68/1000 | Loss: 0.00002167
Iteration 69/1000 | Loss: 0.00002166
Iteration 70/1000 | Loss: 0.00002165
Iteration 71/1000 | Loss: 0.00002165
Iteration 72/1000 | Loss: 0.00002165
Iteration 73/1000 | Loss: 0.00002165
Iteration 74/1000 | Loss: 0.00002165
Iteration 75/1000 | Loss: 0.00002165
Iteration 76/1000 | Loss: 0.00002165
Iteration 77/1000 | Loss: 0.00002165
Iteration 78/1000 | Loss: 0.00002165
Iteration 79/1000 | Loss: 0.00002165
Iteration 80/1000 | Loss: 0.00002165
Iteration 81/1000 | Loss: 0.00002164
Iteration 82/1000 | Loss: 0.00002164
Iteration 83/1000 | Loss: 0.00002164
Iteration 84/1000 | Loss: 0.00002164
Iteration 85/1000 | Loss: 0.00002164
Iteration 86/1000 | Loss: 0.00002164
Iteration 87/1000 | Loss: 0.00002163
Iteration 88/1000 | Loss: 0.00002163
Iteration 89/1000 | Loss: 0.00002163
Iteration 90/1000 | Loss: 0.00002163
Iteration 91/1000 | Loss: 0.00002163
Iteration 92/1000 | Loss: 0.00002163
Iteration 93/1000 | Loss: 0.00002163
Iteration 94/1000 | Loss: 0.00002163
Iteration 95/1000 | Loss: 0.00002163
Iteration 96/1000 | Loss: 0.00002163
Iteration 97/1000 | Loss: 0.00002163
Iteration 98/1000 | Loss: 0.00002163
Iteration 99/1000 | Loss: 0.00002163
Iteration 100/1000 | Loss: 0.00002163
Iteration 101/1000 | Loss: 0.00002163
Iteration 102/1000 | Loss: 0.00002162
Iteration 103/1000 | Loss: 0.00002162
Iteration 104/1000 | Loss: 0.00002162
Iteration 105/1000 | Loss: 0.00002162
Iteration 106/1000 | Loss: 0.00002162
Iteration 107/1000 | Loss: 0.00002162
Iteration 108/1000 | Loss: 0.00002162
Iteration 109/1000 | Loss: 0.00002162
Iteration 110/1000 | Loss: 0.00002161
Iteration 111/1000 | Loss: 0.00002161
Iteration 112/1000 | Loss: 0.00002161
Iteration 113/1000 | Loss: 0.00002161
Iteration 114/1000 | Loss: 0.00002161
Iteration 115/1000 | Loss: 0.00002161
Iteration 116/1000 | Loss: 0.00002161
Iteration 117/1000 | Loss: 0.00002161
Iteration 118/1000 | Loss: 0.00002161
Iteration 119/1000 | Loss: 0.00002161
Iteration 120/1000 | Loss: 0.00002161
Iteration 121/1000 | Loss: 0.00002161
Iteration 122/1000 | Loss: 0.00002161
Iteration 123/1000 | Loss: 0.00002161
Iteration 124/1000 | Loss: 0.00002161
Iteration 125/1000 | Loss: 0.00002161
Iteration 126/1000 | Loss: 0.00002161
Iteration 127/1000 | Loss: 0.00002161
Iteration 128/1000 | Loss: 0.00002161
Iteration 129/1000 | Loss: 0.00002161
Iteration 130/1000 | Loss: 0.00002160
Iteration 131/1000 | Loss: 0.00002160
Iteration 132/1000 | Loss: 0.00002160
Iteration 133/1000 | Loss: 0.00002160
Iteration 134/1000 | Loss: 0.00002160
Iteration 135/1000 | Loss: 0.00002160
Iteration 136/1000 | Loss: 0.00002160
Iteration 137/1000 | Loss: 0.00002160
Iteration 138/1000 | Loss: 0.00002160
Iteration 139/1000 | Loss: 0.00002160
Iteration 140/1000 | Loss: 0.00002160
Iteration 141/1000 | Loss: 0.00002160
Iteration 142/1000 | Loss: 0.00002160
Iteration 143/1000 | Loss: 0.00002160
Iteration 144/1000 | Loss: 0.00002160
Iteration 145/1000 | Loss: 0.00002160
Iteration 146/1000 | Loss: 0.00002160
Iteration 147/1000 | Loss: 0.00002160
Iteration 148/1000 | Loss: 0.00002160
Iteration 149/1000 | Loss: 0.00002160
Iteration 150/1000 | Loss: 0.00002160
Iteration 151/1000 | Loss: 0.00002160
Iteration 152/1000 | Loss: 0.00002160
Iteration 153/1000 | Loss: 0.00002160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.160167059628293e-05, 2.160167059628293e-05, 2.160167059628293e-05, 2.160167059628293e-05, 2.160167059628293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.160167059628293e-05

Optimization complete. Final v2v error: 3.905033588409424 mm

Highest mean error: 4.7702107429504395 mm for frame 40

Lowest mean error: 3.206005573272705 mm for frame 0

Saving results

Total time: 55.816333532333374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058385
Iteration 2/25 | Loss: 0.01058384
Iteration 3/25 | Loss: 0.01058384
Iteration 4/25 | Loss: 0.01058384
Iteration 5/25 | Loss: 0.01058383
Iteration 6/25 | Loss: 0.01058383
Iteration 7/25 | Loss: 0.00398613
Iteration 8/25 | Loss: 0.00234427
Iteration 9/25 | Loss: 0.00209966
Iteration 10/25 | Loss: 0.00200727
Iteration 11/25 | Loss: 0.00194469
Iteration 12/25 | Loss: 0.00188382
Iteration 13/25 | Loss: 0.00182019
Iteration 14/25 | Loss: 0.00177490
Iteration 15/25 | Loss: 0.00170109
Iteration 16/25 | Loss: 0.00169104
Iteration 17/25 | Loss: 0.00165072
Iteration 18/25 | Loss: 0.00163939
Iteration 19/25 | Loss: 0.00163190
Iteration 20/25 | Loss: 0.00163031
Iteration 21/25 | Loss: 0.00162507
Iteration 22/25 | Loss: 0.00162388
Iteration 23/25 | Loss: 0.00162602
Iteration 24/25 | Loss: 0.00162184
Iteration 25/25 | Loss: 0.00162116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38196826
Iteration 2/25 | Loss: 0.00567991
Iteration 3/25 | Loss: 0.00563680
Iteration 4/25 | Loss: 0.00563680
Iteration 5/25 | Loss: 0.00563680
Iteration 6/25 | Loss: 0.00563680
Iteration 7/25 | Loss: 0.00563680
Iteration 8/25 | Loss: 0.00563680
Iteration 9/25 | Loss: 0.00563680
Iteration 10/25 | Loss: 0.00563680
Iteration 11/25 | Loss: 0.00563680
Iteration 12/25 | Loss: 0.00563680
Iteration 13/25 | Loss: 0.00563680
Iteration 14/25 | Loss: 0.00563680
Iteration 15/25 | Loss: 0.00563680
Iteration 16/25 | Loss: 0.00563680
Iteration 17/25 | Loss: 0.00563680
Iteration 18/25 | Loss: 0.00563680
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.005636798217892647, 0.005636798217892647, 0.005636798217892647, 0.005636798217892647, 0.005636798217892647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005636798217892647

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00563680
Iteration 2/1000 | Loss: 0.00166565
Iteration 3/1000 | Loss: 0.00156589
Iteration 4/1000 | Loss: 0.00174938
Iteration 5/1000 | Loss: 0.00364324
Iteration 6/1000 | Loss: 0.00114982
Iteration 7/1000 | Loss: 0.00124992
Iteration 8/1000 | Loss: 0.00086522
Iteration 9/1000 | Loss: 0.00060133
Iteration 10/1000 | Loss: 0.00109641
Iteration 11/1000 | Loss: 0.00092770
Iteration 12/1000 | Loss: 0.00108245
Iteration 13/1000 | Loss: 0.00128924
Iteration 14/1000 | Loss: 0.00098270
Iteration 15/1000 | Loss: 0.00308223
Iteration 16/1000 | Loss: 0.00698858
Iteration 17/1000 | Loss: 0.01305031
Iteration 18/1000 | Loss: 0.00194167
Iteration 19/1000 | Loss: 0.00105194
Iteration 20/1000 | Loss: 0.00058581
Iteration 21/1000 | Loss: 0.00081006
Iteration 22/1000 | Loss: 0.00029484
Iteration 23/1000 | Loss: 0.00026366
Iteration 24/1000 | Loss: 0.00040465
Iteration 25/1000 | Loss: 0.00022384
Iteration 26/1000 | Loss: 0.00015427
Iteration 27/1000 | Loss: 0.00011559
Iteration 28/1000 | Loss: 0.00017767
Iteration 29/1000 | Loss: 0.00017144
Iteration 30/1000 | Loss: 0.00017950
Iteration 31/1000 | Loss: 0.00008166
Iteration 32/1000 | Loss: 0.00024437
Iteration 33/1000 | Loss: 0.00035917
Iteration 34/1000 | Loss: 0.00007574
Iteration 35/1000 | Loss: 0.00013594
Iteration 36/1000 | Loss: 0.00010134
Iteration 37/1000 | Loss: 0.00012955
Iteration 38/1000 | Loss: 0.00016025
Iteration 39/1000 | Loss: 0.00009930
Iteration 40/1000 | Loss: 0.00009885
Iteration 41/1000 | Loss: 0.00025491
Iteration 42/1000 | Loss: 0.00010508
Iteration 43/1000 | Loss: 0.00026026
Iteration 44/1000 | Loss: 0.00004615
Iteration 45/1000 | Loss: 0.00029520
Iteration 46/1000 | Loss: 0.00003050
Iteration 47/1000 | Loss: 0.00002771
Iteration 48/1000 | Loss: 0.00002645
Iteration 49/1000 | Loss: 0.00002559
Iteration 50/1000 | Loss: 0.00021859
Iteration 51/1000 | Loss: 0.00003418
Iteration 52/1000 | Loss: 0.00002691
Iteration 53/1000 | Loss: 0.00002440
Iteration 54/1000 | Loss: 0.00002313
Iteration 55/1000 | Loss: 0.00023174
Iteration 56/1000 | Loss: 0.00005330
Iteration 57/1000 | Loss: 0.00003842
Iteration 58/1000 | Loss: 0.00002309
Iteration 59/1000 | Loss: 0.00002190
Iteration 60/1000 | Loss: 0.00021086
Iteration 61/1000 | Loss: 0.00002244
Iteration 62/1000 | Loss: 0.00002105
Iteration 63/1000 | Loss: 0.00002055
Iteration 64/1000 | Loss: 0.00002051
Iteration 65/1000 | Loss: 0.00002039
Iteration 66/1000 | Loss: 0.00002039
Iteration 67/1000 | Loss: 0.00002034
Iteration 68/1000 | Loss: 0.00002033
Iteration 69/1000 | Loss: 0.00002032
Iteration 70/1000 | Loss: 0.00002031
Iteration 71/1000 | Loss: 0.00002027
Iteration 72/1000 | Loss: 0.00002026
Iteration 73/1000 | Loss: 0.00002024
Iteration 74/1000 | Loss: 0.00002023
Iteration 75/1000 | Loss: 0.00002023
Iteration 76/1000 | Loss: 0.00002022
Iteration 77/1000 | Loss: 0.00002022
Iteration 78/1000 | Loss: 0.00002021
Iteration 79/1000 | Loss: 0.00002021
Iteration 80/1000 | Loss: 0.00002020
Iteration 81/1000 | Loss: 0.00002019
Iteration 82/1000 | Loss: 0.00002019
Iteration 83/1000 | Loss: 0.00002019
Iteration 84/1000 | Loss: 0.00002018
Iteration 85/1000 | Loss: 0.00002018
Iteration 86/1000 | Loss: 0.00002017
Iteration 87/1000 | Loss: 0.00002017
Iteration 88/1000 | Loss: 0.00002017
Iteration 89/1000 | Loss: 0.00002016
Iteration 90/1000 | Loss: 0.00002016
Iteration 91/1000 | Loss: 0.00002016
Iteration 92/1000 | Loss: 0.00002016
Iteration 93/1000 | Loss: 0.00002015
Iteration 94/1000 | Loss: 0.00002015
Iteration 95/1000 | Loss: 0.00002014
Iteration 96/1000 | Loss: 0.00002013
Iteration 97/1000 | Loss: 0.00002013
Iteration 98/1000 | Loss: 0.00002013
Iteration 99/1000 | Loss: 0.00002013
Iteration 100/1000 | Loss: 0.00002013
Iteration 101/1000 | Loss: 0.00002013
Iteration 102/1000 | Loss: 0.00002013
Iteration 103/1000 | Loss: 0.00002012
Iteration 104/1000 | Loss: 0.00002012
Iteration 105/1000 | Loss: 0.00002012
Iteration 106/1000 | Loss: 0.00002012
Iteration 107/1000 | Loss: 0.00002012
Iteration 108/1000 | Loss: 0.00002011
Iteration 109/1000 | Loss: 0.00002011
Iteration 110/1000 | Loss: 0.00002011
Iteration 111/1000 | Loss: 0.00002011
Iteration 112/1000 | Loss: 0.00002011
Iteration 113/1000 | Loss: 0.00002011
Iteration 114/1000 | Loss: 0.00002011
Iteration 115/1000 | Loss: 0.00002011
Iteration 116/1000 | Loss: 0.00002011
Iteration 117/1000 | Loss: 0.00002011
Iteration 118/1000 | Loss: 0.00002010
Iteration 119/1000 | Loss: 0.00002010
Iteration 120/1000 | Loss: 0.00002010
Iteration 121/1000 | Loss: 0.00002010
Iteration 122/1000 | Loss: 0.00002010
Iteration 123/1000 | Loss: 0.00002010
Iteration 124/1000 | Loss: 0.00002010
Iteration 125/1000 | Loss: 0.00002010
Iteration 126/1000 | Loss: 0.00002010
Iteration 127/1000 | Loss: 0.00002010
Iteration 128/1000 | Loss: 0.00002010
Iteration 129/1000 | Loss: 0.00002010
Iteration 130/1000 | Loss: 0.00002009
Iteration 131/1000 | Loss: 0.00002009
Iteration 132/1000 | Loss: 0.00002009
Iteration 133/1000 | Loss: 0.00002009
Iteration 134/1000 | Loss: 0.00002009
Iteration 135/1000 | Loss: 0.00002009
Iteration 136/1000 | Loss: 0.00002009
Iteration 137/1000 | Loss: 0.00002009
Iteration 138/1000 | Loss: 0.00002009
Iteration 139/1000 | Loss: 0.00002008
Iteration 140/1000 | Loss: 0.00002008
Iteration 141/1000 | Loss: 0.00002008
Iteration 142/1000 | Loss: 0.00002008
Iteration 143/1000 | Loss: 0.00002008
Iteration 144/1000 | Loss: 0.00002008
Iteration 145/1000 | Loss: 0.00002008
Iteration 146/1000 | Loss: 0.00002008
Iteration 147/1000 | Loss: 0.00002007
Iteration 148/1000 | Loss: 0.00002007
Iteration 149/1000 | Loss: 0.00002007
Iteration 150/1000 | Loss: 0.00002007
Iteration 151/1000 | Loss: 0.00002007
Iteration 152/1000 | Loss: 0.00002007
Iteration 153/1000 | Loss: 0.00002007
Iteration 154/1000 | Loss: 0.00002007
Iteration 155/1000 | Loss: 0.00002007
Iteration 156/1000 | Loss: 0.00002007
Iteration 157/1000 | Loss: 0.00002007
Iteration 158/1000 | Loss: 0.00002007
Iteration 159/1000 | Loss: 0.00002007
Iteration 160/1000 | Loss: 0.00002007
Iteration 161/1000 | Loss: 0.00002007
Iteration 162/1000 | Loss: 0.00002007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.006911745411344e-05, 2.006911745411344e-05, 2.006911745411344e-05, 2.006911745411344e-05, 2.006911745411344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.006911745411344e-05

Optimization complete. Final v2v error: 3.73030161857605 mm

Highest mean error: 4.43865966796875 mm for frame 179

Lowest mean error: 3.5281529426574707 mm for frame 108

Saving results

Total time: 155.74639773368835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844478
Iteration 2/25 | Loss: 0.00134800
Iteration 3/25 | Loss: 0.00093006
Iteration 4/25 | Loss: 0.00088203
Iteration 5/25 | Loss: 0.00087195
Iteration 6/25 | Loss: 0.00086819
Iteration 7/25 | Loss: 0.00086693
Iteration 8/25 | Loss: 0.00086693
Iteration 9/25 | Loss: 0.00086693
Iteration 10/25 | Loss: 0.00086693
Iteration 11/25 | Loss: 0.00086693
Iteration 12/25 | Loss: 0.00086693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00086693384218961, 0.00086693384218961, 0.00086693384218961, 0.00086693384218961, 0.00086693384218961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00086693384218961

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43610811
Iteration 2/25 | Loss: 0.00038434
Iteration 3/25 | Loss: 0.00038434
Iteration 4/25 | Loss: 0.00038434
Iteration 5/25 | Loss: 0.00038434
Iteration 6/25 | Loss: 0.00038434
Iteration 7/25 | Loss: 0.00038434
Iteration 8/25 | Loss: 0.00038434
Iteration 9/25 | Loss: 0.00038434
Iteration 10/25 | Loss: 0.00038434
Iteration 11/25 | Loss: 0.00038434
Iteration 12/25 | Loss: 0.00038434
Iteration 13/25 | Loss: 0.00038434
Iteration 14/25 | Loss: 0.00038434
Iteration 15/25 | Loss: 0.00038434
Iteration 16/25 | Loss: 0.00038434
Iteration 17/25 | Loss: 0.00038434
Iteration 18/25 | Loss: 0.00038434
Iteration 19/25 | Loss: 0.00038434
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00038433942245319486, 0.00038433942245319486, 0.00038433942245319486, 0.00038433942245319486, 0.00038433942245319486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038433942245319486

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038434
Iteration 2/1000 | Loss: 0.00003765
Iteration 3/1000 | Loss: 0.00002870
Iteration 4/1000 | Loss: 0.00002623
Iteration 5/1000 | Loss: 0.00002494
Iteration 6/1000 | Loss: 0.00002405
Iteration 7/1000 | Loss: 0.00002357
Iteration 8/1000 | Loss: 0.00002318
Iteration 9/1000 | Loss: 0.00002297
Iteration 10/1000 | Loss: 0.00002281
Iteration 11/1000 | Loss: 0.00002281
Iteration 12/1000 | Loss: 0.00002280
Iteration 13/1000 | Loss: 0.00002280
Iteration 14/1000 | Loss: 0.00002276
Iteration 15/1000 | Loss: 0.00002269
Iteration 16/1000 | Loss: 0.00002268
Iteration 17/1000 | Loss: 0.00002268
Iteration 18/1000 | Loss: 0.00002268
Iteration 19/1000 | Loss: 0.00002267
Iteration 20/1000 | Loss: 0.00002267
Iteration 21/1000 | Loss: 0.00002267
Iteration 22/1000 | Loss: 0.00002266
Iteration 23/1000 | Loss: 0.00002266
Iteration 24/1000 | Loss: 0.00002266
Iteration 25/1000 | Loss: 0.00002265
Iteration 26/1000 | Loss: 0.00002265
Iteration 27/1000 | Loss: 0.00002265
Iteration 28/1000 | Loss: 0.00002264
Iteration 29/1000 | Loss: 0.00002264
Iteration 30/1000 | Loss: 0.00002264
Iteration 31/1000 | Loss: 0.00002264
Iteration 32/1000 | Loss: 0.00002264
Iteration 33/1000 | Loss: 0.00002264
Iteration 34/1000 | Loss: 0.00002264
Iteration 35/1000 | Loss: 0.00002264
Iteration 36/1000 | Loss: 0.00002263
Iteration 37/1000 | Loss: 0.00002263
Iteration 38/1000 | Loss: 0.00002263
Iteration 39/1000 | Loss: 0.00002263
Iteration 40/1000 | Loss: 0.00002263
Iteration 41/1000 | Loss: 0.00002262
Iteration 42/1000 | Loss: 0.00002262
Iteration 43/1000 | Loss: 0.00002262
Iteration 44/1000 | Loss: 0.00002262
Iteration 45/1000 | Loss: 0.00002261
Iteration 46/1000 | Loss: 0.00002261
Iteration 47/1000 | Loss: 0.00002261
Iteration 48/1000 | Loss: 0.00002261
Iteration 49/1000 | Loss: 0.00002261
Iteration 50/1000 | Loss: 0.00002261
Iteration 51/1000 | Loss: 0.00002261
Iteration 52/1000 | Loss: 0.00002261
Iteration 53/1000 | Loss: 0.00002260
Iteration 54/1000 | Loss: 0.00002260
Iteration 55/1000 | Loss: 0.00002260
Iteration 56/1000 | Loss: 0.00002260
Iteration 57/1000 | Loss: 0.00002260
Iteration 58/1000 | Loss: 0.00002260
Iteration 59/1000 | Loss: 0.00002260
Iteration 60/1000 | Loss: 0.00002260
Iteration 61/1000 | Loss: 0.00002260
Iteration 62/1000 | Loss: 0.00002260
Iteration 63/1000 | Loss: 0.00002260
Iteration 64/1000 | Loss: 0.00002260
Iteration 65/1000 | Loss: 0.00002260
Iteration 66/1000 | Loss: 0.00002260
Iteration 67/1000 | Loss: 0.00002260
Iteration 68/1000 | Loss: 0.00002260
Iteration 69/1000 | Loss: 0.00002259
Iteration 70/1000 | Loss: 0.00002259
Iteration 71/1000 | Loss: 0.00002259
Iteration 72/1000 | Loss: 0.00002259
Iteration 73/1000 | Loss: 0.00002259
Iteration 74/1000 | Loss: 0.00002259
Iteration 75/1000 | Loss: 0.00002259
Iteration 76/1000 | Loss: 0.00002259
Iteration 77/1000 | Loss: 0.00002259
Iteration 78/1000 | Loss: 0.00002259
Iteration 79/1000 | Loss: 0.00002259
Iteration 80/1000 | Loss: 0.00002259
Iteration 81/1000 | Loss: 0.00002259
Iteration 82/1000 | Loss: 0.00002258
Iteration 83/1000 | Loss: 0.00002258
Iteration 84/1000 | Loss: 0.00002258
Iteration 85/1000 | Loss: 0.00002258
Iteration 86/1000 | Loss: 0.00002258
Iteration 87/1000 | Loss: 0.00002258
Iteration 88/1000 | Loss: 0.00002258
Iteration 89/1000 | Loss: 0.00002258
Iteration 90/1000 | Loss: 0.00002258
Iteration 91/1000 | Loss: 0.00002258
Iteration 92/1000 | Loss: 0.00002257
Iteration 93/1000 | Loss: 0.00002257
Iteration 94/1000 | Loss: 0.00002257
Iteration 95/1000 | Loss: 0.00002257
Iteration 96/1000 | Loss: 0.00002257
Iteration 97/1000 | Loss: 0.00002257
Iteration 98/1000 | Loss: 0.00002257
Iteration 99/1000 | Loss: 0.00002257
Iteration 100/1000 | Loss: 0.00002257
Iteration 101/1000 | Loss: 0.00002257
Iteration 102/1000 | Loss: 0.00002257
Iteration 103/1000 | Loss: 0.00002257
Iteration 104/1000 | Loss: 0.00002257
Iteration 105/1000 | Loss: 0.00002257
Iteration 106/1000 | Loss: 0.00002257
Iteration 107/1000 | Loss: 0.00002257
Iteration 108/1000 | Loss: 0.00002257
Iteration 109/1000 | Loss: 0.00002257
Iteration 110/1000 | Loss: 0.00002257
Iteration 111/1000 | Loss: 0.00002257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.2573101887246594e-05, 2.2573101887246594e-05, 2.2573101887246594e-05, 2.2573101887246594e-05, 2.2573101887246594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2573101887246594e-05

Optimization complete. Final v2v error: 3.950040817260742 mm

Highest mean error: 4.551531791687012 mm for frame 56

Lowest mean error: 3.3070600032806396 mm for frame 0

Saving results

Total time: 36.687814474105835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840466
Iteration 2/25 | Loss: 0.00127593
Iteration 3/25 | Loss: 0.00095754
Iteration 4/25 | Loss: 0.00091239
Iteration 5/25 | Loss: 0.00090690
Iteration 6/25 | Loss: 0.00089644
Iteration 7/25 | Loss: 0.00088687
Iteration 8/25 | Loss: 0.00087974
Iteration 9/25 | Loss: 0.00087479
Iteration 10/25 | Loss: 0.00086902
Iteration 11/25 | Loss: 0.00086833
Iteration 12/25 | Loss: 0.00086826
Iteration 13/25 | Loss: 0.00087299
Iteration 14/25 | Loss: 0.00086483
Iteration 15/25 | Loss: 0.00086441
Iteration 16/25 | Loss: 0.00086650
Iteration 17/25 | Loss: 0.00086649
Iteration 18/25 | Loss: 0.00086625
Iteration 19/25 | Loss: 0.00086529
Iteration 20/25 | Loss: 0.00086588
Iteration 21/25 | Loss: 0.00086412
Iteration 22/25 | Loss: 0.00086411
Iteration 23/25 | Loss: 0.00086411
Iteration 24/25 | Loss: 0.00086411
Iteration 25/25 | Loss: 0.00086411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69508326
Iteration 2/25 | Loss: 0.00049742
Iteration 3/25 | Loss: 0.00049742
Iteration 4/25 | Loss: 0.00049742
Iteration 5/25 | Loss: 0.00049741
Iteration 6/25 | Loss: 0.00049741
Iteration 7/25 | Loss: 0.00049741
Iteration 8/25 | Loss: 0.00049741
Iteration 9/25 | Loss: 0.00049741
Iteration 10/25 | Loss: 0.00049741
Iteration 11/25 | Loss: 0.00049741
Iteration 12/25 | Loss: 0.00049741
Iteration 13/25 | Loss: 0.00049741
Iteration 14/25 | Loss: 0.00049741
Iteration 15/25 | Loss: 0.00049741
Iteration 16/25 | Loss: 0.00049741
Iteration 17/25 | Loss: 0.00049741
Iteration 18/25 | Loss: 0.00049741
Iteration 19/25 | Loss: 0.00049741
Iteration 20/25 | Loss: 0.00049741
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000497414090204984, 0.000497414090204984, 0.000497414090204984, 0.000497414090204984, 0.000497414090204984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000497414090204984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049741
Iteration 2/1000 | Loss: 0.00003463
Iteration 3/1000 | Loss: 0.00002616
Iteration 4/1000 | Loss: 0.00002373
Iteration 5/1000 | Loss: 0.00002252
Iteration 6/1000 | Loss: 0.00002178
Iteration 7/1000 | Loss: 0.00002145
Iteration 8/1000 | Loss: 0.00002129
Iteration 9/1000 | Loss: 0.00002126
Iteration 10/1000 | Loss: 0.00002123
Iteration 11/1000 | Loss: 0.00002118
Iteration 12/1000 | Loss: 0.00002118
Iteration 13/1000 | Loss: 0.00002117
Iteration 14/1000 | Loss: 0.00002115
Iteration 15/1000 | Loss: 0.00002114
Iteration 16/1000 | Loss: 0.00004763
Iteration 17/1000 | Loss: 0.00002113
Iteration 18/1000 | Loss: 0.00002105
Iteration 19/1000 | Loss: 0.00002104
Iteration 20/1000 | Loss: 0.00002104
Iteration 21/1000 | Loss: 0.00002102
Iteration 22/1000 | Loss: 0.00002101
Iteration 23/1000 | Loss: 0.00002101
Iteration 24/1000 | Loss: 0.00002101
Iteration 25/1000 | Loss: 0.00002100
Iteration 26/1000 | Loss: 0.00002100
Iteration 27/1000 | Loss: 0.00002099
Iteration 28/1000 | Loss: 0.00002098
Iteration 29/1000 | Loss: 0.00002098
Iteration 30/1000 | Loss: 0.00002098
Iteration 31/1000 | Loss: 0.00002098
Iteration 32/1000 | Loss: 0.00002098
Iteration 33/1000 | Loss: 0.00002098
Iteration 34/1000 | Loss: 0.00002098
Iteration 35/1000 | Loss: 0.00002098
Iteration 36/1000 | Loss: 0.00002098
Iteration 37/1000 | Loss: 0.00002098
Iteration 38/1000 | Loss: 0.00002098
Iteration 39/1000 | Loss: 0.00002097
Iteration 40/1000 | Loss: 0.00002096
Iteration 41/1000 | Loss: 0.00002095
Iteration 42/1000 | Loss: 0.00002095
Iteration 43/1000 | Loss: 0.00002094
Iteration 44/1000 | Loss: 0.00002094
Iteration 45/1000 | Loss: 0.00002094
Iteration 46/1000 | Loss: 0.00002093
Iteration 47/1000 | Loss: 0.00002093
Iteration 48/1000 | Loss: 0.00002093
Iteration 49/1000 | Loss: 0.00002093
Iteration 50/1000 | Loss: 0.00002093
Iteration 51/1000 | Loss: 0.00002093
Iteration 52/1000 | Loss: 0.00002093
Iteration 53/1000 | Loss: 0.00002093
Iteration 54/1000 | Loss: 0.00002093
Iteration 55/1000 | Loss: 0.00002093
Iteration 56/1000 | Loss: 0.00002092
Iteration 57/1000 | Loss: 0.00002092
Iteration 58/1000 | Loss: 0.00002092
Iteration 59/1000 | Loss: 0.00002092
Iteration 60/1000 | Loss: 0.00002092
Iteration 61/1000 | Loss: 0.00002092
Iteration 62/1000 | Loss: 0.00002092
Iteration 63/1000 | Loss: 0.00002092
Iteration 64/1000 | Loss: 0.00002092
Iteration 65/1000 | Loss: 0.00002092
Iteration 66/1000 | Loss: 0.00002091
Iteration 67/1000 | Loss: 0.00002091
Iteration 68/1000 | Loss: 0.00002091
Iteration 69/1000 | Loss: 0.00002091
Iteration 70/1000 | Loss: 0.00002091
Iteration 71/1000 | Loss: 0.00002091
Iteration 72/1000 | Loss: 0.00002090
Iteration 73/1000 | Loss: 0.00002090
Iteration 74/1000 | Loss: 0.00002090
Iteration 75/1000 | Loss: 0.00002090
Iteration 76/1000 | Loss: 0.00002090
Iteration 77/1000 | Loss: 0.00002090
Iteration 78/1000 | Loss: 0.00002090
Iteration 79/1000 | Loss: 0.00002090
Iteration 80/1000 | Loss: 0.00002090
Iteration 81/1000 | Loss: 0.00002090
Iteration 82/1000 | Loss: 0.00002090
Iteration 83/1000 | Loss: 0.00002090
Iteration 84/1000 | Loss: 0.00002090
Iteration 85/1000 | Loss: 0.00002089
Iteration 86/1000 | Loss: 0.00002089
Iteration 87/1000 | Loss: 0.00002089
Iteration 88/1000 | Loss: 0.00002089
Iteration 89/1000 | Loss: 0.00002089
Iteration 90/1000 | Loss: 0.00002089
Iteration 91/1000 | Loss: 0.00002089
Iteration 92/1000 | Loss: 0.00002089
Iteration 93/1000 | Loss: 0.00002089
Iteration 94/1000 | Loss: 0.00002089
Iteration 95/1000 | Loss: 0.00002089
Iteration 96/1000 | Loss: 0.00002089
Iteration 97/1000 | Loss: 0.00002089
Iteration 98/1000 | Loss: 0.00002089
Iteration 99/1000 | Loss: 0.00002089
Iteration 100/1000 | Loss: 0.00002089
Iteration 101/1000 | Loss: 0.00002089
Iteration 102/1000 | Loss: 0.00002088
Iteration 103/1000 | Loss: 0.00002088
Iteration 104/1000 | Loss: 0.00002088
Iteration 105/1000 | Loss: 0.00002088
Iteration 106/1000 | Loss: 0.00002088
Iteration 107/1000 | Loss: 0.00002088
Iteration 108/1000 | Loss: 0.00002088
Iteration 109/1000 | Loss: 0.00002088
Iteration 110/1000 | Loss: 0.00002088
Iteration 111/1000 | Loss: 0.00002088
Iteration 112/1000 | Loss: 0.00002088
Iteration 113/1000 | Loss: 0.00002088
Iteration 114/1000 | Loss: 0.00002088
Iteration 115/1000 | Loss: 0.00002088
Iteration 116/1000 | Loss: 0.00002088
Iteration 117/1000 | Loss: 0.00002088
Iteration 118/1000 | Loss: 0.00002088
Iteration 119/1000 | Loss: 0.00002088
Iteration 120/1000 | Loss: 0.00002088
Iteration 121/1000 | Loss: 0.00002088
Iteration 122/1000 | Loss: 0.00002088
Iteration 123/1000 | Loss: 0.00002088
Iteration 124/1000 | Loss: 0.00002088
Iteration 125/1000 | Loss: 0.00002088
Iteration 126/1000 | Loss: 0.00002088
Iteration 127/1000 | Loss: 0.00002088
Iteration 128/1000 | Loss: 0.00002088
Iteration 129/1000 | Loss: 0.00002088
Iteration 130/1000 | Loss: 0.00002088
Iteration 131/1000 | Loss: 0.00002088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.0883868273813277e-05, 2.0883868273813277e-05, 2.0883868273813277e-05, 2.0883868273813277e-05, 2.0883868273813277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0883868273813277e-05

Optimization complete. Final v2v error: 3.834336280822754 mm

Highest mean error: 4.692793846130371 mm for frame 44

Lowest mean error: 3.3632073402404785 mm for frame 140

Saving results

Total time: 56.30618190765381
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819056
Iteration 2/25 | Loss: 0.00128649
Iteration 3/25 | Loss: 0.00093960
Iteration 4/25 | Loss: 0.00089255
Iteration 5/25 | Loss: 0.00087893
Iteration 6/25 | Loss: 0.00087440
Iteration 7/25 | Loss: 0.00087276
Iteration 8/25 | Loss: 0.00087252
Iteration 9/25 | Loss: 0.00087252
Iteration 10/25 | Loss: 0.00087252
Iteration 11/25 | Loss: 0.00087252
Iteration 12/25 | Loss: 0.00087252
Iteration 13/25 | Loss: 0.00087252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008725248044356704, 0.0008725248044356704, 0.0008725248044356704, 0.0008725248044356704, 0.0008725248044356704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008725248044356704

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.42315912
Iteration 2/25 | Loss: 0.00052936
Iteration 3/25 | Loss: 0.00052935
Iteration 4/25 | Loss: 0.00052935
Iteration 5/25 | Loss: 0.00052935
Iteration 6/25 | Loss: 0.00052935
Iteration 7/25 | Loss: 0.00052935
Iteration 8/25 | Loss: 0.00052935
Iteration 9/25 | Loss: 0.00052935
Iteration 10/25 | Loss: 0.00052935
Iteration 11/25 | Loss: 0.00052935
Iteration 12/25 | Loss: 0.00052935
Iteration 13/25 | Loss: 0.00052935
Iteration 14/25 | Loss: 0.00052935
Iteration 15/25 | Loss: 0.00052935
Iteration 16/25 | Loss: 0.00052935
Iteration 17/25 | Loss: 0.00052935
Iteration 18/25 | Loss: 0.00052935
Iteration 19/25 | Loss: 0.00052935
Iteration 20/25 | Loss: 0.00052935
Iteration 21/25 | Loss: 0.00052935
Iteration 22/25 | Loss: 0.00052935
Iteration 23/25 | Loss: 0.00052935
Iteration 24/25 | Loss: 0.00052935
Iteration 25/25 | Loss: 0.00052935

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052935
Iteration 2/1000 | Loss: 0.00004447
Iteration 3/1000 | Loss: 0.00002691
Iteration 4/1000 | Loss: 0.00002349
Iteration 5/1000 | Loss: 0.00002189
Iteration 6/1000 | Loss: 0.00002088
Iteration 7/1000 | Loss: 0.00002021
Iteration 8/1000 | Loss: 0.00001986
Iteration 9/1000 | Loss: 0.00001963
Iteration 10/1000 | Loss: 0.00001961
Iteration 11/1000 | Loss: 0.00001954
Iteration 12/1000 | Loss: 0.00001953
Iteration 13/1000 | Loss: 0.00001937
Iteration 14/1000 | Loss: 0.00001932
Iteration 15/1000 | Loss: 0.00001926
Iteration 16/1000 | Loss: 0.00001924
Iteration 17/1000 | Loss: 0.00001923
Iteration 18/1000 | Loss: 0.00001920
Iteration 19/1000 | Loss: 0.00001919
Iteration 20/1000 | Loss: 0.00001919
Iteration 21/1000 | Loss: 0.00001918
Iteration 22/1000 | Loss: 0.00001918
Iteration 23/1000 | Loss: 0.00001917
Iteration 24/1000 | Loss: 0.00001917
Iteration 25/1000 | Loss: 0.00001917
Iteration 26/1000 | Loss: 0.00001916
Iteration 27/1000 | Loss: 0.00001916
Iteration 28/1000 | Loss: 0.00001915
Iteration 29/1000 | Loss: 0.00001915
Iteration 30/1000 | Loss: 0.00001915
Iteration 31/1000 | Loss: 0.00001915
Iteration 32/1000 | Loss: 0.00001915
Iteration 33/1000 | Loss: 0.00001915
Iteration 34/1000 | Loss: 0.00001915
Iteration 35/1000 | Loss: 0.00001915
Iteration 36/1000 | Loss: 0.00001915
Iteration 37/1000 | Loss: 0.00001915
Iteration 38/1000 | Loss: 0.00001915
Iteration 39/1000 | Loss: 0.00001915
Iteration 40/1000 | Loss: 0.00001914
Iteration 41/1000 | Loss: 0.00001914
Iteration 42/1000 | Loss: 0.00001914
Iteration 43/1000 | Loss: 0.00001914
Iteration 44/1000 | Loss: 0.00001914
Iteration 45/1000 | Loss: 0.00001914
Iteration 46/1000 | Loss: 0.00001913
Iteration 47/1000 | Loss: 0.00001912
Iteration 48/1000 | Loss: 0.00001912
Iteration 49/1000 | Loss: 0.00001912
Iteration 50/1000 | Loss: 0.00001912
Iteration 51/1000 | Loss: 0.00001912
Iteration 52/1000 | Loss: 0.00001912
Iteration 53/1000 | Loss: 0.00001911
Iteration 54/1000 | Loss: 0.00001911
Iteration 55/1000 | Loss: 0.00001911
Iteration 56/1000 | Loss: 0.00001911
Iteration 57/1000 | Loss: 0.00001911
Iteration 58/1000 | Loss: 0.00001911
Iteration 59/1000 | Loss: 0.00001910
Iteration 60/1000 | Loss: 0.00001910
Iteration 61/1000 | Loss: 0.00001910
Iteration 62/1000 | Loss: 0.00001910
Iteration 63/1000 | Loss: 0.00001910
Iteration 64/1000 | Loss: 0.00001910
Iteration 65/1000 | Loss: 0.00001910
Iteration 66/1000 | Loss: 0.00001910
Iteration 67/1000 | Loss: 0.00001910
Iteration 68/1000 | Loss: 0.00001910
Iteration 69/1000 | Loss: 0.00001910
Iteration 70/1000 | Loss: 0.00001910
Iteration 71/1000 | Loss: 0.00001910
Iteration 72/1000 | Loss: 0.00001910
Iteration 73/1000 | Loss: 0.00001910
Iteration 74/1000 | Loss: 0.00001910
Iteration 75/1000 | Loss: 0.00001910
Iteration 76/1000 | Loss: 0.00001910
Iteration 77/1000 | Loss: 0.00001910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.9104243619949557e-05, 1.9104243619949557e-05, 1.9104243619949557e-05, 1.9104243619949557e-05, 1.9104243619949557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9104243619949557e-05

Optimization complete. Final v2v error: 3.7690179347991943 mm

Highest mean error: 4.434514045715332 mm for frame 78

Lowest mean error: 3.201550006866455 mm for frame 110

Saving results

Total time: 35.72212886810303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884550
Iteration 2/25 | Loss: 0.00117354
Iteration 3/25 | Loss: 0.00098930
Iteration 4/25 | Loss: 0.00094414
Iteration 5/25 | Loss: 0.00092027
Iteration 6/25 | Loss: 0.00091617
Iteration 7/25 | Loss: 0.00091504
Iteration 8/25 | Loss: 0.00091468
Iteration 9/25 | Loss: 0.00091468
Iteration 10/25 | Loss: 0.00091468
Iteration 11/25 | Loss: 0.00091468
Iteration 12/25 | Loss: 0.00091468
Iteration 13/25 | Loss: 0.00091468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000914677104447037, 0.000914677104447037, 0.000914677104447037, 0.000914677104447037, 0.000914677104447037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000914677104447037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59026647
Iteration 2/25 | Loss: 0.00057500
Iteration 3/25 | Loss: 0.00057500
Iteration 4/25 | Loss: 0.00057500
Iteration 5/25 | Loss: 0.00057500
Iteration 6/25 | Loss: 0.00057500
Iteration 7/25 | Loss: 0.00057500
Iteration 8/25 | Loss: 0.00057500
Iteration 9/25 | Loss: 0.00057500
Iteration 10/25 | Loss: 0.00057500
Iteration 11/25 | Loss: 0.00057500
Iteration 12/25 | Loss: 0.00057500
Iteration 13/25 | Loss: 0.00057500
Iteration 14/25 | Loss: 0.00057500
Iteration 15/25 | Loss: 0.00057500
Iteration 16/25 | Loss: 0.00057500
Iteration 17/25 | Loss: 0.00057500
Iteration 18/25 | Loss: 0.00057500
Iteration 19/25 | Loss: 0.00057500
Iteration 20/25 | Loss: 0.00057500
Iteration 21/25 | Loss: 0.00057500
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005749997799284756, 0.0005749997799284756, 0.0005749997799284756, 0.0005749997799284756, 0.0005749997799284756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005749997799284756

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057500
Iteration 2/1000 | Loss: 0.00005372
Iteration 3/1000 | Loss: 0.00003967
Iteration 4/1000 | Loss: 0.00003237
Iteration 5/1000 | Loss: 0.00002959
Iteration 6/1000 | Loss: 0.00002836
Iteration 7/1000 | Loss: 0.00002741
Iteration 8/1000 | Loss: 0.00002643
Iteration 9/1000 | Loss: 0.00002568
Iteration 10/1000 | Loss: 0.00002515
Iteration 11/1000 | Loss: 0.00002490
Iteration 12/1000 | Loss: 0.00002464
Iteration 13/1000 | Loss: 0.00002444
Iteration 14/1000 | Loss: 0.00002443
Iteration 15/1000 | Loss: 0.00002429
Iteration 16/1000 | Loss: 0.00002428
Iteration 17/1000 | Loss: 0.00002420
Iteration 18/1000 | Loss: 0.00002420
Iteration 19/1000 | Loss: 0.00002418
Iteration 20/1000 | Loss: 0.00002418
Iteration 21/1000 | Loss: 0.00002412
Iteration 22/1000 | Loss: 0.00002411
Iteration 23/1000 | Loss: 0.00002407
Iteration 24/1000 | Loss: 0.00002403
Iteration 25/1000 | Loss: 0.00002402
Iteration 26/1000 | Loss: 0.00002402
Iteration 27/1000 | Loss: 0.00002402
Iteration 28/1000 | Loss: 0.00002402
Iteration 29/1000 | Loss: 0.00002402
Iteration 30/1000 | Loss: 0.00002402
Iteration 31/1000 | Loss: 0.00002402
Iteration 32/1000 | Loss: 0.00002402
Iteration 33/1000 | Loss: 0.00002402
Iteration 34/1000 | Loss: 0.00002402
Iteration 35/1000 | Loss: 0.00002401
Iteration 36/1000 | Loss: 0.00002401
Iteration 37/1000 | Loss: 0.00002400
Iteration 38/1000 | Loss: 0.00002400
Iteration 39/1000 | Loss: 0.00002399
Iteration 40/1000 | Loss: 0.00002399
Iteration 41/1000 | Loss: 0.00002399
Iteration 42/1000 | Loss: 0.00002398
Iteration 43/1000 | Loss: 0.00002397
Iteration 44/1000 | Loss: 0.00002397
Iteration 45/1000 | Loss: 0.00002397
Iteration 46/1000 | Loss: 0.00002396
Iteration 47/1000 | Loss: 0.00002396
Iteration 48/1000 | Loss: 0.00002396
Iteration 49/1000 | Loss: 0.00002395
Iteration 50/1000 | Loss: 0.00002395
Iteration 51/1000 | Loss: 0.00002395
Iteration 52/1000 | Loss: 0.00002395
Iteration 53/1000 | Loss: 0.00002395
Iteration 54/1000 | Loss: 0.00002395
Iteration 55/1000 | Loss: 0.00002394
Iteration 56/1000 | Loss: 0.00002394
Iteration 57/1000 | Loss: 0.00002393
Iteration 58/1000 | Loss: 0.00002393
Iteration 59/1000 | Loss: 0.00002392
Iteration 60/1000 | Loss: 0.00002392
Iteration 61/1000 | Loss: 0.00002392
Iteration 62/1000 | Loss: 0.00002391
Iteration 63/1000 | Loss: 0.00002391
Iteration 64/1000 | Loss: 0.00002391
Iteration 65/1000 | Loss: 0.00002391
Iteration 66/1000 | Loss: 0.00002390
Iteration 67/1000 | Loss: 0.00002390
Iteration 68/1000 | Loss: 0.00002390
Iteration 69/1000 | Loss: 0.00002390
Iteration 70/1000 | Loss: 0.00002390
Iteration 71/1000 | Loss: 0.00002389
Iteration 72/1000 | Loss: 0.00002389
Iteration 73/1000 | Loss: 0.00002389
Iteration 74/1000 | Loss: 0.00002389
Iteration 75/1000 | Loss: 0.00002388
Iteration 76/1000 | Loss: 0.00002388
Iteration 77/1000 | Loss: 0.00002387
Iteration 78/1000 | Loss: 0.00002387
Iteration 79/1000 | Loss: 0.00002387
Iteration 80/1000 | Loss: 0.00002387
Iteration 81/1000 | Loss: 0.00002387
Iteration 82/1000 | Loss: 0.00002387
Iteration 83/1000 | Loss: 0.00002387
Iteration 84/1000 | Loss: 0.00002387
Iteration 85/1000 | Loss: 0.00002386
Iteration 86/1000 | Loss: 0.00002386
Iteration 87/1000 | Loss: 0.00002386
Iteration 88/1000 | Loss: 0.00002386
Iteration 89/1000 | Loss: 0.00002385
Iteration 90/1000 | Loss: 0.00002385
Iteration 91/1000 | Loss: 0.00002385
Iteration 92/1000 | Loss: 0.00002385
Iteration 93/1000 | Loss: 0.00002385
Iteration 94/1000 | Loss: 0.00002384
Iteration 95/1000 | Loss: 0.00002384
Iteration 96/1000 | Loss: 0.00002384
Iteration 97/1000 | Loss: 0.00002383
Iteration 98/1000 | Loss: 0.00002383
Iteration 99/1000 | Loss: 0.00002383
Iteration 100/1000 | Loss: 0.00002383
Iteration 101/1000 | Loss: 0.00002383
Iteration 102/1000 | Loss: 0.00002382
Iteration 103/1000 | Loss: 0.00002382
Iteration 104/1000 | Loss: 0.00002382
Iteration 105/1000 | Loss: 0.00002382
Iteration 106/1000 | Loss: 0.00002382
Iteration 107/1000 | Loss: 0.00002382
Iteration 108/1000 | Loss: 0.00002382
Iteration 109/1000 | Loss: 0.00002382
Iteration 110/1000 | Loss: 0.00002381
Iteration 111/1000 | Loss: 0.00002381
Iteration 112/1000 | Loss: 0.00002381
Iteration 113/1000 | Loss: 0.00002381
Iteration 114/1000 | Loss: 0.00002381
Iteration 115/1000 | Loss: 0.00002381
Iteration 116/1000 | Loss: 0.00002381
Iteration 117/1000 | Loss: 0.00002381
Iteration 118/1000 | Loss: 0.00002381
Iteration 119/1000 | Loss: 0.00002380
Iteration 120/1000 | Loss: 0.00002380
Iteration 121/1000 | Loss: 0.00002380
Iteration 122/1000 | Loss: 0.00002380
Iteration 123/1000 | Loss: 0.00002380
Iteration 124/1000 | Loss: 0.00002380
Iteration 125/1000 | Loss: 0.00002380
Iteration 126/1000 | Loss: 0.00002379
Iteration 127/1000 | Loss: 0.00002379
Iteration 128/1000 | Loss: 0.00002379
Iteration 129/1000 | Loss: 0.00002379
Iteration 130/1000 | Loss: 0.00002379
Iteration 131/1000 | Loss: 0.00002379
Iteration 132/1000 | Loss: 0.00002379
Iteration 133/1000 | Loss: 0.00002378
Iteration 134/1000 | Loss: 0.00002378
Iteration 135/1000 | Loss: 0.00002378
Iteration 136/1000 | Loss: 0.00002378
Iteration 137/1000 | Loss: 0.00002378
Iteration 138/1000 | Loss: 0.00002378
Iteration 139/1000 | Loss: 0.00002378
Iteration 140/1000 | Loss: 0.00002377
Iteration 141/1000 | Loss: 0.00002377
Iteration 142/1000 | Loss: 0.00002377
Iteration 143/1000 | Loss: 0.00002377
Iteration 144/1000 | Loss: 0.00002377
Iteration 145/1000 | Loss: 0.00002377
Iteration 146/1000 | Loss: 0.00002377
Iteration 147/1000 | Loss: 0.00002377
Iteration 148/1000 | Loss: 0.00002377
Iteration 149/1000 | Loss: 0.00002377
Iteration 150/1000 | Loss: 0.00002376
Iteration 151/1000 | Loss: 0.00002376
Iteration 152/1000 | Loss: 0.00002376
Iteration 153/1000 | Loss: 0.00002376
Iteration 154/1000 | Loss: 0.00002376
Iteration 155/1000 | Loss: 0.00002376
Iteration 156/1000 | Loss: 0.00002376
Iteration 157/1000 | Loss: 0.00002375
Iteration 158/1000 | Loss: 0.00002375
Iteration 159/1000 | Loss: 0.00002375
Iteration 160/1000 | Loss: 0.00002375
Iteration 161/1000 | Loss: 0.00002375
Iteration 162/1000 | Loss: 0.00002375
Iteration 163/1000 | Loss: 0.00002374
Iteration 164/1000 | Loss: 0.00002374
Iteration 165/1000 | Loss: 0.00002374
Iteration 166/1000 | Loss: 0.00002374
Iteration 167/1000 | Loss: 0.00002374
Iteration 168/1000 | Loss: 0.00002374
Iteration 169/1000 | Loss: 0.00002374
Iteration 170/1000 | Loss: 0.00002374
Iteration 171/1000 | Loss: 0.00002374
Iteration 172/1000 | Loss: 0.00002374
Iteration 173/1000 | Loss: 0.00002374
Iteration 174/1000 | Loss: 0.00002374
Iteration 175/1000 | Loss: 0.00002374
Iteration 176/1000 | Loss: 0.00002374
Iteration 177/1000 | Loss: 0.00002374
Iteration 178/1000 | Loss: 0.00002374
Iteration 179/1000 | Loss: 0.00002374
Iteration 180/1000 | Loss: 0.00002374
Iteration 181/1000 | Loss: 0.00002374
Iteration 182/1000 | Loss: 0.00002374
Iteration 183/1000 | Loss: 0.00002374
Iteration 184/1000 | Loss: 0.00002373
Iteration 185/1000 | Loss: 0.00002373
Iteration 186/1000 | Loss: 0.00002373
Iteration 187/1000 | Loss: 0.00002373
Iteration 188/1000 | Loss: 0.00002373
Iteration 189/1000 | Loss: 0.00002373
Iteration 190/1000 | Loss: 0.00002373
Iteration 191/1000 | Loss: 0.00002373
Iteration 192/1000 | Loss: 0.00002373
Iteration 193/1000 | Loss: 0.00002373
Iteration 194/1000 | Loss: 0.00002373
Iteration 195/1000 | Loss: 0.00002373
Iteration 196/1000 | Loss: 0.00002373
Iteration 197/1000 | Loss: 0.00002373
Iteration 198/1000 | Loss: 0.00002373
Iteration 199/1000 | Loss: 0.00002373
Iteration 200/1000 | Loss: 0.00002373
Iteration 201/1000 | Loss: 0.00002373
Iteration 202/1000 | Loss: 0.00002373
Iteration 203/1000 | Loss: 0.00002373
Iteration 204/1000 | Loss: 0.00002373
Iteration 205/1000 | Loss: 0.00002373
Iteration 206/1000 | Loss: 0.00002373
Iteration 207/1000 | Loss: 0.00002373
Iteration 208/1000 | Loss: 0.00002373
Iteration 209/1000 | Loss: 0.00002373
Iteration 210/1000 | Loss: 0.00002373
Iteration 211/1000 | Loss: 0.00002373
Iteration 212/1000 | Loss: 0.00002373
Iteration 213/1000 | Loss: 0.00002373
Iteration 214/1000 | Loss: 0.00002373
Iteration 215/1000 | Loss: 0.00002373
Iteration 216/1000 | Loss: 0.00002373
Iteration 217/1000 | Loss: 0.00002373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.3729746317258105e-05, 2.3729746317258105e-05, 2.3729746317258105e-05, 2.3729746317258105e-05, 2.3729746317258105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3729746317258105e-05

Optimization complete. Final v2v error: 4.119523048400879 mm

Highest mean error: 6.260466575622559 mm for frame 86

Lowest mean error: 2.993027687072754 mm for frame 1

Saving results

Total time: 46.06223940849304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032607
Iteration 2/25 | Loss: 0.00111088
Iteration 3/25 | Loss: 0.00086548
Iteration 4/25 | Loss: 0.00084239
Iteration 5/25 | Loss: 0.00083819
Iteration 6/25 | Loss: 0.00083714
Iteration 7/25 | Loss: 0.00083705
Iteration 8/25 | Loss: 0.00083705
Iteration 9/25 | Loss: 0.00083705
Iteration 10/25 | Loss: 0.00083705
Iteration 11/25 | Loss: 0.00083705
Iteration 12/25 | Loss: 0.00083705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008370494470000267, 0.0008370494470000267, 0.0008370494470000267, 0.0008370494470000267, 0.0008370494470000267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008370494470000267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.69733882
Iteration 2/25 | Loss: 0.00045207
Iteration 3/25 | Loss: 0.00045206
Iteration 4/25 | Loss: 0.00045206
Iteration 5/25 | Loss: 0.00045206
Iteration 6/25 | Loss: 0.00045206
Iteration 7/25 | Loss: 0.00045206
Iteration 8/25 | Loss: 0.00045206
Iteration 9/25 | Loss: 0.00045206
Iteration 10/25 | Loss: 0.00045206
Iteration 11/25 | Loss: 0.00045206
Iteration 12/25 | Loss: 0.00045206
Iteration 13/25 | Loss: 0.00045206
Iteration 14/25 | Loss: 0.00045206
Iteration 15/25 | Loss: 0.00045206
Iteration 16/25 | Loss: 0.00045206
Iteration 17/25 | Loss: 0.00045206
Iteration 18/25 | Loss: 0.00045206
Iteration 19/25 | Loss: 0.00045206
Iteration 20/25 | Loss: 0.00045206
Iteration 21/25 | Loss: 0.00045206
Iteration 22/25 | Loss: 0.00045206
Iteration 23/25 | Loss: 0.00045206
Iteration 24/25 | Loss: 0.00045206
Iteration 25/25 | Loss: 0.00045206
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0004520603106357157, 0.0004520603106357157, 0.0004520603106357157, 0.0004520603106357157, 0.0004520603106357157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004520603106357157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045206
Iteration 2/1000 | Loss: 0.00003458
Iteration 3/1000 | Loss: 0.00002452
Iteration 4/1000 | Loss: 0.00002162
Iteration 5/1000 | Loss: 0.00002020
Iteration 6/1000 | Loss: 0.00001955
Iteration 7/1000 | Loss: 0.00001910
Iteration 8/1000 | Loss: 0.00001888
Iteration 9/1000 | Loss: 0.00001887
Iteration 10/1000 | Loss: 0.00001886
Iteration 11/1000 | Loss: 0.00001886
Iteration 12/1000 | Loss: 0.00001886
Iteration 13/1000 | Loss: 0.00001871
Iteration 14/1000 | Loss: 0.00001869
Iteration 15/1000 | Loss: 0.00001864
Iteration 16/1000 | Loss: 0.00001863
Iteration 17/1000 | Loss: 0.00001863
Iteration 18/1000 | Loss: 0.00001862
Iteration 19/1000 | Loss: 0.00001862
Iteration 20/1000 | Loss: 0.00001858
Iteration 21/1000 | Loss: 0.00001858
Iteration 22/1000 | Loss: 0.00001857
Iteration 23/1000 | Loss: 0.00001857
Iteration 24/1000 | Loss: 0.00001857
Iteration 25/1000 | Loss: 0.00001857
Iteration 26/1000 | Loss: 0.00001857
Iteration 27/1000 | Loss: 0.00001857
Iteration 28/1000 | Loss: 0.00001857
Iteration 29/1000 | Loss: 0.00001857
Iteration 30/1000 | Loss: 0.00001857
Iteration 31/1000 | Loss: 0.00001857
Iteration 32/1000 | Loss: 0.00001856
Iteration 33/1000 | Loss: 0.00001856
Iteration 34/1000 | Loss: 0.00001856
Iteration 35/1000 | Loss: 0.00001853
Iteration 36/1000 | Loss: 0.00001853
Iteration 37/1000 | Loss: 0.00001853
Iteration 38/1000 | Loss: 0.00001853
Iteration 39/1000 | Loss: 0.00001852
Iteration 40/1000 | Loss: 0.00001852
Iteration 41/1000 | Loss: 0.00001852
Iteration 42/1000 | Loss: 0.00001851
Iteration 43/1000 | Loss: 0.00001851
Iteration 44/1000 | Loss: 0.00001850
Iteration 45/1000 | Loss: 0.00001850
Iteration 46/1000 | Loss: 0.00001850
Iteration 47/1000 | Loss: 0.00001850
Iteration 48/1000 | Loss: 0.00001849
Iteration 49/1000 | Loss: 0.00001849
Iteration 50/1000 | Loss: 0.00001849
Iteration 51/1000 | Loss: 0.00001848
Iteration 52/1000 | Loss: 0.00001848
Iteration 53/1000 | Loss: 0.00001848
Iteration 54/1000 | Loss: 0.00001847
Iteration 55/1000 | Loss: 0.00001847
Iteration 56/1000 | Loss: 0.00001847
Iteration 57/1000 | Loss: 0.00001847
Iteration 58/1000 | Loss: 0.00001847
Iteration 59/1000 | Loss: 0.00001847
Iteration 60/1000 | Loss: 0.00001846
Iteration 61/1000 | Loss: 0.00001846
Iteration 62/1000 | Loss: 0.00001846
Iteration 63/1000 | Loss: 0.00001846
Iteration 64/1000 | Loss: 0.00001846
Iteration 65/1000 | Loss: 0.00001846
Iteration 66/1000 | Loss: 0.00001845
Iteration 67/1000 | Loss: 0.00001845
Iteration 68/1000 | Loss: 0.00001845
Iteration 69/1000 | Loss: 0.00001845
Iteration 70/1000 | Loss: 0.00001845
Iteration 71/1000 | Loss: 0.00001845
Iteration 72/1000 | Loss: 0.00001845
Iteration 73/1000 | Loss: 0.00001845
Iteration 74/1000 | Loss: 0.00001845
Iteration 75/1000 | Loss: 0.00001845
Iteration 76/1000 | Loss: 0.00001845
Iteration 77/1000 | Loss: 0.00001845
Iteration 78/1000 | Loss: 0.00001845
Iteration 79/1000 | Loss: 0.00001845
Iteration 80/1000 | Loss: 0.00001845
Iteration 81/1000 | Loss: 0.00001845
Iteration 82/1000 | Loss: 0.00001845
Iteration 83/1000 | Loss: 0.00001845
Iteration 84/1000 | Loss: 0.00001845
Iteration 85/1000 | Loss: 0.00001845
Iteration 86/1000 | Loss: 0.00001845
Iteration 87/1000 | Loss: 0.00001845
Iteration 88/1000 | Loss: 0.00001845
Iteration 89/1000 | Loss: 0.00001845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.844675171014387e-05, 1.844675171014387e-05, 1.844675171014387e-05, 1.844675171014387e-05, 1.844675171014387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.844675171014387e-05

Optimization complete. Final v2v error: 3.7135744094848633 mm

Highest mean error: 4.17451810836792 mm for frame 120

Lowest mean error: 3.478308916091919 mm for frame 24

Saving results

Total time: 27.166821718215942
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746581
Iteration 2/25 | Loss: 0.00126509
Iteration 3/25 | Loss: 0.00092968
Iteration 4/25 | Loss: 0.00088354
Iteration 5/25 | Loss: 0.00087431
Iteration 6/25 | Loss: 0.00087223
Iteration 7/25 | Loss: 0.00087186
Iteration 8/25 | Loss: 0.00087186
Iteration 9/25 | Loss: 0.00087186
Iteration 10/25 | Loss: 0.00087186
Iteration 11/25 | Loss: 0.00087186
Iteration 12/25 | Loss: 0.00087186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008718569879420102, 0.0008718569879420102, 0.0008718569879420102, 0.0008718569879420102, 0.0008718569879420102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008718569879420102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38548064
Iteration 2/25 | Loss: 0.00037731
Iteration 3/25 | Loss: 0.00037731
Iteration 4/25 | Loss: 0.00037731
Iteration 5/25 | Loss: 0.00037731
Iteration 6/25 | Loss: 0.00037731
Iteration 7/25 | Loss: 0.00037731
Iteration 8/25 | Loss: 0.00037731
Iteration 9/25 | Loss: 0.00037731
Iteration 10/25 | Loss: 0.00037731
Iteration 11/25 | Loss: 0.00037731
Iteration 12/25 | Loss: 0.00037731
Iteration 13/25 | Loss: 0.00037731
Iteration 14/25 | Loss: 0.00037731
Iteration 15/25 | Loss: 0.00037731
Iteration 16/25 | Loss: 0.00037731
Iteration 17/25 | Loss: 0.00037731
Iteration 18/25 | Loss: 0.00037731
Iteration 19/25 | Loss: 0.00037731
Iteration 20/25 | Loss: 0.00037731
Iteration 21/25 | Loss: 0.00037731
Iteration 22/25 | Loss: 0.00037731
Iteration 23/25 | Loss: 0.00037731
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0003773101489059627, 0.0003773101489059627, 0.0003773101489059627, 0.0003773101489059627, 0.0003773101489059627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003773101489059627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037731
Iteration 2/1000 | Loss: 0.00003368
Iteration 3/1000 | Loss: 0.00002797
Iteration 4/1000 | Loss: 0.00002516
Iteration 5/1000 | Loss: 0.00002389
Iteration 6/1000 | Loss: 0.00002317
Iteration 7/1000 | Loss: 0.00002264
Iteration 8/1000 | Loss: 0.00002240
Iteration 9/1000 | Loss: 0.00002211
Iteration 10/1000 | Loss: 0.00002190
Iteration 11/1000 | Loss: 0.00002190
Iteration 12/1000 | Loss: 0.00002181
Iteration 13/1000 | Loss: 0.00002177
Iteration 14/1000 | Loss: 0.00002176
Iteration 15/1000 | Loss: 0.00002176
Iteration 16/1000 | Loss: 0.00002172
Iteration 17/1000 | Loss: 0.00002170
Iteration 18/1000 | Loss: 0.00002170
Iteration 19/1000 | Loss: 0.00002170
Iteration 20/1000 | Loss: 0.00002168
Iteration 21/1000 | Loss: 0.00002168
Iteration 22/1000 | Loss: 0.00002167
Iteration 23/1000 | Loss: 0.00002167
Iteration 24/1000 | Loss: 0.00002167
Iteration 25/1000 | Loss: 0.00002167
Iteration 26/1000 | Loss: 0.00002167
Iteration 27/1000 | Loss: 0.00002166
Iteration 28/1000 | Loss: 0.00002166
Iteration 29/1000 | Loss: 0.00002166
Iteration 30/1000 | Loss: 0.00002166
Iteration 31/1000 | Loss: 0.00002165
Iteration 32/1000 | Loss: 0.00002165
Iteration 33/1000 | Loss: 0.00002165
Iteration 34/1000 | Loss: 0.00002165
Iteration 35/1000 | Loss: 0.00002165
Iteration 36/1000 | Loss: 0.00002165
Iteration 37/1000 | Loss: 0.00002165
Iteration 38/1000 | Loss: 0.00002165
Iteration 39/1000 | Loss: 0.00002165
Iteration 40/1000 | Loss: 0.00002165
Iteration 41/1000 | Loss: 0.00002165
Iteration 42/1000 | Loss: 0.00002164
Iteration 43/1000 | Loss: 0.00002164
Iteration 44/1000 | Loss: 0.00002164
Iteration 45/1000 | Loss: 0.00002164
Iteration 46/1000 | Loss: 0.00002164
Iteration 47/1000 | Loss: 0.00002164
Iteration 48/1000 | Loss: 0.00002163
Iteration 49/1000 | Loss: 0.00002163
Iteration 50/1000 | Loss: 0.00002163
Iteration 51/1000 | Loss: 0.00002163
Iteration 52/1000 | Loss: 0.00002162
Iteration 53/1000 | Loss: 0.00002162
Iteration 54/1000 | Loss: 0.00002162
Iteration 55/1000 | Loss: 0.00002162
Iteration 56/1000 | Loss: 0.00002162
Iteration 57/1000 | Loss: 0.00002161
Iteration 58/1000 | Loss: 0.00002161
Iteration 59/1000 | Loss: 0.00002161
Iteration 60/1000 | Loss: 0.00002161
Iteration 61/1000 | Loss: 0.00002160
Iteration 62/1000 | Loss: 0.00002160
Iteration 63/1000 | Loss: 0.00002160
Iteration 64/1000 | Loss: 0.00002160
Iteration 65/1000 | Loss: 0.00002160
Iteration 66/1000 | Loss: 0.00002160
Iteration 67/1000 | Loss: 0.00002159
Iteration 68/1000 | Loss: 0.00002159
Iteration 69/1000 | Loss: 0.00002159
Iteration 70/1000 | Loss: 0.00002159
Iteration 71/1000 | Loss: 0.00002159
Iteration 72/1000 | Loss: 0.00002158
Iteration 73/1000 | Loss: 0.00002158
Iteration 74/1000 | Loss: 0.00002158
Iteration 75/1000 | Loss: 0.00002157
Iteration 76/1000 | Loss: 0.00002157
Iteration 77/1000 | Loss: 0.00002157
Iteration 78/1000 | Loss: 0.00002157
Iteration 79/1000 | Loss: 0.00002157
Iteration 80/1000 | Loss: 0.00002157
Iteration 81/1000 | Loss: 0.00002157
Iteration 82/1000 | Loss: 0.00002157
Iteration 83/1000 | Loss: 0.00002157
Iteration 84/1000 | Loss: 0.00002157
Iteration 85/1000 | Loss: 0.00002157
Iteration 86/1000 | Loss: 0.00002157
Iteration 87/1000 | Loss: 0.00002157
Iteration 88/1000 | Loss: 0.00002157
Iteration 89/1000 | Loss: 0.00002157
Iteration 90/1000 | Loss: 0.00002157
Iteration 91/1000 | Loss: 0.00002157
Iteration 92/1000 | Loss: 0.00002157
Iteration 93/1000 | Loss: 0.00002157
Iteration 94/1000 | Loss: 0.00002157
Iteration 95/1000 | Loss: 0.00002157
Iteration 96/1000 | Loss: 0.00002157
Iteration 97/1000 | Loss: 0.00002157
Iteration 98/1000 | Loss: 0.00002157
Iteration 99/1000 | Loss: 0.00002157
Iteration 100/1000 | Loss: 0.00002157
Iteration 101/1000 | Loss: 0.00002157
Iteration 102/1000 | Loss: 0.00002157
Iteration 103/1000 | Loss: 0.00002157
Iteration 104/1000 | Loss: 0.00002157
Iteration 105/1000 | Loss: 0.00002157
Iteration 106/1000 | Loss: 0.00002157
Iteration 107/1000 | Loss: 0.00002157
Iteration 108/1000 | Loss: 0.00002157
Iteration 109/1000 | Loss: 0.00002157
Iteration 110/1000 | Loss: 0.00002157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.1568894226220436e-05, 2.1568894226220436e-05, 2.1568894226220436e-05, 2.1568894226220436e-05, 2.1568894226220436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1568894226220436e-05

Optimization complete. Final v2v error: 4.009524345397949 mm

Highest mean error: 4.216736793518066 mm for frame 2

Lowest mean error: 3.728461742401123 mm for frame 71

Saving results

Total time: 33.947983264923096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090890
Iteration 2/25 | Loss: 0.00149578
Iteration 3/25 | Loss: 0.00110705
Iteration 4/25 | Loss: 0.00104131
Iteration 5/25 | Loss: 0.00103040
Iteration 6/25 | Loss: 0.00102757
Iteration 7/25 | Loss: 0.00102663
Iteration 8/25 | Loss: 0.00102650
Iteration 9/25 | Loss: 0.00102650
Iteration 10/25 | Loss: 0.00102650
Iteration 11/25 | Loss: 0.00102650
Iteration 12/25 | Loss: 0.00102650
Iteration 13/25 | Loss: 0.00102650
Iteration 14/25 | Loss: 0.00102650
Iteration 15/25 | Loss: 0.00102650
Iteration 16/25 | Loss: 0.00102650
Iteration 17/25 | Loss: 0.00102650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001026496640406549, 0.001026496640406549, 0.001026496640406549, 0.001026496640406549, 0.001026496640406549]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001026496640406549

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78374904
Iteration 2/25 | Loss: 0.00041255
Iteration 3/25 | Loss: 0.00041250
Iteration 4/25 | Loss: 0.00041250
Iteration 5/25 | Loss: 0.00041250
Iteration 6/25 | Loss: 0.00041250
Iteration 7/25 | Loss: 0.00041250
Iteration 8/25 | Loss: 0.00041250
Iteration 9/25 | Loss: 0.00041250
Iteration 10/25 | Loss: 0.00041250
Iteration 11/25 | Loss: 0.00041250
Iteration 12/25 | Loss: 0.00041250
Iteration 13/25 | Loss: 0.00041250
Iteration 14/25 | Loss: 0.00041250
Iteration 15/25 | Loss: 0.00041250
Iteration 16/25 | Loss: 0.00041250
Iteration 17/25 | Loss: 0.00041250
Iteration 18/25 | Loss: 0.00041250
Iteration 19/25 | Loss: 0.00041250
Iteration 20/25 | Loss: 0.00041250
Iteration 21/25 | Loss: 0.00041250
Iteration 22/25 | Loss: 0.00041250
Iteration 23/25 | Loss: 0.00041250
Iteration 24/25 | Loss: 0.00041250
Iteration 25/25 | Loss: 0.00041250

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041250
Iteration 2/1000 | Loss: 0.00006317
Iteration 3/1000 | Loss: 0.00004238
Iteration 4/1000 | Loss: 0.00003491
Iteration 5/1000 | Loss: 0.00003245
Iteration 6/1000 | Loss: 0.00003134
Iteration 7/1000 | Loss: 0.00003014
Iteration 8/1000 | Loss: 0.00002926
Iteration 9/1000 | Loss: 0.00002871
Iteration 10/1000 | Loss: 0.00002833
Iteration 11/1000 | Loss: 0.00002806
Iteration 12/1000 | Loss: 0.00002783
Iteration 13/1000 | Loss: 0.00002768
Iteration 14/1000 | Loss: 0.00002760
Iteration 15/1000 | Loss: 0.00002754
Iteration 16/1000 | Loss: 0.00002754
Iteration 17/1000 | Loss: 0.00002752
Iteration 18/1000 | Loss: 0.00002752
Iteration 19/1000 | Loss: 0.00002751
Iteration 20/1000 | Loss: 0.00002751
Iteration 21/1000 | Loss: 0.00002751
Iteration 22/1000 | Loss: 0.00002751
Iteration 23/1000 | Loss: 0.00002750
Iteration 24/1000 | Loss: 0.00002750
Iteration 25/1000 | Loss: 0.00002749
Iteration 26/1000 | Loss: 0.00002749
Iteration 27/1000 | Loss: 0.00002749
Iteration 28/1000 | Loss: 0.00002749
Iteration 29/1000 | Loss: 0.00002749
Iteration 30/1000 | Loss: 0.00002748
Iteration 31/1000 | Loss: 0.00002748
Iteration 32/1000 | Loss: 0.00002748
Iteration 33/1000 | Loss: 0.00002748
Iteration 34/1000 | Loss: 0.00002748
Iteration 35/1000 | Loss: 0.00002748
Iteration 36/1000 | Loss: 0.00002748
Iteration 37/1000 | Loss: 0.00002748
Iteration 38/1000 | Loss: 0.00002748
Iteration 39/1000 | Loss: 0.00002748
Iteration 40/1000 | Loss: 0.00002747
Iteration 41/1000 | Loss: 0.00002747
Iteration 42/1000 | Loss: 0.00002747
Iteration 43/1000 | Loss: 0.00002747
Iteration 44/1000 | Loss: 0.00002747
Iteration 45/1000 | Loss: 0.00002747
Iteration 46/1000 | Loss: 0.00002746
Iteration 47/1000 | Loss: 0.00002746
Iteration 48/1000 | Loss: 0.00002746
Iteration 49/1000 | Loss: 0.00002746
Iteration 50/1000 | Loss: 0.00002746
Iteration 51/1000 | Loss: 0.00002746
Iteration 52/1000 | Loss: 0.00002745
Iteration 53/1000 | Loss: 0.00002745
Iteration 54/1000 | Loss: 0.00002745
Iteration 55/1000 | Loss: 0.00002745
Iteration 56/1000 | Loss: 0.00002745
Iteration 57/1000 | Loss: 0.00002745
Iteration 58/1000 | Loss: 0.00002745
Iteration 59/1000 | Loss: 0.00002744
Iteration 60/1000 | Loss: 0.00002744
Iteration 61/1000 | Loss: 0.00002744
Iteration 62/1000 | Loss: 0.00002744
Iteration 63/1000 | Loss: 0.00002744
Iteration 64/1000 | Loss: 0.00002744
Iteration 65/1000 | Loss: 0.00002744
Iteration 66/1000 | Loss: 0.00002743
Iteration 67/1000 | Loss: 0.00002743
Iteration 68/1000 | Loss: 0.00002743
Iteration 69/1000 | Loss: 0.00002743
Iteration 70/1000 | Loss: 0.00002742
Iteration 71/1000 | Loss: 0.00002742
Iteration 72/1000 | Loss: 0.00002742
Iteration 73/1000 | Loss: 0.00002742
Iteration 74/1000 | Loss: 0.00002741
Iteration 75/1000 | Loss: 0.00002741
Iteration 76/1000 | Loss: 0.00002741
Iteration 77/1000 | Loss: 0.00002741
Iteration 78/1000 | Loss: 0.00002741
Iteration 79/1000 | Loss: 0.00002741
Iteration 80/1000 | Loss: 0.00002741
Iteration 81/1000 | Loss: 0.00002741
Iteration 82/1000 | Loss: 0.00002741
Iteration 83/1000 | Loss: 0.00002741
Iteration 84/1000 | Loss: 0.00002741
Iteration 85/1000 | Loss: 0.00002740
Iteration 86/1000 | Loss: 0.00002740
Iteration 87/1000 | Loss: 0.00002740
Iteration 88/1000 | Loss: 0.00002740
Iteration 89/1000 | Loss: 0.00002740
Iteration 90/1000 | Loss: 0.00002740
Iteration 91/1000 | Loss: 0.00002740
Iteration 92/1000 | Loss: 0.00002740
Iteration 93/1000 | Loss: 0.00002740
Iteration 94/1000 | Loss: 0.00002740
Iteration 95/1000 | Loss: 0.00002740
Iteration 96/1000 | Loss: 0.00002740
Iteration 97/1000 | Loss: 0.00002739
Iteration 98/1000 | Loss: 0.00002739
Iteration 99/1000 | Loss: 0.00002739
Iteration 100/1000 | Loss: 0.00002739
Iteration 101/1000 | Loss: 0.00002739
Iteration 102/1000 | Loss: 0.00002738
Iteration 103/1000 | Loss: 0.00002738
Iteration 104/1000 | Loss: 0.00002738
Iteration 105/1000 | Loss: 0.00002738
Iteration 106/1000 | Loss: 0.00002738
Iteration 107/1000 | Loss: 0.00002738
Iteration 108/1000 | Loss: 0.00002738
Iteration 109/1000 | Loss: 0.00002738
Iteration 110/1000 | Loss: 0.00002737
Iteration 111/1000 | Loss: 0.00002737
Iteration 112/1000 | Loss: 0.00002737
Iteration 113/1000 | Loss: 0.00002736
Iteration 114/1000 | Loss: 0.00002736
Iteration 115/1000 | Loss: 0.00002736
Iteration 116/1000 | Loss: 0.00002735
Iteration 117/1000 | Loss: 0.00002735
Iteration 118/1000 | Loss: 0.00002734
Iteration 119/1000 | Loss: 0.00002734
Iteration 120/1000 | Loss: 0.00002733
Iteration 121/1000 | Loss: 0.00002732
Iteration 122/1000 | Loss: 0.00002732
Iteration 123/1000 | Loss: 0.00002732
Iteration 124/1000 | Loss: 0.00002732
Iteration 125/1000 | Loss: 0.00002732
Iteration 126/1000 | Loss: 0.00002732
Iteration 127/1000 | Loss: 0.00002732
Iteration 128/1000 | Loss: 0.00002732
Iteration 129/1000 | Loss: 0.00002731
Iteration 130/1000 | Loss: 0.00002731
Iteration 131/1000 | Loss: 0.00002731
Iteration 132/1000 | Loss: 0.00002731
Iteration 133/1000 | Loss: 0.00002731
Iteration 134/1000 | Loss: 0.00002730
Iteration 135/1000 | Loss: 0.00002730
Iteration 136/1000 | Loss: 0.00002730
Iteration 137/1000 | Loss: 0.00002730
Iteration 138/1000 | Loss: 0.00002730
Iteration 139/1000 | Loss: 0.00002729
Iteration 140/1000 | Loss: 0.00002729
Iteration 141/1000 | Loss: 0.00002729
Iteration 142/1000 | Loss: 0.00002729
Iteration 143/1000 | Loss: 0.00002729
Iteration 144/1000 | Loss: 0.00002729
Iteration 145/1000 | Loss: 0.00002729
Iteration 146/1000 | Loss: 0.00002729
Iteration 147/1000 | Loss: 0.00002728
Iteration 148/1000 | Loss: 0.00002728
Iteration 149/1000 | Loss: 0.00002728
Iteration 150/1000 | Loss: 0.00002728
Iteration 151/1000 | Loss: 0.00002727
Iteration 152/1000 | Loss: 0.00002727
Iteration 153/1000 | Loss: 0.00002727
Iteration 154/1000 | Loss: 0.00002727
Iteration 155/1000 | Loss: 0.00002727
Iteration 156/1000 | Loss: 0.00002727
Iteration 157/1000 | Loss: 0.00002727
Iteration 158/1000 | Loss: 0.00002726
Iteration 159/1000 | Loss: 0.00002726
Iteration 160/1000 | Loss: 0.00002726
Iteration 161/1000 | Loss: 0.00002726
Iteration 162/1000 | Loss: 0.00002726
Iteration 163/1000 | Loss: 0.00002726
Iteration 164/1000 | Loss: 0.00002726
Iteration 165/1000 | Loss: 0.00002726
Iteration 166/1000 | Loss: 0.00002726
Iteration 167/1000 | Loss: 0.00002725
Iteration 168/1000 | Loss: 0.00002725
Iteration 169/1000 | Loss: 0.00002725
Iteration 170/1000 | Loss: 0.00002725
Iteration 171/1000 | Loss: 0.00002725
Iteration 172/1000 | Loss: 0.00002725
Iteration 173/1000 | Loss: 0.00002725
Iteration 174/1000 | Loss: 0.00002725
Iteration 175/1000 | Loss: 0.00002725
Iteration 176/1000 | Loss: 0.00002725
Iteration 177/1000 | Loss: 0.00002725
Iteration 178/1000 | Loss: 0.00002725
Iteration 179/1000 | Loss: 0.00002725
Iteration 180/1000 | Loss: 0.00002725
Iteration 181/1000 | Loss: 0.00002725
Iteration 182/1000 | Loss: 0.00002725
Iteration 183/1000 | Loss: 0.00002725
Iteration 184/1000 | Loss: 0.00002725
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.7251151550444774e-05, 2.7251151550444774e-05, 2.7251151550444774e-05, 2.7251151550444774e-05, 2.7251151550444774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7251151550444774e-05

Optimization complete. Final v2v error: 4.314951419830322 mm

Highest mean error: 5.664499759674072 mm for frame 0

Lowest mean error: 3.8126018047332764 mm for frame 114

Saving results

Total time: 43.30821132659912
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088521
Iteration 2/25 | Loss: 0.01088521
Iteration 3/25 | Loss: 0.00313130
Iteration 4/25 | Loss: 0.00178604
Iteration 5/25 | Loss: 0.00156602
Iteration 6/25 | Loss: 0.00146352
Iteration 7/25 | Loss: 0.00142568
Iteration 8/25 | Loss: 0.00136982
Iteration 9/25 | Loss: 0.00134867
Iteration 10/25 | Loss: 0.00129731
Iteration 11/25 | Loss: 0.00126969
Iteration 12/25 | Loss: 0.00124270
Iteration 13/25 | Loss: 0.00119241
Iteration 14/25 | Loss: 0.00117183
Iteration 15/25 | Loss: 0.00115050
Iteration 16/25 | Loss: 0.00113651
Iteration 17/25 | Loss: 0.00111667
Iteration 18/25 | Loss: 0.00110540
Iteration 19/25 | Loss: 0.00110107
Iteration 20/25 | Loss: 0.00109921
Iteration 21/25 | Loss: 0.00109764
Iteration 22/25 | Loss: 0.00109458
Iteration 23/25 | Loss: 0.00109968
Iteration 24/25 | Loss: 0.00110476
Iteration 25/25 | Loss: 0.00109386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34485638
Iteration 2/25 | Loss: 0.00345922
Iteration 3/25 | Loss: 0.00235332
Iteration 4/25 | Loss: 0.00235326
Iteration 5/25 | Loss: 0.00235326
Iteration 6/25 | Loss: 0.00235326
Iteration 7/25 | Loss: 0.00235326
Iteration 8/25 | Loss: 0.00235326
Iteration 9/25 | Loss: 0.00235325
Iteration 10/25 | Loss: 0.00235325
Iteration 11/25 | Loss: 0.00235325
Iteration 12/25 | Loss: 0.00235325
Iteration 13/25 | Loss: 0.00235325
Iteration 14/25 | Loss: 0.00235325
Iteration 15/25 | Loss: 0.00235325
Iteration 16/25 | Loss: 0.00235325
Iteration 17/25 | Loss: 0.00235325
Iteration 18/25 | Loss: 0.00235325
Iteration 19/25 | Loss: 0.00235325
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002353253308683634, 0.002353253308683634, 0.002353253308683634, 0.002353253308683634, 0.002353253308683634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002353253308683634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00235325
Iteration 2/1000 | Loss: 0.00206018
Iteration 3/1000 | Loss: 0.00123209
Iteration 4/1000 | Loss: 0.00077573
Iteration 5/1000 | Loss: 0.00044128
Iteration 6/1000 | Loss: 0.00035762
Iteration 7/1000 | Loss: 0.00104124
Iteration 8/1000 | Loss: 0.00120610
Iteration 9/1000 | Loss: 0.00031664
Iteration 10/1000 | Loss: 0.00212306
Iteration 11/1000 | Loss: 0.00044394
Iteration 12/1000 | Loss: 0.00055557
Iteration 13/1000 | Loss: 0.00024215
Iteration 14/1000 | Loss: 0.00162477
Iteration 15/1000 | Loss: 0.00083030
Iteration 16/1000 | Loss: 0.00129461
Iteration 17/1000 | Loss: 0.00067292
Iteration 18/1000 | Loss: 0.00090775
Iteration 19/1000 | Loss: 0.00063218
Iteration 20/1000 | Loss: 0.00030940
Iteration 21/1000 | Loss: 0.00061473
Iteration 22/1000 | Loss: 0.00077933
Iteration 23/1000 | Loss: 0.00074354
Iteration 24/1000 | Loss: 0.00317040
Iteration 25/1000 | Loss: 0.00220274
Iteration 26/1000 | Loss: 0.00169549
Iteration 27/1000 | Loss: 0.00175186
Iteration 28/1000 | Loss: 0.00242166
Iteration 29/1000 | Loss: 0.00324371
Iteration 30/1000 | Loss: 0.00198514
Iteration 31/1000 | Loss: 0.00050623
Iteration 32/1000 | Loss: 0.00085210
Iteration 33/1000 | Loss: 0.00050814
Iteration 34/1000 | Loss: 0.00276646
Iteration 35/1000 | Loss: 0.00351430
Iteration 36/1000 | Loss: 0.00128137
Iteration 37/1000 | Loss: 0.00128534
Iteration 38/1000 | Loss: 0.00257603
Iteration 39/1000 | Loss: 0.00123119
Iteration 40/1000 | Loss: 0.00148980
Iteration 41/1000 | Loss: 0.00116760
Iteration 42/1000 | Loss: 0.00070427
Iteration 43/1000 | Loss: 0.00166211
Iteration 44/1000 | Loss: 0.00064925
Iteration 45/1000 | Loss: 0.00118249
Iteration 46/1000 | Loss: 0.00470130
Iteration 47/1000 | Loss: 0.00419186
Iteration 48/1000 | Loss: 0.00222495
Iteration 49/1000 | Loss: 0.00160347
Iteration 50/1000 | Loss: 0.00104436
Iteration 51/1000 | Loss: 0.00050533
Iteration 52/1000 | Loss: 0.00198136
Iteration 53/1000 | Loss: 0.00155927
Iteration 54/1000 | Loss: 0.00125910
Iteration 55/1000 | Loss: 0.00066895
Iteration 56/1000 | Loss: 0.00275389
Iteration 57/1000 | Loss: 0.00065149
Iteration 58/1000 | Loss: 0.00089488
Iteration 59/1000 | Loss: 0.00038060
Iteration 60/1000 | Loss: 0.00054812
Iteration 61/1000 | Loss: 0.00041045
Iteration 62/1000 | Loss: 0.00035099
Iteration 63/1000 | Loss: 0.00049025
Iteration 64/1000 | Loss: 0.00350974
Iteration 65/1000 | Loss: 0.00149754
Iteration 66/1000 | Loss: 0.00154102
Iteration 67/1000 | Loss: 0.00050594
Iteration 68/1000 | Loss: 0.00336122
Iteration 69/1000 | Loss: 0.00326723
Iteration 70/1000 | Loss: 0.00255746
Iteration 71/1000 | Loss: 0.00314788
Iteration 72/1000 | Loss: 0.00308142
Iteration 73/1000 | Loss: 0.00085033
Iteration 74/1000 | Loss: 0.00078000
Iteration 75/1000 | Loss: 0.00274234
Iteration 76/1000 | Loss: 0.00077606
Iteration 77/1000 | Loss: 0.00029605
Iteration 78/1000 | Loss: 0.00263768
Iteration 79/1000 | Loss: 0.00053648
Iteration 80/1000 | Loss: 0.00082067
Iteration 81/1000 | Loss: 0.00077829
Iteration 82/1000 | Loss: 0.00095944
Iteration 83/1000 | Loss: 0.00147358
Iteration 84/1000 | Loss: 0.00175617
Iteration 85/1000 | Loss: 0.00294004
Iteration 86/1000 | Loss: 0.00357848
Iteration 87/1000 | Loss: 0.00411837
Iteration 88/1000 | Loss: 0.00438621
Iteration 89/1000 | Loss: 0.00381966
Iteration 90/1000 | Loss: 0.00564505
Iteration 91/1000 | Loss: 0.00311543
Iteration 92/1000 | Loss: 0.00409729
Iteration 93/1000 | Loss: 0.00507013
Iteration 94/1000 | Loss: 0.00321399
Iteration 95/1000 | Loss: 0.00097314
Iteration 96/1000 | Loss: 0.00231419
Iteration 97/1000 | Loss: 0.00208577
Iteration 98/1000 | Loss: 0.00391247
Iteration 99/1000 | Loss: 0.00111869
Iteration 100/1000 | Loss: 0.00173332
Iteration 101/1000 | Loss: 0.00518385
Iteration 102/1000 | Loss: 0.00083381
Iteration 103/1000 | Loss: 0.00066018
Iteration 104/1000 | Loss: 0.00091765
Iteration 105/1000 | Loss: 0.00145233
Iteration 106/1000 | Loss: 0.00101080
Iteration 107/1000 | Loss: 0.00036242
Iteration 108/1000 | Loss: 0.00051018
Iteration 109/1000 | Loss: 0.00083498
Iteration 110/1000 | Loss: 0.00042999
Iteration 111/1000 | Loss: 0.00032394
Iteration 112/1000 | Loss: 0.00012389
Iteration 113/1000 | Loss: 0.00026673
Iteration 114/1000 | Loss: 0.00028131
Iteration 115/1000 | Loss: 0.00131513
Iteration 116/1000 | Loss: 0.00058952
Iteration 117/1000 | Loss: 0.00097541
Iteration 118/1000 | Loss: 0.00076338
Iteration 119/1000 | Loss: 0.00145773
Iteration 120/1000 | Loss: 0.00080201
Iteration 121/1000 | Loss: 0.00087077
Iteration 122/1000 | Loss: 0.00093292
Iteration 123/1000 | Loss: 0.00077845
Iteration 124/1000 | Loss: 0.00065167
Iteration 125/1000 | Loss: 0.00029420
Iteration 126/1000 | Loss: 0.00024464
Iteration 127/1000 | Loss: 0.00026840
Iteration 128/1000 | Loss: 0.00014818
Iteration 129/1000 | Loss: 0.00006635
Iteration 130/1000 | Loss: 0.00006081
Iteration 131/1000 | Loss: 0.00063483
Iteration 132/1000 | Loss: 0.00005839
Iteration 133/1000 | Loss: 0.00009750
Iteration 134/1000 | Loss: 0.00008305
Iteration 135/1000 | Loss: 0.00005088
Iteration 136/1000 | Loss: 0.00213997
Iteration 137/1000 | Loss: 0.00059476
Iteration 138/1000 | Loss: 0.00052030
Iteration 139/1000 | Loss: 0.00260225
Iteration 140/1000 | Loss: 0.00081736
Iteration 141/1000 | Loss: 0.00061085
Iteration 142/1000 | Loss: 0.00007688
Iteration 143/1000 | Loss: 0.00005450
Iteration 144/1000 | Loss: 0.00029267
Iteration 145/1000 | Loss: 0.00039845
Iteration 146/1000 | Loss: 0.00028066
Iteration 147/1000 | Loss: 0.00264537
Iteration 148/1000 | Loss: 0.00174897
Iteration 149/1000 | Loss: 0.00312726
Iteration 150/1000 | Loss: 0.00219216
Iteration 151/1000 | Loss: 0.00029919
Iteration 152/1000 | Loss: 0.00129316
Iteration 153/1000 | Loss: 0.00068646
Iteration 154/1000 | Loss: 0.00007788
Iteration 155/1000 | Loss: 0.00088670
Iteration 156/1000 | Loss: 0.00027076
Iteration 157/1000 | Loss: 0.00027124
Iteration 158/1000 | Loss: 0.00017071
Iteration 159/1000 | Loss: 0.00155357
Iteration 160/1000 | Loss: 0.00227553
Iteration 161/1000 | Loss: 0.00058848
Iteration 162/1000 | Loss: 0.00099805
Iteration 163/1000 | Loss: 0.00268028
Iteration 164/1000 | Loss: 0.00084649
Iteration 165/1000 | Loss: 0.00043048
Iteration 166/1000 | Loss: 0.00057353
Iteration 167/1000 | Loss: 0.00051010
Iteration 168/1000 | Loss: 0.00036430
Iteration 169/1000 | Loss: 0.00026959
Iteration 170/1000 | Loss: 0.00030268
Iteration 171/1000 | Loss: 0.00025713
Iteration 172/1000 | Loss: 0.00056872
Iteration 173/1000 | Loss: 0.00014850
Iteration 174/1000 | Loss: 0.00024128
Iteration 175/1000 | Loss: 0.00022621
Iteration 176/1000 | Loss: 0.00107333
Iteration 177/1000 | Loss: 0.00058109
Iteration 178/1000 | Loss: 0.00029517
Iteration 179/1000 | Loss: 0.00006013
Iteration 180/1000 | Loss: 0.00004704
Iteration 181/1000 | Loss: 0.00004359
Iteration 182/1000 | Loss: 0.00004156
Iteration 183/1000 | Loss: 0.00003934
Iteration 184/1000 | Loss: 0.00003814
Iteration 185/1000 | Loss: 0.00003737
Iteration 186/1000 | Loss: 0.00003684
Iteration 187/1000 | Loss: 0.00003633
Iteration 188/1000 | Loss: 0.00003596
Iteration 189/1000 | Loss: 0.00003554
Iteration 190/1000 | Loss: 0.00003543
Iteration 191/1000 | Loss: 0.00003533
Iteration 192/1000 | Loss: 0.00003528
Iteration 193/1000 | Loss: 0.00003525
Iteration 194/1000 | Loss: 0.00003520
Iteration 195/1000 | Loss: 0.00003518
Iteration 196/1000 | Loss: 0.00003500
Iteration 197/1000 | Loss: 0.00003498
Iteration 198/1000 | Loss: 0.00003480
Iteration 199/1000 | Loss: 0.00003479
Iteration 200/1000 | Loss: 0.00003472
Iteration 201/1000 | Loss: 0.00003456
Iteration 202/1000 | Loss: 0.00003448
Iteration 203/1000 | Loss: 0.00003442
Iteration 204/1000 | Loss: 0.00003437
Iteration 205/1000 | Loss: 0.00003436
Iteration 206/1000 | Loss: 0.00003436
Iteration 207/1000 | Loss: 0.00003434
Iteration 208/1000 | Loss: 0.00003434
Iteration 209/1000 | Loss: 0.00003431
Iteration 210/1000 | Loss: 0.00003426
Iteration 211/1000 | Loss: 0.00003408
Iteration 212/1000 | Loss: 0.00003401
Iteration 213/1000 | Loss: 0.00003395
Iteration 214/1000 | Loss: 0.00003393
Iteration 215/1000 | Loss: 0.00003392
Iteration 216/1000 | Loss: 0.00003391
Iteration 217/1000 | Loss: 0.00003388
Iteration 218/1000 | Loss: 0.00003388
Iteration 219/1000 | Loss: 0.00003387
Iteration 220/1000 | Loss: 0.00003387
Iteration 221/1000 | Loss: 0.00003386
Iteration 222/1000 | Loss: 0.00003386
Iteration 223/1000 | Loss: 0.00003385
Iteration 224/1000 | Loss: 0.00003385
Iteration 225/1000 | Loss: 0.00003384
Iteration 226/1000 | Loss: 0.00003384
Iteration 227/1000 | Loss: 0.00003384
Iteration 228/1000 | Loss: 0.00003383
Iteration 229/1000 | Loss: 0.00003383
Iteration 230/1000 | Loss: 0.00003383
Iteration 231/1000 | Loss: 0.00003383
Iteration 232/1000 | Loss: 0.00003382
Iteration 233/1000 | Loss: 0.00003382
Iteration 234/1000 | Loss: 0.00003382
Iteration 235/1000 | Loss: 0.00003382
Iteration 236/1000 | Loss: 0.00003381
Iteration 237/1000 | Loss: 0.00003381
Iteration 238/1000 | Loss: 0.00003381
Iteration 239/1000 | Loss: 0.00003380
Iteration 240/1000 | Loss: 0.00003380
Iteration 241/1000 | Loss: 0.00003380
Iteration 242/1000 | Loss: 0.00003380
Iteration 243/1000 | Loss: 0.00003379
Iteration 244/1000 | Loss: 0.00003379
Iteration 245/1000 | Loss: 0.00003379
Iteration 246/1000 | Loss: 0.00003379
Iteration 247/1000 | Loss: 0.00003378
Iteration 248/1000 | Loss: 0.00003378
Iteration 249/1000 | Loss: 0.00003378
Iteration 250/1000 | Loss: 0.00003378
Iteration 251/1000 | Loss: 0.00003378
Iteration 252/1000 | Loss: 0.00003378
Iteration 253/1000 | Loss: 0.00003378
Iteration 254/1000 | Loss: 0.00003378
Iteration 255/1000 | Loss: 0.00003378
Iteration 256/1000 | Loss: 0.00003377
Iteration 257/1000 | Loss: 0.00003377
Iteration 258/1000 | Loss: 0.00003377
Iteration 259/1000 | Loss: 0.00003377
Iteration 260/1000 | Loss: 0.00003377
Iteration 261/1000 | Loss: 0.00003377
Iteration 262/1000 | Loss: 0.00003377
Iteration 263/1000 | Loss: 0.00003377
Iteration 264/1000 | Loss: 0.00003377
Iteration 265/1000 | Loss: 0.00003377
Iteration 266/1000 | Loss: 0.00003377
Iteration 267/1000 | Loss: 0.00003377
Iteration 268/1000 | Loss: 0.00003376
Iteration 269/1000 | Loss: 0.00003376
Iteration 270/1000 | Loss: 0.00003376
Iteration 271/1000 | Loss: 0.00003376
Iteration 272/1000 | Loss: 0.00003376
Iteration 273/1000 | Loss: 0.00003376
Iteration 274/1000 | Loss: 0.00003376
Iteration 275/1000 | Loss: 0.00003376
Iteration 276/1000 | Loss: 0.00003376
Iteration 277/1000 | Loss: 0.00003376
Iteration 278/1000 | Loss: 0.00003376
Iteration 279/1000 | Loss: 0.00003376
Iteration 280/1000 | Loss: 0.00003376
Iteration 281/1000 | Loss: 0.00003376
Iteration 282/1000 | Loss: 0.00003376
Iteration 283/1000 | Loss: 0.00003376
Iteration 284/1000 | Loss: 0.00003376
Iteration 285/1000 | Loss: 0.00003376
Iteration 286/1000 | Loss: 0.00003376
Iteration 287/1000 | Loss: 0.00003375
Iteration 288/1000 | Loss: 0.00003375
Iteration 289/1000 | Loss: 0.00003375
Iteration 290/1000 | Loss: 0.00003375
Iteration 291/1000 | Loss: 0.00003375
Iteration 292/1000 | Loss: 0.00003375
Iteration 293/1000 | Loss: 0.00003375
Iteration 294/1000 | Loss: 0.00003375
Iteration 295/1000 | Loss: 0.00003375
Iteration 296/1000 | Loss: 0.00003375
Iteration 297/1000 | Loss: 0.00003375
Iteration 298/1000 | Loss: 0.00003375
Iteration 299/1000 | Loss: 0.00003375
Iteration 300/1000 | Loss: 0.00003375
Iteration 301/1000 | Loss: 0.00003375
Iteration 302/1000 | Loss: 0.00003375
Iteration 303/1000 | Loss: 0.00003375
Iteration 304/1000 | Loss: 0.00003375
Iteration 305/1000 | Loss: 0.00003374
Iteration 306/1000 | Loss: 0.00003374
Iteration 307/1000 | Loss: 0.00003374
Iteration 308/1000 | Loss: 0.00003374
Iteration 309/1000 | Loss: 0.00003374
Iteration 310/1000 | Loss: 0.00003374
Iteration 311/1000 | Loss: 0.00003374
Iteration 312/1000 | Loss: 0.00003374
Iteration 313/1000 | Loss: 0.00003374
Iteration 314/1000 | Loss: 0.00003374
Iteration 315/1000 | Loss: 0.00003374
Iteration 316/1000 | Loss: 0.00003374
Iteration 317/1000 | Loss: 0.00003374
Iteration 318/1000 | Loss: 0.00003374
Iteration 319/1000 | Loss: 0.00003374
Iteration 320/1000 | Loss: 0.00003374
Iteration 321/1000 | Loss: 0.00003374
Iteration 322/1000 | Loss: 0.00003374
Iteration 323/1000 | Loss: 0.00003374
Iteration 324/1000 | Loss: 0.00003373
Iteration 325/1000 | Loss: 0.00003373
Iteration 326/1000 | Loss: 0.00003373
Iteration 327/1000 | Loss: 0.00003373
Iteration 328/1000 | Loss: 0.00003373
Iteration 329/1000 | Loss: 0.00003373
Iteration 330/1000 | Loss: 0.00003373
Iteration 331/1000 | Loss: 0.00003373
Iteration 332/1000 | Loss: 0.00003373
Iteration 333/1000 | Loss: 0.00003373
Iteration 334/1000 | Loss: 0.00003373
Iteration 335/1000 | Loss: 0.00003373
Iteration 336/1000 | Loss: 0.00003373
Iteration 337/1000 | Loss: 0.00003373
Iteration 338/1000 | Loss: 0.00003373
Iteration 339/1000 | Loss: 0.00003373
Iteration 340/1000 | Loss: 0.00003373
Iteration 341/1000 | Loss: 0.00003373
Iteration 342/1000 | Loss: 0.00003373
Iteration 343/1000 | Loss: 0.00003373
Iteration 344/1000 | Loss: 0.00003373
Iteration 345/1000 | Loss: 0.00003372
Iteration 346/1000 | Loss: 0.00003372
Iteration 347/1000 | Loss: 0.00003372
Iteration 348/1000 | Loss: 0.00003372
Iteration 349/1000 | Loss: 0.00003372
Iteration 350/1000 | Loss: 0.00003372
Iteration 351/1000 | Loss: 0.00003372
Iteration 352/1000 | Loss: 0.00003372
Iteration 353/1000 | Loss: 0.00003372
Iteration 354/1000 | Loss: 0.00003372
Iteration 355/1000 | Loss: 0.00003372
Iteration 356/1000 | Loss: 0.00003372
Iteration 357/1000 | Loss: 0.00003372
Iteration 358/1000 | Loss: 0.00003372
Iteration 359/1000 | Loss: 0.00003372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 359. Stopping optimization.
Last 5 losses: [3.37241253873799e-05, 3.37241253873799e-05, 3.37241253873799e-05, 3.37241253873799e-05, 3.37241253873799e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.37241253873799e-05

Optimization complete. Final v2v error: 4.125514507293701 mm

Highest mean error: 21.508535385131836 mm for frame 131

Lowest mean error: 2.9823830127716064 mm for frame 235

Saving results

Total time: 379.4233560562134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079909
Iteration 2/25 | Loss: 0.00225416
Iteration 3/25 | Loss: 0.00138232
Iteration 4/25 | Loss: 0.00120381
Iteration 5/25 | Loss: 0.00124459
Iteration 6/25 | Loss: 0.00120222
Iteration 7/25 | Loss: 0.00107988
Iteration 8/25 | Loss: 0.00097825
Iteration 9/25 | Loss: 0.00092620
Iteration 10/25 | Loss: 0.00091266
Iteration 11/25 | Loss: 0.00090832
Iteration 12/25 | Loss: 0.00090697
Iteration 13/25 | Loss: 0.00090656
Iteration 14/25 | Loss: 0.00090648
Iteration 15/25 | Loss: 0.00090648
Iteration 16/25 | Loss: 0.00090648
Iteration 17/25 | Loss: 0.00090648
Iteration 18/25 | Loss: 0.00090648
Iteration 19/25 | Loss: 0.00090647
Iteration 20/25 | Loss: 0.00090647
Iteration 21/25 | Loss: 0.00090647
Iteration 22/25 | Loss: 0.00090647
Iteration 23/25 | Loss: 0.00090647
Iteration 24/25 | Loss: 0.00090647
Iteration 25/25 | Loss: 0.00090647

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41078639
Iteration 2/25 | Loss: 0.00042892
Iteration 3/25 | Loss: 0.00042892
Iteration 4/25 | Loss: 0.00042892
Iteration 5/25 | Loss: 0.00042892
Iteration 6/25 | Loss: 0.00042892
Iteration 7/25 | Loss: 0.00042892
Iteration 8/25 | Loss: 0.00042892
Iteration 9/25 | Loss: 0.00042892
Iteration 10/25 | Loss: 0.00042892
Iteration 11/25 | Loss: 0.00042892
Iteration 12/25 | Loss: 0.00042892
Iteration 13/25 | Loss: 0.00042892
Iteration 14/25 | Loss: 0.00042892
Iteration 15/25 | Loss: 0.00042892
Iteration 16/25 | Loss: 0.00042892
Iteration 17/25 | Loss: 0.00042892
Iteration 18/25 | Loss: 0.00042892
Iteration 19/25 | Loss: 0.00042892
Iteration 20/25 | Loss: 0.00042892
Iteration 21/25 | Loss: 0.00042892
Iteration 22/25 | Loss: 0.00042892
Iteration 23/25 | Loss: 0.00042892
Iteration 24/25 | Loss: 0.00042892
Iteration 25/25 | Loss: 0.00042892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042892
Iteration 2/1000 | Loss: 0.00003745
Iteration 3/1000 | Loss: 0.00003137
Iteration 4/1000 | Loss: 0.00002980
Iteration 5/1000 | Loss: 0.00002900
Iteration 6/1000 | Loss: 0.00002844
Iteration 7/1000 | Loss: 0.00002807
Iteration 8/1000 | Loss: 0.00002780
Iteration 9/1000 | Loss: 0.00002763
Iteration 10/1000 | Loss: 0.00002761
Iteration 11/1000 | Loss: 0.00002758
Iteration 12/1000 | Loss: 0.00002758
Iteration 13/1000 | Loss: 0.00002757
Iteration 14/1000 | Loss: 0.00002757
Iteration 15/1000 | Loss: 0.00002757
Iteration 16/1000 | Loss: 0.00002752
Iteration 17/1000 | Loss: 0.00002751
Iteration 18/1000 | Loss: 0.00002748
Iteration 19/1000 | Loss: 0.00002747
Iteration 20/1000 | Loss: 0.00002745
Iteration 21/1000 | Loss: 0.00002745
Iteration 22/1000 | Loss: 0.00002744
Iteration 23/1000 | Loss: 0.00002744
Iteration 24/1000 | Loss: 0.00002744
Iteration 25/1000 | Loss: 0.00002743
Iteration 26/1000 | Loss: 0.00002743
Iteration 27/1000 | Loss: 0.00002743
Iteration 28/1000 | Loss: 0.00002743
Iteration 29/1000 | Loss: 0.00002742
Iteration 30/1000 | Loss: 0.00002742
Iteration 31/1000 | Loss: 0.00002742
Iteration 32/1000 | Loss: 0.00002742
Iteration 33/1000 | Loss: 0.00002742
Iteration 34/1000 | Loss: 0.00002742
Iteration 35/1000 | Loss: 0.00002741
Iteration 36/1000 | Loss: 0.00002741
Iteration 37/1000 | Loss: 0.00002741
Iteration 38/1000 | Loss: 0.00002741
Iteration 39/1000 | Loss: 0.00002740
Iteration 40/1000 | Loss: 0.00002740
Iteration 41/1000 | Loss: 0.00002740
Iteration 42/1000 | Loss: 0.00002740
Iteration 43/1000 | Loss: 0.00002740
Iteration 44/1000 | Loss: 0.00002740
Iteration 45/1000 | Loss: 0.00002740
Iteration 46/1000 | Loss: 0.00002740
Iteration 47/1000 | Loss: 0.00002740
Iteration 48/1000 | Loss: 0.00002740
Iteration 49/1000 | Loss: 0.00002740
Iteration 50/1000 | Loss: 0.00002740
Iteration 51/1000 | Loss: 0.00002740
Iteration 52/1000 | Loss: 0.00002740
Iteration 53/1000 | Loss: 0.00002740
Iteration 54/1000 | Loss: 0.00002740
Iteration 55/1000 | Loss: 0.00002740
Iteration 56/1000 | Loss: 0.00002740
Iteration 57/1000 | Loss: 0.00002740
Iteration 58/1000 | Loss: 0.00002740
Iteration 59/1000 | Loss: 0.00002740
Iteration 60/1000 | Loss: 0.00002740
Iteration 61/1000 | Loss: 0.00002740
Iteration 62/1000 | Loss: 0.00002740
Iteration 63/1000 | Loss: 0.00002740
Iteration 64/1000 | Loss: 0.00002740
Iteration 65/1000 | Loss: 0.00002740
Iteration 66/1000 | Loss: 0.00002740
Iteration 67/1000 | Loss: 0.00002740
Iteration 68/1000 | Loss: 0.00002740
Iteration 69/1000 | Loss: 0.00002740
Iteration 70/1000 | Loss: 0.00002740
Iteration 71/1000 | Loss: 0.00002740
Iteration 72/1000 | Loss: 0.00002740
Iteration 73/1000 | Loss: 0.00002740
Iteration 74/1000 | Loss: 0.00002740
Iteration 75/1000 | Loss: 0.00002740
Iteration 76/1000 | Loss: 0.00002740
Iteration 77/1000 | Loss: 0.00002740
Iteration 78/1000 | Loss: 0.00002740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [2.7396672521717846e-05, 2.7396672521717846e-05, 2.7396672521717846e-05, 2.7396672521717846e-05, 2.7396672521717846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7396672521717846e-05

Optimization complete. Final v2v error: 4.359158039093018 mm

Highest mean error: 4.595880508422852 mm for frame 174

Lowest mean error: 4.226956844329834 mm for frame 227

Saving results

Total time: 46.21827507019043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825753
Iteration 2/25 | Loss: 0.00118092
Iteration 3/25 | Loss: 0.00101209
Iteration 4/25 | Loss: 0.00092636
Iteration 5/25 | Loss: 0.00091526
Iteration 6/25 | Loss: 0.00091192
Iteration 7/25 | Loss: 0.00091070
Iteration 8/25 | Loss: 0.00091019
Iteration 9/25 | Loss: 0.00091010
Iteration 10/25 | Loss: 0.00091010
Iteration 11/25 | Loss: 0.00091010
Iteration 12/25 | Loss: 0.00091010
Iteration 13/25 | Loss: 0.00091010
Iteration 14/25 | Loss: 0.00091010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009101008763536811, 0.0009101008763536811, 0.0009101008763536811, 0.0009101008763536811, 0.0009101008763536811]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009101008763536811

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48286819
Iteration 2/25 | Loss: 0.00043161
Iteration 3/25 | Loss: 0.00043158
Iteration 4/25 | Loss: 0.00043158
Iteration 5/25 | Loss: 0.00043158
Iteration 6/25 | Loss: 0.00043158
Iteration 7/25 | Loss: 0.00043158
Iteration 8/25 | Loss: 0.00043158
Iteration 9/25 | Loss: 0.00043158
Iteration 10/25 | Loss: 0.00043158
Iteration 11/25 | Loss: 0.00043158
Iteration 12/25 | Loss: 0.00043158
Iteration 13/25 | Loss: 0.00043158
Iteration 14/25 | Loss: 0.00043158
Iteration 15/25 | Loss: 0.00043158
Iteration 16/25 | Loss: 0.00043158
Iteration 17/25 | Loss: 0.00043158
Iteration 18/25 | Loss: 0.00043158
Iteration 19/25 | Loss: 0.00043158
Iteration 20/25 | Loss: 0.00043158
Iteration 21/25 | Loss: 0.00043158
Iteration 22/25 | Loss: 0.00043158
Iteration 23/25 | Loss: 0.00043158
Iteration 24/25 | Loss: 0.00043158
Iteration 25/25 | Loss: 0.00043158

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043158
Iteration 2/1000 | Loss: 0.00004727
Iteration 3/1000 | Loss: 0.00002443
Iteration 4/1000 | Loss: 0.00001971
Iteration 5/1000 | Loss: 0.00001801
Iteration 6/1000 | Loss: 0.00001680
Iteration 7/1000 | Loss: 0.00001602
Iteration 8/1000 | Loss: 0.00001564
Iteration 9/1000 | Loss: 0.00001536
Iteration 10/1000 | Loss: 0.00001516
Iteration 11/1000 | Loss: 0.00001503
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001489
Iteration 14/1000 | Loss: 0.00001488
Iteration 15/1000 | Loss: 0.00001487
Iteration 16/1000 | Loss: 0.00001484
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001483
Iteration 19/1000 | Loss: 0.00001483
Iteration 20/1000 | Loss: 0.00001483
Iteration 21/1000 | Loss: 0.00001482
Iteration 22/1000 | Loss: 0.00001482
Iteration 23/1000 | Loss: 0.00001482
Iteration 24/1000 | Loss: 0.00001482
Iteration 25/1000 | Loss: 0.00001481
Iteration 26/1000 | Loss: 0.00001481
Iteration 27/1000 | Loss: 0.00001481
Iteration 28/1000 | Loss: 0.00001480
Iteration 29/1000 | Loss: 0.00001480
Iteration 30/1000 | Loss: 0.00001479
Iteration 31/1000 | Loss: 0.00001479
Iteration 32/1000 | Loss: 0.00001479
Iteration 33/1000 | Loss: 0.00001479
Iteration 34/1000 | Loss: 0.00001479
Iteration 35/1000 | Loss: 0.00001479
Iteration 36/1000 | Loss: 0.00001478
Iteration 37/1000 | Loss: 0.00001478
Iteration 38/1000 | Loss: 0.00001478
Iteration 39/1000 | Loss: 0.00001478
Iteration 40/1000 | Loss: 0.00001477
Iteration 41/1000 | Loss: 0.00001476
Iteration 42/1000 | Loss: 0.00001475
Iteration 43/1000 | Loss: 0.00001475
Iteration 44/1000 | Loss: 0.00001474
Iteration 45/1000 | Loss: 0.00001474
Iteration 46/1000 | Loss: 0.00001474
Iteration 47/1000 | Loss: 0.00001474
Iteration 48/1000 | Loss: 0.00001474
Iteration 49/1000 | Loss: 0.00001472
Iteration 50/1000 | Loss: 0.00001472
Iteration 51/1000 | Loss: 0.00001471
Iteration 52/1000 | Loss: 0.00001471
Iteration 53/1000 | Loss: 0.00001471
Iteration 54/1000 | Loss: 0.00001471
Iteration 55/1000 | Loss: 0.00001471
Iteration 56/1000 | Loss: 0.00001471
Iteration 57/1000 | Loss: 0.00001471
Iteration 58/1000 | Loss: 0.00001471
Iteration 59/1000 | Loss: 0.00001471
Iteration 60/1000 | Loss: 0.00001470
Iteration 61/1000 | Loss: 0.00001470
Iteration 62/1000 | Loss: 0.00001470
Iteration 63/1000 | Loss: 0.00001469
Iteration 64/1000 | Loss: 0.00001469
Iteration 65/1000 | Loss: 0.00001469
Iteration 66/1000 | Loss: 0.00001469
Iteration 67/1000 | Loss: 0.00001469
Iteration 68/1000 | Loss: 0.00001469
Iteration 69/1000 | Loss: 0.00001469
Iteration 70/1000 | Loss: 0.00001469
Iteration 71/1000 | Loss: 0.00001468
Iteration 72/1000 | Loss: 0.00001468
Iteration 73/1000 | Loss: 0.00001468
Iteration 74/1000 | Loss: 0.00001468
Iteration 75/1000 | Loss: 0.00001468
Iteration 76/1000 | Loss: 0.00001468
Iteration 77/1000 | Loss: 0.00001467
Iteration 78/1000 | Loss: 0.00001466
Iteration 79/1000 | Loss: 0.00001466
Iteration 80/1000 | Loss: 0.00001466
Iteration 81/1000 | Loss: 0.00001465
Iteration 82/1000 | Loss: 0.00001465
Iteration 83/1000 | Loss: 0.00001465
Iteration 84/1000 | Loss: 0.00001465
Iteration 85/1000 | Loss: 0.00001464
Iteration 86/1000 | Loss: 0.00001464
Iteration 87/1000 | Loss: 0.00001464
Iteration 88/1000 | Loss: 0.00001464
Iteration 89/1000 | Loss: 0.00001463
Iteration 90/1000 | Loss: 0.00001463
Iteration 91/1000 | Loss: 0.00001463
Iteration 92/1000 | Loss: 0.00001463
Iteration 93/1000 | Loss: 0.00001462
Iteration 94/1000 | Loss: 0.00001462
Iteration 95/1000 | Loss: 0.00001462
Iteration 96/1000 | Loss: 0.00001462
Iteration 97/1000 | Loss: 0.00001462
Iteration 98/1000 | Loss: 0.00001462
Iteration 99/1000 | Loss: 0.00001462
Iteration 100/1000 | Loss: 0.00001462
Iteration 101/1000 | Loss: 0.00001462
Iteration 102/1000 | Loss: 0.00001462
Iteration 103/1000 | Loss: 0.00001462
Iteration 104/1000 | Loss: 0.00001462
Iteration 105/1000 | Loss: 0.00001462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.4623376046074554e-05, 1.4623376046074554e-05, 1.4623376046074554e-05, 1.4623376046074554e-05, 1.4623376046074554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4623376046074554e-05

Optimization complete. Final v2v error: 3.2419495582580566 mm

Highest mean error: 3.687516212463379 mm for frame 2

Lowest mean error: 2.812251567840576 mm for frame 142

Saving results

Total time: 38.03423261642456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857873
Iteration 2/25 | Loss: 0.00121510
Iteration 3/25 | Loss: 0.00096237
Iteration 4/25 | Loss: 0.00094230
Iteration 5/25 | Loss: 0.00093776
Iteration 6/25 | Loss: 0.00093666
Iteration 7/25 | Loss: 0.00093666
Iteration 8/25 | Loss: 0.00093666
Iteration 9/25 | Loss: 0.00093666
Iteration 10/25 | Loss: 0.00093666
Iteration 11/25 | Loss: 0.00093666
Iteration 12/25 | Loss: 0.00093666
Iteration 13/25 | Loss: 0.00093666
Iteration 14/25 | Loss: 0.00093666
Iteration 15/25 | Loss: 0.00093666
Iteration 16/25 | Loss: 0.00093666
Iteration 17/25 | Loss: 0.00093666
Iteration 18/25 | Loss: 0.00093666
Iteration 19/25 | Loss: 0.00093666
Iteration 20/25 | Loss: 0.00093666
Iteration 21/25 | Loss: 0.00093666
Iteration 22/25 | Loss: 0.00093666
Iteration 23/25 | Loss: 0.00093666
Iteration 24/25 | Loss: 0.00093666
Iteration 25/25 | Loss: 0.00093666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.04756737
Iteration 2/25 | Loss: 0.00037277
Iteration 3/25 | Loss: 0.00037277
Iteration 4/25 | Loss: 0.00037277
Iteration 5/25 | Loss: 0.00037277
Iteration 6/25 | Loss: 0.00037277
Iteration 7/25 | Loss: 0.00037277
Iteration 8/25 | Loss: 0.00037277
Iteration 9/25 | Loss: 0.00037277
Iteration 10/25 | Loss: 0.00037277
Iteration 11/25 | Loss: 0.00037277
Iteration 12/25 | Loss: 0.00037277
Iteration 13/25 | Loss: 0.00037277
Iteration 14/25 | Loss: 0.00037277
Iteration 15/25 | Loss: 0.00037277
Iteration 16/25 | Loss: 0.00037277
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00037276590592227876, 0.00037276590592227876, 0.00037276590592227876, 0.00037276590592227876, 0.00037276590592227876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037276590592227876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037277
Iteration 2/1000 | Loss: 0.00005271
Iteration 3/1000 | Loss: 0.00003199
Iteration 4/1000 | Loss: 0.00002840
Iteration 5/1000 | Loss: 0.00002645
Iteration 6/1000 | Loss: 0.00002509
Iteration 7/1000 | Loss: 0.00002435
Iteration 8/1000 | Loss: 0.00002381
Iteration 9/1000 | Loss: 0.00002347
Iteration 10/1000 | Loss: 0.00002320
Iteration 11/1000 | Loss: 0.00002292
Iteration 12/1000 | Loss: 0.00002272
Iteration 13/1000 | Loss: 0.00002269
Iteration 14/1000 | Loss: 0.00002254
Iteration 15/1000 | Loss: 0.00002250
Iteration 16/1000 | Loss: 0.00002244
Iteration 17/1000 | Loss: 0.00002243
Iteration 18/1000 | Loss: 0.00002242
Iteration 19/1000 | Loss: 0.00002242
Iteration 20/1000 | Loss: 0.00002241
Iteration 21/1000 | Loss: 0.00002241
Iteration 22/1000 | Loss: 0.00002239
Iteration 23/1000 | Loss: 0.00002239
Iteration 24/1000 | Loss: 0.00002238
Iteration 25/1000 | Loss: 0.00002237
Iteration 26/1000 | Loss: 0.00002235
Iteration 27/1000 | Loss: 0.00002235
Iteration 28/1000 | Loss: 0.00002235
Iteration 29/1000 | Loss: 0.00002234
Iteration 30/1000 | Loss: 0.00002233
Iteration 31/1000 | Loss: 0.00002232
Iteration 32/1000 | Loss: 0.00002232
Iteration 33/1000 | Loss: 0.00002231
Iteration 34/1000 | Loss: 0.00002231
Iteration 35/1000 | Loss: 0.00002231
Iteration 36/1000 | Loss: 0.00002230
Iteration 37/1000 | Loss: 0.00002230
Iteration 38/1000 | Loss: 0.00002230
Iteration 39/1000 | Loss: 0.00002230
Iteration 40/1000 | Loss: 0.00002230
Iteration 41/1000 | Loss: 0.00002230
Iteration 42/1000 | Loss: 0.00002230
Iteration 43/1000 | Loss: 0.00002229
Iteration 44/1000 | Loss: 0.00002229
Iteration 45/1000 | Loss: 0.00002229
Iteration 46/1000 | Loss: 0.00002229
Iteration 47/1000 | Loss: 0.00002228
Iteration 48/1000 | Loss: 0.00002228
Iteration 49/1000 | Loss: 0.00002228
Iteration 50/1000 | Loss: 0.00002228
Iteration 51/1000 | Loss: 0.00002228
Iteration 52/1000 | Loss: 0.00002227
Iteration 53/1000 | Loss: 0.00002227
Iteration 54/1000 | Loss: 0.00002227
Iteration 55/1000 | Loss: 0.00002227
Iteration 56/1000 | Loss: 0.00002227
Iteration 57/1000 | Loss: 0.00002227
Iteration 58/1000 | Loss: 0.00002227
Iteration 59/1000 | Loss: 0.00002226
Iteration 60/1000 | Loss: 0.00002226
Iteration 61/1000 | Loss: 0.00002226
Iteration 62/1000 | Loss: 0.00002226
Iteration 63/1000 | Loss: 0.00002226
Iteration 64/1000 | Loss: 0.00002225
Iteration 65/1000 | Loss: 0.00002225
Iteration 66/1000 | Loss: 0.00002225
Iteration 67/1000 | Loss: 0.00002224
Iteration 68/1000 | Loss: 0.00002224
Iteration 69/1000 | Loss: 0.00002224
Iteration 70/1000 | Loss: 0.00002224
Iteration 71/1000 | Loss: 0.00002223
Iteration 72/1000 | Loss: 0.00002223
Iteration 73/1000 | Loss: 0.00002223
Iteration 74/1000 | Loss: 0.00002223
Iteration 75/1000 | Loss: 0.00002223
Iteration 76/1000 | Loss: 0.00002223
Iteration 77/1000 | Loss: 0.00002223
Iteration 78/1000 | Loss: 0.00002223
Iteration 79/1000 | Loss: 0.00002223
Iteration 80/1000 | Loss: 0.00002223
Iteration 81/1000 | Loss: 0.00002223
Iteration 82/1000 | Loss: 0.00002223
Iteration 83/1000 | Loss: 0.00002223
Iteration 84/1000 | Loss: 0.00002223
Iteration 85/1000 | Loss: 0.00002223
Iteration 86/1000 | Loss: 0.00002223
Iteration 87/1000 | Loss: 0.00002223
Iteration 88/1000 | Loss: 0.00002223
Iteration 89/1000 | Loss: 0.00002223
Iteration 90/1000 | Loss: 0.00002223
Iteration 91/1000 | Loss: 0.00002223
Iteration 92/1000 | Loss: 0.00002223
Iteration 93/1000 | Loss: 0.00002223
Iteration 94/1000 | Loss: 0.00002223
Iteration 95/1000 | Loss: 0.00002223
Iteration 96/1000 | Loss: 0.00002223
Iteration 97/1000 | Loss: 0.00002223
Iteration 98/1000 | Loss: 0.00002223
Iteration 99/1000 | Loss: 0.00002223
Iteration 100/1000 | Loss: 0.00002223
Iteration 101/1000 | Loss: 0.00002223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.222823786723893e-05, 2.222823786723893e-05, 2.222823786723893e-05, 2.222823786723893e-05, 2.222823786723893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.222823786723893e-05

Optimization complete. Final v2v error: 4.058069229125977 mm

Highest mean error: 4.612087249755859 mm for frame 2

Lowest mean error: 3.7157275676727295 mm for frame 173

Saving results

Total time: 35.30842208862305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01141484
Iteration 2/25 | Loss: 0.00191414
Iteration 3/25 | Loss: 0.00135117
Iteration 4/25 | Loss: 0.00123923
Iteration 5/25 | Loss: 0.00121320
Iteration 6/25 | Loss: 0.00117916
Iteration 7/25 | Loss: 0.00114028
Iteration 8/25 | Loss: 0.00116052
Iteration 9/25 | Loss: 0.00115075
Iteration 10/25 | Loss: 0.00109365
Iteration 11/25 | Loss: 0.00106220
Iteration 12/25 | Loss: 0.00104973
Iteration 13/25 | Loss: 0.00105146
Iteration 14/25 | Loss: 0.00105424
Iteration 15/25 | Loss: 0.00105095
Iteration 16/25 | Loss: 0.00104849
Iteration 17/25 | Loss: 0.00104615
Iteration 18/25 | Loss: 0.00104439
Iteration 19/25 | Loss: 0.00104312
Iteration 20/25 | Loss: 0.00104500
Iteration 21/25 | Loss: 0.00104147
Iteration 22/25 | Loss: 0.00104012
Iteration 23/25 | Loss: 0.00103952
Iteration 24/25 | Loss: 0.00104208
Iteration 25/25 | Loss: 0.00104076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26873314
Iteration 2/25 | Loss: 0.00052486
Iteration 3/25 | Loss: 0.00052482
Iteration 4/25 | Loss: 0.00052482
Iteration 5/25 | Loss: 0.00052482
Iteration 6/25 | Loss: 0.00052482
Iteration 7/25 | Loss: 0.00052482
Iteration 8/25 | Loss: 0.00052482
Iteration 9/25 | Loss: 0.00052482
Iteration 10/25 | Loss: 0.00052482
Iteration 11/25 | Loss: 0.00052482
Iteration 12/25 | Loss: 0.00052482
Iteration 13/25 | Loss: 0.00052482
Iteration 14/25 | Loss: 0.00052482
Iteration 15/25 | Loss: 0.00052482
Iteration 16/25 | Loss: 0.00052482
Iteration 17/25 | Loss: 0.00052482
Iteration 18/25 | Loss: 0.00052482
Iteration 19/25 | Loss: 0.00052482
Iteration 20/25 | Loss: 0.00052482
Iteration 21/25 | Loss: 0.00052482
Iteration 22/25 | Loss: 0.00052482
Iteration 23/25 | Loss: 0.00052482
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000524818548001349, 0.000524818548001349, 0.000524818548001349, 0.000524818548001349, 0.000524818548001349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000524818548001349

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052482
Iteration 2/1000 | Loss: 0.00038506
Iteration 3/1000 | Loss: 0.00031785
Iteration 4/1000 | Loss: 0.00023056
Iteration 5/1000 | Loss: 0.00018355
Iteration 6/1000 | Loss: 0.00026124
Iteration 7/1000 | Loss: 0.00014730
Iteration 8/1000 | Loss: 0.00015544
Iteration 9/1000 | Loss: 0.00018223
Iteration 10/1000 | Loss: 0.00014180
Iteration 11/1000 | Loss: 0.00025708
Iteration 12/1000 | Loss: 0.00019375
Iteration 13/1000 | Loss: 0.00007963
Iteration 14/1000 | Loss: 0.00016079
Iteration 15/1000 | Loss: 0.00069783
Iteration 16/1000 | Loss: 0.00031455
Iteration 17/1000 | Loss: 0.00006146
Iteration 18/1000 | Loss: 0.00295043
Iteration 19/1000 | Loss: 0.00072637
Iteration 20/1000 | Loss: 0.00348299
Iteration 21/1000 | Loss: 0.00069450
Iteration 22/1000 | Loss: 0.00014197
Iteration 23/1000 | Loss: 0.00062563
Iteration 24/1000 | Loss: 0.00054232
Iteration 25/1000 | Loss: 0.00307509
Iteration 26/1000 | Loss: 0.00067953
Iteration 27/1000 | Loss: 0.00128384
Iteration 28/1000 | Loss: 0.00026530
Iteration 29/1000 | Loss: 0.00119311
Iteration 30/1000 | Loss: 0.00037637
Iteration 31/1000 | Loss: 0.00079990
Iteration 32/1000 | Loss: 0.00018757
Iteration 33/1000 | Loss: 0.00017987
Iteration 34/1000 | Loss: 0.00016443
Iteration 35/1000 | Loss: 0.00006196
Iteration 36/1000 | Loss: 0.00017970
Iteration 37/1000 | Loss: 0.00006605
Iteration 38/1000 | Loss: 0.00015573
Iteration 39/1000 | Loss: 0.00005857
Iteration 40/1000 | Loss: 0.00005384
Iteration 41/1000 | Loss: 0.00012254
Iteration 42/1000 | Loss: 0.00010140
Iteration 43/1000 | Loss: 0.00010022
Iteration 44/1000 | Loss: 0.00004395
Iteration 45/1000 | Loss: 0.00003966
Iteration 46/1000 | Loss: 0.00026336
Iteration 47/1000 | Loss: 0.00006498
Iteration 48/1000 | Loss: 0.00010389
Iteration 49/1000 | Loss: 0.00005424
Iteration 50/1000 | Loss: 0.00018779
Iteration 51/1000 | Loss: 0.00009708
Iteration 52/1000 | Loss: 0.00017044
Iteration 53/1000 | Loss: 0.00012934
Iteration 54/1000 | Loss: 0.00030740
Iteration 55/1000 | Loss: 0.00006281
Iteration 56/1000 | Loss: 0.00005494
Iteration 57/1000 | Loss: 0.00003932
Iteration 58/1000 | Loss: 0.00003676
Iteration 59/1000 | Loss: 0.00003531
Iteration 60/1000 | Loss: 0.00003467
Iteration 61/1000 | Loss: 0.00003426
Iteration 62/1000 | Loss: 0.00003389
Iteration 63/1000 | Loss: 0.00003365
Iteration 64/1000 | Loss: 0.00003342
Iteration 65/1000 | Loss: 0.00003320
Iteration 66/1000 | Loss: 0.00003293
Iteration 67/1000 | Loss: 0.00003263
Iteration 68/1000 | Loss: 0.00003234
Iteration 69/1000 | Loss: 0.00003210
Iteration 70/1000 | Loss: 0.00003193
Iteration 71/1000 | Loss: 0.00003188
Iteration 72/1000 | Loss: 0.00003186
Iteration 73/1000 | Loss: 0.00003182
Iteration 74/1000 | Loss: 0.00003182
Iteration 75/1000 | Loss: 0.00003182
Iteration 76/1000 | Loss: 0.00003182
Iteration 77/1000 | Loss: 0.00003182
Iteration 78/1000 | Loss: 0.00003181
Iteration 79/1000 | Loss: 0.00003181
Iteration 80/1000 | Loss: 0.00003178
Iteration 81/1000 | Loss: 0.00003178
Iteration 82/1000 | Loss: 0.00003178
Iteration 83/1000 | Loss: 0.00003177
Iteration 84/1000 | Loss: 0.00003177
Iteration 85/1000 | Loss: 0.00003177
Iteration 86/1000 | Loss: 0.00003176
Iteration 87/1000 | Loss: 0.00003175
Iteration 88/1000 | Loss: 0.00003175
Iteration 89/1000 | Loss: 0.00003174
Iteration 90/1000 | Loss: 0.00003174
Iteration 91/1000 | Loss: 0.00003173
Iteration 92/1000 | Loss: 0.00003173
Iteration 93/1000 | Loss: 0.00003172
Iteration 94/1000 | Loss: 0.00003170
Iteration 95/1000 | Loss: 0.00003170
Iteration 96/1000 | Loss: 0.00003169
Iteration 97/1000 | Loss: 0.00003168
Iteration 98/1000 | Loss: 0.00003167
Iteration 99/1000 | Loss: 0.00003167
Iteration 100/1000 | Loss: 0.00003167
Iteration 101/1000 | Loss: 0.00003166
Iteration 102/1000 | Loss: 0.00003165
Iteration 103/1000 | Loss: 0.00003165
Iteration 104/1000 | Loss: 0.00003165
Iteration 105/1000 | Loss: 0.00003165
Iteration 106/1000 | Loss: 0.00003165
Iteration 107/1000 | Loss: 0.00003165
Iteration 108/1000 | Loss: 0.00003165
Iteration 109/1000 | Loss: 0.00003165
Iteration 110/1000 | Loss: 0.00003165
Iteration 111/1000 | Loss: 0.00003164
Iteration 112/1000 | Loss: 0.00003164
Iteration 113/1000 | Loss: 0.00003164
Iteration 114/1000 | Loss: 0.00003164
Iteration 115/1000 | Loss: 0.00003164
Iteration 116/1000 | Loss: 0.00003164
Iteration 117/1000 | Loss: 0.00003164
Iteration 118/1000 | Loss: 0.00003164
Iteration 119/1000 | Loss: 0.00003164
Iteration 120/1000 | Loss: 0.00003164
Iteration 121/1000 | Loss: 0.00003164
Iteration 122/1000 | Loss: 0.00003164
Iteration 123/1000 | Loss: 0.00003163
Iteration 124/1000 | Loss: 0.00003163
Iteration 125/1000 | Loss: 0.00003163
Iteration 126/1000 | Loss: 0.00003163
Iteration 127/1000 | Loss: 0.00003163
Iteration 128/1000 | Loss: 0.00003162
Iteration 129/1000 | Loss: 0.00003162
Iteration 130/1000 | Loss: 0.00003162
Iteration 131/1000 | Loss: 0.00003162
Iteration 132/1000 | Loss: 0.00003162
Iteration 133/1000 | Loss: 0.00003162
Iteration 134/1000 | Loss: 0.00003162
Iteration 135/1000 | Loss: 0.00003161
Iteration 136/1000 | Loss: 0.00003161
Iteration 137/1000 | Loss: 0.00003161
Iteration 138/1000 | Loss: 0.00003161
Iteration 139/1000 | Loss: 0.00003161
Iteration 140/1000 | Loss: 0.00003160
Iteration 141/1000 | Loss: 0.00003160
Iteration 142/1000 | Loss: 0.00003160
Iteration 143/1000 | Loss: 0.00003160
Iteration 144/1000 | Loss: 0.00003160
Iteration 145/1000 | Loss: 0.00003160
Iteration 146/1000 | Loss: 0.00003160
Iteration 147/1000 | Loss: 0.00003160
Iteration 148/1000 | Loss: 0.00003160
Iteration 149/1000 | Loss: 0.00003160
Iteration 150/1000 | Loss: 0.00003160
Iteration 151/1000 | Loss: 0.00003160
Iteration 152/1000 | Loss: 0.00003159
Iteration 153/1000 | Loss: 0.00003159
Iteration 154/1000 | Loss: 0.00003159
Iteration 155/1000 | Loss: 0.00003159
Iteration 156/1000 | Loss: 0.00003159
Iteration 157/1000 | Loss: 0.00003159
Iteration 158/1000 | Loss: 0.00003159
Iteration 159/1000 | Loss: 0.00003159
Iteration 160/1000 | Loss: 0.00003159
Iteration 161/1000 | Loss: 0.00003159
Iteration 162/1000 | Loss: 0.00003159
Iteration 163/1000 | Loss: 0.00003159
Iteration 164/1000 | Loss: 0.00003159
Iteration 165/1000 | Loss: 0.00003159
Iteration 166/1000 | Loss: 0.00003159
Iteration 167/1000 | Loss: 0.00003159
Iteration 168/1000 | Loss: 0.00003159
Iteration 169/1000 | Loss: 0.00003159
Iteration 170/1000 | Loss: 0.00003159
Iteration 171/1000 | Loss: 0.00003159
Iteration 172/1000 | Loss: 0.00003159
Iteration 173/1000 | Loss: 0.00003159
Iteration 174/1000 | Loss: 0.00003159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [3.15879333356861e-05, 3.15879333356861e-05, 3.15879333356861e-05, 3.15879333356861e-05, 3.15879333356861e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.15879333356861e-05

Optimization complete. Final v2v error: 4.665745735168457 mm

Highest mean error: 6.071396827697754 mm for frame 229

Lowest mean error: 3.978194236755371 mm for frame 108

Saving results

Total time: 172.72150921821594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384316
Iteration 2/25 | Loss: 0.00095894
Iteration 3/25 | Loss: 0.00083877
Iteration 4/25 | Loss: 0.00081528
Iteration 5/25 | Loss: 0.00080461
Iteration 6/25 | Loss: 0.00080196
Iteration 7/25 | Loss: 0.00080124
Iteration 8/25 | Loss: 0.00080124
Iteration 9/25 | Loss: 0.00080124
Iteration 10/25 | Loss: 0.00080124
Iteration 11/25 | Loss: 0.00080124
Iteration 12/25 | Loss: 0.00080124
Iteration 13/25 | Loss: 0.00080124
Iteration 14/25 | Loss: 0.00080124
Iteration 15/25 | Loss: 0.00080124
Iteration 16/25 | Loss: 0.00080124
Iteration 17/25 | Loss: 0.00080124
Iteration 18/25 | Loss: 0.00080124
Iteration 19/25 | Loss: 0.00080124
Iteration 20/25 | Loss: 0.00080124
Iteration 21/25 | Loss: 0.00080124
Iteration 22/25 | Loss: 0.00080124
Iteration 23/25 | Loss: 0.00080124
Iteration 24/25 | Loss: 0.00080124
Iteration 25/25 | Loss: 0.00080124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.96733284
Iteration 2/25 | Loss: 0.00033387
Iteration 3/25 | Loss: 0.00033386
Iteration 4/25 | Loss: 0.00033386
Iteration 5/25 | Loss: 0.00033386
Iteration 6/25 | Loss: 0.00033386
Iteration 7/25 | Loss: 0.00033386
Iteration 8/25 | Loss: 0.00033386
Iteration 9/25 | Loss: 0.00033386
Iteration 10/25 | Loss: 0.00033386
Iteration 11/25 | Loss: 0.00033386
Iteration 12/25 | Loss: 0.00033386
Iteration 13/25 | Loss: 0.00033386
Iteration 14/25 | Loss: 0.00033386
Iteration 15/25 | Loss: 0.00033386
Iteration 16/25 | Loss: 0.00033386
Iteration 17/25 | Loss: 0.00033386
Iteration 18/25 | Loss: 0.00033386
Iteration 19/25 | Loss: 0.00033386
Iteration 20/25 | Loss: 0.00033386
Iteration 21/25 | Loss: 0.00033386
Iteration 22/25 | Loss: 0.00033386
Iteration 23/25 | Loss: 0.00033386
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00033385632559657097, 0.00033385632559657097, 0.00033385632559657097, 0.00033385632559657097, 0.00033385632559657097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033385632559657097

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033386
Iteration 2/1000 | Loss: 0.00003170
Iteration 3/1000 | Loss: 0.00002084
Iteration 4/1000 | Loss: 0.00001831
Iteration 5/1000 | Loss: 0.00001728
Iteration 6/1000 | Loss: 0.00001663
Iteration 7/1000 | Loss: 0.00001623
Iteration 8/1000 | Loss: 0.00001615
Iteration 9/1000 | Loss: 0.00001606
Iteration 10/1000 | Loss: 0.00001601
Iteration 11/1000 | Loss: 0.00001600
Iteration 12/1000 | Loss: 0.00001600
Iteration 13/1000 | Loss: 0.00001599
Iteration 14/1000 | Loss: 0.00001590
Iteration 15/1000 | Loss: 0.00001582
Iteration 16/1000 | Loss: 0.00001581
Iteration 17/1000 | Loss: 0.00001581
Iteration 18/1000 | Loss: 0.00001581
Iteration 19/1000 | Loss: 0.00001581
Iteration 20/1000 | Loss: 0.00001581
Iteration 21/1000 | Loss: 0.00001581
Iteration 22/1000 | Loss: 0.00001581
Iteration 23/1000 | Loss: 0.00001578
Iteration 24/1000 | Loss: 0.00001578
Iteration 25/1000 | Loss: 0.00001576
Iteration 26/1000 | Loss: 0.00001576
Iteration 27/1000 | Loss: 0.00001576
Iteration 28/1000 | Loss: 0.00001575
Iteration 29/1000 | Loss: 0.00001575
Iteration 30/1000 | Loss: 0.00001575
Iteration 31/1000 | Loss: 0.00001574
Iteration 32/1000 | Loss: 0.00001574
Iteration 33/1000 | Loss: 0.00001574
Iteration 34/1000 | Loss: 0.00001573
Iteration 35/1000 | Loss: 0.00001573
Iteration 36/1000 | Loss: 0.00001572
Iteration 37/1000 | Loss: 0.00001571
Iteration 38/1000 | Loss: 0.00001571
Iteration 39/1000 | Loss: 0.00001570
Iteration 40/1000 | Loss: 0.00001570
Iteration 41/1000 | Loss: 0.00001570
Iteration 42/1000 | Loss: 0.00001570
Iteration 43/1000 | Loss: 0.00001570
Iteration 44/1000 | Loss: 0.00001569
Iteration 45/1000 | Loss: 0.00001569
Iteration 46/1000 | Loss: 0.00001569
Iteration 47/1000 | Loss: 0.00001568
Iteration 48/1000 | Loss: 0.00001568
Iteration 49/1000 | Loss: 0.00001568
Iteration 50/1000 | Loss: 0.00001568
Iteration 51/1000 | Loss: 0.00001568
Iteration 52/1000 | Loss: 0.00001567
Iteration 53/1000 | Loss: 0.00001567
Iteration 54/1000 | Loss: 0.00001567
Iteration 55/1000 | Loss: 0.00001567
Iteration 56/1000 | Loss: 0.00001567
Iteration 57/1000 | Loss: 0.00001567
Iteration 58/1000 | Loss: 0.00001567
Iteration 59/1000 | Loss: 0.00001567
Iteration 60/1000 | Loss: 0.00001567
Iteration 61/1000 | Loss: 0.00001567
Iteration 62/1000 | Loss: 0.00001567
Iteration 63/1000 | Loss: 0.00001567
Iteration 64/1000 | Loss: 0.00001567
Iteration 65/1000 | Loss: 0.00001567
Iteration 66/1000 | Loss: 0.00001567
Iteration 67/1000 | Loss: 0.00001567
Iteration 68/1000 | Loss: 0.00001567
Iteration 69/1000 | Loss: 0.00001566
Iteration 70/1000 | Loss: 0.00001566
Iteration 71/1000 | Loss: 0.00001566
Iteration 72/1000 | Loss: 0.00001566
Iteration 73/1000 | Loss: 0.00001566
Iteration 74/1000 | Loss: 0.00001566
Iteration 75/1000 | Loss: 0.00001566
Iteration 76/1000 | Loss: 0.00001566
Iteration 77/1000 | Loss: 0.00001566
Iteration 78/1000 | Loss: 0.00001566
Iteration 79/1000 | Loss: 0.00001566
Iteration 80/1000 | Loss: 0.00001566
Iteration 81/1000 | Loss: 0.00001566
Iteration 82/1000 | Loss: 0.00001566
Iteration 83/1000 | Loss: 0.00001566
Iteration 84/1000 | Loss: 0.00001566
Iteration 85/1000 | Loss: 0.00001566
Iteration 86/1000 | Loss: 0.00001565
Iteration 87/1000 | Loss: 0.00001565
Iteration 88/1000 | Loss: 0.00001565
Iteration 89/1000 | Loss: 0.00001565
Iteration 90/1000 | Loss: 0.00001565
Iteration 91/1000 | Loss: 0.00001565
Iteration 92/1000 | Loss: 0.00001565
Iteration 93/1000 | Loss: 0.00001565
Iteration 94/1000 | Loss: 0.00001565
Iteration 95/1000 | Loss: 0.00001565
Iteration 96/1000 | Loss: 0.00001565
Iteration 97/1000 | Loss: 0.00001565
Iteration 98/1000 | Loss: 0.00001565
Iteration 99/1000 | Loss: 0.00001565
Iteration 100/1000 | Loss: 0.00001565
Iteration 101/1000 | Loss: 0.00001565
Iteration 102/1000 | Loss: 0.00001565
Iteration 103/1000 | Loss: 0.00001565
Iteration 104/1000 | Loss: 0.00001565
Iteration 105/1000 | Loss: 0.00001565
Iteration 106/1000 | Loss: 0.00001565
Iteration 107/1000 | Loss: 0.00001565
Iteration 108/1000 | Loss: 0.00001565
Iteration 109/1000 | Loss: 0.00001565
Iteration 110/1000 | Loss: 0.00001565
Iteration 111/1000 | Loss: 0.00001565
Iteration 112/1000 | Loss: 0.00001565
Iteration 113/1000 | Loss: 0.00001565
Iteration 114/1000 | Loss: 0.00001565
Iteration 115/1000 | Loss: 0.00001565
Iteration 116/1000 | Loss: 0.00001565
Iteration 117/1000 | Loss: 0.00001565
Iteration 118/1000 | Loss: 0.00001565
Iteration 119/1000 | Loss: 0.00001565
Iteration 120/1000 | Loss: 0.00001565
Iteration 121/1000 | Loss: 0.00001565
Iteration 122/1000 | Loss: 0.00001565
Iteration 123/1000 | Loss: 0.00001565
Iteration 124/1000 | Loss: 0.00001565
Iteration 125/1000 | Loss: 0.00001565
Iteration 126/1000 | Loss: 0.00001565
Iteration 127/1000 | Loss: 0.00001565
Iteration 128/1000 | Loss: 0.00001565
Iteration 129/1000 | Loss: 0.00001565
Iteration 130/1000 | Loss: 0.00001565
Iteration 131/1000 | Loss: 0.00001565
Iteration 132/1000 | Loss: 0.00001565
Iteration 133/1000 | Loss: 0.00001565
Iteration 134/1000 | Loss: 0.00001565
Iteration 135/1000 | Loss: 0.00001565
Iteration 136/1000 | Loss: 0.00001565
Iteration 137/1000 | Loss: 0.00001565
Iteration 138/1000 | Loss: 0.00001565
Iteration 139/1000 | Loss: 0.00001565
Iteration 140/1000 | Loss: 0.00001565
Iteration 141/1000 | Loss: 0.00001565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.5645149687770754e-05, 1.5645149687770754e-05, 1.5645149687770754e-05, 1.5645149687770754e-05, 1.5645149687770754e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5645149687770754e-05

Optimization complete. Final v2v error: 3.3198771476745605 mm

Highest mean error: 3.7999589443206787 mm for frame 95

Lowest mean error: 2.9688985347747803 mm for frame 125

Saving results

Total time: 32.25613284111023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848833
Iteration 2/25 | Loss: 0.00122708
Iteration 3/25 | Loss: 0.00092499
Iteration 4/25 | Loss: 0.00087599
Iteration 5/25 | Loss: 0.00086584
Iteration 6/25 | Loss: 0.00086245
Iteration 7/25 | Loss: 0.00086190
Iteration 8/25 | Loss: 0.00086190
Iteration 9/25 | Loss: 0.00086190
Iteration 10/25 | Loss: 0.00086190
Iteration 11/25 | Loss: 0.00086190
Iteration 12/25 | Loss: 0.00086190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008618964347988367, 0.0008618964347988367, 0.0008618964347988367, 0.0008618964347988367, 0.0008618964347988367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008618964347988367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35125613
Iteration 2/25 | Loss: 0.00037052
Iteration 3/25 | Loss: 0.00037052
Iteration 4/25 | Loss: 0.00037052
Iteration 5/25 | Loss: 0.00037052
Iteration 6/25 | Loss: 0.00037052
Iteration 7/25 | Loss: 0.00037052
Iteration 8/25 | Loss: 0.00037052
Iteration 9/25 | Loss: 0.00037052
Iteration 10/25 | Loss: 0.00037052
Iteration 11/25 | Loss: 0.00037052
Iteration 12/25 | Loss: 0.00037052
Iteration 13/25 | Loss: 0.00037052
Iteration 14/25 | Loss: 0.00037052
Iteration 15/25 | Loss: 0.00037052
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0003705160925164819, 0.0003705160925164819, 0.0003705160925164819, 0.0003705160925164819, 0.0003705160925164819]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003705160925164819

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037052
Iteration 2/1000 | Loss: 0.00003449
Iteration 3/1000 | Loss: 0.00002639
Iteration 4/1000 | Loss: 0.00002383
Iteration 5/1000 | Loss: 0.00002294
Iteration 6/1000 | Loss: 0.00002214
Iteration 7/1000 | Loss: 0.00002149
Iteration 8/1000 | Loss: 0.00002109
Iteration 9/1000 | Loss: 0.00002085
Iteration 10/1000 | Loss: 0.00002068
Iteration 11/1000 | Loss: 0.00002066
Iteration 12/1000 | Loss: 0.00002052
Iteration 13/1000 | Loss: 0.00002049
Iteration 14/1000 | Loss: 0.00002049
Iteration 15/1000 | Loss: 0.00002047
Iteration 16/1000 | Loss: 0.00002047
Iteration 17/1000 | Loss: 0.00002047
Iteration 18/1000 | Loss: 0.00002046
Iteration 19/1000 | Loss: 0.00002043
Iteration 20/1000 | Loss: 0.00002043
Iteration 21/1000 | Loss: 0.00002043
Iteration 22/1000 | Loss: 0.00002043
Iteration 23/1000 | Loss: 0.00002041
Iteration 24/1000 | Loss: 0.00002041
Iteration 25/1000 | Loss: 0.00002041
Iteration 26/1000 | Loss: 0.00002041
Iteration 27/1000 | Loss: 0.00002040
Iteration 28/1000 | Loss: 0.00002040
Iteration 29/1000 | Loss: 0.00002040
Iteration 30/1000 | Loss: 0.00002040
Iteration 31/1000 | Loss: 0.00002040
Iteration 32/1000 | Loss: 0.00002040
Iteration 33/1000 | Loss: 0.00002040
Iteration 34/1000 | Loss: 0.00002040
Iteration 35/1000 | Loss: 0.00002040
Iteration 36/1000 | Loss: 0.00002040
Iteration 37/1000 | Loss: 0.00002040
Iteration 38/1000 | Loss: 0.00002039
Iteration 39/1000 | Loss: 0.00002039
Iteration 40/1000 | Loss: 0.00002039
Iteration 41/1000 | Loss: 0.00002038
Iteration 42/1000 | Loss: 0.00002038
Iteration 43/1000 | Loss: 0.00002038
Iteration 44/1000 | Loss: 0.00002038
Iteration 45/1000 | Loss: 0.00002038
Iteration 46/1000 | Loss: 0.00002038
Iteration 47/1000 | Loss: 0.00002038
Iteration 48/1000 | Loss: 0.00002038
Iteration 49/1000 | Loss: 0.00002037
Iteration 50/1000 | Loss: 0.00002037
Iteration 51/1000 | Loss: 0.00002037
Iteration 52/1000 | Loss: 0.00002037
Iteration 53/1000 | Loss: 0.00002037
Iteration 54/1000 | Loss: 0.00002037
Iteration 55/1000 | Loss: 0.00002037
Iteration 56/1000 | Loss: 0.00002037
Iteration 57/1000 | Loss: 0.00002037
Iteration 58/1000 | Loss: 0.00002037
Iteration 59/1000 | Loss: 0.00002037
Iteration 60/1000 | Loss: 0.00002037
Iteration 61/1000 | Loss: 0.00002037
Iteration 62/1000 | Loss: 0.00002036
Iteration 63/1000 | Loss: 0.00002036
Iteration 64/1000 | Loss: 0.00002036
Iteration 65/1000 | Loss: 0.00002036
Iteration 66/1000 | Loss: 0.00002036
Iteration 67/1000 | Loss: 0.00002036
Iteration 68/1000 | Loss: 0.00002036
Iteration 69/1000 | Loss: 0.00002036
Iteration 70/1000 | Loss: 0.00002036
Iteration 71/1000 | Loss: 0.00002036
Iteration 72/1000 | Loss: 0.00002036
Iteration 73/1000 | Loss: 0.00002036
Iteration 74/1000 | Loss: 0.00002036
Iteration 75/1000 | Loss: 0.00002036
Iteration 76/1000 | Loss: 0.00002036
Iteration 77/1000 | Loss: 0.00002036
Iteration 78/1000 | Loss: 0.00002035
Iteration 79/1000 | Loss: 0.00002035
Iteration 80/1000 | Loss: 0.00002035
Iteration 81/1000 | Loss: 0.00002035
Iteration 82/1000 | Loss: 0.00002035
Iteration 83/1000 | Loss: 0.00002035
Iteration 84/1000 | Loss: 0.00002035
Iteration 85/1000 | Loss: 0.00002035
Iteration 86/1000 | Loss: 0.00002035
Iteration 87/1000 | Loss: 0.00002035
Iteration 88/1000 | Loss: 0.00002035
Iteration 89/1000 | Loss: 0.00002034
Iteration 90/1000 | Loss: 0.00002034
Iteration 91/1000 | Loss: 0.00002034
Iteration 92/1000 | Loss: 0.00002034
Iteration 93/1000 | Loss: 0.00002034
Iteration 94/1000 | Loss: 0.00002034
Iteration 95/1000 | Loss: 0.00002034
Iteration 96/1000 | Loss: 0.00002034
Iteration 97/1000 | Loss: 0.00002034
Iteration 98/1000 | Loss: 0.00002034
Iteration 99/1000 | Loss: 0.00002034
Iteration 100/1000 | Loss: 0.00002034
Iteration 101/1000 | Loss: 0.00002034
Iteration 102/1000 | Loss: 0.00002033
Iteration 103/1000 | Loss: 0.00002033
Iteration 104/1000 | Loss: 0.00002033
Iteration 105/1000 | Loss: 0.00002033
Iteration 106/1000 | Loss: 0.00002033
Iteration 107/1000 | Loss: 0.00002033
Iteration 108/1000 | Loss: 0.00002033
Iteration 109/1000 | Loss: 0.00002033
Iteration 110/1000 | Loss: 0.00002032
Iteration 111/1000 | Loss: 0.00002032
Iteration 112/1000 | Loss: 0.00002032
Iteration 113/1000 | Loss: 0.00002032
Iteration 114/1000 | Loss: 0.00002032
Iteration 115/1000 | Loss: 0.00002032
Iteration 116/1000 | Loss: 0.00002032
Iteration 117/1000 | Loss: 0.00002032
Iteration 118/1000 | Loss: 0.00002032
Iteration 119/1000 | Loss: 0.00002032
Iteration 120/1000 | Loss: 0.00002032
Iteration 121/1000 | Loss: 0.00002032
Iteration 122/1000 | Loss: 0.00002032
Iteration 123/1000 | Loss: 0.00002032
Iteration 124/1000 | Loss: 0.00002032
Iteration 125/1000 | Loss: 0.00002031
Iteration 126/1000 | Loss: 0.00002031
Iteration 127/1000 | Loss: 0.00002031
Iteration 128/1000 | Loss: 0.00002031
Iteration 129/1000 | Loss: 0.00002031
Iteration 130/1000 | Loss: 0.00002031
Iteration 131/1000 | Loss: 0.00002031
Iteration 132/1000 | Loss: 0.00002031
Iteration 133/1000 | Loss: 0.00002031
Iteration 134/1000 | Loss: 0.00002031
Iteration 135/1000 | Loss: 0.00002031
Iteration 136/1000 | Loss: 0.00002031
Iteration 137/1000 | Loss: 0.00002031
Iteration 138/1000 | Loss: 0.00002031
Iteration 139/1000 | Loss: 0.00002031
Iteration 140/1000 | Loss: 0.00002031
Iteration 141/1000 | Loss: 0.00002031
Iteration 142/1000 | Loss: 0.00002031
Iteration 143/1000 | Loss: 0.00002031
Iteration 144/1000 | Loss: 0.00002031
Iteration 145/1000 | Loss: 0.00002031
Iteration 146/1000 | Loss: 0.00002031
Iteration 147/1000 | Loss: 0.00002031
Iteration 148/1000 | Loss: 0.00002031
Iteration 149/1000 | Loss: 0.00002031
Iteration 150/1000 | Loss: 0.00002031
Iteration 151/1000 | Loss: 0.00002031
Iteration 152/1000 | Loss: 0.00002031
Iteration 153/1000 | Loss: 0.00002031
Iteration 154/1000 | Loss: 0.00002031
Iteration 155/1000 | Loss: 0.00002031
Iteration 156/1000 | Loss: 0.00002031
Iteration 157/1000 | Loss: 0.00002031
Iteration 158/1000 | Loss: 0.00002031
Iteration 159/1000 | Loss: 0.00002031
Iteration 160/1000 | Loss: 0.00002031
Iteration 161/1000 | Loss: 0.00002031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.031227086263243e-05, 2.031227086263243e-05, 2.031227086263243e-05, 2.031227086263243e-05, 2.031227086263243e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.031227086263243e-05

Optimization complete. Final v2v error: 3.8106038570404053 mm

Highest mean error: 5.13836669921875 mm for frame 202

Lowest mean error: 3.141108989715576 mm for frame 158

Saving results

Total time: 39.156248807907104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462020
Iteration 2/25 | Loss: 0.00105933
Iteration 3/25 | Loss: 0.00087490
Iteration 4/25 | Loss: 0.00085019
Iteration 5/25 | Loss: 0.00083781
Iteration 6/25 | Loss: 0.00083526
Iteration 7/25 | Loss: 0.00083451
Iteration 8/25 | Loss: 0.00083451
Iteration 9/25 | Loss: 0.00083451
Iteration 10/25 | Loss: 0.00083451
Iteration 11/25 | Loss: 0.00083451
Iteration 12/25 | Loss: 0.00083451
Iteration 13/25 | Loss: 0.00083451
Iteration 14/25 | Loss: 0.00083451
Iteration 15/25 | Loss: 0.00083451
Iteration 16/25 | Loss: 0.00083451
Iteration 17/25 | Loss: 0.00083451
Iteration 18/25 | Loss: 0.00083451
Iteration 19/25 | Loss: 0.00083451
Iteration 20/25 | Loss: 0.00083451
Iteration 21/25 | Loss: 0.00083451
Iteration 22/25 | Loss: 0.00083451
Iteration 23/25 | Loss: 0.00083451
Iteration 24/25 | Loss: 0.00083451
Iteration 25/25 | Loss: 0.00083451

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46904898
Iteration 2/25 | Loss: 0.00041328
Iteration 3/25 | Loss: 0.00041328
Iteration 4/25 | Loss: 0.00041328
Iteration 5/25 | Loss: 0.00041327
Iteration 6/25 | Loss: 0.00041327
Iteration 7/25 | Loss: 0.00041327
Iteration 8/25 | Loss: 0.00041327
Iteration 9/25 | Loss: 0.00041327
Iteration 10/25 | Loss: 0.00041327
Iteration 11/25 | Loss: 0.00041327
Iteration 12/25 | Loss: 0.00041327
Iteration 13/25 | Loss: 0.00041327
Iteration 14/25 | Loss: 0.00041327
Iteration 15/25 | Loss: 0.00041327
Iteration 16/25 | Loss: 0.00041327
Iteration 17/25 | Loss: 0.00041327
Iteration 18/25 | Loss: 0.00041327
Iteration 19/25 | Loss: 0.00041327
Iteration 20/25 | Loss: 0.00041327
Iteration 21/25 | Loss: 0.00041327
Iteration 22/25 | Loss: 0.00041327
Iteration 23/25 | Loss: 0.00041327
Iteration 24/25 | Loss: 0.00041327
Iteration 25/25 | Loss: 0.00041327
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00041327349026687443, 0.00041327349026687443, 0.00041327349026687443, 0.00041327349026687443, 0.00041327349026687443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041327349026687443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041327
Iteration 2/1000 | Loss: 0.00003631
Iteration 3/1000 | Loss: 0.00002482
Iteration 4/1000 | Loss: 0.00002239
Iteration 5/1000 | Loss: 0.00002093
Iteration 6/1000 | Loss: 0.00002030
Iteration 7/1000 | Loss: 0.00001988
Iteration 8/1000 | Loss: 0.00001956
Iteration 9/1000 | Loss: 0.00001939
Iteration 10/1000 | Loss: 0.00001936
Iteration 11/1000 | Loss: 0.00001935
Iteration 12/1000 | Loss: 0.00001933
Iteration 13/1000 | Loss: 0.00001932
Iteration 14/1000 | Loss: 0.00001928
Iteration 15/1000 | Loss: 0.00001927
Iteration 16/1000 | Loss: 0.00001926
Iteration 17/1000 | Loss: 0.00001924
Iteration 18/1000 | Loss: 0.00001924
Iteration 19/1000 | Loss: 0.00001924
Iteration 20/1000 | Loss: 0.00001923
Iteration 21/1000 | Loss: 0.00001923
Iteration 22/1000 | Loss: 0.00001923
Iteration 23/1000 | Loss: 0.00001923
Iteration 24/1000 | Loss: 0.00001923
Iteration 25/1000 | Loss: 0.00001923
Iteration 26/1000 | Loss: 0.00001923
Iteration 27/1000 | Loss: 0.00001923
Iteration 28/1000 | Loss: 0.00001923
Iteration 29/1000 | Loss: 0.00001923
Iteration 30/1000 | Loss: 0.00001923
Iteration 31/1000 | Loss: 0.00001923
Iteration 32/1000 | Loss: 0.00001923
Iteration 33/1000 | Loss: 0.00001922
Iteration 34/1000 | Loss: 0.00001922
Iteration 35/1000 | Loss: 0.00001922
Iteration 36/1000 | Loss: 0.00001921
Iteration 37/1000 | Loss: 0.00001921
Iteration 38/1000 | Loss: 0.00001921
Iteration 39/1000 | Loss: 0.00001921
Iteration 40/1000 | Loss: 0.00001921
Iteration 41/1000 | Loss: 0.00001921
Iteration 42/1000 | Loss: 0.00001921
Iteration 43/1000 | Loss: 0.00001921
Iteration 44/1000 | Loss: 0.00001921
Iteration 45/1000 | Loss: 0.00001921
Iteration 46/1000 | Loss: 0.00001921
Iteration 47/1000 | Loss: 0.00001921
Iteration 48/1000 | Loss: 0.00001921
Iteration 49/1000 | Loss: 0.00001921
Iteration 50/1000 | Loss: 0.00001921
Iteration 51/1000 | Loss: 0.00001921
Iteration 52/1000 | Loss: 0.00001921
Iteration 53/1000 | Loss: 0.00001921
Iteration 54/1000 | Loss: 0.00001921
Iteration 55/1000 | Loss: 0.00001921
Iteration 56/1000 | Loss: 0.00001921
Iteration 57/1000 | Loss: 0.00001921
Iteration 58/1000 | Loss: 0.00001921
Iteration 59/1000 | Loss: 0.00001921
Iteration 60/1000 | Loss: 0.00001921
Iteration 61/1000 | Loss: 0.00001921
Iteration 62/1000 | Loss: 0.00001921
Iteration 63/1000 | Loss: 0.00001921
Iteration 64/1000 | Loss: 0.00001921
Iteration 65/1000 | Loss: 0.00001921
Iteration 66/1000 | Loss: 0.00001921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [1.920515569509007e-05, 1.920515569509007e-05, 1.920515569509007e-05, 1.920515569509007e-05, 1.920515569509007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.920515569509007e-05

Optimization complete. Final v2v error: 3.768887996673584 mm

Highest mean error: 4.778169631958008 mm for frame 160

Lowest mean error: 3.0712196826934814 mm for frame 5

Saving results

Total time: 31.034985303878784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413458
Iteration 2/25 | Loss: 0.00092907
Iteration 3/25 | Loss: 0.00080297
Iteration 4/25 | Loss: 0.00078570
Iteration 5/25 | Loss: 0.00077961
Iteration 6/25 | Loss: 0.00077809
Iteration 7/25 | Loss: 0.00077783
Iteration 8/25 | Loss: 0.00077783
Iteration 9/25 | Loss: 0.00077783
Iteration 10/25 | Loss: 0.00077783
Iteration 11/25 | Loss: 0.00077783
Iteration 12/25 | Loss: 0.00077783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007778274011798203, 0.0007778274011798203, 0.0007778274011798203, 0.0007778274011798203, 0.0007778274011798203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007778274011798203

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41656387
Iteration 2/25 | Loss: 0.00033278
Iteration 3/25 | Loss: 0.00033278
Iteration 4/25 | Loss: 0.00033278
Iteration 5/25 | Loss: 0.00033278
Iteration 6/25 | Loss: 0.00033278
Iteration 7/25 | Loss: 0.00033278
Iteration 8/25 | Loss: 0.00033278
Iteration 9/25 | Loss: 0.00033278
Iteration 10/25 | Loss: 0.00033278
Iteration 11/25 | Loss: 0.00033278
Iteration 12/25 | Loss: 0.00033278
Iteration 13/25 | Loss: 0.00033278
Iteration 14/25 | Loss: 0.00033278
Iteration 15/25 | Loss: 0.00033278
Iteration 16/25 | Loss: 0.00033278
Iteration 17/25 | Loss: 0.00033278
Iteration 18/25 | Loss: 0.00033278
Iteration 19/25 | Loss: 0.00033278
Iteration 20/25 | Loss: 0.00033278
Iteration 21/25 | Loss: 0.00033278
Iteration 22/25 | Loss: 0.00033278
Iteration 23/25 | Loss: 0.00033278
Iteration 24/25 | Loss: 0.00033278
Iteration 25/25 | Loss: 0.00033278

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033278
Iteration 2/1000 | Loss: 0.00002729
Iteration 3/1000 | Loss: 0.00001666
Iteration 4/1000 | Loss: 0.00001445
Iteration 5/1000 | Loss: 0.00001373
Iteration 6/1000 | Loss: 0.00001306
Iteration 7/1000 | Loss: 0.00001272
Iteration 8/1000 | Loss: 0.00001269
Iteration 9/1000 | Loss: 0.00001261
Iteration 10/1000 | Loss: 0.00001259
Iteration 11/1000 | Loss: 0.00001257
Iteration 12/1000 | Loss: 0.00001257
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001254
Iteration 15/1000 | Loss: 0.00001252
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001250
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001243
Iteration 20/1000 | Loss: 0.00001242
Iteration 21/1000 | Loss: 0.00001241
Iteration 22/1000 | Loss: 0.00001241
Iteration 23/1000 | Loss: 0.00001241
Iteration 24/1000 | Loss: 0.00001241
Iteration 25/1000 | Loss: 0.00001241
Iteration 26/1000 | Loss: 0.00001240
Iteration 27/1000 | Loss: 0.00001240
Iteration 28/1000 | Loss: 0.00001240
Iteration 29/1000 | Loss: 0.00001239
Iteration 30/1000 | Loss: 0.00001238
Iteration 31/1000 | Loss: 0.00001237
Iteration 32/1000 | Loss: 0.00001236
Iteration 33/1000 | Loss: 0.00001235
Iteration 34/1000 | Loss: 0.00001233
Iteration 35/1000 | Loss: 0.00001230
Iteration 36/1000 | Loss: 0.00001230
Iteration 37/1000 | Loss: 0.00001229
Iteration 38/1000 | Loss: 0.00001229
Iteration 39/1000 | Loss: 0.00001229
Iteration 40/1000 | Loss: 0.00001229
Iteration 41/1000 | Loss: 0.00001229
Iteration 42/1000 | Loss: 0.00001229
Iteration 43/1000 | Loss: 0.00001229
Iteration 44/1000 | Loss: 0.00001229
Iteration 45/1000 | Loss: 0.00001229
Iteration 46/1000 | Loss: 0.00001229
Iteration 47/1000 | Loss: 0.00001229
Iteration 48/1000 | Loss: 0.00001229
Iteration 49/1000 | Loss: 0.00001228
Iteration 50/1000 | Loss: 0.00001228
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001227
Iteration 53/1000 | Loss: 0.00001226
Iteration 54/1000 | Loss: 0.00001226
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001225
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001225
Iteration 62/1000 | Loss: 0.00001225
Iteration 63/1000 | Loss: 0.00001224
Iteration 64/1000 | Loss: 0.00001222
Iteration 65/1000 | Loss: 0.00001220
Iteration 66/1000 | Loss: 0.00001220
Iteration 67/1000 | Loss: 0.00001220
Iteration 68/1000 | Loss: 0.00001220
Iteration 69/1000 | Loss: 0.00001220
Iteration 70/1000 | Loss: 0.00001220
Iteration 71/1000 | Loss: 0.00001220
Iteration 72/1000 | Loss: 0.00001220
Iteration 73/1000 | Loss: 0.00001220
Iteration 74/1000 | Loss: 0.00001219
Iteration 75/1000 | Loss: 0.00001219
Iteration 76/1000 | Loss: 0.00001219
Iteration 77/1000 | Loss: 0.00001219
Iteration 78/1000 | Loss: 0.00001219
Iteration 79/1000 | Loss: 0.00001218
Iteration 80/1000 | Loss: 0.00001218
Iteration 81/1000 | Loss: 0.00001218
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001218
Iteration 85/1000 | Loss: 0.00001218
Iteration 86/1000 | Loss: 0.00001217
Iteration 87/1000 | Loss: 0.00001217
Iteration 88/1000 | Loss: 0.00001217
Iteration 89/1000 | Loss: 0.00001217
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001217
Iteration 94/1000 | Loss: 0.00001217
Iteration 95/1000 | Loss: 0.00001217
Iteration 96/1000 | Loss: 0.00001217
Iteration 97/1000 | Loss: 0.00001217
Iteration 98/1000 | Loss: 0.00001217
Iteration 99/1000 | Loss: 0.00001216
Iteration 100/1000 | Loss: 0.00001216
Iteration 101/1000 | Loss: 0.00001216
Iteration 102/1000 | Loss: 0.00001216
Iteration 103/1000 | Loss: 0.00001216
Iteration 104/1000 | Loss: 0.00001216
Iteration 105/1000 | Loss: 0.00001216
Iteration 106/1000 | Loss: 0.00001216
Iteration 107/1000 | Loss: 0.00001216
Iteration 108/1000 | Loss: 0.00001216
Iteration 109/1000 | Loss: 0.00001216
Iteration 110/1000 | Loss: 0.00001216
Iteration 111/1000 | Loss: 0.00001216
Iteration 112/1000 | Loss: 0.00001216
Iteration 113/1000 | Loss: 0.00001216
Iteration 114/1000 | Loss: 0.00001216
Iteration 115/1000 | Loss: 0.00001216
Iteration 116/1000 | Loss: 0.00001216
Iteration 117/1000 | Loss: 0.00001216
Iteration 118/1000 | Loss: 0.00001216
Iteration 119/1000 | Loss: 0.00001216
Iteration 120/1000 | Loss: 0.00001216
Iteration 121/1000 | Loss: 0.00001216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.2162364328105468e-05, 1.2162364328105468e-05, 1.2162364328105468e-05, 1.2162364328105468e-05, 1.2162364328105468e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2162364328105468e-05

Optimization complete. Final v2v error: 2.9783294200897217 mm

Highest mean error: 3.102916717529297 mm for frame 127

Lowest mean error: 2.840475559234619 mm for frame 15

Saving results

Total time: 29.13260793685913
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01111268
Iteration 2/25 | Loss: 0.00201599
Iteration 3/25 | Loss: 0.00147929
Iteration 4/25 | Loss: 0.00172588
Iteration 5/25 | Loss: 0.00146060
Iteration 6/25 | Loss: 0.00138172
Iteration 7/25 | Loss: 0.00119131
Iteration 8/25 | Loss: 0.00096583
Iteration 9/25 | Loss: 0.00092787
Iteration 10/25 | Loss: 0.00082055
Iteration 11/25 | Loss: 0.00080301
Iteration 12/25 | Loss: 0.00079433
Iteration 13/25 | Loss: 0.00078162
Iteration 14/25 | Loss: 0.00077446
Iteration 15/25 | Loss: 0.00078211
Iteration 16/25 | Loss: 0.00079190
Iteration 17/25 | Loss: 0.00077617
Iteration 18/25 | Loss: 0.00077674
Iteration 19/25 | Loss: 0.00077476
Iteration 20/25 | Loss: 0.00077077
Iteration 21/25 | Loss: 0.00076911
Iteration 22/25 | Loss: 0.00077079
Iteration 23/25 | Loss: 0.00077552
Iteration 24/25 | Loss: 0.00077236
Iteration 25/25 | Loss: 0.00077263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43632066
Iteration 2/25 | Loss: 0.00141750
Iteration 3/25 | Loss: 0.00049741
Iteration 4/25 | Loss: 0.00049741
Iteration 5/25 | Loss: 0.00049741
Iteration 6/25 | Loss: 0.00049741
Iteration 7/25 | Loss: 0.00049741
Iteration 8/25 | Loss: 0.00049741
Iteration 9/25 | Loss: 0.00049741
Iteration 10/25 | Loss: 0.00049741
Iteration 11/25 | Loss: 0.00049741
Iteration 12/25 | Loss: 0.00049741
Iteration 13/25 | Loss: 0.00049741
Iteration 14/25 | Loss: 0.00049741
Iteration 15/25 | Loss: 0.00049741
Iteration 16/25 | Loss: 0.00049741
Iteration 17/25 | Loss: 0.00049741
Iteration 18/25 | Loss: 0.00049741
Iteration 19/25 | Loss: 0.00049741
Iteration 20/25 | Loss: 0.00049741
Iteration 21/25 | Loss: 0.00049741
Iteration 22/25 | Loss: 0.00049741
Iteration 23/25 | Loss: 0.00049741
Iteration 24/25 | Loss: 0.00049741
Iteration 25/25 | Loss: 0.00049741

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049741
Iteration 2/1000 | Loss: 0.00011103
Iteration 3/1000 | Loss: 0.00068475
Iteration 4/1000 | Loss: 0.00020788
Iteration 5/1000 | Loss: 0.00023958
Iteration 6/1000 | Loss: 0.00029897
Iteration 7/1000 | Loss: 0.00031320
Iteration 8/1000 | Loss: 0.00032193
Iteration 9/1000 | Loss: 0.00035090
Iteration 10/1000 | Loss: 0.00031105
Iteration 11/1000 | Loss: 0.00028504
Iteration 12/1000 | Loss: 0.00127841
Iteration 13/1000 | Loss: 0.00014932
Iteration 14/1000 | Loss: 0.00028845
Iteration 15/1000 | Loss: 0.00023747
Iteration 16/1000 | Loss: 0.00027473
Iteration 17/1000 | Loss: 0.00037265
Iteration 18/1000 | Loss: 0.00025080
Iteration 19/1000 | Loss: 0.00007491
Iteration 20/1000 | Loss: 0.00011906
Iteration 21/1000 | Loss: 0.00021843
Iteration 22/1000 | Loss: 0.00027887
Iteration 23/1000 | Loss: 0.00023181
Iteration 24/1000 | Loss: 0.00032010
Iteration 25/1000 | Loss: 0.00028904
Iteration 26/1000 | Loss: 0.00074295
Iteration 27/1000 | Loss: 0.00112573
Iteration 28/1000 | Loss: 0.00005764
Iteration 29/1000 | Loss: 0.00068606
Iteration 30/1000 | Loss: 0.00060446
Iteration 31/1000 | Loss: 0.00046220
Iteration 32/1000 | Loss: 0.00004589
Iteration 33/1000 | Loss: 0.00027319
Iteration 34/1000 | Loss: 0.00026950
Iteration 35/1000 | Loss: 0.00025272
Iteration 36/1000 | Loss: 0.00028366
Iteration 37/1000 | Loss: 0.00023547
Iteration 38/1000 | Loss: 0.00006011
Iteration 39/1000 | Loss: 0.00022478
Iteration 40/1000 | Loss: 0.00004541
Iteration 41/1000 | Loss: 0.00003527
Iteration 42/1000 | Loss: 0.00002980
Iteration 43/1000 | Loss: 0.00002404
Iteration 44/1000 | Loss: 0.00002269
Iteration 45/1000 | Loss: 0.00002144
Iteration 46/1000 | Loss: 0.00031401
Iteration 47/1000 | Loss: 0.00024692
Iteration 48/1000 | Loss: 0.00032226
Iteration 49/1000 | Loss: 0.00075589
Iteration 50/1000 | Loss: 0.00024143
Iteration 51/1000 | Loss: 0.00034048
Iteration 52/1000 | Loss: 0.00002294
Iteration 53/1000 | Loss: 0.00025161
Iteration 54/1000 | Loss: 0.00023327
Iteration 55/1000 | Loss: 0.00002126
Iteration 56/1000 | Loss: 0.00001983
Iteration 57/1000 | Loss: 0.00001970
Iteration 58/1000 | Loss: 0.00001928
Iteration 59/1000 | Loss: 0.00002202
Iteration 60/1000 | Loss: 0.00001847
Iteration 61/1000 | Loss: 0.00006004
Iteration 62/1000 | Loss: 0.00002686
Iteration 63/1000 | Loss: 0.00002860
Iteration 64/1000 | Loss: 0.00001929
Iteration 65/1000 | Loss: 0.00034421
Iteration 66/1000 | Loss: 0.00043517
Iteration 67/1000 | Loss: 0.00011248
Iteration 68/1000 | Loss: 0.00063713
Iteration 69/1000 | Loss: 0.00002800
Iteration 70/1000 | Loss: 0.00002019
Iteration 71/1000 | Loss: 0.00006425
Iteration 72/1000 | Loss: 0.00006501
Iteration 73/1000 | Loss: 0.00006110
Iteration 74/1000 | Loss: 0.00006100
Iteration 75/1000 | Loss: 0.00002242
Iteration 76/1000 | Loss: 0.00001852
Iteration 77/1000 | Loss: 0.00001691
Iteration 78/1000 | Loss: 0.00001970
Iteration 79/1000 | Loss: 0.00008755
Iteration 80/1000 | Loss: 0.00006107
Iteration 81/1000 | Loss: 0.00002556
Iteration 82/1000 | Loss: 0.00001691
Iteration 83/1000 | Loss: 0.00001700
Iteration 84/1000 | Loss: 0.00001854
Iteration 85/1000 | Loss: 0.00001854
Iteration 86/1000 | Loss: 0.00001854
Iteration 87/1000 | Loss: 0.00001854
Iteration 88/1000 | Loss: 0.00001854
Iteration 89/1000 | Loss: 0.00001854
Iteration 90/1000 | Loss: 0.00001854
Iteration 91/1000 | Loss: 0.00001854
Iteration 92/1000 | Loss: 0.00001854
Iteration 93/1000 | Loss: 0.00001854
Iteration 94/1000 | Loss: 0.00001854
Iteration 95/1000 | Loss: 0.00001854
Iteration 96/1000 | Loss: 0.00001854
Iteration 97/1000 | Loss: 0.00001854
Iteration 98/1000 | Loss: 0.00001854
Iteration 99/1000 | Loss: 0.00001854
Iteration 100/1000 | Loss: 0.00001854
Iteration 101/1000 | Loss: 0.00001854
Iteration 102/1000 | Loss: 0.00001854
Iteration 103/1000 | Loss: 0.00001854
Iteration 104/1000 | Loss: 0.00001854
Iteration 105/1000 | Loss: 0.00001854
Iteration 106/1000 | Loss: 0.00001854
Iteration 107/1000 | Loss: 0.00001854
Iteration 108/1000 | Loss: 0.00001854
Iteration 109/1000 | Loss: 0.00001854
Iteration 110/1000 | Loss: 0.00001854
Iteration 111/1000 | Loss: 0.00001854
Iteration 112/1000 | Loss: 0.00001854
Iteration 113/1000 | Loss: 0.00001854
Iteration 114/1000 | Loss: 0.00001854
Iteration 115/1000 | Loss: 0.00001854
Iteration 116/1000 | Loss: 0.00001854
Iteration 117/1000 | Loss: 0.00001854
Iteration 118/1000 | Loss: 0.00001854
Iteration 119/1000 | Loss: 0.00001854
Iteration 120/1000 | Loss: 0.00001854
Iteration 121/1000 | Loss: 0.00001854
Iteration 122/1000 | Loss: 0.00001854
Iteration 123/1000 | Loss: 0.00001854
Iteration 124/1000 | Loss: 0.00001854
Iteration 125/1000 | Loss: 0.00001854
Iteration 126/1000 | Loss: 0.00001854
Iteration 127/1000 | Loss: 0.00001854
Iteration 128/1000 | Loss: 0.00001854
Iteration 129/1000 | Loss: 0.00001854
Iteration 130/1000 | Loss: 0.00001854
Iteration 131/1000 | Loss: 0.00001854
Iteration 132/1000 | Loss: 0.00001854
Iteration 133/1000 | Loss: 0.00001854
Iteration 134/1000 | Loss: 0.00001854
Iteration 135/1000 | Loss: 0.00001854
Iteration 136/1000 | Loss: 0.00001854
Iteration 137/1000 | Loss: 0.00001854
Iteration 138/1000 | Loss: 0.00001854
Iteration 139/1000 | Loss: 0.00001854
Iteration 140/1000 | Loss: 0.00001854
Iteration 141/1000 | Loss: 0.00001854
Iteration 142/1000 | Loss: 0.00001854
Iteration 143/1000 | Loss: 0.00001854
Iteration 144/1000 | Loss: 0.00001854
Iteration 145/1000 | Loss: 0.00001854
Iteration 146/1000 | Loss: 0.00001854
Iteration 147/1000 | Loss: 0.00001854
Iteration 148/1000 | Loss: 0.00001854
Iteration 149/1000 | Loss: 0.00001854
Iteration 150/1000 | Loss: 0.00001854
Iteration 151/1000 | Loss: 0.00001854
Iteration 152/1000 | Loss: 0.00001854
Iteration 153/1000 | Loss: 0.00001854
Iteration 154/1000 | Loss: 0.00001854
Iteration 155/1000 | Loss: 0.00001854
Iteration 156/1000 | Loss: 0.00001854
Iteration 157/1000 | Loss: 0.00001854
Iteration 158/1000 | Loss: 0.00001854
Iteration 159/1000 | Loss: 0.00001854
Iteration 160/1000 | Loss: 0.00001854
Iteration 161/1000 | Loss: 0.00001854
Iteration 162/1000 | Loss: 0.00001854
Iteration 163/1000 | Loss: 0.00001854
Iteration 164/1000 | Loss: 0.00001854
Iteration 165/1000 | Loss: 0.00001854
Iteration 166/1000 | Loss: 0.00001854
Iteration 167/1000 | Loss: 0.00001854
Iteration 168/1000 | Loss: 0.00001854
Iteration 169/1000 | Loss: 0.00001854
Iteration 170/1000 | Loss: 0.00001854
Iteration 171/1000 | Loss: 0.00001854
Iteration 172/1000 | Loss: 0.00001854
Iteration 173/1000 | Loss: 0.00001854
Iteration 174/1000 | Loss: 0.00001854
Iteration 175/1000 | Loss: 0.00001854
Iteration 176/1000 | Loss: 0.00001854
Iteration 177/1000 | Loss: 0.00001854
Iteration 178/1000 | Loss: 0.00001854
Iteration 179/1000 | Loss: 0.00001854
Iteration 180/1000 | Loss: 0.00001854
Iteration 181/1000 | Loss: 0.00001854
Iteration 182/1000 | Loss: 0.00001854
Iteration 183/1000 | Loss: 0.00001854
Iteration 184/1000 | Loss: 0.00001854
Iteration 185/1000 | Loss: 0.00001854
Iteration 186/1000 | Loss: 0.00001854
Iteration 187/1000 | Loss: 0.00001854
Iteration 188/1000 | Loss: 0.00001854
Iteration 189/1000 | Loss: 0.00001854
Iteration 190/1000 | Loss: 0.00001854
Iteration 191/1000 | Loss: 0.00001854
Iteration 192/1000 | Loss: 0.00001854
Iteration 193/1000 | Loss: 0.00001854
Iteration 194/1000 | Loss: 0.00001854
Iteration 195/1000 | Loss: 0.00001854
Iteration 196/1000 | Loss: 0.00001854
Iteration 197/1000 | Loss: 0.00001854
Iteration 198/1000 | Loss: 0.00001854
Iteration 199/1000 | Loss: 0.00001854
Iteration 200/1000 | Loss: 0.00001854
Iteration 201/1000 | Loss: 0.00001854
Iteration 202/1000 | Loss: 0.00001854
Iteration 203/1000 | Loss: 0.00001854
Iteration 204/1000 | Loss: 0.00001854
Iteration 205/1000 | Loss: 0.00001854
Iteration 206/1000 | Loss: 0.00001854
Iteration 207/1000 | Loss: 0.00001854
Iteration 208/1000 | Loss: 0.00001854
Iteration 209/1000 | Loss: 0.00001854
Iteration 210/1000 | Loss: 0.00001854
Iteration 211/1000 | Loss: 0.00001854
Iteration 212/1000 | Loss: 0.00001854
Iteration 213/1000 | Loss: 0.00001854
Iteration 214/1000 | Loss: 0.00001854
Iteration 215/1000 | Loss: 0.00001854
Iteration 216/1000 | Loss: 0.00001854
Iteration 217/1000 | Loss: 0.00001854
Iteration 218/1000 | Loss: 0.00001854
Iteration 219/1000 | Loss: 0.00001854
Iteration 220/1000 | Loss: 0.00001854
Iteration 221/1000 | Loss: 0.00001854
Iteration 222/1000 | Loss: 0.00001854
Iteration 223/1000 | Loss: 0.00001854
Iteration 224/1000 | Loss: 0.00001854
Iteration 225/1000 | Loss: 0.00001854
Iteration 226/1000 | Loss: 0.00001854
Iteration 227/1000 | Loss: 0.00001854
Iteration 228/1000 | Loss: 0.00001854
Iteration 229/1000 | Loss: 0.00001854
Iteration 230/1000 | Loss: 0.00001854
Iteration 231/1000 | Loss: 0.00001854
Iteration 232/1000 | Loss: 0.00001854
Iteration 233/1000 | Loss: 0.00001854
Iteration 234/1000 | Loss: 0.00001854
Iteration 235/1000 | Loss: 0.00001854
Iteration 236/1000 | Loss: 0.00001854
Iteration 237/1000 | Loss: 0.00001854
Iteration 238/1000 | Loss: 0.00001854
Iteration 239/1000 | Loss: 0.00001854
Iteration 240/1000 | Loss: 0.00001854
Iteration 241/1000 | Loss: 0.00001854
Iteration 242/1000 | Loss: 0.00001854
Iteration 243/1000 | Loss: 0.00001854
Iteration 244/1000 | Loss: 0.00001854
Iteration 245/1000 | Loss: 0.00001854
Iteration 246/1000 | Loss: 0.00001854
Iteration 247/1000 | Loss: 0.00001854
Iteration 248/1000 | Loss: 0.00001854
Iteration 249/1000 | Loss: 0.00001854
Iteration 250/1000 | Loss: 0.00001854
Iteration 251/1000 | Loss: 0.00001854
Iteration 252/1000 | Loss: 0.00001854
Iteration 253/1000 | Loss: 0.00001854
Iteration 254/1000 | Loss: 0.00001854
Iteration 255/1000 | Loss: 0.00001854
Iteration 256/1000 | Loss: 0.00001854
Iteration 257/1000 | Loss: 0.00001854
Iteration 258/1000 | Loss: 0.00001854
Iteration 259/1000 | Loss: 0.00001854
Iteration 260/1000 | Loss: 0.00001854
Iteration 261/1000 | Loss: 0.00001854
Iteration 262/1000 | Loss: 0.00001854
Iteration 263/1000 | Loss: 0.00001854
Iteration 264/1000 | Loss: 0.00001854
Iteration 265/1000 | Loss: 0.00001854
Iteration 266/1000 | Loss: 0.00001853
Iteration 267/1000 | Loss: 0.00001853
Iteration 268/1000 | Loss: 0.00001853
Iteration 269/1000 | Loss: 0.00001853
Iteration 270/1000 | Loss: 0.00001853
Iteration 271/1000 | Loss: 0.00001853
Iteration 272/1000 | Loss: 0.00001853
Iteration 273/1000 | Loss: 0.00001853
Iteration 274/1000 | Loss: 0.00001853
Iteration 275/1000 | Loss: 0.00001853
Iteration 276/1000 | Loss: 0.00001853
Iteration 277/1000 | Loss: 0.00001853
Iteration 278/1000 | Loss: 0.00001853
Iteration 279/1000 | Loss: 0.00001853
Iteration 280/1000 | Loss: 0.00001853
Iteration 281/1000 | Loss: 0.00001853
Iteration 282/1000 | Loss: 0.00001853
Iteration 283/1000 | Loss: 0.00001853
Iteration 284/1000 | Loss: 0.00001853
Iteration 285/1000 | Loss: 0.00001853
Iteration 286/1000 | Loss: 0.00001853
Iteration 287/1000 | Loss: 0.00001853
Iteration 288/1000 | Loss: 0.00001853
Iteration 289/1000 | Loss: 0.00001853
Iteration 290/1000 | Loss: 0.00001853
Iteration 291/1000 | Loss: 0.00001853
Iteration 292/1000 | Loss: 0.00001853
Iteration 293/1000 | Loss: 0.00001853
Iteration 294/1000 | Loss: 0.00001853
Iteration 295/1000 | Loss: 0.00001853
Iteration 296/1000 | Loss: 0.00001853
Iteration 297/1000 | Loss: 0.00001853
Iteration 298/1000 | Loss: 0.00001853
Iteration 299/1000 | Loss: 0.00001853
Iteration 300/1000 | Loss: 0.00001853
Iteration 301/1000 | Loss: 0.00001853
Iteration 302/1000 | Loss: 0.00001853
Iteration 303/1000 | Loss: 0.00001853
Iteration 304/1000 | Loss: 0.00001853
Iteration 305/1000 | Loss: 0.00001853
Iteration 306/1000 | Loss: 0.00001853
Iteration 307/1000 | Loss: 0.00001853
Iteration 308/1000 | Loss: 0.00001853
Iteration 309/1000 | Loss: 0.00001853
Iteration 310/1000 | Loss: 0.00001853
Iteration 311/1000 | Loss: 0.00001853
Iteration 312/1000 | Loss: 0.00001853
Iteration 313/1000 | Loss: 0.00001853
Iteration 314/1000 | Loss: 0.00001853
Iteration 315/1000 | Loss: 0.00001853
Iteration 316/1000 | Loss: 0.00001853
Iteration 317/1000 | Loss: 0.00001853
Iteration 318/1000 | Loss: 0.00001853
Iteration 319/1000 | Loss: 0.00001853
Iteration 320/1000 | Loss: 0.00001853
Iteration 321/1000 | Loss: 0.00001853
Iteration 322/1000 | Loss: 0.00001853
Iteration 323/1000 | Loss: 0.00001853
Iteration 324/1000 | Loss: 0.00001853
Iteration 325/1000 | Loss: 0.00001853
Iteration 326/1000 | Loss: 0.00001853
Iteration 327/1000 | Loss: 0.00001853
Iteration 328/1000 | Loss: 0.00001853
Iteration 329/1000 | Loss: 0.00001853
Iteration 330/1000 | Loss: 0.00001853
Iteration 331/1000 | Loss: 0.00001853
Iteration 332/1000 | Loss: 0.00001853
Iteration 333/1000 | Loss: 0.00001853
Iteration 334/1000 | Loss: 0.00001853
Iteration 335/1000 | Loss: 0.00001853
Iteration 336/1000 | Loss: 0.00001853
Iteration 337/1000 | Loss: 0.00001853
Iteration 338/1000 | Loss: 0.00001853
Iteration 339/1000 | Loss: 0.00001853
Iteration 340/1000 | Loss: 0.00001853
Iteration 341/1000 | Loss: 0.00001853
Iteration 342/1000 | Loss: 0.00001853
Iteration 343/1000 | Loss: 0.00001853
Iteration 344/1000 | Loss: 0.00001853
Iteration 345/1000 | Loss: 0.00001853
Iteration 346/1000 | Loss: 0.00001853
Iteration 347/1000 | Loss: 0.00001853
Iteration 348/1000 | Loss: 0.00001853
Iteration 349/1000 | Loss: 0.00001853
Iteration 350/1000 | Loss: 0.00001853
Iteration 351/1000 | Loss: 0.00001853
Iteration 352/1000 | Loss: 0.00001853
Iteration 353/1000 | Loss: 0.00001853
Iteration 354/1000 | Loss: 0.00001853
Iteration 355/1000 | Loss: 0.00001853
Iteration 356/1000 | Loss: 0.00001853
Iteration 357/1000 | Loss: 0.00001853
Iteration 358/1000 | Loss: 0.00001853
Iteration 359/1000 | Loss: 0.00001853
Iteration 360/1000 | Loss: 0.00001853
Iteration 361/1000 | Loss: 0.00001853
Iteration 362/1000 | Loss: 0.00001853
Iteration 363/1000 | Loss: 0.00001853
Iteration 364/1000 | Loss: 0.00001853
Iteration 365/1000 | Loss: 0.00001853
Iteration 366/1000 | Loss: 0.00001853
Iteration 367/1000 | Loss: 0.00001853
Iteration 368/1000 | Loss: 0.00001853
Iteration 369/1000 | Loss: 0.00001853
Iteration 370/1000 | Loss: 0.00001853
Iteration 371/1000 | Loss: 0.00001853
Iteration 372/1000 | Loss: 0.00001853
Iteration 373/1000 | Loss: 0.00001853
Iteration 374/1000 | Loss: 0.00001853
Iteration 375/1000 | Loss: 0.00001853
Iteration 376/1000 | Loss: 0.00001853
Iteration 377/1000 | Loss: 0.00001853
Iteration 378/1000 | Loss: 0.00001853
Iteration 379/1000 | Loss: 0.00001853
Iteration 380/1000 | Loss: 0.00001853
Iteration 381/1000 | Loss: 0.00001853
Iteration 382/1000 | Loss: 0.00001853
Iteration 383/1000 | Loss: 0.00001853
Iteration 384/1000 | Loss: 0.00001853
Iteration 385/1000 | Loss: 0.00001853
Iteration 386/1000 | Loss: 0.00001853
Iteration 387/1000 | Loss: 0.00001853
Iteration 388/1000 | Loss: 0.00001853
Iteration 389/1000 | Loss: 0.00001853
Iteration 390/1000 | Loss: 0.00001853
Iteration 391/1000 | Loss: 0.00001853
Iteration 392/1000 | Loss: 0.00001853
Iteration 393/1000 | Loss: 0.00001853
Iteration 394/1000 | Loss: 0.00001853
Iteration 395/1000 | Loss: 0.00001853
Iteration 396/1000 | Loss: 0.00001853
Iteration 397/1000 | Loss: 0.00001853
Iteration 398/1000 | Loss: 0.00001853
Iteration 399/1000 | Loss: 0.00001853
Iteration 400/1000 | Loss: 0.00001853
Iteration 401/1000 | Loss: 0.00001853
Iteration 402/1000 | Loss: 0.00001853
Iteration 403/1000 | Loss: 0.00001853
Iteration 404/1000 | Loss: 0.00001853
Iteration 405/1000 | Loss: 0.00001853
Iteration 406/1000 | Loss: 0.00001853
Iteration 407/1000 | Loss: 0.00001853
Iteration 408/1000 | Loss: 0.00001853
Iteration 409/1000 | Loss: 0.00001853
Iteration 410/1000 | Loss: 0.00001853
Iteration 411/1000 | Loss: 0.00001853
Iteration 412/1000 | Loss: 0.00001853
Iteration 413/1000 | Loss: 0.00001853
Iteration 414/1000 | Loss: 0.00001853
Iteration 415/1000 | Loss: 0.00001853
Iteration 416/1000 | Loss: 0.00001853
Iteration 417/1000 | Loss: 0.00001853
Iteration 418/1000 | Loss: 0.00001853
Iteration 419/1000 | Loss: 0.00001853
Iteration 420/1000 | Loss: 0.00001853
Iteration 421/1000 | Loss: 0.00001853
Iteration 422/1000 | Loss: 0.00001853
Iteration 423/1000 | Loss: 0.00001853
Iteration 424/1000 | Loss: 0.00001853
Iteration 425/1000 | Loss: 0.00001853
Iteration 426/1000 | Loss: 0.00001853
Iteration 427/1000 | Loss: 0.00001853
Iteration 428/1000 | Loss: 0.00001853
Iteration 429/1000 | Loss: 0.00001853
Iteration 430/1000 | Loss: 0.00001853
Iteration 431/1000 | Loss: 0.00001853
Iteration 432/1000 | Loss: 0.00001853
Iteration 433/1000 | Loss: 0.00001853
Iteration 434/1000 | Loss: 0.00001853
Iteration 435/1000 | Loss: 0.00001853
Iteration 436/1000 | Loss: 0.00001853
Iteration 437/1000 | Loss: 0.00001853
Iteration 438/1000 | Loss: 0.00001853
Iteration 439/1000 | Loss: 0.00001853
Iteration 440/1000 | Loss: 0.00001853
Iteration 441/1000 | Loss: 0.00001853
Iteration 442/1000 | Loss: 0.00001853
Iteration 443/1000 | Loss: 0.00001853
Iteration 444/1000 | Loss: 0.00001853
Iteration 445/1000 | Loss: 0.00001853
Iteration 446/1000 | Loss: 0.00001853
Iteration 447/1000 | Loss: 0.00001853
Iteration 448/1000 | Loss: 0.00001853
Iteration 449/1000 | Loss: 0.00001853
Iteration 450/1000 | Loss: 0.00001853
Iteration 451/1000 | Loss: 0.00001853
Iteration 452/1000 | Loss: 0.00001853
Iteration 453/1000 | Loss: 0.00001853
Iteration 454/1000 | Loss: 0.00001853
Iteration 455/1000 | Loss: 0.00001853
Iteration 456/1000 | Loss: 0.00001853
Iteration 457/1000 | Loss: 0.00001853
Iteration 458/1000 | Loss: 0.00001853
Iteration 459/1000 | Loss: 0.00001853
Iteration 460/1000 | Loss: 0.00001853
Iteration 461/1000 | Loss: 0.00001853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 461. Stopping optimization.
Last 5 losses: [1.8533299225964583e-05, 1.8533299225964583e-05, 1.8533299225964583e-05, 1.8533299225964583e-05, 1.8533299225964583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8533299225964583e-05

Optimization complete. Final v2v error: 3.388549327850342 mm

Highest mean error: 15.314715385437012 mm for frame 119

Lowest mean error: 2.703009843826294 mm for frame 103

Saving results

Total time: 180.99131989479065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791551
Iteration 2/25 | Loss: 0.00146730
Iteration 3/25 | Loss: 0.00102041
Iteration 4/25 | Loss: 0.00094528
Iteration 5/25 | Loss: 0.00092890
Iteration 6/25 | Loss: 0.00090454
Iteration 7/25 | Loss: 0.00089999
Iteration 8/25 | Loss: 0.00089701
Iteration 9/25 | Loss: 0.00089378
Iteration 10/25 | Loss: 0.00089123
Iteration 11/25 | Loss: 0.00088966
Iteration 12/25 | Loss: 0.00088486
Iteration 13/25 | Loss: 0.00088397
Iteration 14/25 | Loss: 0.00088350
Iteration 15/25 | Loss: 0.00088331
Iteration 16/25 | Loss: 0.00088322
Iteration 17/25 | Loss: 0.00088322
Iteration 18/25 | Loss: 0.00088322
Iteration 19/25 | Loss: 0.00088322
Iteration 20/25 | Loss: 0.00088322
Iteration 21/25 | Loss: 0.00088322
Iteration 22/25 | Loss: 0.00088322
Iteration 23/25 | Loss: 0.00088322
Iteration 24/25 | Loss: 0.00088321
Iteration 25/25 | Loss: 0.00088321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.91746616
Iteration 2/25 | Loss: 0.00048696
Iteration 3/25 | Loss: 0.00048683
Iteration 4/25 | Loss: 0.00048683
Iteration 5/25 | Loss: 0.00048683
Iteration 6/25 | Loss: 0.00048683
Iteration 7/25 | Loss: 0.00048683
Iteration 8/25 | Loss: 0.00048683
Iteration 9/25 | Loss: 0.00048683
Iteration 10/25 | Loss: 0.00048683
Iteration 11/25 | Loss: 0.00048683
Iteration 12/25 | Loss: 0.00048683
Iteration 13/25 | Loss: 0.00048683
Iteration 14/25 | Loss: 0.00048683
Iteration 15/25 | Loss: 0.00048683
Iteration 16/25 | Loss: 0.00048683
Iteration 17/25 | Loss: 0.00048683
Iteration 18/25 | Loss: 0.00048683
Iteration 19/25 | Loss: 0.00048683
Iteration 20/25 | Loss: 0.00048683
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00048682853230275214, 0.00048682853230275214, 0.00048682853230275214, 0.00048682853230275214, 0.00048682853230275214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00048682853230275214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048683
Iteration 2/1000 | Loss: 0.00004474
Iteration 3/1000 | Loss: 0.00002857
Iteration 4/1000 | Loss: 0.00002448
Iteration 5/1000 | Loss: 0.00002257
Iteration 6/1000 | Loss: 0.00002144
Iteration 7/1000 | Loss: 0.00002069
Iteration 8/1000 | Loss: 0.00002020
Iteration 9/1000 | Loss: 0.00001990
Iteration 10/1000 | Loss: 0.00001968
Iteration 11/1000 | Loss: 0.00001949
Iteration 12/1000 | Loss: 0.00001940
Iteration 13/1000 | Loss: 0.00001936
Iteration 14/1000 | Loss: 0.00001935
Iteration 15/1000 | Loss: 0.00001934
Iteration 16/1000 | Loss: 0.00001934
Iteration 17/1000 | Loss: 0.00001930
Iteration 18/1000 | Loss: 0.00001930
Iteration 19/1000 | Loss: 0.00001926
Iteration 20/1000 | Loss: 0.00001924
Iteration 21/1000 | Loss: 0.00001923
Iteration 22/1000 | Loss: 0.00001923
Iteration 23/1000 | Loss: 0.00001922
Iteration 24/1000 | Loss: 0.00001922
Iteration 25/1000 | Loss: 0.00001921
Iteration 26/1000 | Loss: 0.00001921
Iteration 27/1000 | Loss: 0.00001921
Iteration 28/1000 | Loss: 0.00001920
Iteration 29/1000 | Loss: 0.00001919
Iteration 30/1000 | Loss: 0.00001918
Iteration 31/1000 | Loss: 0.00001918
Iteration 32/1000 | Loss: 0.00001917
Iteration 33/1000 | Loss: 0.00001917
Iteration 34/1000 | Loss: 0.00001916
Iteration 35/1000 | Loss: 0.00001915
Iteration 36/1000 | Loss: 0.00001915
Iteration 37/1000 | Loss: 0.00001914
Iteration 38/1000 | Loss: 0.00001914
Iteration 39/1000 | Loss: 0.00001914
Iteration 40/1000 | Loss: 0.00001914
Iteration 41/1000 | Loss: 0.00001914
Iteration 42/1000 | Loss: 0.00001913
Iteration 43/1000 | Loss: 0.00001913
Iteration 44/1000 | Loss: 0.00001912
Iteration 45/1000 | Loss: 0.00001912
Iteration 46/1000 | Loss: 0.00001911
Iteration 47/1000 | Loss: 0.00001911
Iteration 48/1000 | Loss: 0.00001911
Iteration 49/1000 | Loss: 0.00001910
Iteration 50/1000 | Loss: 0.00001910
Iteration 51/1000 | Loss: 0.00001910
Iteration 52/1000 | Loss: 0.00001910
Iteration 53/1000 | Loss: 0.00001909
Iteration 54/1000 | Loss: 0.00001909
Iteration 55/1000 | Loss: 0.00001909
Iteration 56/1000 | Loss: 0.00001909
Iteration 57/1000 | Loss: 0.00001909
Iteration 58/1000 | Loss: 0.00001909
Iteration 59/1000 | Loss: 0.00001908
Iteration 60/1000 | Loss: 0.00001908
Iteration 61/1000 | Loss: 0.00001908
Iteration 62/1000 | Loss: 0.00001907
Iteration 63/1000 | Loss: 0.00001907
Iteration 64/1000 | Loss: 0.00001907
Iteration 65/1000 | Loss: 0.00001907
Iteration 66/1000 | Loss: 0.00001906
Iteration 67/1000 | Loss: 0.00001906
Iteration 68/1000 | Loss: 0.00001906
Iteration 69/1000 | Loss: 0.00001905
Iteration 70/1000 | Loss: 0.00001905
Iteration 71/1000 | Loss: 0.00001905
Iteration 72/1000 | Loss: 0.00001905
Iteration 73/1000 | Loss: 0.00001904
Iteration 74/1000 | Loss: 0.00001904
Iteration 75/1000 | Loss: 0.00001904
Iteration 76/1000 | Loss: 0.00001904
Iteration 77/1000 | Loss: 0.00001904
Iteration 78/1000 | Loss: 0.00001904
Iteration 79/1000 | Loss: 0.00001904
Iteration 80/1000 | Loss: 0.00001904
Iteration 81/1000 | Loss: 0.00001904
Iteration 82/1000 | Loss: 0.00001904
Iteration 83/1000 | Loss: 0.00001904
Iteration 84/1000 | Loss: 0.00001904
Iteration 85/1000 | Loss: 0.00001903
Iteration 86/1000 | Loss: 0.00001903
Iteration 87/1000 | Loss: 0.00001903
Iteration 88/1000 | Loss: 0.00001903
Iteration 89/1000 | Loss: 0.00001903
Iteration 90/1000 | Loss: 0.00001903
Iteration 91/1000 | Loss: 0.00001903
Iteration 92/1000 | Loss: 0.00001902
Iteration 93/1000 | Loss: 0.00001902
Iteration 94/1000 | Loss: 0.00001902
Iteration 95/1000 | Loss: 0.00001902
Iteration 96/1000 | Loss: 0.00001902
Iteration 97/1000 | Loss: 0.00001902
Iteration 98/1000 | Loss: 0.00001902
Iteration 99/1000 | Loss: 0.00001901
Iteration 100/1000 | Loss: 0.00001901
Iteration 101/1000 | Loss: 0.00001901
Iteration 102/1000 | Loss: 0.00001901
Iteration 103/1000 | Loss: 0.00001900
Iteration 104/1000 | Loss: 0.00001900
Iteration 105/1000 | Loss: 0.00001900
Iteration 106/1000 | Loss: 0.00001900
Iteration 107/1000 | Loss: 0.00001900
Iteration 108/1000 | Loss: 0.00001900
Iteration 109/1000 | Loss: 0.00001900
Iteration 110/1000 | Loss: 0.00001900
Iteration 111/1000 | Loss: 0.00001899
Iteration 112/1000 | Loss: 0.00001899
Iteration 113/1000 | Loss: 0.00001899
Iteration 114/1000 | Loss: 0.00001899
Iteration 115/1000 | Loss: 0.00001898
Iteration 116/1000 | Loss: 0.00001898
Iteration 117/1000 | Loss: 0.00001898
Iteration 118/1000 | Loss: 0.00001898
Iteration 119/1000 | Loss: 0.00001898
Iteration 120/1000 | Loss: 0.00001898
Iteration 121/1000 | Loss: 0.00001897
Iteration 122/1000 | Loss: 0.00001897
Iteration 123/1000 | Loss: 0.00001897
Iteration 124/1000 | Loss: 0.00001897
Iteration 125/1000 | Loss: 0.00001896
Iteration 126/1000 | Loss: 0.00001896
Iteration 127/1000 | Loss: 0.00001896
Iteration 128/1000 | Loss: 0.00001896
Iteration 129/1000 | Loss: 0.00001895
Iteration 130/1000 | Loss: 0.00001895
Iteration 131/1000 | Loss: 0.00001895
Iteration 132/1000 | Loss: 0.00001895
Iteration 133/1000 | Loss: 0.00001895
Iteration 134/1000 | Loss: 0.00001894
Iteration 135/1000 | Loss: 0.00001894
Iteration 136/1000 | Loss: 0.00001894
Iteration 137/1000 | Loss: 0.00001894
Iteration 138/1000 | Loss: 0.00001894
Iteration 139/1000 | Loss: 0.00001894
Iteration 140/1000 | Loss: 0.00001894
Iteration 141/1000 | Loss: 0.00001894
Iteration 142/1000 | Loss: 0.00001894
Iteration 143/1000 | Loss: 0.00001894
Iteration 144/1000 | Loss: 0.00001894
Iteration 145/1000 | Loss: 0.00001893
Iteration 146/1000 | Loss: 0.00001893
Iteration 147/1000 | Loss: 0.00001893
Iteration 148/1000 | Loss: 0.00001893
Iteration 149/1000 | Loss: 0.00001893
Iteration 150/1000 | Loss: 0.00001893
Iteration 151/1000 | Loss: 0.00001893
Iteration 152/1000 | Loss: 0.00001893
Iteration 153/1000 | Loss: 0.00001893
Iteration 154/1000 | Loss: 0.00001893
Iteration 155/1000 | Loss: 0.00001893
Iteration 156/1000 | Loss: 0.00001892
Iteration 157/1000 | Loss: 0.00001892
Iteration 158/1000 | Loss: 0.00001892
Iteration 159/1000 | Loss: 0.00001892
Iteration 160/1000 | Loss: 0.00001892
Iteration 161/1000 | Loss: 0.00001892
Iteration 162/1000 | Loss: 0.00001892
Iteration 163/1000 | Loss: 0.00001892
Iteration 164/1000 | Loss: 0.00001892
Iteration 165/1000 | Loss: 0.00001892
Iteration 166/1000 | Loss: 0.00001892
Iteration 167/1000 | Loss: 0.00001892
Iteration 168/1000 | Loss: 0.00001892
Iteration 169/1000 | Loss: 0.00001892
Iteration 170/1000 | Loss: 0.00001892
Iteration 171/1000 | Loss: 0.00001892
Iteration 172/1000 | Loss: 0.00001892
Iteration 173/1000 | Loss: 0.00001892
Iteration 174/1000 | Loss: 0.00001892
Iteration 175/1000 | Loss: 0.00001892
Iteration 176/1000 | Loss: 0.00001892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.891938882181421e-05, 1.891938882181421e-05, 1.891938882181421e-05, 1.891938882181421e-05, 1.891938882181421e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.891938882181421e-05

Optimization complete. Final v2v error: 3.644751787185669 mm

Highest mean error: 4.4556097984313965 mm for frame 129

Lowest mean error: 3.1236684322357178 mm for frame 90

Saving results

Total time: 65.19590902328491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_6070/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_6070/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964404
Iteration 2/25 | Loss: 0.00159379
Iteration 3/25 | Loss: 0.00110466
Iteration 4/25 | Loss: 0.00104655
Iteration 5/25 | Loss: 0.00102538
Iteration 6/25 | Loss: 0.00101300
Iteration 7/25 | Loss: 0.00100922
Iteration 8/25 | Loss: 0.00101600
Iteration 9/25 | Loss: 0.00100109
Iteration 10/25 | Loss: 0.00100136
Iteration 11/25 | Loss: 0.00100426
Iteration 12/25 | Loss: 0.00099819
Iteration 13/25 | Loss: 0.00100862
Iteration 14/25 | Loss: 0.00099901
Iteration 15/25 | Loss: 0.00099245
Iteration 16/25 | Loss: 0.00098579
Iteration 17/25 | Loss: 0.00097890
Iteration 18/25 | Loss: 0.00097740
Iteration 19/25 | Loss: 0.00101439
Iteration 20/25 | Loss: 0.00104008
Iteration 21/25 | Loss: 0.00102096
Iteration 22/25 | Loss: 0.00103748
Iteration 23/25 | Loss: 0.00101942
Iteration 24/25 | Loss: 0.00100888
Iteration 25/25 | Loss: 0.00100806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45657468
Iteration 2/25 | Loss: 0.00083971
Iteration 3/25 | Loss: 0.00083959
Iteration 4/25 | Loss: 0.00083959
Iteration 5/25 | Loss: 0.00083959
Iteration 6/25 | Loss: 0.00083959
Iteration 7/25 | Loss: 0.00083959
Iteration 8/25 | Loss: 0.00083959
Iteration 9/25 | Loss: 0.00083959
Iteration 10/25 | Loss: 0.00083959
Iteration 11/25 | Loss: 0.00083959
Iteration 12/25 | Loss: 0.00083958
Iteration 13/25 | Loss: 0.00083958
Iteration 14/25 | Loss: 0.00083958
Iteration 15/25 | Loss: 0.00083958
Iteration 16/25 | Loss: 0.00083958
Iteration 17/25 | Loss: 0.00083958
Iteration 18/25 | Loss: 0.00083958
Iteration 19/25 | Loss: 0.00083958
Iteration 20/25 | Loss: 0.00083958
Iteration 21/25 | Loss: 0.00083958
Iteration 22/25 | Loss: 0.00083958
Iteration 23/25 | Loss: 0.00083958
Iteration 24/25 | Loss: 0.00083958
Iteration 25/25 | Loss: 0.00083958

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083958
Iteration 2/1000 | Loss: 0.00646235
Iteration 3/1000 | Loss: 0.00235106
Iteration 4/1000 | Loss: 0.00511515
Iteration 5/1000 | Loss: 0.00321697
Iteration 6/1000 | Loss: 0.00564755
Iteration 7/1000 | Loss: 0.00343963
Iteration 8/1000 | Loss: 0.00345271
Iteration 9/1000 | Loss: 0.00303162
Iteration 10/1000 | Loss: 0.00529067
Iteration 11/1000 | Loss: 0.00443078
Iteration 12/1000 | Loss: 0.00318891
Iteration 13/1000 | Loss: 0.00084078
Iteration 14/1000 | Loss: 0.00051016
Iteration 15/1000 | Loss: 0.00048233
Iteration 16/1000 | Loss: 0.00050637
Iteration 17/1000 | Loss: 0.00063197
Iteration 18/1000 | Loss: 0.00064593
Iteration 19/1000 | Loss: 0.00049505
Iteration 20/1000 | Loss: 0.00047700
Iteration 21/1000 | Loss: 0.00038314
Iteration 22/1000 | Loss: 0.00075914
Iteration 23/1000 | Loss: 0.00047734
Iteration 24/1000 | Loss: 0.00056444
Iteration 25/1000 | Loss: 0.00041268
Iteration 26/1000 | Loss: 0.00070203
Iteration 27/1000 | Loss: 0.00061397
Iteration 28/1000 | Loss: 0.00061878
Iteration 29/1000 | Loss: 0.00046112
Iteration 30/1000 | Loss: 0.00063872
Iteration 31/1000 | Loss: 0.00032698
Iteration 32/1000 | Loss: 0.00050315
Iteration 33/1000 | Loss: 0.00053067
Iteration 34/1000 | Loss: 0.00076813
Iteration 35/1000 | Loss: 0.00063598
Iteration 36/1000 | Loss: 0.00066796
Iteration 37/1000 | Loss: 0.00062943
Iteration 38/1000 | Loss: 0.00077002
Iteration 39/1000 | Loss: 0.00062456
Iteration 40/1000 | Loss: 0.00047344
Iteration 41/1000 | Loss: 0.00022626
Iteration 42/1000 | Loss: 0.00054982
Iteration 43/1000 | Loss: 0.00053999
Iteration 44/1000 | Loss: 0.00066602
Iteration 45/1000 | Loss: 0.00043301
Iteration 46/1000 | Loss: 0.00045263
Iteration 47/1000 | Loss: 0.00063027
Iteration 48/1000 | Loss: 0.00037494
Iteration 49/1000 | Loss: 0.00054424
Iteration 50/1000 | Loss: 0.00037823
Iteration 51/1000 | Loss: 0.00048625
Iteration 52/1000 | Loss: 0.00062086
Iteration 53/1000 | Loss: 0.00082823
Iteration 54/1000 | Loss: 0.00046748
Iteration 55/1000 | Loss: 0.00079570
Iteration 56/1000 | Loss: 0.00055191
Iteration 57/1000 | Loss: 0.00059819
Iteration 58/1000 | Loss: 0.00087564
Iteration 59/1000 | Loss: 0.00047660
Iteration 60/1000 | Loss: 0.00062551
Iteration 61/1000 | Loss: 0.00042340
Iteration 62/1000 | Loss: 0.00067220
Iteration 63/1000 | Loss: 0.00063415
Iteration 64/1000 | Loss: 0.00028726
Iteration 65/1000 | Loss: 0.00051839
Iteration 66/1000 | Loss: 0.00044843
Iteration 67/1000 | Loss: 0.00048049
Iteration 68/1000 | Loss: 0.00098290
Iteration 69/1000 | Loss: 0.00058392
Iteration 70/1000 | Loss: 0.00051085
Iteration 71/1000 | Loss: 0.00060966
Iteration 72/1000 | Loss: 0.00059828
Iteration 73/1000 | Loss: 0.00045437
Iteration 74/1000 | Loss: 0.00056648
Iteration 75/1000 | Loss: 0.00057409
Iteration 76/1000 | Loss: 0.00067884
Iteration 77/1000 | Loss: 0.00057401
Iteration 78/1000 | Loss: 0.00062440
Iteration 79/1000 | Loss: 0.00052081
Iteration 80/1000 | Loss: 0.00046709
Iteration 81/1000 | Loss: 0.00064192
Iteration 82/1000 | Loss: 0.00126700
Iteration 83/1000 | Loss: 0.00116677
Iteration 84/1000 | Loss: 0.00124214
Iteration 85/1000 | Loss: 0.00123708
Iteration 86/1000 | Loss: 0.00118020
Iteration 87/1000 | Loss: 0.00098374
Iteration 88/1000 | Loss: 0.00133896
Iteration 89/1000 | Loss: 0.00094989
Iteration 90/1000 | Loss: 0.00054735
Iteration 91/1000 | Loss: 0.00083489
Iteration 92/1000 | Loss: 0.00056338
Iteration 93/1000 | Loss: 0.00061099
Iteration 94/1000 | Loss: 0.00159146
Iteration 95/1000 | Loss: 0.00053683
Iteration 96/1000 | Loss: 0.00047261
Iteration 97/1000 | Loss: 0.00096794
Iteration 98/1000 | Loss: 0.00047375
Iteration 99/1000 | Loss: 0.00063279
Iteration 100/1000 | Loss: 0.00058901
Iteration 101/1000 | Loss: 0.00049419
Iteration 102/1000 | Loss: 0.00028461
Iteration 103/1000 | Loss: 0.00031860
Iteration 104/1000 | Loss: 0.00032704
Iteration 105/1000 | Loss: 0.00042579
Iteration 106/1000 | Loss: 0.00043440
Iteration 107/1000 | Loss: 0.00041428
Iteration 108/1000 | Loss: 0.00042977
Iteration 109/1000 | Loss: 0.00025066
Iteration 110/1000 | Loss: 0.00032883
Iteration 111/1000 | Loss: 0.00029592
Iteration 112/1000 | Loss: 0.00019950
Iteration 113/1000 | Loss: 0.00025093
Iteration 114/1000 | Loss: 0.00027155
Iteration 115/1000 | Loss: 0.00029142
Iteration 116/1000 | Loss: 0.00027688
Iteration 117/1000 | Loss: 0.00037416
Iteration 118/1000 | Loss: 0.00031124
Iteration 119/1000 | Loss: 0.00020262
Iteration 120/1000 | Loss: 0.00024504
Iteration 121/1000 | Loss: 0.00036204
Iteration 122/1000 | Loss: 0.00041737
Iteration 123/1000 | Loss: 0.00044398
Iteration 124/1000 | Loss: 0.00041498
Iteration 125/1000 | Loss: 0.00040116
Iteration 126/1000 | Loss: 0.00041680
Iteration 127/1000 | Loss: 0.00039718
Iteration 128/1000 | Loss: 0.00038528
Iteration 129/1000 | Loss: 0.00039457
Iteration 130/1000 | Loss: 0.00028495
Iteration 131/1000 | Loss: 0.00041663
Iteration 132/1000 | Loss: 0.00032034
Iteration 133/1000 | Loss: 0.00032385
Iteration 134/1000 | Loss: 0.00022247
Iteration 135/1000 | Loss: 0.00025715
Iteration 136/1000 | Loss: 0.00027142
Iteration 137/1000 | Loss: 0.00011678
Iteration 138/1000 | Loss: 0.00015068
Iteration 139/1000 | Loss: 0.00004508
Iteration 140/1000 | Loss: 0.00013774
Iteration 141/1000 | Loss: 0.00022227
Iteration 142/1000 | Loss: 0.00040291
Iteration 143/1000 | Loss: 0.00016261
Iteration 144/1000 | Loss: 0.00013020
Iteration 145/1000 | Loss: 0.00017120
Iteration 146/1000 | Loss: 0.00031711
Iteration 147/1000 | Loss: 0.00029897
Iteration 148/1000 | Loss: 0.00038345
Iteration 149/1000 | Loss: 0.00043385
Iteration 150/1000 | Loss: 0.00041594
Iteration 151/1000 | Loss: 0.00034384
Iteration 152/1000 | Loss: 0.00053090
Iteration 153/1000 | Loss: 0.00054679
Iteration 154/1000 | Loss: 0.00031577
Iteration 155/1000 | Loss: 0.00038353
Iteration 156/1000 | Loss: 0.00014709
Iteration 157/1000 | Loss: 0.00015413
Iteration 158/1000 | Loss: 0.00037269
Iteration 159/1000 | Loss: 0.00042622
Iteration 160/1000 | Loss: 0.00046447
Iteration 161/1000 | Loss: 0.00039457
Iteration 162/1000 | Loss: 0.00053793
Iteration 163/1000 | Loss: 0.00029613
Iteration 164/1000 | Loss: 0.00032052
Iteration 165/1000 | Loss: 0.00039060
Iteration 166/1000 | Loss: 0.00015950
Iteration 167/1000 | Loss: 0.00039803
Iteration 168/1000 | Loss: 0.00039003
Iteration 169/1000 | Loss: 0.00029073
Iteration 170/1000 | Loss: 0.00050532
Iteration 171/1000 | Loss: 0.00018430
Iteration 172/1000 | Loss: 0.00022305
Iteration 173/1000 | Loss: 0.00038893
Iteration 174/1000 | Loss: 0.00015214
Iteration 175/1000 | Loss: 0.00029087
Iteration 176/1000 | Loss: 0.00039675
Iteration 177/1000 | Loss: 0.00032549
Iteration 178/1000 | Loss: 0.00018595
Iteration 179/1000 | Loss: 0.00023977
Iteration 180/1000 | Loss: 0.00015736
Iteration 181/1000 | Loss: 0.00008744
Iteration 182/1000 | Loss: 0.00033492
Iteration 183/1000 | Loss: 0.00074337
Iteration 184/1000 | Loss: 0.00039188
Iteration 185/1000 | Loss: 0.00028884
Iteration 186/1000 | Loss: 0.00013992
Iteration 187/1000 | Loss: 0.00026394
Iteration 188/1000 | Loss: 0.00061291
Iteration 189/1000 | Loss: 0.00062802
Iteration 190/1000 | Loss: 0.00082500
Iteration 191/1000 | Loss: 0.00083316
Iteration 192/1000 | Loss: 0.00037710
Iteration 193/1000 | Loss: 0.00028939
Iteration 194/1000 | Loss: 0.00070502
Iteration 195/1000 | Loss: 0.00031485
Iteration 196/1000 | Loss: 0.00008428
Iteration 197/1000 | Loss: 0.00032630
Iteration 198/1000 | Loss: 0.00027546
Iteration 199/1000 | Loss: 0.00015995
Iteration 200/1000 | Loss: 0.00013362
Iteration 201/1000 | Loss: 0.00045514
Iteration 202/1000 | Loss: 0.00026065
Iteration 203/1000 | Loss: 0.00006256
Iteration 204/1000 | Loss: 0.00025460
Iteration 205/1000 | Loss: 0.00019617
Iteration 206/1000 | Loss: 0.00023911
Iteration 207/1000 | Loss: 0.00018931
Iteration 208/1000 | Loss: 0.00047126
Iteration 209/1000 | Loss: 0.00037644
Iteration 210/1000 | Loss: 0.00015407
Iteration 211/1000 | Loss: 0.00009153
Iteration 212/1000 | Loss: 0.00008936
Iteration 213/1000 | Loss: 0.00010841
Iteration 214/1000 | Loss: 0.00008370
Iteration 215/1000 | Loss: 0.00007278
Iteration 216/1000 | Loss: 0.00020619
Iteration 217/1000 | Loss: 0.00017753
Iteration 218/1000 | Loss: 0.00020058
Iteration 219/1000 | Loss: 0.00018501
Iteration 220/1000 | Loss: 0.00007119
Iteration 221/1000 | Loss: 0.00006724
Iteration 222/1000 | Loss: 0.00025896
Iteration 223/1000 | Loss: 0.00023322
Iteration 224/1000 | Loss: 0.00018529
Iteration 225/1000 | Loss: 0.00017649
Iteration 226/1000 | Loss: 0.00016327
Iteration 227/1000 | Loss: 0.00026679
Iteration 228/1000 | Loss: 0.00012267
Iteration 229/1000 | Loss: 0.00043786
Iteration 230/1000 | Loss: 0.00033421
Iteration 231/1000 | Loss: 0.00005434
Iteration 232/1000 | Loss: 0.00021881
Iteration 233/1000 | Loss: 0.00020133
Iteration 234/1000 | Loss: 0.00006843
Iteration 235/1000 | Loss: 0.00008173
Iteration 236/1000 | Loss: 0.00017064
Iteration 237/1000 | Loss: 0.00017827
Iteration 238/1000 | Loss: 0.00016939
Iteration 239/1000 | Loss: 0.00006110
Iteration 240/1000 | Loss: 0.00013502
Iteration 241/1000 | Loss: 0.00014094
Iteration 242/1000 | Loss: 0.00013332
Iteration 243/1000 | Loss: 0.00021859
Iteration 244/1000 | Loss: 0.00013045
Iteration 245/1000 | Loss: 0.00021453
Iteration 246/1000 | Loss: 0.00014765
Iteration 247/1000 | Loss: 0.00021795
Iteration 248/1000 | Loss: 0.00034513
Iteration 249/1000 | Loss: 0.00013500
Iteration 250/1000 | Loss: 0.00013174
Iteration 251/1000 | Loss: 0.00022321
Iteration 252/1000 | Loss: 0.00017964
Iteration 253/1000 | Loss: 0.00024803
Iteration 254/1000 | Loss: 0.00022535
Iteration 255/1000 | Loss: 0.00015390
Iteration 256/1000 | Loss: 0.00018148
Iteration 257/1000 | Loss: 0.00022396
Iteration 258/1000 | Loss: 0.00007835
Iteration 259/1000 | Loss: 0.00007978
Iteration 260/1000 | Loss: 0.00003628
Iteration 261/1000 | Loss: 0.00003320
Iteration 262/1000 | Loss: 0.00006911
Iteration 263/1000 | Loss: 0.00005168
Iteration 264/1000 | Loss: 0.00006822
Iteration 265/1000 | Loss: 0.00007364
Iteration 266/1000 | Loss: 0.00006927
Iteration 267/1000 | Loss: 0.00005002
Iteration 268/1000 | Loss: 0.00006925
Iteration 269/1000 | Loss: 0.00005190
Iteration 270/1000 | Loss: 0.00003174
Iteration 271/1000 | Loss: 0.00005177
Iteration 272/1000 | Loss: 0.00002798
Iteration 273/1000 | Loss: 0.00003294
Iteration 274/1000 | Loss: 0.00002759
Iteration 275/1000 | Loss: 0.00002701
Iteration 276/1000 | Loss: 0.00002659
Iteration 277/1000 | Loss: 0.00003630
Iteration 278/1000 | Loss: 0.00002683
Iteration 279/1000 | Loss: 0.00002626
Iteration 280/1000 | Loss: 0.00002579
Iteration 281/1000 | Loss: 0.00002544
Iteration 282/1000 | Loss: 0.00002505
Iteration 283/1000 | Loss: 0.00002498
Iteration 284/1000 | Loss: 0.00002493
Iteration 285/1000 | Loss: 0.00002469
Iteration 286/1000 | Loss: 0.00002462
Iteration 287/1000 | Loss: 0.00002455
Iteration 288/1000 | Loss: 0.00002452
Iteration 289/1000 | Loss: 0.00002435
Iteration 290/1000 | Loss: 0.00002433
Iteration 291/1000 | Loss: 0.00002432
Iteration 292/1000 | Loss: 0.00002432
Iteration 293/1000 | Loss: 0.00002430
Iteration 294/1000 | Loss: 0.00002430
Iteration 295/1000 | Loss: 0.00002427
Iteration 296/1000 | Loss: 0.00002425
Iteration 297/1000 | Loss: 0.00002424
Iteration 298/1000 | Loss: 0.00002424
Iteration 299/1000 | Loss: 0.00002424
Iteration 300/1000 | Loss: 0.00002424
Iteration 301/1000 | Loss: 0.00002423
Iteration 302/1000 | Loss: 0.00002423
Iteration 303/1000 | Loss: 0.00002422
Iteration 304/1000 | Loss: 0.00002422
Iteration 305/1000 | Loss: 0.00002421
Iteration 306/1000 | Loss: 0.00002421
Iteration 307/1000 | Loss: 0.00002421
Iteration 308/1000 | Loss: 0.00002420
Iteration 309/1000 | Loss: 0.00002420
Iteration 310/1000 | Loss: 0.00002419
Iteration 311/1000 | Loss: 0.00002419
Iteration 312/1000 | Loss: 0.00002419
Iteration 313/1000 | Loss: 0.00002418
Iteration 314/1000 | Loss: 0.00002417
Iteration 315/1000 | Loss: 0.00002417
Iteration 316/1000 | Loss: 0.00002416
Iteration 317/1000 | Loss: 0.00002416
Iteration 318/1000 | Loss: 0.00002413
Iteration 319/1000 | Loss: 0.00002413
Iteration 320/1000 | Loss: 0.00002412
Iteration 321/1000 | Loss: 0.00002412
Iteration 322/1000 | Loss: 0.00002412
Iteration 323/1000 | Loss: 0.00002411
Iteration 324/1000 | Loss: 0.00002411
Iteration 325/1000 | Loss: 0.00002410
Iteration 326/1000 | Loss: 0.00002409
Iteration 327/1000 | Loss: 0.00002409
Iteration 328/1000 | Loss: 0.00002409
Iteration 329/1000 | Loss: 0.00002409
Iteration 330/1000 | Loss: 0.00002408
Iteration 331/1000 | Loss: 0.00002408
Iteration 332/1000 | Loss: 0.00002407
Iteration 333/1000 | Loss: 0.00002407
Iteration 334/1000 | Loss: 0.00002407
Iteration 335/1000 | Loss: 0.00002406
Iteration 336/1000 | Loss: 0.00002406
Iteration 337/1000 | Loss: 0.00002406
Iteration 338/1000 | Loss: 0.00002406
Iteration 339/1000 | Loss: 0.00002405
Iteration 340/1000 | Loss: 0.00002405
Iteration 341/1000 | Loss: 0.00002405
Iteration 342/1000 | Loss: 0.00002405
Iteration 343/1000 | Loss: 0.00002405
Iteration 344/1000 | Loss: 0.00002404
Iteration 345/1000 | Loss: 0.00002404
Iteration 346/1000 | Loss: 0.00002404
Iteration 347/1000 | Loss: 0.00002403
Iteration 348/1000 | Loss: 0.00002403
Iteration 349/1000 | Loss: 0.00002403
Iteration 350/1000 | Loss: 0.00002403
Iteration 351/1000 | Loss: 0.00002403
Iteration 352/1000 | Loss: 0.00002403
Iteration 353/1000 | Loss: 0.00002403
Iteration 354/1000 | Loss: 0.00002402
Iteration 355/1000 | Loss: 0.00002402
Iteration 356/1000 | Loss: 0.00002402
Iteration 357/1000 | Loss: 0.00002402
Iteration 358/1000 | Loss: 0.00002402
Iteration 359/1000 | Loss: 0.00002402
Iteration 360/1000 | Loss: 0.00002402
Iteration 361/1000 | Loss: 0.00002401
Iteration 362/1000 | Loss: 0.00002401
Iteration 363/1000 | Loss: 0.00002401
Iteration 364/1000 | Loss: 0.00002401
Iteration 365/1000 | Loss: 0.00002401
Iteration 366/1000 | Loss: 0.00002401
Iteration 367/1000 | Loss: 0.00002401
Iteration 368/1000 | Loss: 0.00002401
Iteration 369/1000 | Loss: 0.00002401
Iteration 370/1000 | Loss: 0.00002401
Iteration 371/1000 | Loss: 0.00002401
Iteration 372/1000 | Loss: 0.00002401
Iteration 373/1000 | Loss: 0.00002401
Iteration 374/1000 | Loss: 0.00002401
Iteration 375/1000 | Loss: 0.00002401
Iteration 376/1000 | Loss: 0.00002400
Iteration 377/1000 | Loss: 0.00002400
Iteration 378/1000 | Loss: 0.00002400
Iteration 379/1000 | Loss: 0.00002400
Iteration 380/1000 | Loss: 0.00002400
Iteration 381/1000 | Loss: 0.00002400
Iteration 382/1000 | Loss: 0.00002400
Iteration 383/1000 | Loss: 0.00002399
Iteration 384/1000 | Loss: 0.00002399
Iteration 385/1000 | Loss: 0.00002399
Iteration 386/1000 | Loss: 0.00002399
Iteration 387/1000 | Loss: 0.00002399
Iteration 388/1000 | Loss: 0.00002399
Iteration 389/1000 | Loss: 0.00002399
Iteration 390/1000 | Loss: 0.00002399
Iteration 391/1000 | Loss: 0.00002398
Iteration 392/1000 | Loss: 0.00002398
Iteration 393/1000 | Loss: 0.00002398
Iteration 394/1000 | Loss: 0.00002398
Iteration 395/1000 | Loss: 0.00002398
Iteration 396/1000 | Loss: 0.00002398
Iteration 397/1000 | Loss: 0.00002397
Iteration 398/1000 | Loss: 0.00002397
Iteration 399/1000 | Loss: 0.00002397
Iteration 400/1000 | Loss: 0.00002397
Iteration 401/1000 | Loss: 0.00002397
Iteration 402/1000 | Loss: 0.00002397
Iteration 403/1000 | Loss: 0.00002397
Iteration 404/1000 | Loss: 0.00002396
Iteration 405/1000 | Loss: 0.00002396
Iteration 406/1000 | Loss: 0.00002396
Iteration 407/1000 | Loss: 0.00002396
Iteration 408/1000 | Loss: 0.00002396
Iteration 409/1000 | Loss: 0.00002395
Iteration 410/1000 | Loss: 0.00002395
Iteration 411/1000 | Loss: 0.00002395
Iteration 412/1000 | Loss: 0.00002395
Iteration 413/1000 | Loss: 0.00002395
Iteration 414/1000 | Loss: 0.00002395
Iteration 415/1000 | Loss: 0.00002395
Iteration 416/1000 | Loss: 0.00002395
Iteration 417/1000 | Loss: 0.00002395
Iteration 418/1000 | Loss: 0.00002395
Iteration 419/1000 | Loss: 0.00002395
Iteration 420/1000 | Loss: 0.00002395
Iteration 421/1000 | Loss: 0.00002395
Iteration 422/1000 | Loss: 0.00002395
Iteration 423/1000 | Loss: 0.00002395
Iteration 424/1000 | Loss: 0.00002394
Iteration 425/1000 | Loss: 0.00002394
Iteration 426/1000 | Loss: 0.00002394
Iteration 427/1000 | Loss: 0.00002394
Iteration 428/1000 | Loss: 0.00002394
Iteration 429/1000 | Loss: 0.00002394
Iteration 430/1000 | Loss: 0.00002394
Iteration 431/1000 | Loss: 0.00002394
Iteration 432/1000 | Loss: 0.00002394
Iteration 433/1000 | Loss: 0.00002394
Iteration 434/1000 | Loss: 0.00002394
Iteration 435/1000 | Loss: 0.00002394
Iteration 436/1000 | Loss: 0.00002394
Iteration 437/1000 | Loss: 0.00002394
Iteration 438/1000 | Loss: 0.00002394
Iteration 439/1000 | Loss: 0.00002394
Iteration 440/1000 | Loss: 0.00002394
Iteration 441/1000 | Loss: 0.00002394
Iteration 442/1000 | Loss: 0.00002394
Iteration 443/1000 | Loss: 0.00002394
Iteration 444/1000 | Loss: 0.00002394
Iteration 445/1000 | Loss: 0.00002394
Iteration 446/1000 | Loss: 0.00002394
Iteration 447/1000 | Loss: 0.00002394
Iteration 448/1000 | Loss: 0.00002394
Iteration 449/1000 | Loss: 0.00002394
Iteration 450/1000 | Loss: 0.00002394
Iteration 451/1000 | Loss: 0.00002394
Iteration 452/1000 | Loss: 0.00002394
Iteration 453/1000 | Loss: 0.00002394
Iteration 454/1000 | Loss: 0.00002394
Iteration 455/1000 | Loss: 0.00002394
Iteration 456/1000 | Loss: 0.00002394
Iteration 457/1000 | Loss: 0.00002394
Iteration 458/1000 | Loss: 0.00002394
Iteration 459/1000 | Loss: 0.00002394
Iteration 460/1000 | Loss: 0.00002394
Iteration 461/1000 | Loss: 0.00002394
Iteration 462/1000 | Loss: 0.00002394
Iteration 463/1000 | Loss: 0.00002394
Iteration 464/1000 | Loss: 0.00002394
Iteration 465/1000 | Loss: 0.00002394
Iteration 466/1000 | Loss: 0.00002394
Iteration 467/1000 | Loss: 0.00002394
Iteration 468/1000 | Loss: 0.00002394
Iteration 469/1000 | Loss: 0.00002394
Iteration 470/1000 | Loss: 0.00002394
Iteration 471/1000 | Loss: 0.00002394
Iteration 472/1000 | Loss: 0.00002394
Iteration 473/1000 | Loss: 0.00002394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 473. Stopping optimization.
Last 5 losses: [2.393604336248245e-05, 2.393604336248245e-05, 2.393604336248245e-05, 2.393604336248245e-05, 2.393604336248245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.393604336248245e-05

Optimization complete. Final v2v error: 4.121828079223633 mm

Highest mean error: 5.339591979980469 mm for frame 98

Lowest mean error: 3.1842823028564453 mm for frame 144

Saving results

Total time: 446.0368375778198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045880
Iteration 2/25 | Loss: 0.00208988
Iteration 3/25 | Loss: 0.00153664
Iteration 4/25 | Loss: 0.00147411
Iteration 5/25 | Loss: 0.00118945
Iteration 6/25 | Loss: 0.00136838
Iteration 7/25 | Loss: 0.00113519
Iteration 8/25 | Loss: 0.00098543
Iteration 9/25 | Loss: 0.00093277
Iteration 10/25 | Loss: 0.00089472
Iteration 11/25 | Loss: 0.00088430
Iteration 12/25 | Loss: 0.00086959
Iteration 13/25 | Loss: 0.00087568
Iteration 14/25 | Loss: 0.00086125
Iteration 15/25 | Loss: 0.00085858
Iteration 16/25 | Loss: 0.00085936
Iteration 17/25 | Loss: 0.00085203
Iteration 18/25 | Loss: 0.00084569
Iteration 19/25 | Loss: 0.00084089
Iteration 20/25 | Loss: 0.00083923
Iteration 21/25 | Loss: 0.00083834
Iteration 22/25 | Loss: 0.00083799
Iteration 23/25 | Loss: 0.00083775
Iteration 24/25 | Loss: 0.00084147
Iteration 25/25 | Loss: 0.00083827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53048754
Iteration 2/25 | Loss: 0.00070017
Iteration 3/25 | Loss: 0.00070016
Iteration 4/25 | Loss: 0.00070016
Iteration 5/25 | Loss: 0.00070016
Iteration 6/25 | Loss: 0.00070016
Iteration 7/25 | Loss: 0.00070016
Iteration 8/25 | Loss: 0.00070016
Iteration 9/25 | Loss: 0.00070016
Iteration 10/25 | Loss: 0.00070016
Iteration 11/25 | Loss: 0.00070016
Iteration 12/25 | Loss: 0.00070016
Iteration 13/25 | Loss: 0.00070016
Iteration 14/25 | Loss: 0.00070016
Iteration 15/25 | Loss: 0.00070016
Iteration 16/25 | Loss: 0.00070016
Iteration 17/25 | Loss: 0.00070016
Iteration 18/25 | Loss: 0.00070016
Iteration 19/25 | Loss: 0.00070016
Iteration 20/25 | Loss: 0.00070016
Iteration 21/25 | Loss: 0.00070016
Iteration 22/25 | Loss: 0.00070016
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007001622579991817, 0.0007001622579991817, 0.0007001622579991817, 0.0007001622579991817, 0.0007001622579991817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007001622579991817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070016
Iteration 2/1000 | Loss: 0.00005869
Iteration 3/1000 | Loss: 0.00004164
Iteration 4/1000 | Loss: 0.00003795
Iteration 5/1000 | Loss: 0.00011728
Iteration 6/1000 | Loss: 0.00027974
Iteration 7/1000 | Loss: 0.00005176
Iteration 8/1000 | Loss: 0.00003851
Iteration 9/1000 | Loss: 0.00003349
Iteration 10/1000 | Loss: 0.00002923
Iteration 11/1000 | Loss: 0.00002725
Iteration 12/1000 | Loss: 0.00002637
Iteration 13/1000 | Loss: 0.00002558
Iteration 14/1000 | Loss: 0.00002498
Iteration 15/1000 | Loss: 0.00109190
Iteration 16/1000 | Loss: 0.00102832
Iteration 17/1000 | Loss: 0.00114172
Iteration 18/1000 | Loss: 0.00080928
Iteration 19/1000 | Loss: 0.00089468
Iteration 20/1000 | Loss: 0.00024439
Iteration 21/1000 | Loss: 0.00002518
Iteration 22/1000 | Loss: 0.00002307
Iteration 23/1000 | Loss: 0.00002098
Iteration 24/1000 | Loss: 0.00001991
Iteration 25/1000 | Loss: 0.00001927
Iteration 26/1000 | Loss: 0.00001886
Iteration 27/1000 | Loss: 0.00001855
Iteration 28/1000 | Loss: 0.00001833
Iteration 29/1000 | Loss: 0.00001815
Iteration 30/1000 | Loss: 0.00001805
Iteration 31/1000 | Loss: 0.00001804
Iteration 32/1000 | Loss: 0.00001803
Iteration 33/1000 | Loss: 0.00001803
Iteration 34/1000 | Loss: 0.00001801
Iteration 35/1000 | Loss: 0.00001800
Iteration 36/1000 | Loss: 0.00001799
Iteration 37/1000 | Loss: 0.00001799
Iteration 38/1000 | Loss: 0.00001797
Iteration 39/1000 | Loss: 0.00001797
Iteration 40/1000 | Loss: 0.00001796
Iteration 41/1000 | Loss: 0.00001796
Iteration 42/1000 | Loss: 0.00001796
Iteration 43/1000 | Loss: 0.00001795
Iteration 44/1000 | Loss: 0.00001794
Iteration 45/1000 | Loss: 0.00001794
Iteration 46/1000 | Loss: 0.00001793
Iteration 47/1000 | Loss: 0.00001793
Iteration 48/1000 | Loss: 0.00001793
Iteration 49/1000 | Loss: 0.00001793
Iteration 50/1000 | Loss: 0.00001792
Iteration 51/1000 | Loss: 0.00001792
Iteration 52/1000 | Loss: 0.00001792
Iteration 53/1000 | Loss: 0.00001792
Iteration 54/1000 | Loss: 0.00001791
Iteration 55/1000 | Loss: 0.00001791
Iteration 56/1000 | Loss: 0.00001791
Iteration 57/1000 | Loss: 0.00001790
Iteration 58/1000 | Loss: 0.00001790
Iteration 59/1000 | Loss: 0.00001790
Iteration 60/1000 | Loss: 0.00001790
Iteration 61/1000 | Loss: 0.00001789
Iteration 62/1000 | Loss: 0.00001789
Iteration 63/1000 | Loss: 0.00001789
Iteration 64/1000 | Loss: 0.00001789
Iteration 65/1000 | Loss: 0.00001789
Iteration 66/1000 | Loss: 0.00001788
Iteration 67/1000 | Loss: 0.00001788
Iteration 68/1000 | Loss: 0.00001788
Iteration 69/1000 | Loss: 0.00001788
Iteration 70/1000 | Loss: 0.00001788
Iteration 71/1000 | Loss: 0.00001787
Iteration 72/1000 | Loss: 0.00001787
Iteration 73/1000 | Loss: 0.00001787
Iteration 74/1000 | Loss: 0.00001787
Iteration 75/1000 | Loss: 0.00001786
Iteration 76/1000 | Loss: 0.00001786
Iteration 77/1000 | Loss: 0.00001786
Iteration 78/1000 | Loss: 0.00001786
Iteration 79/1000 | Loss: 0.00001786
Iteration 80/1000 | Loss: 0.00001786
Iteration 81/1000 | Loss: 0.00001786
Iteration 82/1000 | Loss: 0.00001785
Iteration 83/1000 | Loss: 0.00001785
Iteration 84/1000 | Loss: 0.00001785
Iteration 85/1000 | Loss: 0.00001785
Iteration 86/1000 | Loss: 0.00001785
Iteration 87/1000 | Loss: 0.00001785
Iteration 88/1000 | Loss: 0.00001785
Iteration 89/1000 | Loss: 0.00001785
Iteration 90/1000 | Loss: 0.00001785
Iteration 91/1000 | Loss: 0.00001785
Iteration 92/1000 | Loss: 0.00001784
Iteration 93/1000 | Loss: 0.00001784
Iteration 94/1000 | Loss: 0.00001784
Iteration 95/1000 | Loss: 0.00001784
Iteration 96/1000 | Loss: 0.00001784
Iteration 97/1000 | Loss: 0.00001784
Iteration 98/1000 | Loss: 0.00001784
Iteration 99/1000 | Loss: 0.00001784
Iteration 100/1000 | Loss: 0.00001784
Iteration 101/1000 | Loss: 0.00001784
Iteration 102/1000 | Loss: 0.00001784
Iteration 103/1000 | Loss: 0.00001784
Iteration 104/1000 | Loss: 0.00001784
Iteration 105/1000 | Loss: 0.00001784
Iteration 106/1000 | Loss: 0.00001784
Iteration 107/1000 | Loss: 0.00001784
Iteration 108/1000 | Loss: 0.00001784
Iteration 109/1000 | Loss: 0.00001783
Iteration 110/1000 | Loss: 0.00001783
Iteration 111/1000 | Loss: 0.00001783
Iteration 112/1000 | Loss: 0.00001783
Iteration 113/1000 | Loss: 0.00001783
Iteration 114/1000 | Loss: 0.00001783
Iteration 115/1000 | Loss: 0.00001783
Iteration 116/1000 | Loss: 0.00001783
Iteration 117/1000 | Loss: 0.00001783
Iteration 118/1000 | Loss: 0.00001783
Iteration 119/1000 | Loss: 0.00001783
Iteration 120/1000 | Loss: 0.00001783
Iteration 121/1000 | Loss: 0.00001782
Iteration 122/1000 | Loss: 0.00001782
Iteration 123/1000 | Loss: 0.00001782
Iteration 124/1000 | Loss: 0.00001782
Iteration 125/1000 | Loss: 0.00001782
Iteration 126/1000 | Loss: 0.00001782
Iteration 127/1000 | Loss: 0.00001782
Iteration 128/1000 | Loss: 0.00001782
Iteration 129/1000 | Loss: 0.00001782
Iteration 130/1000 | Loss: 0.00001782
Iteration 131/1000 | Loss: 0.00001782
Iteration 132/1000 | Loss: 0.00001782
Iteration 133/1000 | Loss: 0.00001782
Iteration 134/1000 | Loss: 0.00001782
Iteration 135/1000 | Loss: 0.00001782
Iteration 136/1000 | Loss: 0.00001782
Iteration 137/1000 | Loss: 0.00001782
Iteration 138/1000 | Loss: 0.00001782
Iteration 139/1000 | Loss: 0.00001782
Iteration 140/1000 | Loss: 0.00001781
Iteration 141/1000 | Loss: 0.00001781
Iteration 142/1000 | Loss: 0.00001781
Iteration 143/1000 | Loss: 0.00001781
Iteration 144/1000 | Loss: 0.00001781
Iteration 145/1000 | Loss: 0.00001781
Iteration 146/1000 | Loss: 0.00001781
Iteration 147/1000 | Loss: 0.00001781
Iteration 148/1000 | Loss: 0.00001781
Iteration 149/1000 | Loss: 0.00001781
Iteration 150/1000 | Loss: 0.00001781
Iteration 151/1000 | Loss: 0.00001781
Iteration 152/1000 | Loss: 0.00001781
Iteration 153/1000 | Loss: 0.00001781
Iteration 154/1000 | Loss: 0.00001781
Iteration 155/1000 | Loss: 0.00001781
Iteration 156/1000 | Loss: 0.00001781
Iteration 157/1000 | Loss: 0.00001781
Iteration 158/1000 | Loss: 0.00001780
Iteration 159/1000 | Loss: 0.00001780
Iteration 160/1000 | Loss: 0.00001780
Iteration 161/1000 | Loss: 0.00001780
Iteration 162/1000 | Loss: 0.00001780
Iteration 163/1000 | Loss: 0.00001780
Iteration 164/1000 | Loss: 0.00001780
Iteration 165/1000 | Loss: 0.00001780
Iteration 166/1000 | Loss: 0.00001780
Iteration 167/1000 | Loss: 0.00001780
Iteration 168/1000 | Loss: 0.00001780
Iteration 169/1000 | Loss: 0.00001780
Iteration 170/1000 | Loss: 0.00001780
Iteration 171/1000 | Loss: 0.00001780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.7800390196498483e-05, 1.7800390196498483e-05, 1.7800390196498483e-05, 1.7800390196498483e-05, 1.7800390196498483e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7800390196498483e-05

Optimization complete. Final v2v error: 3.5458059310913086 mm

Highest mean error: 4.546731948852539 mm for frame 64

Lowest mean error: 2.8806710243225098 mm for frame 42

Saving results

Total time: 93.36643290519714
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392616
Iteration 2/25 | Loss: 0.00084989
Iteration 3/25 | Loss: 0.00074074
Iteration 4/25 | Loss: 0.00072203
Iteration 5/25 | Loss: 0.00071900
Iteration 6/25 | Loss: 0.00071772
Iteration 7/25 | Loss: 0.00071763
Iteration 8/25 | Loss: 0.00071763
Iteration 9/25 | Loss: 0.00071763
Iteration 10/25 | Loss: 0.00071763
Iteration 11/25 | Loss: 0.00071763
Iteration 12/25 | Loss: 0.00071763
Iteration 13/25 | Loss: 0.00071763
Iteration 14/25 | Loss: 0.00071763
Iteration 15/25 | Loss: 0.00071763
Iteration 16/25 | Loss: 0.00071763
Iteration 17/25 | Loss: 0.00071763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007176271756179631, 0.0007176271756179631, 0.0007176271756179631, 0.0007176271756179631, 0.0007176271756179631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007176271756179631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50579166
Iteration 2/25 | Loss: 0.00045582
Iteration 3/25 | Loss: 0.00045582
Iteration 4/25 | Loss: 0.00045582
Iteration 5/25 | Loss: 0.00045582
Iteration 6/25 | Loss: 0.00045582
Iteration 7/25 | Loss: 0.00045582
Iteration 8/25 | Loss: 0.00045582
Iteration 9/25 | Loss: 0.00045582
Iteration 10/25 | Loss: 0.00045582
Iteration 11/25 | Loss: 0.00045582
Iteration 12/25 | Loss: 0.00045582
Iteration 13/25 | Loss: 0.00045582
Iteration 14/25 | Loss: 0.00045582
Iteration 15/25 | Loss: 0.00045582
Iteration 16/25 | Loss: 0.00045582
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00045581557787954807, 0.00045581557787954807, 0.00045581557787954807, 0.00045581557787954807, 0.00045581557787954807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045581557787954807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045582
Iteration 2/1000 | Loss: 0.00001899
Iteration 3/1000 | Loss: 0.00001463
Iteration 4/1000 | Loss: 0.00001408
Iteration 5/1000 | Loss: 0.00001328
Iteration 6/1000 | Loss: 0.00001281
Iteration 7/1000 | Loss: 0.00001258
Iteration 8/1000 | Loss: 0.00001258
Iteration 9/1000 | Loss: 0.00001233
Iteration 10/1000 | Loss: 0.00001228
Iteration 11/1000 | Loss: 0.00001221
Iteration 12/1000 | Loss: 0.00001220
Iteration 13/1000 | Loss: 0.00001220
Iteration 14/1000 | Loss: 0.00001219
Iteration 15/1000 | Loss: 0.00001218
Iteration 16/1000 | Loss: 0.00001216
Iteration 17/1000 | Loss: 0.00001213
Iteration 18/1000 | Loss: 0.00001213
Iteration 19/1000 | Loss: 0.00001212
Iteration 20/1000 | Loss: 0.00001211
Iteration 21/1000 | Loss: 0.00001211
Iteration 22/1000 | Loss: 0.00001211
Iteration 23/1000 | Loss: 0.00001210
Iteration 24/1000 | Loss: 0.00001209
Iteration 25/1000 | Loss: 0.00001208
Iteration 26/1000 | Loss: 0.00001208
Iteration 27/1000 | Loss: 0.00001207
Iteration 28/1000 | Loss: 0.00001207
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001207
Iteration 31/1000 | Loss: 0.00001206
Iteration 32/1000 | Loss: 0.00001206
Iteration 33/1000 | Loss: 0.00001204
Iteration 34/1000 | Loss: 0.00001202
Iteration 35/1000 | Loss: 0.00001202
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001202
Iteration 38/1000 | Loss: 0.00001202
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001198
Iteration 42/1000 | Loss: 0.00001198
Iteration 43/1000 | Loss: 0.00001198
Iteration 44/1000 | Loss: 0.00001197
Iteration 45/1000 | Loss: 0.00001197
Iteration 46/1000 | Loss: 0.00001197
Iteration 47/1000 | Loss: 0.00001197
Iteration 48/1000 | Loss: 0.00001197
Iteration 49/1000 | Loss: 0.00001197
Iteration 50/1000 | Loss: 0.00001197
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001197
Iteration 53/1000 | Loss: 0.00001197
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001196
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001195
Iteration 60/1000 | Loss: 0.00001195
Iteration 61/1000 | Loss: 0.00001195
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001195
Iteration 64/1000 | Loss: 0.00001195
Iteration 65/1000 | Loss: 0.00001195
Iteration 66/1000 | Loss: 0.00001195
Iteration 67/1000 | Loss: 0.00001195
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001195
Iteration 72/1000 | Loss: 0.00001195
Iteration 73/1000 | Loss: 0.00001195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [1.1947080565732904e-05, 1.1947080565732904e-05, 1.1947080565732904e-05, 1.1947080565732904e-05, 1.1947080565732904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1947080565732904e-05

Optimization complete. Final v2v error: 2.937927722930908 mm

Highest mean error: 3.0703024864196777 mm for frame 51

Lowest mean error: 2.7966856956481934 mm for frame 165

Saving results

Total time: 28.24839186668396
