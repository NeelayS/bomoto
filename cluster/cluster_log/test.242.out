Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=242, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13552-13607
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029566
Iteration 2/25 | Loss: 0.00142204
Iteration 3/25 | Loss: 0.00124219
Iteration 4/25 | Loss: 0.00121698
Iteration 5/25 | Loss: 0.00121386
Iteration 6/25 | Loss: 0.00121333
Iteration 7/25 | Loss: 0.00121333
Iteration 8/25 | Loss: 0.00121333
Iteration 9/25 | Loss: 0.00121333
Iteration 10/25 | Loss: 0.00121333
Iteration 11/25 | Loss: 0.00121333
Iteration 12/25 | Loss: 0.00121333
Iteration 13/25 | Loss: 0.00121333
Iteration 14/25 | Loss: 0.00121333
Iteration 15/25 | Loss: 0.00121333
Iteration 16/25 | Loss: 0.00121333
Iteration 17/25 | Loss: 0.00121333
Iteration 18/25 | Loss: 0.00121333
Iteration 19/25 | Loss: 0.00121333
Iteration 20/25 | Loss: 0.00121333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012133264681324363, 0.0012133264681324363, 0.0012133264681324363, 0.0012133264681324363, 0.0012133264681324363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012133264681324363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.49563980
Iteration 2/25 | Loss: 0.00152112
Iteration 3/25 | Loss: 0.00152112
Iteration 4/25 | Loss: 0.00152112
Iteration 5/25 | Loss: 0.00152112
Iteration 6/25 | Loss: 0.00152112
Iteration 7/25 | Loss: 0.00152112
Iteration 8/25 | Loss: 0.00152112
Iteration 9/25 | Loss: 0.00152112
Iteration 10/25 | Loss: 0.00152112
Iteration 11/25 | Loss: 0.00152112
Iteration 12/25 | Loss: 0.00152112
Iteration 13/25 | Loss: 0.00152112
Iteration 14/25 | Loss: 0.00152112
Iteration 15/25 | Loss: 0.00152112
Iteration 16/25 | Loss: 0.00152112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015211154241114855, 0.0015211154241114855, 0.0015211154241114855, 0.0015211154241114855, 0.0015211154241114855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015211154241114855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152112
Iteration 2/1000 | Loss: 0.00002341
Iteration 3/1000 | Loss: 0.00001650
Iteration 4/1000 | Loss: 0.00001508
Iteration 5/1000 | Loss: 0.00001404
Iteration 6/1000 | Loss: 0.00001352
Iteration 7/1000 | Loss: 0.00001319
Iteration 8/1000 | Loss: 0.00001273
Iteration 9/1000 | Loss: 0.00001233
Iteration 10/1000 | Loss: 0.00001209
Iteration 11/1000 | Loss: 0.00001190
Iteration 12/1000 | Loss: 0.00001182
Iteration 13/1000 | Loss: 0.00001180
Iteration 14/1000 | Loss: 0.00001169
Iteration 15/1000 | Loss: 0.00001169
Iteration 16/1000 | Loss: 0.00001168
Iteration 17/1000 | Loss: 0.00001166
Iteration 18/1000 | Loss: 0.00001165
Iteration 19/1000 | Loss: 0.00001158
Iteration 20/1000 | Loss: 0.00001157
Iteration 21/1000 | Loss: 0.00001154
Iteration 22/1000 | Loss: 0.00001153
Iteration 23/1000 | Loss: 0.00001152
Iteration 24/1000 | Loss: 0.00001151
Iteration 25/1000 | Loss: 0.00001151
Iteration 26/1000 | Loss: 0.00001151
Iteration 27/1000 | Loss: 0.00001150
Iteration 28/1000 | Loss: 0.00001150
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001149
Iteration 31/1000 | Loss: 0.00001149
Iteration 32/1000 | Loss: 0.00001148
Iteration 33/1000 | Loss: 0.00001148
Iteration 34/1000 | Loss: 0.00001148
Iteration 35/1000 | Loss: 0.00001148
Iteration 36/1000 | Loss: 0.00001148
Iteration 37/1000 | Loss: 0.00001148
Iteration 38/1000 | Loss: 0.00001148
Iteration 39/1000 | Loss: 0.00001148
Iteration 40/1000 | Loss: 0.00001148
Iteration 41/1000 | Loss: 0.00001147
Iteration 42/1000 | Loss: 0.00001147
Iteration 43/1000 | Loss: 0.00001146
Iteration 44/1000 | Loss: 0.00001146
Iteration 45/1000 | Loss: 0.00001146
Iteration 46/1000 | Loss: 0.00001146
Iteration 47/1000 | Loss: 0.00001146
Iteration 48/1000 | Loss: 0.00001146
Iteration 49/1000 | Loss: 0.00001145
Iteration 50/1000 | Loss: 0.00001145
Iteration 51/1000 | Loss: 0.00001145
Iteration 52/1000 | Loss: 0.00001145
Iteration 53/1000 | Loss: 0.00001145
Iteration 54/1000 | Loss: 0.00001145
Iteration 55/1000 | Loss: 0.00001144
Iteration 56/1000 | Loss: 0.00001143
Iteration 57/1000 | Loss: 0.00001142
Iteration 58/1000 | Loss: 0.00001142
Iteration 59/1000 | Loss: 0.00001142
Iteration 60/1000 | Loss: 0.00001142
Iteration 61/1000 | Loss: 0.00001142
Iteration 62/1000 | Loss: 0.00001141
Iteration 63/1000 | Loss: 0.00001141
Iteration 64/1000 | Loss: 0.00001141
Iteration 65/1000 | Loss: 0.00001141
Iteration 66/1000 | Loss: 0.00001141
Iteration 67/1000 | Loss: 0.00001141
Iteration 68/1000 | Loss: 0.00001141
Iteration 69/1000 | Loss: 0.00001140
Iteration 70/1000 | Loss: 0.00001140
Iteration 71/1000 | Loss: 0.00001139
Iteration 72/1000 | Loss: 0.00001139
Iteration 73/1000 | Loss: 0.00001139
Iteration 74/1000 | Loss: 0.00001139
Iteration 75/1000 | Loss: 0.00001139
Iteration 76/1000 | Loss: 0.00001138
Iteration 77/1000 | Loss: 0.00001138
Iteration 78/1000 | Loss: 0.00001138
Iteration 79/1000 | Loss: 0.00001138
Iteration 80/1000 | Loss: 0.00001138
Iteration 81/1000 | Loss: 0.00001138
Iteration 82/1000 | Loss: 0.00001138
Iteration 83/1000 | Loss: 0.00001138
Iteration 84/1000 | Loss: 0.00001138
Iteration 85/1000 | Loss: 0.00001138
Iteration 86/1000 | Loss: 0.00001137
Iteration 87/1000 | Loss: 0.00001137
Iteration 88/1000 | Loss: 0.00001137
Iteration 89/1000 | Loss: 0.00001137
Iteration 90/1000 | Loss: 0.00001137
Iteration 91/1000 | Loss: 0.00001137
Iteration 92/1000 | Loss: 0.00001136
Iteration 93/1000 | Loss: 0.00001136
Iteration 94/1000 | Loss: 0.00001136
Iteration 95/1000 | Loss: 0.00001135
Iteration 96/1000 | Loss: 0.00001135
Iteration 97/1000 | Loss: 0.00001135
Iteration 98/1000 | Loss: 0.00001135
Iteration 99/1000 | Loss: 0.00001135
Iteration 100/1000 | Loss: 0.00001135
Iteration 101/1000 | Loss: 0.00001135
Iteration 102/1000 | Loss: 0.00001135
Iteration 103/1000 | Loss: 0.00001135
Iteration 104/1000 | Loss: 0.00001135
Iteration 105/1000 | Loss: 0.00001135
Iteration 106/1000 | Loss: 0.00001135
Iteration 107/1000 | Loss: 0.00001134
Iteration 108/1000 | Loss: 0.00001134
Iteration 109/1000 | Loss: 0.00001134
Iteration 110/1000 | Loss: 0.00001134
Iteration 111/1000 | Loss: 0.00001134
Iteration 112/1000 | Loss: 0.00001133
Iteration 113/1000 | Loss: 0.00001133
Iteration 114/1000 | Loss: 0.00001133
Iteration 115/1000 | Loss: 0.00001132
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001132
Iteration 118/1000 | Loss: 0.00001132
Iteration 119/1000 | Loss: 0.00001132
Iteration 120/1000 | Loss: 0.00001132
Iteration 121/1000 | Loss: 0.00001131
Iteration 122/1000 | Loss: 0.00001131
Iteration 123/1000 | Loss: 0.00001131
Iteration 124/1000 | Loss: 0.00001130
Iteration 125/1000 | Loss: 0.00001130
Iteration 126/1000 | Loss: 0.00001130
Iteration 127/1000 | Loss: 0.00001130
Iteration 128/1000 | Loss: 0.00001130
Iteration 129/1000 | Loss: 0.00001130
Iteration 130/1000 | Loss: 0.00001130
Iteration 131/1000 | Loss: 0.00001129
Iteration 132/1000 | Loss: 0.00001129
Iteration 133/1000 | Loss: 0.00001129
Iteration 134/1000 | Loss: 0.00001129
Iteration 135/1000 | Loss: 0.00001129
Iteration 136/1000 | Loss: 0.00001129
Iteration 137/1000 | Loss: 0.00001128
Iteration 138/1000 | Loss: 0.00001128
Iteration 139/1000 | Loss: 0.00001128
Iteration 140/1000 | Loss: 0.00001128
Iteration 141/1000 | Loss: 0.00001128
Iteration 142/1000 | Loss: 0.00001128
Iteration 143/1000 | Loss: 0.00001128
Iteration 144/1000 | Loss: 0.00001128
Iteration 145/1000 | Loss: 0.00001128
Iteration 146/1000 | Loss: 0.00001128
Iteration 147/1000 | Loss: 0.00001128
Iteration 148/1000 | Loss: 0.00001128
Iteration 149/1000 | Loss: 0.00001128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.1283343155810144e-05, 1.1283343155810144e-05, 1.1283343155810144e-05, 1.1283343155810144e-05, 1.1283343155810144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1283343155810144e-05

Optimization complete. Final v2v error: 2.885176181793213 mm

Highest mean error: 3.153836488723755 mm for frame 42

Lowest mean error: 2.5981664657592773 mm for frame 205

Saving results

Total time: 46.485679388046265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00550022
Iteration 2/25 | Loss: 0.00163432
Iteration 3/25 | Loss: 0.00137059
Iteration 4/25 | Loss: 0.00135227
Iteration 5/25 | Loss: 0.00134962
Iteration 6/25 | Loss: 0.00134922
Iteration 7/25 | Loss: 0.00134922
Iteration 8/25 | Loss: 0.00134922
Iteration 9/25 | Loss: 0.00134922
Iteration 10/25 | Loss: 0.00134922
Iteration 11/25 | Loss: 0.00134922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013492220314219594, 0.0013492220314219594, 0.0013492220314219594, 0.0013492220314219594, 0.0013492220314219594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013492220314219594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97670722
Iteration 2/25 | Loss: 0.00129376
Iteration 3/25 | Loss: 0.00129374
Iteration 4/25 | Loss: 0.00129374
Iteration 5/25 | Loss: 0.00129374
Iteration 6/25 | Loss: 0.00129374
Iteration 7/25 | Loss: 0.00129374
Iteration 8/25 | Loss: 0.00129374
Iteration 9/25 | Loss: 0.00129374
Iteration 10/25 | Loss: 0.00129374
Iteration 11/25 | Loss: 0.00129374
Iteration 12/25 | Loss: 0.00129374
Iteration 13/25 | Loss: 0.00129374
Iteration 14/25 | Loss: 0.00129374
Iteration 15/25 | Loss: 0.00129374
Iteration 16/25 | Loss: 0.00129374
Iteration 17/25 | Loss: 0.00129374
Iteration 18/25 | Loss: 0.00129374
Iteration 19/25 | Loss: 0.00129374
Iteration 20/25 | Loss: 0.00129374
Iteration 21/25 | Loss: 0.00129374
Iteration 22/25 | Loss: 0.00129374
Iteration 23/25 | Loss: 0.00129374
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012937394203618169, 0.0012937394203618169, 0.0012937394203618169, 0.0012937394203618169, 0.0012937394203618169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012937394203618169

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129374
Iteration 2/1000 | Loss: 0.00005061
Iteration 3/1000 | Loss: 0.00003515
Iteration 4/1000 | Loss: 0.00002753
Iteration 5/1000 | Loss: 0.00002500
Iteration 6/1000 | Loss: 0.00002415
Iteration 7/1000 | Loss: 0.00002358
Iteration 8/1000 | Loss: 0.00002300
Iteration 9/1000 | Loss: 0.00002258
Iteration 10/1000 | Loss: 0.00002220
Iteration 11/1000 | Loss: 0.00002195
Iteration 12/1000 | Loss: 0.00002168
Iteration 13/1000 | Loss: 0.00002143
Iteration 14/1000 | Loss: 0.00002121
Iteration 15/1000 | Loss: 0.00002101
Iteration 16/1000 | Loss: 0.00002087
Iteration 17/1000 | Loss: 0.00002087
Iteration 18/1000 | Loss: 0.00002085
Iteration 19/1000 | Loss: 0.00002078
Iteration 20/1000 | Loss: 0.00002068
Iteration 21/1000 | Loss: 0.00002064
Iteration 22/1000 | Loss: 0.00002061
Iteration 23/1000 | Loss: 0.00002061
Iteration 24/1000 | Loss: 0.00002061
Iteration 25/1000 | Loss: 0.00002060
Iteration 26/1000 | Loss: 0.00002057
Iteration 27/1000 | Loss: 0.00002055
Iteration 28/1000 | Loss: 0.00002054
Iteration 29/1000 | Loss: 0.00002049
Iteration 30/1000 | Loss: 0.00002048
Iteration 31/1000 | Loss: 0.00002046
Iteration 32/1000 | Loss: 0.00002046
Iteration 33/1000 | Loss: 0.00002045
Iteration 34/1000 | Loss: 0.00002044
Iteration 35/1000 | Loss: 0.00002043
Iteration 36/1000 | Loss: 0.00002043
Iteration 37/1000 | Loss: 0.00002043
Iteration 38/1000 | Loss: 0.00002043
Iteration 39/1000 | Loss: 0.00002042
Iteration 40/1000 | Loss: 0.00002042
Iteration 41/1000 | Loss: 0.00002042
Iteration 42/1000 | Loss: 0.00002041
Iteration 43/1000 | Loss: 0.00002040
Iteration 44/1000 | Loss: 0.00002040
Iteration 45/1000 | Loss: 0.00002039
Iteration 46/1000 | Loss: 0.00002039
Iteration 47/1000 | Loss: 0.00002039
Iteration 48/1000 | Loss: 0.00002039
Iteration 49/1000 | Loss: 0.00002039
Iteration 50/1000 | Loss: 0.00002039
Iteration 51/1000 | Loss: 0.00002039
Iteration 52/1000 | Loss: 0.00002039
Iteration 53/1000 | Loss: 0.00002039
Iteration 54/1000 | Loss: 0.00002038
Iteration 55/1000 | Loss: 0.00002038
Iteration 56/1000 | Loss: 0.00002038
Iteration 57/1000 | Loss: 0.00002038
Iteration 58/1000 | Loss: 0.00002038
Iteration 59/1000 | Loss: 0.00002037
Iteration 60/1000 | Loss: 0.00002037
Iteration 61/1000 | Loss: 0.00002037
Iteration 62/1000 | Loss: 0.00002036
Iteration 63/1000 | Loss: 0.00002036
Iteration 64/1000 | Loss: 0.00002036
Iteration 65/1000 | Loss: 0.00002036
Iteration 66/1000 | Loss: 0.00002035
Iteration 67/1000 | Loss: 0.00002035
Iteration 68/1000 | Loss: 0.00002035
Iteration 69/1000 | Loss: 0.00002034
Iteration 70/1000 | Loss: 0.00002034
Iteration 71/1000 | Loss: 0.00002034
Iteration 72/1000 | Loss: 0.00002033
Iteration 73/1000 | Loss: 0.00002033
Iteration 74/1000 | Loss: 0.00002032
Iteration 75/1000 | Loss: 0.00002032
Iteration 76/1000 | Loss: 0.00002032
Iteration 77/1000 | Loss: 0.00002031
Iteration 78/1000 | Loss: 0.00002031
Iteration 79/1000 | Loss: 0.00002031
Iteration 80/1000 | Loss: 0.00002031
Iteration 81/1000 | Loss: 0.00002031
Iteration 82/1000 | Loss: 0.00002031
Iteration 83/1000 | Loss: 0.00002031
Iteration 84/1000 | Loss: 0.00002031
Iteration 85/1000 | Loss: 0.00002030
Iteration 86/1000 | Loss: 0.00002030
Iteration 87/1000 | Loss: 0.00002030
Iteration 88/1000 | Loss: 0.00002029
Iteration 89/1000 | Loss: 0.00002029
Iteration 90/1000 | Loss: 0.00002029
Iteration 91/1000 | Loss: 0.00002029
Iteration 92/1000 | Loss: 0.00002029
Iteration 93/1000 | Loss: 0.00002029
Iteration 94/1000 | Loss: 0.00002029
Iteration 95/1000 | Loss: 0.00002029
Iteration 96/1000 | Loss: 0.00002028
Iteration 97/1000 | Loss: 0.00002028
Iteration 98/1000 | Loss: 0.00002028
Iteration 99/1000 | Loss: 0.00002028
Iteration 100/1000 | Loss: 0.00002028
Iteration 101/1000 | Loss: 0.00002028
Iteration 102/1000 | Loss: 0.00002028
Iteration 103/1000 | Loss: 0.00002028
Iteration 104/1000 | Loss: 0.00002027
Iteration 105/1000 | Loss: 0.00002027
Iteration 106/1000 | Loss: 0.00002027
Iteration 107/1000 | Loss: 0.00002027
Iteration 108/1000 | Loss: 0.00002027
Iteration 109/1000 | Loss: 0.00002027
Iteration 110/1000 | Loss: 0.00002027
Iteration 111/1000 | Loss: 0.00002027
Iteration 112/1000 | Loss: 0.00002027
Iteration 113/1000 | Loss: 0.00002027
Iteration 114/1000 | Loss: 0.00002027
Iteration 115/1000 | Loss: 0.00002026
Iteration 116/1000 | Loss: 0.00002026
Iteration 117/1000 | Loss: 0.00002026
Iteration 118/1000 | Loss: 0.00002026
Iteration 119/1000 | Loss: 0.00002026
Iteration 120/1000 | Loss: 0.00002026
Iteration 121/1000 | Loss: 0.00002025
Iteration 122/1000 | Loss: 0.00002025
Iteration 123/1000 | Loss: 0.00002025
Iteration 124/1000 | Loss: 0.00002025
Iteration 125/1000 | Loss: 0.00002025
Iteration 126/1000 | Loss: 0.00002025
Iteration 127/1000 | Loss: 0.00002025
Iteration 128/1000 | Loss: 0.00002025
Iteration 129/1000 | Loss: 0.00002025
Iteration 130/1000 | Loss: 0.00002025
Iteration 131/1000 | Loss: 0.00002024
Iteration 132/1000 | Loss: 0.00002024
Iteration 133/1000 | Loss: 0.00002024
Iteration 134/1000 | Loss: 0.00002024
Iteration 135/1000 | Loss: 0.00002023
Iteration 136/1000 | Loss: 0.00002023
Iteration 137/1000 | Loss: 0.00002023
Iteration 138/1000 | Loss: 0.00002023
Iteration 139/1000 | Loss: 0.00002023
Iteration 140/1000 | Loss: 0.00002023
Iteration 141/1000 | Loss: 0.00002023
Iteration 142/1000 | Loss: 0.00002023
Iteration 143/1000 | Loss: 0.00002023
Iteration 144/1000 | Loss: 0.00002023
Iteration 145/1000 | Loss: 0.00002023
Iteration 146/1000 | Loss: 0.00002022
Iteration 147/1000 | Loss: 0.00002022
Iteration 148/1000 | Loss: 0.00002022
Iteration 149/1000 | Loss: 0.00002022
Iteration 150/1000 | Loss: 0.00002022
Iteration 151/1000 | Loss: 0.00002022
Iteration 152/1000 | Loss: 0.00002022
Iteration 153/1000 | Loss: 0.00002022
Iteration 154/1000 | Loss: 0.00002022
Iteration 155/1000 | Loss: 0.00002022
Iteration 156/1000 | Loss: 0.00002022
Iteration 157/1000 | Loss: 0.00002022
Iteration 158/1000 | Loss: 0.00002022
Iteration 159/1000 | Loss: 0.00002022
Iteration 160/1000 | Loss: 0.00002022
Iteration 161/1000 | Loss: 0.00002022
Iteration 162/1000 | Loss: 0.00002022
Iteration 163/1000 | Loss: 0.00002022
Iteration 164/1000 | Loss: 0.00002022
Iteration 165/1000 | Loss: 0.00002022
Iteration 166/1000 | Loss: 0.00002022
Iteration 167/1000 | Loss: 0.00002022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [2.0223860701662488e-05, 2.0223860701662488e-05, 2.0223860701662488e-05, 2.0223860701662488e-05, 2.0223860701662488e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0223860701662488e-05

Optimization complete. Final v2v error: 3.5935044288635254 mm

Highest mean error: 4.624382495880127 mm for frame 59

Lowest mean error: 2.739557981491089 mm for frame 137

Saving results

Total time: 48.63221335411072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825518
Iteration 2/25 | Loss: 0.00150096
Iteration 3/25 | Loss: 0.00134484
Iteration 4/25 | Loss: 0.00132705
Iteration 5/25 | Loss: 0.00132477
Iteration 6/25 | Loss: 0.00131019
Iteration 7/25 | Loss: 0.00130513
Iteration 8/25 | Loss: 0.00130364
Iteration 9/25 | Loss: 0.00130313
Iteration 10/25 | Loss: 0.00130282
Iteration 11/25 | Loss: 0.00130264
Iteration 12/25 | Loss: 0.00130257
Iteration 13/25 | Loss: 0.00130256
Iteration 14/25 | Loss: 0.00130256
Iteration 15/25 | Loss: 0.00130256
Iteration 16/25 | Loss: 0.00130255
Iteration 17/25 | Loss: 0.00130255
Iteration 18/25 | Loss: 0.00130255
Iteration 19/25 | Loss: 0.00130255
Iteration 20/25 | Loss: 0.00130255
Iteration 21/25 | Loss: 0.00130255
Iteration 22/25 | Loss: 0.00130255
Iteration 23/25 | Loss: 0.00130255
Iteration 24/25 | Loss: 0.00130255
Iteration 25/25 | Loss: 0.00130255

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14397955
Iteration 2/25 | Loss: 0.00161924
Iteration 3/25 | Loss: 0.00161920
Iteration 4/25 | Loss: 0.00161920
Iteration 5/25 | Loss: 0.00161920
Iteration 6/25 | Loss: 0.00161920
Iteration 7/25 | Loss: 0.00161920
Iteration 8/25 | Loss: 0.00161920
Iteration 9/25 | Loss: 0.00161920
Iteration 10/25 | Loss: 0.00161920
Iteration 11/25 | Loss: 0.00161920
Iteration 12/25 | Loss: 0.00161920
Iteration 13/25 | Loss: 0.00161920
Iteration 14/25 | Loss: 0.00161920
Iteration 15/25 | Loss: 0.00161920
Iteration 16/25 | Loss: 0.00161920
Iteration 17/25 | Loss: 0.00161920
Iteration 18/25 | Loss: 0.00161920
Iteration 19/25 | Loss: 0.00161920
Iteration 20/25 | Loss: 0.00161920
Iteration 21/25 | Loss: 0.00161920
Iteration 22/25 | Loss: 0.00161920
Iteration 23/25 | Loss: 0.00161920
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016191977774724364, 0.0016191977774724364, 0.0016191977774724364, 0.0016191977774724364, 0.0016191977774724364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016191977774724364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161920
Iteration 2/1000 | Loss: 0.00011899
Iteration 3/1000 | Loss: 0.00008481
Iteration 4/1000 | Loss: 0.00007003
Iteration 5/1000 | Loss: 0.00006474
Iteration 6/1000 | Loss: 0.00006062
Iteration 7/1000 | Loss: 0.00005776
Iteration 8/1000 | Loss: 0.00005595
Iteration 9/1000 | Loss: 0.00005508
Iteration 10/1000 | Loss: 0.00005398
Iteration 11/1000 | Loss: 0.00005326
Iteration 12/1000 | Loss: 0.00005230
Iteration 13/1000 | Loss: 0.00005135
Iteration 14/1000 | Loss: 0.00005038
Iteration 15/1000 | Loss: 0.00004979
Iteration 16/1000 | Loss: 0.00004925
Iteration 17/1000 | Loss: 0.00004880
Iteration 18/1000 | Loss: 0.00004833
Iteration 19/1000 | Loss: 0.00087853
Iteration 20/1000 | Loss: 0.00437872
Iteration 21/1000 | Loss: 0.00472038
Iteration 22/1000 | Loss: 0.00024009
Iteration 23/1000 | Loss: 0.00065703
Iteration 24/1000 | Loss: 0.00416943
Iteration 25/1000 | Loss: 0.00066994
Iteration 26/1000 | Loss: 0.00011268
Iteration 27/1000 | Loss: 0.00008466
Iteration 28/1000 | Loss: 0.00007112
Iteration 29/1000 | Loss: 0.00005746
Iteration 30/1000 | Loss: 0.00051443
Iteration 31/1000 | Loss: 0.00004465
Iteration 32/1000 | Loss: 0.00003712
Iteration 33/1000 | Loss: 0.00003437
Iteration 34/1000 | Loss: 0.00003226
Iteration 35/1000 | Loss: 0.00020583
Iteration 36/1000 | Loss: 0.00003799
Iteration 37/1000 | Loss: 0.00003227
Iteration 38/1000 | Loss: 0.00002950
Iteration 39/1000 | Loss: 0.00045229
Iteration 40/1000 | Loss: 0.00043330
Iteration 41/1000 | Loss: 0.00044700
Iteration 42/1000 | Loss: 0.00060922
Iteration 43/1000 | Loss: 0.00071991
Iteration 44/1000 | Loss: 0.00043637
Iteration 45/1000 | Loss: 0.00007311
Iteration 46/1000 | Loss: 0.00028824
Iteration 47/1000 | Loss: 0.00011218
Iteration 48/1000 | Loss: 0.00018195
Iteration 49/1000 | Loss: 0.00037444
Iteration 50/1000 | Loss: 0.00032966
Iteration 51/1000 | Loss: 0.00008638
Iteration 52/1000 | Loss: 0.00012065
Iteration 53/1000 | Loss: 0.00033341
Iteration 54/1000 | Loss: 0.00030403
Iteration 55/1000 | Loss: 0.00005203
Iteration 56/1000 | Loss: 0.00005331
Iteration 57/1000 | Loss: 0.00005382
Iteration 58/1000 | Loss: 0.00006373
Iteration 59/1000 | Loss: 0.00035123
Iteration 60/1000 | Loss: 0.00025623
Iteration 61/1000 | Loss: 0.00035373
Iteration 62/1000 | Loss: 0.00051886
Iteration 63/1000 | Loss: 0.00008607
Iteration 64/1000 | Loss: 0.00031824
Iteration 65/1000 | Loss: 0.00010714
Iteration 66/1000 | Loss: 0.00036858
Iteration 67/1000 | Loss: 0.00029997
Iteration 68/1000 | Loss: 0.00028624
Iteration 69/1000 | Loss: 0.00003715
Iteration 70/1000 | Loss: 0.00003444
Iteration 71/1000 | Loss: 0.00003256
Iteration 72/1000 | Loss: 0.00003055
Iteration 73/1000 | Loss: 0.00032743
Iteration 74/1000 | Loss: 0.00023090
Iteration 75/1000 | Loss: 0.00068787
Iteration 76/1000 | Loss: 0.00044904
Iteration 77/1000 | Loss: 0.00005612
Iteration 78/1000 | Loss: 0.00004061
Iteration 79/1000 | Loss: 0.00003465
Iteration 80/1000 | Loss: 0.00002969
Iteration 81/1000 | Loss: 0.00002686
Iteration 82/1000 | Loss: 0.00004236
Iteration 83/1000 | Loss: 0.00002535
Iteration 84/1000 | Loss: 0.00002324
Iteration 85/1000 | Loss: 0.00002248
Iteration 86/1000 | Loss: 0.00002182
Iteration 87/1000 | Loss: 0.00088450
Iteration 88/1000 | Loss: 0.00004592
Iteration 89/1000 | Loss: 0.00002829
Iteration 90/1000 | Loss: 0.00002323
Iteration 91/1000 | Loss: 0.00002143
Iteration 92/1000 | Loss: 0.00001985
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001819
Iteration 95/1000 | Loss: 0.00001747
Iteration 96/1000 | Loss: 0.00001701
Iteration 97/1000 | Loss: 0.00001666
Iteration 98/1000 | Loss: 0.00001649
Iteration 99/1000 | Loss: 0.00001643
Iteration 100/1000 | Loss: 0.00001635
Iteration 101/1000 | Loss: 0.00001633
Iteration 102/1000 | Loss: 0.00001633
Iteration 103/1000 | Loss: 0.00001633
Iteration 104/1000 | Loss: 0.00001632
Iteration 105/1000 | Loss: 0.00001632
Iteration 106/1000 | Loss: 0.00001632
Iteration 107/1000 | Loss: 0.00001631
Iteration 108/1000 | Loss: 0.00001631
Iteration 109/1000 | Loss: 0.00001631
Iteration 110/1000 | Loss: 0.00001630
Iteration 111/1000 | Loss: 0.00001628
Iteration 112/1000 | Loss: 0.00001627
Iteration 113/1000 | Loss: 0.00001626
Iteration 114/1000 | Loss: 0.00001626
Iteration 115/1000 | Loss: 0.00001625
Iteration 116/1000 | Loss: 0.00001625
Iteration 117/1000 | Loss: 0.00001624
Iteration 118/1000 | Loss: 0.00001623
Iteration 119/1000 | Loss: 0.00001622
Iteration 120/1000 | Loss: 0.00001621
Iteration 121/1000 | Loss: 0.00001620
Iteration 122/1000 | Loss: 0.00001620
Iteration 123/1000 | Loss: 0.00001619
Iteration 124/1000 | Loss: 0.00001619
Iteration 125/1000 | Loss: 0.00001619
Iteration 126/1000 | Loss: 0.00001618
Iteration 127/1000 | Loss: 0.00001618
Iteration 128/1000 | Loss: 0.00001618
Iteration 129/1000 | Loss: 0.00001617
Iteration 130/1000 | Loss: 0.00001617
Iteration 131/1000 | Loss: 0.00001615
Iteration 132/1000 | Loss: 0.00001614
Iteration 133/1000 | Loss: 0.00001614
Iteration 134/1000 | Loss: 0.00001614
Iteration 135/1000 | Loss: 0.00001613
Iteration 136/1000 | Loss: 0.00001613
Iteration 137/1000 | Loss: 0.00001613
Iteration 138/1000 | Loss: 0.00001612
Iteration 139/1000 | Loss: 0.00001612
Iteration 140/1000 | Loss: 0.00001612
Iteration 141/1000 | Loss: 0.00001612
Iteration 142/1000 | Loss: 0.00001611
Iteration 143/1000 | Loss: 0.00001611
Iteration 144/1000 | Loss: 0.00001611
Iteration 145/1000 | Loss: 0.00001610
Iteration 146/1000 | Loss: 0.00001610
Iteration 147/1000 | Loss: 0.00001608
Iteration 148/1000 | Loss: 0.00001608
Iteration 149/1000 | Loss: 0.00001607
Iteration 150/1000 | Loss: 0.00001607
Iteration 151/1000 | Loss: 0.00001606
Iteration 152/1000 | Loss: 0.00001606
Iteration 153/1000 | Loss: 0.00001606
Iteration 154/1000 | Loss: 0.00001606
Iteration 155/1000 | Loss: 0.00001606
Iteration 156/1000 | Loss: 0.00001606
Iteration 157/1000 | Loss: 0.00001606
Iteration 158/1000 | Loss: 0.00001606
Iteration 159/1000 | Loss: 0.00001606
Iteration 160/1000 | Loss: 0.00001605
Iteration 161/1000 | Loss: 0.00001605
Iteration 162/1000 | Loss: 0.00001605
Iteration 163/1000 | Loss: 0.00001605
Iteration 164/1000 | Loss: 0.00001605
Iteration 165/1000 | Loss: 0.00001605
Iteration 166/1000 | Loss: 0.00001605
Iteration 167/1000 | Loss: 0.00001605
Iteration 168/1000 | Loss: 0.00001605
Iteration 169/1000 | Loss: 0.00001604
Iteration 170/1000 | Loss: 0.00001604
Iteration 171/1000 | Loss: 0.00001604
Iteration 172/1000 | Loss: 0.00001604
Iteration 173/1000 | Loss: 0.00001604
Iteration 174/1000 | Loss: 0.00001604
Iteration 175/1000 | Loss: 0.00001604
Iteration 176/1000 | Loss: 0.00001604
Iteration 177/1000 | Loss: 0.00001604
Iteration 178/1000 | Loss: 0.00001604
Iteration 179/1000 | Loss: 0.00001604
Iteration 180/1000 | Loss: 0.00001604
Iteration 181/1000 | Loss: 0.00001604
Iteration 182/1000 | Loss: 0.00001604
Iteration 183/1000 | Loss: 0.00001604
Iteration 184/1000 | Loss: 0.00001604
Iteration 185/1000 | Loss: 0.00001603
Iteration 186/1000 | Loss: 0.00001603
Iteration 187/1000 | Loss: 0.00001603
Iteration 188/1000 | Loss: 0.00001603
Iteration 189/1000 | Loss: 0.00001603
Iteration 190/1000 | Loss: 0.00001603
Iteration 191/1000 | Loss: 0.00001603
Iteration 192/1000 | Loss: 0.00001603
Iteration 193/1000 | Loss: 0.00001603
Iteration 194/1000 | Loss: 0.00001602
Iteration 195/1000 | Loss: 0.00001602
Iteration 196/1000 | Loss: 0.00001602
Iteration 197/1000 | Loss: 0.00001602
Iteration 198/1000 | Loss: 0.00001602
Iteration 199/1000 | Loss: 0.00001602
Iteration 200/1000 | Loss: 0.00001602
Iteration 201/1000 | Loss: 0.00001602
Iteration 202/1000 | Loss: 0.00001602
Iteration 203/1000 | Loss: 0.00001602
Iteration 204/1000 | Loss: 0.00001602
Iteration 205/1000 | Loss: 0.00001601
Iteration 206/1000 | Loss: 0.00001601
Iteration 207/1000 | Loss: 0.00001601
Iteration 208/1000 | Loss: 0.00001601
Iteration 209/1000 | Loss: 0.00001601
Iteration 210/1000 | Loss: 0.00001600
Iteration 211/1000 | Loss: 0.00001600
Iteration 212/1000 | Loss: 0.00001600
Iteration 213/1000 | Loss: 0.00001599
Iteration 214/1000 | Loss: 0.00001599
Iteration 215/1000 | Loss: 0.00001598
Iteration 216/1000 | Loss: 0.00001598
Iteration 217/1000 | Loss: 0.00001598
Iteration 218/1000 | Loss: 0.00001598
Iteration 219/1000 | Loss: 0.00001597
Iteration 220/1000 | Loss: 0.00001597
Iteration 221/1000 | Loss: 0.00001597
Iteration 222/1000 | Loss: 0.00001596
Iteration 223/1000 | Loss: 0.00001596
Iteration 224/1000 | Loss: 0.00001596
Iteration 225/1000 | Loss: 0.00001596
Iteration 226/1000 | Loss: 0.00001595
Iteration 227/1000 | Loss: 0.00001595
Iteration 228/1000 | Loss: 0.00001595
Iteration 229/1000 | Loss: 0.00001594
Iteration 230/1000 | Loss: 0.00001594
Iteration 231/1000 | Loss: 0.00001594
Iteration 232/1000 | Loss: 0.00001594
Iteration 233/1000 | Loss: 0.00001594
Iteration 234/1000 | Loss: 0.00001593
Iteration 235/1000 | Loss: 0.00001593
Iteration 236/1000 | Loss: 0.00001593
Iteration 237/1000 | Loss: 0.00001593
Iteration 238/1000 | Loss: 0.00001593
Iteration 239/1000 | Loss: 0.00001593
Iteration 240/1000 | Loss: 0.00001593
Iteration 241/1000 | Loss: 0.00001593
Iteration 242/1000 | Loss: 0.00001593
Iteration 243/1000 | Loss: 0.00001593
Iteration 244/1000 | Loss: 0.00001593
Iteration 245/1000 | Loss: 0.00001593
Iteration 246/1000 | Loss: 0.00001593
Iteration 247/1000 | Loss: 0.00001593
Iteration 248/1000 | Loss: 0.00001593
Iteration 249/1000 | Loss: 0.00001593
Iteration 250/1000 | Loss: 0.00001593
Iteration 251/1000 | Loss: 0.00001593
Iteration 252/1000 | Loss: 0.00001593
Iteration 253/1000 | Loss: 0.00001593
Iteration 254/1000 | Loss: 0.00001593
Iteration 255/1000 | Loss: 0.00001592
Iteration 256/1000 | Loss: 0.00001592
Iteration 257/1000 | Loss: 0.00001592
Iteration 258/1000 | Loss: 0.00001592
Iteration 259/1000 | Loss: 0.00001592
Iteration 260/1000 | Loss: 0.00001592
Iteration 261/1000 | Loss: 0.00001592
Iteration 262/1000 | Loss: 0.00001592
Iteration 263/1000 | Loss: 0.00001592
Iteration 264/1000 | Loss: 0.00001592
Iteration 265/1000 | Loss: 0.00001592
Iteration 266/1000 | Loss: 0.00001592
Iteration 267/1000 | Loss: 0.00001592
Iteration 268/1000 | Loss: 0.00001592
Iteration 269/1000 | Loss: 0.00001592
Iteration 270/1000 | Loss: 0.00001592
Iteration 271/1000 | Loss: 0.00001592
Iteration 272/1000 | Loss: 0.00001592
Iteration 273/1000 | Loss: 0.00001592
Iteration 274/1000 | Loss: 0.00001592
Iteration 275/1000 | Loss: 0.00001592
Iteration 276/1000 | Loss: 0.00001592
Iteration 277/1000 | Loss: 0.00001592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [1.5921539670671336e-05, 1.5921539670671336e-05, 1.5921539670671336e-05, 1.5921539670671336e-05, 1.5921539670671336e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5921539670671336e-05

Optimization complete. Final v2v error: 3.286221504211426 mm

Highest mean error: 4.395448207855225 mm for frame 74

Lowest mean error: 2.9487202167510986 mm for frame 59

Saving results

Total time: 175.26816129684448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755435
Iteration 2/25 | Loss: 0.00133649
Iteration 3/25 | Loss: 0.00124518
Iteration 4/25 | Loss: 0.00122822
Iteration 5/25 | Loss: 0.00124208
Iteration 6/25 | Loss: 0.00119423
Iteration 7/25 | Loss: 0.00118835
Iteration 8/25 | Loss: 0.00118762
Iteration 9/25 | Loss: 0.00118744
Iteration 10/25 | Loss: 0.00118744
Iteration 11/25 | Loss: 0.00118744
Iteration 12/25 | Loss: 0.00118744
Iteration 13/25 | Loss: 0.00118744
Iteration 14/25 | Loss: 0.00118744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011874374467879534, 0.0011874374467879534, 0.0011874374467879534, 0.0011874374467879534, 0.0011874374467879534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011874374467879534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27572322
Iteration 2/25 | Loss: 0.00163100
Iteration 3/25 | Loss: 0.00163099
Iteration 4/25 | Loss: 0.00163099
Iteration 5/25 | Loss: 0.00163099
Iteration 6/25 | Loss: 0.00163099
Iteration 7/25 | Loss: 0.00163099
Iteration 8/25 | Loss: 0.00163099
Iteration 9/25 | Loss: 0.00163099
Iteration 10/25 | Loss: 0.00163099
Iteration 11/25 | Loss: 0.00163099
Iteration 12/25 | Loss: 0.00163099
Iteration 13/25 | Loss: 0.00163099
Iteration 14/25 | Loss: 0.00163099
Iteration 15/25 | Loss: 0.00163099
Iteration 16/25 | Loss: 0.00163099
Iteration 17/25 | Loss: 0.00163099
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016309896018356085, 0.0016309896018356085, 0.0016309896018356085, 0.0016309896018356085, 0.0016309896018356085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016309896018356085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163099
Iteration 2/1000 | Loss: 0.00002647
Iteration 3/1000 | Loss: 0.00001909
Iteration 4/1000 | Loss: 0.00001668
Iteration 5/1000 | Loss: 0.00001544
Iteration 6/1000 | Loss: 0.00001478
Iteration 7/1000 | Loss: 0.00001423
Iteration 8/1000 | Loss: 0.00001383
Iteration 9/1000 | Loss: 0.00001361
Iteration 10/1000 | Loss: 0.00001359
Iteration 11/1000 | Loss: 0.00001327
Iteration 12/1000 | Loss: 0.00001298
Iteration 13/1000 | Loss: 0.00001294
Iteration 14/1000 | Loss: 0.00001283
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001267
Iteration 17/1000 | Loss: 0.00001261
Iteration 18/1000 | Loss: 0.00001257
Iteration 19/1000 | Loss: 0.00001255
Iteration 20/1000 | Loss: 0.00001243
Iteration 21/1000 | Loss: 0.00001240
Iteration 22/1000 | Loss: 0.00001239
Iteration 23/1000 | Loss: 0.00001237
Iteration 24/1000 | Loss: 0.00001235
Iteration 25/1000 | Loss: 0.00001230
Iteration 26/1000 | Loss: 0.00001229
Iteration 27/1000 | Loss: 0.00001228
Iteration 28/1000 | Loss: 0.00001227
Iteration 29/1000 | Loss: 0.00001222
Iteration 30/1000 | Loss: 0.00001216
Iteration 31/1000 | Loss: 0.00001214
Iteration 32/1000 | Loss: 0.00001214
Iteration 33/1000 | Loss: 0.00001214
Iteration 34/1000 | Loss: 0.00001213
Iteration 35/1000 | Loss: 0.00001211
Iteration 36/1000 | Loss: 0.00001210
Iteration 37/1000 | Loss: 0.00001210
Iteration 38/1000 | Loss: 0.00001209
Iteration 39/1000 | Loss: 0.00001208
Iteration 40/1000 | Loss: 0.00001208
Iteration 41/1000 | Loss: 0.00001207
Iteration 42/1000 | Loss: 0.00001207
Iteration 43/1000 | Loss: 0.00001206
Iteration 44/1000 | Loss: 0.00001206
Iteration 45/1000 | Loss: 0.00001206
Iteration 46/1000 | Loss: 0.00001205
Iteration 47/1000 | Loss: 0.00001205
Iteration 48/1000 | Loss: 0.00001204
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001203
Iteration 51/1000 | Loss: 0.00001202
Iteration 52/1000 | Loss: 0.00001202
Iteration 53/1000 | Loss: 0.00001202
Iteration 54/1000 | Loss: 0.00001202
Iteration 55/1000 | Loss: 0.00001202
Iteration 56/1000 | Loss: 0.00001202
Iteration 57/1000 | Loss: 0.00001201
Iteration 58/1000 | Loss: 0.00001201
Iteration 59/1000 | Loss: 0.00001201
Iteration 60/1000 | Loss: 0.00001199
Iteration 61/1000 | Loss: 0.00001199
Iteration 62/1000 | Loss: 0.00001198
Iteration 63/1000 | Loss: 0.00001198
Iteration 64/1000 | Loss: 0.00001198
Iteration 65/1000 | Loss: 0.00001196
Iteration 66/1000 | Loss: 0.00001196
Iteration 67/1000 | Loss: 0.00001196
Iteration 68/1000 | Loss: 0.00001196
Iteration 69/1000 | Loss: 0.00001196
Iteration 70/1000 | Loss: 0.00001196
Iteration 71/1000 | Loss: 0.00001196
Iteration 72/1000 | Loss: 0.00001196
Iteration 73/1000 | Loss: 0.00001196
Iteration 74/1000 | Loss: 0.00001196
Iteration 75/1000 | Loss: 0.00001195
Iteration 76/1000 | Loss: 0.00001195
Iteration 77/1000 | Loss: 0.00001195
Iteration 78/1000 | Loss: 0.00001195
Iteration 79/1000 | Loss: 0.00001195
Iteration 80/1000 | Loss: 0.00001195
Iteration 81/1000 | Loss: 0.00001195
Iteration 82/1000 | Loss: 0.00001194
Iteration 83/1000 | Loss: 0.00001193
Iteration 84/1000 | Loss: 0.00001192
Iteration 85/1000 | Loss: 0.00001192
Iteration 86/1000 | Loss: 0.00001192
Iteration 87/1000 | Loss: 0.00001192
Iteration 88/1000 | Loss: 0.00001192
Iteration 89/1000 | Loss: 0.00001191
Iteration 90/1000 | Loss: 0.00001191
Iteration 91/1000 | Loss: 0.00001191
Iteration 92/1000 | Loss: 0.00001191
Iteration 93/1000 | Loss: 0.00001190
Iteration 94/1000 | Loss: 0.00001190
Iteration 95/1000 | Loss: 0.00001190
Iteration 96/1000 | Loss: 0.00001190
Iteration 97/1000 | Loss: 0.00001190
Iteration 98/1000 | Loss: 0.00001190
Iteration 99/1000 | Loss: 0.00001190
Iteration 100/1000 | Loss: 0.00001189
Iteration 101/1000 | Loss: 0.00001189
Iteration 102/1000 | Loss: 0.00001189
Iteration 103/1000 | Loss: 0.00001189
Iteration 104/1000 | Loss: 0.00001188
Iteration 105/1000 | Loss: 0.00001188
Iteration 106/1000 | Loss: 0.00001188
Iteration 107/1000 | Loss: 0.00001187
Iteration 108/1000 | Loss: 0.00001187
Iteration 109/1000 | Loss: 0.00001186
Iteration 110/1000 | Loss: 0.00001186
Iteration 111/1000 | Loss: 0.00001186
Iteration 112/1000 | Loss: 0.00001186
Iteration 113/1000 | Loss: 0.00001186
Iteration 114/1000 | Loss: 0.00001185
Iteration 115/1000 | Loss: 0.00001185
Iteration 116/1000 | Loss: 0.00001185
Iteration 117/1000 | Loss: 0.00001185
Iteration 118/1000 | Loss: 0.00001185
Iteration 119/1000 | Loss: 0.00001185
Iteration 120/1000 | Loss: 0.00001185
Iteration 121/1000 | Loss: 0.00001184
Iteration 122/1000 | Loss: 0.00001184
Iteration 123/1000 | Loss: 0.00001184
Iteration 124/1000 | Loss: 0.00001183
Iteration 125/1000 | Loss: 0.00001183
Iteration 126/1000 | Loss: 0.00001183
Iteration 127/1000 | Loss: 0.00001182
Iteration 128/1000 | Loss: 0.00001182
Iteration 129/1000 | Loss: 0.00001182
Iteration 130/1000 | Loss: 0.00001182
Iteration 131/1000 | Loss: 0.00001182
Iteration 132/1000 | Loss: 0.00001182
Iteration 133/1000 | Loss: 0.00001181
Iteration 134/1000 | Loss: 0.00001181
Iteration 135/1000 | Loss: 0.00001181
Iteration 136/1000 | Loss: 0.00001181
Iteration 137/1000 | Loss: 0.00001181
Iteration 138/1000 | Loss: 0.00001181
Iteration 139/1000 | Loss: 0.00001181
Iteration 140/1000 | Loss: 0.00001181
Iteration 141/1000 | Loss: 0.00001181
Iteration 142/1000 | Loss: 0.00001181
Iteration 143/1000 | Loss: 0.00001181
Iteration 144/1000 | Loss: 0.00001180
Iteration 145/1000 | Loss: 0.00001180
Iteration 146/1000 | Loss: 0.00001180
Iteration 147/1000 | Loss: 0.00001180
Iteration 148/1000 | Loss: 0.00001180
Iteration 149/1000 | Loss: 0.00001180
Iteration 150/1000 | Loss: 0.00001180
Iteration 151/1000 | Loss: 0.00001180
Iteration 152/1000 | Loss: 0.00001180
Iteration 153/1000 | Loss: 0.00001180
Iteration 154/1000 | Loss: 0.00001180
Iteration 155/1000 | Loss: 0.00001180
Iteration 156/1000 | Loss: 0.00001180
Iteration 157/1000 | Loss: 0.00001179
Iteration 158/1000 | Loss: 0.00001179
Iteration 159/1000 | Loss: 0.00001179
Iteration 160/1000 | Loss: 0.00001179
Iteration 161/1000 | Loss: 0.00001179
Iteration 162/1000 | Loss: 0.00001179
Iteration 163/1000 | Loss: 0.00001179
Iteration 164/1000 | Loss: 0.00001179
Iteration 165/1000 | Loss: 0.00001179
Iteration 166/1000 | Loss: 0.00001179
Iteration 167/1000 | Loss: 0.00001179
Iteration 168/1000 | Loss: 0.00001179
Iteration 169/1000 | Loss: 0.00001179
Iteration 170/1000 | Loss: 0.00001179
Iteration 171/1000 | Loss: 0.00001179
Iteration 172/1000 | Loss: 0.00001179
Iteration 173/1000 | Loss: 0.00001179
Iteration 174/1000 | Loss: 0.00001179
Iteration 175/1000 | Loss: 0.00001179
Iteration 176/1000 | Loss: 0.00001179
Iteration 177/1000 | Loss: 0.00001179
Iteration 178/1000 | Loss: 0.00001179
Iteration 179/1000 | Loss: 0.00001179
Iteration 180/1000 | Loss: 0.00001179
Iteration 181/1000 | Loss: 0.00001179
Iteration 182/1000 | Loss: 0.00001179
Iteration 183/1000 | Loss: 0.00001179
Iteration 184/1000 | Loss: 0.00001179
Iteration 185/1000 | Loss: 0.00001179
Iteration 186/1000 | Loss: 0.00001179
Iteration 187/1000 | Loss: 0.00001179
Iteration 188/1000 | Loss: 0.00001179
Iteration 189/1000 | Loss: 0.00001179
Iteration 190/1000 | Loss: 0.00001179
Iteration 191/1000 | Loss: 0.00001179
Iteration 192/1000 | Loss: 0.00001179
Iteration 193/1000 | Loss: 0.00001179
Iteration 194/1000 | Loss: 0.00001179
Iteration 195/1000 | Loss: 0.00001179
Iteration 196/1000 | Loss: 0.00001179
Iteration 197/1000 | Loss: 0.00001179
Iteration 198/1000 | Loss: 0.00001179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.1786652066803072e-05, 1.1786652066803072e-05, 1.1786652066803072e-05, 1.1786652066803072e-05, 1.1786652066803072e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1786652066803072e-05

Optimization complete. Final v2v error: 2.9599788188934326 mm

Highest mean error: 3.4796929359436035 mm for frame 112

Lowest mean error: 2.4446489810943604 mm for frame 203

Saving results

Total time: 58.32663083076477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024039
Iteration 2/25 | Loss: 0.00319304
Iteration 3/25 | Loss: 0.00278140
Iteration 4/25 | Loss: 0.00283249
Iteration 5/25 | Loss: 0.00258497
Iteration 6/25 | Loss: 0.00254206
Iteration 7/25 | Loss: 0.00247264
Iteration 8/25 | Loss: 0.00242077
Iteration 9/25 | Loss: 0.00240670
Iteration 10/25 | Loss: 0.00237963
Iteration 11/25 | Loss: 0.00232448
Iteration 12/25 | Loss: 0.00232532
Iteration 13/25 | Loss: 0.00231536
Iteration 14/25 | Loss: 0.00229975
Iteration 15/25 | Loss: 0.00227254
Iteration 16/25 | Loss: 0.00226672
Iteration 17/25 | Loss: 0.00226027
Iteration 18/25 | Loss: 0.00224994
Iteration 19/25 | Loss: 0.00224704
Iteration 20/25 | Loss: 0.00224444
Iteration 21/25 | Loss: 0.00224250
Iteration 22/25 | Loss: 0.00224187
Iteration 23/25 | Loss: 0.00224152
Iteration 24/25 | Loss: 0.00224224
Iteration 25/25 | Loss: 0.00224132

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08265781
Iteration 2/25 | Loss: 0.00438095
Iteration 3/25 | Loss: 0.00438091
Iteration 4/25 | Loss: 0.00438091
Iteration 5/25 | Loss: 0.00438091
Iteration 6/25 | Loss: 0.00438091
Iteration 7/25 | Loss: 0.00438091
Iteration 8/25 | Loss: 0.00438090
Iteration 9/25 | Loss: 0.00438090
Iteration 10/25 | Loss: 0.00438090
Iteration 11/25 | Loss: 0.00438090
Iteration 12/25 | Loss: 0.00438090
Iteration 13/25 | Loss: 0.00438090
Iteration 14/25 | Loss: 0.00438090
Iteration 15/25 | Loss: 0.00438090
Iteration 16/25 | Loss: 0.00438090
Iteration 17/25 | Loss: 0.00438090
Iteration 18/25 | Loss: 0.00438090
Iteration 19/25 | Loss: 0.00438090
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004380902741104364, 0.004380902741104364, 0.004380902741104364, 0.004380902741104364, 0.004380902741104364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004380902741104364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00438090
Iteration 2/1000 | Loss: 0.00143016
Iteration 3/1000 | Loss: 0.00178361
Iteration 4/1000 | Loss: 0.00474505
Iteration 5/1000 | Loss: 0.00941452
Iteration 6/1000 | Loss: 0.00398043
Iteration 7/1000 | Loss: 0.00183414
Iteration 8/1000 | Loss: 0.00089819
Iteration 9/1000 | Loss: 0.00327522
Iteration 10/1000 | Loss: 0.00139343
Iteration 11/1000 | Loss: 0.00167230
Iteration 12/1000 | Loss: 0.00432697
Iteration 13/1000 | Loss: 0.00434145
Iteration 14/1000 | Loss: 0.00226360
Iteration 15/1000 | Loss: 0.00095851
Iteration 16/1000 | Loss: 0.00075348
Iteration 17/1000 | Loss: 0.00127861
Iteration 18/1000 | Loss: 0.00092998
Iteration 19/1000 | Loss: 0.00076852
Iteration 20/1000 | Loss: 0.00122453
Iteration 21/1000 | Loss: 0.00148042
Iteration 22/1000 | Loss: 0.00137633
Iteration 23/1000 | Loss: 0.00103402
Iteration 24/1000 | Loss: 0.00063556
Iteration 25/1000 | Loss: 0.00115940
Iteration 26/1000 | Loss: 0.00105990
Iteration 27/1000 | Loss: 0.00118460
Iteration 28/1000 | Loss: 0.00059488
Iteration 29/1000 | Loss: 0.00046792
Iteration 30/1000 | Loss: 0.00548750
Iteration 31/1000 | Loss: 0.01226176
Iteration 32/1000 | Loss: 0.01689336
Iteration 33/1000 | Loss: 0.03335595
Iteration 34/1000 | Loss: 0.02585391
Iteration 35/1000 | Loss: 0.01787043
Iteration 36/1000 | Loss: 0.01123902
Iteration 37/1000 | Loss: 0.00694437
Iteration 38/1000 | Loss: 0.00742388
Iteration 39/1000 | Loss: 0.00790071
Iteration 40/1000 | Loss: 0.00305317
Iteration 41/1000 | Loss: 0.00167081
Iteration 42/1000 | Loss: 0.00205618
Iteration 43/1000 | Loss: 0.00121927
Iteration 44/1000 | Loss: 0.00115887
Iteration 45/1000 | Loss: 0.00086222
Iteration 46/1000 | Loss: 0.00350917
Iteration 47/1000 | Loss: 0.00147431
Iteration 48/1000 | Loss: 0.00095875
Iteration 49/1000 | Loss: 0.00640030
Iteration 50/1000 | Loss: 0.00515285
Iteration 51/1000 | Loss: 0.00540003
Iteration 52/1000 | Loss: 0.01074753
Iteration 53/1000 | Loss: 0.00198149
Iteration 54/1000 | Loss: 0.00074273
Iteration 55/1000 | Loss: 0.00070588
Iteration 56/1000 | Loss: 0.00384669
Iteration 57/1000 | Loss: 0.00097618
Iteration 58/1000 | Loss: 0.00140400
Iteration 59/1000 | Loss: 0.00064536
Iteration 60/1000 | Loss: 0.00047651
Iteration 61/1000 | Loss: 0.00150845
Iteration 62/1000 | Loss: 0.00183245
Iteration 63/1000 | Loss: 0.00132545
Iteration 64/1000 | Loss: 0.00128742
Iteration 65/1000 | Loss: 0.00179817
Iteration 66/1000 | Loss: 0.00114083
Iteration 67/1000 | Loss: 0.00210857
Iteration 68/1000 | Loss: 0.00103995
Iteration 69/1000 | Loss: 0.00150650
Iteration 70/1000 | Loss: 0.00253151
Iteration 71/1000 | Loss: 0.00051825
Iteration 72/1000 | Loss: 0.00202896
Iteration 73/1000 | Loss: 0.00053647
Iteration 74/1000 | Loss: 0.00049711
Iteration 75/1000 | Loss: 0.00051573
Iteration 76/1000 | Loss: 0.00041273
Iteration 77/1000 | Loss: 0.00059064
Iteration 78/1000 | Loss: 0.00069038
Iteration 79/1000 | Loss: 0.00066774
Iteration 80/1000 | Loss: 0.00086989
Iteration 81/1000 | Loss: 0.00032269
Iteration 82/1000 | Loss: 0.00036180
Iteration 83/1000 | Loss: 0.00031981
Iteration 84/1000 | Loss: 0.00033088
Iteration 85/1000 | Loss: 0.00030664
Iteration 86/1000 | Loss: 0.00037156
Iteration 87/1000 | Loss: 0.00084103
Iteration 88/1000 | Loss: 0.00055099
Iteration 89/1000 | Loss: 0.00088319
Iteration 90/1000 | Loss: 0.00056126
Iteration 91/1000 | Loss: 0.00026940
Iteration 92/1000 | Loss: 0.00039762
Iteration 93/1000 | Loss: 0.00025464
Iteration 94/1000 | Loss: 0.00058183
Iteration 95/1000 | Loss: 0.00032595
Iteration 96/1000 | Loss: 0.00027774
Iteration 97/1000 | Loss: 0.00046887
Iteration 98/1000 | Loss: 0.00048993
Iteration 99/1000 | Loss: 0.00031098
Iteration 100/1000 | Loss: 0.00029260
Iteration 101/1000 | Loss: 0.00190749
Iteration 102/1000 | Loss: 0.00050357
Iteration 103/1000 | Loss: 0.00026782
Iteration 104/1000 | Loss: 0.00024459
Iteration 105/1000 | Loss: 0.00027650
Iteration 106/1000 | Loss: 0.00037678
Iteration 107/1000 | Loss: 0.00038212
Iteration 108/1000 | Loss: 0.00040850
Iteration 109/1000 | Loss: 0.00032016
Iteration 110/1000 | Loss: 0.00026893
Iteration 111/1000 | Loss: 0.00024822
Iteration 112/1000 | Loss: 0.00027010
Iteration 113/1000 | Loss: 0.00024260
Iteration 114/1000 | Loss: 0.00028321
Iteration 115/1000 | Loss: 0.00041477
Iteration 116/1000 | Loss: 0.00027220
Iteration 117/1000 | Loss: 0.00032919
Iteration 118/1000 | Loss: 0.00027333
Iteration 119/1000 | Loss: 0.00041409
Iteration 120/1000 | Loss: 0.00022624
Iteration 121/1000 | Loss: 0.00027287
Iteration 122/1000 | Loss: 0.00098418
Iteration 123/1000 | Loss: 0.00032340
Iteration 124/1000 | Loss: 0.00025343
Iteration 125/1000 | Loss: 0.00021958
Iteration 126/1000 | Loss: 0.00046480
Iteration 127/1000 | Loss: 0.00077030
Iteration 128/1000 | Loss: 0.00021361
Iteration 129/1000 | Loss: 0.00020687
Iteration 130/1000 | Loss: 0.00020333
Iteration 131/1000 | Loss: 0.00070903
Iteration 132/1000 | Loss: 0.00027495
Iteration 133/1000 | Loss: 0.00066177
Iteration 134/1000 | Loss: 0.00033480
Iteration 135/1000 | Loss: 0.00151430
Iteration 136/1000 | Loss: 0.00038381
Iteration 137/1000 | Loss: 0.00032531
Iteration 138/1000 | Loss: 0.00027409
Iteration 139/1000 | Loss: 0.00017978
Iteration 140/1000 | Loss: 0.00035744
Iteration 141/1000 | Loss: 0.00037475
Iteration 142/1000 | Loss: 0.00025898
Iteration 143/1000 | Loss: 0.00074320
Iteration 144/1000 | Loss: 0.00026496
Iteration 145/1000 | Loss: 0.00047869
Iteration 146/1000 | Loss: 0.00087262
Iteration 147/1000 | Loss: 0.00070726
Iteration 148/1000 | Loss: 0.00034152
Iteration 149/1000 | Loss: 0.00051147
Iteration 150/1000 | Loss: 0.00041759
Iteration 151/1000 | Loss: 0.00056316
Iteration 152/1000 | Loss: 0.00027348
Iteration 153/1000 | Loss: 0.00019180
Iteration 154/1000 | Loss: 0.00026672
Iteration 155/1000 | Loss: 0.00032709
Iteration 156/1000 | Loss: 0.00015081
Iteration 157/1000 | Loss: 0.00008666
Iteration 158/1000 | Loss: 0.00009321
Iteration 159/1000 | Loss: 0.00010066
Iteration 160/1000 | Loss: 0.00009265
Iteration 161/1000 | Loss: 0.00008568
Iteration 162/1000 | Loss: 0.00007405
Iteration 163/1000 | Loss: 0.00010207
Iteration 164/1000 | Loss: 0.00007594
Iteration 165/1000 | Loss: 0.00025614
Iteration 166/1000 | Loss: 0.00016412
Iteration 167/1000 | Loss: 0.00006373
Iteration 168/1000 | Loss: 0.00017879
Iteration 169/1000 | Loss: 0.00018928
Iteration 170/1000 | Loss: 0.00014882
Iteration 171/1000 | Loss: 0.00008011
Iteration 172/1000 | Loss: 0.00006117
Iteration 173/1000 | Loss: 0.00005718
Iteration 174/1000 | Loss: 0.00005529
Iteration 175/1000 | Loss: 0.00005394
Iteration 176/1000 | Loss: 0.00005268
Iteration 177/1000 | Loss: 0.00005176
Iteration 178/1000 | Loss: 0.00005090
Iteration 179/1000 | Loss: 0.00004987
Iteration 180/1000 | Loss: 0.00004923
Iteration 181/1000 | Loss: 0.00006607
Iteration 182/1000 | Loss: 0.00005126
Iteration 183/1000 | Loss: 0.00004906
Iteration 184/1000 | Loss: 0.00004790
Iteration 185/1000 | Loss: 0.00005988
Iteration 186/1000 | Loss: 0.00006702
Iteration 187/1000 | Loss: 0.00006255
Iteration 188/1000 | Loss: 0.00005521
Iteration 189/1000 | Loss: 0.00005958
Iteration 190/1000 | Loss: 0.00005927
Iteration 191/1000 | Loss: 0.00005299
Iteration 192/1000 | Loss: 0.00005609
Iteration 193/1000 | Loss: 0.00005952
Iteration 194/1000 | Loss: 0.00005479
Iteration 195/1000 | Loss: 0.00005899
Iteration 196/1000 | Loss: 0.00005987
Iteration 197/1000 | Loss: 0.00005971
Iteration 198/1000 | Loss: 0.00008392
Iteration 199/1000 | Loss: 0.00007524
Iteration 200/1000 | Loss: 0.00005592
Iteration 201/1000 | Loss: 0.00005338
Iteration 202/1000 | Loss: 0.00006161
Iteration 203/1000 | Loss: 0.00006633
Iteration 204/1000 | Loss: 0.00008790
Iteration 205/1000 | Loss: 0.00008345
Iteration 206/1000 | Loss: 0.00005758
Iteration 207/1000 | Loss: 0.00007112
Iteration 208/1000 | Loss: 0.00006913
Iteration 209/1000 | Loss: 0.00006204
Iteration 210/1000 | Loss: 0.00006345
Iteration 211/1000 | Loss: 0.00005849
Iteration 212/1000 | Loss: 0.00006617
Iteration 213/1000 | Loss: 0.00006122
Iteration 214/1000 | Loss: 0.00007647
Iteration 215/1000 | Loss: 0.00005683
Iteration 216/1000 | Loss: 0.00004971
Iteration 217/1000 | Loss: 0.00004829
Iteration 218/1000 | Loss: 0.00004750
Iteration 219/1000 | Loss: 0.00004717
Iteration 220/1000 | Loss: 0.00004686
Iteration 221/1000 | Loss: 0.00004674
Iteration 222/1000 | Loss: 0.00004670
Iteration 223/1000 | Loss: 0.00004665
Iteration 224/1000 | Loss: 0.00004665
Iteration 225/1000 | Loss: 0.00004653
Iteration 226/1000 | Loss: 0.00004635
Iteration 227/1000 | Loss: 0.00007366
Iteration 228/1000 | Loss: 0.00005430
Iteration 229/1000 | Loss: 0.00005245
Iteration 230/1000 | Loss: 0.00005000
Iteration 231/1000 | Loss: 0.00004856
Iteration 232/1000 | Loss: 0.00004806
Iteration 233/1000 | Loss: 0.00004767
Iteration 234/1000 | Loss: 0.00004739
Iteration 235/1000 | Loss: 0.00004728
Iteration 236/1000 | Loss: 0.00004728
Iteration 237/1000 | Loss: 0.00004727
Iteration 238/1000 | Loss: 0.00004723
Iteration 239/1000 | Loss: 0.00004718
Iteration 240/1000 | Loss: 0.00004718
Iteration 241/1000 | Loss: 0.00004717
Iteration 242/1000 | Loss: 0.00004717
Iteration 243/1000 | Loss: 0.00004717
Iteration 244/1000 | Loss: 0.00004717
Iteration 245/1000 | Loss: 0.00004717
Iteration 246/1000 | Loss: 0.00004717
Iteration 247/1000 | Loss: 0.00004717
Iteration 248/1000 | Loss: 0.00004717
Iteration 249/1000 | Loss: 0.00004717
Iteration 250/1000 | Loss: 0.00004716
Iteration 251/1000 | Loss: 0.00004716
Iteration 252/1000 | Loss: 0.00004716
Iteration 253/1000 | Loss: 0.00004715
Iteration 254/1000 | Loss: 0.00004715
Iteration 255/1000 | Loss: 0.00004715
Iteration 256/1000 | Loss: 0.00004715
Iteration 257/1000 | Loss: 0.00004715
Iteration 258/1000 | Loss: 0.00004715
Iteration 259/1000 | Loss: 0.00004715
Iteration 260/1000 | Loss: 0.00004715
Iteration 261/1000 | Loss: 0.00004715
Iteration 262/1000 | Loss: 0.00004714
Iteration 263/1000 | Loss: 0.00004714
Iteration 264/1000 | Loss: 0.00004714
Iteration 265/1000 | Loss: 0.00004713
Iteration 266/1000 | Loss: 0.00004713
Iteration 267/1000 | Loss: 0.00004713
Iteration 268/1000 | Loss: 0.00004713
Iteration 269/1000 | Loss: 0.00004712
Iteration 270/1000 | Loss: 0.00004712
Iteration 271/1000 | Loss: 0.00004712
Iteration 272/1000 | Loss: 0.00004711
Iteration 273/1000 | Loss: 0.00004711
Iteration 274/1000 | Loss: 0.00004710
Iteration 275/1000 | Loss: 0.00004709
Iteration 276/1000 | Loss: 0.00004709
Iteration 277/1000 | Loss: 0.00004708
Iteration 278/1000 | Loss: 0.00004702
Iteration 279/1000 | Loss: 0.00004701
Iteration 280/1000 | Loss: 0.00004683
Iteration 281/1000 | Loss: 0.00004673
Iteration 282/1000 | Loss: 0.00004655
Iteration 283/1000 | Loss: 0.00004651
Iteration 284/1000 | Loss: 0.00004648
Iteration 285/1000 | Loss: 0.00004633
Iteration 286/1000 | Loss: 0.00005131
Iteration 287/1000 | Loss: 0.00004876
Iteration 288/1000 | Loss: 0.00004791
Iteration 289/1000 | Loss: 0.00004744
Iteration 290/1000 | Loss: 0.00004692
Iteration 291/1000 | Loss: 0.00004630
Iteration 292/1000 | Loss: 0.00004586
Iteration 293/1000 | Loss: 0.00004578
Iteration 294/1000 | Loss: 0.00004566
Iteration 295/1000 | Loss: 0.00004565
Iteration 296/1000 | Loss: 0.00004565
Iteration 297/1000 | Loss: 0.00004562
Iteration 298/1000 | Loss: 0.00004561
Iteration 299/1000 | Loss: 0.00004559
Iteration 300/1000 | Loss: 0.00004558
Iteration 301/1000 | Loss: 0.00004554
Iteration 302/1000 | Loss: 0.00004553
Iteration 303/1000 | Loss: 0.00004552
Iteration 304/1000 | Loss: 0.00004549
Iteration 305/1000 | Loss: 0.00004549
Iteration 306/1000 | Loss: 0.00004549
Iteration 307/1000 | Loss: 0.00004549
Iteration 308/1000 | Loss: 0.00004549
Iteration 309/1000 | Loss: 0.00004549
Iteration 310/1000 | Loss: 0.00004549
Iteration 311/1000 | Loss: 0.00004549
Iteration 312/1000 | Loss: 0.00004548
Iteration 313/1000 | Loss: 0.00004548
Iteration 314/1000 | Loss: 0.00004547
Iteration 315/1000 | Loss: 0.00004547
Iteration 316/1000 | Loss: 0.00004547
Iteration 317/1000 | Loss: 0.00004547
Iteration 318/1000 | Loss: 0.00004546
Iteration 319/1000 | Loss: 0.00004546
Iteration 320/1000 | Loss: 0.00004546
Iteration 321/1000 | Loss: 0.00004546
Iteration 322/1000 | Loss: 0.00004545
Iteration 323/1000 | Loss: 0.00004545
Iteration 324/1000 | Loss: 0.00004545
Iteration 325/1000 | Loss: 0.00004545
Iteration 326/1000 | Loss: 0.00004545
Iteration 327/1000 | Loss: 0.00004545
Iteration 328/1000 | Loss: 0.00004545
Iteration 329/1000 | Loss: 0.00004544
Iteration 330/1000 | Loss: 0.00004544
Iteration 331/1000 | Loss: 0.00004544
Iteration 332/1000 | Loss: 0.00004544
Iteration 333/1000 | Loss: 0.00004544
Iteration 334/1000 | Loss: 0.00004544
Iteration 335/1000 | Loss: 0.00004544
Iteration 336/1000 | Loss: 0.00004544
Iteration 337/1000 | Loss: 0.00004544
Iteration 338/1000 | Loss: 0.00004544
Iteration 339/1000 | Loss: 0.00004544
Iteration 340/1000 | Loss: 0.00004544
Iteration 341/1000 | Loss: 0.00004544
Iteration 342/1000 | Loss: 0.00004544
Iteration 343/1000 | Loss: 0.00004544
Iteration 344/1000 | Loss: 0.00004544
Iteration 345/1000 | Loss: 0.00004544
Iteration 346/1000 | Loss: 0.00004544
Iteration 347/1000 | Loss: 0.00004544
Iteration 348/1000 | Loss: 0.00004544
Iteration 349/1000 | Loss: 0.00004544
Iteration 350/1000 | Loss: 0.00004544
Iteration 351/1000 | Loss: 0.00004544
Iteration 352/1000 | Loss: 0.00004544
Iteration 353/1000 | Loss: 0.00004544
Iteration 354/1000 | Loss: 0.00004544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 354. Stopping optimization.
Last 5 losses: [4.5443823182722554e-05, 4.5443823182722554e-05, 4.5443823182722554e-05, 4.5443823182722554e-05, 4.5443823182722554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.5443823182722554e-05

Optimization complete. Final v2v error: 5.623133659362793 mm

Highest mean error: 7.663520336151123 mm for frame 213

Lowest mean error: 4.164194107055664 mm for frame 0

Saving results

Total time: 462.71282291412354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775853
Iteration 2/25 | Loss: 0.00124240
Iteration 3/25 | Loss: 0.00118261
Iteration 4/25 | Loss: 0.00117338
Iteration 5/25 | Loss: 0.00117075
Iteration 6/25 | Loss: 0.00117040
Iteration 7/25 | Loss: 0.00117040
Iteration 8/25 | Loss: 0.00117040
Iteration 9/25 | Loss: 0.00117040
Iteration 10/25 | Loss: 0.00117040
Iteration 11/25 | Loss: 0.00117040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011704006465151906, 0.0011704006465151906, 0.0011704006465151906, 0.0011704006465151906, 0.0011704006465151906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011704006465151906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33564186
Iteration 2/25 | Loss: 0.00151617
Iteration 3/25 | Loss: 0.00151617
Iteration 4/25 | Loss: 0.00151617
Iteration 5/25 | Loss: 0.00151617
Iteration 6/25 | Loss: 0.00151617
Iteration 7/25 | Loss: 0.00151617
Iteration 8/25 | Loss: 0.00151616
Iteration 9/25 | Loss: 0.00151616
Iteration 10/25 | Loss: 0.00151616
Iteration 11/25 | Loss: 0.00151616
Iteration 12/25 | Loss: 0.00151616
Iteration 13/25 | Loss: 0.00151616
Iteration 14/25 | Loss: 0.00151616
Iteration 15/25 | Loss: 0.00151616
Iteration 16/25 | Loss: 0.00151616
Iteration 17/25 | Loss: 0.00151616
Iteration 18/25 | Loss: 0.00151616
Iteration 19/25 | Loss: 0.00151616
Iteration 20/25 | Loss: 0.00151616
Iteration 21/25 | Loss: 0.00151616
Iteration 22/25 | Loss: 0.00151616
Iteration 23/25 | Loss: 0.00151616
Iteration 24/25 | Loss: 0.00151616
Iteration 25/25 | Loss: 0.00151616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151616
Iteration 2/1000 | Loss: 0.00002037
Iteration 3/1000 | Loss: 0.00001504
Iteration 4/1000 | Loss: 0.00001311
Iteration 5/1000 | Loss: 0.00001230
Iteration 6/1000 | Loss: 0.00001154
Iteration 7/1000 | Loss: 0.00001093
Iteration 8/1000 | Loss: 0.00001063
Iteration 9/1000 | Loss: 0.00001035
Iteration 10/1000 | Loss: 0.00001025
Iteration 11/1000 | Loss: 0.00001014
Iteration 12/1000 | Loss: 0.00001005
Iteration 13/1000 | Loss: 0.00000991
Iteration 14/1000 | Loss: 0.00000987
Iteration 15/1000 | Loss: 0.00000980
Iteration 16/1000 | Loss: 0.00000978
Iteration 17/1000 | Loss: 0.00000978
Iteration 18/1000 | Loss: 0.00000974
Iteration 19/1000 | Loss: 0.00000973
Iteration 20/1000 | Loss: 0.00000973
Iteration 21/1000 | Loss: 0.00000973
Iteration 22/1000 | Loss: 0.00000973
Iteration 23/1000 | Loss: 0.00000973
Iteration 24/1000 | Loss: 0.00000972
Iteration 25/1000 | Loss: 0.00000972
Iteration 26/1000 | Loss: 0.00000971
Iteration 27/1000 | Loss: 0.00000970
Iteration 28/1000 | Loss: 0.00000967
Iteration 29/1000 | Loss: 0.00000966
Iteration 30/1000 | Loss: 0.00000961
Iteration 31/1000 | Loss: 0.00000961
Iteration 32/1000 | Loss: 0.00000959
Iteration 33/1000 | Loss: 0.00000959
Iteration 34/1000 | Loss: 0.00000959
Iteration 35/1000 | Loss: 0.00000959
Iteration 36/1000 | Loss: 0.00000959
Iteration 37/1000 | Loss: 0.00000959
Iteration 38/1000 | Loss: 0.00000958
Iteration 39/1000 | Loss: 0.00000958
Iteration 40/1000 | Loss: 0.00000958
Iteration 41/1000 | Loss: 0.00000958
Iteration 42/1000 | Loss: 0.00000958
Iteration 43/1000 | Loss: 0.00000958
Iteration 44/1000 | Loss: 0.00000958
Iteration 45/1000 | Loss: 0.00000957
Iteration 46/1000 | Loss: 0.00000955
Iteration 47/1000 | Loss: 0.00000955
Iteration 48/1000 | Loss: 0.00000954
Iteration 49/1000 | Loss: 0.00000953
Iteration 50/1000 | Loss: 0.00000953
Iteration 51/1000 | Loss: 0.00000953
Iteration 52/1000 | Loss: 0.00000953
Iteration 53/1000 | Loss: 0.00000953
Iteration 54/1000 | Loss: 0.00000953
Iteration 55/1000 | Loss: 0.00000952
Iteration 56/1000 | Loss: 0.00000952
Iteration 57/1000 | Loss: 0.00000951
Iteration 58/1000 | Loss: 0.00000950
Iteration 59/1000 | Loss: 0.00000950
Iteration 60/1000 | Loss: 0.00000949
Iteration 61/1000 | Loss: 0.00000948
Iteration 62/1000 | Loss: 0.00000948
Iteration 63/1000 | Loss: 0.00000948
Iteration 64/1000 | Loss: 0.00000947
Iteration 65/1000 | Loss: 0.00000947
Iteration 66/1000 | Loss: 0.00000947
Iteration 67/1000 | Loss: 0.00000947
Iteration 68/1000 | Loss: 0.00000947
Iteration 69/1000 | Loss: 0.00000947
Iteration 70/1000 | Loss: 0.00000946
Iteration 71/1000 | Loss: 0.00000946
Iteration 72/1000 | Loss: 0.00000946
Iteration 73/1000 | Loss: 0.00000946
Iteration 74/1000 | Loss: 0.00000946
Iteration 75/1000 | Loss: 0.00000946
Iteration 76/1000 | Loss: 0.00000946
Iteration 77/1000 | Loss: 0.00000945
Iteration 78/1000 | Loss: 0.00000945
Iteration 79/1000 | Loss: 0.00000944
Iteration 80/1000 | Loss: 0.00000943
Iteration 81/1000 | Loss: 0.00000943
Iteration 82/1000 | Loss: 0.00000943
Iteration 83/1000 | Loss: 0.00000942
Iteration 84/1000 | Loss: 0.00000942
Iteration 85/1000 | Loss: 0.00000942
Iteration 86/1000 | Loss: 0.00000942
Iteration 87/1000 | Loss: 0.00000942
Iteration 88/1000 | Loss: 0.00000942
Iteration 89/1000 | Loss: 0.00000942
Iteration 90/1000 | Loss: 0.00000942
Iteration 91/1000 | Loss: 0.00000941
Iteration 92/1000 | Loss: 0.00000941
Iteration 93/1000 | Loss: 0.00000941
Iteration 94/1000 | Loss: 0.00000941
Iteration 95/1000 | Loss: 0.00000941
Iteration 96/1000 | Loss: 0.00000941
Iteration 97/1000 | Loss: 0.00000940
Iteration 98/1000 | Loss: 0.00000940
Iteration 99/1000 | Loss: 0.00000940
Iteration 100/1000 | Loss: 0.00000940
Iteration 101/1000 | Loss: 0.00000940
Iteration 102/1000 | Loss: 0.00000939
Iteration 103/1000 | Loss: 0.00000939
Iteration 104/1000 | Loss: 0.00000938
Iteration 105/1000 | Loss: 0.00000938
Iteration 106/1000 | Loss: 0.00000937
Iteration 107/1000 | Loss: 0.00000937
Iteration 108/1000 | Loss: 0.00000937
Iteration 109/1000 | Loss: 0.00000937
Iteration 110/1000 | Loss: 0.00000936
Iteration 111/1000 | Loss: 0.00000936
Iteration 112/1000 | Loss: 0.00000936
Iteration 113/1000 | Loss: 0.00000936
Iteration 114/1000 | Loss: 0.00000936
Iteration 115/1000 | Loss: 0.00000936
Iteration 116/1000 | Loss: 0.00000935
Iteration 117/1000 | Loss: 0.00000935
Iteration 118/1000 | Loss: 0.00000935
Iteration 119/1000 | Loss: 0.00000935
Iteration 120/1000 | Loss: 0.00000934
Iteration 121/1000 | Loss: 0.00000934
Iteration 122/1000 | Loss: 0.00000934
Iteration 123/1000 | Loss: 0.00000934
Iteration 124/1000 | Loss: 0.00000934
Iteration 125/1000 | Loss: 0.00000934
Iteration 126/1000 | Loss: 0.00000933
Iteration 127/1000 | Loss: 0.00000933
Iteration 128/1000 | Loss: 0.00000933
Iteration 129/1000 | Loss: 0.00000933
Iteration 130/1000 | Loss: 0.00000933
Iteration 131/1000 | Loss: 0.00000933
Iteration 132/1000 | Loss: 0.00000933
Iteration 133/1000 | Loss: 0.00000933
Iteration 134/1000 | Loss: 0.00000933
Iteration 135/1000 | Loss: 0.00000933
Iteration 136/1000 | Loss: 0.00000933
Iteration 137/1000 | Loss: 0.00000933
Iteration 138/1000 | Loss: 0.00000933
Iteration 139/1000 | Loss: 0.00000933
Iteration 140/1000 | Loss: 0.00000932
Iteration 141/1000 | Loss: 0.00000932
Iteration 142/1000 | Loss: 0.00000932
Iteration 143/1000 | Loss: 0.00000932
Iteration 144/1000 | Loss: 0.00000932
Iteration 145/1000 | Loss: 0.00000932
Iteration 146/1000 | Loss: 0.00000931
Iteration 147/1000 | Loss: 0.00000931
Iteration 148/1000 | Loss: 0.00000931
Iteration 149/1000 | Loss: 0.00000931
Iteration 150/1000 | Loss: 0.00000931
Iteration 151/1000 | Loss: 0.00000931
Iteration 152/1000 | Loss: 0.00000931
Iteration 153/1000 | Loss: 0.00000931
Iteration 154/1000 | Loss: 0.00000931
Iteration 155/1000 | Loss: 0.00000930
Iteration 156/1000 | Loss: 0.00000930
Iteration 157/1000 | Loss: 0.00000930
Iteration 158/1000 | Loss: 0.00000930
Iteration 159/1000 | Loss: 0.00000930
Iteration 160/1000 | Loss: 0.00000930
Iteration 161/1000 | Loss: 0.00000930
Iteration 162/1000 | Loss: 0.00000930
Iteration 163/1000 | Loss: 0.00000930
Iteration 164/1000 | Loss: 0.00000930
Iteration 165/1000 | Loss: 0.00000930
Iteration 166/1000 | Loss: 0.00000930
Iteration 167/1000 | Loss: 0.00000930
Iteration 168/1000 | Loss: 0.00000930
Iteration 169/1000 | Loss: 0.00000930
Iteration 170/1000 | Loss: 0.00000930
Iteration 171/1000 | Loss: 0.00000929
Iteration 172/1000 | Loss: 0.00000929
Iteration 173/1000 | Loss: 0.00000929
Iteration 174/1000 | Loss: 0.00000929
Iteration 175/1000 | Loss: 0.00000929
Iteration 176/1000 | Loss: 0.00000929
Iteration 177/1000 | Loss: 0.00000929
Iteration 178/1000 | Loss: 0.00000929
Iteration 179/1000 | Loss: 0.00000929
Iteration 180/1000 | Loss: 0.00000929
Iteration 181/1000 | Loss: 0.00000929
Iteration 182/1000 | Loss: 0.00000929
Iteration 183/1000 | Loss: 0.00000929
Iteration 184/1000 | Loss: 0.00000929
Iteration 185/1000 | Loss: 0.00000928
Iteration 186/1000 | Loss: 0.00000928
Iteration 187/1000 | Loss: 0.00000928
Iteration 188/1000 | Loss: 0.00000928
Iteration 189/1000 | Loss: 0.00000928
Iteration 190/1000 | Loss: 0.00000927
Iteration 191/1000 | Loss: 0.00000927
Iteration 192/1000 | Loss: 0.00000927
Iteration 193/1000 | Loss: 0.00000927
Iteration 194/1000 | Loss: 0.00000927
Iteration 195/1000 | Loss: 0.00000927
Iteration 196/1000 | Loss: 0.00000927
Iteration 197/1000 | Loss: 0.00000926
Iteration 198/1000 | Loss: 0.00000926
Iteration 199/1000 | Loss: 0.00000926
Iteration 200/1000 | Loss: 0.00000926
Iteration 201/1000 | Loss: 0.00000926
Iteration 202/1000 | Loss: 0.00000926
Iteration 203/1000 | Loss: 0.00000926
Iteration 204/1000 | Loss: 0.00000926
Iteration 205/1000 | Loss: 0.00000926
Iteration 206/1000 | Loss: 0.00000926
Iteration 207/1000 | Loss: 0.00000926
Iteration 208/1000 | Loss: 0.00000926
Iteration 209/1000 | Loss: 0.00000925
Iteration 210/1000 | Loss: 0.00000925
Iteration 211/1000 | Loss: 0.00000925
Iteration 212/1000 | Loss: 0.00000925
Iteration 213/1000 | Loss: 0.00000925
Iteration 214/1000 | Loss: 0.00000925
Iteration 215/1000 | Loss: 0.00000925
Iteration 216/1000 | Loss: 0.00000925
Iteration 217/1000 | Loss: 0.00000925
Iteration 218/1000 | Loss: 0.00000925
Iteration 219/1000 | Loss: 0.00000925
Iteration 220/1000 | Loss: 0.00000925
Iteration 221/1000 | Loss: 0.00000925
Iteration 222/1000 | Loss: 0.00000925
Iteration 223/1000 | Loss: 0.00000925
Iteration 224/1000 | Loss: 0.00000925
Iteration 225/1000 | Loss: 0.00000925
Iteration 226/1000 | Loss: 0.00000925
Iteration 227/1000 | Loss: 0.00000925
Iteration 228/1000 | Loss: 0.00000925
Iteration 229/1000 | Loss: 0.00000925
Iteration 230/1000 | Loss: 0.00000925
Iteration 231/1000 | Loss: 0.00000924
Iteration 232/1000 | Loss: 0.00000924
Iteration 233/1000 | Loss: 0.00000924
Iteration 234/1000 | Loss: 0.00000924
Iteration 235/1000 | Loss: 0.00000924
Iteration 236/1000 | Loss: 0.00000923
Iteration 237/1000 | Loss: 0.00000923
Iteration 238/1000 | Loss: 0.00000923
Iteration 239/1000 | Loss: 0.00000923
Iteration 240/1000 | Loss: 0.00000923
Iteration 241/1000 | Loss: 0.00000923
Iteration 242/1000 | Loss: 0.00000923
Iteration 243/1000 | Loss: 0.00000923
Iteration 244/1000 | Loss: 0.00000923
Iteration 245/1000 | Loss: 0.00000923
Iteration 246/1000 | Loss: 0.00000923
Iteration 247/1000 | Loss: 0.00000923
Iteration 248/1000 | Loss: 0.00000923
Iteration 249/1000 | Loss: 0.00000923
Iteration 250/1000 | Loss: 0.00000923
Iteration 251/1000 | Loss: 0.00000923
Iteration 252/1000 | Loss: 0.00000923
Iteration 253/1000 | Loss: 0.00000922
Iteration 254/1000 | Loss: 0.00000922
Iteration 255/1000 | Loss: 0.00000922
Iteration 256/1000 | Loss: 0.00000922
Iteration 257/1000 | Loss: 0.00000922
Iteration 258/1000 | Loss: 0.00000922
Iteration 259/1000 | Loss: 0.00000922
Iteration 260/1000 | Loss: 0.00000922
Iteration 261/1000 | Loss: 0.00000922
Iteration 262/1000 | Loss: 0.00000922
Iteration 263/1000 | Loss: 0.00000922
Iteration 264/1000 | Loss: 0.00000922
Iteration 265/1000 | Loss: 0.00000922
Iteration 266/1000 | Loss: 0.00000922
Iteration 267/1000 | Loss: 0.00000922
Iteration 268/1000 | Loss: 0.00000921
Iteration 269/1000 | Loss: 0.00000921
Iteration 270/1000 | Loss: 0.00000921
Iteration 271/1000 | Loss: 0.00000921
Iteration 272/1000 | Loss: 0.00000921
Iteration 273/1000 | Loss: 0.00000921
Iteration 274/1000 | Loss: 0.00000921
Iteration 275/1000 | Loss: 0.00000921
Iteration 276/1000 | Loss: 0.00000921
Iteration 277/1000 | Loss: 0.00000921
Iteration 278/1000 | Loss: 0.00000921
Iteration 279/1000 | Loss: 0.00000921
Iteration 280/1000 | Loss: 0.00000921
Iteration 281/1000 | Loss: 0.00000921
Iteration 282/1000 | Loss: 0.00000921
Iteration 283/1000 | Loss: 0.00000921
Iteration 284/1000 | Loss: 0.00000921
Iteration 285/1000 | Loss: 0.00000921
Iteration 286/1000 | Loss: 0.00000921
Iteration 287/1000 | Loss: 0.00000921
Iteration 288/1000 | Loss: 0.00000921
Iteration 289/1000 | Loss: 0.00000921
Iteration 290/1000 | Loss: 0.00000921
Iteration 291/1000 | Loss: 0.00000921
Iteration 292/1000 | Loss: 0.00000921
Iteration 293/1000 | Loss: 0.00000921
Iteration 294/1000 | Loss: 0.00000921
Iteration 295/1000 | Loss: 0.00000921
Iteration 296/1000 | Loss: 0.00000921
Iteration 297/1000 | Loss: 0.00000921
Iteration 298/1000 | Loss: 0.00000921
Iteration 299/1000 | Loss: 0.00000921
Iteration 300/1000 | Loss: 0.00000921
Iteration 301/1000 | Loss: 0.00000921
Iteration 302/1000 | Loss: 0.00000921
Iteration 303/1000 | Loss: 0.00000921
Iteration 304/1000 | Loss: 0.00000921
Iteration 305/1000 | Loss: 0.00000921
Iteration 306/1000 | Loss: 0.00000921
Iteration 307/1000 | Loss: 0.00000921
Iteration 308/1000 | Loss: 0.00000921
Iteration 309/1000 | Loss: 0.00000921
Iteration 310/1000 | Loss: 0.00000921
Iteration 311/1000 | Loss: 0.00000921
Iteration 312/1000 | Loss: 0.00000921
Iteration 313/1000 | Loss: 0.00000921
Iteration 314/1000 | Loss: 0.00000921
Iteration 315/1000 | Loss: 0.00000921
Iteration 316/1000 | Loss: 0.00000921
Iteration 317/1000 | Loss: 0.00000921
Iteration 318/1000 | Loss: 0.00000921
Iteration 319/1000 | Loss: 0.00000921
Iteration 320/1000 | Loss: 0.00000921
Iteration 321/1000 | Loss: 0.00000921
Iteration 322/1000 | Loss: 0.00000921
Iteration 323/1000 | Loss: 0.00000921
Iteration 324/1000 | Loss: 0.00000921
Iteration 325/1000 | Loss: 0.00000921
Iteration 326/1000 | Loss: 0.00000921
Iteration 327/1000 | Loss: 0.00000921
Iteration 328/1000 | Loss: 0.00000921
Iteration 329/1000 | Loss: 0.00000921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 329. Stopping optimization.
Last 5 losses: [9.213859812007286e-06, 9.213859812007286e-06, 9.213859812007286e-06, 9.213859812007286e-06, 9.213859812007286e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.213859812007286e-06

Optimization complete. Final v2v error: 2.656031847000122 mm

Highest mean error: 2.8384952545166016 mm for frame 84

Lowest mean error: 2.531345844268799 mm for frame 38

Saving results

Total time: 44.762250661849976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421714
Iteration 2/25 | Loss: 0.00132363
Iteration 3/25 | Loss: 0.00125438
Iteration 4/25 | Loss: 0.00124538
Iteration 5/25 | Loss: 0.00124387
Iteration 6/25 | Loss: 0.00124337
Iteration 7/25 | Loss: 0.00124337
Iteration 8/25 | Loss: 0.00124337
Iteration 9/25 | Loss: 0.00124337
Iteration 10/25 | Loss: 0.00124337
Iteration 11/25 | Loss: 0.00124337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001243369304575026, 0.001243369304575026, 0.001243369304575026, 0.001243369304575026, 0.001243369304575026]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001243369304575026

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28807020
Iteration 2/25 | Loss: 0.00174287
Iteration 3/25 | Loss: 0.00174287
Iteration 4/25 | Loss: 0.00174287
Iteration 5/25 | Loss: 0.00174287
Iteration 6/25 | Loss: 0.00174286
Iteration 7/25 | Loss: 0.00174286
Iteration 8/25 | Loss: 0.00174286
Iteration 9/25 | Loss: 0.00174286
Iteration 10/25 | Loss: 0.00174286
Iteration 11/25 | Loss: 0.00174286
Iteration 12/25 | Loss: 0.00174286
Iteration 13/25 | Loss: 0.00174286
Iteration 14/25 | Loss: 0.00174286
Iteration 15/25 | Loss: 0.00174286
Iteration 16/25 | Loss: 0.00174286
Iteration 17/25 | Loss: 0.00174286
Iteration 18/25 | Loss: 0.00174286
Iteration 19/25 | Loss: 0.00174286
Iteration 20/25 | Loss: 0.00174286
Iteration 21/25 | Loss: 0.00174286
Iteration 22/25 | Loss: 0.00174286
Iteration 23/25 | Loss: 0.00174286
Iteration 24/25 | Loss: 0.00174286
Iteration 25/25 | Loss: 0.00174286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174286
Iteration 2/1000 | Loss: 0.00002629
Iteration 3/1000 | Loss: 0.00002022
Iteration 4/1000 | Loss: 0.00001884
Iteration 5/1000 | Loss: 0.00001819
Iteration 6/1000 | Loss: 0.00001757
Iteration 7/1000 | Loss: 0.00001697
Iteration 8/1000 | Loss: 0.00001675
Iteration 9/1000 | Loss: 0.00001647
Iteration 10/1000 | Loss: 0.00001622
Iteration 11/1000 | Loss: 0.00001606
Iteration 12/1000 | Loss: 0.00001595
Iteration 13/1000 | Loss: 0.00001591
Iteration 14/1000 | Loss: 0.00001591
Iteration 15/1000 | Loss: 0.00001590
Iteration 16/1000 | Loss: 0.00001590
Iteration 17/1000 | Loss: 0.00001589
Iteration 18/1000 | Loss: 0.00001589
Iteration 19/1000 | Loss: 0.00001588
Iteration 20/1000 | Loss: 0.00001588
Iteration 21/1000 | Loss: 0.00001587
Iteration 22/1000 | Loss: 0.00001586
Iteration 23/1000 | Loss: 0.00001585
Iteration 24/1000 | Loss: 0.00001585
Iteration 25/1000 | Loss: 0.00001577
Iteration 26/1000 | Loss: 0.00001577
Iteration 27/1000 | Loss: 0.00001575
Iteration 28/1000 | Loss: 0.00001574
Iteration 29/1000 | Loss: 0.00001574
Iteration 30/1000 | Loss: 0.00001572
Iteration 31/1000 | Loss: 0.00001571
Iteration 32/1000 | Loss: 0.00001567
Iteration 33/1000 | Loss: 0.00001565
Iteration 34/1000 | Loss: 0.00001564
Iteration 35/1000 | Loss: 0.00001564
Iteration 36/1000 | Loss: 0.00001563
Iteration 37/1000 | Loss: 0.00001563
Iteration 38/1000 | Loss: 0.00001563
Iteration 39/1000 | Loss: 0.00001563
Iteration 40/1000 | Loss: 0.00001563
Iteration 41/1000 | Loss: 0.00001563
Iteration 42/1000 | Loss: 0.00001563
Iteration 43/1000 | Loss: 0.00001563
Iteration 44/1000 | Loss: 0.00001563
Iteration 45/1000 | Loss: 0.00001563
Iteration 46/1000 | Loss: 0.00001562
Iteration 47/1000 | Loss: 0.00001562
Iteration 48/1000 | Loss: 0.00001562
Iteration 49/1000 | Loss: 0.00001561
Iteration 50/1000 | Loss: 0.00001561
Iteration 51/1000 | Loss: 0.00001561
Iteration 52/1000 | Loss: 0.00001560
Iteration 53/1000 | Loss: 0.00001560
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001560
Iteration 56/1000 | Loss: 0.00001560
Iteration 57/1000 | Loss: 0.00001560
Iteration 58/1000 | Loss: 0.00001560
Iteration 59/1000 | Loss: 0.00001560
Iteration 60/1000 | Loss: 0.00001560
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001559
Iteration 64/1000 | Loss: 0.00001559
Iteration 65/1000 | Loss: 0.00001558
Iteration 66/1000 | Loss: 0.00001558
Iteration 67/1000 | Loss: 0.00001558
Iteration 68/1000 | Loss: 0.00001558
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001558
Iteration 71/1000 | Loss: 0.00001558
Iteration 72/1000 | Loss: 0.00001558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [1.5579153114231303e-05, 1.5579153114231303e-05, 1.5579153114231303e-05, 1.5579153114231303e-05, 1.5579153114231303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5579153114231303e-05

Optimization complete. Final v2v error: 3.3299717903137207 mm

Highest mean error: 3.821481704711914 mm for frame 22

Lowest mean error: 3.0742578506469727 mm for frame 39

Saving results

Total time: 31.305089473724365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891490
Iteration 2/25 | Loss: 0.00174084
Iteration 3/25 | Loss: 0.00140624
Iteration 4/25 | Loss: 0.00138386
Iteration 5/25 | Loss: 0.00138226
Iteration 6/25 | Loss: 0.00136028
Iteration 7/25 | Loss: 0.00136969
Iteration 8/25 | Loss: 0.00135827
Iteration 9/25 | Loss: 0.00135066
Iteration 10/25 | Loss: 0.00134418
Iteration 11/25 | Loss: 0.00134206
Iteration 12/25 | Loss: 0.00134228
Iteration 13/25 | Loss: 0.00133309
Iteration 14/25 | Loss: 0.00133490
Iteration 15/25 | Loss: 0.00133364
Iteration 16/25 | Loss: 0.00133059
Iteration 17/25 | Loss: 0.00132893
Iteration 18/25 | Loss: 0.00133119
Iteration 19/25 | Loss: 0.00132914
Iteration 20/25 | Loss: 0.00133094
Iteration 21/25 | Loss: 0.00132898
Iteration 22/25 | Loss: 0.00132745
Iteration 23/25 | Loss: 0.00132705
Iteration 24/25 | Loss: 0.00132727
Iteration 25/25 | Loss: 0.00132622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58207035
Iteration 2/25 | Loss: 0.00122803
Iteration 3/25 | Loss: 0.00122803
Iteration 4/25 | Loss: 0.00122803
Iteration 5/25 | Loss: 0.00122803
Iteration 6/25 | Loss: 0.00122803
Iteration 7/25 | Loss: 0.00122803
Iteration 8/25 | Loss: 0.00122803
Iteration 9/25 | Loss: 0.00122803
Iteration 10/25 | Loss: 0.00122803
Iteration 11/25 | Loss: 0.00122803
Iteration 12/25 | Loss: 0.00122803
Iteration 13/25 | Loss: 0.00122803
Iteration 14/25 | Loss: 0.00122803
Iteration 15/25 | Loss: 0.00122803
Iteration 16/25 | Loss: 0.00122803
Iteration 17/25 | Loss: 0.00122803
Iteration 18/25 | Loss: 0.00122803
Iteration 19/25 | Loss: 0.00122803
Iteration 20/25 | Loss: 0.00122803
Iteration 21/25 | Loss: 0.00122803
Iteration 22/25 | Loss: 0.00122803
Iteration 23/25 | Loss: 0.00122803
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012280292576178908, 0.0012280292576178908, 0.0012280292576178908, 0.0012280292576178908, 0.0012280292576178908]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012280292576178908

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122803
Iteration 2/1000 | Loss: 0.00008789
Iteration 3/1000 | Loss: 0.00005702
Iteration 4/1000 | Loss: 0.00004994
Iteration 5/1000 | Loss: 0.00004660
Iteration 6/1000 | Loss: 0.00004418
Iteration 7/1000 | Loss: 0.00020057
Iteration 8/1000 | Loss: 0.00017840
Iteration 9/1000 | Loss: 0.00015285
Iteration 10/1000 | Loss: 0.00004016
Iteration 11/1000 | Loss: 0.00003861
Iteration 12/1000 | Loss: 0.00003701
Iteration 13/1000 | Loss: 0.00075680
Iteration 14/1000 | Loss: 0.00011539
Iteration 15/1000 | Loss: 0.00074847
Iteration 16/1000 | Loss: 0.00215274
Iteration 17/1000 | Loss: 0.00057688
Iteration 18/1000 | Loss: 0.00008396
Iteration 19/1000 | Loss: 0.00017303
Iteration 20/1000 | Loss: 0.00011364
Iteration 21/1000 | Loss: 0.00006460
Iteration 22/1000 | Loss: 0.00064224
Iteration 23/1000 | Loss: 0.00041281
Iteration 24/1000 | Loss: 0.00009933
Iteration 25/1000 | Loss: 0.00130342
Iteration 26/1000 | Loss: 0.00148773
Iteration 27/1000 | Loss: 0.00036025
Iteration 28/1000 | Loss: 0.00063732
Iteration 29/1000 | Loss: 0.00030127
Iteration 30/1000 | Loss: 0.00004252
Iteration 31/1000 | Loss: 0.00081935
Iteration 32/1000 | Loss: 0.00037631
Iteration 33/1000 | Loss: 0.00003529
Iteration 34/1000 | Loss: 0.00003353
Iteration 35/1000 | Loss: 0.00019974
Iteration 36/1000 | Loss: 0.00020086
Iteration 37/1000 | Loss: 0.00003254
Iteration 38/1000 | Loss: 0.00003135
Iteration 39/1000 | Loss: 0.00003080
Iteration 40/1000 | Loss: 0.00003013
Iteration 41/1000 | Loss: 0.00014610
Iteration 42/1000 | Loss: 0.00002953
Iteration 43/1000 | Loss: 0.00002886
Iteration 44/1000 | Loss: 0.00109045
Iteration 45/1000 | Loss: 0.00053632
Iteration 46/1000 | Loss: 0.00025734
Iteration 47/1000 | Loss: 0.00045647
Iteration 48/1000 | Loss: 0.00015189
Iteration 49/1000 | Loss: 0.00003647
Iteration 50/1000 | Loss: 0.00035605
Iteration 51/1000 | Loss: 0.00026126
Iteration 52/1000 | Loss: 0.00004654
Iteration 53/1000 | Loss: 0.00003088
Iteration 54/1000 | Loss: 0.00015594
Iteration 55/1000 | Loss: 0.00002871
Iteration 56/1000 | Loss: 0.00002766
Iteration 57/1000 | Loss: 0.00002678
Iteration 58/1000 | Loss: 0.00002599
Iteration 59/1000 | Loss: 0.00045778
Iteration 60/1000 | Loss: 0.00008983
Iteration 61/1000 | Loss: 0.00008039
Iteration 62/1000 | Loss: 0.00017777
Iteration 63/1000 | Loss: 0.00033499
Iteration 64/1000 | Loss: 0.00032679
Iteration 65/1000 | Loss: 0.00018099
Iteration 66/1000 | Loss: 0.00050408
Iteration 67/1000 | Loss: 0.00013757
Iteration 68/1000 | Loss: 0.00005597
Iteration 69/1000 | Loss: 0.00002619
Iteration 70/1000 | Loss: 0.00002444
Iteration 71/1000 | Loss: 0.00017180
Iteration 72/1000 | Loss: 0.00015899
Iteration 73/1000 | Loss: 0.00002417
Iteration 74/1000 | Loss: 0.00034183
Iteration 75/1000 | Loss: 0.00006230
Iteration 76/1000 | Loss: 0.00008975
Iteration 77/1000 | Loss: 0.00008427
Iteration 78/1000 | Loss: 0.00010072
Iteration 79/1000 | Loss: 0.00006645
Iteration 80/1000 | Loss: 0.00007220
Iteration 81/1000 | Loss: 0.00006300
Iteration 82/1000 | Loss: 0.00003339
Iteration 83/1000 | Loss: 0.00002824
Iteration 84/1000 | Loss: 0.00009411
Iteration 85/1000 | Loss: 0.00005658
Iteration 86/1000 | Loss: 0.00010608
Iteration 87/1000 | Loss: 0.00017542
Iteration 88/1000 | Loss: 0.00014919
Iteration 89/1000 | Loss: 0.00008530
Iteration 90/1000 | Loss: 0.00008000
Iteration 91/1000 | Loss: 0.00011009
Iteration 92/1000 | Loss: 0.00007074
Iteration 93/1000 | Loss: 0.00005932
Iteration 94/1000 | Loss: 0.00005173
Iteration 95/1000 | Loss: 0.00005407
Iteration 96/1000 | Loss: 0.00005076
Iteration 97/1000 | Loss: 0.00004515
Iteration 98/1000 | Loss: 0.00004211
Iteration 99/1000 | Loss: 0.00004375
Iteration 100/1000 | Loss: 0.00013631
Iteration 101/1000 | Loss: 0.00008509
Iteration 102/1000 | Loss: 0.00005945
Iteration 103/1000 | Loss: 0.00014889
Iteration 104/1000 | Loss: 0.00020889
Iteration 105/1000 | Loss: 0.00003864
Iteration 106/1000 | Loss: 0.00004052
Iteration 107/1000 | Loss: 0.00016152
Iteration 108/1000 | Loss: 0.00003172
Iteration 109/1000 | Loss: 0.00016728
Iteration 110/1000 | Loss: 0.00016281
Iteration 111/1000 | Loss: 0.00002670
Iteration 112/1000 | Loss: 0.00002449
Iteration 113/1000 | Loss: 0.00002361
Iteration 114/1000 | Loss: 0.00002301
Iteration 115/1000 | Loss: 0.00002220
Iteration 116/1000 | Loss: 0.00002190
Iteration 117/1000 | Loss: 0.00002170
Iteration 118/1000 | Loss: 0.00002162
Iteration 119/1000 | Loss: 0.00002162
Iteration 120/1000 | Loss: 0.00002161
Iteration 121/1000 | Loss: 0.00002161
Iteration 122/1000 | Loss: 0.00002160
Iteration 123/1000 | Loss: 0.00002160
Iteration 124/1000 | Loss: 0.00002159
Iteration 125/1000 | Loss: 0.00002159
Iteration 126/1000 | Loss: 0.00002159
Iteration 127/1000 | Loss: 0.00002158
Iteration 128/1000 | Loss: 0.00002158
Iteration 129/1000 | Loss: 0.00002157
Iteration 130/1000 | Loss: 0.00002157
Iteration 131/1000 | Loss: 0.00002156
Iteration 132/1000 | Loss: 0.00002156
Iteration 133/1000 | Loss: 0.00002156
Iteration 134/1000 | Loss: 0.00002155
Iteration 135/1000 | Loss: 0.00002155
Iteration 136/1000 | Loss: 0.00002155
Iteration 137/1000 | Loss: 0.00002154
Iteration 138/1000 | Loss: 0.00002153
Iteration 139/1000 | Loss: 0.00002153
Iteration 140/1000 | Loss: 0.00002152
Iteration 141/1000 | Loss: 0.00002152
Iteration 142/1000 | Loss: 0.00002152
Iteration 143/1000 | Loss: 0.00002151
Iteration 144/1000 | Loss: 0.00002151
Iteration 145/1000 | Loss: 0.00002151
Iteration 146/1000 | Loss: 0.00002151
Iteration 147/1000 | Loss: 0.00002150
Iteration 148/1000 | Loss: 0.00002148
Iteration 149/1000 | Loss: 0.00002148
Iteration 150/1000 | Loss: 0.00002147
Iteration 151/1000 | Loss: 0.00002147
Iteration 152/1000 | Loss: 0.00002147
Iteration 153/1000 | Loss: 0.00002147
Iteration 154/1000 | Loss: 0.00002145
Iteration 155/1000 | Loss: 0.00002145
Iteration 156/1000 | Loss: 0.00002145
Iteration 157/1000 | Loss: 0.00002145
Iteration 158/1000 | Loss: 0.00002145
Iteration 159/1000 | Loss: 0.00002145
Iteration 160/1000 | Loss: 0.00002145
Iteration 161/1000 | Loss: 0.00002145
Iteration 162/1000 | Loss: 0.00002145
Iteration 163/1000 | Loss: 0.00002144
Iteration 164/1000 | Loss: 0.00002144
Iteration 165/1000 | Loss: 0.00002144
Iteration 166/1000 | Loss: 0.00002144
Iteration 167/1000 | Loss: 0.00002144
Iteration 168/1000 | Loss: 0.00002144
Iteration 169/1000 | Loss: 0.00002144
Iteration 170/1000 | Loss: 0.00002143
Iteration 171/1000 | Loss: 0.00002143
Iteration 172/1000 | Loss: 0.00002143
Iteration 173/1000 | Loss: 0.00002142
Iteration 174/1000 | Loss: 0.00002142
Iteration 175/1000 | Loss: 0.00002142
Iteration 176/1000 | Loss: 0.00002142
Iteration 177/1000 | Loss: 0.00002142
Iteration 178/1000 | Loss: 0.00002142
Iteration 179/1000 | Loss: 0.00002142
Iteration 180/1000 | Loss: 0.00002142
Iteration 181/1000 | Loss: 0.00002141
Iteration 182/1000 | Loss: 0.00002141
Iteration 183/1000 | Loss: 0.00002141
Iteration 184/1000 | Loss: 0.00002141
Iteration 185/1000 | Loss: 0.00002141
Iteration 186/1000 | Loss: 0.00002141
Iteration 187/1000 | Loss: 0.00002140
Iteration 188/1000 | Loss: 0.00002140
Iteration 189/1000 | Loss: 0.00002139
Iteration 190/1000 | Loss: 0.00002139
Iteration 191/1000 | Loss: 0.00002139
Iteration 192/1000 | Loss: 0.00002137
Iteration 193/1000 | Loss: 0.00002137
Iteration 194/1000 | Loss: 0.00002137
Iteration 195/1000 | Loss: 0.00002137
Iteration 196/1000 | Loss: 0.00002137
Iteration 197/1000 | Loss: 0.00002137
Iteration 198/1000 | Loss: 0.00002136
Iteration 199/1000 | Loss: 0.00002136
Iteration 200/1000 | Loss: 0.00002136
Iteration 201/1000 | Loss: 0.00002136
Iteration 202/1000 | Loss: 0.00002136
Iteration 203/1000 | Loss: 0.00002135
Iteration 204/1000 | Loss: 0.00002135
Iteration 205/1000 | Loss: 0.00002135
Iteration 206/1000 | Loss: 0.00002135
Iteration 207/1000 | Loss: 0.00002135
Iteration 208/1000 | Loss: 0.00002135
Iteration 209/1000 | Loss: 0.00002135
Iteration 210/1000 | Loss: 0.00002135
Iteration 211/1000 | Loss: 0.00002135
Iteration 212/1000 | Loss: 0.00002135
Iteration 213/1000 | Loss: 0.00002135
Iteration 214/1000 | Loss: 0.00002135
Iteration 215/1000 | Loss: 0.00002135
Iteration 216/1000 | Loss: 0.00002135
Iteration 217/1000 | Loss: 0.00002134
Iteration 218/1000 | Loss: 0.00002134
Iteration 219/1000 | Loss: 0.00002134
Iteration 220/1000 | Loss: 0.00002134
Iteration 221/1000 | Loss: 0.00002134
Iteration 222/1000 | Loss: 0.00002134
Iteration 223/1000 | Loss: 0.00002134
Iteration 224/1000 | Loss: 0.00002134
Iteration 225/1000 | Loss: 0.00002134
Iteration 226/1000 | Loss: 0.00002134
Iteration 227/1000 | Loss: 0.00002134
Iteration 228/1000 | Loss: 0.00002134
Iteration 229/1000 | Loss: 0.00002134
Iteration 230/1000 | Loss: 0.00002134
Iteration 231/1000 | Loss: 0.00002134
Iteration 232/1000 | Loss: 0.00002134
Iteration 233/1000 | Loss: 0.00002134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [2.1340878447517753e-05, 2.1340878447517753e-05, 2.1340878447517753e-05, 2.1340878447517753e-05, 2.1340878447517753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1340878447517753e-05

Optimization complete. Final v2v error: 3.656688928604126 mm

Highest mean error: 12.687708854675293 mm for frame 39

Lowest mean error: 2.947396993637085 mm for frame 140

Saving results

Total time: 250.54261374473572
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00706838
Iteration 2/25 | Loss: 0.00163982
Iteration 3/25 | Loss: 0.00138292
Iteration 4/25 | Loss: 0.00134825
Iteration 5/25 | Loss: 0.00133711
Iteration 6/25 | Loss: 0.00133422
Iteration 7/25 | Loss: 0.00133284
Iteration 8/25 | Loss: 0.00133207
Iteration 9/25 | Loss: 0.00133150
Iteration 10/25 | Loss: 0.00133116
Iteration 11/25 | Loss: 0.00133099
Iteration 12/25 | Loss: 0.00133099
Iteration 13/25 | Loss: 0.00133099
Iteration 14/25 | Loss: 0.00133099
Iteration 15/25 | Loss: 0.00133099
Iteration 16/25 | Loss: 0.00133099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013309925561770797, 0.0013309925561770797, 0.0013309925561770797, 0.0013309925561770797, 0.0013309925561770797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013309925561770797

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19767666
Iteration 2/25 | Loss: 0.00172167
Iteration 3/25 | Loss: 0.00172164
Iteration 4/25 | Loss: 0.00172164
Iteration 5/25 | Loss: 0.00172164
Iteration 6/25 | Loss: 0.00172164
Iteration 7/25 | Loss: 0.00172163
Iteration 8/25 | Loss: 0.00172163
Iteration 9/25 | Loss: 0.00172163
Iteration 10/25 | Loss: 0.00172163
Iteration 11/25 | Loss: 0.00172163
Iteration 12/25 | Loss: 0.00172163
Iteration 13/25 | Loss: 0.00172163
Iteration 14/25 | Loss: 0.00172163
Iteration 15/25 | Loss: 0.00172163
Iteration 16/25 | Loss: 0.00172163
Iteration 17/25 | Loss: 0.00172163
Iteration 18/25 | Loss: 0.00172163
Iteration 19/25 | Loss: 0.00172163
Iteration 20/25 | Loss: 0.00172163
Iteration 21/25 | Loss: 0.00172163
Iteration 22/25 | Loss: 0.00172163
Iteration 23/25 | Loss: 0.00172163
Iteration 24/25 | Loss: 0.00172163
Iteration 25/25 | Loss: 0.00172163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172163
Iteration 2/1000 | Loss: 0.00008077
Iteration 3/1000 | Loss: 0.00005867
Iteration 4/1000 | Loss: 0.00005268
Iteration 5/1000 | Loss: 0.00005017
Iteration 6/1000 | Loss: 0.00004857
Iteration 7/1000 | Loss: 0.00004743
Iteration 8/1000 | Loss: 0.00004669
Iteration 9/1000 | Loss: 0.00004608
Iteration 10/1000 | Loss: 0.00004550
Iteration 11/1000 | Loss: 0.00004518
Iteration 12/1000 | Loss: 0.00004479
Iteration 13/1000 | Loss: 0.00004436
Iteration 14/1000 | Loss: 0.00004408
Iteration 15/1000 | Loss: 0.00004382
Iteration 16/1000 | Loss: 0.00004364
Iteration 17/1000 | Loss: 0.00004347
Iteration 18/1000 | Loss: 0.00004340
Iteration 19/1000 | Loss: 0.00004340
Iteration 20/1000 | Loss: 0.00004339
Iteration 21/1000 | Loss: 0.00004335
Iteration 22/1000 | Loss: 0.00004334
Iteration 23/1000 | Loss: 0.00004331
Iteration 24/1000 | Loss: 0.00004328
Iteration 25/1000 | Loss: 0.00004320
Iteration 26/1000 | Loss: 0.00004319
Iteration 27/1000 | Loss: 0.00004313
Iteration 28/1000 | Loss: 0.00004311
Iteration 29/1000 | Loss: 0.00004310
Iteration 30/1000 | Loss: 0.00004309
Iteration 31/1000 | Loss: 0.00004305
Iteration 32/1000 | Loss: 0.00004304
Iteration 33/1000 | Loss: 0.00004302
Iteration 34/1000 | Loss: 0.00004301
Iteration 35/1000 | Loss: 0.00004299
Iteration 36/1000 | Loss: 0.00004299
Iteration 37/1000 | Loss: 0.00004299
Iteration 38/1000 | Loss: 0.00004299
Iteration 39/1000 | Loss: 0.00004298
Iteration 40/1000 | Loss: 0.00004298
Iteration 41/1000 | Loss: 0.00004296
Iteration 42/1000 | Loss: 0.00004296
Iteration 43/1000 | Loss: 0.00004295
Iteration 44/1000 | Loss: 0.00004295
Iteration 45/1000 | Loss: 0.00004294
Iteration 46/1000 | Loss: 0.00004294
Iteration 47/1000 | Loss: 0.00004293
Iteration 48/1000 | Loss: 0.00004293
Iteration 49/1000 | Loss: 0.00004293
Iteration 50/1000 | Loss: 0.00004293
Iteration 51/1000 | Loss: 0.00004293
Iteration 52/1000 | Loss: 0.00004292
Iteration 53/1000 | Loss: 0.00004292
Iteration 54/1000 | Loss: 0.00004291
Iteration 55/1000 | Loss: 0.00004291
Iteration 56/1000 | Loss: 0.00004291
Iteration 57/1000 | Loss: 0.00004291
Iteration 58/1000 | Loss: 0.00004290
Iteration 59/1000 | Loss: 0.00004290
Iteration 60/1000 | Loss: 0.00004289
Iteration 61/1000 | Loss: 0.00004289
Iteration 62/1000 | Loss: 0.00004289
Iteration 63/1000 | Loss: 0.00004289
Iteration 64/1000 | Loss: 0.00004288
Iteration 65/1000 | Loss: 0.00004288
Iteration 66/1000 | Loss: 0.00004288
Iteration 67/1000 | Loss: 0.00004288
Iteration 68/1000 | Loss: 0.00004288
Iteration 69/1000 | Loss: 0.00004288
Iteration 70/1000 | Loss: 0.00004288
Iteration 71/1000 | Loss: 0.00004288
Iteration 72/1000 | Loss: 0.00004287
Iteration 73/1000 | Loss: 0.00004287
Iteration 74/1000 | Loss: 0.00004287
Iteration 75/1000 | Loss: 0.00004287
Iteration 76/1000 | Loss: 0.00004287
Iteration 77/1000 | Loss: 0.00004286
Iteration 78/1000 | Loss: 0.00004286
Iteration 79/1000 | Loss: 0.00004286
Iteration 80/1000 | Loss: 0.00004286
Iteration 81/1000 | Loss: 0.00004286
Iteration 82/1000 | Loss: 0.00004286
Iteration 83/1000 | Loss: 0.00004286
Iteration 84/1000 | Loss: 0.00004286
Iteration 85/1000 | Loss: 0.00004286
Iteration 86/1000 | Loss: 0.00004285
Iteration 87/1000 | Loss: 0.00004285
Iteration 88/1000 | Loss: 0.00004285
Iteration 89/1000 | Loss: 0.00004285
Iteration 90/1000 | Loss: 0.00004285
Iteration 91/1000 | Loss: 0.00004285
Iteration 92/1000 | Loss: 0.00004285
Iteration 93/1000 | Loss: 0.00004284
Iteration 94/1000 | Loss: 0.00004284
Iteration 95/1000 | Loss: 0.00004284
Iteration 96/1000 | Loss: 0.00004283
Iteration 97/1000 | Loss: 0.00004283
Iteration 98/1000 | Loss: 0.00004283
Iteration 99/1000 | Loss: 0.00004283
Iteration 100/1000 | Loss: 0.00004283
Iteration 101/1000 | Loss: 0.00004283
Iteration 102/1000 | Loss: 0.00004283
Iteration 103/1000 | Loss: 0.00004283
Iteration 104/1000 | Loss: 0.00004283
Iteration 105/1000 | Loss: 0.00004283
Iteration 106/1000 | Loss: 0.00004283
Iteration 107/1000 | Loss: 0.00004283
Iteration 108/1000 | Loss: 0.00004283
Iteration 109/1000 | Loss: 0.00004283
Iteration 110/1000 | Loss: 0.00004283
Iteration 111/1000 | Loss: 0.00004283
Iteration 112/1000 | Loss: 0.00004283
Iteration 113/1000 | Loss: 0.00004283
Iteration 114/1000 | Loss: 0.00004283
Iteration 115/1000 | Loss: 0.00004283
Iteration 116/1000 | Loss: 0.00004283
Iteration 117/1000 | Loss: 0.00004283
Iteration 118/1000 | Loss: 0.00004283
Iteration 119/1000 | Loss: 0.00004283
Iteration 120/1000 | Loss: 0.00004283
Iteration 121/1000 | Loss: 0.00004283
Iteration 122/1000 | Loss: 0.00004283
Iteration 123/1000 | Loss: 0.00004283
Iteration 124/1000 | Loss: 0.00004283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [4.2825871787499636e-05, 4.2825871787499636e-05, 4.2825871787499636e-05, 4.2825871787499636e-05, 4.2825871787499636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.2825871787499636e-05

Optimization complete. Final v2v error: 4.023189067840576 mm

Highest mean error: 12.801590919494629 mm for frame 20

Lowest mean error: 2.835286855697632 mm for frame 237

Saving results

Total time: 60.75273323059082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044887
Iteration 2/25 | Loss: 0.00141343
Iteration 3/25 | Loss: 0.00123951
Iteration 4/25 | Loss: 0.00121764
Iteration 5/25 | Loss: 0.00121314
Iteration 6/25 | Loss: 0.00121314
Iteration 7/25 | Loss: 0.00121314
Iteration 8/25 | Loss: 0.00121314
Iteration 9/25 | Loss: 0.00121314
Iteration 10/25 | Loss: 0.00121314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001213143696077168, 0.001213143696077168, 0.001213143696077168, 0.001213143696077168, 0.001213143696077168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001213143696077168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04730868
Iteration 2/25 | Loss: 0.00151724
Iteration 3/25 | Loss: 0.00151724
Iteration 4/25 | Loss: 0.00151724
Iteration 5/25 | Loss: 0.00151724
Iteration 6/25 | Loss: 0.00151724
Iteration 7/25 | Loss: 0.00151724
Iteration 8/25 | Loss: 0.00151723
Iteration 9/25 | Loss: 0.00151723
Iteration 10/25 | Loss: 0.00151723
Iteration 11/25 | Loss: 0.00151723
Iteration 12/25 | Loss: 0.00151723
Iteration 13/25 | Loss: 0.00151723
Iteration 14/25 | Loss: 0.00151723
Iteration 15/25 | Loss: 0.00151723
Iteration 16/25 | Loss: 0.00151723
Iteration 17/25 | Loss: 0.00151723
Iteration 18/25 | Loss: 0.00151723
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001517234486527741, 0.001517234486527741, 0.001517234486527741, 0.001517234486527741, 0.001517234486527741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001517234486527741

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151723
Iteration 2/1000 | Loss: 0.00002504
Iteration 3/1000 | Loss: 0.00001792
Iteration 4/1000 | Loss: 0.00001577
Iteration 5/1000 | Loss: 0.00001439
Iteration 6/1000 | Loss: 0.00001349
Iteration 7/1000 | Loss: 0.00001284
Iteration 8/1000 | Loss: 0.00001248
Iteration 9/1000 | Loss: 0.00001204
Iteration 10/1000 | Loss: 0.00001172
Iteration 11/1000 | Loss: 0.00001150
Iteration 12/1000 | Loss: 0.00001133
Iteration 13/1000 | Loss: 0.00001116
Iteration 14/1000 | Loss: 0.00001112
Iteration 15/1000 | Loss: 0.00001110
Iteration 16/1000 | Loss: 0.00001110
Iteration 17/1000 | Loss: 0.00001109
Iteration 18/1000 | Loss: 0.00001109
Iteration 19/1000 | Loss: 0.00001107
Iteration 20/1000 | Loss: 0.00001103
Iteration 21/1000 | Loss: 0.00001102
Iteration 22/1000 | Loss: 0.00001102
Iteration 23/1000 | Loss: 0.00001102
Iteration 24/1000 | Loss: 0.00001101
Iteration 25/1000 | Loss: 0.00001099
Iteration 26/1000 | Loss: 0.00001098
Iteration 27/1000 | Loss: 0.00001098
Iteration 28/1000 | Loss: 0.00001097
Iteration 29/1000 | Loss: 0.00001096
Iteration 30/1000 | Loss: 0.00001096
Iteration 31/1000 | Loss: 0.00001096
Iteration 32/1000 | Loss: 0.00001095
Iteration 33/1000 | Loss: 0.00001095
Iteration 34/1000 | Loss: 0.00001094
Iteration 35/1000 | Loss: 0.00001093
Iteration 36/1000 | Loss: 0.00001092
Iteration 37/1000 | Loss: 0.00001092
Iteration 38/1000 | Loss: 0.00001091
Iteration 39/1000 | Loss: 0.00001090
Iteration 40/1000 | Loss: 0.00001089
Iteration 41/1000 | Loss: 0.00001089
Iteration 42/1000 | Loss: 0.00001089
Iteration 43/1000 | Loss: 0.00001089
Iteration 44/1000 | Loss: 0.00001089
Iteration 45/1000 | Loss: 0.00001089
Iteration 46/1000 | Loss: 0.00001089
Iteration 47/1000 | Loss: 0.00001089
Iteration 48/1000 | Loss: 0.00001089
Iteration 49/1000 | Loss: 0.00001087
Iteration 50/1000 | Loss: 0.00001087
Iteration 51/1000 | Loss: 0.00001086
Iteration 52/1000 | Loss: 0.00001086
Iteration 53/1000 | Loss: 0.00001086
Iteration 54/1000 | Loss: 0.00001086
Iteration 55/1000 | Loss: 0.00001086
Iteration 56/1000 | Loss: 0.00001085
Iteration 57/1000 | Loss: 0.00001082
Iteration 58/1000 | Loss: 0.00001082
Iteration 59/1000 | Loss: 0.00001082
Iteration 60/1000 | Loss: 0.00001082
Iteration 61/1000 | Loss: 0.00001082
Iteration 62/1000 | Loss: 0.00001082
Iteration 63/1000 | Loss: 0.00001082
Iteration 64/1000 | Loss: 0.00001082
Iteration 65/1000 | Loss: 0.00001082
Iteration 66/1000 | Loss: 0.00001082
Iteration 67/1000 | Loss: 0.00001081
Iteration 68/1000 | Loss: 0.00001081
Iteration 69/1000 | Loss: 0.00001081
Iteration 70/1000 | Loss: 0.00001080
Iteration 71/1000 | Loss: 0.00001080
Iteration 72/1000 | Loss: 0.00001080
Iteration 73/1000 | Loss: 0.00001080
Iteration 74/1000 | Loss: 0.00001080
Iteration 75/1000 | Loss: 0.00001079
Iteration 76/1000 | Loss: 0.00001079
Iteration 77/1000 | Loss: 0.00001079
Iteration 78/1000 | Loss: 0.00001079
Iteration 79/1000 | Loss: 0.00001079
Iteration 80/1000 | Loss: 0.00001079
Iteration 81/1000 | Loss: 0.00001078
Iteration 82/1000 | Loss: 0.00001078
Iteration 83/1000 | Loss: 0.00001078
Iteration 84/1000 | Loss: 0.00001078
Iteration 85/1000 | Loss: 0.00001078
Iteration 86/1000 | Loss: 0.00001078
Iteration 87/1000 | Loss: 0.00001078
Iteration 88/1000 | Loss: 0.00001078
Iteration 89/1000 | Loss: 0.00001077
Iteration 90/1000 | Loss: 0.00001077
Iteration 91/1000 | Loss: 0.00001077
Iteration 92/1000 | Loss: 0.00001077
Iteration 93/1000 | Loss: 0.00001077
Iteration 94/1000 | Loss: 0.00001077
Iteration 95/1000 | Loss: 0.00001076
Iteration 96/1000 | Loss: 0.00001076
Iteration 97/1000 | Loss: 0.00001076
Iteration 98/1000 | Loss: 0.00001075
Iteration 99/1000 | Loss: 0.00001075
Iteration 100/1000 | Loss: 0.00001075
Iteration 101/1000 | Loss: 0.00001075
Iteration 102/1000 | Loss: 0.00001075
Iteration 103/1000 | Loss: 0.00001075
Iteration 104/1000 | Loss: 0.00001075
Iteration 105/1000 | Loss: 0.00001074
Iteration 106/1000 | Loss: 0.00001074
Iteration 107/1000 | Loss: 0.00001074
Iteration 108/1000 | Loss: 0.00001074
Iteration 109/1000 | Loss: 0.00001074
Iteration 110/1000 | Loss: 0.00001074
Iteration 111/1000 | Loss: 0.00001073
Iteration 112/1000 | Loss: 0.00001073
Iteration 113/1000 | Loss: 0.00001073
Iteration 114/1000 | Loss: 0.00001073
Iteration 115/1000 | Loss: 0.00001073
Iteration 116/1000 | Loss: 0.00001073
Iteration 117/1000 | Loss: 0.00001072
Iteration 118/1000 | Loss: 0.00001072
Iteration 119/1000 | Loss: 0.00001072
Iteration 120/1000 | Loss: 0.00001072
Iteration 121/1000 | Loss: 0.00001072
Iteration 122/1000 | Loss: 0.00001071
Iteration 123/1000 | Loss: 0.00001071
Iteration 124/1000 | Loss: 0.00001071
Iteration 125/1000 | Loss: 0.00001071
Iteration 126/1000 | Loss: 0.00001071
Iteration 127/1000 | Loss: 0.00001071
Iteration 128/1000 | Loss: 0.00001071
Iteration 129/1000 | Loss: 0.00001071
Iteration 130/1000 | Loss: 0.00001071
Iteration 131/1000 | Loss: 0.00001070
Iteration 132/1000 | Loss: 0.00001070
Iteration 133/1000 | Loss: 0.00001070
Iteration 134/1000 | Loss: 0.00001069
Iteration 135/1000 | Loss: 0.00001069
Iteration 136/1000 | Loss: 0.00001069
Iteration 137/1000 | Loss: 0.00001068
Iteration 138/1000 | Loss: 0.00001068
Iteration 139/1000 | Loss: 0.00001068
Iteration 140/1000 | Loss: 0.00001068
Iteration 141/1000 | Loss: 0.00001068
Iteration 142/1000 | Loss: 0.00001067
Iteration 143/1000 | Loss: 0.00001067
Iteration 144/1000 | Loss: 0.00001067
Iteration 145/1000 | Loss: 0.00001067
Iteration 146/1000 | Loss: 0.00001067
Iteration 147/1000 | Loss: 0.00001067
Iteration 148/1000 | Loss: 0.00001066
Iteration 149/1000 | Loss: 0.00001066
Iteration 150/1000 | Loss: 0.00001066
Iteration 151/1000 | Loss: 0.00001066
Iteration 152/1000 | Loss: 0.00001065
Iteration 153/1000 | Loss: 0.00001065
Iteration 154/1000 | Loss: 0.00001065
Iteration 155/1000 | Loss: 0.00001065
Iteration 156/1000 | Loss: 0.00001065
Iteration 157/1000 | Loss: 0.00001065
Iteration 158/1000 | Loss: 0.00001064
Iteration 159/1000 | Loss: 0.00001064
Iteration 160/1000 | Loss: 0.00001064
Iteration 161/1000 | Loss: 0.00001064
Iteration 162/1000 | Loss: 0.00001064
Iteration 163/1000 | Loss: 0.00001064
Iteration 164/1000 | Loss: 0.00001063
Iteration 165/1000 | Loss: 0.00001063
Iteration 166/1000 | Loss: 0.00001062
Iteration 167/1000 | Loss: 0.00001062
Iteration 168/1000 | Loss: 0.00001062
Iteration 169/1000 | Loss: 0.00001062
Iteration 170/1000 | Loss: 0.00001062
Iteration 171/1000 | Loss: 0.00001062
Iteration 172/1000 | Loss: 0.00001062
Iteration 173/1000 | Loss: 0.00001062
Iteration 174/1000 | Loss: 0.00001061
Iteration 175/1000 | Loss: 0.00001061
Iteration 176/1000 | Loss: 0.00001061
Iteration 177/1000 | Loss: 0.00001061
Iteration 178/1000 | Loss: 0.00001061
Iteration 179/1000 | Loss: 0.00001061
Iteration 180/1000 | Loss: 0.00001061
Iteration 181/1000 | Loss: 0.00001061
Iteration 182/1000 | Loss: 0.00001061
Iteration 183/1000 | Loss: 0.00001061
Iteration 184/1000 | Loss: 0.00001061
Iteration 185/1000 | Loss: 0.00001061
Iteration 186/1000 | Loss: 0.00001061
Iteration 187/1000 | Loss: 0.00001061
Iteration 188/1000 | Loss: 0.00001061
Iteration 189/1000 | Loss: 0.00001061
Iteration 190/1000 | Loss: 0.00001061
Iteration 191/1000 | Loss: 0.00001061
Iteration 192/1000 | Loss: 0.00001061
Iteration 193/1000 | Loss: 0.00001061
Iteration 194/1000 | Loss: 0.00001061
Iteration 195/1000 | Loss: 0.00001061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.0612373444018885e-05, 1.0612373444018885e-05, 1.0612373444018885e-05, 1.0612373444018885e-05, 1.0612373444018885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0612373444018885e-05

Optimization complete. Final v2v error: 2.792532205581665 mm

Highest mean error: 3.0840561389923096 mm for frame 202

Lowest mean error: 2.594841718673706 mm for frame 255

Saving results

Total time: 46.747299671173096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826770
Iteration 2/25 | Loss: 0.00148269
Iteration 3/25 | Loss: 0.00126429
Iteration 4/25 | Loss: 0.00122772
Iteration 5/25 | Loss: 0.00122342
Iteration 6/25 | Loss: 0.00122318
Iteration 7/25 | Loss: 0.00122318
Iteration 8/25 | Loss: 0.00122318
Iteration 9/25 | Loss: 0.00122318
Iteration 10/25 | Loss: 0.00122318
Iteration 11/25 | Loss: 0.00122318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012231782311573625, 0.0012231782311573625, 0.0012231782311573625, 0.0012231782311573625, 0.0012231782311573625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012231782311573625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28384137
Iteration 2/25 | Loss: 0.00146004
Iteration 3/25 | Loss: 0.00146004
Iteration 4/25 | Loss: 0.00146003
Iteration 5/25 | Loss: 0.00146003
Iteration 6/25 | Loss: 0.00146003
Iteration 7/25 | Loss: 0.00146003
Iteration 8/25 | Loss: 0.00146003
Iteration 9/25 | Loss: 0.00146003
Iteration 10/25 | Loss: 0.00146003
Iteration 11/25 | Loss: 0.00146003
Iteration 12/25 | Loss: 0.00146003
Iteration 13/25 | Loss: 0.00146003
Iteration 14/25 | Loss: 0.00146003
Iteration 15/25 | Loss: 0.00146003
Iteration 16/25 | Loss: 0.00146003
Iteration 17/25 | Loss: 0.00146003
Iteration 18/25 | Loss: 0.00146003
Iteration 19/25 | Loss: 0.00146003
Iteration 20/25 | Loss: 0.00146003
Iteration 21/25 | Loss: 0.00146003
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014600319555029273, 0.0014600319555029273, 0.0014600319555029273, 0.0014600319555029273, 0.0014600319555029273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014600319555029273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146003
Iteration 2/1000 | Loss: 0.00003308
Iteration 3/1000 | Loss: 0.00002259
Iteration 4/1000 | Loss: 0.00001785
Iteration 5/1000 | Loss: 0.00001619
Iteration 6/1000 | Loss: 0.00001508
Iteration 7/1000 | Loss: 0.00001426
Iteration 8/1000 | Loss: 0.00001384
Iteration 9/1000 | Loss: 0.00001347
Iteration 10/1000 | Loss: 0.00001297
Iteration 11/1000 | Loss: 0.00001271
Iteration 12/1000 | Loss: 0.00001249
Iteration 13/1000 | Loss: 0.00001248
Iteration 14/1000 | Loss: 0.00001236
Iteration 15/1000 | Loss: 0.00001227
Iteration 16/1000 | Loss: 0.00001223
Iteration 17/1000 | Loss: 0.00001222
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001221
Iteration 20/1000 | Loss: 0.00001221
Iteration 21/1000 | Loss: 0.00001213
Iteration 22/1000 | Loss: 0.00001211
Iteration 23/1000 | Loss: 0.00001208
Iteration 24/1000 | Loss: 0.00001207
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00001206
Iteration 27/1000 | Loss: 0.00001202
Iteration 28/1000 | Loss: 0.00001200
Iteration 29/1000 | Loss: 0.00001199
Iteration 30/1000 | Loss: 0.00001199
Iteration 31/1000 | Loss: 0.00001198
Iteration 32/1000 | Loss: 0.00001198
Iteration 33/1000 | Loss: 0.00001197
Iteration 34/1000 | Loss: 0.00001196
Iteration 35/1000 | Loss: 0.00001196
Iteration 36/1000 | Loss: 0.00001195
Iteration 37/1000 | Loss: 0.00001195
Iteration 38/1000 | Loss: 0.00001194
Iteration 39/1000 | Loss: 0.00001194
Iteration 40/1000 | Loss: 0.00001194
Iteration 41/1000 | Loss: 0.00001191
Iteration 42/1000 | Loss: 0.00001190
Iteration 43/1000 | Loss: 0.00001190
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001189
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001188
Iteration 49/1000 | Loss: 0.00001187
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001186
Iteration 52/1000 | Loss: 0.00001185
Iteration 53/1000 | Loss: 0.00001184
Iteration 54/1000 | Loss: 0.00001184
Iteration 55/1000 | Loss: 0.00001184
Iteration 56/1000 | Loss: 0.00001184
Iteration 57/1000 | Loss: 0.00001184
Iteration 58/1000 | Loss: 0.00001184
Iteration 59/1000 | Loss: 0.00001184
Iteration 60/1000 | Loss: 0.00001184
Iteration 61/1000 | Loss: 0.00001183
Iteration 62/1000 | Loss: 0.00001183
Iteration 63/1000 | Loss: 0.00001183
Iteration 64/1000 | Loss: 0.00001182
Iteration 65/1000 | Loss: 0.00001181
Iteration 66/1000 | Loss: 0.00001181
Iteration 67/1000 | Loss: 0.00001181
Iteration 68/1000 | Loss: 0.00001181
Iteration 69/1000 | Loss: 0.00001180
Iteration 70/1000 | Loss: 0.00001180
Iteration 71/1000 | Loss: 0.00001180
Iteration 72/1000 | Loss: 0.00001180
Iteration 73/1000 | Loss: 0.00001179
Iteration 74/1000 | Loss: 0.00001179
Iteration 75/1000 | Loss: 0.00001179
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001178
Iteration 78/1000 | Loss: 0.00001178
Iteration 79/1000 | Loss: 0.00001177
Iteration 80/1000 | Loss: 0.00001176
Iteration 81/1000 | Loss: 0.00001176
Iteration 82/1000 | Loss: 0.00001176
Iteration 83/1000 | Loss: 0.00001176
Iteration 84/1000 | Loss: 0.00001175
Iteration 85/1000 | Loss: 0.00001175
Iteration 86/1000 | Loss: 0.00001175
Iteration 87/1000 | Loss: 0.00001174
Iteration 88/1000 | Loss: 0.00001173
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001169
Iteration 92/1000 | Loss: 0.00001169
Iteration 93/1000 | Loss: 0.00001168
Iteration 94/1000 | Loss: 0.00001167
Iteration 95/1000 | Loss: 0.00001167
Iteration 96/1000 | Loss: 0.00001166
Iteration 97/1000 | Loss: 0.00001166
Iteration 98/1000 | Loss: 0.00001166
Iteration 99/1000 | Loss: 0.00001166
Iteration 100/1000 | Loss: 0.00001165
Iteration 101/1000 | Loss: 0.00001165
Iteration 102/1000 | Loss: 0.00001165
Iteration 103/1000 | Loss: 0.00001165
Iteration 104/1000 | Loss: 0.00001164
Iteration 105/1000 | Loss: 0.00001164
Iteration 106/1000 | Loss: 0.00001164
Iteration 107/1000 | Loss: 0.00001163
Iteration 108/1000 | Loss: 0.00001163
Iteration 109/1000 | Loss: 0.00001163
Iteration 110/1000 | Loss: 0.00001163
Iteration 111/1000 | Loss: 0.00001163
Iteration 112/1000 | Loss: 0.00001163
Iteration 113/1000 | Loss: 0.00001163
Iteration 114/1000 | Loss: 0.00001163
Iteration 115/1000 | Loss: 0.00001162
Iteration 116/1000 | Loss: 0.00001162
Iteration 117/1000 | Loss: 0.00001162
Iteration 118/1000 | Loss: 0.00001162
Iteration 119/1000 | Loss: 0.00001162
Iteration 120/1000 | Loss: 0.00001162
Iteration 121/1000 | Loss: 0.00001162
Iteration 122/1000 | Loss: 0.00001162
Iteration 123/1000 | Loss: 0.00001162
Iteration 124/1000 | Loss: 0.00001162
Iteration 125/1000 | Loss: 0.00001162
Iteration 126/1000 | Loss: 0.00001162
Iteration 127/1000 | Loss: 0.00001162
Iteration 128/1000 | Loss: 0.00001162
Iteration 129/1000 | Loss: 0.00001162
Iteration 130/1000 | Loss: 0.00001162
Iteration 131/1000 | Loss: 0.00001162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.1617022209975403e-05, 1.1617022209975403e-05, 1.1617022209975403e-05, 1.1617022209975403e-05, 1.1617022209975403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1617022209975403e-05

Optimization complete. Final v2v error: 2.9094913005828857 mm

Highest mean error: 3.6937174797058105 mm for frame 97

Lowest mean error: 2.5244529247283936 mm for frame 1

Saving results

Total time: 45.79815196990967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828287
Iteration 2/25 | Loss: 0.00136270
Iteration 3/25 | Loss: 0.00124494
Iteration 4/25 | Loss: 0.00123527
Iteration 5/25 | Loss: 0.00123318
Iteration 6/25 | Loss: 0.00123312
Iteration 7/25 | Loss: 0.00123312
Iteration 8/25 | Loss: 0.00123312
Iteration 9/25 | Loss: 0.00123312
Iteration 10/25 | Loss: 0.00123312
Iteration 11/25 | Loss: 0.00123312
Iteration 12/25 | Loss: 0.00123312
Iteration 13/25 | Loss: 0.00123312
Iteration 14/25 | Loss: 0.00123312
Iteration 15/25 | Loss: 0.00123312
Iteration 16/25 | Loss: 0.00123312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012331197503954172, 0.0012331197503954172, 0.0012331197503954172, 0.0012331197503954172, 0.0012331197503954172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012331197503954172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38642251
Iteration 2/25 | Loss: 0.00133032
Iteration 3/25 | Loss: 0.00133032
Iteration 4/25 | Loss: 0.00133032
Iteration 5/25 | Loss: 0.00133032
Iteration 6/25 | Loss: 0.00133032
Iteration 7/25 | Loss: 0.00133031
Iteration 8/25 | Loss: 0.00133031
Iteration 9/25 | Loss: 0.00133031
Iteration 10/25 | Loss: 0.00133031
Iteration 11/25 | Loss: 0.00133031
Iteration 12/25 | Loss: 0.00133031
Iteration 13/25 | Loss: 0.00133031
Iteration 14/25 | Loss: 0.00133031
Iteration 15/25 | Loss: 0.00133031
Iteration 16/25 | Loss: 0.00133031
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013303137384355068, 0.0013303137384355068, 0.0013303137384355068, 0.0013303137384355068, 0.0013303137384355068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013303137384355068

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133031
Iteration 2/1000 | Loss: 0.00003303
Iteration 3/1000 | Loss: 0.00001995
Iteration 4/1000 | Loss: 0.00001583
Iteration 5/1000 | Loss: 0.00001467
Iteration 6/1000 | Loss: 0.00001399
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001307
Iteration 9/1000 | Loss: 0.00001275
Iteration 10/1000 | Loss: 0.00001244
Iteration 11/1000 | Loss: 0.00001227
Iteration 12/1000 | Loss: 0.00001217
Iteration 13/1000 | Loss: 0.00001214
Iteration 14/1000 | Loss: 0.00001200
Iteration 15/1000 | Loss: 0.00001198
Iteration 16/1000 | Loss: 0.00001197
Iteration 17/1000 | Loss: 0.00001194
Iteration 18/1000 | Loss: 0.00001188
Iteration 19/1000 | Loss: 0.00001182
Iteration 20/1000 | Loss: 0.00001180
Iteration 21/1000 | Loss: 0.00001177
Iteration 22/1000 | Loss: 0.00001172
Iteration 23/1000 | Loss: 0.00001170
Iteration 24/1000 | Loss: 0.00001169
Iteration 25/1000 | Loss: 0.00001164
Iteration 26/1000 | Loss: 0.00001163
Iteration 27/1000 | Loss: 0.00001161
Iteration 28/1000 | Loss: 0.00001160
Iteration 29/1000 | Loss: 0.00001160
Iteration 30/1000 | Loss: 0.00001159
Iteration 31/1000 | Loss: 0.00001159
Iteration 32/1000 | Loss: 0.00001159
Iteration 33/1000 | Loss: 0.00001159
Iteration 34/1000 | Loss: 0.00001159
Iteration 35/1000 | Loss: 0.00001159
Iteration 36/1000 | Loss: 0.00001159
Iteration 37/1000 | Loss: 0.00001159
Iteration 38/1000 | Loss: 0.00001158
Iteration 39/1000 | Loss: 0.00001158
Iteration 40/1000 | Loss: 0.00001158
Iteration 41/1000 | Loss: 0.00001158
Iteration 42/1000 | Loss: 0.00001158
Iteration 43/1000 | Loss: 0.00001158
Iteration 44/1000 | Loss: 0.00001158
Iteration 45/1000 | Loss: 0.00001157
Iteration 46/1000 | Loss: 0.00001157
Iteration 47/1000 | Loss: 0.00001156
Iteration 48/1000 | Loss: 0.00001156
Iteration 49/1000 | Loss: 0.00001155
Iteration 50/1000 | Loss: 0.00001155
Iteration 51/1000 | Loss: 0.00001155
Iteration 52/1000 | Loss: 0.00001155
Iteration 53/1000 | Loss: 0.00001155
Iteration 54/1000 | Loss: 0.00001155
Iteration 55/1000 | Loss: 0.00001155
Iteration 56/1000 | Loss: 0.00001154
Iteration 57/1000 | Loss: 0.00001154
Iteration 58/1000 | Loss: 0.00001154
Iteration 59/1000 | Loss: 0.00001154
Iteration 60/1000 | Loss: 0.00001153
Iteration 61/1000 | Loss: 0.00001153
Iteration 62/1000 | Loss: 0.00001153
Iteration 63/1000 | Loss: 0.00001153
Iteration 64/1000 | Loss: 0.00001153
Iteration 65/1000 | Loss: 0.00001153
Iteration 66/1000 | Loss: 0.00001153
Iteration 67/1000 | Loss: 0.00001153
Iteration 68/1000 | Loss: 0.00001152
Iteration 69/1000 | Loss: 0.00001152
Iteration 70/1000 | Loss: 0.00001152
Iteration 71/1000 | Loss: 0.00001152
Iteration 72/1000 | Loss: 0.00001152
Iteration 73/1000 | Loss: 0.00001152
Iteration 74/1000 | Loss: 0.00001152
Iteration 75/1000 | Loss: 0.00001152
Iteration 76/1000 | Loss: 0.00001152
Iteration 77/1000 | Loss: 0.00001151
Iteration 78/1000 | Loss: 0.00001151
Iteration 79/1000 | Loss: 0.00001150
Iteration 80/1000 | Loss: 0.00001150
Iteration 81/1000 | Loss: 0.00001150
Iteration 82/1000 | Loss: 0.00001149
Iteration 83/1000 | Loss: 0.00001149
Iteration 84/1000 | Loss: 0.00001148
Iteration 85/1000 | Loss: 0.00001147
Iteration 86/1000 | Loss: 0.00001147
Iteration 87/1000 | Loss: 0.00001147
Iteration 88/1000 | Loss: 0.00001146
Iteration 89/1000 | Loss: 0.00001146
Iteration 90/1000 | Loss: 0.00001146
Iteration 91/1000 | Loss: 0.00001145
Iteration 92/1000 | Loss: 0.00001144
Iteration 93/1000 | Loss: 0.00001144
Iteration 94/1000 | Loss: 0.00001144
Iteration 95/1000 | Loss: 0.00001144
Iteration 96/1000 | Loss: 0.00001144
Iteration 97/1000 | Loss: 0.00001144
Iteration 98/1000 | Loss: 0.00001143
Iteration 99/1000 | Loss: 0.00001143
Iteration 100/1000 | Loss: 0.00001143
Iteration 101/1000 | Loss: 0.00001143
Iteration 102/1000 | Loss: 0.00001143
Iteration 103/1000 | Loss: 0.00001143
Iteration 104/1000 | Loss: 0.00001142
Iteration 105/1000 | Loss: 0.00001142
Iteration 106/1000 | Loss: 0.00001142
Iteration 107/1000 | Loss: 0.00001142
Iteration 108/1000 | Loss: 0.00001142
Iteration 109/1000 | Loss: 0.00001142
Iteration 110/1000 | Loss: 0.00001142
Iteration 111/1000 | Loss: 0.00001141
Iteration 112/1000 | Loss: 0.00001141
Iteration 113/1000 | Loss: 0.00001141
Iteration 114/1000 | Loss: 0.00001141
Iteration 115/1000 | Loss: 0.00001141
Iteration 116/1000 | Loss: 0.00001141
Iteration 117/1000 | Loss: 0.00001141
Iteration 118/1000 | Loss: 0.00001140
Iteration 119/1000 | Loss: 0.00001140
Iteration 120/1000 | Loss: 0.00001140
Iteration 121/1000 | Loss: 0.00001140
Iteration 122/1000 | Loss: 0.00001140
Iteration 123/1000 | Loss: 0.00001140
Iteration 124/1000 | Loss: 0.00001140
Iteration 125/1000 | Loss: 0.00001140
Iteration 126/1000 | Loss: 0.00001139
Iteration 127/1000 | Loss: 0.00001139
Iteration 128/1000 | Loss: 0.00001139
Iteration 129/1000 | Loss: 0.00001139
Iteration 130/1000 | Loss: 0.00001139
Iteration 131/1000 | Loss: 0.00001139
Iteration 132/1000 | Loss: 0.00001139
Iteration 133/1000 | Loss: 0.00001139
Iteration 134/1000 | Loss: 0.00001138
Iteration 135/1000 | Loss: 0.00001138
Iteration 136/1000 | Loss: 0.00001138
Iteration 137/1000 | Loss: 0.00001138
Iteration 138/1000 | Loss: 0.00001138
Iteration 139/1000 | Loss: 0.00001138
Iteration 140/1000 | Loss: 0.00001138
Iteration 141/1000 | Loss: 0.00001138
Iteration 142/1000 | Loss: 0.00001138
Iteration 143/1000 | Loss: 0.00001137
Iteration 144/1000 | Loss: 0.00001137
Iteration 145/1000 | Loss: 0.00001137
Iteration 146/1000 | Loss: 0.00001137
Iteration 147/1000 | Loss: 0.00001137
Iteration 148/1000 | Loss: 0.00001137
Iteration 149/1000 | Loss: 0.00001137
Iteration 150/1000 | Loss: 0.00001137
Iteration 151/1000 | Loss: 0.00001137
Iteration 152/1000 | Loss: 0.00001136
Iteration 153/1000 | Loss: 0.00001136
Iteration 154/1000 | Loss: 0.00001136
Iteration 155/1000 | Loss: 0.00001136
Iteration 156/1000 | Loss: 0.00001136
Iteration 157/1000 | Loss: 0.00001135
Iteration 158/1000 | Loss: 0.00001135
Iteration 159/1000 | Loss: 0.00001135
Iteration 160/1000 | Loss: 0.00001135
Iteration 161/1000 | Loss: 0.00001135
Iteration 162/1000 | Loss: 0.00001135
Iteration 163/1000 | Loss: 0.00001135
Iteration 164/1000 | Loss: 0.00001135
Iteration 165/1000 | Loss: 0.00001135
Iteration 166/1000 | Loss: 0.00001134
Iteration 167/1000 | Loss: 0.00001134
Iteration 168/1000 | Loss: 0.00001134
Iteration 169/1000 | Loss: 0.00001134
Iteration 170/1000 | Loss: 0.00001133
Iteration 171/1000 | Loss: 0.00001133
Iteration 172/1000 | Loss: 0.00001133
Iteration 173/1000 | Loss: 0.00001133
Iteration 174/1000 | Loss: 0.00001133
Iteration 175/1000 | Loss: 0.00001133
Iteration 176/1000 | Loss: 0.00001133
Iteration 177/1000 | Loss: 0.00001133
Iteration 178/1000 | Loss: 0.00001133
Iteration 179/1000 | Loss: 0.00001133
Iteration 180/1000 | Loss: 0.00001133
Iteration 181/1000 | Loss: 0.00001133
Iteration 182/1000 | Loss: 0.00001132
Iteration 183/1000 | Loss: 0.00001132
Iteration 184/1000 | Loss: 0.00001132
Iteration 185/1000 | Loss: 0.00001132
Iteration 186/1000 | Loss: 0.00001132
Iteration 187/1000 | Loss: 0.00001132
Iteration 188/1000 | Loss: 0.00001131
Iteration 189/1000 | Loss: 0.00001131
Iteration 190/1000 | Loss: 0.00001131
Iteration 191/1000 | Loss: 0.00001131
Iteration 192/1000 | Loss: 0.00001131
Iteration 193/1000 | Loss: 0.00001131
Iteration 194/1000 | Loss: 0.00001131
Iteration 195/1000 | Loss: 0.00001130
Iteration 196/1000 | Loss: 0.00001130
Iteration 197/1000 | Loss: 0.00001130
Iteration 198/1000 | Loss: 0.00001130
Iteration 199/1000 | Loss: 0.00001130
Iteration 200/1000 | Loss: 0.00001130
Iteration 201/1000 | Loss: 0.00001130
Iteration 202/1000 | Loss: 0.00001130
Iteration 203/1000 | Loss: 0.00001130
Iteration 204/1000 | Loss: 0.00001130
Iteration 205/1000 | Loss: 0.00001130
Iteration 206/1000 | Loss: 0.00001130
Iteration 207/1000 | Loss: 0.00001130
Iteration 208/1000 | Loss: 0.00001130
Iteration 209/1000 | Loss: 0.00001130
Iteration 210/1000 | Loss: 0.00001130
Iteration 211/1000 | Loss: 0.00001130
Iteration 212/1000 | Loss: 0.00001130
Iteration 213/1000 | Loss: 0.00001130
Iteration 214/1000 | Loss: 0.00001130
Iteration 215/1000 | Loss: 0.00001130
Iteration 216/1000 | Loss: 0.00001130
Iteration 217/1000 | Loss: 0.00001130
Iteration 218/1000 | Loss: 0.00001130
Iteration 219/1000 | Loss: 0.00001130
Iteration 220/1000 | Loss: 0.00001130
Iteration 221/1000 | Loss: 0.00001130
Iteration 222/1000 | Loss: 0.00001130
Iteration 223/1000 | Loss: 0.00001130
Iteration 224/1000 | Loss: 0.00001130
Iteration 225/1000 | Loss: 0.00001130
Iteration 226/1000 | Loss: 0.00001130
Iteration 227/1000 | Loss: 0.00001130
Iteration 228/1000 | Loss: 0.00001130
Iteration 229/1000 | Loss: 0.00001130
Iteration 230/1000 | Loss: 0.00001130
Iteration 231/1000 | Loss: 0.00001130
Iteration 232/1000 | Loss: 0.00001130
Iteration 233/1000 | Loss: 0.00001130
Iteration 234/1000 | Loss: 0.00001130
Iteration 235/1000 | Loss: 0.00001130
Iteration 236/1000 | Loss: 0.00001130
Iteration 237/1000 | Loss: 0.00001130
Iteration 238/1000 | Loss: 0.00001130
Iteration 239/1000 | Loss: 0.00001130
Iteration 240/1000 | Loss: 0.00001130
Iteration 241/1000 | Loss: 0.00001130
Iteration 242/1000 | Loss: 0.00001130
Iteration 243/1000 | Loss: 0.00001130
Iteration 244/1000 | Loss: 0.00001130
Iteration 245/1000 | Loss: 0.00001130
Iteration 246/1000 | Loss: 0.00001130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.129957763623679e-05, 1.129957763623679e-05, 1.129957763623679e-05, 1.129957763623679e-05, 1.129957763623679e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.129957763623679e-05

Optimization complete. Final v2v error: 2.8465113639831543 mm

Highest mean error: 3.805701971054077 mm for frame 119

Lowest mean error: 2.5608110427856445 mm for frame 37

Saving results

Total time: 45.756149768829346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00545980
Iteration 2/25 | Loss: 0.00127454
Iteration 3/25 | Loss: 0.00120685
Iteration 4/25 | Loss: 0.00119601
Iteration 5/25 | Loss: 0.00119233
Iteration 6/25 | Loss: 0.00119152
Iteration 7/25 | Loss: 0.00119152
Iteration 8/25 | Loss: 0.00119152
Iteration 9/25 | Loss: 0.00119152
Iteration 10/25 | Loss: 0.00119152
Iteration 11/25 | Loss: 0.00119152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011915244394913316, 0.0011915244394913316, 0.0011915244394913316, 0.0011915244394913316, 0.0011915244394913316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011915244394913316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.75920463
Iteration 2/25 | Loss: 0.00141134
Iteration 3/25 | Loss: 0.00141134
Iteration 4/25 | Loss: 0.00141134
Iteration 5/25 | Loss: 0.00141134
Iteration 6/25 | Loss: 0.00141134
Iteration 7/25 | Loss: 0.00141134
Iteration 8/25 | Loss: 0.00141134
Iteration 9/25 | Loss: 0.00141133
Iteration 10/25 | Loss: 0.00141133
Iteration 11/25 | Loss: 0.00141133
Iteration 12/25 | Loss: 0.00141133
Iteration 13/25 | Loss: 0.00141133
Iteration 14/25 | Loss: 0.00141133
Iteration 15/25 | Loss: 0.00141133
Iteration 16/25 | Loss: 0.00141133
Iteration 17/25 | Loss: 0.00141133
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001411334378644824, 0.001411334378644824, 0.001411334378644824, 0.001411334378644824, 0.001411334378644824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001411334378644824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141133
Iteration 2/1000 | Loss: 0.00002398
Iteration 3/1000 | Loss: 0.00001730
Iteration 4/1000 | Loss: 0.00001513
Iteration 5/1000 | Loss: 0.00001400
Iteration 6/1000 | Loss: 0.00001346
Iteration 7/1000 | Loss: 0.00001297
Iteration 8/1000 | Loss: 0.00001240
Iteration 9/1000 | Loss: 0.00001209
Iteration 10/1000 | Loss: 0.00001176
Iteration 11/1000 | Loss: 0.00001150
Iteration 12/1000 | Loss: 0.00001145
Iteration 13/1000 | Loss: 0.00001140
Iteration 14/1000 | Loss: 0.00001131
Iteration 15/1000 | Loss: 0.00001129
Iteration 16/1000 | Loss: 0.00001123
Iteration 17/1000 | Loss: 0.00001121
Iteration 18/1000 | Loss: 0.00001120
Iteration 19/1000 | Loss: 0.00001114
Iteration 20/1000 | Loss: 0.00001108
Iteration 21/1000 | Loss: 0.00001104
Iteration 22/1000 | Loss: 0.00001101
Iteration 23/1000 | Loss: 0.00001099
Iteration 24/1000 | Loss: 0.00001097
Iteration 25/1000 | Loss: 0.00001096
Iteration 26/1000 | Loss: 0.00001096
Iteration 27/1000 | Loss: 0.00001095
Iteration 28/1000 | Loss: 0.00001093
Iteration 29/1000 | Loss: 0.00001092
Iteration 30/1000 | Loss: 0.00001092
Iteration 31/1000 | Loss: 0.00001088
Iteration 32/1000 | Loss: 0.00001084
Iteration 33/1000 | Loss: 0.00001084
Iteration 34/1000 | Loss: 0.00001082
Iteration 35/1000 | Loss: 0.00001074
Iteration 36/1000 | Loss: 0.00001074
Iteration 37/1000 | Loss: 0.00001069
Iteration 38/1000 | Loss: 0.00001069
Iteration 39/1000 | Loss: 0.00001069
Iteration 40/1000 | Loss: 0.00001068
Iteration 41/1000 | Loss: 0.00001063
Iteration 42/1000 | Loss: 0.00001063
Iteration 43/1000 | Loss: 0.00001063
Iteration 44/1000 | Loss: 0.00001063
Iteration 45/1000 | Loss: 0.00001062
Iteration 46/1000 | Loss: 0.00001062
Iteration 47/1000 | Loss: 0.00001061
Iteration 48/1000 | Loss: 0.00001061
Iteration 49/1000 | Loss: 0.00001061
Iteration 50/1000 | Loss: 0.00001060
Iteration 51/1000 | Loss: 0.00001060
Iteration 52/1000 | Loss: 0.00001060
Iteration 53/1000 | Loss: 0.00001059
Iteration 54/1000 | Loss: 0.00001059
Iteration 55/1000 | Loss: 0.00001059
Iteration 56/1000 | Loss: 0.00001059
Iteration 57/1000 | Loss: 0.00001059
Iteration 58/1000 | Loss: 0.00001059
Iteration 59/1000 | Loss: 0.00001059
Iteration 60/1000 | Loss: 0.00001058
Iteration 61/1000 | Loss: 0.00001058
Iteration 62/1000 | Loss: 0.00001058
Iteration 63/1000 | Loss: 0.00001057
Iteration 64/1000 | Loss: 0.00001057
Iteration 65/1000 | Loss: 0.00001056
Iteration 66/1000 | Loss: 0.00001056
Iteration 67/1000 | Loss: 0.00001056
Iteration 68/1000 | Loss: 0.00001056
Iteration 69/1000 | Loss: 0.00001055
Iteration 70/1000 | Loss: 0.00001055
Iteration 71/1000 | Loss: 0.00001055
Iteration 72/1000 | Loss: 0.00001055
Iteration 73/1000 | Loss: 0.00001055
Iteration 74/1000 | Loss: 0.00001054
Iteration 75/1000 | Loss: 0.00001054
Iteration 76/1000 | Loss: 0.00001053
Iteration 77/1000 | Loss: 0.00001053
Iteration 78/1000 | Loss: 0.00001053
Iteration 79/1000 | Loss: 0.00001052
Iteration 80/1000 | Loss: 0.00001052
Iteration 81/1000 | Loss: 0.00001052
Iteration 82/1000 | Loss: 0.00001051
Iteration 83/1000 | Loss: 0.00001051
Iteration 84/1000 | Loss: 0.00001050
Iteration 85/1000 | Loss: 0.00001050
Iteration 86/1000 | Loss: 0.00001050
Iteration 87/1000 | Loss: 0.00001050
Iteration 88/1000 | Loss: 0.00001050
Iteration 89/1000 | Loss: 0.00001049
Iteration 90/1000 | Loss: 0.00001049
Iteration 91/1000 | Loss: 0.00001049
Iteration 92/1000 | Loss: 0.00001048
Iteration 93/1000 | Loss: 0.00001047
Iteration 94/1000 | Loss: 0.00001047
Iteration 95/1000 | Loss: 0.00001047
Iteration 96/1000 | Loss: 0.00001047
Iteration 97/1000 | Loss: 0.00001047
Iteration 98/1000 | Loss: 0.00001047
Iteration 99/1000 | Loss: 0.00001047
Iteration 100/1000 | Loss: 0.00001047
Iteration 101/1000 | Loss: 0.00001047
Iteration 102/1000 | Loss: 0.00001046
Iteration 103/1000 | Loss: 0.00001046
Iteration 104/1000 | Loss: 0.00001046
Iteration 105/1000 | Loss: 0.00001046
Iteration 106/1000 | Loss: 0.00001046
Iteration 107/1000 | Loss: 0.00001046
Iteration 108/1000 | Loss: 0.00001046
Iteration 109/1000 | Loss: 0.00001046
Iteration 110/1000 | Loss: 0.00001046
Iteration 111/1000 | Loss: 0.00001046
Iteration 112/1000 | Loss: 0.00001046
Iteration 113/1000 | Loss: 0.00001046
Iteration 114/1000 | Loss: 0.00001045
Iteration 115/1000 | Loss: 0.00001045
Iteration 116/1000 | Loss: 0.00001045
Iteration 117/1000 | Loss: 0.00001045
Iteration 118/1000 | Loss: 0.00001044
Iteration 119/1000 | Loss: 0.00001044
Iteration 120/1000 | Loss: 0.00001044
Iteration 121/1000 | Loss: 0.00001044
Iteration 122/1000 | Loss: 0.00001044
Iteration 123/1000 | Loss: 0.00001043
Iteration 124/1000 | Loss: 0.00001043
Iteration 125/1000 | Loss: 0.00001043
Iteration 126/1000 | Loss: 0.00001043
Iteration 127/1000 | Loss: 0.00001043
Iteration 128/1000 | Loss: 0.00001043
Iteration 129/1000 | Loss: 0.00001043
Iteration 130/1000 | Loss: 0.00001043
Iteration 131/1000 | Loss: 0.00001042
Iteration 132/1000 | Loss: 0.00001042
Iteration 133/1000 | Loss: 0.00001042
Iteration 134/1000 | Loss: 0.00001042
Iteration 135/1000 | Loss: 0.00001042
Iteration 136/1000 | Loss: 0.00001042
Iteration 137/1000 | Loss: 0.00001042
Iteration 138/1000 | Loss: 0.00001042
Iteration 139/1000 | Loss: 0.00001042
Iteration 140/1000 | Loss: 0.00001041
Iteration 141/1000 | Loss: 0.00001041
Iteration 142/1000 | Loss: 0.00001041
Iteration 143/1000 | Loss: 0.00001041
Iteration 144/1000 | Loss: 0.00001041
Iteration 145/1000 | Loss: 0.00001040
Iteration 146/1000 | Loss: 0.00001040
Iteration 147/1000 | Loss: 0.00001040
Iteration 148/1000 | Loss: 0.00001040
Iteration 149/1000 | Loss: 0.00001040
Iteration 150/1000 | Loss: 0.00001040
Iteration 151/1000 | Loss: 0.00001040
Iteration 152/1000 | Loss: 0.00001040
Iteration 153/1000 | Loss: 0.00001040
Iteration 154/1000 | Loss: 0.00001040
Iteration 155/1000 | Loss: 0.00001040
Iteration 156/1000 | Loss: 0.00001040
Iteration 157/1000 | Loss: 0.00001039
Iteration 158/1000 | Loss: 0.00001039
Iteration 159/1000 | Loss: 0.00001039
Iteration 160/1000 | Loss: 0.00001039
Iteration 161/1000 | Loss: 0.00001039
Iteration 162/1000 | Loss: 0.00001039
Iteration 163/1000 | Loss: 0.00001039
Iteration 164/1000 | Loss: 0.00001039
Iteration 165/1000 | Loss: 0.00001039
Iteration 166/1000 | Loss: 0.00001039
Iteration 167/1000 | Loss: 0.00001039
Iteration 168/1000 | Loss: 0.00001038
Iteration 169/1000 | Loss: 0.00001038
Iteration 170/1000 | Loss: 0.00001038
Iteration 171/1000 | Loss: 0.00001038
Iteration 172/1000 | Loss: 0.00001038
Iteration 173/1000 | Loss: 0.00001038
Iteration 174/1000 | Loss: 0.00001038
Iteration 175/1000 | Loss: 0.00001038
Iteration 176/1000 | Loss: 0.00001038
Iteration 177/1000 | Loss: 0.00001038
Iteration 178/1000 | Loss: 0.00001038
Iteration 179/1000 | Loss: 0.00001038
Iteration 180/1000 | Loss: 0.00001038
Iteration 181/1000 | Loss: 0.00001038
Iteration 182/1000 | Loss: 0.00001038
Iteration 183/1000 | Loss: 0.00001038
Iteration 184/1000 | Loss: 0.00001038
Iteration 185/1000 | Loss: 0.00001038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.0381721949670464e-05, 1.0381721949670464e-05, 1.0381721949670464e-05, 1.0381721949670464e-05, 1.0381721949670464e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0381721949670464e-05

Optimization complete. Final v2v error: 2.8088276386260986 mm

Highest mean error: 3.1723415851593018 mm for frame 136

Lowest mean error: 2.623305082321167 mm for frame 155

Saving results

Total time: 44.45502209663391
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775498
Iteration 2/25 | Loss: 0.00184114
Iteration 3/25 | Loss: 0.00143805
Iteration 4/25 | Loss: 0.00134478
Iteration 5/25 | Loss: 0.00131293
Iteration 6/25 | Loss: 0.00129289
Iteration 7/25 | Loss: 0.00128518
Iteration 8/25 | Loss: 0.00124533
Iteration 9/25 | Loss: 0.00122168
Iteration 10/25 | Loss: 0.00121793
Iteration 11/25 | Loss: 0.00120870
Iteration 12/25 | Loss: 0.00120610
Iteration 13/25 | Loss: 0.00121076
Iteration 14/25 | Loss: 0.00120818
Iteration 15/25 | Loss: 0.00120367
Iteration 16/25 | Loss: 0.00120171
Iteration 17/25 | Loss: 0.00120049
Iteration 18/25 | Loss: 0.00119938
Iteration 19/25 | Loss: 0.00119961
Iteration 20/25 | Loss: 0.00119845
Iteration 21/25 | Loss: 0.00120124
Iteration 22/25 | Loss: 0.00119915
Iteration 23/25 | Loss: 0.00119812
Iteration 24/25 | Loss: 0.00119912
Iteration 25/25 | Loss: 0.00120201

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95526791
Iteration 2/25 | Loss: 0.00140615
Iteration 3/25 | Loss: 0.00140614
Iteration 4/25 | Loss: 0.00140614
Iteration 5/25 | Loss: 0.00140614
Iteration 6/25 | Loss: 0.00140614
Iteration 7/25 | Loss: 0.00140614
Iteration 8/25 | Loss: 0.00140614
Iteration 9/25 | Loss: 0.00140614
Iteration 10/25 | Loss: 0.00140614
Iteration 11/25 | Loss: 0.00140614
Iteration 12/25 | Loss: 0.00140614
Iteration 13/25 | Loss: 0.00140614
Iteration 14/25 | Loss: 0.00140614
Iteration 15/25 | Loss: 0.00140614
Iteration 16/25 | Loss: 0.00140614
Iteration 17/25 | Loss: 0.00140614
Iteration 18/25 | Loss: 0.00140614
Iteration 19/25 | Loss: 0.00140614
Iteration 20/25 | Loss: 0.00140614
Iteration 21/25 | Loss: 0.00140614
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014061422552913427, 0.0014061422552913427, 0.0014061422552913427, 0.0014061422552913427, 0.0014061422552913427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014061422552913427

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140614
Iteration 2/1000 | Loss: 0.00002863
Iteration 3/1000 | Loss: 0.00013532
Iteration 4/1000 | Loss: 0.00001961
Iteration 5/1000 | Loss: 0.00002937
Iteration 6/1000 | Loss: 0.00002375
Iteration 7/1000 | Loss: 0.00003499
Iteration 8/1000 | Loss: 0.00002482
Iteration 9/1000 | Loss: 0.00002056
Iteration 10/1000 | Loss: 0.00003831
Iteration 11/1000 | Loss: 0.00003492
Iteration 12/1000 | Loss: 0.00003225
Iteration 13/1000 | Loss: 0.00002779
Iteration 14/1000 | Loss: 0.00002510
Iteration 15/1000 | Loss: 0.00003094
Iteration 16/1000 | Loss: 0.00002513
Iteration 17/1000 | Loss: 0.00002810
Iteration 18/1000 | Loss: 0.00002596
Iteration 19/1000 | Loss: 0.00001993
Iteration 20/1000 | Loss: 0.00001838
Iteration 21/1000 | Loss: 0.00002771
Iteration 22/1000 | Loss: 0.00004953
Iteration 23/1000 | Loss: 0.00001948
Iteration 24/1000 | Loss: 0.00005796
Iteration 25/1000 | Loss: 0.00003907
Iteration 26/1000 | Loss: 0.00001635
Iteration 27/1000 | Loss: 0.00001405
Iteration 28/1000 | Loss: 0.00001340
Iteration 29/1000 | Loss: 0.00001277
Iteration 30/1000 | Loss: 0.00001251
Iteration 31/1000 | Loss: 0.00001240
Iteration 32/1000 | Loss: 0.00004410
Iteration 33/1000 | Loss: 0.00001230
Iteration 34/1000 | Loss: 0.00001219
Iteration 35/1000 | Loss: 0.00001217
Iteration 36/1000 | Loss: 0.00001215
Iteration 37/1000 | Loss: 0.00001213
Iteration 38/1000 | Loss: 0.00001212
Iteration 39/1000 | Loss: 0.00001211
Iteration 40/1000 | Loss: 0.00001209
Iteration 41/1000 | Loss: 0.00001199
Iteration 42/1000 | Loss: 0.00001194
Iteration 43/1000 | Loss: 0.00001194
Iteration 44/1000 | Loss: 0.00001192
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001187
Iteration 47/1000 | Loss: 0.00001187
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001186
Iteration 50/1000 | Loss: 0.00001184
Iteration 51/1000 | Loss: 0.00001184
Iteration 52/1000 | Loss: 0.00001184
Iteration 53/1000 | Loss: 0.00001184
Iteration 54/1000 | Loss: 0.00001183
Iteration 55/1000 | Loss: 0.00001183
Iteration 56/1000 | Loss: 0.00001183
Iteration 57/1000 | Loss: 0.00001182
Iteration 58/1000 | Loss: 0.00001180
Iteration 59/1000 | Loss: 0.00001180
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001180
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001179
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001179
Iteration 71/1000 | Loss: 0.00001179
Iteration 72/1000 | Loss: 0.00001179
Iteration 73/1000 | Loss: 0.00001179
Iteration 74/1000 | Loss: 0.00001179
Iteration 75/1000 | Loss: 0.00001179
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001178
Iteration 78/1000 | Loss: 0.00001177
Iteration 79/1000 | Loss: 0.00001177
Iteration 80/1000 | Loss: 0.00001176
Iteration 81/1000 | Loss: 0.00001176
Iteration 82/1000 | Loss: 0.00001176
Iteration 83/1000 | Loss: 0.00001175
Iteration 84/1000 | Loss: 0.00001175
Iteration 85/1000 | Loss: 0.00001174
Iteration 86/1000 | Loss: 0.00001174
Iteration 87/1000 | Loss: 0.00001174
Iteration 88/1000 | Loss: 0.00001174
Iteration 89/1000 | Loss: 0.00001174
Iteration 90/1000 | Loss: 0.00001173
Iteration 91/1000 | Loss: 0.00001173
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001172
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001170
Iteration 98/1000 | Loss: 0.00001170
Iteration 99/1000 | Loss: 0.00001170
Iteration 100/1000 | Loss: 0.00001170
Iteration 101/1000 | Loss: 0.00001170
Iteration 102/1000 | Loss: 0.00001170
Iteration 103/1000 | Loss: 0.00001170
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001169
Iteration 106/1000 | Loss: 0.00001169
Iteration 107/1000 | Loss: 0.00001169
Iteration 108/1000 | Loss: 0.00001168
Iteration 109/1000 | Loss: 0.00001168
Iteration 110/1000 | Loss: 0.00001168
Iteration 111/1000 | Loss: 0.00001168
Iteration 112/1000 | Loss: 0.00001167
Iteration 113/1000 | Loss: 0.00001167
Iteration 114/1000 | Loss: 0.00001167
Iteration 115/1000 | Loss: 0.00001166
Iteration 116/1000 | Loss: 0.00001166
Iteration 117/1000 | Loss: 0.00001165
Iteration 118/1000 | Loss: 0.00001165
Iteration 119/1000 | Loss: 0.00001165
Iteration 120/1000 | Loss: 0.00001165
Iteration 121/1000 | Loss: 0.00001165
Iteration 122/1000 | Loss: 0.00001165
Iteration 123/1000 | Loss: 0.00001164
Iteration 124/1000 | Loss: 0.00001164
Iteration 125/1000 | Loss: 0.00001164
Iteration 126/1000 | Loss: 0.00001164
Iteration 127/1000 | Loss: 0.00001164
Iteration 128/1000 | Loss: 0.00001163
Iteration 129/1000 | Loss: 0.00001163
Iteration 130/1000 | Loss: 0.00001162
Iteration 131/1000 | Loss: 0.00001162
Iteration 132/1000 | Loss: 0.00001161
Iteration 133/1000 | Loss: 0.00001161
Iteration 134/1000 | Loss: 0.00001161
Iteration 135/1000 | Loss: 0.00001161
Iteration 136/1000 | Loss: 0.00001160
Iteration 137/1000 | Loss: 0.00001160
Iteration 138/1000 | Loss: 0.00001160
Iteration 139/1000 | Loss: 0.00001159
Iteration 140/1000 | Loss: 0.00001158
Iteration 141/1000 | Loss: 0.00001156
Iteration 142/1000 | Loss: 0.00001156
Iteration 143/1000 | Loss: 0.00001156
Iteration 144/1000 | Loss: 0.00001156
Iteration 145/1000 | Loss: 0.00001156
Iteration 146/1000 | Loss: 0.00001156
Iteration 147/1000 | Loss: 0.00001156
Iteration 148/1000 | Loss: 0.00001156
Iteration 149/1000 | Loss: 0.00001156
Iteration 150/1000 | Loss: 0.00001156
Iteration 151/1000 | Loss: 0.00001155
Iteration 152/1000 | Loss: 0.00001155
Iteration 153/1000 | Loss: 0.00001155
Iteration 154/1000 | Loss: 0.00001155
Iteration 155/1000 | Loss: 0.00001154
Iteration 156/1000 | Loss: 0.00001154
Iteration 157/1000 | Loss: 0.00001154
Iteration 158/1000 | Loss: 0.00001154
Iteration 159/1000 | Loss: 0.00001153
Iteration 160/1000 | Loss: 0.00001153
Iteration 161/1000 | Loss: 0.00001153
Iteration 162/1000 | Loss: 0.00001153
Iteration 163/1000 | Loss: 0.00001152
Iteration 164/1000 | Loss: 0.00001152
Iteration 165/1000 | Loss: 0.00001152
Iteration 166/1000 | Loss: 0.00001152
Iteration 167/1000 | Loss: 0.00001152
Iteration 168/1000 | Loss: 0.00001152
Iteration 169/1000 | Loss: 0.00001151
Iteration 170/1000 | Loss: 0.00001151
Iteration 171/1000 | Loss: 0.00001151
Iteration 172/1000 | Loss: 0.00001150
Iteration 173/1000 | Loss: 0.00001150
Iteration 174/1000 | Loss: 0.00001150
Iteration 175/1000 | Loss: 0.00001150
Iteration 176/1000 | Loss: 0.00001150
Iteration 177/1000 | Loss: 0.00001150
Iteration 178/1000 | Loss: 0.00001150
Iteration 179/1000 | Loss: 0.00001149
Iteration 180/1000 | Loss: 0.00001149
Iteration 181/1000 | Loss: 0.00001149
Iteration 182/1000 | Loss: 0.00001149
Iteration 183/1000 | Loss: 0.00001149
Iteration 184/1000 | Loss: 0.00001149
Iteration 185/1000 | Loss: 0.00001149
Iteration 186/1000 | Loss: 0.00001148
Iteration 187/1000 | Loss: 0.00001148
Iteration 188/1000 | Loss: 0.00001148
Iteration 189/1000 | Loss: 0.00001148
Iteration 190/1000 | Loss: 0.00001148
Iteration 191/1000 | Loss: 0.00001148
Iteration 192/1000 | Loss: 0.00001147
Iteration 193/1000 | Loss: 0.00001147
Iteration 194/1000 | Loss: 0.00001147
Iteration 195/1000 | Loss: 0.00001147
Iteration 196/1000 | Loss: 0.00001147
Iteration 197/1000 | Loss: 0.00001147
Iteration 198/1000 | Loss: 0.00001147
Iteration 199/1000 | Loss: 0.00001147
Iteration 200/1000 | Loss: 0.00001147
Iteration 201/1000 | Loss: 0.00001146
Iteration 202/1000 | Loss: 0.00001146
Iteration 203/1000 | Loss: 0.00001146
Iteration 204/1000 | Loss: 0.00001146
Iteration 205/1000 | Loss: 0.00001146
Iteration 206/1000 | Loss: 0.00001146
Iteration 207/1000 | Loss: 0.00001146
Iteration 208/1000 | Loss: 0.00001146
Iteration 209/1000 | Loss: 0.00001146
Iteration 210/1000 | Loss: 0.00001146
Iteration 211/1000 | Loss: 0.00001145
Iteration 212/1000 | Loss: 0.00001145
Iteration 213/1000 | Loss: 0.00001145
Iteration 214/1000 | Loss: 0.00001145
Iteration 215/1000 | Loss: 0.00001145
Iteration 216/1000 | Loss: 0.00001145
Iteration 217/1000 | Loss: 0.00001145
Iteration 218/1000 | Loss: 0.00001145
Iteration 219/1000 | Loss: 0.00002960
Iteration 220/1000 | Loss: 0.00001235
Iteration 221/1000 | Loss: 0.00001145
Iteration 222/1000 | Loss: 0.00001145
Iteration 223/1000 | Loss: 0.00001145
Iteration 224/1000 | Loss: 0.00001145
Iteration 225/1000 | Loss: 0.00001144
Iteration 226/1000 | Loss: 0.00001144
Iteration 227/1000 | Loss: 0.00001144
Iteration 228/1000 | Loss: 0.00001144
Iteration 229/1000 | Loss: 0.00001144
Iteration 230/1000 | Loss: 0.00001144
Iteration 231/1000 | Loss: 0.00001144
Iteration 232/1000 | Loss: 0.00001144
Iteration 233/1000 | Loss: 0.00001144
Iteration 234/1000 | Loss: 0.00001144
Iteration 235/1000 | Loss: 0.00001144
Iteration 236/1000 | Loss: 0.00001144
Iteration 237/1000 | Loss: 0.00001144
Iteration 238/1000 | Loss: 0.00001144
Iteration 239/1000 | Loss: 0.00001144
Iteration 240/1000 | Loss: 0.00001144
Iteration 241/1000 | Loss: 0.00001144
Iteration 242/1000 | Loss: 0.00001143
Iteration 243/1000 | Loss: 0.00001143
Iteration 244/1000 | Loss: 0.00001143
Iteration 245/1000 | Loss: 0.00001143
Iteration 246/1000 | Loss: 0.00001143
Iteration 247/1000 | Loss: 0.00001143
Iteration 248/1000 | Loss: 0.00001143
Iteration 249/1000 | Loss: 0.00001143
Iteration 250/1000 | Loss: 0.00001143
Iteration 251/1000 | Loss: 0.00001143
Iteration 252/1000 | Loss: 0.00001143
Iteration 253/1000 | Loss: 0.00001143
Iteration 254/1000 | Loss: 0.00001143
Iteration 255/1000 | Loss: 0.00001143
Iteration 256/1000 | Loss: 0.00001142
Iteration 257/1000 | Loss: 0.00001142
Iteration 258/1000 | Loss: 0.00001142
Iteration 259/1000 | Loss: 0.00001142
Iteration 260/1000 | Loss: 0.00001142
Iteration 261/1000 | Loss: 0.00001142
Iteration 262/1000 | Loss: 0.00001142
Iteration 263/1000 | Loss: 0.00001142
Iteration 264/1000 | Loss: 0.00001142
Iteration 265/1000 | Loss: 0.00001142
Iteration 266/1000 | Loss: 0.00001142
Iteration 267/1000 | Loss: 0.00001142
Iteration 268/1000 | Loss: 0.00001142
Iteration 269/1000 | Loss: 0.00001142
Iteration 270/1000 | Loss: 0.00001142
Iteration 271/1000 | Loss: 0.00001142
Iteration 272/1000 | Loss: 0.00001142
Iteration 273/1000 | Loss: 0.00001142
Iteration 274/1000 | Loss: 0.00001142
Iteration 275/1000 | Loss: 0.00001142
Iteration 276/1000 | Loss: 0.00001142
Iteration 277/1000 | Loss: 0.00001142
Iteration 278/1000 | Loss: 0.00001142
Iteration 279/1000 | Loss: 0.00001142
Iteration 280/1000 | Loss: 0.00001142
Iteration 281/1000 | Loss: 0.00001142
Iteration 282/1000 | Loss: 0.00001142
Iteration 283/1000 | Loss: 0.00001142
Iteration 284/1000 | Loss: 0.00001142
Iteration 285/1000 | Loss: 0.00001142
Iteration 286/1000 | Loss: 0.00001142
Iteration 287/1000 | Loss: 0.00001142
Iteration 288/1000 | Loss: 0.00001142
Iteration 289/1000 | Loss: 0.00001142
Iteration 290/1000 | Loss: 0.00001142
Iteration 291/1000 | Loss: 0.00001142
Iteration 292/1000 | Loss: 0.00001142
Iteration 293/1000 | Loss: 0.00001142
Iteration 294/1000 | Loss: 0.00001142
Iteration 295/1000 | Loss: 0.00001142
Iteration 296/1000 | Loss: 0.00001142
Iteration 297/1000 | Loss: 0.00001142
Iteration 298/1000 | Loss: 0.00001142
Iteration 299/1000 | Loss: 0.00001142
Iteration 300/1000 | Loss: 0.00001142
Iteration 301/1000 | Loss: 0.00001142
Iteration 302/1000 | Loss: 0.00001142
Iteration 303/1000 | Loss: 0.00001142
Iteration 304/1000 | Loss: 0.00001142
Iteration 305/1000 | Loss: 0.00001142
Iteration 306/1000 | Loss: 0.00001142
Iteration 307/1000 | Loss: 0.00001142
Iteration 308/1000 | Loss: 0.00001142
Iteration 309/1000 | Loss: 0.00001142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 309. Stopping optimization.
Last 5 losses: [1.1417630048526917e-05, 1.1417630048526917e-05, 1.1417630048526917e-05, 1.1417630048526917e-05, 1.1417630048526917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1417630048526917e-05

Optimization complete. Final v2v error: 2.903402090072632 mm

Highest mean error: 3.6714212894439697 mm for frame 76

Lowest mean error: 2.694697856903076 mm for frame 172

Saving results

Total time: 120.11576843261719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00650659
Iteration 2/25 | Loss: 0.00129131
Iteration 3/25 | Loss: 0.00120710
Iteration 4/25 | Loss: 0.00119369
Iteration 5/25 | Loss: 0.00118948
Iteration 6/25 | Loss: 0.00118859
Iteration 7/25 | Loss: 0.00118859
Iteration 8/25 | Loss: 0.00118859
Iteration 9/25 | Loss: 0.00118859
Iteration 10/25 | Loss: 0.00118859
Iteration 11/25 | Loss: 0.00118859
Iteration 12/25 | Loss: 0.00118859
Iteration 13/25 | Loss: 0.00118859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011885875137522817, 0.0011885875137522817, 0.0011885875137522817, 0.0011885875137522817, 0.0011885875137522817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011885875137522817

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.07674408
Iteration 2/25 | Loss: 0.00149997
Iteration 3/25 | Loss: 0.00149997
Iteration 4/25 | Loss: 0.00149996
Iteration 5/25 | Loss: 0.00149996
Iteration 6/25 | Loss: 0.00149996
Iteration 7/25 | Loss: 0.00149996
Iteration 8/25 | Loss: 0.00149996
Iteration 9/25 | Loss: 0.00149996
Iteration 10/25 | Loss: 0.00149996
Iteration 11/25 | Loss: 0.00149996
Iteration 12/25 | Loss: 0.00149996
Iteration 13/25 | Loss: 0.00149996
Iteration 14/25 | Loss: 0.00149996
Iteration 15/25 | Loss: 0.00149996
Iteration 16/25 | Loss: 0.00149996
Iteration 17/25 | Loss: 0.00149996
Iteration 18/25 | Loss: 0.00149996
Iteration 19/25 | Loss: 0.00149996
Iteration 20/25 | Loss: 0.00149996
Iteration 21/25 | Loss: 0.00149996
Iteration 22/25 | Loss: 0.00149996
Iteration 23/25 | Loss: 0.00149996
Iteration 24/25 | Loss: 0.00149996
Iteration 25/25 | Loss: 0.00149996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149996
Iteration 2/1000 | Loss: 0.00001885
Iteration 3/1000 | Loss: 0.00001351
Iteration 4/1000 | Loss: 0.00001231
Iteration 5/1000 | Loss: 0.00001167
Iteration 6/1000 | Loss: 0.00001113
Iteration 7/1000 | Loss: 0.00001074
Iteration 8/1000 | Loss: 0.00001050
Iteration 9/1000 | Loss: 0.00001017
Iteration 10/1000 | Loss: 0.00001002
Iteration 11/1000 | Loss: 0.00000994
Iteration 12/1000 | Loss: 0.00000981
Iteration 13/1000 | Loss: 0.00000970
Iteration 14/1000 | Loss: 0.00000969
Iteration 15/1000 | Loss: 0.00000967
Iteration 16/1000 | Loss: 0.00000966
Iteration 17/1000 | Loss: 0.00000955
Iteration 18/1000 | Loss: 0.00000953
Iteration 19/1000 | Loss: 0.00000950
Iteration 20/1000 | Loss: 0.00000949
Iteration 21/1000 | Loss: 0.00000948
Iteration 22/1000 | Loss: 0.00000947
Iteration 23/1000 | Loss: 0.00000946
Iteration 24/1000 | Loss: 0.00000945
Iteration 25/1000 | Loss: 0.00000945
Iteration 26/1000 | Loss: 0.00000943
Iteration 27/1000 | Loss: 0.00000943
Iteration 28/1000 | Loss: 0.00000943
Iteration 29/1000 | Loss: 0.00000942
Iteration 30/1000 | Loss: 0.00000942
Iteration 31/1000 | Loss: 0.00000939
Iteration 32/1000 | Loss: 0.00000938
Iteration 33/1000 | Loss: 0.00000938
Iteration 34/1000 | Loss: 0.00000937
Iteration 35/1000 | Loss: 0.00000937
Iteration 36/1000 | Loss: 0.00000937
Iteration 37/1000 | Loss: 0.00000937
Iteration 38/1000 | Loss: 0.00000936
Iteration 39/1000 | Loss: 0.00000936
Iteration 40/1000 | Loss: 0.00000935
Iteration 41/1000 | Loss: 0.00000935
Iteration 42/1000 | Loss: 0.00000934
Iteration 43/1000 | Loss: 0.00000934
Iteration 44/1000 | Loss: 0.00000933
Iteration 45/1000 | Loss: 0.00000933
Iteration 46/1000 | Loss: 0.00000932
Iteration 47/1000 | Loss: 0.00000932
Iteration 48/1000 | Loss: 0.00000932
Iteration 49/1000 | Loss: 0.00000931
Iteration 50/1000 | Loss: 0.00000930
Iteration 51/1000 | Loss: 0.00000930
Iteration 52/1000 | Loss: 0.00000929
Iteration 53/1000 | Loss: 0.00000929
Iteration 54/1000 | Loss: 0.00000928
Iteration 55/1000 | Loss: 0.00000928
Iteration 56/1000 | Loss: 0.00000927
Iteration 57/1000 | Loss: 0.00000927
Iteration 58/1000 | Loss: 0.00000927
Iteration 59/1000 | Loss: 0.00000926
Iteration 60/1000 | Loss: 0.00000926
Iteration 61/1000 | Loss: 0.00000926
Iteration 62/1000 | Loss: 0.00000926
Iteration 63/1000 | Loss: 0.00000926
Iteration 64/1000 | Loss: 0.00000926
Iteration 65/1000 | Loss: 0.00000925
Iteration 66/1000 | Loss: 0.00000925
Iteration 67/1000 | Loss: 0.00000925
Iteration 68/1000 | Loss: 0.00000925
Iteration 69/1000 | Loss: 0.00000925
Iteration 70/1000 | Loss: 0.00000925
Iteration 71/1000 | Loss: 0.00000925
Iteration 72/1000 | Loss: 0.00000925
Iteration 73/1000 | Loss: 0.00000924
Iteration 74/1000 | Loss: 0.00000924
Iteration 75/1000 | Loss: 0.00000924
Iteration 76/1000 | Loss: 0.00000924
Iteration 77/1000 | Loss: 0.00000924
Iteration 78/1000 | Loss: 0.00000924
Iteration 79/1000 | Loss: 0.00000923
Iteration 80/1000 | Loss: 0.00000923
Iteration 81/1000 | Loss: 0.00000923
Iteration 82/1000 | Loss: 0.00000923
Iteration 83/1000 | Loss: 0.00000923
Iteration 84/1000 | Loss: 0.00000923
Iteration 85/1000 | Loss: 0.00000923
Iteration 86/1000 | Loss: 0.00000923
Iteration 87/1000 | Loss: 0.00000923
Iteration 88/1000 | Loss: 0.00000923
Iteration 89/1000 | Loss: 0.00000922
Iteration 90/1000 | Loss: 0.00000922
Iteration 91/1000 | Loss: 0.00000922
Iteration 92/1000 | Loss: 0.00000922
Iteration 93/1000 | Loss: 0.00000921
Iteration 94/1000 | Loss: 0.00000921
Iteration 95/1000 | Loss: 0.00000920
Iteration 96/1000 | Loss: 0.00000920
Iteration 97/1000 | Loss: 0.00000920
Iteration 98/1000 | Loss: 0.00000920
Iteration 99/1000 | Loss: 0.00000919
Iteration 100/1000 | Loss: 0.00000919
Iteration 101/1000 | Loss: 0.00000919
Iteration 102/1000 | Loss: 0.00000919
Iteration 103/1000 | Loss: 0.00000919
Iteration 104/1000 | Loss: 0.00000919
Iteration 105/1000 | Loss: 0.00000919
Iteration 106/1000 | Loss: 0.00000919
Iteration 107/1000 | Loss: 0.00000919
Iteration 108/1000 | Loss: 0.00000919
Iteration 109/1000 | Loss: 0.00000918
Iteration 110/1000 | Loss: 0.00000918
Iteration 111/1000 | Loss: 0.00000918
Iteration 112/1000 | Loss: 0.00000917
Iteration 113/1000 | Loss: 0.00000917
Iteration 114/1000 | Loss: 0.00000917
Iteration 115/1000 | Loss: 0.00000916
Iteration 116/1000 | Loss: 0.00000916
Iteration 117/1000 | Loss: 0.00000916
Iteration 118/1000 | Loss: 0.00000916
Iteration 119/1000 | Loss: 0.00000916
Iteration 120/1000 | Loss: 0.00000916
Iteration 121/1000 | Loss: 0.00000916
Iteration 122/1000 | Loss: 0.00000916
Iteration 123/1000 | Loss: 0.00000915
Iteration 124/1000 | Loss: 0.00000915
Iteration 125/1000 | Loss: 0.00000915
Iteration 126/1000 | Loss: 0.00000915
Iteration 127/1000 | Loss: 0.00000914
Iteration 128/1000 | Loss: 0.00000914
Iteration 129/1000 | Loss: 0.00000914
Iteration 130/1000 | Loss: 0.00000914
Iteration 131/1000 | Loss: 0.00000914
Iteration 132/1000 | Loss: 0.00000914
Iteration 133/1000 | Loss: 0.00000914
Iteration 134/1000 | Loss: 0.00000914
Iteration 135/1000 | Loss: 0.00000914
Iteration 136/1000 | Loss: 0.00000913
Iteration 137/1000 | Loss: 0.00000913
Iteration 138/1000 | Loss: 0.00000913
Iteration 139/1000 | Loss: 0.00000913
Iteration 140/1000 | Loss: 0.00000913
Iteration 141/1000 | Loss: 0.00000912
Iteration 142/1000 | Loss: 0.00000912
Iteration 143/1000 | Loss: 0.00000912
Iteration 144/1000 | Loss: 0.00000912
Iteration 145/1000 | Loss: 0.00000912
Iteration 146/1000 | Loss: 0.00000911
Iteration 147/1000 | Loss: 0.00000911
Iteration 148/1000 | Loss: 0.00000911
Iteration 149/1000 | Loss: 0.00000911
Iteration 150/1000 | Loss: 0.00000911
Iteration 151/1000 | Loss: 0.00000911
Iteration 152/1000 | Loss: 0.00000911
Iteration 153/1000 | Loss: 0.00000911
Iteration 154/1000 | Loss: 0.00000911
Iteration 155/1000 | Loss: 0.00000911
Iteration 156/1000 | Loss: 0.00000911
Iteration 157/1000 | Loss: 0.00000911
Iteration 158/1000 | Loss: 0.00000911
Iteration 159/1000 | Loss: 0.00000911
Iteration 160/1000 | Loss: 0.00000911
Iteration 161/1000 | Loss: 0.00000911
Iteration 162/1000 | Loss: 0.00000911
Iteration 163/1000 | Loss: 0.00000911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [9.113082342082635e-06, 9.113082342082635e-06, 9.113082342082635e-06, 9.113082342082635e-06, 9.113082342082635e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.113082342082635e-06

Optimization complete. Final v2v error: 2.626390218734741 mm

Highest mean error: 2.9616947174072266 mm for frame 78

Lowest mean error: 2.4705381393432617 mm for frame 108

Saving results

Total time: 39.84055852890015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455635
Iteration 2/25 | Loss: 0.00136270
Iteration 3/25 | Loss: 0.00125736
Iteration 4/25 | Loss: 0.00125019
Iteration 5/25 | Loss: 0.00124790
Iteration 6/25 | Loss: 0.00124731
Iteration 7/25 | Loss: 0.00124716
Iteration 8/25 | Loss: 0.00124716
Iteration 9/25 | Loss: 0.00124716
Iteration 10/25 | Loss: 0.00124716
Iteration 11/25 | Loss: 0.00124716
Iteration 12/25 | Loss: 0.00124716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012471622321754694, 0.0012471622321754694, 0.0012471622321754694, 0.0012471622321754694, 0.0012471622321754694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012471622321754694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42228365
Iteration 2/25 | Loss: 0.00119441
Iteration 3/25 | Loss: 0.00119441
Iteration 4/25 | Loss: 0.00119440
Iteration 5/25 | Loss: 0.00119440
Iteration 6/25 | Loss: 0.00119440
Iteration 7/25 | Loss: 0.00119440
Iteration 8/25 | Loss: 0.00119440
Iteration 9/25 | Loss: 0.00119440
Iteration 10/25 | Loss: 0.00119440
Iteration 11/25 | Loss: 0.00119440
Iteration 12/25 | Loss: 0.00119440
Iteration 13/25 | Loss: 0.00119440
Iteration 14/25 | Loss: 0.00119440
Iteration 15/25 | Loss: 0.00119440
Iteration 16/25 | Loss: 0.00119440
Iteration 17/25 | Loss: 0.00119440
Iteration 18/25 | Loss: 0.00119440
Iteration 19/25 | Loss: 0.00119440
Iteration 20/25 | Loss: 0.00119440
Iteration 21/25 | Loss: 0.00119440
Iteration 22/25 | Loss: 0.00119440
Iteration 23/25 | Loss: 0.00119440
Iteration 24/25 | Loss: 0.00119440
Iteration 25/25 | Loss: 0.00119440
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011944019934162498, 0.0011944019934162498, 0.0011944019934162498, 0.0011944019934162498, 0.0011944019934162498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011944019934162498

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119440
Iteration 2/1000 | Loss: 0.00003346
Iteration 3/1000 | Loss: 0.00002107
Iteration 4/1000 | Loss: 0.00001828
Iteration 5/1000 | Loss: 0.00001713
Iteration 6/1000 | Loss: 0.00001635
Iteration 7/1000 | Loss: 0.00001585
Iteration 8/1000 | Loss: 0.00001534
Iteration 9/1000 | Loss: 0.00001502
Iteration 10/1000 | Loss: 0.00001476
Iteration 11/1000 | Loss: 0.00001455
Iteration 12/1000 | Loss: 0.00001438
Iteration 13/1000 | Loss: 0.00001423
Iteration 14/1000 | Loss: 0.00001422
Iteration 15/1000 | Loss: 0.00001417
Iteration 16/1000 | Loss: 0.00001409
Iteration 17/1000 | Loss: 0.00001405
Iteration 18/1000 | Loss: 0.00001401
Iteration 19/1000 | Loss: 0.00001400
Iteration 20/1000 | Loss: 0.00001396
Iteration 21/1000 | Loss: 0.00001392
Iteration 22/1000 | Loss: 0.00001392
Iteration 23/1000 | Loss: 0.00001389
Iteration 24/1000 | Loss: 0.00001388
Iteration 25/1000 | Loss: 0.00001387
Iteration 26/1000 | Loss: 0.00001387
Iteration 27/1000 | Loss: 0.00001386
Iteration 28/1000 | Loss: 0.00001385
Iteration 29/1000 | Loss: 0.00001384
Iteration 30/1000 | Loss: 0.00001384
Iteration 31/1000 | Loss: 0.00001383
Iteration 32/1000 | Loss: 0.00001382
Iteration 33/1000 | Loss: 0.00001382
Iteration 34/1000 | Loss: 0.00001382
Iteration 35/1000 | Loss: 0.00001382
Iteration 36/1000 | Loss: 0.00001381
Iteration 37/1000 | Loss: 0.00001381
Iteration 38/1000 | Loss: 0.00001381
Iteration 39/1000 | Loss: 0.00001380
Iteration 40/1000 | Loss: 0.00001380
Iteration 41/1000 | Loss: 0.00001378
Iteration 42/1000 | Loss: 0.00001377
Iteration 43/1000 | Loss: 0.00001377
Iteration 44/1000 | Loss: 0.00001376
Iteration 45/1000 | Loss: 0.00001376
Iteration 46/1000 | Loss: 0.00001372
Iteration 47/1000 | Loss: 0.00001372
Iteration 48/1000 | Loss: 0.00001371
Iteration 49/1000 | Loss: 0.00001371
Iteration 50/1000 | Loss: 0.00001370
Iteration 51/1000 | Loss: 0.00001369
Iteration 52/1000 | Loss: 0.00001369
Iteration 53/1000 | Loss: 0.00001369
Iteration 54/1000 | Loss: 0.00001369
Iteration 55/1000 | Loss: 0.00001369
Iteration 56/1000 | Loss: 0.00001368
Iteration 57/1000 | Loss: 0.00001368
Iteration 58/1000 | Loss: 0.00001367
Iteration 59/1000 | Loss: 0.00001367
Iteration 60/1000 | Loss: 0.00001367
Iteration 61/1000 | Loss: 0.00001366
Iteration 62/1000 | Loss: 0.00001366
Iteration 63/1000 | Loss: 0.00001365
Iteration 64/1000 | Loss: 0.00001364
Iteration 65/1000 | Loss: 0.00001364
Iteration 66/1000 | Loss: 0.00001364
Iteration 67/1000 | Loss: 0.00001364
Iteration 68/1000 | Loss: 0.00001364
Iteration 69/1000 | Loss: 0.00001363
Iteration 70/1000 | Loss: 0.00001363
Iteration 71/1000 | Loss: 0.00001363
Iteration 72/1000 | Loss: 0.00001363
Iteration 73/1000 | Loss: 0.00001363
Iteration 74/1000 | Loss: 0.00001363
Iteration 75/1000 | Loss: 0.00001363
Iteration 76/1000 | Loss: 0.00001363
Iteration 77/1000 | Loss: 0.00001363
Iteration 78/1000 | Loss: 0.00001363
Iteration 79/1000 | Loss: 0.00001363
Iteration 80/1000 | Loss: 0.00001363
Iteration 81/1000 | Loss: 0.00001362
Iteration 82/1000 | Loss: 0.00001362
Iteration 83/1000 | Loss: 0.00001362
Iteration 84/1000 | Loss: 0.00001362
Iteration 85/1000 | Loss: 0.00001361
Iteration 86/1000 | Loss: 0.00001360
Iteration 87/1000 | Loss: 0.00001360
Iteration 88/1000 | Loss: 0.00001360
Iteration 89/1000 | Loss: 0.00001360
Iteration 90/1000 | Loss: 0.00001360
Iteration 91/1000 | Loss: 0.00001360
Iteration 92/1000 | Loss: 0.00001360
Iteration 93/1000 | Loss: 0.00001359
Iteration 94/1000 | Loss: 0.00001359
Iteration 95/1000 | Loss: 0.00001359
Iteration 96/1000 | Loss: 0.00001359
Iteration 97/1000 | Loss: 0.00001359
Iteration 98/1000 | Loss: 0.00001358
Iteration 99/1000 | Loss: 0.00001358
Iteration 100/1000 | Loss: 0.00001358
Iteration 101/1000 | Loss: 0.00001358
Iteration 102/1000 | Loss: 0.00001358
Iteration 103/1000 | Loss: 0.00001357
Iteration 104/1000 | Loss: 0.00001357
Iteration 105/1000 | Loss: 0.00001357
Iteration 106/1000 | Loss: 0.00001357
Iteration 107/1000 | Loss: 0.00001356
Iteration 108/1000 | Loss: 0.00001356
Iteration 109/1000 | Loss: 0.00001356
Iteration 110/1000 | Loss: 0.00001356
Iteration 111/1000 | Loss: 0.00001356
Iteration 112/1000 | Loss: 0.00001355
Iteration 113/1000 | Loss: 0.00001355
Iteration 114/1000 | Loss: 0.00001355
Iteration 115/1000 | Loss: 0.00001355
Iteration 116/1000 | Loss: 0.00001355
Iteration 117/1000 | Loss: 0.00001355
Iteration 118/1000 | Loss: 0.00001355
Iteration 119/1000 | Loss: 0.00001355
Iteration 120/1000 | Loss: 0.00001355
Iteration 121/1000 | Loss: 0.00001354
Iteration 122/1000 | Loss: 0.00001354
Iteration 123/1000 | Loss: 0.00001354
Iteration 124/1000 | Loss: 0.00001354
Iteration 125/1000 | Loss: 0.00001353
Iteration 126/1000 | Loss: 0.00001353
Iteration 127/1000 | Loss: 0.00001353
Iteration 128/1000 | Loss: 0.00001353
Iteration 129/1000 | Loss: 0.00001353
Iteration 130/1000 | Loss: 0.00001353
Iteration 131/1000 | Loss: 0.00001353
Iteration 132/1000 | Loss: 0.00001353
Iteration 133/1000 | Loss: 0.00001353
Iteration 134/1000 | Loss: 0.00001352
Iteration 135/1000 | Loss: 0.00001352
Iteration 136/1000 | Loss: 0.00001352
Iteration 137/1000 | Loss: 0.00001352
Iteration 138/1000 | Loss: 0.00001352
Iteration 139/1000 | Loss: 0.00001352
Iteration 140/1000 | Loss: 0.00001351
Iteration 141/1000 | Loss: 0.00001351
Iteration 142/1000 | Loss: 0.00001351
Iteration 143/1000 | Loss: 0.00001351
Iteration 144/1000 | Loss: 0.00001351
Iteration 145/1000 | Loss: 0.00001351
Iteration 146/1000 | Loss: 0.00001351
Iteration 147/1000 | Loss: 0.00001351
Iteration 148/1000 | Loss: 0.00001351
Iteration 149/1000 | Loss: 0.00001351
Iteration 150/1000 | Loss: 0.00001350
Iteration 151/1000 | Loss: 0.00001350
Iteration 152/1000 | Loss: 0.00001350
Iteration 153/1000 | Loss: 0.00001350
Iteration 154/1000 | Loss: 0.00001350
Iteration 155/1000 | Loss: 0.00001350
Iteration 156/1000 | Loss: 0.00001350
Iteration 157/1000 | Loss: 0.00001350
Iteration 158/1000 | Loss: 0.00001350
Iteration 159/1000 | Loss: 0.00001350
Iteration 160/1000 | Loss: 0.00001350
Iteration 161/1000 | Loss: 0.00001350
Iteration 162/1000 | Loss: 0.00001349
Iteration 163/1000 | Loss: 0.00001349
Iteration 164/1000 | Loss: 0.00001349
Iteration 165/1000 | Loss: 0.00001349
Iteration 166/1000 | Loss: 0.00001349
Iteration 167/1000 | Loss: 0.00001349
Iteration 168/1000 | Loss: 0.00001349
Iteration 169/1000 | Loss: 0.00001349
Iteration 170/1000 | Loss: 0.00001349
Iteration 171/1000 | Loss: 0.00001349
Iteration 172/1000 | Loss: 0.00001349
Iteration 173/1000 | Loss: 0.00001349
Iteration 174/1000 | Loss: 0.00001349
Iteration 175/1000 | Loss: 0.00001349
Iteration 176/1000 | Loss: 0.00001348
Iteration 177/1000 | Loss: 0.00001348
Iteration 178/1000 | Loss: 0.00001348
Iteration 179/1000 | Loss: 0.00001348
Iteration 180/1000 | Loss: 0.00001348
Iteration 181/1000 | Loss: 0.00001348
Iteration 182/1000 | Loss: 0.00001348
Iteration 183/1000 | Loss: 0.00001348
Iteration 184/1000 | Loss: 0.00001348
Iteration 185/1000 | Loss: 0.00001348
Iteration 186/1000 | Loss: 0.00001348
Iteration 187/1000 | Loss: 0.00001348
Iteration 188/1000 | Loss: 0.00001348
Iteration 189/1000 | Loss: 0.00001348
Iteration 190/1000 | Loss: 0.00001348
Iteration 191/1000 | Loss: 0.00001348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.348485420749057e-05, 1.348485420749057e-05, 1.348485420749057e-05, 1.348485420749057e-05, 1.348485420749057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.348485420749057e-05

Optimization complete. Final v2v error: 3.080965518951416 mm

Highest mean error: 4.4341206550598145 mm for frame 67

Lowest mean error: 2.5854861736297607 mm for frame 3

Saving results

Total time: 44.95302414894104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478210
Iteration 2/25 | Loss: 0.00130082
Iteration 3/25 | Loss: 0.00121843
Iteration 4/25 | Loss: 0.00120840
Iteration 5/25 | Loss: 0.00120627
Iteration 6/25 | Loss: 0.00120627
Iteration 7/25 | Loss: 0.00120627
Iteration 8/25 | Loss: 0.00120627
Iteration 9/25 | Loss: 0.00120627
Iteration 10/25 | Loss: 0.00120627
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012062652967870235, 0.0012062652967870235, 0.0012062652967870235, 0.0012062652967870235, 0.0012062652967870235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012062652967870235

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.65514994
Iteration 2/25 | Loss: 0.00133549
Iteration 3/25 | Loss: 0.00133548
Iteration 4/25 | Loss: 0.00133548
Iteration 5/25 | Loss: 0.00133548
Iteration 6/25 | Loss: 0.00133548
Iteration 7/25 | Loss: 0.00133548
Iteration 8/25 | Loss: 0.00133548
Iteration 9/25 | Loss: 0.00133548
Iteration 10/25 | Loss: 0.00133548
Iteration 11/25 | Loss: 0.00133548
Iteration 12/25 | Loss: 0.00133548
Iteration 13/25 | Loss: 0.00133548
Iteration 14/25 | Loss: 0.00133548
Iteration 15/25 | Loss: 0.00133548
Iteration 16/25 | Loss: 0.00133548
Iteration 17/25 | Loss: 0.00133548
Iteration 18/25 | Loss: 0.00133548
Iteration 19/25 | Loss: 0.00133548
Iteration 20/25 | Loss: 0.00133548
Iteration 21/25 | Loss: 0.00133548
Iteration 22/25 | Loss: 0.00133548
Iteration 23/25 | Loss: 0.00133548
Iteration 24/25 | Loss: 0.00133548
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013354782713577151, 0.0013354782713577151, 0.0013354782713577151, 0.0013354782713577151, 0.0013354782713577151]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013354782713577151

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133548
Iteration 2/1000 | Loss: 0.00002043
Iteration 3/1000 | Loss: 0.00001646
Iteration 4/1000 | Loss: 0.00001534
Iteration 5/1000 | Loss: 0.00001447
Iteration 6/1000 | Loss: 0.00001403
Iteration 7/1000 | Loss: 0.00001367
Iteration 8/1000 | Loss: 0.00001326
Iteration 9/1000 | Loss: 0.00001285
Iteration 10/1000 | Loss: 0.00001281
Iteration 11/1000 | Loss: 0.00001270
Iteration 12/1000 | Loss: 0.00001252
Iteration 13/1000 | Loss: 0.00001242
Iteration 14/1000 | Loss: 0.00001239
Iteration 15/1000 | Loss: 0.00001236
Iteration 16/1000 | Loss: 0.00001229
Iteration 17/1000 | Loss: 0.00001228
Iteration 18/1000 | Loss: 0.00001227
Iteration 19/1000 | Loss: 0.00001225
Iteration 20/1000 | Loss: 0.00001225
Iteration 21/1000 | Loss: 0.00001223
Iteration 22/1000 | Loss: 0.00001216
Iteration 23/1000 | Loss: 0.00001212
Iteration 24/1000 | Loss: 0.00001210
Iteration 25/1000 | Loss: 0.00001210
Iteration 26/1000 | Loss: 0.00001209
Iteration 27/1000 | Loss: 0.00001209
Iteration 28/1000 | Loss: 0.00001206
Iteration 29/1000 | Loss: 0.00001204
Iteration 30/1000 | Loss: 0.00001203
Iteration 31/1000 | Loss: 0.00001203
Iteration 32/1000 | Loss: 0.00001203
Iteration 33/1000 | Loss: 0.00001202
Iteration 34/1000 | Loss: 0.00001201
Iteration 35/1000 | Loss: 0.00001200
Iteration 36/1000 | Loss: 0.00001200
Iteration 37/1000 | Loss: 0.00001199
Iteration 38/1000 | Loss: 0.00001199
Iteration 39/1000 | Loss: 0.00001198
Iteration 40/1000 | Loss: 0.00001198
Iteration 41/1000 | Loss: 0.00001197
Iteration 42/1000 | Loss: 0.00001196
Iteration 43/1000 | Loss: 0.00001195
Iteration 44/1000 | Loss: 0.00001194
Iteration 45/1000 | Loss: 0.00001194
Iteration 46/1000 | Loss: 0.00001194
Iteration 47/1000 | Loss: 0.00001193
Iteration 48/1000 | Loss: 0.00001193
Iteration 49/1000 | Loss: 0.00001192
Iteration 50/1000 | Loss: 0.00001192
Iteration 51/1000 | Loss: 0.00001189
Iteration 52/1000 | Loss: 0.00001189
Iteration 53/1000 | Loss: 0.00001188
Iteration 54/1000 | Loss: 0.00001188
Iteration 55/1000 | Loss: 0.00001187
Iteration 56/1000 | Loss: 0.00001187
Iteration 57/1000 | Loss: 0.00001185
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001184
Iteration 60/1000 | Loss: 0.00001184
Iteration 61/1000 | Loss: 0.00001184
Iteration 62/1000 | Loss: 0.00001184
Iteration 63/1000 | Loss: 0.00001183
Iteration 64/1000 | Loss: 0.00001182
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001179
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001176
Iteration 72/1000 | Loss: 0.00001176
Iteration 73/1000 | Loss: 0.00001175
Iteration 74/1000 | Loss: 0.00001175
Iteration 75/1000 | Loss: 0.00001175
Iteration 76/1000 | Loss: 0.00001175
Iteration 77/1000 | Loss: 0.00001173
Iteration 78/1000 | Loss: 0.00001173
Iteration 79/1000 | Loss: 0.00001171
Iteration 80/1000 | Loss: 0.00001171
Iteration 81/1000 | Loss: 0.00001171
Iteration 82/1000 | Loss: 0.00001171
Iteration 83/1000 | Loss: 0.00001170
Iteration 84/1000 | Loss: 0.00001170
Iteration 85/1000 | Loss: 0.00001170
Iteration 86/1000 | Loss: 0.00001169
Iteration 87/1000 | Loss: 0.00001169
Iteration 88/1000 | Loss: 0.00001169
Iteration 89/1000 | Loss: 0.00001168
Iteration 90/1000 | Loss: 0.00001168
Iteration 91/1000 | Loss: 0.00001167
Iteration 92/1000 | Loss: 0.00001167
Iteration 93/1000 | Loss: 0.00001167
Iteration 94/1000 | Loss: 0.00001167
Iteration 95/1000 | Loss: 0.00001167
Iteration 96/1000 | Loss: 0.00001167
Iteration 97/1000 | Loss: 0.00001166
Iteration 98/1000 | Loss: 0.00001166
Iteration 99/1000 | Loss: 0.00001165
Iteration 100/1000 | Loss: 0.00001165
Iteration 101/1000 | Loss: 0.00001165
Iteration 102/1000 | Loss: 0.00001165
Iteration 103/1000 | Loss: 0.00001164
Iteration 104/1000 | Loss: 0.00001164
Iteration 105/1000 | Loss: 0.00001164
Iteration 106/1000 | Loss: 0.00001164
Iteration 107/1000 | Loss: 0.00001164
Iteration 108/1000 | Loss: 0.00001163
Iteration 109/1000 | Loss: 0.00001163
Iteration 110/1000 | Loss: 0.00001163
Iteration 111/1000 | Loss: 0.00001163
Iteration 112/1000 | Loss: 0.00001163
Iteration 113/1000 | Loss: 0.00001163
Iteration 114/1000 | Loss: 0.00001163
Iteration 115/1000 | Loss: 0.00001163
Iteration 116/1000 | Loss: 0.00001163
Iteration 117/1000 | Loss: 0.00001163
Iteration 118/1000 | Loss: 0.00001163
Iteration 119/1000 | Loss: 0.00001163
Iteration 120/1000 | Loss: 0.00001163
Iteration 121/1000 | Loss: 0.00001163
Iteration 122/1000 | Loss: 0.00001163
Iteration 123/1000 | Loss: 0.00001163
Iteration 124/1000 | Loss: 0.00001163
Iteration 125/1000 | Loss: 0.00001163
Iteration 126/1000 | Loss: 0.00001163
Iteration 127/1000 | Loss: 0.00001163
Iteration 128/1000 | Loss: 0.00001163
Iteration 129/1000 | Loss: 0.00001163
Iteration 130/1000 | Loss: 0.00001163
Iteration 131/1000 | Loss: 0.00001163
Iteration 132/1000 | Loss: 0.00001163
Iteration 133/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.162660828413209e-05, 1.162660828413209e-05, 1.162660828413209e-05, 1.162660828413209e-05, 1.162660828413209e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.162660828413209e-05

Optimization complete. Final v2v error: 2.924312114715576 mm

Highest mean error: 3.3167665004730225 mm for frame 191

Lowest mean error: 2.667530059814453 mm for frame 215

Saving results

Total time: 42.05315971374512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955142
Iteration 2/25 | Loss: 0.00955141
Iteration 3/25 | Loss: 0.00439320
Iteration 4/25 | Loss: 0.00244974
Iteration 5/25 | Loss: 0.00203504
Iteration 6/25 | Loss: 0.00186242
Iteration 7/25 | Loss: 0.00173665
Iteration 8/25 | Loss: 0.00177417
Iteration 9/25 | Loss: 0.00170069
Iteration 10/25 | Loss: 0.00155338
Iteration 11/25 | Loss: 0.00149843
Iteration 12/25 | Loss: 0.00146808
Iteration 13/25 | Loss: 0.00145786
Iteration 14/25 | Loss: 0.00145881
Iteration 15/25 | Loss: 0.00142393
Iteration 16/25 | Loss: 0.00141539
Iteration 17/25 | Loss: 0.00142271
Iteration 18/25 | Loss: 0.00141644
Iteration 19/25 | Loss: 0.00139122
Iteration 20/25 | Loss: 0.00138200
Iteration 21/25 | Loss: 0.00137811
Iteration 22/25 | Loss: 0.00138029
Iteration 23/25 | Loss: 0.00137416
Iteration 24/25 | Loss: 0.00137278
Iteration 25/25 | Loss: 0.00136322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27652419
Iteration 2/25 | Loss: 0.00279279
Iteration 3/25 | Loss: 0.00270443
Iteration 4/25 | Loss: 0.00270442
Iteration 5/25 | Loss: 0.00270442
Iteration 6/25 | Loss: 0.00270442
Iteration 7/25 | Loss: 0.00270442
Iteration 8/25 | Loss: 0.00270442
Iteration 9/25 | Loss: 0.00270442
Iteration 10/25 | Loss: 0.00270442
Iteration 11/25 | Loss: 0.00270442
Iteration 12/25 | Loss: 0.00270442
Iteration 13/25 | Loss: 0.00270442
Iteration 14/25 | Loss: 0.00270442
Iteration 15/25 | Loss: 0.00270442
Iteration 16/25 | Loss: 0.00270442
Iteration 17/25 | Loss: 0.00270442
Iteration 18/25 | Loss: 0.00270442
Iteration 19/25 | Loss: 0.00270442
Iteration 20/25 | Loss: 0.00270442
Iteration 21/25 | Loss: 0.00270442
Iteration 22/25 | Loss: 0.00270442
Iteration 23/25 | Loss: 0.00270442
Iteration 24/25 | Loss: 0.00270442
Iteration 25/25 | Loss: 0.00270442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00270442
Iteration 2/1000 | Loss: 0.00083283
Iteration 3/1000 | Loss: 0.00213624
Iteration 4/1000 | Loss: 0.00026480
Iteration 5/1000 | Loss: 0.00056961
Iteration 6/1000 | Loss: 0.00014028
Iteration 7/1000 | Loss: 0.00026426
Iteration 8/1000 | Loss: 0.00022131
Iteration 9/1000 | Loss: 0.00049285
Iteration 10/1000 | Loss: 0.00012437
Iteration 11/1000 | Loss: 0.00015325
Iteration 12/1000 | Loss: 0.00017781
Iteration 13/1000 | Loss: 0.00011926
Iteration 14/1000 | Loss: 0.00006654
Iteration 15/1000 | Loss: 0.00010371
Iteration 16/1000 | Loss: 0.00006210
Iteration 17/1000 | Loss: 0.00013265
Iteration 18/1000 | Loss: 0.00088143
Iteration 19/1000 | Loss: 0.00080460
Iteration 20/1000 | Loss: 0.00544895
Iteration 21/1000 | Loss: 0.00703757
Iteration 22/1000 | Loss: 0.00540514
Iteration 23/1000 | Loss: 0.00417072
Iteration 24/1000 | Loss: 0.00428440
Iteration 25/1000 | Loss: 0.00203390
Iteration 26/1000 | Loss: 0.00121571
Iteration 27/1000 | Loss: 0.00083700
Iteration 28/1000 | Loss: 0.00238660
Iteration 29/1000 | Loss: 0.00051741
Iteration 30/1000 | Loss: 0.00146329
Iteration 31/1000 | Loss: 0.00009803
Iteration 32/1000 | Loss: 0.00060625
Iteration 33/1000 | Loss: 0.00007863
Iteration 34/1000 | Loss: 0.00055618
Iteration 35/1000 | Loss: 0.00222581
Iteration 36/1000 | Loss: 0.00142643
Iteration 37/1000 | Loss: 0.00047878
Iteration 38/1000 | Loss: 0.00028693
Iteration 39/1000 | Loss: 0.00041380
Iteration 40/1000 | Loss: 0.00027985
Iteration 41/1000 | Loss: 0.00024190
Iteration 42/1000 | Loss: 0.00084273
Iteration 43/1000 | Loss: 0.00024454
Iteration 44/1000 | Loss: 0.00019233
Iteration 45/1000 | Loss: 0.00028311
Iteration 46/1000 | Loss: 0.00034751
Iteration 47/1000 | Loss: 0.00050907
Iteration 48/1000 | Loss: 0.00182534
Iteration 49/1000 | Loss: 0.00022993
Iteration 50/1000 | Loss: 0.00024501
Iteration 51/1000 | Loss: 0.00006063
Iteration 52/1000 | Loss: 0.00006339
Iteration 53/1000 | Loss: 0.00006664
Iteration 54/1000 | Loss: 0.00003912
Iteration 55/1000 | Loss: 0.00004800
Iteration 56/1000 | Loss: 0.00009438
Iteration 57/1000 | Loss: 0.00029774
Iteration 58/1000 | Loss: 0.00020016
Iteration 59/1000 | Loss: 0.00031745
Iteration 60/1000 | Loss: 0.00026467
Iteration 61/1000 | Loss: 0.00010564
Iteration 62/1000 | Loss: 0.00003180
Iteration 63/1000 | Loss: 0.00007880
Iteration 64/1000 | Loss: 0.00007457
Iteration 65/1000 | Loss: 0.00003137
Iteration 66/1000 | Loss: 0.00005704
Iteration 67/1000 | Loss: 0.00002692
Iteration 68/1000 | Loss: 0.00002997
Iteration 69/1000 | Loss: 0.00005492
Iteration 70/1000 | Loss: 0.00002349
Iteration 71/1000 | Loss: 0.00003815
Iteration 72/1000 | Loss: 0.00027821
Iteration 73/1000 | Loss: 0.00002573
Iteration 74/1000 | Loss: 0.00006710
Iteration 75/1000 | Loss: 0.00002968
Iteration 76/1000 | Loss: 0.00008209
Iteration 77/1000 | Loss: 0.00002645
Iteration 78/1000 | Loss: 0.00008077
Iteration 79/1000 | Loss: 0.00069156
Iteration 80/1000 | Loss: 0.00016575
Iteration 81/1000 | Loss: 0.00003911
Iteration 82/1000 | Loss: 0.00010111
Iteration 83/1000 | Loss: 0.00002088
Iteration 84/1000 | Loss: 0.00022010
Iteration 85/1000 | Loss: 0.00005882
Iteration 86/1000 | Loss: 0.00007633
Iteration 87/1000 | Loss: 0.00004856
Iteration 88/1000 | Loss: 0.00002061
Iteration 89/1000 | Loss: 0.00005472
Iteration 90/1000 | Loss: 0.00001976
Iteration 91/1000 | Loss: 0.00002130
Iteration 92/1000 | Loss: 0.00005798
Iteration 93/1000 | Loss: 0.00002071
Iteration 94/1000 | Loss: 0.00002416
Iteration 95/1000 | Loss: 0.00001695
Iteration 96/1000 | Loss: 0.00003896
Iteration 97/1000 | Loss: 0.00002377
Iteration 98/1000 | Loss: 0.00001643
Iteration 99/1000 | Loss: 0.00001736
Iteration 100/1000 | Loss: 0.00001697
Iteration 101/1000 | Loss: 0.00001661
Iteration 102/1000 | Loss: 0.00001814
Iteration 103/1000 | Loss: 0.00001814
Iteration 104/1000 | Loss: 0.00001834
Iteration 105/1000 | Loss: 0.00001618
Iteration 106/1000 | Loss: 0.00004536
Iteration 107/1000 | Loss: 0.00006693
Iteration 108/1000 | Loss: 0.00008220
Iteration 109/1000 | Loss: 0.00003172
Iteration 110/1000 | Loss: 0.00001674
Iteration 111/1000 | Loss: 0.00002246
Iteration 112/1000 | Loss: 0.00002922
Iteration 113/1000 | Loss: 0.00001872
Iteration 114/1000 | Loss: 0.00002447
Iteration 115/1000 | Loss: 0.00001758
Iteration 116/1000 | Loss: 0.00001598
Iteration 117/1000 | Loss: 0.00001598
Iteration 118/1000 | Loss: 0.00001598
Iteration 119/1000 | Loss: 0.00001598
Iteration 120/1000 | Loss: 0.00001598
Iteration 121/1000 | Loss: 0.00001598
Iteration 122/1000 | Loss: 0.00001598
Iteration 123/1000 | Loss: 0.00001598
Iteration 124/1000 | Loss: 0.00001598
Iteration 125/1000 | Loss: 0.00001607
Iteration 126/1000 | Loss: 0.00001601
Iteration 127/1000 | Loss: 0.00001595
Iteration 128/1000 | Loss: 0.00001595
Iteration 129/1000 | Loss: 0.00001595
Iteration 130/1000 | Loss: 0.00001595
Iteration 131/1000 | Loss: 0.00001595
Iteration 132/1000 | Loss: 0.00001595
Iteration 133/1000 | Loss: 0.00001594
Iteration 134/1000 | Loss: 0.00001594
Iteration 135/1000 | Loss: 0.00001594
Iteration 136/1000 | Loss: 0.00001594
Iteration 137/1000 | Loss: 0.00001602
Iteration 138/1000 | Loss: 0.00001594
Iteration 139/1000 | Loss: 0.00001594
Iteration 140/1000 | Loss: 0.00001594
Iteration 141/1000 | Loss: 0.00001593
Iteration 142/1000 | Loss: 0.00001593
Iteration 143/1000 | Loss: 0.00001593
Iteration 144/1000 | Loss: 0.00001593
Iteration 145/1000 | Loss: 0.00001593
Iteration 146/1000 | Loss: 0.00001593
Iteration 147/1000 | Loss: 0.00001593
Iteration 148/1000 | Loss: 0.00001593
Iteration 149/1000 | Loss: 0.00001593
Iteration 150/1000 | Loss: 0.00001593
Iteration 151/1000 | Loss: 0.00001593
Iteration 152/1000 | Loss: 0.00001593
Iteration 153/1000 | Loss: 0.00001692
Iteration 154/1000 | Loss: 0.00001588
Iteration 155/1000 | Loss: 0.00001588
Iteration 156/1000 | Loss: 0.00001588
Iteration 157/1000 | Loss: 0.00001588
Iteration 158/1000 | Loss: 0.00001588
Iteration 159/1000 | Loss: 0.00001588
Iteration 160/1000 | Loss: 0.00001588
Iteration 161/1000 | Loss: 0.00001588
Iteration 162/1000 | Loss: 0.00001588
Iteration 163/1000 | Loss: 0.00001588
Iteration 164/1000 | Loss: 0.00001587
Iteration 165/1000 | Loss: 0.00001587
Iteration 166/1000 | Loss: 0.00001587
Iteration 167/1000 | Loss: 0.00001586
Iteration 168/1000 | Loss: 0.00001823
Iteration 169/1000 | Loss: 0.00004838
Iteration 170/1000 | Loss: 0.00001726
Iteration 171/1000 | Loss: 0.00001979
Iteration 172/1000 | Loss: 0.00001571
Iteration 173/1000 | Loss: 0.00001570
Iteration 174/1000 | Loss: 0.00007249
Iteration 175/1000 | Loss: 0.00022873
Iteration 176/1000 | Loss: 0.00014044
Iteration 177/1000 | Loss: 0.00003759
Iteration 178/1000 | Loss: 0.00018815
Iteration 179/1000 | Loss: 0.00018187
Iteration 180/1000 | Loss: 0.00013802
Iteration 181/1000 | Loss: 0.00011689
Iteration 182/1000 | Loss: 0.00014409
Iteration 183/1000 | Loss: 0.00006404
Iteration 184/1000 | Loss: 0.00003772
Iteration 185/1000 | Loss: 0.00005120
Iteration 186/1000 | Loss: 0.00004100
Iteration 187/1000 | Loss: 0.00001666
Iteration 188/1000 | Loss: 0.00004164
Iteration 189/1000 | Loss: 0.00001734
Iteration 190/1000 | Loss: 0.00001800
Iteration 191/1000 | Loss: 0.00004321
Iteration 192/1000 | Loss: 0.00002380
Iteration 193/1000 | Loss: 0.00003793
Iteration 194/1000 | Loss: 0.00031634
Iteration 195/1000 | Loss: 0.00008533
Iteration 196/1000 | Loss: 0.00006415
Iteration 197/1000 | Loss: 0.00021904
Iteration 198/1000 | Loss: 0.00016585
Iteration 199/1000 | Loss: 0.00046509
Iteration 200/1000 | Loss: 0.00014790
Iteration 201/1000 | Loss: 0.00013159
Iteration 202/1000 | Loss: 0.00015470
Iteration 203/1000 | Loss: 0.00011893
Iteration 204/1000 | Loss: 0.00003422
Iteration 205/1000 | Loss: 0.00009643
Iteration 206/1000 | Loss: 0.00003034
Iteration 207/1000 | Loss: 0.00002210
Iteration 208/1000 | Loss: 0.00004319
Iteration 209/1000 | Loss: 0.00005842
Iteration 210/1000 | Loss: 0.00018426
Iteration 211/1000 | Loss: 0.00009814
Iteration 212/1000 | Loss: 0.00003304
Iteration 213/1000 | Loss: 0.00003132
Iteration 214/1000 | Loss: 0.00003877
Iteration 215/1000 | Loss: 0.00002340
Iteration 216/1000 | Loss: 0.00005933
Iteration 217/1000 | Loss: 0.00001554
Iteration 218/1000 | Loss: 0.00002025
Iteration 219/1000 | Loss: 0.00001666
Iteration 220/1000 | Loss: 0.00002421
Iteration 221/1000 | Loss: 0.00001314
Iteration 222/1000 | Loss: 0.00002221
Iteration 223/1000 | Loss: 0.00004717
Iteration 224/1000 | Loss: 0.00001598
Iteration 225/1000 | Loss: 0.00001771
Iteration 226/1000 | Loss: 0.00006306
Iteration 227/1000 | Loss: 0.00001497
Iteration 228/1000 | Loss: 0.00024842
Iteration 229/1000 | Loss: 0.00019381
Iteration 230/1000 | Loss: 0.00003662
Iteration 231/1000 | Loss: 0.00002183
Iteration 232/1000 | Loss: 0.00002392
Iteration 233/1000 | Loss: 0.00002163
Iteration 234/1000 | Loss: 0.00002665
Iteration 235/1000 | Loss: 0.00001775
Iteration 236/1000 | Loss: 0.00005963
Iteration 237/1000 | Loss: 0.00001194
Iteration 238/1000 | Loss: 0.00001760
Iteration 239/1000 | Loss: 0.00001137
Iteration 240/1000 | Loss: 0.00002640
Iteration 241/1000 | Loss: 0.00001109
Iteration 242/1000 | Loss: 0.00001105
Iteration 243/1000 | Loss: 0.00001249
Iteration 244/1000 | Loss: 0.00003004
Iteration 245/1000 | Loss: 0.00001094
Iteration 246/1000 | Loss: 0.00001398
Iteration 247/1000 | Loss: 0.00001081
Iteration 248/1000 | Loss: 0.00001080
Iteration 249/1000 | Loss: 0.00001080
Iteration 250/1000 | Loss: 0.00001079
Iteration 251/1000 | Loss: 0.00001410
Iteration 252/1000 | Loss: 0.00001076
Iteration 253/1000 | Loss: 0.00001075
Iteration 254/1000 | Loss: 0.00001098
Iteration 255/1000 | Loss: 0.00002117
Iteration 256/1000 | Loss: 0.00001068
Iteration 257/1000 | Loss: 0.00001078
Iteration 258/1000 | Loss: 0.00001078
Iteration 259/1000 | Loss: 0.00001220
Iteration 260/1000 | Loss: 0.00005452
Iteration 261/1000 | Loss: 0.00001384
Iteration 262/1000 | Loss: 0.00005630
Iteration 263/1000 | Loss: 0.00001240
Iteration 264/1000 | Loss: 0.00001118
Iteration 265/1000 | Loss: 0.00001057
Iteration 266/1000 | Loss: 0.00001057
Iteration 267/1000 | Loss: 0.00001057
Iteration 268/1000 | Loss: 0.00001057
Iteration 269/1000 | Loss: 0.00001057
Iteration 270/1000 | Loss: 0.00001057
Iteration 271/1000 | Loss: 0.00001056
Iteration 272/1000 | Loss: 0.00001056
Iteration 273/1000 | Loss: 0.00001056
Iteration 274/1000 | Loss: 0.00001056
Iteration 275/1000 | Loss: 0.00001056
Iteration 276/1000 | Loss: 0.00001056
Iteration 277/1000 | Loss: 0.00001056
Iteration 278/1000 | Loss: 0.00001056
Iteration 279/1000 | Loss: 0.00001056
Iteration 280/1000 | Loss: 0.00001055
Iteration 281/1000 | Loss: 0.00001055
Iteration 282/1000 | Loss: 0.00001055
Iteration 283/1000 | Loss: 0.00001055
Iteration 284/1000 | Loss: 0.00001055
Iteration 285/1000 | Loss: 0.00001055
Iteration 286/1000 | Loss: 0.00001055
Iteration 287/1000 | Loss: 0.00001055
Iteration 288/1000 | Loss: 0.00001055
Iteration 289/1000 | Loss: 0.00001055
Iteration 290/1000 | Loss: 0.00001054
Iteration 291/1000 | Loss: 0.00001054
Iteration 292/1000 | Loss: 0.00001054
Iteration 293/1000 | Loss: 0.00001054
Iteration 294/1000 | Loss: 0.00001054
Iteration 295/1000 | Loss: 0.00001054
Iteration 296/1000 | Loss: 0.00001054
Iteration 297/1000 | Loss: 0.00001054
Iteration 298/1000 | Loss: 0.00001054
Iteration 299/1000 | Loss: 0.00001054
Iteration 300/1000 | Loss: 0.00001054
Iteration 301/1000 | Loss: 0.00001054
Iteration 302/1000 | Loss: 0.00001054
Iteration 303/1000 | Loss: 0.00001054
Iteration 304/1000 | Loss: 0.00001054
Iteration 305/1000 | Loss: 0.00001053
Iteration 306/1000 | Loss: 0.00001053
Iteration 307/1000 | Loss: 0.00001053
Iteration 308/1000 | Loss: 0.00001053
Iteration 309/1000 | Loss: 0.00001053
Iteration 310/1000 | Loss: 0.00001053
Iteration 311/1000 | Loss: 0.00001053
Iteration 312/1000 | Loss: 0.00001053
Iteration 313/1000 | Loss: 0.00001053
Iteration 314/1000 | Loss: 0.00001053
Iteration 315/1000 | Loss: 0.00001053
Iteration 316/1000 | Loss: 0.00001053
Iteration 317/1000 | Loss: 0.00001053
Iteration 318/1000 | Loss: 0.00001053
Iteration 319/1000 | Loss: 0.00001053
Iteration 320/1000 | Loss: 0.00001053
Iteration 321/1000 | Loss: 0.00001053
Iteration 322/1000 | Loss: 0.00001053
Iteration 323/1000 | Loss: 0.00001052
Iteration 324/1000 | Loss: 0.00001052
Iteration 325/1000 | Loss: 0.00001052
Iteration 326/1000 | Loss: 0.00001052
Iteration 327/1000 | Loss: 0.00001052
Iteration 328/1000 | Loss: 0.00001052
Iteration 329/1000 | Loss: 0.00001052
Iteration 330/1000 | Loss: 0.00001052
Iteration 331/1000 | Loss: 0.00001052
Iteration 332/1000 | Loss: 0.00001052
Iteration 333/1000 | Loss: 0.00001052
Iteration 334/1000 | Loss: 0.00001052
Iteration 335/1000 | Loss: 0.00001052
Iteration 336/1000 | Loss: 0.00001052
Iteration 337/1000 | Loss: 0.00001052
Iteration 338/1000 | Loss: 0.00001052
Iteration 339/1000 | Loss: 0.00001052
Iteration 340/1000 | Loss: 0.00001052
Iteration 341/1000 | Loss: 0.00001051
Iteration 342/1000 | Loss: 0.00001051
Iteration 343/1000 | Loss: 0.00001051
Iteration 344/1000 | Loss: 0.00001051
Iteration 345/1000 | Loss: 0.00001051
Iteration 346/1000 | Loss: 0.00001051
Iteration 347/1000 | Loss: 0.00001051
Iteration 348/1000 | Loss: 0.00001051
Iteration 349/1000 | Loss: 0.00001051
Iteration 350/1000 | Loss: 0.00001050
Iteration 351/1000 | Loss: 0.00001050
Iteration 352/1000 | Loss: 0.00001888
Iteration 353/1000 | Loss: 0.00001310
Iteration 354/1000 | Loss: 0.00001049
Iteration 355/1000 | Loss: 0.00001049
Iteration 356/1000 | Loss: 0.00001049
Iteration 357/1000 | Loss: 0.00001049
Iteration 358/1000 | Loss: 0.00001049
Iteration 359/1000 | Loss: 0.00001049
Iteration 360/1000 | Loss: 0.00001049
Iteration 361/1000 | Loss: 0.00001049
Iteration 362/1000 | Loss: 0.00001048
Iteration 363/1000 | Loss: 0.00001048
Iteration 364/1000 | Loss: 0.00001048
Iteration 365/1000 | Loss: 0.00001048
Iteration 366/1000 | Loss: 0.00001048
Iteration 367/1000 | Loss: 0.00001048
Iteration 368/1000 | Loss: 0.00001048
Iteration 369/1000 | Loss: 0.00001048
Iteration 370/1000 | Loss: 0.00001048
Iteration 371/1000 | Loss: 0.00001048
Iteration 372/1000 | Loss: 0.00001048
Iteration 373/1000 | Loss: 0.00001048
Iteration 374/1000 | Loss: 0.00001048
Iteration 375/1000 | Loss: 0.00001048
Iteration 376/1000 | Loss: 0.00001048
Iteration 377/1000 | Loss: 0.00001048
Iteration 378/1000 | Loss: 0.00001047
Iteration 379/1000 | Loss: 0.00001047
Iteration 380/1000 | Loss: 0.00001047
Iteration 381/1000 | Loss: 0.00001047
Iteration 382/1000 | Loss: 0.00001047
Iteration 383/1000 | Loss: 0.00001047
Iteration 384/1000 | Loss: 0.00001047
Iteration 385/1000 | Loss: 0.00001047
Iteration 386/1000 | Loss: 0.00001047
Iteration 387/1000 | Loss: 0.00001047
Iteration 388/1000 | Loss: 0.00001047
Iteration 389/1000 | Loss: 0.00001047
Iteration 390/1000 | Loss: 0.00001047
Iteration 391/1000 | Loss: 0.00001047
Iteration 392/1000 | Loss: 0.00001047
Iteration 393/1000 | Loss: 0.00001047
Iteration 394/1000 | Loss: 0.00001047
Iteration 395/1000 | Loss: 0.00001047
Iteration 396/1000 | Loss: 0.00001047
Iteration 397/1000 | Loss: 0.00001047
Iteration 398/1000 | Loss: 0.00001047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 398. Stopping optimization.
Last 5 losses: [1.0472866051713936e-05, 1.0472866051713936e-05, 1.0472866051713936e-05, 1.0472866051713936e-05, 1.0472866051713936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0472866051713936e-05

Optimization complete. Final v2v error: 2.76131010055542 mm

Highest mean error: 4.306050777435303 mm for frame 13

Lowest mean error: 2.491966724395752 mm for frame 221

Saving results

Total time: 384.01344180107117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00217902
Iteration 2/25 | Loss: 0.00130605
Iteration 3/25 | Loss: 0.00121256
Iteration 4/25 | Loss: 0.00118793
Iteration 5/25 | Loss: 0.00117842
Iteration 6/25 | Loss: 0.00117547
Iteration 7/25 | Loss: 0.00117459
Iteration 8/25 | Loss: 0.00117459
Iteration 9/25 | Loss: 0.00117459
Iteration 10/25 | Loss: 0.00117459
Iteration 11/25 | Loss: 0.00117459
Iteration 12/25 | Loss: 0.00117459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011745915981009603, 0.0011745915981009603, 0.0011745915981009603, 0.0011745915981009603, 0.0011745915981009603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011745915981009603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25718582
Iteration 2/25 | Loss: 0.00272061
Iteration 3/25 | Loss: 0.00272061
Iteration 4/25 | Loss: 0.00272061
Iteration 5/25 | Loss: 0.00272060
Iteration 6/25 | Loss: 0.00272060
Iteration 7/25 | Loss: 0.00272060
Iteration 8/25 | Loss: 0.00272060
Iteration 9/25 | Loss: 0.00272060
Iteration 10/25 | Loss: 0.00272060
Iteration 11/25 | Loss: 0.00272060
Iteration 12/25 | Loss: 0.00272060
Iteration 13/25 | Loss: 0.00272060
Iteration 14/25 | Loss: 0.00272060
Iteration 15/25 | Loss: 0.00272060
Iteration 16/25 | Loss: 0.00272060
Iteration 17/25 | Loss: 0.00272060
Iteration 18/25 | Loss: 0.00272060
Iteration 19/25 | Loss: 0.00272060
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002720603486523032, 0.002720603486523032, 0.002720603486523032, 0.002720603486523032, 0.002720603486523032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002720603486523032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00272060
Iteration 2/1000 | Loss: 0.00004644
Iteration 3/1000 | Loss: 0.00002784
Iteration 4/1000 | Loss: 0.00002076
Iteration 5/1000 | Loss: 0.00001944
Iteration 6/1000 | Loss: 0.00001822
Iteration 7/1000 | Loss: 0.00001746
Iteration 8/1000 | Loss: 0.00001676
Iteration 9/1000 | Loss: 0.00001634
Iteration 10/1000 | Loss: 0.00001606
Iteration 11/1000 | Loss: 0.00001579
Iteration 12/1000 | Loss: 0.00001557
Iteration 13/1000 | Loss: 0.00001556
Iteration 14/1000 | Loss: 0.00001556
Iteration 15/1000 | Loss: 0.00001556
Iteration 16/1000 | Loss: 0.00001551
Iteration 17/1000 | Loss: 0.00001548
Iteration 18/1000 | Loss: 0.00001548
Iteration 19/1000 | Loss: 0.00001547
Iteration 20/1000 | Loss: 0.00001544
Iteration 21/1000 | Loss: 0.00001544
Iteration 22/1000 | Loss: 0.00001543
Iteration 23/1000 | Loss: 0.00001543
Iteration 24/1000 | Loss: 0.00001542
Iteration 25/1000 | Loss: 0.00001536
Iteration 26/1000 | Loss: 0.00001536
Iteration 27/1000 | Loss: 0.00001536
Iteration 28/1000 | Loss: 0.00001536
Iteration 29/1000 | Loss: 0.00001536
Iteration 30/1000 | Loss: 0.00001536
Iteration 31/1000 | Loss: 0.00001536
Iteration 32/1000 | Loss: 0.00001536
Iteration 33/1000 | Loss: 0.00001536
Iteration 34/1000 | Loss: 0.00001536
Iteration 35/1000 | Loss: 0.00001536
Iteration 36/1000 | Loss: 0.00001535
Iteration 37/1000 | Loss: 0.00001535
Iteration 38/1000 | Loss: 0.00001525
Iteration 39/1000 | Loss: 0.00001524
Iteration 40/1000 | Loss: 0.00001522
Iteration 41/1000 | Loss: 0.00001521
Iteration 42/1000 | Loss: 0.00001521
Iteration 43/1000 | Loss: 0.00001521
Iteration 44/1000 | Loss: 0.00001520
Iteration 45/1000 | Loss: 0.00001518
Iteration 46/1000 | Loss: 0.00001518
Iteration 47/1000 | Loss: 0.00001518
Iteration 48/1000 | Loss: 0.00001518
Iteration 49/1000 | Loss: 0.00001517
Iteration 50/1000 | Loss: 0.00001517
Iteration 51/1000 | Loss: 0.00001514
Iteration 52/1000 | Loss: 0.00001514
Iteration 53/1000 | Loss: 0.00001514
Iteration 54/1000 | Loss: 0.00001513
Iteration 55/1000 | Loss: 0.00001512
Iteration 56/1000 | Loss: 0.00001509
Iteration 57/1000 | Loss: 0.00001508
Iteration 58/1000 | Loss: 0.00001508
Iteration 59/1000 | Loss: 0.00001508
Iteration 60/1000 | Loss: 0.00001507
Iteration 61/1000 | Loss: 0.00001506
Iteration 62/1000 | Loss: 0.00001506
Iteration 63/1000 | Loss: 0.00001505
Iteration 64/1000 | Loss: 0.00001505
Iteration 65/1000 | Loss: 0.00001504
Iteration 66/1000 | Loss: 0.00001504
Iteration 67/1000 | Loss: 0.00001504
Iteration 68/1000 | Loss: 0.00001503
Iteration 69/1000 | Loss: 0.00001503
Iteration 70/1000 | Loss: 0.00001502
Iteration 71/1000 | Loss: 0.00001502
Iteration 72/1000 | Loss: 0.00001502
Iteration 73/1000 | Loss: 0.00001502
Iteration 74/1000 | Loss: 0.00001502
Iteration 75/1000 | Loss: 0.00001502
Iteration 76/1000 | Loss: 0.00001502
Iteration 77/1000 | Loss: 0.00001501
Iteration 78/1000 | Loss: 0.00001501
Iteration 79/1000 | Loss: 0.00001500
Iteration 80/1000 | Loss: 0.00001500
Iteration 81/1000 | Loss: 0.00001500
Iteration 82/1000 | Loss: 0.00001500
Iteration 83/1000 | Loss: 0.00001500
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001499
Iteration 86/1000 | Loss: 0.00001499
Iteration 87/1000 | Loss: 0.00001499
Iteration 88/1000 | Loss: 0.00001499
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001498
Iteration 91/1000 | Loss: 0.00001498
Iteration 92/1000 | Loss: 0.00001498
Iteration 93/1000 | Loss: 0.00001498
Iteration 94/1000 | Loss: 0.00001497
Iteration 95/1000 | Loss: 0.00001497
Iteration 96/1000 | Loss: 0.00001497
Iteration 97/1000 | Loss: 0.00001497
Iteration 98/1000 | Loss: 0.00001496
Iteration 99/1000 | Loss: 0.00001496
Iteration 100/1000 | Loss: 0.00001496
Iteration 101/1000 | Loss: 0.00001496
Iteration 102/1000 | Loss: 0.00001496
Iteration 103/1000 | Loss: 0.00001496
Iteration 104/1000 | Loss: 0.00001496
Iteration 105/1000 | Loss: 0.00001496
Iteration 106/1000 | Loss: 0.00001496
Iteration 107/1000 | Loss: 0.00001495
Iteration 108/1000 | Loss: 0.00001495
Iteration 109/1000 | Loss: 0.00001495
Iteration 110/1000 | Loss: 0.00001495
Iteration 111/1000 | Loss: 0.00001495
Iteration 112/1000 | Loss: 0.00001494
Iteration 113/1000 | Loss: 0.00001494
Iteration 114/1000 | Loss: 0.00001494
Iteration 115/1000 | Loss: 0.00001494
Iteration 116/1000 | Loss: 0.00001494
Iteration 117/1000 | Loss: 0.00001494
Iteration 118/1000 | Loss: 0.00001493
Iteration 119/1000 | Loss: 0.00001493
Iteration 120/1000 | Loss: 0.00001492
Iteration 121/1000 | Loss: 0.00001492
Iteration 122/1000 | Loss: 0.00001492
Iteration 123/1000 | Loss: 0.00001492
Iteration 124/1000 | Loss: 0.00001492
Iteration 125/1000 | Loss: 0.00001492
Iteration 126/1000 | Loss: 0.00001492
Iteration 127/1000 | Loss: 0.00001492
Iteration 128/1000 | Loss: 0.00001492
Iteration 129/1000 | Loss: 0.00001492
Iteration 130/1000 | Loss: 0.00001492
Iteration 131/1000 | Loss: 0.00001492
Iteration 132/1000 | Loss: 0.00001492
Iteration 133/1000 | Loss: 0.00001491
Iteration 134/1000 | Loss: 0.00001491
Iteration 135/1000 | Loss: 0.00001491
Iteration 136/1000 | Loss: 0.00001491
Iteration 137/1000 | Loss: 0.00001491
Iteration 138/1000 | Loss: 0.00001491
Iteration 139/1000 | Loss: 0.00001491
Iteration 140/1000 | Loss: 0.00001491
Iteration 141/1000 | Loss: 0.00001491
Iteration 142/1000 | Loss: 0.00001491
Iteration 143/1000 | Loss: 0.00001491
Iteration 144/1000 | Loss: 0.00001491
Iteration 145/1000 | Loss: 0.00001491
Iteration 146/1000 | Loss: 0.00001491
Iteration 147/1000 | Loss: 0.00001491
Iteration 148/1000 | Loss: 0.00001490
Iteration 149/1000 | Loss: 0.00001490
Iteration 150/1000 | Loss: 0.00001490
Iteration 151/1000 | Loss: 0.00001490
Iteration 152/1000 | Loss: 0.00001489
Iteration 153/1000 | Loss: 0.00001489
Iteration 154/1000 | Loss: 0.00001489
Iteration 155/1000 | Loss: 0.00001489
Iteration 156/1000 | Loss: 0.00001489
Iteration 157/1000 | Loss: 0.00001489
Iteration 158/1000 | Loss: 0.00001489
Iteration 159/1000 | Loss: 0.00001489
Iteration 160/1000 | Loss: 0.00001489
Iteration 161/1000 | Loss: 0.00001489
Iteration 162/1000 | Loss: 0.00001489
Iteration 163/1000 | Loss: 0.00001489
Iteration 164/1000 | Loss: 0.00001489
Iteration 165/1000 | Loss: 0.00001489
Iteration 166/1000 | Loss: 0.00001489
Iteration 167/1000 | Loss: 0.00001489
Iteration 168/1000 | Loss: 0.00001489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.4886482858855743e-05, 1.4886482858855743e-05, 1.4886482858855743e-05, 1.4886482858855743e-05, 1.4886482858855743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4886482858855743e-05

Optimization complete. Final v2v error: 3.2817940711975098 mm

Highest mean error: 3.757354259490967 mm for frame 61

Lowest mean error: 2.9151575565338135 mm for frame 102

Saving results

Total time: 42.91471457481384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802210
Iteration 2/25 | Loss: 0.00184274
Iteration 3/25 | Loss: 0.00139836
Iteration 4/25 | Loss: 0.00137021
Iteration 5/25 | Loss: 0.00136346
Iteration 6/25 | Loss: 0.00136213
Iteration 7/25 | Loss: 0.00136213
Iteration 8/25 | Loss: 0.00136213
Iteration 9/25 | Loss: 0.00136213
Iteration 10/25 | Loss: 0.00136213
Iteration 11/25 | Loss: 0.00136213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013621327234432101, 0.0013621327234432101, 0.0013621327234432101, 0.0013621327234432101, 0.0013621327234432101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013621327234432101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.34803554
Iteration 2/25 | Loss: 0.00112627
Iteration 3/25 | Loss: 0.00112625
Iteration 4/25 | Loss: 0.00112625
Iteration 5/25 | Loss: 0.00112624
Iteration 6/25 | Loss: 0.00112624
Iteration 7/25 | Loss: 0.00112624
Iteration 8/25 | Loss: 0.00112624
Iteration 9/25 | Loss: 0.00112624
Iteration 10/25 | Loss: 0.00112624
Iteration 11/25 | Loss: 0.00112624
Iteration 12/25 | Loss: 0.00112624
Iteration 13/25 | Loss: 0.00112624
Iteration 14/25 | Loss: 0.00112624
Iteration 15/25 | Loss: 0.00112624
Iteration 16/25 | Loss: 0.00112624
Iteration 17/25 | Loss: 0.00112624
Iteration 18/25 | Loss: 0.00112624
Iteration 19/25 | Loss: 0.00112624
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011262432672083378, 0.0011262432672083378, 0.0011262432672083378, 0.0011262432672083378, 0.0011262432672083378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011262432672083378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112624
Iteration 2/1000 | Loss: 0.00009920
Iteration 3/1000 | Loss: 0.00005710
Iteration 4/1000 | Loss: 0.00004364
Iteration 5/1000 | Loss: 0.00003938
Iteration 6/1000 | Loss: 0.00003732
Iteration 7/1000 | Loss: 0.00003611
Iteration 8/1000 | Loss: 0.00003535
Iteration 9/1000 | Loss: 0.00003447
Iteration 10/1000 | Loss: 0.00003385
Iteration 11/1000 | Loss: 0.00003328
Iteration 12/1000 | Loss: 0.00003284
Iteration 13/1000 | Loss: 0.00003252
Iteration 14/1000 | Loss: 0.00003223
Iteration 15/1000 | Loss: 0.00003219
Iteration 16/1000 | Loss: 0.00003192
Iteration 17/1000 | Loss: 0.00003172
Iteration 18/1000 | Loss: 0.00003154
Iteration 19/1000 | Loss: 0.00003150
Iteration 20/1000 | Loss: 0.00003135
Iteration 21/1000 | Loss: 0.00003132
Iteration 22/1000 | Loss: 0.00003125
Iteration 23/1000 | Loss: 0.00003108
Iteration 24/1000 | Loss: 0.00003100
Iteration 25/1000 | Loss: 0.00003099
Iteration 26/1000 | Loss: 0.00003092
Iteration 27/1000 | Loss: 0.00003083
Iteration 28/1000 | Loss: 0.00003078
Iteration 29/1000 | Loss: 0.00003078
Iteration 30/1000 | Loss: 0.00003076
Iteration 31/1000 | Loss: 0.00003076
Iteration 32/1000 | Loss: 0.00003075
Iteration 33/1000 | Loss: 0.00003075
Iteration 34/1000 | Loss: 0.00003074
Iteration 35/1000 | Loss: 0.00003074
Iteration 36/1000 | Loss: 0.00003074
Iteration 37/1000 | Loss: 0.00003074
Iteration 38/1000 | Loss: 0.00003074
Iteration 39/1000 | Loss: 0.00003074
Iteration 40/1000 | Loss: 0.00003074
Iteration 41/1000 | Loss: 0.00003073
Iteration 42/1000 | Loss: 0.00003073
Iteration 43/1000 | Loss: 0.00003073
Iteration 44/1000 | Loss: 0.00003073
Iteration 45/1000 | Loss: 0.00003072
Iteration 46/1000 | Loss: 0.00003072
Iteration 47/1000 | Loss: 0.00003072
Iteration 48/1000 | Loss: 0.00003072
Iteration 49/1000 | Loss: 0.00003072
Iteration 50/1000 | Loss: 0.00003071
Iteration 51/1000 | Loss: 0.00003071
Iteration 52/1000 | Loss: 0.00003071
Iteration 53/1000 | Loss: 0.00003071
Iteration 54/1000 | Loss: 0.00003070
Iteration 55/1000 | Loss: 0.00003070
Iteration 56/1000 | Loss: 0.00003070
Iteration 57/1000 | Loss: 0.00003070
Iteration 58/1000 | Loss: 0.00003070
Iteration 59/1000 | Loss: 0.00003070
Iteration 60/1000 | Loss: 0.00003069
Iteration 61/1000 | Loss: 0.00003069
Iteration 62/1000 | Loss: 0.00003069
Iteration 63/1000 | Loss: 0.00003069
Iteration 64/1000 | Loss: 0.00003068
Iteration 65/1000 | Loss: 0.00003068
Iteration 66/1000 | Loss: 0.00003068
Iteration 67/1000 | Loss: 0.00003068
Iteration 68/1000 | Loss: 0.00003067
Iteration 69/1000 | Loss: 0.00003067
Iteration 70/1000 | Loss: 0.00003067
Iteration 71/1000 | Loss: 0.00003067
Iteration 72/1000 | Loss: 0.00003067
Iteration 73/1000 | Loss: 0.00003067
Iteration 74/1000 | Loss: 0.00003067
Iteration 75/1000 | Loss: 0.00003067
Iteration 76/1000 | Loss: 0.00003067
Iteration 77/1000 | Loss: 0.00003066
Iteration 78/1000 | Loss: 0.00003066
Iteration 79/1000 | Loss: 0.00003066
Iteration 80/1000 | Loss: 0.00003066
Iteration 81/1000 | Loss: 0.00003066
Iteration 82/1000 | Loss: 0.00003066
Iteration 83/1000 | Loss: 0.00003066
Iteration 84/1000 | Loss: 0.00003066
Iteration 85/1000 | Loss: 0.00003065
Iteration 86/1000 | Loss: 0.00003065
Iteration 87/1000 | Loss: 0.00003065
Iteration 88/1000 | Loss: 0.00003065
Iteration 89/1000 | Loss: 0.00003065
Iteration 90/1000 | Loss: 0.00003065
Iteration 91/1000 | Loss: 0.00003065
Iteration 92/1000 | Loss: 0.00003065
Iteration 93/1000 | Loss: 0.00003065
Iteration 94/1000 | Loss: 0.00003064
Iteration 95/1000 | Loss: 0.00003064
Iteration 96/1000 | Loss: 0.00003064
Iteration 97/1000 | Loss: 0.00003064
Iteration 98/1000 | Loss: 0.00003064
Iteration 99/1000 | Loss: 0.00003064
Iteration 100/1000 | Loss: 0.00003063
Iteration 101/1000 | Loss: 0.00003063
Iteration 102/1000 | Loss: 0.00003063
Iteration 103/1000 | Loss: 0.00003063
Iteration 104/1000 | Loss: 0.00003063
Iteration 105/1000 | Loss: 0.00003063
Iteration 106/1000 | Loss: 0.00003063
Iteration 107/1000 | Loss: 0.00003062
Iteration 108/1000 | Loss: 0.00003062
Iteration 109/1000 | Loss: 0.00003062
Iteration 110/1000 | Loss: 0.00003062
Iteration 111/1000 | Loss: 0.00003062
Iteration 112/1000 | Loss: 0.00003062
Iteration 113/1000 | Loss: 0.00003062
Iteration 114/1000 | Loss: 0.00003062
Iteration 115/1000 | Loss: 0.00003061
Iteration 116/1000 | Loss: 0.00003061
Iteration 117/1000 | Loss: 0.00003061
Iteration 118/1000 | Loss: 0.00003061
Iteration 119/1000 | Loss: 0.00003061
Iteration 120/1000 | Loss: 0.00003061
Iteration 121/1000 | Loss: 0.00003061
Iteration 122/1000 | Loss: 0.00003061
Iteration 123/1000 | Loss: 0.00003061
Iteration 124/1000 | Loss: 0.00003061
Iteration 125/1000 | Loss: 0.00003061
Iteration 126/1000 | Loss: 0.00003061
Iteration 127/1000 | Loss: 0.00003061
Iteration 128/1000 | Loss: 0.00003060
Iteration 129/1000 | Loss: 0.00003060
Iteration 130/1000 | Loss: 0.00003060
Iteration 131/1000 | Loss: 0.00003060
Iteration 132/1000 | Loss: 0.00003060
Iteration 133/1000 | Loss: 0.00003059
Iteration 134/1000 | Loss: 0.00003059
Iteration 135/1000 | Loss: 0.00003059
Iteration 136/1000 | Loss: 0.00003059
Iteration 137/1000 | Loss: 0.00003059
Iteration 138/1000 | Loss: 0.00003059
Iteration 139/1000 | Loss: 0.00003059
Iteration 140/1000 | Loss: 0.00003059
Iteration 141/1000 | Loss: 0.00003059
Iteration 142/1000 | Loss: 0.00003059
Iteration 143/1000 | Loss: 0.00003059
Iteration 144/1000 | Loss: 0.00003059
Iteration 145/1000 | Loss: 0.00003059
Iteration 146/1000 | Loss: 0.00003059
Iteration 147/1000 | Loss: 0.00003059
Iteration 148/1000 | Loss: 0.00003059
Iteration 149/1000 | Loss: 0.00003059
Iteration 150/1000 | Loss: 0.00003058
Iteration 151/1000 | Loss: 0.00003058
Iteration 152/1000 | Loss: 0.00003058
Iteration 153/1000 | Loss: 0.00003058
Iteration 154/1000 | Loss: 0.00003058
Iteration 155/1000 | Loss: 0.00003058
Iteration 156/1000 | Loss: 0.00003058
Iteration 157/1000 | Loss: 0.00003058
Iteration 158/1000 | Loss: 0.00003058
Iteration 159/1000 | Loss: 0.00003058
Iteration 160/1000 | Loss: 0.00003057
Iteration 161/1000 | Loss: 0.00003057
Iteration 162/1000 | Loss: 0.00003057
Iteration 163/1000 | Loss: 0.00003057
Iteration 164/1000 | Loss: 0.00003057
Iteration 165/1000 | Loss: 0.00003057
Iteration 166/1000 | Loss: 0.00003057
Iteration 167/1000 | Loss: 0.00003057
Iteration 168/1000 | Loss: 0.00003057
Iteration 169/1000 | Loss: 0.00003057
Iteration 170/1000 | Loss: 0.00003057
Iteration 171/1000 | Loss: 0.00003056
Iteration 172/1000 | Loss: 0.00003056
Iteration 173/1000 | Loss: 0.00003056
Iteration 174/1000 | Loss: 0.00003056
Iteration 175/1000 | Loss: 0.00003056
Iteration 176/1000 | Loss: 0.00003056
Iteration 177/1000 | Loss: 0.00003056
Iteration 178/1000 | Loss: 0.00003056
Iteration 179/1000 | Loss: 0.00003056
Iteration 180/1000 | Loss: 0.00003056
Iteration 181/1000 | Loss: 0.00003056
Iteration 182/1000 | Loss: 0.00003056
Iteration 183/1000 | Loss: 0.00003055
Iteration 184/1000 | Loss: 0.00003055
Iteration 185/1000 | Loss: 0.00003055
Iteration 186/1000 | Loss: 0.00003055
Iteration 187/1000 | Loss: 0.00003055
Iteration 188/1000 | Loss: 0.00003055
Iteration 189/1000 | Loss: 0.00003055
Iteration 190/1000 | Loss: 0.00003055
Iteration 191/1000 | Loss: 0.00003055
Iteration 192/1000 | Loss: 0.00003055
Iteration 193/1000 | Loss: 0.00003055
Iteration 194/1000 | Loss: 0.00003055
Iteration 195/1000 | Loss: 0.00003055
Iteration 196/1000 | Loss: 0.00003054
Iteration 197/1000 | Loss: 0.00003054
Iteration 198/1000 | Loss: 0.00003054
Iteration 199/1000 | Loss: 0.00003054
Iteration 200/1000 | Loss: 0.00003054
Iteration 201/1000 | Loss: 0.00003054
Iteration 202/1000 | Loss: 0.00003054
Iteration 203/1000 | Loss: 0.00003054
Iteration 204/1000 | Loss: 0.00003054
Iteration 205/1000 | Loss: 0.00003054
Iteration 206/1000 | Loss: 0.00003054
Iteration 207/1000 | Loss: 0.00003054
Iteration 208/1000 | Loss: 0.00003054
Iteration 209/1000 | Loss: 0.00003053
Iteration 210/1000 | Loss: 0.00003053
Iteration 211/1000 | Loss: 0.00003053
Iteration 212/1000 | Loss: 0.00003053
Iteration 213/1000 | Loss: 0.00003053
Iteration 214/1000 | Loss: 0.00003053
Iteration 215/1000 | Loss: 0.00003053
Iteration 216/1000 | Loss: 0.00003053
Iteration 217/1000 | Loss: 0.00003053
Iteration 218/1000 | Loss: 0.00003053
Iteration 219/1000 | Loss: 0.00003053
Iteration 220/1000 | Loss: 0.00003053
Iteration 221/1000 | Loss: 0.00003053
Iteration 222/1000 | Loss: 0.00003053
Iteration 223/1000 | Loss: 0.00003053
Iteration 224/1000 | Loss: 0.00003053
Iteration 225/1000 | Loss: 0.00003053
Iteration 226/1000 | Loss: 0.00003053
Iteration 227/1000 | Loss: 0.00003053
Iteration 228/1000 | Loss: 0.00003053
Iteration 229/1000 | Loss: 0.00003053
Iteration 230/1000 | Loss: 0.00003053
Iteration 231/1000 | Loss: 0.00003053
Iteration 232/1000 | Loss: 0.00003053
Iteration 233/1000 | Loss: 0.00003053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [3.0528142815455794e-05, 3.0528142815455794e-05, 3.0528142815455794e-05, 3.0528142815455794e-05, 3.0528142815455794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0528142815455794e-05

Optimization complete. Final v2v error: 4.409514904022217 mm

Highest mean error: 5.929889678955078 mm for frame 58

Lowest mean error: 3.088820219039917 mm for frame 98

Saving results

Total time: 56.907336950302124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401200
Iteration 2/25 | Loss: 0.00133155
Iteration 3/25 | Loss: 0.00121589
Iteration 4/25 | Loss: 0.00120156
Iteration 5/25 | Loss: 0.00119931
Iteration 6/25 | Loss: 0.00119925
Iteration 7/25 | Loss: 0.00119925
Iteration 8/25 | Loss: 0.00119925
Iteration 9/25 | Loss: 0.00119925
Iteration 10/25 | Loss: 0.00119925
Iteration 11/25 | Loss: 0.00119925
Iteration 12/25 | Loss: 0.00119925
Iteration 13/25 | Loss: 0.00119925
Iteration 14/25 | Loss: 0.00119925
Iteration 15/25 | Loss: 0.00119925
Iteration 16/25 | Loss: 0.00119925
Iteration 17/25 | Loss: 0.00119925
Iteration 18/25 | Loss: 0.00119925
Iteration 19/25 | Loss: 0.00119925
Iteration 20/25 | Loss: 0.00119925
Iteration 21/25 | Loss: 0.00119925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011992522049695253, 0.0011992522049695253, 0.0011992522049695253, 0.0011992522049695253, 0.0011992522049695253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011992522049695253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28102875
Iteration 2/25 | Loss: 0.00123023
Iteration 3/25 | Loss: 0.00123022
Iteration 4/25 | Loss: 0.00123022
Iteration 5/25 | Loss: 0.00123022
Iteration 6/25 | Loss: 0.00123022
Iteration 7/25 | Loss: 0.00123022
Iteration 8/25 | Loss: 0.00123022
Iteration 9/25 | Loss: 0.00123022
Iteration 10/25 | Loss: 0.00123022
Iteration 11/25 | Loss: 0.00123022
Iteration 12/25 | Loss: 0.00123022
Iteration 13/25 | Loss: 0.00123022
Iteration 14/25 | Loss: 0.00123022
Iteration 15/25 | Loss: 0.00123022
Iteration 16/25 | Loss: 0.00123022
Iteration 17/25 | Loss: 0.00123022
Iteration 18/25 | Loss: 0.00123022
Iteration 19/25 | Loss: 0.00123022
Iteration 20/25 | Loss: 0.00123022
Iteration 21/25 | Loss: 0.00123022
Iteration 22/25 | Loss: 0.00123022
Iteration 23/25 | Loss: 0.00123022
Iteration 24/25 | Loss: 0.00123022
Iteration 25/25 | Loss: 0.00123022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123022
Iteration 2/1000 | Loss: 0.00002350
Iteration 3/1000 | Loss: 0.00001732
Iteration 4/1000 | Loss: 0.00001570
Iteration 5/1000 | Loss: 0.00001486
Iteration 6/1000 | Loss: 0.00001413
Iteration 7/1000 | Loss: 0.00001361
Iteration 8/1000 | Loss: 0.00001327
Iteration 9/1000 | Loss: 0.00001284
Iteration 10/1000 | Loss: 0.00001261
Iteration 11/1000 | Loss: 0.00001242
Iteration 12/1000 | Loss: 0.00001224
Iteration 13/1000 | Loss: 0.00001224
Iteration 14/1000 | Loss: 0.00001220
Iteration 15/1000 | Loss: 0.00001212
Iteration 16/1000 | Loss: 0.00001206
Iteration 17/1000 | Loss: 0.00001206
Iteration 18/1000 | Loss: 0.00001205
Iteration 19/1000 | Loss: 0.00001195
Iteration 20/1000 | Loss: 0.00001190
Iteration 21/1000 | Loss: 0.00001190
Iteration 22/1000 | Loss: 0.00001187
Iteration 23/1000 | Loss: 0.00001184
Iteration 24/1000 | Loss: 0.00001183
Iteration 25/1000 | Loss: 0.00001182
Iteration 26/1000 | Loss: 0.00001181
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001175
Iteration 29/1000 | Loss: 0.00001175
Iteration 30/1000 | Loss: 0.00001171
Iteration 31/1000 | Loss: 0.00001171
Iteration 32/1000 | Loss: 0.00001170
Iteration 33/1000 | Loss: 0.00001166
Iteration 34/1000 | Loss: 0.00001165
Iteration 35/1000 | Loss: 0.00001164
Iteration 36/1000 | Loss: 0.00001163
Iteration 37/1000 | Loss: 0.00001162
Iteration 38/1000 | Loss: 0.00001162
Iteration 39/1000 | Loss: 0.00001162
Iteration 40/1000 | Loss: 0.00001161
Iteration 41/1000 | Loss: 0.00001161
Iteration 42/1000 | Loss: 0.00001161
Iteration 43/1000 | Loss: 0.00001161
Iteration 44/1000 | Loss: 0.00001161
Iteration 45/1000 | Loss: 0.00001161
Iteration 46/1000 | Loss: 0.00001160
Iteration 47/1000 | Loss: 0.00001159
Iteration 48/1000 | Loss: 0.00001159
Iteration 49/1000 | Loss: 0.00001158
Iteration 50/1000 | Loss: 0.00001158
Iteration 51/1000 | Loss: 0.00001158
Iteration 52/1000 | Loss: 0.00001158
Iteration 53/1000 | Loss: 0.00001157
Iteration 54/1000 | Loss: 0.00001157
Iteration 55/1000 | Loss: 0.00001157
Iteration 56/1000 | Loss: 0.00001157
Iteration 57/1000 | Loss: 0.00001157
Iteration 58/1000 | Loss: 0.00001157
Iteration 59/1000 | Loss: 0.00001156
Iteration 60/1000 | Loss: 0.00001156
Iteration 61/1000 | Loss: 0.00001154
Iteration 62/1000 | Loss: 0.00001154
Iteration 63/1000 | Loss: 0.00001153
Iteration 64/1000 | Loss: 0.00001153
Iteration 65/1000 | Loss: 0.00001153
Iteration 66/1000 | Loss: 0.00001153
Iteration 67/1000 | Loss: 0.00001152
Iteration 68/1000 | Loss: 0.00001152
Iteration 69/1000 | Loss: 0.00001152
Iteration 70/1000 | Loss: 0.00001152
Iteration 71/1000 | Loss: 0.00001152
Iteration 72/1000 | Loss: 0.00001151
Iteration 73/1000 | Loss: 0.00001151
Iteration 74/1000 | Loss: 0.00001151
Iteration 75/1000 | Loss: 0.00001150
Iteration 76/1000 | Loss: 0.00001150
Iteration 77/1000 | Loss: 0.00001150
Iteration 78/1000 | Loss: 0.00001150
Iteration 79/1000 | Loss: 0.00001150
Iteration 80/1000 | Loss: 0.00001150
Iteration 81/1000 | Loss: 0.00001150
Iteration 82/1000 | Loss: 0.00001150
Iteration 83/1000 | Loss: 0.00001149
Iteration 84/1000 | Loss: 0.00001148
Iteration 85/1000 | Loss: 0.00001148
Iteration 86/1000 | Loss: 0.00001147
Iteration 87/1000 | Loss: 0.00001147
Iteration 88/1000 | Loss: 0.00001147
Iteration 89/1000 | Loss: 0.00001146
Iteration 90/1000 | Loss: 0.00001146
Iteration 91/1000 | Loss: 0.00001146
Iteration 92/1000 | Loss: 0.00001146
Iteration 93/1000 | Loss: 0.00001146
Iteration 94/1000 | Loss: 0.00001146
Iteration 95/1000 | Loss: 0.00001145
Iteration 96/1000 | Loss: 0.00001145
Iteration 97/1000 | Loss: 0.00001145
Iteration 98/1000 | Loss: 0.00001145
Iteration 99/1000 | Loss: 0.00001145
Iteration 100/1000 | Loss: 0.00001145
Iteration 101/1000 | Loss: 0.00001145
Iteration 102/1000 | Loss: 0.00001145
Iteration 103/1000 | Loss: 0.00001145
Iteration 104/1000 | Loss: 0.00001145
Iteration 105/1000 | Loss: 0.00001145
Iteration 106/1000 | Loss: 0.00001145
Iteration 107/1000 | Loss: 0.00001145
Iteration 108/1000 | Loss: 0.00001145
Iteration 109/1000 | Loss: 0.00001145
Iteration 110/1000 | Loss: 0.00001145
Iteration 111/1000 | Loss: 0.00001144
Iteration 112/1000 | Loss: 0.00001144
Iteration 113/1000 | Loss: 0.00001143
Iteration 114/1000 | Loss: 0.00001143
Iteration 115/1000 | Loss: 0.00001143
Iteration 116/1000 | Loss: 0.00001142
Iteration 117/1000 | Loss: 0.00001142
Iteration 118/1000 | Loss: 0.00001142
Iteration 119/1000 | Loss: 0.00001142
Iteration 120/1000 | Loss: 0.00001142
Iteration 121/1000 | Loss: 0.00001142
Iteration 122/1000 | Loss: 0.00001141
Iteration 123/1000 | Loss: 0.00001141
Iteration 124/1000 | Loss: 0.00001141
Iteration 125/1000 | Loss: 0.00001141
Iteration 126/1000 | Loss: 0.00001141
Iteration 127/1000 | Loss: 0.00001141
Iteration 128/1000 | Loss: 0.00001141
Iteration 129/1000 | Loss: 0.00001141
Iteration 130/1000 | Loss: 0.00001141
Iteration 131/1000 | Loss: 0.00001140
Iteration 132/1000 | Loss: 0.00001140
Iteration 133/1000 | Loss: 0.00001140
Iteration 134/1000 | Loss: 0.00001140
Iteration 135/1000 | Loss: 0.00001140
Iteration 136/1000 | Loss: 0.00001140
Iteration 137/1000 | Loss: 0.00001140
Iteration 138/1000 | Loss: 0.00001140
Iteration 139/1000 | Loss: 0.00001140
Iteration 140/1000 | Loss: 0.00001140
Iteration 141/1000 | Loss: 0.00001140
Iteration 142/1000 | Loss: 0.00001140
Iteration 143/1000 | Loss: 0.00001140
Iteration 144/1000 | Loss: 0.00001140
Iteration 145/1000 | Loss: 0.00001140
Iteration 146/1000 | Loss: 0.00001140
Iteration 147/1000 | Loss: 0.00001140
Iteration 148/1000 | Loss: 0.00001139
Iteration 149/1000 | Loss: 0.00001139
Iteration 150/1000 | Loss: 0.00001139
Iteration 151/1000 | Loss: 0.00001139
Iteration 152/1000 | Loss: 0.00001139
Iteration 153/1000 | Loss: 0.00001139
Iteration 154/1000 | Loss: 0.00001139
Iteration 155/1000 | Loss: 0.00001139
Iteration 156/1000 | Loss: 0.00001139
Iteration 157/1000 | Loss: 0.00001139
Iteration 158/1000 | Loss: 0.00001139
Iteration 159/1000 | Loss: 0.00001139
Iteration 160/1000 | Loss: 0.00001139
Iteration 161/1000 | Loss: 0.00001139
Iteration 162/1000 | Loss: 0.00001139
Iteration 163/1000 | Loss: 0.00001139
Iteration 164/1000 | Loss: 0.00001139
Iteration 165/1000 | Loss: 0.00001139
Iteration 166/1000 | Loss: 0.00001139
Iteration 167/1000 | Loss: 0.00001139
Iteration 168/1000 | Loss: 0.00001139
Iteration 169/1000 | Loss: 0.00001139
Iteration 170/1000 | Loss: 0.00001139
Iteration 171/1000 | Loss: 0.00001139
Iteration 172/1000 | Loss: 0.00001139
Iteration 173/1000 | Loss: 0.00001139
Iteration 174/1000 | Loss: 0.00001139
Iteration 175/1000 | Loss: 0.00001139
Iteration 176/1000 | Loss: 0.00001139
Iteration 177/1000 | Loss: 0.00001139
Iteration 178/1000 | Loss: 0.00001139
Iteration 179/1000 | Loss: 0.00001139
Iteration 180/1000 | Loss: 0.00001139
Iteration 181/1000 | Loss: 0.00001139
Iteration 182/1000 | Loss: 0.00001139
Iteration 183/1000 | Loss: 0.00001139
Iteration 184/1000 | Loss: 0.00001139
Iteration 185/1000 | Loss: 0.00001139
Iteration 186/1000 | Loss: 0.00001139
Iteration 187/1000 | Loss: 0.00001139
Iteration 188/1000 | Loss: 0.00001139
Iteration 189/1000 | Loss: 0.00001139
Iteration 190/1000 | Loss: 0.00001139
Iteration 191/1000 | Loss: 0.00001139
Iteration 192/1000 | Loss: 0.00001139
Iteration 193/1000 | Loss: 0.00001139
Iteration 194/1000 | Loss: 0.00001139
Iteration 195/1000 | Loss: 0.00001139
Iteration 196/1000 | Loss: 0.00001139
Iteration 197/1000 | Loss: 0.00001139
Iteration 198/1000 | Loss: 0.00001139
Iteration 199/1000 | Loss: 0.00001139
Iteration 200/1000 | Loss: 0.00001139
Iteration 201/1000 | Loss: 0.00001139
Iteration 202/1000 | Loss: 0.00001139
Iteration 203/1000 | Loss: 0.00001139
Iteration 204/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.1389422979846131e-05, 1.1389422979846131e-05, 1.1389422979846131e-05, 1.1389422979846131e-05, 1.1389422979846131e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1389422979846131e-05

Optimization complete. Final v2v error: 2.8968842029571533 mm

Highest mean error: 3.1626102924346924 mm for frame 121

Lowest mean error: 2.6530873775482178 mm for frame 8

Saving results

Total time: 42.693668365478516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00516876
Iteration 2/25 | Loss: 0.00131889
Iteration 3/25 | Loss: 0.00123731
Iteration 4/25 | Loss: 0.00122657
Iteration 5/25 | Loss: 0.00122486
Iteration 6/25 | Loss: 0.00122486
Iteration 7/25 | Loss: 0.00122486
Iteration 8/25 | Loss: 0.00122486
Iteration 9/25 | Loss: 0.00122486
Iteration 10/25 | Loss: 0.00122486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012248636921867728, 0.0012248636921867728, 0.0012248636921867728, 0.0012248636921867728, 0.0012248636921867728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012248636921867728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.69750118
Iteration 2/25 | Loss: 0.00150746
Iteration 3/25 | Loss: 0.00150745
Iteration 4/25 | Loss: 0.00150745
Iteration 5/25 | Loss: 0.00150745
Iteration 6/25 | Loss: 0.00150745
Iteration 7/25 | Loss: 0.00150745
Iteration 8/25 | Loss: 0.00150745
Iteration 9/25 | Loss: 0.00150745
Iteration 10/25 | Loss: 0.00150745
Iteration 11/25 | Loss: 0.00150745
Iteration 12/25 | Loss: 0.00150745
Iteration 13/25 | Loss: 0.00150745
Iteration 14/25 | Loss: 0.00150745
Iteration 15/25 | Loss: 0.00150745
Iteration 16/25 | Loss: 0.00150745
Iteration 17/25 | Loss: 0.00150745
Iteration 18/25 | Loss: 0.00150745
Iteration 19/25 | Loss: 0.00150745
Iteration 20/25 | Loss: 0.00150745
Iteration 21/25 | Loss: 0.00150745
Iteration 22/25 | Loss: 0.00150745
Iteration 23/25 | Loss: 0.00150745
Iteration 24/25 | Loss: 0.00150745
Iteration 25/25 | Loss: 0.00150745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150745
Iteration 2/1000 | Loss: 0.00002582
Iteration 3/1000 | Loss: 0.00001951
Iteration 4/1000 | Loss: 0.00001816
Iteration 5/1000 | Loss: 0.00001690
Iteration 6/1000 | Loss: 0.00001608
Iteration 7/1000 | Loss: 0.00001560
Iteration 8/1000 | Loss: 0.00001514
Iteration 9/1000 | Loss: 0.00001474
Iteration 10/1000 | Loss: 0.00001441
Iteration 11/1000 | Loss: 0.00001413
Iteration 12/1000 | Loss: 0.00001402
Iteration 13/1000 | Loss: 0.00001398
Iteration 14/1000 | Loss: 0.00001391
Iteration 15/1000 | Loss: 0.00001390
Iteration 16/1000 | Loss: 0.00001381
Iteration 17/1000 | Loss: 0.00001372
Iteration 18/1000 | Loss: 0.00001365
Iteration 19/1000 | Loss: 0.00001365
Iteration 20/1000 | Loss: 0.00001365
Iteration 21/1000 | Loss: 0.00001365
Iteration 22/1000 | Loss: 0.00001365
Iteration 23/1000 | Loss: 0.00001365
Iteration 24/1000 | Loss: 0.00001362
Iteration 25/1000 | Loss: 0.00001360
Iteration 26/1000 | Loss: 0.00001357
Iteration 27/1000 | Loss: 0.00001356
Iteration 28/1000 | Loss: 0.00001354
Iteration 29/1000 | Loss: 0.00001352
Iteration 30/1000 | Loss: 0.00001351
Iteration 31/1000 | Loss: 0.00001350
Iteration 32/1000 | Loss: 0.00001350
Iteration 33/1000 | Loss: 0.00001349
Iteration 34/1000 | Loss: 0.00001349
Iteration 35/1000 | Loss: 0.00001349
Iteration 36/1000 | Loss: 0.00001349
Iteration 37/1000 | Loss: 0.00001348
Iteration 38/1000 | Loss: 0.00001348
Iteration 39/1000 | Loss: 0.00001348
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001347
Iteration 42/1000 | Loss: 0.00001346
Iteration 43/1000 | Loss: 0.00001346
Iteration 44/1000 | Loss: 0.00001346
Iteration 45/1000 | Loss: 0.00001345
Iteration 46/1000 | Loss: 0.00001345
Iteration 47/1000 | Loss: 0.00001345
Iteration 48/1000 | Loss: 0.00001345
Iteration 49/1000 | Loss: 0.00001345
Iteration 50/1000 | Loss: 0.00001345
Iteration 51/1000 | Loss: 0.00001345
Iteration 52/1000 | Loss: 0.00001345
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001345
Iteration 55/1000 | Loss: 0.00001345
Iteration 56/1000 | Loss: 0.00001345
Iteration 57/1000 | Loss: 0.00001344
Iteration 58/1000 | Loss: 0.00001344
Iteration 59/1000 | Loss: 0.00001344
Iteration 60/1000 | Loss: 0.00001343
Iteration 61/1000 | Loss: 0.00001343
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001341
Iteration 64/1000 | Loss: 0.00001341
Iteration 65/1000 | Loss: 0.00001341
Iteration 66/1000 | Loss: 0.00001340
Iteration 67/1000 | Loss: 0.00001340
Iteration 68/1000 | Loss: 0.00001339
Iteration 69/1000 | Loss: 0.00001339
Iteration 70/1000 | Loss: 0.00001339
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001339
Iteration 74/1000 | Loss: 0.00001338
Iteration 75/1000 | Loss: 0.00001338
Iteration 76/1000 | Loss: 0.00001338
Iteration 77/1000 | Loss: 0.00001338
Iteration 78/1000 | Loss: 0.00001338
Iteration 79/1000 | Loss: 0.00001338
Iteration 80/1000 | Loss: 0.00001338
Iteration 81/1000 | Loss: 0.00001338
Iteration 82/1000 | Loss: 0.00001338
Iteration 83/1000 | Loss: 0.00001338
Iteration 84/1000 | Loss: 0.00001338
Iteration 85/1000 | Loss: 0.00001338
Iteration 86/1000 | Loss: 0.00001338
Iteration 87/1000 | Loss: 0.00001338
Iteration 88/1000 | Loss: 0.00001338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.3384415979089681e-05, 1.3384415979089681e-05, 1.3384415979089681e-05, 1.3384415979089681e-05, 1.3384415979089681e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3384415979089681e-05

Optimization complete. Final v2v error: 3.1327359676361084 mm

Highest mean error: 3.448695659637451 mm for frame 125

Lowest mean error: 2.897627592086792 mm for frame 255

Saving results

Total time: 38.2986855506897
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808808
Iteration 2/25 | Loss: 0.00148762
Iteration 3/25 | Loss: 0.00126805
Iteration 4/25 | Loss: 0.00125081
Iteration 5/25 | Loss: 0.00124783
Iteration 6/25 | Loss: 0.00124752
Iteration 7/25 | Loss: 0.00124752
Iteration 8/25 | Loss: 0.00124752
Iteration 9/25 | Loss: 0.00124752
Iteration 10/25 | Loss: 0.00124752
Iteration 11/25 | Loss: 0.00124752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012475168332457542, 0.0012475168332457542, 0.0012475168332457542, 0.0012475168332457542, 0.0012475168332457542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012475168332457542

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86200613
Iteration 2/25 | Loss: 0.00084538
Iteration 3/25 | Loss: 0.00084538
Iteration 4/25 | Loss: 0.00084538
Iteration 5/25 | Loss: 0.00084538
Iteration 6/25 | Loss: 0.00084538
Iteration 7/25 | Loss: 0.00084538
Iteration 8/25 | Loss: 0.00084538
Iteration 9/25 | Loss: 0.00084538
Iteration 10/25 | Loss: 0.00084538
Iteration 11/25 | Loss: 0.00084538
Iteration 12/25 | Loss: 0.00084538
Iteration 13/25 | Loss: 0.00084538
Iteration 14/25 | Loss: 0.00084538
Iteration 15/25 | Loss: 0.00084538
Iteration 16/25 | Loss: 0.00084538
Iteration 17/25 | Loss: 0.00084538
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008453793707303703, 0.0008453793707303703, 0.0008453793707303703, 0.0008453793707303703, 0.0008453793707303703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008453793707303703

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084538
Iteration 2/1000 | Loss: 0.00003484
Iteration 3/1000 | Loss: 0.00002573
Iteration 4/1000 | Loss: 0.00002352
Iteration 5/1000 | Loss: 0.00002264
Iteration 6/1000 | Loss: 0.00002205
Iteration 7/1000 | Loss: 0.00002140
Iteration 8/1000 | Loss: 0.00002106
Iteration 9/1000 | Loss: 0.00002071
Iteration 10/1000 | Loss: 0.00002063
Iteration 11/1000 | Loss: 0.00002041
Iteration 12/1000 | Loss: 0.00002026
Iteration 13/1000 | Loss: 0.00002017
Iteration 14/1000 | Loss: 0.00002004
Iteration 15/1000 | Loss: 0.00002001
Iteration 16/1000 | Loss: 0.00001991
Iteration 17/1000 | Loss: 0.00001990
Iteration 18/1000 | Loss: 0.00001990
Iteration 19/1000 | Loss: 0.00001990
Iteration 20/1000 | Loss: 0.00001990
Iteration 21/1000 | Loss: 0.00001990
Iteration 22/1000 | Loss: 0.00001990
Iteration 23/1000 | Loss: 0.00001990
Iteration 24/1000 | Loss: 0.00001990
Iteration 25/1000 | Loss: 0.00001989
Iteration 26/1000 | Loss: 0.00001989
Iteration 27/1000 | Loss: 0.00001981
Iteration 28/1000 | Loss: 0.00001979
Iteration 29/1000 | Loss: 0.00001978
Iteration 30/1000 | Loss: 0.00001978
Iteration 31/1000 | Loss: 0.00001976
Iteration 32/1000 | Loss: 0.00001976
Iteration 33/1000 | Loss: 0.00001976
Iteration 34/1000 | Loss: 0.00001975
Iteration 35/1000 | Loss: 0.00001975
Iteration 36/1000 | Loss: 0.00001973
Iteration 37/1000 | Loss: 0.00001968
Iteration 38/1000 | Loss: 0.00001968
Iteration 39/1000 | Loss: 0.00001968
Iteration 40/1000 | Loss: 0.00001968
Iteration 41/1000 | Loss: 0.00001968
Iteration 42/1000 | Loss: 0.00001968
Iteration 43/1000 | Loss: 0.00001968
Iteration 44/1000 | Loss: 0.00001967
Iteration 45/1000 | Loss: 0.00001967
Iteration 46/1000 | Loss: 0.00001967
Iteration 47/1000 | Loss: 0.00001967
Iteration 48/1000 | Loss: 0.00001966
Iteration 49/1000 | Loss: 0.00001966
Iteration 50/1000 | Loss: 0.00001966
Iteration 51/1000 | Loss: 0.00001966
Iteration 52/1000 | Loss: 0.00001966
Iteration 53/1000 | Loss: 0.00001965
Iteration 54/1000 | Loss: 0.00001965
Iteration 55/1000 | Loss: 0.00001964
Iteration 56/1000 | Loss: 0.00001964
Iteration 57/1000 | Loss: 0.00001963
Iteration 58/1000 | Loss: 0.00001963
Iteration 59/1000 | Loss: 0.00001963
Iteration 60/1000 | Loss: 0.00001963
Iteration 61/1000 | Loss: 0.00001962
Iteration 62/1000 | Loss: 0.00001962
Iteration 63/1000 | Loss: 0.00001961
Iteration 64/1000 | Loss: 0.00001961
Iteration 65/1000 | Loss: 0.00001960
Iteration 66/1000 | Loss: 0.00001960
Iteration 67/1000 | Loss: 0.00001959
Iteration 68/1000 | Loss: 0.00001959
Iteration 69/1000 | Loss: 0.00001959
Iteration 70/1000 | Loss: 0.00001958
Iteration 71/1000 | Loss: 0.00001957
Iteration 72/1000 | Loss: 0.00001957
Iteration 73/1000 | Loss: 0.00001957
Iteration 74/1000 | Loss: 0.00001956
Iteration 75/1000 | Loss: 0.00001956
Iteration 76/1000 | Loss: 0.00001956
Iteration 77/1000 | Loss: 0.00001955
Iteration 78/1000 | Loss: 0.00001955
Iteration 79/1000 | Loss: 0.00001955
Iteration 80/1000 | Loss: 0.00001955
Iteration 81/1000 | Loss: 0.00001955
Iteration 82/1000 | Loss: 0.00001954
Iteration 83/1000 | Loss: 0.00001954
Iteration 84/1000 | Loss: 0.00001954
Iteration 85/1000 | Loss: 0.00001954
Iteration 86/1000 | Loss: 0.00001954
Iteration 87/1000 | Loss: 0.00001954
Iteration 88/1000 | Loss: 0.00001954
Iteration 89/1000 | Loss: 0.00001954
Iteration 90/1000 | Loss: 0.00001953
Iteration 91/1000 | Loss: 0.00001953
Iteration 92/1000 | Loss: 0.00001953
Iteration 93/1000 | Loss: 0.00001953
Iteration 94/1000 | Loss: 0.00001953
Iteration 95/1000 | Loss: 0.00001953
Iteration 96/1000 | Loss: 0.00001953
Iteration 97/1000 | Loss: 0.00001953
Iteration 98/1000 | Loss: 0.00001952
Iteration 99/1000 | Loss: 0.00001952
Iteration 100/1000 | Loss: 0.00001951
Iteration 101/1000 | Loss: 0.00001951
Iteration 102/1000 | Loss: 0.00001951
Iteration 103/1000 | Loss: 0.00001951
Iteration 104/1000 | Loss: 0.00001951
Iteration 105/1000 | Loss: 0.00001951
Iteration 106/1000 | Loss: 0.00001950
Iteration 107/1000 | Loss: 0.00001949
Iteration 108/1000 | Loss: 0.00001949
Iteration 109/1000 | Loss: 0.00001949
Iteration 110/1000 | Loss: 0.00001949
Iteration 111/1000 | Loss: 0.00001949
Iteration 112/1000 | Loss: 0.00001949
Iteration 113/1000 | Loss: 0.00001949
Iteration 114/1000 | Loss: 0.00001949
Iteration 115/1000 | Loss: 0.00001949
Iteration 116/1000 | Loss: 0.00001949
Iteration 117/1000 | Loss: 0.00001949
Iteration 118/1000 | Loss: 0.00001949
Iteration 119/1000 | Loss: 0.00001948
Iteration 120/1000 | Loss: 0.00001948
Iteration 121/1000 | Loss: 0.00001948
Iteration 122/1000 | Loss: 0.00001948
Iteration 123/1000 | Loss: 0.00001948
Iteration 124/1000 | Loss: 0.00001948
Iteration 125/1000 | Loss: 0.00001948
Iteration 126/1000 | Loss: 0.00001948
Iteration 127/1000 | Loss: 0.00001948
Iteration 128/1000 | Loss: 0.00001947
Iteration 129/1000 | Loss: 0.00001947
Iteration 130/1000 | Loss: 0.00001947
Iteration 131/1000 | Loss: 0.00001947
Iteration 132/1000 | Loss: 0.00001947
Iteration 133/1000 | Loss: 0.00001947
Iteration 134/1000 | Loss: 0.00001947
Iteration 135/1000 | Loss: 0.00001947
Iteration 136/1000 | Loss: 0.00001947
Iteration 137/1000 | Loss: 0.00001946
Iteration 138/1000 | Loss: 0.00001946
Iteration 139/1000 | Loss: 0.00001946
Iteration 140/1000 | Loss: 0.00001946
Iteration 141/1000 | Loss: 0.00001946
Iteration 142/1000 | Loss: 0.00001946
Iteration 143/1000 | Loss: 0.00001946
Iteration 144/1000 | Loss: 0.00001946
Iteration 145/1000 | Loss: 0.00001946
Iteration 146/1000 | Loss: 0.00001946
Iteration 147/1000 | Loss: 0.00001946
Iteration 148/1000 | Loss: 0.00001946
Iteration 149/1000 | Loss: 0.00001946
Iteration 150/1000 | Loss: 0.00001946
Iteration 151/1000 | Loss: 0.00001946
Iteration 152/1000 | Loss: 0.00001946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.9458186216070317e-05, 1.9458186216070317e-05, 1.9458186216070317e-05, 1.9458186216070317e-05, 1.9458186216070317e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9458186216070317e-05

Optimization complete. Final v2v error: 3.6937384605407715 mm

Highest mean error: 3.8882107734680176 mm for frame 121

Lowest mean error: 3.5718202590942383 mm for frame 136

Saving results

Total time: 39.33990669250488
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772038
Iteration 2/25 | Loss: 0.00153665
Iteration 3/25 | Loss: 0.00130079
Iteration 4/25 | Loss: 0.00126473
Iteration 5/25 | Loss: 0.00125952
Iteration 6/25 | Loss: 0.00125871
Iteration 7/25 | Loss: 0.00125871
Iteration 8/25 | Loss: 0.00125871
Iteration 9/25 | Loss: 0.00125871
Iteration 10/25 | Loss: 0.00125871
Iteration 11/25 | Loss: 0.00125871
Iteration 12/25 | Loss: 0.00125871
Iteration 13/25 | Loss: 0.00125871
Iteration 14/25 | Loss: 0.00125871
Iteration 15/25 | Loss: 0.00125871
Iteration 16/25 | Loss: 0.00125871
Iteration 17/25 | Loss: 0.00125871
Iteration 18/25 | Loss: 0.00125871
Iteration 19/25 | Loss: 0.00125871
Iteration 20/25 | Loss: 0.00125871
Iteration 21/25 | Loss: 0.00125871
Iteration 22/25 | Loss: 0.00125871
Iteration 23/25 | Loss: 0.00125871
Iteration 24/25 | Loss: 0.00125871
Iteration 25/25 | Loss: 0.00125871

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16251361
Iteration 2/25 | Loss: 0.00133638
Iteration 3/25 | Loss: 0.00133636
Iteration 4/25 | Loss: 0.00133636
Iteration 5/25 | Loss: 0.00133636
Iteration 6/25 | Loss: 0.00133636
Iteration 7/25 | Loss: 0.00133636
Iteration 8/25 | Loss: 0.00133636
Iteration 9/25 | Loss: 0.00133636
Iteration 10/25 | Loss: 0.00133636
Iteration 11/25 | Loss: 0.00133636
Iteration 12/25 | Loss: 0.00133636
Iteration 13/25 | Loss: 0.00133636
Iteration 14/25 | Loss: 0.00133636
Iteration 15/25 | Loss: 0.00133636
Iteration 16/25 | Loss: 0.00133636
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013363610487431288, 0.0013363610487431288, 0.0013363610487431288, 0.0013363610487431288, 0.0013363610487431288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013363610487431288

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133636
Iteration 2/1000 | Loss: 0.00005126
Iteration 3/1000 | Loss: 0.00003289
Iteration 4/1000 | Loss: 0.00002757
Iteration 5/1000 | Loss: 0.00002613
Iteration 6/1000 | Loss: 0.00002476
Iteration 7/1000 | Loss: 0.00002383
Iteration 8/1000 | Loss: 0.00002345
Iteration 9/1000 | Loss: 0.00002307
Iteration 10/1000 | Loss: 0.00002263
Iteration 11/1000 | Loss: 0.00002222
Iteration 12/1000 | Loss: 0.00002198
Iteration 13/1000 | Loss: 0.00002173
Iteration 14/1000 | Loss: 0.00002150
Iteration 15/1000 | Loss: 0.00002135
Iteration 16/1000 | Loss: 0.00002117
Iteration 17/1000 | Loss: 0.00002114
Iteration 18/1000 | Loss: 0.00002113
Iteration 19/1000 | Loss: 0.00002112
Iteration 20/1000 | Loss: 0.00002110
Iteration 21/1000 | Loss: 0.00002105
Iteration 22/1000 | Loss: 0.00002102
Iteration 23/1000 | Loss: 0.00002099
Iteration 24/1000 | Loss: 0.00002097
Iteration 25/1000 | Loss: 0.00002096
Iteration 26/1000 | Loss: 0.00002095
Iteration 27/1000 | Loss: 0.00002095
Iteration 28/1000 | Loss: 0.00002095
Iteration 29/1000 | Loss: 0.00002094
Iteration 30/1000 | Loss: 0.00002094
Iteration 31/1000 | Loss: 0.00002094
Iteration 32/1000 | Loss: 0.00002094
Iteration 33/1000 | Loss: 0.00002093
Iteration 34/1000 | Loss: 0.00002093
Iteration 35/1000 | Loss: 0.00002092
Iteration 36/1000 | Loss: 0.00002092
Iteration 37/1000 | Loss: 0.00002092
Iteration 38/1000 | Loss: 0.00002091
Iteration 39/1000 | Loss: 0.00002091
Iteration 40/1000 | Loss: 0.00002090
Iteration 41/1000 | Loss: 0.00002090
Iteration 42/1000 | Loss: 0.00002090
Iteration 43/1000 | Loss: 0.00002090
Iteration 44/1000 | Loss: 0.00002089
Iteration 45/1000 | Loss: 0.00002089
Iteration 46/1000 | Loss: 0.00002089
Iteration 47/1000 | Loss: 0.00002088
Iteration 48/1000 | Loss: 0.00002088
Iteration 49/1000 | Loss: 0.00002088
Iteration 50/1000 | Loss: 0.00002087
Iteration 51/1000 | Loss: 0.00002087
Iteration 52/1000 | Loss: 0.00002086
Iteration 53/1000 | Loss: 0.00002086
Iteration 54/1000 | Loss: 0.00002086
Iteration 55/1000 | Loss: 0.00002086
Iteration 56/1000 | Loss: 0.00002086
Iteration 57/1000 | Loss: 0.00002086
Iteration 58/1000 | Loss: 0.00002086
Iteration 59/1000 | Loss: 0.00002086
Iteration 60/1000 | Loss: 0.00002086
Iteration 61/1000 | Loss: 0.00002086
Iteration 62/1000 | Loss: 0.00002086
Iteration 63/1000 | Loss: 0.00002085
Iteration 64/1000 | Loss: 0.00002085
Iteration 65/1000 | Loss: 0.00002085
Iteration 66/1000 | Loss: 0.00002085
Iteration 67/1000 | Loss: 0.00002085
Iteration 68/1000 | Loss: 0.00002085
Iteration 69/1000 | Loss: 0.00002085
Iteration 70/1000 | Loss: 0.00002084
Iteration 71/1000 | Loss: 0.00002084
Iteration 72/1000 | Loss: 0.00002084
Iteration 73/1000 | Loss: 0.00002084
Iteration 74/1000 | Loss: 0.00002084
Iteration 75/1000 | Loss: 0.00002084
Iteration 76/1000 | Loss: 0.00002083
Iteration 77/1000 | Loss: 0.00002083
Iteration 78/1000 | Loss: 0.00002083
Iteration 79/1000 | Loss: 0.00002083
Iteration 80/1000 | Loss: 0.00002083
Iteration 81/1000 | Loss: 0.00002083
Iteration 82/1000 | Loss: 0.00002083
Iteration 83/1000 | Loss: 0.00002083
Iteration 84/1000 | Loss: 0.00002083
Iteration 85/1000 | Loss: 0.00002083
Iteration 86/1000 | Loss: 0.00002083
Iteration 87/1000 | Loss: 0.00002083
Iteration 88/1000 | Loss: 0.00002083
Iteration 89/1000 | Loss: 0.00002083
Iteration 90/1000 | Loss: 0.00002083
Iteration 91/1000 | Loss: 0.00002083
Iteration 92/1000 | Loss: 0.00002083
Iteration 93/1000 | Loss: 0.00002082
Iteration 94/1000 | Loss: 0.00002082
Iteration 95/1000 | Loss: 0.00002082
Iteration 96/1000 | Loss: 0.00002082
Iteration 97/1000 | Loss: 0.00002081
Iteration 98/1000 | Loss: 0.00002081
Iteration 99/1000 | Loss: 0.00002081
Iteration 100/1000 | Loss: 0.00002081
Iteration 101/1000 | Loss: 0.00002081
Iteration 102/1000 | Loss: 0.00002081
Iteration 103/1000 | Loss: 0.00002080
Iteration 104/1000 | Loss: 0.00002080
Iteration 105/1000 | Loss: 0.00002080
Iteration 106/1000 | Loss: 0.00002080
Iteration 107/1000 | Loss: 0.00002080
Iteration 108/1000 | Loss: 0.00002080
Iteration 109/1000 | Loss: 0.00002080
Iteration 110/1000 | Loss: 0.00002080
Iteration 111/1000 | Loss: 0.00002080
Iteration 112/1000 | Loss: 0.00002080
Iteration 113/1000 | Loss: 0.00002080
Iteration 114/1000 | Loss: 0.00002080
Iteration 115/1000 | Loss: 0.00002080
Iteration 116/1000 | Loss: 0.00002080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.0796993339899927e-05, 2.0796993339899927e-05, 2.0796993339899927e-05, 2.0796993339899927e-05, 2.0796993339899927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0796993339899927e-05

Optimization complete. Final v2v error: 3.806734085083008 mm

Highest mean error: 4.864767074584961 mm for frame 139

Lowest mean error: 3.240507125854492 mm for frame 58

Saving results

Total time: 45.42682957649231
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836523
Iteration 2/25 | Loss: 0.00161159
Iteration 3/25 | Loss: 0.00138474
Iteration 4/25 | Loss: 0.00134989
Iteration 5/25 | Loss: 0.00133838
Iteration 6/25 | Loss: 0.00133595
Iteration 7/25 | Loss: 0.00132441
Iteration 8/25 | Loss: 0.00127124
Iteration 9/25 | Loss: 0.00126003
Iteration 10/25 | Loss: 0.00126264
Iteration 11/25 | Loss: 0.00124064
Iteration 12/25 | Loss: 0.00123172
Iteration 13/25 | Loss: 0.00123048
Iteration 14/25 | Loss: 0.00123039
Iteration 15/25 | Loss: 0.00123039
Iteration 16/25 | Loss: 0.00123039
Iteration 17/25 | Loss: 0.00123039
Iteration 18/25 | Loss: 0.00123039
Iteration 19/25 | Loss: 0.00123039
Iteration 20/25 | Loss: 0.00123038
Iteration 21/25 | Loss: 0.00123038
Iteration 22/25 | Loss: 0.00123038
Iteration 23/25 | Loss: 0.00123038
Iteration 24/25 | Loss: 0.00123038
Iteration 25/25 | Loss: 0.00123038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87225169
Iteration 2/25 | Loss: 0.00085802
Iteration 3/25 | Loss: 0.00085801
Iteration 4/25 | Loss: 0.00085801
Iteration 5/25 | Loss: 0.00085801
Iteration 6/25 | Loss: 0.00085801
Iteration 7/25 | Loss: 0.00085801
Iteration 8/25 | Loss: 0.00085801
Iteration 9/25 | Loss: 0.00085801
Iteration 10/25 | Loss: 0.00085801
Iteration 11/25 | Loss: 0.00085801
Iteration 12/25 | Loss: 0.00085801
Iteration 13/25 | Loss: 0.00085801
Iteration 14/25 | Loss: 0.00085801
Iteration 15/25 | Loss: 0.00085801
Iteration 16/25 | Loss: 0.00085801
Iteration 17/25 | Loss: 0.00085801
Iteration 18/25 | Loss: 0.00085801
Iteration 19/25 | Loss: 0.00085801
Iteration 20/25 | Loss: 0.00085801
Iteration 21/25 | Loss: 0.00085801
Iteration 22/25 | Loss: 0.00085801
Iteration 23/25 | Loss: 0.00085801
Iteration 24/25 | Loss: 0.00085801
Iteration 25/25 | Loss: 0.00085801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085801
Iteration 2/1000 | Loss: 0.00003361
Iteration 3/1000 | Loss: 0.00002529
Iteration 4/1000 | Loss: 0.00002258
Iteration 5/1000 | Loss: 0.00002174
Iteration 6/1000 | Loss: 0.00002102
Iteration 7/1000 | Loss: 0.00002040
Iteration 8/1000 | Loss: 0.00002002
Iteration 9/1000 | Loss: 0.00001966
Iteration 10/1000 | Loss: 0.00001929
Iteration 11/1000 | Loss: 0.00001908
Iteration 12/1000 | Loss: 0.00001902
Iteration 13/1000 | Loss: 0.00001886
Iteration 14/1000 | Loss: 0.00001881
Iteration 15/1000 | Loss: 0.00001873
Iteration 16/1000 | Loss: 0.00001872
Iteration 17/1000 | Loss: 0.00001872
Iteration 18/1000 | Loss: 0.00001871
Iteration 19/1000 | Loss: 0.00001869
Iteration 20/1000 | Loss: 0.00001869
Iteration 21/1000 | Loss: 0.00001868
Iteration 22/1000 | Loss: 0.00001868
Iteration 23/1000 | Loss: 0.00001867
Iteration 24/1000 | Loss: 0.00001866
Iteration 25/1000 | Loss: 0.00001866
Iteration 26/1000 | Loss: 0.00001866
Iteration 27/1000 | Loss: 0.00001866
Iteration 28/1000 | Loss: 0.00001866
Iteration 29/1000 | Loss: 0.00001866
Iteration 30/1000 | Loss: 0.00001866
Iteration 31/1000 | Loss: 0.00001866
Iteration 32/1000 | Loss: 0.00001866
Iteration 33/1000 | Loss: 0.00001863
Iteration 34/1000 | Loss: 0.00001863
Iteration 35/1000 | Loss: 0.00001860
Iteration 36/1000 | Loss: 0.00001860
Iteration 37/1000 | Loss: 0.00001859
Iteration 38/1000 | Loss: 0.00001859
Iteration 39/1000 | Loss: 0.00001859
Iteration 40/1000 | Loss: 0.00001859
Iteration 41/1000 | Loss: 0.00001859
Iteration 42/1000 | Loss: 0.00001858
Iteration 43/1000 | Loss: 0.00001858
Iteration 44/1000 | Loss: 0.00001858
Iteration 45/1000 | Loss: 0.00001857
Iteration 46/1000 | Loss: 0.00001856
Iteration 47/1000 | Loss: 0.00001856
Iteration 48/1000 | Loss: 0.00001856
Iteration 49/1000 | Loss: 0.00001855
Iteration 50/1000 | Loss: 0.00001855
Iteration 51/1000 | Loss: 0.00001854
Iteration 52/1000 | Loss: 0.00001854
Iteration 53/1000 | Loss: 0.00001853
Iteration 54/1000 | Loss: 0.00001852
Iteration 55/1000 | Loss: 0.00001852
Iteration 56/1000 | Loss: 0.00001852
Iteration 57/1000 | Loss: 0.00001851
Iteration 58/1000 | Loss: 0.00001851
Iteration 59/1000 | Loss: 0.00001851
Iteration 60/1000 | Loss: 0.00001851
Iteration 61/1000 | Loss: 0.00001851
Iteration 62/1000 | Loss: 0.00001851
Iteration 63/1000 | Loss: 0.00001851
Iteration 64/1000 | Loss: 0.00001851
Iteration 65/1000 | Loss: 0.00001850
Iteration 66/1000 | Loss: 0.00001850
Iteration 67/1000 | Loss: 0.00001850
Iteration 68/1000 | Loss: 0.00001850
Iteration 69/1000 | Loss: 0.00001850
Iteration 70/1000 | Loss: 0.00001850
Iteration 71/1000 | Loss: 0.00001850
Iteration 72/1000 | Loss: 0.00001850
Iteration 73/1000 | Loss: 0.00001850
Iteration 74/1000 | Loss: 0.00001849
Iteration 75/1000 | Loss: 0.00001849
Iteration 76/1000 | Loss: 0.00001849
Iteration 77/1000 | Loss: 0.00001849
Iteration 78/1000 | Loss: 0.00001849
Iteration 79/1000 | Loss: 0.00001849
Iteration 80/1000 | Loss: 0.00001849
Iteration 81/1000 | Loss: 0.00001849
Iteration 82/1000 | Loss: 0.00001849
Iteration 83/1000 | Loss: 0.00001849
Iteration 84/1000 | Loss: 0.00001849
Iteration 85/1000 | Loss: 0.00001848
Iteration 86/1000 | Loss: 0.00001848
Iteration 87/1000 | Loss: 0.00001848
Iteration 88/1000 | Loss: 0.00001847
Iteration 89/1000 | Loss: 0.00001847
Iteration 90/1000 | Loss: 0.00001847
Iteration 91/1000 | Loss: 0.00001846
Iteration 92/1000 | Loss: 0.00001846
Iteration 93/1000 | Loss: 0.00001846
Iteration 94/1000 | Loss: 0.00001846
Iteration 95/1000 | Loss: 0.00001846
Iteration 96/1000 | Loss: 0.00001846
Iteration 97/1000 | Loss: 0.00001846
Iteration 98/1000 | Loss: 0.00001846
Iteration 99/1000 | Loss: 0.00001846
Iteration 100/1000 | Loss: 0.00001846
Iteration 101/1000 | Loss: 0.00001845
Iteration 102/1000 | Loss: 0.00001845
Iteration 103/1000 | Loss: 0.00001845
Iteration 104/1000 | Loss: 0.00001844
Iteration 105/1000 | Loss: 0.00001844
Iteration 106/1000 | Loss: 0.00001844
Iteration 107/1000 | Loss: 0.00001844
Iteration 108/1000 | Loss: 0.00001844
Iteration 109/1000 | Loss: 0.00001844
Iteration 110/1000 | Loss: 0.00001844
Iteration 111/1000 | Loss: 0.00001844
Iteration 112/1000 | Loss: 0.00001844
Iteration 113/1000 | Loss: 0.00001843
Iteration 114/1000 | Loss: 0.00001843
Iteration 115/1000 | Loss: 0.00001843
Iteration 116/1000 | Loss: 0.00001843
Iteration 117/1000 | Loss: 0.00001843
Iteration 118/1000 | Loss: 0.00001843
Iteration 119/1000 | Loss: 0.00001843
Iteration 120/1000 | Loss: 0.00001843
Iteration 121/1000 | Loss: 0.00001843
Iteration 122/1000 | Loss: 0.00001843
Iteration 123/1000 | Loss: 0.00001843
Iteration 124/1000 | Loss: 0.00001843
Iteration 125/1000 | Loss: 0.00001843
Iteration 126/1000 | Loss: 0.00001843
Iteration 127/1000 | Loss: 0.00001843
Iteration 128/1000 | Loss: 0.00001843
Iteration 129/1000 | Loss: 0.00001843
Iteration 130/1000 | Loss: 0.00001843
Iteration 131/1000 | Loss: 0.00001843
Iteration 132/1000 | Loss: 0.00001843
Iteration 133/1000 | Loss: 0.00001843
Iteration 134/1000 | Loss: 0.00001843
Iteration 135/1000 | Loss: 0.00001843
Iteration 136/1000 | Loss: 0.00001843
Iteration 137/1000 | Loss: 0.00001843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.842967867560219e-05, 1.842967867560219e-05, 1.842967867560219e-05, 1.842967867560219e-05, 1.842967867560219e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.842967867560219e-05

Optimization complete. Final v2v error: 3.651686429977417 mm

Highest mean error: 3.839053153991699 mm for frame 20

Lowest mean error: 3.5335633754730225 mm for frame 0

Saving results

Total time: 52.31007742881775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00699639
Iteration 2/25 | Loss: 0.00161586
Iteration 3/25 | Loss: 0.00134622
Iteration 4/25 | Loss: 0.00129917
Iteration 5/25 | Loss: 0.00128843
Iteration 6/25 | Loss: 0.00128368
Iteration 7/25 | Loss: 0.00128206
Iteration 8/25 | Loss: 0.00128162
Iteration 9/25 | Loss: 0.00128146
Iteration 10/25 | Loss: 0.00128144
Iteration 11/25 | Loss: 0.00128144
Iteration 12/25 | Loss: 0.00128144
Iteration 13/25 | Loss: 0.00128144
Iteration 14/25 | Loss: 0.00128144
Iteration 15/25 | Loss: 0.00128144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001281442935578525, 0.001281442935578525, 0.001281442935578525, 0.001281442935578525, 0.001281442935578525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001281442935578525

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.44584036
Iteration 2/25 | Loss: 0.00204518
Iteration 3/25 | Loss: 0.00204514
Iteration 4/25 | Loss: 0.00204514
Iteration 5/25 | Loss: 0.00204514
Iteration 6/25 | Loss: 0.00204514
Iteration 7/25 | Loss: 0.00204513
Iteration 8/25 | Loss: 0.00204513
Iteration 9/25 | Loss: 0.00204513
Iteration 10/25 | Loss: 0.00204513
Iteration 11/25 | Loss: 0.00204513
Iteration 12/25 | Loss: 0.00204513
Iteration 13/25 | Loss: 0.00204513
Iteration 14/25 | Loss: 0.00204513
Iteration 15/25 | Loss: 0.00204513
Iteration 16/25 | Loss: 0.00204513
Iteration 17/25 | Loss: 0.00204513
Iteration 18/25 | Loss: 0.00204513
Iteration 19/25 | Loss: 0.00204513
Iteration 20/25 | Loss: 0.00204513
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0020451333839446306, 0.0020451333839446306, 0.0020451333839446306, 0.0020451333839446306, 0.0020451333839446306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020451333839446306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204513
Iteration 2/1000 | Loss: 0.00008725
Iteration 3/1000 | Loss: 0.00004714
Iteration 4/1000 | Loss: 0.00003452
Iteration 5/1000 | Loss: 0.00003114
Iteration 6/1000 | Loss: 0.00002842
Iteration 7/1000 | Loss: 0.00004413
Iteration 8/1000 | Loss: 0.00002815
Iteration 9/1000 | Loss: 0.00002635
Iteration 10/1000 | Loss: 0.00002561
Iteration 11/1000 | Loss: 0.00003185
Iteration 12/1000 | Loss: 0.00002699
Iteration 13/1000 | Loss: 0.00002495
Iteration 14/1000 | Loss: 0.00002446
Iteration 15/1000 | Loss: 0.00002390
Iteration 16/1000 | Loss: 0.00002341
Iteration 17/1000 | Loss: 0.00002304
Iteration 18/1000 | Loss: 0.00002260
Iteration 19/1000 | Loss: 0.00002236
Iteration 20/1000 | Loss: 0.00002235
Iteration 21/1000 | Loss: 0.00002216
Iteration 22/1000 | Loss: 0.00002188
Iteration 23/1000 | Loss: 0.00002164
Iteration 24/1000 | Loss: 0.00002143
Iteration 25/1000 | Loss: 0.00002134
Iteration 26/1000 | Loss: 0.00002120
Iteration 27/1000 | Loss: 0.00002116
Iteration 28/1000 | Loss: 0.00002115
Iteration 29/1000 | Loss: 0.00002115
Iteration 30/1000 | Loss: 0.00002112
Iteration 31/1000 | Loss: 0.00002112
Iteration 32/1000 | Loss: 0.00002111
Iteration 33/1000 | Loss: 0.00002111
Iteration 34/1000 | Loss: 0.00002110
Iteration 35/1000 | Loss: 0.00002110
Iteration 36/1000 | Loss: 0.00002109
Iteration 37/1000 | Loss: 0.00002109
Iteration 38/1000 | Loss: 0.00002108
Iteration 39/1000 | Loss: 0.00002108
Iteration 40/1000 | Loss: 0.00002105
Iteration 41/1000 | Loss: 0.00002104
Iteration 42/1000 | Loss: 0.00002104
Iteration 43/1000 | Loss: 0.00002104
Iteration 44/1000 | Loss: 0.00002103
Iteration 45/1000 | Loss: 0.00002103
Iteration 46/1000 | Loss: 0.00002103
Iteration 47/1000 | Loss: 0.00002103
Iteration 48/1000 | Loss: 0.00002102
Iteration 49/1000 | Loss: 0.00002102
Iteration 50/1000 | Loss: 0.00002102
Iteration 51/1000 | Loss: 0.00002102
Iteration 52/1000 | Loss: 0.00002101
Iteration 53/1000 | Loss: 0.00002101
Iteration 54/1000 | Loss: 0.00002100
Iteration 55/1000 | Loss: 0.00002099
Iteration 56/1000 | Loss: 0.00002099
Iteration 57/1000 | Loss: 0.00002099
Iteration 58/1000 | Loss: 0.00002098
Iteration 59/1000 | Loss: 0.00002098
Iteration 60/1000 | Loss: 0.00002098
Iteration 61/1000 | Loss: 0.00002098
Iteration 62/1000 | Loss: 0.00002098
Iteration 63/1000 | Loss: 0.00002098
Iteration 64/1000 | Loss: 0.00002098
Iteration 65/1000 | Loss: 0.00002097
Iteration 66/1000 | Loss: 0.00002097
Iteration 67/1000 | Loss: 0.00002095
Iteration 68/1000 | Loss: 0.00002095
Iteration 69/1000 | Loss: 0.00002094
Iteration 70/1000 | Loss: 0.00002093
Iteration 71/1000 | Loss: 0.00002092
Iteration 72/1000 | Loss: 0.00002092
Iteration 73/1000 | Loss: 0.00002092
Iteration 74/1000 | Loss: 0.00002092
Iteration 75/1000 | Loss: 0.00002092
Iteration 76/1000 | Loss: 0.00002092
Iteration 77/1000 | Loss: 0.00002092
Iteration 78/1000 | Loss: 0.00002092
Iteration 79/1000 | Loss: 0.00002092
Iteration 80/1000 | Loss: 0.00002091
Iteration 81/1000 | Loss: 0.00002091
Iteration 82/1000 | Loss: 0.00002091
Iteration 83/1000 | Loss: 0.00002091
Iteration 84/1000 | Loss: 0.00002091
Iteration 85/1000 | Loss: 0.00002091
Iteration 86/1000 | Loss: 0.00002091
Iteration 87/1000 | Loss: 0.00002091
Iteration 88/1000 | Loss: 0.00002091
Iteration 89/1000 | Loss: 0.00002090
Iteration 90/1000 | Loss: 0.00002090
Iteration 91/1000 | Loss: 0.00002090
Iteration 92/1000 | Loss: 0.00002089
Iteration 93/1000 | Loss: 0.00002089
Iteration 94/1000 | Loss: 0.00002089
Iteration 95/1000 | Loss: 0.00002088
Iteration 96/1000 | Loss: 0.00002088
Iteration 97/1000 | Loss: 0.00002088
Iteration 98/1000 | Loss: 0.00002088
Iteration 99/1000 | Loss: 0.00002088
Iteration 100/1000 | Loss: 0.00002088
Iteration 101/1000 | Loss: 0.00002088
Iteration 102/1000 | Loss: 0.00002088
Iteration 103/1000 | Loss: 0.00002088
Iteration 104/1000 | Loss: 0.00002087
Iteration 105/1000 | Loss: 0.00002087
Iteration 106/1000 | Loss: 0.00002087
Iteration 107/1000 | Loss: 0.00002087
Iteration 108/1000 | Loss: 0.00002087
Iteration 109/1000 | Loss: 0.00002086
Iteration 110/1000 | Loss: 0.00002084
Iteration 111/1000 | Loss: 0.00002084
Iteration 112/1000 | Loss: 0.00002084
Iteration 113/1000 | Loss: 0.00002084
Iteration 114/1000 | Loss: 0.00002084
Iteration 115/1000 | Loss: 0.00002084
Iteration 116/1000 | Loss: 0.00002084
Iteration 117/1000 | Loss: 0.00002083
Iteration 118/1000 | Loss: 0.00002082
Iteration 119/1000 | Loss: 0.00002082
Iteration 120/1000 | Loss: 0.00002082
Iteration 121/1000 | Loss: 0.00002081
Iteration 122/1000 | Loss: 0.00002081
Iteration 123/1000 | Loss: 0.00002081
Iteration 124/1000 | Loss: 0.00002080
Iteration 125/1000 | Loss: 0.00002080
Iteration 126/1000 | Loss: 0.00002080
Iteration 127/1000 | Loss: 0.00002079
Iteration 128/1000 | Loss: 0.00002079
Iteration 129/1000 | Loss: 0.00002079
Iteration 130/1000 | Loss: 0.00002078
Iteration 131/1000 | Loss: 0.00002078
Iteration 132/1000 | Loss: 0.00002078
Iteration 133/1000 | Loss: 0.00002077
Iteration 134/1000 | Loss: 0.00002077
Iteration 135/1000 | Loss: 0.00002077
Iteration 136/1000 | Loss: 0.00002077
Iteration 137/1000 | Loss: 0.00002076
Iteration 138/1000 | Loss: 0.00002076
Iteration 139/1000 | Loss: 0.00002076
Iteration 140/1000 | Loss: 0.00002076
Iteration 141/1000 | Loss: 0.00002075
Iteration 142/1000 | Loss: 0.00002075
Iteration 143/1000 | Loss: 0.00002075
Iteration 144/1000 | Loss: 0.00002075
Iteration 145/1000 | Loss: 0.00002075
Iteration 146/1000 | Loss: 0.00002075
Iteration 147/1000 | Loss: 0.00002075
Iteration 148/1000 | Loss: 0.00002075
Iteration 149/1000 | Loss: 0.00002075
Iteration 150/1000 | Loss: 0.00002075
Iteration 151/1000 | Loss: 0.00002075
Iteration 152/1000 | Loss: 0.00002075
Iteration 153/1000 | Loss: 0.00002075
Iteration 154/1000 | Loss: 0.00002075
Iteration 155/1000 | Loss: 0.00002075
Iteration 156/1000 | Loss: 0.00002075
Iteration 157/1000 | Loss: 0.00002074
Iteration 158/1000 | Loss: 0.00002074
Iteration 159/1000 | Loss: 0.00002074
Iteration 160/1000 | Loss: 0.00002074
Iteration 161/1000 | Loss: 0.00002074
Iteration 162/1000 | Loss: 0.00002074
Iteration 163/1000 | Loss: 0.00002074
Iteration 164/1000 | Loss: 0.00002074
Iteration 165/1000 | Loss: 0.00002074
Iteration 166/1000 | Loss: 0.00002074
Iteration 167/1000 | Loss: 0.00002074
Iteration 168/1000 | Loss: 0.00002074
Iteration 169/1000 | Loss: 0.00002074
Iteration 170/1000 | Loss: 0.00002074
Iteration 171/1000 | Loss: 0.00002074
Iteration 172/1000 | Loss: 0.00002074
Iteration 173/1000 | Loss: 0.00002074
Iteration 174/1000 | Loss: 0.00002074
Iteration 175/1000 | Loss: 0.00002074
Iteration 176/1000 | Loss: 0.00002074
Iteration 177/1000 | Loss: 0.00002074
Iteration 178/1000 | Loss: 0.00002074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.07434750336688e-05, 2.07434750336688e-05, 2.07434750336688e-05, 2.07434750336688e-05, 2.07434750336688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.07434750336688e-05

Optimization complete. Final v2v error: 3.733513593673706 mm

Highest mean error: 5.298060417175293 mm for frame 38

Lowest mean error: 2.803130626678467 mm for frame 212

Saving results

Total time: 70.75125503540039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965705
Iteration 2/25 | Loss: 0.00249606
Iteration 3/25 | Loss: 0.00185387
Iteration 4/25 | Loss: 0.00178432
Iteration 5/25 | Loss: 0.00167513
Iteration 6/25 | Loss: 0.00159162
Iteration 7/25 | Loss: 0.00147157
Iteration 8/25 | Loss: 0.00146033
Iteration 9/25 | Loss: 0.00139561
Iteration 10/25 | Loss: 0.00136180
Iteration 11/25 | Loss: 0.00135450
Iteration 12/25 | Loss: 0.00135310
Iteration 13/25 | Loss: 0.00135277
Iteration 14/25 | Loss: 0.00135220
Iteration 15/25 | Loss: 0.00135074
Iteration 16/25 | Loss: 0.00135277
Iteration 17/25 | Loss: 0.00135059
Iteration 18/25 | Loss: 0.00134624
Iteration 19/25 | Loss: 0.00135345
Iteration 20/25 | Loss: 0.00134950
Iteration 21/25 | Loss: 0.00135186
Iteration 22/25 | Loss: 0.00134442
Iteration 23/25 | Loss: 0.00135354
Iteration 24/25 | Loss: 0.00134652
Iteration 25/25 | Loss: 0.00135180

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24948251
Iteration 2/25 | Loss: 0.00262464
Iteration 3/25 | Loss: 0.00262464
Iteration 4/25 | Loss: 0.00262464
Iteration 5/25 | Loss: 0.00262464
Iteration 6/25 | Loss: 0.00262464
Iteration 7/25 | Loss: 0.00262464
Iteration 8/25 | Loss: 0.00262464
Iteration 9/25 | Loss: 0.00262464
Iteration 10/25 | Loss: 0.00262464
Iteration 11/25 | Loss: 0.00262464
Iteration 12/25 | Loss: 0.00262464
Iteration 13/25 | Loss: 0.00262464
Iteration 14/25 | Loss: 0.00262464
Iteration 15/25 | Loss: 0.00262464
Iteration 16/25 | Loss: 0.00262464
Iteration 17/25 | Loss: 0.00262464
Iteration 18/25 | Loss: 0.00262464
Iteration 19/25 | Loss: 0.00262464
Iteration 20/25 | Loss: 0.00262464
Iteration 21/25 | Loss: 0.00262464
Iteration 22/25 | Loss: 0.00262464
Iteration 23/25 | Loss: 0.00262464
Iteration 24/25 | Loss: 0.00262464
Iteration 25/25 | Loss: 0.00262464
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.002624636748805642, 0.002624636748805642, 0.002624636748805642, 0.002624636748805642, 0.002624636748805642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002624636748805642

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00262464
Iteration 2/1000 | Loss: 0.00020211
Iteration 3/1000 | Loss: 0.00032901
Iteration 4/1000 | Loss: 0.00024856
Iteration 5/1000 | Loss: 0.00048518
Iteration 6/1000 | Loss: 0.00029685
Iteration 7/1000 | Loss: 0.00012013
Iteration 8/1000 | Loss: 0.00026393
Iteration 9/1000 | Loss: 0.00032329
Iteration 10/1000 | Loss: 0.00038188
Iteration 11/1000 | Loss: 0.00041890
Iteration 12/1000 | Loss: 0.00014166
Iteration 13/1000 | Loss: 0.00019560
Iteration 14/1000 | Loss: 0.00023072
Iteration 15/1000 | Loss: 0.00034437
Iteration 16/1000 | Loss: 0.00017483
Iteration 17/1000 | Loss: 0.00022555
Iteration 18/1000 | Loss: 0.00051954
Iteration 19/1000 | Loss: 0.00029793
Iteration 20/1000 | Loss: 0.00020332
Iteration 21/1000 | Loss: 0.00021129
Iteration 22/1000 | Loss: 0.00023404
Iteration 23/1000 | Loss: 0.00020176
Iteration 24/1000 | Loss: 0.00021728
Iteration 25/1000 | Loss: 0.00027436
Iteration 26/1000 | Loss: 0.00008011
Iteration 27/1000 | Loss: 0.00007124
Iteration 28/1000 | Loss: 0.00006676
Iteration 29/1000 | Loss: 0.00015122
Iteration 30/1000 | Loss: 0.00006613
Iteration 31/1000 | Loss: 0.00006389
Iteration 32/1000 | Loss: 0.00016845
Iteration 33/1000 | Loss: 0.00006561
Iteration 34/1000 | Loss: 0.00033101
Iteration 35/1000 | Loss: 0.00088374
Iteration 36/1000 | Loss: 0.00032717
Iteration 37/1000 | Loss: 0.00023777
Iteration 38/1000 | Loss: 0.00020395
Iteration 39/1000 | Loss: 0.00008058
Iteration 40/1000 | Loss: 0.00016887
Iteration 41/1000 | Loss: 0.00017443
Iteration 42/1000 | Loss: 0.00013099
Iteration 43/1000 | Loss: 0.00006457
Iteration 44/1000 | Loss: 0.00005958
Iteration 45/1000 | Loss: 0.00005560
Iteration 46/1000 | Loss: 0.00005259
Iteration 47/1000 | Loss: 0.00005049
Iteration 48/1000 | Loss: 0.00004845
Iteration 49/1000 | Loss: 0.00004721
Iteration 50/1000 | Loss: 0.00004619
Iteration 51/1000 | Loss: 0.00004546
Iteration 52/1000 | Loss: 0.00004464
Iteration 53/1000 | Loss: 0.00004413
Iteration 54/1000 | Loss: 0.00004369
Iteration 55/1000 | Loss: 0.00171645
Iteration 56/1000 | Loss: 0.00005602
Iteration 57/1000 | Loss: 0.00134744
Iteration 58/1000 | Loss: 0.00004602
Iteration 59/1000 | Loss: 0.00004111
Iteration 60/1000 | Loss: 0.00003816
Iteration 61/1000 | Loss: 0.00003396
Iteration 62/1000 | Loss: 0.00003187
Iteration 63/1000 | Loss: 0.00003055
Iteration 64/1000 | Loss: 0.00002977
Iteration 65/1000 | Loss: 0.00002924
Iteration 66/1000 | Loss: 0.00002876
Iteration 67/1000 | Loss: 0.00002831
Iteration 68/1000 | Loss: 0.00002797
Iteration 69/1000 | Loss: 0.00002778
Iteration 70/1000 | Loss: 0.00003562
Iteration 71/1000 | Loss: 0.00002754
Iteration 72/1000 | Loss: 0.00002751
Iteration 73/1000 | Loss: 0.00002750
Iteration 74/1000 | Loss: 0.00002750
Iteration 75/1000 | Loss: 0.00002748
Iteration 76/1000 | Loss: 0.00002747
Iteration 77/1000 | Loss: 0.00002747
Iteration 78/1000 | Loss: 0.00002747
Iteration 79/1000 | Loss: 0.00002742
Iteration 80/1000 | Loss: 0.00002740
Iteration 81/1000 | Loss: 0.00002739
Iteration 82/1000 | Loss: 0.00002739
Iteration 83/1000 | Loss: 0.00002738
Iteration 84/1000 | Loss: 0.00002737
Iteration 85/1000 | Loss: 0.00002733
Iteration 86/1000 | Loss: 0.00002731
Iteration 87/1000 | Loss: 0.00002731
Iteration 88/1000 | Loss: 0.00002730
Iteration 89/1000 | Loss: 0.00002729
Iteration 90/1000 | Loss: 0.00002729
Iteration 91/1000 | Loss: 0.00002729
Iteration 92/1000 | Loss: 0.00002728
Iteration 93/1000 | Loss: 0.00002728
Iteration 94/1000 | Loss: 0.00002728
Iteration 95/1000 | Loss: 0.00002728
Iteration 96/1000 | Loss: 0.00002727
Iteration 97/1000 | Loss: 0.00002727
Iteration 98/1000 | Loss: 0.00002727
Iteration 99/1000 | Loss: 0.00002727
Iteration 100/1000 | Loss: 0.00002726
Iteration 101/1000 | Loss: 0.00002726
Iteration 102/1000 | Loss: 0.00002726
Iteration 103/1000 | Loss: 0.00002725
Iteration 104/1000 | Loss: 0.00002725
Iteration 105/1000 | Loss: 0.00002725
Iteration 106/1000 | Loss: 0.00002724
Iteration 107/1000 | Loss: 0.00002724
Iteration 108/1000 | Loss: 0.00002724
Iteration 109/1000 | Loss: 0.00061915
Iteration 110/1000 | Loss: 0.00033694
Iteration 111/1000 | Loss: 0.00062516
Iteration 112/1000 | Loss: 0.00017449
Iteration 113/1000 | Loss: 0.00003344
Iteration 114/1000 | Loss: 0.00006259
Iteration 115/1000 | Loss: 0.00002926
Iteration 116/1000 | Loss: 0.00002797
Iteration 117/1000 | Loss: 0.00002704
Iteration 118/1000 | Loss: 0.00002636
Iteration 119/1000 | Loss: 0.00002595
Iteration 120/1000 | Loss: 0.00002558
Iteration 121/1000 | Loss: 0.00002536
Iteration 122/1000 | Loss: 0.00002536
Iteration 123/1000 | Loss: 0.00002534
Iteration 124/1000 | Loss: 0.00002523
Iteration 125/1000 | Loss: 0.00002520
Iteration 126/1000 | Loss: 0.00002518
Iteration 127/1000 | Loss: 0.00002517
Iteration 128/1000 | Loss: 0.00002516
Iteration 129/1000 | Loss: 0.00002515
Iteration 130/1000 | Loss: 0.00002515
Iteration 131/1000 | Loss: 0.00002514
Iteration 132/1000 | Loss: 0.00002507
Iteration 133/1000 | Loss: 0.00002493
Iteration 134/1000 | Loss: 0.00002491
Iteration 135/1000 | Loss: 0.00002491
Iteration 136/1000 | Loss: 0.00002490
Iteration 137/1000 | Loss: 0.00002489
Iteration 138/1000 | Loss: 0.00002489
Iteration 139/1000 | Loss: 0.00002489
Iteration 140/1000 | Loss: 0.00002488
Iteration 141/1000 | Loss: 0.00002488
Iteration 142/1000 | Loss: 0.00002488
Iteration 143/1000 | Loss: 0.00002487
Iteration 144/1000 | Loss: 0.00002913
Iteration 145/1000 | Loss: 0.00002485
Iteration 146/1000 | Loss: 0.00002484
Iteration 147/1000 | Loss: 0.00002484
Iteration 148/1000 | Loss: 0.00002484
Iteration 149/1000 | Loss: 0.00002484
Iteration 150/1000 | Loss: 0.00002484
Iteration 151/1000 | Loss: 0.00002484
Iteration 152/1000 | Loss: 0.00002484
Iteration 153/1000 | Loss: 0.00002484
Iteration 154/1000 | Loss: 0.00002484
Iteration 155/1000 | Loss: 0.00002483
Iteration 156/1000 | Loss: 0.00002483
Iteration 157/1000 | Loss: 0.00002482
Iteration 158/1000 | Loss: 0.00002482
Iteration 159/1000 | Loss: 0.00002482
Iteration 160/1000 | Loss: 0.00002481
Iteration 161/1000 | Loss: 0.00002481
Iteration 162/1000 | Loss: 0.00002481
Iteration 163/1000 | Loss: 0.00002481
Iteration 164/1000 | Loss: 0.00002481
Iteration 165/1000 | Loss: 0.00002481
Iteration 166/1000 | Loss: 0.00002480
Iteration 167/1000 | Loss: 0.00002480
Iteration 168/1000 | Loss: 0.00002480
Iteration 169/1000 | Loss: 0.00002480
Iteration 170/1000 | Loss: 0.00002480
Iteration 171/1000 | Loss: 0.00002479
Iteration 172/1000 | Loss: 0.00002479
Iteration 173/1000 | Loss: 0.00002479
Iteration 174/1000 | Loss: 0.00002479
Iteration 175/1000 | Loss: 0.00002477
Iteration 176/1000 | Loss: 0.00002477
Iteration 177/1000 | Loss: 0.00002477
Iteration 178/1000 | Loss: 0.00002477
Iteration 179/1000 | Loss: 0.00002477
Iteration 180/1000 | Loss: 0.00002477
Iteration 181/1000 | Loss: 0.00002477
Iteration 182/1000 | Loss: 0.00002477
Iteration 183/1000 | Loss: 0.00002476
Iteration 184/1000 | Loss: 0.00002476
Iteration 185/1000 | Loss: 0.00002476
Iteration 186/1000 | Loss: 0.00002476
Iteration 187/1000 | Loss: 0.00002476
Iteration 188/1000 | Loss: 0.00002476
Iteration 189/1000 | Loss: 0.00002476
Iteration 190/1000 | Loss: 0.00002476
Iteration 191/1000 | Loss: 0.00002476
Iteration 192/1000 | Loss: 0.00002476
Iteration 193/1000 | Loss: 0.00002476
Iteration 194/1000 | Loss: 0.00002476
Iteration 195/1000 | Loss: 0.00002476
Iteration 196/1000 | Loss: 0.00002475
Iteration 197/1000 | Loss: 0.00002475
Iteration 198/1000 | Loss: 0.00002474
Iteration 199/1000 | Loss: 0.00002474
Iteration 200/1000 | Loss: 0.00002474
Iteration 201/1000 | Loss: 0.00002474
Iteration 202/1000 | Loss: 0.00002474
Iteration 203/1000 | Loss: 0.00002474
Iteration 204/1000 | Loss: 0.00002473
Iteration 205/1000 | Loss: 0.00002473
Iteration 206/1000 | Loss: 0.00002473
Iteration 207/1000 | Loss: 0.00002473
Iteration 208/1000 | Loss: 0.00002473
Iteration 209/1000 | Loss: 0.00002473
Iteration 210/1000 | Loss: 0.00002473
Iteration 211/1000 | Loss: 0.00002473
Iteration 212/1000 | Loss: 0.00002472
Iteration 213/1000 | Loss: 0.00002472
Iteration 214/1000 | Loss: 0.00002472
Iteration 215/1000 | Loss: 0.00002472
Iteration 216/1000 | Loss: 0.00002472
Iteration 217/1000 | Loss: 0.00002472
Iteration 218/1000 | Loss: 0.00002472
Iteration 219/1000 | Loss: 0.00002472
Iteration 220/1000 | Loss: 0.00002472
Iteration 221/1000 | Loss: 0.00002472
Iteration 222/1000 | Loss: 0.00002472
Iteration 223/1000 | Loss: 0.00002471
Iteration 224/1000 | Loss: 0.00002471
Iteration 225/1000 | Loss: 0.00002471
Iteration 226/1000 | Loss: 0.00002471
Iteration 227/1000 | Loss: 0.00002471
Iteration 228/1000 | Loss: 0.00002471
Iteration 229/1000 | Loss: 0.00002470
Iteration 230/1000 | Loss: 0.00002470
Iteration 231/1000 | Loss: 0.00002470
Iteration 232/1000 | Loss: 0.00002470
Iteration 233/1000 | Loss: 0.00002470
Iteration 234/1000 | Loss: 0.00002470
Iteration 235/1000 | Loss: 0.00002469
Iteration 236/1000 | Loss: 0.00002469
Iteration 237/1000 | Loss: 0.00002469
Iteration 238/1000 | Loss: 0.00002468
Iteration 239/1000 | Loss: 0.00002468
Iteration 240/1000 | Loss: 0.00002468
Iteration 241/1000 | Loss: 0.00002468
Iteration 242/1000 | Loss: 0.00002467
Iteration 243/1000 | Loss: 0.00002467
Iteration 244/1000 | Loss: 0.00002467
Iteration 245/1000 | Loss: 0.00002467
Iteration 246/1000 | Loss: 0.00002467
Iteration 247/1000 | Loss: 0.00002467
Iteration 248/1000 | Loss: 0.00002467
Iteration 249/1000 | Loss: 0.00002467
Iteration 250/1000 | Loss: 0.00002467
Iteration 251/1000 | Loss: 0.00002467
Iteration 252/1000 | Loss: 0.00002466
Iteration 253/1000 | Loss: 0.00002466
Iteration 254/1000 | Loss: 0.00002466
Iteration 255/1000 | Loss: 0.00002466
Iteration 256/1000 | Loss: 0.00002466
Iteration 257/1000 | Loss: 0.00002466
Iteration 258/1000 | Loss: 0.00002466
Iteration 259/1000 | Loss: 0.00002466
Iteration 260/1000 | Loss: 0.00002466
Iteration 261/1000 | Loss: 0.00002466
Iteration 262/1000 | Loss: 0.00002466
Iteration 263/1000 | Loss: 0.00002466
Iteration 264/1000 | Loss: 0.00002466
Iteration 265/1000 | Loss: 0.00002466
Iteration 266/1000 | Loss: 0.00002466
Iteration 267/1000 | Loss: 0.00002466
Iteration 268/1000 | Loss: 0.00002466
Iteration 269/1000 | Loss: 0.00002466
Iteration 270/1000 | Loss: 0.00002466
Iteration 271/1000 | Loss: 0.00002466
Iteration 272/1000 | Loss: 0.00002466
Iteration 273/1000 | Loss: 0.00002466
Iteration 274/1000 | Loss: 0.00002466
Iteration 275/1000 | Loss: 0.00002466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [2.465617399138864e-05, 2.465617399138864e-05, 2.465617399138864e-05, 2.465617399138864e-05, 2.465617399138864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.465617399138864e-05

Optimization complete. Final v2v error: 3.914294719696045 mm

Highest mean error: 8.06239128112793 mm for frame 109

Lowest mean error: 2.573486804962158 mm for frame 37

Saving results

Total time: 183.28033018112183
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034447
Iteration 2/25 | Loss: 0.00222055
Iteration 3/25 | Loss: 0.00154319
Iteration 4/25 | Loss: 0.00144967
Iteration 5/25 | Loss: 0.00141401
Iteration 6/25 | Loss: 0.00140230
Iteration 7/25 | Loss: 0.00137055
Iteration 8/25 | Loss: 0.00136255
Iteration 9/25 | Loss: 0.00134578
Iteration 10/25 | Loss: 0.00134332
Iteration 11/25 | Loss: 0.00133988
Iteration 12/25 | Loss: 0.00134166
Iteration 13/25 | Loss: 0.00133982
Iteration 14/25 | Loss: 0.00133982
Iteration 15/25 | Loss: 0.00133982
Iteration 16/25 | Loss: 0.00133981
Iteration 17/25 | Loss: 0.00133981
Iteration 18/25 | Loss: 0.00133981
Iteration 19/25 | Loss: 0.00133981
Iteration 20/25 | Loss: 0.00133981
Iteration 21/25 | Loss: 0.00133981
Iteration 22/25 | Loss: 0.00133981
Iteration 23/25 | Loss: 0.00133982
Iteration 24/25 | Loss: 0.00133979
Iteration 25/25 | Loss: 0.00133979

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29754531
Iteration 2/25 | Loss: 0.00168547
Iteration 3/25 | Loss: 0.00157015
Iteration 4/25 | Loss: 0.00157015
Iteration 5/25 | Loss: 0.00157014
Iteration 6/25 | Loss: 0.00157014
Iteration 7/25 | Loss: 0.00157014
Iteration 8/25 | Loss: 0.00157014
Iteration 9/25 | Loss: 0.00157014
Iteration 10/25 | Loss: 0.00157014
Iteration 11/25 | Loss: 0.00157014
Iteration 12/25 | Loss: 0.00157014
Iteration 13/25 | Loss: 0.00157014
Iteration 14/25 | Loss: 0.00157014
Iteration 15/25 | Loss: 0.00157014
Iteration 16/25 | Loss: 0.00157014
Iteration 17/25 | Loss: 0.00157014
Iteration 18/25 | Loss: 0.00157014
Iteration 19/25 | Loss: 0.00157014
Iteration 20/25 | Loss: 0.00157014
Iteration 21/25 | Loss: 0.00157014
Iteration 22/25 | Loss: 0.00157014
Iteration 23/25 | Loss: 0.00157014
Iteration 24/25 | Loss: 0.00157014
Iteration 25/25 | Loss: 0.00157014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157014
Iteration 2/1000 | Loss: 0.00016401
Iteration 3/1000 | Loss: 0.00007937
Iteration 4/1000 | Loss: 0.00009190
Iteration 5/1000 | Loss: 0.00006542
Iteration 6/1000 | Loss: 0.00012968
Iteration 7/1000 | Loss: 0.00010041
Iteration 8/1000 | Loss: 0.00074353
Iteration 9/1000 | Loss: 0.00019036
Iteration 10/1000 | Loss: 0.00004914
Iteration 11/1000 | Loss: 0.00003983
Iteration 12/1000 | Loss: 0.00002856
Iteration 13/1000 | Loss: 0.00002811
Iteration 14/1000 | Loss: 0.00002784
Iteration 15/1000 | Loss: 0.00002742
Iteration 16/1000 | Loss: 0.00002710
Iteration 17/1000 | Loss: 0.00008297
Iteration 18/1000 | Loss: 0.00003385
Iteration 19/1000 | Loss: 0.00002653
Iteration 20/1000 | Loss: 0.00002858
Iteration 21/1000 | Loss: 0.00004209
Iteration 22/1000 | Loss: 0.00002621
Iteration 23/1000 | Loss: 0.00004878
Iteration 24/1000 | Loss: 0.00026351
Iteration 25/1000 | Loss: 0.00003495
Iteration 26/1000 | Loss: 0.00002605
Iteration 27/1000 | Loss: 0.00006997
Iteration 28/1000 | Loss: 0.00003306
Iteration 29/1000 | Loss: 0.00002882
Iteration 30/1000 | Loss: 0.00003603
Iteration 31/1000 | Loss: 0.00003504
Iteration 32/1000 | Loss: 0.00003836
Iteration 33/1000 | Loss: 0.00002572
Iteration 34/1000 | Loss: 0.00002570
Iteration 35/1000 | Loss: 0.00002570
Iteration 36/1000 | Loss: 0.00002570
Iteration 37/1000 | Loss: 0.00002570
Iteration 38/1000 | Loss: 0.00002570
Iteration 39/1000 | Loss: 0.00002570
Iteration 40/1000 | Loss: 0.00002570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 40. Stopping optimization.
Last 5 losses: [2.570166179793887e-05, 2.570166179793887e-05, 2.570166179793887e-05, 2.570166179793887e-05, 2.570166179793887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.570166179793887e-05

Optimization complete. Final v2v error: 4.210310935974121 mm

Highest mean error: 4.929758071899414 mm for frame 58

Lowest mean error: 3.8862504959106445 mm for frame 130

Saving results

Total time: 68.79703068733215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394739
Iteration 2/25 | Loss: 0.00127181
Iteration 3/25 | Loss: 0.00119177
Iteration 4/25 | Loss: 0.00118308
Iteration 5/25 | Loss: 0.00118090
Iteration 6/25 | Loss: 0.00118055
Iteration 7/25 | Loss: 0.00118055
Iteration 8/25 | Loss: 0.00118055
Iteration 9/25 | Loss: 0.00118055
Iteration 10/25 | Loss: 0.00118055
Iteration 11/25 | Loss: 0.00118055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011805465910583735, 0.0011805465910583735, 0.0011805465910583735, 0.0011805465910583735, 0.0011805465910583735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011805465910583735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29816067
Iteration 2/25 | Loss: 0.00159666
Iteration 3/25 | Loss: 0.00159665
Iteration 4/25 | Loss: 0.00159665
Iteration 5/25 | Loss: 0.00159665
Iteration 6/25 | Loss: 0.00159665
Iteration 7/25 | Loss: 0.00159665
Iteration 8/25 | Loss: 0.00159665
Iteration 9/25 | Loss: 0.00159665
Iteration 10/25 | Loss: 0.00159665
Iteration 11/25 | Loss: 0.00159665
Iteration 12/25 | Loss: 0.00159665
Iteration 13/25 | Loss: 0.00159665
Iteration 14/25 | Loss: 0.00159665
Iteration 15/25 | Loss: 0.00159665
Iteration 16/25 | Loss: 0.00159665
Iteration 17/25 | Loss: 0.00159665
Iteration 18/25 | Loss: 0.00159665
Iteration 19/25 | Loss: 0.00159665
Iteration 20/25 | Loss: 0.00159665
Iteration 21/25 | Loss: 0.00159665
Iteration 22/25 | Loss: 0.00159665
Iteration 23/25 | Loss: 0.00159665
Iteration 24/25 | Loss: 0.00159665
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0015966483624652028, 0.0015966483624652028, 0.0015966483624652028, 0.0015966483624652028, 0.0015966483624652028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015966483624652028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159665
Iteration 2/1000 | Loss: 0.00002507
Iteration 3/1000 | Loss: 0.00001605
Iteration 4/1000 | Loss: 0.00001313
Iteration 5/1000 | Loss: 0.00001192
Iteration 6/1000 | Loss: 0.00001116
Iteration 7/1000 | Loss: 0.00001065
Iteration 8/1000 | Loss: 0.00001016
Iteration 9/1000 | Loss: 0.00000991
Iteration 10/1000 | Loss: 0.00000982
Iteration 11/1000 | Loss: 0.00000961
Iteration 12/1000 | Loss: 0.00000939
Iteration 13/1000 | Loss: 0.00000928
Iteration 14/1000 | Loss: 0.00000916
Iteration 15/1000 | Loss: 0.00000916
Iteration 16/1000 | Loss: 0.00000915
Iteration 17/1000 | Loss: 0.00000907
Iteration 18/1000 | Loss: 0.00000904
Iteration 19/1000 | Loss: 0.00000903
Iteration 20/1000 | Loss: 0.00000901
Iteration 21/1000 | Loss: 0.00000900
Iteration 22/1000 | Loss: 0.00000898
Iteration 23/1000 | Loss: 0.00000898
Iteration 24/1000 | Loss: 0.00000895
Iteration 25/1000 | Loss: 0.00000894
Iteration 26/1000 | Loss: 0.00000894
Iteration 27/1000 | Loss: 0.00000894
Iteration 28/1000 | Loss: 0.00000893
Iteration 29/1000 | Loss: 0.00000892
Iteration 30/1000 | Loss: 0.00000892
Iteration 31/1000 | Loss: 0.00000892
Iteration 32/1000 | Loss: 0.00000890
Iteration 33/1000 | Loss: 0.00000890
Iteration 34/1000 | Loss: 0.00000889
Iteration 35/1000 | Loss: 0.00000889
Iteration 36/1000 | Loss: 0.00000889
Iteration 37/1000 | Loss: 0.00000888
Iteration 38/1000 | Loss: 0.00000886
Iteration 39/1000 | Loss: 0.00000886
Iteration 40/1000 | Loss: 0.00000885
Iteration 41/1000 | Loss: 0.00000885
Iteration 42/1000 | Loss: 0.00000883
Iteration 43/1000 | Loss: 0.00000883
Iteration 44/1000 | Loss: 0.00000882
Iteration 45/1000 | Loss: 0.00000881
Iteration 46/1000 | Loss: 0.00000881
Iteration 47/1000 | Loss: 0.00000880
Iteration 48/1000 | Loss: 0.00000879
Iteration 49/1000 | Loss: 0.00000879
Iteration 50/1000 | Loss: 0.00000879
Iteration 51/1000 | Loss: 0.00000878
Iteration 52/1000 | Loss: 0.00000878
Iteration 53/1000 | Loss: 0.00000877
Iteration 54/1000 | Loss: 0.00000877
Iteration 55/1000 | Loss: 0.00000876
Iteration 56/1000 | Loss: 0.00000876
Iteration 57/1000 | Loss: 0.00000876
Iteration 58/1000 | Loss: 0.00000876
Iteration 59/1000 | Loss: 0.00000875
Iteration 60/1000 | Loss: 0.00000875
Iteration 61/1000 | Loss: 0.00000874
Iteration 62/1000 | Loss: 0.00000874
Iteration 63/1000 | Loss: 0.00000874
Iteration 64/1000 | Loss: 0.00000874
Iteration 65/1000 | Loss: 0.00000873
Iteration 66/1000 | Loss: 0.00000873
Iteration 67/1000 | Loss: 0.00000873
Iteration 68/1000 | Loss: 0.00000873
Iteration 69/1000 | Loss: 0.00000872
Iteration 70/1000 | Loss: 0.00000872
Iteration 71/1000 | Loss: 0.00000872
Iteration 72/1000 | Loss: 0.00000871
Iteration 73/1000 | Loss: 0.00000871
Iteration 74/1000 | Loss: 0.00000870
Iteration 75/1000 | Loss: 0.00000870
Iteration 76/1000 | Loss: 0.00000870
Iteration 77/1000 | Loss: 0.00000869
Iteration 78/1000 | Loss: 0.00000869
Iteration 79/1000 | Loss: 0.00000869
Iteration 80/1000 | Loss: 0.00000868
Iteration 81/1000 | Loss: 0.00000868
Iteration 82/1000 | Loss: 0.00000868
Iteration 83/1000 | Loss: 0.00000868
Iteration 84/1000 | Loss: 0.00000867
Iteration 85/1000 | Loss: 0.00000867
Iteration 86/1000 | Loss: 0.00000866
Iteration 87/1000 | Loss: 0.00000866
Iteration 88/1000 | Loss: 0.00000866
Iteration 89/1000 | Loss: 0.00000866
Iteration 90/1000 | Loss: 0.00000865
Iteration 91/1000 | Loss: 0.00000865
Iteration 92/1000 | Loss: 0.00000865
Iteration 93/1000 | Loss: 0.00000864
Iteration 94/1000 | Loss: 0.00000864
Iteration 95/1000 | Loss: 0.00000864
Iteration 96/1000 | Loss: 0.00000864
Iteration 97/1000 | Loss: 0.00000864
Iteration 98/1000 | Loss: 0.00000863
Iteration 99/1000 | Loss: 0.00000863
Iteration 100/1000 | Loss: 0.00000863
Iteration 101/1000 | Loss: 0.00000863
Iteration 102/1000 | Loss: 0.00000863
Iteration 103/1000 | Loss: 0.00000863
Iteration 104/1000 | Loss: 0.00000863
Iteration 105/1000 | Loss: 0.00000863
Iteration 106/1000 | Loss: 0.00000863
Iteration 107/1000 | Loss: 0.00000863
Iteration 108/1000 | Loss: 0.00000862
Iteration 109/1000 | Loss: 0.00000862
Iteration 110/1000 | Loss: 0.00000862
Iteration 111/1000 | Loss: 0.00000862
Iteration 112/1000 | Loss: 0.00000862
Iteration 113/1000 | Loss: 0.00000862
Iteration 114/1000 | Loss: 0.00000862
Iteration 115/1000 | Loss: 0.00000862
Iteration 116/1000 | Loss: 0.00000862
Iteration 117/1000 | Loss: 0.00000862
Iteration 118/1000 | Loss: 0.00000862
Iteration 119/1000 | Loss: 0.00000862
Iteration 120/1000 | Loss: 0.00000862
Iteration 121/1000 | Loss: 0.00000861
Iteration 122/1000 | Loss: 0.00000861
Iteration 123/1000 | Loss: 0.00000861
Iteration 124/1000 | Loss: 0.00000861
Iteration 125/1000 | Loss: 0.00000861
Iteration 126/1000 | Loss: 0.00000861
Iteration 127/1000 | Loss: 0.00000860
Iteration 128/1000 | Loss: 0.00000860
Iteration 129/1000 | Loss: 0.00000860
Iteration 130/1000 | Loss: 0.00000859
Iteration 131/1000 | Loss: 0.00000859
Iteration 132/1000 | Loss: 0.00000859
Iteration 133/1000 | Loss: 0.00000858
Iteration 134/1000 | Loss: 0.00000858
Iteration 135/1000 | Loss: 0.00000858
Iteration 136/1000 | Loss: 0.00000858
Iteration 137/1000 | Loss: 0.00000858
Iteration 138/1000 | Loss: 0.00000858
Iteration 139/1000 | Loss: 0.00000857
Iteration 140/1000 | Loss: 0.00000857
Iteration 141/1000 | Loss: 0.00000857
Iteration 142/1000 | Loss: 0.00000857
Iteration 143/1000 | Loss: 0.00000857
Iteration 144/1000 | Loss: 0.00000857
Iteration 145/1000 | Loss: 0.00000857
Iteration 146/1000 | Loss: 0.00000857
Iteration 147/1000 | Loss: 0.00000857
Iteration 148/1000 | Loss: 0.00000857
Iteration 149/1000 | Loss: 0.00000857
Iteration 150/1000 | Loss: 0.00000856
Iteration 151/1000 | Loss: 0.00000856
Iteration 152/1000 | Loss: 0.00000856
Iteration 153/1000 | Loss: 0.00000856
Iteration 154/1000 | Loss: 0.00000856
Iteration 155/1000 | Loss: 0.00000856
Iteration 156/1000 | Loss: 0.00000856
Iteration 157/1000 | Loss: 0.00000855
Iteration 158/1000 | Loss: 0.00000855
Iteration 159/1000 | Loss: 0.00000855
Iteration 160/1000 | Loss: 0.00000855
Iteration 161/1000 | Loss: 0.00000855
Iteration 162/1000 | Loss: 0.00000854
Iteration 163/1000 | Loss: 0.00000854
Iteration 164/1000 | Loss: 0.00000854
Iteration 165/1000 | Loss: 0.00000854
Iteration 166/1000 | Loss: 0.00000854
Iteration 167/1000 | Loss: 0.00000853
Iteration 168/1000 | Loss: 0.00000853
Iteration 169/1000 | Loss: 0.00000853
Iteration 170/1000 | Loss: 0.00000853
Iteration 171/1000 | Loss: 0.00000853
Iteration 172/1000 | Loss: 0.00000853
Iteration 173/1000 | Loss: 0.00000852
Iteration 174/1000 | Loss: 0.00000852
Iteration 175/1000 | Loss: 0.00000852
Iteration 176/1000 | Loss: 0.00000851
Iteration 177/1000 | Loss: 0.00000851
Iteration 178/1000 | Loss: 0.00000851
Iteration 179/1000 | Loss: 0.00000851
Iteration 180/1000 | Loss: 0.00000850
Iteration 181/1000 | Loss: 0.00000850
Iteration 182/1000 | Loss: 0.00000850
Iteration 183/1000 | Loss: 0.00000849
Iteration 184/1000 | Loss: 0.00000849
Iteration 185/1000 | Loss: 0.00000849
Iteration 186/1000 | Loss: 0.00000849
Iteration 187/1000 | Loss: 0.00000849
Iteration 188/1000 | Loss: 0.00000848
Iteration 189/1000 | Loss: 0.00000848
Iteration 190/1000 | Loss: 0.00000848
Iteration 191/1000 | Loss: 0.00000848
Iteration 192/1000 | Loss: 0.00000848
Iteration 193/1000 | Loss: 0.00000848
Iteration 194/1000 | Loss: 0.00000848
Iteration 195/1000 | Loss: 0.00000848
Iteration 196/1000 | Loss: 0.00000848
Iteration 197/1000 | Loss: 0.00000848
Iteration 198/1000 | Loss: 0.00000847
Iteration 199/1000 | Loss: 0.00000847
Iteration 200/1000 | Loss: 0.00000847
Iteration 201/1000 | Loss: 0.00000847
Iteration 202/1000 | Loss: 0.00000847
Iteration 203/1000 | Loss: 0.00000847
Iteration 204/1000 | Loss: 0.00000847
Iteration 205/1000 | Loss: 0.00000847
Iteration 206/1000 | Loss: 0.00000847
Iteration 207/1000 | Loss: 0.00000847
Iteration 208/1000 | Loss: 0.00000847
Iteration 209/1000 | Loss: 0.00000847
Iteration 210/1000 | Loss: 0.00000847
Iteration 211/1000 | Loss: 0.00000847
Iteration 212/1000 | Loss: 0.00000847
Iteration 213/1000 | Loss: 0.00000846
Iteration 214/1000 | Loss: 0.00000846
Iteration 215/1000 | Loss: 0.00000846
Iteration 216/1000 | Loss: 0.00000846
Iteration 217/1000 | Loss: 0.00000846
Iteration 218/1000 | Loss: 0.00000846
Iteration 219/1000 | Loss: 0.00000846
Iteration 220/1000 | Loss: 0.00000846
Iteration 221/1000 | Loss: 0.00000846
Iteration 222/1000 | Loss: 0.00000846
Iteration 223/1000 | Loss: 0.00000846
Iteration 224/1000 | Loss: 0.00000846
Iteration 225/1000 | Loss: 0.00000846
Iteration 226/1000 | Loss: 0.00000846
Iteration 227/1000 | Loss: 0.00000846
Iteration 228/1000 | Loss: 0.00000846
Iteration 229/1000 | Loss: 0.00000846
Iteration 230/1000 | Loss: 0.00000846
Iteration 231/1000 | Loss: 0.00000846
Iteration 232/1000 | Loss: 0.00000845
Iteration 233/1000 | Loss: 0.00000845
Iteration 234/1000 | Loss: 0.00000845
Iteration 235/1000 | Loss: 0.00000845
Iteration 236/1000 | Loss: 0.00000845
Iteration 237/1000 | Loss: 0.00000845
Iteration 238/1000 | Loss: 0.00000845
Iteration 239/1000 | Loss: 0.00000845
Iteration 240/1000 | Loss: 0.00000845
Iteration 241/1000 | Loss: 0.00000844
Iteration 242/1000 | Loss: 0.00000844
Iteration 243/1000 | Loss: 0.00000844
Iteration 244/1000 | Loss: 0.00000844
Iteration 245/1000 | Loss: 0.00000844
Iteration 246/1000 | Loss: 0.00000844
Iteration 247/1000 | Loss: 0.00000844
Iteration 248/1000 | Loss: 0.00000844
Iteration 249/1000 | Loss: 0.00000844
Iteration 250/1000 | Loss: 0.00000844
Iteration 251/1000 | Loss: 0.00000844
Iteration 252/1000 | Loss: 0.00000844
Iteration 253/1000 | Loss: 0.00000844
Iteration 254/1000 | Loss: 0.00000844
Iteration 255/1000 | Loss: 0.00000844
Iteration 256/1000 | Loss: 0.00000844
Iteration 257/1000 | Loss: 0.00000844
Iteration 258/1000 | Loss: 0.00000844
Iteration 259/1000 | Loss: 0.00000843
Iteration 260/1000 | Loss: 0.00000843
Iteration 261/1000 | Loss: 0.00000843
Iteration 262/1000 | Loss: 0.00000843
Iteration 263/1000 | Loss: 0.00000843
Iteration 264/1000 | Loss: 0.00000843
Iteration 265/1000 | Loss: 0.00000843
Iteration 266/1000 | Loss: 0.00000843
Iteration 267/1000 | Loss: 0.00000843
Iteration 268/1000 | Loss: 0.00000843
Iteration 269/1000 | Loss: 0.00000843
Iteration 270/1000 | Loss: 0.00000843
Iteration 271/1000 | Loss: 0.00000843
Iteration 272/1000 | Loss: 0.00000843
Iteration 273/1000 | Loss: 0.00000843
Iteration 274/1000 | Loss: 0.00000843
Iteration 275/1000 | Loss: 0.00000843
Iteration 276/1000 | Loss: 0.00000843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 276. Stopping optimization.
Last 5 losses: [8.432236427324824e-06, 8.432236427324824e-06, 8.432236427324824e-06, 8.432236427324824e-06, 8.432236427324824e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.432236427324824e-06

Optimization complete. Final v2v error: 2.490370273590088 mm

Highest mean error: 3.4153566360473633 mm for frame 71

Lowest mean error: 2.3120219707489014 mm for frame 114

Saving results

Total time: 48.74648833274841
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433987
Iteration 2/25 | Loss: 0.00177099
Iteration 3/25 | Loss: 0.00151589
Iteration 4/25 | Loss: 0.00146167
Iteration 5/25 | Loss: 0.00144820
Iteration 6/25 | Loss: 0.00145502
Iteration 7/25 | Loss: 0.00154886
Iteration 8/25 | Loss: 0.00158714
Iteration 9/25 | Loss: 0.00144309
Iteration 10/25 | Loss: 0.00140335
Iteration 11/25 | Loss: 0.00138741
Iteration 12/25 | Loss: 0.00137948
Iteration 13/25 | Loss: 0.00137675
Iteration 14/25 | Loss: 0.00136916
Iteration 15/25 | Loss: 0.00136742
Iteration 16/25 | Loss: 0.00136681
Iteration 17/25 | Loss: 0.00136646
Iteration 18/25 | Loss: 0.00136629
Iteration 19/25 | Loss: 0.00136620
Iteration 20/25 | Loss: 0.00136619
Iteration 21/25 | Loss: 0.00136619
Iteration 22/25 | Loss: 0.00136619
Iteration 23/25 | Loss: 0.00136619
Iteration 24/25 | Loss: 0.00136618
Iteration 25/25 | Loss: 0.00136618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25583363
Iteration 2/25 | Loss: 0.00161872
Iteration 3/25 | Loss: 0.00161871
Iteration 4/25 | Loss: 0.00161871
Iteration 5/25 | Loss: 0.00161871
Iteration 6/25 | Loss: 0.00161871
Iteration 7/25 | Loss: 0.00161871
Iteration 8/25 | Loss: 0.00161871
Iteration 9/25 | Loss: 0.00161871
Iteration 10/25 | Loss: 0.00161871
Iteration 11/25 | Loss: 0.00161871
Iteration 12/25 | Loss: 0.00161871
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0016187127912417054, 0.0016187127912417054, 0.0016187127912417054, 0.0016187127912417054, 0.0016187127912417054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016187127912417054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161871
Iteration 2/1000 | Loss: 0.00423226
Iteration 3/1000 | Loss: 0.00190571
Iteration 4/1000 | Loss: 0.00510498
Iteration 5/1000 | Loss: 0.00075063
Iteration 6/1000 | Loss: 0.00141142
Iteration 7/1000 | Loss: 0.00045535
Iteration 8/1000 | Loss: 0.00058952
Iteration 9/1000 | Loss: 0.00026893
Iteration 10/1000 | Loss: 0.00040706
Iteration 11/1000 | Loss: 0.00024770
Iteration 12/1000 | Loss: 0.00021329
Iteration 13/1000 | Loss: 0.00017594
Iteration 14/1000 | Loss: 0.00010900
Iteration 15/1000 | Loss: 0.00007352
Iteration 16/1000 | Loss: 0.00006397
Iteration 17/1000 | Loss: 0.00044656
Iteration 18/1000 | Loss: 0.00011919
Iteration 19/1000 | Loss: 0.00006040
Iteration 20/1000 | Loss: 0.00005084
Iteration 21/1000 | Loss: 0.00004684
Iteration 22/1000 | Loss: 0.00004449
Iteration 23/1000 | Loss: 0.00004296
Iteration 24/1000 | Loss: 0.00004142
Iteration 25/1000 | Loss: 0.00004062
Iteration 26/1000 | Loss: 0.00003981
Iteration 27/1000 | Loss: 0.00003937
Iteration 28/1000 | Loss: 0.00003873
Iteration 29/1000 | Loss: 0.00003812
Iteration 30/1000 | Loss: 0.00003778
Iteration 31/1000 | Loss: 0.00003739
Iteration 32/1000 | Loss: 0.00003705
Iteration 33/1000 | Loss: 0.00003676
Iteration 34/1000 | Loss: 0.00003657
Iteration 35/1000 | Loss: 0.00003641
Iteration 36/1000 | Loss: 0.00003629
Iteration 37/1000 | Loss: 0.00003626
Iteration 38/1000 | Loss: 0.00003625
Iteration 39/1000 | Loss: 0.00003623
Iteration 40/1000 | Loss: 0.00003623
Iteration 41/1000 | Loss: 0.00003618
Iteration 42/1000 | Loss: 0.00003616
Iteration 43/1000 | Loss: 0.00003612
Iteration 44/1000 | Loss: 0.00003609
Iteration 45/1000 | Loss: 0.00003609
Iteration 46/1000 | Loss: 0.00003609
Iteration 47/1000 | Loss: 0.00003609
Iteration 48/1000 | Loss: 0.00003607
Iteration 49/1000 | Loss: 0.00003606
Iteration 50/1000 | Loss: 0.00003606
Iteration 51/1000 | Loss: 0.00003605
Iteration 52/1000 | Loss: 0.00003605
Iteration 53/1000 | Loss: 0.00003604
Iteration 54/1000 | Loss: 0.00003603
Iteration 55/1000 | Loss: 0.00003602
Iteration 56/1000 | Loss: 0.00003600
Iteration 57/1000 | Loss: 0.00003600
Iteration 58/1000 | Loss: 0.00003599
Iteration 59/1000 | Loss: 0.00003599
Iteration 60/1000 | Loss: 0.00003598
Iteration 61/1000 | Loss: 0.00003598
Iteration 62/1000 | Loss: 0.00003597
Iteration 63/1000 | Loss: 0.00003597
Iteration 64/1000 | Loss: 0.00003596
Iteration 65/1000 | Loss: 0.00003596
Iteration 66/1000 | Loss: 0.00003596
Iteration 67/1000 | Loss: 0.00003596
Iteration 68/1000 | Loss: 0.00003595
Iteration 69/1000 | Loss: 0.00003595
Iteration 70/1000 | Loss: 0.00003595
Iteration 71/1000 | Loss: 0.00003595
Iteration 72/1000 | Loss: 0.00003595
Iteration 73/1000 | Loss: 0.00003595
Iteration 74/1000 | Loss: 0.00003595
Iteration 75/1000 | Loss: 0.00003595
Iteration 76/1000 | Loss: 0.00003595
Iteration 77/1000 | Loss: 0.00003594
Iteration 78/1000 | Loss: 0.00003594
Iteration 79/1000 | Loss: 0.00003593
Iteration 80/1000 | Loss: 0.00003593
Iteration 81/1000 | Loss: 0.00003592
Iteration 82/1000 | Loss: 0.00003592
Iteration 83/1000 | Loss: 0.00003591
Iteration 84/1000 | Loss: 0.00003591
Iteration 85/1000 | Loss: 0.00003591
Iteration 86/1000 | Loss: 0.00003590
Iteration 87/1000 | Loss: 0.00003590
Iteration 88/1000 | Loss: 0.00003590
Iteration 89/1000 | Loss: 0.00003589
Iteration 90/1000 | Loss: 0.00003589
Iteration 91/1000 | Loss: 0.00003589
Iteration 92/1000 | Loss: 0.00003589
Iteration 93/1000 | Loss: 0.00003589
Iteration 94/1000 | Loss: 0.00003589
Iteration 95/1000 | Loss: 0.00003588
Iteration 96/1000 | Loss: 0.00003588
Iteration 97/1000 | Loss: 0.00003588
Iteration 98/1000 | Loss: 0.00003588
Iteration 99/1000 | Loss: 0.00003588
Iteration 100/1000 | Loss: 0.00003588
Iteration 101/1000 | Loss: 0.00003588
Iteration 102/1000 | Loss: 0.00003588
Iteration 103/1000 | Loss: 0.00003587
Iteration 104/1000 | Loss: 0.00003587
Iteration 105/1000 | Loss: 0.00003587
Iteration 106/1000 | Loss: 0.00003587
Iteration 107/1000 | Loss: 0.00003587
Iteration 108/1000 | Loss: 0.00003587
Iteration 109/1000 | Loss: 0.00003587
Iteration 110/1000 | Loss: 0.00003586
Iteration 111/1000 | Loss: 0.00003586
Iteration 112/1000 | Loss: 0.00003586
Iteration 113/1000 | Loss: 0.00003586
Iteration 114/1000 | Loss: 0.00003586
Iteration 115/1000 | Loss: 0.00003586
Iteration 116/1000 | Loss: 0.00003586
Iteration 117/1000 | Loss: 0.00003585
Iteration 118/1000 | Loss: 0.00003585
Iteration 119/1000 | Loss: 0.00003585
Iteration 120/1000 | Loss: 0.00003585
Iteration 121/1000 | Loss: 0.00003585
Iteration 122/1000 | Loss: 0.00003585
Iteration 123/1000 | Loss: 0.00003585
Iteration 124/1000 | Loss: 0.00003585
Iteration 125/1000 | Loss: 0.00003585
Iteration 126/1000 | Loss: 0.00003585
Iteration 127/1000 | Loss: 0.00003584
Iteration 128/1000 | Loss: 0.00003584
Iteration 129/1000 | Loss: 0.00003584
Iteration 130/1000 | Loss: 0.00003584
Iteration 131/1000 | Loss: 0.00003584
Iteration 132/1000 | Loss: 0.00003584
Iteration 133/1000 | Loss: 0.00003584
Iteration 134/1000 | Loss: 0.00003584
Iteration 135/1000 | Loss: 0.00003584
Iteration 136/1000 | Loss: 0.00003584
Iteration 137/1000 | Loss: 0.00003584
Iteration 138/1000 | Loss: 0.00003584
Iteration 139/1000 | Loss: 0.00003583
Iteration 140/1000 | Loss: 0.00003583
Iteration 141/1000 | Loss: 0.00003583
Iteration 142/1000 | Loss: 0.00003583
Iteration 143/1000 | Loss: 0.00003583
Iteration 144/1000 | Loss: 0.00003583
Iteration 145/1000 | Loss: 0.00003583
Iteration 146/1000 | Loss: 0.00003583
Iteration 147/1000 | Loss: 0.00003583
Iteration 148/1000 | Loss: 0.00003583
Iteration 149/1000 | Loss: 0.00003583
Iteration 150/1000 | Loss: 0.00003583
Iteration 151/1000 | Loss: 0.00003583
Iteration 152/1000 | Loss: 0.00003583
Iteration 153/1000 | Loss: 0.00003583
Iteration 154/1000 | Loss: 0.00003583
Iteration 155/1000 | Loss: 0.00003582
Iteration 156/1000 | Loss: 0.00003582
Iteration 157/1000 | Loss: 0.00003582
Iteration 158/1000 | Loss: 0.00003582
Iteration 159/1000 | Loss: 0.00003582
Iteration 160/1000 | Loss: 0.00003582
Iteration 161/1000 | Loss: 0.00003582
Iteration 162/1000 | Loss: 0.00003582
Iteration 163/1000 | Loss: 0.00003582
Iteration 164/1000 | Loss: 0.00003582
Iteration 165/1000 | Loss: 0.00003582
Iteration 166/1000 | Loss: 0.00003582
Iteration 167/1000 | Loss: 0.00003582
Iteration 168/1000 | Loss: 0.00003582
Iteration 169/1000 | Loss: 0.00003582
Iteration 170/1000 | Loss: 0.00003582
Iteration 171/1000 | Loss: 0.00003582
Iteration 172/1000 | Loss: 0.00003582
Iteration 173/1000 | Loss: 0.00003582
Iteration 174/1000 | Loss: 0.00003582
Iteration 175/1000 | Loss: 0.00003582
Iteration 176/1000 | Loss: 0.00003582
Iteration 177/1000 | Loss: 0.00003582
Iteration 178/1000 | Loss: 0.00003582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [3.58163051714655e-05, 3.58163051714655e-05, 3.58163051714655e-05, 3.58163051714655e-05, 3.58163051714655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.58163051714655e-05

Optimization complete. Final v2v error: 4.584839344024658 mm

Highest mean error: 5.941007137298584 mm for frame 155

Lowest mean error: 4.106725215911865 mm for frame 143

Saving results

Total time: 109.43498969078064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00927169
Iteration 2/25 | Loss: 0.00314868
Iteration 3/25 | Loss: 0.00227181
Iteration 4/25 | Loss: 0.00196734
Iteration 5/25 | Loss: 0.00169354
Iteration 6/25 | Loss: 0.00156726
Iteration 7/25 | Loss: 0.00150071
Iteration 8/25 | Loss: 0.00144042
Iteration 9/25 | Loss: 0.00140368
Iteration 10/25 | Loss: 0.00138174
Iteration 11/25 | Loss: 0.00136975
Iteration 12/25 | Loss: 0.00136536
Iteration 13/25 | Loss: 0.00136267
Iteration 14/25 | Loss: 0.00135942
Iteration 15/25 | Loss: 0.00135596
Iteration 16/25 | Loss: 0.00135505
Iteration 17/25 | Loss: 0.00135380
Iteration 18/25 | Loss: 0.00135262
Iteration 19/25 | Loss: 0.00135322
Iteration 20/25 | Loss: 0.00134974
Iteration 21/25 | Loss: 0.00135158
Iteration 22/25 | Loss: 0.00134788
Iteration 23/25 | Loss: 0.00134769
Iteration 24/25 | Loss: 0.00134766
Iteration 25/25 | Loss: 0.00134765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27197707
Iteration 2/25 | Loss: 0.00181404
Iteration 3/25 | Loss: 0.00175114
Iteration 4/25 | Loss: 0.00175114
Iteration 5/25 | Loss: 0.00175114
Iteration 6/25 | Loss: 0.00175114
Iteration 7/25 | Loss: 0.00175114
Iteration 8/25 | Loss: 0.00175114
Iteration 9/25 | Loss: 0.00175114
Iteration 10/25 | Loss: 0.00175114
Iteration 11/25 | Loss: 0.00175114
Iteration 12/25 | Loss: 0.00175114
Iteration 13/25 | Loss: 0.00175114
Iteration 14/25 | Loss: 0.00175114
Iteration 15/25 | Loss: 0.00175114
Iteration 16/25 | Loss: 0.00175114
Iteration 17/25 | Loss: 0.00175114
Iteration 18/25 | Loss: 0.00175114
Iteration 19/25 | Loss: 0.00175114
Iteration 20/25 | Loss: 0.00175114
Iteration 21/25 | Loss: 0.00175114
Iteration 22/25 | Loss: 0.00175114
Iteration 23/25 | Loss: 0.00175114
Iteration 24/25 | Loss: 0.00175114
Iteration 25/25 | Loss: 0.00175114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175114
Iteration 2/1000 | Loss: 0.00052469
Iteration 3/1000 | Loss: 0.00022357
Iteration 4/1000 | Loss: 0.00022769
Iteration 5/1000 | Loss: 0.00057472
Iteration 6/1000 | Loss: 0.00007716
Iteration 7/1000 | Loss: 0.00006413
Iteration 8/1000 | Loss: 0.00005638
Iteration 9/1000 | Loss: 0.00005037
Iteration 10/1000 | Loss: 0.00004643
Iteration 11/1000 | Loss: 0.00004367
Iteration 12/1000 | Loss: 0.00042335
Iteration 13/1000 | Loss: 0.00046186
Iteration 14/1000 | Loss: 0.00006021
Iteration 15/1000 | Loss: 0.00004515
Iteration 16/1000 | Loss: 0.00003655
Iteration 17/1000 | Loss: 0.00002943
Iteration 18/1000 | Loss: 0.00002530
Iteration 19/1000 | Loss: 0.00003928
Iteration 20/1000 | Loss: 0.00002409
Iteration 21/1000 | Loss: 0.00002667
Iteration 22/1000 | Loss: 0.00005025
Iteration 23/1000 | Loss: 0.00002108
Iteration 24/1000 | Loss: 0.00002381
Iteration 25/1000 | Loss: 0.00001810
Iteration 26/1000 | Loss: 0.00002049
Iteration 27/1000 | Loss: 0.00001706
Iteration 28/1000 | Loss: 0.00001661
Iteration 29/1000 | Loss: 0.00001627
Iteration 30/1000 | Loss: 0.00001591
Iteration 31/1000 | Loss: 0.00001561
Iteration 32/1000 | Loss: 0.00001545
Iteration 33/1000 | Loss: 0.00001544
Iteration 34/1000 | Loss: 0.00001541
Iteration 35/1000 | Loss: 0.00001541
Iteration 36/1000 | Loss: 0.00001540
Iteration 37/1000 | Loss: 0.00001538
Iteration 38/1000 | Loss: 0.00001534
Iteration 39/1000 | Loss: 0.00001534
Iteration 40/1000 | Loss: 0.00001534
Iteration 41/1000 | Loss: 0.00001529
Iteration 42/1000 | Loss: 0.00001526
Iteration 43/1000 | Loss: 0.00001524
Iteration 44/1000 | Loss: 0.00001524
Iteration 45/1000 | Loss: 0.00001523
Iteration 46/1000 | Loss: 0.00001523
Iteration 47/1000 | Loss: 0.00001523
Iteration 48/1000 | Loss: 0.00001522
Iteration 49/1000 | Loss: 0.00001522
Iteration 50/1000 | Loss: 0.00001522
Iteration 51/1000 | Loss: 0.00001522
Iteration 52/1000 | Loss: 0.00001522
Iteration 53/1000 | Loss: 0.00001522
Iteration 54/1000 | Loss: 0.00001522
Iteration 55/1000 | Loss: 0.00001521
Iteration 56/1000 | Loss: 0.00001521
Iteration 57/1000 | Loss: 0.00001521
Iteration 58/1000 | Loss: 0.00001521
Iteration 59/1000 | Loss: 0.00001521
Iteration 60/1000 | Loss: 0.00001521
Iteration 61/1000 | Loss: 0.00001521
Iteration 62/1000 | Loss: 0.00001521
Iteration 63/1000 | Loss: 0.00001521
Iteration 64/1000 | Loss: 0.00001521
Iteration 65/1000 | Loss: 0.00001520
Iteration 66/1000 | Loss: 0.00001520
Iteration 67/1000 | Loss: 0.00001519
Iteration 68/1000 | Loss: 0.00001519
Iteration 69/1000 | Loss: 0.00001519
Iteration 70/1000 | Loss: 0.00001519
Iteration 71/1000 | Loss: 0.00001519
Iteration 72/1000 | Loss: 0.00001519
Iteration 73/1000 | Loss: 0.00001519
Iteration 74/1000 | Loss: 0.00001519
Iteration 75/1000 | Loss: 0.00001519
Iteration 76/1000 | Loss: 0.00001519
Iteration 77/1000 | Loss: 0.00001519
Iteration 78/1000 | Loss: 0.00001519
Iteration 79/1000 | Loss: 0.00001519
Iteration 80/1000 | Loss: 0.00001519
Iteration 81/1000 | Loss: 0.00001519
Iteration 82/1000 | Loss: 0.00001519
Iteration 83/1000 | Loss: 0.00001519
Iteration 84/1000 | Loss: 0.00001518
Iteration 85/1000 | Loss: 0.00001518
Iteration 86/1000 | Loss: 0.00001518
Iteration 87/1000 | Loss: 0.00001518
Iteration 88/1000 | Loss: 0.00001518
Iteration 89/1000 | Loss: 0.00001518
Iteration 90/1000 | Loss: 0.00001518
Iteration 91/1000 | Loss: 0.00001518
Iteration 92/1000 | Loss: 0.00001518
Iteration 93/1000 | Loss: 0.00001518
Iteration 94/1000 | Loss: 0.00001518
Iteration 95/1000 | Loss: 0.00001518
Iteration 96/1000 | Loss: 0.00001518
Iteration 97/1000 | Loss: 0.00001518
Iteration 98/1000 | Loss: 0.00001517
Iteration 99/1000 | Loss: 0.00001517
Iteration 100/1000 | Loss: 0.00001517
Iteration 101/1000 | Loss: 0.00001517
Iteration 102/1000 | Loss: 0.00001517
Iteration 103/1000 | Loss: 0.00001517
Iteration 104/1000 | Loss: 0.00001517
Iteration 105/1000 | Loss: 0.00001517
Iteration 106/1000 | Loss: 0.00001517
Iteration 107/1000 | Loss: 0.00001517
Iteration 108/1000 | Loss: 0.00001517
Iteration 109/1000 | Loss: 0.00001517
Iteration 110/1000 | Loss: 0.00001517
Iteration 111/1000 | Loss: 0.00001517
Iteration 112/1000 | Loss: 0.00001517
Iteration 113/1000 | Loss: 0.00001517
Iteration 114/1000 | Loss: 0.00001517
Iteration 115/1000 | Loss: 0.00001517
Iteration 116/1000 | Loss: 0.00001517
Iteration 117/1000 | Loss: 0.00001517
Iteration 118/1000 | Loss: 0.00001517
Iteration 119/1000 | Loss: 0.00001517
Iteration 120/1000 | Loss: 0.00001517
Iteration 121/1000 | Loss: 0.00001517
Iteration 122/1000 | Loss: 0.00001517
Iteration 123/1000 | Loss: 0.00001517
Iteration 124/1000 | Loss: 0.00001517
Iteration 125/1000 | Loss: 0.00001517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.5172213352343533e-05, 1.5172213352343533e-05, 1.5172213352343533e-05, 1.5172213352343533e-05, 1.5172213352343533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5172213352343533e-05

Optimization complete. Final v2v error: 3.3004491329193115 mm

Highest mean error: 3.771934986114502 mm for frame 80

Lowest mean error: 2.96339750289917 mm for frame 206

Saving results

Total time: 107.05828833580017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387330
Iteration 2/25 | Loss: 0.00126992
Iteration 3/25 | Loss: 0.00119867
Iteration 4/25 | Loss: 0.00119093
Iteration 5/25 | Loss: 0.00119032
Iteration 6/25 | Loss: 0.00119032
Iteration 7/25 | Loss: 0.00119032
Iteration 8/25 | Loss: 0.00119032
Iteration 9/25 | Loss: 0.00119032
Iteration 10/25 | Loss: 0.00119032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011903162812814116, 0.0011903162812814116, 0.0011903162812814116, 0.0011903162812814116, 0.0011903162812814116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011903162812814116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29175699
Iteration 2/25 | Loss: 0.00139144
Iteration 3/25 | Loss: 0.00139144
Iteration 4/25 | Loss: 0.00139143
Iteration 5/25 | Loss: 0.00139143
Iteration 6/25 | Loss: 0.00139143
Iteration 7/25 | Loss: 0.00139143
Iteration 8/25 | Loss: 0.00139143
Iteration 9/25 | Loss: 0.00139143
Iteration 10/25 | Loss: 0.00139143
Iteration 11/25 | Loss: 0.00139143
Iteration 12/25 | Loss: 0.00139143
Iteration 13/25 | Loss: 0.00139143
Iteration 14/25 | Loss: 0.00139143
Iteration 15/25 | Loss: 0.00139143
Iteration 16/25 | Loss: 0.00139143
Iteration 17/25 | Loss: 0.00139143
Iteration 18/25 | Loss: 0.00139143
Iteration 19/25 | Loss: 0.00139143
Iteration 20/25 | Loss: 0.00139143
Iteration 21/25 | Loss: 0.00139143
Iteration 22/25 | Loss: 0.00139143
Iteration 23/25 | Loss: 0.00139143
Iteration 24/25 | Loss: 0.00139143
Iteration 25/25 | Loss: 0.00139143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139143
Iteration 2/1000 | Loss: 0.00002167
Iteration 3/1000 | Loss: 0.00001557
Iteration 4/1000 | Loss: 0.00001425
Iteration 5/1000 | Loss: 0.00001352
Iteration 6/1000 | Loss: 0.00001292
Iteration 7/1000 | Loss: 0.00001279
Iteration 8/1000 | Loss: 0.00001229
Iteration 9/1000 | Loss: 0.00001199
Iteration 10/1000 | Loss: 0.00001198
Iteration 11/1000 | Loss: 0.00001190
Iteration 12/1000 | Loss: 0.00001171
Iteration 13/1000 | Loss: 0.00001168
Iteration 14/1000 | Loss: 0.00001159
Iteration 15/1000 | Loss: 0.00001147
Iteration 16/1000 | Loss: 0.00001140
Iteration 17/1000 | Loss: 0.00001140
Iteration 18/1000 | Loss: 0.00001139
Iteration 19/1000 | Loss: 0.00001127
Iteration 20/1000 | Loss: 0.00001126
Iteration 21/1000 | Loss: 0.00001125
Iteration 22/1000 | Loss: 0.00001125
Iteration 23/1000 | Loss: 0.00001122
Iteration 24/1000 | Loss: 0.00001119
Iteration 25/1000 | Loss: 0.00001115
Iteration 26/1000 | Loss: 0.00001115
Iteration 27/1000 | Loss: 0.00001114
Iteration 28/1000 | Loss: 0.00001113
Iteration 29/1000 | Loss: 0.00001110
Iteration 30/1000 | Loss: 0.00001109
Iteration 31/1000 | Loss: 0.00001109
Iteration 32/1000 | Loss: 0.00001108
Iteration 33/1000 | Loss: 0.00001108
Iteration 34/1000 | Loss: 0.00001108
Iteration 35/1000 | Loss: 0.00001106
Iteration 36/1000 | Loss: 0.00001103
Iteration 37/1000 | Loss: 0.00001103
Iteration 38/1000 | Loss: 0.00001102
Iteration 39/1000 | Loss: 0.00001101
Iteration 40/1000 | Loss: 0.00001097
Iteration 41/1000 | Loss: 0.00001095
Iteration 42/1000 | Loss: 0.00001095
Iteration 43/1000 | Loss: 0.00001092
Iteration 44/1000 | Loss: 0.00001092
Iteration 45/1000 | Loss: 0.00001090
Iteration 46/1000 | Loss: 0.00001090
Iteration 47/1000 | Loss: 0.00001088
Iteration 48/1000 | Loss: 0.00001087
Iteration 49/1000 | Loss: 0.00001085
Iteration 50/1000 | Loss: 0.00001083
Iteration 51/1000 | Loss: 0.00001081
Iteration 52/1000 | Loss: 0.00001081
Iteration 53/1000 | Loss: 0.00001081
Iteration 54/1000 | Loss: 0.00001080
Iteration 55/1000 | Loss: 0.00001080
Iteration 56/1000 | Loss: 0.00001080
Iteration 57/1000 | Loss: 0.00001079
Iteration 58/1000 | Loss: 0.00001079
Iteration 59/1000 | Loss: 0.00001079
Iteration 60/1000 | Loss: 0.00001079
Iteration 61/1000 | Loss: 0.00001079
Iteration 62/1000 | Loss: 0.00001079
Iteration 63/1000 | Loss: 0.00001078
Iteration 64/1000 | Loss: 0.00001078
Iteration 65/1000 | Loss: 0.00001077
Iteration 66/1000 | Loss: 0.00001077
Iteration 67/1000 | Loss: 0.00001076
Iteration 68/1000 | Loss: 0.00001076
Iteration 69/1000 | Loss: 0.00001075
Iteration 70/1000 | Loss: 0.00001075
Iteration 71/1000 | Loss: 0.00001075
Iteration 72/1000 | Loss: 0.00001075
Iteration 73/1000 | Loss: 0.00001074
Iteration 74/1000 | Loss: 0.00001074
Iteration 75/1000 | Loss: 0.00001074
Iteration 76/1000 | Loss: 0.00001073
Iteration 77/1000 | Loss: 0.00001073
Iteration 78/1000 | Loss: 0.00001073
Iteration 79/1000 | Loss: 0.00001073
Iteration 80/1000 | Loss: 0.00001073
Iteration 81/1000 | Loss: 0.00001073
Iteration 82/1000 | Loss: 0.00001073
Iteration 83/1000 | Loss: 0.00001073
Iteration 84/1000 | Loss: 0.00001073
Iteration 85/1000 | Loss: 0.00001072
Iteration 86/1000 | Loss: 0.00001072
Iteration 87/1000 | Loss: 0.00001071
Iteration 88/1000 | Loss: 0.00001070
Iteration 89/1000 | Loss: 0.00001069
Iteration 90/1000 | Loss: 0.00001069
Iteration 91/1000 | Loss: 0.00001068
Iteration 92/1000 | Loss: 0.00001068
Iteration 93/1000 | Loss: 0.00001068
Iteration 94/1000 | Loss: 0.00001067
Iteration 95/1000 | Loss: 0.00001067
Iteration 96/1000 | Loss: 0.00001067
Iteration 97/1000 | Loss: 0.00001067
Iteration 98/1000 | Loss: 0.00001067
Iteration 99/1000 | Loss: 0.00001067
Iteration 100/1000 | Loss: 0.00001067
Iteration 101/1000 | Loss: 0.00001066
Iteration 102/1000 | Loss: 0.00001066
Iteration 103/1000 | Loss: 0.00001066
Iteration 104/1000 | Loss: 0.00001066
Iteration 105/1000 | Loss: 0.00001065
Iteration 106/1000 | Loss: 0.00001065
Iteration 107/1000 | Loss: 0.00001065
Iteration 108/1000 | Loss: 0.00001065
Iteration 109/1000 | Loss: 0.00001065
Iteration 110/1000 | Loss: 0.00001065
Iteration 111/1000 | Loss: 0.00001065
Iteration 112/1000 | Loss: 0.00001065
Iteration 113/1000 | Loss: 0.00001064
Iteration 114/1000 | Loss: 0.00001064
Iteration 115/1000 | Loss: 0.00001064
Iteration 116/1000 | Loss: 0.00001064
Iteration 117/1000 | Loss: 0.00001064
Iteration 118/1000 | Loss: 0.00001064
Iteration 119/1000 | Loss: 0.00001064
Iteration 120/1000 | Loss: 0.00001064
Iteration 121/1000 | Loss: 0.00001063
Iteration 122/1000 | Loss: 0.00001063
Iteration 123/1000 | Loss: 0.00001063
Iteration 124/1000 | Loss: 0.00001063
Iteration 125/1000 | Loss: 0.00001063
Iteration 126/1000 | Loss: 0.00001063
Iteration 127/1000 | Loss: 0.00001063
Iteration 128/1000 | Loss: 0.00001063
Iteration 129/1000 | Loss: 0.00001063
Iteration 130/1000 | Loss: 0.00001063
Iteration 131/1000 | Loss: 0.00001063
Iteration 132/1000 | Loss: 0.00001063
Iteration 133/1000 | Loss: 0.00001063
Iteration 134/1000 | Loss: 0.00001063
Iteration 135/1000 | Loss: 0.00001063
Iteration 136/1000 | Loss: 0.00001063
Iteration 137/1000 | Loss: 0.00001063
Iteration 138/1000 | Loss: 0.00001063
Iteration 139/1000 | Loss: 0.00001063
Iteration 140/1000 | Loss: 0.00001063
Iteration 141/1000 | Loss: 0.00001063
Iteration 142/1000 | Loss: 0.00001063
Iteration 143/1000 | Loss: 0.00001063
Iteration 144/1000 | Loss: 0.00001063
Iteration 145/1000 | Loss: 0.00001063
Iteration 146/1000 | Loss: 0.00001063
Iteration 147/1000 | Loss: 0.00001063
Iteration 148/1000 | Loss: 0.00001063
Iteration 149/1000 | Loss: 0.00001063
Iteration 150/1000 | Loss: 0.00001063
Iteration 151/1000 | Loss: 0.00001063
Iteration 152/1000 | Loss: 0.00001063
Iteration 153/1000 | Loss: 0.00001063
Iteration 154/1000 | Loss: 0.00001063
Iteration 155/1000 | Loss: 0.00001063
Iteration 156/1000 | Loss: 0.00001063
Iteration 157/1000 | Loss: 0.00001063
Iteration 158/1000 | Loss: 0.00001063
Iteration 159/1000 | Loss: 0.00001063
Iteration 160/1000 | Loss: 0.00001063
Iteration 161/1000 | Loss: 0.00001063
Iteration 162/1000 | Loss: 0.00001063
Iteration 163/1000 | Loss: 0.00001063
Iteration 164/1000 | Loss: 0.00001063
Iteration 165/1000 | Loss: 0.00001063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.0630717952153645e-05, 1.0630717952153645e-05, 1.0630717952153645e-05, 1.0630717952153645e-05, 1.0630717952153645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0630717952153645e-05

Optimization complete. Final v2v error: 2.828949451446533 mm

Highest mean error: 2.9291419982910156 mm for frame 210

Lowest mean error: 2.7468433380126953 mm for frame 231

Saving results

Total time: 43.100990295410156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997718
Iteration 2/25 | Loss: 0.00358319
Iteration 3/25 | Loss: 0.00204970
Iteration 4/25 | Loss: 0.00199598
Iteration 5/25 | Loss: 0.00172133
Iteration 6/25 | Loss: 0.00147287
Iteration 7/25 | Loss: 0.00135658
Iteration 8/25 | Loss: 0.00132858
Iteration 9/25 | Loss: 0.00127986
Iteration 10/25 | Loss: 0.00126937
Iteration 11/25 | Loss: 0.00126172
Iteration 12/25 | Loss: 0.00124486
Iteration 13/25 | Loss: 0.00123090
Iteration 14/25 | Loss: 0.00122525
Iteration 15/25 | Loss: 0.00122173
Iteration 16/25 | Loss: 0.00122384
Iteration 17/25 | Loss: 0.00121931
Iteration 18/25 | Loss: 0.00122027
Iteration 19/25 | Loss: 0.00121905
Iteration 20/25 | Loss: 0.00121891
Iteration 21/25 | Loss: 0.00121876
Iteration 22/25 | Loss: 0.00121984
Iteration 23/25 | Loss: 0.00121949
Iteration 24/25 | Loss: 0.00121951
Iteration 25/25 | Loss: 0.00121842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27137411
Iteration 2/25 | Loss: 0.00172613
Iteration 3/25 | Loss: 0.00172613
Iteration 4/25 | Loss: 0.00172613
Iteration 5/25 | Loss: 0.00172613
Iteration 6/25 | Loss: 0.00171136
Iteration 7/25 | Loss: 0.00171136
Iteration 8/25 | Loss: 0.00171136
Iteration 9/25 | Loss: 0.00171136
Iteration 10/25 | Loss: 0.00171136
Iteration 11/25 | Loss: 0.00171136
Iteration 12/25 | Loss: 0.00171136
Iteration 13/25 | Loss: 0.00171135
Iteration 14/25 | Loss: 0.00171135
Iteration 15/25 | Loss: 0.00171135
Iteration 16/25 | Loss: 0.00171135
Iteration 17/25 | Loss: 0.00171135
Iteration 18/25 | Loss: 0.00171135
Iteration 19/25 | Loss: 0.00171135
Iteration 20/25 | Loss: 0.00171135
Iteration 21/25 | Loss: 0.00171135
Iteration 22/25 | Loss: 0.00171135
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0017113533103838563, 0.0017113533103838563, 0.0017113533103838563, 0.0017113533103838563, 0.0017113533103838563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017113533103838563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171135
Iteration 2/1000 | Loss: 0.00007932
Iteration 3/1000 | Loss: 0.00004443
Iteration 4/1000 | Loss: 0.00003560
Iteration 5/1000 | Loss: 0.00003060
Iteration 6/1000 | Loss: 0.00003110
Iteration 7/1000 | Loss: 0.00002798
Iteration 8/1000 | Loss: 0.00002791
Iteration 9/1000 | Loss: 0.00002766
Iteration 10/1000 | Loss: 0.00002646
Iteration 11/1000 | Loss: 0.00003161
Iteration 12/1000 | Loss: 0.00008541
Iteration 13/1000 | Loss: 0.00002596
Iteration 14/1000 | Loss: 0.00002557
Iteration 15/1000 | Loss: 0.00003059
Iteration 16/1000 | Loss: 0.00004714
Iteration 17/1000 | Loss: 0.00002590
Iteration 18/1000 | Loss: 0.00002931
Iteration 19/1000 | Loss: 0.00002516
Iteration 20/1000 | Loss: 0.00002994
Iteration 21/1000 | Loss: 0.00003081
Iteration 22/1000 | Loss: 0.00003010
Iteration 23/1000 | Loss: 0.00004441
Iteration 24/1000 | Loss: 0.00004312
Iteration 25/1000 | Loss: 0.00002926
Iteration 26/1000 | Loss: 0.00003179
Iteration 27/1000 | Loss: 0.00002505
Iteration 28/1000 | Loss: 0.00002565
Iteration 29/1000 | Loss: 0.00002477
Iteration 30/1000 | Loss: 0.00002494
Iteration 31/1000 | Loss: 0.00002494
Iteration 32/1000 | Loss: 0.00002494
Iteration 33/1000 | Loss: 0.00002475
Iteration 34/1000 | Loss: 0.00002474
Iteration 35/1000 | Loss: 0.00002474
Iteration 36/1000 | Loss: 0.00002474
Iteration 37/1000 | Loss: 0.00002473
Iteration 38/1000 | Loss: 0.00002473
Iteration 39/1000 | Loss: 0.00002473
Iteration 40/1000 | Loss: 0.00002473
Iteration 41/1000 | Loss: 0.00002473
Iteration 42/1000 | Loss: 0.00002473
Iteration 43/1000 | Loss: 0.00002473
Iteration 44/1000 | Loss: 0.00002473
Iteration 45/1000 | Loss: 0.00002473
Iteration 46/1000 | Loss: 0.00002473
Iteration 47/1000 | Loss: 0.00002473
Iteration 48/1000 | Loss: 0.00002472
Iteration 49/1000 | Loss: 0.00002472
Iteration 50/1000 | Loss: 0.00002471
Iteration 51/1000 | Loss: 0.00002481
Iteration 52/1000 | Loss: 0.00002481
Iteration 53/1000 | Loss: 0.00002492
Iteration 54/1000 | Loss: 0.00002568
Iteration 55/1000 | Loss: 0.00002467
Iteration 56/1000 | Loss: 0.00002467
Iteration 57/1000 | Loss: 0.00002467
Iteration 58/1000 | Loss: 0.00002467
Iteration 59/1000 | Loss: 0.00002467
Iteration 60/1000 | Loss: 0.00002466
Iteration 61/1000 | Loss: 0.00002466
Iteration 62/1000 | Loss: 0.00002466
Iteration 63/1000 | Loss: 0.00002466
Iteration 64/1000 | Loss: 0.00002465
Iteration 65/1000 | Loss: 0.00002465
Iteration 66/1000 | Loss: 0.00002465
Iteration 67/1000 | Loss: 0.00002465
Iteration 68/1000 | Loss: 0.00002465
Iteration 69/1000 | Loss: 0.00002486
Iteration 70/1000 | Loss: 0.00002566
Iteration 71/1000 | Loss: 0.00002577
Iteration 72/1000 | Loss: 0.00002469
Iteration 73/1000 | Loss: 0.00002458
Iteration 74/1000 | Loss: 0.00002458
Iteration 75/1000 | Loss: 0.00002458
Iteration 76/1000 | Loss: 0.00002458
Iteration 77/1000 | Loss: 0.00002458
Iteration 78/1000 | Loss: 0.00002458
Iteration 79/1000 | Loss: 0.00002458
Iteration 80/1000 | Loss: 0.00002458
Iteration 81/1000 | Loss: 0.00002458
Iteration 82/1000 | Loss: 0.00002458
Iteration 83/1000 | Loss: 0.00002457
Iteration 84/1000 | Loss: 0.00002462
Iteration 85/1000 | Loss: 0.00003092
Iteration 86/1000 | Loss: 0.00002988
Iteration 87/1000 | Loss: 0.00002510
Iteration 88/1000 | Loss: 0.00002608
Iteration 89/1000 | Loss: 0.00002716
Iteration 90/1000 | Loss: 0.00002452
Iteration 91/1000 | Loss: 0.00002449
Iteration 92/1000 | Loss: 0.00002449
Iteration 93/1000 | Loss: 0.00002449
Iteration 94/1000 | Loss: 0.00002449
Iteration 95/1000 | Loss: 0.00002449
Iteration 96/1000 | Loss: 0.00002449
Iteration 97/1000 | Loss: 0.00002449
Iteration 98/1000 | Loss: 0.00002449
Iteration 99/1000 | Loss: 0.00002449
Iteration 100/1000 | Loss: 0.00002449
Iteration 101/1000 | Loss: 0.00002449
Iteration 102/1000 | Loss: 0.00002449
Iteration 103/1000 | Loss: 0.00002449
Iteration 104/1000 | Loss: 0.00002449
Iteration 105/1000 | Loss: 0.00002449
Iteration 106/1000 | Loss: 0.00002449
Iteration 107/1000 | Loss: 0.00002449
Iteration 108/1000 | Loss: 0.00002449
Iteration 109/1000 | Loss: 0.00002449
Iteration 110/1000 | Loss: 0.00002449
Iteration 111/1000 | Loss: 0.00002449
Iteration 112/1000 | Loss: 0.00002449
Iteration 113/1000 | Loss: 0.00002449
Iteration 114/1000 | Loss: 0.00002449
Iteration 115/1000 | Loss: 0.00002449
Iteration 116/1000 | Loss: 0.00002449
Iteration 117/1000 | Loss: 0.00002449
Iteration 118/1000 | Loss: 0.00002449
Iteration 119/1000 | Loss: 0.00002449
Iteration 120/1000 | Loss: 0.00002449
Iteration 121/1000 | Loss: 0.00002449
Iteration 122/1000 | Loss: 0.00002449
Iteration 123/1000 | Loss: 0.00002449
Iteration 124/1000 | Loss: 0.00002449
Iteration 125/1000 | Loss: 0.00002449
Iteration 126/1000 | Loss: 0.00002449
Iteration 127/1000 | Loss: 0.00002449
Iteration 128/1000 | Loss: 0.00002449
Iteration 129/1000 | Loss: 0.00002449
Iteration 130/1000 | Loss: 0.00002449
Iteration 131/1000 | Loss: 0.00002449
Iteration 132/1000 | Loss: 0.00002449
Iteration 133/1000 | Loss: 0.00002449
Iteration 134/1000 | Loss: 0.00002449
Iteration 135/1000 | Loss: 0.00002449
Iteration 136/1000 | Loss: 0.00002449
Iteration 137/1000 | Loss: 0.00002449
Iteration 138/1000 | Loss: 0.00002449
Iteration 139/1000 | Loss: 0.00002449
Iteration 140/1000 | Loss: 0.00002449
Iteration 141/1000 | Loss: 0.00002449
Iteration 142/1000 | Loss: 0.00002449
Iteration 143/1000 | Loss: 0.00002449
Iteration 144/1000 | Loss: 0.00002449
Iteration 145/1000 | Loss: 0.00002449
Iteration 146/1000 | Loss: 0.00002449
Iteration 147/1000 | Loss: 0.00002449
Iteration 148/1000 | Loss: 0.00002449
Iteration 149/1000 | Loss: 0.00002449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.4488575945724733e-05, 2.4488575945724733e-05, 2.4488575945724733e-05, 2.4488575945724733e-05, 2.4488575945724733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4488575945724733e-05

Optimization complete. Final v2v error: 3.078007698059082 mm

Highest mean error: 10.023828506469727 mm for frame 30

Lowest mean error: 2.2971560955047607 mm for frame 14

Saving results

Total time: 100.75871253013611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424772
Iteration 2/25 | Loss: 0.00134842
Iteration 3/25 | Loss: 0.00124008
Iteration 4/25 | Loss: 0.00122095
Iteration 5/25 | Loss: 0.00121622
Iteration 6/25 | Loss: 0.00121582
Iteration 7/25 | Loss: 0.00121582
Iteration 8/25 | Loss: 0.00121582
Iteration 9/25 | Loss: 0.00121582
Iteration 10/25 | Loss: 0.00121582
Iteration 11/25 | Loss: 0.00121582
Iteration 12/25 | Loss: 0.00121582
Iteration 13/25 | Loss: 0.00121582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012158241588622332, 0.0012158241588622332, 0.0012158241588622332, 0.0012158241588622332, 0.0012158241588622332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012158241588622332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25604045
Iteration 2/25 | Loss: 0.00217314
Iteration 3/25 | Loss: 0.00217314
Iteration 4/25 | Loss: 0.00217314
Iteration 5/25 | Loss: 0.00217314
Iteration 6/25 | Loss: 0.00217314
Iteration 7/25 | Loss: 0.00217314
Iteration 8/25 | Loss: 0.00217314
Iteration 9/25 | Loss: 0.00217314
Iteration 10/25 | Loss: 0.00217314
Iteration 11/25 | Loss: 0.00217314
Iteration 12/25 | Loss: 0.00217314
Iteration 13/25 | Loss: 0.00217314
Iteration 14/25 | Loss: 0.00217314
Iteration 15/25 | Loss: 0.00217314
Iteration 16/25 | Loss: 0.00217314
Iteration 17/25 | Loss: 0.00217314
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0021731359884142876, 0.0021731359884142876, 0.0021731359884142876, 0.0021731359884142876, 0.0021731359884142876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021731359884142876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217314
Iteration 2/1000 | Loss: 0.00005117
Iteration 3/1000 | Loss: 0.00003483
Iteration 4/1000 | Loss: 0.00002833
Iteration 5/1000 | Loss: 0.00002624
Iteration 6/1000 | Loss: 0.00002445
Iteration 7/1000 | Loss: 0.00002335
Iteration 8/1000 | Loss: 0.00002262
Iteration 9/1000 | Loss: 0.00002215
Iteration 10/1000 | Loss: 0.00002174
Iteration 11/1000 | Loss: 0.00002141
Iteration 12/1000 | Loss: 0.00002122
Iteration 13/1000 | Loss: 0.00002102
Iteration 14/1000 | Loss: 0.00002099
Iteration 15/1000 | Loss: 0.00002082
Iteration 16/1000 | Loss: 0.00002076
Iteration 17/1000 | Loss: 0.00002067
Iteration 18/1000 | Loss: 0.00002056
Iteration 19/1000 | Loss: 0.00002056
Iteration 20/1000 | Loss: 0.00002055
Iteration 21/1000 | Loss: 0.00002055
Iteration 22/1000 | Loss: 0.00002054
Iteration 23/1000 | Loss: 0.00002052
Iteration 24/1000 | Loss: 0.00002051
Iteration 25/1000 | Loss: 0.00002047
Iteration 26/1000 | Loss: 0.00002047
Iteration 27/1000 | Loss: 0.00002045
Iteration 28/1000 | Loss: 0.00002044
Iteration 29/1000 | Loss: 0.00002042
Iteration 30/1000 | Loss: 0.00002041
Iteration 31/1000 | Loss: 0.00002041
Iteration 32/1000 | Loss: 0.00002039
Iteration 33/1000 | Loss: 0.00002039
Iteration 34/1000 | Loss: 0.00002038
Iteration 35/1000 | Loss: 0.00002038
Iteration 36/1000 | Loss: 0.00002035
Iteration 37/1000 | Loss: 0.00002034
Iteration 38/1000 | Loss: 0.00002033
Iteration 39/1000 | Loss: 0.00002033
Iteration 40/1000 | Loss: 0.00002032
Iteration 41/1000 | Loss: 0.00002031
Iteration 42/1000 | Loss: 0.00002031
Iteration 43/1000 | Loss: 0.00002031
Iteration 44/1000 | Loss: 0.00002031
Iteration 45/1000 | Loss: 0.00002030
Iteration 46/1000 | Loss: 0.00002030
Iteration 47/1000 | Loss: 0.00002029
Iteration 48/1000 | Loss: 0.00002029
Iteration 49/1000 | Loss: 0.00002029
Iteration 50/1000 | Loss: 0.00002028
Iteration 51/1000 | Loss: 0.00002028
Iteration 52/1000 | Loss: 0.00002027
Iteration 53/1000 | Loss: 0.00002027
Iteration 54/1000 | Loss: 0.00002027
Iteration 55/1000 | Loss: 0.00002026
Iteration 56/1000 | Loss: 0.00002026
Iteration 57/1000 | Loss: 0.00002026
Iteration 58/1000 | Loss: 0.00002025
Iteration 59/1000 | Loss: 0.00002024
Iteration 60/1000 | Loss: 0.00002023
Iteration 61/1000 | Loss: 0.00002023
Iteration 62/1000 | Loss: 0.00002023
Iteration 63/1000 | Loss: 0.00002023
Iteration 64/1000 | Loss: 0.00002023
Iteration 65/1000 | Loss: 0.00002022
Iteration 66/1000 | Loss: 0.00002022
Iteration 67/1000 | Loss: 0.00002021
Iteration 68/1000 | Loss: 0.00002021
Iteration 69/1000 | Loss: 0.00002021
Iteration 70/1000 | Loss: 0.00002021
Iteration 71/1000 | Loss: 0.00002020
Iteration 72/1000 | Loss: 0.00002020
Iteration 73/1000 | Loss: 0.00002020
Iteration 74/1000 | Loss: 0.00002019
Iteration 75/1000 | Loss: 0.00002019
Iteration 76/1000 | Loss: 0.00002019
Iteration 77/1000 | Loss: 0.00002019
Iteration 78/1000 | Loss: 0.00002019
Iteration 79/1000 | Loss: 0.00002019
Iteration 80/1000 | Loss: 0.00002018
Iteration 81/1000 | Loss: 0.00002018
Iteration 82/1000 | Loss: 0.00002018
Iteration 83/1000 | Loss: 0.00002018
Iteration 84/1000 | Loss: 0.00002017
Iteration 85/1000 | Loss: 0.00002016
Iteration 86/1000 | Loss: 0.00002016
Iteration 87/1000 | Loss: 0.00002016
Iteration 88/1000 | Loss: 0.00002016
Iteration 89/1000 | Loss: 0.00002015
Iteration 90/1000 | Loss: 0.00002015
Iteration 91/1000 | Loss: 0.00002014
Iteration 92/1000 | Loss: 0.00002014
Iteration 93/1000 | Loss: 0.00002014
Iteration 94/1000 | Loss: 0.00002014
Iteration 95/1000 | Loss: 0.00002013
Iteration 96/1000 | Loss: 0.00002013
Iteration 97/1000 | Loss: 0.00002013
Iteration 98/1000 | Loss: 0.00002013
Iteration 99/1000 | Loss: 0.00002013
Iteration 100/1000 | Loss: 0.00002012
Iteration 101/1000 | Loss: 0.00002012
Iteration 102/1000 | Loss: 0.00002012
Iteration 103/1000 | Loss: 0.00002012
Iteration 104/1000 | Loss: 0.00002011
Iteration 105/1000 | Loss: 0.00002011
Iteration 106/1000 | Loss: 0.00002010
Iteration 107/1000 | Loss: 0.00002010
Iteration 108/1000 | Loss: 0.00002010
Iteration 109/1000 | Loss: 0.00002009
Iteration 110/1000 | Loss: 0.00002008
Iteration 111/1000 | Loss: 0.00002008
Iteration 112/1000 | Loss: 0.00002007
Iteration 113/1000 | Loss: 0.00002007
Iteration 114/1000 | Loss: 0.00002006
Iteration 115/1000 | Loss: 0.00002005
Iteration 116/1000 | Loss: 0.00002005
Iteration 117/1000 | Loss: 0.00002005
Iteration 118/1000 | Loss: 0.00002004
Iteration 119/1000 | Loss: 0.00002004
Iteration 120/1000 | Loss: 0.00002004
Iteration 121/1000 | Loss: 0.00002004
Iteration 122/1000 | Loss: 0.00002003
Iteration 123/1000 | Loss: 0.00002003
Iteration 124/1000 | Loss: 0.00002003
Iteration 125/1000 | Loss: 0.00002002
Iteration 126/1000 | Loss: 0.00002002
Iteration 127/1000 | Loss: 0.00002002
Iteration 128/1000 | Loss: 0.00002002
Iteration 129/1000 | Loss: 0.00002001
Iteration 130/1000 | Loss: 0.00002001
Iteration 131/1000 | Loss: 0.00002001
Iteration 132/1000 | Loss: 0.00002001
Iteration 133/1000 | Loss: 0.00002001
Iteration 134/1000 | Loss: 0.00002001
Iteration 135/1000 | Loss: 0.00002000
Iteration 136/1000 | Loss: 0.00002000
Iteration 137/1000 | Loss: 0.00002000
Iteration 138/1000 | Loss: 0.00001999
Iteration 139/1000 | Loss: 0.00001999
Iteration 140/1000 | Loss: 0.00001999
Iteration 141/1000 | Loss: 0.00001999
Iteration 142/1000 | Loss: 0.00001999
Iteration 143/1000 | Loss: 0.00001999
Iteration 144/1000 | Loss: 0.00001999
Iteration 145/1000 | Loss: 0.00001999
Iteration 146/1000 | Loss: 0.00001998
Iteration 147/1000 | Loss: 0.00001998
Iteration 148/1000 | Loss: 0.00001998
Iteration 149/1000 | Loss: 0.00001998
Iteration 150/1000 | Loss: 0.00001998
Iteration 151/1000 | Loss: 0.00001998
Iteration 152/1000 | Loss: 0.00001998
Iteration 153/1000 | Loss: 0.00001998
Iteration 154/1000 | Loss: 0.00001997
Iteration 155/1000 | Loss: 0.00001997
Iteration 156/1000 | Loss: 0.00001997
Iteration 157/1000 | Loss: 0.00001997
Iteration 158/1000 | Loss: 0.00001997
Iteration 159/1000 | Loss: 0.00001997
Iteration 160/1000 | Loss: 0.00001997
Iteration 161/1000 | Loss: 0.00001996
Iteration 162/1000 | Loss: 0.00001996
Iteration 163/1000 | Loss: 0.00001996
Iteration 164/1000 | Loss: 0.00001996
Iteration 165/1000 | Loss: 0.00001996
Iteration 166/1000 | Loss: 0.00001996
Iteration 167/1000 | Loss: 0.00001996
Iteration 168/1000 | Loss: 0.00001996
Iteration 169/1000 | Loss: 0.00001996
Iteration 170/1000 | Loss: 0.00001996
Iteration 171/1000 | Loss: 0.00001995
Iteration 172/1000 | Loss: 0.00001995
Iteration 173/1000 | Loss: 0.00001995
Iteration 174/1000 | Loss: 0.00001995
Iteration 175/1000 | Loss: 0.00001995
Iteration 176/1000 | Loss: 0.00001995
Iteration 177/1000 | Loss: 0.00001995
Iteration 178/1000 | Loss: 0.00001995
Iteration 179/1000 | Loss: 0.00001995
Iteration 180/1000 | Loss: 0.00001995
Iteration 181/1000 | Loss: 0.00001995
Iteration 182/1000 | Loss: 0.00001995
Iteration 183/1000 | Loss: 0.00001995
Iteration 184/1000 | Loss: 0.00001995
Iteration 185/1000 | Loss: 0.00001995
Iteration 186/1000 | Loss: 0.00001995
Iteration 187/1000 | Loss: 0.00001995
Iteration 188/1000 | Loss: 0.00001995
Iteration 189/1000 | Loss: 0.00001995
Iteration 190/1000 | Loss: 0.00001995
Iteration 191/1000 | Loss: 0.00001995
Iteration 192/1000 | Loss: 0.00001995
Iteration 193/1000 | Loss: 0.00001995
Iteration 194/1000 | Loss: 0.00001995
Iteration 195/1000 | Loss: 0.00001995
Iteration 196/1000 | Loss: 0.00001995
Iteration 197/1000 | Loss: 0.00001995
Iteration 198/1000 | Loss: 0.00001995
Iteration 199/1000 | Loss: 0.00001995
Iteration 200/1000 | Loss: 0.00001995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.994689228013158e-05, 1.994689228013158e-05, 1.994689228013158e-05, 1.994689228013158e-05, 1.994689228013158e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.994689228013158e-05

Optimization complete. Final v2v error: 3.635942220687866 mm

Highest mean error: 4.284313678741455 mm for frame 200

Lowest mean error: 2.9390957355499268 mm for frame 132

Saving results

Total time: 53.64207100868225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897428
Iteration 2/25 | Loss: 0.00190793
Iteration 3/25 | Loss: 0.00160665
Iteration 4/25 | Loss: 0.00129575
Iteration 5/25 | Loss: 0.00127112
Iteration 6/25 | Loss: 0.00126751
Iteration 7/25 | Loss: 0.00126643
Iteration 8/25 | Loss: 0.00126633
Iteration 9/25 | Loss: 0.00126633
Iteration 10/25 | Loss: 0.00126633
Iteration 11/25 | Loss: 0.00126633
Iteration 12/25 | Loss: 0.00126633
Iteration 13/25 | Loss: 0.00126633
Iteration 14/25 | Loss: 0.00126633
Iteration 15/25 | Loss: 0.00126633
Iteration 16/25 | Loss: 0.00126633
Iteration 17/25 | Loss: 0.00126633
Iteration 18/25 | Loss: 0.00126633
Iteration 19/25 | Loss: 0.00126633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012663339730352163, 0.0012663339730352163, 0.0012663339730352163, 0.0012663339730352163, 0.0012663339730352163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012663339730352163

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26121485
Iteration 2/25 | Loss: 0.00121702
Iteration 3/25 | Loss: 0.00121702
Iteration 4/25 | Loss: 0.00121702
Iteration 5/25 | Loss: 0.00121701
Iteration 6/25 | Loss: 0.00121701
Iteration 7/25 | Loss: 0.00121701
Iteration 8/25 | Loss: 0.00121701
Iteration 9/25 | Loss: 0.00121701
Iteration 10/25 | Loss: 0.00121701
Iteration 11/25 | Loss: 0.00121701
Iteration 12/25 | Loss: 0.00121701
Iteration 13/25 | Loss: 0.00121701
Iteration 14/25 | Loss: 0.00121701
Iteration 15/25 | Loss: 0.00121701
Iteration 16/25 | Loss: 0.00121701
Iteration 17/25 | Loss: 0.00121701
Iteration 18/25 | Loss: 0.00121701
Iteration 19/25 | Loss: 0.00121701
Iteration 20/25 | Loss: 0.00121701
Iteration 21/25 | Loss: 0.00121701
Iteration 22/25 | Loss: 0.00121701
Iteration 23/25 | Loss: 0.00121701
Iteration 24/25 | Loss: 0.00121701
Iteration 25/25 | Loss: 0.00121701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121701
Iteration 2/1000 | Loss: 0.00003982
Iteration 3/1000 | Loss: 0.00002817
Iteration 4/1000 | Loss: 0.00002413
Iteration 5/1000 | Loss: 0.00002273
Iteration 6/1000 | Loss: 0.00002163
Iteration 7/1000 | Loss: 0.00002090
Iteration 8/1000 | Loss: 0.00002013
Iteration 9/1000 | Loss: 0.00001965
Iteration 10/1000 | Loss: 0.00001936
Iteration 11/1000 | Loss: 0.00001913
Iteration 12/1000 | Loss: 0.00001891
Iteration 13/1000 | Loss: 0.00001878
Iteration 14/1000 | Loss: 0.00001866
Iteration 15/1000 | Loss: 0.00001864
Iteration 16/1000 | Loss: 0.00001862
Iteration 17/1000 | Loss: 0.00001861
Iteration 18/1000 | Loss: 0.00001855
Iteration 19/1000 | Loss: 0.00001852
Iteration 20/1000 | Loss: 0.00001852
Iteration 21/1000 | Loss: 0.00001852
Iteration 22/1000 | Loss: 0.00001851
Iteration 23/1000 | Loss: 0.00001851
Iteration 24/1000 | Loss: 0.00001850
Iteration 25/1000 | Loss: 0.00001850
Iteration 26/1000 | Loss: 0.00001849
Iteration 27/1000 | Loss: 0.00001849
Iteration 28/1000 | Loss: 0.00001848
Iteration 29/1000 | Loss: 0.00001848
Iteration 30/1000 | Loss: 0.00001847
Iteration 31/1000 | Loss: 0.00001847
Iteration 32/1000 | Loss: 0.00001846
Iteration 33/1000 | Loss: 0.00001846
Iteration 34/1000 | Loss: 0.00001846
Iteration 35/1000 | Loss: 0.00001845
Iteration 36/1000 | Loss: 0.00001844
Iteration 37/1000 | Loss: 0.00001844
Iteration 38/1000 | Loss: 0.00001844
Iteration 39/1000 | Loss: 0.00001843
Iteration 40/1000 | Loss: 0.00001843
Iteration 41/1000 | Loss: 0.00001843
Iteration 42/1000 | Loss: 0.00001843
Iteration 43/1000 | Loss: 0.00001842
Iteration 44/1000 | Loss: 0.00001842
Iteration 45/1000 | Loss: 0.00001841
Iteration 46/1000 | Loss: 0.00001841
Iteration 47/1000 | Loss: 0.00001840
Iteration 48/1000 | Loss: 0.00001840
Iteration 49/1000 | Loss: 0.00001839
Iteration 50/1000 | Loss: 0.00001839
Iteration 51/1000 | Loss: 0.00001839
Iteration 52/1000 | Loss: 0.00001839
Iteration 53/1000 | Loss: 0.00001839
Iteration 54/1000 | Loss: 0.00001838
Iteration 55/1000 | Loss: 0.00001838
Iteration 56/1000 | Loss: 0.00001838
Iteration 57/1000 | Loss: 0.00001838
Iteration 58/1000 | Loss: 0.00001835
Iteration 59/1000 | Loss: 0.00001835
Iteration 60/1000 | Loss: 0.00001835
Iteration 61/1000 | Loss: 0.00001834
Iteration 62/1000 | Loss: 0.00001834
Iteration 63/1000 | Loss: 0.00001834
Iteration 64/1000 | Loss: 0.00001833
Iteration 65/1000 | Loss: 0.00001832
Iteration 66/1000 | Loss: 0.00001832
Iteration 67/1000 | Loss: 0.00001831
Iteration 68/1000 | Loss: 0.00001831
Iteration 69/1000 | Loss: 0.00001830
Iteration 70/1000 | Loss: 0.00001830
Iteration 71/1000 | Loss: 0.00001830
Iteration 72/1000 | Loss: 0.00001830
Iteration 73/1000 | Loss: 0.00001830
Iteration 74/1000 | Loss: 0.00001830
Iteration 75/1000 | Loss: 0.00001829
Iteration 76/1000 | Loss: 0.00001829
Iteration 77/1000 | Loss: 0.00001829
Iteration 78/1000 | Loss: 0.00001829
Iteration 79/1000 | Loss: 0.00001829
Iteration 80/1000 | Loss: 0.00001829
Iteration 81/1000 | Loss: 0.00001828
Iteration 82/1000 | Loss: 0.00001828
Iteration 83/1000 | Loss: 0.00001828
Iteration 84/1000 | Loss: 0.00001828
Iteration 85/1000 | Loss: 0.00001828
Iteration 86/1000 | Loss: 0.00001828
Iteration 87/1000 | Loss: 0.00001827
Iteration 88/1000 | Loss: 0.00001827
Iteration 89/1000 | Loss: 0.00001827
Iteration 90/1000 | Loss: 0.00001827
Iteration 91/1000 | Loss: 0.00001827
Iteration 92/1000 | Loss: 0.00001827
Iteration 93/1000 | Loss: 0.00001827
Iteration 94/1000 | Loss: 0.00001826
Iteration 95/1000 | Loss: 0.00001826
Iteration 96/1000 | Loss: 0.00001826
Iteration 97/1000 | Loss: 0.00001826
Iteration 98/1000 | Loss: 0.00001826
Iteration 99/1000 | Loss: 0.00001825
Iteration 100/1000 | Loss: 0.00001825
Iteration 101/1000 | Loss: 0.00001825
Iteration 102/1000 | Loss: 0.00001825
Iteration 103/1000 | Loss: 0.00001825
Iteration 104/1000 | Loss: 0.00001824
Iteration 105/1000 | Loss: 0.00001824
Iteration 106/1000 | Loss: 0.00001824
Iteration 107/1000 | Loss: 0.00001824
Iteration 108/1000 | Loss: 0.00001824
Iteration 109/1000 | Loss: 0.00001824
Iteration 110/1000 | Loss: 0.00001824
Iteration 111/1000 | Loss: 0.00001824
Iteration 112/1000 | Loss: 0.00001824
Iteration 113/1000 | Loss: 0.00001824
Iteration 114/1000 | Loss: 0.00001824
Iteration 115/1000 | Loss: 0.00001823
Iteration 116/1000 | Loss: 0.00001823
Iteration 117/1000 | Loss: 0.00001823
Iteration 118/1000 | Loss: 0.00001823
Iteration 119/1000 | Loss: 0.00001823
Iteration 120/1000 | Loss: 0.00001823
Iteration 121/1000 | Loss: 0.00001823
Iteration 122/1000 | Loss: 0.00001823
Iteration 123/1000 | Loss: 0.00001823
Iteration 124/1000 | Loss: 0.00001822
Iteration 125/1000 | Loss: 0.00001822
Iteration 126/1000 | Loss: 0.00001822
Iteration 127/1000 | Loss: 0.00001822
Iteration 128/1000 | Loss: 0.00001822
Iteration 129/1000 | Loss: 0.00001821
Iteration 130/1000 | Loss: 0.00001821
Iteration 131/1000 | Loss: 0.00001821
Iteration 132/1000 | Loss: 0.00001821
Iteration 133/1000 | Loss: 0.00001821
Iteration 134/1000 | Loss: 0.00001821
Iteration 135/1000 | Loss: 0.00001820
Iteration 136/1000 | Loss: 0.00001820
Iteration 137/1000 | Loss: 0.00001820
Iteration 138/1000 | Loss: 0.00001819
Iteration 139/1000 | Loss: 0.00001819
Iteration 140/1000 | Loss: 0.00001819
Iteration 141/1000 | Loss: 0.00001819
Iteration 142/1000 | Loss: 0.00001819
Iteration 143/1000 | Loss: 0.00001818
Iteration 144/1000 | Loss: 0.00001818
Iteration 145/1000 | Loss: 0.00001818
Iteration 146/1000 | Loss: 0.00001818
Iteration 147/1000 | Loss: 0.00001818
Iteration 148/1000 | Loss: 0.00001818
Iteration 149/1000 | Loss: 0.00001818
Iteration 150/1000 | Loss: 0.00001818
Iteration 151/1000 | Loss: 0.00001817
Iteration 152/1000 | Loss: 0.00001817
Iteration 153/1000 | Loss: 0.00001817
Iteration 154/1000 | Loss: 0.00001816
Iteration 155/1000 | Loss: 0.00001816
Iteration 156/1000 | Loss: 0.00001816
Iteration 157/1000 | Loss: 0.00001816
Iteration 158/1000 | Loss: 0.00001816
Iteration 159/1000 | Loss: 0.00001816
Iteration 160/1000 | Loss: 0.00001816
Iteration 161/1000 | Loss: 0.00001815
Iteration 162/1000 | Loss: 0.00001815
Iteration 163/1000 | Loss: 0.00001815
Iteration 164/1000 | Loss: 0.00001815
Iteration 165/1000 | Loss: 0.00001815
Iteration 166/1000 | Loss: 0.00001815
Iteration 167/1000 | Loss: 0.00001815
Iteration 168/1000 | Loss: 0.00001815
Iteration 169/1000 | Loss: 0.00001814
Iteration 170/1000 | Loss: 0.00001814
Iteration 171/1000 | Loss: 0.00001814
Iteration 172/1000 | Loss: 0.00001813
Iteration 173/1000 | Loss: 0.00001813
Iteration 174/1000 | Loss: 0.00001813
Iteration 175/1000 | Loss: 0.00001813
Iteration 176/1000 | Loss: 0.00001813
Iteration 177/1000 | Loss: 0.00001813
Iteration 178/1000 | Loss: 0.00001813
Iteration 179/1000 | Loss: 0.00001813
Iteration 180/1000 | Loss: 0.00001812
Iteration 181/1000 | Loss: 0.00001812
Iteration 182/1000 | Loss: 0.00001812
Iteration 183/1000 | Loss: 0.00001812
Iteration 184/1000 | Loss: 0.00001812
Iteration 185/1000 | Loss: 0.00001812
Iteration 186/1000 | Loss: 0.00001811
Iteration 187/1000 | Loss: 0.00001811
Iteration 188/1000 | Loss: 0.00001811
Iteration 189/1000 | Loss: 0.00001811
Iteration 190/1000 | Loss: 0.00001811
Iteration 191/1000 | Loss: 0.00001811
Iteration 192/1000 | Loss: 0.00001810
Iteration 193/1000 | Loss: 0.00001810
Iteration 194/1000 | Loss: 0.00001810
Iteration 195/1000 | Loss: 0.00001810
Iteration 196/1000 | Loss: 0.00001810
Iteration 197/1000 | Loss: 0.00001810
Iteration 198/1000 | Loss: 0.00001810
Iteration 199/1000 | Loss: 0.00001810
Iteration 200/1000 | Loss: 0.00001810
Iteration 201/1000 | Loss: 0.00001810
Iteration 202/1000 | Loss: 0.00001810
Iteration 203/1000 | Loss: 0.00001810
Iteration 204/1000 | Loss: 0.00001810
Iteration 205/1000 | Loss: 0.00001810
Iteration 206/1000 | Loss: 0.00001810
Iteration 207/1000 | Loss: 0.00001810
Iteration 208/1000 | Loss: 0.00001810
Iteration 209/1000 | Loss: 0.00001809
Iteration 210/1000 | Loss: 0.00001809
Iteration 211/1000 | Loss: 0.00001809
Iteration 212/1000 | Loss: 0.00001809
Iteration 213/1000 | Loss: 0.00001809
Iteration 214/1000 | Loss: 0.00001809
Iteration 215/1000 | Loss: 0.00001809
Iteration 216/1000 | Loss: 0.00001809
Iteration 217/1000 | Loss: 0.00001809
Iteration 218/1000 | Loss: 0.00001809
Iteration 219/1000 | Loss: 0.00001809
Iteration 220/1000 | Loss: 0.00001809
Iteration 221/1000 | Loss: 0.00001809
Iteration 222/1000 | Loss: 0.00001809
Iteration 223/1000 | Loss: 0.00001809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.809014065656811e-05, 1.809014065656811e-05, 1.809014065656811e-05, 1.809014065656811e-05, 1.809014065656811e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.809014065656811e-05

Optimization complete. Final v2v error: 3.603727340698242 mm

Highest mean error: 4.710057735443115 mm for frame 68

Lowest mean error: 3.0099709033966064 mm for frame 47

Saving results

Total time: 46.810389041900635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063705
Iteration 2/25 | Loss: 0.00629595
Iteration 3/25 | Loss: 0.00376306
Iteration 4/25 | Loss: 0.00334235
Iteration 5/25 | Loss: 0.00277884
Iteration 6/25 | Loss: 0.00220481
Iteration 7/25 | Loss: 0.00175868
Iteration 8/25 | Loss: 0.00156327
Iteration 9/25 | Loss: 0.00147613
Iteration 10/25 | Loss: 0.00143223
Iteration 11/25 | Loss: 0.00145487
Iteration 12/25 | Loss: 0.00139401
Iteration 13/25 | Loss: 0.00138526
Iteration 14/25 | Loss: 0.00137954
Iteration 15/25 | Loss: 0.00137369
Iteration 16/25 | Loss: 0.00137281
Iteration 17/25 | Loss: 0.00137262
Iteration 18/25 | Loss: 0.00137262
Iteration 19/25 | Loss: 0.00137262
Iteration 20/25 | Loss: 0.00137262
Iteration 21/25 | Loss: 0.00137262
Iteration 22/25 | Loss: 0.00137262
Iteration 23/25 | Loss: 0.00137261
Iteration 24/25 | Loss: 0.00137261
Iteration 25/25 | Loss: 0.00137261

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53733021
Iteration 2/25 | Loss: 0.00127059
Iteration 3/25 | Loss: 0.00125542
Iteration 4/25 | Loss: 0.00125541
Iteration 5/25 | Loss: 0.00125541
Iteration 6/25 | Loss: 0.00125541
Iteration 7/25 | Loss: 0.00125541
Iteration 8/25 | Loss: 0.00125541
Iteration 9/25 | Loss: 0.00125541
Iteration 10/25 | Loss: 0.00125541
Iteration 11/25 | Loss: 0.00125541
Iteration 12/25 | Loss: 0.00125541
Iteration 13/25 | Loss: 0.00125541
Iteration 14/25 | Loss: 0.00125541
Iteration 15/25 | Loss: 0.00125541
Iteration 16/25 | Loss: 0.00125541
Iteration 17/25 | Loss: 0.00125541
Iteration 18/25 | Loss: 0.00125541
Iteration 19/25 | Loss: 0.00125541
Iteration 20/25 | Loss: 0.00125541
Iteration 21/25 | Loss: 0.00125541
Iteration 22/25 | Loss: 0.00125541
Iteration 23/25 | Loss: 0.00125541
Iteration 24/25 | Loss: 0.00125541
Iteration 25/25 | Loss: 0.00125541

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125541
Iteration 2/1000 | Loss: 0.00009244
Iteration 3/1000 | Loss: 0.00003587
Iteration 4/1000 | Loss: 0.00003227
Iteration 5/1000 | Loss: 0.00003075
Iteration 6/1000 | Loss: 0.00002984
Iteration 7/1000 | Loss: 0.00002920
Iteration 8/1000 | Loss: 0.00002863
Iteration 9/1000 | Loss: 0.00002823
Iteration 10/1000 | Loss: 0.00002793
Iteration 11/1000 | Loss: 0.00130155
Iteration 12/1000 | Loss: 0.00065185
Iteration 13/1000 | Loss: 0.00005769
Iteration 14/1000 | Loss: 0.00008672
Iteration 15/1000 | Loss: 0.00002984
Iteration 16/1000 | Loss: 0.00006803
Iteration 17/1000 | Loss: 0.00002455
Iteration 18/1000 | Loss: 0.00002300
Iteration 19/1000 | Loss: 0.00002163
Iteration 20/1000 | Loss: 0.00002754
Iteration 21/1000 | Loss: 0.00002138
Iteration 22/1000 | Loss: 0.00001990
Iteration 23/1000 | Loss: 0.00001934
Iteration 24/1000 | Loss: 0.00002852
Iteration 25/1000 | Loss: 0.00002952
Iteration 26/1000 | Loss: 0.00003163
Iteration 27/1000 | Loss: 0.00002275
Iteration 28/1000 | Loss: 0.00003726
Iteration 29/1000 | Loss: 0.00001791
Iteration 30/1000 | Loss: 0.00001783
Iteration 31/1000 | Loss: 0.00001783
Iteration 32/1000 | Loss: 0.00001767
Iteration 33/1000 | Loss: 0.00001758
Iteration 34/1000 | Loss: 0.00001757
Iteration 35/1000 | Loss: 0.00001757
Iteration 36/1000 | Loss: 0.00001754
Iteration 37/1000 | Loss: 0.00001753
Iteration 38/1000 | Loss: 0.00001751
Iteration 39/1000 | Loss: 0.00001750
Iteration 40/1000 | Loss: 0.00001749
Iteration 41/1000 | Loss: 0.00001749
Iteration 42/1000 | Loss: 0.00001749
Iteration 43/1000 | Loss: 0.00001749
Iteration 44/1000 | Loss: 0.00001749
Iteration 45/1000 | Loss: 0.00001749
Iteration 46/1000 | Loss: 0.00001749
Iteration 47/1000 | Loss: 0.00001749
Iteration 48/1000 | Loss: 0.00001749
Iteration 49/1000 | Loss: 0.00001749
Iteration 50/1000 | Loss: 0.00001749
Iteration 51/1000 | Loss: 0.00001746
Iteration 52/1000 | Loss: 0.00001746
Iteration 53/1000 | Loss: 0.00001746
Iteration 54/1000 | Loss: 0.00001746
Iteration 55/1000 | Loss: 0.00001746
Iteration 56/1000 | Loss: 0.00001746
Iteration 57/1000 | Loss: 0.00001746
Iteration 58/1000 | Loss: 0.00001746
Iteration 59/1000 | Loss: 0.00001746
Iteration 60/1000 | Loss: 0.00001744
Iteration 61/1000 | Loss: 0.00001743
Iteration 62/1000 | Loss: 0.00001742
Iteration 63/1000 | Loss: 0.00001742
Iteration 64/1000 | Loss: 0.00001742
Iteration 65/1000 | Loss: 0.00001742
Iteration 66/1000 | Loss: 0.00001742
Iteration 67/1000 | Loss: 0.00001742
Iteration 68/1000 | Loss: 0.00001741
Iteration 69/1000 | Loss: 0.00001741
Iteration 70/1000 | Loss: 0.00001741
Iteration 71/1000 | Loss: 0.00001741
Iteration 72/1000 | Loss: 0.00001741
Iteration 73/1000 | Loss: 0.00001741
Iteration 74/1000 | Loss: 0.00001741
Iteration 75/1000 | Loss: 0.00001741
Iteration 76/1000 | Loss: 0.00001741
Iteration 77/1000 | Loss: 0.00001741
Iteration 78/1000 | Loss: 0.00001740
Iteration 79/1000 | Loss: 0.00001740
Iteration 80/1000 | Loss: 0.00001740
Iteration 81/1000 | Loss: 0.00001739
Iteration 82/1000 | Loss: 0.00001739
Iteration 83/1000 | Loss: 0.00001739
Iteration 84/1000 | Loss: 0.00001739
Iteration 85/1000 | Loss: 0.00001739
Iteration 86/1000 | Loss: 0.00001739
Iteration 87/1000 | Loss: 0.00001739
Iteration 88/1000 | Loss: 0.00001739
Iteration 89/1000 | Loss: 0.00001739
Iteration 90/1000 | Loss: 0.00001739
Iteration 91/1000 | Loss: 0.00001738
Iteration 92/1000 | Loss: 0.00001738
Iteration 93/1000 | Loss: 0.00001738
Iteration 94/1000 | Loss: 0.00001738
Iteration 95/1000 | Loss: 0.00001738
Iteration 96/1000 | Loss: 0.00001738
Iteration 97/1000 | Loss: 0.00001738
Iteration 98/1000 | Loss: 0.00001737
Iteration 99/1000 | Loss: 0.00001737
Iteration 100/1000 | Loss: 0.00001737
Iteration 101/1000 | Loss: 0.00001737
Iteration 102/1000 | Loss: 0.00001737
Iteration 103/1000 | Loss: 0.00001737
Iteration 104/1000 | Loss: 0.00001737
Iteration 105/1000 | Loss: 0.00001737
Iteration 106/1000 | Loss: 0.00001737
Iteration 107/1000 | Loss: 0.00001736
Iteration 108/1000 | Loss: 0.00001736
Iteration 109/1000 | Loss: 0.00001736
Iteration 110/1000 | Loss: 0.00001736
Iteration 111/1000 | Loss: 0.00001736
Iteration 112/1000 | Loss: 0.00001736
Iteration 113/1000 | Loss: 0.00001736
Iteration 114/1000 | Loss: 0.00001736
Iteration 115/1000 | Loss: 0.00001736
Iteration 116/1000 | Loss: 0.00001736
Iteration 117/1000 | Loss: 0.00001736
Iteration 118/1000 | Loss: 0.00001735
Iteration 119/1000 | Loss: 0.00001735
Iteration 120/1000 | Loss: 0.00001735
Iteration 121/1000 | Loss: 0.00001735
Iteration 122/1000 | Loss: 0.00001735
Iteration 123/1000 | Loss: 0.00001735
Iteration 124/1000 | Loss: 0.00001735
Iteration 125/1000 | Loss: 0.00001734
Iteration 126/1000 | Loss: 0.00001734
Iteration 127/1000 | Loss: 0.00001734
Iteration 128/1000 | Loss: 0.00001734
Iteration 129/1000 | Loss: 0.00001734
Iteration 130/1000 | Loss: 0.00001734
Iteration 131/1000 | Loss: 0.00001734
Iteration 132/1000 | Loss: 0.00001734
Iteration 133/1000 | Loss: 0.00001734
Iteration 134/1000 | Loss: 0.00001734
Iteration 135/1000 | Loss: 0.00001734
Iteration 136/1000 | Loss: 0.00001734
Iteration 137/1000 | Loss: 0.00001734
Iteration 138/1000 | Loss: 0.00001734
Iteration 139/1000 | Loss: 0.00001734
Iteration 140/1000 | Loss: 0.00001734
Iteration 141/1000 | Loss: 0.00001734
Iteration 142/1000 | Loss: 0.00001734
Iteration 143/1000 | Loss: 0.00001734
Iteration 144/1000 | Loss: 0.00001733
Iteration 145/1000 | Loss: 0.00001733
Iteration 146/1000 | Loss: 0.00001733
Iteration 147/1000 | Loss: 0.00001733
Iteration 148/1000 | Loss: 0.00001733
Iteration 149/1000 | Loss: 0.00001733
Iteration 150/1000 | Loss: 0.00001733
Iteration 151/1000 | Loss: 0.00001733
Iteration 152/1000 | Loss: 0.00001733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.733467979647685e-05, 1.733467979647685e-05, 1.733467979647685e-05, 1.733467979647685e-05, 1.733467979647685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.733467979647685e-05

Optimization complete. Final v2v error: 3.6135072708129883 mm

Highest mean error: 3.930396318435669 mm for frame 80

Lowest mean error: 3.4983952045440674 mm for frame 196

Saving results

Total time: 93.46346116065979
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769994
Iteration 2/25 | Loss: 0.00199682
Iteration 3/25 | Loss: 0.00135339
Iteration 4/25 | Loss: 0.00125466
Iteration 5/25 | Loss: 0.00124133
Iteration 6/25 | Loss: 0.00122401
Iteration 7/25 | Loss: 0.00119737
Iteration 8/25 | Loss: 0.00119058
Iteration 9/25 | Loss: 0.00118692
Iteration 10/25 | Loss: 0.00119001
Iteration 11/25 | Loss: 0.00118905
Iteration 12/25 | Loss: 0.00118560
Iteration 13/25 | Loss: 0.00118558
Iteration 14/25 | Loss: 0.00118558
Iteration 15/25 | Loss: 0.00118557
Iteration 16/25 | Loss: 0.00118557
Iteration 17/25 | Loss: 0.00118557
Iteration 18/25 | Loss: 0.00118557
Iteration 19/25 | Loss: 0.00118557
Iteration 20/25 | Loss: 0.00118557
Iteration 21/25 | Loss: 0.00118557
Iteration 22/25 | Loss: 0.00118557
Iteration 23/25 | Loss: 0.00118557
Iteration 24/25 | Loss: 0.00118557
Iteration 25/25 | Loss: 0.00118556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34396088
Iteration 2/25 | Loss: 0.00164327
Iteration 3/25 | Loss: 0.00159546
Iteration 4/25 | Loss: 0.00159546
Iteration 5/25 | Loss: 0.00159546
Iteration 6/25 | Loss: 0.00159546
Iteration 7/25 | Loss: 0.00159546
Iteration 8/25 | Loss: 0.00159546
Iteration 9/25 | Loss: 0.00159546
Iteration 10/25 | Loss: 0.00159545
Iteration 11/25 | Loss: 0.00159545
Iteration 12/25 | Loss: 0.00159545
Iteration 13/25 | Loss: 0.00159545
Iteration 14/25 | Loss: 0.00159545
Iteration 15/25 | Loss: 0.00159545
Iteration 16/25 | Loss: 0.00159545
Iteration 17/25 | Loss: 0.00159545
Iteration 18/25 | Loss: 0.00159545
Iteration 19/25 | Loss: 0.00159545
Iteration 20/25 | Loss: 0.00159545
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015954540576785803, 0.0015954540576785803, 0.0015954540576785803, 0.0015954540576785803, 0.0015954540576785803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015954540576785803

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159545
Iteration 2/1000 | Loss: 0.00002364
Iteration 3/1000 | Loss: 0.00006479
Iteration 4/1000 | Loss: 0.00001378
Iteration 5/1000 | Loss: 0.00021239
Iteration 6/1000 | Loss: 0.00064768
Iteration 7/1000 | Loss: 0.00004761
Iteration 8/1000 | Loss: 0.00007221
Iteration 9/1000 | Loss: 0.00001221
Iteration 10/1000 | Loss: 0.00003596
Iteration 11/1000 | Loss: 0.00002603
Iteration 12/1000 | Loss: 0.00001136
Iteration 13/1000 | Loss: 0.00001097
Iteration 14/1000 | Loss: 0.00001096
Iteration 15/1000 | Loss: 0.00001071
Iteration 16/1000 | Loss: 0.00001064
Iteration 17/1000 | Loss: 0.00024405
Iteration 18/1000 | Loss: 0.00003822
Iteration 19/1000 | Loss: 0.00046382
Iteration 20/1000 | Loss: 0.00001580
Iteration 21/1000 | Loss: 0.00001065
Iteration 22/1000 | Loss: 0.00001012
Iteration 23/1000 | Loss: 0.00001011
Iteration 24/1000 | Loss: 0.00001010
Iteration 25/1000 | Loss: 0.00001010
Iteration 26/1000 | Loss: 0.00001009
Iteration 27/1000 | Loss: 0.00001009
Iteration 28/1000 | Loss: 0.00001008
Iteration 29/1000 | Loss: 0.00001006
Iteration 30/1000 | Loss: 0.00000997
Iteration 31/1000 | Loss: 0.00000997
Iteration 32/1000 | Loss: 0.00000995
Iteration 33/1000 | Loss: 0.00000993
Iteration 34/1000 | Loss: 0.00000993
Iteration 35/1000 | Loss: 0.00000988
Iteration 36/1000 | Loss: 0.00000986
Iteration 37/1000 | Loss: 0.00000986
Iteration 38/1000 | Loss: 0.00006549
Iteration 39/1000 | Loss: 0.00025116
Iteration 40/1000 | Loss: 0.00001232
Iteration 41/1000 | Loss: 0.00001023
Iteration 42/1000 | Loss: 0.00006444
Iteration 43/1000 | Loss: 0.00001670
Iteration 44/1000 | Loss: 0.00000988
Iteration 45/1000 | Loss: 0.00000982
Iteration 46/1000 | Loss: 0.00000981
Iteration 47/1000 | Loss: 0.00000981
Iteration 48/1000 | Loss: 0.00000981
Iteration 49/1000 | Loss: 0.00000970
Iteration 50/1000 | Loss: 0.00006349
Iteration 51/1000 | Loss: 0.00006349
Iteration 52/1000 | Loss: 0.00042138
Iteration 53/1000 | Loss: 0.00006483
Iteration 54/1000 | Loss: 0.00001502
Iteration 55/1000 | Loss: 0.00005785
Iteration 56/1000 | Loss: 0.00003869
Iteration 57/1000 | Loss: 0.00000994
Iteration 58/1000 | Loss: 0.00002962
Iteration 59/1000 | Loss: 0.00001010
Iteration 60/1000 | Loss: 0.00001153
Iteration 61/1000 | Loss: 0.00000975
Iteration 62/1000 | Loss: 0.00000974
Iteration 63/1000 | Loss: 0.00000974
Iteration 64/1000 | Loss: 0.00000973
Iteration 65/1000 | Loss: 0.00000973
Iteration 66/1000 | Loss: 0.00000973
Iteration 67/1000 | Loss: 0.00000972
Iteration 68/1000 | Loss: 0.00000968
Iteration 69/1000 | Loss: 0.00000968
Iteration 70/1000 | Loss: 0.00000967
Iteration 71/1000 | Loss: 0.00000967
Iteration 72/1000 | Loss: 0.00000967
Iteration 73/1000 | Loss: 0.00000965
Iteration 74/1000 | Loss: 0.00000961
Iteration 75/1000 | Loss: 0.00000961
Iteration 76/1000 | Loss: 0.00000959
Iteration 77/1000 | Loss: 0.00008135
Iteration 78/1000 | Loss: 0.00028857
Iteration 79/1000 | Loss: 0.00001409
Iteration 80/1000 | Loss: 0.00000977
Iteration 81/1000 | Loss: 0.00000954
Iteration 82/1000 | Loss: 0.00000951
Iteration 83/1000 | Loss: 0.00000950
Iteration 84/1000 | Loss: 0.00000949
Iteration 85/1000 | Loss: 0.00000949
Iteration 86/1000 | Loss: 0.00000948
Iteration 87/1000 | Loss: 0.00000948
Iteration 88/1000 | Loss: 0.00000948
Iteration 89/1000 | Loss: 0.00000948
Iteration 90/1000 | Loss: 0.00000947
Iteration 91/1000 | Loss: 0.00000947
Iteration 92/1000 | Loss: 0.00000947
Iteration 93/1000 | Loss: 0.00000947
Iteration 94/1000 | Loss: 0.00000947
Iteration 95/1000 | Loss: 0.00000947
Iteration 96/1000 | Loss: 0.00000947
Iteration 97/1000 | Loss: 0.00000947
Iteration 98/1000 | Loss: 0.00000947
Iteration 99/1000 | Loss: 0.00000947
Iteration 100/1000 | Loss: 0.00000947
Iteration 101/1000 | Loss: 0.00000947
Iteration 102/1000 | Loss: 0.00000946
Iteration 103/1000 | Loss: 0.00000946
Iteration 104/1000 | Loss: 0.00000946
Iteration 105/1000 | Loss: 0.00000946
Iteration 106/1000 | Loss: 0.00000946
Iteration 107/1000 | Loss: 0.00000946
Iteration 108/1000 | Loss: 0.00000946
Iteration 109/1000 | Loss: 0.00000946
Iteration 110/1000 | Loss: 0.00000946
Iteration 111/1000 | Loss: 0.00000946
Iteration 112/1000 | Loss: 0.00000946
Iteration 113/1000 | Loss: 0.00000946
Iteration 114/1000 | Loss: 0.00000946
Iteration 115/1000 | Loss: 0.00000946
Iteration 116/1000 | Loss: 0.00000945
Iteration 117/1000 | Loss: 0.00000945
Iteration 118/1000 | Loss: 0.00000945
Iteration 119/1000 | Loss: 0.00000945
Iteration 120/1000 | Loss: 0.00000945
Iteration 121/1000 | Loss: 0.00005444
Iteration 122/1000 | Loss: 0.00000952
Iteration 123/1000 | Loss: 0.00000948
Iteration 124/1000 | Loss: 0.00000948
Iteration 125/1000 | Loss: 0.00000948
Iteration 126/1000 | Loss: 0.00000947
Iteration 127/1000 | Loss: 0.00000946
Iteration 128/1000 | Loss: 0.00000946
Iteration 129/1000 | Loss: 0.00000946
Iteration 130/1000 | Loss: 0.00000946
Iteration 131/1000 | Loss: 0.00000945
Iteration 132/1000 | Loss: 0.00000945
Iteration 133/1000 | Loss: 0.00000945
Iteration 134/1000 | Loss: 0.00000944
Iteration 135/1000 | Loss: 0.00000944
Iteration 136/1000 | Loss: 0.00000944
Iteration 137/1000 | Loss: 0.00000944
Iteration 138/1000 | Loss: 0.00000944
Iteration 139/1000 | Loss: 0.00003787
Iteration 140/1000 | Loss: 0.00004154
Iteration 141/1000 | Loss: 0.00000947
Iteration 142/1000 | Loss: 0.00000944
Iteration 143/1000 | Loss: 0.00000943
Iteration 144/1000 | Loss: 0.00000943
Iteration 145/1000 | Loss: 0.00000943
Iteration 146/1000 | Loss: 0.00002728
Iteration 147/1000 | Loss: 0.00001053
Iteration 148/1000 | Loss: 0.00000983
Iteration 149/1000 | Loss: 0.00000946
Iteration 150/1000 | Loss: 0.00000942
Iteration 151/1000 | Loss: 0.00000942
Iteration 152/1000 | Loss: 0.00000942
Iteration 153/1000 | Loss: 0.00000942
Iteration 154/1000 | Loss: 0.00000942
Iteration 155/1000 | Loss: 0.00000941
Iteration 156/1000 | Loss: 0.00000941
Iteration 157/1000 | Loss: 0.00000941
Iteration 158/1000 | Loss: 0.00000941
Iteration 159/1000 | Loss: 0.00000941
Iteration 160/1000 | Loss: 0.00000941
Iteration 161/1000 | Loss: 0.00000941
Iteration 162/1000 | Loss: 0.00000941
Iteration 163/1000 | Loss: 0.00000940
Iteration 164/1000 | Loss: 0.00000940
Iteration 165/1000 | Loss: 0.00000940
Iteration 166/1000 | Loss: 0.00000940
Iteration 167/1000 | Loss: 0.00000940
Iteration 168/1000 | Loss: 0.00000940
Iteration 169/1000 | Loss: 0.00000940
Iteration 170/1000 | Loss: 0.00000940
Iteration 171/1000 | Loss: 0.00000940
Iteration 172/1000 | Loss: 0.00000940
Iteration 173/1000 | Loss: 0.00000940
Iteration 174/1000 | Loss: 0.00000940
Iteration 175/1000 | Loss: 0.00000940
Iteration 176/1000 | Loss: 0.00000940
Iteration 177/1000 | Loss: 0.00000940
Iteration 178/1000 | Loss: 0.00000940
Iteration 179/1000 | Loss: 0.00000940
Iteration 180/1000 | Loss: 0.00000940
Iteration 181/1000 | Loss: 0.00000940
Iteration 182/1000 | Loss: 0.00000940
Iteration 183/1000 | Loss: 0.00000940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [9.400573617313057e-06, 9.400573617313057e-06, 9.400573617313057e-06, 9.400573617313057e-06, 9.400573617313057e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.400573617313057e-06

Optimization complete. Final v2v error: 2.6795976161956787 mm

Highest mean error: 3.0052244663238525 mm for frame 57

Lowest mean error: 2.4886081218719482 mm for frame 157

Saving results

Total time: 109.53108811378479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500240
Iteration 2/25 | Loss: 0.00133629
Iteration 3/25 | Loss: 0.00124998
Iteration 4/25 | Loss: 0.00122945
Iteration 5/25 | Loss: 0.00122275
Iteration 6/25 | Loss: 0.00122112
Iteration 7/25 | Loss: 0.00122109
Iteration 8/25 | Loss: 0.00122109
Iteration 9/25 | Loss: 0.00122109
Iteration 10/25 | Loss: 0.00122109
Iteration 11/25 | Loss: 0.00122109
Iteration 12/25 | Loss: 0.00122109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001221091952174902, 0.001221091952174902, 0.001221091952174902, 0.001221091952174902, 0.001221091952174902]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001221091952174902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.61965656
Iteration 2/25 | Loss: 0.00154635
Iteration 3/25 | Loss: 0.00154635
Iteration 4/25 | Loss: 0.00154635
Iteration 5/25 | Loss: 0.00154635
Iteration 6/25 | Loss: 0.00154635
Iteration 7/25 | Loss: 0.00154635
Iteration 8/25 | Loss: 0.00154635
Iteration 9/25 | Loss: 0.00154635
Iteration 10/25 | Loss: 0.00154635
Iteration 11/25 | Loss: 0.00154635
Iteration 12/25 | Loss: 0.00154635
Iteration 13/25 | Loss: 0.00154635
Iteration 14/25 | Loss: 0.00154635
Iteration 15/25 | Loss: 0.00154635
Iteration 16/25 | Loss: 0.00154635
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015463503077626228, 0.0015463503077626228, 0.0015463503077626228, 0.0015463503077626228, 0.0015463503077626228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015463503077626228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154635
Iteration 2/1000 | Loss: 0.00003947
Iteration 3/1000 | Loss: 0.00002801
Iteration 4/1000 | Loss: 0.00002397
Iteration 5/1000 | Loss: 0.00002225
Iteration 6/1000 | Loss: 0.00002122
Iteration 7/1000 | Loss: 0.00002055
Iteration 8/1000 | Loss: 0.00002002
Iteration 9/1000 | Loss: 0.00001946
Iteration 10/1000 | Loss: 0.00001909
Iteration 11/1000 | Loss: 0.00001875
Iteration 12/1000 | Loss: 0.00001841
Iteration 13/1000 | Loss: 0.00001821
Iteration 14/1000 | Loss: 0.00001803
Iteration 15/1000 | Loss: 0.00001780
Iteration 16/1000 | Loss: 0.00001778
Iteration 17/1000 | Loss: 0.00001775
Iteration 18/1000 | Loss: 0.00001774
Iteration 19/1000 | Loss: 0.00001773
Iteration 20/1000 | Loss: 0.00001765
Iteration 21/1000 | Loss: 0.00001763
Iteration 22/1000 | Loss: 0.00001760
Iteration 23/1000 | Loss: 0.00001755
Iteration 24/1000 | Loss: 0.00001752
Iteration 25/1000 | Loss: 0.00001751
Iteration 26/1000 | Loss: 0.00001751
Iteration 27/1000 | Loss: 0.00001751
Iteration 28/1000 | Loss: 0.00001747
Iteration 29/1000 | Loss: 0.00001746
Iteration 30/1000 | Loss: 0.00001746
Iteration 31/1000 | Loss: 0.00001746
Iteration 32/1000 | Loss: 0.00001746
Iteration 33/1000 | Loss: 0.00001745
Iteration 34/1000 | Loss: 0.00001745
Iteration 35/1000 | Loss: 0.00001743
Iteration 36/1000 | Loss: 0.00001742
Iteration 37/1000 | Loss: 0.00001742
Iteration 38/1000 | Loss: 0.00001742
Iteration 39/1000 | Loss: 0.00001741
Iteration 40/1000 | Loss: 0.00001741
Iteration 41/1000 | Loss: 0.00001741
Iteration 42/1000 | Loss: 0.00001741
Iteration 43/1000 | Loss: 0.00001741
Iteration 44/1000 | Loss: 0.00001741
Iteration 45/1000 | Loss: 0.00001741
Iteration 46/1000 | Loss: 0.00001741
Iteration 47/1000 | Loss: 0.00001741
Iteration 48/1000 | Loss: 0.00001741
Iteration 49/1000 | Loss: 0.00001740
Iteration 50/1000 | Loss: 0.00001738
Iteration 51/1000 | Loss: 0.00001738
Iteration 52/1000 | Loss: 0.00001738
Iteration 53/1000 | Loss: 0.00001737
Iteration 54/1000 | Loss: 0.00001737
Iteration 55/1000 | Loss: 0.00001737
Iteration 56/1000 | Loss: 0.00001737
Iteration 57/1000 | Loss: 0.00001736
Iteration 58/1000 | Loss: 0.00001736
Iteration 59/1000 | Loss: 0.00001736
Iteration 60/1000 | Loss: 0.00001736
Iteration 61/1000 | Loss: 0.00001735
Iteration 62/1000 | Loss: 0.00001735
Iteration 63/1000 | Loss: 0.00001734
Iteration 64/1000 | Loss: 0.00001734
Iteration 65/1000 | Loss: 0.00001734
Iteration 66/1000 | Loss: 0.00001734
Iteration 67/1000 | Loss: 0.00001733
Iteration 68/1000 | Loss: 0.00001733
Iteration 69/1000 | Loss: 0.00001733
Iteration 70/1000 | Loss: 0.00001732
Iteration 71/1000 | Loss: 0.00001732
Iteration 72/1000 | Loss: 0.00001731
Iteration 73/1000 | Loss: 0.00001731
Iteration 74/1000 | Loss: 0.00001731
Iteration 75/1000 | Loss: 0.00001731
Iteration 76/1000 | Loss: 0.00001730
Iteration 77/1000 | Loss: 0.00001730
Iteration 78/1000 | Loss: 0.00001730
Iteration 79/1000 | Loss: 0.00001730
Iteration 80/1000 | Loss: 0.00001729
Iteration 81/1000 | Loss: 0.00001729
Iteration 82/1000 | Loss: 0.00001729
Iteration 83/1000 | Loss: 0.00001728
Iteration 84/1000 | Loss: 0.00001728
Iteration 85/1000 | Loss: 0.00001728
Iteration 86/1000 | Loss: 0.00001728
Iteration 87/1000 | Loss: 0.00001728
Iteration 88/1000 | Loss: 0.00001728
Iteration 89/1000 | Loss: 0.00001728
Iteration 90/1000 | Loss: 0.00001728
Iteration 91/1000 | Loss: 0.00001727
Iteration 92/1000 | Loss: 0.00001727
Iteration 93/1000 | Loss: 0.00001727
Iteration 94/1000 | Loss: 0.00001726
Iteration 95/1000 | Loss: 0.00001726
Iteration 96/1000 | Loss: 0.00001726
Iteration 97/1000 | Loss: 0.00001726
Iteration 98/1000 | Loss: 0.00001726
Iteration 99/1000 | Loss: 0.00001726
Iteration 100/1000 | Loss: 0.00001725
Iteration 101/1000 | Loss: 0.00001725
Iteration 102/1000 | Loss: 0.00001725
Iteration 103/1000 | Loss: 0.00001725
Iteration 104/1000 | Loss: 0.00001725
Iteration 105/1000 | Loss: 0.00001725
Iteration 106/1000 | Loss: 0.00001724
Iteration 107/1000 | Loss: 0.00001724
Iteration 108/1000 | Loss: 0.00001724
Iteration 109/1000 | Loss: 0.00001723
Iteration 110/1000 | Loss: 0.00001723
Iteration 111/1000 | Loss: 0.00001723
Iteration 112/1000 | Loss: 0.00001723
Iteration 113/1000 | Loss: 0.00001723
Iteration 114/1000 | Loss: 0.00001723
Iteration 115/1000 | Loss: 0.00001723
Iteration 116/1000 | Loss: 0.00001723
Iteration 117/1000 | Loss: 0.00001723
Iteration 118/1000 | Loss: 0.00001723
Iteration 119/1000 | Loss: 0.00001723
Iteration 120/1000 | Loss: 0.00001723
Iteration 121/1000 | Loss: 0.00001722
Iteration 122/1000 | Loss: 0.00001722
Iteration 123/1000 | Loss: 0.00001722
Iteration 124/1000 | Loss: 0.00001722
Iteration 125/1000 | Loss: 0.00001722
Iteration 126/1000 | Loss: 0.00001722
Iteration 127/1000 | Loss: 0.00001722
Iteration 128/1000 | Loss: 0.00001721
Iteration 129/1000 | Loss: 0.00001721
Iteration 130/1000 | Loss: 0.00001721
Iteration 131/1000 | Loss: 0.00001721
Iteration 132/1000 | Loss: 0.00001721
Iteration 133/1000 | Loss: 0.00001721
Iteration 134/1000 | Loss: 0.00001721
Iteration 135/1000 | Loss: 0.00001721
Iteration 136/1000 | Loss: 0.00001721
Iteration 137/1000 | Loss: 0.00001721
Iteration 138/1000 | Loss: 0.00001721
Iteration 139/1000 | Loss: 0.00001721
Iteration 140/1000 | Loss: 0.00001721
Iteration 141/1000 | Loss: 0.00001721
Iteration 142/1000 | Loss: 0.00001720
Iteration 143/1000 | Loss: 0.00001720
Iteration 144/1000 | Loss: 0.00001720
Iteration 145/1000 | Loss: 0.00001720
Iteration 146/1000 | Loss: 0.00001720
Iteration 147/1000 | Loss: 0.00001720
Iteration 148/1000 | Loss: 0.00001720
Iteration 149/1000 | Loss: 0.00001720
Iteration 150/1000 | Loss: 0.00001720
Iteration 151/1000 | Loss: 0.00001720
Iteration 152/1000 | Loss: 0.00001720
Iteration 153/1000 | Loss: 0.00001720
Iteration 154/1000 | Loss: 0.00001720
Iteration 155/1000 | Loss: 0.00001720
Iteration 156/1000 | Loss: 0.00001720
Iteration 157/1000 | Loss: 0.00001720
Iteration 158/1000 | Loss: 0.00001720
Iteration 159/1000 | Loss: 0.00001720
Iteration 160/1000 | Loss: 0.00001720
Iteration 161/1000 | Loss: 0.00001719
Iteration 162/1000 | Loss: 0.00001719
Iteration 163/1000 | Loss: 0.00001719
Iteration 164/1000 | Loss: 0.00001719
Iteration 165/1000 | Loss: 0.00001719
Iteration 166/1000 | Loss: 0.00001719
Iteration 167/1000 | Loss: 0.00001719
Iteration 168/1000 | Loss: 0.00001719
Iteration 169/1000 | Loss: 0.00001719
Iteration 170/1000 | Loss: 0.00001719
Iteration 171/1000 | Loss: 0.00001719
Iteration 172/1000 | Loss: 0.00001719
Iteration 173/1000 | Loss: 0.00001719
Iteration 174/1000 | Loss: 0.00001719
Iteration 175/1000 | Loss: 0.00001718
Iteration 176/1000 | Loss: 0.00001718
Iteration 177/1000 | Loss: 0.00001718
Iteration 178/1000 | Loss: 0.00001718
Iteration 179/1000 | Loss: 0.00001718
Iteration 180/1000 | Loss: 0.00001718
Iteration 181/1000 | Loss: 0.00001718
Iteration 182/1000 | Loss: 0.00001718
Iteration 183/1000 | Loss: 0.00001718
Iteration 184/1000 | Loss: 0.00001718
Iteration 185/1000 | Loss: 0.00001718
Iteration 186/1000 | Loss: 0.00001718
Iteration 187/1000 | Loss: 0.00001718
Iteration 188/1000 | Loss: 0.00001717
Iteration 189/1000 | Loss: 0.00001717
Iteration 190/1000 | Loss: 0.00001717
Iteration 191/1000 | Loss: 0.00001717
Iteration 192/1000 | Loss: 0.00001717
Iteration 193/1000 | Loss: 0.00001717
Iteration 194/1000 | Loss: 0.00001717
Iteration 195/1000 | Loss: 0.00001717
Iteration 196/1000 | Loss: 0.00001717
Iteration 197/1000 | Loss: 0.00001717
Iteration 198/1000 | Loss: 0.00001717
Iteration 199/1000 | Loss: 0.00001717
Iteration 200/1000 | Loss: 0.00001717
Iteration 201/1000 | Loss: 0.00001717
Iteration 202/1000 | Loss: 0.00001717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.7174515960505232e-05, 1.7174515960505232e-05, 1.7174515960505232e-05, 1.7174515960505232e-05, 1.7174515960505232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7174515960505232e-05

Optimization complete. Final v2v error: 3.547518730163574 mm

Highest mean error: 3.8224806785583496 mm for frame 51

Lowest mean error: 3.16900634765625 mm for frame 35

Saving results

Total time: 45.24960017204285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432921
Iteration 2/25 | Loss: 0.00140722
Iteration 3/25 | Loss: 0.00126349
Iteration 4/25 | Loss: 0.00125103
Iteration 5/25 | Loss: 0.00124956
Iteration 6/25 | Loss: 0.00124956
Iteration 7/25 | Loss: 0.00124956
Iteration 8/25 | Loss: 0.00124956
Iteration 9/25 | Loss: 0.00124956
Iteration 10/25 | Loss: 0.00124956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012495641130954027, 0.0012495641130954027, 0.0012495641130954027, 0.0012495641130954027, 0.0012495641130954027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012495641130954027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.72841334
Iteration 2/25 | Loss: 0.00103648
Iteration 3/25 | Loss: 0.00103646
Iteration 4/25 | Loss: 0.00103646
Iteration 5/25 | Loss: 0.00103646
Iteration 6/25 | Loss: 0.00103646
Iteration 7/25 | Loss: 0.00103646
Iteration 8/25 | Loss: 0.00103646
Iteration 9/25 | Loss: 0.00103646
Iteration 10/25 | Loss: 0.00103646
Iteration 11/25 | Loss: 0.00103646
Iteration 12/25 | Loss: 0.00103646
Iteration 13/25 | Loss: 0.00103646
Iteration 14/25 | Loss: 0.00103646
Iteration 15/25 | Loss: 0.00103646
Iteration 16/25 | Loss: 0.00103646
Iteration 17/25 | Loss: 0.00103646
Iteration 18/25 | Loss: 0.00103646
Iteration 19/25 | Loss: 0.00103646
Iteration 20/25 | Loss: 0.00103646
Iteration 21/25 | Loss: 0.00103646
Iteration 22/25 | Loss: 0.00103646
Iteration 23/25 | Loss: 0.00103646
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010364595800638199, 0.0010364595800638199, 0.0010364595800638199, 0.0010364595800638199, 0.0010364595800638199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010364595800638199

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103646
Iteration 2/1000 | Loss: 0.00002936
Iteration 3/1000 | Loss: 0.00001916
Iteration 4/1000 | Loss: 0.00001674
Iteration 5/1000 | Loss: 0.00001581
Iteration 6/1000 | Loss: 0.00001521
Iteration 7/1000 | Loss: 0.00001476
Iteration 8/1000 | Loss: 0.00001436
Iteration 9/1000 | Loss: 0.00001387
Iteration 10/1000 | Loss: 0.00001358
Iteration 11/1000 | Loss: 0.00001344
Iteration 12/1000 | Loss: 0.00001337
Iteration 13/1000 | Loss: 0.00001337
Iteration 14/1000 | Loss: 0.00001336
Iteration 15/1000 | Loss: 0.00001325
Iteration 16/1000 | Loss: 0.00001315
Iteration 17/1000 | Loss: 0.00001314
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001311
Iteration 20/1000 | Loss: 0.00001310
Iteration 21/1000 | Loss: 0.00001310
Iteration 22/1000 | Loss: 0.00001307
Iteration 23/1000 | Loss: 0.00001305
Iteration 24/1000 | Loss: 0.00001304
Iteration 25/1000 | Loss: 0.00001303
Iteration 26/1000 | Loss: 0.00001303
Iteration 27/1000 | Loss: 0.00001303
Iteration 28/1000 | Loss: 0.00001302
Iteration 29/1000 | Loss: 0.00001301
Iteration 30/1000 | Loss: 0.00001297
Iteration 31/1000 | Loss: 0.00001292
Iteration 32/1000 | Loss: 0.00001292
Iteration 33/1000 | Loss: 0.00001292
Iteration 34/1000 | Loss: 0.00001292
Iteration 35/1000 | Loss: 0.00001288
Iteration 36/1000 | Loss: 0.00001288
Iteration 37/1000 | Loss: 0.00001288
Iteration 38/1000 | Loss: 0.00001285
Iteration 39/1000 | Loss: 0.00001285
Iteration 40/1000 | Loss: 0.00001284
Iteration 41/1000 | Loss: 0.00001283
Iteration 42/1000 | Loss: 0.00001283
Iteration 43/1000 | Loss: 0.00001283
Iteration 44/1000 | Loss: 0.00001282
Iteration 45/1000 | Loss: 0.00001282
Iteration 46/1000 | Loss: 0.00001280
Iteration 47/1000 | Loss: 0.00001280
Iteration 48/1000 | Loss: 0.00001279
Iteration 49/1000 | Loss: 0.00001279
Iteration 50/1000 | Loss: 0.00001278
Iteration 51/1000 | Loss: 0.00001278
Iteration 52/1000 | Loss: 0.00001277
Iteration 53/1000 | Loss: 0.00001276
Iteration 54/1000 | Loss: 0.00001276
Iteration 55/1000 | Loss: 0.00001276
Iteration 56/1000 | Loss: 0.00001276
Iteration 57/1000 | Loss: 0.00001275
Iteration 58/1000 | Loss: 0.00001273
Iteration 59/1000 | Loss: 0.00001272
Iteration 60/1000 | Loss: 0.00001272
Iteration 61/1000 | Loss: 0.00001272
Iteration 62/1000 | Loss: 0.00001271
Iteration 63/1000 | Loss: 0.00001271
Iteration 64/1000 | Loss: 0.00001271
Iteration 65/1000 | Loss: 0.00001271
Iteration 66/1000 | Loss: 0.00001270
Iteration 67/1000 | Loss: 0.00001270
Iteration 68/1000 | Loss: 0.00001269
Iteration 69/1000 | Loss: 0.00001269
Iteration 70/1000 | Loss: 0.00001269
Iteration 71/1000 | Loss: 0.00001269
Iteration 72/1000 | Loss: 0.00001269
Iteration 73/1000 | Loss: 0.00001269
Iteration 74/1000 | Loss: 0.00001268
Iteration 75/1000 | Loss: 0.00001268
Iteration 76/1000 | Loss: 0.00001268
Iteration 77/1000 | Loss: 0.00001267
Iteration 78/1000 | Loss: 0.00001267
Iteration 79/1000 | Loss: 0.00001267
Iteration 80/1000 | Loss: 0.00001266
Iteration 81/1000 | Loss: 0.00001266
Iteration 82/1000 | Loss: 0.00001266
Iteration 83/1000 | Loss: 0.00001266
Iteration 84/1000 | Loss: 0.00001265
Iteration 85/1000 | Loss: 0.00001265
Iteration 86/1000 | Loss: 0.00001265
Iteration 87/1000 | Loss: 0.00001265
Iteration 88/1000 | Loss: 0.00001265
Iteration 89/1000 | Loss: 0.00001265
Iteration 90/1000 | Loss: 0.00001265
Iteration 91/1000 | Loss: 0.00001265
Iteration 92/1000 | Loss: 0.00001265
Iteration 93/1000 | Loss: 0.00001265
Iteration 94/1000 | Loss: 0.00001265
Iteration 95/1000 | Loss: 0.00001264
Iteration 96/1000 | Loss: 0.00001264
Iteration 97/1000 | Loss: 0.00001264
Iteration 98/1000 | Loss: 0.00001264
Iteration 99/1000 | Loss: 0.00001264
Iteration 100/1000 | Loss: 0.00001264
Iteration 101/1000 | Loss: 0.00001263
Iteration 102/1000 | Loss: 0.00001263
Iteration 103/1000 | Loss: 0.00001263
Iteration 104/1000 | Loss: 0.00001263
Iteration 105/1000 | Loss: 0.00001263
Iteration 106/1000 | Loss: 0.00001263
Iteration 107/1000 | Loss: 0.00001263
Iteration 108/1000 | Loss: 0.00001263
Iteration 109/1000 | Loss: 0.00001263
Iteration 110/1000 | Loss: 0.00001263
Iteration 111/1000 | Loss: 0.00001263
Iteration 112/1000 | Loss: 0.00001263
Iteration 113/1000 | Loss: 0.00001263
Iteration 114/1000 | Loss: 0.00001263
Iteration 115/1000 | Loss: 0.00001263
Iteration 116/1000 | Loss: 0.00001263
Iteration 117/1000 | Loss: 0.00001263
Iteration 118/1000 | Loss: 0.00001263
Iteration 119/1000 | Loss: 0.00001262
Iteration 120/1000 | Loss: 0.00001262
Iteration 121/1000 | Loss: 0.00001262
Iteration 122/1000 | Loss: 0.00001262
Iteration 123/1000 | Loss: 0.00001262
Iteration 124/1000 | Loss: 0.00001262
Iteration 125/1000 | Loss: 0.00001262
Iteration 126/1000 | Loss: 0.00001262
Iteration 127/1000 | Loss: 0.00001262
Iteration 128/1000 | Loss: 0.00001262
Iteration 129/1000 | Loss: 0.00001262
Iteration 130/1000 | Loss: 0.00001262
Iteration 131/1000 | Loss: 0.00001262
Iteration 132/1000 | Loss: 0.00001262
Iteration 133/1000 | Loss: 0.00001261
Iteration 134/1000 | Loss: 0.00001261
Iteration 135/1000 | Loss: 0.00001261
Iteration 136/1000 | Loss: 0.00001261
Iteration 137/1000 | Loss: 0.00001261
Iteration 138/1000 | Loss: 0.00001261
Iteration 139/1000 | Loss: 0.00001261
Iteration 140/1000 | Loss: 0.00001261
Iteration 141/1000 | Loss: 0.00001261
Iteration 142/1000 | Loss: 0.00001261
Iteration 143/1000 | Loss: 0.00001261
Iteration 144/1000 | Loss: 0.00001261
Iteration 145/1000 | Loss: 0.00001261
Iteration 146/1000 | Loss: 0.00001261
Iteration 147/1000 | Loss: 0.00001261
Iteration 148/1000 | Loss: 0.00001261
Iteration 149/1000 | Loss: 0.00001261
Iteration 150/1000 | Loss: 0.00001261
Iteration 151/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.261374563910067e-05, 1.261374563910067e-05, 1.261374563910067e-05, 1.261374563910067e-05, 1.261374563910067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.261374563910067e-05

Optimization complete. Final v2v error: 3.0239555835723877 mm

Highest mean error: 3.5080432891845703 mm for frame 88

Lowest mean error: 2.797942876815796 mm for frame 51

Saving results

Total time: 37.295576095581055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934360
Iteration 2/25 | Loss: 0.00934360
Iteration 3/25 | Loss: 0.00934360
Iteration 4/25 | Loss: 0.00934360
Iteration 5/25 | Loss: 0.00934359
Iteration 6/25 | Loss: 0.00934359
Iteration 7/25 | Loss: 0.00934359
Iteration 8/25 | Loss: 0.00265620
Iteration 9/25 | Loss: 0.00201539
Iteration 10/25 | Loss: 0.00180605
Iteration 11/25 | Loss: 0.00163577
Iteration 12/25 | Loss: 0.00149293
Iteration 13/25 | Loss: 0.00147134
Iteration 14/25 | Loss: 0.00142507
Iteration 15/25 | Loss: 0.00141634
Iteration 16/25 | Loss: 0.00141309
Iteration 17/25 | Loss: 0.00142193
Iteration 18/25 | Loss: 0.00140161
Iteration 19/25 | Loss: 0.00139915
Iteration 20/25 | Loss: 0.00139054
Iteration 21/25 | Loss: 0.00138893
Iteration 22/25 | Loss: 0.00138787
Iteration 23/25 | Loss: 0.00138738
Iteration 24/25 | Loss: 0.00138691
Iteration 25/25 | Loss: 0.00138635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27446413
Iteration 2/25 | Loss: 0.00485455
Iteration 3/25 | Loss: 0.00271063
Iteration 4/25 | Loss: 0.00271056
Iteration 5/25 | Loss: 0.00271056
Iteration 6/25 | Loss: 0.00271056
Iteration 7/25 | Loss: 0.00271055
Iteration 8/25 | Loss: 0.00271055
Iteration 9/25 | Loss: 0.00271055
Iteration 10/25 | Loss: 0.00271055
Iteration 11/25 | Loss: 0.00271055
Iteration 12/25 | Loss: 0.00271055
Iteration 13/25 | Loss: 0.00271055
Iteration 14/25 | Loss: 0.00271055
Iteration 15/25 | Loss: 0.00271055
Iteration 16/25 | Loss: 0.00271055
Iteration 17/25 | Loss: 0.00271055
Iteration 18/25 | Loss: 0.00271055
Iteration 19/25 | Loss: 0.00271055
Iteration 20/25 | Loss: 0.00271055
Iteration 21/25 | Loss: 0.00271055
Iteration 22/25 | Loss: 0.00271055
Iteration 23/25 | Loss: 0.00271055
Iteration 24/25 | Loss: 0.00271055
Iteration 25/25 | Loss: 0.00271055

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00271055
Iteration 2/1000 | Loss: 0.00148507
Iteration 3/1000 | Loss: 0.00211864
Iteration 4/1000 | Loss: 0.00140480
Iteration 5/1000 | Loss: 0.00067900
Iteration 6/1000 | Loss: 0.00035986
Iteration 7/1000 | Loss: 0.00016860
Iteration 8/1000 | Loss: 0.00017515
Iteration 9/1000 | Loss: 0.00061481
Iteration 10/1000 | Loss: 0.00022624
Iteration 11/1000 | Loss: 0.00021573
Iteration 12/1000 | Loss: 0.00018770
Iteration 13/1000 | Loss: 0.00006639
Iteration 14/1000 | Loss: 0.00006135
Iteration 15/1000 | Loss: 0.00005552
Iteration 16/1000 | Loss: 0.00049092
Iteration 17/1000 | Loss: 0.00005060
Iteration 18/1000 | Loss: 0.00004847
Iteration 19/1000 | Loss: 0.00029806
Iteration 20/1000 | Loss: 0.00004966
Iteration 21/1000 | Loss: 0.00004655
Iteration 22/1000 | Loss: 0.00110792
Iteration 23/1000 | Loss: 0.00006293
Iteration 24/1000 | Loss: 0.00005135
Iteration 25/1000 | Loss: 0.00124752
Iteration 26/1000 | Loss: 0.00005401
Iteration 27/1000 | Loss: 0.00004585
Iteration 28/1000 | Loss: 0.00072537
Iteration 29/1000 | Loss: 0.00200754
Iteration 30/1000 | Loss: 0.00068164
Iteration 31/1000 | Loss: 0.00010062
Iteration 32/1000 | Loss: 0.00005565
Iteration 33/1000 | Loss: 0.00038466
Iteration 34/1000 | Loss: 0.00009523
Iteration 35/1000 | Loss: 0.00023421
Iteration 36/1000 | Loss: 0.00010711
Iteration 37/1000 | Loss: 0.00007157
Iteration 38/1000 | Loss: 0.00032402
Iteration 39/1000 | Loss: 0.00006528
Iteration 40/1000 | Loss: 0.00006307
Iteration 41/1000 | Loss: 0.00005039
Iteration 42/1000 | Loss: 0.00030642
Iteration 43/1000 | Loss: 0.00005595
Iteration 44/1000 | Loss: 0.00022871
Iteration 45/1000 | Loss: 0.00012960
Iteration 46/1000 | Loss: 0.00010482
Iteration 47/1000 | Loss: 0.00003181
Iteration 48/1000 | Loss: 0.00012166
Iteration 49/1000 | Loss: 0.00009917
Iteration 50/1000 | Loss: 0.00010921
Iteration 51/1000 | Loss: 0.00003702
Iteration 52/1000 | Loss: 0.00002993
Iteration 53/1000 | Loss: 0.00005748
Iteration 54/1000 | Loss: 0.00002924
Iteration 55/1000 | Loss: 0.00043549
Iteration 56/1000 | Loss: 0.00007181
Iteration 57/1000 | Loss: 0.00007948
Iteration 58/1000 | Loss: 0.00007761
Iteration 59/1000 | Loss: 0.00003461
Iteration 60/1000 | Loss: 0.00007716
Iteration 61/1000 | Loss: 0.00003028
Iteration 62/1000 | Loss: 0.00025106
Iteration 63/1000 | Loss: 0.00002849
Iteration 64/1000 | Loss: 0.00013915
Iteration 65/1000 | Loss: 0.00004754
Iteration 66/1000 | Loss: 0.00002790
Iteration 67/1000 | Loss: 0.00002749
Iteration 68/1000 | Loss: 0.00027178
Iteration 69/1000 | Loss: 0.00005977
Iteration 70/1000 | Loss: 0.00002681
Iteration 71/1000 | Loss: 0.00002645
Iteration 72/1000 | Loss: 0.00017795
Iteration 73/1000 | Loss: 0.00017795
Iteration 74/1000 | Loss: 0.00126821
Iteration 75/1000 | Loss: 0.00003842
Iteration 76/1000 | Loss: 0.00019942
Iteration 77/1000 | Loss: 0.00011050
Iteration 78/1000 | Loss: 0.00006225
Iteration 79/1000 | Loss: 0.00004605
Iteration 80/1000 | Loss: 0.00006001
Iteration 81/1000 | Loss: 0.00002573
Iteration 82/1000 | Loss: 0.00002931
Iteration 83/1000 | Loss: 0.00002551
Iteration 84/1000 | Loss: 0.00002467
Iteration 85/1000 | Loss: 0.00002435
Iteration 86/1000 | Loss: 0.00002422
Iteration 87/1000 | Loss: 0.00002411
Iteration 88/1000 | Loss: 0.00002402
Iteration 89/1000 | Loss: 0.00002397
Iteration 90/1000 | Loss: 0.00002397
Iteration 91/1000 | Loss: 0.00002397
Iteration 92/1000 | Loss: 0.00002397
Iteration 93/1000 | Loss: 0.00002397
Iteration 94/1000 | Loss: 0.00002397
Iteration 95/1000 | Loss: 0.00002396
Iteration 96/1000 | Loss: 0.00002396
Iteration 97/1000 | Loss: 0.00002394
Iteration 98/1000 | Loss: 0.00002391
Iteration 99/1000 | Loss: 0.00002386
Iteration 100/1000 | Loss: 0.00002386
Iteration 101/1000 | Loss: 0.00002386
Iteration 102/1000 | Loss: 0.00002386
Iteration 103/1000 | Loss: 0.00002386
Iteration 104/1000 | Loss: 0.00002386
Iteration 105/1000 | Loss: 0.00002386
Iteration 106/1000 | Loss: 0.00002385
Iteration 107/1000 | Loss: 0.00002385
Iteration 108/1000 | Loss: 0.00002385
Iteration 109/1000 | Loss: 0.00002385
Iteration 110/1000 | Loss: 0.00002384
Iteration 111/1000 | Loss: 0.00002384
Iteration 112/1000 | Loss: 0.00002383
Iteration 113/1000 | Loss: 0.00002383
Iteration 114/1000 | Loss: 0.00002383
Iteration 115/1000 | Loss: 0.00002383
Iteration 116/1000 | Loss: 0.00002383
Iteration 117/1000 | Loss: 0.00002383
Iteration 118/1000 | Loss: 0.00002382
Iteration 119/1000 | Loss: 0.00002382
Iteration 120/1000 | Loss: 0.00002382
Iteration 121/1000 | Loss: 0.00002382
Iteration 122/1000 | Loss: 0.00002382
Iteration 123/1000 | Loss: 0.00002382
Iteration 124/1000 | Loss: 0.00002382
Iteration 125/1000 | Loss: 0.00002381
Iteration 126/1000 | Loss: 0.00002381
Iteration 127/1000 | Loss: 0.00002381
Iteration 128/1000 | Loss: 0.00002381
Iteration 129/1000 | Loss: 0.00015032
Iteration 130/1000 | Loss: 0.00022401
Iteration 131/1000 | Loss: 0.00002428
Iteration 132/1000 | Loss: 0.00002350
Iteration 133/1000 | Loss: 0.00002313
Iteration 134/1000 | Loss: 0.00002261
Iteration 135/1000 | Loss: 0.00037382
Iteration 136/1000 | Loss: 0.00004866
Iteration 137/1000 | Loss: 0.00008567
Iteration 138/1000 | Loss: 0.00021263
Iteration 139/1000 | Loss: 0.00008230
Iteration 140/1000 | Loss: 0.00030594
Iteration 141/1000 | Loss: 0.00003112
Iteration 142/1000 | Loss: 0.00019337
Iteration 143/1000 | Loss: 0.00002305
Iteration 144/1000 | Loss: 0.00005926
Iteration 145/1000 | Loss: 0.00002114
Iteration 146/1000 | Loss: 0.00002012
Iteration 147/1000 | Loss: 0.00001941
Iteration 148/1000 | Loss: 0.00001898
Iteration 149/1000 | Loss: 0.00001863
Iteration 150/1000 | Loss: 0.00001856
Iteration 151/1000 | Loss: 0.00001847
Iteration 152/1000 | Loss: 0.00001830
Iteration 153/1000 | Loss: 0.00001829
Iteration 154/1000 | Loss: 0.00001829
Iteration 155/1000 | Loss: 0.00001827
Iteration 156/1000 | Loss: 0.00001824
Iteration 157/1000 | Loss: 0.00001823
Iteration 158/1000 | Loss: 0.00001823
Iteration 159/1000 | Loss: 0.00001822
Iteration 160/1000 | Loss: 0.00001821
Iteration 161/1000 | Loss: 0.00001818
Iteration 162/1000 | Loss: 0.00001817
Iteration 163/1000 | Loss: 0.00001816
Iteration 164/1000 | Loss: 0.00001814
Iteration 165/1000 | Loss: 0.00001814
Iteration 166/1000 | Loss: 0.00001814
Iteration 167/1000 | Loss: 0.00001814
Iteration 168/1000 | Loss: 0.00001814
Iteration 169/1000 | Loss: 0.00001813
Iteration 170/1000 | Loss: 0.00001813
Iteration 171/1000 | Loss: 0.00001813
Iteration 172/1000 | Loss: 0.00001813
Iteration 173/1000 | Loss: 0.00001813
Iteration 174/1000 | Loss: 0.00001813
Iteration 175/1000 | Loss: 0.00001813
Iteration 176/1000 | Loss: 0.00001813
Iteration 177/1000 | Loss: 0.00001813
Iteration 178/1000 | Loss: 0.00001813
Iteration 179/1000 | Loss: 0.00001812
Iteration 180/1000 | Loss: 0.00001812
Iteration 181/1000 | Loss: 0.00001812
Iteration 182/1000 | Loss: 0.00001812
Iteration 183/1000 | Loss: 0.00001812
Iteration 184/1000 | Loss: 0.00001811
Iteration 185/1000 | Loss: 0.00001811
Iteration 186/1000 | Loss: 0.00001811
Iteration 187/1000 | Loss: 0.00001811
Iteration 188/1000 | Loss: 0.00001810
Iteration 189/1000 | Loss: 0.00001810
Iteration 190/1000 | Loss: 0.00001810
Iteration 191/1000 | Loss: 0.00001810
Iteration 192/1000 | Loss: 0.00001810
Iteration 193/1000 | Loss: 0.00001810
Iteration 194/1000 | Loss: 0.00001810
Iteration 195/1000 | Loss: 0.00001810
Iteration 196/1000 | Loss: 0.00001810
Iteration 197/1000 | Loss: 0.00001810
Iteration 198/1000 | Loss: 0.00001810
Iteration 199/1000 | Loss: 0.00001810
Iteration 200/1000 | Loss: 0.00001810
Iteration 201/1000 | Loss: 0.00001810
Iteration 202/1000 | Loss: 0.00001810
Iteration 203/1000 | Loss: 0.00001810
Iteration 204/1000 | Loss: 0.00001810
Iteration 205/1000 | Loss: 0.00001810
Iteration 206/1000 | Loss: 0.00001810
Iteration 207/1000 | Loss: 0.00001810
Iteration 208/1000 | Loss: 0.00001810
Iteration 209/1000 | Loss: 0.00001810
Iteration 210/1000 | Loss: 0.00001810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.809554305509664e-05, 1.809554305509664e-05, 1.809554305509664e-05, 1.809554305509664e-05, 1.809554305509664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.809554305509664e-05

Optimization complete. Final v2v error: 3.401402235031128 mm

Highest mean error: 11.446168899536133 mm for frame 9

Lowest mean error: 3.0006754398345947 mm for frame 235

Saving results

Total time: 226.90429592132568
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00693759
Iteration 2/25 | Loss: 0.00154734
Iteration 3/25 | Loss: 0.00129335
Iteration 4/25 | Loss: 0.00127418
Iteration 5/25 | Loss: 0.00127459
Iteration 6/25 | Loss: 0.00127261
Iteration 7/25 | Loss: 0.00126393
Iteration 8/25 | Loss: 0.00126253
Iteration 9/25 | Loss: 0.00126092
Iteration 10/25 | Loss: 0.00124761
Iteration 11/25 | Loss: 0.00124494
Iteration 12/25 | Loss: 0.00124172
Iteration 13/25 | Loss: 0.00124265
Iteration 14/25 | Loss: 0.00124235
Iteration 15/25 | Loss: 0.00124054
Iteration 16/25 | Loss: 0.00123885
Iteration 17/25 | Loss: 0.00123855
Iteration 18/25 | Loss: 0.00123654
Iteration 19/25 | Loss: 0.00123769
Iteration 20/25 | Loss: 0.00124057
Iteration 21/25 | Loss: 0.00124056
Iteration 22/25 | Loss: 0.00124028
Iteration 23/25 | Loss: 0.00123900
Iteration 24/25 | Loss: 0.00124050
Iteration 25/25 | Loss: 0.00123815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.95382333
Iteration 2/25 | Loss: 0.00146438
Iteration 3/25 | Loss: 0.00146437
Iteration 4/25 | Loss: 0.00146437
Iteration 5/25 | Loss: 0.00146436
Iteration 6/25 | Loss: 0.00146436
Iteration 7/25 | Loss: 0.00146436
Iteration 8/25 | Loss: 0.00146436
Iteration 9/25 | Loss: 0.00146436
Iteration 10/25 | Loss: 0.00146436
Iteration 11/25 | Loss: 0.00146436
Iteration 12/25 | Loss: 0.00146436
Iteration 13/25 | Loss: 0.00146436
Iteration 14/25 | Loss: 0.00146436
Iteration 15/25 | Loss: 0.00146436
Iteration 16/25 | Loss: 0.00146436
Iteration 17/25 | Loss: 0.00146436
Iteration 18/25 | Loss: 0.00146436
Iteration 19/25 | Loss: 0.00146436
Iteration 20/25 | Loss: 0.00146436
Iteration 21/25 | Loss: 0.00146436
Iteration 22/25 | Loss: 0.00146436
Iteration 23/25 | Loss: 0.00146436
Iteration 24/25 | Loss: 0.00146436
Iteration 25/25 | Loss: 0.00146436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146436
Iteration 2/1000 | Loss: 0.00002634
Iteration 3/1000 | Loss: 0.00003039
Iteration 4/1000 | Loss: 0.00001887
Iteration 5/1000 | Loss: 0.00003440
Iteration 6/1000 | Loss: 0.00001816
Iteration 7/1000 | Loss: 0.00002363
Iteration 8/1000 | Loss: 0.00002202
Iteration 9/1000 | Loss: 0.00003368
Iteration 10/1000 | Loss: 0.00003312
Iteration 11/1000 | Loss: 0.00003354
Iteration 12/1000 | Loss: 0.00003374
Iteration 13/1000 | Loss: 0.00003303
Iteration 14/1000 | Loss: 0.00003408
Iteration 15/1000 | Loss: 0.00003211
Iteration 16/1000 | Loss: 0.00002306
Iteration 17/1000 | Loss: 0.00001946
Iteration 18/1000 | Loss: 0.00003227
Iteration 19/1000 | Loss: 0.00002901
Iteration 20/1000 | Loss: 0.00003152
Iteration 21/1000 | Loss: 0.00002857
Iteration 22/1000 | Loss: 0.00016457
Iteration 23/1000 | Loss: 0.00002902
Iteration 24/1000 | Loss: 0.00003146
Iteration 25/1000 | Loss: 0.00002801
Iteration 26/1000 | Loss: 0.00002947
Iteration 27/1000 | Loss: 0.00002555
Iteration 28/1000 | Loss: 0.00002791
Iteration 29/1000 | Loss: 0.00002453
Iteration 30/1000 | Loss: 0.00002576
Iteration 31/1000 | Loss: 0.00002289
Iteration 32/1000 | Loss: 0.00002605
Iteration 33/1000 | Loss: 0.00002258
Iteration 34/1000 | Loss: 0.00002983
Iteration 35/1000 | Loss: 0.00002768
Iteration 36/1000 | Loss: 0.00002447
Iteration 37/1000 | Loss: 0.00003422
Iteration 38/1000 | Loss: 0.00001689
Iteration 39/1000 | Loss: 0.00001441
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001315
Iteration 42/1000 | Loss: 0.00001314
Iteration 43/1000 | Loss: 0.00001313
Iteration 44/1000 | Loss: 0.00001313
Iteration 45/1000 | Loss: 0.00001312
Iteration 46/1000 | Loss: 0.00001311
Iteration 47/1000 | Loss: 0.00001311
Iteration 48/1000 | Loss: 0.00001311
Iteration 49/1000 | Loss: 0.00001310
Iteration 50/1000 | Loss: 0.00001310
Iteration 51/1000 | Loss: 0.00001308
Iteration 52/1000 | Loss: 0.00001307
Iteration 53/1000 | Loss: 0.00001306
Iteration 54/1000 | Loss: 0.00001305
Iteration 55/1000 | Loss: 0.00001305
Iteration 56/1000 | Loss: 0.00001304
Iteration 57/1000 | Loss: 0.00001296
Iteration 58/1000 | Loss: 0.00001293
Iteration 59/1000 | Loss: 0.00001293
Iteration 60/1000 | Loss: 0.00001292
Iteration 61/1000 | Loss: 0.00001290
Iteration 62/1000 | Loss: 0.00001290
Iteration 63/1000 | Loss: 0.00001290
Iteration 64/1000 | Loss: 0.00001289
Iteration 65/1000 | Loss: 0.00001289
Iteration 66/1000 | Loss: 0.00001289
Iteration 67/1000 | Loss: 0.00001289
Iteration 68/1000 | Loss: 0.00001288
Iteration 69/1000 | Loss: 0.00001288
Iteration 70/1000 | Loss: 0.00001287
Iteration 71/1000 | Loss: 0.00001287
Iteration 72/1000 | Loss: 0.00001287
Iteration 73/1000 | Loss: 0.00001287
Iteration 74/1000 | Loss: 0.00001286
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001284
Iteration 81/1000 | Loss: 0.00001284
Iteration 82/1000 | Loss: 0.00001284
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001284
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001283
Iteration 87/1000 | Loss: 0.00001282
Iteration 88/1000 | Loss: 0.00001282
Iteration 89/1000 | Loss: 0.00001281
Iteration 90/1000 | Loss: 0.00001281
Iteration 91/1000 | Loss: 0.00001281
Iteration 92/1000 | Loss: 0.00001281
Iteration 93/1000 | Loss: 0.00001281
Iteration 94/1000 | Loss: 0.00001281
Iteration 95/1000 | Loss: 0.00001280
Iteration 96/1000 | Loss: 0.00001280
Iteration 97/1000 | Loss: 0.00001280
Iteration 98/1000 | Loss: 0.00001280
Iteration 99/1000 | Loss: 0.00001280
Iteration 100/1000 | Loss: 0.00001280
Iteration 101/1000 | Loss: 0.00001279
Iteration 102/1000 | Loss: 0.00001279
Iteration 103/1000 | Loss: 0.00001278
Iteration 104/1000 | Loss: 0.00001278
Iteration 105/1000 | Loss: 0.00001278
Iteration 106/1000 | Loss: 0.00001277
Iteration 107/1000 | Loss: 0.00001277
Iteration 108/1000 | Loss: 0.00001276
Iteration 109/1000 | Loss: 0.00001276
Iteration 110/1000 | Loss: 0.00001276
Iteration 111/1000 | Loss: 0.00001276
Iteration 112/1000 | Loss: 0.00001275
Iteration 113/1000 | Loss: 0.00001275
Iteration 114/1000 | Loss: 0.00001275
Iteration 115/1000 | Loss: 0.00001274
Iteration 116/1000 | Loss: 0.00001274
Iteration 117/1000 | Loss: 0.00001274
Iteration 118/1000 | Loss: 0.00001273
Iteration 119/1000 | Loss: 0.00001273
Iteration 120/1000 | Loss: 0.00001273
Iteration 121/1000 | Loss: 0.00001273
Iteration 122/1000 | Loss: 0.00001273
Iteration 123/1000 | Loss: 0.00001272
Iteration 124/1000 | Loss: 0.00001272
Iteration 125/1000 | Loss: 0.00001272
Iteration 126/1000 | Loss: 0.00001272
Iteration 127/1000 | Loss: 0.00001272
Iteration 128/1000 | Loss: 0.00001272
Iteration 129/1000 | Loss: 0.00001271
Iteration 130/1000 | Loss: 0.00001271
Iteration 131/1000 | Loss: 0.00001271
Iteration 132/1000 | Loss: 0.00001270
Iteration 133/1000 | Loss: 0.00001270
Iteration 134/1000 | Loss: 0.00001270
Iteration 135/1000 | Loss: 0.00001270
Iteration 136/1000 | Loss: 0.00001269
Iteration 137/1000 | Loss: 0.00001269
Iteration 138/1000 | Loss: 0.00001268
Iteration 139/1000 | Loss: 0.00001268
Iteration 140/1000 | Loss: 0.00001267
Iteration 141/1000 | Loss: 0.00001267
Iteration 142/1000 | Loss: 0.00001267
Iteration 143/1000 | Loss: 0.00001266
Iteration 144/1000 | Loss: 0.00001266
Iteration 145/1000 | Loss: 0.00001266
Iteration 146/1000 | Loss: 0.00001266
Iteration 147/1000 | Loss: 0.00001265
Iteration 148/1000 | Loss: 0.00001265
Iteration 149/1000 | Loss: 0.00001265
Iteration 150/1000 | Loss: 0.00001265
Iteration 151/1000 | Loss: 0.00001265
Iteration 152/1000 | Loss: 0.00001265
Iteration 153/1000 | Loss: 0.00001265
Iteration 154/1000 | Loss: 0.00001265
Iteration 155/1000 | Loss: 0.00001264
Iteration 156/1000 | Loss: 0.00001264
Iteration 157/1000 | Loss: 0.00001264
Iteration 158/1000 | Loss: 0.00001264
Iteration 159/1000 | Loss: 0.00001264
Iteration 160/1000 | Loss: 0.00001264
Iteration 161/1000 | Loss: 0.00001264
Iteration 162/1000 | Loss: 0.00001264
Iteration 163/1000 | Loss: 0.00001264
Iteration 164/1000 | Loss: 0.00001264
Iteration 165/1000 | Loss: 0.00001264
Iteration 166/1000 | Loss: 0.00001264
Iteration 167/1000 | Loss: 0.00001264
Iteration 168/1000 | Loss: 0.00001263
Iteration 169/1000 | Loss: 0.00001263
Iteration 170/1000 | Loss: 0.00001263
Iteration 171/1000 | Loss: 0.00001263
Iteration 172/1000 | Loss: 0.00001262
Iteration 173/1000 | Loss: 0.00001262
Iteration 174/1000 | Loss: 0.00001262
Iteration 175/1000 | Loss: 0.00001262
Iteration 176/1000 | Loss: 0.00001262
Iteration 177/1000 | Loss: 0.00001262
Iteration 178/1000 | Loss: 0.00001262
Iteration 179/1000 | Loss: 0.00001262
Iteration 180/1000 | Loss: 0.00001262
Iteration 181/1000 | Loss: 0.00001262
Iteration 182/1000 | Loss: 0.00001261
Iteration 183/1000 | Loss: 0.00001261
Iteration 184/1000 | Loss: 0.00001261
Iteration 185/1000 | Loss: 0.00001261
Iteration 186/1000 | Loss: 0.00001261
Iteration 187/1000 | Loss: 0.00001261
Iteration 188/1000 | Loss: 0.00001261
Iteration 189/1000 | Loss: 0.00001261
Iteration 190/1000 | Loss: 0.00001261
Iteration 191/1000 | Loss: 0.00001261
Iteration 192/1000 | Loss: 0.00001261
Iteration 193/1000 | Loss: 0.00001261
Iteration 194/1000 | Loss: 0.00001261
Iteration 195/1000 | Loss: 0.00001261
Iteration 196/1000 | Loss: 0.00001260
Iteration 197/1000 | Loss: 0.00001260
Iteration 198/1000 | Loss: 0.00001260
Iteration 199/1000 | Loss: 0.00001260
Iteration 200/1000 | Loss: 0.00001260
Iteration 201/1000 | Loss: 0.00001260
Iteration 202/1000 | Loss: 0.00001260
Iteration 203/1000 | Loss: 0.00001260
Iteration 204/1000 | Loss: 0.00001260
Iteration 205/1000 | Loss: 0.00001260
Iteration 206/1000 | Loss: 0.00001260
Iteration 207/1000 | Loss: 0.00001260
Iteration 208/1000 | Loss: 0.00001260
Iteration 209/1000 | Loss: 0.00001259
Iteration 210/1000 | Loss: 0.00001259
Iteration 211/1000 | Loss: 0.00001259
Iteration 212/1000 | Loss: 0.00001259
Iteration 213/1000 | Loss: 0.00001259
Iteration 214/1000 | Loss: 0.00001259
Iteration 215/1000 | Loss: 0.00001259
Iteration 216/1000 | Loss: 0.00001259
Iteration 217/1000 | Loss: 0.00001259
Iteration 218/1000 | Loss: 0.00001259
Iteration 219/1000 | Loss: 0.00001259
Iteration 220/1000 | Loss: 0.00001259
Iteration 221/1000 | Loss: 0.00001259
Iteration 222/1000 | Loss: 0.00001259
Iteration 223/1000 | Loss: 0.00001259
Iteration 224/1000 | Loss: 0.00001259
Iteration 225/1000 | Loss: 0.00001258
Iteration 226/1000 | Loss: 0.00001258
Iteration 227/1000 | Loss: 0.00001258
Iteration 228/1000 | Loss: 0.00001258
Iteration 229/1000 | Loss: 0.00001258
Iteration 230/1000 | Loss: 0.00001258
Iteration 231/1000 | Loss: 0.00001258
Iteration 232/1000 | Loss: 0.00001258
Iteration 233/1000 | Loss: 0.00001258
Iteration 234/1000 | Loss: 0.00001258
Iteration 235/1000 | Loss: 0.00001258
Iteration 236/1000 | Loss: 0.00001257
Iteration 237/1000 | Loss: 0.00001257
Iteration 238/1000 | Loss: 0.00001257
Iteration 239/1000 | Loss: 0.00001257
Iteration 240/1000 | Loss: 0.00001257
Iteration 241/1000 | Loss: 0.00001257
Iteration 242/1000 | Loss: 0.00001257
Iteration 243/1000 | Loss: 0.00001257
Iteration 244/1000 | Loss: 0.00001257
Iteration 245/1000 | Loss: 0.00001257
Iteration 246/1000 | Loss: 0.00001257
Iteration 247/1000 | Loss: 0.00001257
Iteration 248/1000 | Loss: 0.00001257
Iteration 249/1000 | Loss: 0.00001257
Iteration 250/1000 | Loss: 0.00001257
Iteration 251/1000 | Loss: 0.00001257
Iteration 252/1000 | Loss: 0.00001257
Iteration 253/1000 | Loss: 0.00001257
Iteration 254/1000 | Loss: 0.00001257
Iteration 255/1000 | Loss: 0.00001257
Iteration 256/1000 | Loss: 0.00001257
Iteration 257/1000 | Loss: 0.00001256
Iteration 258/1000 | Loss: 0.00001256
Iteration 259/1000 | Loss: 0.00001256
Iteration 260/1000 | Loss: 0.00001256
Iteration 261/1000 | Loss: 0.00001256
Iteration 262/1000 | Loss: 0.00001256
Iteration 263/1000 | Loss: 0.00001256
Iteration 264/1000 | Loss: 0.00001256
Iteration 265/1000 | Loss: 0.00001256
Iteration 266/1000 | Loss: 0.00001256
Iteration 267/1000 | Loss: 0.00001256
Iteration 268/1000 | Loss: 0.00001256
Iteration 269/1000 | Loss: 0.00001256
Iteration 270/1000 | Loss: 0.00001256
Iteration 271/1000 | Loss: 0.00001256
Iteration 272/1000 | Loss: 0.00001256
Iteration 273/1000 | Loss: 0.00001256
Iteration 274/1000 | Loss: 0.00001256
Iteration 275/1000 | Loss: 0.00001256
Iteration 276/1000 | Loss: 0.00001256
Iteration 277/1000 | Loss: 0.00001256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [1.25621027109446e-05, 1.25621027109446e-05, 1.25621027109446e-05, 1.25621027109446e-05, 1.25621027109446e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.25621027109446e-05

Optimization complete. Final v2v error: 3.0033395290374756 mm

Highest mean error: 4.0060343742370605 mm for frame 158

Lowest mean error: 2.48210072517395 mm for frame 219

Saving results

Total time: 136.36832404136658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415080
Iteration 2/25 | Loss: 0.00133152
Iteration 3/25 | Loss: 0.00124256
Iteration 4/25 | Loss: 0.00122685
Iteration 5/25 | Loss: 0.00122234
Iteration 6/25 | Loss: 0.00122152
Iteration 7/25 | Loss: 0.00122152
Iteration 8/25 | Loss: 0.00122152
Iteration 9/25 | Loss: 0.00122152
Iteration 10/25 | Loss: 0.00122152
Iteration 11/25 | Loss: 0.00122152
Iteration 12/25 | Loss: 0.00122152
Iteration 13/25 | Loss: 0.00122152
Iteration 14/25 | Loss: 0.00122152
Iteration 15/25 | Loss: 0.00122152
Iteration 16/25 | Loss: 0.00122152
Iteration 17/25 | Loss: 0.00122152
Iteration 18/25 | Loss: 0.00122152
Iteration 19/25 | Loss: 0.00122152
Iteration 20/25 | Loss: 0.00122152
Iteration 21/25 | Loss: 0.00122152
Iteration 22/25 | Loss: 0.00122152
Iteration 23/25 | Loss: 0.00122152
Iteration 24/25 | Loss: 0.00122152
Iteration 25/25 | Loss: 0.00122152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012215205933898687, 0.0012215205933898687, 0.0012215205933898687, 0.0012215205933898687, 0.0012215205933898687]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012215205933898687

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33508992
Iteration 2/25 | Loss: 0.00144123
Iteration 3/25 | Loss: 0.00144123
Iteration 4/25 | Loss: 0.00144123
Iteration 5/25 | Loss: 0.00144123
Iteration 6/25 | Loss: 0.00144123
Iteration 7/25 | Loss: 0.00144123
Iteration 8/25 | Loss: 0.00144123
Iteration 9/25 | Loss: 0.00144123
Iteration 10/25 | Loss: 0.00144123
Iteration 11/25 | Loss: 0.00144123
Iteration 12/25 | Loss: 0.00144123
Iteration 13/25 | Loss: 0.00144123
Iteration 14/25 | Loss: 0.00144123
Iteration 15/25 | Loss: 0.00144123
Iteration 16/25 | Loss: 0.00144123
Iteration 17/25 | Loss: 0.00144123
Iteration 18/25 | Loss: 0.00144123
Iteration 19/25 | Loss: 0.00144123
Iteration 20/25 | Loss: 0.00144123
Iteration 21/25 | Loss: 0.00144123
Iteration 22/25 | Loss: 0.00144123
Iteration 23/25 | Loss: 0.00144123
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014412307646125555, 0.0014412307646125555, 0.0014412307646125555, 0.0014412307646125555, 0.0014412307646125555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014412307646125555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144123
Iteration 2/1000 | Loss: 0.00002649
Iteration 3/1000 | Loss: 0.00002101
Iteration 4/1000 | Loss: 0.00001956
Iteration 5/1000 | Loss: 0.00001873
Iteration 6/1000 | Loss: 0.00001818
Iteration 7/1000 | Loss: 0.00001764
Iteration 8/1000 | Loss: 0.00001725
Iteration 9/1000 | Loss: 0.00001690
Iteration 10/1000 | Loss: 0.00001655
Iteration 11/1000 | Loss: 0.00001632
Iteration 12/1000 | Loss: 0.00001608
Iteration 13/1000 | Loss: 0.00001596
Iteration 14/1000 | Loss: 0.00001591
Iteration 15/1000 | Loss: 0.00001591
Iteration 16/1000 | Loss: 0.00001590
Iteration 17/1000 | Loss: 0.00001588
Iteration 18/1000 | Loss: 0.00001585
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001582
Iteration 21/1000 | Loss: 0.00001576
Iteration 22/1000 | Loss: 0.00001576
Iteration 23/1000 | Loss: 0.00001573
Iteration 24/1000 | Loss: 0.00001572
Iteration 25/1000 | Loss: 0.00001571
Iteration 26/1000 | Loss: 0.00001571
Iteration 27/1000 | Loss: 0.00001570
Iteration 28/1000 | Loss: 0.00001568
Iteration 29/1000 | Loss: 0.00001565
Iteration 30/1000 | Loss: 0.00001564
Iteration 31/1000 | Loss: 0.00001564
Iteration 32/1000 | Loss: 0.00001563
Iteration 33/1000 | Loss: 0.00001563
Iteration 34/1000 | Loss: 0.00001562
Iteration 35/1000 | Loss: 0.00001561
Iteration 36/1000 | Loss: 0.00001558
Iteration 37/1000 | Loss: 0.00001558
Iteration 38/1000 | Loss: 0.00001558
Iteration 39/1000 | Loss: 0.00001558
Iteration 40/1000 | Loss: 0.00001558
Iteration 41/1000 | Loss: 0.00001558
Iteration 42/1000 | Loss: 0.00001558
Iteration 43/1000 | Loss: 0.00001558
Iteration 44/1000 | Loss: 0.00001558
Iteration 45/1000 | Loss: 0.00001557
Iteration 46/1000 | Loss: 0.00001557
Iteration 47/1000 | Loss: 0.00001557
Iteration 48/1000 | Loss: 0.00001557
Iteration 49/1000 | Loss: 0.00001557
Iteration 50/1000 | Loss: 0.00001555
Iteration 51/1000 | Loss: 0.00001554
Iteration 52/1000 | Loss: 0.00001554
Iteration 53/1000 | Loss: 0.00001553
Iteration 54/1000 | Loss: 0.00001553
Iteration 55/1000 | Loss: 0.00001553
Iteration 56/1000 | Loss: 0.00001552
Iteration 57/1000 | Loss: 0.00001551
Iteration 58/1000 | Loss: 0.00001551
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001550
Iteration 61/1000 | Loss: 0.00001550
Iteration 62/1000 | Loss: 0.00001550
Iteration 63/1000 | Loss: 0.00001550
Iteration 64/1000 | Loss: 0.00001550
Iteration 65/1000 | Loss: 0.00001550
Iteration 66/1000 | Loss: 0.00001550
Iteration 67/1000 | Loss: 0.00001550
Iteration 68/1000 | Loss: 0.00001550
Iteration 69/1000 | Loss: 0.00001550
Iteration 70/1000 | Loss: 0.00001550
Iteration 71/1000 | Loss: 0.00001549
Iteration 72/1000 | Loss: 0.00001549
Iteration 73/1000 | Loss: 0.00001548
Iteration 74/1000 | Loss: 0.00001547
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001546
Iteration 77/1000 | Loss: 0.00001546
Iteration 78/1000 | Loss: 0.00001546
Iteration 79/1000 | Loss: 0.00001546
Iteration 80/1000 | Loss: 0.00001545
Iteration 81/1000 | Loss: 0.00001545
Iteration 82/1000 | Loss: 0.00001545
Iteration 83/1000 | Loss: 0.00001545
Iteration 84/1000 | Loss: 0.00001545
Iteration 85/1000 | Loss: 0.00001545
Iteration 86/1000 | Loss: 0.00001545
Iteration 87/1000 | Loss: 0.00001545
Iteration 88/1000 | Loss: 0.00001545
Iteration 89/1000 | Loss: 0.00001545
Iteration 90/1000 | Loss: 0.00001544
Iteration 91/1000 | Loss: 0.00001544
Iteration 92/1000 | Loss: 0.00001544
Iteration 93/1000 | Loss: 0.00001543
Iteration 94/1000 | Loss: 0.00001543
Iteration 95/1000 | Loss: 0.00001543
Iteration 96/1000 | Loss: 0.00001543
Iteration 97/1000 | Loss: 0.00001543
Iteration 98/1000 | Loss: 0.00001542
Iteration 99/1000 | Loss: 0.00001542
Iteration 100/1000 | Loss: 0.00001542
Iteration 101/1000 | Loss: 0.00001542
Iteration 102/1000 | Loss: 0.00001542
Iteration 103/1000 | Loss: 0.00001542
Iteration 104/1000 | Loss: 0.00001542
Iteration 105/1000 | Loss: 0.00001542
Iteration 106/1000 | Loss: 0.00001542
Iteration 107/1000 | Loss: 0.00001541
Iteration 108/1000 | Loss: 0.00001541
Iteration 109/1000 | Loss: 0.00001541
Iteration 110/1000 | Loss: 0.00001540
Iteration 111/1000 | Loss: 0.00001540
Iteration 112/1000 | Loss: 0.00001540
Iteration 113/1000 | Loss: 0.00001540
Iteration 114/1000 | Loss: 0.00001540
Iteration 115/1000 | Loss: 0.00001540
Iteration 116/1000 | Loss: 0.00001540
Iteration 117/1000 | Loss: 0.00001540
Iteration 118/1000 | Loss: 0.00001540
Iteration 119/1000 | Loss: 0.00001540
Iteration 120/1000 | Loss: 0.00001540
Iteration 121/1000 | Loss: 0.00001540
Iteration 122/1000 | Loss: 0.00001540
Iteration 123/1000 | Loss: 0.00001539
Iteration 124/1000 | Loss: 0.00001539
Iteration 125/1000 | Loss: 0.00001539
Iteration 126/1000 | Loss: 0.00001539
Iteration 127/1000 | Loss: 0.00001539
Iteration 128/1000 | Loss: 0.00001539
Iteration 129/1000 | Loss: 0.00001539
Iteration 130/1000 | Loss: 0.00001539
Iteration 131/1000 | Loss: 0.00001538
Iteration 132/1000 | Loss: 0.00001538
Iteration 133/1000 | Loss: 0.00001538
Iteration 134/1000 | Loss: 0.00001538
Iteration 135/1000 | Loss: 0.00001538
Iteration 136/1000 | Loss: 0.00001538
Iteration 137/1000 | Loss: 0.00001538
Iteration 138/1000 | Loss: 0.00001538
Iteration 139/1000 | Loss: 0.00001538
Iteration 140/1000 | Loss: 0.00001537
Iteration 141/1000 | Loss: 0.00001537
Iteration 142/1000 | Loss: 0.00001537
Iteration 143/1000 | Loss: 0.00001537
Iteration 144/1000 | Loss: 0.00001537
Iteration 145/1000 | Loss: 0.00001537
Iteration 146/1000 | Loss: 0.00001537
Iteration 147/1000 | Loss: 0.00001537
Iteration 148/1000 | Loss: 0.00001537
Iteration 149/1000 | Loss: 0.00001537
Iteration 150/1000 | Loss: 0.00001537
Iteration 151/1000 | Loss: 0.00001537
Iteration 152/1000 | Loss: 0.00001537
Iteration 153/1000 | Loss: 0.00001537
Iteration 154/1000 | Loss: 0.00001537
Iteration 155/1000 | Loss: 0.00001537
Iteration 156/1000 | Loss: 0.00001537
Iteration 157/1000 | Loss: 0.00001537
Iteration 158/1000 | Loss: 0.00001537
Iteration 159/1000 | Loss: 0.00001537
Iteration 160/1000 | Loss: 0.00001537
Iteration 161/1000 | Loss: 0.00001537
Iteration 162/1000 | Loss: 0.00001537
Iteration 163/1000 | Loss: 0.00001537
Iteration 164/1000 | Loss: 0.00001537
Iteration 165/1000 | Loss: 0.00001537
Iteration 166/1000 | Loss: 0.00001537
Iteration 167/1000 | Loss: 0.00001537
Iteration 168/1000 | Loss: 0.00001537
Iteration 169/1000 | Loss: 0.00001537
Iteration 170/1000 | Loss: 0.00001537
Iteration 171/1000 | Loss: 0.00001537
Iteration 172/1000 | Loss: 0.00001537
Iteration 173/1000 | Loss: 0.00001537
Iteration 174/1000 | Loss: 0.00001537
Iteration 175/1000 | Loss: 0.00001537
Iteration 176/1000 | Loss: 0.00001537
Iteration 177/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.53652854351094e-05, 1.53652854351094e-05, 1.53652854351094e-05, 1.53652854351094e-05, 1.53652854351094e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.53652854351094e-05

Optimization complete. Final v2v error: 3.3808910846710205 mm

Highest mean error: 3.7618179321289062 mm for frame 25

Lowest mean error: 3.1312785148620605 mm for frame 46

Saving results

Total time: 39.94000959396362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00756443
Iteration 2/25 | Loss: 0.00137211
Iteration 3/25 | Loss: 0.00125731
Iteration 4/25 | Loss: 0.00124185
Iteration 5/25 | Loss: 0.00123856
Iteration 6/25 | Loss: 0.00123856
Iteration 7/25 | Loss: 0.00123856
Iteration 8/25 | Loss: 0.00123856
Iteration 9/25 | Loss: 0.00123856
Iteration 10/25 | Loss: 0.00123856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001238559721969068, 0.001238559721969068, 0.001238559721969068, 0.001238559721969068, 0.001238559721969068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001238559721969068

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.58068991
Iteration 2/25 | Loss: 0.00116991
Iteration 3/25 | Loss: 0.00116989
Iteration 4/25 | Loss: 0.00116988
Iteration 5/25 | Loss: 0.00116988
Iteration 6/25 | Loss: 0.00116988
Iteration 7/25 | Loss: 0.00116988
Iteration 8/25 | Loss: 0.00116988
Iteration 9/25 | Loss: 0.00116988
Iteration 10/25 | Loss: 0.00116988
Iteration 11/25 | Loss: 0.00116988
Iteration 12/25 | Loss: 0.00116988
Iteration 13/25 | Loss: 0.00116988
Iteration 14/25 | Loss: 0.00116988
Iteration 15/25 | Loss: 0.00116988
Iteration 16/25 | Loss: 0.00116988
Iteration 17/25 | Loss: 0.00116988
Iteration 18/25 | Loss: 0.00116988
Iteration 19/25 | Loss: 0.00116988
Iteration 20/25 | Loss: 0.00116988
Iteration 21/25 | Loss: 0.00116988
Iteration 22/25 | Loss: 0.00116988
Iteration 23/25 | Loss: 0.00116988
Iteration 24/25 | Loss: 0.00116988
Iteration 25/25 | Loss: 0.00116988

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116988
Iteration 2/1000 | Loss: 0.00002771
Iteration 3/1000 | Loss: 0.00001943
Iteration 4/1000 | Loss: 0.00001816
Iteration 5/1000 | Loss: 0.00001729
Iteration 6/1000 | Loss: 0.00001660
Iteration 7/1000 | Loss: 0.00001632
Iteration 8/1000 | Loss: 0.00001592
Iteration 9/1000 | Loss: 0.00001558
Iteration 10/1000 | Loss: 0.00001532
Iteration 11/1000 | Loss: 0.00001516
Iteration 12/1000 | Loss: 0.00001507
Iteration 13/1000 | Loss: 0.00001502
Iteration 14/1000 | Loss: 0.00001501
Iteration 15/1000 | Loss: 0.00001498
Iteration 16/1000 | Loss: 0.00001495
Iteration 17/1000 | Loss: 0.00001493
Iteration 18/1000 | Loss: 0.00001493
Iteration 19/1000 | Loss: 0.00001492
Iteration 20/1000 | Loss: 0.00001491
Iteration 21/1000 | Loss: 0.00001491
Iteration 22/1000 | Loss: 0.00001491
Iteration 23/1000 | Loss: 0.00001488
Iteration 24/1000 | Loss: 0.00001488
Iteration 25/1000 | Loss: 0.00001487
Iteration 26/1000 | Loss: 0.00001487
Iteration 27/1000 | Loss: 0.00001486
Iteration 28/1000 | Loss: 0.00001485
Iteration 29/1000 | Loss: 0.00001484
Iteration 30/1000 | Loss: 0.00001480
Iteration 31/1000 | Loss: 0.00001474
Iteration 32/1000 | Loss: 0.00001474
Iteration 33/1000 | Loss: 0.00001473
Iteration 34/1000 | Loss: 0.00001473
Iteration 35/1000 | Loss: 0.00001472
Iteration 36/1000 | Loss: 0.00001472
Iteration 37/1000 | Loss: 0.00001472
Iteration 38/1000 | Loss: 0.00001470
Iteration 39/1000 | Loss: 0.00001469
Iteration 40/1000 | Loss: 0.00001469
Iteration 41/1000 | Loss: 0.00001468
Iteration 42/1000 | Loss: 0.00001468
Iteration 43/1000 | Loss: 0.00001468
Iteration 44/1000 | Loss: 0.00001467
Iteration 45/1000 | Loss: 0.00001467
Iteration 46/1000 | Loss: 0.00001464
Iteration 47/1000 | Loss: 0.00001464
Iteration 48/1000 | Loss: 0.00001464
Iteration 49/1000 | Loss: 0.00001464
Iteration 50/1000 | Loss: 0.00001464
Iteration 51/1000 | Loss: 0.00001464
Iteration 52/1000 | Loss: 0.00001464
Iteration 53/1000 | Loss: 0.00001464
Iteration 54/1000 | Loss: 0.00001464
Iteration 55/1000 | Loss: 0.00001464
Iteration 56/1000 | Loss: 0.00001464
Iteration 57/1000 | Loss: 0.00001463
Iteration 58/1000 | Loss: 0.00001463
Iteration 59/1000 | Loss: 0.00001463
Iteration 60/1000 | Loss: 0.00001463
Iteration 61/1000 | Loss: 0.00001463
Iteration 62/1000 | Loss: 0.00001463
Iteration 63/1000 | Loss: 0.00001463
Iteration 64/1000 | Loss: 0.00001463
Iteration 65/1000 | Loss: 0.00001463
Iteration 66/1000 | Loss: 0.00001463
Iteration 67/1000 | Loss: 0.00001463
Iteration 68/1000 | Loss: 0.00001463
Iteration 69/1000 | Loss: 0.00001463
Iteration 70/1000 | Loss: 0.00001463
Iteration 71/1000 | Loss: 0.00001463
Iteration 72/1000 | Loss: 0.00001463
Iteration 73/1000 | Loss: 0.00001463
Iteration 74/1000 | Loss: 0.00001463
Iteration 75/1000 | Loss: 0.00001463
Iteration 76/1000 | Loss: 0.00001463
Iteration 77/1000 | Loss: 0.00001463
Iteration 78/1000 | Loss: 0.00001463
Iteration 79/1000 | Loss: 0.00001463
Iteration 80/1000 | Loss: 0.00001463
Iteration 81/1000 | Loss: 0.00001463
Iteration 82/1000 | Loss: 0.00001463
Iteration 83/1000 | Loss: 0.00001463
Iteration 84/1000 | Loss: 0.00001463
Iteration 85/1000 | Loss: 0.00001463
Iteration 86/1000 | Loss: 0.00001463
Iteration 87/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.4634826584369875e-05, 1.4634826584369875e-05, 1.4634826584369875e-05, 1.4634826584369875e-05, 1.4634826584369875e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4634826584369875e-05

Optimization complete. Final v2v error: 3.27925181388855 mm

Highest mean error: 3.65346622467041 mm for frame 146

Lowest mean error: 2.96272349357605 mm for frame 212

Saving results

Total time: 35.22121238708496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962980
Iteration 2/25 | Loss: 0.00173882
Iteration 3/25 | Loss: 0.00139255
Iteration 4/25 | Loss: 0.00134763
Iteration 5/25 | Loss: 0.00135536
Iteration 6/25 | Loss: 0.00132979
Iteration 7/25 | Loss: 0.00128285
Iteration 8/25 | Loss: 0.00127125
Iteration 9/25 | Loss: 0.00126550
Iteration 10/25 | Loss: 0.00125626
Iteration 11/25 | Loss: 0.00124694
Iteration 12/25 | Loss: 0.00124797
Iteration 13/25 | Loss: 0.00124324
Iteration 14/25 | Loss: 0.00124066
Iteration 15/25 | Loss: 0.00123651
Iteration 16/25 | Loss: 0.00123404
Iteration 17/25 | Loss: 0.00123258
Iteration 18/25 | Loss: 0.00123087
Iteration 19/25 | Loss: 0.00123002
Iteration 20/25 | Loss: 0.00122854
Iteration 21/25 | Loss: 0.00122739
Iteration 22/25 | Loss: 0.00122686
Iteration 23/25 | Loss: 0.00122667
Iteration 24/25 | Loss: 0.00122657
Iteration 25/25 | Loss: 0.00122655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39401281
Iteration 2/25 | Loss: 0.00180379
Iteration 3/25 | Loss: 0.00180379
Iteration 4/25 | Loss: 0.00180378
Iteration 5/25 | Loss: 0.00180378
Iteration 6/25 | Loss: 0.00180378
Iteration 7/25 | Loss: 0.00180378
Iteration 8/25 | Loss: 0.00180378
Iteration 9/25 | Loss: 0.00180378
Iteration 10/25 | Loss: 0.00180378
Iteration 11/25 | Loss: 0.00180378
Iteration 12/25 | Loss: 0.00180378
Iteration 13/25 | Loss: 0.00180378
Iteration 14/25 | Loss: 0.00180378
Iteration 15/25 | Loss: 0.00180378
Iteration 16/25 | Loss: 0.00180378
Iteration 17/25 | Loss: 0.00180378
Iteration 18/25 | Loss: 0.00180378
Iteration 19/25 | Loss: 0.00180378
Iteration 20/25 | Loss: 0.00180378
Iteration 21/25 | Loss: 0.00180378
Iteration 22/25 | Loss: 0.00180378
Iteration 23/25 | Loss: 0.00180378
Iteration 24/25 | Loss: 0.00180378
Iteration 25/25 | Loss: 0.00180378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180378
Iteration 2/1000 | Loss: 0.00004405
Iteration 3/1000 | Loss: 0.00002992
Iteration 4/1000 | Loss: 0.00012075
Iteration 5/1000 | Loss: 0.00002330
Iteration 6/1000 | Loss: 0.00002218
Iteration 7/1000 | Loss: 0.00002128
Iteration 8/1000 | Loss: 0.00002069
Iteration 9/1000 | Loss: 0.00002032
Iteration 10/1000 | Loss: 0.00002013
Iteration 11/1000 | Loss: 0.00001982
Iteration 12/1000 | Loss: 0.00001958
Iteration 13/1000 | Loss: 0.00001953
Iteration 14/1000 | Loss: 0.00009999
Iteration 15/1000 | Loss: 0.00002351
Iteration 16/1000 | Loss: 0.00009035
Iteration 17/1000 | Loss: 0.00006515
Iteration 18/1000 | Loss: 0.00002089
Iteration 19/1000 | Loss: 0.00007439
Iteration 20/1000 | Loss: 0.00001708
Iteration 21/1000 | Loss: 0.00001655
Iteration 22/1000 | Loss: 0.00011553
Iteration 23/1000 | Loss: 0.00012079
Iteration 24/1000 | Loss: 0.00002001
Iteration 25/1000 | Loss: 0.00004208
Iteration 26/1000 | Loss: 0.00001822
Iteration 27/1000 | Loss: 0.00002144
Iteration 28/1000 | Loss: 0.00013109
Iteration 29/1000 | Loss: 0.00006349
Iteration 30/1000 | Loss: 0.00001629
Iteration 31/1000 | Loss: 0.00001603
Iteration 32/1000 | Loss: 0.00001603
Iteration 33/1000 | Loss: 0.00002687
Iteration 34/1000 | Loss: 0.00002149
Iteration 35/1000 | Loss: 0.00002723
Iteration 36/1000 | Loss: 0.00012445
Iteration 37/1000 | Loss: 0.00002226
Iteration 38/1000 | Loss: 0.00005598
Iteration 39/1000 | Loss: 0.00004610
Iteration 40/1000 | Loss: 0.00002773
Iteration 41/1000 | Loss: 0.00002071
Iteration 42/1000 | Loss: 0.00003433
Iteration 43/1000 | Loss: 0.00001381
Iteration 44/1000 | Loss: 0.00005660
Iteration 45/1000 | Loss: 0.00001450
Iteration 46/1000 | Loss: 0.00001338
Iteration 47/1000 | Loss: 0.00001682
Iteration 48/1000 | Loss: 0.00002194
Iteration 49/1000 | Loss: 0.00001321
Iteration 50/1000 | Loss: 0.00001319
Iteration 51/1000 | Loss: 0.00001318
Iteration 52/1000 | Loss: 0.00001304
Iteration 53/1000 | Loss: 0.00001302
Iteration 54/1000 | Loss: 0.00003239
Iteration 55/1000 | Loss: 0.00001385
Iteration 56/1000 | Loss: 0.00001270
Iteration 57/1000 | Loss: 0.00001269
Iteration 58/1000 | Loss: 0.00001268
Iteration 59/1000 | Loss: 0.00001253
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001247
Iteration 62/1000 | Loss: 0.00001247
Iteration 63/1000 | Loss: 0.00001246
Iteration 64/1000 | Loss: 0.00001716
Iteration 65/1000 | Loss: 0.00001242
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001239
Iteration 69/1000 | Loss: 0.00001238
Iteration 70/1000 | Loss: 0.00001236
Iteration 71/1000 | Loss: 0.00001487
Iteration 72/1000 | Loss: 0.00001237
Iteration 73/1000 | Loss: 0.00001236
Iteration 74/1000 | Loss: 0.00001235
Iteration 75/1000 | Loss: 0.00001235
Iteration 76/1000 | Loss: 0.00001235
Iteration 77/1000 | Loss: 0.00001235
Iteration 78/1000 | Loss: 0.00001234
Iteration 79/1000 | Loss: 0.00001234
Iteration 80/1000 | Loss: 0.00001234
Iteration 81/1000 | Loss: 0.00001233
Iteration 82/1000 | Loss: 0.00001233
Iteration 83/1000 | Loss: 0.00001233
Iteration 84/1000 | Loss: 0.00001232
Iteration 85/1000 | Loss: 0.00001232
Iteration 86/1000 | Loss: 0.00001232
Iteration 87/1000 | Loss: 0.00001232
Iteration 88/1000 | Loss: 0.00001232
Iteration 89/1000 | Loss: 0.00001232
Iteration 90/1000 | Loss: 0.00001232
Iteration 91/1000 | Loss: 0.00001232
Iteration 92/1000 | Loss: 0.00001232
Iteration 93/1000 | Loss: 0.00001231
Iteration 94/1000 | Loss: 0.00001231
Iteration 95/1000 | Loss: 0.00001231
Iteration 96/1000 | Loss: 0.00001230
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001228
Iteration 100/1000 | Loss: 0.00001228
Iteration 101/1000 | Loss: 0.00001227
Iteration 102/1000 | Loss: 0.00001227
Iteration 103/1000 | Loss: 0.00001227
Iteration 104/1000 | Loss: 0.00001227
Iteration 105/1000 | Loss: 0.00001227
Iteration 106/1000 | Loss: 0.00001227
Iteration 107/1000 | Loss: 0.00001227
Iteration 108/1000 | Loss: 0.00001226
Iteration 109/1000 | Loss: 0.00001226
Iteration 110/1000 | Loss: 0.00001226
Iteration 111/1000 | Loss: 0.00001226
Iteration 112/1000 | Loss: 0.00001226
Iteration 113/1000 | Loss: 0.00001226
Iteration 114/1000 | Loss: 0.00001226
Iteration 115/1000 | Loss: 0.00001225
Iteration 116/1000 | Loss: 0.00001225
Iteration 117/1000 | Loss: 0.00001804
Iteration 118/1000 | Loss: 0.00001251
Iteration 119/1000 | Loss: 0.00001553
Iteration 120/1000 | Loss: 0.00001242
Iteration 121/1000 | Loss: 0.00001250
Iteration 122/1000 | Loss: 0.00001225
Iteration 123/1000 | Loss: 0.00001225
Iteration 124/1000 | Loss: 0.00001225
Iteration 125/1000 | Loss: 0.00001225
Iteration 126/1000 | Loss: 0.00001225
Iteration 127/1000 | Loss: 0.00001225
Iteration 128/1000 | Loss: 0.00001225
Iteration 129/1000 | Loss: 0.00001225
Iteration 130/1000 | Loss: 0.00001225
Iteration 131/1000 | Loss: 0.00001224
Iteration 132/1000 | Loss: 0.00001224
Iteration 133/1000 | Loss: 0.00001224
Iteration 134/1000 | Loss: 0.00001224
Iteration 135/1000 | Loss: 0.00001224
Iteration 136/1000 | Loss: 0.00001224
Iteration 137/1000 | Loss: 0.00001224
Iteration 138/1000 | Loss: 0.00001224
Iteration 139/1000 | Loss: 0.00001223
Iteration 140/1000 | Loss: 0.00001223
Iteration 141/1000 | Loss: 0.00001223
Iteration 142/1000 | Loss: 0.00001223
Iteration 143/1000 | Loss: 0.00001223
Iteration 144/1000 | Loss: 0.00001223
Iteration 145/1000 | Loss: 0.00001223
Iteration 146/1000 | Loss: 0.00001223
Iteration 147/1000 | Loss: 0.00001223
Iteration 148/1000 | Loss: 0.00001223
Iteration 149/1000 | Loss: 0.00001223
Iteration 150/1000 | Loss: 0.00001223
Iteration 151/1000 | Loss: 0.00001223
Iteration 152/1000 | Loss: 0.00001223
Iteration 153/1000 | Loss: 0.00001223
Iteration 154/1000 | Loss: 0.00001222
Iteration 155/1000 | Loss: 0.00001222
Iteration 156/1000 | Loss: 0.00001222
Iteration 157/1000 | Loss: 0.00001222
Iteration 158/1000 | Loss: 0.00001222
Iteration 159/1000 | Loss: 0.00001222
Iteration 160/1000 | Loss: 0.00001222
Iteration 161/1000 | Loss: 0.00001222
Iteration 162/1000 | Loss: 0.00001222
Iteration 163/1000 | Loss: 0.00001222
Iteration 164/1000 | Loss: 0.00001222
Iteration 165/1000 | Loss: 0.00001222
Iteration 166/1000 | Loss: 0.00001222
Iteration 167/1000 | Loss: 0.00001222
Iteration 168/1000 | Loss: 0.00001222
Iteration 169/1000 | Loss: 0.00001222
Iteration 170/1000 | Loss: 0.00001222
Iteration 171/1000 | Loss: 0.00001222
Iteration 172/1000 | Loss: 0.00001222
Iteration 173/1000 | Loss: 0.00001222
Iteration 174/1000 | Loss: 0.00001222
Iteration 175/1000 | Loss: 0.00001222
Iteration 176/1000 | Loss: 0.00001222
Iteration 177/1000 | Loss: 0.00001222
Iteration 178/1000 | Loss: 0.00001222
Iteration 179/1000 | Loss: 0.00001222
Iteration 180/1000 | Loss: 0.00001222
Iteration 181/1000 | Loss: 0.00001222
Iteration 182/1000 | Loss: 0.00001222
Iteration 183/1000 | Loss: 0.00001222
Iteration 184/1000 | Loss: 0.00001222
Iteration 185/1000 | Loss: 0.00001222
Iteration 186/1000 | Loss: 0.00001222
Iteration 187/1000 | Loss: 0.00001222
Iteration 188/1000 | Loss: 0.00001222
Iteration 189/1000 | Loss: 0.00001222
Iteration 190/1000 | Loss: 0.00001222
Iteration 191/1000 | Loss: 0.00001222
Iteration 192/1000 | Loss: 0.00001222
Iteration 193/1000 | Loss: 0.00001222
Iteration 194/1000 | Loss: 0.00001222
Iteration 195/1000 | Loss: 0.00001222
Iteration 196/1000 | Loss: 0.00001222
Iteration 197/1000 | Loss: 0.00001222
Iteration 198/1000 | Loss: 0.00001222
Iteration 199/1000 | Loss: 0.00001222
Iteration 200/1000 | Loss: 0.00001222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.2218212759762537e-05, 1.2218212759762537e-05, 1.2218212759762537e-05, 1.2218212759762537e-05, 1.2218212759762537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2218212759762537e-05

Optimization complete. Final v2v error: 2.9823334217071533 mm

Highest mean error: 4.567159175872803 mm for frame 58

Lowest mean error: 2.393794059753418 mm for frame 97

Saving results

Total time: 130.1951823234558
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374024
Iteration 2/25 | Loss: 0.00125187
Iteration 3/25 | Loss: 0.00118863
Iteration 4/25 | Loss: 0.00118394
Iteration 5/25 | Loss: 0.00118284
Iteration 6/25 | Loss: 0.00118284
Iteration 7/25 | Loss: 0.00118284
Iteration 8/25 | Loss: 0.00118284
Iteration 9/25 | Loss: 0.00118284
Iteration 10/25 | Loss: 0.00118284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011828428832814097, 0.0011828428832814097, 0.0011828428832814097, 0.0011828428832814097, 0.0011828428832814097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011828428832814097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81478179
Iteration 2/25 | Loss: 0.00149552
Iteration 3/25 | Loss: 0.00149551
Iteration 4/25 | Loss: 0.00149551
Iteration 5/25 | Loss: 0.00149551
Iteration 6/25 | Loss: 0.00149551
Iteration 7/25 | Loss: 0.00149551
Iteration 8/25 | Loss: 0.00149551
Iteration 9/25 | Loss: 0.00149551
Iteration 10/25 | Loss: 0.00149551
Iteration 11/25 | Loss: 0.00149551
Iteration 12/25 | Loss: 0.00149551
Iteration 13/25 | Loss: 0.00149551
Iteration 14/25 | Loss: 0.00149551
Iteration 15/25 | Loss: 0.00149551
Iteration 16/25 | Loss: 0.00149551
Iteration 17/25 | Loss: 0.00149551
Iteration 18/25 | Loss: 0.00149551
Iteration 19/25 | Loss: 0.00149551
Iteration 20/25 | Loss: 0.00149551
Iteration 21/25 | Loss: 0.00149551
Iteration 22/25 | Loss: 0.00149551
Iteration 23/25 | Loss: 0.00149551
Iteration 24/25 | Loss: 0.00149551
Iteration 25/25 | Loss: 0.00149551

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149551
Iteration 2/1000 | Loss: 0.00002447
Iteration 3/1000 | Loss: 0.00001593
Iteration 4/1000 | Loss: 0.00001296
Iteration 5/1000 | Loss: 0.00001161
Iteration 6/1000 | Loss: 0.00001095
Iteration 7/1000 | Loss: 0.00001029
Iteration 8/1000 | Loss: 0.00000990
Iteration 9/1000 | Loss: 0.00000967
Iteration 10/1000 | Loss: 0.00000934
Iteration 11/1000 | Loss: 0.00000923
Iteration 12/1000 | Loss: 0.00000913
Iteration 13/1000 | Loss: 0.00000911
Iteration 14/1000 | Loss: 0.00000910
Iteration 15/1000 | Loss: 0.00000910
Iteration 16/1000 | Loss: 0.00000909
Iteration 17/1000 | Loss: 0.00000903
Iteration 18/1000 | Loss: 0.00000902
Iteration 19/1000 | Loss: 0.00000893
Iteration 20/1000 | Loss: 0.00000892
Iteration 21/1000 | Loss: 0.00000892
Iteration 22/1000 | Loss: 0.00000890
Iteration 23/1000 | Loss: 0.00000889
Iteration 24/1000 | Loss: 0.00000888
Iteration 25/1000 | Loss: 0.00000887
Iteration 26/1000 | Loss: 0.00000886
Iteration 27/1000 | Loss: 0.00000886
Iteration 28/1000 | Loss: 0.00000885
Iteration 29/1000 | Loss: 0.00000884
Iteration 30/1000 | Loss: 0.00000884
Iteration 31/1000 | Loss: 0.00000883
Iteration 32/1000 | Loss: 0.00000877
Iteration 33/1000 | Loss: 0.00000874
Iteration 34/1000 | Loss: 0.00000873
Iteration 35/1000 | Loss: 0.00000873
Iteration 36/1000 | Loss: 0.00000871
Iteration 37/1000 | Loss: 0.00000870
Iteration 38/1000 | Loss: 0.00000869
Iteration 39/1000 | Loss: 0.00000869
Iteration 40/1000 | Loss: 0.00000868
Iteration 41/1000 | Loss: 0.00000868
Iteration 42/1000 | Loss: 0.00000868
Iteration 43/1000 | Loss: 0.00000867
Iteration 44/1000 | Loss: 0.00000867
Iteration 45/1000 | Loss: 0.00000866
Iteration 46/1000 | Loss: 0.00000866
Iteration 47/1000 | Loss: 0.00000863
Iteration 48/1000 | Loss: 0.00000863
Iteration 49/1000 | Loss: 0.00000863
Iteration 50/1000 | Loss: 0.00000862
Iteration 51/1000 | Loss: 0.00000860
Iteration 52/1000 | Loss: 0.00000858
Iteration 53/1000 | Loss: 0.00000858
Iteration 54/1000 | Loss: 0.00000857
Iteration 55/1000 | Loss: 0.00000857
Iteration 56/1000 | Loss: 0.00000856
Iteration 57/1000 | Loss: 0.00000855
Iteration 58/1000 | Loss: 0.00000855
Iteration 59/1000 | Loss: 0.00000854
Iteration 60/1000 | Loss: 0.00000853
Iteration 61/1000 | Loss: 0.00000853
Iteration 62/1000 | Loss: 0.00000852
Iteration 63/1000 | Loss: 0.00000852
Iteration 64/1000 | Loss: 0.00000852
Iteration 65/1000 | Loss: 0.00000852
Iteration 66/1000 | Loss: 0.00000852
Iteration 67/1000 | Loss: 0.00000851
Iteration 68/1000 | Loss: 0.00000850
Iteration 69/1000 | Loss: 0.00000850
Iteration 70/1000 | Loss: 0.00000849
Iteration 71/1000 | Loss: 0.00000849
Iteration 72/1000 | Loss: 0.00000848
Iteration 73/1000 | Loss: 0.00000848
Iteration 74/1000 | Loss: 0.00000848
Iteration 75/1000 | Loss: 0.00000848
Iteration 76/1000 | Loss: 0.00000847
Iteration 77/1000 | Loss: 0.00000847
Iteration 78/1000 | Loss: 0.00000847
Iteration 79/1000 | Loss: 0.00000847
Iteration 80/1000 | Loss: 0.00000846
Iteration 81/1000 | Loss: 0.00000846
Iteration 82/1000 | Loss: 0.00000846
Iteration 83/1000 | Loss: 0.00000845
Iteration 84/1000 | Loss: 0.00000845
Iteration 85/1000 | Loss: 0.00000845
Iteration 86/1000 | Loss: 0.00000845
Iteration 87/1000 | Loss: 0.00000845
Iteration 88/1000 | Loss: 0.00000845
Iteration 89/1000 | Loss: 0.00000844
Iteration 90/1000 | Loss: 0.00000844
Iteration 91/1000 | Loss: 0.00000844
Iteration 92/1000 | Loss: 0.00000844
Iteration 93/1000 | Loss: 0.00000843
Iteration 94/1000 | Loss: 0.00000843
Iteration 95/1000 | Loss: 0.00000843
Iteration 96/1000 | Loss: 0.00000843
Iteration 97/1000 | Loss: 0.00000843
Iteration 98/1000 | Loss: 0.00000842
Iteration 99/1000 | Loss: 0.00000842
Iteration 100/1000 | Loss: 0.00000841
Iteration 101/1000 | Loss: 0.00000841
Iteration 102/1000 | Loss: 0.00000841
Iteration 103/1000 | Loss: 0.00000841
Iteration 104/1000 | Loss: 0.00000840
Iteration 105/1000 | Loss: 0.00000840
Iteration 106/1000 | Loss: 0.00000840
Iteration 107/1000 | Loss: 0.00000840
Iteration 108/1000 | Loss: 0.00000840
Iteration 109/1000 | Loss: 0.00000839
Iteration 110/1000 | Loss: 0.00000839
Iteration 111/1000 | Loss: 0.00000839
Iteration 112/1000 | Loss: 0.00000839
Iteration 113/1000 | Loss: 0.00000839
Iteration 114/1000 | Loss: 0.00000838
Iteration 115/1000 | Loss: 0.00000838
Iteration 116/1000 | Loss: 0.00000838
Iteration 117/1000 | Loss: 0.00000837
Iteration 118/1000 | Loss: 0.00000837
Iteration 119/1000 | Loss: 0.00000837
Iteration 120/1000 | Loss: 0.00000837
Iteration 121/1000 | Loss: 0.00000837
Iteration 122/1000 | Loss: 0.00000837
Iteration 123/1000 | Loss: 0.00000837
Iteration 124/1000 | Loss: 0.00000837
Iteration 125/1000 | Loss: 0.00000836
Iteration 126/1000 | Loss: 0.00000836
Iteration 127/1000 | Loss: 0.00000836
Iteration 128/1000 | Loss: 0.00000836
Iteration 129/1000 | Loss: 0.00000836
Iteration 130/1000 | Loss: 0.00000836
Iteration 131/1000 | Loss: 0.00000836
Iteration 132/1000 | Loss: 0.00000835
Iteration 133/1000 | Loss: 0.00000835
Iteration 134/1000 | Loss: 0.00000835
Iteration 135/1000 | Loss: 0.00000835
Iteration 136/1000 | Loss: 0.00000835
Iteration 137/1000 | Loss: 0.00000835
Iteration 138/1000 | Loss: 0.00000834
Iteration 139/1000 | Loss: 0.00000834
Iteration 140/1000 | Loss: 0.00000834
Iteration 141/1000 | Loss: 0.00000834
Iteration 142/1000 | Loss: 0.00000834
Iteration 143/1000 | Loss: 0.00000834
Iteration 144/1000 | Loss: 0.00000834
Iteration 145/1000 | Loss: 0.00000834
Iteration 146/1000 | Loss: 0.00000834
Iteration 147/1000 | Loss: 0.00000834
Iteration 148/1000 | Loss: 0.00000834
Iteration 149/1000 | Loss: 0.00000834
Iteration 150/1000 | Loss: 0.00000834
Iteration 151/1000 | Loss: 0.00000834
Iteration 152/1000 | Loss: 0.00000834
Iteration 153/1000 | Loss: 0.00000834
Iteration 154/1000 | Loss: 0.00000834
Iteration 155/1000 | Loss: 0.00000834
Iteration 156/1000 | Loss: 0.00000834
Iteration 157/1000 | Loss: 0.00000834
Iteration 158/1000 | Loss: 0.00000834
Iteration 159/1000 | Loss: 0.00000834
Iteration 160/1000 | Loss: 0.00000834
Iteration 161/1000 | Loss: 0.00000834
Iteration 162/1000 | Loss: 0.00000834
Iteration 163/1000 | Loss: 0.00000834
Iteration 164/1000 | Loss: 0.00000834
Iteration 165/1000 | Loss: 0.00000834
Iteration 166/1000 | Loss: 0.00000834
Iteration 167/1000 | Loss: 0.00000834
Iteration 168/1000 | Loss: 0.00000834
Iteration 169/1000 | Loss: 0.00000834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [8.33736703498289e-06, 8.33736703498289e-06, 8.33736703498289e-06, 8.33736703498289e-06, 8.33736703498289e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.33736703498289e-06

Optimization complete. Final v2v error: 2.4888761043548584 mm

Highest mean error: 3.1161534786224365 mm for frame 77

Lowest mean error: 2.3024790287017822 mm for frame 151

Saving results

Total time: 40.33206510543823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419953
Iteration 2/25 | Loss: 0.00129643
Iteration 3/25 | Loss: 0.00123055
Iteration 4/25 | Loss: 0.00121312
Iteration 5/25 | Loss: 0.00120818
Iteration 6/25 | Loss: 0.00120818
Iteration 7/25 | Loss: 0.00120818
Iteration 8/25 | Loss: 0.00120818
Iteration 9/25 | Loss: 0.00120818
Iteration 10/25 | Loss: 0.00120818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012081770692020655, 0.0012081770692020655, 0.0012081770692020655, 0.0012081770692020655, 0.0012081770692020655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012081770692020655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29480243
Iteration 2/25 | Loss: 0.00137543
Iteration 3/25 | Loss: 0.00137542
Iteration 4/25 | Loss: 0.00137542
Iteration 5/25 | Loss: 0.00137542
Iteration 6/25 | Loss: 0.00137542
Iteration 7/25 | Loss: 0.00137542
Iteration 8/25 | Loss: 0.00137542
Iteration 9/25 | Loss: 0.00137542
Iteration 10/25 | Loss: 0.00137542
Iteration 11/25 | Loss: 0.00137542
Iteration 12/25 | Loss: 0.00137542
Iteration 13/25 | Loss: 0.00137542
Iteration 14/25 | Loss: 0.00137542
Iteration 15/25 | Loss: 0.00137542
Iteration 16/25 | Loss: 0.00137542
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013754211831837893, 0.0013754211831837893, 0.0013754211831837893, 0.0013754211831837893, 0.0013754211831837893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013754211831837893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137542
Iteration 2/1000 | Loss: 0.00002048
Iteration 3/1000 | Loss: 0.00001701
Iteration 4/1000 | Loss: 0.00001588
Iteration 5/1000 | Loss: 0.00001488
Iteration 6/1000 | Loss: 0.00001454
Iteration 7/1000 | Loss: 0.00001436
Iteration 8/1000 | Loss: 0.00001393
Iteration 9/1000 | Loss: 0.00001360
Iteration 10/1000 | Loss: 0.00001336
Iteration 11/1000 | Loss: 0.00001314
Iteration 12/1000 | Loss: 0.00001300
Iteration 13/1000 | Loss: 0.00001294
Iteration 14/1000 | Loss: 0.00001293
Iteration 15/1000 | Loss: 0.00001291
Iteration 16/1000 | Loss: 0.00001291
Iteration 17/1000 | Loss: 0.00001291
Iteration 18/1000 | Loss: 0.00001290
Iteration 19/1000 | Loss: 0.00001290
Iteration 20/1000 | Loss: 0.00001288
Iteration 21/1000 | Loss: 0.00001282
Iteration 22/1000 | Loss: 0.00001281
Iteration 23/1000 | Loss: 0.00001280
Iteration 24/1000 | Loss: 0.00001280
Iteration 25/1000 | Loss: 0.00001278
Iteration 26/1000 | Loss: 0.00001277
Iteration 27/1000 | Loss: 0.00001273
Iteration 28/1000 | Loss: 0.00001272
Iteration 29/1000 | Loss: 0.00001262
Iteration 30/1000 | Loss: 0.00001262
Iteration 31/1000 | Loss: 0.00001259
Iteration 32/1000 | Loss: 0.00001255
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001249
Iteration 35/1000 | Loss: 0.00001243
Iteration 36/1000 | Loss: 0.00001234
Iteration 37/1000 | Loss: 0.00001230
Iteration 38/1000 | Loss: 0.00001230
Iteration 39/1000 | Loss: 0.00001230
Iteration 40/1000 | Loss: 0.00001229
Iteration 41/1000 | Loss: 0.00001229
Iteration 42/1000 | Loss: 0.00001227
Iteration 43/1000 | Loss: 0.00001227
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001227
Iteration 47/1000 | Loss: 0.00001227
Iteration 48/1000 | Loss: 0.00001227
Iteration 49/1000 | Loss: 0.00001227
Iteration 50/1000 | Loss: 0.00001227
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001227
Iteration 53/1000 | Loss: 0.00001226
Iteration 54/1000 | Loss: 0.00001226
Iteration 55/1000 | Loss: 0.00001225
Iteration 56/1000 | Loss: 0.00001225
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001224
Iteration 59/1000 | Loss: 0.00001223
Iteration 60/1000 | Loss: 0.00001223
Iteration 61/1000 | Loss: 0.00001222
Iteration 62/1000 | Loss: 0.00001221
Iteration 63/1000 | Loss: 0.00001221
Iteration 64/1000 | Loss: 0.00001221
Iteration 65/1000 | Loss: 0.00001221
Iteration 66/1000 | Loss: 0.00001221
Iteration 67/1000 | Loss: 0.00001221
Iteration 68/1000 | Loss: 0.00001221
Iteration 69/1000 | Loss: 0.00001221
Iteration 70/1000 | Loss: 0.00001221
Iteration 71/1000 | Loss: 0.00001221
Iteration 72/1000 | Loss: 0.00001221
Iteration 73/1000 | Loss: 0.00001220
Iteration 74/1000 | Loss: 0.00001220
Iteration 75/1000 | Loss: 0.00001219
Iteration 76/1000 | Loss: 0.00001219
Iteration 77/1000 | Loss: 0.00001218
Iteration 78/1000 | Loss: 0.00001218
Iteration 79/1000 | Loss: 0.00001218
Iteration 80/1000 | Loss: 0.00001218
Iteration 81/1000 | Loss: 0.00001218
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001218
Iteration 85/1000 | Loss: 0.00001218
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001218
Iteration 89/1000 | Loss: 0.00001218
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001218
Iteration 92/1000 | Loss: 0.00001218
Iteration 93/1000 | Loss: 0.00001218
Iteration 94/1000 | Loss: 0.00001218
Iteration 95/1000 | Loss: 0.00001218
Iteration 96/1000 | Loss: 0.00001218
Iteration 97/1000 | Loss: 0.00001218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.2175000847491901e-05, 1.2175000847491901e-05, 1.2175000847491901e-05, 1.2175000847491901e-05, 1.2175000847491901e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2175000847491901e-05

Optimization complete. Final v2v error: 3.0257363319396973 mm

Highest mean error: 3.1124584674835205 mm for frame 161

Lowest mean error: 2.9448835849761963 mm for frame 47

Saving results

Total time: 39.46107530593872
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407940
Iteration 2/25 | Loss: 0.00130532
Iteration 3/25 | Loss: 0.00121178
Iteration 4/25 | Loss: 0.00120297
Iteration 5/25 | Loss: 0.00120079
Iteration 6/25 | Loss: 0.00120042
Iteration 7/25 | Loss: 0.00120042
Iteration 8/25 | Loss: 0.00120042
Iteration 9/25 | Loss: 0.00120042
Iteration 10/25 | Loss: 0.00120042
Iteration 11/25 | Loss: 0.00120042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012004171730950475, 0.0012004171730950475, 0.0012004171730950475, 0.0012004171730950475, 0.0012004171730950475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012004171730950475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46059918
Iteration 2/25 | Loss: 0.00141115
Iteration 3/25 | Loss: 0.00141114
Iteration 4/25 | Loss: 0.00141114
Iteration 5/25 | Loss: 0.00141114
Iteration 6/25 | Loss: 0.00141114
Iteration 7/25 | Loss: 0.00141114
Iteration 8/25 | Loss: 0.00141114
Iteration 9/25 | Loss: 0.00141114
Iteration 10/25 | Loss: 0.00141114
Iteration 11/25 | Loss: 0.00141114
Iteration 12/25 | Loss: 0.00141114
Iteration 13/25 | Loss: 0.00141113
Iteration 14/25 | Loss: 0.00141114
Iteration 15/25 | Loss: 0.00141113
Iteration 16/25 | Loss: 0.00141113
Iteration 17/25 | Loss: 0.00141113
Iteration 18/25 | Loss: 0.00141113
Iteration 19/25 | Loss: 0.00141113
Iteration 20/25 | Loss: 0.00141113
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014111349591985345, 0.0014111349591985345, 0.0014111349591985345, 0.0014111349591985345, 0.0014111349591985345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014111349591985345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141113
Iteration 2/1000 | Loss: 0.00002508
Iteration 3/1000 | Loss: 0.00001751
Iteration 4/1000 | Loss: 0.00001516
Iteration 5/1000 | Loss: 0.00001400
Iteration 6/1000 | Loss: 0.00001322
Iteration 7/1000 | Loss: 0.00001265
Iteration 8/1000 | Loss: 0.00001221
Iteration 9/1000 | Loss: 0.00001191
Iteration 10/1000 | Loss: 0.00001182
Iteration 11/1000 | Loss: 0.00001156
Iteration 12/1000 | Loss: 0.00001129
Iteration 13/1000 | Loss: 0.00001124
Iteration 14/1000 | Loss: 0.00001118
Iteration 15/1000 | Loss: 0.00001116
Iteration 16/1000 | Loss: 0.00001115
Iteration 17/1000 | Loss: 0.00001114
Iteration 18/1000 | Loss: 0.00001110
Iteration 19/1000 | Loss: 0.00001106
Iteration 20/1000 | Loss: 0.00001103
Iteration 21/1000 | Loss: 0.00001102
Iteration 22/1000 | Loss: 0.00001096
Iteration 23/1000 | Loss: 0.00001092
Iteration 24/1000 | Loss: 0.00001092
Iteration 25/1000 | Loss: 0.00001090
Iteration 26/1000 | Loss: 0.00001089
Iteration 27/1000 | Loss: 0.00001089
Iteration 28/1000 | Loss: 0.00001088
Iteration 29/1000 | Loss: 0.00001088
Iteration 30/1000 | Loss: 0.00001088
Iteration 31/1000 | Loss: 0.00001087
Iteration 32/1000 | Loss: 0.00001087
Iteration 33/1000 | Loss: 0.00001086
Iteration 34/1000 | Loss: 0.00001086
Iteration 35/1000 | Loss: 0.00001085
Iteration 36/1000 | Loss: 0.00001085
Iteration 37/1000 | Loss: 0.00001085
Iteration 38/1000 | Loss: 0.00001084
Iteration 39/1000 | Loss: 0.00001082
Iteration 40/1000 | Loss: 0.00001082
Iteration 41/1000 | Loss: 0.00001081
Iteration 42/1000 | Loss: 0.00001080
Iteration 43/1000 | Loss: 0.00001079
Iteration 44/1000 | Loss: 0.00001078
Iteration 45/1000 | Loss: 0.00001078
Iteration 46/1000 | Loss: 0.00001077
Iteration 47/1000 | Loss: 0.00001077
Iteration 48/1000 | Loss: 0.00001077
Iteration 49/1000 | Loss: 0.00001076
Iteration 50/1000 | Loss: 0.00001076
Iteration 51/1000 | Loss: 0.00001075
Iteration 52/1000 | Loss: 0.00001075
Iteration 53/1000 | Loss: 0.00001074
Iteration 54/1000 | Loss: 0.00001074
Iteration 55/1000 | Loss: 0.00001074
Iteration 56/1000 | Loss: 0.00001073
Iteration 57/1000 | Loss: 0.00001073
Iteration 58/1000 | Loss: 0.00001072
Iteration 59/1000 | Loss: 0.00001072
Iteration 60/1000 | Loss: 0.00001071
Iteration 61/1000 | Loss: 0.00001067
Iteration 62/1000 | Loss: 0.00001064
Iteration 63/1000 | Loss: 0.00001063
Iteration 64/1000 | Loss: 0.00001062
Iteration 65/1000 | Loss: 0.00001062
Iteration 66/1000 | Loss: 0.00001062
Iteration 67/1000 | Loss: 0.00001062
Iteration 68/1000 | Loss: 0.00001062
Iteration 69/1000 | Loss: 0.00001061
Iteration 70/1000 | Loss: 0.00001060
Iteration 71/1000 | Loss: 0.00001060
Iteration 72/1000 | Loss: 0.00001060
Iteration 73/1000 | Loss: 0.00001059
Iteration 74/1000 | Loss: 0.00001059
Iteration 75/1000 | Loss: 0.00001059
Iteration 76/1000 | Loss: 0.00001058
Iteration 77/1000 | Loss: 0.00001057
Iteration 78/1000 | Loss: 0.00001057
Iteration 79/1000 | Loss: 0.00001057
Iteration 80/1000 | Loss: 0.00001057
Iteration 81/1000 | Loss: 0.00001057
Iteration 82/1000 | Loss: 0.00001056
Iteration 83/1000 | Loss: 0.00001056
Iteration 84/1000 | Loss: 0.00001055
Iteration 85/1000 | Loss: 0.00001055
Iteration 86/1000 | Loss: 0.00001054
Iteration 87/1000 | Loss: 0.00001054
Iteration 88/1000 | Loss: 0.00001054
Iteration 89/1000 | Loss: 0.00001054
Iteration 90/1000 | Loss: 0.00001053
Iteration 91/1000 | Loss: 0.00001053
Iteration 92/1000 | Loss: 0.00001053
Iteration 93/1000 | Loss: 0.00001053
Iteration 94/1000 | Loss: 0.00001053
Iteration 95/1000 | Loss: 0.00001053
Iteration 96/1000 | Loss: 0.00001053
Iteration 97/1000 | Loss: 0.00001053
Iteration 98/1000 | Loss: 0.00001052
Iteration 99/1000 | Loss: 0.00001052
Iteration 100/1000 | Loss: 0.00001052
Iteration 101/1000 | Loss: 0.00001052
Iteration 102/1000 | Loss: 0.00001052
Iteration 103/1000 | Loss: 0.00001051
Iteration 104/1000 | Loss: 0.00001051
Iteration 105/1000 | Loss: 0.00001051
Iteration 106/1000 | Loss: 0.00001051
Iteration 107/1000 | Loss: 0.00001051
Iteration 108/1000 | Loss: 0.00001051
Iteration 109/1000 | Loss: 0.00001051
Iteration 110/1000 | Loss: 0.00001050
Iteration 111/1000 | Loss: 0.00001050
Iteration 112/1000 | Loss: 0.00001050
Iteration 113/1000 | Loss: 0.00001049
Iteration 114/1000 | Loss: 0.00001049
Iteration 115/1000 | Loss: 0.00001049
Iteration 116/1000 | Loss: 0.00001049
Iteration 117/1000 | Loss: 0.00001049
Iteration 118/1000 | Loss: 0.00001049
Iteration 119/1000 | Loss: 0.00001048
Iteration 120/1000 | Loss: 0.00001048
Iteration 121/1000 | Loss: 0.00001048
Iteration 122/1000 | Loss: 0.00001048
Iteration 123/1000 | Loss: 0.00001048
Iteration 124/1000 | Loss: 0.00001048
Iteration 125/1000 | Loss: 0.00001048
Iteration 126/1000 | Loss: 0.00001048
Iteration 127/1000 | Loss: 0.00001048
Iteration 128/1000 | Loss: 0.00001048
Iteration 129/1000 | Loss: 0.00001047
Iteration 130/1000 | Loss: 0.00001047
Iteration 131/1000 | Loss: 0.00001047
Iteration 132/1000 | Loss: 0.00001047
Iteration 133/1000 | Loss: 0.00001046
Iteration 134/1000 | Loss: 0.00001046
Iteration 135/1000 | Loss: 0.00001046
Iteration 136/1000 | Loss: 0.00001046
Iteration 137/1000 | Loss: 0.00001046
Iteration 138/1000 | Loss: 0.00001046
Iteration 139/1000 | Loss: 0.00001046
Iteration 140/1000 | Loss: 0.00001046
Iteration 141/1000 | Loss: 0.00001045
Iteration 142/1000 | Loss: 0.00001045
Iteration 143/1000 | Loss: 0.00001045
Iteration 144/1000 | Loss: 0.00001045
Iteration 145/1000 | Loss: 0.00001044
Iteration 146/1000 | Loss: 0.00001044
Iteration 147/1000 | Loss: 0.00001044
Iteration 148/1000 | Loss: 0.00001043
Iteration 149/1000 | Loss: 0.00001043
Iteration 150/1000 | Loss: 0.00001043
Iteration 151/1000 | Loss: 0.00001043
Iteration 152/1000 | Loss: 0.00001043
Iteration 153/1000 | Loss: 0.00001042
Iteration 154/1000 | Loss: 0.00001042
Iteration 155/1000 | Loss: 0.00001042
Iteration 156/1000 | Loss: 0.00001042
Iteration 157/1000 | Loss: 0.00001042
Iteration 158/1000 | Loss: 0.00001042
Iteration 159/1000 | Loss: 0.00001042
Iteration 160/1000 | Loss: 0.00001042
Iteration 161/1000 | Loss: 0.00001042
Iteration 162/1000 | Loss: 0.00001041
Iteration 163/1000 | Loss: 0.00001041
Iteration 164/1000 | Loss: 0.00001041
Iteration 165/1000 | Loss: 0.00001041
Iteration 166/1000 | Loss: 0.00001041
Iteration 167/1000 | Loss: 0.00001041
Iteration 168/1000 | Loss: 0.00001040
Iteration 169/1000 | Loss: 0.00001040
Iteration 170/1000 | Loss: 0.00001040
Iteration 171/1000 | Loss: 0.00001040
Iteration 172/1000 | Loss: 0.00001040
Iteration 173/1000 | Loss: 0.00001039
Iteration 174/1000 | Loss: 0.00001039
Iteration 175/1000 | Loss: 0.00001039
Iteration 176/1000 | Loss: 0.00001039
Iteration 177/1000 | Loss: 0.00001039
Iteration 178/1000 | Loss: 0.00001039
Iteration 179/1000 | Loss: 0.00001039
Iteration 180/1000 | Loss: 0.00001039
Iteration 181/1000 | Loss: 0.00001039
Iteration 182/1000 | Loss: 0.00001038
Iteration 183/1000 | Loss: 0.00001038
Iteration 184/1000 | Loss: 0.00001038
Iteration 185/1000 | Loss: 0.00001038
Iteration 186/1000 | Loss: 0.00001038
Iteration 187/1000 | Loss: 0.00001038
Iteration 188/1000 | Loss: 0.00001038
Iteration 189/1000 | Loss: 0.00001038
Iteration 190/1000 | Loss: 0.00001038
Iteration 191/1000 | Loss: 0.00001037
Iteration 192/1000 | Loss: 0.00001037
Iteration 193/1000 | Loss: 0.00001037
Iteration 194/1000 | Loss: 0.00001037
Iteration 195/1000 | Loss: 0.00001037
Iteration 196/1000 | Loss: 0.00001036
Iteration 197/1000 | Loss: 0.00001036
Iteration 198/1000 | Loss: 0.00001036
Iteration 199/1000 | Loss: 0.00001036
Iteration 200/1000 | Loss: 0.00001036
Iteration 201/1000 | Loss: 0.00001036
Iteration 202/1000 | Loss: 0.00001036
Iteration 203/1000 | Loss: 0.00001036
Iteration 204/1000 | Loss: 0.00001035
Iteration 205/1000 | Loss: 0.00001035
Iteration 206/1000 | Loss: 0.00001035
Iteration 207/1000 | Loss: 0.00001035
Iteration 208/1000 | Loss: 0.00001034
Iteration 209/1000 | Loss: 0.00001034
Iteration 210/1000 | Loss: 0.00001034
Iteration 211/1000 | Loss: 0.00001033
Iteration 212/1000 | Loss: 0.00001033
Iteration 213/1000 | Loss: 0.00001033
Iteration 214/1000 | Loss: 0.00001033
Iteration 215/1000 | Loss: 0.00001033
Iteration 216/1000 | Loss: 0.00001032
Iteration 217/1000 | Loss: 0.00001032
Iteration 218/1000 | Loss: 0.00001032
Iteration 219/1000 | Loss: 0.00001032
Iteration 220/1000 | Loss: 0.00001032
Iteration 221/1000 | Loss: 0.00001032
Iteration 222/1000 | Loss: 0.00001031
Iteration 223/1000 | Loss: 0.00001031
Iteration 224/1000 | Loss: 0.00001031
Iteration 225/1000 | Loss: 0.00001031
Iteration 226/1000 | Loss: 0.00001031
Iteration 227/1000 | Loss: 0.00001031
Iteration 228/1000 | Loss: 0.00001031
Iteration 229/1000 | Loss: 0.00001031
Iteration 230/1000 | Loss: 0.00001031
Iteration 231/1000 | Loss: 0.00001030
Iteration 232/1000 | Loss: 0.00001030
Iteration 233/1000 | Loss: 0.00001030
Iteration 234/1000 | Loss: 0.00001030
Iteration 235/1000 | Loss: 0.00001029
Iteration 236/1000 | Loss: 0.00001029
Iteration 237/1000 | Loss: 0.00001029
Iteration 238/1000 | Loss: 0.00001029
Iteration 239/1000 | Loss: 0.00001029
Iteration 240/1000 | Loss: 0.00001029
Iteration 241/1000 | Loss: 0.00001029
Iteration 242/1000 | Loss: 0.00001029
Iteration 243/1000 | Loss: 0.00001029
Iteration 244/1000 | Loss: 0.00001029
Iteration 245/1000 | Loss: 0.00001029
Iteration 246/1000 | Loss: 0.00001029
Iteration 247/1000 | Loss: 0.00001029
Iteration 248/1000 | Loss: 0.00001029
Iteration 249/1000 | Loss: 0.00001029
Iteration 250/1000 | Loss: 0.00001029
Iteration 251/1000 | Loss: 0.00001028
Iteration 252/1000 | Loss: 0.00001028
Iteration 253/1000 | Loss: 0.00001028
Iteration 254/1000 | Loss: 0.00001028
Iteration 255/1000 | Loss: 0.00001028
Iteration 256/1000 | Loss: 0.00001028
Iteration 257/1000 | Loss: 0.00001028
Iteration 258/1000 | Loss: 0.00001028
Iteration 259/1000 | Loss: 0.00001027
Iteration 260/1000 | Loss: 0.00001027
Iteration 261/1000 | Loss: 0.00001027
Iteration 262/1000 | Loss: 0.00001027
Iteration 263/1000 | Loss: 0.00001027
Iteration 264/1000 | Loss: 0.00001027
Iteration 265/1000 | Loss: 0.00001027
Iteration 266/1000 | Loss: 0.00001027
Iteration 267/1000 | Loss: 0.00001027
Iteration 268/1000 | Loss: 0.00001027
Iteration 269/1000 | Loss: 0.00001027
Iteration 270/1000 | Loss: 0.00001027
Iteration 271/1000 | Loss: 0.00001027
Iteration 272/1000 | Loss: 0.00001027
Iteration 273/1000 | Loss: 0.00001027
Iteration 274/1000 | Loss: 0.00001027
Iteration 275/1000 | Loss: 0.00001027
Iteration 276/1000 | Loss: 0.00001027
Iteration 277/1000 | Loss: 0.00001027
Iteration 278/1000 | Loss: 0.00001027
Iteration 279/1000 | Loss: 0.00001027
Iteration 280/1000 | Loss: 0.00001026
Iteration 281/1000 | Loss: 0.00001026
Iteration 282/1000 | Loss: 0.00001026
Iteration 283/1000 | Loss: 0.00001026
Iteration 284/1000 | Loss: 0.00001026
Iteration 285/1000 | Loss: 0.00001026
Iteration 286/1000 | Loss: 0.00001026
Iteration 287/1000 | Loss: 0.00001026
Iteration 288/1000 | Loss: 0.00001026
Iteration 289/1000 | Loss: 0.00001026
Iteration 290/1000 | Loss: 0.00001026
Iteration 291/1000 | Loss: 0.00001026
Iteration 292/1000 | Loss: 0.00001026
Iteration 293/1000 | Loss: 0.00001026
Iteration 294/1000 | Loss: 0.00001026
Iteration 295/1000 | Loss: 0.00001026
Iteration 296/1000 | Loss: 0.00001026
Iteration 297/1000 | Loss: 0.00001026
Iteration 298/1000 | Loss: 0.00001026
Iteration 299/1000 | Loss: 0.00001026
Iteration 300/1000 | Loss: 0.00001026
Iteration 301/1000 | Loss: 0.00001026
Iteration 302/1000 | Loss: 0.00001026
Iteration 303/1000 | Loss: 0.00001025
Iteration 304/1000 | Loss: 0.00001025
Iteration 305/1000 | Loss: 0.00001025
Iteration 306/1000 | Loss: 0.00001025
Iteration 307/1000 | Loss: 0.00001025
Iteration 308/1000 | Loss: 0.00001025
Iteration 309/1000 | Loss: 0.00001025
Iteration 310/1000 | Loss: 0.00001025
Iteration 311/1000 | Loss: 0.00001025
Iteration 312/1000 | Loss: 0.00001025
Iteration 313/1000 | Loss: 0.00001025
Iteration 314/1000 | Loss: 0.00001025
Iteration 315/1000 | Loss: 0.00001025
Iteration 316/1000 | Loss: 0.00001025
Iteration 317/1000 | Loss: 0.00001025
Iteration 318/1000 | Loss: 0.00001025
Iteration 319/1000 | Loss: 0.00001025
Iteration 320/1000 | Loss: 0.00001025
Iteration 321/1000 | Loss: 0.00001024
Iteration 322/1000 | Loss: 0.00001024
Iteration 323/1000 | Loss: 0.00001024
Iteration 324/1000 | Loss: 0.00001024
Iteration 325/1000 | Loss: 0.00001024
Iteration 326/1000 | Loss: 0.00001024
Iteration 327/1000 | Loss: 0.00001024
Iteration 328/1000 | Loss: 0.00001024
Iteration 329/1000 | Loss: 0.00001024
Iteration 330/1000 | Loss: 0.00001024
Iteration 331/1000 | Loss: 0.00001024
Iteration 332/1000 | Loss: 0.00001024
Iteration 333/1000 | Loss: 0.00001024
Iteration 334/1000 | Loss: 0.00001024
Iteration 335/1000 | Loss: 0.00001024
Iteration 336/1000 | Loss: 0.00001024
Iteration 337/1000 | Loss: 0.00001024
Iteration 338/1000 | Loss: 0.00001024
Iteration 339/1000 | Loss: 0.00001024
Iteration 340/1000 | Loss: 0.00001024
Iteration 341/1000 | Loss: 0.00001024
Iteration 342/1000 | Loss: 0.00001023
Iteration 343/1000 | Loss: 0.00001023
Iteration 344/1000 | Loss: 0.00001023
Iteration 345/1000 | Loss: 0.00001023
Iteration 346/1000 | Loss: 0.00001023
Iteration 347/1000 | Loss: 0.00001023
Iteration 348/1000 | Loss: 0.00001023
Iteration 349/1000 | Loss: 0.00001023
Iteration 350/1000 | Loss: 0.00001023
Iteration 351/1000 | Loss: 0.00001023
Iteration 352/1000 | Loss: 0.00001023
Iteration 353/1000 | Loss: 0.00001023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 353. Stopping optimization.
Last 5 losses: [1.0233165994577575e-05, 1.0233165994577575e-05, 1.0233165994577575e-05, 1.0233165994577575e-05, 1.0233165994577575e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0233165994577575e-05

Optimization complete. Final v2v error: 2.6985158920288086 mm

Highest mean error: 4.07339334487915 mm for frame 168

Lowest mean error: 2.400029182434082 mm for frame 148

Saving results

Total time: 59.338276863098145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00359532
Iteration 2/25 | Loss: 0.00127855
Iteration 3/25 | Loss: 0.00119608
Iteration 4/25 | Loss: 0.00118202
Iteration 5/25 | Loss: 0.00117745
Iteration 6/25 | Loss: 0.00117661
Iteration 7/25 | Loss: 0.00117661
Iteration 8/25 | Loss: 0.00117661
Iteration 9/25 | Loss: 0.00117661
Iteration 10/25 | Loss: 0.00117661
Iteration 11/25 | Loss: 0.00117661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011766116367653012, 0.0011766116367653012, 0.0011766116367653012, 0.0011766116367653012, 0.0011766116367653012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011766116367653012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33293939
Iteration 2/25 | Loss: 0.00173286
Iteration 3/25 | Loss: 0.00173286
Iteration 4/25 | Loss: 0.00173286
Iteration 5/25 | Loss: 0.00173285
Iteration 6/25 | Loss: 0.00173285
Iteration 7/25 | Loss: 0.00173285
Iteration 8/25 | Loss: 0.00173285
Iteration 9/25 | Loss: 0.00173285
Iteration 10/25 | Loss: 0.00173285
Iteration 11/25 | Loss: 0.00173285
Iteration 12/25 | Loss: 0.00173285
Iteration 13/25 | Loss: 0.00173285
Iteration 14/25 | Loss: 0.00173285
Iteration 15/25 | Loss: 0.00173285
Iteration 16/25 | Loss: 0.00173285
Iteration 17/25 | Loss: 0.00173285
Iteration 18/25 | Loss: 0.00173285
Iteration 19/25 | Loss: 0.00173285
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0017328530084341764, 0.0017328530084341764, 0.0017328530084341764, 0.0017328530084341764, 0.0017328530084341764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017328530084341764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173285
Iteration 2/1000 | Loss: 0.00002092
Iteration 3/1000 | Loss: 0.00001369
Iteration 4/1000 | Loss: 0.00001243
Iteration 5/1000 | Loss: 0.00001178
Iteration 6/1000 | Loss: 0.00001134
Iteration 7/1000 | Loss: 0.00001095
Iteration 8/1000 | Loss: 0.00001073
Iteration 9/1000 | Loss: 0.00001052
Iteration 10/1000 | Loss: 0.00001039
Iteration 11/1000 | Loss: 0.00001033
Iteration 12/1000 | Loss: 0.00001032
Iteration 13/1000 | Loss: 0.00001032
Iteration 14/1000 | Loss: 0.00001031
Iteration 15/1000 | Loss: 0.00001029
Iteration 16/1000 | Loss: 0.00001012
Iteration 17/1000 | Loss: 0.00001008
Iteration 18/1000 | Loss: 0.00001007
Iteration 19/1000 | Loss: 0.00001005
Iteration 20/1000 | Loss: 0.00001004
Iteration 21/1000 | Loss: 0.00001004
Iteration 22/1000 | Loss: 0.00001003
Iteration 23/1000 | Loss: 0.00001002
Iteration 24/1000 | Loss: 0.00001000
Iteration 25/1000 | Loss: 0.00000993
Iteration 26/1000 | Loss: 0.00000990
Iteration 27/1000 | Loss: 0.00000988
Iteration 28/1000 | Loss: 0.00000988
Iteration 29/1000 | Loss: 0.00000988
Iteration 30/1000 | Loss: 0.00000988
Iteration 31/1000 | Loss: 0.00000987
Iteration 32/1000 | Loss: 0.00000987
Iteration 33/1000 | Loss: 0.00000986
Iteration 34/1000 | Loss: 0.00000985
Iteration 35/1000 | Loss: 0.00000982
Iteration 36/1000 | Loss: 0.00000982
Iteration 37/1000 | Loss: 0.00000981
Iteration 38/1000 | Loss: 0.00000981
Iteration 39/1000 | Loss: 0.00000980
Iteration 40/1000 | Loss: 0.00000980
Iteration 41/1000 | Loss: 0.00000979
Iteration 42/1000 | Loss: 0.00000979
Iteration 43/1000 | Loss: 0.00000978
Iteration 44/1000 | Loss: 0.00000977
Iteration 45/1000 | Loss: 0.00000977
Iteration 46/1000 | Loss: 0.00000976
Iteration 47/1000 | Loss: 0.00000976
Iteration 48/1000 | Loss: 0.00000976
Iteration 49/1000 | Loss: 0.00000976
Iteration 50/1000 | Loss: 0.00000975
Iteration 51/1000 | Loss: 0.00000975
Iteration 52/1000 | Loss: 0.00000974
Iteration 53/1000 | Loss: 0.00000974
Iteration 54/1000 | Loss: 0.00000974
Iteration 55/1000 | Loss: 0.00000974
Iteration 56/1000 | Loss: 0.00000973
Iteration 57/1000 | Loss: 0.00000973
Iteration 58/1000 | Loss: 0.00000972
Iteration 59/1000 | Loss: 0.00000971
Iteration 60/1000 | Loss: 0.00000971
Iteration 61/1000 | Loss: 0.00000971
Iteration 62/1000 | Loss: 0.00000971
Iteration 63/1000 | Loss: 0.00000970
Iteration 64/1000 | Loss: 0.00000970
Iteration 65/1000 | Loss: 0.00000969
Iteration 66/1000 | Loss: 0.00000969
Iteration 67/1000 | Loss: 0.00000969
Iteration 68/1000 | Loss: 0.00000969
Iteration 69/1000 | Loss: 0.00000967
Iteration 70/1000 | Loss: 0.00000967
Iteration 71/1000 | Loss: 0.00000967
Iteration 72/1000 | Loss: 0.00000967
Iteration 73/1000 | Loss: 0.00000967
Iteration 74/1000 | Loss: 0.00000966
Iteration 75/1000 | Loss: 0.00000966
Iteration 76/1000 | Loss: 0.00000966
Iteration 77/1000 | Loss: 0.00000966
Iteration 78/1000 | Loss: 0.00000966
Iteration 79/1000 | Loss: 0.00000966
Iteration 80/1000 | Loss: 0.00000966
Iteration 81/1000 | Loss: 0.00000966
Iteration 82/1000 | Loss: 0.00000966
Iteration 83/1000 | Loss: 0.00000965
Iteration 84/1000 | Loss: 0.00000965
Iteration 85/1000 | Loss: 0.00000964
Iteration 86/1000 | Loss: 0.00000964
Iteration 87/1000 | Loss: 0.00000964
Iteration 88/1000 | Loss: 0.00000964
Iteration 89/1000 | Loss: 0.00000963
Iteration 90/1000 | Loss: 0.00000963
Iteration 91/1000 | Loss: 0.00000962
Iteration 92/1000 | Loss: 0.00000962
Iteration 93/1000 | Loss: 0.00000962
Iteration 94/1000 | Loss: 0.00000962
Iteration 95/1000 | Loss: 0.00000962
Iteration 96/1000 | Loss: 0.00000962
Iteration 97/1000 | Loss: 0.00000962
Iteration 98/1000 | Loss: 0.00000962
Iteration 99/1000 | Loss: 0.00000962
Iteration 100/1000 | Loss: 0.00000961
Iteration 101/1000 | Loss: 0.00000961
Iteration 102/1000 | Loss: 0.00000961
Iteration 103/1000 | Loss: 0.00000961
Iteration 104/1000 | Loss: 0.00000960
Iteration 105/1000 | Loss: 0.00000960
Iteration 106/1000 | Loss: 0.00000960
Iteration 107/1000 | Loss: 0.00000960
Iteration 108/1000 | Loss: 0.00000960
Iteration 109/1000 | Loss: 0.00000960
Iteration 110/1000 | Loss: 0.00000960
Iteration 111/1000 | Loss: 0.00000959
Iteration 112/1000 | Loss: 0.00000959
Iteration 113/1000 | Loss: 0.00000959
Iteration 114/1000 | Loss: 0.00000959
Iteration 115/1000 | Loss: 0.00000959
Iteration 116/1000 | Loss: 0.00000959
Iteration 117/1000 | Loss: 0.00000959
Iteration 118/1000 | Loss: 0.00000959
Iteration 119/1000 | Loss: 0.00000959
Iteration 120/1000 | Loss: 0.00000958
Iteration 121/1000 | Loss: 0.00000958
Iteration 122/1000 | Loss: 0.00000958
Iteration 123/1000 | Loss: 0.00000958
Iteration 124/1000 | Loss: 0.00000958
Iteration 125/1000 | Loss: 0.00000958
Iteration 126/1000 | Loss: 0.00000958
Iteration 127/1000 | Loss: 0.00000958
Iteration 128/1000 | Loss: 0.00000958
Iteration 129/1000 | Loss: 0.00000958
Iteration 130/1000 | Loss: 0.00000958
Iteration 131/1000 | Loss: 0.00000958
Iteration 132/1000 | Loss: 0.00000957
Iteration 133/1000 | Loss: 0.00000957
Iteration 134/1000 | Loss: 0.00000957
Iteration 135/1000 | Loss: 0.00000957
Iteration 136/1000 | Loss: 0.00000957
Iteration 137/1000 | Loss: 0.00000957
Iteration 138/1000 | Loss: 0.00000957
Iteration 139/1000 | Loss: 0.00000957
Iteration 140/1000 | Loss: 0.00000957
Iteration 141/1000 | Loss: 0.00000957
Iteration 142/1000 | Loss: 0.00000956
Iteration 143/1000 | Loss: 0.00000956
Iteration 144/1000 | Loss: 0.00000956
Iteration 145/1000 | Loss: 0.00000956
Iteration 146/1000 | Loss: 0.00000955
Iteration 147/1000 | Loss: 0.00000955
Iteration 148/1000 | Loss: 0.00000955
Iteration 149/1000 | Loss: 0.00000955
Iteration 150/1000 | Loss: 0.00000955
Iteration 151/1000 | Loss: 0.00000955
Iteration 152/1000 | Loss: 0.00000955
Iteration 153/1000 | Loss: 0.00000955
Iteration 154/1000 | Loss: 0.00000955
Iteration 155/1000 | Loss: 0.00000955
Iteration 156/1000 | Loss: 0.00000954
Iteration 157/1000 | Loss: 0.00000954
Iteration 158/1000 | Loss: 0.00000954
Iteration 159/1000 | Loss: 0.00000954
Iteration 160/1000 | Loss: 0.00000954
Iteration 161/1000 | Loss: 0.00000954
Iteration 162/1000 | Loss: 0.00000954
Iteration 163/1000 | Loss: 0.00000954
Iteration 164/1000 | Loss: 0.00000954
Iteration 165/1000 | Loss: 0.00000953
Iteration 166/1000 | Loss: 0.00000953
Iteration 167/1000 | Loss: 0.00000953
Iteration 168/1000 | Loss: 0.00000953
Iteration 169/1000 | Loss: 0.00000953
Iteration 170/1000 | Loss: 0.00000953
Iteration 171/1000 | Loss: 0.00000953
Iteration 172/1000 | Loss: 0.00000953
Iteration 173/1000 | Loss: 0.00000953
Iteration 174/1000 | Loss: 0.00000953
Iteration 175/1000 | Loss: 0.00000953
Iteration 176/1000 | Loss: 0.00000953
Iteration 177/1000 | Loss: 0.00000953
Iteration 178/1000 | Loss: 0.00000953
Iteration 179/1000 | Loss: 0.00000953
Iteration 180/1000 | Loss: 0.00000952
Iteration 181/1000 | Loss: 0.00000952
Iteration 182/1000 | Loss: 0.00000952
Iteration 183/1000 | Loss: 0.00000952
Iteration 184/1000 | Loss: 0.00000952
Iteration 185/1000 | Loss: 0.00000952
Iteration 186/1000 | Loss: 0.00000952
Iteration 187/1000 | Loss: 0.00000951
Iteration 188/1000 | Loss: 0.00000951
Iteration 189/1000 | Loss: 0.00000951
Iteration 190/1000 | Loss: 0.00000951
Iteration 191/1000 | Loss: 0.00000951
Iteration 192/1000 | Loss: 0.00000951
Iteration 193/1000 | Loss: 0.00000951
Iteration 194/1000 | Loss: 0.00000951
Iteration 195/1000 | Loss: 0.00000951
Iteration 196/1000 | Loss: 0.00000951
Iteration 197/1000 | Loss: 0.00000951
Iteration 198/1000 | Loss: 0.00000951
Iteration 199/1000 | Loss: 0.00000951
Iteration 200/1000 | Loss: 0.00000950
Iteration 201/1000 | Loss: 0.00000950
Iteration 202/1000 | Loss: 0.00000950
Iteration 203/1000 | Loss: 0.00000950
Iteration 204/1000 | Loss: 0.00000950
Iteration 205/1000 | Loss: 0.00000950
Iteration 206/1000 | Loss: 0.00000950
Iteration 207/1000 | Loss: 0.00000950
Iteration 208/1000 | Loss: 0.00000949
Iteration 209/1000 | Loss: 0.00000949
Iteration 210/1000 | Loss: 0.00000949
Iteration 211/1000 | Loss: 0.00000949
Iteration 212/1000 | Loss: 0.00000949
Iteration 213/1000 | Loss: 0.00000949
Iteration 214/1000 | Loss: 0.00000949
Iteration 215/1000 | Loss: 0.00000949
Iteration 216/1000 | Loss: 0.00000949
Iteration 217/1000 | Loss: 0.00000949
Iteration 218/1000 | Loss: 0.00000949
Iteration 219/1000 | Loss: 0.00000949
Iteration 220/1000 | Loss: 0.00000949
Iteration 221/1000 | Loss: 0.00000949
Iteration 222/1000 | Loss: 0.00000949
Iteration 223/1000 | Loss: 0.00000949
Iteration 224/1000 | Loss: 0.00000949
Iteration 225/1000 | Loss: 0.00000949
Iteration 226/1000 | Loss: 0.00000949
Iteration 227/1000 | Loss: 0.00000949
Iteration 228/1000 | Loss: 0.00000949
Iteration 229/1000 | Loss: 0.00000949
Iteration 230/1000 | Loss: 0.00000949
Iteration 231/1000 | Loss: 0.00000949
Iteration 232/1000 | Loss: 0.00000949
Iteration 233/1000 | Loss: 0.00000949
Iteration 234/1000 | Loss: 0.00000949
Iteration 235/1000 | Loss: 0.00000949
Iteration 236/1000 | Loss: 0.00000949
Iteration 237/1000 | Loss: 0.00000949
Iteration 238/1000 | Loss: 0.00000949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [9.486573617323302e-06, 9.486573617323302e-06, 9.486573617323302e-06, 9.486573617323302e-06, 9.486573617323302e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.486573617323302e-06

Optimization complete. Final v2v error: 2.684750556945801 mm

Highest mean error: 2.947889566421509 mm for frame 34

Lowest mean error: 2.4156906604766846 mm for frame 134

Saving results

Total time: 42.43548083305359
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895480
Iteration 2/25 | Loss: 0.00152626
Iteration 3/25 | Loss: 0.00133312
Iteration 4/25 | Loss: 0.00131075
Iteration 5/25 | Loss: 0.00130496
Iteration 6/25 | Loss: 0.00130364
Iteration 7/25 | Loss: 0.00130364
Iteration 8/25 | Loss: 0.00130364
Iteration 9/25 | Loss: 0.00130364
Iteration 10/25 | Loss: 0.00130364
Iteration 11/25 | Loss: 0.00130364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013036435702815652, 0.0013036435702815652, 0.0013036435702815652, 0.0013036435702815652, 0.0013036435702815652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013036435702815652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04619670
Iteration 2/25 | Loss: 0.00120367
Iteration 3/25 | Loss: 0.00120366
Iteration 4/25 | Loss: 0.00120366
Iteration 5/25 | Loss: 0.00120366
Iteration 6/25 | Loss: 0.00120366
Iteration 7/25 | Loss: 0.00120366
Iteration 8/25 | Loss: 0.00120366
Iteration 9/25 | Loss: 0.00120366
Iteration 10/25 | Loss: 0.00120366
Iteration 11/25 | Loss: 0.00120366
Iteration 12/25 | Loss: 0.00120366
Iteration 13/25 | Loss: 0.00120366
Iteration 14/25 | Loss: 0.00120366
Iteration 15/25 | Loss: 0.00120366
Iteration 16/25 | Loss: 0.00120366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012036554981023073, 0.0012036554981023073, 0.0012036554981023073, 0.0012036554981023073, 0.0012036554981023073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012036554981023073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120366
Iteration 2/1000 | Loss: 0.00004509
Iteration 3/1000 | Loss: 0.00003236
Iteration 4/1000 | Loss: 0.00002768
Iteration 5/1000 | Loss: 0.00002606
Iteration 6/1000 | Loss: 0.00002520
Iteration 7/1000 | Loss: 0.00002445
Iteration 8/1000 | Loss: 0.00002392
Iteration 9/1000 | Loss: 0.00002352
Iteration 10/1000 | Loss: 0.00002316
Iteration 11/1000 | Loss: 0.00002289
Iteration 12/1000 | Loss: 0.00002263
Iteration 13/1000 | Loss: 0.00002240
Iteration 14/1000 | Loss: 0.00002219
Iteration 15/1000 | Loss: 0.00002197
Iteration 16/1000 | Loss: 0.00002180
Iteration 17/1000 | Loss: 0.00002165
Iteration 18/1000 | Loss: 0.00002154
Iteration 19/1000 | Loss: 0.00002150
Iteration 20/1000 | Loss: 0.00002150
Iteration 21/1000 | Loss: 0.00002143
Iteration 22/1000 | Loss: 0.00002141
Iteration 23/1000 | Loss: 0.00002134
Iteration 24/1000 | Loss: 0.00002134
Iteration 25/1000 | Loss: 0.00002131
Iteration 26/1000 | Loss: 0.00002130
Iteration 27/1000 | Loss: 0.00002126
Iteration 28/1000 | Loss: 0.00002126
Iteration 29/1000 | Loss: 0.00002123
Iteration 30/1000 | Loss: 0.00002123
Iteration 31/1000 | Loss: 0.00002122
Iteration 32/1000 | Loss: 0.00002122
Iteration 33/1000 | Loss: 0.00002122
Iteration 34/1000 | Loss: 0.00002122
Iteration 35/1000 | Loss: 0.00002120
Iteration 36/1000 | Loss: 0.00002120
Iteration 37/1000 | Loss: 0.00002120
Iteration 38/1000 | Loss: 0.00002120
Iteration 39/1000 | Loss: 0.00002120
Iteration 40/1000 | Loss: 0.00002119
Iteration 41/1000 | Loss: 0.00002119
Iteration 42/1000 | Loss: 0.00002119
Iteration 43/1000 | Loss: 0.00002119
Iteration 44/1000 | Loss: 0.00002118
Iteration 45/1000 | Loss: 0.00002118
Iteration 46/1000 | Loss: 0.00002118
Iteration 47/1000 | Loss: 0.00002117
Iteration 48/1000 | Loss: 0.00002117
Iteration 49/1000 | Loss: 0.00002117
Iteration 50/1000 | Loss: 0.00002117
Iteration 51/1000 | Loss: 0.00002116
Iteration 52/1000 | Loss: 0.00002116
Iteration 53/1000 | Loss: 0.00002116
Iteration 54/1000 | Loss: 0.00002116
Iteration 55/1000 | Loss: 0.00002115
Iteration 56/1000 | Loss: 0.00002115
Iteration 57/1000 | Loss: 0.00002115
Iteration 58/1000 | Loss: 0.00002115
Iteration 59/1000 | Loss: 0.00002115
Iteration 60/1000 | Loss: 0.00002114
Iteration 61/1000 | Loss: 0.00002114
Iteration 62/1000 | Loss: 0.00002114
Iteration 63/1000 | Loss: 0.00002114
Iteration 64/1000 | Loss: 0.00002114
Iteration 65/1000 | Loss: 0.00002114
Iteration 66/1000 | Loss: 0.00002114
Iteration 67/1000 | Loss: 0.00002114
Iteration 68/1000 | Loss: 0.00002113
Iteration 69/1000 | Loss: 0.00002113
Iteration 70/1000 | Loss: 0.00002113
Iteration 71/1000 | Loss: 0.00002113
Iteration 72/1000 | Loss: 0.00002113
Iteration 73/1000 | Loss: 0.00002112
Iteration 74/1000 | Loss: 0.00002112
Iteration 75/1000 | Loss: 0.00002112
Iteration 76/1000 | Loss: 0.00002112
Iteration 77/1000 | Loss: 0.00002112
Iteration 78/1000 | Loss: 0.00002112
Iteration 79/1000 | Loss: 0.00002112
Iteration 80/1000 | Loss: 0.00002112
Iteration 81/1000 | Loss: 0.00002112
Iteration 82/1000 | Loss: 0.00002112
Iteration 83/1000 | Loss: 0.00002112
Iteration 84/1000 | Loss: 0.00002112
Iteration 85/1000 | Loss: 0.00002112
Iteration 86/1000 | Loss: 0.00002112
Iteration 87/1000 | Loss: 0.00002112
Iteration 88/1000 | Loss: 0.00002112
Iteration 89/1000 | Loss: 0.00002112
Iteration 90/1000 | Loss: 0.00002112
Iteration 91/1000 | Loss: 0.00002111
Iteration 92/1000 | Loss: 0.00002111
Iteration 93/1000 | Loss: 0.00002111
Iteration 94/1000 | Loss: 0.00002111
Iteration 95/1000 | Loss: 0.00002111
Iteration 96/1000 | Loss: 0.00002111
Iteration 97/1000 | Loss: 0.00002111
Iteration 98/1000 | Loss: 0.00002111
Iteration 99/1000 | Loss: 0.00002111
Iteration 100/1000 | Loss: 0.00002111
Iteration 101/1000 | Loss: 0.00002111
Iteration 102/1000 | Loss: 0.00002111
Iteration 103/1000 | Loss: 0.00002111
Iteration 104/1000 | Loss: 0.00002111
Iteration 105/1000 | Loss: 0.00002111
Iteration 106/1000 | Loss: 0.00002110
Iteration 107/1000 | Loss: 0.00002110
Iteration 108/1000 | Loss: 0.00002110
Iteration 109/1000 | Loss: 0.00002110
Iteration 110/1000 | Loss: 0.00002110
Iteration 111/1000 | Loss: 0.00002110
Iteration 112/1000 | Loss: 0.00002110
Iteration 113/1000 | Loss: 0.00002110
Iteration 114/1000 | Loss: 0.00002110
Iteration 115/1000 | Loss: 0.00002110
Iteration 116/1000 | Loss: 0.00002110
Iteration 117/1000 | Loss: 0.00002110
Iteration 118/1000 | Loss: 0.00002110
Iteration 119/1000 | Loss: 0.00002110
Iteration 120/1000 | Loss: 0.00002110
Iteration 121/1000 | Loss: 0.00002110
Iteration 122/1000 | Loss: 0.00002110
Iteration 123/1000 | Loss: 0.00002110
Iteration 124/1000 | Loss: 0.00002109
Iteration 125/1000 | Loss: 0.00002109
Iteration 126/1000 | Loss: 0.00002109
Iteration 127/1000 | Loss: 0.00002109
Iteration 128/1000 | Loss: 0.00002109
Iteration 129/1000 | Loss: 0.00002109
Iteration 130/1000 | Loss: 0.00002109
Iteration 131/1000 | Loss: 0.00002109
Iteration 132/1000 | Loss: 0.00002109
Iteration 133/1000 | Loss: 0.00002109
Iteration 134/1000 | Loss: 0.00002109
Iteration 135/1000 | Loss: 0.00002109
Iteration 136/1000 | Loss: 0.00002109
Iteration 137/1000 | Loss: 0.00002109
Iteration 138/1000 | Loss: 0.00002109
Iteration 139/1000 | Loss: 0.00002109
Iteration 140/1000 | Loss: 0.00002109
Iteration 141/1000 | Loss: 0.00002109
Iteration 142/1000 | Loss: 0.00002109
Iteration 143/1000 | Loss: 0.00002109
Iteration 144/1000 | Loss: 0.00002109
Iteration 145/1000 | Loss: 0.00002109
Iteration 146/1000 | Loss: 0.00002109
Iteration 147/1000 | Loss: 0.00002109
Iteration 148/1000 | Loss: 0.00002109
Iteration 149/1000 | Loss: 0.00002109
Iteration 150/1000 | Loss: 0.00002109
Iteration 151/1000 | Loss: 0.00002109
Iteration 152/1000 | Loss: 0.00002109
Iteration 153/1000 | Loss: 0.00002109
Iteration 154/1000 | Loss: 0.00002109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.1089503206894733e-05, 2.1089503206894733e-05, 2.1089503206894733e-05, 2.1089503206894733e-05, 2.1089503206894733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1089503206894733e-05

Optimization complete. Final v2v error: 3.8082327842712402 mm

Highest mean error: 5.154456615447998 mm for frame 103

Lowest mean error: 3.0021533966064453 mm for frame 122

Saving results

Total time: 45.88010501861572
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979387
Iteration 2/25 | Loss: 0.00979386
Iteration 3/25 | Loss: 0.00979386
Iteration 4/25 | Loss: 0.00242004
Iteration 5/25 | Loss: 0.00150213
Iteration 6/25 | Loss: 0.00135262
Iteration 7/25 | Loss: 0.00133516
Iteration 8/25 | Loss: 0.00132019
Iteration 9/25 | Loss: 0.00130682
Iteration 10/25 | Loss: 0.00130496
Iteration 11/25 | Loss: 0.00130400
Iteration 12/25 | Loss: 0.00130291
Iteration 13/25 | Loss: 0.00130140
Iteration 14/25 | Loss: 0.00130118
Iteration 15/25 | Loss: 0.00130117
Iteration 16/25 | Loss: 0.00130116
Iteration 17/25 | Loss: 0.00130116
Iteration 18/25 | Loss: 0.00130116
Iteration 19/25 | Loss: 0.00130116
Iteration 20/25 | Loss: 0.00130116
Iteration 21/25 | Loss: 0.00130116
Iteration 22/25 | Loss: 0.00130116
Iteration 23/25 | Loss: 0.00130116
Iteration 24/25 | Loss: 0.00130116
Iteration 25/25 | Loss: 0.00130116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26733530
Iteration 2/25 | Loss: 0.00140815
Iteration 3/25 | Loss: 0.00140815
Iteration 4/25 | Loss: 0.00140815
Iteration 5/25 | Loss: 0.00140815
Iteration 6/25 | Loss: 0.00140815
Iteration 7/25 | Loss: 0.00140815
Iteration 8/25 | Loss: 0.00140814
Iteration 9/25 | Loss: 0.00140814
Iteration 10/25 | Loss: 0.00140814
Iteration 11/25 | Loss: 0.00140814
Iteration 12/25 | Loss: 0.00140814
Iteration 13/25 | Loss: 0.00140814
Iteration 14/25 | Loss: 0.00140814
Iteration 15/25 | Loss: 0.00140814
Iteration 16/25 | Loss: 0.00140814
Iteration 17/25 | Loss: 0.00140814
Iteration 18/25 | Loss: 0.00140814
Iteration 19/25 | Loss: 0.00140814
Iteration 20/25 | Loss: 0.00140814
Iteration 21/25 | Loss: 0.00140814
Iteration 22/25 | Loss: 0.00140814
Iteration 23/25 | Loss: 0.00140814
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014081430854275823, 0.0014081430854275823, 0.0014081430854275823, 0.0014081430854275823, 0.0014081430854275823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014081430854275823

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140814
Iteration 2/1000 | Loss: 0.00003683
Iteration 3/1000 | Loss: 0.00002559
Iteration 4/1000 | Loss: 0.00002436
Iteration 5/1000 | Loss: 0.00002327
Iteration 6/1000 | Loss: 0.00002268
Iteration 7/1000 | Loss: 0.00002227
Iteration 8/1000 | Loss: 0.00002197
Iteration 9/1000 | Loss: 0.00002164
Iteration 10/1000 | Loss: 0.00002163
Iteration 11/1000 | Loss: 0.00002145
Iteration 12/1000 | Loss: 0.00002128
Iteration 13/1000 | Loss: 0.00002124
Iteration 14/1000 | Loss: 0.00002109
Iteration 15/1000 | Loss: 0.00002107
Iteration 16/1000 | Loss: 0.00002107
Iteration 17/1000 | Loss: 0.00002106
Iteration 18/1000 | Loss: 0.00002106
Iteration 19/1000 | Loss: 0.00002100
Iteration 20/1000 | Loss: 0.00002080
Iteration 21/1000 | Loss: 0.00002077
Iteration 22/1000 | Loss: 0.00002075
Iteration 23/1000 | Loss: 0.00002074
Iteration 24/1000 | Loss: 0.00002074
Iteration 25/1000 | Loss: 0.00002074
Iteration 26/1000 | Loss: 0.00002073
Iteration 27/1000 | Loss: 0.00002072
Iteration 28/1000 | Loss: 0.00002072
Iteration 29/1000 | Loss: 0.00002071
Iteration 30/1000 | Loss: 0.00002071
Iteration 31/1000 | Loss: 0.00002070
Iteration 32/1000 | Loss: 0.00002070
Iteration 33/1000 | Loss: 0.00002070
Iteration 34/1000 | Loss: 0.00002069
Iteration 35/1000 | Loss: 0.00002069
Iteration 36/1000 | Loss: 0.00002069
Iteration 37/1000 | Loss: 0.00002069
Iteration 38/1000 | Loss: 0.00002069
Iteration 39/1000 | Loss: 0.00002069
Iteration 40/1000 | Loss: 0.00002069
Iteration 41/1000 | Loss: 0.00002069
Iteration 42/1000 | Loss: 0.00002069
Iteration 43/1000 | Loss: 0.00002068
Iteration 44/1000 | Loss: 0.00002065
Iteration 45/1000 | Loss: 0.00002065
Iteration 46/1000 | Loss: 0.00002065
Iteration 47/1000 | Loss: 0.00002064
Iteration 48/1000 | Loss: 0.00002064
Iteration 49/1000 | Loss: 0.00002064
Iteration 50/1000 | Loss: 0.00002064
Iteration 51/1000 | Loss: 0.00002063
Iteration 52/1000 | Loss: 0.00002062
Iteration 53/1000 | Loss: 0.00002061
Iteration 54/1000 | Loss: 0.00002060
Iteration 55/1000 | Loss: 0.00002059
Iteration 56/1000 | Loss: 0.00002059
Iteration 57/1000 | Loss: 0.00002055
Iteration 58/1000 | Loss: 0.00002055
Iteration 59/1000 | Loss: 0.00002055
Iteration 60/1000 | Loss: 0.00002054
Iteration 61/1000 | Loss: 0.00002052
Iteration 62/1000 | Loss: 0.00002052
Iteration 63/1000 | Loss: 0.00002052
Iteration 64/1000 | Loss: 0.00002052
Iteration 65/1000 | Loss: 0.00002052
Iteration 66/1000 | Loss: 0.00002052
Iteration 67/1000 | Loss: 0.00002052
Iteration 68/1000 | Loss: 0.00002051
Iteration 69/1000 | Loss: 0.00002050
Iteration 70/1000 | Loss: 0.00002050
Iteration 71/1000 | Loss: 0.00002049
Iteration 72/1000 | Loss: 0.00002049
Iteration 73/1000 | Loss: 0.00002049
Iteration 74/1000 | Loss: 0.00002047
Iteration 75/1000 | Loss: 0.00002046
Iteration 76/1000 | Loss: 0.00002046
Iteration 77/1000 | Loss: 0.00002045
Iteration 78/1000 | Loss: 0.00002045
Iteration 79/1000 | Loss: 0.00002045
Iteration 80/1000 | Loss: 0.00002045
Iteration 81/1000 | Loss: 0.00002045
Iteration 82/1000 | Loss: 0.00002044
Iteration 83/1000 | Loss: 0.00002044
Iteration 84/1000 | Loss: 0.00002044
Iteration 85/1000 | Loss: 0.00002044
Iteration 86/1000 | Loss: 0.00002044
Iteration 87/1000 | Loss: 0.00002044
Iteration 88/1000 | Loss: 0.00002043
Iteration 89/1000 | Loss: 0.00002043
Iteration 90/1000 | Loss: 0.00002043
Iteration 91/1000 | Loss: 0.00002043
Iteration 92/1000 | Loss: 0.00002043
Iteration 93/1000 | Loss: 0.00002043
Iteration 94/1000 | Loss: 0.00002043
Iteration 95/1000 | Loss: 0.00002043
Iteration 96/1000 | Loss: 0.00002043
Iteration 97/1000 | Loss: 0.00002043
Iteration 98/1000 | Loss: 0.00002043
Iteration 99/1000 | Loss: 0.00002043
Iteration 100/1000 | Loss: 0.00002043
Iteration 101/1000 | Loss: 0.00002043
Iteration 102/1000 | Loss: 0.00002043
Iteration 103/1000 | Loss: 0.00002042
Iteration 104/1000 | Loss: 0.00002042
Iteration 105/1000 | Loss: 0.00002042
Iteration 106/1000 | Loss: 0.00002042
Iteration 107/1000 | Loss: 0.00002042
Iteration 108/1000 | Loss: 0.00002042
Iteration 109/1000 | Loss: 0.00002042
Iteration 110/1000 | Loss: 0.00002042
Iteration 111/1000 | Loss: 0.00002042
Iteration 112/1000 | Loss: 0.00002041
Iteration 113/1000 | Loss: 0.00002041
Iteration 114/1000 | Loss: 0.00002041
Iteration 115/1000 | Loss: 0.00002041
Iteration 116/1000 | Loss: 0.00002041
Iteration 117/1000 | Loss: 0.00002041
Iteration 118/1000 | Loss: 0.00002041
Iteration 119/1000 | Loss: 0.00002041
Iteration 120/1000 | Loss: 0.00002041
Iteration 121/1000 | Loss: 0.00002041
Iteration 122/1000 | Loss: 0.00002041
Iteration 123/1000 | Loss: 0.00002041
Iteration 124/1000 | Loss: 0.00002041
Iteration 125/1000 | Loss: 0.00002041
Iteration 126/1000 | Loss: 0.00002041
Iteration 127/1000 | Loss: 0.00002041
Iteration 128/1000 | Loss: 0.00002041
Iteration 129/1000 | Loss: 0.00002041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.0412984667927958e-05, 2.0412984667927958e-05, 2.0412984667927958e-05, 2.0412984667927958e-05, 2.0412984667927958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0412984667927958e-05

Optimization complete. Final v2v error: 3.898871421813965 mm

Highest mean error: 3.940682888031006 mm for frame 39

Lowest mean error: 3.846409320831299 mm for frame 93

Saving results

Total time: 56.54459547996521
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420415
Iteration 2/25 | Loss: 0.00140092
Iteration 3/25 | Loss: 0.00128932
Iteration 4/25 | Loss: 0.00127258
Iteration 5/25 | Loss: 0.00126845
Iteration 6/25 | Loss: 0.00126717
Iteration 7/25 | Loss: 0.00126685
Iteration 8/25 | Loss: 0.00126685
Iteration 9/25 | Loss: 0.00126685
Iteration 10/25 | Loss: 0.00126685
Iteration 11/25 | Loss: 0.00126685
Iteration 12/25 | Loss: 0.00126685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012668515555560589, 0.0012668515555560589, 0.0012668515555560589, 0.0012668515555560589, 0.0012668515555560589]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012668515555560589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.81532049
Iteration 2/25 | Loss: 0.00131469
Iteration 3/25 | Loss: 0.00131467
Iteration 4/25 | Loss: 0.00131467
Iteration 5/25 | Loss: 0.00131467
Iteration 6/25 | Loss: 0.00131467
Iteration 7/25 | Loss: 0.00131467
Iteration 8/25 | Loss: 0.00131467
Iteration 9/25 | Loss: 0.00131467
Iteration 10/25 | Loss: 0.00131467
Iteration 11/25 | Loss: 0.00131467
Iteration 12/25 | Loss: 0.00131467
Iteration 13/25 | Loss: 0.00131467
Iteration 14/25 | Loss: 0.00131467
Iteration 15/25 | Loss: 0.00131467
Iteration 16/25 | Loss: 0.00131467
Iteration 17/25 | Loss: 0.00131467
Iteration 18/25 | Loss: 0.00131467
Iteration 19/25 | Loss: 0.00131467
Iteration 20/25 | Loss: 0.00131467
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013146683340892196, 0.0013146683340892196, 0.0013146683340892196, 0.0013146683340892196, 0.0013146683340892196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013146683340892196

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131467
Iteration 2/1000 | Loss: 0.00003332
Iteration 3/1000 | Loss: 0.00002423
Iteration 4/1000 | Loss: 0.00002248
Iteration 5/1000 | Loss: 0.00002160
Iteration 6/1000 | Loss: 0.00002102
Iteration 7/1000 | Loss: 0.00002038
Iteration 8/1000 | Loss: 0.00001997
Iteration 9/1000 | Loss: 0.00001958
Iteration 10/1000 | Loss: 0.00001925
Iteration 11/1000 | Loss: 0.00001907
Iteration 12/1000 | Loss: 0.00001890
Iteration 13/1000 | Loss: 0.00001873
Iteration 14/1000 | Loss: 0.00001872
Iteration 15/1000 | Loss: 0.00001871
Iteration 16/1000 | Loss: 0.00001865
Iteration 17/1000 | Loss: 0.00001859
Iteration 18/1000 | Loss: 0.00001858
Iteration 19/1000 | Loss: 0.00001851
Iteration 20/1000 | Loss: 0.00001851
Iteration 21/1000 | Loss: 0.00001849
Iteration 22/1000 | Loss: 0.00001847
Iteration 23/1000 | Loss: 0.00001842
Iteration 24/1000 | Loss: 0.00001842
Iteration 25/1000 | Loss: 0.00001842
Iteration 26/1000 | Loss: 0.00001841
Iteration 27/1000 | Loss: 0.00001840
Iteration 28/1000 | Loss: 0.00001840
Iteration 29/1000 | Loss: 0.00001840
Iteration 30/1000 | Loss: 0.00001840
Iteration 31/1000 | Loss: 0.00001840
Iteration 32/1000 | Loss: 0.00001840
Iteration 33/1000 | Loss: 0.00001839
Iteration 34/1000 | Loss: 0.00001839
Iteration 35/1000 | Loss: 0.00001839
Iteration 36/1000 | Loss: 0.00001839
Iteration 37/1000 | Loss: 0.00001839
Iteration 38/1000 | Loss: 0.00001838
Iteration 39/1000 | Loss: 0.00001838
Iteration 40/1000 | Loss: 0.00001837
Iteration 41/1000 | Loss: 0.00001837
Iteration 42/1000 | Loss: 0.00001837
Iteration 43/1000 | Loss: 0.00001836
Iteration 44/1000 | Loss: 0.00001836
Iteration 45/1000 | Loss: 0.00001835
Iteration 46/1000 | Loss: 0.00001835
Iteration 47/1000 | Loss: 0.00001835
Iteration 48/1000 | Loss: 0.00001835
Iteration 49/1000 | Loss: 0.00001835
Iteration 50/1000 | Loss: 0.00001835
Iteration 51/1000 | Loss: 0.00001835
Iteration 52/1000 | Loss: 0.00001835
Iteration 53/1000 | Loss: 0.00001835
Iteration 54/1000 | Loss: 0.00001835
Iteration 55/1000 | Loss: 0.00001834
Iteration 56/1000 | Loss: 0.00001834
Iteration 57/1000 | Loss: 0.00001834
Iteration 58/1000 | Loss: 0.00001834
Iteration 59/1000 | Loss: 0.00001833
Iteration 60/1000 | Loss: 0.00001833
Iteration 61/1000 | Loss: 0.00001833
Iteration 62/1000 | Loss: 0.00001833
Iteration 63/1000 | Loss: 0.00001833
Iteration 64/1000 | Loss: 0.00001832
Iteration 65/1000 | Loss: 0.00001832
Iteration 66/1000 | Loss: 0.00001832
Iteration 67/1000 | Loss: 0.00001832
Iteration 68/1000 | Loss: 0.00001832
Iteration 69/1000 | Loss: 0.00001832
Iteration 70/1000 | Loss: 0.00001832
Iteration 71/1000 | Loss: 0.00001831
Iteration 72/1000 | Loss: 0.00001831
Iteration 73/1000 | Loss: 0.00001831
Iteration 74/1000 | Loss: 0.00001830
Iteration 75/1000 | Loss: 0.00001830
Iteration 76/1000 | Loss: 0.00001830
Iteration 77/1000 | Loss: 0.00001830
Iteration 78/1000 | Loss: 0.00001830
Iteration 79/1000 | Loss: 0.00001830
Iteration 80/1000 | Loss: 0.00001830
Iteration 81/1000 | Loss: 0.00001830
Iteration 82/1000 | Loss: 0.00001830
Iteration 83/1000 | Loss: 0.00001830
Iteration 84/1000 | Loss: 0.00001830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.8296848793397658e-05, 1.8296848793397658e-05, 1.8296848793397658e-05, 1.8296848793397658e-05, 1.8296848793397658e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8296848793397658e-05

Optimization complete. Final v2v error: 3.6364216804504395 mm

Highest mean error: 4.268632888793945 mm for frame 49

Lowest mean error: 3.2729506492614746 mm for frame 1

Saving results

Total time: 36.55556845664978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812615
Iteration 2/25 | Loss: 0.00147097
Iteration 3/25 | Loss: 0.00126935
Iteration 4/25 | Loss: 0.00125443
Iteration 5/25 | Loss: 0.00125443
Iteration 6/25 | Loss: 0.00125443
Iteration 7/25 | Loss: 0.00125443
Iteration 8/25 | Loss: 0.00125443
Iteration 9/25 | Loss: 0.00125443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0012544319033622742, 0.0012544319033622742, 0.0012544319033622742, 0.0012544319033622742, 0.0012544319033622742]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012544319033622742

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91907382
Iteration 2/25 | Loss: 0.00080059
Iteration 3/25 | Loss: 0.00080059
Iteration 4/25 | Loss: 0.00080059
Iteration 5/25 | Loss: 0.00080059
Iteration 6/25 | Loss: 0.00080059
Iteration 7/25 | Loss: 0.00080059
Iteration 8/25 | Loss: 0.00080059
Iteration 9/25 | Loss: 0.00080059
Iteration 10/25 | Loss: 0.00080059
Iteration 11/25 | Loss: 0.00080059
Iteration 12/25 | Loss: 0.00080059
Iteration 13/25 | Loss: 0.00080059
Iteration 14/25 | Loss: 0.00080059
Iteration 15/25 | Loss: 0.00080059
Iteration 16/25 | Loss: 0.00080059
Iteration 17/25 | Loss: 0.00080059
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008005857816897333, 0.0008005857816897333, 0.0008005857816897333, 0.0008005857816897333, 0.0008005857816897333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008005857816897333

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080059
Iteration 2/1000 | Loss: 0.00003231
Iteration 3/1000 | Loss: 0.00002528
Iteration 4/1000 | Loss: 0.00002339
Iteration 5/1000 | Loss: 0.00002222
Iteration 6/1000 | Loss: 0.00002157
Iteration 7/1000 | Loss: 0.00002114
Iteration 8/1000 | Loss: 0.00002062
Iteration 9/1000 | Loss: 0.00002053
Iteration 10/1000 | Loss: 0.00002025
Iteration 11/1000 | Loss: 0.00002005
Iteration 12/1000 | Loss: 0.00001987
Iteration 13/1000 | Loss: 0.00001974
Iteration 14/1000 | Loss: 0.00001972
Iteration 15/1000 | Loss: 0.00001961
Iteration 16/1000 | Loss: 0.00001960
Iteration 17/1000 | Loss: 0.00001960
Iteration 18/1000 | Loss: 0.00001950
Iteration 19/1000 | Loss: 0.00001949
Iteration 20/1000 | Loss: 0.00001949
Iteration 21/1000 | Loss: 0.00001947
Iteration 22/1000 | Loss: 0.00001947
Iteration 23/1000 | Loss: 0.00001947
Iteration 24/1000 | Loss: 0.00001947
Iteration 25/1000 | Loss: 0.00001947
Iteration 26/1000 | Loss: 0.00001947
Iteration 27/1000 | Loss: 0.00001946
Iteration 28/1000 | Loss: 0.00001946
Iteration 29/1000 | Loss: 0.00001946
Iteration 30/1000 | Loss: 0.00001946
Iteration 31/1000 | Loss: 0.00001946
Iteration 32/1000 | Loss: 0.00001946
Iteration 33/1000 | Loss: 0.00001946
Iteration 34/1000 | Loss: 0.00001944
Iteration 35/1000 | Loss: 0.00001944
Iteration 36/1000 | Loss: 0.00001943
Iteration 37/1000 | Loss: 0.00001943
Iteration 38/1000 | Loss: 0.00001942
Iteration 39/1000 | Loss: 0.00001941
Iteration 40/1000 | Loss: 0.00001940
Iteration 41/1000 | Loss: 0.00001937
Iteration 42/1000 | Loss: 0.00001934
Iteration 43/1000 | Loss: 0.00001933
Iteration 44/1000 | Loss: 0.00001933
Iteration 45/1000 | Loss: 0.00001932
Iteration 46/1000 | Loss: 0.00001931
Iteration 47/1000 | Loss: 0.00001926
Iteration 48/1000 | Loss: 0.00001925
Iteration 49/1000 | Loss: 0.00001923
Iteration 50/1000 | Loss: 0.00001923
Iteration 51/1000 | Loss: 0.00001923
Iteration 52/1000 | Loss: 0.00001923
Iteration 53/1000 | Loss: 0.00001923
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001922
Iteration 56/1000 | Loss: 0.00001922
Iteration 57/1000 | Loss: 0.00001922
Iteration 58/1000 | Loss: 0.00001921
Iteration 59/1000 | Loss: 0.00001920
Iteration 60/1000 | Loss: 0.00001920
Iteration 61/1000 | Loss: 0.00001920
Iteration 62/1000 | Loss: 0.00001919
Iteration 63/1000 | Loss: 0.00001919
Iteration 64/1000 | Loss: 0.00001918
Iteration 65/1000 | Loss: 0.00001918
Iteration 66/1000 | Loss: 0.00001918
Iteration 67/1000 | Loss: 0.00001918
Iteration 68/1000 | Loss: 0.00001917
Iteration 69/1000 | Loss: 0.00001917
Iteration 70/1000 | Loss: 0.00001915
Iteration 71/1000 | Loss: 0.00001915
Iteration 72/1000 | Loss: 0.00001915
Iteration 73/1000 | Loss: 0.00001915
Iteration 74/1000 | Loss: 0.00001915
Iteration 75/1000 | Loss: 0.00001915
Iteration 76/1000 | Loss: 0.00001915
Iteration 77/1000 | Loss: 0.00001915
Iteration 78/1000 | Loss: 0.00001915
Iteration 79/1000 | Loss: 0.00001915
Iteration 80/1000 | Loss: 0.00001915
Iteration 81/1000 | Loss: 0.00001915
Iteration 82/1000 | Loss: 0.00001915
Iteration 83/1000 | Loss: 0.00001915
Iteration 84/1000 | Loss: 0.00001915
Iteration 85/1000 | Loss: 0.00001914
Iteration 86/1000 | Loss: 0.00001914
Iteration 87/1000 | Loss: 0.00001914
Iteration 88/1000 | Loss: 0.00001914
Iteration 89/1000 | Loss: 0.00001914
Iteration 90/1000 | Loss: 0.00001913
Iteration 91/1000 | Loss: 0.00001913
Iteration 92/1000 | Loss: 0.00001913
Iteration 93/1000 | Loss: 0.00001913
Iteration 94/1000 | Loss: 0.00001913
Iteration 95/1000 | Loss: 0.00001913
Iteration 96/1000 | Loss: 0.00001913
Iteration 97/1000 | Loss: 0.00001912
Iteration 98/1000 | Loss: 0.00001912
Iteration 99/1000 | Loss: 0.00001912
Iteration 100/1000 | Loss: 0.00001911
Iteration 101/1000 | Loss: 0.00001911
Iteration 102/1000 | Loss: 0.00001911
Iteration 103/1000 | Loss: 0.00001911
Iteration 104/1000 | Loss: 0.00001911
Iteration 105/1000 | Loss: 0.00001910
Iteration 106/1000 | Loss: 0.00001910
Iteration 107/1000 | Loss: 0.00001910
Iteration 108/1000 | Loss: 0.00001910
Iteration 109/1000 | Loss: 0.00001910
Iteration 110/1000 | Loss: 0.00001910
Iteration 111/1000 | Loss: 0.00001910
Iteration 112/1000 | Loss: 0.00001910
Iteration 113/1000 | Loss: 0.00001910
Iteration 114/1000 | Loss: 0.00001910
Iteration 115/1000 | Loss: 0.00001910
Iteration 116/1000 | Loss: 0.00001910
Iteration 117/1000 | Loss: 0.00001910
Iteration 118/1000 | Loss: 0.00001910
Iteration 119/1000 | Loss: 0.00001910
Iteration 120/1000 | Loss: 0.00001910
Iteration 121/1000 | Loss: 0.00001910
Iteration 122/1000 | Loss: 0.00001910
Iteration 123/1000 | Loss: 0.00001910
Iteration 124/1000 | Loss: 0.00001910
Iteration 125/1000 | Loss: 0.00001909
Iteration 126/1000 | Loss: 0.00001909
Iteration 127/1000 | Loss: 0.00001909
Iteration 128/1000 | Loss: 0.00001909
Iteration 129/1000 | Loss: 0.00001909
Iteration 130/1000 | Loss: 0.00001909
Iteration 131/1000 | Loss: 0.00001909
Iteration 132/1000 | Loss: 0.00001909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.90948267118074e-05, 1.90948267118074e-05, 1.90948267118074e-05, 1.90948267118074e-05, 1.90948267118074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.90948267118074e-05

Optimization complete. Final v2v error: 3.657200574874878 mm

Highest mean error: 3.7727959156036377 mm for frame 100

Lowest mean error: 3.5480735301971436 mm for frame 227

Saving results

Total time: 41.395864963531494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055706
Iteration 2/25 | Loss: 0.01055706
Iteration 3/25 | Loss: 0.01055706
Iteration 4/25 | Loss: 0.00405298
Iteration 5/25 | Loss: 0.00234098
Iteration 6/25 | Loss: 0.00205089
Iteration 7/25 | Loss: 0.00226914
Iteration 8/25 | Loss: 0.00197086
Iteration 9/25 | Loss: 0.00182289
Iteration 10/25 | Loss: 0.00175965
Iteration 11/25 | Loss: 0.00170946
Iteration 12/25 | Loss: 0.00166370
Iteration 13/25 | Loss: 0.00164170
Iteration 14/25 | Loss: 0.00162994
Iteration 15/25 | Loss: 0.00161895
Iteration 16/25 | Loss: 0.00161476
Iteration 17/25 | Loss: 0.00160354
Iteration 18/25 | Loss: 0.00160215
Iteration 19/25 | Loss: 0.00159881
Iteration 20/25 | Loss: 0.00160044
Iteration 21/25 | Loss: 0.00159795
Iteration 22/25 | Loss: 0.00160094
Iteration 23/25 | Loss: 0.00159601
Iteration 24/25 | Loss: 0.00159448
Iteration 25/25 | Loss: 0.00159573

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.51399666
Iteration 2/25 | Loss: 0.00311915
Iteration 3/25 | Loss: 0.00233490
Iteration 4/25 | Loss: 0.00233479
Iteration 5/25 | Loss: 0.00233479
Iteration 6/25 | Loss: 0.00233479
Iteration 7/25 | Loss: 0.00233479
Iteration 8/25 | Loss: 0.00233479
Iteration 9/25 | Loss: 0.00233479
Iteration 10/25 | Loss: 0.00233479
Iteration 11/25 | Loss: 0.00233479
Iteration 12/25 | Loss: 0.00233479
Iteration 13/25 | Loss: 0.00233479
Iteration 14/25 | Loss: 0.00233479
Iteration 15/25 | Loss: 0.00233479
Iteration 16/25 | Loss: 0.00233479
Iteration 17/25 | Loss: 0.00233479
Iteration 18/25 | Loss: 0.00233479
Iteration 19/25 | Loss: 0.00233479
Iteration 20/25 | Loss: 0.00233479
Iteration 21/25 | Loss: 0.00233479
Iteration 22/25 | Loss: 0.00233479
Iteration 23/25 | Loss: 0.00233479
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0023347886744886637, 0.0023347886744886637, 0.0023347886744886637, 0.0023347886744886637, 0.0023347886744886637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023347886744886637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233479
Iteration 2/1000 | Loss: 0.00036214
Iteration 3/1000 | Loss: 0.00049293
Iteration 4/1000 | Loss: 0.00031611
Iteration 5/1000 | Loss: 0.00067023
Iteration 6/1000 | Loss: 0.00095314
Iteration 7/1000 | Loss: 0.00015630
Iteration 8/1000 | Loss: 0.00016128
Iteration 9/1000 | Loss: 0.00014799
Iteration 10/1000 | Loss: 0.00013675
Iteration 11/1000 | Loss: 0.00014433
Iteration 12/1000 | Loss: 0.00016868
Iteration 13/1000 | Loss: 0.00014540
Iteration 14/1000 | Loss: 0.00014642
Iteration 15/1000 | Loss: 0.00014343
Iteration 16/1000 | Loss: 0.00016088
Iteration 17/1000 | Loss: 0.00108704
Iteration 18/1000 | Loss: 0.00388615
Iteration 19/1000 | Loss: 0.00069951
Iteration 20/1000 | Loss: 0.00038197
Iteration 21/1000 | Loss: 0.00016408
Iteration 22/1000 | Loss: 0.00020708
Iteration 23/1000 | Loss: 0.00074646
Iteration 24/1000 | Loss: 0.00010196
Iteration 25/1000 | Loss: 0.00028468
Iteration 26/1000 | Loss: 0.00007045
Iteration 27/1000 | Loss: 0.00006730
Iteration 28/1000 | Loss: 0.00018130
Iteration 29/1000 | Loss: 0.00005566
Iteration 30/1000 | Loss: 0.00005608
Iteration 31/1000 | Loss: 0.00037611
Iteration 32/1000 | Loss: 0.00003928
Iteration 33/1000 | Loss: 0.00012389
Iteration 34/1000 | Loss: 0.00003777
Iteration 35/1000 | Loss: 0.00005031
Iteration 36/1000 | Loss: 0.00026179
Iteration 37/1000 | Loss: 0.00003088
Iteration 38/1000 | Loss: 0.00008850
Iteration 39/1000 | Loss: 0.00009980
Iteration 40/1000 | Loss: 0.00007425
Iteration 41/1000 | Loss: 0.00003617
Iteration 42/1000 | Loss: 0.00009796
Iteration 43/1000 | Loss: 0.00002833
Iteration 44/1000 | Loss: 0.00002460
Iteration 45/1000 | Loss: 0.00004692
Iteration 46/1000 | Loss: 0.00002432
Iteration 47/1000 | Loss: 0.00002546
Iteration 48/1000 | Loss: 0.00011957
Iteration 49/1000 | Loss: 0.00003687
Iteration 50/1000 | Loss: 0.00003045
Iteration 51/1000 | Loss: 0.00004051
Iteration 52/1000 | Loss: 0.00002310
Iteration 53/1000 | Loss: 0.00003971
Iteration 54/1000 | Loss: 0.00002329
Iteration 55/1000 | Loss: 0.00003532
Iteration 56/1000 | Loss: 0.00002276
Iteration 57/1000 | Loss: 0.00002274
Iteration 58/1000 | Loss: 0.00004980
Iteration 59/1000 | Loss: 0.00002852
Iteration 60/1000 | Loss: 0.00002768
Iteration 61/1000 | Loss: 0.00003426
Iteration 62/1000 | Loss: 0.00002341
Iteration 63/1000 | Loss: 0.00002367
Iteration 64/1000 | Loss: 0.00002328
Iteration 65/1000 | Loss: 0.00002246
Iteration 66/1000 | Loss: 0.00002245
Iteration 67/1000 | Loss: 0.00002245
Iteration 68/1000 | Loss: 0.00002245
Iteration 69/1000 | Loss: 0.00002245
Iteration 70/1000 | Loss: 0.00002245
Iteration 71/1000 | Loss: 0.00002245
Iteration 72/1000 | Loss: 0.00002245
Iteration 73/1000 | Loss: 0.00002245
Iteration 74/1000 | Loss: 0.00002245
Iteration 75/1000 | Loss: 0.00002244
Iteration 76/1000 | Loss: 0.00002244
Iteration 77/1000 | Loss: 0.00002244
Iteration 78/1000 | Loss: 0.00002244
Iteration 79/1000 | Loss: 0.00002244
Iteration 80/1000 | Loss: 0.00002244
Iteration 81/1000 | Loss: 0.00002244
Iteration 82/1000 | Loss: 0.00002244
Iteration 83/1000 | Loss: 0.00002244
Iteration 84/1000 | Loss: 0.00002244
Iteration 85/1000 | Loss: 0.00002244
Iteration 86/1000 | Loss: 0.00002244
Iteration 87/1000 | Loss: 0.00002244
Iteration 88/1000 | Loss: 0.00002243
Iteration 89/1000 | Loss: 0.00002243
Iteration 90/1000 | Loss: 0.00002243
Iteration 91/1000 | Loss: 0.00002243
Iteration 92/1000 | Loss: 0.00002243
Iteration 93/1000 | Loss: 0.00002243
Iteration 94/1000 | Loss: 0.00002242
Iteration 95/1000 | Loss: 0.00002242
Iteration 96/1000 | Loss: 0.00002242
Iteration 97/1000 | Loss: 0.00002242
Iteration 98/1000 | Loss: 0.00002242
Iteration 99/1000 | Loss: 0.00002242
Iteration 100/1000 | Loss: 0.00002273
Iteration 101/1000 | Loss: 0.00002917
Iteration 102/1000 | Loss: 0.00002235
Iteration 103/1000 | Loss: 0.00002272
Iteration 104/1000 | Loss: 0.00022322
Iteration 105/1000 | Loss: 0.00039230
Iteration 106/1000 | Loss: 0.00026408
Iteration 107/1000 | Loss: 0.00024092
Iteration 108/1000 | Loss: 0.00012190
Iteration 109/1000 | Loss: 0.00002785
Iteration 110/1000 | Loss: 0.00005062
Iteration 111/1000 | Loss: 0.00002274
Iteration 112/1000 | Loss: 0.00002568
Iteration 113/1000 | Loss: 0.00002332
Iteration 114/1000 | Loss: 0.00002238
Iteration 115/1000 | Loss: 0.00002711
Iteration 116/1000 | Loss: 0.00002231
Iteration 117/1000 | Loss: 0.00002231
Iteration 118/1000 | Loss: 0.00002231
Iteration 119/1000 | Loss: 0.00002231
Iteration 120/1000 | Loss: 0.00002230
Iteration 121/1000 | Loss: 0.00002230
Iteration 122/1000 | Loss: 0.00002230
Iteration 123/1000 | Loss: 0.00002230
Iteration 124/1000 | Loss: 0.00002230
Iteration 125/1000 | Loss: 0.00002230
Iteration 126/1000 | Loss: 0.00002230
Iteration 127/1000 | Loss: 0.00002230
Iteration 128/1000 | Loss: 0.00002230
Iteration 129/1000 | Loss: 0.00002230
Iteration 130/1000 | Loss: 0.00002230
Iteration 131/1000 | Loss: 0.00002230
Iteration 132/1000 | Loss: 0.00002231
Iteration 133/1000 | Loss: 0.00002309
Iteration 134/1000 | Loss: 0.00002254
Iteration 135/1000 | Loss: 0.00002230
Iteration 136/1000 | Loss: 0.00002230
Iteration 137/1000 | Loss: 0.00002230
Iteration 138/1000 | Loss: 0.00002230
Iteration 139/1000 | Loss: 0.00002230
Iteration 140/1000 | Loss: 0.00002230
Iteration 141/1000 | Loss: 0.00002230
Iteration 142/1000 | Loss: 0.00002250
Iteration 143/1000 | Loss: 0.00002238
Iteration 144/1000 | Loss: 0.00002255
Iteration 145/1000 | Loss: 0.00002229
Iteration 146/1000 | Loss: 0.00002229
Iteration 147/1000 | Loss: 0.00002229
Iteration 148/1000 | Loss: 0.00002229
Iteration 149/1000 | Loss: 0.00002229
Iteration 150/1000 | Loss: 0.00002229
Iteration 151/1000 | Loss: 0.00002229
Iteration 152/1000 | Loss: 0.00002229
Iteration 153/1000 | Loss: 0.00002229
Iteration 154/1000 | Loss: 0.00002229
Iteration 155/1000 | Loss: 0.00002229
Iteration 156/1000 | Loss: 0.00002229
Iteration 157/1000 | Loss: 0.00002229
Iteration 158/1000 | Loss: 0.00002229
Iteration 159/1000 | Loss: 0.00002229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.2290167180472054e-05, 2.2290167180472054e-05, 2.2290167180472054e-05, 2.2290167180472054e-05, 2.2290167180472054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2290167180472054e-05

Optimization complete. Final v2v error: 3.9322316646575928 mm

Highest mean error: 8.669367790222168 mm for frame 38

Lowest mean error: 3.61517333984375 mm for frame 122

Saving results

Total time: 160.632306098938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816797
Iteration 2/25 | Loss: 0.00151334
Iteration 3/25 | Loss: 0.00132213
Iteration 4/25 | Loss: 0.00131345
Iteration 5/25 | Loss: 0.00131249
Iteration 6/25 | Loss: 0.00131249
Iteration 7/25 | Loss: 0.00131249
Iteration 8/25 | Loss: 0.00131249
Iteration 9/25 | Loss: 0.00131249
Iteration 10/25 | Loss: 0.00131249
Iteration 11/25 | Loss: 0.00131249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013124871766194701, 0.0013124871766194701, 0.0013124871766194701, 0.0013124871766194701, 0.0013124871766194701]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013124871766194701

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92463875
Iteration 2/25 | Loss: 0.00100400
Iteration 3/25 | Loss: 0.00100400
Iteration 4/25 | Loss: 0.00100400
Iteration 5/25 | Loss: 0.00100400
Iteration 6/25 | Loss: 0.00100400
Iteration 7/25 | Loss: 0.00100400
Iteration 8/25 | Loss: 0.00100400
Iteration 9/25 | Loss: 0.00100400
Iteration 10/25 | Loss: 0.00100400
Iteration 11/25 | Loss: 0.00100400
Iteration 12/25 | Loss: 0.00100400
Iteration 13/25 | Loss: 0.00100400
Iteration 14/25 | Loss: 0.00100400
Iteration 15/25 | Loss: 0.00100400
Iteration 16/25 | Loss: 0.00100400
Iteration 17/25 | Loss: 0.00100400
Iteration 18/25 | Loss: 0.00100400
Iteration 19/25 | Loss: 0.00100400
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010039955377578735, 0.0010039955377578735, 0.0010039955377578735, 0.0010039955377578735, 0.0010039955377578735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010039955377578735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100400
Iteration 2/1000 | Loss: 0.00004478
Iteration 3/1000 | Loss: 0.00003106
Iteration 4/1000 | Loss: 0.00002813
Iteration 5/1000 | Loss: 0.00002667
Iteration 6/1000 | Loss: 0.00002553
Iteration 7/1000 | Loss: 0.00002473
Iteration 8/1000 | Loss: 0.00002429
Iteration 9/1000 | Loss: 0.00002388
Iteration 10/1000 | Loss: 0.00002349
Iteration 11/1000 | Loss: 0.00002348
Iteration 12/1000 | Loss: 0.00002338
Iteration 13/1000 | Loss: 0.00002306
Iteration 14/1000 | Loss: 0.00002260
Iteration 15/1000 | Loss: 0.00002244
Iteration 16/1000 | Loss: 0.00002226
Iteration 17/1000 | Loss: 0.00002224
Iteration 18/1000 | Loss: 0.00002203
Iteration 19/1000 | Loss: 0.00002201
Iteration 20/1000 | Loss: 0.00002201
Iteration 21/1000 | Loss: 0.00002201
Iteration 22/1000 | Loss: 0.00002201
Iteration 23/1000 | Loss: 0.00002200
Iteration 24/1000 | Loss: 0.00002180
Iteration 25/1000 | Loss: 0.00002175
Iteration 26/1000 | Loss: 0.00002175
Iteration 27/1000 | Loss: 0.00002163
Iteration 28/1000 | Loss: 0.00002161
Iteration 29/1000 | Loss: 0.00002154
Iteration 30/1000 | Loss: 0.00002149
Iteration 31/1000 | Loss: 0.00002149
Iteration 32/1000 | Loss: 0.00002149
Iteration 33/1000 | Loss: 0.00002149
Iteration 34/1000 | Loss: 0.00002149
Iteration 35/1000 | Loss: 0.00002149
Iteration 36/1000 | Loss: 0.00002148
Iteration 37/1000 | Loss: 0.00002148
Iteration 38/1000 | Loss: 0.00002148
Iteration 39/1000 | Loss: 0.00002148
Iteration 40/1000 | Loss: 0.00002148
Iteration 41/1000 | Loss: 0.00002148
Iteration 42/1000 | Loss: 0.00002148
Iteration 43/1000 | Loss: 0.00002148
Iteration 44/1000 | Loss: 0.00002148
Iteration 45/1000 | Loss: 0.00002147
Iteration 46/1000 | Loss: 0.00002147
Iteration 47/1000 | Loss: 0.00002146
Iteration 48/1000 | Loss: 0.00002146
Iteration 49/1000 | Loss: 0.00002146
Iteration 50/1000 | Loss: 0.00002146
Iteration 51/1000 | Loss: 0.00002146
Iteration 52/1000 | Loss: 0.00002146
Iteration 53/1000 | Loss: 0.00002145
Iteration 54/1000 | Loss: 0.00002145
Iteration 55/1000 | Loss: 0.00002145
Iteration 56/1000 | Loss: 0.00002145
Iteration 57/1000 | Loss: 0.00002145
Iteration 58/1000 | Loss: 0.00002144
Iteration 59/1000 | Loss: 0.00002144
Iteration 60/1000 | Loss: 0.00002144
Iteration 61/1000 | Loss: 0.00002144
Iteration 62/1000 | Loss: 0.00002144
Iteration 63/1000 | Loss: 0.00002144
Iteration 64/1000 | Loss: 0.00002144
Iteration 65/1000 | Loss: 0.00002144
Iteration 66/1000 | Loss: 0.00002144
Iteration 67/1000 | Loss: 0.00002143
Iteration 68/1000 | Loss: 0.00002143
Iteration 69/1000 | Loss: 0.00002143
Iteration 70/1000 | Loss: 0.00002143
Iteration 71/1000 | Loss: 0.00002142
Iteration 72/1000 | Loss: 0.00002142
Iteration 73/1000 | Loss: 0.00002142
Iteration 74/1000 | Loss: 0.00002142
Iteration 75/1000 | Loss: 0.00002142
Iteration 76/1000 | Loss: 0.00002142
Iteration 77/1000 | Loss: 0.00002142
Iteration 78/1000 | Loss: 0.00002142
Iteration 79/1000 | Loss: 0.00002141
Iteration 80/1000 | Loss: 0.00002141
Iteration 81/1000 | Loss: 0.00002141
Iteration 82/1000 | Loss: 0.00002141
Iteration 83/1000 | Loss: 0.00002141
Iteration 84/1000 | Loss: 0.00002141
Iteration 85/1000 | Loss: 0.00002141
Iteration 86/1000 | Loss: 0.00002141
Iteration 87/1000 | Loss: 0.00002141
Iteration 88/1000 | Loss: 0.00002141
Iteration 89/1000 | Loss: 0.00002140
Iteration 90/1000 | Loss: 0.00002140
Iteration 91/1000 | Loss: 0.00002140
Iteration 92/1000 | Loss: 0.00002140
Iteration 93/1000 | Loss: 0.00002139
Iteration 94/1000 | Loss: 0.00002139
Iteration 95/1000 | Loss: 0.00002139
Iteration 96/1000 | Loss: 0.00002139
Iteration 97/1000 | Loss: 0.00002138
Iteration 98/1000 | Loss: 0.00002138
Iteration 99/1000 | Loss: 0.00002138
Iteration 100/1000 | Loss: 0.00002138
Iteration 101/1000 | Loss: 0.00002138
Iteration 102/1000 | Loss: 0.00002137
Iteration 103/1000 | Loss: 0.00002136
Iteration 104/1000 | Loss: 0.00002135
Iteration 105/1000 | Loss: 0.00002135
Iteration 106/1000 | Loss: 0.00002135
Iteration 107/1000 | Loss: 0.00002135
Iteration 108/1000 | Loss: 0.00002135
Iteration 109/1000 | Loss: 0.00002135
Iteration 110/1000 | Loss: 0.00002135
Iteration 111/1000 | Loss: 0.00002135
Iteration 112/1000 | Loss: 0.00002135
Iteration 113/1000 | Loss: 0.00002135
Iteration 114/1000 | Loss: 0.00002135
Iteration 115/1000 | Loss: 0.00002135
Iteration 116/1000 | Loss: 0.00002135
Iteration 117/1000 | Loss: 0.00002135
Iteration 118/1000 | Loss: 0.00002135
Iteration 119/1000 | Loss: 0.00002135
Iteration 120/1000 | Loss: 0.00002135
Iteration 121/1000 | Loss: 0.00002135
Iteration 122/1000 | Loss: 0.00002135
Iteration 123/1000 | Loss: 0.00002135
Iteration 124/1000 | Loss: 0.00002135
Iteration 125/1000 | Loss: 0.00002135
Iteration 126/1000 | Loss: 0.00002135
Iteration 127/1000 | Loss: 0.00002135
Iteration 128/1000 | Loss: 0.00002135
Iteration 129/1000 | Loss: 0.00002135
Iteration 130/1000 | Loss: 0.00002135
Iteration 131/1000 | Loss: 0.00002135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.1348672817111947e-05, 2.1348672817111947e-05, 2.1348672817111947e-05, 2.1348672817111947e-05, 2.1348672817111947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1348672817111947e-05

Optimization complete. Final v2v error: 3.906891107559204 mm

Highest mean error: 3.944866418838501 mm for frame 141

Lowest mean error: 3.8546369075775146 mm for frame 83

Saving results

Total time: 40.33514952659607
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873823
Iteration 2/25 | Loss: 0.00175920
Iteration 3/25 | Loss: 0.00139441
Iteration 4/25 | Loss: 0.00134083
Iteration 5/25 | Loss: 0.00132508
Iteration 6/25 | Loss: 0.00132027
Iteration 7/25 | Loss: 0.00130124
Iteration 8/25 | Loss: 0.00129659
Iteration 9/25 | Loss: 0.00130141
Iteration 10/25 | Loss: 0.00129324
Iteration 11/25 | Loss: 0.00129162
Iteration 12/25 | Loss: 0.00129772
Iteration 13/25 | Loss: 0.00129466
Iteration 14/25 | Loss: 0.00128937
Iteration 15/25 | Loss: 0.00128738
Iteration 16/25 | Loss: 0.00128673
Iteration 17/25 | Loss: 0.00128629
Iteration 18/25 | Loss: 0.00128622
Iteration 19/25 | Loss: 0.00128621
Iteration 20/25 | Loss: 0.00128621
Iteration 21/25 | Loss: 0.00128621
Iteration 22/25 | Loss: 0.00128621
Iteration 23/25 | Loss: 0.00128620
Iteration 24/25 | Loss: 0.00128620
Iteration 25/25 | Loss: 0.00128620

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.64495564
Iteration 2/25 | Loss: 0.00130317
Iteration 3/25 | Loss: 0.00130296
Iteration 4/25 | Loss: 0.00130296
Iteration 5/25 | Loss: 0.00130296
Iteration 6/25 | Loss: 0.00130295
Iteration 7/25 | Loss: 0.00130295
Iteration 8/25 | Loss: 0.00130295
Iteration 9/25 | Loss: 0.00130295
Iteration 10/25 | Loss: 0.00130295
Iteration 11/25 | Loss: 0.00130295
Iteration 12/25 | Loss: 0.00130295
Iteration 13/25 | Loss: 0.00130295
Iteration 14/25 | Loss: 0.00130295
Iteration 15/25 | Loss: 0.00130295
Iteration 16/25 | Loss: 0.00130295
Iteration 17/25 | Loss: 0.00130295
Iteration 18/25 | Loss: 0.00130295
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013029520632699132, 0.0013029520632699132, 0.0013029520632699132, 0.0013029520632699132, 0.0013029520632699132]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013029520632699132

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130295
Iteration 2/1000 | Loss: 0.00030813
Iteration 3/1000 | Loss: 0.00020239
Iteration 4/1000 | Loss: 0.00002703
Iteration 5/1000 | Loss: 0.00002361
Iteration 6/1000 | Loss: 0.00002214
Iteration 7/1000 | Loss: 0.00002134
Iteration 8/1000 | Loss: 0.00002069
Iteration 9/1000 | Loss: 0.00002022
Iteration 10/1000 | Loss: 0.00001989
Iteration 11/1000 | Loss: 0.00001957
Iteration 12/1000 | Loss: 0.00001934
Iteration 13/1000 | Loss: 0.00009563
Iteration 14/1000 | Loss: 0.00001912
Iteration 15/1000 | Loss: 0.00001907
Iteration 16/1000 | Loss: 0.00004795
Iteration 17/1000 | Loss: 0.00003695
Iteration 18/1000 | Loss: 0.00001901
Iteration 19/1000 | Loss: 0.00001901
Iteration 20/1000 | Loss: 0.00001900
Iteration 21/1000 | Loss: 0.00001899
Iteration 22/1000 | Loss: 0.00001886
Iteration 23/1000 | Loss: 0.00001884
Iteration 24/1000 | Loss: 0.00001879
Iteration 25/1000 | Loss: 0.00001878
Iteration 26/1000 | Loss: 0.00001875
Iteration 27/1000 | Loss: 0.00001868
Iteration 28/1000 | Loss: 0.00001866
Iteration 29/1000 | Loss: 0.00001866
Iteration 30/1000 | Loss: 0.00001865
Iteration 31/1000 | Loss: 0.00001865
Iteration 32/1000 | Loss: 0.00001865
Iteration 33/1000 | Loss: 0.00001864
Iteration 34/1000 | Loss: 0.00001860
Iteration 35/1000 | Loss: 0.00001858
Iteration 36/1000 | Loss: 0.00001857
Iteration 37/1000 | Loss: 0.00001857
Iteration 38/1000 | Loss: 0.00001856
Iteration 39/1000 | Loss: 0.00001856
Iteration 40/1000 | Loss: 0.00001855
Iteration 41/1000 | Loss: 0.00001855
Iteration 42/1000 | Loss: 0.00001855
Iteration 43/1000 | Loss: 0.00001854
Iteration 44/1000 | Loss: 0.00001854
Iteration 45/1000 | Loss: 0.00001849
Iteration 46/1000 | Loss: 0.00001849
Iteration 47/1000 | Loss: 0.00001847
Iteration 48/1000 | Loss: 0.00001846
Iteration 49/1000 | Loss: 0.00001846
Iteration 50/1000 | Loss: 0.00001845
Iteration 51/1000 | Loss: 0.00001845
Iteration 52/1000 | Loss: 0.00001844
Iteration 53/1000 | Loss: 0.00001843
Iteration 54/1000 | Loss: 0.00001843
Iteration 55/1000 | Loss: 0.00001843
Iteration 56/1000 | Loss: 0.00001843
Iteration 57/1000 | Loss: 0.00001842
Iteration 58/1000 | Loss: 0.00001842
Iteration 59/1000 | Loss: 0.00001842
Iteration 60/1000 | Loss: 0.00001842
Iteration 61/1000 | Loss: 0.00001842
Iteration 62/1000 | Loss: 0.00001842
Iteration 63/1000 | Loss: 0.00001842
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001842
Iteration 66/1000 | Loss: 0.00001841
Iteration 67/1000 | Loss: 0.00001841
Iteration 68/1000 | Loss: 0.00001841
Iteration 69/1000 | Loss: 0.00001840
Iteration 70/1000 | Loss: 0.00001840
Iteration 71/1000 | Loss: 0.00001840
Iteration 72/1000 | Loss: 0.00001839
Iteration 73/1000 | Loss: 0.00001839
Iteration 74/1000 | Loss: 0.00001839
Iteration 75/1000 | Loss: 0.00001838
Iteration 76/1000 | Loss: 0.00001838
Iteration 77/1000 | Loss: 0.00001837
Iteration 78/1000 | Loss: 0.00001837
Iteration 79/1000 | Loss: 0.00001836
Iteration 80/1000 | Loss: 0.00001836
Iteration 81/1000 | Loss: 0.00001836
Iteration 82/1000 | Loss: 0.00001836
Iteration 83/1000 | Loss: 0.00001836
Iteration 84/1000 | Loss: 0.00001835
Iteration 85/1000 | Loss: 0.00001835
Iteration 86/1000 | Loss: 0.00001835
Iteration 87/1000 | Loss: 0.00001835
Iteration 88/1000 | Loss: 0.00001834
Iteration 89/1000 | Loss: 0.00001834
Iteration 90/1000 | Loss: 0.00001834
Iteration 91/1000 | Loss: 0.00001834
Iteration 92/1000 | Loss: 0.00001833
Iteration 93/1000 | Loss: 0.00001833
Iteration 94/1000 | Loss: 0.00001833
Iteration 95/1000 | Loss: 0.00001833
Iteration 96/1000 | Loss: 0.00001833
Iteration 97/1000 | Loss: 0.00001833
Iteration 98/1000 | Loss: 0.00001833
Iteration 99/1000 | Loss: 0.00001832
Iteration 100/1000 | Loss: 0.00001832
Iteration 101/1000 | Loss: 0.00001832
Iteration 102/1000 | Loss: 0.00001832
Iteration 103/1000 | Loss: 0.00001831
Iteration 104/1000 | Loss: 0.00001831
Iteration 105/1000 | Loss: 0.00001831
Iteration 106/1000 | Loss: 0.00001831
Iteration 107/1000 | Loss: 0.00001830
Iteration 108/1000 | Loss: 0.00001830
Iteration 109/1000 | Loss: 0.00001830
Iteration 110/1000 | Loss: 0.00001830
Iteration 111/1000 | Loss: 0.00018575
Iteration 112/1000 | Loss: 0.00006925
Iteration 113/1000 | Loss: 0.00001846
Iteration 114/1000 | Loss: 0.00001831
Iteration 115/1000 | Loss: 0.00001831
Iteration 116/1000 | Loss: 0.00001830
Iteration 117/1000 | Loss: 0.00001830
Iteration 118/1000 | Loss: 0.00001829
Iteration 119/1000 | Loss: 0.00018160
Iteration 120/1000 | Loss: 0.00005420
Iteration 121/1000 | Loss: 0.00002399
Iteration 122/1000 | Loss: 0.00002742
Iteration 123/1000 | Loss: 0.00001950
Iteration 124/1000 | Loss: 0.00002041
Iteration 125/1000 | Loss: 0.00001900
Iteration 126/1000 | Loss: 0.00001893
Iteration 127/1000 | Loss: 0.00001890
Iteration 128/1000 | Loss: 0.00001885
Iteration 129/1000 | Loss: 0.00001884
Iteration 130/1000 | Loss: 0.00001871
Iteration 131/1000 | Loss: 0.00001870
Iteration 132/1000 | Loss: 0.00001869
Iteration 133/1000 | Loss: 0.00001868
Iteration 134/1000 | Loss: 0.00001868
Iteration 135/1000 | Loss: 0.00001868
Iteration 136/1000 | Loss: 0.00001868
Iteration 137/1000 | Loss: 0.00001868
Iteration 138/1000 | Loss: 0.00001867
Iteration 139/1000 | Loss: 0.00001867
Iteration 140/1000 | Loss: 0.00001867
Iteration 141/1000 | Loss: 0.00001867
Iteration 142/1000 | Loss: 0.00001867
Iteration 143/1000 | Loss: 0.00001867
Iteration 144/1000 | Loss: 0.00001867
Iteration 145/1000 | Loss: 0.00001867
Iteration 146/1000 | Loss: 0.00001867
Iteration 147/1000 | Loss: 0.00001866
Iteration 148/1000 | Loss: 0.00001866
Iteration 149/1000 | Loss: 0.00001866
Iteration 150/1000 | Loss: 0.00001866
Iteration 151/1000 | Loss: 0.00001866
Iteration 152/1000 | Loss: 0.00001866
Iteration 153/1000 | Loss: 0.00001866
Iteration 154/1000 | Loss: 0.00001866
Iteration 155/1000 | Loss: 0.00001866
Iteration 156/1000 | Loss: 0.00001866
Iteration 157/1000 | Loss: 0.00001866
Iteration 158/1000 | Loss: 0.00001866
Iteration 159/1000 | Loss: 0.00001866
Iteration 160/1000 | Loss: 0.00001866
Iteration 161/1000 | Loss: 0.00001866
Iteration 162/1000 | Loss: 0.00001866
Iteration 163/1000 | Loss: 0.00001866
Iteration 164/1000 | Loss: 0.00001866
Iteration 165/1000 | Loss: 0.00001866
Iteration 166/1000 | Loss: 0.00001866
Iteration 167/1000 | Loss: 0.00001866
Iteration 168/1000 | Loss: 0.00001866
Iteration 169/1000 | Loss: 0.00001866
Iteration 170/1000 | Loss: 0.00001866
Iteration 171/1000 | Loss: 0.00001866
Iteration 172/1000 | Loss: 0.00001866
Iteration 173/1000 | Loss: 0.00001866
Iteration 174/1000 | Loss: 0.00001866
Iteration 175/1000 | Loss: 0.00001866
Iteration 176/1000 | Loss: 0.00001866
Iteration 177/1000 | Loss: 0.00001866
Iteration 178/1000 | Loss: 0.00001866
Iteration 179/1000 | Loss: 0.00001866
Iteration 180/1000 | Loss: 0.00001866
Iteration 181/1000 | Loss: 0.00001866
Iteration 182/1000 | Loss: 0.00001866
Iteration 183/1000 | Loss: 0.00001866
Iteration 184/1000 | Loss: 0.00001866
Iteration 185/1000 | Loss: 0.00001866
Iteration 186/1000 | Loss: 0.00001866
Iteration 187/1000 | Loss: 0.00001866
Iteration 188/1000 | Loss: 0.00001866
Iteration 189/1000 | Loss: 0.00001866
Iteration 190/1000 | Loss: 0.00001866
Iteration 191/1000 | Loss: 0.00001866
Iteration 192/1000 | Loss: 0.00001866
Iteration 193/1000 | Loss: 0.00001866
Iteration 194/1000 | Loss: 0.00001866
Iteration 195/1000 | Loss: 0.00001866
Iteration 196/1000 | Loss: 0.00001866
Iteration 197/1000 | Loss: 0.00001866
Iteration 198/1000 | Loss: 0.00001866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.86576562555274e-05, 1.86576562555274e-05, 1.86576562555274e-05, 1.86576562555274e-05, 1.86576562555274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.86576562555274e-05

Optimization complete. Final v2v error: 3.5408620834350586 mm

Highest mean error: 8.684183120727539 mm for frame 93

Lowest mean error: 2.8430960178375244 mm for frame 17

Saving results

Total time: 89.87684679031372
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955455
Iteration 2/25 | Loss: 0.00175019
Iteration 3/25 | Loss: 0.00145602
Iteration 4/25 | Loss: 0.00143598
Iteration 5/25 | Loss: 0.00143022
Iteration 6/25 | Loss: 0.00142924
Iteration 7/25 | Loss: 0.00142924
Iteration 8/25 | Loss: 0.00142924
Iteration 9/25 | Loss: 0.00142924
Iteration 10/25 | Loss: 0.00142924
Iteration 11/25 | Loss: 0.00142924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014292416162788868, 0.0014292416162788868, 0.0014292416162788868, 0.0014292416162788868, 0.0014292416162788868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014292416162788868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.51300788
Iteration 2/25 | Loss: 0.00151664
Iteration 3/25 | Loss: 0.00151664
Iteration 4/25 | Loss: 0.00151664
Iteration 5/25 | Loss: 0.00151664
Iteration 6/25 | Loss: 0.00151664
Iteration 7/25 | Loss: 0.00151664
Iteration 8/25 | Loss: 0.00151664
Iteration 9/25 | Loss: 0.00151664
Iteration 10/25 | Loss: 0.00151664
Iteration 11/25 | Loss: 0.00151664
Iteration 12/25 | Loss: 0.00151664
Iteration 13/25 | Loss: 0.00151664
Iteration 14/25 | Loss: 0.00151664
Iteration 15/25 | Loss: 0.00151664
Iteration 16/25 | Loss: 0.00151664
Iteration 17/25 | Loss: 0.00151664
Iteration 18/25 | Loss: 0.00151664
Iteration 19/25 | Loss: 0.00151664
Iteration 20/25 | Loss: 0.00151664
Iteration 21/25 | Loss: 0.00151664
Iteration 22/25 | Loss: 0.00151664
Iteration 23/25 | Loss: 0.00151664
Iteration 24/25 | Loss: 0.00151664
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0015166376251727343, 0.0015166376251727343, 0.0015166376251727343, 0.0015166376251727343, 0.0015166376251727343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015166376251727343

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151664
Iteration 2/1000 | Loss: 0.00006195
Iteration 3/1000 | Loss: 0.00004532
Iteration 4/1000 | Loss: 0.00003982
Iteration 5/1000 | Loss: 0.00003771
Iteration 6/1000 | Loss: 0.00003661
Iteration 7/1000 | Loss: 0.00003583
Iteration 8/1000 | Loss: 0.00003516
Iteration 9/1000 | Loss: 0.00003468
Iteration 10/1000 | Loss: 0.00003432
Iteration 11/1000 | Loss: 0.00003399
Iteration 12/1000 | Loss: 0.00003369
Iteration 13/1000 | Loss: 0.00003339
Iteration 14/1000 | Loss: 0.00003304
Iteration 15/1000 | Loss: 0.00003279
Iteration 16/1000 | Loss: 0.00003254
Iteration 17/1000 | Loss: 0.00003232
Iteration 18/1000 | Loss: 0.00003213
Iteration 19/1000 | Loss: 0.00003210
Iteration 20/1000 | Loss: 0.00003198
Iteration 21/1000 | Loss: 0.00003194
Iteration 22/1000 | Loss: 0.00003194
Iteration 23/1000 | Loss: 0.00003186
Iteration 24/1000 | Loss: 0.00003186
Iteration 25/1000 | Loss: 0.00003178
Iteration 26/1000 | Loss: 0.00003178
Iteration 27/1000 | Loss: 0.00003177
Iteration 28/1000 | Loss: 0.00003176
Iteration 29/1000 | Loss: 0.00003175
Iteration 30/1000 | Loss: 0.00003175
Iteration 31/1000 | Loss: 0.00003174
Iteration 32/1000 | Loss: 0.00003174
Iteration 33/1000 | Loss: 0.00003174
Iteration 34/1000 | Loss: 0.00003174
Iteration 35/1000 | Loss: 0.00003174
Iteration 36/1000 | Loss: 0.00003173
Iteration 37/1000 | Loss: 0.00003173
Iteration 38/1000 | Loss: 0.00003172
Iteration 39/1000 | Loss: 0.00003171
Iteration 40/1000 | Loss: 0.00003170
Iteration 41/1000 | Loss: 0.00003170
Iteration 42/1000 | Loss: 0.00003170
Iteration 43/1000 | Loss: 0.00003169
Iteration 44/1000 | Loss: 0.00003169
Iteration 45/1000 | Loss: 0.00003169
Iteration 46/1000 | Loss: 0.00003169
Iteration 47/1000 | Loss: 0.00003169
Iteration 48/1000 | Loss: 0.00003168
Iteration 49/1000 | Loss: 0.00003168
Iteration 50/1000 | Loss: 0.00003168
Iteration 51/1000 | Loss: 0.00003168
Iteration 52/1000 | Loss: 0.00003168
Iteration 53/1000 | Loss: 0.00003168
Iteration 54/1000 | Loss: 0.00003168
Iteration 55/1000 | Loss: 0.00003167
Iteration 56/1000 | Loss: 0.00003167
Iteration 57/1000 | Loss: 0.00003167
Iteration 58/1000 | Loss: 0.00003167
Iteration 59/1000 | Loss: 0.00003167
Iteration 60/1000 | Loss: 0.00003167
Iteration 61/1000 | Loss: 0.00003167
Iteration 62/1000 | Loss: 0.00003167
Iteration 63/1000 | Loss: 0.00003167
Iteration 64/1000 | Loss: 0.00003167
Iteration 65/1000 | Loss: 0.00003166
Iteration 66/1000 | Loss: 0.00003166
Iteration 67/1000 | Loss: 0.00003166
Iteration 68/1000 | Loss: 0.00003166
Iteration 69/1000 | Loss: 0.00003166
Iteration 70/1000 | Loss: 0.00003166
Iteration 71/1000 | Loss: 0.00003165
Iteration 72/1000 | Loss: 0.00003165
Iteration 73/1000 | Loss: 0.00003165
Iteration 74/1000 | Loss: 0.00003165
Iteration 75/1000 | Loss: 0.00003165
Iteration 76/1000 | Loss: 0.00003165
Iteration 77/1000 | Loss: 0.00003165
Iteration 78/1000 | Loss: 0.00003165
Iteration 79/1000 | Loss: 0.00003165
Iteration 80/1000 | Loss: 0.00003165
Iteration 81/1000 | Loss: 0.00003165
Iteration 82/1000 | Loss: 0.00003165
Iteration 83/1000 | Loss: 0.00003164
Iteration 84/1000 | Loss: 0.00003164
Iteration 85/1000 | Loss: 0.00003164
Iteration 86/1000 | Loss: 0.00003164
Iteration 87/1000 | Loss: 0.00003164
Iteration 88/1000 | Loss: 0.00003164
Iteration 89/1000 | Loss: 0.00003164
Iteration 90/1000 | Loss: 0.00003164
Iteration 91/1000 | Loss: 0.00003164
Iteration 92/1000 | Loss: 0.00003164
Iteration 93/1000 | Loss: 0.00003164
Iteration 94/1000 | Loss: 0.00003164
Iteration 95/1000 | Loss: 0.00003164
Iteration 96/1000 | Loss: 0.00003164
Iteration 97/1000 | Loss: 0.00003164
Iteration 98/1000 | Loss: 0.00003164
Iteration 99/1000 | Loss: 0.00003164
Iteration 100/1000 | Loss: 0.00003163
Iteration 101/1000 | Loss: 0.00003163
Iteration 102/1000 | Loss: 0.00003163
Iteration 103/1000 | Loss: 0.00003163
Iteration 104/1000 | Loss: 0.00003163
Iteration 105/1000 | Loss: 0.00003162
Iteration 106/1000 | Loss: 0.00003162
Iteration 107/1000 | Loss: 0.00003162
Iteration 108/1000 | Loss: 0.00003162
Iteration 109/1000 | Loss: 0.00003162
Iteration 110/1000 | Loss: 0.00003162
Iteration 111/1000 | Loss: 0.00003162
Iteration 112/1000 | Loss: 0.00003162
Iteration 113/1000 | Loss: 0.00003162
Iteration 114/1000 | Loss: 0.00003162
Iteration 115/1000 | Loss: 0.00003162
Iteration 116/1000 | Loss: 0.00003162
Iteration 117/1000 | Loss: 0.00003162
Iteration 118/1000 | Loss: 0.00003162
Iteration 119/1000 | Loss: 0.00003162
Iteration 120/1000 | Loss: 0.00003162
Iteration 121/1000 | Loss: 0.00003162
Iteration 122/1000 | Loss: 0.00003162
Iteration 123/1000 | Loss: 0.00003162
Iteration 124/1000 | Loss: 0.00003162
Iteration 125/1000 | Loss: 0.00003162
Iteration 126/1000 | Loss: 0.00003162
Iteration 127/1000 | Loss: 0.00003162
Iteration 128/1000 | Loss: 0.00003162
Iteration 129/1000 | Loss: 0.00003162
Iteration 130/1000 | Loss: 0.00003162
Iteration 131/1000 | Loss: 0.00003162
Iteration 132/1000 | Loss: 0.00003162
Iteration 133/1000 | Loss: 0.00003162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [3.1618976208847016e-05, 3.1618976208847016e-05, 3.1618976208847016e-05, 3.1618976208847016e-05, 3.1618976208847016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1618976208847016e-05

Optimization complete. Final v2v error: 4.6819963455200195 mm

Highest mean error: 5.51258659362793 mm for frame 8

Lowest mean error: 4.3132805824279785 mm for frame 60

Saving results

Total time: 47.52324604988098
