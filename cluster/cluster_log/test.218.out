Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=218, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12208-12263
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853303
Iteration 2/25 | Loss: 0.00112652
Iteration 3/25 | Loss: 0.00076836
Iteration 4/25 | Loss: 0.00069641
Iteration 5/25 | Loss: 0.00067967
Iteration 6/25 | Loss: 0.00067583
Iteration 7/25 | Loss: 0.00067476
Iteration 8/25 | Loss: 0.00067465
Iteration 9/25 | Loss: 0.00067465
Iteration 10/25 | Loss: 0.00067465
Iteration 11/25 | Loss: 0.00067465
Iteration 12/25 | Loss: 0.00067465
Iteration 13/25 | Loss: 0.00067465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006746450671926141, 0.0006746450671926141, 0.0006746450671926141, 0.0006746450671926141, 0.0006746450671926141]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006746450671926141

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41059101
Iteration 2/25 | Loss: 0.00013498
Iteration 3/25 | Loss: 0.00013498
Iteration 4/25 | Loss: 0.00013498
Iteration 5/25 | Loss: 0.00013498
Iteration 6/25 | Loss: 0.00013498
Iteration 7/25 | Loss: 0.00013498
Iteration 8/25 | Loss: 0.00013498
Iteration 9/25 | Loss: 0.00013498
Iteration 10/25 | Loss: 0.00013497
Iteration 11/25 | Loss: 0.00013497
Iteration 12/25 | Loss: 0.00013497
Iteration 13/25 | Loss: 0.00013497
Iteration 14/25 | Loss: 0.00013497
Iteration 15/25 | Loss: 0.00013497
Iteration 16/25 | Loss: 0.00013497
Iteration 17/25 | Loss: 0.00013497
Iteration 18/25 | Loss: 0.00013497
Iteration 19/25 | Loss: 0.00013497
Iteration 20/25 | Loss: 0.00013497
Iteration 21/25 | Loss: 0.00013497
Iteration 22/25 | Loss: 0.00013497
Iteration 23/25 | Loss: 0.00013497
Iteration 24/25 | Loss: 0.00013497
Iteration 25/25 | Loss: 0.00013497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00013497
Iteration 2/1000 | Loss: 0.00003304
Iteration 3/1000 | Loss: 0.00001975
Iteration 4/1000 | Loss: 0.00001749
Iteration 5/1000 | Loss: 0.00001642
Iteration 6/1000 | Loss: 0.00001582
Iteration 7/1000 | Loss: 0.00001525
Iteration 8/1000 | Loss: 0.00001499
Iteration 9/1000 | Loss: 0.00001466
Iteration 10/1000 | Loss: 0.00001444
Iteration 11/1000 | Loss: 0.00001443
Iteration 12/1000 | Loss: 0.00001442
Iteration 13/1000 | Loss: 0.00001442
Iteration 14/1000 | Loss: 0.00001441
Iteration 15/1000 | Loss: 0.00001439
Iteration 16/1000 | Loss: 0.00001428
Iteration 17/1000 | Loss: 0.00001427
Iteration 18/1000 | Loss: 0.00001423
Iteration 19/1000 | Loss: 0.00001420
Iteration 20/1000 | Loss: 0.00001420
Iteration 21/1000 | Loss: 0.00001419
Iteration 22/1000 | Loss: 0.00001419
Iteration 23/1000 | Loss: 0.00001418
Iteration 24/1000 | Loss: 0.00001417
Iteration 25/1000 | Loss: 0.00001412
Iteration 26/1000 | Loss: 0.00001409
Iteration 27/1000 | Loss: 0.00001409
Iteration 28/1000 | Loss: 0.00001407
Iteration 29/1000 | Loss: 0.00001407
Iteration 30/1000 | Loss: 0.00001407
Iteration 31/1000 | Loss: 0.00001406
Iteration 32/1000 | Loss: 0.00001406
Iteration 33/1000 | Loss: 0.00001406
Iteration 34/1000 | Loss: 0.00001405
Iteration 35/1000 | Loss: 0.00001405
Iteration 36/1000 | Loss: 0.00001405
Iteration 37/1000 | Loss: 0.00001405
Iteration 38/1000 | Loss: 0.00001404
Iteration 39/1000 | Loss: 0.00001404
Iteration 40/1000 | Loss: 0.00001404
Iteration 41/1000 | Loss: 0.00001404
Iteration 42/1000 | Loss: 0.00001404
Iteration 43/1000 | Loss: 0.00001404
Iteration 44/1000 | Loss: 0.00001404
Iteration 45/1000 | Loss: 0.00001404
Iteration 46/1000 | Loss: 0.00001403
Iteration 47/1000 | Loss: 0.00001403
Iteration 48/1000 | Loss: 0.00001403
Iteration 49/1000 | Loss: 0.00001402
Iteration 50/1000 | Loss: 0.00001402
Iteration 51/1000 | Loss: 0.00001402
Iteration 52/1000 | Loss: 0.00001401
Iteration 53/1000 | Loss: 0.00001401
Iteration 54/1000 | Loss: 0.00001401
Iteration 55/1000 | Loss: 0.00001401
Iteration 56/1000 | Loss: 0.00001400
Iteration 57/1000 | Loss: 0.00001400
Iteration 58/1000 | Loss: 0.00001400
Iteration 59/1000 | Loss: 0.00001400
Iteration 60/1000 | Loss: 0.00001400
Iteration 61/1000 | Loss: 0.00001400
Iteration 62/1000 | Loss: 0.00001400
Iteration 63/1000 | Loss: 0.00001399
Iteration 64/1000 | Loss: 0.00001399
Iteration 65/1000 | Loss: 0.00001399
Iteration 66/1000 | Loss: 0.00001399
Iteration 67/1000 | Loss: 0.00001399
Iteration 68/1000 | Loss: 0.00001399
Iteration 69/1000 | Loss: 0.00001398
Iteration 70/1000 | Loss: 0.00001398
Iteration 71/1000 | Loss: 0.00001398
Iteration 72/1000 | Loss: 0.00001398
Iteration 73/1000 | Loss: 0.00001398
Iteration 74/1000 | Loss: 0.00001398
Iteration 75/1000 | Loss: 0.00001398
Iteration 76/1000 | Loss: 0.00001398
Iteration 77/1000 | Loss: 0.00001398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.3975763977214228e-05, 1.3975763977214228e-05, 1.3975763977214228e-05, 1.3975763977214228e-05, 1.3975763977214228e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3975763977214228e-05

Optimization complete. Final v2v error: 3.2231316566467285 mm

Highest mean error: 3.5988972187042236 mm for frame 99

Lowest mean error: 2.8890461921691895 mm for frame 15

Saving results

Total time: 40.32141709327698
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438373
Iteration 2/25 | Loss: 0.00088483
Iteration 3/25 | Loss: 0.00077209
Iteration 4/25 | Loss: 0.00073885
Iteration 5/25 | Loss: 0.00073212
Iteration 6/25 | Loss: 0.00073051
Iteration 7/25 | Loss: 0.00072964
Iteration 8/25 | Loss: 0.00072956
Iteration 9/25 | Loss: 0.00072956
Iteration 10/25 | Loss: 0.00072956
Iteration 11/25 | Loss: 0.00072956
Iteration 12/25 | Loss: 0.00072956
Iteration 13/25 | Loss: 0.00072956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007295640534721315, 0.0007295640534721315, 0.0007295640534721315, 0.0007295640534721315, 0.0007295640534721315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007295640534721315

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27784610
Iteration 2/25 | Loss: 0.00014618
Iteration 3/25 | Loss: 0.00014615
Iteration 4/25 | Loss: 0.00014615
Iteration 5/25 | Loss: 0.00014615
Iteration 6/25 | Loss: 0.00014615
Iteration 7/25 | Loss: 0.00014615
Iteration 8/25 | Loss: 0.00014615
Iteration 9/25 | Loss: 0.00014615
Iteration 10/25 | Loss: 0.00014615
Iteration 11/25 | Loss: 0.00014615
Iteration 12/25 | Loss: 0.00014615
Iteration 13/25 | Loss: 0.00014615
Iteration 14/25 | Loss: 0.00014615
Iteration 15/25 | Loss: 0.00014615
Iteration 16/25 | Loss: 0.00014615
Iteration 17/25 | Loss: 0.00014615
Iteration 18/25 | Loss: 0.00014615
Iteration 19/25 | Loss: 0.00014615
Iteration 20/25 | Loss: 0.00014615
Iteration 21/25 | Loss: 0.00014615
Iteration 22/25 | Loss: 0.00014615
Iteration 23/25 | Loss: 0.00014615
Iteration 24/25 | Loss: 0.00014615
Iteration 25/25 | Loss: 0.00014615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00014615
Iteration 2/1000 | Loss: 0.00006273
Iteration 3/1000 | Loss: 0.00003515
Iteration 4/1000 | Loss: 0.00002972
Iteration 5/1000 | Loss: 0.00002742
Iteration 6/1000 | Loss: 0.00002642
Iteration 7/1000 | Loss: 0.00002539
Iteration 8/1000 | Loss: 0.00002469
Iteration 9/1000 | Loss: 0.00002428
Iteration 10/1000 | Loss: 0.00002398
Iteration 11/1000 | Loss: 0.00002375
Iteration 12/1000 | Loss: 0.00002354
Iteration 13/1000 | Loss: 0.00002334
Iteration 14/1000 | Loss: 0.00002323
Iteration 15/1000 | Loss: 0.00002322
Iteration 16/1000 | Loss: 0.00002322
Iteration 17/1000 | Loss: 0.00002322
Iteration 18/1000 | Loss: 0.00002321
Iteration 19/1000 | Loss: 0.00002320
Iteration 20/1000 | Loss: 0.00002319
Iteration 21/1000 | Loss: 0.00002311
Iteration 22/1000 | Loss: 0.00002305
Iteration 23/1000 | Loss: 0.00002305
Iteration 24/1000 | Loss: 0.00002304
Iteration 25/1000 | Loss: 0.00002304
Iteration 26/1000 | Loss: 0.00002304
Iteration 27/1000 | Loss: 0.00002303
Iteration 28/1000 | Loss: 0.00002303
Iteration 29/1000 | Loss: 0.00002303
Iteration 30/1000 | Loss: 0.00002301
Iteration 31/1000 | Loss: 0.00002301
Iteration 32/1000 | Loss: 0.00002300
Iteration 33/1000 | Loss: 0.00002300
Iteration 34/1000 | Loss: 0.00002299
Iteration 35/1000 | Loss: 0.00002298
Iteration 36/1000 | Loss: 0.00002298
Iteration 37/1000 | Loss: 0.00002298
Iteration 38/1000 | Loss: 0.00002297
Iteration 39/1000 | Loss: 0.00002297
Iteration 40/1000 | Loss: 0.00002296
Iteration 41/1000 | Loss: 0.00002296
Iteration 42/1000 | Loss: 0.00002295
Iteration 43/1000 | Loss: 0.00002295
Iteration 44/1000 | Loss: 0.00002295
Iteration 45/1000 | Loss: 0.00002294
Iteration 46/1000 | Loss: 0.00002294
Iteration 47/1000 | Loss: 0.00002294
Iteration 48/1000 | Loss: 0.00002294
Iteration 49/1000 | Loss: 0.00002294
Iteration 50/1000 | Loss: 0.00002293
Iteration 51/1000 | Loss: 0.00002293
Iteration 52/1000 | Loss: 0.00002293
Iteration 53/1000 | Loss: 0.00002292
Iteration 54/1000 | Loss: 0.00002292
Iteration 55/1000 | Loss: 0.00002292
Iteration 56/1000 | Loss: 0.00002292
Iteration 57/1000 | Loss: 0.00002292
Iteration 58/1000 | Loss: 0.00002292
Iteration 59/1000 | Loss: 0.00002292
Iteration 60/1000 | Loss: 0.00002291
Iteration 61/1000 | Loss: 0.00002291
Iteration 62/1000 | Loss: 0.00002291
Iteration 63/1000 | Loss: 0.00002288
Iteration 64/1000 | Loss: 0.00002288
Iteration 65/1000 | Loss: 0.00002288
Iteration 66/1000 | Loss: 0.00002288
Iteration 67/1000 | Loss: 0.00002288
Iteration 68/1000 | Loss: 0.00002288
Iteration 69/1000 | Loss: 0.00002288
Iteration 70/1000 | Loss: 0.00002288
Iteration 71/1000 | Loss: 0.00002288
Iteration 72/1000 | Loss: 0.00002287
Iteration 73/1000 | Loss: 0.00002287
Iteration 74/1000 | Loss: 0.00002287
Iteration 75/1000 | Loss: 0.00002286
Iteration 76/1000 | Loss: 0.00002286
Iteration 77/1000 | Loss: 0.00002286
Iteration 78/1000 | Loss: 0.00002285
Iteration 79/1000 | Loss: 0.00002285
Iteration 80/1000 | Loss: 0.00002284
Iteration 81/1000 | Loss: 0.00002284
Iteration 82/1000 | Loss: 0.00002284
Iteration 83/1000 | Loss: 0.00002284
Iteration 84/1000 | Loss: 0.00002284
Iteration 85/1000 | Loss: 0.00002284
Iteration 86/1000 | Loss: 0.00002284
Iteration 87/1000 | Loss: 0.00002284
Iteration 88/1000 | Loss: 0.00002284
Iteration 89/1000 | Loss: 0.00002284
Iteration 90/1000 | Loss: 0.00002284
Iteration 91/1000 | Loss: 0.00002284
Iteration 92/1000 | Loss: 0.00002284
Iteration 93/1000 | Loss: 0.00002283
Iteration 94/1000 | Loss: 0.00002283
Iteration 95/1000 | Loss: 0.00002283
Iteration 96/1000 | Loss: 0.00002283
Iteration 97/1000 | Loss: 0.00002283
Iteration 98/1000 | Loss: 0.00002283
Iteration 99/1000 | Loss: 0.00002283
Iteration 100/1000 | Loss: 0.00002283
Iteration 101/1000 | Loss: 0.00002283
Iteration 102/1000 | Loss: 0.00002283
Iteration 103/1000 | Loss: 0.00002283
Iteration 104/1000 | Loss: 0.00002283
Iteration 105/1000 | Loss: 0.00002283
Iteration 106/1000 | Loss: 0.00002283
Iteration 107/1000 | Loss: 0.00002283
Iteration 108/1000 | Loss: 0.00002283
Iteration 109/1000 | Loss: 0.00002283
Iteration 110/1000 | Loss: 0.00002283
Iteration 111/1000 | Loss: 0.00002283
Iteration 112/1000 | Loss: 0.00002283
Iteration 113/1000 | Loss: 0.00002283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.2833461116533726e-05, 2.2833461116533726e-05, 2.2833461116533726e-05, 2.2833461116533726e-05, 2.2833461116533726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2833461116533726e-05

Optimization complete. Final v2v error: 4.095048904418945 mm

Highest mean error: 4.490905284881592 mm for frame 22

Lowest mean error: 3.636582374572754 mm for frame 131

Saving results

Total time: 39.33143091201782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408141
Iteration 2/25 | Loss: 0.00077251
Iteration 3/25 | Loss: 0.00067860
Iteration 4/25 | Loss: 0.00066607
Iteration 5/25 | Loss: 0.00066351
Iteration 6/25 | Loss: 0.00066327
Iteration 7/25 | Loss: 0.00066327
Iteration 8/25 | Loss: 0.00066327
Iteration 9/25 | Loss: 0.00066327
Iteration 10/25 | Loss: 0.00066327
Iteration 11/25 | Loss: 0.00066327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006632679142057896, 0.0006632679142057896, 0.0006632679142057896, 0.0006632679142057896, 0.0006632679142057896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006632679142057896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.41096163
Iteration 2/25 | Loss: 0.00011311
Iteration 3/25 | Loss: 0.00011311
Iteration 4/25 | Loss: 0.00011311
Iteration 5/25 | Loss: 0.00011311
Iteration 6/25 | Loss: 0.00011311
Iteration 7/25 | Loss: 0.00011311
Iteration 8/25 | Loss: 0.00011311
Iteration 9/25 | Loss: 0.00011311
Iteration 10/25 | Loss: 0.00011310
Iteration 11/25 | Loss: 0.00011310
Iteration 12/25 | Loss: 0.00011310
Iteration 13/25 | Loss: 0.00011310
Iteration 14/25 | Loss: 0.00011310
Iteration 15/25 | Loss: 0.00011310
Iteration 16/25 | Loss: 0.00011310
Iteration 17/25 | Loss: 0.00011310
Iteration 18/25 | Loss: 0.00011310
Iteration 19/25 | Loss: 0.00011310
Iteration 20/25 | Loss: 0.00011310
Iteration 21/25 | Loss: 0.00011310
Iteration 22/25 | Loss: 0.00011310
Iteration 23/25 | Loss: 0.00011310
Iteration 24/25 | Loss: 0.00011310
Iteration 25/25 | Loss: 0.00011310

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00011310
Iteration 2/1000 | Loss: 0.00003696
Iteration 3/1000 | Loss: 0.00002871
Iteration 4/1000 | Loss: 0.00002657
Iteration 5/1000 | Loss: 0.00002498
Iteration 6/1000 | Loss: 0.00002356
Iteration 7/1000 | Loss: 0.00002313
Iteration 8/1000 | Loss: 0.00002273
Iteration 9/1000 | Loss: 0.00002257
Iteration 10/1000 | Loss: 0.00002244
Iteration 11/1000 | Loss: 0.00002222
Iteration 12/1000 | Loss: 0.00002202
Iteration 13/1000 | Loss: 0.00002191
Iteration 14/1000 | Loss: 0.00002191
Iteration 15/1000 | Loss: 0.00002183
Iteration 16/1000 | Loss: 0.00002183
Iteration 17/1000 | Loss: 0.00002178
Iteration 18/1000 | Loss: 0.00002177
Iteration 19/1000 | Loss: 0.00002177
Iteration 20/1000 | Loss: 0.00002176
Iteration 21/1000 | Loss: 0.00002170
Iteration 22/1000 | Loss: 0.00002169
Iteration 23/1000 | Loss: 0.00002163
Iteration 24/1000 | Loss: 0.00002163
Iteration 25/1000 | Loss: 0.00002162
Iteration 26/1000 | Loss: 0.00002162
Iteration 27/1000 | Loss: 0.00002162
Iteration 28/1000 | Loss: 0.00002162
Iteration 29/1000 | Loss: 0.00002161
Iteration 30/1000 | Loss: 0.00002161
Iteration 31/1000 | Loss: 0.00002160
Iteration 32/1000 | Loss: 0.00002160
Iteration 33/1000 | Loss: 0.00002159
Iteration 34/1000 | Loss: 0.00002159
Iteration 35/1000 | Loss: 0.00002159
Iteration 36/1000 | Loss: 0.00002158
Iteration 37/1000 | Loss: 0.00002158
Iteration 38/1000 | Loss: 0.00002158
Iteration 39/1000 | Loss: 0.00002157
Iteration 40/1000 | Loss: 0.00002157
Iteration 41/1000 | Loss: 0.00002156
Iteration 42/1000 | Loss: 0.00002156
Iteration 43/1000 | Loss: 0.00002156
Iteration 44/1000 | Loss: 0.00002156
Iteration 45/1000 | Loss: 0.00002156
Iteration 46/1000 | Loss: 0.00002156
Iteration 47/1000 | Loss: 0.00002155
Iteration 48/1000 | Loss: 0.00002155
Iteration 49/1000 | Loss: 0.00002155
Iteration 50/1000 | Loss: 0.00002155
Iteration 51/1000 | Loss: 0.00002155
Iteration 52/1000 | Loss: 0.00002154
Iteration 53/1000 | Loss: 0.00002154
Iteration 54/1000 | Loss: 0.00002154
Iteration 55/1000 | Loss: 0.00002153
Iteration 56/1000 | Loss: 0.00002153
Iteration 57/1000 | Loss: 0.00002153
Iteration 58/1000 | Loss: 0.00002153
Iteration 59/1000 | Loss: 0.00002153
Iteration 60/1000 | Loss: 0.00002153
Iteration 61/1000 | Loss: 0.00002153
Iteration 62/1000 | Loss: 0.00002153
Iteration 63/1000 | Loss: 0.00002152
Iteration 64/1000 | Loss: 0.00002152
Iteration 65/1000 | Loss: 0.00002151
Iteration 66/1000 | Loss: 0.00002151
Iteration 67/1000 | Loss: 0.00002151
Iteration 68/1000 | Loss: 0.00002151
Iteration 69/1000 | Loss: 0.00002151
Iteration 70/1000 | Loss: 0.00002151
Iteration 71/1000 | Loss: 0.00002151
Iteration 72/1000 | Loss: 0.00002150
Iteration 73/1000 | Loss: 0.00002150
Iteration 74/1000 | Loss: 0.00002150
Iteration 75/1000 | Loss: 0.00002150
Iteration 76/1000 | Loss: 0.00002150
Iteration 77/1000 | Loss: 0.00002150
Iteration 78/1000 | Loss: 0.00002150
Iteration 79/1000 | Loss: 0.00002150
Iteration 80/1000 | Loss: 0.00002149
Iteration 81/1000 | Loss: 0.00002149
Iteration 82/1000 | Loss: 0.00002149
Iteration 83/1000 | Loss: 0.00002148
Iteration 84/1000 | Loss: 0.00002148
Iteration 85/1000 | Loss: 0.00002148
Iteration 86/1000 | Loss: 0.00002148
Iteration 87/1000 | Loss: 0.00002148
Iteration 88/1000 | Loss: 0.00002148
Iteration 89/1000 | Loss: 0.00002148
Iteration 90/1000 | Loss: 0.00002148
Iteration 91/1000 | Loss: 0.00002148
Iteration 92/1000 | Loss: 0.00002148
Iteration 93/1000 | Loss: 0.00002148
Iteration 94/1000 | Loss: 0.00002148
Iteration 95/1000 | Loss: 0.00002148
Iteration 96/1000 | Loss: 0.00002148
Iteration 97/1000 | Loss: 0.00002148
Iteration 98/1000 | Loss: 0.00002148
Iteration 99/1000 | Loss: 0.00002148
Iteration 100/1000 | Loss: 0.00002148
Iteration 101/1000 | Loss: 0.00002148
Iteration 102/1000 | Loss: 0.00002148
Iteration 103/1000 | Loss: 0.00002148
Iteration 104/1000 | Loss: 0.00002148
Iteration 105/1000 | Loss: 0.00002148
Iteration 106/1000 | Loss: 0.00002148
Iteration 107/1000 | Loss: 0.00002148
Iteration 108/1000 | Loss: 0.00002148
Iteration 109/1000 | Loss: 0.00002148
Iteration 110/1000 | Loss: 0.00002148
Iteration 111/1000 | Loss: 0.00002148
Iteration 112/1000 | Loss: 0.00002148
Iteration 113/1000 | Loss: 0.00002148
Iteration 114/1000 | Loss: 0.00002148
Iteration 115/1000 | Loss: 0.00002148
Iteration 116/1000 | Loss: 0.00002148
Iteration 117/1000 | Loss: 0.00002148
Iteration 118/1000 | Loss: 0.00002148
Iteration 119/1000 | Loss: 0.00002148
Iteration 120/1000 | Loss: 0.00002148
Iteration 121/1000 | Loss: 0.00002148
Iteration 122/1000 | Loss: 0.00002148
Iteration 123/1000 | Loss: 0.00002148
Iteration 124/1000 | Loss: 0.00002148
Iteration 125/1000 | Loss: 0.00002148
Iteration 126/1000 | Loss: 0.00002148
Iteration 127/1000 | Loss: 0.00002148
Iteration 128/1000 | Loss: 0.00002148
Iteration 129/1000 | Loss: 0.00002148
Iteration 130/1000 | Loss: 0.00002148
Iteration 131/1000 | Loss: 0.00002148
Iteration 132/1000 | Loss: 0.00002148
Iteration 133/1000 | Loss: 0.00002148
Iteration 134/1000 | Loss: 0.00002148
Iteration 135/1000 | Loss: 0.00002148
Iteration 136/1000 | Loss: 0.00002148
Iteration 137/1000 | Loss: 0.00002148
Iteration 138/1000 | Loss: 0.00002148
Iteration 139/1000 | Loss: 0.00002148
Iteration 140/1000 | Loss: 0.00002148
Iteration 141/1000 | Loss: 0.00002148
Iteration 142/1000 | Loss: 0.00002148
Iteration 143/1000 | Loss: 0.00002148
Iteration 144/1000 | Loss: 0.00002148
Iteration 145/1000 | Loss: 0.00002148
Iteration 146/1000 | Loss: 0.00002148
Iteration 147/1000 | Loss: 0.00002148
Iteration 148/1000 | Loss: 0.00002148
Iteration 149/1000 | Loss: 0.00002148
Iteration 150/1000 | Loss: 0.00002148
Iteration 151/1000 | Loss: 0.00002148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.1477437257999554e-05, 2.1477437257999554e-05, 2.1477437257999554e-05, 2.1477437257999554e-05, 2.1477437257999554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1477437257999554e-05

Optimization complete. Final v2v error: 3.91799259185791 mm

Highest mean error: 4.2194013595581055 mm for frame 155

Lowest mean error: 3.645991563796997 mm for frame 178

Saving results

Total time: 36.44079303741455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417170
Iteration 2/25 | Loss: 0.00111504
Iteration 3/25 | Loss: 0.00073172
Iteration 4/25 | Loss: 0.00066277
Iteration 5/25 | Loss: 0.00065689
Iteration 6/25 | Loss: 0.00065476
Iteration 7/25 | Loss: 0.00065453
Iteration 8/25 | Loss: 0.00065453
Iteration 9/25 | Loss: 0.00065453
Iteration 10/25 | Loss: 0.00065453
Iteration 11/25 | Loss: 0.00065453
Iteration 12/25 | Loss: 0.00065453
Iteration 13/25 | Loss: 0.00065453
Iteration 14/25 | Loss: 0.00065453
Iteration 15/25 | Loss: 0.00065453
Iteration 16/25 | Loss: 0.00065453
Iteration 17/25 | Loss: 0.00065453
Iteration 18/25 | Loss: 0.00065453
Iteration 19/25 | Loss: 0.00065453
Iteration 20/25 | Loss: 0.00065453
Iteration 21/25 | Loss: 0.00065453
Iteration 22/25 | Loss: 0.00065453
Iteration 23/25 | Loss: 0.00065453
Iteration 24/25 | Loss: 0.00065453
Iteration 25/25 | Loss: 0.00065453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.85872126
Iteration 2/25 | Loss: 0.00011325
Iteration 3/25 | Loss: 0.00011320
Iteration 4/25 | Loss: 0.00011320
Iteration 5/25 | Loss: 0.00011320
Iteration 6/25 | Loss: 0.00011320
Iteration 7/25 | Loss: 0.00011320
Iteration 8/25 | Loss: 0.00011320
Iteration 9/25 | Loss: 0.00011319
Iteration 10/25 | Loss: 0.00011319
Iteration 11/25 | Loss: 0.00011319
Iteration 12/25 | Loss: 0.00011319
Iteration 13/25 | Loss: 0.00011319
Iteration 14/25 | Loss: 0.00011319
Iteration 15/25 | Loss: 0.00011319
Iteration 16/25 | Loss: 0.00011319
Iteration 17/25 | Loss: 0.00011319
Iteration 18/25 | Loss: 0.00011319
Iteration 19/25 | Loss: 0.00011319
Iteration 20/25 | Loss: 0.00011319
Iteration 21/25 | Loss: 0.00011319
Iteration 22/25 | Loss: 0.00011319
Iteration 23/25 | Loss: 0.00011319
Iteration 24/25 | Loss: 0.00011319
Iteration 25/25 | Loss: 0.00011319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00011319
Iteration 2/1000 | Loss: 0.00003012
Iteration 3/1000 | Loss: 0.00002035
Iteration 4/1000 | Loss: 0.00001842
Iteration 5/1000 | Loss: 0.00001701
Iteration 6/1000 | Loss: 0.00001647
Iteration 7/1000 | Loss: 0.00001582
Iteration 8/1000 | Loss: 0.00001543
Iteration 9/1000 | Loss: 0.00001523
Iteration 10/1000 | Loss: 0.00001515
Iteration 11/1000 | Loss: 0.00001508
Iteration 12/1000 | Loss: 0.00001508
Iteration 13/1000 | Loss: 0.00001495
Iteration 14/1000 | Loss: 0.00001495
Iteration 15/1000 | Loss: 0.00001494
Iteration 16/1000 | Loss: 0.00001493
Iteration 17/1000 | Loss: 0.00001493
Iteration 18/1000 | Loss: 0.00001492
Iteration 19/1000 | Loss: 0.00001486
Iteration 20/1000 | Loss: 0.00001477
Iteration 21/1000 | Loss: 0.00001475
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001469
Iteration 24/1000 | Loss: 0.00001467
Iteration 25/1000 | Loss: 0.00001467
Iteration 26/1000 | Loss: 0.00001467
Iteration 27/1000 | Loss: 0.00001466
Iteration 28/1000 | Loss: 0.00001466
Iteration 29/1000 | Loss: 0.00001466
Iteration 30/1000 | Loss: 0.00001465
Iteration 31/1000 | Loss: 0.00001465
Iteration 32/1000 | Loss: 0.00001465
Iteration 33/1000 | Loss: 0.00001464
Iteration 34/1000 | Loss: 0.00001464
Iteration 35/1000 | Loss: 0.00001464
Iteration 36/1000 | Loss: 0.00001463
Iteration 37/1000 | Loss: 0.00001463
Iteration 38/1000 | Loss: 0.00001463
Iteration 39/1000 | Loss: 0.00001462
Iteration 40/1000 | Loss: 0.00001462
Iteration 41/1000 | Loss: 0.00001462
Iteration 42/1000 | Loss: 0.00001462
Iteration 43/1000 | Loss: 0.00001461
Iteration 44/1000 | Loss: 0.00001461
Iteration 45/1000 | Loss: 0.00001461
Iteration 46/1000 | Loss: 0.00001461
Iteration 47/1000 | Loss: 0.00001461
Iteration 48/1000 | Loss: 0.00001460
Iteration 49/1000 | Loss: 0.00001460
Iteration 50/1000 | Loss: 0.00001460
Iteration 51/1000 | Loss: 0.00001460
Iteration 52/1000 | Loss: 0.00001459
Iteration 53/1000 | Loss: 0.00001459
Iteration 54/1000 | Loss: 0.00001458
Iteration 55/1000 | Loss: 0.00001458
Iteration 56/1000 | Loss: 0.00001458
Iteration 57/1000 | Loss: 0.00001458
Iteration 58/1000 | Loss: 0.00001457
Iteration 59/1000 | Loss: 0.00001457
Iteration 60/1000 | Loss: 0.00001457
Iteration 61/1000 | Loss: 0.00001457
Iteration 62/1000 | Loss: 0.00001457
Iteration 63/1000 | Loss: 0.00001457
Iteration 64/1000 | Loss: 0.00001457
Iteration 65/1000 | Loss: 0.00001456
Iteration 66/1000 | Loss: 0.00001456
Iteration 67/1000 | Loss: 0.00001455
Iteration 68/1000 | Loss: 0.00001455
Iteration 69/1000 | Loss: 0.00001455
Iteration 70/1000 | Loss: 0.00001455
Iteration 71/1000 | Loss: 0.00001454
Iteration 72/1000 | Loss: 0.00001454
Iteration 73/1000 | Loss: 0.00001454
Iteration 74/1000 | Loss: 0.00001454
Iteration 75/1000 | Loss: 0.00001454
Iteration 76/1000 | Loss: 0.00001454
Iteration 77/1000 | Loss: 0.00001454
Iteration 78/1000 | Loss: 0.00001453
Iteration 79/1000 | Loss: 0.00001453
Iteration 80/1000 | Loss: 0.00001452
Iteration 81/1000 | Loss: 0.00001452
Iteration 82/1000 | Loss: 0.00001452
Iteration 83/1000 | Loss: 0.00001452
Iteration 84/1000 | Loss: 0.00001451
Iteration 85/1000 | Loss: 0.00001451
Iteration 86/1000 | Loss: 0.00001451
Iteration 87/1000 | Loss: 0.00001451
Iteration 88/1000 | Loss: 0.00001451
Iteration 89/1000 | Loss: 0.00001451
Iteration 90/1000 | Loss: 0.00001451
Iteration 91/1000 | Loss: 0.00001451
Iteration 92/1000 | Loss: 0.00001451
Iteration 93/1000 | Loss: 0.00001451
Iteration 94/1000 | Loss: 0.00001451
Iteration 95/1000 | Loss: 0.00001451
Iteration 96/1000 | Loss: 0.00001451
Iteration 97/1000 | Loss: 0.00001451
Iteration 98/1000 | Loss: 0.00001451
Iteration 99/1000 | Loss: 0.00001451
Iteration 100/1000 | Loss: 0.00001450
Iteration 101/1000 | Loss: 0.00001450
Iteration 102/1000 | Loss: 0.00001450
Iteration 103/1000 | Loss: 0.00001450
Iteration 104/1000 | Loss: 0.00001450
Iteration 105/1000 | Loss: 0.00001450
Iteration 106/1000 | Loss: 0.00001449
Iteration 107/1000 | Loss: 0.00001449
Iteration 108/1000 | Loss: 0.00001449
Iteration 109/1000 | Loss: 0.00001449
Iteration 110/1000 | Loss: 0.00001449
Iteration 111/1000 | Loss: 0.00001449
Iteration 112/1000 | Loss: 0.00001449
Iteration 113/1000 | Loss: 0.00001449
Iteration 114/1000 | Loss: 0.00001449
Iteration 115/1000 | Loss: 0.00001449
Iteration 116/1000 | Loss: 0.00001449
Iteration 117/1000 | Loss: 0.00001449
Iteration 118/1000 | Loss: 0.00001449
Iteration 119/1000 | Loss: 0.00001449
Iteration 120/1000 | Loss: 0.00001448
Iteration 121/1000 | Loss: 0.00001448
Iteration 122/1000 | Loss: 0.00001448
Iteration 123/1000 | Loss: 0.00001448
Iteration 124/1000 | Loss: 0.00001448
Iteration 125/1000 | Loss: 0.00001448
Iteration 126/1000 | Loss: 0.00001448
Iteration 127/1000 | Loss: 0.00001448
Iteration 128/1000 | Loss: 0.00001448
Iteration 129/1000 | Loss: 0.00001448
Iteration 130/1000 | Loss: 0.00001448
Iteration 131/1000 | Loss: 0.00001448
Iteration 132/1000 | Loss: 0.00001448
Iteration 133/1000 | Loss: 0.00001448
Iteration 134/1000 | Loss: 0.00001448
Iteration 135/1000 | Loss: 0.00001448
Iteration 136/1000 | Loss: 0.00001448
Iteration 137/1000 | Loss: 0.00001448
Iteration 138/1000 | Loss: 0.00001448
Iteration 139/1000 | Loss: 0.00001448
Iteration 140/1000 | Loss: 0.00001448
Iteration 141/1000 | Loss: 0.00001448
Iteration 142/1000 | Loss: 0.00001448
Iteration 143/1000 | Loss: 0.00001448
Iteration 144/1000 | Loss: 0.00001448
Iteration 145/1000 | Loss: 0.00001448
Iteration 146/1000 | Loss: 0.00001448
Iteration 147/1000 | Loss: 0.00001448
Iteration 148/1000 | Loss: 0.00001448
Iteration 149/1000 | Loss: 0.00001448
Iteration 150/1000 | Loss: 0.00001448
Iteration 151/1000 | Loss: 0.00001448
Iteration 152/1000 | Loss: 0.00001448
Iteration 153/1000 | Loss: 0.00001448
Iteration 154/1000 | Loss: 0.00001448
Iteration 155/1000 | Loss: 0.00001448
Iteration 156/1000 | Loss: 0.00001448
Iteration 157/1000 | Loss: 0.00001448
Iteration 158/1000 | Loss: 0.00001448
Iteration 159/1000 | Loss: 0.00001448
Iteration 160/1000 | Loss: 0.00001448
Iteration 161/1000 | Loss: 0.00001448
Iteration 162/1000 | Loss: 0.00001448
Iteration 163/1000 | Loss: 0.00001448
Iteration 164/1000 | Loss: 0.00001448
Iteration 165/1000 | Loss: 0.00001448
Iteration 166/1000 | Loss: 0.00001448
Iteration 167/1000 | Loss: 0.00001448
Iteration 168/1000 | Loss: 0.00001448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.4481910511676688e-05, 1.4481910511676688e-05, 1.4481910511676688e-05, 1.4481910511676688e-05, 1.4481910511676688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4481910511676688e-05

Optimization complete. Final v2v error: 3.2280161380767822 mm

Highest mean error: 3.791743755340576 mm for frame 72

Lowest mean error: 2.82364821434021 mm for frame 117

Saving results

Total time: 37.38418769836426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043397
Iteration 2/25 | Loss: 0.00466702
Iteration 3/25 | Loss: 0.00316550
Iteration 4/25 | Loss: 0.00215144
Iteration 5/25 | Loss: 0.00187058
Iteration 6/25 | Loss: 0.00178223
Iteration 7/25 | Loss: 0.00175143
Iteration 8/25 | Loss: 0.00185794
Iteration 9/25 | Loss: 0.00162100
Iteration 10/25 | Loss: 0.00154031
Iteration 11/25 | Loss: 0.00147864
Iteration 12/25 | Loss: 0.00142883
Iteration 13/25 | Loss: 0.00138137
Iteration 14/25 | Loss: 0.00134003
Iteration 15/25 | Loss: 0.00131139
Iteration 16/25 | Loss: 0.00127479
Iteration 17/25 | Loss: 0.00126072
Iteration 18/25 | Loss: 0.00124340
Iteration 19/25 | Loss: 0.00122083
Iteration 20/25 | Loss: 0.00121271
Iteration 21/25 | Loss: 0.00120291
Iteration 22/25 | Loss: 0.00120486
Iteration 23/25 | Loss: 0.00119881
Iteration 24/25 | Loss: 0.00120404
Iteration 25/25 | Loss: 0.00119462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35167027
Iteration 2/25 | Loss: 0.00484503
Iteration 3/25 | Loss: 0.00349490
Iteration 4/25 | Loss: 0.00349490
Iteration 5/25 | Loss: 0.00349490
Iteration 6/25 | Loss: 0.00349490
Iteration 7/25 | Loss: 0.00349490
Iteration 8/25 | Loss: 0.00349490
Iteration 9/25 | Loss: 0.00349490
Iteration 10/25 | Loss: 0.00349490
Iteration 11/25 | Loss: 0.00349490
Iteration 12/25 | Loss: 0.00349490
Iteration 13/25 | Loss: 0.00349490
Iteration 14/25 | Loss: 0.00349490
Iteration 15/25 | Loss: 0.00349490
Iteration 16/25 | Loss: 0.00349490
Iteration 17/25 | Loss: 0.00349490
Iteration 18/25 | Loss: 0.00349490
Iteration 19/25 | Loss: 0.00349490
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0034948966931551695, 0.0034948966931551695, 0.0034948966931551695, 0.0034948966931551695, 0.0034948966931551695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034948966931551695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00349490
Iteration 2/1000 | Loss: 0.00546295
Iteration 3/1000 | Loss: 0.00780536
Iteration 4/1000 | Loss: 0.00096198
Iteration 5/1000 | Loss: 0.00118563
Iteration 6/1000 | Loss: 0.00445684
Iteration 7/1000 | Loss: 0.00100013
Iteration 8/1000 | Loss: 0.00069326
Iteration 9/1000 | Loss: 0.00123131
Iteration 10/1000 | Loss: 0.00145924
Iteration 11/1000 | Loss: 0.00286771
Iteration 12/1000 | Loss: 0.00062938
Iteration 13/1000 | Loss: 0.00034500
Iteration 14/1000 | Loss: 0.00033293
Iteration 15/1000 | Loss: 0.00032433
Iteration 16/1000 | Loss: 0.00060585
Iteration 17/1000 | Loss: 0.00047983
Iteration 18/1000 | Loss: 0.00041480
Iteration 19/1000 | Loss: 0.00098536
Iteration 20/1000 | Loss: 0.00271700
Iteration 21/1000 | Loss: 0.00329327
Iteration 22/1000 | Loss: 0.00083825
Iteration 23/1000 | Loss: 0.00100490
Iteration 24/1000 | Loss: 0.00091593
Iteration 25/1000 | Loss: 0.00088921
Iteration 26/1000 | Loss: 0.00018410
Iteration 27/1000 | Loss: 0.00101186
Iteration 28/1000 | Loss: 0.00011157
Iteration 29/1000 | Loss: 0.00097457
Iteration 30/1000 | Loss: 0.00062821
Iteration 31/1000 | Loss: 0.00103601
Iteration 32/1000 | Loss: 0.00016096
Iteration 33/1000 | Loss: 0.00078462
Iteration 34/1000 | Loss: 0.00058690
Iteration 35/1000 | Loss: 0.00086475
Iteration 36/1000 | Loss: 0.00026883
Iteration 37/1000 | Loss: 0.00095237
Iteration 38/1000 | Loss: 0.00072969
Iteration 39/1000 | Loss: 0.00027099
Iteration 40/1000 | Loss: 0.00056105
Iteration 41/1000 | Loss: 0.00007795
Iteration 42/1000 | Loss: 0.00055064
Iteration 43/1000 | Loss: 0.00021694
Iteration 44/1000 | Loss: 0.00059999
Iteration 45/1000 | Loss: 0.00102289
Iteration 46/1000 | Loss: 0.00096165
Iteration 47/1000 | Loss: 0.00114213
Iteration 48/1000 | Loss: 0.00045104
Iteration 49/1000 | Loss: 0.00039699
Iteration 50/1000 | Loss: 0.00070096
Iteration 51/1000 | Loss: 0.00057886
Iteration 52/1000 | Loss: 0.00049649
Iteration 53/1000 | Loss: 0.00074788
Iteration 54/1000 | Loss: 0.00031785
Iteration 55/1000 | Loss: 0.00075174
Iteration 56/1000 | Loss: 0.00132750
Iteration 57/1000 | Loss: 0.00125237
Iteration 58/1000 | Loss: 0.00087893
Iteration 59/1000 | Loss: 0.00098942
Iteration 60/1000 | Loss: 0.00061630
Iteration 61/1000 | Loss: 0.00019154
Iteration 62/1000 | Loss: 0.00026597
Iteration 63/1000 | Loss: 0.00030203
Iteration 64/1000 | Loss: 0.00063919
Iteration 65/1000 | Loss: 0.00016931
Iteration 66/1000 | Loss: 0.00029149
Iteration 67/1000 | Loss: 0.00012896
Iteration 68/1000 | Loss: 0.00006150
Iteration 69/1000 | Loss: 0.00028412
Iteration 70/1000 | Loss: 0.00023604
Iteration 71/1000 | Loss: 0.00005946
Iteration 72/1000 | Loss: 0.00007429
Iteration 73/1000 | Loss: 0.00037922
Iteration 74/1000 | Loss: 0.00080538
Iteration 75/1000 | Loss: 0.00042234
Iteration 76/1000 | Loss: 0.00016798
Iteration 77/1000 | Loss: 0.00004877
Iteration 78/1000 | Loss: 0.00006587
Iteration 79/1000 | Loss: 0.00039079
Iteration 80/1000 | Loss: 0.00047113
Iteration 81/1000 | Loss: 0.00054182
Iteration 82/1000 | Loss: 0.00037740
Iteration 83/1000 | Loss: 0.00005302
Iteration 84/1000 | Loss: 0.00055031
Iteration 85/1000 | Loss: 0.00016332
Iteration 86/1000 | Loss: 0.00018469
Iteration 87/1000 | Loss: 0.00005899
Iteration 88/1000 | Loss: 0.00007460
Iteration 89/1000 | Loss: 0.00046113
Iteration 90/1000 | Loss: 0.00006442
Iteration 91/1000 | Loss: 0.00006210
Iteration 92/1000 | Loss: 0.00005322
Iteration 93/1000 | Loss: 0.00007010
Iteration 94/1000 | Loss: 0.00008816
Iteration 95/1000 | Loss: 0.00004788
Iteration 96/1000 | Loss: 0.00006021
Iteration 97/1000 | Loss: 0.00006165
Iteration 98/1000 | Loss: 0.00003879
Iteration 99/1000 | Loss: 0.00013129
Iteration 100/1000 | Loss: 0.00006877
Iteration 101/1000 | Loss: 0.00004827
Iteration 102/1000 | Loss: 0.00003924
Iteration 103/1000 | Loss: 0.00006240
Iteration 104/1000 | Loss: 0.00005564
Iteration 105/1000 | Loss: 0.00008382
Iteration 106/1000 | Loss: 0.00010705
Iteration 107/1000 | Loss: 0.00007460
Iteration 108/1000 | Loss: 0.00009727
Iteration 109/1000 | Loss: 0.00005139
Iteration 110/1000 | Loss: 0.00003820
Iteration 111/1000 | Loss: 0.00004206
Iteration 112/1000 | Loss: 0.00020435
Iteration 113/1000 | Loss: 0.00038129
Iteration 114/1000 | Loss: 0.00009061
Iteration 115/1000 | Loss: 0.00034439
Iteration 116/1000 | Loss: 0.00005894
Iteration 117/1000 | Loss: 0.00005387
Iteration 118/1000 | Loss: 0.00018795
Iteration 119/1000 | Loss: 0.00029174
Iteration 120/1000 | Loss: 0.00018229
Iteration 121/1000 | Loss: 0.00013771
Iteration 122/1000 | Loss: 0.00028897
Iteration 123/1000 | Loss: 0.00021329
Iteration 124/1000 | Loss: 0.00004509
Iteration 125/1000 | Loss: 0.00004916
Iteration 126/1000 | Loss: 0.00003809
Iteration 127/1000 | Loss: 0.00008363
Iteration 128/1000 | Loss: 0.00003956
Iteration 129/1000 | Loss: 0.00004191
Iteration 130/1000 | Loss: 0.00003739
Iteration 131/1000 | Loss: 0.00003739
Iteration 132/1000 | Loss: 0.00003739
Iteration 133/1000 | Loss: 0.00003739
Iteration 134/1000 | Loss: 0.00003739
Iteration 135/1000 | Loss: 0.00003739
Iteration 136/1000 | Loss: 0.00003739
Iteration 137/1000 | Loss: 0.00003739
Iteration 138/1000 | Loss: 0.00003739
Iteration 139/1000 | Loss: 0.00003738
Iteration 140/1000 | Loss: 0.00003737
Iteration 141/1000 | Loss: 0.00003737
Iteration 142/1000 | Loss: 0.00003737
Iteration 143/1000 | Loss: 0.00003736
Iteration 144/1000 | Loss: 0.00003736
Iteration 145/1000 | Loss: 0.00003735
Iteration 146/1000 | Loss: 0.00003735
Iteration 147/1000 | Loss: 0.00003735
Iteration 148/1000 | Loss: 0.00003734
Iteration 149/1000 | Loss: 0.00003733
Iteration 150/1000 | Loss: 0.00005135
Iteration 151/1000 | Loss: 0.00004116
Iteration 152/1000 | Loss: 0.00005232
Iteration 153/1000 | Loss: 0.00005231
Iteration 154/1000 | Loss: 0.00014752
Iteration 155/1000 | Loss: 0.00007230
Iteration 156/1000 | Loss: 0.00014702
Iteration 157/1000 | Loss: 0.00008073
Iteration 158/1000 | Loss: 0.00005595
Iteration 159/1000 | Loss: 0.00007471
Iteration 160/1000 | Loss: 0.00003829
Iteration 161/1000 | Loss: 0.00005622
Iteration 162/1000 | Loss: 0.00007564
Iteration 163/1000 | Loss: 0.00010899
Iteration 164/1000 | Loss: 0.00011187
Iteration 165/1000 | Loss: 0.00007170
Iteration 166/1000 | Loss: 0.00016414
Iteration 167/1000 | Loss: 0.00003916
Iteration 168/1000 | Loss: 0.00008352
Iteration 169/1000 | Loss: 0.00009581
Iteration 170/1000 | Loss: 0.00005183
Iteration 171/1000 | Loss: 0.00005515
Iteration 172/1000 | Loss: 0.00008254
Iteration 173/1000 | Loss: 0.00029869
Iteration 174/1000 | Loss: 0.00020865
Iteration 175/1000 | Loss: 0.00004922
Iteration 176/1000 | Loss: 0.00007402
Iteration 177/1000 | Loss: 0.00005053
Iteration 178/1000 | Loss: 0.00006615
Iteration 179/1000 | Loss: 0.00007758
Iteration 180/1000 | Loss: 0.00004874
Iteration 181/1000 | Loss: 0.00006064
Iteration 182/1000 | Loss: 0.00004840
Iteration 183/1000 | Loss: 0.00008740
Iteration 184/1000 | Loss: 0.00007332
Iteration 185/1000 | Loss: 0.00005000
Iteration 186/1000 | Loss: 0.00005185
Iteration 187/1000 | Loss: 0.00009902
Iteration 188/1000 | Loss: 0.00010763
Iteration 189/1000 | Loss: 0.00019835
Iteration 190/1000 | Loss: 0.00010614
Iteration 191/1000 | Loss: 0.00005397
Iteration 192/1000 | Loss: 0.00015653
Iteration 193/1000 | Loss: 0.00010927
Iteration 194/1000 | Loss: 0.00007951
Iteration 195/1000 | Loss: 0.00025625
Iteration 196/1000 | Loss: 0.00003855
Iteration 197/1000 | Loss: 0.00006000
Iteration 198/1000 | Loss: 0.00003843
Iteration 199/1000 | Loss: 0.00003869
Iteration 200/1000 | Loss: 0.00005760
Iteration 201/1000 | Loss: 0.00004363
Iteration 202/1000 | Loss: 0.00005217
Iteration 203/1000 | Loss: 0.00003588
Iteration 204/1000 | Loss: 0.00003582
Iteration 205/1000 | Loss: 0.00003582
Iteration 206/1000 | Loss: 0.00007269
Iteration 207/1000 | Loss: 0.00007269
Iteration 208/1000 | Loss: 0.00012260
Iteration 209/1000 | Loss: 0.00004919
Iteration 210/1000 | Loss: 0.00006423
Iteration 211/1000 | Loss: 0.00005047
Iteration 212/1000 | Loss: 0.00003766
Iteration 213/1000 | Loss: 0.00003800
Iteration 214/1000 | Loss: 0.00008517
Iteration 215/1000 | Loss: 0.00019007
Iteration 216/1000 | Loss: 0.00006679
Iteration 217/1000 | Loss: 0.00003919
Iteration 218/1000 | Loss: 0.00003586
Iteration 219/1000 | Loss: 0.00005422
Iteration 220/1000 | Loss: 0.00003894
Iteration 221/1000 | Loss: 0.00003570
Iteration 222/1000 | Loss: 0.00003570
Iteration 223/1000 | Loss: 0.00003570
Iteration 224/1000 | Loss: 0.00004163
Iteration 225/1000 | Loss: 0.00003569
Iteration 226/1000 | Loss: 0.00003569
Iteration 227/1000 | Loss: 0.00003569
Iteration 228/1000 | Loss: 0.00003569
Iteration 229/1000 | Loss: 0.00003569
Iteration 230/1000 | Loss: 0.00003569
Iteration 231/1000 | Loss: 0.00003569
Iteration 232/1000 | Loss: 0.00003569
Iteration 233/1000 | Loss: 0.00003568
Iteration 234/1000 | Loss: 0.00003568
Iteration 235/1000 | Loss: 0.00003568
Iteration 236/1000 | Loss: 0.00003568
Iteration 237/1000 | Loss: 0.00003568
Iteration 238/1000 | Loss: 0.00003568
Iteration 239/1000 | Loss: 0.00003568
Iteration 240/1000 | Loss: 0.00003568
Iteration 241/1000 | Loss: 0.00003568
Iteration 242/1000 | Loss: 0.00003568
Iteration 243/1000 | Loss: 0.00003568
Iteration 244/1000 | Loss: 0.00003568
Iteration 245/1000 | Loss: 0.00003568
Iteration 246/1000 | Loss: 0.00003568
Iteration 247/1000 | Loss: 0.00003568
Iteration 248/1000 | Loss: 0.00003568
Iteration 249/1000 | Loss: 0.00003568
Iteration 250/1000 | Loss: 0.00003568
Iteration 251/1000 | Loss: 0.00003568
Iteration 252/1000 | Loss: 0.00003568
Iteration 253/1000 | Loss: 0.00003568
Iteration 254/1000 | Loss: 0.00003568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [3.568033571355045e-05, 3.568033571355045e-05, 3.568033571355045e-05, 3.568033571355045e-05, 3.568033571355045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.568033571355045e-05

Optimization complete. Final v2v error: 4.621613502502441 mm

Highest mean error: 13.326333045959473 mm for frame 67

Lowest mean error: 4.106156826019287 mm for frame 123

Saving results

Total time: 369.9887571334839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00201596
Iteration 2/25 | Loss: 0.00083814
Iteration 3/25 | Loss: 0.00072894
Iteration 4/25 | Loss: 0.00070218
Iteration 5/25 | Loss: 0.00069569
Iteration 6/25 | Loss: 0.00069430
Iteration 7/25 | Loss: 0.00069430
Iteration 8/25 | Loss: 0.00069430
Iteration 9/25 | Loss: 0.00069430
Iteration 10/25 | Loss: 0.00069430
Iteration 11/25 | Loss: 0.00069430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006942950421944261, 0.0006942950421944261, 0.0006942950421944261, 0.0006942950421944261, 0.0006942950421944261]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006942950421944261

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39561057
Iteration 2/25 | Loss: 0.00027598
Iteration 3/25 | Loss: 0.00027598
Iteration 4/25 | Loss: 0.00027598
Iteration 5/25 | Loss: 0.00027598
Iteration 6/25 | Loss: 0.00027598
Iteration 7/25 | Loss: 0.00027598
Iteration 8/25 | Loss: 0.00027598
Iteration 9/25 | Loss: 0.00027598
Iteration 10/25 | Loss: 0.00027598
Iteration 11/25 | Loss: 0.00027598
Iteration 12/25 | Loss: 0.00027598
Iteration 13/25 | Loss: 0.00027598
Iteration 14/25 | Loss: 0.00027598
Iteration 15/25 | Loss: 0.00027598
Iteration 16/25 | Loss: 0.00027598
Iteration 17/25 | Loss: 0.00027598
Iteration 18/25 | Loss: 0.00027598
Iteration 19/25 | Loss: 0.00027598
Iteration 20/25 | Loss: 0.00027598
Iteration 21/25 | Loss: 0.00027598
Iteration 22/25 | Loss: 0.00027598
Iteration 23/25 | Loss: 0.00027598
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002759785857051611, 0.0002759785857051611, 0.0002759785857051611, 0.0002759785857051611, 0.0002759785857051611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002759785857051611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027598
Iteration 2/1000 | Loss: 0.00005046
Iteration 3/1000 | Loss: 0.00002439
Iteration 4/1000 | Loss: 0.00001927
Iteration 5/1000 | Loss: 0.00001805
Iteration 6/1000 | Loss: 0.00001716
Iteration 7/1000 | Loss: 0.00001673
Iteration 8/1000 | Loss: 0.00001629
Iteration 9/1000 | Loss: 0.00001595
Iteration 10/1000 | Loss: 0.00001575
Iteration 11/1000 | Loss: 0.00001566
Iteration 12/1000 | Loss: 0.00001566
Iteration 13/1000 | Loss: 0.00001553
Iteration 14/1000 | Loss: 0.00001551
Iteration 15/1000 | Loss: 0.00001545
Iteration 16/1000 | Loss: 0.00001544
Iteration 17/1000 | Loss: 0.00001544
Iteration 18/1000 | Loss: 0.00001544
Iteration 19/1000 | Loss: 0.00001544
Iteration 20/1000 | Loss: 0.00001543
Iteration 21/1000 | Loss: 0.00001542
Iteration 22/1000 | Loss: 0.00001535
Iteration 23/1000 | Loss: 0.00001535
Iteration 24/1000 | Loss: 0.00001531
Iteration 25/1000 | Loss: 0.00001529
Iteration 26/1000 | Loss: 0.00001523
Iteration 27/1000 | Loss: 0.00001523
Iteration 28/1000 | Loss: 0.00001523
Iteration 29/1000 | Loss: 0.00001522
Iteration 30/1000 | Loss: 0.00001522
Iteration 31/1000 | Loss: 0.00001521
Iteration 32/1000 | Loss: 0.00001521
Iteration 33/1000 | Loss: 0.00001518
Iteration 34/1000 | Loss: 0.00001518
Iteration 35/1000 | Loss: 0.00001518
Iteration 36/1000 | Loss: 0.00001517
Iteration 37/1000 | Loss: 0.00001517
Iteration 38/1000 | Loss: 0.00001517
Iteration 39/1000 | Loss: 0.00001517
Iteration 40/1000 | Loss: 0.00001516
Iteration 41/1000 | Loss: 0.00001516
Iteration 42/1000 | Loss: 0.00001516
Iteration 43/1000 | Loss: 0.00001515
Iteration 44/1000 | Loss: 0.00001515
Iteration 45/1000 | Loss: 0.00001515
Iteration 46/1000 | Loss: 0.00001514
Iteration 47/1000 | Loss: 0.00001514
Iteration 48/1000 | Loss: 0.00001514
Iteration 49/1000 | Loss: 0.00001514
Iteration 50/1000 | Loss: 0.00001513
Iteration 51/1000 | Loss: 0.00001513
Iteration 52/1000 | Loss: 0.00001513
Iteration 53/1000 | Loss: 0.00001513
Iteration 54/1000 | Loss: 0.00001513
Iteration 55/1000 | Loss: 0.00001512
Iteration 56/1000 | Loss: 0.00001512
Iteration 57/1000 | Loss: 0.00001512
Iteration 58/1000 | Loss: 0.00001512
Iteration 59/1000 | Loss: 0.00001512
Iteration 60/1000 | Loss: 0.00001512
Iteration 61/1000 | Loss: 0.00001512
Iteration 62/1000 | Loss: 0.00001512
Iteration 63/1000 | Loss: 0.00001512
Iteration 64/1000 | Loss: 0.00001512
Iteration 65/1000 | Loss: 0.00001511
Iteration 66/1000 | Loss: 0.00001511
Iteration 67/1000 | Loss: 0.00001511
Iteration 68/1000 | Loss: 0.00001511
Iteration 69/1000 | Loss: 0.00001511
Iteration 70/1000 | Loss: 0.00001510
Iteration 71/1000 | Loss: 0.00001510
Iteration 72/1000 | Loss: 0.00001510
Iteration 73/1000 | Loss: 0.00001510
Iteration 74/1000 | Loss: 0.00001510
Iteration 75/1000 | Loss: 0.00001510
Iteration 76/1000 | Loss: 0.00001510
Iteration 77/1000 | Loss: 0.00001510
Iteration 78/1000 | Loss: 0.00001510
Iteration 79/1000 | Loss: 0.00001510
Iteration 80/1000 | Loss: 0.00001509
Iteration 81/1000 | Loss: 0.00001509
Iteration 82/1000 | Loss: 0.00001509
Iteration 83/1000 | Loss: 0.00001509
Iteration 84/1000 | Loss: 0.00001509
Iteration 85/1000 | Loss: 0.00001509
Iteration 86/1000 | Loss: 0.00001509
Iteration 87/1000 | Loss: 0.00001508
Iteration 88/1000 | Loss: 0.00001508
Iteration 89/1000 | Loss: 0.00001508
Iteration 90/1000 | Loss: 0.00001508
Iteration 91/1000 | Loss: 0.00001508
Iteration 92/1000 | Loss: 0.00001508
Iteration 93/1000 | Loss: 0.00001508
Iteration 94/1000 | Loss: 0.00001508
Iteration 95/1000 | Loss: 0.00001508
Iteration 96/1000 | Loss: 0.00001508
Iteration 97/1000 | Loss: 0.00001508
Iteration 98/1000 | Loss: 0.00001508
Iteration 99/1000 | Loss: 0.00001508
Iteration 100/1000 | Loss: 0.00001508
Iteration 101/1000 | Loss: 0.00001507
Iteration 102/1000 | Loss: 0.00001507
Iteration 103/1000 | Loss: 0.00001507
Iteration 104/1000 | Loss: 0.00001507
Iteration 105/1000 | Loss: 0.00001507
Iteration 106/1000 | Loss: 0.00001507
Iteration 107/1000 | Loss: 0.00001506
Iteration 108/1000 | Loss: 0.00001506
Iteration 109/1000 | Loss: 0.00001506
Iteration 110/1000 | Loss: 0.00001506
Iteration 111/1000 | Loss: 0.00001506
Iteration 112/1000 | Loss: 0.00001506
Iteration 113/1000 | Loss: 0.00001506
Iteration 114/1000 | Loss: 0.00001506
Iteration 115/1000 | Loss: 0.00001506
Iteration 116/1000 | Loss: 0.00001506
Iteration 117/1000 | Loss: 0.00001506
Iteration 118/1000 | Loss: 0.00001506
Iteration 119/1000 | Loss: 0.00001506
Iteration 120/1000 | Loss: 0.00001506
Iteration 121/1000 | Loss: 0.00001506
Iteration 122/1000 | Loss: 0.00001505
Iteration 123/1000 | Loss: 0.00001505
Iteration 124/1000 | Loss: 0.00001505
Iteration 125/1000 | Loss: 0.00001505
Iteration 126/1000 | Loss: 0.00001505
Iteration 127/1000 | Loss: 0.00001505
Iteration 128/1000 | Loss: 0.00001505
Iteration 129/1000 | Loss: 0.00001505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.5054881259857211e-05, 1.5054881259857211e-05, 1.5054881259857211e-05, 1.5054881259857211e-05, 1.5054881259857211e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5054881259857211e-05

Optimization complete. Final v2v error: 3.3352229595184326 mm

Highest mean error: 3.768836259841919 mm for frame 42

Lowest mean error: 2.97096586227417 mm for frame 188

Saving results

Total time: 36.14988613128662
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842564
Iteration 2/25 | Loss: 0.00078752
Iteration 3/25 | Loss: 0.00063952
Iteration 4/25 | Loss: 0.00062321
Iteration 5/25 | Loss: 0.00061982
Iteration 6/25 | Loss: 0.00061921
Iteration 7/25 | Loss: 0.00061921
Iteration 8/25 | Loss: 0.00061921
Iteration 9/25 | Loss: 0.00061921
Iteration 10/25 | Loss: 0.00061921
Iteration 11/25 | Loss: 0.00061921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006192088476382196, 0.0006192088476382196, 0.0006192088476382196, 0.0006192088476382196, 0.0006192088476382196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006192088476382196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40358686
Iteration 2/25 | Loss: 0.00012228
Iteration 3/25 | Loss: 0.00012228
Iteration 4/25 | Loss: 0.00012228
Iteration 5/25 | Loss: 0.00012228
Iteration 6/25 | Loss: 0.00012228
Iteration 7/25 | Loss: 0.00012228
Iteration 8/25 | Loss: 0.00012228
Iteration 9/25 | Loss: 0.00012228
Iteration 10/25 | Loss: 0.00012228
Iteration 11/25 | Loss: 0.00012228
Iteration 12/25 | Loss: 0.00012228
Iteration 13/25 | Loss: 0.00012228
Iteration 14/25 | Loss: 0.00012228
Iteration 15/25 | Loss: 0.00012228
Iteration 16/25 | Loss: 0.00012228
Iteration 17/25 | Loss: 0.00012228
Iteration 18/25 | Loss: 0.00012228
Iteration 19/25 | Loss: 0.00012228
Iteration 20/25 | Loss: 0.00012228
Iteration 21/25 | Loss: 0.00012228
Iteration 22/25 | Loss: 0.00012228
Iteration 23/25 | Loss: 0.00012228
Iteration 24/25 | Loss: 0.00012228
Iteration 25/25 | Loss: 0.00012228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00012228
Iteration 2/1000 | Loss: 0.00002547
Iteration 3/1000 | Loss: 0.00001716
Iteration 4/1000 | Loss: 0.00001570
Iteration 5/1000 | Loss: 0.00001473
Iteration 6/1000 | Loss: 0.00001406
Iteration 7/1000 | Loss: 0.00001360
Iteration 8/1000 | Loss: 0.00001332
Iteration 9/1000 | Loss: 0.00001311
Iteration 10/1000 | Loss: 0.00001306
Iteration 11/1000 | Loss: 0.00001292
Iteration 12/1000 | Loss: 0.00001290
Iteration 13/1000 | Loss: 0.00001286
Iteration 14/1000 | Loss: 0.00001284
Iteration 15/1000 | Loss: 0.00001273
Iteration 16/1000 | Loss: 0.00001271
Iteration 17/1000 | Loss: 0.00001269
Iteration 18/1000 | Loss: 0.00001265
Iteration 19/1000 | Loss: 0.00001265
Iteration 20/1000 | Loss: 0.00001264
Iteration 21/1000 | Loss: 0.00001264
Iteration 22/1000 | Loss: 0.00001264
Iteration 23/1000 | Loss: 0.00001263
Iteration 24/1000 | Loss: 0.00001262
Iteration 25/1000 | Loss: 0.00001260
Iteration 26/1000 | Loss: 0.00001259
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001258
Iteration 29/1000 | Loss: 0.00001258
Iteration 30/1000 | Loss: 0.00001257
Iteration 31/1000 | Loss: 0.00001257
Iteration 32/1000 | Loss: 0.00001256
Iteration 33/1000 | Loss: 0.00001255
Iteration 34/1000 | Loss: 0.00001255
Iteration 35/1000 | Loss: 0.00001254
Iteration 36/1000 | Loss: 0.00001247
Iteration 37/1000 | Loss: 0.00001246
Iteration 38/1000 | Loss: 0.00001245
Iteration 39/1000 | Loss: 0.00001244
Iteration 40/1000 | Loss: 0.00001241
Iteration 41/1000 | Loss: 0.00001241
Iteration 42/1000 | Loss: 0.00001241
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001241
Iteration 45/1000 | Loss: 0.00001241
Iteration 46/1000 | Loss: 0.00001241
Iteration 47/1000 | Loss: 0.00001240
Iteration 48/1000 | Loss: 0.00001240
Iteration 49/1000 | Loss: 0.00001240
Iteration 50/1000 | Loss: 0.00001238
Iteration 51/1000 | Loss: 0.00001238
Iteration 52/1000 | Loss: 0.00001237
Iteration 53/1000 | Loss: 0.00001235
Iteration 54/1000 | Loss: 0.00001235
Iteration 55/1000 | Loss: 0.00001234
Iteration 56/1000 | Loss: 0.00001234
Iteration 57/1000 | Loss: 0.00001234
Iteration 58/1000 | Loss: 0.00001233
Iteration 59/1000 | Loss: 0.00001233
Iteration 60/1000 | Loss: 0.00001233
Iteration 61/1000 | Loss: 0.00001232
Iteration 62/1000 | Loss: 0.00001232
Iteration 63/1000 | Loss: 0.00001232
Iteration 64/1000 | Loss: 0.00001230
Iteration 65/1000 | Loss: 0.00001230
Iteration 66/1000 | Loss: 0.00001230
Iteration 67/1000 | Loss: 0.00001229
Iteration 68/1000 | Loss: 0.00001229
Iteration 69/1000 | Loss: 0.00001229
Iteration 70/1000 | Loss: 0.00001229
Iteration 71/1000 | Loss: 0.00001229
Iteration 72/1000 | Loss: 0.00001229
Iteration 73/1000 | Loss: 0.00001229
Iteration 74/1000 | Loss: 0.00001229
Iteration 75/1000 | Loss: 0.00001229
Iteration 76/1000 | Loss: 0.00001229
Iteration 77/1000 | Loss: 0.00001229
Iteration 78/1000 | Loss: 0.00001229
Iteration 79/1000 | Loss: 0.00001229
Iteration 80/1000 | Loss: 0.00001229
Iteration 81/1000 | Loss: 0.00001228
Iteration 82/1000 | Loss: 0.00001228
Iteration 83/1000 | Loss: 0.00001228
Iteration 84/1000 | Loss: 0.00001228
Iteration 85/1000 | Loss: 0.00001228
Iteration 86/1000 | Loss: 0.00001228
Iteration 87/1000 | Loss: 0.00001228
Iteration 88/1000 | Loss: 0.00001228
Iteration 89/1000 | Loss: 0.00001228
Iteration 90/1000 | Loss: 0.00001228
Iteration 91/1000 | Loss: 0.00001228
Iteration 92/1000 | Loss: 0.00001228
Iteration 93/1000 | Loss: 0.00001227
Iteration 94/1000 | Loss: 0.00001227
Iteration 95/1000 | Loss: 0.00001227
Iteration 96/1000 | Loss: 0.00001227
Iteration 97/1000 | Loss: 0.00001227
Iteration 98/1000 | Loss: 0.00001227
Iteration 99/1000 | Loss: 0.00001227
Iteration 100/1000 | Loss: 0.00001227
Iteration 101/1000 | Loss: 0.00001227
Iteration 102/1000 | Loss: 0.00001227
Iteration 103/1000 | Loss: 0.00001227
Iteration 104/1000 | Loss: 0.00001227
Iteration 105/1000 | Loss: 0.00001227
Iteration 106/1000 | Loss: 0.00001227
Iteration 107/1000 | Loss: 0.00001227
Iteration 108/1000 | Loss: 0.00001227
Iteration 109/1000 | Loss: 0.00001227
Iteration 110/1000 | Loss: 0.00001226
Iteration 111/1000 | Loss: 0.00001226
Iteration 112/1000 | Loss: 0.00001226
Iteration 113/1000 | Loss: 0.00001226
Iteration 114/1000 | Loss: 0.00001226
Iteration 115/1000 | Loss: 0.00001226
Iteration 116/1000 | Loss: 0.00001226
Iteration 117/1000 | Loss: 0.00001226
Iteration 118/1000 | Loss: 0.00001226
Iteration 119/1000 | Loss: 0.00001226
Iteration 120/1000 | Loss: 0.00001225
Iteration 121/1000 | Loss: 0.00001225
Iteration 122/1000 | Loss: 0.00001225
Iteration 123/1000 | Loss: 0.00001225
Iteration 124/1000 | Loss: 0.00001225
Iteration 125/1000 | Loss: 0.00001225
Iteration 126/1000 | Loss: 0.00001225
Iteration 127/1000 | Loss: 0.00001225
Iteration 128/1000 | Loss: 0.00001225
Iteration 129/1000 | Loss: 0.00001225
Iteration 130/1000 | Loss: 0.00001225
Iteration 131/1000 | Loss: 0.00001225
Iteration 132/1000 | Loss: 0.00001225
Iteration 133/1000 | Loss: 0.00001225
Iteration 134/1000 | Loss: 0.00001225
Iteration 135/1000 | Loss: 0.00001225
Iteration 136/1000 | Loss: 0.00001225
Iteration 137/1000 | Loss: 0.00001225
Iteration 138/1000 | Loss: 0.00001225
Iteration 139/1000 | Loss: 0.00001225
Iteration 140/1000 | Loss: 0.00001225
Iteration 141/1000 | Loss: 0.00001225
Iteration 142/1000 | Loss: 0.00001224
Iteration 143/1000 | Loss: 0.00001224
Iteration 144/1000 | Loss: 0.00001224
Iteration 145/1000 | Loss: 0.00001224
Iteration 146/1000 | Loss: 0.00001224
Iteration 147/1000 | Loss: 0.00001224
Iteration 148/1000 | Loss: 0.00001223
Iteration 149/1000 | Loss: 0.00001223
Iteration 150/1000 | Loss: 0.00001223
Iteration 151/1000 | Loss: 0.00001223
Iteration 152/1000 | Loss: 0.00001222
Iteration 153/1000 | Loss: 0.00001222
Iteration 154/1000 | Loss: 0.00001222
Iteration 155/1000 | Loss: 0.00001221
Iteration 156/1000 | Loss: 0.00001221
Iteration 157/1000 | Loss: 0.00001221
Iteration 158/1000 | Loss: 0.00001221
Iteration 159/1000 | Loss: 0.00001221
Iteration 160/1000 | Loss: 0.00001220
Iteration 161/1000 | Loss: 0.00001220
Iteration 162/1000 | Loss: 0.00001220
Iteration 163/1000 | Loss: 0.00001220
Iteration 164/1000 | Loss: 0.00001220
Iteration 165/1000 | Loss: 0.00001220
Iteration 166/1000 | Loss: 0.00001220
Iteration 167/1000 | Loss: 0.00001220
Iteration 168/1000 | Loss: 0.00001220
Iteration 169/1000 | Loss: 0.00001219
Iteration 170/1000 | Loss: 0.00001219
Iteration 171/1000 | Loss: 0.00001219
Iteration 172/1000 | Loss: 0.00001219
Iteration 173/1000 | Loss: 0.00001219
Iteration 174/1000 | Loss: 0.00001219
Iteration 175/1000 | Loss: 0.00001218
Iteration 176/1000 | Loss: 0.00001218
Iteration 177/1000 | Loss: 0.00001218
Iteration 178/1000 | Loss: 0.00001218
Iteration 179/1000 | Loss: 0.00001218
Iteration 180/1000 | Loss: 0.00001218
Iteration 181/1000 | Loss: 0.00001218
Iteration 182/1000 | Loss: 0.00001218
Iteration 183/1000 | Loss: 0.00001218
Iteration 184/1000 | Loss: 0.00001218
Iteration 185/1000 | Loss: 0.00001218
Iteration 186/1000 | Loss: 0.00001218
Iteration 187/1000 | Loss: 0.00001218
Iteration 188/1000 | Loss: 0.00001218
Iteration 189/1000 | Loss: 0.00001218
Iteration 190/1000 | Loss: 0.00001218
Iteration 191/1000 | Loss: 0.00001218
Iteration 192/1000 | Loss: 0.00001218
Iteration 193/1000 | Loss: 0.00001218
Iteration 194/1000 | Loss: 0.00001218
Iteration 195/1000 | Loss: 0.00001218
Iteration 196/1000 | Loss: 0.00001218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.2179265468148515e-05, 1.2179265468148515e-05, 1.2179265468148515e-05, 1.2179265468148515e-05, 1.2179265468148515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2179265468148515e-05

Optimization complete. Final v2v error: 2.9608426094055176 mm

Highest mean error: 3.278494358062744 mm for frame 105

Lowest mean error: 2.6739110946655273 mm for frame 204

Saving results

Total time: 44.80491495132446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894138
Iteration 2/25 | Loss: 0.00129477
Iteration 3/25 | Loss: 0.00092110
Iteration 4/25 | Loss: 0.00085926
Iteration 5/25 | Loss: 0.00084862
Iteration 6/25 | Loss: 0.00084755
Iteration 7/25 | Loss: 0.00084798
Iteration 8/25 | Loss: 0.00084226
Iteration 9/25 | Loss: 0.00083697
Iteration 10/25 | Loss: 0.00083451
Iteration 11/25 | Loss: 0.00083376
Iteration 12/25 | Loss: 0.00083357
Iteration 13/25 | Loss: 0.00083354
Iteration 14/25 | Loss: 0.00083354
Iteration 15/25 | Loss: 0.00083352
Iteration 16/25 | Loss: 0.00083352
Iteration 17/25 | Loss: 0.00083352
Iteration 18/25 | Loss: 0.00083351
Iteration 19/25 | Loss: 0.00083351
Iteration 20/25 | Loss: 0.00083351
Iteration 21/25 | Loss: 0.00083351
Iteration 22/25 | Loss: 0.00083351
Iteration 23/25 | Loss: 0.00083351
Iteration 24/25 | Loss: 0.00083351
Iteration 25/25 | Loss: 0.00083351

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94977927
Iteration 2/25 | Loss: 0.00023891
Iteration 3/25 | Loss: 0.00023890
Iteration 4/25 | Loss: 0.00023890
Iteration 5/25 | Loss: 0.00023890
Iteration 6/25 | Loss: 0.00023890
Iteration 7/25 | Loss: 0.00023890
Iteration 8/25 | Loss: 0.00023890
Iteration 9/25 | Loss: 0.00023890
Iteration 10/25 | Loss: 0.00023890
Iteration 11/25 | Loss: 0.00023890
Iteration 12/25 | Loss: 0.00023890
Iteration 13/25 | Loss: 0.00023890
Iteration 14/25 | Loss: 0.00023890
Iteration 15/25 | Loss: 0.00023890
Iteration 16/25 | Loss: 0.00023890
Iteration 17/25 | Loss: 0.00023890
Iteration 18/25 | Loss: 0.00023890
Iteration 19/25 | Loss: 0.00023890
Iteration 20/25 | Loss: 0.00023890
Iteration 21/25 | Loss: 0.00023890
Iteration 22/25 | Loss: 0.00023890
Iteration 23/25 | Loss: 0.00023890
Iteration 24/25 | Loss: 0.00023890
Iteration 25/25 | Loss: 0.00023890

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023890
Iteration 2/1000 | Loss: 0.00006860
Iteration 3/1000 | Loss: 0.00005257
Iteration 4/1000 | Loss: 0.00004743
Iteration 5/1000 | Loss: 0.00004513
Iteration 6/1000 | Loss: 0.00004341
Iteration 7/1000 | Loss: 0.00004186
Iteration 8/1000 | Loss: 0.00004106
Iteration 9/1000 | Loss: 0.00004064
Iteration 10/1000 | Loss: 0.00004030
Iteration 11/1000 | Loss: 0.00004003
Iteration 12/1000 | Loss: 0.00003986
Iteration 13/1000 | Loss: 0.00003971
Iteration 14/1000 | Loss: 0.00003968
Iteration 15/1000 | Loss: 0.00003968
Iteration 16/1000 | Loss: 0.00003968
Iteration 17/1000 | Loss: 0.00003968
Iteration 18/1000 | Loss: 0.00003967
Iteration 19/1000 | Loss: 0.00003966
Iteration 20/1000 | Loss: 0.00003966
Iteration 21/1000 | Loss: 0.00003966
Iteration 22/1000 | Loss: 0.00003965
Iteration 23/1000 | Loss: 0.00003963
Iteration 24/1000 | Loss: 0.00003963
Iteration 25/1000 | Loss: 0.00003963
Iteration 26/1000 | Loss: 0.00003963
Iteration 27/1000 | Loss: 0.00003963
Iteration 28/1000 | Loss: 0.00003962
Iteration 29/1000 | Loss: 0.00003961
Iteration 30/1000 | Loss: 0.00003960
Iteration 31/1000 | Loss: 0.00003951
Iteration 32/1000 | Loss: 0.00003949
Iteration 33/1000 | Loss: 0.00003948
Iteration 34/1000 | Loss: 0.00003948
Iteration 35/1000 | Loss: 0.00003947
Iteration 36/1000 | Loss: 0.00003946
Iteration 37/1000 | Loss: 0.00003946
Iteration 38/1000 | Loss: 0.00003946
Iteration 39/1000 | Loss: 0.00003946
Iteration 40/1000 | Loss: 0.00003937
Iteration 41/1000 | Loss: 0.00003937
Iteration 42/1000 | Loss: 0.00003937
Iteration 43/1000 | Loss: 0.00003937
Iteration 44/1000 | Loss: 0.00003937
Iteration 45/1000 | Loss: 0.00003937
Iteration 46/1000 | Loss: 0.00003937
Iteration 47/1000 | Loss: 0.00003937
Iteration 48/1000 | Loss: 0.00003936
Iteration 49/1000 | Loss: 0.00003936
Iteration 50/1000 | Loss: 0.00003936
Iteration 51/1000 | Loss: 0.00003936
Iteration 52/1000 | Loss: 0.00003935
Iteration 53/1000 | Loss: 0.00003935
Iteration 54/1000 | Loss: 0.00003935
Iteration 55/1000 | Loss: 0.00003935
Iteration 56/1000 | Loss: 0.00003935
Iteration 57/1000 | Loss: 0.00003935
Iteration 58/1000 | Loss: 0.00003935
Iteration 59/1000 | Loss: 0.00003934
Iteration 60/1000 | Loss: 0.00003934
Iteration 61/1000 | Loss: 0.00003934
Iteration 62/1000 | Loss: 0.00003934
Iteration 63/1000 | Loss: 0.00003933
Iteration 64/1000 | Loss: 0.00003933
Iteration 65/1000 | Loss: 0.00003933
Iteration 66/1000 | Loss: 0.00003933
Iteration 67/1000 | Loss: 0.00003933
Iteration 68/1000 | Loss: 0.00003933
Iteration 69/1000 | Loss: 0.00003932
Iteration 70/1000 | Loss: 0.00003932
Iteration 71/1000 | Loss: 0.00003932
Iteration 72/1000 | Loss: 0.00003932
Iteration 73/1000 | Loss: 0.00003932
Iteration 74/1000 | Loss: 0.00003932
Iteration 75/1000 | Loss: 0.00003932
Iteration 76/1000 | Loss: 0.00003932
Iteration 77/1000 | Loss: 0.00003932
Iteration 78/1000 | Loss: 0.00003932
Iteration 79/1000 | Loss: 0.00003932
Iteration 80/1000 | Loss: 0.00003932
Iteration 81/1000 | Loss: 0.00003932
Iteration 82/1000 | Loss: 0.00003932
Iteration 83/1000 | Loss: 0.00003932
Iteration 84/1000 | Loss: 0.00003932
Iteration 85/1000 | Loss: 0.00003932
Iteration 86/1000 | Loss: 0.00003932
Iteration 87/1000 | Loss: 0.00003932
Iteration 88/1000 | Loss: 0.00003932
Iteration 89/1000 | Loss: 0.00003932
Iteration 90/1000 | Loss: 0.00003932
Iteration 91/1000 | Loss: 0.00003932
Iteration 92/1000 | Loss: 0.00003932
Iteration 93/1000 | Loss: 0.00003932
Iteration 94/1000 | Loss: 0.00003932
Iteration 95/1000 | Loss: 0.00003932
Iteration 96/1000 | Loss: 0.00003932
Iteration 97/1000 | Loss: 0.00003932
Iteration 98/1000 | Loss: 0.00003932
Iteration 99/1000 | Loss: 0.00003930
Iteration 100/1000 | Loss: 0.00003930
Iteration 101/1000 | Loss: 0.00003930
Iteration 102/1000 | Loss: 0.00003930
Iteration 103/1000 | Loss: 0.00003930
Iteration 104/1000 | Loss: 0.00003930
Iteration 105/1000 | Loss: 0.00003930
Iteration 106/1000 | Loss: 0.00003930
Iteration 107/1000 | Loss: 0.00003930
Iteration 108/1000 | Loss: 0.00003930
Iteration 109/1000 | Loss: 0.00003929
Iteration 110/1000 | Loss: 0.00003929
Iteration 111/1000 | Loss: 0.00003929
Iteration 112/1000 | Loss: 0.00003929
Iteration 113/1000 | Loss: 0.00003929
Iteration 114/1000 | Loss: 0.00003929
Iteration 115/1000 | Loss: 0.00003928
Iteration 116/1000 | Loss: 0.00003928
Iteration 117/1000 | Loss: 0.00003928
Iteration 118/1000 | Loss: 0.00003928
Iteration 119/1000 | Loss: 0.00003927
Iteration 120/1000 | Loss: 0.00003927
Iteration 121/1000 | Loss: 0.00003927
Iteration 122/1000 | Loss: 0.00003927
Iteration 123/1000 | Loss: 0.00003927
Iteration 124/1000 | Loss: 0.00003927
Iteration 125/1000 | Loss: 0.00003927
Iteration 126/1000 | Loss: 0.00003927
Iteration 127/1000 | Loss: 0.00003927
Iteration 128/1000 | Loss: 0.00003927
Iteration 129/1000 | Loss: 0.00003927
Iteration 130/1000 | Loss: 0.00003927
Iteration 131/1000 | Loss: 0.00003927
Iteration 132/1000 | Loss: 0.00003927
Iteration 133/1000 | Loss: 0.00003926
Iteration 134/1000 | Loss: 0.00003926
Iteration 135/1000 | Loss: 0.00003926
Iteration 136/1000 | Loss: 0.00003926
Iteration 137/1000 | Loss: 0.00003926
Iteration 138/1000 | Loss: 0.00003926
Iteration 139/1000 | Loss: 0.00003926
Iteration 140/1000 | Loss: 0.00003926
Iteration 141/1000 | Loss: 0.00003926
Iteration 142/1000 | Loss: 0.00003926
Iteration 143/1000 | Loss: 0.00003925
Iteration 144/1000 | Loss: 0.00003925
Iteration 145/1000 | Loss: 0.00003925
Iteration 146/1000 | Loss: 0.00003925
Iteration 147/1000 | Loss: 0.00003925
Iteration 148/1000 | Loss: 0.00003925
Iteration 149/1000 | Loss: 0.00003925
Iteration 150/1000 | Loss: 0.00003925
Iteration 151/1000 | Loss: 0.00003925
Iteration 152/1000 | Loss: 0.00003925
Iteration 153/1000 | Loss: 0.00003925
Iteration 154/1000 | Loss: 0.00003925
Iteration 155/1000 | Loss: 0.00003925
Iteration 156/1000 | Loss: 0.00003925
Iteration 157/1000 | Loss: 0.00003925
Iteration 158/1000 | Loss: 0.00003925
Iteration 159/1000 | Loss: 0.00003925
Iteration 160/1000 | Loss: 0.00003925
Iteration 161/1000 | Loss: 0.00003924
Iteration 162/1000 | Loss: 0.00003924
Iteration 163/1000 | Loss: 0.00003924
Iteration 164/1000 | Loss: 0.00003924
Iteration 165/1000 | Loss: 0.00003924
Iteration 166/1000 | Loss: 0.00003924
Iteration 167/1000 | Loss: 0.00003924
Iteration 168/1000 | Loss: 0.00003924
Iteration 169/1000 | Loss: 0.00003924
Iteration 170/1000 | Loss: 0.00003924
Iteration 171/1000 | Loss: 0.00003924
Iteration 172/1000 | Loss: 0.00003924
Iteration 173/1000 | Loss: 0.00003924
Iteration 174/1000 | Loss: 0.00003924
Iteration 175/1000 | Loss: 0.00003924
Iteration 176/1000 | Loss: 0.00003924
Iteration 177/1000 | Loss: 0.00003924
Iteration 178/1000 | Loss: 0.00003924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [3.9239941543200985e-05, 3.9239941543200985e-05, 3.9239941543200985e-05, 3.9239941543200985e-05, 3.9239941543200985e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9239941543200985e-05

Optimization complete. Final v2v error: 5.226820468902588 mm

Highest mean error: 5.495247840881348 mm for frame 59

Lowest mean error: 5.037088871002197 mm for frame 111

Saving results

Total time: 53.24983072280884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873832
Iteration 2/25 | Loss: 0.00136296
Iteration 3/25 | Loss: 0.00086666
Iteration 4/25 | Loss: 0.00081238
Iteration 5/25 | Loss: 0.00075109
Iteration 6/25 | Loss: 0.00072976
Iteration 7/25 | Loss: 0.00074370
Iteration 8/25 | Loss: 0.00071759
Iteration 9/25 | Loss: 0.00071599
Iteration 10/25 | Loss: 0.00070695
Iteration 11/25 | Loss: 0.00070494
Iteration 12/25 | Loss: 0.00070596
Iteration 13/25 | Loss: 0.00070109
Iteration 14/25 | Loss: 0.00069912
Iteration 15/25 | Loss: 0.00069866
Iteration 16/25 | Loss: 0.00069854
Iteration 17/25 | Loss: 0.00069851
Iteration 18/25 | Loss: 0.00069851
Iteration 19/25 | Loss: 0.00069851
Iteration 20/25 | Loss: 0.00069851
Iteration 21/25 | Loss: 0.00069851
Iteration 22/25 | Loss: 0.00069851
Iteration 23/25 | Loss: 0.00069851
Iteration 24/25 | Loss: 0.00069851
Iteration 25/25 | Loss: 0.00069851

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 12.42493629
Iteration 2/25 | Loss: 0.00021881
Iteration 3/25 | Loss: 0.00021879
Iteration 4/25 | Loss: 0.00021879
Iteration 5/25 | Loss: 0.00021879
Iteration 6/25 | Loss: 0.00021879
Iteration 7/25 | Loss: 0.00021879
Iteration 8/25 | Loss: 0.00021879
Iteration 9/25 | Loss: 0.00021879
Iteration 10/25 | Loss: 0.00021879
Iteration 11/25 | Loss: 0.00021879
Iteration 12/25 | Loss: 0.00021879
Iteration 13/25 | Loss: 0.00021879
Iteration 14/25 | Loss: 0.00021879
Iteration 15/25 | Loss: 0.00021879
Iteration 16/25 | Loss: 0.00021879
Iteration 17/25 | Loss: 0.00021879
Iteration 18/25 | Loss: 0.00021879
Iteration 19/25 | Loss: 0.00021879
Iteration 20/25 | Loss: 0.00021879
Iteration 21/25 | Loss: 0.00021879
Iteration 22/25 | Loss: 0.00021879
Iteration 23/25 | Loss: 0.00021879
Iteration 24/25 | Loss: 0.00021879
Iteration 25/25 | Loss: 0.00021879
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0002187856298405677, 0.0002187856298405677, 0.0002187856298405677, 0.0002187856298405677, 0.0002187856298405677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002187856298405677

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021879
Iteration 2/1000 | Loss: 0.00010090
Iteration 3/1000 | Loss: 0.00003685
Iteration 4/1000 | Loss: 0.00002642
Iteration 5/1000 | Loss: 0.00002464
Iteration 6/1000 | Loss: 0.00002358
Iteration 7/1000 | Loss: 0.00005642
Iteration 8/1000 | Loss: 0.00002279
Iteration 9/1000 | Loss: 0.00006015
Iteration 10/1000 | Loss: 0.00003439
Iteration 11/1000 | Loss: 0.00002203
Iteration 12/1000 | Loss: 0.00002170
Iteration 13/1000 | Loss: 0.00002147
Iteration 14/1000 | Loss: 0.00009172
Iteration 15/1000 | Loss: 0.00007436
Iteration 16/1000 | Loss: 0.00006136
Iteration 17/1000 | Loss: 0.00011994
Iteration 18/1000 | Loss: 0.00004900
Iteration 19/1000 | Loss: 0.00003090
Iteration 20/1000 | Loss: 0.00003103
Iteration 21/1000 | Loss: 0.00002121
Iteration 22/1000 | Loss: 0.00002117
Iteration 23/1000 | Loss: 0.00002117
Iteration 24/1000 | Loss: 0.00002116
Iteration 25/1000 | Loss: 0.00002112
Iteration 26/1000 | Loss: 0.00002109
Iteration 27/1000 | Loss: 0.00002109
Iteration 28/1000 | Loss: 0.00002109
Iteration 29/1000 | Loss: 0.00002108
Iteration 30/1000 | Loss: 0.00002108
Iteration 31/1000 | Loss: 0.00002108
Iteration 32/1000 | Loss: 0.00002108
Iteration 33/1000 | Loss: 0.00002107
Iteration 34/1000 | Loss: 0.00002107
Iteration 35/1000 | Loss: 0.00002107
Iteration 36/1000 | Loss: 0.00002106
Iteration 37/1000 | Loss: 0.00002106
Iteration 38/1000 | Loss: 0.00002104
Iteration 39/1000 | Loss: 0.00002100
Iteration 40/1000 | Loss: 0.00002099
Iteration 41/1000 | Loss: 0.00002099
Iteration 42/1000 | Loss: 0.00002098
Iteration 43/1000 | Loss: 0.00002098
Iteration 44/1000 | Loss: 0.00002098
Iteration 45/1000 | Loss: 0.00002097
Iteration 46/1000 | Loss: 0.00002097
Iteration 47/1000 | Loss: 0.00002097
Iteration 48/1000 | Loss: 0.00002096
Iteration 49/1000 | Loss: 0.00002094
Iteration 50/1000 | Loss: 0.00002094
Iteration 51/1000 | Loss: 0.00002093
Iteration 52/1000 | Loss: 0.00002093
Iteration 53/1000 | Loss: 0.00002093
Iteration 54/1000 | Loss: 0.00002092
Iteration 55/1000 | Loss: 0.00002092
Iteration 56/1000 | Loss: 0.00002091
Iteration 57/1000 | Loss: 0.00002091
Iteration 58/1000 | Loss: 0.00002091
Iteration 59/1000 | Loss: 0.00002091
Iteration 60/1000 | Loss: 0.00002090
Iteration 61/1000 | Loss: 0.00002090
Iteration 62/1000 | Loss: 0.00002090
Iteration 63/1000 | Loss: 0.00002089
Iteration 64/1000 | Loss: 0.00002089
Iteration 65/1000 | Loss: 0.00002089
Iteration 66/1000 | Loss: 0.00002089
Iteration 67/1000 | Loss: 0.00002088
Iteration 68/1000 | Loss: 0.00002088
Iteration 69/1000 | Loss: 0.00002088
Iteration 70/1000 | Loss: 0.00002087
Iteration 71/1000 | Loss: 0.00002087
Iteration 72/1000 | Loss: 0.00002087
Iteration 73/1000 | Loss: 0.00002087
Iteration 74/1000 | Loss: 0.00002087
Iteration 75/1000 | Loss: 0.00002087
Iteration 76/1000 | Loss: 0.00002087
Iteration 77/1000 | Loss: 0.00002087
Iteration 78/1000 | Loss: 0.00002086
Iteration 79/1000 | Loss: 0.00002086
Iteration 80/1000 | Loss: 0.00002086
Iteration 81/1000 | Loss: 0.00002086
Iteration 82/1000 | Loss: 0.00002086
Iteration 83/1000 | Loss: 0.00002086
Iteration 84/1000 | Loss: 0.00002086
Iteration 85/1000 | Loss: 0.00002086
Iteration 86/1000 | Loss: 0.00002086
Iteration 87/1000 | Loss: 0.00002086
Iteration 88/1000 | Loss: 0.00002086
Iteration 89/1000 | Loss: 0.00002086
Iteration 90/1000 | Loss: 0.00002086
Iteration 91/1000 | Loss: 0.00002086
Iteration 92/1000 | Loss: 0.00002086
Iteration 93/1000 | Loss: 0.00002085
Iteration 94/1000 | Loss: 0.00002085
Iteration 95/1000 | Loss: 0.00002085
Iteration 96/1000 | Loss: 0.00002085
Iteration 97/1000 | Loss: 0.00008468
Iteration 98/1000 | Loss: 0.00008467
Iteration 99/1000 | Loss: 0.00122253
Iteration 100/1000 | Loss: 0.00189669
Iteration 101/1000 | Loss: 0.00065573
Iteration 102/1000 | Loss: 0.00013459
Iteration 103/1000 | Loss: 0.00020974
Iteration 104/1000 | Loss: 0.00003637
Iteration 105/1000 | Loss: 0.00002216
Iteration 106/1000 | Loss: 0.00011812
Iteration 107/1000 | Loss: 0.00002540
Iteration 108/1000 | Loss: 0.00002324
Iteration 109/1000 | Loss: 0.00002123
Iteration 110/1000 | Loss: 0.00002101
Iteration 111/1000 | Loss: 0.00002093
Iteration 112/1000 | Loss: 0.00002093
Iteration 113/1000 | Loss: 0.00002091
Iteration 114/1000 | Loss: 0.00002090
Iteration 115/1000 | Loss: 0.00002089
Iteration 116/1000 | Loss: 0.00002089
Iteration 117/1000 | Loss: 0.00002089
Iteration 118/1000 | Loss: 0.00002088
Iteration 119/1000 | Loss: 0.00002088
Iteration 120/1000 | Loss: 0.00002087
Iteration 121/1000 | Loss: 0.00002087
Iteration 122/1000 | Loss: 0.00002083
Iteration 123/1000 | Loss: 0.00002083
Iteration 124/1000 | Loss: 0.00002081
Iteration 125/1000 | Loss: 0.00002080
Iteration 126/1000 | Loss: 0.00002080
Iteration 127/1000 | Loss: 0.00002080
Iteration 128/1000 | Loss: 0.00002079
Iteration 129/1000 | Loss: 0.00002078
Iteration 130/1000 | Loss: 0.00002078
Iteration 131/1000 | Loss: 0.00002078
Iteration 132/1000 | Loss: 0.00002077
Iteration 133/1000 | Loss: 0.00002077
Iteration 134/1000 | Loss: 0.00002077
Iteration 135/1000 | Loss: 0.00002077
Iteration 136/1000 | Loss: 0.00002077
Iteration 137/1000 | Loss: 0.00002077
Iteration 138/1000 | Loss: 0.00002077
Iteration 139/1000 | Loss: 0.00002077
Iteration 140/1000 | Loss: 0.00002077
Iteration 141/1000 | Loss: 0.00002077
Iteration 142/1000 | Loss: 0.00002077
Iteration 143/1000 | Loss: 0.00002077
Iteration 144/1000 | Loss: 0.00002077
Iteration 145/1000 | Loss: 0.00002077
Iteration 146/1000 | Loss: 0.00002077
Iteration 147/1000 | Loss: 0.00002077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.076656346616801e-05, 2.076656346616801e-05, 2.076656346616801e-05, 2.076656346616801e-05, 2.076656346616801e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.076656346616801e-05

Optimization complete. Final v2v error: 3.878711462020874 mm

Highest mean error: 4.891822814941406 mm for frame 95

Lowest mean error: 3.0793066024780273 mm for frame 0

Saving results

Total time: 90.48099780082703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00602727
Iteration 2/25 | Loss: 0.00117350
Iteration 3/25 | Loss: 0.00078932
Iteration 4/25 | Loss: 0.00071980
Iteration 5/25 | Loss: 0.00071045
Iteration 6/25 | Loss: 0.00070839
Iteration 7/25 | Loss: 0.00070813
Iteration 8/25 | Loss: 0.00070813
Iteration 9/25 | Loss: 0.00070813
Iteration 10/25 | Loss: 0.00070813
Iteration 11/25 | Loss: 0.00070813
Iteration 12/25 | Loss: 0.00070813
Iteration 13/25 | Loss: 0.00070813
Iteration 14/25 | Loss: 0.00070813
Iteration 15/25 | Loss: 0.00070813
Iteration 16/25 | Loss: 0.00070813
Iteration 17/25 | Loss: 0.00070813
Iteration 18/25 | Loss: 0.00070813
Iteration 19/25 | Loss: 0.00070813
Iteration 20/25 | Loss: 0.00070813
Iteration 21/25 | Loss: 0.00070813
Iteration 22/25 | Loss: 0.00070813
Iteration 23/25 | Loss: 0.00070813
Iteration 24/25 | Loss: 0.00070813
Iteration 25/25 | Loss: 0.00070813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20661807
Iteration 2/25 | Loss: 0.00011895
Iteration 3/25 | Loss: 0.00011895
Iteration 4/25 | Loss: 0.00011895
Iteration 5/25 | Loss: 0.00011895
Iteration 6/25 | Loss: 0.00011895
Iteration 7/25 | Loss: 0.00011895
Iteration 8/25 | Loss: 0.00011895
Iteration 9/25 | Loss: 0.00011895
Iteration 10/25 | Loss: 0.00011895
Iteration 11/25 | Loss: 0.00011895
Iteration 12/25 | Loss: 0.00011895
Iteration 13/25 | Loss: 0.00011895
Iteration 14/25 | Loss: 0.00011895
Iteration 15/25 | Loss: 0.00011895
Iteration 16/25 | Loss: 0.00011895
Iteration 17/25 | Loss: 0.00011895
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0001189498434541747, 0.0001189498434541747, 0.0001189498434541747, 0.0001189498434541747, 0.0001189498434541747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001189498434541747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00011895
Iteration 2/1000 | Loss: 0.00005051
Iteration 3/1000 | Loss: 0.00002961
Iteration 4/1000 | Loss: 0.00002349
Iteration 5/1000 | Loss: 0.00002192
Iteration 6/1000 | Loss: 0.00002097
Iteration 7/1000 | Loss: 0.00002052
Iteration 8/1000 | Loss: 0.00002004
Iteration 9/1000 | Loss: 0.00001969
Iteration 10/1000 | Loss: 0.00001933
Iteration 11/1000 | Loss: 0.00001905
Iteration 12/1000 | Loss: 0.00001888
Iteration 13/1000 | Loss: 0.00001870
Iteration 14/1000 | Loss: 0.00001869
Iteration 15/1000 | Loss: 0.00001863
Iteration 16/1000 | Loss: 0.00001862
Iteration 17/1000 | Loss: 0.00001860
Iteration 18/1000 | Loss: 0.00001859
Iteration 19/1000 | Loss: 0.00001857
Iteration 20/1000 | Loss: 0.00001857
Iteration 21/1000 | Loss: 0.00001854
Iteration 22/1000 | Loss: 0.00001852
Iteration 23/1000 | Loss: 0.00001849
Iteration 24/1000 | Loss: 0.00001847
Iteration 25/1000 | Loss: 0.00001847
Iteration 26/1000 | Loss: 0.00001847
Iteration 27/1000 | Loss: 0.00001847
Iteration 28/1000 | Loss: 0.00001847
Iteration 29/1000 | Loss: 0.00001847
Iteration 30/1000 | Loss: 0.00001846
Iteration 31/1000 | Loss: 0.00001846
Iteration 32/1000 | Loss: 0.00001846
Iteration 33/1000 | Loss: 0.00001846
Iteration 34/1000 | Loss: 0.00001845
Iteration 35/1000 | Loss: 0.00001845
Iteration 36/1000 | Loss: 0.00001845
Iteration 37/1000 | Loss: 0.00001845
Iteration 38/1000 | Loss: 0.00001845
Iteration 39/1000 | Loss: 0.00001844
Iteration 40/1000 | Loss: 0.00001844
Iteration 41/1000 | Loss: 0.00001844
Iteration 42/1000 | Loss: 0.00001844
Iteration 43/1000 | Loss: 0.00001844
Iteration 44/1000 | Loss: 0.00001844
Iteration 45/1000 | Loss: 0.00001844
Iteration 46/1000 | Loss: 0.00001844
Iteration 47/1000 | Loss: 0.00001844
Iteration 48/1000 | Loss: 0.00001844
Iteration 49/1000 | Loss: 0.00001844
Iteration 50/1000 | Loss: 0.00001843
Iteration 51/1000 | Loss: 0.00001843
Iteration 52/1000 | Loss: 0.00001842
Iteration 53/1000 | Loss: 0.00001842
Iteration 54/1000 | Loss: 0.00001842
Iteration 55/1000 | Loss: 0.00001841
Iteration 56/1000 | Loss: 0.00001841
Iteration 57/1000 | Loss: 0.00001841
Iteration 58/1000 | Loss: 0.00001840
Iteration 59/1000 | Loss: 0.00001840
Iteration 60/1000 | Loss: 0.00001839
Iteration 61/1000 | Loss: 0.00001839
Iteration 62/1000 | Loss: 0.00001839
Iteration 63/1000 | Loss: 0.00001839
Iteration 64/1000 | Loss: 0.00001839
Iteration 65/1000 | Loss: 0.00001839
Iteration 66/1000 | Loss: 0.00001839
Iteration 67/1000 | Loss: 0.00001839
Iteration 68/1000 | Loss: 0.00001839
Iteration 69/1000 | Loss: 0.00001838
Iteration 70/1000 | Loss: 0.00001838
Iteration 71/1000 | Loss: 0.00001838
Iteration 72/1000 | Loss: 0.00001838
Iteration 73/1000 | Loss: 0.00001838
Iteration 74/1000 | Loss: 0.00001838
Iteration 75/1000 | Loss: 0.00001837
Iteration 76/1000 | Loss: 0.00001837
Iteration 77/1000 | Loss: 0.00001837
Iteration 78/1000 | Loss: 0.00001836
Iteration 79/1000 | Loss: 0.00001836
Iteration 80/1000 | Loss: 0.00001836
Iteration 81/1000 | Loss: 0.00001836
Iteration 82/1000 | Loss: 0.00001836
Iteration 83/1000 | Loss: 0.00001835
Iteration 84/1000 | Loss: 0.00001835
Iteration 85/1000 | Loss: 0.00001835
Iteration 86/1000 | Loss: 0.00001835
Iteration 87/1000 | Loss: 0.00001835
Iteration 88/1000 | Loss: 0.00001835
Iteration 89/1000 | Loss: 0.00001835
Iteration 90/1000 | Loss: 0.00001835
Iteration 91/1000 | Loss: 0.00001834
Iteration 92/1000 | Loss: 0.00001834
Iteration 93/1000 | Loss: 0.00001834
Iteration 94/1000 | Loss: 0.00001834
Iteration 95/1000 | Loss: 0.00001834
Iteration 96/1000 | Loss: 0.00001834
Iteration 97/1000 | Loss: 0.00001833
Iteration 98/1000 | Loss: 0.00001833
Iteration 99/1000 | Loss: 0.00001833
Iteration 100/1000 | Loss: 0.00001833
Iteration 101/1000 | Loss: 0.00001833
Iteration 102/1000 | Loss: 0.00001833
Iteration 103/1000 | Loss: 0.00001833
Iteration 104/1000 | Loss: 0.00001833
Iteration 105/1000 | Loss: 0.00001832
Iteration 106/1000 | Loss: 0.00001832
Iteration 107/1000 | Loss: 0.00001832
Iteration 108/1000 | Loss: 0.00001832
Iteration 109/1000 | Loss: 0.00001832
Iteration 110/1000 | Loss: 0.00001832
Iteration 111/1000 | Loss: 0.00001832
Iteration 112/1000 | Loss: 0.00001832
Iteration 113/1000 | Loss: 0.00001832
Iteration 114/1000 | Loss: 0.00001832
Iteration 115/1000 | Loss: 0.00001832
Iteration 116/1000 | Loss: 0.00001832
Iteration 117/1000 | Loss: 0.00001832
Iteration 118/1000 | Loss: 0.00001832
Iteration 119/1000 | Loss: 0.00001831
Iteration 120/1000 | Loss: 0.00001831
Iteration 121/1000 | Loss: 0.00001831
Iteration 122/1000 | Loss: 0.00001831
Iteration 123/1000 | Loss: 0.00001831
Iteration 124/1000 | Loss: 0.00001831
Iteration 125/1000 | Loss: 0.00001831
Iteration 126/1000 | Loss: 0.00001831
Iteration 127/1000 | Loss: 0.00001831
Iteration 128/1000 | Loss: 0.00001831
Iteration 129/1000 | Loss: 0.00001831
Iteration 130/1000 | Loss: 0.00001831
Iteration 131/1000 | Loss: 0.00001831
Iteration 132/1000 | Loss: 0.00001831
Iteration 133/1000 | Loss: 0.00001831
Iteration 134/1000 | Loss: 0.00001831
Iteration 135/1000 | Loss: 0.00001831
Iteration 136/1000 | Loss: 0.00001831
Iteration 137/1000 | Loss: 0.00001831
Iteration 138/1000 | Loss: 0.00001831
Iteration 139/1000 | Loss: 0.00001831
Iteration 140/1000 | Loss: 0.00001831
Iteration 141/1000 | Loss: 0.00001831
Iteration 142/1000 | Loss: 0.00001831
Iteration 143/1000 | Loss: 0.00001831
Iteration 144/1000 | Loss: 0.00001831
Iteration 145/1000 | Loss: 0.00001831
Iteration 146/1000 | Loss: 0.00001831
Iteration 147/1000 | Loss: 0.00001831
Iteration 148/1000 | Loss: 0.00001831
Iteration 149/1000 | Loss: 0.00001831
Iteration 150/1000 | Loss: 0.00001831
Iteration 151/1000 | Loss: 0.00001831
Iteration 152/1000 | Loss: 0.00001831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.8309681763639674e-05, 1.8309681763639674e-05, 1.8309681763639674e-05, 1.8309681763639674e-05, 1.8309681763639674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8309681763639674e-05

Optimization complete. Final v2v error: 3.5950067043304443 mm

Highest mean error: 5.389967441558838 mm for frame 59

Lowest mean error: 3.0537452697753906 mm for frame 143

Saving results

Total time: 39.26327872276306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079017
Iteration 2/25 | Loss: 0.01079017
Iteration 3/25 | Loss: 0.01079017
Iteration 4/25 | Loss: 0.01079017
Iteration 5/25 | Loss: 0.01079016
Iteration 6/25 | Loss: 0.01079016
Iteration 7/25 | Loss: 0.01079016
Iteration 8/25 | Loss: 0.01079016
Iteration 9/25 | Loss: 0.01079016
Iteration 10/25 | Loss: 0.01079016
Iteration 11/25 | Loss: 0.01079016
Iteration 12/25 | Loss: 0.01079015
Iteration 13/25 | Loss: 0.01079015
Iteration 14/25 | Loss: 0.01079015
Iteration 15/25 | Loss: 0.01079015
Iteration 16/25 | Loss: 0.01079015
Iteration 17/25 | Loss: 0.01079015
Iteration 18/25 | Loss: 0.01079015
Iteration 19/25 | Loss: 0.01079015
Iteration 20/25 | Loss: 0.01079014
Iteration 21/25 | Loss: 0.01079014
Iteration 22/25 | Loss: 0.01079014
Iteration 23/25 | Loss: 0.01079014
Iteration 24/25 | Loss: 0.01079014
Iteration 25/25 | Loss: 0.01079014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54675281
Iteration 2/25 | Loss: 0.10906956
Iteration 3/25 | Loss: 0.10882156
Iteration 4/25 | Loss: 0.10882155
Iteration 5/25 | Loss: 0.10882155
Iteration 6/25 | Loss: 0.10882153
Iteration 7/25 | Loss: 0.10882153
Iteration 8/25 | Loss: 0.10882153
Iteration 9/25 | Loss: 0.10882153
Iteration 10/25 | Loss: 0.10882153
Iteration 11/25 | Loss: 0.10882153
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.10882153362035751, 0.10882153362035751, 0.10882153362035751, 0.10882153362035751, 0.10882153362035751]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10882153362035751

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10882153
Iteration 2/1000 | Loss: 0.00140282
Iteration 3/1000 | Loss: 0.00075167
Iteration 4/1000 | Loss: 0.00016814
Iteration 5/1000 | Loss: 0.00029271
Iteration 6/1000 | Loss: 0.00007881
Iteration 7/1000 | Loss: 0.00010283
Iteration 8/1000 | Loss: 0.00005171
Iteration 9/1000 | Loss: 0.00002689
Iteration 10/1000 | Loss: 0.00002363
Iteration 11/1000 | Loss: 0.00002039
Iteration 12/1000 | Loss: 0.00007309
Iteration 13/1000 | Loss: 0.00001794
Iteration 14/1000 | Loss: 0.00001700
Iteration 15/1000 | Loss: 0.00005122
Iteration 16/1000 | Loss: 0.00009424
Iteration 17/1000 | Loss: 0.00003971
Iteration 18/1000 | Loss: 0.00003487
Iteration 19/1000 | Loss: 0.00001587
Iteration 20/1000 | Loss: 0.00001463
Iteration 21/1000 | Loss: 0.00003433
Iteration 22/1000 | Loss: 0.00001396
Iteration 23/1000 | Loss: 0.00002849
Iteration 24/1000 | Loss: 0.00001328
Iteration 25/1000 | Loss: 0.00001286
Iteration 26/1000 | Loss: 0.00001251
Iteration 27/1000 | Loss: 0.00001231
Iteration 28/1000 | Loss: 0.00001227
Iteration 29/1000 | Loss: 0.00001208
Iteration 30/1000 | Loss: 0.00001204
Iteration 31/1000 | Loss: 0.00001195
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001194
Iteration 34/1000 | Loss: 0.00001193
Iteration 35/1000 | Loss: 0.00001192
Iteration 36/1000 | Loss: 0.00001191
Iteration 37/1000 | Loss: 0.00001188
Iteration 38/1000 | Loss: 0.00001183
Iteration 39/1000 | Loss: 0.00001183
Iteration 40/1000 | Loss: 0.00001183
Iteration 41/1000 | Loss: 0.00001182
Iteration 42/1000 | Loss: 0.00001182
Iteration 43/1000 | Loss: 0.00001182
Iteration 44/1000 | Loss: 0.00001181
Iteration 45/1000 | Loss: 0.00001181
Iteration 46/1000 | Loss: 0.00001180
Iteration 47/1000 | Loss: 0.00001179
Iteration 48/1000 | Loss: 0.00001179
Iteration 49/1000 | Loss: 0.00001178
Iteration 50/1000 | Loss: 0.00001178
Iteration 51/1000 | Loss: 0.00001178
Iteration 52/1000 | Loss: 0.00001177
Iteration 53/1000 | Loss: 0.00001177
Iteration 54/1000 | Loss: 0.00001177
Iteration 55/1000 | Loss: 0.00001177
Iteration 56/1000 | Loss: 0.00001177
Iteration 57/1000 | Loss: 0.00001176
Iteration 58/1000 | Loss: 0.00001176
Iteration 59/1000 | Loss: 0.00001176
Iteration 60/1000 | Loss: 0.00001176
Iteration 61/1000 | Loss: 0.00001176
Iteration 62/1000 | Loss: 0.00001176
Iteration 63/1000 | Loss: 0.00001175
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001175
Iteration 66/1000 | Loss: 0.00001174
Iteration 67/1000 | Loss: 0.00001174
Iteration 68/1000 | Loss: 0.00001174
Iteration 69/1000 | Loss: 0.00001174
Iteration 70/1000 | Loss: 0.00001174
Iteration 71/1000 | Loss: 0.00001174
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001174
Iteration 75/1000 | Loss: 0.00001173
Iteration 76/1000 | Loss: 0.00001173
Iteration 77/1000 | Loss: 0.00001173
Iteration 78/1000 | Loss: 0.00001173
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001172
Iteration 81/1000 | Loss: 0.00001172
Iteration 82/1000 | Loss: 0.00001172
Iteration 83/1000 | Loss: 0.00001172
Iteration 84/1000 | Loss: 0.00001172
Iteration 85/1000 | Loss: 0.00001172
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001171
Iteration 89/1000 | Loss: 0.00001171
Iteration 90/1000 | Loss: 0.00001171
Iteration 91/1000 | Loss: 0.00001171
Iteration 92/1000 | Loss: 0.00001171
Iteration 93/1000 | Loss: 0.00001170
Iteration 94/1000 | Loss: 0.00001170
Iteration 95/1000 | Loss: 0.00001170
Iteration 96/1000 | Loss: 0.00001170
Iteration 97/1000 | Loss: 0.00001170
Iteration 98/1000 | Loss: 0.00001170
Iteration 99/1000 | Loss: 0.00001170
Iteration 100/1000 | Loss: 0.00001170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.1702107258315664e-05, 1.1702107258315664e-05, 1.1702107258315664e-05, 1.1702107258315664e-05, 1.1702107258315664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1702107258315664e-05

Optimization complete. Final v2v error: 2.955540418624878 mm

Highest mean error: 3.587951421737671 mm for frame 31

Lowest mean error: 2.6785812377929688 mm for frame 59

Saving results

Total time: 60.65124869346619
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808217
Iteration 2/25 | Loss: 0.00147866
Iteration 3/25 | Loss: 0.00098782
Iteration 4/25 | Loss: 0.00094519
Iteration 5/25 | Loss: 0.00093269
Iteration 6/25 | Loss: 0.00092965
Iteration 7/25 | Loss: 0.00092894
Iteration 8/25 | Loss: 0.00092894
Iteration 9/25 | Loss: 0.00092894
Iteration 10/25 | Loss: 0.00092894
Iteration 11/25 | Loss: 0.00092894
Iteration 12/25 | Loss: 0.00092894
Iteration 13/25 | Loss: 0.00092894
Iteration 14/25 | Loss: 0.00092894
Iteration 15/25 | Loss: 0.00092894
Iteration 16/25 | Loss: 0.00092894
Iteration 17/25 | Loss: 0.00092894
Iteration 18/25 | Loss: 0.00092894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009289361187256873, 0.0009289361187256873, 0.0009289361187256873, 0.0009289361187256873, 0.0009289361187256873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009289361187256873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.26503152
Iteration 2/25 | Loss: 0.00024723
Iteration 3/25 | Loss: 0.00024723
Iteration 4/25 | Loss: 0.00024723
Iteration 5/25 | Loss: 0.00024723
Iteration 6/25 | Loss: 0.00024723
Iteration 7/25 | Loss: 0.00024723
Iteration 8/25 | Loss: 0.00024723
Iteration 9/25 | Loss: 0.00024723
Iteration 10/25 | Loss: 0.00024723
Iteration 11/25 | Loss: 0.00024723
Iteration 12/25 | Loss: 0.00024723
Iteration 13/25 | Loss: 0.00024723
Iteration 14/25 | Loss: 0.00024723
Iteration 15/25 | Loss: 0.00024723
Iteration 16/25 | Loss: 0.00024723
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00024722833768464625, 0.00024722833768464625, 0.00024722833768464625, 0.00024722833768464625, 0.00024722833768464625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024722833768464625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024723
Iteration 2/1000 | Loss: 0.00008837
Iteration 3/1000 | Loss: 0.00005701
Iteration 4/1000 | Loss: 0.00004794
Iteration 5/1000 | Loss: 0.00004518
Iteration 6/1000 | Loss: 0.00004381
Iteration 7/1000 | Loss: 0.00004259
Iteration 8/1000 | Loss: 0.00004128
Iteration 9/1000 | Loss: 0.00004059
Iteration 10/1000 | Loss: 0.00004000
Iteration 11/1000 | Loss: 0.00003952
Iteration 12/1000 | Loss: 0.00003919
Iteration 13/1000 | Loss: 0.00003893
Iteration 14/1000 | Loss: 0.00003867
Iteration 15/1000 | Loss: 0.00003845
Iteration 16/1000 | Loss: 0.00003826
Iteration 17/1000 | Loss: 0.00003803
Iteration 18/1000 | Loss: 0.00003783
Iteration 19/1000 | Loss: 0.00003776
Iteration 20/1000 | Loss: 0.00003763
Iteration 21/1000 | Loss: 0.00003759
Iteration 22/1000 | Loss: 0.00003750
Iteration 23/1000 | Loss: 0.00003742
Iteration 24/1000 | Loss: 0.00003742
Iteration 25/1000 | Loss: 0.00003739
Iteration 26/1000 | Loss: 0.00003733
Iteration 27/1000 | Loss: 0.00003732
Iteration 28/1000 | Loss: 0.00003731
Iteration 29/1000 | Loss: 0.00003730
Iteration 30/1000 | Loss: 0.00003729
Iteration 31/1000 | Loss: 0.00003729
Iteration 32/1000 | Loss: 0.00003729
Iteration 33/1000 | Loss: 0.00003728
Iteration 34/1000 | Loss: 0.00003727
Iteration 35/1000 | Loss: 0.00003727
Iteration 36/1000 | Loss: 0.00003726
Iteration 37/1000 | Loss: 0.00003726
Iteration 38/1000 | Loss: 0.00003726
Iteration 39/1000 | Loss: 0.00003726
Iteration 40/1000 | Loss: 0.00003725
Iteration 41/1000 | Loss: 0.00003723
Iteration 42/1000 | Loss: 0.00003723
Iteration 43/1000 | Loss: 0.00003722
Iteration 44/1000 | Loss: 0.00003722
Iteration 45/1000 | Loss: 0.00003721
Iteration 46/1000 | Loss: 0.00003721
Iteration 47/1000 | Loss: 0.00003720
Iteration 48/1000 | Loss: 0.00003720
Iteration 49/1000 | Loss: 0.00003720
Iteration 50/1000 | Loss: 0.00003719
Iteration 51/1000 | Loss: 0.00003719
Iteration 52/1000 | Loss: 0.00003718
Iteration 53/1000 | Loss: 0.00003718
Iteration 54/1000 | Loss: 0.00003718
Iteration 55/1000 | Loss: 0.00003717
Iteration 56/1000 | Loss: 0.00003717
Iteration 57/1000 | Loss: 0.00003717
Iteration 58/1000 | Loss: 0.00003717
Iteration 59/1000 | Loss: 0.00003717
Iteration 60/1000 | Loss: 0.00003716
Iteration 61/1000 | Loss: 0.00003716
Iteration 62/1000 | Loss: 0.00003716
Iteration 63/1000 | Loss: 0.00003715
Iteration 64/1000 | Loss: 0.00003715
Iteration 65/1000 | Loss: 0.00003715
Iteration 66/1000 | Loss: 0.00003714
Iteration 67/1000 | Loss: 0.00003714
Iteration 68/1000 | Loss: 0.00003714
Iteration 69/1000 | Loss: 0.00003714
Iteration 70/1000 | Loss: 0.00003713
Iteration 71/1000 | Loss: 0.00003713
Iteration 72/1000 | Loss: 0.00003713
Iteration 73/1000 | Loss: 0.00003712
Iteration 74/1000 | Loss: 0.00003712
Iteration 75/1000 | Loss: 0.00003712
Iteration 76/1000 | Loss: 0.00003712
Iteration 77/1000 | Loss: 0.00003712
Iteration 78/1000 | Loss: 0.00003712
Iteration 79/1000 | Loss: 0.00003711
Iteration 80/1000 | Loss: 0.00003711
Iteration 81/1000 | Loss: 0.00003711
Iteration 82/1000 | Loss: 0.00003710
Iteration 83/1000 | Loss: 0.00003710
Iteration 84/1000 | Loss: 0.00003710
Iteration 85/1000 | Loss: 0.00003710
Iteration 86/1000 | Loss: 0.00003710
Iteration 87/1000 | Loss: 0.00003710
Iteration 88/1000 | Loss: 0.00003709
Iteration 89/1000 | Loss: 0.00003709
Iteration 90/1000 | Loss: 0.00003709
Iteration 91/1000 | Loss: 0.00003709
Iteration 92/1000 | Loss: 0.00003709
Iteration 93/1000 | Loss: 0.00003709
Iteration 94/1000 | Loss: 0.00003709
Iteration 95/1000 | Loss: 0.00003709
Iteration 96/1000 | Loss: 0.00003709
Iteration 97/1000 | Loss: 0.00003709
Iteration 98/1000 | Loss: 0.00003708
Iteration 99/1000 | Loss: 0.00003708
Iteration 100/1000 | Loss: 0.00003708
Iteration 101/1000 | Loss: 0.00003708
Iteration 102/1000 | Loss: 0.00003708
Iteration 103/1000 | Loss: 0.00003708
Iteration 104/1000 | Loss: 0.00003708
Iteration 105/1000 | Loss: 0.00003708
Iteration 106/1000 | Loss: 0.00003708
Iteration 107/1000 | Loss: 0.00003708
Iteration 108/1000 | Loss: 0.00003707
Iteration 109/1000 | Loss: 0.00003707
Iteration 110/1000 | Loss: 0.00003707
Iteration 111/1000 | Loss: 0.00003707
Iteration 112/1000 | Loss: 0.00003707
Iteration 113/1000 | Loss: 0.00003706
Iteration 114/1000 | Loss: 0.00003706
Iteration 115/1000 | Loss: 0.00003706
Iteration 116/1000 | Loss: 0.00003706
Iteration 117/1000 | Loss: 0.00003706
Iteration 118/1000 | Loss: 0.00003706
Iteration 119/1000 | Loss: 0.00003706
Iteration 120/1000 | Loss: 0.00003706
Iteration 121/1000 | Loss: 0.00003706
Iteration 122/1000 | Loss: 0.00003706
Iteration 123/1000 | Loss: 0.00003705
Iteration 124/1000 | Loss: 0.00003705
Iteration 125/1000 | Loss: 0.00003705
Iteration 126/1000 | Loss: 0.00003705
Iteration 127/1000 | Loss: 0.00003705
Iteration 128/1000 | Loss: 0.00003705
Iteration 129/1000 | Loss: 0.00003705
Iteration 130/1000 | Loss: 0.00003705
Iteration 131/1000 | Loss: 0.00003705
Iteration 132/1000 | Loss: 0.00003705
Iteration 133/1000 | Loss: 0.00003705
Iteration 134/1000 | Loss: 0.00003705
Iteration 135/1000 | Loss: 0.00003704
Iteration 136/1000 | Loss: 0.00003704
Iteration 137/1000 | Loss: 0.00003704
Iteration 138/1000 | Loss: 0.00003704
Iteration 139/1000 | Loss: 0.00003703
Iteration 140/1000 | Loss: 0.00003703
Iteration 141/1000 | Loss: 0.00003703
Iteration 142/1000 | Loss: 0.00003703
Iteration 143/1000 | Loss: 0.00003703
Iteration 144/1000 | Loss: 0.00003703
Iteration 145/1000 | Loss: 0.00003703
Iteration 146/1000 | Loss: 0.00003703
Iteration 147/1000 | Loss: 0.00003703
Iteration 148/1000 | Loss: 0.00003703
Iteration 149/1000 | Loss: 0.00003703
Iteration 150/1000 | Loss: 0.00003703
Iteration 151/1000 | Loss: 0.00003703
Iteration 152/1000 | Loss: 0.00003703
Iteration 153/1000 | Loss: 0.00003702
Iteration 154/1000 | Loss: 0.00003702
Iteration 155/1000 | Loss: 0.00003702
Iteration 156/1000 | Loss: 0.00003702
Iteration 157/1000 | Loss: 0.00003702
Iteration 158/1000 | Loss: 0.00003702
Iteration 159/1000 | Loss: 0.00003702
Iteration 160/1000 | Loss: 0.00003702
Iteration 161/1000 | Loss: 0.00003702
Iteration 162/1000 | Loss: 0.00003702
Iteration 163/1000 | Loss: 0.00003702
Iteration 164/1000 | Loss: 0.00003702
Iteration 165/1000 | Loss: 0.00003702
Iteration 166/1000 | Loss: 0.00003701
Iteration 167/1000 | Loss: 0.00003701
Iteration 168/1000 | Loss: 0.00003701
Iteration 169/1000 | Loss: 0.00003701
Iteration 170/1000 | Loss: 0.00003701
Iteration 171/1000 | Loss: 0.00003701
Iteration 172/1000 | Loss: 0.00003701
Iteration 173/1000 | Loss: 0.00003701
Iteration 174/1000 | Loss: 0.00003701
Iteration 175/1000 | Loss: 0.00003701
Iteration 176/1000 | Loss: 0.00003701
Iteration 177/1000 | Loss: 0.00003701
Iteration 178/1000 | Loss: 0.00003701
Iteration 179/1000 | Loss: 0.00003701
Iteration 180/1000 | Loss: 0.00003700
Iteration 181/1000 | Loss: 0.00003700
Iteration 182/1000 | Loss: 0.00003700
Iteration 183/1000 | Loss: 0.00003700
Iteration 184/1000 | Loss: 0.00003700
Iteration 185/1000 | Loss: 0.00003700
Iteration 186/1000 | Loss: 0.00003700
Iteration 187/1000 | Loss: 0.00003700
Iteration 188/1000 | Loss: 0.00003700
Iteration 189/1000 | Loss: 0.00003700
Iteration 190/1000 | Loss: 0.00003700
Iteration 191/1000 | Loss: 0.00003700
Iteration 192/1000 | Loss: 0.00003700
Iteration 193/1000 | Loss: 0.00003700
Iteration 194/1000 | Loss: 0.00003700
Iteration 195/1000 | Loss: 0.00003700
Iteration 196/1000 | Loss: 0.00003700
Iteration 197/1000 | Loss: 0.00003700
Iteration 198/1000 | Loss: 0.00003700
Iteration 199/1000 | Loss: 0.00003700
Iteration 200/1000 | Loss: 0.00003699
Iteration 201/1000 | Loss: 0.00003699
Iteration 202/1000 | Loss: 0.00003699
Iteration 203/1000 | Loss: 0.00003699
Iteration 204/1000 | Loss: 0.00003699
Iteration 205/1000 | Loss: 0.00003699
Iteration 206/1000 | Loss: 0.00003699
Iteration 207/1000 | Loss: 0.00003699
Iteration 208/1000 | Loss: 0.00003699
Iteration 209/1000 | Loss: 0.00003699
Iteration 210/1000 | Loss: 0.00003699
Iteration 211/1000 | Loss: 0.00003699
Iteration 212/1000 | Loss: 0.00003698
Iteration 213/1000 | Loss: 0.00003698
Iteration 214/1000 | Loss: 0.00003698
Iteration 215/1000 | Loss: 0.00003698
Iteration 216/1000 | Loss: 0.00003698
Iteration 217/1000 | Loss: 0.00003698
Iteration 218/1000 | Loss: 0.00003698
Iteration 219/1000 | Loss: 0.00003698
Iteration 220/1000 | Loss: 0.00003698
Iteration 221/1000 | Loss: 0.00003698
Iteration 222/1000 | Loss: 0.00003698
Iteration 223/1000 | Loss: 0.00003698
Iteration 224/1000 | Loss: 0.00003698
Iteration 225/1000 | Loss: 0.00003698
Iteration 226/1000 | Loss: 0.00003698
Iteration 227/1000 | Loss: 0.00003698
Iteration 228/1000 | Loss: 0.00003698
Iteration 229/1000 | Loss: 0.00003698
Iteration 230/1000 | Loss: 0.00003698
Iteration 231/1000 | Loss: 0.00003698
Iteration 232/1000 | Loss: 0.00003698
Iteration 233/1000 | Loss: 0.00003698
Iteration 234/1000 | Loss: 0.00003698
Iteration 235/1000 | Loss: 0.00003698
Iteration 236/1000 | Loss: 0.00003698
Iteration 237/1000 | Loss: 0.00003698
Iteration 238/1000 | Loss: 0.00003698
Iteration 239/1000 | Loss: 0.00003698
Iteration 240/1000 | Loss: 0.00003698
Iteration 241/1000 | Loss: 0.00003698
Iteration 242/1000 | Loss: 0.00003698
Iteration 243/1000 | Loss: 0.00003698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [3.6975772673031315e-05, 3.6975772673031315e-05, 3.6975772673031315e-05, 3.6975772673031315e-05, 3.6975772673031315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6975772673031315e-05

Optimization complete. Final v2v error: 4.777676105499268 mm

Highest mean error: 6.26709508895874 mm for frame 139

Lowest mean error: 3.56471848487854 mm for frame 15

Saving results

Total time: 58.10090684890747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826172
Iteration 2/25 | Loss: 0.00082549
Iteration 3/25 | Loss: 0.00070879
Iteration 4/25 | Loss: 0.00068584
Iteration 5/25 | Loss: 0.00067906
Iteration 6/25 | Loss: 0.00067795
Iteration 7/25 | Loss: 0.00067791
Iteration 8/25 | Loss: 0.00067790
Iteration 9/25 | Loss: 0.00067789
Iteration 10/25 | Loss: 0.00067789
Iteration 11/25 | Loss: 0.00067789
Iteration 12/25 | Loss: 0.00067789
Iteration 13/25 | Loss: 0.00067789
Iteration 14/25 | Loss: 0.00067789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006778911338187754, 0.0006778911338187754, 0.0006778911338187754, 0.0006778911338187754, 0.0006778911338187754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006778911338187754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.51404762
Iteration 2/25 | Loss: 0.00012607
Iteration 3/25 | Loss: 0.00012604
Iteration 4/25 | Loss: 0.00012604
Iteration 5/25 | Loss: 0.00012604
Iteration 6/25 | Loss: 0.00012604
Iteration 7/25 | Loss: 0.00012604
Iteration 8/25 | Loss: 0.00012604
Iteration 9/25 | Loss: 0.00012604
Iteration 10/25 | Loss: 0.00012604
Iteration 11/25 | Loss: 0.00012604
Iteration 12/25 | Loss: 0.00012604
Iteration 13/25 | Loss: 0.00012604
Iteration 14/25 | Loss: 0.00012604
Iteration 15/25 | Loss: 0.00012604
Iteration 16/25 | Loss: 0.00012604
Iteration 17/25 | Loss: 0.00012604
Iteration 18/25 | Loss: 0.00012604
Iteration 19/25 | Loss: 0.00012604
Iteration 20/25 | Loss: 0.00012604
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0001260414137504995, 0.0001260414137504995, 0.0001260414137504995, 0.0001260414137504995, 0.0001260414137504995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001260414137504995

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00012604
Iteration 2/1000 | Loss: 0.00004247
Iteration 3/1000 | Loss: 0.00003050
Iteration 4/1000 | Loss: 0.00002748
Iteration 5/1000 | Loss: 0.00002573
Iteration 6/1000 | Loss: 0.00002465
Iteration 7/1000 | Loss: 0.00002355
Iteration 8/1000 | Loss: 0.00002290
Iteration 9/1000 | Loss: 0.00002251
Iteration 10/1000 | Loss: 0.00002232
Iteration 11/1000 | Loss: 0.00002217
Iteration 12/1000 | Loss: 0.00002217
Iteration 13/1000 | Loss: 0.00002216
Iteration 14/1000 | Loss: 0.00002208
Iteration 15/1000 | Loss: 0.00002196
Iteration 16/1000 | Loss: 0.00002194
Iteration 17/1000 | Loss: 0.00002191
Iteration 18/1000 | Loss: 0.00002191
Iteration 19/1000 | Loss: 0.00002191
Iteration 20/1000 | Loss: 0.00002190
Iteration 21/1000 | Loss: 0.00002190
Iteration 22/1000 | Loss: 0.00002189
Iteration 23/1000 | Loss: 0.00002189
Iteration 24/1000 | Loss: 0.00002188
Iteration 25/1000 | Loss: 0.00002188
Iteration 26/1000 | Loss: 0.00002188
Iteration 27/1000 | Loss: 0.00002187
Iteration 28/1000 | Loss: 0.00002187
Iteration 29/1000 | Loss: 0.00002187
Iteration 30/1000 | Loss: 0.00002186
Iteration 31/1000 | Loss: 0.00002186
Iteration 32/1000 | Loss: 0.00002186
Iteration 33/1000 | Loss: 0.00002186
Iteration 34/1000 | Loss: 0.00002185
Iteration 35/1000 | Loss: 0.00002185
Iteration 36/1000 | Loss: 0.00002184
Iteration 37/1000 | Loss: 0.00002184
Iteration 38/1000 | Loss: 0.00002184
Iteration 39/1000 | Loss: 0.00002184
Iteration 40/1000 | Loss: 0.00002184
Iteration 41/1000 | Loss: 0.00002184
Iteration 42/1000 | Loss: 0.00002184
Iteration 43/1000 | Loss: 0.00002183
Iteration 44/1000 | Loss: 0.00002183
Iteration 45/1000 | Loss: 0.00002182
Iteration 46/1000 | Loss: 0.00002182
Iteration 47/1000 | Loss: 0.00002182
Iteration 48/1000 | Loss: 0.00002182
Iteration 49/1000 | Loss: 0.00002181
Iteration 50/1000 | Loss: 0.00002181
Iteration 51/1000 | Loss: 0.00002181
Iteration 52/1000 | Loss: 0.00002180
Iteration 53/1000 | Loss: 0.00002180
Iteration 54/1000 | Loss: 0.00002180
Iteration 55/1000 | Loss: 0.00002180
Iteration 56/1000 | Loss: 0.00002180
Iteration 57/1000 | Loss: 0.00002180
Iteration 58/1000 | Loss: 0.00002180
Iteration 59/1000 | Loss: 0.00002180
Iteration 60/1000 | Loss: 0.00002180
Iteration 61/1000 | Loss: 0.00002180
Iteration 62/1000 | Loss: 0.00002180
Iteration 63/1000 | Loss: 0.00002180
Iteration 64/1000 | Loss: 0.00002180
Iteration 65/1000 | Loss: 0.00002180
Iteration 66/1000 | Loss: 0.00002180
Iteration 67/1000 | Loss: 0.00002180
Iteration 68/1000 | Loss: 0.00002180
Iteration 69/1000 | Loss: 0.00002180
Iteration 70/1000 | Loss: 0.00002180
Iteration 71/1000 | Loss: 0.00002180
Iteration 72/1000 | Loss: 0.00002180
Iteration 73/1000 | Loss: 0.00002180
Iteration 74/1000 | Loss: 0.00002180
Iteration 75/1000 | Loss: 0.00002180
Iteration 76/1000 | Loss: 0.00002180
Iteration 77/1000 | Loss: 0.00002180
Iteration 78/1000 | Loss: 0.00002180
Iteration 79/1000 | Loss: 0.00002180
Iteration 80/1000 | Loss: 0.00002180
Iteration 81/1000 | Loss: 0.00002180
Iteration 82/1000 | Loss: 0.00002180
Iteration 83/1000 | Loss: 0.00002180
Iteration 84/1000 | Loss: 0.00002180
Iteration 85/1000 | Loss: 0.00002180
Iteration 86/1000 | Loss: 0.00002180
Iteration 87/1000 | Loss: 0.00002180
Iteration 88/1000 | Loss: 0.00002180
Iteration 89/1000 | Loss: 0.00002180
Iteration 90/1000 | Loss: 0.00002180
Iteration 91/1000 | Loss: 0.00002180
Iteration 92/1000 | Loss: 0.00002180
Iteration 93/1000 | Loss: 0.00002180
Iteration 94/1000 | Loss: 0.00002180
Iteration 95/1000 | Loss: 0.00002180
Iteration 96/1000 | Loss: 0.00002180
Iteration 97/1000 | Loss: 0.00002180
Iteration 98/1000 | Loss: 0.00002180
Iteration 99/1000 | Loss: 0.00002180
Iteration 100/1000 | Loss: 0.00002180
Iteration 101/1000 | Loss: 0.00002180
Iteration 102/1000 | Loss: 0.00002180
Iteration 103/1000 | Loss: 0.00002180
Iteration 104/1000 | Loss: 0.00002180
Iteration 105/1000 | Loss: 0.00002180
Iteration 106/1000 | Loss: 0.00002180
Iteration 107/1000 | Loss: 0.00002180
Iteration 108/1000 | Loss: 0.00002180
Iteration 109/1000 | Loss: 0.00002180
Iteration 110/1000 | Loss: 0.00002180
Iteration 111/1000 | Loss: 0.00002180
Iteration 112/1000 | Loss: 0.00002180
Iteration 113/1000 | Loss: 0.00002180
Iteration 114/1000 | Loss: 0.00002180
Iteration 115/1000 | Loss: 0.00002180
Iteration 116/1000 | Loss: 0.00002180
Iteration 117/1000 | Loss: 0.00002180
Iteration 118/1000 | Loss: 0.00002180
Iteration 119/1000 | Loss: 0.00002180
Iteration 120/1000 | Loss: 0.00002180
Iteration 121/1000 | Loss: 0.00002180
Iteration 122/1000 | Loss: 0.00002180
Iteration 123/1000 | Loss: 0.00002180
Iteration 124/1000 | Loss: 0.00002180
Iteration 125/1000 | Loss: 0.00002180
Iteration 126/1000 | Loss: 0.00002180
Iteration 127/1000 | Loss: 0.00002180
Iteration 128/1000 | Loss: 0.00002180
Iteration 129/1000 | Loss: 0.00002180
Iteration 130/1000 | Loss: 0.00002180
Iteration 131/1000 | Loss: 0.00002180
Iteration 132/1000 | Loss: 0.00002180
Iteration 133/1000 | Loss: 0.00002180
Iteration 134/1000 | Loss: 0.00002180
Iteration 135/1000 | Loss: 0.00002180
Iteration 136/1000 | Loss: 0.00002180
Iteration 137/1000 | Loss: 0.00002180
Iteration 138/1000 | Loss: 0.00002180
Iteration 139/1000 | Loss: 0.00002180
Iteration 140/1000 | Loss: 0.00002180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.1796939108753577e-05, 2.1796939108753577e-05, 2.1796939108753577e-05, 2.1796939108753577e-05, 2.1796939108753577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1796939108753577e-05

Optimization complete. Final v2v error: 3.9195446968078613 mm

Highest mean error: 4.537330627441406 mm for frame 146

Lowest mean error: 3.2434957027435303 mm for frame 0

Saving results

Total time: 33.963048696517944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770883
Iteration 2/25 | Loss: 0.00156983
Iteration 3/25 | Loss: 0.00094677
Iteration 4/25 | Loss: 0.00085149
Iteration 5/25 | Loss: 0.00084603
Iteration 6/25 | Loss: 0.00084582
Iteration 7/25 | Loss: 0.00084582
Iteration 8/25 | Loss: 0.00084582
Iteration 9/25 | Loss: 0.00084582
Iteration 10/25 | Loss: 0.00084582
Iteration 11/25 | Loss: 0.00084582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008458198863081634, 0.0008458198863081634, 0.0008458198863081634, 0.0008458198863081634, 0.0008458198863081634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008458198863081634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42603290
Iteration 2/25 | Loss: 0.00015450
Iteration 3/25 | Loss: 0.00015448
Iteration 4/25 | Loss: 0.00015448
Iteration 5/25 | Loss: 0.00015448
Iteration 6/25 | Loss: 0.00015448
Iteration 7/25 | Loss: 0.00015448
Iteration 8/25 | Loss: 0.00015448
Iteration 9/25 | Loss: 0.00015448
Iteration 10/25 | Loss: 0.00015448
Iteration 11/25 | Loss: 0.00015448
Iteration 12/25 | Loss: 0.00015448
Iteration 13/25 | Loss: 0.00015448
Iteration 14/25 | Loss: 0.00015448
Iteration 15/25 | Loss: 0.00015448
Iteration 16/25 | Loss: 0.00015448
Iteration 17/25 | Loss: 0.00015448
Iteration 18/25 | Loss: 0.00015448
Iteration 19/25 | Loss: 0.00015448
Iteration 20/25 | Loss: 0.00015448
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00015447501209564507, 0.00015447501209564507, 0.00015447501209564507, 0.00015447501209564507, 0.00015447501209564507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00015447501209564507

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015448
Iteration 2/1000 | Loss: 0.00004733
Iteration 3/1000 | Loss: 0.00003464
Iteration 4/1000 | Loss: 0.00003237
Iteration 5/1000 | Loss: 0.00003091
Iteration 6/1000 | Loss: 0.00003002
Iteration 7/1000 | Loss: 0.00002928
Iteration 8/1000 | Loss: 0.00002861
Iteration 9/1000 | Loss: 0.00002812
Iteration 10/1000 | Loss: 0.00002783
Iteration 11/1000 | Loss: 0.00002755
Iteration 12/1000 | Loss: 0.00002735
Iteration 13/1000 | Loss: 0.00002726
Iteration 14/1000 | Loss: 0.00002725
Iteration 15/1000 | Loss: 0.00002725
Iteration 16/1000 | Loss: 0.00002723
Iteration 17/1000 | Loss: 0.00002713
Iteration 18/1000 | Loss: 0.00002712
Iteration 19/1000 | Loss: 0.00002711
Iteration 20/1000 | Loss: 0.00002711
Iteration 21/1000 | Loss: 0.00002711
Iteration 22/1000 | Loss: 0.00002710
Iteration 23/1000 | Loss: 0.00002709
Iteration 24/1000 | Loss: 0.00002708
Iteration 25/1000 | Loss: 0.00002708
Iteration 26/1000 | Loss: 0.00002708
Iteration 27/1000 | Loss: 0.00002708
Iteration 28/1000 | Loss: 0.00002708
Iteration 29/1000 | Loss: 0.00002708
Iteration 30/1000 | Loss: 0.00002708
Iteration 31/1000 | Loss: 0.00002708
Iteration 32/1000 | Loss: 0.00002708
Iteration 33/1000 | Loss: 0.00002708
Iteration 34/1000 | Loss: 0.00002708
Iteration 35/1000 | Loss: 0.00002708
Iteration 36/1000 | Loss: 0.00002708
Iteration 37/1000 | Loss: 0.00002707
Iteration 38/1000 | Loss: 0.00002707
Iteration 39/1000 | Loss: 0.00002707
Iteration 40/1000 | Loss: 0.00002706
Iteration 41/1000 | Loss: 0.00002706
Iteration 42/1000 | Loss: 0.00002706
Iteration 43/1000 | Loss: 0.00002705
Iteration 44/1000 | Loss: 0.00002705
Iteration 45/1000 | Loss: 0.00002705
Iteration 46/1000 | Loss: 0.00002704
Iteration 47/1000 | Loss: 0.00002704
Iteration 48/1000 | Loss: 0.00002704
Iteration 49/1000 | Loss: 0.00002704
Iteration 50/1000 | Loss: 0.00002703
Iteration 51/1000 | Loss: 0.00002703
Iteration 52/1000 | Loss: 0.00002703
Iteration 53/1000 | Loss: 0.00002702
Iteration 54/1000 | Loss: 0.00002702
Iteration 55/1000 | Loss: 0.00002702
Iteration 56/1000 | Loss: 0.00002702
Iteration 57/1000 | Loss: 0.00002702
Iteration 58/1000 | Loss: 0.00002702
Iteration 59/1000 | Loss: 0.00002702
Iteration 60/1000 | Loss: 0.00002702
Iteration 61/1000 | Loss: 0.00002702
Iteration 62/1000 | Loss: 0.00002702
Iteration 63/1000 | Loss: 0.00002701
Iteration 64/1000 | Loss: 0.00002701
Iteration 65/1000 | Loss: 0.00002701
Iteration 66/1000 | Loss: 0.00002701
Iteration 67/1000 | Loss: 0.00002701
Iteration 68/1000 | Loss: 0.00002701
Iteration 69/1000 | Loss: 0.00002701
Iteration 70/1000 | Loss: 0.00002701
Iteration 71/1000 | Loss: 0.00002700
Iteration 72/1000 | Loss: 0.00002700
Iteration 73/1000 | Loss: 0.00002700
Iteration 74/1000 | Loss: 0.00002700
Iteration 75/1000 | Loss: 0.00002700
Iteration 76/1000 | Loss: 0.00002700
Iteration 77/1000 | Loss: 0.00002700
Iteration 78/1000 | Loss: 0.00002700
Iteration 79/1000 | Loss: 0.00002700
Iteration 80/1000 | Loss: 0.00002700
Iteration 81/1000 | Loss: 0.00002700
Iteration 82/1000 | Loss: 0.00002700
Iteration 83/1000 | Loss: 0.00002699
Iteration 84/1000 | Loss: 0.00002699
Iteration 85/1000 | Loss: 0.00002699
Iteration 86/1000 | Loss: 0.00002699
Iteration 87/1000 | Loss: 0.00002699
Iteration 88/1000 | Loss: 0.00002699
Iteration 89/1000 | Loss: 0.00002699
Iteration 90/1000 | Loss: 0.00002699
Iteration 91/1000 | Loss: 0.00002699
Iteration 92/1000 | Loss: 0.00002699
Iteration 93/1000 | Loss: 0.00002699
Iteration 94/1000 | Loss: 0.00002699
Iteration 95/1000 | Loss: 0.00002698
Iteration 96/1000 | Loss: 0.00002698
Iteration 97/1000 | Loss: 0.00002698
Iteration 98/1000 | Loss: 0.00002698
Iteration 99/1000 | Loss: 0.00002698
Iteration 100/1000 | Loss: 0.00002697
Iteration 101/1000 | Loss: 0.00002697
Iteration 102/1000 | Loss: 0.00002697
Iteration 103/1000 | Loss: 0.00002697
Iteration 104/1000 | Loss: 0.00002697
Iteration 105/1000 | Loss: 0.00002697
Iteration 106/1000 | Loss: 0.00002697
Iteration 107/1000 | Loss: 0.00002697
Iteration 108/1000 | Loss: 0.00002697
Iteration 109/1000 | Loss: 0.00002697
Iteration 110/1000 | Loss: 0.00002696
Iteration 111/1000 | Loss: 0.00002696
Iteration 112/1000 | Loss: 0.00002696
Iteration 113/1000 | Loss: 0.00002696
Iteration 114/1000 | Loss: 0.00002696
Iteration 115/1000 | Loss: 0.00002696
Iteration 116/1000 | Loss: 0.00002696
Iteration 117/1000 | Loss: 0.00002696
Iteration 118/1000 | Loss: 0.00002696
Iteration 119/1000 | Loss: 0.00002696
Iteration 120/1000 | Loss: 0.00002696
Iteration 121/1000 | Loss: 0.00002696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.696166848181747e-05, 2.696166848181747e-05, 2.696166848181747e-05, 2.696166848181747e-05, 2.696166848181747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.696166848181747e-05

Optimization complete. Final v2v error: 4.429116249084473 mm

Highest mean error: 4.796728610992432 mm for frame 3

Lowest mean error: 3.8838932514190674 mm for frame 54

Saving results

Total time: 37.77725863456726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032513
Iteration 2/25 | Loss: 0.00215391
Iteration 3/25 | Loss: 0.00153296
Iteration 4/25 | Loss: 0.00120291
Iteration 5/25 | Loss: 0.00171786
Iteration 6/25 | Loss: 0.00119044
Iteration 7/25 | Loss: 0.00095976
Iteration 8/25 | Loss: 0.00088507
Iteration 9/25 | Loss: 0.00085985
Iteration 10/25 | Loss: 0.00084234
Iteration 11/25 | Loss: 0.00083307
Iteration 12/25 | Loss: 0.00083227
Iteration 13/25 | Loss: 0.00083164
Iteration 14/25 | Loss: 0.00083313
Iteration 15/25 | Loss: 0.00083168
Iteration 16/25 | Loss: 0.00083264
Iteration 17/25 | Loss: 0.00083367
Iteration 18/25 | Loss: 0.00083171
Iteration 19/25 | Loss: 0.00083217
Iteration 20/25 | Loss: 0.00083125
Iteration 21/25 | Loss: 0.00083135
Iteration 22/25 | Loss: 0.00083181
Iteration 23/25 | Loss: 0.00082738
Iteration 24/25 | Loss: 0.00082623
Iteration 25/25 | Loss: 0.00082608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39235997
Iteration 2/25 | Loss: 0.00029005
Iteration 3/25 | Loss: 0.00029005
Iteration 4/25 | Loss: 0.00029004
Iteration 5/25 | Loss: 0.00029004
Iteration 6/25 | Loss: 0.00029004
Iteration 7/25 | Loss: 0.00029004
Iteration 8/25 | Loss: 0.00029004
Iteration 9/25 | Loss: 0.00029004
Iteration 10/25 | Loss: 0.00029004
Iteration 11/25 | Loss: 0.00029004
Iteration 12/25 | Loss: 0.00029004
Iteration 13/25 | Loss: 0.00029004
Iteration 14/25 | Loss: 0.00029004
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00029004309908486903, 0.00029004309908486903, 0.00029004309908486903, 0.00029004309908486903, 0.00029004309908486903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029004309908486903

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029004
Iteration 2/1000 | Loss: 0.00005827
Iteration 3/1000 | Loss: 0.00004600
Iteration 4/1000 | Loss: 0.00003620
Iteration 5/1000 | Loss: 0.00003265
Iteration 6/1000 | Loss: 0.00003793
Iteration 7/1000 | Loss: 0.00003382
Iteration 8/1000 | Loss: 0.00004023
Iteration 9/1000 | Loss: 0.00004530
Iteration 10/1000 | Loss: 0.00002948
Iteration 11/1000 | Loss: 0.00002744
Iteration 12/1000 | Loss: 0.00002647
Iteration 13/1000 | Loss: 0.00002587
Iteration 14/1000 | Loss: 0.00002545
Iteration 15/1000 | Loss: 0.00002516
Iteration 16/1000 | Loss: 0.00002506
Iteration 17/1000 | Loss: 0.00002506
Iteration 18/1000 | Loss: 0.00002484
Iteration 19/1000 | Loss: 0.00002463
Iteration 20/1000 | Loss: 0.00002462
Iteration 21/1000 | Loss: 0.00033050
Iteration 22/1000 | Loss: 0.00003163
Iteration 23/1000 | Loss: 0.00003796
Iteration 24/1000 | Loss: 0.00002552
Iteration 25/1000 | Loss: 0.00002446
Iteration 26/1000 | Loss: 0.00009926
Iteration 27/1000 | Loss: 0.00005359
Iteration 28/1000 | Loss: 0.00003894
Iteration 29/1000 | Loss: 0.00002394
Iteration 30/1000 | Loss: 0.00002381
Iteration 31/1000 | Loss: 0.00002379
Iteration 32/1000 | Loss: 0.00002379
Iteration 33/1000 | Loss: 0.00002376
Iteration 34/1000 | Loss: 0.00002376
Iteration 35/1000 | Loss: 0.00002375
Iteration 36/1000 | Loss: 0.00002375
Iteration 37/1000 | Loss: 0.00002375
Iteration 38/1000 | Loss: 0.00002374
Iteration 39/1000 | Loss: 0.00002374
Iteration 40/1000 | Loss: 0.00002374
Iteration 41/1000 | Loss: 0.00002372
Iteration 42/1000 | Loss: 0.00002370
Iteration 43/1000 | Loss: 0.00002370
Iteration 44/1000 | Loss: 0.00002370
Iteration 45/1000 | Loss: 0.00002370
Iteration 46/1000 | Loss: 0.00002370
Iteration 47/1000 | Loss: 0.00002369
Iteration 48/1000 | Loss: 0.00002369
Iteration 49/1000 | Loss: 0.00002369
Iteration 50/1000 | Loss: 0.00002369
Iteration 51/1000 | Loss: 0.00002369
Iteration 52/1000 | Loss: 0.00002368
Iteration 53/1000 | Loss: 0.00002368
Iteration 54/1000 | Loss: 0.00002368
Iteration 55/1000 | Loss: 0.00002367
Iteration 56/1000 | Loss: 0.00002367
Iteration 57/1000 | Loss: 0.00002367
Iteration 58/1000 | Loss: 0.00002367
Iteration 59/1000 | Loss: 0.00002367
Iteration 60/1000 | Loss: 0.00002367
Iteration 61/1000 | Loss: 0.00002367
Iteration 62/1000 | Loss: 0.00002366
Iteration 63/1000 | Loss: 0.00002366
Iteration 64/1000 | Loss: 0.00002366
Iteration 65/1000 | Loss: 0.00002366
Iteration 66/1000 | Loss: 0.00002366
Iteration 67/1000 | Loss: 0.00002366
Iteration 68/1000 | Loss: 0.00002366
Iteration 69/1000 | Loss: 0.00002366
Iteration 70/1000 | Loss: 0.00002366
Iteration 71/1000 | Loss: 0.00002366
Iteration 72/1000 | Loss: 0.00002365
Iteration 73/1000 | Loss: 0.00002365
Iteration 74/1000 | Loss: 0.00002365
Iteration 75/1000 | Loss: 0.00002365
Iteration 76/1000 | Loss: 0.00002365
Iteration 77/1000 | Loss: 0.00002365
Iteration 78/1000 | Loss: 0.00002365
Iteration 79/1000 | Loss: 0.00002365
Iteration 80/1000 | Loss: 0.00002365
Iteration 81/1000 | Loss: 0.00002365
Iteration 82/1000 | Loss: 0.00002365
Iteration 83/1000 | Loss: 0.00002365
Iteration 84/1000 | Loss: 0.00002365
Iteration 85/1000 | Loss: 0.00002365
Iteration 86/1000 | Loss: 0.00002365
Iteration 87/1000 | Loss: 0.00002365
Iteration 88/1000 | Loss: 0.00002365
Iteration 89/1000 | Loss: 0.00002365
Iteration 90/1000 | Loss: 0.00002365
Iteration 91/1000 | Loss: 0.00002365
Iteration 92/1000 | Loss: 0.00002365
Iteration 93/1000 | Loss: 0.00002365
Iteration 94/1000 | Loss: 0.00002365
Iteration 95/1000 | Loss: 0.00002365
Iteration 96/1000 | Loss: 0.00002365
Iteration 97/1000 | Loss: 0.00002365
Iteration 98/1000 | Loss: 0.00002365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.3647662601433694e-05, 2.3647662601433694e-05, 2.3647662601433694e-05, 2.3647662601433694e-05, 2.3647662601433694e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3647662601433694e-05

Optimization complete. Final v2v error: 3.9891488552093506 mm

Highest mean error: 10.667424201965332 mm for frame 61

Lowest mean error: 3.7199153900146484 mm for frame 42

Saving results

Total time: 87.2148175239563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390262
Iteration 2/25 | Loss: 0.00087079
Iteration 3/25 | Loss: 0.00074906
Iteration 4/25 | Loss: 0.00072427
Iteration 5/25 | Loss: 0.00071435
Iteration 6/25 | Loss: 0.00071162
Iteration 7/25 | Loss: 0.00071125
Iteration 8/25 | Loss: 0.00071125
Iteration 9/25 | Loss: 0.00071125
Iteration 10/25 | Loss: 0.00071125
Iteration 11/25 | Loss: 0.00071125
Iteration 12/25 | Loss: 0.00071125
Iteration 13/25 | Loss: 0.00071125
Iteration 14/25 | Loss: 0.00071125
Iteration 15/25 | Loss: 0.00071125
Iteration 16/25 | Loss: 0.00071125
Iteration 17/25 | Loss: 0.00071125
Iteration 18/25 | Loss: 0.00071125
Iteration 19/25 | Loss: 0.00071125
Iteration 20/25 | Loss: 0.00071125
Iteration 21/25 | Loss: 0.00071125
Iteration 22/25 | Loss: 0.00071125
Iteration 23/25 | Loss: 0.00071125
Iteration 24/25 | Loss: 0.00071125
Iteration 25/25 | Loss: 0.00071125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.00050187
Iteration 2/25 | Loss: 0.00014587
Iteration 3/25 | Loss: 0.00014586
Iteration 4/25 | Loss: 0.00014586
Iteration 5/25 | Loss: 0.00014586
Iteration 6/25 | Loss: 0.00014586
Iteration 7/25 | Loss: 0.00014586
Iteration 8/25 | Loss: 0.00014586
Iteration 9/25 | Loss: 0.00014586
Iteration 10/25 | Loss: 0.00014586
Iteration 11/25 | Loss: 0.00014586
Iteration 12/25 | Loss: 0.00014586
Iteration 13/25 | Loss: 0.00014586
Iteration 14/25 | Loss: 0.00014586
Iteration 15/25 | Loss: 0.00014586
Iteration 16/25 | Loss: 0.00014586
Iteration 17/25 | Loss: 0.00014586
Iteration 18/25 | Loss: 0.00014586
Iteration 19/25 | Loss: 0.00014586
Iteration 20/25 | Loss: 0.00014586
Iteration 21/25 | Loss: 0.00014586
Iteration 22/25 | Loss: 0.00014586
Iteration 23/25 | Loss: 0.00014586
Iteration 24/25 | Loss: 0.00014586
Iteration 25/25 | Loss: 0.00014586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00014586
Iteration 2/1000 | Loss: 0.00006429
Iteration 3/1000 | Loss: 0.00004091
Iteration 4/1000 | Loss: 0.00003593
Iteration 5/1000 | Loss: 0.00003361
Iteration 6/1000 | Loss: 0.00003195
Iteration 7/1000 | Loss: 0.00003099
Iteration 8/1000 | Loss: 0.00002974
Iteration 9/1000 | Loss: 0.00002900
Iteration 10/1000 | Loss: 0.00002856
Iteration 11/1000 | Loss: 0.00002823
Iteration 12/1000 | Loss: 0.00002800
Iteration 13/1000 | Loss: 0.00002778
Iteration 14/1000 | Loss: 0.00002763
Iteration 15/1000 | Loss: 0.00002749
Iteration 16/1000 | Loss: 0.00002744
Iteration 17/1000 | Loss: 0.00002737
Iteration 18/1000 | Loss: 0.00002732
Iteration 19/1000 | Loss: 0.00002731
Iteration 20/1000 | Loss: 0.00002731
Iteration 21/1000 | Loss: 0.00002730
Iteration 22/1000 | Loss: 0.00002727
Iteration 23/1000 | Loss: 0.00002724
Iteration 24/1000 | Loss: 0.00002724
Iteration 25/1000 | Loss: 0.00002718
Iteration 26/1000 | Loss: 0.00002712
Iteration 27/1000 | Loss: 0.00002709
Iteration 28/1000 | Loss: 0.00002704
Iteration 29/1000 | Loss: 0.00002703
Iteration 30/1000 | Loss: 0.00002701
Iteration 31/1000 | Loss: 0.00002701
Iteration 32/1000 | Loss: 0.00002698
Iteration 33/1000 | Loss: 0.00002698
Iteration 34/1000 | Loss: 0.00002698
Iteration 35/1000 | Loss: 0.00002698
Iteration 36/1000 | Loss: 0.00002698
Iteration 37/1000 | Loss: 0.00002698
Iteration 38/1000 | Loss: 0.00002697
Iteration 39/1000 | Loss: 0.00002697
Iteration 40/1000 | Loss: 0.00002697
Iteration 41/1000 | Loss: 0.00002697
Iteration 42/1000 | Loss: 0.00002697
Iteration 43/1000 | Loss: 0.00002697
Iteration 44/1000 | Loss: 0.00002696
Iteration 45/1000 | Loss: 0.00002695
Iteration 46/1000 | Loss: 0.00002695
Iteration 47/1000 | Loss: 0.00002695
Iteration 48/1000 | Loss: 0.00002695
Iteration 49/1000 | Loss: 0.00002694
Iteration 50/1000 | Loss: 0.00002694
Iteration 51/1000 | Loss: 0.00002694
Iteration 52/1000 | Loss: 0.00002694
Iteration 53/1000 | Loss: 0.00002693
Iteration 54/1000 | Loss: 0.00002693
Iteration 55/1000 | Loss: 0.00002693
Iteration 56/1000 | Loss: 0.00002693
Iteration 57/1000 | Loss: 0.00002692
Iteration 58/1000 | Loss: 0.00002692
Iteration 59/1000 | Loss: 0.00002692
Iteration 60/1000 | Loss: 0.00002692
Iteration 61/1000 | Loss: 0.00002691
Iteration 62/1000 | Loss: 0.00002691
Iteration 63/1000 | Loss: 0.00002691
Iteration 64/1000 | Loss: 0.00002691
Iteration 65/1000 | Loss: 0.00002691
Iteration 66/1000 | Loss: 0.00002691
Iteration 67/1000 | Loss: 0.00002691
Iteration 68/1000 | Loss: 0.00002691
Iteration 69/1000 | Loss: 0.00002690
Iteration 70/1000 | Loss: 0.00002690
Iteration 71/1000 | Loss: 0.00002690
Iteration 72/1000 | Loss: 0.00002690
Iteration 73/1000 | Loss: 0.00002690
Iteration 74/1000 | Loss: 0.00002689
Iteration 75/1000 | Loss: 0.00002689
Iteration 76/1000 | Loss: 0.00002688
Iteration 77/1000 | Loss: 0.00002688
Iteration 78/1000 | Loss: 0.00002688
Iteration 79/1000 | Loss: 0.00002688
Iteration 80/1000 | Loss: 0.00002687
Iteration 81/1000 | Loss: 0.00002687
Iteration 82/1000 | Loss: 0.00002686
Iteration 83/1000 | Loss: 0.00002686
Iteration 84/1000 | Loss: 0.00002686
Iteration 85/1000 | Loss: 0.00002685
Iteration 86/1000 | Loss: 0.00002685
Iteration 87/1000 | Loss: 0.00002685
Iteration 88/1000 | Loss: 0.00002685
Iteration 89/1000 | Loss: 0.00002685
Iteration 90/1000 | Loss: 0.00002684
Iteration 91/1000 | Loss: 0.00002684
Iteration 92/1000 | Loss: 0.00002684
Iteration 93/1000 | Loss: 0.00002684
Iteration 94/1000 | Loss: 0.00002684
Iteration 95/1000 | Loss: 0.00002684
Iteration 96/1000 | Loss: 0.00002684
Iteration 97/1000 | Loss: 0.00002684
Iteration 98/1000 | Loss: 0.00002684
Iteration 99/1000 | Loss: 0.00002684
Iteration 100/1000 | Loss: 0.00002684
Iteration 101/1000 | Loss: 0.00002684
Iteration 102/1000 | Loss: 0.00002683
Iteration 103/1000 | Loss: 0.00002683
Iteration 104/1000 | Loss: 0.00002683
Iteration 105/1000 | Loss: 0.00002683
Iteration 106/1000 | Loss: 0.00002683
Iteration 107/1000 | Loss: 0.00002683
Iteration 108/1000 | Loss: 0.00002683
Iteration 109/1000 | Loss: 0.00002683
Iteration 110/1000 | Loss: 0.00002683
Iteration 111/1000 | Loss: 0.00002683
Iteration 112/1000 | Loss: 0.00002683
Iteration 113/1000 | Loss: 0.00002683
Iteration 114/1000 | Loss: 0.00002683
Iteration 115/1000 | Loss: 0.00002683
Iteration 116/1000 | Loss: 0.00002682
Iteration 117/1000 | Loss: 0.00002682
Iteration 118/1000 | Loss: 0.00002682
Iteration 119/1000 | Loss: 0.00002682
Iteration 120/1000 | Loss: 0.00002682
Iteration 121/1000 | Loss: 0.00002682
Iteration 122/1000 | Loss: 0.00002682
Iteration 123/1000 | Loss: 0.00002682
Iteration 124/1000 | Loss: 0.00002682
Iteration 125/1000 | Loss: 0.00002681
Iteration 126/1000 | Loss: 0.00002681
Iteration 127/1000 | Loss: 0.00002681
Iteration 128/1000 | Loss: 0.00002681
Iteration 129/1000 | Loss: 0.00002681
Iteration 130/1000 | Loss: 0.00002681
Iteration 131/1000 | Loss: 0.00002681
Iteration 132/1000 | Loss: 0.00002681
Iteration 133/1000 | Loss: 0.00002681
Iteration 134/1000 | Loss: 0.00002681
Iteration 135/1000 | Loss: 0.00002680
Iteration 136/1000 | Loss: 0.00002680
Iteration 137/1000 | Loss: 0.00002680
Iteration 138/1000 | Loss: 0.00002680
Iteration 139/1000 | Loss: 0.00002680
Iteration 140/1000 | Loss: 0.00002680
Iteration 141/1000 | Loss: 0.00002680
Iteration 142/1000 | Loss: 0.00002680
Iteration 143/1000 | Loss: 0.00002680
Iteration 144/1000 | Loss: 0.00002680
Iteration 145/1000 | Loss: 0.00002680
Iteration 146/1000 | Loss: 0.00002680
Iteration 147/1000 | Loss: 0.00002680
Iteration 148/1000 | Loss: 0.00002680
Iteration 149/1000 | Loss: 0.00002680
Iteration 150/1000 | Loss: 0.00002680
Iteration 151/1000 | Loss: 0.00002680
Iteration 152/1000 | Loss: 0.00002680
Iteration 153/1000 | Loss: 0.00002680
Iteration 154/1000 | Loss: 0.00002680
Iteration 155/1000 | Loss: 0.00002680
Iteration 156/1000 | Loss: 0.00002680
Iteration 157/1000 | Loss: 0.00002680
Iteration 158/1000 | Loss: 0.00002680
Iteration 159/1000 | Loss: 0.00002680
Iteration 160/1000 | Loss: 0.00002680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.679827593965456e-05, 2.679827593965456e-05, 2.679827593965456e-05, 2.679827593965456e-05, 2.679827593965456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.679827593965456e-05

Optimization complete. Final v2v error: 4.3273162841796875 mm

Highest mean error: 5.286308765411377 mm for frame 63

Lowest mean error: 3.8361029624938965 mm for frame 132

Saving results

Total time: 45.07075548171997
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844469
Iteration 2/25 | Loss: 0.00075080
Iteration 3/25 | Loss: 0.00061542
Iteration 4/25 | Loss: 0.00060363
Iteration 5/25 | Loss: 0.00059940
Iteration 6/25 | Loss: 0.00059844
Iteration 7/25 | Loss: 0.00059844
Iteration 8/25 | Loss: 0.00059844
Iteration 9/25 | Loss: 0.00059844
Iteration 10/25 | Loss: 0.00059844
Iteration 11/25 | Loss: 0.00059844
Iteration 12/25 | Loss: 0.00059844
Iteration 13/25 | Loss: 0.00059844
Iteration 14/25 | Loss: 0.00059844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0005984415183775127, 0.0005984415183775127, 0.0005984415183775127, 0.0005984415183775127, 0.0005984415183775127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005984415183775127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40012765
Iteration 2/25 | Loss: 0.00010080
Iteration 3/25 | Loss: 0.00010080
Iteration 4/25 | Loss: 0.00010080
Iteration 5/25 | Loss: 0.00010080
Iteration 6/25 | Loss: 0.00010080
Iteration 7/25 | Loss: 0.00010080
Iteration 8/25 | Loss: 0.00010080
Iteration 9/25 | Loss: 0.00010080
Iteration 10/25 | Loss: 0.00010080
Iteration 11/25 | Loss: 0.00010080
Iteration 12/25 | Loss: 0.00010080
Iteration 13/25 | Loss: 0.00010080
Iteration 14/25 | Loss: 0.00010080
Iteration 15/25 | Loss: 0.00010080
Iteration 16/25 | Loss: 0.00010080
Iteration 17/25 | Loss: 0.00010080
Iteration 18/25 | Loss: 0.00010080
Iteration 19/25 | Loss: 0.00010080
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00010079739149659872, 0.00010079739149659872, 0.00010079739149659872, 0.00010079739149659872, 0.00010079739149659872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00010079739149659872

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00010080
Iteration 2/1000 | Loss: 0.00002389
Iteration 3/1000 | Loss: 0.00001560
Iteration 4/1000 | Loss: 0.00001430
Iteration 5/1000 | Loss: 0.00001337
Iteration 6/1000 | Loss: 0.00001277
Iteration 7/1000 | Loss: 0.00001238
Iteration 8/1000 | Loss: 0.00001213
Iteration 9/1000 | Loss: 0.00001194
Iteration 10/1000 | Loss: 0.00001191
Iteration 11/1000 | Loss: 0.00001188
Iteration 12/1000 | Loss: 0.00001185
Iteration 13/1000 | Loss: 0.00001185
Iteration 14/1000 | Loss: 0.00001184
Iteration 15/1000 | Loss: 0.00001184
Iteration 16/1000 | Loss: 0.00001183
Iteration 17/1000 | Loss: 0.00001180
Iteration 18/1000 | Loss: 0.00001179
Iteration 19/1000 | Loss: 0.00001179
Iteration 20/1000 | Loss: 0.00001178
Iteration 21/1000 | Loss: 0.00001174
Iteration 22/1000 | Loss: 0.00001162
Iteration 23/1000 | Loss: 0.00001162
Iteration 24/1000 | Loss: 0.00001162
Iteration 25/1000 | Loss: 0.00001162
Iteration 26/1000 | Loss: 0.00001162
Iteration 27/1000 | Loss: 0.00001161
Iteration 28/1000 | Loss: 0.00001159
Iteration 29/1000 | Loss: 0.00001159
Iteration 30/1000 | Loss: 0.00001158
Iteration 31/1000 | Loss: 0.00001158
Iteration 32/1000 | Loss: 0.00001158
Iteration 33/1000 | Loss: 0.00001158
Iteration 34/1000 | Loss: 0.00001158
Iteration 35/1000 | Loss: 0.00001158
Iteration 36/1000 | Loss: 0.00001156
Iteration 37/1000 | Loss: 0.00001156
Iteration 38/1000 | Loss: 0.00001155
Iteration 39/1000 | Loss: 0.00001155
Iteration 40/1000 | Loss: 0.00001155
Iteration 41/1000 | Loss: 0.00001155
Iteration 42/1000 | Loss: 0.00001155
Iteration 43/1000 | Loss: 0.00001155
Iteration 44/1000 | Loss: 0.00001154
Iteration 45/1000 | Loss: 0.00001154
Iteration 46/1000 | Loss: 0.00001153
Iteration 47/1000 | Loss: 0.00001153
Iteration 48/1000 | Loss: 0.00001151
Iteration 49/1000 | Loss: 0.00001151
Iteration 50/1000 | Loss: 0.00001150
Iteration 51/1000 | Loss: 0.00001150
Iteration 52/1000 | Loss: 0.00001150
Iteration 53/1000 | Loss: 0.00001150
Iteration 54/1000 | Loss: 0.00001149
Iteration 55/1000 | Loss: 0.00001148
Iteration 56/1000 | Loss: 0.00001148
Iteration 57/1000 | Loss: 0.00001146
Iteration 58/1000 | Loss: 0.00001146
Iteration 59/1000 | Loss: 0.00001146
Iteration 60/1000 | Loss: 0.00001145
Iteration 61/1000 | Loss: 0.00001145
Iteration 62/1000 | Loss: 0.00001145
Iteration 63/1000 | Loss: 0.00001145
Iteration 64/1000 | Loss: 0.00001145
Iteration 65/1000 | Loss: 0.00001145
Iteration 66/1000 | Loss: 0.00001145
Iteration 67/1000 | Loss: 0.00001144
Iteration 68/1000 | Loss: 0.00001144
Iteration 69/1000 | Loss: 0.00001143
Iteration 70/1000 | Loss: 0.00001143
Iteration 71/1000 | Loss: 0.00001143
Iteration 72/1000 | Loss: 0.00001142
Iteration 73/1000 | Loss: 0.00001142
Iteration 74/1000 | Loss: 0.00001142
Iteration 75/1000 | Loss: 0.00001142
Iteration 76/1000 | Loss: 0.00001141
Iteration 77/1000 | Loss: 0.00001141
Iteration 78/1000 | Loss: 0.00001141
Iteration 79/1000 | Loss: 0.00001141
Iteration 80/1000 | Loss: 0.00001141
Iteration 81/1000 | Loss: 0.00001140
Iteration 82/1000 | Loss: 0.00001140
Iteration 83/1000 | Loss: 0.00001140
Iteration 84/1000 | Loss: 0.00001140
Iteration 85/1000 | Loss: 0.00001138
Iteration 86/1000 | Loss: 0.00001137
Iteration 87/1000 | Loss: 0.00001137
Iteration 88/1000 | Loss: 0.00001136
Iteration 89/1000 | Loss: 0.00001136
Iteration 90/1000 | Loss: 0.00001136
Iteration 91/1000 | Loss: 0.00001136
Iteration 92/1000 | Loss: 0.00001135
Iteration 93/1000 | Loss: 0.00001135
Iteration 94/1000 | Loss: 0.00001135
Iteration 95/1000 | Loss: 0.00001135
Iteration 96/1000 | Loss: 0.00001135
Iteration 97/1000 | Loss: 0.00001135
Iteration 98/1000 | Loss: 0.00001134
Iteration 99/1000 | Loss: 0.00001134
Iteration 100/1000 | Loss: 0.00001134
Iteration 101/1000 | Loss: 0.00001134
Iteration 102/1000 | Loss: 0.00001134
Iteration 103/1000 | Loss: 0.00001134
Iteration 104/1000 | Loss: 0.00001134
Iteration 105/1000 | Loss: 0.00001134
Iteration 106/1000 | Loss: 0.00001134
Iteration 107/1000 | Loss: 0.00001134
Iteration 108/1000 | Loss: 0.00001134
Iteration 109/1000 | Loss: 0.00001134
Iteration 110/1000 | Loss: 0.00001134
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001133
Iteration 113/1000 | Loss: 0.00001133
Iteration 114/1000 | Loss: 0.00001133
Iteration 115/1000 | Loss: 0.00001133
Iteration 116/1000 | Loss: 0.00001133
Iteration 117/1000 | Loss: 0.00001133
Iteration 118/1000 | Loss: 0.00001133
Iteration 119/1000 | Loss: 0.00001132
Iteration 120/1000 | Loss: 0.00001132
Iteration 121/1000 | Loss: 0.00001132
Iteration 122/1000 | Loss: 0.00001132
Iteration 123/1000 | Loss: 0.00001132
Iteration 124/1000 | Loss: 0.00001132
Iteration 125/1000 | Loss: 0.00001132
Iteration 126/1000 | Loss: 0.00001132
Iteration 127/1000 | Loss: 0.00001132
Iteration 128/1000 | Loss: 0.00001132
Iteration 129/1000 | Loss: 0.00001132
Iteration 130/1000 | Loss: 0.00001132
Iteration 131/1000 | Loss: 0.00001132
Iteration 132/1000 | Loss: 0.00001132
Iteration 133/1000 | Loss: 0.00001132
Iteration 134/1000 | Loss: 0.00001132
Iteration 135/1000 | Loss: 0.00001131
Iteration 136/1000 | Loss: 0.00001131
Iteration 137/1000 | Loss: 0.00001131
Iteration 138/1000 | Loss: 0.00001131
Iteration 139/1000 | Loss: 0.00001131
Iteration 140/1000 | Loss: 0.00001131
Iteration 141/1000 | Loss: 0.00001131
Iteration 142/1000 | Loss: 0.00001131
Iteration 143/1000 | Loss: 0.00001131
Iteration 144/1000 | Loss: 0.00001131
Iteration 145/1000 | Loss: 0.00001131
Iteration 146/1000 | Loss: 0.00001131
Iteration 147/1000 | Loss: 0.00001131
Iteration 148/1000 | Loss: 0.00001131
Iteration 149/1000 | Loss: 0.00001131
Iteration 150/1000 | Loss: 0.00001131
Iteration 151/1000 | Loss: 0.00001131
Iteration 152/1000 | Loss: 0.00001131
Iteration 153/1000 | Loss: 0.00001131
Iteration 154/1000 | Loss: 0.00001131
Iteration 155/1000 | Loss: 0.00001131
Iteration 156/1000 | Loss: 0.00001131
Iteration 157/1000 | Loss: 0.00001131
Iteration 158/1000 | Loss: 0.00001131
Iteration 159/1000 | Loss: 0.00001131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.1306806300126482e-05, 1.1306806300126482e-05, 1.1306806300126482e-05, 1.1306806300126482e-05, 1.1306806300126482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1306806300126482e-05

Optimization complete. Final v2v error: 2.8716177940368652 mm

Highest mean error: 3.0739870071411133 mm for frame 109

Lowest mean error: 2.573953628540039 mm for frame 63

Saving results

Total time: 38.43855142593384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426520
Iteration 2/25 | Loss: 0.00135637
Iteration 3/25 | Loss: 0.00125406
Iteration 4/25 | Loss: 0.00124055
Iteration 5/25 | Loss: 0.00123703
Iteration 6/25 | Loss: 0.00123620
Iteration 7/25 | Loss: 0.00123620
Iteration 8/25 | Loss: 0.00123620
Iteration 9/25 | Loss: 0.00123620
Iteration 10/25 | Loss: 0.00123620
Iteration 11/25 | Loss: 0.00123620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012362023117020726, 0.0012362023117020726, 0.0012362023117020726, 0.0012362023117020726, 0.0012362023117020726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012362023117020726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32602203
Iteration 2/25 | Loss: 0.00112361
Iteration 3/25 | Loss: 0.00112361
Iteration 4/25 | Loss: 0.00112360
Iteration 5/25 | Loss: 0.00112360
Iteration 6/25 | Loss: 0.00112360
Iteration 7/25 | Loss: 0.00112360
Iteration 8/25 | Loss: 0.00112360
Iteration 9/25 | Loss: 0.00112360
Iteration 10/25 | Loss: 0.00112360
Iteration 11/25 | Loss: 0.00112360
Iteration 12/25 | Loss: 0.00112360
Iteration 13/25 | Loss: 0.00112360
Iteration 14/25 | Loss: 0.00112360
Iteration 15/25 | Loss: 0.00112360
Iteration 16/25 | Loss: 0.00112360
Iteration 17/25 | Loss: 0.00112360
Iteration 18/25 | Loss: 0.00112360
Iteration 19/25 | Loss: 0.00112360
Iteration 20/25 | Loss: 0.00112360
Iteration 21/25 | Loss: 0.00112360
Iteration 22/25 | Loss: 0.00112360
Iteration 23/25 | Loss: 0.00112360
Iteration 24/25 | Loss: 0.00112360
Iteration 25/25 | Loss: 0.00112360
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011236026184633374, 0.0011236026184633374, 0.0011236026184633374, 0.0011236026184633374, 0.0011236026184633374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011236026184633374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112360
Iteration 2/1000 | Loss: 0.00002966
Iteration 3/1000 | Loss: 0.00001869
Iteration 4/1000 | Loss: 0.00001617
Iteration 5/1000 | Loss: 0.00001504
Iteration 6/1000 | Loss: 0.00001428
Iteration 7/1000 | Loss: 0.00001367
Iteration 8/1000 | Loss: 0.00001330
Iteration 9/1000 | Loss: 0.00001310
Iteration 10/1000 | Loss: 0.00001280
Iteration 11/1000 | Loss: 0.00001262
Iteration 12/1000 | Loss: 0.00001249
Iteration 13/1000 | Loss: 0.00001247
Iteration 14/1000 | Loss: 0.00001243
Iteration 15/1000 | Loss: 0.00001243
Iteration 16/1000 | Loss: 0.00001242
Iteration 17/1000 | Loss: 0.00001241
Iteration 18/1000 | Loss: 0.00001239
Iteration 19/1000 | Loss: 0.00001237
Iteration 20/1000 | Loss: 0.00001237
Iteration 21/1000 | Loss: 0.00001236
Iteration 22/1000 | Loss: 0.00001235
Iteration 23/1000 | Loss: 0.00001235
Iteration 24/1000 | Loss: 0.00001233
Iteration 25/1000 | Loss: 0.00001233
Iteration 26/1000 | Loss: 0.00001232
Iteration 27/1000 | Loss: 0.00001230
Iteration 28/1000 | Loss: 0.00001229
Iteration 29/1000 | Loss: 0.00001228
Iteration 30/1000 | Loss: 0.00001228
Iteration 31/1000 | Loss: 0.00001228
Iteration 32/1000 | Loss: 0.00001227
Iteration 33/1000 | Loss: 0.00001226
Iteration 34/1000 | Loss: 0.00001225
Iteration 35/1000 | Loss: 0.00001224
Iteration 36/1000 | Loss: 0.00001223
Iteration 37/1000 | Loss: 0.00001223
Iteration 38/1000 | Loss: 0.00001222
Iteration 39/1000 | Loss: 0.00001222
Iteration 40/1000 | Loss: 0.00001221
Iteration 41/1000 | Loss: 0.00001221
Iteration 42/1000 | Loss: 0.00001220
Iteration 43/1000 | Loss: 0.00001220
Iteration 44/1000 | Loss: 0.00001220
Iteration 45/1000 | Loss: 0.00001218
Iteration 46/1000 | Loss: 0.00001218
Iteration 47/1000 | Loss: 0.00001217
Iteration 48/1000 | Loss: 0.00001217
Iteration 49/1000 | Loss: 0.00001217
Iteration 50/1000 | Loss: 0.00001217
Iteration 51/1000 | Loss: 0.00001217
Iteration 52/1000 | Loss: 0.00001217
Iteration 53/1000 | Loss: 0.00001217
Iteration 54/1000 | Loss: 0.00001217
Iteration 55/1000 | Loss: 0.00001217
Iteration 56/1000 | Loss: 0.00001217
Iteration 57/1000 | Loss: 0.00001217
Iteration 58/1000 | Loss: 0.00001216
Iteration 59/1000 | Loss: 0.00001216
Iteration 60/1000 | Loss: 0.00001214
Iteration 61/1000 | Loss: 0.00001214
Iteration 62/1000 | Loss: 0.00001214
Iteration 63/1000 | Loss: 0.00001214
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001213
Iteration 67/1000 | Loss: 0.00001213
Iteration 68/1000 | Loss: 0.00001213
Iteration 69/1000 | Loss: 0.00001213
Iteration 70/1000 | Loss: 0.00001213
Iteration 71/1000 | Loss: 0.00001213
Iteration 72/1000 | Loss: 0.00001213
Iteration 73/1000 | Loss: 0.00001213
Iteration 74/1000 | Loss: 0.00001213
Iteration 75/1000 | Loss: 0.00001213
Iteration 76/1000 | Loss: 0.00001212
Iteration 77/1000 | Loss: 0.00001212
Iteration 78/1000 | Loss: 0.00001211
Iteration 79/1000 | Loss: 0.00001211
Iteration 80/1000 | Loss: 0.00001210
Iteration 81/1000 | Loss: 0.00001209
Iteration 82/1000 | Loss: 0.00001208
Iteration 83/1000 | Loss: 0.00001208
Iteration 84/1000 | Loss: 0.00001208
Iteration 85/1000 | Loss: 0.00001208
Iteration 86/1000 | Loss: 0.00001208
Iteration 87/1000 | Loss: 0.00001208
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001207
Iteration 91/1000 | Loss: 0.00001207
Iteration 92/1000 | Loss: 0.00001207
Iteration 93/1000 | Loss: 0.00001207
Iteration 94/1000 | Loss: 0.00001206
Iteration 95/1000 | Loss: 0.00001204
Iteration 96/1000 | Loss: 0.00001204
Iteration 97/1000 | Loss: 0.00001203
Iteration 98/1000 | Loss: 0.00001203
Iteration 99/1000 | Loss: 0.00001203
Iteration 100/1000 | Loss: 0.00001203
Iteration 101/1000 | Loss: 0.00001203
Iteration 102/1000 | Loss: 0.00001203
Iteration 103/1000 | Loss: 0.00001203
Iteration 104/1000 | Loss: 0.00001203
Iteration 105/1000 | Loss: 0.00001202
Iteration 106/1000 | Loss: 0.00001202
Iteration 107/1000 | Loss: 0.00001202
Iteration 108/1000 | Loss: 0.00001202
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001200
Iteration 111/1000 | Loss: 0.00001200
Iteration 112/1000 | Loss: 0.00001200
Iteration 113/1000 | Loss: 0.00001200
Iteration 114/1000 | Loss: 0.00001199
Iteration 115/1000 | Loss: 0.00001199
Iteration 116/1000 | Loss: 0.00001199
Iteration 117/1000 | Loss: 0.00001199
Iteration 118/1000 | Loss: 0.00001199
Iteration 119/1000 | Loss: 0.00001199
Iteration 120/1000 | Loss: 0.00001198
Iteration 121/1000 | Loss: 0.00001198
Iteration 122/1000 | Loss: 0.00001198
Iteration 123/1000 | Loss: 0.00001198
Iteration 124/1000 | Loss: 0.00001198
Iteration 125/1000 | Loss: 0.00001198
Iteration 126/1000 | Loss: 0.00001198
Iteration 127/1000 | Loss: 0.00001198
Iteration 128/1000 | Loss: 0.00001198
Iteration 129/1000 | Loss: 0.00001198
Iteration 130/1000 | Loss: 0.00001197
Iteration 131/1000 | Loss: 0.00001197
Iteration 132/1000 | Loss: 0.00001197
Iteration 133/1000 | Loss: 0.00001197
Iteration 134/1000 | Loss: 0.00001197
Iteration 135/1000 | Loss: 0.00001196
Iteration 136/1000 | Loss: 0.00001196
Iteration 137/1000 | Loss: 0.00001196
Iteration 138/1000 | Loss: 0.00001195
Iteration 139/1000 | Loss: 0.00001195
Iteration 140/1000 | Loss: 0.00001195
Iteration 141/1000 | Loss: 0.00001195
Iteration 142/1000 | Loss: 0.00001195
Iteration 143/1000 | Loss: 0.00001195
Iteration 144/1000 | Loss: 0.00001195
Iteration 145/1000 | Loss: 0.00001195
Iteration 146/1000 | Loss: 0.00001195
Iteration 147/1000 | Loss: 0.00001195
Iteration 148/1000 | Loss: 0.00001194
Iteration 149/1000 | Loss: 0.00001194
Iteration 150/1000 | Loss: 0.00001194
Iteration 151/1000 | Loss: 0.00001193
Iteration 152/1000 | Loss: 0.00001193
Iteration 153/1000 | Loss: 0.00001193
Iteration 154/1000 | Loss: 0.00001192
Iteration 155/1000 | Loss: 0.00001192
Iteration 156/1000 | Loss: 0.00001192
Iteration 157/1000 | Loss: 0.00001192
Iteration 158/1000 | Loss: 0.00001192
Iteration 159/1000 | Loss: 0.00001192
Iteration 160/1000 | Loss: 0.00001192
Iteration 161/1000 | Loss: 0.00001192
Iteration 162/1000 | Loss: 0.00001192
Iteration 163/1000 | Loss: 0.00001191
Iteration 164/1000 | Loss: 0.00001191
Iteration 165/1000 | Loss: 0.00001191
Iteration 166/1000 | Loss: 0.00001191
Iteration 167/1000 | Loss: 0.00001191
Iteration 168/1000 | Loss: 0.00001191
Iteration 169/1000 | Loss: 0.00001191
Iteration 170/1000 | Loss: 0.00001191
Iteration 171/1000 | Loss: 0.00001191
Iteration 172/1000 | Loss: 0.00001191
Iteration 173/1000 | Loss: 0.00001191
Iteration 174/1000 | Loss: 0.00001190
Iteration 175/1000 | Loss: 0.00001190
Iteration 176/1000 | Loss: 0.00001190
Iteration 177/1000 | Loss: 0.00001190
Iteration 178/1000 | Loss: 0.00001189
Iteration 179/1000 | Loss: 0.00001189
Iteration 180/1000 | Loss: 0.00001189
Iteration 181/1000 | Loss: 0.00001189
Iteration 182/1000 | Loss: 0.00001189
Iteration 183/1000 | Loss: 0.00001189
Iteration 184/1000 | Loss: 0.00001189
Iteration 185/1000 | Loss: 0.00001189
Iteration 186/1000 | Loss: 0.00001189
Iteration 187/1000 | Loss: 0.00001189
Iteration 188/1000 | Loss: 0.00001189
Iteration 189/1000 | Loss: 0.00001189
Iteration 190/1000 | Loss: 0.00001189
Iteration 191/1000 | Loss: 0.00001188
Iteration 192/1000 | Loss: 0.00001188
Iteration 193/1000 | Loss: 0.00001188
Iteration 194/1000 | Loss: 0.00001188
Iteration 195/1000 | Loss: 0.00001188
Iteration 196/1000 | Loss: 0.00001187
Iteration 197/1000 | Loss: 0.00001187
Iteration 198/1000 | Loss: 0.00001187
Iteration 199/1000 | Loss: 0.00001187
Iteration 200/1000 | Loss: 0.00001187
Iteration 201/1000 | Loss: 0.00001187
Iteration 202/1000 | Loss: 0.00001187
Iteration 203/1000 | Loss: 0.00001187
Iteration 204/1000 | Loss: 0.00001187
Iteration 205/1000 | Loss: 0.00001187
Iteration 206/1000 | Loss: 0.00001186
Iteration 207/1000 | Loss: 0.00001186
Iteration 208/1000 | Loss: 0.00001186
Iteration 209/1000 | Loss: 0.00001186
Iteration 210/1000 | Loss: 0.00001186
Iteration 211/1000 | Loss: 0.00001186
Iteration 212/1000 | Loss: 0.00001186
Iteration 213/1000 | Loss: 0.00001186
Iteration 214/1000 | Loss: 0.00001186
Iteration 215/1000 | Loss: 0.00001186
Iteration 216/1000 | Loss: 0.00001186
Iteration 217/1000 | Loss: 0.00001186
Iteration 218/1000 | Loss: 0.00001186
Iteration 219/1000 | Loss: 0.00001186
Iteration 220/1000 | Loss: 0.00001186
Iteration 221/1000 | Loss: 0.00001186
Iteration 222/1000 | Loss: 0.00001186
Iteration 223/1000 | Loss: 0.00001186
Iteration 224/1000 | Loss: 0.00001186
Iteration 225/1000 | Loss: 0.00001186
Iteration 226/1000 | Loss: 0.00001186
Iteration 227/1000 | Loss: 0.00001186
Iteration 228/1000 | Loss: 0.00001186
Iteration 229/1000 | Loss: 0.00001186
Iteration 230/1000 | Loss: 0.00001186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.1861179700645152e-05, 1.1861179700645152e-05, 1.1861179700645152e-05, 1.1861179700645152e-05, 1.1861179700645152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1861179700645152e-05

Optimization complete. Final v2v error: 2.9488697052001953 mm

Highest mean error: 3.9329655170440674 mm for frame 55

Lowest mean error: 2.68534255027771 mm for frame 174

Saving results

Total time: 47.704707622528076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813760
Iteration 2/25 | Loss: 0.00149306
Iteration 3/25 | Loss: 0.00126902
Iteration 4/25 | Loss: 0.00125241
Iteration 5/25 | Loss: 0.00125005
Iteration 6/25 | Loss: 0.00124982
Iteration 7/25 | Loss: 0.00124982
Iteration 8/25 | Loss: 0.00124982
Iteration 9/25 | Loss: 0.00124982
Iteration 10/25 | Loss: 0.00124982
Iteration 11/25 | Loss: 0.00124982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012498244177550077, 0.0012498244177550077, 0.0012498244177550077, 0.0012498244177550077, 0.0012498244177550077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012498244177550077

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35060191
Iteration 2/25 | Loss: 0.00090617
Iteration 3/25 | Loss: 0.00090615
Iteration 4/25 | Loss: 0.00090615
Iteration 5/25 | Loss: 0.00090615
Iteration 6/25 | Loss: 0.00090615
Iteration 7/25 | Loss: 0.00090614
Iteration 8/25 | Loss: 0.00090614
Iteration 9/25 | Loss: 0.00090614
Iteration 10/25 | Loss: 0.00090614
Iteration 11/25 | Loss: 0.00090614
Iteration 12/25 | Loss: 0.00090614
Iteration 13/25 | Loss: 0.00090614
Iteration 14/25 | Loss: 0.00090614
Iteration 15/25 | Loss: 0.00090614
Iteration 16/25 | Loss: 0.00090614
Iteration 17/25 | Loss: 0.00090614
Iteration 18/25 | Loss: 0.00090614
Iteration 19/25 | Loss: 0.00090614
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009061438031494617, 0.0009061438031494617, 0.0009061438031494617, 0.0009061438031494617, 0.0009061438031494617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009061438031494617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090614
Iteration 2/1000 | Loss: 0.00002407
Iteration 3/1000 | Loss: 0.00001584
Iteration 4/1000 | Loss: 0.00001409
Iteration 5/1000 | Loss: 0.00001319
Iteration 6/1000 | Loss: 0.00001252
Iteration 7/1000 | Loss: 0.00001213
Iteration 8/1000 | Loss: 0.00001180
Iteration 9/1000 | Loss: 0.00001177
Iteration 10/1000 | Loss: 0.00001170
Iteration 11/1000 | Loss: 0.00001168
Iteration 12/1000 | Loss: 0.00001140
Iteration 13/1000 | Loss: 0.00001126
Iteration 14/1000 | Loss: 0.00001114
Iteration 15/1000 | Loss: 0.00001110
Iteration 16/1000 | Loss: 0.00001102
Iteration 17/1000 | Loss: 0.00001089
Iteration 18/1000 | Loss: 0.00001071
Iteration 19/1000 | Loss: 0.00001064
Iteration 20/1000 | Loss: 0.00001056
Iteration 21/1000 | Loss: 0.00001056
Iteration 22/1000 | Loss: 0.00001055
Iteration 23/1000 | Loss: 0.00001053
Iteration 24/1000 | Loss: 0.00001051
Iteration 25/1000 | Loss: 0.00001049
Iteration 26/1000 | Loss: 0.00001049
Iteration 27/1000 | Loss: 0.00001048
Iteration 28/1000 | Loss: 0.00001047
Iteration 29/1000 | Loss: 0.00001047
Iteration 30/1000 | Loss: 0.00001046
Iteration 31/1000 | Loss: 0.00001046
Iteration 32/1000 | Loss: 0.00001045
Iteration 33/1000 | Loss: 0.00001045
Iteration 34/1000 | Loss: 0.00001045
Iteration 35/1000 | Loss: 0.00001044
Iteration 36/1000 | Loss: 0.00001044
Iteration 37/1000 | Loss: 0.00001044
Iteration 38/1000 | Loss: 0.00001044
Iteration 39/1000 | Loss: 0.00001044
Iteration 40/1000 | Loss: 0.00001043
Iteration 41/1000 | Loss: 0.00001043
Iteration 42/1000 | Loss: 0.00001043
Iteration 43/1000 | Loss: 0.00001043
Iteration 44/1000 | Loss: 0.00001043
Iteration 45/1000 | Loss: 0.00001043
Iteration 46/1000 | Loss: 0.00001043
Iteration 47/1000 | Loss: 0.00001040
Iteration 48/1000 | Loss: 0.00001040
Iteration 49/1000 | Loss: 0.00001039
Iteration 50/1000 | Loss: 0.00001038
Iteration 51/1000 | Loss: 0.00001038
Iteration 52/1000 | Loss: 0.00001037
Iteration 53/1000 | Loss: 0.00001035
Iteration 54/1000 | Loss: 0.00001035
Iteration 55/1000 | Loss: 0.00001035
Iteration 56/1000 | Loss: 0.00001035
Iteration 57/1000 | Loss: 0.00001035
Iteration 58/1000 | Loss: 0.00001035
Iteration 59/1000 | Loss: 0.00001035
Iteration 60/1000 | Loss: 0.00001035
Iteration 61/1000 | Loss: 0.00001035
Iteration 62/1000 | Loss: 0.00001035
Iteration 63/1000 | Loss: 0.00001034
Iteration 64/1000 | Loss: 0.00001034
Iteration 65/1000 | Loss: 0.00001034
Iteration 66/1000 | Loss: 0.00001034
Iteration 67/1000 | Loss: 0.00001034
Iteration 68/1000 | Loss: 0.00001033
Iteration 69/1000 | Loss: 0.00001033
Iteration 70/1000 | Loss: 0.00001032
Iteration 71/1000 | Loss: 0.00001032
Iteration 72/1000 | Loss: 0.00001032
Iteration 73/1000 | Loss: 0.00001031
Iteration 74/1000 | Loss: 0.00001031
Iteration 75/1000 | Loss: 0.00001031
Iteration 76/1000 | Loss: 0.00001031
Iteration 77/1000 | Loss: 0.00001031
Iteration 78/1000 | Loss: 0.00001030
Iteration 79/1000 | Loss: 0.00001030
Iteration 80/1000 | Loss: 0.00001030
Iteration 81/1000 | Loss: 0.00001030
Iteration 82/1000 | Loss: 0.00001030
Iteration 83/1000 | Loss: 0.00001030
Iteration 84/1000 | Loss: 0.00001029
Iteration 85/1000 | Loss: 0.00001028
Iteration 86/1000 | Loss: 0.00001028
Iteration 87/1000 | Loss: 0.00001028
Iteration 88/1000 | Loss: 0.00001028
Iteration 89/1000 | Loss: 0.00001028
Iteration 90/1000 | Loss: 0.00001028
Iteration 91/1000 | Loss: 0.00001027
Iteration 92/1000 | Loss: 0.00001027
Iteration 93/1000 | Loss: 0.00001027
Iteration 94/1000 | Loss: 0.00001027
Iteration 95/1000 | Loss: 0.00001026
Iteration 96/1000 | Loss: 0.00001026
Iteration 97/1000 | Loss: 0.00001025
Iteration 98/1000 | Loss: 0.00001025
Iteration 99/1000 | Loss: 0.00001025
Iteration 100/1000 | Loss: 0.00001025
Iteration 101/1000 | Loss: 0.00001025
Iteration 102/1000 | Loss: 0.00001024
Iteration 103/1000 | Loss: 0.00001024
Iteration 104/1000 | Loss: 0.00001024
Iteration 105/1000 | Loss: 0.00001024
Iteration 106/1000 | Loss: 0.00001024
Iteration 107/1000 | Loss: 0.00001024
Iteration 108/1000 | Loss: 0.00001024
Iteration 109/1000 | Loss: 0.00001024
Iteration 110/1000 | Loss: 0.00001024
Iteration 111/1000 | Loss: 0.00001023
Iteration 112/1000 | Loss: 0.00001023
Iteration 113/1000 | Loss: 0.00001023
Iteration 114/1000 | Loss: 0.00001023
Iteration 115/1000 | Loss: 0.00001023
Iteration 116/1000 | Loss: 0.00001023
Iteration 117/1000 | Loss: 0.00001023
Iteration 118/1000 | Loss: 0.00001023
Iteration 119/1000 | Loss: 0.00001023
Iteration 120/1000 | Loss: 0.00001023
Iteration 121/1000 | Loss: 0.00001023
Iteration 122/1000 | Loss: 0.00001023
Iteration 123/1000 | Loss: 0.00001022
Iteration 124/1000 | Loss: 0.00001022
Iteration 125/1000 | Loss: 0.00001022
Iteration 126/1000 | Loss: 0.00001022
Iteration 127/1000 | Loss: 0.00001022
Iteration 128/1000 | Loss: 0.00001022
Iteration 129/1000 | Loss: 0.00001022
Iteration 130/1000 | Loss: 0.00001022
Iteration 131/1000 | Loss: 0.00001022
Iteration 132/1000 | Loss: 0.00001022
Iteration 133/1000 | Loss: 0.00001022
Iteration 134/1000 | Loss: 0.00001022
Iteration 135/1000 | Loss: 0.00001022
Iteration 136/1000 | Loss: 0.00001022
Iteration 137/1000 | Loss: 0.00001022
Iteration 138/1000 | Loss: 0.00001022
Iteration 139/1000 | Loss: 0.00001021
Iteration 140/1000 | Loss: 0.00001021
Iteration 141/1000 | Loss: 0.00001021
Iteration 142/1000 | Loss: 0.00001021
Iteration 143/1000 | Loss: 0.00001021
Iteration 144/1000 | Loss: 0.00001021
Iteration 145/1000 | Loss: 0.00001021
Iteration 146/1000 | Loss: 0.00001021
Iteration 147/1000 | Loss: 0.00001021
Iteration 148/1000 | Loss: 0.00001021
Iteration 149/1000 | Loss: 0.00001021
Iteration 150/1000 | Loss: 0.00001021
Iteration 151/1000 | Loss: 0.00001021
Iteration 152/1000 | Loss: 0.00001021
Iteration 153/1000 | Loss: 0.00001021
Iteration 154/1000 | Loss: 0.00001021
Iteration 155/1000 | Loss: 0.00001021
Iteration 156/1000 | Loss: 0.00001021
Iteration 157/1000 | Loss: 0.00001021
Iteration 158/1000 | Loss: 0.00001021
Iteration 159/1000 | Loss: 0.00001021
Iteration 160/1000 | Loss: 0.00001021
Iteration 161/1000 | Loss: 0.00001020
Iteration 162/1000 | Loss: 0.00001020
Iteration 163/1000 | Loss: 0.00001020
Iteration 164/1000 | Loss: 0.00001020
Iteration 165/1000 | Loss: 0.00001020
Iteration 166/1000 | Loss: 0.00001020
Iteration 167/1000 | Loss: 0.00001020
Iteration 168/1000 | Loss: 0.00001020
Iteration 169/1000 | Loss: 0.00001020
Iteration 170/1000 | Loss: 0.00001020
Iteration 171/1000 | Loss: 0.00001020
Iteration 172/1000 | Loss: 0.00001020
Iteration 173/1000 | Loss: 0.00001020
Iteration 174/1000 | Loss: 0.00001020
Iteration 175/1000 | Loss: 0.00001020
Iteration 176/1000 | Loss: 0.00001020
Iteration 177/1000 | Loss: 0.00001020
Iteration 178/1000 | Loss: 0.00001020
Iteration 179/1000 | Loss: 0.00001020
Iteration 180/1000 | Loss: 0.00001020
Iteration 181/1000 | Loss: 0.00001020
Iteration 182/1000 | Loss: 0.00001020
Iteration 183/1000 | Loss: 0.00001020
Iteration 184/1000 | Loss: 0.00001020
Iteration 185/1000 | Loss: 0.00001020
Iteration 186/1000 | Loss: 0.00001020
Iteration 187/1000 | Loss: 0.00001020
Iteration 188/1000 | Loss: 0.00001020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.020068066281965e-05, 1.020068066281965e-05, 1.020068066281965e-05, 1.020068066281965e-05, 1.020068066281965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.020068066281965e-05

Optimization complete. Final v2v error: 2.765284538269043 mm

Highest mean error: 3.1957712173461914 mm for frame 166

Lowest mean error: 2.5844240188598633 mm for frame 81

Saving results

Total time: 40.50484228134155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002217
Iteration 2/25 | Loss: 0.00186913
Iteration 3/25 | Loss: 0.00143302
Iteration 4/25 | Loss: 0.00132927
Iteration 5/25 | Loss: 0.00131943
Iteration 6/25 | Loss: 0.00131867
Iteration 7/25 | Loss: 0.00131867
Iteration 8/25 | Loss: 0.00131867
Iteration 9/25 | Loss: 0.00131867
Iteration 10/25 | Loss: 0.00131867
Iteration 11/25 | Loss: 0.00131867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001318667666055262, 0.001318667666055262, 0.001318667666055262, 0.001318667666055262, 0.001318667666055262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001318667666055262

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32593572
Iteration 2/25 | Loss: 0.00097493
Iteration 3/25 | Loss: 0.00097493
Iteration 4/25 | Loss: 0.00097493
Iteration 5/25 | Loss: 0.00097493
Iteration 6/25 | Loss: 0.00097493
Iteration 7/25 | Loss: 0.00097493
Iteration 8/25 | Loss: 0.00097493
Iteration 9/25 | Loss: 0.00097493
Iteration 10/25 | Loss: 0.00097493
Iteration 11/25 | Loss: 0.00097493
Iteration 12/25 | Loss: 0.00097493
Iteration 13/25 | Loss: 0.00097493
Iteration 14/25 | Loss: 0.00097493
Iteration 15/25 | Loss: 0.00097493
Iteration 16/25 | Loss: 0.00097493
Iteration 17/25 | Loss: 0.00097493
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009749289602041245, 0.0009749289602041245, 0.0009749289602041245, 0.0009749289602041245, 0.0009749289602041245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009749289602041245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097493
Iteration 2/1000 | Loss: 0.00003591
Iteration 3/1000 | Loss: 0.00002661
Iteration 4/1000 | Loss: 0.00002503
Iteration 5/1000 | Loss: 0.00002430
Iteration 6/1000 | Loss: 0.00002384
Iteration 7/1000 | Loss: 0.00002343
Iteration 8/1000 | Loss: 0.00002341
Iteration 9/1000 | Loss: 0.00002339
Iteration 10/1000 | Loss: 0.00002329
Iteration 11/1000 | Loss: 0.00002305
Iteration 12/1000 | Loss: 0.00002292
Iteration 13/1000 | Loss: 0.00002291
Iteration 14/1000 | Loss: 0.00002285
Iteration 15/1000 | Loss: 0.00002284
Iteration 16/1000 | Loss: 0.00002279
Iteration 17/1000 | Loss: 0.00002277
Iteration 18/1000 | Loss: 0.00002277
Iteration 19/1000 | Loss: 0.00002268
Iteration 20/1000 | Loss: 0.00002268
Iteration 21/1000 | Loss: 0.00002265
Iteration 22/1000 | Loss: 0.00002265
Iteration 23/1000 | Loss: 0.00002265
Iteration 24/1000 | Loss: 0.00002265
Iteration 25/1000 | Loss: 0.00002264
Iteration 26/1000 | Loss: 0.00002264
Iteration 27/1000 | Loss: 0.00002264
Iteration 28/1000 | Loss: 0.00002264
Iteration 29/1000 | Loss: 0.00002263
Iteration 30/1000 | Loss: 0.00002262
Iteration 31/1000 | Loss: 0.00002262
Iteration 32/1000 | Loss: 0.00002262
Iteration 33/1000 | Loss: 0.00002262
Iteration 34/1000 | Loss: 0.00002261
Iteration 35/1000 | Loss: 0.00002261
Iteration 36/1000 | Loss: 0.00002261
Iteration 37/1000 | Loss: 0.00002261
Iteration 38/1000 | Loss: 0.00002261
Iteration 39/1000 | Loss: 0.00002261
Iteration 40/1000 | Loss: 0.00002261
Iteration 41/1000 | Loss: 0.00002261
Iteration 42/1000 | Loss: 0.00002261
Iteration 43/1000 | Loss: 0.00002261
Iteration 44/1000 | Loss: 0.00002260
Iteration 45/1000 | Loss: 0.00002260
Iteration 46/1000 | Loss: 0.00002260
Iteration 47/1000 | Loss: 0.00002259
Iteration 48/1000 | Loss: 0.00002259
Iteration 49/1000 | Loss: 0.00002259
Iteration 50/1000 | Loss: 0.00002259
Iteration 51/1000 | Loss: 0.00002259
Iteration 52/1000 | Loss: 0.00002259
Iteration 53/1000 | Loss: 0.00002258
Iteration 54/1000 | Loss: 0.00002258
Iteration 55/1000 | Loss: 0.00002258
Iteration 56/1000 | Loss: 0.00002258
Iteration 57/1000 | Loss: 0.00002258
Iteration 58/1000 | Loss: 0.00002258
Iteration 59/1000 | Loss: 0.00002257
Iteration 60/1000 | Loss: 0.00002257
Iteration 61/1000 | Loss: 0.00002257
Iteration 62/1000 | Loss: 0.00002257
Iteration 63/1000 | Loss: 0.00002257
Iteration 64/1000 | Loss: 0.00002257
Iteration 65/1000 | Loss: 0.00002256
Iteration 66/1000 | Loss: 0.00002256
Iteration 67/1000 | Loss: 0.00002256
Iteration 68/1000 | Loss: 0.00002256
Iteration 69/1000 | Loss: 0.00002256
Iteration 70/1000 | Loss: 0.00002256
Iteration 71/1000 | Loss: 0.00002256
Iteration 72/1000 | Loss: 0.00002256
Iteration 73/1000 | Loss: 0.00002256
Iteration 74/1000 | Loss: 0.00002256
Iteration 75/1000 | Loss: 0.00002256
Iteration 76/1000 | Loss: 0.00002255
Iteration 77/1000 | Loss: 0.00002255
Iteration 78/1000 | Loss: 0.00002255
Iteration 79/1000 | Loss: 0.00002255
Iteration 80/1000 | Loss: 0.00002255
Iteration 81/1000 | Loss: 0.00002255
Iteration 82/1000 | Loss: 0.00002255
Iteration 83/1000 | Loss: 0.00002255
Iteration 84/1000 | Loss: 0.00002255
Iteration 85/1000 | Loss: 0.00002255
Iteration 86/1000 | Loss: 0.00002255
Iteration 87/1000 | Loss: 0.00002255
Iteration 88/1000 | Loss: 0.00002255
Iteration 89/1000 | Loss: 0.00002255
Iteration 90/1000 | Loss: 0.00002255
Iteration 91/1000 | Loss: 0.00002255
Iteration 92/1000 | Loss: 0.00002255
Iteration 93/1000 | Loss: 0.00002255
Iteration 94/1000 | Loss: 0.00002255
Iteration 95/1000 | Loss: 0.00002255
Iteration 96/1000 | Loss: 0.00002255
Iteration 97/1000 | Loss: 0.00002255
Iteration 98/1000 | Loss: 0.00002255
Iteration 99/1000 | Loss: 0.00002255
Iteration 100/1000 | Loss: 0.00002255
Iteration 101/1000 | Loss: 0.00002255
Iteration 102/1000 | Loss: 0.00002255
Iteration 103/1000 | Loss: 0.00002255
Iteration 104/1000 | Loss: 0.00002255
Iteration 105/1000 | Loss: 0.00002255
Iteration 106/1000 | Loss: 0.00002255
Iteration 107/1000 | Loss: 0.00002255
Iteration 108/1000 | Loss: 0.00002255
Iteration 109/1000 | Loss: 0.00002255
Iteration 110/1000 | Loss: 0.00002255
Iteration 111/1000 | Loss: 0.00002255
Iteration 112/1000 | Loss: 0.00002255
Iteration 113/1000 | Loss: 0.00002255
Iteration 114/1000 | Loss: 0.00002255
Iteration 115/1000 | Loss: 0.00002255
Iteration 116/1000 | Loss: 0.00002255
Iteration 117/1000 | Loss: 0.00002255
Iteration 118/1000 | Loss: 0.00002255
Iteration 119/1000 | Loss: 0.00002255
Iteration 120/1000 | Loss: 0.00002255
Iteration 121/1000 | Loss: 0.00002255
Iteration 122/1000 | Loss: 0.00002255
Iteration 123/1000 | Loss: 0.00002255
Iteration 124/1000 | Loss: 0.00002255
Iteration 125/1000 | Loss: 0.00002255
Iteration 126/1000 | Loss: 0.00002255
Iteration 127/1000 | Loss: 0.00002255
Iteration 128/1000 | Loss: 0.00002255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.2547601474798284e-05, 2.2547601474798284e-05, 2.2547601474798284e-05, 2.2547601474798284e-05, 2.2547601474798284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2547601474798284e-05

Optimization complete. Final v2v error: 4.01274299621582 mm

Highest mean error: 4.222750663757324 mm for frame 22

Lowest mean error: 3.8670125007629395 mm for frame 94

Saving results

Total time: 30.281542778015137
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381272
Iteration 2/25 | Loss: 0.00127571
Iteration 3/25 | Loss: 0.00120613
Iteration 4/25 | Loss: 0.00119807
Iteration 5/25 | Loss: 0.00119602
Iteration 6/25 | Loss: 0.00119602
Iteration 7/25 | Loss: 0.00119602
Iteration 8/25 | Loss: 0.00119602
Iteration 9/25 | Loss: 0.00119602
Iteration 10/25 | Loss: 0.00119602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001196022960357368, 0.001196022960357368, 0.001196022960357368, 0.001196022960357368, 0.001196022960357368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001196022960357368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34873331
Iteration 2/25 | Loss: 0.00113221
Iteration 3/25 | Loss: 0.00113220
Iteration 4/25 | Loss: 0.00113220
Iteration 5/25 | Loss: 0.00113220
Iteration 6/25 | Loss: 0.00113220
Iteration 7/25 | Loss: 0.00113220
Iteration 8/25 | Loss: 0.00113220
Iteration 9/25 | Loss: 0.00113220
Iteration 10/25 | Loss: 0.00113220
Iteration 11/25 | Loss: 0.00113220
Iteration 12/25 | Loss: 0.00113220
Iteration 13/25 | Loss: 0.00113220
Iteration 14/25 | Loss: 0.00113220
Iteration 15/25 | Loss: 0.00113220
Iteration 16/25 | Loss: 0.00113220
Iteration 17/25 | Loss: 0.00113220
Iteration 18/25 | Loss: 0.00113220
Iteration 19/25 | Loss: 0.00113220
Iteration 20/25 | Loss: 0.00113220
Iteration 21/25 | Loss: 0.00113220
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001132200937718153, 0.001132200937718153, 0.001132200937718153, 0.001132200937718153, 0.001132200937718153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001132200937718153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113220
Iteration 2/1000 | Loss: 0.00002594
Iteration 3/1000 | Loss: 0.00001634
Iteration 4/1000 | Loss: 0.00001341
Iteration 5/1000 | Loss: 0.00001191
Iteration 6/1000 | Loss: 0.00001110
Iteration 7/1000 | Loss: 0.00001050
Iteration 8/1000 | Loss: 0.00001021
Iteration 9/1000 | Loss: 0.00001006
Iteration 10/1000 | Loss: 0.00000998
Iteration 11/1000 | Loss: 0.00000977
Iteration 12/1000 | Loss: 0.00000977
Iteration 13/1000 | Loss: 0.00000975
Iteration 14/1000 | Loss: 0.00000968
Iteration 15/1000 | Loss: 0.00000967
Iteration 16/1000 | Loss: 0.00000965
Iteration 17/1000 | Loss: 0.00000965
Iteration 18/1000 | Loss: 0.00000963
Iteration 19/1000 | Loss: 0.00000962
Iteration 20/1000 | Loss: 0.00000959
Iteration 21/1000 | Loss: 0.00000956
Iteration 22/1000 | Loss: 0.00000955
Iteration 23/1000 | Loss: 0.00000953
Iteration 24/1000 | Loss: 0.00000949
Iteration 25/1000 | Loss: 0.00000941
Iteration 26/1000 | Loss: 0.00000941
Iteration 27/1000 | Loss: 0.00000940
Iteration 28/1000 | Loss: 0.00000939
Iteration 29/1000 | Loss: 0.00000939
Iteration 30/1000 | Loss: 0.00000939
Iteration 31/1000 | Loss: 0.00000939
Iteration 32/1000 | Loss: 0.00000939
Iteration 33/1000 | Loss: 0.00000939
Iteration 34/1000 | Loss: 0.00000939
Iteration 35/1000 | Loss: 0.00000938
Iteration 36/1000 | Loss: 0.00000938
Iteration 37/1000 | Loss: 0.00000935
Iteration 38/1000 | Loss: 0.00000934
Iteration 39/1000 | Loss: 0.00000934
Iteration 40/1000 | Loss: 0.00000933
Iteration 41/1000 | Loss: 0.00000933
Iteration 42/1000 | Loss: 0.00000932
Iteration 43/1000 | Loss: 0.00000931
Iteration 44/1000 | Loss: 0.00000930
Iteration 45/1000 | Loss: 0.00000930
Iteration 46/1000 | Loss: 0.00000929
Iteration 47/1000 | Loss: 0.00000929
Iteration 48/1000 | Loss: 0.00000929
Iteration 49/1000 | Loss: 0.00000929
Iteration 50/1000 | Loss: 0.00000928
Iteration 51/1000 | Loss: 0.00000928
Iteration 52/1000 | Loss: 0.00000927
Iteration 53/1000 | Loss: 0.00000926
Iteration 54/1000 | Loss: 0.00000925
Iteration 55/1000 | Loss: 0.00000925
Iteration 56/1000 | Loss: 0.00000924
Iteration 57/1000 | Loss: 0.00000924
Iteration 58/1000 | Loss: 0.00000924
Iteration 59/1000 | Loss: 0.00000924
Iteration 60/1000 | Loss: 0.00000923
Iteration 61/1000 | Loss: 0.00000923
Iteration 62/1000 | Loss: 0.00000922
Iteration 63/1000 | Loss: 0.00000920
Iteration 64/1000 | Loss: 0.00000919
Iteration 65/1000 | Loss: 0.00000919
Iteration 66/1000 | Loss: 0.00000919
Iteration 67/1000 | Loss: 0.00000918
Iteration 68/1000 | Loss: 0.00000918
Iteration 69/1000 | Loss: 0.00000917
Iteration 70/1000 | Loss: 0.00000917
Iteration 71/1000 | Loss: 0.00000916
Iteration 72/1000 | Loss: 0.00000916
Iteration 73/1000 | Loss: 0.00000915
Iteration 74/1000 | Loss: 0.00000915
Iteration 75/1000 | Loss: 0.00000915
Iteration 76/1000 | Loss: 0.00000914
Iteration 77/1000 | Loss: 0.00000914
Iteration 78/1000 | Loss: 0.00000914
Iteration 79/1000 | Loss: 0.00000913
Iteration 80/1000 | Loss: 0.00000913
Iteration 81/1000 | Loss: 0.00000912
Iteration 82/1000 | Loss: 0.00000911
Iteration 83/1000 | Loss: 0.00000911
Iteration 84/1000 | Loss: 0.00000910
Iteration 85/1000 | Loss: 0.00000909
Iteration 86/1000 | Loss: 0.00000908
Iteration 87/1000 | Loss: 0.00000908
Iteration 88/1000 | Loss: 0.00000908
Iteration 89/1000 | Loss: 0.00000908
Iteration 90/1000 | Loss: 0.00000907
Iteration 91/1000 | Loss: 0.00000907
Iteration 92/1000 | Loss: 0.00000906
Iteration 93/1000 | Loss: 0.00000906
Iteration 94/1000 | Loss: 0.00000906
Iteration 95/1000 | Loss: 0.00000905
Iteration 96/1000 | Loss: 0.00000905
Iteration 97/1000 | Loss: 0.00000905
Iteration 98/1000 | Loss: 0.00000905
Iteration 99/1000 | Loss: 0.00000904
Iteration 100/1000 | Loss: 0.00000903
Iteration 101/1000 | Loss: 0.00000902
Iteration 102/1000 | Loss: 0.00000902
Iteration 103/1000 | Loss: 0.00000902
Iteration 104/1000 | Loss: 0.00000902
Iteration 105/1000 | Loss: 0.00000902
Iteration 106/1000 | Loss: 0.00000902
Iteration 107/1000 | Loss: 0.00000902
Iteration 108/1000 | Loss: 0.00000902
Iteration 109/1000 | Loss: 0.00000901
Iteration 110/1000 | Loss: 0.00000901
Iteration 111/1000 | Loss: 0.00000901
Iteration 112/1000 | Loss: 0.00000901
Iteration 113/1000 | Loss: 0.00000901
Iteration 114/1000 | Loss: 0.00000901
Iteration 115/1000 | Loss: 0.00000901
Iteration 116/1000 | Loss: 0.00000901
Iteration 117/1000 | Loss: 0.00000901
Iteration 118/1000 | Loss: 0.00000900
Iteration 119/1000 | Loss: 0.00000900
Iteration 120/1000 | Loss: 0.00000900
Iteration 121/1000 | Loss: 0.00000900
Iteration 122/1000 | Loss: 0.00000899
Iteration 123/1000 | Loss: 0.00000899
Iteration 124/1000 | Loss: 0.00000899
Iteration 125/1000 | Loss: 0.00000899
Iteration 126/1000 | Loss: 0.00000898
Iteration 127/1000 | Loss: 0.00000898
Iteration 128/1000 | Loss: 0.00000898
Iteration 129/1000 | Loss: 0.00000898
Iteration 130/1000 | Loss: 0.00000898
Iteration 131/1000 | Loss: 0.00000898
Iteration 132/1000 | Loss: 0.00000897
Iteration 133/1000 | Loss: 0.00000897
Iteration 134/1000 | Loss: 0.00000897
Iteration 135/1000 | Loss: 0.00000897
Iteration 136/1000 | Loss: 0.00000897
Iteration 137/1000 | Loss: 0.00000896
Iteration 138/1000 | Loss: 0.00000896
Iteration 139/1000 | Loss: 0.00000896
Iteration 140/1000 | Loss: 0.00000895
Iteration 141/1000 | Loss: 0.00000895
Iteration 142/1000 | Loss: 0.00000895
Iteration 143/1000 | Loss: 0.00000895
Iteration 144/1000 | Loss: 0.00000895
Iteration 145/1000 | Loss: 0.00000895
Iteration 146/1000 | Loss: 0.00000895
Iteration 147/1000 | Loss: 0.00000895
Iteration 148/1000 | Loss: 0.00000894
Iteration 149/1000 | Loss: 0.00000894
Iteration 150/1000 | Loss: 0.00000894
Iteration 151/1000 | Loss: 0.00000894
Iteration 152/1000 | Loss: 0.00000894
Iteration 153/1000 | Loss: 0.00000894
Iteration 154/1000 | Loss: 0.00000894
Iteration 155/1000 | Loss: 0.00000894
Iteration 156/1000 | Loss: 0.00000894
Iteration 157/1000 | Loss: 0.00000894
Iteration 158/1000 | Loss: 0.00000894
Iteration 159/1000 | Loss: 0.00000894
Iteration 160/1000 | Loss: 0.00000894
Iteration 161/1000 | Loss: 0.00000894
Iteration 162/1000 | Loss: 0.00000894
Iteration 163/1000 | Loss: 0.00000893
Iteration 164/1000 | Loss: 0.00000893
Iteration 165/1000 | Loss: 0.00000893
Iteration 166/1000 | Loss: 0.00000893
Iteration 167/1000 | Loss: 0.00000893
Iteration 168/1000 | Loss: 0.00000893
Iteration 169/1000 | Loss: 0.00000893
Iteration 170/1000 | Loss: 0.00000893
Iteration 171/1000 | Loss: 0.00000893
Iteration 172/1000 | Loss: 0.00000893
Iteration 173/1000 | Loss: 0.00000893
Iteration 174/1000 | Loss: 0.00000893
Iteration 175/1000 | Loss: 0.00000893
Iteration 176/1000 | Loss: 0.00000893
Iteration 177/1000 | Loss: 0.00000893
Iteration 178/1000 | Loss: 0.00000893
Iteration 179/1000 | Loss: 0.00000893
Iteration 180/1000 | Loss: 0.00000893
Iteration 181/1000 | Loss: 0.00000893
Iteration 182/1000 | Loss: 0.00000893
Iteration 183/1000 | Loss: 0.00000893
Iteration 184/1000 | Loss: 0.00000893
Iteration 185/1000 | Loss: 0.00000893
Iteration 186/1000 | Loss: 0.00000893
Iteration 187/1000 | Loss: 0.00000893
Iteration 188/1000 | Loss: 0.00000893
Iteration 189/1000 | Loss: 0.00000893
Iteration 190/1000 | Loss: 0.00000893
Iteration 191/1000 | Loss: 0.00000893
Iteration 192/1000 | Loss: 0.00000893
Iteration 193/1000 | Loss: 0.00000893
Iteration 194/1000 | Loss: 0.00000893
Iteration 195/1000 | Loss: 0.00000893
Iteration 196/1000 | Loss: 0.00000893
Iteration 197/1000 | Loss: 0.00000893
Iteration 198/1000 | Loss: 0.00000893
Iteration 199/1000 | Loss: 0.00000893
Iteration 200/1000 | Loss: 0.00000893
Iteration 201/1000 | Loss: 0.00000893
Iteration 202/1000 | Loss: 0.00000893
Iteration 203/1000 | Loss: 0.00000893
Iteration 204/1000 | Loss: 0.00000893
Iteration 205/1000 | Loss: 0.00000893
Iteration 206/1000 | Loss: 0.00000893
Iteration 207/1000 | Loss: 0.00000893
Iteration 208/1000 | Loss: 0.00000893
Iteration 209/1000 | Loss: 0.00000893
Iteration 210/1000 | Loss: 0.00000893
Iteration 211/1000 | Loss: 0.00000893
Iteration 212/1000 | Loss: 0.00000893
Iteration 213/1000 | Loss: 0.00000893
Iteration 214/1000 | Loss: 0.00000893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [8.92800380825065e-06, 8.92800380825065e-06, 8.92800380825065e-06, 8.92800380825065e-06, 8.92800380825065e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.92800380825065e-06

Optimization complete. Final v2v error: 2.578432321548462 mm

Highest mean error: 2.692411422729492 mm for frame 76

Lowest mean error: 2.5210697650909424 mm for frame 112

Saving results

Total time: 40.74193358421326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814331
Iteration 2/25 | Loss: 0.00152134
Iteration 3/25 | Loss: 0.00132610
Iteration 4/25 | Loss: 0.00128836
Iteration 5/25 | Loss: 0.00127601
Iteration 6/25 | Loss: 0.00127294
Iteration 7/25 | Loss: 0.00127310
Iteration 8/25 | Loss: 0.00126771
Iteration 9/25 | Loss: 0.00126612
Iteration 10/25 | Loss: 0.00126569
Iteration 11/25 | Loss: 0.00126561
Iteration 12/25 | Loss: 0.00126560
Iteration 13/25 | Loss: 0.00126560
Iteration 14/25 | Loss: 0.00126560
Iteration 15/25 | Loss: 0.00126560
Iteration 16/25 | Loss: 0.00126560
Iteration 17/25 | Loss: 0.00126560
Iteration 18/25 | Loss: 0.00126557
Iteration 19/25 | Loss: 0.00126556
Iteration 20/25 | Loss: 0.00126556
Iteration 21/25 | Loss: 0.00126556
Iteration 22/25 | Loss: 0.00126556
Iteration 23/25 | Loss: 0.00126556
Iteration 24/25 | Loss: 0.00126555
Iteration 25/25 | Loss: 0.00126555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91820931
Iteration 2/25 | Loss: 0.00118356
Iteration 3/25 | Loss: 0.00118355
Iteration 4/25 | Loss: 0.00118355
Iteration 5/25 | Loss: 0.00118355
Iteration 6/25 | Loss: 0.00118355
Iteration 7/25 | Loss: 0.00118355
Iteration 8/25 | Loss: 0.00118355
Iteration 9/25 | Loss: 0.00118355
Iteration 10/25 | Loss: 0.00118355
Iteration 11/25 | Loss: 0.00118355
Iteration 12/25 | Loss: 0.00118355
Iteration 13/25 | Loss: 0.00118355
Iteration 14/25 | Loss: 0.00118355
Iteration 15/25 | Loss: 0.00118355
Iteration 16/25 | Loss: 0.00118355
Iteration 17/25 | Loss: 0.00118355
Iteration 18/25 | Loss: 0.00118355
Iteration 19/25 | Loss: 0.00118355
Iteration 20/25 | Loss: 0.00118355
Iteration 21/25 | Loss: 0.00118355
Iteration 22/25 | Loss: 0.00118355
Iteration 23/25 | Loss: 0.00118355
Iteration 24/25 | Loss: 0.00118355
Iteration 25/25 | Loss: 0.00118355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118355
Iteration 2/1000 | Loss: 0.00003588
Iteration 3/1000 | Loss: 0.00002349
Iteration 4/1000 | Loss: 0.00001950
Iteration 5/1000 | Loss: 0.00001844
Iteration 6/1000 | Loss: 0.00001750
Iteration 7/1000 | Loss: 0.00001703
Iteration 8/1000 | Loss: 0.00001660
Iteration 9/1000 | Loss: 0.00001644
Iteration 10/1000 | Loss: 0.00001636
Iteration 11/1000 | Loss: 0.00001624
Iteration 12/1000 | Loss: 0.00001610
Iteration 13/1000 | Loss: 0.00001601
Iteration 14/1000 | Loss: 0.00001594
Iteration 15/1000 | Loss: 0.00001583
Iteration 16/1000 | Loss: 0.00001582
Iteration 17/1000 | Loss: 0.00001581
Iteration 18/1000 | Loss: 0.00001573
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001567
Iteration 21/1000 | Loss: 0.00001561
Iteration 22/1000 | Loss: 0.00001559
Iteration 23/1000 | Loss: 0.00001559
Iteration 24/1000 | Loss: 0.00001558
Iteration 25/1000 | Loss: 0.00001555
Iteration 26/1000 | Loss: 0.00001553
Iteration 27/1000 | Loss: 0.00001552
Iteration 28/1000 | Loss: 0.00001552
Iteration 29/1000 | Loss: 0.00001551
Iteration 30/1000 | Loss: 0.00001546
Iteration 31/1000 | Loss: 0.00001545
Iteration 32/1000 | Loss: 0.00001543
Iteration 33/1000 | Loss: 0.00001543
Iteration 34/1000 | Loss: 0.00001542
Iteration 35/1000 | Loss: 0.00001541
Iteration 36/1000 | Loss: 0.00001539
Iteration 37/1000 | Loss: 0.00001539
Iteration 38/1000 | Loss: 0.00001539
Iteration 39/1000 | Loss: 0.00001538
Iteration 40/1000 | Loss: 0.00001538
Iteration 41/1000 | Loss: 0.00001537
Iteration 42/1000 | Loss: 0.00001537
Iteration 43/1000 | Loss: 0.00001537
Iteration 44/1000 | Loss: 0.00001536
Iteration 45/1000 | Loss: 0.00001535
Iteration 46/1000 | Loss: 0.00001535
Iteration 47/1000 | Loss: 0.00001535
Iteration 48/1000 | Loss: 0.00001535
Iteration 49/1000 | Loss: 0.00001535
Iteration 50/1000 | Loss: 0.00001535
Iteration 51/1000 | Loss: 0.00001535
Iteration 52/1000 | Loss: 0.00001534
Iteration 53/1000 | Loss: 0.00001534
Iteration 54/1000 | Loss: 0.00001534
Iteration 55/1000 | Loss: 0.00001534
Iteration 56/1000 | Loss: 0.00001534
Iteration 57/1000 | Loss: 0.00001533
Iteration 58/1000 | Loss: 0.00001533
Iteration 59/1000 | Loss: 0.00001533
Iteration 60/1000 | Loss: 0.00001532
Iteration 61/1000 | Loss: 0.00001532
Iteration 62/1000 | Loss: 0.00001532
Iteration 63/1000 | Loss: 0.00001531
Iteration 64/1000 | Loss: 0.00001530
Iteration 65/1000 | Loss: 0.00001529
Iteration 66/1000 | Loss: 0.00001529
Iteration 67/1000 | Loss: 0.00001529
Iteration 68/1000 | Loss: 0.00001528
Iteration 69/1000 | Loss: 0.00001528
Iteration 70/1000 | Loss: 0.00001527
Iteration 71/1000 | Loss: 0.00001527
Iteration 72/1000 | Loss: 0.00001527
Iteration 73/1000 | Loss: 0.00001527
Iteration 74/1000 | Loss: 0.00001527
Iteration 75/1000 | Loss: 0.00001526
Iteration 76/1000 | Loss: 0.00001526
Iteration 77/1000 | Loss: 0.00001525
Iteration 78/1000 | Loss: 0.00001525
Iteration 79/1000 | Loss: 0.00001525
Iteration 80/1000 | Loss: 0.00001525
Iteration 81/1000 | Loss: 0.00001524
Iteration 82/1000 | Loss: 0.00001524
Iteration 83/1000 | Loss: 0.00001524
Iteration 84/1000 | Loss: 0.00001524
Iteration 85/1000 | Loss: 0.00001524
Iteration 86/1000 | Loss: 0.00001524
Iteration 87/1000 | Loss: 0.00001524
Iteration 88/1000 | Loss: 0.00001524
Iteration 89/1000 | Loss: 0.00001524
Iteration 90/1000 | Loss: 0.00001524
Iteration 91/1000 | Loss: 0.00001523
Iteration 92/1000 | Loss: 0.00001523
Iteration 93/1000 | Loss: 0.00001523
Iteration 94/1000 | Loss: 0.00001523
Iteration 95/1000 | Loss: 0.00001523
Iteration 96/1000 | Loss: 0.00001522
Iteration 97/1000 | Loss: 0.00001522
Iteration 98/1000 | Loss: 0.00001522
Iteration 99/1000 | Loss: 0.00001522
Iteration 100/1000 | Loss: 0.00001522
Iteration 101/1000 | Loss: 0.00001521
Iteration 102/1000 | Loss: 0.00001521
Iteration 103/1000 | Loss: 0.00001521
Iteration 104/1000 | Loss: 0.00001521
Iteration 105/1000 | Loss: 0.00001521
Iteration 106/1000 | Loss: 0.00001521
Iteration 107/1000 | Loss: 0.00001521
Iteration 108/1000 | Loss: 0.00001521
Iteration 109/1000 | Loss: 0.00001521
Iteration 110/1000 | Loss: 0.00001521
Iteration 111/1000 | Loss: 0.00001521
Iteration 112/1000 | Loss: 0.00001521
Iteration 113/1000 | Loss: 0.00001521
Iteration 114/1000 | Loss: 0.00001521
Iteration 115/1000 | Loss: 0.00001521
Iteration 116/1000 | Loss: 0.00001521
Iteration 117/1000 | Loss: 0.00001521
Iteration 118/1000 | Loss: 0.00001520
Iteration 119/1000 | Loss: 0.00001520
Iteration 120/1000 | Loss: 0.00001520
Iteration 121/1000 | Loss: 0.00001520
Iteration 122/1000 | Loss: 0.00001520
Iteration 123/1000 | Loss: 0.00001520
Iteration 124/1000 | Loss: 0.00001520
Iteration 125/1000 | Loss: 0.00001519
Iteration 126/1000 | Loss: 0.00001519
Iteration 127/1000 | Loss: 0.00001519
Iteration 128/1000 | Loss: 0.00001519
Iteration 129/1000 | Loss: 0.00001519
Iteration 130/1000 | Loss: 0.00001519
Iteration 131/1000 | Loss: 0.00001519
Iteration 132/1000 | Loss: 0.00001519
Iteration 133/1000 | Loss: 0.00001519
Iteration 134/1000 | Loss: 0.00001519
Iteration 135/1000 | Loss: 0.00001518
Iteration 136/1000 | Loss: 0.00001518
Iteration 137/1000 | Loss: 0.00001518
Iteration 138/1000 | Loss: 0.00001518
Iteration 139/1000 | Loss: 0.00001518
Iteration 140/1000 | Loss: 0.00001518
Iteration 141/1000 | Loss: 0.00001518
Iteration 142/1000 | Loss: 0.00001518
Iteration 143/1000 | Loss: 0.00001518
Iteration 144/1000 | Loss: 0.00001518
Iteration 145/1000 | Loss: 0.00001517
Iteration 146/1000 | Loss: 0.00001517
Iteration 147/1000 | Loss: 0.00001517
Iteration 148/1000 | Loss: 0.00001517
Iteration 149/1000 | Loss: 0.00001517
Iteration 150/1000 | Loss: 0.00001517
Iteration 151/1000 | Loss: 0.00001516
Iteration 152/1000 | Loss: 0.00001516
Iteration 153/1000 | Loss: 0.00001516
Iteration 154/1000 | Loss: 0.00001516
Iteration 155/1000 | Loss: 0.00001516
Iteration 156/1000 | Loss: 0.00001516
Iteration 157/1000 | Loss: 0.00001516
Iteration 158/1000 | Loss: 0.00001516
Iteration 159/1000 | Loss: 0.00001516
Iteration 160/1000 | Loss: 0.00001516
Iteration 161/1000 | Loss: 0.00001516
Iteration 162/1000 | Loss: 0.00001516
Iteration 163/1000 | Loss: 0.00001516
Iteration 164/1000 | Loss: 0.00001516
Iteration 165/1000 | Loss: 0.00001516
Iteration 166/1000 | Loss: 0.00001516
Iteration 167/1000 | Loss: 0.00001515
Iteration 168/1000 | Loss: 0.00001515
Iteration 169/1000 | Loss: 0.00001515
Iteration 170/1000 | Loss: 0.00001515
Iteration 171/1000 | Loss: 0.00001515
Iteration 172/1000 | Loss: 0.00001515
Iteration 173/1000 | Loss: 0.00001515
Iteration 174/1000 | Loss: 0.00001515
Iteration 175/1000 | Loss: 0.00001515
Iteration 176/1000 | Loss: 0.00001514
Iteration 177/1000 | Loss: 0.00001514
Iteration 178/1000 | Loss: 0.00001514
Iteration 179/1000 | Loss: 0.00001514
Iteration 180/1000 | Loss: 0.00001514
Iteration 181/1000 | Loss: 0.00001513
Iteration 182/1000 | Loss: 0.00001513
Iteration 183/1000 | Loss: 0.00001513
Iteration 184/1000 | Loss: 0.00001513
Iteration 185/1000 | Loss: 0.00001513
Iteration 186/1000 | Loss: 0.00001513
Iteration 187/1000 | Loss: 0.00001513
Iteration 188/1000 | Loss: 0.00001513
Iteration 189/1000 | Loss: 0.00001513
Iteration 190/1000 | Loss: 0.00001513
Iteration 191/1000 | Loss: 0.00001513
Iteration 192/1000 | Loss: 0.00001513
Iteration 193/1000 | Loss: 0.00001513
Iteration 194/1000 | Loss: 0.00001513
Iteration 195/1000 | Loss: 0.00001513
Iteration 196/1000 | Loss: 0.00001513
Iteration 197/1000 | Loss: 0.00001513
Iteration 198/1000 | Loss: 0.00001513
Iteration 199/1000 | Loss: 0.00001513
Iteration 200/1000 | Loss: 0.00001513
Iteration 201/1000 | Loss: 0.00001513
Iteration 202/1000 | Loss: 0.00001513
Iteration 203/1000 | Loss: 0.00001513
Iteration 204/1000 | Loss: 0.00001513
Iteration 205/1000 | Loss: 0.00001513
Iteration 206/1000 | Loss: 0.00001513
Iteration 207/1000 | Loss: 0.00001513
Iteration 208/1000 | Loss: 0.00001513
Iteration 209/1000 | Loss: 0.00001513
Iteration 210/1000 | Loss: 0.00001513
Iteration 211/1000 | Loss: 0.00001513
Iteration 212/1000 | Loss: 0.00001513
Iteration 213/1000 | Loss: 0.00001513
Iteration 214/1000 | Loss: 0.00001513
Iteration 215/1000 | Loss: 0.00001513
Iteration 216/1000 | Loss: 0.00001513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.512535527581349e-05, 1.512535527581349e-05, 1.512535527581349e-05, 1.512535527581349e-05, 1.512535527581349e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.512535527581349e-05

Optimization complete. Final v2v error: 3.3024771213531494 mm

Highest mean error: 3.917271852493286 mm for frame 80

Lowest mean error: 2.806906223297119 mm for frame 127

Saving results

Total time: 53.28563618659973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797656
Iteration 2/25 | Loss: 0.00178416
Iteration 3/25 | Loss: 0.00144928
Iteration 4/25 | Loss: 0.00142854
Iteration 5/25 | Loss: 0.00142263
Iteration 6/25 | Loss: 0.00142066
Iteration 7/25 | Loss: 0.00142056
Iteration 8/25 | Loss: 0.00142056
Iteration 9/25 | Loss: 0.00142056
Iteration 10/25 | Loss: 0.00142056
Iteration 11/25 | Loss: 0.00142056
Iteration 12/25 | Loss: 0.00142056
Iteration 13/25 | Loss: 0.00142056
Iteration 14/25 | Loss: 0.00142056
Iteration 15/25 | Loss: 0.00142056
Iteration 16/25 | Loss: 0.00142056
Iteration 17/25 | Loss: 0.00142056
Iteration 18/25 | Loss: 0.00142056
Iteration 19/25 | Loss: 0.00142056
Iteration 20/25 | Loss: 0.00142056
Iteration 21/25 | Loss: 0.00142056
Iteration 22/25 | Loss: 0.00142056
Iteration 23/25 | Loss: 0.00142056
Iteration 24/25 | Loss: 0.00142056
Iteration 25/25 | Loss: 0.00142056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.25984842
Iteration 2/25 | Loss: 0.00130847
Iteration 3/25 | Loss: 0.00130847
Iteration 4/25 | Loss: 0.00130847
Iteration 5/25 | Loss: 0.00130847
Iteration 6/25 | Loss: 0.00130847
Iteration 7/25 | Loss: 0.00130847
Iteration 8/25 | Loss: 0.00130847
Iteration 9/25 | Loss: 0.00130847
Iteration 10/25 | Loss: 0.00130847
Iteration 11/25 | Loss: 0.00130847
Iteration 12/25 | Loss: 0.00130846
Iteration 13/25 | Loss: 0.00130847
Iteration 14/25 | Loss: 0.00130847
Iteration 15/25 | Loss: 0.00130847
Iteration 16/25 | Loss: 0.00130847
Iteration 17/25 | Loss: 0.00130847
Iteration 18/25 | Loss: 0.00130847
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013084650272503495, 0.0013084650272503495, 0.0013084650272503495, 0.0013084650272503495, 0.0013084650272503495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013084650272503495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130847
Iteration 2/1000 | Loss: 0.00008649
Iteration 3/1000 | Loss: 0.00005562
Iteration 4/1000 | Loss: 0.00004285
Iteration 5/1000 | Loss: 0.00003935
Iteration 6/1000 | Loss: 0.00003760
Iteration 7/1000 | Loss: 0.00003631
Iteration 8/1000 | Loss: 0.00003516
Iteration 9/1000 | Loss: 0.00003443
Iteration 10/1000 | Loss: 0.00003390
Iteration 11/1000 | Loss: 0.00003346
Iteration 12/1000 | Loss: 0.00003300
Iteration 13/1000 | Loss: 0.00003269
Iteration 14/1000 | Loss: 0.00003240
Iteration 15/1000 | Loss: 0.00003213
Iteration 16/1000 | Loss: 0.00003194
Iteration 17/1000 | Loss: 0.00003178
Iteration 18/1000 | Loss: 0.00003166
Iteration 19/1000 | Loss: 0.00003154
Iteration 20/1000 | Loss: 0.00003148
Iteration 21/1000 | Loss: 0.00003144
Iteration 22/1000 | Loss: 0.00003134
Iteration 23/1000 | Loss: 0.00003134
Iteration 24/1000 | Loss: 0.00003132
Iteration 25/1000 | Loss: 0.00003130
Iteration 26/1000 | Loss: 0.00003129
Iteration 27/1000 | Loss: 0.00003129
Iteration 28/1000 | Loss: 0.00003126
Iteration 29/1000 | Loss: 0.00003126
Iteration 30/1000 | Loss: 0.00003125
Iteration 31/1000 | Loss: 0.00003125
Iteration 32/1000 | Loss: 0.00003125
Iteration 33/1000 | Loss: 0.00003122
Iteration 34/1000 | Loss: 0.00003122
Iteration 35/1000 | Loss: 0.00003122
Iteration 36/1000 | Loss: 0.00003122
Iteration 37/1000 | Loss: 0.00003122
Iteration 38/1000 | Loss: 0.00003122
Iteration 39/1000 | Loss: 0.00003122
Iteration 40/1000 | Loss: 0.00003122
Iteration 41/1000 | Loss: 0.00003121
Iteration 42/1000 | Loss: 0.00003120
Iteration 43/1000 | Loss: 0.00003120
Iteration 44/1000 | Loss: 0.00003119
Iteration 45/1000 | Loss: 0.00003119
Iteration 46/1000 | Loss: 0.00003119
Iteration 47/1000 | Loss: 0.00003119
Iteration 48/1000 | Loss: 0.00003119
Iteration 49/1000 | Loss: 0.00003119
Iteration 50/1000 | Loss: 0.00003119
Iteration 51/1000 | Loss: 0.00003119
Iteration 52/1000 | Loss: 0.00003118
Iteration 53/1000 | Loss: 0.00003118
Iteration 54/1000 | Loss: 0.00003117
Iteration 55/1000 | Loss: 0.00003117
Iteration 56/1000 | Loss: 0.00003116
Iteration 57/1000 | Loss: 0.00003116
Iteration 58/1000 | Loss: 0.00003115
Iteration 59/1000 | Loss: 0.00003115
Iteration 60/1000 | Loss: 0.00003115
Iteration 61/1000 | Loss: 0.00003114
Iteration 62/1000 | Loss: 0.00003114
Iteration 63/1000 | Loss: 0.00003114
Iteration 64/1000 | Loss: 0.00003114
Iteration 65/1000 | Loss: 0.00003114
Iteration 66/1000 | Loss: 0.00003114
Iteration 67/1000 | Loss: 0.00003114
Iteration 68/1000 | Loss: 0.00003114
Iteration 69/1000 | Loss: 0.00003114
Iteration 70/1000 | Loss: 0.00003114
Iteration 71/1000 | Loss: 0.00003114
Iteration 72/1000 | Loss: 0.00003113
Iteration 73/1000 | Loss: 0.00003113
Iteration 74/1000 | Loss: 0.00003113
Iteration 75/1000 | Loss: 0.00003113
Iteration 76/1000 | Loss: 0.00003113
Iteration 77/1000 | Loss: 0.00003113
Iteration 78/1000 | Loss: 0.00003113
Iteration 79/1000 | Loss: 0.00003113
Iteration 80/1000 | Loss: 0.00003113
Iteration 81/1000 | Loss: 0.00003112
Iteration 82/1000 | Loss: 0.00003112
Iteration 83/1000 | Loss: 0.00003111
Iteration 84/1000 | Loss: 0.00003111
Iteration 85/1000 | Loss: 0.00003111
Iteration 86/1000 | Loss: 0.00003111
Iteration 87/1000 | Loss: 0.00003111
Iteration 88/1000 | Loss: 0.00003110
Iteration 89/1000 | Loss: 0.00003110
Iteration 90/1000 | Loss: 0.00003110
Iteration 91/1000 | Loss: 0.00003110
Iteration 92/1000 | Loss: 0.00003110
Iteration 93/1000 | Loss: 0.00003109
Iteration 94/1000 | Loss: 0.00003109
Iteration 95/1000 | Loss: 0.00003109
Iteration 96/1000 | Loss: 0.00003109
Iteration 97/1000 | Loss: 0.00003109
Iteration 98/1000 | Loss: 0.00003109
Iteration 99/1000 | Loss: 0.00003109
Iteration 100/1000 | Loss: 0.00003109
Iteration 101/1000 | Loss: 0.00003108
Iteration 102/1000 | Loss: 0.00003108
Iteration 103/1000 | Loss: 0.00003108
Iteration 104/1000 | Loss: 0.00003108
Iteration 105/1000 | Loss: 0.00003108
Iteration 106/1000 | Loss: 0.00003108
Iteration 107/1000 | Loss: 0.00003108
Iteration 108/1000 | Loss: 0.00003108
Iteration 109/1000 | Loss: 0.00003108
Iteration 110/1000 | Loss: 0.00003108
Iteration 111/1000 | Loss: 0.00003108
Iteration 112/1000 | Loss: 0.00003108
Iteration 113/1000 | Loss: 0.00003108
Iteration 114/1000 | Loss: 0.00003107
Iteration 115/1000 | Loss: 0.00003107
Iteration 116/1000 | Loss: 0.00003107
Iteration 117/1000 | Loss: 0.00003107
Iteration 118/1000 | Loss: 0.00003106
Iteration 119/1000 | Loss: 0.00003106
Iteration 120/1000 | Loss: 0.00003106
Iteration 121/1000 | Loss: 0.00003106
Iteration 122/1000 | Loss: 0.00003105
Iteration 123/1000 | Loss: 0.00003105
Iteration 124/1000 | Loss: 0.00003105
Iteration 125/1000 | Loss: 0.00003105
Iteration 126/1000 | Loss: 0.00003105
Iteration 127/1000 | Loss: 0.00003105
Iteration 128/1000 | Loss: 0.00003105
Iteration 129/1000 | Loss: 0.00003105
Iteration 130/1000 | Loss: 0.00003105
Iteration 131/1000 | Loss: 0.00003104
Iteration 132/1000 | Loss: 0.00003104
Iteration 133/1000 | Loss: 0.00003104
Iteration 134/1000 | Loss: 0.00003104
Iteration 135/1000 | Loss: 0.00003103
Iteration 136/1000 | Loss: 0.00003103
Iteration 137/1000 | Loss: 0.00003103
Iteration 138/1000 | Loss: 0.00003103
Iteration 139/1000 | Loss: 0.00003103
Iteration 140/1000 | Loss: 0.00003103
Iteration 141/1000 | Loss: 0.00003103
Iteration 142/1000 | Loss: 0.00003103
Iteration 143/1000 | Loss: 0.00003102
Iteration 144/1000 | Loss: 0.00003102
Iteration 145/1000 | Loss: 0.00003102
Iteration 146/1000 | Loss: 0.00003102
Iteration 147/1000 | Loss: 0.00003102
Iteration 148/1000 | Loss: 0.00003102
Iteration 149/1000 | Loss: 0.00003102
Iteration 150/1000 | Loss: 0.00003102
Iteration 151/1000 | Loss: 0.00003102
Iteration 152/1000 | Loss: 0.00003102
Iteration 153/1000 | Loss: 0.00003101
Iteration 154/1000 | Loss: 0.00003101
Iteration 155/1000 | Loss: 0.00003101
Iteration 156/1000 | Loss: 0.00003101
Iteration 157/1000 | Loss: 0.00003101
Iteration 158/1000 | Loss: 0.00003101
Iteration 159/1000 | Loss: 0.00003101
Iteration 160/1000 | Loss: 0.00003101
Iteration 161/1000 | Loss: 0.00003101
Iteration 162/1000 | Loss: 0.00003101
Iteration 163/1000 | Loss: 0.00003100
Iteration 164/1000 | Loss: 0.00003100
Iteration 165/1000 | Loss: 0.00003100
Iteration 166/1000 | Loss: 0.00003100
Iteration 167/1000 | Loss: 0.00003100
Iteration 168/1000 | Loss: 0.00003100
Iteration 169/1000 | Loss: 0.00003100
Iteration 170/1000 | Loss: 0.00003100
Iteration 171/1000 | Loss: 0.00003100
Iteration 172/1000 | Loss: 0.00003099
Iteration 173/1000 | Loss: 0.00003099
Iteration 174/1000 | Loss: 0.00003099
Iteration 175/1000 | Loss: 0.00003099
Iteration 176/1000 | Loss: 0.00003099
Iteration 177/1000 | Loss: 0.00003099
Iteration 178/1000 | Loss: 0.00003099
Iteration 179/1000 | Loss: 0.00003099
Iteration 180/1000 | Loss: 0.00003099
Iteration 181/1000 | Loss: 0.00003099
Iteration 182/1000 | Loss: 0.00003099
Iteration 183/1000 | Loss: 0.00003099
Iteration 184/1000 | Loss: 0.00003099
Iteration 185/1000 | Loss: 0.00003099
Iteration 186/1000 | Loss: 0.00003099
Iteration 187/1000 | Loss: 0.00003099
Iteration 188/1000 | Loss: 0.00003099
Iteration 189/1000 | Loss: 0.00003099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [3.098810338997282e-05, 3.098810338997282e-05, 3.098810338997282e-05, 3.098810338997282e-05, 3.098810338997282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.098810338997282e-05

Optimization complete. Final v2v error: 4.499915599822998 mm

Highest mean error: 5.662132740020752 mm for frame 151

Lowest mean error: 3.509568691253662 mm for frame 15

Saving results

Total time: 51.497923612594604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00708665
Iteration 2/25 | Loss: 0.00150343
Iteration 3/25 | Loss: 0.00139070
Iteration 4/25 | Loss: 0.00138331
Iteration 5/25 | Loss: 0.00138268
Iteration 6/25 | Loss: 0.00138268
Iteration 7/25 | Loss: 0.00138268
Iteration 8/25 | Loss: 0.00138268
Iteration 9/25 | Loss: 0.00138268
Iteration 10/25 | Loss: 0.00138268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013826785143464804, 0.0013826785143464804, 0.0013826785143464804, 0.0013826785143464804, 0.0013826785143464804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013826785143464804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47302461
Iteration 2/25 | Loss: 0.00097133
Iteration 3/25 | Loss: 0.00097132
Iteration 4/25 | Loss: 0.00097132
Iteration 5/25 | Loss: 0.00097132
Iteration 6/25 | Loss: 0.00097132
Iteration 7/25 | Loss: 0.00097132
Iteration 8/25 | Loss: 0.00097132
Iteration 9/25 | Loss: 0.00097132
Iteration 10/25 | Loss: 0.00097132
Iteration 11/25 | Loss: 0.00097132
Iteration 12/25 | Loss: 0.00097132
Iteration 13/25 | Loss: 0.00097132
Iteration 14/25 | Loss: 0.00097132
Iteration 15/25 | Loss: 0.00097132
Iteration 16/25 | Loss: 0.00097132
Iteration 17/25 | Loss: 0.00097132
Iteration 18/25 | Loss: 0.00097132
Iteration 19/25 | Loss: 0.00097132
Iteration 20/25 | Loss: 0.00097131
Iteration 21/25 | Loss: 0.00097131
Iteration 22/25 | Loss: 0.00097131
Iteration 23/25 | Loss: 0.00097131
Iteration 24/25 | Loss: 0.00097131
Iteration 25/25 | Loss: 0.00097131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097131
Iteration 2/1000 | Loss: 0.00004057
Iteration 3/1000 | Loss: 0.00003006
Iteration 4/1000 | Loss: 0.00002767
Iteration 5/1000 | Loss: 0.00002669
Iteration 6/1000 | Loss: 0.00002611
Iteration 7/1000 | Loss: 0.00002579
Iteration 8/1000 | Loss: 0.00002547
Iteration 9/1000 | Loss: 0.00002526
Iteration 10/1000 | Loss: 0.00002508
Iteration 11/1000 | Loss: 0.00002500
Iteration 12/1000 | Loss: 0.00002479
Iteration 13/1000 | Loss: 0.00002456
Iteration 14/1000 | Loss: 0.00002453
Iteration 15/1000 | Loss: 0.00002440
Iteration 16/1000 | Loss: 0.00002439
Iteration 17/1000 | Loss: 0.00002438
Iteration 18/1000 | Loss: 0.00002438
Iteration 19/1000 | Loss: 0.00002431
Iteration 20/1000 | Loss: 0.00002412
Iteration 21/1000 | Loss: 0.00002410
Iteration 22/1000 | Loss: 0.00002410
Iteration 23/1000 | Loss: 0.00002405
Iteration 24/1000 | Loss: 0.00002390
Iteration 25/1000 | Loss: 0.00002375
Iteration 26/1000 | Loss: 0.00002364
Iteration 27/1000 | Loss: 0.00002362
Iteration 28/1000 | Loss: 0.00002358
Iteration 29/1000 | Loss: 0.00002356
Iteration 30/1000 | Loss: 0.00002355
Iteration 31/1000 | Loss: 0.00002354
Iteration 32/1000 | Loss: 0.00002353
Iteration 33/1000 | Loss: 0.00002353
Iteration 34/1000 | Loss: 0.00002351
Iteration 35/1000 | Loss: 0.00002351
Iteration 36/1000 | Loss: 0.00002350
Iteration 37/1000 | Loss: 0.00002350
Iteration 38/1000 | Loss: 0.00002349
Iteration 39/1000 | Loss: 0.00002348
Iteration 40/1000 | Loss: 0.00002347
Iteration 41/1000 | Loss: 0.00002347
Iteration 42/1000 | Loss: 0.00002347
Iteration 43/1000 | Loss: 0.00002347
Iteration 44/1000 | Loss: 0.00002347
Iteration 45/1000 | Loss: 0.00002347
Iteration 46/1000 | Loss: 0.00002347
Iteration 47/1000 | Loss: 0.00002347
Iteration 48/1000 | Loss: 0.00002346
Iteration 49/1000 | Loss: 0.00002346
Iteration 50/1000 | Loss: 0.00002345
Iteration 51/1000 | Loss: 0.00002345
Iteration 52/1000 | Loss: 0.00002345
Iteration 53/1000 | Loss: 0.00002345
Iteration 54/1000 | Loss: 0.00002345
Iteration 55/1000 | Loss: 0.00002345
Iteration 56/1000 | Loss: 0.00002345
Iteration 57/1000 | Loss: 0.00002345
Iteration 58/1000 | Loss: 0.00002344
Iteration 59/1000 | Loss: 0.00002344
Iteration 60/1000 | Loss: 0.00002344
Iteration 61/1000 | Loss: 0.00002344
Iteration 62/1000 | Loss: 0.00002344
Iteration 63/1000 | Loss: 0.00002344
Iteration 64/1000 | Loss: 0.00002343
Iteration 65/1000 | Loss: 0.00002343
Iteration 66/1000 | Loss: 0.00002343
Iteration 67/1000 | Loss: 0.00002343
Iteration 68/1000 | Loss: 0.00002343
Iteration 69/1000 | Loss: 0.00002343
Iteration 70/1000 | Loss: 0.00002343
Iteration 71/1000 | Loss: 0.00002343
Iteration 72/1000 | Loss: 0.00002343
Iteration 73/1000 | Loss: 0.00002343
Iteration 74/1000 | Loss: 0.00002343
Iteration 75/1000 | Loss: 0.00002342
Iteration 76/1000 | Loss: 0.00002342
Iteration 77/1000 | Loss: 0.00002342
Iteration 78/1000 | Loss: 0.00002342
Iteration 79/1000 | Loss: 0.00002342
Iteration 80/1000 | Loss: 0.00002341
Iteration 81/1000 | Loss: 0.00002341
Iteration 82/1000 | Loss: 0.00002341
Iteration 83/1000 | Loss: 0.00002341
Iteration 84/1000 | Loss: 0.00002341
Iteration 85/1000 | Loss: 0.00002341
Iteration 86/1000 | Loss: 0.00002341
Iteration 87/1000 | Loss: 0.00002341
Iteration 88/1000 | Loss: 0.00002341
Iteration 89/1000 | Loss: 0.00002341
Iteration 90/1000 | Loss: 0.00002341
Iteration 91/1000 | Loss: 0.00002341
Iteration 92/1000 | Loss: 0.00002341
Iteration 93/1000 | Loss: 0.00002341
Iteration 94/1000 | Loss: 0.00002341
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [2.3405007596011274e-05, 2.3405007596011274e-05, 2.3405007596011274e-05, 2.3405007596011274e-05, 2.3405007596011274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3405007596011274e-05

Optimization complete. Final v2v error: 3.8002867698669434 mm

Highest mean error: 4.242714881896973 mm for frame 98

Lowest mean error: 3.3637988567352295 mm for frame 158

Saving results

Total time: 43.4778938293457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00354874
Iteration 2/25 | Loss: 0.00126264
Iteration 3/25 | Loss: 0.00120052
Iteration 4/25 | Loss: 0.00119112
Iteration 5/25 | Loss: 0.00118720
Iteration 6/25 | Loss: 0.00118634
Iteration 7/25 | Loss: 0.00118634
Iteration 8/25 | Loss: 0.00118634
Iteration 9/25 | Loss: 0.00118634
Iteration 10/25 | Loss: 0.00118634
Iteration 11/25 | Loss: 0.00118634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011863408144563437, 0.0011863408144563437, 0.0011863408144563437, 0.0011863408144563437, 0.0011863408144563437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011863408144563437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34611213
Iteration 2/25 | Loss: 0.00120066
Iteration 3/25 | Loss: 0.00120066
Iteration 4/25 | Loss: 0.00120066
Iteration 5/25 | Loss: 0.00120066
Iteration 6/25 | Loss: 0.00120066
Iteration 7/25 | Loss: 0.00120066
Iteration 8/25 | Loss: 0.00120066
Iteration 9/25 | Loss: 0.00120065
Iteration 10/25 | Loss: 0.00120065
Iteration 11/25 | Loss: 0.00120065
Iteration 12/25 | Loss: 0.00120065
Iteration 13/25 | Loss: 0.00120065
Iteration 14/25 | Loss: 0.00120065
Iteration 15/25 | Loss: 0.00120065
Iteration 16/25 | Loss: 0.00120065
Iteration 17/25 | Loss: 0.00120065
Iteration 18/25 | Loss: 0.00120065
Iteration 19/25 | Loss: 0.00120065
Iteration 20/25 | Loss: 0.00120065
Iteration 21/25 | Loss: 0.00120065
Iteration 22/25 | Loss: 0.00120065
Iteration 23/25 | Loss: 0.00120065
Iteration 24/25 | Loss: 0.00120065
Iteration 25/25 | Loss: 0.00120065

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120065
Iteration 2/1000 | Loss: 0.00002135
Iteration 3/1000 | Loss: 0.00001290
Iteration 4/1000 | Loss: 0.00001130
Iteration 5/1000 | Loss: 0.00001067
Iteration 6/1000 | Loss: 0.00000997
Iteration 7/1000 | Loss: 0.00000965
Iteration 8/1000 | Loss: 0.00000959
Iteration 9/1000 | Loss: 0.00000952
Iteration 10/1000 | Loss: 0.00000934
Iteration 11/1000 | Loss: 0.00000930
Iteration 12/1000 | Loss: 0.00000929
Iteration 13/1000 | Loss: 0.00000928
Iteration 14/1000 | Loss: 0.00000928
Iteration 15/1000 | Loss: 0.00000928
Iteration 16/1000 | Loss: 0.00000928
Iteration 17/1000 | Loss: 0.00000925
Iteration 18/1000 | Loss: 0.00000923
Iteration 19/1000 | Loss: 0.00000923
Iteration 20/1000 | Loss: 0.00000922
Iteration 21/1000 | Loss: 0.00000922
Iteration 22/1000 | Loss: 0.00000922
Iteration 23/1000 | Loss: 0.00000922
Iteration 24/1000 | Loss: 0.00000922
Iteration 25/1000 | Loss: 0.00000922
Iteration 26/1000 | Loss: 0.00000921
Iteration 27/1000 | Loss: 0.00000920
Iteration 28/1000 | Loss: 0.00000920
Iteration 29/1000 | Loss: 0.00000919
Iteration 30/1000 | Loss: 0.00000919
Iteration 31/1000 | Loss: 0.00000918
Iteration 32/1000 | Loss: 0.00000917
Iteration 33/1000 | Loss: 0.00000916
Iteration 34/1000 | Loss: 0.00000916
Iteration 35/1000 | Loss: 0.00000916
Iteration 36/1000 | Loss: 0.00000916
Iteration 37/1000 | Loss: 0.00000915
Iteration 38/1000 | Loss: 0.00000915
Iteration 39/1000 | Loss: 0.00000915
Iteration 40/1000 | Loss: 0.00000915
Iteration 41/1000 | Loss: 0.00000915
Iteration 42/1000 | Loss: 0.00000914
Iteration 43/1000 | Loss: 0.00000913
Iteration 44/1000 | Loss: 0.00000911
Iteration 45/1000 | Loss: 0.00000910
Iteration 46/1000 | Loss: 0.00000909
Iteration 47/1000 | Loss: 0.00000908
Iteration 48/1000 | Loss: 0.00000908
Iteration 49/1000 | Loss: 0.00000908
Iteration 50/1000 | Loss: 0.00000908
Iteration 51/1000 | Loss: 0.00000907
Iteration 52/1000 | Loss: 0.00000907
Iteration 53/1000 | Loss: 0.00000907
Iteration 54/1000 | Loss: 0.00000907
Iteration 55/1000 | Loss: 0.00000906
Iteration 56/1000 | Loss: 0.00000905
Iteration 57/1000 | Loss: 0.00000905
Iteration 58/1000 | Loss: 0.00000905
Iteration 59/1000 | Loss: 0.00000904
Iteration 60/1000 | Loss: 0.00000904
Iteration 61/1000 | Loss: 0.00000904
Iteration 62/1000 | Loss: 0.00000903
Iteration 63/1000 | Loss: 0.00000903
Iteration 64/1000 | Loss: 0.00000903
Iteration 65/1000 | Loss: 0.00000903
Iteration 66/1000 | Loss: 0.00000903
Iteration 67/1000 | Loss: 0.00000902
Iteration 68/1000 | Loss: 0.00000902
Iteration 69/1000 | Loss: 0.00000902
Iteration 70/1000 | Loss: 0.00000902
Iteration 71/1000 | Loss: 0.00000902
Iteration 72/1000 | Loss: 0.00000902
Iteration 73/1000 | Loss: 0.00000901
Iteration 74/1000 | Loss: 0.00000901
Iteration 75/1000 | Loss: 0.00000901
Iteration 76/1000 | Loss: 0.00000901
Iteration 77/1000 | Loss: 0.00000900
Iteration 78/1000 | Loss: 0.00000900
Iteration 79/1000 | Loss: 0.00000899
Iteration 80/1000 | Loss: 0.00000899
Iteration 81/1000 | Loss: 0.00000899
Iteration 82/1000 | Loss: 0.00000898
Iteration 83/1000 | Loss: 0.00000898
Iteration 84/1000 | Loss: 0.00000898
Iteration 85/1000 | Loss: 0.00000897
Iteration 86/1000 | Loss: 0.00000897
Iteration 87/1000 | Loss: 0.00000897
Iteration 88/1000 | Loss: 0.00000897
Iteration 89/1000 | Loss: 0.00000897
Iteration 90/1000 | Loss: 0.00000897
Iteration 91/1000 | Loss: 0.00000896
Iteration 92/1000 | Loss: 0.00000896
Iteration 93/1000 | Loss: 0.00000895
Iteration 94/1000 | Loss: 0.00000895
Iteration 95/1000 | Loss: 0.00000895
Iteration 96/1000 | Loss: 0.00000895
Iteration 97/1000 | Loss: 0.00000895
Iteration 98/1000 | Loss: 0.00000894
Iteration 99/1000 | Loss: 0.00000894
Iteration 100/1000 | Loss: 0.00000894
Iteration 101/1000 | Loss: 0.00000894
Iteration 102/1000 | Loss: 0.00000894
Iteration 103/1000 | Loss: 0.00000893
Iteration 104/1000 | Loss: 0.00000893
Iteration 105/1000 | Loss: 0.00000893
Iteration 106/1000 | Loss: 0.00000893
Iteration 107/1000 | Loss: 0.00000892
Iteration 108/1000 | Loss: 0.00000892
Iteration 109/1000 | Loss: 0.00000891
Iteration 110/1000 | Loss: 0.00000891
Iteration 111/1000 | Loss: 0.00000891
Iteration 112/1000 | Loss: 0.00000890
Iteration 113/1000 | Loss: 0.00000890
Iteration 114/1000 | Loss: 0.00000890
Iteration 115/1000 | Loss: 0.00000889
Iteration 116/1000 | Loss: 0.00000889
Iteration 117/1000 | Loss: 0.00000889
Iteration 118/1000 | Loss: 0.00000889
Iteration 119/1000 | Loss: 0.00000889
Iteration 120/1000 | Loss: 0.00000888
Iteration 121/1000 | Loss: 0.00000888
Iteration 122/1000 | Loss: 0.00000888
Iteration 123/1000 | Loss: 0.00000888
Iteration 124/1000 | Loss: 0.00000888
Iteration 125/1000 | Loss: 0.00000888
Iteration 126/1000 | Loss: 0.00000887
Iteration 127/1000 | Loss: 0.00000887
Iteration 128/1000 | Loss: 0.00000887
Iteration 129/1000 | Loss: 0.00000887
Iteration 130/1000 | Loss: 0.00000886
Iteration 131/1000 | Loss: 0.00000886
Iteration 132/1000 | Loss: 0.00000886
Iteration 133/1000 | Loss: 0.00000886
Iteration 134/1000 | Loss: 0.00000886
Iteration 135/1000 | Loss: 0.00000886
Iteration 136/1000 | Loss: 0.00000886
Iteration 137/1000 | Loss: 0.00000886
Iteration 138/1000 | Loss: 0.00000886
Iteration 139/1000 | Loss: 0.00000886
Iteration 140/1000 | Loss: 0.00000885
Iteration 141/1000 | Loss: 0.00000884
Iteration 142/1000 | Loss: 0.00000884
Iteration 143/1000 | Loss: 0.00000884
Iteration 144/1000 | Loss: 0.00000884
Iteration 145/1000 | Loss: 0.00000884
Iteration 146/1000 | Loss: 0.00000884
Iteration 147/1000 | Loss: 0.00000884
Iteration 148/1000 | Loss: 0.00000884
Iteration 149/1000 | Loss: 0.00000883
Iteration 150/1000 | Loss: 0.00000882
Iteration 151/1000 | Loss: 0.00000882
Iteration 152/1000 | Loss: 0.00000881
Iteration 153/1000 | Loss: 0.00000881
Iteration 154/1000 | Loss: 0.00000881
Iteration 155/1000 | Loss: 0.00000881
Iteration 156/1000 | Loss: 0.00000881
Iteration 157/1000 | Loss: 0.00000881
Iteration 158/1000 | Loss: 0.00000881
Iteration 159/1000 | Loss: 0.00000881
Iteration 160/1000 | Loss: 0.00000881
Iteration 161/1000 | Loss: 0.00000881
Iteration 162/1000 | Loss: 0.00000881
Iteration 163/1000 | Loss: 0.00000880
Iteration 164/1000 | Loss: 0.00000880
Iteration 165/1000 | Loss: 0.00000880
Iteration 166/1000 | Loss: 0.00000880
Iteration 167/1000 | Loss: 0.00000880
Iteration 168/1000 | Loss: 0.00000880
Iteration 169/1000 | Loss: 0.00000879
Iteration 170/1000 | Loss: 0.00000879
Iteration 171/1000 | Loss: 0.00000879
Iteration 172/1000 | Loss: 0.00000879
Iteration 173/1000 | Loss: 0.00000879
Iteration 174/1000 | Loss: 0.00000879
Iteration 175/1000 | Loss: 0.00000879
Iteration 176/1000 | Loss: 0.00000879
Iteration 177/1000 | Loss: 0.00000879
Iteration 178/1000 | Loss: 0.00000879
Iteration 179/1000 | Loss: 0.00000879
Iteration 180/1000 | Loss: 0.00000879
Iteration 181/1000 | Loss: 0.00000878
Iteration 182/1000 | Loss: 0.00000878
Iteration 183/1000 | Loss: 0.00000878
Iteration 184/1000 | Loss: 0.00000878
Iteration 185/1000 | Loss: 0.00000878
Iteration 186/1000 | Loss: 0.00000878
Iteration 187/1000 | Loss: 0.00000878
Iteration 188/1000 | Loss: 0.00000877
Iteration 189/1000 | Loss: 0.00000877
Iteration 190/1000 | Loss: 0.00000877
Iteration 191/1000 | Loss: 0.00000877
Iteration 192/1000 | Loss: 0.00000877
Iteration 193/1000 | Loss: 0.00000876
Iteration 194/1000 | Loss: 0.00000876
Iteration 195/1000 | Loss: 0.00000876
Iteration 196/1000 | Loss: 0.00000876
Iteration 197/1000 | Loss: 0.00000876
Iteration 198/1000 | Loss: 0.00000876
Iteration 199/1000 | Loss: 0.00000876
Iteration 200/1000 | Loss: 0.00000876
Iteration 201/1000 | Loss: 0.00000876
Iteration 202/1000 | Loss: 0.00000876
Iteration 203/1000 | Loss: 0.00000876
Iteration 204/1000 | Loss: 0.00000876
Iteration 205/1000 | Loss: 0.00000876
Iteration 206/1000 | Loss: 0.00000876
Iteration 207/1000 | Loss: 0.00000876
Iteration 208/1000 | Loss: 0.00000876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [8.756945135246497e-06, 8.756945135246497e-06, 8.756945135246497e-06, 8.756945135246497e-06, 8.756945135246497e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.756945135246497e-06

Optimization complete. Final v2v error: 2.5796844959259033 mm

Highest mean error: 2.889918088912964 mm for frame 134

Lowest mean error: 2.3451385498046875 mm for frame 9

Saving results

Total time: 37.48780345916748
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472241
Iteration 2/25 | Loss: 0.00146561
Iteration 3/25 | Loss: 0.00130501
Iteration 4/25 | Loss: 0.00129029
Iteration 5/25 | Loss: 0.00128737
Iteration 6/25 | Loss: 0.00128658
Iteration 7/25 | Loss: 0.00128658
Iteration 8/25 | Loss: 0.00128658
Iteration 9/25 | Loss: 0.00128658
Iteration 10/25 | Loss: 0.00128658
Iteration 11/25 | Loss: 0.00128658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012865838361904025, 0.0012865838361904025, 0.0012865838361904025, 0.0012865838361904025, 0.0012865838361904025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012865838361904025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40592933
Iteration 2/25 | Loss: 0.00097249
Iteration 3/25 | Loss: 0.00097248
Iteration 4/25 | Loss: 0.00097248
Iteration 5/25 | Loss: 0.00097247
Iteration 6/25 | Loss: 0.00097247
Iteration 7/25 | Loss: 0.00097247
Iteration 8/25 | Loss: 0.00097247
Iteration 9/25 | Loss: 0.00097247
Iteration 10/25 | Loss: 0.00097247
Iteration 11/25 | Loss: 0.00097247
Iteration 12/25 | Loss: 0.00097247
Iteration 13/25 | Loss: 0.00097247
Iteration 14/25 | Loss: 0.00097247
Iteration 15/25 | Loss: 0.00097247
Iteration 16/25 | Loss: 0.00097247
Iteration 17/25 | Loss: 0.00097247
Iteration 18/25 | Loss: 0.00097247
Iteration 19/25 | Loss: 0.00097247
Iteration 20/25 | Loss: 0.00097247
Iteration 21/25 | Loss: 0.00097247
Iteration 22/25 | Loss: 0.00097247
Iteration 23/25 | Loss: 0.00097247
Iteration 24/25 | Loss: 0.00097247
Iteration 25/25 | Loss: 0.00097247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097247
Iteration 2/1000 | Loss: 0.00003558
Iteration 3/1000 | Loss: 0.00002467
Iteration 4/1000 | Loss: 0.00002026
Iteration 5/1000 | Loss: 0.00001907
Iteration 6/1000 | Loss: 0.00001829
Iteration 7/1000 | Loss: 0.00001778
Iteration 8/1000 | Loss: 0.00001738
Iteration 9/1000 | Loss: 0.00001710
Iteration 10/1000 | Loss: 0.00001686
Iteration 11/1000 | Loss: 0.00001664
Iteration 12/1000 | Loss: 0.00001655
Iteration 13/1000 | Loss: 0.00001650
Iteration 14/1000 | Loss: 0.00001649
Iteration 15/1000 | Loss: 0.00001648
Iteration 16/1000 | Loss: 0.00001641
Iteration 17/1000 | Loss: 0.00001625
Iteration 18/1000 | Loss: 0.00001616
Iteration 19/1000 | Loss: 0.00001615
Iteration 20/1000 | Loss: 0.00001614
Iteration 21/1000 | Loss: 0.00001613
Iteration 22/1000 | Loss: 0.00001611
Iteration 23/1000 | Loss: 0.00001609
Iteration 24/1000 | Loss: 0.00001604
Iteration 25/1000 | Loss: 0.00001597
Iteration 26/1000 | Loss: 0.00001596
Iteration 27/1000 | Loss: 0.00001595
Iteration 28/1000 | Loss: 0.00001594
Iteration 29/1000 | Loss: 0.00001590
Iteration 30/1000 | Loss: 0.00001588
Iteration 31/1000 | Loss: 0.00001588
Iteration 32/1000 | Loss: 0.00001587
Iteration 33/1000 | Loss: 0.00001587
Iteration 34/1000 | Loss: 0.00001586
Iteration 35/1000 | Loss: 0.00001584
Iteration 36/1000 | Loss: 0.00001584
Iteration 37/1000 | Loss: 0.00001582
Iteration 38/1000 | Loss: 0.00001582
Iteration 39/1000 | Loss: 0.00001582
Iteration 40/1000 | Loss: 0.00001580
Iteration 41/1000 | Loss: 0.00001579
Iteration 42/1000 | Loss: 0.00001577
Iteration 43/1000 | Loss: 0.00001576
Iteration 44/1000 | Loss: 0.00001576
Iteration 45/1000 | Loss: 0.00001572
Iteration 46/1000 | Loss: 0.00001572
Iteration 47/1000 | Loss: 0.00001565
Iteration 48/1000 | Loss: 0.00001565
Iteration 49/1000 | Loss: 0.00001562
Iteration 50/1000 | Loss: 0.00001562
Iteration 51/1000 | Loss: 0.00001561
Iteration 52/1000 | Loss: 0.00001561
Iteration 53/1000 | Loss: 0.00001560
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001560
Iteration 56/1000 | Loss: 0.00001560
Iteration 57/1000 | Loss: 0.00001560
Iteration 58/1000 | Loss: 0.00001560
Iteration 59/1000 | Loss: 0.00001560
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001558
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001558
Iteration 66/1000 | Loss: 0.00001558
Iteration 67/1000 | Loss: 0.00001558
Iteration 68/1000 | Loss: 0.00001558
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001558
Iteration 71/1000 | Loss: 0.00001558
Iteration 72/1000 | Loss: 0.00001557
Iteration 73/1000 | Loss: 0.00001557
Iteration 74/1000 | Loss: 0.00001557
Iteration 75/1000 | Loss: 0.00001557
Iteration 76/1000 | Loss: 0.00001557
Iteration 77/1000 | Loss: 0.00001557
Iteration 78/1000 | Loss: 0.00001556
Iteration 79/1000 | Loss: 0.00001556
Iteration 80/1000 | Loss: 0.00001556
Iteration 81/1000 | Loss: 0.00001556
Iteration 82/1000 | Loss: 0.00001556
Iteration 83/1000 | Loss: 0.00001556
Iteration 84/1000 | Loss: 0.00001556
Iteration 85/1000 | Loss: 0.00001555
Iteration 86/1000 | Loss: 0.00001555
Iteration 87/1000 | Loss: 0.00001555
Iteration 88/1000 | Loss: 0.00001555
Iteration 89/1000 | Loss: 0.00001555
Iteration 90/1000 | Loss: 0.00001555
Iteration 91/1000 | Loss: 0.00001555
Iteration 92/1000 | Loss: 0.00001555
Iteration 93/1000 | Loss: 0.00001555
Iteration 94/1000 | Loss: 0.00001555
Iteration 95/1000 | Loss: 0.00001554
Iteration 96/1000 | Loss: 0.00001554
Iteration 97/1000 | Loss: 0.00001554
Iteration 98/1000 | Loss: 0.00001554
Iteration 99/1000 | Loss: 0.00001554
Iteration 100/1000 | Loss: 0.00001554
Iteration 101/1000 | Loss: 0.00001554
Iteration 102/1000 | Loss: 0.00001554
Iteration 103/1000 | Loss: 0.00001554
Iteration 104/1000 | Loss: 0.00001553
Iteration 105/1000 | Loss: 0.00001553
Iteration 106/1000 | Loss: 0.00001553
Iteration 107/1000 | Loss: 0.00001553
Iteration 108/1000 | Loss: 0.00001553
Iteration 109/1000 | Loss: 0.00001553
Iteration 110/1000 | Loss: 0.00001553
Iteration 111/1000 | Loss: 0.00001552
Iteration 112/1000 | Loss: 0.00001552
Iteration 113/1000 | Loss: 0.00001552
Iteration 114/1000 | Loss: 0.00001552
Iteration 115/1000 | Loss: 0.00001552
Iteration 116/1000 | Loss: 0.00001551
Iteration 117/1000 | Loss: 0.00001551
Iteration 118/1000 | Loss: 0.00001551
Iteration 119/1000 | Loss: 0.00001551
Iteration 120/1000 | Loss: 0.00001551
Iteration 121/1000 | Loss: 0.00001550
Iteration 122/1000 | Loss: 0.00001550
Iteration 123/1000 | Loss: 0.00001550
Iteration 124/1000 | Loss: 0.00001550
Iteration 125/1000 | Loss: 0.00001550
Iteration 126/1000 | Loss: 0.00001550
Iteration 127/1000 | Loss: 0.00001550
Iteration 128/1000 | Loss: 0.00001549
Iteration 129/1000 | Loss: 0.00001549
Iteration 130/1000 | Loss: 0.00001549
Iteration 131/1000 | Loss: 0.00001549
Iteration 132/1000 | Loss: 0.00001549
Iteration 133/1000 | Loss: 0.00001549
Iteration 134/1000 | Loss: 0.00001549
Iteration 135/1000 | Loss: 0.00001549
Iteration 136/1000 | Loss: 0.00001549
Iteration 137/1000 | Loss: 0.00001549
Iteration 138/1000 | Loss: 0.00001549
Iteration 139/1000 | Loss: 0.00001549
Iteration 140/1000 | Loss: 0.00001548
Iteration 141/1000 | Loss: 0.00001548
Iteration 142/1000 | Loss: 0.00001548
Iteration 143/1000 | Loss: 0.00001548
Iteration 144/1000 | Loss: 0.00001548
Iteration 145/1000 | Loss: 0.00001548
Iteration 146/1000 | Loss: 0.00001548
Iteration 147/1000 | Loss: 0.00001548
Iteration 148/1000 | Loss: 0.00001547
Iteration 149/1000 | Loss: 0.00001547
Iteration 150/1000 | Loss: 0.00001547
Iteration 151/1000 | Loss: 0.00001547
Iteration 152/1000 | Loss: 0.00001546
Iteration 153/1000 | Loss: 0.00001546
Iteration 154/1000 | Loss: 0.00001546
Iteration 155/1000 | Loss: 0.00001546
Iteration 156/1000 | Loss: 0.00001546
Iteration 157/1000 | Loss: 0.00001546
Iteration 158/1000 | Loss: 0.00001546
Iteration 159/1000 | Loss: 0.00001546
Iteration 160/1000 | Loss: 0.00001546
Iteration 161/1000 | Loss: 0.00001546
Iteration 162/1000 | Loss: 0.00001546
Iteration 163/1000 | Loss: 0.00001546
Iteration 164/1000 | Loss: 0.00001546
Iteration 165/1000 | Loss: 0.00001545
Iteration 166/1000 | Loss: 0.00001545
Iteration 167/1000 | Loss: 0.00001545
Iteration 168/1000 | Loss: 0.00001545
Iteration 169/1000 | Loss: 0.00001545
Iteration 170/1000 | Loss: 0.00001545
Iteration 171/1000 | Loss: 0.00001545
Iteration 172/1000 | Loss: 0.00001545
Iteration 173/1000 | Loss: 0.00001545
Iteration 174/1000 | Loss: 0.00001545
Iteration 175/1000 | Loss: 0.00001545
Iteration 176/1000 | Loss: 0.00001545
Iteration 177/1000 | Loss: 0.00001545
Iteration 178/1000 | Loss: 0.00001545
Iteration 179/1000 | Loss: 0.00001545
Iteration 180/1000 | Loss: 0.00001545
Iteration 181/1000 | Loss: 0.00001545
Iteration 182/1000 | Loss: 0.00001545
Iteration 183/1000 | Loss: 0.00001545
Iteration 184/1000 | Loss: 0.00001545
Iteration 185/1000 | Loss: 0.00001545
Iteration 186/1000 | Loss: 0.00001545
Iteration 187/1000 | Loss: 0.00001545
Iteration 188/1000 | Loss: 0.00001545
Iteration 189/1000 | Loss: 0.00001545
Iteration 190/1000 | Loss: 0.00001545
Iteration 191/1000 | Loss: 0.00001545
Iteration 192/1000 | Loss: 0.00001545
Iteration 193/1000 | Loss: 0.00001545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.5447014448000118e-05, 1.5447014448000118e-05, 1.5447014448000118e-05, 1.5447014448000118e-05, 1.5447014448000118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5447014448000118e-05

Optimization complete. Final v2v error: 3.25807785987854 mm

Highest mean error: 3.8753108978271484 mm for frame 70

Lowest mean error: 2.6775643825531006 mm for frame 147

Saving results

Total time: 45.53334832191467
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807372
Iteration 2/25 | Loss: 0.00181660
Iteration 3/25 | Loss: 0.00147998
Iteration 4/25 | Loss: 0.00142297
Iteration 5/25 | Loss: 0.00138611
Iteration 6/25 | Loss: 0.00137680
Iteration 7/25 | Loss: 0.00137748
Iteration 8/25 | Loss: 0.00141561
Iteration 9/25 | Loss: 0.00138141
Iteration 10/25 | Loss: 0.00137392
Iteration 11/25 | Loss: 0.00137164
Iteration 12/25 | Loss: 0.00136926
Iteration 13/25 | Loss: 0.00136608
Iteration 14/25 | Loss: 0.00136531
Iteration 15/25 | Loss: 0.00137108
Iteration 16/25 | Loss: 0.00137146
Iteration 17/25 | Loss: 0.00136918
Iteration 18/25 | Loss: 0.00137060
Iteration 19/25 | Loss: 0.00136994
Iteration 20/25 | Loss: 0.00137079
Iteration 21/25 | Loss: 0.00136659
Iteration 22/25 | Loss: 0.00136937
Iteration 23/25 | Loss: 0.00136965
Iteration 24/25 | Loss: 0.00135683
Iteration 25/25 | Loss: 0.00135843

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.45676327
Iteration 2/25 | Loss: 0.00184806
Iteration 3/25 | Loss: 0.00184779
Iteration 4/25 | Loss: 0.00184779
Iteration 5/25 | Loss: 0.00184779
Iteration 6/25 | Loss: 0.00184779
Iteration 7/25 | Loss: 0.00184779
Iteration 8/25 | Loss: 0.00184779
Iteration 9/25 | Loss: 0.00184779
Iteration 10/25 | Loss: 0.00184779
Iteration 11/25 | Loss: 0.00184779
Iteration 12/25 | Loss: 0.00184779
Iteration 13/25 | Loss: 0.00184779
Iteration 14/25 | Loss: 0.00184779
Iteration 15/25 | Loss: 0.00184779
Iteration 16/25 | Loss: 0.00184779
Iteration 17/25 | Loss: 0.00184779
Iteration 18/25 | Loss: 0.00184779
Iteration 19/25 | Loss: 0.00184779
Iteration 20/25 | Loss: 0.00184779
Iteration 21/25 | Loss: 0.00184779
Iteration 22/25 | Loss: 0.00184779
Iteration 23/25 | Loss: 0.00184779
Iteration 24/25 | Loss: 0.00184779
Iteration 25/25 | Loss: 0.00184779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184779
Iteration 2/1000 | Loss: 0.00029514
Iteration 3/1000 | Loss: 0.00035914
Iteration 4/1000 | Loss: 0.00010721
Iteration 5/1000 | Loss: 0.00015277
Iteration 6/1000 | Loss: 0.00018390
Iteration 7/1000 | Loss: 0.00016799
Iteration 8/1000 | Loss: 0.00013684
Iteration 9/1000 | Loss: 0.00018611
Iteration 10/1000 | Loss: 0.00022916
Iteration 11/1000 | Loss: 0.00020226
Iteration 12/1000 | Loss: 0.00022973
Iteration 13/1000 | Loss: 0.00013798
Iteration 14/1000 | Loss: 0.00015575
Iteration 15/1000 | Loss: 0.00028766
Iteration 16/1000 | Loss: 0.00023579
Iteration 17/1000 | Loss: 0.00023167
Iteration 18/1000 | Loss: 0.00024393
Iteration 19/1000 | Loss: 0.00027086
Iteration 20/1000 | Loss: 0.00024699
Iteration 21/1000 | Loss: 0.00027161
Iteration 22/1000 | Loss: 0.00028111
Iteration 23/1000 | Loss: 0.00025834
Iteration 24/1000 | Loss: 0.00018016
Iteration 25/1000 | Loss: 0.00023145
Iteration 26/1000 | Loss: 0.00017548
Iteration 27/1000 | Loss: 0.00037458
Iteration 28/1000 | Loss: 0.00023801
Iteration 29/1000 | Loss: 0.00113198
Iteration 30/1000 | Loss: 0.00094697
Iteration 31/1000 | Loss: 0.00128277
Iteration 32/1000 | Loss: 0.00107570
Iteration 33/1000 | Loss: 0.00008493
Iteration 34/1000 | Loss: 0.00007422
Iteration 35/1000 | Loss: 0.00129741
Iteration 36/1000 | Loss: 0.00008087
Iteration 37/1000 | Loss: 0.00007307
Iteration 38/1000 | Loss: 0.00007084
Iteration 39/1000 | Loss: 0.00006880
Iteration 40/1000 | Loss: 0.00006679
Iteration 41/1000 | Loss: 0.00006510
Iteration 42/1000 | Loss: 0.00006381
Iteration 43/1000 | Loss: 0.00006274
Iteration 44/1000 | Loss: 0.00006201
Iteration 45/1000 | Loss: 0.00006139
Iteration 46/1000 | Loss: 0.00006091
Iteration 47/1000 | Loss: 0.00058390
Iteration 48/1000 | Loss: 0.00129489
Iteration 49/1000 | Loss: 0.00056946
Iteration 50/1000 | Loss: 0.00092330
Iteration 51/1000 | Loss: 0.00074581
Iteration 52/1000 | Loss: 0.00008669
Iteration 53/1000 | Loss: 0.00097712
Iteration 54/1000 | Loss: 0.00057449
Iteration 55/1000 | Loss: 0.00097712
Iteration 56/1000 | Loss: 0.00039990
Iteration 57/1000 | Loss: 0.00007289
Iteration 58/1000 | Loss: 0.00006180
Iteration 59/1000 | Loss: 0.00005756
Iteration 60/1000 | Loss: 0.00013093
Iteration 61/1000 | Loss: 0.00005307
Iteration 62/1000 | Loss: 0.00004948
Iteration 63/1000 | Loss: 0.00004786
Iteration 64/1000 | Loss: 0.00004665
Iteration 65/1000 | Loss: 0.00004570
Iteration 66/1000 | Loss: 0.00004517
Iteration 67/1000 | Loss: 0.00004457
Iteration 68/1000 | Loss: 0.00004416
Iteration 69/1000 | Loss: 0.00004396
Iteration 70/1000 | Loss: 0.00004376
Iteration 71/1000 | Loss: 0.00004354
Iteration 72/1000 | Loss: 0.00004345
Iteration 73/1000 | Loss: 0.00004325
Iteration 74/1000 | Loss: 0.00004310
Iteration 75/1000 | Loss: 0.00004309
Iteration 76/1000 | Loss: 0.00004309
Iteration 77/1000 | Loss: 0.00004308
Iteration 78/1000 | Loss: 0.00004307
Iteration 79/1000 | Loss: 0.00004306
Iteration 80/1000 | Loss: 0.00004306
Iteration 81/1000 | Loss: 0.00004306
Iteration 82/1000 | Loss: 0.00004305
Iteration 83/1000 | Loss: 0.00004305
Iteration 84/1000 | Loss: 0.00004304
Iteration 85/1000 | Loss: 0.00004304
Iteration 86/1000 | Loss: 0.00004303
Iteration 87/1000 | Loss: 0.00004303
Iteration 88/1000 | Loss: 0.00004302
Iteration 89/1000 | Loss: 0.00004302
Iteration 90/1000 | Loss: 0.00004302
Iteration 91/1000 | Loss: 0.00004301
Iteration 92/1000 | Loss: 0.00004301
Iteration 93/1000 | Loss: 0.00004301
Iteration 94/1000 | Loss: 0.00004300
Iteration 95/1000 | Loss: 0.00004298
Iteration 96/1000 | Loss: 0.00004298
Iteration 97/1000 | Loss: 0.00004297
Iteration 98/1000 | Loss: 0.00004296
Iteration 99/1000 | Loss: 0.00004295
Iteration 100/1000 | Loss: 0.00004293
Iteration 101/1000 | Loss: 0.00004293
Iteration 102/1000 | Loss: 0.00004292
Iteration 103/1000 | Loss: 0.00004292
Iteration 104/1000 | Loss: 0.00004291
Iteration 105/1000 | Loss: 0.00004290
Iteration 106/1000 | Loss: 0.00004290
Iteration 107/1000 | Loss: 0.00004289
Iteration 108/1000 | Loss: 0.00004289
Iteration 109/1000 | Loss: 0.00004289
Iteration 110/1000 | Loss: 0.00004288
Iteration 111/1000 | Loss: 0.00004288
Iteration 112/1000 | Loss: 0.00004287
Iteration 113/1000 | Loss: 0.00004286
Iteration 114/1000 | Loss: 0.00004285
Iteration 115/1000 | Loss: 0.00004285
Iteration 116/1000 | Loss: 0.00004284
Iteration 117/1000 | Loss: 0.00004284
Iteration 118/1000 | Loss: 0.00004283
Iteration 119/1000 | Loss: 0.00004283
Iteration 120/1000 | Loss: 0.00004281
Iteration 121/1000 | Loss: 0.00004280
Iteration 122/1000 | Loss: 0.00004279
Iteration 123/1000 | Loss: 0.00004279
Iteration 124/1000 | Loss: 0.00004278
Iteration 125/1000 | Loss: 0.00004278
Iteration 126/1000 | Loss: 0.00004277
Iteration 127/1000 | Loss: 0.00004277
Iteration 128/1000 | Loss: 0.00004277
Iteration 129/1000 | Loss: 0.00004276
Iteration 130/1000 | Loss: 0.00004276
Iteration 131/1000 | Loss: 0.00004276
Iteration 132/1000 | Loss: 0.00004276
Iteration 133/1000 | Loss: 0.00004276
Iteration 134/1000 | Loss: 0.00004276
Iteration 135/1000 | Loss: 0.00004276
Iteration 136/1000 | Loss: 0.00004276
Iteration 137/1000 | Loss: 0.00004275
Iteration 138/1000 | Loss: 0.00004275
Iteration 139/1000 | Loss: 0.00004275
Iteration 140/1000 | Loss: 0.00004275
Iteration 141/1000 | Loss: 0.00004275
Iteration 142/1000 | Loss: 0.00004275
Iteration 143/1000 | Loss: 0.00004274
Iteration 144/1000 | Loss: 0.00004273
Iteration 145/1000 | Loss: 0.00004273
Iteration 146/1000 | Loss: 0.00004273
Iteration 147/1000 | Loss: 0.00004273
Iteration 148/1000 | Loss: 0.00004273
Iteration 149/1000 | Loss: 0.00004273
Iteration 150/1000 | Loss: 0.00004273
Iteration 151/1000 | Loss: 0.00004273
Iteration 152/1000 | Loss: 0.00004273
Iteration 153/1000 | Loss: 0.00004273
Iteration 154/1000 | Loss: 0.00004272
Iteration 155/1000 | Loss: 0.00004272
Iteration 156/1000 | Loss: 0.00004272
Iteration 157/1000 | Loss: 0.00004272
Iteration 158/1000 | Loss: 0.00004272
Iteration 159/1000 | Loss: 0.00004272
Iteration 160/1000 | Loss: 0.00004272
Iteration 161/1000 | Loss: 0.00004271
Iteration 162/1000 | Loss: 0.00004271
Iteration 163/1000 | Loss: 0.00004271
Iteration 164/1000 | Loss: 0.00004271
Iteration 165/1000 | Loss: 0.00004271
Iteration 166/1000 | Loss: 0.00004270
Iteration 167/1000 | Loss: 0.00004270
Iteration 168/1000 | Loss: 0.00004270
Iteration 169/1000 | Loss: 0.00004270
Iteration 170/1000 | Loss: 0.00004270
Iteration 171/1000 | Loss: 0.00004270
Iteration 172/1000 | Loss: 0.00004270
Iteration 173/1000 | Loss: 0.00004270
Iteration 174/1000 | Loss: 0.00004270
Iteration 175/1000 | Loss: 0.00004270
Iteration 176/1000 | Loss: 0.00004269
Iteration 177/1000 | Loss: 0.00004269
Iteration 178/1000 | Loss: 0.00004268
Iteration 179/1000 | Loss: 0.00004268
Iteration 180/1000 | Loss: 0.00004267
Iteration 181/1000 | Loss: 0.00004267
Iteration 182/1000 | Loss: 0.00004267
Iteration 183/1000 | Loss: 0.00004267
Iteration 184/1000 | Loss: 0.00004267
Iteration 185/1000 | Loss: 0.00004266
Iteration 186/1000 | Loss: 0.00004266
Iteration 187/1000 | Loss: 0.00004265
Iteration 188/1000 | Loss: 0.00004265
Iteration 189/1000 | Loss: 0.00004265
Iteration 190/1000 | Loss: 0.00004264
Iteration 191/1000 | Loss: 0.00004263
Iteration 192/1000 | Loss: 0.00004262
Iteration 193/1000 | Loss: 0.00004262
Iteration 194/1000 | Loss: 0.00004262
Iteration 195/1000 | Loss: 0.00004261
Iteration 196/1000 | Loss: 0.00004261
Iteration 197/1000 | Loss: 0.00004260
Iteration 198/1000 | Loss: 0.00004260
Iteration 199/1000 | Loss: 0.00004260
Iteration 200/1000 | Loss: 0.00004259
Iteration 201/1000 | Loss: 0.00004259
Iteration 202/1000 | Loss: 0.00004259
Iteration 203/1000 | Loss: 0.00004258
Iteration 204/1000 | Loss: 0.00004258
Iteration 205/1000 | Loss: 0.00004258
Iteration 206/1000 | Loss: 0.00004257
Iteration 207/1000 | Loss: 0.00004257
Iteration 208/1000 | Loss: 0.00004257
Iteration 209/1000 | Loss: 0.00004257
Iteration 210/1000 | Loss: 0.00004257
Iteration 211/1000 | Loss: 0.00004257
Iteration 212/1000 | Loss: 0.00004257
Iteration 213/1000 | Loss: 0.00004257
Iteration 214/1000 | Loss: 0.00004257
Iteration 215/1000 | Loss: 0.00004257
Iteration 216/1000 | Loss: 0.00004257
Iteration 217/1000 | Loss: 0.00004256
Iteration 218/1000 | Loss: 0.00004256
Iteration 219/1000 | Loss: 0.00004256
Iteration 220/1000 | Loss: 0.00004256
Iteration 221/1000 | Loss: 0.00004256
Iteration 222/1000 | Loss: 0.00004255
Iteration 223/1000 | Loss: 0.00004255
Iteration 224/1000 | Loss: 0.00004255
Iteration 225/1000 | Loss: 0.00004255
Iteration 226/1000 | Loss: 0.00004254
Iteration 227/1000 | Loss: 0.00004254
Iteration 228/1000 | Loss: 0.00004253
Iteration 229/1000 | Loss: 0.00004253
Iteration 230/1000 | Loss: 0.00004253
Iteration 231/1000 | Loss: 0.00004253
Iteration 232/1000 | Loss: 0.00004252
Iteration 233/1000 | Loss: 0.00004252
Iteration 234/1000 | Loss: 0.00004252
Iteration 235/1000 | Loss: 0.00004251
Iteration 236/1000 | Loss: 0.00004251
Iteration 237/1000 | Loss: 0.00004251
Iteration 238/1000 | Loss: 0.00004251
Iteration 239/1000 | Loss: 0.00004250
Iteration 240/1000 | Loss: 0.00004250
Iteration 241/1000 | Loss: 0.00004250
Iteration 242/1000 | Loss: 0.00004250
Iteration 243/1000 | Loss: 0.00004249
Iteration 244/1000 | Loss: 0.00004249
Iteration 245/1000 | Loss: 0.00004249
Iteration 246/1000 | Loss: 0.00004248
Iteration 247/1000 | Loss: 0.00004248
Iteration 248/1000 | Loss: 0.00004248
Iteration 249/1000 | Loss: 0.00004247
Iteration 250/1000 | Loss: 0.00004247
Iteration 251/1000 | Loss: 0.00004246
Iteration 252/1000 | Loss: 0.00004246
Iteration 253/1000 | Loss: 0.00004246
Iteration 254/1000 | Loss: 0.00004245
Iteration 255/1000 | Loss: 0.00004245
Iteration 256/1000 | Loss: 0.00004244
Iteration 257/1000 | Loss: 0.00004244
Iteration 258/1000 | Loss: 0.00004243
Iteration 259/1000 | Loss: 0.00004243
Iteration 260/1000 | Loss: 0.00004240
Iteration 261/1000 | Loss: 0.00004240
Iteration 262/1000 | Loss: 0.00004237
Iteration 263/1000 | Loss: 0.00004237
Iteration 264/1000 | Loss: 0.00004236
Iteration 265/1000 | Loss: 0.00004236
Iteration 266/1000 | Loss: 0.00004236
Iteration 267/1000 | Loss: 0.00004236
Iteration 268/1000 | Loss: 0.00004236
Iteration 269/1000 | Loss: 0.00004236
Iteration 270/1000 | Loss: 0.00013323
Iteration 271/1000 | Loss: 0.00005818
Iteration 272/1000 | Loss: 0.00003942
Iteration 273/1000 | Loss: 0.00003846
Iteration 274/1000 | Loss: 0.00003755
Iteration 275/1000 | Loss: 0.00003697
Iteration 276/1000 | Loss: 0.00003665
Iteration 277/1000 | Loss: 0.00003651
Iteration 278/1000 | Loss: 0.00003632
Iteration 279/1000 | Loss: 0.00003619
Iteration 280/1000 | Loss: 0.00003607
Iteration 281/1000 | Loss: 0.00003597
Iteration 282/1000 | Loss: 0.00003587
Iteration 283/1000 | Loss: 0.00003586
Iteration 284/1000 | Loss: 0.00003584
Iteration 285/1000 | Loss: 0.00003584
Iteration 286/1000 | Loss: 0.00003583
Iteration 287/1000 | Loss: 0.00003583
Iteration 288/1000 | Loss: 0.00003582
Iteration 289/1000 | Loss: 0.00003582
Iteration 290/1000 | Loss: 0.00003581
Iteration 291/1000 | Loss: 0.00003581
Iteration 292/1000 | Loss: 0.00003581
Iteration 293/1000 | Loss: 0.00003580
Iteration 294/1000 | Loss: 0.00003580
Iteration 295/1000 | Loss: 0.00003579
Iteration 296/1000 | Loss: 0.00003579
Iteration 297/1000 | Loss: 0.00003579
Iteration 298/1000 | Loss: 0.00003577
Iteration 299/1000 | Loss: 0.00003577
Iteration 300/1000 | Loss: 0.00003577
Iteration 301/1000 | Loss: 0.00003575
Iteration 302/1000 | Loss: 0.00003575
Iteration 303/1000 | Loss: 0.00003575
Iteration 304/1000 | Loss: 0.00003575
Iteration 305/1000 | Loss: 0.00003575
Iteration 306/1000 | Loss: 0.00003574
Iteration 307/1000 | Loss: 0.00003574
Iteration 308/1000 | Loss: 0.00003574
Iteration 309/1000 | Loss: 0.00003574
Iteration 310/1000 | Loss: 0.00003573
Iteration 311/1000 | Loss: 0.00003573
Iteration 312/1000 | Loss: 0.00003571
Iteration 313/1000 | Loss: 0.00003570
Iteration 314/1000 | Loss: 0.00003569
Iteration 315/1000 | Loss: 0.00003569
Iteration 316/1000 | Loss: 0.00003569
Iteration 317/1000 | Loss: 0.00003569
Iteration 318/1000 | Loss: 0.00003569
Iteration 319/1000 | Loss: 0.00003569
Iteration 320/1000 | Loss: 0.00003569
Iteration 321/1000 | Loss: 0.00003569
Iteration 322/1000 | Loss: 0.00003569
Iteration 323/1000 | Loss: 0.00003569
Iteration 324/1000 | Loss: 0.00003569
Iteration 325/1000 | Loss: 0.00003568
Iteration 326/1000 | Loss: 0.00003568
Iteration 327/1000 | Loss: 0.00003568
Iteration 328/1000 | Loss: 0.00003568
Iteration 329/1000 | Loss: 0.00003568
Iteration 330/1000 | Loss: 0.00003568
Iteration 331/1000 | Loss: 0.00003568
Iteration 332/1000 | Loss: 0.00003568
Iteration 333/1000 | Loss: 0.00003568
Iteration 334/1000 | Loss: 0.00003568
Iteration 335/1000 | Loss: 0.00003568
Iteration 336/1000 | Loss: 0.00003568
Iteration 337/1000 | Loss: 0.00003568
Iteration 338/1000 | Loss: 0.00003568
Iteration 339/1000 | Loss: 0.00003568
Iteration 340/1000 | Loss: 0.00003566
Iteration 341/1000 | Loss: 0.00003566
Iteration 342/1000 | Loss: 0.00003566
Iteration 343/1000 | Loss: 0.00003566
Iteration 344/1000 | Loss: 0.00003566
Iteration 345/1000 | Loss: 0.00003566
Iteration 346/1000 | Loss: 0.00003566
Iteration 347/1000 | Loss: 0.00003566
Iteration 348/1000 | Loss: 0.00003565
Iteration 349/1000 | Loss: 0.00003565
Iteration 350/1000 | Loss: 0.00003565
Iteration 351/1000 | Loss: 0.00003565
Iteration 352/1000 | Loss: 0.00003565
Iteration 353/1000 | Loss: 0.00003564
Iteration 354/1000 | Loss: 0.00003564
Iteration 355/1000 | Loss: 0.00003564
Iteration 356/1000 | Loss: 0.00003563
Iteration 357/1000 | Loss: 0.00003563
Iteration 358/1000 | Loss: 0.00003563
Iteration 359/1000 | Loss: 0.00003562
Iteration 360/1000 | Loss: 0.00003562
Iteration 361/1000 | Loss: 0.00003562
Iteration 362/1000 | Loss: 0.00003562
Iteration 363/1000 | Loss: 0.00003561
Iteration 364/1000 | Loss: 0.00003561
Iteration 365/1000 | Loss: 0.00003561
Iteration 366/1000 | Loss: 0.00003561
Iteration 367/1000 | Loss: 0.00003561
Iteration 368/1000 | Loss: 0.00003561
Iteration 369/1000 | Loss: 0.00003561
Iteration 370/1000 | Loss: 0.00003560
Iteration 371/1000 | Loss: 0.00003560
Iteration 372/1000 | Loss: 0.00003560
Iteration 373/1000 | Loss: 0.00003559
Iteration 374/1000 | Loss: 0.00003559
Iteration 375/1000 | Loss: 0.00003559
Iteration 376/1000 | Loss: 0.00003559
Iteration 377/1000 | Loss: 0.00003559
Iteration 378/1000 | Loss: 0.00003559
Iteration 379/1000 | Loss: 0.00003559
Iteration 380/1000 | Loss: 0.00003558
Iteration 381/1000 | Loss: 0.00003558
Iteration 382/1000 | Loss: 0.00003558
Iteration 383/1000 | Loss: 0.00003558
Iteration 384/1000 | Loss: 0.00003558
Iteration 385/1000 | Loss: 0.00003557
Iteration 386/1000 | Loss: 0.00003557
Iteration 387/1000 | Loss: 0.00003557
Iteration 388/1000 | Loss: 0.00003557
Iteration 389/1000 | Loss: 0.00003557
Iteration 390/1000 | Loss: 0.00003557
Iteration 391/1000 | Loss: 0.00003557
Iteration 392/1000 | Loss: 0.00003556
Iteration 393/1000 | Loss: 0.00003556
Iteration 394/1000 | Loss: 0.00003556
Iteration 395/1000 | Loss: 0.00003556
Iteration 396/1000 | Loss: 0.00003555
Iteration 397/1000 | Loss: 0.00003555
Iteration 398/1000 | Loss: 0.00003555
Iteration 399/1000 | Loss: 0.00003555
Iteration 400/1000 | Loss: 0.00003555
Iteration 401/1000 | Loss: 0.00003555
Iteration 402/1000 | Loss: 0.00003555
Iteration 403/1000 | Loss: 0.00003554
Iteration 404/1000 | Loss: 0.00003554
Iteration 405/1000 | Loss: 0.00003554
Iteration 406/1000 | Loss: 0.00003554
Iteration 407/1000 | Loss: 0.00003553
Iteration 408/1000 | Loss: 0.00003553
Iteration 409/1000 | Loss: 0.00003553
Iteration 410/1000 | Loss: 0.00003553
Iteration 411/1000 | Loss: 0.00003553
Iteration 412/1000 | Loss: 0.00003553
Iteration 413/1000 | Loss: 0.00003553
Iteration 414/1000 | Loss: 0.00003552
Iteration 415/1000 | Loss: 0.00003552
Iteration 416/1000 | Loss: 0.00003552
Iteration 417/1000 | Loss: 0.00003552
Iteration 418/1000 | Loss: 0.00003552
Iteration 419/1000 | Loss: 0.00003551
Iteration 420/1000 | Loss: 0.00003551
Iteration 421/1000 | Loss: 0.00003551
Iteration 422/1000 | Loss: 0.00003551
Iteration 423/1000 | Loss: 0.00003551
Iteration 424/1000 | Loss: 0.00003551
Iteration 425/1000 | Loss: 0.00003551
Iteration 426/1000 | Loss: 0.00003551
Iteration 427/1000 | Loss: 0.00003551
Iteration 428/1000 | Loss: 0.00003551
Iteration 429/1000 | Loss: 0.00003550
Iteration 430/1000 | Loss: 0.00003550
Iteration 431/1000 | Loss: 0.00003550
Iteration 432/1000 | Loss: 0.00003550
Iteration 433/1000 | Loss: 0.00003550
Iteration 434/1000 | Loss: 0.00003550
Iteration 435/1000 | Loss: 0.00003550
Iteration 436/1000 | Loss: 0.00003550
Iteration 437/1000 | Loss: 0.00003550
Iteration 438/1000 | Loss: 0.00003550
Iteration 439/1000 | Loss: 0.00003550
Iteration 440/1000 | Loss: 0.00003550
Iteration 441/1000 | Loss: 0.00003550
Iteration 442/1000 | Loss: 0.00003550
Iteration 443/1000 | Loss: 0.00003549
Iteration 444/1000 | Loss: 0.00003549
Iteration 445/1000 | Loss: 0.00003549
Iteration 446/1000 | Loss: 0.00003549
Iteration 447/1000 | Loss: 0.00003549
Iteration 448/1000 | Loss: 0.00003549
Iteration 449/1000 | Loss: 0.00003549
Iteration 450/1000 | Loss: 0.00003549
Iteration 451/1000 | Loss: 0.00003549
Iteration 452/1000 | Loss: 0.00003549
Iteration 453/1000 | Loss: 0.00003549
Iteration 454/1000 | Loss: 0.00003549
Iteration 455/1000 | Loss: 0.00003549
Iteration 456/1000 | Loss: 0.00003549
Iteration 457/1000 | Loss: 0.00003549
Iteration 458/1000 | Loss: 0.00003549
Iteration 459/1000 | Loss: 0.00003549
Iteration 460/1000 | Loss: 0.00003549
Iteration 461/1000 | Loss: 0.00003548
Iteration 462/1000 | Loss: 0.00003548
Iteration 463/1000 | Loss: 0.00003548
Iteration 464/1000 | Loss: 0.00003548
Iteration 465/1000 | Loss: 0.00003548
Iteration 466/1000 | Loss: 0.00003548
Iteration 467/1000 | Loss: 0.00003548
Iteration 468/1000 | Loss: 0.00003548
Iteration 469/1000 | Loss: 0.00003548
Iteration 470/1000 | Loss: 0.00003548
Iteration 471/1000 | Loss: 0.00003548
Iteration 472/1000 | Loss: 0.00003548
Iteration 473/1000 | Loss: 0.00003548
Iteration 474/1000 | Loss: 0.00003548
Iteration 475/1000 | Loss: 0.00003548
Iteration 476/1000 | Loss: 0.00003548
Iteration 477/1000 | Loss: 0.00003548
Iteration 478/1000 | Loss: 0.00003547
Iteration 479/1000 | Loss: 0.00003547
Iteration 480/1000 | Loss: 0.00003547
Iteration 481/1000 | Loss: 0.00003547
Iteration 482/1000 | Loss: 0.00003547
Iteration 483/1000 | Loss: 0.00003547
Iteration 484/1000 | Loss: 0.00003547
Iteration 485/1000 | Loss: 0.00003547
Iteration 486/1000 | Loss: 0.00003547
Iteration 487/1000 | Loss: 0.00003547
Iteration 488/1000 | Loss: 0.00003547
Iteration 489/1000 | Loss: 0.00003547
Iteration 490/1000 | Loss: 0.00003547
Iteration 491/1000 | Loss: 0.00003547
Iteration 492/1000 | Loss: 0.00003547
Iteration 493/1000 | Loss: 0.00003547
Iteration 494/1000 | Loss: 0.00003547
Iteration 495/1000 | Loss: 0.00003547
Iteration 496/1000 | Loss: 0.00003547
Iteration 497/1000 | Loss: 0.00003547
Iteration 498/1000 | Loss: 0.00003547
Iteration 499/1000 | Loss: 0.00003547
Iteration 500/1000 | Loss: 0.00003547
Iteration 501/1000 | Loss: 0.00003547
Iteration 502/1000 | Loss: 0.00003547
Iteration 503/1000 | Loss: 0.00003547
Iteration 504/1000 | Loss: 0.00003547
Iteration 505/1000 | Loss: 0.00003547
Iteration 506/1000 | Loss: 0.00003547
Iteration 507/1000 | Loss: 0.00003547
Iteration 508/1000 | Loss: 0.00003547
Iteration 509/1000 | Loss: 0.00003547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 509. Stopping optimization.
Last 5 losses: [3.5471090086502954e-05, 3.5471090086502954e-05, 3.5471090086502954e-05, 3.5471090086502954e-05, 3.5471090086502954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5471090086502954e-05

Optimization complete. Final v2v error: 4.393886089324951 mm

Highest mean error: 12.90560245513916 mm for frame 64

Lowest mean error: 2.9039666652679443 mm for frame 105

Saving results

Total time: 194.26033091545105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391032
Iteration 2/25 | Loss: 0.00132405
Iteration 3/25 | Loss: 0.00121542
Iteration 4/25 | Loss: 0.00119892
Iteration 5/25 | Loss: 0.00119356
Iteration 6/25 | Loss: 0.00119214
Iteration 7/25 | Loss: 0.00119195
Iteration 8/25 | Loss: 0.00119195
Iteration 9/25 | Loss: 0.00119195
Iteration 10/25 | Loss: 0.00119195
Iteration 11/25 | Loss: 0.00119195
Iteration 12/25 | Loss: 0.00119195
Iteration 13/25 | Loss: 0.00119195
Iteration 14/25 | Loss: 0.00119195
Iteration 15/25 | Loss: 0.00119195
Iteration 16/25 | Loss: 0.00119195
Iteration 17/25 | Loss: 0.00119195
Iteration 18/25 | Loss: 0.00119195
Iteration 19/25 | Loss: 0.00119195
Iteration 20/25 | Loss: 0.00119195
Iteration 21/25 | Loss: 0.00119195
Iteration 22/25 | Loss: 0.00119195
Iteration 23/25 | Loss: 0.00119195
Iteration 24/25 | Loss: 0.00119195
Iteration 25/25 | Loss: 0.00119195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30657399
Iteration 2/25 | Loss: 0.00114358
Iteration 3/25 | Loss: 0.00114358
Iteration 4/25 | Loss: 0.00114358
Iteration 5/25 | Loss: 0.00114358
Iteration 6/25 | Loss: 0.00114358
Iteration 7/25 | Loss: 0.00114358
Iteration 8/25 | Loss: 0.00114358
Iteration 9/25 | Loss: 0.00114358
Iteration 10/25 | Loss: 0.00114358
Iteration 11/25 | Loss: 0.00114358
Iteration 12/25 | Loss: 0.00114358
Iteration 13/25 | Loss: 0.00114358
Iteration 14/25 | Loss: 0.00114358
Iteration 15/25 | Loss: 0.00114358
Iteration 16/25 | Loss: 0.00114358
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00114357634447515, 0.00114357634447515, 0.00114357634447515, 0.00114357634447515, 0.00114357634447515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00114357634447515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114358
Iteration 2/1000 | Loss: 0.00004884
Iteration 3/1000 | Loss: 0.00002934
Iteration 4/1000 | Loss: 0.00002060
Iteration 5/1000 | Loss: 0.00001861
Iteration 6/1000 | Loss: 0.00001743
Iteration 7/1000 | Loss: 0.00001635
Iteration 8/1000 | Loss: 0.00001559
Iteration 9/1000 | Loss: 0.00001511
Iteration 10/1000 | Loss: 0.00001480
Iteration 11/1000 | Loss: 0.00001453
Iteration 12/1000 | Loss: 0.00001437
Iteration 13/1000 | Loss: 0.00001415
Iteration 14/1000 | Loss: 0.00001407
Iteration 15/1000 | Loss: 0.00001399
Iteration 16/1000 | Loss: 0.00001384
Iteration 17/1000 | Loss: 0.00001381
Iteration 18/1000 | Loss: 0.00001373
Iteration 19/1000 | Loss: 0.00001357
Iteration 20/1000 | Loss: 0.00001350
Iteration 21/1000 | Loss: 0.00001346
Iteration 22/1000 | Loss: 0.00001346
Iteration 23/1000 | Loss: 0.00001346
Iteration 24/1000 | Loss: 0.00001345
Iteration 25/1000 | Loss: 0.00001344
Iteration 26/1000 | Loss: 0.00001344
Iteration 27/1000 | Loss: 0.00001344
Iteration 28/1000 | Loss: 0.00001344
Iteration 29/1000 | Loss: 0.00001343
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001343
Iteration 32/1000 | Loss: 0.00001343
Iteration 33/1000 | Loss: 0.00001343
Iteration 34/1000 | Loss: 0.00001343
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001342
Iteration 37/1000 | Loss: 0.00001342
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001342
Iteration 40/1000 | Loss: 0.00001342
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001341
Iteration 43/1000 | Loss: 0.00001341
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001340
Iteration 50/1000 | Loss: 0.00001340
Iteration 51/1000 | Loss: 0.00001340
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001339
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001339
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001338
Iteration 58/1000 | Loss: 0.00001338
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001337
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001336
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001335
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001335
Iteration 71/1000 | Loss: 0.00001335
Iteration 72/1000 | Loss: 0.00001335
Iteration 73/1000 | Loss: 0.00001335
Iteration 74/1000 | Loss: 0.00001335
Iteration 75/1000 | Loss: 0.00001335
Iteration 76/1000 | Loss: 0.00001335
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001334
Iteration 79/1000 | Loss: 0.00001334
Iteration 80/1000 | Loss: 0.00001334
Iteration 81/1000 | Loss: 0.00001334
Iteration 82/1000 | Loss: 0.00001334
Iteration 83/1000 | Loss: 0.00001334
Iteration 84/1000 | Loss: 0.00001334
Iteration 85/1000 | Loss: 0.00001334
Iteration 86/1000 | Loss: 0.00001334
Iteration 87/1000 | Loss: 0.00001334
Iteration 88/1000 | Loss: 0.00001334
Iteration 89/1000 | Loss: 0.00001333
Iteration 90/1000 | Loss: 0.00001333
Iteration 91/1000 | Loss: 0.00001333
Iteration 92/1000 | Loss: 0.00001333
Iteration 93/1000 | Loss: 0.00001333
Iteration 94/1000 | Loss: 0.00001333
Iteration 95/1000 | Loss: 0.00001333
Iteration 96/1000 | Loss: 0.00001333
Iteration 97/1000 | Loss: 0.00001333
Iteration 98/1000 | Loss: 0.00001333
Iteration 99/1000 | Loss: 0.00001332
Iteration 100/1000 | Loss: 0.00001332
Iteration 101/1000 | Loss: 0.00001332
Iteration 102/1000 | Loss: 0.00001332
Iteration 103/1000 | Loss: 0.00001332
Iteration 104/1000 | Loss: 0.00001332
Iteration 105/1000 | Loss: 0.00001332
Iteration 106/1000 | Loss: 0.00001332
Iteration 107/1000 | Loss: 0.00001332
Iteration 108/1000 | Loss: 0.00001332
Iteration 109/1000 | Loss: 0.00001332
Iteration 110/1000 | Loss: 0.00001332
Iteration 111/1000 | Loss: 0.00001331
Iteration 112/1000 | Loss: 0.00001331
Iteration 113/1000 | Loss: 0.00001331
Iteration 114/1000 | Loss: 0.00001331
Iteration 115/1000 | Loss: 0.00001331
Iteration 116/1000 | Loss: 0.00001331
Iteration 117/1000 | Loss: 0.00001331
Iteration 118/1000 | Loss: 0.00001331
Iteration 119/1000 | Loss: 0.00001331
Iteration 120/1000 | Loss: 0.00001331
Iteration 121/1000 | Loss: 0.00001331
Iteration 122/1000 | Loss: 0.00001331
Iteration 123/1000 | Loss: 0.00001330
Iteration 124/1000 | Loss: 0.00001330
Iteration 125/1000 | Loss: 0.00001330
Iteration 126/1000 | Loss: 0.00001330
Iteration 127/1000 | Loss: 0.00001330
Iteration 128/1000 | Loss: 0.00001330
Iteration 129/1000 | Loss: 0.00001330
Iteration 130/1000 | Loss: 0.00001330
Iteration 131/1000 | Loss: 0.00001330
Iteration 132/1000 | Loss: 0.00001330
Iteration 133/1000 | Loss: 0.00001330
Iteration 134/1000 | Loss: 0.00001330
Iteration 135/1000 | Loss: 0.00001330
Iteration 136/1000 | Loss: 0.00001329
Iteration 137/1000 | Loss: 0.00001329
Iteration 138/1000 | Loss: 0.00001329
Iteration 139/1000 | Loss: 0.00001329
Iteration 140/1000 | Loss: 0.00001329
Iteration 141/1000 | Loss: 0.00001329
Iteration 142/1000 | Loss: 0.00001329
Iteration 143/1000 | Loss: 0.00001329
Iteration 144/1000 | Loss: 0.00001329
Iteration 145/1000 | Loss: 0.00001328
Iteration 146/1000 | Loss: 0.00001328
Iteration 147/1000 | Loss: 0.00001328
Iteration 148/1000 | Loss: 0.00001328
Iteration 149/1000 | Loss: 0.00001328
Iteration 150/1000 | Loss: 0.00001327
Iteration 151/1000 | Loss: 0.00001327
Iteration 152/1000 | Loss: 0.00001327
Iteration 153/1000 | Loss: 0.00001327
Iteration 154/1000 | Loss: 0.00001327
Iteration 155/1000 | Loss: 0.00001327
Iteration 156/1000 | Loss: 0.00001327
Iteration 157/1000 | Loss: 0.00001327
Iteration 158/1000 | Loss: 0.00001326
Iteration 159/1000 | Loss: 0.00001326
Iteration 160/1000 | Loss: 0.00001326
Iteration 161/1000 | Loss: 0.00001326
Iteration 162/1000 | Loss: 0.00001326
Iteration 163/1000 | Loss: 0.00001326
Iteration 164/1000 | Loss: 0.00001326
Iteration 165/1000 | Loss: 0.00001326
Iteration 166/1000 | Loss: 0.00001326
Iteration 167/1000 | Loss: 0.00001325
Iteration 168/1000 | Loss: 0.00001325
Iteration 169/1000 | Loss: 0.00001325
Iteration 170/1000 | Loss: 0.00001325
Iteration 171/1000 | Loss: 0.00001325
Iteration 172/1000 | Loss: 0.00001325
Iteration 173/1000 | Loss: 0.00001325
Iteration 174/1000 | Loss: 0.00001325
Iteration 175/1000 | Loss: 0.00001325
Iteration 176/1000 | Loss: 0.00001325
Iteration 177/1000 | Loss: 0.00001324
Iteration 178/1000 | Loss: 0.00001324
Iteration 179/1000 | Loss: 0.00001324
Iteration 180/1000 | Loss: 0.00001324
Iteration 181/1000 | Loss: 0.00001324
Iteration 182/1000 | Loss: 0.00001324
Iteration 183/1000 | Loss: 0.00001324
Iteration 184/1000 | Loss: 0.00001324
Iteration 185/1000 | Loss: 0.00001323
Iteration 186/1000 | Loss: 0.00001323
Iteration 187/1000 | Loss: 0.00001323
Iteration 188/1000 | Loss: 0.00001323
Iteration 189/1000 | Loss: 0.00001323
Iteration 190/1000 | Loss: 0.00001323
Iteration 191/1000 | Loss: 0.00001323
Iteration 192/1000 | Loss: 0.00001322
Iteration 193/1000 | Loss: 0.00001322
Iteration 194/1000 | Loss: 0.00001322
Iteration 195/1000 | Loss: 0.00001322
Iteration 196/1000 | Loss: 0.00001322
Iteration 197/1000 | Loss: 0.00001322
Iteration 198/1000 | Loss: 0.00001322
Iteration 199/1000 | Loss: 0.00001322
Iteration 200/1000 | Loss: 0.00001322
Iteration 201/1000 | Loss: 0.00001322
Iteration 202/1000 | Loss: 0.00001322
Iteration 203/1000 | Loss: 0.00001322
Iteration 204/1000 | Loss: 0.00001322
Iteration 205/1000 | Loss: 0.00001322
Iteration 206/1000 | Loss: 0.00001322
Iteration 207/1000 | Loss: 0.00001322
Iteration 208/1000 | Loss: 0.00001322
Iteration 209/1000 | Loss: 0.00001322
Iteration 210/1000 | Loss: 0.00001322
Iteration 211/1000 | Loss: 0.00001322
Iteration 212/1000 | Loss: 0.00001322
Iteration 213/1000 | Loss: 0.00001322
Iteration 214/1000 | Loss: 0.00001322
Iteration 215/1000 | Loss: 0.00001322
Iteration 216/1000 | Loss: 0.00001322
Iteration 217/1000 | Loss: 0.00001322
Iteration 218/1000 | Loss: 0.00001322
Iteration 219/1000 | Loss: 0.00001322
Iteration 220/1000 | Loss: 0.00001322
Iteration 221/1000 | Loss: 0.00001322
Iteration 222/1000 | Loss: 0.00001322
Iteration 223/1000 | Loss: 0.00001322
Iteration 224/1000 | Loss: 0.00001322
Iteration 225/1000 | Loss: 0.00001322
Iteration 226/1000 | Loss: 0.00001322
Iteration 227/1000 | Loss: 0.00001322
Iteration 228/1000 | Loss: 0.00001322
Iteration 229/1000 | Loss: 0.00001322
Iteration 230/1000 | Loss: 0.00001322
Iteration 231/1000 | Loss: 0.00001322
Iteration 232/1000 | Loss: 0.00001322
Iteration 233/1000 | Loss: 0.00001322
Iteration 234/1000 | Loss: 0.00001322
Iteration 235/1000 | Loss: 0.00001322
Iteration 236/1000 | Loss: 0.00001322
Iteration 237/1000 | Loss: 0.00001322
Iteration 238/1000 | Loss: 0.00001322
Iteration 239/1000 | Loss: 0.00001322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.3218542335380334e-05, 1.3218542335380334e-05, 1.3218542335380334e-05, 1.3218542335380334e-05, 1.3218542335380334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3218542335380334e-05

Optimization complete. Final v2v error: 3.124390125274658 mm

Highest mean error: 3.5393636226654053 mm for frame 74

Lowest mean error: 2.8215084075927734 mm for frame 136

Saving results

Total time: 46.73221182823181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801886
Iteration 2/25 | Loss: 0.00125751
Iteration 3/25 | Loss: 0.00120580
Iteration 4/25 | Loss: 0.00118873
Iteration 5/25 | Loss: 0.00118460
Iteration 6/25 | Loss: 0.00118436
Iteration 7/25 | Loss: 0.00118436
Iteration 8/25 | Loss: 0.00118436
Iteration 9/25 | Loss: 0.00118436
Iteration 10/25 | Loss: 0.00118436
Iteration 11/25 | Loss: 0.00118436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011843574466183782, 0.0011843574466183782, 0.0011843574466183782, 0.0011843574466183782, 0.0011843574466183782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011843574466183782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.55809450
Iteration 2/25 | Loss: 0.00145400
Iteration 3/25 | Loss: 0.00145395
Iteration 4/25 | Loss: 0.00145395
Iteration 5/25 | Loss: 0.00145395
Iteration 6/25 | Loss: 0.00145395
Iteration 7/25 | Loss: 0.00145395
Iteration 8/25 | Loss: 0.00145395
Iteration 9/25 | Loss: 0.00145395
Iteration 10/25 | Loss: 0.00145395
Iteration 11/25 | Loss: 0.00145394
Iteration 12/25 | Loss: 0.00145394
Iteration 13/25 | Loss: 0.00145394
Iteration 14/25 | Loss: 0.00145394
Iteration 15/25 | Loss: 0.00145394
Iteration 16/25 | Loss: 0.00145394
Iteration 17/25 | Loss: 0.00145394
Iteration 18/25 | Loss: 0.00145394
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001453944481909275, 0.001453944481909275, 0.001453944481909275, 0.001453944481909275, 0.001453944481909275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001453944481909275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145394
Iteration 2/1000 | Loss: 0.00002795
Iteration 3/1000 | Loss: 0.00001848
Iteration 4/1000 | Loss: 0.00001706
Iteration 5/1000 | Loss: 0.00001587
Iteration 6/1000 | Loss: 0.00001515
Iteration 7/1000 | Loss: 0.00001469
Iteration 8/1000 | Loss: 0.00001425
Iteration 9/1000 | Loss: 0.00001389
Iteration 10/1000 | Loss: 0.00001365
Iteration 11/1000 | Loss: 0.00001346
Iteration 12/1000 | Loss: 0.00001338
Iteration 13/1000 | Loss: 0.00001332
Iteration 14/1000 | Loss: 0.00001331
Iteration 15/1000 | Loss: 0.00001329
Iteration 16/1000 | Loss: 0.00001327
Iteration 17/1000 | Loss: 0.00001327
Iteration 18/1000 | Loss: 0.00001326
Iteration 19/1000 | Loss: 0.00001325
Iteration 20/1000 | Loss: 0.00001325
Iteration 21/1000 | Loss: 0.00001325
Iteration 22/1000 | Loss: 0.00001325
Iteration 23/1000 | Loss: 0.00001324
Iteration 24/1000 | Loss: 0.00001323
Iteration 25/1000 | Loss: 0.00001322
Iteration 26/1000 | Loss: 0.00001322
Iteration 27/1000 | Loss: 0.00001317
Iteration 28/1000 | Loss: 0.00001316
Iteration 29/1000 | Loss: 0.00001315
Iteration 30/1000 | Loss: 0.00001314
Iteration 31/1000 | Loss: 0.00001313
Iteration 32/1000 | Loss: 0.00001310
Iteration 33/1000 | Loss: 0.00001310
Iteration 34/1000 | Loss: 0.00001309
Iteration 35/1000 | Loss: 0.00001309
Iteration 36/1000 | Loss: 0.00001309
Iteration 37/1000 | Loss: 0.00001308
Iteration 38/1000 | Loss: 0.00001308
Iteration 39/1000 | Loss: 0.00001308
Iteration 40/1000 | Loss: 0.00001308
Iteration 41/1000 | Loss: 0.00001308
Iteration 42/1000 | Loss: 0.00001307
Iteration 43/1000 | Loss: 0.00001307
Iteration 44/1000 | Loss: 0.00001307
Iteration 45/1000 | Loss: 0.00001306
Iteration 46/1000 | Loss: 0.00001306
Iteration 47/1000 | Loss: 0.00001306
Iteration 48/1000 | Loss: 0.00001305
Iteration 49/1000 | Loss: 0.00001305
Iteration 50/1000 | Loss: 0.00001305
Iteration 51/1000 | Loss: 0.00001305
Iteration 52/1000 | Loss: 0.00001304
Iteration 53/1000 | Loss: 0.00001304
Iteration 54/1000 | Loss: 0.00001304
Iteration 55/1000 | Loss: 0.00001304
Iteration 56/1000 | Loss: 0.00001304
Iteration 57/1000 | Loss: 0.00001304
Iteration 58/1000 | Loss: 0.00001304
Iteration 59/1000 | Loss: 0.00001304
Iteration 60/1000 | Loss: 0.00001304
Iteration 61/1000 | Loss: 0.00001304
Iteration 62/1000 | Loss: 0.00001304
Iteration 63/1000 | Loss: 0.00001304
Iteration 64/1000 | Loss: 0.00001303
Iteration 65/1000 | Loss: 0.00001303
Iteration 66/1000 | Loss: 0.00001303
Iteration 67/1000 | Loss: 0.00001303
Iteration 68/1000 | Loss: 0.00001303
Iteration 69/1000 | Loss: 0.00001303
Iteration 70/1000 | Loss: 0.00001303
Iteration 71/1000 | Loss: 0.00001303
Iteration 72/1000 | Loss: 0.00001302
Iteration 73/1000 | Loss: 0.00001302
Iteration 74/1000 | Loss: 0.00001302
Iteration 75/1000 | Loss: 0.00001302
Iteration 76/1000 | Loss: 0.00001302
Iteration 77/1000 | Loss: 0.00001302
Iteration 78/1000 | Loss: 0.00001302
Iteration 79/1000 | Loss: 0.00001302
Iteration 80/1000 | Loss: 0.00001301
Iteration 81/1000 | Loss: 0.00001301
Iteration 82/1000 | Loss: 0.00001301
Iteration 83/1000 | Loss: 0.00001301
Iteration 84/1000 | Loss: 0.00001301
Iteration 85/1000 | Loss: 0.00001301
Iteration 86/1000 | Loss: 0.00001301
Iteration 87/1000 | Loss: 0.00001301
Iteration 88/1000 | Loss: 0.00001301
Iteration 89/1000 | Loss: 0.00001301
Iteration 90/1000 | Loss: 0.00001300
Iteration 91/1000 | Loss: 0.00001300
Iteration 92/1000 | Loss: 0.00001300
Iteration 93/1000 | Loss: 0.00001300
Iteration 94/1000 | Loss: 0.00001300
Iteration 95/1000 | Loss: 0.00001300
Iteration 96/1000 | Loss: 0.00001300
Iteration 97/1000 | Loss: 0.00001300
Iteration 98/1000 | Loss: 0.00001300
Iteration 99/1000 | Loss: 0.00001300
Iteration 100/1000 | Loss: 0.00001300
Iteration 101/1000 | Loss: 0.00001300
Iteration 102/1000 | Loss: 0.00001300
Iteration 103/1000 | Loss: 0.00001300
Iteration 104/1000 | Loss: 0.00001300
Iteration 105/1000 | Loss: 0.00001300
Iteration 106/1000 | Loss: 0.00001300
Iteration 107/1000 | Loss: 0.00001300
Iteration 108/1000 | Loss: 0.00001299
Iteration 109/1000 | Loss: 0.00001299
Iteration 110/1000 | Loss: 0.00001299
Iteration 111/1000 | Loss: 0.00001299
Iteration 112/1000 | Loss: 0.00001299
Iteration 113/1000 | Loss: 0.00001299
Iteration 114/1000 | Loss: 0.00001299
Iteration 115/1000 | Loss: 0.00001299
Iteration 116/1000 | Loss: 0.00001299
Iteration 117/1000 | Loss: 0.00001299
Iteration 118/1000 | Loss: 0.00001299
Iteration 119/1000 | Loss: 0.00001299
Iteration 120/1000 | Loss: 0.00001298
Iteration 121/1000 | Loss: 0.00001298
Iteration 122/1000 | Loss: 0.00001298
Iteration 123/1000 | Loss: 0.00001298
Iteration 124/1000 | Loss: 0.00001298
Iteration 125/1000 | Loss: 0.00001298
Iteration 126/1000 | Loss: 0.00001298
Iteration 127/1000 | Loss: 0.00001298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.2982736734556966e-05, 1.2982736734556966e-05, 1.2982736734556966e-05, 1.2982736734556966e-05, 1.2982736734556966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2982736734556966e-05

Optimization complete. Final v2v error: 3.095205068588257 mm

Highest mean error: 3.491018772125244 mm for frame 28

Lowest mean error: 2.7700142860412598 mm for frame 236

Saving results

Total time: 38.463605880737305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965626
Iteration 2/25 | Loss: 0.00313564
Iteration 3/25 | Loss: 0.00218917
Iteration 4/25 | Loss: 0.00206653
Iteration 5/25 | Loss: 0.00196555
Iteration 6/25 | Loss: 0.00195481
Iteration 7/25 | Loss: 0.00186332
Iteration 8/25 | Loss: 0.00175483
Iteration 9/25 | Loss: 0.00171129
Iteration 10/25 | Loss: 0.00167896
Iteration 11/25 | Loss: 0.00166255
Iteration 12/25 | Loss: 0.00161823
Iteration 13/25 | Loss: 0.00158126
Iteration 14/25 | Loss: 0.00156327
Iteration 15/25 | Loss: 0.00155870
Iteration 16/25 | Loss: 0.00155643
Iteration 17/25 | Loss: 0.00155585
Iteration 18/25 | Loss: 0.00155304
Iteration 19/25 | Loss: 0.00155251
Iteration 20/25 | Loss: 0.00155218
Iteration 21/25 | Loss: 0.00155198
Iteration 22/25 | Loss: 0.00156031
Iteration 23/25 | Loss: 0.00155588
Iteration 24/25 | Loss: 0.00155709
Iteration 25/25 | Loss: 0.00155530

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32690668
Iteration 2/25 | Loss: 0.00419977
Iteration 3/25 | Loss: 0.00320214
Iteration 4/25 | Loss: 0.00320204
Iteration 5/25 | Loss: 0.00320204
Iteration 6/25 | Loss: 0.00320204
Iteration 7/25 | Loss: 0.00320204
Iteration 8/25 | Loss: 0.00320204
Iteration 9/25 | Loss: 0.00320204
Iteration 10/25 | Loss: 0.00320204
Iteration 11/25 | Loss: 0.00320203
Iteration 12/25 | Loss: 0.00320203
Iteration 13/25 | Loss: 0.00320203
Iteration 14/25 | Loss: 0.00320203
Iteration 15/25 | Loss: 0.00320203
Iteration 16/25 | Loss: 0.00320203
Iteration 17/25 | Loss: 0.00320203
Iteration 18/25 | Loss: 0.00320203
Iteration 19/25 | Loss: 0.00320203
Iteration 20/25 | Loss: 0.00320203
Iteration 21/25 | Loss: 0.00320203
Iteration 22/25 | Loss: 0.00320203
Iteration 23/25 | Loss: 0.00320203
Iteration 24/25 | Loss: 0.00320203
Iteration 25/25 | Loss: 0.00320203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00320203
Iteration 2/1000 | Loss: 0.00162339
Iteration 3/1000 | Loss: 0.00271236
Iteration 4/1000 | Loss: 0.00088124
Iteration 5/1000 | Loss: 0.00041677
Iteration 6/1000 | Loss: 0.00043703
Iteration 7/1000 | Loss: 0.00054574
Iteration 8/1000 | Loss: 0.00027405
Iteration 9/1000 | Loss: 0.00031115
Iteration 10/1000 | Loss: 0.00053880
Iteration 11/1000 | Loss: 0.00177007
Iteration 12/1000 | Loss: 0.00036452
Iteration 13/1000 | Loss: 0.00118280
Iteration 14/1000 | Loss: 0.00330555
Iteration 15/1000 | Loss: 0.00565718
Iteration 16/1000 | Loss: 0.00323943
Iteration 17/1000 | Loss: 0.00032506
Iteration 18/1000 | Loss: 0.00107731
Iteration 19/1000 | Loss: 0.00026920
Iteration 20/1000 | Loss: 0.00014031
Iteration 21/1000 | Loss: 0.00072085
Iteration 22/1000 | Loss: 0.00083777
Iteration 23/1000 | Loss: 0.00034369
Iteration 24/1000 | Loss: 0.00009594
Iteration 25/1000 | Loss: 0.00021697
Iteration 26/1000 | Loss: 0.00027248
Iteration 27/1000 | Loss: 0.00052760
Iteration 28/1000 | Loss: 0.00083607
Iteration 29/1000 | Loss: 0.00052896
Iteration 30/1000 | Loss: 0.00020298
Iteration 31/1000 | Loss: 0.00035409
Iteration 32/1000 | Loss: 0.00046710
Iteration 33/1000 | Loss: 0.00010064
Iteration 34/1000 | Loss: 0.00007874
Iteration 35/1000 | Loss: 0.00047571
Iteration 36/1000 | Loss: 0.00008207
Iteration 37/1000 | Loss: 0.00010110
Iteration 38/1000 | Loss: 0.00034976
Iteration 39/1000 | Loss: 0.00055148
Iteration 40/1000 | Loss: 0.00010677
Iteration 41/1000 | Loss: 0.00008827
Iteration 42/1000 | Loss: 0.00007629
Iteration 43/1000 | Loss: 0.00007227
Iteration 44/1000 | Loss: 0.00022604
Iteration 45/1000 | Loss: 0.00017676
Iteration 46/1000 | Loss: 0.00073474
Iteration 47/1000 | Loss: 0.00373598
Iteration 48/1000 | Loss: 0.00292318
Iteration 49/1000 | Loss: 0.00044077
Iteration 50/1000 | Loss: 0.00032234
Iteration 51/1000 | Loss: 0.00027612
Iteration 52/1000 | Loss: 0.00010454
Iteration 53/1000 | Loss: 0.00008047
Iteration 54/1000 | Loss: 0.00005691
Iteration 55/1000 | Loss: 0.00004545
Iteration 56/1000 | Loss: 0.00006113
Iteration 57/1000 | Loss: 0.00020722
Iteration 58/1000 | Loss: 0.00005773
Iteration 59/1000 | Loss: 0.00048931
Iteration 60/1000 | Loss: 0.00012431
Iteration 61/1000 | Loss: 0.00005047
Iteration 62/1000 | Loss: 0.00019068
Iteration 63/1000 | Loss: 0.00015473
Iteration 64/1000 | Loss: 0.00050504
Iteration 65/1000 | Loss: 0.00057286
Iteration 66/1000 | Loss: 0.00008562
Iteration 67/1000 | Loss: 0.00004013
Iteration 68/1000 | Loss: 0.00003232
Iteration 69/1000 | Loss: 0.00003011
Iteration 70/1000 | Loss: 0.00017241
Iteration 71/1000 | Loss: 0.00059105
Iteration 72/1000 | Loss: 0.00003028
Iteration 73/1000 | Loss: 0.00002667
Iteration 74/1000 | Loss: 0.00002552
Iteration 75/1000 | Loss: 0.00002463
Iteration 76/1000 | Loss: 0.00002395
Iteration 77/1000 | Loss: 0.00002340
Iteration 78/1000 | Loss: 0.00023398
Iteration 79/1000 | Loss: 0.00002359
Iteration 80/1000 | Loss: 0.00002278
Iteration 81/1000 | Loss: 0.00002252
Iteration 82/1000 | Loss: 0.00002250
Iteration 83/1000 | Loss: 0.00002230
Iteration 84/1000 | Loss: 0.00002229
Iteration 85/1000 | Loss: 0.00002227
Iteration 86/1000 | Loss: 0.00002223
Iteration 87/1000 | Loss: 0.00002223
Iteration 88/1000 | Loss: 0.00002220
Iteration 89/1000 | Loss: 0.00002214
Iteration 90/1000 | Loss: 0.00002209
Iteration 91/1000 | Loss: 0.00002208
Iteration 92/1000 | Loss: 0.00002207
Iteration 93/1000 | Loss: 0.00002206
Iteration 94/1000 | Loss: 0.00002203
Iteration 95/1000 | Loss: 0.00002200
Iteration 96/1000 | Loss: 0.00002199
Iteration 97/1000 | Loss: 0.00002199
Iteration 98/1000 | Loss: 0.00003776
Iteration 99/1000 | Loss: 0.00033434
Iteration 100/1000 | Loss: 0.00011370
Iteration 101/1000 | Loss: 0.00014631
Iteration 102/1000 | Loss: 0.00004111
Iteration 103/1000 | Loss: 0.00010481
Iteration 104/1000 | Loss: 0.00023696
Iteration 105/1000 | Loss: 0.00008253
Iteration 106/1000 | Loss: 0.00012848
Iteration 107/1000 | Loss: 0.00009782
Iteration 108/1000 | Loss: 0.00002942
Iteration 109/1000 | Loss: 0.00002693
Iteration 110/1000 | Loss: 0.00002534
Iteration 111/1000 | Loss: 0.00002465
Iteration 112/1000 | Loss: 0.00002415
Iteration 113/1000 | Loss: 0.00002375
Iteration 114/1000 | Loss: 0.00002358
Iteration 115/1000 | Loss: 0.00002344
Iteration 116/1000 | Loss: 0.00020802
Iteration 117/1000 | Loss: 0.00011228
Iteration 118/1000 | Loss: 0.00007691
Iteration 119/1000 | Loss: 0.00014763
Iteration 120/1000 | Loss: 0.00011181
Iteration 121/1000 | Loss: 0.00007462
Iteration 122/1000 | Loss: 0.00002615
Iteration 123/1000 | Loss: 0.00002311
Iteration 124/1000 | Loss: 0.00002206
Iteration 125/1000 | Loss: 0.00002170
Iteration 126/1000 | Loss: 0.00002153
Iteration 127/1000 | Loss: 0.00002149
Iteration 128/1000 | Loss: 0.00002148
Iteration 129/1000 | Loss: 0.00002147
Iteration 130/1000 | Loss: 0.00002143
Iteration 131/1000 | Loss: 0.00002141
Iteration 132/1000 | Loss: 0.00002141
Iteration 133/1000 | Loss: 0.00002140
Iteration 134/1000 | Loss: 0.00002140
Iteration 135/1000 | Loss: 0.00002139
Iteration 136/1000 | Loss: 0.00002139
Iteration 137/1000 | Loss: 0.00002138
Iteration 138/1000 | Loss: 0.00002138
Iteration 139/1000 | Loss: 0.00002138
Iteration 140/1000 | Loss: 0.00002137
Iteration 141/1000 | Loss: 0.00002137
Iteration 142/1000 | Loss: 0.00002134
Iteration 143/1000 | Loss: 0.00002133
Iteration 144/1000 | Loss: 0.00002132
Iteration 145/1000 | Loss: 0.00002132
Iteration 146/1000 | Loss: 0.00002132
Iteration 147/1000 | Loss: 0.00002131
Iteration 148/1000 | Loss: 0.00002131
Iteration 149/1000 | Loss: 0.00002130
Iteration 150/1000 | Loss: 0.00002129
Iteration 151/1000 | Loss: 0.00002129
Iteration 152/1000 | Loss: 0.00002129
Iteration 153/1000 | Loss: 0.00002128
Iteration 154/1000 | Loss: 0.00002128
Iteration 155/1000 | Loss: 0.00002127
Iteration 156/1000 | Loss: 0.00002127
Iteration 157/1000 | Loss: 0.00002127
Iteration 158/1000 | Loss: 0.00002126
Iteration 159/1000 | Loss: 0.00002126
Iteration 160/1000 | Loss: 0.00002126
Iteration 161/1000 | Loss: 0.00002125
Iteration 162/1000 | Loss: 0.00002124
Iteration 163/1000 | Loss: 0.00002124
Iteration 164/1000 | Loss: 0.00002123
Iteration 165/1000 | Loss: 0.00002123
Iteration 166/1000 | Loss: 0.00002122
Iteration 167/1000 | Loss: 0.00002122
Iteration 168/1000 | Loss: 0.00002121
Iteration 169/1000 | Loss: 0.00002121
Iteration 170/1000 | Loss: 0.00002120
Iteration 171/1000 | Loss: 0.00002120
Iteration 172/1000 | Loss: 0.00002120
Iteration 173/1000 | Loss: 0.00002120
Iteration 174/1000 | Loss: 0.00002120
Iteration 175/1000 | Loss: 0.00002120
Iteration 176/1000 | Loss: 0.00002120
Iteration 177/1000 | Loss: 0.00002119
Iteration 178/1000 | Loss: 0.00002119
Iteration 179/1000 | Loss: 0.00002119
Iteration 180/1000 | Loss: 0.00002119
Iteration 181/1000 | Loss: 0.00002119
Iteration 182/1000 | Loss: 0.00002119
Iteration 183/1000 | Loss: 0.00002119
Iteration 184/1000 | Loss: 0.00002119
Iteration 185/1000 | Loss: 0.00002119
Iteration 186/1000 | Loss: 0.00002119
Iteration 187/1000 | Loss: 0.00002119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.1191157429711893e-05, 2.1191157429711893e-05, 2.1191157429711893e-05, 2.1191157429711893e-05, 2.1191157429711893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1191157429711893e-05

Optimization complete. Final v2v error: 3.669583320617676 mm

Highest mean error: 5.43369197845459 mm for frame 2

Lowest mean error: 3.2971949577331543 mm for frame 126

Saving results

Total time: 228.67600655555725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820371
Iteration 2/25 | Loss: 0.00155397
Iteration 3/25 | Loss: 0.00133017
Iteration 4/25 | Loss: 0.00131035
Iteration 5/25 | Loss: 0.00130830
Iteration 6/25 | Loss: 0.00130803
Iteration 7/25 | Loss: 0.00130803
Iteration 8/25 | Loss: 0.00130803
Iteration 9/25 | Loss: 0.00130803
Iteration 10/25 | Loss: 0.00130803
Iteration 11/25 | Loss: 0.00130803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013080349890515208, 0.0013080349890515208, 0.0013080349890515208, 0.0013080349890515208, 0.0013080349890515208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013080349890515208

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96607566
Iteration 2/25 | Loss: 0.00063028
Iteration 3/25 | Loss: 0.00063027
Iteration 4/25 | Loss: 0.00063027
Iteration 5/25 | Loss: 0.00063027
Iteration 6/25 | Loss: 0.00063027
Iteration 7/25 | Loss: 0.00063027
Iteration 8/25 | Loss: 0.00063027
Iteration 9/25 | Loss: 0.00063027
Iteration 10/25 | Loss: 0.00063027
Iteration 11/25 | Loss: 0.00063027
Iteration 12/25 | Loss: 0.00063027
Iteration 13/25 | Loss: 0.00063027
Iteration 14/25 | Loss: 0.00063027
Iteration 15/25 | Loss: 0.00063027
Iteration 16/25 | Loss: 0.00063027
Iteration 17/25 | Loss: 0.00063027
Iteration 18/25 | Loss: 0.00063027
Iteration 19/25 | Loss: 0.00063027
Iteration 20/25 | Loss: 0.00063027
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006302715628407896, 0.0006302715628407896, 0.0006302715628407896, 0.0006302715628407896, 0.0006302715628407896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006302715628407896

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063027
Iteration 2/1000 | Loss: 0.00003646
Iteration 3/1000 | Loss: 0.00002913
Iteration 4/1000 | Loss: 0.00002652
Iteration 5/1000 | Loss: 0.00002539
Iteration 6/1000 | Loss: 0.00002469
Iteration 7/1000 | Loss: 0.00002410
Iteration 8/1000 | Loss: 0.00002372
Iteration 9/1000 | Loss: 0.00002339
Iteration 10/1000 | Loss: 0.00002312
Iteration 11/1000 | Loss: 0.00002294
Iteration 12/1000 | Loss: 0.00002287
Iteration 13/1000 | Loss: 0.00002271
Iteration 14/1000 | Loss: 0.00002267
Iteration 15/1000 | Loss: 0.00002266
Iteration 16/1000 | Loss: 0.00002266
Iteration 17/1000 | Loss: 0.00002266
Iteration 18/1000 | Loss: 0.00002266
Iteration 19/1000 | Loss: 0.00002266
Iteration 20/1000 | Loss: 0.00002266
Iteration 21/1000 | Loss: 0.00002266
Iteration 22/1000 | Loss: 0.00002260
Iteration 23/1000 | Loss: 0.00002258
Iteration 24/1000 | Loss: 0.00002257
Iteration 25/1000 | Loss: 0.00002251
Iteration 26/1000 | Loss: 0.00002251
Iteration 27/1000 | Loss: 0.00002251
Iteration 28/1000 | Loss: 0.00002250
Iteration 29/1000 | Loss: 0.00002250
Iteration 30/1000 | Loss: 0.00002248
Iteration 31/1000 | Loss: 0.00002247
Iteration 32/1000 | Loss: 0.00002247
Iteration 33/1000 | Loss: 0.00002247
Iteration 34/1000 | Loss: 0.00002247
Iteration 35/1000 | Loss: 0.00002246
Iteration 36/1000 | Loss: 0.00002246
Iteration 37/1000 | Loss: 0.00002246
Iteration 38/1000 | Loss: 0.00002246
Iteration 39/1000 | Loss: 0.00002245
Iteration 40/1000 | Loss: 0.00002244
Iteration 41/1000 | Loss: 0.00002244
Iteration 42/1000 | Loss: 0.00002243
Iteration 43/1000 | Loss: 0.00002243
Iteration 44/1000 | Loss: 0.00002243
Iteration 45/1000 | Loss: 0.00002243
Iteration 46/1000 | Loss: 0.00002243
Iteration 47/1000 | Loss: 0.00002243
Iteration 48/1000 | Loss: 0.00002243
Iteration 49/1000 | Loss: 0.00002242
Iteration 50/1000 | Loss: 0.00002242
Iteration 51/1000 | Loss: 0.00002240
Iteration 52/1000 | Loss: 0.00002240
Iteration 53/1000 | Loss: 0.00002240
Iteration 54/1000 | Loss: 0.00002239
Iteration 55/1000 | Loss: 0.00002238
Iteration 56/1000 | Loss: 0.00002238
Iteration 57/1000 | Loss: 0.00002236
Iteration 58/1000 | Loss: 0.00002235
Iteration 59/1000 | Loss: 0.00002235
Iteration 60/1000 | Loss: 0.00002234
Iteration 61/1000 | Loss: 0.00002234
Iteration 62/1000 | Loss: 0.00002234
Iteration 63/1000 | Loss: 0.00002234
Iteration 64/1000 | Loss: 0.00002232
Iteration 65/1000 | Loss: 0.00002232
Iteration 66/1000 | Loss: 0.00002232
Iteration 67/1000 | Loss: 0.00002232
Iteration 68/1000 | Loss: 0.00002232
Iteration 69/1000 | Loss: 0.00002231
Iteration 70/1000 | Loss: 0.00002231
Iteration 71/1000 | Loss: 0.00002231
Iteration 72/1000 | Loss: 0.00002231
Iteration 73/1000 | Loss: 0.00002231
Iteration 74/1000 | Loss: 0.00002230
Iteration 75/1000 | Loss: 0.00002230
Iteration 76/1000 | Loss: 0.00002229
Iteration 77/1000 | Loss: 0.00002228
Iteration 78/1000 | Loss: 0.00002228
Iteration 79/1000 | Loss: 0.00002228
Iteration 80/1000 | Loss: 0.00002228
Iteration 81/1000 | Loss: 0.00002228
Iteration 82/1000 | Loss: 0.00002228
Iteration 83/1000 | Loss: 0.00002228
Iteration 84/1000 | Loss: 0.00002228
Iteration 85/1000 | Loss: 0.00002227
Iteration 86/1000 | Loss: 0.00002227
Iteration 87/1000 | Loss: 0.00002227
Iteration 88/1000 | Loss: 0.00002227
Iteration 89/1000 | Loss: 0.00002227
Iteration 90/1000 | Loss: 0.00002227
Iteration 91/1000 | Loss: 0.00002227
Iteration 92/1000 | Loss: 0.00002227
Iteration 93/1000 | Loss: 0.00002227
Iteration 94/1000 | Loss: 0.00002227
Iteration 95/1000 | Loss: 0.00002225
Iteration 96/1000 | Loss: 0.00002225
Iteration 97/1000 | Loss: 0.00002225
Iteration 98/1000 | Loss: 0.00002225
Iteration 99/1000 | Loss: 0.00002224
Iteration 100/1000 | Loss: 0.00002224
Iteration 101/1000 | Loss: 0.00002224
Iteration 102/1000 | Loss: 0.00002224
Iteration 103/1000 | Loss: 0.00002224
Iteration 104/1000 | Loss: 0.00002224
Iteration 105/1000 | Loss: 0.00002224
Iteration 106/1000 | Loss: 0.00002224
Iteration 107/1000 | Loss: 0.00002222
Iteration 108/1000 | Loss: 0.00002222
Iteration 109/1000 | Loss: 0.00002222
Iteration 110/1000 | Loss: 0.00002222
Iteration 111/1000 | Loss: 0.00002222
Iteration 112/1000 | Loss: 0.00002221
Iteration 113/1000 | Loss: 0.00002221
Iteration 114/1000 | Loss: 0.00002220
Iteration 115/1000 | Loss: 0.00002220
Iteration 116/1000 | Loss: 0.00002220
Iteration 117/1000 | Loss: 0.00002220
Iteration 118/1000 | Loss: 0.00002220
Iteration 119/1000 | Loss: 0.00002220
Iteration 120/1000 | Loss: 0.00002220
Iteration 121/1000 | Loss: 0.00002220
Iteration 122/1000 | Loss: 0.00002219
Iteration 123/1000 | Loss: 0.00002219
Iteration 124/1000 | Loss: 0.00002219
Iteration 125/1000 | Loss: 0.00002219
Iteration 126/1000 | Loss: 0.00002219
Iteration 127/1000 | Loss: 0.00002219
Iteration 128/1000 | Loss: 0.00002219
Iteration 129/1000 | Loss: 0.00002219
Iteration 130/1000 | Loss: 0.00002219
Iteration 131/1000 | Loss: 0.00002219
Iteration 132/1000 | Loss: 0.00002219
Iteration 133/1000 | Loss: 0.00002219
Iteration 134/1000 | Loss: 0.00002219
Iteration 135/1000 | Loss: 0.00002218
Iteration 136/1000 | Loss: 0.00002218
Iteration 137/1000 | Loss: 0.00002218
Iteration 138/1000 | Loss: 0.00002218
Iteration 139/1000 | Loss: 0.00002218
Iteration 140/1000 | Loss: 0.00002218
Iteration 141/1000 | Loss: 0.00002218
Iteration 142/1000 | Loss: 0.00002218
Iteration 143/1000 | Loss: 0.00002218
Iteration 144/1000 | Loss: 0.00002218
Iteration 145/1000 | Loss: 0.00002218
Iteration 146/1000 | Loss: 0.00002217
Iteration 147/1000 | Loss: 0.00002217
Iteration 148/1000 | Loss: 0.00002217
Iteration 149/1000 | Loss: 0.00002217
Iteration 150/1000 | Loss: 0.00002217
Iteration 151/1000 | Loss: 0.00002217
Iteration 152/1000 | Loss: 0.00002217
Iteration 153/1000 | Loss: 0.00002216
Iteration 154/1000 | Loss: 0.00002216
Iteration 155/1000 | Loss: 0.00002216
Iteration 156/1000 | Loss: 0.00002216
Iteration 157/1000 | Loss: 0.00002216
Iteration 158/1000 | Loss: 0.00002216
Iteration 159/1000 | Loss: 0.00002216
Iteration 160/1000 | Loss: 0.00002216
Iteration 161/1000 | Loss: 0.00002216
Iteration 162/1000 | Loss: 0.00002216
Iteration 163/1000 | Loss: 0.00002216
Iteration 164/1000 | Loss: 0.00002216
Iteration 165/1000 | Loss: 0.00002216
Iteration 166/1000 | Loss: 0.00002216
Iteration 167/1000 | Loss: 0.00002216
Iteration 168/1000 | Loss: 0.00002215
Iteration 169/1000 | Loss: 0.00002215
Iteration 170/1000 | Loss: 0.00002215
Iteration 171/1000 | Loss: 0.00002215
Iteration 172/1000 | Loss: 0.00002215
Iteration 173/1000 | Loss: 0.00002215
Iteration 174/1000 | Loss: 0.00002215
Iteration 175/1000 | Loss: 0.00002214
Iteration 176/1000 | Loss: 0.00002214
Iteration 177/1000 | Loss: 0.00002214
Iteration 178/1000 | Loss: 0.00002214
Iteration 179/1000 | Loss: 0.00002214
Iteration 180/1000 | Loss: 0.00002214
Iteration 181/1000 | Loss: 0.00002214
Iteration 182/1000 | Loss: 0.00002214
Iteration 183/1000 | Loss: 0.00002214
Iteration 184/1000 | Loss: 0.00002214
Iteration 185/1000 | Loss: 0.00002213
Iteration 186/1000 | Loss: 0.00002213
Iteration 187/1000 | Loss: 0.00002213
Iteration 188/1000 | Loss: 0.00002213
Iteration 189/1000 | Loss: 0.00002213
Iteration 190/1000 | Loss: 0.00002213
Iteration 191/1000 | Loss: 0.00002213
Iteration 192/1000 | Loss: 0.00002213
Iteration 193/1000 | Loss: 0.00002213
Iteration 194/1000 | Loss: 0.00002213
Iteration 195/1000 | Loss: 0.00002213
Iteration 196/1000 | Loss: 0.00002213
Iteration 197/1000 | Loss: 0.00002213
Iteration 198/1000 | Loss: 0.00002213
Iteration 199/1000 | Loss: 0.00002213
Iteration 200/1000 | Loss: 0.00002213
Iteration 201/1000 | Loss: 0.00002213
Iteration 202/1000 | Loss: 0.00002213
Iteration 203/1000 | Loss: 0.00002212
Iteration 204/1000 | Loss: 0.00002212
Iteration 205/1000 | Loss: 0.00002212
Iteration 206/1000 | Loss: 0.00002212
Iteration 207/1000 | Loss: 0.00002212
Iteration 208/1000 | Loss: 0.00002212
Iteration 209/1000 | Loss: 0.00002212
Iteration 210/1000 | Loss: 0.00002212
Iteration 211/1000 | Loss: 0.00002212
Iteration 212/1000 | Loss: 0.00002212
Iteration 213/1000 | Loss: 0.00002212
Iteration 214/1000 | Loss: 0.00002212
Iteration 215/1000 | Loss: 0.00002212
Iteration 216/1000 | Loss: 0.00002212
Iteration 217/1000 | Loss: 0.00002212
Iteration 218/1000 | Loss: 0.00002211
Iteration 219/1000 | Loss: 0.00002211
Iteration 220/1000 | Loss: 0.00002211
Iteration 221/1000 | Loss: 0.00002211
Iteration 222/1000 | Loss: 0.00002211
Iteration 223/1000 | Loss: 0.00002211
Iteration 224/1000 | Loss: 0.00002211
Iteration 225/1000 | Loss: 0.00002211
Iteration 226/1000 | Loss: 0.00002211
Iteration 227/1000 | Loss: 0.00002211
Iteration 228/1000 | Loss: 0.00002211
Iteration 229/1000 | Loss: 0.00002211
Iteration 230/1000 | Loss: 0.00002211
Iteration 231/1000 | Loss: 0.00002211
Iteration 232/1000 | Loss: 0.00002211
Iteration 233/1000 | Loss: 0.00002211
Iteration 234/1000 | Loss: 0.00002211
Iteration 235/1000 | Loss: 0.00002211
Iteration 236/1000 | Loss: 0.00002211
Iteration 237/1000 | Loss: 0.00002211
Iteration 238/1000 | Loss: 0.00002211
Iteration 239/1000 | Loss: 0.00002211
Iteration 240/1000 | Loss: 0.00002211
Iteration 241/1000 | Loss: 0.00002211
Iteration 242/1000 | Loss: 0.00002211
Iteration 243/1000 | Loss: 0.00002211
Iteration 244/1000 | Loss: 0.00002211
Iteration 245/1000 | Loss: 0.00002211
Iteration 246/1000 | Loss: 0.00002211
Iteration 247/1000 | Loss: 0.00002211
Iteration 248/1000 | Loss: 0.00002211
Iteration 249/1000 | Loss: 0.00002211
Iteration 250/1000 | Loss: 0.00002211
Iteration 251/1000 | Loss: 0.00002211
Iteration 252/1000 | Loss: 0.00002211
Iteration 253/1000 | Loss: 0.00002211
Iteration 254/1000 | Loss: 0.00002211
Iteration 255/1000 | Loss: 0.00002211
Iteration 256/1000 | Loss: 0.00002211
Iteration 257/1000 | Loss: 0.00002211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [2.2112288206699304e-05, 2.2112288206699304e-05, 2.2112288206699304e-05, 2.2112288206699304e-05, 2.2112288206699304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2112288206699304e-05

Optimization complete. Final v2v error: 3.942902088165283 mm

Highest mean error: 4.248307228088379 mm for frame 61

Lowest mean error: 3.7241530418395996 mm for frame 124

Saving results

Total time: 43.60663080215454
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480495
Iteration 2/25 | Loss: 0.00131045
Iteration 3/25 | Loss: 0.00123072
Iteration 4/25 | Loss: 0.00122085
Iteration 5/25 | Loss: 0.00121813
Iteration 6/25 | Loss: 0.00121803
Iteration 7/25 | Loss: 0.00121803
Iteration 8/25 | Loss: 0.00121803
Iteration 9/25 | Loss: 0.00121803
Iteration 10/25 | Loss: 0.00121803
Iteration 11/25 | Loss: 0.00121803
Iteration 12/25 | Loss: 0.00121803
Iteration 13/25 | Loss: 0.00121803
Iteration 14/25 | Loss: 0.00121803
Iteration 15/25 | Loss: 0.00121803
Iteration 16/25 | Loss: 0.00121803
Iteration 17/25 | Loss: 0.00121803
Iteration 18/25 | Loss: 0.00121803
Iteration 19/25 | Loss: 0.00121803
Iteration 20/25 | Loss: 0.00121803
Iteration 21/25 | Loss: 0.00121803
Iteration 22/25 | Loss: 0.00121803
Iteration 23/25 | Loss: 0.00121803
Iteration 24/25 | Loss: 0.00121803
Iteration 25/25 | Loss: 0.00121803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.59353590
Iteration 2/25 | Loss: 0.00102317
Iteration 3/25 | Loss: 0.00102317
Iteration 4/25 | Loss: 0.00102317
Iteration 5/25 | Loss: 0.00102317
Iteration 6/25 | Loss: 0.00102317
Iteration 7/25 | Loss: 0.00102317
Iteration 8/25 | Loss: 0.00102317
Iteration 9/25 | Loss: 0.00102317
Iteration 10/25 | Loss: 0.00102317
Iteration 11/25 | Loss: 0.00102317
Iteration 12/25 | Loss: 0.00102317
Iteration 13/25 | Loss: 0.00102317
Iteration 14/25 | Loss: 0.00102317
Iteration 15/25 | Loss: 0.00102317
Iteration 16/25 | Loss: 0.00102317
Iteration 17/25 | Loss: 0.00102317
Iteration 18/25 | Loss: 0.00102317
Iteration 19/25 | Loss: 0.00102317
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001023166230879724, 0.001023166230879724, 0.001023166230879724, 0.001023166230879724, 0.001023166230879724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001023166230879724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102317
Iteration 2/1000 | Loss: 0.00002873
Iteration 3/1000 | Loss: 0.00002001
Iteration 4/1000 | Loss: 0.00001731
Iteration 5/1000 | Loss: 0.00001577
Iteration 6/1000 | Loss: 0.00001504
Iteration 7/1000 | Loss: 0.00001443
Iteration 8/1000 | Loss: 0.00001390
Iteration 9/1000 | Loss: 0.00001349
Iteration 10/1000 | Loss: 0.00001325
Iteration 11/1000 | Loss: 0.00001300
Iteration 12/1000 | Loss: 0.00001281
Iteration 13/1000 | Loss: 0.00001266
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001254
Iteration 16/1000 | Loss: 0.00001249
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001248
Iteration 19/1000 | Loss: 0.00001246
Iteration 20/1000 | Loss: 0.00001245
Iteration 21/1000 | Loss: 0.00001244
Iteration 22/1000 | Loss: 0.00001239
Iteration 23/1000 | Loss: 0.00001235
Iteration 24/1000 | Loss: 0.00001234
Iteration 25/1000 | Loss: 0.00001234
Iteration 26/1000 | Loss: 0.00001233
Iteration 27/1000 | Loss: 0.00001233
Iteration 28/1000 | Loss: 0.00001231
Iteration 29/1000 | Loss: 0.00001231
Iteration 30/1000 | Loss: 0.00001230
Iteration 31/1000 | Loss: 0.00001230
Iteration 32/1000 | Loss: 0.00001225
Iteration 33/1000 | Loss: 0.00001224
Iteration 34/1000 | Loss: 0.00001222
Iteration 35/1000 | Loss: 0.00001221
Iteration 36/1000 | Loss: 0.00001220
Iteration 37/1000 | Loss: 0.00001219
Iteration 38/1000 | Loss: 0.00001218
Iteration 39/1000 | Loss: 0.00001218
Iteration 40/1000 | Loss: 0.00001217
Iteration 41/1000 | Loss: 0.00001217
Iteration 42/1000 | Loss: 0.00001216
Iteration 43/1000 | Loss: 0.00001216
Iteration 44/1000 | Loss: 0.00001215
Iteration 45/1000 | Loss: 0.00001215
Iteration 46/1000 | Loss: 0.00001214
Iteration 47/1000 | Loss: 0.00001214
Iteration 48/1000 | Loss: 0.00001213
Iteration 49/1000 | Loss: 0.00001213
Iteration 50/1000 | Loss: 0.00001212
Iteration 51/1000 | Loss: 0.00001211
Iteration 52/1000 | Loss: 0.00001206
Iteration 53/1000 | Loss: 0.00001204
Iteration 54/1000 | Loss: 0.00001203
Iteration 55/1000 | Loss: 0.00001202
Iteration 56/1000 | Loss: 0.00001201
Iteration 57/1000 | Loss: 0.00001199
Iteration 58/1000 | Loss: 0.00001199
Iteration 59/1000 | Loss: 0.00001198
Iteration 60/1000 | Loss: 0.00001198
Iteration 61/1000 | Loss: 0.00001197
Iteration 62/1000 | Loss: 0.00001197
Iteration 63/1000 | Loss: 0.00001197
Iteration 64/1000 | Loss: 0.00001196
Iteration 65/1000 | Loss: 0.00001196
Iteration 66/1000 | Loss: 0.00001196
Iteration 67/1000 | Loss: 0.00001195
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001194
Iteration 74/1000 | Loss: 0.00001194
Iteration 75/1000 | Loss: 0.00001194
Iteration 76/1000 | Loss: 0.00001194
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001193
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001193
Iteration 81/1000 | Loss: 0.00001193
Iteration 82/1000 | Loss: 0.00001192
Iteration 83/1000 | Loss: 0.00001192
Iteration 84/1000 | Loss: 0.00001192
Iteration 85/1000 | Loss: 0.00001192
Iteration 86/1000 | Loss: 0.00001191
Iteration 87/1000 | Loss: 0.00001191
Iteration 88/1000 | Loss: 0.00001191
Iteration 89/1000 | Loss: 0.00001191
Iteration 90/1000 | Loss: 0.00001190
Iteration 91/1000 | Loss: 0.00001190
Iteration 92/1000 | Loss: 0.00001190
Iteration 93/1000 | Loss: 0.00001190
Iteration 94/1000 | Loss: 0.00001190
Iteration 95/1000 | Loss: 0.00001189
Iteration 96/1000 | Loss: 0.00001189
Iteration 97/1000 | Loss: 0.00001189
Iteration 98/1000 | Loss: 0.00001189
Iteration 99/1000 | Loss: 0.00001189
Iteration 100/1000 | Loss: 0.00001189
Iteration 101/1000 | Loss: 0.00001189
Iteration 102/1000 | Loss: 0.00001189
Iteration 103/1000 | Loss: 0.00001188
Iteration 104/1000 | Loss: 0.00001188
Iteration 105/1000 | Loss: 0.00001188
Iteration 106/1000 | Loss: 0.00001188
Iteration 107/1000 | Loss: 0.00001188
Iteration 108/1000 | Loss: 0.00001187
Iteration 109/1000 | Loss: 0.00001187
Iteration 110/1000 | Loss: 0.00001187
Iteration 111/1000 | Loss: 0.00001187
Iteration 112/1000 | Loss: 0.00001187
Iteration 113/1000 | Loss: 0.00001187
Iteration 114/1000 | Loss: 0.00001187
Iteration 115/1000 | Loss: 0.00001187
Iteration 116/1000 | Loss: 0.00001187
Iteration 117/1000 | Loss: 0.00001187
Iteration 118/1000 | Loss: 0.00001186
Iteration 119/1000 | Loss: 0.00001186
Iteration 120/1000 | Loss: 0.00001186
Iteration 121/1000 | Loss: 0.00001186
Iteration 122/1000 | Loss: 0.00001186
Iteration 123/1000 | Loss: 0.00001186
Iteration 124/1000 | Loss: 0.00001186
Iteration 125/1000 | Loss: 0.00001186
Iteration 126/1000 | Loss: 0.00001186
Iteration 127/1000 | Loss: 0.00001186
Iteration 128/1000 | Loss: 0.00001186
Iteration 129/1000 | Loss: 0.00001186
Iteration 130/1000 | Loss: 0.00001186
Iteration 131/1000 | Loss: 0.00001185
Iteration 132/1000 | Loss: 0.00001185
Iteration 133/1000 | Loss: 0.00001185
Iteration 134/1000 | Loss: 0.00001185
Iteration 135/1000 | Loss: 0.00001185
Iteration 136/1000 | Loss: 0.00001185
Iteration 137/1000 | Loss: 0.00001185
Iteration 138/1000 | Loss: 0.00001184
Iteration 139/1000 | Loss: 0.00001184
Iteration 140/1000 | Loss: 0.00001183
Iteration 141/1000 | Loss: 0.00001183
Iteration 142/1000 | Loss: 0.00001183
Iteration 143/1000 | Loss: 0.00001183
Iteration 144/1000 | Loss: 0.00001183
Iteration 145/1000 | Loss: 0.00001183
Iteration 146/1000 | Loss: 0.00001183
Iteration 147/1000 | Loss: 0.00001182
Iteration 148/1000 | Loss: 0.00001182
Iteration 149/1000 | Loss: 0.00001182
Iteration 150/1000 | Loss: 0.00001182
Iteration 151/1000 | Loss: 0.00001182
Iteration 152/1000 | Loss: 0.00001182
Iteration 153/1000 | Loss: 0.00001182
Iteration 154/1000 | Loss: 0.00001182
Iteration 155/1000 | Loss: 0.00001182
Iteration 156/1000 | Loss: 0.00001182
Iteration 157/1000 | Loss: 0.00001182
Iteration 158/1000 | Loss: 0.00001182
Iteration 159/1000 | Loss: 0.00001181
Iteration 160/1000 | Loss: 0.00001181
Iteration 161/1000 | Loss: 0.00001181
Iteration 162/1000 | Loss: 0.00001181
Iteration 163/1000 | Loss: 0.00001181
Iteration 164/1000 | Loss: 0.00001181
Iteration 165/1000 | Loss: 0.00001181
Iteration 166/1000 | Loss: 0.00001181
Iteration 167/1000 | Loss: 0.00001181
Iteration 168/1000 | Loss: 0.00001181
Iteration 169/1000 | Loss: 0.00001181
Iteration 170/1000 | Loss: 0.00001181
Iteration 171/1000 | Loss: 0.00001180
Iteration 172/1000 | Loss: 0.00001180
Iteration 173/1000 | Loss: 0.00001180
Iteration 174/1000 | Loss: 0.00001180
Iteration 175/1000 | Loss: 0.00001180
Iteration 176/1000 | Loss: 0.00001180
Iteration 177/1000 | Loss: 0.00001180
Iteration 178/1000 | Loss: 0.00001180
Iteration 179/1000 | Loss: 0.00001180
Iteration 180/1000 | Loss: 0.00001180
Iteration 181/1000 | Loss: 0.00001180
Iteration 182/1000 | Loss: 0.00001180
Iteration 183/1000 | Loss: 0.00001180
Iteration 184/1000 | Loss: 0.00001180
Iteration 185/1000 | Loss: 0.00001179
Iteration 186/1000 | Loss: 0.00001179
Iteration 187/1000 | Loss: 0.00001179
Iteration 188/1000 | Loss: 0.00001179
Iteration 189/1000 | Loss: 0.00001179
Iteration 190/1000 | Loss: 0.00001179
Iteration 191/1000 | Loss: 0.00001179
Iteration 192/1000 | Loss: 0.00001179
Iteration 193/1000 | Loss: 0.00001179
Iteration 194/1000 | Loss: 0.00001179
Iteration 195/1000 | Loss: 0.00001178
Iteration 196/1000 | Loss: 0.00001178
Iteration 197/1000 | Loss: 0.00001178
Iteration 198/1000 | Loss: 0.00001178
Iteration 199/1000 | Loss: 0.00001178
Iteration 200/1000 | Loss: 0.00001178
Iteration 201/1000 | Loss: 0.00001178
Iteration 202/1000 | Loss: 0.00001178
Iteration 203/1000 | Loss: 0.00001178
Iteration 204/1000 | Loss: 0.00001178
Iteration 205/1000 | Loss: 0.00001178
Iteration 206/1000 | Loss: 0.00001178
Iteration 207/1000 | Loss: 0.00001178
Iteration 208/1000 | Loss: 0.00001178
Iteration 209/1000 | Loss: 0.00001178
Iteration 210/1000 | Loss: 0.00001178
Iteration 211/1000 | Loss: 0.00001177
Iteration 212/1000 | Loss: 0.00001177
Iteration 213/1000 | Loss: 0.00001177
Iteration 214/1000 | Loss: 0.00001177
Iteration 215/1000 | Loss: 0.00001177
Iteration 216/1000 | Loss: 0.00001177
Iteration 217/1000 | Loss: 0.00001177
Iteration 218/1000 | Loss: 0.00001177
Iteration 219/1000 | Loss: 0.00001177
Iteration 220/1000 | Loss: 0.00001177
Iteration 221/1000 | Loss: 0.00001177
Iteration 222/1000 | Loss: 0.00001177
Iteration 223/1000 | Loss: 0.00001177
Iteration 224/1000 | Loss: 0.00001177
Iteration 225/1000 | Loss: 0.00001177
Iteration 226/1000 | Loss: 0.00001177
Iteration 227/1000 | Loss: 0.00001177
Iteration 228/1000 | Loss: 0.00001176
Iteration 229/1000 | Loss: 0.00001176
Iteration 230/1000 | Loss: 0.00001176
Iteration 231/1000 | Loss: 0.00001176
Iteration 232/1000 | Loss: 0.00001176
Iteration 233/1000 | Loss: 0.00001176
Iteration 234/1000 | Loss: 0.00001176
Iteration 235/1000 | Loss: 0.00001176
Iteration 236/1000 | Loss: 0.00001176
Iteration 237/1000 | Loss: 0.00001176
Iteration 238/1000 | Loss: 0.00001176
Iteration 239/1000 | Loss: 0.00001176
Iteration 240/1000 | Loss: 0.00001176
Iteration 241/1000 | Loss: 0.00001176
Iteration 242/1000 | Loss: 0.00001176
Iteration 243/1000 | Loss: 0.00001176
Iteration 244/1000 | Loss: 0.00001176
Iteration 245/1000 | Loss: 0.00001176
Iteration 246/1000 | Loss: 0.00001176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.1764078408305068e-05, 1.1764078408305068e-05, 1.1764078408305068e-05, 1.1764078408305068e-05, 1.1764078408305068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1764078408305068e-05

Optimization complete. Final v2v error: 2.947739362716675 mm

Highest mean error: 3.3876025676727295 mm for frame 62

Lowest mean error: 2.6843297481536865 mm for frame 133

Saving results

Total time: 46.55639147758484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807369
Iteration 2/25 | Loss: 0.00167597
Iteration 3/25 | Loss: 0.00145990
Iteration 4/25 | Loss: 0.00140346
Iteration 5/25 | Loss: 0.00138775
Iteration 6/25 | Loss: 0.00138725
Iteration 7/25 | Loss: 0.00137949
Iteration 8/25 | Loss: 0.00137515
Iteration 9/25 | Loss: 0.00136651
Iteration 10/25 | Loss: 0.00141109
Iteration 11/25 | Loss: 0.00136113
Iteration 12/25 | Loss: 0.00137199
Iteration 13/25 | Loss: 0.00137219
Iteration 14/25 | Loss: 0.00136014
Iteration 15/25 | Loss: 0.00135521
Iteration 16/25 | Loss: 0.00135416
Iteration 17/25 | Loss: 0.00135306
Iteration 18/25 | Loss: 0.00135784
Iteration 19/25 | Loss: 0.00135832
Iteration 20/25 | Loss: 0.00135620
Iteration 21/25 | Loss: 0.00135518
Iteration 22/25 | Loss: 0.00135472
Iteration 23/25 | Loss: 0.00135600
Iteration 24/25 | Loss: 0.00135496
Iteration 25/25 | Loss: 0.00135854

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.45516253
Iteration 2/25 | Loss: 0.00182979
Iteration 3/25 | Loss: 0.00182953
Iteration 4/25 | Loss: 0.00182952
Iteration 5/25 | Loss: 0.00182952
Iteration 6/25 | Loss: 0.00182952
Iteration 7/25 | Loss: 0.00182952
Iteration 8/25 | Loss: 0.00182952
Iteration 9/25 | Loss: 0.00182952
Iteration 10/25 | Loss: 0.00182952
Iteration 11/25 | Loss: 0.00182952
Iteration 12/25 | Loss: 0.00182952
Iteration 13/25 | Loss: 0.00182952
Iteration 14/25 | Loss: 0.00182952
Iteration 15/25 | Loss: 0.00182952
Iteration 16/25 | Loss: 0.00182952
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0018295204499736428, 0.0018295204499736428, 0.0018295204499736428, 0.0018295204499736428, 0.0018295204499736428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018295204499736428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182952
Iteration 2/1000 | Loss: 0.00020607
Iteration 3/1000 | Loss: 0.00010761
Iteration 4/1000 | Loss: 0.00025004
Iteration 5/1000 | Loss: 0.00013020
Iteration 6/1000 | Loss: 0.00013201
Iteration 7/1000 | Loss: 0.00026654
Iteration 8/1000 | Loss: 0.00049385
Iteration 9/1000 | Loss: 0.00027003
Iteration 10/1000 | Loss: 0.00008411
Iteration 11/1000 | Loss: 0.00007224
Iteration 12/1000 | Loss: 0.00006696
Iteration 13/1000 | Loss: 0.00006378
Iteration 14/1000 | Loss: 0.00006174
Iteration 15/1000 | Loss: 0.00005988
Iteration 16/1000 | Loss: 0.00005876
Iteration 17/1000 | Loss: 0.00005772
Iteration 18/1000 | Loss: 0.00005671
Iteration 19/1000 | Loss: 0.00005586
Iteration 20/1000 | Loss: 0.00005505
Iteration 21/1000 | Loss: 0.00072742
Iteration 22/1000 | Loss: 0.00165272
Iteration 23/1000 | Loss: 0.00182987
Iteration 24/1000 | Loss: 0.00209329
Iteration 25/1000 | Loss: 0.00184819
Iteration 26/1000 | Loss: 0.00033221
Iteration 27/1000 | Loss: 0.00034243
Iteration 28/1000 | Loss: 0.00009608
Iteration 29/1000 | Loss: 0.00008023
Iteration 30/1000 | Loss: 0.00005512
Iteration 31/1000 | Loss: 0.00005107
Iteration 32/1000 | Loss: 0.00004913
Iteration 33/1000 | Loss: 0.00004780
Iteration 34/1000 | Loss: 0.00030591
Iteration 35/1000 | Loss: 0.00058425
Iteration 36/1000 | Loss: 0.00012520
Iteration 37/1000 | Loss: 0.00011201
Iteration 38/1000 | Loss: 0.00004201
Iteration 39/1000 | Loss: 0.00004000
Iteration 40/1000 | Loss: 0.00003805
Iteration 41/1000 | Loss: 0.00003634
Iteration 42/1000 | Loss: 0.00003536
Iteration 43/1000 | Loss: 0.00003465
Iteration 44/1000 | Loss: 0.00003419
Iteration 45/1000 | Loss: 0.00003378
Iteration 46/1000 | Loss: 0.00003340
Iteration 47/1000 | Loss: 0.00003315
Iteration 48/1000 | Loss: 0.00003292
Iteration 49/1000 | Loss: 0.00003290
Iteration 50/1000 | Loss: 0.00003273
Iteration 51/1000 | Loss: 0.00003258
Iteration 52/1000 | Loss: 0.00003248
Iteration 53/1000 | Loss: 0.00003246
Iteration 54/1000 | Loss: 0.00003232
Iteration 55/1000 | Loss: 0.00017685
Iteration 56/1000 | Loss: 0.00003177
Iteration 57/1000 | Loss: 0.00003102
Iteration 58/1000 | Loss: 0.00003048
Iteration 59/1000 | Loss: 0.00002992
Iteration 60/1000 | Loss: 0.00002946
Iteration 61/1000 | Loss: 0.00002925
Iteration 62/1000 | Loss: 0.00002911
Iteration 63/1000 | Loss: 0.00002903
Iteration 64/1000 | Loss: 0.00002901
Iteration 65/1000 | Loss: 0.00002900
Iteration 66/1000 | Loss: 0.00002893
Iteration 67/1000 | Loss: 0.00002892
Iteration 68/1000 | Loss: 0.00002891
Iteration 69/1000 | Loss: 0.00002890
Iteration 70/1000 | Loss: 0.00002888
Iteration 71/1000 | Loss: 0.00002888
Iteration 72/1000 | Loss: 0.00002886
Iteration 73/1000 | Loss: 0.00002885
Iteration 74/1000 | Loss: 0.00002883
Iteration 75/1000 | Loss: 0.00002883
Iteration 76/1000 | Loss: 0.00002883
Iteration 77/1000 | Loss: 0.00002883
Iteration 78/1000 | Loss: 0.00002883
Iteration 79/1000 | Loss: 0.00002883
Iteration 80/1000 | Loss: 0.00002883
Iteration 81/1000 | Loss: 0.00002883
Iteration 82/1000 | Loss: 0.00002883
Iteration 83/1000 | Loss: 0.00002883
Iteration 84/1000 | Loss: 0.00002882
Iteration 85/1000 | Loss: 0.00002882
Iteration 86/1000 | Loss: 0.00002882
Iteration 87/1000 | Loss: 0.00002882
Iteration 88/1000 | Loss: 0.00002882
Iteration 89/1000 | Loss: 0.00002882
Iteration 90/1000 | Loss: 0.00002882
Iteration 91/1000 | Loss: 0.00002882
Iteration 92/1000 | Loss: 0.00002882
Iteration 93/1000 | Loss: 0.00002882
Iteration 94/1000 | Loss: 0.00002882
Iteration 95/1000 | Loss: 0.00002882
Iteration 96/1000 | Loss: 0.00002881
Iteration 97/1000 | Loss: 0.00002880
Iteration 98/1000 | Loss: 0.00002880
Iteration 99/1000 | Loss: 0.00002879
Iteration 100/1000 | Loss: 0.00002879
Iteration 101/1000 | Loss: 0.00002879
Iteration 102/1000 | Loss: 0.00002879
Iteration 103/1000 | Loss: 0.00002879
Iteration 104/1000 | Loss: 0.00002878
Iteration 105/1000 | Loss: 0.00002878
Iteration 106/1000 | Loss: 0.00002878
Iteration 107/1000 | Loss: 0.00002878
Iteration 108/1000 | Loss: 0.00002878
Iteration 109/1000 | Loss: 0.00002878
Iteration 110/1000 | Loss: 0.00002878
Iteration 111/1000 | Loss: 0.00002878
Iteration 112/1000 | Loss: 0.00002878
Iteration 113/1000 | Loss: 0.00002878
Iteration 114/1000 | Loss: 0.00002878
Iteration 115/1000 | Loss: 0.00002878
Iteration 116/1000 | Loss: 0.00002878
Iteration 117/1000 | Loss: 0.00002878
Iteration 118/1000 | Loss: 0.00002878
Iteration 119/1000 | Loss: 0.00002878
Iteration 120/1000 | Loss: 0.00002878
Iteration 121/1000 | Loss: 0.00002878
Iteration 122/1000 | Loss: 0.00002878
Iteration 123/1000 | Loss: 0.00002878
Iteration 124/1000 | Loss: 0.00002878
Iteration 125/1000 | Loss: 0.00002878
Iteration 126/1000 | Loss: 0.00002878
Iteration 127/1000 | Loss: 0.00002878
Iteration 128/1000 | Loss: 0.00002878
Iteration 129/1000 | Loss: 0.00002878
Iteration 130/1000 | Loss: 0.00002878
Iteration 131/1000 | Loss: 0.00002878
Iteration 132/1000 | Loss: 0.00002878
Iteration 133/1000 | Loss: 0.00002878
Iteration 134/1000 | Loss: 0.00002878
Iteration 135/1000 | Loss: 0.00002878
Iteration 136/1000 | Loss: 0.00002878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.8777678380720317e-05, 2.8777678380720317e-05, 2.8777678380720317e-05, 2.8777678380720317e-05, 2.8777678380720317e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8777678380720317e-05

Optimization complete. Final v2v error: 4.289376258850098 mm

Highest mean error: 7.073512554168701 mm for frame 56

Lowest mean error: 2.8788881301879883 mm for frame 102

Saving results

Total time: 134.2847921848297
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449259
Iteration 2/25 | Loss: 0.00148481
Iteration 3/25 | Loss: 0.00131635
Iteration 4/25 | Loss: 0.00129048
Iteration 5/25 | Loss: 0.00128225
Iteration 6/25 | Loss: 0.00128081
Iteration 7/25 | Loss: 0.00128021
Iteration 8/25 | Loss: 0.00128021
Iteration 9/25 | Loss: 0.00128021
Iteration 10/25 | Loss: 0.00128021
Iteration 11/25 | Loss: 0.00128021
Iteration 12/25 | Loss: 0.00128021
Iteration 13/25 | Loss: 0.00128021
Iteration 14/25 | Loss: 0.00128021
Iteration 15/25 | Loss: 0.00128021
Iteration 16/25 | Loss: 0.00128021
Iteration 17/25 | Loss: 0.00128021
Iteration 18/25 | Loss: 0.00128021
Iteration 19/25 | Loss: 0.00128021
Iteration 20/25 | Loss: 0.00128021
Iteration 21/25 | Loss: 0.00128021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012802121927961707, 0.0012802121927961707, 0.0012802121927961707, 0.0012802121927961707, 0.0012802121927961707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012802121927961707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17405653
Iteration 2/25 | Loss: 0.00129808
Iteration 3/25 | Loss: 0.00129806
Iteration 4/25 | Loss: 0.00129806
Iteration 5/25 | Loss: 0.00129806
Iteration 6/25 | Loss: 0.00129806
Iteration 7/25 | Loss: 0.00129806
Iteration 8/25 | Loss: 0.00129806
Iteration 9/25 | Loss: 0.00129806
Iteration 10/25 | Loss: 0.00129806
Iteration 11/25 | Loss: 0.00129806
Iteration 12/25 | Loss: 0.00129805
Iteration 13/25 | Loss: 0.00129806
Iteration 14/25 | Loss: 0.00129806
Iteration 15/25 | Loss: 0.00129806
Iteration 16/25 | Loss: 0.00129806
Iteration 17/25 | Loss: 0.00129805
Iteration 18/25 | Loss: 0.00129805
Iteration 19/25 | Loss: 0.00129805
Iteration 20/25 | Loss: 0.00129805
Iteration 21/25 | Loss: 0.00129805
Iteration 22/25 | Loss: 0.00129805
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012980548199266195, 0.0012980548199266195, 0.0012980548199266195, 0.0012980548199266195, 0.0012980548199266195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012980548199266195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129805
Iteration 2/1000 | Loss: 0.00007255
Iteration 3/1000 | Loss: 0.00004221
Iteration 4/1000 | Loss: 0.00003318
Iteration 5/1000 | Loss: 0.00003077
Iteration 6/1000 | Loss: 0.00002907
Iteration 7/1000 | Loss: 0.00002796
Iteration 8/1000 | Loss: 0.00002726
Iteration 9/1000 | Loss: 0.00002669
Iteration 10/1000 | Loss: 0.00002615
Iteration 11/1000 | Loss: 0.00002586
Iteration 12/1000 | Loss: 0.00002564
Iteration 13/1000 | Loss: 0.00002542
Iteration 14/1000 | Loss: 0.00002538
Iteration 15/1000 | Loss: 0.00002535
Iteration 16/1000 | Loss: 0.00002525
Iteration 17/1000 | Loss: 0.00002523
Iteration 18/1000 | Loss: 0.00002521
Iteration 19/1000 | Loss: 0.00002508
Iteration 20/1000 | Loss: 0.00002503
Iteration 21/1000 | Loss: 0.00002499
Iteration 22/1000 | Loss: 0.00002498
Iteration 23/1000 | Loss: 0.00002498
Iteration 24/1000 | Loss: 0.00002497
Iteration 25/1000 | Loss: 0.00002493
Iteration 26/1000 | Loss: 0.00002490
Iteration 27/1000 | Loss: 0.00002479
Iteration 28/1000 | Loss: 0.00002472
Iteration 29/1000 | Loss: 0.00002471
Iteration 30/1000 | Loss: 0.00002469
Iteration 31/1000 | Loss: 0.00002463
Iteration 32/1000 | Loss: 0.00002462
Iteration 33/1000 | Loss: 0.00002462
Iteration 34/1000 | Loss: 0.00002454
Iteration 35/1000 | Loss: 0.00002453
Iteration 36/1000 | Loss: 0.00002453
Iteration 37/1000 | Loss: 0.00002453
Iteration 38/1000 | Loss: 0.00002451
Iteration 39/1000 | Loss: 0.00002451
Iteration 40/1000 | Loss: 0.00002450
Iteration 41/1000 | Loss: 0.00002450
Iteration 42/1000 | Loss: 0.00002449
Iteration 43/1000 | Loss: 0.00002448
Iteration 44/1000 | Loss: 0.00002448
Iteration 45/1000 | Loss: 0.00002448
Iteration 46/1000 | Loss: 0.00002447
Iteration 47/1000 | Loss: 0.00002447
Iteration 48/1000 | Loss: 0.00002446
Iteration 49/1000 | Loss: 0.00002446
Iteration 50/1000 | Loss: 0.00002446
Iteration 51/1000 | Loss: 0.00002445
Iteration 52/1000 | Loss: 0.00002444
Iteration 53/1000 | Loss: 0.00002444
Iteration 54/1000 | Loss: 0.00002443
Iteration 55/1000 | Loss: 0.00002443
Iteration 56/1000 | Loss: 0.00002442
Iteration 57/1000 | Loss: 0.00002442
Iteration 58/1000 | Loss: 0.00002442
Iteration 59/1000 | Loss: 0.00002441
Iteration 60/1000 | Loss: 0.00002441
Iteration 61/1000 | Loss: 0.00002441
Iteration 62/1000 | Loss: 0.00002439
Iteration 63/1000 | Loss: 0.00002439
Iteration 64/1000 | Loss: 0.00002439
Iteration 65/1000 | Loss: 0.00002439
Iteration 66/1000 | Loss: 0.00002439
Iteration 67/1000 | Loss: 0.00002439
Iteration 68/1000 | Loss: 0.00002439
Iteration 69/1000 | Loss: 0.00002439
Iteration 70/1000 | Loss: 0.00002439
Iteration 71/1000 | Loss: 0.00002438
Iteration 72/1000 | Loss: 0.00002438
Iteration 73/1000 | Loss: 0.00002438
Iteration 74/1000 | Loss: 0.00002438
Iteration 75/1000 | Loss: 0.00002437
Iteration 76/1000 | Loss: 0.00002437
Iteration 77/1000 | Loss: 0.00002437
Iteration 78/1000 | Loss: 0.00002436
Iteration 79/1000 | Loss: 0.00002436
Iteration 80/1000 | Loss: 0.00002435
Iteration 81/1000 | Loss: 0.00002435
Iteration 82/1000 | Loss: 0.00002435
Iteration 83/1000 | Loss: 0.00002435
Iteration 84/1000 | Loss: 0.00002435
Iteration 85/1000 | Loss: 0.00002435
Iteration 86/1000 | Loss: 0.00002435
Iteration 87/1000 | Loss: 0.00002435
Iteration 88/1000 | Loss: 0.00002435
Iteration 89/1000 | Loss: 0.00002434
Iteration 90/1000 | Loss: 0.00002434
Iteration 91/1000 | Loss: 0.00002434
Iteration 92/1000 | Loss: 0.00002434
Iteration 93/1000 | Loss: 0.00002434
Iteration 94/1000 | Loss: 0.00002434
Iteration 95/1000 | Loss: 0.00002433
Iteration 96/1000 | Loss: 0.00002433
Iteration 97/1000 | Loss: 0.00002433
Iteration 98/1000 | Loss: 0.00002433
Iteration 99/1000 | Loss: 0.00002433
Iteration 100/1000 | Loss: 0.00002433
Iteration 101/1000 | Loss: 0.00002433
Iteration 102/1000 | Loss: 0.00002433
Iteration 103/1000 | Loss: 0.00002433
Iteration 104/1000 | Loss: 0.00002433
Iteration 105/1000 | Loss: 0.00002433
Iteration 106/1000 | Loss: 0.00002433
Iteration 107/1000 | Loss: 0.00002432
Iteration 108/1000 | Loss: 0.00002432
Iteration 109/1000 | Loss: 0.00002432
Iteration 110/1000 | Loss: 0.00002432
Iteration 111/1000 | Loss: 0.00002432
Iteration 112/1000 | Loss: 0.00002432
Iteration 113/1000 | Loss: 0.00002431
Iteration 114/1000 | Loss: 0.00002431
Iteration 115/1000 | Loss: 0.00002431
Iteration 116/1000 | Loss: 0.00002431
Iteration 117/1000 | Loss: 0.00002431
Iteration 118/1000 | Loss: 0.00002431
Iteration 119/1000 | Loss: 0.00002431
Iteration 120/1000 | Loss: 0.00002431
Iteration 121/1000 | Loss: 0.00002430
Iteration 122/1000 | Loss: 0.00002430
Iteration 123/1000 | Loss: 0.00002430
Iteration 124/1000 | Loss: 0.00002430
Iteration 125/1000 | Loss: 0.00002430
Iteration 126/1000 | Loss: 0.00002430
Iteration 127/1000 | Loss: 0.00002429
Iteration 128/1000 | Loss: 0.00002429
Iteration 129/1000 | Loss: 0.00002429
Iteration 130/1000 | Loss: 0.00002429
Iteration 131/1000 | Loss: 0.00002429
Iteration 132/1000 | Loss: 0.00002428
Iteration 133/1000 | Loss: 0.00002428
Iteration 134/1000 | Loss: 0.00002428
Iteration 135/1000 | Loss: 0.00002428
Iteration 136/1000 | Loss: 0.00002428
Iteration 137/1000 | Loss: 0.00002428
Iteration 138/1000 | Loss: 0.00002428
Iteration 139/1000 | Loss: 0.00002428
Iteration 140/1000 | Loss: 0.00002428
Iteration 141/1000 | Loss: 0.00002427
Iteration 142/1000 | Loss: 0.00002427
Iteration 143/1000 | Loss: 0.00002427
Iteration 144/1000 | Loss: 0.00002427
Iteration 145/1000 | Loss: 0.00002427
Iteration 146/1000 | Loss: 0.00002427
Iteration 147/1000 | Loss: 0.00002427
Iteration 148/1000 | Loss: 0.00002427
Iteration 149/1000 | Loss: 0.00002427
Iteration 150/1000 | Loss: 0.00002427
Iteration 151/1000 | Loss: 0.00002427
Iteration 152/1000 | Loss: 0.00002427
Iteration 153/1000 | Loss: 0.00002427
Iteration 154/1000 | Loss: 0.00002426
Iteration 155/1000 | Loss: 0.00002426
Iteration 156/1000 | Loss: 0.00002426
Iteration 157/1000 | Loss: 0.00002426
Iteration 158/1000 | Loss: 0.00002426
Iteration 159/1000 | Loss: 0.00002426
Iteration 160/1000 | Loss: 0.00002426
Iteration 161/1000 | Loss: 0.00002426
Iteration 162/1000 | Loss: 0.00002426
Iteration 163/1000 | Loss: 0.00002426
Iteration 164/1000 | Loss: 0.00002426
Iteration 165/1000 | Loss: 0.00002426
Iteration 166/1000 | Loss: 0.00002426
Iteration 167/1000 | Loss: 0.00002426
Iteration 168/1000 | Loss: 0.00002426
Iteration 169/1000 | Loss: 0.00002426
Iteration 170/1000 | Loss: 0.00002426
Iteration 171/1000 | Loss: 0.00002426
Iteration 172/1000 | Loss: 0.00002426
Iteration 173/1000 | Loss: 0.00002426
Iteration 174/1000 | Loss: 0.00002426
Iteration 175/1000 | Loss: 0.00002426
Iteration 176/1000 | Loss: 0.00002426
Iteration 177/1000 | Loss: 0.00002426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.4259348720079288e-05, 2.4259348720079288e-05, 2.4259348720079288e-05, 2.4259348720079288e-05, 2.4259348720079288e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4259348720079288e-05

Optimization complete. Final v2v error: 3.9740521907806396 mm

Highest mean error: 5.843246936798096 mm for frame 82

Lowest mean error: 3.1570003032684326 mm for frame 49

Saving results

Total time: 48.02164268493652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401263
Iteration 2/25 | Loss: 0.00125990
Iteration 3/25 | Loss: 0.00120472
Iteration 4/25 | Loss: 0.00119597
Iteration 5/25 | Loss: 0.00119354
Iteration 6/25 | Loss: 0.00119321
Iteration 7/25 | Loss: 0.00119321
Iteration 8/25 | Loss: 0.00119321
Iteration 9/25 | Loss: 0.00119321
Iteration 10/25 | Loss: 0.00119321
Iteration 11/25 | Loss: 0.00119321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011932067573070526, 0.0011932067573070526, 0.0011932067573070526, 0.0011932067573070526, 0.0011932067573070526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011932067573070526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.51590419
Iteration 2/25 | Loss: 0.00097996
Iteration 3/25 | Loss: 0.00097996
Iteration 4/25 | Loss: 0.00097996
Iteration 5/25 | Loss: 0.00097996
Iteration 6/25 | Loss: 0.00097996
Iteration 7/25 | Loss: 0.00097996
Iteration 8/25 | Loss: 0.00097996
Iteration 9/25 | Loss: 0.00097996
Iteration 10/25 | Loss: 0.00097996
Iteration 11/25 | Loss: 0.00097996
Iteration 12/25 | Loss: 0.00097996
Iteration 13/25 | Loss: 0.00097996
Iteration 14/25 | Loss: 0.00097996
Iteration 15/25 | Loss: 0.00097996
Iteration 16/25 | Loss: 0.00097996
Iteration 17/25 | Loss: 0.00097996
Iteration 18/25 | Loss: 0.00097996
Iteration 19/25 | Loss: 0.00097996
Iteration 20/25 | Loss: 0.00097996
Iteration 21/25 | Loss: 0.00097996
Iteration 22/25 | Loss: 0.00097996
Iteration 23/25 | Loss: 0.00097996
Iteration 24/25 | Loss: 0.00097996
Iteration 25/25 | Loss: 0.00097996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097996
Iteration 2/1000 | Loss: 0.00002136
Iteration 3/1000 | Loss: 0.00001596
Iteration 4/1000 | Loss: 0.00001423
Iteration 5/1000 | Loss: 0.00001341
Iteration 6/1000 | Loss: 0.00001287
Iteration 7/1000 | Loss: 0.00001249
Iteration 8/1000 | Loss: 0.00001222
Iteration 9/1000 | Loss: 0.00001193
Iteration 10/1000 | Loss: 0.00001174
Iteration 11/1000 | Loss: 0.00001169
Iteration 12/1000 | Loss: 0.00001165
Iteration 13/1000 | Loss: 0.00001152
Iteration 14/1000 | Loss: 0.00001144
Iteration 15/1000 | Loss: 0.00001140
Iteration 16/1000 | Loss: 0.00001139
Iteration 17/1000 | Loss: 0.00001139
Iteration 18/1000 | Loss: 0.00001139
Iteration 19/1000 | Loss: 0.00001137
Iteration 20/1000 | Loss: 0.00001135
Iteration 21/1000 | Loss: 0.00001132
Iteration 22/1000 | Loss: 0.00001132
Iteration 23/1000 | Loss: 0.00001131
Iteration 24/1000 | Loss: 0.00001129
Iteration 25/1000 | Loss: 0.00001129
Iteration 26/1000 | Loss: 0.00001129
Iteration 27/1000 | Loss: 0.00001129
Iteration 28/1000 | Loss: 0.00001128
Iteration 29/1000 | Loss: 0.00001127
Iteration 30/1000 | Loss: 0.00001126
Iteration 31/1000 | Loss: 0.00001124
Iteration 32/1000 | Loss: 0.00001124
Iteration 33/1000 | Loss: 0.00001124
Iteration 34/1000 | Loss: 0.00001123
Iteration 35/1000 | Loss: 0.00001122
Iteration 36/1000 | Loss: 0.00001118
Iteration 37/1000 | Loss: 0.00001118
Iteration 38/1000 | Loss: 0.00001115
Iteration 39/1000 | Loss: 0.00001114
Iteration 40/1000 | Loss: 0.00001114
Iteration 41/1000 | Loss: 0.00001114
Iteration 42/1000 | Loss: 0.00001113
Iteration 43/1000 | Loss: 0.00001113
Iteration 44/1000 | Loss: 0.00001112
Iteration 45/1000 | Loss: 0.00001111
Iteration 46/1000 | Loss: 0.00001111
Iteration 47/1000 | Loss: 0.00001110
Iteration 48/1000 | Loss: 0.00001110
Iteration 49/1000 | Loss: 0.00001109
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001109
Iteration 52/1000 | Loss: 0.00001108
Iteration 53/1000 | Loss: 0.00001108
Iteration 54/1000 | Loss: 0.00001108
Iteration 55/1000 | Loss: 0.00001107
Iteration 56/1000 | Loss: 0.00001107
Iteration 57/1000 | Loss: 0.00001107
Iteration 58/1000 | Loss: 0.00001107
Iteration 59/1000 | Loss: 0.00001107
Iteration 60/1000 | Loss: 0.00001107
Iteration 61/1000 | Loss: 0.00001106
Iteration 62/1000 | Loss: 0.00001106
Iteration 63/1000 | Loss: 0.00001105
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001104
Iteration 67/1000 | Loss: 0.00001103
Iteration 68/1000 | Loss: 0.00001103
Iteration 69/1000 | Loss: 0.00001102
Iteration 70/1000 | Loss: 0.00001102
Iteration 71/1000 | Loss: 0.00001102
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001101
Iteration 74/1000 | Loss: 0.00001100
Iteration 75/1000 | Loss: 0.00001100
Iteration 76/1000 | Loss: 0.00001100
Iteration 77/1000 | Loss: 0.00001100
Iteration 78/1000 | Loss: 0.00001100
Iteration 79/1000 | Loss: 0.00001099
Iteration 80/1000 | Loss: 0.00001099
Iteration 81/1000 | Loss: 0.00001099
Iteration 82/1000 | Loss: 0.00001098
Iteration 83/1000 | Loss: 0.00001097
Iteration 84/1000 | Loss: 0.00001096
Iteration 85/1000 | Loss: 0.00001096
Iteration 86/1000 | Loss: 0.00001096
Iteration 87/1000 | Loss: 0.00001096
Iteration 88/1000 | Loss: 0.00001096
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001096
Iteration 91/1000 | Loss: 0.00001096
Iteration 92/1000 | Loss: 0.00001095
Iteration 93/1000 | Loss: 0.00001095
Iteration 94/1000 | Loss: 0.00001095
Iteration 95/1000 | Loss: 0.00001095
Iteration 96/1000 | Loss: 0.00001095
Iteration 97/1000 | Loss: 0.00001095
Iteration 98/1000 | Loss: 0.00001095
Iteration 99/1000 | Loss: 0.00001095
Iteration 100/1000 | Loss: 0.00001095
Iteration 101/1000 | Loss: 0.00001094
Iteration 102/1000 | Loss: 0.00001094
Iteration 103/1000 | Loss: 0.00001094
Iteration 104/1000 | Loss: 0.00001094
Iteration 105/1000 | Loss: 0.00001094
Iteration 106/1000 | Loss: 0.00001093
Iteration 107/1000 | Loss: 0.00001093
Iteration 108/1000 | Loss: 0.00001093
Iteration 109/1000 | Loss: 0.00001093
Iteration 110/1000 | Loss: 0.00001093
Iteration 111/1000 | Loss: 0.00001092
Iteration 112/1000 | Loss: 0.00001092
Iteration 113/1000 | Loss: 0.00001092
Iteration 114/1000 | Loss: 0.00001092
Iteration 115/1000 | Loss: 0.00001092
Iteration 116/1000 | Loss: 0.00001092
Iteration 117/1000 | Loss: 0.00001092
Iteration 118/1000 | Loss: 0.00001092
Iteration 119/1000 | Loss: 0.00001092
Iteration 120/1000 | Loss: 0.00001091
Iteration 121/1000 | Loss: 0.00001091
Iteration 122/1000 | Loss: 0.00001091
Iteration 123/1000 | Loss: 0.00001091
Iteration 124/1000 | Loss: 0.00001091
Iteration 125/1000 | Loss: 0.00001091
Iteration 126/1000 | Loss: 0.00001091
Iteration 127/1000 | Loss: 0.00001090
Iteration 128/1000 | Loss: 0.00001090
Iteration 129/1000 | Loss: 0.00001090
Iteration 130/1000 | Loss: 0.00001090
Iteration 131/1000 | Loss: 0.00001090
Iteration 132/1000 | Loss: 0.00001090
Iteration 133/1000 | Loss: 0.00001090
Iteration 134/1000 | Loss: 0.00001090
Iteration 135/1000 | Loss: 0.00001090
Iteration 136/1000 | Loss: 0.00001090
Iteration 137/1000 | Loss: 0.00001090
Iteration 138/1000 | Loss: 0.00001090
Iteration 139/1000 | Loss: 0.00001090
Iteration 140/1000 | Loss: 0.00001090
Iteration 141/1000 | Loss: 0.00001090
Iteration 142/1000 | Loss: 0.00001090
Iteration 143/1000 | Loss: 0.00001090
Iteration 144/1000 | Loss: 0.00001090
Iteration 145/1000 | Loss: 0.00001090
Iteration 146/1000 | Loss: 0.00001090
Iteration 147/1000 | Loss: 0.00001090
Iteration 148/1000 | Loss: 0.00001090
Iteration 149/1000 | Loss: 0.00001090
Iteration 150/1000 | Loss: 0.00001090
Iteration 151/1000 | Loss: 0.00001090
Iteration 152/1000 | Loss: 0.00001090
Iteration 153/1000 | Loss: 0.00001090
Iteration 154/1000 | Loss: 0.00001090
Iteration 155/1000 | Loss: 0.00001090
Iteration 156/1000 | Loss: 0.00001090
Iteration 157/1000 | Loss: 0.00001090
Iteration 158/1000 | Loss: 0.00001090
Iteration 159/1000 | Loss: 0.00001090
Iteration 160/1000 | Loss: 0.00001090
Iteration 161/1000 | Loss: 0.00001090
Iteration 162/1000 | Loss: 0.00001090
Iteration 163/1000 | Loss: 0.00001090
Iteration 164/1000 | Loss: 0.00001090
Iteration 165/1000 | Loss: 0.00001090
Iteration 166/1000 | Loss: 0.00001090
Iteration 167/1000 | Loss: 0.00001090
Iteration 168/1000 | Loss: 0.00001090
Iteration 169/1000 | Loss: 0.00001090
Iteration 170/1000 | Loss: 0.00001090
Iteration 171/1000 | Loss: 0.00001090
Iteration 172/1000 | Loss: 0.00001090
Iteration 173/1000 | Loss: 0.00001090
Iteration 174/1000 | Loss: 0.00001090
Iteration 175/1000 | Loss: 0.00001090
Iteration 176/1000 | Loss: 0.00001090
Iteration 177/1000 | Loss: 0.00001090
Iteration 178/1000 | Loss: 0.00001090
Iteration 179/1000 | Loss: 0.00001090
Iteration 180/1000 | Loss: 0.00001090
Iteration 181/1000 | Loss: 0.00001090
Iteration 182/1000 | Loss: 0.00001090
Iteration 183/1000 | Loss: 0.00001090
Iteration 184/1000 | Loss: 0.00001090
Iteration 185/1000 | Loss: 0.00001090
Iteration 186/1000 | Loss: 0.00001090
Iteration 187/1000 | Loss: 0.00001090
Iteration 188/1000 | Loss: 0.00001090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.0899413609877229e-05, 1.0899413609877229e-05, 1.0899413609877229e-05, 1.0899413609877229e-05, 1.0899413609877229e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0899413609877229e-05

Optimization complete. Final v2v error: 2.85221529006958 mm

Highest mean error: 3.175893783569336 mm for frame 78

Lowest mean error: 2.7273213863372803 mm for frame 1

Saving results

Total time: 38.803452253341675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00690234
Iteration 2/25 | Loss: 0.00151792
Iteration 3/25 | Loss: 0.00135260
Iteration 4/25 | Loss: 0.00125864
Iteration 5/25 | Loss: 0.00125200
Iteration 6/25 | Loss: 0.00123848
Iteration 7/25 | Loss: 0.00124317
Iteration 8/25 | Loss: 0.00123513
Iteration 9/25 | Loss: 0.00123466
Iteration 10/25 | Loss: 0.00123452
Iteration 11/25 | Loss: 0.00123451
Iteration 12/25 | Loss: 0.00123450
Iteration 13/25 | Loss: 0.00123450
Iteration 14/25 | Loss: 0.00123450
Iteration 15/25 | Loss: 0.00123449
Iteration 16/25 | Loss: 0.00123449
Iteration 17/25 | Loss: 0.00123449
Iteration 18/25 | Loss: 0.00123449
Iteration 19/25 | Loss: 0.00123449
Iteration 20/25 | Loss: 0.00123449
Iteration 21/25 | Loss: 0.00123449
Iteration 22/25 | Loss: 0.00123449
Iteration 23/25 | Loss: 0.00123449
Iteration 24/25 | Loss: 0.00123449
Iteration 25/25 | Loss: 0.00123449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66342914
Iteration 2/25 | Loss: 0.00119990
Iteration 3/25 | Loss: 0.00119990
Iteration 4/25 | Loss: 0.00118120
Iteration 5/25 | Loss: 0.00118120
Iteration 6/25 | Loss: 0.00118120
Iteration 7/25 | Loss: 0.00118120
Iteration 8/25 | Loss: 0.00118120
Iteration 9/25 | Loss: 0.00118120
Iteration 10/25 | Loss: 0.00118120
Iteration 11/25 | Loss: 0.00118120
Iteration 12/25 | Loss: 0.00118119
Iteration 13/25 | Loss: 0.00118119
Iteration 14/25 | Loss: 0.00118119
Iteration 15/25 | Loss: 0.00118119
Iteration 16/25 | Loss: 0.00118119
Iteration 17/25 | Loss: 0.00118119
Iteration 18/25 | Loss: 0.00118119
Iteration 19/25 | Loss: 0.00118119
Iteration 20/25 | Loss: 0.00118119
Iteration 21/25 | Loss: 0.00118119
Iteration 22/25 | Loss: 0.00118119
Iteration 23/25 | Loss: 0.00118119
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001181193976663053, 0.001181193976663053, 0.001181193976663053, 0.001181193976663053, 0.001181193976663053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001181193976663053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118119
Iteration 2/1000 | Loss: 0.00002423
Iteration 3/1000 | Loss: 0.00005009
Iteration 4/1000 | Loss: 0.00001606
Iteration 5/1000 | Loss: 0.00001522
Iteration 6/1000 | Loss: 0.00001478
Iteration 7/1000 | Loss: 0.00003909
Iteration 8/1000 | Loss: 0.00001437
Iteration 9/1000 | Loss: 0.00001415
Iteration 10/1000 | Loss: 0.00001413
Iteration 11/1000 | Loss: 0.00001381
Iteration 12/1000 | Loss: 0.00001362
Iteration 13/1000 | Loss: 0.00001351
Iteration 14/1000 | Loss: 0.00001343
Iteration 15/1000 | Loss: 0.00004179
Iteration 16/1000 | Loss: 0.00001334
Iteration 17/1000 | Loss: 0.00001322
Iteration 18/1000 | Loss: 0.00001322
Iteration 19/1000 | Loss: 0.00001321
Iteration 20/1000 | Loss: 0.00001321
Iteration 21/1000 | Loss: 0.00001321
Iteration 22/1000 | Loss: 0.00001320
Iteration 23/1000 | Loss: 0.00001320
Iteration 24/1000 | Loss: 0.00001320
Iteration 25/1000 | Loss: 0.00001319
Iteration 26/1000 | Loss: 0.00001319
Iteration 27/1000 | Loss: 0.00001319
Iteration 28/1000 | Loss: 0.00001318
Iteration 29/1000 | Loss: 0.00001318
Iteration 30/1000 | Loss: 0.00001318
Iteration 31/1000 | Loss: 0.00001318
Iteration 32/1000 | Loss: 0.00001318
Iteration 33/1000 | Loss: 0.00001318
Iteration 34/1000 | Loss: 0.00001313
Iteration 35/1000 | Loss: 0.00001313
Iteration 36/1000 | Loss: 0.00001313
Iteration 37/1000 | Loss: 0.00001313
Iteration 38/1000 | Loss: 0.00001313
Iteration 39/1000 | Loss: 0.00001312
Iteration 40/1000 | Loss: 0.00001310
Iteration 41/1000 | Loss: 0.00001308
Iteration 42/1000 | Loss: 0.00001308
Iteration 43/1000 | Loss: 0.00001308
Iteration 44/1000 | Loss: 0.00001308
Iteration 45/1000 | Loss: 0.00001308
Iteration 46/1000 | Loss: 0.00001307
Iteration 47/1000 | Loss: 0.00001307
Iteration 48/1000 | Loss: 0.00001306
Iteration 49/1000 | Loss: 0.00001305
Iteration 50/1000 | Loss: 0.00001305
Iteration 51/1000 | Loss: 0.00001305
Iteration 52/1000 | Loss: 0.00001305
Iteration 53/1000 | Loss: 0.00001305
Iteration 54/1000 | Loss: 0.00001305
Iteration 55/1000 | Loss: 0.00001305
Iteration 56/1000 | Loss: 0.00001304
Iteration 57/1000 | Loss: 0.00001304
Iteration 58/1000 | Loss: 0.00001304
Iteration 59/1000 | Loss: 0.00001304
Iteration 60/1000 | Loss: 0.00001304
Iteration 61/1000 | Loss: 0.00001303
Iteration 62/1000 | Loss: 0.00001303
Iteration 63/1000 | Loss: 0.00001303
Iteration 64/1000 | Loss: 0.00001302
Iteration 65/1000 | Loss: 0.00001302
Iteration 66/1000 | Loss: 0.00001302
Iteration 67/1000 | Loss: 0.00001302
Iteration 68/1000 | Loss: 0.00001301
Iteration 69/1000 | Loss: 0.00001301
Iteration 70/1000 | Loss: 0.00001301
Iteration 71/1000 | Loss: 0.00001301
Iteration 72/1000 | Loss: 0.00001301
Iteration 73/1000 | Loss: 0.00001301
Iteration 74/1000 | Loss: 0.00001301
Iteration 75/1000 | Loss: 0.00001301
Iteration 76/1000 | Loss: 0.00001301
Iteration 77/1000 | Loss: 0.00001300
Iteration 78/1000 | Loss: 0.00001300
Iteration 79/1000 | Loss: 0.00001300
Iteration 80/1000 | Loss: 0.00001300
Iteration 81/1000 | Loss: 0.00001300
Iteration 82/1000 | Loss: 0.00001300
Iteration 83/1000 | Loss: 0.00001299
Iteration 84/1000 | Loss: 0.00001299
Iteration 85/1000 | Loss: 0.00001298
Iteration 86/1000 | Loss: 0.00001298
Iteration 87/1000 | Loss: 0.00001298
Iteration 88/1000 | Loss: 0.00001298
Iteration 89/1000 | Loss: 0.00001298
Iteration 90/1000 | Loss: 0.00001298
Iteration 91/1000 | Loss: 0.00001298
Iteration 92/1000 | Loss: 0.00001298
Iteration 93/1000 | Loss: 0.00001297
Iteration 94/1000 | Loss: 0.00001297
Iteration 95/1000 | Loss: 0.00001297
Iteration 96/1000 | Loss: 0.00001297
Iteration 97/1000 | Loss: 0.00001297
Iteration 98/1000 | Loss: 0.00001297
Iteration 99/1000 | Loss: 0.00001297
Iteration 100/1000 | Loss: 0.00001296
Iteration 101/1000 | Loss: 0.00001296
Iteration 102/1000 | Loss: 0.00001295
Iteration 103/1000 | Loss: 0.00001295
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001294
Iteration 106/1000 | Loss: 0.00001294
Iteration 107/1000 | Loss: 0.00001294
Iteration 108/1000 | Loss: 0.00001294
Iteration 109/1000 | Loss: 0.00001294
Iteration 110/1000 | Loss: 0.00001293
Iteration 111/1000 | Loss: 0.00001293
Iteration 112/1000 | Loss: 0.00001293
Iteration 113/1000 | Loss: 0.00001292
Iteration 114/1000 | Loss: 0.00001292
Iteration 115/1000 | Loss: 0.00001292
Iteration 116/1000 | Loss: 0.00001292
Iteration 117/1000 | Loss: 0.00001292
Iteration 118/1000 | Loss: 0.00001292
Iteration 119/1000 | Loss: 0.00001292
Iteration 120/1000 | Loss: 0.00001292
Iteration 121/1000 | Loss: 0.00001291
Iteration 122/1000 | Loss: 0.00001291
Iteration 123/1000 | Loss: 0.00001291
Iteration 124/1000 | Loss: 0.00001291
Iteration 125/1000 | Loss: 0.00001291
Iteration 126/1000 | Loss: 0.00001291
Iteration 127/1000 | Loss: 0.00001291
Iteration 128/1000 | Loss: 0.00001291
Iteration 129/1000 | Loss: 0.00001291
Iteration 130/1000 | Loss: 0.00001291
Iteration 131/1000 | Loss: 0.00001291
Iteration 132/1000 | Loss: 0.00001291
Iteration 133/1000 | Loss: 0.00001291
Iteration 134/1000 | Loss: 0.00001291
Iteration 135/1000 | Loss: 0.00001291
Iteration 136/1000 | Loss: 0.00001291
Iteration 137/1000 | Loss: 0.00001291
Iteration 138/1000 | Loss: 0.00001291
Iteration 139/1000 | Loss: 0.00001291
Iteration 140/1000 | Loss: 0.00001291
Iteration 141/1000 | Loss: 0.00001291
Iteration 142/1000 | Loss: 0.00001291
Iteration 143/1000 | Loss: 0.00001291
Iteration 144/1000 | Loss: 0.00001291
Iteration 145/1000 | Loss: 0.00001291
Iteration 146/1000 | Loss: 0.00001291
Iteration 147/1000 | Loss: 0.00001291
Iteration 148/1000 | Loss: 0.00001291
Iteration 149/1000 | Loss: 0.00001291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.2906582924188115e-05, 1.2906582924188115e-05, 1.2906582924188115e-05, 1.2906582924188115e-05, 1.2906582924188115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2906582924188115e-05

Optimization complete. Final v2v error: 3.0627052783966064 mm

Highest mean error: 3.5602591037750244 mm for frame 84

Lowest mean error: 2.706449270248413 mm for frame 28

Saving results

Total time: 50.198235511779785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00750675
Iteration 2/25 | Loss: 0.00174274
Iteration 3/25 | Loss: 0.00152085
Iteration 4/25 | Loss: 0.00131919
Iteration 5/25 | Loss: 0.00126676
Iteration 6/25 | Loss: 0.00126615
Iteration 7/25 | Loss: 0.00123884
Iteration 8/25 | Loss: 0.00124656
Iteration 9/25 | Loss: 0.00124573
Iteration 10/25 | Loss: 0.00123287
Iteration 11/25 | Loss: 0.00122967
Iteration 12/25 | Loss: 0.00122923
Iteration 13/25 | Loss: 0.00122905
Iteration 14/25 | Loss: 0.00123440
Iteration 15/25 | Loss: 0.00122962
Iteration 16/25 | Loss: 0.00122861
Iteration 17/25 | Loss: 0.00122795
Iteration 18/25 | Loss: 0.00122778
Iteration 19/25 | Loss: 0.00122776
Iteration 20/25 | Loss: 0.00122776
Iteration 21/25 | Loss: 0.00122775
Iteration 22/25 | Loss: 0.00122775
Iteration 23/25 | Loss: 0.00122775
Iteration 24/25 | Loss: 0.00122775
Iteration 25/25 | Loss: 0.00122775

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84767687
Iteration 2/25 | Loss: 0.00135617
Iteration 3/25 | Loss: 0.00135617
Iteration 4/25 | Loss: 0.00120512
Iteration 5/25 | Loss: 0.00120512
Iteration 6/25 | Loss: 0.00120511
Iteration 7/25 | Loss: 0.00120511
Iteration 8/25 | Loss: 0.00120511
Iteration 9/25 | Loss: 0.00120511
Iteration 10/25 | Loss: 0.00120511
Iteration 11/25 | Loss: 0.00120511
Iteration 12/25 | Loss: 0.00120511
Iteration 13/25 | Loss: 0.00120511
Iteration 14/25 | Loss: 0.00120511
Iteration 15/25 | Loss: 0.00120511
Iteration 16/25 | Loss: 0.00120511
Iteration 17/25 | Loss: 0.00120511
Iteration 18/25 | Loss: 0.00120511
Iteration 19/25 | Loss: 0.00120511
Iteration 20/25 | Loss: 0.00120511
Iteration 21/25 | Loss: 0.00120511
Iteration 22/25 | Loss: 0.00120511
Iteration 23/25 | Loss: 0.00120511
Iteration 24/25 | Loss: 0.00120511
Iteration 25/25 | Loss: 0.00120511

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120511
Iteration 2/1000 | Loss: 0.00002205
Iteration 3/1000 | Loss: 0.00001643
Iteration 4/1000 | Loss: 0.00001496
Iteration 5/1000 | Loss: 0.00024781
Iteration 6/1000 | Loss: 0.00001429
Iteration 7/1000 | Loss: 0.00010931
Iteration 8/1000 | Loss: 0.00001387
Iteration 9/1000 | Loss: 0.00001351
Iteration 10/1000 | Loss: 0.00001348
Iteration 11/1000 | Loss: 0.00001348
Iteration 12/1000 | Loss: 0.00001318
Iteration 13/1000 | Loss: 0.00001295
Iteration 14/1000 | Loss: 0.00001276
Iteration 15/1000 | Loss: 0.00001272
Iteration 16/1000 | Loss: 0.00001272
Iteration 17/1000 | Loss: 0.00001267
Iteration 18/1000 | Loss: 0.00001267
Iteration 19/1000 | Loss: 0.00001264
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001257
Iteration 22/1000 | Loss: 0.00001255
Iteration 23/1000 | Loss: 0.00001249
Iteration 24/1000 | Loss: 0.00001249
Iteration 25/1000 | Loss: 0.00001248
Iteration 26/1000 | Loss: 0.00001248
Iteration 27/1000 | Loss: 0.00001242
Iteration 28/1000 | Loss: 0.00001242
Iteration 29/1000 | Loss: 0.00001242
Iteration 30/1000 | Loss: 0.00001242
Iteration 31/1000 | Loss: 0.00001238
Iteration 32/1000 | Loss: 0.00001237
Iteration 33/1000 | Loss: 0.00001237
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001236
Iteration 36/1000 | Loss: 0.00001236
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001236
Iteration 39/1000 | Loss: 0.00001236
Iteration 40/1000 | Loss: 0.00001236
Iteration 41/1000 | Loss: 0.00001236
Iteration 42/1000 | Loss: 0.00001235
Iteration 43/1000 | Loss: 0.00001235
Iteration 44/1000 | Loss: 0.00001233
Iteration 45/1000 | Loss: 0.00001233
Iteration 46/1000 | Loss: 0.00001232
Iteration 47/1000 | Loss: 0.00001232
Iteration 48/1000 | Loss: 0.00001230
Iteration 49/1000 | Loss: 0.00001230
Iteration 50/1000 | Loss: 0.00001230
Iteration 51/1000 | Loss: 0.00001229
Iteration 52/1000 | Loss: 0.00001229
Iteration 53/1000 | Loss: 0.00001228
Iteration 54/1000 | Loss: 0.00001228
Iteration 55/1000 | Loss: 0.00001227
Iteration 56/1000 | Loss: 0.00001227
Iteration 57/1000 | Loss: 0.00001227
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001225
Iteration 62/1000 | Loss: 0.00001225
Iteration 63/1000 | Loss: 0.00001225
Iteration 64/1000 | Loss: 0.00001225
Iteration 65/1000 | Loss: 0.00001224
Iteration 66/1000 | Loss: 0.00001224
Iteration 67/1000 | Loss: 0.00001224
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001223
Iteration 70/1000 | Loss: 0.00001223
Iteration 71/1000 | Loss: 0.00001222
Iteration 72/1000 | Loss: 0.00001222
Iteration 73/1000 | Loss: 0.00001221
Iteration 74/1000 | Loss: 0.00001221
Iteration 75/1000 | Loss: 0.00001221
Iteration 76/1000 | Loss: 0.00001221
Iteration 77/1000 | Loss: 0.00001221
Iteration 78/1000 | Loss: 0.00001220
Iteration 79/1000 | Loss: 0.00001220
Iteration 80/1000 | Loss: 0.00001219
Iteration 81/1000 | Loss: 0.00001219
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001218
Iteration 85/1000 | Loss: 0.00001217
Iteration 86/1000 | Loss: 0.00001217
Iteration 87/1000 | Loss: 0.00001216
Iteration 88/1000 | Loss: 0.00001216
Iteration 89/1000 | Loss: 0.00001215
Iteration 90/1000 | Loss: 0.00001215
Iteration 91/1000 | Loss: 0.00001215
Iteration 92/1000 | Loss: 0.00001215
Iteration 93/1000 | Loss: 0.00001214
Iteration 94/1000 | Loss: 0.00001214
Iteration 95/1000 | Loss: 0.00001214
Iteration 96/1000 | Loss: 0.00001214
Iteration 97/1000 | Loss: 0.00001214
Iteration 98/1000 | Loss: 0.00001213
Iteration 99/1000 | Loss: 0.00001213
Iteration 100/1000 | Loss: 0.00001213
Iteration 101/1000 | Loss: 0.00001213
Iteration 102/1000 | Loss: 0.00001212
Iteration 103/1000 | Loss: 0.00001212
Iteration 104/1000 | Loss: 0.00001212
Iteration 105/1000 | Loss: 0.00001211
Iteration 106/1000 | Loss: 0.00001211
Iteration 107/1000 | Loss: 0.00001211
Iteration 108/1000 | Loss: 0.00001211
Iteration 109/1000 | Loss: 0.00001211
Iteration 110/1000 | Loss: 0.00001211
Iteration 111/1000 | Loss: 0.00001211
Iteration 112/1000 | Loss: 0.00001211
Iteration 113/1000 | Loss: 0.00001211
Iteration 114/1000 | Loss: 0.00001211
Iteration 115/1000 | Loss: 0.00001211
Iteration 116/1000 | Loss: 0.00001211
Iteration 117/1000 | Loss: 0.00001211
Iteration 118/1000 | Loss: 0.00001211
Iteration 119/1000 | Loss: 0.00001211
Iteration 120/1000 | Loss: 0.00001211
Iteration 121/1000 | Loss: 0.00001211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.211094786413014e-05, 1.211094786413014e-05, 1.211094786413014e-05, 1.211094786413014e-05, 1.211094786413014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.211094786413014e-05

Optimization complete. Final v2v error: 2.9745073318481445 mm

Highest mean error: 3.4133965969085693 mm for frame 63

Lowest mean error: 2.673124313354492 mm for frame 2

Saving results

Total time: 63.805187463760376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485790
Iteration 2/25 | Loss: 0.00134492
Iteration 3/25 | Loss: 0.00125562
Iteration 4/25 | Loss: 0.00124688
Iteration 5/25 | Loss: 0.00124497
Iteration 6/25 | Loss: 0.00124497
Iteration 7/25 | Loss: 0.00124497
Iteration 8/25 | Loss: 0.00124497
Iteration 9/25 | Loss: 0.00124497
Iteration 10/25 | Loss: 0.00124497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00124497024808079, 0.00124497024808079, 0.00124497024808079, 0.00124497024808079, 0.00124497024808079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00124497024808079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.64930010
Iteration 2/25 | Loss: 0.00102607
Iteration 3/25 | Loss: 0.00102606
Iteration 4/25 | Loss: 0.00102606
Iteration 5/25 | Loss: 0.00102606
Iteration 6/25 | Loss: 0.00102606
Iteration 7/25 | Loss: 0.00102606
Iteration 8/25 | Loss: 0.00102606
Iteration 9/25 | Loss: 0.00102606
Iteration 10/25 | Loss: 0.00102606
Iteration 11/25 | Loss: 0.00102606
Iteration 12/25 | Loss: 0.00102606
Iteration 13/25 | Loss: 0.00102606
Iteration 14/25 | Loss: 0.00102606
Iteration 15/25 | Loss: 0.00102606
Iteration 16/25 | Loss: 0.00102606
Iteration 17/25 | Loss: 0.00102606
Iteration 18/25 | Loss: 0.00102606
Iteration 19/25 | Loss: 0.00102606
Iteration 20/25 | Loss: 0.00102606
Iteration 21/25 | Loss: 0.00102606
Iteration 22/25 | Loss: 0.00102606
Iteration 23/25 | Loss: 0.00102606
Iteration 24/25 | Loss: 0.00102606
Iteration 25/25 | Loss: 0.00102606

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102606
Iteration 2/1000 | Loss: 0.00002271
Iteration 3/1000 | Loss: 0.00001812
Iteration 4/1000 | Loss: 0.00001642
Iteration 5/1000 | Loss: 0.00001544
Iteration 6/1000 | Loss: 0.00001494
Iteration 7/1000 | Loss: 0.00001481
Iteration 8/1000 | Loss: 0.00001450
Iteration 9/1000 | Loss: 0.00001420
Iteration 10/1000 | Loss: 0.00001387
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001356
Iteration 13/1000 | Loss: 0.00001349
Iteration 14/1000 | Loss: 0.00001339
Iteration 15/1000 | Loss: 0.00001336
Iteration 16/1000 | Loss: 0.00001332
Iteration 17/1000 | Loss: 0.00001331
Iteration 18/1000 | Loss: 0.00001330
Iteration 19/1000 | Loss: 0.00001329
Iteration 20/1000 | Loss: 0.00001327
Iteration 21/1000 | Loss: 0.00001321
Iteration 22/1000 | Loss: 0.00001317
Iteration 23/1000 | Loss: 0.00001316
Iteration 24/1000 | Loss: 0.00001310
Iteration 25/1000 | Loss: 0.00001305
Iteration 26/1000 | Loss: 0.00001304
Iteration 27/1000 | Loss: 0.00001304
Iteration 28/1000 | Loss: 0.00001303
Iteration 29/1000 | Loss: 0.00001303
Iteration 30/1000 | Loss: 0.00001302
Iteration 31/1000 | Loss: 0.00001301
Iteration 32/1000 | Loss: 0.00001301
Iteration 33/1000 | Loss: 0.00001300
Iteration 34/1000 | Loss: 0.00001299
Iteration 35/1000 | Loss: 0.00001298
Iteration 36/1000 | Loss: 0.00001298
Iteration 37/1000 | Loss: 0.00001298
Iteration 38/1000 | Loss: 0.00001298
Iteration 39/1000 | Loss: 0.00001297
Iteration 40/1000 | Loss: 0.00001297
Iteration 41/1000 | Loss: 0.00001296
Iteration 42/1000 | Loss: 0.00001294
Iteration 43/1000 | Loss: 0.00001294
Iteration 44/1000 | Loss: 0.00001293
Iteration 45/1000 | Loss: 0.00001292
Iteration 46/1000 | Loss: 0.00001292
Iteration 47/1000 | Loss: 0.00001291
Iteration 48/1000 | Loss: 0.00001291
Iteration 49/1000 | Loss: 0.00001291
Iteration 50/1000 | Loss: 0.00001291
Iteration 51/1000 | Loss: 0.00001291
Iteration 52/1000 | Loss: 0.00001291
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001289
Iteration 55/1000 | Loss: 0.00001289
Iteration 56/1000 | Loss: 0.00001288
Iteration 57/1000 | Loss: 0.00001286
Iteration 58/1000 | Loss: 0.00001286
Iteration 59/1000 | Loss: 0.00001286
Iteration 60/1000 | Loss: 0.00001286
Iteration 61/1000 | Loss: 0.00001286
Iteration 62/1000 | Loss: 0.00001286
Iteration 63/1000 | Loss: 0.00001285
Iteration 64/1000 | Loss: 0.00001285
Iteration 65/1000 | Loss: 0.00001284
Iteration 66/1000 | Loss: 0.00001284
Iteration 67/1000 | Loss: 0.00001284
Iteration 68/1000 | Loss: 0.00001284
Iteration 69/1000 | Loss: 0.00001283
Iteration 70/1000 | Loss: 0.00001283
Iteration 71/1000 | Loss: 0.00001282
Iteration 72/1000 | Loss: 0.00001281
Iteration 73/1000 | Loss: 0.00001281
Iteration 74/1000 | Loss: 0.00001280
Iteration 75/1000 | Loss: 0.00001280
Iteration 76/1000 | Loss: 0.00001280
Iteration 77/1000 | Loss: 0.00001279
Iteration 78/1000 | Loss: 0.00001279
Iteration 79/1000 | Loss: 0.00001278
Iteration 80/1000 | Loss: 0.00001278
Iteration 81/1000 | Loss: 0.00001278
Iteration 82/1000 | Loss: 0.00001277
Iteration 83/1000 | Loss: 0.00001277
Iteration 84/1000 | Loss: 0.00001277
Iteration 85/1000 | Loss: 0.00001277
Iteration 86/1000 | Loss: 0.00001277
Iteration 87/1000 | Loss: 0.00001277
Iteration 88/1000 | Loss: 0.00001277
Iteration 89/1000 | Loss: 0.00001277
Iteration 90/1000 | Loss: 0.00001277
Iteration 91/1000 | Loss: 0.00001277
Iteration 92/1000 | Loss: 0.00001277
Iteration 93/1000 | Loss: 0.00001276
Iteration 94/1000 | Loss: 0.00001276
Iteration 95/1000 | Loss: 0.00001275
Iteration 96/1000 | Loss: 0.00001274
Iteration 97/1000 | Loss: 0.00001274
Iteration 98/1000 | Loss: 0.00001274
Iteration 99/1000 | Loss: 0.00001274
Iteration 100/1000 | Loss: 0.00001273
Iteration 101/1000 | Loss: 0.00001273
Iteration 102/1000 | Loss: 0.00001273
Iteration 103/1000 | Loss: 0.00001272
Iteration 104/1000 | Loss: 0.00001272
Iteration 105/1000 | Loss: 0.00001272
Iteration 106/1000 | Loss: 0.00001271
Iteration 107/1000 | Loss: 0.00001270
Iteration 108/1000 | Loss: 0.00001270
Iteration 109/1000 | Loss: 0.00001270
Iteration 110/1000 | Loss: 0.00001270
Iteration 111/1000 | Loss: 0.00001270
Iteration 112/1000 | Loss: 0.00001270
Iteration 113/1000 | Loss: 0.00001269
Iteration 114/1000 | Loss: 0.00001269
Iteration 115/1000 | Loss: 0.00001269
Iteration 116/1000 | Loss: 0.00001269
Iteration 117/1000 | Loss: 0.00001269
Iteration 118/1000 | Loss: 0.00001269
Iteration 119/1000 | Loss: 0.00001269
Iteration 120/1000 | Loss: 0.00001269
Iteration 121/1000 | Loss: 0.00001269
Iteration 122/1000 | Loss: 0.00001269
Iteration 123/1000 | Loss: 0.00001268
Iteration 124/1000 | Loss: 0.00001268
Iteration 125/1000 | Loss: 0.00001268
Iteration 126/1000 | Loss: 0.00001267
Iteration 127/1000 | Loss: 0.00001267
Iteration 128/1000 | Loss: 0.00001267
Iteration 129/1000 | Loss: 0.00001267
Iteration 130/1000 | Loss: 0.00001267
Iteration 131/1000 | Loss: 0.00001267
Iteration 132/1000 | Loss: 0.00001267
Iteration 133/1000 | Loss: 0.00001267
Iteration 134/1000 | Loss: 0.00001267
Iteration 135/1000 | Loss: 0.00001267
Iteration 136/1000 | Loss: 0.00001267
Iteration 137/1000 | Loss: 0.00001267
Iteration 138/1000 | Loss: 0.00001267
Iteration 139/1000 | Loss: 0.00001267
Iteration 140/1000 | Loss: 0.00001267
Iteration 141/1000 | Loss: 0.00001267
Iteration 142/1000 | Loss: 0.00001267
Iteration 143/1000 | Loss: 0.00001267
Iteration 144/1000 | Loss: 0.00001267
Iteration 145/1000 | Loss: 0.00001267
Iteration 146/1000 | Loss: 0.00001267
Iteration 147/1000 | Loss: 0.00001267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.2666043403442018e-05, 1.2666043403442018e-05, 1.2666043403442018e-05, 1.2666043403442018e-05, 1.2666043403442018e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2666043403442018e-05

Optimization complete. Final v2v error: 3.038684606552124 mm

Highest mean error: 3.3199191093444824 mm for frame 214

Lowest mean error: 2.7834250926971436 mm for frame 117

Saving results

Total time: 42.887728452682495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781053
Iteration 2/25 | Loss: 0.00159103
Iteration 3/25 | Loss: 0.00130027
Iteration 4/25 | Loss: 0.00128048
Iteration 5/25 | Loss: 0.00127732
Iteration 6/25 | Loss: 0.00127732
Iteration 7/25 | Loss: 0.00127732
Iteration 8/25 | Loss: 0.00127732
Iteration 9/25 | Loss: 0.00127732
Iteration 10/25 | Loss: 0.00127732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012773178750649095, 0.0012773178750649095, 0.0012773178750649095, 0.0012773178750649095, 0.0012773178750649095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012773178750649095

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36982250
Iteration 2/25 | Loss: 0.00099593
Iteration 3/25 | Loss: 0.00099592
Iteration 4/25 | Loss: 0.00099592
Iteration 5/25 | Loss: 0.00099592
Iteration 6/25 | Loss: 0.00099592
Iteration 7/25 | Loss: 0.00099592
Iteration 8/25 | Loss: 0.00099592
Iteration 9/25 | Loss: 0.00099592
Iteration 10/25 | Loss: 0.00099592
Iteration 11/25 | Loss: 0.00099592
Iteration 12/25 | Loss: 0.00099592
Iteration 13/25 | Loss: 0.00099592
Iteration 14/25 | Loss: 0.00099592
Iteration 15/25 | Loss: 0.00099592
Iteration 16/25 | Loss: 0.00099592
Iteration 17/25 | Loss: 0.00099592
Iteration 18/25 | Loss: 0.00099592
Iteration 19/25 | Loss: 0.00099592
Iteration 20/25 | Loss: 0.00099592
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009959193412214518, 0.0009959193412214518, 0.0009959193412214518, 0.0009959193412214518, 0.0009959193412214518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009959193412214518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099592
Iteration 2/1000 | Loss: 0.00003549
Iteration 3/1000 | Loss: 0.00002458
Iteration 4/1000 | Loss: 0.00002188
Iteration 5/1000 | Loss: 0.00001992
Iteration 6/1000 | Loss: 0.00001885
Iteration 7/1000 | Loss: 0.00001824
Iteration 8/1000 | Loss: 0.00001773
Iteration 9/1000 | Loss: 0.00001728
Iteration 10/1000 | Loss: 0.00001685
Iteration 11/1000 | Loss: 0.00001663
Iteration 12/1000 | Loss: 0.00001649
Iteration 13/1000 | Loss: 0.00001643
Iteration 14/1000 | Loss: 0.00001633
Iteration 15/1000 | Loss: 0.00001629
Iteration 16/1000 | Loss: 0.00001629
Iteration 17/1000 | Loss: 0.00001628
Iteration 18/1000 | Loss: 0.00001627
Iteration 19/1000 | Loss: 0.00001626
Iteration 20/1000 | Loss: 0.00001625
Iteration 21/1000 | Loss: 0.00001624
Iteration 22/1000 | Loss: 0.00001623
Iteration 23/1000 | Loss: 0.00001622
Iteration 24/1000 | Loss: 0.00001620
Iteration 25/1000 | Loss: 0.00001620
Iteration 26/1000 | Loss: 0.00001619
Iteration 27/1000 | Loss: 0.00001616
Iteration 28/1000 | Loss: 0.00001616
Iteration 29/1000 | Loss: 0.00001613
Iteration 30/1000 | Loss: 0.00001612
Iteration 31/1000 | Loss: 0.00001611
Iteration 32/1000 | Loss: 0.00001611
Iteration 33/1000 | Loss: 0.00001611
Iteration 34/1000 | Loss: 0.00001609
Iteration 35/1000 | Loss: 0.00001609
Iteration 36/1000 | Loss: 0.00001609
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001609
Iteration 39/1000 | Loss: 0.00001609
Iteration 40/1000 | Loss: 0.00001608
Iteration 41/1000 | Loss: 0.00001608
Iteration 42/1000 | Loss: 0.00001608
Iteration 43/1000 | Loss: 0.00001607
Iteration 44/1000 | Loss: 0.00001605
Iteration 45/1000 | Loss: 0.00001604
Iteration 46/1000 | Loss: 0.00001604
Iteration 47/1000 | Loss: 0.00001603
Iteration 48/1000 | Loss: 0.00001603
Iteration 49/1000 | Loss: 0.00001603
Iteration 50/1000 | Loss: 0.00001602
Iteration 51/1000 | Loss: 0.00001602
Iteration 52/1000 | Loss: 0.00001602
Iteration 53/1000 | Loss: 0.00001601
Iteration 54/1000 | Loss: 0.00001601
Iteration 55/1000 | Loss: 0.00001600
Iteration 56/1000 | Loss: 0.00001599
Iteration 57/1000 | Loss: 0.00001599
Iteration 58/1000 | Loss: 0.00001598
Iteration 59/1000 | Loss: 0.00001597
Iteration 60/1000 | Loss: 0.00001597
Iteration 61/1000 | Loss: 0.00001597
Iteration 62/1000 | Loss: 0.00001597
Iteration 63/1000 | Loss: 0.00001596
Iteration 64/1000 | Loss: 0.00001596
Iteration 65/1000 | Loss: 0.00001596
Iteration 66/1000 | Loss: 0.00001596
Iteration 67/1000 | Loss: 0.00001596
Iteration 68/1000 | Loss: 0.00001596
Iteration 69/1000 | Loss: 0.00001596
Iteration 70/1000 | Loss: 0.00001596
Iteration 71/1000 | Loss: 0.00001596
Iteration 72/1000 | Loss: 0.00001596
Iteration 73/1000 | Loss: 0.00001595
Iteration 74/1000 | Loss: 0.00001595
Iteration 75/1000 | Loss: 0.00001595
Iteration 76/1000 | Loss: 0.00001594
Iteration 77/1000 | Loss: 0.00001594
Iteration 78/1000 | Loss: 0.00001593
Iteration 79/1000 | Loss: 0.00001593
Iteration 80/1000 | Loss: 0.00001592
Iteration 81/1000 | Loss: 0.00001592
Iteration 82/1000 | Loss: 0.00001591
Iteration 83/1000 | Loss: 0.00001591
Iteration 84/1000 | Loss: 0.00001591
Iteration 85/1000 | Loss: 0.00001591
Iteration 86/1000 | Loss: 0.00001591
Iteration 87/1000 | Loss: 0.00001590
Iteration 88/1000 | Loss: 0.00001589
Iteration 89/1000 | Loss: 0.00001589
Iteration 90/1000 | Loss: 0.00001589
Iteration 91/1000 | Loss: 0.00001588
Iteration 92/1000 | Loss: 0.00001588
Iteration 93/1000 | Loss: 0.00001588
Iteration 94/1000 | Loss: 0.00001587
Iteration 95/1000 | Loss: 0.00001587
Iteration 96/1000 | Loss: 0.00001587
Iteration 97/1000 | Loss: 0.00001587
Iteration 98/1000 | Loss: 0.00001587
Iteration 99/1000 | Loss: 0.00001587
Iteration 100/1000 | Loss: 0.00001587
Iteration 101/1000 | Loss: 0.00001586
Iteration 102/1000 | Loss: 0.00001586
Iteration 103/1000 | Loss: 0.00001586
Iteration 104/1000 | Loss: 0.00001586
Iteration 105/1000 | Loss: 0.00001586
Iteration 106/1000 | Loss: 0.00001586
Iteration 107/1000 | Loss: 0.00001585
Iteration 108/1000 | Loss: 0.00001585
Iteration 109/1000 | Loss: 0.00001585
Iteration 110/1000 | Loss: 0.00001585
Iteration 111/1000 | Loss: 0.00001585
Iteration 112/1000 | Loss: 0.00001585
Iteration 113/1000 | Loss: 0.00001585
Iteration 114/1000 | Loss: 0.00001585
Iteration 115/1000 | Loss: 0.00001585
Iteration 116/1000 | Loss: 0.00001584
Iteration 117/1000 | Loss: 0.00001584
Iteration 118/1000 | Loss: 0.00001584
Iteration 119/1000 | Loss: 0.00001584
Iteration 120/1000 | Loss: 0.00001584
Iteration 121/1000 | Loss: 0.00001583
Iteration 122/1000 | Loss: 0.00001583
Iteration 123/1000 | Loss: 0.00001583
Iteration 124/1000 | Loss: 0.00001583
Iteration 125/1000 | Loss: 0.00001582
Iteration 126/1000 | Loss: 0.00001582
Iteration 127/1000 | Loss: 0.00001582
Iteration 128/1000 | Loss: 0.00001582
Iteration 129/1000 | Loss: 0.00001581
Iteration 130/1000 | Loss: 0.00001581
Iteration 131/1000 | Loss: 0.00001581
Iteration 132/1000 | Loss: 0.00001580
Iteration 133/1000 | Loss: 0.00001580
Iteration 134/1000 | Loss: 0.00001580
Iteration 135/1000 | Loss: 0.00001580
Iteration 136/1000 | Loss: 0.00001580
Iteration 137/1000 | Loss: 0.00001580
Iteration 138/1000 | Loss: 0.00001580
Iteration 139/1000 | Loss: 0.00001579
Iteration 140/1000 | Loss: 0.00001579
Iteration 141/1000 | Loss: 0.00001579
Iteration 142/1000 | Loss: 0.00001579
Iteration 143/1000 | Loss: 0.00001579
Iteration 144/1000 | Loss: 0.00001579
Iteration 145/1000 | Loss: 0.00001579
Iteration 146/1000 | Loss: 0.00001579
Iteration 147/1000 | Loss: 0.00001579
Iteration 148/1000 | Loss: 0.00001578
Iteration 149/1000 | Loss: 0.00001578
Iteration 150/1000 | Loss: 0.00001578
Iteration 151/1000 | Loss: 0.00001578
Iteration 152/1000 | Loss: 0.00001578
Iteration 153/1000 | Loss: 0.00001578
Iteration 154/1000 | Loss: 0.00001578
Iteration 155/1000 | Loss: 0.00001578
Iteration 156/1000 | Loss: 0.00001578
Iteration 157/1000 | Loss: 0.00001578
Iteration 158/1000 | Loss: 0.00001578
Iteration 159/1000 | Loss: 0.00001578
Iteration 160/1000 | Loss: 0.00001578
Iteration 161/1000 | Loss: 0.00001577
Iteration 162/1000 | Loss: 0.00001577
Iteration 163/1000 | Loss: 0.00001577
Iteration 164/1000 | Loss: 0.00001577
Iteration 165/1000 | Loss: 0.00001577
Iteration 166/1000 | Loss: 0.00001577
Iteration 167/1000 | Loss: 0.00001577
Iteration 168/1000 | Loss: 0.00001577
Iteration 169/1000 | Loss: 0.00001577
Iteration 170/1000 | Loss: 0.00001577
Iteration 171/1000 | Loss: 0.00001577
Iteration 172/1000 | Loss: 0.00001576
Iteration 173/1000 | Loss: 0.00001576
Iteration 174/1000 | Loss: 0.00001576
Iteration 175/1000 | Loss: 0.00001576
Iteration 176/1000 | Loss: 0.00001576
Iteration 177/1000 | Loss: 0.00001576
Iteration 178/1000 | Loss: 0.00001576
Iteration 179/1000 | Loss: 0.00001576
Iteration 180/1000 | Loss: 0.00001576
Iteration 181/1000 | Loss: 0.00001576
Iteration 182/1000 | Loss: 0.00001576
Iteration 183/1000 | Loss: 0.00001576
Iteration 184/1000 | Loss: 0.00001576
Iteration 185/1000 | Loss: 0.00001576
Iteration 186/1000 | Loss: 0.00001575
Iteration 187/1000 | Loss: 0.00001575
Iteration 188/1000 | Loss: 0.00001575
Iteration 189/1000 | Loss: 0.00001575
Iteration 190/1000 | Loss: 0.00001575
Iteration 191/1000 | Loss: 0.00001575
Iteration 192/1000 | Loss: 0.00001575
Iteration 193/1000 | Loss: 0.00001575
Iteration 194/1000 | Loss: 0.00001575
Iteration 195/1000 | Loss: 0.00001575
Iteration 196/1000 | Loss: 0.00001575
Iteration 197/1000 | Loss: 0.00001575
Iteration 198/1000 | Loss: 0.00001575
Iteration 199/1000 | Loss: 0.00001575
Iteration 200/1000 | Loss: 0.00001575
Iteration 201/1000 | Loss: 0.00001575
Iteration 202/1000 | Loss: 0.00001575
Iteration 203/1000 | Loss: 0.00001575
Iteration 204/1000 | Loss: 0.00001575
Iteration 205/1000 | Loss: 0.00001575
Iteration 206/1000 | Loss: 0.00001575
Iteration 207/1000 | Loss: 0.00001575
Iteration 208/1000 | Loss: 0.00001575
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.57466947712237e-05, 1.57466947712237e-05, 1.57466947712237e-05, 1.57466947712237e-05, 1.57466947712237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.57466947712237e-05

Optimization complete. Final v2v error: 3.3608415126800537 mm

Highest mean error: 3.673875093460083 mm for frame 181

Lowest mean error: 3.0491483211517334 mm for frame 219

Saving results

Total time: 47.044992446899414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00596285
Iteration 2/25 | Loss: 0.00129490
Iteration 3/25 | Loss: 0.00121856
Iteration 4/25 | Loss: 0.00120972
Iteration 5/25 | Loss: 0.00120772
Iteration 6/25 | Loss: 0.00120772
Iteration 7/25 | Loss: 0.00120772
Iteration 8/25 | Loss: 0.00120772
Iteration 9/25 | Loss: 0.00120772
Iteration 10/25 | Loss: 0.00120768
Iteration 11/25 | Loss: 0.00120768
Iteration 12/25 | Loss: 0.00120768
Iteration 13/25 | Loss: 0.00120768
Iteration 14/25 | Loss: 0.00120768
Iteration 15/25 | Loss: 0.00120768
Iteration 16/25 | Loss: 0.00120768
Iteration 17/25 | Loss: 0.00120768
Iteration 18/25 | Loss: 0.00120768
Iteration 19/25 | Loss: 0.00120768
Iteration 20/25 | Loss: 0.00120768
Iteration 21/25 | Loss: 0.00120768
Iteration 22/25 | Loss: 0.00120768
Iteration 23/25 | Loss: 0.00120768
Iteration 24/25 | Loss: 0.00120768
Iteration 25/25 | Loss: 0.00120768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.08454275
Iteration 2/25 | Loss: 0.00108659
Iteration 3/25 | Loss: 0.00108659
Iteration 4/25 | Loss: 0.00108659
Iteration 5/25 | Loss: 0.00108658
Iteration 6/25 | Loss: 0.00108658
Iteration 7/25 | Loss: 0.00108658
Iteration 8/25 | Loss: 0.00108658
Iteration 9/25 | Loss: 0.00108658
Iteration 10/25 | Loss: 0.00108658
Iteration 11/25 | Loss: 0.00108658
Iteration 12/25 | Loss: 0.00108658
Iteration 13/25 | Loss: 0.00108658
Iteration 14/25 | Loss: 0.00108658
Iteration 15/25 | Loss: 0.00108658
Iteration 16/25 | Loss: 0.00108658
Iteration 17/25 | Loss: 0.00108658
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010865837102755904, 0.0010865837102755904, 0.0010865837102755904, 0.0010865837102755904, 0.0010865837102755904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010865837102755904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108658
Iteration 2/1000 | Loss: 0.00002567
Iteration 3/1000 | Loss: 0.00001771
Iteration 4/1000 | Loss: 0.00001541
Iteration 5/1000 | Loss: 0.00001409
Iteration 6/1000 | Loss: 0.00001329
Iteration 7/1000 | Loss: 0.00001284
Iteration 8/1000 | Loss: 0.00001243
Iteration 9/1000 | Loss: 0.00001224
Iteration 10/1000 | Loss: 0.00001201
Iteration 11/1000 | Loss: 0.00001180
Iteration 12/1000 | Loss: 0.00001178
Iteration 13/1000 | Loss: 0.00001177
Iteration 14/1000 | Loss: 0.00001174
Iteration 15/1000 | Loss: 0.00001161
Iteration 16/1000 | Loss: 0.00001153
Iteration 17/1000 | Loss: 0.00001149
Iteration 18/1000 | Loss: 0.00001146
Iteration 19/1000 | Loss: 0.00001146
Iteration 20/1000 | Loss: 0.00001145
Iteration 21/1000 | Loss: 0.00001145
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001144
Iteration 24/1000 | Loss: 0.00001144
Iteration 25/1000 | Loss: 0.00001143
Iteration 26/1000 | Loss: 0.00001143
Iteration 27/1000 | Loss: 0.00001142
Iteration 28/1000 | Loss: 0.00001141
Iteration 29/1000 | Loss: 0.00001140
Iteration 30/1000 | Loss: 0.00001140
Iteration 31/1000 | Loss: 0.00001139
Iteration 32/1000 | Loss: 0.00001139
Iteration 33/1000 | Loss: 0.00001138
Iteration 34/1000 | Loss: 0.00001137
Iteration 35/1000 | Loss: 0.00001137
Iteration 36/1000 | Loss: 0.00001136
Iteration 37/1000 | Loss: 0.00001136
Iteration 38/1000 | Loss: 0.00001135
Iteration 39/1000 | Loss: 0.00001133
Iteration 40/1000 | Loss: 0.00001129
Iteration 41/1000 | Loss: 0.00001127
Iteration 42/1000 | Loss: 0.00001126
Iteration 43/1000 | Loss: 0.00001125
Iteration 44/1000 | Loss: 0.00001125
Iteration 45/1000 | Loss: 0.00001125
Iteration 46/1000 | Loss: 0.00001125
Iteration 47/1000 | Loss: 0.00001124
Iteration 48/1000 | Loss: 0.00001124
Iteration 49/1000 | Loss: 0.00001124
Iteration 50/1000 | Loss: 0.00001123
Iteration 51/1000 | Loss: 0.00001123
Iteration 52/1000 | Loss: 0.00001123
Iteration 53/1000 | Loss: 0.00001121
Iteration 54/1000 | Loss: 0.00001121
Iteration 55/1000 | Loss: 0.00001120
Iteration 56/1000 | Loss: 0.00001120
Iteration 57/1000 | Loss: 0.00001120
Iteration 58/1000 | Loss: 0.00001120
Iteration 59/1000 | Loss: 0.00001120
Iteration 60/1000 | Loss: 0.00001120
Iteration 61/1000 | Loss: 0.00001120
Iteration 62/1000 | Loss: 0.00001119
Iteration 63/1000 | Loss: 0.00001117
Iteration 64/1000 | Loss: 0.00001117
Iteration 65/1000 | Loss: 0.00001117
Iteration 66/1000 | Loss: 0.00001117
Iteration 67/1000 | Loss: 0.00001117
Iteration 68/1000 | Loss: 0.00001117
Iteration 69/1000 | Loss: 0.00001116
Iteration 70/1000 | Loss: 0.00001116
Iteration 71/1000 | Loss: 0.00001116
Iteration 72/1000 | Loss: 0.00001116
Iteration 73/1000 | Loss: 0.00001116
Iteration 74/1000 | Loss: 0.00001116
Iteration 75/1000 | Loss: 0.00001115
Iteration 76/1000 | Loss: 0.00001115
Iteration 77/1000 | Loss: 0.00001114
Iteration 78/1000 | Loss: 0.00001114
Iteration 79/1000 | Loss: 0.00001112
Iteration 80/1000 | Loss: 0.00001111
Iteration 81/1000 | Loss: 0.00001111
Iteration 82/1000 | Loss: 0.00001111
Iteration 83/1000 | Loss: 0.00001111
Iteration 84/1000 | Loss: 0.00001111
Iteration 85/1000 | Loss: 0.00001111
Iteration 86/1000 | Loss: 0.00001110
Iteration 87/1000 | Loss: 0.00001110
Iteration 88/1000 | Loss: 0.00001110
Iteration 89/1000 | Loss: 0.00001110
Iteration 90/1000 | Loss: 0.00001109
Iteration 91/1000 | Loss: 0.00001109
Iteration 92/1000 | Loss: 0.00001108
Iteration 93/1000 | Loss: 0.00001108
Iteration 94/1000 | Loss: 0.00001108
Iteration 95/1000 | Loss: 0.00001108
Iteration 96/1000 | Loss: 0.00001107
Iteration 97/1000 | Loss: 0.00001106
Iteration 98/1000 | Loss: 0.00001106
Iteration 99/1000 | Loss: 0.00001106
Iteration 100/1000 | Loss: 0.00001106
Iteration 101/1000 | Loss: 0.00001106
Iteration 102/1000 | Loss: 0.00001106
Iteration 103/1000 | Loss: 0.00001106
Iteration 104/1000 | Loss: 0.00001106
Iteration 105/1000 | Loss: 0.00001106
Iteration 106/1000 | Loss: 0.00001106
Iteration 107/1000 | Loss: 0.00001106
Iteration 108/1000 | Loss: 0.00001106
Iteration 109/1000 | Loss: 0.00001106
Iteration 110/1000 | Loss: 0.00001106
Iteration 111/1000 | Loss: 0.00001105
Iteration 112/1000 | Loss: 0.00001105
Iteration 113/1000 | Loss: 0.00001105
Iteration 114/1000 | Loss: 0.00001105
Iteration 115/1000 | Loss: 0.00001105
Iteration 116/1000 | Loss: 0.00001105
Iteration 117/1000 | Loss: 0.00001105
Iteration 118/1000 | Loss: 0.00001105
Iteration 119/1000 | Loss: 0.00001105
Iteration 120/1000 | Loss: 0.00001105
Iteration 121/1000 | Loss: 0.00001104
Iteration 122/1000 | Loss: 0.00001104
Iteration 123/1000 | Loss: 0.00001104
Iteration 124/1000 | Loss: 0.00001104
Iteration 125/1000 | Loss: 0.00001104
Iteration 126/1000 | Loss: 0.00001104
Iteration 127/1000 | Loss: 0.00001104
Iteration 128/1000 | Loss: 0.00001104
Iteration 129/1000 | Loss: 0.00001104
Iteration 130/1000 | Loss: 0.00001104
Iteration 131/1000 | Loss: 0.00001103
Iteration 132/1000 | Loss: 0.00001103
Iteration 133/1000 | Loss: 0.00001103
Iteration 134/1000 | Loss: 0.00001103
Iteration 135/1000 | Loss: 0.00001103
Iteration 136/1000 | Loss: 0.00001103
Iteration 137/1000 | Loss: 0.00001103
Iteration 138/1000 | Loss: 0.00001103
Iteration 139/1000 | Loss: 0.00001102
Iteration 140/1000 | Loss: 0.00001102
Iteration 141/1000 | Loss: 0.00001102
Iteration 142/1000 | Loss: 0.00001102
Iteration 143/1000 | Loss: 0.00001102
Iteration 144/1000 | Loss: 0.00001102
Iteration 145/1000 | Loss: 0.00001102
Iteration 146/1000 | Loss: 0.00001102
Iteration 147/1000 | Loss: 0.00001102
Iteration 148/1000 | Loss: 0.00001102
Iteration 149/1000 | Loss: 0.00001102
Iteration 150/1000 | Loss: 0.00001102
Iteration 151/1000 | Loss: 0.00001102
Iteration 152/1000 | Loss: 0.00001102
Iteration 153/1000 | Loss: 0.00001102
Iteration 154/1000 | Loss: 0.00001101
Iteration 155/1000 | Loss: 0.00001101
Iteration 156/1000 | Loss: 0.00001101
Iteration 157/1000 | Loss: 0.00001101
Iteration 158/1000 | Loss: 0.00001101
Iteration 159/1000 | Loss: 0.00001101
Iteration 160/1000 | Loss: 0.00001101
Iteration 161/1000 | Loss: 0.00001101
Iteration 162/1000 | Loss: 0.00001100
Iteration 163/1000 | Loss: 0.00001100
Iteration 164/1000 | Loss: 0.00001100
Iteration 165/1000 | Loss: 0.00001100
Iteration 166/1000 | Loss: 0.00001100
Iteration 167/1000 | Loss: 0.00001100
Iteration 168/1000 | Loss: 0.00001100
Iteration 169/1000 | Loss: 0.00001100
Iteration 170/1000 | Loss: 0.00001100
Iteration 171/1000 | Loss: 0.00001100
Iteration 172/1000 | Loss: 0.00001100
Iteration 173/1000 | Loss: 0.00001100
Iteration 174/1000 | Loss: 0.00001100
Iteration 175/1000 | Loss: 0.00001100
Iteration 176/1000 | Loss: 0.00001100
Iteration 177/1000 | Loss: 0.00001100
Iteration 178/1000 | Loss: 0.00001100
Iteration 179/1000 | Loss: 0.00001100
Iteration 180/1000 | Loss: 0.00001100
Iteration 181/1000 | Loss: 0.00001100
Iteration 182/1000 | Loss: 0.00001100
Iteration 183/1000 | Loss: 0.00001100
Iteration 184/1000 | Loss: 0.00001100
Iteration 185/1000 | Loss: 0.00001100
Iteration 186/1000 | Loss: 0.00001100
Iteration 187/1000 | Loss: 0.00001100
Iteration 188/1000 | Loss: 0.00001100
Iteration 189/1000 | Loss: 0.00001100
Iteration 190/1000 | Loss: 0.00001100
Iteration 191/1000 | Loss: 0.00001100
Iteration 192/1000 | Loss: 0.00001100
Iteration 193/1000 | Loss: 0.00001100
Iteration 194/1000 | Loss: 0.00001100
Iteration 195/1000 | Loss: 0.00001100
Iteration 196/1000 | Loss: 0.00001100
Iteration 197/1000 | Loss: 0.00001100
Iteration 198/1000 | Loss: 0.00001100
Iteration 199/1000 | Loss: 0.00001100
Iteration 200/1000 | Loss: 0.00001100
Iteration 201/1000 | Loss: 0.00001100
Iteration 202/1000 | Loss: 0.00001100
Iteration 203/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.0996238415827975e-05, 1.0996238415827975e-05, 1.0996238415827975e-05, 1.0996238415827975e-05, 1.0996238415827975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0996238415827975e-05

Optimization complete. Final v2v error: 2.8590707778930664 mm

Highest mean error: 3.1273610591888428 mm for frame 61

Lowest mean error: 2.6986799240112305 mm for frame 119

Saving results

Total time: 40.75163507461548
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00571356
Iteration 2/25 | Loss: 0.00159907
Iteration 3/25 | Loss: 0.00134890
Iteration 4/25 | Loss: 0.00132859
Iteration 5/25 | Loss: 0.00132495
Iteration 6/25 | Loss: 0.00132430
Iteration 7/25 | Loss: 0.00132430
Iteration 8/25 | Loss: 0.00132430
Iteration 9/25 | Loss: 0.00132430
Iteration 10/25 | Loss: 0.00132430
Iteration 11/25 | Loss: 0.00132430
Iteration 12/25 | Loss: 0.00132430
Iteration 13/25 | Loss: 0.00132430
Iteration 14/25 | Loss: 0.00132430
Iteration 15/25 | Loss: 0.00132430
Iteration 16/25 | Loss: 0.00132430
Iteration 17/25 | Loss: 0.00132430
Iteration 18/25 | Loss: 0.00132430
Iteration 19/25 | Loss: 0.00132430
Iteration 20/25 | Loss: 0.00132430
Iteration 21/25 | Loss: 0.00132430
Iteration 22/25 | Loss: 0.00132430
Iteration 23/25 | Loss: 0.00132430
Iteration 24/25 | Loss: 0.00132430
Iteration 25/25 | Loss: 0.00132430

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58405519
Iteration 2/25 | Loss: 0.00092884
Iteration 3/25 | Loss: 0.00092882
Iteration 4/25 | Loss: 0.00092882
Iteration 5/25 | Loss: 0.00092882
Iteration 6/25 | Loss: 0.00092882
Iteration 7/25 | Loss: 0.00092881
Iteration 8/25 | Loss: 0.00092881
Iteration 9/25 | Loss: 0.00092881
Iteration 10/25 | Loss: 0.00092881
Iteration 11/25 | Loss: 0.00092881
Iteration 12/25 | Loss: 0.00092881
Iteration 13/25 | Loss: 0.00092881
Iteration 14/25 | Loss: 0.00092881
Iteration 15/25 | Loss: 0.00092881
Iteration 16/25 | Loss: 0.00092881
Iteration 17/25 | Loss: 0.00092881
Iteration 18/25 | Loss: 0.00092881
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009288135333918035, 0.0009288135333918035, 0.0009288135333918035, 0.0009288135333918035, 0.0009288135333918035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009288135333918035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092881
Iteration 2/1000 | Loss: 0.00003884
Iteration 3/1000 | Loss: 0.00003055
Iteration 4/1000 | Loss: 0.00002718
Iteration 5/1000 | Loss: 0.00002535
Iteration 6/1000 | Loss: 0.00002384
Iteration 7/1000 | Loss: 0.00002317
Iteration 8/1000 | Loss: 0.00002272
Iteration 9/1000 | Loss: 0.00002235
Iteration 10/1000 | Loss: 0.00002202
Iteration 11/1000 | Loss: 0.00002170
Iteration 12/1000 | Loss: 0.00002143
Iteration 13/1000 | Loss: 0.00002118
Iteration 14/1000 | Loss: 0.00002095
Iteration 15/1000 | Loss: 0.00002079
Iteration 16/1000 | Loss: 0.00002072
Iteration 17/1000 | Loss: 0.00002062
Iteration 18/1000 | Loss: 0.00002052
Iteration 19/1000 | Loss: 0.00002046
Iteration 20/1000 | Loss: 0.00002039
Iteration 21/1000 | Loss: 0.00002035
Iteration 22/1000 | Loss: 0.00002035
Iteration 23/1000 | Loss: 0.00002034
Iteration 24/1000 | Loss: 0.00002032
Iteration 25/1000 | Loss: 0.00002031
Iteration 26/1000 | Loss: 0.00002031
Iteration 27/1000 | Loss: 0.00002031
Iteration 28/1000 | Loss: 0.00002031
Iteration 29/1000 | Loss: 0.00002031
Iteration 30/1000 | Loss: 0.00002031
Iteration 31/1000 | Loss: 0.00002031
Iteration 32/1000 | Loss: 0.00002031
Iteration 33/1000 | Loss: 0.00002031
Iteration 34/1000 | Loss: 0.00002030
Iteration 35/1000 | Loss: 0.00002030
Iteration 36/1000 | Loss: 0.00002030
Iteration 37/1000 | Loss: 0.00002030
Iteration 38/1000 | Loss: 0.00002030
Iteration 39/1000 | Loss: 0.00002028
Iteration 40/1000 | Loss: 0.00002027
Iteration 41/1000 | Loss: 0.00002027
Iteration 42/1000 | Loss: 0.00002027
Iteration 43/1000 | Loss: 0.00002027
Iteration 44/1000 | Loss: 0.00002027
Iteration 45/1000 | Loss: 0.00002026
Iteration 46/1000 | Loss: 0.00002026
Iteration 47/1000 | Loss: 0.00002026
Iteration 48/1000 | Loss: 0.00002026
Iteration 49/1000 | Loss: 0.00002025
Iteration 50/1000 | Loss: 0.00002025
Iteration 51/1000 | Loss: 0.00002025
Iteration 52/1000 | Loss: 0.00002025
Iteration 53/1000 | Loss: 0.00002025
Iteration 54/1000 | Loss: 0.00002025
Iteration 55/1000 | Loss: 0.00002025
Iteration 56/1000 | Loss: 0.00002025
Iteration 57/1000 | Loss: 0.00002025
Iteration 58/1000 | Loss: 0.00002025
Iteration 59/1000 | Loss: 0.00002025
Iteration 60/1000 | Loss: 0.00002024
Iteration 61/1000 | Loss: 0.00002024
Iteration 62/1000 | Loss: 0.00002024
Iteration 63/1000 | Loss: 0.00002023
Iteration 64/1000 | Loss: 0.00002023
Iteration 65/1000 | Loss: 0.00002023
Iteration 66/1000 | Loss: 0.00002022
Iteration 67/1000 | Loss: 0.00002022
Iteration 68/1000 | Loss: 0.00002022
Iteration 69/1000 | Loss: 0.00002022
Iteration 70/1000 | Loss: 0.00002022
Iteration 71/1000 | Loss: 0.00002022
Iteration 72/1000 | Loss: 0.00002022
Iteration 73/1000 | Loss: 0.00002021
Iteration 74/1000 | Loss: 0.00002021
Iteration 75/1000 | Loss: 0.00002021
Iteration 76/1000 | Loss: 0.00002021
Iteration 77/1000 | Loss: 0.00002021
Iteration 78/1000 | Loss: 0.00002021
Iteration 79/1000 | Loss: 0.00002021
Iteration 80/1000 | Loss: 0.00002021
Iteration 81/1000 | Loss: 0.00002021
Iteration 82/1000 | Loss: 0.00002020
Iteration 83/1000 | Loss: 0.00002020
Iteration 84/1000 | Loss: 0.00002020
Iteration 85/1000 | Loss: 0.00002020
Iteration 86/1000 | Loss: 0.00002020
Iteration 87/1000 | Loss: 0.00002020
Iteration 88/1000 | Loss: 0.00002020
Iteration 89/1000 | Loss: 0.00002019
Iteration 90/1000 | Loss: 0.00002019
Iteration 91/1000 | Loss: 0.00002019
Iteration 92/1000 | Loss: 0.00002019
Iteration 93/1000 | Loss: 0.00002019
Iteration 94/1000 | Loss: 0.00002019
Iteration 95/1000 | Loss: 0.00002019
Iteration 96/1000 | Loss: 0.00002019
Iteration 97/1000 | Loss: 0.00002019
Iteration 98/1000 | Loss: 0.00002019
Iteration 99/1000 | Loss: 0.00002019
Iteration 100/1000 | Loss: 0.00002019
Iteration 101/1000 | Loss: 0.00002019
Iteration 102/1000 | Loss: 0.00002019
Iteration 103/1000 | Loss: 0.00002019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.019299427047372e-05, 2.019299427047372e-05, 2.019299427047372e-05, 2.019299427047372e-05, 2.019299427047372e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.019299427047372e-05

Optimization complete. Final v2v error: 3.7349815368652344 mm

Highest mean error: 4.2258453369140625 mm for frame 117

Lowest mean error: 3.0241849422454834 mm for frame 11

Saving results

Total time: 43.068941593170166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013510
Iteration 2/25 | Loss: 0.00381369
Iteration 3/25 | Loss: 0.00200827
Iteration 4/25 | Loss: 0.00143354
Iteration 5/25 | Loss: 0.00129520
Iteration 6/25 | Loss: 0.00126305
Iteration 7/25 | Loss: 0.00127655
Iteration 8/25 | Loss: 0.00127121
Iteration 9/25 | Loss: 0.00127671
Iteration 10/25 | Loss: 0.00125996
Iteration 11/25 | Loss: 0.00126568
Iteration 12/25 | Loss: 0.00125836
Iteration 13/25 | Loss: 0.00126400
Iteration 14/25 | Loss: 0.00125869
Iteration 15/25 | Loss: 0.00125807
Iteration 16/25 | Loss: 0.00125803
Iteration 17/25 | Loss: 0.00125803
Iteration 18/25 | Loss: 0.00125802
Iteration 19/25 | Loss: 0.00125802
Iteration 20/25 | Loss: 0.00125802
Iteration 21/25 | Loss: 0.00125802
Iteration 22/25 | Loss: 0.00125802
Iteration 23/25 | Loss: 0.00125802
Iteration 24/25 | Loss: 0.00125802
Iteration 25/25 | Loss: 0.00125802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30762279
Iteration 2/25 | Loss: 0.00125224
Iteration 3/25 | Loss: 0.00125223
Iteration 4/25 | Loss: 0.00110066
Iteration 5/25 | Loss: 0.00110060
Iteration 6/25 | Loss: 0.00110059
Iteration 7/25 | Loss: 0.00110059
Iteration 8/25 | Loss: 0.00110059
Iteration 9/25 | Loss: 0.00110059
Iteration 10/25 | Loss: 0.00110059
Iteration 11/25 | Loss: 0.00110059
Iteration 12/25 | Loss: 0.00110059
Iteration 13/25 | Loss: 0.00110059
Iteration 14/25 | Loss: 0.00110059
Iteration 15/25 | Loss: 0.00110059
Iteration 16/25 | Loss: 0.00110059
Iteration 17/25 | Loss: 0.00110059
Iteration 18/25 | Loss: 0.00110059
Iteration 19/25 | Loss: 0.00110059
Iteration 20/25 | Loss: 0.00110059
Iteration 21/25 | Loss: 0.00110059
Iteration 22/25 | Loss: 0.00110059
Iteration 23/25 | Loss: 0.00110059
Iteration 24/25 | Loss: 0.00110059
Iteration 25/25 | Loss: 0.00110059

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110059
Iteration 2/1000 | Loss: 0.00016546
Iteration 3/1000 | Loss: 0.00003446
Iteration 4/1000 | Loss: 0.00011095
Iteration 5/1000 | Loss: 0.00031370
Iteration 6/1000 | Loss: 0.00082257
Iteration 7/1000 | Loss: 0.00002650
Iteration 8/1000 | Loss: 0.00006528
Iteration 9/1000 | Loss: 0.00019382
Iteration 10/1000 | Loss: 0.00002255
Iteration 11/1000 | Loss: 0.00004085
Iteration 12/1000 | Loss: 0.00002082
Iteration 13/1000 | Loss: 0.00002020
Iteration 14/1000 | Loss: 0.00021098
Iteration 15/1000 | Loss: 0.00002890
Iteration 16/1000 | Loss: 0.00002241
Iteration 17/1000 | Loss: 0.00015290
Iteration 18/1000 | Loss: 0.00001951
Iteration 19/1000 | Loss: 0.00004530
Iteration 20/1000 | Loss: 0.00009072
Iteration 21/1000 | Loss: 0.00004217
Iteration 22/1000 | Loss: 0.00001886
Iteration 23/1000 | Loss: 0.00001857
Iteration 24/1000 | Loss: 0.00007118
Iteration 25/1000 | Loss: 0.00001934
Iteration 26/1000 | Loss: 0.00001832
Iteration 27/1000 | Loss: 0.00001806
Iteration 28/1000 | Loss: 0.00001787
Iteration 29/1000 | Loss: 0.00001777
Iteration 30/1000 | Loss: 0.00001777
Iteration 31/1000 | Loss: 0.00001777
Iteration 32/1000 | Loss: 0.00001777
Iteration 33/1000 | Loss: 0.00001777
Iteration 34/1000 | Loss: 0.00001776
Iteration 35/1000 | Loss: 0.00001776
Iteration 36/1000 | Loss: 0.00001772
Iteration 37/1000 | Loss: 0.00001772
Iteration 38/1000 | Loss: 0.00001772
Iteration 39/1000 | Loss: 0.00001771
Iteration 40/1000 | Loss: 0.00001759
Iteration 41/1000 | Loss: 0.00010813
Iteration 42/1000 | Loss: 0.00001765
Iteration 43/1000 | Loss: 0.00001750
Iteration 44/1000 | Loss: 0.00001749
Iteration 45/1000 | Loss: 0.00001749
Iteration 46/1000 | Loss: 0.00001749
Iteration 47/1000 | Loss: 0.00001748
Iteration 48/1000 | Loss: 0.00001745
Iteration 49/1000 | Loss: 0.00001744
Iteration 50/1000 | Loss: 0.00001742
Iteration 51/1000 | Loss: 0.00001742
Iteration 52/1000 | Loss: 0.00001739
Iteration 53/1000 | Loss: 0.00008121
Iteration 54/1000 | Loss: 0.00001845
Iteration 55/1000 | Loss: 0.00001766
Iteration 56/1000 | Loss: 0.00001733
Iteration 57/1000 | Loss: 0.00001730
Iteration 58/1000 | Loss: 0.00001730
Iteration 59/1000 | Loss: 0.00001730
Iteration 60/1000 | Loss: 0.00001730
Iteration 61/1000 | Loss: 0.00001730
Iteration 62/1000 | Loss: 0.00001730
Iteration 63/1000 | Loss: 0.00001730
Iteration 64/1000 | Loss: 0.00001726
Iteration 65/1000 | Loss: 0.00001726
Iteration 66/1000 | Loss: 0.00001726
Iteration 67/1000 | Loss: 0.00001726
Iteration 68/1000 | Loss: 0.00001726
Iteration 69/1000 | Loss: 0.00001725
Iteration 70/1000 | Loss: 0.00001725
Iteration 71/1000 | Loss: 0.00001725
Iteration 72/1000 | Loss: 0.00001725
Iteration 73/1000 | Loss: 0.00001725
Iteration 74/1000 | Loss: 0.00001725
Iteration 75/1000 | Loss: 0.00001725
Iteration 76/1000 | Loss: 0.00001725
Iteration 77/1000 | Loss: 0.00001724
Iteration 78/1000 | Loss: 0.00001724
Iteration 79/1000 | Loss: 0.00001723
Iteration 80/1000 | Loss: 0.00001722
Iteration 81/1000 | Loss: 0.00001722
Iteration 82/1000 | Loss: 0.00001722
Iteration 83/1000 | Loss: 0.00001722
Iteration 84/1000 | Loss: 0.00001721
Iteration 85/1000 | Loss: 0.00001721
Iteration 86/1000 | Loss: 0.00001721
Iteration 87/1000 | Loss: 0.00001721
Iteration 88/1000 | Loss: 0.00001721
Iteration 89/1000 | Loss: 0.00001721
Iteration 90/1000 | Loss: 0.00001720
Iteration 91/1000 | Loss: 0.00001718
Iteration 92/1000 | Loss: 0.00001718
Iteration 93/1000 | Loss: 0.00001717
Iteration 94/1000 | Loss: 0.00001714
Iteration 95/1000 | Loss: 0.00001714
Iteration 96/1000 | Loss: 0.00001714
Iteration 97/1000 | Loss: 0.00001713
Iteration 98/1000 | Loss: 0.00001713
Iteration 99/1000 | Loss: 0.00001713
Iteration 100/1000 | Loss: 0.00001713
Iteration 101/1000 | Loss: 0.00001713
Iteration 102/1000 | Loss: 0.00001713
Iteration 103/1000 | Loss: 0.00001713
Iteration 104/1000 | Loss: 0.00001713
Iteration 105/1000 | Loss: 0.00001713
Iteration 106/1000 | Loss: 0.00001712
Iteration 107/1000 | Loss: 0.00001712
Iteration 108/1000 | Loss: 0.00001712
Iteration 109/1000 | Loss: 0.00001712
Iteration 110/1000 | Loss: 0.00001712
Iteration 111/1000 | Loss: 0.00001711
Iteration 112/1000 | Loss: 0.00001711
Iteration 113/1000 | Loss: 0.00001711
Iteration 114/1000 | Loss: 0.00001711
Iteration 115/1000 | Loss: 0.00001711
Iteration 116/1000 | Loss: 0.00001711
Iteration 117/1000 | Loss: 0.00001711
Iteration 118/1000 | Loss: 0.00001710
Iteration 119/1000 | Loss: 0.00001710
Iteration 120/1000 | Loss: 0.00001710
Iteration 121/1000 | Loss: 0.00001710
Iteration 122/1000 | Loss: 0.00001710
Iteration 123/1000 | Loss: 0.00001710
Iteration 124/1000 | Loss: 0.00001710
Iteration 125/1000 | Loss: 0.00001709
Iteration 126/1000 | Loss: 0.00001709
Iteration 127/1000 | Loss: 0.00001709
Iteration 128/1000 | Loss: 0.00001709
Iteration 129/1000 | Loss: 0.00001709
Iteration 130/1000 | Loss: 0.00001709
Iteration 131/1000 | Loss: 0.00001708
Iteration 132/1000 | Loss: 0.00001708
Iteration 133/1000 | Loss: 0.00001708
Iteration 134/1000 | Loss: 0.00001707
Iteration 135/1000 | Loss: 0.00001707
Iteration 136/1000 | Loss: 0.00001707
Iteration 137/1000 | Loss: 0.00001707
Iteration 138/1000 | Loss: 0.00001707
Iteration 139/1000 | Loss: 0.00001707
Iteration 140/1000 | Loss: 0.00001707
Iteration 141/1000 | Loss: 0.00001707
Iteration 142/1000 | Loss: 0.00001706
Iteration 143/1000 | Loss: 0.00001706
Iteration 144/1000 | Loss: 0.00001706
Iteration 145/1000 | Loss: 0.00001706
Iteration 146/1000 | Loss: 0.00001706
Iteration 147/1000 | Loss: 0.00001705
Iteration 148/1000 | Loss: 0.00001705
Iteration 149/1000 | Loss: 0.00001705
Iteration 150/1000 | Loss: 0.00001705
Iteration 151/1000 | Loss: 0.00001705
Iteration 152/1000 | Loss: 0.00001705
Iteration 153/1000 | Loss: 0.00001705
Iteration 154/1000 | Loss: 0.00001705
Iteration 155/1000 | Loss: 0.00001705
Iteration 156/1000 | Loss: 0.00001705
Iteration 157/1000 | Loss: 0.00001705
Iteration 158/1000 | Loss: 0.00001705
Iteration 159/1000 | Loss: 0.00001704
Iteration 160/1000 | Loss: 0.00001703
Iteration 161/1000 | Loss: 0.00001703
Iteration 162/1000 | Loss: 0.00001702
Iteration 163/1000 | Loss: 0.00001702
Iteration 164/1000 | Loss: 0.00001702
Iteration 165/1000 | Loss: 0.00001701
Iteration 166/1000 | Loss: 0.00001701
Iteration 167/1000 | Loss: 0.00001701
Iteration 168/1000 | Loss: 0.00001701
Iteration 169/1000 | Loss: 0.00001701
Iteration 170/1000 | Loss: 0.00001701
Iteration 171/1000 | Loss: 0.00001701
Iteration 172/1000 | Loss: 0.00001701
Iteration 173/1000 | Loss: 0.00001701
Iteration 174/1000 | Loss: 0.00001701
Iteration 175/1000 | Loss: 0.00001701
Iteration 176/1000 | Loss: 0.00001701
Iteration 177/1000 | Loss: 0.00001701
Iteration 178/1000 | Loss: 0.00001701
Iteration 179/1000 | Loss: 0.00001701
Iteration 180/1000 | Loss: 0.00001701
Iteration 181/1000 | Loss: 0.00001701
Iteration 182/1000 | Loss: 0.00001701
Iteration 183/1000 | Loss: 0.00001701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.700951725069899e-05, 1.700951725069899e-05, 1.700951725069899e-05, 1.700951725069899e-05, 1.700951725069899e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.700951725069899e-05

Optimization complete. Final v2v error: 3.2214369773864746 mm

Highest mean error: 11.012459754943848 mm for frame 140

Lowest mean error: 2.916379928588867 mm for frame 4

Saving results

Total time: 89.77291655540466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00759525
Iteration 2/25 | Loss: 0.00171533
Iteration 3/25 | Loss: 0.00137822
Iteration 4/25 | Loss: 0.00137231
Iteration 5/25 | Loss: 0.00140055
Iteration 6/25 | Loss: 0.00133219
Iteration 7/25 | Loss: 0.00130539
Iteration 8/25 | Loss: 0.00129793
Iteration 9/25 | Loss: 0.00129605
Iteration 10/25 | Loss: 0.00129561
Iteration 11/25 | Loss: 0.00129531
Iteration 12/25 | Loss: 0.00129508
Iteration 13/25 | Loss: 0.00129488
Iteration 14/25 | Loss: 0.00129867
Iteration 15/25 | Loss: 0.00129409
Iteration 16/25 | Loss: 0.00129335
Iteration 17/25 | Loss: 0.00129321
Iteration 18/25 | Loss: 0.00129321
Iteration 19/25 | Loss: 0.00129320
Iteration 20/25 | Loss: 0.00129320
Iteration 21/25 | Loss: 0.00129320
Iteration 22/25 | Loss: 0.00129320
Iteration 23/25 | Loss: 0.00129320
Iteration 24/25 | Loss: 0.00129320
Iteration 25/25 | Loss: 0.00129320

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 13.85855770
Iteration 2/25 | Loss: 0.00076451
Iteration 3/25 | Loss: 0.00076430
Iteration 4/25 | Loss: 0.00076430
Iteration 5/25 | Loss: 0.00076429
Iteration 6/25 | Loss: 0.00076429
Iteration 7/25 | Loss: 0.00076429
Iteration 8/25 | Loss: 0.00076429
Iteration 9/25 | Loss: 0.00076429
Iteration 10/25 | Loss: 0.00076429
Iteration 11/25 | Loss: 0.00076429
Iteration 12/25 | Loss: 0.00076429
Iteration 13/25 | Loss: 0.00076429
Iteration 14/25 | Loss: 0.00076429
Iteration 15/25 | Loss: 0.00076429
Iteration 16/25 | Loss: 0.00076429
Iteration 17/25 | Loss: 0.00076429
Iteration 18/25 | Loss: 0.00076429
Iteration 19/25 | Loss: 0.00076429
Iteration 20/25 | Loss: 0.00076429
Iteration 21/25 | Loss: 0.00076429
Iteration 22/25 | Loss: 0.00076429
Iteration 23/25 | Loss: 0.00076429
Iteration 24/25 | Loss: 0.00076429
Iteration 25/25 | Loss: 0.00076429
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007642933633178473, 0.0007642933633178473, 0.0007642933633178473, 0.0007642933633178473, 0.0007642933633178473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007642933633178473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076429
Iteration 2/1000 | Loss: 0.00002782
Iteration 3/1000 | Loss: 0.00002130
Iteration 4/1000 | Loss: 0.00001968
Iteration 5/1000 | Loss: 0.00001858
Iteration 6/1000 | Loss: 0.00001802
Iteration 7/1000 | Loss: 0.00001742
Iteration 8/1000 | Loss: 0.00001700
Iteration 9/1000 | Loss: 0.00001673
Iteration 10/1000 | Loss: 0.00001649
Iteration 11/1000 | Loss: 0.00001641
Iteration 12/1000 | Loss: 0.00001637
Iteration 13/1000 | Loss: 0.00001637
Iteration 14/1000 | Loss: 0.00001636
Iteration 15/1000 | Loss: 0.00001636
Iteration 16/1000 | Loss: 0.00001636
Iteration 17/1000 | Loss: 0.00001636
Iteration 18/1000 | Loss: 0.00001635
Iteration 19/1000 | Loss: 0.00001635
Iteration 20/1000 | Loss: 0.00001634
Iteration 21/1000 | Loss: 0.00001633
Iteration 22/1000 | Loss: 0.00001631
Iteration 23/1000 | Loss: 0.00001631
Iteration 24/1000 | Loss: 0.00001631
Iteration 25/1000 | Loss: 0.00001630
Iteration 26/1000 | Loss: 0.00001630
Iteration 27/1000 | Loss: 0.00001629
Iteration 28/1000 | Loss: 0.00001629
Iteration 29/1000 | Loss: 0.00001628
Iteration 30/1000 | Loss: 0.00001627
Iteration 31/1000 | Loss: 0.00001627
Iteration 32/1000 | Loss: 0.00001627
Iteration 33/1000 | Loss: 0.00001627
Iteration 34/1000 | Loss: 0.00001626
Iteration 35/1000 | Loss: 0.00001626
Iteration 36/1000 | Loss: 0.00001626
Iteration 37/1000 | Loss: 0.00001619
Iteration 38/1000 | Loss: 0.00001617
Iteration 39/1000 | Loss: 0.00001616
Iteration 40/1000 | Loss: 0.00001616
Iteration 41/1000 | Loss: 0.00001616
Iteration 42/1000 | Loss: 0.00001616
Iteration 43/1000 | Loss: 0.00001616
Iteration 44/1000 | Loss: 0.00001616
Iteration 45/1000 | Loss: 0.00001616
Iteration 46/1000 | Loss: 0.00001616
Iteration 47/1000 | Loss: 0.00001615
Iteration 48/1000 | Loss: 0.00001615
Iteration 49/1000 | Loss: 0.00001615
Iteration 50/1000 | Loss: 0.00001614
Iteration 51/1000 | Loss: 0.00001607
Iteration 52/1000 | Loss: 0.00001602
Iteration 53/1000 | Loss: 0.00001602
Iteration 54/1000 | Loss: 0.00001602
Iteration 55/1000 | Loss: 0.00001602
Iteration 56/1000 | Loss: 0.00001602
Iteration 57/1000 | Loss: 0.00001602
Iteration 58/1000 | Loss: 0.00001601
Iteration 59/1000 | Loss: 0.00001601
Iteration 60/1000 | Loss: 0.00001601
Iteration 61/1000 | Loss: 0.00001601
Iteration 62/1000 | Loss: 0.00001600
Iteration 63/1000 | Loss: 0.00001599
Iteration 64/1000 | Loss: 0.00001598
Iteration 65/1000 | Loss: 0.00001598
Iteration 66/1000 | Loss: 0.00001597
Iteration 67/1000 | Loss: 0.00001597
Iteration 68/1000 | Loss: 0.00001597
Iteration 69/1000 | Loss: 0.00001597
Iteration 70/1000 | Loss: 0.00001596
Iteration 71/1000 | Loss: 0.00001595
Iteration 72/1000 | Loss: 0.00001595
Iteration 73/1000 | Loss: 0.00001595
Iteration 74/1000 | Loss: 0.00001594
Iteration 75/1000 | Loss: 0.00001594
Iteration 76/1000 | Loss: 0.00001594
Iteration 77/1000 | Loss: 0.00001594
Iteration 78/1000 | Loss: 0.00001593
Iteration 79/1000 | Loss: 0.00001593
Iteration 80/1000 | Loss: 0.00001592
Iteration 81/1000 | Loss: 0.00001592
Iteration 82/1000 | Loss: 0.00001592
Iteration 83/1000 | Loss: 0.00001591
Iteration 84/1000 | Loss: 0.00001590
Iteration 85/1000 | Loss: 0.00001590
Iteration 86/1000 | Loss: 0.00001590
Iteration 87/1000 | Loss: 0.00001590
Iteration 88/1000 | Loss: 0.00001589
Iteration 89/1000 | Loss: 0.00001589
Iteration 90/1000 | Loss: 0.00001589
Iteration 91/1000 | Loss: 0.00001589
Iteration 92/1000 | Loss: 0.00001589
Iteration 93/1000 | Loss: 0.00001589
Iteration 94/1000 | Loss: 0.00001589
Iteration 95/1000 | Loss: 0.00001589
Iteration 96/1000 | Loss: 0.00001588
Iteration 97/1000 | Loss: 0.00001588
Iteration 98/1000 | Loss: 0.00001587
Iteration 99/1000 | Loss: 0.00001586
Iteration 100/1000 | Loss: 0.00001586
Iteration 101/1000 | Loss: 0.00001586
Iteration 102/1000 | Loss: 0.00001586
Iteration 103/1000 | Loss: 0.00001585
Iteration 104/1000 | Loss: 0.00001585
Iteration 105/1000 | Loss: 0.00001585
Iteration 106/1000 | Loss: 0.00001585
Iteration 107/1000 | Loss: 0.00001585
Iteration 108/1000 | Loss: 0.00001585
Iteration 109/1000 | Loss: 0.00001584
Iteration 110/1000 | Loss: 0.00001584
Iteration 111/1000 | Loss: 0.00001584
Iteration 112/1000 | Loss: 0.00001583
Iteration 113/1000 | Loss: 0.00001583
Iteration 114/1000 | Loss: 0.00001583
Iteration 115/1000 | Loss: 0.00001583
Iteration 116/1000 | Loss: 0.00001583
Iteration 117/1000 | Loss: 0.00001583
Iteration 118/1000 | Loss: 0.00001583
Iteration 119/1000 | Loss: 0.00001583
Iteration 120/1000 | Loss: 0.00001583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.5833751604077406e-05, 1.5833751604077406e-05, 1.5833751604077406e-05, 1.5833751604077406e-05, 1.5833751604077406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5833751604077406e-05

Optimization complete. Final v2v error: 3.390310049057007 mm

Highest mean error: 3.645106792449951 mm for frame 12

Lowest mean error: 3.141101837158203 mm for frame 96

Saving results

Total time: 64.63779640197754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839231
Iteration 2/25 | Loss: 0.00126298
Iteration 3/25 | Loss: 0.00121387
Iteration 4/25 | Loss: 0.00120636
Iteration 5/25 | Loss: 0.00120418
Iteration 6/25 | Loss: 0.00120406
Iteration 7/25 | Loss: 0.00120406
Iteration 8/25 | Loss: 0.00120406
Iteration 9/25 | Loss: 0.00120406
Iteration 10/25 | Loss: 0.00120406
Iteration 11/25 | Loss: 0.00120406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001204063999466598, 0.001204063999466598, 0.001204063999466598, 0.001204063999466598, 0.001204063999466598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001204063999466598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65141523
Iteration 2/25 | Loss: 0.00102814
Iteration 3/25 | Loss: 0.00102813
Iteration 4/25 | Loss: 0.00102813
Iteration 5/25 | Loss: 0.00102813
Iteration 6/25 | Loss: 0.00102813
Iteration 7/25 | Loss: 0.00102813
Iteration 8/25 | Loss: 0.00102813
Iteration 9/25 | Loss: 0.00102813
Iteration 10/25 | Loss: 0.00102813
Iteration 11/25 | Loss: 0.00102813
Iteration 12/25 | Loss: 0.00102813
Iteration 13/25 | Loss: 0.00102813
Iteration 14/25 | Loss: 0.00102813
Iteration 15/25 | Loss: 0.00102813
Iteration 16/25 | Loss: 0.00102813
Iteration 17/25 | Loss: 0.00102813
Iteration 18/25 | Loss: 0.00102813
Iteration 19/25 | Loss: 0.00102813
Iteration 20/25 | Loss: 0.00102813
Iteration 21/25 | Loss: 0.00102813
Iteration 22/25 | Loss: 0.00102813
Iteration 23/25 | Loss: 0.00102813
Iteration 24/25 | Loss: 0.00102813
Iteration 25/25 | Loss: 0.00102813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102813
Iteration 2/1000 | Loss: 0.00002425
Iteration 3/1000 | Loss: 0.00001668
Iteration 4/1000 | Loss: 0.00001435
Iteration 5/1000 | Loss: 0.00001334
Iteration 6/1000 | Loss: 0.00001269
Iteration 7/1000 | Loss: 0.00001223
Iteration 8/1000 | Loss: 0.00001186
Iteration 9/1000 | Loss: 0.00001174
Iteration 10/1000 | Loss: 0.00001170
Iteration 11/1000 | Loss: 0.00001163
Iteration 12/1000 | Loss: 0.00001162
Iteration 13/1000 | Loss: 0.00001140
Iteration 14/1000 | Loss: 0.00001139
Iteration 15/1000 | Loss: 0.00001129
Iteration 16/1000 | Loss: 0.00001129
Iteration 17/1000 | Loss: 0.00001123
Iteration 18/1000 | Loss: 0.00001121
Iteration 19/1000 | Loss: 0.00001121
Iteration 20/1000 | Loss: 0.00001119
Iteration 21/1000 | Loss: 0.00001115
Iteration 22/1000 | Loss: 0.00001114
Iteration 23/1000 | Loss: 0.00001111
Iteration 24/1000 | Loss: 0.00001107
Iteration 25/1000 | Loss: 0.00001106
Iteration 26/1000 | Loss: 0.00001103
Iteration 27/1000 | Loss: 0.00001098
Iteration 28/1000 | Loss: 0.00001098
Iteration 29/1000 | Loss: 0.00001097
Iteration 30/1000 | Loss: 0.00001097
Iteration 31/1000 | Loss: 0.00001096
Iteration 32/1000 | Loss: 0.00001095
Iteration 33/1000 | Loss: 0.00001095
Iteration 34/1000 | Loss: 0.00001095
Iteration 35/1000 | Loss: 0.00001094
Iteration 36/1000 | Loss: 0.00001093
Iteration 37/1000 | Loss: 0.00001093
Iteration 38/1000 | Loss: 0.00001092
Iteration 39/1000 | Loss: 0.00001091
Iteration 40/1000 | Loss: 0.00001091
Iteration 41/1000 | Loss: 0.00001091
Iteration 42/1000 | Loss: 0.00001090
Iteration 43/1000 | Loss: 0.00001090
Iteration 44/1000 | Loss: 0.00001087
Iteration 45/1000 | Loss: 0.00001086
Iteration 46/1000 | Loss: 0.00001086
Iteration 47/1000 | Loss: 0.00001085
Iteration 48/1000 | Loss: 0.00001085
Iteration 49/1000 | Loss: 0.00001085
Iteration 50/1000 | Loss: 0.00001084
Iteration 51/1000 | Loss: 0.00001083
Iteration 52/1000 | Loss: 0.00001082
Iteration 53/1000 | Loss: 0.00001081
Iteration 54/1000 | Loss: 0.00001080
Iteration 55/1000 | Loss: 0.00001080
Iteration 56/1000 | Loss: 0.00001079
Iteration 57/1000 | Loss: 0.00001079
Iteration 58/1000 | Loss: 0.00001078
Iteration 59/1000 | Loss: 0.00001077
Iteration 60/1000 | Loss: 0.00001076
Iteration 61/1000 | Loss: 0.00001076
Iteration 62/1000 | Loss: 0.00001076
Iteration 63/1000 | Loss: 0.00001075
Iteration 64/1000 | Loss: 0.00001075
Iteration 65/1000 | Loss: 0.00001075
Iteration 66/1000 | Loss: 0.00001072
Iteration 67/1000 | Loss: 0.00001072
Iteration 68/1000 | Loss: 0.00001072
Iteration 69/1000 | Loss: 0.00001071
Iteration 70/1000 | Loss: 0.00001070
Iteration 71/1000 | Loss: 0.00001067
Iteration 72/1000 | Loss: 0.00001067
Iteration 73/1000 | Loss: 0.00001067
Iteration 74/1000 | Loss: 0.00001066
Iteration 75/1000 | Loss: 0.00001066
Iteration 76/1000 | Loss: 0.00001065
Iteration 77/1000 | Loss: 0.00001060
Iteration 78/1000 | Loss: 0.00001060
Iteration 79/1000 | Loss: 0.00001060
Iteration 80/1000 | Loss: 0.00001060
Iteration 81/1000 | Loss: 0.00001060
Iteration 82/1000 | Loss: 0.00001060
Iteration 83/1000 | Loss: 0.00001060
Iteration 84/1000 | Loss: 0.00001059
Iteration 85/1000 | Loss: 0.00001058
Iteration 86/1000 | Loss: 0.00001057
Iteration 87/1000 | Loss: 0.00001057
Iteration 88/1000 | Loss: 0.00001056
Iteration 89/1000 | Loss: 0.00001056
Iteration 90/1000 | Loss: 0.00001056
Iteration 91/1000 | Loss: 0.00001056
Iteration 92/1000 | Loss: 0.00001056
Iteration 93/1000 | Loss: 0.00001055
Iteration 94/1000 | Loss: 0.00001054
Iteration 95/1000 | Loss: 0.00001054
Iteration 96/1000 | Loss: 0.00001054
Iteration 97/1000 | Loss: 0.00001054
Iteration 98/1000 | Loss: 0.00001054
Iteration 99/1000 | Loss: 0.00001054
Iteration 100/1000 | Loss: 0.00001054
Iteration 101/1000 | Loss: 0.00001054
Iteration 102/1000 | Loss: 0.00001053
Iteration 103/1000 | Loss: 0.00001053
Iteration 104/1000 | Loss: 0.00001053
Iteration 105/1000 | Loss: 0.00001053
Iteration 106/1000 | Loss: 0.00001052
Iteration 107/1000 | Loss: 0.00001052
Iteration 108/1000 | Loss: 0.00001052
Iteration 109/1000 | Loss: 0.00001052
Iteration 110/1000 | Loss: 0.00001052
Iteration 111/1000 | Loss: 0.00001052
Iteration 112/1000 | Loss: 0.00001051
Iteration 113/1000 | Loss: 0.00001051
Iteration 114/1000 | Loss: 0.00001051
Iteration 115/1000 | Loss: 0.00001051
Iteration 116/1000 | Loss: 0.00001051
Iteration 117/1000 | Loss: 0.00001051
Iteration 118/1000 | Loss: 0.00001051
Iteration 119/1000 | Loss: 0.00001051
Iteration 120/1000 | Loss: 0.00001050
Iteration 121/1000 | Loss: 0.00001050
Iteration 122/1000 | Loss: 0.00001050
Iteration 123/1000 | Loss: 0.00001050
Iteration 124/1000 | Loss: 0.00001050
Iteration 125/1000 | Loss: 0.00001050
Iteration 126/1000 | Loss: 0.00001049
Iteration 127/1000 | Loss: 0.00001049
Iteration 128/1000 | Loss: 0.00001049
Iteration 129/1000 | Loss: 0.00001049
Iteration 130/1000 | Loss: 0.00001049
Iteration 131/1000 | Loss: 0.00001049
Iteration 132/1000 | Loss: 0.00001049
Iteration 133/1000 | Loss: 0.00001049
Iteration 134/1000 | Loss: 0.00001049
Iteration 135/1000 | Loss: 0.00001049
Iteration 136/1000 | Loss: 0.00001049
Iteration 137/1000 | Loss: 0.00001049
Iteration 138/1000 | Loss: 0.00001049
Iteration 139/1000 | Loss: 0.00001049
Iteration 140/1000 | Loss: 0.00001048
Iteration 141/1000 | Loss: 0.00001048
Iteration 142/1000 | Loss: 0.00001048
Iteration 143/1000 | Loss: 0.00001048
Iteration 144/1000 | Loss: 0.00001048
Iteration 145/1000 | Loss: 0.00001048
Iteration 146/1000 | Loss: 0.00001047
Iteration 147/1000 | Loss: 0.00001047
Iteration 148/1000 | Loss: 0.00001047
Iteration 149/1000 | Loss: 0.00001047
Iteration 150/1000 | Loss: 0.00001046
Iteration 151/1000 | Loss: 0.00001046
Iteration 152/1000 | Loss: 0.00001046
Iteration 153/1000 | Loss: 0.00001046
Iteration 154/1000 | Loss: 0.00001046
Iteration 155/1000 | Loss: 0.00001046
Iteration 156/1000 | Loss: 0.00001046
Iteration 157/1000 | Loss: 0.00001046
Iteration 158/1000 | Loss: 0.00001046
Iteration 159/1000 | Loss: 0.00001046
Iteration 160/1000 | Loss: 0.00001046
Iteration 161/1000 | Loss: 0.00001046
Iteration 162/1000 | Loss: 0.00001046
Iteration 163/1000 | Loss: 0.00001046
Iteration 164/1000 | Loss: 0.00001046
Iteration 165/1000 | Loss: 0.00001046
Iteration 166/1000 | Loss: 0.00001046
Iteration 167/1000 | Loss: 0.00001045
Iteration 168/1000 | Loss: 0.00001045
Iteration 169/1000 | Loss: 0.00001045
Iteration 170/1000 | Loss: 0.00001045
Iteration 171/1000 | Loss: 0.00001045
Iteration 172/1000 | Loss: 0.00001045
Iteration 173/1000 | Loss: 0.00001045
Iteration 174/1000 | Loss: 0.00001045
Iteration 175/1000 | Loss: 0.00001045
Iteration 176/1000 | Loss: 0.00001045
Iteration 177/1000 | Loss: 0.00001045
Iteration 178/1000 | Loss: 0.00001045
Iteration 179/1000 | Loss: 0.00001045
Iteration 180/1000 | Loss: 0.00001045
Iteration 181/1000 | Loss: 0.00001045
Iteration 182/1000 | Loss: 0.00001045
Iteration 183/1000 | Loss: 0.00001045
Iteration 184/1000 | Loss: 0.00001045
Iteration 185/1000 | Loss: 0.00001045
Iteration 186/1000 | Loss: 0.00001045
Iteration 187/1000 | Loss: 0.00001045
Iteration 188/1000 | Loss: 0.00001044
Iteration 189/1000 | Loss: 0.00001044
Iteration 190/1000 | Loss: 0.00001044
Iteration 191/1000 | Loss: 0.00001044
Iteration 192/1000 | Loss: 0.00001044
Iteration 193/1000 | Loss: 0.00001044
Iteration 194/1000 | Loss: 0.00001044
Iteration 195/1000 | Loss: 0.00001044
Iteration 196/1000 | Loss: 0.00001044
Iteration 197/1000 | Loss: 0.00001044
Iteration 198/1000 | Loss: 0.00001044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.0444899089634418e-05, 1.0444899089634418e-05, 1.0444899089634418e-05, 1.0444899089634418e-05, 1.0444899089634418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0444899089634418e-05

Optimization complete. Final v2v error: 2.755160093307495 mm

Highest mean error: 3.4984302520751953 mm for frame 44

Lowest mean error: 2.5881028175354004 mm for frame 99

Saving results

Total time: 40.261961221694946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00684665
Iteration 2/25 | Loss: 0.00159472
Iteration 3/25 | Loss: 0.00134969
Iteration 4/25 | Loss: 0.00131685
Iteration 5/25 | Loss: 0.00130696
Iteration 6/25 | Loss: 0.00129871
Iteration 7/25 | Loss: 0.00128909
Iteration 8/25 | Loss: 0.00128418
Iteration 9/25 | Loss: 0.00127815
Iteration 10/25 | Loss: 0.00127644
Iteration 11/25 | Loss: 0.00127635
Iteration 12/25 | Loss: 0.00127635
Iteration 13/25 | Loss: 0.00127635
Iteration 14/25 | Loss: 0.00127635
Iteration 15/25 | Loss: 0.00127635
Iteration 16/25 | Loss: 0.00127635
Iteration 17/25 | Loss: 0.00127635
Iteration 18/25 | Loss: 0.00127635
Iteration 19/25 | Loss: 0.00127635
Iteration 20/25 | Loss: 0.00127634
Iteration 21/25 | Loss: 0.00127634
Iteration 22/25 | Loss: 0.00127634
Iteration 23/25 | Loss: 0.00127634
Iteration 24/25 | Loss: 0.00127634
Iteration 25/25 | Loss: 0.00127634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04400611
Iteration 2/25 | Loss: 0.00125554
Iteration 3/25 | Loss: 0.00125524
Iteration 4/25 | Loss: 0.00125524
Iteration 5/25 | Loss: 0.00125524
Iteration 6/25 | Loss: 0.00125524
Iteration 7/25 | Loss: 0.00125524
Iteration 8/25 | Loss: 0.00125524
Iteration 9/25 | Loss: 0.00125523
Iteration 10/25 | Loss: 0.00125523
Iteration 11/25 | Loss: 0.00125523
Iteration 12/25 | Loss: 0.00125523
Iteration 13/25 | Loss: 0.00125523
Iteration 14/25 | Loss: 0.00125523
Iteration 15/25 | Loss: 0.00125523
Iteration 16/25 | Loss: 0.00125523
Iteration 17/25 | Loss: 0.00125523
Iteration 18/25 | Loss: 0.00125523
Iteration 19/25 | Loss: 0.00125523
Iteration 20/25 | Loss: 0.00125523
Iteration 21/25 | Loss: 0.00125523
Iteration 22/25 | Loss: 0.00125523
Iteration 23/25 | Loss: 0.00125523
Iteration 24/25 | Loss: 0.00125523
Iteration 25/25 | Loss: 0.00125523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125523
Iteration 2/1000 | Loss: 0.00006149
Iteration 3/1000 | Loss: 0.00003962
Iteration 4/1000 | Loss: 0.00003270
Iteration 5/1000 | Loss: 0.00002966
Iteration 6/1000 | Loss: 0.00002780
Iteration 7/1000 | Loss: 0.00002668
Iteration 8/1000 | Loss: 0.00002584
Iteration 9/1000 | Loss: 0.00002512
Iteration 10/1000 | Loss: 0.00002468
Iteration 11/1000 | Loss: 0.00002423
Iteration 12/1000 | Loss: 0.00002393
Iteration 13/1000 | Loss: 0.00002366
Iteration 14/1000 | Loss: 0.00002352
Iteration 15/1000 | Loss: 0.00002337
Iteration 16/1000 | Loss: 0.00002324
Iteration 17/1000 | Loss: 0.00002313
Iteration 18/1000 | Loss: 0.00002308
Iteration 19/1000 | Loss: 0.00002307
Iteration 20/1000 | Loss: 0.00002307
Iteration 21/1000 | Loss: 0.00002306
Iteration 22/1000 | Loss: 0.00002293
Iteration 23/1000 | Loss: 0.00002293
Iteration 24/1000 | Loss: 0.00002289
Iteration 25/1000 | Loss: 0.00002286
Iteration 26/1000 | Loss: 0.00002286
Iteration 27/1000 | Loss: 0.00002284
Iteration 28/1000 | Loss: 0.00002284
Iteration 29/1000 | Loss: 0.00002281
Iteration 30/1000 | Loss: 0.00002279
Iteration 31/1000 | Loss: 0.00002278
Iteration 32/1000 | Loss: 0.00002278
Iteration 33/1000 | Loss: 0.00002269
Iteration 34/1000 | Loss: 0.00002269
Iteration 35/1000 | Loss: 0.00002267
Iteration 36/1000 | Loss: 0.00002267
Iteration 37/1000 | Loss: 0.00002266
Iteration 38/1000 | Loss: 0.00002265
Iteration 39/1000 | Loss: 0.00002265
Iteration 40/1000 | Loss: 0.00002265
Iteration 41/1000 | Loss: 0.00002265
Iteration 42/1000 | Loss: 0.00002265
Iteration 43/1000 | Loss: 0.00002265
Iteration 44/1000 | Loss: 0.00002264
Iteration 45/1000 | Loss: 0.00002264
Iteration 46/1000 | Loss: 0.00002264
Iteration 47/1000 | Loss: 0.00002263
Iteration 48/1000 | Loss: 0.00002263
Iteration 49/1000 | Loss: 0.00002262
Iteration 50/1000 | Loss: 0.00002262
Iteration 51/1000 | Loss: 0.00002261
Iteration 52/1000 | Loss: 0.00002260
Iteration 53/1000 | Loss: 0.00002260
Iteration 54/1000 | Loss: 0.00002260
Iteration 55/1000 | Loss: 0.00002260
Iteration 56/1000 | Loss: 0.00002259
Iteration 57/1000 | Loss: 0.00002259
Iteration 58/1000 | Loss: 0.00002259
Iteration 59/1000 | Loss: 0.00002259
Iteration 60/1000 | Loss: 0.00002259
Iteration 61/1000 | Loss: 0.00002259
Iteration 62/1000 | Loss: 0.00002259
Iteration 63/1000 | Loss: 0.00002259
Iteration 64/1000 | Loss: 0.00002259
Iteration 65/1000 | Loss: 0.00002259
Iteration 66/1000 | Loss: 0.00002259
Iteration 67/1000 | Loss: 0.00002258
Iteration 68/1000 | Loss: 0.00002258
Iteration 69/1000 | Loss: 0.00002258
Iteration 70/1000 | Loss: 0.00002258
Iteration 71/1000 | Loss: 0.00002258
Iteration 72/1000 | Loss: 0.00002258
Iteration 73/1000 | Loss: 0.00002258
Iteration 74/1000 | Loss: 0.00002258
Iteration 75/1000 | Loss: 0.00002258
Iteration 76/1000 | Loss: 0.00002257
Iteration 77/1000 | Loss: 0.00002257
Iteration 78/1000 | Loss: 0.00002256
Iteration 79/1000 | Loss: 0.00002255
Iteration 80/1000 | Loss: 0.00002254
Iteration 81/1000 | Loss: 0.00002254
Iteration 82/1000 | Loss: 0.00002253
Iteration 83/1000 | Loss: 0.00002253
Iteration 84/1000 | Loss: 0.00002252
Iteration 85/1000 | Loss: 0.00002252
Iteration 86/1000 | Loss: 0.00002252
Iteration 87/1000 | Loss: 0.00002252
Iteration 88/1000 | Loss: 0.00002252
Iteration 89/1000 | Loss: 0.00002252
Iteration 90/1000 | Loss: 0.00002252
Iteration 91/1000 | Loss: 0.00002252
Iteration 92/1000 | Loss: 0.00002252
Iteration 93/1000 | Loss: 0.00002252
Iteration 94/1000 | Loss: 0.00002252
Iteration 95/1000 | Loss: 0.00002251
Iteration 96/1000 | Loss: 0.00002251
Iteration 97/1000 | Loss: 0.00002251
Iteration 98/1000 | Loss: 0.00002250
Iteration 99/1000 | Loss: 0.00002250
Iteration 100/1000 | Loss: 0.00002249
Iteration 101/1000 | Loss: 0.00002249
Iteration 102/1000 | Loss: 0.00002249
Iteration 103/1000 | Loss: 0.00002248
Iteration 104/1000 | Loss: 0.00002248
Iteration 105/1000 | Loss: 0.00002248
Iteration 106/1000 | Loss: 0.00002247
Iteration 107/1000 | Loss: 0.00002247
Iteration 108/1000 | Loss: 0.00002247
Iteration 109/1000 | Loss: 0.00002247
Iteration 110/1000 | Loss: 0.00002247
Iteration 111/1000 | Loss: 0.00002247
Iteration 112/1000 | Loss: 0.00002246
Iteration 113/1000 | Loss: 0.00002246
Iteration 114/1000 | Loss: 0.00002246
Iteration 115/1000 | Loss: 0.00002246
Iteration 116/1000 | Loss: 0.00002246
Iteration 117/1000 | Loss: 0.00002245
Iteration 118/1000 | Loss: 0.00002245
Iteration 119/1000 | Loss: 0.00002245
Iteration 120/1000 | Loss: 0.00002244
Iteration 121/1000 | Loss: 0.00002244
Iteration 122/1000 | Loss: 0.00002244
Iteration 123/1000 | Loss: 0.00002243
Iteration 124/1000 | Loss: 0.00002243
Iteration 125/1000 | Loss: 0.00002243
Iteration 126/1000 | Loss: 0.00002243
Iteration 127/1000 | Loss: 0.00002243
Iteration 128/1000 | Loss: 0.00002243
Iteration 129/1000 | Loss: 0.00002243
Iteration 130/1000 | Loss: 0.00002243
Iteration 131/1000 | Loss: 0.00002243
Iteration 132/1000 | Loss: 0.00002243
Iteration 133/1000 | Loss: 0.00002243
Iteration 134/1000 | Loss: 0.00002243
Iteration 135/1000 | Loss: 0.00002243
Iteration 136/1000 | Loss: 0.00002243
Iteration 137/1000 | Loss: 0.00002243
Iteration 138/1000 | Loss: 0.00002243
Iteration 139/1000 | Loss: 0.00002243
Iteration 140/1000 | Loss: 0.00002243
Iteration 141/1000 | Loss: 0.00002243
Iteration 142/1000 | Loss: 0.00002243
Iteration 143/1000 | Loss: 0.00002243
Iteration 144/1000 | Loss: 0.00002243
Iteration 145/1000 | Loss: 0.00002243
Iteration 146/1000 | Loss: 0.00002243
Iteration 147/1000 | Loss: 0.00002243
Iteration 148/1000 | Loss: 0.00002243
Iteration 149/1000 | Loss: 0.00002243
Iteration 150/1000 | Loss: 0.00002243
Iteration 151/1000 | Loss: 0.00002243
Iteration 152/1000 | Loss: 0.00002243
Iteration 153/1000 | Loss: 0.00002243
Iteration 154/1000 | Loss: 0.00002243
Iteration 155/1000 | Loss: 0.00002243
Iteration 156/1000 | Loss: 0.00002243
Iteration 157/1000 | Loss: 0.00002243
Iteration 158/1000 | Loss: 0.00002243
Iteration 159/1000 | Loss: 0.00002243
Iteration 160/1000 | Loss: 0.00002243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.2428017473430373e-05, 2.2428017473430373e-05, 2.2428017473430373e-05, 2.2428017473430373e-05, 2.2428017473430373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2428017473430373e-05

Optimization complete. Final v2v error: 3.830848217010498 mm

Highest mean error: 6.113443374633789 mm for frame 132

Lowest mean error: 2.893233060836792 mm for frame 177

Saving results

Total time: 64.36466574668884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444600
Iteration 2/25 | Loss: 0.00130064
Iteration 3/25 | Loss: 0.00124607
Iteration 4/25 | Loss: 0.00123298
Iteration 5/25 | Loss: 0.00122976
Iteration 6/25 | Loss: 0.00122976
Iteration 7/25 | Loss: 0.00122976
Iteration 8/25 | Loss: 0.00122976
Iteration 9/25 | Loss: 0.00122976
Iteration 10/25 | Loss: 0.00122976
Iteration 11/25 | Loss: 0.00122976
Iteration 12/25 | Loss: 0.00122976
Iteration 13/25 | Loss: 0.00122976
Iteration 14/25 | Loss: 0.00122976
Iteration 15/25 | Loss: 0.00122976
Iteration 16/25 | Loss: 0.00122976
Iteration 17/25 | Loss: 0.00122976
Iteration 18/25 | Loss: 0.00122976
Iteration 19/25 | Loss: 0.00122976
Iteration 20/25 | Loss: 0.00122976
Iteration 21/25 | Loss: 0.00122976
Iteration 22/25 | Loss: 0.00122976
Iteration 23/25 | Loss: 0.00122976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001229755813255906, 0.001229755813255906, 0.001229755813255906, 0.001229755813255906, 0.001229755813255906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001229755813255906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35738134
Iteration 2/25 | Loss: 0.00101436
Iteration 3/25 | Loss: 0.00101436
Iteration 4/25 | Loss: 0.00101436
Iteration 5/25 | Loss: 0.00101436
Iteration 6/25 | Loss: 0.00101436
Iteration 7/25 | Loss: 0.00101436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 7. Stopping optimization.
Last 5 losses: [0.0010143578983843327, 0.0010143578983843327, 0.0010143578983843327, 0.0010143578983843327, 0.0010143578983843327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010143578983843327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101436
Iteration 2/1000 | Loss: 0.00002677
Iteration 3/1000 | Loss: 0.00002063
Iteration 4/1000 | Loss: 0.00001903
Iteration 5/1000 | Loss: 0.00001793
Iteration 6/1000 | Loss: 0.00001732
Iteration 7/1000 | Loss: 0.00001682
Iteration 8/1000 | Loss: 0.00001671
Iteration 9/1000 | Loss: 0.00001634
Iteration 10/1000 | Loss: 0.00001585
Iteration 11/1000 | Loss: 0.00001573
Iteration 12/1000 | Loss: 0.00001569
Iteration 13/1000 | Loss: 0.00001560
Iteration 14/1000 | Loss: 0.00001559
Iteration 15/1000 | Loss: 0.00001558
Iteration 16/1000 | Loss: 0.00001558
Iteration 17/1000 | Loss: 0.00001542
Iteration 18/1000 | Loss: 0.00001526
Iteration 19/1000 | Loss: 0.00001521
Iteration 20/1000 | Loss: 0.00001514
Iteration 21/1000 | Loss: 0.00001513
Iteration 22/1000 | Loss: 0.00001505
Iteration 23/1000 | Loss: 0.00001504
Iteration 24/1000 | Loss: 0.00001503
Iteration 25/1000 | Loss: 0.00001501
Iteration 26/1000 | Loss: 0.00001500
Iteration 27/1000 | Loss: 0.00001491
Iteration 28/1000 | Loss: 0.00001489
Iteration 29/1000 | Loss: 0.00001485
Iteration 30/1000 | Loss: 0.00001485
Iteration 31/1000 | Loss: 0.00001485
Iteration 32/1000 | Loss: 0.00001485
Iteration 33/1000 | Loss: 0.00001484
Iteration 34/1000 | Loss: 0.00001483
Iteration 35/1000 | Loss: 0.00001480
Iteration 36/1000 | Loss: 0.00001480
Iteration 37/1000 | Loss: 0.00001480
Iteration 38/1000 | Loss: 0.00001476
Iteration 39/1000 | Loss: 0.00001467
Iteration 40/1000 | Loss: 0.00001466
Iteration 41/1000 | Loss: 0.00001463
Iteration 42/1000 | Loss: 0.00001462
Iteration 43/1000 | Loss: 0.00001461
Iteration 44/1000 | Loss: 0.00001458
Iteration 45/1000 | Loss: 0.00001458
Iteration 46/1000 | Loss: 0.00001458
Iteration 47/1000 | Loss: 0.00001458
Iteration 48/1000 | Loss: 0.00001458
Iteration 49/1000 | Loss: 0.00001458
Iteration 50/1000 | Loss: 0.00001457
Iteration 51/1000 | Loss: 0.00001457
Iteration 52/1000 | Loss: 0.00001457
Iteration 53/1000 | Loss: 0.00001457
Iteration 54/1000 | Loss: 0.00001457
Iteration 55/1000 | Loss: 0.00001457
Iteration 56/1000 | Loss: 0.00001456
Iteration 57/1000 | Loss: 0.00001455
Iteration 58/1000 | Loss: 0.00001454
Iteration 59/1000 | Loss: 0.00001454
Iteration 60/1000 | Loss: 0.00001454
Iteration 61/1000 | Loss: 0.00001453
Iteration 62/1000 | Loss: 0.00001453
Iteration 63/1000 | Loss: 0.00001453
Iteration 64/1000 | Loss: 0.00001453
Iteration 65/1000 | Loss: 0.00001453
Iteration 66/1000 | Loss: 0.00001453
Iteration 67/1000 | Loss: 0.00001453
Iteration 68/1000 | Loss: 0.00001452
Iteration 69/1000 | Loss: 0.00001452
Iteration 70/1000 | Loss: 0.00001451
Iteration 71/1000 | Loss: 0.00001451
Iteration 72/1000 | Loss: 0.00001451
Iteration 73/1000 | Loss: 0.00001451
Iteration 74/1000 | Loss: 0.00001451
Iteration 75/1000 | Loss: 0.00001451
Iteration 76/1000 | Loss: 0.00001451
Iteration 77/1000 | Loss: 0.00001450
Iteration 78/1000 | Loss: 0.00001450
Iteration 79/1000 | Loss: 0.00001449
Iteration 80/1000 | Loss: 0.00001449
Iteration 81/1000 | Loss: 0.00001449
Iteration 82/1000 | Loss: 0.00001448
Iteration 83/1000 | Loss: 0.00001448
Iteration 84/1000 | Loss: 0.00001448
Iteration 85/1000 | Loss: 0.00001447
Iteration 86/1000 | Loss: 0.00001447
Iteration 87/1000 | Loss: 0.00001447
Iteration 88/1000 | Loss: 0.00001447
Iteration 89/1000 | Loss: 0.00001446
Iteration 90/1000 | Loss: 0.00001446
Iteration 91/1000 | Loss: 0.00001446
Iteration 92/1000 | Loss: 0.00001446
Iteration 93/1000 | Loss: 0.00001446
Iteration 94/1000 | Loss: 0.00001445
Iteration 95/1000 | Loss: 0.00001445
Iteration 96/1000 | Loss: 0.00001445
Iteration 97/1000 | Loss: 0.00001444
Iteration 98/1000 | Loss: 0.00001444
Iteration 99/1000 | Loss: 0.00001444
Iteration 100/1000 | Loss: 0.00001444
Iteration 101/1000 | Loss: 0.00001444
Iteration 102/1000 | Loss: 0.00001444
Iteration 103/1000 | Loss: 0.00001444
Iteration 104/1000 | Loss: 0.00001443
Iteration 105/1000 | Loss: 0.00001443
Iteration 106/1000 | Loss: 0.00001443
Iteration 107/1000 | Loss: 0.00001443
Iteration 108/1000 | Loss: 0.00001443
Iteration 109/1000 | Loss: 0.00001443
Iteration 110/1000 | Loss: 0.00001443
Iteration 111/1000 | Loss: 0.00001442
Iteration 112/1000 | Loss: 0.00001442
Iteration 113/1000 | Loss: 0.00001441
Iteration 114/1000 | Loss: 0.00001441
Iteration 115/1000 | Loss: 0.00001441
Iteration 116/1000 | Loss: 0.00001441
Iteration 117/1000 | Loss: 0.00001441
Iteration 118/1000 | Loss: 0.00001441
Iteration 119/1000 | Loss: 0.00001441
Iteration 120/1000 | Loss: 0.00001441
Iteration 121/1000 | Loss: 0.00001441
Iteration 122/1000 | Loss: 0.00001441
Iteration 123/1000 | Loss: 0.00001441
Iteration 124/1000 | Loss: 0.00001441
Iteration 125/1000 | Loss: 0.00001441
Iteration 126/1000 | Loss: 0.00001441
Iteration 127/1000 | Loss: 0.00001441
Iteration 128/1000 | Loss: 0.00001441
Iteration 129/1000 | Loss: 0.00001441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.440807227481855e-05, 1.440807227481855e-05, 1.440807227481855e-05, 1.440807227481855e-05, 1.440807227481855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.440807227481855e-05

Optimization complete. Final v2v error: 3.2353153228759766 mm

Highest mean error: 3.409290075302124 mm for frame 112

Lowest mean error: 3.070450782775879 mm for frame 16

Saving results

Total time: 39.393073320388794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967821
Iteration 2/25 | Loss: 0.00181266
Iteration 3/25 | Loss: 0.00137347
Iteration 4/25 | Loss: 0.00131929
Iteration 5/25 | Loss: 0.00131174
Iteration 6/25 | Loss: 0.00131168
Iteration 7/25 | Loss: 0.00131168
Iteration 8/25 | Loss: 0.00131168
Iteration 9/25 | Loss: 0.00131168
Iteration 10/25 | Loss: 0.00131168
Iteration 11/25 | Loss: 0.00131168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013116808841004968, 0.0013116808841004968, 0.0013116808841004968, 0.0013116808841004968, 0.0013116808841004968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013116808841004968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33964407
Iteration 2/25 | Loss: 0.00087470
Iteration 3/25 | Loss: 0.00087470
Iteration 4/25 | Loss: 0.00087470
Iteration 5/25 | Loss: 0.00087470
Iteration 6/25 | Loss: 0.00087470
Iteration 7/25 | Loss: 0.00087470
Iteration 8/25 | Loss: 0.00087470
Iteration 9/25 | Loss: 0.00087470
Iteration 10/25 | Loss: 0.00087469
Iteration 11/25 | Loss: 0.00087469
Iteration 12/25 | Loss: 0.00087469
Iteration 13/25 | Loss: 0.00087469
Iteration 14/25 | Loss: 0.00087469
Iteration 15/25 | Loss: 0.00087469
Iteration 16/25 | Loss: 0.00087469
Iteration 17/25 | Loss: 0.00087469
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008746945532038808, 0.0008746945532038808, 0.0008746945532038808, 0.0008746945532038808, 0.0008746945532038808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008746945532038808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087469
Iteration 2/1000 | Loss: 0.00003196
Iteration 3/1000 | Loss: 0.00002496
Iteration 4/1000 | Loss: 0.00002368
Iteration 5/1000 | Loss: 0.00002288
Iteration 6/1000 | Loss: 0.00002232
Iteration 7/1000 | Loss: 0.00002191
Iteration 8/1000 | Loss: 0.00002153
Iteration 9/1000 | Loss: 0.00002122
Iteration 10/1000 | Loss: 0.00002119
Iteration 11/1000 | Loss: 0.00002099
Iteration 12/1000 | Loss: 0.00002089
Iteration 13/1000 | Loss: 0.00002077
Iteration 14/1000 | Loss: 0.00002071
Iteration 15/1000 | Loss: 0.00002071
Iteration 16/1000 | Loss: 0.00002071
Iteration 17/1000 | Loss: 0.00002070
Iteration 18/1000 | Loss: 0.00002070
Iteration 19/1000 | Loss: 0.00002070
Iteration 20/1000 | Loss: 0.00002070
Iteration 21/1000 | Loss: 0.00002070
Iteration 22/1000 | Loss: 0.00002070
Iteration 23/1000 | Loss: 0.00002070
Iteration 24/1000 | Loss: 0.00002070
Iteration 25/1000 | Loss: 0.00002070
Iteration 26/1000 | Loss: 0.00002070
Iteration 27/1000 | Loss: 0.00002067
Iteration 28/1000 | Loss: 0.00002067
Iteration 29/1000 | Loss: 0.00002067
Iteration 30/1000 | Loss: 0.00002066
Iteration 31/1000 | Loss: 0.00002065
Iteration 32/1000 | Loss: 0.00002064
Iteration 33/1000 | Loss: 0.00002061
Iteration 34/1000 | Loss: 0.00002061
Iteration 35/1000 | Loss: 0.00002060
Iteration 36/1000 | Loss: 0.00002059
Iteration 37/1000 | Loss: 0.00002058
Iteration 38/1000 | Loss: 0.00002058
Iteration 39/1000 | Loss: 0.00002057
Iteration 40/1000 | Loss: 0.00002057
Iteration 41/1000 | Loss: 0.00002057
Iteration 42/1000 | Loss: 0.00002056
Iteration 43/1000 | Loss: 0.00002056
Iteration 44/1000 | Loss: 0.00002056
Iteration 45/1000 | Loss: 0.00002055
Iteration 46/1000 | Loss: 0.00002055
Iteration 47/1000 | Loss: 0.00002055
Iteration 48/1000 | Loss: 0.00002054
Iteration 49/1000 | Loss: 0.00002054
Iteration 50/1000 | Loss: 0.00002054
Iteration 51/1000 | Loss: 0.00002054
Iteration 52/1000 | Loss: 0.00002053
Iteration 53/1000 | Loss: 0.00002053
Iteration 54/1000 | Loss: 0.00002052
Iteration 55/1000 | Loss: 0.00002052
Iteration 56/1000 | Loss: 0.00002051
Iteration 57/1000 | Loss: 0.00002051
Iteration 58/1000 | Loss: 0.00002051
Iteration 59/1000 | Loss: 0.00002051
Iteration 60/1000 | Loss: 0.00002050
Iteration 61/1000 | Loss: 0.00002050
Iteration 62/1000 | Loss: 0.00002049
Iteration 63/1000 | Loss: 0.00002049
Iteration 64/1000 | Loss: 0.00002049
Iteration 65/1000 | Loss: 0.00002049
Iteration 66/1000 | Loss: 0.00002049
Iteration 67/1000 | Loss: 0.00002049
Iteration 68/1000 | Loss: 0.00002049
Iteration 69/1000 | Loss: 0.00002049
Iteration 70/1000 | Loss: 0.00002049
Iteration 71/1000 | Loss: 0.00002048
Iteration 72/1000 | Loss: 0.00002048
Iteration 73/1000 | Loss: 0.00002048
Iteration 74/1000 | Loss: 0.00002048
Iteration 75/1000 | Loss: 0.00002048
Iteration 76/1000 | Loss: 0.00002048
Iteration 77/1000 | Loss: 0.00002048
Iteration 78/1000 | Loss: 0.00002048
Iteration 79/1000 | Loss: 0.00002048
Iteration 80/1000 | Loss: 0.00002048
Iteration 81/1000 | Loss: 0.00002048
Iteration 82/1000 | Loss: 0.00002048
Iteration 83/1000 | Loss: 0.00002048
Iteration 84/1000 | Loss: 0.00002048
Iteration 85/1000 | Loss: 0.00002048
Iteration 86/1000 | Loss: 0.00002048
Iteration 87/1000 | Loss: 0.00002048
Iteration 88/1000 | Loss: 0.00002048
Iteration 89/1000 | Loss: 0.00002048
Iteration 90/1000 | Loss: 0.00002048
Iteration 91/1000 | Loss: 0.00002048
Iteration 92/1000 | Loss: 0.00002048
Iteration 93/1000 | Loss: 0.00002048
Iteration 94/1000 | Loss: 0.00002048
Iteration 95/1000 | Loss: 0.00002048
Iteration 96/1000 | Loss: 0.00002048
Iteration 97/1000 | Loss: 0.00002048
Iteration 98/1000 | Loss: 0.00002048
Iteration 99/1000 | Loss: 0.00002048
Iteration 100/1000 | Loss: 0.00002048
Iteration 101/1000 | Loss: 0.00002048
Iteration 102/1000 | Loss: 0.00002048
Iteration 103/1000 | Loss: 0.00002048
Iteration 104/1000 | Loss: 0.00002048
Iteration 105/1000 | Loss: 0.00002048
Iteration 106/1000 | Loss: 0.00002048
Iteration 107/1000 | Loss: 0.00002048
Iteration 108/1000 | Loss: 0.00002048
Iteration 109/1000 | Loss: 0.00002048
Iteration 110/1000 | Loss: 0.00002048
Iteration 111/1000 | Loss: 0.00002048
Iteration 112/1000 | Loss: 0.00002048
Iteration 113/1000 | Loss: 0.00002048
Iteration 114/1000 | Loss: 0.00002048
Iteration 115/1000 | Loss: 0.00002048
Iteration 116/1000 | Loss: 0.00002048
Iteration 117/1000 | Loss: 0.00002048
Iteration 118/1000 | Loss: 0.00002048
Iteration 119/1000 | Loss: 0.00002048
Iteration 120/1000 | Loss: 0.00002048
Iteration 121/1000 | Loss: 0.00002048
Iteration 122/1000 | Loss: 0.00002048
Iteration 123/1000 | Loss: 0.00002048
Iteration 124/1000 | Loss: 0.00002048
Iteration 125/1000 | Loss: 0.00002048
Iteration 126/1000 | Loss: 0.00002048
Iteration 127/1000 | Loss: 0.00002048
Iteration 128/1000 | Loss: 0.00002048
Iteration 129/1000 | Loss: 0.00002048
Iteration 130/1000 | Loss: 0.00002048
Iteration 131/1000 | Loss: 0.00002048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.0477931684581563e-05, 2.0477931684581563e-05, 2.0477931684581563e-05, 2.0477931684581563e-05, 2.0477931684581563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0477931684581563e-05

Optimization complete. Final v2v error: 3.796166181564331 mm

Highest mean error: 3.844247817993164 mm for frame 71

Lowest mean error: 3.5296812057495117 mm for frame 0

Saving results

Total time: 33.30679893493652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053035
Iteration 2/25 | Loss: 0.00504290
Iteration 3/25 | Loss: 0.00487732
Iteration 4/25 | Loss: 0.00288625
Iteration 5/25 | Loss: 0.00261122
Iteration 6/25 | Loss: 0.00236999
Iteration 7/25 | Loss: 0.00216708
Iteration 8/25 | Loss: 0.00202765
Iteration 9/25 | Loss: 0.00188501
Iteration 10/25 | Loss: 0.00183161
Iteration 11/25 | Loss: 0.00177377
Iteration 12/25 | Loss: 0.00172483
Iteration 13/25 | Loss: 0.00170564
Iteration 14/25 | Loss: 0.00163626
Iteration 15/25 | Loss: 0.00157220
Iteration 16/25 | Loss: 0.00154747
Iteration 17/25 | Loss: 0.00152883
Iteration 18/25 | Loss: 0.00152370
Iteration 19/25 | Loss: 0.00151398
Iteration 20/25 | Loss: 0.00149753
Iteration 21/25 | Loss: 0.00149825
Iteration 22/25 | Loss: 0.00148539
Iteration 23/25 | Loss: 0.00148560
Iteration 24/25 | Loss: 0.00148100
Iteration 25/25 | Loss: 0.00147421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56817114
Iteration 2/25 | Loss: 0.00243845
Iteration 3/25 | Loss: 0.00243845
Iteration 4/25 | Loss: 0.00243845
Iteration 5/25 | Loss: 0.00243845
Iteration 6/25 | Loss: 0.00243845
Iteration 7/25 | Loss: 0.00243845
Iteration 8/25 | Loss: 0.00243845
Iteration 9/25 | Loss: 0.00243845
Iteration 10/25 | Loss: 0.00243845
Iteration 11/25 | Loss: 0.00243845
Iteration 12/25 | Loss: 0.00243845
Iteration 13/25 | Loss: 0.00243845
Iteration 14/25 | Loss: 0.00243845
Iteration 15/25 | Loss: 0.00243845
Iteration 16/25 | Loss: 0.00243845
Iteration 17/25 | Loss: 0.00243845
Iteration 18/25 | Loss: 0.00243845
Iteration 19/25 | Loss: 0.00243845
Iteration 20/25 | Loss: 0.00243845
Iteration 21/25 | Loss: 0.00243845
Iteration 22/25 | Loss: 0.00243845
Iteration 23/25 | Loss: 0.00243845
Iteration 24/25 | Loss: 0.00243845
Iteration 25/25 | Loss: 0.00243845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243845
Iteration 2/1000 | Loss: 0.00023592
Iteration 3/1000 | Loss: 0.00017459
Iteration 4/1000 | Loss: 0.00014877
Iteration 5/1000 | Loss: 0.00012875
Iteration 6/1000 | Loss: 0.00011905
Iteration 7/1000 | Loss: 0.00011389
Iteration 8/1000 | Loss: 0.00035235
Iteration 9/1000 | Loss: 0.00019644
Iteration 10/1000 | Loss: 0.00043253
Iteration 11/1000 | Loss: 0.00020511
Iteration 12/1000 | Loss: 0.00032624
Iteration 13/1000 | Loss: 0.00018021
Iteration 14/1000 | Loss: 0.00019923
Iteration 15/1000 | Loss: 0.00010036
Iteration 16/1000 | Loss: 0.00009534
Iteration 17/1000 | Loss: 0.00009084
Iteration 18/1000 | Loss: 0.00008886
Iteration 19/1000 | Loss: 0.00008759
Iteration 20/1000 | Loss: 0.00008663
Iteration 21/1000 | Loss: 0.00008551
Iteration 22/1000 | Loss: 0.00020422
Iteration 23/1000 | Loss: 0.00009196
Iteration 24/1000 | Loss: 0.00008600
Iteration 25/1000 | Loss: 0.00008414
Iteration 26/1000 | Loss: 0.00008243
Iteration 27/1000 | Loss: 0.00008131
Iteration 28/1000 | Loss: 0.00008092
Iteration 29/1000 | Loss: 0.00008058
Iteration 30/1000 | Loss: 0.00008016
Iteration 31/1000 | Loss: 0.00007969
Iteration 32/1000 | Loss: 0.00007927
Iteration 33/1000 | Loss: 0.00007890
Iteration 34/1000 | Loss: 0.00007860
Iteration 35/1000 | Loss: 0.00016280
Iteration 36/1000 | Loss: 0.00019221
Iteration 37/1000 | Loss: 0.00008084
Iteration 38/1000 | Loss: 0.00007710
Iteration 39/1000 | Loss: 0.00007487
Iteration 40/1000 | Loss: 0.00007282
Iteration 41/1000 | Loss: 0.00007156
Iteration 42/1000 | Loss: 0.00013018
Iteration 43/1000 | Loss: 0.00007244
Iteration 44/1000 | Loss: 0.00006952
Iteration 45/1000 | Loss: 0.00006780
Iteration 46/1000 | Loss: 0.00006604
Iteration 47/1000 | Loss: 0.00006479
Iteration 48/1000 | Loss: 0.00006402
Iteration 49/1000 | Loss: 0.00006324
Iteration 50/1000 | Loss: 0.00013923
Iteration 51/1000 | Loss: 0.00041928
Iteration 52/1000 | Loss: 0.00065467
Iteration 53/1000 | Loss: 0.00013884
Iteration 54/1000 | Loss: 0.00008396
Iteration 55/1000 | Loss: 0.00007489
Iteration 56/1000 | Loss: 0.00005728
Iteration 57/1000 | Loss: 0.00004727
Iteration 58/1000 | Loss: 0.00004227
Iteration 59/1000 | Loss: 0.00004045
Iteration 60/1000 | Loss: 0.00003892
Iteration 61/1000 | Loss: 0.00003793
Iteration 62/1000 | Loss: 0.00003717
Iteration 63/1000 | Loss: 0.00003658
Iteration 64/1000 | Loss: 0.00003598
Iteration 65/1000 | Loss: 0.00003572
Iteration 66/1000 | Loss: 0.00003559
Iteration 67/1000 | Loss: 0.00003555
Iteration 68/1000 | Loss: 0.00003552
Iteration 69/1000 | Loss: 0.00003552
Iteration 70/1000 | Loss: 0.00003551
Iteration 71/1000 | Loss: 0.00003551
Iteration 72/1000 | Loss: 0.00003550
Iteration 73/1000 | Loss: 0.00003549
Iteration 74/1000 | Loss: 0.00003548
Iteration 75/1000 | Loss: 0.00003545
Iteration 76/1000 | Loss: 0.00003545
Iteration 77/1000 | Loss: 0.00003545
Iteration 78/1000 | Loss: 0.00003545
Iteration 79/1000 | Loss: 0.00003545
Iteration 80/1000 | Loss: 0.00003545
Iteration 81/1000 | Loss: 0.00003545
Iteration 82/1000 | Loss: 0.00003545
Iteration 83/1000 | Loss: 0.00003545
Iteration 84/1000 | Loss: 0.00003545
Iteration 85/1000 | Loss: 0.00003544
Iteration 86/1000 | Loss: 0.00003544
Iteration 87/1000 | Loss: 0.00003544
Iteration 88/1000 | Loss: 0.00003544
Iteration 89/1000 | Loss: 0.00003544
Iteration 90/1000 | Loss: 0.00003544
Iteration 91/1000 | Loss: 0.00003544
Iteration 92/1000 | Loss: 0.00003544
Iteration 93/1000 | Loss: 0.00003544
Iteration 94/1000 | Loss: 0.00003544
Iteration 95/1000 | Loss: 0.00003544
Iteration 96/1000 | Loss: 0.00003544
Iteration 97/1000 | Loss: 0.00003544
Iteration 98/1000 | Loss: 0.00003544
Iteration 99/1000 | Loss: 0.00003543
Iteration 100/1000 | Loss: 0.00003543
Iteration 101/1000 | Loss: 0.00003543
Iteration 102/1000 | Loss: 0.00003543
Iteration 103/1000 | Loss: 0.00003543
Iteration 104/1000 | Loss: 0.00003543
Iteration 105/1000 | Loss: 0.00003543
Iteration 106/1000 | Loss: 0.00003543
Iteration 107/1000 | Loss: 0.00003543
Iteration 108/1000 | Loss: 0.00003542
Iteration 109/1000 | Loss: 0.00003542
Iteration 110/1000 | Loss: 0.00003542
Iteration 111/1000 | Loss: 0.00003542
Iteration 112/1000 | Loss: 0.00003542
Iteration 113/1000 | Loss: 0.00003542
Iteration 114/1000 | Loss: 0.00003542
Iteration 115/1000 | Loss: 0.00003541
Iteration 116/1000 | Loss: 0.00003541
Iteration 117/1000 | Loss: 0.00003541
Iteration 118/1000 | Loss: 0.00003541
Iteration 119/1000 | Loss: 0.00003541
Iteration 120/1000 | Loss: 0.00003541
Iteration 121/1000 | Loss: 0.00003540
Iteration 122/1000 | Loss: 0.00003540
Iteration 123/1000 | Loss: 0.00003540
Iteration 124/1000 | Loss: 0.00003540
Iteration 125/1000 | Loss: 0.00003540
Iteration 126/1000 | Loss: 0.00003539
Iteration 127/1000 | Loss: 0.00003539
Iteration 128/1000 | Loss: 0.00003539
Iteration 129/1000 | Loss: 0.00003538
Iteration 130/1000 | Loss: 0.00003538
Iteration 131/1000 | Loss: 0.00003538
Iteration 132/1000 | Loss: 0.00003538
Iteration 133/1000 | Loss: 0.00003538
Iteration 134/1000 | Loss: 0.00003538
Iteration 135/1000 | Loss: 0.00003538
Iteration 136/1000 | Loss: 0.00003537
Iteration 137/1000 | Loss: 0.00003536
Iteration 138/1000 | Loss: 0.00003536
Iteration 139/1000 | Loss: 0.00003536
Iteration 140/1000 | Loss: 0.00003536
Iteration 141/1000 | Loss: 0.00003536
Iteration 142/1000 | Loss: 0.00003536
Iteration 143/1000 | Loss: 0.00003536
Iteration 144/1000 | Loss: 0.00003536
Iteration 145/1000 | Loss: 0.00003535
Iteration 146/1000 | Loss: 0.00003535
Iteration 147/1000 | Loss: 0.00003535
Iteration 148/1000 | Loss: 0.00003535
Iteration 149/1000 | Loss: 0.00003535
Iteration 150/1000 | Loss: 0.00003534
Iteration 151/1000 | Loss: 0.00003534
Iteration 152/1000 | Loss: 0.00003534
Iteration 153/1000 | Loss: 0.00003534
Iteration 154/1000 | Loss: 0.00003534
Iteration 155/1000 | Loss: 0.00003533
Iteration 156/1000 | Loss: 0.00003533
Iteration 157/1000 | Loss: 0.00003533
Iteration 158/1000 | Loss: 0.00003533
Iteration 159/1000 | Loss: 0.00003532
Iteration 160/1000 | Loss: 0.00003532
Iteration 161/1000 | Loss: 0.00003532
Iteration 162/1000 | Loss: 0.00003532
Iteration 163/1000 | Loss: 0.00003532
Iteration 164/1000 | Loss: 0.00003532
Iteration 165/1000 | Loss: 0.00003532
Iteration 166/1000 | Loss: 0.00003532
Iteration 167/1000 | Loss: 0.00003532
Iteration 168/1000 | Loss: 0.00003532
Iteration 169/1000 | Loss: 0.00003532
Iteration 170/1000 | Loss: 0.00003531
Iteration 171/1000 | Loss: 0.00003531
Iteration 172/1000 | Loss: 0.00003531
Iteration 173/1000 | Loss: 0.00003531
Iteration 174/1000 | Loss: 0.00003531
Iteration 175/1000 | Loss: 0.00003531
Iteration 176/1000 | Loss: 0.00003530
Iteration 177/1000 | Loss: 0.00003530
Iteration 178/1000 | Loss: 0.00003530
Iteration 179/1000 | Loss: 0.00003530
Iteration 180/1000 | Loss: 0.00003530
Iteration 181/1000 | Loss: 0.00003530
Iteration 182/1000 | Loss: 0.00003529
Iteration 183/1000 | Loss: 0.00003529
Iteration 184/1000 | Loss: 0.00003529
Iteration 185/1000 | Loss: 0.00003529
Iteration 186/1000 | Loss: 0.00003529
Iteration 187/1000 | Loss: 0.00003529
Iteration 188/1000 | Loss: 0.00003529
Iteration 189/1000 | Loss: 0.00003529
Iteration 190/1000 | Loss: 0.00003529
Iteration 191/1000 | Loss: 0.00003529
Iteration 192/1000 | Loss: 0.00003529
Iteration 193/1000 | Loss: 0.00003529
Iteration 194/1000 | Loss: 0.00003529
Iteration 195/1000 | Loss: 0.00003529
Iteration 196/1000 | Loss: 0.00003529
Iteration 197/1000 | Loss: 0.00003529
Iteration 198/1000 | Loss: 0.00003529
Iteration 199/1000 | Loss: 0.00003529
Iteration 200/1000 | Loss: 0.00003529
Iteration 201/1000 | Loss: 0.00003529
Iteration 202/1000 | Loss: 0.00003529
Iteration 203/1000 | Loss: 0.00003529
Iteration 204/1000 | Loss: 0.00003529
Iteration 205/1000 | Loss: 0.00003529
Iteration 206/1000 | Loss: 0.00003529
Iteration 207/1000 | Loss: 0.00003529
Iteration 208/1000 | Loss: 0.00003529
Iteration 209/1000 | Loss: 0.00003529
Iteration 210/1000 | Loss: 0.00003529
Iteration 211/1000 | Loss: 0.00003529
Iteration 212/1000 | Loss: 0.00003529
Iteration 213/1000 | Loss: 0.00003529
Iteration 214/1000 | Loss: 0.00003529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [3.528540037223138e-05, 3.528540037223138e-05, 3.528540037223138e-05, 3.528540037223138e-05, 3.528540037223138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.528540037223138e-05

Optimization complete. Final v2v error: 4.293429374694824 mm

Highest mean error: 11.205869674682617 mm for frame 142

Lowest mean error: 3.8525490760803223 mm for frame 55

Saving results

Total time: 152.76881861686707
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736917
Iteration 2/25 | Loss: 0.00159848
Iteration 3/25 | Loss: 0.00134593
Iteration 4/25 | Loss: 0.00131428
Iteration 5/25 | Loss: 0.00130270
Iteration 6/25 | Loss: 0.00129278
Iteration 7/25 | Loss: 0.00128983
Iteration 8/25 | Loss: 0.00128470
Iteration 9/25 | Loss: 0.00128290
Iteration 10/25 | Loss: 0.00128175
Iteration 11/25 | Loss: 0.00128373
Iteration 12/25 | Loss: 0.00128569
Iteration 13/25 | Loss: 0.00128183
Iteration 14/25 | Loss: 0.00127958
Iteration 15/25 | Loss: 0.00127764
Iteration 16/25 | Loss: 0.00127697
Iteration 17/25 | Loss: 0.00127672
Iteration 18/25 | Loss: 0.00127633
Iteration 19/25 | Loss: 0.00127587
Iteration 20/25 | Loss: 0.00127559
Iteration 21/25 | Loss: 0.00127535
Iteration 22/25 | Loss: 0.00127763
Iteration 23/25 | Loss: 0.00127800
Iteration 24/25 | Loss: 0.00127598
Iteration 25/25 | Loss: 0.00127381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.79876232
Iteration 2/25 | Loss: 0.00111972
Iteration 3/25 | Loss: 0.00111959
Iteration 4/25 | Loss: 0.00111959
Iteration 5/25 | Loss: 0.00111959
Iteration 6/25 | Loss: 0.00111959
Iteration 7/25 | Loss: 0.00111959
Iteration 8/25 | Loss: 0.00111959
Iteration 9/25 | Loss: 0.00111959
Iteration 10/25 | Loss: 0.00111959
Iteration 11/25 | Loss: 0.00111959
Iteration 12/25 | Loss: 0.00111959
Iteration 13/25 | Loss: 0.00111959
Iteration 14/25 | Loss: 0.00111959
Iteration 15/25 | Loss: 0.00111959
Iteration 16/25 | Loss: 0.00111959
Iteration 17/25 | Loss: 0.00111959
Iteration 18/25 | Loss: 0.00111959
Iteration 19/25 | Loss: 0.00111959
Iteration 20/25 | Loss: 0.00111959
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011195865226909518, 0.0011195865226909518, 0.0011195865226909518, 0.0011195865226909518, 0.0011195865226909518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011195865226909518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111959
Iteration 2/1000 | Loss: 0.00004503
Iteration 3/1000 | Loss: 0.00007424
Iteration 4/1000 | Loss: 0.00006059
Iteration 5/1000 | Loss: 0.00007485
Iteration 6/1000 | Loss: 0.00006232
Iteration 7/1000 | Loss: 0.00007800
Iteration 8/1000 | Loss: 0.00006785
Iteration 9/1000 | Loss: 0.00008605
Iteration 10/1000 | Loss: 0.00013438
Iteration 11/1000 | Loss: 0.00002862
Iteration 12/1000 | Loss: 0.00006733
Iteration 13/1000 | Loss: 0.00006984
Iteration 14/1000 | Loss: 0.00007146
Iteration 15/1000 | Loss: 0.00007552
Iteration 16/1000 | Loss: 0.00006185
Iteration 17/1000 | Loss: 0.00006256
Iteration 18/1000 | Loss: 0.00006204
Iteration 19/1000 | Loss: 0.00006367
Iteration 20/1000 | Loss: 0.00006122
Iteration 21/1000 | Loss: 0.00007409
Iteration 22/1000 | Loss: 0.00006102
Iteration 23/1000 | Loss: 0.00007505
Iteration 24/1000 | Loss: 0.00007403
Iteration 25/1000 | Loss: 0.00007401
Iteration 26/1000 | Loss: 0.00007250
Iteration 27/1000 | Loss: 0.00007077
Iteration 28/1000 | Loss: 0.00006916
Iteration 29/1000 | Loss: 0.00007003
Iteration 30/1000 | Loss: 0.00007313
Iteration 31/1000 | Loss: 0.00005603
Iteration 32/1000 | Loss: 0.00004187
Iteration 33/1000 | Loss: 0.00004504
Iteration 34/1000 | Loss: 0.00002492
Iteration 35/1000 | Loss: 0.00003312
Iteration 36/1000 | Loss: 0.00002320
Iteration 37/1000 | Loss: 0.00002258
Iteration 38/1000 | Loss: 0.00002190
Iteration 39/1000 | Loss: 0.00002455
Iteration 40/1000 | Loss: 0.00002109
Iteration 41/1000 | Loss: 0.00002070
Iteration 42/1000 | Loss: 0.00002054
Iteration 43/1000 | Loss: 0.00002030
Iteration 44/1000 | Loss: 0.00002019
Iteration 45/1000 | Loss: 0.00002015
Iteration 46/1000 | Loss: 0.00002015
Iteration 47/1000 | Loss: 0.00002015
Iteration 48/1000 | Loss: 0.00002014
Iteration 49/1000 | Loss: 0.00002014
Iteration 50/1000 | Loss: 0.00002013
Iteration 51/1000 | Loss: 0.00002013
Iteration 52/1000 | Loss: 0.00002013
Iteration 53/1000 | Loss: 0.00002012
Iteration 54/1000 | Loss: 0.00003716
Iteration 55/1000 | Loss: 0.00002008
Iteration 56/1000 | Loss: 0.00002006
Iteration 57/1000 | Loss: 0.00002006
Iteration 58/1000 | Loss: 0.00002005
Iteration 59/1000 | Loss: 0.00002004
Iteration 60/1000 | Loss: 0.00002004
Iteration 61/1000 | Loss: 0.00002003
Iteration 62/1000 | Loss: 0.00002003
Iteration 63/1000 | Loss: 0.00002003
Iteration 64/1000 | Loss: 0.00002002
Iteration 65/1000 | Loss: 0.00002002
Iteration 66/1000 | Loss: 0.00002001
Iteration 67/1000 | Loss: 0.00002001
Iteration 68/1000 | Loss: 0.00002000
Iteration 69/1000 | Loss: 0.00002000
Iteration 70/1000 | Loss: 0.00002000
Iteration 71/1000 | Loss: 0.00001999
Iteration 72/1000 | Loss: 0.00001998
Iteration 73/1000 | Loss: 0.00001998
Iteration 74/1000 | Loss: 0.00001997
Iteration 75/1000 | Loss: 0.00001997
Iteration 76/1000 | Loss: 0.00001996
Iteration 77/1000 | Loss: 0.00001996
Iteration 78/1000 | Loss: 0.00001995
Iteration 79/1000 | Loss: 0.00001994
Iteration 80/1000 | Loss: 0.00001994
Iteration 81/1000 | Loss: 0.00001994
Iteration 82/1000 | Loss: 0.00001993
Iteration 83/1000 | Loss: 0.00001992
Iteration 84/1000 | Loss: 0.00001992
Iteration 85/1000 | Loss: 0.00001991
Iteration 86/1000 | Loss: 0.00001991
Iteration 87/1000 | Loss: 0.00001991
Iteration 88/1000 | Loss: 0.00001990
Iteration 89/1000 | Loss: 0.00001990
Iteration 90/1000 | Loss: 0.00001989
Iteration 91/1000 | Loss: 0.00001988
Iteration 92/1000 | Loss: 0.00001987
Iteration 93/1000 | Loss: 0.00001987
Iteration 94/1000 | Loss: 0.00001987
Iteration 95/1000 | Loss: 0.00001987
Iteration 96/1000 | Loss: 0.00001987
Iteration 97/1000 | Loss: 0.00001987
Iteration 98/1000 | Loss: 0.00001987
Iteration 99/1000 | Loss: 0.00001987
Iteration 100/1000 | Loss: 0.00001987
Iteration 101/1000 | Loss: 0.00001985
Iteration 102/1000 | Loss: 0.00001984
Iteration 103/1000 | Loss: 0.00001984
Iteration 104/1000 | Loss: 0.00001984
Iteration 105/1000 | Loss: 0.00001983
Iteration 106/1000 | Loss: 0.00001983
Iteration 107/1000 | Loss: 0.00001982
Iteration 108/1000 | Loss: 0.00001982
Iteration 109/1000 | Loss: 0.00001982
Iteration 110/1000 | Loss: 0.00001982
Iteration 111/1000 | Loss: 0.00001981
Iteration 112/1000 | Loss: 0.00001981
Iteration 113/1000 | Loss: 0.00001980
Iteration 114/1000 | Loss: 0.00001980
Iteration 115/1000 | Loss: 0.00001980
Iteration 116/1000 | Loss: 0.00001980
Iteration 117/1000 | Loss: 0.00001979
Iteration 118/1000 | Loss: 0.00001979
Iteration 119/1000 | Loss: 0.00001979
Iteration 120/1000 | Loss: 0.00001979
Iteration 121/1000 | Loss: 0.00001979
Iteration 122/1000 | Loss: 0.00001979
Iteration 123/1000 | Loss: 0.00001979
Iteration 124/1000 | Loss: 0.00001978
Iteration 125/1000 | Loss: 0.00001978
Iteration 126/1000 | Loss: 0.00001978
Iteration 127/1000 | Loss: 0.00001978
Iteration 128/1000 | Loss: 0.00001978
Iteration 129/1000 | Loss: 0.00001978
Iteration 130/1000 | Loss: 0.00001978
Iteration 131/1000 | Loss: 0.00001978
Iteration 132/1000 | Loss: 0.00001978
Iteration 133/1000 | Loss: 0.00001978
Iteration 134/1000 | Loss: 0.00001978
Iteration 135/1000 | Loss: 0.00001978
Iteration 136/1000 | Loss: 0.00001977
Iteration 137/1000 | Loss: 0.00001977
Iteration 138/1000 | Loss: 0.00001977
Iteration 139/1000 | Loss: 0.00001977
Iteration 140/1000 | Loss: 0.00003045
Iteration 141/1000 | Loss: 0.00001978
Iteration 142/1000 | Loss: 0.00001974
Iteration 143/1000 | Loss: 0.00001972
Iteration 144/1000 | Loss: 0.00001972
Iteration 145/1000 | Loss: 0.00001971
Iteration 146/1000 | Loss: 0.00001971
Iteration 147/1000 | Loss: 0.00001970
Iteration 148/1000 | Loss: 0.00001970
Iteration 149/1000 | Loss: 0.00001970
Iteration 150/1000 | Loss: 0.00001970
Iteration 151/1000 | Loss: 0.00001970
Iteration 152/1000 | Loss: 0.00001969
Iteration 153/1000 | Loss: 0.00001969
Iteration 154/1000 | Loss: 0.00001968
Iteration 155/1000 | Loss: 0.00001964
Iteration 156/1000 | Loss: 0.00001960
Iteration 157/1000 | Loss: 0.00001957
Iteration 158/1000 | Loss: 0.00003827
Iteration 159/1000 | Loss: 0.00002836
Iteration 160/1000 | Loss: 0.00002504
Iteration 161/1000 | Loss: 0.00002084
Iteration 162/1000 | Loss: 0.00001940
Iteration 163/1000 | Loss: 0.00003836
Iteration 164/1000 | Loss: 0.00003066
Iteration 165/1000 | Loss: 0.00002125
Iteration 166/1000 | Loss: 0.00001985
Iteration 167/1000 | Loss: 0.00001947
Iteration 168/1000 | Loss: 0.00001940
Iteration 169/1000 | Loss: 0.00003602
Iteration 170/1000 | Loss: 0.00002694
Iteration 171/1000 | Loss: 0.00001933
Iteration 172/1000 | Loss: 0.00001927
Iteration 173/1000 | Loss: 0.00001927
Iteration 174/1000 | Loss: 0.00001923
Iteration 175/1000 | Loss: 0.00001923
Iteration 176/1000 | Loss: 0.00001922
Iteration 177/1000 | Loss: 0.00001921
Iteration 178/1000 | Loss: 0.00001921
Iteration 179/1000 | Loss: 0.00001921
Iteration 180/1000 | Loss: 0.00001921
Iteration 181/1000 | Loss: 0.00001921
Iteration 182/1000 | Loss: 0.00001921
Iteration 183/1000 | Loss: 0.00001920
Iteration 184/1000 | Loss: 0.00001920
Iteration 185/1000 | Loss: 0.00001920
Iteration 186/1000 | Loss: 0.00001920
Iteration 187/1000 | Loss: 0.00001919
Iteration 188/1000 | Loss: 0.00001919
Iteration 189/1000 | Loss: 0.00001919
Iteration 190/1000 | Loss: 0.00001919
Iteration 191/1000 | Loss: 0.00001918
Iteration 192/1000 | Loss: 0.00001918
Iteration 193/1000 | Loss: 0.00001918
Iteration 194/1000 | Loss: 0.00001918
Iteration 195/1000 | Loss: 0.00001918
Iteration 196/1000 | Loss: 0.00001918
Iteration 197/1000 | Loss: 0.00001918
Iteration 198/1000 | Loss: 0.00001917
Iteration 199/1000 | Loss: 0.00001917
Iteration 200/1000 | Loss: 0.00001916
Iteration 201/1000 | Loss: 0.00001916
Iteration 202/1000 | Loss: 0.00001916
Iteration 203/1000 | Loss: 0.00001916
Iteration 204/1000 | Loss: 0.00001915
Iteration 205/1000 | Loss: 0.00001915
Iteration 206/1000 | Loss: 0.00001914
Iteration 207/1000 | Loss: 0.00001912
Iteration 208/1000 | Loss: 0.00001912
Iteration 209/1000 | Loss: 0.00001912
Iteration 210/1000 | Loss: 0.00001911
Iteration 211/1000 | Loss: 0.00001911
Iteration 212/1000 | Loss: 0.00001911
Iteration 213/1000 | Loss: 0.00001910
Iteration 214/1000 | Loss: 0.00001910
Iteration 215/1000 | Loss: 0.00001910
Iteration 216/1000 | Loss: 0.00001910
Iteration 217/1000 | Loss: 0.00001910
Iteration 218/1000 | Loss: 0.00001909
Iteration 219/1000 | Loss: 0.00001909
Iteration 220/1000 | Loss: 0.00001909
Iteration 221/1000 | Loss: 0.00001909
Iteration 222/1000 | Loss: 0.00001909
Iteration 223/1000 | Loss: 0.00001909
Iteration 224/1000 | Loss: 0.00001908
Iteration 225/1000 | Loss: 0.00001908
Iteration 226/1000 | Loss: 0.00001908
Iteration 227/1000 | Loss: 0.00001908
Iteration 228/1000 | Loss: 0.00001908
Iteration 229/1000 | Loss: 0.00001908
Iteration 230/1000 | Loss: 0.00001908
Iteration 231/1000 | Loss: 0.00001908
Iteration 232/1000 | Loss: 0.00001908
Iteration 233/1000 | Loss: 0.00001908
Iteration 234/1000 | Loss: 0.00001908
Iteration 235/1000 | Loss: 0.00001908
Iteration 236/1000 | Loss: 0.00001907
Iteration 237/1000 | Loss: 0.00001907
Iteration 238/1000 | Loss: 0.00001907
Iteration 239/1000 | Loss: 0.00001907
Iteration 240/1000 | Loss: 0.00001907
Iteration 241/1000 | Loss: 0.00001907
Iteration 242/1000 | Loss: 0.00001907
Iteration 243/1000 | Loss: 0.00001907
Iteration 244/1000 | Loss: 0.00001907
Iteration 245/1000 | Loss: 0.00001907
Iteration 246/1000 | Loss: 0.00001907
Iteration 247/1000 | Loss: 0.00001907
Iteration 248/1000 | Loss: 0.00001907
Iteration 249/1000 | Loss: 0.00001907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.9074164811172523e-05, 1.9074164811172523e-05, 1.9074164811172523e-05, 1.9074164811172523e-05, 1.9074164811172523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9074164811172523e-05

Optimization complete. Final v2v error: 3.6055028438568115 mm

Highest mean error: 7.187517166137695 mm for frame 194

Lowest mean error: 2.8176345825195312 mm for frame 118

Saving results

Total time: 164.32763671875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00700500
Iteration 2/25 | Loss: 0.00184779
Iteration 3/25 | Loss: 0.00153836
Iteration 4/25 | Loss: 0.00145878
Iteration 5/25 | Loss: 0.00140056
Iteration 6/25 | Loss: 0.00135383
Iteration 7/25 | Loss: 0.00134622
Iteration 8/25 | Loss: 0.00132409
Iteration 9/25 | Loss: 0.00132825
Iteration 10/25 | Loss: 0.00132544
Iteration 11/25 | Loss: 0.00131866
Iteration 12/25 | Loss: 0.00132314
Iteration 13/25 | Loss: 0.00131621
Iteration 14/25 | Loss: 0.00131554
Iteration 15/25 | Loss: 0.00131510
Iteration 16/25 | Loss: 0.00131485
Iteration 17/25 | Loss: 0.00131463
Iteration 18/25 | Loss: 0.00131449
Iteration 19/25 | Loss: 0.00131438
Iteration 20/25 | Loss: 0.00131431
Iteration 21/25 | Loss: 0.00131430
Iteration 22/25 | Loss: 0.00131430
Iteration 23/25 | Loss: 0.00131430
Iteration 24/25 | Loss: 0.00131430
Iteration 25/25 | Loss: 0.00131430

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.68905544
Iteration 2/25 | Loss: 0.00325957
Iteration 3/25 | Loss: 0.00192326
Iteration 4/25 | Loss: 0.00192324
Iteration 5/25 | Loss: 0.00192324
Iteration 6/25 | Loss: 0.00192324
Iteration 7/25 | Loss: 0.00192324
Iteration 8/25 | Loss: 0.00192324
Iteration 9/25 | Loss: 0.00192324
Iteration 10/25 | Loss: 0.00192324
Iteration 11/25 | Loss: 0.00192324
Iteration 12/25 | Loss: 0.00192324
Iteration 13/25 | Loss: 0.00192324
Iteration 14/25 | Loss: 0.00192324
Iteration 15/25 | Loss: 0.00192324
Iteration 16/25 | Loss: 0.00192324
Iteration 17/25 | Loss: 0.00192324
Iteration 18/25 | Loss: 0.00192324
Iteration 19/25 | Loss: 0.00192324
Iteration 20/25 | Loss: 0.00192324
Iteration 21/25 | Loss: 0.00192324
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0019232378108426929, 0.0019232378108426929, 0.0019232378108426929, 0.0019232378108426929, 0.0019232378108426929]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019232378108426929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192324
Iteration 2/1000 | Loss: 0.00032034
Iteration 3/1000 | Loss: 0.00189387
Iteration 4/1000 | Loss: 0.00264197
Iteration 5/1000 | Loss: 0.00323147
Iteration 6/1000 | Loss: 0.00061450
Iteration 7/1000 | Loss: 0.00037729
Iteration 8/1000 | Loss: 0.00293045
Iteration 9/1000 | Loss: 0.00391911
Iteration 10/1000 | Loss: 0.00253293
Iteration 11/1000 | Loss: 0.00459637
Iteration 12/1000 | Loss: 0.00274844
Iteration 13/1000 | Loss: 0.00360876
Iteration 14/1000 | Loss: 0.01164926
Iteration 15/1000 | Loss: 0.00344570
Iteration 16/1000 | Loss: 0.00497622
Iteration 17/1000 | Loss: 0.00188950
Iteration 18/1000 | Loss: 0.00250168
Iteration 19/1000 | Loss: 0.00255777
Iteration 20/1000 | Loss: 0.00301889
Iteration 21/1000 | Loss: 0.00101101
Iteration 22/1000 | Loss: 0.00213513
Iteration 23/1000 | Loss: 0.00132638
Iteration 24/1000 | Loss: 0.00106242
Iteration 25/1000 | Loss: 0.00147575
Iteration 26/1000 | Loss: 0.00050125
Iteration 27/1000 | Loss: 0.00091259
Iteration 28/1000 | Loss: 0.00179689
Iteration 29/1000 | Loss: 0.00054770
Iteration 30/1000 | Loss: 0.00086314
Iteration 31/1000 | Loss: 0.00085250
Iteration 32/1000 | Loss: 0.00223985
Iteration 33/1000 | Loss: 0.00097688
Iteration 34/1000 | Loss: 0.00074922
Iteration 35/1000 | Loss: 0.00046544
Iteration 36/1000 | Loss: 0.00048816
Iteration 37/1000 | Loss: 0.00077058
Iteration 38/1000 | Loss: 0.00077999
Iteration 39/1000 | Loss: 0.00061288
Iteration 40/1000 | Loss: 0.00012709
Iteration 41/1000 | Loss: 0.00045435
Iteration 42/1000 | Loss: 0.00026562
Iteration 43/1000 | Loss: 0.00041050
Iteration 44/1000 | Loss: 0.00012015
Iteration 45/1000 | Loss: 0.00047677
Iteration 46/1000 | Loss: 0.00018268
Iteration 47/1000 | Loss: 0.00023558
Iteration 48/1000 | Loss: 0.00048641
Iteration 49/1000 | Loss: 0.00029530
Iteration 50/1000 | Loss: 0.00058727
Iteration 51/1000 | Loss: 0.00052867
Iteration 52/1000 | Loss: 0.00086822
Iteration 53/1000 | Loss: 0.00064384
Iteration 54/1000 | Loss: 0.00038581
Iteration 55/1000 | Loss: 0.00040752
Iteration 56/1000 | Loss: 0.00036040
Iteration 57/1000 | Loss: 0.00031394
Iteration 58/1000 | Loss: 0.00010465
Iteration 59/1000 | Loss: 0.00012939
Iteration 60/1000 | Loss: 0.00004737
Iteration 61/1000 | Loss: 0.00084058
Iteration 62/1000 | Loss: 0.00042282
Iteration 63/1000 | Loss: 0.00073004
Iteration 64/1000 | Loss: 0.00030721
Iteration 65/1000 | Loss: 0.00004545
Iteration 66/1000 | Loss: 0.00005069
Iteration 67/1000 | Loss: 0.00125660
Iteration 68/1000 | Loss: 0.00083726
Iteration 69/1000 | Loss: 0.00037674
Iteration 70/1000 | Loss: 0.00008314
Iteration 71/1000 | Loss: 0.00014602
Iteration 72/1000 | Loss: 0.00010154
Iteration 73/1000 | Loss: 0.00004353
Iteration 74/1000 | Loss: 0.00041880
Iteration 75/1000 | Loss: 0.00014711
Iteration 76/1000 | Loss: 0.00016362
Iteration 77/1000 | Loss: 0.00082900
Iteration 78/1000 | Loss: 0.00023370
Iteration 79/1000 | Loss: 0.00007458
Iteration 80/1000 | Loss: 0.00009098
Iteration 81/1000 | Loss: 0.00008212
Iteration 82/1000 | Loss: 0.00041416
Iteration 83/1000 | Loss: 0.00058816
Iteration 84/1000 | Loss: 0.00041992
Iteration 85/1000 | Loss: 0.00003760
Iteration 86/1000 | Loss: 0.00010066
Iteration 87/1000 | Loss: 0.00005067
Iteration 88/1000 | Loss: 0.00002890
Iteration 89/1000 | Loss: 0.00004791
Iteration 90/1000 | Loss: 0.00002771
Iteration 91/1000 | Loss: 0.00036452
Iteration 92/1000 | Loss: 0.00003033
Iteration 93/1000 | Loss: 0.00002723
Iteration 94/1000 | Loss: 0.00059900
Iteration 95/1000 | Loss: 0.00003272
Iteration 96/1000 | Loss: 0.00017985
Iteration 97/1000 | Loss: 0.00046943
Iteration 98/1000 | Loss: 0.00003181
Iteration 99/1000 | Loss: 0.00002542
Iteration 100/1000 | Loss: 0.00002382
Iteration 101/1000 | Loss: 0.00002318
Iteration 102/1000 | Loss: 0.00002264
Iteration 103/1000 | Loss: 0.00034058
Iteration 104/1000 | Loss: 0.00072772
Iteration 105/1000 | Loss: 0.00059382
Iteration 106/1000 | Loss: 0.00008414
Iteration 107/1000 | Loss: 0.00004132
Iteration 108/1000 | Loss: 0.00002978
Iteration 109/1000 | Loss: 0.00002613
Iteration 110/1000 | Loss: 0.00002374
Iteration 111/1000 | Loss: 0.00002289
Iteration 112/1000 | Loss: 0.00039723
Iteration 113/1000 | Loss: 0.00004620
Iteration 114/1000 | Loss: 0.00002260
Iteration 115/1000 | Loss: 0.00012920
Iteration 116/1000 | Loss: 0.00002665
Iteration 117/1000 | Loss: 0.00004864
Iteration 118/1000 | Loss: 0.00002212
Iteration 119/1000 | Loss: 0.00002178
Iteration 120/1000 | Loss: 0.00002134
Iteration 121/1000 | Loss: 0.00059317
Iteration 122/1000 | Loss: 0.00054728
Iteration 123/1000 | Loss: 0.00055632
Iteration 124/1000 | Loss: 0.00009880
Iteration 125/1000 | Loss: 0.00002213
Iteration 126/1000 | Loss: 0.00001993
Iteration 127/1000 | Loss: 0.00001868
Iteration 128/1000 | Loss: 0.00025698
Iteration 129/1000 | Loss: 0.00002205
Iteration 130/1000 | Loss: 0.00020082
Iteration 131/1000 | Loss: 0.00018080
Iteration 132/1000 | Loss: 0.00003404
Iteration 133/1000 | Loss: 0.00001711
Iteration 134/1000 | Loss: 0.00001687
Iteration 135/1000 | Loss: 0.00001666
Iteration 136/1000 | Loss: 0.00001646
Iteration 137/1000 | Loss: 0.00001640
Iteration 138/1000 | Loss: 0.00001638
Iteration 139/1000 | Loss: 0.00001638
Iteration 140/1000 | Loss: 0.00001638
Iteration 141/1000 | Loss: 0.00001636
Iteration 142/1000 | Loss: 0.00001635
Iteration 143/1000 | Loss: 0.00001635
Iteration 144/1000 | Loss: 0.00001627
Iteration 145/1000 | Loss: 0.00001625
Iteration 146/1000 | Loss: 0.00001624
Iteration 147/1000 | Loss: 0.00001624
Iteration 148/1000 | Loss: 0.00001623
Iteration 149/1000 | Loss: 0.00001623
Iteration 150/1000 | Loss: 0.00001621
Iteration 151/1000 | Loss: 0.00041983
Iteration 152/1000 | Loss: 0.00002431
Iteration 153/1000 | Loss: 0.00001958
Iteration 154/1000 | Loss: 0.00001675
Iteration 155/1000 | Loss: 0.00001615
Iteration 156/1000 | Loss: 0.00001613
Iteration 157/1000 | Loss: 0.00001603
Iteration 158/1000 | Loss: 0.00001603
Iteration 159/1000 | Loss: 0.00001603
Iteration 160/1000 | Loss: 0.00001602
Iteration 161/1000 | Loss: 0.00001602
Iteration 162/1000 | Loss: 0.00001602
Iteration 163/1000 | Loss: 0.00001602
Iteration 164/1000 | Loss: 0.00001601
Iteration 165/1000 | Loss: 0.00001601
Iteration 166/1000 | Loss: 0.00001601
Iteration 167/1000 | Loss: 0.00001601
Iteration 168/1000 | Loss: 0.00001601
Iteration 169/1000 | Loss: 0.00001601
Iteration 170/1000 | Loss: 0.00001601
Iteration 171/1000 | Loss: 0.00001601
Iteration 172/1000 | Loss: 0.00001601
Iteration 173/1000 | Loss: 0.00001600
Iteration 174/1000 | Loss: 0.00001600
Iteration 175/1000 | Loss: 0.00001600
Iteration 176/1000 | Loss: 0.00001600
Iteration 177/1000 | Loss: 0.00001600
Iteration 178/1000 | Loss: 0.00001600
Iteration 179/1000 | Loss: 0.00001600
Iteration 180/1000 | Loss: 0.00001600
Iteration 181/1000 | Loss: 0.00001600
Iteration 182/1000 | Loss: 0.00001599
Iteration 183/1000 | Loss: 0.00001599
Iteration 184/1000 | Loss: 0.00001599
Iteration 185/1000 | Loss: 0.00001599
Iteration 186/1000 | Loss: 0.00001599
Iteration 187/1000 | Loss: 0.00001599
Iteration 188/1000 | Loss: 0.00001599
Iteration 189/1000 | Loss: 0.00001599
Iteration 190/1000 | Loss: 0.00001599
Iteration 191/1000 | Loss: 0.00001599
Iteration 192/1000 | Loss: 0.00001599
Iteration 193/1000 | Loss: 0.00001599
Iteration 194/1000 | Loss: 0.00001599
Iteration 195/1000 | Loss: 0.00001598
Iteration 196/1000 | Loss: 0.00001598
Iteration 197/1000 | Loss: 0.00001598
Iteration 198/1000 | Loss: 0.00001598
Iteration 199/1000 | Loss: 0.00001598
Iteration 200/1000 | Loss: 0.00001598
Iteration 201/1000 | Loss: 0.00001598
Iteration 202/1000 | Loss: 0.00001598
Iteration 203/1000 | Loss: 0.00001597
Iteration 204/1000 | Loss: 0.00001597
Iteration 205/1000 | Loss: 0.00001597
Iteration 206/1000 | Loss: 0.00001597
Iteration 207/1000 | Loss: 0.00001597
Iteration 208/1000 | Loss: 0.00001597
Iteration 209/1000 | Loss: 0.00001597
Iteration 210/1000 | Loss: 0.00001597
Iteration 211/1000 | Loss: 0.00001597
Iteration 212/1000 | Loss: 0.00001597
Iteration 213/1000 | Loss: 0.00001597
Iteration 214/1000 | Loss: 0.00001597
Iteration 215/1000 | Loss: 0.00001597
Iteration 216/1000 | Loss: 0.00001597
Iteration 217/1000 | Loss: 0.00001597
Iteration 218/1000 | Loss: 0.00001597
Iteration 219/1000 | Loss: 0.00001597
Iteration 220/1000 | Loss: 0.00001597
Iteration 221/1000 | Loss: 0.00001597
Iteration 222/1000 | Loss: 0.00001597
Iteration 223/1000 | Loss: 0.00001597
Iteration 224/1000 | Loss: 0.00001597
Iteration 225/1000 | Loss: 0.00001597
Iteration 226/1000 | Loss: 0.00001597
Iteration 227/1000 | Loss: 0.00001597
Iteration 228/1000 | Loss: 0.00001597
Iteration 229/1000 | Loss: 0.00001597
Iteration 230/1000 | Loss: 0.00001597
Iteration 231/1000 | Loss: 0.00001597
Iteration 232/1000 | Loss: 0.00001597
Iteration 233/1000 | Loss: 0.00001597
Iteration 234/1000 | Loss: 0.00001597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.5968627849360928e-05, 1.5968627849360928e-05, 1.5968627849360928e-05, 1.5968627849360928e-05, 1.5968627849360928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5968627849360928e-05

Optimization complete. Final v2v error: 3.225797414779663 mm

Highest mean error: 11.980396270751953 mm for frame 0

Lowest mean error: 2.830664873123169 mm for frame 28

Saving results

Total time: 276.4294490814209
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039399
Iteration 2/25 | Loss: 0.00167797
Iteration 3/25 | Loss: 0.00161547
Iteration 4/25 | Loss: 0.00125789
Iteration 5/25 | Loss: 0.00124440
Iteration 6/25 | Loss: 0.00123199
Iteration 7/25 | Loss: 0.00122955
Iteration 8/25 | Loss: 0.00122877
Iteration 9/25 | Loss: 0.00122844
Iteration 10/25 | Loss: 0.00122830
Iteration 11/25 | Loss: 0.00122829
Iteration 12/25 | Loss: 0.00122828
Iteration 13/25 | Loss: 0.00122828
Iteration 14/25 | Loss: 0.00122828
Iteration 15/25 | Loss: 0.00122827
Iteration 16/25 | Loss: 0.00122827
Iteration 17/25 | Loss: 0.00122827
Iteration 18/25 | Loss: 0.00122827
Iteration 19/25 | Loss: 0.00122827
Iteration 20/25 | Loss: 0.00122827
Iteration 21/25 | Loss: 0.00122826
Iteration 22/25 | Loss: 0.00122826
Iteration 23/25 | Loss: 0.00122826
Iteration 24/25 | Loss: 0.00122826
Iteration 25/25 | Loss: 0.00122826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.91511631
Iteration 2/25 | Loss: 0.00143970
Iteration 3/25 | Loss: 0.00102777
Iteration 4/25 | Loss: 0.00102777
Iteration 5/25 | Loss: 0.00102777
Iteration 6/25 | Loss: 0.00102777
Iteration 7/25 | Loss: 0.00102777
Iteration 8/25 | Loss: 0.00102777
Iteration 9/25 | Loss: 0.00102777
Iteration 10/25 | Loss: 0.00102777
Iteration 11/25 | Loss: 0.00102777
Iteration 12/25 | Loss: 0.00102777
Iteration 13/25 | Loss: 0.00102777
Iteration 14/25 | Loss: 0.00102777
Iteration 15/25 | Loss: 0.00102777
Iteration 16/25 | Loss: 0.00102777
Iteration 17/25 | Loss: 0.00102777
Iteration 18/25 | Loss: 0.00102777
Iteration 19/25 | Loss: 0.00102777
Iteration 20/25 | Loss: 0.00102777
Iteration 21/25 | Loss: 0.00102777
Iteration 22/25 | Loss: 0.00102777
Iteration 23/25 | Loss: 0.00102777
Iteration 24/25 | Loss: 0.00102777
Iteration 25/25 | Loss: 0.00102777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102777
Iteration 2/1000 | Loss: 0.00051874
Iteration 3/1000 | Loss: 0.00013611
Iteration 4/1000 | Loss: 0.00021452
Iteration 5/1000 | Loss: 0.00005678
Iteration 6/1000 | Loss: 0.00004610
Iteration 7/1000 | Loss: 0.00002005
Iteration 8/1000 | Loss: 0.00001573
Iteration 9/1000 | Loss: 0.00005479
Iteration 10/1000 | Loss: 0.00001482
Iteration 11/1000 | Loss: 0.00001452
Iteration 12/1000 | Loss: 0.00001417
Iteration 13/1000 | Loss: 0.00005244
Iteration 14/1000 | Loss: 0.00001382
Iteration 15/1000 | Loss: 0.00001360
Iteration 16/1000 | Loss: 0.00001338
Iteration 17/1000 | Loss: 0.00001336
Iteration 18/1000 | Loss: 0.00001331
Iteration 19/1000 | Loss: 0.00001329
Iteration 20/1000 | Loss: 0.00001320
Iteration 21/1000 | Loss: 0.00005969
Iteration 22/1000 | Loss: 0.00002300
Iteration 23/1000 | Loss: 0.00001303
Iteration 24/1000 | Loss: 0.00001302
Iteration 25/1000 | Loss: 0.00001302
Iteration 26/1000 | Loss: 0.00001301
Iteration 27/1000 | Loss: 0.00003085
Iteration 28/1000 | Loss: 0.00001298
Iteration 29/1000 | Loss: 0.00001295
Iteration 30/1000 | Loss: 0.00001295
Iteration 31/1000 | Loss: 0.00001295
Iteration 32/1000 | Loss: 0.00001295
Iteration 33/1000 | Loss: 0.00006046
Iteration 34/1000 | Loss: 0.00001303
Iteration 35/1000 | Loss: 0.00001294
Iteration 36/1000 | Loss: 0.00001294
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001294
Iteration 40/1000 | Loss: 0.00001294
Iteration 41/1000 | Loss: 0.00001294
Iteration 42/1000 | Loss: 0.00001294
Iteration 43/1000 | Loss: 0.00001293
Iteration 44/1000 | Loss: 0.00001293
Iteration 45/1000 | Loss: 0.00001293
Iteration 46/1000 | Loss: 0.00001292
Iteration 47/1000 | Loss: 0.00001292
Iteration 48/1000 | Loss: 0.00001292
Iteration 49/1000 | Loss: 0.00001292
Iteration 50/1000 | Loss: 0.00001290
Iteration 51/1000 | Loss: 0.00001290
Iteration 52/1000 | Loss: 0.00001289
Iteration 53/1000 | Loss: 0.00001289
Iteration 54/1000 | Loss: 0.00001289
Iteration 55/1000 | Loss: 0.00001288
Iteration 56/1000 | Loss: 0.00001288
Iteration 57/1000 | Loss: 0.00001288
Iteration 58/1000 | Loss: 0.00001288
Iteration 59/1000 | Loss: 0.00001288
Iteration 60/1000 | Loss: 0.00001288
Iteration 61/1000 | Loss: 0.00001288
Iteration 62/1000 | Loss: 0.00001288
Iteration 63/1000 | Loss: 0.00001287
Iteration 64/1000 | Loss: 0.00001287
Iteration 65/1000 | Loss: 0.00001287
Iteration 66/1000 | Loss: 0.00001287
Iteration 67/1000 | Loss: 0.00001287
Iteration 68/1000 | Loss: 0.00001286
Iteration 69/1000 | Loss: 0.00001286
Iteration 70/1000 | Loss: 0.00001286
Iteration 71/1000 | Loss: 0.00001285
Iteration 72/1000 | Loss: 0.00001285
Iteration 73/1000 | Loss: 0.00001285
Iteration 74/1000 | Loss: 0.00001285
Iteration 75/1000 | Loss: 0.00001284
Iteration 76/1000 | Loss: 0.00001282
Iteration 77/1000 | Loss: 0.00001282
Iteration 78/1000 | Loss: 0.00001282
Iteration 79/1000 | Loss: 0.00001282
Iteration 80/1000 | Loss: 0.00001282
Iteration 81/1000 | Loss: 0.00001282
Iteration 82/1000 | Loss: 0.00001281
Iteration 83/1000 | Loss: 0.00001281
Iteration 84/1000 | Loss: 0.00001281
Iteration 85/1000 | Loss: 0.00001281
Iteration 86/1000 | Loss: 0.00001280
Iteration 87/1000 | Loss: 0.00001280
Iteration 88/1000 | Loss: 0.00001280
Iteration 89/1000 | Loss: 0.00001280
Iteration 90/1000 | Loss: 0.00001279
Iteration 91/1000 | Loss: 0.00001279
Iteration 92/1000 | Loss: 0.00001279
Iteration 93/1000 | Loss: 0.00001278
Iteration 94/1000 | Loss: 0.00001278
Iteration 95/1000 | Loss: 0.00001278
Iteration 96/1000 | Loss: 0.00001277
Iteration 97/1000 | Loss: 0.00001277
Iteration 98/1000 | Loss: 0.00001277
Iteration 99/1000 | Loss: 0.00001277
Iteration 100/1000 | Loss: 0.00001277
Iteration 101/1000 | Loss: 0.00001276
Iteration 102/1000 | Loss: 0.00001276
Iteration 103/1000 | Loss: 0.00001276
Iteration 104/1000 | Loss: 0.00001276
Iteration 105/1000 | Loss: 0.00001276
Iteration 106/1000 | Loss: 0.00001276
Iteration 107/1000 | Loss: 0.00001276
Iteration 108/1000 | Loss: 0.00001275
Iteration 109/1000 | Loss: 0.00001275
Iteration 110/1000 | Loss: 0.00001274
Iteration 111/1000 | Loss: 0.00001274
Iteration 112/1000 | Loss: 0.00001273
Iteration 113/1000 | Loss: 0.00001273
Iteration 114/1000 | Loss: 0.00001273
Iteration 115/1000 | Loss: 0.00001272
Iteration 116/1000 | Loss: 0.00001272
Iteration 117/1000 | Loss: 0.00001272
Iteration 118/1000 | Loss: 0.00001271
Iteration 119/1000 | Loss: 0.00001271
Iteration 120/1000 | Loss: 0.00001270
Iteration 121/1000 | Loss: 0.00001269
Iteration 122/1000 | Loss: 0.00001269
Iteration 123/1000 | Loss: 0.00001269
Iteration 124/1000 | Loss: 0.00001268
Iteration 125/1000 | Loss: 0.00001268
Iteration 126/1000 | Loss: 0.00001268
Iteration 127/1000 | Loss: 0.00001268
Iteration 128/1000 | Loss: 0.00001268
Iteration 129/1000 | Loss: 0.00001268
Iteration 130/1000 | Loss: 0.00001268
Iteration 131/1000 | Loss: 0.00001268
Iteration 132/1000 | Loss: 0.00001268
Iteration 133/1000 | Loss: 0.00001267
Iteration 134/1000 | Loss: 0.00001267
Iteration 135/1000 | Loss: 0.00001267
Iteration 136/1000 | Loss: 0.00001267
Iteration 137/1000 | Loss: 0.00001267
Iteration 138/1000 | Loss: 0.00001267
Iteration 139/1000 | Loss: 0.00001266
Iteration 140/1000 | Loss: 0.00001266
Iteration 141/1000 | Loss: 0.00001266
Iteration 142/1000 | Loss: 0.00001265
Iteration 143/1000 | Loss: 0.00001265
Iteration 144/1000 | Loss: 0.00001265
Iteration 145/1000 | Loss: 0.00001265
Iteration 146/1000 | Loss: 0.00001265
Iteration 147/1000 | Loss: 0.00001265
Iteration 148/1000 | Loss: 0.00001264
Iteration 149/1000 | Loss: 0.00001264
Iteration 150/1000 | Loss: 0.00005076
Iteration 151/1000 | Loss: 0.00003378
Iteration 152/1000 | Loss: 0.00001276
Iteration 153/1000 | Loss: 0.00001261
Iteration 154/1000 | Loss: 0.00001260
Iteration 155/1000 | Loss: 0.00001260
Iteration 156/1000 | Loss: 0.00001260
Iteration 157/1000 | Loss: 0.00001260
Iteration 158/1000 | Loss: 0.00001260
Iteration 159/1000 | Loss: 0.00001260
Iteration 160/1000 | Loss: 0.00001260
Iteration 161/1000 | Loss: 0.00001260
Iteration 162/1000 | Loss: 0.00001260
Iteration 163/1000 | Loss: 0.00001260
Iteration 164/1000 | Loss: 0.00001260
Iteration 165/1000 | Loss: 0.00001260
Iteration 166/1000 | Loss: 0.00001260
Iteration 167/1000 | Loss: 0.00001259
Iteration 168/1000 | Loss: 0.00001259
Iteration 169/1000 | Loss: 0.00001259
Iteration 170/1000 | Loss: 0.00001259
Iteration 171/1000 | Loss: 0.00001259
Iteration 172/1000 | Loss: 0.00001259
Iteration 173/1000 | Loss: 0.00001259
Iteration 174/1000 | Loss: 0.00001259
Iteration 175/1000 | Loss: 0.00001259
Iteration 176/1000 | Loss: 0.00001259
Iteration 177/1000 | Loss: 0.00001259
Iteration 178/1000 | Loss: 0.00001259
Iteration 179/1000 | Loss: 0.00001258
Iteration 180/1000 | Loss: 0.00001258
Iteration 181/1000 | Loss: 0.00001258
Iteration 182/1000 | Loss: 0.00001258
Iteration 183/1000 | Loss: 0.00001258
Iteration 184/1000 | Loss: 0.00001258
Iteration 185/1000 | Loss: 0.00001258
Iteration 186/1000 | Loss: 0.00001258
Iteration 187/1000 | Loss: 0.00001258
Iteration 188/1000 | Loss: 0.00001258
Iteration 189/1000 | Loss: 0.00001258
Iteration 190/1000 | Loss: 0.00001258
Iteration 191/1000 | Loss: 0.00001258
Iteration 192/1000 | Loss: 0.00001258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.2580945622175932e-05, 1.2580945622175932e-05, 1.2580945622175932e-05, 1.2580945622175932e-05, 1.2580945622175932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2580945622175932e-05

Optimization complete. Final v2v error: 3.032001256942749 mm

Highest mean error: 3.298201084136963 mm for frame 4

Lowest mean error: 2.793164014816284 mm for frame 30

Saving results

Total time: 65.25920391082764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044845
Iteration 2/25 | Loss: 0.01044845
Iteration 3/25 | Loss: 0.01044845
Iteration 4/25 | Loss: 0.01044845
Iteration 5/25 | Loss: 0.01044845
Iteration 6/25 | Loss: 0.01044845
Iteration 7/25 | Loss: 0.01044844
Iteration 8/25 | Loss: 0.01044844
Iteration 9/25 | Loss: 0.01044844
Iteration 10/25 | Loss: 0.01044844
Iteration 11/25 | Loss: 0.01044844
Iteration 12/25 | Loss: 0.01044844
Iteration 13/25 | Loss: 0.01044844
Iteration 14/25 | Loss: 0.01044844
Iteration 15/25 | Loss: 0.01044843
Iteration 16/25 | Loss: 0.01044843
Iteration 17/25 | Loss: 0.01044843
Iteration 18/25 | Loss: 0.01044843
Iteration 19/25 | Loss: 0.01044843
Iteration 20/25 | Loss: 0.01044843
Iteration 21/25 | Loss: 0.01044842
Iteration 22/25 | Loss: 0.01044842
Iteration 23/25 | Loss: 0.01044842
Iteration 24/25 | Loss: 0.01044842
Iteration 25/25 | Loss: 0.01044842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81453812
Iteration 2/25 | Loss: 0.10157891
Iteration 3/25 | Loss: 0.10037258
Iteration 4/25 | Loss: 0.09969217
Iteration 5/25 | Loss: 0.10229580
Iteration 6/25 | Loss: 0.10005663
Iteration 7/25 | Loss: 0.09917895
Iteration 8/25 | Loss: 0.09917890
Iteration 9/25 | Loss: 0.09917885
Iteration 10/25 | Loss: 0.09917884
Iteration 11/25 | Loss: 0.09917884
Iteration 12/25 | Loss: 0.09917883
Iteration 13/25 | Loss: 0.09917883
Iteration 14/25 | Loss: 0.09917883
Iteration 15/25 | Loss: 0.09917883
Iteration 16/25 | Loss: 0.09917883
Iteration 17/25 | Loss: 0.09917883
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.09917882829904556, 0.09917882829904556, 0.09917882829904556, 0.09917882829904556, 0.09917882829904556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09917882829904556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09917883
Iteration 2/1000 | Loss: 0.00302380
Iteration 3/1000 | Loss: 0.00140518
Iteration 4/1000 | Loss: 0.00045907
Iteration 5/1000 | Loss: 0.00054457
Iteration 6/1000 | Loss: 0.00011664
Iteration 7/1000 | Loss: 0.00027643
Iteration 8/1000 | Loss: 0.00005280
Iteration 9/1000 | Loss: 0.00009030
Iteration 10/1000 | Loss: 0.00004376
Iteration 11/1000 | Loss: 0.00008080
Iteration 12/1000 | Loss: 0.00006926
Iteration 13/1000 | Loss: 0.00002665
Iteration 14/1000 | Loss: 0.00004569
Iteration 15/1000 | Loss: 0.00002242
Iteration 16/1000 | Loss: 0.00002641
Iteration 17/1000 | Loss: 0.00001952
Iteration 18/1000 | Loss: 0.00008286
Iteration 19/1000 | Loss: 0.00001834
Iteration 20/1000 | Loss: 0.00005774
Iteration 21/1000 | Loss: 0.00003253
Iteration 22/1000 | Loss: 0.00001953
Iteration 23/1000 | Loss: 0.00001615
Iteration 24/1000 | Loss: 0.00003826
Iteration 25/1000 | Loss: 0.00001687
Iteration 26/1000 | Loss: 0.00001520
Iteration 27/1000 | Loss: 0.00003982
Iteration 28/1000 | Loss: 0.00001482
Iteration 29/1000 | Loss: 0.00002500
Iteration 30/1000 | Loss: 0.00001425
Iteration 31/1000 | Loss: 0.00002385
Iteration 32/1000 | Loss: 0.00004550
Iteration 33/1000 | Loss: 0.00002351
Iteration 34/1000 | Loss: 0.00001367
Iteration 35/1000 | Loss: 0.00001355
Iteration 36/1000 | Loss: 0.00001354
Iteration 37/1000 | Loss: 0.00002686
Iteration 38/1000 | Loss: 0.00001354
Iteration 39/1000 | Loss: 0.00001350
Iteration 40/1000 | Loss: 0.00001347
Iteration 41/1000 | Loss: 0.00002774
Iteration 42/1000 | Loss: 0.00001345
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001320
Iteration 45/1000 | Loss: 0.00001316
Iteration 46/1000 | Loss: 0.00001315
Iteration 47/1000 | Loss: 0.00001314
Iteration 48/1000 | Loss: 0.00001313
Iteration 49/1000 | Loss: 0.00003831
Iteration 50/1000 | Loss: 0.00005707
Iteration 51/1000 | Loss: 0.00002973
Iteration 52/1000 | Loss: 0.00002145
Iteration 53/1000 | Loss: 0.00001314
Iteration 54/1000 | Loss: 0.00001308
Iteration 55/1000 | Loss: 0.00001308
Iteration 56/1000 | Loss: 0.00001308
Iteration 57/1000 | Loss: 0.00001307
Iteration 58/1000 | Loss: 0.00001307
Iteration 59/1000 | Loss: 0.00001307
Iteration 60/1000 | Loss: 0.00001307
Iteration 61/1000 | Loss: 0.00001306
Iteration 62/1000 | Loss: 0.00001306
Iteration 63/1000 | Loss: 0.00001306
Iteration 64/1000 | Loss: 0.00001305
Iteration 65/1000 | Loss: 0.00001305
Iteration 66/1000 | Loss: 0.00001304
Iteration 67/1000 | Loss: 0.00001304
Iteration 68/1000 | Loss: 0.00001303
Iteration 69/1000 | Loss: 0.00001631
Iteration 70/1000 | Loss: 0.00001302
Iteration 71/1000 | Loss: 0.00001301
Iteration 72/1000 | Loss: 0.00001301
Iteration 73/1000 | Loss: 0.00001300
Iteration 74/1000 | Loss: 0.00001299
Iteration 75/1000 | Loss: 0.00001541
Iteration 76/1000 | Loss: 0.00001298
Iteration 77/1000 | Loss: 0.00001297
Iteration 78/1000 | Loss: 0.00001297
Iteration 79/1000 | Loss: 0.00001297
Iteration 80/1000 | Loss: 0.00001296
Iteration 81/1000 | Loss: 0.00001296
Iteration 82/1000 | Loss: 0.00001296
Iteration 83/1000 | Loss: 0.00001296
Iteration 84/1000 | Loss: 0.00001295
Iteration 85/1000 | Loss: 0.00001295
Iteration 86/1000 | Loss: 0.00001295
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001294
Iteration 89/1000 | Loss: 0.00001294
Iteration 90/1000 | Loss: 0.00001293
Iteration 91/1000 | Loss: 0.00001293
Iteration 92/1000 | Loss: 0.00001293
Iteration 93/1000 | Loss: 0.00001488
Iteration 94/1000 | Loss: 0.00001290
Iteration 95/1000 | Loss: 0.00001290
Iteration 96/1000 | Loss: 0.00001290
Iteration 97/1000 | Loss: 0.00001290
Iteration 98/1000 | Loss: 0.00001289
Iteration 99/1000 | Loss: 0.00001289
Iteration 100/1000 | Loss: 0.00001289
Iteration 101/1000 | Loss: 0.00001289
Iteration 102/1000 | Loss: 0.00001289
Iteration 103/1000 | Loss: 0.00001289
Iteration 104/1000 | Loss: 0.00001289
Iteration 105/1000 | Loss: 0.00001289
Iteration 106/1000 | Loss: 0.00001289
Iteration 107/1000 | Loss: 0.00001289
Iteration 108/1000 | Loss: 0.00001289
Iteration 109/1000 | Loss: 0.00001289
Iteration 110/1000 | Loss: 0.00001289
Iteration 111/1000 | Loss: 0.00001289
Iteration 112/1000 | Loss: 0.00001289
Iteration 113/1000 | Loss: 0.00001289
Iteration 114/1000 | Loss: 0.00001289
Iteration 115/1000 | Loss: 0.00001289
Iteration 116/1000 | Loss: 0.00001288
Iteration 117/1000 | Loss: 0.00001288
Iteration 118/1000 | Loss: 0.00001288
Iteration 119/1000 | Loss: 0.00001288
Iteration 120/1000 | Loss: 0.00001288
Iteration 121/1000 | Loss: 0.00001286
Iteration 122/1000 | Loss: 0.00001286
Iteration 123/1000 | Loss: 0.00001286
Iteration 124/1000 | Loss: 0.00001286
Iteration 125/1000 | Loss: 0.00001286
Iteration 126/1000 | Loss: 0.00001286
Iteration 127/1000 | Loss: 0.00001286
Iteration 128/1000 | Loss: 0.00001286
Iteration 129/1000 | Loss: 0.00001285
Iteration 130/1000 | Loss: 0.00001285
Iteration 131/1000 | Loss: 0.00001285
Iteration 132/1000 | Loss: 0.00001285
Iteration 133/1000 | Loss: 0.00001285
Iteration 134/1000 | Loss: 0.00001285
Iteration 135/1000 | Loss: 0.00001284
Iteration 136/1000 | Loss: 0.00001284
Iteration 137/1000 | Loss: 0.00001284
Iteration 138/1000 | Loss: 0.00001284
Iteration 139/1000 | Loss: 0.00001284
Iteration 140/1000 | Loss: 0.00001284
Iteration 141/1000 | Loss: 0.00001283
Iteration 142/1000 | Loss: 0.00001283
Iteration 143/1000 | Loss: 0.00001283
Iteration 144/1000 | Loss: 0.00001283
Iteration 145/1000 | Loss: 0.00001283
Iteration 146/1000 | Loss: 0.00001283
Iteration 147/1000 | Loss: 0.00001283
Iteration 148/1000 | Loss: 0.00001283
Iteration 149/1000 | Loss: 0.00001283
Iteration 150/1000 | Loss: 0.00001283
Iteration 151/1000 | Loss: 0.00001283
Iteration 152/1000 | Loss: 0.00001283
Iteration 153/1000 | Loss: 0.00001283
Iteration 154/1000 | Loss: 0.00001282
Iteration 155/1000 | Loss: 0.00001282
Iteration 156/1000 | Loss: 0.00001282
Iteration 157/1000 | Loss: 0.00001282
Iteration 158/1000 | Loss: 0.00001282
Iteration 159/1000 | Loss: 0.00001282
Iteration 160/1000 | Loss: 0.00001282
Iteration 161/1000 | Loss: 0.00001282
Iteration 162/1000 | Loss: 0.00001281
Iteration 163/1000 | Loss: 0.00001281
Iteration 164/1000 | Loss: 0.00001281
Iteration 165/1000 | Loss: 0.00001281
Iteration 166/1000 | Loss: 0.00001281
Iteration 167/1000 | Loss: 0.00001281
Iteration 168/1000 | Loss: 0.00001281
Iteration 169/1000 | Loss: 0.00001280
Iteration 170/1000 | Loss: 0.00001280
Iteration 171/1000 | Loss: 0.00001280
Iteration 172/1000 | Loss: 0.00001279
Iteration 173/1000 | Loss: 0.00001279
Iteration 174/1000 | Loss: 0.00001279
Iteration 175/1000 | Loss: 0.00001279
Iteration 176/1000 | Loss: 0.00001279
Iteration 177/1000 | Loss: 0.00001279
Iteration 178/1000 | Loss: 0.00001279
Iteration 179/1000 | Loss: 0.00001279
Iteration 180/1000 | Loss: 0.00001279
Iteration 181/1000 | Loss: 0.00001279
Iteration 182/1000 | Loss: 0.00001279
Iteration 183/1000 | Loss: 0.00001278
Iteration 184/1000 | Loss: 0.00001278
Iteration 185/1000 | Loss: 0.00001278
Iteration 186/1000 | Loss: 0.00001278
Iteration 187/1000 | Loss: 0.00001278
Iteration 188/1000 | Loss: 0.00001278
Iteration 189/1000 | Loss: 0.00001278
Iteration 190/1000 | Loss: 0.00001278
Iteration 191/1000 | Loss: 0.00001278
Iteration 192/1000 | Loss: 0.00001278
Iteration 193/1000 | Loss: 0.00001278
Iteration 194/1000 | Loss: 0.00001278
Iteration 195/1000 | Loss: 0.00001278
Iteration 196/1000 | Loss: 0.00001278
Iteration 197/1000 | Loss: 0.00001278
Iteration 198/1000 | Loss: 0.00001278
Iteration 199/1000 | Loss: 0.00001278
Iteration 200/1000 | Loss: 0.00001278
Iteration 201/1000 | Loss: 0.00001278
Iteration 202/1000 | Loss: 0.00001277
Iteration 203/1000 | Loss: 0.00001277
Iteration 204/1000 | Loss: 0.00001277
Iteration 205/1000 | Loss: 0.00001277
Iteration 206/1000 | Loss: 0.00001277
Iteration 207/1000 | Loss: 0.00001277
Iteration 208/1000 | Loss: 0.00001277
Iteration 209/1000 | Loss: 0.00001277
Iteration 210/1000 | Loss: 0.00001277
Iteration 211/1000 | Loss: 0.00001277
Iteration 212/1000 | Loss: 0.00001277
Iteration 213/1000 | Loss: 0.00001277
Iteration 214/1000 | Loss: 0.00001277
Iteration 215/1000 | Loss: 0.00001277
Iteration 216/1000 | Loss: 0.00001277
Iteration 217/1000 | Loss: 0.00001277
Iteration 218/1000 | Loss: 0.00001277
Iteration 219/1000 | Loss: 0.00001277
Iteration 220/1000 | Loss: 0.00001277
Iteration 221/1000 | Loss: 0.00001277
Iteration 222/1000 | Loss: 0.00001277
Iteration 223/1000 | Loss: 0.00001277
Iteration 224/1000 | Loss: 0.00001277
Iteration 225/1000 | Loss: 0.00001277
Iteration 226/1000 | Loss: 0.00001276
Iteration 227/1000 | Loss: 0.00001276
Iteration 228/1000 | Loss: 0.00001276
Iteration 229/1000 | Loss: 0.00001276
Iteration 230/1000 | Loss: 0.00001276
Iteration 231/1000 | Loss: 0.00001276
Iteration 232/1000 | Loss: 0.00001276
Iteration 233/1000 | Loss: 0.00001276
Iteration 234/1000 | Loss: 0.00001276
Iteration 235/1000 | Loss: 0.00001276
Iteration 236/1000 | Loss: 0.00001276
Iteration 237/1000 | Loss: 0.00001276
Iteration 238/1000 | Loss: 0.00001276
Iteration 239/1000 | Loss: 0.00001276
Iteration 240/1000 | Loss: 0.00001276
Iteration 241/1000 | Loss: 0.00001276
Iteration 242/1000 | Loss: 0.00001276
Iteration 243/1000 | Loss: 0.00001276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.2764201528625563e-05, 1.2764201528625563e-05, 1.2764201528625563e-05, 1.2764201528625563e-05, 1.2764201528625563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2764201528625563e-05

Optimization complete. Final v2v error: 3.0623974800109863 mm

Highest mean error: 3.667236804962158 mm for frame 205

Lowest mean error: 2.641599655151367 mm for frame 236

Saving results

Total time: 102.15593600273132
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848154
Iteration 2/25 | Loss: 0.00189042
Iteration 3/25 | Loss: 0.00158972
Iteration 4/25 | Loss: 0.00151268
Iteration 5/25 | Loss: 0.00150649
Iteration 6/25 | Loss: 0.00150801
Iteration 7/25 | Loss: 0.00150903
Iteration 8/25 | Loss: 0.00150112
Iteration 9/25 | Loss: 0.00150006
Iteration 10/25 | Loss: 0.00149986
Iteration 11/25 | Loss: 0.00149878
Iteration 12/25 | Loss: 0.00149844
Iteration 13/25 | Loss: 0.00149829
Iteration 14/25 | Loss: 0.00149807
Iteration 15/25 | Loss: 0.00149879
Iteration 16/25 | Loss: 0.00149745
Iteration 17/25 | Loss: 0.00149668
Iteration 18/25 | Loss: 0.00149460
Iteration 19/25 | Loss: 0.00149383
Iteration 20/25 | Loss: 0.00149367
Iteration 21/25 | Loss: 0.00149365
Iteration 22/25 | Loss: 0.00149364
Iteration 23/25 | Loss: 0.00149364
Iteration 24/25 | Loss: 0.00149364
Iteration 25/25 | Loss: 0.00149364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.29887223
Iteration 2/25 | Loss: 0.00132943
Iteration 3/25 | Loss: 0.00132943
Iteration 4/25 | Loss: 0.00132943
Iteration 5/25 | Loss: 0.00132943
Iteration 6/25 | Loss: 0.00132943
Iteration 7/25 | Loss: 0.00132943
Iteration 8/25 | Loss: 0.00132943
Iteration 9/25 | Loss: 0.00132943
Iteration 10/25 | Loss: 0.00132943
Iteration 11/25 | Loss: 0.00132942
Iteration 12/25 | Loss: 0.00132942
Iteration 13/25 | Loss: 0.00132942
Iteration 14/25 | Loss: 0.00132942
Iteration 15/25 | Loss: 0.00132942
Iteration 16/25 | Loss: 0.00132942
Iteration 17/25 | Loss: 0.00132942
Iteration 18/25 | Loss: 0.00132942
Iteration 19/25 | Loss: 0.00132942
Iteration 20/25 | Loss: 0.00132942
Iteration 21/25 | Loss: 0.00132942
Iteration 22/25 | Loss: 0.00132942
Iteration 23/25 | Loss: 0.00132942
Iteration 24/25 | Loss: 0.00132942
Iteration 25/25 | Loss: 0.00132942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132942
Iteration 2/1000 | Loss: 0.00028943
Iteration 3/1000 | Loss: 0.00048363
Iteration 4/1000 | Loss: 0.00060996
Iteration 5/1000 | Loss: 0.00036207
Iteration 6/1000 | Loss: 0.00028034
Iteration 7/1000 | Loss: 0.00005176
Iteration 8/1000 | Loss: 0.00008849
Iteration 9/1000 | Loss: 0.00004746
Iteration 10/1000 | Loss: 0.00003777
Iteration 11/1000 | Loss: 0.00007746
Iteration 12/1000 | Loss: 0.00003580
Iteration 13/1000 | Loss: 0.00008114
Iteration 14/1000 | Loss: 0.00003476
Iteration 15/1000 | Loss: 0.00012318
Iteration 16/1000 | Loss: 0.00003434
Iteration 17/1000 | Loss: 0.00003375
Iteration 18/1000 | Loss: 0.00009921
Iteration 19/1000 | Loss: 0.00008991
Iteration 20/1000 | Loss: 0.00011300
Iteration 21/1000 | Loss: 0.00042191
Iteration 22/1000 | Loss: 0.00006495
Iteration 23/1000 | Loss: 0.00003297
Iteration 24/1000 | Loss: 0.00003271
Iteration 25/1000 | Loss: 0.00014533
Iteration 26/1000 | Loss: 0.00015025
Iteration 27/1000 | Loss: 0.00003781
Iteration 28/1000 | Loss: 0.00003996
Iteration 29/1000 | Loss: 0.00003215
Iteration 30/1000 | Loss: 0.00003197
Iteration 31/1000 | Loss: 0.00003186
Iteration 32/1000 | Loss: 0.00013339
Iteration 33/1000 | Loss: 0.00004356
Iteration 34/1000 | Loss: 0.00006775
Iteration 35/1000 | Loss: 0.00003824
Iteration 36/1000 | Loss: 0.00003160
Iteration 37/1000 | Loss: 0.00003159
Iteration 38/1000 | Loss: 0.00003159
Iteration 39/1000 | Loss: 0.00003157
Iteration 40/1000 | Loss: 0.00003157
Iteration 41/1000 | Loss: 0.00003156
Iteration 42/1000 | Loss: 0.00003155
Iteration 43/1000 | Loss: 0.00003154
Iteration 44/1000 | Loss: 0.00003154
Iteration 45/1000 | Loss: 0.00003154
Iteration 46/1000 | Loss: 0.00003154
Iteration 47/1000 | Loss: 0.00003154
Iteration 48/1000 | Loss: 0.00003154
Iteration 49/1000 | Loss: 0.00003154
Iteration 50/1000 | Loss: 0.00003154
Iteration 51/1000 | Loss: 0.00003154
Iteration 52/1000 | Loss: 0.00003153
Iteration 53/1000 | Loss: 0.00003153
Iteration 54/1000 | Loss: 0.00003153
Iteration 55/1000 | Loss: 0.00003152
Iteration 56/1000 | Loss: 0.00003152
Iteration 57/1000 | Loss: 0.00003152
Iteration 58/1000 | Loss: 0.00003151
Iteration 59/1000 | Loss: 0.00003151
Iteration 60/1000 | Loss: 0.00003151
Iteration 61/1000 | Loss: 0.00003151
Iteration 62/1000 | Loss: 0.00003150
Iteration 63/1000 | Loss: 0.00003149
Iteration 64/1000 | Loss: 0.00003148
Iteration 65/1000 | Loss: 0.00003148
Iteration 66/1000 | Loss: 0.00006450
Iteration 67/1000 | Loss: 0.00003145
Iteration 68/1000 | Loss: 0.00003141
Iteration 69/1000 | Loss: 0.00006254
Iteration 70/1000 | Loss: 0.00003151
Iteration 71/1000 | Loss: 0.00003143
Iteration 72/1000 | Loss: 0.00003139
Iteration 73/1000 | Loss: 0.00003139
Iteration 74/1000 | Loss: 0.00003139
Iteration 75/1000 | Loss: 0.00003139
Iteration 76/1000 | Loss: 0.00003138
Iteration 77/1000 | Loss: 0.00007664
Iteration 78/1000 | Loss: 0.00003157
Iteration 79/1000 | Loss: 0.00004596
Iteration 80/1000 | Loss: 0.00003148
Iteration 81/1000 | Loss: 0.00003724
Iteration 82/1000 | Loss: 0.00003135
Iteration 83/1000 | Loss: 0.00003133
Iteration 84/1000 | Loss: 0.00003132
Iteration 85/1000 | Loss: 0.00003132
Iteration 86/1000 | Loss: 0.00003132
Iteration 87/1000 | Loss: 0.00003131
Iteration 88/1000 | Loss: 0.00003131
Iteration 89/1000 | Loss: 0.00003131
Iteration 90/1000 | Loss: 0.00003131
Iteration 91/1000 | Loss: 0.00003131
Iteration 92/1000 | Loss: 0.00003131
Iteration 93/1000 | Loss: 0.00003131
Iteration 94/1000 | Loss: 0.00003131
Iteration 95/1000 | Loss: 0.00003130
Iteration 96/1000 | Loss: 0.00003130
Iteration 97/1000 | Loss: 0.00003130
Iteration 98/1000 | Loss: 0.00003130
Iteration 99/1000 | Loss: 0.00003130
Iteration 100/1000 | Loss: 0.00003130
Iteration 101/1000 | Loss: 0.00003130
Iteration 102/1000 | Loss: 0.00003130
Iteration 103/1000 | Loss: 0.00003130
Iteration 104/1000 | Loss: 0.00003129
Iteration 105/1000 | Loss: 0.00003129
Iteration 106/1000 | Loss: 0.00003129
Iteration 107/1000 | Loss: 0.00003129
Iteration 108/1000 | Loss: 0.00003129
Iteration 109/1000 | Loss: 0.00003129
Iteration 110/1000 | Loss: 0.00003129
Iteration 111/1000 | Loss: 0.00003129
Iteration 112/1000 | Loss: 0.00003129
Iteration 113/1000 | Loss: 0.00003129
Iteration 114/1000 | Loss: 0.00003129
Iteration 115/1000 | Loss: 0.00003129
Iteration 116/1000 | Loss: 0.00003129
Iteration 117/1000 | Loss: 0.00003129
Iteration 118/1000 | Loss: 0.00003129
Iteration 119/1000 | Loss: 0.00003129
Iteration 120/1000 | Loss: 0.00003128
Iteration 121/1000 | Loss: 0.00003128
Iteration 122/1000 | Loss: 0.00003128
Iteration 123/1000 | Loss: 0.00003128
Iteration 124/1000 | Loss: 0.00003128
Iteration 125/1000 | Loss: 0.00003128
Iteration 126/1000 | Loss: 0.00003128
Iteration 127/1000 | Loss: 0.00003128
Iteration 128/1000 | Loss: 0.00003128
Iteration 129/1000 | Loss: 0.00003128
Iteration 130/1000 | Loss: 0.00003128
Iteration 131/1000 | Loss: 0.00003128
Iteration 132/1000 | Loss: 0.00003128
Iteration 133/1000 | Loss: 0.00003128
Iteration 134/1000 | Loss: 0.00003128
Iteration 135/1000 | Loss: 0.00003127
Iteration 136/1000 | Loss: 0.00003127
Iteration 137/1000 | Loss: 0.00003127
Iteration 138/1000 | Loss: 0.00003127
Iteration 139/1000 | Loss: 0.00003127
Iteration 140/1000 | Loss: 0.00003127
Iteration 141/1000 | Loss: 0.00003127
Iteration 142/1000 | Loss: 0.00003127
Iteration 143/1000 | Loss: 0.00003127
Iteration 144/1000 | Loss: 0.00003127
Iteration 145/1000 | Loss: 0.00003127
Iteration 146/1000 | Loss: 0.00003127
Iteration 147/1000 | Loss: 0.00003127
Iteration 148/1000 | Loss: 0.00003127
Iteration 149/1000 | Loss: 0.00003126
Iteration 150/1000 | Loss: 0.00003126
Iteration 151/1000 | Loss: 0.00003126
Iteration 152/1000 | Loss: 0.00003126
Iteration 153/1000 | Loss: 0.00003126
Iteration 154/1000 | Loss: 0.00003126
Iteration 155/1000 | Loss: 0.00003126
Iteration 156/1000 | Loss: 0.00003126
Iteration 157/1000 | Loss: 0.00003126
Iteration 158/1000 | Loss: 0.00003126
Iteration 159/1000 | Loss: 0.00003126
Iteration 160/1000 | Loss: 0.00003126
Iteration 161/1000 | Loss: 0.00003125
Iteration 162/1000 | Loss: 0.00003125
Iteration 163/1000 | Loss: 0.00003125
Iteration 164/1000 | Loss: 0.00003125
Iteration 165/1000 | Loss: 0.00003125
Iteration 166/1000 | Loss: 0.00003125
Iteration 167/1000 | Loss: 0.00003125
Iteration 168/1000 | Loss: 0.00003125
Iteration 169/1000 | Loss: 0.00003125
Iteration 170/1000 | Loss: 0.00003125
Iteration 171/1000 | Loss: 0.00003125
Iteration 172/1000 | Loss: 0.00003125
Iteration 173/1000 | Loss: 0.00003125
Iteration 174/1000 | Loss: 0.00003125
Iteration 175/1000 | Loss: 0.00003125
Iteration 176/1000 | Loss: 0.00003125
Iteration 177/1000 | Loss: 0.00003125
Iteration 178/1000 | Loss: 0.00003125
Iteration 179/1000 | Loss: 0.00003125
Iteration 180/1000 | Loss: 0.00003125
Iteration 181/1000 | Loss: 0.00003125
Iteration 182/1000 | Loss: 0.00003125
Iteration 183/1000 | Loss: 0.00003125
Iteration 184/1000 | Loss: 0.00003125
Iteration 185/1000 | Loss: 0.00003125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [3.1246698199538514e-05, 3.1246698199538514e-05, 3.1246698199538514e-05, 3.1246698199538514e-05, 3.1246698199538514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1246698199538514e-05

Optimization complete. Final v2v error: 4.496750831604004 mm

Highest mean error: 5.500847339630127 mm for frame 224

Lowest mean error: 4.007960796356201 mm for frame 1

Saving results

Total time: 120.26459169387817
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434015
Iteration 2/25 | Loss: 0.00144044
Iteration 3/25 | Loss: 0.00128526
Iteration 4/25 | Loss: 0.00127950
Iteration 5/25 | Loss: 0.00127813
Iteration 6/25 | Loss: 0.00127813
Iteration 7/25 | Loss: 0.00127813
Iteration 8/25 | Loss: 0.00127813
Iteration 9/25 | Loss: 0.00127813
Iteration 10/25 | Loss: 0.00127813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001278125331737101, 0.001278125331737101, 0.001278125331737101, 0.001278125331737101, 0.001278125331737101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001278125331737101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.81108475
Iteration 2/25 | Loss: 0.00093129
Iteration 3/25 | Loss: 0.00093125
Iteration 4/25 | Loss: 0.00093125
Iteration 5/25 | Loss: 0.00093125
Iteration 6/25 | Loss: 0.00093125
Iteration 7/25 | Loss: 0.00093125
Iteration 8/25 | Loss: 0.00093125
Iteration 9/25 | Loss: 0.00093125
Iteration 10/25 | Loss: 0.00093125
Iteration 11/25 | Loss: 0.00093125
Iteration 12/25 | Loss: 0.00093125
Iteration 13/25 | Loss: 0.00093125
Iteration 14/25 | Loss: 0.00093125
Iteration 15/25 | Loss: 0.00093125
Iteration 16/25 | Loss: 0.00093125
Iteration 17/25 | Loss: 0.00093125
Iteration 18/25 | Loss: 0.00093125
Iteration 19/25 | Loss: 0.00093125
Iteration 20/25 | Loss: 0.00093125
Iteration 21/25 | Loss: 0.00093125
Iteration 22/25 | Loss: 0.00093125
Iteration 23/25 | Loss: 0.00093125
Iteration 24/25 | Loss: 0.00093125
Iteration 25/25 | Loss: 0.00093125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093125
Iteration 2/1000 | Loss: 0.00002980
Iteration 3/1000 | Loss: 0.00002188
Iteration 4/1000 | Loss: 0.00001906
Iteration 5/1000 | Loss: 0.00001769
Iteration 6/1000 | Loss: 0.00001695
Iteration 7/1000 | Loss: 0.00001632
Iteration 8/1000 | Loss: 0.00001578
Iteration 9/1000 | Loss: 0.00001553
Iteration 10/1000 | Loss: 0.00001525
Iteration 11/1000 | Loss: 0.00001510
Iteration 12/1000 | Loss: 0.00001494
Iteration 13/1000 | Loss: 0.00001492
Iteration 14/1000 | Loss: 0.00001486
Iteration 15/1000 | Loss: 0.00001473
Iteration 16/1000 | Loss: 0.00001470
Iteration 17/1000 | Loss: 0.00001466
Iteration 18/1000 | Loss: 0.00001466
Iteration 19/1000 | Loss: 0.00001466
Iteration 20/1000 | Loss: 0.00001465
Iteration 21/1000 | Loss: 0.00001465
Iteration 22/1000 | Loss: 0.00001465
Iteration 23/1000 | Loss: 0.00001465
Iteration 24/1000 | Loss: 0.00001464
Iteration 25/1000 | Loss: 0.00001464
Iteration 26/1000 | Loss: 0.00001462
Iteration 27/1000 | Loss: 0.00001462
Iteration 28/1000 | Loss: 0.00001462
Iteration 29/1000 | Loss: 0.00001462
Iteration 30/1000 | Loss: 0.00001461
Iteration 31/1000 | Loss: 0.00001461
Iteration 32/1000 | Loss: 0.00001461
Iteration 33/1000 | Loss: 0.00001461
Iteration 34/1000 | Loss: 0.00001461
Iteration 35/1000 | Loss: 0.00001461
Iteration 36/1000 | Loss: 0.00001461
Iteration 37/1000 | Loss: 0.00001461
Iteration 38/1000 | Loss: 0.00001459
Iteration 39/1000 | Loss: 0.00001459
Iteration 40/1000 | Loss: 0.00001455
Iteration 41/1000 | Loss: 0.00001454
Iteration 42/1000 | Loss: 0.00001453
Iteration 43/1000 | Loss: 0.00001453
Iteration 44/1000 | Loss: 0.00001452
Iteration 45/1000 | Loss: 0.00001452
Iteration 46/1000 | Loss: 0.00001450
Iteration 47/1000 | Loss: 0.00001447
Iteration 48/1000 | Loss: 0.00001446
Iteration 49/1000 | Loss: 0.00001444
Iteration 50/1000 | Loss: 0.00001444
Iteration 51/1000 | Loss: 0.00001444
Iteration 52/1000 | Loss: 0.00001444
Iteration 53/1000 | Loss: 0.00001443
Iteration 54/1000 | Loss: 0.00001443
Iteration 55/1000 | Loss: 0.00001443
Iteration 56/1000 | Loss: 0.00001443
Iteration 57/1000 | Loss: 0.00001443
Iteration 58/1000 | Loss: 0.00001443
Iteration 59/1000 | Loss: 0.00001443
Iteration 60/1000 | Loss: 0.00001443
Iteration 61/1000 | Loss: 0.00001443
Iteration 62/1000 | Loss: 0.00001443
Iteration 63/1000 | Loss: 0.00001441
Iteration 64/1000 | Loss: 0.00001440
Iteration 65/1000 | Loss: 0.00001440
Iteration 66/1000 | Loss: 0.00001440
Iteration 67/1000 | Loss: 0.00001440
Iteration 68/1000 | Loss: 0.00001440
Iteration 69/1000 | Loss: 0.00001440
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001440
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001439
Iteration 74/1000 | Loss: 0.00001439
Iteration 75/1000 | Loss: 0.00001439
Iteration 76/1000 | Loss: 0.00001439
Iteration 77/1000 | Loss: 0.00001439
Iteration 78/1000 | Loss: 0.00001439
Iteration 79/1000 | Loss: 0.00001439
Iteration 80/1000 | Loss: 0.00001438
Iteration 81/1000 | Loss: 0.00001438
Iteration 82/1000 | Loss: 0.00001437
Iteration 83/1000 | Loss: 0.00001437
Iteration 84/1000 | Loss: 0.00001437
Iteration 85/1000 | Loss: 0.00001437
Iteration 86/1000 | Loss: 0.00001436
Iteration 87/1000 | Loss: 0.00001436
Iteration 88/1000 | Loss: 0.00001436
Iteration 89/1000 | Loss: 0.00001436
Iteration 90/1000 | Loss: 0.00001436
Iteration 91/1000 | Loss: 0.00001436
Iteration 92/1000 | Loss: 0.00001436
Iteration 93/1000 | Loss: 0.00001436
Iteration 94/1000 | Loss: 0.00001435
Iteration 95/1000 | Loss: 0.00001435
Iteration 96/1000 | Loss: 0.00001435
Iteration 97/1000 | Loss: 0.00001435
Iteration 98/1000 | Loss: 0.00001434
Iteration 99/1000 | Loss: 0.00001434
Iteration 100/1000 | Loss: 0.00001433
Iteration 101/1000 | Loss: 0.00001433
Iteration 102/1000 | Loss: 0.00001433
Iteration 103/1000 | Loss: 0.00001433
Iteration 104/1000 | Loss: 0.00001433
Iteration 105/1000 | Loss: 0.00001433
Iteration 106/1000 | Loss: 0.00001432
Iteration 107/1000 | Loss: 0.00001432
Iteration 108/1000 | Loss: 0.00001432
Iteration 109/1000 | Loss: 0.00001432
Iteration 110/1000 | Loss: 0.00001432
Iteration 111/1000 | Loss: 0.00001432
Iteration 112/1000 | Loss: 0.00001432
Iteration 113/1000 | Loss: 0.00001432
Iteration 114/1000 | Loss: 0.00001432
Iteration 115/1000 | Loss: 0.00001431
Iteration 116/1000 | Loss: 0.00001431
Iteration 117/1000 | Loss: 0.00001431
Iteration 118/1000 | Loss: 0.00001431
Iteration 119/1000 | Loss: 0.00001430
Iteration 120/1000 | Loss: 0.00001430
Iteration 121/1000 | Loss: 0.00001430
Iteration 122/1000 | Loss: 0.00001429
Iteration 123/1000 | Loss: 0.00001429
Iteration 124/1000 | Loss: 0.00001429
Iteration 125/1000 | Loss: 0.00001429
Iteration 126/1000 | Loss: 0.00001428
Iteration 127/1000 | Loss: 0.00001428
Iteration 128/1000 | Loss: 0.00001428
Iteration 129/1000 | Loss: 0.00001427
Iteration 130/1000 | Loss: 0.00001427
Iteration 131/1000 | Loss: 0.00001427
Iteration 132/1000 | Loss: 0.00001427
Iteration 133/1000 | Loss: 0.00001427
Iteration 134/1000 | Loss: 0.00001427
Iteration 135/1000 | Loss: 0.00001426
Iteration 136/1000 | Loss: 0.00001426
Iteration 137/1000 | Loss: 0.00001426
Iteration 138/1000 | Loss: 0.00001426
Iteration 139/1000 | Loss: 0.00001426
Iteration 140/1000 | Loss: 0.00001426
Iteration 141/1000 | Loss: 0.00001426
Iteration 142/1000 | Loss: 0.00001426
Iteration 143/1000 | Loss: 0.00001426
Iteration 144/1000 | Loss: 0.00001426
Iteration 145/1000 | Loss: 0.00001426
Iteration 146/1000 | Loss: 0.00001426
Iteration 147/1000 | Loss: 0.00001426
Iteration 148/1000 | Loss: 0.00001425
Iteration 149/1000 | Loss: 0.00001425
Iteration 150/1000 | Loss: 0.00001425
Iteration 151/1000 | Loss: 0.00001425
Iteration 152/1000 | Loss: 0.00001425
Iteration 153/1000 | Loss: 0.00001425
Iteration 154/1000 | Loss: 0.00001425
Iteration 155/1000 | Loss: 0.00001425
Iteration 156/1000 | Loss: 0.00001425
Iteration 157/1000 | Loss: 0.00001425
Iteration 158/1000 | Loss: 0.00001425
Iteration 159/1000 | Loss: 0.00001425
Iteration 160/1000 | Loss: 0.00001425
Iteration 161/1000 | Loss: 0.00001425
Iteration 162/1000 | Loss: 0.00001425
Iteration 163/1000 | Loss: 0.00001425
Iteration 164/1000 | Loss: 0.00001425
Iteration 165/1000 | Loss: 0.00001425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.4252375876822043e-05, 1.4252375876822043e-05, 1.4252375876822043e-05, 1.4252375876822043e-05, 1.4252375876822043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4252375876822043e-05

Optimization complete. Final v2v error: 3.190556049346924 mm

Highest mean error: 3.782548427581787 mm for frame 86

Lowest mean error: 2.8539509773254395 mm for frame 4

Saving results

Total time: 38.846338510513306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405816
Iteration 2/25 | Loss: 0.00128528
Iteration 3/25 | Loss: 0.00120926
Iteration 4/25 | Loss: 0.00119564
Iteration 5/25 | Loss: 0.00119130
Iteration 6/25 | Loss: 0.00119093
Iteration 7/25 | Loss: 0.00119093
Iteration 8/25 | Loss: 0.00119093
Iteration 9/25 | Loss: 0.00119093
Iteration 10/25 | Loss: 0.00119093
Iteration 11/25 | Loss: 0.00119093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001190932816825807, 0.001190932816825807, 0.001190932816825807, 0.001190932816825807, 0.001190932816825807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001190932816825807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75605226
Iteration 2/25 | Loss: 0.00092580
Iteration 3/25 | Loss: 0.00092580
Iteration 4/25 | Loss: 0.00092580
Iteration 5/25 | Loss: 0.00092580
Iteration 6/25 | Loss: 0.00092580
Iteration 7/25 | Loss: 0.00092580
Iteration 8/25 | Loss: 0.00092580
Iteration 9/25 | Loss: 0.00092580
Iteration 10/25 | Loss: 0.00092580
Iteration 11/25 | Loss: 0.00092580
Iteration 12/25 | Loss: 0.00092580
Iteration 13/25 | Loss: 0.00092580
Iteration 14/25 | Loss: 0.00092580
Iteration 15/25 | Loss: 0.00092580
Iteration 16/25 | Loss: 0.00092580
Iteration 17/25 | Loss: 0.00092580
Iteration 18/25 | Loss: 0.00092580
Iteration 19/25 | Loss: 0.00092580
Iteration 20/25 | Loss: 0.00092580
Iteration 21/25 | Loss: 0.00092580
Iteration 22/25 | Loss: 0.00092580
Iteration 23/25 | Loss: 0.00092580
Iteration 24/25 | Loss: 0.00092580
Iteration 25/25 | Loss: 0.00092580

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092580
Iteration 2/1000 | Loss: 0.00002060
Iteration 3/1000 | Loss: 0.00001602
Iteration 4/1000 | Loss: 0.00001504
Iteration 5/1000 | Loss: 0.00001440
Iteration 6/1000 | Loss: 0.00001389
Iteration 7/1000 | Loss: 0.00001355
Iteration 8/1000 | Loss: 0.00001330
Iteration 9/1000 | Loss: 0.00001300
Iteration 10/1000 | Loss: 0.00001300
Iteration 11/1000 | Loss: 0.00001291
Iteration 12/1000 | Loss: 0.00001290
Iteration 13/1000 | Loss: 0.00001290
Iteration 14/1000 | Loss: 0.00001289
Iteration 15/1000 | Loss: 0.00001289
Iteration 16/1000 | Loss: 0.00001288
Iteration 17/1000 | Loss: 0.00001286
Iteration 18/1000 | Loss: 0.00001286
Iteration 19/1000 | Loss: 0.00001280
Iteration 20/1000 | Loss: 0.00001272
Iteration 21/1000 | Loss: 0.00001264
Iteration 22/1000 | Loss: 0.00001260
Iteration 23/1000 | Loss: 0.00001259
Iteration 24/1000 | Loss: 0.00001254
Iteration 25/1000 | Loss: 0.00001248
Iteration 26/1000 | Loss: 0.00001246
Iteration 27/1000 | Loss: 0.00001246
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001245
Iteration 31/1000 | Loss: 0.00001244
Iteration 32/1000 | Loss: 0.00001244
Iteration 33/1000 | Loss: 0.00001244
Iteration 34/1000 | Loss: 0.00001243
Iteration 35/1000 | Loss: 0.00001236
Iteration 36/1000 | Loss: 0.00001230
Iteration 37/1000 | Loss: 0.00001230
Iteration 38/1000 | Loss: 0.00001229
Iteration 39/1000 | Loss: 0.00001229
Iteration 40/1000 | Loss: 0.00001228
Iteration 41/1000 | Loss: 0.00001227
Iteration 42/1000 | Loss: 0.00001227
Iteration 43/1000 | Loss: 0.00001226
Iteration 44/1000 | Loss: 0.00001225
Iteration 45/1000 | Loss: 0.00001225
Iteration 46/1000 | Loss: 0.00001225
Iteration 47/1000 | Loss: 0.00001224
Iteration 48/1000 | Loss: 0.00001224
Iteration 49/1000 | Loss: 0.00001224
Iteration 50/1000 | Loss: 0.00001223
Iteration 51/1000 | Loss: 0.00001223
Iteration 52/1000 | Loss: 0.00001223
Iteration 53/1000 | Loss: 0.00001223
Iteration 54/1000 | Loss: 0.00001223
Iteration 55/1000 | Loss: 0.00001223
Iteration 56/1000 | Loss: 0.00001223
Iteration 57/1000 | Loss: 0.00001223
Iteration 58/1000 | Loss: 0.00001223
Iteration 59/1000 | Loss: 0.00001222
Iteration 60/1000 | Loss: 0.00001222
Iteration 61/1000 | Loss: 0.00001222
Iteration 62/1000 | Loss: 0.00001221
Iteration 63/1000 | Loss: 0.00001221
Iteration 64/1000 | Loss: 0.00001220
Iteration 65/1000 | Loss: 0.00001220
Iteration 66/1000 | Loss: 0.00001220
Iteration 67/1000 | Loss: 0.00001219
Iteration 68/1000 | Loss: 0.00001219
Iteration 69/1000 | Loss: 0.00001219
Iteration 70/1000 | Loss: 0.00001219
Iteration 71/1000 | Loss: 0.00001219
Iteration 72/1000 | Loss: 0.00001218
Iteration 73/1000 | Loss: 0.00001218
Iteration 74/1000 | Loss: 0.00001218
Iteration 75/1000 | Loss: 0.00001218
Iteration 76/1000 | Loss: 0.00001217
Iteration 77/1000 | Loss: 0.00001217
Iteration 78/1000 | Loss: 0.00001216
Iteration 79/1000 | Loss: 0.00001215
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001213
Iteration 85/1000 | Loss: 0.00001213
Iteration 86/1000 | Loss: 0.00001212
Iteration 87/1000 | Loss: 0.00001212
Iteration 88/1000 | Loss: 0.00001212
Iteration 89/1000 | Loss: 0.00001211
Iteration 90/1000 | Loss: 0.00001211
Iteration 91/1000 | Loss: 0.00001210
Iteration 92/1000 | Loss: 0.00001210
Iteration 93/1000 | Loss: 0.00001210
Iteration 94/1000 | Loss: 0.00001210
Iteration 95/1000 | Loss: 0.00001210
Iteration 96/1000 | Loss: 0.00001210
Iteration 97/1000 | Loss: 0.00001210
Iteration 98/1000 | Loss: 0.00001209
Iteration 99/1000 | Loss: 0.00001209
Iteration 100/1000 | Loss: 0.00001209
Iteration 101/1000 | Loss: 0.00001209
Iteration 102/1000 | Loss: 0.00001209
Iteration 103/1000 | Loss: 0.00001208
Iteration 104/1000 | Loss: 0.00001208
Iteration 105/1000 | Loss: 0.00001208
Iteration 106/1000 | Loss: 0.00001208
Iteration 107/1000 | Loss: 0.00001208
Iteration 108/1000 | Loss: 0.00001207
Iteration 109/1000 | Loss: 0.00001207
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001207
Iteration 112/1000 | Loss: 0.00001207
Iteration 113/1000 | Loss: 0.00001207
Iteration 114/1000 | Loss: 0.00001207
Iteration 115/1000 | Loss: 0.00001207
Iteration 116/1000 | Loss: 0.00001207
Iteration 117/1000 | Loss: 0.00001206
Iteration 118/1000 | Loss: 0.00001206
Iteration 119/1000 | Loss: 0.00001206
Iteration 120/1000 | Loss: 0.00001205
Iteration 121/1000 | Loss: 0.00001205
Iteration 122/1000 | Loss: 0.00001205
Iteration 123/1000 | Loss: 0.00001205
Iteration 124/1000 | Loss: 0.00001205
Iteration 125/1000 | Loss: 0.00001205
Iteration 126/1000 | Loss: 0.00001204
Iteration 127/1000 | Loss: 0.00001204
Iteration 128/1000 | Loss: 0.00001204
Iteration 129/1000 | Loss: 0.00001204
Iteration 130/1000 | Loss: 0.00001204
Iteration 131/1000 | Loss: 0.00001204
Iteration 132/1000 | Loss: 0.00001203
Iteration 133/1000 | Loss: 0.00001203
Iteration 134/1000 | Loss: 0.00001203
Iteration 135/1000 | Loss: 0.00001203
Iteration 136/1000 | Loss: 0.00001203
Iteration 137/1000 | Loss: 0.00001203
Iteration 138/1000 | Loss: 0.00001203
Iteration 139/1000 | Loss: 0.00001203
Iteration 140/1000 | Loss: 0.00001203
Iteration 141/1000 | Loss: 0.00001203
Iteration 142/1000 | Loss: 0.00001202
Iteration 143/1000 | Loss: 0.00001202
Iteration 144/1000 | Loss: 0.00001202
Iteration 145/1000 | Loss: 0.00001202
Iteration 146/1000 | Loss: 0.00001201
Iteration 147/1000 | Loss: 0.00001201
Iteration 148/1000 | Loss: 0.00001201
Iteration 149/1000 | Loss: 0.00001201
Iteration 150/1000 | Loss: 0.00001201
Iteration 151/1000 | Loss: 0.00001201
Iteration 152/1000 | Loss: 0.00001201
Iteration 153/1000 | Loss: 0.00001200
Iteration 154/1000 | Loss: 0.00001200
Iteration 155/1000 | Loss: 0.00001200
Iteration 156/1000 | Loss: 0.00001200
Iteration 157/1000 | Loss: 0.00001200
Iteration 158/1000 | Loss: 0.00001200
Iteration 159/1000 | Loss: 0.00001200
Iteration 160/1000 | Loss: 0.00001200
Iteration 161/1000 | Loss: 0.00001200
Iteration 162/1000 | Loss: 0.00001200
Iteration 163/1000 | Loss: 0.00001200
Iteration 164/1000 | Loss: 0.00001200
Iteration 165/1000 | Loss: 0.00001200
Iteration 166/1000 | Loss: 0.00001200
Iteration 167/1000 | Loss: 0.00001200
Iteration 168/1000 | Loss: 0.00001200
Iteration 169/1000 | Loss: 0.00001200
Iteration 170/1000 | Loss: 0.00001199
Iteration 171/1000 | Loss: 0.00001199
Iteration 172/1000 | Loss: 0.00001199
Iteration 173/1000 | Loss: 0.00001199
Iteration 174/1000 | Loss: 0.00001199
Iteration 175/1000 | Loss: 0.00001199
Iteration 176/1000 | Loss: 0.00001199
Iteration 177/1000 | Loss: 0.00001199
Iteration 178/1000 | Loss: 0.00001199
Iteration 179/1000 | Loss: 0.00001199
Iteration 180/1000 | Loss: 0.00001199
Iteration 181/1000 | Loss: 0.00001199
Iteration 182/1000 | Loss: 0.00001199
Iteration 183/1000 | Loss: 0.00001199
Iteration 184/1000 | Loss: 0.00001199
Iteration 185/1000 | Loss: 0.00001199
Iteration 186/1000 | Loss: 0.00001199
Iteration 187/1000 | Loss: 0.00001199
Iteration 188/1000 | Loss: 0.00001199
Iteration 189/1000 | Loss: 0.00001199
Iteration 190/1000 | Loss: 0.00001199
Iteration 191/1000 | Loss: 0.00001199
Iteration 192/1000 | Loss: 0.00001199
Iteration 193/1000 | Loss: 0.00001199
Iteration 194/1000 | Loss: 0.00001199
Iteration 195/1000 | Loss: 0.00001199
Iteration 196/1000 | Loss: 0.00001199
Iteration 197/1000 | Loss: 0.00001199
Iteration 198/1000 | Loss: 0.00001199
Iteration 199/1000 | Loss: 0.00001199
Iteration 200/1000 | Loss: 0.00001199
Iteration 201/1000 | Loss: 0.00001199
Iteration 202/1000 | Loss: 0.00001199
Iteration 203/1000 | Loss: 0.00001199
Iteration 204/1000 | Loss: 0.00001199
Iteration 205/1000 | Loss: 0.00001199
Iteration 206/1000 | Loss: 0.00001199
Iteration 207/1000 | Loss: 0.00001199
Iteration 208/1000 | Loss: 0.00001199
Iteration 209/1000 | Loss: 0.00001199
Iteration 210/1000 | Loss: 0.00001199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.1988750884484034e-05, 1.1988750884484034e-05, 1.1988750884484034e-05, 1.1988750884484034e-05, 1.1988750884484034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1988750884484034e-05

Optimization complete. Final v2v error: 2.9746479988098145 mm

Highest mean error: 3.257964849472046 mm for frame 130

Lowest mean error: 2.8427443504333496 mm for frame 114

Saving results

Total time: 42.3764545917511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_006/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_006/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533305
Iteration 2/25 | Loss: 0.00150891
Iteration 3/25 | Loss: 0.00133766
Iteration 4/25 | Loss: 0.00132023
Iteration 5/25 | Loss: 0.00131449
Iteration 6/25 | Loss: 0.00131361
Iteration 7/25 | Loss: 0.00131361
Iteration 8/25 | Loss: 0.00131361
Iteration 9/25 | Loss: 0.00131361
Iteration 10/25 | Loss: 0.00131361
Iteration 11/25 | Loss: 0.00131361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013136073248460889, 0.0013136073248460889, 0.0013136073248460889, 0.0013136073248460889, 0.0013136073248460889]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013136073248460889

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62906152
Iteration 2/25 | Loss: 0.00109155
Iteration 3/25 | Loss: 0.00109155
Iteration 4/25 | Loss: 0.00109154
Iteration 5/25 | Loss: 0.00109154
Iteration 6/25 | Loss: 0.00109154
Iteration 7/25 | Loss: 0.00109154
Iteration 8/25 | Loss: 0.00109154
Iteration 9/25 | Loss: 0.00109154
Iteration 10/25 | Loss: 0.00109154
Iteration 11/25 | Loss: 0.00109154
Iteration 12/25 | Loss: 0.00109154
Iteration 13/25 | Loss: 0.00109154
Iteration 14/25 | Loss: 0.00109154
Iteration 15/25 | Loss: 0.00109154
Iteration 16/25 | Loss: 0.00109154
Iteration 17/25 | Loss: 0.00109154
Iteration 18/25 | Loss: 0.00109154
Iteration 19/25 | Loss: 0.00109154
Iteration 20/25 | Loss: 0.00109154
Iteration 21/25 | Loss: 0.00109154
Iteration 22/25 | Loss: 0.00109154
Iteration 23/25 | Loss: 0.00109154
Iteration 24/25 | Loss: 0.00109154
Iteration 25/25 | Loss: 0.00109154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109154
Iteration 2/1000 | Loss: 0.00004304
Iteration 3/1000 | Loss: 0.00003096
Iteration 4/1000 | Loss: 0.00002817
Iteration 5/1000 | Loss: 0.00002699
Iteration 6/1000 | Loss: 0.00002596
Iteration 7/1000 | Loss: 0.00002515
Iteration 8/1000 | Loss: 0.00002466
Iteration 9/1000 | Loss: 0.00002417
Iteration 10/1000 | Loss: 0.00002369
Iteration 11/1000 | Loss: 0.00002336
Iteration 12/1000 | Loss: 0.00002298
Iteration 13/1000 | Loss: 0.00002266
Iteration 14/1000 | Loss: 0.00002241
Iteration 15/1000 | Loss: 0.00002213
Iteration 16/1000 | Loss: 0.00002190
Iteration 17/1000 | Loss: 0.00002169
Iteration 18/1000 | Loss: 0.00002154
Iteration 19/1000 | Loss: 0.00002147
Iteration 20/1000 | Loss: 0.00002143
Iteration 21/1000 | Loss: 0.00002139
Iteration 22/1000 | Loss: 0.00002135
Iteration 23/1000 | Loss: 0.00002130
Iteration 24/1000 | Loss: 0.00002130
Iteration 25/1000 | Loss: 0.00002127
Iteration 26/1000 | Loss: 0.00002126
Iteration 27/1000 | Loss: 0.00002126
Iteration 28/1000 | Loss: 0.00002126
Iteration 29/1000 | Loss: 0.00002125
Iteration 30/1000 | Loss: 0.00002124
Iteration 31/1000 | Loss: 0.00002121
Iteration 32/1000 | Loss: 0.00002120
Iteration 33/1000 | Loss: 0.00002120
Iteration 34/1000 | Loss: 0.00002118
Iteration 35/1000 | Loss: 0.00002118
Iteration 36/1000 | Loss: 0.00002117
Iteration 37/1000 | Loss: 0.00002117
Iteration 38/1000 | Loss: 0.00002117
Iteration 39/1000 | Loss: 0.00002117
Iteration 40/1000 | Loss: 0.00002116
Iteration 41/1000 | Loss: 0.00002116
Iteration 42/1000 | Loss: 0.00002116
Iteration 43/1000 | Loss: 0.00002113
Iteration 44/1000 | Loss: 0.00002113
Iteration 45/1000 | Loss: 0.00002113
Iteration 46/1000 | Loss: 0.00002113
Iteration 47/1000 | Loss: 0.00002113
Iteration 48/1000 | Loss: 0.00002112
Iteration 49/1000 | Loss: 0.00002112
Iteration 50/1000 | Loss: 0.00002112
Iteration 51/1000 | Loss: 0.00002109
Iteration 52/1000 | Loss: 0.00002108
Iteration 53/1000 | Loss: 0.00002108
Iteration 54/1000 | Loss: 0.00002108
Iteration 55/1000 | Loss: 0.00002107
Iteration 56/1000 | Loss: 0.00002106
Iteration 57/1000 | Loss: 0.00002106
Iteration 58/1000 | Loss: 0.00002105
Iteration 59/1000 | Loss: 0.00002105
Iteration 60/1000 | Loss: 0.00002105
Iteration 61/1000 | Loss: 0.00002105
Iteration 62/1000 | Loss: 0.00002105
Iteration 63/1000 | Loss: 0.00002104
Iteration 64/1000 | Loss: 0.00002104
Iteration 65/1000 | Loss: 0.00002104
Iteration 66/1000 | Loss: 0.00002104
Iteration 67/1000 | Loss: 0.00002104
Iteration 68/1000 | Loss: 0.00002104
Iteration 69/1000 | Loss: 0.00002104
Iteration 70/1000 | Loss: 0.00002103
Iteration 71/1000 | Loss: 0.00002103
Iteration 72/1000 | Loss: 0.00002103
Iteration 73/1000 | Loss: 0.00002103
Iteration 74/1000 | Loss: 0.00002103
Iteration 75/1000 | Loss: 0.00002103
Iteration 76/1000 | Loss: 0.00002102
Iteration 77/1000 | Loss: 0.00002102
Iteration 78/1000 | Loss: 0.00002102
Iteration 79/1000 | Loss: 0.00002101
Iteration 80/1000 | Loss: 0.00002101
Iteration 81/1000 | Loss: 0.00002101
Iteration 82/1000 | Loss: 0.00002101
Iteration 83/1000 | Loss: 0.00002101
Iteration 84/1000 | Loss: 0.00002101
Iteration 85/1000 | Loss: 0.00002101
Iteration 86/1000 | Loss: 0.00002101
Iteration 87/1000 | Loss: 0.00002101
Iteration 88/1000 | Loss: 0.00002101
Iteration 89/1000 | Loss: 0.00002100
Iteration 90/1000 | Loss: 0.00002099
Iteration 91/1000 | Loss: 0.00002099
Iteration 92/1000 | Loss: 0.00002099
Iteration 93/1000 | Loss: 0.00002098
Iteration 94/1000 | Loss: 0.00002098
Iteration 95/1000 | Loss: 0.00002097
Iteration 96/1000 | Loss: 0.00002097
Iteration 97/1000 | Loss: 0.00002097
Iteration 98/1000 | Loss: 0.00002096
Iteration 99/1000 | Loss: 0.00002096
Iteration 100/1000 | Loss: 0.00002096
Iteration 101/1000 | Loss: 0.00002095
Iteration 102/1000 | Loss: 0.00002095
Iteration 103/1000 | Loss: 0.00002094
Iteration 104/1000 | Loss: 0.00002094
Iteration 105/1000 | Loss: 0.00002093
Iteration 106/1000 | Loss: 0.00002093
Iteration 107/1000 | Loss: 0.00002093
Iteration 108/1000 | Loss: 0.00002092
Iteration 109/1000 | Loss: 0.00002092
Iteration 110/1000 | Loss: 0.00002092
Iteration 111/1000 | Loss: 0.00002092
Iteration 112/1000 | Loss: 0.00002092
Iteration 113/1000 | Loss: 0.00002092
Iteration 114/1000 | Loss: 0.00002091
Iteration 115/1000 | Loss: 0.00002091
Iteration 116/1000 | Loss: 0.00002091
Iteration 117/1000 | Loss: 0.00002090
Iteration 118/1000 | Loss: 0.00002090
Iteration 119/1000 | Loss: 0.00002090
Iteration 120/1000 | Loss: 0.00002090
Iteration 121/1000 | Loss: 0.00002090
Iteration 122/1000 | Loss: 0.00002089
Iteration 123/1000 | Loss: 0.00002089
Iteration 124/1000 | Loss: 0.00002089
Iteration 125/1000 | Loss: 0.00002089
Iteration 126/1000 | Loss: 0.00002089
Iteration 127/1000 | Loss: 0.00002088
Iteration 128/1000 | Loss: 0.00002088
Iteration 129/1000 | Loss: 0.00002088
Iteration 130/1000 | Loss: 0.00002088
Iteration 131/1000 | Loss: 0.00002088
Iteration 132/1000 | Loss: 0.00002088
Iteration 133/1000 | Loss: 0.00002088
Iteration 134/1000 | Loss: 0.00002088
Iteration 135/1000 | Loss: 0.00002088
Iteration 136/1000 | Loss: 0.00002087
Iteration 137/1000 | Loss: 0.00002087
Iteration 138/1000 | Loss: 0.00002087
Iteration 139/1000 | Loss: 0.00002087
Iteration 140/1000 | Loss: 0.00002087
Iteration 141/1000 | Loss: 0.00002087
Iteration 142/1000 | Loss: 0.00002087
Iteration 143/1000 | Loss: 0.00002087
Iteration 144/1000 | Loss: 0.00002087
Iteration 145/1000 | Loss: 0.00002087
Iteration 146/1000 | Loss: 0.00002087
Iteration 147/1000 | Loss: 0.00002087
Iteration 148/1000 | Loss: 0.00002086
Iteration 149/1000 | Loss: 0.00002086
Iteration 150/1000 | Loss: 0.00002086
Iteration 151/1000 | Loss: 0.00002086
Iteration 152/1000 | Loss: 0.00002086
Iteration 153/1000 | Loss: 0.00002086
Iteration 154/1000 | Loss: 0.00002086
Iteration 155/1000 | Loss: 0.00002085
Iteration 156/1000 | Loss: 0.00002085
Iteration 157/1000 | Loss: 0.00002085
Iteration 158/1000 | Loss: 0.00002085
Iteration 159/1000 | Loss: 0.00002085
Iteration 160/1000 | Loss: 0.00002085
Iteration 161/1000 | Loss: 0.00002085
Iteration 162/1000 | Loss: 0.00002085
Iteration 163/1000 | Loss: 0.00002085
Iteration 164/1000 | Loss: 0.00002085
Iteration 165/1000 | Loss: 0.00002085
Iteration 166/1000 | Loss: 0.00002085
Iteration 167/1000 | Loss: 0.00002085
Iteration 168/1000 | Loss: 0.00002084
Iteration 169/1000 | Loss: 0.00002084
Iteration 170/1000 | Loss: 0.00002084
Iteration 171/1000 | Loss: 0.00002084
Iteration 172/1000 | Loss: 0.00002084
Iteration 173/1000 | Loss: 0.00002084
Iteration 174/1000 | Loss: 0.00002084
Iteration 175/1000 | Loss: 0.00002083
Iteration 176/1000 | Loss: 0.00002083
Iteration 177/1000 | Loss: 0.00002083
Iteration 178/1000 | Loss: 0.00002083
Iteration 179/1000 | Loss: 0.00002083
Iteration 180/1000 | Loss: 0.00002083
Iteration 181/1000 | Loss: 0.00002083
Iteration 182/1000 | Loss: 0.00002083
Iteration 183/1000 | Loss: 0.00002083
Iteration 184/1000 | Loss: 0.00002083
Iteration 185/1000 | Loss: 0.00002083
Iteration 186/1000 | Loss: 0.00002083
Iteration 187/1000 | Loss: 0.00002083
Iteration 188/1000 | Loss: 0.00002083
Iteration 189/1000 | Loss: 0.00002082
Iteration 190/1000 | Loss: 0.00002082
Iteration 191/1000 | Loss: 0.00002082
Iteration 192/1000 | Loss: 0.00002082
Iteration 193/1000 | Loss: 0.00002082
Iteration 194/1000 | Loss: 0.00002082
Iteration 195/1000 | Loss: 0.00002082
Iteration 196/1000 | Loss: 0.00002082
Iteration 197/1000 | Loss: 0.00002082
Iteration 198/1000 | Loss: 0.00002081
Iteration 199/1000 | Loss: 0.00002081
Iteration 200/1000 | Loss: 0.00002081
Iteration 201/1000 | Loss: 0.00002081
Iteration 202/1000 | Loss: 0.00002080
Iteration 203/1000 | Loss: 0.00002080
Iteration 204/1000 | Loss: 0.00002080
Iteration 205/1000 | Loss: 0.00002080
Iteration 206/1000 | Loss: 0.00002080
Iteration 207/1000 | Loss: 0.00002080
Iteration 208/1000 | Loss: 0.00002080
Iteration 209/1000 | Loss: 0.00002080
Iteration 210/1000 | Loss: 0.00002079
Iteration 211/1000 | Loss: 0.00002079
Iteration 212/1000 | Loss: 0.00002079
Iteration 213/1000 | Loss: 0.00002079
Iteration 214/1000 | Loss: 0.00002079
Iteration 215/1000 | Loss: 0.00002079
Iteration 216/1000 | Loss: 0.00002078
Iteration 217/1000 | Loss: 0.00002078
Iteration 218/1000 | Loss: 0.00002078
Iteration 219/1000 | Loss: 0.00002078
Iteration 220/1000 | Loss: 0.00002078
Iteration 221/1000 | Loss: 0.00002078
Iteration 222/1000 | Loss: 0.00002078
Iteration 223/1000 | Loss: 0.00002078
Iteration 224/1000 | Loss: 0.00002078
Iteration 225/1000 | Loss: 0.00002077
Iteration 226/1000 | Loss: 0.00002077
Iteration 227/1000 | Loss: 0.00002077
Iteration 228/1000 | Loss: 0.00002077
Iteration 229/1000 | Loss: 0.00002077
Iteration 230/1000 | Loss: 0.00002076
Iteration 231/1000 | Loss: 0.00002076
Iteration 232/1000 | Loss: 0.00002076
Iteration 233/1000 | Loss: 0.00002076
Iteration 234/1000 | Loss: 0.00002076
Iteration 235/1000 | Loss: 0.00002076
Iteration 236/1000 | Loss: 0.00002076
Iteration 237/1000 | Loss: 0.00002076
Iteration 238/1000 | Loss: 0.00002075
Iteration 239/1000 | Loss: 0.00002075
Iteration 240/1000 | Loss: 0.00002075
Iteration 241/1000 | Loss: 0.00002075
Iteration 242/1000 | Loss: 0.00002075
Iteration 243/1000 | Loss: 0.00002075
Iteration 244/1000 | Loss: 0.00002075
Iteration 245/1000 | Loss: 0.00002075
Iteration 246/1000 | Loss: 0.00002075
Iteration 247/1000 | Loss: 0.00002075
Iteration 248/1000 | Loss: 0.00002075
Iteration 249/1000 | Loss: 0.00002075
Iteration 250/1000 | Loss: 0.00002074
Iteration 251/1000 | Loss: 0.00002074
Iteration 252/1000 | Loss: 0.00002074
Iteration 253/1000 | Loss: 0.00002074
Iteration 254/1000 | Loss: 0.00002074
Iteration 255/1000 | Loss: 0.00002074
Iteration 256/1000 | Loss: 0.00002074
Iteration 257/1000 | Loss: 0.00002074
Iteration 258/1000 | Loss: 0.00002074
Iteration 259/1000 | Loss: 0.00002074
Iteration 260/1000 | Loss: 0.00002074
Iteration 261/1000 | Loss: 0.00002074
Iteration 262/1000 | Loss: 0.00002073
Iteration 263/1000 | Loss: 0.00002073
Iteration 264/1000 | Loss: 0.00002073
Iteration 265/1000 | Loss: 0.00002073
Iteration 266/1000 | Loss: 0.00002073
Iteration 267/1000 | Loss: 0.00002072
Iteration 268/1000 | Loss: 0.00002072
Iteration 269/1000 | Loss: 0.00002072
Iteration 270/1000 | Loss: 0.00002072
Iteration 271/1000 | Loss: 0.00002072
Iteration 272/1000 | Loss: 0.00002072
Iteration 273/1000 | Loss: 0.00002072
Iteration 274/1000 | Loss: 0.00002072
Iteration 275/1000 | Loss: 0.00002072
Iteration 276/1000 | Loss: 0.00002072
Iteration 277/1000 | Loss: 0.00002072
Iteration 278/1000 | Loss: 0.00002072
Iteration 279/1000 | Loss: 0.00002072
Iteration 280/1000 | Loss: 0.00002071
Iteration 281/1000 | Loss: 0.00002071
Iteration 282/1000 | Loss: 0.00002071
Iteration 283/1000 | Loss: 0.00002071
Iteration 284/1000 | Loss: 0.00002071
Iteration 285/1000 | Loss: 0.00002071
Iteration 286/1000 | Loss: 0.00002071
Iteration 287/1000 | Loss: 0.00002071
Iteration 288/1000 | Loss: 0.00002071
Iteration 289/1000 | Loss: 0.00002071
Iteration 290/1000 | Loss: 0.00002071
Iteration 291/1000 | Loss: 0.00002071
Iteration 292/1000 | Loss: 0.00002071
Iteration 293/1000 | Loss: 0.00002070
Iteration 294/1000 | Loss: 0.00002070
Iteration 295/1000 | Loss: 0.00002070
Iteration 296/1000 | Loss: 0.00002070
Iteration 297/1000 | Loss: 0.00002070
Iteration 298/1000 | Loss: 0.00002070
Iteration 299/1000 | Loss: 0.00002070
Iteration 300/1000 | Loss: 0.00002070
Iteration 301/1000 | Loss: 0.00002070
Iteration 302/1000 | Loss: 0.00002070
Iteration 303/1000 | Loss: 0.00002070
Iteration 304/1000 | Loss: 0.00002070
Iteration 305/1000 | Loss: 0.00002070
Iteration 306/1000 | Loss: 0.00002070
Iteration 307/1000 | Loss: 0.00002070
Iteration 308/1000 | Loss: 0.00002070
Iteration 309/1000 | Loss: 0.00002070
Iteration 310/1000 | Loss: 0.00002070
Iteration 311/1000 | Loss: 0.00002070
Iteration 312/1000 | Loss: 0.00002070
Iteration 313/1000 | Loss: 0.00002070
Iteration 314/1000 | Loss: 0.00002070
Iteration 315/1000 | Loss: 0.00002070
Iteration 316/1000 | Loss: 0.00002070
Iteration 317/1000 | Loss: 0.00002070
Iteration 318/1000 | Loss: 0.00002069
Iteration 319/1000 | Loss: 0.00002069
Iteration 320/1000 | Loss: 0.00002069
Iteration 321/1000 | Loss: 0.00002069
Iteration 322/1000 | Loss: 0.00002069
Iteration 323/1000 | Loss: 0.00002069
Iteration 324/1000 | Loss: 0.00002069
Iteration 325/1000 | Loss: 0.00002069
Iteration 326/1000 | Loss: 0.00002069
Iteration 327/1000 | Loss: 0.00002069
Iteration 328/1000 | Loss: 0.00002069
Iteration 329/1000 | Loss: 0.00002069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 329. Stopping optimization.
Last 5 losses: [2.0694491468020715e-05, 2.0694491468020715e-05, 2.0694491468020715e-05, 2.0694491468020715e-05, 2.0694491468020715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0694491468020715e-05

Optimization complete. Final v2v error: 3.766541004180908 mm

Highest mean error: 4.1354079246521 mm for frame 26

Lowest mean error: 3.6654927730560303 mm for frame 118

Saving results

Total time: 70.29136896133423
