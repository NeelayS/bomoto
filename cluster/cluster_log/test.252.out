Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=252, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14112-14167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991852
Iteration 2/25 | Loss: 0.00126358
Iteration 3/25 | Loss: 0.00114396
Iteration 4/25 | Loss: 0.00113195
Iteration 5/25 | Loss: 0.00112864
Iteration 6/25 | Loss: 0.00112783
Iteration 7/25 | Loss: 0.00112779
Iteration 8/25 | Loss: 0.00112779
Iteration 9/25 | Loss: 0.00112779
Iteration 10/25 | Loss: 0.00112779
Iteration 11/25 | Loss: 0.00112779
Iteration 12/25 | Loss: 0.00112779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011277913581579924, 0.0011277913581579924, 0.0011277913581579924, 0.0011277913581579924, 0.0011277913581579924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011277913581579924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.04584694
Iteration 2/25 | Loss: 0.00086884
Iteration 3/25 | Loss: 0.00086884
Iteration 4/25 | Loss: 0.00086884
Iteration 5/25 | Loss: 0.00086884
Iteration 6/25 | Loss: 0.00086884
Iteration 7/25 | Loss: 0.00086884
Iteration 8/25 | Loss: 0.00086884
Iteration 9/25 | Loss: 0.00086884
Iteration 10/25 | Loss: 0.00086884
Iteration 11/25 | Loss: 0.00086884
Iteration 12/25 | Loss: 0.00086884
Iteration 13/25 | Loss: 0.00086884
Iteration 14/25 | Loss: 0.00086884
Iteration 15/25 | Loss: 0.00086884
Iteration 16/25 | Loss: 0.00086884
Iteration 17/25 | Loss: 0.00086884
Iteration 18/25 | Loss: 0.00086884
Iteration 19/25 | Loss: 0.00086884
Iteration 20/25 | Loss: 0.00086884
Iteration 21/25 | Loss: 0.00086884
Iteration 22/25 | Loss: 0.00086884
Iteration 23/25 | Loss: 0.00086884
Iteration 24/25 | Loss: 0.00086884
Iteration 25/25 | Loss: 0.00086884

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086884
Iteration 2/1000 | Loss: 0.00002517
Iteration 3/1000 | Loss: 0.00001657
Iteration 4/1000 | Loss: 0.00001477
Iteration 5/1000 | Loss: 0.00001414
Iteration 6/1000 | Loss: 0.00001380
Iteration 7/1000 | Loss: 0.00001341
Iteration 8/1000 | Loss: 0.00001303
Iteration 9/1000 | Loss: 0.00001282
Iteration 10/1000 | Loss: 0.00001280
Iteration 11/1000 | Loss: 0.00001264
Iteration 12/1000 | Loss: 0.00001262
Iteration 13/1000 | Loss: 0.00001262
Iteration 14/1000 | Loss: 0.00001254
Iteration 15/1000 | Loss: 0.00001252
Iteration 16/1000 | Loss: 0.00001245
Iteration 17/1000 | Loss: 0.00001242
Iteration 18/1000 | Loss: 0.00001241
Iteration 19/1000 | Loss: 0.00001241
Iteration 20/1000 | Loss: 0.00001236
Iteration 21/1000 | Loss: 0.00001236
Iteration 22/1000 | Loss: 0.00001236
Iteration 23/1000 | Loss: 0.00001234
Iteration 24/1000 | Loss: 0.00001233
Iteration 25/1000 | Loss: 0.00001230
Iteration 26/1000 | Loss: 0.00001230
Iteration 27/1000 | Loss: 0.00001229
Iteration 28/1000 | Loss: 0.00001229
Iteration 29/1000 | Loss: 0.00001226
Iteration 30/1000 | Loss: 0.00001226
Iteration 31/1000 | Loss: 0.00001225
Iteration 32/1000 | Loss: 0.00001225
Iteration 33/1000 | Loss: 0.00001225
Iteration 34/1000 | Loss: 0.00001224
Iteration 35/1000 | Loss: 0.00001224
Iteration 36/1000 | Loss: 0.00001223
Iteration 37/1000 | Loss: 0.00001223
Iteration 38/1000 | Loss: 0.00001222
Iteration 39/1000 | Loss: 0.00001222
Iteration 40/1000 | Loss: 0.00001222
Iteration 41/1000 | Loss: 0.00001222
Iteration 42/1000 | Loss: 0.00001222
Iteration 43/1000 | Loss: 0.00001222
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001221
Iteration 46/1000 | Loss: 0.00001221
Iteration 47/1000 | Loss: 0.00001221
Iteration 48/1000 | Loss: 0.00001220
Iteration 49/1000 | Loss: 0.00001220
Iteration 50/1000 | Loss: 0.00001220
Iteration 51/1000 | Loss: 0.00001219
Iteration 52/1000 | Loss: 0.00001219
Iteration 53/1000 | Loss: 0.00001219
Iteration 54/1000 | Loss: 0.00001219
Iteration 55/1000 | Loss: 0.00001219
Iteration 56/1000 | Loss: 0.00001218
Iteration 57/1000 | Loss: 0.00001218
Iteration 58/1000 | Loss: 0.00001218
Iteration 59/1000 | Loss: 0.00001218
Iteration 60/1000 | Loss: 0.00001218
Iteration 61/1000 | Loss: 0.00001218
Iteration 62/1000 | Loss: 0.00001218
Iteration 63/1000 | Loss: 0.00001217
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00001217
Iteration 66/1000 | Loss: 0.00001217
Iteration 67/1000 | Loss: 0.00001217
Iteration 68/1000 | Loss: 0.00001217
Iteration 69/1000 | Loss: 0.00001216
Iteration 70/1000 | Loss: 0.00001216
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001216
Iteration 73/1000 | Loss: 0.00001216
Iteration 74/1000 | Loss: 0.00001215
Iteration 75/1000 | Loss: 0.00001215
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001214
Iteration 78/1000 | Loss: 0.00001214
Iteration 79/1000 | Loss: 0.00001214
Iteration 80/1000 | Loss: 0.00001214
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001213
Iteration 85/1000 | Loss: 0.00001213
Iteration 86/1000 | Loss: 0.00001213
Iteration 87/1000 | Loss: 0.00001213
Iteration 88/1000 | Loss: 0.00001212
Iteration 89/1000 | Loss: 0.00001212
Iteration 90/1000 | Loss: 0.00001212
Iteration 91/1000 | Loss: 0.00001212
Iteration 92/1000 | Loss: 0.00001212
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001211
Iteration 95/1000 | Loss: 0.00001211
Iteration 96/1000 | Loss: 0.00001211
Iteration 97/1000 | Loss: 0.00001211
Iteration 98/1000 | Loss: 0.00001210
Iteration 99/1000 | Loss: 0.00001210
Iteration 100/1000 | Loss: 0.00001210
Iteration 101/1000 | Loss: 0.00001209
Iteration 102/1000 | Loss: 0.00001209
Iteration 103/1000 | Loss: 0.00001208
Iteration 104/1000 | Loss: 0.00001208
Iteration 105/1000 | Loss: 0.00001208
Iteration 106/1000 | Loss: 0.00001207
Iteration 107/1000 | Loss: 0.00001207
Iteration 108/1000 | Loss: 0.00001207
Iteration 109/1000 | Loss: 0.00001207
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001207
Iteration 112/1000 | Loss: 0.00001207
Iteration 113/1000 | Loss: 0.00001207
Iteration 114/1000 | Loss: 0.00001206
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001206
Iteration 117/1000 | Loss: 0.00001205
Iteration 118/1000 | Loss: 0.00001205
Iteration 119/1000 | Loss: 0.00001204
Iteration 120/1000 | Loss: 0.00001204
Iteration 121/1000 | Loss: 0.00001204
Iteration 122/1000 | Loss: 0.00001203
Iteration 123/1000 | Loss: 0.00001203
Iteration 124/1000 | Loss: 0.00001203
Iteration 125/1000 | Loss: 0.00001203
Iteration 126/1000 | Loss: 0.00001203
Iteration 127/1000 | Loss: 0.00001203
Iteration 128/1000 | Loss: 0.00001203
Iteration 129/1000 | Loss: 0.00001202
Iteration 130/1000 | Loss: 0.00001201
Iteration 131/1000 | Loss: 0.00001201
Iteration 132/1000 | Loss: 0.00001201
Iteration 133/1000 | Loss: 0.00001200
Iteration 134/1000 | Loss: 0.00001200
Iteration 135/1000 | Loss: 0.00001200
Iteration 136/1000 | Loss: 0.00001200
Iteration 137/1000 | Loss: 0.00001200
Iteration 138/1000 | Loss: 0.00001199
Iteration 139/1000 | Loss: 0.00001198
Iteration 140/1000 | Loss: 0.00001198
Iteration 141/1000 | Loss: 0.00001198
Iteration 142/1000 | Loss: 0.00001198
Iteration 143/1000 | Loss: 0.00001198
Iteration 144/1000 | Loss: 0.00001197
Iteration 145/1000 | Loss: 0.00001197
Iteration 146/1000 | Loss: 0.00001197
Iteration 147/1000 | Loss: 0.00001197
Iteration 148/1000 | Loss: 0.00001197
Iteration 149/1000 | Loss: 0.00001196
Iteration 150/1000 | Loss: 0.00001196
Iteration 151/1000 | Loss: 0.00001196
Iteration 152/1000 | Loss: 0.00001196
Iteration 153/1000 | Loss: 0.00001196
Iteration 154/1000 | Loss: 0.00001196
Iteration 155/1000 | Loss: 0.00001196
Iteration 156/1000 | Loss: 0.00001195
Iteration 157/1000 | Loss: 0.00001195
Iteration 158/1000 | Loss: 0.00001195
Iteration 159/1000 | Loss: 0.00001195
Iteration 160/1000 | Loss: 0.00001195
Iteration 161/1000 | Loss: 0.00001195
Iteration 162/1000 | Loss: 0.00001195
Iteration 163/1000 | Loss: 0.00001195
Iteration 164/1000 | Loss: 0.00001194
Iteration 165/1000 | Loss: 0.00001194
Iteration 166/1000 | Loss: 0.00001194
Iteration 167/1000 | Loss: 0.00001194
Iteration 168/1000 | Loss: 0.00001194
Iteration 169/1000 | Loss: 0.00001194
Iteration 170/1000 | Loss: 0.00001194
Iteration 171/1000 | Loss: 0.00001194
Iteration 172/1000 | Loss: 0.00001194
Iteration 173/1000 | Loss: 0.00001194
Iteration 174/1000 | Loss: 0.00001193
Iteration 175/1000 | Loss: 0.00001193
Iteration 176/1000 | Loss: 0.00001193
Iteration 177/1000 | Loss: 0.00001193
Iteration 178/1000 | Loss: 0.00001193
Iteration 179/1000 | Loss: 0.00001193
Iteration 180/1000 | Loss: 0.00001193
Iteration 181/1000 | Loss: 0.00001193
Iteration 182/1000 | Loss: 0.00001193
Iteration 183/1000 | Loss: 0.00001193
Iteration 184/1000 | Loss: 0.00001193
Iteration 185/1000 | Loss: 0.00001193
Iteration 186/1000 | Loss: 0.00001192
Iteration 187/1000 | Loss: 0.00001192
Iteration 188/1000 | Loss: 0.00001192
Iteration 189/1000 | Loss: 0.00001192
Iteration 190/1000 | Loss: 0.00001192
Iteration 191/1000 | Loss: 0.00001192
Iteration 192/1000 | Loss: 0.00001192
Iteration 193/1000 | Loss: 0.00001192
Iteration 194/1000 | Loss: 0.00001191
Iteration 195/1000 | Loss: 0.00001191
Iteration 196/1000 | Loss: 0.00001191
Iteration 197/1000 | Loss: 0.00001191
Iteration 198/1000 | Loss: 0.00001191
Iteration 199/1000 | Loss: 0.00001191
Iteration 200/1000 | Loss: 0.00001191
Iteration 201/1000 | Loss: 0.00001191
Iteration 202/1000 | Loss: 0.00001191
Iteration 203/1000 | Loss: 0.00001191
Iteration 204/1000 | Loss: 0.00001191
Iteration 205/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.1911269211850595e-05, 1.1911269211850595e-05, 1.1911269211850595e-05, 1.1911269211850595e-05, 1.1911269211850595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1911269211850595e-05

Optimization complete. Final v2v error: 2.916656970977783 mm

Highest mean error: 3.176015853881836 mm for frame 92

Lowest mean error: 2.4617035388946533 mm for frame 148

Saving results

Total time: 43.215181827545166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798984
Iteration 2/25 | Loss: 0.00149763
Iteration 3/25 | Loss: 0.00131510
Iteration 4/25 | Loss: 0.00116273
Iteration 5/25 | Loss: 0.00114572
Iteration 6/25 | Loss: 0.00113482
Iteration 7/25 | Loss: 0.00112055
Iteration 8/25 | Loss: 0.00112359
Iteration 9/25 | Loss: 0.00111391
Iteration 10/25 | Loss: 0.00111160
Iteration 11/25 | Loss: 0.00111143
Iteration 12/25 | Loss: 0.00111285
Iteration 13/25 | Loss: 0.00111219
Iteration 14/25 | Loss: 0.00111131
Iteration 15/25 | Loss: 0.00111131
Iteration 16/25 | Loss: 0.00111131
Iteration 17/25 | Loss: 0.00111131
Iteration 18/25 | Loss: 0.00111131
Iteration 19/25 | Loss: 0.00111131
Iteration 20/25 | Loss: 0.00111131
Iteration 21/25 | Loss: 0.00111131
Iteration 22/25 | Loss: 0.00111131
Iteration 23/25 | Loss: 0.00111131
Iteration 24/25 | Loss: 0.00111131
Iteration 25/25 | Loss: 0.00111131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83543766
Iteration 2/25 | Loss: 0.00112265
Iteration 3/25 | Loss: 0.00086948
Iteration 4/25 | Loss: 0.00086948
Iteration 5/25 | Loss: 0.00086948
Iteration 6/25 | Loss: 0.00086948
Iteration 7/25 | Loss: 0.00086948
Iteration 8/25 | Loss: 0.00086948
Iteration 9/25 | Loss: 0.00086948
Iteration 10/25 | Loss: 0.00086948
Iteration 11/25 | Loss: 0.00086948
Iteration 12/25 | Loss: 0.00086948
Iteration 13/25 | Loss: 0.00086948
Iteration 14/25 | Loss: 0.00086948
Iteration 15/25 | Loss: 0.00086948
Iteration 16/25 | Loss: 0.00086948
Iteration 17/25 | Loss: 0.00086948
Iteration 18/25 | Loss: 0.00086948
Iteration 19/25 | Loss: 0.00086948
Iteration 20/25 | Loss: 0.00086948
Iteration 21/25 | Loss: 0.00086948
Iteration 22/25 | Loss: 0.00086948
Iteration 23/25 | Loss: 0.00086948
Iteration 24/25 | Loss: 0.00086948
Iteration 25/25 | Loss: 0.00086948

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086948
Iteration 2/1000 | Loss: 0.00003131
Iteration 3/1000 | Loss: 0.00034179
Iteration 4/1000 | Loss: 0.00017135
Iteration 5/1000 | Loss: 0.00002979
Iteration 6/1000 | Loss: 0.00002119
Iteration 7/1000 | Loss: 0.00001926
Iteration 8/1000 | Loss: 0.00010521
Iteration 9/1000 | Loss: 0.00006308
Iteration 10/1000 | Loss: 0.00008194
Iteration 11/1000 | Loss: 0.00001843
Iteration 12/1000 | Loss: 0.00004063
Iteration 13/1000 | Loss: 0.00001837
Iteration 14/1000 | Loss: 0.00003447
Iteration 15/1000 | Loss: 0.00007639
Iteration 16/1000 | Loss: 0.00023329
Iteration 17/1000 | Loss: 0.00017789
Iteration 18/1000 | Loss: 0.00004069
Iteration 19/1000 | Loss: 0.00004442
Iteration 20/1000 | Loss: 0.00008120
Iteration 21/1000 | Loss: 0.00002463
Iteration 22/1000 | Loss: 0.00002237
Iteration 23/1000 | Loss: 0.00005008
Iteration 24/1000 | Loss: 0.00001759
Iteration 25/1000 | Loss: 0.00001705
Iteration 26/1000 | Loss: 0.00001697
Iteration 27/1000 | Loss: 0.00004837
Iteration 28/1000 | Loss: 0.00002464
Iteration 29/1000 | Loss: 0.00002343
Iteration 30/1000 | Loss: 0.00001683
Iteration 31/1000 | Loss: 0.00001681
Iteration 32/1000 | Loss: 0.00001678
Iteration 33/1000 | Loss: 0.00001666
Iteration 34/1000 | Loss: 0.00001663
Iteration 35/1000 | Loss: 0.00001663
Iteration 36/1000 | Loss: 0.00001663
Iteration 37/1000 | Loss: 0.00001663
Iteration 38/1000 | Loss: 0.00001663
Iteration 39/1000 | Loss: 0.00001663
Iteration 40/1000 | Loss: 0.00001663
Iteration 41/1000 | Loss: 0.00001663
Iteration 42/1000 | Loss: 0.00001653
Iteration 43/1000 | Loss: 0.00001651
Iteration 44/1000 | Loss: 0.00001645
Iteration 45/1000 | Loss: 0.00001644
Iteration 46/1000 | Loss: 0.00008473
Iteration 47/1000 | Loss: 0.00001642
Iteration 48/1000 | Loss: 0.00001640
Iteration 49/1000 | Loss: 0.00001637
Iteration 50/1000 | Loss: 0.00001636
Iteration 51/1000 | Loss: 0.00001636
Iteration 52/1000 | Loss: 0.00001636
Iteration 53/1000 | Loss: 0.00001636
Iteration 54/1000 | Loss: 0.00001635
Iteration 55/1000 | Loss: 0.00001635
Iteration 56/1000 | Loss: 0.00001634
Iteration 57/1000 | Loss: 0.00001633
Iteration 58/1000 | Loss: 0.00001631
Iteration 59/1000 | Loss: 0.00001631
Iteration 60/1000 | Loss: 0.00001631
Iteration 61/1000 | Loss: 0.00001631
Iteration 62/1000 | Loss: 0.00001631
Iteration 63/1000 | Loss: 0.00001631
Iteration 64/1000 | Loss: 0.00001631
Iteration 65/1000 | Loss: 0.00001631
Iteration 66/1000 | Loss: 0.00001630
Iteration 67/1000 | Loss: 0.00001630
Iteration 68/1000 | Loss: 0.00001630
Iteration 69/1000 | Loss: 0.00001630
Iteration 70/1000 | Loss: 0.00001630
Iteration 71/1000 | Loss: 0.00001629
Iteration 72/1000 | Loss: 0.00001628
Iteration 73/1000 | Loss: 0.00001626
Iteration 74/1000 | Loss: 0.00001626
Iteration 75/1000 | Loss: 0.00001626
Iteration 76/1000 | Loss: 0.00001624
Iteration 77/1000 | Loss: 0.00001622
Iteration 78/1000 | Loss: 0.00001622
Iteration 79/1000 | Loss: 0.00001621
Iteration 80/1000 | Loss: 0.00001621
Iteration 81/1000 | Loss: 0.00001621
Iteration 82/1000 | Loss: 0.00001620
Iteration 83/1000 | Loss: 0.00001620
Iteration 84/1000 | Loss: 0.00001619
Iteration 85/1000 | Loss: 0.00001619
Iteration 86/1000 | Loss: 0.00010646
Iteration 87/1000 | Loss: 0.00001641
Iteration 88/1000 | Loss: 0.00001617
Iteration 89/1000 | Loss: 0.00001614
Iteration 90/1000 | Loss: 0.00001614
Iteration 91/1000 | Loss: 0.00001613
Iteration 92/1000 | Loss: 0.00001612
Iteration 93/1000 | Loss: 0.00001612
Iteration 94/1000 | Loss: 0.00001612
Iteration 95/1000 | Loss: 0.00001611
Iteration 96/1000 | Loss: 0.00001611
Iteration 97/1000 | Loss: 0.00001610
Iteration 98/1000 | Loss: 0.00001610
Iteration 99/1000 | Loss: 0.00001610
Iteration 100/1000 | Loss: 0.00001610
Iteration 101/1000 | Loss: 0.00001610
Iteration 102/1000 | Loss: 0.00001609
Iteration 103/1000 | Loss: 0.00001609
Iteration 104/1000 | Loss: 0.00001609
Iteration 105/1000 | Loss: 0.00001609
Iteration 106/1000 | Loss: 0.00001609
Iteration 107/1000 | Loss: 0.00001609
Iteration 108/1000 | Loss: 0.00001609
Iteration 109/1000 | Loss: 0.00001609
Iteration 110/1000 | Loss: 0.00001609
Iteration 111/1000 | Loss: 0.00001609
Iteration 112/1000 | Loss: 0.00001608
Iteration 113/1000 | Loss: 0.00001608
Iteration 114/1000 | Loss: 0.00001608
Iteration 115/1000 | Loss: 0.00001608
Iteration 116/1000 | Loss: 0.00001608
Iteration 117/1000 | Loss: 0.00001608
Iteration 118/1000 | Loss: 0.00001608
Iteration 119/1000 | Loss: 0.00001608
Iteration 120/1000 | Loss: 0.00001608
Iteration 121/1000 | Loss: 0.00001608
Iteration 122/1000 | Loss: 0.00001608
Iteration 123/1000 | Loss: 0.00001608
Iteration 124/1000 | Loss: 0.00001608
Iteration 125/1000 | Loss: 0.00001608
Iteration 126/1000 | Loss: 0.00001607
Iteration 127/1000 | Loss: 0.00001607
Iteration 128/1000 | Loss: 0.00001607
Iteration 129/1000 | Loss: 0.00001607
Iteration 130/1000 | Loss: 0.00001607
Iteration 131/1000 | Loss: 0.00001607
Iteration 132/1000 | Loss: 0.00001607
Iteration 133/1000 | Loss: 0.00001607
Iteration 134/1000 | Loss: 0.00001607
Iteration 135/1000 | Loss: 0.00001607
Iteration 136/1000 | Loss: 0.00001607
Iteration 137/1000 | Loss: 0.00001607
Iteration 138/1000 | Loss: 0.00001607
Iteration 139/1000 | Loss: 0.00001607
Iteration 140/1000 | Loss: 0.00001607
Iteration 141/1000 | Loss: 0.00001607
Iteration 142/1000 | Loss: 0.00001607
Iteration 143/1000 | Loss: 0.00001607
Iteration 144/1000 | Loss: 0.00001606
Iteration 145/1000 | Loss: 0.00001606
Iteration 146/1000 | Loss: 0.00001606
Iteration 147/1000 | Loss: 0.00001606
Iteration 148/1000 | Loss: 0.00001606
Iteration 149/1000 | Loss: 0.00001606
Iteration 150/1000 | Loss: 0.00001606
Iteration 151/1000 | Loss: 0.00001606
Iteration 152/1000 | Loss: 0.00001606
Iteration 153/1000 | Loss: 0.00001606
Iteration 154/1000 | Loss: 0.00001606
Iteration 155/1000 | Loss: 0.00001606
Iteration 156/1000 | Loss: 0.00001606
Iteration 157/1000 | Loss: 0.00001606
Iteration 158/1000 | Loss: 0.00001606
Iteration 159/1000 | Loss: 0.00001606
Iteration 160/1000 | Loss: 0.00001606
Iteration 161/1000 | Loss: 0.00001606
Iteration 162/1000 | Loss: 0.00001606
Iteration 163/1000 | Loss: 0.00001606
Iteration 164/1000 | Loss: 0.00001605
Iteration 165/1000 | Loss: 0.00001605
Iteration 166/1000 | Loss: 0.00001605
Iteration 167/1000 | Loss: 0.00001605
Iteration 168/1000 | Loss: 0.00001605
Iteration 169/1000 | Loss: 0.00001605
Iteration 170/1000 | Loss: 0.00001605
Iteration 171/1000 | Loss: 0.00001605
Iteration 172/1000 | Loss: 0.00001605
Iteration 173/1000 | Loss: 0.00001605
Iteration 174/1000 | Loss: 0.00001605
Iteration 175/1000 | Loss: 0.00001605
Iteration 176/1000 | Loss: 0.00001605
Iteration 177/1000 | Loss: 0.00001605
Iteration 178/1000 | Loss: 0.00001605
Iteration 179/1000 | Loss: 0.00001605
Iteration 180/1000 | Loss: 0.00001605
Iteration 181/1000 | Loss: 0.00001605
Iteration 182/1000 | Loss: 0.00001605
Iteration 183/1000 | Loss: 0.00001605
Iteration 184/1000 | Loss: 0.00001605
Iteration 185/1000 | Loss: 0.00001605
Iteration 186/1000 | Loss: 0.00001605
Iteration 187/1000 | Loss: 0.00001605
Iteration 188/1000 | Loss: 0.00001605
Iteration 189/1000 | Loss: 0.00001605
Iteration 190/1000 | Loss: 0.00001605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.6046020391513593e-05, 1.6046020391513593e-05, 1.6046020391513593e-05, 1.6046020391513593e-05, 1.6046020391513593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6046020391513593e-05

Optimization complete. Final v2v error: 3.3939247131347656 mm

Highest mean error: 3.79962158203125 mm for frame 104

Lowest mean error: 3.057882785797119 mm for frame 134

Saving results

Total time: 87.89365267753601
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795748
Iteration 2/25 | Loss: 0.00183934
Iteration 3/25 | Loss: 0.00141277
Iteration 4/25 | Loss: 0.00142569
Iteration 5/25 | Loss: 0.00138668
Iteration 6/25 | Loss: 0.00129494
Iteration 7/25 | Loss: 0.00124619
Iteration 8/25 | Loss: 0.00124776
Iteration 9/25 | Loss: 0.00123388
Iteration 10/25 | Loss: 0.00122426
Iteration 11/25 | Loss: 0.00120988
Iteration 12/25 | Loss: 0.00120018
Iteration 13/25 | Loss: 0.00122343
Iteration 14/25 | Loss: 0.00119413
Iteration 15/25 | Loss: 0.00119143
Iteration 16/25 | Loss: 0.00119127
Iteration 17/25 | Loss: 0.00119116
Iteration 18/25 | Loss: 0.00119087
Iteration 19/25 | Loss: 0.00119118
Iteration 20/25 | Loss: 0.00118855
Iteration 21/25 | Loss: 0.00118804
Iteration 22/25 | Loss: 0.00118803
Iteration 23/25 | Loss: 0.00118803
Iteration 24/25 | Loss: 0.00118803
Iteration 25/25 | Loss: 0.00118803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36943781
Iteration 2/25 | Loss: 0.00105915
Iteration 3/25 | Loss: 0.00105915
Iteration 4/25 | Loss: 0.00105915
Iteration 5/25 | Loss: 0.00105914
Iteration 6/25 | Loss: 0.00105914
Iteration 7/25 | Loss: 0.00105914
Iteration 8/25 | Loss: 0.00105914
Iteration 9/25 | Loss: 0.00105914
Iteration 10/25 | Loss: 0.00105914
Iteration 11/25 | Loss: 0.00105914
Iteration 12/25 | Loss: 0.00105914
Iteration 13/25 | Loss: 0.00105914
Iteration 14/25 | Loss: 0.00105914
Iteration 15/25 | Loss: 0.00105914
Iteration 16/25 | Loss: 0.00105914
Iteration 17/25 | Loss: 0.00105914
Iteration 18/25 | Loss: 0.00105914
Iteration 19/25 | Loss: 0.00105914
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010591419413685799, 0.0010591419413685799, 0.0010591419413685799, 0.0010591419413685799, 0.0010591419413685799]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010591419413685799

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105914
Iteration 2/1000 | Loss: 0.00017534
Iteration 3/1000 | Loss: 0.00031980
Iteration 4/1000 | Loss: 0.00005668
Iteration 5/1000 | Loss: 0.00004568
Iteration 6/1000 | Loss: 0.00003558
Iteration 7/1000 | Loss: 0.00003019
Iteration 8/1000 | Loss: 0.00002745
Iteration 9/1000 | Loss: 0.00002577
Iteration 10/1000 | Loss: 0.00002475
Iteration 11/1000 | Loss: 0.00002406
Iteration 12/1000 | Loss: 0.00002357
Iteration 13/1000 | Loss: 0.00002316
Iteration 14/1000 | Loss: 0.00002275
Iteration 15/1000 | Loss: 0.00002242
Iteration 16/1000 | Loss: 0.00002234
Iteration 17/1000 | Loss: 0.00002229
Iteration 18/1000 | Loss: 0.00002228
Iteration 19/1000 | Loss: 0.00002215
Iteration 20/1000 | Loss: 0.00002212
Iteration 21/1000 | Loss: 0.00002198
Iteration 22/1000 | Loss: 0.00002197
Iteration 23/1000 | Loss: 0.00002194
Iteration 24/1000 | Loss: 0.00002194
Iteration 25/1000 | Loss: 0.00002182
Iteration 26/1000 | Loss: 0.00002180
Iteration 27/1000 | Loss: 0.00002180
Iteration 28/1000 | Loss: 0.00002175
Iteration 29/1000 | Loss: 0.00002175
Iteration 30/1000 | Loss: 0.00002174
Iteration 31/1000 | Loss: 0.00002173
Iteration 32/1000 | Loss: 0.00002173
Iteration 33/1000 | Loss: 0.00002173
Iteration 34/1000 | Loss: 0.00002173
Iteration 35/1000 | Loss: 0.00002173
Iteration 36/1000 | Loss: 0.00002173
Iteration 37/1000 | Loss: 0.00002173
Iteration 38/1000 | Loss: 0.00002172
Iteration 39/1000 | Loss: 0.00002172
Iteration 40/1000 | Loss: 0.00002171
Iteration 41/1000 | Loss: 0.00002171
Iteration 42/1000 | Loss: 0.00002170
Iteration 43/1000 | Loss: 0.00002170
Iteration 44/1000 | Loss: 0.00002170
Iteration 45/1000 | Loss: 0.00002168
Iteration 46/1000 | Loss: 0.00002168
Iteration 47/1000 | Loss: 0.00002168
Iteration 48/1000 | Loss: 0.00002167
Iteration 49/1000 | Loss: 0.00002167
Iteration 50/1000 | Loss: 0.00002166
Iteration 51/1000 | Loss: 0.00002166
Iteration 52/1000 | Loss: 0.00002165
Iteration 53/1000 | Loss: 0.00002165
Iteration 54/1000 | Loss: 0.00002164
Iteration 55/1000 | Loss: 0.00002164
Iteration 56/1000 | Loss: 0.00002162
Iteration 57/1000 | Loss: 0.00002162
Iteration 58/1000 | Loss: 0.00002162
Iteration 59/1000 | Loss: 0.00002161
Iteration 60/1000 | Loss: 0.00002161
Iteration 61/1000 | Loss: 0.00002161
Iteration 62/1000 | Loss: 0.00002161
Iteration 63/1000 | Loss: 0.00002160
Iteration 64/1000 | Loss: 0.00002160
Iteration 65/1000 | Loss: 0.00002159
Iteration 66/1000 | Loss: 0.00002159
Iteration 67/1000 | Loss: 0.00002159
Iteration 68/1000 | Loss: 0.00002158
Iteration 69/1000 | Loss: 0.00002158
Iteration 70/1000 | Loss: 0.00002158
Iteration 71/1000 | Loss: 0.00002157
Iteration 72/1000 | Loss: 0.00002157
Iteration 73/1000 | Loss: 0.00002156
Iteration 74/1000 | Loss: 0.00002156
Iteration 75/1000 | Loss: 0.00002156
Iteration 76/1000 | Loss: 0.00002155
Iteration 77/1000 | Loss: 0.00002155
Iteration 78/1000 | Loss: 0.00002154
Iteration 79/1000 | Loss: 0.00002154
Iteration 80/1000 | Loss: 0.00002154
Iteration 81/1000 | Loss: 0.00002154
Iteration 82/1000 | Loss: 0.00002154
Iteration 83/1000 | Loss: 0.00002153
Iteration 84/1000 | Loss: 0.00002153
Iteration 85/1000 | Loss: 0.00002153
Iteration 86/1000 | Loss: 0.00002153
Iteration 87/1000 | Loss: 0.00002152
Iteration 88/1000 | Loss: 0.00002152
Iteration 89/1000 | Loss: 0.00002152
Iteration 90/1000 | Loss: 0.00002152
Iteration 91/1000 | Loss: 0.00002152
Iteration 92/1000 | Loss: 0.00002152
Iteration 93/1000 | Loss: 0.00002152
Iteration 94/1000 | Loss: 0.00002152
Iteration 95/1000 | Loss: 0.00002151
Iteration 96/1000 | Loss: 0.00002151
Iteration 97/1000 | Loss: 0.00002150
Iteration 98/1000 | Loss: 0.00002150
Iteration 99/1000 | Loss: 0.00002150
Iteration 100/1000 | Loss: 0.00002148
Iteration 101/1000 | Loss: 0.00002148
Iteration 102/1000 | Loss: 0.00002148
Iteration 103/1000 | Loss: 0.00002148
Iteration 104/1000 | Loss: 0.00002148
Iteration 105/1000 | Loss: 0.00002147
Iteration 106/1000 | Loss: 0.00002147
Iteration 107/1000 | Loss: 0.00002146
Iteration 108/1000 | Loss: 0.00002146
Iteration 109/1000 | Loss: 0.00002145
Iteration 110/1000 | Loss: 0.00002144
Iteration 111/1000 | Loss: 0.00002144
Iteration 112/1000 | Loss: 0.00002144
Iteration 113/1000 | Loss: 0.00002144
Iteration 114/1000 | Loss: 0.00002144
Iteration 115/1000 | Loss: 0.00002144
Iteration 116/1000 | Loss: 0.00002143
Iteration 117/1000 | Loss: 0.00002143
Iteration 118/1000 | Loss: 0.00002143
Iteration 119/1000 | Loss: 0.00002143
Iteration 120/1000 | Loss: 0.00002143
Iteration 121/1000 | Loss: 0.00002143
Iteration 122/1000 | Loss: 0.00002143
Iteration 123/1000 | Loss: 0.00002143
Iteration 124/1000 | Loss: 0.00002142
Iteration 125/1000 | Loss: 0.00002142
Iteration 126/1000 | Loss: 0.00002142
Iteration 127/1000 | Loss: 0.00002142
Iteration 128/1000 | Loss: 0.00002142
Iteration 129/1000 | Loss: 0.00002142
Iteration 130/1000 | Loss: 0.00002142
Iteration 131/1000 | Loss: 0.00002142
Iteration 132/1000 | Loss: 0.00002141
Iteration 133/1000 | Loss: 0.00002141
Iteration 134/1000 | Loss: 0.00002141
Iteration 135/1000 | Loss: 0.00002140
Iteration 136/1000 | Loss: 0.00002140
Iteration 137/1000 | Loss: 0.00002140
Iteration 138/1000 | Loss: 0.00002140
Iteration 139/1000 | Loss: 0.00002139
Iteration 140/1000 | Loss: 0.00002139
Iteration 141/1000 | Loss: 0.00002139
Iteration 142/1000 | Loss: 0.00002139
Iteration 143/1000 | Loss: 0.00002139
Iteration 144/1000 | Loss: 0.00002138
Iteration 145/1000 | Loss: 0.00002138
Iteration 146/1000 | Loss: 0.00002138
Iteration 147/1000 | Loss: 0.00002138
Iteration 148/1000 | Loss: 0.00002138
Iteration 149/1000 | Loss: 0.00002138
Iteration 150/1000 | Loss: 0.00002138
Iteration 151/1000 | Loss: 0.00002138
Iteration 152/1000 | Loss: 0.00002138
Iteration 153/1000 | Loss: 0.00002138
Iteration 154/1000 | Loss: 0.00002138
Iteration 155/1000 | Loss: 0.00002138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.138018680852838e-05, 2.138018680852838e-05, 2.138018680852838e-05, 2.138018680852838e-05, 2.138018680852838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.138018680852838e-05

Optimization complete. Final v2v error: 3.8536622524261475 mm

Highest mean error: 4.460221767425537 mm for frame 212

Lowest mean error: 3.2433667182922363 mm for frame 239

Saving results

Total time: 88.41936755180359
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439282
Iteration 2/25 | Loss: 0.00119751
Iteration 3/25 | Loss: 0.00111994
Iteration 4/25 | Loss: 0.00110668
Iteration 5/25 | Loss: 0.00110049
Iteration 6/25 | Loss: 0.00109900
Iteration 7/25 | Loss: 0.00109900
Iteration 8/25 | Loss: 0.00109900
Iteration 9/25 | Loss: 0.00109900
Iteration 10/25 | Loss: 0.00109900
Iteration 11/25 | Loss: 0.00109900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010990006849169731, 0.0010990006849169731, 0.0010990006849169731, 0.0010990006849169731, 0.0010990006849169731]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010990006849169731

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51675665
Iteration 2/25 | Loss: 0.00091142
Iteration 3/25 | Loss: 0.00091142
Iteration 4/25 | Loss: 0.00091142
Iteration 5/25 | Loss: 0.00091142
Iteration 6/25 | Loss: 0.00091141
Iteration 7/25 | Loss: 0.00091141
Iteration 8/25 | Loss: 0.00091141
Iteration 9/25 | Loss: 0.00091141
Iteration 10/25 | Loss: 0.00091141
Iteration 11/25 | Loss: 0.00091141
Iteration 12/25 | Loss: 0.00091141
Iteration 13/25 | Loss: 0.00091141
Iteration 14/25 | Loss: 0.00091141
Iteration 15/25 | Loss: 0.00091141
Iteration 16/25 | Loss: 0.00091141
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009114129934459925, 0.0009114129934459925, 0.0009114129934459925, 0.0009114129934459925, 0.0009114129934459925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009114129934459925

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091141
Iteration 2/1000 | Loss: 0.00004671
Iteration 3/1000 | Loss: 0.00002968
Iteration 4/1000 | Loss: 0.00002458
Iteration 5/1000 | Loss: 0.00002246
Iteration 6/1000 | Loss: 0.00002118
Iteration 7/1000 | Loss: 0.00002020
Iteration 8/1000 | Loss: 0.00001955
Iteration 9/1000 | Loss: 0.00001901
Iteration 10/1000 | Loss: 0.00001865
Iteration 11/1000 | Loss: 0.00001837
Iteration 12/1000 | Loss: 0.00001808
Iteration 13/1000 | Loss: 0.00001790
Iteration 14/1000 | Loss: 0.00001781
Iteration 15/1000 | Loss: 0.00001768
Iteration 16/1000 | Loss: 0.00001762
Iteration 17/1000 | Loss: 0.00001760
Iteration 18/1000 | Loss: 0.00001759
Iteration 19/1000 | Loss: 0.00001756
Iteration 20/1000 | Loss: 0.00001755
Iteration 21/1000 | Loss: 0.00001751
Iteration 22/1000 | Loss: 0.00001751
Iteration 23/1000 | Loss: 0.00001749
Iteration 24/1000 | Loss: 0.00001749
Iteration 25/1000 | Loss: 0.00001749
Iteration 26/1000 | Loss: 0.00001748
Iteration 27/1000 | Loss: 0.00001748
Iteration 28/1000 | Loss: 0.00001747
Iteration 29/1000 | Loss: 0.00001747
Iteration 30/1000 | Loss: 0.00001746
Iteration 31/1000 | Loss: 0.00001746
Iteration 32/1000 | Loss: 0.00001745
Iteration 33/1000 | Loss: 0.00001744
Iteration 34/1000 | Loss: 0.00001744
Iteration 35/1000 | Loss: 0.00001743
Iteration 36/1000 | Loss: 0.00001743
Iteration 37/1000 | Loss: 0.00001739
Iteration 38/1000 | Loss: 0.00001739
Iteration 39/1000 | Loss: 0.00001737
Iteration 40/1000 | Loss: 0.00001737
Iteration 41/1000 | Loss: 0.00001736
Iteration 42/1000 | Loss: 0.00001733
Iteration 43/1000 | Loss: 0.00001733
Iteration 44/1000 | Loss: 0.00001733
Iteration 45/1000 | Loss: 0.00001733
Iteration 46/1000 | Loss: 0.00001732
Iteration 47/1000 | Loss: 0.00001732
Iteration 48/1000 | Loss: 0.00001731
Iteration 49/1000 | Loss: 0.00001731
Iteration 50/1000 | Loss: 0.00001731
Iteration 51/1000 | Loss: 0.00001730
Iteration 52/1000 | Loss: 0.00001730
Iteration 53/1000 | Loss: 0.00001730
Iteration 54/1000 | Loss: 0.00001729
Iteration 55/1000 | Loss: 0.00001729
Iteration 56/1000 | Loss: 0.00001729
Iteration 57/1000 | Loss: 0.00001728
Iteration 58/1000 | Loss: 0.00001728
Iteration 59/1000 | Loss: 0.00001728
Iteration 60/1000 | Loss: 0.00001727
Iteration 61/1000 | Loss: 0.00001727
Iteration 62/1000 | Loss: 0.00001727
Iteration 63/1000 | Loss: 0.00001727
Iteration 64/1000 | Loss: 0.00001727
Iteration 65/1000 | Loss: 0.00001726
Iteration 66/1000 | Loss: 0.00001726
Iteration 67/1000 | Loss: 0.00001726
Iteration 68/1000 | Loss: 0.00001726
Iteration 69/1000 | Loss: 0.00001726
Iteration 70/1000 | Loss: 0.00001726
Iteration 71/1000 | Loss: 0.00001725
Iteration 72/1000 | Loss: 0.00001725
Iteration 73/1000 | Loss: 0.00001725
Iteration 74/1000 | Loss: 0.00001725
Iteration 75/1000 | Loss: 0.00001724
Iteration 76/1000 | Loss: 0.00001724
Iteration 77/1000 | Loss: 0.00001724
Iteration 78/1000 | Loss: 0.00001724
Iteration 79/1000 | Loss: 0.00001724
Iteration 80/1000 | Loss: 0.00001724
Iteration 81/1000 | Loss: 0.00001724
Iteration 82/1000 | Loss: 0.00001723
Iteration 83/1000 | Loss: 0.00001723
Iteration 84/1000 | Loss: 0.00001723
Iteration 85/1000 | Loss: 0.00001723
Iteration 86/1000 | Loss: 0.00001723
Iteration 87/1000 | Loss: 0.00001723
Iteration 88/1000 | Loss: 0.00001723
Iteration 89/1000 | Loss: 0.00001723
Iteration 90/1000 | Loss: 0.00001723
Iteration 91/1000 | Loss: 0.00001723
Iteration 92/1000 | Loss: 0.00001723
Iteration 93/1000 | Loss: 0.00001722
Iteration 94/1000 | Loss: 0.00001722
Iteration 95/1000 | Loss: 0.00001722
Iteration 96/1000 | Loss: 0.00001722
Iteration 97/1000 | Loss: 0.00001721
Iteration 98/1000 | Loss: 0.00001721
Iteration 99/1000 | Loss: 0.00001721
Iteration 100/1000 | Loss: 0.00001721
Iteration 101/1000 | Loss: 0.00001721
Iteration 102/1000 | Loss: 0.00001721
Iteration 103/1000 | Loss: 0.00001721
Iteration 104/1000 | Loss: 0.00001721
Iteration 105/1000 | Loss: 0.00001721
Iteration 106/1000 | Loss: 0.00001720
Iteration 107/1000 | Loss: 0.00001720
Iteration 108/1000 | Loss: 0.00001720
Iteration 109/1000 | Loss: 0.00001720
Iteration 110/1000 | Loss: 0.00001720
Iteration 111/1000 | Loss: 0.00001719
Iteration 112/1000 | Loss: 0.00001719
Iteration 113/1000 | Loss: 0.00001719
Iteration 114/1000 | Loss: 0.00001719
Iteration 115/1000 | Loss: 0.00001719
Iteration 116/1000 | Loss: 0.00001719
Iteration 117/1000 | Loss: 0.00001719
Iteration 118/1000 | Loss: 0.00001719
Iteration 119/1000 | Loss: 0.00001719
Iteration 120/1000 | Loss: 0.00001718
Iteration 121/1000 | Loss: 0.00001718
Iteration 122/1000 | Loss: 0.00001718
Iteration 123/1000 | Loss: 0.00001718
Iteration 124/1000 | Loss: 0.00001718
Iteration 125/1000 | Loss: 0.00001718
Iteration 126/1000 | Loss: 0.00001718
Iteration 127/1000 | Loss: 0.00001718
Iteration 128/1000 | Loss: 0.00001718
Iteration 129/1000 | Loss: 0.00001718
Iteration 130/1000 | Loss: 0.00001718
Iteration 131/1000 | Loss: 0.00001718
Iteration 132/1000 | Loss: 0.00001717
Iteration 133/1000 | Loss: 0.00001717
Iteration 134/1000 | Loss: 0.00001717
Iteration 135/1000 | Loss: 0.00001717
Iteration 136/1000 | Loss: 0.00001717
Iteration 137/1000 | Loss: 0.00001716
Iteration 138/1000 | Loss: 0.00001716
Iteration 139/1000 | Loss: 0.00001716
Iteration 140/1000 | Loss: 0.00001716
Iteration 141/1000 | Loss: 0.00001715
Iteration 142/1000 | Loss: 0.00001715
Iteration 143/1000 | Loss: 0.00001715
Iteration 144/1000 | Loss: 0.00001715
Iteration 145/1000 | Loss: 0.00001715
Iteration 146/1000 | Loss: 0.00001715
Iteration 147/1000 | Loss: 0.00001715
Iteration 148/1000 | Loss: 0.00001715
Iteration 149/1000 | Loss: 0.00001715
Iteration 150/1000 | Loss: 0.00001714
Iteration 151/1000 | Loss: 0.00001714
Iteration 152/1000 | Loss: 0.00001714
Iteration 153/1000 | Loss: 0.00001714
Iteration 154/1000 | Loss: 0.00001714
Iteration 155/1000 | Loss: 0.00001714
Iteration 156/1000 | Loss: 0.00001714
Iteration 157/1000 | Loss: 0.00001714
Iteration 158/1000 | Loss: 0.00001714
Iteration 159/1000 | Loss: 0.00001714
Iteration 160/1000 | Loss: 0.00001714
Iteration 161/1000 | Loss: 0.00001714
Iteration 162/1000 | Loss: 0.00001714
Iteration 163/1000 | Loss: 0.00001714
Iteration 164/1000 | Loss: 0.00001714
Iteration 165/1000 | Loss: 0.00001714
Iteration 166/1000 | Loss: 0.00001714
Iteration 167/1000 | Loss: 0.00001714
Iteration 168/1000 | Loss: 0.00001714
Iteration 169/1000 | Loss: 0.00001714
Iteration 170/1000 | Loss: 0.00001714
Iteration 171/1000 | Loss: 0.00001714
Iteration 172/1000 | Loss: 0.00001714
Iteration 173/1000 | Loss: 0.00001713
Iteration 174/1000 | Loss: 0.00001713
Iteration 175/1000 | Loss: 0.00001713
Iteration 176/1000 | Loss: 0.00001713
Iteration 177/1000 | Loss: 0.00001713
Iteration 178/1000 | Loss: 0.00001713
Iteration 179/1000 | Loss: 0.00001713
Iteration 180/1000 | Loss: 0.00001713
Iteration 181/1000 | Loss: 0.00001713
Iteration 182/1000 | Loss: 0.00001713
Iteration 183/1000 | Loss: 0.00001712
Iteration 184/1000 | Loss: 0.00001712
Iteration 185/1000 | Loss: 0.00001712
Iteration 186/1000 | Loss: 0.00001712
Iteration 187/1000 | Loss: 0.00001712
Iteration 188/1000 | Loss: 0.00001712
Iteration 189/1000 | Loss: 0.00001712
Iteration 190/1000 | Loss: 0.00001712
Iteration 191/1000 | Loss: 0.00001712
Iteration 192/1000 | Loss: 0.00001712
Iteration 193/1000 | Loss: 0.00001712
Iteration 194/1000 | Loss: 0.00001712
Iteration 195/1000 | Loss: 0.00001712
Iteration 196/1000 | Loss: 0.00001712
Iteration 197/1000 | Loss: 0.00001712
Iteration 198/1000 | Loss: 0.00001712
Iteration 199/1000 | Loss: 0.00001712
Iteration 200/1000 | Loss: 0.00001711
Iteration 201/1000 | Loss: 0.00001711
Iteration 202/1000 | Loss: 0.00001711
Iteration 203/1000 | Loss: 0.00001711
Iteration 204/1000 | Loss: 0.00001711
Iteration 205/1000 | Loss: 0.00001711
Iteration 206/1000 | Loss: 0.00001711
Iteration 207/1000 | Loss: 0.00001711
Iteration 208/1000 | Loss: 0.00001711
Iteration 209/1000 | Loss: 0.00001711
Iteration 210/1000 | Loss: 0.00001711
Iteration 211/1000 | Loss: 0.00001711
Iteration 212/1000 | Loss: 0.00001711
Iteration 213/1000 | Loss: 0.00001711
Iteration 214/1000 | Loss: 0.00001711
Iteration 215/1000 | Loss: 0.00001711
Iteration 216/1000 | Loss: 0.00001711
Iteration 217/1000 | Loss: 0.00001711
Iteration 218/1000 | Loss: 0.00001711
Iteration 219/1000 | Loss: 0.00001710
Iteration 220/1000 | Loss: 0.00001710
Iteration 221/1000 | Loss: 0.00001710
Iteration 222/1000 | Loss: 0.00001710
Iteration 223/1000 | Loss: 0.00001710
Iteration 224/1000 | Loss: 0.00001710
Iteration 225/1000 | Loss: 0.00001710
Iteration 226/1000 | Loss: 0.00001710
Iteration 227/1000 | Loss: 0.00001710
Iteration 228/1000 | Loss: 0.00001710
Iteration 229/1000 | Loss: 0.00001710
Iteration 230/1000 | Loss: 0.00001710
Iteration 231/1000 | Loss: 0.00001710
Iteration 232/1000 | Loss: 0.00001710
Iteration 233/1000 | Loss: 0.00001710
Iteration 234/1000 | Loss: 0.00001710
Iteration 235/1000 | Loss: 0.00001710
Iteration 236/1000 | Loss: 0.00001710
Iteration 237/1000 | Loss: 0.00001710
Iteration 238/1000 | Loss: 0.00001710
Iteration 239/1000 | Loss: 0.00001710
Iteration 240/1000 | Loss: 0.00001710
Iteration 241/1000 | Loss: 0.00001710
Iteration 242/1000 | Loss: 0.00001709
Iteration 243/1000 | Loss: 0.00001709
Iteration 244/1000 | Loss: 0.00001709
Iteration 245/1000 | Loss: 0.00001709
Iteration 246/1000 | Loss: 0.00001709
Iteration 247/1000 | Loss: 0.00001709
Iteration 248/1000 | Loss: 0.00001709
Iteration 249/1000 | Loss: 0.00001709
Iteration 250/1000 | Loss: 0.00001709
Iteration 251/1000 | Loss: 0.00001709
Iteration 252/1000 | Loss: 0.00001709
Iteration 253/1000 | Loss: 0.00001709
Iteration 254/1000 | Loss: 0.00001709
Iteration 255/1000 | Loss: 0.00001709
Iteration 256/1000 | Loss: 0.00001709
Iteration 257/1000 | Loss: 0.00001709
Iteration 258/1000 | Loss: 0.00001708
Iteration 259/1000 | Loss: 0.00001708
Iteration 260/1000 | Loss: 0.00001708
Iteration 261/1000 | Loss: 0.00001708
Iteration 262/1000 | Loss: 0.00001708
Iteration 263/1000 | Loss: 0.00001708
Iteration 264/1000 | Loss: 0.00001708
Iteration 265/1000 | Loss: 0.00001708
Iteration 266/1000 | Loss: 0.00001708
Iteration 267/1000 | Loss: 0.00001708
Iteration 268/1000 | Loss: 0.00001708
Iteration 269/1000 | Loss: 0.00001708
Iteration 270/1000 | Loss: 0.00001708
Iteration 271/1000 | Loss: 0.00001708
Iteration 272/1000 | Loss: 0.00001707
Iteration 273/1000 | Loss: 0.00001707
Iteration 274/1000 | Loss: 0.00001707
Iteration 275/1000 | Loss: 0.00001707
Iteration 276/1000 | Loss: 0.00001707
Iteration 277/1000 | Loss: 0.00001707
Iteration 278/1000 | Loss: 0.00001707
Iteration 279/1000 | Loss: 0.00001707
Iteration 280/1000 | Loss: 0.00001707
Iteration 281/1000 | Loss: 0.00001707
Iteration 282/1000 | Loss: 0.00001707
Iteration 283/1000 | Loss: 0.00001707
Iteration 284/1000 | Loss: 0.00001707
Iteration 285/1000 | Loss: 0.00001707
Iteration 286/1000 | Loss: 0.00001707
Iteration 287/1000 | Loss: 0.00001707
Iteration 288/1000 | Loss: 0.00001707
Iteration 289/1000 | Loss: 0.00001707
Iteration 290/1000 | Loss: 0.00001707
Iteration 291/1000 | Loss: 0.00001707
Iteration 292/1000 | Loss: 0.00001707
Iteration 293/1000 | Loss: 0.00001707
Iteration 294/1000 | Loss: 0.00001707
Iteration 295/1000 | Loss: 0.00001707
Iteration 296/1000 | Loss: 0.00001707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 296. Stopping optimization.
Last 5 losses: [1.7070135072572157e-05, 1.7070135072572157e-05, 1.7070135072572157e-05, 1.7070135072572157e-05, 1.7070135072572157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7070135072572157e-05

Optimization complete. Final v2v error: 3.399567127227783 mm

Highest mean error: 5.839420318603516 mm for frame 65

Lowest mean error: 2.465778350830078 mm for frame 105

Saving results

Total time: 58.10475468635559
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00733544
Iteration 2/25 | Loss: 0.00188991
Iteration 3/25 | Loss: 0.00159036
Iteration 4/25 | Loss: 0.00149418
Iteration 5/25 | Loss: 0.00147635
Iteration 6/25 | Loss: 0.00140354
Iteration 7/25 | Loss: 0.00137718
Iteration 8/25 | Loss: 0.00137235
Iteration 9/25 | Loss: 0.00137169
Iteration 10/25 | Loss: 0.00137169
Iteration 11/25 | Loss: 0.00137143
Iteration 12/25 | Loss: 0.00137116
Iteration 13/25 | Loss: 0.00137090
Iteration 14/25 | Loss: 0.00137082
Iteration 15/25 | Loss: 0.00137082
Iteration 16/25 | Loss: 0.00137082
Iteration 17/25 | Loss: 0.00137082
Iteration 18/25 | Loss: 0.00137082
Iteration 19/25 | Loss: 0.00137082
Iteration 20/25 | Loss: 0.00137082
Iteration 21/25 | Loss: 0.00137082
Iteration 22/25 | Loss: 0.00137082
Iteration 23/25 | Loss: 0.00137082
Iteration 24/25 | Loss: 0.00137082
Iteration 25/25 | Loss: 0.00137082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013708180049434304, 0.0013708180049434304, 0.0013708180049434304, 0.0013708180049434304, 0.0013708180049434304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013708180049434304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35101116
Iteration 2/25 | Loss: 0.00107966
Iteration 3/25 | Loss: 0.00107966
Iteration 4/25 | Loss: 0.00107966
Iteration 5/25 | Loss: 0.00107966
Iteration 6/25 | Loss: 0.00107966
Iteration 7/25 | Loss: 0.00107966
Iteration 8/25 | Loss: 0.00107966
Iteration 9/25 | Loss: 0.00107966
Iteration 10/25 | Loss: 0.00107966
Iteration 11/25 | Loss: 0.00107966
Iteration 12/25 | Loss: 0.00107966
Iteration 13/25 | Loss: 0.00107966
Iteration 14/25 | Loss: 0.00107966
Iteration 15/25 | Loss: 0.00107966
Iteration 16/25 | Loss: 0.00107966
Iteration 17/25 | Loss: 0.00107966
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010796576971188188, 0.0010796576971188188, 0.0010796576971188188, 0.0010796576971188188, 0.0010796576971188188]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010796576971188188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107966
Iteration 2/1000 | Loss: 0.00010465
Iteration 3/1000 | Loss: 0.00006589
Iteration 4/1000 | Loss: 0.00005783
Iteration 5/1000 | Loss: 0.00005481
Iteration 6/1000 | Loss: 0.00005260
Iteration 7/1000 | Loss: 0.00005123
Iteration 8/1000 | Loss: 0.00005024
Iteration 9/1000 | Loss: 0.00004956
Iteration 10/1000 | Loss: 0.00004901
Iteration 11/1000 | Loss: 0.00004850
Iteration 12/1000 | Loss: 0.00004822
Iteration 13/1000 | Loss: 0.00004803
Iteration 14/1000 | Loss: 0.00004789
Iteration 15/1000 | Loss: 0.00004772
Iteration 16/1000 | Loss: 0.00004765
Iteration 17/1000 | Loss: 0.00004759
Iteration 18/1000 | Loss: 0.00004756
Iteration 19/1000 | Loss: 0.00004755
Iteration 20/1000 | Loss: 0.00004753
Iteration 21/1000 | Loss: 0.00004752
Iteration 22/1000 | Loss: 0.00004752
Iteration 23/1000 | Loss: 0.00004752
Iteration 24/1000 | Loss: 0.00004750
Iteration 25/1000 | Loss: 0.00004750
Iteration 26/1000 | Loss: 0.00004750
Iteration 27/1000 | Loss: 0.00004750
Iteration 28/1000 | Loss: 0.00004749
Iteration 29/1000 | Loss: 0.00004749
Iteration 30/1000 | Loss: 0.00004749
Iteration 31/1000 | Loss: 0.00004749
Iteration 32/1000 | Loss: 0.00004749
Iteration 33/1000 | Loss: 0.00004749
Iteration 34/1000 | Loss: 0.00004748
Iteration 35/1000 | Loss: 0.00004748
Iteration 36/1000 | Loss: 0.00004747
Iteration 37/1000 | Loss: 0.00004747
Iteration 38/1000 | Loss: 0.00004747
Iteration 39/1000 | Loss: 0.00004747
Iteration 40/1000 | Loss: 0.00004747
Iteration 41/1000 | Loss: 0.00004747
Iteration 42/1000 | Loss: 0.00004746
Iteration 43/1000 | Loss: 0.00004746
Iteration 44/1000 | Loss: 0.00004746
Iteration 45/1000 | Loss: 0.00004746
Iteration 46/1000 | Loss: 0.00004745
Iteration 47/1000 | Loss: 0.00004745
Iteration 48/1000 | Loss: 0.00004745
Iteration 49/1000 | Loss: 0.00004745
Iteration 50/1000 | Loss: 0.00004745
Iteration 51/1000 | Loss: 0.00004745
Iteration 52/1000 | Loss: 0.00004745
Iteration 53/1000 | Loss: 0.00004745
Iteration 54/1000 | Loss: 0.00004745
Iteration 55/1000 | Loss: 0.00004745
Iteration 56/1000 | Loss: 0.00004745
Iteration 57/1000 | Loss: 0.00004745
Iteration 58/1000 | Loss: 0.00004745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [4.7447738324990496e-05, 4.7447738324990496e-05, 4.7447738324990496e-05, 4.7447738324990496e-05, 4.7447738324990496e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.7447738324990496e-05

Optimization complete. Final v2v error: 5.600102424621582 mm

Highest mean error: 6.536299228668213 mm for frame 39

Lowest mean error: 4.830735683441162 mm for frame 216

Saving results

Total time: 56.84404921531677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885243
Iteration 2/25 | Loss: 0.00140973
Iteration 3/25 | Loss: 0.00119983
Iteration 4/25 | Loss: 0.00118104
Iteration 5/25 | Loss: 0.00118077
Iteration 6/25 | Loss: 0.00118067
Iteration 7/25 | Loss: 0.00117674
Iteration 8/25 | Loss: 0.00117502
Iteration 9/25 | Loss: 0.00117467
Iteration 10/25 | Loss: 0.00117443
Iteration 11/25 | Loss: 0.00117857
Iteration 12/25 | Loss: 0.00117756
Iteration 13/25 | Loss: 0.00117757
Iteration 14/25 | Loss: 0.00117740
Iteration 15/25 | Loss: 0.00117866
Iteration 16/25 | Loss: 0.00117813
Iteration 17/25 | Loss: 0.00117705
Iteration 18/25 | Loss: 0.00117458
Iteration 19/25 | Loss: 0.00117828
Iteration 20/25 | Loss: 0.00117682
Iteration 21/25 | Loss: 0.00117437
Iteration 22/25 | Loss: 0.00117421
Iteration 23/25 | Loss: 0.00117414
Iteration 24/25 | Loss: 0.00117411
Iteration 25/25 | Loss: 0.00117410

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 13.34585190
Iteration 2/25 | Loss: 0.00066824
Iteration 3/25 | Loss: 0.00066811
Iteration 4/25 | Loss: 0.00066811
Iteration 5/25 | Loss: 0.00066811
Iteration 6/25 | Loss: 0.00066811
Iteration 7/25 | Loss: 0.00066811
Iteration 8/25 | Loss: 0.00066811
Iteration 9/25 | Loss: 0.00066811
Iteration 10/25 | Loss: 0.00066811
Iteration 11/25 | Loss: 0.00066811
Iteration 12/25 | Loss: 0.00066811
Iteration 13/25 | Loss: 0.00066811
Iteration 14/25 | Loss: 0.00066811
Iteration 15/25 | Loss: 0.00066811
Iteration 16/25 | Loss: 0.00066811
Iteration 17/25 | Loss: 0.00066811
Iteration 18/25 | Loss: 0.00066811
Iteration 19/25 | Loss: 0.00066811
Iteration 20/25 | Loss: 0.00066811
Iteration 21/25 | Loss: 0.00066811
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006681092781946063, 0.0006681092781946063, 0.0006681092781946063, 0.0006681092781946063, 0.0006681092781946063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006681092781946063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066811
Iteration 2/1000 | Loss: 0.00003942
Iteration 3/1000 | Loss: 0.00002849
Iteration 4/1000 | Loss: 0.00002546
Iteration 5/1000 | Loss: 0.00002431
Iteration 6/1000 | Loss: 0.00002338
Iteration 7/1000 | Loss: 0.00002260
Iteration 8/1000 | Loss: 0.00002214
Iteration 9/1000 | Loss: 0.00002167
Iteration 10/1000 | Loss: 0.00002134
Iteration 11/1000 | Loss: 0.00002117
Iteration 12/1000 | Loss: 0.00002111
Iteration 13/1000 | Loss: 0.00002111
Iteration 14/1000 | Loss: 0.00002102
Iteration 15/1000 | Loss: 0.00002101
Iteration 16/1000 | Loss: 0.00002096
Iteration 17/1000 | Loss: 0.00002092
Iteration 18/1000 | Loss: 0.00002091
Iteration 19/1000 | Loss: 0.00002090
Iteration 20/1000 | Loss: 0.00002089
Iteration 21/1000 | Loss: 0.00002088
Iteration 22/1000 | Loss: 0.00002087
Iteration 23/1000 | Loss: 0.00002083
Iteration 24/1000 | Loss: 0.00002080
Iteration 25/1000 | Loss: 0.00002079
Iteration 26/1000 | Loss: 0.00002079
Iteration 27/1000 | Loss: 0.00002078
Iteration 28/1000 | Loss: 0.00002078
Iteration 29/1000 | Loss: 0.00002078
Iteration 30/1000 | Loss: 0.00002078
Iteration 31/1000 | Loss: 0.00002078
Iteration 32/1000 | Loss: 0.00002078
Iteration 33/1000 | Loss: 0.00002078
Iteration 34/1000 | Loss: 0.00002077
Iteration 35/1000 | Loss: 0.00002077
Iteration 36/1000 | Loss: 0.00002076
Iteration 37/1000 | Loss: 0.00002076
Iteration 38/1000 | Loss: 0.00002076
Iteration 39/1000 | Loss: 0.00002075
Iteration 40/1000 | Loss: 0.00002075
Iteration 41/1000 | Loss: 0.00002075
Iteration 42/1000 | Loss: 0.00002075
Iteration 43/1000 | Loss: 0.00002075
Iteration 44/1000 | Loss: 0.00002075
Iteration 45/1000 | Loss: 0.00002075
Iteration 46/1000 | Loss: 0.00002075
Iteration 47/1000 | Loss: 0.00002074
Iteration 48/1000 | Loss: 0.00002074
Iteration 49/1000 | Loss: 0.00002074
Iteration 50/1000 | Loss: 0.00002073
Iteration 51/1000 | Loss: 0.00002073
Iteration 52/1000 | Loss: 0.00002073
Iteration 53/1000 | Loss: 0.00002072
Iteration 54/1000 | Loss: 0.00002072
Iteration 55/1000 | Loss: 0.00002072
Iteration 56/1000 | Loss: 0.00002071
Iteration 57/1000 | Loss: 0.00002071
Iteration 58/1000 | Loss: 0.00002071
Iteration 59/1000 | Loss: 0.00002071
Iteration 60/1000 | Loss: 0.00002071
Iteration 61/1000 | Loss: 0.00002071
Iteration 62/1000 | Loss: 0.00002070
Iteration 63/1000 | Loss: 0.00002070
Iteration 64/1000 | Loss: 0.00017241
Iteration 65/1000 | Loss: 0.00002607
Iteration 66/1000 | Loss: 0.00002351
Iteration 67/1000 | Loss: 0.00002258
Iteration 68/1000 | Loss: 0.00002187
Iteration 69/1000 | Loss: 0.00002153
Iteration 70/1000 | Loss: 0.00002126
Iteration 71/1000 | Loss: 0.00002106
Iteration 72/1000 | Loss: 0.00002099
Iteration 73/1000 | Loss: 0.00002097
Iteration 74/1000 | Loss: 0.00002097
Iteration 75/1000 | Loss: 0.00002095
Iteration 76/1000 | Loss: 0.00002093
Iteration 77/1000 | Loss: 0.00002092
Iteration 78/1000 | Loss: 0.00002087
Iteration 79/1000 | Loss: 0.00002087
Iteration 80/1000 | Loss: 0.00002085
Iteration 81/1000 | Loss: 0.00002084
Iteration 82/1000 | Loss: 0.00002081
Iteration 83/1000 | Loss: 0.00002081
Iteration 84/1000 | Loss: 0.00002079
Iteration 85/1000 | Loss: 0.00002079
Iteration 86/1000 | Loss: 0.00002078
Iteration 87/1000 | Loss: 0.00002078
Iteration 88/1000 | Loss: 0.00002078
Iteration 89/1000 | Loss: 0.00002078
Iteration 90/1000 | Loss: 0.00002077
Iteration 91/1000 | Loss: 0.00002077
Iteration 92/1000 | Loss: 0.00002077
Iteration 93/1000 | Loss: 0.00002077
Iteration 94/1000 | Loss: 0.00002077
Iteration 95/1000 | Loss: 0.00002077
Iteration 96/1000 | Loss: 0.00002077
Iteration 97/1000 | Loss: 0.00002077
Iteration 98/1000 | Loss: 0.00002076
Iteration 99/1000 | Loss: 0.00002076
Iteration 100/1000 | Loss: 0.00002076
Iteration 101/1000 | Loss: 0.00002076
Iteration 102/1000 | Loss: 0.00002076
Iteration 103/1000 | Loss: 0.00002076
Iteration 104/1000 | Loss: 0.00002076
Iteration 105/1000 | Loss: 0.00002076
Iteration 106/1000 | Loss: 0.00002076
Iteration 107/1000 | Loss: 0.00002075
Iteration 108/1000 | Loss: 0.00002075
Iteration 109/1000 | Loss: 0.00002075
Iteration 110/1000 | Loss: 0.00002075
Iteration 111/1000 | Loss: 0.00002075
Iteration 112/1000 | Loss: 0.00002075
Iteration 113/1000 | Loss: 0.00002075
Iteration 114/1000 | Loss: 0.00002074
Iteration 115/1000 | Loss: 0.00002074
Iteration 116/1000 | Loss: 0.00002074
Iteration 117/1000 | Loss: 0.00002074
Iteration 118/1000 | Loss: 0.00002074
Iteration 119/1000 | Loss: 0.00002074
Iteration 120/1000 | Loss: 0.00002074
Iteration 121/1000 | Loss: 0.00002074
Iteration 122/1000 | Loss: 0.00002074
Iteration 123/1000 | Loss: 0.00002073
Iteration 124/1000 | Loss: 0.00002073
Iteration 125/1000 | Loss: 0.00002073
Iteration 126/1000 | Loss: 0.00002073
Iteration 127/1000 | Loss: 0.00002073
Iteration 128/1000 | Loss: 0.00002073
Iteration 129/1000 | Loss: 0.00002073
Iteration 130/1000 | Loss: 0.00002073
Iteration 131/1000 | Loss: 0.00002073
Iteration 132/1000 | Loss: 0.00002073
Iteration 133/1000 | Loss: 0.00002073
Iteration 134/1000 | Loss: 0.00002073
Iteration 135/1000 | Loss: 0.00002073
Iteration 136/1000 | Loss: 0.00002073
Iteration 137/1000 | Loss: 0.00002073
Iteration 138/1000 | Loss: 0.00002072
Iteration 139/1000 | Loss: 0.00002072
Iteration 140/1000 | Loss: 0.00002072
Iteration 141/1000 | Loss: 0.00002072
Iteration 142/1000 | Loss: 0.00002072
Iteration 143/1000 | Loss: 0.00002072
Iteration 144/1000 | Loss: 0.00002072
Iteration 145/1000 | Loss: 0.00002072
Iteration 146/1000 | Loss: 0.00002072
Iteration 147/1000 | Loss: 0.00002072
Iteration 148/1000 | Loss: 0.00002072
Iteration 149/1000 | Loss: 0.00002072
Iteration 150/1000 | Loss: 0.00002072
Iteration 151/1000 | Loss: 0.00002072
Iteration 152/1000 | Loss: 0.00002072
Iteration 153/1000 | Loss: 0.00002072
Iteration 154/1000 | Loss: 0.00002072
Iteration 155/1000 | Loss: 0.00002072
Iteration 156/1000 | Loss: 0.00002072
Iteration 157/1000 | Loss: 0.00002072
Iteration 158/1000 | Loss: 0.00002071
Iteration 159/1000 | Loss: 0.00002071
Iteration 160/1000 | Loss: 0.00002071
Iteration 161/1000 | Loss: 0.00002071
Iteration 162/1000 | Loss: 0.00002071
Iteration 163/1000 | Loss: 0.00002071
Iteration 164/1000 | Loss: 0.00002071
Iteration 165/1000 | Loss: 0.00002071
Iteration 166/1000 | Loss: 0.00002071
Iteration 167/1000 | Loss: 0.00002071
Iteration 168/1000 | Loss: 0.00002071
Iteration 169/1000 | Loss: 0.00002071
Iteration 170/1000 | Loss: 0.00002071
Iteration 171/1000 | Loss: 0.00002070
Iteration 172/1000 | Loss: 0.00002070
Iteration 173/1000 | Loss: 0.00002070
Iteration 174/1000 | Loss: 0.00002070
Iteration 175/1000 | Loss: 0.00002070
Iteration 176/1000 | Loss: 0.00002070
Iteration 177/1000 | Loss: 0.00002070
Iteration 178/1000 | Loss: 0.00002070
Iteration 179/1000 | Loss: 0.00002070
Iteration 180/1000 | Loss: 0.00002070
Iteration 181/1000 | Loss: 0.00002070
Iteration 182/1000 | Loss: 0.00002070
Iteration 183/1000 | Loss: 0.00002070
Iteration 184/1000 | Loss: 0.00002070
Iteration 185/1000 | Loss: 0.00002070
Iteration 186/1000 | Loss: 0.00002070
Iteration 187/1000 | Loss: 0.00002070
Iteration 188/1000 | Loss: 0.00002069
Iteration 189/1000 | Loss: 0.00002069
Iteration 190/1000 | Loss: 0.00002069
Iteration 191/1000 | Loss: 0.00002069
Iteration 192/1000 | Loss: 0.00002069
Iteration 193/1000 | Loss: 0.00002069
Iteration 194/1000 | Loss: 0.00002069
Iteration 195/1000 | Loss: 0.00002069
Iteration 196/1000 | Loss: 0.00002069
Iteration 197/1000 | Loss: 0.00002069
Iteration 198/1000 | Loss: 0.00002069
Iteration 199/1000 | Loss: 0.00002069
Iteration 200/1000 | Loss: 0.00002069
Iteration 201/1000 | Loss: 0.00002069
Iteration 202/1000 | Loss: 0.00002069
Iteration 203/1000 | Loss: 0.00002069
Iteration 204/1000 | Loss: 0.00002069
Iteration 205/1000 | Loss: 0.00002068
Iteration 206/1000 | Loss: 0.00002068
Iteration 207/1000 | Loss: 0.00002068
Iteration 208/1000 | Loss: 0.00002068
Iteration 209/1000 | Loss: 0.00002068
Iteration 210/1000 | Loss: 0.00002068
Iteration 211/1000 | Loss: 0.00002068
Iteration 212/1000 | Loss: 0.00002068
Iteration 213/1000 | Loss: 0.00002068
Iteration 214/1000 | Loss: 0.00002068
Iteration 215/1000 | Loss: 0.00002068
Iteration 216/1000 | Loss: 0.00002068
Iteration 217/1000 | Loss: 0.00002068
Iteration 218/1000 | Loss: 0.00002068
Iteration 219/1000 | Loss: 0.00002068
Iteration 220/1000 | Loss: 0.00002068
Iteration 221/1000 | Loss: 0.00002068
Iteration 222/1000 | Loss: 0.00002068
Iteration 223/1000 | Loss: 0.00002068
Iteration 224/1000 | Loss: 0.00002068
Iteration 225/1000 | Loss: 0.00002068
Iteration 226/1000 | Loss: 0.00002067
Iteration 227/1000 | Loss: 0.00002067
Iteration 228/1000 | Loss: 0.00002067
Iteration 229/1000 | Loss: 0.00002067
Iteration 230/1000 | Loss: 0.00002067
Iteration 231/1000 | Loss: 0.00002067
Iteration 232/1000 | Loss: 0.00002067
Iteration 233/1000 | Loss: 0.00002067
Iteration 234/1000 | Loss: 0.00002067
Iteration 235/1000 | Loss: 0.00002067
Iteration 236/1000 | Loss: 0.00002067
Iteration 237/1000 | Loss: 0.00002067
Iteration 238/1000 | Loss: 0.00002067
Iteration 239/1000 | Loss: 0.00002067
Iteration 240/1000 | Loss: 0.00002067
Iteration 241/1000 | Loss: 0.00002067
Iteration 242/1000 | Loss: 0.00002067
Iteration 243/1000 | Loss: 0.00002067
Iteration 244/1000 | Loss: 0.00002067
Iteration 245/1000 | Loss: 0.00002067
Iteration 246/1000 | Loss: 0.00002067
Iteration 247/1000 | Loss: 0.00002067
Iteration 248/1000 | Loss: 0.00002067
Iteration 249/1000 | Loss: 0.00002067
Iteration 250/1000 | Loss: 0.00002067
Iteration 251/1000 | Loss: 0.00002067
Iteration 252/1000 | Loss: 0.00002067
Iteration 253/1000 | Loss: 0.00002067
Iteration 254/1000 | Loss: 0.00002067
Iteration 255/1000 | Loss: 0.00002067
Iteration 256/1000 | Loss: 0.00002067
Iteration 257/1000 | Loss: 0.00002067
Iteration 258/1000 | Loss: 0.00002067
Iteration 259/1000 | Loss: 0.00002067
Iteration 260/1000 | Loss: 0.00002067
Iteration 261/1000 | Loss: 0.00002067
Iteration 262/1000 | Loss: 0.00002067
Iteration 263/1000 | Loss: 0.00002067
Iteration 264/1000 | Loss: 0.00002067
Iteration 265/1000 | Loss: 0.00002067
Iteration 266/1000 | Loss: 0.00002067
Iteration 267/1000 | Loss: 0.00002067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [2.0668700017267838e-05, 2.0668700017267838e-05, 2.0668700017267838e-05, 2.0668700017267838e-05, 2.0668700017267838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0668700017267838e-05

Optimization complete. Final v2v error: 3.7415003776550293 mm

Highest mean error: 6.577664852142334 mm for frame 112

Lowest mean error: 3.1345391273498535 mm for frame 206

Saving results

Total time: 97.38135600090027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005847
Iteration 2/25 | Loss: 0.00358679
Iteration 3/25 | Loss: 0.00234025
Iteration 4/25 | Loss: 0.00200933
Iteration 5/25 | Loss: 0.00159984
Iteration 6/25 | Loss: 0.00141990
Iteration 7/25 | Loss: 0.00128761
Iteration 8/25 | Loss: 0.00125508
Iteration 9/25 | Loss: 0.00123116
Iteration 10/25 | Loss: 0.00124999
Iteration 11/25 | Loss: 0.00122594
Iteration 12/25 | Loss: 0.00121163
Iteration 13/25 | Loss: 0.00120559
Iteration 14/25 | Loss: 0.00120480
Iteration 15/25 | Loss: 0.00120467
Iteration 16/25 | Loss: 0.00120462
Iteration 17/25 | Loss: 0.00120462
Iteration 18/25 | Loss: 0.00120462
Iteration 19/25 | Loss: 0.00120462
Iteration 20/25 | Loss: 0.00120462
Iteration 21/25 | Loss: 0.00120461
Iteration 22/25 | Loss: 0.00120461
Iteration 23/25 | Loss: 0.00120461
Iteration 24/25 | Loss: 0.00120461
Iteration 25/25 | Loss: 0.00120461

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32143998
Iteration 2/25 | Loss: 0.00099631
Iteration 3/25 | Loss: 0.00099631
Iteration 4/25 | Loss: 0.00099631
Iteration 5/25 | Loss: 0.00099631
Iteration 6/25 | Loss: 0.00099631
Iteration 7/25 | Loss: 0.00099631
Iteration 8/25 | Loss: 0.00099630
Iteration 9/25 | Loss: 0.00099630
Iteration 10/25 | Loss: 0.00099630
Iteration 11/25 | Loss: 0.00099630
Iteration 12/25 | Loss: 0.00099630
Iteration 13/25 | Loss: 0.00099630
Iteration 14/25 | Loss: 0.00099630
Iteration 15/25 | Loss: 0.00099630
Iteration 16/25 | Loss: 0.00099630
Iteration 17/25 | Loss: 0.00099630
Iteration 18/25 | Loss: 0.00099630
Iteration 19/25 | Loss: 0.00099630
Iteration 20/25 | Loss: 0.00099630
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009963044431060553, 0.0009963044431060553, 0.0009963044431060553, 0.0009963044431060553, 0.0009963044431060553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009963044431060553

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099630
Iteration 2/1000 | Loss: 0.00007464
Iteration 3/1000 | Loss: 0.00379544
Iteration 4/1000 | Loss: 0.00014185
Iteration 5/1000 | Loss: 0.00253890
Iteration 6/1000 | Loss: 0.00043231
Iteration 7/1000 | Loss: 0.00004989
Iteration 8/1000 | Loss: 0.00182470
Iteration 9/1000 | Loss: 0.00004037
Iteration 10/1000 | Loss: 0.00004244
Iteration 11/1000 | Loss: 0.00003497
Iteration 12/1000 | Loss: 0.00003310
Iteration 13/1000 | Loss: 0.00002981
Iteration 14/1000 | Loss: 0.00004107
Iteration 15/1000 | Loss: 0.00002599
Iteration 16/1000 | Loss: 0.00003122
Iteration 17/1000 | Loss: 0.00002443
Iteration 18/1000 | Loss: 0.00002412
Iteration 19/1000 | Loss: 0.00002387
Iteration 20/1000 | Loss: 0.00002369
Iteration 21/1000 | Loss: 0.00002350
Iteration 22/1000 | Loss: 0.00002348
Iteration 23/1000 | Loss: 0.00002337
Iteration 24/1000 | Loss: 0.00002337
Iteration 25/1000 | Loss: 0.00002336
Iteration 26/1000 | Loss: 0.00002336
Iteration 27/1000 | Loss: 0.00002335
Iteration 28/1000 | Loss: 0.00002335
Iteration 29/1000 | Loss: 0.00002335
Iteration 30/1000 | Loss: 0.00002334
Iteration 31/1000 | Loss: 0.00002331
Iteration 32/1000 | Loss: 0.00002331
Iteration 33/1000 | Loss: 0.00002329
Iteration 34/1000 | Loss: 0.00002329
Iteration 35/1000 | Loss: 0.00002328
Iteration 36/1000 | Loss: 0.00002327
Iteration 37/1000 | Loss: 0.00002326
Iteration 38/1000 | Loss: 0.00002326
Iteration 39/1000 | Loss: 0.00002325
Iteration 40/1000 | Loss: 0.00002325
Iteration 41/1000 | Loss: 0.00002324
Iteration 42/1000 | Loss: 0.00002324
Iteration 43/1000 | Loss: 0.00002323
Iteration 44/1000 | Loss: 0.00002320
Iteration 45/1000 | Loss: 0.00002320
Iteration 46/1000 | Loss: 0.00002320
Iteration 47/1000 | Loss: 0.00002319
Iteration 48/1000 | Loss: 0.00002319
Iteration 49/1000 | Loss: 0.00002318
Iteration 50/1000 | Loss: 0.00002318
Iteration 51/1000 | Loss: 0.00002317
Iteration 52/1000 | Loss: 0.00002316
Iteration 53/1000 | Loss: 0.00002315
Iteration 54/1000 | Loss: 0.00002315
Iteration 55/1000 | Loss: 0.00002314
Iteration 56/1000 | Loss: 0.00002313
Iteration 57/1000 | Loss: 0.00002313
Iteration 58/1000 | Loss: 0.00002312
Iteration 59/1000 | Loss: 0.00002311
Iteration 60/1000 | Loss: 0.00002311
Iteration 61/1000 | Loss: 0.00002308
Iteration 62/1000 | Loss: 0.00002307
Iteration 63/1000 | Loss: 0.00002307
Iteration 64/1000 | Loss: 0.00002306
Iteration 65/1000 | Loss: 0.00002306
Iteration 66/1000 | Loss: 0.00002305
Iteration 67/1000 | Loss: 0.00002305
Iteration 68/1000 | Loss: 0.00002305
Iteration 69/1000 | Loss: 0.00002304
Iteration 70/1000 | Loss: 0.00002304
Iteration 71/1000 | Loss: 0.00002303
Iteration 72/1000 | Loss: 0.00002302
Iteration 73/1000 | Loss: 0.00002302
Iteration 74/1000 | Loss: 0.00002301
Iteration 75/1000 | Loss: 0.00002301
Iteration 76/1000 | Loss: 0.00002301
Iteration 77/1000 | Loss: 0.00002301
Iteration 78/1000 | Loss: 0.00002300
Iteration 79/1000 | Loss: 0.00002300
Iteration 80/1000 | Loss: 0.00002300
Iteration 81/1000 | Loss: 0.00002300
Iteration 82/1000 | Loss: 0.00002299
Iteration 83/1000 | Loss: 0.00002299
Iteration 84/1000 | Loss: 0.00002299
Iteration 85/1000 | Loss: 0.00002299
Iteration 86/1000 | Loss: 0.00002298
Iteration 87/1000 | Loss: 0.00002298
Iteration 88/1000 | Loss: 0.00002298
Iteration 89/1000 | Loss: 0.00002298
Iteration 90/1000 | Loss: 0.00002297
Iteration 91/1000 | Loss: 0.00002297
Iteration 92/1000 | Loss: 0.00002297
Iteration 93/1000 | Loss: 0.00002297
Iteration 94/1000 | Loss: 0.00002297
Iteration 95/1000 | Loss: 0.00002297
Iteration 96/1000 | Loss: 0.00002296
Iteration 97/1000 | Loss: 0.00002296
Iteration 98/1000 | Loss: 0.00002296
Iteration 99/1000 | Loss: 0.00002296
Iteration 100/1000 | Loss: 0.00002295
Iteration 101/1000 | Loss: 0.00002295
Iteration 102/1000 | Loss: 0.00002295
Iteration 103/1000 | Loss: 0.00002295
Iteration 104/1000 | Loss: 0.00002295
Iteration 105/1000 | Loss: 0.00002295
Iteration 106/1000 | Loss: 0.00002294
Iteration 107/1000 | Loss: 0.00002294
Iteration 108/1000 | Loss: 0.00002294
Iteration 109/1000 | Loss: 0.00002293
Iteration 110/1000 | Loss: 0.00002293
Iteration 111/1000 | Loss: 0.00002292
Iteration 112/1000 | Loss: 0.00002291
Iteration 113/1000 | Loss: 0.00002291
Iteration 114/1000 | Loss: 0.00002291
Iteration 115/1000 | Loss: 0.00002291
Iteration 116/1000 | Loss: 0.00002290
Iteration 117/1000 | Loss: 0.00002290
Iteration 118/1000 | Loss: 0.00002290
Iteration 119/1000 | Loss: 0.00002290
Iteration 120/1000 | Loss: 0.00002290
Iteration 121/1000 | Loss: 0.00002290
Iteration 122/1000 | Loss: 0.00002289
Iteration 123/1000 | Loss: 0.00002289
Iteration 124/1000 | Loss: 0.00002289
Iteration 125/1000 | Loss: 0.00002289
Iteration 126/1000 | Loss: 0.00002288
Iteration 127/1000 | Loss: 0.00002288
Iteration 128/1000 | Loss: 0.00002288
Iteration 129/1000 | Loss: 0.00002287
Iteration 130/1000 | Loss: 0.00002287
Iteration 131/1000 | Loss: 0.00002287
Iteration 132/1000 | Loss: 0.00002287
Iteration 133/1000 | Loss: 0.00002286
Iteration 134/1000 | Loss: 0.00002286
Iteration 135/1000 | Loss: 0.00002286
Iteration 136/1000 | Loss: 0.00002285
Iteration 137/1000 | Loss: 0.00002285
Iteration 138/1000 | Loss: 0.00002285
Iteration 139/1000 | Loss: 0.00002285
Iteration 140/1000 | Loss: 0.00002284
Iteration 141/1000 | Loss: 0.00002284
Iteration 142/1000 | Loss: 0.00002284
Iteration 143/1000 | Loss: 0.00002284
Iteration 144/1000 | Loss: 0.00002283
Iteration 145/1000 | Loss: 0.00002283
Iteration 146/1000 | Loss: 0.00002283
Iteration 147/1000 | Loss: 0.00002283
Iteration 148/1000 | Loss: 0.00002283
Iteration 149/1000 | Loss: 0.00002282
Iteration 150/1000 | Loss: 0.00002282
Iteration 151/1000 | Loss: 0.00002282
Iteration 152/1000 | Loss: 0.00002282
Iteration 153/1000 | Loss: 0.00002281
Iteration 154/1000 | Loss: 0.00002281
Iteration 155/1000 | Loss: 0.00002281
Iteration 156/1000 | Loss: 0.00002280
Iteration 157/1000 | Loss: 0.00002280
Iteration 158/1000 | Loss: 0.00002280
Iteration 159/1000 | Loss: 0.00002280
Iteration 160/1000 | Loss: 0.00002280
Iteration 161/1000 | Loss: 0.00002280
Iteration 162/1000 | Loss: 0.00002279
Iteration 163/1000 | Loss: 0.00002279
Iteration 164/1000 | Loss: 0.00002279
Iteration 165/1000 | Loss: 0.00002279
Iteration 166/1000 | Loss: 0.00002279
Iteration 167/1000 | Loss: 0.00002279
Iteration 168/1000 | Loss: 0.00002279
Iteration 169/1000 | Loss: 0.00002279
Iteration 170/1000 | Loss: 0.00002279
Iteration 171/1000 | Loss: 0.00002279
Iteration 172/1000 | Loss: 0.00002279
Iteration 173/1000 | Loss: 0.00002279
Iteration 174/1000 | Loss: 0.00002279
Iteration 175/1000 | Loss: 0.00002279
Iteration 176/1000 | Loss: 0.00002279
Iteration 177/1000 | Loss: 0.00002278
Iteration 178/1000 | Loss: 0.00002278
Iteration 179/1000 | Loss: 0.00002278
Iteration 180/1000 | Loss: 0.00002278
Iteration 181/1000 | Loss: 0.00002278
Iteration 182/1000 | Loss: 0.00002278
Iteration 183/1000 | Loss: 0.00002278
Iteration 184/1000 | Loss: 0.00002278
Iteration 185/1000 | Loss: 0.00002278
Iteration 186/1000 | Loss: 0.00002278
Iteration 187/1000 | Loss: 0.00002278
Iteration 188/1000 | Loss: 0.00002278
Iteration 189/1000 | Loss: 0.00002277
Iteration 190/1000 | Loss: 0.00002277
Iteration 191/1000 | Loss: 0.00002277
Iteration 192/1000 | Loss: 0.00002277
Iteration 193/1000 | Loss: 0.00002277
Iteration 194/1000 | Loss: 0.00002277
Iteration 195/1000 | Loss: 0.00002276
Iteration 196/1000 | Loss: 0.00002276
Iteration 197/1000 | Loss: 0.00002276
Iteration 198/1000 | Loss: 0.00002276
Iteration 199/1000 | Loss: 0.00002276
Iteration 200/1000 | Loss: 0.00002276
Iteration 201/1000 | Loss: 0.00002276
Iteration 202/1000 | Loss: 0.00002276
Iteration 203/1000 | Loss: 0.00002276
Iteration 204/1000 | Loss: 0.00002276
Iteration 205/1000 | Loss: 0.00002276
Iteration 206/1000 | Loss: 0.00002276
Iteration 207/1000 | Loss: 0.00002276
Iteration 208/1000 | Loss: 0.00002276
Iteration 209/1000 | Loss: 0.00002276
Iteration 210/1000 | Loss: 0.00002276
Iteration 211/1000 | Loss: 0.00002276
Iteration 212/1000 | Loss: 0.00002276
Iteration 213/1000 | Loss: 0.00002276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [2.2760610590921715e-05, 2.2760610590921715e-05, 2.2760610590921715e-05, 2.2760610590921715e-05, 2.2760610590921715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2760610590921715e-05

Optimization complete. Final v2v error: 4.099912166595459 mm

Highest mean error: 4.606681823730469 mm for frame 158

Lowest mean error: 3.6888585090637207 mm for frame 148

Saving results

Total time: 85.49486637115479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403854
Iteration 2/25 | Loss: 0.00126588
Iteration 3/25 | Loss: 0.00111552
Iteration 4/25 | Loss: 0.00109666
Iteration 5/25 | Loss: 0.00109098
Iteration 6/25 | Loss: 0.00108916
Iteration 7/25 | Loss: 0.00108897
Iteration 8/25 | Loss: 0.00108897
Iteration 9/25 | Loss: 0.00108897
Iteration 10/25 | Loss: 0.00108897
Iteration 11/25 | Loss: 0.00108897
Iteration 12/25 | Loss: 0.00108897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010889688273891807, 0.0010889688273891807, 0.0010889688273891807, 0.0010889688273891807, 0.0010889688273891807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010889688273891807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32537293
Iteration 2/25 | Loss: 0.00094197
Iteration 3/25 | Loss: 0.00094194
Iteration 4/25 | Loss: 0.00094194
Iteration 5/25 | Loss: 0.00094194
Iteration 6/25 | Loss: 0.00094194
Iteration 7/25 | Loss: 0.00094194
Iteration 8/25 | Loss: 0.00094194
Iteration 9/25 | Loss: 0.00094194
Iteration 10/25 | Loss: 0.00094194
Iteration 11/25 | Loss: 0.00094194
Iteration 12/25 | Loss: 0.00094194
Iteration 13/25 | Loss: 0.00094194
Iteration 14/25 | Loss: 0.00094194
Iteration 15/25 | Loss: 0.00094194
Iteration 16/25 | Loss: 0.00094194
Iteration 17/25 | Loss: 0.00094194
Iteration 18/25 | Loss: 0.00094194
Iteration 19/25 | Loss: 0.00094194
Iteration 20/25 | Loss: 0.00094194
Iteration 21/25 | Loss: 0.00094194
Iteration 22/25 | Loss: 0.00094194
Iteration 23/25 | Loss: 0.00094194
Iteration 24/25 | Loss: 0.00094194
Iteration 25/25 | Loss: 0.00094194

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094194
Iteration 2/1000 | Loss: 0.00005232
Iteration 3/1000 | Loss: 0.00003298
Iteration 4/1000 | Loss: 0.00002526
Iteration 5/1000 | Loss: 0.00002041
Iteration 6/1000 | Loss: 0.00001886
Iteration 7/1000 | Loss: 0.00001751
Iteration 8/1000 | Loss: 0.00001684
Iteration 9/1000 | Loss: 0.00001636
Iteration 10/1000 | Loss: 0.00001589
Iteration 11/1000 | Loss: 0.00001558
Iteration 12/1000 | Loss: 0.00001538
Iteration 13/1000 | Loss: 0.00001516
Iteration 14/1000 | Loss: 0.00001506
Iteration 15/1000 | Loss: 0.00001505
Iteration 16/1000 | Loss: 0.00001504
Iteration 17/1000 | Loss: 0.00001504
Iteration 18/1000 | Loss: 0.00001502
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001500
Iteration 21/1000 | Loss: 0.00001498
Iteration 22/1000 | Loss: 0.00001498
Iteration 23/1000 | Loss: 0.00001498
Iteration 24/1000 | Loss: 0.00001496
Iteration 25/1000 | Loss: 0.00001494
Iteration 26/1000 | Loss: 0.00001493
Iteration 27/1000 | Loss: 0.00001493
Iteration 28/1000 | Loss: 0.00001492
Iteration 29/1000 | Loss: 0.00001492
Iteration 30/1000 | Loss: 0.00001492
Iteration 31/1000 | Loss: 0.00001492
Iteration 32/1000 | Loss: 0.00001491
Iteration 33/1000 | Loss: 0.00001491
Iteration 34/1000 | Loss: 0.00001491
Iteration 35/1000 | Loss: 0.00001491
Iteration 36/1000 | Loss: 0.00001491
Iteration 37/1000 | Loss: 0.00001490
Iteration 38/1000 | Loss: 0.00001490
Iteration 39/1000 | Loss: 0.00001490
Iteration 40/1000 | Loss: 0.00001490
Iteration 41/1000 | Loss: 0.00001489
Iteration 42/1000 | Loss: 0.00001489
Iteration 43/1000 | Loss: 0.00001489
Iteration 44/1000 | Loss: 0.00001487
Iteration 45/1000 | Loss: 0.00001486
Iteration 46/1000 | Loss: 0.00001486
Iteration 47/1000 | Loss: 0.00001485
Iteration 48/1000 | Loss: 0.00001485
Iteration 49/1000 | Loss: 0.00001484
Iteration 50/1000 | Loss: 0.00001483
Iteration 51/1000 | Loss: 0.00001483
Iteration 52/1000 | Loss: 0.00001483
Iteration 53/1000 | Loss: 0.00001482
Iteration 54/1000 | Loss: 0.00001482
Iteration 55/1000 | Loss: 0.00001481
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001479
Iteration 58/1000 | Loss: 0.00001478
Iteration 59/1000 | Loss: 0.00001477
Iteration 60/1000 | Loss: 0.00001477
Iteration 61/1000 | Loss: 0.00001474
Iteration 62/1000 | Loss: 0.00001471
Iteration 63/1000 | Loss: 0.00001471
Iteration 64/1000 | Loss: 0.00001470
Iteration 65/1000 | Loss: 0.00001470
Iteration 66/1000 | Loss: 0.00001470
Iteration 67/1000 | Loss: 0.00001470
Iteration 68/1000 | Loss: 0.00001469
Iteration 69/1000 | Loss: 0.00001469
Iteration 70/1000 | Loss: 0.00001469
Iteration 71/1000 | Loss: 0.00001469
Iteration 72/1000 | Loss: 0.00001468
Iteration 73/1000 | Loss: 0.00001468
Iteration 74/1000 | Loss: 0.00001468
Iteration 75/1000 | Loss: 0.00001468
Iteration 76/1000 | Loss: 0.00001468
Iteration 77/1000 | Loss: 0.00001467
Iteration 78/1000 | Loss: 0.00001466
Iteration 79/1000 | Loss: 0.00001466
Iteration 80/1000 | Loss: 0.00001466
Iteration 81/1000 | Loss: 0.00001465
Iteration 82/1000 | Loss: 0.00001465
Iteration 83/1000 | Loss: 0.00001464
Iteration 84/1000 | Loss: 0.00001464
Iteration 85/1000 | Loss: 0.00001463
Iteration 86/1000 | Loss: 0.00001463
Iteration 87/1000 | Loss: 0.00001463
Iteration 88/1000 | Loss: 0.00001462
Iteration 89/1000 | Loss: 0.00001462
Iteration 90/1000 | Loss: 0.00001462
Iteration 91/1000 | Loss: 0.00001461
Iteration 92/1000 | Loss: 0.00001461
Iteration 93/1000 | Loss: 0.00001461
Iteration 94/1000 | Loss: 0.00001461
Iteration 95/1000 | Loss: 0.00001460
Iteration 96/1000 | Loss: 0.00001460
Iteration 97/1000 | Loss: 0.00001460
Iteration 98/1000 | Loss: 0.00001460
Iteration 99/1000 | Loss: 0.00001460
Iteration 100/1000 | Loss: 0.00001460
Iteration 101/1000 | Loss: 0.00001460
Iteration 102/1000 | Loss: 0.00001459
Iteration 103/1000 | Loss: 0.00001459
Iteration 104/1000 | Loss: 0.00001459
Iteration 105/1000 | Loss: 0.00001459
Iteration 106/1000 | Loss: 0.00001459
Iteration 107/1000 | Loss: 0.00001458
Iteration 108/1000 | Loss: 0.00001458
Iteration 109/1000 | Loss: 0.00001458
Iteration 110/1000 | Loss: 0.00001458
Iteration 111/1000 | Loss: 0.00001457
Iteration 112/1000 | Loss: 0.00001457
Iteration 113/1000 | Loss: 0.00001457
Iteration 114/1000 | Loss: 0.00001456
Iteration 115/1000 | Loss: 0.00001456
Iteration 116/1000 | Loss: 0.00001455
Iteration 117/1000 | Loss: 0.00001455
Iteration 118/1000 | Loss: 0.00001455
Iteration 119/1000 | Loss: 0.00001455
Iteration 120/1000 | Loss: 0.00001455
Iteration 121/1000 | Loss: 0.00001455
Iteration 122/1000 | Loss: 0.00001454
Iteration 123/1000 | Loss: 0.00001454
Iteration 124/1000 | Loss: 0.00001454
Iteration 125/1000 | Loss: 0.00001454
Iteration 126/1000 | Loss: 0.00001454
Iteration 127/1000 | Loss: 0.00001454
Iteration 128/1000 | Loss: 0.00001454
Iteration 129/1000 | Loss: 0.00001454
Iteration 130/1000 | Loss: 0.00001454
Iteration 131/1000 | Loss: 0.00001453
Iteration 132/1000 | Loss: 0.00001453
Iteration 133/1000 | Loss: 0.00001453
Iteration 134/1000 | Loss: 0.00001453
Iteration 135/1000 | Loss: 0.00001452
Iteration 136/1000 | Loss: 0.00001452
Iteration 137/1000 | Loss: 0.00001452
Iteration 138/1000 | Loss: 0.00001452
Iteration 139/1000 | Loss: 0.00001452
Iteration 140/1000 | Loss: 0.00001452
Iteration 141/1000 | Loss: 0.00001452
Iteration 142/1000 | Loss: 0.00001452
Iteration 143/1000 | Loss: 0.00001452
Iteration 144/1000 | Loss: 0.00001451
Iteration 145/1000 | Loss: 0.00001451
Iteration 146/1000 | Loss: 0.00001451
Iteration 147/1000 | Loss: 0.00001451
Iteration 148/1000 | Loss: 0.00001450
Iteration 149/1000 | Loss: 0.00001450
Iteration 150/1000 | Loss: 0.00001450
Iteration 151/1000 | Loss: 0.00001450
Iteration 152/1000 | Loss: 0.00001450
Iteration 153/1000 | Loss: 0.00001450
Iteration 154/1000 | Loss: 0.00001450
Iteration 155/1000 | Loss: 0.00001450
Iteration 156/1000 | Loss: 0.00001450
Iteration 157/1000 | Loss: 0.00001449
Iteration 158/1000 | Loss: 0.00001449
Iteration 159/1000 | Loss: 0.00001449
Iteration 160/1000 | Loss: 0.00001449
Iteration 161/1000 | Loss: 0.00001449
Iteration 162/1000 | Loss: 0.00001448
Iteration 163/1000 | Loss: 0.00001448
Iteration 164/1000 | Loss: 0.00001448
Iteration 165/1000 | Loss: 0.00001448
Iteration 166/1000 | Loss: 0.00001448
Iteration 167/1000 | Loss: 0.00001448
Iteration 168/1000 | Loss: 0.00001447
Iteration 169/1000 | Loss: 0.00001447
Iteration 170/1000 | Loss: 0.00001447
Iteration 171/1000 | Loss: 0.00001447
Iteration 172/1000 | Loss: 0.00001447
Iteration 173/1000 | Loss: 0.00001447
Iteration 174/1000 | Loss: 0.00001446
Iteration 175/1000 | Loss: 0.00001446
Iteration 176/1000 | Loss: 0.00001446
Iteration 177/1000 | Loss: 0.00001446
Iteration 178/1000 | Loss: 0.00001446
Iteration 179/1000 | Loss: 0.00001446
Iteration 180/1000 | Loss: 0.00001446
Iteration 181/1000 | Loss: 0.00001446
Iteration 182/1000 | Loss: 0.00001446
Iteration 183/1000 | Loss: 0.00001446
Iteration 184/1000 | Loss: 0.00001445
Iteration 185/1000 | Loss: 0.00001445
Iteration 186/1000 | Loss: 0.00001445
Iteration 187/1000 | Loss: 0.00001444
Iteration 188/1000 | Loss: 0.00001444
Iteration 189/1000 | Loss: 0.00001444
Iteration 190/1000 | Loss: 0.00001444
Iteration 191/1000 | Loss: 0.00001444
Iteration 192/1000 | Loss: 0.00001444
Iteration 193/1000 | Loss: 0.00001444
Iteration 194/1000 | Loss: 0.00001443
Iteration 195/1000 | Loss: 0.00001443
Iteration 196/1000 | Loss: 0.00001443
Iteration 197/1000 | Loss: 0.00001443
Iteration 198/1000 | Loss: 0.00001443
Iteration 199/1000 | Loss: 0.00001443
Iteration 200/1000 | Loss: 0.00001443
Iteration 201/1000 | Loss: 0.00001442
Iteration 202/1000 | Loss: 0.00001442
Iteration 203/1000 | Loss: 0.00001442
Iteration 204/1000 | Loss: 0.00001442
Iteration 205/1000 | Loss: 0.00001442
Iteration 206/1000 | Loss: 0.00001442
Iteration 207/1000 | Loss: 0.00001442
Iteration 208/1000 | Loss: 0.00001442
Iteration 209/1000 | Loss: 0.00001442
Iteration 210/1000 | Loss: 0.00001441
Iteration 211/1000 | Loss: 0.00001441
Iteration 212/1000 | Loss: 0.00001441
Iteration 213/1000 | Loss: 0.00001441
Iteration 214/1000 | Loss: 0.00001441
Iteration 215/1000 | Loss: 0.00001441
Iteration 216/1000 | Loss: 0.00001441
Iteration 217/1000 | Loss: 0.00001441
Iteration 218/1000 | Loss: 0.00001441
Iteration 219/1000 | Loss: 0.00001441
Iteration 220/1000 | Loss: 0.00001440
Iteration 221/1000 | Loss: 0.00001440
Iteration 222/1000 | Loss: 0.00001440
Iteration 223/1000 | Loss: 0.00001440
Iteration 224/1000 | Loss: 0.00001440
Iteration 225/1000 | Loss: 0.00001440
Iteration 226/1000 | Loss: 0.00001440
Iteration 227/1000 | Loss: 0.00001440
Iteration 228/1000 | Loss: 0.00001440
Iteration 229/1000 | Loss: 0.00001440
Iteration 230/1000 | Loss: 0.00001440
Iteration 231/1000 | Loss: 0.00001439
Iteration 232/1000 | Loss: 0.00001439
Iteration 233/1000 | Loss: 0.00001439
Iteration 234/1000 | Loss: 0.00001439
Iteration 235/1000 | Loss: 0.00001439
Iteration 236/1000 | Loss: 0.00001438
Iteration 237/1000 | Loss: 0.00001438
Iteration 238/1000 | Loss: 0.00001438
Iteration 239/1000 | Loss: 0.00001438
Iteration 240/1000 | Loss: 0.00001438
Iteration 241/1000 | Loss: 0.00001438
Iteration 242/1000 | Loss: 0.00001438
Iteration 243/1000 | Loss: 0.00001438
Iteration 244/1000 | Loss: 0.00001438
Iteration 245/1000 | Loss: 0.00001438
Iteration 246/1000 | Loss: 0.00001438
Iteration 247/1000 | Loss: 0.00001438
Iteration 248/1000 | Loss: 0.00001438
Iteration 249/1000 | Loss: 0.00001438
Iteration 250/1000 | Loss: 0.00001438
Iteration 251/1000 | Loss: 0.00001438
Iteration 252/1000 | Loss: 0.00001438
Iteration 253/1000 | Loss: 0.00001438
Iteration 254/1000 | Loss: 0.00001438
Iteration 255/1000 | Loss: 0.00001438
Iteration 256/1000 | Loss: 0.00001438
Iteration 257/1000 | Loss: 0.00001438
Iteration 258/1000 | Loss: 0.00001438
Iteration 259/1000 | Loss: 0.00001438
Iteration 260/1000 | Loss: 0.00001438
Iteration 261/1000 | Loss: 0.00001438
Iteration 262/1000 | Loss: 0.00001438
Iteration 263/1000 | Loss: 0.00001438
Iteration 264/1000 | Loss: 0.00001438
Iteration 265/1000 | Loss: 0.00001438
Iteration 266/1000 | Loss: 0.00001438
Iteration 267/1000 | Loss: 0.00001438
Iteration 268/1000 | Loss: 0.00001438
Iteration 269/1000 | Loss: 0.00001438
Iteration 270/1000 | Loss: 0.00001438
Iteration 271/1000 | Loss: 0.00001438
Iteration 272/1000 | Loss: 0.00001438
Iteration 273/1000 | Loss: 0.00001438
Iteration 274/1000 | Loss: 0.00001438
Iteration 275/1000 | Loss: 0.00001438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.4376345461641904e-05, 1.4376345461641904e-05, 1.4376345461641904e-05, 1.4376345461641904e-05, 1.4376345461641904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4376345461641904e-05

Optimization complete. Final v2v error: 3.0284645557403564 mm

Highest mean error: 5.087029933929443 mm for frame 94

Lowest mean error: 2.388176918029785 mm for frame 35

Saving results

Total time: 48.55915355682373
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461117
Iteration 2/25 | Loss: 0.00122220
Iteration 3/25 | Loss: 0.00113583
Iteration 4/25 | Loss: 0.00112048
Iteration 5/25 | Loss: 0.00111505
Iteration 6/25 | Loss: 0.00111411
Iteration 7/25 | Loss: 0.00111411
Iteration 8/25 | Loss: 0.00111411
Iteration 9/25 | Loss: 0.00111411
Iteration 10/25 | Loss: 0.00111411
Iteration 11/25 | Loss: 0.00111411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001114106853492558, 0.001114106853492558, 0.001114106853492558, 0.001114106853492558, 0.001114106853492558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001114106853492558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.78803444
Iteration 2/25 | Loss: 0.00093478
Iteration 3/25 | Loss: 0.00093477
Iteration 4/25 | Loss: 0.00093477
Iteration 5/25 | Loss: 0.00093477
Iteration 6/25 | Loss: 0.00093477
Iteration 7/25 | Loss: 0.00093477
Iteration 8/25 | Loss: 0.00093477
Iteration 9/25 | Loss: 0.00093477
Iteration 10/25 | Loss: 0.00093477
Iteration 11/25 | Loss: 0.00093477
Iteration 12/25 | Loss: 0.00093477
Iteration 13/25 | Loss: 0.00093477
Iteration 14/25 | Loss: 0.00093477
Iteration 15/25 | Loss: 0.00093477
Iteration 16/25 | Loss: 0.00093477
Iteration 17/25 | Loss: 0.00093477
Iteration 18/25 | Loss: 0.00093477
Iteration 19/25 | Loss: 0.00093477
Iteration 20/25 | Loss: 0.00093477
Iteration 21/25 | Loss: 0.00093477
Iteration 22/25 | Loss: 0.00093477
Iteration 23/25 | Loss: 0.00093477
Iteration 24/25 | Loss: 0.00093477
Iteration 25/25 | Loss: 0.00093477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093477
Iteration 2/1000 | Loss: 0.00002453
Iteration 3/1000 | Loss: 0.00002020
Iteration 4/1000 | Loss: 0.00001913
Iteration 5/1000 | Loss: 0.00001853
Iteration 6/1000 | Loss: 0.00001794
Iteration 7/1000 | Loss: 0.00001757
Iteration 8/1000 | Loss: 0.00001721
Iteration 9/1000 | Loss: 0.00001701
Iteration 10/1000 | Loss: 0.00001679
Iteration 11/1000 | Loss: 0.00001668
Iteration 12/1000 | Loss: 0.00001667
Iteration 13/1000 | Loss: 0.00001659
Iteration 14/1000 | Loss: 0.00001657
Iteration 15/1000 | Loss: 0.00001653
Iteration 16/1000 | Loss: 0.00001652
Iteration 17/1000 | Loss: 0.00001650
Iteration 18/1000 | Loss: 0.00001645
Iteration 19/1000 | Loss: 0.00001643
Iteration 20/1000 | Loss: 0.00001642
Iteration 21/1000 | Loss: 0.00001640
Iteration 22/1000 | Loss: 0.00001639
Iteration 23/1000 | Loss: 0.00001638
Iteration 24/1000 | Loss: 0.00001638
Iteration 25/1000 | Loss: 0.00001637
Iteration 26/1000 | Loss: 0.00001636
Iteration 27/1000 | Loss: 0.00001636
Iteration 28/1000 | Loss: 0.00001635
Iteration 29/1000 | Loss: 0.00001634
Iteration 30/1000 | Loss: 0.00001634
Iteration 31/1000 | Loss: 0.00001634
Iteration 32/1000 | Loss: 0.00001633
Iteration 33/1000 | Loss: 0.00001633
Iteration 34/1000 | Loss: 0.00001633
Iteration 35/1000 | Loss: 0.00001632
Iteration 36/1000 | Loss: 0.00001632
Iteration 37/1000 | Loss: 0.00001632
Iteration 38/1000 | Loss: 0.00001632
Iteration 39/1000 | Loss: 0.00001631
Iteration 40/1000 | Loss: 0.00001631
Iteration 41/1000 | Loss: 0.00001631
Iteration 42/1000 | Loss: 0.00001631
Iteration 43/1000 | Loss: 0.00001630
Iteration 44/1000 | Loss: 0.00001630
Iteration 45/1000 | Loss: 0.00001630
Iteration 46/1000 | Loss: 0.00001629
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001629
Iteration 49/1000 | Loss: 0.00001629
Iteration 50/1000 | Loss: 0.00001629
Iteration 51/1000 | Loss: 0.00001629
Iteration 52/1000 | Loss: 0.00001628
Iteration 53/1000 | Loss: 0.00001627
Iteration 54/1000 | Loss: 0.00001627
Iteration 55/1000 | Loss: 0.00001626
Iteration 56/1000 | Loss: 0.00001626
Iteration 57/1000 | Loss: 0.00001626
Iteration 58/1000 | Loss: 0.00001626
Iteration 59/1000 | Loss: 0.00001626
Iteration 60/1000 | Loss: 0.00001626
Iteration 61/1000 | Loss: 0.00001626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [1.6261948985629715e-05, 1.6261948985629715e-05, 1.6261948985629715e-05, 1.6261948985629715e-05, 1.6261948985629715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6261948985629715e-05

Optimization complete. Final v2v error: 3.416767120361328 mm

Highest mean error: 3.983806848526001 mm for frame 228

Lowest mean error: 3.040693521499634 mm for frame 251

Saving results

Total time: 33.87817978858948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389674
Iteration 2/25 | Loss: 0.00111134
Iteration 3/25 | Loss: 0.00105237
Iteration 4/25 | Loss: 0.00104264
Iteration 5/25 | Loss: 0.00103973
Iteration 6/25 | Loss: 0.00103955
Iteration 7/25 | Loss: 0.00103955
Iteration 8/25 | Loss: 0.00103955
Iteration 9/25 | Loss: 0.00103955
Iteration 10/25 | Loss: 0.00103955
Iteration 11/25 | Loss: 0.00103955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001039551105350256, 0.001039551105350256, 0.001039551105350256, 0.001039551105350256, 0.001039551105350256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001039551105350256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27690363
Iteration 2/25 | Loss: 0.00082157
Iteration 3/25 | Loss: 0.00082157
Iteration 4/25 | Loss: 0.00082157
Iteration 5/25 | Loss: 0.00082157
Iteration 6/25 | Loss: 0.00082157
Iteration 7/25 | Loss: 0.00082156
Iteration 8/25 | Loss: 0.00082156
Iteration 9/25 | Loss: 0.00082156
Iteration 10/25 | Loss: 0.00082156
Iteration 11/25 | Loss: 0.00082156
Iteration 12/25 | Loss: 0.00082156
Iteration 13/25 | Loss: 0.00082156
Iteration 14/25 | Loss: 0.00082156
Iteration 15/25 | Loss: 0.00082156
Iteration 16/25 | Loss: 0.00082156
Iteration 17/25 | Loss: 0.00082156
Iteration 18/25 | Loss: 0.00082156
Iteration 19/25 | Loss: 0.00082156
Iteration 20/25 | Loss: 0.00082156
Iteration 21/25 | Loss: 0.00082156
Iteration 22/25 | Loss: 0.00082156
Iteration 23/25 | Loss: 0.00082156
Iteration 24/25 | Loss: 0.00082156
Iteration 25/25 | Loss: 0.00082156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082156
Iteration 2/1000 | Loss: 0.00001546
Iteration 3/1000 | Loss: 0.00001166
Iteration 4/1000 | Loss: 0.00001074
Iteration 5/1000 | Loss: 0.00001011
Iteration 6/1000 | Loss: 0.00000966
Iteration 7/1000 | Loss: 0.00000955
Iteration 8/1000 | Loss: 0.00000944
Iteration 9/1000 | Loss: 0.00000908
Iteration 10/1000 | Loss: 0.00000893
Iteration 11/1000 | Loss: 0.00000880
Iteration 12/1000 | Loss: 0.00000875
Iteration 13/1000 | Loss: 0.00000871
Iteration 14/1000 | Loss: 0.00000870
Iteration 15/1000 | Loss: 0.00000866
Iteration 16/1000 | Loss: 0.00000866
Iteration 17/1000 | Loss: 0.00000865
Iteration 18/1000 | Loss: 0.00000865
Iteration 19/1000 | Loss: 0.00000864
Iteration 20/1000 | Loss: 0.00000857
Iteration 21/1000 | Loss: 0.00000856
Iteration 22/1000 | Loss: 0.00000855
Iteration 23/1000 | Loss: 0.00000854
Iteration 24/1000 | Loss: 0.00000854
Iteration 25/1000 | Loss: 0.00000853
Iteration 26/1000 | Loss: 0.00000853
Iteration 27/1000 | Loss: 0.00000853
Iteration 28/1000 | Loss: 0.00000850
Iteration 29/1000 | Loss: 0.00000848
Iteration 30/1000 | Loss: 0.00000847
Iteration 31/1000 | Loss: 0.00000844
Iteration 32/1000 | Loss: 0.00000842
Iteration 33/1000 | Loss: 0.00000842
Iteration 34/1000 | Loss: 0.00000842
Iteration 35/1000 | Loss: 0.00000839
Iteration 36/1000 | Loss: 0.00000838
Iteration 37/1000 | Loss: 0.00000838
Iteration 38/1000 | Loss: 0.00000835
Iteration 39/1000 | Loss: 0.00000834
Iteration 40/1000 | Loss: 0.00000833
Iteration 41/1000 | Loss: 0.00000833
Iteration 42/1000 | Loss: 0.00000833
Iteration 43/1000 | Loss: 0.00000832
Iteration 44/1000 | Loss: 0.00000832
Iteration 45/1000 | Loss: 0.00000832
Iteration 46/1000 | Loss: 0.00000831
Iteration 47/1000 | Loss: 0.00000830
Iteration 48/1000 | Loss: 0.00000829
Iteration 49/1000 | Loss: 0.00000829
Iteration 50/1000 | Loss: 0.00000828
Iteration 51/1000 | Loss: 0.00000828
Iteration 52/1000 | Loss: 0.00000827
Iteration 53/1000 | Loss: 0.00000827
Iteration 54/1000 | Loss: 0.00000826
Iteration 55/1000 | Loss: 0.00000826
Iteration 56/1000 | Loss: 0.00000826
Iteration 57/1000 | Loss: 0.00000825
Iteration 58/1000 | Loss: 0.00000825
Iteration 59/1000 | Loss: 0.00000825
Iteration 60/1000 | Loss: 0.00000824
Iteration 61/1000 | Loss: 0.00000824
Iteration 62/1000 | Loss: 0.00000824
Iteration 63/1000 | Loss: 0.00000824
Iteration 64/1000 | Loss: 0.00000824
Iteration 65/1000 | Loss: 0.00000824
Iteration 66/1000 | Loss: 0.00000824
Iteration 67/1000 | Loss: 0.00000824
Iteration 68/1000 | Loss: 0.00000823
Iteration 69/1000 | Loss: 0.00000823
Iteration 70/1000 | Loss: 0.00000823
Iteration 71/1000 | Loss: 0.00000823
Iteration 72/1000 | Loss: 0.00000823
Iteration 73/1000 | Loss: 0.00000823
Iteration 74/1000 | Loss: 0.00000823
Iteration 75/1000 | Loss: 0.00000823
Iteration 76/1000 | Loss: 0.00000823
Iteration 77/1000 | Loss: 0.00000822
Iteration 78/1000 | Loss: 0.00000822
Iteration 79/1000 | Loss: 0.00000821
Iteration 80/1000 | Loss: 0.00000821
Iteration 81/1000 | Loss: 0.00000820
Iteration 82/1000 | Loss: 0.00000820
Iteration 83/1000 | Loss: 0.00000820
Iteration 84/1000 | Loss: 0.00000820
Iteration 85/1000 | Loss: 0.00000820
Iteration 86/1000 | Loss: 0.00000820
Iteration 87/1000 | Loss: 0.00000819
Iteration 88/1000 | Loss: 0.00000819
Iteration 89/1000 | Loss: 0.00000819
Iteration 90/1000 | Loss: 0.00000819
Iteration 91/1000 | Loss: 0.00000819
Iteration 92/1000 | Loss: 0.00000818
Iteration 93/1000 | Loss: 0.00000818
Iteration 94/1000 | Loss: 0.00000817
Iteration 95/1000 | Loss: 0.00000817
Iteration 96/1000 | Loss: 0.00000817
Iteration 97/1000 | Loss: 0.00000817
Iteration 98/1000 | Loss: 0.00000817
Iteration 99/1000 | Loss: 0.00000817
Iteration 100/1000 | Loss: 0.00000817
Iteration 101/1000 | Loss: 0.00000817
Iteration 102/1000 | Loss: 0.00000816
Iteration 103/1000 | Loss: 0.00000816
Iteration 104/1000 | Loss: 0.00000816
Iteration 105/1000 | Loss: 0.00000816
Iteration 106/1000 | Loss: 0.00000816
Iteration 107/1000 | Loss: 0.00000816
Iteration 108/1000 | Loss: 0.00000815
Iteration 109/1000 | Loss: 0.00000814
Iteration 110/1000 | Loss: 0.00000814
Iteration 111/1000 | Loss: 0.00000814
Iteration 112/1000 | Loss: 0.00000814
Iteration 113/1000 | Loss: 0.00000814
Iteration 114/1000 | Loss: 0.00000814
Iteration 115/1000 | Loss: 0.00000814
Iteration 116/1000 | Loss: 0.00000813
Iteration 117/1000 | Loss: 0.00000813
Iteration 118/1000 | Loss: 0.00000813
Iteration 119/1000 | Loss: 0.00000813
Iteration 120/1000 | Loss: 0.00000813
Iteration 121/1000 | Loss: 0.00000812
Iteration 122/1000 | Loss: 0.00000812
Iteration 123/1000 | Loss: 0.00000812
Iteration 124/1000 | Loss: 0.00000812
Iteration 125/1000 | Loss: 0.00000811
Iteration 126/1000 | Loss: 0.00000811
Iteration 127/1000 | Loss: 0.00000811
Iteration 128/1000 | Loss: 0.00000811
Iteration 129/1000 | Loss: 0.00000811
Iteration 130/1000 | Loss: 0.00000811
Iteration 131/1000 | Loss: 0.00000810
Iteration 132/1000 | Loss: 0.00000810
Iteration 133/1000 | Loss: 0.00000810
Iteration 134/1000 | Loss: 0.00000810
Iteration 135/1000 | Loss: 0.00000810
Iteration 136/1000 | Loss: 0.00000810
Iteration 137/1000 | Loss: 0.00000810
Iteration 138/1000 | Loss: 0.00000810
Iteration 139/1000 | Loss: 0.00000810
Iteration 140/1000 | Loss: 0.00000809
Iteration 141/1000 | Loss: 0.00000809
Iteration 142/1000 | Loss: 0.00000809
Iteration 143/1000 | Loss: 0.00000809
Iteration 144/1000 | Loss: 0.00000809
Iteration 145/1000 | Loss: 0.00000808
Iteration 146/1000 | Loss: 0.00000808
Iteration 147/1000 | Loss: 0.00000808
Iteration 148/1000 | Loss: 0.00000807
Iteration 149/1000 | Loss: 0.00000807
Iteration 150/1000 | Loss: 0.00000807
Iteration 151/1000 | Loss: 0.00000807
Iteration 152/1000 | Loss: 0.00000807
Iteration 153/1000 | Loss: 0.00000807
Iteration 154/1000 | Loss: 0.00000807
Iteration 155/1000 | Loss: 0.00000807
Iteration 156/1000 | Loss: 0.00000807
Iteration 157/1000 | Loss: 0.00000807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [8.071038791968022e-06, 8.071038791968022e-06, 8.071038791968022e-06, 8.071038791968022e-06, 8.071038791968022e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.071038791968022e-06

Optimization complete. Final v2v error: 2.487787961959839 mm

Highest mean error: 2.635871410369873 mm for frame 85

Lowest mean error: 2.3982198238372803 mm for frame 161

Saving results

Total time: 40.668633222579956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047681
Iteration 2/25 | Loss: 0.00138830
Iteration 3/25 | Loss: 0.00113197
Iteration 4/25 | Loss: 0.00111556
Iteration 5/25 | Loss: 0.00109968
Iteration 6/25 | Loss: 0.00108623
Iteration 7/25 | Loss: 0.00107917
Iteration 8/25 | Loss: 0.00107966
Iteration 9/25 | Loss: 0.00107939
Iteration 10/25 | Loss: 0.00107853
Iteration 11/25 | Loss: 0.00107830
Iteration 12/25 | Loss: 0.00107792
Iteration 13/25 | Loss: 0.00107762
Iteration 14/25 | Loss: 0.00107755
Iteration 15/25 | Loss: 0.00107720
Iteration 16/25 | Loss: 0.00107771
Iteration 17/25 | Loss: 0.00107898
Iteration 18/25 | Loss: 0.00107891
Iteration 19/25 | Loss: 0.00107937
Iteration 20/25 | Loss: 0.00107805
Iteration 21/25 | Loss: 0.00107737
Iteration 22/25 | Loss: 0.00107436
Iteration 23/25 | Loss: 0.00107449
Iteration 24/25 | Loss: 0.00107328
Iteration 25/25 | Loss: 0.00107290

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.28245091
Iteration 2/25 | Loss: 0.00090021
Iteration 3/25 | Loss: 0.00090020
Iteration 4/25 | Loss: 0.00090020
Iteration 5/25 | Loss: 0.00090020
Iteration 6/25 | Loss: 0.00090020
Iteration 7/25 | Loss: 0.00090020
Iteration 8/25 | Loss: 0.00090020
Iteration 9/25 | Loss: 0.00090020
Iteration 10/25 | Loss: 0.00090020
Iteration 11/25 | Loss: 0.00090020
Iteration 12/25 | Loss: 0.00090020
Iteration 13/25 | Loss: 0.00090020
Iteration 14/25 | Loss: 0.00090020
Iteration 15/25 | Loss: 0.00090020
Iteration 16/25 | Loss: 0.00090020
Iteration 17/25 | Loss: 0.00090020
Iteration 18/25 | Loss: 0.00090020
Iteration 19/25 | Loss: 0.00090020
Iteration 20/25 | Loss: 0.00090020
Iteration 21/25 | Loss: 0.00090020
Iteration 22/25 | Loss: 0.00090020
Iteration 23/25 | Loss: 0.00090020
Iteration 24/25 | Loss: 0.00090020
Iteration 25/25 | Loss: 0.00090020
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009002001606859267, 0.0009002001606859267, 0.0009002001606859267, 0.0009002001606859267, 0.0009002001606859267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009002001606859267

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090020
Iteration 2/1000 | Loss: 0.00003808
Iteration 3/1000 | Loss: 0.00008073
Iteration 4/1000 | Loss: 0.00008875
Iteration 5/1000 | Loss: 0.00003282
Iteration 6/1000 | Loss: 0.00007350
Iteration 7/1000 | Loss: 0.00010680
Iteration 8/1000 | Loss: 0.00013305
Iteration 9/1000 | Loss: 0.00006388
Iteration 10/1000 | Loss: 0.00007425
Iteration 11/1000 | Loss: 0.00009887
Iteration 12/1000 | Loss: 0.00009163
Iteration 13/1000 | Loss: 0.00006813
Iteration 14/1000 | Loss: 0.00003598
Iteration 15/1000 | Loss: 0.00005060
Iteration 16/1000 | Loss: 0.00005333
Iteration 17/1000 | Loss: 0.00003049
Iteration 18/1000 | Loss: 0.00002701
Iteration 19/1000 | Loss: 0.00001902
Iteration 20/1000 | Loss: 0.00001329
Iteration 21/1000 | Loss: 0.00014997
Iteration 22/1000 | Loss: 0.00009084
Iteration 23/1000 | Loss: 0.00007713
Iteration 24/1000 | Loss: 0.00009040
Iteration 25/1000 | Loss: 0.00008528
Iteration 26/1000 | Loss: 0.00009882
Iteration 27/1000 | Loss: 0.00009587
Iteration 28/1000 | Loss: 0.00009862
Iteration 29/1000 | Loss: 0.00011159
Iteration 30/1000 | Loss: 0.00008494
Iteration 31/1000 | Loss: 0.00009258
Iteration 32/1000 | Loss: 0.00013520
Iteration 33/1000 | Loss: 0.00003234
Iteration 34/1000 | Loss: 0.00009910
Iteration 35/1000 | Loss: 0.00014843
Iteration 36/1000 | Loss: 0.00009754
Iteration 37/1000 | Loss: 0.00010308
Iteration 38/1000 | Loss: 0.00014874
Iteration 39/1000 | Loss: 0.00010481
Iteration 40/1000 | Loss: 0.00004802
Iteration 41/1000 | Loss: 0.00013255
Iteration 42/1000 | Loss: 0.00010677
Iteration 43/1000 | Loss: 0.00010471
Iteration 44/1000 | Loss: 0.00009761
Iteration 45/1000 | Loss: 0.00014822
Iteration 46/1000 | Loss: 0.00002330
Iteration 47/1000 | Loss: 0.00001638
Iteration 48/1000 | Loss: 0.00001300
Iteration 49/1000 | Loss: 0.00001179
Iteration 50/1000 | Loss: 0.00001121
Iteration 51/1000 | Loss: 0.00001086
Iteration 52/1000 | Loss: 0.00001056
Iteration 53/1000 | Loss: 0.00001029
Iteration 54/1000 | Loss: 0.00001022
Iteration 55/1000 | Loss: 0.00001021
Iteration 56/1000 | Loss: 0.00001019
Iteration 57/1000 | Loss: 0.00001012
Iteration 58/1000 | Loss: 0.00000993
Iteration 59/1000 | Loss: 0.00000993
Iteration 60/1000 | Loss: 0.00000991
Iteration 61/1000 | Loss: 0.00000990
Iteration 62/1000 | Loss: 0.00000982
Iteration 63/1000 | Loss: 0.00000979
Iteration 64/1000 | Loss: 0.00000979
Iteration 65/1000 | Loss: 0.00000978
Iteration 66/1000 | Loss: 0.00000977
Iteration 67/1000 | Loss: 0.00000974
Iteration 68/1000 | Loss: 0.00000973
Iteration 69/1000 | Loss: 0.00000972
Iteration 70/1000 | Loss: 0.00000971
Iteration 71/1000 | Loss: 0.00000971
Iteration 72/1000 | Loss: 0.00000971
Iteration 73/1000 | Loss: 0.00000970
Iteration 74/1000 | Loss: 0.00000970
Iteration 75/1000 | Loss: 0.00000970
Iteration 76/1000 | Loss: 0.00000970
Iteration 77/1000 | Loss: 0.00000969
Iteration 78/1000 | Loss: 0.00000969
Iteration 79/1000 | Loss: 0.00000969
Iteration 80/1000 | Loss: 0.00000968
Iteration 81/1000 | Loss: 0.00000968
Iteration 82/1000 | Loss: 0.00000967
Iteration 83/1000 | Loss: 0.00000967
Iteration 84/1000 | Loss: 0.00000967
Iteration 85/1000 | Loss: 0.00000967
Iteration 86/1000 | Loss: 0.00000967
Iteration 87/1000 | Loss: 0.00000966
Iteration 88/1000 | Loss: 0.00000966
Iteration 89/1000 | Loss: 0.00000966
Iteration 90/1000 | Loss: 0.00000965
Iteration 91/1000 | Loss: 0.00000965
Iteration 92/1000 | Loss: 0.00000964
Iteration 93/1000 | Loss: 0.00000964
Iteration 94/1000 | Loss: 0.00000964
Iteration 95/1000 | Loss: 0.00000963
Iteration 96/1000 | Loss: 0.00000963
Iteration 97/1000 | Loss: 0.00000963
Iteration 98/1000 | Loss: 0.00000963
Iteration 99/1000 | Loss: 0.00000963
Iteration 100/1000 | Loss: 0.00000962
Iteration 101/1000 | Loss: 0.00000962
Iteration 102/1000 | Loss: 0.00000962
Iteration 103/1000 | Loss: 0.00000962
Iteration 104/1000 | Loss: 0.00000962
Iteration 105/1000 | Loss: 0.00000962
Iteration 106/1000 | Loss: 0.00000962
Iteration 107/1000 | Loss: 0.00000962
Iteration 108/1000 | Loss: 0.00000962
Iteration 109/1000 | Loss: 0.00000962
Iteration 110/1000 | Loss: 0.00000962
Iteration 111/1000 | Loss: 0.00000962
Iteration 112/1000 | Loss: 0.00000961
Iteration 113/1000 | Loss: 0.00000961
Iteration 114/1000 | Loss: 0.00000961
Iteration 115/1000 | Loss: 0.00000961
Iteration 116/1000 | Loss: 0.00000961
Iteration 117/1000 | Loss: 0.00000960
Iteration 118/1000 | Loss: 0.00000960
Iteration 119/1000 | Loss: 0.00000960
Iteration 120/1000 | Loss: 0.00000960
Iteration 121/1000 | Loss: 0.00000960
Iteration 122/1000 | Loss: 0.00000960
Iteration 123/1000 | Loss: 0.00000959
Iteration 124/1000 | Loss: 0.00000959
Iteration 125/1000 | Loss: 0.00000958
Iteration 126/1000 | Loss: 0.00000958
Iteration 127/1000 | Loss: 0.00000958
Iteration 128/1000 | Loss: 0.00000957
Iteration 129/1000 | Loss: 0.00000957
Iteration 130/1000 | Loss: 0.00000956
Iteration 131/1000 | Loss: 0.00000956
Iteration 132/1000 | Loss: 0.00000956
Iteration 133/1000 | Loss: 0.00000956
Iteration 134/1000 | Loss: 0.00000956
Iteration 135/1000 | Loss: 0.00000955
Iteration 136/1000 | Loss: 0.00000955
Iteration 137/1000 | Loss: 0.00000955
Iteration 138/1000 | Loss: 0.00000955
Iteration 139/1000 | Loss: 0.00000955
Iteration 140/1000 | Loss: 0.00000955
Iteration 141/1000 | Loss: 0.00000955
Iteration 142/1000 | Loss: 0.00000955
Iteration 143/1000 | Loss: 0.00000955
Iteration 144/1000 | Loss: 0.00000955
Iteration 145/1000 | Loss: 0.00000955
Iteration 146/1000 | Loss: 0.00000955
Iteration 147/1000 | Loss: 0.00000955
Iteration 148/1000 | Loss: 0.00000955
Iteration 149/1000 | Loss: 0.00000954
Iteration 150/1000 | Loss: 0.00000954
Iteration 151/1000 | Loss: 0.00000954
Iteration 152/1000 | Loss: 0.00000954
Iteration 153/1000 | Loss: 0.00000954
Iteration 154/1000 | Loss: 0.00000953
Iteration 155/1000 | Loss: 0.00000953
Iteration 156/1000 | Loss: 0.00000953
Iteration 157/1000 | Loss: 0.00000953
Iteration 158/1000 | Loss: 0.00000953
Iteration 159/1000 | Loss: 0.00000953
Iteration 160/1000 | Loss: 0.00000953
Iteration 161/1000 | Loss: 0.00000953
Iteration 162/1000 | Loss: 0.00000953
Iteration 163/1000 | Loss: 0.00000952
Iteration 164/1000 | Loss: 0.00000952
Iteration 165/1000 | Loss: 0.00000952
Iteration 166/1000 | Loss: 0.00000952
Iteration 167/1000 | Loss: 0.00000952
Iteration 168/1000 | Loss: 0.00000952
Iteration 169/1000 | Loss: 0.00000952
Iteration 170/1000 | Loss: 0.00000952
Iteration 171/1000 | Loss: 0.00000952
Iteration 172/1000 | Loss: 0.00000952
Iteration 173/1000 | Loss: 0.00000952
Iteration 174/1000 | Loss: 0.00000951
Iteration 175/1000 | Loss: 0.00000951
Iteration 176/1000 | Loss: 0.00000951
Iteration 177/1000 | Loss: 0.00000951
Iteration 178/1000 | Loss: 0.00000951
Iteration 179/1000 | Loss: 0.00000951
Iteration 180/1000 | Loss: 0.00000951
Iteration 181/1000 | Loss: 0.00000951
Iteration 182/1000 | Loss: 0.00000951
Iteration 183/1000 | Loss: 0.00000951
Iteration 184/1000 | Loss: 0.00000951
Iteration 185/1000 | Loss: 0.00000951
Iteration 186/1000 | Loss: 0.00000951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [9.512594260741025e-06, 9.512594260741025e-06, 9.512594260741025e-06, 9.512594260741025e-06, 9.512594260741025e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.512594260741025e-06

Optimization complete. Final v2v error: 2.631293535232544 mm

Highest mean error: 3.628868818283081 mm for frame 12

Lowest mean error: 2.4393763542175293 mm for frame 255

Saving results

Total time: 155.56191444396973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987957
Iteration 2/25 | Loss: 0.00258445
Iteration 3/25 | Loss: 0.00161343
Iteration 4/25 | Loss: 0.00147005
Iteration 5/25 | Loss: 0.00159626
Iteration 6/25 | Loss: 0.00152596
Iteration 7/25 | Loss: 0.00126286
Iteration 8/25 | Loss: 0.00120753
Iteration 9/25 | Loss: 0.00118647
Iteration 10/25 | Loss: 0.00114052
Iteration 11/25 | Loss: 0.00114076
Iteration 12/25 | Loss: 0.00113429
Iteration 13/25 | Loss: 0.00113625
Iteration 14/25 | Loss: 0.00112342
Iteration 15/25 | Loss: 0.00113048
Iteration 16/25 | Loss: 0.00114344
Iteration 17/25 | Loss: 0.00113809
Iteration 18/25 | Loss: 0.00112217
Iteration 19/25 | Loss: 0.00111705
Iteration 20/25 | Loss: 0.00111246
Iteration 21/25 | Loss: 0.00110828
Iteration 22/25 | Loss: 0.00110715
Iteration 23/25 | Loss: 0.00110666
Iteration 24/25 | Loss: 0.00110650
Iteration 25/25 | Loss: 0.00110639

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46151543
Iteration 2/25 | Loss: 0.00079115
Iteration 3/25 | Loss: 0.00079115
Iteration 4/25 | Loss: 0.00079115
Iteration 5/25 | Loss: 0.00079115
Iteration 6/25 | Loss: 0.00079115
Iteration 7/25 | Loss: 0.00079115
Iteration 8/25 | Loss: 0.00079115
Iteration 9/25 | Loss: 0.00079115
Iteration 10/25 | Loss: 0.00079115
Iteration 11/25 | Loss: 0.00079115
Iteration 12/25 | Loss: 0.00079114
Iteration 13/25 | Loss: 0.00079114
Iteration 14/25 | Loss: 0.00079114
Iteration 15/25 | Loss: 0.00079114
Iteration 16/25 | Loss: 0.00079114
Iteration 17/25 | Loss: 0.00079114
Iteration 18/25 | Loss: 0.00079114
Iteration 19/25 | Loss: 0.00079114
Iteration 20/25 | Loss: 0.00079114
Iteration 21/25 | Loss: 0.00079114
Iteration 22/25 | Loss: 0.00079114
Iteration 23/25 | Loss: 0.00079114
Iteration 24/25 | Loss: 0.00079114
Iteration 25/25 | Loss: 0.00079114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079114
Iteration 2/1000 | Loss: 0.00004574
Iteration 3/1000 | Loss: 0.00003167
Iteration 4/1000 | Loss: 0.00003734
Iteration 5/1000 | Loss: 0.00002649
Iteration 6/1000 | Loss: 0.00002527
Iteration 7/1000 | Loss: 0.00002420
Iteration 8/1000 | Loss: 0.00002355
Iteration 9/1000 | Loss: 0.00002282
Iteration 10/1000 | Loss: 0.00002234
Iteration 11/1000 | Loss: 0.00002189
Iteration 12/1000 | Loss: 0.00002145
Iteration 13/1000 | Loss: 0.00150353
Iteration 14/1000 | Loss: 0.00074984
Iteration 15/1000 | Loss: 0.00111819
Iteration 16/1000 | Loss: 0.00153926
Iteration 17/1000 | Loss: 0.00051567
Iteration 18/1000 | Loss: 0.00117630
Iteration 19/1000 | Loss: 0.00039352
Iteration 20/1000 | Loss: 0.00203801
Iteration 21/1000 | Loss: 0.00175730
Iteration 22/1000 | Loss: 0.00005191
Iteration 23/1000 | Loss: 0.00023614
Iteration 24/1000 | Loss: 0.00008963
Iteration 25/1000 | Loss: 0.00002181
Iteration 26/1000 | Loss: 0.00001931
Iteration 27/1000 | Loss: 0.00002810
Iteration 28/1000 | Loss: 0.00067897
Iteration 29/1000 | Loss: 0.00004007
Iteration 30/1000 | Loss: 0.00005710
Iteration 31/1000 | Loss: 0.00001407
Iteration 32/1000 | Loss: 0.00001836
Iteration 33/1000 | Loss: 0.00001276
Iteration 34/1000 | Loss: 0.00001406
Iteration 35/1000 | Loss: 0.00001242
Iteration 36/1000 | Loss: 0.00001242
Iteration 37/1000 | Loss: 0.00001241
Iteration 38/1000 | Loss: 0.00001241
Iteration 39/1000 | Loss: 0.00001241
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001240
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00002715
Iteration 45/1000 | Loss: 0.00001221
Iteration 46/1000 | Loss: 0.00001210
Iteration 47/1000 | Loss: 0.00001210
Iteration 48/1000 | Loss: 0.00001209
Iteration 49/1000 | Loss: 0.00001209
Iteration 50/1000 | Loss: 0.00001209
Iteration 51/1000 | Loss: 0.00001209
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001209
Iteration 54/1000 | Loss: 0.00001209
Iteration 55/1000 | Loss: 0.00001209
Iteration 56/1000 | Loss: 0.00001209
Iteration 57/1000 | Loss: 0.00001209
Iteration 58/1000 | Loss: 0.00001209
Iteration 59/1000 | Loss: 0.00001208
Iteration 60/1000 | Loss: 0.00001208
Iteration 61/1000 | Loss: 0.00001208
Iteration 62/1000 | Loss: 0.00001208
Iteration 63/1000 | Loss: 0.00001208
Iteration 64/1000 | Loss: 0.00001208
Iteration 65/1000 | Loss: 0.00001208
Iteration 66/1000 | Loss: 0.00002352
Iteration 67/1000 | Loss: 0.00001210
Iteration 68/1000 | Loss: 0.00001209
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001207
Iteration 71/1000 | Loss: 0.00001206
Iteration 72/1000 | Loss: 0.00001204
Iteration 73/1000 | Loss: 0.00001203
Iteration 74/1000 | Loss: 0.00001202
Iteration 75/1000 | Loss: 0.00001202
Iteration 76/1000 | Loss: 0.00001201
Iteration 77/1000 | Loss: 0.00001201
Iteration 78/1000 | Loss: 0.00001200
Iteration 79/1000 | Loss: 0.00001200
Iteration 80/1000 | Loss: 0.00001200
Iteration 81/1000 | Loss: 0.00001200
Iteration 82/1000 | Loss: 0.00001199
Iteration 83/1000 | Loss: 0.00001199
Iteration 84/1000 | Loss: 0.00001199
Iteration 85/1000 | Loss: 0.00001199
Iteration 86/1000 | Loss: 0.00001198
Iteration 87/1000 | Loss: 0.00001198
Iteration 88/1000 | Loss: 0.00001198
Iteration 89/1000 | Loss: 0.00001198
Iteration 90/1000 | Loss: 0.00001198
Iteration 91/1000 | Loss: 0.00001198
Iteration 92/1000 | Loss: 0.00001198
Iteration 93/1000 | Loss: 0.00001198
Iteration 94/1000 | Loss: 0.00001198
Iteration 95/1000 | Loss: 0.00001198
Iteration 96/1000 | Loss: 0.00001198
Iteration 97/1000 | Loss: 0.00001198
Iteration 98/1000 | Loss: 0.00001198
Iteration 99/1000 | Loss: 0.00001197
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001197
Iteration 102/1000 | Loss: 0.00001197
Iteration 103/1000 | Loss: 0.00001197
Iteration 104/1000 | Loss: 0.00001197
Iteration 105/1000 | Loss: 0.00001197
Iteration 106/1000 | Loss: 0.00001197
Iteration 107/1000 | Loss: 0.00001197
Iteration 108/1000 | Loss: 0.00001197
Iteration 109/1000 | Loss: 0.00001196
Iteration 110/1000 | Loss: 0.00001196
Iteration 111/1000 | Loss: 0.00001196
Iteration 112/1000 | Loss: 0.00001195
Iteration 113/1000 | Loss: 0.00001195
Iteration 114/1000 | Loss: 0.00001195
Iteration 115/1000 | Loss: 0.00001194
Iteration 116/1000 | Loss: 0.00001194
Iteration 117/1000 | Loss: 0.00001193
Iteration 118/1000 | Loss: 0.00001193
Iteration 119/1000 | Loss: 0.00001193
Iteration 120/1000 | Loss: 0.00001193
Iteration 121/1000 | Loss: 0.00001193
Iteration 122/1000 | Loss: 0.00001193
Iteration 123/1000 | Loss: 0.00001193
Iteration 124/1000 | Loss: 0.00001193
Iteration 125/1000 | Loss: 0.00001192
Iteration 126/1000 | Loss: 0.00001192
Iteration 127/1000 | Loss: 0.00001191
Iteration 128/1000 | Loss: 0.00001191
Iteration 129/1000 | Loss: 0.00001190
Iteration 130/1000 | Loss: 0.00001190
Iteration 131/1000 | Loss: 0.00001190
Iteration 132/1000 | Loss: 0.00001189
Iteration 133/1000 | Loss: 0.00001189
Iteration 134/1000 | Loss: 0.00001189
Iteration 135/1000 | Loss: 0.00001187
Iteration 136/1000 | Loss: 0.00001187
Iteration 137/1000 | Loss: 0.00001186
Iteration 138/1000 | Loss: 0.00001186
Iteration 139/1000 | Loss: 0.00001186
Iteration 140/1000 | Loss: 0.00001185
Iteration 141/1000 | Loss: 0.00001185
Iteration 142/1000 | Loss: 0.00001185
Iteration 143/1000 | Loss: 0.00001185
Iteration 144/1000 | Loss: 0.00001184
Iteration 145/1000 | Loss: 0.00001184
Iteration 146/1000 | Loss: 0.00001184
Iteration 147/1000 | Loss: 0.00001184
Iteration 148/1000 | Loss: 0.00001184
Iteration 149/1000 | Loss: 0.00001183
Iteration 150/1000 | Loss: 0.00001183
Iteration 151/1000 | Loss: 0.00001183
Iteration 152/1000 | Loss: 0.00001182
Iteration 153/1000 | Loss: 0.00001182
Iteration 154/1000 | Loss: 0.00001182
Iteration 155/1000 | Loss: 0.00001182
Iteration 156/1000 | Loss: 0.00001182
Iteration 157/1000 | Loss: 0.00001182
Iteration 158/1000 | Loss: 0.00001182
Iteration 159/1000 | Loss: 0.00001182
Iteration 160/1000 | Loss: 0.00001182
Iteration 161/1000 | Loss: 0.00001182
Iteration 162/1000 | Loss: 0.00001182
Iteration 163/1000 | Loss: 0.00001181
Iteration 164/1000 | Loss: 0.00001181
Iteration 165/1000 | Loss: 0.00001181
Iteration 166/1000 | Loss: 0.00001181
Iteration 167/1000 | Loss: 0.00001181
Iteration 168/1000 | Loss: 0.00001181
Iteration 169/1000 | Loss: 0.00001181
Iteration 170/1000 | Loss: 0.00001181
Iteration 171/1000 | Loss: 0.00001181
Iteration 172/1000 | Loss: 0.00001181
Iteration 173/1000 | Loss: 0.00001180
Iteration 174/1000 | Loss: 0.00001180
Iteration 175/1000 | Loss: 0.00001180
Iteration 176/1000 | Loss: 0.00001180
Iteration 177/1000 | Loss: 0.00001180
Iteration 178/1000 | Loss: 0.00001180
Iteration 179/1000 | Loss: 0.00001180
Iteration 180/1000 | Loss: 0.00001180
Iteration 181/1000 | Loss: 0.00001180
Iteration 182/1000 | Loss: 0.00001180
Iteration 183/1000 | Loss: 0.00001179
Iteration 184/1000 | Loss: 0.00001179
Iteration 185/1000 | Loss: 0.00001179
Iteration 186/1000 | Loss: 0.00001179
Iteration 187/1000 | Loss: 0.00001179
Iteration 188/1000 | Loss: 0.00001179
Iteration 189/1000 | Loss: 0.00001179
Iteration 190/1000 | Loss: 0.00001179
Iteration 191/1000 | Loss: 0.00001179
Iteration 192/1000 | Loss: 0.00001179
Iteration 193/1000 | Loss: 0.00001179
Iteration 194/1000 | Loss: 0.00001179
Iteration 195/1000 | Loss: 0.00001179
Iteration 196/1000 | Loss: 0.00001179
Iteration 197/1000 | Loss: 0.00001178
Iteration 198/1000 | Loss: 0.00001178
Iteration 199/1000 | Loss: 0.00001178
Iteration 200/1000 | Loss: 0.00001178
Iteration 201/1000 | Loss: 0.00001178
Iteration 202/1000 | Loss: 0.00001178
Iteration 203/1000 | Loss: 0.00001178
Iteration 204/1000 | Loss: 0.00001178
Iteration 205/1000 | Loss: 0.00001178
Iteration 206/1000 | Loss: 0.00001178
Iteration 207/1000 | Loss: 0.00001178
Iteration 208/1000 | Loss: 0.00001178
Iteration 209/1000 | Loss: 0.00001178
Iteration 210/1000 | Loss: 0.00001178
Iteration 211/1000 | Loss: 0.00001178
Iteration 212/1000 | Loss: 0.00001178
Iteration 213/1000 | Loss: 0.00001178
Iteration 214/1000 | Loss: 0.00001178
Iteration 215/1000 | Loss: 0.00001178
Iteration 216/1000 | Loss: 0.00001178
Iteration 217/1000 | Loss: 0.00001178
Iteration 218/1000 | Loss: 0.00001178
Iteration 219/1000 | Loss: 0.00001178
Iteration 220/1000 | Loss: 0.00001178
Iteration 221/1000 | Loss: 0.00001178
Iteration 222/1000 | Loss: 0.00001178
Iteration 223/1000 | Loss: 0.00001178
Iteration 224/1000 | Loss: 0.00001178
Iteration 225/1000 | Loss: 0.00001178
Iteration 226/1000 | Loss: 0.00001178
Iteration 227/1000 | Loss: 0.00001178
Iteration 228/1000 | Loss: 0.00001178
Iteration 229/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.178231104859151e-05, 1.178231104859151e-05, 1.178231104859151e-05, 1.178231104859151e-05, 1.178231104859151e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.178231104859151e-05

Optimization complete. Final v2v error: 2.884502410888672 mm

Highest mean error: 4.617771148681641 mm for frame 79

Lowest mean error: 2.405397415161133 mm for frame 122

Saving results

Total time: 109.50266718864441
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878956
Iteration 2/25 | Loss: 0.00266970
Iteration 3/25 | Loss: 0.00180607
Iteration 4/25 | Loss: 0.00150562
Iteration 5/25 | Loss: 0.00147443
Iteration 6/25 | Loss: 0.00147533
Iteration 7/25 | Loss: 0.00143919
Iteration 8/25 | Loss: 0.00142939
Iteration 9/25 | Loss: 0.00141282
Iteration 10/25 | Loss: 0.00140648
Iteration 11/25 | Loss: 0.00140030
Iteration 12/25 | Loss: 0.00139535
Iteration 13/25 | Loss: 0.00138975
Iteration 14/25 | Loss: 0.00138828
Iteration 15/25 | Loss: 0.00139188
Iteration 16/25 | Loss: 0.00138702
Iteration 17/25 | Loss: 0.00139046
Iteration 18/25 | Loss: 0.00138535
Iteration 19/25 | Loss: 0.00138903
Iteration 20/25 | Loss: 0.00138696
Iteration 21/25 | Loss: 0.00138231
Iteration 22/25 | Loss: 0.00138120
Iteration 23/25 | Loss: 0.00137920
Iteration 24/25 | Loss: 0.00137684
Iteration 25/25 | Loss: 0.00138633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.03415966
Iteration 2/25 | Loss: 0.00407246
Iteration 3/25 | Loss: 0.00390781
Iteration 4/25 | Loss: 0.00390781
Iteration 5/25 | Loss: 0.00390781
Iteration 6/25 | Loss: 0.00390781
Iteration 7/25 | Loss: 0.00390780
Iteration 8/25 | Loss: 0.00390780
Iteration 9/25 | Loss: 0.00390780
Iteration 10/25 | Loss: 0.00390780
Iteration 11/25 | Loss: 0.00390780
Iteration 12/25 | Loss: 0.00390780
Iteration 13/25 | Loss: 0.00390780
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.003907803446054459, 0.003907803446054459, 0.003907803446054459, 0.003907803446054459, 0.003907803446054459]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003907803446054459

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00390780
Iteration 2/1000 | Loss: 0.00109115
Iteration 3/1000 | Loss: 0.00113401
Iteration 4/1000 | Loss: 0.00086636
Iteration 5/1000 | Loss: 0.00042148
Iteration 6/1000 | Loss: 0.00318234
Iteration 7/1000 | Loss: 0.00325450
Iteration 8/1000 | Loss: 0.00017654
Iteration 9/1000 | Loss: 0.00069092
Iteration 10/1000 | Loss: 0.00012889
Iteration 11/1000 | Loss: 0.00066123
Iteration 12/1000 | Loss: 0.00013315
Iteration 13/1000 | Loss: 0.00057866
Iteration 14/1000 | Loss: 0.00088113
Iteration 15/1000 | Loss: 0.00008155
Iteration 16/1000 | Loss: 0.00008951
Iteration 17/1000 | Loss: 0.00059959
Iteration 18/1000 | Loss: 0.00009869
Iteration 19/1000 | Loss: 0.00044794
Iteration 20/1000 | Loss: 0.00010829
Iteration 21/1000 | Loss: 0.00093356
Iteration 22/1000 | Loss: 0.00010124
Iteration 23/1000 | Loss: 0.00028837
Iteration 24/1000 | Loss: 0.00006881
Iteration 25/1000 | Loss: 0.00025064
Iteration 26/1000 | Loss: 0.00013401
Iteration 27/1000 | Loss: 0.00004480
Iteration 28/1000 | Loss: 0.00009583
Iteration 29/1000 | Loss: 0.00004865
Iteration 30/1000 | Loss: 0.00008047
Iteration 31/1000 | Loss: 0.00093838
Iteration 32/1000 | Loss: 0.00013912
Iteration 33/1000 | Loss: 0.00004994
Iteration 34/1000 | Loss: 0.00003509
Iteration 35/1000 | Loss: 0.00003371
Iteration 36/1000 | Loss: 0.00003233
Iteration 37/1000 | Loss: 0.00085041
Iteration 38/1000 | Loss: 0.00021810
Iteration 39/1000 | Loss: 0.00003610
Iteration 40/1000 | Loss: 0.00031988
Iteration 41/1000 | Loss: 0.00026817
Iteration 42/1000 | Loss: 0.00031897
Iteration 43/1000 | Loss: 0.00024991
Iteration 44/1000 | Loss: 0.00029813
Iteration 45/1000 | Loss: 0.00024870
Iteration 46/1000 | Loss: 0.00024482
Iteration 47/1000 | Loss: 0.00003313
Iteration 48/1000 | Loss: 0.00003007
Iteration 49/1000 | Loss: 0.00002856
Iteration 50/1000 | Loss: 0.00025140
Iteration 51/1000 | Loss: 0.00004652
Iteration 52/1000 | Loss: 0.00003548
Iteration 53/1000 | Loss: 0.00003137
Iteration 54/1000 | Loss: 0.00002899
Iteration 55/1000 | Loss: 0.00002796
Iteration 56/1000 | Loss: 0.00002709
Iteration 57/1000 | Loss: 0.00002586
Iteration 58/1000 | Loss: 0.00002496
Iteration 59/1000 | Loss: 0.00002438
Iteration 60/1000 | Loss: 0.00002405
Iteration 61/1000 | Loss: 0.00002380
Iteration 62/1000 | Loss: 0.00002362
Iteration 63/1000 | Loss: 0.00002358
Iteration 64/1000 | Loss: 0.00002351
Iteration 65/1000 | Loss: 0.00002349
Iteration 66/1000 | Loss: 0.00002348
Iteration 67/1000 | Loss: 0.00002348
Iteration 68/1000 | Loss: 0.00059078
Iteration 69/1000 | Loss: 0.00013069
Iteration 70/1000 | Loss: 0.00002424
Iteration 71/1000 | Loss: 0.00002348
Iteration 72/1000 | Loss: 0.00002339
Iteration 73/1000 | Loss: 0.00002336
Iteration 74/1000 | Loss: 0.00002336
Iteration 75/1000 | Loss: 0.00002335
Iteration 76/1000 | Loss: 0.00002335
Iteration 77/1000 | Loss: 0.00002334
Iteration 78/1000 | Loss: 0.00002334
Iteration 79/1000 | Loss: 0.00002334
Iteration 80/1000 | Loss: 0.00002334
Iteration 81/1000 | Loss: 0.00002334
Iteration 82/1000 | Loss: 0.00002333
Iteration 83/1000 | Loss: 0.00002333
Iteration 84/1000 | Loss: 0.00002333
Iteration 85/1000 | Loss: 0.00002333
Iteration 86/1000 | Loss: 0.00002333
Iteration 87/1000 | Loss: 0.00002333
Iteration 88/1000 | Loss: 0.00002333
Iteration 89/1000 | Loss: 0.00002333
Iteration 90/1000 | Loss: 0.00002332
Iteration 91/1000 | Loss: 0.00002332
Iteration 92/1000 | Loss: 0.00002331
Iteration 93/1000 | Loss: 0.00002330
Iteration 94/1000 | Loss: 0.00002330
Iteration 95/1000 | Loss: 0.00002329
Iteration 96/1000 | Loss: 0.00002329
Iteration 97/1000 | Loss: 0.00002328
Iteration 98/1000 | Loss: 0.00002328
Iteration 99/1000 | Loss: 0.00002328
Iteration 100/1000 | Loss: 0.00002327
Iteration 101/1000 | Loss: 0.00002327
Iteration 102/1000 | Loss: 0.00002324
Iteration 103/1000 | Loss: 0.00002319
Iteration 104/1000 | Loss: 0.00002318
Iteration 105/1000 | Loss: 0.00058109
Iteration 106/1000 | Loss: 0.00068315
Iteration 107/1000 | Loss: 0.00040765
Iteration 108/1000 | Loss: 0.00028684
Iteration 109/1000 | Loss: 0.00011569
Iteration 110/1000 | Loss: 0.00002599
Iteration 111/1000 | Loss: 0.00048939
Iteration 112/1000 | Loss: 0.00056102
Iteration 113/1000 | Loss: 0.00006284
Iteration 114/1000 | Loss: 0.00031917
Iteration 115/1000 | Loss: 0.00006731
Iteration 116/1000 | Loss: 0.00002433
Iteration 117/1000 | Loss: 0.00002360
Iteration 118/1000 | Loss: 0.00043460
Iteration 119/1000 | Loss: 0.00009482
Iteration 120/1000 | Loss: 0.00034574
Iteration 121/1000 | Loss: 0.00012843
Iteration 122/1000 | Loss: 0.00002744
Iteration 123/1000 | Loss: 0.00002374
Iteration 124/1000 | Loss: 0.00002332
Iteration 125/1000 | Loss: 0.00002326
Iteration 126/1000 | Loss: 0.00002318
Iteration 127/1000 | Loss: 0.00002317
Iteration 128/1000 | Loss: 0.00002316
Iteration 129/1000 | Loss: 0.00002316
Iteration 130/1000 | Loss: 0.00002315
Iteration 131/1000 | Loss: 0.00002315
Iteration 132/1000 | Loss: 0.00002314
Iteration 133/1000 | Loss: 0.00002314
Iteration 134/1000 | Loss: 0.00002313
Iteration 135/1000 | Loss: 0.00002312
Iteration 136/1000 | Loss: 0.00002312
Iteration 137/1000 | Loss: 0.00002312
Iteration 138/1000 | Loss: 0.00002312
Iteration 139/1000 | Loss: 0.00002311
Iteration 140/1000 | Loss: 0.00002311
Iteration 141/1000 | Loss: 0.00002311
Iteration 142/1000 | Loss: 0.00002310
Iteration 143/1000 | Loss: 0.00002310
Iteration 144/1000 | Loss: 0.00002309
Iteration 145/1000 | Loss: 0.00002309
Iteration 146/1000 | Loss: 0.00002309
Iteration 147/1000 | Loss: 0.00002308
Iteration 148/1000 | Loss: 0.00002308
Iteration 149/1000 | Loss: 0.00002307
Iteration 150/1000 | Loss: 0.00002307
Iteration 151/1000 | Loss: 0.00002307
Iteration 152/1000 | Loss: 0.00002306
Iteration 153/1000 | Loss: 0.00002306
Iteration 154/1000 | Loss: 0.00002305
Iteration 155/1000 | Loss: 0.00002305
Iteration 156/1000 | Loss: 0.00002305
Iteration 157/1000 | Loss: 0.00002304
Iteration 158/1000 | Loss: 0.00002303
Iteration 159/1000 | Loss: 0.00002302
Iteration 160/1000 | Loss: 0.00044936
Iteration 161/1000 | Loss: 0.00003533
Iteration 162/1000 | Loss: 0.00002358
Iteration 163/1000 | Loss: 0.00002306
Iteration 164/1000 | Loss: 0.00002303
Iteration 165/1000 | Loss: 0.00002303
Iteration 166/1000 | Loss: 0.00002303
Iteration 167/1000 | Loss: 0.00002301
Iteration 168/1000 | Loss: 0.00002301
Iteration 169/1000 | Loss: 0.00002300
Iteration 170/1000 | Loss: 0.00002300
Iteration 171/1000 | Loss: 0.00002300
Iteration 172/1000 | Loss: 0.00002300
Iteration 173/1000 | Loss: 0.00002300
Iteration 174/1000 | Loss: 0.00002300
Iteration 175/1000 | Loss: 0.00002300
Iteration 176/1000 | Loss: 0.00002300
Iteration 177/1000 | Loss: 0.00002300
Iteration 178/1000 | Loss: 0.00002300
Iteration 179/1000 | Loss: 0.00002300
Iteration 180/1000 | Loss: 0.00002299
Iteration 181/1000 | Loss: 0.00002299
Iteration 182/1000 | Loss: 0.00002299
Iteration 183/1000 | Loss: 0.00002299
Iteration 184/1000 | Loss: 0.00002299
Iteration 185/1000 | Loss: 0.00002299
Iteration 186/1000 | Loss: 0.00002299
Iteration 187/1000 | Loss: 0.00002299
Iteration 188/1000 | Loss: 0.00002298
Iteration 189/1000 | Loss: 0.00044093
Iteration 190/1000 | Loss: 0.00003893
Iteration 191/1000 | Loss: 0.00021854
Iteration 192/1000 | Loss: 0.00002987
Iteration 193/1000 | Loss: 0.00076669
Iteration 194/1000 | Loss: 0.00016051
Iteration 195/1000 | Loss: 0.00002388
Iteration 196/1000 | Loss: 0.00002219
Iteration 197/1000 | Loss: 0.00002107
Iteration 198/1000 | Loss: 0.00002047
Iteration 199/1000 | Loss: 0.00002024
Iteration 200/1000 | Loss: 0.00002024
Iteration 201/1000 | Loss: 0.00002017
Iteration 202/1000 | Loss: 0.00001999
Iteration 203/1000 | Loss: 0.00001997
Iteration 204/1000 | Loss: 0.00001993
Iteration 205/1000 | Loss: 0.00001992
Iteration 206/1000 | Loss: 0.00001991
Iteration 207/1000 | Loss: 0.00001990
Iteration 208/1000 | Loss: 0.00001989
Iteration 209/1000 | Loss: 0.00001989
Iteration 210/1000 | Loss: 0.00001988
Iteration 211/1000 | Loss: 0.00001987
Iteration 212/1000 | Loss: 0.00001986
Iteration 213/1000 | Loss: 0.00001986
Iteration 214/1000 | Loss: 0.00001986
Iteration 215/1000 | Loss: 0.00001985
Iteration 216/1000 | Loss: 0.00001985
Iteration 217/1000 | Loss: 0.00001985
Iteration 218/1000 | Loss: 0.00001984
Iteration 219/1000 | Loss: 0.00001984
Iteration 220/1000 | Loss: 0.00001984
Iteration 221/1000 | Loss: 0.00001983
Iteration 222/1000 | Loss: 0.00001983
Iteration 223/1000 | Loss: 0.00001983
Iteration 224/1000 | Loss: 0.00001982
Iteration 225/1000 | Loss: 0.00001982
Iteration 226/1000 | Loss: 0.00001981
Iteration 227/1000 | Loss: 0.00001981
Iteration 228/1000 | Loss: 0.00001981
Iteration 229/1000 | Loss: 0.00001980
Iteration 230/1000 | Loss: 0.00001979
Iteration 231/1000 | Loss: 0.00001979
Iteration 232/1000 | Loss: 0.00001978
Iteration 233/1000 | Loss: 0.00001977
Iteration 234/1000 | Loss: 0.00001977
Iteration 235/1000 | Loss: 0.00001977
Iteration 236/1000 | Loss: 0.00001977
Iteration 237/1000 | Loss: 0.00001977
Iteration 238/1000 | Loss: 0.00001976
Iteration 239/1000 | Loss: 0.00001976
Iteration 240/1000 | Loss: 0.00001976
Iteration 241/1000 | Loss: 0.00001975
Iteration 242/1000 | Loss: 0.00001975
Iteration 243/1000 | Loss: 0.00001975
Iteration 244/1000 | Loss: 0.00001975
Iteration 245/1000 | Loss: 0.00001975
Iteration 246/1000 | Loss: 0.00001974
Iteration 247/1000 | Loss: 0.00001974
Iteration 248/1000 | Loss: 0.00001973
Iteration 249/1000 | Loss: 0.00001973
Iteration 250/1000 | Loss: 0.00001973
Iteration 251/1000 | Loss: 0.00001972
Iteration 252/1000 | Loss: 0.00001972
Iteration 253/1000 | Loss: 0.00001972
Iteration 254/1000 | Loss: 0.00001972
Iteration 255/1000 | Loss: 0.00001971
Iteration 256/1000 | Loss: 0.00001971
Iteration 257/1000 | Loss: 0.00001971
Iteration 258/1000 | Loss: 0.00001971
Iteration 259/1000 | Loss: 0.00001971
Iteration 260/1000 | Loss: 0.00001970
Iteration 261/1000 | Loss: 0.00001970
Iteration 262/1000 | Loss: 0.00001970
Iteration 263/1000 | Loss: 0.00001970
Iteration 264/1000 | Loss: 0.00001970
Iteration 265/1000 | Loss: 0.00001970
Iteration 266/1000 | Loss: 0.00001970
Iteration 267/1000 | Loss: 0.00001970
Iteration 268/1000 | Loss: 0.00001970
Iteration 269/1000 | Loss: 0.00001970
Iteration 270/1000 | Loss: 0.00001970
Iteration 271/1000 | Loss: 0.00001970
Iteration 272/1000 | Loss: 0.00001970
Iteration 273/1000 | Loss: 0.00001970
Iteration 274/1000 | Loss: 0.00001970
Iteration 275/1000 | Loss: 0.00001969
Iteration 276/1000 | Loss: 0.00001969
Iteration 277/1000 | Loss: 0.00001969
Iteration 278/1000 | Loss: 0.00001969
Iteration 279/1000 | Loss: 0.00001969
Iteration 280/1000 | Loss: 0.00001969
Iteration 281/1000 | Loss: 0.00001969
Iteration 282/1000 | Loss: 0.00001969
Iteration 283/1000 | Loss: 0.00001969
Iteration 284/1000 | Loss: 0.00001969
Iteration 285/1000 | Loss: 0.00001969
Iteration 286/1000 | Loss: 0.00001969
Iteration 287/1000 | Loss: 0.00001969
Iteration 288/1000 | Loss: 0.00001969
Iteration 289/1000 | Loss: 0.00001969
Iteration 290/1000 | Loss: 0.00001969
Iteration 291/1000 | Loss: 0.00001969
Iteration 292/1000 | Loss: 0.00001969
Iteration 293/1000 | Loss: 0.00001969
Iteration 294/1000 | Loss: 0.00001969
Iteration 295/1000 | Loss: 0.00001969
Iteration 296/1000 | Loss: 0.00001969
Iteration 297/1000 | Loss: 0.00001969
Iteration 298/1000 | Loss: 0.00001969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 298. Stopping optimization.
Last 5 losses: [1.9689528926392086e-05, 1.9689528926392086e-05, 1.9689528926392086e-05, 1.9689528926392086e-05, 1.9689528926392086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9689528926392086e-05

Optimization complete. Final v2v error: 3.132328987121582 mm

Highest mean error: 12.718647956848145 mm for frame 87

Lowest mean error: 2.4403433799743652 mm for frame 112

Saving results

Total time: 215.35838890075684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489064
Iteration 2/25 | Loss: 0.00121901
Iteration 3/25 | Loss: 0.00114662
Iteration 4/25 | Loss: 0.00113405
Iteration 5/25 | Loss: 0.00112964
Iteration 6/25 | Loss: 0.00112932
Iteration 7/25 | Loss: 0.00112932
Iteration 8/25 | Loss: 0.00112932
Iteration 9/25 | Loss: 0.00112932
Iteration 10/25 | Loss: 0.00112932
Iteration 11/25 | Loss: 0.00112932
Iteration 12/25 | Loss: 0.00112932
Iteration 13/25 | Loss: 0.00112932
Iteration 14/25 | Loss: 0.00112932
Iteration 15/25 | Loss: 0.00112932
Iteration 16/25 | Loss: 0.00112932
Iteration 17/25 | Loss: 0.00112932
Iteration 18/25 | Loss: 0.00112932
Iteration 19/25 | Loss: 0.00112932
Iteration 20/25 | Loss: 0.00112932
Iteration 21/25 | Loss: 0.00112932
Iteration 22/25 | Loss: 0.00112932
Iteration 23/25 | Loss: 0.00112932
Iteration 24/25 | Loss: 0.00112932
Iteration 25/25 | Loss: 0.00112932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34188843
Iteration 2/25 | Loss: 0.00088512
Iteration 3/25 | Loss: 0.00088511
Iteration 4/25 | Loss: 0.00088511
Iteration 5/25 | Loss: 0.00088511
Iteration 6/25 | Loss: 0.00088511
Iteration 7/25 | Loss: 0.00088511
Iteration 8/25 | Loss: 0.00088511
Iteration 9/25 | Loss: 0.00088511
Iteration 10/25 | Loss: 0.00088511
Iteration 11/25 | Loss: 0.00088511
Iteration 12/25 | Loss: 0.00088511
Iteration 13/25 | Loss: 0.00088511
Iteration 14/25 | Loss: 0.00088511
Iteration 15/25 | Loss: 0.00088511
Iteration 16/25 | Loss: 0.00088511
Iteration 17/25 | Loss: 0.00088511
Iteration 18/25 | Loss: 0.00088511
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008851087186485529, 0.0008851087186485529, 0.0008851087186485529, 0.0008851087186485529, 0.0008851087186485529]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008851087186485529

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088511
Iteration 2/1000 | Loss: 0.00004001
Iteration 3/1000 | Loss: 0.00002908
Iteration 4/1000 | Loss: 0.00002569
Iteration 5/1000 | Loss: 0.00002425
Iteration 6/1000 | Loss: 0.00002340
Iteration 7/1000 | Loss: 0.00002291
Iteration 8/1000 | Loss: 0.00002240
Iteration 9/1000 | Loss: 0.00002216
Iteration 10/1000 | Loss: 0.00002188
Iteration 11/1000 | Loss: 0.00002164
Iteration 12/1000 | Loss: 0.00002150
Iteration 13/1000 | Loss: 0.00002142
Iteration 14/1000 | Loss: 0.00002141
Iteration 15/1000 | Loss: 0.00002140
Iteration 16/1000 | Loss: 0.00002139
Iteration 17/1000 | Loss: 0.00002139
Iteration 18/1000 | Loss: 0.00002134
Iteration 19/1000 | Loss: 0.00002129
Iteration 20/1000 | Loss: 0.00002126
Iteration 21/1000 | Loss: 0.00002125
Iteration 22/1000 | Loss: 0.00002125
Iteration 23/1000 | Loss: 0.00002124
Iteration 24/1000 | Loss: 0.00002124
Iteration 25/1000 | Loss: 0.00002123
Iteration 26/1000 | Loss: 0.00002123
Iteration 27/1000 | Loss: 0.00002123
Iteration 28/1000 | Loss: 0.00002122
Iteration 29/1000 | Loss: 0.00002121
Iteration 30/1000 | Loss: 0.00002121
Iteration 31/1000 | Loss: 0.00002120
Iteration 32/1000 | Loss: 0.00002120
Iteration 33/1000 | Loss: 0.00002116
Iteration 34/1000 | Loss: 0.00002113
Iteration 35/1000 | Loss: 0.00002110
Iteration 36/1000 | Loss: 0.00002110
Iteration 37/1000 | Loss: 0.00002106
Iteration 38/1000 | Loss: 0.00002106
Iteration 39/1000 | Loss: 0.00002106
Iteration 40/1000 | Loss: 0.00002106
Iteration 41/1000 | Loss: 0.00002102
Iteration 42/1000 | Loss: 0.00002101
Iteration 43/1000 | Loss: 0.00002101
Iteration 44/1000 | Loss: 0.00002100
Iteration 45/1000 | Loss: 0.00002100
Iteration 46/1000 | Loss: 0.00002099
Iteration 47/1000 | Loss: 0.00002099
Iteration 48/1000 | Loss: 0.00002099
Iteration 49/1000 | Loss: 0.00002098
Iteration 50/1000 | Loss: 0.00002098
Iteration 51/1000 | Loss: 0.00002098
Iteration 52/1000 | Loss: 0.00002097
Iteration 53/1000 | Loss: 0.00002096
Iteration 54/1000 | Loss: 0.00002095
Iteration 55/1000 | Loss: 0.00002095
Iteration 56/1000 | Loss: 0.00002094
Iteration 57/1000 | Loss: 0.00002093
Iteration 58/1000 | Loss: 0.00002093
Iteration 59/1000 | Loss: 0.00002093
Iteration 60/1000 | Loss: 0.00002093
Iteration 61/1000 | Loss: 0.00002093
Iteration 62/1000 | Loss: 0.00002093
Iteration 63/1000 | Loss: 0.00002093
Iteration 64/1000 | Loss: 0.00002093
Iteration 65/1000 | Loss: 0.00002093
Iteration 66/1000 | Loss: 0.00002093
Iteration 67/1000 | Loss: 0.00002093
Iteration 68/1000 | Loss: 0.00002092
Iteration 69/1000 | Loss: 0.00002091
Iteration 70/1000 | Loss: 0.00002089
Iteration 71/1000 | Loss: 0.00002089
Iteration 72/1000 | Loss: 0.00002089
Iteration 73/1000 | Loss: 0.00002089
Iteration 74/1000 | Loss: 0.00002089
Iteration 75/1000 | Loss: 0.00002089
Iteration 76/1000 | Loss: 0.00002089
Iteration 77/1000 | Loss: 0.00002089
Iteration 78/1000 | Loss: 0.00002089
Iteration 79/1000 | Loss: 0.00002088
Iteration 80/1000 | Loss: 0.00002087
Iteration 81/1000 | Loss: 0.00002087
Iteration 82/1000 | Loss: 0.00002086
Iteration 83/1000 | Loss: 0.00002086
Iteration 84/1000 | Loss: 0.00002086
Iteration 85/1000 | Loss: 0.00002085
Iteration 86/1000 | Loss: 0.00002085
Iteration 87/1000 | Loss: 0.00002085
Iteration 88/1000 | Loss: 0.00002085
Iteration 89/1000 | Loss: 0.00002085
Iteration 90/1000 | Loss: 0.00002084
Iteration 91/1000 | Loss: 0.00002084
Iteration 92/1000 | Loss: 0.00002084
Iteration 93/1000 | Loss: 0.00002084
Iteration 94/1000 | Loss: 0.00002083
Iteration 95/1000 | Loss: 0.00002083
Iteration 96/1000 | Loss: 0.00002083
Iteration 97/1000 | Loss: 0.00002083
Iteration 98/1000 | Loss: 0.00002083
Iteration 99/1000 | Loss: 0.00002082
Iteration 100/1000 | Loss: 0.00002082
Iteration 101/1000 | Loss: 0.00002082
Iteration 102/1000 | Loss: 0.00002081
Iteration 103/1000 | Loss: 0.00002081
Iteration 104/1000 | Loss: 0.00002081
Iteration 105/1000 | Loss: 0.00002081
Iteration 106/1000 | Loss: 0.00002081
Iteration 107/1000 | Loss: 0.00002080
Iteration 108/1000 | Loss: 0.00002080
Iteration 109/1000 | Loss: 0.00002080
Iteration 110/1000 | Loss: 0.00002080
Iteration 111/1000 | Loss: 0.00002080
Iteration 112/1000 | Loss: 0.00002080
Iteration 113/1000 | Loss: 0.00002079
Iteration 114/1000 | Loss: 0.00002079
Iteration 115/1000 | Loss: 0.00002079
Iteration 116/1000 | Loss: 0.00002079
Iteration 117/1000 | Loss: 0.00002079
Iteration 118/1000 | Loss: 0.00002079
Iteration 119/1000 | Loss: 0.00002079
Iteration 120/1000 | Loss: 0.00002079
Iteration 121/1000 | Loss: 0.00002079
Iteration 122/1000 | Loss: 0.00002079
Iteration 123/1000 | Loss: 0.00002079
Iteration 124/1000 | Loss: 0.00002078
Iteration 125/1000 | Loss: 0.00002078
Iteration 126/1000 | Loss: 0.00002078
Iteration 127/1000 | Loss: 0.00002078
Iteration 128/1000 | Loss: 0.00002077
Iteration 129/1000 | Loss: 0.00002077
Iteration 130/1000 | Loss: 0.00002077
Iteration 131/1000 | Loss: 0.00002077
Iteration 132/1000 | Loss: 0.00002077
Iteration 133/1000 | Loss: 0.00002077
Iteration 134/1000 | Loss: 0.00002077
Iteration 135/1000 | Loss: 0.00002077
Iteration 136/1000 | Loss: 0.00002077
Iteration 137/1000 | Loss: 0.00002077
Iteration 138/1000 | Loss: 0.00002077
Iteration 139/1000 | Loss: 0.00002076
Iteration 140/1000 | Loss: 0.00002076
Iteration 141/1000 | Loss: 0.00002076
Iteration 142/1000 | Loss: 0.00002076
Iteration 143/1000 | Loss: 0.00002076
Iteration 144/1000 | Loss: 0.00002076
Iteration 145/1000 | Loss: 0.00002076
Iteration 146/1000 | Loss: 0.00002076
Iteration 147/1000 | Loss: 0.00002076
Iteration 148/1000 | Loss: 0.00002075
Iteration 149/1000 | Loss: 0.00002075
Iteration 150/1000 | Loss: 0.00002075
Iteration 151/1000 | Loss: 0.00002075
Iteration 152/1000 | Loss: 0.00002075
Iteration 153/1000 | Loss: 0.00002074
Iteration 154/1000 | Loss: 0.00002074
Iteration 155/1000 | Loss: 0.00002074
Iteration 156/1000 | Loss: 0.00002074
Iteration 157/1000 | Loss: 0.00002074
Iteration 158/1000 | Loss: 0.00002073
Iteration 159/1000 | Loss: 0.00002073
Iteration 160/1000 | Loss: 0.00002073
Iteration 161/1000 | Loss: 0.00002073
Iteration 162/1000 | Loss: 0.00002073
Iteration 163/1000 | Loss: 0.00002073
Iteration 164/1000 | Loss: 0.00002073
Iteration 165/1000 | Loss: 0.00002073
Iteration 166/1000 | Loss: 0.00002073
Iteration 167/1000 | Loss: 0.00002073
Iteration 168/1000 | Loss: 0.00002072
Iteration 169/1000 | Loss: 0.00002072
Iteration 170/1000 | Loss: 0.00002072
Iteration 171/1000 | Loss: 0.00002072
Iteration 172/1000 | Loss: 0.00002072
Iteration 173/1000 | Loss: 0.00002072
Iteration 174/1000 | Loss: 0.00002071
Iteration 175/1000 | Loss: 0.00002071
Iteration 176/1000 | Loss: 0.00002071
Iteration 177/1000 | Loss: 0.00002071
Iteration 178/1000 | Loss: 0.00002071
Iteration 179/1000 | Loss: 0.00002071
Iteration 180/1000 | Loss: 0.00002071
Iteration 181/1000 | Loss: 0.00002071
Iteration 182/1000 | Loss: 0.00002071
Iteration 183/1000 | Loss: 0.00002071
Iteration 184/1000 | Loss: 0.00002071
Iteration 185/1000 | Loss: 0.00002071
Iteration 186/1000 | Loss: 0.00002071
Iteration 187/1000 | Loss: 0.00002071
Iteration 188/1000 | Loss: 0.00002071
Iteration 189/1000 | Loss: 0.00002071
Iteration 190/1000 | Loss: 0.00002071
Iteration 191/1000 | Loss: 0.00002070
Iteration 192/1000 | Loss: 0.00002070
Iteration 193/1000 | Loss: 0.00002070
Iteration 194/1000 | Loss: 0.00002070
Iteration 195/1000 | Loss: 0.00002070
Iteration 196/1000 | Loss: 0.00002070
Iteration 197/1000 | Loss: 0.00002070
Iteration 198/1000 | Loss: 0.00002070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.070401569653768e-05, 2.070401569653768e-05, 2.070401569653768e-05, 2.070401569653768e-05, 2.070401569653768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.070401569653768e-05

Optimization complete. Final v2v error: 3.4579007625579834 mm

Highest mean error: 4.515803337097168 mm for frame 93

Lowest mean error: 2.942082166671753 mm for frame 134

Saving results

Total time: 45.690104484558105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485949
Iteration 2/25 | Loss: 0.00125079
Iteration 3/25 | Loss: 0.00113752
Iteration 4/25 | Loss: 0.00112736
Iteration 5/25 | Loss: 0.00112454
Iteration 6/25 | Loss: 0.00112414
Iteration 7/25 | Loss: 0.00112414
Iteration 8/25 | Loss: 0.00112414
Iteration 9/25 | Loss: 0.00112414
Iteration 10/25 | Loss: 0.00112414
Iteration 11/25 | Loss: 0.00112414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011241404572501779, 0.0011241404572501779, 0.0011241404572501779, 0.0011241404572501779, 0.0011241404572501779]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011241404572501779

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45242667
Iteration 2/25 | Loss: 0.00078863
Iteration 3/25 | Loss: 0.00078861
Iteration 4/25 | Loss: 0.00078861
Iteration 5/25 | Loss: 0.00078861
Iteration 6/25 | Loss: 0.00078861
Iteration 7/25 | Loss: 0.00078861
Iteration 8/25 | Loss: 0.00078861
Iteration 9/25 | Loss: 0.00078861
Iteration 10/25 | Loss: 0.00078861
Iteration 11/25 | Loss: 0.00078861
Iteration 12/25 | Loss: 0.00078861
Iteration 13/25 | Loss: 0.00078861
Iteration 14/25 | Loss: 0.00078861
Iteration 15/25 | Loss: 0.00078861
Iteration 16/25 | Loss: 0.00078861
Iteration 17/25 | Loss: 0.00078861
Iteration 18/25 | Loss: 0.00078861
Iteration 19/25 | Loss: 0.00078861
Iteration 20/25 | Loss: 0.00078861
Iteration 21/25 | Loss: 0.00078861
Iteration 22/25 | Loss: 0.00078861
Iteration 23/25 | Loss: 0.00078861
Iteration 24/25 | Loss: 0.00078861
Iteration 25/25 | Loss: 0.00078861

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078861
Iteration 2/1000 | Loss: 0.00002675
Iteration 3/1000 | Loss: 0.00001848
Iteration 4/1000 | Loss: 0.00001677
Iteration 5/1000 | Loss: 0.00001574
Iteration 6/1000 | Loss: 0.00001528
Iteration 7/1000 | Loss: 0.00001479
Iteration 8/1000 | Loss: 0.00001451
Iteration 9/1000 | Loss: 0.00001422
Iteration 10/1000 | Loss: 0.00001401
Iteration 11/1000 | Loss: 0.00001400
Iteration 12/1000 | Loss: 0.00001400
Iteration 13/1000 | Loss: 0.00001399
Iteration 14/1000 | Loss: 0.00001393
Iteration 15/1000 | Loss: 0.00001385
Iteration 16/1000 | Loss: 0.00001380
Iteration 17/1000 | Loss: 0.00001378
Iteration 18/1000 | Loss: 0.00001378
Iteration 19/1000 | Loss: 0.00001373
Iteration 20/1000 | Loss: 0.00001370
Iteration 21/1000 | Loss: 0.00001369
Iteration 22/1000 | Loss: 0.00001369
Iteration 23/1000 | Loss: 0.00001369
Iteration 24/1000 | Loss: 0.00001368
Iteration 25/1000 | Loss: 0.00001367
Iteration 26/1000 | Loss: 0.00001367
Iteration 27/1000 | Loss: 0.00001366
Iteration 28/1000 | Loss: 0.00001366
Iteration 29/1000 | Loss: 0.00001365
Iteration 30/1000 | Loss: 0.00001365
Iteration 31/1000 | Loss: 0.00001364
Iteration 32/1000 | Loss: 0.00001364
Iteration 33/1000 | Loss: 0.00001364
Iteration 34/1000 | Loss: 0.00001363
Iteration 35/1000 | Loss: 0.00001363
Iteration 36/1000 | Loss: 0.00001363
Iteration 37/1000 | Loss: 0.00001362
Iteration 38/1000 | Loss: 0.00001362
Iteration 39/1000 | Loss: 0.00001362
Iteration 40/1000 | Loss: 0.00001362
Iteration 41/1000 | Loss: 0.00001361
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001361
Iteration 45/1000 | Loss: 0.00001360
Iteration 46/1000 | Loss: 0.00001360
Iteration 47/1000 | Loss: 0.00001360
Iteration 48/1000 | Loss: 0.00001359
Iteration 49/1000 | Loss: 0.00001359
Iteration 50/1000 | Loss: 0.00001359
Iteration 51/1000 | Loss: 0.00001358
Iteration 52/1000 | Loss: 0.00001358
Iteration 53/1000 | Loss: 0.00001358
Iteration 54/1000 | Loss: 0.00001358
Iteration 55/1000 | Loss: 0.00001358
Iteration 56/1000 | Loss: 0.00001358
Iteration 57/1000 | Loss: 0.00001358
Iteration 58/1000 | Loss: 0.00001357
Iteration 59/1000 | Loss: 0.00001357
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001356
Iteration 64/1000 | Loss: 0.00001356
Iteration 65/1000 | Loss: 0.00001356
Iteration 66/1000 | Loss: 0.00001356
Iteration 67/1000 | Loss: 0.00001356
Iteration 68/1000 | Loss: 0.00001356
Iteration 69/1000 | Loss: 0.00001356
Iteration 70/1000 | Loss: 0.00001356
Iteration 71/1000 | Loss: 0.00001355
Iteration 72/1000 | Loss: 0.00001355
Iteration 73/1000 | Loss: 0.00001355
Iteration 74/1000 | Loss: 0.00001355
Iteration 75/1000 | Loss: 0.00001354
Iteration 76/1000 | Loss: 0.00001354
Iteration 77/1000 | Loss: 0.00001354
Iteration 78/1000 | Loss: 0.00001353
Iteration 79/1000 | Loss: 0.00001353
Iteration 80/1000 | Loss: 0.00001353
Iteration 81/1000 | Loss: 0.00001353
Iteration 82/1000 | Loss: 0.00001352
Iteration 83/1000 | Loss: 0.00001352
Iteration 84/1000 | Loss: 0.00001352
Iteration 85/1000 | Loss: 0.00001351
Iteration 86/1000 | Loss: 0.00001351
Iteration 87/1000 | Loss: 0.00001351
Iteration 88/1000 | Loss: 0.00001351
Iteration 89/1000 | Loss: 0.00001351
Iteration 90/1000 | Loss: 0.00001350
Iteration 91/1000 | Loss: 0.00001350
Iteration 92/1000 | Loss: 0.00001350
Iteration 93/1000 | Loss: 0.00001350
Iteration 94/1000 | Loss: 0.00001350
Iteration 95/1000 | Loss: 0.00001350
Iteration 96/1000 | Loss: 0.00001350
Iteration 97/1000 | Loss: 0.00001349
Iteration 98/1000 | Loss: 0.00001349
Iteration 99/1000 | Loss: 0.00001348
Iteration 100/1000 | Loss: 0.00001348
Iteration 101/1000 | Loss: 0.00001348
Iteration 102/1000 | Loss: 0.00001348
Iteration 103/1000 | Loss: 0.00001347
Iteration 104/1000 | Loss: 0.00001347
Iteration 105/1000 | Loss: 0.00001347
Iteration 106/1000 | Loss: 0.00001346
Iteration 107/1000 | Loss: 0.00001346
Iteration 108/1000 | Loss: 0.00001345
Iteration 109/1000 | Loss: 0.00001345
Iteration 110/1000 | Loss: 0.00001345
Iteration 111/1000 | Loss: 0.00001344
Iteration 112/1000 | Loss: 0.00001344
Iteration 113/1000 | Loss: 0.00001344
Iteration 114/1000 | Loss: 0.00001342
Iteration 115/1000 | Loss: 0.00001342
Iteration 116/1000 | Loss: 0.00001342
Iteration 117/1000 | Loss: 0.00001342
Iteration 118/1000 | Loss: 0.00001342
Iteration 119/1000 | Loss: 0.00001342
Iteration 120/1000 | Loss: 0.00001341
Iteration 121/1000 | Loss: 0.00001341
Iteration 122/1000 | Loss: 0.00001341
Iteration 123/1000 | Loss: 0.00001341
Iteration 124/1000 | Loss: 0.00001341
Iteration 125/1000 | Loss: 0.00001341
Iteration 126/1000 | Loss: 0.00001341
Iteration 127/1000 | Loss: 0.00001341
Iteration 128/1000 | Loss: 0.00001340
Iteration 129/1000 | Loss: 0.00001339
Iteration 130/1000 | Loss: 0.00001339
Iteration 131/1000 | Loss: 0.00001339
Iteration 132/1000 | Loss: 0.00001339
Iteration 133/1000 | Loss: 0.00001339
Iteration 134/1000 | Loss: 0.00001339
Iteration 135/1000 | Loss: 0.00001338
Iteration 136/1000 | Loss: 0.00001338
Iteration 137/1000 | Loss: 0.00001338
Iteration 138/1000 | Loss: 0.00001338
Iteration 139/1000 | Loss: 0.00001337
Iteration 140/1000 | Loss: 0.00001337
Iteration 141/1000 | Loss: 0.00001336
Iteration 142/1000 | Loss: 0.00001336
Iteration 143/1000 | Loss: 0.00001336
Iteration 144/1000 | Loss: 0.00001336
Iteration 145/1000 | Loss: 0.00001336
Iteration 146/1000 | Loss: 0.00001336
Iteration 147/1000 | Loss: 0.00001336
Iteration 148/1000 | Loss: 0.00001335
Iteration 149/1000 | Loss: 0.00001335
Iteration 150/1000 | Loss: 0.00001335
Iteration 151/1000 | Loss: 0.00001334
Iteration 152/1000 | Loss: 0.00001334
Iteration 153/1000 | Loss: 0.00001334
Iteration 154/1000 | Loss: 0.00001334
Iteration 155/1000 | Loss: 0.00001333
Iteration 156/1000 | Loss: 0.00001333
Iteration 157/1000 | Loss: 0.00001333
Iteration 158/1000 | Loss: 0.00001333
Iteration 159/1000 | Loss: 0.00001333
Iteration 160/1000 | Loss: 0.00001333
Iteration 161/1000 | Loss: 0.00001333
Iteration 162/1000 | Loss: 0.00001333
Iteration 163/1000 | Loss: 0.00001333
Iteration 164/1000 | Loss: 0.00001333
Iteration 165/1000 | Loss: 0.00001333
Iteration 166/1000 | Loss: 0.00001332
Iteration 167/1000 | Loss: 0.00001332
Iteration 168/1000 | Loss: 0.00001332
Iteration 169/1000 | Loss: 0.00001332
Iteration 170/1000 | Loss: 0.00001332
Iteration 171/1000 | Loss: 0.00001332
Iteration 172/1000 | Loss: 0.00001332
Iteration 173/1000 | Loss: 0.00001332
Iteration 174/1000 | Loss: 0.00001332
Iteration 175/1000 | Loss: 0.00001332
Iteration 176/1000 | Loss: 0.00001332
Iteration 177/1000 | Loss: 0.00001332
Iteration 178/1000 | Loss: 0.00001332
Iteration 179/1000 | Loss: 0.00001332
Iteration 180/1000 | Loss: 0.00001332
Iteration 181/1000 | Loss: 0.00001332
Iteration 182/1000 | Loss: 0.00001332
Iteration 183/1000 | Loss: 0.00001332
Iteration 184/1000 | Loss: 0.00001332
Iteration 185/1000 | Loss: 0.00001332
Iteration 186/1000 | Loss: 0.00001332
Iteration 187/1000 | Loss: 0.00001332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.33151206682669e-05, 1.33151206682669e-05, 1.33151206682669e-05, 1.33151206682669e-05, 1.33151206682669e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.33151206682669e-05

Optimization complete. Final v2v error: 2.9890687465667725 mm

Highest mean error: 4.149319648742676 mm for frame 110

Lowest mean error: 2.380009412765503 mm for frame 1

Saving results

Total time: 38.64737892150879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00912731
Iteration 2/25 | Loss: 0.00162486
Iteration 3/25 | Loss: 0.00127536
Iteration 4/25 | Loss: 0.00124464
Iteration 5/25 | Loss: 0.00124198
Iteration 6/25 | Loss: 0.00122457
Iteration 7/25 | Loss: 0.00122391
Iteration 8/25 | Loss: 0.00121069
Iteration 9/25 | Loss: 0.00121368
Iteration 10/25 | Loss: 0.00120259
Iteration 11/25 | Loss: 0.00119965
Iteration 12/25 | Loss: 0.00119758
Iteration 13/25 | Loss: 0.00119844
Iteration 14/25 | Loss: 0.00119249
Iteration 15/25 | Loss: 0.00119019
Iteration 16/25 | Loss: 0.00118919
Iteration 17/25 | Loss: 0.00118859
Iteration 18/25 | Loss: 0.00118891
Iteration 19/25 | Loss: 0.00119333
Iteration 20/25 | Loss: 0.00119006
Iteration 21/25 | Loss: 0.00118743
Iteration 22/25 | Loss: 0.00118679
Iteration 23/25 | Loss: 0.00118639
Iteration 24/25 | Loss: 0.00118621
Iteration 25/25 | Loss: 0.00118608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80376840
Iteration 2/25 | Loss: 0.00081627
Iteration 3/25 | Loss: 0.00081627
Iteration 4/25 | Loss: 0.00081627
Iteration 5/25 | Loss: 0.00081627
Iteration 6/25 | Loss: 0.00081627
Iteration 7/25 | Loss: 0.00081627
Iteration 8/25 | Loss: 0.00081627
Iteration 9/25 | Loss: 0.00081627
Iteration 10/25 | Loss: 0.00081627
Iteration 11/25 | Loss: 0.00081627
Iteration 12/25 | Loss: 0.00081627
Iteration 13/25 | Loss: 0.00081627
Iteration 14/25 | Loss: 0.00081627
Iteration 15/25 | Loss: 0.00081627
Iteration 16/25 | Loss: 0.00081627
Iteration 17/25 | Loss: 0.00081627
Iteration 18/25 | Loss: 0.00081627
Iteration 19/25 | Loss: 0.00081627
Iteration 20/25 | Loss: 0.00081627
Iteration 21/25 | Loss: 0.00081627
Iteration 22/25 | Loss: 0.00081627
Iteration 23/25 | Loss: 0.00081627
Iteration 24/25 | Loss: 0.00081627
Iteration 25/25 | Loss: 0.00081627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081627
Iteration 2/1000 | Loss: 0.00007074
Iteration 3/1000 | Loss: 0.00005384
Iteration 4/1000 | Loss: 0.00019451
Iteration 5/1000 | Loss: 0.00005234
Iteration 6/1000 | Loss: 0.00004823
Iteration 7/1000 | Loss: 0.00004549
Iteration 8/1000 | Loss: 0.00004350
Iteration 9/1000 | Loss: 0.00028637
Iteration 10/1000 | Loss: 0.00012704
Iteration 11/1000 | Loss: 0.00003942
Iteration 12/1000 | Loss: 0.00007509
Iteration 13/1000 | Loss: 0.00003833
Iteration 14/1000 | Loss: 0.00003641
Iteration 15/1000 | Loss: 0.00106278
Iteration 16/1000 | Loss: 0.00012118
Iteration 17/1000 | Loss: 0.00087324
Iteration 18/1000 | Loss: 0.00010774
Iteration 19/1000 | Loss: 0.00011361
Iteration 20/1000 | Loss: 0.00003590
Iteration 21/1000 | Loss: 0.00222303
Iteration 22/1000 | Loss: 0.00049431
Iteration 23/1000 | Loss: 0.00004517
Iteration 24/1000 | Loss: 0.00013351
Iteration 25/1000 | Loss: 0.00078877
Iteration 26/1000 | Loss: 0.00012564
Iteration 27/1000 | Loss: 0.00007329
Iteration 28/1000 | Loss: 0.00007455
Iteration 29/1000 | Loss: 0.00005530
Iteration 30/1000 | Loss: 0.00003491
Iteration 31/1000 | Loss: 0.00069745
Iteration 32/1000 | Loss: 0.00018892
Iteration 33/1000 | Loss: 0.00004437
Iteration 34/1000 | Loss: 0.00003561
Iteration 35/1000 | Loss: 0.00003436
Iteration 36/1000 | Loss: 0.00003383
Iteration 37/1000 | Loss: 0.00296493
Iteration 38/1000 | Loss: 0.00045298
Iteration 39/1000 | Loss: 0.00009166
Iteration 40/1000 | Loss: 0.00144719
Iteration 41/1000 | Loss: 0.00038620
Iteration 42/1000 | Loss: 0.00003698
Iteration 43/1000 | Loss: 0.00003424
Iteration 44/1000 | Loss: 0.00003343
Iteration 45/1000 | Loss: 0.00145516
Iteration 46/1000 | Loss: 0.00067291
Iteration 47/1000 | Loss: 0.00003992
Iteration 48/1000 | Loss: 0.00071976
Iteration 49/1000 | Loss: 0.00015382
Iteration 50/1000 | Loss: 0.00067946
Iteration 51/1000 | Loss: 0.00077590
Iteration 52/1000 | Loss: 0.00164574
Iteration 53/1000 | Loss: 0.00076514
Iteration 54/1000 | Loss: 0.00010464
Iteration 55/1000 | Loss: 0.00028190
Iteration 56/1000 | Loss: 0.00021130
Iteration 57/1000 | Loss: 0.00005607
Iteration 58/1000 | Loss: 0.00003771
Iteration 59/1000 | Loss: 0.00025521
Iteration 60/1000 | Loss: 0.00025739
Iteration 61/1000 | Loss: 0.00027420
Iteration 62/1000 | Loss: 0.00023433
Iteration 63/1000 | Loss: 0.00003582
Iteration 64/1000 | Loss: 0.00028098
Iteration 65/1000 | Loss: 0.00004044
Iteration 66/1000 | Loss: 0.00082985
Iteration 67/1000 | Loss: 0.00022277
Iteration 68/1000 | Loss: 0.00011709
Iteration 69/1000 | Loss: 0.00033009
Iteration 70/1000 | Loss: 0.00003655
Iteration 71/1000 | Loss: 0.00011897
Iteration 72/1000 | Loss: 0.00003436
Iteration 73/1000 | Loss: 0.00003262
Iteration 74/1000 | Loss: 0.00080605
Iteration 75/1000 | Loss: 0.00035298
Iteration 76/1000 | Loss: 0.00013322
Iteration 77/1000 | Loss: 0.00014140
Iteration 78/1000 | Loss: 0.00011762
Iteration 79/1000 | Loss: 0.00022671
Iteration 80/1000 | Loss: 0.00011356
Iteration 81/1000 | Loss: 0.00003572
Iteration 82/1000 | Loss: 0.00003356
Iteration 83/1000 | Loss: 0.00003223
Iteration 84/1000 | Loss: 0.00003073
Iteration 85/1000 | Loss: 0.00002932
Iteration 86/1000 | Loss: 0.00023711
Iteration 87/1000 | Loss: 0.00003890
Iteration 88/1000 | Loss: 0.00002819
Iteration 89/1000 | Loss: 0.00002784
Iteration 90/1000 | Loss: 0.00002731
Iteration 91/1000 | Loss: 0.00002703
Iteration 92/1000 | Loss: 0.00002682
Iteration 93/1000 | Loss: 0.00002674
Iteration 94/1000 | Loss: 0.00002673
Iteration 95/1000 | Loss: 0.00002663
Iteration 96/1000 | Loss: 0.00002663
Iteration 97/1000 | Loss: 0.00002663
Iteration 98/1000 | Loss: 0.00002663
Iteration 99/1000 | Loss: 0.00002663
Iteration 100/1000 | Loss: 0.00002662
Iteration 101/1000 | Loss: 0.00002660
Iteration 102/1000 | Loss: 0.00002660
Iteration 103/1000 | Loss: 0.00002660
Iteration 104/1000 | Loss: 0.00002660
Iteration 105/1000 | Loss: 0.00002660
Iteration 106/1000 | Loss: 0.00002659
Iteration 107/1000 | Loss: 0.00002659
Iteration 108/1000 | Loss: 0.00002659
Iteration 109/1000 | Loss: 0.00002659
Iteration 110/1000 | Loss: 0.00002658
Iteration 111/1000 | Loss: 0.00002658
Iteration 112/1000 | Loss: 0.00002657
Iteration 113/1000 | Loss: 0.00002657
Iteration 114/1000 | Loss: 0.00002657
Iteration 115/1000 | Loss: 0.00002657
Iteration 116/1000 | Loss: 0.00002656
Iteration 117/1000 | Loss: 0.00002656
Iteration 118/1000 | Loss: 0.00002656
Iteration 119/1000 | Loss: 0.00002656
Iteration 120/1000 | Loss: 0.00002656
Iteration 121/1000 | Loss: 0.00002656
Iteration 122/1000 | Loss: 0.00002655
Iteration 123/1000 | Loss: 0.00002655
Iteration 124/1000 | Loss: 0.00002655
Iteration 125/1000 | Loss: 0.00002655
Iteration 126/1000 | Loss: 0.00002655
Iteration 127/1000 | Loss: 0.00002655
Iteration 128/1000 | Loss: 0.00002655
Iteration 129/1000 | Loss: 0.00002655
Iteration 130/1000 | Loss: 0.00002654
Iteration 131/1000 | Loss: 0.00002654
Iteration 132/1000 | Loss: 0.00002654
Iteration 133/1000 | Loss: 0.00002654
Iteration 134/1000 | Loss: 0.00002654
Iteration 135/1000 | Loss: 0.00002654
Iteration 136/1000 | Loss: 0.00002653
Iteration 137/1000 | Loss: 0.00002653
Iteration 138/1000 | Loss: 0.00002653
Iteration 139/1000 | Loss: 0.00002653
Iteration 140/1000 | Loss: 0.00002653
Iteration 141/1000 | Loss: 0.00002653
Iteration 142/1000 | Loss: 0.00002653
Iteration 143/1000 | Loss: 0.00002653
Iteration 144/1000 | Loss: 0.00002653
Iteration 145/1000 | Loss: 0.00002652
Iteration 146/1000 | Loss: 0.00002652
Iteration 147/1000 | Loss: 0.00002652
Iteration 148/1000 | Loss: 0.00002652
Iteration 149/1000 | Loss: 0.00002652
Iteration 150/1000 | Loss: 0.00002652
Iteration 151/1000 | Loss: 0.00002652
Iteration 152/1000 | Loss: 0.00002651
Iteration 153/1000 | Loss: 0.00002651
Iteration 154/1000 | Loss: 0.00002651
Iteration 155/1000 | Loss: 0.00002651
Iteration 156/1000 | Loss: 0.00002651
Iteration 157/1000 | Loss: 0.00002651
Iteration 158/1000 | Loss: 0.00002651
Iteration 159/1000 | Loss: 0.00002651
Iteration 160/1000 | Loss: 0.00002650
Iteration 161/1000 | Loss: 0.00002650
Iteration 162/1000 | Loss: 0.00002650
Iteration 163/1000 | Loss: 0.00002650
Iteration 164/1000 | Loss: 0.00002650
Iteration 165/1000 | Loss: 0.00002650
Iteration 166/1000 | Loss: 0.00002650
Iteration 167/1000 | Loss: 0.00002650
Iteration 168/1000 | Loss: 0.00002650
Iteration 169/1000 | Loss: 0.00002650
Iteration 170/1000 | Loss: 0.00002650
Iteration 171/1000 | Loss: 0.00002650
Iteration 172/1000 | Loss: 0.00002650
Iteration 173/1000 | Loss: 0.00002650
Iteration 174/1000 | Loss: 0.00002650
Iteration 175/1000 | Loss: 0.00002650
Iteration 176/1000 | Loss: 0.00002650
Iteration 177/1000 | Loss: 0.00002650
Iteration 178/1000 | Loss: 0.00002650
Iteration 179/1000 | Loss: 0.00002650
Iteration 180/1000 | Loss: 0.00002650
Iteration 181/1000 | Loss: 0.00002650
Iteration 182/1000 | Loss: 0.00002650
Iteration 183/1000 | Loss: 0.00002650
Iteration 184/1000 | Loss: 0.00002650
Iteration 185/1000 | Loss: 0.00002650
Iteration 186/1000 | Loss: 0.00002650
Iteration 187/1000 | Loss: 0.00002650
Iteration 188/1000 | Loss: 0.00002650
Iteration 189/1000 | Loss: 0.00002650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.6504947527428158e-05, 2.6504947527428158e-05, 2.6504947527428158e-05, 2.6504947527428158e-05, 2.6504947527428158e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6504947527428158e-05

Optimization complete. Final v2v error: 3.6873080730438232 mm

Highest mean error: 12.62389087677002 mm for frame 141

Lowest mean error: 2.881838321685791 mm for frame 239

Saving results

Total time: 205.78294372558594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00357965
Iteration 2/25 | Loss: 0.00118234
Iteration 3/25 | Loss: 0.00108561
Iteration 4/25 | Loss: 0.00106632
Iteration 5/25 | Loss: 0.00105944
Iteration 6/25 | Loss: 0.00105763
Iteration 7/25 | Loss: 0.00105735
Iteration 8/25 | Loss: 0.00105735
Iteration 9/25 | Loss: 0.00105735
Iteration 10/25 | Loss: 0.00105735
Iteration 11/25 | Loss: 0.00105735
Iteration 12/25 | Loss: 0.00105735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010573475155979395, 0.0010573475155979395, 0.0010573475155979395, 0.0010573475155979395, 0.0010573475155979395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010573475155979395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36726904
Iteration 2/25 | Loss: 0.00099162
Iteration 3/25 | Loss: 0.00099162
Iteration 4/25 | Loss: 0.00099162
Iteration 5/25 | Loss: 0.00099162
Iteration 6/25 | Loss: 0.00099162
Iteration 7/25 | Loss: 0.00099162
Iteration 8/25 | Loss: 0.00099162
Iteration 9/25 | Loss: 0.00099162
Iteration 10/25 | Loss: 0.00099162
Iteration 11/25 | Loss: 0.00099162
Iteration 12/25 | Loss: 0.00099162
Iteration 13/25 | Loss: 0.00099162
Iteration 14/25 | Loss: 0.00099162
Iteration 15/25 | Loss: 0.00099162
Iteration 16/25 | Loss: 0.00099162
Iteration 17/25 | Loss: 0.00099162
Iteration 18/25 | Loss: 0.00099162
Iteration 19/25 | Loss: 0.00099162
Iteration 20/25 | Loss: 0.00099162
Iteration 21/25 | Loss: 0.00099162
Iteration 22/25 | Loss: 0.00099162
Iteration 23/25 | Loss: 0.00099162
Iteration 24/25 | Loss: 0.00099162
Iteration 25/25 | Loss: 0.00099162

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099162
Iteration 2/1000 | Loss: 0.00004600
Iteration 3/1000 | Loss: 0.00003073
Iteration 4/1000 | Loss: 0.00002091
Iteration 5/1000 | Loss: 0.00001889
Iteration 6/1000 | Loss: 0.00001783
Iteration 7/1000 | Loss: 0.00001677
Iteration 8/1000 | Loss: 0.00001609
Iteration 9/1000 | Loss: 0.00001561
Iteration 10/1000 | Loss: 0.00001521
Iteration 11/1000 | Loss: 0.00001492
Iteration 12/1000 | Loss: 0.00001470
Iteration 13/1000 | Loss: 0.00001452
Iteration 14/1000 | Loss: 0.00001448
Iteration 15/1000 | Loss: 0.00001447
Iteration 16/1000 | Loss: 0.00001446
Iteration 17/1000 | Loss: 0.00001444
Iteration 18/1000 | Loss: 0.00001439
Iteration 19/1000 | Loss: 0.00001434
Iteration 20/1000 | Loss: 0.00001434
Iteration 21/1000 | Loss: 0.00001433
Iteration 22/1000 | Loss: 0.00001429
Iteration 23/1000 | Loss: 0.00001428
Iteration 24/1000 | Loss: 0.00001426
Iteration 25/1000 | Loss: 0.00001426
Iteration 26/1000 | Loss: 0.00001425
Iteration 27/1000 | Loss: 0.00001425
Iteration 28/1000 | Loss: 0.00001424
Iteration 29/1000 | Loss: 0.00001424
Iteration 30/1000 | Loss: 0.00001421
Iteration 31/1000 | Loss: 0.00001421
Iteration 32/1000 | Loss: 0.00001414
Iteration 33/1000 | Loss: 0.00001414
Iteration 34/1000 | Loss: 0.00001414
Iteration 35/1000 | Loss: 0.00001413
Iteration 36/1000 | Loss: 0.00001412
Iteration 37/1000 | Loss: 0.00001412
Iteration 38/1000 | Loss: 0.00001412
Iteration 39/1000 | Loss: 0.00001411
Iteration 40/1000 | Loss: 0.00001411
Iteration 41/1000 | Loss: 0.00001411
Iteration 42/1000 | Loss: 0.00001411
Iteration 43/1000 | Loss: 0.00001411
Iteration 44/1000 | Loss: 0.00001410
Iteration 45/1000 | Loss: 0.00001410
Iteration 46/1000 | Loss: 0.00001410
Iteration 47/1000 | Loss: 0.00001410
Iteration 48/1000 | Loss: 0.00001410
Iteration 49/1000 | Loss: 0.00001410
Iteration 50/1000 | Loss: 0.00001410
Iteration 51/1000 | Loss: 0.00001410
Iteration 52/1000 | Loss: 0.00001410
Iteration 53/1000 | Loss: 0.00001410
Iteration 54/1000 | Loss: 0.00001410
Iteration 55/1000 | Loss: 0.00001410
Iteration 56/1000 | Loss: 0.00001409
Iteration 57/1000 | Loss: 0.00001409
Iteration 58/1000 | Loss: 0.00001409
Iteration 59/1000 | Loss: 0.00001409
Iteration 60/1000 | Loss: 0.00001408
Iteration 61/1000 | Loss: 0.00001408
Iteration 62/1000 | Loss: 0.00001408
Iteration 63/1000 | Loss: 0.00001407
Iteration 64/1000 | Loss: 0.00001407
Iteration 65/1000 | Loss: 0.00001407
Iteration 66/1000 | Loss: 0.00001407
Iteration 67/1000 | Loss: 0.00001407
Iteration 68/1000 | Loss: 0.00001407
Iteration 69/1000 | Loss: 0.00001406
Iteration 70/1000 | Loss: 0.00001406
Iteration 71/1000 | Loss: 0.00001406
Iteration 72/1000 | Loss: 0.00001406
Iteration 73/1000 | Loss: 0.00001406
Iteration 74/1000 | Loss: 0.00001405
Iteration 75/1000 | Loss: 0.00001405
Iteration 76/1000 | Loss: 0.00001405
Iteration 77/1000 | Loss: 0.00001404
Iteration 78/1000 | Loss: 0.00001404
Iteration 79/1000 | Loss: 0.00001404
Iteration 80/1000 | Loss: 0.00001404
Iteration 81/1000 | Loss: 0.00001404
Iteration 82/1000 | Loss: 0.00001403
Iteration 83/1000 | Loss: 0.00001403
Iteration 84/1000 | Loss: 0.00001403
Iteration 85/1000 | Loss: 0.00001402
Iteration 86/1000 | Loss: 0.00001402
Iteration 87/1000 | Loss: 0.00001402
Iteration 88/1000 | Loss: 0.00001402
Iteration 89/1000 | Loss: 0.00001401
Iteration 90/1000 | Loss: 0.00001401
Iteration 91/1000 | Loss: 0.00001401
Iteration 92/1000 | Loss: 0.00001400
Iteration 93/1000 | Loss: 0.00001400
Iteration 94/1000 | Loss: 0.00001400
Iteration 95/1000 | Loss: 0.00001400
Iteration 96/1000 | Loss: 0.00001400
Iteration 97/1000 | Loss: 0.00001400
Iteration 98/1000 | Loss: 0.00001399
Iteration 99/1000 | Loss: 0.00001399
Iteration 100/1000 | Loss: 0.00001399
Iteration 101/1000 | Loss: 0.00001398
Iteration 102/1000 | Loss: 0.00001398
Iteration 103/1000 | Loss: 0.00001398
Iteration 104/1000 | Loss: 0.00001398
Iteration 105/1000 | Loss: 0.00001398
Iteration 106/1000 | Loss: 0.00001398
Iteration 107/1000 | Loss: 0.00001397
Iteration 108/1000 | Loss: 0.00001397
Iteration 109/1000 | Loss: 0.00001397
Iteration 110/1000 | Loss: 0.00001397
Iteration 111/1000 | Loss: 0.00001396
Iteration 112/1000 | Loss: 0.00001396
Iteration 113/1000 | Loss: 0.00001396
Iteration 114/1000 | Loss: 0.00001396
Iteration 115/1000 | Loss: 0.00001395
Iteration 116/1000 | Loss: 0.00001395
Iteration 117/1000 | Loss: 0.00001395
Iteration 118/1000 | Loss: 0.00001395
Iteration 119/1000 | Loss: 0.00001395
Iteration 120/1000 | Loss: 0.00001395
Iteration 121/1000 | Loss: 0.00001394
Iteration 122/1000 | Loss: 0.00001394
Iteration 123/1000 | Loss: 0.00001394
Iteration 124/1000 | Loss: 0.00001394
Iteration 125/1000 | Loss: 0.00001394
Iteration 126/1000 | Loss: 0.00001394
Iteration 127/1000 | Loss: 0.00001394
Iteration 128/1000 | Loss: 0.00001394
Iteration 129/1000 | Loss: 0.00001393
Iteration 130/1000 | Loss: 0.00001393
Iteration 131/1000 | Loss: 0.00001393
Iteration 132/1000 | Loss: 0.00001393
Iteration 133/1000 | Loss: 0.00001393
Iteration 134/1000 | Loss: 0.00001393
Iteration 135/1000 | Loss: 0.00001393
Iteration 136/1000 | Loss: 0.00001393
Iteration 137/1000 | Loss: 0.00001393
Iteration 138/1000 | Loss: 0.00001393
Iteration 139/1000 | Loss: 0.00001393
Iteration 140/1000 | Loss: 0.00001393
Iteration 141/1000 | Loss: 0.00001393
Iteration 142/1000 | Loss: 0.00001393
Iteration 143/1000 | Loss: 0.00001393
Iteration 144/1000 | Loss: 0.00001393
Iteration 145/1000 | Loss: 0.00001393
Iteration 146/1000 | Loss: 0.00001393
Iteration 147/1000 | Loss: 0.00001393
Iteration 148/1000 | Loss: 0.00001393
Iteration 149/1000 | Loss: 0.00001393
Iteration 150/1000 | Loss: 0.00001393
Iteration 151/1000 | Loss: 0.00001393
Iteration 152/1000 | Loss: 0.00001393
Iteration 153/1000 | Loss: 0.00001393
Iteration 154/1000 | Loss: 0.00001393
Iteration 155/1000 | Loss: 0.00001393
Iteration 156/1000 | Loss: 0.00001393
Iteration 157/1000 | Loss: 0.00001393
Iteration 158/1000 | Loss: 0.00001393
Iteration 159/1000 | Loss: 0.00001393
Iteration 160/1000 | Loss: 0.00001393
Iteration 161/1000 | Loss: 0.00001393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.393233014823636e-05, 1.393233014823636e-05, 1.393233014823636e-05, 1.393233014823636e-05, 1.393233014823636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.393233014823636e-05

Optimization complete. Final v2v error: 3.1018905639648438 mm

Highest mean error: 4.700557708740234 mm for frame 151

Lowest mean error: 2.1907224655151367 mm for frame 165

Saving results

Total time: 41.78715109825134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00723382
Iteration 2/25 | Loss: 0.00143208
Iteration 3/25 | Loss: 0.00125909
Iteration 4/25 | Loss: 0.00124452
Iteration 5/25 | Loss: 0.00123904
Iteration 6/25 | Loss: 0.00123804
Iteration 7/25 | Loss: 0.00123804
Iteration 8/25 | Loss: 0.00123804
Iteration 9/25 | Loss: 0.00123804
Iteration 10/25 | Loss: 0.00123804
Iteration 11/25 | Loss: 0.00123804
Iteration 12/25 | Loss: 0.00123804
Iteration 13/25 | Loss: 0.00123804
Iteration 14/25 | Loss: 0.00123804
Iteration 15/25 | Loss: 0.00123804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012380413245409727, 0.0012380413245409727, 0.0012380413245409727, 0.0012380413245409727, 0.0012380413245409727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012380413245409727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79239619
Iteration 2/25 | Loss: 0.00088984
Iteration 3/25 | Loss: 0.00088978
Iteration 4/25 | Loss: 0.00088978
Iteration 5/25 | Loss: 0.00088978
Iteration 6/25 | Loss: 0.00088978
Iteration 7/25 | Loss: 0.00088978
Iteration 8/25 | Loss: 0.00088978
Iteration 9/25 | Loss: 0.00088978
Iteration 10/25 | Loss: 0.00088978
Iteration 11/25 | Loss: 0.00088978
Iteration 12/25 | Loss: 0.00088978
Iteration 13/25 | Loss: 0.00088978
Iteration 14/25 | Loss: 0.00088978
Iteration 15/25 | Loss: 0.00088978
Iteration 16/25 | Loss: 0.00088978
Iteration 17/25 | Loss: 0.00088978
Iteration 18/25 | Loss: 0.00088978
Iteration 19/25 | Loss: 0.00088978
Iteration 20/25 | Loss: 0.00088978
Iteration 21/25 | Loss: 0.00088978
Iteration 22/25 | Loss: 0.00088978
Iteration 23/25 | Loss: 0.00088978
Iteration 24/25 | Loss: 0.00088978
Iteration 25/25 | Loss: 0.00088978

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088978
Iteration 2/1000 | Loss: 0.00007751
Iteration 3/1000 | Loss: 0.00004692
Iteration 4/1000 | Loss: 0.00003827
Iteration 5/1000 | Loss: 0.00003481
Iteration 6/1000 | Loss: 0.00003362
Iteration 7/1000 | Loss: 0.00003273
Iteration 8/1000 | Loss: 0.00003225
Iteration 9/1000 | Loss: 0.00003153
Iteration 10/1000 | Loss: 0.00003105
Iteration 11/1000 | Loss: 0.00003066
Iteration 12/1000 | Loss: 0.00003033
Iteration 13/1000 | Loss: 0.00003001
Iteration 14/1000 | Loss: 0.00002975
Iteration 15/1000 | Loss: 0.00002954
Iteration 16/1000 | Loss: 0.00002931
Iteration 17/1000 | Loss: 0.00002912
Iteration 18/1000 | Loss: 0.00002903
Iteration 19/1000 | Loss: 0.00002895
Iteration 20/1000 | Loss: 0.00002895
Iteration 21/1000 | Loss: 0.00002892
Iteration 22/1000 | Loss: 0.00002892
Iteration 23/1000 | Loss: 0.00002891
Iteration 24/1000 | Loss: 0.00002890
Iteration 25/1000 | Loss: 0.00002890
Iteration 26/1000 | Loss: 0.00002890
Iteration 27/1000 | Loss: 0.00002887
Iteration 28/1000 | Loss: 0.00002875
Iteration 29/1000 | Loss: 0.00002873
Iteration 30/1000 | Loss: 0.00002864
Iteration 31/1000 | Loss: 0.00002863
Iteration 32/1000 | Loss: 0.00002858
Iteration 33/1000 | Loss: 0.00002858
Iteration 34/1000 | Loss: 0.00002854
Iteration 35/1000 | Loss: 0.00002854
Iteration 36/1000 | Loss: 0.00002853
Iteration 37/1000 | Loss: 0.00002852
Iteration 38/1000 | Loss: 0.00002852
Iteration 39/1000 | Loss: 0.00002851
Iteration 40/1000 | Loss: 0.00002851
Iteration 41/1000 | Loss: 0.00002850
Iteration 42/1000 | Loss: 0.00002849
Iteration 43/1000 | Loss: 0.00002849
Iteration 44/1000 | Loss: 0.00002849
Iteration 45/1000 | Loss: 0.00002849
Iteration 46/1000 | Loss: 0.00002849
Iteration 47/1000 | Loss: 0.00002848
Iteration 48/1000 | Loss: 0.00002846
Iteration 49/1000 | Loss: 0.00002846
Iteration 50/1000 | Loss: 0.00002846
Iteration 51/1000 | Loss: 0.00002846
Iteration 52/1000 | Loss: 0.00002845
Iteration 53/1000 | Loss: 0.00002845
Iteration 54/1000 | Loss: 0.00002842
Iteration 55/1000 | Loss: 0.00002841
Iteration 56/1000 | Loss: 0.00002841
Iteration 57/1000 | Loss: 0.00002841
Iteration 58/1000 | Loss: 0.00002840
Iteration 59/1000 | Loss: 0.00002840
Iteration 60/1000 | Loss: 0.00002839
Iteration 61/1000 | Loss: 0.00002839
Iteration 62/1000 | Loss: 0.00002837
Iteration 63/1000 | Loss: 0.00002836
Iteration 64/1000 | Loss: 0.00002836
Iteration 65/1000 | Loss: 0.00002833
Iteration 66/1000 | Loss: 0.00002833
Iteration 67/1000 | Loss: 0.00002833
Iteration 68/1000 | Loss: 0.00002832
Iteration 69/1000 | Loss: 0.00002832
Iteration 70/1000 | Loss: 0.00002832
Iteration 71/1000 | Loss: 0.00002832
Iteration 72/1000 | Loss: 0.00002832
Iteration 73/1000 | Loss: 0.00002832
Iteration 74/1000 | Loss: 0.00002832
Iteration 75/1000 | Loss: 0.00002832
Iteration 76/1000 | Loss: 0.00002832
Iteration 77/1000 | Loss: 0.00002831
Iteration 78/1000 | Loss: 0.00002831
Iteration 79/1000 | Loss: 0.00002831
Iteration 80/1000 | Loss: 0.00002831
Iteration 81/1000 | Loss: 0.00002831
Iteration 82/1000 | Loss: 0.00002830
Iteration 83/1000 | Loss: 0.00002830
Iteration 84/1000 | Loss: 0.00002830
Iteration 85/1000 | Loss: 0.00002828
Iteration 86/1000 | Loss: 0.00002828
Iteration 87/1000 | Loss: 0.00002828
Iteration 88/1000 | Loss: 0.00002828
Iteration 89/1000 | Loss: 0.00002828
Iteration 90/1000 | Loss: 0.00002828
Iteration 91/1000 | Loss: 0.00002828
Iteration 92/1000 | Loss: 0.00002828
Iteration 93/1000 | Loss: 0.00002828
Iteration 94/1000 | Loss: 0.00002828
Iteration 95/1000 | Loss: 0.00002828
Iteration 96/1000 | Loss: 0.00002828
Iteration 97/1000 | Loss: 0.00002828
Iteration 98/1000 | Loss: 0.00002828
Iteration 99/1000 | Loss: 0.00002828
Iteration 100/1000 | Loss: 0.00002828
Iteration 101/1000 | Loss: 0.00002828
Iteration 102/1000 | Loss: 0.00002828
Iteration 103/1000 | Loss: 0.00002827
Iteration 104/1000 | Loss: 0.00002826
Iteration 105/1000 | Loss: 0.00002826
Iteration 106/1000 | Loss: 0.00002826
Iteration 107/1000 | Loss: 0.00002826
Iteration 108/1000 | Loss: 0.00002826
Iteration 109/1000 | Loss: 0.00002826
Iteration 110/1000 | Loss: 0.00002826
Iteration 111/1000 | Loss: 0.00002826
Iteration 112/1000 | Loss: 0.00002826
Iteration 113/1000 | Loss: 0.00002825
Iteration 114/1000 | Loss: 0.00002825
Iteration 115/1000 | Loss: 0.00002825
Iteration 116/1000 | Loss: 0.00002824
Iteration 117/1000 | Loss: 0.00002824
Iteration 118/1000 | Loss: 0.00002824
Iteration 119/1000 | Loss: 0.00002824
Iteration 120/1000 | Loss: 0.00002824
Iteration 121/1000 | Loss: 0.00002823
Iteration 122/1000 | Loss: 0.00002823
Iteration 123/1000 | Loss: 0.00002823
Iteration 124/1000 | Loss: 0.00002823
Iteration 125/1000 | Loss: 0.00002823
Iteration 126/1000 | Loss: 0.00002823
Iteration 127/1000 | Loss: 0.00002823
Iteration 128/1000 | Loss: 0.00002823
Iteration 129/1000 | Loss: 0.00002823
Iteration 130/1000 | Loss: 0.00002823
Iteration 131/1000 | Loss: 0.00002822
Iteration 132/1000 | Loss: 0.00002822
Iteration 133/1000 | Loss: 0.00002822
Iteration 134/1000 | Loss: 0.00002822
Iteration 135/1000 | Loss: 0.00002822
Iteration 136/1000 | Loss: 0.00002822
Iteration 137/1000 | Loss: 0.00002822
Iteration 138/1000 | Loss: 0.00002822
Iteration 139/1000 | Loss: 0.00002822
Iteration 140/1000 | Loss: 0.00002821
Iteration 141/1000 | Loss: 0.00002821
Iteration 142/1000 | Loss: 0.00002821
Iteration 143/1000 | Loss: 0.00002821
Iteration 144/1000 | Loss: 0.00002821
Iteration 145/1000 | Loss: 0.00002821
Iteration 146/1000 | Loss: 0.00002821
Iteration 147/1000 | Loss: 0.00002821
Iteration 148/1000 | Loss: 0.00002820
Iteration 149/1000 | Loss: 0.00002820
Iteration 150/1000 | Loss: 0.00002820
Iteration 151/1000 | Loss: 0.00002820
Iteration 152/1000 | Loss: 0.00002820
Iteration 153/1000 | Loss: 0.00002819
Iteration 154/1000 | Loss: 0.00002819
Iteration 155/1000 | Loss: 0.00002819
Iteration 156/1000 | Loss: 0.00002819
Iteration 157/1000 | Loss: 0.00002818
Iteration 158/1000 | Loss: 0.00002818
Iteration 159/1000 | Loss: 0.00002818
Iteration 160/1000 | Loss: 0.00002818
Iteration 161/1000 | Loss: 0.00002818
Iteration 162/1000 | Loss: 0.00002817
Iteration 163/1000 | Loss: 0.00002817
Iteration 164/1000 | Loss: 0.00002817
Iteration 165/1000 | Loss: 0.00002817
Iteration 166/1000 | Loss: 0.00002817
Iteration 167/1000 | Loss: 0.00002817
Iteration 168/1000 | Loss: 0.00002816
Iteration 169/1000 | Loss: 0.00002816
Iteration 170/1000 | Loss: 0.00002816
Iteration 171/1000 | Loss: 0.00002816
Iteration 172/1000 | Loss: 0.00002816
Iteration 173/1000 | Loss: 0.00002816
Iteration 174/1000 | Loss: 0.00002816
Iteration 175/1000 | Loss: 0.00002816
Iteration 176/1000 | Loss: 0.00002816
Iteration 177/1000 | Loss: 0.00002816
Iteration 178/1000 | Loss: 0.00002816
Iteration 179/1000 | Loss: 0.00002816
Iteration 180/1000 | Loss: 0.00002816
Iteration 181/1000 | Loss: 0.00002816
Iteration 182/1000 | Loss: 0.00002816
Iteration 183/1000 | Loss: 0.00002816
Iteration 184/1000 | Loss: 0.00002816
Iteration 185/1000 | Loss: 0.00002816
Iteration 186/1000 | Loss: 0.00002815
Iteration 187/1000 | Loss: 0.00002815
Iteration 188/1000 | Loss: 0.00002815
Iteration 189/1000 | Loss: 0.00002815
Iteration 190/1000 | Loss: 0.00002815
Iteration 191/1000 | Loss: 0.00002815
Iteration 192/1000 | Loss: 0.00002815
Iteration 193/1000 | Loss: 0.00002814
Iteration 194/1000 | Loss: 0.00002814
Iteration 195/1000 | Loss: 0.00002814
Iteration 196/1000 | Loss: 0.00002814
Iteration 197/1000 | Loss: 0.00002814
Iteration 198/1000 | Loss: 0.00002814
Iteration 199/1000 | Loss: 0.00002814
Iteration 200/1000 | Loss: 0.00002814
Iteration 201/1000 | Loss: 0.00002814
Iteration 202/1000 | Loss: 0.00002814
Iteration 203/1000 | Loss: 0.00002814
Iteration 204/1000 | Loss: 0.00002814
Iteration 205/1000 | Loss: 0.00002814
Iteration 206/1000 | Loss: 0.00002814
Iteration 207/1000 | Loss: 0.00002814
Iteration 208/1000 | Loss: 0.00002814
Iteration 209/1000 | Loss: 0.00002814
Iteration 210/1000 | Loss: 0.00002814
Iteration 211/1000 | Loss: 0.00002814
Iteration 212/1000 | Loss: 0.00002814
Iteration 213/1000 | Loss: 0.00002814
Iteration 214/1000 | Loss: 0.00002814
Iteration 215/1000 | Loss: 0.00002814
Iteration 216/1000 | Loss: 0.00002814
Iteration 217/1000 | Loss: 0.00002814
Iteration 218/1000 | Loss: 0.00002814
Iteration 219/1000 | Loss: 0.00002814
Iteration 220/1000 | Loss: 0.00002814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [2.8143047529738396e-05, 2.8143047529738396e-05, 2.8143047529738396e-05, 2.8143047529738396e-05, 2.8143047529738396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8143047529738396e-05

Optimization complete. Final v2v error: 4.182437419891357 mm

Highest mean error: 5.62946891784668 mm for frame 126

Lowest mean error: 3.254368305206299 mm for frame 0

Saving results

Total time: 53.72269868850708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822877
Iteration 2/25 | Loss: 0.00142440
Iteration 3/25 | Loss: 0.00115001
Iteration 4/25 | Loss: 0.00112413
Iteration 5/25 | Loss: 0.00112159
Iteration 6/25 | Loss: 0.00112145
Iteration 7/25 | Loss: 0.00112145
Iteration 8/25 | Loss: 0.00112145
Iteration 9/25 | Loss: 0.00112145
Iteration 10/25 | Loss: 0.00112145
Iteration 11/25 | Loss: 0.00112145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011214453261345625, 0.0011214453261345625, 0.0011214453261345625, 0.0011214453261345625, 0.0011214453261345625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011214453261345625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96909267
Iteration 2/25 | Loss: 0.00041060
Iteration 3/25 | Loss: 0.00041059
Iteration 4/25 | Loss: 0.00041059
Iteration 5/25 | Loss: 0.00041059
Iteration 6/25 | Loss: 0.00041059
Iteration 7/25 | Loss: 0.00041059
Iteration 8/25 | Loss: 0.00041059
Iteration 9/25 | Loss: 0.00041059
Iteration 10/25 | Loss: 0.00041059
Iteration 11/25 | Loss: 0.00041059
Iteration 12/25 | Loss: 0.00041059
Iteration 13/25 | Loss: 0.00041059
Iteration 14/25 | Loss: 0.00041059
Iteration 15/25 | Loss: 0.00041059
Iteration 16/25 | Loss: 0.00041059
Iteration 17/25 | Loss: 0.00041059
Iteration 18/25 | Loss: 0.00041059
Iteration 19/25 | Loss: 0.00041059
Iteration 20/25 | Loss: 0.00041059
Iteration 21/25 | Loss: 0.00041059
Iteration 22/25 | Loss: 0.00041059
Iteration 23/25 | Loss: 0.00041059
Iteration 24/25 | Loss: 0.00041059
Iteration 25/25 | Loss: 0.00041059

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041059
Iteration 2/1000 | Loss: 0.00002836
Iteration 3/1000 | Loss: 0.00002101
Iteration 4/1000 | Loss: 0.00001871
Iteration 5/1000 | Loss: 0.00001802
Iteration 6/1000 | Loss: 0.00001763
Iteration 7/1000 | Loss: 0.00001728
Iteration 8/1000 | Loss: 0.00001693
Iteration 9/1000 | Loss: 0.00001661
Iteration 10/1000 | Loss: 0.00001640
Iteration 11/1000 | Loss: 0.00001624
Iteration 12/1000 | Loss: 0.00001614
Iteration 13/1000 | Loss: 0.00001614
Iteration 14/1000 | Loss: 0.00001613
Iteration 15/1000 | Loss: 0.00001613
Iteration 16/1000 | Loss: 0.00001613
Iteration 17/1000 | Loss: 0.00001613
Iteration 18/1000 | Loss: 0.00001613
Iteration 19/1000 | Loss: 0.00001602
Iteration 20/1000 | Loss: 0.00001601
Iteration 21/1000 | Loss: 0.00001595
Iteration 22/1000 | Loss: 0.00001591
Iteration 23/1000 | Loss: 0.00001591
Iteration 24/1000 | Loss: 0.00001590
Iteration 25/1000 | Loss: 0.00001589
Iteration 26/1000 | Loss: 0.00001589
Iteration 27/1000 | Loss: 0.00001589
Iteration 28/1000 | Loss: 0.00001588
Iteration 29/1000 | Loss: 0.00001588
Iteration 30/1000 | Loss: 0.00001588
Iteration 31/1000 | Loss: 0.00001587
Iteration 32/1000 | Loss: 0.00001587
Iteration 33/1000 | Loss: 0.00001587
Iteration 34/1000 | Loss: 0.00001586
Iteration 35/1000 | Loss: 0.00001586
Iteration 36/1000 | Loss: 0.00001586
Iteration 37/1000 | Loss: 0.00001585
Iteration 38/1000 | Loss: 0.00001585
Iteration 39/1000 | Loss: 0.00001584
Iteration 40/1000 | Loss: 0.00001583
Iteration 41/1000 | Loss: 0.00001579
Iteration 42/1000 | Loss: 0.00001578
Iteration 43/1000 | Loss: 0.00001578
Iteration 44/1000 | Loss: 0.00001578
Iteration 45/1000 | Loss: 0.00001578
Iteration 46/1000 | Loss: 0.00001577
Iteration 47/1000 | Loss: 0.00001577
Iteration 48/1000 | Loss: 0.00001577
Iteration 49/1000 | Loss: 0.00001577
Iteration 50/1000 | Loss: 0.00001577
Iteration 51/1000 | Loss: 0.00001577
Iteration 52/1000 | Loss: 0.00001577
Iteration 53/1000 | Loss: 0.00001577
Iteration 54/1000 | Loss: 0.00001577
Iteration 55/1000 | Loss: 0.00001576
Iteration 56/1000 | Loss: 0.00001576
Iteration 57/1000 | Loss: 0.00001576
Iteration 58/1000 | Loss: 0.00001576
Iteration 59/1000 | Loss: 0.00001576
Iteration 60/1000 | Loss: 0.00001575
Iteration 61/1000 | Loss: 0.00001575
Iteration 62/1000 | Loss: 0.00001575
Iteration 63/1000 | Loss: 0.00001575
Iteration 64/1000 | Loss: 0.00001575
Iteration 65/1000 | Loss: 0.00001575
Iteration 66/1000 | Loss: 0.00001575
Iteration 67/1000 | Loss: 0.00001574
Iteration 68/1000 | Loss: 0.00001574
Iteration 69/1000 | Loss: 0.00001574
Iteration 70/1000 | Loss: 0.00001574
Iteration 71/1000 | Loss: 0.00001574
Iteration 72/1000 | Loss: 0.00001574
Iteration 73/1000 | Loss: 0.00001574
Iteration 74/1000 | Loss: 0.00001574
Iteration 75/1000 | Loss: 0.00001573
Iteration 76/1000 | Loss: 0.00001573
Iteration 77/1000 | Loss: 0.00001573
Iteration 78/1000 | Loss: 0.00001573
Iteration 79/1000 | Loss: 0.00001573
Iteration 80/1000 | Loss: 0.00001573
Iteration 81/1000 | Loss: 0.00001573
Iteration 82/1000 | Loss: 0.00001573
Iteration 83/1000 | Loss: 0.00001573
Iteration 84/1000 | Loss: 0.00001573
Iteration 85/1000 | Loss: 0.00001573
Iteration 86/1000 | Loss: 0.00001572
Iteration 87/1000 | Loss: 0.00001572
Iteration 88/1000 | Loss: 0.00001572
Iteration 89/1000 | Loss: 0.00001571
Iteration 90/1000 | Loss: 0.00001570
Iteration 91/1000 | Loss: 0.00001569
Iteration 92/1000 | Loss: 0.00001569
Iteration 93/1000 | Loss: 0.00001569
Iteration 94/1000 | Loss: 0.00001569
Iteration 95/1000 | Loss: 0.00001569
Iteration 96/1000 | Loss: 0.00001569
Iteration 97/1000 | Loss: 0.00001569
Iteration 98/1000 | Loss: 0.00001568
Iteration 99/1000 | Loss: 0.00001568
Iteration 100/1000 | Loss: 0.00001568
Iteration 101/1000 | Loss: 0.00001568
Iteration 102/1000 | Loss: 0.00001568
Iteration 103/1000 | Loss: 0.00001568
Iteration 104/1000 | Loss: 0.00001568
Iteration 105/1000 | Loss: 0.00001567
Iteration 106/1000 | Loss: 0.00001567
Iteration 107/1000 | Loss: 0.00001567
Iteration 108/1000 | Loss: 0.00001567
Iteration 109/1000 | Loss: 0.00001567
Iteration 110/1000 | Loss: 0.00001567
Iteration 111/1000 | Loss: 0.00001567
Iteration 112/1000 | Loss: 0.00001567
Iteration 113/1000 | Loss: 0.00001567
Iteration 114/1000 | Loss: 0.00001567
Iteration 115/1000 | Loss: 0.00001567
Iteration 116/1000 | Loss: 0.00001567
Iteration 117/1000 | Loss: 0.00001567
Iteration 118/1000 | Loss: 0.00001567
Iteration 119/1000 | Loss: 0.00001567
Iteration 120/1000 | Loss: 0.00001567
Iteration 121/1000 | Loss: 0.00001567
Iteration 122/1000 | Loss: 0.00001567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.5667930711060762e-05, 1.5667930711060762e-05, 1.5667930711060762e-05, 1.5667930711060762e-05, 1.5667930711060762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5667930711060762e-05

Optimization complete. Final v2v error: 3.3177056312561035 mm

Highest mean error: 3.6540491580963135 mm for frame 25

Lowest mean error: 3.1295387744903564 mm for frame 140

Saving results

Total time: 34.79162859916687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568718
Iteration 2/25 | Loss: 0.00114671
Iteration 3/25 | Loss: 0.00106380
Iteration 4/25 | Loss: 0.00105556
Iteration 5/25 | Loss: 0.00105249
Iteration 6/25 | Loss: 0.00105194
Iteration 7/25 | Loss: 0.00105194
Iteration 8/25 | Loss: 0.00105194
Iteration 9/25 | Loss: 0.00105194
Iteration 10/25 | Loss: 0.00105194
Iteration 11/25 | Loss: 0.00105194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001051936182193458, 0.001051936182193458, 0.001051936182193458, 0.001051936182193458, 0.001051936182193458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001051936182193458

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.76993203
Iteration 2/25 | Loss: 0.00080052
Iteration 3/25 | Loss: 0.00080052
Iteration 4/25 | Loss: 0.00080051
Iteration 5/25 | Loss: 0.00080051
Iteration 6/25 | Loss: 0.00080051
Iteration 7/25 | Loss: 0.00080051
Iteration 8/25 | Loss: 0.00080051
Iteration 9/25 | Loss: 0.00080051
Iteration 10/25 | Loss: 0.00080051
Iteration 11/25 | Loss: 0.00080051
Iteration 12/25 | Loss: 0.00080051
Iteration 13/25 | Loss: 0.00080051
Iteration 14/25 | Loss: 0.00080051
Iteration 15/25 | Loss: 0.00080051
Iteration 16/25 | Loss: 0.00080051
Iteration 17/25 | Loss: 0.00080051
Iteration 18/25 | Loss: 0.00080051
Iteration 19/25 | Loss: 0.00080051
Iteration 20/25 | Loss: 0.00080051
Iteration 21/25 | Loss: 0.00080051
Iteration 22/25 | Loss: 0.00080051
Iteration 23/25 | Loss: 0.00080051
Iteration 24/25 | Loss: 0.00080051
Iteration 25/25 | Loss: 0.00080051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080051
Iteration 2/1000 | Loss: 0.00001842
Iteration 3/1000 | Loss: 0.00001214
Iteration 4/1000 | Loss: 0.00001087
Iteration 5/1000 | Loss: 0.00001025
Iteration 6/1000 | Loss: 0.00000980
Iteration 7/1000 | Loss: 0.00000959
Iteration 8/1000 | Loss: 0.00000958
Iteration 9/1000 | Loss: 0.00000949
Iteration 10/1000 | Loss: 0.00000932
Iteration 11/1000 | Loss: 0.00000922
Iteration 12/1000 | Loss: 0.00000922
Iteration 13/1000 | Loss: 0.00000918
Iteration 14/1000 | Loss: 0.00000917
Iteration 15/1000 | Loss: 0.00000914
Iteration 16/1000 | Loss: 0.00000914
Iteration 17/1000 | Loss: 0.00000912
Iteration 18/1000 | Loss: 0.00000911
Iteration 19/1000 | Loss: 0.00000910
Iteration 20/1000 | Loss: 0.00000909
Iteration 21/1000 | Loss: 0.00000909
Iteration 22/1000 | Loss: 0.00000909
Iteration 23/1000 | Loss: 0.00000905
Iteration 24/1000 | Loss: 0.00000901
Iteration 25/1000 | Loss: 0.00000899
Iteration 26/1000 | Loss: 0.00000898
Iteration 27/1000 | Loss: 0.00000897
Iteration 28/1000 | Loss: 0.00000897
Iteration 29/1000 | Loss: 0.00000897
Iteration 30/1000 | Loss: 0.00000897
Iteration 31/1000 | Loss: 0.00000897
Iteration 32/1000 | Loss: 0.00000897
Iteration 33/1000 | Loss: 0.00000896
Iteration 34/1000 | Loss: 0.00000896
Iteration 35/1000 | Loss: 0.00000896
Iteration 36/1000 | Loss: 0.00000896
Iteration 37/1000 | Loss: 0.00000895
Iteration 38/1000 | Loss: 0.00000894
Iteration 39/1000 | Loss: 0.00000894
Iteration 40/1000 | Loss: 0.00000893
Iteration 41/1000 | Loss: 0.00000892
Iteration 42/1000 | Loss: 0.00000891
Iteration 43/1000 | Loss: 0.00000891
Iteration 44/1000 | Loss: 0.00000890
Iteration 45/1000 | Loss: 0.00000889
Iteration 46/1000 | Loss: 0.00000889
Iteration 47/1000 | Loss: 0.00000888
Iteration 48/1000 | Loss: 0.00000888
Iteration 49/1000 | Loss: 0.00000887
Iteration 50/1000 | Loss: 0.00000887
Iteration 51/1000 | Loss: 0.00000886
Iteration 52/1000 | Loss: 0.00000886
Iteration 53/1000 | Loss: 0.00000886
Iteration 54/1000 | Loss: 0.00000885
Iteration 55/1000 | Loss: 0.00000885
Iteration 56/1000 | Loss: 0.00000884
Iteration 57/1000 | Loss: 0.00000884
Iteration 58/1000 | Loss: 0.00000883
Iteration 59/1000 | Loss: 0.00000883
Iteration 60/1000 | Loss: 0.00000881
Iteration 61/1000 | Loss: 0.00000881
Iteration 62/1000 | Loss: 0.00000881
Iteration 63/1000 | Loss: 0.00000881
Iteration 64/1000 | Loss: 0.00000880
Iteration 65/1000 | Loss: 0.00000880
Iteration 66/1000 | Loss: 0.00000880
Iteration 67/1000 | Loss: 0.00000880
Iteration 68/1000 | Loss: 0.00000880
Iteration 69/1000 | Loss: 0.00000880
Iteration 70/1000 | Loss: 0.00000880
Iteration 71/1000 | Loss: 0.00000880
Iteration 72/1000 | Loss: 0.00000880
Iteration 73/1000 | Loss: 0.00000879
Iteration 74/1000 | Loss: 0.00000879
Iteration 75/1000 | Loss: 0.00000879
Iteration 76/1000 | Loss: 0.00000878
Iteration 77/1000 | Loss: 0.00000878
Iteration 78/1000 | Loss: 0.00000877
Iteration 79/1000 | Loss: 0.00000877
Iteration 80/1000 | Loss: 0.00000877
Iteration 81/1000 | Loss: 0.00000876
Iteration 82/1000 | Loss: 0.00000876
Iteration 83/1000 | Loss: 0.00000876
Iteration 84/1000 | Loss: 0.00000876
Iteration 85/1000 | Loss: 0.00000876
Iteration 86/1000 | Loss: 0.00000875
Iteration 87/1000 | Loss: 0.00000874
Iteration 88/1000 | Loss: 0.00000873
Iteration 89/1000 | Loss: 0.00000873
Iteration 90/1000 | Loss: 0.00000873
Iteration 91/1000 | Loss: 0.00000873
Iteration 92/1000 | Loss: 0.00000873
Iteration 93/1000 | Loss: 0.00000873
Iteration 94/1000 | Loss: 0.00000873
Iteration 95/1000 | Loss: 0.00000873
Iteration 96/1000 | Loss: 0.00000872
Iteration 97/1000 | Loss: 0.00000872
Iteration 98/1000 | Loss: 0.00000872
Iteration 99/1000 | Loss: 0.00000872
Iteration 100/1000 | Loss: 0.00000872
Iteration 101/1000 | Loss: 0.00000872
Iteration 102/1000 | Loss: 0.00000872
Iteration 103/1000 | Loss: 0.00000871
Iteration 104/1000 | Loss: 0.00000871
Iteration 105/1000 | Loss: 0.00000870
Iteration 106/1000 | Loss: 0.00000868
Iteration 107/1000 | Loss: 0.00000868
Iteration 108/1000 | Loss: 0.00000868
Iteration 109/1000 | Loss: 0.00000868
Iteration 110/1000 | Loss: 0.00000867
Iteration 111/1000 | Loss: 0.00000867
Iteration 112/1000 | Loss: 0.00000867
Iteration 113/1000 | Loss: 0.00000867
Iteration 114/1000 | Loss: 0.00000867
Iteration 115/1000 | Loss: 0.00000867
Iteration 116/1000 | Loss: 0.00000867
Iteration 117/1000 | Loss: 0.00000866
Iteration 118/1000 | Loss: 0.00000866
Iteration 119/1000 | Loss: 0.00000866
Iteration 120/1000 | Loss: 0.00000866
Iteration 121/1000 | Loss: 0.00000866
Iteration 122/1000 | Loss: 0.00000865
Iteration 123/1000 | Loss: 0.00000865
Iteration 124/1000 | Loss: 0.00000865
Iteration 125/1000 | Loss: 0.00000865
Iteration 126/1000 | Loss: 0.00000865
Iteration 127/1000 | Loss: 0.00000865
Iteration 128/1000 | Loss: 0.00000864
Iteration 129/1000 | Loss: 0.00000864
Iteration 130/1000 | Loss: 0.00000864
Iteration 131/1000 | Loss: 0.00000864
Iteration 132/1000 | Loss: 0.00000864
Iteration 133/1000 | Loss: 0.00000864
Iteration 134/1000 | Loss: 0.00000864
Iteration 135/1000 | Loss: 0.00000864
Iteration 136/1000 | Loss: 0.00000864
Iteration 137/1000 | Loss: 0.00000863
Iteration 138/1000 | Loss: 0.00000863
Iteration 139/1000 | Loss: 0.00000863
Iteration 140/1000 | Loss: 0.00000863
Iteration 141/1000 | Loss: 0.00000863
Iteration 142/1000 | Loss: 0.00000863
Iteration 143/1000 | Loss: 0.00000863
Iteration 144/1000 | Loss: 0.00000862
Iteration 145/1000 | Loss: 0.00000862
Iteration 146/1000 | Loss: 0.00000862
Iteration 147/1000 | Loss: 0.00000861
Iteration 148/1000 | Loss: 0.00000861
Iteration 149/1000 | Loss: 0.00000861
Iteration 150/1000 | Loss: 0.00000861
Iteration 151/1000 | Loss: 0.00000860
Iteration 152/1000 | Loss: 0.00000860
Iteration 153/1000 | Loss: 0.00000860
Iteration 154/1000 | Loss: 0.00000860
Iteration 155/1000 | Loss: 0.00000860
Iteration 156/1000 | Loss: 0.00000860
Iteration 157/1000 | Loss: 0.00000860
Iteration 158/1000 | Loss: 0.00000860
Iteration 159/1000 | Loss: 0.00000859
Iteration 160/1000 | Loss: 0.00000859
Iteration 161/1000 | Loss: 0.00000859
Iteration 162/1000 | Loss: 0.00000859
Iteration 163/1000 | Loss: 0.00000859
Iteration 164/1000 | Loss: 0.00000859
Iteration 165/1000 | Loss: 0.00000859
Iteration 166/1000 | Loss: 0.00000859
Iteration 167/1000 | Loss: 0.00000858
Iteration 168/1000 | Loss: 0.00000858
Iteration 169/1000 | Loss: 0.00000858
Iteration 170/1000 | Loss: 0.00000858
Iteration 171/1000 | Loss: 0.00000858
Iteration 172/1000 | Loss: 0.00000858
Iteration 173/1000 | Loss: 0.00000858
Iteration 174/1000 | Loss: 0.00000858
Iteration 175/1000 | Loss: 0.00000858
Iteration 176/1000 | Loss: 0.00000858
Iteration 177/1000 | Loss: 0.00000858
Iteration 178/1000 | Loss: 0.00000858
Iteration 179/1000 | Loss: 0.00000858
Iteration 180/1000 | Loss: 0.00000858
Iteration 181/1000 | Loss: 0.00000858
Iteration 182/1000 | Loss: 0.00000858
Iteration 183/1000 | Loss: 0.00000857
Iteration 184/1000 | Loss: 0.00000857
Iteration 185/1000 | Loss: 0.00000857
Iteration 186/1000 | Loss: 0.00000857
Iteration 187/1000 | Loss: 0.00000857
Iteration 188/1000 | Loss: 0.00000857
Iteration 189/1000 | Loss: 0.00000857
Iteration 190/1000 | Loss: 0.00000856
Iteration 191/1000 | Loss: 0.00000856
Iteration 192/1000 | Loss: 0.00000856
Iteration 193/1000 | Loss: 0.00000856
Iteration 194/1000 | Loss: 0.00000856
Iteration 195/1000 | Loss: 0.00000856
Iteration 196/1000 | Loss: 0.00000856
Iteration 197/1000 | Loss: 0.00000856
Iteration 198/1000 | Loss: 0.00000856
Iteration 199/1000 | Loss: 0.00000856
Iteration 200/1000 | Loss: 0.00000856
Iteration 201/1000 | Loss: 0.00000856
Iteration 202/1000 | Loss: 0.00000856
Iteration 203/1000 | Loss: 0.00000856
Iteration 204/1000 | Loss: 0.00000856
Iteration 205/1000 | Loss: 0.00000856
Iteration 206/1000 | Loss: 0.00000856
Iteration 207/1000 | Loss: 0.00000856
Iteration 208/1000 | Loss: 0.00000856
Iteration 209/1000 | Loss: 0.00000856
Iteration 210/1000 | Loss: 0.00000856
Iteration 211/1000 | Loss: 0.00000856
Iteration 212/1000 | Loss: 0.00000856
Iteration 213/1000 | Loss: 0.00000856
Iteration 214/1000 | Loss: 0.00000856
Iteration 215/1000 | Loss: 0.00000856
Iteration 216/1000 | Loss: 0.00000856
Iteration 217/1000 | Loss: 0.00000856
Iteration 218/1000 | Loss: 0.00000856
Iteration 219/1000 | Loss: 0.00000856
Iteration 220/1000 | Loss: 0.00000856
Iteration 221/1000 | Loss: 0.00000856
Iteration 222/1000 | Loss: 0.00000856
Iteration 223/1000 | Loss: 0.00000856
Iteration 224/1000 | Loss: 0.00000856
Iteration 225/1000 | Loss: 0.00000856
Iteration 226/1000 | Loss: 0.00000856
Iteration 227/1000 | Loss: 0.00000856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [8.560536116419826e-06, 8.560536116419826e-06, 8.560536116419826e-06, 8.560536116419826e-06, 8.560536116419826e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.560536116419826e-06

Optimization complete. Final v2v error: 2.527406692504883 mm

Highest mean error: 2.8984076976776123 mm for frame 112

Lowest mean error: 2.3761184215545654 mm for frame 40

Saving results

Total time: 37.98251700401306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810964
Iteration 2/25 | Loss: 0.00135443
Iteration 3/25 | Loss: 0.00117094
Iteration 4/25 | Loss: 0.00115433
Iteration 5/25 | Loss: 0.00115321
Iteration 6/25 | Loss: 0.00115321
Iteration 7/25 | Loss: 0.00115321
Iteration 8/25 | Loss: 0.00115321
Iteration 9/25 | Loss: 0.00115321
Iteration 10/25 | Loss: 0.00115321
Iteration 11/25 | Loss: 0.00115321
Iteration 12/25 | Loss: 0.00115321
Iteration 13/25 | Loss: 0.00115321
Iteration 14/25 | Loss: 0.00115321
Iteration 15/25 | Loss: 0.00115321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011532142525538802, 0.0011532142525538802, 0.0011532142525538802, 0.0011532142525538802, 0.0011532142525538802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011532142525538802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26285660
Iteration 2/25 | Loss: 0.00062570
Iteration 3/25 | Loss: 0.00062569
Iteration 4/25 | Loss: 0.00062569
Iteration 5/25 | Loss: 0.00062569
Iteration 6/25 | Loss: 0.00062569
Iteration 7/25 | Loss: 0.00062569
Iteration 8/25 | Loss: 0.00062569
Iteration 9/25 | Loss: 0.00062569
Iteration 10/25 | Loss: 0.00062569
Iteration 11/25 | Loss: 0.00062569
Iteration 12/25 | Loss: 0.00062569
Iteration 13/25 | Loss: 0.00062569
Iteration 14/25 | Loss: 0.00062569
Iteration 15/25 | Loss: 0.00062569
Iteration 16/25 | Loss: 0.00062569
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006256902124732733, 0.0006256902124732733, 0.0006256902124732733, 0.0006256902124732733, 0.0006256902124732733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006256902124732733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062569
Iteration 2/1000 | Loss: 0.00002705
Iteration 3/1000 | Loss: 0.00002099
Iteration 4/1000 | Loss: 0.00001937
Iteration 5/1000 | Loss: 0.00001846
Iteration 6/1000 | Loss: 0.00001797
Iteration 7/1000 | Loss: 0.00001751
Iteration 8/1000 | Loss: 0.00001707
Iteration 9/1000 | Loss: 0.00001669
Iteration 10/1000 | Loss: 0.00001653
Iteration 11/1000 | Loss: 0.00001635
Iteration 12/1000 | Loss: 0.00001618
Iteration 13/1000 | Loss: 0.00001616
Iteration 14/1000 | Loss: 0.00001610
Iteration 15/1000 | Loss: 0.00001604
Iteration 16/1000 | Loss: 0.00001593
Iteration 17/1000 | Loss: 0.00001586
Iteration 18/1000 | Loss: 0.00001586
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001584
Iteration 21/1000 | Loss: 0.00001584
Iteration 22/1000 | Loss: 0.00001584
Iteration 23/1000 | Loss: 0.00001584
Iteration 24/1000 | Loss: 0.00001584
Iteration 25/1000 | Loss: 0.00001584
Iteration 26/1000 | Loss: 0.00001584
Iteration 27/1000 | Loss: 0.00001584
Iteration 28/1000 | Loss: 0.00001584
Iteration 29/1000 | Loss: 0.00001584
Iteration 30/1000 | Loss: 0.00001582
Iteration 31/1000 | Loss: 0.00001582
Iteration 32/1000 | Loss: 0.00001582
Iteration 33/1000 | Loss: 0.00001582
Iteration 34/1000 | Loss: 0.00001580
Iteration 35/1000 | Loss: 0.00001571
Iteration 36/1000 | Loss: 0.00001570
Iteration 37/1000 | Loss: 0.00001570
Iteration 38/1000 | Loss: 0.00001570
Iteration 39/1000 | Loss: 0.00001570
Iteration 40/1000 | Loss: 0.00001570
Iteration 41/1000 | Loss: 0.00001570
Iteration 42/1000 | Loss: 0.00001570
Iteration 43/1000 | Loss: 0.00001569
Iteration 44/1000 | Loss: 0.00001569
Iteration 45/1000 | Loss: 0.00001567
Iteration 46/1000 | Loss: 0.00001566
Iteration 47/1000 | Loss: 0.00001566
Iteration 48/1000 | Loss: 0.00001565
Iteration 49/1000 | Loss: 0.00001565
Iteration 50/1000 | Loss: 0.00001565
Iteration 51/1000 | Loss: 0.00001565
Iteration 52/1000 | Loss: 0.00001565
Iteration 53/1000 | Loss: 0.00001565
Iteration 54/1000 | Loss: 0.00001565
Iteration 55/1000 | Loss: 0.00001565
Iteration 56/1000 | Loss: 0.00001564
Iteration 57/1000 | Loss: 0.00001564
Iteration 58/1000 | Loss: 0.00001563
Iteration 59/1000 | Loss: 0.00001563
Iteration 60/1000 | Loss: 0.00001562
Iteration 61/1000 | Loss: 0.00001562
Iteration 62/1000 | Loss: 0.00001562
Iteration 63/1000 | Loss: 0.00001562
Iteration 64/1000 | Loss: 0.00001560
Iteration 65/1000 | Loss: 0.00001560
Iteration 66/1000 | Loss: 0.00001560
Iteration 67/1000 | Loss: 0.00001559
Iteration 68/1000 | Loss: 0.00001558
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001558
Iteration 71/1000 | Loss: 0.00001557
Iteration 72/1000 | Loss: 0.00001557
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00001556
Iteration 75/1000 | Loss: 0.00001555
Iteration 76/1000 | Loss: 0.00001554
Iteration 77/1000 | Loss: 0.00001554
Iteration 78/1000 | Loss: 0.00001554
Iteration 79/1000 | Loss: 0.00001554
Iteration 80/1000 | Loss: 0.00001554
Iteration 81/1000 | Loss: 0.00001553
Iteration 82/1000 | Loss: 0.00001553
Iteration 83/1000 | Loss: 0.00001553
Iteration 84/1000 | Loss: 0.00001552
Iteration 85/1000 | Loss: 0.00001552
Iteration 86/1000 | Loss: 0.00001552
Iteration 87/1000 | Loss: 0.00001552
Iteration 88/1000 | Loss: 0.00001551
Iteration 89/1000 | Loss: 0.00001551
Iteration 90/1000 | Loss: 0.00001551
Iteration 91/1000 | Loss: 0.00001551
Iteration 92/1000 | Loss: 0.00001551
Iteration 93/1000 | Loss: 0.00001551
Iteration 94/1000 | Loss: 0.00001551
Iteration 95/1000 | Loss: 0.00001551
Iteration 96/1000 | Loss: 0.00001551
Iteration 97/1000 | Loss: 0.00001551
Iteration 98/1000 | Loss: 0.00001551
Iteration 99/1000 | Loss: 0.00001551
Iteration 100/1000 | Loss: 0.00001551
Iteration 101/1000 | Loss: 0.00001551
Iteration 102/1000 | Loss: 0.00001551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.5506724594160914e-05, 1.5506724594160914e-05, 1.5506724594160914e-05, 1.5506724594160914e-05, 1.5506724594160914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5506724594160914e-05

Optimization complete. Final v2v error: 3.3406331539154053 mm

Highest mean error: 3.5120794773101807 mm for frame 210

Lowest mean error: 3.21079683303833 mm for frame 60

Saving results

Total time: 38.7317271232605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00642716
Iteration 2/25 | Loss: 0.00137106
Iteration 3/25 | Loss: 0.00121821
Iteration 4/25 | Loss: 0.00120471
Iteration 5/25 | Loss: 0.00120123
Iteration 6/25 | Loss: 0.00120088
Iteration 7/25 | Loss: 0.00120088
Iteration 8/25 | Loss: 0.00120088
Iteration 9/25 | Loss: 0.00120088
Iteration 10/25 | Loss: 0.00120088
Iteration 11/25 | Loss: 0.00120088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012008774792775512, 0.0012008774792775512, 0.0012008774792775512, 0.0012008774792775512, 0.0012008774792775512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012008774792775512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56694961
Iteration 2/25 | Loss: 0.00094266
Iteration 3/25 | Loss: 0.00094266
Iteration 4/25 | Loss: 0.00094266
Iteration 5/25 | Loss: 0.00094266
Iteration 6/25 | Loss: 0.00094266
Iteration 7/25 | Loss: 0.00094266
Iteration 8/25 | Loss: 0.00094266
Iteration 9/25 | Loss: 0.00094266
Iteration 10/25 | Loss: 0.00094266
Iteration 11/25 | Loss: 0.00094265
Iteration 12/25 | Loss: 0.00094265
Iteration 13/25 | Loss: 0.00094265
Iteration 14/25 | Loss: 0.00094265
Iteration 15/25 | Loss: 0.00094265
Iteration 16/25 | Loss: 0.00094265
Iteration 17/25 | Loss: 0.00094265
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009426549077033997, 0.0009426549077033997, 0.0009426549077033997, 0.0009426549077033997, 0.0009426549077033997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009426549077033997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094265
Iteration 2/1000 | Loss: 0.00006539
Iteration 3/1000 | Loss: 0.00003515
Iteration 4/1000 | Loss: 0.00002837
Iteration 5/1000 | Loss: 0.00002521
Iteration 6/1000 | Loss: 0.00002361
Iteration 7/1000 | Loss: 0.00002254
Iteration 8/1000 | Loss: 0.00002176
Iteration 9/1000 | Loss: 0.00002125
Iteration 10/1000 | Loss: 0.00002092
Iteration 11/1000 | Loss: 0.00002065
Iteration 12/1000 | Loss: 0.00002041
Iteration 13/1000 | Loss: 0.00002029
Iteration 14/1000 | Loss: 0.00002017
Iteration 15/1000 | Loss: 0.00002012
Iteration 16/1000 | Loss: 0.00001999
Iteration 17/1000 | Loss: 0.00001990
Iteration 18/1000 | Loss: 0.00001983
Iteration 19/1000 | Loss: 0.00001976
Iteration 20/1000 | Loss: 0.00001973
Iteration 21/1000 | Loss: 0.00001971
Iteration 22/1000 | Loss: 0.00001970
Iteration 23/1000 | Loss: 0.00001970
Iteration 24/1000 | Loss: 0.00001970
Iteration 25/1000 | Loss: 0.00001970
Iteration 26/1000 | Loss: 0.00001970
Iteration 27/1000 | Loss: 0.00001970
Iteration 28/1000 | Loss: 0.00001970
Iteration 29/1000 | Loss: 0.00001970
Iteration 30/1000 | Loss: 0.00001969
Iteration 31/1000 | Loss: 0.00001969
Iteration 32/1000 | Loss: 0.00001968
Iteration 33/1000 | Loss: 0.00001968
Iteration 34/1000 | Loss: 0.00001968
Iteration 35/1000 | Loss: 0.00001967
Iteration 36/1000 | Loss: 0.00001967
Iteration 37/1000 | Loss: 0.00001967
Iteration 38/1000 | Loss: 0.00001966
Iteration 39/1000 | Loss: 0.00001966
Iteration 40/1000 | Loss: 0.00001966
Iteration 41/1000 | Loss: 0.00001965
Iteration 42/1000 | Loss: 0.00001965
Iteration 43/1000 | Loss: 0.00001964
Iteration 44/1000 | Loss: 0.00001964
Iteration 45/1000 | Loss: 0.00001963
Iteration 46/1000 | Loss: 0.00001963
Iteration 47/1000 | Loss: 0.00001963
Iteration 48/1000 | Loss: 0.00001962
Iteration 49/1000 | Loss: 0.00001962
Iteration 50/1000 | Loss: 0.00001962
Iteration 51/1000 | Loss: 0.00001961
Iteration 52/1000 | Loss: 0.00001960
Iteration 53/1000 | Loss: 0.00001959
Iteration 54/1000 | Loss: 0.00001958
Iteration 55/1000 | Loss: 0.00001958
Iteration 56/1000 | Loss: 0.00001958
Iteration 57/1000 | Loss: 0.00001958
Iteration 58/1000 | Loss: 0.00001958
Iteration 59/1000 | Loss: 0.00001957
Iteration 60/1000 | Loss: 0.00001957
Iteration 61/1000 | Loss: 0.00001957
Iteration 62/1000 | Loss: 0.00001956
Iteration 63/1000 | Loss: 0.00001956
Iteration 64/1000 | Loss: 0.00001955
Iteration 65/1000 | Loss: 0.00001955
Iteration 66/1000 | Loss: 0.00001954
Iteration 67/1000 | Loss: 0.00001954
Iteration 68/1000 | Loss: 0.00001953
Iteration 69/1000 | Loss: 0.00001953
Iteration 70/1000 | Loss: 0.00001953
Iteration 71/1000 | Loss: 0.00001952
Iteration 72/1000 | Loss: 0.00001952
Iteration 73/1000 | Loss: 0.00001952
Iteration 74/1000 | Loss: 0.00001951
Iteration 75/1000 | Loss: 0.00001951
Iteration 76/1000 | Loss: 0.00001951
Iteration 77/1000 | Loss: 0.00001950
Iteration 78/1000 | Loss: 0.00001949
Iteration 79/1000 | Loss: 0.00001949
Iteration 80/1000 | Loss: 0.00001949
Iteration 81/1000 | Loss: 0.00001949
Iteration 82/1000 | Loss: 0.00001949
Iteration 83/1000 | Loss: 0.00001949
Iteration 84/1000 | Loss: 0.00001949
Iteration 85/1000 | Loss: 0.00001949
Iteration 86/1000 | Loss: 0.00001949
Iteration 87/1000 | Loss: 0.00001948
Iteration 88/1000 | Loss: 0.00001948
Iteration 89/1000 | Loss: 0.00001947
Iteration 90/1000 | Loss: 0.00001947
Iteration 91/1000 | Loss: 0.00001947
Iteration 92/1000 | Loss: 0.00001947
Iteration 93/1000 | Loss: 0.00001946
Iteration 94/1000 | Loss: 0.00001946
Iteration 95/1000 | Loss: 0.00001946
Iteration 96/1000 | Loss: 0.00001945
Iteration 97/1000 | Loss: 0.00001945
Iteration 98/1000 | Loss: 0.00001945
Iteration 99/1000 | Loss: 0.00001944
Iteration 100/1000 | Loss: 0.00001944
Iteration 101/1000 | Loss: 0.00001944
Iteration 102/1000 | Loss: 0.00001944
Iteration 103/1000 | Loss: 0.00001943
Iteration 104/1000 | Loss: 0.00001943
Iteration 105/1000 | Loss: 0.00001943
Iteration 106/1000 | Loss: 0.00001942
Iteration 107/1000 | Loss: 0.00001942
Iteration 108/1000 | Loss: 0.00001942
Iteration 109/1000 | Loss: 0.00001942
Iteration 110/1000 | Loss: 0.00001941
Iteration 111/1000 | Loss: 0.00001941
Iteration 112/1000 | Loss: 0.00001940
Iteration 113/1000 | Loss: 0.00001940
Iteration 114/1000 | Loss: 0.00001940
Iteration 115/1000 | Loss: 0.00001939
Iteration 116/1000 | Loss: 0.00001939
Iteration 117/1000 | Loss: 0.00001939
Iteration 118/1000 | Loss: 0.00001939
Iteration 119/1000 | Loss: 0.00001938
Iteration 120/1000 | Loss: 0.00001938
Iteration 121/1000 | Loss: 0.00001938
Iteration 122/1000 | Loss: 0.00001937
Iteration 123/1000 | Loss: 0.00001937
Iteration 124/1000 | Loss: 0.00001936
Iteration 125/1000 | Loss: 0.00001936
Iteration 126/1000 | Loss: 0.00001936
Iteration 127/1000 | Loss: 0.00001935
Iteration 128/1000 | Loss: 0.00001935
Iteration 129/1000 | Loss: 0.00001935
Iteration 130/1000 | Loss: 0.00001934
Iteration 131/1000 | Loss: 0.00001934
Iteration 132/1000 | Loss: 0.00001934
Iteration 133/1000 | Loss: 0.00001934
Iteration 134/1000 | Loss: 0.00001934
Iteration 135/1000 | Loss: 0.00001933
Iteration 136/1000 | Loss: 0.00001933
Iteration 137/1000 | Loss: 0.00001933
Iteration 138/1000 | Loss: 0.00001933
Iteration 139/1000 | Loss: 0.00001933
Iteration 140/1000 | Loss: 0.00001933
Iteration 141/1000 | Loss: 0.00001932
Iteration 142/1000 | Loss: 0.00001932
Iteration 143/1000 | Loss: 0.00001932
Iteration 144/1000 | Loss: 0.00001932
Iteration 145/1000 | Loss: 0.00001932
Iteration 146/1000 | Loss: 0.00001932
Iteration 147/1000 | Loss: 0.00001931
Iteration 148/1000 | Loss: 0.00001931
Iteration 149/1000 | Loss: 0.00001931
Iteration 150/1000 | Loss: 0.00001931
Iteration 151/1000 | Loss: 0.00001930
Iteration 152/1000 | Loss: 0.00001930
Iteration 153/1000 | Loss: 0.00001930
Iteration 154/1000 | Loss: 0.00001929
Iteration 155/1000 | Loss: 0.00001929
Iteration 156/1000 | Loss: 0.00001929
Iteration 157/1000 | Loss: 0.00001928
Iteration 158/1000 | Loss: 0.00001928
Iteration 159/1000 | Loss: 0.00001928
Iteration 160/1000 | Loss: 0.00001928
Iteration 161/1000 | Loss: 0.00001927
Iteration 162/1000 | Loss: 0.00001927
Iteration 163/1000 | Loss: 0.00001927
Iteration 164/1000 | Loss: 0.00001927
Iteration 165/1000 | Loss: 0.00001926
Iteration 166/1000 | Loss: 0.00001926
Iteration 167/1000 | Loss: 0.00001926
Iteration 168/1000 | Loss: 0.00001926
Iteration 169/1000 | Loss: 0.00001926
Iteration 170/1000 | Loss: 0.00001925
Iteration 171/1000 | Loss: 0.00001925
Iteration 172/1000 | Loss: 0.00001925
Iteration 173/1000 | Loss: 0.00001925
Iteration 174/1000 | Loss: 0.00001925
Iteration 175/1000 | Loss: 0.00001924
Iteration 176/1000 | Loss: 0.00001924
Iteration 177/1000 | Loss: 0.00001924
Iteration 178/1000 | Loss: 0.00001924
Iteration 179/1000 | Loss: 0.00001924
Iteration 180/1000 | Loss: 0.00001924
Iteration 181/1000 | Loss: 0.00001924
Iteration 182/1000 | Loss: 0.00001924
Iteration 183/1000 | Loss: 0.00001923
Iteration 184/1000 | Loss: 0.00001923
Iteration 185/1000 | Loss: 0.00001923
Iteration 186/1000 | Loss: 0.00001923
Iteration 187/1000 | Loss: 0.00001923
Iteration 188/1000 | Loss: 0.00001923
Iteration 189/1000 | Loss: 0.00001923
Iteration 190/1000 | Loss: 0.00001923
Iteration 191/1000 | Loss: 0.00001923
Iteration 192/1000 | Loss: 0.00001923
Iteration 193/1000 | Loss: 0.00001923
Iteration 194/1000 | Loss: 0.00001922
Iteration 195/1000 | Loss: 0.00001922
Iteration 196/1000 | Loss: 0.00001922
Iteration 197/1000 | Loss: 0.00001922
Iteration 198/1000 | Loss: 0.00001922
Iteration 199/1000 | Loss: 0.00001922
Iteration 200/1000 | Loss: 0.00001922
Iteration 201/1000 | Loss: 0.00001922
Iteration 202/1000 | Loss: 0.00001922
Iteration 203/1000 | Loss: 0.00001922
Iteration 204/1000 | Loss: 0.00001922
Iteration 205/1000 | Loss: 0.00001922
Iteration 206/1000 | Loss: 0.00001922
Iteration 207/1000 | Loss: 0.00001921
Iteration 208/1000 | Loss: 0.00001921
Iteration 209/1000 | Loss: 0.00001921
Iteration 210/1000 | Loss: 0.00001921
Iteration 211/1000 | Loss: 0.00001921
Iteration 212/1000 | Loss: 0.00001921
Iteration 213/1000 | Loss: 0.00001921
Iteration 214/1000 | Loss: 0.00001921
Iteration 215/1000 | Loss: 0.00001921
Iteration 216/1000 | Loss: 0.00001921
Iteration 217/1000 | Loss: 0.00001920
Iteration 218/1000 | Loss: 0.00001920
Iteration 219/1000 | Loss: 0.00001920
Iteration 220/1000 | Loss: 0.00001920
Iteration 221/1000 | Loss: 0.00001920
Iteration 222/1000 | Loss: 0.00001920
Iteration 223/1000 | Loss: 0.00001919
Iteration 224/1000 | Loss: 0.00001919
Iteration 225/1000 | Loss: 0.00001919
Iteration 226/1000 | Loss: 0.00001919
Iteration 227/1000 | Loss: 0.00001919
Iteration 228/1000 | Loss: 0.00001919
Iteration 229/1000 | Loss: 0.00001919
Iteration 230/1000 | Loss: 0.00001919
Iteration 231/1000 | Loss: 0.00001919
Iteration 232/1000 | Loss: 0.00001919
Iteration 233/1000 | Loss: 0.00001919
Iteration 234/1000 | Loss: 0.00001919
Iteration 235/1000 | Loss: 0.00001919
Iteration 236/1000 | Loss: 0.00001919
Iteration 237/1000 | Loss: 0.00001919
Iteration 238/1000 | Loss: 0.00001919
Iteration 239/1000 | Loss: 0.00001919
Iteration 240/1000 | Loss: 0.00001918
Iteration 241/1000 | Loss: 0.00001918
Iteration 242/1000 | Loss: 0.00001918
Iteration 243/1000 | Loss: 0.00001918
Iteration 244/1000 | Loss: 0.00001918
Iteration 245/1000 | Loss: 0.00001918
Iteration 246/1000 | Loss: 0.00001917
Iteration 247/1000 | Loss: 0.00001917
Iteration 248/1000 | Loss: 0.00001917
Iteration 249/1000 | Loss: 0.00001917
Iteration 250/1000 | Loss: 0.00001917
Iteration 251/1000 | Loss: 0.00001917
Iteration 252/1000 | Loss: 0.00001917
Iteration 253/1000 | Loss: 0.00001917
Iteration 254/1000 | Loss: 0.00001917
Iteration 255/1000 | Loss: 0.00001917
Iteration 256/1000 | Loss: 0.00001917
Iteration 257/1000 | Loss: 0.00001917
Iteration 258/1000 | Loss: 0.00001916
Iteration 259/1000 | Loss: 0.00001916
Iteration 260/1000 | Loss: 0.00001916
Iteration 261/1000 | Loss: 0.00001916
Iteration 262/1000 | Loss: 0.00001916
Iteration 263/1000 | Loss: 0.00001916
Iteration 264/1000 | Loss: 0.00001916
Iteration 265/1000 | Loss: 0.00001916
Iteration 266/1000 | Loss: 0.00001916
Iteration 267/1000 | Loss: 0.00001916
Iteration 268/1000 | Loss: 0.00001916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [1.9161621821695007e-05, 1.9161621821695007e-05, 1.9161621821695007e-05, 1.9161621821695007e-05, 1.9161621821695007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9161621821695007e-05

Optimization complete. Final v2v error: 3.542982339859009 mm

Highest mean error: 4.551489353179932 mm for frame 80

Lowest mean error: 2.8722598552703857 mm for frame 38

Saving results

Total time: 58.73012375831604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827819
Iteration 2/25 | Loss: 0.00144415
Iteration 3/25 | Loss: 0.00119147
Iteration 4/25 | Loss: 0.00116718
Iteration 5/25 | Loss: 0.00116496
Iteration 6/25 | Loss: 0.00116490
Iteration 7/25 | Loss: 0.00116490
Iteration 8/25 | Loss: 0.00116490
Iteration 9/25 | Loss: 0.00116490
Iteration 10/25 | Loss: 0.00116490
Iteration 11/25 | Loss: 0.00116490
Iteration 12/25 | Loss: 0.00116490
Iteration 13/25 | Loss: 0.00116490
Iteration 14/25 | Loss: 0.00116490
Iteration 15/25 | Loss: 0.00116490
Iteration 16/25 | Loss: 0.00116490
Iteration 17/25 | Loss: 0.00116490
Iteration 18/25 | Loss: 0.00116490
Iteration 19/25 | Loss: 0.00116490
Iteration 20/25 | Loss: 0.00116490
Iteration 21/25 | Loss: 0.00116490
Iteration 22/25 | Loss: 0.00116490
Iteration 23/25 | Loss: 0.00116490
Iteration 24/25 | Loss: 0.00116490
Iteration 25/25 | Loss: 0.00116490

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97275245
Iteration 2/25 | Loss: 0.00041357
Iteration 3/25 | Loss: 0.00041357
Iteration 4/25 | Loss: 0.00041356
Iteration 5/25 | Loss: 0.00041356
Iteration 6/25 | Loss: 0.00041356
Iteration 7/25 | Loss: 0.00041356
Iteration 8/25 | Loss: 0.00041356
Iteration 9/25 | Loss: 0.00041356
Iteration 10/25 | Loss: 0.00041356
Iteration 11/25 | Loss: 0.00041356
Iteration 12/25 | Loss: 0.00041356
Iteration 13/25 | Loss: 0.00041356
Iteration 14/25 | Loss: 0.00041356
Iteration 15/25 | Loss: 0.00041356
Iteration 16/25 | Loss: 0.00041356
Iteration 17/25 | Loss: 0.00041356
Iteration 18/25 | Loss: 0.00041356
Iteration 19/25 | Loss: 0.00041356
Iteration 20/25 | Loss: 0.00041356
Iteration 21/25 | Loss: 0.00041356
Iteration 22/25 | Loss: 0.00041356
Iteration 23/25 | Loss: 0.00041356
Iteration 24/25 | Loss: 0.00041356
Iteration 25/25 | Loss: 0.00041356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041356
Iteration 2/1000 | Loss: 0.00003337
Iteration 3/1000 | Loss: 0.00002581
Iteration 4/1000 | Loss: 0.00002344
Iteration 5/1000 | Loss: 0.00002252
Iteration 6/1000 | Loss: 0.00002214
Iteration 7/1000 | Loss: 0.00002151
Iteration 8/1000 | Loss: 0.00002115
Iteration 9/1000 | Loss: 0.00002089
Iteration 10/1000 | Loss: 0.00002066
Iteration 11/1000 | Loss: 0.00002051
Iteration 12/1000 | Loss: 0.00002047
Iteration 13/1000 | Loss: 0.00002047
Iteration 14/1000 | Loss: 0.00002047
Iteration 15/1000 | Loss: 0.00002043
Iteration 16/1000 | Loss: 0.00002043
Iteration 17/1000 | Loss: 0.00002032
Iteration 18/1000 | Loss: 0.00002027
Iteration 19/1000 | Loss: 0.00002027
Iteration 20/1000 | Loss: 0.00002027
Iteration 21/1000 | Loss: 0.00002027
Iteration 22/1000 | Loss: 0.00002026
Iteration 23/1000 | Loss: 0.00002024
Iteration 24/1000 | Loss: 0.00002023
Iteration 25/1000 | Loss: 0.00002023
Iteration 26/1000 | Loss: 0.00002023
Iteration 27/1000 | Loss: 0.00002023
Iteration 28/1000 | Loss: 0.00002023
Iteration 29/1000 | Loss: 0.00002023
Iteration 30/1000 | Loss: 0.00002022
Iteration 31/1000 | Loss: 0.00002022
Iteration 32/1000 | Loss: 0.00002020
Iteration 33/1000 | Loss: 0.00002020
Iteration 34/1000 | Loss: 0.00002020
Iteration 35/1000 | Loss: 0.00002019
Iteration 36/1000 | Loss: 0.00002018
Iteration 37/1000 | Loss: 0.00002018
Iteration 38/1000 | Loss: 0.00002018
Iteration 39/1000 | Loss: 0.00002018
Iteration 40/1000 | Loss: 0.00002018
Iteration 41/1000 | Loss: 0.00002017
Iteration 42/1000 | Loss: 0.00002016
Iteration 43/1000 | Loss: 0.00002016
Iteration 44/1000 | Loss: 0.00002016
Iteration 45/1000 | Loss: 0.00002015
Iteration 46/1000 | Loss: 0.00002015
Iteration 47/1000 | Loss: 0.00002014
Iteration 48/1000 | Loss: 0.00002014
Iteration 49/1000 | Loss: 0.00002013
Iteration 50/1000 | Loss: 0.00002012
Iteration 51/1000 | Loss: 0.00002012
Iteration 52/1000 | Loss: 0.00002012
Iteration 53/1000 | Loss: 0.00002012
Iteration 54/1000 | Loss: 0.00002011
Iteration 55/1000 | Loss: 0.00002011
Iteration 56/1000 | Loss: 0.00002011
Iteration 57/1000 | Loss: 0.00002011
Iteration 58/1000 | Loss: 0.00002011
Iteration 59/1000 | Loss: 0.00002011
Iteration 60/1000 | Loss: 0.00002011
Iteration 61/1000 | Loss: 0.00002010
Iteration 62/1000 | Loss: 0.00002010
Iteration 63/1000 | Loss: 0.00002010
Iteration 64/1000 | Loss: 0.00002010
Iteration 65/1000 | Loss: 0.00002010
Iteration 66/1000 | Loss: 0.00002010
Iteration 67/1000 | Loss: 0.00002010
Iteration 68/1000 | Loss: 0.00002010
Iteration 69/1000 | Loss: 0.00002009
Iteration 70/1000 | Loss: 0.00002009
Iteration 71/1000 | Loss: 0.00002009
Iteration 72/1000 | Loss: 0.00002009
Iteration 73/1000 | Loss: 0.00002009
Iteration 74/1000 | Loss: 0.00002009
Iteration 75/1000 | Loss: 0.00002009
Iteration 76/1000 | Loss: 0.00002009
Iteration 77/1000 | Loss: 0.00002008
Iteration 78/1000 | Loss: 0.00002008
Iteration 79/1000 | Loss: 0.00002007
Iteration 80/1000 | Loss: 0.00002004
Iteration 81/1000 | Loss: 0.00002003
Iteration 82/1000 | Loss: 0.00002003
Iteration 83/1000 | Loss: 0.00002003
Iteration 84/1000 | Loss: 0.00002003
Iteration 85/1000 | Loss: 0.00002002
Iteration 86/1000 | Loss: 0.00002002
Iteration 87/1000 | Loss: 0.00002002
Iteration 88/1000 | Loss: 0.00002002
Iteration 89/1000 | Loss: 0.00002002
Iteration 90/1000 | Loss: 0.00002002
Iteration 91/1000 | Loss: 0.00002002
Iteration 92/1000 | Loss: 0.00002002
Iteration 93/1000 | Loss: 0.00002002
Iteration 94/1000 | Loss: 0.00002002
Iteration 95/1000 | Loss: 0.00002002
Iteration 96/1000 | Loss: 0.00002002
Iteration 97/1000 | Loss: 0.00002002
Iteration 98/1000 | Loss: 0.00002002
Iteration 99/1000 | Loss: 0.00002002
Iteration 100/1000 | Loss: 0.00002002
Iteration 101/1000 | Loss: 0.00002001
Iteration 102/1000 | Loss: 0.00002001
Iteration 103/1000 | Loss: 0.00002001
Iteration 104/1000 | Loss: 0.00002001
Iteration 105/1000 | Loss: 0.00002001
Iteration 106/1000 | Loss: 0.00002001
Iteration 107/1000 | Loss: 0.00002001
Iteration 108/1000 | Loss: 0.00002001
Iteration 109/1000 | Loss: 0.00002001
Iteration 110/1000 | Loss: 0.00002001
Iteration 111/1000 | Loss: 0.00002001
Iteration 112/1000 | Loss: 0.00002001
Iteration 113/1000 | Loss: 0.00002001
Iteration 114/1000 | Loss: 0.00002001
Iteration 115/1000 | Loss: 0.00002001
Iteration 116/1000 | Loss: 0.00002001
Iteration 117/1000 | Loss: 0.00002001
Iteration 118/1000 | Loss: 0.00002001
Iteration 119/1000 | Loss: 0.00002001
Iteration 120/1000 | Loss: 0.00002001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.001257598749362e-05, 2.001257598749362e-05, 2.001257598749362e-05, 2.001257598749362e-05, 2.001257598749362e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.001257598749362e-05

Optimization complete. Final v2v error: 3.76289701461792 mm

Highest mean error: 4.061160564422607 mm for frame 61

Lowest mean error: 3.5227975845336914 mm for frame 124

Saving results

Total time: 32.68755507469177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027905
Iteration 2/25 | Loss: 0.00147657
Iteration 3/25 | Loss: 0.00117739
Iteration 4/25 | Loss: 0.00114228
Iteration 5/25 | Loss: 0.00113260
Iteration 6/25 | Loss: 0.00113157
Iteration 7/25 | Loss: 0.00113157
Iteration 8/25 | Loss: 0.00113157
Iteration 9/25 | Loss: 0.00113157
Iteration 10/25 | Loss: 0.00113157
Iteration 11/25 | Loss: 0.00113157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00113156926818192, 0.00113156926818192, 0.00113156926818192, 0.00113156926818192, 0.00113156926818192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00113156926818192

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55865681
Iteration 2/25 | Loss: 0.00099391
Iteration 3/25 | Loss: 0.00099391
Iteration 4/25 | Loss: 0.00099391
Iteration 5/25 | Loss: 0.00099391
Iteration 6/25 | Loss: 0.00099391
Iteration 7/25 | Loss: 0.00099391
Iteration 8/25 | Loss: 0.00099391
Iteration 9/25 | Loss: 0.00099391
Iteration 10/25 | Loss: 0.00099391
Iteration 11/25 | Loss: 0.00099391
Iteration 12/25 | Loss: 0.00099391
Iteration 13/25 | Loss: 0.00099391
Iteration 14/25 | Loss: 0.00099391
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009939068695530295, 0.0009939068695530295, 0.0009939068695530295, 0.0009939068695530295, 0.0009939068695530295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009939068695530295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099391
Iteration 2/1000 | Loss: 0.00003635
Iteration 3/1000 | Loss: 0.00002383
Iteration 4/1000 | Loss: 0.00001988
Iteration 5/1000 | Loss: 0.00001873
Iteration 6/1000 | Loss: 0.00001799
Iteration 7/1000 | Loss: 0.00001726
Iteration 8/1000 | Loss: 0.00001674
Iteration 9/1000 | Loss: 0.00001637
Iteration 10/1000 | Loss: 0.00001608
Iteration 11/1000 | Loss: 0.00001579
Iteration 12/1000 | Loss: 0.00001559
Iteration 13/1000 | Loss: 0.00001545
Iteration 14/1000 | Loss: 0.00001543
Iteration 15/1000 | Loss: 0.00001527
Iteration 16/1000 | Loss: 0.00001525
Iteration 17/1000 | Loss: 0.00001525
Iteration 18/1000 | Loss: 0.00001523
Iteration 19/1000 | Loss: 0.00001522
Iteration 20/1000 | Loss: 0.00001522
Iteration 21/1000 | Loss: 0.00001521
Iteration 22/1000 | Loss: 0.00001521
Iteration 23/1000 | Loss: 0.00001520
Iteration 24/1000 | Loss: 0.00001520
Iteration 25/1000 | Loss: 0.00001520
Iteration 26/1000 | Loss: 0.00001519
Iteration 27/1000 | Loss: 0.00001519
Iteration 28/1000 | Loss: 0.00001519
Iteration 29/1000 | Loss: 0.00001519
Iteration 30/1000 | Loss: 0.00001519
Iteration 31/1000 | Loss: 0.00001518
Iteration 32/1000 | Loss: 0.00001517
Iteration 33/1000 | Loss: 0.00001517
Iteration 34/1000 | Loss: 0.00001517
Iteration 35/1000 | Loss: 0.00001516
Iteration 36/1000 | Loss: 0.00001516
Iteration 37/1000 | Loss: 0.00001516
Iteration 38/1000 | Loss: 0.00001516
Iteration 39/1000 | Loss: 0.00001516
Iteration 40/1000 | Loss: 0.00001515
Iteration 41/1000 | Loss: 0.00001515
Iteration 42/1000 | Loss: 0.00001514
Iteration 43/1000 | Loss: 0.00001514
Iteration 44/1000 | Loss: 0.00001514
Iteration 45/1000 | Loss: 0.00001514
Iteration 46/1000 | Loss: 0.00001514
Iteration 47/1000 | Loss: 0.00001514
Iteration 48/1000 | Loss: 0.00001514
Iteration 49/1000 | Loss: 0.00001514
Iteration 50/1000 | Loss: 0.00001514
Iteration 51/1000 | Loss: 0.00001514
Iteration 52/1000 | Loss: 0.00001513
Iteration 53/1000 | Loss: 0.00001513
Iteration 54/1000 | Loss: 0.00001513
Iteration 55/1000 | Loss: 0.00001513
Iteration 56/1000 | Loss: 0.00001513
Iteration 57/1000 | Loss: 0.00001513
Iteration 58/1000 | Loss: 0.00001512
Iteration 59/1000 | Loss: 0.00001512
Iteration 60/1000 | Loss: 0.00001512
Iteration 61/1000 | Loss: 0.00001512
Iteration 62/1000 | Loss: 0.00001512
Iteration 63/1000 | Loss: 0.00001511
Iteration 64/1000 | Loss: 0.00001511
Iteration 65/1000 | Loss: 0.00001511
Iteration 66/1000 | Loss: 0.00001511
Iteration 67/1000 | Loss: 0.00001510
Iteration 68/1000 | Loss: 0.00001510
Iteration 69/1000 | Loss: 0.00001509
Iteration 70/1000 | Loss: 0.00001509
Iteration 71/1000 | Loss: 0.00001509
Iteration 72/1000 | Loss: 0.00001509
Iteration 73/1000 | Loss: 0.00001508
Iteration 74/1000 | Loss: 0.00001508
Iteration 75/1000 | Loss: 0.00001508
Iteration 76/1000 | Loss: 0.00001508
Iteration 77/1000 | Loss: 0.00001508
Iteration 78/1000 | Loss: 0.00001508
Iteration 79/1000 | Loss: 0.00001508
Iteration 80/1000 | Loss: 0.00001507
Iteration 81/1000 | Loss: 0.00001507
Iteration 82/1000 | Loss: 0.00001507
Iteration 83/1000 | Loss: 0.00001507
Iteration 84/1000 | Loss: 0.00001507
Iteration 85/1000 | Loss: 0.00001507
Iteration 86/1000 | Loss: 0.00001507
Iteration 87/1000 | Loss: 0.00001506
Iteration 88/1000 | Loss: 0.00001506
Iteration 89/1000 | Loss: 0.00001506
Iteration 90/1000 | Loss: 0.00001506
Iteration 91/1000 | Loss: 0.00001506
Iteration 92/1000 | Loss: 0.00001506
Iteration 93/1000 | Loss: 0.00001506
Iteration 94/1000 | Loss: 0.00001506
Iteration 95/1000 | Loss: 0.00001506
Iteration 96/1000 | Loss: 0.00001506
Iteration 97/1000 | Loss: 0.00001506
Iteration 98/1000 | Loss: 0.00001506
Iteration 99/1000 | Loss: 0.00001506
Iteration 100/1000 | Loss: 0.00001505
Iteration 101/1000 | Loss: 0.00001505
Iteration 102/1000 | Loss: 0.00001505
Iteration 103/1000 | Loss: 0.00001505
Iteration 104/1000 | Loss: 0.00001505
Iteration 105/1000 | Loss: 0.00001505
Iteration 106/1000 | Loss: 0.00001505
Iteration 107/1000 | Loss: 0.00001505
Iteration 108/1000 | Loss: 0.00001505
Iteration 109/1000 | Loss: 0.00001504
Iteration 110/1000 | Loss: 0.00001504
Iteration 111/1000 | Loss: 0.00001504
Iteration 112/1000 | Loss: 0.00001504
Iteration 113/1000 | Loss: 0.00001504
Iteration 114/1000 | Loss: 0.00001504
Iteration 115/1000 | Loss: 0.00001504
Iteration 116/1000 | Loss: 0.00001504
Iteration 117/1000 | Loss: 0.00001504
Iteration 118/1000 | Loss: 0.00001504
Iteration 119/1000 | Loss: 0.00001504
Iteration 120/1000 | Loss: 0.00001504
Iteration 121/1000 | Loss: 0.00001504
Iteration 122/1000 | Loss: 0.00001504
Iteration 123/1000 | Loss: 0.00001504
Iteration 124/1000 | Loss: 0.00001504
Iteration 125/1000 | Loss: 0.00001504
Iteration 126/1000 | Loss: 0.00001504
Iteration 127/1000 | Loss: 0.00001504
Iteration 128/1000 | Loss: 0.00001504
Iteration 129/1000 | Loss: 0.00001504
Iteration 130/1000 | Loss: 0.00001504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.503722342022229e-05, 1.503722342022229e-05, 1.503722342022229e-05, 1.503722342022229e-05, 1.503722342022229e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.503722342022229e-05

Optimization complete. Final v2v error: 3.327026128768921 mm

Highest mean error: 3.915752649307251 mm for frame 111

Lowest mean error: 2.757622241973877 mm for frame 170

Saving results

Total time: 37.25329804420471
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455402
Iteration 2/25 | Loss: 0.00116110
Iteration 3/25 | Loss: 0.00109765
Iteration 4/25 | Loss: 0.00108782
Iteration 5/25 | Loss: 0.00108483
Iteration 6/25 | Loss: 0.00108462
Iteration 7/25 | Loss: 0.00108462
Iteration 8/25 | Loss: 0.00108462
Iteration 9/25 | Loss: 0.00108462
Iteration 10/25 | Loss: 0.00108462
Iteration 11/25 | Loss: 0.00108462
Iteration 12/25 | Loss: 0.00108462
Iteration 13/25 | Loss: 0.00108462
Iteration 14/25 | Loss: 0.00108462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010846243239939213, 0.0010846243239939213, 0.0010846243239939213, 0.0010846243239939213, 0.0010846243239939213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010846243239939213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31927550
Iteration 2/25 | Loss: 0.00073981
Iteration 3/25 | Loss: 0.00073981
Iteration 4/25 | Loss: 0.00073981
Iteration 5/25 | Loss: 0.00073981
Iteration 6/25 | Loss: 0.00073981
Iteration 7/25 | Loss: 0.00073981
Iteration 8/25 | Loss: 0.00073981
Iteration 9/25 | Loss: 0.00073981
Iteration 10/25 | Loss: 0.00073981
Iteration 11/25 | Loss: 0.00073981
Iteration 12/25 | Loss: 0.00073981
Iteration 13/25 | Loss: 0.00073981
Iteration 14/25 | Loss: 0.00073981
Iteration 15/25 | Loss: 0.00073981
Iteration 16/25 | Loss: 0.00073981
Iteration 17/25 | Loss: 0.00073981
Iteration 18/25 | Loss: 0.00073981
Iteration 19/25 | Loss: 0.00073981
Iteration 20/25 | Loss: 0.00073981
Iteration 21/25 | Loss: 0.00073981
Iteration 22/25 | Loss: 0.00073981
Iteration 23/25 | Loss: 0.00073981
Iteration 24/25 | Loss: 0.00073981
Iteration 25/25 | Loss: 0.00073981

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073981
Iteration 2/1000 | Loss: 0.00002395
Iteration 3/1000 | Loss: 0.00001575
Iteration 4/1000 | Loss: 0.00001423
Iteration 5/1000 | Loss: 0.00001362
Iteration 6/1000 | Loss: 0.00001321
Iteration 7/1000 | Loss: 0.00001273
Iteration 8/1000 | Loss: 0.00001255
Iteration 9/1000 | Loss: 0.00001234
Iteration 10/1000 | Loss: 0.00001215
Iteration 11/1000 | Loss: 0.00001196
Iteration 12/1000 | Loss: 0.00001192
Iteration 13/1000 | Loss: 0.00001189
Iteration 14/1000 | Loss: 0.00001179
Iteration 15/1000 | Loss: 0.00001173
Iteration 16/1000 | Loss: 0.00001172
Iteration 17/1000 | Loss: 0.00001171
Iteration 18/1000 | Loss: 0.00001170
Iteration 19/1000 | Loss: 0.00001170
Iteration 20/1000 | Loss: 0.00001168
Iteration 21/1000 | Loss: 0.00001165
Iteration 22/1000 | Loss: 0.00001160
Iteration 23/1000 | Loss: 0.00001160
Iteration 24/1000 | Loss: 0.00001160
Iteration 25/1000 | Loss: 0.00001159
Iteration 26/1000 | Loss: 0.00001159
Iteration 27/1000 | Loss: 0.00001159
Iteration 28/1000 | Loss: 0.00001158
Iteration 29/1000 | Loss: 0.00001155
Iteration 30/1000 | Loss: 0.00001155
Iteration 31/1000 | Loss: 0.00001155
Iteration 32/1000 | Loss: 0.00001154
Iteration 33/1000 | Loss: 0.00001154
Iteration 34/1000 | Loss: 0.00001153
Iteration 35/1000 | Loss: 0.00001153
Iteration 36/1000 | Loss: 0.00001153
Iteration 37/1000 | Loss: 0.00001152
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001151
Iteration 40/1000 | Loss: 0.00001151
Iteration 41/1000 | Loss: 0.00001150
Iteration 42/1000 | Loss: 0.00001149
Iteration 43/1000 | Loss: 0.00001149
Iteration 44/1000 | Loss: 0.00001148
Iteration 45/1000 | Loss: 0.00001148
Iteration 46/1000 | Loss: 0.00001148
Iteration 47/1000 | Loss: 0.00001147
Iteration 48/1000 | Loss: 0.00001147
Iteration 49/1000 | Loss: 0.00001147
Iteration 50/1000 | Loss: 0.00001146
Iteration 51/1000 | Loss: 0.00001146
Iteration 52/1000 | Loss: 0.00001145
Iteration 53/1000 | Loss: 0.00001145
Iteration 54/1000 | Loss: 0.00001144
Iteration 55/1000 | Loss: 0.00001143
Iteration 56/1000 | Loss: 0.00001143
Iteration 57/1000 | Loss: 0.00001143
Iteration 58/1000 | Loss: 0.00001143
Iteration 59/1000 | Loss: 0.00001142
Iteration 60/1000 | Loss: 0.00001141
Iteration 61/1000 | Loss: 0.00001141
Iteration 62/1000 | Loss: 0.00001140
Iteration 63/1000 | Loss: 0.00001140
Iteration 64/1000 | Loss: 0.00001139
Iteration 65/1000 | Loss: 0.00001139
Iteration 66/1000 | Loss: 0.00001139
Iteration 67/1000 | Loss: 0.00001135
Iteration 68/1000 | Loss: 0.00001134
Iteration 69/1000 | Loss: 0.00001134
Iteration 70/1000 | Loss: 0.00001132
Iteration 71/1000 | Loss: 0.00001129
Iteration 72/1000 | Loss: 0.00001128
Iteration 73/1000 | Loss: 0.00001128
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001127
Iteration 76/1000 | Loss: 0.00001126
Iteration 77/1000 | Loss: 0.00001126
Iteration 78/1000 | Loss: 0.00001123
Iteration 79/1000 | Loss: 0.00001122
Iteration 80/1000 | Loss: 0.00001122
Iteration 81/1000 | Loss: 0.00001122
Iteration 82/1000 | Loss: 0.00001121
Iteration 83/1000 | Loss: 0.00001120
Iteration 84/1000 | Loss: 0.00001120
Iteration 85/1000 | Loss: 0.00001120
Iteration 86/1000 | Loss: 0.00001120
Iteration 87/1000 | Loss: 0.00001120
Iteration 88/1000 | Loss: 0.00001120
Iteration 89/1000 | Loss: 0.00001120
Iteration 90/1000 | Loss: 0.00001120
Iteration 91/1000 | Loss: 0.00001120
Iteration 92/1000 | Loss: 0.00001120
Iteration 93/1000 | Loss: 0.00001119
Iteration 94/1000 | Loss: 0.00001119
Iteration 95/1000 | Loss: 0.00001119
Iteration 96/1000 | Loss: 0.00001119
Iteration 97/1000 | Loss: 0.00001118
Iteration 98/1000 | Loss: 0.00001118
Iteration 99/1000 | Loss: 0.00001117
Iteration 100/1000 | Loss: 0.00001117
Iteration 101/1000 | Loss: 0.00001117
Iteration 102/1000 | Loss: 0.00001116
Iteration 103/1000 | Loss: 0.00001116
Iteration 104/1000 | Loss: 0.00001116
Iteration 105/1000 | Loss: 0.00001115
Iteration 106/1000 | Loss: 0.00001115
Iteration 107/1000 | Loss: 0.00001115
Iteration 108/1000 | Loss: 0.00001115
Iteration 109/1000 | Loss: 0.00001115
Iteration 110/1000 | Loss: 0.00001115
Iteration 111/1000 | Loss: 0.00001114
Iteration 112/1000 | Loss: 0.00001114
Iteration 113/1000 | Loss: 0.00001114
Iteration 114/1000 | Loss: 0.00001114
Iteration 115/1000 | Loss: 0.00001114
Iteration 116/1000 | Loss: 0.00001113
Iteration 117/1000 | Loss: 0.00001113
Iteration 118/1000 | Loss: 0.00001113
Iteration 119/1000 | Loss: 0.00001112
Iteration 120/1000 | Loss: 0.00001112
Iteration 121/1000 | Loss: 0.00001112
Iteration 122/1000 | Loss: 0.00001111
Iteration 123/1000 | Loss: 0.00001111
Iteration 124/1000 | Loss: 0.00001111
Iteration 125/1000 | Loss: 0.00001111
Iteration 126/1000 | Loss: 0.00001111
Iteration 127/1000 | Loss: 0.00001111
Iteration 128/1000 | Loss: 0.00001111
Iteration 129/1000 | Loss: 0.00001111
Iteration 130/1000 | Loss: 0.00001111
Iteration 131/1000 | Loss: 0.00001111
Iteration 132/1000 | Loss: 0.00001110
Iteration 133/1000 | Loss: 0.00001110
Iteration 134/1000 | Loss: 0.00001110
Iteration 135/1000 | Loss: 0.00001110
Iteration 136/1000 | Loss: 0.00001109
Iteration 137/1000 | Loss: 0.00001109
Iteration 138/1000 | Loss: 0.00001109
Iteration 139/1000 | Loss: 0.00001109
Iteration 140/1000 | Loss: 0.00001109
Iteration 141/1000 | Loss: 0.00001109
Iteration 142/1000 | Loss: 0.00001109
Iteration 143/1000 | Loss: 0.00001108
Iteration 144/1000 | Loss: 0.00001108
Iteration 145/1000 | Loss: 0.00001108
Iteration 146/1000 | Loss: 0.00001108
Iteration 147/1000 | Loss: 0.00001108
Iteration 148/1000 | Loss: 0.00001108
Iteration 149/1000 | Loss: 0.00001108
Iteration 150/1000 | Loss: 0.00001108
Iteration 151/1000 | Loss: 0.00001108
Iteration 152/1000 | Loss: 0.00001108
Iteration 153/1000 | Loss: 0.00001108
Iteration 154/1000 | Loss: 0.00001108
Iteration 155/1000 | Loss: 0.00001108
Iteration 156/1000 | Loss: 0.00001108
Iteration 157/1000 | Loss: 0.00001108
Iteration 158/1000 | Loss: 0.00001107
Iteration 159/1000 | Loss: 0.00001107
Iteration 160/1000 | Loss: 0.00001107
Iteration 161/1000 | Loss: 0.00001107
Iteration 162/1000 | Loss: 0.00001107
Iteration 163/1000 | Loss: 0.00001107
Iteration 164/1000 | Loss: 0.00001107
Iteration 165/1000 | Loss: 0.00001107
Iteration 166/1000 | Loss: 0.00001107
Iteration 167/1000 | Loss: 0.00001107
Iteration 168/1000 | Loss: 0.00001107
Iteration 169/1000 | Loss: 0.00001107
Iteration 170/1000 | Loss: 0.00001107
Iteration 171/1000 | Loss: 0.00001107
Iteration 172/1000 | Loss: 0.00001107
Iteration 173/1000 | Loss: 0.00001107
Iteration 174/1000 | Loss: 0.00001107
Iteration 175/1000 | Loss: 0.00001107
Iteration 176/1000 | Loss: 0.00001107
Iteration 177/1000 | Loss: 0.00001106
Iteration 178/1000 | Loss: 0.00001106
Iteration 179/1000 | Loss: 0.00001106
Iteration 180/1000 | Loss: 0.00001106
Iteration 181/1000 | Loss: 0.00001106
Iteration 182/1000 | Loss: 0.00001106
Iteration 183/1000 | Loss: 0.00001106
Iteration 184/1000 | Loss: 0.00001106
Iteration 185/1000 | Loss: 0.00001106
Iteration 186/1000 | Loss: 0.00001106
Iteration 187/1000 | Loss: 0.00001106
Iteration 188/1000 | Loss: 0.00001106
Iteration 189/1000 | Loss: 0.00001106
Iteration 190/1000 | Loss: 0.00001106
Iteration 191/1000 | Loss: 0.00001106
Iteration 192/1000 | Loss: 0.00001106
Iteration 193/1000 | Loss: 0.00001106
Iteration 194/1000 | Loss: 0.00001105
Iteration 195/1000 | Loss: 0.00001105
Iteration 196/1000 | Loss: 0.00001105
Iteration 197/1000 | Loss: 0.00001105
Iteration 198/1000 | Loss: 0.00001105
Iteration 199/1000 | Loss: 0.00001105
Iteration 200/1000 | Loss: 0.00001105
Iteration 201/1000 | Loss: 0.00001105
Iteration 202/1000 | Loss: 0.00001105
Iteration 203/1000 | Loss: 0.00001105
Iteration 204/1000 | Loss: 0.00001105
Iteration 205/1000 | Loss: 0.00001105
Iteration 206/1000 | Loss: 0.00001105
Iteration 207/1000 | Loss: 0.00001105
Iteration 208/1000 | Loss: 0.00001105
Iteration 209/1000 | Loss: 0.00001105
Iteration 210/1000 | Loss: 0.00001105
Iteration 211/1000 | Loss: 0.00001104
Iteration 212/1000 | Loss: 0.00001104
Iteration 213/1000 | Loss: 0.00001104
Iteration 214/1000 | Loss: 0.00001104
Iteration 215/1000 | Loss: 0.00001104
Iteration 216/1000 | Loss: 0.00001104
Iteration 217/1000 | Loss: 0.00001104
Iteration 218/1000 | Loss: 0.00001104
Iteration 219/1000 | Loss: 0.00001104
Iteration 220/1000 | Loss: 0.00001104
Iteration 221/1000 | Loss: 0.00001104
Iteration 222/1000 | Loss: 0.00001104
Iteration 223/1000 | Loss: 0.00001104
Iteration 224/1000 | Loss: 0.00001104
Iteration 225/1000 | Loss: 0.00001104
Iteration 226/1000 | Loss: 0.00001104
Iteration 227/1000 | Loss: 0.00001104
Iteration 228/1000 | Loss: 0.00001103
Iteration 229/1000 | Loss: 0.00001103
Iteration 230/1000 | Loss: 0.00001103
Iteration 231/1000 | Loss: 0.00001103
Iteration 232/1000 | Loss: 0.00001103
Iteration 233/1000 | Loss: 0.00001103
Iteration 234/1000 | Loss: 0.00001103
Iteration 235/1000 | Loss: 0.00001103
Iteration 236/1000 | Loss: 0.00001103
Iteration 237/1000 | Loss: 0.00001103
Iteration 238/1000 | Loss: 0.00001103
Iteration 239/1000 | Loss: 0.00001103
Iteration 240/1000 | Loss: 0.00001103
Iteration 241/1000 | Loss: 0.00001103
Iteration 242/1000 | Loss: 0.00001103
Iteration 243/1000 | Loss: 0.00001103
Iteration 244/1000 | Loss: 0.00001103
Iteration 245/1000 | Loss: 0.00001103
Iteration 246/1000 | Loss: 0.00001103
Iteration 247/1000 | Loss: 0.00001103
Iteration 248/1000 | Loss: 0.00001103
Iteration 249/1000 | Loss: 0.00001103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.1033491318812594e-05, 1.1033491318812594e-05, 1.1033491318812594e-05, 1.1033491318812594e-05, 1.1033491318812594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1033491318812594e-05

Optimization complete. Final v2v error: 2.842738628387451 mm

Highest mean error: 2.982241630554199 mm for frame 73

Lowest mean error: 2.711141347885132 mm for frame 58

Saving results

Total time: 42.68727493286133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805502
Iteration 2/25 | Loss: 0.00124196
Iteration 3/25 | Loss: 0.00110472
Iteration 4/25 | Loss: 0.00107650
Iteration 5/25 | Loss: 0.00106949
Iteration 6/25 | Loss: 0.00106764
Iteration 7/25 | Loss: 0.00106737
Iteration 8/25 | Loss: 0.00106737
Iteration 9/25 | Loss: 0.00106737
Iteration 10/25 | Loss: 0.00106737
Iteration 11/25 | Loss: 0.00106737
Iteration 12/25 | Loss: 0.00106737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001067374600097537, 0.001067374600097537, 0.001067374600097537, 0.001067374600097537, 0.001067374600097537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001067374600097537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32513916
Iteration 2/25 | Loss: 0.00064087
Iteration 3/25 | Loss: 0.00064087
Iteration 4/25 | Loss: 0.00064087
Iteration 5/25 | Loss: 0.00064087
Iteration 6/25 | Loss: 0.00064087
Iteration 7/25 | Loss: 0.00064087
Iteration 8/25 | Loss: 0.00064087
Iteration 9/25 | Loss: 0.00064087
Iteration 10/25 | Loss: 0.00064087
Iteration 11/25 | Loss: 0.00064087
Iteration 12/25 | Loss: 0.00064087
Iteration 13/25 | Loss: 0.00064087
Iteration 14/25 | Loss: 0.00064087
Iteration 15/25 | Loss: 0.00064087
Iteration 16/25 | Loss: 0.00064087
Iteration 17/25 | Loss: 0.00064087
Iteration 18/25 | Loss: 0.00064087
Iteration 19/25 | Loss: 0.00064087
Iteration 20/25 | Loss: 0.00064087
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006408679182641208, 0.0006408679182641208, 0.0006408679182641208, 0.0006408679182641208, 0.0006408679182641208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006408679182641208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064087
Iteration 2/1000 | Loss: 0.00003887
Iteration 3/1000 | Loss: 0.00002828
Iteration 4/1000 | Loss: 0.00002269
Iteration 5/1000 | Loss: 0.00002134
Iteration 6/1000 | Loss: 0.00002049
Iteration 7/1000 | Loss: 0.00001975
Iteration 8/1000 | Loss: 0.00001928
Iteration 9/1000 | Loss: 0.00001879
Iteration 10/1000 | Loss: 0.00001846
Iteration 11/1000 | Loss: 0.00001821
Iteration 12/1000 | Loss: 0.00001798
Iteration 13/1000 | Loss: 0.00001783
Iteration 14/1000 | Loss: 0.00001776
Iteration 15/1000 | Loss: 0.00001767
Iteration 16/1000 | Loss: 0.00001766
Iteration 17/1000 | Loss: 0.00001766
Iteration 18/1000 | Loss: 0.00001765
Iteration 19/1000 | Loss: 0.00001764
Iteration 20/1000 | Loss: 0.00001764
Iteration 21/1000 | Loss: 0.00001763
Iteration 22/1000 | Loss: 0.00001760
Iteration 23/1000 | Loss: 0.00001759
Iteration 24/1000 | Loss: 0.00001758
Iteration 25/1000 | Loss: 0.00001756
Iteration 26/1000 | Loss: 0.00001756
Iteration 27/1000 | Loss: 0.00001755
Iteration 28/1000 | Loss: 0.00001755
Iteration 29/1000 | Loss: 0.00001749
Iteration 30/1000 | Loss: 0.00001747
Iteration 31/1000 | Loss: 0.00001747
Iteration 32/1000 | Loss: 0.00001742
Iteration 33/1000 | Loss: 0.00001741
Iteration 34/1000 | Loss: 0.00001740
Iteration 35/1000 | Loss: 0.00001740
Iteration 36/1000 | Loss: 0.00001739
Iteration 37/1000 | Loss: 0.00001739
Iteration 38/1000 | Loss: 0.00001738
Iteration 39/1000 | Loss: 0.00001738
Iteration 40/1000 | Loss: 0.00001738
Iteration 41/1000 | Loss: 0.00001737
Iteration 42/1000 | Loss: 0.00001737
Iteration 43/1000 | Loss: 0.00001737
Iteration 44/1000 | Loss: 0.00001737
Iteration 45/1000 | Loss: 0.00001737
Iteration 46/1000 | Loss: 0.00001737
Iteration 47/1000 | Loss: 0.00001737
Iteration 48/1000 | Loss: 0.00001737
Iteration 49/1000 | Loss: 0.00001737
Iteration 50/1000 | Loss: 0.00001736
Iteration 51/1000 | Loss: 0.00001736
Iteration 52/1000 | Loss: 0.00001736
Iteration 53/1000 | Loss: 0.00001736
Iteration 54/1000 | Loss: 0.00001736
Iteration 55/1000 | Loss: 0.00001736
Iteration 56/1000 | Loss: 0.00001736
Iteration 57/1000 | Loss: 0.00001735
Iteration 58/1000 | Loss: 0.00001735
Iteration 59/1000 | Loss: 0.00001735
Iteration 60/1000 | Loss: 0.00001734
Iteration 61/1000 | Loss: 0.00001734
Iteration 62/1000 | Loss: 0.00001734
Iteration 63/1000 | Loss: 0.00001733
Iteration 64/1000 | Loss: 0.00001733
Iteration 65/1000 | Loss: 0.00001733
Iteration 66/1000 | Loss: 0.00001733
Iteration 67/1000 | Loss: 0.00001733
Iteration 68/1000 | Loss: 0.00001733
Iteration 69/1000 | Loss: 0.00001733
Iteration 70/1000 | Loss: 0.00001733
Iteration 71/1000 | Loss: 0.00001732
Iteration 72/1000 | Loss: 0.00001732
Iteration 73/1000 | Loss: 0.00001732
Iteration 74/1000 | Loss: 0.00001732
Iteration 75/1000 | Loss: 0.00001732
Iteration 76/1000 | Loss: 0.00001732
Iteration 77/1000 | Loss: 0.00001732
Iteration 78/1000 | Loss: 0.00001732
Iteration 79/1000 | Loss: 0.00001732
Iteration 80/1000 | Loss: 0.00001731
Iteration 81/1000 | Loss: 0.00001731
Iteration 82/1000 | Loss: 0.00001731
Iteration 83/1000 | Loss: 0.00001731
Iteration 84/1000 | Loss: 0.00001731
Iteration 85/1000 | Loss: 0.00001731
Iteration 86/1000 | Loss: 0.00001731
Iteration 87/1000 | Loss: 0.00001731
Iteration 88/1000 | Loss: 0.00001731
Iteration 89/1000 | Loss: 0.00001730
Iteration 90/1000 | Loss: 0.00001730
Iteration 91/1000 | Loss: 0.00001730
Iteration 92/1000 | Loss: 0.00001730
Iteration 93/1000 | Loss: 0.00001730
Iteration 94/1000 | Loss: 0.00001730
Iteration 95/1000 | Loss: 0.00001729
Iteration 96/1000 | Loss: 0.00001729
Iteration 97/1000 | Loss: 0.00001729
Iteration 98/1000 | Loss: 0.00001729
Iteration 99/1000 | Loss: 0.00001729
Iteration 100/1000 | Loss: 0.00001728
Iteration 101/1000 | Loss: 0.00001728
Iteration 102/1000 | Loss: 0.00001728
Iteration 103/1000 | Loss: 0.00001728
Iteration 104/1000 | Loss: 0.00001727
Iteration 105/1000 | Loss: 0.00001727
Iteration 106/1000 | Loss: 0.00001727
Iteration 107/1000 | Loss: 0.00001727
Iteration 108/1000 | Loss: 0.00001726
Iteration 109/1000 | Loss: 0.00001726
Iteration 110/1000 | Loss: 0.00001726
Iteration 111/1000 | Loss: 0.00001726
Iteration 112/1000 | Loss: 0.00001726
Iteration 113/1000 | Loss: 0.00001725
Iteration 114/1000 | Loss: 0.00001725
Iteration 115/1000 | Loss: 0.00001725
Iteration 116/1000 | Loss: 0.00001725
Iteration 117/1000 | Loss: 0.00001725
Iteration 118/1000 | Loss: 0.00001725
Iteration 119/1000 | Loss: 0.00001725
Iteration 120/1000 | Loss: 0.00001725
Iteration 121/1000 | Loss: 0.00001725
Iteration 122/1000 | Loss: 0.00001724
Iteration 123/1000 | Loss: 0.00001724
Iteration 124/1000 | Loss: 0.00001724
Iteration 125/1000 | Loss: 0.00001724
Iteration 126/1000 | Loss: 0.00001724
Iteration 127/1000 | Loss: 0.00001723
Iteration 128/1000 | Loss: 0.00001723
Iteration 129/1000 | Loss: 0.00001723
Iteration 130/1000 | Loss: 0.00001723
Iteration 131/1000 | Loss: 0.00001723
Iteration 132/1000 | Loss: 0.00001723
Iteration 133/1000 | Loss: 0.00001723
Iteration 134/1000 | Loss: 0.00001723
Iteration 135/1000 | Loss: 0.00001723
Iteration 136/1000 | Loss: 0.00001723
Iteration 137/1000 | Loss: 0.00001723
Iteration 138/1000 | Loss: 0.00001723
Iteration 139/1000 | Loss: 0.00001722
Iteration 140/1000 | Loss: 0.00001722
Iteration 141/1000 | Loss: 0.00001722
Iteration 142/1000 | Loss: 0.00001722
Iteration 143/1000 | Loss: 0.00001722
Iteration 144/1000 | Loss: 0.00001722
Iteration 145/1000 | Loss: 0.00001722
Iteration 146/1000 | Loss: 0.00001722
Iteration 147/1000 | Loss: 0.00001722
Iteration 148/1000 | Loss: 0.00001722
Iteration 149/1000 | Loss: 0.00001722
Iteration 150/1000 | Loss: 0.00001722
Iteration 151/1000 | Loss: 0.00001722
Iteration 152/1000 | Loss: 0.00001722
Iteration 153/1000 | Loss: 0.00001722
Iteration 154/1000 | Loss: 0.00001722
Iteration 155/1000 | Loss: 0.00001722
Iteration 156/1000 | Loss: 0.00001722
Iteration 157/1000 | Loss: 0.00001722
Iteration 158/1000 | Loss: 0.00001721
Iteration 159/1000 | Loss: 0.00001721
Iteration 160/1000 | Loss: 0.00001721
Iteration 161/1000 | Loss: 0.00001721
Iteration 162/1000 | Loss: 0.00001721
Iteration 163/1000 | Loss: 0.00001721
Iteration 164/1000 | Loss: 0.00001721
Iteration 165/1000 | Loss: 0.00001721
Iteration 166/1000 | Loss: 0.00001721
Iteration 167/1000 | Loss: 0.00001721
Iteration 168/1000 | Loss: 0.00001721
Iteration 169/1000 | Loss: 0.00001721
Iteration 170/1000 | Loss: 0.00001721
Iteration 171/1000 | Loss: 0.00001721
Iteration 172/1000 | Loss: 0.00001721
Iteration 173/1000 | Loss: 0.00001720
Iteration 174/1000 | Loss: 0.00001720
Iteration 175/1000 | Loss: 0.00001720
Iteration 176/1000 | Loss: 0.00001720
Iteration 177/1000 | Loss: 0.00001720
Iteration 178/1000 | Loss: 0.00001720
Iteration 179/1000 | Loss: 0.00001720
Iteration 180/1000 | Loss: 0.00001720
Iteration 181/1000 | Loss: 0.00001720
Iteration 182/1000 | Loss: 0.00001720
Iteration 183/1000 | Loss: 0.00001720
Iteration 184/1000 | Loss: 0.00001720
Iteration 185/1000 | Loss: 0.00001720
Iteration 186/1000 | Loss: 0.00001720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.720282671158202e-05, 1.720282671158202e-05, 1.720282671158202e-05, 1.720282671158202e-05, 1.720282671158202e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.720282671158202e-05

Optimization complete. Final v2v error: 3.5574522018432617 mm

Highest mean error: 3.940718173980713 mm for frame 27

Lowest mean error: 3.280702829360962 mm for frame 140

Saving results

Total time: 41.17126417160034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00589383
Iteration 2/25 | Loss: 0.00132699
Iteration 3/25 | Loss: 0.00123979
Iteration 4/25 | Loss: 0.00110765
Iteration 5/25 | Loss: 0.00109976
Iteration 6/25 | Loss: 0.00109871
Iteration 7/25 | Loss: 0.00108842
Iteration 8/25 | Loss: 0.00108660
Iteration 9/25 | Loss: 0.00108492
Iteration 10/25 | Loss: 0.00108376
Iteration 11/25 | Loss: 0.00108323
Iteration 12/25 | Loss: 0.00108290
Iteration 13/25 | Loss: 0.00108507
Iteration 14/25 | Loss: 0.00108101
Iteration 15/25 | Loss: 0.00108016
Iteration 16/25 | Loss: 0.00107984
Iteration 17/25 | Loss: 0.00107980
Iteration 18/25 | Loss: 0.00107979
Iteration 19/25 | Loss: 0.00107979
Iteration 20/25 | Loss: 0.00107979
Iteration 21/25 | Loss: 0.00107979
Iteration 22/25 | Loss: 0.00107979
Iteration 23/25 | Loss: 0.00107979
Iteration 24/25 | Loss: 0.00107979
Iteration 25/25 | Loss: 0.00107978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.19632268
Iteration 2/25 | Loss: 0.00079088
Iteration 3/25 | Loss: 0.00079088
Iteration 4/25 | Loss: 0.00079088
Iteration 5/25 | Loss: 0.00079088
Iteration 6/25 | Loss: 0.00079087
Iteration 7/25 | Loss: 0.00079087
Iteration 8/25 | Loss: 0.00079087
Iteration 9/25 | Loss: 0.00079087
Iteration 10/25 | Loss: 0.00079087
Iteration 11/25 | Loss: 0.00079087
Iteration 12/25 | Loss: 0.00079087
Iteration 13/25 | Loss: 0.00079087
Iteration 14/25 | Loss: 0.00079087
Iteration 15/25 | Loss: 0.00079087
Iteration 16/25 | Loss: 0.00079087
Iteration 17/25 | Loss: 0.00079087
Iteration 18/25 | Loss: 0.00079087
Iteration 19/25 | Loss: 0.00079087
Iteration 20/25 | Loss: 0.00079087
Iteration 21/25 | Loss: 0.00079087
Iteration 22/25 | Loss: 0.00079087
Iteration 23/25 | Loss: 0.00079087
Iteration 24/25 | Loss: 0.00079087
Iteration 25/25 | Loss: 0.00079087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079087
Iteration 2/1000 | Loss: 0.00002041
Iteration 3/1000 | Loss: 0.00001443
Iteration 4/1000 | Loss: 0.00001308
Iteration 5/1000 | Loss: 0.00001250
Iteration 6/1000 | Loss: 0.00001192
Iteration 7/1000 | Loss: 0.00001155
Iteration 8/1000 | Loss: 0.00001140
Iteration 9/1000 | Loss: 0.00001120
Iteration 10/1000 | Loss: 0.00001117
Iteration 11/1000 | Loss: 0.00001103
Iteration 12/1000 | Loss: 0.00001098
Iteration 13/1000 | Loss: 0.00001097
Iteration 14/1000 | Loss: 0.00001096
Iteration 15/1000 | Loss: 0.00001093
Iteration 16/1000 | Loss: 0.00001083
Iteration 17/1000 | Loss: 0.00001080
Iteration 18/1000 | Loss: 0.00001079
Iteration 19/1000 | Loss: 0.00001078
Iteration 20/1000 | Loss: 0.00001078
Iteration 21/1000 | Loss: 0.00001077
Iteration 22/1000 | Loss: 0.00001076
Iteration 23/1000 | Loss: 0.00001075
Iteration 24/1000 | Loss: 0.00001074
Iteration 25/1000 | Loss: 0.00001073
Iteration 26/1000 | Loss: 0.00001072
Iteration 27/1000 | Loss: 0.00001072
Iteration 28/1000 | Loss: 0.00001072
Iteration 29/1000 | Loss: 0.00001072
Iteration 30/1000 | Loss: 0.00001071
Iteration 31/1000 | Loss: 0.00001071
Iteration 32/1000 | Loss: 0.00001070
Iteration 33/1000 | Loss: 0.00001070
Iteration 34/1000 | Loss: 0.00001070
Iteration 35/1000 | Loss: 0.00001070
Iteration 36/1000 | Loss: 0.00001070
Iteration 37/1000 | Loss: 0.00001070
Iteration 38/1000 | Loss: 0.00001069
Iteration 39/1000 | Loss: 0.00001069
Iteration 40/1000 | Loss: 0.00001069
Iteration 41/1000 | Loss: 0.00001069
Iteration 42/1000 | Loss: 0.00001069
Iteration 43/1000 | Loss: 0.00001068
Iteration 44/1000 | Loss: 0.00001068
Iteration 45/1000 | Loss: 0.00001067
Iteration 46/1000 | Loss: 0.00001066
Iteration 47/1000 | Loss: 0.00001066
Iteration 48/1000 | Loss: 0.00001065
Iteration 49/1000 | Loss: 0.00001065
Iteration 50/1000 | Loss: 0.00001064
Iteration 51/1000 | Loss: 0.00001063
Iteration 52/1000 | Loss: 0.00001063
Iteration 53/1000 | Loss: 0.00001063
Iteration 54/1000 | Loss: 0.00001063
Iteration 55/1000 | Loss: 0.00001062
Iteration 56/1000 | Loss: 0.00001062
Iteration 57/1000 | Loss: 0.00001062
Iteration 58/1000 | Loss: 0.00001062
Iteration 59/1000 | Loss: 0.00001062
Iteration 60/1000 | Loss: 0.00001062
Iteration 61/1000 | Loss: 0.00001061
Iteration 62/1000 | Loss: 0.00001061
Iteration 63/1000 | Loss: 0.00001061
Iteration 64/1000 | Loss: 0.00001061
Iteration 65/1000 | Loss: 0.00001061
Iteration 66/1000 | Loss: 0.00001061
Iteration 67/1000 | Loss: 0.00001061
Iteration 68/1000 | Loss: 0.00001060
Iteration 69/1000 | Loss: 0.00001060
Iteration 70/1000 | Loss: 0.00001060
Iteration 71/1000 | Loss: 0.00001059
Iteration 72/1000 | Loss: 0.00001059
Iteration 73/1000 | Loss: 0.00001059
Iteration 74/1000 | Loss: 0.00001059
Iteration 75/1000 | Loss: 0.00001058
Iteration 76/1000 | Loss: 0.00001058
Iteration 77/1000 | Loss: 0.00001058
Iteration 78/1000 | Loss: 0.00001058
Iteration 79/1000 | Loss: 0.00001057
Iteration 80/1000 | Loss: 0.00001056
Iteration 81/1000 | Loss: 0.00001056
Iteration 82/1000 | Loss: 0.00001056
Iteration 83/1000 | Loss: 0.00001055
Iteration 84/1000 | Loss: 0.00001055
Iteration 85/1000 | Loss: 0.00001055
Iteration 86/1000 | Loss: 0.00001055
Iteration 87/1000 | Loss: 0.00001055
Iteration 88/1000 | Loss: 0.00001055
Iteration 89/1000 | Loss: 0.00001055
Iteration 90/1000 | Loss: 0.00001054
Iteration 91/1000 | Loss: 0.00001054
Iteration 92/1000 | Loss: 0.00001054
Iteration 93/1000 | Loss: 0.00001054
Iteration 94/1000 | Loss: 0.00001053
Iteration 95/1000 | Loss: 0.00001053
Iteration 96/1000 | Loss: 0.00001053
Iteration 97/1000 | Loss: 0.00001053
Iteration 98/1000 | Loss: 0.00001052
Iteration 99/1000 | Loss: 0.00001052
Iteration 100/1000 | Loss: 0.00001052
Iteration 101/1000 | Loss: 0.00001051
Iteration 102/1000 | Loss: 0.00001051
Iteration 103/1000 | Loss: 0.00001051
Iteration 104/1000 | Loss: 0.00001050
Iteration 105/1000 | Loss: 0.00001050
Iteration 106/1000 | Loss: 0.00001050
Iteration 107/1000 | Loss: 0.00001050
Iteration 108/1000 | Loss: 0.00001050
Iteration 109/1000 | Loss: 0.00001050
Iteration 110/1000 | Loss: 0.00001050
Iteration 111/1000 | Loss: 0.00001050
Iteration 112/1000 | Loss: 0.00001049
Iteration 113/1000 | Loss: 0.00001049
Iteration 114/1000 | Loss: 0.00001049
Iteration 115/1000 | Loss: 0.00001049
Iteration 116/1000 | Loss: 0.00001049
Iteration 117/1000 | Loss: 0.00001049
Iteration 118/1000 | Loss: 0.00001049
Iteration 119/1000 | Loss: 0.00001049
Iteration 120/1000 | Loss: 0.00001049
Iteration 121/1000 | Loss: 0.00001049
Iteration 122/1000 | Loss: 0.00001049
Iteration 123/1000 | Loss: 0.00001048
Iteration 124/1000 | Loss: 0.00001048
Iteration 125/1000 | Loss: 0.00001048
Iteration 126/1000 | Loss: 0.00001048
Iteration 127/1000 | Loss: 0.00001047
Iteration 128/1000 | Loss: 0.00001047
Iteration 129/1000 | Loss: 0.00001047
Iteration 130/1000 | Loss: 0.00001047
Iteration 131/1000 | Loss: 0.00001047
Iteration 132/1000 | Loss: 0.00001047
Iteration 133/1000 | Loss: 0.00001047
Iteration 134/1000 | Loss: 0.00001047
Iteration 135/1000 | Loss: 0.00001047
Iteration 136/1000 | Loss: 0.00001047
Iteration 137/1000 | Loss: 0.00001047
Iteration 138/1000 | Loss: 0.00001047
Iteration 139/1000 | Loss: 0.00001047
Iteration 140/1000 | Loss: 0.00001047
Iteration 141/1000 | Loss: 0.00001047
Iteration 142/1000 | Loss: 0.00001047
Iteration 143/1000 | Loss: 0.00001047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.0471003406564705e-05, 1.0471003406564705e-05, 1.0471003406564705e-05, 1.0471003406564705e-05, 1.0471003406564705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0471003406564705e-05

Optimization complete. Final v2v error: 2.753053665161133 mm

Highest mean error: 3.1399402618408203 mm for frame 66

Lowest mean error: 2.4595894813537598 mm for frame 128

Saving results

Total time: 60.553040504455566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485017
Iteration 2/25 | Loss: 0.00124982
Iteration 3/25 | Loss: 0.00115032
Iteration 4/25 | Loss: 0.00113419
Iteration 5/25 | Loss: 0.00112912
Iteration 6/25 | Loss: 0.00112863
Iteration 7/25 | Loss: 0.00112863
Iteration 8/25 | Loss: 0.00112863
Iteration 9/25 | Loss: 0.00112863
Iteration 10/25 | Loss: 0.00112863
Iteration 11/25 | Loss: 0.00112863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011286346707493067, 0.0011286346707493067, 0.0011286346707493067, 0.0011286346707493067, 0.0011286346707493067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011286346707493067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34256220
Iteration 2/25 | Loss: 0.00083953
Iteration 3/25 | Loss: 0.00083952
Iteration 4/25 | Loss: 0.00083952
Iteration 5/25 | Loss: 0.00083952
Iteration 6/25 | Loss: 0.00083952
Iteration 7/25 | Loss: 0.00083952
Iteration 8/25 | Loss: 0.00083952
Iteration 9/25 | Loss: 0.00083952
Iteration 10/25 | Loss: 0.00083952
Iteration 11/25 | Loss: 0.00083952
Iteration 12/25 | Loss: 0.00083952
Iteration 13/25 | Loss: 0.00083952
Iteration 14/25 | Loss: 0.00083952
Iteration 15/25 | Loss: 0.00083952
Iteration 16/25 | Loss: 0.00083952
Iteration 17/25 | Loss: 0.00083952
Iteration 18/25 | Loss: 0.00083952
Iteration 19/25 | Loss: 0.00083952
Iteration 20/25 | Loss: 0.00083952
Iteration 21/25 | Loss: 0.00083952
Iteration 22/25 | Loss: 0.00083952
Iteration 23/25 | Loss: 0.00083952
Iteration 24/25 | Loss: 0.00083952
Iteration 25/25 | Loss: 0.00083952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083952
Iteration 2/1000 | Loss: 0.00004274
Iteration 3/1000 | Loss: 0.00002927
Iteration 4/1000 | Loss: 0.00002530
Iteration 5/1000 | Loss: 0.00002353
Iteration 6/1000 | Loss: 0.00002262
Iteration 7/1000 | Loss: 0.00002196
Iteration 8/1000 | Loss: 0.00002144
Iteration 9/1000 | Loss: 0.00002113
Iteration 10/1000 | Loss: 0.00002089
Iteration 11/1000 | Loss: 0.00002070
Iteration 12/1000 | Loss: 0.00002067
Iteration 13/1000 | Loss: 0.00002060
Iteration 14/1000 | Loss: 0.00002048
Iteration 15/1000 | Loss: 0.00002043
Iteration 16/1000 | Loss: 0.00002042
Iteration 17/1000 | Loss: 0.00002035
Iteration 18/1000 | Loss: 0.00002031
Iteration 19/1000 | Loss: 0.00002029
Iteration 20/1000 | Loss: 0.00002028
Iteration 21/1000 | Loss: 0.00002026
Iteration 22/1000 | Loss: 0.00002025
Iteration 23/1000 | Loss: 0.00002025
Iteration 24/1000 | Loss: 0.00002024
Iteration 25/1000 | Loss: 0.00002023
Iteration 26/1000 | Loss: 0.00002023
Iteration 27/1000 | Loss: 0.00002022
Iteration 28/1000 | Loss: 0.00002022
Iteration 29/1000 | Loss: 0.00002020
Iteration 30/1000 | Loss: 0.00002019
Iteration 31/1000 | Loss: 0.00002019
Iteration 32/1000 | Loss: 0.00002018
Iteration 33/1000 | Loss: 0.00002017
Iteration 34/1000 | Loss: 0.00002017
Iteration 35/1000 | Loss: 0.00002015
Iteration 36/1000 | Loss: 0.00002015
Iteration 37/1000 | Loss: 0.00002014
Iteration 38/1000 | Loss: 0.00002013
Iteration 39/1000 | Loss: 0.00002013
Iteration 40/1000 | Loss: 0.00002012
Iteration 41/1000 | Loss: 0.00002011
Iteration 42/1000 | Loss: 0.00002011
Iteration 43/1000 | Loss: 0.00002010
Iteration 44/1000 | Loss: 0.00002010
Iteration 45/1000 | Loss: 0.00002010
Iteration 46/1000 | Loss: 0.00002009
Iteration 47/1000 | Loss: 0.00002008
Iteration 48/1000 | Loss: 0.00002008
Iteration 49/1000 | Loss: 0.00002008
Iteration 50/1000 | Loss: 0.00002006
Iteration 51/1000 | Loss: 0.00002006
Iteration 52/1000 | Loss: 0.00002002
Iteration 53/1000 | Loss: 0.00002002
Iteration 54/1000 | Loss: 0.00002001
Iteration 55/1000 | Loss: 0.00002001
Iteration 56/1000 | Loss: 0.00002000
Iteration 57/1000 | Loss: 0.00002000
Iteration 58/1000 | Loss: 0.00001996
Iteration 59/1000 | Loss: 0.00001996
Iteration 60/1000 | Loss: 0.00001996
Iteration 61/1000 | Loss: 0.00001996
Iteration 62/1000 | Loss: 0.00001995
Iteration 63/1000 | Loss: 0.00001995
Iteration 64/1000 | Loss: 0.00001994
Iteration 65/1000 | Loss: 0.00001993
Iteration 66/1000 | Loss: 0.00001993
Iteration 67/1000 | Loss: 0.00001993
Iteration 68/1000 | Loss: 0.00001992
Iteration 69/1000 | Loss: 0.00001992
Iteration 70/1000 | Loss: 0.00001992
Iteration 71/1000 | Loss: 0.00001992
Iteration 72/1000 | Loss: 0.00001992
Iteration 73/1000 | Loss: 0.00001991
Iteration 74/1000 | Loss: 0.00001991
Iteration 75/1000 | Loss: 0.00001991
Iteration 76/1000 | Loss: 0.00001991
Iteration 77/1000 | Loss: 0.00001990
Iteration 78/1000 | Loss: 0.00001990
Iteration 79/1000 | Loss: 0.00001989
Iteration 80/1000 | Loss: 0.00001989
Iteration 81/1000 | Loss: 0.00001989
Iteration 82/1000 | Loss: 0.00001988
Iteration 83/1000 | Loss: 0.00001988
Iteration 84/1000 | Loss: 0.00001988
Iteration 85/1000 | Loss: 0.00001988
Iteration 86/1000 | Loss: 0.00001987
Iteration 87/1000 | Loss: 0.00001987
Iteration 88/1000 | Loss: 0.00001987
Iteration 89/1000 | Loss: 0.00001987
Iteration 90/1000 | Loss: 0.00001986
Iteration 91/1000 | Loss: 0.00001986
Iteration 92/1000 | Loss: 0.00001986
Iteration 93/1000 | Loss: 0.00001986
Iteration 94/1000 | Loss: 0.00001986
Iteration 95/1000 | Loss: 0.00001985
Iteration 96/1000 | Loss: 0.00001985
Iteration 97/1000 | Loss: 0.00001985
Iteration 98/1000 | Loss: 0.00001985
Iteration 99/1000 | Loss: 0.00001985
Iteration 100/1000 | Loss: 0.00001985
Iteration 101/1000 | Loss: 0.00001985
Iteration 102/1000 | Loss: 0.00001985
Iteration 103/1000 | Loss: 0.00001984
Iteration 104/1000 | Loss: 0.00001984
Iteration 105/1000 | Loss: 0.00001984
Iteration 106/1000 | Loss: 0.00001984
Iteration 107/1000 | Loss: 0.00001984
Iteration 108/1000 | Loss: 0.00001984
Iteration 109/1000 | Loss: 0.00001983
Iteration 110/1000 | Loss: 0.00001983
Iteration 111/1000 | Loss: 0.00001983
Iteration 112/1000 | Loss: 0.00001983
Iteration 113/1000 | Loss: 0.00001983
Iteration 114/1000 | Loss: 0.00001983
Iteration 115/1000 | Loss: 0.00001982
Iteration 116/1000 | Loss: 0.00001982
Iteration 117/1000 | Loss: 0.00001982
Iteration 118/1000 | Loss: 0.00001982
Iteration 119/1000 | Loss: 0.00001982
Iteration 120/1000 | Loss: 0.00001982
Iteration 121/1000 | Loss: 0.00001982
Iteration 122/1000 | Loss: 0.00001982
Iteration 123/1000 | Loss: 0.00001982
Iteration 124/1000 | Loss: 0.00001982
Iteration 125/1000 | Loss: 0.00001982
Iteration 126/1000 | Loss: 0.00001981
Iteration 127/1000 | Loss: 0.00001981
Iteration 128/1000 | Loss: 0.00001981
Iteration 129/1000 | Loss: 0.00001981
Iteration 130/1000 | Loss: 0.00001981
Iteration 131/1000 | Loss: 0.00001981
Iteration 132/1000 | Loss: 0.00001981
Iteration 133/1000 | Loss: 0.00001981
Iteration 134/1000 | Loss: 0.00001981
Iteration 135/1000 | Loss: 0.00001981
Iteration 136/1000 | Loss: 0.00001980
Iteration 137/1000 | Loss: 0.00001980
Iteration 138/1000 | Loss: 0.00001980
Iteration 139/1000 | Loss: 0.00001980
Iteration 140/1000 | Loss: 0.00001980
Iteration 141/1000 | Loss: 0.00001980
Iteration 142/1000 | Loss: 0.00001979
Iteration 143/1000 | Loss: 0.00001979
Iteration 144/1000 | Loss: 0.00001979
Iteration 145/1000 | Loss: 0.00001979
Iteration 146/1000 | Loss: 0.00001979
Iteration 147/1000 | Loss: 0.00001979
Iteration 148/1000 | Loss: 0.00001979
Iteration 149/1000 | Loss: 0.00001979
Iteration 150/1000 | Loss: 0.00001978
Iteration 151/1000 | Loss: 0.00001978
Iteration 152/1000 | Loss: 0.00001978
Iteration 153/1000 | Loss: 0.00001978
Iteration 154/1000 | Loss: 0.00001978
Iteration 155/1000 | Loss: 0.00001978
Iteration 156/1000 | Loss: 0.00001978
Iteration 157/1000 | Loss: 0.00001978
Iteration 158/1000 | Loss: 0.00001978
Iteration 159/1000 | Loss: 0.00001977
Iteration 160/1000 | Loss: 0.00001977
Iteration 161/1000 | Loss: 0.00001977
Iteration 162/1000 | Loss: 0.00001977
Iteration 163/1000 | Loss: 0.00001977
Iteration 164/1000 | Loss: 0.00001977
Iteration 165/1000 | Loss: 0.00001977
Iteration 166/1000 | Loss: 0.00001977
Iteration 167/1000 | Loss: 0.00001977
Iteration 168/1000 | Loss: 0.00001977
Iteration 169/1000 | Loss: 0.00001977
Iteration 170/1000 | Loss: 0.00001977
Iteration 171/1000 | Loss: 0.00001977
Iteration 172/1000 | Loss: 0.00001977
Iteration 173/1000 | Loss: 0.00001977
Iteration 174/1000 | Loss: 0.00001977
Iteration 175/1000 | Loss: 0.00001977
Iteration 176/1000 | Loss: 0.00001977
Iteration 177/1000 | Loss: 0.00001976
Iteration 178/1000 | Loss: 0.00001976
Iteration 179/1000 | Loss: 0.00001976
Iteration 180/1000 | Loss: 0.00001976
Iteration 181/1000 | Loss: 0.00001976
Iteration 182/1000 | Loss: 0.00001976
Iteration 183/1000 | Loss: 0.00001976
Iteration 184/1000 | Loss: 0.00001976
Iteration 185/1000 | Loss: 0.00001976
Iteration 186/1000 | Loss: 0.00001976
Iteration 187/1000 | Loss: 0.00001976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.9762948795687407e-05, 1.9762948795687407e-05, 1.9762948795687407e-05, 1.9762948795687407e-05, 1.9762948795687407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9762948795687407e-05

Optimization complete. Final v2v error: 3.4502599239349365 mm

Highest mean error: 4.50387716293335 mm for frame 76

Lowest mean error: 2.990370035171509 mm for frame 116

Saving results

Total time: 41.871803522109985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441637
Iteration 2/25 | Loss: 0.00122919
Iteration 3/25 | Loss: 0.00114224
Iteration 4/25 | Loss: 0.00113315
Iteration 5/25 | Loss: 0.00113079
Iteration 6/25 | Loss: 0.00113028
Iteration 7/25 | Loss: 0.00113028
Iteration 8/25 | Loss: 0.00113028
Iteration 9/25 | Loss: 0.00113028
Iteration 10/25 | Loss: 0.00113028
Iteration 11/25 | Loss: 0.00113028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011302847415208817, 0.0011302847415208817, 0.0011302847415208817, 0.0011302847415208817, 0.0011302847415208817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011302847415208817

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36618853
Iteration 2/25 | Loss: 0.00084503
Iteration 3/25 | Loss: 0.00084503
Iteration 4/25 | Loss: 0.00084503
Iteration 5/25 | Loss: 0.00084503
Iteration 6/25 | Loss: 0.00084503
Iteration 7/25 | Loss: 0.00084502
Iteration 8/25 | Loss: 0.00084502
Iteration 9/25 | Loss: 0.00084502
Iteration 10/25 | Loss: 0.00084502
Iteration 11/25 | Loss: 0.00084502
Iteration 12/25 | Loss: 0.00084502
Iteration 13/25 | Loss: 0.00084502
Iteration 14/25 | Loss: 0.00084502
Iteration 15/25 | Loss: 0.00084502
Iteration 16/25 | Loss: 0.00084502
Iteration 17/25 | Loss: 0.00084502
Iteration 18/25 | Loss: 0.00084502
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008450235472992063, 0.0008450235472992063, 0.0008450235472992063, 0.0008450235472992063, 0.0008450235472992063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008450235472992063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084502
Iteration 2/1000 | Loss: 0.00002679
Iteration 3/1000 | Loss: 0.00001807
Iteration 4/1000 | Loss: 0.00001638
Iteration 5/1000 | Loss: 0.00001563
Iteration 6/1000 | Loss: 0.00001514
Iteration 7/1000 | Loss: 0.00001489
Iteration 8/1000 | Loss: 0.00001467
Iteration 9/1000 | Loss: 0.00001463
Iteration 10/1000 | Loss: 0.00001449
Iteration 11/1000 | Loss: 0.00001449
Iteration 12/1000 | Loss: 0.00001447
Iteration 13/1000 | Loss: 0.00001447
Iteration 14/1000 | Loss: 0.00001441
Iteration 15/1000 | Loss: 0.00001441
Iteration 16/1000 | Loss: 0.00001440
Iteration 17/1000 | Loss: 0.00001440
Iteration 18/1000 | Loss: 0.00001434
Iteration 19/1000 | Loss: 0.00001434
Iteration 20/1000 | Loss: 0.00001433
Iteration 21/1000 | Loss: 0.00001432
Iteration 22/1000 | Loss: 0.00001432
Iteration 23/1000 | Loss: 0.00001431
Iteration 24/1000 | Loss: 0.00001431
Iteration 25/1000 | Loss: 0.00001431
Iteration 26/1000 | Loss: 0.00001430
Iteration 27/1000 | Loss: 0.00001430
Iteration 28/1000 | Loss: 0.00001427
Iteration 29/1000 | Loss: 0.00001421
Iteration 30/1000 | Loss: 0.00001420
Iteration 31/1000 | Loss: 0.00001420
Iteration 32/1000 | Loss: 0.00001419
Iteration 33/1000 | Loss: 0.00001419
Iteration 34/1000 | Loss: 0.00001418
Iteration 35/1000 | Loss: 0.00001418
Iteration 36/1000 | Loss: 0.00001417
Iteration 37/1000 | Loss: 0.00001417
Iteration 38/1000 | Loss: 0.00001416
Iteration 39/1000 | Loss: 0.00001416
Iteration 40/1000 | Loss: 0.00001416
Iteration 41/1000 | Loss: 0.00001416
Iteration 42/1000 | Loss: 0.00001415
Iteration 43/1000 | Loss: 0.00001415
Iteration 44/1000 | Loss: 0.00001414
Iteration 45/1000 | Loss: 0.00001414
Iteration 46/1000 | Loss: 0.00001414
Iteration 47/1000 | Loss: 0.00001413
Iteration 48/1000 | Loss: 0.00001413
Iteration 49/1000 | Loss: 0.00001413
Iteration 50/1000 | Loss: 0.00001413
Iteration 51/1000 | Loss: 0.00001412
Iteration 52/1000 | Loss: 0.00001411
Iteration 53/1000 | Loss: 0.00001411
Iteration 54/1000 | Loss: 0.00001410
Iteration 55/1000 | Loss: 0.00001410
Iteration 56/1000 | Loss: 0.00001409
Iteration 57/1000 | Loss: 0.00001409
Iteration 58/1000 | Loss: 0.00001408
Iteration 59/1000 | Loss: 0.00001407
Iteration 60/1000 | Loss: 0.00001407
Iteration 61/1000 | Loss: 0.00001406
Iteration 62/1000 | Loss: 0.00001406
Iteration 63/1000 | Loss: 0.00001405
Iteration 64/1000 | Loss: 0.00001405
Iteration 65/1000 | Loss: 0.00001404
Iteration 66/1000 | Loss: 0.00001404
Iteration 67/1000 | Loss: 0.00001404
Iteration 68/1000 | Loss: 0.00001404
Iteration 69/1000 | Loss: 0.00001403
Iteration 70/1000 | Loss: 0.00001403
Iteration 71/1000 | Loss: 0.00001403
Iteration 72/1000 | Loss: 0.00001402
Iteration 73/1000 | Loss: 0.00001402
Iteration 74/1000 | Loss: 0.00001402
Iteration 75/1000 | Loss: 0.00001401
Iteration 76/1000 | Loss: 0.00001401
Iteration 77/1000 | Loss: 0.00001401
Iteration 78/1000 | Loss: 0.00001400
Iteration 79/1000 | Loss: 0.00001400
Iteration 80/1000 | Loss: 0.00001400
Iteration 81/1000 | Loss: 0.00001399
Iteration 82/1000 | Loss: 0.00001399
Iteration 83/1000 | Loss: 0.00001399
Iteration 84/1000 | Loss: 0.00001398
Iteration 85/1000 | Loss: 0.00001398
Iteration 86/1000 | Loss: 0.00001398
Iteration 87/1000 | Loss: 0.00001397
Iteration 88/1000 | Loss: 0.00001397
Iteration 89/1000 | Loss: 0.00001397
Iteration 90/1000 | Loss: 0.00001397
Iteration 91/1000 | Loss: 0.00001397
Iteration 92/1000 | Loss: 0.00001397
Iteration 93/1000 | Loss: 0.00001397
Iteration 94/1000 | Loss: 0.00001397
Iteration 95/1000 | Loss: 0.00001396
Iteration 96/1000 | Loss: 0.00001396
Iteration 97/1000 | Loss: 0.00001396
Iteration 98/1000 | Loss: 0.00001396
Iteration 99/1000 | Loss: 0.00001396
Iteration 100/1000 | Loss: 0.00001396
Iteration 101/1000 | Loss: 0.00001396
Iteration 102/1000 | Loss: 0.00001396
Iteration 103/1000 | Loss: 0.00001396
Iteration 104/1000 | Loss: 0.00001396
Iteration 105/1000 | Loss: 0.00001396
Iteration 106/1000 | Loss: 0.00001396
Iteration 107/1000 | Loss: 0.00001396
Iteration 108/1000 | Loss: 0.00001395
Iteration 109/1000 | Loss: 0.00001394
Iteration 110/1000 | Loss: 0.00001394
Iteration 111/1000 | Loss: 0.00001394
Iteration 112/1000 | Loss: 0.00001394
Iteration 113/1000 | Loss: 0.00001394
Iteration 114/1000 | Loss: 0.00001394
Iteration 115/1000 | Loss: 0.00001393
Iteration 116/1000 | Loss: 0.00001393
Iteration 117/1000 | Loss: 0.00001393
Iteration 118/1000 | Loss: 0.00001393
Iteration 119/1000 | Loss: 0.00001392
Iteration 120/1000 | Loss: 0.00001392
Iteration 121/1000 | Loss: 0.00001392
Iteration 122/1000 | Loss: 0.00001391
Iteration 123/1000 | Loss: 0.00001391
Iteration 124/1000 | Loss: 0.00001391
Iteration 125/1000 | Loss: 0.00001391
Iteration 126/1000 | Loss: 0.00001391
Iteration 127/1000 | Loss: 0.00001390
Iteration 128/1000 | Loss: 0.00001390
Iteration 129/1000 | Loss: 0.00001390
Iteration 130/1000 | Loss: 0.00001390
Iteration 131/1000 | Loss: 0.00001390
Iteration 132/1000 | Loss: 0.00001390
Iteration 133/1000 | Loss: 0.00001390
Iteration 134/1000 | Loss: 0.00001389
Iteration 135/1000 | Loss: 0.00001389
Iteration 136/1000 | Loss: 0.00001389
Iteration 137/1000 | Loss: 0.00001388
Iteration 138/1000 | Loss: 0.00001388
Iteration 139/1000 | Loss: 0.00001388
Iteration 140/1000 | Loss: 0.00001388
Iteration 141/1000 | Loss: 0.00001387
Iteration 142/1000 | Loss: 0.00001387
Iteration 143/1000 | Loss: 0.00001387
Iteration 144/1000 | Loss: 0.00001387
Iteration 145/1000 | Loss: 0.00001387
Iteration 146/1000 | Loss: 0.00001387
Iteration 147/1000 | Loss: 0.00001387
Iteration 148/1000 | Loss: 0.00001387
Iteration 149/1000 | Loss: 0.00001386
Iteration 150/1000 | Loss: 0.00001386
Iteration 151/1000 | Loss: 0.00001386
Iteration 152/1000 | Loss: 0.00001386
Iteration 153/1000 | Loss: 0.00001386
Iteration 154/1000 | Loss: 0.00001386
Iteration 155/1000 | Loss: 0.00001386
Iteration 156/1000 | Loss: 0.00001385
Iteration 157/1000 | Loss: 0.00001385
Iteration 158/1000 | Loss: 0.00001385
Iteration 159/1000 | Loss: 0.00001385
Iteration 160/1000 | Loss: 0.00001384
Iteration 161/1000 | Loss: 0.00001384
Iteration 162/1000 | Loss: 0.00001384
Iteration 163/1000 | Loss: 0.00001384
Iteration 164/1000 | Loss: 0.00001384
Iteration 165/1000 | Loss: 0.00001384
Iteration 166/1000 | Loss: 0.00001384
Iteration 167/1000 | Loss: 0.00001384
Iteration 168/1000 | Loss: 0.00001384
Iteration 169/1000 | Loss: 0.00001384
Iteration 170/1000 | Loss: 0.00001384
Iteration 171/1000 | Loss: 0.00001383
Iteration 172/1000 | Loss: 0.00001383
Iteration 173/1000 | Loss: 0.00001383
Iteration 174/1000 | Loss: 0.00001383
Iteration 175/1000 | Loss: 0.00001383
Iteration 176/1000 | Loss: 0.00001383
Iteration 177/1000 | Loss: 0.00001383
Iteration 178/1000 | Loss: 0.00001383
Iteration 179/1000 | Loss: 0.00001383
Iteration 180/1000 | Loss: 0.00001383
Iteration 181/1000 | Loss: 0.00001383
Iteration 182/1000 | Loss: 0.00001383
Iteration 183/1000 | Loss: 0.00001383
Iteration 184/1000 | Loss: 0.00001383
Iteration 185/1000 | Loss: 0.00001383
Iteration 186/1000 | Loss: 0.00001383
Iteration 187/1000 | Loss: 0.00001383
Iteration 188/1000 | Loss: 0.00001383
Iteration 189/1000 | Loss: 0.00001383
Iteration 190/1000 | Loss: 0.00001383
Iteration 191/1000 | Loss: 0.00001383
Iteration 192/1000 | Loss: 0.00001383
Iteration 193/1000 | Loss: 0.00001383
Iteration 194/1000 | Loss: 0.00001383
Iteration 195/1000 | Loss: 0.00001383
Iteration 196/1000 | Loss: 0.00001383
Iteration 197/1000 | Loss: 0.00001383
Iteration 198/1000 | Loss: 0.00001383
Iteration 199/1000 | Loss: 0.00001383
Iteration 200/1000 | Loss: 0.00001383
Iteration 201/1000 | Loss: 0.00001383
Iteration 202/1000 | Loss: 0.00001383
Iteration 203/1000 | Loss: 0.00001383
Iteration 204/1000 | Loss: 0.00001383
Iteration 205/1000 | Loss: 0.00001383
Iteration 206/1000 | Loss: 0.00001383
Iteration 207/1000 | Loss: 0.00001383
Iteration 208/1000 | Loss: 0.00001383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.3830069292453118e-05, 1.3830069292453118e-05, 1.3830069292453118e-05, 1.3830069292453118e-05, 1.3830069292453118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3830069292453118e-05

Optimization complete. Final v2v error: 3.0480856895446777 mm

Highest mean error: 3.465921401977539 mm for frame 145

Lowest mean error: 2.6992077827453613 mm for frame 131

Saving results

Total time: 42.05371952056885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00662514
Iteration 2/25 | Loss: 0.00171456
Iteration 3/25 | Loss: 0.00130768
Iteration 4/25 | Loss: 0.00126740
Iteration 5/25 | Loss: 0.00125734
Iteration 6/25 | Loss: 0.00126237
Iteration 7/25 | Loss: 0.00124465
Iteration 8/25 | Loss: 0.00123447
Iteration 9/25 | Loss: 0.00123354
Iteration 10/25 | Loss: 0.00123333
Iteration 11/25 | Loss: 0.00123124
Iteration 12/25 | Loss: 0.00123098
Iteration 13/25 | Loss: 0.00123370
Iteration 14/25 | Loss: 0.00123323
Iteration 15/25 | Loss: 0.00123112
Iteration 16/25 | Loss: 0.00123366
Iteration 17/25 | Loss: 0.00123291
Iteration 18/25 | Loss: 0.00123352
Iteration 19/25 | Loss: 0.00123281
Iteration 20/25 | Loss: 0.00123340
Iteration 21/25 | Loss: 0.00123275
Iteration 22/25 | Loss: 0.00123312
Iteration 23/25 | Loss: 0.00123248
Iteration 24/25 | Loss: 0.00123236
Iteration 25/25 | Loss: 0.00123169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46811092
Iteration 2/25 | Loss: 0.00076744
Iteration 3/25 | Loss: 0.00076742
Iteration 4/25 | Loss: 0.00076742
Iteration 5/25 | Loss: 0.00076742
Iteration 6/25 | Loss: 0.00076742
Iteration 7/25 | Loss: 0.00076742
Iteration 8/25 | Loss: 0.00076742
Iteration 9/25 | Loss: 0.00076742
Iteration 10/25 | Loss: 0.00076742
Iteration 11/25 | Loss: 0.00076742
Iteration 12/25 | Loss: 0.00076742
Iteration 13/25 | Loss: 0.00076742
Iteration 14/25 | Loss: 0.00076742
Iteration 15/25 | Loss: 0.00076742
Iteration 16/25 | Loss: 0.00076742
Iteration 17/25 | Loss: 0.00076742
Iteration 18/25 | Loss: 0.00076742
Iteration 19/25 | Loss: 0.00076742
Iteration 20/25 | Loss: 0.00076742
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007674176595173776, 0.0007674176595173776, 0.0007674176595173776, 0.0007674176595173776, 0.0007674176595173776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007674176595173776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076742
Iteration 2/1000 | Loss: 0.00011249
Iteration 3/1000 | Loss: 0.00004450
Iteration 4/1000 | Loss: 0.00007748
Iteration 5/1000 | Loss: 0.00006583
Iteration 6/1000 | Loss: 0.00005420
Iteration 7/1000 | Loss: 0.00004347
Iteration 8/1000 | Loss: 0.00003727
Iteration 9/1000 | Loss: 0.00003185
Iteration 10/1000 | Loss: 0.00002891
Iteration 11/1000 | Loss: 0.00002767
Iteration 12/1000 | Loss: 0.00002685
Iteration 13/1000 | Loss: 0.00002640
Iteration 14/1000 | Loss: 0.00002608
Iteration 15/1000 | Loss: 0.00002573
Iteration 16/1000 | Loss: 0.00002537
Iteration 17/1000 | Loss: 0.00002510
Iteration 18/1000 | Loss: 0.00002496
Iteration 19/1000 | Loss: 0.00002492
Iteration 20/1000 | Loss: 0.00002492
Iteration 21/1000 | Loss: 0.00002491
Iteration 22/1000 | Loss: 0.00002491
Iteration 23/1000 | Loss: 0.00002490
Iteration 24/1000 | Loss: 0.00002490
Iteration 25/1000 | Loss: 0.00002490
Iteration 26/1000 | Loss: 0.00002489
Iteration 27/1000 | Loss: 0.00002489
Iteration 28/1000 | Loss: 0.00002489
Iteration 29/1000 | Loss: 0.00002488
Iteration 30/1000 | Loss: 0.00002483
Iteration 31/1000 | Loss: 0.00002479
Iteration 32/1000 | Loss: 0.00002479
Iteration 33/1000 | Loss: 0.00002478
Iteration 34/1000 | Loss: 0.00002476
Iteration 35/1000 | Loss: 0.00002475
Iteration 36/1000 | Loss: 0.00002475
Iteration 37/1000 | Loss: 0.00002473
Iteration 38/1000 | Loss: 0.00002466
Iteration 39/1000 | Loss: 0.00002465
Iteration 40/1000 | Loss: 0.00002465
Iteration 41/1000 | Loss: 0.00002464
Iteration 42/1000 | Loss: 0.00002464
Iteration 43/1000 | Loss: 0.00002463
Iteration 44/1000 | Loss: 0.00002463
Iteration 45/1000 | Loss: 0.00002462
Iteration 46/1000 | Loss: 0.00002462
Iteration 47/1000 | Loss: 0.00002458
Iteration 48/1000 | Loss: 0.00002456
Iteration 49/1000 | Loss: 0.00002456
Iteration 50/1000 | Loss: 0.00002455
Iteration 51/1000 | Loss: 0.00002455
Iteration 52/1000 | Loss: 0.00002455
Iteration 53/1000 | Loss: 0.00002452
Iteration 54/1000 | Loss: 0.00002452
Iteration 55/1000 | Loss: 0.00002446
Iteration 56/1000 | Loss: 0.00002446
Iteration 57/1000 | Loss: 0.00002440
Iteration 58/1000 | Loss: 0.00002437
Iteration 59/1000 | Loss: 0.00002437
Iteration 60/1000 | Loss: 0.00002436
Iteration 61/1000 | Loss: 0.00002436
Iteration 62/1000 | Loss: 0.00002436
Iteration 63/1000 | Loss: 0.00002436
Iteration 64/1000 | Loss: 0.00002435
Iteration 65/1000 | Loss: 0.00002435
Iteration 66/1000 | Loss: 0.00002435
Iteration 67/1000 | Loss: 0.00002435
Iteration 68/1000 | Loss: 0.00002435
Iteration 69/1000 | Loss: 0.00002435
Iteration 70/1000 | Loss: 0.00002434
Iteration 71/1000 | Loss: 0.00002434
Iteration 72/1000 | Loss: 0.00002433
Iteration 73/1000 | Loss: 0.00002433
Iteration 74/1000 | Loss: 0.00002432
Iteration 75/1000 | Loss: 0.00002429
Iteration 76/1000 | Loss: 0.00002429
Iteration 77/1000 | Loss: 0.00002429
Iteration 78/1000 | Loss: 0.00002428
Iteration 79/1000 | Loss: 0.00002428
Iteration 80/1000 | Loss: 0.00002427
Iteration 81/1000 | Loss: 0.00002427
Iteration 82/1000 | Loss: 0.00002426
Iteration 83/1000 | Loss: 0.00002425
Iteration 84/1000 | Loss: 0.00002419
Iteration 85/1000 | Loss: 0.00002945
Iteration 86/1000 | Loss: 0.00002446
Iteration 87/1000 | Loss: 0.00002416
Iteration 88/1000 | Loss: 0.00002397
Iteration 89/1000 | Loss: 0.00002394
Iteration 90/1000 | Loss: 0.00002392
Iteration 91/1000 | Loss: 0.00002392
Iteration 92/1000 | Loss: 0.00002392
Iteration 93/1000 | Loss: 0.00002392
Iteration 94/1000 | Loss: 0.00002392
Iteration 95/1000 | Loss: 0.00002392
Iteration 96/1000 | Loss: 0.00002392
Iteration 97/1000 | Loss: 0.00002391
Iteration 98/1000 | Loss: 0.00002391
Iteration 99/1000 | Loss: 0.00002391
Iteration 100/1000 | Loss: 0.00002391
Iteration 101/1000 | Loss: 0.00002391
Iteration 102/1000 | Loss: 0.00002391
Iteration 103/1000 | Loss: 0.00002391
Iteration 104/1000 | Loss: 0.00002390
Iteration 105/1000 | Loss: 0.00002390
Iteration 106/1000 | Loss: 0.00002389
Iteration 107/1000 | Loss: 0.00002389
Iteration 108/1000 | Loss: 0.00002389
Iteration 109/1000 | Loss: 0.00002388
Iteration 110/1000 | Loss: 0.00002388
Iteration 111/1000 | Loss: 0.00002388
Iteration 112/1000 | Loss: 0.00002388
Iteration 113/1000 | Loss: 0.00002388
Iteration 114/1000 | Loss: 0.00002388
Iteration 115/1000 | Loss: 0.00002388
Iteration 116/1000 | Loss: 0.00002387
Iteration 117/1000 | Loss: 0.00002387
Iteration 118/1000 | Loss: 0.00002387
Iteration 119/1000 | Loss: 0.00002387
Iteration 120/1000 | Loss: 0.00002387
Iteration 121/1000 | Loss: 0.00002387
Iteration 122/1000 | Loss: 0.00002387
Iteration 123/1000 | Loss: 0.00002386
Iteration 124/1000 | Loss: 0.00002386
Iteration 125/1000 | Loss: 0.00002386
Iteration 126/1000 | Loss: 0.00002386
Iteration 127/1000 | Loss: 0.00002386
Iteration 128/1000 | Loss: 0.00002386
Iteration 129/1000 | Loss: 0.00002386
Iteration 130/1000 | Loss: 0.00002386
Iteration 131/1000 | Loss: 0.00002385
Iteration 132/1000 | Loss: 0.00002385
Iteration 133/1000 | Loss: 0.00002385
Iteration 134/1000 | Loss: 0.00002385
Iteration 135/1000 | Loss: 0.00002385
Iteration 136/1000 | Loss: 0.00002384
Iteration 137/1000 | Loss: 0.00002384
Iteration 138/1000 | Loss: 0.00002384
Iteration 139/1000 | Loss: 0.00002384
Iteration 140/1000 | Loss: 0.00002384
Iteration 141/1000 | Loss: 0.00002383
Iteration 142/1000 | Loss: 0.00002383
Iteration 143/1000 | Loss: 0.00002383
Iteration 144/1000 | Loss: 0.00002383
Iteration 145/1000 | Loss: 0.00002383
Iteration 146/1000 | Loss: 0.00002383
Iteration 147/1000 | Loss: 0.00002383
Iteration 148/1000 | Loss: 0.00002383
Iteration 149/1000 | Loss: 0.00002383
Iteration 150/1000 | Loss: 0.00002383
Iteration 151/1000 | Loss: 0.00002383
Iteration 152/1000 | Loss: 0.00002383
Iteration 153/1000 | Loss: 0.00002382
Iteration 154/1000 | Loss: 0.00002382
Iteration 155/1000 | Loss: 0.00002382
Iteration 156/1000 | Loss: 0.00002382
Iteration 157/1000 | Loss: 0.00002382
Iteration 158/1000 | Loss: 0.00002382
Iteration 159/1000 | Loss: 0.00002382
Iteration 160/1000 | Loss: 0.00002382
Iteration 161/1000 | Loss: 0.00002382
Iteration 162/1000 | Loss: 0.00002382
Iteration 163/1000 | Loss: 0.00002382
Iteration 164/1000 | Loss: 0.00002382
Iteration 165/1000 | Loss: 0.00002382
Iteration 166/1000 | Loss: 0.00002382
Iteration 167/1000 | Loss: 0.00002382
Iteration 168/1000 | Loss: 0.00002382
Iteration 169/1000 | Loss: 0.00002382
Iteration 170/1000 | Loss: 0.00002382
Iteration 171/1000 | Loss: 0.00002382
Iteration 172/1000 | Loss: 0.00002382
Iteration 173/1000 | Loss: 0.00002382
Iteration 174/1000 | Loss: 0.00002382
Iteration 175/1000 | Loss: 0.00002382
Iteration 176/1000 | Loss: 0.00002382
Iteration 177/1000 | Loss: 0.00002382
Iteration 178/1000 | Loss: 0.00002381
Iteration 179/1000 | Loss: 0.00002381
Iteration 180/1000 | Loss: 0.00002381
Iteration 181/1000 | Loss: 0.00002381
Iteration 182/1000 | Loss: 0.00002381
Iteration 183/1000 | Loss: 0.00002381
Iteration 184/1000 | Loss: 0.00002381
Iteration 185/1000 | Loss: 0.00002381
Iteration 186/1000 | Loss: 0.00002381
Iteration 187/1000 | Loss: 0.00002381
Iteration 188/1000 | Loss: 0.00002381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.3814378437236883e-05, 2.3814378437236883e-05, 2.3814378437236883e-05, 2.3814378437236883e-05, 2.3814378437236883e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3814378437236883e-05

Optimization complete. Final v2v error: 4.051265239715576 mm

Highest mean error: 5.119466304779053 mm for frame 162

Lowest mean error: 3.3844380378723145 mm for frame 136

Saving results

Total time: 107.96660494804382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056594
Iteration 2/25 | Loss: 0.01056594
Iteration 3/25 | Loss: 0.01056594
Iteration 4/25 | Loss: 0.01056594
Iteration 5/25 | Loss: 0.01056594
Iteration 6/25 | Loss: 0.01056594
Iteration 7/25 | Loss: 0.01056593
Iteration 8/25 | Loss: 0.01056593
Iteration 9/25 | Loss: 0.01056593
Iteration 10/25 | Loss: 0.01056593
Iteration 11/25 | Loss: 0.01056593
Iteration 12/25 | Loss: 0.01056593
Iteration 13/25 | Loss: 0.01056593
Iteration 14/25 | Loss: 0.01056593
Iteration 15/25 | Loss: 0.01056593
Iteration 16/25 | Loss: 0.01056593
Iteration 17/25 | Loss: 0.01056593
Iteration 18/25 | Loss: 0.01056593
Iteration 19/25 | Loss: 0.01056593
Iteration 20/25 | Loss: 0.01056593
Iteration 21/25 | Loss: 0.01056593
Iteration 22/25 | Loss: 0.01056593
Iteration 23/25 | Loss: 0.01056593
Iteration 24/25 | Loss: 0.01056592
Iteration 25/25 | Loss: 0.01056592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51399744
Iteration 2/25 | Loss: 0.10480999
Iteration 3/25 | Loss: 0.09094790
Iteration 4/25 | Loss: 0.09081309
Iteration 5/25 | Loss: 0.09081307
Iteration 6/25 | Loss: 0.09081307
Iteration 7/25 | Loss: 0.09081305
Iteration 8/25 | Loss: 0.09081305
Iteration 9/25 | Loss: 0.09081305
Iteration 10/25 | Loss: 0.09081305
Iteration 11/25 | Loss: 0.09081304
Iteration 12/25 | Loss: 0.09081303
Iteration 13/25 | Loss: 0.09081303
Iteration 14/25 | Loss: 0.09081303
Iteration 15/25 | Loss: 0.09081303
Iteration 16/25 | Loss: 0.09081303
Iteration 17/25 | Loss: 0.09081303
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0908130332827568, 0.0908130332827568, 0.0908130332827568, 0.0908130332827568, 0.0908130332827568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0908130332827568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09081303
Iteration 2/1000 | Loss: 0.00188034
Iteration 3/1000 | Loss: 0.00360043
Iteration 4/1000 | Loss: 0.00049305
Iteration 5/1000 | Loss: 0.00084262
Iteration 6/1000 | Loss: 0.00050559
Iteration 7/1000 | Loss: 0.00005024
Iteration 8/1000 | Loss: 0.00061565
Iteration 9/1000 | Loss: 0.00011948
Iteration 10/1000 | Loss: 0.00029851
Iteration 11/1000 | Loss: 0.00009590
Iteration 12/1000 | Loss: 0.00005235
Iteration 13/1000 | Loss: 0.00022773
Iteration 14/1000 | Loss: 0.00025355
Iteration 15/1000 | Loss: 0.00056614
Iteration 16/1000 | Loss: 0.00005603
Iteration 17/1000 | Loss: 0.00003816
Iteration 18/1000 | Loss: 0.00001932
Iteration 19/1000 | Loss: 0.00001781
Iteration 20/1000 | Loss: 0.00001649
Iteration 21/1000 | Loss: 0.00020578
Iteration 22/1000 | Loss: 0.00002293
Iteration 23/1000 | Loss: 0.00002796
Iteration 24/1000 | Loss: 0.00001492
Iteration 25/1000 | Loss: 0.00016716
Iteration 26/1000 | Loss: 0.00010661
Iteration 27/1000 | Loss: 0.00018398
Iteration 28/1000 | Loss: 0.00029102
Iteration 29/1000 | Loss: 0.00008908
Iteration 30/1000 | Loss: 0.00018464
Iteration 31/1000 | Loss: 0.00001402
Iteration 32/1000 | Loss: 0.00001353
Iteration 33/1000 | Loss: 0.00001326
Iteration 34/1000 | Loss: 0.00025990
Iteration 35/1000 | Loss: 0.00002135
Iteration 36/1000 | Loss: 0.00003239
Iteration 37/1000 | Loss: 0.00001280
Iteration 38/1000 | Loss: 0.00001262
Iteration 39/1000 | Loss: 0.00001244
Iteration 40/1000 | Loss: 0.00001220
Iteration 41/1000 | Loss: 0.00012438
Iteration 42/1000 | Loss: 0.00001199
Iteration 43/1000 | Loss: 0.00010031
Iteration 44/1000 | Loss: 0.00001176
Iteration 45/1000 | Loss: 0.00001171
Iteration 46/1000 | Loss: 0.00001170
Iteration 47/1000 | Loss: 0.00001170
Iteration 48/1000 | Loss: 0.00001163
Iteration 49/1000 | Loss: 0.00001162
Iteration 50/1000 | Loss: 0.00001161
Iteration 51/1000 | Loss: 0.00001161
Iteration 52/1000 | Loss: 0.00001160
Iteration 53/1000 | Loss: 0.00008739
Iteration 54/1000 | Loss: 0.00001180
Iteration 55/1000 | Loss: 0.00003928
Iteration 56/1000 | Loss: 0.00001136
Iteration 57/1000 | Loss: 0.00001133
Iteration 58/1000 | Loss: 0.00001131
Iteration 59/1000 | Loss: 0.00001129
Iteration 60/1000 | Loss: 0.00001129
Iteration 61/1000 | Loss: 0.00001129
Iteration 62/1000 | Loss: 0.00001127
Iteration 63/1000 | Loss: 0.00001124
Iteration 64/1000 | Loss: 0.00001124
Iteration 65/1000 | Loss: 0.00001123
Iteration 66/1000 | Loss: 0.00001123
Iteration 67/1000 | Loss: 0.00001123
Iteration 68/1000 | Loss: 0.00001123
Iteration 69/1000 | Loss: 0.00001123
Iteration 70/1000 | Loss: 0.00001122
Iteration 71/1000 | Loss: 0.00001122
Iteration 72/1000 | Loss: 0.00001121
Iteration 73/1000 | Loss: 0.00001119
Iteration 74/1000 | Loss: 0.00004870
Iteration 75/1000 | Loss: 0.00001140
Iteration 76/1000 | Loss: 0.00001112
Iteration 77/1000 | Loss: 0.00001110
Iteration 78/1000 | Loss: 0.00001107
Iteration 79/1000 | Loss: 0.00001107
Iteration 80/1000 | Loss: 0.00001106
Iteration 81/1000 | Loss: 0.00001105
Iteration 82/1000 | Loss: 0.00001105
Iteration 83/1000 | Loss: 0.00001105
Iteration 84/1000 | Loss: 0.00001105
Iteration 85/1000 | Loss: 0.00001105
Iteration 86/1000 | Loss: 0.00001105
Iteration 87/1000 | Loss: 0.00001104
Iteration 88/1000 | Loss: 0.00001103
Iteration 89/1000 | Loss: 0.00001103
Iteration 90/1000 | Loss: 0.00001103
Iteration 91/1000 | Loss: 0.00001103
Iteration 92/1000 | Loss: 0.00001102
Iteration 93/1000 | Loss: 0.00001102
Iteration 94/1000 | Loss: 0.00001102
Iteration 95/1000 | Loss: 0.00001102
Iteration 96/1000 | Loss: 0.00001102
Iteration 97/1000 | Loss: 0.00001102
Iteration 98/1000 | Loss: 0.00001102
Iteration 99/1000 | Loss: 0.00001102
Iteration 100/1000 | Loss: 0.00001101
Iteration 101/1000 | Loss: 0.00001101
Iteration 102/1000 | Loss: 0.00001100
Iteration 103/1000 | Loss: 0.00001100
Iteration 104/1000 | Loss: 0.00001099
Iteration 105/1000 | Loss: 0.00001099
Iteration 106/1000 | Loss: 0.00001099
Iteration 107/1000 | Loss: 0.00001099
Iteration 108/1000 | Loss: 0.00001098
Iteration 109/1000 | Loss: 0.00001098
Iteration 110/1000 | Loss: 0.00001098
Iteration 111/1000 | Loss: 0.00001098
Iteration 112/1000 | Loss: 0.00001098
Iteration 113/1000 | Loss: 0.00001098
Iteration 114/1000 | Loss: 0.00001097
Iteration 115/1000 | Loss: 0.00001097
Iteration 116/1000 | Loss: 0.00001097
Iteration 117/1000 | Loss: 0.00001097
Iteration 118/1000 | Loss: 0.00001097
Iteration 119/1000 | Loss: 0.00001097
Iteration 120/1000 | Loss: 0.00001097
Iteration 121/1000 | Loss: 0.00001096
Iteration 122/1000 | Loss: 0.00001096
Iteration 123/1000 | Loss: 0.00001096
Iteration 124/1000 | Loss: 0.00001096
Iteration 125/1000 | Loss: 0.00001096
Iteration 126/1000 | Loss: 0.00001095
Iteration 127/1000 | Loss: 0.00001095
Iteration 128/1000 | Loss: 0.00001095
Iteration 129/1000 | Loss: 0.00001095
Iteration 130/1000 | Loss: 0.00001095
Iteration 131/1000 | Loss: 0.00001095
Iteration 132/1000 | Loss: 0.00001094
Iteration 133/1000 | Loss: 0.00001094
Iteration 134/1000 | Loss: 0.00001094
Iteration 135/1000 | Loss: 0.00001094
Iteration 136/1000 | Loss: 0.00001094
Iteration 137/1000 | Loss: 0.00001094
Iteration 138/1000 | Loss: 0.00001094
Iteration 139/1000 | Loss: 0.00001094
Iteration 140/1000 | Loss: 0.00001094
Iteration 141/1000 | Loss: 0.00001093
Iteration 142/1000 | Loss: 0.00001093
Iteration 143/1000 | Loss: 0.00001093
Iteration 144/1000 | Loss: 0.00001093
Iteration 145/1000 | Loss: 0.00001093
Iteration 146/1000 | Loss: 0.00001093
Iteration 147/1000 | Loss: 0.00001093
Iteration 148/1000 | Loss: 0.00001093
Iteration 149/1000 | Loss: 0.00001093
Iteration 150/1000 | Loss: 0.00001093
Iteration 151/1000 | Loss: 0.00001093
Iteration 152/1000 | Loss: 0.00001092
Iteration 153/1000 | Loss: 0.00001092
Iteration 154/1000 | Loss: 0.00007220
Iteration 155/1000 | Loss: 0.00002248
Iteration 156/1000 | Loss: 0.00001096
Iteration 157/1000 | Loss: 0.00001091
Iteration 158/1000 | Loss: 0.00001091
Iteration 159/1000 | Loss: 0.00001091
Iteration 160/1000 | Loss: 0.00001091
Iteration 161/1000 | Loss: 0.00001091
Iteration 162/1000 | Loss: 0.00003265
Iteration 163/1000 | Loss: 0.00001098
Iteration 164/1000 | Loss: 0.00001094
Iteration 165/1000 | Loss: 0.00001094
Iteration 166/1000 | Loss: 0.00001094
Iteration 167/1000 | Loss: 0.00001094
Iteration 168/1000 | Loss: 0.00001094
Iteration 169/1000 | Loss: 0.00001094
Iteration 170/1000 | Loss: 0.00001094
Iteration 171/1000 | Loss: 0.00001094
Iteration 172/1000 | Loss: 0.00001093
Iteration 173/1000 | Loss: 0.00001093
Iteration 174/1000 | Loss: 0.00001092
Iteration 175/1000 | Loss: 0.00001092
Iteration 176/1000 | Loss: 0.00001092
Iteration 177/1000 | Loss: 0.00001092
Iteration 178/1000 | Loss: 0.00001091
Iteration 179/1000 | Loss: 0.00001091
Iteration 180/1000 | Loss: 0.00001091
Iteration 181/1000 | Loss: 0.00001091
Iteration 182/1000 | Loss: 0.00001090
Iteration 183/1000 | Loss: 0.00001090
Iteration 184/1000 | Loss: 0.00001090
Iteration 185/1000 | Loss: 0.00001089
Iteration 186/1000 | Loss: 0.00001089
Iteration 187/1000 | Loss: 0.00001089
Iteration 188/1000 | Loss: 0.00001089
Iteration 189/1000 | Loss: 0.00001089
Iteration 190/1000 | Loss: 0.00001089
Iteration 191/1000 | Loss: 0.00001089
Iteration 192/1000 | Loss: 0.00001089
Iteration 193/1000 | Loss: 0.00001088
Iteration 194/1000 | Loss: 0.00001088
Iteration 195/1000 | Loss: 0.00001088
Iteration 196/1000 | Loss: 0.00001088
Iteration 197/1000 | Loss: 0.00001088
Iteration 198/1000 | Loss: 0.00001087
Iteration 199/1000 | Loss: 0.00001087
Iteration 200/1000 | Loss: 0.00004104
Iteration 201/1000 | Loss: 0.00001099
Iteration 202/1000 | Loss: 0.00001172
Iteration 203/1000 | Loss: 0.00001098
Iteration 204/1000 | Loss: 0.00001098
Iteration 205/1000 | Loss: 0.00001097
Iteration 206/1000 | Loss: 0.00001097
Iteration 207/1000 | Loss: 0.00001096
Iteration 208/1000 | Loss: 0.00001096
Iteration 209/1000 | Loss: 0.00001095
Iteration 210/1000 | Loss: 0.00001095
Iteration 211/1000 | Loss: 0.00001094
Iteration 212/1000 | Loss: 0.00001093
Iteration 213/1000 | Loss: 0.00001092
Iteration 214/1000 | Loss: 0.00001092
Iteration 215/1000 | Loss: 0.00001092
Iteration 216/1000 | Loss: 0.00001092
Iteration 217/1000 | Loss: 0.00001092
Iteration 218/1000 | Loss: 0.00001092
Iteration 219/1000 | Loss: 0.00001292
Iteration 220/1000 | Loss: 0.00001094
Iteration 221/1000 | Loss: 0.00001093
Iteration 222/1000 | Loss: 0.00001092
Iteration 223/1000 | Loss: 0.00001092
Iteration 224/1000 | Loss: 0.00001092
Iteration 225/1000 | Loss: 0.00001091
Iteration 226/1000 | Loss: 0.00001091
Iteration 227/1000 | Loss: 0.00001091
Iteration 228/1000 | Loss: 0.00001091
Iteration 229/1000 | Loss: 0.00001090
Iteration 230/1000 | Loss: 0.00001090
Iteration 231/1000 | Loss: 0.00001090
Iteration 232/1000 | Loss: 0.00001090
Iteration 233/1000 | Loss: 0.00001089
Iteration 234/1000 | Loss: 0.00001089
Iteration 235/1000 | Loss: 0.00001088
Iteration 236/1000 | Loss: 0.00001088
Iteration 237/1000 | Loss: 0.00001088
Iteration 238/1000 | Loss: 0.00001087
Iteration 239/1000 | Loss: 0.00001087
Iteration 240/1000 | Loss: 0.00001087
Iteration 241/1000 | Loss: 0.00001086
Iteration 242/1000 | Loss: 0.00001086
Iteration 243/1000 | Loss: 0.00001086
Iteration 244/1000 | Loss: 0.00001086
Iteration 245/1000 | Loss: 0.00001086
Iteration 246/1000 | Loss: 0.00001086
Iteration 247/1000 | Loss: 0.00001086
Iteration 248/1000 | Loss: 0.00001086
Iteration 249/1000 | Loss: 0.00001086
Iteration 250/1000 | Loss: 0.00001085
Iteration 251/1000 | Loss: 0.00001085
Iteration 252/1000 | Loss: 0.00001085
Iteration 253/1000 | Loss: 0.00001085
Iteration 254/1000 | Loss: 0.00001085
Iteration 255/1000 | Loss: 0.00001085
Iteration 256/1000 | Loss: 0.00001085
Iteration 257/1000 | Loss: 0.00001085
Iteration 258/1000 | Loss: 0.00001085
Iteration 259/1000 | Loss: 0.00001085
Iteration 260/1000 | Loss: 0.00001085
Iteration 261/1000 | Loss: 0.00001085
Iteration 262/1000 | Loss: 0.00001085
Iteration 263/1000 | Loss: 0.00001084
Iteration 264/1000 | Loss: 0.00001084
Iteration 265/1000 | Loss: 0.00001084
Iteration 266/1000 | Loss: 0.00001084
Iteration 267/1000 | Loss: 0.00001084
Iteration 268/1000 | Loss: 0.00001084
Iteration 269/1000 | Loss: 0.00001084
Iteration 270/1000 | Loss: 0.00001084
Iteration 271/1000 | Loss: 0.00001084
Iteration 272/1000 | Loss: 0.00001084
Iteration 273/1000 | Loss: 0.00001084
Iteration 274/1000 | Loss: 0.00001084
Iteration 275/1000 | Loss: 0.00001084
Iteration 276/1000 | Loss: 0.00001084
Iteration 277/1000 | Loss: 0.00001084
Iteration 278/1000 | Loss: 0.00001084
Iteration 279/1000 | Loss: 0.00001084
Iteration 280/1000 | Loss: 0.00001084
Iteration 281/1000 | Loss: 0.00001084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [1.083895313058747e-05, 1.083895313058747e-05, 1.083895313058747e-05, 1.083895313058747e-05, 1.083895313058747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.083895313058747e-05

Optimization complete. Final v2v error: 2.7781898975372314 mm

Highest mean error: 3.2388267517089844 mm for frame 32

Lowest mean error: 2.4842875003814697 mm for frame 246

Saving results

Total time: 120.57978940010071
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923472
Iteration 2/25 | Loss: 0.00238955
Iteration 3/25 | Loss: 0.00174558
Iteration 4/25 | Loss: 0.00157070
Iteration 5/25 | Loss: 0.00151215
Iteration 6/25 | Loss: 0.00151029
Iteration 7/25 | Loss: 0.00138042
Iteration 8/25 | Loss: 0.00135985
Iteration 9/25 | Loss: 0.00137822
Iteration 10/25 | Loss: 0.00131658
Iteration 11/25 | Loss: 0.00127020
Iteration 12/25 | Loss: 0.00125493
Iteration 13/25 | Loss: 0.00124003
Iteration 14/25 | Loss: 0.00122654
Iteration 15/25 | Loss: 0.00121695
Iteration 16/25 | Loss: 0.00121443
Iteration 17/25 | Loss: 0.00121196
Iteration 18/25 | Loss: 0.00120718
Iteration 19/25 | Loss: 0.00120394
Iteration 20/25 | Loss: 0.00120353
Iteration 21/25 | Loss: 0.00120333
Iteration 22/25 | Loss: 0.00120310
Iteration 23/25 | Loss: 0.00120753
Iteration 24/25 | Loss: 0.00120243
Iteration 25/25 | Loss: 0.00120051

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31544912
Iteration 2/25 | Loss: 0.00096509
Iteration 3/25 | Loss: 0.00096508
Iteration 4/25 | Loss: 0.00096508
Iteration 5/25 | Loss: 0.00096508
Iteration 6/25 | Loss: 0.00096508
Iteration 7/25 | Loss: 0.00096508
Iteration 8/25 | Loss: 0.00096508
Iteration 9/25 | Loss: 0.00096508
Iteration 10/25 | Loss: 0.00096508
Iteration 11/25 | Loss: 0.00096508
Iteration 12/25 | Loss: 0.00096508
Iteration 13/25 | Loss: 0.00096508
Iteration 14/25 | Loss: 0.00096508
Iteration 15/25 | Loss: 0.00096508
Iteration 16/25 | Loss: 0.00096508
Iteration 17/25 | Loss: 0.00096508
Iteration 18/25 | Loss: 0.00096508
Iteration 19/25 | Loss: 0.00096508
Iteration 20/25 | Loss: 0.00096508
Iteration 21/25 | Loss: 0.00096508
Iteration 22/25 | Loss: 0.00096508
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009650817955844104, 0.0009650817955844104, 0.0009650817955844104, 0.0009650817955844104, 0.0009650817955844104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009650817955844104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096508
Iteration 2/1000 | Loss: 0.00171151
Iteration 3/1000 | Loss: 0.00142470
Iteration 4/1000 | Loss: 0.00182291
Iteration 5/1000 | Loss: 0.00079741
Iteration 6/1000 | Loss: 0.00055961
Iteration 7/1000 | Loss: 0.00028062
Iteration 8/1000 | Loss: 0.00007400
Iteration 9/1000 | Loss: 0.00124595
Iteration 10/1000 | Loss: 0.00048761
Iteration 11/1000 | Loss: 0.00214055
Iteration 12/1000 | Loss: 0.00117198
Iteration 13/1000 | Loss: 0.00215420
Iteration 14/1000 | Loss: 0.00148294
Iteration 15/1000 | Loss: 0.00154044
Iteration 16/1000 | Loss: 0.00083902
Iteration 17/1000 | Loss: 0.00017740
Iteration 18/1000 | Loss: 0.00150326
Iteration 19/1000 | Loss: 0.00010209
Iteration 20/1000 | Loss: 0.00007404
Iteration 21/1000 | Loss: 0.00011864
Iteration 22/1000 | Loss: 0.00064317
Iteration 23/1000 | Loss: 0.00037402
Iteration 24/1000 | Loss: 0.00143118
Iteration 25/1000 | Loss: 0.00104650
Iteration 26/1000 | Loss: 0.00068651
Iteration 27/1000 | Loss: 0.00041200
Iteration 28/1000 | Loss: 0.00014663
Iteration 29/1000 | Loss: 0.00005799
Iteration 30/1000 | Loss: 0.00016406
Iteration 31/1000 | Loss: 0.00023967
Iteration 32/1000 | Loss: 0.00005525
Iteration 33/1000 | Loss: 0.00010551
Iteration 34/1000 | Loss: 0.00007782
Iteration 35/1000 | Loss: 0.00041154
Iteration 36/1000 | Loss: 0.00025262
Iteration 37/1000 | Loss: 0.00007043
Iteration 38/1000 | Loss: 0.00060068
Iteration 39/1000 | Loss: 0.00005402
Iteration 40/1000 | Loss: 0.00041914
Iteration 41/1000 | Loss: 0.00147018
Iteration 42/1000 | Loss: 0.00005468
Iteration 43/1000 | Loss: 0.00042612
Iteration 44/1000 | Loss: 0.00006741
Iteration 45/1000 | Loss: 0.00019731
Iteration 46/1000 | Loss: 0.00004099
Iteration 47/1000 | Loss: 0.00039317
Iteration 48/1000 | Loss: 0.00026081
Iteration 49/1000 | Loss: 0.00003817
Iteration 50/1000 | Loss: 0.00015274
Iteration 51/1000 | Loss: 0.00003636
Iteration 52/1000 | Loss: 0.00003531
Iteration 53/1000 | Loss: 0.00003437
Iteration 54/1000 | Loss: 0.00003383
Iteration 55/1000 | Loss: 0.00003353
Iteration 56/1000 | Loss: 0.00003323
Iteration 57/1000 | Loss: 0.00003304
Iteration 58/1000 | Loss: 0.00003301
Iteration 59/1000 | Loss: 0.00003296
Iteration 60/1000 | Loss: 0.00003295
Iteration 61/1000 | Loss: 0.00003293
Iteration 62/1000 | Loss: 0.00003293
Iteration 63/1000 | Loss: 0.00003293
Iteration 64/1000 | Loss: 0.00003293
Iteration 65/1000 | Loss: 0.00003292
Iteration 66/1000 | Loss: 0.00003291
Iteration 67/1000 | Loss: 0.00003291
Iteration 68/1000 | Loss: 0.00003291
Iteration 69/1000 | Loss: 0.00003290
Iteration 70/1000 | Loss: 0.00003290
Iteration 71/1000 | Loss: 0.00003289
Iteration 72/1000 | Loss: 0.00003289
Iteration 73/1000 | Loss: 0.00003289
Iteration 74/1000 | Loss: 0.00003288
Iteration 75/1000 | Loss: 0.00003288
Iteration 76/1000 | Loss: 0.00003287
Iteration 77/1000 | Loss: 0.00003287
Iteration 78/1000 | Loss: 0.00003286
Iteration 79/1000 | Loss: 0.00003286
Iteration 80/1000 | Loss: 0.00003286
Iteration 81/1000 | Loss: 0.00003286
Iteration 82/1000 | Loss: 0.00003286
Iteration 83/1000 | Loss: 0.00003286
Iteration 84/1000 | Loss: 0.00003285
Iteration 85/1000 | Loss: 0.00003285
Iteration 86/1000 | Loss: 0.00003285
Iteration 87/1000 | Loss: 0.00003285
Iteration 88/1000 | Loss: 0.00003284
Iteration 89/1000 | Loss: 0.00003284
Iteration 90/1000 | Loss: 0.00003284
Iteration 91/1000 | Loss: 0.00003284
Iteration 92/1000 | Loss: 0.00003283
Iteration 93/1000 | Loss: 0.00003283
Iteration 94/1000 | Loss: 0.00003283
Iteration 95/1000 | Loss: 0.00003282
Iteration 96/1000 | Loss: 0.00003282
Iteration 97/1000 | Loss: 0.00003282
Iteration 98/1000 | Loss: 0.00003282
Iteration 99/1000 | Loss: 0.00003282
Iteration 100/1000 | Loss: 0.00003282
Iteration 101/1000 | Loss: 0.00003282
Iteration 102/1000 | Loss: 0.00003282
Iteration 103/1000 | Loss: 0.00003282
Iteration 104/1000 | Loss: 0.00003281
Iteration 105/1000 | Loss: 0.00003281
Iteration 106/1000 | Loss: 0.00003281
Iteration 107/1000 | Loss: 0.00003281
Iteration 108/1000 | Loss: 0.00003281
Iteration 109/1000 | Loss: 0.00003281
Iteration 110/1000 | Loss: 0.00003281
Iteration 111/1000 | Loss: 0.00003281
Iteration 112/1000 | Loss: 0.00003280
Iteration 113/1000 | Loss: 0.00003280
Iteration 114/1000 | Loss: 0.00003280
Iteration 115/1000 | Loss: 0.00003280
Iteration 116/1000 | Loss: 0.00003280
Iteration 117/1000 | Loss: 0.00003280
Iteration 118/1000 | Loss: 0.00003280
Iteration 119/1000 | Loss: 0.00003280
Iteration 120/1000 | Loss: 0.00003280
Iteration 121/1000 | Loss: 0.00003280
Iteration 122/1000 | Loss: 0.00003280
Iteration 123/1000 | Loss: 0.00003280
Iteration 124/1000 | Loss: 0.00003280
Iteration 125/1000 | Loss: 0.00003280
Iteration 126/1000 | Loss: 0.00003279
Iteration 127/1000 | Loss: 0.00003279
Iteration 128/1000 | Loss: 0.00003279
Iteration 129/1000 | Loss: 0.00003279
Iteration 130/1000 | Loss: 0.00003279
Iteration 131/1000 | Loss: 0.00003279
Iteration 132/1000 | Loss: 0.00003279
Iteration 133/1000 | Loss: 0.00003279
Iteration 134/1000 | Loss: 0.00003279
Iteration 135/1000 | Loss: 0.00003278
Iteration 136/1000 | Loss: 0.00003278
Iteration 137/1000 | Loss: 0.00003278
Iteration 138/1000 | Loss: 0.00003278
Iteration 139/1000 | Loss: 0.00003278
Iteration 140/1000 | Loss: 0.00003277
Iteration 141/1000 | Loss: 0.00003277
Iteration 142/1000 | Loss: 0.00003277
Iteration 143/1000 | Loss: 0.00003277
Iteration 144/1000 | Loss: 0.00003277
Iteration 145/1000 | Loss: 0.00003277
Iteration 146/1000 | Loss: 0.00003277
Iteration 147/1000 | Loss: 0.00003276
Iteration 148/1000 | Loss: 0.00003276
Iteration 149/1000 | Loss: 0.00003276
Iteration 150/1000 | Loss: 0.00003275
Iteration 151/1000 | Loss: 0.00003275
Iteration 152/1000 | Loss: 0.00003275
Iteration 153/1000 | Loss: 0.00003275
Iteration 154/1000 | Loss: 0.00003275
Iteration 155/1000 | Loss: 0.00003275
Iteration 156/1000 | Loss: 0.00003275
Iteration 157/1000 | Loss: 0.00003275
Iteration 158/1000 | Loss: 0.00003275
Iteration 159/1000 | Loss: 0.00003275
Iteration 160/1000 | Loss: 0.00003275
Iteration 161/1000 | Loss: 0.00003275
Iteration 162/1000 | Loss: 0.00003275
Iteration 163/1000 | Loss: 0.00003274
Iteration 164/1000 | Loss: 0.00003274
Iteration 165/1000 | Loss: 0.00003274
Iteration 166/1000 | Loss: 0.00003274
Iteration 167/1000 | Loss: 0.00003274
Iteration 168/1000 | Loss: 0.00003274
Iteration 169/1000 | Loss: 0.00003274
Iteration 170/1000 | Loss: 0.00003274
Iteration 171/1000 | Loss: 0.00003274
Iteration 172/1000 | Loss: 0.00003274
Iteration 173/1000 | Loss: 0.00003274
Iteration 174/1000 | Loss: 0.00003274
Iteration 175/1000 | Loss: 0.00003273
Iteration 176/1000 | Loss: 0.00003273
Iteration 177/1000 | Loss: 0.00003273
Iteration 178/1000 | Loss: 0.00003273
Iteration 179/1000 | Loss: 0.00003273
Iteration 180/1000 | Loss: 0.00003273
Iteration 181/1000 | Loss: 0.00003273
Iteration 182/1000 | Loss: 0.00003273
Iteration 183/1000 | Loss: 0.00003273
Iteration 184/1000 | Loss: 0.00003273
Iteration 185/1000 | Loss: 0.00003273
Iteration 186/1000 | Loss: 0.00003273
Iteration 187/1000 | Loss: 0.00003273
Iteration 188/1000 | Loss: 0.00003273
Iteration 189/1000 | Loss: 0.00003273
Iteration 190/1000 | Loss: 0.00003273
Iteration 191/1000 | Loss: 0.00003273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [3.273396214353852e-05, 3.273396214353852e-05, 3.273396214353852e-05, 3.273396214353852e-05, 3.273396214353852e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.273396214353852e-05

Optimization complete. Final v2v error: 3.737126588821411 mm

Highest mean error: 11.009265899658203 mm for frame 118

Lowest mean error: 2.8288588523864746 mm for frame 23

Saving results

Total time: 131.29412841796875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834935
Iteration 2/25 | Loss: 0.00131279
Iteration 3/25 | Loss: 0.00108435
Iteration 4/25 | Loss: 0.00106300
Iteration 5/25 | Loss: 0.00105766
Iteration 6/25 | Loss: 0.00105693
Iteration 7/25 | Loss: 0.00105693
Iteration 8/25 | Loss: 0.00105693
Iteration 9/25 | Loss: 0.00105693
Iteration 10/25 | Loss: 0.00105693
Iteration 11/25 | Loss: 0.00105693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010569324949756265, 0.0010569324949756265, 0.0010569324949756265, 0.0010569324949756265, 0.0010569324949756265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010569324949756265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82736111
Iteration 2/25 | Loss: 0.00078400
Iteration 3/25 | Loss: 0.00078400
Iteration 4/25 | Loss: 0.00078400
Iteration 5/25 | Loss: 0.00078400
Iteration 6/25 | Loss: 0.00078400
Iteration 7/25 | Loss: 0.00078399
Iteration 8/25 | Loss: 0.00078399
Iteration 9/25 | Loss: 0.00078399
Iteration 10/25 | Loss: 0.00078399
Iteration 11/25 | Loss: 0.00078399
Iteration 12/25 | Loss: 0.00078399
Iteration 13/25 | Loss: 0.00078399
Iteration 14/25 | Loss: 0.00078399
Iteration 15/25 | Loss: 0.00078399
Iteration 16/25 | Loss: 0.00078399
Iteration 17/25 | Loss: 0.00078399
Iteration 18/25 | Loss: 0.00078399
Iteration 19/25 | Loss: 0.00078399
Iteration 20/25 | Loss: 0.00078399
Iteration 21/25 | Loss: 0.00078399
Iteration 22/25 | Loss: 0.00078399
Iteration 23/25 | Loss: 0.00078399
Iteration 24/25 | Loss: 0.00078399
Iteration 25/25 | Loss: 0.00078399

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078399
Iteration 2/1000 | Loss: 0.00002269
Iteration 3/1000 | Loss: 0.00001551
Iteration 4/1000 | Loss: 0.00001330
Iteration 5/1000 | Loss: 0.00001242
Iteration 6/1000 | Loss: 0.00001190
Iteration 7/1000 | Loss: 0.00001149
Iteration 8/1000 | Loss: 0.00001148
Iteration 9/1000 | Loss: 0.00001121
Iteration 10/1000 | Loss: 0.00001093
Iteration 11/1000 | Loss: 0.00001084
Iteration 12/1000 | Loss: 0.00001072
Iteration 13/1000 | Loss: 0.00001068
Iteration 14/1000 | Loss: 0.00001066
Iteration 15/1000 | Loss: 0.00001062
Iteration 16/1000 | Loss: 0.00001061
Iteration 17/1000 | Loss: 0.00001055
Iteration 18/1000 | Loss: 0.00001048
Iteration 19/1000 | Loss: 0.00001045
Iteration 20/1000 | Loss: 0.00001045
Iteration 21/1000 | Loss: 0.00001045
Iteration 22/1000 | Loss: 0.00001044
Iteration 23/1000 | Loss: 0.00001043
Iteration 24/1000 | Loss: 0.00001041
Iteration 25/1000 | Loss: 0.00001040
Iteration 26/1000 | Loss: 0.00001039
Iteration 27/1000 | Loss: 0.00001038
Iteration 28/1000 | Loss: 0.00001037
Iteration 29/1000 | Loss: 0.00001037
Iteration 30/1000 | Loss: 0.00001037
Iteration 31/1000 | Loss: 0.00001037
Iteration 32/1000 | Loss: 0.00001035
Iteration 33/1000 | Loss: 0.00001035
Iteration 34/1000 | Loss: 0.00001033
Iteration 35/1000 | Loss: 0.00001033
Iteration 36/1000 | Loss: 0.00001033
Iteration 37/1000 | Loss: 0.00001033
Iteration 38/1000 | Loss: 0.00001033
Iteration 39/1000 | Loss: 0.00001033
Iteration 40/1000 | Loss: 0.00001033
Iteration 41/1000 | Loss: 0.00001032
Iteration 42/1000 | Loss: 0.00001032
Iteration 43/1000 | Loss: 0.00001032
Iteration 44/1000 | Loss: 0.00001031
Iteration 45/1000 | Loss: 0.00001028
Iteration 46/1000 | Loss: 0.00001027
Iteration 47/1000 | Loss: 0.00001027
Iteration 48/1000 | Loss: 0.00001027
Iteration 49/1000 | Loss: 0.00001026
Iteration 50/1000 | Loss: 0.00001026
Iteration 51/1000 | Loss: 0.00001024
Iteration 52/1000 | Loss: 0.00001023
Iteration 53/1000 | Loss: 0.00001023
Iteration 54/1000 | Loss: 0.00001023
Iteration 55/1000 | Loss: 0.00001022
Iteration 56/1000 | Loss: 0.00001022
Iteration 57/1000 | Loss: 0.00001022
Iteration 58/1000 | Loss: 0.00001022
Iteration 59/1000 | Loss: 0.00001022
Iteration 60/1000 | Loss: 0.00001021
Iteration 61/1000 | Loss: 0.00001019
Iteration 62/1000 | Loss: 0.00001018
Iteration 63/1000 | Loss: 0.00001018
Iteration 64/1000 | Loss: 0.00001018
Iteration 65/1000 | Loss: 0.00001017
Iteration 66/1000 | Loss: 0.00001017
Iteration 67/1000 | Loss: 0.00001017
Iteration 68/1000 | Loss: 0.00001017
Iteration 69/1000 | Loss: 0.00001017
Iteration 70/1000 | Loss: 0.00001017
Iteration 71/1000 | Loss: 0.00001017
Iteration 72/1000 | Loss: 0.00001016
Iteration 73/1000 | Loss: 0.00001016
Iteration 74/1000 | Loss: 0.00001016
Iteration 75/1000 | Loss: 0.00001015
Iteration 76/1000 | Loss: 0.00001015
Iteration 77/1000 | Loss: 0.00001015
Iteration 78/1000 | Loss: 0.00001015
Iteration 79/1000 | Loss: 0.00001015
Iteration 80/1000 | Loss: 0.00001015
Iteration 81/1000 | Loss: 0.00001014
Iteration 82/1000 | Loss: 0.00001014
Iteration 83/1000 | Loss: 0.00001014
Iteration 84/1000 | Loss: 0.00001014
Iteration 85/1000 | Loss: 0.00001014
Iteration 86/1000 | Loss: 0.00001014
Iteration 87/1000 | Loss: 0.00001014
Iteration 88/1000 | Loss: 0.00001014
Iteration 89/1000 | Loss: 0.00001014
Iteration 90/1000 | Loss: 0.00001013
Iteration 91/1000 | Loss: 0.00001013
Iteration 92/1000 | Loss: 0.00001013
Iteration 93/1000 | Loss: 0.00001013
Iteration 94/1000 | Loss: 0.00001013
Iteration 95/1000 | Loss: 0.00001012
Iteration 96/1000 | Loss: 0.00001012
Iteration 97/1000 | Loss: 0.00001012
Iteration 98/1000 | Loss: 0.00001012
Iteration 99/1000 | Loss: 0.00001011
Iteration 100/1000 | Loss: 0.00001011
Iteration 101/1000 | Loss: 0.00001011
Iteration 102/1000 | Loss: 0.00001010
Iteration 103/1000 | Loss: 0.00001010
Iteration 104/1000 | Loss: 0.00001010
Iteration 105/1000 | Loss: 0.00001010
Iteration 106/1000 | Loss: 0.00001010
Iteration 107/1000 | Loss: 0.00001010
Iteration 108/1000 | Loss: 0.00001010
Iteration 109/1000 | Loss: 0.00001010
Iteration 110/1000 | Loss: 0.00001009
Iteration 111/1000 | Loss: 0.00001008
Iteration 112/1000 | Loss: 0.00001007
Iteration 113/1000 | Loss: 0.00001007
Iteration 114/1000 | Loss: 0.00001007
Iteration 115/1000 | Loss: 0.00001007
Iteration 116/1000 | Loss: 0.00001006
Iteration 117/1000 | Loss: 0.00001006
Iteration 118/1000 | Loss: 0.00001006
Iteration 119/1000 | Loss: 0.00001005
Iteration 120/1000 | Loss: 0.00001004
Iteration 121/1000 | Loss: 0.00001004
Iteration 122/1000 | Loss: 0.00001004
Iteration 123/1000 | Loss: 0.00001004
Iteration 124/1000 | Loss: 0.00001003
Iteration 125/1000 | Loss: 0.00001003
Iteration 126/1000 | Loss: 0.00001003
Iteration 127/1000 | Loss: 0.00001003
Iteration 128/1000 | Loss: 0.00001003
Iteration 129/1000 | Loss: 0.00001003
Iteration 130/1000 | Loss: 0.00001003
Iteration 131/1000 | Loss: 0.00001002
Iteration 132/1000 | Loss: 0.00001002
Iteration 133/1000 | Loss: 0.00001002
Iteration 134/1000 | Loss: 0.00001001
Iteration 135/1000 | Loss: 0.00001001
Iteration 136/1000 | Loss: 0.00001001
Iteration 137/1000 | Loss: 0.00001001
Iteration 138/1000 | Loss: 0.00001001
Iteration 139/1000 | Loss: 0.00001001
Iteration 140/1000 | Loss: 0.00001001
Iteration 141/1000 | Loss: 0.00001000
Iteration 142/1000 | Loss: 0.00001000
Iteration 143/1000 | Loss: 0.00001000
Iteration 144/1000 | Loss: 0.00001000
Iteration 145/1000 | Loss: 0.00001000
Iteration 146/1000 | Loss: 0.00001000
Iteration 147/1000 | Loss: 0.00001000
Iteration 148/1000 | Loss: 0.00001000
Iteration 149/1000 | Loss: 0.00001000
Iteration 150/1000 | Loss: 0.00001000
Iteration 151/1000 | Loss: 0.00001000
Iteration 152/1000 | Loss: 0.00001000
Iteration 153/1000 | Loss: 0.00001000
Iteration 154/1000 | Loss: 0.00001000
Iteration 155/1000 | Loss: 0.00001000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.0000403563026339e-05, 1.0000403563026339e-05, 1.0000403563026339e-05, 1.0000403563026339e-05, 1.0000403563026339e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0000403563026339e-05

Optimization complete. Final v2v error: 2.7340621948242188 mm

Highest mean error: 3.0698561668395996 mm for frame 172

Lowest mean error: 2.429257392883301 mm for frame 2

Saving results

Total time: 41.230743408203125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795043
Iteration 2/25 | Loss: 0.00141489
Iteration 3/25 | Loss: 0.00125572
Iteration 4/25 | Loss: 0.00116287
Iteration 5/25 | Loss: 0.00112378
Iteration 6/25 | Loss: 0.00110698
Iteration 7/25 | Loss: 0.00109825
Iteration 8/25 | Loss: 0.00109203
Iteration 9/25 | Loss: 0.00109129
Iteration 10/25 | Loss: 0.00109117
Iteration 11/25 | Loss: 0.00109117
Iteration 12/25 | Loss: 0.00109117
Iteration 13/25 | Loss: 0.00109117
Iteration 14/25 | Loss: 0.00109117
Iteration 15/25 | Loss: 0.00109116
Iteration 16/25 | Loss: 0.00109116
Iteration 17/25 | Loss: 0.00109116
Iteration 18/25 | Loss: 0.00109116
Iteration 19/25 | Loss: 0.00109116
Iteration 20/25 | Loss: 0.00109116
Iteration 21/25 | Loss: 0.00109116
Iteration 22/25 | Loss: 0.00109116
Iteration 23/25 | Loss: 0.00109116
Iteration 24/25 | Loss: 0.00109115
Iteration 25/25 | Loss: 0.00109115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94999385
Iteration 2/25 | Loss: 0.00083539
Iteration 3/25 | Loss: 0.00083539
Iteration 4/25 | Loss: 0.00083539
Iteration 5/25 | Loss: 0.00083539
Iteration 6/25 | Loss: 0.00083539
Iteration 7/25 | Loss: 0.00083539
Iteration 8/25 | Loss: 0.00083539
Iteration 9/25 | Loss: 0.00083539
Iteration 10/25 | Loss: 0.00083538
Iteration 11/25 | Loss: 0.00083538
Iteration 12/25 | Loss: 0.00083538
Iteration 13/25 | Loss: 0.00083538
Iteration 14/25 | Loss: 0.00083538
Iteration 15/25 | Loss: 0.00083538
Iteration 16/25 | Loss: 0.00083538
Iteration 17/25 | Loss: 0.00083538
Iteration 18/25 | Loss: 0.00083538
Iteration 19/25 | Loss: 0.00083538
Iteration 20/25 | Loss: 0.00083538
Iteration 21/25 | Loss: 0.00083538
Iteration 22/25 | Loss: 0.00083538
Iteration 23/25 | Loss: 0.00083538
Iteration 24/25 | Loss: 0.00083538
Iteration 25/25 | Loss: 0.00083538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083538
Iteration 2/1000 | Loss: 0.00002762
Iteration 3/1000 | Loss: 0.00002012
Iteration 4/1000 | Loss: 0.00001702
Iteration 5/1000 | Loss: 0.00001608
Iteration 6/1000 | Loss: 0.00001551
Iteration 7/1000 | Loss: 0.00001510
Iteration 8/1000 | Loss: 0.00012479
Iteration 9/1000 | Loss: 0.00002641
Iteration 10/1000 | Loss: 0.00001497
Iteration 11/1000 | Loss: 0.00002138
Iteration 12/1000 | Loss: 0.00001467
Iteration 13/1000 | Loss: 0.00001657
Iteration 14/1000 | Loss: 0.00001453
Iteration 15/1000 | Loss: 0.00001451
Iteration 16/1000 | Loss: 0.00003139
Iteration 17/1000 | Loss: 0.00001670
Iteration 18/1000 | Loss: 0.00001427
Iteration 19/1000 | Loss: 0.00001417
Iteration 20/1000 | Loss: 0.00001416
Iteration 21/1000 | Loss: 0.00001414
Iteration 22/1000 | Loss: 0.00001414
Iteration 23/1000 | Loss: 0.00001413
Iteration 24/1000 | Loss: 0.00001411
Iteration 25/1000 | Loss: 0.00001410
Iteration 26/1000 | Loss: 0.00001410
Iteration 27/1000 | Loss: 0.00001409
Iteration 28/1000 | Loss: 0.00001409
Iteration 29/1000 | Loss: 0.00001409
Iteration 30/1000 | Loss: 0.00001408
Iteration 31/1000 | Loss: 0.00001408
Iteration 32/1000 | Loss: 0.00001408
Iteration 33/1000 | Loss: 0.00001407
Iteration 34/1000 | Loss: 0.00001405
Iteration 35/1000 | Loss: 0.00001404
Iteration 36/1000 | Loss: 0.00001404
Iteration 37/1000 | Loss: 0.00001403
Iteration 38/1000 | Loss: 0.00001403
Iteration 39/1000 | Loss: 0.00001403
Iteration 40/1000 | Loss: 0.00001403
Iteration 41/1000 | Loss: 0.00001402
Iteration 42/1000 | Loss: 0.00001399
Iteration 43/1000 | Loss: 0.00001399
Iteration 44/1000 | Loss: 0.00001399
Iteration 45/1000 | Loss: 0.00001398
Iteration 46/1000 | Loss: 0.00001398
Iteration 47/1000 | Loss: 0.00001397
Iteration 48/1000 | Loss: 0.00001396
Iteration 49/1000 | Loss: 0.00001394
Iteration 50/1000 | Loss: 0.00001393
Iteration 51/1000 | Loss: 0.00001393
Iteration 52/1000 | Loss: 0.00001393
Iteration 53/1000 | Loss: 0.00001393
Iteration 54/1000 | Loss: 0.00001392
Iteration 55/1000 | Loss: 0.00001392
Iteration 56/1000 | Loss: 0.00001391
Iteration 57/1000 | Loss: 0.00001391
Iteration 58/1000 | Loss: 0.00001391
Iteration 59/1000 | Loss: 0.00001391
Iteration 60/1000 | Loss: 0.00001391
Iteration 61/1000 | Loss: 0.00001391
Iteration 62/1000 | Loss: 0.00001390
Iteration 63/1000 | Loss: 0.00001390
Iteration 64/1000 | Loss: 0.00001390
Iteration 65/1000 | Loss: 0.00001389
Iteration 66/1000 | Loss: 0.00001389
Iteration 67/1000 | Loss: 0.00001389
Iteration 68/1000 | Loss: 0.00001388
Iteration 69/1000 | Loss: 0.00001387
Iteration 70/1000 | Loss: 0.00001387
Iteration 71/1000 | Loss: 0.00001386
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001385
Iteration 74/1000 | Loss: 0.00001385
Iteration 75/1000 | Loss: 0.00001384
Iteration 76/1000 | Loss: 0.00001384
Iteration 77/1000 | Loss: 0.00001384
Iteration 78/1000 | Loss: 0.00001384
Iteration 79/1000 | Loss: 0.00001383
Iteration 80/1000 | Loss: 0.00001383
Iteration 81/1000 | Loss: 0.00001383
Iteration 82/1000 | Loss: 0.00001383
Iteration 83/1000 | Loss: 0.00001382
Iteration 84/1000 | Loss: 0.00001382
Iteration 85/1000 | Loss: 0.00001382
Iteration 86/1000 | Loss: 0.00001382
Iteration 87/1000 | Loss: 0.00001382
Iteration 88/1000 | Loss: 0.00001381
Iteration 89/1000 | Loss: 0.00001381
Iteration 90/1000 | Loss: 0.00001381
Iteration 91/1000 | Loss: 0.00001381
Iteration 92/1000 | Loss: 0.00001381
Iteration 93/1000 | Loss: 0.00001381
Iteration 94/1000 | Loss: 0.00001381
Iteration 95/1000 | Loss: 0.00001381
Iteration 96/1000 | Loss: 0.00001381
Iteration 97/1000 | Loss: 0.00001381
Iteration 98/1000 | Loss: 0.00001381
Iteration 99/1000 | Loss: 0.00001381
Iteration 100/1000 | Loss: 0.00001380
Iteration 101/1000 | Loss: 0.00001380
Iteration 102/1000 | Loss: 0.00001380
Iteration 103/1000 | Loss: 0.00001380
Iteration 104/1000 | Loss: 0.00001380
Iteration 105/1000 | Loss: 0.00001380
Iteration 106/1000 | Loss: 0.00001379
Iteration 107/1000 | Loss: 0.00001379
Iteration 108/1000 | Loss: 0.00001379
Iteration 109/1000 | Loss: 0.00001379
Iteration 110/1000 | Loss: 0.00001379
Iteration 111/1000 | Loss: 0.00001379
Iteration 112/1000 | Loss: 0.00001378
Iteration 113/1000 | Loss: 0.00001378
Iteration 114/1000 | Loss: 0.00001378
Iteration 115/1000 | Loss: 0.00001378
Iteration 116/1000 | Loss: 0.00001378
Iteration 117/1000 | Loss: 0.00001378
Iteration 118/1000 | Loss: 0.00001377
Iteration 119/1000 | Loss: 0.00001377
Iteration 120/1000 | Loss: 0.00001377
Iteration 121/1000 | Loss: 0.00001377
Iteration 122/1000 | Loss: 0.00001376
Iteration 123/1000 | Loss: 0.00001376
Iteration 124/1000 | Loss: 0.00001375
Iteration 125/1000 | Loss: 0.00001375
Iteration 126/1000 | Loss: 0.00001375
Iteration 127/1000 | Loss: 0.00001375
Iteration 128/1000 | Loss: 0.00001375
Iteration 129/1000 | Loss: 0.00001374
Iteration 130/1000 | Loss: 0.00001374
Iteration 131/1000 | Loss: 0.00001374
Iteration 132/1000 | Loss: 0.00001374
Iteration 133/1000 | Loss: 0.00001374
Iteration 134/1000 | Loss: 0.00001374
Iteration 135/1000 | Loss: 0.00001373
Iteration 136/1000 | Loss: 0.00001373
Iteration 137/1000 | Loss: 0.00001373
Iteration 138/1000 | Loss: 0.00001373
Iteration 139/1000 | Loss: 0.00001372
Iteration 140/1000 | Loss: 0.00001372
Iteration 141/1000 | Loss: 0.00001372
Iteration 142/1000 | Loss: 0.00001372
Iteration 143/1000 | Loss: 0.00015324
Iteration 144/1000 | Loss: 0.00001392
Iteration 145/1000 | Loss: 0.00001383
Iteration 146/1000 | Loss: 0.00001373
Iteration 147/1000 | Loss: 0.00001369
Iteration 148/1000 | Loss: 0.00001368
Iteration 149/1000 | Loss: 0.00001368
Iteration 150/1000 | Loss: 0.00001367
Iteration 151/1000 | Loss: 0.00001367
Iteration 152/1000 | Loss: 0.00001367
Iteration 153/1000 | Loss: 0.00001366
Iteration 154/1000 | Loss: 0.00001366
Iteration 155/1000 | Loss: 0.00001366
Iteration 156/1000 | Loss: 0.00001366
Iteration 157/1000 | Loss: 0.00001366
Iteration 158/1000 | Loss: 0.00001366
Iteration 159/1000 | Loss: 0.00001366
Iteration 160/1000 | Loss: 0.00001366
Iteration 161/1000 | Loss: 0.00001366
Iteration 162/1000 | Loss: 0.00001366
Iteration 163/1000 | Loss: 0.00001366
Iteration 164/1000 | Loss: 0.00001366
Iteration 165/1000 | Loss: 0.00001366
Iteration 166/1000 | Loss: 0.00001366
Iteration 167/1000 | Loss: 0.00001366
Iteration 168/1000 | Loss: 0.00001366
Iteration 169/1000 | Loss: 0.00001366
Iteration 170/1000 | Loss: 0.00001366
Iteration 171/1000 | Loss: 0.00001366
Iteration 172/1000 | Loss: 0.00001366
Iteration 173/1000 | Loss: 0.00001366
Iteration 174/1000 | Loss: 0.00001366
Iteration 175/1000 | Loss: 0.00001366
Iteration 176/1000 | Loss: 0.00001366
Iteration 177/1000 | Loss: 0.00001366
Iteration 178/1000 | Loss: 0.00001366
Iteration 179/1000 | Loss: 0.00001366
Iteration 180/1000 | Loss: 0.00001366
Iteration 181/1000 | Loss: 0.00001366
Iteration 182/1000 | Loss: 0.00001366
Iteration 183/1000 | Loss: 0.00001366
Iteration 184/1000 | Loss: 0.00001366
Iteration 185/1000 | Loss: 0.00001366
Iteration 186/1000 | Loss: 0.00001366
Iteration 187/1000 | Loss: 0.00001366
Iteration 188/1000 | Loss: 0.00001366
Iteration 189/1000 | Loss: 0.00001366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.3656283954333048e-05, 1.3656283954333048e-05, 1.3656283954333048e-05, 1.3656283954333048e-05, 1.3656283954333048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3656283954333048e-05

Optimization complete. Final v2v error: 3.092012643814087 mm

Highest mean error: 3.873903274536133 mm for frame 107

Lowest mean error: 2.6558704376220703 mm for frame 1

Saving results

Total time: 58.33834099769592
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022347
Iteration 2/25 | Loss: 0.00166336
Iteration 3/25 | Loss: 0.00132697
Iteration 4/25 | Loss: 0.00133212
Iteration 5/25 | Loss: 0.00124553
Iteration 6/25 | Loss: 0.00115950
Iteration 7/25 | Loss: 0.00114524
Iteration 8/25 | Loss: 0.00115337
Iteration 9/25 | Loss: 0.00114985
Iteration 10/25 | Loss: 0.00113955
Iteration 11/25 | Loss: 0.00111535
Iteration 12/25 | Loss: 0.00110695
Iteration 13/25 | Loss: 0.00110363
Iteration 14/25 | Loss: 0.00109522
Iteration 15/25 | Loss: 0.00108949
Iteration 16/25 | Loss: 0.00109271
Iteration 17/25 | Loss: 0.00109513
Iteration 18/25 | Loss: 0.00109088
Iteration 19/25 | Loss: 0.00109031
Iteration 20/25 | Loss: 0.00108864
Iteration 21/25 | Loss: 0.00108099
Iteration 22/25 | Loss: 0.00107828
Iteration 23/25 | Loss: 0.00107619
Iteration 24/25 | Loss: 0.00107500
Iteration 25/25 | Loss: 0.00107434

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44192779
Iteration 2/25 | Loss: 0.00081176
Iteration 3/25 | Loss: 0.00081176
Iteration 4/25 | Loss: 0.00081176
Iteration 5/25 | Loss: 0.00081176
Iteration 6/25 | Loss: 0.00081176
Iteration 7/25 | Loss: 0.00081176
Iteration 8/25 | Loss: 0.00081176
Iteration 9/25 | Loss: 0.00081176
Iteration 10/25 | Loss: 0.00081176
Iteration 11/25 | Loss: 0.00081176
Iteration 12/25 | Loss: 0.00081176
Iteration 13/25 | Loss: 0.00081176
Iteration 14/25 | Loss: 0.00081176
Iteration 15/25 | Loss: 0.00081176
Iteration 16/25 | Loss: 0.00081176
Iteration 17/25 | Loss: 0.00081176
Iteration 18/25 | Loss: 0.00081176
Iteration 19/25 | Loss: 0.00081176
Iteration 20/25 | Loss: 0.00081176
Iteration 21/25 | Loss: 0.00081176
Iteration 22/25 | Loss: 0.00081176
Iteration 23/25 | Loss: 0.00081176
Iteration 24/25 | Loss: 0.00081176
Iteration 25/25 | Loss: 0.00081176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081176
Iteration 2/1000 | Loss: 0.00002485
Iteration 3/1000 | Loss: 0.00001789
Iteration 4/1000 | Loss: 0.00001514
Iteration 5/1000 | Loss: 0.00001413
Iteration 6/1000 | Loss: 0.00001362
Iteration 7/1000 | Loss: 0.00001687
Iteration 8/1000 | Loss: 0.00001299
Iteration 9/1000 | Loss: 0.00012892
Iteration 10/1000 | Loss: 0.00013843
Iteration 11/1000 | Loss: 0.00014454
Iteration 12/1000 | Loss: 0.00005332
Iteration 13/1000 | Loss: 0.00009207
Iteration 14/1000 | Loss: 0.00007628
Iteration 15/1000 | Loss: 0.00001925
Iteration 16/1000 | Loss: 0.00001756
Iteration 17/1000 | Loss: 0.00003220
Iteration 18/1000 | Loss: 0.00001509
Iteration 19/1000 | Loss: 0.00002183
Iteration 20/1000 | Loss: 0.00001462
Iteration 21/1000 | Loss: 0.00001413
Iteration 22/1000 | Loss: 0.00024794
Iteration 23/1000 | Loss: 0.00020916
Iteration 24/1000 | Loss: 0.00011788
Iteration 25/1000 | Loss: 0.00010936
Iteration 26/1000 | Loss: 0.00016330
Iteration 27/1000 | Loss: 0.00026048
Iteration 28/1000 | Loss: 0.00001760
Iteration 29/1000 | Loss: 0.00001669
Iteration 30/1000 | Loss: 0.00001659
Iteration 31/1000 | Loss: 0.00029862
Iteration 32/1000 | Loss: 0.00016901
Iteration 33/1000 | Loss: 0.00011123
Iteration 34/1000 | Loss: 0.00001798
Iteration 35/1000 | Loss: 0.00004746
Iteration 36/1000 | Loss: 0.00001487
Iteration 37/1000 | Loss: 0.00002978
Iteration 38/1000 | Loss: 0.00032368
Iteration 39/1000 | Loss: 0.00016046
Iteration 40/1000 | Loss: 0.00021803
Iteration 41/1000 | Loss: 0.00009051
Iteration 42/1000 | Loss: 0.00002664
Iteration 43/1000 | Loss: 0.00001972
Iteration 44/1000 | Loss: 0.00005009
Iteration 45/1000 | Loss: 0.00002022
Iteration 46/1000 | Loss: 0.00001496
Iteration 47/1000 | Loss: 0.00001343
Iteration 48/1000 | Loss: 0.00001286
Iteration 49/1000 | Loss: 0.00001241
Iteration 50/1000 | Loss: 0.00001212
Iteration 51/1000 | Loss: 0.00001187
Iteration 52/1000 | Loss: 0.00002133
Iteration 53/1000 | Loss: 0.00001247
Iteration 54/1000 | Loss: 0.00001141
Iteration 55/1000 | Loss: 0.00001213
Iteration 56/1000 | Loss: 0.00001164
Iteration 57/1000 | Loss: 0.00001271
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001127
Iteration 60/1000 | Loss: 0.00001115
Iteration 61/1000 | Loss: 0.00001114
Iteration 62/1000 | Loss: 0.00001114
Iteration 63/1000 | Loss: 0.00001114
Iteration 64/1000 | Loss: 0.00001114
Iteration 65/1000 | Loss: 0.00001114
Iteration 66/1000 | Loss: 0.00001114
Iteration 67/1000 | Loss: 0.00001114
Iteration 68/1000 | Loss: 0.00001114
Iteration 69/1000 | Loss: 0.00001114
Iteration 70/1000 | Loss: 0.00001114
Iteration 71/1000 | Loss: 0.00001113
Iteration 72/1000 | Loss: 0.00001113
Iteration 73/1000 | Loss: 0.00001113
Iteration 74/1000 | Loss: 0.00001113
Iteration 75/1000 | Loss: 0.00001113
Iteration 76/1000 | Loss: 0.00001113
Iteration 77/1000 | Loss: 0.00001113
Iteration 78/1000 | Loss: 0.00001113
Iteration 79/1000 | Loss: 0.00001112
Iteration 80/1000 | Loss: 0.00001112
Iteration 81/1000 | Loss: 0.00001111
Iteration 82/1000 | Loss: 0.00001110
Iteration 83/1000 | Loss: 0.00001110
Iteration 84/1000 | Loss: 0.00001110
Iteration 85/1000 | Loss: 0.00001110
Iteration 86/1000 | Loss: 0.00001109
Iteration 87/1000 | Loss: 0.00001109
Iteration 88/1000 | Loss: 0.00001109
Iteration 89/1000 | Loss: 0.00001108
Iteration 90/1000 | Loss: 0.00001108
Iteration 91/1000 | Loss: 0.00001108
Iteration 92/1000 | Loss: 0.00001108
Iteration 93/1000 | Loss: 0.00001107
Iteration 94/1000 | Loss: 0.00001107
Iteration 95/1000 | Loss: 0.00001106
Iteration 96/1000 | Loss: 0.00001106
Iteration 97/1000 | Loss: 0.00001105
Iteration 98/1000 | Loss: 0.00001105
Iteration 99/1000 | Loss: 0.00001105
Iteration 100/1000 | Loss: 0.00001104
Iteration 101/1000 | Loss: 0.00001104
Iteration 102/1000 | Loss: 0.00001103
Iteration 103/1000 | Loss: 0.00001102
Iteration 104/1000 | Loss: 0.00001101
Iteration 105/1000 | Loss: 0.00001101
Iteration 106/1000 | Loss: 0.00001100
Iteration 107/1000 | Loss: 0.00001100
Iteration 108/1000 | Loss: 0.00001099
Iteration 109/1000 | Loss: 0.00001099
Iteration 110/1000 | Loss: 0.00001276
Iteration 111/1000 | Loss: 0.00001101
Iteration 112/1000 | Loss: 0.00001096
Iteration 113/1000 | Loss: 0.00001096
Iteration 114/1000 | Loss: 0.00001096
Iteration 115/1000 | Loss: 0.00001096
Iteration 116/1000 | Loss: 0.00001096
Iteration 117/1000 | Loss: 0.00001096
Iteration 118/1000 | Loss: 0.00001096
Iteration 119/1000 | Loss: 0.00001096
Iteration 120/1000 | Loss: 0.00001096
Iteration 121/1000 | Loss: 0.00001096
Iteration 122/1000 | Loss: 0.00001096
Iteration 123/1000 | Loss: 0.00001096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.0960159670503344e-05, 1.0960159670503344e-05, 1.0960159670503344e-05, 1.0960159670503344e-05, 1.0960159670503344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0960159670503344e-05

Optimization complete. Final v2v error: 2.7887697219848633 mm

Highest mean error: 3.477764129638672 mm for frame 65

Lowest mean error: 2.483717918395996 mm for frame 17

Saving results

Total time: 130.84024357795715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824909
Iteration 2/25 | Loss: 0.00113753
Iteration 3/25 | Loss: 0.00108246
Iteration 4/25 | Loss: 0.00107230
Iteration 5/25 | Loss: 0.00106990
Iteration 6/25 | Loss: 0.00106990
Iteration 7/25 | Loss: 0.00106990
Iteration 8/25 | Loss: 0.00106990
Iteration 9/25 | Loss: 0.00106990
Iteration 10/25 | Loss: 0.00106990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010699046542868018, 0.0010699046542868018, 0.0010699046542868018, 0.0010699046542868018, 0.0010699046542868018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010699046542868018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.01605415
Iteration 2/25 | Loss: 0.00083367
Iteration 3/25 | Loss: 0.00083364
Iteration 4/25 | Loss: 0.00083364
Iteration 5/25 | Loss: 0.00083364
Iteration 6/25 | Loss: 0.00083364
Iteration 7/25 | Loss: 0.00083364
Iteration 8/25 | Loss: 0.00083364
Iteration 9/25 | Loss: 0.00083364
Iteration 10/25 | Loss: 0.00083364
Iteration 11/25 | Loss: 0.00083364
Iteration 12/25 | Loss: 0.00083364
Iteration 13/25 | Loss: 0.00083364
Iteration 14/25 | Loss: 0.00083364
Iteration 15/25 | Loss: 0.00083364
Iteration 16/25 | Loss: 0.00083364
Iteration 17/25 | Loss: 0.00083364
Iteration 18/25 | Loss: 0.00083364
Iteration 19/25 | Loss: 0.00083364
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008336364408023655, 0.0008336364408023655, 0.0008336364408023655, 0.0008336364408023655, 0.0008336364408023655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008336364408023655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083364
Iteration 2/1000 | Loss: 0.00002278
Iteration 3/1000 | Loss: 0.00001788
Iteration 4/1000 | Loss: 0.00001634
Iteration 5/1000 | Loss: 0.00001523
Iteration 6/1000 | Loss: 0.00001454
Iteration 7/1000 | Loss: 0.00001407
Iteration 8/1000 | Loss: 0.00001371
Iteration 9/1000 | Loss: 0.00001340
Iteration 10/1000 | Loss: 0.00001331
Iteration 11/1000 | Loss: 0.00001331
Iteration 12/1000 | Loss: 0.00001316
Iteration 13/1000 | Loss: 0.00001310
Iteration 14/1000 | Loss: 0.00001309
Iteration 15/1000 | Loss: 0.00001309
Iteration 16/1000 | Loss: 0.00001301
Iteration 17/1000 | Loss: 0.00001299
Iteration 18/1000 | Loss: 0.00001299
Iteration 19/1000 | Loss: 0.00001294
Iteration 20/1000 | Loss: 0.00001291
Iteration 21/1000 | Loss: 0.00001289
Iteration 22/1000 | Loss: 0.00001289
Iteration 23/1000 | Loss: 0.00001288
Iteration 24/1000 | Loss: 0.00001287
Iteration 25/1000 | Loss: 0.00001282
Iteration 26/1000 | Loss: 0.00001274
Iteration 27/1000 | Loss: 0.00001270
Iteration 28/1000 | Loss: 0.00001262
Iteration 29/1000 | Loss: 0.00001259
Iteration 30/1000 | Loss: 0.00001259
Iteration 31/1000 | Loss: 0.00001258
Iteration 32/1000 | Loss: 0.00001258
Iteration 33/1000 | Loss: 0.00001258
Iteration 34/1000 | Loss: 0.00001257
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001255
Iteration 37/1000 | Loss: 0.00001255
Iteration 38/1000 | Loss: 0.00001254
Iteration 39/1000 | Loss: 0.00001254
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001253
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001252
Iteration 44/1000 | Loss: 0.00001252
Iteration 45/1000 | Loss: 0.00001251
Iteration 46/1000 | Loss: 0.00001251
Iteration 47/1000 | Loss: 0.00001250
Iteration 48/1000 | Loss: 0.00001250
Iteration 49/1000 | Loss: 0.00001249
Iteration 50/1000 | Loss: 0.00001249
Iteration 51/1000 | Loss: 0.00001248
Iteration 52/1000 | Loss: 0.00001248
Iteration 53/1000 | Loss: 0.00001248
Iteration 54/1000 | Loss: 0.00001248
Iteration 55/1000 | Loss: 0.00001247
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001246
Iteration 58/1000 | Loss: 0.00001246
Iteration 59/1000 | Loss: 0.00001246
Iteration 60/1000 | Loss: 0.00001245
Iteration 61/1000 | Loss: 0.00001245
Iteration 62/1000 | Loss: 0.00001245
Iteration 63/1000 | Loss: 0.00001244
Iteration 64/1000 | Loss: 0.00001244
Iteration 65/1000 | Loss: 0.00001243
Iteration 66/1000 | Loss: 0.00001243
Iteration 67/1000 | Loss: 0.00001243
Iteration 68/1000 | Loss: 0.00001242
Iteration 69/1000 | Loss: 0.00001242
Iteration 70/1000 | Loss: 0.00001242
Iteration 71/1000 | Loss: 0.00001242
Iteration 72/1000 | Loss: 0.00001242
Iteration 73/1000 | Loss: 0.00001241
Iteration 74/1000 | Loss: 0.00001241
Iteration 75/1000 | Loss: 0.00001240
Iteration 76/1000 | Loss: 0.00001240
Iteration 77/1000 | Loss: 0.00001240
Iteration 78/1000 | Loss: 0.00001240
Iteration 79/1000 | Loss: 0.00001240
Iteration 80/1000 | Loss: 0.00001240
Iteration 81/1000 | Loss: 0.00001240
Iteration 82/1000 | Loss: 0.00001240
Iteration 83/1000 | Loss: 0.00001240
Iteration 84/1000 | Loss: 0.00001240
Iteration 85/1000 | Loss: 0.00001239
Iteration 86/1000 | Loss: 0.00001239
Iteration 87/1000 | Loss: 0.00001238
Iteration 88/1000 | Loss: 0.00001238
Iteration 89/1000 | Loss: 0.00001238
Iteration 90/1000 | Loss: 0.00001238
Iteration 91/1000 | Loss: 0.00001238
Iteration 92/1000 | Loss: 0.00001237
Iteration 93/1000 | Loss: 0.00001237
Iteration 94/1000 | Loss: 0.00001237
Iteration 95/1000 | Loss: 0.00001237
Iteration 96/1000 | Loss: 0.00001236
Iteration 97/1000 | Loss: 0.00001235
Iteration 98/1000 | Loss: 0.00001235
Iteration 99/1000 | Loss: 0.00001235
Iteration 100/1000 | Loss: 0.00001235
Iteration 101/1000 | Loss: 0.00001235
Iteration 102/1000 | Loss: 0.00001234
Iteration 103/1000 | Loss: 0.00001234
Iteration 104/1000 | Loss: 0.00001234
Iteration 105/1000 | Loss: 0.00001234
Iteration 106/1000 | Loss: 0.00001233
Iteration 107/1000 | Loss: 0.00001233
Iteration 108/1000 | Loss: 0.00001233
Iteration 109/1000 | Loss: 0.00001232
Iteration 110/1000 | Loss: 0.00001232
Iteration 111/1000 | Loss: 0.00001232
Iteration 112/1000 | Loss: 0.00001232
Iteration 113/1000 | Loss: 0.00001232
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001231
Iteration 116/1000 | Loss: 0.00001231
Iteration 117/1000 | Loss: 0.00001231
Iteration 118/1000 | Loss: 0.00001231
Iteration 119/1000 | Loss: 0.00001231
Iteration 120/1000 | Loss: 0.00001231
Iteration 121/1000 | Loss: 0.00001231
Iteration 122/1000 | Loss: 0.00001231
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001231
Iteration 125/1000 | Loss: 0.00001231
Iteration 126/1000 | Loss: 0.00001231
Iteration 127/1000 | Loss: 0.00001231
Iteration 128/1000 | Loss: 0.00001231
Iteration 129/1000 | Loss: 0.00001231
Iteration 130/1000 | Loss: 0.00001231
Iteration 131/1000 | Loss: 0.00001231
Iteration 132/1000 | Loss: 0.00001231
Iteration 133/1000 | Loss: 0.00001231
Iteration 134/1000 | Loss: 0.00001231
Iteration 135/1000 | Loss: 0.00001231
Iteration 136/1000 | Loss: 0.00001231
Iteration 137/1000 | Loss: 0.00001231
Iteration 138/1000 | Loss: 0.00001231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.2307256838539615e-05, 1.2307256838539615e-05, 1.2307256838539615e-05, 1.2307256838539615e-05, 1.2307256838539615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2307256838539615e-05

Optimization complete. Final v2v error: 2.997403621673584 mm

Highest mean error: 3.6814467906951904 mm for frame 141

Lowest mean error: 2.553380250930786 mm for frame 23

Saving results

Total time: 42.8791663646698
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972139
Iteration 2/25 | Loss: 0.00972139
Iteration 3/25 | Loss: 0.00405959
Iteration 4/25 | Loss: 0.00256782
Iteration 5/25 | Loss: 0.00204299
Iteration 6/25 | Loss: 0.00172502
Iteration 7/25 | Loss: 0.00168470
Iteration 8/25 | Loss: 0.00158972
Iteration 9/25 | Loss: 0.00147471
Iteration 10/25 | Loss: 0.00137297
Iteration 11/25 | Loss: 0.00136414
Iteration 12/25 | Loss: 0.00132222
Iteration 13/25 | Loss: 0.00131731
Iteration 14/25 | Loss: 0.00129733
Iteration 15/25 | Loss: 0.00129211
Iteration 16/25 | Loss: 0.00128853
Iteration 17/25 | Loss: 0.00128267
Iteration 18/25 | Loss: 0.00128043
Iteration 19/25 | Loss: 0.00127513
Iteration 20/25 | Loss: 0.00127400
Iteration 21/25 | Loss: 0.00126878
Iteration 22/25 | Loss: 0.00127139
Iteration 23/25 | Loss: 0.00126659
Iteration 24/25 | Loss: 0.00126306
Iteration 25/25 | Loss: 0.00126277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31454515
Iteration 2/25 | Loss: 0.00113684
Iteration 3/25 | Loss: 0.00113684
Iteration 4/25 | Loss: 0.00113684
Iteration 5/25 | Loss: 0.00113684
Iteration 6/25 | Loss: 0.00113684
Iteration 7/25 | Loss: 0.00113684
Iteration 8/25 | Loss: 0.00113684
Iteration 9/25 | Loss: 0.00113684
Iteration 10/25 | Loss: 0.00113684
Iteration 11/25 | Loss: 0.00113684
Iteration 12/25 | Loss: 0.00113684
Iteration 13/25 | Loss: 0.00113684
Iteration 14/25 | Loss: 0.00113684
Iteration 15/25 | Loss: 0.00113684
Iteration 16/25 | Loss: 0.00113684
Iteration 17/25 | Loss: 0.00113684
Iteration 18/25 | Loss: 0.00113684
Iteration 19/25 | Loss: 0.00113684
Iteration 20/25 | Loss: 0.00113684
Iteration 21/25 | Loss: 0.00113684
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011368385748937726, 0.0011368385748937726, 0.0011368385748937726, 0.0011368385748937726, 0.0011368385748937726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011368385748937726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113684
Iteration 2/1000 | Loss: 0.00025902
Iteration 3/1000 | Loss: 0.00015234
Iteration 4/1000 | Loss: 0.00013116
Iteration 5/1000 | Loss: 0.00015771
Iteration 6/1000 | Loss: 0.00008967
Iteration 7/1000 | Loss: 0.00007779
Iteration 8/1000 | Loss: 0.00007109
Iteration 9/1000 | Loss: 0.00006712
Iteration 10/1000 | Loss: 0.00006293
Iteration 11/1000 | Loss: 0.00015132
Iteration 12/1000 | Loss: 0.00087487
Iteration 13/1000 | Loss: 0.00026375
Iteration 14/1000 | Loss: 0.00022683
Iteration 15/1000 | Loss: 0.00010816
Iteration 16/1000 | Loss: 0.00015289
Iteration 17/1000 | Loss: 0.00051019
Iteration 18/1000 | Loss: 0.00043437
Iteration 19/1000 | Loss: 0.00004564
Iteration 20/1000 | Loss: 0.00007498
Iteration 21/1000 | Loss: 0.00004268
Iteration 22/1000 | Loss: 0.00014461
Iteration 23/1000 | Loss: 0.00010910
Iteration 24/1000 | Loss: 0.00018515
Iteration 25/1000 | Loss: 0.00011584
Iteration 26/1000 | Loss: 0.00020901
Iteration 27/1000 | Loss: 0.00003940
Iteration 28/1000 | Loss: 0.00002467
Iteration 29/1000 | Loss: 0.00005107
Iteration 30/1000 | Loss: 0.00002744
Iteration 31/1000 | Loss: 0.00002739
Iteration 32/1000 | Loss: 0.00002108
Iteration 33/1000 | Loss: 0.00006391
Iteration 34/1000 | Loss: 0.00002474
Iteration 35/1000 | Loss: 0.00003555
Iteration 36/1000 | Loss: 0.00002134
Iteration 37/1000 | Loss: 0.00001985
Iteration 38/1000 | Loss: 0.00002002
Iteration 39/1000 | Loss: 0.00001859
Iteration 40/1000 | Loss: 0.00001839
Iteration 41/1000 | Loss: 0.00001817
Iteration 42/1000 | Loss: 0.00001796
Iteration 43/1000 | Loss: 0.00001783
Iteration 44/1000 | Loss: 0.00001781
Iteration 45/1000 | Loss: 0.00001781
Iteration 46/1000 | Loss: 0.00001781
Iteration 47/1000 | Loss: 0.00001780
Iteration 48/1000 | Loss: 0.00001778
Iteration 49/1000 | Loss: 0.00001778
Iteration 50/1000 | Loss: 0.00001776
Iteration 51/1000 | Loss: 0.00001774
Iteration 52/1000 | Loss: 0.00001772
Iteration 53/1000 | Loss: 0.00001772
Iteration 54/1000 | Loss: 0.00001771
Iteration 55/1000 | Loss: 0.00001771
Iteration 56/1000 | Loss: 0.00001771
Iteration 57/1000 | Loss: 0.00001771
Iteration 58/1000 | Loss: 0.00001771
Iteration 59/1000 | Loss: 0.00001770
Iteration 60/1000 | Loss: 0.00001770
Iteration 61/1000 | Loss: 0.00001770
Iteration 62/1000 | Loss: 0.00001770
Iteration 63/1000 | Loss: 0.00001770
Iteration 64/1000 | Loss: 0.00001770
Iteration 65/1000 | Loss: 0.00001770
Iteration 66/1000 | Loss: 0.00001769
Iteration 67/1000 | Loss: 0.00001769
Iteration 68/1000 | Loss: 0.00001769
Iteration 69/1000 | Loss: 0.00001769
Iteration 70/1000 | Loss: 0.00001769
Iteration 71/1000 | Loss: 0.00001769
Iteration 72/1000 | Loss: 0.00001769
Iteration 73/1000 | Loss: 0.00001769
Iteration 74/1000 | Loss: 0.00001769
Iteration 75/1000 | Loss: 0.00001768
Iteration 76/1000 | Loss: 0.00001768
Iteration 77/1000 | Loss: 0.00001768
Iteration 78/1000 | Loss: 0.00001768
Iteration 79/1000 | Loss: 0.00001767
Iteration 80/1000 | Loss: 0.00001767
Iteration 81/1000 | Loss: 0.00001767
Iteration 82/1000 | Loss: 0.00001767
Iteration 83/1000 | Loss: 0.00001767
Iteration 84/1000 | Loss: 0.00001767
Iteration 85/1000 | Loss: 0.00001767
Iteration 86/1000 | Loss: 0.00001767
Iteration 87/1000 | Loss: 0.00001767
Iteration 88/1000 | Loss: 0.00001767
Iteration 89/1000 | Loss: 0.00001767
Iteration 90/1000 | Loss: 0.00001767
Iteration 91/1000 | Loss: 0.00001767
Iteration 92/1000 | Loss: 0.00001767
Iteration 93/1000 | Loss: 0.00001767
Iteration 94/1000 | Loss: 0.00001767
Iteration 95/1000 | Loss: 0.00001766
Iteration 96/1000 | Loss: 0.00001766
Iteration 97/1000 | Loss: 0.00001766
Iteration 98/1000 | Loss: 0.00001766
Iteration 99/1000 | Loss: 0.00001766
Iteration 100/1000 | Loss: 0.00001766
Iteration 101/1000 | Loss: 0.00001766
Iteration 102/1000 | Loss: 0.00001765
Iteration 103/1000 | Loss: 0.00001765
Iteration 104/1000 | Loss: 0.00001765
Iteration 105/1000 | Loss: 0.00001765
Iteration 106/1000 | Loss: 0.00001765
Iteration 107/1000 | Loss: 0.00001765
Iteration 108/1000 | Loss: 0.00001765
Iteration 109/1000 | Loss: 0.00001765
Iteration 110/1000 | Loss: 0.00001765
Iteration 111/1000 | Loss: 0.00001765
Iteration 112/1000 | Loss: 0.00001765
Iteration 113/1000 | Loss: 0.00001765
Iteration 114/1000 | Loss: 0.00001765
Iteration 115/1000 | Loss: 0.00001765
Iteration 116/1000 | Loss: 0.00001765
Iteration 117/1000 | Loss: 0.00001765
Iteration 118/1000 | Loss: 0.00001765
Iteration 119/1000 | Loss: 0.00001764
Iteration 120/1000 | Loss: 0.00001764
Iteration 121/1000 | Loss: 0.00001764
Iteration 122/1000 | Loss: 0.00001764
Iteration 123/1000 | Loss: 0.00001764
Iteration 124/1000 | Loss: 0.00001764
Iteration 125/1000 | Loss: 0.00001764
Iteration 126/1000 | Loss: 0.00001764
Iteration 127/1000 | Loss: 0.00001764
Iteration 128/1000 | Loss: 0.00001764
Iteration 129/1000 | Loss: 0.00001764
Iteration 130/1000 | Loss: 0.00001764
Iteration 131/1000 | Loss: 0.00001764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.7639429643168114e-05, 1.7639429643168114e-05, 1.7639429643168114e-05, 1.7639429643168114e-05, 1.7639429643168114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7639429643168114e-05

Optimization complete. Final v2v error: 3.5653023719787598 mm

Highest mean error: 4.715875625610352 mm for frame 81

Lowest mean error: 3.2833268642425537 mm for frame 189

Saving results

Total time: 124.17936968803406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417919
Iteration 2/25 | Loss: 0.00116588
Iteration 3/25 | Loss: 0.00108302
Iteration 4/25 | Loss: 0.00107195
Iteration 5/25 | Loss: 0.00106844
Iteration 6/25 | Loss: 0.00106751
Iteration 7/25 | Loss: 0.00106744
Iteration 8/25 | Loss: 0.00106744
Iteration 9/25 | Loss: 0.00106744
Iteration 10/25 | Loss: 0.00106744
Iteration 11/25 | Loss: 0.00106744
Iteration 12/25 | Loss: 0.00106744
Iteration 13/25 | Loss: 0.00106744
Iteration 14/25 | Loss: 0.00106744
Iteration 15/25 | Loss: 0.00106744
Iteration 16/25 | Loss: 0.00106744
Iteration 17/25 | Loss: 0.00106744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001067441888153553, 0.001067441888153553, 0.001067441888153553, 0.001067441888153553, 0.001067441888153553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001067441888153553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13607025
Iteration 2/25 | Loss: 0.00059450
Iteration 3/25 | Loss: 0.00059450
Iteration 4/25 | Loss: 0.00059450
Iteration 5/25 | Loss: 0.00059450
Iteration 6/25 | Loss: 0.00059450
Iteration 7/25 | Loss: 0.00059450
Iteration 8/25 | Loss: 0.00059450
Iteration 9/25 | Loss: 0.00059450
Iteration 10/25 | Loss: 0.00059450
Iteration 11/25 | Loss: 0.00059450
Iteration 12/25 | Loss: 0.00059450
Iteration 13/25 | Loss: 0.00059450
Iteration 14/25 | Loss: 0.00059450
Iteration 15/25 | Loss: 0.00059450
Iteration 16/25 | Loss: 0.00059450
Iteration 17/25 | Loss: 0.00059450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005944996955804527, 0.0005944996955804527, 0.0005944996955804527, 0.0005944996955804527, 0.0005944996955804527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005944996955804527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059450
Iteration 2/1000 | Loss: 0.00003254
Iteration 3/1000 | Loss: 0.00001856
Iteration 4/1000 | Loss: 0.00001513
Iteration 5/1000 | Loss: 0.00001408
Iteration 6/1000 | Loss: 0.00001337
Iteration 7/1000 | Loss: 0.00001280
Iteration 8/1000 | Loss: 0.00001247
Iteration 9/1000 | Loss: 0.00001209
Iteration 10/1000 | Loss: 0.00001177
Iteration 11/1000 | Loss: 0.00001157
Iteration 12/1000 | Loss: 0.00001134
Iteration 13/1000 | Loss: 0.00001131
Iteration 14/1000 | Loss: 0.00001130
Iteration 15/1000 | Loss: 0.00001129
Iteration 16/1000 | Loss: 0.00001129
Iteration 17/1000 | Loss: 0.00001128
Iteration 18/1000 | Loss: 0.00001128
Iteration 19/1000 | Loss: 0.00001112
Iteration 20/1000 | Loss: 0.00001107
Iteration 21/1000 | Loss: 0.00001103
Iteration 22/1000 | Loss: 0.00001102
Iteration 23/1000 | Loss: 0.00001102
Iteration 24/1000 | Loss: 0.00001102
Iteration 25/1000 | Loss: 0.00001102
Iteration 26/1000 | Loss: 0.00001102
Iteration 27/1000 | Loss: 0.00001101
Iteration 28/1000 | Loss: 0.00001101
Iteration 29/1000 | Loss: 0.00001101
Iteration 30/1000 | Loss: 0.00001101
Iteration 31/1000 | Loss: 0.00001101
Iteration 32/1000 | Loss: 0.00001100
Iteration 33/1000 | Loss: 0.00001100
Iteration 34/1000 | Loss: 0.00001100
Iteration 35/1000 | Loss: 0.00001100
Iteration 36/1000 | Loss: 0.00001099
Iteration 37/1000 | Loss: 0.00001099
Iteration 38/1000 | Loss: 0.00001099
Iteration 39/1000 | Loss: 0.00001099
Iteration 40/1000 | Loss: 0.00001099
Iteration 41/1000 | Loss: 0.00001098
Iteration 42/1000 | Loss: 0.00001098
Iteration 43/1000 | Loss: 0.00001098
Iteration 44/1000 | Loss: 0.00001098
Iteration 45/1000 | Loss: 0.00001097
Iteration 46/1000 | Loss: 0.00001097
Iteration 47/1000 | Loss: 0.00001096
Iteration 48/1000 | Loss: 0.00001096
Iteration 49/1000 | Loss: 0.00001095
Iteration 50/1000 | Loss: 0.00001095
Iteration 51/1000 | Loss: 0.00001095
Iteration 52/1000 | Loss: 0.00001094
Iteration 53/1000 | Loss: 0.00001094
Iteration 54/1000 | Loss: 0.00001094
Iteration 55/1000 | Loss: 0.00001094
Iteration 56/1000 | Loss: 0.00001093
Iteration 57/1000 | Loss: 0.00001093
Iteration 58/1000 | Loss: 0.00001093
Iteration 59/1000 | Loss: 0.00001093
Iteration 60/1000 | Loss: 0.00001093
Iteration 61/1000 | Loss: 0.00001093
Iteration 62/1000 | Loss: 0.00001093
Iteration 63/1000 | Loss: 0.00001093
Iteration 64/1000 | Loss: 0.00001093
Iteration 65/1000 | Loss: 0.00001093
Iteration 66/1000 | Loss: 0.00001093
Iteration 67/1000 | Loss: 0.00001093
Iteration 68/1000 | Loss: 0.00001092
Iteration 69/1000 | Loss: 0.00001092
Iteration 70/1000 | Loss: 0.00001092
Iteration 71/1000 | Loss: 0.00001092
Iteration 72/1000 | Loss: 0.00001092
Iteration 73/1000 | Loss: 0.00001092
Iteration 74/1000 | Loss: 0.00001092
Iteration 75/1000 | Loss: 0.00001091
Iteration 76/1000 | Loss: 0.00001091
Iteration 77/1000 | Loss: 0.00001091
Iteration 78/1000 | Loss: 0.00001091
Iteration 79/1000 | Loss: 0.00001091
Iteration 80/1000 | Loss: 0.00001091
Iteration 81/1000 | Loss: 0.00001091
Iteration 82/1000 | Loss: 0.00001091
Iteration 83/1000 | Loss: 0.00001090
Iteration 84/1000 | Loss: 0.00001090
Iteration 85/1000 | Loss: 0.00001090
Iteration 86/1000 | Loss: 0.00001089
Iteration 87/1000 | Loss: 0.00001089
Iteration 88/1000 | Loss: 0.00001089
Iteration 89/1000 | Loss: 0.00001089
Iteration 90/1000 | Loss: 0.00001089
Iteration 91/1000 | Loss: 0.00001088
Iteration 92/1000 | Loss: 0.00001088
Iteration 93/1000 | Loss: 0.00001088
Iteration 94/1000 | Loss: 0.00001088
Iteration 95/1000 | Loss: 0.00001088
Iteration 96/1000 | Loss: 0.00001088
Iteration 97/1000 | Loss: 0.00001088
Iteration 98/1000 | Loss: 0.00001088
Iteration 99/1000 | Loss: 0.00001088
Iteration 100/1000 | Loss: 0.00001088
Iteration 101/1000 | Loss: 0.00001088
Iteration 102/1000 | Loss: 0.00001088
Iteration 103/1000 | Loss: 0.00001088
Iteration 104/1000 | Loss: 0.00001088
Iteration 105/1000 | Loss: 0.00001088
Iteration 106/1000 | Loss: 0.00001088
Iteration 107/1000 | Loss: 0.00001088
Iteration 108/1000 | Loss: 0.00001088
Iteration 109/1000 | Loss: 0.00001088
Iteration 110/1000 | Loss: 0.00001088
Iteration 111/1000 | Loss: 0.00001088
Iteration 112/1000 | Loss: 0.00001088
Iteration 113/1000 | Loss: 0.00001088
Iteration 114/1000 | Loss: 0.00001088
Iteration 115/1000 | Loss: 0.00001088
Iteration 116/1000 | Loss: 0.00001088
Iteration 117/1000 | Loss: 0.00001088
Iteration 118/1000 | Loss: 0.00001088
Iteration 119/1000 | Loss: 0.00001088
Iteration 120/1000 | Loss: 0.00001088
Iteration 121/1000 | Loss: 0.00001088
Iteration 122/1000 | Loss: 0.00001088
Iteration 123/1000 | Loss: 0.00001088
Iteration 124/1000 | Loss: 0.00001088
Iteration 125/1000 | Loss: 0.00001088
Iteration 126/1000 | Loss: 0.00001088
Iteration 127/1000 | Loss: 0.00001088
Iteration 128/1000 | Loss: 0.00001088
Iteration 129/1000 | Loss: 0.00001088
Iteration 130/1000 | Loss: 0.00001088
Iteration 131/1000 | Loss: 0.00001088
Iteration 132/1000 | Loss: 0.00001088
Iteration 133/1000 | Loss: 0.00001088
Iteration 134/1000 | Loss: 0.00001088
Iteration 135/1000 | Loss: 0.00001088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.0879564797505736e-05, 1.0879564797505736e-05, 1.0879564797505736e-05, 1.0879564797505736e-05, 1.0879564797505736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0879564797505736e-05

Optimization complete. Final v2v error: 2.8551881313323975 mm

Highest mean error: 2.900413990020752 mm for frame 41

Lowest mean error: 2.8416836261749268 mm for frame 119

Saving results

Total time: 34.10693407058716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068606
Iteration 2/25 | Loss: 0.00273495
Iteration 3/25 | Loss: 0.00193797
Iteration 4/25 | Loss: 0.00189972
Iteration 5/25 | Loss: 0.00173689
Iteration 6/25 | Loss: 0.00158580
Iteration 7/25 | Loss: 0.00153754
Iteration 8/25 | Loss: 0.00146592
Iteration 9/25 | Loss: 0.00142435
Iteration 10/25 | Loss: 0.00140752
Iteration 11/25 | Loss: 0.00139809
Iteration 12/25 | Loss: 0.00139161
Iteration 13/25 | Loss: 0.00138953
Iteration 14/25 | Loss: 0.00138773
Iteration 15/25 | Loss: 0.00138702
Iteration 16/25 | Loss: 0.00138792
Iteration 17/25 | Loss: 0.00138504
Iteration 18/25 | Loss: 0.00138200
Iteration 19/25 | Loss: 0.00138676
Iteration 20/25 | Loss: 0.00138716
Iteration 21/25 | Loss: 0.00138473
Iteration 22/25 | Loss: 0.00138144
Iteration 23/25 | Loss: 0.00138117
Iteration 24/25 | Loss: 0.00138044
Iteration 25/25 | Loss: 0.00138012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.49077320
Iteration 2/25 | Loss: 0.00130267
Iteration 3/25 | Loss: 0.00118198
Iteration 4/25 | Loss: 0.00118198
Iteration 5/25 | Loss: 0.00118198
Iteration 6/25 | Loss: 0.00118198
Iteration 7/25 | Loss: 0.00118198
Iteration 8/25 | Loss: 0.00118198
Iteration 9/25 | Loss: 0.00118198
Iteration 10/25 | Loss: 0.00118198
Iteration 11/25 | Loss: 0.00118198
Iteration 12/25 | Loss: 0.00118198
Iteration 13/25 | Loss: 0.00118198
Iteration 14/25 | Loss: 0.00118198
Iteration 15/25 | Loss: 0.00118198
Iteration 16/25 | Loss: 0.00118198
Iteration 17/25 | Loss: 0.00118198
Iteration 18/25 | Loss: 0.00118198
Iteration 19/25 | Loss: 0.00118198
Iteration 20/25 | Loss: 0.00118198
Iteration 21/25 | Loss: 0.00118198
Iteration 22/25 | Loss: 0.00118198
Iteration 23/25 | Loss: 0.00118198
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011819784995168447, 0.0011819784995168447, 0.0011819784995168447, 0.0011819784995168447, 0.0011819784995168447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011819784995168447

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118198
Iteration 2/1000 | Loss: 0.00055074
Iteration 3/1000 | Loss: 0.00011487
Iteration 4/1000 | Loss: 0.00012441
Iteration 5/1000 | Loss: 0.00016879
Iteration 6/1000 | Loss: 0.00021180
Iteration 7/1000 | Loss: 0.00011391
Iteration 8/1000 | Loss: 0.00009487
Iteration 9/1000 | Loss: 0.00011056
Iteration 10/1000 | Loss: 0.00015933
Iteration 11/1000 | Loss: 0.00009120
Iteration 12/1000 | Loss: 0.00138230
Iteration 13/1000 | Loss: 0.00393557
Iteration 14/1000 | Loss: 0.00028587
Iteration 15/1000 | Loss: 0.00244665
Iteration 16/1000 | Loss: 0.00029135
Iteration 17/1000 | Loss: 0.00029692
Iteration 18/1000 | Loss: 0.00009016
Iteration 19/1000 | Loss: 0.00006798
Iteration 20/1000 | Loss: 0.00015141
Iteration 21/1000 | Loss: 0.00027156
Iteration 22/1000 | Loss: 0.00005064
Iteration 23/1000 | Loss: 0.00010314
Iteration 24/1000 | Loss: 0.00027010
Iteration 25/1000 | Loss: 0.00007440
Iteration 26/1000 | Loss: 0.00005865
Iteration 27/1000 | Loss: 0.00004417
Iteration 28/1000 | Loss: 0.00012033
Iteration 29/1000 | Loss: 0.00004258
Iteration 30/1000 | Loss: 0.00006493
Iteration 31/1000 | Loss: 0.00012535
Iteration 32/1000 | Loss: 0.00004105
Iteration 33/1000 | Loss: 0.00011421
Iteration 34/1000 | Loss: 0.00004705
Iteration 35/1000 | Loss: 0.00007930
Iteration 36/1000 | Loss: 0.00008199
Iteration 37/1000 | Loss: 0.00004405
Iteration 38/1000 | Loss: 0.00005675
Iteration 39/1000 | Loss: 0.00005260
Iteration 40/1000 | Loss: 0.00003963
Iteration 41/1000 | Loss: 0.00006857
Iteration 42/1000 | Loss: 0.00003955
Iteration 43/1000 | Loss: 0.00003939
Iteration 44/1000 | Loss: 0.00003931
Iteration 45/1000 | Loss: 0.00003931
Iteration 46/1000 | Loss: 0.00003930
Iteration 47/1000 | Loss: 0.00003928
Iteration 48/1000 | Loss: 0.00003928
Iteration 49/1000 | Loss: 0.00003928
Iteration 50/1000 | Loss: 0.00003928
Iteration 51/1000 | Loss: 0.00003925
Iteration 52/1000 | Loss: 0.00003924
Iteration 53/1000 | Loss: 0.00003924
Iteration 54/1000 | Loss: 0.00003923
Iteration 55/1000 | Loss: 0.00008754
Iteration 56/1000 | Loss: 0.00022674
Iteration 57/1000 | Loss: 0.00009391
Iteration 58/1000 | Loss: 0.00008410
Iteration 59/1000 | Loss: 0.00008117
Iteration 60/1000 | Loss: 0.00003931
Iteration 61/1000 | Loss: 0.00008988
Iteration 62/1000 | Loss: 0.00003924
Iteration 63/1000 | Loss: 0.00005086
Iteration 64/1000 | Loss: 0.00003914
Iteration 65/1000 | Loss: 0.00003907
Iteration 66/1000 | Loss: 0.00003906
Iteration 67/1000 | Loss: 0.00003906
Iteration 68/1000 | Loss: 0.00003906
Iteration 69/1000 | Loss: 0.00003906
Iteration 70/1000 | Loss: 0.00003905
Iteration 71/1000 | Loss: 0.00003905
Iteration 72/1000 | Loss: 0.00003905
Iteration 73/1000 | Loss: 0.00003905
Iteration 74/1000 | Loss: 0.00003905
Iteration 75/1000 | Loss: 0.00003905
Iteration 76/1000 | Loss: 0.00003905
Iteration 77/1000 | Loss: 0.00003905
Iteration 78/1000 | Loss: 0.00003905
Iteration 79/1000 | Loss: 0.00003905
Iteration 80/1000 | Loss: 0.00003905
Iteration 81/1000 | Loss: 0.00003905
Iteration 82/1000 | Loss: 0.00003905
Iteration 83/1000 | Loss: 0.00003905
Iteration 84/1000 | Loss: 0.00003905
Iteration 85/1000 | Loss: 0.00003905
Iteration 86/1000 | Loss: 0.00003905
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [3.9049493352649733e-05, 3.9049493352649733e-05, 3.9049493352649733e-05, 3.9049493352649733e-05, 3.9049493352649733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9049493352649733e-05

Optimization complete. Final v2v error: 5.223127841949463 mm

Highest mean error: 5.950050354003906 mm for frame 184

Lowest mean error: 4.844836711883545 mm for frame 160

Saving results

Total time: 129.3599362373352
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390112
Iteration 2/25 | Loss: 0.00117403
Iteration 3/25 | Loss: 0.00106891
Iteration 4/25 | Loss: 0.00105593
Iteration 5/25 | Loss: 0.00105332
Iteration 6/25 | Loss: 0.00105332
Iteration 7/25 | Loss: 0.00105332
Iteration 8/25 | Loss: 0.00105332
Iteration 9/25 | Loss: 0.00105332
Iteration 10/25 | Loss: 0.00105332
Iteration 11/25 | Loss: 0.00105332
Iteration 12/25 | Loss: 0.00105332
Iteration 13/25 | Loss: 0.00105332
Iteration 14/25 | Loss: 0.00105332
Iteration 15/25 | Loss: 0.00105332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010533223394304514, 0.0010533223394304514, 0.0010533223394304514, 0.0010533223394304514, 0.0010533223394304514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010533223394304514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34393322
Iteration 2/25 | Loss: 0.00065915
Iteration 3/25 | Loss: 0.00065915
Iteration 4/25 | Loss: 0.00065915
Iteration 5/25 | Loss: 0.00065915
Iteration 6/25 | Loss: 0.00065915
Iteration 7/25 | Loss: 0.00065915
Iteration 8/25 | Loss: 0.00065915
Iteration 9/25 | Loss: 0.00065915
Iteration 10/25 | Loss: 0.00065915
Iteration 11/25 | Loss: 0.00065915
Iteration 12/25 | Loss: 0.00065915
Iteration 13/25 | Loss: 0.00065915
Iteration 14/25 | Loss: 0.00065915
Iteration 15/25 | Loss: 0.00065915
Iteration 16/25 | Loss: 0.00065915
Iteration 17/25 | Loss: 0.00065915
Iteration 18/25 | Loss: 0.00065915
Iteration 19/25 | Loss: 0.00065915
Iteration 20/25 | Loss: 0.00065915
Iteration 21/25 | Loss: 0.00065915
Iteration 22/25 | Loss: 0.00065915
Iteration 23/25 | Loss: 0.00065915
Iteration 24/25 | Loss: 0.00065915
Iteration 25/25 | Loss: 0.00065915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065915
Iteration 2/1000 | Loss: 0.00002517
Iteration 3/1000 | Loss: 0.00001906
Iteration 4/1000 | Loss: 0.00001639
Iteration 5/1000 | Loss: 0.00001549
Iteration 6/1000 | Loss: 0.00001481
Iteration 7/1000 | Loss: 0.00001428
Iteration 8/1000 | Loss: 0.00001381
Iteration 9/1000 | Loss: 0.00001351
Iteration 10/1000 | Loss: 0.00001312
Iteration 11/1000 | Loss: 0.00001297
Iteration 12/1000 | Loss: 0.00001279
Iteration 13/1000 | Loss: 0.00001275
Iteration 14/1000 | Loss: 0.00001265
Iteration 15/1000 | Loss: 0.00001248
Iteration 16/1000 | Loss: 0.00001241
Iteration 17/1000 | Loss: 0.00001241
Iteration 18/1000 | Loss: 0.00001239
Iteration 19/1000 | Loss: 0.00001238
Iteration 20/1000 | Loss: 0.00001238
Iteration 21/1000 | Loss: 0.00001237
Iteration 22/1000 | Loss: 0.00001237
Iteration 23/1000 | Loss: 0.00001236
Iteration 24/1000 | Loss: 0.00001235
Iteration 25/1000 | Loss: 0.00001235
Iteration 26/1000 | Loss: 0.00001234
Iteration 27/1000 | Loss: 0.00001233
Iteration 28/1000 | Loss: 0.00001230
Iteration 29/1000 | Loss: 0.00001224
Iteration 30/1000 | Loss: 0.00001224
Iteration 31/1000 | Loss: 0.00001223
Iteration 32/1000 | Loss: 0.00001223
Iteration 33/1000 | Loss: 0.00001222
Iteration 34/1000 | Loss: 0.00001222
Iteration 35/1000 | Loss: 0.00001222
Iteration 36/1000 | Loss: 0.00001221
Iteration 37/1000 | Loss: 0.00001220
Iteration 38/1000 | Loss: 0.00001219
Iteration 39/1000 | Loss: 0.00001219
Iteration 40/1000 | Loss: 0.00001218
Iteration 41/1000 | Loss: 0.00001218
Iteration 42/1000 | Loss: 0.00001217
Iteration 43/1000 | Loss: 0.00001216
Iteration 44/1000 | Loss: 0.00001215
Iteration 45/1000 | Loss: 0.00001215
Iteration 46/1000 | Loss: 0.00001214
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001212
Iteration 49/1000 | Loss: 0.00001210
Iteration 50/1000 | Loss: 0.00001210
Iteration 51/1000 | Loss: 0.00001209
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001209
Iteration 54/1000 | Loss: 0.00001209
Iteration 55/1000 | Loss: 0.00001209
Iteration 56/1000 | Loss: 0.00001209
Iteration 57/1000 | Loss: 0.00001209
Iteration 58/1000 | Loss: 0.00001209
Iteration 59/1000 | Loss: 0.00001209
Iteration 60/1000 | Loss: 0.00001209
Iteration 61/1000 | Loss: 0.00001209
Iteration 62/1000 | Loss: 0.00001208
Iteration 63/1000 | Loss: 0.00001207
Iteration 64/1000 | Loss: 0.00001207
Iteration 65/1000 | Loss: 0.00001205
Iteration 66/1000 | Loss: 0.00001205
Iteration 67/1000 | Loss: 0.00001205
Iteration 68/1000 | Loss: 0.00001205
Iteration 69/1000 | Loss: 0.00001205
Iteration 70/1000 | Loss: 0.00001205
Iteration 71/1000 | Loss: 0.00001205
Iteration 72/1000 | Loss: 0.00001205
Iteration 73/1000 | Loss: 0.00001205
Iteration 74/1000 | Loss: 0.00001204
Iteration 75/1000 | Loss: 0.00001204
Iteration 76/1000 | Loss: 0.00001204
Iteration 77/1000 | Loss: 0.00001204
Iteration 78/1000 | Loss: 0.00001204
Iteration 79/1000 | Loss: 0.00001201
Iteration 80/1000 | Loss: 0.00001201
Iteration 81/1000 | Loss: 0.00001201
Iteration 82/1000 | Loss: 0.00001201
Iteration 83/1000 | Loss: 0.00001201
Iteration 84/1000 | Loss: 0.00001201
Iteration 85/1000 | Loss: 0.00001201
Iteration 86/1000 | Loss: 0.00001200
Iteration 87/1000 | Loss: 0.00001200
Iteration 88/1000 | Loss: 0.00001200
Iteration 89/1000 | Loss: 0.00001200
Iteration 90/1000 | Loss: 0.00001200
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001199
Iteration 93/1000 | Loss: 0.00001199
Iteration 94/1000 | Loss: 0.00001198
Iteration 95/1000 | Loss: 0.00001198
Iteration 96/1000 | Loss: 0.00001198
Iteration 97/1000 | Loss: 0.00001197
Iteration 98/1000 | Loss: 0.00001197
Iteration 99/1000 | Loss: 0.00001197
Iteration 100/1000 | Loss: 0.00001196
Iteration 101/1000 | Loss: 0.00001196
Iteration 102/1000 | Loss: 0.00001196
Iteration 103/1000 | Loss: 0.00001195
Iteration 104/1000 | Loss: 0.00001195
Iteration 105/1000 | Loss: 0.00001195
Iteration 106/1000 | Loss: 0.00001194
Iteration 107/1000 | Loss: 0.00001194
Iteration 108/1000 | Loss: 0.00001194
Iteration 109/1000 | Loss: 0.00001194
Iteration 110/1000 | Loss: 0.00001194
Iteration 111/1000 | Loss: 0.00001194
Iteration 112/1000 | Loss: 0.00001194
Iteration 113/1000 | Loss: 0.00001194
Iteration 114/1000 | Loss: 0.00001194
Iteration 115/1000 | Loss: 0.00001194
Iteration 116/1000 | Loss: 0.00001194
Iteration 117/1000 | Loss: 0.00001194
Iteration 118/1000 | Loss: 0.00001194
Iteration 119/1000 | Loss: 0.00001194
Iteration 120/1000 | Loss: 0.00001194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.1939853720832616e-05, 1.1939853720832616e-05, 1.1939853720832616e-05, 1.1939853720832616e-05, 1.1939853720832616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1939853720832616e-05

Optimization complete. Final v2v error: 2.8966636657714844 mm

Highest mean error: 3.461559772491455 mm for frame 121

Lowest mean error: 2.54733943939209 mm for frame 23

Saving results

Total time: 38.89127159118652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785655
Iteration 2/25 | Loss: 0.00142823
Iteration 3/25 | Loss: 0.00115899
Iteration 4/25 | Loss: 0.00112697
Iteration 5/25 | Loss: 0.00111658
Iteration 6/25 | Loss: 0.00111436
Iteration 7/25 | Loss: 0.00111377
Iteration 8/25 | Loss: 0.00111361
Iteration 9/25 | Loss: 0.00111350
Iteration 10/25 | Loss: 0.00111349
Iteration 11/25 | Loss: 0.00111349
Iteration 12/25 | Loss: 0.00111348
Iteration 13/25 | Loss: 0.00111348
Iteration 14/25 | Loss: 0.00111348
Iteration 15/25 | Loss: 0.00111348
Iteration 16/25 | Loss: 0.00111347
Iteration 17/25 | Loss: 0.00111347
Iteration 18/25 | Loss: 0.00111347
Iteration 19/25 | Loss: 0.00111347
Iteration 20/25 | Loss: 0.00111347
Iteration 21/25 | Loss: 0.00111347
Iteration 22/25 | Loss: 0.00111347
Iteration 23/25 | Loss: 0.00111347
Iteration 24/25 | Loss: 0.00111347
Iteration 25/25 | Loss: 0.00111347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14011812
Iteration 2/25 | Loss: 0.00088135
Iteration 3/25 | Loss: 0.00088135
Iteration 4/25 | Loss: 0.00088135
Iteration 5/25 | Loss: 0.00088135
Iteration 6/25 | Loss: 0.00088135
Iteration 7/25 | Loss: 0.00088135
Iteration 8/25 | Loss: 0.00088135
Iteration 9/25 | Loss: 0.00088135
Iteration 10/25 | Loss: 0.00088134
Iteration 11/25 | Loss: 0.00088134
Iteration 12/25 | Loss: 0.00088134
Iteration 13/25 | Loss: 0.00088134
Iteration 14/25 | Loss: 0.00088134
Iteration 15/25 | Loss: 0.00088134
Iteration 16/25 | Loss: 0.00088134
Iteration 17/25 | Loss: 0.00088134
Iteration 18/25 | Loss: 0.00088134
Iteration 19/25 | Loss: 0.00088134
Iteration 20/25 | Loss: 0.00088134
Iteration 21/25 | Loss: 0.00088134
Iteration 22/25 | Loss: 0.00088134
Iteration 23/25 | Loss: 0.00088134
Iteration 24/25 | Loss: 0.00088134
Iteration 25/25 | Loss: 0.00088134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088134
Iteration 2/1000 | Loss: 0.00002403
Iteration 3/1000 | Loss: 0.00001851
Iteration 4/1000 | Loss: 0.00001700
Iteration 5/1000 | Loss: 0.00002000
Iteration 6/1000 | Loss: 0.00001564
Iteration 7/1000 | Loss: 0.00001523
Iteration 8/1000 | Loss: 0.00001491
Iteration 9/1000 | Loss: 0.00001453
Iteration 10/1000 | Loss: 0.00001978
Iteration 11/1000 | Loss: 0.00001487
Iteration 12/1000 | Loss: 0.00001411
Iteration 13/1000 | Loss: 0.00001405
Iteration 14/1000 | Loss: 0.00001401
Iteration 15/1000 | Loss: 0.00001401
Iteration 16/1000 | Loss: 0.00001401
Iteration 17/1000 | Loss: 0.00001399
Iteration 18/1000 | Loss: 0.00002046
Iteration 19/1000 | Loss: 0.00001389
Iteration 20/1000 | Loss: 0.00001386
Iteration 21/1000 | Loss: 0.00001385
Iteration 22/1000 | Loss: 0.00001385
Iteration 23/1000 | Loss: 0.00001385
Iteration 24/1000 | Loss: 0.00001385
Iteration 25/1000 | Loss: 0.00001385
Iteration 26/1000 | Loss: 0.00001385
Iteration 27/1000 | Loss: 0.00001384
Iteration 28/1000 | Loss: 0.00001384
Iteration 29/1000 | Loss: 0.00001384
Iteration 30/1000 | Loss: 0.00001384
Iteration 31/1000 | Loss: 0.00001383
Iteration 32/1000 | Loss: 0.00001716
Iteration 33/1000 | Loss: 0.00001380
Iteration 34/1000 | Loss: 0.00001380
Iteration 35/1000 | Loss: 0.00001379
Iteration 36/1000 | Loss: 0.00001379
Iteration 37/1000 | Loss: 0.00001374
Iteration 38/1000 | Loss: 0.00001374
Iteration 39/1000 | Loss: 0.00001374
Iteration 40/1000 | Loss: 0.00001374
Iteration 41/1000 | Loss: 0.00001373
Iteration 42/1000 | Loss: 0.00001373
Iteration 43/1000 | Loss: 0.00001373
Iteration 44/1000 | Loss: 0.00001373
Iteration 45/1000 | Loss: 0.00001373
Iteration 46/1000 | Loss: 0.00001372
Iteration 47/1000 | Loss: 0.00001372
Iteration 48/1000 | Loss: 0.00001372
Iteration 49/1000 | Loss: 0.00001372
Iteration 50/1000 | Loss: 0.00001372
Iteration 51/1000 | Loss: 0.00001372
Iteration 52/1000 | Loss: 0.00001372
Iteration 53/1000 | Loss: 0.00001371
Iteration 54/1000 | Loss: 0.00001371
Iteration 55/1000 | Loss: 0.00001371
Iteration 56/1000 | Loss: 0.00001371
Iteration 57/1000 | Loss: 0.00001371
Iteration 58/1000 | Loss: 0.00001371
Iteration 59/1000 | Loss: 0.00001371
Iteration 60/1000 | Loss: 0.00001371
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001371
Iteration 64/1000 | Loss: 0.00001371
Iteration 65/1000 | Loss: 0.00001371
Iteration 66/1000 | Loss: 0.00001370
Iteration 67/1000 | Loss: 0.00001370
Iteration 68/1000 | Loss: 0.00001370
Iteration 69/1000 | Loss: 0.00001370
Iteration 70/1000 | Loss: 0.00001370
Iteration 71/1000 | Loss: 0.00001370
Iteration 72/1000 | Loss: 0.00001369
Iteration 73/1000 | Loss: 0.00001369
Iteration 74/1000 | Loss: 0.00001369
Iteration 75/1000 | Loss: 0.00001369
Iteration 76/1000 | Loss: 0.00001369
Iteration 77/1000 | Loss: 0.00001369
Iteration 78/1000 | Loss: 0.00001369
Iteration 79/1000 | Loss: 0.00001369
Iteration 80/1000 | Loss: 0.00001369
Iteration 81/1000 | Loss: 0.00001368
Iteration 82/1000 | Loss: 0.00001368
Iteration 83/1000 | Loss: 0.00001368
Iteration 84/1000 | Loss: 0.00001368
Iteration 85/1000 | Loss: 0.00001368
Iteration 86/1000 | Loss: 0.00001368
Iteration 87/1000 | Loss: 0.00001368
Iteration 88/1000 | Loss: 0.00001368
Iteration 89/1000 | Loss: 0.00001368
Iteration 90/1000 | Loss: 0.00001368
Iteration 91/1000 | Loss: 0.00001368
Iteration 92/1000 | Loss: 0.00001368
Iteration 93/1000 | Loss: 0.00001368
Iteration 94/1000 | Loss: 0.00001368
Iteration 95/1000 | Loss: 0.00001368
Iteration 96/1000 | Loss: 0.00001368
Iteration 97/1000 | Loss: 0.00001368
Iteration 98/1000 | Loss: 0.00001368
Iteration 99/1000 | Loss: 0.00001368
Iteration 100/1000 | Loss: 0.00001368
Iteration 101/1000 | Loss: 0.00001368
Iteration 102/1000 | Loss: 0.00001368
Iteration 103/1000 | Loss: 0.00001368
Iteration 104/1000 | Loss: 0.00001368
Iteration 105/1000 | Loss: 0.00001368
Iteration 106/1000 | Loss: 0.00001368
Iteration 107/1000 | Loss: 0.00001368
Iteration 108/1000 | Loss: 0.00001368
Iteration 109/1000 | Loss: 0.00001368
Iteration 110/1000 | Loss: 0.00001368
Iteration 111/1000 | Loss: 0.00001368
Iteration 112/1000 | Loss: 0.00001368
Iteration 113/1000 | Loss: 0.00001368
Iteration 114/1000 | Loss: 0.00001368
Iteration 115/1000 | Loss: 0.00001368
Iteration 116/1000 | Loss: 0.00001368
Iteration 117/1000 | Loss: 0.00001368
Iteration 118/1000 | Loss: 0.00001368
Iteration 119/1000 | Loss: 0.00001368
Iteration 120/1000 | Loss: 0.00001368
Iteration 121/1000 | Loss: 0.00001368
Iteration 122/1000 | Loss: 0.00001368
Iteration 123/1000 | Loss: 0.00001368
Iteration 124/1000 | Loss: 0.00001368
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.3675156878889538e-05, 1.3675156878889538e-05, 1.3675156878889538e-05, 1.3675156878889538e-05, 1.3675156878889538e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3675156878889538e-05

Optimization complete. Final v2v error: 3.05676007270813 mm

Highest mean error: 5.154549598693848 mm for frame 48

Lowest mean error: 2.6974027156829834 mm for frame 151

Saving results

Total time: 49.12517428398132
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406254
Iteration 2/25 | Loss: 0.00123388
Iteration 3/25 | Loss: 0.00113012
Iteration 4/25 | Loss: 0.00111391
Iteration 5/25 | Loss: 0.00110930
Iteration 6/25 | Loss: 0.00110840
Iteration 7/25 | Loss: 0.00110832
Iteration 8/25 | Loss: 0.00110832
Iteration 9/25 | Loss: 0.00110832
Iteration 10/25 | Loss: 0.00110832
Iteration 11/25 | Loss: 0.00110832
Iteration 12/25 | Loss: 0.00110832
Iteration 13/25 | Loss: 0.00110832
Iteration 14/25 | Loss: 0.00110832
Iteration 15/25 | Loss: 0.00110832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001108323922380805, 0.001108323922380805, 0.001108323922380805, 0.001108323922380805, 0.001108323922380805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001108323922380805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99726701
Iteration 2/25 | Loss: 0.00074993
Iteration 3/25 | Loss: 0.00074993
Iteration 4/25 | Loss: 0.00074993
Iteration 5/25 | Loss: 0.00074993
Iteration 6/25 | Loss: 0.00074993
Iteration 7/25 | Loss: 0.00074993
Iteration 8/25 | Loss: 0.00074993
Iteration 9/25 | Loss: 0.00074993
Iteration 10/25 | Loss: 0.00074993
Iteration 11/25 | Loss: 0.00074993
Iteration 12/25 | Loss: 0.00074993
Iteration 13/25 | Loss: 0.00074993
Iteration 14/25 | Loss: 0.00074993
Iteration 15/25 | Loss: 0.00074993
Iteration 16/25 | Loss: 0.00074993
Iteration 17/25 | Loss: 0.00074993
Iteration 18/25 | Loss: 0.00074993
Iteration 19/25 | Loss: 0.00074993
Iteration 20/25 | Loss: 0.00074993
Iteration 21/25 | Loss: 0.00074993
Iteration 22/25 | Loss: 0.00074993
Iteration 23/25 | Loss: 0.00074993
Iteration 24/25 | Loss: 0.00074993
Iteration 25/25 | Loss: 0.00074993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074993
Iteration 2/1000 | Loss: 0.00002982
Iteration 3/1000 | Loss: 0.00002285
Iteration 4/1000 | Loss: 0.00002031
Iteration 5/1000 | Loss: 0.00001941
Iteration 6/1000 | Loss: 0.00001863
Iteration 7/1000 | Loss: 0.00001800
Iteration 8/1000 | Loss: 0.00001746
Iteration 9/1000 | Loss: 0.00001712
Iteration 10/1000 | Loss: 0.00001680
Iteration 11/1000 | Loss: 0.00001656
Iteration 12/1000 | Loss: 0.00001648
Iteration 13/1000 | Loss: 0.00001639
Iteration 14/1000 | Loss: 0.00001638
Iteration 15/1000 | Loss: 0.00001625
Iteration 16/1000 | Loss: 0.00001623
Iteration 17/1000 | Loss: 0.00001623
Iteration 18/1000 | Loss: 0.00001623
Iteration 19/1000 | Loss: 0.00001623
Iteration 20/1000 | Loss: 0.00001623
Iteration 21/1000 | Loss: 0.00001620
Iteration 22/1000 | Loss: 0.00001619
Iteration 23/1000 | Loss: 0.00001618
Iteration 24/1000 | Loss: 0.00001618
Iteration 25/1000 | Loss: 0.00001617
Iteration 26/1000 | Loss: 0.00001616
Iteration 27/1000 | Loss: 0.00001615
Iteration 28/1000 | Loss: 0.00001614
Iteration 29/1000 | Loss: 0.00001613
Iteration 30/1000 | Loss: 0.00001613
Iteration 31/1000 | Loss: 0.00001613
Iteration 32/1000 | Loss: 0.00001612
Iteration 33/1000 | Loss: 0.00001612
Iteration 34/1000 | Loss: 0.00001611
Iteration 35/1000 | Loss: 0.00001610
Iteration 36/1000 | Loss: 0.00001609
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001609
Iteration 39/1000 | Loss: 0.00001609
Iteration 40/1000 | Loss: 0.00001608
Iteration 41/1000 | Loss: 0.00001606
Iteration 42/1000 | Loss: 0.00001606
Iteration 43/1000 | Loss: 0.00001605
Iteration 44/1000 | Loss: 0.00001603
Iteration 45/1000 | Loss: 0.00001602
Iteration 46/1000 | Loss: 0.00001602
Iteration 47/1000 | Loss: 0.00001601
Iteration 48/1000 | Loss: 0.00001601
Iteration 49/1000 | Loss: 0.00001600
Iteration 50/1000 | Loss: 0.00001600
Iteration 51/1000 | Loss: 0.00001600
Iteration 52/1000 | Loss: 0.00001599
Iteration 53/1000 | Loss: 0.00001598
Iteration 54/1000 | Loss: 0.00001595
Iteration 55/1000 | Loss: 0.00001595
Iteration 56/1000 | Loss: 0.00001595
Iteration 57/1000 | Loss: 0.00001595
Iteration 58/1000 | Loss: 0.00001595
Iteration 59/1000 | Loss: 0.00001593
Iteration 60/1000 | Loss: 0.00001593
Iteration 61/1000 | Loss: 0.00001593
Iteration 62/1000 | Loss: 0.00001592
Iteration 63/1000 | Loss: 0.00001592
Iteration 64/1000 | Loss: 0.00001592
Iteration 65/1000 | Loss: 0.00001591
Iteration 66/1000 | Loss: 0.00001591
Iteration 67/1000 | Loss: 0.00001590
Iteration 68/1000 | Loss: 0.00001590
Iteration 69/1000 | Loss: 0.00001590
Iteration 70/1000 | Loss: 0.00001589
Iteration 71/1000 | Loss: 0.00001588
Iteration 72/1000 | Loss: 0.00001588
Iteration 73/1000 | Loss: 0.00001588
Iteration 74/1000 | Loss: 0.00001587
Iteration 75/1000 | Loss: 0.00001587
Iteration 76/1000 | Loss: 0.00001587
Iteration 77/1000 | Loss: 0.00001586
Iteration 78/1000 | Loss: 0.00001586
Iteration 79/1000 | Loss: 0.00001586
Iteration 80/1000 | Loss: 0.00001586
Iteration 81/1000 | Loss: 0.00001585
Iteration 82/1000 | Loss: 0.00001585
Iteration 83/1000 | Loss: 0.00001585
Iteration 84/1000 | Loss: 0.00001585
Iteration 85/1000 | Loss: 0.00001585
Iteration 86/1000 | Loss: 0.00001585
Iteration 87/1000 | Loss: 0.00001584
Iteration 88/1000 | Loss: 0.00001584
Iteration 89/1000 | Loss: 0.00001584
Iteration 90/1000 | Loss: 0.00001583
Iteration 91/1000 | Loss: 0.00001583
Iteration 92/1000 | Loss: 0.00001582
Iteration 93/1000 | Loss: 0.00001582
Iteration 94/1000 | Loss: 0.00001582
Iteration 95/1000 | Loss: 0.00001582
Iteration 96/1000 | Loss: 0.00001582
Iteration 97/1000 | Loss: 0.00001581
Iteration 98/1000 | Loss: 0.00001581
Iteration 99/1000 | Loss: 0.00001581
Iteration 100/1000 | Loss: 0.00001581
Iteration 101/1000 | Loss: 0.00001581
Iteration 102/1000 | Loss: 0.00001581
Iteration 103/1000 | Loss: 0.00001581
Iteration 104/1000 | Loss: 0.00001581
Iteration 105/1000 | Loss: 0.00001581
Iteration 106/1000 | Loss: 0.00001581
Iteration 107/1000 | Loss: 0.00001581
Iteration 108/1000 | Loss: 0.00001581
Iteration 109/1000 | Loss: 0.00001581
Iteration 110/1000 | Loss: 0.00001580
Iteration 111/1000 | Loss: 0.00001580
Iteration 112/1000 | Loss: 0.00001580
Iteration 113/1000 | Loss: 0.00001580
Iteration 114/1000 | Loss: 0.00001579
Iteration 115/1000 | Loss: 0.00001579
Iteration 116/1000 | Loss: 0.00001579
Iteration 117/1000 | Loss: 0.00001579
Iteration 118/1000 | Loss: 0.00001579
Iteration 119/1000 | Loss: 0.00001579
Iteration 120/1000 | Loss: 0.00001579
Iteration 121/1000 | Loss: 0.00001578
Iteration 122/1000 | Loss: 0.00001578
Iteration 123/1000 | Loss: 0.00001578
Iteration 124/1000 | Loss: 0.00001578
Iteration 125/1000 | Loss: 0.00001578
Iteration 126/1000 | Loss: 0.00001577
Iteration 127/1000 | Loss: 0.00001577
Iteration 128/1000 | Loss: 0.00001577
Iteration 129/1000 | Loss: 0.00001577
Iteration 130/1000 | Loss: 0.00001577
Iteration 131/1000 | Loss: 0.00001577
Iteration 132/1000 | Loss: 0.00001577
Iteration 133/1000 | Loss: 0.00001577
Iteration 134/1000 | Loss: 0.00001577
Iteration 135/1000 | Loss: 0.00001577
Iteration 136/1000 | Loss: 0.00001577
Iteration 137/1000 | Loss: 0.00001577
Iteration 138/1000 | Loss: 0.00001577
Iteration 139/1000 | Loss: 0.00001577
Iteration 140/1000 | Loss: 0.00001577
Iteration 141/1000 | Loss: 0.00001577
Iteration 142/1000 | Loss: 0.00001577
Iteration 143/1000 | Loss: 0.00001577
Iteration 144/1000 | Loss: 0.00001577
Iteration 145/1000 | Loss: 0.00001577
Iteration 146/1000 | Loss: 0.00001577
Iteration 147/1000 | Loss: 0.00001577
Iteration 148/1000 | Loss: 0.00001577
Iteration 149/1000 | Loss: 0.00001577
Iteration 150/1000 | Loss: 0.00001577
Iteration 151/1000 | Loss: 0.00001577
Iteration 152/1000 | Loss: 0.00001577
Iteration 153/1000 | Loss: 0.00001577
Iteration 154/1000 | Loss: 0.00001577
Iteration 155/1000 | Loss: 0.00001577
Iteration 156/1000 | Loss: 0.00001577
Iteration 157/1000 | Loss: 0.00001577
Iteration 158/1000 | Loss: 0.00001577
Iteration 159/1000 | Loss: 0.00001577
Iteration 160/1000 | Loss: 0.00001577
Iteration 161/1000 | Loss: 0.00001577
Iteration 162/1000 | Loss: 0.00001577
Iteration 163/1000 | Loss: 0.00001577
Iteration 164/1000 | Loss: 0.00001577
Iteration 165/1000 | Loss: 0.00001577
Iteration 166/1000 | Loss: 0.00001577
Iteration 167/1000 | Loss: 0.00001577
Iteration 168/1000 | Loss: 0.00001577
Iteration 169/1000 | Loss: 0.00001577
Iteration 170/1000 | Loss: 0.00001577
Iteration 171/1000 | Loss: 0.00001577
Iteration 172/1000 | Loss: 0.00001577
Iteration 173/1000 | Loss: 0.00001577
Iteration 174/1000 | Loss: 0.00001577
Iteration 175/1000 | Loss: 0.00001577
Iteration 176/1000 | Loss: 0.00001577
Iteration 177/1000 | Loss: 0.00001577
Iteration 178/1000 | Loss: 0.00001577
Iteration 179/1000 | Loss: 0.00001577
Iteration 180/1000 | Loss: 0.00001577
Iteration 181/1000 | Loss: 0.00001577
Iteration 182/1000 | Loss: 0.00001577
Iteration 183/1000 | Loss: 0.00001577
Iteration 184/1000 | Loss: 0.00001577
Iteration 185/1000 | Loss: 0.00001577
Iteration 186/1000 | Loss: 0.00001577
Iteration 187/1000 | Loss: 0.00001577
Iteration 188/1000 | Loss: 0.00001577
Iteration 189/1000 | Loss: 0.00001577
Iteration 190/1000 | Loss: 0.00001577
Iteration 191/1000 | Loss: 0.00001577
Iteration 192/1000 | Loss: 0.00001577
Iteration 193/1000 | Loss: 0.00001577
Iteration 194/1000 | Loss: 0.00001577
Iteration 195/1000 | Loss: 0.00001577
Iteration 196/1000 | Loss: 0.00001577
Iteration 197/1000 | Loss: 0.00001577
Iteration 198/1000 | Loss: 0.00001577
Iteration 199/1000 | Loss: 0.00001577
Iteration 200/1000 | Loss: 0.00001577
Iteration 201/1000 | Loss: 0.00001577
Iteration 202/1000 | Loss: 0.00001577
Iteration 203/1000 | Loss: 0.00001577
Iteration 204/1000 | Loss: 0.00001577
Iteration 205/1000 | Loss: 0.00001577
Iteration 206/1000 | Loss: 0.00001577
Iteration 207/1000 | Loss: 0.00001577
Iteration 208/1000 | Loss: 0.00001577
Iteration 209/1000 | Loss: 0.00001577
Iteration 210/1000 | Loss: 0.00001577
Iteration 211/1000 | Loss: 0.00001577
Iteration 212/1000 | Loss: 0.00001577
Iteration 213/1000 | Loss: 0.00001577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.5765677744639106e-05, 1.5765677744639106e-05, 1.5765677744639106e-05, 1.5765677744639106e-05, 1.5765677744639106e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5765677744639106e-05

Optimization complete. Final v2v error: 3.3491880893707275 mm

Highest mean error: 3.926572799682617 mm for frame 29

Lowest mean error: 3.0570666790008545 mm for frame 60

Saving results

Total time: 40.96918344497681
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889414
Iteration 2/25 | Loss: 0.00220995
Iteration 3/25 | Loss: 0.00129254
Iteration 4/25 | Loss: 0.00123984
Iteration 5/25 | Loss: 0.00121922
Iteration 6/25 | Loss: 0.00116242
Iteration 7/25 | Loss: 0.00117075
Iteration 8/25 | Loss: 0.00110870
Iteration 9/25 | Loss: 0.00109019
Iteration 10/25 | Loss: 0.00107655
Iteration 11/25 | Loss: 0.00107417
Iteration 12/25 | Loss: 0.00106539
Iteration 13/25 | Loss: 0.00106006
Iteration 14/25 | Loss: 0.00105772
Iteration 15/25 | Loss: 0.00105723
Iteration 16/25 | Loss: 0.00105708
Iteration 17/25 | Loss: 0.00105706
Iteration 18/25 | Loss: 0.00105705
Iteration 19/25 | Loss: 0.00105705
Iteration 20/25 | Loss: 0.00105705
Iteration 21/25 | Loss: 0.00105705
Iteration 22/25 | Loss: 0.00105704
Iteration 23/25 | Loss: 0.00105704
Iteration 24/25 | Loss: 0.00105703
Iteration 25/25 | Loss: 0.00105703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96995437
Iteration 2/25 | Loss: 0.00089898
Iteration 3/25 | Loss: 0.00079354
Iteration 4/25 | Loss: 0.00079354
Iteration 5/25 | Loss: 0.00079354
Iteration 6/25 | Loss: 0.00079354
Iteration 7/25 | Loss: 0.00079354
Iteration 8/25 | Loss: 0.00079354
Iteration 9/25 | Loss: 0.00079354
Iteration 10/25 | Loss: 0.00079354
Iteration 11/25 | Loss: 0.00079354
Iteration 12/25 | Loss: 0.00079354
Iteration 13/25 | Loss: 0.00079354
Iteration 14/25 | Loss: 0.00079354
Iteration 15/25 | Loss: 0.00079354
Iteration 16/25 | Loss: 0.00079354
Iteration 17/25 | Loss: 0.00079354
Iteration 18/25 | Loss: 0.00079354
Iteration 19/25 | Loss: 0.00079354
Iteration 20/25 | Loss: 0.00079354
Iteration 21/25 | Loss: 0.00079354
Iteration 22/25 | Loss: 0.00079354
Iteration 23/25 | Loss: 0.00079354
Iteration 24/25 | Loss: 0.00079354
Iteration 25/25 | Loss: 0.00079354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079354
Iteration 2/1000 | Loss: 0.00001954
Iteration 3/1000 | Loss: 0.00011153
Iteration 4/1000 | Loss: 0.00001354
Iteration 5/1000 | Loss: 0.00008489
Iteration 6/1000 | Loss: 0.00001226
Iteration 7/1000 | Loss: 0.00004506
Iteration 8/1000 | Loss: 0.00001152
Iteration 9/1000 | Loss: 0.00004050
Iteration 10/1000 | Loss: 0.00001124
Iteration 11/1000 | Loss: 0.00003219
Iteration 12/1000 | Loss: 0.00001160
Iteration 13/1000 | Loss: 0.00001089
Iteration 14/1000 | Loss: 0.00001089
Iteration 15/1000 | Loss: 0.00001089
Iteration 16/1000 | Loss: 0.00001089
Iteration 17/1000 | Loss: 0.00001088
Iteration 18/1000 | Loss: 0.00001088
Iteration 19/1000 | Loss: 0.00001088
Iteration 20/1000 | Loss: 0.00001088
Iteration 21/1000 | Loss: 0.00001088
Iteration 22/1000 | Loss: 0.00001088
Iteration 23/1000 | Loss: 0.00001076
Iteration 24/1000 | Loss: 0.00001074
Iteration 25/1000 | Loss: 0.00001073
Iteration 26/1000 | Loss: 0.00001073
Iteration 27/1000 | Loss: 0.00001072
Iteration 28/1000 | Loss: 0.00001068
Iteration 29/1000 | Loss: 0.00001068
Iteration 30/1000 | Loss: 0.00001064
Iteration 31/1000 | Loss: 0.00001058
Iteration 32/1000 | Loss: 0.00001058
Iteration 33/1000 | Loss: 0.00001057
Iteration 34/1000 | Loss: 0.00001056
Iteration 35/1000 | Loss: 0.00004461
Iteration 36/1000 | Loss: 0.00001049
Iteration 37/1000 | Loss: 0.00001047
Iteration 38/1000 | Loss: 0.00001047
Iteration 39/1000 | Loss: 0.00001047
Iteration 40/1000 | Loss: 0.00001047
Iteration 41/1000 | Loss: 0.00001047
Iteration 42/1000 | Loss: 0.00001047
Iteration 43/1000 | Loss: 0.00001047
Iteration 44/1000 | Loss: 0.00001047
Iteration 45/1000 | Loss: 0.00001047
Iteration 46/1000 | Loss: 0.00001046
Iteration 47/1000 | Loss: 0.00001046
Iteration 48/1000 | Loss: 0.00001046
Iteration 49/1000 | Loss: 0.00001045
Iteration 50/1000 | Loss: 0.00001045
Iteration 51/1000 | Loss: 0.00001045
Iteration 52/1000 | Loss: 0.00001045
Iteration 53/1000 | Loss: 0.00001045
Iteration 54/1000 | Loss: 0.00001045
Iteration 55/1000 | Loss: 0.00001045
Iteration 56/1000 | Loss: 0.00001045
Iteration 57/1000 | Loss: 0.00001044
Iteration 58/1000 | Loss: 0.00001044
Iteration 59/1000 | Loss: 0.00001043
Iteration 60/1000 | Loss: 0.00001041
Iteration 61/1000 | Loss: 0.00001041
Iteration 62/1000 | Loss: 0.00001041
Iteration 63/1000 | Loss: 0.00001040
Iteration 64/1000 | Loss: 0.00001040
Iteration 65/1000 | Loss: 0.00001040
Iteration 66/1000 | Loss: 0.00001040
Iteration 67/1000 | Loss: 0.00001040
Iteration 68/1000 | Loss: 0.00001039
Iteration 69/1000 | Loss: 0.00001039
Iteration 70/1000 | Loss: 0.00001039
Iteration 71/1000 | Loss: 0.00001039
Iteration 72/1000 | Loss: 0.00001038
Iteration 73/1000 | Loss: 0.00001038
Iteration 74/1000 | Loss: 0.00001037
Iteration 75/1000 | Loss: 0.00001037
Iteration 76/1000 | Loss: 0.00001036
Iteration 77/1000 | Loss: 0.00001036
Iteration 78/1000 | Loss: 0.00005520
Iteration 79/1000 | Loss: 0.00005867
Iteration 80/1000 | Loss: 0.00003267
Iteration 81/1000 | Loss: 0.00002843
Iteration 82/1000 | Loss: 0.00001279
Iteration 83/1000 | Loss: 0.00001030
Iteration 84/1000 | Loss: 0.00001030
Iteration 85/1000 | Loss: 0.00001030
Iteration 86/1000 | Loss: 0.00001030
Iteration 87/1000 | Loss: 0.00001030
Iteration 88/1000 | Loss: 0.00001030
Iteration 89/1000 | Loss: 0.00001030
Iteration 90/1000 | Loss: 0.00001030
Iteration 91/1000 | Loss: 0.00001030
Iteration 92/1000 | Loss: 0.00001030
Iteration 93/1000 | Loss: 0.00001029
Iteration 94/1000 | Loss: 0.00001029
Iteration 95/1000 | Loss: 0.00001028
Iteration 96/1000 | Loss: 0.00001028
Iteration 97/1000 | Loss: 0.00001028
Iteration 98/1000 | Loss: 0.00001028
Iteration 99/1000 | Loss: 0.00001028
Iteration 100/1000 | Loss: 0.00001028
Iteration 101/1000 | Loss: 0.00001028
Iteration 102/1000 | Loss: 0.00001028
Iteration 103/1000 | Loss: 0.00001027
Iteration 104/1000 | Loss: 0.00001027
Iteration 105/1000 | Loss: 0.00001027
Iteration 106/1000 | Loss: 0.00001027
Iteration 107/1000 | Loss: 0.00001027
Iteration 108/1000 | Loss: 0.00001027
Iteration 109/1000 | Loss: 0.00001027
Iteration 110/1000 | Loss: 0.00001027
Iteration 111/1000 | Loss: 0.00001027
Iteration 112/1000 | Loss: 0.00001027
Iteration 113/1000 | Loss: 0.00001027
Iteration 114/1000 | Loss: 0.00001027
Iteration 115/1000 | Loss: 0.00001026
Iteration 116/1000 | Loss: 0.00001026
Iteration 117/1000 | Loss: 0.00001025
Iteration 118/1000 | Loss: 0.00001025
Iteration 119/1000 | Loss: 0.00001025
Iteration 120/1000 | Loss: 0.00001025
Iteration 121/1000 | Loss: 0.00001025
Iteration 122/1000 | Loss: 0.00001025
Iteration 123/1000 | Loss: 0.00001024
Iteration 124/1000 | Loss: 0.00001024
Iteration 125/1000 | Loss: 0.00001024
Iteration 126/1000 | Loss: 0.00001024
Iteration 127/1000 | Loss: 0.00001024
Iteration 128/1000 | Loss: 0.00001024
Iteration 129/1000 | Loss: 0.00001023
Iteration 130/1000 | Loss: 0.00001023
Iteration 131/1000 | Loss: 0.00001023
Iteration 132/1000 | Loss: 0.00001023
Iteration 133/1000 | Loss: 0.00001023
Iteration 134/1000 | Loss: 0.00001023
Iteration 135/1000 | Loss: 0.00001023
Iteration 136/1000 | Loss: 0.00001022
Iteration 137/1000 | Loss: 0.00001022
Iteration 138/1000 | Loss: 0.00001021
Iteration 139/1000 | Loss: 0.00001021
Iteration 140/1000 | Loss: 0.00001021
Iteration 141/1000 | Loss: 0.00001020
Iteration 142/1000 | Loss: 0.00001020
Iteration 143/1000 | Loss: 0.00001020
Iteration 144/1000 | Loss: 0.00001019
Iteration 145/1000 | Loss: 0.00001019
Iteration 146/1000 | Loss: 0.00001019
Iteration 147/1000 | Loss: 0.00001019
Iteration 148/1000 | Loss: 0.00001018
Iteration 149/1000 | Loss: 0.00001018
Iteration 150/1000 | Loss: 0.00001018
Iteration 151/1000 | Loss: 0.00001018
Iteration 152/1000 | Loss: 0.00001018
Iteration 153/1000 | Loss: 0.00001018
Iteration 154/1000 | Loss: 0.00001018
Iteration 155/1000 | Loss: 0.00001018
Iteration 156/1000 | Loss: 0.00001017
Iteration 157/1000 | Loss: 0.00001017
Iteration 158/1000 | Loss: 0.00001017
Iteration 159/1000 | Loss: 0.00001017
Iteration 160/1000 | Loss: 0.00001017
Iteration 161/1000 | Loss: 0.00001017
Iteration 162/1000 | Loss: 0.00001017
Iteration 163/1000 | Loss: 0.00001017
Iteration 164/1000 | Loss: 0.00001017
Iteration 165/1000 | Loss: 0.00001017
Iteration 166/1000 | Loss: 0.00001017
Iteration 167/1000 | Loss: 0.00001017
Iteration 168/1000 | Loss: 0.00001017
Iteration 169/1000 | Loss: 0.00001017
Iteration 170/1000 | Loss: 0.00001017
Iteration 171/1000 | Loss: 0.00001017
Iteration 172/1000 | Loss: 0.00001017
Iteration 173/1000 | Loss: 0.00001017
Iteration 174/1000 | Loss: 0.00001017
Iteration 175/1000 | Loss: 0.00001017
Iteration 176/1000 | Loss: 0.00001017
Iteration 177/1000 | Loss: 0.00001017
Iteration 178/1000 | Loss: 0.00001017
Iteration 179/1000 | Loss: 0.00001017
Iteration 180/1000 | Loss: 0.00001017
Iteration 181/1000 | Loss: 0.00001017
Iteration 182/1000 | Loss: 0.00001017
Iteration 183/1000 | Loss: 0.00001017
Iteration 184/1000 | Loss: 0.00001017
Iteration 185/1000 | Loss: 0.00001017
Iteration 186/1000 | Loss: 0.00001017
Iteration 187/1000 | Loss: 0.00001017
Iteration 188/1000 | Loss: 0.00001017
Iteration 189/1000 | Loss: 0.00001017
Iteration 190/1000 | Loss: 0.00001017
Iteration 191/1000 | Loss: 0.00001017
Iteration 192/1000 | Loss: 0.00001017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.016547957988223e-05, 1.016547957988223e-05, 1.016547957988223e-05, 1.016547957988223e-05, 1.016547957988223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.016547957988223e-05

Optimization complete. Final v2v error: 2.7354214191436768 mm

Highest mean error: 3.1398372650146484 mm for frame 128

Lowest mean error: 2.448354721069336 mm for frame 11

Saving results

Total time: 80.50367736816406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770621
Iteration 2/25 | Loss: 0.00211859
Iteration 3/25 | Loss: 0.00153951
Iteration 4/25 | Loss: 0.00144146
Iteration 5/25 | Loss: 0.00139869
Iteration 6/25 | Loss: 0.00129026
Iteration 7/25 | Loss: 0.00126542
Iteration 8/25 | Loss: 0.00124329
Iteration 9/25 | Loss: 0.00124251
Iteration 10/25 | Loss: 0.00122695
Iteration 11/25 | Loss: 0.00122239
Iteration 12/25 | Loss: 0.00121593
Iteration 13/25 | Loss: 0.00121907
Iteration 14/25 | Loss: 0.00121451
Iteration 15/25 | Loss: 0.00121457
Iteration 16/25 | Loss: 0.00121350
Iteration 17/25 | Loss: 0.00121370
Iteration 18/25 | Loss: 0.00121321
Iteration 19/25 | Loss: 0.00121219
Iteration 20/25 | Loss: 0.00121144
Iteration 21/25 | Loss: 0.00120860
Iteration 22/25 | Loss: 0.00121200
Iteration 23/25 | Loss: 0.00121000
Iteration 24/25 | Loss: 0.00121382
Iteration 25/25 | Loss: 0.00120990

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.13029861
Iteration 2/25 | Loss: 0.00219663
Iteration 3/25 | Loss: 0.00114759
Iteration 4/25 | Loss: 0.00114758
Iteration 5/25 | Loss: 0.00114758
Iteration 6/25 | Loss: 0.00114758
Iteration 7/25 | Loss: 0.00114758
Iteration 8/25 | Loss: 0.00114758
Iteration 9/25 | Loss: 0.00114758
Iteration 10/25 | Loss: 0.00114758
Iteration 11/25 | Loss: 0.00114758
Iteration 12/25 | Loss: 0.00114758
Iteration 13/25 | Loss: 0.00114758
Iteration 14/25 | Loss: 0.00114758
Iteration 15/25 | Loss: 0.00114758
Iteration 16/25 | Loss: 0.00114758
Iteration 17/25 | Loss: 0.00114758
Iteration 18/25 | Loss: 0.00114758
Iteration 19/25 | Loss: 0.00114758
Iteration 20/25 | Loss: 0.00114758
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001147580798715353, 0.001147580798715353, 0.001147580798715353, 0.001147580798715353, 0.001147580798715353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001147580798715353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114758
Iteration 2/1000 | Loss: 0.00033131
Iteration 3/1000 | Loss: 0.00074382
Iteration 4/1000 | Loss: 0.00035726
Iteration 5/1000 | Loss: 0.00044194
Iteration 6/1000 | Loss: 0.00030700
Iteration 7/1000 | Loss: 0.00060016
Iteration 8/1000 | Loss: 0.00047692
Iteration 9/1000 | Loss: 0.00038465
Iteration 10/1000 | Loss: 0.00031584
Iteration 11/1000 | Loss: 0.00030071
Iteration 12/1000 | Loss: 0.00023048
Iteration 13/1000 | Loss: 0.00042136
Iteration 14/1000 | Loss: 0.00040349
Iteration 15/1000 | Loss: 0.00019200
Iteration 16/1000 | Loss: 0.00037202
Iteration 17/1000 | Loss: 0.00030592
Iteration 18/1000 | Loss: 0.00047358
Iteration 19/1000 | Loss: 0.00029159
Iteration 20/1000 | Loss: 0.00023948
Iteration 21/1000 | Loss: 0.00032964
Iteration 22/1000 | Loss: 0.00033301
Iteration 23/1000 | Loss: 0.00059956
Iteration 24/1000 | Loss: 0.00025801
Iteration 25/1000 | Loss: 0.00032724
Iteration 26/1000 | Loss: 0.00042083
Iteration 27/1000 | Loss: 0.00031605
Iteration 28/1000 | Loss: 0.00032432
Iteration 29/1000 | Loss: 0.00066936
Iteration 30/1000 | Loss: 0.00005598
Iteration 31/1000 | Loss: 0.00147185
Iteration 32/1000 | Loss: 0.00003787
Iteration 33/1000 | Loss: 0.00054891
Iteration 34/1000 | Loss: 0.00002334
Iteration 35/1000 | Loss: 0.00002208
Iteration 36/1000 | Loss: 0.00002147
Iteration 37/1000 | Loss: 0.00011838
Iteration 38/1000 | Loss: 0.00002489
Iteration 39/1000 | Loss: 0.00002222
Iteration 40/1000 | Loss: 0.00002088
Iteration 41/1000 | Loss: 0.00001993
Iteration 42/1000 | Loss: 0.00001923
Iteration 43/1000 | Loss: 0.00001889
Iteration 44/1000 | Loss: 0.00001879
Iteration 45/1000 | Loss: 0.00001863
Iteration 46/1000 | Loss: 0.00001846
Iteration 47/1000 | Loss: 0.00001837
Iteration 48/1000 | Loss: 0.00001835
Iteration 49/1000 | Loss: 0.00001834
Iteration 50/1000 | Loss: 0.00001833
Iteration 51/1000 | Loss: 0.00001832
Iteration 52/1000 | Loss: 0.00001832
Iteration 53/1000 | Loss: 0.00001831
Iteration 54/1000 | Loss: 0.00001830
Iteration 55/1000 | Loss: 0.00001830
Iteration 56/1000 | Loss: 0.00001815
Iteration 57/1000 | Loss: 0.00001814
Iteration 58/1000 | Loss: 0.00001813
Iteration 59/1000 | Loss: 0.00001813
Iteration 60/1000 | Loss: 0.00001813
Iteration 61/1000 | Loss: 0.00001812
Iteration 62/1000 | Loss: 0.00001812
Iteration 63/1000 | Loss: 0.00001812
Iteration 64/1000 | Loss: 0.00001812
Iteration 65/1000 | Loss: 0.00001811
Iteration 66/1000 | Loss: 0.00001809
Iteration 67/1000 | Loss: 0.00001809
Iteration 68/1000 | Loss: 0.00001808
Iteration 69/1000 | Loss: 0.00001808
Iteration 70/1000 | Loss: 0.00001808
Iteration 71/1000 | Loss: 0.00001808
Iteration 72/1000 | Loss: 0.00001808
Iteration 73/1000 | Loss: 0.00001808
Iteration 74/1000 | Loss: 0.00001808
Iteration 75/1000 | Loss: 0.00001808
Iteration 76/1000 | Loss: 0.00001807
Iteration 77/1000 | Loss: 0.00001807
Iteration 78/1000 | Loss: 0.00001807
Iteration 79/1000 | Loss: 0.00001806
Iteration 80/1000 | Loss: 0.00001806
Iteration 81/1000 | Loss: 0.00001806
Iteration 82/1000 | Loss: 0.00001806
Iteration 83/1000 | Loss: 0.00001805
Iteration 84/1000 | Loss: 0.00001805
Iteration 85/1000 | Loss: 0.00001805
Iteration 86/1000 | Loss: 0.00001804
Iteration 87/1000 | Loss: 0.00001804
Iteration 88/1000 | Loss: 0.00001804
Iteration 89/1000 | Loss: 0.00001804
Iteration 90/1000 | Loss: 0.00001804
Iteration 91/1000 | Loss: 0.00001803
Iteration 92/1000 | Loss: 0.00001803
Iteration 93/1000 | Loss: 0.00001803
Iteration 94/1000 | Loss: 0.00001803
Iteration 95/1000 | Loss: 0.00001802
Iteration 96/1000 | Loss: 0.00001802
Iteration 97/1000 | Loss: 0.00001802
Iteration 98/1000 | Loss: 0.00001802
Iteration 99/1000 | Loss: 0.00001801
Iteration 100/1000 | Loss: 0.00001801
Iteration 101/1000 | Loss: 0.00001801
Iteration 102/1000 | Loss: 0.00001801
Iteration 103/1000 | Loss: 0.00001801
Iteration 104/1000 | Loss: 0.00001801
Iteration 105/1000 | Loss: 0.00001801
Iteration 106/1000 | Loss: 0.00001800
Iteration 107/1000 | Loss: 0.00001800
Iteration 108/1000 | Loss: 0.00001800
Iteration 109/1000 | Loss: 0.00001800
Iteration 110/1000 | Loss: 0.00001800
Iteration 111/1000 | Loss: 0.00001800
Iteration 112/1000 | Loss: 0.00001799
Iteration 113/1000 | Loss: 0.00001799
Iteration 114/1000 | Loss: 0.00001799
Iteration 115/1000 | Loss: 0.00001799
Iteration 116/1000 | Loss: 0.00001799
Iteration 117/1000 | Loss: 0.00001799
Iteration 118/1000 | Loss: 0.00001799
Iteration 119/1000 | Loss: 0.00001799
Iteration 120/1000 | Loss: 0.00001798
Iteration 121/1000 | Loss: 0.00001798
Iteration 122/1000 | Loss: 0.00001798
Iteration 123/1000 | Loss: 0.00001798
Iteration 124/1000 | Loss: 0.00001798
Iteration 125/1000 | Loss: 0.00001798
Iteration 126/1000 | Loss: 0.00001798
Iteration 127/1000 | Loss: 0.00001798
Iteration 128/1000 | Loss: 0.00001798
Iteration 129/1000 | Loss: 0.00001798
Iteration 130/1000 | Loss: 0.00001797
Iteration 131/1000 | Loss: 0.00001797
Iteration 132/1000 | Loss: 0.00001797
Iteration 133/1000 | Loss: 0.00001797
Iteration 134/1000 | Loss: 0.00001797
Iteration 135/1000 | Loss: 0.00001797
Iteration 136/1000 | Loss: 0.00001797
Iteration 137/1000 | Loss: 0.00001797
Iteration 138/1000 | Loss: 0.00001797
Iteration 139/1000 | Loss: 0.00001796
Iteration 140/1000 | Loss: 0.00001796
Iteration 141/1000 | Loss: 0.00001796
Iteration 142/1000 | Loss: 0.00001796
Iteration 143/1000 | Loss: 0.00001796
Iteration 144/1000 | Loss: 0.00001796
Iteration 145/1000 | Loss: 0.00001796
Iteration 146/1000 | Loss: 0.00001796
Iteration 147/1000 | Loss: 0.00001796
Iteration 148/1000 | Loss: 0.00001796
Iteration 149/1000 | Loss: 0.00001796
Iteration 150/1000 | Loss: 0.00001796
Iteration 151/1000 | Loss: 0.00001796
Iteration 152/1000 | Loss: 0.00001796
Iteration 153/1000 | Loss: 0.00001796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.7962916899705306e-05, 1.7962916899705306e-05, 1.7962916899705306e-05, 1.7962916899705306e-05, 1.7962916899705306e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7962916899705306e-05

Optimization complete. Final v2v error: 3.504244804382324 mm

Highest mean error: 4.105602741241455 mm for frame 150

Lowest mean error: 3.175114631652832 mm for frame 148

Saving results

Total time: 123.88564682006836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036440
Iteration 2/25 | Loss: 0.00337904
Iteration 3/25 | Loss: 0.00284633
Iteration 4/25 | Loss: 0.00270938
Iteration 5/25 | Loss: 0.00269580
Iteration 6/25 | Loss: 0.00253943
Iteration 7/25 | Loss: 0.00250773
Iteration 8/25 | Loss: 0.00242751
Iteration 9/25 | Loss: 0.00237924
Iteration 10/25 | Loss: 0.00235159
Iteration 11/25 | Loss: 0.00233941
Iteration 12/25 | Loss: 0.00232465
Iteration 13/25 | Loss: 0.00229070
Iteration 14/25 | Loss: 0.00227512
Iteration 15/25 | Loss: 0.00226436
Iteration 16/25 | Loss: 0.00225465
Iteration 17/25 | Loss: 0.00224651
Iteration 18/25 | Loss: 0.00222600
Iteration 19/25 | Loss: 0.00221045
Iteration 20/25 | Loss: 0.00220711
Iteration 21/25 | Loss: 0.00221046
Iteration 22/25 | Loss: 0.00219818
Iteration 23/25 | Loss: 0.00219609
Iteration 24/25 | Loss: 0.00218695
Iteration 25/25 | Loss: 0.00218407

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11448383
Iteration 2/25 | Loss: 0.00466411
Iteration 3/25 | Loss: 0.00466411
Iteration 4/25 | Loss: 0.00466411
Iteration 5/25 | Loss: 0.00466410
Iteration 6/25 | Loss: 0.00466410
Iteration 7/25 | Loss: 0.00466410
Iteration 8/25 | Loss: 0.00466410
Iteration 9/25 | Loss: 0.00466410
Iteration 10/25 | Loss: 0.00466410
Iteration 11/25 | Loss: 0.00466410
Iteration 12/25 | Loss: 0.00466410
Iteration 13/25 | Loss: 0.00466410
Iteration 14/25 | Loss: 0.00466410
Iteration 15/25 | Loss: 0.00466410
Iteration 16/25 | Loss: 0.00466410
Iteration 17/25 | Loss: 0.00466410
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004664102103561163, 0.004664102103561163, 0.004664102103561163, 0.004664102103561163, 0.004664102103561163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004664102103561163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00466410
Iteration 2/1000 | Loss: 0.00757873
Iteration 3/1000 | Loss: 0.00226929
Iteration 4/1000 | Loss: 0.00877550
Iteration 5/1000 | Loss: 0.00826908
Iteration 6/1000 | Loss: 0.00200265
Iteration 7/1000 | Loss: 0.00568233
Iteration 8/1000 | Loss: 0.00949699
Iteration 9/1000 | Loss: 0.00195286
Iteration 10/1000 | Loss: 0.00070400
Iteration 11/1000 | Loss: 0.00721938
Iteration 12/1000 | Loss: 0.00251565
Iteration 13/1000 | Loss: 0.00117988
Iteration 14/1000 | Loss: 0.00120866
Iteration 15/1000 | Loss: 0.00143667
Iteration 16/1000 | Loss: 0.00110511
Iteration 17/1000 | Loss: 0.00124047
Iteration 18/1000 | Loss: 0.00104095
Iteration 19/1000 | Loss: 0.00099272
Iteration 20/1000 | Loss: 0.00073974
Iteration 21/1000 | Loss: 0.00068362
Iteration 22/1000 | Loss: 0.00057589
Iteration 23/1000 | Loss: 0.00129865
Iteration 24/1000 | Loss: 0.00093630
Iteration 25/1000 | Loss: 0.00455886
Iteration 26/1000 | Loss: 0.01706451
Iteration 27/1000 | Loss: 0.03052827
Iteration 28/1000 | Loss: 0.04118685
Iteration 29/1000 | Loss: 0.01345684
Iteration 30/1000 | Loss: 0.00513770
Iteration 31/1000 | Loss: 0.00282469
Iteration 32/1000 | Loss: 0.00273537
Iteration 33/1000 | Loss: 0.00222392
Iteration 34/1000 | Loss: 0.00242095
Iteration 35/1000 | Loss: 0.00170920
Iteration 36/1000 | Loss: 0.00072344
Iteration 37/1000 | Loss: 0.00082186
Iteration 38/1000 | Loss: 0.00147870
Iteration 39/1000 | Loss: 0.00146827
Iteration 40/1000 | Loss: 0.00110564
Iteration 41/1000 | Loss: 0.00282834
Iteration 42/1000 | Loss: 0.00056156
Iteration 43/1000 | Loss: 0.00044456
Iteration 44/1000 | Loss: 0.00069438
Iteration 45/1000 | Loss: 0.00104009
Iteration 46/1000 | Loss: 0.00351617
Iteration 47/1000 | Loss: 0.00275182
Iteration 48/1000 | Loss: 0.00054193
Iteration 49/1000 | Loss: 0.00081420
Iteration 50/1000 | Loss: 0.00074710
Iteration 51/1000 | Loss: 0.00053164
Iteration 52/1000 | Loss: 0.00052208
Iteration 53/1000 | Loss: 0.00243536
Iteration 54/1000 | Loss: 0.00336245
Iteration 55/1000 | Loss: 0.00525583
Iteration 56/1000 | Loss: 0.00266573
Iteration 57/1000 | Loss: 0.00238635
Iteration 58/1000 | Loss: 0.00342501
Iteration 59/1000 | Loss: 0.00275313
Iteration 60/1000 | Loss: 0.00128583
Iteration 61/1000 | Loss: 0.00164177
Iteration 62/1000 | Loss: 0.00097523
Iteration 63/1000 | Loss: 0.00101002
Iteration 64/1000 | Loss: 0.00044916
Iteration 65/1000 | Loss: 0.00040196
Iteration 66/1000 | Loss: 0.00297191
Iteration 67/1000 | Loss: 0.00050039
Iteration 68/1000 | Loss: 0.00088459
Iteration 69/1000 | Loss: 0.00081854
Iteration 70/1000 | Loss: 0.00069399
Iteration 71/1000 | Loss: 0.00040236
Iteration 72/1000 | Loss: 0.00278390
Iteration 73/1000 | Loss: 0.00039131
Iteration 74/1000 | Loss: 0.00042974
Iteration 75/1000 | Loss: 0.00031126
Iteration 76/1000 | Loss: 0.00147356
Iteration 77/1000 | Loss: 0.00167449
Iteration 78/1000 | Loss: 0.00120984
Iteration 79/1000 | Loss: 0.00043247
Iteration 80/1000 | Loss: 0.00219720
Iteration 81/1000 | Loss: 0.00102243
Iteration 82/1000 | Loss: 0.00140109
Iteration 83/1000 | Loss: 0.00057928
Iteration 84/1000 | Loss: 0.00105966
Iteration 85/1000 | Loss: 0.00092474
Iteration 86/1000 | Loss: 0.00090970
Iteration 87/1000 | Loss: 0.00269701
Iteration 88/1000 | Loss: 0.00050965
Iteration 89/1000 | Loss: 0.00033536
Iteration 90/1000 | Loss: 0.00073490
Iteration 91/1000 | Loss: 0.00208680
Iteration 92/1000 | Loss: 0.00091271
Iteration 93/1000 | Loss: 0.00154708
Iteration 94/1000 | Loss: 0.00138607
Iteration 95/1000 | Loss: 0.00174590
Iteration 96/1000 | Loss: 0.00168662
Iteration 97/1000 | Loss: 0.00112898
Iteration 98/1000 | Loss: 0.00043012
Iteration 99/1000 | Loss: 0.00137628
Iteration 100/1000 | Loss: 0.00135484
Iteration 101/1000 | Loss: 0.00086496
Iteration 102/1000 | Loss: 0.00150574
Iteration 103/1000 | Loss: 0.00093720
Iteration 104/1000 | Loss: 0.00103735
Iteration 105/1000 | Loss: 0.00222541
Iteration 106/1000 | Loss: 0.00079239
Iteration 107/1000 | Loss: 0.00075384
Iteration 108/1000 | Loss: 0.00251157
Iteration 109/1000 | Loss: 0.00300029
Iteration 110/1000 | Loss: 0.00055853
Iteration 111/1000 | Loss: 0.00052278
Iteration 112/1000 | Loss: 0.00078671
Iteration 113/1000 | Loss: 0.00128696
Iteration 114/1000 | Loss: 0.00076463
Iteration 115/1000 | Loss: 0.00043980
Iteration 116/1000 | Loss: 0.00070289
Iteration 117/1000 | Loss: 0.00071743
Iteration 118/1000 | Loss: 0.00056835
Iteration 119/1000 | Loss: 0.00053831
Iteration 120/1000 | Loss: 0.00060917
Iteration 121/1000 | Loss: 0.00031464
Iteration 122/1000 | Loss: 0.00047342
Iteration 123/1000 | Loss: 0.00047333
Iteration 124/1000 | Loss: 0.00059686
Iteration 125/1000 | Loss: 0.00114720
Iteration 126/1000 | Loss: 0.00089969
Iteration 127/1000 | Loss: 0.00103630
Iteration 128/1000 | Loss: 0.00057128
Iteration 129/1000 | Loss: 0.00038195
Iteration 130/1000 | Loss: 0.00047978
Iteration 131/1000 | Loss: 0.00041639
Iteration 132/1000 | Loss: 0.00074040
Iteration 133/1000 | Loss: 0.00072100
Iteration 134/1000 | Loss: 0.00043819
Iteration 135/1000 | Loss: 0.00032241
Iteration 136/1000 | Loss: 0.00050397
Iteration 137/1000 | Loss: 0.00102889
Iteration 138/1000 | Loss: 0.00077779
Iteration 139/1000 | Loss: 0.00135052
Iteration 140/1000 | Loss: 0.00084840
Iteration 141/1000 | Loss: 0.00037257
Iteration 142/1000 | Loss: 0.00024764
Iteration 143/1000 | Loss: 0.00030922
Iteration 144/1000 | Loss: 0.00023790
Iteration 145/1000 | Loss: 0.00045196
Iteration 146/1000 | Loss: 0.00030135
Iteration 147/1000 | Loss: 0.00078216
Iteration 148/1000 | Loss: 0.00076460
Iteration 149/1000 | Loss: 0.00024042
Iteration 150/1000 | Loss: 0.00022967
Iteration 151/1000 | Loss: 0.00022554
Iteration 152/1000 | Loss: 0.00069758
Iteration 153/1000 | Loss: 0.00027695
Iteration 154/1000 | Loss: 0.00092556
Iteration 155/1000 | Loss: 0.00053404
Iteration 156/1000 | Loss: 0.00104538
Iteration 157/1000 | Loss: 0.00063708
Iteration 158/1000 | Loss: 0.00046739
Iteration 159/1000 | Loss: 0.00055955
Iteration 160/1000 | Loss: 0.00043925
Iteration 161/1000 | Loss: 0.00037623
Iteration 162/1000 | Loss: 0.00048980
Iteration 163/1000 | Loss: 0.00040350
Iteration 164/1000 | Loss: 0.00033471
Iteration 165/1000 | Loss: 0.00024580
Iteration 166/1000 | Loss: 0.00052469
Iteration 167/1000 | Loss: 0.00075302
Iteration 168/1000 | Loss: 0.00071484
Iteration 169/1000 | Loss: 0.00021930
Iteration 170/1000 | Loss: 0.00049988
Iteration 171/1000 | Loss: 0.00030708
Iteration 172/1000 | Loss: 0.00047245
Iteration 173/1000 | Loss: 0.00023194
Iteration 174/1000 | Loss: 0.00037172
Iteration 175/1000 | Loss: 0.00076060
Iteration 176/1000 | Loss: 0.00043840
Iteration 177/1000 | Loss: 0.00030709
Iteration 178/1000 | Loss: 0.00023932
Iteration 179/1000 | Loss: 0.00031086
Iteration 180/1000 | Loss: 0.00024463
Iteration 181/1000 | Loss: 0.00028456
Iteration 182/1000 | Loss: 0.00028807
Iteration 183/1000 | Loss: 0.00020243
Iteration 184/1000 | Loss: 0.00029817
Iteration 185/1000 | Loss: 0.00059336
Iteration 186/1000 | Loss: 0.00018445
Iteration 187/1000 | Loss: 0.00049674
Iteration 188/1000 | Loss: 0.00020106
Iteration 189/1000 | Loss: 0.00040560
Iteration 190/1000 | Loss: 0.00070126
Iteration 191/1000 | Loss: 0.00038135
Iteration 192/1000 | Loss: 0.00019631
Iteration 193/1000 | Loss: 0.00030357
Iteration 194/1000 | Loss: 0.00026990
Iteration 195/1000 | Loss: 0.00023702
Iteration 196/1000 | Loss: 0.00020619
Iteration 197/1000 | Loss: 0.00017848
Iteration 198/1000 | Loss: 0.00025439
Iteration 199/1000 | Loss: 0.00047121
Iteration 200/1000 | Loss: 0.00033596
Iteration 201/1000 | Loss: 0.00018582
Iteration 202/1000 | Loss: 0.00049746
Iteration 203/1000 | Loss: 0.00160924
Iteration 204/1000 | Loss: 0.00043469
Iteration 205/1000 | Loss: 0.00054766
Iteration 206/1000 | Loss: 0.00046068
Iteration 207/1000 | Loss: 0.00046259
Iteration 208/1000 | Loss: 0.00024133
Iteration 209/1000 | Loss: 0.00063427
Iteration 210/1000 | Loss: 0.00074587
Iteration 211/1000 | Loss: 0.00068423
Iteration 212/1000 | Loss: 0.00018239
Iteration 213/1000 | Loss: 0.00015229
Iteration 214/1000 | Loss: 0.00014543
Iteration 215/1000 | Loss: 0.00014822
Iteration 216/1000 | Loss: 0.00017113
Iteration 217/1000 | Loss: 0.00019898
Iteration 218/1000 | Loss: 0.00018279
Iteration 219/1000 | Loss: 0.00017005
Iteration 220/1000 | Loss: 0.00023683
Iteration 221/1000 | Loss: 0.00017802
Iteration 222/1000 | Loss: 0.00018812
Iteration 223/1000 | Loss: 0.00039511
Iteration 224/1000 | Loss: 0.00015464
Iteration 225/1000 | Loss: 0.00022077
Iteration 226/1000 | Loss: 0.00013827
Iteration 227/1000 | Loss: 0.00013274
Iteration 228/1000 | Loss: 0.00013218
Iteration 229/1000 | Loss: 0.00032818
Iteration 230/1000 | Loss: 0.00013761
Iteration 231/1000 | Loss: 0.00049547
Iteration 232/1000 | Loss: 0.00028370
Iteration 233/1000 | Loss: 0.00013515
Iteration 234/1000 | Loss: 0.00046756
Iteration 235/1000 | Loss: 0.00118088
Iteration 236/1000 | Loss: 0.00095830
Iteration 237/1000 | Loss: 0.00057906
Iteration 238/1000 | Loss: 0.00027820
Iteration 239/1000 | Loss: 0.00029429
Iteration 240/1000 | Loss: 0.00026271
Iteration 241/1000 | Loss: 0.00013423
Iteration 242/1000 | Loss: 0.00016993
Iteration 243/1000 | Loss: 0.00023371
Iteration 244/1000 | Loss: 0.00023195
Iteration 245/1000 | Loss: 0.00020372
Iteration 246/1000 | Loss: 0.00031797
Iteration 247/1000 | Loss: 0.00018264
Iteration 248/1000 | Loss: 0.00038726
Iteration 249/1000 | Loss: 0.00034448
Iteration 250/1000 | Loss: 0.00046817
Iteration 251/1000 | Loss: 0.00028003
Iteration 252/1000 | Loss: 0.00039730
Iteration 253/1000 | Loss: 0.00014281
Iteration 254/1000 | Loss: 0.00009141
Iteration 255/1000 | Loss: 0.00019415
Iteration 256/1000 | Loss: 0.00009206
Iteration 257/1000 | Loss: 0.00009331
Iteration 258/1000 | Loss: 0.00012602
Iteration 259/1000 | Loss: 0.00014378
Iteration 260/1000 | Loss: 0.00008177
Iteration 261/1000 | Loss: 0.00011258
Iteration 262/1000 | Loss: 0.00008064
Iteration 263/1000 | Loss: 0.00020351
Iteration 264/1000 | Loss: 0.00030043
Iteration 265/1000 | Loss: 0.00020026
Iteration 266/1000 | Loss: 0.00020909
Iteration 267/1000 | Loss: 0.00015259
Iteration 268/1000 | Loss: 0.00006834
Iteration 269/1000 | Loss: 0.00011461
Iteration 270/1000 | Loss: 0.00050440
Iteration 271/1000 | Loss: 0.00012063
Iteration 272/1000 | Loss: 0.00006481
Iteration 273/1000 | Loss: 0.00009366
Iteration 274/1000 | Loss: 0.00005985
Iteration 275/1000 | Loss: 0.00005824
Iteration 276/1000 | Loss: 0.00005688
Iteration 277/1000 | Loss: 0.00006574
Iteration 278/1000 | Loss: 0.00005729
Iteration 279/1000 | Loss: 0.00005437
Iteration 280/1000 | Loss: 0.00005754
Iteration 281/1000 | Loss: 0.00006164
Iteration 282/1000 | Loss: 0.00014914
Iteration 283/1000 | Loss: 0.00014679
Iteration 284/1000 | Loss: 0.00010305
Iteration 285/1000 | Loss: 0.00006520
Iteration 286/1000 | Loss: 0.00014788
Iteration 287/1000 | Loss: 0.00008660
Iteration 288/1000 | Loss: 0.00006886
Iteration 289/1000 | Loss: 0.00011972
Iteration 290/1000 | Loss: 0.00008728
Iteration 291/1000 | Loss: 0.00006228
Iteration 292/1000 | Loss: 0.00012059
Iteration 293/1000 | Loss: 0.00005952
Iteration 294/1000 | Loss: 0.00010870
Iteration 295/1000 | Loss: 0.00014157
Iteration 296/1000 | Loss: 0.00010104
Iteration 297/1000 | Loss: 0.00005377
Iteration 298/1000 | Loss: 0.00005117
Iteration 299/1000 | Loss: 0.00005015
Iteration 300/1000 | Loss: 0.00005992
Iteration 301/1000 | Loss: 0.00005672
Iteration 302/1000 | Loss: 0.00005874
Iteration 303/1000 | Loss: 0.00005493
Iteration 304/1000 | Loss: 0.00004953
Iteration 305/1000 | Loss: 0.00005565
Iteration 306/1000 | Loss: 0.00005386
Iteration 307/1000 | Loss: 0.00005423
Iteration 308/1000 | Loss: 0.00005209
Iteration 309/1000 | Loss: 0.00005298
Iteration 310/1000 | Loss: 0.00005229
Iteration 311/1000 | Loss: 0.00005401
Iteration 312/1000 | Loss: 0.00004998
Iteration 313/1000 | Loss: 0.00005560
Iteration 314/1000 | Loss: 0.00005393
Iteration 315/1000 | Loss: 0.00005536
Iteration 316/1000 | Loss: 0.00005052
Iteration 317/1000 | Loss: 0.00004978
Iteration 318/1000 | Loss: 0.00004931
Iteration 319/1000 | Loss: 0.00004886
Iteration 320/1000 | Loss: 0.00004846
Iteration 321/1000 | Loss: 0.00004817
Iteration 322/1000 | Loss: 0.00004811
Iteration 323/1000 | Loss: 0.00004810
Iteration 324/1000 | Loss: 0.00004810
Iteration 325/1000 | Loss: 0.00004810
Iteration 326/1000 | Loss: 0.00004810
Iteration 327/1000 | Loss: 0.00004810
Iteration 328/1000 | Loss: 0.00004810
Iteration 329/1000 | Loss: 0.00004809
Iteration 330/1000 | Loss: 0.00004809
Iteration 331/1000 | Loss: 0.00004809
Iteration 332/1000 | Loss: 0.00004809
Iteration 333/1000 | Loss: 0.00004809
Iteration 334/1000 | Loss: 0.00004808
Iteration 335/1000 | Loss: 0.00004808
Iteration 336/1000 | Loss: 0.00004808
Iteration 337/1000 | Loss: 0.00004807
Iteration 338/1000 | Loss: 0.00004807
Iteration 339/1000 | Loss: 0.00022062
Iteration 340/1000 | Loss: 0.00025038
Iteration 341/1000 | Loss: 0.00006137
Iteration 342/1000 | Loss: 0.00005363
Iteration 343/1000 | Loss: 0.00011184
Iteration 344/1000 | Loss: 0.00005294
Iteration 345/1000 | Loss: 0.00017475
Iteration 346/1000 | Loss: 0.00018704
Iteration 347/1000 | Loss: 0.00018999
Iteration 348/1000 | Loss: 0.00030140
Iteration 349/1000 | Loss: 0.00028552
Iteration 350/1000 | Loss: 0.00016549
Iteration 351/1000 | Loss: 0.00005149
Iteration 352/1000 | Loss: 0.00004862
Iteration 353/1000 | Loss: 0.00004820
Iteration 354/1000 | Loss: 0.00004797
Iteration 355/1000 | Loss: 0.00004793
Iteration 356/1000 | Loss: 0.00004791
Iteration 357/1000 | Loss: 0.00004787
Iteration 358/1000 | Loss: 0.00004785
Iteration 359/1000 | Loss: 0.00004785
Iteration 360/1000 | Loss: 0.00004784
Iteration 361/1000 | Loss: 0.00004784
Iteration 362/1000 | Loss: 0.00004782
Iteration 363/1000 | Loss: 0.00004781
Iteration 364/1000 | Loss: 0.00004781
Iteration 365/1000 | Loss: 0.00004781
Iteration 366/1000 | Loss: 0.00004780
Iteration 367/1000 | Loss: 0.00004779
Iteration 368/1000 | Loss: 0.00004778
Iteration 369/1000 | Loss: 0.00004778
Iteration 370/1000 | Loss: 0.00004778
Iteration 371/1000 | Loss: 0.00004778
Iteration 372/1000 | Loss: 0.00004778
Iteration 373/1000 | Loss: 0.00004778
Iteration 374/1000 | Loss: 0.00004778
Iteration 375/1000 | Loss: 0.00004777
Iteration 376/1000 | Loss: 0.00004777
Iteration 377/1000 | Loss: 0.00004777
Iteration 378/1000 | Loss: 0.00004777
Iteration 379/1000 | Loss: 0.00004777
Iteration 380/1000 | Loss: 0.00004776
Iteration 381/1000 | Loss: 0.00004776
Iteration 382/1000 | Loss: 0.00004776
Iteration 383/1000 | Loss: 0.00004776
Iteration 384/1000 | Loss: 0.00004776
Iteration 385/1000 | Loss: 0.00004775
Iteration 386/1000 | Loss: 0.00004775
Iteration 387/1000 | Loss: 0.00004775
Iteration 388/1000 | Loss: 0.00004775
Iteration 389/1000 | Loss: 0.00004775
Iteration 390/1000 | Loss: 0.00004775
Iteration 391/1000 | Loss: 0.00004775
Iteration 392/1000 | Loss: 0.00004774
Iteration 393/1000 | Loss: 0.00004774
Iteration 394/1000 | Loss: 0.00004774
Iteration 395/1000 | Loss: 0.00004774
Iteration 396/1000 | Loss: 0.00004774
Iteration 397/1000 | Loss: 0.00004774
Iteration 398/1000 | Loss: 0.00004774
Iteration 399/1000 | Loss: 0.00004773
Iteration 400/1000 | Loss: 0.00004773
Iteration 401/1000 | Loss: 0.00004773
Iteration 402/1000 | Loss: 0.00004773
Iteration 403/1000 | Loss: 0.00004773
Iteration 404/1000 | Loss: 0.00004773
Iteration 405/1000 | Loss: 0.00004773
Iteration 406/1000 | Loss: 0.00004773
Iteration 407/1000 | Loss: 0.00004773
Iteration 408/1000 | Loss: 0.00004772
Iteration 409/1000 | Loss: 0.00004772
Iteration 410/1000 | Loss: 0.00004772
Iteration 411/1000 | Loss: 0.00004772
Iteration 412/1000 | Loss: 0.00004772
Iteration 413/1000 | Loss: 0.00004772
Iteration 414/1000 | Loss: 0.00004772
Iteration 415/1000 | Loss: 0.00004771
Iteration 416/1000 | Loss: 0.00004771
Iteration 417/1000 | Loss: 0.00004771
Iteration 418/1000 | Loss: 0.00004770
Iteration 419/1000 | Loss: 0.00004770
Iteration 420/1000 | Loss: 0.00004770
Iteration 421/1000 | Loss: 0.00004770
Iteration 422/1000 | Loss: 0.00004770
Iteration 423/1000 | Loss: 0.00004769
Iteration 424/1000 | Loss: 0.00004769
Iteration 425/1000 | Loss: 0.00004769
Iteration 426/1000 | Loss: 0.00004768
Iteration 427/1000 | Loss: 0.00004768
Iteration 428/1000 | Loss: 0.00004768
Iteration 429/1000 | Loss: 0.00004767
Iteration 430/1000 | Loss: 0.00004767
Iteration 431/1000 | Loss: 0.00004767
Iteration 432/1000 | Loss: 0.00004766
Iteration 433/1000 | Loss: 0.00004766
Iteration 434/1000 | Loss: 0.00020679
Iteration 435/1000 | Loss: 0.00009329
Iteration 436/1000 | Loss: 0.00006166
Iteration 437/1000 | Loss: 0.00053636
Iteration 438/1000 | Loss: 0.00017261
Iteration 439/1000 | Loss: 0.00046844
Iteration 440/1000 | Loss: 0.00016760
Iteration 441/1000 | Loss: 0.00012244
Iteration 442/1000 | Loss: 0.00015793
Iteration 443/1000 | Loss: 0.00012026
Iteration 444/1000 | Loss: 0.00023041
Iteration 445/1000 | Loss: 0.00013949
Iteration 446/1000 | Loss: 0.00028170
Iteration 447/1000 | Loss: 0.00020661
Iteration 448/1000 | Loss: 0.00007097
Iteration 449/1000 | Loss: 0.00007066
Iteration 450/1000 | Loss: 0.00023041
Iteration 451/1000 | Loss: 0.00007166
Iteration 452/1000 | Loss: 0.00006513
Iteration 453/1000 | Loss: 0.00006294
Iteration 454/1000 | Loss: 0.00005252
Iteration 455/1000 | Loss: 0.00005160
Iteration 456/1000 | Loss: 0.00023328
Iteration 457/1000 | Loss: 0.00005569
Iteration 458/1000 | Loss: 0.00005266
Iteration 459/1000 | Loss: 0.00005131
Iteration 460/1000 | Loss: 0.00005062
Iteration 461/1000 | Loss: 0.00004997
Iteration 462/1000 | Loss: 0.00004959
Iteration 463/1000 | Loss: 0.00017613
Iteration 464/1000 | Loss: 0.00005401
Iteration 465/1000 | Loss: 0.00004983
Iteration 466/1000 | Loss: 0.00004856
Iteration 467/1000 | Loss: 0.00004823
Iteration 468/1000 | Loss: 0.00004817
Iteration 469/1000 | Loss: 0.00004800
Iteration 470/1000 | Loss: 0.00004793
Iteration 471/1000 | Loss: 0.00004792
Iteration 472/1000 | Loss: 0.00004792
Iteration 473/1000 | Loss: 0.00004790
Iteration 474/1000 | Loss: 0.00004788
Iteration 475/1000 | Loss: 0.00004784
Iteration 476/1000 | Loss: 0.00004780
Iteration 477/1000 | Loss: 0.00004779
Iteration 478/1000 | Loss: 0.00004779
Iteration 479/1000 | Loss: 0.00004779
Iteration 480/1000 | Loss: 0.00004779
Iteration 481/1000 | Loss: 0.00004779
Iteration 482/1000 | Loss: 0.00004779
Iteration 483/1000 | Loss: 0.00004778
Iteration 484/1000 | Loss: 0.00004778
Iteration 485/1000 | Loss: 0.00004778
Iteration 486/1000 | Loss: 0.00004778
Iteration 487/1000 | Loss: 0.00004778
Iteration 488/1000 | Loss: 0.00004778
Iteration 489/1000 | Loss: 0.00004778
Iteration 490/1000 | Loss: 0.00004778
Iteration 491/1000 | Loss: 0.00004778
Iteration 492/1000 | Loss: 0.00004778
Iteration 493/1000 | Loss: 0.00004778
Iteration 494/1000 | Loss: 0.00004777
Iteration 495/1000 | Loss: 0.00004777
Iteration 496/1000 | Loss: 0.00004777
Iteration 497/1000 | Loss: 0.00004777
Iteration 498/1000 | Loss: 0.00004777
Iteration 499/1000 | Loss: 0.00004777
Iteration 500/1000 | Loss: 0.00004777
Iteration 501/1000 | Loss: 0.00004777
Iteration 502/1000 | Loss: 0.00004777
Iteration 503/1000 | Loss: 0.00004777
Iteration 504/1000 | Loss: 0.00004777
Iteration 505/1000 | Loss: 0.00004777
Iteration 506/1000 | Loss: 0.00004777
Iteration 507/1000 | Loss: 0.00004777
Iteration 508/1000 | Loss: 0.00004776
Iteration 509/1000 | Loss: 0.00004776
Iteration 510/1000 | Loss: 0.00004776
Iteration 511/1000 | Loss: 0.00004776
Iteration 512/1000 | Loss: 0.00004776
Iteration 513/1000 | Loss: 0.00004775
Iteration 514/1000 | Loss: 0.00004775
Iteration 515/1000 | Loss: 0.00004775
Iteration 516/1000 | Loss: 0.00004775
Iteration 517/1000 | Loss: 0.00004775
Iteration 518/1000 | Loss: 0.00004775
Iteration 519/1000 | Loss: 0.00004774
Iteration 520/1000 | Loss: 0.00004774
Iteration 521/1000 | Loss: 0.00004774
Iteration 522/1000 | Loss: 0.00004774
Iteration 523/1000 | Loss: 0.00004774
Iteration 524/1000 | Loss: 0.00004774
Iteration 525/1000 | Loss: 0.00004774
Iteration 526/1000 | Loss: 0.00004774
Iteration 527/1000 | Loss: 0.00004774
Iteration 528/1000 | Loss: 0.00004774
Iteration 529/1000 | Loss: 0.00004774
Iteration 530/1000 | Loss: 0.00004774
Iteration 531/1000 | Loss: 0.00004774
Iteration 532/1000 | Loss: 0.00004774
Iteration 533/1000 | Loss: 0.00004774
Iteration 534/1000 | Loss: 0.00004774
Iteration 535/1000 | Loss: 0.00004774
Iteration 536/1000 | Loss: 0.00004774
Iteration 537/1000 | Loss: 0.00004774
Iteration 538/1000 | Loss: 0.00004774
Iteration 539/1000 | Loss: 0.00004774
Iteration 540/1000 | Loss: 0.00004774
Iteration 541/1000 | Loss: 0.00004774
Iteration 542/1000 | Loss: 0.00004774
Iteration 543/1000 | Loss: 0.00004774
Iteration 544/1000 | Loss: 0.00004774
Iteration 545/1000 | Loss: 0.00004774
Iteration 546/1000 | Loss: 0.00004774
Iteration 547/1000 | Loss: 0.00004774
Iteration 548/1000 | Loss: 0.00004774
Iteration 549/1000 | Loss: 0.00004774
Iteration 550/1000 | Loss: 0.00004774
Iteration 551/1000 | Loss: 0.00004774
Iteration 552/1000 | Loss: 0.00004774
Iteration 553/1000 | Loss: 0.00004774
Iteration 554/1000 | Loss: 0.00004774
Iteration 555/1000 | Loss: 0.00004774
Iteration 556/1000 | Loss: 0.00004774
Iteration 557/1000 | Loss: 0.00004774
Iteration 558/1000 | Loss: 0.00004774
Iteration 559/1000 | Loss: 0.00004774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 559. Stopping optimization.
Last 5 losses: [4.774372064275667e-05, 4.774372064275667e-05, 4.774372064275667e-05, 4.774372064275667e-05, 4.774372064275667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.774372064275667e-05

Optimization complete. Final v2v error: 5.59581184387207 mm

Highest mean error: 13.08249568939209 mm for frame 54

Lowest mean error: 4.184006690979004 mm for frame 0

Saving results

Total time: 663.7638988494873
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00538654
Iteration 2/25 | Loss: 0.00129681
Iteration 3/25 | Loss: 0.00117276
Iteration 4/25 | Loss: 0.00116354
Iteration 5/25 | Loss: 0.00116135
Iteration 6/25 | Loss: 0.00116066
Iteration 7/25 | Loss: 0.00116066
Iteration 8/25 | Loss: 0.00116066
Iteration 9/25 | Loss: 0.00116066
Iteration 10/25 | Loss: 0.00116066
Iteration 11/25 | Loss: 0.00116066
Iteration 12/25 | Loss: 0.00116066
Iteration 13/25 | Loss: 0.00116066
Iteration 14/25 | Loss: 0.00116066
Iteration 15/25 | Loss: 0.00116066
Iteration 16/25 | Loss: 0.00116066
Iteration 17/25 | Loss: 0.00116066
Iteration 18/25 | Loss: 0.00116066
Iteration 19/25 | Loss: 0.00116066
Iteration 20/25 | Loss: 0.00116066
Iteration 21/25 | Loss: 0.00116066
Iteration 22/25 | Loss: 0.00116066
Iteration 23/25 | Loss: 0.00116066
Iteration 24/25 | Loss: 0.00116066
Iteration 25/25 | Loss: 0.00116066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75887275
Iteration 2/25 | Loss: 0.00077248
Iteration 3/25 | Loss: 0.00077247
Iteration 4/25 | Loss: 0.00077247
Iteration 5/25 | Loss: 0.00077247
Iteration 6/25 | Loss: 0.00077247
Iteration 7/25 | Loss: 0.00077247
Iteration 8/25 | Loss: 0.00077247
Iteration 9/25 | Loss: 0.00077247
Iteration 10/25 | Loss: 0.00077247
Iteration 11/25 | Loss: 0.00077247
Iteration 12/25 | Loss: 0.00077247
Iteration 13/25 | Loss: 0.00077247
Iteration 14/25 | Loss: 0.00077247
Iteration 15/25 | Loss: 0.00077247
Iteration 16/25 | Loss: 0.00077247
Iteration 17/25 | Loss: 0.00077247
Iteration 18/25 | Loss: 0.00077247
Iteration 19/25 | Loss: 0.00077247
Iteration 20/25 | Loss: 0.00077247
Iteration 21/25 | Loss: 0.00077247
Iteration 22/25 | Loss: 0.00077247
Iteration 23/25 | Loss: 0.00077247
Iteration 24/25 | Loss: 0.00077247
Iteration 25/25 | Loss: 0.00077247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077247
Iteration 2/1000 | Loss: 0.00003562
Iteration 3/1000 | Loss: 0.00002459
Iteration 4/1000 | Loss: 0.00002148
Iteration 5/1000 | Loss: 0.00002035
Iteration 6/1000 | Loss: 0.00001978
Iteration 7/1000 | Loss: 0.00001931
Iteration 8/1000 | Loss: 0.00001888
Iteration 9/1000 | Loss: 0.00001867
Iteration 10/1000 | Loss: 0.00001841
Iteration 11/1000 | Loss: 0.00001816
Iteration 12/1000 | Loss: 0.00001793
Iteration 13/1000 | Loss: 0.00001768
Iteration 14/1000 | Loss: 0.00001746
Iteration 15/1000 | Loss: 0.00001731
Iteration 16/1000 | Loss: 0.00001715
Iteration 17/1000 | Loss: 0.00001695
Iteration 18/1000 | Loss: 0.00001690
Iteration 19/1000 | Loss: 0.00001688
Iteration 20/1000 | Loss: 0.00001682
Iteration 21/1000 | Loss: 0.00001675
Iteration 22/1000 | Loss: 0.00001670
Iteration 23/1000 | Loss: 0.00001670
Iteration 24/1000 | Loss: 0.00001664
Iteration 25/1000 | Loss: 0.00001660
Iteration 26/1000 | Loss: 0.00001659
Iteration 27/1000 | Loss: 0.00001658
Iteration 28/1000 | Loss: 0.00001657
Iteration 29/1000 | Loss: 0.00001657
Iteration 30/1000 | Loss: 0.00001657
Iteration 31/1000 | Loss: 0.00001657
Iteration 32/1000 | Loss: 0.00001657
Iteration 33/1000 | Loss: 0.00001657
Iteration 34/1000 | Loss: 0.00001656
Iteration 35/1000 | Loss: 0.00001656
Iteration 36/1000 | Loss: 0.00001656
Iteration 37/1000 | Loss: 0.00001656
Iteration 38/1000 | Loss: 0.00001656
Iteration 39/1000 | Loss: 0.00001656
Iteration 40/1000 | Loss: 0.00001655
Iteration 41/1000 | Loss: 0.00001652
Iteration 42/1000 | Loss: 0.00001652
Iteration 43/1000 | Loss: 0.00001652
Iteration 44/1000 | Loss: 0.00001651
Iteration 45/1000 | Loss: 0.00001651
Iteration 46/1000 | Loss: 0.00001651
Iteration 47/1000 | Loss: 0.00001651
Iteration 48/1000 | Loss: 0.00001650
Iteration 49/1000 | Loss: 0.00001649
Iteration 50/1000 | Loss: 0.00001649
Iteration 51/1000 | Loss: 0.00001647
Iteration 52/1000 | Loss: 0.00001647
Iteration 53/1000 | Loss: 0.00001647
Iteration 54/1000 | Loss: 0.00001646
Iteration 55/1000 | Loss: 0.00001646
Iteration 56/1000 | Loss: 0.00001646
Iteration 57/1000 | Loss: 0.00001645
Iteration 58/1000 | Loss: 0.00001645
Iteration 59/1000 | Loss: 0.00001644
Iteration 60/1000 | Loss: 0.00001643
Iteration 61/1000 | Loss: 0.00001643
Iteration 62/1000 | Loss: 0.00001643
Iteration 63/1000 | Loss: 0.00001643
Iteration 64/1000 | Loss: 0.00001643
Iteration 65/1000 | Loss: 0.00001643
Iteration 66/1000 | Loss: 0.00001643
Iteration 67/1000 | Loss: 0.00001643
Iteration 68/1000 | Loss: 0.00001643
Iteration 69/1000 | Loss: 0.00001642
Iteration 70/1000 | Loss: 0.00001642
Iteration 71/1000 | Loss: 0.00001642
Iteration 72/1000 | Loss: 0.00001642
Iteration 73/1000 | Loss: 0.00001641
Iteration 74/1000 | Loss: 0.00001640
Iteration 75/1000 | Loss: 0.00001639
Iteration 76/1000 | Loss: 0.00001639
Iteration 77/1000 | Loss: 0.00001639
Iteration 78/1000 | Loss: 0.00001639
Iteration 79/1000 | Loss: 0.00001639
Iteration 80/1000 | Loss: 0.00001639
Iteration 81/1000 | Loss: 0.00001639
Iteration 82/1000 | Loss: 0.00001639
Iteration 83/1000 | Loss: 0.00001639
Iteration 84/1000 | Loss: 0.00001639
Iteration 85/1000 | Loss: 0.00001639
Iteration 86/1000 | Loss: 0.00001639
Iteration 87/1000 | Loss: 0.00001639
Iteration 88/1000 | Loss: 0.00001639
Iteration 89/1000 | Loss: 0.00001639
Iteration 90/1000 | Loss: 0.00001639
Iteration 91/1000 | Loss: 0.00001639
Iteration 92/1000 | Loss: 0.00001639
Iteration 93/1000 | Loss: 0.00001638
Iteration 94/1000 | Loss: 0.00001638
Iteration 95/1000 | Loss: 0.00001638
Iteration 96/1000 | Loss: 0.00001638
Iteration 97/1000 | Loss: 0.00001638
Iteration 98/1000 | Loss: 0.00001638
Iteration 99/1000 | Loss: 0.00001638
Iteration 100/1000 | Loss: 0.00001638
Iteration 101/1000 | Loss: 0.00001638
Iteration 102/1000 | Loss: 0.00001638
Iteration 103/1000 | Loss: 0.00001638
Iteration 104/1000 | Loss: 0.00001638
Iteration 105/1000 | Loss: 0.00001638
Iteration 106/1000 | Loss: 0.00001638
Iteration 107/1000 | Loss: 0.00001638
Iteration 108/1000 | Loss: 0.00001638
Iteration 109/1000 | Loss: 0.00001638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.6383246475015767e-05, 1.6383246475015767e-05, 1.6383246475015767e-05, 1.6383246475015767e-05, 1.6383246475015767e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6383246475015767e-05

Optimization complete. Final v2v error: 3.3431575298309326 mm

Highest mean error: 3.7983803749084473 mm for frame 9

Lowest mean error: 3.0911483764648438 mm for frame 45

Saving results

Total time: 44.03817892074585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423104
Iteration 2/25 | Loss: 0.00114556
Iteration 3/25 | Loss: 0.00107560
Iteration 4/25 | Loss: 0.00106554
Iteration 5/25 | Loss: 0.00106204
Iteration 6/25 | Loss: 0.00106119
Iteration 7/25 | Loss: 0.00106119
Iteration 8/25 | Loss: 0.00106119
Iteration 9/25 | Loss: 0.00106119
Iteration 10/25 | Loss: 0.00106119
Iteration 11/25 | Loss: 0.00106119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010611868929117918, 0.0010611868929117918, 0.0010611868929117918, 0.0010611868929117918, 0.0010611868929117918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010611868929117918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.80111766
Iteration 2/25 | Loss: 0.00083936
Iteration 3/25 | Loss: 0.00083934
Iteration 4/25 | Loss: 0.00083934
Iteration 5/25 | Loss: 0.00083934
Iteration 6/25 | Loss: 0.00083934
Iteration 7/25 | Loss: 0.00083934
Iteration 8/25 | Loss: 0.00083934
Iteration 9/25 | Loss: 0.00083934
Iteration 10/25 | Loss: 0.00083934
Iteration 11/25 | Loss: 0.00083934
Iteration 12/25 | Loss: 0.00083934
Iteration 13/25 | Loss: 0.00083934
Iteration 14/25 | Loss: 0.00083934
Iteration 15/25 | Loss: 0.00083934
Iteration 16/25 | Loss: 0.00083934
Iteration 17/25 | Loss: 0.00083933
Iteration 18/25 | Loss: 0.00083934
Iteration 19/25 | Loss: 0.00083934
Iteration 20/25 | Loss: 0.00083933
Iteration 21/25 | Loss: 0.00083933
Iteration 22/25 | Loss: 0.00083933
Iteration 23/25 | Loss: 0.00083933
Iteration 24/25 | Loss: 0.00083933
Iteration 25/25 | Loss: 0.00083933
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000839334970805794, 0.000839334970805794, 0.000839334970805794, 0.000839334970805794, 0.000839334970805794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000839334970805794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083933
Iteration 2/1000 | Loss: 0.00002894
Iteration 3/1000 | Loss: 0.00001745
Iteration 4/1000 | Loss: 0.00001476
Iteration 5/1000 | Loss: 0.00001357
Iteration 6/1000 | Loss: 0.00001283
Iteration 7/1000 | Loss: 0.00001238
Iteration 8/1000 | Loss: 0.00001195
Iteration 9/1000 | Loss: 0.00001187
Iteration 10/1000 | Loss: 0.00001163
Iteration 11/1000 | Loss: 0.00001137
Iteration 12/1000 | Loss: 0.00001118
Iteration 13/1000 | Loss: 0.00001110
Iteration 14/1000 | Loss: 0.00001108
Iteration 15/1000 | Loss: 0.00001107
Iteration 16/1000 | Loss: 0.00001103
Iteration 17/1000 | Loss: 0.00001103
Iteration 18/1000 | Loss: 0.00001099
Iteration 19/1000 | Loss: 0.00001095
Iteration 20/1000 | Loss: 0.00001094
Iteration 21/1000 | Loss: 0.00001093
Iteration 22/1000 | Loss: 0.00001088
Iteration 23/1000 | Loss: 0.00001087
Iteration 24/1000 | Loss: 0.00001086
Iteration 25/1000 | Loss: 0.00001085
Iteration 26/1000 | Loss: 0.00001084
Iteration 27/1000 | Loss: 0.00001084
Iteration 28/1000 | Loss: 0.00001083
Iteration 29/1000 | Loss: 0.00001083
Iteration 30/1000 | Loss: 0.00001082
Iteration 31/1000 | Loss: 0.00001082
Iteration 32/1000 | Loss: 0.00001081
Iteration 33/1000 | Loss: 0.00001081
Iteration 34/1000 | Loss: 0.00001081
Iteration 35/1000 | Loss: 0.00001081
Iteration 36/1000 | Loss: 0.00001081
Iteration 37/1000 | Loss: 0.00001081
Iteration 38/1000 | Loss: 0.00001080
Iteration 39/1000 | Loss: 0.00001079
Iteration 40/1000 | Loss: 0.00001079
Iteration 41/1000 | Loss: 0.00001079
Iteration 42/1000 | Loss: 0.00001079
Iteration 43/1000 | Loss: 0.00001079
Iteration 44/1000 | Loss: 0.00001078
Iteration 45/1000 | Loss: 0.00001078
Iteration 46/1000 | Loss: 0.00001078
Iteration 47/1000 | Loss: 0.00001077
Iteration 48/1000 | Loss: 0.00001077
Iteration 49/1000 | Loss: 0.00001076
Iteration 50/1000 | Loss: 0.00001075
Iteration 51/1000 | Loss: 0.00001075
Iteration 52/1000 | Loss: 0.00001074
Iteration 53/1000 | Loss: 0.00001074
Iteration 54/1000 | Loss: 0.00001073
Iteration 55/1000 | Loss: 0.00001073
Iteration 56/1000 | Loss: 0.00001073
Iteration 57/1000 | Loss: 0.00001073
Iteration 58/1000 | Loss: 0.00001073
Iteration 59/1000 | Loss: 0.00001073
Iteration 60/1000 | Loss: 0.00001072
Iteration 61/1000 | Loss: 0.00001072
Iteration 62/1000 | Loss: 0.00001071
Iteration 63/1000 | Loss: 0.00001071
Iteration 64/1000 | Loss: 0.00001071
Iteration 65/1000 | Loss: 0.00001070
Iteration 66/1000 | Loss: 0.00001070
Iteration 67/1000 | Loss: 0.00001069
Iteration 68/1000 | Loss: 0.00001069
Iteration 69/1000 | Loss: 0.00001069
Iteration 70/1000 | Loss: 0.00001068
Iteration 71/1000 | Loss: 0.00001068
Iteration 72/1000 | Loss: 0.00001068
Iteration 73/1000 | Loss: 0.00001068
Iteration 74/1000 | Loss: 0.00001067
Iteration 75/1000 | Loss: 0.00001067
Iteration 76/1000 | Loss: 0.00001067
Iteration 77/1000 | Loss: 0.00001067
Iteration 78/1000 | Loss: 0.00001067
Iteration 79/1000 | Loss: 0.00001066
Iteration 80/1000 | Loss: 0.00001066
Iteration 81/1000 | Loss: 0.00001066
Iteration 82/1000 | Loss: 0.00001066
Iteration 83/1000 | Loss: 0.00001066
Iteration 84/1000 | Loss: 0.00001065
Iteration 85/1000 | Loss: 0.00001065
Iteration 86/1000 | Loss: 0.00001065
Iteration 87/1000 | Loss: 0.00001065
Iteration 88/1000 | Loss: 0.00001064
Iteration 89/1000 | Loss: 0.00001064
Iteration 90/1000 | Loss: 0.00001064
Iteration 91/1000 | Loss: 0.00001064
Iteration 92/1000 | Loss: 0.00001063
Iteration 93/1000 | Loss: 0.00001063
Iteration 94/1000 | Loss: 0.00001063
Iteration 95/1000 | Loss: 0.00001063
Iteration 96/1000 | Loss: 0.00001063
Iteration 97/1000 | Loss: 0.00001063
Iteration 98/1000 | Loss: 0.00001062
Iteration 99/1000 | Loss: 0.00001062
Iteration 100/1000 | Loss: 0.00001062
Iteration 101/1000 | Loss: 0.00001061
Iteration 102/1000 | Loss: 0.00001061
Iteration 103/1000 | Loss: 0.00001061
Iteration 104/1000 | Loss: 0.00001061
Iteration 105/1000 | Loss: 0.00001061
Iteration 106/1000 | Loss: 0.00001061
Iteration 107/1000 | Loss: 0.00001061
Iteration 108/1000 | Loss: 0.00001061
Iteration 109/1000 | Loss: 0.00001061
Iteration 110/1000 | Loss: 0.00001061
Iteration 111/1000 | Loss: 0.00001061
Iteration 112/1000 | Loss: 0.00001061
Iteration 113/1000 | Loss: 0.00001061
Iteration 114/1000 | Loss: 0.00001060
Iteration 115/1000 | Loss: 0.00001060
Iteration 116/1000 | Loss: 0.00001060
Iteration 117/1000 | Loss: 0.00001060
Iteration 118/1000 | Loss: 0.00001060
Iteration 119/1000 | Loss: 0.00001059
Iteration 120/1000 | Loss: 0.00001059
Iteration 121/1000 | Loss: 0.00001059
Iteration 122/1000 | Loss: 0.00001059
Iteration 123/1000 | Loss: 0.00001059
Iteration 124/1000 | Loss: 0.00001058
Iteration 125/1000 | Loss: 0.00001058
Iteration 126/1000 | Loss: 0.00001058
Iteration 127/1000 | Loss: 0.00001058
Iteration 128/1000 | Loss: 0.00001058
Iteration 129/1000 | Loss: 0.00001057
Iteration 130/1000 | Loss: 0.00001057
Iteration 131/1000 | Loss: 0.00001057
Iteration 132/1000 | Loss: 0.00001057
Iteration 133/1000 | Loss: 0.00001057
Iteration 134/1000 | Loss: 0.00001056
Iteration 135/1000 | Loss: 0.00001056
Iteration 136/1000 | Loss: 0.00001056
Iteration 137/1000 | Loss: 0.00001056
Iteration 138/1000 | Loss: 0.00001056
Iteration 139/1000 | Loss: 0.00001056
Iteration 140/1000 | Loss: 0.00001055
Iteration 141/1000 | Loss: 0.00001055
Iteration 142/1000 | Loss: 0.00001055
Iteration 143/1000 | Loss: 0.00001055
Iteration 144/1000 | Loss: 0.00001055
Iteration 145/1000 | Loss: 0.00001055
Iteration 146/1000 | Loss: 0.00001054
Iteration 147/1000 | Loss: 0.00001054
Iteration 148/1000 | Loss: 0.00001054
Iteration 149/1000 | Loss: 0.00001054
Iteration 150/1000 | Loss: 0.00001054
Iteration 151/1000 | Loss: 0.00001054
Iteration 152/1000 | Loss: 0.00001054
Iteration 153/1000 | Loss: 0.00001054
Iteration 154/1000 | Loss: 0.00001054
Iteration 155/1000 | Loss: 0.00001054
Iteration 156/1000 | Loss: 0.00001054
Iteration 157/1000 | Loss: 0.00001054
Iteration 158/1000 | Loss: 0.00001054
Iteration 159/1000 | Loss: 0.00001054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.053662799677113e-05, 1.053662799677113e-05, 1.053662799677113e-05, 1.053662799677113e-05, 1.053662799677113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.053662799677113e-05

Optimization complete. Final v2v error: 2.7627415657043457 mm

Highest mean error: 3.1685078144073486 mm for frame 82

Lowest mean error: 2.3480982780456543 mm for frame 7

Saving results

Total time: 38.886194467544556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403481
Iteration 2/25 | Loss: 0.00120680
Iteration 3/25 | Loss: 0.00109749
Iteration 4/25 | Loss: 0.00108173
Iteration 5/25 | Loss: 0.00107691
Iteration 6/25 | Loss: 0.00107541
Iteration 7/25 | Loss: 0.00107530
Iteration 8/25 | Loss: 0.00107530
Iteration 9/25 | Loss: 0.00107530
Iteration 10/25 | Loss: 0.00107530
Iteration 11/25 | Loss: 0.00107530
Iteration 12/25 | Loss: 0.00107530
Iteration 13/25 | Loss: 0.00107530
Iteration 14/25 | Loss: 0.00107530
Iteration 15/25 | Loss: 0.00107530
Iteration 16/25 | Loss: 0.00107530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010753021342679858, 0.0010753021342679858, 0.0010753021342679858, 0.0010753021342679858, 0.0010753021342679858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010753021342679858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35010982
Iteration 2/25 | Loss: 0.00090037
Iteration 3/25 | Loss: 0.00090034
Iteration 4/25 | Loss: 0.00090034
Iteration 5/25 | Loss: 0.00090034
Iteration 6/25 | Loss: 0.00090034
Iteration 7/25 | Loss: 0.00090034
Iteration 8/25 | Loss: 0.00090034
Iteration 9/25 | Loss: 0.00090034
Iteration 10/25 | Loss: 0.00090034
Iteration 11/25 | Loss: 0.00090034
Iteration 12/25 | Loss: 0.00090034
Iteration 13/25 | Loss: 0.00090034
Iteration 14/25 | Loss: 0.00090034
Iteration 15/25 | Loss: 0.00090034
Iteration 16/25 | Loss: 0.00090034
Iteration 17/25 | Loss: 0.00090034
Iteration 18/25 | Loss: 0.00090034
Iteration 19/25 | Loss: 0.00090034
Iteration 20/25 | Loss: 0.00090034
Iteration 21/25 | Loss: 0.00090034
Iteration 22/25 | Loss: 0.00090034
Iteration 23/25 | Loss: 0.00090034
Iteration 24/25 | Loss: 0.00090034
Iteration 25/25 | Loss: 0.00090034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090034
Iteration 2/1000 | Loss: 0.00004402
Iteration 3/1000 | Loss: 0.00002740
Iteration 4/1000 | Loss: 0.00001927
Iteration 5/1000 | Loss: 0.00001729
Iteration 6/1000 | Loss: 0.00001589
Iteration 7/1000 | Loss: 0.00001487
Iteration 8/1000 | Loss: 0.00001436
Iteration 9/1000 | Loss: 0.00001393
Iteration 10/1000 | Loss: 0.00001367
Iteration 11/1000 | Loss: 0.00001344
Iteration 12/1000 | Loss: 0.00001324
Iteration 13/1000 | Loss: 0.00001309
Iteration 14/1000 | Loss: 0.00001307
Iteration 15/1000 | Loss: 0.00001305
Iteration 16/1000 | Loss: 0.00001299
Iteration 17/1000 | Loss: 0.00001299
Iteration 18/1000 | Loss: 0.00001298
Iteration 19/1000 | Loss: 0.00001298
Iteration 20/1000 | Loss: 0.00001297
Iteration 21/1000 | Loss: 0.00001297
Iteration 22/1000 | Loss: 0.00001296
Iteration 23/1000 | Loss: 0.00001296
Iteration 24/1000 | Loss: 0.00001295
Iteration 25/1000 | Loss: 0.00001295
Iteration 26/1000 | Loss: 0.00001294
Iteration 27/1000 | Loss: 0.00001293
Iteration 28/1000 | Loss: 0.00001293
Iteration 29/1000 | Loss: 0.00001293
Iteration 30/1000 | Loss: 0.00001293
Iteration 31/1000 | Loss: 0.00001293
Iteration 32/1000 | Loss: 0.00001292
Iteration 33/1000 | Loss: 0.00001292
Iteration 34/1000 | Loss: 0.00001292
Iteration 35/1000 | Loss: 0.00001292
Iteration 36/1000 | Loss: 0.00001292
Iteration 37/1000 | Loss: 0.00001291
Iteration 38/1000 | Loss: 0.00001291
Iteration 39/1000 | Loss: 0.00001291
Iteration 40/1000 | Loss: 0.00001291
Iteration 41/1000 | Loss: 0.00001291
Iteration 42/1000 | Loss: 0.00001290
Iteration 43/1000 | Loss: 0.00001290
Iteration 44/1000 | Loss: 0.00001290
Iteration 45/1000 | Loss: 0.00001289
Iteration 46/1000 | Loss: 0.00001289
Iteration 47/1000 | Loss: 0.00001289
Iteration 48/1000 | Loss: 0.00001288
Iteration 49/1000 | Loss: 0.00001288
Iteration 50/1000 | Loss: 0.00001287
Iteration 51/1000 | Loss: 0.00001287
Iteration 52/1000 | Loss: 0.00001287
Iteration 53/1000 | Loss: 0.00001287
Iteration 54/1000 | Loss: 0.00001286
Iteration 55/1000 | Loss: 0.00001286
Iteration 56/1000 | Loss: 0.00001286
Iteration 57/1000 | Loss: 0.00001285
Iteration 58/1000 | Loss: 0.00001285
Iteration 59/1000 | Loss: 0.00001284
Iteration 60/1000 | Loss: 0.00001284
Iteration 61/1000 | Loss: 0.00001283
Iteration 62/1000 | Loss: 0.00001282
Iteration 63/1000 | Loss: 0.00001281
Iteration 64/1000 | Loss: 0.00001281
Iteration 65/1000 | Loss: 0.00001280
Iteration 66/1000 | Loss: 0.00001280
Iteration 67/1000 | Loss: 0.00001280
Iteration 68/1000 | Loss: 0.00001279
Iteration 69/1000 | Loss: 0.00001279
Iteration 70/1000 | Loss: 0.00001279
Iteration 71/1000 | Loss: 0.00001275
Iteration 72/1000 | Loss: 0.00001275
Iteration 73/1000 | Loss: 0.00001274
Iteration 74/1000 | Loss: 0.00001274
Iteration 75/1000 | Loss: 0.00001273
Iteration 76/1000 | Loss: 0.00001273
Iteration 77/1000 | Loss: 0.00001273
Iteration 78/1000 | Loss: 0.00001273
Iteration 79/1000 | Loss: 0.00001272
Iteration 80/1000 | Loss: 0.00001272
Iteration 81/1000 | Loss: 0.00001272
Iteration 82/1000 | Loss: 0.00001272
Iteration 83/1000 | Loss: 0.00001272
Iteration 84/1000 | Loss: 0.00001272
Iteration 85/1000 | Loss: 0.00001271
Iteration 86/1000 | Loss: 0.00001271
Iteration 87/1000 | Loss: 0.00001271
Iteration 88/1000 | Loss: 0.00001271
Iteration 89/1000 | Loss: 0.00001271
Iteration 90/1000 | Loss: 0.00001271
Iteration 91/1000 | Loss: 0.00001270
Iteration 92/1000 | Loss: 0.00001270
Iteration 93/1000 | Loss: 0.00001270
Iteration 94/1000 | Loss: 0.00001269
Iteration 95/1000 | Loss: 0.00001269
Iteration 96/1000 | Loss: 0.00001269
Iteration 97/1000 | Loss: 0.00001268
Iteration 98/1000 | Loss: 0.00001268
Iteration 99/1000 | Loss: 0.00001267
Iteration 100/1000 | Loss: 0.00001267
Iteration 101/1000 | Loss: 0.00001267
Iteration 102/1000 | Loss: 0.00001267
Iteration 103/1000 | Loss: 0.00001267
Iteration 104/1000 | Loss: 0.00001267
Iteration 105/1000 | Loss: 0.00001267
Iteration 106/1000 | Loss: 0.00001267
Iteration 107/1000 | Loss: 0.00001266
Iteration 108/1000 | Loss: 0.00001266
Iteration 109/1000 | Loss: 0.00001265
Iteration 110/1000 | Loss: 0.00001264
Iteration 111/1000 | Loss: 0.00001264
Iteration 112/1000 | Loss: 0.00001264
Iteration 113/1000 | Loss: 0.00001264
Iteration 114/1000 | Loss: 0.00001263
Iteration 115/1000 | Loss: 0.00001263
Iteration 116/1000 | Loss: 0.00001263
Iteration 117/1000 | Loss: 0.00001263
Iteration 118/1000 | Loss: 0.00001263
Iteration 119/1000 | Loss: 0.00001263
Iteration 120/1000 | Loss: 0.00001263
Iteration 121/1000 | Loss: 0.00001263
Iteration 122/1000 | Loss: 0.00001263
Iteration 123/1000 | Loss: 0.00001263
Iteration 124/1000 | Loss: 0.00001263
Iteration 125/1000 | Loss: 0.00001263
Iteration 126/1000 | Loss: 0.00001262
Iteration 127/1000 | Loss: 0.00001262
Iteration 128/1000 | Loss: 0.00001262
Iteration 129/1000 | Loss: 0.00001262
Iteration 130/1000 | Loss: 0.00001262
Iteration 131/1000 | Loss: 0.00001261
Iteration 132/1000 | Loss: 0.00001261
Iteration 133/1000 | Loss: 0.00001261
Iteration 134/1000 | Loss: 0.00001261
Iteration 135/1000 | Loss: 0.00001261
Iteration 136/1000 | Loss: 0.00001261
Iteration 137/1000 | Loss: 0.00001261
Iteration 138/1000 | Loss: 0.00001260
Iteration 139/1000 | Loss: 0.00001260
Iteration 140/1000 | Loss: 0.00001260
Iteration 141/1000 | Loss: 0.00001260
Iteration 142/1000 | Loss: 0.00001259
Iteration 143/1000 | Loss: 0.00001259
Iteration 144/1000 | Loss: 0.00001259
Iteration 145/1000 | Loss: 0.00001259
Iteration 146/1000 | Loss: 0.00001259
Iteration 147/1000 | Loss: 0.00001258
Iteration 148/1000 | Loss: 0.00001258
Iteration 149/1000 | Loss: 0.00001258
Iteration 150/1000 | Loss: 0.00001258
Iteration 151/1000 | Loss: 0.00001258
Iteration 152/1000 | Loss: 0.00001257
Iteration 153/1000 | Loss: 0.00001257
Iteration 154/1000 | Loss: 0.00001257
Iteration 155/1000 | Loss: 0.00001257
Iteration 156/1000 | Loss: 0.00001256
Iteration 157/1000 | Loss: 0.00001256
Iteration 158/1000 | Loss: 0.00001256
Iteration 159/1000 | Loss: 0.00001256
Iteration 160/1000 | Loss: 0.00001256
Iteration 161/1000 | Loss: 0.00001256
Iteration 162/1000 | Loss: 0.00001256
Iteration 163/1000 | Loss: 0.00001256
Iteration 164/1000 | Loss: 0.00001256
Iteration 165/1000 | Loss: 0.00001256
Iteration 166/1000 | Loss: 0.00001255
Iteration 167/1000 | Loss: 0.00001255
Iteration 168/1000 | Loss: 0.00001255
Iteration 169/1000 | Loss: 0.00001255
Iteration 170/1000 | Loss: 0.00001255
Iteration 171/1000 | Loss: 0.00001255
Iteration 172/1000 | Loss: 0.00001255
Iteration 173/1000 | Loss: 0.00001255
Iteration 174/1000 | Loss: 0.00001255
Iteration 175/1000 | Loss: 0.00001255
Iteration 176/1000 | Loss: 0.00001255
Iteration 177/1000 | Loss: 0.00001255
Iteration 178/1000 | Loss: 0.00001255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.255394090549089e-05, 1.255394090549089e-05, 1.255394090549089e-05, 1.255394090549089e-05, 1.255394090549089e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.255394090549089e-05

Optimization complete. Final v2v error: 2.881608724594116 mm

Highest mean error: 4.843748092651367 mm for frame 83

Lowest mean error: 2.387915849685669 mm for frame 176

Saving results

Total time: 41.456220865249634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00356159
Iteration 2/25 | Loss: 0.00112191
Iteration 3/25 | Loss: 0.00105498
Iteration 4/25 | Loss: 0.00104570
Iteration 5/25 | Loss: 0.00104232
Iteration 6/25 | Loss: 0.00104196
Iteration 7/25 | Loss: 0.00104196
Iteration 8/25 | Loss: 0.00104196
Iteration 9/25 | Loss: 0.00104196
Iteration 10/25 | Loss: 0.00104196
Iteration 11/25 | Loss: 0.00104196
Iteration 12/25 | Loss: 0.00104196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010419633472338319, 0.0010419633472338319, 0.0010419633472338319, 0.0010419633472338319, 0.0010419633472338319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010419633472338319

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61158061
Iteration 2/25 | Loss: 0.00083033
Iteration 3/25 | Loss: 0.00083033
Iteration 4/25 | Loss: 0.00083032
Iteration 5/25 | Loss: 0.00083032
Iteration 6/25 | Loss: 0.00083032
Iteration 7/25 | Loss: 0.00083032
Iteration 8/25 | Loss: 0.00083032
Iteration 9/25 | Loss: 0.00083032
Iteration 10/25 | Loss: 0.00083032
Iteration 11/25 | Loss: 0.00083032
Iteration 12/25 | Loss: 0.00083032
Iteration 13/25 | Loss: 0.00083032
Iteration 14/25 | Loss: 0.00083032
Iteration 15/25 | Loss: 0.00083032
Iteration 16/25 | Loss: 0.00083032
Iteration 17/25 | Loss: 0.00083032
Iteration 18/25 | Loss: 0.00083032
Iteration 19/25 | Loss: 0.00083032
Iteration 20/25 | Loss: 0.00083032
Iteration 21/25 | Loss: 0.00083032
Iteration 22/25 | Loss: 0.00083032
Iteration 23/25 | Loss: 0.00083032
Iteration 24/25 | Loss: 0.00083032
Iteration 25/25 | Loss: 0.00083032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083032
Iteration 2/1000 | Loss: 0.00001320
Iteration 3/1000 | Loss: 0.00001031
Iteration 4/1000 | Loss: 0.00000966
Iteration 5/1000 | Loss: 0.00000919
Iteration 6/1000 | Loss: 0.00000891
Iteration 7/1000 | Loss: 0.00000879
Iteration 8/1000 | Loss: 0.00000879
Iteration 9/1000 | Loss: 0.00000878
Iteration 10/1000 | Loss: 0.00000849
Iteration 11/1000 | Loss: 0.00000834
Iteration 12/1000 | Loss: 0.00000826
Iteration 13/1000 | Loss: 0.00000824
Iteration 14/1000 | Loss: 0.00000820
Iteration 15/1000 | Loss: 0.00000819
Iteration 16/1000 | Loss: 0.00000816
Iteration 17/1000 | Loss: 0.00000816
Iteration 18/1000 | Loss: 0.00000815
Iteration 19/1000 | Loss: 0.00000814
Iteration 20/1000 | Loss: 0.00000814
Iteration 21/1000 | Loss: 0.00000813
Iteration 22/1000 | Loss: 0.00000813
Iteration 23/1000 | Loss: 0.00000813
Iteration 24/1000 | Loss: 0.00000812
Iteration 25/1000 | Loss: 0.00000812
Iteration 26/1000 | Loss: 0.00000811
Iteration 27/1000 | Loss: 0.00000810
Iteration 28/1000 | Loss: 0.00000810
Iteration 29/1000 | Loss: 0.00000809
Iteration 30/1000 | Loss: 0.00000807
Iteration 31/1000 | Loss: 0.00000806
Iteration 32/1000 | Loss: 0.00000806
Iteration 33/1000 | Loss: 0.00000804
Iteration 34/1000 | Loss: 0.00000804
Iteration 35/1000 | Loss: 0.00000804
Iteration 36/1000 | Loss: 0.00000804
Iteration 37/1000 | Loss: 0.00000803
Iteration 38/1000 | Loss: 0.00000803
Iteration 39/1000 | Loss: 0.00000803
Iteration 40/1000 | Loss: 0.00000803
Iteration 41/1000 | Loss: 0.00000803
Iteration 42/1000 | Loss: 0.00000802
Iteration 43/1000 | Loss: 0.00000801
Iteration 44/1000 | Loss: 0.00000801
Iteration 45/1000 | Loss: 0.00000801
Iteration 46/1000 | Loss: 0.00000801
Iteration 47/1000 | Loss: 0.00000801
Iteration 48/1000 | Loss: 0.00000801
Iteration 49/1000 | Loss: 0.00000801
Iteration 50/1000 | Loss: 0.00000801
Iteration 51/1000 | Loss: 0.00000801
Iteration 52/1000 | Loss: 0.00000801
Iteration 53/1000 | Loss: 0.00000800
Iteration 54/1000 | Loss: 0.00000800
Iteration 55/1000 | Loss: 0.00000800
Iteration 56/1000 | Loss: 0.00000799
Iteration 57/1000 | Loss: 0.00000799
Iteration 58/1000 | Loss: 0.00000799
Iteration 59/1000 | Loss: 0.00000799
Iteration 60/1000 | Loss: 0.00000799
Iteration 61/1000 | Loss: 0.00000798
Iteration 62/1000 | Loss: 0.00000797
Iteration 63/1000 | Loss: 0.00000797
Iteration 64/1000 | Loss: 0.00000796
Iteration 65/1000 | Loss: 0.00000795
Iteration 66/1000 | Loss: 0.00000795
Iteration 67/1000 | Loss: 0.00000795
Iteration 68/1000 | Loss: 0.00000795
Iteration 69/1000 | Loss: 0.00000794
Iteration 70/1000 | Loss: 0.00000794
Iteration 71/1000 | Loss: 0.00000793
Iteration 72/1000 | Loss: 0.00000793
Iteration 73/1000 | Loss: 0.00000793
Iteration 74/1000 | Loss: 0.00000792
Iteration 75/1000 | Loss: 0.00000792
Iteration 76/1000 | Loss: 0.00000792
Iteration 77/1000 | Loss: 0.00000791
Iteration 78/1000 | Loss: 0.00000791
Iteration 79/1000 | Loss: 0.00000789
Iteration 80/1000 | Loss: 0.00000789
Iteration 81/1000 | Loss: 0.00000789
Iteration 82/1000 | Loss: 0.00000788
Iteration 83/1000 | Loss: 0.00000788
Iteration 84/1000 | Loss: 0.00000788
Iteration 85/1000 | Loss: 0.00000788
Iteration 86/1000 | Loss: 0.00000788
Iteration 87/1000 | Loss: 0.00000788
Iteration 88/1000 | Loss: 0.00000788
Iteration 89/1000 | Loss: 0.00000787
Iteration 90/1000 | Loss: 0.00000786
Iteration 91/1000 | Loss: 0.00000785
Iteration 92/1000 | Loss: 0.00000785
Iteration 93/1000 | Loss: 0.00000785
Iteration 94/1000 | Loss: 0.00000785
Iteration 95/1000 | Loss: 0.00000785
Iteration 96/1000 | Loss: 0.00000785
Iteration 97/1000 | Loss: 0.00000785
Iteration 98/1000 | Loss: 0.00000785
Iteration 99/1000 | Loss: 0.00000785
Iteration 100/1000 | Loss: 0.00000784
Iteration 101/1000 | Loss: 0.00000784
Iteration 102/1000 | Loss: 0.00000784
Iteration 103/1000 | Loss: 0.00000784
Iteration 104/1000 | Loss: 0.00000784
Iteration 105/1000 | Loss: 0.00000783
Iteration 106/1000 | Loss: 0.00000783
Iteration 107/1000 | Loss: 0.00000783
Iteration 108/1000 | Loss: 0.00000783
Iteration 109/1000 | Loss: 0.00000783
Iteration 110/1000 | Loss: 0.00000782
Iteration 111/1000 | Loss: 0.00000782
Iteration 112/1000 | Loss: 0.00000781
Iteration 113/1000 | Loss: 0.00000781
Iteration 114/1000 | Loss: 0.00000781
Iteration 115/1000 | Loss: 0.00000781
Iteration 116/1000 | Loss: 0.00000781
Iteration 117/1000 | Loss: 0.00000781
Iteration 118/1000 | Loss: 0.00000781
Iteration 119/1000 | Loss: 0.00000781
Iteration 120/1000 | Loss: 0.00000781
Iteration 121/1000 | Loss: 0.00000781
Iteration 122/1000 | Loss: 0.00000781
Iteration 123/1000 | Loss: 0.00000781
Iteration 124/1000 | Loss: 0.00000781
Iteration 125/1000 | Loss: 0.00000781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [7.811168870830443e-06, 7.811168870830443e-06, 7.811168870830443e-06, 7.811168870830443e-06, 7.811168870830443e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.811168870830443e-06

Optimization complete. Final v2v error: 2.419975519180298 mm

Highest mean error: 2.8660922050476074 mm for frame 132

Lowest mean error: 2.3413658142089844 mm for frame 211

Saving results

Total time: 35.872122287750244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460270
Iteration 2/25 | Loss: 0.00147950
Iteration 3/25 | Loss: 0.00125815
Iteration 4/25 | Loss: 0.00122873
Iteration 5/25 | Loss: 0.00121958
Iteration 6/25 | Loss: 0.00120075
Iteration 7/25 | Loss: 0.00117605
Iteration 8/25 | Loss: 0.00116964
Iteration 9/25 | Loss: 0.00114256
Iteration 10/25 | Loss: 0.00112336
Iteration 11/25 | Loss: 0.00111806
Iteration 12/25 | Loss: 0.00111562
Iteration 13/25 | Loss: 0.00111476
Iteration 14/25 | Loss: 0.00111446
Iteration 15/25 | Loss: 0.00111441
Iteration 16/25 | Loss: 0.00111441
Iteration 17/25 | Loss: 0.00111440
Iteration 18/25 | Loss: 0.00111440
Iteration 19/25 | Loss: 0.00111440
Iteration 20/25 | Loss: 0.00111440
Iteration 21/25 | Loss: 0.00111440
Iteration 22/25 | Loss: 0.00111440
Iteration 23/25 | Loss: 0.00111440
Iteration 24/25 | Loss: 0.00111440
Iteration 25/25 | Loss: 0.00111440

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38795149
Iteration 2/25 | Loss: 0.00070213
Iteration 3/25 | Loss: 0.00070213
Iteration 4/25 | Loss: 0.00070213
Iteration 5/25 | Loss: 0.00070213
Iteration 6/25 | Loss: 0.00070213
Iteration 7/25 | Loss: 0.00070213
Iteration 8/25 | Loss: 0.00070212
Iteration 9/25 | Loss: 0.00070212
Iteration 10/25 | Loss: 0.00070212
Iteration 11/25 | Loss: 0.00070212
Iteration 12/25 | Loss: 0.00070212
Iteration 13/25 | Loss: 0.00070212
Iteration 14/25 | Loss: 0.00070212
Iteration 15/25 | Loss: 0.00070212
Iteration 16/25 | Loss: 0.00070212
Iteration 17/25 | Loss: 0.00070212
Iteration 18/25 | Loss: 0.00070212
Iteration 19/25 | Loss: 0.00070212
Iteration 20/25 | Loss: 0.00070212
Iteration 21/25 | Loss: 0.00070212
Iteration 22/25 | Loss: 0.00070212
Iteration 23/25 | Loss: 0.00070212
Iteration 24/25 | Loss: 0.00070212
Iteration 25/25 | Loss: 0.00070212

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070212
Iteration 2/1000 | Loss: 0.00002382
Iteration 3/1000 | Loss: 0.00001750
Iteration 4/1000 | Loss: 0.00001569
Iteration 5/1000 | Loss: 0.00001499
Iteration 6/1000 | Loss: 0.00001453
Iteration 7/1000 | Loss: 0.00001414
Iteration 8/1000 | Loss: 0.00001381
Iteration 9/1000 | Loss: 0.00001360
Iteration 10/1000 | Loss: 0.00001358
Iteration 11/1000 | Loss: 0.00001341
Iteration 12/1000 | Loss: 0.00001330
Iteration 13/1000 | Loss: 0.00001328
Iteration 14/1000 | Loss: 0.00001325
Iteration 15/1000 | Loss: 0.00001322
Iteration 16/1000 | Loss: 0.00001322
Iteration 17/1000 | Loss: 0.00001319
Iteration 18/1000 | Loss: 0.00001318
Iteration 19/1000 | Loss: 0.00001318
Iteration 20/1000 | Loss: 0.00001318
Iteration 21/1000 | Loss: 0.00001318
Iteration 22/1000 | Loss: 0.00001318
Iteration 23/1000 | Loss: 0.00001318
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001318
Iteration 26/1000 | Loss: 0.00001318
Iteration 27/1000 | Loss: 0.00001318
Iteration 28/1000 | Loss: 0.00001318
Iteration 29/1000 | Loss: 0.00001318
Iteration 30/1000 | Loss: 0.00001318
Iteration 31/1000 | Loss: 0.00001318
Iteration 32/1000 | Loss: 0.00001318
Iteration 33/1000 | Loss: 0.00001318
Iteration 34/1000 | Loss: 0.00001318
Iteration 35/1000 | Loss: 0.00001317
Iteration 36/1000 | Loss: 0.00001313
Iteration 37/1000 | Loss: 0.00001313
Iteration 38/1000 | Loss: 0.00001310
Iteration 39/1000 | Loss: 0.00001310
Iteration 40/1000 | Loss: 0.00001309
Iteration 41/1000 | Loss: 0.00001309
Iteration 42/1000 | Loss: 0.00001309
Iteration 43/1000 | Loss: 0.00001309
Iteration 44/1000 | Loss: 0.00001308
Iteration 45/1000 | Loss: 0.00001308
Iteration 46/1000 | Loss: 0.00001308
Iteration 47/1000 | Loss: 0.00001308
Iteration 48/1000 | Loss: 0.00001307
Iteration 49/1000 | Loss: 0.00001307
Iteration 50/1000 | Loss: 0.00001307
Iteration 51/1000 | Loss: 0.00001307
Iteration 52/1000 | Loss: 0.00001307
Iteration 53/1000 | Loss: 0.00001307
Iteration 54/1000 | Loss: 0.00001306
Iteration 55/1000 | Loss: 0.00001306
Iteration 56/1000 | Loss: 0.00001306
Iteration 57/1000 | Loss: 0.00001306
Iteration 58/1000 | Loss: 0.00001305
Iteration 59/1000 | Loss: 0.00001305
Iteration 60/1000 | Loss: 0.00001305
Iteration 61/1000 | Loss: 0.00001304
Iteration 62/1000 | Loss: 0.00001303
Iteration 63/1000 | Loss: 0.00001303
Iteration 64/1000 | Loss: 0.00001303
Iteration 65/1000 | Loss: 0.00001303
Iteration 66/1000 | Loss: 0.00001303
Iteration 67/1000 | Loss: 0.00001302
Iteration 68/1000 | Loss: 0.00001302
Iteration 69/1000 | Loss: 0.00001302
Iteration 70/1000 | Loss: 0.00001302
Iteration 71/1000 | Loss: 0.00001302
Iteration 72/1000 | Loss: 0.00001302
Iteration 73/1000 | Loss: 0.00001301
Iteration 74/1000 | Loss: 0.00001301
Iteration 75/1000 | Loss: 0.00001301
Iteration 76/1000 | Loss: 0.00001300
Iteration 77/1000 | Loss: 0.00001300
Iteration 78/1000 | Loss: 0.00001300
Iteration 79/1000 | Loss: 0.00001300
Iteration 80/1000 | Loss: 0.00001300
Iteration 81/1000 | Loss: 0.00001300
Iteration 82/1000 | Loss: 0.00001300
Iteration 83/1000 | Loss: 0.00001300
Iteration 84/1000 | Loss: 0.00001299
Iteration 85/1000 | Loss: 0.00001299
Iteration 86/1000 | Loss: 0.00001299
Iteration 87/1000 | Loss: 0.00001298
Iteration 88/1000 | Loss: 0.00001298
Iteration 89/1000 | Loss: 0.00001298
Iteration 90/1000 | Loss: 0.00001297
Iteration 91/1000 | Loss: 0.00001297
Iteration 92/1000 | Loss: 0.00001297
Iteration 93/1000 | Loss: 0.00001297
Iteration 94/1000 | Loss: 0.00001297
Iteration 95/1000 | Loss: 0.00001296
Iteration 96/1000 | Loss: 0.00001296
Iteration 97/1000 | Loss: 0.00001296
Iteration 98/1000 | Loss: 0.00001296
Iteration 99/1000 | Loss: 0.00001296
Iteration 100/1000 | Loss: 0.00001296
Iteration 101/1000 | Loss: 0.00001296
Iteration 102/1000 | Loss: 0.00001296
Iteration 103/1000 | Loss: 0.00001296
Iteration 104/1000 | Loss: 0.00001296
Iteration 105/1000 | Loss: 0.00001295
Iteration 106/1000 | Loss: 0.00001295
Iteration 107/1000 | Loss: 0.00001294
Iteration 108/1000 | Loss: 0.00001294
Iteration 109/1000 | Loss: 0.00001294
Iteration 110/1000 | Loss: 0.00001294
Iteration 111/1000 | Loss: 0.00001293
Iteration 112/1000 | Loss: 0.00001293
Iteration 113/1000 | Loss: 0.00001293
Iteration 114/1000 | Loss: 0.00001292
Iteration 115/1000 | Loss: 0.00001292
Iteration 116/1000 | Loss: 0.00001292
Iteration 117/1000 | Loss: 0.00001292
Iteration 118/1000 | Loss: 0.00001291
Iteration 119/1000 | Loss: 0.00001291
Iteration 120/1000 | Loss: 0.00001291
Iteration 121/1000 | Loss: 0.00001291
Iteration 122/1000 | Loss: 0.00001291
Iteration 123/1000 | Loss: 0.00001291
Iteration 124/1000 | Loss: 0.00001290
Iteration 125/1000 | Loss: 0.00001290
Iteration 126/1000 | Loss: 0.00001290
Iteration 127/1000 | Loss: 0.00001290
Iteration 128/1000 | Loss: 0.00001289
Iteration 129/1000 | Loss: 0.00001289
Iteration 130/1000 | Loss: 0.00001289
Iteration 131/1000 | Loss: 0.00001289
Iteration 132/1000 | Loss: 0.00001289
Iteration 133/1000 | Loss: 0.00001288
Iteration 134/1000 | Loss: 0.00001288
Iteration 135/1000 | Loss: 0.00001288
Iteration 136/1000 | Loss: 0.00001288
Iteration 137/1000 | Loss: 0.00001288
Iteration 138/1000 | Loss: 0.00001288
Iteration 139/1000 | Loss: 0.00001288
Iteration 140/1000 | Loss: 0.00001288
Iteration 141/1000 | Loss: 0.00001288
Iteration 142/1000 | Loss: 0.00001288
Iteration 143/1000 | Loss: 0.00001288
Iteration 144/1000 | Loss: 0.00001288
Iteration 145/1000 | Loss: 0.00001288
Iteration 146/1000 | Loss: 0.00001288
Iteration 147/1000 | Loss: 0.00001287
Iteration 148/1000 | Loss: 0.00001287
Iteration 149/1000 | Loss: 0.00001287
Iteration 150/1000 | Loss: 0.00001287
Iteration 151/1000 | Loss: 0.00001287
Iteration 152/1000 | Loss: 0.00001287
Iteration 153/1000 | Loss: 0.00001286
Iteration 154/1000 | Loss: 0.00001286
Iteration 155/1000 | Loss: 0.00001286
Iteration 156/1000 | Loss: 0.00001286
Iteration 157/1000 | Loss: 0.00001285
Iteration 158/1000 | Loss: 0.00001285
Iteration 159/1000 | Loss: 0.00001285
Iteration 160/1000 | Loss: 0.00001285
Iteration 161/1000 | Loss: 0.00001285
Iteration 162/1000 | Loss: 0.00001285
Iteration 163/1000 | Loss: 0.00001284
Iteration 164/1000 | Loss: 0.00001284
Iteration 165/1000 | Loss: 0.00001283
Iteration 166/1000 | Loss: 0.00001283
Iteration 167/1000 | Loss: 0.00001283
Iteration 168/1000 | Loss: 0.00001283
Iteration 169/1000 | Loss: 0.00001283
Iteration 170/1000 | Loss: 0.00001283
Iteration 171/1000 | Loss: 0.00001282
Iteration 172/1000 | Loss: 0.00001282
Iteration 173/1000 | Loss: 0.00001282
Iteration 174/1000 | Loss: 0.00001282
Iteration 175/1000 | Loss: 0.00001281
Iteration 176/1000 | Loss: 0.00001281
Iteration 177/1000 | Loss: 0.00001281
Iteration 178/1000 | Loss: 0.00001281
Iteration 179/1000 | Loss: 0.00001281
Iteration 180/1000 | Loss: 0.00001281
Iteration 181/1000 | Loss: 0.00001281
Iteration 182/1000 | Loss: 0.00001280
Iteration 183/1000 | Loss: 0.00001280
Iteration 184/1000 | Loss: 0.00001280
Iteration 185/1000 | Loss: 0.00001280
Iteration 186/1000 | Loss: 0.00001280
Iteration 187/1000 | Loss: 0.00001280
Iteration 188/1000 | Loss: 0.00001280
Iteration 189/1000 | Loss: 0.00001280
Iteration 190/1000 | Loss: 0.00001280
Iteration 191/1000 | Loss: 0.00001280
Iteration 192/1000 | Loss: 0.00001280
Iteration 193/1000 | Loss: 0.00001279
Iteration 194/1000 | Loss: 0.00001279
Iteration 195/1000 | Loss: 0.00001279
Iteration 196/1000 | Loss: 0.00001279
Iteration 197/1000 | Loss: 0.00001279
Iteration 198/1000 | Loss: 0.00001279
Iteration 199/1000 | Loss: 0.00001279
Iteration 200/1000 | Loss: 0.00001279
Iteration 201/1000 | Loss: 0.00001279
Iteration 202/1000 | Loss: 0.00001279
Iteration 203/1000 | Loss: 0.00001279
Iteration 204/1000 | Loss: 0.00001279
Iteration 205/1000 | Loss: 0.00001279
Iteration 206/1000 | Loss: 0.00001279
Iteration 207/1000 | Loss: 0.00001279
Iteration 208/1000 | Loss: 0.00001279
Iteration 209/1000 | Loss: 0.00001279
Iteration 210/1000 | Loss: 0.00001279
Iteration 211/1000 | Loss: 0.00001279
Iteration 212/1000 | Loss: 0.00001279
Iteration 213/1000 | Loss: 0.00001279
Iteration 214/1000 | Loss: 0.00001279
Iteration 215/1000 | Loss: 0.00001279
Iteration 216/1000 | Loss: 0.00001279
Iteration 217/1000 | Loss: 0.00001279
Iteration 218/1000 | Loss: 0.00001279
Iteration 219/1000 | Loss: 0.00001279
Iteration 220/1000 | Loss: 0.00001279
Iteration 221/1000 | Loss: 0.00001279
Iteration 222/1000 | Loss: 0.00001279
Iteration 223/1000 | Loss: 0.00001279
Iteration 224/1000 | Loss: 0.00001279
Iteration 225/1000 | Loss: 0.00001279
Iteration 226/1000 | Loss: 0.00001279
Iteration 227/1000 | Loss: 0.00001279
Iteration 228/1000 | Loss: 0.00001279
Iteration 229/1000 | Loss: 0.00001279
Iteration 230/1000 | Loss: 0.00001279
Iteration 231/1000 | Loss: 0.00001279
Iteration 232/1000 | Loss: 0.00001279
Iteration 233/1000 | Loss: 0.00001279
Iteration 234/1000 | Loss: 0.00001279
Iteration 235/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.2787042578565888e-05, 1.2787042578565888e-05, 1.2787042578565888e-05, 1.2787042578565888e-05, 1.2787042578565888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2787042578565888e-05

Optimization complete. Final v2v error: 3.05240797996521 mm

Highest mean error: 3.4869892597198486 mm for frame 122

Lowest mean error: 2.89013409614563 mm for frame 25

Saving results

Total time: 55.88924431800842
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466518
Iteration 2/25 | Loss: 0.00122882
Iteration 3/25 | Loss: 0.00110206
Iteration 4/25 | Loss: 0.00108878
Iteration 5/25 | Loss: 0.00108581
Iteration 6/25 | Loss: 0.00108536
Iteration 7/25 | Loss: 0.00108536
Iteration 8/25 | Loss: 0.00108536
Iteration 9/25 | Loss: 0.00108536
Iteration 10/25 | Loss: 0.00108536
Iteration 11/25 | Loss: 0.00108536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010853573912754655, 0.0010853573912754655, 0.0010853573912754655, 0.0010853573912754655, 0.0010853573912754655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010853573912754655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.10978031
Iteration 2/25 | Loss: 0.00073695
Iteration 3/25 | Loss: 0.00073693
Iteration 4/25 | Loss: 0.00073693
Iteration 5/25 | Loss: 0.00073693
Iteration 6/25 | Loss: 0.00073693
Iteration 7/25 | Loss: 0.00073693
Iteration 8/25 | Loss: 0.00073693
Iteration 9/25 | Loss: 0.00073693
Iteration 10/25 | Loss: 0.00073693
Iteration 11/25 | Loss: 0.00073693
Iteration 12/25 | Loss: 0.00073693
Iteration 13/25 | Loss: 0.00073693
Iteration 14/25 | Loss: 0.00073693
Iteration 15/25 | Loss: 0.00073693
Iteration 16/25 | Loss: 0.00073693
Iteration 17/25 | Loss: 0.00073693
Iteration 18/25 | Loss: 0.00073693
Iteration 19/25 | Loss: 0.00073693
Iteration 20/25 | Loss: 0.00073693
Iteration 21/25 | Loss: 0.00073693
Iteration 22/25 | Loss: 0.00073693
Iteration 23/25 | Loss: 0.00073693
Iteration 24/25 | Loss: 0.00073693
Iteration 25/25 | Loss: 0.00073693

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073693
Iteration 2/1000 | Loss: 0.00002192
Iteration 3/1000 | Loss: 0.00001585
Iteration 4/1000 | Loss: 0.00001437
Iteration 5/1000 | Loss: 0.00001359
Iteration 6/1000 | Loss: 0.00001309
Iteration 7/1000 | Loss: 0.00001272
Iteration 8/1000 | Loss: 0.00001247
Iteration 9/1000 | Loss: 0.00001213
Iteration 10/1000 | Loss: 0.00001206
Iteration 11/1000 | Loss: 0.00001202
Iteration 12/1000 | Loss: 0.00001202
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001187
Iteration 15/1000 | Loss: 0.00001186
Iteration 16/1000 | Loss: 0.00001179
Iteration 17/1000 | Loss: 0.00001175
Iteration 18/1000 | Loss: 0.00001174
Iteration 19/1000 | Loss: 0.00001174
Iteration 20/1000 | Loss: 0.00001173
Iteration 21/1000 | Loss: 0.00001170
Iteration 22/1000 | Loss: 0.00001169
Iteration 23/1000 | Loss: 0.00001168
Iteration 24/1000 | Loss: 0.00001168
Iteration 25/1000 | Loss: 0.00001167
Iteration 26/1000 | Loss: 0.00001167
Iteration 27/1000 | Loss: 0.00001164
Iteration 28/1000 | Loss: 0.00001164
Iteration 29/1000 | Loss: 0.00001164
Iteration 30/1000 | Loss: 0.00001164
Iteration 31/1000 | Loss: 0.00001164
Iteration 32/1000 | Loss: 0.00001164
Iteration 33/1000 | Loss: 0.00001163
Iteration 34/1000 | Loss: 0.00001163
Iteration 35/1000 | Loss: 0.00001162
Iteration 36/1000 | Loss: 0.00001162
Iteration 37/1000 | Loss: 0.00001162
Iteration 38/1000 | Loss: 0.00001162
Iteration 39/1000 | Loss: 0.00001161
Iteration 40/1000 | Loss: 0.00001160
Iteration 41/1000 | Loss: 0.00001160
Iteration 42/1000 | Loss: 0.00001160
Iteration 43/1000 | Loss: 0.00001160
Iteration 44/1000 | Loss: 0.00001160
Iteration 45/1000 | Loss: 0.00001160
Iteration 46/1000 | Loss: 0.00001160
Iteration 47/1000 | Loss: 0.00001160
Iteration 48/1000 | Loss: 0.00001160
Iteration 49/1000 | Loss: 0.00001159
Iteration 50/1000 | Loss: 0.00001159
Iteration 51/1000 | Loss: 0.00001159
Iteration 52/1000 | Loss: 0.00001159
Iteration 53/1000 | Loss: 0.00001158
Iteration 54/1000 | Loss: 0.00001158
Iteration 55/1000 | Loss: 0.00001158
Iteration 56/1000 | Loss: 0.00001158
Iteration 57/1000 | Loss: 0.00001157
Iteration 58/1000 | Loss: 0.00001157
Iteration 59/1000 | Loss: 0.00001157
Iteration 60/1000 | Loss: 0.00001157
Iteration 61/1000 | Loss: 0.00001157
Iteration 62/1000 | Loss: 0.00001157
Iteration 63/1000 | Loss: 0.00001156
Iteration 64/1000 | Loss: 0.00001156
Iteration 65/1000 | Loss: 0.00001156
Iteration 66/1000 | Loss: 0.00001156
Iteration 67/1000 | Loss: 0.00001156
Iteration 68/1000 | Loss: 0.00001155
Iteration 69/1000 | Loss: 0.00001155
Iteration 70/1000 | Loss: 0.00001155
Iteration 71/1000 | Loss: 0.00001155
Iteration 72/1000 | Loss: 0.00001154
Iteration 73/1000 | Loss: 0.00001154
Iteration 74/1000 | Loss: 0.00001153
Iteration 75/1000 | Loss: 0.00001153
Iteration 76/1000 | Loss: 0.00001153
Iteration 77/1000 | Loss: 0.00001152
Iteration 78/1000 | Loss: 0.00001152
Iteration 79/1000 | Loss: 0.00001152
Iteration 80/1000 | Loss: 0.00001151
Iteration 81/1000 | Loss: 0.00001151
Iteration 82/1000 | Loss: 0.00001151
Iteration 83/1000 | Loss: 0.00001150
Iteration 84/1000 | Loss: 0.00001150
Iteration 85/1000 | Loss: 0.00001150
Iteration 86/1000 | Loss: 0.00001150
Iteration 87/1000 | Loss: 0.00001150
Iteration 88/1000 | Loss: 0.00001149
Iteration 89/1000 | Loss: 0.00001149
Iteration 90/1000 | Loss: 0.00001149
Iteration 91/1000 | Loss: 0.00001149
Iteration 92/1000 | Loss: 0.00001149
Iteration 93/1000 | Loss: 0.00001149
Iteration 94/1000 | Loss: 0.00001149
Iteration 95/1000 | Loss: 0.00001149
Iteration 96/1000 | Loss: 0.00001148
Iteration 97/1000 | Loss: 0.00001148
Iteration 98/1000 | Loss: 0.00001148
Iteration 99/1000 | Loss: 0.00001148
Iteration 100/1000 | Loss: 0.00001148
Iteration 101/1000 | Loss: 0.00001148
Iteration 102/1000 | Loss: 0.00001148
Iteration 103/1000 | Loss: 0.00001147
Iteration 104/1000 | Loss: 0.00001147
Iteration 105/1000 | Loss: 0.00001147
Iteration 106/1000 | Loss: 0.00001147
Iteration 107/1000 | Loss: 0.00001147
Iteration 108/1000 | Loss: 0.00001147
Iteration 109/1000 | Loss: 0.00001147
Iteration 110/1000 | Loss: 0.00001146
Iteration 111/1000 | Loss: 0.00001146
Iteration 112/1000 | Loss: 0.00001146
Iteration 113/1000 | Loss: 0.00001146
Iteration 114/1000 | Loss: 0.00001146
Iteration 115/1000 | Loss: 0.00001146
Iteration 116/1000 | Loss: 0.00001145
Iteration 117/1000 | Loss: 0.00001145
Iteration 118/1000 | Loss: 0.00001145
Iteration 119/1000 | Loss: 0.00001144
Iteration 120/1000 | Loss: 0.00001144
Iteration 121/1000 | Loss: 0.00001144
Iteration 122/1000 | Loss: 0.00001143
Iteration 123/1000 | Loss: 0.00001143
Iteration 124/1000 | Loss: 0.00001142
Iteration 125/1000 | Loss: 0.00001142
Iteration 126/1000 | Loss: 0.00001142
Iteration 127/1000 | Loss: 0.00001142
Iteration 128/1000 | Loss: 0.00001142
Iteration 129/1000 | Loss: 0.00001142
Iteration 130/1000 | Loss: 0.00001142
Iteration 131/1000 | Loss: 0.00001142
Iteration 132/1000 | Loss: 0.00001142
Iteration 133/1000 | Loss: 0.00001141
Iteration 134/1000 | Loss: 0.00001141
Iteration 135/1000 | Loss: 0.00001141
Iteration 136/1000 | Loss: 0.00001141
Iteration 137/1000 | Loss: 0.00001141
Iteration 138/1000 | Loss: 0.00001140
Iteration 139/1000 | Loss: 0.00001140
Iteration 140/1000 | Loss: 0.00001139
Iteration 141/1000 | Loss: 0.00001139
Iteration 142/1000 | Loss: 0.00001139
Iteration 143/1000 | Loss: 0.00001138
Iteration 144/1000 | Loss: 0.00001138
Iteration 145/1000 | Loss: 0.00001138
Iteration 146/1000 | Loss: 0.00001138
Iteration 147/1000 | Loss: 0.00001138
Iteration 148/1000 | Loss: 0.00001137
Iteration 149/1000 | Loss: 0.00001137
Iteration 150/1000 | Loss: 0.00001137
Iteration 151/1000 | Loss: 0.00001137
Iteration 152/1000 | Loss: 0.00001137
Iteration 153/1000 | Loss: 0.00001136
Iteration 154/1000 | Loss: 0.00001136
Iteration 155/1000 | Loss: 0.00001136
Iteration 156/1000 | Loss: 0.00001136
Iteration 157/1000 | Loss: 0.00001136
Iteration 158/1000 | Loss: 0.00001136
Iteration 159/1000 | Loss: 0.00001136
Iteration 160/1000 | Loss: 0.00001136
Iteration 161/1000 | Loss: 0.00001135
Iteration 162/1000 | Loss: 0.00001135
Iteration 163/1000 | Loss: 0.00001135
Iteration 164/1000 | Loss: 0.00001135
Iteration 165/1000 | Loss: 0.00001135
Iteration 166/1000 | Loss: 0.00001135
Iteration 167/1000 | Loss: 0.00001135
Iteration 168/1000 | Loss: 0.00001135
Iteration 169/1000 | Loss: 0.00001135
Iteration 170/1000 | Loss: 0.00001135
Iteration 171/1000 | Loss: 0.00001135
Iteration 172/1000 | Loss: 0.00001135
Iteration 173/1000 | Loss: 0.00001135
Iteration 174/1000 | Loss: 0.00001135
Iteration 175/1000 | Loss: 0.00001135
Iteration 176/1000 | Loss: 0.00001135
Iteration 177/1000 | Loss: 0.00001135
Iteration 178/1000 | Loss: 0.00001135
Iteration 179/1000 | Loss: 0.00001135
Iteration 180/1000 | Loss: 0.00001135
Iteration 181/1000 | Loss: 0.00001135
Iteration 182/1000 | Loss: 0.00001135
Iteration 183/1000 | Loss: 0.00001135
Iteration 184/1000 | Loss: 0.00001135
Iteration 185/1000 | Loss: 0.00001135
Iteration 186/1000 | Loss: 0.00001135
Iteration 187/1000 | Loss: 0.00001135
Iteration 188/1000 | Loss: 0.00001135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.1352311958034988e-05, 1.1352311958034988e-05, 1.1352311958034988e-05, 1.1352311958034988e-05, 1.1352311958034988e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1352311958034988e-05

Optimization complete. Final v2v error: 2.869637966156006 mm

Highest mean error: 3.1729228496551514 mm for frame 79

Lowest mean error: 2.6016244888305664 mm for frame 158

Saving results

Total time: 38.3968186378479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00560745
Iteration 2/25 | Loss: 0.00121755
Iteration 3/25 | Loss: 0.00116164
Iteration 4/25 | Loss: 0.00115351
Iteration 5/25 | Loss: 0.00115046
Iteration 6/25 | Loss: 0.00115046
Iteration 7/25 | Loss: 0.00115046
Iteration 8/25 | Loss: 0.00115046
Iteration 9/25 | Loss: 0.00115046
Iteration 10/25 | Loss: 0.00115046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011504595167934895, 0.0011504595167934895, 0.0011504595167934895, 0.0011504595167934895, 0.0011504595167934895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011504595167934895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.80213833
Iteration 2/25 | Loss: 0.00072430
Iteration 3/25 | Loss: 0.00072426
Iteration 4/25 | Loss: 0.00072426
Iteration 5/25 | Loss: 0.00072426
Iteration 6/25 | Loss: 0.00072426
Iteration 7/25 | Loss: 0.00072426
Iteration 8/25 | Loss: 0.00072426
Iteration 9/25 | Loss: 0.00072426
Iteration 10/25 | Loss: 0.00072426
Iteration 11/25 | Loss: 0.00072426
Iteration 12/25 | Loss: 0.00072426
Iteration 13/25 | Loss: 0.00072426
Iteration 14/25 | Loss: 0.00072426
Iteration 15/25 | Loss: 0.00072426
Iteration 16/25 | Loss: 0.00072426
Iteration 17/25 | Loss: 0.00072426
Iteration 18/25 | Loss: 0.00072426
Iteration 19/25 | Loss: 0.00072426
Iteration 20/25 | Loss: 0.00072426
Iteration 21/25 | Loss: 0.00072426
Iteration 22/25 | Loss: 0.00072426
Iteration 23/25 | Loss: 0.00072426
Iteration 24/25 | Loss: 0.00072426
Iteration 25/25 | Loss: 0.00072426
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007242553401738405, 0.0007242553401738405, 0.0007242553401738405, 0.0007242553401738405, 0.0007242553401738405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007242553401738405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072426
Iteration 2/1000 | Loss: 0.00002375
Iteration 3/1000 | Loss: 0.00001756
Iteration 4/1000 | Loss: 0.00001627
Iteration 5/1000 | Loss: 0.00001555
Iteration 6/1000 | Loss: 0.00001522
Iteration 7/1000 | Loss: 0.00001500
Iteration 8/1000 | Loss: 0.00001478
Iteration 9/1000 | Loss: 0.00001452
Iteration 10/1000 | Loss: 0.00001430
Iteration 11/1000 | Loss: 0.00001428
Iteration 12/1000 | Loss: 0.00001427
Iteration 13/1000 | Loss: 0.00001424
Iteration 14/1000 | Loss: 0.00001420
Iteration 15/1000 | Loss: 0.00001411
Iteration 16/1000 | Loss: 0.00001406
Iteration 17/1000 | Loss: 0.00001406
Iteration 18/1000 | Loss: 0.00001406
Iteration 19/1000 | Loss: 0.00001406
Iteration 20/1000 | Loss: 0.00001406
Iteration 21/1000 | Loss: 0.00001404
Iteration 22/1000 | Loss: 0.00001403
Iteration 23/1000 | Loss: 0.00001403
Iteration 24/1000 | Loss: 0.00001403
Iteration 25/1000 | Loss: 0.00001402
Iteration 26/1000 | Loss: 0.00001399
Iteration 27/1000 | Loss: 0.00001398
Iteration 28/1000 | Loss: 0.00001395
Iteration 29/1000 | Loss: 0.00001395
Iteration 30/1000 | Loss: 0.00001394
Iteration 31/1000 | Loss: 0.00001394
Iteration 32/1000 | Loss: 0.00001393
Iteration 33/1000 | Loss: 0.00001391
Iteration 34/1000 | Loss: 0.00001390
Iteration 35/1000 | Loss: 0.00001390
Iteration 36/1000 | Loss: 0.00001384
Iteration 37/1000 | Loss: 0.00001384
Iteration 38/1000 | Loss: 0.00001380
Iteration 39/1000 | Loss: 0.00001380
Iteration 40/1000 | Loss: 0.00001378
Iteration 41/1000 | Loss: 0.00001378
Iteration 42/1000 | Loss: 0.00001376
Iteration 43/1000 | Loss: 0.00001375
Iteration 44/1000 | Loss: 0.00001375
Iteration 45/1000 | Loss: 0.00001375
Iteration 46/1000 | Loss: 0.00001375
Iteration 47/1000 | Loss: 0.00001375
Iteration 48/1000 | Loss: 0.00001375
Iteration 49/1000 | Loss: 0.00001371
Iteration 50/1000 | Loss: 0.00001370
Iteration 51/1000 | Loss: 0.00001369
Iteration 52/1000 | Loss: 0.00001369
Iteration 53/1000 | Loss: 0.00001367
Iteration 54/1000 | Loss: 0.00001367
Iteration 55/1000 | Loss: 0.00001367
Iteration 56/1000 | Loss: 0.00001367
Iteration 57/1000 | Loss: 0.00001367
Iteration 58/1000 | Loss: 0.00001367
Iteration 59/1000 | Loss: 0.00001367
Iteration 60/1000 | Loss: 0.00001366
Iteration 61/1000 | Loss: 0.00001366
Iteration 62/1000 | Loss: 0.00001366
Iteration 63/1000 | Loss: 0.00001365
Iteration 64/1000 | Loss: 0.00001365
Iteration 65/1000 | Loss: 0.00001364
Iteration 66/1000 | Loss: 0.00001364
Iteration 67/1000 | Loss: 0.00001363
Iteration 68/1000 | Loss: 0.00001363
Iteration 69/1000 | Loss: 0.00001363
Iteration 70/1000 | Loss: 0.00001363
Iteration 71/1000 | Loss: 0.00001363
Iteration 72/1000 | Loss: 0.00001363
Iteration 73/1000 | Loss: 0.00001362
Iteration 74/1000 | Loss: 0.00001362
Iteration 75/1000 | Loss: 0.00001362
Iteration 76/1000 | Loss: 0.00001361
Iteration 77/1000 | Loss: 0.00001361
Iteration 78/1000 | Loss: 0.00001358
Iteration 79/1000 | Loss: 0.00001358
Iteration 80/1000 | Loss: 0.00001358
Iteration 81/1000 | Loss: 0.00001358
Iteration 82/1000 | Loss: 0.00001358
Iteration 83/1000 | Loss: 0.00001358
Iteration 84/1000 | Loss: 0.00001357
Iteration 85/1000 | Loss: 0.00001357
Iteration 86/1000 | Loss: 0.00001357
Iteration 87/1000 | Loss: 0.00001357
Iteration 88/1000 | Loss: 0.00001354
Iteration 89/1000 | Loss: 0.00001353
Iteration 90/1000 | Loss: 0.00001353
Iteration 91/1000 | Loss: 0.00001353
Iteration 92/1000 | Loss: 0.00001353
Iteration 93/1000 | Loss: 0.00001353
Iteration 94/1000 | Loss: 0.00001353
Iteration 95/1000 | Loss: 0.00001353
Iteration 96/1000 | Loss: 0.00001352
Iteration 97/1000 | Loss: 0.00001352
Iteration 98/1000 | Loss: 0.00001352
Iteration 99/1000 | Loss: 0.00001352
Iteration 100/1000 | Loss: 0.00001352
Iteration 101/1000 | Loss: 0.00001352
Iteration 102/1000 | Loss: 0.00001352
Iteration 103/1000 | Loss: 0.00001351
Iteration 104/1000 | Loss: 0.00001351
Iteration 105/1000 | Loss: 0.00001351
Iteration 106/1000 | Loss: 0.00001351
Iteration 107/1000 | Loss: 0.00001351
Iteration 108/1000 | Loss: 0.00001351
Iteration 109/1000 | Loss: 0.00001351
Iteration 110/1000 | Loss: 0.00001351
Iteration 111/1000 | Loss: 0.00001351
Iteration 112/1000 | Loss: 0.00001350
Iteration 113/1000 | Loss: 0.00001350
Iteration 114/1000 | Loss: 0.00001350
Iteration 115/1000 | Loss: 0.00001350
Iteration 116/1000 | Loss: 0.00001350
Iteration 117/1000 | Loss: 0.00001350
Iteration 118/1000 | Loss: 0.00001350
Iteration 119/1000 | Loss: 0.00001350
Iteration 120/1000 | Loss: 0.00001350
Iteration 121/1000 | Loss: 0.00001349
Iteration 122/1000 | Loss: 0.00001349
Iteration 123/1000 | Loss: 0.00001349
Iteration 124/1000 | Loss: 0.00001349
Iteration 125/1000 | Loss: 0.00001349
Iteration 126/1000 | Loss: 0.00001349
Iteration 127/1000 | Loss: 0.00001349
Iteration 128/1000 | Loss: 0.00001349
Iteration 129/1000 | Loss: 0.00001348
Iteration 130/1000 | Loss: 0.00001348
Iteration 131/1000 | Loss: 0.00001348
Iteration 132/1000 | Loss: 0.00001348
Iteration 133/1000 | Loss: 0.00001348
Iteration 134/1000 | Loss: 0.00001348
Iteration 135/1000 | Loss: 0.00001348
Iteration 136/1000 | Loss: 0.00001348
Iteration 137/1000 | Loss: 0.00001348
Iteration 138/1000 | Loss: 0.00001348
Iteration 139/1000 | Loss: 0.00001347
Iteration 140/1000 | Loss: 0.00001347
Iteration 141/1000 | Loss: 0.00001347
Iteration 142/1000 | Loss: 0.00001347
Iteration 143/1000 | Loss: 0.00001347
Iteration 144/1000 | Loss: 0.00001347
Iteration 145/1000 | Loss: 0.00001347
Iteration 146/1000 | Loss: 0.00001347
Iteration 147/1000 | Loss: 0.00001347
Iteration 148/1000 | Loss: 0.00001347
Iteration 149/1000 | Loss: 0.00001347
Iteration 150/1000 | Loss: 0.00001347
Iteration 151/1000 | Loss: 0.00001347
Iteration 152/1000 | Loss: 0.00001347
Iteration 153/1000 | Loss: 0.00001347
Iteration 154/1000 | Loss: 0.00001346
Iteration 155/1000 | Loss: 0.00001346
Iteration 156/1000 | Loss: 0.00001346
Iteration 157/1000 | Loss: 0.00001346
Iteration 158/1000 | Loss: 0.00001346
Iteration 159/1000 | Loss: 0.00001346
Iteration 160/1000 | Loss: 0.00001346
Iteration 161/1000 | Loss: 0.00001346
Iteration 162/1000 | Loss: 0.00001346
Iteration 163/1000 | Loss: 0.00001346
Iteration 164/1000 | Loss: 0.00001346
Iteration 165/1000 | Loss: 0.00001346
Iteration 166/1000 | Loss: 0.00001346
Iteration 167/1000 | Loss: 0.00001346
Iteration 168/1000 | Loss: 0.00001346
Iteration 169/1000 | Loss: 0.00001346
Iteration 170/1000 | Loss: 0.00001346
Iteration 171/1000 | Loss: 0.00001346
Iteration 172/1000 | Loss: 0.00001346
Iteration 173/1000 | Loss: 0.00001346
Iteration 174/1000 | Loss: 0.00001346
Iteration 175/1000 | Loss: 0.00001346
Iteration 176/1000 | Loss: 0.00001346
Iteration 177/1000 | Loss: 0.00001346
Iteration 178/1000 | Loss: 0.00001346
Iteration 179/1000 | Loss: 0.00001346
Iteration 180/1000 | Loss: 0.00001346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.3456461601890624e-05, 1.3456461601890624e-05, 1.3456461601890624e-05, 1.3456461601890624e-05, 1.3456461601890624e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3456461601890624e-05

Optimization complete. Final v2v error: 3.083599090576172 mm

Highest mean error: 3.2805850505828857 mm for frame 232

Lowest mean error: 2.9007210731506348 mm for frame 165

Saving results

Total time: 44.3541145324707
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411488
Iteration 2/25 | Loss: 0.00143811
Iteration 3/25 | Loss: 0.00114638
Iteration 4/25 | Loss: 0.00110552
Iteration 5/25 | Loss: 0.00109932
Iteration 6/25 | Loss: 0.00109762
Iteration 7/25 | Loss: 0.00109741
Iteration 8/25 | Loss: 0.00109741
Iteration 9/25 | Loss: 0.00109741
Iteration 10/25 | Loss: 0.00109741
Iteration 11/25 | Loss: 0.00109741
Iteration 12/25 | Loss: 0.00109741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010974135948345065, 0.0010974135948345065, 0.0010974135948345065, 0.0010974135948345065, 0.0010974135948345065]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010974135948345065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32516491
Iteration 2/25 | Loss: 0.00069082
Iteration 3/25 | Loss: 0.00069082
Iteration 4/25 | Loss: 0.00069082
Iteration 5/25 | Loss: 0.00069082
Iteration 6/25 | Loss: 0.00069082
Iteration 7/25 | Loss: 0.00069082
Iteration 8/25 | Loss: 0.00069082
Iteration 9/25 | Loss: 0.00069082
Iteration 10/25 | Loss: 0.00069082
Iteration 11/25 | Loss: 0.00069082
Iteration 12/25 | Loss: 0.00069082
Iteration 13/25 | Loss: 0.00069082
Iteration 14/25 | Loss: 0.00069082
Iteration 15/25 | Loss: 0.00069082
Iteration 16/25 | Loss: 0.00069082
Iteration 17/25 | Loss: 0.00069082
Iteration 18/25 | Loss: 0.00069082
Iteration 19/25 | Loss: 0.00069082
Iteration 20/25 | Loss: 0.00069082
Iteration 21/25 | Loss: 0.00069082
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006908189388923347, 0.0006908189388923347, 0.0006908189388923347, 0.0006908189388923347, 0.0006908189388923347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006908189388923347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069082
Iteration 2/1000 | Loss: 0.00004351
Iteration 3/1000 | Loss: 0.00002419
Iteration 4/1000 | Loss: 0.00001776
Iteration 5/1000 | Loss: 0.00001648
Iteration 6/1000 | Loss: 0.00001556
Iteration 7/1000 | Loss: 0.00001480
Iteration 8/1000 | Loss: 0.00001430
Iteration 9/1000 | Loss: 0.00001393
Iteration 10/1000 | Loss: 0.00001366
Iteration 11/1000 | Loss: 0.00001343
Iteration 12/1000 | Loss: 0.00001339
Iteration 13/1000 | Loss: 0.00001338
Iteration 14/1000 | Loss: 0.00001319
Iteration 15/1000 | Loss: 0.00001317
Iteration 16/1000 | Loss: 0.00001316
Iteration 17/1000 | Loss: 0.00001314
Iteration 18/1000 | Loss: 0.00001313
Iteration 19/1000 | Loss: 0.00001312
Iteration 20/1000 | Loss: 0.00001312
Iteration 21/1000 | Loss: 0.00001311
Iteration 22/1000 | Loss: 0.00001311
Iteration 23/1000 | Loss: 0.00001308
Iteration 24/1000 | Loss: 0.00001307
Iteration 25/1000 | Loss: 0.00001306
Iteration 26/1000 | Loss: 0.00001306
Iteration 27/1000 | Loss: 0.00001306
Iteration 28/1000 | Loss: 0.00001305
Iteration 29/1000 | Loss: 0.00001305
Iteration 30/1000 | Loss: 0.00001304
Iteration 31/1000 | Loss: 0.00001304
Iteration 32/1000 | Loss: 0.00001302
Iteration 33/1000 | Loss: 0.00001302
Iteration 34/1000 | Loss: 0.00001302
Iteration 35/1000 | Loss: 0.00001301
Iteration 36/1000 | Loss: 0.00001301
Iteration 37/1000 | Loss: 0.00001301
Iteration 38/1000 | Loss: 0.00001300
Iteration 39/1000 | Loss: 0.00001299
Iteration 40/1000 | Loss: 0.00001297
Iteration 41/1000 | Loss: 0.00001296
Iteration 42/1000 | Loss: 0.00001296
Iteration 43/1000 | Loss: 0.00001296
Iteration 44/1000 | Loss: 0.00001295
Iteration 45/1000 | Loss: 0.00001292
Iteration 46/1000 | Loss: 0.00001291
Iteration 47/1000 | Loss: 0.00001290
Iteration 48/1000 | Loss: 0.00001290
Iteration 49/1000 | Loss: 0.00001290
Iteration 50/1000 | Loss: 0.00001287
Iteration 51/1000 | Loss: 0.00001287
Iteration 52/1000 | Loss: 0.00001287
Iteration 53/1000 | Loss: 0.00001287
Iteration 54/1000 | Loss: 0.00001287
Iteration 55/1000 | Loss: 0.00001287
Iteration 56/1000 | Loss: 0.00001287
Iteration 57/1000 | Loss: 0.00001287
Iteration 58/1000 | Loss: 0.00001286
Iteration 59/1000 | Loss: 0.00001286
Iteration 60/1000 | Loss: 0.00001286
Iteration 61/1000 | Loss: 0.00001286
Iteration 62/1000 | Loss: 0.00001286
Iteration 63/1000 | Loss: 0.00001286
Iteration 64/1000 | Loss: 0.00001285
Iteration 65/1000 | Loss: 0.00001285
Iteration 66/1000 | Loss: 0.00001284
Iteration 67/1000 | Loss: 0.00001284
Iteration 68/1000 | Loss: 0.00001283
Iteration 69/1000 | Loss: 0.00001283
Iteration 70/1000 | Loss: 0.00001283
Iteration 71/1000 | Loss: 0.00001283
Iteration 72/1000 | Loss: 0.00001283
Iteration 73/1000 | Loss: 0.00001282
Iteration 74/1000 | Loss: 0.00001282
Iteration 75/1000 | Loss: 0.00001282
Iteration 76/1000 | Loss: 0.00001282
Iteration 77/1000 | Loss: 0.00001282
Iteration 78/1000 | Loss: 0.00001282
Iteration 79/1000 | Loss: 0.00001282
Iteration 80/1000 | Loss: 0.00001281
Iteration 81/1000 | Loss: 0.00001281
Iteration 82/1000 | Loss: 0.00001281
Iteration 83/1000 | Loss: 0.00001281
Iteration 84/1000 | Loss: 0.00001281
Iteration 85/1000 | Loss: 0.00001281
Iteration 86/1000 | Loss: 0.00001281
Iteration 87/1000 | Loss: 0.00001281
Iteration 88/1000 | Loss: 0.00001281
Iteration 89/1000 | Loss: 0.00001281
Iteration 90/1000 | Loss: 0.00001281
Iteration 91/1000 | Loss: 0.00001280
Iteration 92/1000 | Loss: 0.00001280
Iteration 93/1000 | Loss: 0.00001280
Iteration 94/1000 | Loss: 0.00001279
Iteration 95/1000 | Loss: 0.00001279
Iteration 96/1000 | Loss: 0.00001279
Iteration 97/1000 | Loss: 0.00001279
Iteration 98/1000 | Loss: 0.00001278
Iteration 99/1000 | Loss: 0.00001278
Iteration 100/1000 | Loss: 0.00001278
Iteration 101/1000 | Loss: 0.00001278
Iteration 102/1000 | Loss: 0.00001278
Iteration 103/1000 | Loss: 0.00001277
Iteration 104/1000 | Loss: 0.00001277
Iteration 105/1000 | Loss: 0.00001277
Iteration 106/1000 | Loss: 0.00001276
Iteration 107/1000 | Loss: 0.00001276
Iteration 108/1000 | Loss: 0.00001275
Iteration 109/1000 | Loss: 0.00001275
Iteration 110/1000 | Loss: 0.00001275
Iteration 111/1000 | Loss: 0.00001274
Iteration 112/1000 | Loss: 0.00001274
Iteration 113/1000 | Loss: 0.00001274
Iteration 114/1000 | Loss: 0.00001273
Iteration 115/1000 | Loss: 0.00001273
Iteration 116/1000 | Loss: 0.00001273
Iteration 117/1000 | Loss: 0.00001273
Iteration 118/1000 | Loss: 0.00001272
Iteration 119/1000 | Loss: 0.00001272
Iteration 120/1000 | Loss: 0.00001271
Iteration 121/1000 | Loss: 0.00001271
Iteration 122/1000 | Loss: 0.00001271
Iteration 123/1000 | Loss: 0.00001270
Iteration 124/1000 | Loss: 0.00001270
Iteration 125/1000 | Loss: 0.00001269
Iteration 126/1000 | Loss: 0.00001269
Iteration 127/1000 | Loss: 0.00001269
Iteration 128/1000 | Loss: 0.00001269
Iteration 129/1000 | Loss: 0.00001268
Iteration 130/1000 | Loss: 0.00001268
Iteration 131/1000 | Loss: 0.00001268
Iteration 132/1000 | Loss: 0.00001268
Iteration 133/1000 | Loss: 0.00001268
Iteration 134/1000 | Loss: 0.00001267
Iteration 135/1000 | Loss: 0.00001267
Iteration 136/1000 | Loss: 0.00001267
Iteration 137/1000 | Loss: 0.00001266
Iteration 138/1000 | Loss: 0.00001266
Iteration 139/1000 | Loss: 0.00001266
Iteration 140/1000 | Loss: 0.00001266
Iteration 141/1000 | Loss: 0.00001266
Iteration 142/1000 | Loss: 0.00001265
Iteration 143/1000 | Loss: 0.00001265
Iteration 144/1000 | Loss: 0.00001265
Iteration 145/1000 | Loss: 0.00001265
Iteration 146/1000 | Loss: 0.00001265
Iteration 147/1000 | Loss: 0.00001264
Iteration 148/1000 | Loss: 0.00001264
Iteration 149/1000 | Loss: 0.00001264
Iteration 150/1000 | Loss: 0.00001264
Iteration 151/1000 | Loss: 0.00001264
Iteration 152/1000 | Loss: 0.00001264
Iteration 153/1000 | Loss: 0.00001264
Iteration 154/1000 | Loss: 0.00001264
Iteration 155/1000 | Loss: 0.00001264
Iteration 156/1000 | Loss: 0.00001263
Iteration 157/1000 | Loss: 0.00001263
Iteration 158/1000 | Loss: 0.00001263
Iteration 159/1000 | Loss: 0.00001263
Iteration 160/1000 | Loss: 0.00001262
Iteration 161/1000 | Loss: 0.00001262
Iteration 162/1000 | Loss: 0.00001262
Iteration 163/1000 | Loss: 0.00001262
Iteration 164/1000 | Loss: 0.00001261
Iteration 165/1000 | Loss: 0.00001261
Iteration 166/1000 | Loss: 0.00001261
Iteration 167/1000 | Loss: 0.00001261
Iteration 168/1000 | Loss: 0.00001261
Iteration 169/1000 | Loss: 0.00001261
Iteration 170/1000 | Loss: 0.00001261
Iteration 171/1000 | Loss: 0.00001261
Iteration 172/1000 | Loss: 0.00001261
Iteration 173/1000 | Loss: 0.00001261
Iteration 174/1000 | Loss: 0.00001261
Iteration 175/1000 | Loss: 0.00001260
Iteration 176/1000 | Loss: 0.00001260
Iteration 177/1000 | Loss: 0.00001260
Iteration 178/1000 | Loss: 0.00001260
Iteration 179/1000 | Loss: 0.00001260
Iteration 180/1000 | Loss: 0.00001260
Iteration 181/1000 | Loss: 0.00001260
Iteration 182/1000 | Loss: 0.00001260
Iteration 183/1000 | Loss: 0.00001259
Iteration 184/1000 | Loss: 0.00001259
Iteration 185/1000 | Loss: 0.00001259
Iteration 186/1000 | Loss: 0.00001259
Iteration 187/1000 | Loss: 0.00001259
Iteration 188/1000 | Loss: 0.00001259
Iteration 189/1000 | Loss: 0.00001259
Iteration 190/1000 | Loss: 0.00001259
Iteration 191/1000 | Loss: 0.00001259
Iteration 192/1000 | Loss: 0.00001258
Iteration 193/1000 | Loss: 0.00001258
Iteration 194/1000 | Loss: 0.00001258
Iteration 195/1000 | Loss: 0.00001257
Iteration 196/1000 | Loss: 0.00001257
Iteration 197/1000 | Loss: 0.00001257
Iteration 198/1000 | Loss: 0.00001257
Iteration 199/1000 | Loss: 0.00001257
Iteration 200/1000 | Loss: 0.00001257
Iteration 201/1000 | Loss: 0.00001257
Iteration 202/1000 | Loss: 0.00001257
Iteration 203/1000 | Loss: 0.00001257
Iteration 204/1000 | Loss: 0.00001257
Iteration 205/1000 | Loss: 0.00001257
Iteration 206/1000 | Loss: 0.00001257
Iteration 207/1000 | Loss: 0.00001257
Iteration 208/1000 | Loss: 0.00001257
Iteration 209/1000 | Loss: 0.00001257
Iteration 210/1000 | Loss: 0.00001256
Iteration 211/1000 | Loss: 0.00001256
Iteration 212/1000 | Loss: 0.00001256
Iteration 213/1000 | Loss: 0.00001256
Iteration 214/1000 | Loss: 0.00001256
Iteration 215/1000 | Loss: 0.00001256
Iteration 216/1000 | Loss: 0.00001256
Iteration 217/1000 | Loss: 0.00001256
Iteration 218/1000 | Loss: 0.00001256
Iteration 219/1000 | Loss: 0.00001256
Iteration 220/1000 | Loss: 0.00001256
Iteration 221/1000 | Loss: 0.00001256
Iteration 222/1000 | Loss: 0.00001256
Iteration 223/1000 | Loss: 0.00001256
Iteration 224/1000 | Loss: 0.00001256
Iteration 225/1000 | Loss: 0.00001256
Iteration 226/1000 | Loss: 0.00001256
Iteration 227/1000 | Loss: 0.00001256
Iteration 228/1000 | Loss: 0.00001256
Iteration 229/1000 | Loss: 0.00001256
Iteration 230/1000 | Loss: 0.00001256
Iteration 231/1000 | Loss: 0.00001256
Iteration 232/1000 | Loss: 0.00001256
Iteration 233/1000 | Loss: 0.00001256
Iteration 234/1000 | Loss: 0.00001256
Iteration 235/1000 | Loss: 0.00001256
Iteration 236/1000 | Loss: 0.00001256
Iteration 237/1000 | Loss: 0.00001256
Iteration 238/1000 | Loss: 0.00001256
Iteration 239/1000 | Loss: 0.00001256
Iteration 240/1000 | Loss: 0.00001256
Iteration 241/1000 | Loss: 0.00001256
Iteration 242/1000 | Loss: 0.00001256
Iteration 243/1000 | Loss: 0.00001256
Iteration 244/1000 | Loss: 0.00001256
Iteration 245/1000 | Loss: 0.00001256
Iteration 246/1000 | Loss: 0.00001256
Iteration 247/1000 | Loss: 0.00001256
Iteration 248/1000 | Loss: 0.00001256
Iteration 249/1000 | Loss: 0.00001256
Iteration 250/1000 | Loss: 0.00001256
Iteration 251/1000 | Loss: 0.00001256
Iteration 252/1000 | Loss: 0.00001256
Iteration 253/1000 | Loss: 0.00001256
Iteration 254/1000 | Loss: 0.00001256
Iteration 255/1000 | Loss: 0.00001256
Iteration 256/1000 | Loss: 0.00001256
Iteration 257/1000 | Loss: 0.00001256
Iteration 258/1000 | Loss: 0.00001256
Iteration 259/1000 | Loss: 0.00001256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.2560028153529856e-05, 1.2560028153529856e-05, 1.2560028153529856e-05, 1.2560028153529856e-05, 1.2560028153529856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2560028153529856e-05

Optimization complete. Final v2v error: 3.013646364212036 mm

Highest mean error: 3.8436615467071533 mm for frame 72

Lowest mean error: 2.546170234680176 mm for frame 13

Saving results

Total time: 43.87924003601074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392516
Iteration 2/25 | Loss: 0.00112880
Iteration 3/25 | Loss: 0.00105506
Iteration 4/25 | Loss: 0.00104326
Iteration 5/25 | Loss: 0.00103901
Iteration 6/25 | Loss: 0.00103871
Iteration 7/25 | Loss: 0.00103871
Iteration 8/25 | Loss: 0.00103871
Iteration 9/25 | Loss: 0.00103871
Iteration 10/25 | Loss: 0.00103871
Iteration 11/25 | Loss: 0.00103871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010387105867266655, 0.0010387105867266655, 0.0010387105867266655, 0.0010387105867266655, 0.0010387105867266655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010387105867266655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.83759546
Iteration 2/25 | Loss: 0.00078762
Iteration 3/25 | Loss: 0.00078762
Iteration 4/25 | Loss: 0.00078762
Iteration 5/25 | Loss: 0.00078762
Iteration 6/25 | Loss: 0.00078762
Iteration 7/25 | Loss: 0.00078762
Iteration 8/25 | Loss: 0.00078762
Iteration 9/25 | Loss: 0.00078762
Iteration 10/25 | Loss: 0.00078762
Iteration 11/25 | Loss: 0.00078762
Iteration 12/25 | Loss: 0.00078762
Iteration 13/25 | Loss: 0.00078762
Iteration 14/25 | Loss: 0.00078762
Iteration 15/25 | Loss: 0.00078762
Iteration 16/25 | Loss: 0.00078762
Iteration 17/25 | Loss: 0.00078762
Iteration 18/25 | Loss: 0.00078762
Iteration 19/25 | Loss: 0.00078762
Iteration 20/25 | Loss: 0.00078762
Iteration 21/25 | Loss: 0.00078762
Iteration 22/25 | Loss: 0.00078762
Iteration 23/25 | Loss: 0.00078762
Iteration 24/25 | Loss: 0.00078762
Iteration 25/25 | Loss: 0.00078762

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078762
Iteration 2/1000 | Loss: 0.00001539
Iteration 3/1000 | Loss: 0.00001219
Iteration 4/1000 | Loss: 0.00001118
Iteration 5/1000 | Loss: 0.00001050
Iteration 6/1000 | Loss: 0.00001007
Iteration 7/1000 | Loss: 0.00000984
Iteration 8/1000 | Loss: 0.00000951
Iteration 9/1000 | Loss: 0.00000935
Iteration 10/1000 | Loss: 0.00000933
Iteration 11/1000 | Loss: 0.00000926
Iteration 12/1000 | Loss: 0.00000925
Iteration 13/1000 | Loss: 0.00000920
Iteration 14/1000 | Loss: 0.00000916
Iteration 15/1000 | Loss: 0.00000910
Iteration 16/1000 | Loss: 0.00000909
Iteration 17/1000 | Loss: 0.00000909
Iteration 18/1000 | Loss: 0.00000908
Iteration 19/1000 | Loss: 0.00000908
Iteration 20/1000 | Loss: 0.00000907
Iteration 21/1000 | Loss: 0.00000907
Iteration 22/1000 | Loss: 0.00000906
Iteration 23/1000 | Loss: 0.00000906
Iteration 24/1000 | Loss: 0.00000905
Iteration 25/1000 | Loss: 0.00000904
Iteration 26/1000 | Loss: 0.00000903
Iteration 27/1000 | Loss: 0.00000903
Iteration 28/1000 | Loss: 0.00000903
Iteration 29/1000 | Loss: 0.00000902
Iteration 30/1000 | Loss: 0.00000902
Iteration 31/1000 | Loss: 0.00000901
Iteration 32/1000 | Loss: 0.00000900
Iteration 33/1000 | Loss: 0.00000898
Iteration 34/1000 | Loss: 0.00000898
Iteration 35/1000 | Loss: 0.00000898
Iteration 36/1000 | Loss: 0.00000898
Iteration 37/1000 | Loss: 0.00000897
Iteration 38/1000 | Loss: 0.00000896
Iteration 39/1000 | Loss: 0.00000893
Iteration 40/1000 | Loss: 0.00000893
Iteration 41/1000 | Loss: 0.00000891
Iteration 42/1000 | Loss: 0.00000889
Iteration 43/1000 | Loss: 0.00000888
Iteration 44/1000 | Loss: 0.00000887
Iteration 45/1000 | Loss: 0.00000886
Iteration 46/1000 | Loss: 0.00000886
Iteration 47/1000 | Loss: 0.00000885
Iteration 48/1000 | Loss: 0.00000884
Iteration 49/1000 | Loss: 0.00000883
Iteration 50/1000 | Loss: 0.00000883
Iteration 51/1000 | Loss: 0.00000883
Iteration 52/1000 | Loss: 0.00000883
Iteration 53/1000 | Loss: 0.00000883
Iteration 54/1000 | Loss: 0.00000883
Iteration 55/1000 | Loss: 0.00000883
Iteration 56/1000 | Loss: 0.00000883
Iteration 57/1000 | Loss: 0.00000883
Iteration 58/1000 | Loss: 0.00000883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [8.829486432659905e-06, 8.829486432659905e-06, 8.829486432659905e-06, 8.829486432659905e-06, 8.829486432659905e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.829486432659905e-06

Optimization complete. Final v2v error: 2.5919601917266846 mm

Highest mean error: 2.8001294136047363 mm for frame 115

Lowest mean error: 2.472630262374878 mm for frame 30

Saving results

Total time: 30.449968099594116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031235
Iteration 2/25 | Loss: 0.00245364
Iteration 3/25 | Loss: 0.00207079
Iteration 4/25 | Loss: 0.00152951
Iteration 5/25 | Loss: 0.00146077
Iteration 6/25 | Loss: 0.00137035
Iteration 7/25 | Loss: 0.00134006
Iteration 8/25 | Loss: 0.00127733
Iteration 9/25 | Loss: 0.00123165
Iteration 10/25 | Loss: 0.00120816
Iteration 11/25 | Loss: 0.00120257
Iteration 12/25 | Loss: 0.00119062
Iteration 13/25 | Loss: 0.00118340
Iteration 14/25 | Loss: 0.00117810
Iteration 15/25 | Loss: 0.00118035
Iteration 16/25 | Loss: 0.00117581
Iteration 17/25 | Loss: 0.00117557
Iteration 18/25 | Loss: 0.00117545
Iteration 19/25 | Loss: 0.00117542
Iteration 20/25 | Loss: 0.00117541
Iteration 21/25 | Loss: 0.00117541
Iteration 22/25 | Loss: 0.00117541
Iteration 23/25 | Loss: 0.00117541
Iteration 24/25 | Loss: 0.00117541
Iteration 25/25 | Loss: 0.00117541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37853193
Iteration 2/25 | Loss: 0.00077042
Iteration 3/25 | Loss: 0.00077042
Iteration 4/25 | Loss: 0.00077042
Iteration 5/25 | Loss: 0.00077042
Iteration 6/25 | Loss: 0.00077042
Iteration 7/25 | Loss: 0.00077042
Iteration 8/25 | Loss: 0.00077042
Iteration 9/25 | Loss: 0.00077042
Iteration 10/25 | Loss: 0.00077042
Iteration 11/25 | Loss: 0.00077042
Iteration 12/25 | Loss: 0.00077042
Iteration 13/25 | Loss: 0.00077042
Iteration 14/25 | Loss: 0.00077042
Iteration 15/25 | Loss: 0.00077042
Iteration 16/25 | Loss: 0.00077042
Iteration 17/25 | Loss: 0.00077042
Iteration 18/25 | Loss: 0.00077042
Iteration 19/25 | Loss: 0.00077042
Iteration 20/25 | Loss: 0.00077042
Iteration 21/25 | Loss: 0.00077042
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000770417507737875, 0.000770417507737875, 0.000770417507737875, 0.000770417507737875, 0.000770417507737875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000770417507737875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077042
Iteration 2/1000 | Loss: 0.00020814
Iteration 3/1000 | Loss: 0.00070672
Iteration 4/1000 | Loss: 0.00003005
Iteration 5/1000 | Loss: 0.00022538
Iteration 6/1000 | Loss: 0.00016063
Iteration 7/1000 | Loss: 0.00002561
Iteration 8/1000 | Loss: 0.00002481
Iteration 9/1000 | Loss: 0.00007635
Iteration 10/1000 | Loss: 0.00002386
Iteration 11/1000 | Loss: 0.00007821
Iteration 12/1000 | Loss: 0.00002342
Iteration 13/1000 | Loss: 0.00002320
Iteration 14/1000 | Loss: 0.00002318
Iteration 15/1000 | Loss: 0.00002295
Iteration 16/1000 | Loss: 0.00002287
Iteration 17/1000 | Loss: 0.00002269
Iteration 18/1000 | Loss: 0.00002253
Iteration 19/1000 | Loss: 0.00002235
Iteration 20/1000 | Loss: 0.00002216
Iteration 21/1000 | Loss: 0.00002213
Iteration 22/1000 | Loss: 0.00002200
Iteration 23/1000 | Loss: 0.00002197
Iteration 24/1000 | Loss: 0.00002196
Iteration 25/1000 | Loss: 0.00002195
Iteration 26/1000 | Loss: 0.00002194
Iteration 27/1000 | Loss: 0.00002194
Iteration 28/1000 | Loss: 0.00002180
Iteration 29/1000 | Loss: 0.00002179
Iteration 30/1000 | Loss: 0.00002178
Iteration 31/1000 | Loss: 0.00002177
Iteration 32/1000 | Loss: 0.00002174
Iteration 33/1000 | Loss: 0.00002172
Iteration 34/1000 | Loss: 0.00002171
Iteration 35/1000 | Loss: 0.00002171
Iteration 36/1000 | Loss: 0.00002167
Iteration 37/1000 | Loss: 0.00002166
Iteration 38/1000 | Loss: 0.00002164
Iteration 39/1000 | Loss: 0.00002163
Iteration 40/1000 | Loss: 0.00002163
Iteration 41/1000 | Loss: 0.00002163
Iteration 42/1000 | Loss: 0.00002163
Iteration 43/1000 | Loss: 0.00002163
Iteration 44/1000 | Loss: 0.00002162
Iteration 45/1000 | Loss: 0.00002162
Iteration 46/1000 | Loss: 0.00002162
Iteration 47/1000 | Loss: 0.00002162
Iteration 48/1000 | Loss: 0.00002162
Iteration 49/1000 | Loss: 0.00002161
Iteration 50/1000 | Loss: 0.00002161
Iteration 51/1000 | Loss: 0.00002161
Iteration 52/1000 | Loss: 0.00002161
Iteration 53/1000 | Loss: 0.00002161
Iteration 54/1000 | Loss: 0.00002161
Iteration 55/1000 | Loss: 0.00002161
Iteration 56/1000 | Loss: 0.00002161
Iteration 57/1000 | Loss: 0.00002161
Iteration 58/1000 | Loss: 0.00002161
Iteration 59/1000 | Loss: 0.00002161
Iteration 60/1000 | Loss: 0.00002161
Iteration 61/1000 | Loss: 0.00002161
Iteration 62/1000 | Loss: 0.00002161
Iteration 63/1000 | Loss: 0.00002161
Iteration 64/1000 | Loss: 0.00002161
Iteration 65/1000 | Loss: 0.00002161
Iteration 66/1000 | Loss: 0.00002161
Iteration 67/1000 | Loss: 0.00002161
Iteration 68/1000 | Loss: 0.00002161
Iteration 69/1000 | Loss: 0.00002161
Iteration 70/1000 | Loss: 0.00002161
Iteration 71/1000 | Loss: 0.00002161
Iteration 72/1000 | Loss: 0.00002161
Iteration 73/1000 | Loss: 0.00002161
Iteration 74/1000 | Loss: 0.00002161
Iteration 75/1000 | Loss: 0.00002161
Iteration 76/1000 | Loss: 0.00002161
Iteration 77/1000 | Loss: 0.00002161
Iteration 78/1000 | Loss: 0.00002161
Iteration 79/1000 | Loss: 0.00002161
Iteration 80/1000 | Loss: 0.00002161
Iteration 81/1000 | Loss: 0.00002161
Iteration 82/1000 | Loss: 0.00002161
Iteration 83/1000 | Loss: 0.00002161
Iteration 84/1000 | Loss: 0.00002161
Iteration 85/1000 | Loss: 0.00002161
Iteration 86/1000 | Loss: 0.00002161
Iteration 87/1000 | Loss: 0.00002161
Iteration 88/1000 | Loss: 0.00002161
Iteration 89/1000 | Loss: 0.00002161
Iteration 90/1000 | Loss: 0.00002161
Iteration 91/1000 | Loss: 0.00002161
Iteration 92/1000 | Loss: 0.00002160
Iteration 93/1000 | Loss: 0.00002160
Iteration 94/1000 | Loss: 0.00002160
Iteration 95/1000 | Loss: 0.00002160
Iteration 96/1000 | Loss: 0.00002160
Iteration 97/1000 | Loss: 0.00002160
Iteration 98/1000 | Loss: 0.00002160
Iteration 99/1000 | Loss: 0.00002160
Iteration 100/1000 | Loss: 0.00002160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.1604662833851762e-05, 2.1604662833851762e-05, 2.1604662833851762e-05, 2.1604662833851762e-05, 2.1604662833851762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1604662833851762e-05

Optimization complete. Final v2v error: 3.5185775756835938 mm

Highest mean error: 11.65495777130127 mm for frame 5

Lowest mean error: 2.8957276344299316 mm for frame 83

Saving results

Total time: 76.88885188102722
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00606798
Iteration 2/25 | Loss: 0.00114914
Iteration 3/25 | Loss: 0.00107899
Iteration 4/25 | Loss: 0.00107054
Iteration 5/25 | Loss: 0.00106867
Iteration 6/25 | Loss: 0.00106867
Iteration 7/25 | Loss: 0.00106867
Iteration 8/25 | Loss: 0.00106867
Iteration 9/25 | Loss: 0.00106867
Iteration 10/25 | Loss: 0.00106867
Iteration 11/25 | Loss: 0.00106867
Iteration 12/25 | Loss: 0.00106867
Iteration 13/25 | Loss: 0.00106867
Iteration 14/25 | Loss: 0.00106867
Iteration 15/25 | Loss: 0.00106867
Iteration 16/25 | Loss: 0.00106867
Iteration 17/25 | Loss: 0.00106867
Iteration 18/25 | Loss: 0.00106867
Iteration 19/25 | Loss: 0.00106867
Iteration 20/25 | Loss: 0.00106867
Iteration 21/25 | Loss: 0.00106867
Iteration 22/25 | Loss: 0.00106867
Iteration 23/25 | Loss: 0.00106867
Iteration 24/25 | Loss: 0.00106867
Iteration 25/25 | Loss: 0.00106867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.72855663
Iteration 2/25 | Loss: 0.00074130
Iteration 3/25 | Loss: 0.00074130
Iteration 4/25 | Loss: 0.00074130
Iteration 5/25 | Loss: 0.00074130
Iteration 6/25 | Loss: 0.00074130
Iteration 7/25 | Loss: 0.00074130
Iteration 8/25 | Loss: 0.00074130
Iteration 9/25 | Loss: 0.00074130
Iteration 10/25 | Loss: 0.00074130
Iteration 11/25 | Loss: 0.00074130
Iteration 12/25 | Loss: 0.00074130
Iteration 13/25 | Loss: 0.00074130
Iteration 14/25 | Loss: 0.00074130
Iteration 15/25 | Loss: 0.00074130
Iteration 16/25 | Loss: 0.00074130
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007413008715957403, 0.0007413008715957403, 0.0007413008715957403, 0.0007413008715957403, 0.0007413008715957403]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007413008715957403

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074130
Iteration 2/1000 | Loss: 0.00001752
Iteration 3/1000 | Loss: 0.00001331
Iteration 4/1000 | Loss: 0.00001212
Iteration 5/1000 | Loss: 0.00001156
Iteration 6/1000 | Loss: 0.00001113
Iteration 7/1000 | Loss: 0.00001089
Iteration 8/1000 | Loss: 0.00001062
Iteration 9/1000 | Loss: 0.00001037
Iteration 10/1000 | Loss: 0.00001033
Iteration 11/1000 | Loss: 0.00001015
Iteration 12/1000 | Loss: 0.00001014
Iteration 13/1000 | Loss: 0.00001004
Iteration 14/1000 | Loss: 0.00000999
Iteration 15/1000 | Loss: 0.00000998
Iteration 16/1000 | Loss: 0.00000994
Iteration 17/1000 | Loss: 0.00000993
Iteration 18/1000 | Loss: 0.00000992
Iteration 19/1000 | Loss: 0.00000992
Iteration 20/1000 | Loss: 0.00000992
Iteration 21/1000 | Loss: 0.00000991
Iteration 22/1000 | Loss: 0.00000990
Iteration 23/1000 | Loss: 0.00000990
Iteration 24/1000 | Loss: 0.00000990
Iteration 25/1000 | Loss: 0.00000989
Iteration 26/1000 | Loss: 0.00000989
Iteration 27/1000 | Loss: 0.00000988
Iteration 28/1000 | Loss: 0.00000988
Iteration 29/1000 | Loss: 0.00000987
Iteration 30/1000 | Loss: 0.00000986
Iteration 31/1000 | Loss: 0.00000986
Iteration 32/1000 | Loss: 0.00000986
Iteration 33/1000 | Loss: 0.00000985
Iteration 34/1000 | Loss: 0.00000984
Iteration 35/1000 | Loss: 0.00000983
Iteration 36/1000 | Loss: 0.00000983
Iteration 37/1000 | Loss: 0.00000982
Iteration 38/1000 | Loss: 0.00000981
Iteration 39/1000 | Loss: 0.00000981
Iteration 40/1000 | Loss: 0.00000980
Iteration 41/1000 | Loss: 0.00000980
Iteration 42/1000 | Loss: 0.00000980
Iteration 43/1000 | Loss: 0.00000979
Iteration 44/1000 | Loss: 0.00000978
Iteration 45/1000 | Loss: 0.00000978
Iteration 46/1000 | Loss: 0.00000978
Iteration 47/1000 | Loss: 0.00000977
Iteration 48/1000 | Loss: 0.00000977
Iteration 49/1000 | Loss: 0.00000976
Iteration 50/1000 | Loss: 0.00000976
Iteration 51/1000 | Loss: 0.00000976
Iteration 52/1000 | Loss: 0.00000975
Iteration 53/1000 | Loss: 0.00000975
Iteration 54/1000 | Loss: 0.00000975
Iteration 55/1000 | Loss: 0.00000974
Iteration 56/1000 | Loss: 0.00000974
Iteration 57/1000 | Loss: 0.00000974
Iteration 58/1000 | Loss: 0.00000973
Iteration 59/1000 | Loss: 0.00000972
Iteration 60/1000 | Loss: 0.00000971
Iteration 61/1000 | Loss: 0.00000971
Iteration 62/1000 | Loss: 0.00000971
Iteration 63/1000 | Loss: 0.00000970
Iteration 64/1000 | Loss: 0.00000970
Iteration 65/1000 | Loss: 0.00000970
Iteration 66/1000 | Loss: 0.00000968
Iteration 67/1000 | Loss: 0.00000968
Iteration 68/1000 | Loss: 0.00000968
Iteration 69/1000 | Loss: 0.00000968
Iteration 70/1000 | Loss: 0.00000968
Iteration 71/1000 | Loss: 0.00000968
Iteration 72/1000 | Loss: 0.00000968
Iteration 73/1000 | Loss: 0.00000967
Iteration 74/1000 | Loss: 0.00000966
Iteration 75/1000 | Loss: 0.00000965
Iteration 76/1000 | Loss: 0.00000965
Iteration 77/1000 | Loss: 0.00000965
Iteration 78/1000 | Loss: 0.00000965
Iteration 79/1000 | Loss: 0.00000965
Iteration 80/1000 | Loss: 0.00000965
Iteration 81/1000 | Loss: 0.00000964
Iteration 82/1000 | Loss: 0.00000963
Iteration 83/1000 | Loss: 0.00000963
Iteration 84/1000 | Loss: 0.00000963
Iteration 85/1000 | Loss: 0.00000963
Iteration 86/1000 | Loss: 0.00000963
Iteration 87/1000 | Loss: 0.00000962
Iteration 88/1000 | Loss: 0.00000962
Iteration 89/1000 | Loss: 0.00000962
Iteration 90/1000 | Loss: 0.00000962
Iteration 91/1000 | Loss: 0.00000962
Iteration 92/1000 | Loss: 0.00000962
Iteration 93/1000 | Loss: 0.00000961
Iteration 94/1000 | Loss: 0.00000961
Iteration 95/1000 | Loss: 0.00000961
Iteration 96/1000 | Loss: 0.00000961
Iteration 97/1000 | Loss: 0.00000961
Iteration 98/1000 | Loss: 0.00000961
Iteration 99/1000 | Loss: 0.00000961
Iteration 100/1000 | Loss: 0.00000960
Iteration 101/1000 | Loss: 0.00000960
Iteration 102/1000 | Loss: 0.00000960
Iteration 103/1000 | Loss: 0.00000960
Iteration 104/1000 | Loss: 0.00000960
Iteration 105/1000 | Loss: 0.00000959
Iteration 106/1000 | Loss: 0.00000959
Iteration 107/1000 | Loss: 0.00000959
Iteration 108/1000 | Loss: 0.00000959
Iteration 109/1000 | Loss: 0.00000959
Iteration 110/1000 | Loss: 0.00000959
Iteration 111/1000 | Loss: 0.00000959
Iteration 112/1000 | Loss: 0.00000959
Iteration 113/1000 | Loss: 0.00000958
Iteration 114/1000 | Loss: 0.00000958
Iteration 115/1000 | Loss: 0.00000958
Iteration 116/1000 | Loss: 0.00000958
Iteration 117/1000 | Loss: 0.00000958
Iteration 118/1000 | Loss: 0.00000958
Iteration 119/1000 | Loss: 0.00000957
Iteration 120/1000 | Loss: 0.00000957
Iteration 121/1000 | Loss: 0.00000957
Iteration 122/1000 | Loss: 0.00000957
Iteration 123/1000 | Loss: 0.00000957
Iteration 124/1000 | Loss: 0.00000957
Iteration 125/1000 | Loss: 0.00000957
Iteration 126/1000 | Loss: 0.00000957
Iteration 127/1000 | Loss: 0.00000957
Iteration 128/1000 | Loss: 0.00000956
Iteration 129/1000 | Loss: 0.00000956
Iteration 130/1000 | Loss: 0.00000956
Iteration 131/1000 | Loss: 0.00000956
Iteration 132/1000 | Loss: 0.00000956
Iteration 133/1000 | Loss: 0.00000956
Iteration 134/1000 | Loss: 0.00000956
Iteration 135/1000 | Loss: 0.00000956
Iteration 136/1000 | Loss: 0.00000956
Iteration 137/1000 | Loss: 0.00000955
Iteration 138/1000 | Loss: 0.00000955
Iteration 139/1000 | Loss: 0.00000955
Iteration 140/1000 | Loss: 0.00000955
Iteration 141/1000 | Loss: 0.00000955
Iteration 142/1000 | Loss: 0.00000955
Iteration 143/1000 | Loss: 0.00000955
Iteration 144/1000 | Loss: 0.00000954
Iteration 145/1000 | Loss: 0.00000954
Iteration 146/1000 | Loss: 0.00000954
Iteration 147/1000 | Loss: 0.00000954
Iteration 148/1000 | Loss: 0.00000954
Iteration 149/1000 | Loss: 0.00000954
Iteration 150/1000 | Loss: 0.00000954
Iteration 151/1000 | Loss: 0.00000954
Iteration 152/1000 | Loss: 0.00000953
Iteration 153/1000 | Loss: 0.00000953
Iteration 154/1000 | Loss: 0.00000953
Iteration 155/1000 | Loss: 0.00000953
Iteration 156/1000 | Loss: 0.00000953
Iteration 157/1000 | Loss: 0.00000953
Iteration 158/1000 | Loss: 0.00000953
Iteration 159/1000 | Loss: 0.00000953
Iteration 160/1000 | Loss: 0.00000953
Iteration 161/1000 | Loss: 0.00000953
Iteration 162/1000 | Loss: 0.00000953
Iteration 163/1000 | Loss: 0.00000953
Iteration 164/1000 | Loss: 0.00000952
Iteration 165/1000 | Loss: 0.00000952
Iteration 166/1000 | Loss: 0.00000951
Iteration 167/1000 | Loss: 0.00000951
Iteration 168/1000 | Loss: 0.00000951
Iteration 169/1000 | Loss: 0.00000950
Iteration 170/1000 | Loss: 0.00000950
Iteration 171/1000 | Loss: 0.00000950
Iteration 172/1000 | Loss: 0.00000950
Iteration 173/1000 | Loss: 0.00000949
Iteration 174/1000 | Loss: 0.00000949
Iteration 175/1000 | Loss: 0.00000949
Iteration 176/1000 | Loss: 0.00000948
Iteration 177/1000 | Loss: 0.00000948
Iteration 178/1000 | Loss: 0.00000948
Iteration 179/1000 | Loss: 0.00000947
Iteration 180/1000 | Loss: 0.00000947
Iteration 181/1000 | Loss: 0.00000947
Iteration 182/1000 | Loss: 0.00000946
Iteration 183/1000 | Loss: 0.00000946
Iteration 184/1000 | Loss: 0.00000946
Iteration 185/1000 | Loss: 0.00000946
Iteration 186/1000 | Loss: 0.00000946
Iteration 187/1000 | Loss: 0.00000946
Iteration 188/1000 | Loss: 0.00000946
Iteration 189/1000 | Loss: 0.00000946
Iteration 190/1000 | Loss: 0.00000946
Iteration 191/1000 | Loss: 0.00000945
Iteration 192/1000 | Loss: 0.00000945
Iteration 193/1000 | Loss: 0.00000945
Iteration 194/1000 | Loss: 0.00000945
Iteration 195/1000 | Loss: 0.00000945
Iteration 196/1000 | Loss: 0.00000945
Iteration 197/1000 | Loss: 0.00000945
Iteration 198/1000 | Loss: 0.00000945
Iteration 199/1000 | Loss: 0.00000945
Iteration 200/1000 | Loss: 0.00000944
Iteration 201/1000 | Loss: 0.00000944
Iteration 202/1000 | Loss: 0.00000944
Iteration 203/1000 | Loss: 0.00000944
Iteration 204/1000 | Loss: 0.00000944
Iteration 205/1000 | Loss: 0.00000944
Iteration 206/1000 | Loss: 0.00000944
Iteration 207/1000 | Loss: 0.00000944
Iteration 208/1000 | Loss: 0.00000944
Iteration 209/1000 | Loss: 0.00000943
Iteration 210/1000 | Loss: 0.00000943
Iteration 211/1000 | Loss: 0.00000943
Iteration 212/1000 | Loss: 0.00000943
Iteration 213/1000 | Loss: 0.00000943
Iteration 214/1000 | Loss: 0.00000943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [9.433718332729768e-06, 9.433718332729768e-06, 9.433718332729768e-06, 9.433718332729768e-06, 9.433718332729768e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.433718332729768e-06

Optimization complete. Final v2v error: 2.6327574253082275 mm

Highest mean error: 2.8674874305725098 mm for frame 139

Lowest mean error: 2.4435231685638428 mm for frame 18

Saving results

Total time: 44.940935373306274
