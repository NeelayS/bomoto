Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=119, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6664-6719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081032
Iteration 2/25 | Loss: 0.01081032
Iteration 3/25 | Loss: 0.01081032
Iteration 4/25 | Loss: 0.01081031
Iteration 5/25 | Loss: 0.01081031
Iteration 6/25 | Loss: 0.01081031
Iteration 7/25 | Loss: 0.01081031
Iteration 8/25 | Loss: 0.01081031
Iteration 9/25 | Loss: 0.01081031
Iteration 10/25 | Loss: 0.01081031
Iteration 11/25 | Loss: 0.01081031
Iteration 12/25 | Loss: 0.01081031
Iteration 13/25 | Loss: 0.01081031
Iteration 14/25 | Loss: 0.01081031
Iteration 15/25 | Loss: 0.01081031
Iteration 16/25 | Loss: 0.01081030
Iteration 17/25 | Loss: 0.01081030
Iteration 18/25 | Loss: 0.01081030
Iteration 19/25 | Loss: 0.01081030
Iteration 20/25 | Loss: 0.01081030
Iteration 21/25 | Loss: 0.01081030
Iteration 22/25 | Loss: 0.01081030
Iteration 23/25 | Loss: 0.01081030
Iteration 24/25 | Loss: 0.01081030
Iteration 25/25 | Loss: 0.01081030

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02257609
Iteration 2/25 | Loss: 0.05217693
Iteration 3/25 | Loss: 0.05213653
Iteration 4/25 | Loss: 0.05213653
Iteration 5/25 | Loss: 0.05213653
Iteration 6/25 | Loss: 0.05213652
Iteration 7/25 | Loss: 0.05213652
Iteration 8/25 | Loss: 0.05213652
Iteration 9/25 | Loss: 0.05213652
Iteration 10/25 | Loss: 0.05213652
Iteration 11/25 | Loss: 0.05213652
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.05213652178645134, 0.05213652178645134, 0.05213652178645134, 0.05213652178645134, 0.05213652178645134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.05213652178645134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05213652
Iteration 2/1000 | Loss: 0.00375306
Iteration 3/1000 | Loss: 0.00135557
Iteration 4/1000 | Loss: 0.00066666
Iteration 5/1000 | Loss: 0.00739481
Iteration 6/1000 | Loss: 0.02201596
Iteration 7/1000 | Loss: 0.00150146
Iteration 8/1000 | Loss: 0.00065498
Iteration 9/1000 | Loss: 0.00048030
Iteration 10/1000 | Loss: 0.00615828
Iteration 11/1000 | Loss: 0.00139362
Iteration 12/1000 | Loss: 0.00124976
Iteration 13/1000 | Loss: 0.00052534
Iteration 14/1000 | Loss: 0.00080177
Iteration 15/1000 | Loss: 0.00549670
Iteration 16/1000 | Loss: 0.00450273
Iteration 17/1000 | Loss: 0.00035019
Iteration 18/1000 | Loss: 0.00038529
Iteration 19/1000 | Loss: 0.00207424
Iteration 20/1000 | Loss: 0.00010845
Iteration 21/1000 | Loss: 0.00015740
Iteration 22/1000 | Loss: 0.00005489
Iteration 23/1000 | Loss: 0.00015301
Iteration 24/1000 | Loss: 0.00012221
Iteration 25/1000 | Loss: 0.00008130
Iteration 26/1000 | Loss: 0.00225814
Iteration 27/1000 | Loss: 0.00134543
Iteration 28/1000 | Loss: 0.00231570
Iteration 29/1000 | Loss: 0.00012994
Iteration 30/1000 | Loss: 0.00006421
Iteration 31/1000 | Loss: 0.00014965
Iteration 32/1000 | Loss: 0.00003588
Iteration 33/1000 | Loss: 0.00009431
Iteration 34/1000 | Loss: 0.00119036
Iteration 35/1000 | Loss: 0.00011103
Iteration 36/1000 | Loss: 0.00017838
Iteration 37/1000 | Loss: 0.00005032
Iteration 38/1000 | Loss: 0.00007364
Iteration 39/1000 | Loss: 0.00003811
Iteration 40/1000 | Loss: 0.00005726
Iteration 41/1000 | Loss: 0.00005703
Iteration 42/1000 | Loss: 0.00002450
Iteration 43/1000 | Loss: 0.00002471
Iteration 44/1000 | Loss: 0.00007342
Iteration 45/1000 | Loss: 0.00006930
Iteration 46/1000 | Loss: 0.00016048
Iteration 47/1000 | Loss: 0.00045877
Iteration 48/1000 | Loss: 0.00002287
Iteration 49/1000 | Loss: 0.00002849
Iteration 50/1000 | Loss: 0.00005301
Iteration 51/1000 | Loss: 0.00003066
Iteration 52/1000 | Loss: 0.00003175
Iteration 53/1000 | Loss: 0.00007747
Iteration 54/1000 | Loss: 0.00006323
Iteration 55/1000 | Loss: 0.00002675
Iteration 56/1000 | Loss: 0.00002354
Iteration 57/1000 | Loss: 0.00003858
Iteration 58/1000 | Loss: 0.00001817
Iteration 59/1000 | Loss: 0.00005260
Iteration 60/1000 | Loss: 0.00076769
Iteration 61/1000 | Loss: 0.00004302
Iteration 62/1000 | Loss: 0.00008981
Iteration 63/1000 | Loss: 0.00002546
Iteration 64/1000 | Loss: 0.00005293
Iteration 65/1000 | Loss: 0.00002457
Iteration 66/1000 | Loss: 0.00001780
Iteration 67/1000 | Loss: 0.00002220
Iteration 68/1000 | Loss: 0.00004475
Iteration 69/1000 | Loss: 0.00002009
Iteration 70/1000 | Loss: 0.00001726
Iteration 71/1000 | Loss: 0.00002314
Iteration 72/1000 | Loss: 0.00001867
Iteration 73/1000 | Loss: 0.00001719
Iteration 74/1000 | Loss: 0.00001719
Iteration 75/1000 | Loss: 0.00001718
Iteration 76/1000 | Loss: 0.00001718
Iteration 77/1000 | Loss: 0.00001718
Iteration 78/1000 | Loss: 0.00001718
Iteration 79/1000 | Loss: 0.00001718
Iteration 80/1000 | Loss: 0.00001722
Iteration 81/1000 | Loss: 0.00001718
Iteration 82/1000 | Loss: 0.00001718
Iteration 83/1000 | Loss: 0.00001718
Iteration 84/1000 | Loss: 0.00001718
Iteration 85/1000 | Loss: 0.00001718
Iteration 86/1000 | Loss: 0.00001718
Iteration 87/1000 | Loss: 0.00001718
Iteration 88/1000 | Loss: 0.00001718
Iteration 89/1000 | Loss: 0.00001717
Iteration 90/1000 | Loss: 0.00001717
Iteration 91/1000 | Loss: 0.00001717
Iteration 92/1000 | Loss: 0.00001717
Iteration 93/1000 | Loss: 0.00001717
Iteration 94/1000 | Loss: 0.00001717
Iteration 95/1000 | Loss: 0.00001717
Iteration 96/1000 | Loss: 0.00001717
Iteration 97/1000 | Loss: 0.00001717
Iteration 98/1000 | Loss: 0.00001717
Iteration 99/1000 | Loss: 0.00001717
Iteration 100/1000 | Loss: 0.00001716
Iteration 101/1000 | Loss: 0.00001716
Iteration 102/1000 | Loss: 0.00001716
Iteration 103/1000 | Loss: 0.00001716
Iteration 104/1000 | Loss: 0.00001715
Iteration 105/1000 | Loss: 0.00001715
Iteration 106/1000 | Loss: 0.00001715
Iteration 107/1000 | Loss: 0.00001715
Iteration 108/1000 | Loss: 0.00001715
Iteration 109/1000 | Loss: 0.00001714
Iteration 110/1000 | Loss: 0.00001714
Iteration 111/1000 | Loss: 0.00001713
Iteration 112/1000 | Loss: 0.00003868
Iteration 113/1000 | Loss: 0.00005395
Iteration 114/1000 | Loss: 0.00002001
Iteration 115/1000 | Loss: 0.00002254
Iteration 116/1000 | Loss: 0.00002680
Iteration 117/1000 | Loss: 0.00003791
Iteration 118/1000 | Loss: 0.00001897
Iteration 119/1000 | Loss: 0.00002570
Iteration 120/1000 | Loss: 0.00001717
Iteration 121/1000 | Loss: 0.00001693
Iteration 122/1000 | Loss: 0.00001693
Iteration 123/1000 | Loss: 0.00001693
Iteration 124/1000 | Loss: 0.00001693
Iteration 125/1000 | Loss: 0.00001693
Iteration 126/1000 | Loss: 0.00001693
Iteration 127/1000 | Loss: 0.00001693
Iteration 128/1000 | Loss: 0.00001693
Iteration 129/1000 | Loss: 0.00001693
Iteration 130/1000 | Loss: 0.00001693
Iteration 131/1000 | Loss: 0.00001692
Iteration 132/1000 | Loss: 0.00001692
Iteration 133/1000 | Loss: 0.00001691
Iteration 134/1000 | Loss: 0.00001691
Iteration 135/1000 | Loss: 0.00001691
Iteration 136/1000 | Loss: 0.00001691
Iteration 137/1000 | Loss: 0.00001691
Iteration 138/1000 | Loss: 0.00001691
Iteration 139/1000 | Loss: 0.00001691
Iteration 140/1000 | Loss: 0.00001691
Iteration 141/1000 | Loss: 0.00001691
Iteration 142/1000 | Loss: 0.00001691
Iteration 143/1000 | Loss: 0.00001691
Iteration 144/1000 | Loss: 0.00001691
Iteration 145/1000 | Loss: 0.00001691
Iteration 146/1000 | Loss: 0.00001691
Iteration 147/1000 | Loss: 0.00001691
Iteration 148/1000 | Loss: 0.00001691
Iteration 149/1000 | Loss: 0.00001691
Iteration 150/1000 | Loss: 0.00001691
Iteration 151/1000 | Loss: 0.00001691
Iteration 152/1000 | Loss: 0.00001691
Iteration 153/1000 | Loss: 0.00001691
Iteration 154/1000 | Loss: 0.00001691
Iteration 155/1000 | Loss: 0.00001691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.69086160894949e-05, 1.69086160894949e-05, 1.69086160894949e-05, 1.69086160894949e-05, 1.69086160894949e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.69086160894949e-05

Optimization complete. Final v2v error: 3.324514865875244 mm

Highest mean error: 5.024839878082275 mm for frame 68

Lowest mean error: 2.742183208465576 mm for frame 132

Saving results

Total time: 139.3239028453827
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00513212
Iteration 2/25 | Loss: 0.00095361
Iteration 3/25 | Loss: 0.00072101
Iteration 4/25 | Loss: 0.00066793
Iteration 5/25 | Loss: 0.00065621
Iteration 6/25 | Loss: 0.00065461
Iteration 7/25 | Loss: 0.00065461
Iteration 8/25 | Loss: 0.00065461
Iteration 9/25 | Loss: 0.00065461
Iteration 10/25 | Loss: 0.00065461
Iteration 11/25 | Loss: 0.00065461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006546069053001702, 0.0006546069053001702, 0.0006546069053001702, 0.0006546069053001702, 0.0006546069053001702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006546069053001702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81238043
Iteration 2/25 | Loss: 0.00020696
Iteration 3/25 | Loss: 0.00020696
Iteration 4/25 | Loss: 0.00020696
Iteration 5/25 | Loss: 0.00020695
Iteration 6/25 | Loss: 0.00020695
Iteration 7/25 | Loss: 0.00020695
Iteration 8/25 | Loss: 0.00020695
Iteration 9/25 | Loss: 0.00020695
Iteration 10/25 | Loss: 0.00020695
Iteration 11/25 | Loss: 0.00020695
Iteration 12/25 | Loss: 0.00020695
Iteration 13/25 | Loss: 0.00020695
Iteration 14/25 | Loss: 0.00020695
Iteration 15/25 | Loss: 0.00020695
Iteration 16/25 | Loss: 0.00020695
Iteration 17/25 | Loss: 0.00020695
Iteration 18/25 | Loss: 0.00020695
Iteration 19/25 | Loss: 0.00020695
Iteration 20/25 | Loss: 0.00020695
Iteration 21/25 | Loss: 0.00020695
Iteration 22/25 | Loss: 0.00020695
Iteration 23/25 | Loss: 0.00020695
Iteration 24/25 | Loss: 0.00020695
Iteration 25/25 | Loss: 0.00020695
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00020695306011475623, 0.00020695306011475623, 0.00020695306011475623, 0.00020695306011475623, 0.00020695306011475623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020695306011475623

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020695
Iteration 2/1000 | Loss: 0.00003191
Iteration 3/1000 | Loss: 0.00002469
Iteration 4/1000 | Loss: 0.00002330
Iteration 5/1000 | Loss: 0.00002229
Iteration 6/1000 | Loss: 0.00002148
Iteration 7/1000 | Loss: 0.00002072
Iteration 8/1000 | Loss: 0.00002030
Iteration 9/1000 | Loss: 0.00002002
Iteration 10/1000 | Loss: 0.00001975
Iteration 11/1000 | Loss: 0.00001957
Iteration 12/1000 | Loss: 0.00001949
Iteration 13/1000 | Loss: 0.00001944
Iteration 14/1000 | Loss: 0.00001941
Iteration 15/1000 | Loss: 0.00001938
Iteration 16/1000 | Loss: 0.00001924
Iteration 17/1000 | Loss: 0.00001921
Iteration 18/1000 | Loss: 0.00001920
Iteration 19/1000 | Loss: 0.00001914
Iteration 20/1000 | Loss: 0.00001914
Iteration 21/1000 | Loss: 0.00001911
Iteration 22/1000 | Loss: 0.00001909
Iteration 23/1000 | Loss: 0.00001909
Iteration 24/1000 | Loss: 0.00001907
Iteration 25/1000 | Loss: 0.00001907
Iteration 26/1000 | Loss: 0.00001907
Iteration 27/1000 | Loss: 0.00001907
Iteration 28/1000 | Loss: 0.00001907
Iteration 29/1000 | Loss: 0.00001907
Iteration 30/1000 | Loss: 0.00001907
Iteration 31/1000 | Loss: 0.00001907
Iteration 32/1000 | Loss: 0.00001907
Iteration 33/1000 | Loss: 0.00001904
Iteration 34/1000 | Loss: 0.00001904
Iteration 35/1000 | Loss: 0.00001903
Iteration 36/1000 | Loss: 0.00001903
Iteration 37/1000 | Loss: 0.00001903
Iteration 38/1000 | Loss: 0.00001902
Iteration 39/1000 | Loss: 0.00001901
Iteration 40/1000 | Loss: 0.00001900
Iteration 41/1000 | Loss: 0.00001900
Iteration 42/1000 | Loss: 0.00001899
Iteration 43/1000 | Loss: 0.00001899
Iteration 44/1000 | Loss: 0.00001899
Iteration 45/1000 | Loss: 0.00001899
Iteration 46/1000 | Loss: 0.00001898
Iteration 47/1000 | Loss: 0.00001897
Iteration 48/1000 | Loss: 0.00001896
Iteration 49/1000 | Loss: 0.00001896
Iteration 50/1000 | Loss: 0.00001896
Iteration 51/1000 | Loss: 0.00001896
Iteration 52/1000 | Loss: 0.00001895
Iteration 53/1000 | Loss: 0.00001895
Iteration 54/1000 | Loss: 0.00001895
Iteration 55/1000 | Loss: 0.00001894
Iteration 56/1000 | Loss: 0.00001894
Iteration 57/1000 | Loss: 0.00001894
Iteration 58/1000 | Loss: 0.00001894
Iteration 59/1000 | Loss: 0.00001894
Iteration 60/1000 | Loss: 0.00001893
Iteration 61/1000 | Loss: 0.00001893
Iteration 62/1000 | Loss: 0.00001893
Iteration 63/1000 | Loss: 0.00001893
Iteration 64/1000 | Loss: 0.00001892
Iteration 65/1000 | Loss: 0.00001892
Iteration 66/1000 | Loss: 0.00001892
Iteration 67/1000 | Loss: 0.00001891
Iteration 68/1000 | Loss: 0.00001891
Iteration 69/1000 | Loss: 0.00001891
Iteration 70/1000 | Loss: 0.00001890
Iteration 71/1000 | Loss: 0.00001890
Iteration 72/1000 | Loss: 0.00001890
Iteration 73/1000 | Loss: 0.00001889
Iteration 74/1000 | Loss: 0.00001889
Iteration 75/1000 | Loss: 0.00001889
Iteration 76/1000 | Loss: 0.00001889
Iteration 77/1000 | Loss: 0.00001889
Iteration 78/1000 | Loss: 0.00001888
Iteration 79/1000 | Loss: 0.00001888
Iteration 80/1000 | Loss: 0.00001888
Iteration 81/1000 | Loss: 0.00001888
Iteration 82/1000 | Loss: 0.00001886
Iteration 83/1000 | Loss: 0.00001886
Iteration 84/1000 | Loss: 0.00001886
Iteration 85/1000 | Loss: 0.00001885
Iteration 86/1000 | Loss: 0.00001885
Iteration 87/1000 | Loss: 0.00001885
Iteration 88/1000 | Loss: 0.00001885
Iteration 89/1000 | Loss: 0.00001884
Iteration 90/1000 | Loss: 0.00001884
Iteration 91/1000 | Loss: 0.00001884
Iteration 92/1000 | Loss: 0.00001884
Iteration 93/1000 | Loss: 0.00001883
Iteration 94/1000 | Loss: 0.00001883
Iteration 95/1000 | Loss: 0.00001883
Iteration 96/1000 | Loss: 0.00001883
Iteration 97/1000 | Loss: 0.00001883
Iteration 98/1000 | Loss: 0.00001883
Iteration 99/1000 | Loss: 0.00001883
Iteration 100/1000 | Loss: 0.00001883
Iteration 101/1000 | Loss: 0.00001883
Iteration 102/1000 | Loss: 0.00001883
Iteration 103/1000 | Loss: 0.00001883
Iteration 104/1000 | Loss: 0.00001882
Iteration 105/1000 | Loss: 0.00001882
Iteration 106/1000 | Loss: 0.00001882
Iteration 107/1000 | Loss: 0.00001882
Iteration 108/1000 | Loss: 0.00001882
Iteration 109/1000 | Loss: 0.00001881
Iteration 110/1000 | Loss: 0.00001881
Iteration 111/1000 | Loss: 0.00001881
Iteration 112/1000 | Loss: 0.00001881
Iteration 113/1000 | Loss: 0.00001881
Iteration 114/1000 | Loss: 0.00001881
Iteration 115/1000 | Loss: 0.00001881
Iteration 116/1000 | Loss: 0.00001881
Iteration 117/1000 | Loss: 0.00001881
Iteration 118/1000 | Loss: 0.00001881
Iteration 119/1000 | Loss: 0.00001881
Iteration 120/1000 | Loss: 0.00001881
Iteration 121/1000 | Loss: 0.00001881
Iteration 122/1000 | Loss: 0.00001881
Iteration 123/1000 | Loss: 0.00001880
Iteration 124/1000 | Loss: 0.00001880
Iteration 125/1000 | Loss: 0.00001880
Iteration 126/1000 | Loss: 0.00001880
Iteration 127/1000 | Loss: 0.00001880
Iteration 128/1000 | Loss: 0.00001880
Iteration 129/1000 | Loss: 0.00001880
Iteration 130/1000 | Loss: 0.00001880
Iteration 131/1000 | Loss: 0.00001880
Iteration 132/1000 | Loss: 0.00001880
Iteration 133/1000 | Loss: 0.00001880
Iteration 134/1000 | Loss: 0.00001880
Iteration 135/1000 | Loss: 0.00001880
Iteration 136/1000 | Loss: 0.00001880
Iteration 137/1000 | Loss: 0.00001879
Iteration 138/1000 | Loss: 0.00001879
Iteration 139/1000 | Loss: 0.00001879
Iteration 140/1000 | Loss: 0.00001879
Iteration 141/1000 | Loss: 0.00001879
Iteration 142/1000 | Loss: 0.00001879
Iteration 143/1000 | Loss: 0.00001879
Iteration 144/1000 | Loss: 0.00001879
Iteration 145/1000 | Loss: 0.00001879
Iteration 146/1000 | Loss: 0.00001879
Iteration 147/1000 | Loss: 0.00001879
Iteration 148/1000 | Loss: 0.00001879
Iteration 149/1000 | Loss: 0.00001879
Iteration 150/1000 | Loss: 0.00001879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.8790711692417972e-05, 1.8790711692417972e-05, 1.8790711692417972e-05, 1.8790711692417972e-05, 1.8790711692417972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8790711692417972e-05

Optimization complete. Final v2v error: 3.6725175380706787 mm

Highest mean error: 4.636046886444092 mm for frame 265

Lowest mean error: 3.570274591445923 mm for frame 103

Saving results

Total time: 46.78964948654175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413211
Iteration 2/25 | Loss: 0.00078751
Iteration 3/25 | Loss: 0.00066313
Iteration 4/25 | Loss: 0.00064656
Iteration 5/25 | Loss: 0.00064337
Iteration 6/25 | Loss: 0.00064329
Iteration 7/25 | Loss: 0.00064329
Iteration 8/25 | Loss: 0.00064329
Iteration 9/25 | Loss: 0.00064329
Iteration 10/25 | Loss: 0.00064329
Iteration 11/25 | Loss: 0.00064329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006432921509258449, 0.0006432921509258449, 0.0006432921509258449, 0.0006432921509258449, 0.0006432921509258449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006432921509258449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47410798
Iteration 2/25 | Loss: 0.00033465
Iteration 3/25 | Loss: 0.00033465
Iteration 4/25 | Loss: 0.00033465
Iteration 5/25 | Loss: 0.00033465
Iteration 6/25 | Loss: 0.00033465
Iteration 7/25 | Loss: 0.00033465
Iteration 8/25 | Loss: 0.00033465
Iteration 9/25 | Loss: 0.00033465
Iteration 10/25 | Loss: 0.00033465
Iteration 11/25 | Loss: 0.00033465
Iteration 12/25 | Loss: 0.00033465
Iteration 13/25 | Loss: 0.00033465
Iteration 14/25 | Loss: 0.00033465
Iteration 15/25 | Loss: 0.00033465
Iteration 16/25 | Loss: 0.00033465
Iteration 17/25 | Loss: 0.00033465
Iteration 18/25 | Loss: 0.00033465
Iteration 19/25 | Loss: 0.00033465
Iteration 20/25 | Loss: 0.00033465
Iteration 21/25 | Loss: 0.00033465
Iteration 22/25 | Loss: 0.00033465
Iteration 23/25 | Loss: 0.00033465
Iteration 24/25 | Loss: 0.00033465
Iteration 25/25 | Loss: 0.00033465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033465
Iteration 2/1000 | Loss: 0.00001934
Iteration 3/1000 | Loss: 0.00001498
Iteration 4/1000 | Loss: 0.00001443
Iteration 5/1000 | Loss: 0.00001434
Iteration 6/1000 | Loss: 0.00001401
Iteration 7/1000 | Loss: 0.00001365
Iteration 8/1000 | Loss: 0.00001356
Iteration 9/1000 | Loss: 0.00001341
Iteration 10/1000 | Loss: 0.00001338
Iteration 11/1000 | Loss: 0.00001335
Iteration 12/1000 | Loss: 0.00001330
Iteration 13/1000 | Loss: 0.00001329
Iteration 14/1000 | Loss: 0.00001329
Iteration 15/1000 | Loss: 0.00001327
Iteration 16/1000 | Loss: 0.00001327
Iteration 17/1000 | Loss: 0.00001327
Iteration 18/1000 | Loss: 0.00001327
Iteration 19/1000 | Loss: 0.00001327
Iteration 20/1000 | Loss: 0.00001327
Iteration 21/1000 | Loss: 0.00001326
Iteration 22/1000 | Loss: 0.00001326
Iteration 23/1000 | Loss: 0.00001326
Iteration 24/1000 | Loss: 0.00001326
Iteration 25/1000 | Loss: 0.00001326
Iteration 26/1000 | Loss: 0.00001326
Iteration 27/1000 | Loss: 0.00001325
Iteration 28/1000 | Loss: 0.00001325
Iteration 29/1000 | Loss: 0.00001324
Iteration 30/1000 | Loss: 0.00001324
Iteration 31/1000 | Loss: 0.00001324
Iteration 32/1000 | Loss: 0.00001323
Iteration 33/1000 | Loss: 0.00001323
Iteration 34/1000 | Loss: 0.00001323
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001322
Iteration 37/1000 | Loss: 0.00001322
Iteration 38/1000 | Loss: 0.00001321
Iteration 39/1000 | Loss: 0.00001321
Iteration 40/1000 | Loss: 0.00001321
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001321
Iteration 47/1000 | Loss: 0.00001321
Iteration 48/1000 | Loss: 0.00001321
Iteration 49/1000 | Loss: 0.00001321
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001320
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001320
Iteration 54/1000 | Loss: 0.00001320
Iteration 55/1000 | Loss: 0.00001320
Iteration 56/1000 | Loss: 0.00001319
Iteration 57/1000 | Loss: 0.00001319
Iteration 58/1000 | Loss: 0.00001319
Iteration 59/1000 | Loss: 0.00001319
Iteration 60/1000 | Loss: 0.00001319
Iteration 61/1000 | Loss: 0.00001318
Iteration 62/1000 | Loss: 0.00001317
Iteration 63/1000 | Loss: 0.00001317
Iteration 64/1000 | Loss: 0.00001317
Iteration 65/1000 | Loss: 0.00001317
Iteration 66/1000 | Loss: 0.00001317
Iteration 67/1000 | Loss: 0.00001317
Iteration 68/1000 | Loss: 0.00001317
Iteration 69/1000 | Loss: 0.00001317
Iteration 70/1000 | Loss: 0.00001317
Iteration 71/1000 | Loss: 0.00001317
Iteration 72/1000 | Loss: 0.00001317
Iteration 73/1000 | Loss: 0.00001317
Iteration 74/1000 | Loss: 0.00001317
Iteration 75/1000 | Loss: 0.00001317
Iteration 76/1000 | Loss: 0.00001317
Iteration 77/1000 | Loss: 0.00001317
Iteration 78/1000 | Loss: 0.00001317
Iteration 79/1000 | Loss: 0.00001317
Iteration 80/1000 | Loss: 0.00001317
Iteration 81/1000 | Loss: 0.00001317
Iteration 82/1000 | Loss: 0.00001317
Iteration 83/1000 | Loss: 0.00001317
Iteration 84/1000 | Loss: 0.00001317
Iteration 85/1000 | Loss: 0.00001317
Iteration 86/1000 | Loss: 0.00001317
Iteration 87/1000 | Loss: 0.00001317
Iteration 88/1000 | Loss: 0.00001317
Iteration 89/1000 | Loss: 0.00001317
Iteration 90/1000 | Loss: 0.00001317
Iteration 91/1000 | Loss: 0.00001317
Iteration 92/1000 | Loss: 0.00001317
Iteration 93/1000 | Loss: 0.00001317
Iteration 94/1000 | Loss: 0.00001317
Iteration 95/1000 | Loss: 0.00001317
Iteration 96/1000 | Loss: 0.00001317
Iteration 97/1000 | Loss: 0.00001317
Iteration 98/1000 | Loss: 0.00001317
Iteration 99/1000 | Loss: 0.00001317
Iteration 100/1000 | Loss: 0.00001317
Iteration 101/1000 | Loss: 0.00001317
Iteration 102/1000 | Loss: 0.00001317
Iteration 103/1000 | Loss: 0.00001317
Iteration 104/1000 | Loss: 0.00001317
Iteration 105/1000 | Loss: 0.00001317
Iteration 106/1000 | Loss: 0.00001317
Iteration 107/1000 | Loss: 0.00001317
Iteration 108/1000 | Loss: 0.00001317
Iteration 109/1000 | Loss: 0.00001317
Iteration 110/1000 | Loss: 0.00001317
Iteration 111/1000 | Loss: 0.00001317
Iteration 112/1000 | Loss: 0.00001317
Iteration 113/1000 | Loss: 0.00001317
Iteration 114/1000 | Loss: 0.00001317
Iteration 115/1000 | Loss: 0.00001317
Iteration 116/1000 | Loss: 0.00001317
Iteration 117/1000 | Loss: 0.00001317
Iteration 118/1000 | Loss: 0.00001317
Iteration 119/1000 | Loss: 0.00001317
Iteration 120/1000 | Loss: 0.00001317
Iteration 121/1000 | Loss: 0.00001317
Iteration 122/1000 | Loss: 0.00001317
Iteration 123/1000 | Loss: 0.00001317
Iteration 124/1000 | Loss: 0.00001317
Iteration 125/1000 | Loss: 0.00001317
Iteration 126/1000 | Loss: 0.00001317
Iteration 127/1000 | Loss: 0.00001317
Iteration 128/1000 | Loss: 0.00001317
Iteration 129/1000 | Loss: 0.00001317
Iteration 130/1000 | Loss: 0.00001317
Iteration 131/1000 | Loss: 0.00001317
Iteration 132/1000 | Loss: 0.00001317
Iteration 133/1000 | Loss: 0.00001317
Iteration 134/1000 | Loss: 0.00001317
Iteration 135/1000 | Loss: 0.00001317
Iteration 136/1000 | Loss: 0.00001317
Iteration 137/1000 | Loss: 0.00001317
Iteration 138/1000 | Loss: 0.00001317
Iteration 139/1000 | Loss: 0.00001317
Iteration 140/1000 | Loss: 0.00001317
Iteration 141/1000 | Loss: 0.00001317
Iteration 142/1000 | Loss: 0.00001317
Iteration 143/1000 | Loss: 0.00001317
Iteration 144/1000 | Loss: 0.00001317
Iteration 145/1000 | Loss: 0.00001317
Iteration 146/1000 | Loss: 0.00001317
Iteration 147/1000 | Loss: 0.00001317
Iteration 148/1000 | Loss: 0.00001317
Iteration 149/1000 | Loss: 0.00001317
Iteration 150/1000 | Loss: 0.00001317
Iteration 151/1000 | Loss: 0.00001317
Iteration 152/1000 | Loss: 0.00001317
Iteration 153/1000 | Loss: 0.00001317
Iteration 154/1000 | Loss: 0.00001317
Iteration 155/1000 | Loss: 0.00001317
Iteration 156/1000 | Loss: 0.00001317
Iteration 157/1000 | Loss: 0.00001317
Iteration 158/1000 | Loss: 0.00001317
Iteration 159/1000 | Loss: 0.00001317
Iteration 160/1000 | Loss: 0.00001317
Iteration 161/1000 | Loss: 0.00001317
Iteration 162/1000 | Loss: 0.00001317
Iteration 163/1000 | Loss: 0.00001317
Iteration 164/1000 | Loss: 0.00001317
Iteration 165/1000 | Loss: 0.00001317
Iteration 166/1000 | Loss: 0.00001317
Iteration 167/1000 | Loss: 0.00001317
Iteration 168/1000 | Loss: 0.00001317
Iteration 169/1000 | Loss: 0.00001317
Iteration 170/1000 | Loss: 0.00001317
Iteration 171/1000 | Loss: 0.00001317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.316669749940047e-05, 1.316669749940047e-05, 1.316669749940047e-05, 1.316669749940047e-05, 1.316669749940047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.316669749940047e-05

Optimization complete. Final v2v error: 3.0589213371276855 mm

Highest mean error: 3.0934879779815674 mm for frame 86

Lowest mean error: 3.0035054683685303 mm for frame 220

Saving results

Total time: 29.746522903442383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00366559
Iteration 2/25 | Loss: 0.00073776
Iteration 3/25 | Loss: 0.00060006
Iteration 4/25 | Loss: 0.00057989
Iteration 5/25 | Loss: 0.00057604
Iteration 6/25 | Loss: 0.00057494
Iteration 7/25 | Loss: 0.00057478
Iteration 8/25 | Loss: 0.00057478
Iteration 9/25 | Loss: 0.00057478
Iteration 10/25 | Loss: 0.00057478
Iteration 11/25 | Loss: 0.00057478
Iteration 12/25 | Loss: 0.00057478
Iteration 13/25 | Loss: 0.00057478
Iteration 14/25 | Loss: 0.00057478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0005747756222262979, 0.0005747756222262979, 0.0005747756222262979, 0.0005747756222262979, 0.0005747756222262979]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005747756222262979

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44632804
Iteration 2/25 | Loss: 0.00028270
Iteration 3/25 | Loss: 0.00028269
Iteration 4/25 | Loss: 0.00028269
Iteration 5/25 | Loss: 0.00028269
Iteration 6/25 | Loss: 0.00028269
Iteration 7/25 | Loss: 0.00028269
Iteration 8/25 | Loss: 0.00028269
Iteration 9/25 | Loss: 0.00028269
Iteration 10/25 | Loss: 0.00028269
Iteration 11/25 | Loss: 0.00028269
Iteration 12/25 | Loss: 0.00028269
Iteration 13/25 | Loss: 0.00028269
Iteration 14/25 | Loss: 0.00028269
Iteration 15/25 | Loss: 0.00028269
Iteration 16/25 | Loss: 0.00028269
Iteration 17/25 | Loss: 0.00028269
Iteration 18/25 | Loss: 0.00028269
Iteration 19/25 | Loss: 0.00028269
Iteration 20/25 | Loss: 0.00028269
Iteration 21/25 | Loss: 0.00028269
Iteration 22/25 | Loss: 0.00028269
Iteration 23/25 | Loss: 0.00028269
Iteration 24/25 | Loss: 0.00028269
Iteration 25/25 | Loss: 0.00028269

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028269
Iteration 2/1000 | Loss: 0.00002081
Iteration 3/1000 | Loss: 0.00001173
Iteration 4/1000 | Loss: 0.00001070
Iteration 5/1000 | Loss: 0.00001016
Iteration 6/1000 | Loss: 0.00000988
Iteration 7/1000 | Loss: 0.00000968
Iteration 8/1000 | Loss: 0.00000962
Iteration 9/1000 | Loss: 0.00000962
Iteration 10/1000 | Loss: 0.00000961
Iteration 11/1000 | Loss: 0.00000961
Iteration 12/1000 | Loss: 0.00000956
Iteration 13/1000 | Loss: 0.00000956
Iteration 14/1000 | Loss: 0.00000955
Iteration 15/1000 | Loss: 0.00000951
Iteration 16/1000 | Loss: 0.00000950
Iteration 17/1000 | Loss: 0.00000950
Iteration 18/1000 | Loss: 0.00000950
Iteration 19/1000 | Loss: 0.00000950
Iteration 20/1000 | Loss: 0.00000949
Iteration 21/1000 | Loss: 0.00000949
Iteration 22/1000 | Loss: 0.00000949
Iteration 23/1000 | Loss: 0.00000948
Iteration 24/1000 | Loss: 0.00000948
Iteration 25/1000 | Loss: 0.00000947
Iteration 26/1000 | Loss: 0.00000943
Iteration 27/1000 | Loss: 0.00000943
Iteration 28/1000 | Loss: 0.00000943
Iteration 29/1000 | Loss: 0.00000943
Iteration 30/1000 | Loss: 0.00000940
Iteration 31/1000 | Loss: 0.00000940
Iteration 32/1000 | Loss: 0.00000938
Iteration 33/1000 | Loss: 0.00000938
Iteration 34/1000 | Loss: 0.00000937
Iteration 35/1000 | Loss: 0.00000937
Iteration 36/1000 | Loss: 0.00000937
Iteration 37/1000 | Loss: 0.00000936
Iteration 38/1000 | Loss: 0.00000936
Iteration 39/1000 | Loss: 0.00000935
Iteration 40/1000 | Loss: 0.00000935
Iteration 41/1000 | Loss: 0.00000934
Iteration 42/1000 | Loss: 0.00000933
Iteration 43/1000 | Loss: 0.00000933
Iteration 44/1000 | Loss: 0.00000933
Iteration 45/1000 | Loss: 0.00000933
Iteration 46/1000 | Loss: 0.00000932
Iteration 47/1000 | Loss: 0.00000932
Iteration 48/1000 | Loss: 0.00000931
Iteration 49/1000 | Loss: 0.00000930
Iteration 50/1000 | Loss: 0.00000930
Iteration 51/1000 | Loss: 0.00000929
Iteration 52/1000 | Loss: 0.00000929
Iteration 53/1000 | Loss: 0.00000929
Iteration 54/1000 | Loss: 0.00000929
Iteration 55/1000 | Loss: 0.00000928
Iteration 56/1000 | Loss: 0.00000928
Iteration 57/1000 | Loss: 0.00000928
Iteration 58/1000 | Loss: 0.00000927
Iteration 59/1000 | Loss: 0.00000927
Iteration 60/1000 | Loss: 0.00000926
Iteration 61/1000 | Loss: 0.00000926
Iteration 62/1000 | Loss: 0.00000923
Iteration 63/1000 | Loss: 0.00000923
Iteration 64/1000 | Loss: 0.00000921
Iteration 65/1000 | Loss: 0.00000921
Iteration 66/1000 | Loss: 0.00000920
Iteration 67/1000 | Loss: 0.00000920
Iteration 68/1000 | Loss: 0.00000919
Iteration 69/1000 | Loss: 0.00000919
Iteration 70/1000 | Loss: 0.00000919
Iteration 71/1000 | Loss: 0.00000918
Iteration 72/1000 | Loss: 0.00000918
Iteration 73/1000 | Loss: 0.00000917
Iteration 74/1000 | Loss: 0.00000917
Iteration 75/1000 | Loss: 0.00000917
Iteration 76/1000 | Loss: 0.00000917
Iteration 77/1000 | Loss: 0.00000917
Iteration 78/1000 | Loss: 0.00000916
Iteration 79/1000 | Loss: 0.00000916
Iteration 80/1000 | Loss: 0.00000916
Iteration 81/1000 | Loss: 0.00000916
Iteration 82/1000 | Loss: 0.00000916
Iteration 83/1000 | Loss: 0.00000916
Iteration 84/1000 | Loss: 0.00000915
Iteration 85/1000 | Loss: 0.00000915
Iteration 86/1000 | Loss: 0.00000915
Iteration 87/1000 | Loss: 0.00000915
Iteration 88/1000 | Loss: 0.00000915
Iteration 89/1000 | Loss: 0.00000914
Iteration 90/1000 | Loss: 0.00000914
Iteration 91/1000 | Loss: 0.00000914
Iteration 92/1000 | Loss: 0.00000914
Iteration 93/1000 | Loss: 0.00000913
Iteration 94/1000 | Loss: 0.00000913
Iteration 95/1000 | Loss: 0.00000913
Iteration 96/1000 | Loss: 0.00000913
Iteration 97/1000 | Loss: 0.00000913
Iteration 98/1000 | Loss: 0.00000913
Iteration 99/1000 | Loss: 0.00000913
Iteration 100/1000 | Loss: 0.00000913
Iteration 101/1000 | Loss: 0.00000912
Iteration 102/1000 | Loss: 0.00000912
Iteration 103/1000 | Loss: 0.00000912
Iteration 104/1000 | Loss: 0.00000912
Iteration 105/1000 | Loss: 0.00000912
Iteration 106/1000 | Loss: 0.00000912
Iteration 107/1000 | Loss: 0.00000912
Iteration 108/1000 | Loss: 0.00000912
Iteration 109/1000 | Loss: 0.00000911
Iteration 110/1000 | Loss: 0.00000911
Iteration 111/1000 | Loss: 0.00000911
Iteration 112/1000 | Loss: 0.00000910
Iteration 113/1000 | Loss: 0.00000910
Iteration 114/1000 | Loss: 0.00000910
Iteration 115/1000 | Loss: 0.00000909
Iteration 116/1000 | Loss: 0.00000909
Iteration 117/1000 | Loss: 0.00000909
Iteration 118/1000 | Loss: 0.00000909
Iteration 119/1000 | Loss: 0.00000908
Iteration 120/1000 | Loss: 0.00000908
Iteration 121/1000 | Loss: 0.00000908
Iteration 122/1000 | Loss: 0.00000908
Iteration 123/1000 | Loss: 0.00000908
Iteration 124/1000 | Loss: 0.00000908
Iteration 125/1000 | Loss: 0.00000908
Iteration 126/1000 | Loss: 0.00000908
Iteration 127/1000 | Loss: 0.00000908
Iteration 128/1000 | Loss: 0.00000907
Iteration 129/1000 | Loss: 0.00000907
Iteration 130/1000 | Loss: 0.00000907
Iteration 131/1000 | Loss: 0.00000907
Iteration 132/1000 | Loss: 0.00000906
Iteration 133/1000 | Loss: 0.00000906
Iteration 134/1000 | Loss: 0.00000906
Iteration 135/1000 | Loss: 0.00000906
Iteration 136/1000 | Loss: 0.00000905
Iteration 137/1000 | Loss: 0.00000905
Iteration 138/1000 | Loss: 0.00000905
Iteration 139/1000 | Loss: 0.00000905
Iteration 140/1000 | Loss: 0.00000905
Iteration 141/1000 | Loss: 0.00000904
Iteration 142/1000 | Loss: 0.00000904
Iteration 143/1000 | Loss: 0.00000904
Iteration 144/1000 | Loss: 0.00000904
Iteration 145/1000 | Loss: 0.00000904
Iteration 146/1000 | Loss: 0.00000904
Iteration 147/1000 | Loss: 0.00000903
Iteration 148/1000 | Loss: 0.00000903
Iteration 149/1000 | Loss: 0.00000903
Iteration 150/1000 | Loss: 0.00000903
Iteration 151/1000 | Loss: 0.00000903
Iteration 152/1000 | Loss: 0.00000903
Iteration 153/1000 | Loss: 0.00000903
Iteration 154/1000 | Loss: 0.00000903
Iteration 155/1000 | Loss: 0.00000903
Iteration 156/1000 | Loss: 0.00000903
Iteration 157/1000 | Loss: 0.00000903
Iteration 158/1000 | Loss: 0.00000902
Iteration 159/1000 | Loss: 0.00000902
Iteration 160/1000 | Loss: 0.00000902
Iteration 161/1000 | Loss: 0.00000902
Iteration 162/1000 | Loss: 0.00000902
Iteration 163/1000 | Loss: 0.00000902
Iteration 164/1000 | Loss: 0.00000901
Iteration 165/1000 | Loss: 0.00000901
Iteration 166/1000 | Loss: 0.00000901
Iteration 167/1000 | Loss: 0.00000901
Iteration 168/1000 | Loss: 0.00000901
Iteration 169/1000 | Loss: 0.00000901
Iteration 170/1000 | Loss: 0.00000901
Iteration 171/1000 | Loss: 0.00000901
Iteration 172/1000 | Loss: 0.00000901
Iteration 173/1000 | Loss: 0.00000901
Iteration 174/1000 | Loss: 0.00000901
Iteration 175/1000 | Loss: 0.00000901
Iteration 176/1000 | Loss: 0.00000901
Iteration 177/1000 | Loss: 0.00000901
Iteration 178/1000 | Loss: 0.00000901
Iteration 179/1000 | Loss: 0.00000901
Iteration 180/1000 | Loss: 0.00000901
Iteration 181/1000 | Loss: 0.00000901
Iteration 182/1000 | Loss: 0.00000901
Iteration 183/1000 | Loss: 0.00000901
Iteration 184/1000 | Loss: 0.00000901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [9.005215360957664e-06, 9.005215360957664e-06, 9.005215360957664e-06, 9.005215360957664e-06, 9.005215360957664e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.005215360957664e-06

Optimization complete. Final v2v error: 2.558840274810791 mm

Highest mean error: 2.9958674907684326 mm for frame 81

Lowest mean error: 2.4500248432159424 mm for frame 102

Saving results

Total time: 37.41293525695801
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997566
Iteration 2/25 | Loss: 0.00245869
Iteration 3/25 | Loss: 0.00173450
Iteration 4/25 | Loss: 0.00144887
Iteration 5/25 | Loss: 0.00120986
Iteration 6/25 | Loss: 0.00115471
Iteration 7/25 | Loss: 0.00114401
Iteration 8/25 | Loss: 0.00108327
Iteration 9/25 | Loss: 0.00103816
Iteration 10/25 | Loss: 0.00101130
Iteration 11/25 | Loss: 0.00099801
Iteration 12/25 | Loss: 0.00098575
Iteration 13/25 | Loss: 0.00097562
Iteration 14/25 | Loss: 0.00096809
Iteration 15/25 | Loss: 0.00097373
Iteration 16/25 | Loss: 0.00096154
Iteration 17/25 | Loss: 0.00095099
Iteration 18/25 | Loss: 0.00093634
Iteration 19/25 | Loss: 0.00093372
Iteration 20/25 | Loss: 0.00093192
Iteration 21/25 | Loss: 0.00092924
Iteration 22/25 | Loss: 0.00093301
Iteration 23/25 | Loss: 0.00093465
Iteration 24/25 | Loss: 0.00093032
Iteration 25/25 | Loss: 0.00093101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.51252079
Iteration 2/25 | Loss: 0.00266319
Iteration 3/25 | Loss: 0.00266319
Iteration 4/25 | Loss: 0.00266318
Iteration 5/25 | Loss: 0.00266318
Iteration 6/25 | Loss: 0.00266318
Iteration 7/25 | Loss: 0.00266318
Iteration 8/25 | Loss: 0.00266318
Iteration 9/25 | Loss: 0.00266318
Iteration 10/25 | Loss: 0.00266318
Iteration 11/25 | Loss: 0.00266318
Iteration 12/25 | Loss: 0.00266318
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0026631816290318966, 0.0026631816290318966, 0.0026631816290318966, 0.0026631816290318966, 0.0026631816290318966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026631816290318966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266318
Iteration 2/1000 | Loss: 0.00183832
Iteration 3/1000 | Loss: 0.00312580
Iteration 4/1000 | Loss: 0.00386004
Iteration 5/1000 | Loss: 0.00055285
Iteration 6/1000 | Loss: 0.00124797
Iteration 7/1000 | Loss: 0.00077380
Iteration 8/1000 | Loss: 0.00195281
Iteration 9/1000 | Loss: 0.00262926
Iteration 10/1000 | Loss: 0.00134890
Iteration 11/1000 | Loss: 0.00156759
Iteration 12/1000 | Loss: 0.00019030
Iteration 13/1000 | Loss: 0.00023321
Iteration 14/1000 | Loss: 0.00128061
Iteration 15/1000 | Loss: 0.00221608
Iteration 16/1000 | Loss: 0.00061440
Iteration 17/1000 | Loss: 0.00054151
Iteration 18/1000 | Loss: 0.00094583
Iteration 19/1000 | Loss: 0.00147093
Iteration 20/1000 | Loss: 0.00050585
Iteration 21/1000 | Loss: 0.00013776
Iteration 22/1000 | Loss: 0.00023117
Iteration 23/1000 | Loss: 0.00072716
Iteration 24/1000 | Loss: 0.00033851
Iteration 25/1000 | Loss: 0.00011577
Iteration 26/1000 | Loss: 0.00034830
Iteration 27/1000 | Loss: 0.00105747
Iteration 28/1000 | Loss: 0.00068526
Iteration 29/1000 | Loss: 0.00185052
Iteration 30/1000 | Loss: 0.00406804
Iteration 31/1000 | Loss: 0.00230535
Iteration 32/1000 | Loss: 0.00502972
Iteration 33/1000 | Loss: 0.00228245
Iteration 34/1000 | Loss: 0.00357724
Iteration 35/1000 | Loss: 0.00112691
Iteration 36/1000 | Loss: 0.00073464
Iteration 37/1000 | Loss: 0.00043499
Iteration 38/1000 | Loss: 0.00047975
Iteration 39/1000 | Loss: 0.00108405
Iteration 40/1000 | Loss: 0.00221350
Iteration 41/1000 | Loss: 0.00135749
Iteration 42/1000 | Loss: 0.00199482
Iteration 43/1000 | Loss: 0.00151630
Iteration 44/1000 | Loss: 0.00161529
Iteration 45/1000 | Loss: 0.00101319
Iteration 46/1000 | Loss: 0.00008444
Iteration 47/1000 | Loss: 0.00118497
Iteration 48/1000 | Loss: 0.00022171
Iteration 49/1000 | Loss: 0.00019658
Iteration 50/1000 | Loss: 0.00007180
Iteration 51/1000 | Loss: 0.00031124
Iteration 52/1000 | Loss: 0.00041689
Iteration 53/1000 | Loss: 0.00047981
Iteration 54/1000 | Loss: 0.00044002
Iteration 55/1000 | Loss: 0.00043706
Iteration 56/1000 | Loss: 0.00055461
Iteration 57/1000 | Loss: 0.00056430
Iteration 58/1000 | Loss: 0.00055766
Iteration 59/1000 | Loss: 0.00054412
Iteration 60/1000 | Loss: 0.00056804
Iteration 61/1000 | Loss: 0.00042278
Iteration 62/1000 | Loss: 0.00009583
Iteration 63/1000 | Loss: 0.00037630
Iteration 64/1000 | Loss: 0.00006371
Iteration 65/1000 | Loss: 0.00004803
Iteration 66/1000 | Loss: 0.00016268
Iteration 67/1000 | Loss: 0.00008900
Iteration 68/1000 | Loss: 0.00003832
Iteration 69/1000 | Loss: 0.00005611
Iteration 70/1000 | Loss: 0.00037629
Iteration 71/1000 | Loss: 0.00061972
Iteration 72/1000 | Loss: 0.00069953
Iteration 73/1000 | Loss: 0.00057146
Iteration 74/1000 | Loss: 0.00039557
Iteration 75/1000 | Loss: 0.00084486
Iteration 76/1000 | Loss: 0.00022927
Iteration 77/1000 | Loss: 0.00021639
Iteration 78/1000 | Loss: 0.00015050
Iteration 79/1000 | Loss: 0.00018754
Iteration 80/1000 | Loss: 0.00041430
Iteration 81/1000 | Loss: 0.00037014
Iteration 82/1000 | Loss: 0.00016237
Iteration 83/1000 | Loss: 0.00007066
Iteration 84/1000 | Loss: 0.00043654
Iteration 85/1000 | Loss: 0.00041760
Iteration 86/1000 | Loss: 0.00008451
Iteration 87/1000 | Loss: 0.00062453
Iteration 88/1000 | Loss: 0.00011467
Iteration 89/1000 | Loss: 0.00060110
Iteration 90/1000 | Loss: 0.00009688
Iteration 91/1000 | Loss: 0.00004144
Iteration 92/1000 | Loss: 0.00006305
Iteration 93/1000 | Loss: 0.00050221
Iteration 94/1000 | Loss: 0.00098516
Iteration 95/1000 | Loss: 0.00025375
Iteration 96/1000 | Loss: 0.00025127
Iteration 97/1000 | Loss: 0.00028595
Iteration 98/1000 | Loss: 0.00019499
Iteration 99/1000 | Loss: 0.00022100
Iteration 100/1000 | Loss: 0.00016839
Iteration 101/1000 | Loss: 0.00013528
Iteration 102/1000 | Loss: 0.00016850
Iteration 103/1000 | Loss: 0.00019010
Iteration 104/1000 | Loss: 0.00005106
Iteration 105/1000 | Loss: 0.00017683
Iteration 106/1000 | Loss: 0.00065165
Iteration 107/1000 | Loss: 0.00020859
Iteration 108/1000 | Loss: 0.00016984
Iteration 109/1000 | Loss: 0.00018000
Iteration 110/1000 | Loss: 0.00007417
Iteration 111/1000 | Loss: 0.00013059
Iteration 112/1000 | Loss: 0.00004133
Iteration 113/1000 | Loss: 0.00019280
Iteration 114/1000 | Loss: 0.00006235
Iteration 115/1000 | Loss: 0.00022803
Iteration 116/1000 | Loss: 0.00017568
Iteration 117/1000 | Loss: 0.00069228
Iteration 118/1000 | Loss: 0.00024874
Iteration 119/1000 | Loss: 0.00010338
Iteration 120/1000 | Loss: 0.00011631
Iteration 121/1000 | Loss: 0.00019519
Iteration 122/1000 | Loss: 0.00010542
Iteration 123/1000 | Loss: 0.00017526
Iteration 124/1000 | Loss: 0.00010440
Iteration 125/1000 | Loss: 0.00013352
Iteration 126/1000 | Loss: 0.00025533
Iteration 127/1000 | Loss: 0.00002539
Iteration 128/1000 | Loss: 0.00002385
Iteration 129/1000 | Loss: 0.00002289
Iteration 130/1000 | Loss: 0.00002207
Iteration 131/1000 | Loss: 0.00002154
Iteration 132/1000 | Loss: 0.00002123
Iteration 133/1000 | Loss: 0.00002089
Iteration 134/1000 | Loss: 0.00002070
Iteration 135/1000 | Loss: 0.00002065
Iteration 136/1000 | Loss: 0.00002064
Iteration 137/1000 | Loss: 0.00002064
Iteration 138/1000 | Loss: 0.00002059
Iteration 139/1000 | Loss: 0.00002053
Iteration 140/1000 | Loss: 0.00002052
Iteration 141/1000 | Loss: 0.00002052
Iteration 142/1000 | Loss: 0.00002038
Iteration 143/1000 | Loss: 0.00002031
Iteration 144/1000 | Loss: 0.00002030
Iteration 145/1000 | Loss: 0.00002030
Iteration 146/1000 | Loss: 0.00002026
Iteration 147/1000 | Loss: 0.00002025
Iteration 148/1000 | Loss: 0.00002025
Iteration 149/1000 | Loss: 0.00002024
Iteration 150/1000 | Loss: 0.00002024
Iteration 151/1000 | Loss: 0.00002024
Iteration 152/1000 | Loss: 0.00002024
Iteration 153/1000 | Loss: 0.00002023
Iteration 154/1000 | Loss: 0.00002023
Iteration 155/1000 | Loss: 0.00002023
Iteration 156/1000 | Loss: 0.00002022
Iteration 157/1000 | Loss: 0.00002022
Iteration 158/1000 | Loss: 0.00002021
Iteration 159/1000 | Loss: 0.00002021
Iteration 160/1000 | Loss: 0.00002021
Iteration 161/1000 | Loss: 0.00002020
Iteration 162/1000 | Loss: 0.00002020
Iteration 163/1000 | Loss: 0.00002020
Iteration 164/1000 | Loss: 0.00002019
Iteration 165/1000 | Loss: 0.00002019
Iteration 166/1000 | Loss: 0.00002018
Iteration 167/1000 | Loss: 0.00002018
Iteration 168/1000 | Loss: 0.00002014
Iteration 169/1000 | Loss: 0.00002014
Iteration 170/1000 | Loss: 0.00002013
Iteration 171/1000 | Loss: 0.00002013
Iteration 172/1000 | Loss: 0.00002013
Iteration 173/1000 | Loss: 0.00002013
Iteration 174/1000 | Loss: 0.00002013
Iteration 175/1000 | Loss: 0.00002013
Iteration 176/1000 | Loss: 0.00002013
Iteration 177/1000 | Loss: 0.00002013
Iteration 178/1000 | Loss: 0.00002012
Iteration 179/1000 | Loss: 0.00002012
Iteration 180/1000 | Loss: 0.00002012
Iteration 181/1000 | Loss: 0.00002012
Iteration 182/1000 | Loss: 0.00002012
Iteration 183/1000 | Loss: 0.00002011
Iteration 184/1000 | Loss: 0.00002011
Iteration 185/1000 | Loss: 0.00002011
Iteration 186/1000 | Loss: 0.00002011
Iteration 187/1000 | Loss: 0.00002011
Iteration 188/1000 | Loss: 0.00002011
Iteration 189/1000 | Loss: 0.00002011
Iteration 190/1000 | Loss: 0.00002011
Iteration 191/1000 | Loss: 0.00002011
Iteration 192/1000 | Loss: 0.00002011
Iteration 193/1000 | Loss: 0.00002011
Iteration 194/1000 | Loss: 0.00002011
Iteration 195/1000 | Loss: 0.00002011
Iteration 196/1000 | Loss: 0.00002011
Iteration 197/1000 | Loss: 0.00002011
Iteration 198/1000 | Loss: 0.00002011
Iteration 199/1000 | Loss: 0.00002011
Iteration 200/1000 | Loss: 0.00002010
Iteration 201/1000 | Loss: 0.00002010
Iteration 202/1000 | Loss: 0.00002010
Iteration 203/1000 | Loss: 0.00002010
Iteration 204/1000 | Loss: 0.00002010
Iteration 205/1000 | Loss: 0.00002010
Iteration 206/1000 | Loss: 0.00002010
Iteration 207/1000 | Loss: 0.00002010
Iteration 208/1000 | Loss: 0.00002010
Iteration 209/1000 | Loss: 0.00002010
Iteration 210/1000 | Loss: 0.00002010
Iteration 211/1000 | Loss: 0.00002010
Iteration 212/1000 | Loss: 0.00002010
Iteration 213/1000 | Loss: 0.00002010
Iteration 214/1000 | Loss: 0.00002010
Iteration 215/1000 | Loss: 0.00002010
Iteration 216/1000 | Loss: 0.00002010
Iteration 217/1000 | Loss: 0.00002010
Iteration 218/1000 | Loss: 0.00002010
Iteration 219/1000 | Loss: 0.00002010
Iteration 220/1000 | Loss: 0.00002010
Iteration 221/1000 | Loss: 0.00002010
Iteration 222/1000 | Loss: 0.00002010
Iteration 223/1000 | Loss: 0.00002010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [2.0099792891414836e-05, 2.0099792891414836e-05, 2.0099792891414836e-05, 2.0099792891414836e-05, 2.0099792891414836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0099792891414836e-05

Optimization complete. Final v2v error: 3.6405410766601562 mm

Highest mean error: 5.350925922393799 mm for frame 75

Lowest mean error: 2.8639540672302246 mm for frame 129

Saving results

Total time: 254.5930516719818
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491483
Iteration 2/25 | Loss: 0.00092379
Iteration 3/25 | Loss: 0.00076250
Iteration 4/25 | Loss: 0.00071932
Iteration 5/25 | Loss: 0.00071041
Iteration 6/25 | Loss: 0.00070768
Iteration 7/25 | Loss: 0.00070645
Iteration 8/25 | Loss: 0.00070624
Iteration 9/25 | Loss: 0.00070624
Iteration 10/25 | Loss: 0.00070624
Iteration 11/25 | Loss: 0.00070624
Iteration 12/25 | Loss: 0.00070624
Iteration 13/25 | Loss: 0.00070624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007062409422360361, 0.0007062409422360361, 0.0007062409422360361, 0.0007062409422360361, 0.0007062409422360361]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007062409422360361

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64996243
Iteration 2/25 | Loss: 0.00040481
Iteration 3/25 | Loss: 0.00040481
Iteration 4/25 | Loss: 0.00040481
Iteration 5/25 | Loss: 0.00040481
Iteration 6/25 | Loss: 0.00040481
Iteration 7/25 | Loss: 0.00040481
Iteration 8/25 | Loss: 0.00040481
Iteration 9/25 | Loss: 0.00040481
Iteration 10/25 | Loss: 0.00040481
Iteration 11/25 | Loss: 0.00040480
Iteration 12/25 | Loss: 0.00040480
Iteration 13/25 | Loss: 0.00040480
Iteration 14/25 | Loss: 0.00040480
Iteration 15/25 | Loss: 0.00040480
Iteration 16/25 | Loss: 0.00040480
Iteration 17/25 | Loss: 0.00040480
Iteration 18/25 | Loss: 0.00040480
Iteration 19/25 | Loss: 0.00040480
Iteration 20/25 | Loss: 0.00040480
Iteration 21/25 | Loss: 0.00040480
Iteration 22/25 | Loss: 0.00040480
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00040480485768057406, 0.00040480485768057406, 0.00040480485768057406, 0.00040480485768057406, 0.00040480485768057406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00040480485768057406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040480
Iteration 2/1000 | Loss: 0.00004607
Iteration 3/1000 | Loss: 0.00003284
Iteration 4/1000 | Loss: 0.00002802
Iteration 5/1000 | Loss: 0.00002656
Iteration 6/1000 | Loss: 0.00002523
Iteration 7/1000 | Loss: 0.00002428
Iteration 8/1000 | Loss: 0.00002372
Iteration 9/1000 | Loss: 0.00002322
Iteration 10/1000 | Loss: 0.00002293
Iteration 11/1000 | Loss: 0.00002264
Iteration 12/1000 | Loss: 0.00002242
Iteration 13/1000 | Loss: 0.00002220
Iteration 14/1000 | Loss: 0.00002203
Iteration 15/1000 | Loss: 0.00002201
Iteration 16/1000 | Loss: 0.00002194
Iteration 17/1000 | Loss: 0.00002186
Iteration 18/1000 | Loss: 0.00002183
Iteration 19/1000 | Loss: 0.00002181
Iteration 20/1000 | Loss: 0.00002180
Iteration 21/1000 | Loss: 0.00002179
Iteration 22/1000 | Loss: 0.00002178
Iteration 23/1000 | Loss: 0.00002176
Iteration 24/1000 | Loss: 0.00002175
Iteration 25/1000 | Loss: 0.00002172
Iteration 26/1000 | Loss: 0.00002171
Iteration 27/1000 | Loss: 0.00002171
Iteration 28/1000 | Loss: 0.00002171
Iteration 29/1000 | Loss: 0.00002170
Iteration 30/1000 | Loss: 0.00002170
Iteration 31/1000 | Loss: 0.00002169
Iteration 32/1000 | Loss: 0.00002169
Iteration 33/1000 | Loss: 0.00002169
Iteration 34/1000 | Loss: 0.00002168
Iteration 35/1000 | Loss: 0.00002168
Iteration 36/1000 | Loss: 0.00002168
Iteration 37/1000 | Loss: 0.00002168
Iteration 38/1000 | Loss: 0.00002167
Iteration 39/1000 | Loss: 0.00002167
Iteration 40/1000 | Loss: 0.00002167
Iteration 41/1000 | Loss: 0.00002166
Iteration 42/1000 | Loss: 0.00002165
Iteration 43/1000 | Loss: 0.00002165
Iteration 44/1000 | Loss: 0.00002164
Iteration 45/1000 | Loss: 0.00002164
Iteration 46/1000 | Loss: 0.00002163
Iteration 47/1000 | Loss: 0.00002163
Iteration 48/1000 | Loss: 0.00002161
Iteration 49/1000 | Loss: 0.00002161
Iteration 50/1000 | Loss: 0.00002161
Iteration 51/1000 | Loss: 0.00002160
Iteration 52/1000 | Loss: 0.00002159
Iteration 53/1000 | Loss: 0.00002159
Iteration 54/1000 | Loss: 0.00002159
Iteration 55/1000 | Loss: 0.00002159
Iteration 56/1000 | Loss: 0.00002159
Iteration 57/1000 | Loss: 0.00002159
Iteration 58/1000 | Loss: 0.00002158
Iteration 59/1000 | Loss: 0.00002158
Iteration 60/1000 | Loss: 0.00002158
Iteration 61/1000 | Loss: 0.00002158
Iteration 62/1000 | Loss: 0.00002158
Iteration 63/1000 | Loss: 0.00002158
Iteration 64/1000 | Loss: 0.00002158
Iteration 65/1000 | Loss: 0.00002158
Iteration 66/1000 | Loss: 0.00002158
Iteration 67/1000 | Loss: 0.00002157
Iteration 68/1000 | Loss: 0.00002156
Iteration 69/1000 | Loss: 0.00002156
Iteration 70/1000 | Loss: 0.00002155
Iteration 71/1000 | Loss: 0.00002155
Iteration 72/1000 | Loss: 0.00002155
Iteration 73/1000 | Loss: 0.00002155
Iteration 74/1000 | Loss: 0.00002155
Iteration 75/1000 | Loss: 0.00002154
Iteration 76/1000 | Loss: 0.00002154
Iteration 77/1000 | Loss: 0.00002154
Iteration 78/1000 | Loss: 0.00002153
Iteration 79/1000 | Loss: 0.00002153
Iteration 80/1000 | Loss: 0.00002153
Iteration 81/1000 | Loss: 0.00002153
Iteration 82/1000 | Loss: 0.00002152
Iteration 83/1000 | Loss: 0.00002152
Iteration 84/1000 | Loss: 0.00002152
Iteration 85/1000 | Loss: 0.00002151
Iteration 86/1000 | Loss: 0.00002151
Iteration 87/1000 | Loss: 0.00002151
Iteration 88/1000 | Loss: 0.00002151
Iteration 89/1000 | Loss: 0.00002150
Iteration 90/1000 | Loss: 0.00002150
Iteration 91/1000 | Loss: 0.00002150
Iteration 92/1000 | Loss: 0.00002150
Iteration 93/1000 | Loss: 0.00002150
Iteration 94/1000 | Loss: 0.00002149
Iteration 95/1000 | Loss: 0.00002149
Iteration 96/1000 | Loss: 0.00002149
Iteration 97/1000 | Loss: 0.00002149
Iteration 98/1000 | Loss: 0.00002149
Iteration 99/1000 | Loss: 0.00002148
Iteration 100/1000 | Loss: 0.00002148
Iteration 101/1000 | Loss: 0.00002148
Iteration 102/1000 | Loss: 0.00002148
Iteration 103/1000 | Loss: 0.00002148
Iteration 104/1000 | Loss: 0.00002148
Iteration 105/1000 | Loss: 0.00002148
Iteration 106/1000 | Loss: 0.00002148
Iteration 107/1000 | Loss: 0.00002148
Iteration 108/1000 | Loss: 0.00002148
Iteration 109/1000 | Loss: 0.00002148
Iteration 110/1000 | Loss: 0.00002148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.1483494492713362e-05, 2.1483494492713362e-05, 2.1483494492713362e-05, 2.1483494492713362e-05, 2.1483494492713362e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1483494492713362e-05

Optimization complete. Final v2v error: 3.8530144691467285 mm

Highest mean error: 5.090113639831543 mm for frame 53

Lowest mean error: 2.9510581493377686 mm for frame 239

Saving results

Total time: 48.797390937805176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801217
Iteration 2/25 | Loss: 0.00145425
Iteration 3/25 | Loss: 0.00094125
Iteration 4/25 | Loss: 0.00085112
Iteration 5/25 | Loss: 0.00081719
Iteration 6/25 | Loss: 0.00081566
Iteration 7/25 | Loss: 0.00081825
Iteration 8/25 | Loss: 0.00082231
Iteration 9/25 | Loss: 0.00081260
Iteration 10/25 | Loss: 0.00080324
Iteration 11/25 | Loss: 0.00079879
Iteration 12/25 | Loss: 0.00079715
Iteration 13/25 | Loss: 0.00078348
Iteration 14/25 | Loss: 0.00077762
Iteration 15/25 | Loss: 0.00077506
Iteration 16/25 | Loss: 0.00076883
Iteration 17/25 | Loss: 0.00076702
Iteration 18/25 | Loss: 0.00076741
Iteration 19/25 | Loss: 0.00076763
Iteration 20/25 | Loss: 0.00076702
Iteration 21/25 | Loss: 0.00076636
Iteration 22/25 | Loss: 0.00076646
Iteration 23/25 | Loss: 0.00076675
Iteration 24/25 | Loss: 0.00076926
Iteration 25/25 | Loss: 0.00077115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42153358
Iteration 2/25 | Loss: 0.00047149
Iteration 3/25 | Loss: 0.00044135
Iteration 4/25 | Loss: 0.00044135
Iteration 5/25 | Loss: 0.00044135
Iteration 6/25 | Loss: 0.00044135
Iteration 7/25 | Loss: 0.00044135
Iteration 8/25 | Loss: 0.00044135
Iteration 9/25 | Loss: 0.00044135
Iteration 10/25 | Loss: 0.00044135
Iteration 11/25 | Loss: 0.00044135
Iteration 12/25 | Loss: 0.00044135
Iteration 13/25 | Loss: 0.00044135
Iteration 14/25 | Loss: 0.00044135
Iteration 15/25 | Loss: 0.00044135
Iteration 16/25 | Loss: 0.00044135
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00044135149801149964, 0.00044135149801149964, 0.00044135149801149964, 0.00044135149801149964, 0.00044135149801149964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00044135149801149964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044135
Iteration 2/1000 | Loss: 0.00008021
Iteration 3/1000 | Loss: 0.00004346
Iteration 4/1000 | Loss: 0.00003849
Iteration 5/1000 | Loss: 0.00003581
Iteration 6/1000 | Loss: 0.00003379
Iteration 7/1000 | Loss: 0.00035067
Iteration 8/1000 | Loss: 0.00059164
Iteration 9/1000 | Loss: 0.00006455
Iteration 10/1000 | Loss: 0.00004398
Iteration 11/1000 | Loss: 0.00003878
Iteration 12/1000 | Loss: 0.00004498
Iteration 13/1000 | Loss: 0.00003602
Iteration 14/1000 | Loss: 0.00003063
Iteration 15/1000 | Loss: 0.00003690
Iteration 16/1000 | Loss: 0.00002713
Iteration 17/1000 | Loss: 0.00003683
Iteration 18/1000 | Loss: 0.00004178
Iteration 19/1000 | Loss: 0.00004365
Iteration 20/1000 | Loss: 0.00004420
Iteration 21/1000 | Loss: 0.00002876
Iteration 22/1000 | Loss: 0.00002618
Iteration 23/1000 | Loss: 0.00002510
Iteration 24/1000 | Loss: 0.00002464
Iteration 25/1000 | Loss: 0.00002435
Iteration 26/1000 | Loss: 0.00002404
Iteration 27/1000 | Loss: 0.00002369
Iteration 28/1000 | Loss: 0.00002342
Iteration 29/1000 | Loss: 0.00002326
Iteration 30/1000 | Loss: 0.00002304
Iteration 31/1000 | Loss: 0.00002293
Iteration 32/1000 | Loss: 0.00002293
Iteration 33/1000 | Loss: 0.00002292
Iteration 34/1000 | Loss: 0.00002292
Iteration 35/1000 | Loss: 0.00002289
Iteration 36/1000 | Loss: 0.00002275
Iteration 37/1000 | Loss: 0.00002275
Iteration 38/1000 | Loss: 0.00002273
Iteration 39/1000 | Loss: 0.00002269
Iteration 40/1000 | Loss: 0.00002255
Iteration 41/1000 | Loss: 0.00002247
Iteration 42/1000 | Loss: 0.00002243
Iteration 43/1000 | Loss: 0.00002242
Iteration 44/1000 | Loss: 0.00002241
Iteration 45/1000 | Loss: 0.00002241
Iteration 46/1000 | Loss: 0.00002239
Iteration 47/1000 | Loss: 0.00002239
Iteration 48/1000 | Loss: 0.00002238
Iteration 49/1000 | Loss: 0.00002237
Iteration 50/1000 | Loss: 0.00002236
Iteration 51/1000 | Loss: 0.00002236
Iteration 52/1000 | Loss: 0.00002235
Iteration 53/1000 | Loss: 0.00002235
Iteration 54/1000 | Loss: 0.00002234
Iteration 55/1000 | Loss: 0.00002233
Iteration 56/1000 | Loss: 0.00002231
Iteration 57/1000 | Loss: 0.00002231
Iteration 58/1000 | Loss: 0.00002231
Iteration 59/1000 | Loss: 0.00002230
Iteration 60/1000 | Loss: 0.00002230
Iteration 61/1000 | Loss: 0.00002229
Iteration 62/1000 | Loss: 0.00002229
Iteration 63/1000 | Loss: 0.00002228
Iteration 64/1000 | Loss: 0.00002228
Iteration 65/1000 | Loss: 0.00002224
Iteration 66/1000 | Loss: 0.00002220
Iteration 67/1000 | Loss: 0.00002218
Iteration 68/1000 | Loss: 0.00002218
Iteration 69/1000 | Loss: 0.00002213
Iteration 70/1000 | Loss: 0.00002209
Iteration 71/1000 | Loss: 0.00002202
Iteration 72/1000 | Loss: 0.00002198
Iteration 73/1000 | Loss: 0.00002197
Iteration 74/1000 | Loss: 0.00002197
Iteration 75/1000 | Loss: 0.00002197
Iteration 76/1000 | Loss: 0.00002196
Iteration 77/1000 | Loss: 0.00002195
Iteration 78/1000 | Loss: 0.00002195
Iteration 79/1000 | Loss: 0.00002195
Iteration 80/1000 | Loss: 0.00002194
Iteration 81/1000 | Loss: 0.00002194
Iteration 82/1000 | Loss: 0.00002194
Iteration 83/1000 | Loss: 0.00002193
Iteration 84/1000 | Loss: 0.00002193
Iteration 85/1000 | Loss: 0.00002193
Iteration 86/1000 | Loss: 0.00002191
Iteration 87/1000 | Loss: 0.00002191
Iteration 88/1000 | Loss: 0.00002190
Iteration 89/1000 | Loss: 0.00002190
Iteration 90/1000 | Loss: 0.00002190
Iteration 91/1000 | Loss: 0.00002189
Iteration 92/1000 | Loss: 0.00002189
Iteration 93/1000 | Loss: 0.00002189
Iteration 94/1000 | Loss: 0.00002188
Iteration 95/1000 | Loss: 0.00002188
Iteration 96/1000 | Loss: 0.00002188
Iteration 97/1000 | Loss: 0.00002187
Iteration 98/1000 | Loss: 0.00002187
Iteration 99/1000 | Loss: 0.00002187
Iteration 100/1000 | Loss: 0.00002187
Iteration 101/1000 | Loss: 0.00002187
Iteration 102/1000 | Loss: 0.00002187
Iteration 103/1000 | Loss: 0.00002187
Iteration 104/1000 | Loss: 0.00002187
Iteration 105/1000 | Loss: 0.00002186
Iteration 106/1000 | Loss: 0.00002186
Iteration 107/1000 | Loss: 0.00002186
Iteration 108/1000 | Loss: 0.00002186
Iteration 109/1000 | Loss: 0.00002186
Iteration 110/1000 | Loss: 0.00002186
Iteration 111/1000 | Loss: 0.00002186
Iteration 112/1000 | Loss: 0.00002186
Iteration 113/1000 | Loss: 0.00002186
Iteration 114/1000 | Loss: 0.00002186
Iteration 115/1000 | Loss: 0.00002186
Iteration 116/1000 | Loss: 0.00002186
Iteration 117/1000 | Loss: 0.00002186
Iteration 118/1000 | Loss: 0.00002186
Iteration 119/1000 | Loss: 0.00002185
Iteration 120/1000 | Loss: 0.00002185
Iteration 121/1000 | Loss: 0.00002185
Iteration 122/1000 | Loss: 0.00002185
Iteration 123/1000 | Loss: 0.00002185
Iteration 124/1000 | Loss: 0.00002185
Iteration 125/1000 | Loss: 0.00002185
Iteration 126/1000 | Loss: 0.00002185
Iteration 127/1000 | Loss: 0.00002184
Iteration 128/1000 | Loss: 0.00002184
Iteration 129/1000 | Loss: 0.00002184
Iteration 130/1000 | Loss: 0.00002184
Iteration 131/1000 | Loss: 0.00002183
Iteration 132/1000 | Loss: 0.00002183
Iteration 133/1000 | Loss: 0.00002183
Iteration 134/1000 | Loss: 0.00002183
Iteration 135/1000 | Loss: 0.00002183
Iteration 136/1000 | Loss: 0.00002182
Iteration 137/1000 | Loss: 0.00002182
Iteration 138/1000 | Loss: 0.00002182
Iteration 139/1000 | Loss: 0.00002182
Iteration 140/1000 | Loss: 0.00002182
Iteration 141/1000 | Loss: 0.00002182
Iteration 142/1000 | Loss: 0.00002182
Iteration 143/1000 | Loss: 0.00002182
Iteration 144/1000 | Loss: 0.00002182
Iteration 145/1000 | Loss: 0.00002181
Iteration 146/1000 | Loss: 0.00002181
Iteration 147/1000 | Loss: 0.00002181
Iteration 148/1000 | Loss: 0.00002181
Iteration 149/1000 | Loss: 0.00002181
Iteration 150/1000 | Loss: 0.00002181
Iteration 151/1000 | Loss: 0.00002181
Iteration 152/1000 | Loss: 0.00002181
Iteration 153/1000 | Loss: 0.00002181
Iteration 154/1000 | Loss: 0.00002181
Iteration 155/1000 | Loss: 0.00002181
Iteration 156/1000 | Loss: 0.00002181
Iteration 157/1000 | Loss: 0.00002180
Iteration 158/1000 | Loss: 0.00002180
Iteration 159/1000 | Loss: 0.00002180
Iteration 160/1000 | Loss: 0.00002180
Iteration 161/1000 | Loss: 0.00002180
Iteration 162/1000 | Loss: 0.00002180
Iteration 163/1000 | Loss: 0.00002180
Iteration 164/1000 | Loss: 0.00002180
Iteration 165/1000 | Loss: 0.00002180
Iteration 166/1000 | Loss: 0.00002180
Iteration 167/1000 | Loss: 0.00002180
Iteration 168/1000 | Loss: 0.00002180
Iteration 169/1000 | Loss: 0.00002180
Iteration 170/1000 | Loss: 0.00002180
Iteration 171/1000 | Loss: 0.00002180
Iteration 172/1000 | Loss: 0.00002180
Iteration 173/1000 | Loss: 0.00002180
Iteration 174/1000 | Loss: 0.00002180
Iteration 175/1000 | Loss: 0.00002179
Iteration 176/1000 | Loss: 0.00002179
Iteration 177/1000 | Loss: 0.00002179
Iteration 178/1000 | Loss: 0.00002179
Iteration 179/1000 | Loss: 0.00002179
Iteration 180/1000 | Loss: 0.00002179
Iteration 181/1000 | Loss: 0.00002179
Iteration 182/1000 | Loss: 0.00002179
Iteration 183/1000 | Loss: 0.00002179
Iteration 184/1000 | Loss: 0.00002179
Iteration 185/1000 | Loss: 0.00002179
Iteration 186/1000 | Loss: 0.00002179
Iteration 187/1000 | Loss: 0.00002179
Iteration 188/1000 | Loss: 0.00002179
Iteration 189/1000 | Loss: 0.00002179
Iteration 190/1000 | Loss: 0.00002179
Iteration 191/1000 | Loss: 0.00002179
Iteration 192/1000 | Loss: 0.00002179
Iteration 193/1000 | Loss: 0.00002179
Iteration 194/1000 | Loss: 0.00002179
Iteration 195/1000 | Loss: 0.00002179
Iteration 196/1000 | Loss: 0.00002179
Iteration 197/1000 | Loss: 0.00002178
Iteration 198/1000 | Loss: 0.00002178
Iteration 199/1000 | Loss: 0.00002178
Iteration 200/1000 | Loss: 0.00002178
Iteration 201/1000 | Loss: 0.00002178
Iteration 202/1000 | Loss: 0.00002178
Iteration 203/1000 | Loss: 0.00002178
Iteration 204/1000 | Loss: 0.00002178
Iteration 205/1000 | Loss: 0.00002178
Iteration 206/1000 | Loss: 0.00002178
Iteration 207/1000 | Loss: 0.00002178
Iteration 208/1000 | Loss: 0.00002178
Iteration 209/1000 | Loss: 0.00002178
Iteration 210/1000 | Loss: 0.00002178
Iteration 211/1000 | Loss: 0.00002178
Iteration 212/1000 | Loss: 0.00002178
Iteration 213/1000 | Loss: 0.00002178
Iteration 214/1000 | Loss: 0.00002178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.1782543626613915e-05, 2.1782543626613915e-05, 2.1782543626613915e-05, 2.1782543626613915e-05, 2.1782543626613915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1782543626613915e-05

Optimization complete. Final v2v error: 3.874565601348877 mm

Highest mean error: 5.33036994934082 mm for frame 104

Lowest mean error: 3.1816108226776123 mm for frame 186

Saving results

Total time: 127.36926221847534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051395
Iteration 2/25 | Loss: 0.01051395
Iteration 3/25 | Loss: 0.01051395
Iteration 4/25 | Loss: 0.01051395
Iteration 5/25 | Loss: 0.01051395
Iteration 6/25 | Loss: 0.01051394
Iteration 7/25 | Loss: 0.01051394
Iteration 8/25 | Loss: 0.01051394
Iteration 9/25 | Loss: 0.01051394
Iteration 10/25 | Loss: 0.01051394
Iteration 11/25 | Loss: 0.01051394
Iteration 12/25 | Loss: 0.01051394
Iteration 13/25 | Loss: 0.01051394
Iteration 14/25 | Loss: 0.01051394
Iteration 15/25 | Loss: 0.01051394
Iteration 16/25 | Loss: 0.01051394
Iteration 17/25 | Loss: 0.01051394
Iteration 18/25 | Loss: 0.01051393
Iteration 19/25 | Loss: 0.01051393
Iteration 20/25 | Loss: 0.01051393
Iteration 21/25 | Loss: 0.01051393
Iteration 22/25 | Loss: 0.01051393
Iteration 23/25 | Loss: 0.01051393
Iteration 24/25 | Loss: 0.01051393
Iteration 25/25 | Loss: 0.01051393

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76570165
Iteration 2/25 | Loss: 0.10874102
Iteration 3/25 | Loss: 0.10544735
Iteration 4/25 | Loss: 0.10544731
Iteration 5/25 | Loss: 0.10544731
Iteration 6/25 | Loss: 0.10544731
Iteration 7/25 | Loss: 0.10544731
Iteration 8/25 | Loss: 0.10544730
Iteration 9/25 | Loss: 0.10544730
Iteration 10/25 | Loss: 0.10544730
Iteration 11/25 | Loss: 0.10544730
Iteration 12/25 | Loss: 0.10544730
Iteration 13/25 | Loss: 0.10544730
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.10544729977846146, 0.10544729977846146, 0.10544729977846146, 0.10544729977846146, 0.10544729977846146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10544729977846146

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10544730
Iteration 2/1000 | Loss: 0.00548508
Iteration 3/1000 | Loss: 0.00631999
Iteration 4/1000 | Loss: 0.00148370
Iteration 5/1000 | Loss: 0.00026921
Iteration 6/1000 | Loss: 0.00077803
Iteration 7/1000 | Loss: 0.00018797
Iteration 8/1000 | Loss: 0.00062071
Iteration 9/1000 | Loss: 0.00006265
Iteration 10/1000 | Loss: 0.00037799
Iteration 11/1000 | Loss: 0.00186043
Iteration 12/1000 | Loss: 0.00038896
Iteration 13/1000 | Loss: 0.00005269
Iteration 14/1000 | Loss: 0.00007123
Iteration 15/1000 | Loss: 0.00009404
Iteration 16/1000 | Loss: 0.00009364
Iteration 17/1000 | Loss: 0.00005488
Iteration 18/1000 | Loss: 0.00011616
Iteration 19/1000 | Loss: 0.00004933
Iteration 20/1000 | Loss: 0.00008912
Iteration 21/1000 | Loss: 0.00006535
Iteration 22/1000 | Loss: 0.00018668
Iteration 23/1000 | Loss: 0.00004115
Iteration 24/1000 | Loss: 0.00007942
Iteration 25/1000 | Loss: 0.00011558
Iteration 26/1000 | Loss: 0.00002972
Iteration 27/1000 | Loss: 0.00007092
Iteration 28/1000 | Loss: 0.00003368
Iteration 29/1000 | Loss: 0.00003109
Iteration 30/1000 | Loss: 0.00003964
Iteration 31/1000 | Loss: 0.00002564
Iteration 32/1000 | Loss: 0.00003859
Iteration 33/1000 | Loss: 0.00004523
Iteration 34/1000 | Loss: 0.00007535
Iteration 35/1000 | Loss: 0.00001917
Iteration 36/1000 | Loss: 0.00003659
Iteration 37/1000 | Loss: 0.00002011
Iteration 38/1000 | Loss: 0.00004833
Iteration 39/1000 | Loss: 0.00003780
Iteration 40/1000 | Loss: 0.00004195
Iteration 41/1000 | Loss: 0.00003141
Iteration 42/1000 | Loss: 0.00003540
Iteration 43/1000 | Loss: 0.00005703
Iteration 44/1000 | Loss: 0.00002481
Iteration 45/1000 | Loss: 0.00002107
Iteration 46/1000 | Loss: 0.00001835
Iteration 47/1000 | Loss: 0.00003669
Iteration 48/1000 | Loss: 0.00034968
Iteration 49/1000 | Loss: 0.00005207
Iteration 50/1000 | Loss: 0.00002741
Iteration 51/1000 | Loss: 0.00007864
Iteration 52/1000 | Loss: 0.00003289
Iteration 53/1000 | Loss: 0.00001791
Iteration 54/1000 | Loss: 0.00001806
Iteration 55/1000 | Loss: 0.00002813
Iteration 56/1000 | Loss: 0.00001846
Iteration 57/1000 | Loss: 0.00001948
Iteration 58/1000 | Loss: 0.00001723
Iteration 59/1000 | Loss: 0.00001723
Iteration 60/1000 | Loss: 0.00001723
Iteration 61/1000 | Loss: 0.00001723
Iteration 62/1000 | Loss: 0.00001722
Iteration 63/1000 | Loss: 0.00001722
Iteration 64/1000 | Loss: 0.00001722
Iteration 65/1000 | Loss: 0.00001722
Iteration 66/1000 | Loss: 0.00001722
Iteration 67/1000 | Loss: 0.00001722
Iteration 68/1000 | Loss: 0.00001722
Iteration 69/1000 | Loss: 0.00001722
Iteration 70/1000 | Loss: 0.00001722
Iteration 71/1000 | Loss: 0.00001722
Iteration 72/1000 | Loss: 0.00001722
Iteration 73/1000 | Loss: 0.00001722
Iteration 74/1000 | Loss: 0.00001722
Iteration 75/1000 | Loss: 0.00001721
Iteration 76/1000 | Loss: 0.00001721
Iteration 77/1000 | Loss: 0.00001721
Iteration 78/1000 | Loss: 0.00001721
Iteration 79/1000 | Loss: 0.00001721
Iteration 80/1000 | Loss: 0.00001847
Iteration 81/1000 | Loss: 0.00001891
Iteration 82/1000 | Loss: 0.00001879
Iteration 83/1000 | Loss: 0.00002215
Iteration 84/1000 | Loss: 0.00008880
Iteration 85/1000 | Loss: 0.00003764
Iteration 86/1000 | Loss: 0.00004165
Iteration 87/1000 | Loss: 0.00001718
Iteration 88/1000 | Loss: 0.00001816
Iteration 89/1000 | Loss: 0.00002008
Iteration 90/1000 | Loss: 0.00002150
Iteration 91/1000 | Loss: 0.00001845
Iteration 92/1000 | Loss: 0.00001720
Iteration 93/1000 | Loss: 0.00001716
Iteration 94/1000 | Loss: 0.00001716
Iteration 95/1000 | Loss: 0.00001716
Iteration 96/1000 | Loss: 0.00001716
Iteration 97/1000 | Loss: 0.00001716
Iteration 98/1000 | Loss: 0.00001716
Iteration 99/1000 | Loss: 0.00001716
Iteration 100/1000 | Loss: 0.00001716
Iteration 101/1000 | Loss: 0.00001716
Iteration 102/1000 | Loss: 0.00001716
Iteration 103/1000 | Loss: 0.00001716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.7155192836071365e-05, 1.7155192836071365e-05, 1.7155192836071365e-05, 1.7155192836071365e-05, 1.7155192836071365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7155192836071365e-05

Optimization complete. Final v2v error: 3.4953196048736572 mm

Highest mean error: 3.6546428203582764 mm for frame 135

Lowest mean error: 3.4062178134918213 mm for frame 139

Saving results

Total time: 110.51973915100098
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850432
Iteration 2/25 | Loss: 0.00077697
Iteration 3/25 | Loss: 0.00067589
Iteration 4/25 | Loss: 0.00064350
Iteration 5/25 | Loss: 0.00063945
Iteration 6/25 | Loss: 0.00063822
Iteration 7/25 | Loss: 0.00063786
Iteration 8/25 | Loss: 0.00063786
Iteration 9/25 | Loss: 0.00063786
Iteration 10/25 | Loss: 0.00063786
Iteration 11/25 | Loss: 0.00063786
Iteration 12/25 | Loss: 0.00063786
Iteration 13/25 | Loss: 0.00063786
Iteration 14/25 | Loss: 0.00063786
Iteration 15/25 | Loss: 0.00063786
Iteration 16/25 | Loss: 0.00063786
Iteration 17/25 | Loss: 0.00063786
Iteration 18/25 | Loss: 0.00063786
Iteration 19/25 | Loss: 0.00063786
Iteration 20/25 | Loss: 0.00063786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006378568359650671, 0.0006378568359650671, 0.0006378568359650671, 0.0006378568359650671, 0.0006378568359650671]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006378568359650671

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47170246
Iteration 2/25 | Loss: 0.00027014
Iteration 3/25 | Loss: 0.00027014
Iteration 4/25 | Loss: 0.00027014
Iteration 5/25 | Loss: 0.00027014
Iteration 6/25 | Loss: 0.00027014
Iteration 7/25 | Loss: 0.00027014
Iteration 8/25 | Loss: 0.00027014
Iteration 9/25 | Loss: 0.00027014
Iteration 10/25 | Loss: 0.00027014
Iteration 11/25 | Loss: 0.00027014
Iteration 12/25 | Loss: 0.00027014
Iteration 13/25 | Loss: 0.00027014
Iteration 14/25 | Loss: 0.00027014
Iteration 15/25 | Loss: 0.00027014
Iteration 16/25 | Loss: 0.00027014
Iteration 17/25 | Loss: 0.00027014
Iteration 18/25 | Loss: 0.00027014
Iteration 19/25 | Loss: 0.00027014
Iteration 20/25 | Loss: 0.00027014
Iteration 21/25 | Loss: 0.00027014
Iteration 22/25 | Loss: 0.00027014
Iteration 23/25 | Loss: 0.00027014
Iteration 24/25 | Loss: 0.00027014
Iteration 25/25 | Loss: 0.00027014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027014
Iteration 2/1000 | Loss: 0.00003015
Iteration 3/1000 | Loss: 0.00002117
Iteration 4/1000 | Loss: 0.00001963
Iteration 5/1000 | Loss: 0.00001843
Iteration 6/1000 | Loss: 0.00001796
Iteration 7/1000 | Loss: 0.00001731
Iteration 8/1000 | Loss: 0.00001704
Iteration 9/1000 | Loss: 0.00001684
Iteration 10/1000 | Loss: 0.00001679
Iteration 11/1000 | Loss: 0.00001676
Iteration 12/1000 | Loss: 0.00001674
Iteration 13/1000 | Loss: 0.00001666
Iteration 14/1000 | Loss: 0.00001666
Iteration 15/1000 | Loss: 0.00001665
Iteration 16/1000 | Loss: 0.00001664
Iteration 17/1000 | Loss: 0.00001664
Iteration 18/1000 | Loss: 0.00001664
Iteration 19/1000 | Loss: 0.00001663
Iteration 20/1000 | Loss: 0.00001663
Iteration 21/1000 | Loss: 0.00001663
Iteration 22/1000 | Loss: 0.00001663
Iteration 23/1000 | Loss: 0.00001662
Iteration 24/1000 | Loss: 0.00001662
Iteration 25/1000 | Loss: 0.00001662
Iteration 26/1000 | Loss: 0.00001662
Iteration 27/1000 | Loss: 0.00001662
Iteration 28/1000 | Loss: 0.00001662
Iteration 29/1000 | Loss: 0.00001662
Iteration 30/1000 | Loss: 0.00001662
Iteration 31/1000 | Loss: 0.00001662
Iteration 32/1000 | Loss: 0.00001662
Iteration 33/1000 | Loss: 0.00001662
Iteration 34/1000 | Loss: 0.00001662
Iteration 35/1000 | Loss: 0.00001661
Iteration 36/1000 | Loss: 0.00001661
Iteration 37/1000 | Loss: 0.00001660
Iteration 38/1000 | Loss: 0.00001660
Iteration 39/1000 | Loss: 0.00001660
Iteration 40/1000 | Loss: 0.00001659
Iteration 41/1000 | Loss: 0.00001659
Iteration 42/1000 | Loss: 0.00001659
Iteration 43/1000 | Loss: 0.00001659
Iteration 44/1000 | Loss: 0.00001658
Iteration 45/1000 | Loss: 0.00001658
Iteration 46/1000 | Loss: 0.00001658
Iteration 47/1000 | Loss: 0.00001658
Iteration 48/1000 | Loss: 0.00001658
Iteration 49/1000 | Loss: 0.00001658
Iteration 50/1000 | Loss: 0.00001658
Iteration 51/1000 | Loss: 0.00001658
Iteration 52/1000 | Loss: 0.00001657
Iteration 53/1000 | Loss: 0.00001657
Iteration 54/1000 | Loss: 0.00001657
Iteration 55/1000 | Loss: 0.00001657
Iteration 56/1000 | Loss: 0.00001657
Iteration 57/1000 | Loss: 0.00001657
Iteration 58/1000 | Loss: 0.00001656
Iteration 59/1000 | Loss: 0.00001656
Iteration 60/1000 | Loss: 0.00001656
Iteration 61/1000 | Loss: 0.00001656
Iteration 62/1000 | Loss: 0.00001656
Iteration 63/1000 | Loss: 0.00001656
Iteration 64/1000 | Loss: 0.00001656
Iteration 65/1000 | Loss: 0.00001656
Iteration 66/1000 | Loss: 0.00001656
Iteration 67/1000 | Loss: 0.00001656
Iteration 68/1000 | Loss: 0.00001655
Iteration 69/1000 | Loss: 0.00001655
Iteration 70/1000 | Loss: 0.00001655
Iteration 71/1000 | Loss: 0.00001655
Iteration 72/1000 | Loss: 0.00001654
Iteration 73/1000 | Loss: 0.00001654
Iteration 74/1000 | Loss: 0.00001654
Iteration 75/1000 | Loss: 0.00001653
Iteration 76/1000 | Loss: 0.00001653
Iteration 77/1000 | Loss: 0.00001653
Iteration 78/1000 | Loss: 0.00001653
Iteration 79/1000 | Loss: 0.00001653
Iteration 80/1000 | Loss: 0.00001653
Iteration 81/1000 | Loss: 0.00001652
Iteration 82/1000 | Loss: 0.00001652
Iteration 83/1000 | Loss: 0.00001652
Iteration 84/1000 | Loss: 0.00001652
Iteration 85/1000 | Loss: 0.00001652
Iteration 86/1000 | Loss: 0.00001652
Iteration 87/1000 | Loss: 0.00001652
Iteration 88/1000 | Loss: 0.00001652
Iteration 89/1000 | Loss: 0.00001652
Iteration 90/1000 | Loss: 0.00001652
Iteration 91/1000 | Loss: 0.00001652
Iteration 92/1000 | Loss: 0.00001652
Iteration 93/1000 | Loss: 0.00001652
Iteration 94/1000 | Loss: 0.00001652
Iteration 95/1000 | Loss: 0.00001652
Iteration 96/1000 | Loss: 0.00001652
Iteration 97/1000 | Loss: 0.00001652
Iteration 98/1000 | Loss: 0.00001652
Iteration 99/1000 | Loss: 0.00001651
Iteration 100/1000 | Loss: 0.00001651
Iteration 101/1000 | Loss: 0.00001651
Iteration 102/1000 | Loss: 0.00001651
Iteration 103/1000 | Loss: 0.00001651
Iteration 104/1000 | Loss: 0.00001651
Iteration 105/1000 | Loss: 0.00001651
Iteration 106/1000 | Loss: 0.00001651
Iteration 107/1000 | Loss: 0.00001651
Iteration 108/1000 | Loss: 0.00001651
Iteration 109/1000 | Loss: 0.00001650
Iteration 110/1000 | Loss: 0.00001650
Iteration 111/1000 | Loss: 0.00001650
Iteration 112/1000 | Loss: 0.00001649
Iteration 113/1000 | Loss: 0.00001648
Iteration 114/1000 | Loss: 0.00001648
Iteration 115/1000 | Loss: 0.00001647
Iteration 116/1000 | Loss: 0.00001647
Iteration 117/1000 | Loss: 0.00001646
Iteration 118/1000 | Loss: 0.00001646
Iteration 119/1000 | Loss: 0.00001646
Iteration 120/1000 | Loss: 0.00001645
Iteration 121/1000 | Loss: 0.00001645
Iteration 122/1000 | Loss: 0.00001645
Iteration 123/1000 | Loss: 0.00001644
Iteration 124/1000 | Loss: 0.00001644
Iteration 125/1000 | Loss: 0.00001644
Iteration 126/1000 | Loss: 0.00001644
Iteration 127/1000 | Loss: 0.00001644
Iteration 128/1000 | Loss: 0.00001644
Iteration 129/1000 | Loss: 0.00001644
Iteration 130/1000 | Loss: 0.00001644
Iteration 131/1000 | Loss: 0.00001643
Iteration 132/1000 | Loss: 0.00001642
Iteration 133/1000 | Loss: 0.00001642
Iteration 134/1000 | Loss: 0.00001639
Iteration 135/1000 | Loss: 0.00001639
Iteration 136/1000 | Loss: 0.00001639
Iteration 137/1000 | Loss: 0.00001638
Iteration 138/1000 | Loss: 0.00001638
Iteration 139/1000 | Loss: 0.00001637
Iteration 140/1000 | Loss: 0.00001637
Iteration 141/1000 | Loss: 0.00001637
Iteration 142/1000 | Loss: 0.00001637
Iteration 143/1000 | Loss: 0.00001636
Iteration 144/1000 | Loss: 0.00001636
Iteration 145/1000 | Loss: 0.00001636
Iteration 146/1000 | Loss: 0.00001636
Iteration 147/1000 | Loss: 0.00001636
Iteration 148/1000 | Loss: 0.00001636
Iteration 149/1000 | Loss: 0.00001636
Iteration 150/1000 | Loss: 0.00001636
Iteration 151/1000 | Loss: 0.00001636
Iteration 152/1000 | Loss: 0.00001636
Iteration 153/1000 | Loss: 0.00001636
Iteration 154/1000 | Loss: 0.00001636
Iteration 155/1000 | Loss: 0.00001636
Iteration 156/1000 | Loss: 0.00001635
Iteration 157/1000 | Loss: 0.00001635
Iteration 158/1000 | Loss: 0.00001635
Iteration 159/1000 | Loss: 0.00001635
Iteration 160/1000 | Loss: 0.00001635
Iteration 161/1000 | Loss: 0.00001635
Iteration 162/1000 | Loss: 0.00001635
Iteration 163/1000 | Loss: 0.00001635
Iteration 164/1000 | Loss: 0.00001635
Iteration 165/1000 | Loss: 0.00001635
Iteration 166/1000 | Loss: 0.00001635
Iteration 167/1000 | Loss: 0.00001635
Iteration 168/1000 | Loss: 0.00001634
Iteration 169/1000 | Loss: 0.00001634
Iteration 170/1000 | Loss: 0.00001634
Iteration 171/1000 | Loss: 0.00001634
Iteration 172/1000 | Loss: 0.00001634
Iteration 173/1000 | Loss: 0.00001634
Iteration 174/1000 | Loss: 0.00001634
Iteration 175/1000 | Loss: 0.00001634
Iteration 176/1000 | Loss: 0.00001634
Iteration 177/1000 | Loss: 0.00001634
Iteration 178/1000 | Loss: 0.00001634
Iteration 179/1000 | Loss: 0.00001634
Iteration 180/1000 | Loss: 0.00001634
Iteration 181/1000 | Loss: 0.00001633
Iteration 182/1000 | Loss: 0.00001633
Iteration 183/1000 | Loss: 0.00001633
Iteration 184/1000 | Loss: 0.00001633
Iteration 185/1000 | Loss: 0.00001633
Iteration 186/1000 | Loss: 0.00001633
Iteration 187/1000 | Loss: 0.00001633
Iteration 188/1000 | Loss: 0.00001633
Iteration 189/1000 | Loss: 0.00001633
Iteration 190/1000 | Loss: 0.00001633
Iteration 191/1000 | Loss: 0.00001633
Iteration 192/1000 | Loss: 0.00001633
Iteration 193/1000 | Loss: 0.00001633
Iteration 194/1000 | Loss: 0.00001633
Iteration 195/1000 | Loss: 0.00001633
Iteration 196/1000 | Loss: 0.00001633
Iteration 197/1000 | Loss: 0.00001633
Iteration 198/1000 | Loss: 0.00001633
Iteration 199/1000 | Loss: 0.00001633
Iteration 200/1000 | Loss: 0.00001633
Iteration 201/1000 | Loss: 0.00001633
Iteration 202/1000 | Loss: 0.00001633
Iteration 203/1000 | Loss: 0.00001633
Iteration 204/1000 | Loss: 0.00001633
Iteration 205/1000 | Loss: 0.00001633
Iteration 206/1000 | Loss: 0.00001633
Iteration 207/1000 | Loss: 0.00001633
Iteration 208/1000 | Loss: 0.00001633
Iteration 209/1000 | Loss: 0.00001633
Iteration 210/1000 | Loss: 0.00001633
Iteration 211/1000 | Loss: 0.00001633
Iteration 212/1000 | Loss: 0.00001633
Iteration 213/1000 | Loss: 0.00001633
Iteration 214/1000 | Loss: 0.00001633
Iteration 215/1000 | Loss: 0.00001633
Iteration 216/1000 | Loss: 0.00001633
Iteration 217/1000 | Loss: 0.00001633
Iteration 218/1000 | Loss: 0.00001633
Iteration 219/1000 | Loss: 0.00001633
Iteration 220/1000 | Loss: 0.00001633
Iteration 221/1000 | Loss: 0.00001633
Iteration 222/1000 | Loss: 0.00001633
Iteration 223/1000 | Loss: 0.00001633
Iteration 224/1000 | Loss: 0.00001633
Iteration 225/1000 | Loss: 0.00001633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.6326961485901847e-05, 1.6326961485901847e-05, 1.6326961485901847e-05, 1.6326961485901847e-05, 1.6326961485901847e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6326961485901847e-05

Optimization complete. Final v2v error: 3.3889248371124268 mm

Highest mean error: 3.4759891033172607 mm for frame 70

Lowest mean error: 3.3068342208862305 mm for frame 59

Saving results

Total time: 38.86174821853638
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01083246
Iteration 2/25 | Loss: 0.01083246
Iteration 3/25 | Loss: 0.00238664
Iteration 4/25 | Loss: 0.00142876
Iteration 5/25 | Loss: 0.00123517
Iteration 6/25 | Loss: 0.00107812
Iteration 7/25 | Loss: 0.00103900
Iteration 8/25 | Loss: 0.00100647
Iteration 9/25 | Loss: 0.00087488
Iteration 10/25 | Loss: 0.00085296
Iteration 11/25 | Loss: 0.00080664
Iteration 12/25 | Loss: 0.00077919
Iteration 13/25 | Loss: 0.00077982
Iteration 14/25 | Loss: 0.00076440
Iteration 15/25 | Loss: 0.00075705
Iteration 16/25 | Loss: 0.00076126
Iteration 17/25 | Loss: 0.00074064
Iteration 18/25 | Loss: 0.00074402
Iteration 19/25 | Loss: 0.00074974
Iteration 20/25 | Loss: 0.00073268
Iteration 21/25 | Loss: 0.00073578
Iteration 22/25 | Loss: 0.00072649
Iteration 23/25 | Loss: 0.00072964
Iteration 24/25 | Loss: 0.00073677
Iteration 25/25 | Loss: 0.00073307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61395741
Iteration 2/25 | Loss: 0.00051580
Iteration 3/25 | Loss: 0.00046714
Iteration 4/25 | Loss: 0.00046713
Iteration 5/25 | Loss: 0.00046713
Iteration 6/25 | Loss: 0.00046713
Iteration 7/25 | Loss: 0.00046713
Iteration 8/25 | Loss: 0.00046713
Iteration 9/25 | Loss: 0.00046713
Iteration 10/25 | Loss: 0.00046713
Iteration 11/25 | Loss: 0.00046713
Iteration 12/25 | Loss: 0.00046713
Iteration 13/25 | Loss: 0.00046713
Iteration 14/25 | Loss: 0.00046713
Iteration 15/25 | Loss: 0.00046713
Iteration 16/25 | Loss: 0.00046713
Iteration 17/25 | Loss: 0.00046713
Iteration 18/25 | Loss: 0.00046713
Iteration 19/25 | Loss: 0.00046713
Iteration 20/25 | Loss: 0.00046713
Iteration 21/25 | Loss: 0.00046713
Iteration 22/25 | Loss: 0.00046713
Iteration 23/25 | Loss: 0.00046713
Iteration 24/25 | Loss: 0.00046713
Iteration 25/25 | Loss: 0.00046713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046713
Iteration 2/1000 | Loss: 0.00130097
Iteration 3/1000 | Loss: 0.00111294
Iteration 4/1000 | Loss: 0.00152783
Iteration 5/1000 | Loss: 0.00070319
Iteration 6/1000 | Loss: 0.00021515
Iteration 7/1000 | Loss: 0.00013877
Iteration 8/1000 | Loss: 0.00012925
Iteration 9/1000 | Loss: 0.00019743
Iteration 10/1000 | Loss: 0.00014511
Iteration 11/1000 | Loss: 0.00014604
Iteration 12/1000 | Loss: 0.00013917
Iteration 13/1000 | Loss: 0.00075907
Iteration 14/1000 | Loss: 0.00077499
Iteration 15/1000 | Loss: 0.00019138
Iteration 16/1000 | Loss: 0.00077010
Iteration 17/1000 | Loss: 0.00087074
Iteration 18/1000 | Loss: 0.00107321
Iteration 19/1000 | Loss: 0.00066444
Iteration 20/1000 | Loss: 0.00035391
Iteration 21/1000 | Loss: 0.00114108
Iteration 22/1000 | Loss: 0.00143311
Iteration 23/1000 | Loss: 0.00123170
Iteration 24/1000 | Loss: 0.00113218
Iteration 25/1000 | Loss: 0.00080974
Iteration 26/1000 | Loss: 0.00116124
Iteration 27/1000 | Loss: 0.00257895
Iteration 28/1000 | Loss: 0.00101762
Iteration 29/1000 | Loss: 0.00067883
Iteration 30/1000 | Loss: 0.00105875
Iteration 31/1000 | Loss: 0.00053681
Iteration 32/1000 | Loss: 0.00132835
Iteration 33/1000 | Loss: 0.00247823
Iteration 34/1000 | Loss: 0.00282695
Iteration 35/1000 | Loss: 0.00180254
Iteration 36/1000 | Loss: 0.00036369
Iteration 37/1000 | Loss: 0.00042646
Iteration 38/1000 | Loss: 0.00021199
Iteration 39/1000 | Loss: 0.00023905
Iteration 40/1000 | Loss: 0.00018114
Iteration 41/1000 | Loss: 0.00028898
Iteration 42/1000 | Loss: 0.00011467
Iteration 43/1000 | Loss: 0.00017355
Iteration 44/1000 | Loss: 0.00015776
Iteration 45/1000 | Loss: 0.00012228
Iteration 46/1000 | Loss: 0.00010966
Iteration 47/1000 | Loss: 0.00030507
Iteration 48/1000 | Loss: 0.00019254
Iteration 49/1000 | Loss: 0.00030394
Iteration 50/1000 | Loss: 0.00011532
Iteration 51/1000 | Loss: 0.00008821
Iteration 52/1000 | Loss: 0.00029370
Iteration 53/1000 | Loss: 0.00021360
Iteration 54/1000 | Loss: 0.00003054
Iteration 55/1000 | Loss: 0.00011880
Iteration 56/1000 | Loss: 0.00012371
Iteration 57/1000 | Loss: 0.00004286
Iteration 58/1000 | Loss: 0.00007834
Iteration 59/1000 | Loss: 0.00002284
Iteration 60/1000 | Loss: 0.00005683
Iteration 61/1000 | Loss: 0.00004266
Iteration 62/1000 | Loss: 0.00002049
Iteration 63/1000 | Loss: 0.00005708
Iteration 64/1000 | Loss: 0.00018565
Iteration 65/1000 | Loss: 0.00005150
Iteration 66/1000 | Loss: 0.00002211
Iteration 67/1000 | Loss: 0.00002275
Iteration 68/1000 | Loss: 0.00008555
Iteration 69/1000 | Loss: 0.00028624
Iteration 70/1000 | Loss: 0.00009479
Iteration 71/1000 | Loss: 0.00010060
Iteration 72/1000 | Loss: 0.00001961
Iteration 73/1000 | Loss: 0.00001849
Iteration 74/1000 | Loss: 0.00005154
Iteration 75/1000 | Loss: 0.00004959
Iteration 76/1000 | Loss: 0.00001822
Iteration 77/1000 | Loss: 0.00001817
Iteration 78/1000 | Loss: 0.00001816
Iteration 79/1000 | Loss: 0.00001802
Iteration 80/1000 | Loss: 0.00001800
Iteration 81/1000 | Loss: 0.00001798
Iteration 82/1000 | Loss: 0.00001798
Iteration 83/1000 | Loss: 0.00001794
Iteration 84/1000 | Loss: 0.00001794
Iteration 85/1000 | Loss: 0.00001794
Iteration 86/1000 | Loss: 0.00001793
Iteration 87/1000 | Loss: 0.00011230
Iteration 88/1000 | Loss: 0.00001798
Iteration 89/1000 | Loss: 0.00001793
Iteration 90/1000 | Loss: 0.00005660
Iteration 91/1000 | Loss: 0.00001793
Iteration 92/1000 | Loss: 0.00001791
Iteration 93/1000 | Loss: 0.00001790
Iteration 94/1000 | Loss: 0.00001789
Iteration 95/1000 | Loss: 0.00001787
Iteration 96/1000 | Loss: 0.00001787
Iteration 97/1000 | Loss: 0.00001787
Iteration 98/1000 | Loss: 0.00001787
Iteration 99/1000 | Loss: 0.00001787
Iteration 100/1000 | Loss: 0.00001787
Iteration 101/1000 | Loss: 0.00001787
Iteration 102/1000 | Loss: 0.00004936
Iteration 103/1000 | Loss: 0.00001787
Iteration 104/1000 | Loss: 0.00001785
Iteration 105/1000 | Loss: 0.00001784
Iteration 106/1000 | Loss: 0.00001784
Iteration 107/1000 | Loss: 0.00001784
Iteration 108/1000 | Loss: 0.00001784
Iteration 109/1000 | Loss: 0.00001784
Iteration 110/1000 | Loss: 0.00001784
Iteration 111/1000 | Loss: 0.00001784
Iteration 112/1000 | Loss: 0.00001784
Iteration 113/1000 | Loss: 0.00001783
Iteration 114/1000 | Loss: 0.00001783
Iteration 115/1000 | Loss: 0.00001783
Iteration 116/1000 | Loss: 0.00001782
Iteration 117/1000 | Loss: 0.00001782
Iteration 118/1000 | Loss: 0.00001782
Iteration 119/1000 | Loss: 0.00001781
Iteration 120/1000 | Loss: 0.00001781
Iteration 121/1000 | Loss: 0.00001780
Iteration 122/1000 | Loss: 0.00001780
Iteration 123/1000 | Loss: 0.00001779
Iteration 124/1000 | Loss: 0.00001779
Iteration 125/1000 | Loss: 0.00001779
Iteration 126/1000 | Loss: 0.00001778
Iteration 127/1000 | Loss: 0.00001778
Iteration 128/1000 | Loss: 0.00001778
Iteration 129/1000 | Loss: 0.00001777
Iteration 130/1000 | Loss: 0.00001777
Iteration 131/1000 | Loss: 0.00001777
Iteration 132/1000 | Loss: 0.00001777
Iteration 133/1000 | Loss: 0.00001777
Iteration 134/1000 | Loss: 0.00001777
Iteration 135/1000 | Loss: 0.00001776
Iteration 136/1000 | Loss: 0.00001776
Iteration 137/1000 | Loss: 0.00001776
Iteration 138/1000 | Loss: 0.00001775
Iteration 139/1000 | Loss: 0.00001775
Iteration 140/1000 | Loss: 0.00001775
Iteration 141/1000 | Loss: 0.00001775
Iteration 142/1000 | Loss: 0.00001775
Iteration 143/1000 | Loss: 0.00001775
Iteration 144/1000 | Loss: 0.00001775
Iteration 145/1000 | Loss: 0.00001774
Iteration 146/1000 | Loss: 0.00001774
Iteration 147/1000 | Loss: 0.00001774
Iteration 148/1000 | Loss: 0.00001774
Iteration 149/1000 | Loss: 0.00001774
Iteration 150/1000 | Loss: 0.00001774
Iteration 151/1000 | Loss: 0.00001774
Iteration 152/1000 | Loss: 0.00001774
Iteration 153/1000 | Loss: 0.00001774
Iteration 154/1000 | Loss: 0.00001774
Iteration 155/1000 | Loss: 0.00001774
Iteration 156/1000 | Loss: 0.00001774
Iteration 157/1000 | Loss: 0.00001774
Iteration 158/1000 | Loss: 0.00001773
Iteration 159/1000 | Loss: 0.00001773
Iteration 160/1000 | Loss: 0.00001773
Iteration 161/1000 | Loss: 0.00001773
Iteration 162/1000 | Loss: 0.00001773
Iteration 163/1000 | Loss: 0.00001773
Iteration 164/1000 | Loss: 0.00001773
Iteration 165/1000 | Loss: 0.00001773
Iteration 166/1000 | Loss: 0.00001772
Iteration 167/1000 | Loss: 0.00001772
Iteration 168/1000 | Loss: 0.00001772
Iteration 169/1000 | Loss: 0.00001772
Iteration 170/1000 | Loss: 0.00001772
Iteration 171/1000 | Loss: 0.00001771
Iteration 172/1000 | Loss: 0.00001771
Iteration 173/1000 | Loss: 0.00001771
Iteration 174/1000 | Loss: 0.00001771
Iteration 175/1000 | Loss: 0.00001770
Iteration 176/1000 | Loss: 0.00001770
Iteration 177/1000 | Loss: 0.00001770
Iteration 178/1000 | Loss: 0.00001770
Iteration 179/1000 | Loss: 0.00001770
Iteration 180/1000 | Loss: 0.00001770
Iteration 181/1000 | Loss: 0.00001770
Iteration 182/1000 | Loss: 0.00001770
Iteration 183/1000 | Loss: 0.00001770
Iteration 184/1000 | Loss: 0.00001769
Iteration 185/1000 | Loss: 0.00001769
Iteration 186/1000 | Loss: 0.00001769
Iteration 187/1000 | Loss: 0.00001769
Iteration 188/1000 | Loss: 0.00001768
Iteration 189/1000 | Loss: 0.00001768
Iteration 190/1000 | Loss: 0.00001768
Iteration 191/1000 | Loss: 0.00001767
Iteration 192/1000 | Loss: 0.00001767
Iteration 193/1000 | Loss: 0.00001767
Iteration 194/1000 | Loss: 0.00001767
Iteration 195/1000 | Loss: 0.00001766
Iteration 196/1000 | Loss: 0.00001766
Iteration 197/1000 | Loss: 0.00001766
Iteration 198/1000 | Loss: 0.00001765
Iteration 199/1000 | Loss: 0.00001765
Iteration 200/1000 | Loss: 0.00001765
Iteration 201/1000 | Loss: 0.00001765
Iteration 202/1000 | Loss: 0.00001765
Iteration 203/1000 | Loss: 0.00001764
Iteration 204/1000 | Loss: 0.00001764
Iteration 205/1000 | Loss: 0.00001764
Iteration 206/1000 | Loss: 0.00001764
Iteration 207/1000 | Loss: 0.00001764
Iteration 208/1000 | Loss: 0.00001764
Iteration 209/1000 | Loss: 0.00001763
Iteration 210/1000 | Loss: 0.00001763
Iteration 211/1000 | Loss: 0.00001763
Iteration 212/1000 | Loss: 0.00001763
Iteration 213/1000 | Loss: 0.00001763
Iteration 214/1000 | Loss: 0.00001762
Iteration 215/1000 | Loss: 0.00001762
Iteration 216/1000 | Loss: 0.00001762
Iteration 217/1000 | Loss: 0.00001762
Iteration 218/1000 | Loss: 0.00001762
Iteration 219/1000 | Loss: 0.00001762
Iteration 220/1000 | Loss: 0.00001762
Iteration 221/1000 | Loss: 0.00001761
Iteration 222/1000 | Loss: 0.00001761
Iteration 223/1000 | Loss: 0.00001761
Iteration 224/1000 | Loss: 0.00001761
Iteration 225/1000 | Loss: 0.00001761
Iteration 226/1000 | Loss: 0.00001761
Iteration 227/1000 | Loss: 0.00001761
Iteration 228/1000 | Loss: 0.00001761
Iteration 229/1000 | Loss: 0.00001761
Iteration 230/1000 | Loss: 0.00001761
Iteration 231/1000 | Loss: 0.00001761
Iteration 232/1000 | Loss: 0.00001760
Iteration 233/1000 | Loss: 0.00001760
Iteration 234/1000 | Loss: 0.00001760
Iteration 235/1000 | Loss: 0.00001760
Iteration 236/1000 | Loss: 0.00001760
Iteration 237/1000 | Loss: 0.00001760
Iteration 238/1000 | Loss: 0.00001760
Iteration 239/1000 | Loss: 0.00001760
Iteration 240/1000 | Loss: 0.00001760
Iteration 241/1000 | Loss: 0.00001760
Iteration 242/1000 | Loss: 0.00001760
Iteration 243/1000 | Loss: 0.00001760
Iteration 244/1000 | Loss: 0.00001760
Iteration 245/1000 | Loss: 0.00001760
Iteration 246/1000 | Loss: 0.00001760
Iteration 247/1000 | Loss: 0.00001760
Iteration 248/1000 | Loss: 0.00001760
Iteration 249/1000 | Loss: 0.00001760
Iteration 250/1000 | Loss: 0.00001760
Iteration 251/1000 | Loss: 0.00001760
Iteration 252/1000 | Loss: 0.00001760
Iteration 253/1000 | Loss: 0.00001759
Iteration 254/1000 | Loss: 0.00001759
Iteration 255/1000 | Loss: 0.00001759
Iteration 256/1000 | Loss: 0.00001759
Iteration 257/1000 | Loss: 0.00001759
Iteration 258/1000 | Loss: 0.00001759
Iteration 259/1000 | Loss: 0.00001759
Iteration 260/1000 | Loss: 0.00001759
Iteration 261/1000 | Loss: 0.00001759
Iteration 262/1000 | Loss: 0.00001759
Iteration 263/1000 | Loss: 0.00001759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.7593281882000156e-05, 1.7593281882000156e-05, 1.7593281882000156e-05, 1.7593281882000156e-05, 1.7593281882000156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7593281882000156e-05

Optimization complete. Final v2v error: 3.4513583183288574 mm

Highest mean error: 9.523005485534668 mm for frame 14

Lowest mean error: 2.866522789001465 mm for frame 97

Saving results

Total time: 199.4338903427124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783999
Iteration 2/25 | Loss: 0.00090073
Iteration 3/25 | Loss: 0.00064559
Iteration 4/25 | Loss: 0.00061443
Iteration 5/25 | Loss: 0.00060523
Iteration 6/25 | Loss: 0.00060308
Iteration 7/25 | Loss: 0.00060257
Iteration 8/25 | Loss: 0.00060257
Iteration 9/25 | Loss: 0.00060257
Iteration 10/25 | Loss: 0.00060257
Iteration 11/25 | Loss: 0.00060257
Iteration 12/25 | Loss: 0.00060257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006025655311532319, 0.0006025655311532319, 0.0006025655311532319, 0.0006025655311532319, 0.0006025655311532319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006025655311532319

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01556063
Iteration 2/25 | Loss: 0.00027351
Iteration 3/25 | Loss: 0.00027351
Iteration 4/25 | Loss: 0.00027351
Iteration 5/25 | Loss: 0.00027351
Iteration 6/25 | Loss: 0.00027350
Iteration 7/25 | Loss: 0.00027350
Iteration 8/25 | Loss: 0.00027350
Iteration 9/25 | Loss: 0.00027350
Iteration 10/25 | Loss: 0.00027350
Iteration 11/25 | Loss: 0.00027350
Iteration 12/25 | Loss: 0.00027350
Iteration 13/25 | Loss: 0.00027350
Iteration 14/25 | Loss: 0.00027350
Iteration 15/25 | Loss: 0.00027350
Iteration 16/25 | Loss: 0.00027350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00027350356685929, 0.00027350356685929, 0.00027350356685929, 0.00027350356685929, 0.00027350356685929]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027350356685929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027350
Iteration 2/1000 | Loss: 0.00002309
Iteration 3/1000 | Loss: 0.00001668
Iteration 4/1000 | Loss: 0.00001547
Iteration 5/1000 | Loss: 0.00001470
Iteration 6/1000 | Loss: 0.00001427
Iteration 7/1000 | Loss: 0.00001399
Iteration 8/1000 | Loss: 0.00001386
Iteration 9/1000 | Loss: 0.00001372
Iteration 10/1000 | Loss: 0.00001360
Iteration 11/1000 | Loss: 0.00001352
Iteration 12/1000 | Loss: 0.00001350
Iteration 13/1000 | Loss: 0.00001349
Iteration 14/1000 | Loss: 0.00001349
Iteration 15/1000 | Loss: 0.00001347
Iteration 16/1000 | Loss: 0.00001346
Iteration 17/1000 | Loss: 0.00001344
Iteration 18/1000 | Loss: 0.00001344
Iteration 19/1000 | Loss: 0.00001342
Iteration 20/1000 | Loss: 0.00001342
Iteration 21/1000 | Loss: 0.00001341
Iteration 22/1000 | Loss: 0.00001339
Iteration 23/1000 | Loss: 0.00001335
Iteration 24/1000 | Loss: 0.00001334
Iteration 25/1000 | Loss: 0.00001329
Iteration 26/1000 | Loss: 0.00001329
Iteration 27/1000 | Loss: 0.00001328
Iteration 28/1000 | Loss: 0.00001328
Iteration 29/1000 | Loss: 0.00001327
Iteration 30/1000 | Loss: 0.00001327
Iteration 31/1000 | Loss: 0.00001326
Iteration 32/1000 | Loss: 0.00001326
Iteration 33/1000 | Loss: 0.00001324
Iteration 34/1000 | Loss: 0.00001324
Iteration 35/1000 | Loss: 0.00001324
Iteration 36/1000 | Loss: 0.00001324
Iteration 37/1000 | Loss: 0.00001324
Iteration 38/1000 | Loss: 0.00001324
Iteration 39/1000 | Loss: 0.00001324
Iteration 40/1000 | Loss: 0.00001324
Iteration 41/1000 | Loss: 0.00001323
Iteration 42/1000 | Loss: 0.00001323
Iteration 43/1000 | Loss: 0.00001323
Iteration 44/1000 | Loss: 0.00001322
Iteration 45/1000 | Loss: 0.00001322
Iteration 46/1000 | Loss: 0.00001321
Iteration 47/1000 | Loss: 0.00001321
Iteration 48/1000 | Loss: 0.00001321
Iteration 49/1000 | Loss: 0.00001321
Iteration 50/1000 | Loss: 0.00001321
Iteration 51/1000 | Loss: 0.00001321
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001320
Iteration 54/1000 | Loss: 0.00001320
Iteration 55/1000 | Loss: 0.00001320
Iteration 56/1000 | Loss: 0.00001320
Iteration 57/1000 | Loss: 0.00001320
Iteration 58/1000 | Loss: 0.00001320
Iteration 59/1000 | Loss: 0.00001320
Iteration 60/1000 | Loss: 0.00001320
Iteration 61/1000 | Loss: 0.00001320
Iteration 62/1000 | Loss: 0.00001319
Iteration 63/1000 | Loss: 0.00001319
Iteration 64/1000 | Loss: 0.00001319
Iteration 65/1000 | Loss: 0.00001318
Iteration 66/1000 | Loss: 0.00001317
Iteration 67/1000 | Loss: 0.00001316
Iteration 68/1000 | Loss: 0.00001316
Iteration 69/1000 | Loss: 0.00001316
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001316
Iteration 72/1000 | Loss: 0.00001315
Iteration 73/1000 | Loss: 0.00001314
Iteration 74/1000 | Loss: 0.00001313
Iteration 75/1000 | Loss: 0.00001313
Iteration 76/1000 | Loss: 0.00001313
Iteration 77/1000 | Loss: 0.00001313
Iteration 78/1000 | Loss: 0.00001313
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001313
Iteration 81/1000 | Loss: 0.00001312
Iteration 82/1000 | Loss: 0.00001312
Iteration 83/1000 | Loss: 0.00001312
Iteration 84/1000 | Loss: 0.00001312
Iteration 85/1000 | Loss: 0.00001311
Iteration 86/1000 | Loss: 0.00001311
Iteration 87/1000 | Loss: 0.00001309
Iteration 88/1000 | Loss: 0.00001309
Iteration 89/1000 | Loss: 0.00001308
Iteration 90/1000 | Loss: 0.00001308
Iteration 91/1000 | Loss: 0.00001308
Iteration 92/1000 | Loss: 0.00001308
Iteration 93/1000 | Loss: 0.00001308
Iteration 94/1000 | Loss: 0.00001308
Iteration 95/1000 | Loss: 0.00001308
Iteration 96/1000 | Loss: 0.00001308
Iteration 97/1000 | Loss: 0.00001307
Iteration 98/1000 | Loss: 0.00001307
Iteration 99/1000 | Loss: 0.00001307
Iteration 100/1000 | Loss: 0.00001307
Iteration 101/1000 | Loss: 0.00001307
Iteration 102/1000 | Loss: 0.00001307
Iteration 103/1000 | Loss: 0.00001307
Iteration 104/1000 | Loss: 0.00001306
Iteration 105/1000 | Loss: 0.00001306
Iteration 106/1000 | Loss: 0.00001306
Iteration 107/1000 | Loss: 0.00001306
Iteration 108/1000 | Loss: 0.00001306
Iteration 109/1000 | Loss: 0.00001306
Iteration 110/1000 | Loss: 0.00001306
Iteration 111/1000 | Loss: 0.00001306
Iteration 112/1000 | Loss: 0.00001306
Iteration 113/1000 | Loss: 0.00001305
Iteration 114/1000 | Loss: 0.00001305
Iteration 115/1000 | Loss: 0.00001305
Iteration 116/1000 | Loss: 0.00001305
Iteration 117/1000 | Loss: 0.00001305
Iteration 118/1000 | Loss: 0.00001305
Iteration 119/1000 | Loss: 0.00001305
Iteration 120/1000 | Loss: 0.00001305
Iteration 121/1000 | Loss: 0.00001305
Iteration 122/1000 | Loss: 0.00001305
Iteration 123/1000 | Loss: 0.00001305
Iteration 124/1000 | Loss: 0.00001305
Iteration 125/1000 | Loss: 0.00001305
Iteration 126/1000 | Loss: 0.00001305
Iteration 127/1000 | Loss: 0.00001305
Iteration 128/1000 | Loss: 0.00001304
Iteration 129/1000 | Loss: 0.00001304
Iteration 130/1000 | Loss: 0.00001304
Iteration 131/1000 | Loss: 0.00001304
Iteration 132/1000 | Loss: 0.00001304
Iteration 133/1000 | Loss: 0.00001304
Iteration 134/1000 | Loss: 0.00001304
Iteration 135/1000 | Loss: 0.00001304
Iteration 136/1000 | Loss: 0.00001304
Iteration 137/1000 | Loss: 0.00001304
Iteration 138/1000 | Loss: 0.00001304
Iteration 139/1000 | Loss: 0.00001304
Iteration 140/1000 | Loss: 0.00001304
Iteration 141/1000 | Loss: 0.00001304
Iteration 142/1000 | Loss: 0.00001304
Iteration 143/1000 | Loss: 0.00001304
Iteration 144/1000 | Loss: 0.00001303
Iteration 145/1000 | Loss: 0.00001303
Iteration 146/1000 | Loss: 0.00001303
Iteration 147/1000 | Loss: 0.00001303
Iteration 148/1000 | Loss: 0.00001303
Iteration 149/1000 | Loss: 0.00001303
Iteration 150/1000 | Loss: 0.00001303
Iteration 151/1000 | Loss: 0.00001303
Iteration 152/1000 | Loss: 0.00001303
Iteration 153/1000 | Loss: 0.00001303
Iteration 154/1000 | Loss: 0.00001303
Iteration 155/1000 | Loss: 0.00001303
Iteration 156/1000 | Loss: 0.00001303
Iteration 157/1000 | Loss: 0.00001303
Iteration 158/1000 | Loss: 0.00001303
Iteration 159/1000 | Loss: 0.00001303
Iteration 160/1000 | Loss: 0.00001303
Iteration 161/1000 | Loss: 0.00001303
Iteration 162/1000 | Loss: 0.00001303
Iteration 163/1000 | Loss: 0.00001303
Iteration 164/1000 | Loss: 0.00001303
Iteration 165/1000 | Loss: 0.00001302
Iteration 166/1000 | Loss: 0.00001302
Iteration 167/1000 | Loss: 0.00001302
Iteration 168/1000 | Loss: 0.00001302
Iteration 169/1000 | Loss: 0.00001302
Iteration 170/1000 | Loss: 0.00001302
Iteration 171/1000 | Loss: 0.00001302
Iteration 172/1000 | Loss: 0.00001302
Iteration 173/1000 | Loss: 0.00001302
Iteration 174/1000 | Loss: 0.00001302
Iteration 175/1000 | Loss: 0.00001302
Iteration 176/1000 | Loss: 0.00001302
Iteration 177/1000 | Loss: 0.00001302
Iteration 178/1000 | Loss: 0.00001302
Iteration 179/1000 | Loss: 0.00001302
Iteration 180/1000 | Loss: 0.00001302
Iteration 181/1000 | Loss: 0.00001302
Iteration 182/1000 | Loss: 0.00001302
Iteration 183/1000 | Loss: 0.00001302
Iteration 184/1000 | Loss: 0.00001301
Iteration 185/1000 | Loss: 0.00001301
Iteration 186/1000 | Loss: 0.00001301
Iteration 187/1000 | Loss: 0.00001301
Iteration 188/1000 | Loss: 0.00001301
Iteration 189/1000 | Loss: 0.00001301
Iteration 190/1000 | Loss: 0.00001301
Iteration 191/1000 | Loss: 0.00001301
Iteration 192/1000 | Loss: 0.00001301
Iteration 193/1000 | Loss: 0.00001301
Iteration 194/1000 | Loss: 0.00001301
Iteration 195/1000 | Loss: 0.00001301
Iteration 196/1000 | Loss: 0.00001301
Iteration 197/1000 | Loss: 0.00001301
Iteration 198/1000 | Loss: 0.00001301
Iteration 199/1000 | Loss: 0.00001301
Iteration 200/1000 | Loss: 0.00001301
Iteration 201/1000 | Loss: 0.00001301
Iteration 202/1000 | Loss: 0.00001301
Iteration 203/1000 | Loss: 0.00001301
Iteration 204/1000 | Loss: 0.00001301
Iteration 205/1000 | Loss: 0.00001301
Iteration 206/1000 | Loss: 0.00001301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.3014086107432377e-05, 1.3014086107432377e-05, 1.3014086107432377e-05, 1.3014086107432377e-05, 1.3014086107432377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3014086107432377e-05

Optimization complete. Final v2v error: 3.0821990966796875 mm

Highest mean error: 3.357179880142212 mm for frame 96

Lowest mean error: 2.9271295070648193 mm for frame 158

Saving results

Total time: 39.921767473220825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795493
Iteration 2/25 | Loss: 0.00123346
Iteration 3/25 | Loss: 0.00078385
Iteration 4/25 | Loss: 0.00067284
Iteration 5/25 | Loss: 0.00064567
Iteration 6/25 | Loss: 0.00063874
Iteration 7/25 | Loss: 0.00063709
Iteration 8/25 | Loss: 0.00063671
Iteration 9/25 | Loss: 0.00063671
Iteration 10/25 | Loss: 0.00063671
Iteration 11/25 | Loss: 0.00063671
Iteration 12/25 | Loss: 0.00063671
Iteration 13/25 | Loss: 0.00063671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000636707991361618, 0.000636707991361618, 0.000636707991361618, 0.000636707991361618, 0.000636707991361618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000636707991361618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52411377
Iteration 2/25 | Loss: 0.00038475
Iteration 3/25 | Loss: 0.00038475
Iteration 4/25 | Loss: 0.00038475
Iteration 5/25 | Loss: 0.00038475
Iteration 6/25 | Loss: 0.00038475
Iteration 7/25 | Loss: 0.00038475
Iteration 8/25 | Loss: 0.00038475
Iteration 9/25 | Loss: 0.00038475
Iteration 10/25 | Loss: 0.00038475
Iteration 11/25 | Loss: 0.00038475
Iteration 12/25 | Loss: 0.00038475
Iteration 13/25 | Loss: 0.00038475
Iteration 14/25 | Loss: 0.00038475
Iteration 15/25 | Loss: 0.00038475
Iteration 16/25 | Loss: 0.00038475
Iteration 17/25 | Loss: 0.00038475
Iteration 18/25 | Loss: 0.00038475
Iteration 19/25 | Loss: 0.00038475
Iteration 20/25 | Loss: 0.00038475
Iteration 21/25 | Loss: 0.00038475
Iteration 22/25 | Loss: 0.00038475
Iteration 23/25 | Loss: 0.00038475
Iteration 24/25 | Loss: 0.00038475
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00038474847679026425, 0.00038474847679026425, 0.00038474847679026425, 0.00038474847679026425, 0.00038474847679026425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038474847679026425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038475
Iteration 2/1000 | Loss: 0.00002612
Iteration 3/1000 | Loss: 0.00001824
Iteration 4/1000 | Loss: 0.00001542
Iteration 5/1000 | Loss: 0.00001427
Iteration 6/1000 | Loss: 0.00001361
Iteration 7/1000 | Loss: 0.00001332
Iteration 8/1000 | Loss: 0.00001297
Iteration 9/1000 | Loss: 0.00001266
Iteration 10/1000 | Loss: 0.00001244
Iteration 11/1000 | Loss: 0.00001236
Iteration 12/1000 | Loss: 0.00001229
Iteration 13/1000 | Loss: 0.00001227
Iteration 14/1000 | Loss: 0.00001223
Iteration 15/1000 | Loss: 0.00001220
Iteration 16/1000 | Loss: 0.00001220
Iteration 17/1000 | Loss: 0.00001215
Iteration 18/1000 | Loss: 0.00001213
Iteration 19/1000 | Loss: 0.00001211
Iteration 20/1000 | Loss: 0.00001211
Iteration 21/1000 | Loss: 0.00001210
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001210
Iteration 24/1000 | Loss: 0.00001210
Iteration 25/1000 | Loss: 0.00001210
Iteration 26/1000 | Loss: 0.00001210
Iteration 27/1000 | Loss: 0.00001210
Iteration 28/1000 | Loss: 0.00001210
Iteration 29/1000 | Loss: 0.00001210
Iteration 30/1000 | Loss: 0.00001209
Iteration 31/1000 | Loss: 0.00001209
Iteration 32/1000 | Loss: 0.00001209
Iteration 33/1000 | Loss: 0.00001207
Iteration 34/1000 | Loss: 0.00001207
Iteration 35/1000 | Loss: 0.00001207
Iteration 36/1000 | Loss: 0.00001206
Iteration 37/1000 | Loss: 0.00001205
Iteration 38/1000 | Loss: 0.00001205
Iteration 39/1000 | Loss: 0.00001205
Iteration 40/1000 | Loss: 0.00001205
Iteration 41/1000 | Loss: 0.00001205
Iteration 42/1000 | Loss: 0.00001204
Iteration 43/1000 | Loss: 0.00001204
Iteration 44/1000 | Loss: 0.00001204
Iteration 45/1000 | Loss: 0.00001204
Iteration 46/1000 | Loss: 0.00001204
Iteration 47/1000 | Loss: 0.00001204
Iteration 48/1000 | Loss: 0.00001204
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001204
Iteration 51/1000 | Loss: 0.00001204
Iteration 52/1000 | Loss: 0.00001203
Iteration 53/1000 | Loss: 0.00001203
Iteration 54/1000 | Loss: 0.00001202
Iteration 55/1000 | Loss: 0.00001202
Iteration 56/1000 | Loss: 0.00001202
Iteration 57/1000 | Loss: 0.00001201
Iteration 58/1000 | Loss: 0.00001201
Iteration 59/1000 | Loss: 0.00001200
Iteration 60/1000 | Loss: 0.00001200
Iteration 61/1000 | Loss: 0.00001200
Iteration 62/1000 | Loss: 0.00001200
Iteration 63/1000 | Loss: 0.00001200
Iteration 64/1000 | Loss: 0.00001200
Iteration 65/1000 | Loss: 0.00001200
Iteration 66/1000 | Loss: 0.00001199
Iteration 67/1000 | Loss: 0.00001199
Iteration 68/1000 | Loss: 0.00001199
Iteration 69/1000 | Loss: 0.00001198
Iteration 70/1000 | Loss: 0.00001198
Iteration 71/1000 | Loss: 0.00001198
Iteration 72/1000 | Loss: 0.00001198
Iteration 73/1000 | Loss: 0.00001197
Iteration 74/1000 | Loss: 0.00001197
Iteration 75/1000 | Loss: 0.00001197
Iteration 76/1000 | Loss: 0.00001197
Iteration 77/1000 | Loss: 0.00001197
Iteration 78/1000 | Loss: 0.00001197
Iteration 79/1000 | Loss: 0.00001197
Iteration 80/1000 | Loss: 0.00001197
Iteration 81/1000 | Loss: 0.00001196
Iteration 82/1000 | Loss: 0.00001196
Iteration 83/1000 | Loss: 0.00001196
Iteration 84/1000 | Loss: 0.00001196
Iteration 85/1000 | Loss: 0.00001195
Iteration 86/1000 | Loss: 0.00001195
Iteration 87/1000 | Loss: 0.00001195
Iteration 88/1000 | Loss: 0.00001195
Iteration 89/1000 | Loss: 0.00001195
Iteration 90/1000 | Loss: 0.00001195
Iteration 91/1000 | Loss: 0.00001195
Iteration 92/1000 | Loss: 0.00001195
Iteration 93/1000 | Loss: 0.00001195
Iteration 94/1000 | Loss: 0.00001194
Iteration 95/1000 | Loss: 0.00001194
Iteration 96/1000 | Loss: 0.00001194
Iteration 97/1000 | Loss: 0.00001194
Iteration 98/1000 | Loss: 0.00001194
Iteration 99/1000 | Loss: 0.00001194
Iteration 100/1000 | Loss: 0.00001194
Iteration 101/1000 | Loss: 0.00001194
Iteration 102/1000 | Loss: 0.00001194
Iteration 103/1000 | Loss: 0.00001193
Iteration 104/1000 | Loss: 0.00001193
Iteration 105/1000 | Loss: 0.00001193
Iteration 106/1000 | Loss: 0.00001193
Iteration 107/1000 | Loss: 0.00001193
Iteration 108/1000 | Loss: 0.00001193
Iteration 109/1000 | Loss: 0.00001192
Iteration 110/1000 | Loss: 0.00001192
Iteration 111/1000 | Loss: 0.00001192
Iteration 112/1000 | Loss: 0.00001192
Iteration 113/1000 | Loss: 0.00001191
Iteration 114/1000 | Loss: 0.00001191
Iteration 115/1000 | Loss: 0.00001191
Iteration 116/1000 | Loss: 0.00001191
Iteration 117/1000 | Loss: 0.00001191
Iteration 118/1000 | Loss: 0.00001191
Iteration 119/1000 | Loss: 0.00001191
Iteration 120/1000 | Loss: 0.00001191
Iteration 121/1000 | Loss: 0.00001190
Iteration 122/1000 | Loss: 0.00001190
Iteration 123/1000 | Loss: 0.00001190
Iteration 124/1000 | Loss: 0.00001190
Iteration 125/1000 | Loss: 0.00001190
Iteration 126/1000 | Loss: 0.00001190
Iteration 127/1000 | Loss: 0.00001190
Iteration 128/1000 | Loss: 0.00001190
Iteration 129/1000 | Loss: 0.00001190
Iteration 130/1000 | Loss: 0.00001190
Iteration 131/1000 | Loss: 0.00001190
Iteration 132/1000 | Loss: 0.00001189
Iteration 133/1000 | Loss: 0.00001189
Iteration 134/1000 | Loss: 0.00001189
Iteration 135/1000 | Loss: 0.00001189
Iteration 136/1000 | Loss: 0.00001189
Iteration 137/1000 | Loss: 0.00001189
Iteration 138/1000 | Loss: 0.00001189
Iteration 139/1000 | Loss: 0.00001189
Iteration 140/1000 | Loss: 0.00001189
Iteration 141/1000 | Loss: 0.00001189
Iteration 142/1000 | Loss: 0.00001188
Iteration 143/1000 | Loss: 0.00001188
Iteration 144/1000 | Loss: 0.00001188
Iteration 145/1000 | Loss: 0.00001188
Iteration 146/1000 | Loss: 0.00001188
Iteration 147/1000 | Loss: 0.00001188
Iteration 148/1000 | Loss: 0.00001188
Iteration 149/1000 | Loss: 0.00001188
Iteration 150/1000 | Loss: 0.00001188
Iteration 151/1000 | Loss: 0.00001188
Iteration 152/1000 | Loss: 0.00001188
Iteration 153/1000 | Loss: 0.00001188
Iteration 154/1000 | Loss: 0.00001188
Iteration 155/1000 | Loss: 0.00001187
Iteration 156/1000 | Loss: 0.00001187
Iteration 157/1000 | Loss: 0.00001187
Iteration 158/1000 | Loss: 0.00001187
Iteration 159/1000 | Loss: 0.00001187
Iteration 160/1000 | Loss: 0.00001187
Iteration 161/1000 | Loss: 0.00001187
Iteration 162/1000 | Loss: 0.00001187
Iteration 163/1000 | Loss: 0.00001187
Iteration 164/1000 | Loss: 0.00001187
Iteration 165/1000 | Loss: 0.00001187
Iteration 166/1000 | Loss: 0.00001187
Iteration 167/1000 | Loss: 0.00001187
Iteration 168/1000 | Loss: 0.00001187
Iteration 169/1000 | Loss: 0.00001187
Iteration 170/1000 | Loss: 0.00001187
Iteration 171/1000 | Loss: 0.00001186
Iteration 172/1000 | Loss: 0.00001186
Iteration 173/1000 | Loss: 0.00001186
Iteration 174/1000 | Loss: 0.00001186
Iteration 175/1000 | Loss: 0.00001186
Iteration 176/1000 | Loss: 0.00001186
Iteration 177/1000 | Loss: 0.00001185
Iteration 178/1000 | Loss: 0.00001185
Iteration 179/1000 | Loss: 0.00001185
Iteration 180/1000 | Loss: 0.00001185
Iteration 181/1000 | Loss: 0.00001185
Iteration 182/1000 | Loss: 0.00001185
Iteration 183/1000 | Loss: 0.00001185
Iteration 184/1000 | Loss: 0.00001185
Iteration 185/1000 | Loss: 0.00001185
Iteration 186/1000 | Loss: 0.00001185
Iteration 187/1000 | Loss: 0.00001185
Iteration 188/1000 | Loss: 0.00001185
Iteration 189/1000 | Loss: 0.00001185
Iteration 190/1000 | Loss: 0.00001185
Iteration 191/1000 | Loss: 0.00001184
Iteration 192/1000 | Loss: 0.00001184
Iteration 193/1000 | Loss: 0.00001184
Iteration 194/1000 | Loss: 0.00001184
Iteration 195/1000 | Loss: 0.00001184
Iteration 196/1000 | Loss: 0.00001184
Iteration 197/1000 | Loss: 0.00001184
Iteration 198/1000 | Loss: 0.00001184
Iteration 199/1000 | Loss: 0.00001184
Iteration 200/1000 | Loss: 0.00001184
Iteration 201/1000 | Loss: 0.00001184
Iteration 202/1000 | Loss: 0.00001184
Iteration 203/1000 | Loss: 0.00001184
Iteration 204/1000 | Loss: 0.00001184
Iteration 205/1000 | Loss: 0.00001184
Iteration 206/1000 | Loss: 0.00001184
Iteration 207/1000 | Loss: 0.00001184
Iteration 208/1000 | Loss: 0.00001184
Iteration 209/1000 | Loss: 0.00001184
Iteration 210/1000 | Loss: 0.00001184
Iteration 211/1000 | Loss: 0.00001183
Iteration 212/1000 | Loss: 0.00001183
Iteration 213/1000 | Loss: 0.00001183
Iteration 214/1000 | Loss: 0.00001183
Iteration 215/1000 | Loss: 0.00001183
Iteration 216/1000 | Loss: 0.00001183
Iteration 217/1000 | Loss: 0.00001183
Iteration 218/1000 | Loss: 0.00001183
Iteration 219/1000 | Loss: 0.00001183
Iteration 220/1000 | Loss: 0.00001183
Iteration 221/1000 | Loss: 0.00001183
Iteration 222/1000 | Loss: 0.00001183
Iteration 223/1000 | Loss: 0.00001183
Iteration 224/1000 | Loss: 0.00001183
Iteration 225/1000 | Loss: 0.00001183
Iteration 226/1000 | Loss: 0.00001183
Iteration 227/1000 | Loss: 0.00001183
Iteration 228/1000 | Loss: 0.00001183
Iteration 229/1000 | Loss: 0.00001183
Iteration 230/1000 | Loss: 0.00001182
Iteration 231/1000 | Loss: 0.00001182
Iteration 232/1000 | Loss: 0.00001182
Iteration 233/1000 | Loss: 0.00001182
Iteration 234/1000 | Loss: 0.00001182
Iteration 235/1000 | Loss: 0.00001182
Iteration 236/1000 | Loss: 0.00001182
Iteration 237/1000 | Loss: 0.00001182
Iteration 238/1000 | Loss: 0.00001182
Iteration 239/1000 | Loss: 0.00001182
Iteration 240/1000 | Loss: 0.00001182
Iteration 241/1000 | Loss: 0.00001182
Iteration 242/1000 | Loss: 0.00001182
Iteration 243/1000 | Loss: 0.00001182
Iteration 244/1000 | Loss: 0.00001181
Iteration 245/1000 | Loss: 0.00001181
Iteration 246/1000 | Loss: 0.00001181
Iteration 247/1000 | Loss: 0.00001181
Iteration 248/1000 | Loss: 0.00001181
Iteration 249/1000 | Loss: 0.00001181
Iteration 250/1000 | Loss: 0.00001181
Iteration 251/1000 | Loss: 0.00001181
Iteration 252/1000 | Loss: 0.00001181
Iteration 253/1000 | Loss: 0.00001181
Iteration 254/1000 | Loss: 0.00001181
Iteration 255/1000 | Loss: 0.00001181
Iteration 256/1000 | Loss: 0.00001181
Iteration 257/1000 | Loss: 0.00001181
Iteration 258/1000 | Loss: 0.00001181
Iteration 259/1000 | Loss: 0.00001181
Iteration 260/1000 | Loss: 0.00001181
Iteration 261/1000 | Loss: 0.00001181
Iteration 262/1000 | Loss: 0.00001180
Iteration 263/1000 | Loss: 0.00001180
Iteration 264/1000 | Loss: 0.00001180
Iteration 265/1000 | Loss: 0.00001180
Iteration 266/1000 | Loss: 0.00001180
Iteration 267/1000 | Loss: 0.00001180
Iteration 268/1000 | Loss: 0.00001180
Iteration 269/1000 | Loss: 0.00001180
Iteration 270/1000 | Loss: 0.00001180
Iteration 271/1000 | Loss: 0.00001180
Iteration 272/1000 | Loss: 0.00001180
Iteration 273/1000 | Loss: 0.00001180
Iteration 274/1000 | Loss: 0.00001180
Iteration 275/1000 | Loss: 0.00001180
Iteration 276/1000 | Loss: 0.00001180
Iteration 277/1000 | Loss: 0.00001180
Iteration 278/1000 | Loss: 0.00001180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 278. Stopping optimization.
Last 5 losses: [1.1802530025306623e-05, 1.1802530025306623e-05, 1.1802530025306623e-05, 1.1802530025306623e-05, 1.1802530025306623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1802530025306623e-05

Optimization complete. Final v2v error: 2.8993358612060547 mm

Highest mean error: 3.514569044113159 mm for frame 58

Lowest mean error: 2.5213096141815186 mm for frame 22

Saving results

Total time: 45.942068338394165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013593
Iteration 2/25 | Loss: 0.00155267
Iteration 3/25 | Loss: 0.00094755
Iteration 4/25 | Loss: 0.00089158
Iteration 5/25 | Loss: 0.00088151
Iteration 6/25 | Loss: 0.00087848
Iteration 7/25 | Loss: 0.00087787
Iteration 8/25 | Loss: 0.00087787
Iteration 9/25 | Loss: 0.00087787
Iteration 10/25 | Loss: 0.00087787
Iteration 11/25 | Loss: 0.00087787
Iteration 12/25 | Loss: 0.00087787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000877867394592613, 0.000877867394592613, 0.000877867394592613, 0.000877867394592613, 0.000877867394592613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000877867394592613

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.59075946
Iteration 2/25 | Loss: 0.00017490
Iteration 3/25 | Loss: 0.00017490
Iteration 4/25 | Loss: 0.00017490
Iteration 5/25 | Loss: 0.00017489
Iteration 6/25 | Loss: 0.00017489
Iteration 7/25 | Loss: 0.00017489
Iteration 8/25 | Loss: 0.00017489
Iteration 9/25 | Loss: 0.00017489
Iteration 10/25 | Loss: 0.00017489
Iteration 11/25 | Loss: 0.00017489
Iteration 12/25 | Loss: 0.00017489
Iteration 13/25 | Loss: 0.00017489
Iteration 14/25 | Loss: 0.00017489
Iteration 15/25 | Loss: 0.00017489
Iteration 16/25 | Loss: 0.00017489
Iteration 17/25 | Loss: 0.00017489
Iteration 18/25 | Loss: 0.00017489
Iteration 19/25 | Loss: 0.00017489
Iteration 20/25 | Loss: 0.00017489
Iteration 21/25 | Loss: 0.00017489
Iteration 22/25 | Loss: 0.00017489
Iteration 23/25 | Loss: 0.00017489
Iteration 24/25 | Loss: 0.00017489
Iteration 25/25 | Loss: 0.00017489

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017489
Iteration 2/1000 | Loss: 0.00005540
Iteration 3/1000 | Loss: 0.00004175
Iteration 4/1000 | Loss: 0.00003816
Iteration 5/1000 | Loss: 0.00003622
Iteration 6/1000 | Loss: 0.00003542
Iteration 7/1000 | Loss: 0.00003463
Iteration 8/1000 | Loss: 0.00003398
Iteration 9/1000 | Loss: 0.00003360
Iteration 10/1000 | Loss: 0.00003329
Iteration 11/1000 | Loss: 0.00003306
Iteration 12/1000 | Loss: 0.00003289
Iteration 13/1000 | Loss: 0.00003272
Iteration 14/1000 | Loss: 0.00003259
Iteration 15/1000 | Loss: 0.00003252
Iteration 16/1000 | Loss: 0.00003245
Iteration 17/1000 | Loss: 0.00003239
Iteration 18/1000 | Loss: 0.00003236
Iteration 19/1000 | Loss: 0.00003233
Iteration 20/1000 | Loss: 0.00003228
Iteration 21/1000 | Loss: 0.00003225
Iteration 22/1000 | Loss: 0.00003223
Iteration 23/1000 | Loss: 0.00003221
Iteration 24/1000 | Loss: 0.00003220
Iteration 25/1000 | Loss: 0.00003220
Iteration 26/1000 | Loss: 0.00003220
Iteration 27/1000 | Loss: 0.00003220
Iteration 28/1000 | Loss: 0.00003220
Iteration 29/1000 | Loss: 0.00003219
Iteration 30/1000 | Loss: 0.00003217
Iteration 31/1000 | Loss: 0.00003216
Iteration 32/1000 | Loss: 0.00003216
Iteration 33/1000 | Loss: 0.00003216
Iteration 34/1000 | Loss: 0.00003216
Iteration 35/1000 | Loss: 0.00003216
Iteration 36/1000 | Loss: 0.00003216
Iteration 37/1000 | Loss: 0.00003216
Iteration 38/1000 | Loss: 0.00003216
Iteration 39/1000 | Loss: 0.00003216
Iteration 40/1000 | Loss: 0.00003216
Iteration 41/1000 | Loss: 0.00003215
Iteration 42/1000 | Loss: 0.00003215
Iteration 43/1000 | Loss: 0.00003215
Iteration 44/1000 | Loss: 0.00003215
Iteration 45/1000 | Loss: 0.00003213
Iteration 46/1000 | Loss: 0.00003213
Iteration 47/1000 | Loss: 0.00003213
Iteration 48/1000 | Loss: 0.00003213
Iteration 49/1000 | Loss: 0.00003213
Iteration 50/1000 | Loss: 0.00003213
Iteration 51/1000 | Loss: 0.00003213
Iteration 52/1000 | Loss: 0.00003213
Iteration 53/1000 | Loss: 0.00003213
Iteration 54/1000 | Loss: 0.00003213
Iteration 55/1000 | Loss: 0.00003213
Iteration 56/1000 | Loss: 0.00003212
Iteration 57/1000 | Loss: 0.00003212
Iteration 58/1000 | Loss: 0.00003212
Iteration 59/1000 | Loss: 0.00003211
Iteration 60/1000 | Loss: 0.00003211
Iteration 61/1000 | Loss: 0.00003211
Iteration 62/1000 | Loss: 0.00003211
Iteration 63/1000 | Loss: 0.00003211
Iteration 64/1000 | Loss: 0.00003210
Iteration 65/1000 | Loss: 0.00003210
Iteration 66/1000 | Loss: 0.00003210
Iteration 67/1000 | Loss: 0.00003210
Iteration 68/1000 | Loss: 0.00003210
Iteration 69/1000 | Loss: 0.00003210
Iteration 70/1000 | Loss: 0.00003210
Iteration 71/1000 | Loss: 0.00003210
Iteration 72/1000 | Loss: 0.00003209
Iteration 73/1000 | Loss: 0.00003209
Iteration 74/1000 | Loss: 0.00003209
Iteration 75/1000 | Loss: 0.00003209
Iteration 76/1000 | Loss: 0.00003209
Iteration 77/1000 | Loss: 0.00003209
Iteration 78/1000 | Loss: 0.00003209
Iteration 79/1000 | Loss: 0.00003209
Iteration 80/1000 | Loss: 0.00003209
Iteration 81/1000 | Loss: 0.00003209
Iteration 82/1000 | Loss: 0.00003209
Iteration 83/1000 | Loss: 0.00003209
Iteration 84/1000 | Loss: 0.00003209
Iteration 85/1000 | Loss: 0.00003208
Iteration 86/1000 | Loss: 0.00003208
Iteration 87/1000 | Loss: 0.00003207
Iteration 88/1000 | Loss: 0.00003207
Iteration 89/1000 | Loss: 0.00003206
Iteration 90/1000 | Loss: 0.00003206
Iteration 91/1000 | Loss: 0.00003206
Iteration 92/1000 | Loss: 0.00003206
Iteration 93/1000 | Loss: 0.00003206
Iteration 94/1000 | Loss: 0.00003206
Iteration 95/1000 | Loss: 0.00003206
Iteration 96/1000 | Loss: 0.00003206
Iteration 97/1000 | Loss: 0.00003206
Iteration 98/1000 | Loss: 0.00003206
Iteration 99/1000 | Loss: 0.00003206
Iteration 100/1000 | Loss: 0.00003206
Iteration 101/1000 | Loss: 0.00003206
Iteration 102/1000 | Loss: 0.00003206
Iteration 103/1000 | Loss: 0.00003206
Iteration 104/1000 | Loss: 0.00003205
Iteration 105/1000 | Loss: 0.00003205
Iteration 106/1000 | Loss: 0.00003205
Iteration 107/1000 | Loss: 0.00003205
Iteration 108/1000 | Loss: 0.00003204
Iteration 109/1000 | Loss: 0.00003204
Iteration 110/1000 | Loss: 0.00003204
Iteration 111/1000 | Loss: 0.00003204
Iteration 112/1000 | Loss: 0.00003204
Iteration 113/1000 | Loss: 0.00003204
Iteration 114/1000 | Loss: 0.00003204
Iteration 115/1000 | Loss: 0.00003204
Iteration 116/1000 | Loss: 0.00003204
Iteration 117/1000 | Loss: 0.00003203
Iteration 118/1000 | Loss: 0.00003203
Iteration 119/1000 | Loss: 0.00003203
Iteration 120/1000 | Loss: 0.00003203
Iteration 121/1000 | Loss: 0.00003203
Iteration 122/1000 | Loss: 0.00003203
Iteration 123/1000 | Loss: 0.00003203
Iteration 124/1000 | Loss: 0.00003203
Iteration 125/1000 | Loss: 0.00003202
Iteration 126/1000 | Loss: 0.00003202
Iteration 127/1000 | Loss: 0.00003202
Iteration 128/1000 | Loss: 0.00003202
Iteration 129/1000 | Loss: 0.00003202
Iteration 130/1000 | Loss: 0.00003202
Iteration 131/1000 | Loss: 0.00003202
Iteration 132/1000 | Loss: 0.00003202
Iteration 133/1000 | Loss: 0.00003201
Iteration 134/1000 | Loss: 0.00003201
Iteration 135/1000 | Loss: 0.00003201
Iteration 136/1000 | Loss: 0.00003201
Iteration 137/1000 | Loss: 0.00003201
Iteration 138/1000 | Loss: 0.00003201
Iteration 139/1000 | Loss: 0.00003201
Iteration 140/1000 | Loss: 0.00003201
Iteration 141/1000 | Loss: 0.00003201
Iteration 142/1000 | Loss: 0.00003200
Iteration 143/1000 | Loss: 0.00003200
Iteration 144/1000 | Loss: 0.00003200
Iteration 145/1000 | Loss: 0.00003200
Iteration 146/1000 | Loss: 0.00003200
Iteration 147/1000 | Loss: 0.00003200
Iteration 148/1000 | Loss: 0.00003200
Iteration 149/1000 | Loss: 0.00003200
Iteration 150/1000 | Loss: 0.00003200
Iteration 151/1000 | Loss: 0.00003200
Iteration 152/1000 | Loss: 0.00003200
Iteration 153/1000 | Loss: 0.00003200
Iteration 154/1000 | Loss: 0.00003200
Iteration 155/1000 | Loss: 0.00003199
Iteration 156/1000 | Loss: 0.00003199
Iteration 157/1000 | Loss: 0.00003199
Iteration 158/1000 | Loss: 0.00003199
Iteration 159/1000 | Loss: 0.00003199
Iteration 160/1000 | Loss: 0.00003199
Iteration 161/1000 | Loss: 0.00003199
Iteration 162/1000 | Loss: 0.00003199
Iteration 163/1000 | Loss: 0.00003199
Iteration 164/1000 | Loss: 0.00003199
Iteration 165/1000 | Loss: 0.00003199
Iteration 166/1000 | Loss: 0.00003199
Iteration 167/1000 | Loss: 0.00003199
Iteration 168/1000 | Loss: 0.00003198
Iteration 169/1000 | Loss: 0.00003198
Iteration 170/1000 | Loss: 0.00003198
Iteration 171/1000 | Loss: 0.00003198
Iteration 172/1000 | Loss: 0.00003198
Iteration 173/1000 | Loss: 0.00003198
Iteration 174/1000 | Loss: 0.00003198
Iteration 175/1000 | Loss: 0.00003198
Iteration 176/1000 | Loss: 0.00003198
Iteration 177/1000 | Loss: 0.00003197
Iteration 178/1000 | Loss: 0.00003197
Iteration 179/1000 | Loss: 0.00003197
Iteration 180/1000 | Loss: 0.00003197
Iteration 181/1000 | Loss: 0.00003197
Iteration 182/1000 | Loss: 0.00003197
Iteration 183/1000 | Loss: 0.00003197
Iteration 184/1000 | Loss: 0.00003197
Iteration 185/1000 | Loss: 0.00003197
Iteration 186/1000 | Loss: 0.00003197
Iteration 187/1000 | Loss: 0.00003197
Iteration 188/1000 | Loss: 0.00003197
Iteration 189/1000 | Loss: 0.00003196
Iteration 190/1000 | Loss: 0.00003196
Iteration 191/1000 | Loss: 0.00003196
Iteration 192/1000 | Loss: 0.00003196
Iteration 193/1000 | Loss: 0.00003196
Iteration 194/1000 | Loss: 0.00003196
Iteration 195/1000 | Loss: 0.00003196
Iteration 196/1000 | Loss: 0.00003196
Iteration 197/1000 | Loss: 0.00003196
Iteration 198/1000 | Loss: 0.00003196
Iteration 199/1000 | Loss: 0.00003196
Iteration 200/1000 | Loss: 0.00003196
Iteration 201/1000 | Loss: 0.00003196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [3.1959360057953745e-05, 3.1959360057953745e-05, 3.1959360057953745e-05, 3.1959360057953745e-05, 3.1959360057953745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1959360057953745e-05

Optimization complete. Final v2v error: 4.756852149963379 mm

Highest mean error: 5.099491596221924 mm for frame 97

Lowest mean error: 4.016905307769775 mm for frame 41

Saving results

Total time: 48.27069354057312
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790517
Iteration 2/25 | Loss: 0.00197873
Iteration 3/25 | Loss: 0.00113699
Iteration 4/25 | Loss: 0.00092240
Iteration 5/25 | Loss: 0.00090222
Iteration 6/25 | Loss: 0.00084532
Iteration 7/25 | Loss: 0.00081161
Iteration 8/25 | Loss: 0.00082407
Iteration 9/25 | Loss: 0.00083230
Iteration 10/25 | Loss: 0.00081773
Iteration 11/25 | Loss: 0.00081007
Iteration 12/25 | Loss: 0.00079899
Iteration 13/25 | Loss: 0.00079374
Iteration 14/25 | Loss: 0.00079965
Iteration 15/25 | Loss: 0.00079387
Iteration 16/25 | Loss: 0.00079703
Iteration 17/25 | Loss: 0.00078950
Iteration 18/25 | Loss: 0.00078966
Iteration 19/25 | Loss: 0.00077513
Iteration 20/25 | Loss: 0.00077252
Iteration 21/25 | Loss: 0.00076616
Iteration 22/25 | Loss: 0.00076690
Iteration 23/25 | Loss: 0.00076197
Iteration 24/25 | Loss: 0.00075555
Iteration 25/25 | Loss: 0.00075413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45545757
Iteration 2/25 | Loss: 0.00038050
Iteration 3/25 | Loss: 0.00038050
Iteration 4/25 | Loss: 0.00038050
Iteration 5/25 | Loss: 0.00038050
Iteration 6/25 | Loss: 0.00038050
Iteration 7/25 | Loss: 0.00038050
Iteration 8/25 | Loss: 0.00038050
Iteration 9/25 | Loss: 0.00038050
Iteration 10/25 | Loss: 0.00038050
Iteration 11/25 | Loss: 0.00038050
Iteration 12/25 | Loss: 0.00038050
Iteration 13/25 | Loss: 0.00038050
Iteration 14/25 | Loss: 0.00038050
Iteration 15/25 | Loss: 0.00038050
Iteration 16/25 | Loss: 0.00038050
Iteration 17/25 | Loss: 0.00038050
Iteration 18/25 | Loss: 0.00038050
Iteration 19/25 | Loss: 0.00038050
Iteration 20/25 | Loss: 0.00038050
Iteration 21/25 | Loss: 0.00038050
Iteration 22/25 | Loss: 0.00038050
Iteration 23/25 | Loss: 0.00038050
Iteration 24/25 | Loss: 0.00038050
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003804971347562969, 0.0003804971347562969, 0.0003804971347562969, 0.0003804971347562969, 0.0003804971347562969]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003804971347562969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038050
Iteration 2/1000 | Loss: 0.00004219
Iteration 3/1000 | Loss: 0.00003399
Iteration 4/1000 | Loss: 0.00003152
Iteration 5/1000 | Loss: 0.00003039
Iteration 6/1000 | Loss: 0.00007840
Iteration 7/1000 | Loss: 0.00005999
Iteration 8/1000 | Loss: 0.00003557
Iteration 9/1000 | Loss: 0.00003104
Iteration 10/1000 | Loss: 0.00002904
Iteration 11/1000 | Loss: 0.00002775
Iteration 12/1000 | Loss: 0.00002669
Iteration 13/1000 | Loss: 0.00002608
Iteration 14/1000 | Loss: 0.00002566
Iteration 15/1000 | Loss: 0.00002528
Iteration 16/1000 | Loss: 0.00002490
Iteration 17/1000 | Loss: 0.00002455
Iteration 18/1000 | Loss: 0.00002435
Iteration 19/1000 | Loss: 0.00004566
Iteration 20/1000 | Loss: 0.00002881
Iteration 21/1000 | Loss: 0.00004076
Iteration 22/1000 | Loss: 0.00002822
Iteration 23/1000 | Loss: 0.00002633
Iteration 24/1000 | Loss: 0.00004283
Iteration 25/1000 | Loss: 0.00004345
Iteration 26/1000 | Loss: 0.00004242
Iteration 27/1000 | Loss: 0.00004786
Iteration 28/1000 | Loss: 0.00003675
Iteration 29/1000 | Loss: 0.00003974
Iteration 30/1000 | Loss: 0.00004326
Iteration 31/1000 | Loss: 0.00004710
Iteration 32/1000 | Loss: 0.00003978
Iteration 33/1000 | Loss: 0.00004236
Iteration 34/1000 | Loss: 0.00004748
Iteration 35/1000 | Loss: 0.00004117
Iteration 36/1000 | Loss: 0.00004635
Iteration 37/1000 | Loss: 0.00004772
Iteration 38/1000 | Loss: 0.00007511
Iteration 39/1000 | Loss: 0.00005637
Iteration 40/1000 | Loss: 0.00004096
Iteration 41/1000 | Loss: 0.00004663
Iteration 42/1000 | Loss: 0.00004312
Iteration 43/1000 | Loss: 0.00004689
Iteration 44/1000 | Loss: 0.00006186
Iteration 45/1000 | Loss: 0.00005233
Iteration 46/1000 | Loss: 0.00002699
Iteration 47/1000 | Loss: 0.00004614
Iteration 48/1000 | Loss: 0.00002417
Iteration 49/1000 | Loss: 0.00002399
Iteration 50/1000 | Loss: 0.00002398
Iteration 51/1000 | Loss: 0.00002395
Iteration 52/1000 | Loss: 0.00002393
Iteration 53/1000 | Loss: 0.00002393
Iteration 54/1000 | Loss: 0.00002393
Iteration 55/1000 | Loss: 0.00002393
Iteration 56/1000 | Loss: 0.00002392
Iteration 57/1000 | Loss: 0.00002392
Iteration 58/1000 | Loss: 0.00002391
Iteration 59/1000 | Loss: 0.00002391
Iteration 60/1000 | Loss: 0.00002390
Iteration 61/1000 | Loss: 0.00002389
Iteration 62/1000 | Loss: 0.00002384
Iteration 63/1000 | Loss: 0.00002380
Iteration 64/1000 | Loss: 0.00002376
Iteration 65/1000 | Loss: 0.00002376
Iteration 66/1000 | Loss: 0.00002364
Iteration 67/1000 | Loss: 0.00002345
Iteration 68/1000 | Loss: 0.00002344
Iteration 69/1000 | Loss: 0.00002344
Iteration 70/1000 | Loss: 0.00002343
Iteration 71/1000 | Loss: 0.00005666
Iteration 72/1000 | Loss: 0.00004598
Iteration 73/1000 | Loss: 0.00003781
Iteration 74/1000 | Loss: 0.00002617
Iteration 75/1000 | Loss: 0.00002530
Iteration 76/1000 | Loss: 0.00002472
Iteration 77/1000 | Loss: 0.00002428
Iteration 78/1000 | Loss: 0.00002389
Iteration 79/1000 | Loss: 0.00002355
Iteration 80/1000 | Loss: 0.00002322
Iteration 81/1000 | Loss: 0.00002304
Iteration 82/1000 | Loss: 0.00002293
Iteration 83/1000 | Loss: 0.00002292
Iteration 84/1000 | Loss: 0.00002292
Iteration 85/1000 | Loss: 0.00002292
Iteration 86/1000 | Loss: 0.00002292
Iteration 87/1000 | Loss: 0.00002292
Iteration 88/1000 | Loss: 0.00002292
Iteration 89/1000 | Loss: 0.00002292
Iteration 90/1000 | Loss: 0.00002292
Iteration 91/1000 | Loss: 0.00002292
Iteration 92/1000 | Loss: 0.00002291
Iteration 93/1000 | Loss: 0.00002291
Iteration 94/1000 | Loss: 0.00002289
Iteration 95/1000 | Loss: 0.00002289
Iteration 96/1000 | Loss: 0.00002288
Iteration 97/1000 | Loss: 0.00002287
Iteration 98/1000 | Loss: 0.00002286
Iteration 99/1000 | Loss: 0.00002285
Iteration 100/1000 | Loss: 0.00002273
Iteration 101/1000 | Loss: 0.00005348
Iteration 102/1000 | Loss: 0.00011712
Iteration 103/1000 | Loss: 0.00002782
Iteration 104/1000 | Loss: 0.00005468
Iteration 105/1000 | Loss: 0.00007323
Iteration 106/1000 | Loss: 0.00005266
Iteration 107/1000 | Loss: 0.00003735
Iteration 108/1000 | Loss: 0.00004509
Iteration 109/1000 | Loss: 0.00003302
Iteration 110/1000 | Loss: 0.00005335
Iteration 111/1000 | Loss: 0.00004756
Iteration 112/1000 | Loss: 0.00002978
Iteration 113/1000 | Loss: 0.00002805
Iteration 114/1000 | Loss: 0.00005953
Iteration 115/1000 | Loss: 0.00002549
Iteration 116/1000 | Loss: 0.00002446
Iteration 117/1000 | Loss: 0.00002403
Iteration 118/1000 | Loss: 0.00002367
Iteration 119/1000 | Loss: 0.00011189
Iteration 120/1000 | Loss: 0.00068061
Iteration 121/1000 | Loss: 0.00062293
Iteration 122/1000 | Loss: 0.00002722
Iteration 123/1000 | Loss: 0.00002388
Iteration 124/1000 | Loss: 0.00002288
Iteration 125/1000 | Loss: 0.00002251
Iteration 126/1000 | Loss: 0.00002243
Iteration 127/1000 | Loss: 0.00002241
Iteration 128/1000 | Loss: 0.00002240
Iteration 129/1000 | Loss: 0.00002240
Iteration 130/1000 | Loss: 0.00002239
Iteration 131/1000 | Loss: 0.00002239
Iteration 132/1000 | Loss: 0.00002239
Iteration 133/1000 | Loss: 0.00002239
Iteration 134/1000 | Loss: 0.00002238
Iteration 135/1000 | Loss: 0.00002238
Iteration 136/1000 | Loss: 0.00002238
Iteration 137/1000 | Loss: 0.00002238
Iteration 138/1000 | Loss: 0.00002237
Iteration 139/1000 | Loss: 0.00002237
Iteration 140/1000 | Loss: 0.00002237
Iteration 141/1000 | Loss: 0.00002237
Iteration 142/1000 | Loss: 0.00002237
Iteration 143/1000 | Loss: 0.00002237
Iteration 144/1000 | Loss: 0.00002237
Iteration 145/1000 | Loss: 0.00002237
Iteration 146/1000 | Loss: 0.00002237
Iteration 147/1000 | Loss: 0.00002237
Iteration 148/1000 | Loss: 0.00002237
Iteration 149/1000 | Loss: 0.00002237
Iteration 150/1000 | Loss: 0.00002236
Iteration 151/1000 | Loss: 0.00002236
Iteration 152/1000 | Loss: 0.00002236
Iteration 153/1000 | Loss: 0.00002236
Iteration 154/1000 | Loss: 0.00002236
Iteration 155/1000 | Loss: 0.00002236
Iteration 156/1000 | Loss: 0.00002236
Iteration 157/1000 | Loss: 0.00002236
Iteration 158/1000 | Loss: 0.00002236
Iteration 159/1000 | Loss: 0.00002236
Iteration 160/1000 | Loss: 0.00002236
Iteration 161/1000 | Loss: 0.00002236
Iteration 162/1000 | Loss: 0.00002236
Iteration 163/1000 | Loss: 0.00002236
Iteration 164/1000 | Loss: 0.00002236
Iteration 165/1000 | Loss: 0.00002236
Iteration 166/1000 | Loss: 0.00002236
Iteration 167/1000 | Loss: 0.00002236
Iteration 168/1000 | Loss: 0.00002236
Iteration 169/1000 | Loss: 0.00002236
Iteration 170/1000 | Loss: 0.00002236
Iteration 171/1000 | Loss: 0.00002236
Iteration 172/1000 | Loss: 0.00002236
Iteration 173/1000 | Loss: 0.00002236
Iteration 174/1000 | Loss: 0.00002236
Iteration 175/1000 | Loss: 0.00002236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.2356754925567657e-05, 2.2356754925567657e-05, 2.2356754925567657e-05, 2.2356754925567657e-05, 2.2356754925567657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2356754925567657e-05

Optimization complete. Final v2v error: 3.8961246013641357 mm

Highest mean error: 5.457584381103516 mm for frame 91

Lowest mean error: 3.436755657196045 mm for frame 185

Saving results

Total time: 206.00325417518616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851189
Iteration 2/25 | Loss: 0.00091030
Iteration 3/25 | Loss: 0.00070526
Iteration 4/25 | Loss: 0.00067302
Iteration 5/25 | Loss: 0.00066322
Iteration 6/25 | Loss: 0.00066093
Iteration 7/25 | Loss: 0.00066026
Iteration 8/25 | Loss: 0.00066026
Iteration 9/25 | Loss: 0.00066026
Iteration 10/25 | Loss: 0.00066026
Iteration 11/25 | Loss: 0.00066026
Iteration 12/25 | Loss: 0.00066026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006602624780498445, 0.0006602624780498445, 0.0006602624780498445, 0.0006602624780498445, 0.0006602624780498445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006602624780498445

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59066069
Iteration 2/25 | Loss: 0.00034674
Iteration 3/25 | Loss: 0.00034674
Iteration 4/25 | Loss: 0.00034673
Iteration 5/25 | Loss: 0.00034673
Iteration 6/25 | Loss: 0.00034673
Iteration 7/25 | Loss: 0.00034673
Iteration 8/25 | Loss: 0.00034673
Iteration 9/25 | Loss: 0.00034673
Iteration 10/25 | Loss: 0.00034673
Iteration 11/25 | Loss: 0.00034673
Iteration 12/25 | Loss: 0.00034673
Iteration 13/25 | Loss: 0.00034673
Iteration 14/25 | Loss: 0.00034673
Iteration 15/25 | Loss: 0.00034673
Iteration 16/25 | Loss: 0.00034673
Iteration 17/25 | Loss: 0.00034673
Iteration 18/25 | Loss: 0.00034673
Iteration 19/25 | Loss: 0.00034673
Iteration 20/25 | Loss: 0.00034673
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00034673267509788275, 0.00034673267509788275, 0.00034673267509788275, 0.00034673267509788275, 0.00034673267509788275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034673267509788275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034673
Iteration 2/1000 | Loss: 0.00003045
Iteration 3/1000 | Loss: 0.00002086
Iteration 4/1000 | Loss: 0.00001923
Iteration 5/1000 | Loss: 0.00001853
Iteration 6/1000 | Loss: 0.00001782
Iteration 7/1000 | Loss: 0.00001745
Iteration 8/1000 | Loss: 0.00001704
Iteration 9/1000 | Loss: 0.00001682
Iteration 10/1000 | Loss: 0.00001663
Iteration 11/1000 | Loss: 0.00001649
Iteration 12/1000 | Loss: 0.00001641
Iteration 13/1000 | Loss: 0.00001641
Iteration 14/1000 | Loss: 0.00001632
Iteration 15/1000 | Loss: 0.00001630
Iteration 16/1000 | Loss: 0.00001628
Iteration 17/1000 | Loss: 0.00001627
Iteration 18/1000 | Loss: 0.00001626
Iteration 19/1000 | Loss: 0.00001626
Iteration 20/1000 | Loss: 0.00001625
Iteration 21/1000 | Loss: 0.00001622
Iteration 22/1000 | Loss: 0.00001621
Iteration 23/1000 | Loss: 0.00001620
Iteration 24/1000 | Loss: 0.00001620
Iteration 25/1000 | Loss: 0.00001620
Iteration 26/1000 | Loss: 0.00001619
Iteration 27/1000 | Loss: 0.00001619
Iteration 28/1000 | Loss: 0.00001619
Iteration 29/1000 | Loss: 0.00001618
Iteration 30/1000 | Loss: 0.00001618
Iteration 31/1000 | Loss: 0.00001618
Iteration 32/1000 | Loss: 0.00001618
Iteration 33/1000 | Loss: 0.00001618
Iteration 34/1000 | Loss: 0.00001618
Iteration 35/1000 | Loss: 0.00001618
Iteration 36/1000 | Loss: 0.00001618
Iteration 37/1000 | Loss: 0.00001617
Iteration 38/1000 | Loss: 0.00001617
Iteration 39/1000 | Loss: 0.00001617
Iteration 40/1000 | Loss: 0.00001617
Iteration 41/1000 | Loss: 0.00001617
Iteration 42/1000 | Loss: 0.00001617
Iteration 43/1000 | Loss: 0.00001616
Iteration 44/1000 | Loss: 0.00001616
Iteration 45/1000 | Loss: 0.00001616
Iteration 46/1000 | Loss: 0.00001615
Iteration 47/1000 | Loss: 0.00001615
Iteration 48/1000 | Loss: 0.00001615
Iteration 49/1000 | Loss: 0.00001615
Iteration 50/1000 | Loss: 0.00001615
Iteration 51/1000 | Loss: 0.00001615
Iteration 52/1000 | Loss: 0.00001615
Iteration 53/1000 | Loss: 0.00001615
Iteration 54/1000 | Loss: 0.00001615
Iteration 55/1000 | Loss: 0.00001615
Iteration 56/1000 | Loss: 0.00001615
Iteration 57/1000 | Loss: 0.00001615
Iteration 58/1000 | Loss: 0.00001615
Iteration 59/1000 | Loss: 0.00001615
Iteration 60/1000 | Loss: 0.00001615
Iteration 61/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [1.614607208466623e-05, 1.614607208466623e-05, 1.614607208466623e-05, 1.614607208466623e-05, 1.614607208466623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.614607208466623e-05

Optimization complete. Final v2v error: 3.368838310241699 mm

Highest mean error: 4.040314674377441 mm for frame 157

Lowest mean error: 2.8145318031311035 mm for frame 146

Saving results

Total time: 32.9778995513916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00937038
Iteration 2/25 | Loss: 0.00132657
Iteration 3/25 | Loss: 0.00095823
Iteration 4/25 | Loss: 0.00084493
Iteration 5/25 | Loss: 0.00081601
Iteration 6/25 | Loss: 0.00076836
Iteration 7/25 | Loss: 0.00076714
Iteration 8/25 | Loss: 0.00076201
Iteration 9/25 | Loss: 0.00076117
Iteration 10/25 | Loss: 0.00075981
Iteration 11/25 | Loss: 0.00075929
Iteration 12/25 | Loss: 0.00075911
Iteration 13/25 | Loss: 0.00075901
Iteration 14/25 | Loss: 0.00075901
Iteration 15/25 | Loss: 0.00075901
Iteration 16/25 | Loss: 0.00075901
Iteration 17/25 | Loss: 0.00075901
Iteration 18/25 | Loss: 0.00075901
Iteration 19/25 | Loss: 0.00075901
Iteration 20/25 | Loss: 0.00075900
Iteration 21/25 | Loss: 0.00075900
Iteration 22/25 | Loss: 0.00075900
Iteration 23/25 | Loss: 0.00075900
Iteration 24/25 | Loss: 0.00075900
Iteration 25/25 | Loss: 0.00075900

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.24014759
Iteration 2/25 | Loss: 0.00035581
Iteration 3/25 | Loss: 0.00035577
Iteration 4/25 | Loss: 0.00035577
Iteration 5/25 | Loss: 0.00035576
Iteration 6/25 | Loss: 0.00035576
Iteration 7/25 | Loss: 0.00035576
Iteration 8/25 | Loss: 0.00035576
Iteration 9/25 | Loss: 0.00035576
Iteration 10/25 | Loss: 0.00035576
Iteration 11/25 | Loss: 0.00035576
Iteration 12/25 | Loss: 0.00035576
Iteration 13/25 | Loss: 0.00035576
Iteration 14/25 | Loss: 0.00035576
Iteration 15/25 | Loss: 0.00035576
Iteration 16/25 | Loss: 0.00035576
Iteration 17/25 | Loss: 0.00035576
Iteration 18/25 | Loss: 0.00035576
Iteration 19/25 | Loss: 0.00035576
Iteration 20/25 | Loss: 0.00035576
Iteration 21/25 | Loss: 0.00035576
Iteration 22/25 | Loss: 0.00035576
Iteration 23/25 | Loss: 0.00035576
Iteration 24/25 | Loss: 0.00035576
Iteration 25/25 | Loss: 0.00035576
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00035576289519667625, 0.00035576289519667625, 0.00035576289519667625, 0.00035576289519667625, 0.00035576289519667625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00035576289519667625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035576
Iteration 2/1000 | Loss: 0.00003852
Iteration 3/1000 | Loss: 0.00002642
Iteration 4/1000 | Loss: 0.00002380
Iteration 5/1000 | Loss: 0.00002246
Iteration 6/1000 | Loss: 0.00002168
Iteration 7/1000 | Loss: 0.00002111
Iteration 8/1000 | Loss: 0.00002066
Iteration 9/1000 | Loss: 0.00002037
Iteration 10/1000 | Loss: 0.00002009
Iteration 11/1000 | Loss: 0.00001988
Iteration 12/1000 | Loss: 0.00001975
Iteration 13/1000 | Loss: 0.00001961
Iteration 14/1000 | Loss: 0.00001951
Iteration 15/1000 | Loss: 0.00001949
Iteration 16/1000 | Loss: 0.00001942
Iteration 17/1000 | Loss: 0.00001939
Iteration 18/1000 | Loss: 0.00001938
Iteration 19/1000 | Loss: 0.00001934
Iteration 20/1000 | Loss: 0.00001925
Iteration 21/1000 | Loss: 0.00001922
Iteration 22/1000 | Loss: 0.00001922
Iteration 23/1000 | Loss: 0.00001920
Iteration 24/1000 | Loss: 0.00001919
Iteration 25/1000 | Loss: 0.00001919
Iteration 26/1000 | Loss: 0.00001918
Iteration 27/1000 | Loss: 0.00001918
Iteration 28/1000 | Loss: 0.00001917
Iteration 29/1000 | Loss: 0.00001917
Iteration 30/1000 | Loss: 0.00001916
Iteration 31/1000 | Loss: 0.00001916
Iteration 32/1000 | Loss: 0.00001916
Iteration 33/1000 | Loss: 0.00001915
Iteration 34/1000 | Loss: 0.00001914
Iteration 35/1000 | Loss: 0.00001914
Iteration 36/1000 | Loss: 0.00001914
Iteration 37/1000 | Loss: 0.00001913
Iteration 38/1000 | Loss: 0.00001912
Iteration 39/1000 | Loss: 0.00001911
Iteration 40/1000 | Loss: 0.00001911
Iteration 41/1000 | Loss: 0.00001909
Iteration 42/1000 | Loss: 0.00001909
Iteration 43/1000 | Loss: 0.00001909
Iteration 44/1000 | Loss: 0.00001908
Iteration 45/1000 | Loss: 0.00001908
Iteration 46/1000 | Loss: 0.00001908
Iteration 47/1000 | Loss: 0.00001907
Iteration 48/1000 | Loss: 0.00001907
Iteration 49/1000 | Loss: 0.00001907
Iteration 50/1000 | Loss: 0.00001906
Iteration 51/1000 | Loss: 0.00001906
Iteration 52/1000 | Loss: 0.00001906
Iteration 53/1000 | Loss: 0.00001906
Iteration 54/1000 | Loss: 0.00001906
Iteration 55/1000 | Loss: 0.00001906
Iteration 56/1000 | Loss: 0.00001906
Iteration 57/1000 | Loss: 0.00001906
Iteration 58/1000 | Loss: 0.00001906
Iteration 59/1000 | Loss: 0.00001906
Iteration 60/1000 | Loss: 0.00001906
Iteration 61/1000 | Loss: 0.00001905
Iteration 62/1000 | Loss: 0.00001905
Iteration 63/1000 | Loss: 0.00001905
Iteration 64/1000 | Loss: 0.00001905
Iteration 65/1000 | Loss: 0.00001905
Iteration 66/1000 | Loss: 0.00001905
Iteration 67/1000 | Loss: 0.00001905
Iteration 68/1000 | Loss: 0.00001905
Iteration 69/1000 | Loss: 0.00001905
Iteration 70/1000 | Loss: 0.00001905
Iteration 71/1000 | Loss: 0.00001905
Iteration 72/1000 | Loss: 0.00001904
Iteration 73/1000 | Loss: 0.00001904
Iteration 74/1000 | Loss: 0.00001904
Iteration 75/1000 | Loss: 0.00001904
Iteration 76/1000 | Loss: 0.00001904
Iteration 77/1000 | Loss: 0.00001903
Iteration 78/1000 | Loss: 0.00001903
Iteration 79/1000 | Loss: 0.00001903
Iteration 80/1000 | Loss: 0.00001903
Iteration 81/1000 | Loss: 0.00001902
Iteration 82/1000 | Loss: 0.00001902
Iteration 83/1000 | Loss: 0.00001902
Iteration 84/1000 | Loss: 0.00001902
Iteration 85/1000 | Loss: 0.00001902
Iteration 86/1000 | Loss: 0.00001902
Iteration 87/1000 | Loss: 0.00001901
Iteration 88/1000 | Loss: 0.00001901
Iteration 89/1000 | Loss: 0.00001901
Iteration 90/1000 | Loss: 0.00001901
Iteration 91/1000 | Loss: 0.00001900
Iteration 92/1000 | Loss: 0.00001900
Iteration 93/1000 | Loss: 0.00001900
Iteration 94/1000 | Loss: 0.00001900
Iteration 95/1000 | Loss: 0.00001900
Iteration 96/1000 | Loss: 0.00001900
Iteration 97/1000 | Loss: 0.00001900
Iteration 98/1000 | Loss: 0.00001899
Iteration 99/1000 | Loss: 0.00001899
Iteration 100/1000 | Loss: 0.00001899
Iteration 101/1000 | Loss: 0.00001899
Iteration 102/1000 | Loss: 0.00001899
Iteration 103/1000 | Loss: 0.00001898
Iteration 104/1000 | Loss: 0.00001898
Iteration 105/1000 | Loss: 0.00001898
Iteration 106/1000 | Loss: 0.00001898
Iteration 107/1000 | Loss: 0.00001897
Iteration 108/1000 | Loss: 0.00001897
Iteration 109/1000 | Loss: 0.00001897
Iteration 110/1000 | Loss: 0.00001897
Iteration 111/1000 | Loss: 0.00001896
Iteration 112/1000 | Loss: 0.00001896
Iteration 113/1000 | Loss: 0.00001896
Iteration 114/1000 | Loss: 0.00001895
Iteration 115/1000 | Loss: 0.00001895
Iteration 116/1000 | Loss: 0.00001895
Iteration 117/1000 | Loss: 0.00001895
Iteration 118/1000 | Loss: 0.00001895
Iteration 119/1000 | Loss: 0.00001894
Iteration 120/1000 | Loss: 0.00001894
Iteration 121/1000 | Loss: 0.00001894
Iteration 122/1000 | Loss: 0.00001894
Iteration 123/1000 | Loss: 0.00001894
Iteration 124/1000 | Loss: 0.00001893
Iteration 125/1000 | Loss: 0.00001893
Iteration 126/1000 | Loss: 0.00001893
Iteration 127/1000 | Loss: 0.00001893
Iteration 128/1000 | Loss: 0.00001893
Iteration 129/1000 | Loss: 0.00001893
Iteration 130/1000 | Loss: 0.00001893
Iteration 131/1000 | Loss: 0.00001893
Iteration 132/1000 | Loss: 0.00001893
Iteration 133/1000 | Loss: 0.00001892
Iteration 134/1000 | Loss: 0.00001892
Iteration 135/1000 | Loss: 0.00001892
Iteration 136/1000 | Loss: 0.00001931
Iteration 137/1000 | Loss: 0.00001930
Iteration 138/1000 | Loss: 0.00001930
Iteration 139/1000 | Loss: 0.00001930
Iteration 140/1000 | Loss: 0.00001929
Iteration 141/1000 | Loss: 0.00001893
Iteration 142/1000 | Loss: 0.00001888
Iteration 143/1000 | Loss: 0.00001888
Iteration 144/1000 | Loss: 0.00001888
Iteration 145/1000 | Loss: 0.00001888
Iteration 146/1000 | Loss: 0.00001888
Iteration 147/1000 | Loss: 0.00001888
Iteration 148/1000 | Loss: 0.00001888
Iteration 149/1000 | Loss: 0.00001888
Iteration 150/1000 | Loss: 0.00001888
Iteration 151/1000 | Loss: 0.00001888
Iteration 152/1000 | Loss: 0.00001888
Iteration 153/1000 | Loss: 0.00001888
Iteration 154/1000 | Loss: 0.00001888
Iteration 155/1000 | Loss: 0.00001888
Iteration 156/1000 | Loss: 0.00001888
Iteration 157/1000 | Loss: 0.00001888
Iteration 158/1000 | Loss: 0.00001888
Iteration 159/1000 | Loss: 0.00001888
Iteration 160/1000 | Loss: 0.00001888
Iteration 161/1000 | Loss: 0.00001888
Iteration 162/1000 | Loss: 0.00001888
Iteration 163/1000 | Loss: 0.00001888
Iteration 164/1000 | Loss: 0.00001888
Iteration 165/1000 | Loss: 0.00001888
Iteration 166/1000 | Loss: 0.00001888
Iteration 167/1000 | Loss: 0.00001888
Iteration 168/1000 | Loss: 0.00001888
Iteration 169/1000 | Loss: 0.00001888
Iteration 170/1000 | Loss: 0.00001888
Iteration 171/1000 | Loss: 0.00001888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.8877173715736717e-05, 1.8877173715736717e-05, 1.8877173715736717e-05, 1.8877173715736717e-05, 1.8877173715736717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8877173715736717e-05

Optimization complete. Final v2v error: 3.5393874645233154 mm

Highest mean error: 10.943950653076172 mm for frame 68

Lowest mean error: 2.9856648445129395 mm for frame 117

Saving results

Total time: 58.75382161140442
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994364
Iteration 2/25 | Loss: 0.00359601
Iteration 3/25 | Loss: 0.00282824
Iteration 4/25 | Loss: 0.00202810
Iteration 5/25 | Loss: 0.00131175
Iteration 6/25 | Loss: 0.00123526
Iteration 7/25 | Loss: 0.00136881
Iteration 8/25 | Loss: 0.00152536
Iteration 9/25 | Loss: 0.00166009
Iteration 10/25 | Loss: 0.00156723
Iteration 11/25 | Loss: 0.00160729
Iteration 12/25 | Loss: 0.00170526
Iteration 13/25 | Loss: 0.00172475
Iteration 14/25 | Loss: 0.00161273
Iteration 15/25 | Loss: 0.00155486
Iteration 16/25 | Loss: 0.00144382
Iteration 17/25 | Loss: 0.00136431
Iteration 18/25 | Loss: 0.00132981
Iteration 19/25 | Loss: 0.00130079
Iteration 20/25 | Loss: 0.00127123
Iteration 21/25 | Loss: 0.00123529
Iteration 22/25 | Loss: 0.00125167
Iteration 23/25 | Loss: 0.00121084
Iteration 24/25 | Loss: 0.00116578
Iteration 25/25 | Loss: 0.00118173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42641282
Iteration 2/25 | Loss: 0.00537949
Iteration 3/25 | Loss: 0.00369728
Iteration 4/25 | Loss: 0.00369690
Iteration 5/25 | Loss: 0.00369690
Iteration 6/25 | Loss: 0.00369690
Iteration 7/25 | Loss: 0.00369690
Iteration 8/25 | Loss: 0.00369690
Iteration 9/25 | Loss: 0.00369690
Iteration 10/25 | Loss: 0.00369690
Iteration 11/25 | Loss: 0.00369690
Iteration 12/25 | Loss: 0.00369690
Iteration 13/25 | Loss: 0.00369690
Iteration 14/25 | Loss: 0.00369690
Iteration 15/25 | Loss: 0.00369690
Iteration 16/25 | Loss: 0.00369690
Iteration 17/25 | Loss: 0.00369690
Iteration 18/25 | Loss: 0.00369690
Iteration 19/25 | Loss: 0.00369690
Iteration 20/25 | Loss: 0.00369690
Iteration 21/25 | Loss: 0.00369690
Iteration 22/25 | Loss: 0.00369690
Iteration 23/25 | Loss: 0.00369690
Iteration 24/25 | Loss: 0.00369690
Iteration 25/25 | Loss: 0.00369690

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00369690
Iteration 2/1000 | Loss: 0.00359211
Iteration 3/1000 | Loss: 0.00266807
Iteration 4/1000 | Loss: 0.00283948
Iteration 5/1000 | Loss: 0.00184910
Iteration 6/1000 | Loss: 0.00246543
Iteration 7/1000 | Loss: 0.00084502
Iteration 8/1000 | Loss: 0.00115476
Iteration 9/1000 | Loss: 0.00160873
Iteration 10/1000 | Loss: 0.00075016
Iteration 11/1000 | Loss: 0.00078464
Iteration 12/1000 | Loss: 0.00069672
Iteration 13/1000 | Loss: 0.00093873
Iteration 14/1000 | Loss: 0.00058787
Iteration 15/1000 | Loss: 0.00085025
Iteration 16/1000 | Loss: 0.00069022
Iteration 17/1000 | Loss: 0.00075935
Iteration 18/1000 | Loss: 0.00162101
Iteration 19/1000 | Loss: 0.00141274
Iteration 20/1000 | Loss: 0.00070835
Iteration 21/1000 | Loss: 0.00105737
Iteration 22/1000 | Loss: 0.00146461
Iteration 23/1000 | Loss: 0.00102858
Iteration 24/1000 | Loss: 0.00085390
Iteration 25/1000 | Loss: 0.00136585
Iteration 26/1000 | Loss: 0.00096527
Iteration 27/1000 | Loss: 0.00089305
Iteration 28/1000 | Loss: 0.00098991
Iteration 29/1000 | Loss: 0.00044302
Iteration 30/1000 | Loss: 0.00099478
Iteration 31/1000 | Loss: 0.00260834
Iteration 32/1000 | Loss: 0.00195994
Iteration 33/1000 | Loss: 0.00056523
Iteration 34/1000 | Loss: 0.00080031
Iteration 35/1000 | Loss: 0.00127109
Iteration 36/1000 | Loss: 0.00117686
Iteration 37/1000 | Loss: 0.00054043
Iteration 38/1000 | Loss: 0.00064672
Iteration 39/1000 | Loss: 0.00084355
Iteration 40/1000 | Loss: 0.00075724
Iteration 41/1000 | Loss: 0.00057496
Iteration 42/1000 | Loss: 0.00085281
Iteration 43/1000 | Loss: 0.00086363
Iteration 44/1000 | Loss: 0.00054299
Iteration 45/1000 | Loss: 0.00070155
Iteration 46/1000 | Loss: 0.00034017
Iteration 47/1000 | Loss: 0.00026566
Iteration 48/1000 | Loss: 0.00016945
Iteration 49/1000 | Loss: 0.00102916
Iteration 50/1000 | Loss: 0.00059599
Iteration 51/1000 | Loss: 0.00067812
Iteration 52/1000 | Loss: 0.00044232
Iteration 53/1000 | Loss: 0.00084288
Iteration 54/1000 | Loss: 0.00074297
Iteration 55/1000 | Loss: 0.00018484
Iteration 56/1000 | Loss: 0.00020876
Iteration 57/1000 | Loss: 0.00023790
Iteration 58/1000 | Loss: 0.00025311
Iteration 59/1000 | Loss: 0.00065236
Iteration 60/1000 | Loss: 0.00043218
Iteration 61/1000 | Loss: 0.00048359
Iteration 62/1000 | Loss: 0.00063377
Iteration 63/1000 | Loss: 0.00097511
Iteration 64/1000 | Loss: 0.00145314
Iteration 65/1000 | Loss: 0.00123572
Iteration 66/1000 | Loss: 0.00136707
Iteration 67/1000 | Loss: 0.00143823
Iteration 68/1000 | Loss: 0.00079002
Iteration 69/1000 | Loss: 0.00139538
Iteration 70/1000 | Loss: 0.00081153
Iteration 71/1000 | Loss: 0.00056710
Iteration 72/1000 | Loss: 0.00041075
Iteration 73/1000 | Loss: 0.00045628
Iteration 74/1000 | Loss: 0.00063324
Iteration 75/1000 | Loss: 0.00060235
Iteration 76/1000 | Loss: 0.00032262
Iteration 77/1000 | Loss: 0.00072027
Iteration 78/1000 | Loss: 0.00057180
Iteration 79/1000 | Loss: 0.00040723
Iteration 80/1000 | Loss: 0.00049309
Iteration 81/1000 | Loss: 0.00050137
Iteration 82/1000 | Loss: 0.00053828
Iteration 83/1000 | Loss: 0.00141795
Iteration 84/1000 | Loss: 0.00091717
Iteration 85/1000 | Loss: 0.00049605
Iteration 86/1000 | Loss: 0.00131404
Iteration 87/1000 | Loss: 0.00191074
Iteration 88/1000 | Loss: 0.00208244
Iteration 89/1000 | Loss: 0.00076507
Iteration 90/1000 | Loss: 0.00040196
Iteration 91/1000 | Loss: 0.00041115
Iteration 92/1000 | Loss: 0.00085182
Iteration 93/1000 | Loss: 0.00049057
Iteration 94/1000 | Loss: 0.00053464
Iteration 95/1000 | Loss: 0.00053564
Iteration 96/1000 | Loss: 0.00018129
Iteration 97/1000 | Loss: 0.00018071
Iteration 98/1000 | Loss: 0.00052300
Iteration 99/1000 | Loss: 0.00036298
Iteration 100/1000 | Loss: 0.00044293
Iteration 101/1000 | Loss: 0.00019939
Iteration 102/1000 | Loss: 0.00064968
Iteration 103/1000 | Loss: 0.00062105
Iteration 104/1000 | Loss: 0.00085859
Iteration 105/1000 | Loss: 0.00043257
Iteration 106/1000 | Loss: 0.00039247
Iteration 107/1000 | Loss: 0.00054144
Iteration 108/1000 | Loss: 0.00030240
Iteration 109/1000 | Loss: 0.00050414
Iteration 110/1000 | Loss: 0.00048517
Iteration 111/1000 | Loss: 0.00049638
Iteration 112/1000 | Loss: 0.00017193
Iteration 113/1000 | Loss: 0.00051770
Iteration 114/1000 | Loss: 0.00106142
Iteration 115/1000 | Loss: 0.00062996
Iteration 116/1000 | Loss: 0.00114570
Iteration 117/1000 | Loss: 0.00070117
Iteration 118/1000 | Loss: 0.00034713
Iteration 119/1000 | Loss: 0.00063055
Iteration 120/1000 | Loss: 0.00036773
Iteration 121/1000 | Loss: 0.00031561
Iteration 122/1000 | Loss: 0.00047756
Iteration 123/1000 | Loss: 0.00035006
Iteration 124/1000 | Loss: 0.00040968
Iteration 125/1000 | Loss: 0.00035398
Iteration 126/1000 | Loss: 0.00044534
Iteration 127/1000 | Loss: 0.00024751
Iteration 128/1000 | Loss: 0.00079730
Iteration 129/1000 | Loss: 0.00063062
Iteration 130/1000 | Loss: 0.00036427
Iteration 131/1000 | Loss: 0.00025255
Iteration 132/1000 | Loss: 0.00024518
Iteration 133/1000 | Loss: 0.00089253
Iteration 134/1000 | Loss: 0.00059282
Iteration 135/1000 | Loss: 0.00059955
Iteration 136/1000 | Loss: 0.00079119
Iteration 137/1000 | Loss: 0.00038529
Iteration 138/1000 | Loss: 0.00041265
Iteration 139/1000 | Loss: 0.00026174
Iteration 140/1000 | Loss: 0.00063591
Iteration 141/1000 | Loss: 0.00015217
Iteration 142/1000 | Loss: 0.00010995
Iteration 143/1000 | Loss: 0.00040004
Iteration 144/1000 | Loss: 0.00014179
Iteration 145/1000 | Loss: 0.00011373
Iteration 146/1000 | Loss: 0.00012904
Iteration 147/1000 | Loss: 0.00040189
Iteration 148/1000 | Loss: 0.00079542
Iteration 149/1000 | Loss: 0.00012150
Iteration 150/1000 | Loss: 0.00053872
Iteration 151/1000 | Loss: 0.00071034
Iteration 152/1000 | Loss: 0.00035105
Iteration 153/1000 | Loss: 0.00063961
Iteration 154/1000 | Loss: 0.00023189
Iteration 155/1000 | Loss: 0.00017986
Iteration 156/1000 | Loss: 0.00010243
Iteration 157/1000 | Loss: 0.00034013
Iteration 158/1000 | Loss: 0.00085618
Iteration 159/1000 | Loss: 0.00037385
Iteration 160/1000 | Loss: 0.00012839
Iteration 161/1000 | Loss: 0.00009843
Iteration 162/1000 | Loss: 0.00040142
Iteration 163/1000 | Loss: 0.00043166
Iteration 164/1000 | Loss: 0.00035768
Iteration 165/1000 | Loss: 0.00007617
Iteration 166/1000 | Loss: 0.00034676
Iteration 167/1000 | Loss: 0.00106640
Iteration 168/1000 | Loss: 0.00083961
Iteration 169/1000 | Loss: 0.00049864
Iteration 170/1000 | Loss: 0.00053376
Iteration 171/1000 | Loss: 0.00019435
Iteration 172/1000 | Loss: 0.00020393
Iteration 173/1000 | Loss: 0.00022757
Iteration 174/1000 | Loss: 0.00019487
Iteration 175/1000 | Loss: 0.00039403
Iteration 176/1000 | Loss: 0.00045360
Iteration 177/1000 | Loss: 0.00044701
Iteration 178/1000 | Loss: 0.00033250
Iteration 179/1000 | Loss: 0.00022184
Iteration 180/1000 | Loss: 0.00047365
Iteration 181/1000 | Loss: 0.00044150
Iteration 182/1000 | Loss: 0.00020942
Iteration 183/1000 | Loss: 0.00030787
Iteration 184/1000 | Loss: 0.00040987
Iteration 185/1000 | Loss: 0.00026028
Iteration 186/1000 | Loss: 0.00017001
Iteration 187/1000 | Loss: 0.00014053
Iteration 188/1000 | Loss: 0.00035819
Iteration 189/1000 | Loss: 0.00046867
Iteration 190/1000 | Loss: 0.00010377
Iteration 191/1000 | Loss: 0.00042146
Iteration 192/1000 | Loss: 0.00031151
Iteration 193/1000 | Loss: 0.00043764
Iteration 194/1000 | Loss: 0.00048834
Iteration 195/1000 | Loss: 0.00040310
Iteration 196/1000 | Loss: 0.00030959
Iteration 197/1000 | Loss: 0.00020073
Iteration 198/1000 | Loss: 0.00009808
Iteration 199/1000 | Loss: 0.00028076
Iteration 200/1000 | Loss: 0.00036354
Iteration 201/1000 | Loss: 0.00027676
Iteration 202/1000 | Loss: 0.00016869
Iteration 203/1000 | Loss: 0.00013527
Iteration 204/1000 | Loss: 0.00019399
Iteration 205/1000 | Loss: 0.00020933
Iteration 206/1000 | Loss: 0.00016146
Iteration 207/1000 | Loss: 0.00021136
Iteration 208/1000 | Loss: 0.00018774
Iteration 209/1000 | Loss: 0.00014636
Iteration 210/1000 | Loss: 0.00012946
Iteration 211/1000 | Loss: 0.00014594
Iteration 212/1000 | Loss: 0.00012325
Iteration 213/1000 | Loss: 0.00012848
Iteration 214/1000 | Loss: 0.00048490
Iteration 215/1000 | Loss: 0.00013006
Iteration 216/1000 | Loss: 0.00023979
Iteration 217/1000 | Loss: 0.00005920
Iteration 218/1000 | Loss: 0.00007268
Iteration 219/1000 | Loss: 0.00025573
Iteration 220/1000 | Loss: 0.00056447
Iteration 221/1000 | Loss: 0.00084183
Iteration 222/1000 | Loss: 0.00035986
Iteration 223/1000 | Loss: 0.00039165
Iteration 224/1000 | Loss: 0.00029930
Iteration 225/1000 | Loss: 0.00008794
Iteration 226/1000 | Loss: 0.00009496
Iteration 227/1000 | Loss: 0.00008896
Iteration 228/1000 | Loss: 0.00009590
Iteration 229/1000 | Loss: 0.00008412
Iteration 230/1000 | Loss: 0.00008732
Iteration 231/1000 | Loss: 0.00008863
Iteration 232/1000 | Loss: 0.00006098
Iteration 233/1000 | Loss: 0.00007710
Iteration 234/1000 | Loss: 0.00009154
Iteration 235/1000 | Loss: 0.00007553
Iteration 236/1000 | Loss: 0.00006421
Iteration 237/1000 | Loss: 0.00008262
Iteration 238/1000 | Loss: 0.00008865
Iteration 239/1000 | Loss: 0.00008344
Iteration 240/1000 | Loss: 0.00007427
Iteration 241/1000 | Loss: 0.00007244
Iteration 242/1000 | Loss: 0.00007178
Iteration 243/1000 | Loss: 0.00007771
Iteration 244/1000 | Loss: 0.00032032
Iteration 245/1000 | Loss: 0.00128970
Iteration 246/1000 | Loss: 0.00009420
Iteration 247/1000 | Loss: 0.00005242
Iteration 248/1000 | Loss: 0.00004549
Iteration 249/1000 | Loss: 0.00033982
Iteration 250/1000 | Loss: 0.00030622
Iteration 251/1000 | Loss: 0.00020392
Iteration 252/1000 | Loss: 0.00007972
Iteration 253/1000 | Loss: 0.00006121
Iteration 254/1000 | Loss: 0.00005077
Iteration 255/1000 | Loss: 0.00004508
Iteration 256/1000 | Loss: 0.00004304
Iteration 257/1000 | Loss: 0.00004171
Iteration 258/1000 | Loss: 0.00004049
Iteration 259/1000 | Loss: 0.00380502
Iteration 260/1000 | Loss: 0.00513544
Iteration 261/1000 | Loss: 0.00250105
Iteration 262/1000 | Loss: 0.00062092
Iteration 263/1000 | Loss: 0.00031439
Iteration 264/1000 | Loss: 0.00025717
Iteration 265/1000 | Loss: 0.00028862
Iteration 266/1000 | Loss: 0.00075186
Iteration 267/1000 | Loss: 0.00018275
Iteration 268/1000 | Loss: 0.00011613
Iteration 269/1000 | Loss: 0.00011298
Iteration 270/1000 | Loss: 0.00207702
Iteration 271/1000 | Loss: 0.00162926
Iteration 272/1000 | Loss: 0.00034698
Iteration 273/1000 | Loss: 0.00186910
Iteration 274/1000 | Loss: 0.00139544
Iteration 275/1000 | Loss: 0.00010409
Iteration 276/1000 | Loss: 0.00009391
Iteration 277/1000 | Loss: 0.00007783
Iteration 278/1000 | Loss: 0.00007878
Iteration 279/1000 | Loss: 0.00009535
Iteration 280/1000 | Loss: 0.00008651
Iteration 281/1000 | Loss: 0.00029186
Iteration 282/1000 | Loss: 0.00056830
Iteration 283/1000 | Loss: 0.00018943
Iteration 284/1000 | Loss: 0.00007570
Iteration 285/1000 | Loss: 0.00006375
Iteration 286/1000 | Loss: 0.00064414
Iteration 287/1000 | Loss: 0.00065063
Iteration 288/1000 | Loss: 0.00010602
Iteration 289/1000 | Loss: 0.00007688
Iteration 290/1000 | Loss: 0.00026877
Iteration 291/1000 | Loss: 0.00034172
Iteration 292/1000 | Loss: 0.00008693
Iteration 293/1000 | Loss: 0.00007163
Iteration 294/1000 | Loss: 0.00009405
Iteration 295/1000 | Loss: 0.00008678
Iteration 296/1000 | Loss: 0.00006357
Iteration 297/1000 | Loss: 0.00005865
Iteration 298/1000 | Loss: 0.00032753
Iteration 299/1000 | Loss: 0.00031984
Iteration 300/1000 | Loss: 0.00032258
Iteration 301/1000 | Loss: 0.00046334
Iteration 302/1000 | Loss: 0.00031795
Iteration 303/1000 | Loss: 0.00040007
Iteration 304/1000 | Loss: 0.00007937
Iteration 305/1000 | Loss: 0.00006081
Iteration 306/1000 | Loss: 0.00005451
Iteration 307/1000 | Loss: 0.00005165
Iteration 308/1000 | Loss: 0.00037813
Iteration 309/1000 | Loss: 0.00017118
Iteration 310/1000 | Loss: 0.00025588
Iteration 311/1000 | Loss: 0.00004610
Iteration 312/1000 | Loss: 0.00004305
Iteration 313/1000 | Loss: 0.00027959
Iteration 314/1000 | Loss: 0.00093518
Iteration 315/1000 | Loss: 0.00017468
Iteration 316/1000 | Loss: 0.00051023
Iteration 317/1000 | Loss: 0.00030459
Iteration 318/1000 | Loss: 0.00057484
Iteration 319/1000 | Loss: 0.00054106
Iteration 320/1000 | Loss: 0.00005343
Iteration 321/1000 | Loss: 0.00004445
Iteration 322/1000 | Loss: 0.00053369
Iteration 323/1000 | Loss: 0.00139630
Iteration 324/1000 | Loss: 0.00192090
Iteration 325/1000 | Loss: 0.00058318
Iteration 326/1000 | Loss: 0.00096000
Iteration 327/1000 | Loss: 0.00037604
Iteration 328/1000 | Loss: 0.00012601
Iteration 329/1000 | Loss: 0.00021352
Iteration 330/1000 | Loss: 0.00016924
Iteration 331/1000 | Loss: 0.00017235
Iteration 332/1000 | Loss: 0.00012519
Iteration 333/1000 | Loss: 0.00004479
Iteration 334/1000 | Loss: 0.00004209
Iteration 335/1000 | Loss: 0.00029212
Iteration 336/1000 | Loss: 0.00019850
Iteration 337/1000 | Loss: 0.00003845
Iteration 338/1000 | Loss: 0.00003753
Iteration 339/1000 | Loss: 0.00003688
Iteration 340/1000 | Loss: 0.00057135
Iteration 341/1000 | Loss: 0.00574576
Iteration 342/1000 | Loss: 0.00021974
Iteration 343/1000 | Loss: 0.00012354
Iteration 344/1000 | Loss: 0.00009472
Iteration 345/1000 | Loss: 0.00033658
Iteration 346/1000 | Loss: 0.00472221
Iteration 347/1000 | Loss: 0.00292836
Iteration 348/1000 | Loss: 0.00034430
Iteration 349/1000 | Loss: 0.00093098
Iteration 350/1000 | Loss: 0.00048346
Iteration 351/1000 | Loss: 0.00060193
Iteration 352/1000 | Loss: 0.00035580
Iteration 353/1000 | Loss: 0.00026385
Iteration 354/1000 | Loss: 0.00015713
Iteration 355/1000 | Loss: 0.00022084
Iteration 356/1000 | Loss: 0.00017813
Iteration 357/1000 | Loss: 0.00048917
Iteration 358/1000 | Loss: 0.00036122
Iteration 359/1000 | Loss: 0.00018916
Iteration 360/1000 | Loss: 0.00007308
Iteration 361/1000 | Loss: 0.00006576
Iteration 362/1000 | Loss: 0.00035170
Iteration 363/1000 | Loss: 0.00013334
Iteration 364/1000 | Loss: 0.00031264
Iteration 365/1000 | Loss: 0.00006462
Iteration 366/1000 | Loss: 0.00005750
Iteration 367/1000 | Loss: 0.00005335
Iteration 368/1000 | Loss: 0.00037003
Iteration 369/1000 | Loss: 0.00017331
Iteration 370/1000 | Loss: 0.00027197
Iteration 371/1000 | Loss: 0.00083715
Iteration 372/1000 | Loss: 0.00047973
Iteration 373/1000 | Loss: 0.00017752
Iteration 374/1000 | Loss: 0.00005606
Iteration 375/1000 | Loss: 0.00004989
Iteration 376/1000 | Loss: 0.00004582
Iteration 377/1000 | Loss: 0.00004303
Iteration 378/1000 | Loss: 0.00004069
Iteration 379/1000 | Loss: 0.00056865
Iteration 380/1000 | Loss: 0.00030930
Iteration 381/1000 | Loss: 0.00017854
Iteration 382/1000 | Loss: 0.00005242
Iteration 383/1000 | Loss: 0.00004421
Iteration 384/1000 | Loss: 0.00004106
Iteration 385/1000 | Loss: 0.00003828
Iteration 386/1000 | Loss: 0.00003710
Iteration 387/1000 | Loss: 0.00003604
Iteration 388/1000 | Loss: 0.00027647
Iteration 389/1000 | Loss: 0.00012384
Iteration 390/1000 | Loss: 0.00003473
Iteration 391/1000 | Loss: 0.00003374
Iteration 392/1000 | Loss: 0.00272261
Iteration 393/1000 | Loss: 0.00296640
Iteration 394/1000 | Loss: 0.00061983
Iteration 395/1000 | Loss: 0.00014091
Iteration 396/1000 | Loss: 0.00007626
Iteration 397/1000 | Loss: 0.00005560
Iteration 398/1000 | Loss: 0.00032844
Iteration 399/1000 | Loss: 0.00027327
Iteration 400/1000 | Loss: 0.00027999
Iteration 401/1000 | Loss: 0.00008770
Iteration 402/1000 | Loss: 0.00017633
Iteration 403/1000 | Loss: 0.00004208
Iteration 404/1000 | Loss: 0.00013414
Iteration 405/1000 | Loss: 0.00004531
Iteration 406/1000 | Loss: 0.00014345
Iteration 407/1000 | Loss: 0.00004171
Iteration 408/1000 | Loss: 0.00003547
Iteration 409/1000 | Loss: 0.00002910
Iteration 410/1000 | Loss: 0.00002687
Iteration 411/1000 | Loss: 0.00002514
Iteration 412/1000 | Loss: 0.00002422
Iteration 413/1000 | Loss: 0.00031630
Iteration 414/1000 | Loss: 0.00004093
Iteration 415/1000 | Loss: 0.00002765
Iteration 416/1000 | Loss: 0.00002492
Iteration 417/1000 | Loss: 0.00002412
Iteration 418/1000 | Loss: 0.00002352
Iteration 419/1000 | Loss: 0.00002304
Iteration 420/1000 | Loss: 0.00002257
Iteration 421/1000 | Loss: 0.00002212
Iteration 422/1000 | Loss: 0.00002179
Iteration 423/1000 | Loss: 0.00002151
Iteration 424/1000 | Loss: 0.00002130
Iteration 425/1000 | Loss: 0.00002121
Iteration 426/1000 | Loss: 0.00002111
Iteration 427/1000 | Loss: 0.00002102
Iteration 428/1000 | Loss: 0.00002098
Iteration 429/1000 | Loss: 0.00002096
Iteration 430/1000 | Loss: 0.00002089
Iteration 431/1000 | Loss: 0.00002085
Iteration 432/1000 | Loss: 0.00002085
Iteration 433/1000 | Loss: 0.00002084
Iteration 434/1000 | Loss: 0.00002084
Iteration 435/1000 | Loss: 0.00002084
Iteration 436/1000 | Loss: 0.00002084
Iteration 437/1000 | Loss: 0.00002084
Iteration 438/1000 | Loss: 0.00002084
Iteration 439/1000 | Loss: 0.00002084
Iteration 440/1000 | Loss: 0.00002081
Iteration 441/1000 | Loss: 0.00002081
Iteration 442/1000 | Loss: 0.00002081
Iteration 443/1000 | Loss: 0.00002080
Iteration 444/1000 | Loss: 0.00002080
Iteration 445/1000 | Loss: 0.00002080
Iteration 446/1000 | Loss: 0.00002079
Iteration 447/1000 | Loss: 0.00002079
Iteration 448/1000 | Loss: 0.00002079
Iteration 449/1000 | Loss: 0.00002078
Iteration 450/1000 | Loss: 0.00002078
Iteration 451/1000 | Loss: 0.00002077
Iteration 452/1000 | Loss: 0.00002077
Iteration 453/1000 | Loss: 0.00002077
Iteration 454/1000 | Loss: 0.00002076
Iteration 455/1000 | Loss: 0.00002076
Iteration 456/1000 | Loss: 0.00002076
Iteration 457/1000 | Loss: 0.00002076
Iteration 458/1000 | Loss: 0.00002076
Iteration 459/1000 | Loss: 0.00002076
Iteration 460/1000 | Loss: 0.00002076
Iteration 461/1000 | Loss: 0.00002075
Iteration 462/1000 | Loss: 0.00002075
Iteration 463/1000 | Loss: 0.00002075
Iteration 464/1000 | Loss: 0.00002075
Iteration 465/1000 | Loss: 0.00002074
Iteration 466/1000 | Loss: 0.00002074
Iteration 467/1000 | Loss: 0.00002074
Iteration 468/1000 | Loss: 0.00002074
Iteration 469/1000 | Loss: 0.00002074
Iteration 470/1000 | Loss: 0.00002074
Iteration 471/1000 | Loss: 0.00002073
Iteration 472/1000 | Loss: 0.00002073
Iteration 473/1000 | Loss: 0.00002073
Iteration 474/1000 | Loss: 0.00002073
Iteration 475/1000 | Loss: 0.00002073
Iteration 476/1000 | Loss: 0.00002073
Iteration 477/1000 | Loss: 0.00002073
Iteration 478/1000 | Loss: 0.00002073
Iteration 479/1000 | Loss: 0.00002073
Iteration 480/1000 | Loss: 0.00002073
Iteration 481/1000 | Loss: 0.00002073
Iteration 482/1000 | Loss: 0.00002072
Iteration 483/1000 | Loss: 0.00002072
Iteration 484/1000 | Loss: 0.00002072
Iteration 485/1000 | Loss: 0.00002071
Iteration 486/1000 | Loss: 0.00002071
Iteration 487/1000 | Loss: 0.00002071
Iteration 488/1000 | Loss: 0.00002071
Iteration 489/1000 | Loss: 0.00002071
Iteration 490/1000 | Loss: 0.00002070
Iteration 491/1000 | Loss: 0.00002070
Iteration 492/1000 | Loss: 0.00002070
Iteration 493/1000 | Loss: 0.00002070
Iteration 494/1000 | Loss: 0.00002070
Iteration 495/1000 | Loss: 0.00002070
Iteration 496/1000 | Loss: 0.00002070
Iteration 497/1000 | Loss: 0.00002070
Iteration 498/1000 | Loss: 0.00002070
Iteration 499/1000 | Loss: 0.00002070
Iteration 500/1000 | Loss: 0.00002070
Iteration 501/1000 | Loss: 0.00002069
Iteration 502/1000 | Loss: 0.00002069
Iteration 503/1000 | Loss: 0.00002069
Iteration 504/1000 | Loss: 0.00002069
Iteration 505/1000 | Loss: 0.00002069
Iteration 506/1000 | Loss: 0.00002069
Iteration 507/1000 | Loss: 0.00002068
Iteration 508/1000 | Loss: 0.00002068
Iteration 509/1000 | Loss: 0.00002068
Iteration 510/1000 | Loss: 0.00002068
Iteration 511/1000 | Loss: 0.00002068
Iteration 512/1000 | Loss: 0.00002068
Iteration 513/1000 | Loss: 0.00002068
Iteration 514/1000 | Loss: 0.00002068
Iteration 515/1000 | Loss: 0.00002068
Iteration 516/1000 | Loss: 0.00002068
Iteration 517/1000 | Loss: 0.00002068
Iteration 518/1000 | Loss: 0.00002068
Iteration 519/1000 | Loss: 0.00002068
Iteration 520/1000 | Loss: 0.00002068
Iteration 521/1000 | Loss: 0.00002068
Iteration 522/1000 | Loss: 0.00002068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 522. Stopping optimization.
Last 5 losses: [2.068176763714291e-05, 2.068176763714291e-05, 2.068176763714291e-05, 2.068176763714291e-05, 2.068176763714291e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.068176763714291e-05

Optimization complete. Final v2v error: 3.826597213745117 mm

Highest mean error: 5.78746223449707 mm for frame 45

Lowest mean error: 3.7180886268615723 mm for frame 116

Saving results

Total time: 661.8598420619965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00753898
Iteration 2/25 | Loss: 0.00104994
Iteration 3/25 | Loss: 0.00078448
Iteration 4/25 | Loss: 0.00073529
Iteration 5/25 | Loss: 0.00071869
Iteration 6/25 | Loss: 0.00071364
Iteration 7/25 | Loss: 0.00071224
Iteration 8/25 | Loss: 0.00071220
Iteration 9/25 | Loss: 0.00071220
Iteration 10/25 | Loss: 0.00071220
Iteration 11/25 | Loss: 0.00071220
Iteration 12/25 | Loss: 0.00071220
Iteration 13/25 | Loss: 0.00071220
Iteration 14/25 | Loss: 0.00071220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007122008828446269, 0.0007122008828446269, 0.0007122008828446269, 0.0007122008828446269, 0.0007122008828446269]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007122008828446269

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.05876160
Iteration 2/25 | Loss: 0.00033557
Iteration 3/25 | Loss: 0.00033557
Iteration 4/25 | Loss: 0.00033557
Iteration 5/25 | Loss: 0.00033557
Iteration 6/25 | Loss: 0.00033557
Iteration 7/25 | Loss: 0.00033557
Iteration 8/25 | Loss: 0.00033557
Iteration 9/25 | Loss: 0.00033557
Iteration 10/25 | Loss: 0.00033557
Iteration 11/25 | Loss: 0.00033557
Iteration 12/25 | Loss: 0.00033557
Iteration 13/25 | Loss: 0.00033557
Iteration 14/25 | Loss: 0.00033557
Iteration 15/25 | Loss: 0.00033557
Iteration 16/25 | Loss: 0.00033557
Iteration 17/25 | Loss: 0.00033557
Iteration 18/25 | Loss: 0.00033557
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00033557030837982893, 0.00033557030837982893, 0.00033557030837982893, 0.00033557030837982893, 0.00033557030837982893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033557030837982893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033557
Iteration 2/1000 | Loss: 0.00004395
Iteration 3/1000 | Loss: 0.00003449
Iteration 4/1000 | Loss: 0.00003116
Iteration 5/1000 | Loss: 0.00002946
Iteration 6/1000 | Loss: 0.00002824
Iteration 7/1000 | Loss: 0.00002734
Iteration 8/1000 | Loss: 0.00002665
Iteration 9/1000 | Loss: 0.00002604
Iteration 10/1000 | Loss: 0.00002569
Iteration 11/1000 | Loss: 0.00002540
Iteration 12/1000 | Loss: 0.00002517
Iteration 13/1000 | Loss: 0.00002515
Iteration 14/1000 | Loss: 0.00002501
Iteration 15/1000 | Loss: 0.00002487
Iteration 16/1000 | Loss: 0.00002483
Iteration 17/1000 | Loss: 0.00002478
Iteration 18/1000 | Loss: 0.00002478
Iteration 19/1000 | Loss: 0.00002477
Iteration 20/1000 | Loss: 0.00002476
Iteration 21/1000 | Loss: 0.00002474
Iteration 22/1000 | Loss: 0.00002474
Iteration 23/1000 | Loss: 0.00002474
Iteration 24/1000 | Loss: 0.00002474
Iteration 25/1000 | Loss: 0.00002474
Iteration 26/1000 | Loss: 0.00002474
Iteration 27/1000 | Loss: 0.00002469
Iteration 28/1000 | Loss: 0.00002469
Iteration 29/1000 | Loss: 0.00002469
Iteration 30/1000 | Loss: 0.00002468
Iteration 31/1000 | Loss: 0.00002467
Iteration 32/1000 | Loss: 0.00002467
Iteration 33/1000 | Loss: 0.00002467
Iteration 34/1000 | Loss: 0.00002467
Iteration 35/1000 | Loss: 0.00002467
Iteration 36/1000 | Loss: 0.00002467
Iteration 37/1000 | Loss: 0.00002467
Iteration 38/1000 | Loss: 0.00002467
Iteration 39/1000 | Loss: 0.00002467
Iteration 40/1000 | Loss: 0.00002466
Iteration 41/1000 | Loss: 0.00002466
Iteration 42/1000 | Loss: 0.00002466
Iteration 43/1000 | Loss: 0.00002466
Iteration 44/1000 | Loss: 0.00002466
Iteration 45/1000 | Loss: 0.00002466
Iteration 46/1000 | Loss: 0.00002466
Iteration 47/1000 | Loss: 0.00002466
Iteration 48/1000 | Loss: 0.00002466
Iteration 49/1000 | Loss: 0.00002466
Iteration 50/1000 | Loss: 0.00002466
Iteration 51/1000 | Loss: 0.00002466
Iteration 52/1000 | Loss: 0.00002466
Iteration 53/1000 | Loss: 0.00002465
Iteration 54/1000 | Loss: 0.00002465
Iteration 55/1000 | Loss: 0.00002465
Iteration 56/1000 | Loss: 0.00002465
Iteration 57/1000 | Loss: 0.00002465
Iteration 58/1000 | Loss: 0.00002465
Iteration 59/1000 | Loss: 0.00002465
Iteration 60/1000 | Loss: 0.00002465
Iteration 61/1000 | Loss: 0.00002465
Iteration 62/1000 | Loss: 0.00002465
Iteration 63/1000 | Loss: 0.00002465
Iteration 64/1000 | Loss: 0.00002465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [2.4654727894812822e-05, 2.4654727894812822e-05, 2.4654727894812822e-05, 2.4654727894812822e-05, 2.4654727894812822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4654727894812822e-05

Optimization complete. Final v2v error: 4.171358585357666 mm

Highest mean error: 5.278955459594727 mm for frame 184

Lowest mean error: 3.376234769821167 mm for frame 224

Saving results

Total time: 39.77749705314636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437797
Iteration 2/25 | Loss: 0.00080055
Iteration 3/25 | Loss: 0.00063414
Iteration 4/25 | Loss: 0.00060784
Iteration 5/25 | Loss: 0.00059955
Iteration 6/25 | Loss: 0.00059761
Iteration 7/25 | Loss: 0.00059707
Iteration 8/25 | Loss: 0.00059707
Iteration 9/25 | Loss: 0.00059707
Iteration 10/25 | Loss: 0.00059707
Iteration 11/25 | Loss: 0.00059707
Iteration 12/25 | Loss: 0.00059707
Iteration 13/25 | Loss: 0.00059707
Iteration 14/25 | Loss: 0.00059707
Iteration 15/25 | Loss: 0.00059707
Iteration 16/25 | Loss: 0.00059707
Iteration 17/25 | Loss: 0.00059707
Iteration 18/25 | Loss: 0.00059707
Iteration 19/25 | Loss: 0.00059707
Iteration 20/25 | Loss: 0.00059707
Iteration 21/25 | Loss: 0.00059707
Iteration 22/25 | Loss: 0.00059707
Iteration 23/25 | Loss: 0.00059707
Iteration 24/25 | Loss: 0.00059707
Iteration 25/25 | Loss: 0.00059707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46488965
Iteration 2/25 | Loss: 0.00027628
Iteration 3/25 | Loss: 0.00027627
Iteration 4/25 | Loss: 0.00027627
Iteration 5/25 | Loss: 0.00027627
Iteration 6/25 | Loss: 0.00027627
Iteration 7/25 | Loss: 0.00027627
Iteration 8/25 | Loss: 0.00027627
Iteration 9/25 | Loss: 0.00027627
Iteration 10/25 | Loss: 0.00027627
Iteration 11/25 | Loss: 0.00027627
Iteration 12/25 | Loss: 0.00027627
Iteration 13/25 | Loss: 0.00027627
Iteration 14/25 | Loss: 0.00027627
Iteration 15/25 | Loss: 0.00027627
Iteration 16/25 | Loss: 0.00027627
Iteration 17/25 | Loss: 0.00027627
Iteration 18/25 | Loss: 0.00027627
Iteration 19/25 | Loss: 0.00027627
Iteration 20/25 | Loss: 0.00027627
Iteration 21/25 | Loss: 0.00027627
Iteration 22/25 | Loss: 0.00027627
Iteration 23/25 | Loss: 0.00027627
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002762665681075305, 0.0002762665681075305, 0.0002762665681075305, 0.0002762665681075305, 0.0002762665681075305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002762665681075305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027627
Iteration 2/1000 | Loss: 0.00002391
Iteration 3/1000 | Loss: 0.00001452
Iteration 4/1000 | Loss: 0.00001292
Iteration 5/1000 | Loss: 0.00001225
Iteration 6/1000 | Loss: 0.00001185
Iteration 7/1000 | Loss: 0.00001154
Iteration 8/1000 | Loss: 0.00001140
Iteration 9/1000 | Loss: 0.00001123
Iteration 10/1000 | Loss: 0.00001121
Iteration 11/1000 | Loss: 0.00001121
Iteration 12/1000 | Loss: 0.00001121
Iteration 13/1000 | Loss: 0.00001121
Iteration 14/1000 | Loss: 0.00001121
Iteration 15/1000 | Loss: 0.00001116
Iteration 16/1000 | Loss: 0.00001115
Iteration 17/1000 | Loss: 0.00001115
Iteration 18/1000 | Loss: 0.00001114
Iteration 19/1000 | Loss: 0.00001113
Iteration 20/1000 | Loss: 0.00001112
Iteration 21/1000 | Loss: 0.00001111
Iteration 22/1000 | Loss: 0.00001111
Iteration 23/1000 | Loss: 0.00001110
Iteration 24/1000 | Loss: 0.00001110
Iteration 25/1000 | Loss: 0.00001110
Iteration 26/1000 | Loss: 0.00001110
Iteration 27/1000 | Loss: 0.00001109
Iteration 28/1000 | Loss: 0.00001109
Iteration 29/1000 | Loss: 0.00001108
Iteration 30/1000 | Loss: 0.00001107
Iteration 31/1000 | Loss: 0.00001107
Iteration 32/1000 | Loss: 0.00001106
Iteration 33/1000 | Loss: 0.00001106
Iteration 34/1000 | Loss: 0.00001106
Iteration 35/1000 | Loss: 0.00001105
Iteration 36/1000 | Loss: 0.00001105
Iteration 37/1000 | Loss: 0.00001105
Iteration 38/1000 | Loss: 0.00001105
Iteration 39/1000 | Loss: 0.00001104
Iteration 40/1000 | Loss: 0.00001104
Iteration 41/1000 | Loss: 0.00001103
Iteration 42/1000 | Loss: 0.00001103
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001102
Iteration 45/1000 | Loss: 0.00001102
Iteration 46/1000 | Loss: 0.00001102
Iteration 47/1000 | Loss: 0.00001102
Iteration 48/1000 | Loss: 0.00001102
Iteration 49/1000 | Loss: 0.00001102
Iteration 50/1000 | Loss: 0.00001102
Iteration 51/1000 | Loss: 0.00001102
Iteration 52/1000 | Loss: 0.00001101
Iteration 53/1000 | Loss: 0.00001101
Iteration 54/1000 | Loss: 0.00001101
Iteration 55/1000 | Loss: 0.00001100
Iteration 56/1000 | Loss: 0.00001100
Iteration 57/1000 | Loss: 0.00001100
Iteration 58/1000 | Loss: 0.00001099
Iteration 59/1000 | Loss: 0.00001099
Iteration 60/1000 | Loss: 0.00001099
Iteration 61/1000 | Loss: 0.00001099
Iteration 62/1000 | Loss: 0.00001099
Iteration 63/1000 | Loss: 0.00001098
Iteration 64/1000 | Loss: 0.00001098
Iteration 65/1000 | Loss: 0.00001098
Iteration 66/1000 | Loss: 0.00001098
Iteration 67/1000 | Loss: 0.00001098
Iteration 68/1000 | Loss: 0.00001098
Iteration 69/1000 | Loss: 0.00001097
Iteration 70/1000 | Loss: 0.00001097
Iteration 71/1000 | Loss: 0.00001097
Iteration 72/1000 | Loss: 0.00001097
Iteration 73/1000 | Loss: 0.00001097
Iteration 74/1000 | Loss: 0.00001097
Iteration 75/1000 | Loss: 0.00001096
Iteration 76/1000 | Loss: 0.00001096
Iteration 77/1000 | Loss: 0.00001096
Iteration 78/1000 | Loss: 0.00001096
Iteration 79/1000 | Loss: 0.00001096
Iteration 80/1000 | Loss: 0.00001096
Iteration 81/1000 | Loss: 0.00001095
Iteration 82/1000 | Loss: 0.00001095
Iteration 83/1000 | Loss: 0.00001095
Iteration 84/1000 | Loss: 0.00001095
Iteration 85/1000 | Loss: 0.00001095
Iteration 86/1000 | Loss: 0.00001095
Iteration 87/1000 | Loss: 0.00001095
Iteration 88/1000 | Loss: 0.00001095
Iteration 89/1000 | Loss: 0.00001095
Iteration 90/1000 | Loss: 0.00001094
Iteration 91/1000 | Loss: 0.00001094
Iteration 92/1000 | Loss: 0.00001094
Iteration 93/1000 | Loss: 0.00001093
Iteration 94/1000 | Loss: 0.00001093
Iteration 95/1000 | Loss: 0.00001093
Iteration 96/1000 | Loss: 0.00001093
Iteration 97/1000 | Loss: 0.00001093
Iteration 98/1000 | Loss: 0.00001093
Iteration 99/1000 | Loss: 0.00001093
Iteration 100/1000 | Loss: 0.00001093
Iteration 101/1000 | Loss: 0.00001093
Iteration 102/1000 | Loss: 0.00001092
Iteration 103/1000 | Loss: 0.00001092
Iteration 104/1000 | Loss: 0.00001092
Iteration 105/1000 | Loss: 0.00001092
Iteration 106/1000 | Loss: 0.00001092
Iteration 107/1000 | Loss: 0.00001092
Iteration 108/1000 | Loss: 0.00001092
Iteration 109/1000 | Loss: 0.00001092
Iteration 110/1000 | Loss: 0.00001092
Iteration 111/1000 | Loss: 0.00001092
Iteration 112/1000 | Loss: 0.00001092
Iteration 113/1000 | Loss: 0.00001092
Iteration 114/1000 | Loss: 0.00001092
Iteration 115/1000 | Loss: 0.00001092
Iteration 116/1000 | Loss: 0.00001092
Iteration 117/1000 | Loss: 0.00001092
Iteration 118/1000 | Loss: 0.00001092
Iteration 119/1000 | Loss: 0.00001092
Iteration 120/1000 | Loss: 0.00001092
Iteration 121/1000 | Loss: 0.00001092
Iteration 122/1000 | Loss: 0.00001092
Iteration 123/1000 | Loss: 0.00001092
Iteration 124/1000 | Loss: 0.00001092
Iteration 125/1000 | Loss: 0.00001092
Iteration 126/1000 | Loss: 0.00001092
Iteration 127/1000 | Loss: 0.00001092
Iteration 128/1000 | Loss: 0.00001092
Iteration 129/1000 | Loss: 0.00001092
Iteration 130/1000 | Loss: 0.00001092
Iteration 131/1000 | Loss: 0.00001092
Iteration 132/1000 | Loss: 0.00001092
Iteration 133/1000 | Loss: 0.00001092
Iteration 134/1000 | Loss: 0.00001092
Iteration 135/1000 | Loss: 0.00001092
Iteration 136/1000 | Loss: 0.00001092
Iteration 137/1000 | Loss: 0.00001092
Iteration 138/1000 | Loss: 0.00001092
Iteration 139/1000 | Loss: 0.00001092
Iteration 140/1000 | Loss: 0.00001092
Iteration 141/1000 | Loss: 0.00001092
Iteration 142/1000 | Loss: 0.00001092
Iteration 143/1000 | Loss: 0.00001092
Iteration 144/1000 | Loss: 0.00001092
Iteration 145/1000 | Loss: 0.00001092
Iteration 146/1000 | Loss: 0.00001092
Iteration 147/1000 | Loss: 0.00001092
Iteration 148/1000 | Loss: 0.00001092
Iteration 149/1000 | Loss: 0.00001092
Iteration 150/1000 | Loss: 0.00001092
Iteration 151/1000 | Loss: 0.00001092
Iteration 152/1000 | Loss: 0.00001092
Iteration 153/1000 | Loss: 0.00001092
Iteration 154/1000 | Loss: 0.00001092
Iteration 155/1000 | Loss: 0.00001092
Iteration 156/1000 | Loss: 0.00001092
Iteration 157/1000 | Loss: 0.00001092
Iteration 158/1000 | Loss: 0.00001092
Iteration 159/1000 | Loss: 0.00001092
Iteration 160/1000 | Loss: 0.00001092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.0922594810836017e-05, 1.0922594810836017e-05, 1.0922594810836017e-05, 1.0922594810836017e-05, 1.0922594810836017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0922594810836017e-05

Optimization complete. Final v2v error: 2.8335623741149902 mm

Highest mean error: 3.087952136993408 mm for frame 112

Lowest mean error: 2.6193149089813232 mm for frame 20

Saving results

Total time: 32.71373200416565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777648
Iteration 2/25 | Loss: 0.00091886
Iteration 3/25 | Loss: 0.00074537
Iteration 4/25 | Loss: 0.00069713
Iteration 5/25 | Loss: 0.00067473
Iteration 6/25 | Loss: 0.00067020
Iteration 7/25 | Loss: 0.00066865
Iteration 8/25 | Loss: 0.00066809
Iteration 9/25 | Loss: 0.00066809
Iteration 10/25 | Loss: 0.00066809
Iteration 11/25 | Loss: 0.00066809
Iteration 12/25 | Loss: 0.00066809
Iteration 13/25 | Loss: 0.00066809
Iteration 14/25 | Loss: 0.00066809
Iteration 15/25 | Loss: 0.00066809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006680930382572114, 0.0006680930382572114, 0.0006680930382572114, 0.0006680930382572114, 0.0006680930382572114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006680930382572114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46763468
Iteration 2/25 | Loss: 0.00030986
Iteration 3/25 | Loss: 0.00030986
Iteration 4/25 | Loss: 0.00030986
Iteration 5/25 | Loss: 0.00030986
Iteration 6/25 | Loss: 0.00030986
Iteration 7/25 | Loss: 0.00030986
Iteration 8/25 | Loss: 0.00030986
Iteration 9/25 | Loss: 0.00030986
Iteration 10/25 | Loss: 0.00030986
Iteration 11/25 | Loss: 0.00030986
Iteration 12/25 | Loss: 0.00030986
Iteration 13/25 | Loss: 0.00030986
Iteration 14/25 | Loss: 0.00030986
Iteration 15/25 | Loss: 0.00030986
Iteration 16/25 | Loss: 0.00030986
Iteration 17/25 | Loss: 0.00030986
Iteration 18/25 | Loss: 0.00030986
Iteration 19/25 | Loss: 0.00030986
Iteration 20/25 | Loss: 0.00030986
Iteration 21/25 | Loss: 0.00030986
Iteration 22/25 | Loss: 0.00030986
Iteration 23/25 | Loss: 0.00030986
Iteration 24/25 | Loss: 0.00030986
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00030985704506747425, 0.00030985704506747425, 0.00030985704506747425, 0.00030985704506747425, 0.00030985704506747425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030985704506747425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030986
Iteration 2/1000 | Loss: 0.00003893
Iteration 3/1000 | Loss: 0.00002621
Iteration 4/1000 | Loss: 0.00002381
Iteration 5/1000 | Loss: 0.00002259
Iteration 6/1000 | Loss: 0.00002157
Iteration 7/1000 | Loss: 0.00002101
Iteration 8/1000 | Loss: 0.00002049
Iteration 9/1000 | Loss: 0.00002010
Iteration 10/1000 | Loss: 0.00001983
Iteration 11/1000 | Loss: 0.00001966
Iteration 12/1000 | Loss: 0.00001951
Iteration 13/1000 | Loss: 0.00001946
Iteration 14/1000 | Loss: 0.00001945
Iteration 15/1000 | Loss: 0.00001938
Iteration 16/1000 | Loss: 0.00001932
Iteration 17/1000 | Loss: 0.00001931
Iteration 18/1000 | Loss: 0.00001931
Iteration 19/1000 | Loss: 0.00001931
Iteration 20/1000 | Loss: 0.00001929
Iteration 21/1000 | Loss: 0.00001929
Iteration 22/1000 | Loss: 0.00001928
Iteration 23/1000 | Loss: 0.00001928
Iteration 24/1000 | Loss: 0.00001928
Iteration 25/1000 | Loss: 0.00001927
Iteration 26/1000 | Loss: 0.00001926
Iteration 27/1000 | Loss: 0.00001925
Iteration 28/1000 | Loss: 0.00001925
Iteration 29/1000 | Loss: 0.00001925
Iteration 30/1000 | Loss: 0.00001925
Iteration 31/1000 | Loss: 0.00001925
Iteration 32/1000 | Loss: 0.00001925
Iteration 33/1000 | Loss: 0.00001924
Iteration 34/1000 | Loss: 0.00001924
Iteration 35/1000 | Loss: 0.00001923
Iteration 36/1000 | Loss: 0.00001923
Iteration 37/1000 | Loss: 0.00001922
Iteration 38/1000 | Loss: 0.00001922
Iteration 39/1000 | Loss: 0.00001922
Iteration 40/1000 | Loss: 0.00001921
Iteration 41/1000 | Loss: 0.00001921
Iteration 42/1000 | Loss: 0.00001921
Iteration 43/1000 | Loss: 0.00001921
Iteration 44/1000 | Loss: 0.00001920
Iteration 45/1000 | Loss: 0.00001920
Iteration 46/1000 | Loss: 0.00001920
Iteration 47/1000 | Loss: 0.00001919
Iteration 48/1000 | Loss: 0.00001918
Iteration 49/1000 | Loss: 0.00001918
Iteration 50/1000 | Loss: 0.00001918
Iteration 51/1000 | Loss: 0.00001917
Iteration 52/1000 | Loss: 0.00001917
Iteration 53/1000 | Loss: 0.00001917
Iteration 54/1000 | Loss: 0.00001917
Iteration 55/1000 | Loss: 0.00001917
Iteration 56/1000 | Loss: 0.00001916
Iteration 57/1000 | Loss: 0.00001916
Iteration 58/1000 | Loss: 0.00001916
Iteration 59/1000 | Loss: 0.00001916
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001915
Iteration 62/1000 | Loss: 0.00001915
Iteration 63/1000 | Loss: 0.00001915
Iteration 64/1000 | Loss: 0.00001915
Iteration 65/1000 | Loss: 0.00001915
Iteration 66/1000 | Loss: 0.00001915
Iteration 67/1000 | Loss: 0.00001915
Iteration 68/1000 | Loss: 0.00001915
Iteration 69/1000 | Loss: 0.00001914
Iteration 70/1000 | Loss: 0.00001914
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00001914
Iteration 73/1000 | Loss: 0.00001914
Iteration 74/1000 | Loss: 0.00001914
Iteration 75/1000 | Loss: 0.00001914
Iteration 76/1000 | Loss: 0.00001914
Iteration 77/1000 | Loss: 0.00001914
Iteration 78/1000 | Loss: 0.00001913
Iteration 79/1000 | Loss: 0.00001913
Iteration 80/1000 | Loss: 0.00001913
Iteration 81/1000 | Loss: 0.00001913
Iteration 82/1000 | Loss: 0.00001913
Iteration 83/1000 | Loss: 0.00001913
Iteration 84/1000 | Loss: 0.00001913
Iteration 85/1000 | Loss: 0.00001913
Iteration 86/1000 | Loss: 0.00001913
Iteration 87/1000 | Loss: 0.00001913
Iteration 88/1000 | Loss: 0.00001913
Iteration 89/1000 | Loss: 0.00001913
Iteration 90/1000 | Loss: 0.00001913
Iteration 91/1000 | Loss: 0.00001912
Iteration 92/1000 | Loss: 0.00001912
Iteration 93/1000 | Loss: 0.00001912
Iteration 94/1000 | Loss: 0.00001912
Iteration 95/1000 | Loss: 0.00001912
Iteration 96/1000 | Loss: 0.00001912
Iteration 97/1000 | Loss: 0.00001912
Iteration 98/1000 | Loss: 0.00001912
Iteration 99/1000 | Loss: 0.00001912
Iteration 100/1000 | Loss: 0.00001912
Iteration 101/1000 | Loss: 0.00001912
Iteration 102/1000 | Loss: 0.00001912
Iteration 103/1000 | Loss: 0.00001912
Iteration 104/1000 | Loss: 0.00001912
Iteration 105/1000 | Loss: 0.00001912
Iteration 106/1000 | Loss: 0.00001912
Iteration 107/1000 | Loss: 0.00001912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.9115119357593358e-05, 1.9115119357593358e-05, 1.9115119357593358e-05, 1.9115119357593358e-05, 1.9115119357593358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9115119357593358e-05

Optimization complete. Final v2v error: 3.651437997817993 mm

Highest mean error: 4.505234241485596 mm for frame 67

Lowest mean error: 3.0255651473999023 mm for frame 110

Saving results

Total time: 43.20731163024902
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803209
Iteration 2/25 | Loss: 0.00146510
Iteration 3/25 | Loss: 0.00099050
Iteration 4/25 | Loss: 0.00084876
Iteration 5/25 | Loss: 0.00077433
Iteration 6/25 | Loss: 0.00074513
Iteration 7/25 | Loss: 0.00073210
Iteration 8/25 | Loss: 0.00073174
Iteration 9/25 | Loss: 0.00072614
Iteration 10/25 | Loss: 0.00072498
Iteration 11/25 | Loss: 0.00072473
Iteration 12/25 | Loss: 0.00072449
Iteration 13/25 | Loss: 0.00072931
Iteration 14/25 | Loss: 0.00072711
Iteration 15/25 | Loss: 0.00072496
Iteration 16/25 | Loss: 0.00072255
Iteration 17/25 | Loss: 0.00072160
Iteration 18/25 | Loss: 0.00071781
Iteration 19/25 | Loss: 0.00071536
Iteration 20/25 | Loss: 0.00071454
Iteration 21/25 | Loss: 0.00071547
Iteration 22/25 | Loss: 0.00071327
Iteration 23/25 | Loss: 0.00071261
Iteration 24/25 | Loss: 0.00071251
Iteration 25/25 | Loss: 0.00071251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46005273
Iteration 2/25 | Loss: 0.00037229
Iteration 3/25 | Loss: 0.00037227
Iteration 4/25 | Loss: 0.00037227
Iteration 5/25 | Loss: 0.00037226
Iteration 6/25 | Loss: 0.00037226
Iteration 7/25 | Loss: 0.00037226
Iteration 8/25 | Loss: 0.00037226
Iteration 9/25 | Loss: 0.00037226
Iteration 10/25 | Loss: 0.00037226
Iteration 11/25 | Loss: 0.00037226
Iteration 12/25 | Loss: 0.00037226
Iteration 13/25 | Loss: 0.00037226
Iteration 14/25 | Loss: 0.00037226
Iteration 15/25 | Loss: 0.00037226
Iteration 16/25 | Loss: 0.00037226
Iteration 17/25 | Loss: 0.00037226
Iteration 18/25 | Loss: 0.00037226
Iteration 19/25 | Loss: 0.00037226
Iteration 20/25 | Loss: 0.00037226
Iteration 21/25 | Loss: 0.00037226
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0003722630790434778, 0.0003722630790434778, 0.0003722630790434778, 0.0003722630790434778, 0.0003722630790434778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003722630790434778

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037226
Iteration 2/1000 | Loss: 0.00003444
Iteration 3/1000 | Loss: 0.00002611
Iteration 4/1000 | Loss: 0.00002409
Iteration 5/1000 | Loss: 0.00002297
Iteration 6/1000 | Loss: 0.00002227
Iteration 7/1000 | Loss: 0.00002166
Iteration 8/1000 | Loss: 0.00002121
Iteration 9/1000 | Loss: 0.00002099
Iteration 10/1000 | Loss: 0.00002083
Iteration 11/1000 | Loss: 0.00002070
Iteration 12/1000 | Loss: 0.00002066
Iteration 13/1000 | Loss: 0.00002065
Iteration 14/1000 | Loss: 0.00002060
Iteration 15/1000 | Loss: 0.00002060
Iteration 16/1000 | Loss: 0.00002059
Iteration 17/1000 | Loss: 0.00002058
Iteration 18/1000 | Loss: 0.00002053
Iteration 19/1000 | Loss: 0.00002053
Iteration 20/1000 | Loss: 0.00002052
Iteration 21/1000 | Loss: 0.00002047
Iteration 22/1000 | Loss: 0.00002044
Iteration 23/1000 | Loss: 0.00002044
Iteration 24/1000 | Loss: 0.00002044
Iteration 25/1000 | Loss: 0.00002044
Iteration 26/1000 | Loss: 0.00002044
Iteration 27/1000 | Loss: 0.00002044
Iteration 28/1000 | Loss: 0.00002043
Iteration 29/1000 | Loss: 0.00002043
Iteration 30/1000 | Loss: 0.00002042
Iteration 31/1000 | Loss: 0.00002041
Iteration 32/1000 | Loss: 0.00002041
Iteration 33/1000 | Loss: 0.00002040
Iteration 34/1000 | Loss: 0.00002040
Iteration 35/1000 | Loss: 0.00002039
Iteration 36/1000 | Loss: 0.00002039
Iteration 37/1000 | Loss: 0.00002038
Iteration 38/1000 | Loss: 0.00002037
Iteration 39/1000 | Loss: 0.00002037
Iteration 40/1000 | Loss: 0.00002037
Iteration 41/1000 | Loss: 0.00002036
Iteration 42/1000 | Loss: 0.00002036
Iteration 43/1000 | Loss: 0.00002036
Iteration 44/1000 | Loss: 0.00002036
Iteration 45/1000 | Loss: 0.00002036
Iteration 46/1000 | Loss: 0.00002036
Iteration 47/1000 | Loss: 0.00002036
Iteration 48/1000 | Loss: 0.00002036
Iteration 49/1000 | Loss: 0.00002036
Iteration 50/1000 | Loss: 0.00002035
Iteration 51/1000 | Loss: 0.00002035
Iteration 52/1000 | Loss: 0.00002035
Iteration 53/1000 | Loss: 0.00002034
Iteration 54/1000 | Loss: 0.00002034
Iteration 55/1000 | Loss: 0.00002034
Iteration 56/1000 | Loss: 0.00002034
Iteration 57/1000 | Loss: 0.00002034
Iteration 58/1000 | Loss: 0.00002034
Iteration 59/1000 | Loss: 0.00002034
Iteration 60/1000 | Loss: 0.00002034
Iteration 61/1000 | Loss: 0.00002034
Iteration 62/1000 | Loss: 0.00002034
Iteration 63/1000 | Loss: 0.00002034
Iteration 64/1000 | Loss: 0.00002034
Iteration 65/1000 | Loss: 0.00002033
Iteration 66/1000 | Loss: 0.00002032
Iteration 67/1000 | Loss: 0.00002032
Iteration 68/1000 | Loss: 0.00002031
Iteration 69/1000 | Loss: 0.00002031
Iteration 70/1000 | Loss: 0.00002031
Iteration 71/1000 | Loss: 0.00002031
Iteration 72/1000 | Loss: 0.00002031
Iteration 73/1000 | Loss: 0.00002030
Iteration 74/1000 | Loss: 0.00002030
Iteration 75/1000 | Loss: 0.00002030
Iteration 76/1000 | Loss: 0.00002029
Iteration 77/1000 | Loss: 0.00002029
Iteration 78/1000 | Loss: 0.00002029
Iteration 79/1000 | Loss: 0.00002029
Iteration 80/1000 | Loss: 0.00002029
Iteration 81/1000 | Loss: 0.00002029
Iteration 82/1000 | Loss: 0.00002029
Iteration 83/1000 | Loss: 0.00002028
Iteration 84/1000 | Loss: 0.00002028
Iteration 85/1000 | Loss: 0.00002028
Iteration 86/1000 | Loss: 0.00002028
Iteration 87/1000 | Loss: 0.00002028
Iteration 88/1000 | Loss: 0.00002027
Iteration 89/1000 | Loss: 0.00002027
Iteration 90/1000 | Loss: 0.00002027
Iteration 91/1000 | Loss: 0.00002027
Iteration 92/1000 | Loss: 0.00002027
Iteration 93/1000 | Loss: 0.00002027
Iteration 94/1000 | Loss: 0.00002027
Iteration 95/1000 | Loss: 0.00002027
Iteration 96/1000 | Loss: 0.00002027
Iteration 97/1000 | Loss: 0.00002027
Iteration 98/1000 | Loss: 0.00002026
Iteration 99/1000 | Loss: 0.00002026
Iteration 100/1000 | Loss: 0.00002026
Iteration 101/1000 | Loss: 0.00002026
Iteration 102/1000 | Loss: 0.00002026
Iteration 103/1000 | Loss: 0.00002026
Iteration 104/1000 | Loss: 0.00002026
Iteration 105/1000 | Loss: 0.00002026
Iteration 106/1000 | Loss: 0.00002026
Iteration 107/1000 | Loss: 0.00002025
Iteration 108/1000 | Loss: 0.00002025
Iteration 109/1000 | Loss: 0.00002025
Iteration 110/1000 | Loss: 0.00002025
Iteration 111/1000 | Loss: 0.00002025
Iteration 112/1000 | Loss: 0.00002025
Iteration 113/1000 | Loss: 0.00002025
Iteration 114/1000 | Loss: 0.00002025
Iteration 115/1000 | Loss: 0.00002025
Iteration 116/1000 | Loss: 0.00002025
Iteration 117/1000 | Loss: 0.00002025
Iteration 118/1000 | Loss: 0.00002025
Iteration 119/1000 | Loss: 0.00002025
Iteration 120/1000 | Loss: 0.00002025
Iteration 121/1000 | Loss: 0.00002025
Iteration 122/1000 | Loss: 0.00002025
Iteration 123/1000 | Loss: 0.00002025
Iteration 124/1000 | Loss: 0.00002025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.0251723981346004e-05, 2.0251723981346004e-05, 2.0251723981346004e-05, 2.0251723981346004e-05, 2.0251723981346004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0251723981346004e-05

Optimization complete. Final v2v error: 3.871581792831421 mm

Highest mean error: 4.184536933898926 mm for frame 82

Lowest mean error: 3.628715991973877 mm for frame 8

Saving results

Total time: 77.54887461662292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00942122
Iteration 2/25 | Loss: 0.00164962
Iteration 3/25 | Loss: 0.00116584
Iteration 4/25 | Loss: 0.00102885
Iteration 5/25 | Loss: 0.00097847
Iteration 6/25 | Loss: 0.00094227
Iteration 7/25 | Loss: 0.00093116
Iteration 8/25 | Loss: 0.00087666
Iteration 9/25 | Loss: 0.00084768
Iteration 10/25 | Loss: 0.00083052
Iteration 11/25 | Loss: 0.00082080
Iteration 12/25 | Loss: 0.00081744
Iteration 13/25 | Loss: 0.00082090
Iteration 14/25 | Loss: 0.00081350
Iteration 15/25 | Loss: 0.00081454
Iteration 16/25 | Loss: 0.00081521
Iteration 17/25 | Loss: 0.00080972
Iteration 18/25 | Loss: 0.00081203
Iteration 19/25 | Loss: 0.00080771
Iteration 20/25 | Loss: 0.00080575
Iteration 21/25 | Loss: 0.00080445
Iteration 22/25 | Loss: 0.00080379
Iteration 23/25 | Loss: 0.00080530
Iteration 24/25 | Loss: 0.00079786
Iteration 25/25 | Loss: 0.00079610

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.36089420
Iteration 2/25 | Loss: 0.00033831
Iteration 3/25 | Loss: 0.00033829
Iteration 4/25 | Loss: 0.00033829
Iteration 5/25 | Loss: 0.00033829
Iteration 6/25 | Loss: 0.00033829
Iteration 7/25 | Loss: 0.00033829
Iteration 8/25 | Loss: 0.00033829
Iteration 9/25 | Loss: 0.00033829
Iteration 10/25 | Loss: 0.00033829
Iteration 11/25 | Loss: 0.00033829
Iteration 12/25 | Loss: 0.00033829
Iteration 13/25 | Loss: 0.00033829
Iteration 14/25 | Loss: 0.00033829
Iteration 15/25 | Loss: 0.00033829
Iteration 16/25 | Loss: 0.00033829
Iteration 17/25 | Loss: 0.00033829
Iteration 18/25 | Loss: 0.00033829
Iteration 19/25 | Loss: 0.00033829
Iteration 20/25 | Loss: 0.00033829
Iteration 21/25 | Loss: 0.00033829
Iteration 22/25 | Loss: 0.00033829
Iteration 23/25 | Loss: 0.00033829
Iteration 24/25 | Loss: 0.00033829
Iteration 25/25 | Loss: 0.00033829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033829
Iteration 2/1000 | Loss: 0.00062213
Iteration 3/1000 | Loss: 0.00034169
Iteration 4/1000 | Loss: 0.00029741
Iteration 5/1000 | Loss: 0.00021315
Iteration 6/1000 | Loss: 0.00013886
Iteration 7/1000 | Loss: 0.00010913
Iteration 8/1000 | Loss: 0.00019610
Iteration 9/1000 | Loss: 0.00041143
Iteration 10/1000 | Loss: 0.00022040
Iteration 11/1000 | Loss: 0.00017674
Iteration 12/1000 | Loss: 0.00014979
Iteration 13/1000 | Loss: 0.00027393
Iteration 14/1000 | Loss: 0.00015468
Iteration 15/1000 | Loss: 0.00033284
Iteration 16/1000 | Loss: 0.00020955
Iteration 17/1000 | Loss: 0.00019481
Iteration 18/1000 | Loss: 0.00016788
Iteration 19/1000 | Loss: 0.00018086
Iteration 20/1000 | Loss: 0.00011244
Iteration 21/1000 | Loss: 0.00009741
Iteration 22/1000 | Loss: 0.00008770
Iteration 23/1000 | Loss: 0.00007825
Iteration 24/1000 | Loss: 0.00006928
Iteration 25/1000 | Loss: 0.00006327
Iteration 26/1000 | Loss: 0.00021706
Iteration 27/1000 | Loss: 0.00012512
Iteration 28/1000 | Loss: 0.00007277
Iteration 29/1000 | Loss: 0.00008868
Iteration 30/1000 | Loss: 0.00007280
Iteration 31/1000 | Loss: 0.00005668
Iteration 32/1000 | Loss: 0.00005059
Iteration 33/1000 | Loss: 0.00015910
Iteration 34/1000 | Loss: 0.00011552
Iteration 35/1000 | Loss: 0.00014395
Iteration 36/1000 | Loss: 0.00007787
Iteration 37/1000 | Loss: 0.00007914
Iteration 38/1000 | Loss: 0.00006980
Iteration 39/1000 | Loss: 0.00007406
Iteration 40/1000 | Loss: 0.00006334
Iteration 41/1000 | Loss: 0.00004418
Iteration 42/1000 | Loss: 0.00004259
Iteration 43/1000 | Loss: 0.00005646
Iteration 44/1000 | Loss: 0.00010962
Iteration 45/1000 | Loss: 0.00007934
Iteration 46/1000 | Loss: 0.00008683
Iteration 47/1000 | Loss: 0.00007730
Iteration 48/1000 | Loss: 0.00007945
Iteration 49/1000 | Loss: 0.00006292
Iteration 50/1000 | Loss: 0.00006300
Iteration 51/1000 | Loss: 0.00005636
Iteration 52/1000 | Loss: 0.00007103
Iteration 53/1000 | Loss: 0.00006390
Iteration 54/1000 | Loss: 0.00005357
Iteration 55/1000 | Loss: 0.00008711
Iteration 56/1000 | Loss: 0.00007470
Iteration 57/1000 | Loss: 0.00005224
Iteration 58/1000 | Loss: 0.00005072
Iteration 59/1000 | Loss: 0.00004243
Iteration 60/1000 | Loss: 0.00005559
Iteration 61/1000 | Loss: 0.00005922
Iteration 62/1000 | Loss: 0.00008214
Iteration 63/1000 | Loss: 0.00007976
Iteration 64/1000 | Loss: 0.00005769
Iteration 65/1000 | Loss: 0.00005764
Iteration 66/1000 | Loss: 0.00004787
Iteration 67/1000 | Loss: 0.00006001
Iteration 68/1000 | Loss: 0.00006852
Iteration 69/1000 | Loss: 0.00005856
Iteration 70/1000 | Loss: 0.00004958
Iteration 71/1000 | Loss: 0.00005025
Iteration 72/1000 | Loss: 0.00005974
Iteration 73/1000 | Loss: 0.00006208
Iteration 74/1000 | Loss: 0.00006899
Iteration 75/1000 | Loss: 0.00007107
Iteration 76/1000 | Loss: 0.00007562
Iteration 77/1000 | Loss: 0.00007855
Iteration 78/1000 | Loss: 0.00012044
Iteration 79/1000 | Loss: 0.00017878
Iteration 80/1000 | Loss: 0.00017863
Iteration 81/1000 | Loss: 0.00007629
Iteration 82/1000 | Loss: 0.00005289
Iteration 83/1000 | Loss: 0.00006138
Iteration 84/1000 | Loss: 0.00007581
Iteration 85/1000 | Loss: 0.00005568
Iteration 86/1000 | Loss: 0.00005915
Iteration 87/1000 | Loss: 0.00005966
Iteration 88/1000 | Loss: 0.00004994
Iteration 89/1000 | Loss: 0.00005530
Iteration 90/1000 | Loss: 0.00005796
Iteration 91/1000 | Loss: 0.00005861
Iteration 92/1000 | Loss: 0.00005276
Iteration 93/1000 | Loss: 0.00013340
Iteration 94/1000 | Loss: 0.00010526
Iteration 95/1000 | Loss: 0.00006074
Iteration 96/1000 | Loss: 0.00005407
Iteration 97/1000 | Loss: 0.00004663
Iteration 98/1000 | Loss: 0.00006663
Iteration 99/1000 | Loss: 0.00005902
Iteration 100/1000 | Loss: 0.00006578
Iteration 101/1000 | Loss: 0.00005816
Iteration 102/1000 | Loss: 0.00004770
Iteration 103/1000 | Loss: 0.00005153
Iteration 104/1000 | Loss: 0.00006140
Iteration 105/1000 | Loss: 0.00005655
Iteration 106/1000 | Loss: 0.00007130
Iteration 107/1000 | Loss: 0.00005617
Iteration 108/1000 | Loss: 0.00007136
Iteration 109/1000 | Loss: 0.00005755
Iteration 110/1000 | Loss: 0.00015408
Iteration 111/1000 | Loss: 0.00009931
Iteration 112/1000 | Loss: 0.00010682
Iteration 113/1000 | Loss: 0.00005396
Iteration 114/1000 | Loss: 0.00005025
Iteration 115/1000 | Loss: 0.00005003
Iteration 116/1000 | Loss: 0.00009837
Iteration 117/1000 | Loss: 0.00008255
Iteration 118/1000 | Loss: 0.00008735
Iteration 119/1000 | Loss: 0.00018533
Iteration 120/1000 | Loss: 0.00013506
Iteration 121/1000 | Loss: 0.00005255
Iteration 122/1000 | Loss: 0.00004400
Iteration 123/1000 | Loss: 0.00003754
Iteration 124/1000 | Loss: 0.00005022
Iteration 125/1000 | Loss: 0.00005149
Iteration 126/1000 | Loss: 0.00004934
Iteration 127/1000 | Loss: 0.00004821
Iteration 128/1000 | Loss: 0.00009481
Iteration 129/1000 | Loss: 0.00021427
Iteration 130/1000 | Loss: 0.00019338
Iteration 131/1000 | Loss: 0.00014610
Iteration 132/1000 | Loss: 0.00013192
Iteration 133/1000 | Loss: 0.00014180
Iteration 134/1000 | Loss: 0.00017384
Iteration 135/1000 | Loss: 0.00017095
Iteration 136/1000 | Loss: 0.00019016
Iteration 137/1000 | Loss: 0.00022978
Iteration 138/1000 | Loss: 0.00010194
Iteration 139/1000 | Loss: 0.00008135
Iteration 140/1000 | Loss: 0.00006600
Iteration 141/1000 | Loss: 0.00005959
Iteration 142/1000 | Loss: 0.00006686
Iteration 143/1000 | Loss: 0.00005691
Iteration 144/1000 | Loss: 0.00005180
Iteration 145/1000 | Loss: 0.00005691
Iteration 146/1000 | Loss: 0.00004347
Iteration 147/1000 | Loss: 0.00004659
Iteration 148/1000 | Loss: 0.00004387
Iteration 149/1000 | Loss: 0.00004083
Iteration 150/1000 | Loss: 0.00004257
Iteration 151/1000 | Loss: 0.00004407
Iteration 152/1000 | Loss: 0.00004580
Iteration 153/1000 | Loss: 0.00003985
Iteration 154/1000 | Loss: 0.00003690
Iteration 155/1000 | Loss: 0.00005460
Iteration 156/1000 | Loss: 0.00004444
Iteration 157/1000 | Loss: 0.00004206
Iteration 158/1000 | Loss: 0.00004610
Iteration 159/1000 | Loss: 0.00004155
Iteration 160/1000 | Loss: 0.00004942
Iteration 161/1000 | Loss: 0.00005048
Iteration 162/1000 | Loss: 0.00004808
Iteration 163/1000 | Loss: 0.00004256
Iteration 164/1000 | Loss: 0.00004483
Iteration 165/1000 | Loss: 0.00004213
Iteration 166/1000 | Loss: 0.00006562
Iteration 167/1000 | Loss: 0.00004603
Iteration 168/1000 | Loss: 0.00006625
Iteration 169/1000 | Loss: 0.00004144
Iteration 170/1000 | Loss: 0.00004683
Iteration 171/1000 | Loss: 0.00005222
Iteration 172/1000 | Loss: 0.00005404
Iteration 173/1000 | Loss: 0.00005935
Iteration 174/1000 | Loss: 0.00005659
Iteration 175/1000 | Loss: 0.00005662
Iteration 176/1000 | Loss: 0.00005578
Iteration 177/1000 | Loss: 0.00005077
Iteration 178/1000 | Loss: 0.00006105
Iteration 179/1000 | Loss: 0.00006782
Iteration 180/1000 | Loss: 0.00007082
Iteration 181/1000 | Loss: 0.00006929
Iteration 182/1000 | Loss: 0.00006713
Iteration 183/1000 | Loss: 0.00006822
Iteration 184/1000 | Loss: 0.00007114
Iteration 185/1000 | Loss: 0.00006883
Iteration 186/1000 | Loss: 0.00007119
Iteration 187/1000 | Loss: 0.00006411
Iteration 188/1000 | Loss: 0.00006653
Iteration 189/1000 | Loss: 0.00006702
Iteration 190/1000 | Loss: 0.00006271
Iteration 191/1000 | Loss: 0.00006529
Iteration 192/1000 | Loss: 0.00005772
Iteration 193/1000 | Loss: 0.00006014
Iteration 194/1000 | Loss: 0.00006388
Iteration 195/1000 | Loss: 0.00006660
Iteration 196/1000 | Loss: 0.00007064
Iteration 197/1000 | Loss: 0.00007094
Iteration 198/1000 | Loss: 0.00005510
Iteration 199/1000 | Loss: 0.00004273
Iteration 200/1000 | Loss: 0.00005319
Iteration 201/1000 | Loss: 0.00005999
Iteration 202/1000 | Loss: 0.00004951
Iteration 203/1000 | Loss: 0.00005270
Iteration 204/1000 | Loss: 0.00005267
Iteration 205/1000 | Loss: 0.00004758
Iteration 206/1000 | Loss: 0.00004954
Iteration 207/1000 | Loss: 0.00004471
Iteration 208/1000 | Loss: 0.00004745
Iteration 209/1000 | Loss: 0.00005166
Iteration 210/1000 | Loss: 0.00005378
Iteration 211/1000 | Loss: 0.00005212
Iteration 212/1000 | Loss: 0.00005132
Iteration 213/1000 | Loss: 0.00005350
Iteration 214/1000 | Loss: 0.00004694
Iteration 215/1000 | Loss: 0.00004342
Iteration 216/1000 | Loss: 0.00005127
Iteration 217/1000 | Loss: 0.00005040
Iteration 218/1000 | Loss: 0.00004382
Iteration 219/1000 | Loss: 0.00004603
Iteration 220/1000 | Loss: 0.00005139
Iteration 221/1000 | Loss: 0.00004931
Iteration 222/1000 | Loss: 0.00004883
Iteration 223/1000 | Loss: 0.00004810
Iteration 224/1000 | Loss: 0.00005070
Iteration 225/1000 | Loss: 0.00004783
Iteration 226/1000 | Loss: 0.00004941
Iteration 227/1000 | Loss: 0.00004915
Iteration 228/1000 | Loss: 0.00004680
Iteration 229/1000 | Loss: 0.00004409
Iteration 230/1000 | Loss: 0.00004832
Iteration 231/1000 | Loss: 0.00004754
Iteration 232/1000 | Loss: 0.00004415
Iteration 233/1000 | Loss: 0.00004764
Iteration 234/1000 | Loss: 0.00004933
Iteration 235/1000 | Loss: 0.00004971
Iteration 236/1000 | Loss: 0.00004404
Iteration 237/1000 | Loss: 0.00004893
Iteration 238/1000 | Loss: 0.00004930
Iteration 239/1000 | Loss: 0.00004924
Iteration 240/1000 | Loss: 0.00004872
Iteration 241/1000 | Loss: 0.00004891
Iteration 242/1000 | Loss: 0.00004817
Iteration 243/1000 | Loss: 0.00004817
Iteration 244/1000 | Loss: 0.00004831
Iteration 245/1000 | Loss: 0.00004702
Iteration 246/1000 | Loss: 0.00004783
Iteration 247/1000 | Loss: 0.00004691
Iteration 248/1000 | Loss: 0.00004781
Iteration 249/1000 | Loss: 0.00004816
Iteration 250/1000 | Loss: 0.00004877
Iteration 251/1000 | Loss: 0.00004835
Iteration 252/1000 | Loss: 0.00004919
Iteration 253/1000 | Loss: 0.00004893
Iteration 254/1000 | Loss: 0.00004907
Iteration 255/1000 | Loss: 0.00004885
Iteration 256/1000 | Loss: 0.00004872
Iteration 257/1000 | Loss: 0.00004777
Iteration 258/1000 | Loss: 0.00004756
Iteration 259/1000 | Loss: 0.00004777
Iteration 260/1000 | Loss: 0.00004714
Iteration 261/1000 | Loss: 0.00004118
Iteration 262/1000 | Loss: 0.00003774
Iteration 263/1000 | Loss: 0.00003981
Iteration 264/1000 | Loss: 0.00004489
Iteration 265/1000 | Loss: 0.00004805
Iteration 266/1000 | Loss: 0.00006164
Iteration 267/1000 | Loss: 0.00006118
Iteration 268/1000 | Loss: 0.00005981
Iteration 269/1000 | Loss: 0.00004555
Iteration 270/1000 | Loss: 0.00004073
Iteration 271/1000 | Loss: 0.00003795
Iteration 272/1000 | Loss: 0.00003990
Iteration 273/1000 | Loss: 0.00003744
Iteration 274/1000 | Loss: 0.00003598
Iteration 275/1000 | Loss: 0.00003312
Iteration 276/1000 | Loss: 0.00003185
Iteration 277/1000 | Loss: 0.00003136
Iteration 278/1000 | Loss: 0.00003093
Iteration 279/1000 | Loss: 0.00003070
Iteration 280/1000 | Loss: 0.00003069
Iteration 281/1000 | Loss: 0.00003065
Iteration 282/1000 | Loss: 0.00003063
Iteration 283/1000 | Loss: 0.00003063
Iteration 284/1000 | Loss: 0.00003062
Iteration 285/1000 | Loss: 0.00003062
Iteration 286/1000 | Loss: 0.00003062
Iteration 287/1000 | Loss: 0.00003061
Iteration 288/1000 | Loss: 0.00003061
Iteration 289/1000 | Loss: 0.00003061
Iteration 290/1000 | Loss: 0.00003061
Iteration 291/1000 | Loss: 0.00003060
Iteration 292/1000 | Loss: 0.00003060
Iteration 293/1000 | Loss: 0.00003059
Iteration 294/1000 | Loss: 0.00003055
Iteration 295/1000 | Loss: 0.00003055
Iteration 296/1000 | Loss: 0.00003054
Iteration 297/1000 | Loss: 0.00003053
Iteration 298/1000 | Loss: 0.00003053
Iteration 299/1000 | Loss: 0.00003053
Iteration 300/1000 | Loss: 0.00003052
Iteration 301/1000 | Loss: 0.00003052
Iteration 302/1000 | Loss: 0.00003052
Iteration 303/1000 | Loss: 0.00003052
Iteration 304/1000 | Loss: 0.00003052
Iteration 305/1000 | Loss: 0.00003051
Iteration 306/1000 | Loss: 0.00003051
Iteration 307/1000 | Loss: 0.00003051
Iteration 308/1000 | Loss: 0.00003050
Iteration 309/1000 | Loss: 0.00003050
Iteration 310/1000 | Loss: 0.00003050
Iteration 311/1000 | Loss: 0.00003049
Iteration 312/1000 | Loss: 0.00003049
Iteration 313/1000 | Loss: 0.00003048
Iteration 314/1000 | Loss: 0.00003048
Iteration 315/1000 | Loss: 0.00003048
Iteration 316/1000 | Loss: 0.00003047
Iteration 317/1000 | Loss: 0.00003047
Iteration 318/1000 | Loss: 0.00003046
Iteration 319/1000 | Loss: 0.00003045
Iteration 320/1000 | Loss: 0.00003045
Iteration 321/1000 | Loss: 0.00003045
Iteration 322/1000 | Loss: 0.00003044
Iteration 323/1000 | Loss: 0.00003044
Iteration 324/1000 | Loss: 0.00003044
Iteration 325/1000 | Loss: 0.00003043
Iteration 326/1000 | Loss: 0.00003043
Iteration 327/1000 | Loss: 0.00004500
Iteration 328/1000 | Loss: 0.00004499
Iteration 329/1000 | Loss: 0.00004498
Iteration 330/1000 | Loss: 0.00004498
Iteration 331/1000 | Loss: 0.00004380
Iteration 332/1000 | Loss: 0.00003905
Iteration 333/1000 | Loss: 0.00003500
Iteration 334/1000 | Loss: 0.00003272
Iteration 335/1000 | Loss: 0.00003215
Iteration 336/1000 | Loss: 0.00003199
Iteration 337/1000 | Loss: 0.00003181
Iteration 338/1000 | Loss: 0.00003158
Iteration 339/1000 | Loss: 0.00003139
Iteration 340/1000 | Loss: 0.00003137
Iteration 341/1000 | Loss: 0.00003127
Iteration 342/1000 | Loss: 0.00003111
Iteration 343/1000 | Loss: 0.00003093
Iteration 344/1000 | Loss: 0.00003071
Iteration 345/1000 | Loss: 0.00003035
Iteration 346/1000 | Loss: 0.00002992
Iteration 347/1000 | Loss: 0.00002965
Iteration 348/1000 | Loss: 0.00002962
Iteration 349/1000 | Loss: 0.00002960
Iteration 350/1000 | Loss: 0.00002958
Iteration 351/1000 | Loss: 0.00002958
Iteration 352/1000 | Loss: 0.00002956
Iteration 353/1000 | Loss: 0.00002955
Iteration 354/1000 | Loss: 0.00002955
Iteration 355/1000 | Loss: 0.00002953
Iteration 356/1000 | Loss: 0.00002952
Iteration 357/1000 | Loss: 0.00002952
Iteration 358/1000 | Loss: 0.00002952
Iteration 359/1000 | Loss: 0.00002951
Iteration 360/1000 | Loss: 0.00002951
Iteration 361/1000 | Loss: 0.00002951
Iteration 362/1000 | Loss: 0.00002950
Iteration 363/1000 | Loss: 0.00002950
Iteration 364/1000 | Loss: 0.00002950
Iteration 365/1000 | Loss: 0.00002950
Iteration 366/1000 | Loss: 0.00002950
Iteration 367/1000 | Loss: 0.00002950
Iteration 368/1000 | Loss: 0.00002950
Iteration 369/1000 | Loss: 0.00002950
Iteration 370/1000 | Loss: 0.00002950
Iteration 371/1000 | Loss: 0.00002950
Iteration 372/1000 | Loss: 0.00002949
Iteration 373/1000 | Loss: 0.00002949
Iteration 374/1000 | Loss: 0.00002949
Iteration 375/1000 | Loss: 0.00002949
Iteration 376/1000 | Loss: 0.00002949
Iteration 377/1000 | Loss: 0.00002949
Iteration 378/1000 | Loss: 0.00002948
Iteration 379/1000 | Loss: 0.00002948
Iteration 380/1000 | Loss: 0.00002948
Iteration 381/1000 | Loss: 0.00002947
Iteration 382/1000 | Loss: 0.00002947
Iteration 383/1000 | Loss: 0.00002947
Iteration 384/1000 | Loss: 0.00002947
Iteration 385/1000 | Loss: 0.00002947
Iteration 386/1000 | Loss: 0.00002946
Iteration 387/1000 | Loss: 0.00002946
Iteration 388/1000 | Loss: 0.00002946
Iteration 389/1000 | Loss: 0.00002946
Iteration 390/1000 | Loss: 0.00002946
Iteration 391/1000 | Loss: 0.00002946
Iteration 392/1000 | Loss: 0.00002946
Iteration 393/1000 | Loss: 0.00002946
Iteration 394/1000 | Loss: 0.00002946
Iteration 395/1000 | Loss: 0.00002946
Iteration 396/1000 | Loss: 0.00002946
Iteration 397/1000 | Loss: 0.00002946
Iteration 398/1000 | Loss: 0.00002946
Iteration 399/1000 | Loss: 0.00002946
Iteration 400/1000 | Loss: 0.00002946
Iteration 401/1000 | Loss: 0.00002946
Iteration 402/1000 | Loss: 0.00002946
Iteration 403/1000 | Loss: 0.00002946
Iteration 404/1000 | Loss: 0.00002946
Iteration 405/1000 | Loss: 0.00002946
Iteration 406/1000 | Loss: 0.00002946
Iteration 407/1000 | Loss: 0.00002946
Iteration 408/1000 | Loss: 0.00002946
Iteration 409/1000 | Loss: 0.00002946
Iteration 410/1000 | Loss: 0.00002946
Iteration 411/1000 | Loss: 0.00002946
Iteration 412/1000 | Loss: 0.00002946
Iteration 413/1000 | Loss: 0.00002946
Iteration 414/1000 | Loss: 0.00002946
Iteration 415/1000 | Loss: 0.00002946
Iteration 416/1000 | Loss: 0.00002946
Iteration 417/1000 | Loss: 0.00002946
Iteration 418/1000 | Loss: 0.00002946
Iteration 419/1000 | Loss: 0.00002946
Iteration 420/1000 | Loss: 0.00002946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 420. Stopping optimization.
Last 5 losses: [2.946074710052926e-05, 2.946074710052926e-05, 2.946074710052926e-05, 2.946074710052926e-05, 2.946074710052926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.946074710052926e-05

Optimization complete. Final v2v error: 4.300511360168457 mm

Highest mean error: 8.06196117401123 mm for frame 104

Lowest mean error: 3.198866844177246 mm for frame 75

Saving results

Total time: 496.2281334400177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032182
Iteration 2/25 | Loss: 0.00147127
Iteration 3/25 | Loss: 0.00103225
Iteration 4/25 | Loss: 0.00094637
Iteration 5/25 | Loss: 0.00089312
Iteration 6/25 | Loss: 0.00089077
Iteration 7/25 | Loss: 0.00088203
Iteration 8/25 | Loss: 0.00086113
Iteration 9/25 | Loss: 0.00086367
Iteration 10/25 | Loss: 0.00085694
Iteration 11/25 | Loss: 0.00085089
Iteration 12/25 | Loss: 0.00085226
Iteration 13/25 | Loss: 0.00085885
Iteration 14/25 | Loss: 0.00083987
Iteration 15/25 | Loss: 0.00084372
Iteration 16/25 | Loss: 0.00083587
Iteration 17/25 | Loss: 0.00083121
Iteration 18/25 | Loss: 0.00083149
Iteration 19/25 | Loss: 0.00082855
Iteration 20/25 | Loss: 0.00083164
Iteration 21/25 | Loss: 0.00083036
Iteration 22/25 | Loss: 0.00082791
Iteration 23/25 | Loss: 0.00082785
Iteration 24/25 | Loss: 0.00082508
Iteration 25/25 | Loss: 0.00082670

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85244620
Iteration 2/25 | Loss: 0.00154127
Iteration 3/25 | Loss: 0.00154127
Iteration 4/25 | Loss: 0.00154127
Iteration 5/25 | Loss: 0.00154127
Iteration 6/25 | Loss: 0.00154127
Iteration 7/25 | Loss: 0.00154127
Iteration 8/25 | Loss: 0.00154127
Iteration 9/25 | Loss: 0.00154127
Iteration 10/25 | Loss: 0.00154127
Iteration 11/25 | Loss: 0.00154127
Iteration 12/25 | Loss: 0.00154127
Iteration 13/25 | Loss: 0.00154127
Iteration 14/25 | Loss: 0.00154127
Iteration 15/25 | Loss: 0.00154127
Iteration 16/25 | Loss: 0.00154127
Iteration 17/25 | Loss: 0.00154127
Iteration 18/25 | Loss: 0.00154127
Iteration 19/25 | Loss: 0.00154127
Iteration 20/25 | Loss: 0.00154127
Iteration 21/25 | Loss: 0.00154127
Iteration 22/25 | Loss: 0.00154127
Iteration 23/25 | Loss: 0.00154127
Iteration 24/25 | Loss: 0.00154127
Iteration 25/25 | Loss: 0.00154127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154127
Iteration 2/1000 | Loss: 0.00051742
Iteration 3/1000 | Loss: 0.00070392
Iteration 4/1000 | Loss: 0.00059767
Iteration 5/1000 | Loss: 0.00031696
Iteration 6/1000 | Loss: 0.00067965
Iteration 7/1000 | Loss: 0.00026123
Iteration 8/1000 | Loss: 0.00037997
Iteration 9/1000 | Loss: 0.00068286
Iteration 10/1000 | Loss: 0.00185214
Iteration 11/1000 | Loss: 0.00148474
Iteration 12/1000 | Loss: 0.00094220
Iteration 13/1000 | Loss: 0.00091706
Iteration 14/1000 | Loss: 0.00095423
Iteration 15/1000 | Loss: 0.00114401
Iteration 16/1000 | Loss: 0.00089772
Iteration 17/1000 | Loss: 0.00068275
Iteration 18/1000 | Loss: 0.00031482
Iteration 19/1000 | Loss: 0.00038748
Iteration 20/1000 | Loss: 0.00058485
Iteration 21/1000 | Loss: 0.00109106
Iteration 22/1000 | Loss: 0.00162135
Iteration 23/1000 | Loss: 0.00106814
Iteration 24/1000 | Loss: 0.00042400
Iteration 25/1000 | Loss: 0.00022157
Iteration 26/1000 | Loss: 0.00017581
Iteration 27/1000 | Loss: 0.00014470
Iteration 28/1000 | Loss: 0.00008771
Iteration 29/1000 | Loss: 0.00065800
Iteration 30/1000 | Loss: 0.00009550
Iteration 31/1000 | Loss: 0.00079451
Iteration 32/1000 | Loss: 0.00024891
Iteration 33/1000 | Loss: 0.00046473
Iteration 34/1000 | Loss: 0.00008944
Iteration 35/1000 | Loss: 0.00007667
Iteration 36/1000 | Loss: 0.00029457
Iteration 37/1000 | Loss: 0.00038796
Iteration 38/1000 | Loss: 0.00021743
Iteration 39/1000 | Loss: 0.00121671
Iteration 40/1000 | Loss: 0.00016951
Iteration 41/1000 | Loss: 0.00055846
Iteration 42/1000 | Loss: 0.00024557
Iteration 43/1000 | Loss: 0.00028836
Iteration 44/1000 | Loss: 0.00007151
Iteration 45/1000 | Loss: 0.00006505
Iteration 46/1000 | Loss: 0.00066040
Iteration 47/1000 | Loss: 0.00097351
Iteration 48/1000 | Loss: 0.00065107
Iteration 49/1000 | Loss: 0.00031841
Iteration 50/1000 | Loss: 0.00104829
Iteration 51/1000 | Loss: 0.00026053
Iteration 52/1000 | Loss: 0.00006134
Iteration 53/1000 | Loss: 0.00005329
Iteration 54/1000 | Loss: 0.00070169
Iteration 55/1000 | Loss: 0.00007828
Iteration 56/1000 | Loss: 0.00037039
Iteration 57/1000 | Loss: 0.00029550
Iteration 58/1000 | Loss: 0.00030734
Iteration 59/1000 | Loss: 0.00005168
Iteration 60/1000 | Loss: 0.00004408
Iteration 61/1000 | Loss: 0.00004186
Iteration 62/1000 | Loss: 0.00005809
Iteration 63/1000 | Loss: 0.00053908
Iteration 64/1000 | Loss: 0.00041231
Iteration 65/1000 | Loss: 0.00049840
Iteration 66/1000 | Loss: 0.00004383
Iteration 67/1000 | Loss: 0.00003966
Iteration 68/1000 | Loss: 0.00003772
Iteration 69/1000 | Loss: 0.00003624
Iteration 70/1000 | Loss: 0.00003560
Iteration 71/1000 | Loss: 0.00003446
Iteration 72/1000 | Loss: 0.00044807
Iteration 73/1000 | Loss: 0.00027977
Iteration 74/1000 | Loss: 0.00043854
Iteration 75/1000 | Loss: 0.00030187
Iteration 76/1000 | Loss: 0.00027214
Iteration 77/1000 | Loss: 0.00021682
Iteration 78/1000 | Loss: 0.00004885
Iteration 79/1000 | Loss: 0.00003479
Iteration 80/1000 | Loss: 0.00003179
Iteration 81/1000 | Loss: 0.00003015
Iteration 82/1000 | Loss: 0.00002930
Iteration 83/1000 | Loss: 0.00002856
Iteration 84/1000 | Loss: 0.00002817
Iteration 85/1000 | Loss: 0.00002789
Iteration 86/1000 | Loss: 0.00002766
Iteration 87/1000 | Loss: 0.00002763
Iteration 88/1000 | Loss: 0.00002757
Iteration 89/1000 | Loss: 0.00002748
Iteration 90/1000 | Loss: 0.00002732
Iteration 91/1000 | Loss: 0.00002729
Iteration 92/1000 | Loss: 0.00002729
Iteration 93/1000 | Loss: 0.00002728
Iteration 94/1000 | Loss: 0.00002727
Iteration 95/1000 | Loss: 0.00002727
Iteration 96/1000 | Loss: 0.00002726
Iteration 97/1000 | Loss: 0.00002725
Iteration 98/1000 | Loss: 0.00002725
Iteration 99/1000 | Loss: 0.00002722
Iteration 100/1000 | Loss: 0.00002722
Iteration 101/1000 | Loss: 0.00002720
Iteration 102/1000 | Loss: 0.00002720
Iteration 103/1000 | Loss: 0.00002720
Iteration 104/1000 | Loss: 0.00002719
Iteration 105/1000 | Loss: 0.00002719
Iteration 106/1000 | Loss: 0.00002719
Iteration 107/1000 | Loss: 0.00002719
Iteration 108/1000 | Loss: 0.00002719
Iteration 109/1000 | Loss: 0.00002719
Iteration 110/1000 | Loss: 0.00002719
Iteration 111/1000 | Loss: 0.00002719
Iteration 112/1000 | Loss: 0.00002719
Iteration 113/1000 | Loss: 0.00002718
Iteration 114/1000 | Loss: 0.00002718
Iteration 115/1000 | Loss: 0.00002718
Iteration 116/1000 | Loss: 0.00002718
Iteration 117/1000 | Loss: 0.00002717
Iteration 118/1000 | Loss: 0.00002717
Iteration 119/1000 | Loss: 0.00002717
Iteration 120/1000 | Loss: 0.00002717
Iteration 121/1000 | Loss: 0.00002717
Iteration 122/1000 | Loss: 0.00002717
Iteration 123/1000 | Loss: 0.00002717
Iteration 124/1000 | Loss: 0.00002716
Iteration 125/1000 | Loss: 0.00002716
Iteration 126/1000 | Loss: 0.00002716
Iteration 127/1000 | Loss: 0.00002716
Iteration 128/1000 | Loss: 0.00002716
Iteration 129/1000 | Loss: 0.00002716
Iteration 130/1000 | Loss: 0.00002715
Iteration 131/1000 | Loss: 0.00002715
Iteration 132/1000 | Loss: 0.00002715
Iteration 133/1000 | Loss: 0.00002715
Iteration 134/1000 | Loss: 0.00002714
Iteration 135/1000 | Loss: 0.00002714
Iteration 136/1000 | Loss: 0.00002714
Iteration 137/1000 | Loss: 0.00002714
Iteration 138/1000 | Loss: 0.00002713
Iteration 139/1000 | Loss: 0.00002713
Iteration 140/1000 | Loss: 0.00002713
Iteration 141/1000 | Loss: 0.00002713
Iteration 142/1000 | Loss: 0.00002712
Iteration 143/1000 | Loss: 0.00002712
Iteration 144/1000 | Loss: 0.00002712
Iteration 145/1000 | Loss: 0.00002712
Iteration 146/1000 | Loss: 0.00002712
Iteration 147/1000 | Loss: 0.00002712
Iteration 148/1000 | Loss: 0.00002712
Iteration 149/1000 | Loss: 0.00002712
Iteration 150/1000 | Loss: 0.00002712
Iteration 151/1000 | Loss: 0.00002712
Iteration 152/1000 | Loss: 0.00002711
Iteration 153/1000 | Loss: 0.00002711
Iteration 154/1000 | Loss: 0.00002711
Iteration 155/1000 | Loss: 0.00002711
Iteration 156/1000 | Loss: 0.00002711
Iteration 157/1000 | Loss: 0.00002710
Iteration 158/1000 | Loss: 0.00002710
Iteration 159/1000 | Loss: 0.00002710
Iteration 160/1000 | Loss: 0.00002710
Iteration 161/1000 | Loss: 0.00002710
Iteration 162/1000 | Loss: 0.00002710
Iteration 163/1000 | Loss: 0.00002710
Iteration 164/1000 | Loss: 0.00002710
Iteration 165/1000 | Loss: 0.00002710
Iteration 166/1000 | Loss: 0.00002710
Iteration 167/1000 | Loss: 0.00002709
Iteration 168/1000 | Loss: 0.00002709
Iteration 169/1000 | Loss: 0.00002709
Iteration 170/1000 | Loss: 0.00002709
Iteration 171/1000 | Loss: 0.00002709
Iteration 172/1000 | Loss: 0.00002709
Iteration 173/1000 | Loss: 0.00002709
Iteration 174/1000 | Loss: 0.00002709
Iteration 175/1000 | Loss: 0.00002709
Iteration 176/1000 | Loss: 0.00002709
Iteration 177/1000 | Loss: 0.00002709
Iteration 178/1000 | Loss: 0.00002709
Iteration 179/1000 | Loss: 0.00002709
Iteration 180/1000 | Loss: 0.00002709
Iteration 181/1000 | Loss: 0.00002709
Iteration 182/1000 | Loss: 0.00002709
Iteration 183/1000 | Loss: 0.00002708
Iteration 184/1000 | Loss: 0.00002708
Iteration 185/1000 | Loss: 0.00002708
Iteration 186/1000 | Loss: 0.00002708
Iteration 187/1000 | Loss: 0.00002708
Iteration 188/1000 | Loss: 0.00002708
Iteration 189/1000 | Loss: 0.00002708
Iteration 190/1000 | Loss: 0.00002708
Iteration 191/1000 | Loss: 0.00002708
Iteration 192/1000 | Loss: 0.00002708
Iteration 193/1000 | Loss: 0.00002708
Iteration 194/1000 | Loss: 0.00002708
Iteration 195/1000 | Loss: 0.00002708
Iteration 196/1000 | Loss: 0.00002708
Iteration 197/1000 | Loss: 0.00002708
Iteration 198/1000 | Loss: 0.00002707
Iteration 199/1000 | Loss: 0.00002707
Iteration 200/1000 | Loss: 0.00002707
Iteration 201/1000 | Loss: 0.00002707
Iteration 202/1000 | Loss: 0.00002707
Iteration 203/1000 | Loss: 0.00002707
Iteration 204/1000 | Loss: 0.00002707
Iteration 205/1000 | Loss: 0.00002707
Iteration 206/1000 | Loss: 0.00002707
Iteration 207/1000 | Loss: 0.00002707
Iteration 208/1000 | Loss: 0.00002707
Iteration 209/1000 | Loss: 0.00002706
Iteration 210/1000 | Loss: 0.00002706
Iteration 211/1000 | Loss: 0.00002706
Iteration 212/1000 | Loss: 0.00002706
Iteration 213/1000 | Loss: 0.00002706
Iteration 214/1000 | Loss: 0.00002706
Iteration 215/1000 | Loss: 0.00002706
Iteration 216/1000 | Loss: 0.00002706
Iteration 217/1000 | Loss: 0.00002706
Iteration 218/1000 | Loss: 0.00002706
Iteration 219/1000 | Loss: 0.00002706
Iteration 220/1000 | Loss: 0.00002706
Iteration 221/1000 | Loss: 0.00002706
Iteration 222/1000 | Loss: 0.00002706
Iteration 223/1000 | Loss: 0.00002706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [2.7056066755903885e-05, 2.7056066755903885e-05, 2.7056066755903885e-05, 2.7056066755903885e-05, 2.7056066755903885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7056066755903885e-05

Optimization complete. Final v2v error: 4.351917266845703 mm

Highest mean error: 6.047698497772217 mm for frame 29

Lowest mean error: 3.7048656940460205 mm for frame 186

Saving results

Total time: 206.83124828338623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00942138
Iteration 2/25 | Loss: 0.00164899
Iteration 3/25 | Loss: 0.00116346
Iteration 4/25 | Loss: 0.00102788
Iteration 5/25 | Loss: 0.00097816
Iteration 6/25 | Loss: 0.00095022
Iteration 7/25 | Loss: 0.00093148
Iteration 8/25 | Loss: 0.00088339
Iteration 9/25 | Loss: 0.00085226
Iteration 10/25 | Loss: 0.00083877
Iteration 11/25 | Loss: 0.00082773
Iteration 12/25 | Loss: 0.00081924
Iteration 13/25 | Loss: 0.00081920
Iteration 14/25 | Loss: 0.00081246
Iteration 15/25 | Loss: 0.00081375
Iteration 16/25 | Loss: 0.00081199
Iteration 17/25 | Loss: 0.00081354
Iteration 18/25 | Loss: 0.00081046
Iteration 19/25 | Loss: 0.00081294
Iteration 20/25 | Loss: 0.00081421
Iteration 21/25 | Loss: 0.00080961
Iteration 22/25 | Loss: 0.00080565
Iteration 23/25 | Loss: 0.00080362
Iteration 24/25 | Loss: 0.00080514
Iteration 25/25 | Loss: 0.00079823

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.36075401
Iteration 2/25 | Loss: 0.00033784
Iteration 3/25 | Loss: 0.00033762
Iteration 4/25 | Loss: 0.00033761
Iteration 5/25 | Loss: 0.00033761
Iteration 6/25 | Loss: 0.00033761
Iteration 7/25 | Loss: 0.00033761
Iteration 8/25 | Loss: 0.00033761
Iteration 9/25 | Loss: 0.00033761
Iteration 10/25 | Loss: 0.00033761
Iteration 11/25 | Loss: 0.00033761
Iteration 12/25 | Loss: 0.00033761
Iteration 13/25 | Loss: 0.00033761
Iteration 14/25 | Loss: 0.00033761
Iteration 15/25 | Loss: 0.00033761
Iteration 16/25 | Loss: 0.00033761
Iteration 17/25 | Loss: 0.00033761
Iteration 18/25 | Loss: 0.00033761
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003376122622285038, 0.0003376122622285038, 0.0003376122622285038, 0.0003376122622285038, 0.0003376122622285038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003376122622285038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033761
Iteration 2/1000 | Loss: 0.00062091
Iteration 3/1000 | Loss: 0.00034120
Iteration 4/1000 | Loss: 0.00029074
Iteration 5/1000 | Loss: 0.00020573
Iteration 6/1000 | Loss: 0.00013150
Iteration 7/1000 | Loss: 0.00010809
Iteration 8/1000 | Loss: 0.00026600
Iteration 9/1000 | Loss: 0.00019325
Iteration 10/1000 | Loss: 0.00025723
Iteration 11/1000 | Loss: 0.00015322
Iteration 12/1000 | Loss: 0.00022030
Iteration 13/1000 | Loss: 0.00017439
Iteration 14/1000 | Loss: 0.00014320
Iteration 15/1000 | Loss: 0.00033537
Iteration 16/1000 | Loss: 0.00019117
Iteration 17/1000 | Loss: 0.00017183
Iteration 18/1000 | Loss: 0.00012508
Iteration 19/1000 | Loss: 0.00015177
Iteration 20/1000 | Loss: 0.00019137
Iteration 21/1000 | Loss: 0.00010233
Iteration 22/1000 | Loss: 0.00009240
Iteration 23/1000 | Loss: 0.00008383
Iteration 24/1000 | Loss: 0.00007637
Iteration 25/1000 | Loss: 0.00006967
Iteration 26/1000 | Loss: 0.00008178
Iteration 27/1000 | Loss: 0.00008877
Iteration 28/1000 | Loss: 0.00007691
Iteration 29/1000 | Loss: 0.00007926
Iteration 30/1000 | Loss: 0.00007258
Iteration 31/1000 | Loss: 0.00007370
Iteration 32/1000 | Loss: 0.00009101
Iteration 33/1000 | Loss: 0.00006273
Iteration 34/1000 | Loss: 0.00005904
Iteration 35/1000 | Loss: 0.00005580
Iteration 36/1000 | Loss: 0.00005787
Iteration 37/1000 | Loss: 0.00009355
Iteration 38/1000 | Loss: 0.00005853
Iteration 39/1000 | Loss: 0.00007702
Iteration 40/1000 | Loss: 0.00007117
Iteration 41/1000 | Loss: 0.00007163
Iteration 42/1000 | Loss: 0.00007138
Iteration 43/1000 | Loss: 0.00006932
Iteration 44/1000 | Loss: 0.00010533
Iteration 45/1000 | Loss: 0.00009647
Iteration 46/1000 | Loss: 0.00008054
Iteration 47/1000 | Loss: 0.00006656
Iteration 48/1000 | Loss: 0.00006234
Iteration 49/1000 | Loss: 0.00005270
Iteration 50/1000 | Loss: 0.00006329
Iteration 51/1000 | Loss: 0.00006807
Iteration 52/1000 | Loss: 0.00008530
Iteration 53/1000 | Loss: 0.00007355
Iteration 54/1000 | Loss: 0.00007352
Iteration 55/1000 | Loss: 0.00006506
Iteration 56/1000 | Loss: 0.00007431
Iteration 57/1000 | Loss: 0.00006502
Iteration 58/1000 | Loss: 0.00005415
Iteration 59/1000 | Loss: 0.00005373
Iteration 60/1000 | Loss: 0.00006436
Iteration 61/1000 | Loss: 0.00006902
Iteration 62/1000 | Loss: 0.00006374
Iteration 63/1000 | Loss: 0.00005927
Iteration 64/1000 | Loss: 0.00005988
Iteration 65/1000 | Loss: 0.00006328
Iteration 66/1000 | Loss: 0.00005737
Iteration 67/1000 | Loss: 0.00005116
Iteration 68/1000 | Loss: 0.00004670
Iteration 69/1000 | Loss: 0.00006102
Iteration 70/1000 | Loss: 0.00004891
Iteration 71/1000 | Loss: 0.00005799
Iteration 72/1000 | Loss: 0.00005162
Iteration 73/1000 | Loss: 0.00006065
Iteration 74/1000 | Loss: 0.00006796
Iteration 75/1000 | Loss: 0.00006689
Iteration 76/1000 | Loss: 0.00006126
Iteration 77/1000 | Loss: 0.00006580
Iteration 78/1000 | Loss: 0.00005957
Iteration 79/1000 | Loss: 0.00006682
Iteration 80/1000 | Loss: 0.00005751
Iteration 81/1000 | Loss: 0.00006635
Iteration 82/1000 | Loss: 0.00006210
Iteration 83/1000 | Loss: 0.00006753
Iteration 84/1000 | Loss: 0.00005998
Iteration 85/1000 | Loss: 0.00005153
Iteration 86/1000 | Loss: 0.00006811
Iteration 87/1000 | Loss: 0.00006222
Iteration 88/1000 | Loss: 0.00004790
Iteration 89/1000 | Loss: 0.00006653
Iteration 90/1000 | Loss: 0.00006376
Iteration 91/1000 | Loss: 0.00007301
Iteration 92/1000 | Loss: 0.00007443
Iteration 93/1000 | Loss: 0.00005701
Iteration 94/1000 | Loss: 0.00006323
Iteration 95/1000 | Loss: 0.00004620
Iteration 96/1000 | Loss: 0.00005287
Iteration 97/1000 | Loss: 0.00005545
Iteration 98/1000 | Loss: 0.00004947
Iteration 99/1000 | Loss: 0.00005603
Iteration 100/1000 | Loss: 0.00004166
Iteration 101/1000 | Loss: 0.00005465
Iteration 102/1000 | Loss: 0.00005697
Iteration 103/1000 | Loss: 0.00005914
Iteration 104/1000 | Loss: 0.00005540
Iteration 105/1000 | Loss: 0.00005582
Iteration 106/1000 | Loss: 0.00005152
Iteration 107/1000 | Loss: 0.00008594
Iteration 108/1000 | Loss: 0.00007736
Iteration 109/1000 | Loss: 0.00005236
Iteration 110/1000 | Loss: 0.00005357
Iteration 111/1000 | Loss: 0.00005081
Iteration 112/1000 | Loss: 0.00005298
Iteration 113/1000 | Loss: 0.00004869
Iteration 114/1000 | Loss: 0.00005691
Iteration 115/1000 | Loss: 0.00005174
Iteration 116/1000 | Loss: 0.00005155
Iteration 117/1000 | Loss: 0.00004178
Iteration 118/1000 | Loss: 0.00005091
Iteration 119/1000 | Loss: 0.00004547
Iteration 120/1000 | Loss: 0.00004470
Iteration 121/1000 | Loss: 0.00004110
Iteration 122/1000 | Loss: 0.00003988
Iteration 123/1000 | Loss: 0.00007734
Iteration 124/1000 | Loss: 0.00007085
Iteration 125/1000 | Loss: 0.00004122
Iteration 126/1000 | Loss: 0.00005864
Iteration 127/1000 | Loss: 0.00004965
Iteration 128/1000 | Loss: 0.00005075
Iteration 129/1000 | Loss: 0.00004438
Iteration 130/1000 | Loss: 0.00004996
Iteration 131/1000 | Loss: 0.00005030
Iteration 132/1000 | Loss: 0.00005130
Iteration 133/1000 | Loss: 0.00005313
Iteration 134/1000 | Loss: 0.00004041
Iteration 135/1000 | Loss: 0.00005659
Iteration 136/1000 | Loss: 0.00004037
Iteration 137/1000 | Loss: 0.00003752
Iteration 138/1000 | Loss: 0.00003571
Iteration 139/1000 | Loss: 0.00003509
Iteration 140/1000 | Loss: 0.00003469
Iteration 141/1000 | Loss: 0.00003415
Iteration 142/1000 | Loss: 0.00003369
Iteration 143/1000 | Loss: 0.00006730
Iteration 144/1000 | Loss: 0.00004578
Iteration 145/1000 | Loss: 0.00003923
Iteration 146/1000 | Loss: 0.00006337
Iteration 147/1000 | Loss: 0.00006438
Iteration 148/1000 | Loss: 0.00006513
Iteration 149/1000 | Loss: 0.00006418
Iteration 150/1000 | Loss: 0.00006241
Iteration 151/1000 | Loss: 0.00006396
Iteration 152/1000 | Loss: 0.00005841
Iteration 153/1000 | Loss: 0.00006436
Iteration 154/1000 | Loss: 0.00005528
Iteration 155/1000 | Loss: 0.00006446
Iteration 156/1000 | Loss: 0.00005750
Iteration 157/1000 | Loss: 0.00006426
Iteration 158/1000 | Loss: 0.00006341
Iteration 159/1000 | Loss: 0.00003784
Iteration 160/1000 | Loss: 0.00003601
Iteration 161/1000 | Loss: 0.00003506
Iteration 162/1000 | Loss: 0.00003437
Iteration 163/1000 | Loss: 0.00003339
Iteration 164/1000 | Loss: 0.00003283
Iteration 165/1000 | Loss: 0.00003255
Iteration 166/1000 | Loss: 0.00003229
Iteration 167/1000 | Loss: 0.00003213
Iteration 168/1000 | Loss: 0.00003210
Iteration 169/1000 | Loss: 0.00003210
Iteration 170/1000 | Loss: 0.00003209
Iteration 171/1000 | Loss: 0.00003208
Iteration 172/1000 | Loss: 0.00003205
Iteration 173/1000 | Loss: 0.00003203
Iteration 174/1000 | Loss: 0.00003202
Iteration 175/1000 | Loss: 0.00003198
Iteration 176/1000 | Loss: 0.00003192
Iteration 177/1000 | Loss: 0.00003176
Iteration 178/1000 | Loss: 0.00003156
Iteration 179/1000 | Loss: 0.00003147
Iteration 180/1000 | Loss: 0.00003147
Iteration 181/1000 | Loss: 0.00003147
Iteration 182/1000 | Loss: 0.00003147
Iteration 183/1000 | Loss: 0.00003145
Iteration 184/1000 | Loss: 0.00003145
Iteration 185/1000 | Loss: 0.00003144
Iteration 186/1000 | Loss: 0.00003144
Iteration 187/1000 | Loss: 0.00003144
Iteration 188/1000 | Loss: 0.00003143
Iteration 189/1000 | Loss: 0.00003143
Iteration 190/1000 | Loss: 0.00003142
Iteration 191/1000 | Loss: 0.00003142
Iteration 192/1000 | Loss: 0.00003142
Iteration 193/1000 | Loss: 0.00003142
Iteration 194/1000 | Loss: 0.00003142
Iteration 195/1000 | Loss: 0.00003142
Iteration 196/1000 | Loss: 0.00003141
Iteration 197/1000 | Loss: 0.00003141
Iteration 198/1000 | Loss: 0.00003141
Iteration 199/1000 | Loss: 0.00003140
Iteration 200/1000 | Loss: 0.00003140
Iteration 201/1000 | Loss: 0.00003140
Iteration 202/1000 | Loss: 0.00003139
Iteration 203/1000 | Loss: 0.00003139
Iteration 204/1000 | Loss: 0.00003139
Iteration 205/1000 | Loss: 0.00003138
Iteration 206/1000 | Loss: 0.00003138
Iteration 207/1000 | Loss: 0.00003137
Iteration 208/1000 | Loss: 0.00003137
Iteration 209/1000 | Loss: 0.00003137
Iteration 210/1000 | Loss: 0.00003137
Iteration 211/1000 | Loss: 0.00003136
Iteration 212/1000 | Loss: 0.00003136
Iteration 213/1000 | Loss: 0.00003136
Iteration 214/1000 | Loss: 0.00003135
Iteration 215/1000 | Loss: 0.00003135
Iteration 216/1000 | Loss: 0.00003135
Iteration 217/1000 | Loss: 0.00003134
Iteration 218/1000 | Loss: 0.00003134
Iteration 219/1000 | Loss: 0.00003134
Iteration 220/1000 | Loss: 0.00003134
Iteration 221/1000 | Loss: 0.00003133
Iteration 222/1000 | Loss: 0.00003133
Iteration 223/1000 | Loss: 0.00003133
Iteration 224/1000 | Loss: 0.00003133
Iteration 225/1000 | Loss: 0.00003133
Iteration 226/1000 | Loss: 0.00003133
Iteration 227/1000 | Loss: 0.00003133
Iteration 228/1000 | Loss: 0.00003132
Iteration 229/1000 | Loss: 0.00003132
Iteration 230/1000 | Loss: 0.00003132
Iteration 231/1000 | Loss: 0.00003132
Iteration 232/1000 | Loss: 0.00003132
Iteration 233/1000 | Loss: 0.00003132
Iteration 234/1000 | Loss: 0.00003132
Iteration 235/1000 | Loss: 0.00003132
Iteration 236/1000 | Loss: 0.00003132
Iteration 237/1000 | Loss: 0.00003132
Iteration 238/1000 | Loss: 0.00003131
Iteration 239/1000 | Loss: 0.00003131
Iteration 240/1000 | Loss: 0.00003131
Iteration 241/1000 | Loss: 0.00003131
Iteration 242/1000 | Loss: 0.00003131
Iteration 243/1000 | Loss: 0.00003131
Iteration 244/1000 | Loss: 0.00003131
Iteration 245/1000 | Loss: 0.00003131
Iteration 246/1000 | Loss: 0.00003131
Iteration 247/1000 | Loss: 0.00003131
Iteration 248/1000 | Loss: 0.00003131
Iteration 249/1000 | Loss: 0.00003131
Iteration 250/1000 | Loss: 0.00003131
Iteration 251/1000 | Loss: 0.00003130
Iteration 252/1000 | Loss: 0.00003130
Iteration 253/1000 | Loss: 0.00003130
Iteration 254/1000 | Loss: 0.00003130
Iteration 255/1000 | Loss: 0.00003130
Iteration 256/1000 | Loss: 0.00003130
Iteration 257/1000 | Loss: 0.00003130
Iteration 258/1000 | Loss: 0.00003130
Iteration 259/1000 | Loss: 0.00003130
Iteration 260/1000 | Loss: 0.00003130
Iteration 261/1000 | Loss: 0.00003129
Iteration 262/1000 | Loss: 0.00003129
Iteration 263/1000 | Loss: 0.00003129
Iteration 264/1000 | Loss: 0.00003129
Iteration 265/1000 | Loss: 0.00003129
Iteration 266/1000 | Loss: 0.00003129
Iteration 267/1000 | Loss: 0.00003129
Iteration 268/1000 | Loss: 0.00003129
Iteration 269/1000 | Loss: 0.00003129
Iteration 270/1000 | Loss: 0.00003129
Iteration 271/1000 | Loss: 0.00003129
Iteration 272/1000 | Loss: 0.00003129
Iteration 273/1000 | Loss: 0.00003129
Iteration 274/1000 | Loss: 0.00003129
Iteration 275/1000 | Loss: 0.00003129
Iteration 276/1000 | Loss: 0.00003129
Iteration 277/1000 | Loss: 0.00003129
Iteration 278/1000 | Loss: 0.00003129
Iteration 279/1000 | Loss: 0.00003129
Iteration 280/1000 | Loss: 0.00003129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 280. Stopping optimization.
Last 5 losses: [3.12891679641325e-05, 3.12891679641325e-05, 3.12891679641325e-05, 3.12891679641325e-05, 3.12891679641325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.12891679641325e-05

Optimization complete. Final v2v error: 4.361778259277344 mm

Highest mean error: 8.250134468078613 mm for frame 118

Lowest mean error: 3.2005860805511475 mm for frame 75

Saving results

Total time: 307.85108733177185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073446
Iteration 2/25 | Loss: 0.01073446
Iteration 3/25 | Loss: 0.01073446
Iteration 4/25 | Loss: 0.01073445
Iteration 5/25 | Loss: 0.01073445
Iteration 6/25 | Loss: 0.01073445
Iteration 7/25 | Loss: 0.01073445
Iteration 8/25 | Loss: 0.01073445
Iteration 9/25 | Loss: 0.01073444
Iteration 10/25 | Loss: 0.01073444
Iteration 11/25 | Loss: 0.01073444
Iteration 12/25 | Loss: 0.01073444
Iteration 13/25 | Loss: 0.01073444
Iteration 14/25 | Loss: 0.01073443
Iteration 15/25 | Loss: 0.01073443
Iteration 16/25 | Loss: 0.01073443
Iteration 17/25 | Loss: 0.01073443
Iteration 18/25 | Loss: 0.01073443
Iteration 19/25 | Loss: 0.01073442
Iteration 20/25 | Loss: 0.01073442
Iteration 21/25 | Loss: 0.01073442
Iteration 22/25 | Loss: 0.01073442
Iteration 23/25 | Loss: 0.01073442
Iteration 24/25 | Loss: 0.01073441
Iteration 25/25 | Loss: 0.01073441

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06033278
Iteration 2/25 | Loss: 0.17689747
Iteration 3/25 | Loss: 0.17254592
Iteration 4/25 | Loss: 0.17263435
Iteration 5/25 | Loss: 0.17131875
Iteration 6/25 | Loss: 0.16863918
Iteration 7/25 | Loss: 0.16863866
Iteration 8/25 | Loss: 0.16863857
Iteration 9/25 | Loss: 0.16863845
Iteration 10/25 | Loss: 0.16863975
Iteration 11/25 | Loss: 0.16863842
Iteration 12/25 | Loss: 0.16863838
Iteration 13/25 | Loss: 0.16863838
Iteration 14/25 | Loss: 0.16863838
Iteration 15/25 | Loss: 0.16863838
Iteration 16/25 | Loss: 0.16863838
Iteration 17/25 | Loss: 0.16863836
Iteration 18/25 | Loss: 0.16863836
Iteration 19/25 | Loss: 0.16863836
Iteration 20/25 | Loss: 0.16863836
Iteration 21/25 | Loss: 0.16863836
Iteration 22/25 | Loss: 0.16863836
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.16863836348056793, 0.16863836348056793, 0.16863836348056793, 0.16863836348056793, 0.16863836348056793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.16863836348056793

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16863836
Iteration 2/1000 | Loss: 0.00652962
Iteration 3/1000 | Loss: 0.00530910
Iteration 4/1000 | Loss: 0.00507709
Iteration 5/1000 | Loss: 0.00458678
Iteration 6/1000 | Loss: 0.00135812
Iteration 7/1000 | Loss: 0.00264314
Iteration 8/1000 | Loss: 0.00091098
Iteration 9/1000 | Loss: 0.00082980
Iteration 10/1000 | Loss: 0.00093526
Iteration 11/1000 | Loss: 0.00013338
Iteration 12/1000 | Loss: 0.00016744
Iteration 13/1000 | Loss: 0.00093179
Iteration 14/1000 | Loss: 0.00009815
Iteration 15/1000 | Loss: 0.00012902
Iteration 16/1000 | Loss: 0.00021178
Iteration 17/1000 | Loss: 0.00405055
Iteration 18/1000 | Loss: 0.00038693
Iteration 19/1000 | Loss: 0.00010538
Iteration 20/1000 | Loss: 0.00004721
Iteration 21/1000 | Loss: 0.00013845
Iteration 22/1000 | Loss: 0.00003858
Iteration 23/1000 | Loss: 0.00023285
Iteration 24/1000 | Loss: 0.00008626
Iteration 25/1000 | Loss: 0.00003479
Iteration 26/1000 | Loss: 0.00013356
Iteration 27/1000 | Loss: 0.00010285
Iteration 28/1000 | Loss: 0.00003212
Iteration 29/1000 | Loss: 0.00009747
Iteration 30/1000 | Loss: 0.00015378
Iteration 31/1000 | Loss: 0.00003348
Iteration 32/1000 | Loss: 0.00003096
Iteration 33/1000 | Loss: 0.00002890
Iteration 34/1000 | Loss: 0.00024013
Iteration 35/1000 | Loss: 0.00007854
Iteration 36/1000 | Loss: 0.00019452
Iteration 37/1000 | Loss: 0.00019708
Iteration 38/1000 | Loss: 0.00013918
Iteration 39/1000 | Loss: 0.00100646
Iteration 40/1000 | Loss: 0.00021679
Iteration 41/1000 | Loss: 0.00005490
Iteration 42/1000 | Loss: 0.00027752
Iteration 43/1000 | Loss: 0.00037333
Iteration 44/1000 | Loss: 0.00008596
Iteration 45/1000 | Loss: 0.00002729
Iteration 46/1000 | Loss: 0.00002895
Iteration 47/1000 | Loss: 0.00002619
Iteration 48/1000 | Loss: 0.00002576
Iteration 49/1000 | Loss: 0.00002533
Iteration 50/1000 | Loss: 0.00005082
Iteration 51/1000 | Loss: 0.00017813
Iteration 52/1000 | Loss: 0.00002499
Iteration 53/1000 | Loss: 0.00015144
Iteration 54/1000 | Loss: 0.00002601
Iteration 55/1000 | Loss: 0.00002423
Iteration 56/1000 | Loss: 0.00002395
Iteration 57/1000 | Loss: 0.00002383
Iteration 58/1000 | Loss: 0.00002376
Iteration 59/1000 | Loss: 0.00002375
Iteration 60/1000 | Loss: 0.00002366
Iteration 61/1000 | Loss: 0.00002365
Iteration 62/1000 | Loss: 0.00002411
Iteration 63/1000 | Loss: 0.00002355
Iteration 64/1000 | Loss: 0.00002354
Iteration 65/1000 | Loss: 0.00002354
Iteration 66/1000 | Loss: 0.00002354
Iteration 67/1000 | Loss: 0.00002354
Iteration 68/1000 | Loss: 0.00002354
Iteration 69/1000 | Loss: 0.00002353
Iteration 70/1000 | Loss: 0.00002353
Iteration 71/1000 | Loss: 0.00002353
Iteration 72/1000 | Loss: 0.00002353
Iteration 73/1000 | Loss: 0.00002353
Iteration 74/1000 | Loss: 0.00002353
Iteration 75/1000 | Loss: 0.00002353
Iteration 76/1000 | Loss: 0.00002353
Iteration 77/1000 | Loss: 0.00002353
Iteration 78/1000 | Loss: 0.00002353
Iteration 79/1000 | Loss: 0.00002353
Iteration 80/1000 | Loss: 0.00002353
Iteration 81/1000 | Loss: 0.00002352
Iteration 82/1000 | Loss: 0.00002352
Iteration 83/1000 | Loss: 0.00002352
Iteration 84/1000 | Loss: 0.00002351
Iteration 85/1000 | Loss: 0.00002828
Iteration 86/1000 | Loss: 0.00002347
Iteration 87/1000 | Loss: 0.00002345
Iteration 88/1000 | Loss: 0.00002344
Iteration 89/1000 | Loss: 0.00002344
Iteration 90/1000 | Loss: 0.00002344
Iteration 91/1000 | Loss: 0.00002344
Iteration 92/1000 | Loss: 0.00002344
Iteration 93/1000 | Loss: 0.00002344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.3444930775440298e-05, 2.3444930775440298e-05, 2.3444930775440298e-05, 2.3444930775440298e-05, 2.3444930775440298e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3444930775440298e-05

Optimization complete. Final v2v error: 3.933408737182617 mm

Highest mean error: 10.078071594238281 mm for frame 51

Lowest mean error: 2.9376890659332275 mm for frame 37

Saving results

Total time: 114.8524398803711
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485984
Iteration 2/25 | Loss: 0.00104789
Iteration 3/25 | Loss: 0.00071229
Iteration 4/25 | Loss: 0.00069797
Iteration 5/25 | Loss: 0.00069256
Iteration 6/25 | Loss: 0.00069132
Iteration 7/25 | Loss: 0.00069132
Iteration 8/25 | Loss: 0.00069132
Iteration 9/25 | Loss: 0.00069132
Iteration 10/25 | Loss: 0.00069132
Iteration 11/25 | Loss: 0.00069132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000691319874022156, 0.000691319874022156, 0.000691319874022156, 0.000691319874022156, 0.000691319874022156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000691319874022156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45365775
Iteration 2/25 | Loss: 0.00032276
Iteration 3/25 | Loss: 0.00032276
Iteration 4/25 | Loss: 0.00032276
Iteration 5/25 | Loss: 0.00032276
Iteration 6/25 | Loss: 0.00032276
Iteration 7/25 | Loss: 0.00032276
Iteration 8/25 | Loss: 0.00032276
Iteration 9/25 | Loss: 0.00032276
Iteration 10/25 | Loss: 0.00032276
Iteration 11/25 | Loss: 0.00032276
Iteration 12/25 | Loss: 0.00032276
Iteration 13/25 | Loss: 0.00032276
Iteration 14/25 | Loss: 0.00032276
Iteration 15/25 | Loss: 0.00032276
Iteration 16/25 | Loss: 0.00032276
Iteration 17/25 | Loss: 0.00032276
Iteration 18/25 | Loss: 0.00032276
Iteration 19/25 | Loss: 0.00032276
Iteration 20/25 | Loss: 0.00032276
Iteration 21/25 | Loss: 0.00032276
Iteration 22/25 | Loss: 0.00032276
Iteration 23/25 | Loss: 0.00032276
Iteration 24/25 | Loss: 0.00032276
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000322760985000059, 0.000322760985000059, 0.000322760985000059, 0.000322760985000059, 0.000322760985000059]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000322760985000059

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032276
Iteration 2/1000 | Loss: 0.00003194
Iteration 3/1000 | Loss: 0.00002200
Iteration 4/1000 | Loss: 0.00002061
Iteration 5/1000 | Loss: 0.00001985
Iteration 6/1000 | Loss: 0.00001924
Iteration 7/1000 | Loss: 0.00001883
Iteration 8/1000 | Loss: 0.00001862
Iteration 9/1000 | Loss: 0.00001845
Iteration 10/1000 | Loss: 0.00001843
Iteration 11/1000 | Loss: 0.00001830
Iteration 12/1000 | Loss: 0.00001827
Iteration 13/1000 | Loss: 0.00001823
Iteration 14/1000 | Loss: 0.00001821
Iteration 15/1000 | Loss: 0.00001821
Iteration 16/1000 | Loss: 0.00001821
Iteration 17/1000 | Loss: 0.00001820
Iteration 18/1000 | Loss: 0.00001820
Iteration 19/1000 | Loss: 0.00001818
Iteration 20/1000 | Loss: 0.00001818
Iteration 21/1000 | Loss: 0.00001818
Iteration 22/1000 | Loss: 0.00001817
Iteration 23/1000 | Loss: 0.00001817
Iteration 24/1000 | Loss: 0.00001817
Iteration 25/1000 | Loss: 0.00001817
Iteration 26/1000 | Loss: 0.00001817
Iteration 27/1000 | Loss: 0.00001817
Iteration 28/1000 | Loss: 0.00001817
Iteration 29/1000 | Loss: 0.00001817
Iteration 30/1000 | Loss: 0.00001817
Iteration 31/1000 | Loss: 0.00001817
Iteration 32/1000 | Loss: 0.00001816
Iteration 33/1000 | Loss: 0.00001816
Iteration 34/1000 | Loss: 0.00001816
Iteration 35/1000 | Loss: 0.00001816
Iteration 36/1000 | Loss: 0.00001815
Iteration 37/1000 | Loss: 0.00001815
Iteration 38/1000 | Loss: 0.00001815
Iteration 39/1000 | Loss: 0.00001815
Iteration 40/1000 | Loss: 0.00001815
Iteration 41/1000 | Loss: 0.00001815
Iteration 42/1000 | Loss: 0.00001815
Iteration 43/1000 | Loss: 0.00001815
Iteration 44/1000 | Loss: 0.00001815
Iteration 45/1000 | Loss: 0.00001815
Iteration 46/1000 | Loss: 0.00001814
Iteration 47/1000 | Loss: 0.00001814
Iteration 48/1000 | Loss: 0.00001814
Iteration 49/1000 | Loss: 0.00001814
Iteration 50/1000 | Loss: 0.00001814
Iteration 51/1000 | Loss: 0.00001814
Iteration 52/1000 | Loss: 0.00001813
Iteration 53/1000 | Loss: 0.00001813
Iteration 54/1000 | Loss: 0.00001813
Iteration 55/1000 | Loss: 0.00001813
Iteration 56/1000 | Loss: 0.00001813
Iteration 57/1000 | Loss: 0.00001813
Iteration 58/1000 | Loss: 0.00001813
Iteration 59/1000 | Loss: 0.00001813
Iteration 60/1000 | Loss: 0.00001813
Iteration 61/1000 | Loss: 0.00001812
Iteration 62/1000 | Loss: 0.00001812
Iteration 63/1000 | Loss: 0.00001812
Iteration 64/1000 | Loss: 0.00001812
Iteration 65/1000 | Loss: 0.00001812
Iteration 66/1000 | Loss: 0.00001812
Iteration 67/1000 | Loss: 0.00001812
Iteration 68/1000 | Loss: 0.00001812
Iteration 69/1000 | Loss: 0.00001811
Iteration 70/1000 | Loss: 0.00001811
Iteration 71/1000 | Loss: 0.00001811
Iteration 72/1000 | Loss: 0.00001811
Iteration 73/1000 | Loss: 0.00001811
Iteration 74/1000 | Loss: 0.00001811
Iteration 75/1000 | Loss: 0.00001811
Iteration 76/1000 | Loss: 0.00001811
Iteration 77/1000 | Loss: 0.00001811
Iteration 78/1000 | Loss: 0.00001811
Iteration 79/1000 | Loss: 0.00001811
Iteration 80/1000 | Loss: 0.00001811
Iteration 81/1000 | Loss: 0.00001811
Iteration 82/1000 | Loss: 0.00001810
Iteration 83/1000 | Loss: 0.00001810
Iteration 84/1000 | Loss: 0.00001810
Iteration 85/1000 | Loss: 0.00001810
Iteration 86/1000 | Loss: 0.00001810
Iteration 87/1000 | Loss: 0.00001810
Iteration 88/1000 | Loss: 0.00001810
Iteration 89/1000 | Loss: 0.00001810
Iteration 90/1000 | Loss: 0.00001810
Iteration 91/1000 | Loss: 0.00001810
Iteration 92/1000 | Loss: 0.00001810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.810264802770689e-05, 1.810264802770689e-05, 1.810264802770689e-05, 1.810264802770689e-05, 1.810264802770689e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.810264802770689e-05

Optimization complete. Final v2v error: 3.4532508850097656 mm

Highest mean error: 4.011693477630615 mm for frame 156

Lowest mean error: 3.130540370941162 mm for frame 31

Saving results

Total time: 33.53900909423828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995375
Iteration 2/25 | Loss: 0.00995375
Iteration 3/25 | Loss: 0.00995375
Iteration 4/25 | Loss: 0.00995375
Iteration 5/25 | Loss: 0.00995375
Iteration 6/25 | Loss: 0.00995375
Iteration 7/25 | Loss: 0.00995374
Iteration 8/25 | Loss: 0.00995374
Iteration 9/25 | Loss: 0.00995374
Iteration 10/25 | Loss: 0.00995374
Iteration 11/25 | Loss: 0.00995374
Iteration 12/25 | Loss: 0.00995374
Iteration 13/25 | Loss: 0.00995374
Iteration 14/25 | Loss: 0.00995373
Iteration 15/25 | Loss: 0.00995373
Iteration 16/25 | Loss: 0.00995373
Iteration 17/25 | Loss: 0.00995373
Iteration 18/25 | Loss: 0.00995373
Iteration 19/25 | Loss: 0.00995372
Iteration 20/25 | Loss: 0.00995372
Iteration 21/25 | Loss: 0.00995372
Iteration 22/25 | Loss: 0.00995371
Iteration 23/25 | Loss: 0.00995371
Iteration 24/25 | Loss: 0.00995371
Iteration 25/25 | Loss: 0.00995371

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74685800
Iteration 2/25 | Loss: 0.18966416
Iteration 3/25 | Loss: 0.18963662
Iteration 4/25 | Loss: 0.18963659
Iteration 5/25 | Loss: 0.18963659
Iteration 6/25 | Loss: 0.18963659
Iteration 7/25 | Loss: 0.18963659
Iteration 8/25 | Loss: 0.18963659
Iteration 9/25 | Loss: 0.18963659
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.18963658809661865, 0.18963658809661865, 0.18963658809661865, 0.18963658809661865, 0.18963658809661865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18963658809661865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18963659
Iteration 2/1000 | Loss: 0.00320767
Iteration 3/1000 | Loss: 0.00114382
Iteration 4/1000 | Loss: 0.00050100
Iteration 5/1000 | Loss: 0.00025325
Iteration 6/1000 | Loss: 0.00014764
Iteration 7/1000 | Loss: 0.00008791
Iteration 8/1000 | Loss: 0.00006752
Iteration 9/1000 | Loss: 0.00005140
Iteration 10/1000 | Loss: 0.00004315
Iteration 11/1000 | Loss: 0.00003782
Iteration 12/1000 | Loss: 0.00003381
Iteration 13/1000 | Loss: 0.00003090
Iteration 14/1000 | Loss: 0.00002893
Iteration 15/1000 | Loss: 0.00002666
Iteration 16/1000 | Loss: 0.00002401
Iteration 17/1000 | Loss: 0.00002190
Iteration 18/1000 | Loss: 0.00002087
Iteration 19/1000 | Loss: 0.00001999
Iteration 20/1000 | Loss: 0.00001935
Iteration 21/1000 | Loss: 0.00001893
Iteration 22/1000 | Loss: 0.00001860
Iteration 23/1000 | Loss: 0.00001837
Iteration 24/1000 | Loss: 0.00001812
Iteration 25/1000 | Loss: 0.00001809
Iteration 26/1000 | Loss: 0.00001789
Iteration 27/1000 | Loss: 0.00001767
Iteration 28/1000 | Loss: 0.00001766
Iteration 29/1000 | Loss: 0.00001766
Iteration 30/1000 | Loss: 0.00001766
Iteration 31/1000 | Loss: 0.00001765
Iteration 32/1000 | Loss: 0.00001762
Iteration 33/1000 | Loss: 0.00001761
Iteration 34/1000 | Loss: 0.00001760
Iteration 35/1000 | Loss: 0.00001758
Iteration 36/1000 | Loss: 0.00001758
Iteration 37/1000 | Loss: 0.00001757
Iteration 38/1000 | Loss: 0.00001757
Iteration 39/1000 | Loss: 0.00001757
Iteration 40/1000 | Loss: 0.00001757
Iteration 41/1000 | Loss: 0.00001757
Iteration 42/1000 | Loss: 0.00001757
Iteration 43/1000 | Loss: 0.00001757
Iteration 44/1000 | Loss: 0.00001753
Iteration 45/1000 | Loss: 0.00001753
Iteration 46/1000 | Loss: 0.00001752
Iteration 47/1000 | Loss: 0.00001749
Iteration 48/1000 | Loss: 0.00001749
Iteration 49/1000 | Loss: 0.00001749
Iteration 50/1000 | Loss: 0.00001749
Iteration 51/1000 | Loss: 0.00001749
Iteration 52/1000 | Loss: 0.00001749
Iteration 53/1000 | Loss: 0.00001749
Iteration 54/1000 | Loss: 0.00001749
Iteration 55/1000 | Loss: 0.00001749
Iteration 56/1000 | Loss: 0.00001749
Iteration 57/1000 | Loss: 0.00001749
Iteration 58/1000 | Loss: 0.00001748
Iteration 59/1000 | Loss: 0.00001748
Iteration 60/1000 | Loss: 0.00001748
Iteration 61/1000 | Loss: 0.00001747
Iteration 62/1000 | Loss: 0.00001747
Iteration 63/1000 | Loss: 0.00001745
Iteration 64/1000 | Loss: 0.00001744
Iteration 65/1000 | Loss: 0.00001744
Iteration 66/1000 | Loss: 0.00001744
Iteration 67/1000 | Loss: 0.00001744
Iteration 68/1000 | Loss: 0.00001744
Iteration 69/1000 | Loss: 0.00001744
Iteration 70/1000 | Loss: 0.00001744
Iteration 71/1000 | Loss: 0.00001744
Iteration 72/1000 | Loss: 0.00001743
Iteration 73/1000 | Loss: 0.00001743
Iteration 74/1000 | Loss: 0.00001743
Iteration 75/1000 | Loss: 0.00001743
Iteration 76/1000 | Loss: 0.00001743
Iteration 77/1000 | Loss: 0.00001742
Iteration 78/1000 | Loss: 0.00001742
Iteration 79/1000 | Loss: 0.00001742
Iteration 80/1000 | Loss: 0.00001742
Iteration 81/1000 | Loss: 0.00001742
Iteration 82/1000 | Loss: 0.00001742
Iteration 83/1000 | Loss: 0.00001742
Iteration 84/1000 | Loss: 0.00001742
Iteration 85/1000 | Loss: 0.00001742
Iteration 86/1000 | Loss: 0.00001742
Iteration 87/1000 | Loss: 0.00001741
Iteration 88/1000 | Loss: 0.00001741
Iteration 89/1000 | Loss: 0.00001741
Iteration 90/1000 | Loss: 0.00001741
Iteration 91/1000 | Loss: 0.00001741
Iteration 92/1000 | Loss: 0.00001741
Iteration 93/1000 | Loss: 0.00001741
Iteration 94/1000 | Loss: 0.00001741
Iteration 95/1000 | Loss: 0.00001741
Iteration 96/1000 | Loss: 0.00001741
Iteration 97/1000 | Loss: 0.00001741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.7414642570656724e-05, 1.7414642570656724e-05, 1.7414642570656724e-05, 1.7414642570656724e-05, 1.7414642570656724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7414642570656724e-05

Optimization complete. Final v2v error: 3.5926434993743896 mm

Highest mean error: 4.1830034255981445 mm for frame 141

Lowest mean error: 3.403724193572998 mm for frame 125

Saving results

Total time: 56.83821487426758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415395
Iteration 2/25 | Loss: 0.00073259
Iteration 3/25 | Loss: 0.00063227
Iteration 4/25 | Loss: 0.00061809
Iteration 5/25 | Loss: 0.00061545
Iteration 6/25 | Loss: 0.00061497
Iteration 7/25 | Loss: 0.00061497
Iteration 8/25 | Loss: 0.00061497
Iteration 9/25 | Loss: 0.00061497
Iteration 10/25 | Loss: 0.00061497
Iteration 11/25 | Loss: 0.00061497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006149666151031852, 0.0006149666151031852, 0.0006149666151031852, 0.0006149666151031852, 0.0006149666151031852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006149666151031852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45911252
Iteration 2/25 | Loss: 0.00030902
Iteration 3/25 | Loss: 0.00030902
Iteration 4/25 | Loss: 0.00030902
Iteration 5/25 | Loss: 0.00030902
Iteration 6/25 | Loss: 0.00030902
Iteration 7/25 | Loss: 0.00030902
Iteration 8/25 | Loss: 0.00030902
Iteration 9/25 | Loss: 0.00030902
Iteration 10/25 | Loss: 0.00030902
Iteration 11/25 | Loss: 0.00030902
Iteration 12/25 | Loss: 0.00030902
Iteration 13/25 | Loss: 0.00030902
Iteration 14/25 | Loss: 0.00030902
Iteration 15/25 | Loss: 0.00030902
Iteration 16/25 | Loss: 0.00030902
Iteration 17/25 | Loss: 0.00030902
Iteration 18/25 | Loss: 0.00030902
Iteration 19/25 | Loss: 0.00030902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003090179234277457, 0.0003090179234277457, 0.0003090179234277457, 0.0003090179234277457, 0.0003090179234277457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003090179234277457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030902
Iteration 2/1000 | Loss: 0.00002215
Iteration 3/1000 | Loss: 0.00001666
Iteration 4/1000 | Loss: 0.00001547
Iteration 5/1000 | Loss: 0.00001464
Iteration 6/1000 | Loss: 0.00001414
Iteration 7/1000 | Loss: 0.00001384
Iteration 8/1000 | Loss: 0.00001373
Iteration 9/1000 | Loss: 0.00001362
Iteration 10/1000 | Loss: 0.00001349
Iteration 11/1000 | Loss: 0.00001347
Iteration 12/1000 | Loss: 0.00001343
Iteration 13/1000 | Loss: 0.00001342
Iteration 14/1000 | Loss: 0.00001342
Iteration 15/1000 | Loss: 0.00001341
Iteration 16/1000 | Loss: 0.00001341
Iteration 17/1000 | Loss: 0.00001340
Iteration 18/1000 | Loss: 0.00001339
Iteration 19/1000 | Loss: 0.00001339
Iteration 20/1000 | Loss: 0.00001337
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001336
Iteration 23/1000 | Loss: 0.00001336
Iteration 24/1000 | Loss: 0.00001335
Iteration 25/1000 | Loss: 0.00001335
Iteration 26/1000 | Loss: 0.00001335
Iteration 27/1000 | Loss: 0.00001334
Iteration 28/1000 | Loss: 0.00001334
Iteration 29/1000 | Loss: 0.00001334
Iteration 30/1000 | Loss: 0.00001334
Iteration 31/1000 | Loss: 0.00001334
Iteration 32/1000 | Loss: 0.00001333
Iteration 33/1000 | Loss: 0.00001333
Iteration 34/1000 | Loss: 0.00001333
Iteration 35/1000 | Loss: 0.00001333
Iteration 36/1000 | Loss: 0.00001333
Iteration 37/1000 | Loss: 0.00001333
Iteration 38/1000 | Loss: 0.00001333
Iteration 39/1000 | Loss: 0.00001333
Iteration 40/1000 | Loss: 0.00001332
Iteration 41/1000 | Loss: 0.00001332
Iteration 42/1000 | Loss: 0.00001332
Iteration 43/1000 | Loss: 0.00001332
Iteration 44/1000 | Loss: 0.00001332
Iteration 45/1000 | Loss: 0.00001331
Iteration 46/1000 | Loss: 0.00001331
Iteration 47/1000 | Loss: 0.00001330
Iteration 48/1000 | Loss: 0.00001330
Iteration 49/1000 | Loss: 0.00001330
Iteration 50/1000 | Loss: 0.00001329
Iteration 51/1000 | Loss: 0.00001329
Iteration 52/1000 | Loss: 0.00001329
Iteration 53/1000 | Loss: 0.00001329
Iteration 54/1000 | Loss: 0.00001329
Iteration 55/1000 | Loss: 0.00001329
Iteration 56/1000 | Loss: 0.00001329
Iteration 57/1000 | Loss: 0.00001328
Iteration 58/1000 | Loss: 0.00001328
Iteration 59/1000 | Loss: 0.00001328
Iteration 60/1000 | Loss: 0.00001328
Iteration 61/1000 | Loss: 0.00001328
Iteration 62/1000 | Loss: 0.00001328
Iteration 63/1000 | Loss: 0.00001328
Iteration 64/1000 | Loss: 0.00001328
Iteration 65/1000 | Loss: 0.00001328
Iteration 66/1000 | Loss: 0.00001328
Iteration 67/1000 | Loss: 0.00001328
Iteration 68/1000 | Loss: 0.00001328
Iteration 69/1000 | Loss: 0.00001328
Iteration 70/1000 | Loss: 0.00001327
Iteration 71/1000 | Loss: 0.00001327
Iteration 72/1000 | Loss: 0.00001327
Iteration 73/1000 | Loss: 0.00001327
Iteration 74/1000 | Loss: 0.00001327
Iteration 75/1000 | Loss: 0.00001327
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00001327
Iteration 78/1000 | Loss: 0.00001327
Iteration 79/1000 | Loss: 0.00001327
Iteration 80/1000 | Loss: 0.00001327
Iteration 81/1000 | Loss: 0.00001327
Iteration 82/1000 | Loss: 0.00001327
Iteration 83/1000 | Loss: 0.00001327
Iteration 84/1000 | Loss: 0.00001327
Iteration 85/1000 | Loss: 0.00001327
Iteration 86/1000 | Loss: 0.00001327
Iteration 87/1000 | Loss: 0.00001327
Iteration 88/1000 | Loss: 0.00001327
Iteration 89/1000 | Loss: 0.00001327
Iteration 90/1000 | Loss: 0.00001327
Iteration 91/1000 | Loss: 0.00001327
Iteration 92/1000 | Loss: 0.00001327
Iteration 93/1000 | Loss: 0.00001327
Iteration 94/1000 | Loss: 0.00001327
Iteration 95/1000 | Loss: 0.00001327
Iteration 96/1000 | Loss: 0.00001327
Iteration 97/1000 | Loss: 0.00001327
Iteration 98/1000 | Loss: 0.00001327
Iteration 99/1000 | Loss: 0.00001327
Iteration 100/1000 | Loss: 0.00001327
Iteration 101/1000 | Loss: 0.00001327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.3271659554447979e-05, 1.3271659554447979e-05, 1.3271659554447979e-05, 1.3271659554447979e-05, 1.3271659554447979e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3271659554447979e-05

Optimization complete. Final v2v error: 3.0921554565429688 mm

Highest mean error: 3.2962470054626465 mm for frame 79

Lowest mean error: 2.8068630695343018 mm for frame 0

Saving results

Total time: 28.238890647888184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058374
Iteration 2/25 | Loss: 0.00405480
Iteration 3/25 | Loss: 0.00267432
Iteration 4/25 | Loss: 0.00204200
Iteration 5/25 | Loss: 0.00141091
Iteration 6/25 | Loss: 0.00110488
Iteration 7/25 | Loss: 0.00095150
Iteration 8/25 | Loss: 0.00082672
Iteration 9/25 | Loss: 0.00079969
Iteration 10/25 | Loss: 0.00076286
Iteration 11/25 | Loss: 0.00075457
Iteration 12/25 | Loss: 0.00075152
Iteration 13/25 | Loss: 0.00075059
Iteration 14/25 | Loss: 0.00075623
Iteration 15/25 | Loss: 0.00075607
Iteration 16/25 | Loss: 0.00075841
Iteration 17/25 | Loss: 0.00075390
Iteration 18/25 | Loss: 0.00075025
Iteration 19/25 | Loss: 0.00074753
Iteration 20/25 | Loss: 0.00074544
Iteration 21/25 | Loss: 0.00074439
Iteration 22/25 | Loss: 0.00074399
Iteration 23/25 | Loss: 0.00074390
Iteration 24/25 | Loss: 0.00074387
Iteration 25/25 | Loss: 0.00074387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44897199
Iteration 2/25 | Loss: 0.00057864
Iteration 3/25 | Loss: 0.00057864
Iteration 4/25 | Loss: 0.00057864
Iteration 5/25 | Loss: 0.00057864
Iteration 6/25 | Loss: 0.00057864
Iteration 7/25 | Loss: 0.00057864
Iteration 8/25 | Loss: 0.00057864
Iteration 9/25 | Loss: 0.00057864
Iteration 10/25 | Loss: 0.00057864
Iteration 11/25 | Loss: 0.00057864
Iteration 12/25 | Loss: 0.00057864
Iteration 13/25 | Loss: 0.00057864
Iteration 14/25 | Loss: 0.00057864
Iteration 15/25 | Loss: 0.00057864
Iteration 16/25 | Loss: 0.00057864
Iteration 17/25 | Loss: 0.00057864
Iteration 18/25 | Loss: 0.00057864
Iteration 19/25 | Loss: 0.00057864
Iteration 20/25 | Loss: 0.00057864
Iteration 21/25 | Loss: 0.00057864
Iteration 22/25 | Loss: 0.00057864
Iteration 23/25 | Loss: 0.00057864
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005786397960036993, 0.0005786397960036993, 0.0005786397960036993, 0.0005786397960036993, 0.0005786397960036993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005786397960036993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057864
Iteration 2/1000 | Loss: 0.00007639
Iteration 3/1000 | Loss: 0.00005479
Iteration 4/1000 | Loss: 0.00004720
Iteration 5/1000 | Loss: 0.00004174
Iteration 6/1000 | Loss: 0.00003909
Iteration 7/1000 | Loss: 0.00003698
Iteration 8/1000 | Loss: 0.00003586
Iteration 9/1000 | Loss: 0.00165918
Iteration 10/1000 | Loss: 0.00019569
Iteration 11/1000 | Loss: 0.00003921
Iteration 12/1000 | Loss: 0.00002980
Iteration 13/1000 | Loss: 0.00002503
Iteration 14/1000 | Loss: 0.00002088
Iteration 15/1000 | Loss: 0.00001871
Iteration 16/1000 | Loss: 0.00001768
Iteration 17/1000 | Loss: 0.00001712
Iteration 18/1000 | Loss: 0.00001677
Iteration 19/1000 | Loss: 0.00001662
Iteration 20/1000 | Loss: 0.00001641
Iteration 21/1000 | Loss: 0.00001630
Iteration 22/1000 | Loss: 0.00001624
Iteration 23/1000 | Loss: 0.00001620
Iteration 24/1000 | Loss: 0.00001616
Iteration 25/1000 | Loss: 0.00001609
Iteration 26/1000 | Loss: 0.00001609
Iteration 27/1000 | Loss: 0.00001605
Iteration 28/1000 | Loss: 0.00001604
Iteration 29/1000 | Loss: 0.00001603
Iteration 30/1000 | Loss: 0.00001603
Iteration 31/1000 | Loss: 0.00001602
Iteration 32/1000 | Loss: 0.00001602
Iteration 33/1000 | Loss: 0.00001602
Iteration 34/1000 | Loss: 0.00001602
Iteration 35/1000 | Loss: 0.00001601
Iteration 36/1000 | Loss: 0.00001601
Iteration 37/1000 | Loss: 0.00001600
Iteration 38/1000 | Loss: 0.00001600
Iteration 39/1000 | Loss: 0.00001600
Iteration 40/1000 | Loss: 0.00001600
Iteration 41/1000 | Loss: 0.00001599
Iteration 42/1000 | Loss: 0.00001599
Iteration 43/1000 | Loss: 0.00001599
Iteration 44/1000 | Loss: 0.00001599
Iteration 45/1000 | Loss: 0.00001599
Iteration 46/1000 | Loss: 0.00001599
Iteration 47/1000 | Loss: 0.00001599
Iteration 48/1000 | Loss: 0.00001598
Iteration 49/1000 | Loss: 0.00001598
Iteration 50/1000 | Loss: 0.00001598
Iteration 51/1000 | Loss: 0.00001597
Iteration 52/1000 | Loss: 0.00001596
Iteration 53/1000 | Loss: 0.00001596
Iteration 54/1000 | Loss: 0.00001595
Iteration 55/1000 | Loss: 0.00001595
Iteration 56/1000 | Loss: 0.00001595
Iteration 57/1000 | Loss: 0.00001594
Iteration 58/1000 | Loss: 0.00001594
Iteration 59/1000 | Loss: 0.00001593
Iteration 60/1000 | Loss: 0.00001591
Iteration 61/1000 | Loss: 0.00001591
Iteration 62/1000 | Loss: 0.00001590
Iteration 63/1000 | Loss: 0.00001590
Iteration 64/1000 | Loss: 0.00001590
Iteration 65/1000 | Loss: 0.00001589
Iteration 66/1000 | Loss: 0.00001589
Iteration 67/1000 | Loss: 0.00001589
Iteration 68/1000 | Loss: 0.00001589
Iteration 69/1000 | Loss: 0.00001588
Iteration 70/1000 | Loss: 0.00001587
Iteration 71/1000 | Loss: 0.00001587
Iteration 72/1000 | Loss: 0.00001587
Iteration 73/1000 | Loss: 0.00001586
Iteration 74/1000 | Loss: 0.00001586
Iteration 75/1000 | Loss: 0.00001586
Iteration 76/1000 | Loss: 0.00001586
Iteration 77/1000 | Loss: 0.00001586
Iteration 78/1000 | Loss: 0.00001586
Iteration 79/1000 | Loss: 0.00001585
Iteration 80/1000 | Loss: 0.00001585
Iteration 81/1000 | Loss: 0.00001585
Iteration 82/1000 | Loss: 0.00001584
Iteration 83/1000 | Loss: 0.00001584
Iteration 84/1000 | Loss: 0.00001584
Iteration 85/1000 | Loss: 0.00001584
Iteration 86/1000 | Loss: 0.00001584
Iteration 87/1000 | Loss: 0.00001584
Iteration 88/1000 | Loss: 0.00001584
Iteration 89/1000 | Loss: 0.00001584
Iteration 90/1000 | Loss: 0.00001584
Iteration 91/1000 | Loss: 0.00001584
Iteration 92/1000 | Loss: 0.00001583
Iteration 93/1000 | Loss: 0.00001583
Iteration 94/1000 | Loss: 0.00001583
Iteration 95/1000 | Loss: 0.00001583
Iteration 96/1000 | Loss: 0.00001583
Iteration 97/1000 | Loss: 0.00001583
Iteration 98/1000 | Loss: 0.00001583
Iteration 99/1000 | Loss: 0.00001583
Iteration 100/1000 | Loss: 0.00001583
Iteration 101/1000 | Loss: 0.00001583
Iteration 102/1000 | Loss: 0.00001583
Iteration 103/1000 | Loss: 0.00001583
Iteration 104/1000 | Loss: 0.00001583
Iteration 105/1000 | Loss: 0.00001583
Iteration 106/1000 | Loss: 0.00001583
Iteration 107/1000 | Loss: 0.00001583
Iteration 108/1000 | Loss: 0.00001583
Iteration 109/1000 | Loss: 0.00001583
Iteration 110/1000 | Loss: 0.00001583
Iteration 111/1000 | Loss: 0.00001583
Iteration 112/1000 | Loss: 0.00001583
Iteration 113/1000 | Loss: 0.00001583
Iteration 114/1000 | Loss: 0.00001583
Iteration 115/1000 | Loss: 0.00001583
Iteration 116/1000 | Loss: 0.00001583
Iteration 117/1000 | Loss: 0.00001583
Iteration 118/1000 | Loss: 0.00001583
Iteration 119/1000 | Loss: 0.00001583
Iteration 120/1000 | Loss: 0.00001583
Iteration 121/1000 | Loss: 0.00001583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.5826406524865888e-05, 1.5826406524865888e-05, 1.5826406524865888e-05, 1.5826406524865888e-05, 1.5826406524865888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5826406524865888e-05

Optimization complete. Final v2v error: 3.359410285949707 mm

Highest mean error: 3.4916164875030518 mm for frame 17

Lowest mean error: 3.1504483222961426 mm for frame 0

Saving results

Total time: 78.77168321609497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046898
Iteration 2/25 | Loss: 0.00136295
Iteration 3/25 | Loss: 0.00085287
Iteration 4/25 | Loss: 0.00076430
Iteration 5/25 | Loss: 0.00073953
Iteration 6/25 | Loss: 0.00073390
Iteration 7/25 | Loss: 0.00073196
Iteration 8/25 | Loss: 0.00073134
Iteration 9/25 | Loss: 0.00073122
Iteration 10/25 | Loss: 0.00073121
Iteration 11/25 | Loss: 0.00073121
Iteration 12/25 | Loss: 0.00073121
Iteration 13/25 | Loss: 0.00073121
Iteration 14/25 | Loss: 0.00073121
Iteration 15/25 | Loss: 0.00073121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007312131347134709, 0.0007312131347134709, 0.0007312131347134709, 0.0007312131347134709, 0.0007312131347134709]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007312131347134709

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46049953
Iteration 2/25 | Loss: 0.00024626
Iteration 3/25 | Loss: 0.00024626
Iteration 4/25 | Loss: 0.00024626
Iteration 5/25 | Loss: 0.00024625
Iteration 6/25 | Loss: 0.00024625
Iteration 7/25 | Loss: 0.00024625
Iteration 8/25 | Loss: 0.00024625
Iteration 9/25 | Loss: 0.00024625
Iteration 10/25 | Loss: 0.00024625
Iteration 11/25 | Loss: 0.00024625
Iteration 12/25 | Loss: 0.00024625
Iteration 13/25 | Loss: 0.00024625
Iteration 14/25 | Loss: 0.00024625
Iteration 15/25 | Loss: 0.00024625
Iteration 16/25 | Loss: 0.00024625
Iteration 17/25 | Loss: 0.00024625
Iteration 18/25 | Loss: 0.00024625
Iteration 19/25 | Loss: 0.00024625
Iteration 20/25 | Loss: 0.00024625
Iteration 21/25 | Loss: 0.00024625
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0002462535339873284, 0.0002462535339873284, 0.0002462535339873284, 0.0002462535339873284, 0.0002462535339873284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002462535339873284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024625
Iteration 2/1000 | Loss: 0.00003404
Iteration 3/1000 | Loss: 0.00002605
Iteration 4/1000 | Loss: 0.00002487
Iteration 5/1000 | Loss: 0.00002406
Iteration 6/1000 | Loss: 0.00002334
Iteration 7/1000 | Loss: 0.00002295
Iteration 8/1000 | Loss: 0.00002273
Iteration 9/1000 | Loss: 0.00002252
Iteration 10/1000 | Loss: 0.00002247
Iteration 11/1000 | Loss: 0.00002247
Iteration 12/1000 | Loss: 0.00002247
Iteration 13/1000 | Loss: 0.00002244
Iteration 14/1000 | Loss: 0.00002241
Iteration 15/1000 | Loss: 0.00002241
Iteration 16/1000 | Loss: 0.00002241
Iteration 17/1000 | Loss: 0.00002241
Iteration 18/1000 | Loss: 0.00002241
Iteration 19/1000 | Loss: 0.00002241
Iteration 20/1000 | Loss: 0.00002240
Iteration 21/1000 | Loss: 0.00002231
Iteration 22/1000 | Loss: 0.00002230
Iteration 23/1000 | Loss: 0.00002229
Iteration 24/1000 | Loss: 0.00002229
Iteration 25/1000 | Loss: 0.00002228
Iteration 26/1000 | Loss: 0.00002224
Iteration 27/1000 | Loss: 0.00002224
Iteration 28/1000 | Loss: 0.00002224
Iteration 29/1000 | Loss: 0.00002224
Iteration 30/1000 | Loss: 0.00002223
Iteration 31/1000 | Loss: 0.00002223
Iteration 32/1000 | Loss: 0.00002223
Iteration 33/1000 | Loss: 0.00002223
Iteration 34/1000 | Loss: 0.00002223
Iteration 35/1000 | Loss: 0.00002222
Iteration 36/1000 | Loss: 0.00002222
Iteration 37/1000 | Loss: 0.00002221
Iteration 38/1000 | Loss: 0.00002220
Iteration 39/1000 | Loss: 0.00002219
Iteration 40/1000 | Loss: 0.00002219
Iteration 41/1000 | Loss: 0.00002219
Iteration 42/1000 | Loss: 0.00002219
Iteration 43/1000 | Loss: 0.00002218
Iteration 44/1000 | Loss: 0.00002218
Iteration 45/1000 | Loss: 0.00002218
Iteration 46/1000 | Loss: 0.00002218
Iteration 47/1000 | Loss: 0.00002218
Iteration 48/1000 | Loss: 0.00002218
Iteration 49/1000 | Loss: 0.00002218
Iteration 50/1000 | Loss: 0.00002216
Iteration 51/1000 | Loss: 0.00002216
Iteration 52/1000 | Loss: 0.00002216
Iteration 53/1000 | Loss: 0.00002216
Iteration 54/1000 | Loss: 0.00002216
Iteration 55/1000 | Loss: 0.00002215
Iteration 56/1000 | Loss: 0.00002215
Iteration 57/1000 | Loss: 0.00002215
Iteration 58/1000 | Loss: 0.00002215
Iteration 59/1000 | Loss: 0.00002215
Iteration 60/1000 | Loss: 0.00002215
Iteration 61/1000 | Loss: 0.00002214
Iteration 62/1000 | Loss: 0.00002214
Iteration 63/1000 | Loss: 0.00002214
Iteration 64/1000 | Loss: 0.00002213
Iteration 65/1000 | Loss: 0.00002213
Iteration 66/1000 | Loss: 0.00002213
Iteration 67/1000 | Loss: 0.00002212
Iteration 68/1000 | Loss: 0.00002212
Iteration 69/1000 | Loss: 0.00002211
Iteration 70/1000 | Loss: 0.00002211
Iteration 71/1000 | Loss: 0.00002211
Iteration 72/1000 | Loss: 0.00002211
Iteration 73/1000 | Loss: 0.00002211
Iteration 74/1000 | Loss: 0.00002210
Iteration 75/1000 | Loss: 0.00002210
Iteration 76/1000 | Loss: 0.00002210
Iteration 77/1000 | Loss: 0.00002210
Iteration 78/1000 | Loss: 0.00002210
Iteration 79/1000 | Loss: 0.00002210
Iteration 80/1000 | Loss: 0.00002210
Iteration 81/1000 | Loss: 0.00002210
Iteration 82/1000 | Loss: 0.00002209
Iteration 83/1000 | Loss: 0.00002208
Iteration 84/1000 | Loss: 0.00002208
Iteration 85/1000 | Loss: 0.00002208
Iteration 86/1000 | Loss: 0.00002207
Iteration 87/1000 | Loss: 0.00002207
Iteration 88/1000 | Loss: 0.00002207
Iteration 89/1000 | Loss: 0.00002207
Iteration 90/1000 | Loss: 0.00002206
Iteration 91/1000 | Loss: 0.00002206
Iteration 92/1000 | Loss: 0.00002206
Iteration 93/1000 | Loss: 0.00002206
Iteration 94/1000 | Loss: 0.00002205
Iteration 95/1000 | Loss: 0.00002205
Iteration 96/1000 | Loss: 0.00002205
Iteration 97/1000 | Loss: 0.00002205
Iteration 98/1000 | Loss: 0.00002205
Iteration 99/1000 | Loss: 0.00002205
Iteration 100/1000 | Loss: 0.00002205
Iteration 101/1000 | Loss: 0.00002205
Iteration 102/1000 | Loss: 0.00002205
Iteration 103/1000 | Loss: 0.00002205
Iteration 104/1000 | Loss: 0.00002205
Iteration 105/1000 | Loss: 0.00002205
Iteration 106/1000 | Loss: 0.00002205
Iteration 107/1000 | Loss: 0.00002205
Iteration 108/1000 | Loss: 0.00002205
Iteration 109/1000 | Loss: 0.00002205
Iteration 110/1000 | Loss: 0.00002205
Iteration 111/1000 | Loss: 0.00002205
Iteration 112/1000 | Loss: 0.00002205
Iteration 113/1000 | Loss: 0.00002205
Iteration 114/1000 | Loss: 0.00002205
Iteration 115/1000 | Loss: 0.00002205
Iteration 116/1000 | Loss: 0.00002205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.204588963650167e-05, 2.204588963650167e-05, 2.204588963650167e-05, 2.204588963650167e-05, 2.204588963650167e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.204588963650167e-05

Optimization complete. Final v2v error: 3.9946584701538086 mm

Highest mean error: 4.085055828094482 mm for frame 55

Lowest mean error: 3.886726140975952 mm for frame 0

Saving results

Total time: 36.259907245635986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056971
Iteration 2/25 | Loss: 0.01056971
Iteration 3/25 | Loss: 0.00352597
Iteration 4/25 | Loss: 0.00193628
Iteration 5/25 | Loss: 0.00186709
Iteration 6/25 | Loss: 0.00146094
Iteration 7/25 | Loss: 0.00135740
Iteration 8/25 | Loss: 0.00131628
Iteration 9/25 | Loss: 0.00126408
Iteration 10/25 | Loss: 0.00121438
Iteration 11/25 | Loss: 0.00114169
Iteration 12/25 | Loss: 0.00109028
Iteration 13/25 | Loss: 0.00106341
Iteration 14/25 | Loss: 0.00103177
Iteration 15/25 | Loss: 0.00101982
Iteration 16/25 | Loss: 0.00101883
Iteration 17/25 | Loss: 0.00101130
Iteration 18/25 | Loss: 0.00100853
Iteration 19/25 | Loss: 0.00100516
Iteration 20/25 | Loss: 0.00100719
Iteration 21/25 | Loss: 0.00100785
Iteration 22/25 | Loss: 0.00100538
Iteration 23/25 | Loss: 0.00100209
Iteration 24/25 | Loss: 0.00100428
Iteration 25/25 | Loss: 0.00100270

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47627354
Iteration 2/25 | Loss: 0.00298599
Iteration 3/25 | Loss: 0.00298599
Iteration 4/25 | Loss: 0.00298599
Iteration 5/25 | Loss: 0.00281163
Iteration 6/25 | Loss: 0.00281129
Iteration 7/25 | Loss: 0.00281129
Iteration 8/25 | Loss: 0.00281129
Iteration 9/25 | Loss: 0.00281128
Iteration 10/25 | Loss: 0.00281128
Iteration 11/25 | Loss: 0.00281128
Iteration 12/25 | Loss: 0.00281128
Iteration 13/25 | Loss: 0.00281128
Iteration 14/25 | Loss: 0.00281128
Iteration 15/25 | Loss: 0.00281128
Iteration 16/25 | Loss: 0.00281128
Iteration 17/25 | Loss: 0.00281128
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002811284037306905, 0.002811284037306905, 0.002811284037306905, 0.002811284037306905, 0.002811284037306905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002811284037306905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00281128
Iteration 2/1000 | Loss: 0.00068522
Iteration 3/1000 | Loss: 0.00040286
Iteration 4/1000 | Loss: 0.00033342
Iteration 5/1000 | Loss: 0.00033941
Iteration 6/1000 | Loss: 0.00035261
Iteration 7/1000 | Loss: 0.00033892
Iteration 8/1000 | Loss: 0.00052484
Iteration 9/1000 | Loss: 0.00052486
Iteration 10/1000 | Loss: 0.00052536
Iteration 11/1000 | Loss: 0.00026181
Iteration 12/1000 | Loss: 0.00047402
Iteration 13/1000 | Loss: 0.00039167
Iteration 14/1000 | Loss: 0.00036089
Iteration 15/1000 | Loss: 0.00035628
Iteration 16/1000 | Loss: 0.00097164
Iteration 17/1000 | Loss: 0.00099751
Iteration 18/1000 | Loss: 0.00032363
Iteration 19/1000 | Loss: 0.00086451
Iteration 20/1000 | Loss: 0.00040242
Iteration 21/1000 | Loss: 0.00021422
Iteration 22/1000 | Loss: 0.00064878
Iteration 23/1000 | Loss: 0.00076067
Iteration 24/1000 | Loss: 0.00021895
Iteration 25/1000 | Loss: 0.00019758
Iteration 26/1000 | Loss: 0.00020859
Iteration 27/1000 | Loss: 0.00069568
Iteration 28/1000 | Loss: 0.00032619
Iteration 29/1000 | Loss: 0.00033996
Iteration 30/1000 | Loss: 0.00036136
Iteration 31/1000 | Loss: 0.00019131
Iteration 32/1000 | Loss: 0.00019147
Iteration 33/1000 | Loss: 0.00019331
Iteration 34/1000 | Loss: 0.00017815
Iteration 35/1000 | Loss: 0.00017520
Iteration 36/1000 | Loss: 0.00018405
Iteration 37/1000 | Loss: 0.00039315
Iteration 38/1000 | Loss: 0.00058465
Iteration 39/1000 | Loss: 0.00034180
Iteration 40/1000 | Loss: 0.00017573
Iteration 41/1000 | Loss: 0.00018870
Iteration 42/1000 | Loss: 0.00019106
Iteration 43/1000 | Loss: 0.00017534
Iteration 44/1000 | Loss: 0.00018576
Iteration 45/1000 | Loss: 0.00022070
Iteration 46/1000 | Loss: 0.00019508
Iteration 47/1000 | Loss: 0.00017428
Iteration 48/1000 | Loss: 0.00019120
Iteration 49/1000 | Loss: 0.00017250
Iteration 50/1000 | Loss: 0.00026280
Iteration 51/1000 | Loss: 0.00023091
Iteration 52/1000 | Loss: 0.00018801
Iteration 53/1000 | Loss: 0.00019599
Iteration 54/1000 | Loss: 0.00017582
Iteration 55/1000 | Loss: 0.00016633
Iteration 56/1000 | Loss: 0.00018157
Iteration 57/1000 | Loss: 0.00018683
Iteration 58/1000 | Loss: 0.00020718
Iteration 59/1000 | Loss: 0.00017622
Iteration 60/1000 | Loss: 0.00017485
Iteration 61/1000 | Loss: 0.00016204
Iteration 62/1000 | Loss: 0.00018871
Iteration 63/1000 | Loss: 0.00016848
Iteration 64/1000 | Loss: 0.00016897
Iteration 65/1000 | Loss: 0.00019281
Iteration 66/1000 | Loss: 0.00018995
Iteration 67/1000 | Loss: 0.00017759
Iteration 68/1000 | Loss: 0.00020336
Iteration 69/1000 | Loss: 0.00017811
Iteration 70/1000 | Loss: 0.00019535
Iteration 71/1000 | Loss: 0.00018318
Iteration 72/1000 | Loss: 0.00018873
Iteration 73/1000 | Loss: 0.00016522
Iteration 74/1000 | Loss: 0.00019278
Iteration 75/1000 | Loss: 0.00018323
Iteration 76/1000 | Loss: 0.00017814
Iteration 77/1000 | Loss: 0.00019802
Iteration 78/1000 | Loss: 0.00017831
Iteration 79/1000 | Loss: 0.00019703
Iteration 80/1000 | Loss: 0.00017233
Iteration 81/1000 | Loss: 0.00018525
Iteration 82/1000 | Loss: 0.00018347
Iteration 83/1000 | Loss: 0.00016825
Iteration 84/1000 | Loss: 0.00018780
Iteration 85/1000 | Loss: 0.00018095
Iteration 86/1000 | Loss: 0.00017424
Iteration 87/1000 | Loss: 0.00018794
Iteration 88/1000 | Loss: 0.00017184
Iteration 89/1000 | Loss: 0.00016871
Iteration 90/1000 | Loss: 0.00017867
Iteration 91/1000 | Loss: 0.00017918
Iteration 92/1000 | Loss: 0.00015942
Iteration 93/1000 | Loss: 0.00018282
Iteration 94/1000 | Loss: 0.00017574
Iteration 95/1000 | Loss: 0.00017464
Iteration 96/1000 | Loss: 0.00018610
Iteration 97/1000 | Loss: 0.00017977
Iteration 98/1000 | Loss: 0.00017373
Iteration 99/1000 | Loss: 0.00018229
Iteration 100/1000 | Loss: 0.00016965
Iteration 101/1000 | Loss: 0.00017852
Iteration 102/1000 | Loss: 0.00018176
Iteration 103/1000 | Loss: 0.00018412
Iteration 104/1000 | Loss: 0.00016227
Iteration 105/1000 | Loss: 0.00017455
Iteration 106/1000 | Loss: 0.00019747
Iteration 107/1000 | Loss: 0.00018943
Iteration 108/1000 | Loss: 0.00018202
Iteration 109/1000 | Loss: 0.00019083
Iteration 110/1000 | Loss: 0.00017355
Iteration 111/1000 | Loss: 0.00017536
Iteration 112/1000 | Loss: 0.00016408
Iteration 113/1000 | Loss: 0.00017598
Iteration 114/1000 | Loss: 0.00016618
Iteration 115/1000 | Loss: 0.00018481
Iteration 116/1000 | Loss: 0.00017899
Iteration 117/1000 | Loss: 0.00017986
Iteration 118/1000 | Loss: 0.00017489
Iteration 119/1000 | Loss: 0.00017984
Iteration 120/1000 | Loss: 0.00018064
Iteration 121/1000 | Loss: 0.00018079
Iteration 122/1000 | Loss: 0.00018062
Iteration 123/1000 | Loss: 0.00018375
Iteration 124/1000 | Loss: 0.00017198
Iteration 125/1000 | Loss: 0.00017780
Iteration 126/1000 | Loss: 0.00016663
Iteration 127/1000 | Loss: 0.00017180
Iteration 128/1000 | Loss: 0.00016505
Iteration 129/1000 | Loss: 0.00017963
Iteration 130/1000 | Loss: 0.00019456
Iteration 131/1000 | Loss: 0.00018850
Iteration 132/1000 | Loss: 0.00019097
Iteration 133/1000 | Loss: 0.00018147
Iteration 134/1000 | Loss: 0.00017664
Iteration 135/1000 | Loss: 0.00018150
Iteration 136/1000 | Loss: 0.00016038
Iteration 137/1000 | Loss: 0.00018967
Iteration 138/1000 | Loss: 0.00014973
Iteration 139/1000 | Loss: 0.00015489
Iteration 140/1000 | Loss: 0.00015659
Iteration 141/1000 | Loss: 0.00016886
Iteration 142/1000 | Loss: 0.00016778
Iteration 143/1000 | Loss: 0.00016060
Iteration 144/1000 | Loss: 0.00017262
Iteration 145/1000 | Loss: 0.00016440
Iteration 146/1000 | Loss: 0.00015508
Iteration 147/1000 | Loss: 0.00015785
Iteration 148/1000 | Loss: 0.00016209
Iteration 149/1000 | Loss: 0.00017019
Iteration 150/1000 | Loss: 0.00016108
Iteration 151/1000 | Loss: 0.00016646
Iteration 152/1000 | Loss: 0.00016550
Iteration 153/1000 | Loss: 0.00016928
Iteration 154/1000 | Loss: 0.00017320
Iteration 155/1000 | Loss: 0.00015924
Iteration 156/1000 | Loss: 0.00016069
Iteration 157/1000 | Loss: 0.00015706
Iteration 158/1000 | Loss: 0.00014469
Iteration 159/1000 | Loss: 0.00016465
Iteration 160/1000 | Loss: 0.00017771
Iteration 161/1000 | Loss: 0.00017140
Iteration 162/1000 | Loss: 0.00018229
Iteration 163/1000 | Loss: 0.00017004
Iteration 164/1000 | Loss: 0.00018665
Iteration 165/1000 | Loss: 0.00016811
Iteration 166/1000 | Loss: 0.00015085
Iteration 167/1000 | Loss: 0.00015062
Iteration 168/1000 | Loss: 0.00015712
Iteration 169/1000 | Loss: 0.00017414
Iteration 170/1000 | Loss: 0.00017582
Iteration 171/1000 | Loss: 0.00016802
Iteration 172/1000 | Loss: 0.00016469
Iteration 173/1000 | Loss: 0.00016716
Iteration 174/1000 | Loss: 0.00018237
Iteration 175/1000 | Loss: 0.00016105
Iteration 176/1000 | Loss: 0.00015630
Iteration 177/1000 | Loss: 0.00015026
Iteration 178/1000 | Loss: 0.00015390
Iteration 179/1000 | Loss: 0.00016051
Iteration 180/1000 | Loss: 0.00016306
Iteration 181/1000 | Loss: 0.00017948
Iteration 182/1000 | Loss: 0.00015997
Iteration 183/1000 | Loss: 0.00016546
Iteration 184/1000 | Loss: 0.00017363
Iteration 185/1000 | Loss: 0.00016616
Iteration 186/1000 | Loss: 0.00015365
Iteration 187/1000 | Loss: 0.00015104
Iteration 188/1000 | Loss: 0.00015854
Iteration 189/1000 | Loss: 0.00015814
Iteration 190/1000 | Loss: 0.00015975
Iteration 191/1000 | Loss: 0.00017706
Iteration 192/1000 | Loss: 0.00015317
Iteration 193/1000 | Loss: 0.00015153
Iteration 194/1000 | Loss: 0.00013356
Iteration 195/1000 | Loss: 0.00015798
Iteration 196/1000 | Loss: 0.00016007
Iteration 197/1000 | Loss: 0.00014632
Iteration 198/1000 | Loss: 0.00014032
Iteration 199/1000 | Loss: 0.00014563
Iteration 200/1000 | Loss: 0.00015091
Iteration 201/1000 | Loss: 0.00016163
Iteration 202/1000 | Loss: 0.00016315
Iteration 203/1000 | Loss: 0.00016289
Iteration 204/1000 | Loss: 0.00016514
Iteration 205/1000 | Loss: 0.00013119
Iteration 206/1000 | Loss: 0.00013723
Iteration 207/1000 | Loss: 0.00014800
Iteration 208/1000 | Loss: 0.00013352
Iteration 209/1000 | Loss: 0.00049242
Iteration 210/1000 | Loss: 0.00034327
Iteration 211/1000 | Loss: 0.00015956
Iteration 212/1000 | Loss: 0.00014539
Iteration 213/1000 | Loss: 0.00014493
Iteration 214/1000 | Loss: 0.00013300
Iteration 215/1000 | Loss: 0.00013496
Iteration 216/1000 | Loss: 0.00013118
Iteration 217/1000 | Loss: 0.00013565
Iteration 218/1000 | Loss: 0.00014179
Iteration 219/1000 | Loss: 0.00038834
Iteration 220/1000 | Loss: 0.00021011
Iteration 221/1000 | Loss: 0.00062299
Iteration 222/1000 | Loss: 0.00155465
Iteration 223/1000 | Loss: 0.00101846
Iteration 224/1000 | Loss: 0.00015223
Iteration 225/1000 | Loss: 0.00013003
Iteration 226/1000 | Loss: 0.00042196
Iteration 227/1000 | Loss: 0.00036281
Iteration 228/1000 | Loss: 0.00012501
Iteration 229/1000 | Loss: 0.00040479
Iteration 230/1000 | Loss: 0.00040344
Iteration 231/1000 | Loss: 0.00041728
Iteration 232/1000 | Loss: 0.00013111
Iteration 233/1000 | Loss: 0.00012516
Iteration 234/1000 | Loss: 0.00011547
Iteration 235/1000 | Loss: 0.00012450
Iteration 236/1000 | Loss: 0.00011891
Iteration 237/1000 | Loss: 0.00011887
Iteration 238/1000 | Loss: 0.00011774
Iteration 239/1000 | Loss: 0.00012038
Iteration 240/1000 | Loss: 0.00011674
Iteration 241/1000 | Loss: 0.00011913
Iteration 242/1000 | Loss: 0.00011969
Iteration 243/1000 | Loss: 0.00011601
Iteration 244/1000 | Loss: 0.00011822
Iteration 245/1000 | Loss: 0.00012457
Iteration 246/1000 | Loss: 0.00011854
Iteration 247/1000 | Loss: 0.00012270
Iteration 248/1000 | Loss: 0.00011525
Iteration 249/1000 | Loss: 0.00011368
Iteration 250/1000 | Loss: 0.00012001
Iteration 251/1000 | Loss: 0.00012209
Iteration 252/1000 | Loss: 0.00011895
Iteration 253/1000 | Loss: 0.00012277
Iteration 254/1000 | Loss: 0.00012227
Iteration 255/1000 | Loss: 0.00012322
Iteration 256/1000 | Loss: 0.00012542
Iteration 257/1000 | Loss: 0.00012305
Iteration 258/1000 | Loss: 0.00012818
Iteration 259/1000 | Loss: 0.00012687
Iteration 260/1000 | Loss: 0.00011903
Iteration 261/1000 | Loss: 0.00012066
Iteration 262/1000 | Loss: 0.00012383
Iteration 263/1000 | Loss: 0.00011924
Iteration 264/1000 | Loss: 0.00011585
Iteration 265/1000 | Loss: 0.00011738
Iteration 266/1000 | Loss: 0.00011957
Iteration 267/1000 | Loss: 0.00011873
Iteration 268/1000 | Loss: 0.00012068
Iteration 269/1000 | Loss: 0.00011385
Iteration 270/1000 | Loss: 0.00011267
Iteration 271/1000 | Loss: 0.00011937
Iteration 272/1000 | Loss: 0.00011841
Iteration 273/1000 | Loss: 0.00012164
Iteration 274/1000 | Loss: 0.00011822
Iteration 275/1000 | Loss: 0.00011896
Iteration 276/1000 | Loss: 0.00011802
Iteration 277/1000 | Loss: 0.00011879
Iteration 278/1000 | Loss: 0.00011787
Iteration 279/1000 | Loss: 0.00011845
Iteration 280/1000 | Loss: 0.00012337
Iteration 281/1000 | Loss: 0.00012025
Iteration 282/1000 | Loss: 0.00011853
Iteration 283/1000 | Loss: 0.00011828
Iteration 284/1000 | Loss: 0.00013383
Iteration 285/1000 | Loss: 0.00011329
Iteration 286/1000 | Loss: 0.00011160
Iteration 287/1000 | Loss: 0.00011047
Iteration 288/1000 | Loss: 0.00010995
Iteration 289/1000 | Loss: 0.00010964
Iteration 290/1000 | Loss: 0.00031748
Iteration 291/1000 | Loss: 0.00023762
Iteration 292/1000 | Loss: 0.00011727
Iteration 293/1000 | Loss: 0.00044002
Iteration 294/1000 | Loss: 0.00032573
Iteration 295/1000 | Loss: 0.00011993
Iteration 296/1000 | Loss: 0.00011409
Iteration 297/1000 | Loss: 0.00011108
Iteration 298/1000 | Loss: 0.00013693
Iteration 299/1000 | Loss: 0.00013941
Iteration 300/1000 | Loss: 0.00011994
Iteration 301/1000 | Loss: 0.00010926
Iteration 302/1000 | Loss: 0.00010723
Iteration 303/1000 | Loss: 0.00044581
Iteration 304/1000 | Loss: 0.00032378
Iteration 305/1000 | Loss: 0.00011030
Iteration 306/1000 | Loss: 0.00010826
Iteration 307/1000 | Loss: 0.00010696
Iteration 308/1000 | Loss: 0.00010623
Iteration 309/1000 | Loss: 0.00031019
Iteration 310/1000 | Loss: 0.00013668
Iteration 311/1000 | Loss: 0.00010616
Iteration 312/1000 | Loss: 0.00032630
Iteration 313/1000 | Loss: 0.00012535
Iteration 314/1000 | Loss: 0.00025306
Iteration 315/1000 | Loss: 0.00011968
Iteration 316/1000 | Loss: 0.00018715
Iteration 317/1000 | Loss: 0.00010709
Iteration 318/1000 | Loss: 0.00010568
Iteration 319/1000 | Loss: 0.00010493
Iteration 320/1000 | Loss: 0.00010435
Iteration 321/1000 | Loss: 0.00010400
Iteration 322/1000 | Loss: 0.00010384
Iteration 323/1000 | Loss: 0.00010380
Iteration 324/1000 | Loss: 0.00010374
Iteration 325/1000 | Loss: 0.00010367
Iteration 326/1000 | Loss: 0.00010363
Iteration 327/1000 | Loss: 0.00010350
Iteration 328/1000 | Loss: 0.00010349
Iteration 329/1000 | Loss: 0.00010349
Iteration 330/1000 | Loss: 0.00010348
Iteration 331/1000 | Loss: 0.00010348
Iteration 332/1000 | Loss: 0.00010348
Iteration 333/1000 | Loss: 0.00010348
Iteration 334/1000 | Loss: 0.00010348
Iteration 335/1000 | Loss: 0.00010348
Iteration 336/1000 | Loss: 0.00010348
Iteration 337/1000 | Loss: 0.00010348
Iteration 338/1000 | Loss: 0.00010348
Iteration 339/1000 | Loss: 0.00010348
Iteration 340/1000 | Loss: 0.00010348
Iteration 341/1000 | Loss: 0.00010348
Iteration 342/1000 | Loss: 0.00010347
Iteration 343/1000 | Loss: 0.00010347
Iteration 344/1000 | Loss: 0.00010347
Iteration 345/1000 | Loss: 0.00010347
Iteration 346/1000 | Loss: 0.00010347
Iteration 347/1000 | Loss: 0.00010347
Iteration 348/1000 | Loss: 0.00010347
Iteration 349/1000 | Loss: 0.00010347
Iteration 350/1000 | Loss: 0.00010347
Iteration 351/1000 | Loss: 0.00010346
Iteration 352/1000 | Loss: 0.00010346
Iteration 353/1000 | Loss: 0.00010346
Iteration 354/1000 | Loss: 0.00010346
Iteration 355/1000 | Loss: 0.00010345
Iteration 356/1000 | Loss: 0.00010345
Iteration 357/1000 | Loss: 0.00010345
Iteration 358/1000 | Loss: 0.00010345
Iteration 359/1000 | Loss: 0.00010345
Iteration 360/1000 | Loss: 0.00010345
Iteration 361/1000 | Loss: 0.00010345
Iteration 362/1000 | Loss: 0.00010345
Iteration 363/1000 | Loss: 0.00010345
Iteration 364/1000 | Loss: 0.00010345
Iteration 365/1000 | Loss: 0.00010344
Iteration 366/1000 | Loss: 0.00010344
Iteration 367/1000 | Loss: 0.00010344
Iteration 368/1000 | Loss: 0.00010344
Iteration 369/1000 | Loss: 0.00010344
Iteration 370/1000 | Loss: 0.00010343
Iteration 371/1000 | Loss: 0.00010343
Iteration 372/1000 | Loss: 0.00010343
Iteration 373/1000 | Loss: 0.00010343
Iteration 374/1000 | Loss: 0.00010343
Iteration 375/1000 | Loss: 0.00010343
Iteration 376/1000 | Loss: 0.00010342
Iteration 377/1000 | Loss: 0.00010342
Iteration 378/1000 | Loss: 0.00010342
Iteration 379/1000 | Loss: 0.00010342
Iteration 380/1000 | Loss: 0.00010342
Iteration 381/1000 | Loss: 0.00010342
Iteration 382/1000 | Loss: 0.00010342
Iteration 383/1000 | Loss: 0.00010341
Iteration 384/1000 | Loss: 0.00010341
Iteration 385/1000 | Loss: 0.00010341
Iteration 386/1000 | Loss: 0.00010341
Iteration 387/1000 | Loss: 0.00010341
Iteration 388/1000 | Loss: 0.00010341
Iteration 389/1000 | Loss: 0.00010341
Iteration 390/1000 | Loss: 0.00010341
Iteration 391/1000 | Loss: 0.00010341
Iteration 392/1000 | Loss: 0.00010341
Iteration 393/1000 | Loss: 0.00010341
Iteration 394/1000 | Loss: 0.00010340
Iteration 395/1000 | Loss: 0.00010340
Iteration 396/1000 | Loss: 0.00010340
Iteration 397/1000 | Loss: 0.00010340
Iteration 398/1000 | Loss: 0.00010340
Iteration 399/1000 | Loss: 0.00010340
Iteration 400/1000 | Loss: 0.00010340
Iteration 401/1000 | Loss: 0.00010340
Iteration 402/1000 | Loss: 0.00010340
Iteration 403/1000 | Loss: 0.00010339
Iteration 404/1000 | Loss: 0.00010339
Iteration 405/1000 | Loss: 0.00010339
Iteration 406/1000 | Loss: 0.00010339
Iteration 407/1000 | Loss: 0.00010339
Iteration 408/1000 | Loss: 0.00010339
Iteration 409/1000 | Loss: 0.00010339
Iteration 410/1000 | Loss: 0.00010339
Iteration 411/1000 | Loss: 0.00010339
Iteration 412/1000 | Loss: 0.00010339
Iteration 413/1000 | Loss: 0.00010339
Iteration 414/1000 | Loss: 0.00010339
Iteration 415/1000 | Loss: 0.00010339
Iteration 416/1000 | Loss: 0.00010339
Iteration 417/1000 | Loss: 0.00010339
Iteration 418/1000 | Loss: 0.00010339
Iteration 419/1000 | Loss: 0.00010338
Iteration 420/1000 | Loss: 0.00010338
Iteration 421/1000 | Loss: 0.00010338
Iteration 422/1000 | Loss: 0.00010338
Iteration 423/1000 | Loss: 0.00010338
Iteration 424/1000 | Loss: 0.00010338
Iteration 425/1000 | Loss: 0.00010338
Iteration 426/1000 | Loss: 0.00010338
Iteration 427/1000 | Loss: 0.00010338
Iteration 428/1000 | Loss: 0.00010338
Iteration 429/1000 | Loss: 0.00010337
Iteration 430/1000 | Loss: 0.00010337
Iteration 431/1000 | Loss: 0.00010337
Iteration 432/1000 | Loss: 0.00010337
Iteration 433/1000 | Loss: 0.00010337
Iteration 434/1000 | Loss: 0.00010337
Iteration 435/1000 | Loss: 0.00010337
Iteration 436/1000 | Loss: 0.00010337
Iteration 437/1000 | Loss: 0.00010337
Iteration 438/1000 | Loss: 0.00010337
Iteration 439/1000 | Loss: 0.00010337
Iteration 440/1000 | Loss: 0.00010337
Iteration 441/1000 | Loss: 0.00010336
Iteration 442/1000 | Loss: 0.00010336
Iteration 443/1000 | Loss: 0.00010336
Iteration 444/1000 | Loss: 0.00010336
Iteration 445/1000 | Loss: 0.00010336
Iteration 446/1000 | Loss: 0.00010336
Iteration 447/1000 | Loss: 0.00010336
Iteration 448/1000 | Loss: 0.00010336
Iteration 449/1000 | Loss: 0.00010336
Iteration 450/1000 | Loss: 0.00010335
Iteration 451/1000 | Loss: 0.00010335
Iteration 452/1000 | Loss: 0.00010335
Iteration 453/1000 | Loss: 0.00010335
Iteration 454/1000 | Loss: 0.00010335
Iteration 455/1000 | Loss: 0.00010335
Iteration 456/1000 | Loss: 0.00010335
Iteration 457/1000 | Loss: 0.00010335
Iteration 458/1000 | Loss: 0.00010335
Iteration 459/1000 | Loss: 0.00010335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 459. Stopping optimization.
Last 5 losses: [0.00010335081606172025, 0.00010335081606172025, 0.00010335081606172025, 0.00010335081606172025, 0.00010335081606172025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00010335081606172025

Optimization complete. Final v2v error: 5.374047756195068 mm

Highest mean error: 12.20193862915039 mm for frame 20

Lowest mean error: 3.3976855278015137 mm for frame 6

Saving results

Total time: 583.1549117565155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739536
Iteration 2/25 | Loss: 0.00093801
Iteration 3/25 | Loss: 0.00074301
Iteration 4/25 | Loss: 0.00071685
Iteration 5/25 | Loss: 0.00070892
Iteration 6/25 | Loss: 0.00070644
Iteration 7/25 | Loss: 0.00070606
Iteration 8/25 | Loss: 0.00070606
Iteration 9/25 | Loss: 0.00070606
Iteration 10/25 | Loss: 0.00070606
Iteration 11/25 | Loss: 0.00070606
Iteration 12/25 | Loss: 0.00070606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007060616626404226, 0.0007060616626404226, 0.0007060616626404226, 0.0007060616626404226, 0.0007060616626404226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007060616626404226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46507108
Iteration 2/25 | Loss: 0.00033637
Iteration 3/25 | Loss: 0.00033637
Iteration 4/25 | Loss: 0.00033637
Iteration 5/25 | Loss: 0.00033637
Iteration 6/25 | Loss: 0.00033637
Iteration 7/25 | Loss: 0.00033637
Iteration 8/25 | Loss: 0.00033637
Iteration 9/25 | Loss: 0.00033637
Iteration 10/25 | Loss: 0.00033637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00033636734588071704, 0.00033636734588071704, 0.00033636734588071704, 0.00033636734588071704, 0.00033636734588071704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033636734588071704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033637
Iteration 2/1000 | Loss: 0.00003244
Iteration 3/1000 | Loss: 0.00002442
Iteration 4/1000 | Loss: 0.00002272
Iteration 5/1000 | Loss: 0.00002178
Iteration 6/1000 | Loss: 0.00002112
Iteration 7/1000 | Loss: 0.00002061
Iteration 8/1000 | Loss: 0.00002024
Iteration 9/1000 | Loss: 0.00002009
Iteration 10/1000 | Loss: 0.00001984
Iteration 11/1000 | Loss: 0.00001967
Iteration 12/1000 | Loss: 0.00001950
Iteration 13/1000 | Loss: 0.00001948
Iteration 14/1000 | Loss: 0.00001945
Iteration 15/1000 | Loss: 0.00001945
Iteration 16/1000 | Loss: 0.00001943
Iteration 17/1000 | Loss: 0.00001941
Iteration 18/1000 | Loss: 0.00001937
Iteration 19/1000 | Loss: 0.00001937
Iteration 20/1000 | Loss: 0.00001933
Iteration 21/1000 | Loss: 0.00001932
Iteration 22/1000 | Loss: 0.00001930
Iteration 23/1000 | Loss: 0.00001930
Iteration 24/1000 | Loss: 0.00001929
Iteration 25/1000 | Loss: 0.00001929
Iteration 26/1000 | Loss: 0.00001929
Iteration 27/1000 | Loss: 0.00001928
Iteration 28/1000 | Loss: 0.00001928
Iteration 29/1000 | Loss: 0.00001928
Iteration 30/1000 | Loss: 0.00001927
Iteration 31/1000 | Loss: 0.00001927
Iteration 32/1000 | Loss: 0.00001927
Iteration 33/1000 | Loss: 0.00001926
Iteration 34/1000 | Loss: 0.00001926
Iteration 35/1000 | Loss: 0.00001925
Iteration 36/1000 | Loss: 0.00001925
Iteration 37/1000 | Loss: 0.00001925
Iteration 38/1000 | Loss: 0.00001924
Iteration 39/1000 | Loss: 0.00001924
Iteration 40/1000 | Loss: 0.00001924
Iteration 41/1000 | Loss: 0.00001923
Iteration 42/1000 | Loss: 0.00001923
Iteration 43/1000 | Loss: 0.00001923
Iteration 44/1000 | Loss: 0.00001922
Iteration 45/1000 | Loss: 0.00001921
Iteration 46/1000 | Loss: 0.00001921
Iteration 47/1000 | Loss: 0.00001921
Iteration 48/1000 | Loss: 0.00001921
Iteration 49/1000 | Loss: 0.00001921
Iteration 50/1000 | Loss: 0.00001921
Iteration 51/1000 | Loss: 0.00001921
Iteration 52/1000 | Loss: 0.00001921
Iteration 53/1000 | Loss: 0.00001921
Iteration 54/1000 | Loss: 0.00001921
Iteration 55/1000 | Loss: 0.00001921
Iteration 56/1000 | Loss: 0.00001921
Iteration 57/1000 | Loss: 0.00001920
Iteration 58/1000 | Loss: 0.00001920
Iteration 59/1000 | Loss: 0.00001920
Iteration 60/1000 | Loss: 0.00001920
Iteration 61/1000 | Loss: 0.00001920
Iteration 62/1000 | Loss: 0.00001920
Iteration 63/1000 | Loss: 0.00001920
Iteration 64/1000 | Loss: 0.00001920
Iteration 65/1000 | Loss: 0.00001920
Iteration 66/1000 | Loss: 0.00001920
Iteration 67/1000 | Loss: 0.00001919
Iteration 68/1000 | Loss: 0.00001919
Iteration 69/1000 | Loss: 0.00001919
Iteration 70/1000 | Loss: 0.00001919
Iteration 71/1000 | Loss: 0.00001919
Iteration 72/1000 | Loss: 0.00001919
Iteration 73/1000 | Loss: 0.00001918
Iteration 74/1000 | Loss: 0.00001918
Iteration 75/1000 | Loss: 0.00001918
Iteration 76/1000 | Loss: 0.00001918
Iteration 77/1000 | Loss: 0.00001918
Iteration 78/1000 | Loss: 0.00001917
Iteration 79/1000 | Loss: 0.00001917
Iteration 80/1000 | Loss: 0.00001917
Iteration 81/1000 | Loss: 0.00001917
Iteration 82/1000 | Loss: 0.00001917
Iteration 83/1000 | Loss: 0.00001917
Iteration 84/1000 | Loss: 0.00001917
Iteration 85/1000 | Loss: 0.00001917
Iteration 86/1000 | Loss: 0.00001917
Iteration 87/1000 | Loss: 0.00001917
Iteration 88/1000 | Loss: 0.00001917
Iteration 89/1000 | Loss: 0.00001917
Iteration 90/1000 | Loss: 0.00001917
Iteration 91/1000 | Loss: 0.00001917
Iteration 92/1000 | Loss: 0.00001917
Iteration 93/1000 | Loss: 0.00001917
Iteration 94/1000 | Loss: 0.00001917
Iteration 95/1000 | Loss: 0.00001917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.917110239446629e-05, 1.917110239446629e-05, 1.917110239446629e-05, 1.917110239446629e-05, 1.917110239446629e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.917110239446629e-05

Optimization complete. Final v2v error: 3.6781256198883057 mm

Highest mean error: 5.367109298706055 mm for frame 239

Lowest mean error: 3.0747361183166504 mm for frame 18

Saving results

Total time: 37.700937271118164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804275
Iteration 2/25 | Loss: 0.00106032
Iteration 3/25 | Loss: 0.00080422
Iteration 4/25 | Loss: 0.00074824
Iteration 5/25 | Loss: 0.00073061
Iteration 6/25 | Loss: 0.00072547
Iteration 7/25 | Loss: 0.00072386
Iteration 8/25 | Loss: 0.00072363
Iteration 9/25 | Loss: 0.00072363
Iteration 10/25 | Loss: 0.00072363
Iteration 11/25 | Loss: 0.00072363
Iteration 12/25 | Loss: 0.00072363
Iteration 13/25 | Loss: 0.00072363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007236290839500725, 0.0007236290839500725, 0.0007236290839500725, 0.0007236290839500725, 0.0007236290839500725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007236290839500725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65199232
Iteration 2/25 | Loss: 0.00046307
Iteration 3/25 | Loss: 0.00046307
Iteration 4/25 | Loss: 0.00046306
Iteration 5/25 | Loss: 0.00046306
Iteration 6/25 | Loss: 0.00046306
Iteration 7/25 | Loss: 0.00046306
Iteration 8/25 | Loss: 0.00046306
Iteration 9/25 | Loss: 0.00046306
Iteration 10/25 | Loss: 0.00046306
Iteration 11/25 | Loss: 0.00046306
Iteration 12/25 | Loss: 0.00046306
Iteration 13/25 | Loss: 0.00046306
Iteration 14/25 | Loss: 0.00046306
Iteration 15/25 | Loss: 0.00046306
Iteration 16/25 | Loss: 0.00046306
Iteration 17/25 | Loss: 0.00046306
Iteration 18/25 | Loss: 0.00046306
Iteration 19/25 | Loss: 0.00046306
Iteration 20/25 | Loss: 0.00046306
Iteration 21/25 | Loss: 0.00046306
Iteration 22/25 | Loss: 0.00046306
Iteration 23/25 | Loss: 0.00046306
Iteration 24/25 | Loss: 0.00046306
Iteration 25/25 | Loss: 0.00046306
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00046306278090924025, 0.00046306278090924025, 0.00046306278090924025, 0.00046306278090924025, 0.00046306278090924025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046306278090924025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046306
Iteration 2/1000 | Loss: 0.00005306
Iteration 3/1000 | Loss: 0.00003240
Iteration 4/1000 | Loss: 0.00002726
Iteration 5/1000 | Loss: 0.00002534
Iteration 6/1000 | Loss: 0.00002373
Iteration 7/1000 | Loss: 0.00002302
Iteration 8/1000 | Loss: 0.00002249
Iteration 9/1000 | Loss: 0.00002200
Iteration 10/1000 | Loss: 0.00002162
Iteration 11/1000 | Loss: 0.00002135
Iteration 12/1000 | Loss: 0.00002114
Iteration 13/1000 | Loss: 0.00002087
Iteration 14/1000 | Loss: 0.00002072
Iteration 15/1000 | Loss: 0.00002066
Iteration 16/1000 | Loss: 0.00002065
Iteration 17/1000 | Loss: 0.00002060
Iteration 18/1000 | Loss: 0.00002055
Iteration 19/1000 | Loss: 0.00002054
Iteration 20/1000 | Loss: 0.00002054
Iteration 21/1000 | Loss: 0.00002051
Iteration 22/1000 | Loss: 0.00002050
Iteration 23/1000 | Loss: 0.00002049
Iteration 24/1000 | Loss: 0.00002048
Iteration 25/1000 | Loss: 0.00002048
Iteration 26/1000 | Loss: 0.00002046
Iteration 27/1000 | Loss: 0.00002046
Iteration 28/1000 | Loss: 0.00002044
Iteration 29/1000 | Loss: 0.00002044
Iteration 30/1000 | Loss: 0.00002044
Iteration 31/1000 | Loss: 0.00002042
Iteration 32/1000 | Loss: 0.00002042
Iteration 33/1000 | Loss: 0.00002042
Iteration 34/1000 | Loss: 0.00002041
Iteration 35/1000 | Loss: 0.00002041
Iteration 36/1000 | Loss: 0.00002040
Iteration 37/1000 | Loss: 0.00002040
Iteration 38/1000 | Loss: 0.00002039
Iteration 39/1000 | Loss: 0.00002037
Iteration 40/1000 | Loss: 0.00002036
Iteration 41/1000 | Loss: 0.00002035
Iteration 42/1000 | Loss: 0.00002035
Iteration 43/1000 | Loss: 0.00002034
Iteration 44/1000 | Loss: 0.00002034
Iteration 45/1000 | Loss: 0.00002034
Iteration 46/1000 | Loss: 0.00002034
Iteration 47/1000 | Loss: 0.00002033
Iteration 48/1000 | Loss: 0.00002033
Iteration 49/1000 | Loss: 0.00002033
Iteration 50/1000 | Loss: 0.00002033
Iteration 51/1000 | Loss: 0.00002032
Iteration 52/1000 | Loss: 0.00002032
Iteration 53/1000 | Loss: 0.00002032
Iteration 54/1000 | Loss: 0.00002031
Iteration 55/1000 | Loss: 0.00002031
Iteration 56/1000 | Loss: 0.00002030
Iteration 57/1000 | Loss: 0.00002030
Iteration 58/1000 | Loss: 0.00002030
Iteration 59/1000 | Loss: 0.00002029
Iteration 60/1000 | Loss: 0.00002029
Iteration 61/1000 | Loss: 0.00002029
Iteration 62/1000 | Loss: 0.00002028
Iteration 63/1000 | Loss: 0.00002028
Iteration 64/1000 | Loss: 0.00002028
Iteration 65/1000 | Loss: 0.00002027
Iteration 66/1000 | Loss: 0.00002027
Iteration 67/1000 | Loss: 0.00002027
Iteration 68/1000 | Loss: 0.00002026
Iteration 69/1000 | Loss: 0.00002026
Iteration 70/1000 | Loss: 0.00002026
Iteration 71/1000 | Loss: 0.00002025
Iteration 72/1000 | Loss: 0.00002025
Iteration 73/1000 | Loss: 0.00002025
Iteration 74/1000 | Loss: 0.00002025
Iteration 75/1000 | Loss: 0.00002024
Iteration 76/1000 | Loss: 0.00002024
Iteration 77/1000 | Loss: 0.00002024
Iteration 78/1000 | Loss: 0.00002023
Iteration 79/1000 | Loss: 0.00002023
Iteration 80/1000 | Loss: 0.00002023
Iteration 81/1000 | Loss: 0.00002023
Iteration 82/1000 | Loss: 0.00002022
Iteration 83/1000 | Loss: 0.00002022
Iteration 84/1000 | Loss: 0.00002022
Iteration 85/1000 | Loss: 0.00002022
Iteration 86/1000 | Loss: 0.00002021
Iteration 87/1000 | Loss: 0.00002021
Iteration 88/1000 | Loss: 0.00002021
Iteration 89/1000 | Loss: 0.00002021
Iteration 90/1000 | Loss: 0.00002021
Iteration 91/1000 | Loss: 0.00002021
Iteration 92/1000 | Loss: 0.00002021
Iteration 93/1000 | Loss: 0.00002020
Iteration 94/1000 | Loss: 0.00002020
Iteration 95/1000 | Loss: 0.00002020
Iteration 96/1000 | Loss: 0.00002020
Iteration 97/1000 | Loss: 0.00002020
Iteration 98/1000 | Loss: 0.00002020
Iteration 99/1000 | Loss: 0.00002020
Iteration 100/1000 | Loss: 0.00002020
Iteration 101/1000 | Loss: 0.00002020
Iteration 102/1000 | Loss: 0.00002020
Iteration 103/1000 | Loss: 0.00002020
Iteration 104/1000 | Loss: 0.00002019
Iteration 105/1000 | Loss: 0.00002019
Iteration 106/1000 | Loss: 0.00002019
Iteration 107/1000 | Loss: 0.00002019
Iteration 108/1000 | Loss: 0.00002019
Iteration 109/1000 | Loss: 0.00002019
Iteration 110/1000 | Loss: 0.00002019
Iteration 111/1000 | Loss: 0.00002018
Iteration 112/1000 | Loss: 0.00002018
Iteration 113/1000 | Loss: 0.00002018
Iteration 114/1000 | Loss: 0.00002018
Iteration 115/1000 | Loss: 0.00002018
Iteration 116/1000 | Loss: 0.00002018
Iteration 117/1000 | Loss: 0.00002018
Iteration 118/1000 | Loss: 0.00002018
Iteration 119/1000 | Loss: 0.00002018
Iteration 120/1000 | Loss: 0.00002018
Iteration 121/1000 | Loss: 0.00002018
Iteration 122/1000 | Loss: 0.00002018
Iteration 123/1000 | Loss: 0.00002018
Iteration 124/1000 | Loss: 0.00002018
Iteration 125/1000 | Loss: 0.00002018
Iteration 126/1000 | Loss: 0.00002018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.0178977138129994e-05, 2.0178977138129994e-05, 2.0178977138129994e-05, 2.0178977138129994e-05, 2.0178977138129994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0178977138129994e-05

Optimization complete. Final v2v error: 3.7627334594726562 mm

Highest mean error: 5.368077278137207 mm for frame 176

Lowest mean error: 2.8136541843414307 mm for frame 129

Saving results

Total time: 47.18038988113403
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817896
Iteration 2/25 | Loss: 0.00079316
Iteration 3/25 | Loss: 0.00061740
Iteration 4/25 | Loss: 0.00059745
Iteration 5/25 | Loss: 0.00059174
Iteration 6/25 | Loss: 0.00059027
Iteration 7/25 | Loss: 0.00059003
Iteration 8/25 | Loss: 0.00059003
Iteration 9/25 | Loss: 0.00059003
Iteration 10/25 | Loss: 0.00059003
Iteration 11/25 | Loss: 0.00059003
Iteration 12/25 | Loss: 0.00059003
Iteration 13/25 | Loss: 0.00059003
Iteration 14/25 | Loss: 0.00059003
Iteration 15/25 | Loss: 0.00059003
Iteration 16/25 | Loss: 0.00059003
Iteration 17/25 | Loss: 0.00059003
Iteration 18/25 | Loss: 0.00059003
Iteration 19/25 | Loss: 0.00059003
Iteration 20/25 | Loss: 0.00059003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005900305113755167, 0.0005900305113755167, 0.0005900305113755167, 0.0005900305113755167, 0.0005900305113755167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005900305113755167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46781409
Iteration 2/25 | Loss: 0.00029808
Iteration 3/25 | Loss: 0.00029808
Iteration 4/25 | Loss: 0.00029808
Iteration 5/25 | Loss: 0.00029808
Iteration 6/25 | Loss: 0.00029808
Iteration 7/25 | Loss: 0.00029808
Iteration 8/25 | Loss: 0.00029808
Iteration 9/25 | Loss: 0.00029808
Iteration 10/25 | Loss: 0.00029808
Iteration 11/25 | Loss: 0.00029808
Iteration 12/25 | Loss: 0.00029808
Iteration 13/25 | Loss: 0.00029808
Iteration 14/25 | Loss: 0.00029808
Iteration 15/25 | Loss: 0.00029808
Iteration 16/25 | Loss: 0.00029808
Iteration 17/25 | Loss: 0.00029808
Iteration 18/25 | Loss: 0.00029808
Iteration 19/25 | Loss: 0.00029808
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0002980750286951661, 0.0002980750286951661, 0.0002980750286951661, 0.0002980750286951661, 0.0002980750286951661]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002980750286951661

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029808
Iteration 2/1000 | Loss: 0.00001864
Iteration 3/1000 | Loss: 0.00001243
Iteration 4/1000 | Loss: 0.00001140
Iteration 5/1000 | Loss: 0.00001077
Iteration 6/1000 | Loss: 0.00001047
Iteration 7/1000 | Loss: 0.00001026
Iteration 8/1000 | Loss: 0.00001018
Iteration 9/1000 | Loss: 0.00001015
Iteration 10/1000 | Loss: 0.00001011
Iteration 11/1000 | Loss: 0.00001008
Iteration 12/1000 | Loss: 0.00001007
Iteration 13/1000 | Loss: 0.00001007
Iteration 14/1000 | Loss: 0.00001007
Iteration 15/1000 | Loss: 0.00001007
Iteration 16/1000 | Loss: 0.00001006
Iteration 17/1000 | Loss: 0.00001006
Iteration 18/1000 | Loss: 0.00001006
Iteration 19/1000 | Loss: 0.00001005
Iteration 20/1000 | Loss: 0.00001005
Iteration 21/1000 | Loss: 0.00001004
Iteration 22/1000 | Loss: 0.00001004
Iteration 23/1000 | Loss: 0.00001003
Iteration 24/1000 | Loss: 0.00001002
Iteration 25/1000 | Loss: 0.00001002
Iteration 26/1000 | Loss: 0.00001001
Iteration 27/1000 | Loss: 0.00000998
Iteration 28/1000 | Loss: 0.00000998
Iteration 29/1000 | Loss: 0.00000998
Iteration 30/1000 | Loss: 0.00000997
Iteration 31/1000 | Loss: 0.00000997
Iteration 32/1000 | Loss: 0.00000997
Iteration 33/1000 | Loss: 0.00000996
Iteration 34/1000 | Loss: 0.00000996
Iteration 35/1000 | Loss: 0.00000995
Iteration 36/1000 | Loss: 0.00000995
Iteration 37/1000 | Loss: 0.00000994
Iteration 38/1000 | Loss: 0.00000994
Iteration 39/1000 | Loss: 0.00000994
Iteration 40/1000 | Loss: 0.00000994
Iteration 41/1000 | Loss: 0.00000993
Iteration 42/1000 | Loss: 0.00000993
Iteration 43/1000 | Loss: 0.00000993
Iteration 44/1000 | Loss: 0.00000992
Iteration 45/1000 | Loss: 0.00000991
Iteration 46/1000 | Loss: 0.00000991
Iteration 47/1000 | Loss: 0.00000991
Iteration 48/1000 | Loss: 0.00000991
Iteration 49/1000 | Loss: 0.00000991
Iteration 50/1000 | Loss: 0.00000990
Iteration 51/1000 | Loss: 0.00000990
Iteration 52/1000 | Loss: 0.00000990
Iteration 53/1000 | Loss: 0.00000990
Iteration 54/1000 | Loss: 0.00000989
Iteration 55/1000 | Loss: 0.00000988
Iteration 56/1000 | Loss: 0.00000988
Iteration 57/1000 | Loss: 0.00000988
Iteration 58/1000 | Loss: 0.00000988
Iteration 59/1000 | Loss: 0.00000987
Iteration 60/1000 | Loss: 0.00000987
Iteration 61/1000 | Loss: 0.00000987
Iteration 62/1000 | Loss: 0.00000984
Iteration 63/1000 | Loss: 0.00000984
Iteration 64/1000 | Loss: 0.00000984
Iteration 65/1000 | Loss: 0.00000984
Iteration 66/1000 | Loss: 0.00000983
Iteration 67/1000 | Loss: 0.00000983
Iteration 68/1000 | Loss: 0.00000983
Iteration 69/1000 | Loss: 0.00000983
Iteration 70/1000 | Loss: 0.00000983
Iteration 71/1000 | Loss: 0.00000983
Iteration 72/1000 | Loss: 0.00000983
Iteration 73/1000 | Loss: 0.00000983
Iteration 74/1000 | Loss: 0.00000983
Iteration 75/1000 | Loss: 0.00000982
Iteration 76/1000 | Loss: 0.00000982
Iteration 77/1000 | Loss: 0.00000982
Iteration 78/1000 | Loss: 0.00000982
Iteration 79/1000 | Loss: 0.00000982
Iteration 80/1000 | Loss: 0.00000982
Iteration 81/1000 | Loss: 0.00000982
Iteration 82/1000 | Loss: 0.00000982
Iteration 83/1000 | Loss: 0.00000982
Iteration 84/1000 | Loss: 0.00000982
Iteration 85/1000 | Loss: 0.00000982
Iteration 86/1000 | Loss: 0.00000982
Iteration 87/1000 | Loss: 0.00000982
Iteration 88/1000 | Loss: 0.00000981
Iteration 89/1000 | Loss: 0.00000981
Iteration 90/1000 | Loss: 0.00000981
Iteration 91/1000 | Loss: 0.00000981
Iteration 92/1000 | Loss: 0.00000981
Iteration 93/1000 | Loss: 0.00000981
Iteration 94/1000 | Loss: 0.00000981
Iteration 95/1000 | Loss: 0.00000980
Iteration 96/1000 | Loss: 0.00000980
Iteration 97/1000 | Loss: 0.00000980
Iteration 98/1000 | Loss: 0.00000980
Iteration 99/1000 | Loss: 0.00000980
Iteration 100/1000 | Loss: 0.00000980
Iteration 101/1000 | Loss: 0.00000980
Iteration 102/1000 | Loss: 0.00000980
Iteration 103/1000 | Loss: 0.00000980
Iteration 104/1000 | Loss: 0.00000980
Iteration 105/1000 | Loss: 0.00000980
Iteration 106/1000 | Loss: 0.00000980
Iteration 107/1000 | Loss: 0.00000980
Iteration 108/1000 | Loss: 0.00000980
Iteration 109/1000 | Loss: 0.00000980
Iteration 110/1000 | Loss: 0.00000980
Iteration 111/1000 | Loss: 0.00000980
Iteration 112/1000 | Loss: 0.00000980
Iteration 113/1000 | Loss: 0.00000980
Iteration 114/1000 | Loss: 0.00000980
Iteration 115/1000 | Loss: 0.00000979
Iteration 116/1000 | Loss: 0.00000979
Iteration 117/1000 | Loss: 0.00000979
Iteration 118/1000 | Loss: 0.00000979
Iteration 119/1000 | Loss: 0.00000979
Iteration 120/1000 | Loss: 0.00000979
Iteration 121/1000 | Loss: 0.00000979
Iteration 122/1000 | Loss: 0.00000979
Iteration 123/1000 | Loss: 0.00000979
Iteration 124/1000 | Loss: 0.00000978
Iteration 125/1000 | Loss: 0.00000978
Iteration 126/1000 | Loss: 0.00000978
Iteration 127/1000 | Loss: 0.00000978
Iteration 128/1000 | Loss: 0.00000978
Iteration 129/1000 | Loss: 0.00000978
Iteration 130/1000 | Loss: 0.00000978
Iteration 131/1000 | Loss: 0.00000978
Iteration 132/1000 | Loss: 0.00000978
Iteration 133/1000 | Loss: 0.00000978
Iteration 134/1000 | Loss: 0.00000978
Iteration 135/1000 | Loss: 0.00000978
Iteration 136/1000 | Loss: 0.00000978
Iteration 137/1000 | Loss: 0.00000978
Iteration 138/1000 | Loss: 0.00000978
Iteration 139/1000 | Loss: 0.00000978
Iteration 140/1000 | Loss: 0.00000978
Iteration 141/1000 | Loss: 0.00000978
Iteration 142/1000 | Loss: 0.00000978
Iteration 143/1000 | Loss: 0.00000978
Iteration 144/1000 | Loss: 0.00000978
Iteration 145/1000 | Loss: 0.00000978
Iteration 146/1000 | Loss: 0.00000978
Iteration 147/1000 | Loss: 0.00000978
Iteration 148/1000 | Loss: 0.00000978
Iteration 149/1000 | Loss: 0.00000978
Iteration 150/1000 | Loss: 0.00000978
Iteration 151/1000 | Loss: 0.00000978
Iteration 152/1000 | Loss: 0.00000978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [9.778649655345362e-06, 9.778649655345362e-06, 9.778649655345362e-06, 9.778649655345362e-06, 9.778649655345362e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.778649655345362e-06

Optimization complete. Final v2v error: 2.646578788757324 mm

Highest mean error: 2.815831422805786 mm for frame 45

Lowest mean error: 2.529913902282715 mm for frame 160

Saving results

Total time: 32.236151933670044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871540
Iteration 2/25 | Loss: 0.00124501
Iteration 3/25 | Loss: 0.00079250
Iteration 4/25 | Loss: 0.00072368
Iteration 5/25 | Loss: 0.00071053
Iteration 6/25 | Loss: 0.00070710
Iteration 7/25 | Loss: 0.00070630
Iteration 8/25 | Loss: 0.00070630
Iteration 9/25 | Loss: 0.00070630
Iteration 10/25 | Loss: 0.00070630
Iteration 11/25 | Loss: 0.00070630
Iteration 12/25 | Loss: 0.00070630
Iteration 13/25 | Loss: 0.00070630
Iteration 14/25 | Loss: 0.00070630
Iteration 15/25 | Loss: 0.00070630
Iteration 16/25 | Loss: 0.00070630
Iteration 17/25 | Loss: 0.00070630
Iteration 18/25 | Loss: 0.00070630
Iteration 19/25 | Loss: 0.00070630
Iteration 20/25 | Loss: 0.00070630
Iteration 21/25 | Loss: 0.00070630
Iteration 22/25 | Loss: 0.00070630
Iteration 23/25 | Loss: 0.00070630
Iteration 24/25 | Loss: 0.00070630
Iteration 25/25 | Loss: 0.00070630

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21057665
Iteration 2/25 | Loss: 0.00028804
Iteration 3/25 | Loss: 0.00028801
Iteration 4/25 | Loss: 0.00028801
Iteration 5/25 | Loss: 0.00028801
Iteration 6/25 | Loss: 0.00028801
Iteration 7/25 | Loss: 0.00028801
Iteration 8/25 | Loss: 0.00028801
Iteration 9/25 | Loss: 0.00028801
Iteration 10/25 | Loss: 0.00028801
Iteration 11/25 | Loss: 0.00028801
Iteration 12/25 | Loss: 0.00028801
Iteration 13/25 | Loss: 0.00028801
Iteration 14/25 | Loss: 0.00028801
Iteration 15/25 | Loss: 0.00028801
Iteration 16/25 | Loss: 0.00028801
Iteration 17/25 | Loss: 0.00028801
Iteration 18/25 | Loss: 0.00028801
Iteration 19/25 | Loss: 0.00028801
Iteration 20/25 | Loss: 0.00028801
Iteration 21/25 | Loss: 0.00028801
Iteration 22/25 | Loss: 0.00028801
Iteration 23/25 | Loss: 0.00028801
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002880064712371677, 0.0002880064712371677, 0.0002880064712371677, 0.0002880064712371677, 0.0002880064712371677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002880064712371677

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028801
Iteration 2/1000 | Loss: 0.00002622
Iteration 3/1000 | Loss: 0.00001893
Iteration 4/1000 | Loss: 0.00001705
Iteration 5/1000 | Loss: 0.00001613
Iteration 6/1000 | Loss: 0.00001563
Iteration 7/1000 | Loss: 0.00001528
Iteration 8/1000 | Loss: 0.00001506
Iteration 9/1000 | Loss: 0.00001481
Iteration 10/1000 | Loss: 0.00001471
Iteration 11/1000 | Loss: 0.00001467
Iteration 12/1000 | Loss: 0.00001465
Iteration 13/1000 | Loss: 0.00001457
Iteration 14/1000 | Loss: 0.00001451
Iteration 15/1000 | Loss: 0.00001450
Iteration 16/1000 | Loss: 0.00001450
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001449
Iteration 19/1000 | Loss: 0.00001449
Iteration 20/1000 | Loss: 0.00001448
Iteration 21/1000 | Loss: 0.00001448
Iteration 22/1000 | Loss: 0.00001448
Iteration 23/1000 | Loss: 0.00001447
Iteration 24/1000 | Loss: 0.00001447
Iteration 25/1000 | Loss: 0.00001447
Iteration 26/1000 | Loss: 0.00001446
Iteration 27/1000 | Loss: 0.00001446
Iteration 28/1000 | Loss: 0.00001446
Iteration 29/1000 | Loss: 0.00001445
Iteration 30/1000 | Loss: 0.00001445
Iteration 31/1000 | Loss: 0.00001445
Iteration 32/1000 | Loss: 0.00001444
Iteration 33/1000 | Loss: 0.00001443
Iteration 34/1000 | Loss: 0.00001443
Iteration 35/1000 | Loss: 0.00001442
Iteration 36/1000 | Loss: 0.00001442
Iteration 37/1000 | Loss: 0.00001442
Iteration 38/1000 | Loss: 0.00001441
Iteration 39/1000 | Loss: 0.00001441
Iteration 40/1000 | Loss: 0.00001441
Iteration 41/1000 | Loss: 0.00001440
Iteration 42/1000 | Loss: 0.00001440
Iteration 43/1000 | Loss: 0.00001440
Iteration 44/1000 | Loss: 0.00001439
Iteration 45/1000 | Loss: 0.00001439
Iteration 46/1000 | Loss: 0.00001439
Iteration 47/1000 | Loss: 0.00001438
Iteration 48/1000 | Loss: 0.00001438
Iteration 49/1000 | Loss: 0.00001438
Iteration 50/1000 | Loss: 0.00001437
Iteration 51/1000 | Loss: 0.00001437
Iteration 52/1000 | Loss: 0.00001437
Iteration 53/1000 | Loss: 0.00001437
Iteration 54/1000 | Loss: 0.00001436
Iteration 55/1000 | Loss: 0.00001436
Iteration 56/1000 | Loss: 0.00001436
Iteration 57/1000 | Loss: 0.00001435
Iteration 58/1000 | Loss: 0.00001435
Iteration 59/1000 | Loss: 0.00001435
Iteration 60/1000 | Loss: 0.00001435
Iteration 61/1000 | Loss: 0.00001434
Iteration 62/1000 | Loss: 0.00001434
Iteration 63/1000 | Loss: 0.00001434
Iteration 64/1000 | Loss: 0.00001434
Iteration 65/1000 | Loss: 0.00001433
Iteration 66/1000 | Loss: 0.00001433
Iteration 67/1000 | Loss: 0.00001433
Iteration 68/1000 | Loss: 0.00001432
Iteration 69/1000 | Loss: 0.00001432
Iteration 70/1000 | Loss: 0.00001432
Iteration 71/1000 | Loss: 0.00001431
Iteration 72/1000 | Loss: 0.00001431
Iteration 73/1000 | Loss: 0.00001431
Iteration 74/1000 | Loss: 0.00001431
Iteration 75/1000 | Loss: 0.00001431
Iteration 76/1000 | Loss: 0.00001431
Iteration 77/1000 | Loss: 0.00001431
Iteration 78/1000 | Loss: 0.00001431
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001430
Iteration 81/1000 | Loss: 0.00001430
Iteration 82/1000 | Loss: 0.00001430
Iteration 83/1000 | Loss: 0.00001430
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001430
Iteration 86/1000 | Loss: 0.00001430
Iteration 87/1000 | Loss: 0.00001430
Iteration 88/1000 | Loss: 0.00001430
Iteration 89/1000 | Loss: 0.00001430
Iteration 90/1000 | Loss: 0.00001430
Iteration 91/1000 | Loss: 0.00001430
Iteration 92/1000 | Loss: 0.00001429
Iteration 93/1000 | Loss: 0.00001429
Iteration 94/1000 | Loss: 0.00001429
Iteration 95/1000 | Loss: 0.00001429
Iteration 96/1000 | Loss: 0.00001429
Iteration 97/1000 | Loss: 0.00001429
Iteration 98/1000 | Loss: 0.00001429
Iteration 99/1000 | Loss: 0.00001429
Iteration 100/1000 | Loss: 0.00001429
Iteration 101/1000 | Loss: 0.00001429
Iteration 102/1000 | Loss: 0.00001429
Iteration 103/1000 | Loss: 0.00001429
Iteration 104/1000 | Loss: 0.00001429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.4290309081843589e-05, 1.4290309081843589e-05, 1.4290309081843589e-05, 1.4290309081843589e-05, 1.4290309081843589e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4290309081843589e-05

Optimization complete. Final v2v error: 3.240247964859009 mm

Highest mean error: 3.616054058074951 mm for frame 219

Lowest mean error: 2.8732237815856934 mm for frame 203

Saving results

Total time: 37.94360661506653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395665
Iteration 2/25 | Loss: 0.00095840
Iteration 3/25 | Loss: 0.00065455
Iteration 4/25 | Loss: 0.00060998
Iteration 5/25 | Loss: 0.00060044
Iteration 6/25 | Loss: 0.00059791
Iteration 7/25 | Loss: 0.00059733
Iteration 8/25 | Loss: 0.00059730
Iteration 9/25 | Loss: 0.00059730
Iteration 10/25 | Loss: 0.00059730
Iteration 11/25 | Loss: 0.00059730
Iteration 12/25 | Loss: 0.00059730
Iteration 13/25 | Loss: 0.00059730
Iteration 14/25 | Loss: 0.00059730
Iteration 15/25 | Loss: 0.00059730
Iteration 16/25 | Loss: 0.00059730
Iteration 17/25 | Loss: 0.00059730
Iteration 18/25 | Loss: 0.00059730
Iteration 19/25 | Loss: 0.00059730
Iteration 20/25 | Loss: 0.00059730
Iteration 21/25 | Loss: 0.00059730
Iteration 22/25 | Loss: 0.00059730
Iteration 23/25 | Loss: 0.00059730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000597299775108695, 0.000597299775108695, 0.000597299775108695, 0.000597299775108695, 0.000597299775108695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000597299775108695

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45546293
Iteration 2/25 | Loss: 0.00025106
Iteration 3/25 | Loss: 0.00025106
Iteration 4/25 | Loss: 0.00025106
Iteration 5/25 | Loss: 0.00025105
Iteration 6/25 | Loss: 0.00025105
Iteration 7/25 | Loss: 0.00025105
Iteration 8/25 | Loss: 0.00025105
Iteration 9/25 | Loss: 0.00025105
Iteration 10/25 | Loss: 0.00025105
Iteration 11/25 | Loss: 0.00025105
Iteration 12/25 | Loss: 0.00025105
Iteration 13/25 | Loss: 0.00025105
Iteration 14/25 | Loss: 0.00025105
Iteration 15/25 | Loss: 0.00025105
Iteration 16/25 | Loss: 0.00025105
Iteration 17/25 | Loss: 0.00025105
Iteration 18/25 | Loss: 0.00025105
Iteration 19/25 | Loss: 0.00025105
Iteration 20/25 | Loss: 0.00025105
Iteration 21/25 | Loss: 0.00025105
Iteration 22/25 | Loss: 0.00025105
Iteration 23/25 | Loss: 0.00025105
Iteration 24/25 | Loss: 0.00025105
Iteration 25/25 | Loss: 0.00025105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025105
Iteration 2/1000 | Loss: 0.00002035
Iteration 3/1000 | Loss: 0.00001495
Iteration 4/1000 | Loss: 0.00001367
Iteration 5/1000 | Loss: 0.00001284
Iteration 6/1000 | Loss: 0.00001249
Iteration 7/1000 | Loss: 0.00001209
Iteration 8/1000 | Loss: 0.00001188
Iteration 9/1000 | Loss: 0.00001187
Iteration 10/1000 | Loss: 0.00001175
Iteration 11/1000 | Loss: 0.00001172
Iteration 12/1000 | Loss: 0.00001170
Iteration 13/1000 | Loss: 0.00001169
Iteration 14/1000 | Loss: 0.00001165
Iteration 15/1000 | Loss: 0.00001163
Iteration 16/1000 | Loss: 0.00001163
Iteration 17/1000 | Loss: 0.00001162
Iteration 18/1000 | Loss: 0.00001161
Iteration 19/1000 | Loss: 0.00001160
Iteration 20/1000 | Loss: 0.00001157
Iteration 21/1000 | Loss: 0.00001156
Iteration 22/1000 | Loss: 0.00001155
Iteration 23/1000 | Loss: 0.00001154
Iteration 24/1000 | Loss: 0.00001153
Iteration 25/1000 | Loss: 0.00001153
Iteration 26/1000 | Loss: 0.00001153
Iteration 27/1000 | Loss: 0.00001152
Iteration 28/1000 | Loss: 0.00001151
Iteration 29/1000 | Loss: 0.00001150
Iteration 30/1000 | Loss: 0.00001150
Iteration 31/1000 | Loss: 0.00001149
Iteration 32/1000 | Loss: 0.00001149
Iteration 33/1000 | Loss: 0.00001148
Iteration 34/1000 | Loss: 0.00001148
Iteration 35/1000 | Loss: 0.00001147
Iteration 36/1000 | Loss: 0.00001147
Iteration 37/1000 | Loss: 0.00001147
Iteration 38/1000 | Loss: 0.00001147
Iteration 39/1000 | Loss: 0.00001147
Iteration 40/1000 | Loss: 0.00001146
Iteration 41/1000 | Loss: 0.00001146
Iteration 42/1000 | Loss: 0.00001146
Iteration 43/1000 | Loss: 0.00001146
Iteration 44/1000 | Loss: 0.00001146
Iteration 45/1000 | Loss: 0.00001145
Iteration 46/1000 | Loss: 0.00001144
Iteration 47/1000 | Loss: 0.00001144
Iteration 48/1000 | Loss: 0.00001144
Iteration 49/1000 | Loss: 0.00001144
Iteration 50/1000 | Loss: 0.00001143
Iteration 51/1000 | Loss: 0.00001143
Iteration 52/1000 | Loss: 0.00001143
Iteration 53/1000 | Loss: 0.00001142
Iteration 54/1000 | Loss: 0.00001142
Iteration 55/1000 | Loss: 0.00001142
Iteration 56/1000 | Loss: 0.00001141
Iteration 57/1000 | Loss: 0.00001141
Iteration 58/1000 | Loss: 0.00001141
Iteration 59/1000 | Loss: 0.00001140
Iteration 60/1000 | Loss: 0.00001140
Iteration 61/1000 | Loss: 0.00001140
Iteration 62/1000 | Loss: 0.00001139
Iteration 63/1000 | Loss: 0.00001139
Iteration 64/1000 | Loss: 0.00001139
Iteration 65/1000 | Loss: 0.00001138
Iteration 66/1000 | Loss: 0.00001138
Iteration 67/1000 | Loss: 0.00001138
Iteration 68/1000 | Loss: 0.00001138
Iteration 69/1000 | Loss: 0.00001138
Iteration 70/1000 | Loss: 0.00001137
Iteration 71/1000 | Loss: 0.00001137
Iteration 72/1000 | Loss: 0.00001137
Iteration 73/1000 | Loss: 0.00001136
Iteration 74/1000 | Loss: 0.00001136
Iteration 75/1000 | Loss: 0.00001136
Iteration 76/1000 | Loss: 0.00001136
Iteration 77/1000 | Loss: 0.00001135
Iteration 78/1000 | Loss: 0.00001135
Iteration 79/1000 | Loss: 0.00001135
Iteration 80/1000 | Loss: 0.00001135
Iteration 81/1000 | Loss: 0.00001135
Iteration 82/1000 | Loss: 0.00001135
Iteration 83/1000 | Loss: 0.00001135
Iteration 84/1000 | Loss: 0.00001135
Iteration 85/1000 | Loss: 0.00001134
Iteration 86/1000 | Loss: 0.00001134
Iteration 87/1000 | Loss: 0.00001134
Iteration 88/1000 | Loss: 0.00001134
Iteration 89/1000 | Loss: 0.00001134
Iteration 90/1000 | Loss: 0.00001134
Iteration 91/1000 | Loss: 0.00001133
Iteration 92/1000 | Loss: 0.00001133
Iteration 93/1000 | Loss: 0.00001133
Iteration 94/1000 | Loss: 0.00001133
Iteration 95/1000 | Loss: 0.00001133
Iteration 96/1000 | Loss: 0.00001133
Iteration 97/1000 | Loss: 0.00001133
Iteration 98/1000 | Loss: 0.00001133
Iteration 99/1000 | Loss: 0.00001133
Iteration 100/1000 | Loss: 0.00001133
Iteration 101/1000 | Loss: 0.00001133
Iteration 102/1000 | Loss: 0.00001133
Iteration 103/1000 | Loss: 0.00001133
Iteration 104/1000 | Loss: 0.00001133
Iteration 105/1000 | Loss: 0.00001133
Iteration 106/1000 | Loss: 0.00001133
Iteration 107/1000 | Loss: 0.00001133
Iteration 108/1000 | Loss: 0.00001132
Iteration 109/1000 | Loss: 0.00001132
Iteration 110/1000 | Loss: 0.00001132
Iteration 111/1000 | Loss: 0.00001132
Iteration 112/1000 | Loss: 0.00001132
Iteration 113/1000 | Loss: 0.00001132
Iteration 114/1000 | Loss: 0.00001132
Iteration 115/1000 | Loss: 0.00001132
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001131
Iteration 118/1000 | Loss: 0.00001131
Iteration 119/1000 | Loss: 0.00001131
Iteration 120/1000 | Loss: 0.00001131
Iteration 121/1000 | Loss: 0.00001131
Iteration 122/1000 | Loss: 0.00001131
Iteration 123/1000 | Loss: 0.00001131
Iteration 124/1000 | Loss: 0.00001131
Iteration 125/1000 | Loss: 0.00001131
Iteration 126/1000 | Loss: 0.00001130
Iteration 127/1000 | Loss: 0.00001130
Iteration 128/1000 | Loss: 0.00001130
Iteration 129/1000 | Loss: 0.00001130
Iteration 130/1000 | Loss: 0.00001129
Iteration 131/1000 | Loss: 0.00001129
Iteration 132/1000 | Loss: 0.00001129
Iteration 133/1000 | Loss: 0.00001129
Iteration 134/1000 | Loss: 0.00001129
Iteration 135/1000 | Loss: 0.00001129
Iteration 136/1000 | Loss: 0.00001128
Iteration 137/1000 | Loss: 0.00001128
Iteration 138/1000 | Loss: 0.00001128
Iteration 139/1000 | Loss: 0.00001128
Iteration 140/1000 | Loss: 0.00001128
Iteration 141/1000 | Loss: 0.00001128
Iteration 142/1000 | Loss: 0.00001128
Iteration 143/1000 | Loss: 0.00001128
Iteration 144/1000 | Loss: 0.00001127
Iteration 145/1000 | Loss: 0.00001127
Iteration 146/1000 | Loss: 0.00001127
Iteration 147/1000 | Loss: 0.00001127
Iteration 148/1000 | Loss: 0.00001127
Iteration 149/1000 | Loss: 0.00001127
Iteration 150/1000 | Loss: 0.00001127
Iteration 151/1000 | Loss: 0.00001127
Iteration 152/1000 | Loss: 0.00001127
Iteration 153/1000 | Loss: 0.00001127
Iteration 154/1000 | Loss: 0.00001127
Iteration 155/1000 | Loss: 0.00001127
Iteration 156/1000 | Loss: 0.00001127
Iteration 157/1000 | Loss: 0.00001127
Iteration 158/1000 | Loss: 0.00001126
Iteration 159/1000 | Loss: 0.00001126
Iteration 160/1000 | Loss: 0.00001126
Iteration 161/1000 | Loss: 0.00001126
Iteration 162/1000 | Loss: 0.00001126
Iteration 163/1000 | Loss: 0.00001126
Iteration 164/1000 | Loss: 0.00001126
Iteration 165/1000 | Loss: 0.00001126
Iteration 166/1000 | Loss: 0.00001125
Iteration 167/1000 | Loss: 0.00001125
Iteration 168/1000 | Loss: 0.00001125
Iteration 169/1000 | Loss: 0.00001125
Iteration 170/1000 | Loss: 0.00001125
Iteration 171/1000 | Loss: 0.00001125
Iteration 172/1000 | Loss: 0.00001125
Iteration 173/1000 | Loss: 0.00001125
Iteration 174/1000 | Loss: 0.00001125
Iteration 175/1000 | Loss: 0.00001125
Iteration 176/1000 | Loss: 0.00001125
Iteration 177/1000 | Loss: 0.00001125
Iteration 178/1000 | Loss: 0.00001125
Iteration 179/1000 | Loss: 0.00001124
Iteration 180/1000 | Loss: 0.00001124
Iteration 181/1000 | Loss: 0.00001124
Iteration 182/1000 | Loss: 0.00001124
Iteration 183/1000 | Loss: 0.00001124
Iteration 184/1000 | Loss: 0.00001124
Iteration 185/1000 | Loss: 0.00001124
Iteration 186/1000 | Loss: 0.00001124
Iteration 187/1000 | Loss: 0.00001124
Iteration 188/1000 | Loss: 0.00001124
Iteration 189/1000 | Loss: 0.00001124
Iteration 190/1000 | Loss: 0.00001124
Iteration 191/1000 | Loss: 0.00001124
Iteration 192/1000 | Loss: 0.00001124
Iteration 193/1000 | Loss: 0.00001124
Iteration 194/1000 | Loss: 0.00001124
Iteration 195/1000 | Loss: 0.00001124
Iteration 196/1000 | Loss: 0.00001124
Iteration 197/1000 | Loss: 0.00001124
Iteration 198/1000 | Loss: 0.00001124
Iteration 199/1000 | Loss: 0.00001124
Iteration 200/1000 | Loss: 0.00001124
Iteration 201/1000 | Loss: 0.00001124
Iteration 202/1000 | Loss: 0.00001124
Iteration 203/1000 | Loss: 0.00001124
Iteration 204/1000 | Loss: 0.00001124
Iteration 205/1000 | Loss: 0.00001124
Iteration 206/1000 | Loss: 0.00001124
Iteration 207/1000 | Loss: 0.00001124
Iteration 208/1000 | Loss: 0.00001124
Iteration 209/1000 | Loss: 0.00001124
Iteration 210/1000 | Loss: 0.00001124
Iteration 211/1000 | Loss: 0.00001124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.1238676052016672e-05, 1.1238676052016672e-05, 1.1238676052016672e-05, 1.1238676052016672e-05, 1.1238676052016672e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1238676052016672e-05

Optimization complete. Final v2v error: 2.877241849899292 mm

Highest mean error: 2.9565532207489014 mm for frame 85

Lowest mean error: 2.810053825378418 mm for frame 105

Saving results

Total time: 38.65432333946228
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789389
Iteration 2/25 | Loss: 0.00109658
Iteration 3/25 | Loss: 0.00071134
Iteration 4/25 | Loss: 0.00067693
Iteration 5/25 | Loss: 0.00066899
Iteration 6/25 | Loss: 0.00066717
Iteration 7/25 | Loss: 0.00066694
Iteration 8/25 | Loss: 0.00066694
Iteration 9/25 | Loss: 0.00066694
Iteration 10/25 | Loss: 0.00066694
Iteration 11/25 | Loss: 0.00066694
Iteration 12/25 | Loss: 0.00066694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006669435533694923, 0.0006669435533694923, 0.0006669435533694923, 0.0006669435533694923, 0.0006669435533694923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006669435533694923

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49605584
Iteration 2/25 | Loss: 0.00028971
Iteration 3/25 | Loss: 0.00028971
Iteration 4/25 | Loss: 0.00028971
Iteration 5/25 | Loss: 0.00028971
Iteration 6/25 | Loss: 0.00028971
Iteration 7/25 | Loss: 0.00028971
Iteration 8/25 | Loss: 0.00028970
Iteration 9/25 | Loss: 0.00028970
Iteration 10/25 | Loss: 0.00028970
Iteration 11/25 | Loss: 0.00028970
Iteration 12/25 | Loss: 0.00028970
Iteration 13/25 | Loss: 0.00028970
Iteration 14/25 | Loss: 0.00028970
Iteration 15/25 | Loss: 0.00028970
Iteration 16/25 | Loss: 0.00028970
Iteration 17/25 | Loss: 0.00028970
Iteration 18/25 | Loss: 0.00028970
Iteration 19/25 | Loss: 0.00028970
Iteration 20/25 | Loss: 0.00028970
Iteration 21/25 | Loss: 0.00028970
Iteration 22/25 | Loss: 0.00028970
Iteration 23/25 | Loss: 0.00028970
Iteration 24/25 | Loss: 0.00028970
Iteration 25/25 | Loss: 0.00028970

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028970
Iteration 2/1000 | Loss: 0.00003344
Iteration 3/1000 | Loss: 0.00002560
Iteration 4/1000 | Loss: 0.00002290
Iteration 5/1000 | Loss: 0.00002160
Iteration 6/1000 | Loss: 0.00002069
Iteration 7/1000 | Loss: 0.00002006
Iteration 8/1000 | Loss: 0.00001952
Iteration 9/1000 | Loss: 0.00001908
Iteration 10/1000 | Loss: 0.00001881
Iteration 11/1000 | Loss: 0.00001865
Iteration 12/1000 | Loss: 0.00001862
Iteration 13/1000 | Loss: 0.00001858
Iteration 14/1000 | Loss: 0.00001858
Iteration 15/1000 | Loss: 0.00001857
Iteration 16/1000 | Loss: 0.00001857
Iteration 17/1000 | Loss: 0.00001854
Iteration 18/1000 | Loss: 0.00001848
Iteration 19/1000 | Loss: 0.00001846
Iteration 20/1000 | Loss: 0.00001844
Iteration 21/1000 | Loss: 0.00001844
Iteration 22/1000 | Loss: 0.00001839
Iteration 23/1000 | Loss: 0.00001839
Iteration 24/1000 | Loss: 0.00001834
Iteration 25/1000 | Loss: 0.00001832
Iteration 26/1000 | Loss: 0.00001832
Iteration 27/1000 | Loss: 0.00001831
Iteration 28/1000 | Loss: 0.00001830
Iteration 29/1000 | Loss: 0.00001830
Iteration 30/1000 | Loss: 0.00001829
Iteration 31/1000 | Loss: 0.00001829
Iteration 32/1000 | Loss: 0.00001828
Iteration 33/1000 | Loss: 0.00001827
Iteration 34/1000 | Loss: 0.00001827
Iteration 35/1000 | Loss: 0.00001826
Iteration 36/1000 | Loss: 0.00001826
Iteration 37/1000 | Loss: 0.00001825
Iteration 38/1000 | Loss: 0.00001825
Iteration 39/1000 | Loss: 0.00001824
Iteration 40/1000 | Loss: 0.00001823
Iteration 41/1000 | Loss: 0.00001823
Iteration 42/1000 | Loss: 0.00001822
Iteration 43/1000 | Loss: 0.00001822
Iteration 44/1000 | Loss: 0.00001821
Iteration 45/1000 | Loss: 0.00001821
Iteration 46/1000 | Loss: 0.00001821
Iteration 47/1000 | Loss: 0.00001821
Iteration 48/1000 | Loss: 0.00001821
Iteration 49/1000 | Loss: 0.00001821
Iteration 50/1000 | Loss: 0.00001821
Iteration 51/1000 | Loss: 0.00001821
Iteration 52/1000 | Loss: 0.00001820
Iteration 53/1000 | Loss: 0.00001820
Iteration 54/1000 | Loss: 0.00001820
Iteration 55/1000 | Loss: 0.00001819
Iteration 56/1000 | Loss: 0.00001819
Iteration 57/1000 | Loss: 0.00001819
Iteration 58/1000 | Loss: 0.00001819
Iteration 59/1000 | Loss: 0.00001818
Iteration 60/1000 | Loss: 0.00001818
Iteration 61/1000 | Loss: 0.00001818
Iteration 62/1000 | Loss: 0.00001818
Iteration 63/1000 | Loss: 0.00001818
Iteration 64/1000 | Loss: 0.00001818
Iteration 65/1000 | Loss: 0.00001818
Iteration 66/1000 | Loss: 0.00001818
Iteration 67/1000 | Loss: 0.00001818
Iteration 68/1000 | Loss: 0.00001818
Iteration 69/1000 | Loss: 0.00001817
Iteration 70/1000 | Loss: 0.00001817
Iteration 71/1000 | Loss: 0.00001817
Iteration 72/1000 | Loss: 0.00001817
Iteration 73/1000 | Loss: 0.00001817
Iteration 74/1000 | Loss: 0.00001817
Iteration 75/1000 | Loss: 0.00001817
Iteration 76/1000 | Loss: 0.00001817
Iteration 77/1000 | Loss: 0.00001817
Iteration 78/1000 | Loss: 0.00001817
Iteration 79/1000 | Loss: 0.00001817
Iteration 80/1000 | Loss: 0.00001817
Iteration 81/1000 | Loss: 0.00001816
Iteration 82/1000 | Loss: 0.00001816
Iteration 83/1000 | Loss: 0.00001816
Iteration 84/1000 | Loss: 0.00001816
Iteration 85/1000 | Loss: 0.00001816
Iteration 86/1000 | Loss: 0.00001816
Iteration 87/1000 | Loss: 0.00001816
Iteration 88/1000 | Loss: 0.00001816
Iteration 89/1000 | Loss: 0.00001816
Iteration 90/1000 | Loss: 0.00001816
Iteration 91/1000 | Loss: 0.00001816
Iteration 92/1000 | Loss: 0.00001815
Iteration 93/1000 | Loss: 0.00001815
Iteration 94/1000 | Loss: 0.00001815
Iteration 95/1000 | Loss: 0.00001815
Iteration 96/1000 | Loss: 0.00001815
Iteration 97/1000 | Loss: 0.00001815
Iteration 98/1000 | Loss: 0.00001815
Iteration 99/1000 | Loss: 0.00001815
Iteration 100/1000 | Loss: 0.00001814
Iteration 101/1000 | Loss: 0.00001814
Iteration 102/1000 | Loss: 0.00001814
Iteration 103/1000 | Loss: 0.00001814
Iteration 104/1000 | Loss: 0.00001814
Iteration 105/1000 | Loss: 0.00001814
Iteration 106/1000 | Loss: 0.00001814
Iteration 107/1000 | Loss: 0.00001814
Iteration 108/1000 | Loss: 0.00001814
Iteration 109/1000 | Loss: 0.00001814
Iteration 110/1000 | Loss: 0.00001813
Iteration 111/1000 | Loss: 0.00001813
Iteration 112/1000 | Loss: 0.00001813
Iteration 113/1000 | Loss: 0.00001813
Iteration 114/1000 | Loss: 0.00001813
Iteration 115/1000 | Loss: 0.00001813
Iteration 116/1000 | Loss: 0.00001813
Iteration 117/1000 | Loss: 0.00001813
Iteration 118/1000 | Loss: 0.00001812
Iteration 119/1000 | Loss: 0.00001812
Iteration 120/1000 | Loss: 0.00001812
Iteration 121/1000 | Loss: 0.00001812
Iteration 122/1000 | Loss: 0.00001812
Iteration 123/1000 | Loss: 0.00001811
Iteration 124/1000 | Loss: 0.00001811
Iteration 125/1000 | Loss: 0.00001811
Iteration 126/1000 | Loss: 0.00001811
Iteration 127/1000 | Loss: 0.00001811
Iteration 128/1000 | Loss: 0.00001811
Iteration 129/1000 | Loss: 0.00001811
Iteration 130/1000 | Loss: 0.00001811
Iteration 131/1000 | Loss: 0.00001811
Iteration 132/1000 | Loss: 0.00001811
Iteration 133/1000 | Loss: 0.00001811
Iteration 134/1000 | Loss: 0.00001811
Iteration 135/1000 | Loss: 0.00001811
Iteration 136/1000 | Loss: 0.00001811
Iteration 137/1000 | Loss: 0.00001811
Iteration 138/1000 | Loss: 0.00001811
Iteration 139/1000 | Loss: 0.00001810
Iteration 140/1000 | Loss: 0.00001810
Iteration 141/1000 | Loss: 0.00001810
Iteration 142/1000 | Loss: 0.00001810
Iteration 143/1000 | Loss: 0.00001810
Iteration 144/1000 | Loss: 0.00001810
Iteration 145/1000 | Loss: 0.00001810
Iteration 146/1000 | Loss: 0.00001810
Iteration 147/1000 | Loss: 0.00001810
Iteration 148/1000 | Loss: 0.00001810
Iteration 149/1000 | Loss: 0.00001810
Iteration 150/1000 | Loss: 0.00001810
Iteration 151/1000 | Loss: 0.00001810
Iteration 152/1000 | Loss: 0.00001810
Iteration 153/1000 | Loss: 0.00001810
Iteration 154/1000 | Loss: 0.00001810
Iteration 155/1000 | Loss: 0.00001810
Iteration 156/1000 | Loss: 0.00001810
Iteration 157/1000 | Loss: 0.00001810
Iteration 158/1000 | Loss: 0.00001809
Iteration 159/1000 | Loss: 0.00001809
Iteration 160/1000 | Loss: 0.00001809
Iteration 161/1000 | Loss: 0.00001809
Iteration 162/1000 | Loss: 0.00001809
Iteration 163/1000 | Loss: 0.00001809
Iteration 164/1000 | Loss: 0.00001809
Iteration 165/1000 | Loss: 0.00001809
Iteration 166/1000 | Loss: 0.00001809
Iteration 167/1000 | Loss: 0.00001809
Iteration 168/1000 | Loss: 0.00001809
Iteration 169/1000 | Loss: 0.00001809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.809253080864437e-05, 1.809253080864437e-05, 1.809253080864437e-05, 1.809253080864437e-05, 1.809253080864437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.809253080864437e-05

Optimization complete. Final v2v error: 3.5466983318328857 mm

Highest mean error: 4.7514142990112305 mm for frame 0

Lowest mean error: 3.202155351638794 mm for frame 146

Saving results

Total time: 45.24109888076782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00325922
Iteration 2/25 | Loss: 0.00081337
Iteration 3/25 | Loss: 0.00069884
Iteration 4/25 | Loss: 0.00066212
Iteration 5/25 | Loss: 0.00065279
Iteration 6/25 | Loss: 0.00065087
Iteration 7/25 | Loss: 0.00065016
Iteration 8/25 | Loss: 0.00065016
Iteration 9/25 | Loss: 0.00065016
Iteration 10/25 | Loss: 0.00065016
Iteration 11/25 | Loss: 0.00065016
Iteration 12/25 | Loss: 0.00065016
Iteration 13/25 | Loss: 0.00065016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006501594907604158, 0.0006501594907604158, 0.0006501594907604158, 0.0006501594907604158, 0.0006501594907604158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006501594907604158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.88051128
Iteration 2/25 | Loss: 0.00038355
Iteration 3/25 | Loss: 0.00038351
Iteration 4/25 | Loss: 0.00038351
Iteration 5/25 | Loss: 0.00038350
Iteration 6/25 | Loss: 0.00038350
Iteration 7/25 | Loss: 0.00038350
Iteration 8/25 | Loss: 0.00038350
Iteration 9/25 | Loss: 0.00038350
Iteration 10/25 | Loss: 0.00038350
Iteration 11/25 | Loss: 0.00038350
Iteration 12/25 | Loss: 0.00038350
Iteration 13/25 | Loss: 0.00038350
Iteration 14/25 | Loss: 0.00038350
Iteration 15/25 | Loss: 0.00038350
Iteration 16/25 | Loss: 0.00038350
Iteration 17/25 | Loss: 0.00038350
Iteration 18/25 | Loss: 0.00038350
Iteration 19/25 | Loss: 0.00038350
Iteration 20/25 | Loss: 0.00038350
Iteration 21/25 | Loss: 0.00038350
Iteration 22/25 | Loss: 0.00038350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00038350370596162975, 0.00038350370596162975, 0.00038350370596162975, 0.00038350370596162975, 0.00038350370596162975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038350370596162975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038350
Iteration 2/1000 | Loss: 0.00003083
Iteration 3/1000 | Loss: 0.00002061
Iteration 4/1000 | Loss: 0.00001873
Iteration 5/1000 | Loss: 0.00001775
Iteration 6/1000 | Loss: 0.00001724
Iteration 7/1000 | Loss: 0.00001685
Iteration 8/1000 | Loss: 0.00001640
Iteration 9/1000 | Loss: 0.00001613
Iteration 10/1000 | Loss: 0.00001610
Iteration 11/1000 | Loss: 0.00001590
Iteration 12/1000 | Loss: 0.00001573
Iteration 13/1000 | Loss: 0.00001568
Iteration 14/1000 | Loss: 0.00001567
Iteration 15/1000 | Loss: 0.00001559
Iteration 16/1000 | Loss: 0.00001557
Iteration 17/1000 | Loss: 0.00001551
Iteration 18/1000 | Loss: 0.00001543
Iteration 19/1000 | Loss: 0.00001543
Iteration 20/1000 | Loss: 0.00001543
Iteration 21/1000 | Loss: 0.00001542
Iteration 22/1000 | Loss: 0.00001541
Iteration 23/1000 | Loss: 0.00001541
Iteration 24/1000 | Loss: 0.00001540
Iteration 25/1000 | Loss: 0.00001540
Iteration 26/1000 | Loss: 0.00001540
Iteration 27/1000 | Loss: 0.00001539
Iteration 28/1000 | Loss: 0.00001539
Iteration 29/1000 | Loss: 0.00001539
Iteration 30/1000 | Loss: 0.00001539
Iteration 31/1000 | Loss: 0.00001538
Iteration 32/1000 | Loss: 0.00001538
Iteration 33/1000 | Loss: 0.00001538
Iteration 34/1000 | Loss: 0.00001538
Iteration 35/1000 | Loss: 0.00001538
Iteration 36/1000 | Loss: 0.00001538
Iteration 37/1000 | Loss: 0.00001537
Iteration 38/1000 | Loss: 0.00001537
Iteration 39/1000 | Loss: 0.00001536
Iteration 40/1000 | Loss: 0.00001536
Iteration 41/1000 | Loss: 0.00001536
Iteration 42/1000 | Loss: 0.00001535
Iteration 43/1000 | Loss: 0.00001535
Iteration 44/1000 | Loss: 0.00001535
Iteration 45/1000 | Loss: 0.00001535
Iteration 46/1000 | Loss: 0.00001535
Iteration 47/1000 | Loss: 0.00001534
Iteration 48/1000 | Loss: 0.00001534
Iteration 49/1000 | Loss: 0.00001534
Iteration 50/1000 | Loss: 0.00001534
Iteration 51/1000 | Loss: 0.00001533
Iteration 52/1000 | Loss: 0.00001533
Iteration 53/1000 | Loss: 0.00001533
Iteration 54/1000 | Loss: 0.00001532
Iteration 55/1000 | Loss: 0.00001532
Iteration 56/1000 | Loss: 0.00001531
Iteration 57/1000 | Loss: 0.00001531
Iteration 58/1000 | Loss: 0.00001531
Iteration 59/1000 | Loss: 0.00001531
Iteration 60/1000 | Loss: 0.00001531
Iteration 61/1000 | Loss: 0.00001531
Iteration 62/1000 | Loss: 0.00001531
Iteration 63/1000 | Loss: 0.00001530
Iteration 64/1000 | Loss: 0.00001530
Iteration 65/1000 | Loss: 0.00001530
Iteration 66/1000 | Loss: 0.00001530
Iteration 67/1000 | Loss: 0.00001530
Iteration 68/1000 | Loss: 0.00001530
Iteration 69/1000 | Loss: 0.00001529
Iteration 70/1000 | Loss: 0.00001529
Iteration 71/1000 | Loss: 0.00001529
Iteration 72/1000 | Loss: 0.00001529
Iteration 73/1000 | Loss: 0.00001528
Iteration 74/1000 | Loss: 0.00001528
Iteration 75/1000 | Loss: 0.00001528
Iteration 76/1000 | Loss: 0.00001527
Iteration 77/1000 | Loss: 0.00001527
Iteration 78/1000 | Loss: 0.00001527
Iteration 79/1000 | Loss: 0.00001527
Iteration 80/1000 | Loss: 0.00001526
Iteration 81/1000 | Loss: 0.00001526
Iteration 82/1000 | Loss: 0.00001526
Iteration 83/1000 | Loss: 0.00001526
Iteration 84/1000 | Loss: 0.00001525
Iteration 85/1000 | Loss: 0.00001525
Iteration 86/1000 | Loss: 0.00001525
Iteration 87/1000 | Loss: 0.00001525
Iteration 88/1000 | Loss: 0.00001525
Iteration 89/1000 | Loss: 0.00001525
Iteration 90/1000 | Loss: 0.00001524
Iteration 91/1000 | Loss: 0.00001524
Iteration 92/1000 | Loss: 0.00001524
Iteration 93/1000 | Loss: 0.00001524
Iteration 94/1000 | Loss: 0.00001524
Iteration 95/1000 | Loss: 0.00001524
Iteration 96/1000 | Loss: 0.00001524
Iteration 97/1000 | Loss: 0.00001523
Iteration 98/1000 | Loss: 0.00001523
Iteration 99/1000 | Loss: 0.00001523
Iteration 100/1000 | Loss: 0.00001523
Iteration 101/1000 | Loss: 0.00001523
Iteration 102/1000 | Loss: 0.00001523
Iteration 103/1000 | Loss: 0.00001523
Iteration 104/1000 | Loss: 0.00001523
Iteration 105/1000 | Loss: 0.00001523
Iteration 106/1000 | Loss: 0.00001522
Iteration 107/1000 | Loss: 0.00001522
Iteration 108/1000 | Loss: 0.00001522
Iteration 109/1000 | Loss: 0.00001522
Iteration 110/1000 | Loss: 0.00001522
Iteration 111/1000 | Loss: 0.00001522
Iteration 112/1000 | Loss: 0.00001522
Iteration 113/1000 | Loss: 0.00001522
Iteration 114/1000 | Loss: 0.00001522
Iteration 115/1000 | Loss: 0.00001522
Iteration 116/1000 | Loss: 0.00001522
Iteration 117/1000 | Loss: 0.00001522
Iteration 118/1000 | Loss: 0.00001522
Iteration 119/1000 | Loss: 0.00001522
Iteration 120/1000 | Loss: 0.00001522
Iteration 121/1000 | Loss: 0.00001522
Iteration 122/1000 | Loss: 0.00001522
Iteration 123/1000 | Loss: 0.00001522
Iteration 124/1000 | Loss: 0.00001522
Iteration 125/1000 | Loss: 0.00001522
Iteration 126/1000 | Loss: 0.00001522
Iteration 127/1000 | Loss: 0.00001522
Iteration 128/1000 | Loss: 0.00001522
Iteration 129/1000 | Loss: 0.00001522
Iteration 130/1000 | Loss: 0.00001522
Iteration 131/1000 | Loss: 0.00001522
Iteration 132/1000 | Loss: 0.00001522
Iteration 133/1000 | Loss: 0.00001522
Iteration 134/1000 | Loss: 0.00001522
Iteration 135/1000 | Loss: 0.00001522
Iteration 136/1000 | Loss: 0.00001522
Iteration 137/1000 | Loss: 0.00001522
Iteration 138/1000 | Loss: 0.00001522
Iteration 139/1000 | Loss: 0.00001522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.521945796412183e-05, 1.521945796412183e-05, 1.521945796412183e-05, 1.521945796412183e-05, 1.521945796412183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.521945796412183e-05

Optimization complete. Final v2v error: 3.294994354248047 mm

Highest mean error: 3.635406494140625 mm for frame 153

Lowest mean error: 2.9454517364501953 mm for frame 198

Saving results

Total time: 44.703129529953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852808
Iteration 2/25 | Loss: 0.00081168
Iteration 3/25 | Loss: 0.00067122
Iteration 4/25 | Loss: 0.00064488
Iteration 5/25 | Loss: 0.00063661
Iteration 6/25 | Loss: 0.00063482
Iteration 7/25 | Loss: 0.00063457
Iteration 8/25 | Loss: 0.00063457
Iteration 9/25 | Loss: 0.00063457
Iteration 10/25 | Loss: 0.00063457
Iteration 11/25 | Loss: 0.00063457
Iteration 12/25 | Loss: 0.00063454
Iteration 13/25 | Loss: 0.00063454
Iteration 14/25 | Loss: 0.00063454
Iteration 15/25 | Loss: 0.00063454
Iteration 16/25 | Loss: 0.00063454
Iteration 17/25 | Loss: 0.00063454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006345381261780858, 0.0006345381261780858, 0.0006345381261780858, 0.0006345381261780858, 0.0006345381261780858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006345381261780858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52432024
Iteration 2/25 | Loss: 0.00027696
Iteration 3/25 | Loss: 0.00027695
Iteration 4/25 | Loss: 0.00027695
Iteration 5/25 | Loss: 0.00027695
Iteration 6/25 | Loss: 0.00027695
Iteration 7/25 | Loss: 0.00027695
Iteration 8/25 | Loss: 0.00027695
Iteration 9/25 | Loss: 0.00027695
Iteration 10/25 | Loss: 0.00027695
Iteration 11/25 | Loss: 0.00027695
Iteration 12/25 | Loss: 0.00027695
Iteration 13/25 | Loss: 0.00027695
Iteration 14/25 | Loss: 0.00027695
Iteration 15/25 | Loss: 0.00027695
Iteration 16/25 | Loss: 0.00027695
Iteration 17/25 | Loss: 0.00027695
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002769520506262779, 0.0002769520506262779, 0.0002769520506262779, 0.0002769520506262779, 0.0002769520506262779]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002769520506262779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027695
Iteration 2/1000 | Loss: 0.00002823
Iteration 3/1000 | Loss: 0.00001979
Iteration 4/1000 | Loss: 0.00001843
Iteration 5/1000 | Loss: 0.00001747
Iteration 6/1000 | Loss: 0.00001702
Iteration 7/1000 | Loss: 0.00001660
Iteration 8/1000 | Loss: 0.00001638
Iteration 9/1000 | Loss: 0.00001637
Iteration 10/1000 | Loss: 0.00001635
Iteration 11/1000 | Loss: 0.00001632
Iteration 12/1000 | Loss: 0.00001630
Iteration 13/1000 | Loss: 0.00001628
Iteration 14/1000 | Loss: 0.00001626
Iteration 15/1000 | Loss: 0.00001615
Iteration 16/1000 | Loss: 0.00001615
Iteration 17/1000 | Loss: 0.00001611
Iteration 18/1000 | Loss: 0.00001611
Iteration 19/1000 | Loss: 0.00001610
Iteration 20/1000 | Loss: 0.00001610
Iteration 21/1000 | Loss: 0.00001610
Iteration 22/1000 | Loss: 0.00001610
Iteration 23/1000 | Loss: 0.00001610
Iteration 24/1000 | Loss: 0.00001610
Iteration 25/1000 | Loss: 0.00001609
Iteration 26/1000 | Loss: 0.00001609
Iteration 27/1000 | Loss: 0.00001608
Iteration 28/1000 | Loss: 0.00001601
Iteration 29/1000 | Loss: 0.00001599
Iteration 30/1000 | Loss: 0.00001595
Iteration 31/1000 | Loss: 0.00001595
Iteration 32/1000 | Loss: 0.00001595
Iteration 33/1000 | Loss: 0.00001595
Iteration 34/1000 | Loss: 0.00001595
Iteration 35/1000 | Loss: 0.00001595
Iteration 36/1000 | Loss: 0.00001593
Iteration 37/1000 | Loss: 0.00001593
Iteration 38/1000 | Loss: 0.00001592
Iteration 39/1000 | Loss: 0.00001592
Iteration 40/1000 | Loss: 0.00001592
Iteration 41/1000 | Loss: 0.00001592
Iteration 42/1000 | Loss: 0.00001592
Iteration 43/1000 | Loss: 0.00001592
Iteration 44/1000 | Loss: 0.00001591
Iteration 45/1000 | Loss: 0.00001591
Iteration 46/1000 | Loss: 0.00001591
Iteration 47/1000 | Loss: 0.00001591
Iteration 48/1000 | Loss: 0.00001591
Iteration 49/1000 | Loss: 0.00001591
Iteration 50/1000 | Loss: 0.00001591
Iteration 51/1000 | Loss: 0.00001590
Iteration 52/1000 | Loss: 0.00001590
Iteration 53/1000 | Loss: 0.00001590
Iteration 54/1000 | Loss: 0.00001589
Iteration 55/1000 | Loss: 0.00001589
Iteration 56/1000 | Loss: 0.00001588
Iteration 57/1000 | Loss: 0.00001588
Iteration 58/1000 | Loss: 0.00001588
Iteration 59/1000 | Loss: 0.00001588
Iteration 60/1000 | Loss: 0.00001588
Iteration 61/1000 | Loss: 0.00001588
Iteration 62/1000 | Loss: 0.00001588
Iteration 63/1000 | Loss: 0.00001588
Iteration 64/1000 | Loss: 0.00001588
Iteration 65/1000 | Loss: 0.00001587
Iteration 66/1000 | Loss: 0.00001587
Iteration 67/1000 | Loss: 0.00001587
Iteration 68/1000 | Loss: 0.00001587
Iteration 69/1000 | Loss: 0.00001587
Iteration 70/1000 | Loss: 0.00001587
Iteration 71/1000 | Loss: 0.00001587
Iteration 72/1000 | Loss: 0.00001587
Iteration 73/1000 | Loss: 0.00001587
Iteration 74/1000 | Loss: 0.00001587
Iteration 75/1000 | Loss: 0.00001587
Iteration 76/1000 | Loss: 0.00001586
Iteration 77/1000 | Loss: 0.00001586
Iteration 78/1000 | Loss: 0.00001586
Iteration 79/1000 | Loss: 0.00001586
Iteration 80/1000 | Loss: 0.00001586
Iteration 81/1000 | Loss: 0.00001586
Iteration 82/1000 | Loss: 0.00001585
Iteration 83/1000 | Loss: 0.00001585
Iteration 84/1000 | Loss: 0.00001585
Iteration 85/1000 | Loss: 0.00001585
Iteration 86/1000 | Loss: 0.00001585
Iteration 87/1000 | Loss: 0.00001585
Iteration 88/1000 | Loss: 0.00001584
Iteration 89/1000 | Loss: 0.00001584
Iteration 90/1000 | Loss: 0.00001583
Iteration 91/1000 | Loss: 0.00001583
Iteration 92/1000 | Loss: 0.00001583
Iteration 93/1000 | Loss: 0.00001582
Iteration 94/1000 | Loss: 0.00001582
Iteration 95/1000 | Loss: 0.00001582
Iteration 96/1000 | Loss: 0.00001581
Iteration 97/1000 | Loss: 0.00001581
Iteration 98/1000 | Loss: 0.00001581
Iteration 99/1000 | Loss: 0.00001581
Iteration 100/1000 | Loss: 0.00001581
Iteration 101/1000 | Loss: 0.00001580
Iteration 102/1000 | Loss: 0.00001580
Iteration 103/1000 | Loss: 0.00001580
Iteration 104/1000 | Loss: 0.00001580
Iteration 105/1000 | Loss: 0.00001580
Iteration 106/1000 | Loss: 0.00001580
Iteration 107/1000 | Loss: 0.00001580
Iteration 108/1000 | Loss: 0.00001580
Iteration 109/1000 | Loss: 0.00001580
Iteration 110/1000 | Loss: 0.00001580
Iteration 111/1000 | Loss: 0.00001579
Iteration 112/1000 | Loss: 0.00001579
Iteration 113/1000 | Loss: 0.00001579
Iteration 114/1000 | Loss: 0.00001579
Iteration 115/1000 | Loss: 0.00001578
Iteration 116/1000 | Loss: 0.00001578
Iteration 117/1000 | Loss: 0.00001577
Iteration 118/1000 | Loss: 0.00001577
Iteration 119/1000 | Loss: 0.00001577
Iteration 120/1000 | Loss: 0.00001577
Iteration 121/1000 | Loss: 0.00001576
Iteration 122/1000 | Loss: 0.00001576
Iteration 123/1000 | Loss: 0.00001576
Iteration 124/1000 | Loss: 0.00001576
Iteration 125/1000 | Loss: 0.00001576
Iteration 126/1000 | Loss: 0.00001576
Iteration 127/1000 | Loss: 0.00001576
Iteration 128/1000 | Loss: 0.00001576
Iteration 129/1000 | Loss: 0.00001576
Iteration 130/1000 | Loss: 0.00001575
Iteration 131/1000 | Loss: 0.00001575
Iteration 132/1000 | Loss: 0.00001575
Iteration 133/1000 | Loss: 0.00001575
Iteration 134/1000 | Loss: 0.00001575
Iteration 135/1000 | Loss: 0.00001575
Iteration 136/1000 | Loss: 0.00001575
Iteration 137/1000 | Loss: 0.00001575
Iteration 138/1000 | Loss: 0.00001575
Iteration 139/1000 | Loss: 0.00001575
Iteration 140/1000 | Loss: 0.00001574
Iteration 141/1000 | Loss: 0.00001574
Iteration 142/1000 | Loss: 0.00001574
Iteration 143/1000 | Loss: 0.00001574
Iteration 144/1000 | Loss: 0.00001574
Iteration 145/1000 | Loss: 0.00001574
Iteration 146/1000 | Loss: 0.00001574
Iteration 147/1000 | Loss: 0.00001574
Iteration 148/1000 | Loss: 0.00001574
Iteration 149/1000 | Loss: 0.00001574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.5742434698040597e-05, 1.5742434698040597e-05, 1.5742434698040597e-05, 1.5742434698040597e-05, 1.5742434698040597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5742434698040597e-05

Optimization complete. Final v2v error: 3.40028977394104 mm

Highest mean error: 3.837980031967163 mm for frame 176

Lowest mean error: 3.060967206954956 mm for frame 67

Saving results

Total time: 36.90652322769165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791634
Iteration 2/25 | Loss: 0.00214737
Iteration 3/25 | Loss: 0.00108999
Iteration 4/25 | Loss: 0.00085899
Iteration 5/25 | Loss: 0.00072143
Iteration 6/25 | Loss: 0.00071399
Iteration 7/25 | Loss: 0.00067982
Iteration 8/25 | Loss: 0.00067784
Iteration 9/25 | Loss: 0.00064840
Iteration 10/25 | Loss: 0.00064115
Iteration 11/25 | Loss: 0.00062901
Iteration 12/25 | Loss: 0.00062281
Iteration 13/25 | Loss: 0.00062366
Iteration 14/25 | Loss: 0.00061888
Iteration 15/25 | Loss: 0.00061159
Iteration 16/25 | Loss: 0.00061560
Iteration 17/25 | Loss: 0.00062080
Iteration 18/25 | Loss: 0.00061134
Iteration 19/25 | Loss: 0.00060732
Iteration 20/25 | Loss: 0.00061377
Iteration 21/25 | Loss: 0.00060634
Iteration 22/25 | Loss: 0.00060625
Iteration 23/25 | Loss: 0.00060625
Iteration 24/25 | Loss: 0.00060625
Iteration 25/25 | Loss: 0.00060625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.12045860
Iteration 2/25 | Loss: 0.00039800
Iteration 3/25 | Loss: 0.00039800
Iteration 4/25 | Loss: 0.00028160
Iteration 5/25 | Loss: 0.00028160
Iteration 6/25 | Loss: 0.00028160
Iteration 7/25 | Loss: 0.00028160
Iteration 8/25 | Loss: 0.00028160
Iteration 9/25 | Loss: 0.00028160
Iteration 10/25 | Loss: 0.00028160
Iteration 11/25 | Loss: 0.00028160
Iteration 12/25 | Loss: 0.00028160
Iteration 13/25 | Loss: 0.00028160
Iteration 14/25 | Loss: 0.00028160
Iteration 15/25 | Loss: 0.00028160
Iteration 16/25 | Loss: 0.00028160
Iteration 17/25 | Loss: 0.00028160
Iteration 18/25 | Loss: 0.00028160
Iteration 19/25 | Loss: 0.00028160
Iteration 20/25 | Loss: 0.00028160
Iteration 21/25 | Loss: 0.00028160
Iteration 22/25 | Loss: 0.00028160
Iteration 23/25 | Loss: 0.00028160
Iteration 24/25 | Loss: 0.00028160
Iteration 25/25 | Loss: 0.00028160

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028160
Iteration 2/1000 | Loss: 0.00004574
Iteration 3/1000 | Loss: 0.00002847
Iteration 4/1000 | Loss: 0.00001529
Iteration 5/1000 | Loss: 0.00002423
Iteration 6/1000 | Loss: 0.00002994
Iteration 7/1000 | Loss: 0.00002214
Iteration 8/1000 | Loss: 0.00002814
Iteration 9/1000 | Loss: 0.00016917
Iteration 10/1000 | Loss: 0.00001356
Iteration 11/1000 | Loss: 0.00002409
Iteration 12/1000 | Loss: 0.00001559
Iteration 13/1000 | Loss: 0.00001335
Iteration 14/1000 | Loss: 0.00002132
Iteration 15/1000 | Loss: 0.00001310
Iteration 16/1000 | Loss: 0.00001302
Iteration 17/1000 | Loss: 0.00001301
Iteration 18/1000 | Loss: 0.00001301
Iteration 19/1000 | Loss: 0.00001301
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001301
Iteration 22/1000 | Loss: 0.00001301
Iteration 23/1000 | Loss: 0.00001301
Iteration 24/1000 | Loss: 0.00001301
Iteration 25/1000 | Loss: 0.00001301
Iteration 26/1000 | Loss: 0.00001301
Iteration 27/1000 | Loss: 0.00001300
Iteration 28/1000 | Loss: 0.00001299
Iteration 29/1000 | Loss: 0.00001298
Iteration 30/1000 | Loss: 0.00001298
Iteration 31/1000 | Loss: 0.00001298
Iteration 32/1000 | Loss: 0.00001298
Iteration 33/1000 | Loss: 0.00001298
Iteration 34/1000 | Loss: 0.00001298
Iteration 35/1000 | Loss: 0.00001298
Iteration 36/1000 | Loss: 0.00001298
Iteration 37/1000 | Loss: 0.00001298
Iteration 38/1000 | Loss: 0.00001297
Iteration 39/1000 | Loss: 0.00001297
Iteration 40/1000 | Loss: 0.00001296
Iteration 41/1000 | Loss: 0.00001290
Iteration 42/1000 | Loss: 0.00001282
Iteration 43/1000 | Loss: 0.00001282
Iteration 44/1000 | Loss: 0.00001282
Iteration 45/1000 | Loss: 0.00001279
Iteration 46/1000 | Loss: 0.00001275
Iteration 47/1000 | Loss: 0.00003121
Iteration 48/1000 | Loss: 0.00016621
Iteration 49/1000 | Loss: 0.00030910
Iteration 50/1000 | Loss: 0.00007807
Iteration 51/1000 | Loss: 0.00001331
Iteration 52/1000 | Loss: 0.00001323
Iteration 53/1000 | Loss: 0.00004894
Iteration 54/1000 | Loss: 0.00002576
Iteration 55/1000 | Loss: 0.00001635
Iteration 56/1000 | Loss: 0.00001889
Iteration 57/1000 | Loss: 0.00001269
Iteration 58/1000 | Loss: 0.00001261
Iteration 59/1000 | Loss: 0.00001261
Iteration 60/1000 | Loss: 0.00001261
Iteration 61/1000 | Loss: 0.00001261
Iteration 62/1000 | Loss: 0.00001261
Iteration 63/1000 | Loss: 0.00001261
Iteration 64/1000 | Loss: 0.00001261
Iteration 65/1000 | Loss: 0.00001261
Iteration 66/1000 | Loss: 0.00001492
Iteration 67/1000 | Loss: 0.00001260
Iteration 68/1000 | Loss: 0.00001260
Iteration 69/1000 | Loss: 0.00001260
Iteration 70/1000 | Loss: 0.00001260
Iteration 71/1000 | Loss: 0.00001259
Iteration 72/1000 | Loss: 0.00001259
Iteration 73/1000 | Loss: 0.00001259
Iteration 74/1000 | Loss: 0.00001259
Iteration 75/1000 | Loss: 0.00001259
Iteration 76/1000 | Loss: 0.00001259
Iteration 77/1000 | Loss: 0.00001259
Iteration 78/1000 | Loss: 0.00001259
Iteration 79/1000 | Loss: 0.00004924
Iteration 80/1000 | Loss: 0.00004924
Iteration 81/1000 | Loss: 0.00013384
Iteration 82/1000 | Loss: 0.00003932
Iteration 83/1000 | Loss: 0.00003908
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001260
Iteration 86/1000 | Loss: 0.00001257
Iteration 87/1000 | Loss: 0.00001257
Iteration 88/1000 | Loss: 0.00001257
Iteration 89/1000 | Loss: 0.00001257
Iteration 90/1000 | Loss: 0.00001257
Iteration 91/1000 | Loss: 0.00001257
Iteration 92/1000 | Loss: 0.00001257
Iteration 93/1000 | Loss: 0.00001257
Iteration 94/1000 | Loss: 0.00001257
Iteration 95/1000 | Loss: 0.00001256
Iteration 96/1000 | Loss: 0.00001256
Iteration 97/1000 | Loss: 0.00001256
Iteration 98/1000 | Loss: 0.00001256
Iteration 99/1000 | Loss: 0.00001255
Iteration 100/1000 | Loss: 0.00001255
Iteration 101/1000 | Loss: 0.00001255
Iteration 102/1000 | Loss: 0.00004441
Iteration 103/1000 | Loss: 0.00001687
Iteration 104/1000 | Loss: 0.00002207
Iteration 105/1000 | Loss: 0.00001256
Iteration 106/1000 | Loss: 0.00001256
Iteration 107/1000 | Loss: 0.00001256
Iteration 108/1000 | Loss: 0.00001256
Iteration 109/1000 | Loss: 0.00001255
Iteration 110/1000 | Loss: 0.00001255
Iteration 111/1000 | Loss: 0.00001255
Iteration 112/1000 | Loss: 0.00001255
Iteration 113/1000 | Loss: 0.00002072
Iteration 114/1000 | Loss: 0.00002927
Iteration 115/1000 | Loss: 0.00002267
Iteration 116/1000 | Loss: 0.00008641
Iteration 117/1000 | Loss: 0.00001376
Iteration 118/1000 | Loss: 0.00002301
Iteration 119/1000 | Loss: 0.00002301
Iteration 120/1000 | Loss: 0.00003280
Iteration 121/1000 | Loss: 0.00001482
Iteration 122/1000 | Loss: 0.00002638
Iteration 123/1000 | Loss: 0.00001258
Iteration 124/1000 | Loss: 0.00001258
Iteration 125/1000 | Loss: 0.00001257
Iteration 126/1000 | Loss: 0.00001257
Iteration 127/1000 | Loss: 0.00001257
Iteration 128/1000 | Loss: 0.00001256
Iteration 129/1000 | Loss: 0.00001269
Iteration 130/1000 | Loss: 0.00001256
Iteration 131/1000 | Loss: 0.00001254
Iteration 132/1000 | Loss: 0.00001254
Iteration 133/1000 | Loss: 0.00001254
Iteration 134/1000 | Loss: 0.00001254
Iteration 135/1000 | Loss: 0.00001254
Iteration 136/1000 | Loss: 0.00001253
Iteration 137/1000 | Loss: 0.00001253
Iteration 138/1000 | Loss: 0.00001253
Iteration 139/1000 | Loss: 0.00001253
Iteration 140/1000 | Loss: 0.00001253
Iteration 141/1000 | Loss: 0.00001253
Iteration 142/1000 | Loss: 0.00001253
Iteration 143/1000 | Loss: 0.00001253
Iteration 144/1000 | Loss: 0.00001253
Iteration 145/1000 | Loss: 0.00001253
Iteration 146/1000 | Loss: 0.00001253
Iteration 147/1000 | Loss: 0.00001253
Iteration 148/1000 | Loss: 0.00001253
Iteration 149/1000 | Loss: 0.00001253
Iteration 150/1000 | Loss: 0.00001253
Iteration 151/1000 | Loss: 0.00001253
Iteration 152/1000 | Loss: 0.00001253
Iteration 153/1000 | Loss: 0.00001253
Iteration 154/1000 | Loss: 0.00001253
Iteration 155/1000 | Loss: 0.00001253
Iteration 156/1000 | Loss: 0.00001253
Iteration 157/1000 | Loss: 0.00001253
Iteration 158/1000 | Loss: 0.00001253
Iteration 159/1000 | Loss: 0.00001253
Iteration 160/1000 | Loss: 0.00001253
Iteration 161/1000 | Loss: 0.00001253
Iteration 162/1000 | Loss: 0.00001253
Iteration 163/1000 | Loss: 0.00001253
Iteration 164/1000 | Loss: 0.00001253
Iteration 165/1000 | Loss: 0.00001253
Iteration 166/1000 | Loss: 0.00001253
Iteration 167/1000 | Loss: 0.00001253
Iteration 168/1000 | Loss: 0.00001253
Iteration 169/1000 | Loss: 0.00001253
Iteration 170/1000 | Loss: 0.00001253
Iteration 171/1000 | Loss: 0.00001253
Iteration 172/1000 | Loss: 0.00001253
Iteration 173/1000 | Loss: 0.00001253
Iteration 174/1000 | Loss: 0.00001253
Iteration 175/1000 | Loss: 0.00001253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.2532825166999828e-05, 1.2532825166999828e-05, 1.2532825166999828e-05, 1.2532825166999828e-05, 1.2532825166999828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2532825166999828e-05

Optimization complete. Final v2v error: 3.0103065967559814 mm

Highest mean error: 3.5896689891815186 mm for frame 97

Lowest mean error: 2.8238868713378906 mm for frame 146

Saving results

Total time: 114.54448795318604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052319
Iteration 2/25 | Loss: 0.01052319
Iteration 3/25 | Loss: 0.01052319
Iteration 4/25 | Loss: 0.01052319
Iteration 5/25 | Loss: 0.01052319
Iteration 6/25 | Loss: 0.01052319
Iteration 7/25 | Loss: 0.01052318
Iteration 8/25 | Loss: 0.01052318
Iteration 9/25 | Loss: 0.01052318
Iteration 10/25 | Loss: 0.01052318
Iteration 11/25 | Loss: 0.01052318
Iteration 12/25 | Loss: 0.01052318
Iteration 13/25 | Loss: 0.01052318
Iteration 14/25 | Loss: 0.01052318
Iteration 15/25 | Loss: 0.01052318
Iteration 16/25 | Loss: 0.01052318
Iteration 17/25 | Loss: 0.01052317
Iteration 18/25 | Loss: 0.01052317
Iteration 19/25 | Loss: 0.01052317
Iteration 20/25 | Loss: 0.01052317
Iteration 21/25 | Loss: 0.01052317
Iteration 22/25 | Loss: 0.01052317
Iteration 23/25 | Loss: 0.01052317
Iteration 24/25 | Loss: 0.01052317
Iteration 25/25 | Loss: 0.01052317

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99431670
Iteration 2/25 | Loss: 0.09083195
Iteration 3/25 | Loss: 0.08921998
Iteration 4/25 | Loss: 0.08878353
Iteration 5/25 | Loss: 0.08872617
Iteration 6/25 | Loss: 0.08874820
Iteration 7/25 | Loss: 0.08874820
Iteration 8/25 | Loss: 0.08874820
Iteration 9/25 | Loss: 0.08874821
Iteration 10/25 | Loss: 0.08877634
Iteration 11/25 | Loss: 0.08877631
Iteration 12/25 | Loss: 0.08872575
Iteration 13/25 | Loss: 0.08872572
Iteration 14/25 | Loss: 0.08874735
Iteration 15/25 | Loss: 0.08874735
Iteration 16/25 | Loss: 0.08874735
Iteration 17/25 | Loss: 0.08874735
Iteration 18/25 | Loss: 0.08874735
Iteration 19/25 | Loss: 0.08874735
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.08874735236167908, 0.08874735236167908, 0.08874735236167908, 0.08874735236167908, 0.08874735236167908]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08874735236167908

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08874735
Iteration 2/1000 | Loss: 0.00540701
Iteration 3/1000 | Loss: 0.00310627
Iteration 4/1000 | Loss: 0.00218722
Iteration 5/1000 | Loss: 0.00225231
Iteration 6/1000 | Loss: 0.00114083
Iteration 7/1000 | Loss: 0.00027633
Iteration 8/1000 | Loss: 0.00018156
Iteration 9/1000 | Loss: 0.00155406
Iteration 10/1000 | Loss: 0.00070184
Iteration 11/1000 | Loss: 0.00009692
Iteration 12/1000 | Loss: 0.00007398
Iteration 13/1000 | Loss: 0.00008225
Iteration 14/1000 | Loss: 0.00111511
Iteration 15/1000 | Loss: 0.00015049
Iteration 16/1000 | Loss: 0.00030162
Iteration 17/1000 | Loss: 0.00006583
Iteration 18/1000 | Loss: 0.00004446
Iteration 19/1000 | Loss: 0.00011896
Iteration 20/1000 | Loss: 0.00016204
Iteration 21/1000 | Loss: 0.00034768
Iteration 22/1000 | Loss: 0.00107794
Iteration 23/1000 | Loss: 0.00057894
Iteration 24/1000 | Loss: 0.00005840
Iteration 25/1000 | Loss: 0.00011985
Iteration 26/1000 | Loss: 0.00005037
Iteration 27/1000 | Loss: 0.00038390
Iteration 28/1000 | Loss: 0.00006110
Iteration 29/1000 | Loss: 0.00029541
Iteration 30/1000 | Loss: 0.00030397
Iteration 31/1000 | Loss: 0.00028967
Iteration 32/1000 | Loss: 0.00004589
Iteration 33/1000 | Loss: 0.00016806
Iteration 34/1000 | Loss: 0.00004178
Iteration 35/1000 | Loss: 0.00028488
Iteration 36/1000 | Loss: 0.00015149
Iteration 37/1000 | Loss: 0.00002800
Iteration 38/1000 | Loss: 0.00002641
Iteration 39/1000 | Loss: 0.00014417
Iteration 40/1000 | Loss: 0.00004124
Iteration 41/1000 | Loss: 0.00009061
Iteration 42/1000 | Loss: 0.00009470
Iteration 43/1000 | Loss: 0.00004787
Iteration 44/1000 | Loss: 0.00011144
Iteration 45/1000 | Loss: 0.00002456
Iteration 46/1000 | Loss: 0.00002299
Iteration 47/1000 | Loss: 0.00022790
Iteration 48/1000 | Loss: 0.00033300
Iteration 49/1000 | Loss: 0.00029886
Iteration 50/1000 | Loss: 0.00073020
Iteration 51/1000 | Loss: 0.00003236
Iteration 52/1000 | Loss: 0.00010835
Iteration 53/1000 | Loss: 0.00002163
Iteration 54/1000 | Loss: 0.00002106
Iteration 55/1000 | Loss: 0.00002068
Iteration 56/1000 | Loss: 0.00002038
Iteration 57/1000 | Loss: 0.00009695
Iteration 58/1000 | Loss: 0.00034845
Iteration 59/1000 | Loss: 0.00015314
Iteration 60/1000 | Loss: 0.00005111
Iteration 61/1000 | Loss: 0.00002462
Iteration 62/1000 | Loss: 0.00002355
Iteration 63/1000 | Loss: 0.00004081
Iteration 64/1000 | Loss: 0.00001999
Iteration 65/1000 | Loss: 0.00001978
Iteration 66/1000 | Loss: 0.00006218
Iteration 67/1000 | Loss: 0.00002683
Iteration 68/1000 | Loss: 0.00001962
Iteration 69/1000 | Loss: 0.00001959
Iteration 70/1000 | Loss: 0.00001958
Iteration 71/1000 | Loss: 0.00037623
Iteration 72/1000 | Loss: 0.00014981
Iteration 73/1000 | Loss: 0.00004108
Iteration 74/1000 | Loss: 0.00018214
Iteration 75/1000 | Loss: 0.00002353
Iteration 76/1000 | Loss: 0.00011513
Iteration 77/1000 | Loss: 0.00002162
Iteration 78/1000 | Loss: 0.00002122
Iteration 79/1000 | Loss: 0.00036608
Iteration 80/1000 | Loss: 0.00014347
Iteration 81/1000 | Loss: 0.00002106
Iteration 82/1000 | Loss: 0.00038631
Iteration 83/1000 | Loss: 0.00013291
Iteration 84/1000 | Loss: 0.00003855
Iteration 85/1000 | Loss: 0.00002287
Iteration 86/1000 | Loss: 0.00007188
Iteration 87/1000 | Loss: 0.00022886
Iteration 88/1000 | Loss: 0.00018516
Iteration 89/1000 | Loss: 0.00005548
Iteration 90/1000 | Loss: 0.00005405
Iteration 91/1000 | Loss: 0.00003581
Iteration 92/1000 | Loss: 0.00002073
Iteration 93/1000 | Loss: 0.00001963
Iteration 94/1000 | Loss: 0.00005002
Iteration 95/1000 | Loss: 0.00004560
Iteration 96/1000 | Loss: 0.00001948
Iteration 97/1000 | Loss: 0.00001872
Iteration 98/1000 | Loss: 0.00001849
Iteration 99/1000 | Loss: 0.00009107
Iteration 100/1000 | Loss: 0.00002178
Iteration 101/1000 | Loss: 0.00004074
Iteration 102/1000 | Loss: 0.00002084
Iteration 103/1000 | Loss: 0.00001827
Iteration 104/1000 | Loss: 0.00001811
Iteration 105/1000 | Loss: 0.00001810
Iteration 106/1000 | Loss: 0.00001809
Iteration 107/1000 | Loss: 0.00001808
Iteration 108/1000 | Loss: 0.00001805
Iteration 109/1000 | Loss: 0.00001805
Iteration 110/1000 | Loss: 0.00001805
Iteration 111/1000 | Loss: 0.00001805
Iteration 112/1000 | Loss: 0.00001805
Iteration 113/1000 | Loss: 0.00001805
Iteration 114/1000 | Loss: 0.00001805
Iteration 115/1000 | Loss: 0.00001805
Iteration 116/1000 | Loss: 0.00001805
Iteration 117/1000 | Loss: 0.00001805
Iteration 118/1000 | Loss: 0.00001805
Iteration 119/1000 | Loss: 0.00001804
Iteration 120/1000 | Loss: 0.00001804
Iteration 121/1000 | Loss: 0.00001804
Iteration 122/1000 | Loss: 0.00001804
Iteration 123/1000 | Loss: 0.00001804
Iteration 124/1000 | Loss: 0.00001804
Iteration 125/1000 | Loss: 0.00001804
Iteration 126/1000 | Loss: 0.00001804
Iteration 127/1000 | Loss: 0.00001804
Iteration 128/1000 | Loss: 0.00001804
Iteration 129/1000 | Loss: 0.00001804
Iteration 130/1000 | Loss: 0.00001804
Iteration 131/1000 | Loss: 0.00001803
Iteration 132/1000 | Loss: 0.00001802
Iteration 133/1000 | Loss: 0.00001801
Iteration 134/1000 | Loss: 0.00001801
Iteration 135/1000 | Loss: 0.00001800
Iteration 136/1000 | Loss: 0.00001800
Iteration 137/1000 | Loss: 0.00001799
Iteration 138/1000 | Loss: 0.00001796
Iteration 139/1000 | Loss: 0.00001796
Iteration 140/1000 | Loss: 0.00001795
Iteration 141/1000 | Loss: 0.00001793
Iteration 142/1000 | Loss: 0.00001792
Iteration 143/1000 | Loss: 0.00001792
Iteration 144/1000 | Loss: 0.00001791
Iteration 145/1000 | Loss: 0.00001791
Iteration 146/1000 | Loss: 0.00001790
Iteration 147/1000 | Loss: 0.00001789
Iteration 148/1000 | Loss: 0.00001787
Iteration 149/1000 | Loss: 0.00001786
Iteration 150/1000 | Loss: 0.00001784
Iteration 151/1000 | Loss: 0.00001781
Iteration 152/1000 | Loss: 0.00001780
Iteration 153/1000 | Loss: 0.00001778
Iteration 154/1000 | Loss: 0.00001778
Iteration 155/1000 | Loss: 0.00001778
Iteration 156/1000 | Loss: 0.00001777
Iteration 157/1000 | Loss: 0.00001777
Iteration 158/1000 | Loss: 0.00001777
Iteration 159/1000 | Loss: 0.00001777
Iteration 160/1000 | Loss: 0.00001777
Iteration 161/1000 | Loss: 0.00001777
Iteration 162/1000 | Loss: 0.00001777
Iteration 163/1000 | Loss: 0.00001776
Iteration 164/1000 | Loss: 0.00001774
Iteration 165/1000 | Loss: 0.00001773
Iteration 166/1000 | Loss: 0.00001772
Iteration 167/1000 | Loss: 0.00010785
Iteration 168/1000 | Loss: 0.00001781
Iteration 169/1000 | Loss: 0.00001770
Iteration 170/1000 | Loss: 0.00001769
Iteration 171/1000 | Loss: 0.00001768
Iteration 172/1000 | Loss: 0.00001767
Iteration 173/1000 | Loss: 0.00001767
Iteration 174/1000 | Loss: 0.00001767
Iteration 175/1000 | Loss: 0.00001766
Iteration 176/1000 | Loss: 0.00001766
Iteration 177/1000 | Loss: 0.00001765
Iteration 178/1000 | Loss: 0.00001765
Iteration 179/1000 | Loss: 0.00001765
Iteration 180/1000 | Loss: 0.00001765
Iteration 181/1000 | Loss: 0.00001764
Iteration 182/1000 | Loss: 0.00001764
Iteration 183/1000 | Loss: 0.00001764
Iteration 184/1000 | Loss: 0.00001764
Iteration 185/1000 | Loss: 0.00001764
Iteration 186/1000 | Loss: 0.00001764
Iteration 187/1000 | Loss: 0.00001764
Iteration 188/1000 | Loss: 0.00001764
Iteration 189/1000 | Loss: 0.00001764
Iteration 190/1000 | Loss: 0.00001763
Iteration 191/1000 | Loss: 0.00001763
Iteration 192/1000 | Loss: 0.00001763
Iteration 193/1000 | Loss: 0.00001763
Iteration 194/1000 | Loss: 0.00001763
Iteration 195/1000 | Loss: 0.00001763
Iteration 196/1000 | Loss: 0.00001763
Iteration 197/1000 | Loss: 0.00001763
Iteration 198/1000 | Loss: 0.00001762
Iteration 199/1000 | Loss: 0.00001762
Iteration 200/1000 | Loss: 0.00001762
Iteration 201/1000 | Loss: 0.00001762
Iteration 202/1000 | Loss: 0.00001762
Iteration 203/1000 | Loss: 0.00001762
Iteration 204/1000 | Loss: 0.00001761
Iteration 205/1000 | Loss: 0.00001761
Iteration 206/1000 | Loss: 0.00001761
Iteration 207/1000 | Loss: 0.00001761
Iteration 208/1000 | Loss: 0.00001761
Iteration 209/1000 | Loss: 0.00001761
Iteration 210/1000 | Loss: 0.00001760
Iteration 211/1000 | Loss: 0.00001760
Iteration 212/1000 | Loss: 0.00001760
Iteration 213/1000 | Loss: 0.00001759
Iteration 214/1000 | Loss: 0.00001759
Iteration 215/1000 | Loss: 0.00001759
Iteration 216/1000 | Loss: 0.00001758
Iteration 217/1000 | Loss: 0.00001758
Iteration 218/1000 | Loss: 0.00001758
Iteration 219/1000 | Loss: 0.00001758
Iteration 220/1000 | Loss: 0.00001757
Iteration 221/1000 | Loss: 0.00001757
Iteration 222/1000 | Loss: 0.00001757
Iteration 223/1000 | Loss: 0.00001757
Iteration 224/1000 | Loss: 0.00001756
Iteration 225/1000 | Loss: 0.00001756
Iteration 226/1000 | Loss: 0.00001756
Iteration 227/1000 | Loss: 0.00001756
Iteration 228/1000 | Loss: 0.00001755
Iteration 229/1000 | Loss: 0.00001755
Iteration 230/1000 | Loss: 0.00001755
Iteration 231/1000 | Loss: 0.00001755
Iteration 232/1000 | Loss: 0.00001755
Iteration 233/1000 | Loss: 0.00001755
Iteration 234/1000 | Loss: 0.00001755
Iteration 235/1000 | Loss: 0.00001755
Iteration 236/1000 | Loss: 0.00001754
Iteration 237/1000 | Loss: 0.00001754
Iteration 238/1000 | Loss: 0.00001754
Iteration 239/1000 | Loss: 0.00001754
Iteration 240/1000 | Loss: 0.00001754
Iteration 241/1000 | Loss: 0.00001754
Iteration 242/1000 | Loss: 0.00001754
Iteration 243/1000 | Loss: 0.00001753
Iteration 244/1000 | Loss: 0.00001753
Iteration 245/1000 | Loss: 0.00001753
Iteration 246/1000 | Loss: 0.00001753
Iteration 247/1000 | Loss: 0.00001753
Iteration 248/1000 | Loss: 0.00001753
Iteration 249/1000 | Loss: 0.00001753
Iteration 250/1000 | Loss: 0.00001753
Iteration 251/1000 | Loss: 0.00001753
Iteration 252/1000 | Loss: 0.00001753
Iteration 253/1000 | Loss: 0.00001753
Iteration 254/1000 | Loss: 0.00001753
Iteration 255/1000 | Loss: 0.00001753
Iteration 256/1000 | Loss: 0.00001753
Iteration 257/1000 | Loss: 0.00001753
Iteration 258/1000 | Loss: 0.00001753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.752952994138468e-05, 1.752952994138468e-05, 1.752952994138468e-05, 1.752952994138468e-05, 1.752952994138468e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.752952994138468e-05

Optimization complete. Final v2v error: 3.2896227836608887 mm

Highest mean error: 16.46078872680664 mm for frame 172

Lowest mean error: 2.827779531478882 mm for frame 115

Saving results

Total time: 193.97426080703735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882608
Iteration 2/25 | Loss: 0.00094608
Iteration 3/25 | Loss: 0.00069721
Iteration 4/25 | Loss: 0.00066641
Iteration 5/25 | Loss: 0.00065965
Iteration 6/25 | Loss: 0.00065857
Iteration 7/25 | Loss: 0.00065857
Iteration 8/25 | Loss: 0.00065857
Iteration 9/25 | Loss: 0.00065857
Iteration 10/25 | Loss: 0.00065857
Iteration 11/25 | Loss: 0.00065857
Iteration 12/25 | Loss: 0.00065857
Iteration 13/25 | Loss: 0.00065857
Iteration 14/25 | Loss: 0.00065857
Iteration 15/25 | Loss: 0.00065857
Iteration 16/25 | Loss: 0.00065857
Iteration 17/25 | Loss: 0.00065857
Iteration 18/25 | Loss: 0.00065857
Iteration 19/25 | Loss: 0.00065857
Iteration 20/25 | Loss: 0.00065857
Iteration 21/25 | Loss: 0.00065857
Iteration 22/25 | Loss: 0.00065857
Iteration 23/25 | Loss: 0.00065857
Iteration 24/25 | Loss: 0.00065857
Iteration 25/25 | Loss: 0.00065857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48631561
Iteration 2/25 | Loss: 0.00026528
Iteration 3/25 | Loss: 0.00026528
Iteration 4/25 | Loss: 0.00026528
Iteration 5/25 | Loss: 0.00026528
Iteration 6/25 | Loss: 0.00026528
Iteration 7/25 | Loss: 0.00026528
Iteration 8/25 | Loss: 0.00026528
Iteration 9/25 | Loss: 0.00026528
Iteration 10/25 | Loss: 0.00026528
Iteration 11/25 | Loss: 0.00026528
Iteration 12/25 | Loss: 0.00026528
Iteration 13/25 | Loss: 0.00026528
Iteration 14/25 | Loss: 0.00026528
Iteration 15/25 | Loss: 0.00026528
Iteration 16/25 | Loss: 0.00026528
Iteration 17/25 | Loss: 0.00026528
Iteration 18/25 | Loss: 0.00026528
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00026527568115852773, 0.00026527568115852773, 0.00026527568115852773, 0.00026527568115852773, 0.00026527568115852773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026527568115852773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026528
Iteration 2/1000 | Loss: 0.00003142
Iteration 3/1000 | Loss: 0.00002252
Iteration 4/1000 | Loss: 0.00002135
Iteration 5/1000 | Loss: 0.00002029
Iteration 6/1000 | Loss: 0.00001967
Iteration 7/1000 | Loss: 0.00001909
Iteration 8/1000 | Loss: 0.00001879
Iteration 9/1000 | Loss: 0.00001856
Iteration 10/1000 | Loss: 0.00001850
Iteration 11/1000 | Loss: 0.00001849
Iteration 12/1000 | Loss: 0.00001849
Iteration 13/1000 | Loss: 0.00001836
Iteration 14/1000 | Loss: 0.00001831
Iteration 15/1000 | Loss: 0.00001830
Iteration 16/1000 | Loss: 0.00001830
Iteration 17/1000 | Loss: 0.00001825
Iteration 18/1000 | Loss: 0.00001824
Iteration 19/1000 | Loss: 0.00001822
Iteration 20/1000 | Loss: 0.00001821
Iteration 21/1000 | Loss: 0.00001821
Iteration 22/1000 | Loss: 0.00001821
Iteration 23/1000 | Loss: 0.00001820
Iteration 24/1000 | Loss: 0.00001820
Iteration 25/1000 | Loss: 0.00001819
Iteration 26/1000 | Loss: 0.00001819
Iteration 27/1000 | Loss: 0.00001819
Iteration 28/1000 | Loss: 0.00001819
Iteration 29/1000 | Loss: 0.00001819
Iteration 30/1000 | Loss: 0.00001819
Iteration 31/1000 | Loss: 0.00001818
Iteration 32/1000 | Loss: 0.00001818
Iteration 33/1000 | Loss: 0.00001818
Iteration 34/1000 | Loss: 0.00001817
Iteration 35/1000 | Loss: 0.00001817
Iteration 36/1000 | Loss: 0.00001817
Iteration 37/1000 | Loss: 0.00001817
Iteration 38/1000 | Loss: 0.00001817
Iteration 39/1000 | Loss: 0.00001816
Iteration 40/1000 | Loss: 0.00001816
Iteration 41/1000 | Loss: 0.00001816
Iteration 42/1000 | Loss: 0.00001815
Iteration 43/1000 | Loss: 0.00001815
Iteration 44/1000 | Loss: 0.00001815
Iteration 45/1000 | Loss: 0.00001815
Iteration 46/1000 | Loss: 0.00001814
Iteration 47/1000 | Loss: 0.00001814
Iteration 48/1000 | Loss: 0.00001814
Iteration 49/1000 | Loss: 0.00001814
Iteration 50/1000 | Loss: 0.00001814
Iteration 51/1000 | Loss: 0.00001813
Iteration 52/1000 | Loss: 0.00001813
Iteration 53/1000 | Loss: 0.00001813
Iteration 54/1000 | Loss: 0.00001813
Iteration 55/1000 | Loss: 0.00001813
Iteration 56/1000 | Loss: 0.00001813
Iteration 57/1000 | Loss: 0.00001813
Iteration 58/1000 | Loss: 0.00001813
Iteration 59/1000 | Loss: 0.00001813
Iteration 60/1000 | Loss: 0.00001813
Iteration 61/1000 | Loss: 0.00001813
Iteration 62/1000 | Loss: 0.00001813
Iteration 63/1000 | Loss: 0.00001813
Iteration 64/1000 | Loss: 0.00001813
Iteration 65/1000 | Loss: 0.00001813
Iteration 66/1000 | Loss: 0.00001813
Iteration 67/1000 | Loss: 0.00001813
Iteration 68/1000 | Loss: 0.00001813
Iteration 69/1000 | Loss: 0.00001813
Iteration 70/1000 | Loss: 0.00001813
Iteration 71/1000 | Loss: 0.00001813
Iteration 72/1000 | Loss: 0.00001813
Iteration 73/1000 | Loss: 0.00001813
Iteration 74/1000 | Loss: 0.00001813
Iteration 75/1000 | Loss: 0.00001813
Iteration 76/1000 | Loss: 0.00001813
Iteration 77/1000 | Loss: 0.00001813
Iteration 78/1000 | Loss: 0.00001813
Iteration 79/1000 | Loss: 0.00001813
Iteration 80/1000 | Loss: 0.00001813
Iteration 81/1000 | Loss: 0.00001813
Iteration 82/1000 | Loss: 0.00001813
Iteration 83/1000 | Loss: 0.00001813
Iteration 84/1000 | Loss: 0.00001813
Iteration 85/1000 | Loss: 0.00001813
Iteration 86/1000 | Loss: 0.00001813
Iteration 87/1000 | Loss: 0.00001813
Iteration 88/1000 | Loss: 0.00001813
Iteration 89/1000 | Loss: 0.00001813
Iteration 90/1000 | Loss: 0.00001813
Iteration 91/1000 | Loss: 0.00001813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.8131464457837865e-05, 1.8131464457837865e-05, 1.8131464457837865e-05, 1.8131464457837865e-05, 1.8131464457837865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8131464457837865e-05

Optimization complete. Final v2v error: 3.5645458698272705 mm

Highest mean error: 4.449306488037109 mm for frame 165

Lowest mean error: 3.0773706436157227 mm for frame 199

Saving results

Total time: 33.40518403053284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959162
Iteration 2/25 | Loss: 0.00119570
Iteration 3/25 | Loss: 0.00095322
Iteration 4/25 | Loss: 0.00084516
Iteration 5/25 | Loss: 0.00083642
Iteration 6/25 | Loss: 0.00080590
Iteration 7/25 | Loss: 0.00078065
Iteration 8/25 | Loss: 0.00077712
Iteration 9/25 | Loss: 0.00077541
Iteration 10/25 | Loss: 0.00077943
Iteration 11/25 | Loss: 0.00077913
Iteration 12/25 | Loss: 0.00077378
Iteration 13/25 | Loss: 0.00077160
Iteration 14/25 | Loss: 0.00077080
Iteration 15/25 | Loss: 0.00077037
Iteration 16/25 | Loss: 0.00077022
Iteration 17/25 | Loss: 0.00076998
Iteration 18/25 | Loss: 0.00077370
Iteration 19/25 | Loss: 0.00076735
Iteration 20/25 | Loss: 0.00076595
Iteration 21/25 | Loss: 0.00076557
Iteration 22/25 | Loss: 0.00076546
Iteration 23/25 | Loss: 0.00076544
Iteration 24/25 | Loss: 0.00076544
Iteration 25/25 | Loss: 0.00076544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42347693
Iteration 2/25 | Loss: 0.00038177
Iteration 3/25 | Loss: 0.00038177
Iteration 4/25 | Loss: 0.00038177
Iteration 5/25 | Loss: 0.00038177
Iteration 6/25 | Loss: 0.00038177
Iteration 7/25 | Loss: 0.00038177
Iteration 8/25 | Loss: 0.00038177
Iteration 9/25 | Loss: 0.00038177
Iteration 10/25 | Loss: 0.00038177
Iteration 11/25 | Loss: 0.00038177
Iteration 12/25 | Loss: 0.00038177
Iteration 13/25 | Loss: 0.00038177
Iteration 14/25 | Loss: 0.00038177
Iteration 15/25 | Loss: 0.00038177
Iteration 16/25 | Loss: 0.00038177
Iteration 17/25 | Loss: 0.00038177
Iteration 18/25 | Loss: 0.00038177
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00038176600355654955, 0.00038176600355654955, 0.00038176600355654955, 0.00038176600355654955, 0.00038176600355654955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038176600355654955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038177
Iteration 2/1000 | Loss: 0.00044191
Iteration 3/1000 | Loss: 0.00006294
Iteration 4/1000 | Loss: 0.00004548
Iteration 5/1000 | Loss: 0.00003720
Iteration 6/1000 | Loss: 0.00003434
Iteration 7/1000 | Loss: 0.00003252
Iteration 8/1000 | Loss: 0.00003128
Iteration 9/1000 | Loss: 0.00003039
Iteration 10/1000 | Loss: 0.00002974
Iteration 11/1000 | Loss: 0.00002930
Iteration 12/1000 | Loss: 0.00002890
Iteration 13/1000 | Loss: 0.00002863
Iteration 14/1000 | Loss: 0.00002843
Iteration 15/1000 | Loss: 0.00002824
Iteration 16/1000 | Loss: 0.00002821
Iteration 17/1000 | Loss: 0.00002806
Iteration 18/1000 | Loss: 0.00002803
Iteration 19/1000 | Loss: 0.00002803
Iteration 20/1000 | Loss: 0.00002803
Iteration 21/1000 | Loss: 0.00002803
Iteration 22/1000 | Loss: 0.00002802
Iteration 23/1000 | Loss: 0.00002802
Iteration 24/1000 | Loss: 0.00002800
Iteration 25/1000 | Loss: 0.00002799
Iteration 26/1000 | Loss: 0.00002799
Iteration 27/1000 | Loss: 0.00002799
Iteration 28/1000 | Loss: 0.00002798
Iteration 29/1000 | Loss: 0.00002796
Iteration 30/1000 | Loss: 0.00002795
Iteration 31/1000 | Loss: 0.00002794
Iteration 32/1000 | Loss: 0.00002793
Iteration 33/1000 | Loss: 0.00002793
Iteration 34/1000 | Loss: 0.00002792
Iteration 35/1000 | Loss: 0.00002792
Iteration 36/1000 | Loss: 0.00002791
Iteration 37/1000 | Loss: 0.00002791
Iteration 38/1000 | Loss: 0.00002790
Iteration 39/1000 | Loss: 0.00002790
Iteration 40/1000 | Loss: 0.00002790
Iteration 41/1000 | Loss: 0.00002789
Iteration 42/1000 | Loss: 0.00002789
Iteration 43/1000 | Loss: 0.00002789
Iteration 44/1000 | Loss: 0.00002788
Iteration 45/1000 | Loss: 0.00002788
Iteration 46/1000 | Loss: 0.00002788
Iteration 47/1000 | Loss: 0.00002788
Iteration 48/1000 | Loss: 0.00002787
Iteration 49/1000 | Loss: 0.00002787
Iteration 50/1000 | Loss: 0.00002787
Iteration 51/1000 | Loss: 0.00002786
Iteration 52/1000 | Loss: 0.00002786
Iteration 53/1000 | Loss: 0.00002786
Iteration 54/1000 | Loss: 0.00002785
Iteration 55/1000 | Loss: 0.00002785
Iteration 56/1000 | Loss: 0.00002785
Iteration 57/1000 | Loss: 0.00002784
Iteration 58/1000 | Loss: 0.00002784
Iteration 59/1000 | Loss: 0.00002784
Iteration 60/1000 | Loss: 0.00002783
Iteration 61/1000 | Loss: 0.00002783
Iteration 62/1000 | Loss: 0.00002783
Iteration 63/1000 | Loss: 0.00002782
Iteration 64/1000 | Loss: 0.00002782
Iteration 65/1000 | Loss: 0.00002782
Iteration 66/1000 | Loss: 0.00002781
Iteration 67/1000 | Loss: 0.00002781
Iteration 68/1000 | Loss: 0.00002781
Iteration 69/1000 | Loss: 0.00002781
Iteration 70/1000 | Loss: 0.00002781
Iteration 71/1000 | Loss: 0.00002781
Iteration 72/1000 | Loss: 0.00002781
Iteration 73/1000 | Loss: 0.00002781
Iteration 74/1000 | Loss: 0.00002781
Iteration 75/1000 | Loss: 0.00002781
Iteration 76/1000 | Loss: 0.00002781
Iteration 77/1000 | Loss: 0.00002780
Iteration 78/1000 | Loss: 0.00002780
Iteration 79/1000 | Loss: 0.00002780
Iteration 80/1000 | Loss: 0.00002780
Iteration 81/1000 | Loss: 0.00002780
Iteration 82/1000 | Loss: 0.00002780
Iteration 83/1000 | Loss: 0.00002780
Iteration 84/1000 | Loss: 0.00002780
Iteration 85/1000 | Loss: 0.00002780
Iteration 86/1000 | Loss: 0.00002780
Iteration 87/1000 | Loss: 0.00002780
Iteration 88/1000 | Loss: 0.00002780
Iteration 89/1000 | Loss: 0.00002780
Iteration 90/1000 | Loss: 0.00002780
Iteration 91/1000 | Loss: 0.00002780
Iteration 92/1000 | Loss: 0.00002779
Iteration 93/1000 | Loss: 0.00002779
Iteration 94/1000 | Loss: 0.00002779
Iteration 95/1000 | Loss: 0.00002779
Iteration 96/1000 | Loss: 0.00002779
Iteration 97/1000 | Loss: 0.00002779
Iteration 98/1000 | Loss: 0.00002779
Iteration 99/1000 | Loss: 0.00002778
Iteration 100/1000 | Loss: 0.00002778
Iteration 101/1000 | Loss: 0.00002778
Iteration 102/1000 | Loss: 0.00002778
Iteration 103/1000 | Loss: 0.00002778
Iteration 104/1000 | Loss: 0.00002778
Iteration 105/1000 | Loss: 0.00002778
Iteration 106/1000 | Loss: 0.00002778
Iteration 107/1000 | Loss: 0.00002778
Iteration 108/1000 | Loss: 0.00002778
Iteration 109/1000 | Loss: 0.00002778
Iteration 110/1000 | Loss: 0.00002778
Iteration 111/1000 | Loss: 0.00002778
Iteration 112/1000 | Loss: 0.00002778
Iteration 113/1000 | Loss: 0.00002778
Iteration 114/1000 | Loss: 0.00002777
Iteration 115/1000 | Loss: 0.00002777
Iteration 116/1000 | Loss: 0.00002777
Iteration 117/1000 | Loss: 0.00002777
Iteration 118/1000 | Loss: 0.00002777
Iteration 119/1000 | Loss: 0.00002777
Iteration 120/1000 | Loss: 0.00002777
Iteration 121/1000 | Loss: 0.00002777
Iteration 122/1000 | Loss: 0.00002777
Iteration 123/1000 | Loss: 0.00002777
Iteration 124/1000 | Loss: 0.00002777
Iteration 125/1000 | Loss: 0.00002777
Iteration 126/1000 | Loss: 0.00002776
Iteration 127/1000 | Loss: 0.00002776
Iteration 128/1000 | Loss: 0.00002776
Iteration 129/1000 | Loss: 0.00002776
Iteration 130/1000 | Loss: 0.00002776
Iteration 131/1000 | Loss: 0.00002776
Iteration 132/1000 | Loss: 0.00002776
Iteration 133/1000 | Loss: 0.00002776
Iteration 134/1000 | Loss: 0.00002776
Iteration 135/1000 | Loss: 0.00002776
Iteration 136/1000 | Loss: 0.00002776
Iteration 137/1000 | Loss: 0.00002776
Iteration 138/1000 | Loss: 0.00002776
Iteration 139/1000 | Loss: 0.00002776
Iteration 140/1000 | Loss: 0.00002776
Iteration 141/1000 | Loss: 0.00002776
Iteration 142/1000 | Loss: 0.00002775
Iteration 143/1000 | Loss: 0.00002775
Iteration 144/1000 | Loss: 0.00002775
Iteration 145/1000 | Loss: 0.00002775
Iteration 146/1000 | Loss: 0.00002775
Iteration 147/1000 | Loss: 0.00002775
Iteration 148/1000 | Loss: 0.00002775
Iteration 149/1000 | Loss: 0.00002775
Iteration 150/1000 | Loss: 0.00002774
Iteration 151/1000 | Loss: 0.00002774
Iteration 152/1000 | Loss: 0.00002774
Iteration 153/1000 | Loss: 0.00002774
Iteration 154/1000 | Loss: 0.00002774
Iteration 155/1000 | Loss: 0.00002774
Iteration 156/1000 | Loss: 0.00002774
Iteration 157/1000 | Loss: 0.00002774
Iteration 158/1000 | Loss: 0.00002774
Iteration 159/1000 | Loss: 0.00002774
Iteration 160/1000 | Loss: 0.00002774
Iteration 161/1000 | Loss: 0.00002774
Iteration 162/1000 | Loss: 0.00002773
Iteration 163/1000 | Loss: 0.00002773
Iteration 164/1000 | Loss: 0.00002773
Iteration 165/1000 | Loss: 0.00002773
Iteration 166/1000 | Loss: 0.00002773
Iteration 167/1000 | Loss: 0.00002773
Iteration 168/1000 | Loss: 0.00002773
Iteration 169/1000 | Loss: 0.00002773
Iteration 170/1000 | Loss: 0.00002773
Iteration 171/1000 | Loss: 0.00002773
Iteration 172/1000 | Loss: 0.00002773
Iteration 173/1000 | Loss: 0.00002773
Iteration 174/1000 | Loss: 0.00002772
Iteration 175/1000 | Loss: 0.00002772
Iteration 176/1000 | Loss: 0.00002772
Iteration 177/1000 | Loss: 0.00002772
Iteration 178/1000 | Loss: 0.00002772
Iteration 179/1000 | Loss: 0.00002772
Iteration 180/1000 | Loss: 0.00002772
Iteration 181/1000 | Loss: 0.00002772
Iteration 182/1000 | Loss: 0.00002772
Iteration 183/1000 | Loss: 0.00002772
Iteration 184/1000 | Loss: 0.00002772
Iteration 185/1000 | Loss: 0.00002772
Iteration 186/1000 | Loss: 0.00002772
Iteration 187/1000 | Loss: 0.00002772
Iteration 188/1000 | Loss: 0.00002772
Iteration 189/1000 | Loss: 0.00002772
Iteration 190/1000 | Loss: 0.00002771
Iteration 191/1000 | Loss: 0.00002771
Iteration 192/1000 | Loss: 0.00002771
Iteration 193/1000 | Loss: 0.00002771
Iteration 194/1000 | Loss: 0.00002771
Iteration 195/1000 | Loss: 0.00002771
Iteration 196/1000 | Loss: 0.00002771
Iteration 197/1000 | Loss: 0.00002771
Iteration 198/1000 | Loss: 0.00002771
Iteration 199/1000 | Loss: 0.00002771
Iteration 200/1000 | Loss: 0.00002771
Iteration 201/1000 | Loss: 0.00002771
Iteration 202/1000 | Loss: 0.00002771
Iteration 203/1000 | Loss: 0.00002771
Iteration 204/1000 | Loss: 0.00002771
Iteration 205/1000 | Loss: 0.00002771
Iteration 206/1000 | Loss: 0.00002771
Iteration 207/1000 | Loss: 0.00002771
Iteration 208/1000 | Loss: 0.00002771
Iteration 209/1000 | Loss: 0.00002770
Iteration 210/1000 | Loss: 0.00002770
Iteration 211/1000 | Loss: 0.00002770
Iteration 212/1000 | Loss: 0.00002770
Iteration 213/1000 | Loss: 0.00002770
Iteration 214/1000 | Loss: 0.00002770
Iteration 215/1000 | Loss: 0.00002770
Iteration 216/1000 | Loss: 0.00002770
Iteration 217/1000 | Loss: 0.00002770
Iteration 218/1000 | Loss: 0.00002770
Iteration 219/1000 | Loss: 0.00002770
Iteration 220/1000 | Loss: 0.00002770
Iteration 221/1000 | Loss: 0.00002770
Iteration 222/1000 | Loss: 0.00002770
Iteration 223/1000 | Loss: 0.00002770
Iteration 224/1000 | Loss: 0.00002770
Iteration 225/1000 | Loss: 0.00002770
Iteration 226/1000 | Loss: 0.00002769
Iteration 227/1000 | Loss: 0.00002769
Iteration 228/1000 | Loss: 0.00002769
Iteration 229/1000 | Loss: 0.00002769
Iteration 230/1000 | Loss: 0.00002769
Iteration 231/1000 | Loss: 0.00002769
Iteration 232/1000 | Loss: 0.00002769
Iteration 233/1000 | Loss: 0.00002769
Iteration 234/1000 | Loss: 0.00002769
Iteration 235/1000 | Loss: 0.00002769
Iteration 236/1000 | Loss: 0.00002769
Iteration 237/1000 | Loss: 0.00002769
Iteration 238/1000 | Loss: 0.00002769
Iteration 239/1000 | Loss: 0.00002769
Iteration 240/1000 | Loss: 0.00002769
Iteration 241/1000 | Loss: 0.00002769
Iteration 242/1000 | Loss: 0.00002769
Iteration 243/1000 | Loss: 0.00002769
Iteration 244/1000 | Loss: 0.00002768
Iteration 245/1000 | Loss: 0.00002768
Iteration 246/1000 | Loss: 0.00002768
Iteration 247/1000 | Loss: 0.00002768
Iteration 248/1000 | Loss: 0.00002768
Iteration 249/1000 | Loss: 0.00002768
Iteration 250/1000 | Loss: 0.00002768
Iteration 251/1000 | Loss: 0.00002768
Iteration 252/1000 | Loss: 0.00002768
Iteration 253/1000 | Loss: 0.00002768
Iteration 254/1000 | Loss: 0.00002768
Iteration 255/1000 | Loss: 0.00002768
Iteration 256/1000 | Loss: 0.00002768
Iteration 257/1000 | Loss: 0.00002768
Iteration 258/1000 | Loss: 0.00002768
Iteration 259/1000 | Loss: 0.00002768
Iteration 260/1000 | Loss: 0.00002768
Iteration 261/1000 | Loss: 0.00002768
Iteration 262/1000 | Loss: 0.00002768
Iteration 263/1000 | Loss: 0.00002768
Iteration 264/1000 | Loss: 0.00002767
Iteration 265/1000 | Loss: 0.00002767
Iteration 266/1000 | Loss: 0.00002767
Iteration 267/1000 | Loss: 0.00002767
Iteration 268/1000 | Loss: 0.00002767
Iteration 269/1000 | Loss: 0.00002767
Iteration 270/1000 | Loss: 0.00002767
Iteration 271/1000 | Loss: 0.00002767
Iteration 272/1000 | Loss: 0.00002767
Iteration 273/1000 | Loss: 0.00002767
Iteration 274/1000 | Loss: 0.00002767
Iteration 275/1000 | Loss: 0.00002767
Iteration 276/1000 | Loss: 0.00002767
Iteration 277/1000 | Loss: 0.00002767
Iteration 278/1000 | Loss: 0.00002767
Iteration 279/1000 | Loss: 0.00002767
Iteration 280/1000 | Loss: 0.00002767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 280. Stopping optimization.
Last 5 losses: [2.767285877780523e-05, 2.767285877780523e-05, 2.767285877780523e-05, 2.767285877780523e-05, 2.767285877780523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.767285877780523e-05

Optimization complete. Final v2v error: 4.423776149749756 mm

Highest mean error: 6.242916584014893 mm for frame 134

Lowest mean error: 3.8987748622894287 mm for frame 32

Saving results

Total time: 79.62075901031494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078765
Iteration 2/25 | Loss: 0.01078765
Iteration 3/25 | Loss: 0.01078764
Iteration 4/25 | Loss: 0.01078764
Iteration 5/25 | Loss: 0.01078764
Iteration 6/25 | Loss: 0.01078764
Iteration 7/25 | Loss: 0.00269032
Iteration 8/25 | Loss: 0.00186600
Iteration 9/25 | Loss: 0.00143638
Iteration 10/25 | Loss: 0.00122391
Iteration 11/25 | Loss: 0.00125051
Iteration 12/25 | Loss: 0.00122571
Iteration 13/25 | Loss: 0.00098823
Iteration 14/25 | Loss: 0.00092750
Iteration 15/25 | Loss: 0.00090940
Iteration 16/25 | Loss: 0.00087548
Iteration 17/25 | Loss: 0.00090198
Iteration 18/25 | Loss: 0.00085469
Iteration 19/25 | Loss: 0.00085049
Iteration 20/25 | Loss: 0.00080786
Iteration 21/25 | Loss: 0.00078564
Iteration 22/25 | Loss: 0.00077867
Iteration 23/25 | Loss: 0.00076850
Iteration 24/25 | Loss: 0.00079414
Iteration 25/25 | Loss: 0.00076480

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47165883
Iteration 2/25 | Loss: 0.00112695
Iteration 3/25 | Loss: 0.00071927
Iteration 4/25 | Loss: 0.00071927
Iteration 5/25 | Loss: 0.00071927
Iteration 6/25 | Loss: 0.00071927
Iteration 7/25 | Loss: 0.00071927
Iteration 8/25 | Loss: 0.00071927
Iteration 9/25 | Loss: 0.00071927
Iteration 10/25 | Loss: 0.00071927
Iteration 11/25 | Loss: 0.00071927
Iteration 12/25 | Loss: 0.00071927
Iteration 13/25 | Loss: 0.00071927
Iteration 14/25 | Loss: 0.00071927
Iteration 15/25 | Loss: 0.00071927
Iteration 16/25 | Loss: 0.00071927
Iteration 17/25 | Loss: 0.00071927
Iteration 18/25 | Loss: 0.00071927
Iteration 19/25 | Loss: 0.00071927
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000719267874956131, 0.000719267874956131, 0.000719267874956131, 0.000719267874956131, 0.000719267874956131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000719267874956131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071927
Iteration 2/1000 | Loss: 0.00024846
Iteration 3/1000 | Loss: 0.00153194
Iteration 4/1000 | Loss: 0.00028308
Iteration 5/1000 | Loss: 0.00021860
Iteration 6/1000 | Loss: 0.00105947
Iteration 7/1000 | Loss: 0.00008212
Iteration 8/1000 | Loss: 0.00005386
Iteration 9/1000 | Loss: 0.00007082
Iteration 10/1000 | Loss: 0.00014275
Iteration 11/1000 | Loss: 0.00006535
Iteration 12/1000 | Loss: 0.00015636
Iteration 13/1000 | Loss: 0.00004200
Iteration 14/1000 | Loss: 0.00027728
Iteration 15/1000 | Loss: 0.00170601
Iteration 16/1000 | Loss: 0.00274739
Iteration 17/1000 | Loss: 0.00016723
Iteration 18/1000 | Loss: 0.00048836
Iteration 19/1000 | Loss: 0.00029607
Iteration 20/1000 | Loss: 0.00025213
Iteration 21/1000 | Loss: 0.00016433
Iteration 22/1000 | Loss: 0.00029731
Iteration 23/1000 | Loss: 0.00017448
Iteration 24/1000 | Loss: 0.00043763
Iteration 25/1000 | Loss: 0.00002774
Iteration 26/1000 | Loss: 0.00004963
Iteration 27/1000 | Loss: 0.00002104
Iteration 28/1000 | Loss: 0.00046482
Iteration 29/1000 | Loss: 0.00031354
Iteration 30/1000 | Loss: 0.00002554
Iteration 31/1000 | Loss: 0.00001899
Iteration 32/1000 | Loss: 0.00001829
Iteration 33/1000 | Loss: 0.00032950
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00018074
Iteration 36/1000 | Loss: 0.00069868
Iteration 37/1000 | Loss: 0.00002900
Iteration 38/1000 | Loss: 0.00001710
Iteration 39/1000 | Loss: 0.00015968
Iteration 40/1000 | Loss: 0.00001697
Iteration 41/1000 | Loss: 0.00001669
Iteration 42/1000 | Loss: 0.00001668
Iteration 43/1000 | Loss: 0.00001662
Iteration 44/1000 | Loss: 0.00001650
Iteration 45/1000 | Loss: 0.00001643
Iteration 46/1000 | Loss: 0.00001643
Iteration 47/1000 | Loss: 0.00001637
Iteration 48/1000 | Loss: 0.00001632
Iteration 49/1000 | Loss: 0.00001631
Iteration 50/1000 | Loss: 0.00001631
Iteration 51/1000 | Loss: 0.00001630
Iteration 52/1000 | Loss: 0.00001629
Iteration 53/1000 | Loss: 0.00001628
Iteration 54/1000 | Loss: 0.00001627
Iteration 55/1000 | Loss: 0.00001627
Iteration 56/1000 | Loss: 0.00001626
Iteration 57/1000 | Loss: 0.00001626
Iteration 58/1000 | Loss: 0.00001626
Iteration 59/1000 | Loss: 0.00001625
Iteration 60/1000 | Loss: 0.00001625
Iteration 61/1000 | Loss: 0.00001625
Iteration 62/1000 | Loss: 0.00001625
Iteration 63/1000 | Loss: 0.00001625
Iteration 64/1000 | Loss: 0.00001624
Iteration 65/1000 | Loss: 0.00001624
Iteration 66/1000 | Loss: 0.00001624
Iteration 67/1000 | Loss: 0.00001624
Iteration 68/1000 | Loss: 0.00001624
Iteration 69/1000 | Loss: 0.00001623
Iteration 70/1000 | Loss: 0.00001623
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001618
Iteration 73/1000 | Loss: 0.00001618
Iteration 74/1000 | Loss: 0.00001618
Iteration 75/1000 | Loss: 0.00001618
Iteration 76/1000 | Loss: 0.00001618
Iteration 77/1000 | Loss: 0.00001617
Iteration 78/1000 | Loss: 0.00001617
Iteration 79/1000 | Loss: 0.00001617
Iteration 80/1000 | Loss: 0.00001617
Iteration 81/1000 | Loss: 0.00001617
Iteration 82/1000 | Loss: 0.00001617
Iteration 83/1000 | Loss: 0.00001616
Iteration 84/1000 | Loss: 0.00001616
Iteration 85/1000 | Loss: 0.00001616
Iteration 86/1000 | Loss: 0.00001615
Iteration 87/1000 | Loss: 0.00001615
Iteration 88/1000 | Loss: 0.00001615
Iteration 89/1000 | Loss: 0.00001615
Iteration 90/1000 | Loss: 0.00001615
Iteration 91/1000 | Loss: 0.00001614
Iteration 92/1000 | Loss: 0.00001613
Iteration 93/1000 | Loss: 0.00001613
Iteration 94/1000 | Loss: 0.00001613
Iteration 95/1000 | Loss: 0.00001612
Iteration 96/1000 | Loss: 0.00001612
Iteration 97/1000 | Loss: 0.00001612
Iteration 98/1000 | Loss: 0.00001611
Iteration 99/1000 | Loss: 0.00001611
Iteration 100/1000 | Loss: 0.00001611
Iteration 101/1000 | Loss: 0.00001611
Iteration 102/1000 | Loss: 0.00001610
Iteration 103/1000 | Loss: 0.00001610
Iteration 104/1000 | Loss: 0.00001610
Iteration 105/1000 | Loss: 0.00001610
Iteration 106/1000 | Loss: 0.00001610
Iteration 107/1000 | Loss: 0.00001610
Iteration 108/1000 | Loss: 0.00001609
Iteration 109/1000 | Loss: 0.00001609
Iteration 110/1000 | Loss: 0.00001609
Iteration 111/1000 | Loss: 0.00001608
Iteration 112/1000 | Loss: 0.00001608
Iteration 113/1000 | Loss: 0.00001608
Iteration 114/1000 | Loss: 0.00001608
Iteration 115/1000 | Loss: 0.00001608
Iteration 116/1000 | Loss: 0.00001608
Iteration 117/1000 | Loss: 0.00001608
Iteration 118/1000 | Loss: 0.00001607
Iteration 119/1000 | Loss: 0.00001607
Iteration 120/1000 | Loss: 0.00001607
Iteration 121/1000 | Loss: 0.00001607
Iteration 122/1000 | Loss: 0.00001607
Iteration 123/1000 | Loss: 0.00001607
Iteration 124/1000 | Loss: 0.00001607
Iteration 125/1000 | Loss: 0.00001607
Iteration 126/1000 | Loss: 0.00001607
Iteration 127/1000 | Loss: 0.00001607
Iteration 128/1000 | Loss: 0.00001607
Iteration 129/1000 | Loss: 0.00001606
Iteration 130/1000 | Loss: 0.00001606
Iteration 131/1000 | Loss: 0.00001606
Iteration 132/1000 | Loss: 0.00001606
Iteration 133/1000 | Loss: 0.00001606
Iteration 134/1000 | Loss: 0.00001606
Iteration 135/1000 | Loss: 0.00001606
Iteration 136/1000 | Loss: 0.00001606
Iteration 137/1000 | Loss: 0.00001606
Iteration 138/1000 | Loss: 0.00001606
Iteration 139/1000 | Loss: 0.00001606
Iteration 140/1000 | Loss: 0.00001605
Iteration 141/1000 | Loss: 0.00001605
Iteration 142/1000 | Loss: 0.00001605
Iteration 143/1000 | Loss: 0.00001605
Iteration 144/1000 | Loss: 0.00001604
Iteration 145/1000 | Loss: 0.00001604
Iteration 146/1000 | Loss: 0.00001604
Iteration 147/1000 | Loss: 0.00001603
Iteration 148/1000 | Loss: 0.00001603
Iteration 149/1000 | Loss: 0.00001603
Iteration 150/1000 | Loss: 0.00001603
Iteration 151/1000 | Loss: 0.00001603
Iteration 152/1000 | Loss: 0.00001603
Iteration 153/1000 | Loss: 0.00001603
Iteration 154/1000 | Loss: 0.00001603
Iteration 155/1000 | Loss: 0.00001603
Iteration 156/1000 | Loss: 0.00001603
Iteration 157/1000 | Loss: 0.00001603
Iteration 158/1000 | Loss: 0.00001603
Iteration 159/1000 | Loss: 0.00001603
Iteration 160/1000 | Loss: 0.00001603
Iteration 161/1000 | Loss: 0.00001603
Iteration 162/1000 | Loss: 0.00001603
Iteration 163/1000 | Loss: 0.00001603
Iteration 164/1000 | Loss: 0.00001603
Iteration 165/1000 | Loss: 0.00001603
Iteration 166/1000 | Loss: 0.00001603
Iteration 167/1000 | Loss: 0.00001603
Iteration 168/1000 | Loss: 0.00001603
Iteration 169/1000 | Loss: 0.00001603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.602611519047059e-05, 1.602611519047059e-05, 1.602611519047059e-05, 1.602611519047059e-05, 1.602611519047059e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.602611519047059e-05

Optimization complete. Final v2v error: 3.4024605751037598 mm

Highest mean error: 9.00730037689209 mm for frame 230

Lowest mean error: 2.9711427688598633 mm for frame 137

Saving results

Total time: 125.81168031692505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905192
Iteration 2/25 | Loss: 0.00115449
Iteration 3/25 | Loss: 0.00076037
Iteration 4/25 | Loss: 0.00071646
Iteration 5/25 | Loss: 0.00070953
Iteration 6/25 | Loss: 0.00070795
Iteration 7/25 | Loss: 0.00070782
Iteration 8/25 | Loss: 0.00070782
Iteration 9/25 | Loss: 0.00070782
Iteration 10/25 | Loss: 0.00070782
Iteration 11/25 | Loss: 0.00070782
Iteration 12/25 | Loss: 0.00070782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007078249473124743, 0.0007078249473124743, 0.0007078249473124743, 0.0007078249473124743, 0.0007078249473124743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007078249473124743

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44776654
Iteration 2/25 | Loss: 0.00027430
Iteration 3/25 | Loss: 0.00027427
Iteration 4/25 | Loss: 0.00027427
Iteration 5/25 | Loss: 0.00027427
Iteration 6/25 | Loss: 0.00027427
Iteration 7/25 | Loss: 0.00027427
Iteration 8/25 | Loss: 0.00027427
Iteration 9/25 | Loss: 0.00027427
Iteration 10/25 | Loss: 0.00027427
Iteration 11/25 | Loss: 0.00027427
Iteration 12/25 | Loss: 0.00027427
Iteration 13/25 | Loss: 0.00027427
Iteration 14/25 | Loss: 0.00027427
Iteration 15/25 | Loss: 0.00027427
Iteration 16/25 | Loss: 0.00027427
Iteration 17/25 | Loss: 0.00027427
Iteration 18/25 | Loss: 0.00027427
Iteration 19/25 | Loss: 0.00027427
Iteration 20/25 | Loss: 0.00027427
Iteration 21/25 | Loss: 0.00027427
Iteration 22/25 | Loss: 0.00027427
Iteration 23/25 | Loss: 0.00027427
Iteration 24/25 | Loss: 0.00027427
Iteration 25/25 | Loss: 0.00027427

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027427
Iteration 2/1000 | Loss: 0.00002601
Iteration 3/1000 | Loss: 0.00002048
Iteration 4/1000 | Loss: 0.00001887
Iteration 5/1000 | Loss: 0.00001819
Iteration 6/1000 | Loss: 0.00001777
Iteration 7/1000 | Loss: 0.00001748
Iteration 8/1000 | Loss: 0.00001744
Iteration 9/1000 | Loss: 0.00001736
Iteration 10/1000 | Loss: 0.00001723
Iteration 11/1000 | Loss: 0.00001719
Iteration 12/1000 | Loss: 0.00001716
Iteration 13/1000 | Loss: 0.00001716
Iteration 14/1000 | Loss: 0.00001715
Iteration 15/1000 | Loss: 0.00001713
Iteration 16/1000 | Loss: 0.00001713
Iteration 17/1000 | Loss: 0.00001713
Iteration 18/1000 | Loss: 0.00001712
Iteration 19/1000 | Loss: 0.00001711
Iteration 20/1000 | Loss: 0.00001707
Iteration 21/1000 | Loss: 0.00001700
Iteration 22/1000 | Loss: 0.00001696
Iteration 23/1000 | Loss: 0.00001693
Iteration 24/1000 | Loss: 0.00001692
Iteration 25/1000 | Loss: 0.00001688
Iteration 26/1000 | Loss: 0.00001688
Iteration 27/1000 | Loss: 0.00001685
Iteration 28/1000 | Loss: 0.00001685
Iteration 29/1000 | Loss: 0.00001683
Iteration 30/1000 | Loss: 0.00001683
Iteration 31/1000 | Loss: 0.00001683
Iteration 32/1000 | Loss: 0.00001683
Iteration 33/1000 | Loss: 0.00001683
Iteration 34/1000 | Loss: 0.00001682
Iteration 35/1000 | Loss: 0.00001682
Iteration 36/1000 | Loss: 0.00001682
Iteration 37/1000 | Loss: 0.00001681
Iteration 38/1000 | Loss: 0.00001681
Iteration 39/1000 | Loss: 0.00001681
Iteration 40/1000 | Loss: 0.00001680
Iteration 41/1000 | Loss: 0.00001679
Iteration 42/1000 | Loss: 0.00001679
Iteration 43/1000 | Loss: 0.00001679
Iteration 44/1000 | Loss: 0.00001678
Iteration 45/1000 | Loss: 0.00001678
Iteration 46/1000 | Loss: 0.00001678
Iteration 47/1000 | Loss: 0.00001678
Iteration 48/1000 | Loss: 0.00001678
Iteration 49/1000 | Loss: 0.00001678
Iteration 50/1000 | Loss: 0.00001678
Iteration 51/1000 | Loss: 0.00001678
Iteration 52/1000 | Loss: 0.00001678
Iteration 53/1000 | Loss: 0.00001678
Iteration 54/1000 | Loss: 0.00001677
Iteration 55/1000 | Loss: 0.00001676
Iteration 56/1000 | Loss: 0.00001675
Iteration 57/1000 | Loss: 0.00001674
Iteration 58/1000 | Loss: 0.00001674
Iteration 59/1000 | Loss: 0.00001673
Iteration 60/1000 | Loss: 0.00001673
Iteration 61/1000 | Loss: 0.00001672
Iteration 62/1000 | Loss: 0.00001671
Iteration 63/1000 | Loss: 0.00001671
Iteration 64/1000 | Loss: 0.00001670
Iteration 65/1000 | Loss: 0.00001670
Iteration 66/1000 | Loss: 0.00001670
Iteration 67/1000 | Loss: 0.00001669
Iteration 68/1000 | Loss: 0.00001669
Iteration 69/1000 | Loss: 0.00001669
Iteration 70/1000 | Loss: 0.00001669
Iteration 71/1000 | Loss: 0.00001669
Iteration 72/1000 | Loss: 0.00001669
Iteration 73/1000 | Loss: 0.00001669
Iteration 74/1000 | Loss: 0.00001669
Iteration 75/1000 | Loss: 0.00001669
Iteration 76/1000 | Loss: 0.00001669
Iteration 77/1000 | Loss: 0.00001669
Iteration 78/1000 | Loss: 0.00001669
Iteration 79/1000 | Loss: 0.00001669
Iteration 80/1000 | Loss: 0.00001669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.6690346456016414e-05, 1.6690346456016414e-05, 1.6690346456016414e-05, 1.6690346456016414e-05, 1.6690346456016414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6690346456016414e-05

Optimization complete. Final v2v error: 3.4485111236572266 mm

Highest mean error: 3.531816005706787 mm for frame 34

Lowest mean error: 3.237687826156616 mm for frame 0

Saving results

Total time: 30.59491991996765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00731435
Iteration 2/25 | Loss: 0.00101037
Iteration 3/25 | Loss: 0.00077227
Iteration 4/25 | Loss: 0.00073162
Iteration 5/25 | Loss: 0.00072570
Iteration 6/25 | Loss: 0.00072468
Iteration 7/25 | Loss: 0.00072468
Iteration 8/25 | Loss: 0.00072468
Iteration 9/25 | Loss: 0.00072468
Iteration 10/25 | Loss: 0.00072468
Iteration 11/25 | Loss: 0.00072468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007246782188303769, 0.0007246782188303769, 0.0007246782188303769, 0.0007246782188303769, 0.0007246782188303769]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007246782188303769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.11754274
Iteration 2/25 | Loss: 0.00052825
Iteration 3/25 | Loss: 0.00052822
Iteration 4/25 | Loss: 0.00052822
Iteration 5/25 | Loss: 0.00052822
Iteration 6/25 | Loss: 0.00052822
Iteration 7/25 | Loss: 0.00052822
Iteration 8/25 | Loss: 0.00052822
Iteration 9/25 | Loss: 0.00052822
Iteration 10/25 | Loss: 0.00052822
Iteration 11/25 | Loss: 0.00052822
Iteration 12/25 | Loss: 0.00052822
Iteration 13/25 | Loss: 0.00052822
Iteration 14/25 | Loss: 0.00052822
Iteration 15/25 | Loss: 0.00052822
Iteration 16/25 | Loss: 0.00052822
Iteration 17/25 | Loss: 0.00052822
Iteration 18/25 | Loss: 0.00052822
Iteration 19/25 | Loss: 0.00052822
Iteration 20/25 | Loss: 0.00052822
Iteration 21/25 | Loss: 0.00052822
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005282179336063564, 0.0005282179336063564, 0.0005282179336063564, 0.0005282179336063564, 0.0005282179336063564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005282179336063564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052822
Iteration 2/1000 | Loss: 0.00004376
Iteration 3/1000 | Loss: 0.00002677
Iteration 4/1000 | Loss: 0.00002318
Iteration 5/1000 | Loss: 0.00002092
Iteration 6/1000 | Loss: 0.00001970
Iteration 7/1000 | Loss: 0.00001896
Iteration 8/1000 | Loss: 0.00001856
Iteration 9/1000 | Loss: 0.00001836
Iteration 10/1000 | Loss: 0.00001825
Iteration 11/1000 | Loss: 0.00001812
Iteration 12/1000 | Loss: 0.00001799
Iteration 13/1000 | Loss: 0.00001799
Iteration 14/1000 | Loss: 0.00001796
Iteration 15/1000 | Loss: 0.00001796
Iteration 16/1000 | Loss: 0.00001795
Iteration 17/1000 | Loss: 0.00001795
Iteration 18/1000 | Loss: 0.00001794
Iteration 19/1000 | Loss: 0.00001794
Iteration 20/1000 | Loss: 0.00001793
Iteration 21/1000 | Loss: 0.00001793
Iteration 22/1000 | Loss: 0.00001792
Iteration 23/1000 | Loss: 0.00001787
Iteration 24/1000 | Loss: 0.00001784
Iteration 25/1000 | Loss: 0.00001783
Iteration 26/1000 | Loss: 0.00001782
Iteration 27/1000 | Loss: 0.00001782
Iteration 28/1000 | Loss: 0.00001782
Iteration 29/1000 | Loss: 0.00001781
Iteration 30/1000 | Loss: 0.00001781
Iteration 31/1000 | Loss: 0.00001780
Iteration 32/1000 | Loss: 0.00001780
Iteration 33/1000 | Loss: 0.00001780
Iteration 34/1000 | Loss: 0.00001780
Iteration 35/1000 | Loss: 0.00001779
Iteration 36/1000 | Loss: 0.00001778
Iteration 37/1000 | Loss: 0.00001778
Iteration 38/1000 | Loss: 0.00001778
Iteration 39/1000 | Loss: 0.00001778
Iteration 40/1000 | Loss: 0.00001778
Iteration 41/1000 | Loss: 0.00001778
Iteration 42/1000 | Loss: 0.00001777
Iteration 43/1000 | Loss: 0.00001777
Iteration 44/1000 | Loss: 0.00001777
Iteration 45/1000 | Loss: 0.00001776
Iteration 46/1000 | Loss: 0.00001776
Iteration 47/1000 | Loss: 0.00001776
Iteration 48/1000 | Loss: 0.00001776
Iteration 49/1000 | Loss: 0.00001776
Iteration 50/1000 | Loss: 0.00001776
Iteration 51/1000 | Loss: 0.00001775
Iteration 52/1000 | Loss: 0.00001775
Iteration 53/1000 | Loss: 0.00001775
Iteration 54/1000 | Loss: 0.00001775
Iteration 55/1000 | Loss: 0.00001774
Iteration 56/1000 | Loss: 0.00001774
Iteration 57/1000 | Loss: 0.00001774
Iteration 58/1000 | Loss: 0.00001774
Iteration 59/1000 | Loss: 0.00001774
Iteration 60/1000 | Loss: 0.00001774
Iteration 61/1000 | Loss: 0.00001774
Iteration 62/1000 | Loss: 0.00001774
Iteration 63/1000 | Loss: 0.00001774
Iteration 64/1000 | Loss: 0.00001774
Iteration 65/1000 | Loss: 0.00001774
Iteration 66/1000 | Loss: 0.00001774
Iteration 67/1000 | Loss: 0.00001774
Iteration 68/1000 | Loss: 0.00001773
Iteration 69/1000 | Loss: 0.00001773
Iteration 70/1000 | Loss: 0.00001773
Iteration 71/1000 | Loss: 0.00001773
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001773
Iteration 74/1000 | Loss: 0.00001773
Iteration 75/1000 | Loss: 0.00001773
Iteration 76/1000 | Loss: 0.00001773
Iteration 77/1000 | Loss: 0.00001772
Iteration 78/1000 | Loss: 0.00001772
Iteration 79/1000 | Loss: 0.00001772
Iteration 80/1000 | Loss: 0.00001772
Iteration 81/1000 | Loss: 0.00001772
Iteration 82/1000 | Loss: 0.00001772
Iteration 83/1000 | Loss: 0.00001772
Iteration 84/1000 | Loss: 0.00001772
Iteration 85/1000 | Loss: 0.00001772
Iteration 86/1000 | Loss: 0.00001771
Iteration 87/1000 | Loss: 0.00001771
Iteration 88/1000 | Loss: 0.00001771
Iteration 89/1000 | Loss: 0.00001770
Iteration 90/1000 | Loss: 0.00001770
Iteration 91/1000 | Loss: 0.00001770
Iteration 92/1000 | Loss: 0.00001770
Iteration 93/1000 | Loss: 0.00001770
Iteration 94/1000 | Loss: 0.00001770
Iteration 95/1000 | Loss: 0.00001770
Iteration 96/1000 | Loss: 0.00001770
Iteration 97/1000 | Loss: 0.00001770
Iteration 98/1000 | Loss: 0.00001770
Iteration 99/1000 | Loss: 0.00001770
Iteration 100/1000 | Loss: 0.00001769
Iteration 101/1000 | Loss: 0.00001769
Iteration 102/1000 | Loss: 0.00001769
Iteration 103/1000 | Loss: 0.00001769
Iteration 104/1000 | Loss: 0.00001769
Iteration 105/1000 | Loss: 0.00001769
Iteration 106/1000 | Loss: 0.00001769
Iteration 107/1000 | Loss: 0.00001769
Iteration 108/1000 | Loss: 0.00001769
Iteration 109/1000 | Loss: 0.00001769
Iteration 110/1000 | Loss: 0.00001769
Iteration 111/1000 | Loss: 0.00001768
Iteration 112/1000 | Loss: 0.00001768
Iteration 113/1000 | Loss: 0.00001768
Iteration 114/1000 | Loss: 0.00001768
Iteration 115/1000 | Loss: 0.00001768
Iteration 116/1000 | Loss: 0.00001768
Iteration 117/1000 | Loss: 0.00001768
Iteration 118/1000 | Loss: 0.00001768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.7683469195617363e-05, 1.7683469195617363e-05, 1.7683469195617363e-05, 1.7683469195617363e-05, 1.7683469195617363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7683469195617363e-05

Optimization complete. Final v2v error: 3.4874045848846436 mm

Highest mean error: 3.882502555847168 mm for frame 10

Lowest mean error: 3.0545690059661865 mm for frame 238

Saving results

Total time: 38.49339485168457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889885
Iteration 2/25 | Loss: 0.00112355
Iteration 3/25 | Loss: 0.00087312
Iteration 4/25 | Loss: 0.00082790
Iteration 5/25 | Loss: 0.00081858
Iteration 6/25 | Loss: 0.00081692
Iteration 7/25 | Loss: 0.00081672
Iteration 8/25 | Loss: 0.00081672
Iteration 9/25 | Loss: 0.00081672
Iteration 10/25 | Loss: 0.00081672
Iteration 11/25 | Loss: 0.00081672
Iteration 12/25 | Loss: 0.00081672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008167179767042398, 0.0008167179767042398, 0.0008167179767042398, 0.0008167179767042398, 0.0008167179767042398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008167179767042398

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44006395
Iteration 2/25 | Loss: 0.00043973
Iteration 3/25 | Loss: 0.00043973
Iteration 4/25 | Loss: 0.00043972
Iteration 5/25 | Loss: 0.00043972
Iteration 6/25 | Loss: 0.00043972
Iteration 7/25 | Loss: 0.00043972
Iteration 8/25 | Loss: 0.00043972
Iteration 9/25 | Loss: 0.00043972
Iteration 10/25 | Loss: 0.00043972
Iteration 11/25 | Loss: 0.00043972
Iteration 12/25 | Loss: 0.00043972
Iteration 13/25 | Loss: 0.00043972
Iteration 14/25 | Loss: 0.00043972
Iteration 15/25 | Loss: 0.00043972
Iteration 16/25 | Loss: 0.00043972
Iteration 17/25 | Loss: 0.00043972
Iteration 18/25 | Loss: 0.00043972
Iteration 19/25 | Loss: 0.00043972
Iteration 20/25 | Loss: 0.00043972
Iteration 21/25 | Loss: 0.00043972
Iteration 22/25 | Loss: 0.00043972
Iteration 23/25 | Loss: 0.00043972
Iteration 24/25 | Loss: 0.00043972
Iteration 25/25 | Loss: 0.00043972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043972
Iteration 2/1000 | Loss: 0.00005769
Iteration 3/1000 | Loss: 0.00004570
Iteration 4/1000 | Loss: 0.00004047
Iteration 5/1000 | Loss: 0.00003830
Iteration 6/1000 | Loss: 0.00003688
Iteration 7/1000 | Loss: 0.00003597
Iteration 8/1000 | Loss: 0.00003529
Iteration 9/1000 | Loss: 0.00003487
Iteration 10/1000 | Loss: 0.00003456
Iteration 11/1000 | Loss: 0.00003432
Iteration 12/1000 | Loss: 0.00003424
Iteration 13/1000 | Loss: 0.00003416
Iteration 14/1000 | Loss: 0.00003411
Iteration 15/1000 | Loss: 0.00003410
Iteration 16/1000 | Loss: 0.00003410
Iteration 17/1000 | Loss: 0.00003409
Iteration 18/1000 | Loss: 0.00003409
Iteration 19/1000 | Loss: 0.00003409
Iteration 20/1000 | Loss: 0.00003409
Iteration 21/1000 | Loss: 0.00003409
Iteration 22/1000 | Loss: 0.00003409
Iteration 23/1000 | Loss: 0.00003408
Iteration 24/1000 | Loss: 0.00003403
Iteration 25/1000 | Loss: 0.00003403
Iteration 26/1000 | Loss: 0.00003398
Iteration 27/1000 | Loss: 0.00003397
Iteration 28/1000 | Loss: 0.00003396
Iteration 29/1000 | Loss: 0.00003396
Iteration 30/1000 | Loss: 0.00003395
Iteration 31/1000 | Loss: 0.00003394
Iteration 32/1000 | Loss: 0.00003393
Iteration 33/1000 | Loss: 0.00003393
Iteration 34/1000 | Loss: 0.00003392
Iteration 35/1000 | Loss: 0.00003392
Iteration 36/1000 | Loss: 0.00003392
Iteration 37/1000 | Loss: 0.00003392
Iteration 38/1000 | Loss: 0.00003392
Iteration 39/1000 | Loss: 0.00003392
Iteration 40/1000 | Loss: 0.00003392
Iteration 41/1000 | Loss: 0.00003392
Iteration 42/1000 | Loss: 0.00003391
Iteration 43/1000 | Loss: 0.00003390
Iteration 44/1000 | Loss: 0.00003390
Iteration 45/1000 | Loss: 0.00003388
Iteration 46/1000 | Loss: 0.00003388
Iteration 47/1000 | Loss: 0.00003387
Iteration 48/1000 | Loss: 0.00003387
Iteration 49/1000 | Loss: 0.00003387
Iteration 50/1000 | Loss: 0.00003386
Iteration 51/1000 | Loss: 0.00003386
Iteration 52/1000 | Loss: 0.00003385
Iteration 53/1000 | Loss: 0.00003385
Iteration 54/1000 | Loss: 0.00003385
Iteration 55/1000 | Loss: 0.00003385
Iteration 56/1000 | Loss: 0.00003385
Iteration 57/1000 | Loss: 0.00003385
Iteration 58/1000 | Loss: 0.00003385
Iteration 59/1000 | Loss: 0.00003384
Iteration 60/1000 | Loss: 0.00003383
Iteration 61/1000 | Loss: 0.00003383
Iteration 62/1000 | Loss: 0.00003383
Iteration 63/1000 | Loss: 0.00003383
Iteration 64/1000 | Loss: 0.00003383
Iteration 65/1000 | Loss: 0.00003383
Iteration 66/1000 | Loss: 0.00003382
Iteration 67/1000 | Loss: 0.00003382
Iteration 68/1000 | Loss: 0.00003382
Iteration 69/1000 | Loss: 0.00003382
Iteration 70/1000 | Loss: 0.00003381
Iteration 71/1000 | Loss: 0.00003381
Iteration 72/1000 | Loss: 0.00003381
Iteration 73/1000 | Loss: 0.00003380
Iteration 74/1000 | Loss: 0.00003380
Iteration 75/1000 | Loss: 0.00003379
Iteration 76/1000 | Loss: 0.00003379
Iteration 77/1000 | Loss: 0.00003379
Iteration 78/1000 | Loss: 0.00003379
Iteration 79/1000 | Loss: 0.00003378
Iteration 80/1000 | Loss: 0.00003378
Iteration 81/1000 | Loss: 0.00003378
Iteration 82/1000 | Loss: 0.00003378
Iteration 83/1000 | Loss: 0.00003377
Iteration 84/1000 | Loss: 0.00003377
Iteration 85/1000 | Loss: 0.00003377
Iteration 86/1000 | Loss: 0.00003377
Iteration 87/1000 | Loss: 0.00003377
Iteration 88/1000 | Loss: 0.00003377
Iteration 89/1000 | Loss: 0.00003377
Iteration 90/1000 | Loss: 0.00003376
Iteration 91/1000 | Loss: 0.00003376
Iteration 92/1000 | Loss: 0.00003376
Iteration 93/1000 | Loss: 0.00003376
Iteration 94/1000 | Loss: 0.00003376
Iteration 95/1000 | Loss: 0.00003376
Iteration 96/1000 | Loss: 0.00003376
Iteration 97/1000 | Loss: 0.00003376
Iteration 98/1000 | Loss: 0.00003376
Iteration 99/1000 | Loss: 0.00003376
Iteration 100/1000 | Loss: 0.00003376
Iteration 101/1000 | Loss: 0.00003376
Iteration 102/1000 | Loss: 0.00003376
Iteration 103/1000 | Loss: 0.00003376
Iteration 104/1000 | Loss: 0.00003376
Iteration 105/1000 | Loss: 0.00003376
Iteration 106/1000 | Loss: 0.00003376
Iteration 107/1000 | Loss: 0.00003376
Iteration 108/1000 | Loss: 0.00003376
Iteration 109/1000 | Loss: 0.00003376
Iteration 110/1000 | Loss: 0.00003376
Iteration 111/1000 | Loss: 0.00003376
Iteration 112/1000 | Loss: 0.00003376
Iteration 113/1000 | Loss: 0.00003376
Iteration 114/1000 | Loss: 0.00003376
Iteration 115/1000 | Loss: 0.00003376
Iteration 116/1000 | Loss: 0.00003376
Iteration 117/1000 | Loss: 0.00003376
Iteration 118/1000 | Loss: 0.00003376
Iteration 119/1000 | Loss: 0.00003376
Iteration 120/1000 | Loss: 0.00003376
Iteration 121/1000 | Loss: 0.00003376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [3.376216773176566e-05, 3.376216773176566e-05, 3.376216773176566e-05, 3.376216773176566e-05, 3.376216773176566e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.376216773176566e-05

Optimization complete. Final v2v error: 4.870125770568848 mm

Highest mean error: 5.305531978607178 mm for frame 20

Lowest mean error: 4.391560077667236 mm for frame 84

Saving results

Total time: 35.031686782836914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00366323
Iteration 2/25 | Loss: 0.00094341
Iteration 3/25 | Loss: 0.00071543
Iteration 4/25 | Loss: 0.00066899
Iteration 5/25 | Loss: 0.00065646
Iteration 6/25 | Loss: 0.00065253
Iteration 7/25 | Loss: 0.00065156
Iteration 8/25 | Loss: 0.00065156
Iteration 9/25 | Loss: 0.00065156
Iteration 10/25 | Loss: 0.00065156
Iteration 11/25 | Loss: 0.00065156
Iteration 12/25 | Loss: 0.00065156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006515624700114131, 0.0006515624700114131, 0.0006515624700114131, 0.0006515624700114131, 0.0006515624700114131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006515624700114131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45025527
Iteration 2/25 | Loss: 0.00026731
Iteration 3/25 | Loss: 0.00026730
Iteration 4/25 | Loss: 0.00026730
Iteration 5/25 | Loss: 0.00026730
Iteration 6/25 | Loss: 0.00026730
Iteration 7/25 | Loss: 0.00026730
Iteration 8/25 | Loss: 0.00026730
Iteration 9/25 | Loss: 0.00026730
Iteration 10/25 | Loss: 0.00026730
Iteration 11/25 | Loss: 0.00026730
Iteration 12/25 | Loss: 0.00026730
Iteration 13/25 | Loss: 0.00026730
Iteration 14/25 | Loss: 0.00026730
Iteration 15/25 | Loss: 0.00026730
Iteration 16/25 | Loss: 0.00026730
Iteration 17/25 | Loss: 0.00026730
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00026729851379059255, 0.00026729851379059255, 0.00026729851379059255, 0.00026729851379059255, 0.00026729851379059255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026729851379059255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026730
Iteration 2/1000 | Loss: 0.00003324
Iteration 3/1000 | Loss: 0.00002308
Iteration 4/1000 | Loss: 0.00002101
Iteration 5/1000 | Loss: 0.00001991
Iteration 6/1000 | Loss: 0.00001897
Iteration 7/1000 | Loss: 0.00001857
Iteration 8/1000 | Loss: 0.00001814
Iteration 9/1000 | Loss: 0.00001790
Iteration 10/1000 | Loss: 0.00001774
Iteration 11/1000 | Loss: 0.00001771
Iteration 12/1000 | Loss: 0.00001768
Iteration 13/1000 | Loss: 0.00001767
Iteration 14/1000 | Loss: 0.00001762
Iteration 15/1000 | Loss: 0.00001762
Iteration 16/1000 | Loss: 0.00001758
Iteration 17/1000 | Loss: 0.00001756
Iteration 18/1000 | Loss: 0.00001753
Iteration 19/1000 | Loss: 0.00001752
Iteration 20/1000 | Loss: 0.00001751
Iteration 21/1000 | Loss: 0.00001751
Iteration 22/1000 | Loss: 0.00001750
Iteration 23/1000 | Loss: 0.00001747
Iteration 24/1000 | Loss: 0.00001745
Iteration 25/1000 | Loss: 0.00001743
Iteration 26/1000 | Loss: 0.00001743
Iteration 27/1000 | Loss: 0.00001742
Iteration 28/1000 | Loss: 0.00001741
Iteration 29/1000 | Loss: 0.00001736
Iteration 30/1000 | Loss: 0.00001736
Iteration 31/1000 | Loss: 0.00001734
Iteration 32/1000 | Loss: 0.00001734
Iteration 33/1000 | Loss: 0.00001730
Iteration 34/1000 | Loss: 0.00001729
Iteration 35/1000 | Loss: 0.00001727
Iteration 36/1000 | Loss: 0.00001724
Iteration 37/1000 | Loss: 0.00001723
Iteration 38/1000 | Loss: 0.00001723
Iteration 39/1000 | Loss: 0.00001722
Iteration 40/1000 | Loss: 0.00001722
Iteration 41/1000 | Loss: 0.00001722
Iteration 42/1000 | Loss: 0.00001721
Iteration 43/1000 | Loss: 0.00001721
Iteration 44/1000 | Loss: 0.00001721
Iteration 45/1000 | Loss: 0.00001721
Iteration 46/1000 | Loss: 0.00001720
Iteration 47/1000 | Loss: 0.00001720
Iteration 48/1000 | Loss: 0.00001718
Iteration 49/1000 | Loss: 0.00001718
Iteration 50/1000 | Loss: 0.00001718
Iteration 51/1000 | Loss: 0.00001718
Iteration 52/1000 | Loss: 0.00001717
Iteration 53/1000 | Loss: 0.00001717
Iteration 54/1000 | Loss: 0.00001717
Iteration 55/1000 | Loss: 0.00001717
Iteration 56/1000 | Loss: 0.00001716
Iteration 57/1000 | Loss: 0.00001716
Iteration 58/1000 | Loss: 0.00001716
Iteration 59/1000 | Loss: 0.00001716
Iteration 60/1000 | Loss: 0.00001716
Iteration 61/1000 | Loss: 0.00001716
Iteration 62/1000 | Loss: 0.00001716
Iteration 63/1000 | Loss: 0.00001716
Iteration 64/1000 | Loss: 0.00001716
Iteration 65/1000 | Loss: 0.00001716
Iteration 66/1000 | Loss: 0.00001716
Iteration 67/1000 | Loss: 0.00001715
Iteration 68/1000 | Loss: 0.00001715
Iteration 69/1000 | Loss: 0.00001714
Iteration 70/1000 | Loss: 0.00001714
Iteration 71/1000 | Loss: 0.00001714
Iteration 72/1000 | Loss: 0.00001713
Iteration 73/1000 | Loss: 0.00001713
Iteration 74/1000 | Loss: 0.00001713
Iteration 75/1000 | Loss: 0.00001712
Iteration 76/1000 | Loss: 0.00001712
Iteration 77/1000 | Loss: 0.00001711
Iteration 78/1000 | Loss: 0.00001711
Iteration 79/1000 | Loss: 0.00001711
Iteration 80/1000 | Loss: 0.00001711
Iteration 81/1000 | Loss: 0.00001711
Iteration 82/1000 | Loss: 0.00001711
Iteration 83/1000 | Loss: 0.00001711
Iteration 84/1000 | Loss: 0.00001710
Iteration 85/1000 | Loss: 0.00001710
Iteration 86/1000 | Loss: 0.00001710
Iteration 87/1000 | Loss: 0.00001710
Iteration 88/1000 | Loss: 0.00001710
Iteration 89/1000 | Loss: 0.00001710
Iteration 90/1000 | Loss: 0.00001709
Iteration 91/1000 | Loss: 0.00001709
Iteration 92/1000 | Loss: 0.00001708
Iteration 93/1000 | Loss: 0.00001708
Iteration 94/1000 | Loss: 0.00001708
Iteration 95/1000 | Loss: 0.00001708
Iteration 96/1000 | Loss: 0.00001708
Iteration 97/1000 | Loss: 0.00001708
Iteration 98/1000 | Loss: 0.00001708
Iteration 99/1000 | Loss: 0.00001707
Iteration 100/1000 | Loss: 0.00001707
Iteration 101/1000 | Loss: 0.00001707
Iteration 102/1000 | Loss: 0.00001707
Iteration 103/1000 | Loss: 0.00001706
Iteration 104/1000 | Loss: 0.00001706
Iteration 105/1000 | Loss: 0.00001706
Iteration 106/1000 | Loss: 0.00001706
Iteration 107/1000 | Loss: 0.00001706
Iteration 108/1000 | Loss: 0.00001706
Iteration 109/1000 | Loss: 0.00001706
Iteration 110/1000 | Loss: 0.00001705
Iteration 111/1000 | Loss: 0.00001705
Iteration 112/1000 | Loss: 0.00001705
Iteration 113/1000 | Loss: 0.00001705
Iteration 114/1000 | Loss: 0.00001705
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001704
Iteration 117/1000 | Loss: 0.00001704
Iteration 118/1000 | Loss: 0.00001704
Iteration 119/1000 | Loss: 0.00001704
Iteration 120/1000 | Loss: 0.00001703
Iteration 121/1000 | Loss: 0.00001703
Iteration 122/1000 | Loss: 0.00001703
Iteration 123/1000 | Loss: 0.00001703
Iteration 124/1000 | Loss: 0.00001703
Iteration 125/1000 | Loss: 0.00001703
Iteration 126/1000 | Loss: 0.00001703
Iteration 127/1000 | Loss: 0.00001703
Iteration 128/1000 | Loss: 0.00001703
Iteration 129/1000 | Loss: 0.00001703
Iteration 130/1000 | Loss: 0.00001703
Iteration 131/1000 | Loss: 0.00001703
Iteration 132/1000 | Loss: 0.00001703
Iteration 133/1000 | Loss: 0.00001703
Iteration 134/1000 | Loss: 0.00001702
Iteration 135/1000 | Loss: 0.00001702
Iteration 136/1000 | Loss: 0.00001702
Iteration 137/1000 | Loss: 0.00001702
Iteration 138/1000 | Loss: 0.00001702
Iteration 139/1000 | Loss: 0.00001702
Iteration 140/1000 | Loss: 0.00001702
Iteration 141/1000 | Loss: 0.00001701
Iteration 142/1000 | Loss: 0.00001701
Iteration 143/1000 | Loss: 0.00001701
Iteration 144/1000 | Loss: 0.00001701
Iteration 145/1000 | Loss: 0.00001701
Iteration 146/1000 | Loss: 0.00001700
Iteration 147/1000 | Loss: 0.00001700
Iteration 148/1000 | Loss: 0.00001700
Iteration 149/1000 | Loss: 0.00001700
Iteration 150/1000 | Loss: 0.00001700
Iteration 151/1000 | Loss: 0.00001700
Iteration 152/1000 | Loss: 0.00001700
Iteration 153/1000 | Loss: 0.00001700
Iteration 154/1000 | Loss: 0.00001700
Iteration 155/1000 | Loss: 0.00001700
Iteration 156/1000 | Loss: 0.00001700
Iteration 157/1000 | Loss: 0.00001700
Iteration 158/1000 | Loss: 0.00001699
Iteration 159/1000 | Loss: 0.00001699
Iteration 160/1000 | Loss: 0.00001699
Iteration 161/1000 | Loss: 0.00001699
Iteration 162/1000 | Loss: 0.00001699
Iteration 163/1000 | Loss: 0.00001699
Iteration 164/1000 | Loss: 0.00001699
Iteration 165/1000 | Loss: 0.00001698
Iteration 166/1000 | Loss: 0.00001698
Iteration 167/1000 | Loss: 0.00001698
Iteration 168/1000 | Loss: 0.00001698
Iteration 169/1000 | Loss: 0.00001698
Iteration 170/1000 | Loss: 0.00001698
Iteration 171/1000 | Loss: 0.00001698
Iteration 172/1000 | Loss: 0.00001698
Iteration 173/1000 | Loss: 0.00001698
Iteration 174/1000 | Loss: 0.00001697
Iteration 175/1000 | Loss: 0.00001697
Iteration 176/1000 | Loss: 0.00001697
Iteration 177/1000 | Loss: 0.00001697
Iteration 178/1000 | Loss: 0.00001697
Iteration 179/1000 | Loss: 0.00001696
Iteration 180/1000 | Loss: 0.00001696
Iteration 181/1000 | Loss: 0.00001696
Iteration 182/1000 | Loss: 0.00001696
Iteration 183/1000 | Loss: 0.00001696
Iteration 184/1000 | Loss: 0.00001696
Iteration 185/1000 | Loss: 0.00001696
Iteration 186/1000 | Loss: 0.00001696
Iteration 187/1000 | Loss: 0.00001695
Iteration 188/1000 | Loss: 0.00001695
Iteration 189/1000 | Loss: 0.00001695
Iteration 190/1000 | Loss: 0.00001695
Iteration 191/1000 | Loss: 0.00001695
Iteration 192/1000 | Loss: 0.00001695
Iteration 193/1000 | Loss: 0.00001694
Iteration 194/1000 | Loss: 0.00001694
Iteration 195/1000 | Loss: 0.00001694
Iteration 196/1000 | Loss: 0.00001694
Iteration 197/1000 | Loss: 0.00001694
Iteration 198/1000 | Loss: 0.00001694
Iteration 199/1000 | Loss: 0.00001694
Iteration 200/1000 | Loss: 0.00001694
Iteration 201/1000 | Loss: 0.00001694
Iteration 202/1000 | Loss: 0.00001694
Iteration 203/1000 | Loss: 0.00001693
Iteration 204/1000 | Loss: 0.00001693
Iteration 205/1000 | Loss: 0.00001693
Iteration 206/1000 | Loss: 0.00001693
Iteration 207/1000 | Loss: 0.00001693
Iteration 208/1000 | Loss: 0.00001693
Iteration 209/1000 | Loss: 0.00001693
Iteration 210/1000 | Loss: 0.00001693
Iteration 211/1000 | Loss: 0.00001693
Iteration 212/1000 | Loss: 0.00001693
Iteration 213/1000 | Loss: 0.00001693
Iteration 214/1000 | Loss: 0.00001693
Iteration 215/1000 | Loss: 0.00001692
Iteration 216/1000 | Loss: 0.00001692
Iteration 217/1000 | Loss: 0.00001692
Iteration 218/1000 | Loss: 0.00001692
Iteration 219/1000 | Loss: 0.00001692
Iteration 220/1000 | Loss: 0.00001692
Iteration 221/1000 | Loss: 0.00001692
Iteration 222/1000 | Loss: 0.00001692
Iteration 223/1000 | Loss: 0.00001692
Iteration 224/1000 | Loss: 0.00001692
Iteration 225/1000 | Loss: 0.00001692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.6923455405049026e-05, 1.6923455405049026e-05, 1.6923455405049026e-05, 1.6923455405049026e-05, 1.6923455405049026e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6923455405049026e-05

Optimization complete. Final v2v error: 3.390453815460205 mm

Highest mean error: 3.6259636878967285 mm for frame 175

Lowest mean error: 3.182621717453003 mm for frame 147

Saving results

Total time: 45.58452391624451
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017161
Iteration 2/25 | Loss: 0.00290712
Iteration 3/25 | Loss: 0.00138695
Iteration 4/25 | Loss: 0.00124338
Iteration 5/25 | Loss: 0.00112194
Iteration 6/25 | Loss: 0.00110146
Iteration 7/25 | Loss: 0.00098714
Iteration 8/25 | Loss: 0.00089831
Iteration 9/25 | Loss: 0.00084326
Iteration 10/25 | Loss: 0.00079278
Iteration 11/25 | Loss: 0.00071678
Iteration 12/25 | Loss: 0.00068397
Iteration 13/25 | Loss: 0.00067038
Iteration 14/25 | Loss: 0.00066214
Iteration 15/25 | Loss: 0.00066341
Iteration 16/25 | Loss: 0.00064581
Iteration 17/25 | Loss: 0.00064412
Iteration 18/25 | Loss: 0.00063178
Iteration 19/25 | Loss: 0.00062448
Iteration 20/25 | Loss: 0.00062450
Iteration 21/25 | Loss: 0.00061830
Iteration 22/25 | Loss: 0.00061764
Iteration 23/25 | Loss: 0.00061762
Iteration 24/25 | Loss: 0.00061762
Iteration 25/25 | Loss: 0.00061762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47075725
Iteration 2/25 | Loss: 0.00042045
Iteration 3/25 | Loss: 0.00042044
Iteration 4/25 | Loss: 0.00042044
Iteration 5/25 | Loss: 0.00042044
Iteration 6/25 | Loss: 0.00042044
Iteration 7/25 | Loss: 0.00042044
Iteration 8/25 | Loss: 0.00042044
Iteration 9/25 | Loss: 0.00042044
Iteration 10/25 | Loss: 0.00042044
Iteration 11/25 | Loss: 0.00042044
Iteration 12/25 | Loss: 0.00042044
Iteration 13/25 | Loss: 0.00042044
Iteration 14/25 | Loss: 0.00042044
Iteration 15/25 | Loss: 0.00042044
Iteration 16/25 | Loss: 0.00042044
Iteration 17/25 | Loss: 0.00042044
Iteration 18/25 | Loss: 0.00042044
Iteration 19/25 | Loss: 0.00042044
Iteration 20/25 | Loss: 0.00042044
Iteration 21/25 | Loss: 0.00042044
Iteration 22/25 | Loss: 0.00042044
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00042044263682328165, 0.00042044263682328165, 0.00042044263682328165, 0.00042044263682328165, 0.00042044263682328165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00042044263682328165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042044
Iteration 2/1000 | Loss: 0.00004340
Iteration 3/1000 | Loss: 0.00005803
Iteration 4/1000 | Loss: 0.00007165
Iteration 5/1000 | Loss: 0.00001806
Iteration 6/1000 | Loss: 0.00006126
Iteration 7/1000 | Loss: 0.00037227
Iteration 8/1000 | Loss: 0.00012942
Iteration 9/1000 | Loss: 0.00004671
Iteration 10/1000 | Loss: 0.00006930
Iteration 11/1000 | Loss: 0.00009962
Iteration 12/1000 | Loss: 0.00001588
Iteration 13/1000 | Loss: 0.00001537
Iteration 14/1000 | Loss: 0.00006331
Iteration 15/1000 | Loss: 0.00001513
Iteration 16/1000 | Loss: 0.00005551
Iteration 17/1000 | Loss: 0.00017889
Iteration 18/1000 | Loss: 0.00005013
Iteration 19/1000 | Loss: 0.00002335
Iteration 20/1000 | Loss: 0.00001533
Iteration 21/1000 | Loss: 0.00001467
Iteration 22/1000 | Loss: 0.00001465
Iteration 23/1000 | Loss: 0.00001465
Iteration 24/1000 | Loss: 0.00001464
Iteration 25/1000 | Loss: 0.00001461
Iteration 26/1000 | Loss: 0.00001461
Iteration 27/1000 | Loss: 0.00001460
Iteration 28/1000 | Loss: 0.00001544
Iteration 29/1000 | Loss: 0.00001475
Iteration 30/1000 | Loss: 0.00001481
Iteration 31/1000 | Loss: 0.00001481
Iteration 32/1000 | Loss: 0.00001463
Iteration 33/1000 | Loss: 0.00001452
Iteration 34/1000 | Loss: 0.00001454
Iteration 35/1000 | Loss: 0.00001449
Iteration 36/1000 | Loss: 0.00001449
Iteration 37/1000 | Loss: 0.00001449
Iteration 38/1000 | Loss: 0.00001449
Iteration 39/1000 | Loss: 0.00001449
Iteration 40/1000 | Loss: 0.00001449
Iteration 41/1000 | Loss: 0.00001449
Iteration 42/1000 | Loss: 0.00001449
Iteration 43/1000 | Loss: 0.00001449
Iteration 44/1000 | Loss: 0.00001449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 44. Stopping optimization.
Last 5 losses: [1.448945295123849e-05, 1.448945295123849e-05, 1.448945295123849e-05, 1.448945295123849e-05, 1.448945295123849e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.448945295123849e-05

Optimization complete. Final v2v error: 3.148364543914795 mm

Highest mean error: 3.6660287380218506 mm for frame 89

Lowest mean error: 2.7563095092773438 mm for frame 5

Saving results

Total time: 73.61369442939758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846448
Iteration 2/25 | Loss: 0.00081282
Iteration 3/25 | Loss: 0.00063207
Iteration 4/25 | Loss: 0.00059845
Iteration 5/25 | Loss: 0.00059022
Iteration 6/25 | Loss: 0.00058812
Iteration 7/25 | Loss: 0.00058768
Iteration 8/25 | Loss: 0.00058768
Iteration 9/25 | Loss: 0.00058768
Iteration 10/25 | Loss: 0.00058768
Iteration 11/25 | Loss: 0.00058768
Iteration 12/25 | Loss: 0.00058768
Iteration 13/25 | Loss: 0.00058768
Iteration 14/25 | Loss: 0.00058768
Iteration 15/25 | Loss: 0.00058768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005876762443222106, 0.0005876762443222106, 0.0005876762443222106, 0.0005876762443222106, 0.0005876762443222106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005876762443222106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45797718
Iteration 2/25 | Loss: 0.00024748
Iteration 3/25 | Loss: 0.00024748
Iteration 4/25 | Loss: 0.00024748
Iteration 5/25 | Loss: 0.00024748
Iteration 6/25 | Loss: 0.00024748
Iteration 7/25 | Loss: 0.00024748
Iteration 8/25 | Loss: 0.00024748
Iteration 9/25 | Loss: 0.00024748
Iteration 10/25 | Loss: 0.00024748
Iteration 11/25 | Loss: 0.00024748
Iteration 12/25 | Loss: 0.00024748
Iteration 13/25 | Loss: 0.00024748
Iteration 14/25 | Loss: 0.00024748
Iteration 15/25 | Loss: 0.00024748
Iteration 16/25 | Loss: 0.00024748
Iteration 17/25 | Loss: 0.00024748
Iteration 18/25 | Loss: 0.00024748
Iteration 19/25 | Loss: 0.00024748
Iteration 20/25 | Loss: 0.00024748
Iteration 21/25 | Loss: 0.00024748
Iteration 22/25 | Loss: 0.00024748
Iteration 23/25 | Loss: 0.00024748
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002474771172273904, 0.0002474771172273904, 0.0002474771172273904, 0.0002474771172273904, 0.0002474771172273904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002474771172273904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024748
Iteration 2/1000 | Loss: 0.00001879
Iteration 3/1000 | Loss: 0.00001436
Iteration 4/1000 | Loss: 0.00001335
Iteration 5/1000 | Loss: 0.00001245
Iteration 6/1000 | Loss: 0.00001207
Iteration 7/1000 | Loss: 0.00001177
Iteration 8/1000 | Loss: 0.00001174
Iteration 9/1000 | Loss: 0.00001157
Iteration 10/1000 | Loss: 0.00001153
Iteration 11/1000 | Loss: 0.00001148
Iteration 12/1000 | Loss: 0.00001147
Iteration 13/1000 | Loss: 0.00001145
Iteration 14/1000 | Loss: 0.00001143
Iteration 15/1000 | Loss: 0.00001142
Iteration 16/1000 | Loss: 0.00001138
Iteration 17/1000 | Loss: 0.00001136
Iteration 18/1000 | Loss: 0.00001135
Iteration 19/1000 | Loss: 0.00001134
Iteration 20/1000 | Loss: 0.00001134
Iteration 21/1000 | Loss: 0.00001133
Iteration 22/1000 | Loss: 0.00001133
Iteration 23/1000 | Loss: 0.00001133
Iteration 24/1000 | Loss: 0.00001133
Iteration 25/1000 | Loss: 0.00001132
Iteration 26/1000 | Loss: 0.00001132
Iteration 27/1000 | Loss: 0.00001131
Iteration 28/1000 | Loss: 0.00001131
Iteration 29/1000 | Loss: 0.00001130
Iteration 30/1000 | Loss: 0.00001130
Iteration 31/1000 | Loss: 0.00001130
Iteration 32/1000 | Loss: 0.00001129
Iteration 33/1000 | Loss: 0.00001128
Iteration 34/1000 | Loss: 0.00001128
Iteration 35/1000 | Loss: 0.00001127
Iteration 36/1000 | Loss: 0.00001125
Iteration 37/1000 | Loss: 0.00001125
Iteration 38/1000 | Loss: 0.00001124
Iteration 39/1000 | Loss: 0.00001124
Iteration 40/1000 | Loss: 0.00001124
Iteration 41/1000 | Loss: 0.00001123
Iteration 42/1000 | Loss: 0.00001123
Iteration 43/1000 | Loss: 0.00001123
Iteration 44/1000 | Loss: 0.00001123
Iteration 45/1000 | Loss: 0.00001122
Iteration 46/1000 | Loss: 0.00001121
Iteration 47/1000 | Loss: 0.00001121
Iteration 48/1000 | Loss: 0.00001121
Iteration 49/1000 | Loss: 0.00001121
Iteration 50/1000 | Loss: 0.00001121
Iteration 51/1000 | Loss: 0.00001121
Iteration 52/1000 | Loss: 0.00001121
Iteration 53/1000 | Loss: 0.00001121
Iteration 54/1000 | Loss: 0.00001121
Iteration 55/1000 | Loss: 0.00001120
Iteration 56/1000 | Loss: 0.00001120
Iteration 57/1000 | Loss: 0.00001120
Iteration 58/1000 | Loss: 0.00001119
Iteration 59/1000 | Loss: 0.00001119
Iteration 60/1000 | Loss: 0.00001119
Iteration 61/1000 | Loss: 0.00001118
Iteration 62/1000 | Loss: 0.00001118
Iteration 63/1000 | Loss: 0.00001118
Iteration 64/1000 | Loss: 0.00001118
Iteration 65/1000 | Loss: 0.00001118
Iteration 66/1000 | Loss: 0.00001117
Iteration 67/1000 | Loss: 0.00001117
Iteration 68/1000 | Loss: 0.00001117
Iteration 69/1000 | Loss: 0.00001117
Iteration 70/1000 | Loss: 0.00001116
Iteration 71/1000 | Loss: 0.00001116
Iteration 72/1000 | Loss: 0.00001116
Iteration 73/1000 | Loss: 0.00001116
Iteration 74/1000 | Loss: 0.00001116
Iteration 75/1000 | Loss: 0.00001116
Iteration 76/1000 | Loss: 0.00001116
Iteration 77/1000 | Loss: 0.00001116
Iteration 78/1000 | Loss: 0.00001115
Iteration 79/1000 | Loss: 0.00001115
Iteration 80/1000 | Loss: 0.00001115
Iteration 81/1000 | Loss: 0.00001115
Iteration 82/1000 | Loss: 0.00001115
Iteration 83/1000 | Loss: 0.00001115
Iteration 84/1000 | Loss: 0.00001114
Iteration 85/1000 | Loss: 0.00001114
Iteration 86/1000 | Loss: 0.00001114
Iteration 87/1000 | Loss: 0.00001114
Iteration 88/1000 | Loss: 0.00001113
Iteration 89/1000 | Loss: 0.00001113
Iteration 90/1000 | Loss: 0.00001113
Iteration 91/1000 | Loss: 0.00001112
Iteration 92/1000 | Loss: 0.00001112
Iteration 93/1000 | Loss: 0.00001111
Iteration 94/1000 | Loss: 0.00001111
Iteration 95/1000 | Loss: 0.00001111
Iteration 96/1000 | Loss: 0.00001110
Iteration 97/1000 | Loss: 0.00001109
Iteration 98/1000 | Loss: 0.00001109
Iteration 99/1000 | Loss: 0.00001109
Iteration 100/1000 | Loss: 0.00001109
Iteration 101/1000 | Loss: 0.00001109
Iteration 102/1000 | Loss: 0.00001109
Iteration 103/1000 | Loss: 0.00001109
Iteration 104/1000 | Loss: 0.00001109
Iteration 105/1000 | Loss: 0.00001109
Iteration 106/1000 | Loss: 0.00001109
Iteration 107/1000 | Loss: 0.00001109
Iteration 108/1000 | Loss: 0.00001109
Iteration 109/1000 | Loss: 0.00001109
Iteration 110/1000 | Loss: 0.00001109
Iteration 111/1000 | Loss: 0.00001109
Iteration 112/1000 | Loss: 0.00001109
Iteration 113/1000 | Loss: 0.00001109
Iteration 114/1000 | Loss: 0.00001109
Iteration 115/1000 | Loss: 0.00001109
Iteration 116/1000 | Loss: 0.00001109
Iteration 117/1000 | Loss: 0.00001109
Iteration 118/1000 | Loss: 0.00001109
Iteration 119/1000 | Loss: 0.00001109
Iteration 120/1000 | Loss: 0.00001109
Iteration 121/1000 | Loss: 0.00001109
Iteration 122/1000 | Loss: 0.00001109
Iteration 123/1000 | Loss: 0.00001109
Iteration 124/1000 | Loss: 0.00001109
Iteration 125/1000 | Loss: 0.00001109
Iteration 126/1000 | Loss: 0.00001109
Iteration 127/1000 | Loss: 0.00001109
Iteration 128/1000 | Loss: 0.00001109
Iteration 129/1000 | Loss: 0.00001109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.1086961421824526e-05, 1.1086961421824526e-05, 1.1086961421824526e-05, 1.1086961421824526e-05, 1.1086961421824526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1086961421824526e-05

Optimization complete. Final v2v error: 2.794548988342285 mm

Highest mean error: 3.113823652267456 mm for frame 58

Lowest mean error: 2.5928168296813965 mm for frame 11

Saving results

Total time: 37.71005153656006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065545
Iteration 2/25 | Loss: 0.00199132
Iteration 3/25 | Loss: 0.00141999
Iteration 4/25 | Loss: 0.00075686
Iteration 5/25 | Loss: 0.00069504
Iteration 6/25 | Loss: 0.00068362
Iteration 7/25 | Loss: 0.00066966
Iteration 8/25 | Loss: 0.00066789
Iteration 9/25 | Loss: 0.00066737
Iteration 10/25 | Loss: 0.00066702
Iteration 11/25 | Loss: 0.00066670
Iteration 12/25 | Loss: 0.00066670
Iteration 13/25 | Loss: 0.00066670
Iteration 14/25 | Loss: 0.00066669
Iteration 15/25 | Loss: 0.00066669
Iteration 16/25 | Loss: 0.00066669
Iteration 17/25 | Loss: 0.00066669
Iteration 18/25 | Loss: 0.00066669
Iteration 19/25 | Loss: 0.00066669
Iteration 20/25 | Loss: 0.00066669
Iteration 21/25 | Loss: 0.00066669
Iteration 22/25 | Loss: 0.00066669
Iteration 23/25 | Loss: 0.00066669
Iteration 24/25 | Loss: 0.00066669
Iteration 25/25 | Loss: 0.00066669

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45878053
Iteration 2/25 | Loss: 0.00030229
Iteration 3/25 | Loss: 0.00030228
Iteration 4/25 | Loss: 0.00030228
Iteration 5/25 | Loss: 0.00030228
Iteration 6/25 | Loss: 0.00030228
Iteration 7/25 | Loss: 0.00030228
Iteration 8/25 | Loss: 0.00030228
Iteration 9/25 | Loss: 0.00030228
Iteration 10/25 | Loss: 0.00030228
Iteration 11/25 | Loss: 0.00030228
Iteration 12/25 | Loss: 0.00030228
Iteration 13/25 | Loss: 0.00030228
Iteration 14/25 | Loss: 0.00030228
Iteration 15/25 | Loss: 0.00030228
Iteration 16/25 | Loss: 0.00030228
Iteration 17/25 | Loss: 0.00030228
Iteration 18/25 | Loss: 0.00030228
Iteration 19/25 | Loss: 0.00030228
Iteration 20/25 | Loss: 0.00030228
Iteration 21/25 | Loss: 0.00030228
Iteration 22/25 | Loss: 0.00030228
Iteration 23/25 | Loss: 0.00030228
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0003022811724804342, 0.0003022811724804342, 0.0003022811724804342, 0.0003022811724804342, 0.0003022811724804342]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003022811724804342

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030228
Iteration 2/1000 | Loss: 0.00002981
Iteration 3/1000 | Loss: 0.00002135
Iteration 4/1000 | Loss: 0.00001749
Iteration 5/1000 | Loss: 0.00001644
Iteration 6/1000 | Loss: 0.00001569
Iteration 7/1000 | Loss: 0.00001523
Iteration 8/1000 | Loss: 0.00001478
Iteration 9/1000 | Loss: 0.00001465
Iteration 10/1000 | Loss: 0.00001443
Iteration 11/1000 | Loss: 0.00001442
Iteration 12/1000 | Loss: 0.00001440
Iteration 13/1000 | Loss: 0.00001419
Iteration 14/1000 | Loss: 0.00001412
Iteration 15/1000 | Loss: 0.00001407
Iteration 16/1000 | Loss: 0.00001403
Iteration 17/1000 | Loss: 0.00001403
Iteration 18/1000 | Loss: 0.00001403
Iteration 19/1000 | Loss: 0.00001403
Iteration 20/1000 | Loss: 0.00001401
Iteration 21/1000 | Loss: 0.00001401
Iteration 22/1000 | Loss: 0.00001401
Iteration 23/1000 | Loss: 0.00001400
Iteration 24/1000 | Loss: 0.00001400
Iteration 25/1000 | Loss: 0.00001400
Iteration 26/1000 | Loss: 0.00001400
Iteration 27/1000 | Loss: 0.00001399
Iteration 28/1000 | Loss: 0.00001398
Iteration 29/1000 | Loss: 0.00001398
Iteration 30/1000 | Loss: 0.00001397
Iteration 31/1000 | Loss: 0.00001397
Iteration 32/1000 | Loss: 0.00001397
Iteration 33/1000 | Loss: 0.00001396
Iteration 34/1000 | Loss: 0.00001396
Iteration 35/1000 | Loss: 0.00001396
Iteration 36/1000 | Loss: 0.00001396
Iteration 37/1000 | Loss: 0.00001396
Iteration 38/1000 | Loss: 0.00001396
Iteration 39/1000 | Loss: 0.00001396
Iteration 40/1000 | Loss: 0.00001395
Iteration 41/1000 | Loss: 0.00001395
Iteration 42/1000 | Loss: 0.00001395
Iteration 43/1000 | Loss: 0.00001394
Iteration 44/1000 | Loss: 0.00001394
Iteration 45/1000 | Loss: 0.00001392
Iteration 46/1000 | Loss: 0.00001392
Iteration 47/1000 | Loss: 0.00001392
Iteration 48/1000 | Loss: 0.00001392
Iteration 49/1000 | Loss: 0.00001392
Iteration 50/1000 | Loss: 0.00001392
Iteration 51/1000 | Loss: 0.00001392
Iteration 52/1000 | Loss: 0.00001392
Iteration 53/1000 | Loss: 0.00001392
Iteration 54/1000 | Loss: 0.00001392
Iteration 55/1000 | Loss: 0.00001392
Iteration 56/1000 | Loss: 0.00001392
Iteration 57/1000 | Loss: 0.00001391
Iteration 58/1000 | Loss: 0.00001391
Iteration 59/1000 | Loss: 0.00001391
Iteration 60/1000 | Loss: 0.00001391
Iteration 61/1000 | Loss: 0.00001391
Iteration 62/1000 | Loss: 0.00001390
Iteration 63/1000 | Loss: 0.00001389
Iteration 64/1000 | Loss: 0.00001389
Iteration 65/1000 | Loss: 0.00001389
Iteration 66/1000 | Loss: 0.00001389
Iteration 67/1000 | Loss: 0.00001389
Iteration 68/1000 | Loss: 0.00001389
Iteration 69/1000 | Loss: 0.00001389
Iteration 70/1000 | Loss: 0.00001388
Iteration 71/1000 | Loss: 0.00001388
Iteration 72/1000 | Loss: 0.00001387
Iteration 73/1000 | Loss: 0.00001387
Iteration 74/1000 | Loss: 0.00001387
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001387
Iteration 77/1000 | Loss: 0.00001386
Iteration 78/1000 | Loss: 0.00001386
Iteration 79/1000 | Loss: 0.00001385
Iteration 80/1000 | Loss: 0.00001385
Iteration 81/1000 | Loss: 0.00001384
Iteration 82/1000 | Loss: 0.00001384
Iteration 83/1000 | Loss: 0.00001384
Iteration 84/1000 | Loss: 0.00001384
Iteration 85/1000 | Loss: 0.00001383
Iteration 86/1000 | Loss: 0.00001383
Iteration 87/1000 | Loss: 0.00001383
Iteration 88/1000 | Loss: 0.00001382
Iteration 89/1000 | Loss: 0.00001382
Iteration 90/1000 | Loss: 0.00001382
Iteration 91/1000 | Loss: 0.00001382
Iteration 92/1000 | Loss: 0.00001382
Iteration 93/1000 | Loss: 0.00001382
Iteration 94/1000 | Loss: 0.00001381
Iteration 95/1000 | Loss: 0.00001381
Iteration 96/1000 | Loss: 0.00001381
Iteration 97/1000 | Loss: 0.00001381
Iteration 98/1000 | Loss: 0.00001381
Iteration 99/1000 | Loss: 0.00001381
Iteration 100/1000 | Loss: 0.00001381
Iteration 101/1000 | Loss: 0.00001381
Iteration 102/1000 | Loss: 0.00001381
Iteration 103/1000 | Loss: 0.00001381
Iteration 104/1000 | Loss: 0.00001381
Iteration 105/1000 | Loss: 0.00001381
Iteration 106/1000 | Loss: 0.00001381
Iteration 107/1000 | Loss: 0.00001381
Iteration 108/1000 | Loss: 0.00001381
Iteration 109/1000 | Loss: 0.00001381
Iteration 110/1000 | Loss: 0.00001381
Iteration 111/1000 | Loss: 0.00001380
Iteration 112/1000 | Loss: 0.00001380
Iteration 113/1000 | Loss: 0.00001380
Iteration 114/1000 | Loss: 0.00001379
Iteration 115/1000 | Loss: 0.00001379
Iteration 116/1000 | Loss: 0.00001379
Iteration 117/1000 | Loss: 0.00001379
Iteration 118/1000 | Loss: 0.00001379
Iteration 119/1000 | Loss: 0.00001378
Iteration 120/1000 | Loss: 0.00001378
Iteration 121/1000 | Loss: 0.00001378
Iteration 122/1000 | Loss: 0.00001378
Iteration 123/1000 | Loss: 0.00001378
Iteration 124/1000 | Loss: 0.00001378
Iteration 125/1000 | Loss: 0.00001378
Iteration 126/1000 | Loss: 0.00001378
Iteration 127/1000 | Loss: 0.00001378
Iteration 128/1000 | Loss: 0.00001378
Iteration 129/1000 | Loss: 0.00001378
Iteration 130/1000 | Loss: 0.00001378
Iteration 131/1000 | Loss: 0.00001377
Iteration 132/1000 | Loss: 0.00001377
Iteration 133/1000 | Loss: 0.00001377
Iteration 134/1000 | Loss: 0.00001377
Iteration 135/1000 | Loss: 0.00001377
Iteration 136/1000 | Loss: 0.00001377
Iteration 137/1000 | Loss: 0.00001377
Iteration 138/1000 | Loss: 0.00001377
Iteration 139/1000 | Loss: 0.00001377
Iteration 140/1000 | Loss: 0.00001377
Iteration 141/1000 | Loss: 0.00001377
Iteration 142/1000 | Loss: 0.00001377
Iteration 143/1000 | Loss: 0.00001377
Iteration 144/1000 | Loss: 0.00001377
Iteration 145/1000 | Loss: 0.00001376
Iteration 146/1000 | Loss: 0.00001376
Iteration 147/1000 | Loss: 0.00001376
Iteration 148/1000 | Loss: 0.00001376
Iteration 149/1000 | Loss: 0.00001376
Iteration 150/1000 | Loss: 0.00001376
Iteration 151/1000 | Loss: 0.00001376
Iteration 152/1000 | Loss: 0.00001376
Iteration 153/1000 | Loss: 0.00001376
Iteration 154/1000 | Loss: 0.00001376
Iteration 155/1000 | Loss: 0.00001376
Iteration 156/1000 | Loss: 0.00001376
Iteration 157/1000 | Loss: 0.00001376
Iteration 158/1000 | Loss: 0.00001376
Iteration 159/1000 | Loss: 0.00001376
Iteration 160/1000 | Loss: 0.00001376
Iteration 161/1000 | Loss: 0.00001376
Iteration 162/1000 | Loss: 0.00001376
Iteration 163/1000 | Loss: 0.00001376
Iteration 164/1000 | Loss: 0.00001376
Iteration 165/1000 | Loss: 0.00001376
Iteration 166/1000 | Loss: 0.00001376
Iteration 167/1000 | Loss: 0.00001376
Iteration 168/1000 | Loss: 0.00001376
Iteration 169/1000 | Loss: 0.00001376
Iteration 170/1000 | Loss: 0.00001376
Iteration 171/1000 | Loss: 0.00001376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.3760203728452325e-05, 1.3760203728452325e-05, 1.3760203728452325e-05, 1.3760203728452325e-05, 1.3760203728452325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3760203728452325e-05

Optimization complete. Final v2v error: 3.175373077392578 mm

Highest mean error: 3.4215407371520996 mm for frame 108

Lowest mean error: 2.7725205421447754 mm for frame 57

Saving results

Total time: 46.04123663902283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00940877
Iteration 2/25 | Loss: 0.00117814
Iteration 3/25 | Loss: 0.00083952
Iteration 4/25 | Loss: 0.00078870
Iteration 5/25 | Loss: 0.00077824
Iteration 6/25 | Loss: 0.00077612
Iteration 7/25 | Loss: 0.00077606
Iteration 8/25 | Loss: 0.00077606
Iteration 9/25 | Loss: 0.00077606
Iteration 10/25 | Loss: 0.00077606
Iteration 11/25 | Loss: 0.00077606
Iteration 12/25 | Loss: 0.00077606
Iteration 13/25 | Loss: 0.00077606
Iteration 14/25 | Loss: 0.00077606
Iteration 15/25 | Loss: 0.00077606
Iteration 16/25 | Loss: 0.00077606
Iteration 17/25 | Loss: 0.00077606
Iteration 18/25 | Loss: 0.00077606
Iteration 19/25 | Loss: 0.00077606
Iteration 20/25 | Loss: 0.00077606
Iteration 21/25 | Loss: 0.00077606
Iteration 22/25 | Loss: 0.00077606
Iteration 23/25 | Loss: 0.00077606
Iteration 24/25 | Loss: 0.00077606
Iteration 25/25 | Loss: 0.00077606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33202446
Iteration 2/25 | Loss: 0.00018653
Iteration 3/25 | Loss: 0.00018644
Iteration 4/25 | Loss: 0.00018644
Iteration 5/25 | Loss: 0.00018644
Iteration 6/25 | Loss: 0.00018644
Iteration 7/25 | Loss: 0.00018644
Iteration 8/25 | Loss: 0.00018644
Iteration 9/25 | Loss: 0.00018644
Iteration 10/25 | Loss: 0.00018644
Iteration 11/25 | Loss: 0.00018644
Iteration 12/25 | Loss: 0.00018644
Iteration 13/25 | Loss: 0.00018644
Iteration 14/25 | Loss: 0.00018644
Iteration 15/25 | Loss: 0.00018644
Iteration 16/25 | Loss: 0.00018644
Iteration 17/25 | Loss: 0.00018644
Iteration 18/25 | Loss: 0.00018644
Iteration 19/25 | Loss: 0.00018644
Iteration 20/25 | Loss: 0.00018644
Iteration 21/25 | Loss: 0.00018644
Iteration 22/25 | Loss: 0.00018644
Iteration 23/25 | Loss: 0.00018644
Iteration 24/25 | Loss: 0.00018644
Iteration 25/25 | Loss: 0.00018644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018644
Iteration 2/1000 | Loss: 0.00003071
Iteration 3/1000 | Loss: 0.00002422
Iteration 4/1000 | Loss: 0.00002248
Iteration 5/1000 | Loss: 0.00002139
Iteration 6/1000 | Loss: 0.00002064
Iteration 7/1000 | Loss: 0.00002012
Iteration 8/1000 | Loss: 0.00001969
Iteration 9/1000 | Loss: 0.00001937
Iteration 10/1000 | Loss: 0.00001924
Iteration 11/1000 | Loss: 0.00001912
Iteration 12/1000 | Loss: 0.00001908
Iteration 13/1000 | Loss: 0.00001905
Iteration 14/1000 | Loss: 0.00001902
Iteration 15/1000 | Loss: 0.00001901
Iteration 16/1000 | Loss: 0.00001901
Iteration 17/1000 | Loss: 0.00001901
Iteration 18/1000 | Loss: 0.00001901
Iteration 19/1000 | Loss: 0.00001901
Iteration 20/1000 | Loss: 0.00001901
Iteration 21/1000 | Loss: 0.00001900
Iteration 22/1000 | Loss: 0.00001895
Iteration 23/1000 | Loss: 0.00001893
Iteration 24/1000 | Loss: 0.00001891
Iteration 25/1000 | Loss: 0.00001885
Iteration 26/1000 | Loss: 0.00001884
Iteration 27/1000 | Loss: 0.00001884
Iteration 28/1000 | Loss: 0.00001882
Iteration 29/1000 | Loss: 0.00001882
Iteration 30/1000 | Loss: 0.00001882
Iteration 31/1000 | Loss: 0.00001882
Iteration 32/1000 | Loss: 0.00001882
Iteration 33/1000 | Loss: 0.00001882
Iteration 34/1000 | Loss: 0.00001882
Iteration 35/1000 | Loss: 0.00001882
Iteration 36/1000 | Loss: 0.00001882
Iteration 37/1000 | Loss: 0.00001882
Iteration 38/1000 | Loss: 0.00001882
Iteration 39/1000 | Loss: 0.00001882
Iteration 40/1000 | Loss: 0.00001881
Iteration 41/1000 | Loss: 0.00001881
Iteration 42/1000 | Loss: 0.00001881
Iteration 43/1000 | Loss: 0.00001881
Iteration 44/1000 | Loss: 0.00001881
Iteration 45/1000 | Loss: 0.00001881
Iteration 46/1000 | Loss: 0.00001881
Iteration 47/1000 | Loss: 0.00001881
Iteration 48/1000 | Loss: 0.00001881
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001881
Iteration 52/1000 | Loss: 0.00001881
Iteration 53/1000 | Loss: 0.00001881
Iteration 54/1000 | Loss: 0.00001881
Iteration 55/1000 | Loss: 0.00001881
Iteration 56/1000 | Loss: 0.00001881
Iteration 57/1000 | Loss: 0.00001880
Iteration 58/1000 | Loss: 0.00001880
Iteration 59/1000 | Loss: 0.00001880
Iteration 60/1000 | Loss: 0.00001880
Iteration 61/1000 | Loss: 0.00001880
Iteration 62/1000 | Loss: 0.00001880
Iteration 63/1000 | Loss: 0.00001880
Iteration 64/1000 | Loss: 0.00001880
Iteration 65/1000 | Loss: 0.00001880
Iteration 66/1000 | Loss: 0.00001880
Iteration 67/1000 | Loss: 0.00001880
Iteration 68/1000 | Loss: 0.00001880
Iteration 69/1000 | Loss: 0.00001879
Iteration 70/1000 | Loss: 0.00001879
Iteration 71/1000 | Loss: 0.00001879
Iteration 72/1000 | Loss: 0.00001879
Iteration 73/1000 | Loss: 0.00001879
Iteration 74/1000 | Loss: 0.00001879
Iteration 75/1000 | Loss: 0.00001879
Iteration 76/1000 | Loss: 0.00001879
Iteration 77/1000 | Loss: 0.00001879
Iteration 78/1000 | Loss: 0.00001879
Iteration 79/1000 | Loss: 0.00001879
Iteration 80/1000 | Loss: 0.00001879
Iteration 81/1000 | Loss: 0.00001879
Iteration 82/1000 | Loss: 0.00001879
Iteration 83/1000 | Loss: 0.00001879
Iteration 84/1000 | Loss: 0.00001879
Iteration 85/1000 | Loss: 0.00001879
Iteration 86/1000 | Loss: 0.00001879
Iteration 87/1000 | Loss: 0.00001879
Iteration 88/1000 | Loss: 0.00001879
Iteration 89/1000 | Loss: 0.00001879
Iteration 90/1000 | Loss: 0.00001879
Iteration 91/1000 | Loss: 0.00001879
Iteration 92/1000 | Loss: 0.00001879
Iteration 93/1000 | Loss: 0.00001879
Iteration 94/1000 | Loss: 0.00001879
Iteration 95/1000 | Loss: 0.00001879
Iteration 96/1000 | Loss: 0.00001879
Iteration 97/1000 | Loss: 0.00001879
Iteration 98/1000 | Loss: 0.00001879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.8787794033414684e-05, 1.8787794033414684e-05, 1.8787794033414684e-05, 1.8787794033414684e-05, 1.8787794033414684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8787794033414684e-05

Optimization complete. Final v2v error: 3.642526865005493 mm

Highest mean error: 3.8213136196136475 mm for frame 2

Lowest mean error: 3.4597296714782715 mm for frame 239

Saving results

Total time: 37.16427683830261
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803530
Iteration 2/25 | Loss: 0.00125726
Iteration 3/25 | Loss: 0.00096755
Iteration 4/25 | Loss: 0.00088552
Iteration 5/25 | Loss: 0.00083024
Iteration 6/25 | Loss: 0.00080442
Iteration 7/25 | Loss: 0.00079886
Iteration 8/25 | Loss: 0.00079493
Iteration 9/25 | Loss: 0.00079139
Iteration 10/25 | Loss: 0.00079083
Iteration 11/25 | Loss: 0.00079060
Iteration 12/25 | Loss: 0.00078963
Iteration 13/25 | Loss: 0.00078897
Iteration 14/25 | Loss: 0.00078864
Iteration 15/25 | Loss: 0.00078830
Iteration 16/25 | Loss: 0.00078724
Iteration 17/25 | Loss: 0.00079026
Iteration 18/25 | Loss: 0.00079047
Iteration 19/25 | Loss: 0.00079007
Iteration 20/25 | Loss: 0.00078885
Iteration 21/25 | Loss: 0.00078665
Iteration 22/25 | Loss: 0.00078624
Iteration 23/25 | Loss: 0.00078624
Iteration 24/25 | Loss: 0.00078621
Iteration 25/25 | Loss: 0.00078621

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38491249
Iteration 2/25 | Loss: 0.00081038
Iteration 3/25 | Loss: 0.00081032
Iteration 4/25 | Loss: 0.00081032
Iteration 5/25 | Loss: 0.00081032
Iteration 6/25 | Loss: 0.00081032
Iteration 7/25 | Loss: 0.00081032
Iteration 8/25 | Loss: 0.00081032
Iteration 9/25 | Loss: 0.00081032
Iteration 10/25 | Loss: 0.00081032
Iteration 11/25 | Loss: 0.00081032
Iteration 12/25 | Loss: 0.00081032
Iteration 13/25 | Loss: 0.00081032
Iteration 14/25 | Loss: 0.00081032
Iteration 15/25 | Loss: 0.00081032
Iteration 16/25 | Loss: 0.00081032
Iteration 17/25 | Loss: 0.00081032
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008103150757960975, 0.0008103150757960975, 0.0008103150757960975, 0.0008103150757960975, 0.0008103150757960975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008103150757960975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081031
Iteration 2/1000 | Loss: 0.00045204
Iteration 3/1000 | Loss: 0.00040192
Iteration 4/1000 | Loss: 0.00029683
Iteration 5/1000 | Loss: 0.00094384
Iteration 6/1000 | Loss: 0.00071327
Iteration 7/1000 | Loss: 0.00047068
Iteration 8/1000 | Loss: 0.00032697
Iteration 9/1000 | Loss: 0.00055096
Iteration 10/1000 | Loss: 0.00059938
Iteration 11/1000 | Loss: 0.00013704
Iteration 12/1000 | Loss: 0.00072156
Iteration 13/1000 | Loss: 0.00033502
Iteration 14/1000 | Loss: 0.00041067
Iteration 15/1000 | Loss: 0.00024484
Iteration 16/1000 | Loss: 0.00081972
Iteration 17/1000 | Loss: 0.00062667
Iteration 18/1000 | Loss: 0.00067144
Iteration 19/1000 | Loss: 0.00131886
Iteration 20/1000 | Loss: 0.00039117
Iteration 21/1000 | Loss: 0.00035235
Iteration 22/1000 | Loss: 0.00018819
Iteration 23/1000 | Loss: 0.00057143
Iteration 24/1000 | Loss: 0.00062994
Iteration 25/1000 | Loss: 0.00034942
Iteration 26/1000 | Loss: 0.00040299
Iteration 27/1000 | Loss: 0.00066567
Iteration 28/1000 | Loss: 0.00009990
Iteration 29/1000 | Loss: 0.00007251
Iteration 30/1000 | Loss: 0.00041011
Iteration 31/1000 | Loss: 0.00069038
Iteration 32/1000 | Loss: 0.00103786
Iteration 33/1000 | Loss: 0.00026797
Iteration 34/1000 | Loss: 0.00007461
Iteration 35/1000 | Loss: 0.00026285
Iteration 36/1000 | Loss: 0.00019111
Iteration 37/1000 | Loss: 0.00041794
Iteration 38/1000 | Loss: 0.00044050
Iteration 39/1000 | Loss: 0.00198262
Iteration 40/1000 | Loss: 0.00096871
Iteration 41/1000 | Loss: 0.00064414
Iteration 42/1000 | Loss: 0.00038700
Iteration 43/1000 | Loss: 0.00078225
Iteration 44/1000 | Loss: 0.00119469
Iteration 45/1000 | Loss: 0.00064145
Iteration 46/1000 | Loss: 0.00062790
Iteration 47/1000 | Loss: 0.00085989
Iteration 48/1000 | Loss: 0.00054887
Iteration 49/1000 | Loss: 0.00063090
Iteration 50/1000 | Loss: 0.00017201
Iteration 51/1000 | Loss: 0.00034769
Iteration 52/1000 | Loss: 0.00020890
Iteration 53/1000 | Loss: 0.00021903
Iteration 54/1000 | Loss: 0.00006720
Iteration 55/1000 | Loss: 0.00023381
Iteration 56/1000 | Loss: 0.00017234
Iteration 57/1000 | Loss: 0.00019479
Iteration 58/1000 | Loss: 0.00012298
Iteration 59/1000 | Loss: 0.00019125
Iteration 60/1000 | Loss: 0.00026663
Iteration 61/1000 | Loss: 0.00025595
Iteration 62/1000 | Loss: 0.00041276
Iteration 63/1000 | Loss: 0.00025557
Iteration 64/1000 | Loss: 0.00023917
Iteration 65/1000 | Loss: 0.00061976
Iteration 66/1000 | Loss: 0.00071902
Iteration 67/1000 | Loss: 0.00008220
Iteration 68/1000 | Loss: 0.00012528
Iteration 69/1000 | Loss: 0.00043718
Iteration 70/1000 | Loss: 0.00047378
Iteration 71/1000 | Loss: 0.00040563
Iteration 72/1000 | Loss: 0.00059711
Iteration 73/1000 | Loss: 0.00081427
Iteration 74/1000 | Loss: 0.00275504
Iteration 75/1000 | Loss: 0.00060267
Iteration 76/1000 | Loss: 0.00029111
Iteration 77/1000 | Loss: 0.00031183
Iteration 78/1000 | Loss: 0.00023675
Iteration 79/1000 | Loss: 0.00011551
Iteration 80/1000 | Loss: 0.00020951
Iteration 81/1000 | Loss: 0.00005826
Iteration 82/1000 | Loss: 0.00012767
Iteration 83/1000 | Loss: 0.00017090
Iteration 84/1000 | Loss: 0.00061069
Iteration 85/1000 | Loss: 0.00053821
Iteration 86/1000 | Loss: 0.00080881
Iteration 87/1000 | Loss: 0.00049826
Iteration 88/1000 | Loss: 0.00082270
Iteration 89/1000 | Loss: 0.00029241
Iteration 90/1000 | Loss: 0.00004347
Iteration 91/1000 | Loss: 0.00003900
Iteration 92/1000 | Loss: 0.00014986
Iteration 93/1000 | Loss: 0.00027092
Iteration 94/1000 | Loss: 0.00025416
Iteration 95/1000 | Loss: 0.00028740
Iteration 96/1000 | Loss: 0.00030159
Iteration 97/1000 | Loss: 0.00009099
Iteration 98/1000 | Loss: 0.00017797
Iteration 99/1000 | Loss: 0.00016669
Iteration 100/1000 | Loss: 0.00007590
Iteration 101/1000 | Loss: 0.00008736
Iteration 102/1000 | Loss: 0.00003984
Iteration 103/1000 | Loss: 0.00003703
Iteration 104/1000 | Loss: 0.00003516
Iteration 105/1000 | Loss: 0.00003343
Iteration 106/1000 | Loss: 0.00003267
Iteration 107/1000 | Loss: 0.00003189
Iteration 108/1000 | Loss: 0.00032116
Iteration 109/1000 | Loss: 0.00045288
Iteration 110/1000 | Loss: 0.00013349
Iteration 111/1000 | Loss: 0.00023864
Iteration 112/1000 | Loss: 0.00064080
Iteration 113/1000 | Loss: 0.00008204
Iteration 114/1000 | Loss: 0.00004106
Iteration 115/1000 | Loss: 0.00003951
Iteration 116/1000 | Loss: 0.00004132
Iteration 117/1000 | Loss: 0.00004080
Iteration 118/1000 | Loss: 0.00005472
Iteration 119/1000 | Loss: 0.00003847
Iteration 120/1000 | Loss: 0.00004766
Iteration 121/1000 | Loss: 0.00003878
Iteration 122/1000 | Loss: 0.00014770
Iteration 123/1000 | Loss: 0.00017687
Iteration 124/1000 | Loss: 0.00003092
Iteration 125/1000 | Loss: 0.00007910
Iteration 126/1000 | Loss: 0.00039504
Iteration 127/1000 | Loss: 0.00034071
Iteration 128/1000 | Loss: 0.00041519
Iteration 129/1000 | Loss: 0.00052366
Iteration 130/1000 | Loss: 0.00053749
Iteration 131/1000 | Loss: 0.00017031
Iteration 132/1000 | Loss: 0.00004726
Iteration 133/1000 | Loss: 0.00003581
Iteration 134/1000 | Loss: 0.00005069
Iteration 135/1000 | Loss: 0.00003295
Iteration 136/1000 | Loss: 0.00011681
Iteration 137/1000 | Loss: 0.00004856
Iteration 138/1000 | Loss: 0.00014919
Iteration 139/1000 | Loss: 0.00014940
Iteration 140/1000 | Loss: 0.00027017
Iteration 141/1000 | Loss: 0.00020179
Iteration 142/1000 | Loss: 0.00009136
Iteration 143/1000 | Loss: 0.00011868
Iteration 144/1000 | Loss: 0.00004485
Iteration 145/1000 | Loss: 0.00010656
Iteration 146/1000 | Loss: 0.00004714
Iteration 147/1000 | Loss: 0.00009415
Iteration 148/1000 | Loss: 0.00034686
Iteration 149/1000 | Loss: 0.00036904
Iteration 150/1000 | Loss: 0.00043528
Iteration 151/1000 | Loss: 0.00037151
Iteration 152/1000 | Loss: 0.00034214
Iteration 153/1000 | Loss: 0.00053993
Iteration 154/1000 | Loss: 0.00049536
Iteration 155/1000 | Loss: 0.00005908
Iteration 156/1000 | Loss: 0.00028887
Iteration 157/1000 | Loss: 0.00015371
Iteration 158/1000 | Loss: 0.00016136
Iteration 159/1000 | Loss: 0.00012995
Iteration 160/1000 | Loss: 0.00036719
Iteration 161/1000 | Loss: 0.00029871
Iteration 162/1000 | Loss: 0.00011749
Iteration 163/1000 | Loss: 0.00036796
Iteration 164/1000 | Loss: 0.00017825
Iteration 165/1000 | Loss: 0.00031247
Iteration 166/1000 | Loss: 0.00020246
Iteration 167/1000 | Loss: 0.00025476
Iteration 168/1000 | Loss: 0.00010684
Iteration 169/1000 | Loss: 0.00038050
Iteration 170/1000 | Loss: 0.00008622
Iteration 171/1000 | Loss: 0.00026482
Iteration 172/1000 | Loss: 0.00034516
Iteration 173/1000 | Loss: 0.00015461
Iteration 174/1000 | Loss: 0.00013899
Iteration 175/1000 | Loss: 0.00024111
Iteration 176/1000 | Loss: 0.00013943
Iteration 177/1000 | Loss: 0.00020715
Iteration 178/1000 | Loss: 0.00038719
Iteration 179/1000 | Loss: 0.00121376
Iteration 180/1000 | Loss: 0.00308753
Iteration 181/1000 | Loss: 0.00004924
Iteration 182/1000 | Loss: 0.00021556
Iteration 183/1000 | Loss: 0.00026810
Iteration 184/1000 | Loss: 0.00008768
Iteration 185/1000 | Loss: 0.00007894
Iteration 186/1000 | Loss: 0.00007276
Iteration 187/1000 | Loss: 0.00034930
Iteration 188/1000 | Loss: 0.00026443
Iteration 189/1000 | Loss: 0.00088956
Iteration 190/1000 | Loss: 0.00103838
Iteration 191/1000 | Loss: 0.00004838
Iteration 192/1000 | Loss: 0.00004021
Iteration 193/1000 | Loss: 0.00040645
Iteration 194/1000 | Loss: 0.00022478
Iteration 195/1000 | Loss: 0.00004061
Iteration 196/1000 | Loss: 0.00034538
Iteration 197/1000 | Loss: 0.00027448
Iteration 198/1000 | Loss: 0.00026952
Iteration 199/1000 | Loss: 0.00053683
Iteration 200/1000 | Loss: 0.00007973
Iteration 201/1000 | Loss: 0.00005025
Iteration 202/1000 | Loss: 0.00003552
Iteration 203/1000 | Loss: 0.00003446
Iteration 204/1000 | Loss: 0.00003390
Iteration 205/1000 | Loss: 0.00003351
Iteration 206/1000 | Loss: 0.00003346
Iteration 207/1000 | Loss: 0.00007484
Iteration 208/1000 | Loss: 0.00004933
Iteration 209/1000 | Loss: 0.00008445
Iteration 210/1000 | Loss: 0.00004216
Iteration 211/1000 | Loss: 0.00003766
Iteration 212/1000 | Loss: 0.00008211
Iteration 213/1000 | Loss: 0.00011191
Iteration 214/1000 | Loss: 0.00006104
Iteration 215/1000 | Loss: 0.00019258
Iteration 216/1000 | Loss: 0.00016775
Iteration 217/1000 | Loss: 0.00008596
Iteration 218/1000 | Loss: 0.00013319
Iteration 219/1000 | Loss: 0.00024600
Iteration 220/1000 | Loss: 0.00032415
Iteration 221/1000 | Loss: 0.00041551
Iteration 222/1000 | Loss: 0.00008965
Iteration 223/1000 | Loss: 0.00022733
Iteration 224/1000 | Loss: 0.00009694
Iteration 225/1000 | Loss: 0.00017722
Iteration 226/1000 | Loss: 0.00004708
Iteration 227/1000 | Loss: 0.00026854
Iteration 228/1000 | Loss: 0.00031092
Iteration 229/1000 | Loss: 0.00030507
Iteration 230/1000 | Loss: 0.00007129
Iteration 231/1000 | Loss: 0.00004239
Iteration 232/1000 | Loss: 0.00003851
Iteration 233/1000 | Loss: 0.00003637
Iteration 234/1000 | Loss: 0.00019982
Iteration 235/1000 | Loss: 0.00008687
Iteration 236/1000 | Loss: 0.00017248
Iteration 237/1000 | Loss: 0.00013416
Iteration 238/1000 | Loss: 0.00017970
Iteration 239/1000 | Loss: 0.00003792
Iteration 240/1000 | Loss: 0.00011954
Iteration 241/1000 | Loss: 0.00016646
Iteration 242/1000 | Loss: 0.00014892
Iteration 243/1000 | Loss: 0.00013832
Iteration 244/1000 | Loss: 0.00010705
Iteration 245/1000 | Loss: 0.00011945
Iteration 246/1000 | Loss: 0.00003249
Iteration 247/1000 | Loss: 0.00003270
Iteration 248/1000 | Loss: 0.00003113
Iteration 249/1000 | Loss: 0.00013365
Iteration 250/1000 | Loss: 0.00012019
Iteration 251/1000 | Loss: 0.00012680
Iteration 252/1000 | Loss: 0.00014498
Iteration 253/1000 | Loss: 0.00016578
Iteration 254/1000 | Loss: 0.00014744
Iteration 255/1000 | Loss: 0.00009923
Iteration 256/1000 | Loss: 0.00013892
Iteration 257/1000 | Loss: 0.00003195
Iteration 258/1000 | Loss: 0.00016281
Iteration 259/1000 | Loss: 0.00015593
Iteration 260/1000 | Loss: 0.00021894
Iteration 261/1000 | Loss: 0.00010314
Iteration 262/1000 | Loss: 0.00016116
Iteration 263/1000 | Loss: 0.00013983
Iteration 264/1000 | Loss: 0.00010167
Iteration 265/1000 | Loss: 0.00002985
Iteration 266/1000 | Loss: 0.00019936
Iteration 267/1000 | Loss: 0.00018920
Iteration 268/1000 | Loss: 0.00020956
Iteration 269/1000 | Loss: 0.00017508
Iteration 270/1000 | Loss: 0.00002967
Iteration 271/1000 | Loss: 0.00002833
Iteration 272/1000 | Loss: 0.00002733
Iteration 273/1000 | Loss: 0.00002690
Iteration 274/1000 | Loss: 0.00002688
Iteration 275/1000 | Loss: 0.00002669
Iteration 276/1000 | Loss: 0.00002661
Iteration 277/1000 | Loss: 0.00002650
Iteration 278/1000 | Loss: 0.00002640
Iteration 279/1000 | Loss: 0.00002633
Iteration 280/1000 | Loss: 0.00002625
Iteration 281/1000 | Loss: 0.00002625
Iteration 282/1000 | Loss: 0.00002623
Iteration 283/1000 | Loss: 0.00002619
Iteration 284/1000 | Loss: 0.00002617
Iteration 285/1000 | Loss: 0.00002616
Iteration 286/1000 | Loss: 0.00002616
Iteration 287/1000 | Loss: 0.00002613
Iteration 288/1000 | Loss: 0.00002612
Iteration 289/1000 | Loss: 0.00002612
Iteration 290/1000 | Loss: 0.00002611
Iteration 291/1000 | Loss: 0.00002610
Iteration 292/1000 | Loss: 0.00002610
Iteration 293/1000 | Loss: 0.00002609
Iteration 294/1000 | Loss: 0.00002609
Iteration 295/1000 | Loss: 0.00002607
Iteration 296/1000 | Loss: 0.00002606
Iteration 297/1000 | Loss: 0.00002606
Iteration 298/1000 | Loss: 0.00002605
Iteration 299/1000 | Loss: 0.00002605
Iteration 300/1000 | Loss: 0.00002605
Iteration 301/1000 | Loss: 0.00002605
Iteration 302/1000 | Loss: 0.00002604
Iteration 303/1000 | Loss: 0.00002604
Iteration 304/1000 | Loss: 0.00002604
Iteration 305/1000 | Loss: 0.00002604
Iteration 306/1000 | Loss: 0.00002604
Iteration 307/1000 | Loss: 0.00002603
Iteration 308/1000 | Loss: 0.00002603
Iteration 309/1000 | Loss: 0.00002603
Iteration 310/1000 | Loss: 0.00002603
Iteration 311/1000 | Loss: 0.00002603
Iteration 312/1000 | Loss: 0.00002602
Iteration 313/1000 | Loss: 0.00002602
Iteration 314/1000 | Loss: 0.00002602
Iteration 315/1000 | Loss: 0.00002602
Iteration 316/1000 | Loss: 0.00002602
Iteration 317/1000 | Loss: 0.00002602
Iteration 318/1000 | Loss: 0.00002602
Iteration 319/1000 | Loss: 0.00002602
Iteration 320/1000 | Loss: 0.00002602
Iteration 321/1000 | Loss: 0.00002602
Iteration 322/1000 | Loss: 0.00002602
Iteration 323/1000 | Loss: 0.00002601
Iteration 324/1000 | Loss: 0.00002601
Iteration 325/1000 | Loss: 0.00002601
Iteration 326/1000 | Loss: 0.00002601
Iteration 327/1000 | Loss: 0.00002600
Iteration 328/1000 | Loss: 0.00002600
Iteration 329/1000 | Loss: 0.00002600
Iteration 330/1000 | Loss: 0.00002600
Iteration 331/1000 | Loss: 0.00002600
Iteration 332/1000 | Loss: 0.00002600
Iteration 333/1000 | Loss: 0.00002600
Iteration 334/1000 | Loss: 0.00002600
Iteration 335/1000 | Loss: 0.00002600
Iteration 336/1000 | Loss: 0.00002600
Iteration 337/1000 | Loss: 0.00002599
Iteration 338/1000 | Loss: 0.00002599
Iteration 339/1000 | Loss: 0.00002599
Iteration 340/1000 | Loss: 0.00002599
Iteration 341/1000 | Loss: 0.00002599
Iteration 342/1000 | Loss: 0.00002599
Iteration 343/1000 | Loss: 0.00002599
Iteration 344/1000 | Loss: 0.00002599
Iteration 345/1000 | Loss: 0.00002599
Iteration 346/1000 | Loss: 0.00002599
Iteration 347/1000 | Loss: 0.00002599
Iteration 348/1000 | Loss: 0.00002599
Iteration 349/1000 | Loss: 0.00002599
Iteration 350/1000 | Loss: 0.00002599
Iteration 351/1000 | Loss: 0.00002599
Iteration 352/1000 | Loss: 0.00002599
Iteration 353/1000 | Loss: 0.00002599
Iteration 354/1000 | Loss: 0.00002599
Iteration 355/1000 | Loss: 0.00002598
Iteration 356/1000 | Loss: 0.00002598
Iteration 357/1000 | Loss: 0.00002598
Iteration 358/1000 | Loss: 0.00002598
Iteration 359/1000 | Loss: 0.00002598
Iteration 360/1000 | Loss: 0.00002598
Iteration 361/1000 | Loss: 0.00002598
Iteration 362/1000 | Loss: 0.00002598
Iteration 363/1000 | Loss: 0.00002598
Iteration 364/1000 | Loss: 0.00002598
Iteration 365/1000 | Loss: 0.00002597
Iteration 366/1000 | Loss: 0.00002597
Iteration 367/1000 | Loss: 0.00002597
Iteration 368/1000 | Loss: 0.00002597
Iteration 369/1000 | Loss: 0.00002597
Iteration 370/1000 | Loss: 0.00002597
Iteration 371/1000 | Loss: 0.00002596
Iteration 372/1000 | Loss: 0.00002596
Iteration 373/1000 | Loss: 0.00002596
Iteration 374/1000 | Loss: 0.00002596
Iteration 375/1000 | Loss: 0.00002596
Iteration 376/1000 | Loss: 0.00002596
Iteration 377/1000 | Loss: 0.00002596
Iteration 378/1000 | Loss: 0.00002596
Iteration 379/1000 | Loss: 0.00002596
Iteration 380/1000 | Loss: 0.00002596
Iteration 381/1000 | Loss: 0.00002596
Iteration 382/1000 | Loss: 0.00002596
Iteration 383/1000 | Loss: 0.00002596
Iteration 384/1000 | Loss: 0.00002596
Iteration 385/1000 | Loss: 0.00002596
Iteration 386/1000 | Loss: 0.00002596
Iteration 387/1000 | Loss: 0.00002596
Iteration 388/1000 | Loss: 0.00002596
Iteration 389/1000 | Loss: 0.00002596
Iteration 390/1000 | Loss: 0.00002596
Iteration 391/1000 | Loss: 0.00002596
Iteration 392/1000 | Loss: 0.00002596
Iteration 393/1000 | Loss: 0.00002596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 393. Stopping optimization.
Last 5 losses: [2.5962413928937167e-05, 2.5962413928937167e-05, 2.5962413928937167e-05, 2.5962413928937167e-05, 2.5962413928937167e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5962413928937167e-05

Optimization complete. Final v2v error: 4.222586631774902 mm

Highest mean error: 5.441465854644775 mm for frame 184

Lowest mean error: 3.907045364379883 mm for frame 97

Saving results

Total time: 497.88118529319763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057290
Iteration 2/25 | Loss: 0.00273520
Iteration 3/25 | Loss: 0.00191922
Iteration 4/25 | Loss: 0.00142018
Iteration 5/25 | Loss: 0.00099374
Iteration 6/25 | Loss: 0.00087499
Iteration 7/25 | Loss: 0.00088927
Iteration 8/25 | Loss: 0.00085653
Iteration 9/25 | Loss: 0.00079918
Iteration 10/25 | Loss: 0.00078730
Iteration 11/25 | Loss: 0.00078272
Iteration 12/25 | Loss: 0.00075223
Iteration 13/25 | Loss: 0.00071660
Iteration 14/25 | Loss: 0.00072837
Iteration 15/25 | Loss: 0.00071416
Iteration 16/25 | Loss: 0.00071403
Iteration 17/25 | Loss: 0.00071403
Iteration 18/25 | Loss: 0.00071403
Iteration 19/25 | Loss: 0.00071403
Iteration 20/25 | Loss: 0.00071403
Iteration 21/25 | Loss: 0.00071403
Iteration 22/25 | Loss: 0.00071403
Iteration 23/25 | Loss: 0.00071403
Iteration 24/25 | Loss: 0.00071403
Iteration 25/25 | Loss: 0.00071403

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42427206
Iteration 2/25 | Loss: 0.00031732
Iteration 3/25 | Loss: 0.00031732
Iteration 4/25 | Loss: 0.00031732
Iteration 5/25 | Loss: 0.00031732
Iteration 6/25 | Loss: 0.00031732
Iteration 7/25 | Loss: 0.00031732
Iteration 8/25 | Loss: 0.00031732
Iteration 9/25 | Loss: 0.00031732
Iteration 10/25 | Loss: 0.00031732
Iteration 11/25 | Loss: 0.00031732
Iteration 12/25 | Loss: 0.00031732
Iteration 13/25 | Loss: 0.00031732
Iteration 14/25 | Loss: 0.00031732
Iteration 15/25 | Loss: 0.00031732
Iteration 16/25 | Loss: 0.00031732
Iteration 17/25 | Loss: 0.00031732
Iteration 18/25 | Loss: 0.00031732
Iteration 19/25 | Loss: 0.00031732
Iteration 20/25 | Loss: 0.00031732
Iteration 21/25 | Loss: 0.00031732
Iteration 22/25 | Loss: 0.00031732
Iteration 23/25 | Loss: 0.00031732
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0003173200530000031, 0.0003173200530000031, 0.0003173200530000031, 0.0003173200530000031, 0.0003173200530000031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003173200530000031

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031732
Iteration 2/1000 | Loss: 0.00002954
Iteration 3/1000 | Loss: 0.00002180
Iteration 4/1000 | Loss: 0.00001968
Iteration 5/1000 | Loss: 0.00001894
Iteration 6/1000 | Loss: 0.00001842
Iteration 7/1000 | Loss: 0.00001787
Iteration 8/1000 | Loss: 0.00001751
Iteration 9/1000 | Loss: 0.00001730
Iteration 10/1000 | Loss: 0.00001724
Iteration 11/1000 | Loss: 0.00001722
Iteration 12/1000 | Loss: 0.00001713
Iteration 13/1000 | Loss: 0.00001712
Iteration 14/1000 | Loss: 0.00001710
Iteration 15/1000 | Loss: 0.00001708
Iteration 16/1000 | Loss: 0.00001707
Iteration 17/1000 | Loss: 0.00001698
Iteration 18/1000 | Loss: 0.00001697
Iteration 19/1000 | Loss: 0.00001697
Iteration 20/1000 | Loss: 0.00001697
Iteration 21/1000 | Loss: 0.00001697
Iteration 22/1000 | Loss: 0.00001697
Iteration 23/1000 | Loss: 0.00001697
Iteration 24/1000 | Loss: 0.00001697
Iteration 25/1000 | Loss: 0.00001697
Iteration 26/1000 | Loss: 0.00001696
Iteration 27/1000 | Loss: 0.00001696
Iteration 28/1000 | Loss: 0.00001695
Iteration 29/1000 | Loss: 0.00001695
Iteration 30/1000 | Loss: 0.00001695
Iteration 31/1000 | Loss: 0.00001695
Iteration 32/1000 | Loss: 0.00001695
Iteration 33/1000 | Loss: 0.00001694
Iteration 34/1000 | Loss: 0.00001694
Iteration 35/1000 | Loss: 0.00001694
Iteration 36/1000 | Loss: 0.00001693
Iteration 37/1000 | Loss: 0.00001693
Iteration 38/1000 | Loss: 0.00001693
Iteration 39/1000 | Loss: 0.00001693
Iteration 40/1000 | Loss: 0.00001693
Iteration 41/1000 | Loss: 0.00001693
Iteration 42/1000 | Loss: 0.00001693
Iteration 43/1000 | Loss: 0.00001692
Iteration 44/1000 | Loss: 0.00001692
Iteration 45/1000 | Loss: 0.00001691
Iteration 46/1000 | Loss: 0.00001691
Iteration 47/1000 | Loss: 0.00001691
Iteration 48/1000 | Loss: 0.00001691
Iteration 49/1000 | Loss: 0.00001691
Iteration 50/1000 | Loss: 0.00001690
Iteration 51/1000 | Loss: 0.00001690
Iteration 52/1000 | Loss: 0.00001690
Iteration 53/1000 | Loss: 0.00001690
Iteration 54/1000 | Loss: 0.00001690
Iteration 55/1000 | Loss: 0.00001690
Iteration 56/1000 | Loss: 0.00001690
Iteration 57/1000 | Loss: 0.00001689
Iteration 58/1000 | Loss: 0.00001689
Iteration 59/1000 | Loss: 0.00001689
Iteration 60/1000 | Loss: 0.00001689
Iteration 61/1000 | Loss: 0.00001689
Iteration 62/1000 | Loss: 0.00001689
Iteration 63/1000 | Loss: 0.00001689
Iteration 64/1000 | Loss: 0.00001689
Iteration 65/1000 | Loss: 0.00001689
Iteration 66/1000 | Loss: 0.00001689
Iteration 67/1000 | Loss: 0.00001689
Iteration 68/1000 | Loss: 0.00001689
Iteration 69/1000 | Loss: 0.00001688
Iteration 70/1000 | Loss: 0.00001688
Iteration 71/1000 | Loss: 0.00001688
Iteration 72/1000 | Loss: 0.00001688
Iteration 73/1000 | Loss: 0.00001688
Iteration 74/1000 | Loss: 0.00001688
Iteration 75/1000 | Loss: 0.00001687
Iteration 76/1000 | Loss: 0.00001687
Iteration 77/1000 | Loss: 0.00001687
Iteration 78/1000 | Loss: 0.00001687
Iteration 79/1000 | Loss: 0.00001687
Iteration 80/1000 | Loss: 0.00001687
Iteration 81/1000 | Loss: 0.00001687
Iteration 82/1000 | Loss: 0.00001687
Iteration 83/1000 | Loss: 0.00001687
Iteration 84/1000 | Loss: 0.00001687
Iteration 85/1000 | Loss: 0.00001686
Iteration 86/1000 | Loss: 0.00001686
Iteration 87/1000 | Loss: 0.00001686
Iteration 88/1000 | Loss: 0.00001686
Iteration 89/1000 | Loss: 0.00001686
Iteration 90/1000 | Loss: 0.00001686
Iteration 91/1000 | Loss: 0.00001686
Iteration 92/1000 | Loss: 0.00001686
Iteration 93/1000 | Loss: 0.00001686
Iteration 94/1000 | Loss: 0.00001686
Iteration 95/1000 | Loss: 0.00001686
Iteration 96/1000 | Loss: 0.00001686
Iteration 97/1000 | Loss: 0.00001686
Iteration 98/1000 | Loss: 0.00001686
Iteration 99/1000 | Loss: 0.00001686
Iteration 100/1000 | Loss: 0.00001686
Iteration 101/1000 | Loss: 0.00001686
Iteration 102/1000 | Loss: 0.00001686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.6858713934198022e-05, 1.6858713934198022e-05, 1.6858713934198022e-05, 1.6858713934198022e-05, 1.6858713934198022e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6858713934198022e-05

Optimization complete. Final v2v error: 3.3724353313446045 mm

Highest mean error: 3.5318703651428223 mm for frame 4

Lowest mean error: 3.30047869682312 mm for frame 33

Saving results

Total time: 48.37470364570618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843572
Iteration 2/25 | Loss: 0.00109709
Iteration 3/25 | Loss: 0.00079161
Iteration 4/25 | Loss: 0.00073731
Iteration 5/25 | Loss: 0.00073318
Iteration 6/25 | Loss: 0.00072404
Iteration 7/25 | Loss: 0.00070768
Iteration 8/25 | Loss: 0.00070191
Iteration 9/25 | Loss: 0.00070014
Iteration 10/25 | Loss: 0.00069964
Iteration 11/25 | Loss: 0.00069942
Iteration 12/25 | Loss: 0.00069940
Iteration 13/25 | Loss: 0.00069940
Iteration 14/25 | Loss: 0.00069940
Iteration 15/25 | Loss: 0.00069940
Iteration 16/25 | Loss: 0.00069940
Iteration 17/25 | Loss: 0.00069939
Iteration 18/25 | Loss: 0.00069939
Iteration 19/25 | Loss: 0.00069939
Iteration 20/25 | Loss: 0.00069939
Iteration 21/25 | Loss: 0.00069939
Iteration 22/25 | Loss: 0.00069939
Iteration 23/25 | Loss: 0.00069939
Iteration 24/25 | Loss: 0.00069939
Iteration 25/25 | Loss: 0.00069939

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.41705227
Iteration 2/25 | Loss: 0.00037590
Iteration 3/25 | Loss: 0.00037590
Iteration 4/25 | Loss: 0.00037590
Iteration 5/25 | Loss: 0.00037590
Iteration 6/25 | Loss: 0.00037590
Iteration 7/25 | Loss: 0.00037590
Iteration 8/25 | Loss: 0.00037590
Iteration 9/25 | Loss: 0.00037590
Iteration 10/25 | Loss: 0.00037589
Iteration 11/25 | Loss: 0.00037589
Iteration 12/25 | Loss: 0.00037589
Iteration 13/25 | Loss: 0.00037589
Iteration 14/25 | Loss: 0.00037589
Iteration 15/25 | Loss: 0.00037589
Iteration 16/25 | Loss: 0.00037589
Iteration 17/25 | Loss: 0.00037589
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003758949169423431, 0.0003758949169423431, 0.0003758949169423431, 0.0003758949169423431, 0.0003758949169423431]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003758949169423431

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037589
Iteration 2/1000 | Loss: 0.00003623
Iteration 3/1000 | Loss: 0.00002626
Iteration 4/1000 | Loss: 0.00002290
Iteration 5/1000 | Loss: 0.00002163
Iteration 6/1000 | Loss: 0.00002063
Iteration 7/1000 | Loss: 0.00002015
Iteration 8/1000 | Loss: 0.00001977
Iteration 9/1000 | Loss: 0.00001939
Iteration 10/1000 | Loss: 0.00001915
Iteration 11/1000 | Loss: 0.00001895
Iteration 12/1000 | Loss: 0.00001883
Iteration 13/1000 | Loss: 0.00001879
Iteration 14/1000 | Loss: 0.00001878
Iteration 15/1000 | Loss: 0.00001873
Iteration 16/1000 | Loss: 0.00001868
Iteration 17/1000 | Loss: 0.00001865
Iteration 18/1000 | Loss: 0.00001864
Iteration 19/1000 | Loss: 0.00001863
Iteration 20/1000 | Loss: 0.00001863
Iteration 21/1000 | Loss: 0.00001862
Iteration 22/1000 | Loss: 0.00001862
Iteration 23/1000 | Loss: 0.00001861
Iteration 24/1000 | Loss: 0.00001861
Iteration 25/1000 | Loss: 0.00001860
Iteration 26/1000 | Loss: 0.00001860
Iteration 27/1000 | Loss: 0.00001859
Iteration 28/1000 | Loss: 0.00001858
Iteration 29/1000 | Loss: 0.00001858
Iteration 30/1000 | Loss: 0.00001857
Iteration 31/1000 | Loss: 0.00001856
Iteration 32/1000 | Loss: 0.00001855
Iteration 33/1000 | Loss: 0.00001855
Iteration 34/1000 | Loss: 0.00001855
Iteration 35/1000 | Loss: 0.00001854
Iteration 36/1000 | Loss: 0.00001853
Iteration 37/1000 | Loss: 0.00001853
Iteration 38/1000 | Loss: 0.00001852
Iteration 39/1000 | Loss: 0.00001852
Iteration 40/1000 | Loss: 0.00001851
Iteration 41/1000 | Loss: 0.00001851
Iteration 42/1000 | Loss: 0.00001851
Iteration 43/1000 | Loss: 0.00001850
Iteration 44/1000 | Loss: 0.00001850
Iteration 45/1000 | Loss: 0.00001849
Iteration 46/1000 | Loss: 0.00001849
Iteration 47/1000 | Loss: 0.00001849
Iteration 48/1000 | Loss: 0.00001848
Iteration 49/1000 | Loss: 0.00001848
Iteration 50/1000 | Loss: 0.00001848
Iteration 51/1000 | Loss: 0.00001847
Iteration 52/1000 | Loss: 0.00001847
Iteration 53/1000 | Loss: 0.00001847
Iteration 54/1000 | Loss: 0.00001846
Iteration 55/1000 | Loss: 0.00001846
Iteration 56/1000 | Loss: 0.00001846
Iteration 57/1000 | Loss: 0.00001845
Iteration 58/1000 | Loss: 0.00001845
Iteration 59/1000 | Loss: 0.00001845
Iteration 60/1000 | Loss: 0.00001844
Iteration 61/1000 | Loss: 0.00001844
Iteration 62/1000 | Loss: 0.00001844
Iteration 63/1000 | Loss: 0.00001844
Iteration 64/1000 | Loss: 0.00001844
Iteration 65/1000 | Loss: 0.00001844
Iteration 66/1000 | Loss: 0.00001844
Iteration 67/1000 | Loss: 0.00001844
Iteration 68/1000 | Loss: 0.00001844
Iteration 69/1000 | Loss: 0.00001844
Iteration 70/1000 | Loss: 0.00001844
Iteration 71/1000 | Loss: 0.00001844
Iteration 72/1000 | Loss: 0.00001844
Iteration 73/1000 | Loss: 0.00001844
Iteration 74/1000 | Loss: 0.00001844
Iteration 75/1000 | Loss: 0.00001844
Iteration 76/1000 | Loss: 0.00001844
Iteration 77/1000 | Loss: 0.00001844
Iteration 78/1000 | Loss: 0.00001844
Iteration 79/1000 | Loss: 0.00001844
Iteration 80/1000 | Loss: 0.00001844
Iteration 81/1000 | Loss: 0.00001844
Iteration 82/1000 | Loss: 0.00001844
Iteration 83/1000 | Loss: 0.00001844
Iteration 84/1000 | Loss: 0.00001844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.8437052858644165e-05, 1.8437052858644165e-05, 1.8437052858644165e-05, 1.8437052858644165e-05, 1.8437052858644165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8437052858644165e-05

Optimization complete. Final v2v error: 3.5875494480133057 mm

Highest mean error: 4.6254072189331055 mm for frame 159

Lowest mean error: 2.911381959915161 mm for frame 99

Saving results

Total time: 49.720250606536865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00699445
Iteration 2/25 | Loss: 0.00144940
Iteration 3/25 | Loss: 0.00095941
Iteration 4/25 | Loss: 0.00080096
Iteration 5/25 | Loss: 0.00075059
Iteration 6/25 | Loss: 0.00074441
Iteration 7/25 | Loss: 0.00072510
Iteration 8/25 | Loss: 0.00073507
Iteration 9/25 | Loss: 0.00073551
Iteration 10/25 | Loss: 0.00074158
Iteration 11/25 | Loss: 0.00072559
Iteration 12/25 | Loss: 0.00072041
Iteration 13/25 | Loss: 0.00071853
Iteration 14/25 | Loss: 0.00071989
Iteration 15/25 | Loss: 0.00071734
Iteration 16/25 | Loss: 0.00071630
Iteration 17/25 | Loss: 0.00071498
Iteration 18/25 | Loss: 0.00071727
Iteration 19/25 | Loss: 0.00071470
Iteration 20/25 | Loss: 0.00071474
Iteration 21/25 | Loss: 0.00071377
Iteration 22/25 | Loss: 0.00071558
Iteration 23/25 | Loss: 0.00071345
Iteration 24/25 | Loss: 0.00071458
Iteration 25/25 | Loss: 0.00071381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.34162664
Iteration 2/25 | Loss: 0.00039540
Iteration 3/25 | Loss: 0.00039535
Iteration 4/25 | Loss: 0.00039535
Iteration 5/25 | Loss: 0.00039535
Iteration 6/25 | Loss: 0.00039535
Iteration 7/25 | Loss: 0.00039535
Iteration 8/25 | Loss: 0.00039535
Iteration 9/25 | Loss: 0.00039535
Iteration 10/25 | Loss: 0.00039534
Iteration 11/25 | Loss: 0.00039534
Iteration 12/25 | Loss: 0.00039534
Iteration 13/25 | Loss: 0.00039534
Iteration 14/25 | Loss: 0.00039534
Iteration 15/25 | Loss: 0.00039534
Iteration 16/25 | Loss: 0.00039534
Iteration 17/25 | Loss: 0.00039534
Iteration 18/25 | Loss: 0.00039534
Iteration 19/25 | Loss: 0.00039534
Iteration 20/25 | Loss: 0.00039534
Iteration 21/25 | Loss: 0.00039534
Iteration 22/25 | Loss: 0.00039534
Iteration 23/25 | Loss: 0.00039534
Iteration 24/25 | Loss: 0.00039534
Iteration 25/25 | Loss: 0.00039534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039534
Iteration 2/1000 | Loss: 0.00003732
Iteration 3/1000 | Loss: 0.00002820
Iteration 4/1000 | Loss: 0.00015967
Iteration 5/1000 | Loss: 0.00013203
Iteration 6/1000 | Loss: 0.00007148
Iteration 7/1000 | Loss: 0.00002681
Iteration 8/1000 | Loss: 0.00002525
Iteration 9/1000 | Loss: 0.00002383
Iteration 10/1000 | Loss: 0.00002309
Iteration 11/1000 | Loss: 0.00002257
Iteration 12/1000 | Loss: 0.00014022
Iteration 13/1000 | Loss: 0.00002698
Iteration 14/1000 | Loss: 0.00002168
Iteration 15/1000 | Loss: 0.00001965
Iteration 16/1000 | Loss: 0.00001860
Iteration 17/1000 | Loss: 0.00001765
Iteration 18/1000 | Loss: 0.00001714
Iteration 19/1000 | Loss: 0.00001676
Iteration 20/1000 | Loss: 0.00001646
Iteration 21/1000 | Loss: 0.00001638
Iteration 22/1000 | Loss: 0.00001618
Iteration 23/1000 | Loss: 0.00001600
Iteration 24/1000 | Loss: 0.00001589
Iteration 25/1000 | Loss: 0.00001586
Iteration 26/1000 | Loss: 0.00001585
Iteration 27/1000 | Loss: 0.00001583
Iteration 28/1000 | Loss: 0.00001583
Iteration 29/1000 | Loss: 0.00001582
Iteration 30/1000 | Loss: 0.00001581
Iteration 31/1000 | Loss: 0.00001581
Iteration 32/1000 | Loss: 0.00001581
Iteration 33/1000 | Loss: 0.00001581
Iteration 34/1000 | Loss: 0.00001580
Iteration 35/1000 | Loss: 0.00001580
Iteration 36/1000 | Loss: 0.00001580
Iteration 37/1000 | Loss: 0.00001580
Iteration 38/1000 | Loss: 0.00001580
Iteration 39/1000 | Loss: 0.00001580
Iteration 40/1000 | Loss: 0.00001580
Iteration 41/1000 | Loss: 0.00001580
Iteration 42/1000 | Loss: 0.00001580
Iteration 43/1000 | Loss: 0.00001580
Iteration 44/1000 | Loss: 0.00001579
Iteration 45/1000 | Loss: 0.00001578
Iteration 46/1000 | Loss: 0.00001578
Iteration 47/1000 | Loss: 0.00001577
Iteration 48/1000 | Loss: 0.00001577
Iteration 49/1000 | Loss: 0.00001577
Iteration 50/1000 | Loss: 0.00001576
Iteration 51/1000 | Loss: 0.00001576
Iteration 52/1000 | Loss: 0.00001576
Iteration 53/1000 | Loss: 0.00001575
Iteration 54/1000 | Loss: 0.00001575
Iteration 55/1000 | Loss: 0.00001575
Iteration 56/1000 | Loss: 0.00001575
Iteration 57/1000 | Loss: 0.00001575
Iteration 58/1000 | Loss: 0.00001574
Iteration 59/1000 | Loss: 0.00001574
Iteration 60/1000 | Loss: 0.00001574
Iteration 61/1000 | Loss: 0.00001574
Iteration 62/1000 | Loss: 0.00001574
Iteration 63/1000 | Loss: 0.00001574
Iteration 64/1000 | Loss: 0.00001573
Iteration 65/1000 | Loss: 0.00001573
Iteration 66/1000 | Loss: 0.00001573
Iteration 67/1000 | Loss: 0.00001572
Iteration 68/1000 | Loss: 0.00001572
Iteration 69/1000 | Loss: 0.00001572
Iteration 70/1000 | Loss: 0.00001572
Iteration 71/1000 | Loss: 0.00001572
Iteration 72/1000 | Loss: 0.00001572
Iteration 73/1000 | Loss: 0.00001572
Iteration 74/1000 | Loss: 0.00001572
Iteration 75/1000 | Loss: 0.00001572
Iteration 76/1000 | Loss: 0.00001572
Iteration 77/1000 | Loss: 0.00001572
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001571
Iteration 80/1000 | Loss: 0.00001571
Iteration 81/1000 | Loss: 0.00001571
Iteration 82/1000 | Loss: 0.00001571
Iteration 83/1000 | Loss: 0.00001571
Iteration 84/1000 | Loss: 0.00001571
Iteration 85/1000 | Loss: 0.00001571
Iteration 86/1000 | Loss: 0.00001571
Iteration 87/1000 | Loss: 0.00001570
Iteration 88/1000 | Loss: 0.00001570
Iteration 89/1000 | Loss: 0.00001570
Iteration 90/1000 | Loss: 0.00001570
Iteration 91/1000 | Loss: 0.00001570
Iteration 92/1000 | Loss: 0.00001569
Iteration 93/1000 | Loss: 0.00001569
Iteration 94/1000 | Loss: 0.00001569
Iteration 95/1000 | Loss: 0.00001569
Iteration 96/1000 | Loss: 0.00001569
Iteration 97/1000 | Loss: 0.00001569
Iteration 98/1000 | Loss: 0.00001569
Iteration 99/1000 | Loss: 0.00001569
Iteration 100/1000 | Loss: 0.00001569
Iteration 101/1000 | Loss: 0.00001569
Iteration 102/1000 | Loss: 0.00001569
Iteration 103/1000 | Loss: 0.00001568
Iteration 104/1000 | Loss: 0.00001568
Iteration 105/1000 | Loss: 0.00001568
Iteration 106/1000 | Loss: 0.00001568
Iteration 107/1000 | Loss: 0.00001568
Iteration 108/1000 | Loss: 0.00001568
Iteration 109/1000 | Loss: 0.00001568
Iteration 110/1000 | Loss: 0.00001568
Iteration 111/1000 | Loss: 0.00001568
Iteration 112/1000 | Loss: 0.00001568
Iteration 113/1000 | Loss: 0.00001568
Iteration 114/1000 | Loss: 0.00001568
Iteration 115/1000 | Loss: 0.00001568
Iteration 116/1000 | Loss: 0.00001568
Iteration 117/1000 | Loss: 0.00001567
Iteration 118/1000 | Loss: 0.00001567
Iteration 119/1000 | Loss: 0.00001567
Iteration 120/1000 | Loss: 0.00001567
Iteration 121/1000 | Loss: 0.00001567
Iteration 122/1000 | Loss: 0.00001567
Iteration 123/1000 | Loss: 0.00001567
Iteration 124/1000 | Loss: 0.00001567
Iteration 125/1000 | Loss: 0.00001567
Iteration 126/1000 | Loss: 0.00001567
Iteration 127/1000 | Loss: 0.00001567
Iteration 128/1000 | Loss: 0.00001567
Iteration 129/1000 | Loss: 0.00001567
Iteration 130/1000 | Loss: 0.00001567
Iteration 131/1000 | Loss: 0.00001567
Iteration 132/1000 | Loss: 0.00001567
Iteration 133/1000 | Loss: 0.00001567
Iteration 134/1000 | Loss: 0.00001566
Iteration 135/1000 | Loss: 0.00001566
Iteration 136/1000 | Loss: 0.00001566
Iteration 137/1000 | Loss: 0.00001566
Iteration 138/1000 | Loss: 0.00001566
Iteration 139/1000 | Loss: 0.00001566
Iteration 140/1000 | Loss: 0.00001566
Iteration 141/1000 | Loss: 0.00001566
Iteration 142/1000 | Loss: 0.00001566
Iteration 143/1000 | Loss: 0.00001566
Iteration 144/1000 | Loss: 0.00001565
Iteration 145/1000 | Loss: 0.00001565
Iteration 146/1000 | Loss: 0.00001565
Iteration 147/1000 | Loss: 0.00001565
Iteration 148/1000 | Loss: 0.00001565
Iteration 149/1000 | Loss: 0.00001565
Iteration 150/1000 | Loss: 0.00001565
Iteration 151/1000 | Loss: 0.00001565
Iteration 152/1000 | Loss: 0.00001565
Iteration 153/1000 | Loss: 0.00001565
Iteration 154/1000 | Loss: 0.00001565
Iteration 155/1000 | Loss: 0.00001565
Iteration 156/1000 | Loss: 0.00001565
Iteration 157/1000 | Loss: 0.00001565
Iteration 158/1000 | Loss: 0.00001565
Iteration 159/1000 | Loss: 0.00001565
Iteration 160/1000 | Loss: 0.00001564
Iteration 161/1000 | Loss: 0.00001564
Iteration 162/1000 | Loss: 0.00001564
Iteration 163/1000 | Loss: 0.00001564
Iteration 164/1000 | Loss: 0.00001564
Iteration 165/1000 | Loss: 0.00001564
Iteration 166/1000 | Loss: 0.00001564
Iteration 167/1000 | Loss: 0.00001564
Iteration 168/1000 | Loss: 0.00001564
Iteration 169/1000 | Loss: 0.00001564
Iteration 170/1000 | Loss: 0.00001564
Iteration 171/1000 | Loss: 0.00001564
Iteration 172/1000 | Loss: 0.00001564
Iteration 173/1000 | Loss: 0.00001564
Iteration 174/1000 | Loss: 0.00001564
Iteration 175/1000 | Loss: 0.00001564
Iteration 176/1000 | Loss: 0.00001564
Iteration 177/1000 | Loss: 0.00001564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.5641793652321212e-05, 1.5641793652321212e-05, 1.5641793652321212e-05, 1.5641793652321212e-05, 1.5641793652321212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5641793652321212e-05

Optimization complete. Final v2v error: 3.383911609649658 mm

Highest mean error: 3.886906147003174 mm for frame 36

Lowest mean error: 2.854722023010254 mm for frame 132

Saving results

Total time: 89.60325407981873
