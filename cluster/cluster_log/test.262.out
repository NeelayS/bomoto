Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=262, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14672-14727
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00700352
Iteration 2/25 | Loss: 0.00186690
Iteration 3/25 | Loss: 0.00133681
Iteration 4/25 | Loss: 0.00096997
Iteration 5/25 | Loss: 0.00089717
Iteration 6/25 | Loss: 0.00086619
Iteration 7/25 | Loss: 0.00086323
Iteration 8/25 | Loss: 0.00086106
Iteration 9/25 | Loss: 0.00085885
Iteration 10/25 | Loss: 0.00085464
Iteration 11/25 | Loss: 0.00087021
Iteration 12/25 | Loss: 0.00084654
Iteration 13/25 | Loss: 0.00084490
Iteration 14/25 | Loss: 0.00084469
Iteration 15/25 | Loss: 0.00084467
Iteration 16/25 | Loss: 0.00084467
Iteration 17/25 | Loss: 0.00084467
Iteration 18/25 | Loss: 0.00084467
Iteration 19/25 | Loss: 0.00084467
Iteration 20/25 | Loss: 0.00084467
Iteration 21/25 | Loss: 0.00084467
Iteration 22/25 | Loss: 0.00084467
Iteration 23/25 | Loss: 0.00084467
Iteration 24/25 | Loss: 0.00084467
Iteration 25/25 | Loss: 0.00084467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38469338
Iteration 2/25 | Loss: 0.00143973
Iteration 3/25 | Loss: 0.00143973
Iteration 4/25 | Loss: 0.00143973
Iteration 5/25 | Loss: 0.00143972
Iteration 6/25 | Loss: 0.00143972
Iteration 7/25 | Loss: 0.00143474
Iteration 8/25 | Loss: 0.00143474
Iteration 9/25 | Loss: 0.00143474
Iteration 10/25 | Loss: 0.00143474
Iteration 11/25 | Loss: 0.00143474
Iteration 12/25 | Loss: 0.00143474
Iteration 13/25 | Loss: 0.00143474
Iteration 14/25 | Loss: 0.00143474
Iteration 15/25 | Loss: 0.00143474
Iteration 16/25 | Loss: 0.00143474
Iteration 17/25 | Loss: 0.00143474
Iteration 18/25 | Loss: 0.00143474
Iteration 19/25 | Loss: 0.00143474
Iteration 20/25 | Loss: 0.00143474
Iteration 21/25 | Loss: 0.00143474
Iteration 22/25 | Loss: 0.00143474
Iteration 23/25 | Loss: 0.00143474
Iteration 24/25 | Loss: 0.00143474
Iteration 25/25 | Loss: 0.00143474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143474
Iteration 2/1000 | Loss: 0.00005476
Iteration 3/1000 | Loss: 0.00003046
Iteration 4/1000 | Loss: 0.00002563
Iteration 5/1000 | Loss: 0.00002405
Iteration 6/1000 | Loss: 0.00002289
Iteration 7/1000 | Loss: 0.00002224
Iteration 8/1000 | Loss: 0.00002168
Iteration 9/1000 | Loss: 0.00002134
Iteration 10/1000 | Loss: 0.00002107
Iteration 11/1000 | Loss: 0.00002084
Iteration 12/1000 | Loss: 0.00002058
Iteration 13/1000 | Loss: 0.00002038
Iteration 14/1000 | Loss: 0.00002027
Iteration 15/1000 | Loss: 0.00002025
Iteration 16/1000 | Loss: 0.00002023
Iteration 17/1000 | Loss: 0.00002020
Iteration 18/1000 | Loss: 0.00002017
Iteration 19/1000 | Loss: 0.00002016
Iteration 20/1000 | Loss: 0.00002016
Iteration 21/1000 | Loss: 0.00002016
Iteration 22/1000 | Loss: 0.00002015
Iteration 23/1000 | Loss: 0.00002015
Iteration 24/1000 | Loss: 0.00002015
Iteration 25/1000 | Loss: 0.00002013
Iteration 26/1000 | Loss: 0.00002013
Iteration 27/1000 | Loss: 0.00002013
Iteration 28/1000 | Loss: 0.00002011
Iteration 29/1000 | Loss: 0.00002011
Iteration 30/1000 | Loss: 0.00002010
Iteration 31/1000 | Loss: 0.00002009
Iteration 32/1000 | Loss: 0.00002008
Iteration 33/1000 | Loss: 0.00002008
Iteration 34/1000 | Loss: 0.00002007
Iteration 35/1000 | Loss: 0.00002007
Iteration 36/1000 | Loss: 0.00002007
Iteration 37/1000 | Loss: 0.00002006
Iteration 38/1000 | Loss: 0.00002006
Iteration 39/1000 | Loss: 0.00002006
Iteration 40/1000 | Loss: 0.00002005
Iteration 41/1000 | Loss: 0.00002005
Iteration 42/1000 | Loss: 0.00002005
Iteration 43/1000 | Loss: 0.00002004
Iteration 44/1000 | Loss: 0.00002004
Iteration 45/1000 | Loss: 0.00002003
Iteration 46/1000 | Loss: 0.00002003
Iteration 47/1000 | Loss: 0.00002003
Iteration 48/1000 | Loss: 0.00002002
Iteration 49/1000 | Loss: 0.00002002
Iteration 50/1000 | Loss: 0.00002001
Iteration 51/1000 | Loss: 0.00002001
Iteration 52/1000 | Loss: 0.00002001
Iteration 53/1000 | Loss: 0.00002000
Iteration 54/1000 | Loss: 0.00002000
Iteration 55/1000 | Loss: 0.00002000
Iteration 56/1000 | Loss: 0.00001999
Iteration 57/1000 | Loss: 0.00001999
Iteration 58/1000 | Loss: 0.00001999
Iteration 59/1000 | Loss: 0.00001998
Iteration 60/1000 | Loss: 0.00001998
Iteration 61/1000 | Loss: 0.00001998
Iteration 62/1000 | Loss: 0.00001997
Iteration 63/1000 | Loss: 0.00001997
Iteration 64/1000 | Loss: 0.00001997
Iteration 65/1000 | Loss: 0.00001996
Iteration 66/1000 | Loss: 0.00001996
Iteration 67/1000 | Loss: 0.00001996
Iteration 68/1000 | Loss: 0.00001996
Iteration 69/1000 | Loss: 0.00001996
Iteration 70/1000 | Loss: 0.00001995
Iteration 71/1000 | Loss: 0.00001995
Iteration 72/1000 | Loss: 0.00001995
Iteration 73/1000 | Loss: 0.00001994
Iteration 74/1000 | Loss: 0.00001994
Iteration 75/1000 | Loss: 0.00001994
Iteration 76/1000 | Loss: 0.00001993
Iteration 77/1000 | Loss: 0.00001993
Iteration 78/1000 | Loss: 0.00001993
Iteration 79/1000 | Loss: 0.00001993
Iteration 80/1000 | Loss: 0.00001992
Iteration 81/1000 | Loss: 0.00001992
Iteration 82/1000 | Loss: 0.00001992
Iteration 83/1000 | Loss: 0.00001992
Iteration 84/1000 | Loss: 0.00001992
Iteration 85/1000 | Loss: 0.00001991
Iteration 86/1000 | Loss: 0.00001991
Iteration 87/1000 | Loss: 0.00001991
Iteration 88/1000 | Loss: 0.00001991
Iteration 89/1000 | Loss: 0.00001991
Iteration 90/1000 | Loss: 0.00001991
Iteration 91/1000 | Loss: 0.00001991
Iteration 92/1000 | Loss: 0.00001990
Iteration 93/1000 | Loss: 0.00001990
Iteration 94/1000 | Loss: 0.00001990
Iteration 95/1000 | Loss: 0.00001990
Iteration 96/1000 | Loss: 0.00001989
Iteration 97/1000 | Loss: 0.00001989
Iteration 98/1000 | Loss: 0.00001989
Iteration 99/1000 | Loss: 0.00001989
Iteration 100/1000 | Loss: 0.00001988
Iteration 101/1000 | Loss: 0.00001988
Iteration 102/1000 | Loss: 0.00001988
Iteration 103/1000 | Loss: 0.00001987
Iteration 104/1000 | Loss: 0.00001987
Iteration 105/1000 | Loss: 0.00001987
Iteration 106/1000 | Loss: 0.00001987
Iteration 107/1000 | Loss: 0.00001987
Iteration 108/1000 | Loss: 0.00001987
Iteration 109/1000 | Loss: 0.00001987
Iteration 110/1000 | Loss: 0.00001987
Iteration 111/1000 | Loss: 0.00001987
Iteration 112/1000 | Loss: 0.00001987
Iteration 113/1000 | Loss: 0.00001986
Iteration 114/1000 | Loss: 0.00001986
Iteration 115/1000 | Loss: 0.00001986
Iteration 116/1000 | Loss: 0.00001985
Iteration 117/1000 | Loss: 0.00001985
Iteration 118/1000 | Loss: 0.00001985
Iteration 119/1000 | Loss: 0.00001985
Iteration 120/1000 | Loss: 0.00001985
Iteration 121/1000 | Loss: 0.00001985
Iteration 122/1000 | Loss: 0.00001985
Iteration 123/1000 | Loss: 0.00001984
Iteration 124/1000 | Loss: 0.00001984
Iteration 125/1000 | Loss: 0.00001984
Iteration 126/1000 | Loss: 0.00001984
Iteration 127/1000 | Loss: 0.00001984
Iteration 128/1000 | Loss: 0.00001983
Iteration 129/1000 | Loss: 0.00001983
Iteration 130/1000 | Loss: 0.00001983
Iteration 131/1000 | Loss: 0.00001983
Iteration 132/1000 | Loss: 0.00001983
Iteration 133/1000 | Loss: 0.00001983
Iteration 134/1000 | Loss: 0.00001983
Iteration 135/1000 | Loss: 0.00001983
Iteration 136/1000 | Loss: 0.00001982
Iteration 137/1000 | Loss: 0.00001982
Iteration 138/1000 | Loss: 0.00001982
Iteration 139/1000 | Loss: 0.00001982
Iteration 140/1000 | Loss: 0.00001982
Iteration 141/1000 | Loss: 0.00001981
Iteration 142/1000 | Loss: 0.00001981
Iteration 143/1000 | Loss: 0.00001981
Iteration 144/1000 | Loss: 0.00001981
Iteration 145/1000 | Loss: 0.00001981
Iteration 146/1000 | Loss: 0.00001981
Iteration 147/1000 | Loss: 0.00001981
Iteration 148/1000 | Loss: 0.00001980
Iteration 149/1000 | Loss: 0.00001980
Iteration 150/1000 | Loss: 0.00001980
Iteration 151/1000 | Loss: 0.00001980
Iteration 152/1000 | Loss: 0.00001980
Iteration 153/1000 | Loss: 0.00001980
Iteration 154/1000 | Loss: 0.00001980
Iteration 155/1000 | Loss: 0.00001980
Iteration 156/1000 | Loss: 0.00001980
Iteration 157/1000 | Loss: 0.00001980
Iteration 158/1000 | Loss: 0.00001980
Iteration 159/1000 | Loss: 0.00001980
Iteration 160/1000 | Loss: 0.00001979
Iteration 161/1000 | Loss: 0.00001979
Iteration 162/1000 | Loss: 0.00001979
Iteration 163/1000 | Loss: 0.00001979
Iteration 164/1000 | Loss: 0.00001979
Iteration 165/1000 | Loss: 0.00001979
Iteration 166/1000 | Loss: 0.00001979
Iteration 167/1000 | Loss: 0.00001979
Iteration 168/1000 | Loss: 0.00001979
Iteration 169/1000 | Loss: 0.00001979
Iteration 170/1000 | Loss: 0.00001978
Iteration 171/1000 | Loss: 0.00001978
Iteration 172/1000 | Loss: 0.00001978
Iteration 173/1000 | Loss: 0.00001978
Iteration 174/1000 | Loss: 0.00001978
Iteration 175/1000 | Loss: 0.00001978
Iteration 176/1000 | Loss: 0.00001978
Iteration 177/1000 | Loss: 0.00001978
Iteration 178/1000 | Loss: 0.00001978
Iteration 179/1000 | Loss: 0.00001978
Iteration 180/1000 | Loss: 0.00001978
Iteration 181/1000 | Loss: 0.00001978
Iteration 182/1000 | Loss: 0.00001978
Iteration 183/1000 | Loss: 0.00001978
Iteration 184/1000 | Loss: 0.00001978
Iteration 185/1000 | Loss: 0.00001978
Iteration 186/1000 | Loss: 0.00001978
Iteration 187/1000 | Loss: 0.00001978
Iteration 188/1000 | Loss: 0.00001978
Iteration 189/1000 | Loss: 0.00001978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.9778481146204285e-05, 1.9778481146204285e-05, 1.9778481146204285e-05, 1.9778481146204285e-05, 1.9778481146204285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9778481146204285e-05

Optimization complete. Final v2v error: 3.585080146789551 mm

Highest mean error: 5.851855278015137 mm for frame 12

Lowest mean error: 2.6566884517669678 mm for frame 178

Saving results

Total time: 120.44109201431274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834870
Iteration 2/25 | Loss: 0.00106489
Iteration 3/25 | Loss: 0.00084828
Iteration 4/25 | Loss: 0.00080119
Iteration 5/25 | Loss: 0.00081862
Iteration 6/25 | Loss: 0.00078220
Iteration 7/25 | Loss: 0.00077043
Iteration 8/25 | Loss: 0.00076785
Iteration 9/25 | Loss: 0.00076689
Iteration 10/25 | Loss: 0.00076644
Iteration 11/25 | Loss: 0.00076619
Iteration 12/25 | Loss: 0.00076604
Iteration 13/25 | Loss: 0.00076597
Iteration 14/25 | Loss: 0.00076597
Iteration 15/25 | Loss: 0.00076597
Iteration 16/25 | Loss: 0.00076597
Iteration 17/25 | Loss: 0.00076597
Iteration 18/25 | Loss: 0.00076597
Iteration 19/25 | Loss: 0.00076597
Iteration 20/25 | Loss: 0.00076597
Iteration 21/25 | Loss: 0.00076597
Iteration 22/25 | Loss: 0.00076597
Iteration 23/25 | Loss: 0.00076596
Iteration 24/25 | Loss: 0.00076596
Iteration 25/25 | Loss: 0.00076596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57131708
Iteration 2/25 | Loss: 0.00119418
Iteration 3/25 | Loss: 0.00119416
Iteration 4/25 | Loss: 0.00119416
Iteration 5/25 | Loss: 0.00119416
Iteration 6/25 | Loss: 0.00119416
Iteration 7/25 | Loss: 0.00119416
Iteration 8/25 | Loss: 0.00119416
Iteration 9/25 | Loss: 0.00119416
Iteration 10/25 | Loss: 0.00119416
Iteration 11/25 | Loss: 0.00119416
Iteration 12/25 | Loss: 0.00119416
Iteration 13/25 | Loss: 0.00119416
Iteration 14/25 | Loss: 0.00119416
Iteration 15/25 | Loss: 0.00119416
Iteration 16/25 | Loss: 0.00119416
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001194162294268608, 0.001194162294268608, 0.001194162294268608, 0.001194162294268608, 0.001194162294268608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001194162294268608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119416
Iteration 2/1000 | Loss: 0.00003475
Iteration 3/1000 | Loss: 0.00002217
Iteration 4/1000 | Loss: 0.00001867
Iteration 5/1000 | Loss: 0.00001747
Iteration 6/1000 | Loss: 0.00001643
Iteration 7/1000 | Loss: 0.00001596
Iteration 8/1000 | Loss: 0.00001563
Iteration 9/1000 | Loss: 0.00001540
Iteration 10/1000 | Loss: 0.00001517
Iteration 11/1000 | Loss: 0.00001510
Iteration 12/1000 | Loss: 0.00001508
Iteration 13/1000 | Loss: 0.00001502
Iteration 14/1000 | Loss: 0.00001483
Iteration 15/1000 | Loss: 0.00001469
Iteration 16/1000 | Loss: 0.00001463
Iteration 17/1000 | Loss: 0.00001457
Iteration 18/1000 | Loss: 0.00001455
Iteration 19/1000 | Loss: 0.00001454
Iteration 20/1000 | Loss: 0.00001448
Iteration 21/1000 | Loss: 0.00001447
Iteration 22/1000 | Loss: 0.00001444
Iteration 23/1000 | Loss: 0.00001443
Iteration 24/1000 | Loss: 0.00001442
Iteration 25/1000 | Loss: 0.00001441
Iteration 26/1000 | Loss: 0.00001440
Iteration 27/1000 | Loss: 0.00001439
Iteration 28/1000 | Loss: 0.00001438
Iteration 29/1000 | Loss: 0.00001438
Iteration 30/1000 | Loss: 0.00001437
Iteration 31/1000 | Loss: 0.00001437
Iteration 32/1000 | Loss: 0.00001437
Iteration 33/1000 | Loss: 0.00001436
Iteration 34/1000 | Loss: 0.00001431
Iteration 35/1000 | Loss: 0.00001431
Iteration 36/1000 | Loss: 0.00001431
Iteration 37/1000 | Loss: 0.00001430
Iteration 38/1000 | Loss: 0.00001429
Iteration 39/1000 | Loss: 0.00001429
Iteration 40/1000 | Loss: 0.00001428
Iteration 41/1000 | Loss: 0.00001428
Iteration 42/1000 | Loss: 0.00001428
Iteration 43/1000 | Loss: 0.00001427
Iteration 44/1000 | Loss: 0.00001427
Iteration 45/1000 | Loss: 0.00001426
Iteration 46/1000 | Loss: 0.00001426
Iteration 47/1000 | Loss: 0.00001426
Iteration 48/1000 | Loss: 0.00001425
Iteration 49/1000 | Loss: 0.00001425
Iteration 50/1000 | Loss: 0.00001425
Iteration 51/1000 | Loss: 0.00001424
Iteration 52/1000 | Loss: 0.00001424
Iteration 53/1000 | Loss: 0.00001423
Iteration 54/1000 | Loss: 0.00001423
Iteration 55/1000 | Loss: 0.00001423
Iteration 56/1000 | Loss: 0.00001422
Iteration 57/1000 | Loss: 0.00001422
Iteration 58/1000 | Loss: 0.00001422
Iteration 59/1000 | Loss: 0.00001421
Iteration 60/1000 | Loss: 0.00001421
Iteration 61/1000 | Loss: 0.00001421
Iteration 62/1000 | Loss: 0.00001420
Iteration 63/1000 | Loss: 0.00001420
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001419
Iteration 66/1000 | Loss: 0.00001419
Iteration 67/1000 | Loss: 0.00001418
Iteration 68/1000 | Loss: 0.00001418
Iteration 69/1000 | Loss: 0.00001417
Iteration 70/1000 | Loss: 0.00001417
Iteration 71/1000 | Loss: 0.00001417
Iteration 72/1000 | Loss: 0.00001417
Iteration 73/1000 | Loss: 0.00001417
Iteration 74/1000 | Loss: 0.00001417
Iteration 75/1000 | Loss: 0.00001417
Iteration 76/1000 | Loss: 0.00001417
Iteration 77/1000 | Loss: 0.00001416
Iteration 78/1000 | Loss: 0.00001416
Iteration 79/1000 | Loss: 0.00001416
Iteration 80/1000 | Loss: 0.00001415
Iteration 81/1000 | Loss: 0.00001414
Iteration 82/1000 | Loss: 0.00001414
Iteration 83/1000 | Loss: 0.00001413
Iteration 84/1000 | Loss: 0.00001413
Iteration 85/1000 | Loss: 0.00001413
Iteration 86/1000 | Loss: 0.00001412
Iteration 87/1000 | Loss: 0.00001412
Iteration 88/1000 | Loss: 0.00001412
Iteration 89/1000 | Loss: 0.00001412
Iteration 90/1000 | Loss: 0.00001412
Iteration 91/1000 | Loss: 0.00001412
Iteration 92/1000 | Loss: 0.00001412
Iteration 93/1000 | Loss: 0.00001411
Iteration 94/1000 | Loss: 0.00001411
Iteration 95/1000 | Loss: 0.00001411
Iteration 96/1000 | Loss: 0.00001411
Iteration 97/1000 | Loss: 0.00001410
Iteration 98/1000 | Loss: 0.00001410
Iteration 99/1000 | Loss: 0.00001410
Iteration 100/1000 | Loss: 0.00001409
Iteration 101/1000 | Loss: 0.00001409
Iteration 102/1000 | Loss: 0.00001409
Iteration 103/1000 | Loss: 0.00001409
Iteration 104/1000 | Loss: 0.00001408
Iteration 105/1000 | Loss: 0.00001408
Iteration 106/1000 | Loss: 0.00001407
Iteration 107/1000 | Loss: 0.00001407
Iteration 108/1000 | Loss: 0.00001407
Iteration 109/1000 | Loss: 0.00001406
Iteration 110/1000 | Loss: 0.00001406
Iteration 111/1000 | Loss: 0.00001406
Iteration 112/1000 | Loss: 0.00001406
Iteration 113/1000 | Loss: 0.00001406
Iteration 114/1000 | Loss: 0.00001406
Iteration 115/1000 | Loss: 0.00001406
Iteration 116/1000 | Loss: 0.00001406
Iteration 117/1000 | Loss: 0.00001406
Iteration 118/1000 | Loss: 0.00001406
Iteration 119/1000 | Loss: 0.00001406
Iteration 120/1000 | Loss: 0.00001406
Iteration 121/1000 | Loss: 0.00001405
Iteration 122/1000 | Loss: 0.00001405
Iteration 123/1000 | Loss: 0.00001405
Iteration 124/1000 | Loss: 0.00001405
Iteration 125/1000 | Loss: 0.00001405
Iteration 126/1000 | Loss: 0.00001405
Iteration 127/1000 | Loss: 0.00001405
Iteration 128/1000 | Loss: 0.00001405
Iteration 129/1000 | Loss: 0.00001405
Iteration 130/1000 | Loss: 0.00001405
Iteration 131/1000 | Loss: 0.00001405
Iteration 132/1000 | Loss: 0.00001405
Iteration 133/1000 | Loss: 0.00001405
Iteration 134/1000 | Loss: 0.00001405
Iteration 135/1000 | Loss: 0.00001405
Iteration 136/1000 | Loss: 0.00001404
Iteration 137/1000 | Loss: 0.00001404
Iteration 138/1000 | Loss: 0.00001404
Iteration 139/1000 | Loss: 0.00001404
Iteration 140/1000 | Loss: 0.00001404
Iteration 141/1000 | Loss: 0.00001404
Iteration 142/1000 | Loss: 0.00001404
Iteration 143/1000 | Loss: 0.00001404
Iteration 144/1000 | Loss: 0.00001404
Iteration 145/1000 | Loss: 0.00001404
Iteration 146/1000 | Loss: 0.00001404
Iteration 147/1000 | Loss: 0.00001404
Iteration 148/1000 | Loss: 0.00001404
Iteration 149/1000 | Loss: 0.00001403
Iteration 150/1000 | Loss: 0.00001403
Iteration 151/1000 | Loss: 0.00001403
Iteration 152/1000 | Loss: 0.00001403
Iteration 153/1000 | Loss: 0.00001403
Iteration 154/1000 | Loss: 0.00001403
Iteration 155/1000 | Loss: 0.00001403
Iteration 156/1000 | Loss: 0.00001403
Iteration 157/1000 | Loss: 0.00001403
Iteration 158/1000 | Loss: 0.00001403
Iteration 159/1000 | Loss: 0.00001403
Iteration 160/1000 | Loss: 0.00001403
Iteration 161/1000 | Loss: 0.00001403
Iteration 162/1000 | Loss: 0.00001402
Iteration 163/1000 | Loss: 0.00001402
Iteration 164/1000 | Loss: 0.00001402
Iteration 165/1000 | Loss: 0.00001402
Iteration 166/1000 | Loss: 0.00001402
Iteration 167/1000 | Loss: 0.00001402
Iteration 168/1000 | Loss: 0.00001402
Iteration 169/1000 | Loss: 0.00001402
Iteration 170/1000 | Loss: 0.00001401
Iteration 171/1000 | Loss: 0.00001401
Iteration 172/1000 | Loss: 0.00001401
Iteration 173/1000 | Loss: 0.00001401
Iteration 174/1000 | Loss: 0.00001401
Iteration 175/1000 | Loss: 0.00001401
Iteration 176/1000 | Loss: 0.00001401
Iteration 177/1000 | Loss: 0.00001401
Iteration 178/1000 | Loss: 0.00001401
Iteration 179/1000 | Loss: 0.00001401
Iteration 180/1000 | Loss: 0.00001401
Iteration 181/1000 | Loss: 0.00001401
Iteration 182/1000 | Loss: 0.00001401
Iteration 183/1000 | Loss: 0.00001401
Iteration 184/1000 | Loss: 0.00001401
Iteration 185/1000 | Loss: 0.00001401
Iteration 186/1000 | Loss: 0.00001401
Iteration 187/1000 | Loss: 0.00001401
Iteration 188/1000 | Loss: 0.00001401
Iteration 189/1000 | Loss: 0.00001400
Iteration 190/1000 | Loss: 0.00001400
Iteration 191/1000 | Loss: 0.00001400
Iteration 192/1000 | Loss: 0.00001400
Iteration 193/1000 | Loss: 0.00001400
Iteration 194/1000 | Loss: 0.00001400
Iteration 195/1000 | Loss: 0.00001400
Iteration 196/1000 | Loss: 0.00001400
Iteration 197/1000 | Loss: 0.00001400
Iteration 198/1000 | Loss: 0.00001400
Iteration 199/1000 | Loss: 0.00001400
Iteration 200/1000 | Loss: 0.00001400
Iteration 201/1000 | Loss: 0.00001400
Iteration 202/1000 | Loss: 0.00001400
Iteration 203/1000 | Loss: 0.00001400
Iteration 204/1000 | Loss: 0.00001400
Iteration 205/1000 | Loss: 0.00001400
Iteration 206/1000 | Loss: 0.00001400
Iteration 207/1000 | Loss: 0.00001400
Iteration 208/1000 | Loss: 0.00001400
Iteration 209/1000 | Loss: 0.00001400
Iteration 210/1000 | Loss: 0.00001399
Iteration 211/1000 | Loss: 0.00001399
Iteration 212/1000 | Loss: 0.00001399
Iteration 213/1000 | Loss: 0.00001399
Iteration 214/1000 | Loss: 0.00001399
Iteration 215/1000 | Loss: 0.00001399
Iteration 216/1000 | Loss: 0.00001399
Iteration 217/1000 | Loss: 0.00001399
Iteration 218/1000 | Loss: 0.00001399
Iteration 219/1000 | Loss: 0.00001399
Iteration 220/1000 | Loss: 0.00001399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.399363191012526e-05, 1.399363191012526e-05, 1.399363191012526e-05, 1.399363191012526e-05, 1.399363191012526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.399363191012526e-05

Optimization complete. Final v2v error: 3.090094804763794 mm

Highest mean error: 4.461416721343994 mm for frame 222

Lowest mean error: 2.6852450370788574 mm for frame 39

Saving results

Total time: 65.80325937271118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081290
Iteration 2/25 | Loss: 0.00271163
Iteration 3/25 | Loss: 0.00183203
Iteration 4/25 | Loss: 0.00185793
Iteration 5/25 | Loss: 0.00154727
Iteration 6/25 | Loss: 0.00142732
Iteration 7/25 | Loss: 0.00128344
Iteration 8/25 | Loss: 0.00121323
Iteration 9/25 | Loss: 0.00118032
Iteration 10/25 | Loss: 0.00115188
Iteration 11/25 | Loss: 0.00113086
Iteration 12/25 | Loss: 0.00111325
Iteration 13/25 | Loss: 0.00110868
Iteration 14/25 | Loss: 0.00110802
Iteration 15/25 | Loss: 0.00110296
Iteration 16/25 | Loss: 0.00110256
Iteration 17/25 | Loss: 0.00110168
Iteration 18/25 | Loss: 0.00109360
Iteration 19/25 | Loss: 0.00108863
Iteration 20/25 | Loss: 0.00108665
Iteration 21/25 | Loss: 0.00108585
Iteration 22/25 | Loss: 0.00108509
Iteration 23/25 | Loss: 0.00108978
Iteration 24/25 | Loss: 0.00109112
Iteration 25/25 | Loss: 0.00108599

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55639911
Iteration 2/25 | Loss: 0.00430552
Iteration 3/25 | Loss: 0.00424286
Iteration 4/25 | Loss: 0.00424286
Iteration 5/25 | Loss: 0.00424286
Iteration 6/25 | Loss: 0.00424286
Iteration 7/25 | Loss: 0.00424286
Iteration 8/25 | Loss: 0.00424286
Iteration 9/25 | Loss: 0.00424285
Iteration 10/25 | Loss: 0.00424285
Iteration 11/25 | Loss: 0.00424285
Iteration 12/25 | Loss: 0.00424285
Iteration 13/25 | Loss: 0.00424285
Iteration 14/25 | Loss: 0.00424285
Iteration 15/25 | Loss: 0.00424285
Iteration 16/25 | Loss: 0.00424285
Iteration 17/25 | Loss: 0.00424285
Iteration 18/25 | Loss: 0.00424285
Iteration 19/25 | Loss: 0.00424285
Iteration 20/25 | Loss: 0.00424285
Iteration 21/25 | Loss: 0.00424285
Iteration 22/25 | Loss: 0.00424285
Iteration 23/25 | Loss: 0.00424285
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.004242853727191687, 0.004242853727191687, 0.004242853727191687, 0.004242853727191687, 0.004242853727191687]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004242853727191687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00424285
Iteration 2/1000 | Loss: 0.00069625
Iteration 3/1000 | Loss: 0.00323531
Iteration 4/1000 | Loss: 0.00146770
Iteration 5/1000 | Loss: 0.00037766
Iteration 6/1000 | Loss: 0.00029956
Iteration 7/1000 | Loss: 0.00022081
Iteration 8/1000 | Loss: 0.00053752
Iteration 9/1000 | Loss: 0.00017874
Iteration 10/1000 | Loss: 0.00031163
Iteration 11/1000 | Loss: 0.00106734
Iteration 12/1000 | Loss: 0.00066822
Iteration 13/1000 | Loss: 0.00015671
Iteration 14/1000 | Loss: 0.00015551
Iteration 15/1000 | Loss: 0.00034086
Iteration 16/1000 | Loss: 0.00010225
Iteration 17/1000 | Loss: 0.00011138
Iteration 18/1000 | Loss: 0.00009406
Iteration 19/1000 | Loss: 0.00009092
Iteration 20/1000 | Loss: 0.00014299
Iteration 21/1000 | Loss: 0.00014499
Iteration 22/1000 | Loss: 0.00017568
Iteration 23/1000 | Loss: 0.00008092
Iteration 24/1000 | Loss: 0.00029150
Iteration 25/1000 | Loss: 0.00008468
Iteration 26/1000 | Loss: 0.00007796
Iteration 27/1000 | Loss: 0.00007470
Iteration 28/1000 | Loss: 0.00008302
Iteration 29/1000 | Loss: 0.00015430
Iteration 30/1000 | Loss: 0.00125351
Iteration 31/1000 | Loss: 0.00018409
Iteration 32/1000 | Loss: 0.00022755
Iteration 33/1000 | Loss: 0.00013964
Iteration 34/1000 | Loss: 0.00026641
Iteration 35/1000 | Loss: 0.00014523
Iteration 36/1000 | Loss: 0.00044472
Iteration 37/1000 | Loss: 0.00025055
Iteration 38/1000 | Loss: 0.00012283
Iteration 39/1000 | Loss: 0.00003215
Iteration 40/1000 | Loss: 0.00002944
Iteration 41/1000 | Loss: 0.00006874
Iteration 42/1000 | Loss: 0.00008594
Iteration 43/1000 | Loss: 0.00002410
Iteration 44/1000 | Loss: 0.00015298
Iteration 45/1000 | Loss: 0.00006413
Iteration 46/1000 | Loss: 0.00002083
Iteration 47/1000 | Loss: 0.00018983
Iteration 48/1000 | Loss: 0.00001972
Iteration 49/1000 | Loss: 0.00001921
Iteration 50/1000 | Loss: 0.00001888
Iteration 51/1000 | Loss: 0.00001885
Iteration 52/1000 | Loss: 0.00001867
Iteration 53/1000 | Loss: 0.00001857
Iteration 54/1000 | Loss: 0.00001856
Iteration 55/1000 | Loss: 0.00001851
Iteration 56/1000 | Loss: 0.00001849
Iteration 57/1000 | Loss: 0.00001849
Iteration 58/1000 | Loss: 0.00001849
Iteration 59/1000 | Loss: 0.00001849
Iteration 60/1000 | Loss: 0.00001849
Iteration 61/1000 | Loss: 0.00001849
Iteration 62/1000 | Loss: 0.00001848
Iteration 63/1000 | Loss: 0.00001848
Iteration 64/1000 | Loss: 0.00001848
Iteration 65/1000 | Loss: 0.00001848
Iteration 66/1000 | Loss: 0.00001848
Iteration 67/1000 | Loss: 0.00001847
Iteration 68/1000 | Loss: 0.00001847
Iteration 69/1000 | Loss: 0.00001846
Iteration 70/1000 | Loss: 0.00001846
Iteration 71/1000 | Loss: 0.00001846
Iteration 72/1000 | Loss: 0.00001845
Iteration 73/1000 | Loss: 0.00001845
Iteration 74/1000 | Loss: 0.00001844
Iteration 75/1000 | Loss: 0.00001843
Iteration 76/1000 | Loss: 0.00001843
Iteration 77/1000 | Loss: 0.00001842
Iteration 78/1000 | Loss: 0.00001842
Iteration 79/1000 | Loss: 0.00001842
Iteration 80/1000 | Loss: 0.00001842
Iteration 81/1000 | Loss: 0.00001842
Iteration 82/1000 | Loss: 0.00001842
Iteration 83/1000 | Loss: 0.00001842
Iteration 84/1000 | Loss: 0.00001842
Iteration 85/1000 | Loss: 0.00001840
Iteration 86/1000 | Loss: 0.00001840
Iteration 87/1000 | Loss: 0.00001840
Iteration 88/1000 | Loss: 0.00001839
Iteration 89/1000 | Loss: 0.00001839
Iteration 90/1000 | Loss: 0.00001839
Iteration 91/1000 | Loss: 0.00001839
Iteration 92/1000 | Loss: 0.00001838
Iteration 93/1000 | Loss: 0.00001838
Iteration 94/1000 | Loss: 0.00001837
Iteration 95/1000 | Loss: 0.00001837
Iteration 96/1000 | Loss: 0.00001834
Iteration 97/1000 | Loss: 0.00001834
Iteration 98/1000 | Loss: 0.00001833
Iteration 99/1000 | Loss: 0.00001832
Iteration 100/1000 | Loss: 0.00001832
Iteration 101/1000 | Loss: 0.00001831
Iteration 102/1000 | Loss: 0.00001830
Iteration 103/1000 | Loss: 0.00001830
Iteration 104/1000 | Loss: 0.00001830
Iteration 105/1000 | Loss: 0.00001829
Iteration 106/1000 | Loss: 0.00001829
Iteration 107/1000 | Loss: 0.00001829
Iteration 108/1000 | Loss: 0.00001829
Iteration 109/1000 | Loss: 0.00001829
Iteration 110/1000 | Loss: 0.00001829
Iteration 111/1000 | Loss: 0.00001829
Iteration 112/1000 | Loss: 0.00001829
Iteration 113/1000 | Loss: 0.00001829
Iteration 114/1000 | Loss: 0.00001829
Iteration 115/1000 | Loss: 0.00001828
Iteration 116/1000 | Loss: 0.00001828
Iteration 117/1000 | Loss: 0.00001827
Iteration 118/1000 | Loss: 0.00001827
Iteration 119/1000 | Loss: 0.00001826
Iteration 120/1000 | Loss: 0.00001826
Iteration 121/1000 | Loss: 0.00001826
Iteration 122/1000 | Loss: 0.00001826
Iteration 123/1000 | Loss: 0.00001826
Iteration 124/1000 | Loss: 0.00001825
Iteration 125/1000 | Loss: 0.00001825
Iteration 126/1000 | Loss: 0.00001825
Iteration 127/1000 | Loss: 0.00001824
Iteration 128/1000 | Loss: 0.00001824
Iteration 129/1000 | Loss: 0.00008129
Iteration 130/1000 | Loss: 0.00024488
Iteration 131/1000 | Loss: 0.00002560
Iteration 132/1000 | Loss: 0.00004155
Iteration 133/1000 | Loss: 0.00001902
Iteration 134/1000 | Loss: 0.00006308
Iteration 135/1000 | Loss: 0.00001843
Iteration 136/1000 | Loss: 0.00001831
Iteration 137/1000 | Loss: 0.00001831
Iteration 138/1000 | Loss: 0.00007756
Iteration 139/1000 | Loss: 0.00001826
Iteration 140/1000 | Loss: 0.00001819
Iteration 141/1000 | Loss: 0.00001819
Iteration 142/1000 | Loss: 0.00001819
Iteration 143/1000 | Loss: 0.00001818
Iteration 144/1000 | Loss: 0.00001818
Iteration 145/1000 | Loss: 0.00001818
Iteration 146/1000 | Loss: 0.00001818
Iteration 147/1000 | Loss: 0.00001818
Iteration 148/1000 | Loss: 0.00001818
Iteration 149/1000 | Loss: 0.00001818
Iteration 150/1000 | Loss: 0.00001817
Iteration 151/1000 | Loss: 0.00001817
Iteration 152/1000 | Loss: 0.00001817
Iteration 153/1000 | Loss: 0.00001817
Iteration 154/1000 | Loss: 0.00001817
Iteration 155/1000 | Loss: 0.00001817
Iteration 156/1000 | Loss: 0.00001817
Iteration 157/1000 | Loss: 0.00004168
Iteration 158/1000 | Loss: 0.00001971
Iteration 159/1000 | Loss: 0.00001828
Iteration 160/1000 | Loss: 0.00001827
Iteration 161/1000 | Loss: 0.00001826
Iteration 162/1000 | Loss: 0.00001824
Iteration 163/1000 | Loss: 0.00001823
Iteration 164/1000 | Loss: 0.00001823
Iteration 165/1000 | Loss: 0.00001823
Iteration 166/1000 | Loss: 0.00001822
Iteration 167/1000 | Loss: 0.00001822
Iteration 168/1000 | Loss: 0.00001822
Iteration 169/1000 | Loss: 0.00001822
Iteration 170/1000 | Loss: 0.00001864
Iteration 171/1000 | Loss: 0.00001821
Iteration 172/1000 | Loss: 0.00001821
Iteration 173/1000 | Loss: 0.00001821
Iteration 174/1000 | Loss: 0.00001821
Iteration 175/1000 | Loss: 0.00001821
Iteration 176/1000 | Loss: 0.00001821
Iteration 177/1000 | Loss: 0.00001821
Iteration 178/1000 | Loss: 0.00001820
Iteration 179/1000 | Loss: 0.00001820
Iteration 180/1000 | Loss: 0.00001820
Iteration 181/1000 | Loss: 0.00001820
Iteration 182/1000 | Loss: 0.00001820
Iteration 183/1000 | Loss: 0.00001820
Iteration 184/1000 | Loss: 0.00001820
Iteration 185/1000 | Loss: 0.00001820
Iteration 186/1000 | Loss: 0.00001820
Iteration 187/1000 | Loss: 0.00001820
Iteration 188/1000 | Loss: 0.00001820
Iteration 189/1000 | Loss: 0.00001819
Iteration 190/1000 | Loss: 0.00001819
Iteration 191/1000 | Loss: 0.00001819
Iteration 192/1000 | Loss: 0.00001818
Iteration 193/1000 | Loss: 0.00001818
Iteration 194/1000 | Loss: 0.00001818
Iteration 195/1000 | Loss: 0.00001818
Iteration 196/1000 | Loss: 0.00001818
Iteration 197/1000 | Loss: 0.00001818
Iteration 198/1000 | Loss: 0.00001818
Iteration 199/1000 | Loss: 0.00001817
Iteration 200/1000 | Loss: 0.00001817
Iteration 201/1000 | Loss: 0.00001817
Iteration 202/1000 | Loss: 0.00001817
Iteration 203/1000 | Loss: 0.00001817
Iteration 204/1000 | Loss: 0.00001817
Iteration 205/1000 | Loss: 0.00001816
Iteration 206/1000 | Loss: 0.00001816
Iteration 207/1000 | Loss: 0.00001816
Iteration 208/1000 | Loss: 0.00001816
Iteration 209/1000 | Loss: 0.00001816
Iteration 210/1000 | Loss: 0.00001816
Iteration 211/1000 | Loss: 0.00001816
Iteration 212/1000 | Loss: 0.00003392
Iteration 213/1000 | Loss: 0.00001819
Iteration 214/1000 | Loss: 0.00001819
Iteration 215/1000 | Loss: 0.00001818
Iteration 216/1000 | Loss: 0.00001818
Iteration 217/1000 | Loss: 0.00001818
Iteration 218/1000 | Loss: 0.00001818
Iteration 219/1000 | Loss: 0.00001818
Iteration 220/1000 | Loss: 0.00001818
Iteration 221/1000 | Loss: 0.00001818
Iteration 222/1000 | Loss: 0.00001818
Iteration 223/1000 | Loss: 0.00001818
Iteration 224/1000 | Loss: 0.00001818
Iteration 225/1000 | Loss: 0.00001817
Iteration 226/1000 | Loss: 0.00001817
Iteration 227/1000 | Loss: 0.00001816
Iteration 228/1000 | Loss: 0.00001816
Iteration 229/1000 | Loss: 0.00001816
Iteration 230/1000 | Loss: 0.00001816
Iteration 231/1000 | Loss: 0.00001816
Iteration 232/1000 | Loss: 0.00001816
Iteration 233/1000 | Loss: 0.00001816
Iteration 234/1000 | Loss: 0.00001816
Iteration 235/1000 | Loss: 0.00001816
Iteration 236/1000 | Loss: 0.00001816
Iteration 237/1000 | Loss: 0.00001815
Iteration 238/1000 | Loss: 0.00001815
Iteration 239/1000 | Loss: 0.00001815
Iteration 240/1000 | Loss: 0.00001815
Iteration 241/1000 | Loss: 0.00001815
Iteration 242/1000 | Loss: 0.00001815
Iteration 243/1000 | Loss: 0.00001815
Iteration 244/1000 | Loss: 0.00001815
Iteration 245/1000 | Loss: 0.00001815
Iteration 246/1000 | Loss: 0.00001815
Iteration 247/1000 | Loss: 0.00001814
Iteration 248/1000 | Loss: 0.00001814
Iteration 249/1000 | Loss: 0.00001814
Iteration 250/1000 | Loss: 0.00001814
Iteration 251/1000 | Loss: 0.00001814
Iteration 252/1000 | Loss: 0.00001814
Iteration 253/1000 | Loss: 0.00001814
Iteration 254/1000 | Loss: 0.00001814
Iteration 255/1000 | Loss: 0.00001814
Iteration 256/1000 | Loss: 0.00001814
Iteration 257/1000 | Loss: 0.00001814
Iteration 258/1000 | Loss: 0.00001814
Iteration 259/1000 | Loss: 0.00001814
Iteration 260/1000 | Loss: 0.00001814
Iteration 261/1000 | Loss: 0.00001814
Iteration 262/1000 | Loss: 0.00001813
Iteration 263/1000 | Loss: 0.00001813
Iteration 264/1000 | Loss: 0.00001813
Iteration 265/1000 | Loss: 0.00001813
Iteration 266/1000 | Loss: 0.00001813
Iteration 267/1000 | Loss: 0.00001813
Iteration 268/1000 | Loss: 0.00001813
Iteration 269/1000 | Loss: 0.00001813
Iteration 270/1000 | Loss: 0.00001813
Iteration 271/1000 | Loss: 0.00001813
Iteration 272/1000 | Loss: 0.00001813
Iteration 273/1000 | Loss: 0.00001813
Iteration 274/1000 | Loss: 0.00001813
Iteration 275/1000 | Loss: 0.00001813
Iteration 276/1000 | Loss: 0.00001813
Iteration 277/1000 | Loss: 0.00001813
Iteration 278/1000 | Loss: 0.00001813
Iteration 279/1000 | Loss: 0.00001813
Iteration 280/1000 | Loss: 0.00001813
Iteration 281/1000 | Loss: 0.00001813
Iteration 282/1000 | Loss: 0.00001813
Iteration 283/1000 | Loss: 0.00001813
Iteration 284/1000 | Loss: 0.00001813
Iteration 285/1000 | Loss: 0.00001813
Iteration 286/1000 | Loss: 0.00001813
Iteration 287/1000 | Loss: 0.00001813
Iteration 288/1000 | Loss: 0.00001813
Iteration 289/1000 | Loss: 0.00001813
Iteration 290/1000 | Loss: 0.00001813
Iteration 291/1000 | Loss: 0.00001813
Iteration 292/1000 | Loss: 0.00001813
Iteration 293/1000 | Loss: 0.00001813
Iteration 294/1000 | Loss: 0.00001813
Iteration 295/1000 | Loss: 0.00001813
Iteration 296/1000 | Loss: 0.00001813
Iteration 297/1000 | Loss: 0.00001813
Iteration 298/1000 | Loss: 0.00001813
Iteration 299/1000 | Loss: 0.00001813
Iteration 300/1000 | Loss: 0.00001813
Iteration 301/1000 | Loss: 0.00001813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [1.812885057006497e-05, 1.812885057006497e-05, 1.812885057006497e-05, 1.812885057006497e-05, 1.812885057006497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.812885057006497e-05

Optimization complete. Final v2v error: 3.6503660678863525 mm

Highest mean error: 4.051832675933838 mm for frame 127

Lowest mean error: 3.325439214706421 mm for frame 88

Saving results

Total time: 176.22144651412964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909576
Iteration 2/25 | Loss: 0.00092623
Iteration 3/25 | Loss: 0.00078087
Iteration 4/25 | Loss: 0.00076441
Iteration 5/25 | Loss: 0.00076173
Iteration 6/25 | Loss: 0.00076089
Iteration 7/25 | Loss: 0.00076077
Iteration 8/25 | Loss: 0.00076077
Iteration 9/25 | Loss: 0.00076077
Iteration 10/25 | Loss: 0.00076077
Iteration 11/25 | Loss: 0.00076077
Iteration 12/25 | Loss: 0.00076077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007607699371874332, 0.0007607699371874332, 0.0007607699371874332, 0.0007607699371874332, 0.0007607699371874332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007607699371874332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92527449
Iteration 2/25 | Loss: 0.00127995
Iteration 3/25 | Loss: 0.00127995
Iteration 4/25 | Loss: 0.00127995
Iteration 5/25 | Loss: 0.00127995
Iteration 6/25 | Loss: 0.00127995
Iteration 7/25 | Loss: 0.00127995
Iteration 8/25 | Loss: 0.00127995
Iteration 9/25 | Loss: 0.00127995
Iteration 10/25 | Loss: 0.00127995
Iteration 11/25 | Loss: 0.00127995
Iteration 12/25 | Loss: 0.00127995
Iteration 13/25 | Loss: 0.00127995
Iteration 14/25 | Loss: 0.00127995
Iteration 15/25 | Loss: 0.00127995
Iteration 16/25 | Loss: 0.00127995
Iteration 17/25 | Loss: 0.00127995
Iteration 18/25 | Loss: 0.00127995
Iteration 19/25 | Loss: 0.00127995
Iteration 20/25 | Loss: 0.00127995
Iteration 21/25 | Loss: 0.00127995
Iteration 22/25 | Loss: 0.00127995
Iteration 23/25 | Loss: 0.00127995
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012799506075680256, 0.0012799506075680256, 0.0012799506075680256, 0.0012799506075680256, 0.0012799506075680256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012799506075680256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127995
Iteration 2/1000 | Loss: 0.00002420
Iteration 3/1000 | Loss: 0.00001637
Iteration 4/1000 | Loss: 0.00001530
Iteration 5/1000 | Loss: 0.00001467
Iteration 6/1000 | Loss: 0.00001423
Iteration 7/1000 | Loss: 0.00001394
Iteration 8/1000 | Loss: 0.00001378
Iteration 9/1000 | Loss: 0.00001377
Iteration 10/1000 | Loss: 0.00001362
Iteration 11/1000 | Loss: 0.00001357
Iteration 12/1000 | Loss: 0.00001354
Iteration 13/1000 | Loss: 0.00001348
Iteration 14/1000 | Loss: 0.00001341
Iteration 15/1000 | Loss: 0.00001337
Iteration 16/1000 | Loss: 0.00001335
Iteration 17/1000 | Loss: 0.00001335
Iteration 18/1000 | Loss: 0.00001334
Iteration 19/1000 | Loss: 0.00001334
Iteration 20/1000 | Loss: 0.00001334
Iteration 21/1000 | Loss: 0.00001328
Iteration 22/1000 | Loss: 0.00001327
Iteration 23/1000 | Loss: 0.00001322
Iteration 24/1000 | Loss: 0.00001322
Iteration 25/1000 | Loss: 0.00001322
Iteration 26/1000 | Loss: 0.00001322
Iteration 27/1000 | Loss: 0.00001322
Iteration 28/1000 | Loss: 0.00001321
Iteration 29/1000 | Loss: 0.00001321
Iteration 30/1000 | Loss: 0.00001321
Iteration 31/1000 | Loss: 0.00001321
Iteration 32/1000 | Loss: 0.00001321
Iteration 33/1000 | Loss: 0.00001321
Iteration 34/1000 | Loss: 0.00001321
Iteration 35/1000 | Loss: 0.00001320
Iteration 36/1000 | Loss: 0.00001320
Iteration 37/1000 | Loss: 0.00001319
Iteration 38/1000 | Loss: 0.00001318
Iteration 39/1000 | Loss: 0.00001318
Iteration 40/1000 | Loss: 0.00001318
Iteration 41/1000 | Loss: 0.00001318
Iteration 42/1000 | Loss: 0.00001318
Iteration 43/1000 | Loss: 0.00001318
Iteration 44/1000 | Loss: 0.00001317
Iteration 45/1000 | Loss: 0.00001317
Iteration 46/1000 | Loss: 0.00001317
Iteration 47/1000 | Loss: 0.00001317
Iteration 48/1000 | Loss: 0.00001317
Iteration 49/1000 | Loss: 0.00001317
Iteration 50/1000 | Loss: 0.00001317
Iteration 51/1000 | Loss: 0.00001317
Iteration 52/1000 | Loss: 0.00001317
Iteration 53/1000 | Loss: 0.00001316
Iteration 54/1000 | Loss: 0.00001316
Iteration 55/1000 | Loss: 0.00001316
Iteration 56/1000 | Loss: 0.00001316
Iteration 57/1000 | Loss: 0.00001316
Iteration 58/1000 | Loss: 0.00001316
Iteration 59/1000 | Loss: 0.00001315
Iteration 60/1000 | Loss: 0.00001315
Iteration 61/1000 | Loss: 0.00001315
Iteration 62/1000 | Loss: 0.00001314
Iteration 63/1000 | Loss: 0.00001314
Iteration 64/1000 | Loss: 0.00001314
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001313
Iteration 67/1000 | Loss: 0.00001313
Iteration 68/1000 | Loss: 0.00001313
Iteration 69/1000 | Loss: 0.00001313
Iteration 70/1000 | Loss: 0.00001313
Iteration 71/1000 | Loss: 0.00001313
Iteration 72/1000 | Loss: 0.00001313
Iteration 73/1000 | Loss: 0.00001313
Iteration 74/1000 | Loss: 0.00001312
Iteration 75/1000 | Loss: 0.00001312
Iteration 76/1000 | Loss: 0.00001312
Iteration 77/1000 | Loss: 0.00001312
Iteration 78/1000 | Loss: 0.00001312
Iteration 79/1000 | Loss: 0.00001312
Iteration 80/1000 | Loss: 0.00001312
Iteration 81/1000 | Loss: 0.00001311
Iteration 82/1000 | Loss: 0.00001311
Iteration 83/1000 | Loss: 0.00001311
Iteration 84/1000 | Loss: 0.00001310
Iteration 85/1000 | Loss: 0.00001310
Iteration 86/1000 | Loss: 0.00001310
Iteration 87/1000 | Loss: 0.00001310
Iteration 88/1000 | Loss: 0.00001310
Iteration 89/1000 | Loss: 0.00001309
Iteration 90/1000 | Loss: 0.00001309
Iteration 91/1000 | Loss: 0.00001309
Iteration 92/1000 | Loss: 0.00001309
Iteration 93/1000 | Loss: 0.00001308
Iteration 94/1000 | Loss: 0.00001308
Iteration 95/1000 | Loss: 0.00001308
Iteration 96/1000 | Loss: 0.00001308
Iteration 97/1000 | Loss: 0.00001307
Iteration 98/1000 | Loss: 0.00001307
Iteration 99/1000 | Loss: 0.00001306
Iteration 100/1000 | Loss: 0.00001306
Iteration 101/1000 | Loss: 0.00001306
Iteration 102/1000 | Loss: 0.00001306
Iteration 103/1000 | Loss: 0.00001306
Iteration 104/1000 | Loss: 0.00001305
Iteration 105/1000 | Loss: 0.00001305
Iteration 106/1000 | Loss: 0.00001305
Iteration 107/1000 | Loss: 0.00001304
Iteration 108/1000 | Loss: 0.00001304
Iteration 109/1000 | Loss: 0.00001304
Iteration 110/1000 | Loss: 0.00001303
Iteration 111/1000 | Loss: 0.00001303
Iteration 112/1000 | Loss: 0.00001303
Iteration 113/1000 | Loss: 0.00001303
Iteration 114/1000 | Loss: 0.00001303
Iteration 115/1000 | Loss: 0.00001303
Iteration 116/1000 | Loss: 0.00001303
Iteration 117/1000 | Loss: 0.00001303
Iteration 118/1000 | Loss: 0.00001303
Iteration 119/1000 | Loss: 0.00001302
Iteration 120/1000 | Loss: 0.00001302
Iteration 121/1000 | Loss: 0.00001302
Iteration 122/1000 | Loss: 0.00001302
Iteration 123/1000 | Loss: 0.00001302
Iteration 124/1000 | Loss: 0.00001302
Iteration 125/1000 | Loss: 0.00001301
Iteration 126/1000 | Loss: 0.00001301
Iteration 127/1000 | Loss: 0.00001301
Iteration 128/1000 | Loss: 0.00001300
Iteration 129/1000 | Loss: 0.00001300
Iteration 130/1000 | Loss: 0.00001300
Iteration 131/1000 | Loss: 0.00001300
Iteration 132/1000 | Loss: 0.00001300
Iteration 133/1000 | Loss: 0.00001300
Iteration 134/1000 | Loss: 0.00001300
Iteration 135/1000 | Loss: 0.00001300
Iteration 136/1000 | Loss: 0.00001300
Iteration 137/1000 | Loss: 0.00001300
Iteration 138/1000 | Loss: 0.00001300
Iteration 139/1000 | Loss: 0.00001299
Iteration 140/1000 | Loss: 0.00001299
Iteration 141/1000 | Loss: 0.00001299
Iteration 142/1000 | Loss: 0.00001299
Iteration 143/1000 | Loss: 0.00001299
Iteration 144/1000 | Loss: 0.00001299
Iteration 145/1000 | Loss: 0.00001299
Iteration 146/1000 | Loss: 0.00001299
Iteration 147/1000 | Loss: 0.00001299
Iteration 148/1000 | Loss: 0.00001299
Iteration 149/1000 | Loss: 0.00001299
Iteration 150/1000 | Loss: 0.00001299
Iteration 151/1000 | Loss: 0.00001299
Iteration 152/1000 | Loss: 0.00001299
Iteration 153/1000 | Loss: 0.00001299
Iteration 154/1000 | Loss: 0.00001299
Iteration 155/1000 | Loss: 0.00001299
Iteration 156/1000 | Loss: 0.00001299
Iteration 157/1000 | Loss: 0.00001299
Iteration 158/1000 | Loss: 0.00001299
Iteration 159/1000 | Loss: 0.00001299
Iteration 160/1000 | Loss: 0.00001299
Iteration 161/1000 | Loss: 0.00001299
Iteration 162/1000 | Loss: 0.00001299
Iteration 163/1000 | Loss: 0.00001299
Iteration 164/1000 | Loss: 0.00001299
Iteration 165/1000 | Loss: 0.00001299
Iteration 166/1000 | Loss: 0.00001299
Iteration 167/1000 | Loss: 0.00001299
Iteration 168/1000 | Loss: 0.00001299
Iteration 169/1000 | Loss: 0.00001299
Iteration 170/1000 | Loss: 0.00001299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.2993201380595565e-05, 1.2993201380595565e-05, 1.2993201380595565e-05, 1.2993201380595565e-05, 1.2993201380595565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2993201380595565e-05

Optimization complete. Final v2v error: 3.0091428756713867 mm

Highest mean error: 3.970585346221924 mm for frame 96

Lowest mean error: 2.790024757385254 mm for frame 0

Saving results

Total time: 64.24042630195618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845131
Iteration 2/25 | Loss: 0.00128162
Iteration 3/25 | Loss: 0.00097812
Iteration 4/25 | Loss: 0.00090351
Iteration 5/25 | Loss: 0.00089343
Iteration 6/25 | Loss: 0.00088656
Iteration 7/25 | Loss: 0.00087766
Iteration 8/25 | Loss: 0.00087186
Iteration 9/25 | Loss: 0.00086929
Iteration 10/25 | Loss: 0.00086518
Iteration 11/25 | Loss: 0.00086855
Iteration 12/25 | Loss: 0.00086673
Iteration 13/25 | Loss: 0.00086301
Iteration 14/25 | Loss: 0.00086200
Iteration 15/25 | Loss: 0.00086151
Iteration 16/25 | Loss: 0.00086135
Iteration 17/25 | Loss: 0.00086126
Iteration 18/25 | Loss: 0.00086116
Iteration 19/25 | Loss: 0.00086116
Iteration 20/25 | Loss: 0.00086116
Iteration 21/25 | Loss: 0.00086116
Iteration 22/25 | Loss: 0.00086116
Iteration 23/25 | Loss: 0.00086116
Iteration 24/25 | Loss: 0.00086116
Iteration 25/25 | Loss: 0.00086115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68501866
Iteration 2/25 | Loss: 0.00158506
Iteration 3/25 | Loss: 0.00158505
Iteration 4/25 | Loss: 0.00158505
Iteration 5/25 | Loss: 0.00158505
Iteration 6/25 | Loss: 0.00158505
Iteration 7/25 | Loss: 0.00158505
Iteration 8/25 | Loss: 0.00158505
Iteration 9/25 | Loss: 0.00158505
Iteration 10/25 | Loss: 0.00158505
Iteration 11/25 | Loss: 0.00158505
Iteration 12/25 | Loss: 0.00158505
Iteration 13/25 | Loss: 0.00158505
Iteration 14/25 | Loss: 0.00158505
Iteration 15/25 | Loss: 0.00158505
Iteration 16/25 | Loss: 0.00158505
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015850464114919305, 0.0015850464114919305, 0.0015850464114919305, 0.0015850464114919305, 0.0015850464114919305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015850464114919305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158505
Iteration 2/1000 | Loss: 0.00008549
Iteration 3/1000 | Loss: 0.00005887
Iteration 4/1000 | Loss: 0.00005168
Iteration 5/1000 | Loss: 0.00066719
Iteration 6/1000 | Loss: 0.00006416
Iteration 7/1000 | Loss: 0.00004841
Iteration 8/1000 | Loss: 0.00004373
Iteration 9/1000 | Loss: 0.00004081
Iteration 10/1000 | Loss: 0.00057456
Iteration 11/1000 | Loss: 0.00048087
Iteration 12/1000 | Loss: 0.00004099
Iteration 13/1000 | Loss: 0.00003913
Iteration 14/1000 | Loss: 0.00003740
Iteration 15/1000 | Loss: 0.00052890
Iteration 16/1000 | Loss: 0.00225032
Iteration 17/1000 | Loss: 0.00014470
Iteration 18/1000 | Loss: 0.00007648
Iteration 19/1000 | Loss: 0.00004995
Iteration 20/1000 | Loss: 0.00004056
Iteration 21/1000 | Loss: 0.00003381
Iteration 22/1000 | Loss: 0.00003012
Iteration 23/1000 | Loss: 0.00002742
Iteration 24/1000 | Loss: 0.00002660
Iteration 25/1000 | Loss: 0.00002615
Iteration 26/1000 | Loss: 0.00002578
Iteration 27/1000 | Loss: 0.00002550
Iteration 28/1000 | Loss: 0.00002534
Iteration 29/1000 | Loss: 0.00002519
Iteration 30/1000 | Loss: 0.00002515
Iteration 31/1000 | Loss: 0.00002509
Iteration 32/1000 | Loss: 0.00002493
Iteration 33/1000 | Loss: 0.00002492
Iteration 34/1000 | Loss: 0.00002486
Iteration 35/1000 | Loss: 0.00002483
Iteration 36/1000 | Loss: 0.00002482
Iteration 37/1000 | Loss: 0.00002482
Iteration 38/1000 | Loss: 0.00002481
Iteration 39/1000 | Loss: 0.00002479
Iteration 40/1000 | Loss: 0.00002479
Iteration 41/1000 | Loss: 0.00002479
Iteration 42/1000 | Loss: 0.00002478
Iteration 43/1000 | Loss: 0.00002478
Iteration 44/1000 | Loss: 0.00002478
Iteration 45/1000 | Loss: 0.00002478
Iteration 46/1000 | Loss: 0.00002478
Iteration 47/1000 | Loss: 0.00002477
Iteration 48/1000 | Loss: 0.00002477
Iteration 49/1000 | Loss: 0.00002475
Iteration 50/1000 | Loss: 0.00002475
Iteration 51/1000 | Loss: 0.00002475
Iteration 52/1000 | Loss: 0.00002475
Iteration 53/1000 | Loss: 0.00002475
Iteration 54/1000 | Loss: 0.00002475
Iteration 55/1000 | Loss: 0.00002475
Iteration 56/1000 | Loss: 0.00002475
Iteration 57/1000 | Loss: 0.00002475
Iteration 58/1000 | Loss: 0.00002474
Iteration 59/1000 | Loss: 0.00002474
Iteration 60/1000 | Loss: 0.00002473
Iteration 61/1000 | Loss: 0.00002473
Iteration 62/1000 | Loss: 0.00002473
Iteration 63/1000 | Loss: 0.00002472
Iteration 64/1000 | Loss: 0.00002472
Iteration 65/1000 | Loss: 0.00002472
Iteration 66/1000 | Loss: 0.00002472
Iteration 67/1000 | Loss: 0.00002471
Iteration 68/1000 | Loss: 0.00002471
Iteration 69/1000 | Loss: 0.00002471
Iteration 70/1000 | Loss: 0.00002471
Iteration 71/1000 | Loss: 0.00002471
Iteration 72/1000 | Loss: 0.00002470
Iteration 73/1000 | Loss: 0.00002470
Iteration 74/1000 | Loss: 0.00002470
Iteration 75/1000 | Loss: 0.00002470
Iteration 76/1000 | Loss: 0.00002470
Iteration 77/1000 | Loss: 0.00002470
Iteration 78/1000 | Loss: 0.00002470
Iteration 79/1000 | Loss: 0.00002469
Iteration 80/1000 | Loss: 0.00002469
Iteration 81/1000 | Loss: 0.00002469
Iteration 82/1000 | Loss: 0.00002469
Iteration 83/1000 | Loss: 0.00002469
Iteration 84/1000 | Loss: 0.00002469
Iteration 85/1000 | Loss: 0.00002469
Iteration 86/1000 | Loss: 0.00002469
Iteration 87/1000 | Loss: 0.00002469
Iteration 88/1000 | Loss: 0.00002469
Iteration 89/1000 | Loss: 0.00002468
Iteration 90/1000 | Loss: 0.00002468
Iteration 91/1000 | Loss: 0.00002468
Iteration 92/1000 | Loss: 0.00002468
Iteration 93/1000 | Loss: 0.00002468
Iteration 94/1000 | Loss: 0.00002468
Iteration 95/1000 | Loss: 0.00002468
Iteration 96/1000 | Loss: 0.00002468
Iteration 97/1000 | Loss: 0.00002468
Iteration 98/1000 | Loss: 0.00002468
Iteration 99/1000 | Loss: 0.00002468
Iteration 100/1000 | Loss: 0.00002468
Iteration 101/1000 | Loss: 0.00002468
Iteration 102/1000 | Loss: 0.00002468
Iteration 103/1000 | Loss: 0.00002468
Iteration 104/1000 | Loss: 0.00002468
Iteration 105/1000 | Loss: 0.00002468
Iteration 106/1000 | Loss: 0.00002468
Iteration 107/1000 | Loss: 0.00002468
Iteration 108/1000 | Loss: 0.00002468
Iteration 109/1000 | Loss: 0.00002468
Iteration 110/1000 | Loss: 0.00002468
Iteration 111/1000 | Loss: 0.00002468
Iteration 112/1000 | Loss: 0.00002468
Iteration 113/1000 | Loss: 0.00002468
Iteration 114/1000 | Loss: 0.00002468
Iteration 115/1000 | Loss: 0.00002468
Iteration 116/1000 | Loss: 0.00002468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.4677228793734685e-05, 2.4677228793734685e-05, 2.4677228793734685e-05, 2.4677228793734685e-05, 2.4677228793734685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4677228793734685e-05

Optimization complete. Final v2v error: 4.115688800811768 mm

Highest mean error: 4.707887649536133 mm for frame 211

Lowest mean error: 3.5078024864196777 mm for frame 147

Saving results

Total time: 131.07538414001465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924573
Iteration 2/25 | Loss: 0.00159765
Iteration 3/25 | Loss: 0.00113757
Iteration 4/25 | Loss: 0.00105961
Iteration 5/25 | Loss: 0.00105153
Iteration 6/25 | Loss: 0.00099499
Iteration 7/25 | Loss: 0.00098193
Iteration 8/25 | Loss: 0.00098019
Iteration 9/25 | Loss: 0.00097469
Iteration 10/25 | Loss: 0.00097393
Iteration 11/25 | Loss: 0.00097611
Iteration 12/25 | Loss: 0.00095063
Iteration 13/25 | Loss: 0.00095333
Iteration 14/25 | Loss: 0.00095494
Iteration 15/25 | Loss: 0.00094493
Iteration 16/25 | Loss: 0.00094463
Iteration 17/25 | Loss: 0.00094563
Iteration 18/25 | Loss: 0.00094320
Iteration 19/25 | Loss: 0.00094370
Iteration 20/25 | Loss: 0.00093950
Iteration 21/25 | Loss: 0.00093641
Iteration 22/25 | Loss: 0.00094331
Iteration 23/25 | Loss: 0.00094213
Iteration 24/25 | Loss: 0.00093682
Iteration 25/25 | Loss: 0.00093591

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.95727444
Iteration 2/25 | Loss: 0.00209651
Iteration 3/25 | Loss: 0.00209642
Iteration 4/25 | Loss: 0.00209642
Iteration 5/25 | Loss: 0.00209642
Iteration 6/25 | Loss: 0.00209642
Iteration 7/25 | Loss: 0.00209641
Iteration 8/25 | Loss: 0.00209641
Iteration 9/25 | Loss: 0.00209641
Iteration 10/25 | Loss: 0.00209641
Iteration 11/25 | Loss: 0.00209641
Iteration 12/25 | Loss: 0.00209641
Iteration 13/25 | Loss: 0.00209641
Iteration 14/25 | Loss: 0.00209641
Iteration 15/25 | Loss: 0.00209641
Iteration 16/25 | Loss: 0.00209641
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002096413867548108, 0.002096413867548108, 0.002096413867548108, 0.002096413867548108, 0.002096413867548108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002096413867548108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209641
Iteration 2/1000 | Loss: 0.00037161
Iteration 3/1000 | Loss: 0.00064233
Iteration 4/1000 | Loss: 0.00387365
Iteration 5/1000 | Loss: 0.00248255
Iteration 6/1000 | Loss: 0.00050641
Iteration 7/1000 | Loss: 0.00053755
Iteration 8/1000 | Loss: 0.00053350
Iteration 9/1000 | Loss: 0.00017166
Iteration 10/1000 | Loss: 0.00045649
Iteration 11/1000 | Loss: 0.00293010
Iteration 12/1000 | Loss: 0.00095902
Iteration 13/1000 | Loss: 0.00018099
Iteration 14/1000 | Loss: 0.00135626
Iteration 15/1000 | Loss: 0.00086989
Iteration 16/1000 | Loss: 0.00197198
Iteration 17/1000 | Loss: 0.00161914
Iteration 18/1000 | Loss: 0.00037762
Iteration 19/1000 | Loss: 0.00033814
Iteration 20/1000 | Loss: 0.00036908
Iteration 21/1000 | Loss: 0.00009728
Iteration 22/1000 | Loss: 0.00067674
Iteration 23/1000 | Loss: 0.00028597
Iteration 24/1000 | Loss: 0.00011089
Iteration 25/1000 | Loss: 0.00067020
Iteration 26/1000 | Loss: 0.00017585
Iteration 27/1000 | Loss: 0.00007900
Iteration 28/1000 | Loss: 0.00007572
Iteration 29/1000 | Loss: 0.00008181
Iteration 30/1000 | Loss: 0.00005507
Iteration 31/1000 | Loss: 0.00021978
Iteration 32/1000 | Loss: 0.00097953
Iteration 33/1000 | Loss: 0.00033872
Iteration 34/1000 | Loss: 0.00040344
Iteration 35/1000 | Loss: 0.00059451
Iteration 36/1000 | Loss: 0.00064704
Iteration 37/1000 | Loss: 0.00047003
Iteration 38/1000 | Loss: 0.00025315
Iteration 39/1000 | Loss: 0.00024403
Iteration 40/1000 | Loss: 0.00023667
Iteration 41/1000 | Loss: 0.00008462
Iteration 42/1000 | Loss: 0.00038088
Iteration 43/1000 | Loss: 0.00006530
Iteration 44/1000 | Loss: 0.00005366
Iteration 45/1000 | Loss: 0.00005712
Iteration 46/1000 | Loss: 0.00054283
Iteration 47/1000 | Loss: 0.00085535
Iteration 48/1000 | Loss: 0.00005838
Iteration 49/1000 | Loss: 0.00007490
Iteration 50/1000 | Loss: 0.00006592
Iteration 51/1000 | Loss: 0.00005781
Iteration 52/1000 | Loss: 0.00006274
Iteration 53/1000 | Loss: 0.00057379
Iteration 54/1000 | Loss: 0.00065512
Iteration 55/1000 | Loss: 0.00017809
Iteration 56/1000 | Loss: 0.00006591
Iteration 57/1000 | Loss: 0.00006789
Iteration 58/1000 | Loss: 0.00006983
Iteration 59/1000 | Loss: 0.00056591
Iteration 60/1000 | Loss: 0.00055009
Iteration 61/1000 | Loss: 0.00014426
Iteration 62/1000 | Loss: 0.00022826
Iteration 63/1000 | Loss: 0.00134934
Iteration 64/1000 | Loss: 0.00009959
Iteration 65/1000 | Loss: 0.00007598
Iteration 66/1000 | Loss: 0.00006373
Iteration 67/1000 | Loss: 0.00020593
Iteration 68/1000 | Loss: 0.00008065
Iteration 69/1000 | Loss: 0.00020129
Iteration 70/1000 | Loss: 0.00006736
Iteration 71/1000 | Loss: 0.00007799
Iteration 72/1000 | Loss: 0.00006315
Iteration 73/1000 | Loss: 0.00007419
Iteration 74/1000 | Loss: 0.00006799
Iteration 75/1000 | Loss: 0.00093534
Iteration 76/1000 | Loss: 0.00009182
Iteration 77/1000 | Loss: 0.00005732
Iteration 78/1000 | Loss: 0.00004402
Iteration 79/1000 | Loss: 0.00004029
Iteration 80/1000 | Loss: 0.00003817
Iteration 81/1000 | Loss: 0.00003701
Iteration 82/1000 | Loss: 0.00003617
Iteration 83/1000 | Loss: 0.00003571
Iteration 84/1000 | Loss: 0.00003545
Iteration 85/1000 | Loss: 0.00003524
Iteration 86/1000 | Loss: 0.00003504
Iteration 87/1000 | Loss: 0.00003483
Iteration 88/1000 | Loss: 0.00003481
Iteration 89/1000 | Loss: 0.00003481
Iteration 90/1000 | Loss: 0.00003480
Iteration 91/1000 | Loss: 0.00003479
Iteration 92/1000 | Loss: 0.00003479
Iteration 93/1000 | Loss: 0.00003478
Iteration 94/1000 | Loss: 0.00003465
Iteration 95/1000 | Loss: 0.00003464
Iteration 96/1000 | Loss: 0.00003450
Iteration 97/1000 | Loss: 0.00003449
Iteration 98/1000 | Loss: 0.00003448
Iteration 99/1000 | Loss: 0.00003444
Iteration 100/1000 | Loss: 0.00003443
Iteration 101/1000 | Loss: 0.00003440
Iteration 102/1000 | Loss: 0.00003438
Iteration 103/1000 | Loss: 0.00003429
Iteration 104/1000 | Loss: 0.00003423
Iteration 105/1000 | Loss: 0.00003421
Iteration 106/1000 | Loss: 0.00003421
Iteration 107/1000 | Loss: 0.00003420
Iteration 108/1000 | Loss: 0.00003419
Iteration 109/1000 | Loss: 0.00003419
Iteration 110/1000 | Loss: 0.00003419
Iteration 111/1000 | Loss: 0.00003419
Iteration 112/1000 | Loss: 0.00003419
Iteration 113/1000 | Loss: 0.00003419
Iteration 114/1000 | Loss: 0.00003419
Iteration 115/1000 | Loss: 0.00003419
Iteration 116/1000 | Loss: 0.00003419
Iteration 117/1000 | Loss: 0.00003419
Iteration 118/1000 | Loss: 0.00003418
Iteration 119/1000 | Loss: 0.00003418
Iteration 120/1000 | Loss: 0.00003417
Iteration 121/1000 | Loss: 0.00003416
Iteration 122/1000 | Loss: 0.00003416
Iteration 123/1000 | Loss: 0.00003416
Iteration 124/1000 | Loss: 0.00003415
Iteration 125/1000 | Loss: 0.00003415
Iteration 126/1000 | Loss: 0.00003413
Iteration 127/1000 | Loss: 0.00003413
Iteration 128/1000 | Loss: 0.00003413
Iteration 129/1000 | Loss: 0.00003412
Iteration 130/1000 | Loss: 0.00003412
Iteration 131/1000 | Loss: 0.00003410
Iteration 132/1000 | Loss: 0.00003409
Iteration 133/1000 | Loss: 0.00003409
Iteration 134/1000 | Loss: 0.00003409
Iteration 135/1000 | Loss: 0.00003409
Iteration 136/1000 | Loss: 0.00003409
Iteration 137/1000 | Loss: 0.00003409
Iteration 138/1000 | Loss: 0.00003408
Iteration 139/1000 | Loss: 0.00003408
Iteration 140/1000 | Loss: 0.00003407
Iteration 141/1000 | Loss: 0.00003407
Iteration 142/1000 | Loss: 0.00003406
Iteration 143/1000 | Loss: 0.00003406
Iteration 144/1000 | Loss: 0.00003406
Iteration 145/1000 | Loss: 0.00003405
Iteration 146/1000 | Loss: 0.00003405
Iteration 147/1000 | Loss: 0.00003405
Iteration 148/1000 | Loss: 0.00003405
Iteration 149/1000 | Loss: 0.00003405
Iteration 150/1000 | Loss: 0.00003405
Iteration 151/1000 | Loss: 0.00003405
Iteration 152/1000 | Loss: 0.00003404
Iteration 153/1000 | Loss: 0.00003404
Iteration 154/1000 | Loss: 0.00003404
Iteration 155/1000 | Loss: 0.00003404
Iteration 156/1000 | Loss: 0.00003403
Iteration 157/1000 | Loss: 0.00003403
Iteration 158/1000 | Loss: 0.00003403
Iteration 159/1000 | Loss: 0.00003403
Iteration 160/1000 | Loss: 0.00003402
Iteration 161/1000 | Loss: 0.00003402
Iteration 162/1000 | Loss: 0.00003402
Iteration 163/1000 | Loss: 0.00003402
Iteration 164/1000 | Loss: 0.00003402
Iteration 165/1000 | Loss: 0.00003402
Iteration 166/1000 | Loss: 0.00003402
Iteration 167/1000 | Loss: 0.00003402
Iteration 168/1000 | Loss: 0.00003402
Iteration 169/1000 | Loss: 0.00003402
Iteration 170/1000 | Loss: 0.00003401
Iteration 171/1000 | Loss: 0.00003401
Iteration 172/1000 | Loss: 0.00003401
Iteration 173/1000 | Loss: 0.00003401
Iteration 174/1000 | Loss: 0.00003400
Iteration 175/1000 | Loss: 0.00003400
Iteration 176/1000 | Loss: 0.00003400
Iteration 177/1000 | Loss: 0.00003400
Iteration 178/1000 | Loss: 0.00003399
Iteration 179/1000 | Loss: 0.00003399
Iteration 180/1000 | Loss: 0.00003399
Iteration 181/1000 | Loss: 0.00003399
Iteration 182/1000 | Loss: 0.00003399
Iteration 183/1000 | Loss: 0.00003399
Iteration 184/1000 | Loss: 0.00003398
Iteration 185/1000 | Loss: 0.00003398
Iteration 186/1000 | Loss: 0.00003398
Iteration 187/1000 | Loss: 0.00003398
Iteration 188/1000 | Loss: 0.00003398
Iteration 189/1000 | Loss: 0.00003398
Iteration 190/1000 | Loss: 0.00003398
Iteration 191/1000 | Loss: 0.00003398
Iteration 192/1000 | Loss: 0.00003398
Iteration 193/1000 | Loss: 0.00003398
Iteration 194/1000 | Loss: 0.00003398
Iteration 195/1000 | Loss: 0.00003398
Iteration 196/1000 | Loss: 0.00003398
Iteration 197/1000 | Loss: 0.00003397
Iteration 198/1000 | Loss: 0.00003397
Iteration 199/1000 | Loss: 0.00003397
Iteration 200/1000 | Loss: 0.00003397
Iteration 201/1000 | Loss: 0.00003397
Iteration 202/1000 | Loss: 0.00003396
Iteration 203/1000 | Loss: 0.00003396
Iteration 204/1000 | Loss: 0.00003396
Iteration 205/1000 | Loss: 0.00003396
Iteration 206/1000 | Loss: 0.00003396
Iteration 207/1000 | Loss: 0.00003395
Iteration 208/1000 | Loss: 0.00003395
Iteration 209/1000 | Loss: 0.00003395
Iteration 210/1000 | Loss: 0.00003394
Iteration 211/1000 | Loss: 0.00003394
Iteration 212/1000 | Loss: 0.00003394
Iteration 213/1000 | Loss: 0.00003394
Iteration 214/1000 | Loss: 0.00003393
Iteration 215/1000 | Loss: 0.00003393
Iteration 216/1000 | Loss: 0.00003393
Iteration 217/1000 | Loss: 0.00003393
Iteration 218/1000 | Loss: 0.00003393
Iteration 219/1000 | Loss: 0.00003393
Iteration 220/1000 | Loss: 0.00003393
Iteration 221/1000 | Loss: 0.00003392
Iteration 222/1000 | Loss: 0.00003392
Iteration 223/1000 | Loss: 0.00003392
Iteration 224/1000 | Loss: 0.00003392
Iteration 225/1000 | Loss: 0.00003392
Iteration 226/1000 | Loss: 0.00003392
Iteration 227/1000 | Loss: 0.00003392
Iteration 228/1000 | Loss: 0.00003391
Iteration 229/1000 | Loss: 0.00003391
Iteration 230/1000 | Loss: 0.00003391
Iteration 231/1000 | Loss: 0.00003391
Iteration 232/1000 | Loss: 0.00003391
Iteration 233/1000 | Loss: 0.00003391
Iteration 234/1000 | Loss: 0.00003391
Iteration 235/1000 | Loss: 0.00003391
Iteration 236/1000 | Loss: 0.00003391
Iteration 237/1000 | Loss: 0.00003391
Iteration 238/1000 | Loss: 0.00003391
Iteration 239/1000 | Loss: 0.00003391
Iteration 240/1000 | Loss: 0.00003390
Iteration 241/1000 | Loss: 0.00003390
Iteration 242/1000 | Loss: 0.00003390
Iteration 243/1000 | Loss: 0.00003390
Iteration 244/1000 | Loss: 0.00003390
Iteration 245/1000 | Loss: 0.00003390
Iteration 246/1000 | Loss: 0.00003390
Iteration 247/1000 | Loss: 0.00003390
Iteration 248/1000 | Loss: 0.00003390
Iteration 249/1000 | Loss: 0.00003390
Iteration 250/1000 | Loss: 0.00003390
Iteration 251/1000 | Loss: 0.00003389
Iteration 252/1000 | Loss: 0.00003389
Iteration 253/1000 | Loss: 0.00003389
Iteration 254/1000 | Loss: 0.00003389
Iteration 255/1000 | Loss: 0.00003389
Iteration 256/1000 | Loss: 0.00003389
Iteration 257/1000 | Loss: 0.00003389
Iteration 258/1000 | Loss: 0.00003389
Iteration 259/1000 | Loss: 0.00003389
Iteration 260/1000 | Loss: 0.00003389
Iteration 261/1000 | Loss: 0.00003388
Iteration 262/1000 | Loss: 0.00003388
Iteration 263/1000 | Loss: 0.00003388
Iteration 264/1000 | Loss: 0.00003388
Iteration 265/1000 | Loss: 0.00003388
Iteration 266/1000 | Loss: 0.00003388
Iteration 267/1000 | Loss: 0.00003388
Iteration 268/1000 | Loss: 0.00003388
Iteration 269/1000 | Loss: 0.00003388
Iteration 270/1000 | Loss: 0.00003387
Iteration 271/1000 | Loss: 0.00003387
Iteration 272/1000 | Loss: 0.00003387
Iteration 273/1000 | Loss: 0.00003387
Iteration 274/1000 | Loss: 0.00003387
Iteration 275/1000 | Loss: 0.00003387
Iteration 276/1000 | Loss: 0.00003387
Iteration 277/1000 | Loss: 0.00003387
Iteration 278/1000 | Loss: 0.00003387
Iteration 279/1000 | Loss: 0.00003387
Iteration 280/1000 | Loss: 0.00003387
Iteration 281/1000 | Loss: 0.00003387
Iteration 282/1000 | Loss: 0.00003387
Iteration 283/1000 | Loss: 0.00003387
Iteration 284/1000 | Loss: 0.00003387
Iteration 285/1000 | Loss: 0.00003387
Iteration 286/1000 | Loss: 0.00003387
Iteration 287/1000 | Loss: 0.00003387
Iteration 288/1000 | Loss: 0.00003387
Iteration 289/1000 | Loss: 0.00003387
Iteration 290/1000 | Loss: 0.00003387
Iteration 291/1000 | Loss: 0.00003387
Iteration 292/1000 | Loss: 0.00003387
Iteration 293/1000 | Loss: 0.00003387
Iteration 294/1000 | Loss: 0.00003387
Iteration 295/1000 | Loss: 0.00003387
Iteration 296/1000 | Loss: 0.00003387
Iteration 297/1000 | Loss: 0.00003387
Iteration 298/1000 | Loss: 0.00003387
Iteration 299/1000 | Loss: 0.00003387
Iteration 300/1000 | Loss: 0.00003387
Iteration 301/1000 | Loss: 0.00003387
Iteration 302/1000 | Loss: 0.00003387
Iteration 303/1000 | Loss: 0.00003387
Iteration 304/1000 | Loss: 0.00003387
Iteration 305/1000 | Loss: 0.00003387
Iteration 306/1000 | Loss: 0.00003387
Iteration 307/1000 | Loss: 0.00003387
Iteration 308/1000 | Loss: 0.00003387
Iteration 309/1000 | Loss: 0.00003387
Iteration 310/1000 | Loss: 0.00003387
Iteration 311/1000 | Loss: 0.00003387
Iteration 312/1000 | Loss: 0.00003387
Iteration 313/1000 | Loss: 0.00003387
Iteration 314/1000 | Loss: 0.00003387
Iteration 315/1000 | Loss: 0.00003387
Iteration 316/1000 | Loss: 0.00003387
Iteration 317/1000 | Loss: 0.00003387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 317. Stopping optimization.
Last 5 losses: [3.386892785783857e-05, 3.386892785783857e-05, 3.386892785783857e-05, 3.386892785783857e-05, 3.386892785783857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.386892785783857e-05

Optimization complete. Final v2v error: 4.6745500564575195 mm

Highest mean error: 11.759779930114746 mm for frame 66

Lowest mean error: 3.4231839179992676 mm for frame 133

Saving results

Total time: 242.0729022026062
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835878
Iteration 2/25 | Loss: 0.00130354
Iteration 3/25 | Loss: 0.00095280
Iteration 4/25 | Loss: 0.00088118
Iteration 5/25 | Loss: 0.00086437
Iteration 6/25 | Loss: 0.00085997
Iteration 7/25 | Loss: 0.00085442
Iteration 8/25 | Loss: 0.00085336
Iteration 9/25 | Loss: 0.00085649
Iteration 10/25 | Loss: 0.00085576
Iteration 11/25 | Loss: 0.00085388
Iteration 12/25 | Loss: 0.00085086
Iteration 13/25 | Loss: 0.00085245
Iteration 14/25 | Loss: 0.00085061
Iteration 15/25 | Loss: 0.00084889
Iteration 16/25 | Loss: 0.00084888
Iteration 17/25 | Loss: 0.00084888
Iteration 18/25 | Loss: 0.00084888
Iteration 19/25 | Loss: 0.00084888
Iteration 20/25 | Loss: 0.00084888
Iteration 21/25 | Loss: 0.00084888
Iteration 22/25 | Loss: 0.00084888
Iteration 23/25 | Loss: 0.00084888
Iteration 24/25 | Loss: 0.00084888
Iteration 25/25 | Loss: 0.00084888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72118711
Iteration 2/25 | Loss: 0.00141818
Iteration 3/25 | Loss: 0.00141818
Iteration 4/25 | Loss: 0.00141818
Iteration 5/25 | Loss: 0.00141818
Iteration 6/25 | Loss: 0.00141818
Iteration 7/25 | Loss: 0.00141818
Iteration 8/25 | Loss: 0.00141818
Iteration 9/25 | Loss: 0.00141817
Iteration 10/25 | Loss: 0.00141817
Iteration 11/25 | Loss: 0.00141818
Iteration 12/25 | Loss: 0.00141817
Iteration 13/25 | Loss: 0.00141818
Iteration 14/25 | Loss: 0.00141818
Iteration 15/25 | Loss: 0.00141818
Iteration 16/25 | Loss: 0.00141818
Iteration 17/25 | Loss: 0.00141818
Iteration 18/25 | Loss: 0.00141818
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014181750593706965, 0.0014181750593706965, 0.0014181750593706965, 0.0014181750593706965, 0.0014181750593706965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014181750593706965

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141818
Iteration 2/1000 | Loss: 0.00005243
Iteration 3/1000 | Loss: 0.00002853
Iteration 4/1000 | Loss: 0.00002140
Iteration 5/1000 | Loss: 0.00001996
Iteration 6/1000 | Loss: 0.00001881
Iteration 7/1000 | Loss: 0.00001818
Iteration 8/1000 | Loss: 0.00001781
Iteration 9/1000 | Loss: 0.00001761
Iteration 10/1000 | Loss: 0.00001734
Iteration 11/1000 | Loss: 0.00001708
Iteration 12/1000 | Loss: 0.00001697
Iteration 13/1000 | Loss: 0.00001695
Iteration 14/1000 | Loss: 0.00001693
Iteration 15/1000 | Loss: 0.00001688
Iteration 16/1000 | Loss: 0.00001682
Iteration 17/1000 | Loss: 0.00001682
Iteration 18/1000 | Loss: 0.00001680
Iteration 19/1000 | Loss: 0.00001677
Iteration 20/1000 | Loss: 0.00001677
Iteration 21/1000 | Loss: 0.00001675
Iteration 22/1000 | Loss: 0.00001674
Iteration 23/1000 | Loss: 0.00001674
Iteration 24/1000 | Loss: 0.00001672
Iteration 25/1000 | Loss: 0.00001672
Iteration 26/1000 | Loss: 0.00001670
Iteration 27/1000 | Loss: 0.00001669
Iteration 28/1000 | Loss: 0.00001669
Iteration 29/1000 | Loss: 0.00001664
Iteration 30/1000 | Loss: 0.00001663
Iteration 31/1000 | Loss: 0.00001662
Iteration 32/1000 | Loss: 0.00001661
Iteration 33/1000 | Loss: 0.00001661
Iteration 34/1000 | Loss: 0.00001659
Iteration 35/1000 | Loss: 0.00001659
Iteration 36/1000 | Loss: 0.00001658
Iteration 37/1000 | Loss: 0.00001657
Iteration 38/1000 | Loss: 0.00001657
Iteration 39/1000 | Loss: 0.00001656
Iteration 40/1000 | Loss: 0.00001656
Iteration 41/1000 | Loss: 0.00001656
Iteration 42/1000 | Loss: 0.00001655
Iteration 43/1000 | Loss: 0.00001655
Iteration 44/1000 | Loss: 0.00001654
Iteration 45/1000 | Loss: 0.00001654
Iteration 46/1000 | Loss: 0.00001653
Iteration 47/1000 | Loss: 0.00001653
Iteration 48/1000 | Loss: 0.00001653
Iteration 49/1000 | Loss: 0.00001653
Iteration 50/1000 | Loss: 0.00001652
Iteration 51/1000 | Loss: 0.00001651
Iteration 52/1000 | Loss: 0.00001651
Iteration 53/1000 | Loss: 0.00001651
Iteration 54/1000 | Loss: 0.00001650
Iteration 55/1000 | Loss: 0.00001650
Iteration 56/1000 | Loss: 0.00001650
Iteration 57/1000 | Loss: 0.00001649
Iteration 58/1000 | Loss: 0.00001649
Iteration 59/1000 | Loss: 0.00001649
Iteration 60/1000 | Loss: 0.00001649
Iteration 61/1000 | Loss: 0.00001648
Iteration 62/1000 | Loss: 0.00001648
Iteration 63/1000 | Loss: 0.00001648
Iteration 64/1000 | Loss: 0.00001647
Iteration 65/1000 | Loss: 0.00001647
Iteration 66/1000 | Loss: 0.00001647
Iteration 67/1000 | Loss: 0.00001647
Iteration 68/1000 | Loss: 0.00001646
Iteration 69/1000 | Loss: 0.00001646
Iteration 70/1000 | Loss: 0.00001646
Iteration 71/1000 | Loss: 0.00001646
Iteration 72/1000 | Loss: 0.00001646
Iteration 73/1000 | Loss: 0.00001645
Iteration 74/1000 | Loss: 0.00001645
Iteration 75/1000 | Loss: 0.00001645
Iteration 76/1000 | Loss: 0.00001645
Iteration 77/1000 | Loss: 0.00001645
Iteration 78/1000 | Loss: 0.00001645
Iteration 79/1000 | Loss: 0.00001644
Iteration 80/1000 | Loss: 0.00001644
Iteration 81/1000 | Loss: 0.00001644
Iteration 82/1000 | Loss: 0.00001644
Iteration 83/1000 | Loss: 0.00001643
Iteration 84/1000 | Loss: 0.00001643
Iteration 85/1000 | Loss: 0.00001643
Iteration 86/1000 | Loss: 0.00001643
Iteration 87/1000 | Loss: 0.00001643
Iteration 88/1000 | Loss: 0.00001643
Iteration 89/1000 | Loss: 0.00001642
Iteration 90/1000 | Loss: 0.00001642
Iteration 91/1000 | Loss: 0.00001642
Iteration 92/1000 | Loss: 0.00001642
Iteration 93/1000 | Loss: 0.00001642
Iteration 94/1000 | Loss: 0.00001642
Iteration 95/1000 | Loss: 0.00001642
Iteration 96/1000 | Loss: 0.00001642
Iteration 97/1000 | Loss: 0.00001642
Iteration 98/1000 | Loss: 0.00001642
Iteration 99/1000 | Loss: 0.00001642
Iteration 100/1000 | Loss: 0.00001642
Iteration 101/1000 | Loss: 0.00001642
Iteration 102/1000 | Loss: 0.00001642
Iteration 103/1000 | Loss: 0.00001642
Iteration 104/1000 | Loss: 0.00001642
Iteration 105/1000 | Loss: 0.00001642
Iteration 106/1000 | Loss: 0.00001642
Iteration 107/1000 | Loss: 0.00001642
Iteration 108/1000 | Loss: 0.00001642
Iteration 109/1000 | Loss: 0.00001642
Iteration 110/1000 | Loss: 0.00001642
Iteration 111/1000 | Loss: 0.00001642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.6418451195931993e-05, 1.6418451195931993e-05, 1.6418451195931993e-05, 1.6418451195931993e-05, 1.6418451195931993e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6418451195931993e-05

Optimization complete. Final v2v error: 3.4153482913970947 mm

Highest mean error: 3.9012696743011475 mm for frame 0

Lowest mean error: 3.1552650928497314 mm for frame 115

Saving results

Total time: 80.1829001903534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793477
Iteration 2/25 | Loss: 0.00151564
Iteration 3/25 | Loss: 0.00102363
Iteration 4/25 | Loss: 0.00109867
Iteration 5/25 | Loss: 0.00089381
Iteration 6/25 | Loss: 0.00086034
Iteration 7/25 | Loss: 0.00085120
Iteration 8/25 | Loss: 0.00084839
Iteration 9/25 | Loss: 0.00084793
Iteration 10/25 | Loss: 0.00084788
Iteration 11/25 | Loss: 0.00084788
Iteration 12/25 | Loss: 0.00084787
Iteration 13/25 | Loss: 0.00084787
Iteration 14/25 | Loss: 0.00084787
Iteration 15/25 | Loss: 0.00084787
Iteration 16/25 | Loss: 0.00084787
Iteration 17/25 | Loss: 0.00084786
Iteration 18/25 | Loss: 0.00084785
Iteration 19/25 | Loss: 0.00084782
Iteration 20/25 | Loss: 0.00084782
Iteration 21/25 | Loss: 0.00084782
Iteration 22/25 | Loss: 0.00084782
Iteration 23/25 | Loss: 0.00084782
Iteration 24/25 | Loss: 0.00084782
Iteration 25/25 | Loss: 0.00084781

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.02703977
Iteration 2/25 | Loss: 0.00141729
Iteration 3/25 | Loss: 0.00141728
Iteration 4/25 | Loss: 0.00141728
Iteration 5/25 | Loss: 0.00141728
Iteration 6/25 | Loss: 0.00141728
Iteration 7/25 | Loss: 0.00141728
Iteration 8/25 | Loss: 0.00141728
Iteration 9/25 | Loss: 0.00141728
Iteration 10/25 | Loss: 0.00141728
Iteration 11/25 | Loss: 0.00141728
Iteration 12/25 | Loss: 0.00141728
Iteration 13/25 | Loss: 0.00141728
Iteration 14/25 | Loss: 0.00141728
Iteration 15/25 | Loss: 0.00141728
Iteration 16/25 | Loss: 0.00141728
Iteration 17/25 | Loss: 0.00141728
Iteration 18/25 | Loss: 0.00141728
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014172776136547327, 0.0014172776136547327, 0.0014172776136547327, 0.0014172776136547327, 0.0014172776136547327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014172776136547327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141728
Iteration 2/1000 | Loss: 0.00004614
Iteration 3/1000 | Loss: 0.00003168
Iteration 4/1000 | Loss: 0.00002713
Iteration 5/1000 | Loss: 0.00002505
Iteration 6/1000 | Loss: 0.00002353
Iteration 7/1000 | Loss: 0.00002276
Iteration 8/1000 | Loss: 0.00002212
Iteration 9/1000 | Loss: 0.00002170
Iteration 10/1000 | Loss: 0.00002133
Iteration 11/1000 | Loss: 0.00002103
Iteration 12/1000 | Loss: 0.00002084
Iteration 13/1000 | Loss: 0.00002065
Iteration 14/1000 | Loss: 0.00002046
Iteration 15/1000 | Loss: 0.00002045
Iteration 16/1000 | Loss: 0.00002041
Iteration 17/1000 | Loss: 0.00002038
Iteration 18/1000 | Loss: 0.00002035
Iteration 19/1000 | Loss: 0.00002033
Iteration 20/1000 | Loss: 0.00002032
Iteration 21/1000 | Loss: 0.00002032
Iteration 22/1000 | Loss: 0.00002031
Iteration 23/1000 | Loss: 0.00002031
Iteration 24/1000 | Loss: 0.00002030
Iteration 25/1000 | Loss: 0.00002030
Iteration 26/1000 | Loss: 0.00002029
Iteration 27/1000 | Loss: 0.00002029
Iteration 28/1000 | Loss: 0.00002029
Iteration 29/1000 | Loss: 0.00002029
Iteration 30/1000 | Loss: 0.00002029
Iteration 31/1000 | Loss: 0.00002028
Iteration 32/1000 | Loss: 0.00002028
Iteration 33/1000 | Loss: 0.00002027
Iteration 34/1000 | Loss: 0.00002027
Iteration 35/1000 | Loss: 0.00002027
Iteration 36/1000 | Loss: 0.00002026
Iteration 37/1000 | Loss: 0.00002026
Iteration 38/1000 | Loss: 0.00002026
Iteration 39/1000 | Loss: 0.00002025
Iteration 40/1000 | Loss: 0.00002025
Iteration 41/1000 | Loss: 0.00002025
Iteration 42/1000 | Loss: 0.00002024
Iteration 43/1000 | Loss: 0.00002024
Iteration 44/1000 | Loss: 0.00002024
Iteration 45/1000 | Loss: 0.00002024
Iteration 46/1000 | Loss: 0.00002024
Iteration 47/1000 | Loss: 0.00002024
Iteration 48/1000 | Loss: 0.00002024
Iteration 49/1000 | Loss: 0.00002024
Iteration 50/1000 | Loss: 0.00002023
Iteration 51/1000 | Loss: 0.00002023
Iteration 52/1000 | Loss: 0.00002023
Iteration 53/1000 | Loss: 0.00002022
Iteration 54/1000 | Loss: 0.00002022
Iteration 55/1000 | Loss: 0.00002022
Iteration 56/1000 | Loss: 0.00002022
Iteration 57/1000 | Loss: 0.00002022
Iteration 58/1000 | Loss: 0.00002021
Iteration 59/1000 | Loss: 0.00002021
Iteration 60/1000 | Loss: 0.00002021
Iteration 61/1000 | Loss: 0.00002020
Iteration 62/1000 | Loss: 0.00002020
Iteration 63/1000 | Loss: 0.00002020
Iteration 64/1000 | Loss: 0.00002020
Iteration 65/1000 | Loss: 0.00002020
Iteration 66/1000 | Loss: 0.00002020
Iteration 67/1000 | Loss: 0.00002019
Iteration 68/1000 | Loss: 0.00002019
Iteration 69/1000 | Loss: 0.00002019
Iteration 70/1000 | Loss: 0.00002019
Iteration 71/1000 | Loss: 0.00002019
Iteration 72/1000 | Loss: 0.00002019
Iteration 73/1000 | Loss: 0.00002019
Iteration 74/1000 | Loss: 0.00002018
Iteration 75/1000 | Loss: 0.00002018
Iteration 76/1000 | Loss: 0.00002018
Iteration 77/1000 | Loss: 0.00002018
Iteration 78/1000 | Loss: 0.00002017
Iteration 79/1000 | Loss: 0.00002017
Iteration 80/1000 | Loss: 0.00002017
Iteration 81/1000 | Loss: 0.00002017
Iteration 82/1000 | Loss: 0.00002017
Iteration 83/1000 | Loss: 0.00002016
Iteration 84/1000 | Loss: 0.00002016
Iteration 85/1000 | Loss: 0.00002016
Iteration 86/1000 | Loss: 0.00002016
Iteration 87/1000 | Loss: 0.00002016
Iteration 88/1000 | Loss: 0.00002016
Iteration 89/1000 | Loss: 0.00002016
Iteration 90/1000 | Loss: 0.00002016
Iteration 91/1000 | Loss: 0.00002015
Iteration 92/1000 | Loss: 0.00002015
Iteration 93/1000 | Loss: 0.00002015
Iteration 94/1000 | Loss: 0.00002015
Iteration 95/1000 | Loss: 0.00002014
Iteration 96/1000 | Loss: 0.00002014
Iteration 97/1000 | Loss: 0.00002014
Iteration 98/1000 | Loss: 0.00002014
Iteration 99/1000 | Loss: 0.00002014
Iteration 100/1000 | Loss: 0.00002014
Iteration 101/1000 | Loss: 0.00002014
Iteration 102/1000 | Loss: 0.00002014
Iteration 103/1000 | Loss: 0.00002014
Iteration 104/1000 | Loss: 0.00002014
Iteration 105/1000 | Loss: 0.00002014
Iteration 106/1000 | Loss: 0.00002014
Iteration 107/1000 | Loss: 0.00002013
Iteration 108/1000 | Loss: 0.00002013
Iteration 109/1000 | Loss: 0.00002013
Iteration 110/1000 | Loss: 0.00002013
Iteration 111/1000 | Loss: 0.00002013
Iteration 112/1000 | Loss: 0.00002013
Iteration 113/1000 | Loss: 0.00002013
Iteration 114/1000 | Loss: 0.00002013
Iteration 115/1000 | Loss: 0.00002013
Iteration 116/1000 | Loss: 0.00002013
Iteration 117/1000 | Loss: 0.00002013
Iteration 118/1000 | Loss: 0.00002012
Iteration 119/1000 | Loss: 0.00002012
Iteration 120/1000 | Loss: 0.00002012
Iteration 121/1000 | Loss: 0.00002012
Iteration 122/1000 | Loss: 0.00002012
Iteration 123/1000 | Loss: 0.00002012
Iteration 124/1000 | Loss: 0.00002012
Iteration 125/1000 | Loss: 0.00002012
Iteration 126/1000 | Loss: 0.00002012
Iteration 127/1000 | Loss: 0.00002012
Iteration 128/1000 | Loss: 0.00002012
Iteration 129/1000 | Loss: 0.00002012
Iteration 130/1000 | Loss: 0.00002012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.0123174181208014e-05, 2.0123174181208014e-05, 2.0123174181208014e-05, 2.0123174181208014e-05, 2.0123174181208014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0123174181208014e-05

Optimization complete. Final v2v error: 3.658963441848755 mm

Highest mean error: 4.676799774169922 mm for frame 26

Lowest mean error: 2.9190056324005127 mm for frame 47

Saving results

Total time: 84.8293240070343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782483
Iteration 2/25 | Loss: 0.00129647
Iteration 3/25 | Loss: 0.00102926
Iteration 4/25 | Loss: 0.00100050
Iteration 5/25 | Loss: 0.00099209
Iteration 6/25 | Loss: 0.00099003
Iteration 7/25 | Loss: 0.00098997
Iteration 8/25 | Loss: 0.00098997
Iteration 9/25 | Loss: 0.00098997
Iteration 10/25 | Loss: 0.00098997
Iteration 11/25 | Loss: 0.00098997
Iteration 12/25 | Loss: 0.00098997
Iteration 13/25 | Loss: 0.00098997
Iteration 14/25 | Loss: 0.00098997
Iteration 15/25 | Loss: 0.00098997
Iteration 16/25 | Loss: 0.00098997
Iteration 17/25 | Loss: 0.00098997
Iteration 18/25 | Loss: 0.00098997
Iteration 19/25 | Loss: 0.00098997
Iteration 20/25 | Loss: 0.00098997
Iteration 21/25 | Loss: 0.00098997
Iteration 22/25 | Loss: 0.00098997
Iteration 23/25 | Loss: 0.00098997
Iteration 24/25 | Loss: 0.00098997
Iteration 25/25 | Loss: 0.00098997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.52181435
Iteration 2/25 | Loss: 0.00108863
Iteration 3/25 | Loss: 0.00108863
Iteration 4/25 | Loss: 0.00108863
Iteration 5/25 | Loss: 0.00108863
Iteration 6/25 | Loss: 0.00108863
Iteration 7/25 | Loss: 0.00108863
Iteration 8/25 | Loss: 0.00108863
Iteration 9/25 | Loss: 0.00108863
Iteration 10/25 | Loss: 0.00108863
Iteration 11/25 | Loss: 0.00108863
Iteration 12/25 | Loss: 0.00108863
Iteration 13/25 | Loss: 0.00108863
Iteration 14/25 | Loss: 0.00108863
Iteration 15/25 | Loss: 0.00108863
Iteration 16/25 | Loss: 0.00108863
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001088631572201848, 0.001088631572201848, 0.001088631572201848, 0.001088631572201848, 0.001088631572201848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001088631572201848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108863
Iteration 2/1000 | Loss: 0.00005589
Iteration 3/1000 | Loss: 0.00003399
Iteration 4/1000 | Loss: 0.00003074
Iteration 5/1000 | Loss: 0.00002943
Iteration 6/1000 | Loss: 0.00002849
Iteration 7/1000 | Loss: 0.00002795
Iteration 8/1000 | Loss: 0.00002764
Iteration 9/1000 | Loss: 0.00002732
Iteration 10/1000 | Loss: 0.00002707
Iteration 11/1000 | Loss: 0.00002691
Iteration 12/1000 | Loss: 0.00002687
Iteration 13/1000 | Loss: 0.00002672
Iteration 14/1000 | Loss: 0.00002671
Iteration 15/1000 | Loss: 0.00002671
Iteration 16/1000 | Loss: 0.00002671
Iteration 17/1000 | Loss: 0.00002669
Iteration 18/1000 | Loss: 0.00002665
Iteration 19/1000 | Loss: 0.00002665
Iteration 20/1000 | Loss: 0.00002660
Iteration 21/1000 | Loss: 0.00002658
Iteration 22/1000 | Loss: 0.00002657
Iteration 23/1000 | Loss: 0.00002657
Iteration 24/1000 | Loss: 0.00002652
Iteration 25/1000 | Loss: 0.00002647
Iteration 26/1000 | Loss: 0.00002646
Iteration 27/1000 | Loss: 0.00002644
Iteration 28/1000 | Loss: 0.00002642
Iteration 29/1000 | Loss: 0.00002642
Iteration 30/1000 | Loss: 0.00002642
Iteration 31/1000 | Loss: 0.00002641
Iteration 32/1000 | Loss: 0.00002641
Iteration 33/1000 | Loss: 0.00002640
Iteration 34/1000 | Loss: 0.00002640
Iteration 35/1000 | Loss: 0.00002639
Iteration 36/1000 | Loss: 0.00002639
Iteration 37/1000 | Loss: 0.00002639
Iteration 38/1000 | Loss: 0.00002639
Iteration 39/1000 | Loss: 0.00002639
Iteration 40/1000 | Loss: 0.00002639
Iteration 41/1000 | Loss: 0.00002639
Iteration 42/1000 | Loss: 0.00002638
Iteration 43/1000 | Loss: 0.00002638
Iteration 44/1000 | Loss: 0.00002638
Iteration 45/1000 | Loss: 0.00002638
Iteration 46/1000 | Loss: 0.00002638
Iteration 47/1000 | Loss: 0.00002638
Iteration 48/1000 | Loss: 0.00002637
Iteration 49/1000 | Loss: 0.00002637
Iteration 50/1000 | Loss: 0.00002637
Iteration 51/1000 | Loss: 0.00002637
Iteration 52/1000 | Loss: 0.00002637
Iteration 53/1000 | Loss: 0.00002637
Iteration 54/1000 | Loss: 0.00002637
Iteration 55/1000 | Loss: 0.00002637
Iteration 56/1000 | Loss: 0.00002637
Iteration 57/1000 | Loss: 0.00002637
Iteration 58/1000 | Loss: 0.00002637
Iteration 59/1000 | Loss: 0.00002636
Iteration 60/1000 | Loss: 0.00002636
Iteration 61/1000 | Loss: 0.00002636
Iteration 62/1000 | Loss: 0.00002636
Iteration 63/1000 | Loss: 0.00002636
Iteration 64/1000 | Loss: 0.00002636
Iteration 65/1000 | Loss: 0.00002636
Iteration 66/1000 | Loss: 0.00002636
Iteration 67/1000 | Loss: 0.00002636
Iteration 68/1000 | Loss: 0.00002636
Iteration 69/1000 | Loss: 0.00002635
Iteration 70/1000 | Loss: 0.00002635
Iteration 71/1000 | Loss: 0.00002635
Iteration 72/1000 | Loss: 0.00002635
Iteration 73/1000 | Loss: 0.00002635
Iteration 74/1000 | Loss: 0.00002635
Iteration 75/1000 | Loss: 0.00002635
Iteration 76/1000 | Loss: 0.00002635
Iteration 77/1000 | Loss: 0.00002634
Iteration 78/1000 | Loss: 0.00002634
Iteration 79/1000 | Loss: 0.00002634
Iteration 80/1000 | Loss: 0.00002634
Iteration 81/1000 | Loss: 0.00002634
Iteration 82/1000 | Loss: 0.00002634
Iteration 83/1000 | Loss: 0.00002634
Iteration 84/1000 | Loss: 0.00002634
Iteration 85/1000 | Loss: 0.00002634
Iteration 86/1000 | Loss: 0.00002633
Iteration 87/1000 | Loss: 0.00002633
Iteration 88/1000 | Loss: 0.00002633
Iteration 89/1000 | Loss: 0.00002633
Iteration 90/1000 | Loss: 0.00002633
Iteration 91/1000 | Loss: 0.00002632
Iteration 92/1000 | Loss: 0.00002632
Iteration 93/1000 | Loss: 0.00002632
Iteration 94/1000 | Loss: 0.00002632
Iteration 95/1000 | Loss: 0.00002632
Iteration 96/1000 | Loss: 0.00002632
Iteration 97/1000 | Loss: 0.00002632
Iteration 98/1000 | Loss: 0.00002632
Iteration 99/1000 | Loss: 0.00002632
Iteration 100/1000 | Loss: 0.00002632
Iteration 101/1000 | Loss: 0.00002632
Iteration 102/1000 | Loss: 0.00002631
Iteration 103/1000 | Loss: 0.00002631
Iteration 104/1000 | Loss: 0.00002631
Iteration 105/1000 | Loss: 0.00002631
Iteration 106/1000 | Loss: 0.00002631
Iteration 107/1000 | Loss: 0.00002631
Iteration 108/1000 | Loss: 0.00002631
Iteration 109/1000 | Loss: 0.00002631
Iteration 110/1000 | Loss: 0.00002631
Iteration 111/1000 | Loss: 0.00002631
Iteration 112/1000 | Loss: 0.00002631
Iteration 113/1000 | Loss: 0.00002631
Iteration 114/1000 | Loss: 0.00002630
Iteration 115/1000 | Loss: 0.00002630
Iteration 116/1000 | Loss: 0.00002630
Iteration 117/1000 | Loss: 0.00002630
Iteration 118/1000 | Loss: 0.00002630
Iteration 119/1000 | Loss: 0.00002630
Iteration 120/1000 | Loss: 0.00002630
Iteration 121/1000 | Loss: 0.00002630
Iteration 122/1000 | Loss: 0.00002630
Iteration 123/1000 | Loss: 0.00002630
Iteration 124/1000 | Loss: 0.00002630
Iteration 125/1000 | Loss: 0.00002630
Iteration 126/1000 | Loss: 0.00002630
Iteration 127/1000 | Loss: 0.00002630
Iteration 128/1000 | Loss: 0.00002630
Iteration 129/1000 | Loss: 0.00002630
Iteration 130/1000 | Loss: 0.00002630
Iteration 131/1000 | Loss: 0.00002630
Iteration 132/1000 | Loss: 0.00002630
Iteration 133/1000 | Loss: 0.00002630
Iteration 134/1000 | Loss: 0.00002630
Iteration 135/1000 | Loss: 0.00002630
Iteration 136/1000 | Loss: 0.00002630
Iteration 137/1000 | Loss: 0.00002630
Iteration 138/1000 | Loss: 0.00002630
Iteration 139/1000 | Loss: 0.00002630
Iteration 140/1000 | Loss: 0.00002630
Iteration 141/1000 | Loss: 0.00002630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.6302781407139264e-05, 2.6302781407139264e-05, 2.6302781407139264e-05, 2.6302781407139264e-05, 2.6302781407139264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6302781407139264e-05

Optimization complete. Final v2v error: 4.144398212432861 mm

Highest mean error: 4.793475151062012 mm for frame 94

Lowest mean error: 3.4321556091308594 mm for frame 82

Saving results

Total time: 39.4504714012146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099784
Iteration 2/25 | Loss: 0.00111489
Iteration 3/25 | Loss: 0.00083874
Iteration 4/25 | Loss: 0.00078994
Iteration 5/25 | Loss: 0.00078837
Iteration 6/25 | Loss: 0.00078396
Iteration 7/25 | Loss: 0.00077424
Iteration 8/25 | Loss: 0.00077232
Iteration 9/25 | Loss: 0.00077152
Iteration 10/25 | Loss: 0.00077141
Iteration 11/25 | Loss: 0.00077139
Iteration 12/25 | Loss: 0.00077139
Iteration 13/25 | Loss: 0.00077139
Iteration 14/25 | Loss: 0.00077139
Iteration 15/25 | Loss: 0.00077138
Iteration 16/25 | Loss: 0.00077138
Iteration 17/25 | Loss: 0.00077138
Iteration 18/25 | Loss: 0.00077138
Iteration 19/25 | Loss: 0.00077138
Iteration 20/25 | Loss: 0.00077138
Iteration 21/25 | Loss: 0.00077138
Iteration 22/25 | Loss: 0.00077138
Iteration 23/25 | Loss: 0.00077138
Iteration 24/25 | Loss: 0.00077138
Iteration 25/25 | Loss: 0.00077138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007713801460340619, 0.0007713801460340619, 0.0007713801460340619, 0.0007713801460340619, 0.0007713801460340619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007713801460340619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.55819035
Iteration 2/25 | Loss: 0.00120637
Iteration 3/25 | Loss: 0.00120637
Iteration 4/25 | Loss: 0.00120637
Iteration 5/25 | Loss: 0.00120637
Iteration 6/25 | Loss: 0.00120637
Iteration 7/25 | Loss: 0.00120637
Iteration 8/25 | Loss: 0.00120637
Iteration 9/25 | Loss: 0.00120637
Iteration 10/25 | Loss: 0.00120637
Iteration 11/25 | Loss: 0.00120637
Iteration 12/25 | Loss: 0.00120637
Iteration 13/25 | Loss: 0.00120637
Iteration 14/25 | Loss: 0.00120637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012063668109476566, 0.0012063668109476566, 0.0012063668109476566, 0.0012063668109476566, 0.0012063668109476566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012063668109476566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120637
Iteration 2/1000 | Loss: 0.00002813
Iteration 3/1000 | Loss: 0.00001896
Iteration 4/1000 | Loss: 0.00001640
Iteration 5/1000 | Loss: 0.00001483
Iteration 6/1000 | Loss: 0.00001399
Iteration 7/1000 | Loss: 0.00001368
Iteration 8/1000 | Loss: 0.00001334
Iteration 9/1000 | Loss: 0.00001304
Iteration 10/1000 | Loss: 0.00001278
Iteration 11/1000 | Loss: 0.00001264
Iteration 12/1000 | Loss: 0.00001263
Iteration 13/1000 | Loss: 0.00001258
Iteration 14/1000 | Loss: 0.00001258
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001248
Iteration 21/1000 | Loss: 0.00001248
Iteration 22/1000 | Loss: 0.00001246
Iteration 23/1000 | Loss: 0.00001246
Iteration 24/1000 | Loss: 0.00001245
Iteration 25/1000 | Loss: 0.00001245
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001244
Iteration 28/1000 | Loss: 0.00001244
Iteration 29/1000 | Loss: 0.00001243
Iteration 30/1000 | Loss: 0.00001241
Iteration 31/1000 | Loss: 0.00001241
Iteration 32/1000 | Loss: 0.00001241
Iteration 33/1000 | Loss: 0.00001241
Iteration 34/1000 | Loss: 0.00001241
Iteration 35/1000 | Loss: 0.00001241
Iteration 36/1000 | Loss: 0.00001241
Iteration 37/1000 | Loss: 0.00001241
Iteration 38/1000 | Loss: 0.00001240
Iteration 39/1000 | Loss: 0.00001240
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001240
Iteration 43/1000 | Loss: 0.00001240
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001238
Iteration 48/1000 | Loss: 0.00001238
Iteration 49/1000 | Loss: 0.00001237
Iteration 50/1000 | Loss: 0.00001237
Iteration 51/1000 | Loss: 0.00001237
Iteration 52/1000 | Loss: 0.00001237
Iteration 53/1000 | Loss: 0.00001237
Iteration 54/1000 | Loss: 0.00001237
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001237
Iteration 57/1000 | Loss: 0.00001237
Iteration 58/1000 | Loss: 0.00001236
Iteration 59/1000 | Loss: 0.00001236
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001236
Iteration 63/1000 | Loss: 0.00001236
Iteration 64/1000 | Loss: 0.00001236
Iteration 65/1000 | Loss: 0.00001235
Iteration 66/1000 | Loss: 0.00001235
Iteration 67/1000 | Loss: 0.00001235
Iteration 68/1000 | Loss: 0.00001235
Iteration 69/1000 | Loss: 0.00001234
Iteration 70/1000 | Loss: 0.00001234
Iteration 71/1000 | Loss: 0.00001234
Iteration 72/1000 | Loss: 0.00001233
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001233
Iteration 75/1000 | Loss: 0.00001233
Iteration 76/1000 | Loss: 0.00001232
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001231
Iteration 80/1000 | Loss: 0.00001231
Iteration 81/1000 | Loss: 0.00001231
Iteration 82/1000 | Loss: 0.00001231
Iteration 83/1000 | Loss: 0.00001231
Iteration 84/1000 | Loss: 0.00001231
Iteration 85/1000 | Loss: 0.00001231
Iteration 86/1000 | Loss: 0.00001230
Iteration 87/1000 | Loss: 0.00001230
Iteration 88/1000 | Loss: 0.00001230
Iteration 89/1000 | Loss: 0.00001229
Iteration 90/1000 | Loss: 0.00001229
Iteration 91/1000 | Loss: 0.00001229
Iteration 92/1000 | Loss: 0.00001229
Iteration 93/1000 | Loss: 0.00001229
Iteration 94/1000 | Loss: 0.00001229
Iteration 95/1000 | Loss: 0.00001229
Iteration 96/1000 | Loss: 0.00001229
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001228
Iteration 100/1000 | Loss: 0.00001228
Iteration 101/1000 | Loss: 0.00001228
Iteration 102/1000 | Loss: 0.00001228
Iteration 103/1000 | Loss: 0.00001227
Iteration 104/1000 | Loss: 0.00001227
Iteration 105/1000 | Loss: 0.00001227
Iteration 106/1000 | Loss: 0.00001227
Iteration 107/1000 | Loss: 0.00001227
Iteration 108/1000 | Loss: 0.00001227
Iteration 109/1000 | Loss: 0.00001226
Iteration 110/1000 | Loss: 0.00001226
Iteration 111/1000 | Loss: 0.00001226
Iteration 112/1000 | Loss: 0.00001226
Iteration 113/1000 | Loss: 0.00001226
Iteration 114/1000 | Loss: 0.00001226
Iteration 115/1000 | Loss: 0.00001226
Iteration 116/1000 | Loss: 0.00001226
Iteration 117/1000 | Loss: 0.00001226
Iteration 118/1000 | Loss: 0.00001226
Iteration 119/1000 | Loss: 0.00001225
Iteration 120/1000 | Loss: 0.00001225
Iteration 121/1000 | Loss: 0.00001225
Iteration 122/1000 | Loss: 0.00001225
Iteration 123/1000 | Loss: 0.00001225
Iteration 124/1000 | Loss: 0.00001225
Iteration 125/1000 | Loss: 0.00001225
Iteration 126/1000 | Loss: 0.00001225
Iteration 127/1000 | Loss: 0.00001225
Iteration 128/1000 | Loss: 0.00001225
Iteration 129/1000 | Loss: 0.00001225
Iteration 130/1000 | Loss: 0.00001224
Iteration 131/1000 | Loss: 0.00001224
Iteration 132/1000 | Loss: 0.00001224
Iteration 133/1000 | Loss: 0.00001224
Iteration 134/1000 | Loss: 0.00001224
Iteration 135/1000 | Loss: 0.00001224
Iteration 136/1000 | Loss: 0.00001224
Iteration 137/1000 | Loss: 0.00001224
Iteration 138/1000 | Loss: 0.00001224
Iteration 139/1000 | Loss: 0.00001224
Iteration 140/1000 | Loss: 0.00001224
Iteration 141/1000 | Loss: 0.00001224
Iteration 142/1000 | Loss: 0.00001223
Iteration 143/1000 | Loss: 0.00001223
Iteration 144/1000 | Loss: 0.00001223
Iteration 145/1000 | Loss: 0.00001223
Iteration 146/1000 | Loss: 0.00001223
Iteration 147/1000 | Loss: 0.00001223
Iteration 148/1000 | Loss: 0.00001223
Iteration 149/1000 | Loss: 0.00001223
Iteration 150/1000 | Loss: 0.00001223
Iteration 151/1000 | Loss: 0.00001223
Iteration 152/1000 | Loss: 0.00001222
Iteration 153/1000 | Loss: 0.00001222
Iteration 154/1000 | Loss: 0.00001222
Iteration 155/1000 | Loss: 0.00001222
Iteration 156/1000 | Loss: 0.00001222
Iteration 157/1000 | Loss: 0.00001222
Iteration 158/1000 | Loss: 0.00001222
Iteration 159/1000 | Loss: 0.00001222
Iteration 160/1000 | Loss: 0.00001222
Iteration 161/1000 | Loss: 0.00001222
Iteration 162/1000 | Loss: 0.00001222
Iteration 163/1000 | Loss: 0.00001222
Iteration 164/1000 | Loss: 0.00001222
Iteration 165/1000 | Loss: 0.00001222
Iteration 166/1000 | Loss: 0.00001222
Iteration 167/1000 | Loss: 0.00001222
Iteration 168/1000 | Loss: 0.00001222
Iteration 169/1000 | Loss: 0.00001222
Iteration 170/1000 | Loss: 0.00001222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.2217134099046234e-05, 1.2217134099046234e-05, 1.2217134099046234e-05, 1.2217134099046234e-05, 1.2217134099046234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2217134099046234e-05

Optimization complete. Final v2v error: 2.9649932384490967 mm

Highest mean error: 3.15026593208313 mm for frame 124

Lowest mean error: 2.686194896697998 mm for frame 84

Saving results

Total time: 52.18084096908569
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049778
Iteration 2/25 | Loss: 0.00243058
Iteration 3/25 | Loss: 0.00174005
Iteration 4/25 | Loss: 0.00144943
Iteration 5/25 | Loss: 0.00134589
Iteration 6/25 | Loss: 0.00101234
Iteration 7/25 | Loss: 0.00095692
Iteration 8/25 | Loss: 0.00094825
Iteration 9/25 | Loss: 0.00094220
Iteration 10/25 | Loss: 0.00093907
Iteration 11/25 | Loss: 0.00093711
Iteration 12/25 | Loss: 0.00093662
Iteration 13/25 | Loss: 0.00093658
Iteration 14/25 | Loss: 0.00093658
Iteration 15/25 | Loss: 0.00093658
Iteration 16/25 | Loss: 0.00093657
Iteration 17/25 | Loss: 0.00093657
Iteration 18/25 | Loss: 0.00093657
Iteration 19/25 | Loss: 0.00093657
Iteration 20/25 | Loss: 0.00093657
Iteration 21/25 | Loss: 0.00093657
Iteration 22/25 | Loss: 0.00093657
Iteration 23/25 | Loss: 0.00093657
Iteration 24/25 | Loss: 0.00093657
Iteration 25/25 | Loss: 0.00093657

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58651185
Iteration 2/25 | Loss: 0.00168269
Iteration 3/25 | Loss: 0.00168269
Iteration 4/25 | Loss: 0.00168269
Iteration 5/25 | Loss: 0.00168269
Iteration 6/25 | Loss: 0.00168269
Iteration 7/25 | Loss: 0.00168268
Iteration 8/25 | Loss: 0.00168268
Iteration 9/25 | Loss: 0.00168268
Iteration 10/25 | Loss: 0.00168268
Iteration 11/25 | Loss: 0.00168268
Iteration 12/25 | Loss: 0.00168268
Iteration 13/25 | Loss: 0.00168268
Iteration 14/25 | Loss: 0.00168268
Iteration 15/25 | Loss: 0.00168268
Iteration 16/25 | Loss: 0.00168268
Iteration 17/25 | Loss: 0.00168268
Iteration 18/25 | Loss: 0.00168268
Iteration 19/25 | Loss: 0.00168268
Iteration 20/25 | Loss: 0.00168268
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016826840583235025, 0.0016826840583235025, 0.0016826840583235025, 0.0016826840583235025, 0.0016826840583235025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016826840583235025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168268
Iteration 2/1000 | Loss: 0.00003288
Iteration 3/1000 | Loss: 0.00002340
Iteration 4/1000 | Loss: 0.00002163
Iteration 5/1000 | Loss: 0.00002072
Iteration 6/1000 | Loss: 0.00002025
Iteration 7/1000 | Loss: 0.00002081
Iteration 8/1000 | Loss: 0.00002112
Iteration 9/1000 | Loss: 0.00002010
Iteration 10/1000 | Loss: 0.00001980
Iteration 11/1000 | Loss: 0.00001954
Iteration 12/1000 | Loss: 0.00001936
Iteration 13/1000 | Loss: 0.00001932
Iteration 14/1000 | Loss: 0.00001926
Iteration 15/1000 | Loss: 0.00001925
Iteration 16/1000 | Loss: 0.00001924
Iteration 17/1000 | Loss: 0.00001924
Iteration 18/1000 | Loss: 0.00001921
Iteration 19/1000 | Loss: 0.00001920
Iteration 20/1000 | Loss: 0.00001920
Iteration 21/1000 | Loss: 0.00001914
Iteration 22/1000 | Loss: 0.00001913
Iteration 23/1000 | Loss: 0.00001912
Iteration 24/1000 | Loss: 0.00001911
Iteration 25/1000 | Loss: 0.00001911
Iteration 26/1000 | Loss: 0.00001911
Iteration 27/1000 | Loss: 0.00001911
Iteration 28/1000 | Loss: 0.00001911
Iteration 29/1000 | Loss: 0.00001911
Iteration 30/1000 | Loss: 0.00001911
Iteration 31/1000 | Loss: 0.00001911
Iteration 32/1000 | Loss: 0.00001911
Iteration 33/1000 | Loss: 0.00001911
Iteration 34/1000 | Loss: 0.00001910
Iteration 35/1000 | Loss: 0.00001910
Iteration 36/1000 | Loss: 0.00001909
Iteration 37/1000 | Loss: 0.00001909
Iteration 38/1000 | Loss: 0.00001909
Iteration 39/1000 | Loss: 0.00001909
Iteration 40/1000 | Loss: 0.00001909
Iteration 41/1000 | Loss: 0.00001908
Iteration 42/1000 | Loss: 0.00001908
Iteration 43/1000 | Loss: 0.00001908
Iteration 44/1000 | Loss: 0.00001908
Iteration 45/1000 | Loss: 0.00001908
Iteration 46/1000 | Loss: 0.00001908
Iteration 47/1000 | Loss: 0.00001908
Iteration 48/1000 | Loss: 0.00001908
Iteration 49/1000 | Loss: 0.00001908
Iteration 50/1000 | Loss: 0.00001907
Iteration 51/1000 | Loss: 0.00001907
Iteration 52/1000 | Loss: 0.00001907
Iteration 53/1000 | Loss: 0.00001907
Iteration 54/1000 | Loss: 0.00001906
Iteration 55/1000 | Loss: 0.00001906
Iteration 56/1000 | Loss: 0.00001906
Iteration 57/1000 | Loss: 0.00001906
Iteration 58/1000 | Loss: 0.00001906
Iteration 59/1000 | Loss: 0.00001906
Iteration 60/1000 | Loss: 0.00001906
Iteration 61/1000 | Loss: 0.00001906
Iteration 62/1000 | Loss: 0.00001906
Iteration 63/1000 | Loss: 0.00001906
Iteration 64/1000 | Loss: 0.00001906
Iteration 65/1000 | Loss: 0.00001906
Iteration 66/1000 | Loss: 0.00001906
Iteration 67/1000 | Loss: 0.00001906
Iteration 68/1000 | Loss: 0.00001906
Iteration 69/1000 | Loss: 0.00001906
Iteration 70/1000 | Loss: 0.00001906
Iteration 71/1000 | Loss: 0.00001906
Iteration 72/1000 | Loss: 0.00001906
Iteration 73/1000 | Loss: 0.00001905
Iteration 74/1000 | Loss: 0.00001905
Iteration 75/1000 | Loss: 0.00001905
Iteration 76/1000 | Loss: 0.00001905
Iteration 77/1000 | Loss: 0.00001905
Iteration 78/1000 | Loss: 0.00001905
Iteration 79/1000 | Loss: 0.00001905
Iteration 80/1000 | Loss: 0.00001905
Iteration 81/1000 | Loss: 0.00001905
Iteration 82/1000 | Loss: 0.00001905
Iteration 83/1000 | Loss: 0.00001905
Iteration 84/1000 | Loss: 0.00001905
Iteration 85/1000 | Loss: 0.00001905
Iteration 86/1000 | Loss: 0.00001905
Iteration 87/1000 | Loss: 0.00001905
Iteration 88/1000 | Loss: 0.00001905
Iteration 89/1000 | Loss: 0.00001905
Iteration 90/1000 | Loss: 0.00001904
Iteration 91/1000 | Loss: 0.00001904
Iteration 92/1000 | Loss: 0.00001904
Iteration 93/1000 | Loss: 0.00001904
Iteration 94/1000 | Loss: 0.00001904
Iteration 95/1000 | Loss: 0.00001904
Iteration 96/1000 | Loss: 0.00001904
Iteration 97/1000 | Loss: 0.00001904
Iteration 98/1000 | Loss: 0.00001904
Iteration 99/1000 | Loss: 0.00001904
Iteration 100/1000 | Loss: 0.00001904
Iteration 101/1000 | Loss: 0.00001904
Iteration 102/1000 | Loss: 0.00001904
Iteration 103/1000 | Loss: 0.00001904
Iteration 104/1000 | Loss: 0.00001904
Iteration 105/1000 | Loss: 0.00001904
Iteration 106/1000 | Loss: 0.00001904
Iteration 107/1000 | Loss: 0.00001904
Iteration 108/1000 | Loss: 0.00001903
Iteration 109/1000 | Loss: 0.00001903
Iteration 110/1000 | Loss: 0.00001903
Iteration 111/1000 | Loss: 0.00001903
Iteration 112/1000 | Loss: 0.00001903
Iteration 113/1000 | Loss: 0.00001903
Iteration 114/1000 | Loss: 0.00001903
Iteration 115/1000 | Loss: 0.00001903
Iteration 116/1000 | Loss: 0.00001903
Iteration 117/1000 | Loss: 0.00001903
Iteration 118/1000 | Loss: 0.00001903
Iteration 119/1000 | Loss: 0.00001903
Iteration 120/1000 | Loss: 0.00001903
Iteration 121/1000 | Loss: 0.00001903
Iteration 122/1000 | Loss: 0.00001903
Iteration 123/1000 | Loss: 0.00001903
Iteration 124/1000 | Loss: 0.00001903
Iteration 125/1000 | Loss: 0.00001903
Iteration 126/1000 | Loss: 0.00001903
Iteration 127/1000 | Loss: 0.00001903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.9033217540709302e-05, 1.9033217540709302e-05, 1.9033217540709302e-05, 1.9033217540709302e-05, 1.9033217540709302e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9033217540709302e-05

Optimization complete. Final v2v error: 3.5690391063690186 mm

Highest mean error: 10.362699508666992 mm for frame 1

Lowest mean error: 3.341387987136841 mm for frame 112

Saving results

Total time: 142.84887790679932
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868904
Iteration 2/25 | Loss: 0.00090391
Iteration 3/25 | Loss: 0.00080765
Iteration 4/25 | Loss: 0.00078064
Iteration 5/25 | Loss: 0.00077067
Iteration 6/25 | Loss: 0.00076917
Iteration 7/25 | Loss: 0.00076904
Iteration 8/25 | Loss: 0.00076904
Iteration 9/25 | Loss: 0.00076904
Iteration 10/25 | Loss: 0.00076904
Iteration 11/25 | Loss: 0.00076904
Iteration 12/25 | Loss: 0.00076904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007690429338254035, 0.0007690429338254035, 0.0007690429338254035, 0.0007690429338254035, 0.0007690429338254035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007690429338254035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.22248173
Iteration 2/25 | Loss: 0.00128510
Iteration 3/25 | Loss: 0.00128507
Iteration 4/25 | Loss: 0.00128507
Iteration 5/25 | Loss: 0.00128507
Iteration 6/25 | Loss: 0.00128507
Iteration 7/25 | Loss: 0.00128506
Iteration 8/25 | Loss: 0.00128506
Iteration 9/25 | Loss: 0.00128506
Iteration 10/25 | Loss: 0.00128506
Iteration 11/25 | Loss: 0.00128506
Iteration 12/25 | Loss: 0.00128506
Iteration 13/25 | Loss: 0.00128506
Iteration 14/25 | Loss: 0.00128506
Iteration 15/25 | Loss: 0.00128506
Iteration 16/25 | Loss: 0.00128506
Iteration 17/25 | Loss: 0.00128506
Iteration 18/25 | Loss: 0.00128506
Iteration 19/25 | Loss: 0.00128506
Iteration 20/25 | Loss: 0.00128506
Iteration 21/25 | Loss: 0.00128506
Iteration 22/25 | Loss: 0.00128506
Iteration 23/25 | Loss: 0.00128506
Iteration 24/25 | Loss: 0.00128506
Iteration 25/25 | Loss: 0.00128506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128506
Iteration 2/1000 | Loss: 0.00003366
Iteration 3/1000 | Loss: 0.00002279
Iteration 4/1000 | Loss: 0.00002028
Iteration 5/1000 | Loss: 0.00001901
Iteration 6/1000 | Loss: 0.00001838
Iteration 7/1000 | Loss: 0.00001778
Iteration 8/1000 | Loss: 0.00001748
Iteration 9/1000 | Loss: 0.00001716
Iteration 10/1000 | Loss: 0.00001715
Iteration 11/1000 | Loss: 0.00001712
Iteration 12/1000 | Loss: 0.00001696
Iteration 13/1000 | Loss: 0.00001693
Iteration 14/1000 | Loss: 0.00001686
Iteration 15/1000 | Loss: 0.00001682
Iteration 16/1000 | Loss: 0.00001682
Iteration 17/1000 | Loss: 0.00001681
Iteration 18/1000 | Loss: 0.00001677
Iteration 19/1000 | Loss: 0.00001676
Iteration 20/1000 | Loss: 0.00001675
Iteration 21/1000 | Loss: 0.00001674
Iteration 22/1000 | Loss: 0.00001674
Iteration 23/1000 | Loss: 0.00001673
Iteration 24/1000 | Loss: 0.00001673
Iteration 25/1000 | Loss: 0.00001673
Iteration 26/1000 | Loss: 0.00001672
Iteration 27/1000 | Loss: 0.00001671
Iteration 28/1000 | Loss: 0.00001671
Iteration 29/1000 | Loss: 0.00001671
Iteration 30/1000 | Loss: 0.00001670
Iteration 31/1000 | Loss: 0.00001670
Iteration 32/1000 | Loss: 0.00001670
Iteration 33/1000 | Loss: 0.00001670
Iteration 34/1000 | Loss: 0.00001668
Iteration 35/1000 | Loss: 0.00001668
Iteration 36/1000 | Loss: 0.00001668
Iteration 37/1000 | Loss: 0.00001667
Iteration 38/1000 | Loss: 0.00001667
Iteration 39/1000 | Loss: 0.00001666
Iteration 40/1000 | Loss: 0.00001666
Iteration 41/1000 | Loss: 0.00001666
Iteration 42/1000 | Loss: 0.00001666
Iteration 43/1000 | Loss: 0.00001665
Iteration 44/1000 | Loss: 0.00001665
Iteration 45/1000 | Loss: 0.00001665
Iteration 46/1000 | Loss: 0.00001665
Iteration 47/1000 | Loss: 0.00001665
Iteration 48/1000 | Loss: 0.00001664
Iteration 49/1000 | Loss: 0.00001664
Iteration 50/1000 | Loss: 0.00001663
Iteration 51/1000 | Loss: 0.00001662
Iteration 52/1000 | Loss: 0.00001662
Iteration 53/1000 | Loss: 0.00001661
Iteration 54/1000 | Loss: 0.00001661
Iteration 55/1000 | Loss: 0.00001660
Iteration 56/1000 | Loss: 0.00001658
Iteration 57/1000 | Loss: 0.00001656
Iteration 58/1000 | Loss: 0.00001656
Iteration 59/1000 | Loss: 0.00001655
Iteration 60/1000 | Loss: 0.00001654
Iteration 61/1000 | Loss: 0.00001654
Iteration 62/1000 | Loss: 0.00001654
Iteration 63/1000 | Loss: 0.00001653
Iteration 64/1000 | Loss: 0.00001652
Iteration 65/1000 | Loss: 0.00001651
Iteration 66/1000 | Loss: 0.00001651
Iteration 67/1000 | Loss: 0.00001650
Iteration 68/1000 | Loss: 0.00001650
Iteration 69/1000 | Loss: 0.00001650
Iteration 70/1000 | Loss: 0.00001649
Iteration 71/1000 | Loss: 0.00001649
Iteration 72/1000 | Loss: 0.00001648
Iteration 73/1000 | Loss: 0.00001648
Iteration 74/1000 | Loss: 0.00001648
Iteration 75/1000 | Loss: 0.00001648
Iteration 76/1000 | Loss: 0.00001648
Iteration 77/1000 | Loss: 0.00001648
Iteration 78/1000 | Loss: 0.00001648
Iteration 79/1000 | Loss: 0.00001648
Iteration 80/1000 | Loss: 0.00001648
Iteration 81/1000 | Loss: 0.00001647
Iteration 82/1000 | Loss: 0.00001647
Iteration 83/1000 | Loss: 0.00001647
Iteration 84/1000 | Loss: 0.00001646
Iteration 85/1000 | Loss: 0.00001646
Iteration 86/1000 | Loss: 0.00001646
Iteration 87/1000 | Loss: 0.00001646
Iteration 88/1000 | Loss: 0.00001646
Iteration 89/1000 | Loss: 0.00001646
Iteration 90/1000 | Loss: 0.00001646
Iteration 91/1000 | Loss: 0.00001646
Iteration 92/1000 | Loss: 0.00001646
Iteration 93/1000 | Loss: 0.00001646
Iteration 94/1000 | Loss: 0.00001646
Iteration 95/1000 | Loss: 0.00001645
Iteration 96/1000 | Loss: 0.00001645
Iteration 97/1000 | Loss: 0.00001645
Iteration 98/1000 | Loss: 0.00001645
Iteration 99/1000 | Loss: 0.00001644
Iteration 100/1000 | Loss: 0.00001644
Iteration 101/1000 | Loss: 0.00001644
Iteration 102/1000 | Loss: 0.00001644
Iteration 103/1000 | Loss: 0.00001644
Iteration 104/1000 | Loss: 0.00001644
Iteration 105/1000 | Loss: 0.00001644
Iteration 106/1000 | Loss: 0.00001644
Iteration 107/1000 | Loss: 0.00001644
Iteration 108/1000 | Loss: 0.00001644
Iteration 109/1000 | Loss: 0.00001644
Iteration 110/1000 | Loss: 0.00001643
Iteration 111/1000 | Loss: 0.00001643
Iteration 112/1000 | Loss: 0.00001643
Iteration 113/1000 | Loss: 0.00001643
Iteration 114/1000 | Loss: 0.00001643
Iteration 115/1000 | Loss: 0.00001643
Iteration 116/1000 | Loss: 0.00001642
Iteration 117/1000 | Loss: 0.00001642
Iteration 118/1000 | Loss: 0.00001642
Iteration 119/1000 | Loss: 0.00001642
Iteration 120/1000 | Loss: 0.00001642
Iteration 121/1000 | Loss: 0.00001642
Iteration 122/1000 | Loss: 0.00001642
Iteration 123/1000 | Loss: 0.00001642
Iteration 124/1000 | Loss: 0.00001642
Iteration 125/1000 | Loss: 0.00001642
Iteration 126/1000 | Loss: 0.00001642
Iteration 127/1000 | Loss: 0.00001642
Iteration 128/1000 | Loss: 0.00001642
Iteration 129/1000 | Loss: 0.00001642
Iteration 130/1000 | Loss: 0.00001641
Iteration 131/1000 | Loss: 0.00001641
Iteration 132/1000 | Loss: 0.00001641
Iteration 133/1000 | Loss: 0.00001641
Iteration 134/1000 | Loss: 0.00001641
Iteration 135/1000 | Loss: 0.00001641
Iteration 136/1000 | Loss: 0.00001641
Iteration 137/1000 | Loss: 0.00001641
Iteration 138/1000 | Loss: 0.00001641
Iteration 139/1000 | Loss: 0.00001641
Iteration 140/1000 | Loss: 0.00001641
Iteration 141/1000 | Loss: 0.00001641
Iteration 142/1000 | Loss: 0.00001641
Iteration 143/1000 | Loss: 0.00001641
Iteration 144/1000 | Loss: 0.00001641
Iteration 145/1000 | Loss: 0.00001640
Iteration 146/1000 | Loss: 0.00001640
Iteration 147/1000 | Loss: 0.00001640
Iteration 148/1000 | Loss: 0.00001640
Iteration 149/1000 | Loss: 0.00001640
Iteration 150/1000 | Loss: 0.00001640
Iteration 151/1000 | Loss: 0.00001640
Iteration 152/1000 | Loss: 0.00001640
Iteration 153/1000 | Loss: 0.00001640
Iteration 154/1000 | Loss: 0.00001640
Iteration 155/1000 | Loss: 0.00001640
Iteration 156/1000 | Loss: 0.00001640
Iteration 157/1000 | Loss: 0.00001640
Iteration 158/1000 | Loss: 0.00001640
Iteration 159/1000 | Loss: 0.00001640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.6398938896600157e-05, 1.6398938896600157e-05, 1.6398938896600157e-05, 1.6398938896600157e-05, 1.6398938896600157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6398938896600157e-05

Optimization complete. Final v2v error: 3.4483518600463867 mm

Highest mean error: 3.9825024604797363 mm for frame 23

Lowest mean error: 3.203746795654297 mm for frame 181

Saving results

Total time: 247.55350637435913
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007505
Iteration 2/25 | Loss: 0.00281022
Iteration 3/25 | Loss: 0.00202421
Iteration 4/25 | Loss: 0.00177593
Iteration 5/25 | Loss: 0.00153791
Iteration 6/25 | Loss: 0.00135576
Iteration 7/25 | Loss: 0.00119097
Iteration 8/25 | Loss: 0.00115041
Iteration 9/25 | Loss: 0.00108314
Iteration 10/25 | Loss: 0.00105226
Iteration 11/25 | Loss: 0.00103801
Iteration 12/25 | Loss: 0.00102681
Iteration 13/25 | Loss: 0.00102811
Iteration 14/25 | Loss: 0.00102764
Iteration 15/25 | Loss: 0.00101992
Iteration 16/25 | Loss: 0.00101834
Iteration 17/25 | Loss: 0.00101800
Iteration 18/25 | Loss: 0.00101784
Iteration 19/25 | Loss: 0.00101771
Iteration 20/25 | Loss: 0.00101760
Iteration 21/25 | Loss: 0.00101745
Iteration 22/25 | Loss: 0.00101734
Iteration 23/25 | Loss: 0.00101727
Iteration 24/25 | Loss: 0.00101724
Iteration 25/25 | Loss: 0.00101724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60556221
Iteration 2/25 | Loss: 0.00178473
Iteration 3/25 | Loss: 0.00167928
Iteration 4/25 | Loss: 0.00167928
Iteration 5/25 | Loss: 0.00167928
Iteration 6/25 | Loss: 0.00167927
Iteration 7/25 | Loss: 0.00167927
Iteration 8/25 | Loss: 0.00167927
Iteration 9/25 | Loss: 0.00167927
Iteration 10/25 | Loss: 0.00167927
Iteration 11/25 | Loss: 0.00167927
Iteration 12/25 | Loss: 0.00167927
Iteration 13/25 | Loss: 0.00167927
Iteration 14/25 | Loss: 0.00167927
Iteration 15/25 | Loss: 0.00167927
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001679274020716548, 0.001679274020716548, 0.001679274020716548, 0.001679274020716548, 0.001679274020716548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001679274020716548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167927
Iteration 2/1000 | Loss: 0.00008960
Iteration 3/1000 | Loss: 0.00087930
Iteration 4/1000 | Loss: 0.00019975
Iteration 5/1000 | Loss: 0.00011851
Iteration 6/1000 | Loss: 0.00010587
Iteration 7/1000 | Loss: 0.00005558
Iteration 8/1000 | Loss: 0.00012001
Iteration 9/1000 | Loss: 0.00004149
Iteration 10/1000 | Loss: 0.00003881
Iteration 11/1000 | Loss: 0.00003710
Iteration 12/1000 | Loss: 0.00006268
Iteration 13/1000 | Loss: 0.00030555
Iteration 14/1000 | Loss: 0.00011422
Iteration 15/1000 | Loss: 0.00004698
Iteration 16/1000 | Loss: 0.00003807
Iteration 17/1000 | Loss: 0.00005004
Iteration 18/1000 | Loss: 0.00003497
Iteration 19/1000 | Loss: 0.00003377
Iteration 20/1000 | Loss: 0.00003281
Iteration 21/1000 | Loss: 0.00003233
Iteration 22/1000 | Loss: 0.00003206
Iteration 23/1000 | Loss: 0.00003184
Iteration 24/1000 | Loss: 0.00003168
Iteration 25/1000 | Loss: 0.00003166
Iteration 26/1000 | Loss: 0.00003164
Iteration 27/1000 | Loss: 0.00003164
Iteration 28/1000 | Loss: 0.00003160
Iteration 29/1000 | Loss: 0.00003160
Iteration 30/1000 | Loss: 0.00003160
Iteration 31/1000 | Loss: 0.00003160
Iteration 32/1000 | Loss: 0.00003159
Iteration 33/1000 | Loss: 0.00003159
Iteration 34/1000 | Loss: 0.00003159
Iteration 35/1000 | Loss: 0.00003159
Iteration 36/1000 | Loss: 0.00003159
Iteration 37/1000 | Loss: 0.00003159
Iteration 38/1000 | Loss: 0.00003159
Iteration 39/1000 | Loss: 0.00003159
Iteration 40/1000 | Loss: 0.00003157
Iteration 41/1000 | Loss: 0.00003156
Iteration 42/1000 | Loss: 0.00003156
Iteration 43/1000 | Loss: 0.00003156
Iteration 44/1000 | Loss: 0.00003156
Iteration 45/1000 | Loss: 0.00003155
Iteration 46/1000 | Loss: 0.00003151
Iteration 47/1000 | Loss: 0.00003150
Iteration 48/1000 | Loss: 0.00003146
Iteration 49/1000 | Loss: 0.00003146
Iteration 50/1000 | Loss: 0.00003146
Iteration 51/1000 | Loss: 0.00003139
Iteration 52/1000 | Loss: 0.00003139
Iteration 53/1000 | Loss: 0.00003139
Iteration 54/1000 | Loss: 0.00003139
Iteration 55/1000 | Loss: 0.00003139
Iteration 56/1000 | Loss: 0.00003139
Iteration 57/1000 | Loss: 0.00003139
Iteration 58/1000 | Loss: 0.00003139
Iteration 59/1000 | Loss: 0.00003139
Iteration 60/1000 | Loss: 0.00003139
Iteration 61/1000 | Loss: 0.00003139
Iteration 62/1000 | Loss: 0.00003138
Iteration 63/1000 | Loss: 0.00003138
Iteration 64/1000 | Loss: 0.00003138
Iteration 65/1000 | Loss: 0.00003138
Iteration 66/1000 | Loss: 0.00003138
Iteration 67/1000 | Loss: 0.00003138
Iteration 68/1000 | Loss: 0.00003138
Iteration 69/1000 | Loss: 0.00003137
Iteration 70/1000 | Loss: 0.00003137
Iteration 71/1000 | Loss: 0.00003136
Iteration 72/1000 | Loss: 0.00003136
Iteration 73/1000 | Loss: 0.00003136
Iteration 74/1000 | Loss: 0.00003134
Iteration 75/1000 | Loss: 0.00003133
Iteration 76/1000 | Loss: 0.00003132
Iteration 77/1000 | Loss: 0.00003131
Iteration 78/1000 | Loss: 0.00003131
Iteration 79/1000 | Loss: 0.00003131
Iteration 80/1000 | Loss: 0.00003131
Iteration 81/1000 | Loss: 0.00003131
Iteration 82/1000 | Loss: 0.00003131
Iteration 83/1000 | Loss: 0.00003131
Iteration 84/1000 | Loss: 0.00003131
Iteration 85/1000 | Loss: 0.00003131
Iteration 86/1000 | Loss: 0.00003131
Iteration 87/1000 | Loss: 0.00003130
Iteration 88/1000 | Loss: 0.00003130
Iteration 89/1000 | Loss: 0.00003130
Iteration 90/1000 | Loss: 0.00003130
Iteration 91/1000 | Loss: 0.00003130
Iteration 92/1000 | Loss: 0.00003130
Iteration 93/1000 | Loss: 0.00003129
Iteration 94/1000 | Loss: 0.00003129
Iteration 95/1000 | Loss: 0.00003129
Iteration 96/1000 | Loss: 0.00003129
Iteration 97/1000 | Loss: 0.00003129
Iteration 98/1000 | Loss: 0.00003129
Iteration 99/1000 | Loss: 0.00003129
Iteration 100/1000 | Loss: 0.00003129
Iteration 101/1000 | Loss: 0.00003128
Iteration 102/1000 | Loss: 0.00003128
Iteration 103/1000 | Loss: 0.00003128
Iteration 104/1000 | Loss: 0.00003128
Iteration 105/1000 | Loss: 0.00003128
Iteration 106/1000 | Loss: 0.00003128
Iteration 107/1000 | Loss: 0.00003128
Iteration 108/1000 | Loss: 0.00003128
Iteration 109/1000 | Loss: 0.00003128
Iteration 110/1000 | Loss: 0.00003128
Iteration 111/1000 | Loss: 0.00003128
Iteration 112/1000 | Loss: 0.00003128
Iteration 113/1000 | Loss: 0.00003128
Iteration 114/1000 | Loss: 0.00003128
Iteration 115/1000 | Loss: 0.00003128
Iteration 116/1000 | Loss: 0.00003128
Iteration 117/1000 | Loss: 0.00003128
Iteration 118/1000 | Loss: 0.00003128
Iteration 119/1000 | Loss: 0.00003128
Iteration 120/1000 | Loss: 0.00003128
Iteration 121/1000 | Loss: 0.00003128
Iteration 122/1000 | Loss: 0.00003128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [3.1281258998205885e-05, 3.1281258998205885e-05, 3.1281258998205885e-05, 3.1281258998205885e-05, 3.1281258998205885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1281258998205885e-05

Optimization complete. Final v2v error: 4.647065162658691 mm

Highest mean error: 5.430066108703613 mm for frame 231

Lowest mean error: 3.693118095397949 mm for frame 155

Saving results

Total time: 131.5967676639557
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391487
Iteration 2/25 | Loss: 0.00109346
Iteration 3/25 | Loss: 0.00092437
Iteration 4/25 | Loss: 0.00088465
Iteration 5/25 | Loss: 0.00086868
Iteration 6/25 | Loss: 0.00086333
Iteration 7/25 | Loss: 0.00086145
Iteration 8/25 | Loss: 0.00086087
Iteration 9/25 | Loss: 0.00086087
Iteration 10/25 | Loss: 0.00086087
Iteration 11/25 | Loss: 0.00086087
Iteration 12/25 | Loss: 0.00086087
Iteration 13/25 | Loss: 0.00086087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008608683128841221, 0.0008608683128841221, 0.0008608683128841221, 0.0008608683128841221, 0.0008608683128841221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008608683128841221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73478949
Iteration 2/25 | Loss: 0.00202453
Iteration 3/25 | Loss: 0.00202452
Iteration 4/25 | Loss: 0.00202452
Iteration 5/25 | Loss: 0.00202452
Iteration 6/25 | Loss: 0.00202452
Iteration 7/25 | Loss: 0.00202452
Iteration 8/25 | Loss: 0.00202452
Iteration 9/25 | Loss: 0.00202452
Iteration 10/25 | Loss: 0.00202452
Iteration 11/25 | Loss: 0.00202452
Iteration 12/25 | Loss: 0.00202452
Iteration 13/25 | Loss: 0.00202452
Iteration 14/25 | Loss: 0.00202452
Iteration 15/25 | Loss: 0.00202452
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002024522516876459, 0.002024522516876459, 0.002024522516876459, 0.002024522516876459, 0.002024522516876459]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002024522516876459

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202452
Iteration 2/1000 | Loss: 0.00005147
Iteration 3/1000 | Loss: 0.00004113
Iteration 4/1000 | Loss: 0.00003368
Iteration 5/1000 | Loss: 0.00003082
Iteration 6/1000 | Loss: 0.00002925
Iteration 7/1000 | Loss: 0.00002826
Iteration 8/1000 | Loss: 0.00002740
Iteration 9/1000 | Loss: 0.00002673
Iteration 10/1000 | Loss: 0.00002625
Iteration 11/1000 | Loss: 0.00002590
Iteration 12/1000 | Loss: 0.00002562
Iteration 13/1000 | Loss: 0.00002539
Iteration 14/1000 | Loss: 0.00002515
Iteration 15/1000 | Loss: 0.00002495
Iteration 16/1000 | Loss: 0.00002486
Iteration 17/1000 | Loss: 0.00002485
Iteration 18/1000 | Loss: 0.00002485
Iteration 19/1000 | Loss: 0.00002484
Iteration 20/1000 | Loss: 0.00002481
Iteration 21/1000 | Loss: 0.00002480
Iteration 22/1000 | Loss: 0.00002480
Iteration 23/1000 | Loss: 0.00002479
Iteration 24/1000 | Loss: 0.00002478
Iteration 25/1000 | Loss: 0.00002478
Iteration 26/1000 | Loss: 0.00002478
Iteration 27/1000 | Loss: 0.00002477
Iteration 28/1000 | Loss: 0.00002477
Iteration 29/1000 | Loss: 0.00002476
Iteration 30/1000 | Loss: 0.00002476
Iteration 31/1000 | Loss: 0.00002475
Iteration 32/1000 | Loss: 0.00002474
Iteration 33/1000 | Loss: 0.00002472
Iteration 34/1000 | Loss: 0.00002472
Iteration 35/1000 | Loss: 0.00002470
Iteration 36/1000 | Loss: 0.00002470
Iteration 37/1000 | Loss: 0.00002470
Iteration 38/1000 | Loss: 0.00002469
Iteration 39/1000 | Loss: 0.00002469
Iteration 40/1000 | Loss: 0.00002469
Iteration 41/1000 | Loss: 0.00002468
Iteration 42/1000 | Loss: 0.00002467
Iteration 43/1000 | Loss: 0.00002467
Iteration 44/1000 | Loss: 0.00002467
Iteration 45/1000 | Loss: 0.00002467
Iteration 46/1000 | Loss: 0.00002467
Iteration 47/1000 | Loss: 0.00002466
Iteration 48/1000 | Loss: 0.00002466
Iteration 49/1000 | Loss: 0.00002466
Iteration 50/1000 | Loss: 0.00002466
Iteration 51/1000 | Loss: 0.00002466
Iteration 52/1000 | Loss: 0.00002466
Iteration 53/1000 | Loss: 0.00002465
Iteration 54/1000 | Loss: 0.00002465
Iteration 55/1000 | Loss: 0.00002464
Iteration 56/1000 | Loss: 0.00002464
Iteration 57/1000 | Loss: 0.00002464
Iteration 58/1000 | Loss: 0.00002464
Iteration 59/1000 | Loss: 0.00002463
Iteration 60/1000 | Loss: 0.00002463
Iteration 61/1000 | Loss: 0.00002463
Iteration 62/1000 | Loss: 0.00002463
Iteration 63/1000 | Loss: 0.00002462
Iteration 64/1000 | Loss: 0.00002462
Iteration 65/1000 | Loss: 0.00002462
Iteration 66/1000 | Loss: 0.00002462
Iteration 67/1000 | Loss: 0.00002462
Iteration 68/1000 | Loss: 0.00002461
Iteration 69/1000 | Loss: 0.00002461
Iteration 70/1000 | Loss: 0.00002461
Iteration 71/1000 | Loss: 0.00002461
Iteration 72/1000 | Loss: 0.00002460
Iteration 73/1000 | Loss: 0.00002460
Iteration 74/1000 | Loss: 0.00002460
Iteration 75/1000 | Loss: 0.00002460
Iteration 76/1000 | Loss: 0.00002460
Iteration 77/1000 | Loss: 0.00002460
Iteration 78/1000 | Loss: 0.00002460
Iteration 79/1000 | Loss: 0.00002460
Iteration 80/1000 | Loss: 0.00002460
Iteration 81/1000 | Loss: 0.00002460
Iteration 82/1000 | Loss: 0.00002459
Iteration 83/1000 | Loss: 0.00002459
Iteration 84/1000 | Loss: 0.00002459
Iteration 85/1000 | Loss: 0.00002459
Iteration 86/1000 | Loss: 0.00002459
Iteration 87/1000 | Loss: 0.00002459
Iteration 88/1000 | Loss: 0.00002459
Iteration 89/1000 | Loss: 0.00002459
Iteration 90/1000 | Loss: 0.00002459
Iteration 91/1000 | Loss: 0.00002459
Iteration 92/1000 | Loss: 0.00002459
Iteration 93/1000 | Loss: 0.00002459
Iteration 94/1000 | Loss: 0.00002459
Iteration 95/1000 | Loss: 0.00002459
Iteration 96/1000 | Loss: 0.00002459
Iteration 97/1000 | Loss: 0.00002459
Iteration 98/1000 | Loss: 0.00002459
Iteration 99/1000 | Loss: 0.00002459
Iteration 100/1000 | Loss: 0.00002458
Iteration 101/1000 | Loss: 0.00002458
Iteration 102/1000 | Loss: 0.00002458
Iteration 103/1000 | Loss: 0.00002458
Iteration 104/1000 | Loss: 0.00002458
Iteration 105/1000 | Loss: 0.00002458
Iteration 106/1000 | Loss: 0.00002458
Iteration 107/1000 | Loss: 0.00002458
Iteration 108/1000 | Loss: 0.00002458
Iteration 109/1000 | Loss: 0.00002458
Iteration 110/1000 | Loss: 0.00002458
Iteration 111/1000 | Loss: 0.00002457
Iteration 112/1000 | Loss: 0.00002457
Iteration 113/1000 | Loss: 0.00002457
Iteration 114/1000 | Loss: 0.00002457
Iteration 115/1000 | Loss: 0.00002457
Iteration 116/1000 | Loss: 0.00002457
Iteration 117/1000 | Loss: 0.00002457
Iteration 118/1000 | Loss: 0.00002457
Iteration 119/1000 | Loss: 0.00002457
Iteration 120/1000 | Loss: 0.00002457
Iteration 121/1000 | Loss: 0.00002457
Iteration 122/1000 | Loss: 0.00002457
Iteration 123/1000 | Loss: 0.00002456
Iteration 124/1000 | Loss: 0.00002456
Iteration 125/1000 | Loss: 0.00002456
Iteration 126/1000 | Loss: 0.00002456
Iteration 127/1000 | Loss: 0.00002456
Iteration 128/1000 | Loss: 0.00002456
Iteration 129/1000 | Loss: 0.00002456
Iteration 130/1000 | Loss: 0.00002456
Iteration 131/1000 | Loss: 0.00002456
Iteration 132/1000 | Loss: 0.00002456
Iteration 133/1000 | Loss: 0.00002456
Iteration 134/1000 | Loss: 0.00002456
Iteration 135/1000 | Loss: 0.00002456
Iteration 136/1000 | Loss: 0.00002456
Iteration 137/1000 | Loss: 0.00002456
Iteration 138/1000 | Loss: 0.00002456
Iteration 139/1000 | Loss: 0.00002456
Iteration 140/1000 | Loss: 0.00002456
Iteration 141/1000 | Loss: 0.00002456
Iteration 142/1000 | Loss: 0.00002456
Iteration 143/1000 | Loss: 0.00002456
Iteration 144/1000 | Loss: 0.00002456
Iteration 145/1000 | Loss: 0.00002456
Iteration 146/1000 | Loss: 0.00002456
Iteration 147/1000 | Loss: 0.00002456
Iteration 148/1000 | Loss: 0.00002456
Iteration 149/1000 | Loss: 0.00002456
Iteration 150/1000 | Loss: 0.00002456
Iteration 151/1000 | Loss: 0.00002456
Iteration 152/1000 | Loss: 0.00002456
Iteration 153/1000 | Loss: 0.00002456
Iteration 154/1000 | Loss: 0.00002456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.4558414224884473e-05, 2.4558414224884473e-05, 2.4558414224884473e-05, 2.4558414224884473e-05, 2.4558414224884473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4558414224884473e-05

Optimization complete. Final v2v error: 3.9690005779266357 mm

Highest mean error: 5.757399559020996 mm for frame 76

Lowest mean error: 2.7997841835021973 mm for frame 34

Saving results

Total time: 44.752599000930786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01098908
Iteration 2/25 | Loss: 0.01098908
Iteration 3/25 | Loss: 0.00238740
Iteration 4/25 | Loss: 0.00150302
Iteration 5/25 | Loss: 0.00137149
Iteration 6/25 | Loss: 0.00113193
Iteration 7/25 | Loss: 0.00120893
Iteration 8/25 | Loss: 0.00101766
Iteration 9/25 | Loss: 0.00094431
Iteration 10/25 | Loss: 0.00090154
Iteration 11/25 | Loss: 0.00088358
Iteration 12/25 | Loss: 0.00085846
Iteration 13/25 | Loss: 0.00084709
Iteration 14/25 | Loss: 0.00083194
Iteration 15/25 | Loss: 0.00084182
Iteration 16/25 | Loss: 0.00083551
Iteration 17/25 | Loss: 0.00082679
Iteration 18/25 | Loss: 0.00082437
Iteration 19/25 | Loss: 0.00082572
Iteration 20/25 | Loss: 0.00082518
Iteration 21/25 | Loss: 0.00082540
Iteration 22/25 | Loss: 0.00082611
Iteration 23/25 | Loss: 0.00082483
Iteration 24/25 | Loss: 0.00082561
Iteration 25/25 | Loss: 0.00082479

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76273334
Iteration 2/25 | Loss: 0.00166694
Iteration 3/25 | Loss: 0.00166693
Iteration 4/25 | Loss: 0.00166693
Iteration 5/25 | Loss: 0.00166693
Iteration 6/25 | Loss: 0.00166693
Iteration 7/25 | Loss: 0.00166693
Iteration 8/25 | Loss: 0.00166693
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.001666933298110962, 0.001666933298110962, 0.001666933298110962, 0.001666933298110962, 0.001666933298110962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001666933298110962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166693
Iteration 2/1000 | Loss: 0.00019041
Iteration 3/1000 | Loss: 0.00012114
Iteration 4/1000 | Loss: 0.00032342
Iteration 5/1000 | Loss: 0.00008374
Iteration 6/1000 | Loss: 0.00002823
Iteration 7/1000 | Loss: 0.00002936
Iteration 8/1000 | Loss: 0.00003807
Iteration 9/1000 | Loss: 0.00021474
Iteration 10/1000 | Loss: 0.00004120
Iteration 11/1000 | Loss: 0.00016199
Iteration 12/1000 | Loss: 0.00046600
Iteration 13/1000 | Loss: 0.00006903
Iteration 14/1000 | Loss: 0.00005542
Iteration 15/1000 | Loss: 0.00009840
Iteration 16/1000 | Loss: 0.00085296
Iteration 17/1000 | Loss: 0.00068949
Iteration 18/1000 | Loss: 0.00029405
Iteration 19/1000 | Loss: 0.00027949
Iteration 20/1000 | Loss: 0.00011986
Iteration 21/1000 | Loss: 0.00004344
Iteration 22/1000 | Loss: 0.00025944
Iteration 23/1000 | Loss: 0.00003035
Iteration 24/1000 | Loss: 0.00002453
Iteration 25/1000 | Loss: 0.00009577
Iteration 26/1000 | Loss: 0.00002310
Iteration 27/1000 | Loss: 0.00002522
Iteration 28/1000 | Loss: 0.00002282
Iteration 29/1000 | Loss: 0.00003861
Iteration 30/1000 | Loss: 0.00016981
Iteration 31/1000 | Loss: 0.00005301
Iteration 32/1000 | Loss: 0.00003041
Iteration 33/1000 | Loss: 0.00002474
Iteration 34/1000 | Loss: 0.00002359
Iteration 35/1000 | Loss: 0.00012465
Iteration 36/1000 | Loss: 0.00003259
Iteration 37/1000 | Loss: 0.00003160
Iteration 38/1000 | Loss: 0.00008025
Iteration 39/1000 | Loss: 0.00006317
Iteration 40/1000 | Loss: 0.00003040
Iteration 41/1000 | Loss: 0.00006858
Iteration 42/1000 | Loss: 0.00003329
Iteration 43/1000 | Loss: 0.00007342
Iteration 44/1000 | Loss: 0.00002054
Iteration 45/1000 | Loss: 0.00006445
Iteration 46/1000 | Loss: 0.00002001
Iteration 47/1000 | Loss: 0.00001969
Iteration 48/1000 | Loss: 0.00014741
Iteration 49/1000 | Loss: 0.00027359
Iteration 50/1000 | Loss: 0.00006632
Iteration 51/1000 | Loss: 0.00018418
Iteration 52/1000 | Loss: 0.00065376
Iteration 53/1000 | Loss: 0.00005683
Iteration 54/1000 | Loss: 0.00001981
Iteration 55/1000 | Loss: 0.00004168
Iteration 56/1000 | Loss: 0.00001937
Iteration 57/1000 | Loss: 0.00001917
Iteration 58/1000 | Loss: 0.00001909
Iteration 59/1000 | Loss: 0.00001906
Iteration 60/1000 | Loss: 0.00001906
Iteration 61/1000 | Loss: 0.00001906
Iteration 62/1000 | Loss: 0.00001903
Iteration 63/1000 | Loss: 0.00001903
Iteration 64/1000 | Loss: 0.00001903
Iteration 65/1000 | Loss: 0.00001901
Iteration 66/1000 | Loss: 0.00001900
Iteration 67/1000 | Loss: 0.00001899
Iteration 68/1000 | Loss: 0.00001899
Iteration 69/1000 | Loss: 0.00001898
Iteration 70/1000 | Loss: 0.00001898
Iteration 71/1000 | Loss: 0.00001897
Iteration 72/1000 | Loss: 0.00001897
Iteration 73/1000 | Loss: 0.00001897
Iteration 74/1000 | Loss: 0.00001896
Iteration 75/1000 | Loss: 0.00001896
Iteration 76/1000 | Loss: 0.00001896
Iteration 77/1000 | Loss: 0.00001895
Iteration 78/1000 | Loss: 0.00001895
Iteration 79/1000 | Loss: 0.00001895
Iteration 80/1000 | Loss: 0.00001895
Iteration 81/1000 | Loss: 0.00001894
Iteration 82/1000 | Loss: 0.00001894
Iteration 83/1000 | Loss: 0.00001894
Iteration 84/1000 | Loss: 0.00001894
Iteration 85/1000 | Loss: 0.00001893
Iteration 86/1000 | Loss: 0.00001893
Iteration 87/1000 | Loss: 0.00001893
Iteration 88/1000 | Loss: 0.00001892
Iteration 89/1000 | Loss: 0.00001892
Iteration 90/1000 | Loss: 0.00001892
Iteration 91/1000 | Loss: 0.00001891
Iteration 92/1000 | Loss: 0.00001891
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001890
Iteration 95/1000 | Loss: 0.00001890
Iteration 96/1000 | Loss: 0.00001890
Iteration 97/1000 | Loss: 0.00001889
Iteration 98/1000 | Loss: 0.00001889
Iteration 99/1000 | Loss: 0.00001889
Iteration 100/1000 | Loss: 0.00001889
Iteration 101/1000 | Loss: 0.00001889
Iteration 102/1000 | Loss: 0.00001889
Iteration 103/1000 | Loss: 0.00001888
Iteration 104/1000 | Loss: 0.00001888
Iteration 105/1000 | Loss: 0.00001887
Iteration 106/1000 | Loss: 0.00001887
Iteration 107/1000 | Loss: 0.00001886
Iteration 108/1000 | Loss: 0.00001886
Iteration 109/1000 | Loss: 0.00001885
Iteration 110/1000 | Loss: 0.00001885
Iteration 111/1000 | Loss: 0.00001885
Iteration 112/1000 | Loss: 0.00001884
Iteration 113/1000 | Loss: 0.00001884
Iteration 114/1000 | Loss: 0.00001884
Iteration 115/1000 | Loss: 0.00001883
Iteration 116/1000 | Loss: 0.00001883
Iteration 117/1000 | Loss: 0.00001882
Iteration 118/1000 | Loss: 0.00001882
Iteration 119/1000 | Loss: 0.00001882
Iteration 120/1000 | Loss: 0.00001881
Iteration 121/1000 | Loss: 0.00001881
Iteration 122/1000 | Loss: 0.00001881
Iteration 123/1000 | Loss: 0.00001881
Iteration 124/1000 | Loss: 0.00001881
Iteration 125/1000 | Loss: 0.00001881
Iteration 126/1000 | Loss: 0.00001881
Iteration 127/1000 | Loss: 0.00001881
Iteration 128/1000 | Loss: 0.00001881
Iteration 129/1000 | Loss: 0.00001881
Iteration 130/1000 | Loss: 0.00001880
Iteration 131/1000 | Loss: 0.00001880
Iteration 132/1000 | Loss: 0.00001879
Iteration 133/1000 | Loss: 0.00001879
Iteration 134/1000 | Loss: 0.00001879
Iteration 135/1000 | Loss: 0.00001879
Iteration 136/1000 | Loss: 0.00001879
Iteration 137/1000 | Loss: 0.00001879
Iteration 138/1000 | Loss: 0.00001879
Iteration 139/1000 | Loss: 0.00001879
Iteration 140/1000 | Loss: 0.00001879
Iteration 141/1000 | Loss: 0.00001878
Iteration 142/1000 | Loss: 0.00001878
Iteration 143/1000 | Loss: 0.00001878
Iteration 144/1000 | Loss: 0.00001878
Iteration 145/1000 | Loss: 0.00001878
Iteration 146/1000 | Loss: 0.00001878
Iteration 147/1000 | Loss: 0.00001877
Iteration 148/1000 | Loss: 0.00001877
Iteration 149/1000 | Loss: 0.00001877
Iteration 150/1000 | Loss: 0.00001877
Iteration 151/1000 | Loss: 0.00001877
Iteration 152/1000 | Loss: 0.00001877
Iteration 153/1000 | Loss: 0.00001877
Iteration 154/1000 | Loss: 0.00001877
Iteration 155/1000 | Loss: 0.00001877
Iteration 156/1000 | Loss: 0.00001876
Iteration 157/1000 | Loss: 0.00001876
Iteration 158/1000 | Loss: 0.00001876
Iteration 159/1000 | Loss: 0.00001876
Iteration 160/1000 | Loss: 0.00001875
Iteration 161/1000 | Loss: 0.00001875
Iteration 162/1000 | Loss: 0.00001875
Iteration 163/1000 | Loss: 0.00001875
Iteration 164/1000 | Loss: 0.00001875
Iteration 165/1000 | Loss: 0.00001875
Iteration 166/1000 | Loss: 0.00001874
Iteration 167/1000 | Loss: 0.00001874
Iteration 168/1000 | Loss: 0.00001874
Iteration 169/1000 | Loss: 0.00001874
Iteration 170/1000 | Loss: 0.00001874
Iteration 171/1000 | Loss: 0.00001873
Iteration 172/1000 | Loss: 0.00001873
Iteration 173/1000 | Loss: 0.00001873
Iteration 174/1000 | Loss: 0.00001872
Iteration 175/1000 | Loss: 0.00001872
Iteration 176/1000 | Loss: 0.00001872
Iteration 177/1000 | Loss: 0.00001872
Iteration 178/1000 | Loss: 0.00001872
Iteration 179/1000 | Loss: 0.00001872
Iteration 180/1000 | Loss: 0.00001871
Iteration 181/1000 | Loss: 0.00001871
Iteration 182/1000 | Loss: 0.00001871
Iteration 183/1000 | Loss: 0.00001871
Iteration 184/1000 | Loss: 0.00001871
Iteration 185/1000 | Loss: 0.00001871
Iteration 186/1000 | Loss: 0.00001871
Iteration 187/1000 | Loss: 0.00001871
Iteration 188/1000 | Loss: 0.00001871
Iteration 189/1000 | Loss: 0.00001871
Iteration 190/1000 | Loss: 0.00001871
Iteration 191/1000 | Loss: 0.00001870
Iteration 192/1000 | Loss: 0.00001870
Iteration 193/1000 | Loss: 0.00001870
Iteration 194/1000 | Loss: 0.00001870
Iteration 195/1000 | Loss: 0.00001870
Iteration 196/1000 | Loss: 0.00001870
Iteration 197/1000 | Loss: 0.00001870
Iteration 198/1000 | Loss: 0.00001870
Iteration 199/1000 | Loss: 0.00001870
Iteration 200/1000 | Loss: 0.00001870
Iteration 201/1000 | Loss: 0.00001870
Iteration 202/1000 | Loss: 0.00001870
Iteration 203/1000 | Loss: 0.00001870
Iteration 204/1000 | Loss: 0.00001870
Iteration 205/1000 | Loss: 0.00001870
Iteration 206/1000 | Loss: 0.00001870
Iteration 207/1000 | Loss: 0.00001870
Iteration 208/1000 | Loss: 0.00001870
Iteration 209/1000 | Loss: 0.00001870
Iteration 210/1000 | Loss: 0.00001870
Iteration 211/1000 | Loss: 0.00001870
Iteration 212/1000 | Loss: 0.00001870
Iteration 213/1000 | Loss: 0.00001870
Iteration 214/1000 | Loss: 0.00001870
Iteration 215/1000 | Loss: 0.00001870
Iteration 216/1000 | Loss: 0.00001870
Iteration 217/1000 | Loss: 0.00001870
Iteration 218/1000 | Loss: 0.00001870
Iteration 219/1000 | Loss: 0.00001870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.8699514839681797e-05, 1.8699514839681797e-05, 1.8699514839681797e-05, 1.8699514839681797e-05, 1.8699514839681797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8699514839681797e-05

Optimization complete. Final v2v error: 3.4727420806884766 mm

Highest mean error: 11.57199764251709 mm for frame 199

Lowest mean error: 2.916748046875 mm for frame 219

Saving results

Total time: 175.7491970062256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391832
Iteration 2/25 | Loss: 0.00106972
Iteration 3/25 | Loss: 0.00080585
Iteration 4/25 | Loss: 0.00077482
Iteration 5/25 | Loss: 0.00076500
Iteration 6/25 | Loss: 0.00076234
Iteration 7/25 | Loss: 0.00076162
Iteration 8/25 | Loss: 0.00076146
Iteration 9/25 | Loss: 0.00076146
Iteration 10/25 | Loss: 0.00076146
Iteration 11/25 | Loss: 0.00076146
Iteration 12/25 | Loss: 0.00076146
Iteration 13/25 | Loss: 0.00076146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007614571950398386, 0.0007614571950398386, 0.0007614571950398386, 0.0007614571950398386, 0.0007614571950398386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007614571950398386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57774615
Iteration 2/25 | Loss: 0.00145648
Iteration 3/25 | Loss: 0.00145648
Iteration 4/25 | Loss: 0.00145648
Iteration 5/25 | Loss: 0.00145648
Iteration 6/25 | Loss: 0.00145648
Iteration 7/25 | Loss: 0.00145648
Iteration 8/25 | Loss: 0.00145648
Iteration 9/25 | Loss: 0.00145648
Iteration 10/25 | Loss: 0.00145648
Iteration 11/25 | Loss: 0.00145648
Iteration 12/25 | Loss: 0.00145648
Iteration 13/25 | Loss: 0.00145648
Iteration 14/25 | Loss: 0.00145648
Iteration 15/25 | Loss: 0.00145648
Iteration 16/25 | Loss: 0.00145648
Iteration 17/25 | Loss: 0.00145648
Iteration 18/25 | Loss: 0.00145648
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014564780285581946, 0.0014564780285581946, 0.0014564780285581946, 0.0014564780285581946, 0.0014564780285581946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014564780285581946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145648
Iteration 2/1000 | Loss: 0.00003029
Iteration 3/1000 | Loss: 0.00002109
Iteration 4/1000 | Loss: 0.00001653
Iteration 5/1000 | Loss: 0.00001530
Iteration 6/1000 | Loss: 0.00001428
Iteration 7/1000 | Loss: 0.00001372
Iteration 8/1000 | Loss: 0.00001321
Iteration 9/1000 | Loss: 0.00001295
Iteration 10/1000 | Loss: 0.00001278
Iteration 11/1000 | Loss: 0.00001273
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001252
Iteration 14/1000 | Loss: 0.00001252
Iteration 15/1000 | Loss: 0.00001251
Iteration 16/1000 | Loss: 0.00001250
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001245
Iteration 19/1000 | Loss: 0.00001238
Iteration 20/1000 | Loss: 0.00001238
Iteration 21/1000 | Loss: 0.00001232
Iteration 22/1000 | Loss: 0.00001232
Iteration 23/1000 | Loss: 0.00001231
Iteration 24/1000 | Loss: 0.00001228
Iteration 25/1000 | Loss: 0.00001223
Iteration 26/1000 | Loss: 0.00001223
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001221
Iteration 29/1000 | Loss: 0.00001220
Iteration 30/1000 | Loss: 0.00001220
Iteration 31/1000 | Loss: 0.00001220
Iteration 32/1000 | Loss: 0.00001219
Iteration 33/1000 | Loss: 0.00001219
Iteration 34/1000 | Loss: 0.00001218
Iteration 35/1000 | Loss: 0.00001218
Iteration 36/1000 | Loss: 0.00001218
Iteration 37/1000 | Loss: 0.00001217
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001217
Iteration 40/1000 | Loss: 0.00001216
Iteration 41/1000 | Loss: 0.00001216
Iteration 42/1000 | Loss: 0.00001216
Iteration 43/1000 | Loss: 0.00001216
Iteration 44/1000 | Loss: 0.00001216
Iteration 45/1000 | Loss: 0.00001215
Iteration 46/1000 | Loss: 0.00001215
Iteration 47/1000 | Loss: 0.00001215
Iteration 48/1000 | Loss: 0.00001215
Iteration 49/1000 | Loss: 0.00001215
Iteration 50/1000 | Loss: 0.00001215
Iteration 51/1000 | Loss: 0.00001215
Iteration 52/1000 | Loss: 0.00001214
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001214
Iteration 56/1000 | Loss: 0.00001214
Iteration 57/1000 | Loss: 0.00001213
Iteration 58/1000 | Loss: 0.00001213
Iteration 59/1000 | Loss: 0.00001213
Iteration 60/1000 | Loss: 0.00001213
Iteration 61/1000 | Loss: 0.00001213
Iteration 62/1000 | Loss: 0.00001213
Iteration 63/1000 | Loss: 0.00001213
Iteration 64/1000 | Loss: 0.00001213
Iteration 65/1000 | Loss: 0.00001213
Iteration 66/1000 | Loss: 0.00001213
Iteration 67/1000 | Loss: 0.00001213
Iteration 68/1000 | Loss: 0.00001212
Iteration 69/1000 | Loss: 0.00001212
Iteration 70/1000 | Loss: 0.00001212
Iteration 71/1000 | Loss: 0.00001212
Iteration 72/1000 | Loss: 0.00001212
Iteration 73/1000 | Loss: 0.00001212
Iteration 74/1000 | Loss: 0.00001212
Iteration 75/1000 | Loss: 0.00001212
Iteration 76/1000 | Loss: 0.00001212
Iteration 77/1000 | Loss: 0.00001212
Iteration 78/1000 | Loss: 0.00001212
Iteration 79/1000 | Loss: 0.00001212
Iteration 80/1000 | Loss: 0.00001212
Iteration 81/1000 | Loss: 0.00001212
Iteration 82/1000 | Loss: 0.00001212
Iteration 83/1000 | Loss: 0.00001212
Iteration 84/1000 | Loss: 0.00001212
Iteration 85/1000 | Loss: 0.00001212
Iteration 86/1000 | Loss: 0.00001212
Iteration 87/1000 | Loss: 0.00001212
Iteration 88/1000 | Loss: 0.00001212
Iteration 89/1000 | Loss: 0.00001211
Iteration 90/1000 | Loss: 0.00001211
Iteration 91/1000 | Loss: 0.00001211
Iteration 92/1000 | Loss: 0.00001211
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001211
Iteration 95/1000 | Loss: 0.00001211
Iteration 96/1000 | Loss: 0.00001211
Iteration 97/1000 | Loss: 0.00001211
Iteration 98/1000 | Loss: 0.00001211
Iteration 99/1000 | Loss: 0.00001211
Iteration 100/1000 | Loss: 0.00001211
Iteration 101/1000 | Loss: 0.00001210
Iteration 102/1000 | Loss: 0.00001210
Iteration 103/1000 | Loss: 0.00001210
Iteration 104/1000 | Loss: 0.00001210
Iteration 105/1000 | Loss: 0.00001210
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001210
Iteration 109/1000 | Loss: 0.00001210
Iteration 110/1000 | Loss: 0.00001209
Iteration 111/1000 | Loss: 0.00001209
Iteration 112/1000 | Loss: 0.00001209
Iteration 113/1000 | Loss: 0.00001209
Iteration 114/1000 | Loss: 0.00001209
Iteration 115/1000 | Loss: 0.00001209
Iteration 116/1000 | Loss: 0.00001209
Iteration 117/1000 | Loss: 0.00001209
Iteration 118/1000 | Loss: 0.00001209
Iteration 119/1000 | Loss: 0.00001209
Iteration 120/1000 | Loss: 0.00001209
Iteration 121/1000 | Loss: 0.00001209
Iteration 122/1000 | Loss: 0.00001209
Iteration 123/1000 | Loss: 0.00001209
Iteration 124/1000 | Loss: 0.00001208
Iteration 125/1000 | Loss: 0.00001208
Iteration 126/1000 | Loss: 0.00001208
Iteration 127/1000 | Loss: 0.00001208
Iteration 128/1000 | Loss: 0.00001208
Iteration 129/1000 | Loss: 0.00001208
Iteration 130/1000 | Loss: 0.00001208
Iteration 131/1000 | Loss: 0.00001208
Iteration 132/1000 | Loss: 0.00001208
Iteration 133/1000 | Loss: 0.00001208
Iteration 134/1000 | Loss: 0.00001208
Iteration 135/1000 | Loss: 0.00001208
Iteration 136/1000 | Loss: 0.00001208
Iteration 137/1000 | Loss: 0.00001208
Iteration 138/1000 | Loss: 0.00001208
Iteration 139/1000 | Loss: 0.00001208
Iteration 140/1000 | Loss: 0.00001208
Iteration 141/1000 | Loss: 0.00001207
Iteration 142/1000 | Loss: 0.00001207
Iteration 143/1000 | Loss: 0.00001207
Iteration 144/1000 | Loss: 0.00001207
Iteration 145/1000 | Loss: 0.00001207
Iteration 146/1000 | Loss: 0.00001207
Iteration 147/1000 | Loss: 0.00001207
Iteration 148/1000 | Loss: 0.00001207
Iteration 149/1000 | Loss: 0.00001207
Iteration 150/1000 | Loss: 0.00001207
Iteration 151/1000 | Loss: 0.00001207
Iteration 152/1000 | Loss: 0.00001207
Iteration 153/1000 | Loss: 0.00001207
Iteration 154/1000 | Loss: 0.00001207
Iteration 155/1000 | Loss: 0.00001207
Iteration 156/1000 | Loss: 0.00001207
Iteration 157/1000 | Loss: 0.00001207
Iteration 158/1000 | Loss: 0.00001207
Iteration 159/1000 | Loss: 0.00001207
Iteration 160/1000 | Loss: 0.00001207
Iteration 161/1000 | Loss: 0.00001207
Iteration 162/1000 | Loss: 0.00001207
Iteration 163/1000 | Loss: 0.00001207
Iteration 164/1000 | Loss: 0.00001207
Iteration 165/1000 | Loss: 0.00001207
Iteration 166/1000 | Loss: 0.00001207
Iteration 167/1000 | Loss: 0.00001207
Iteration 168/1000 | Loss: 0.00001207
Iteration 169/1000 | Loss: 0.00001207
Iteration 170/1000 | Loss: 0.00001207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.2072385288774967e-05, 1.2072385288774967e-05, 1.2072385288774967e-05, 1.2072385288774967e-05, 1.2072385288774967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2072385288774967e-05

Optimization complete. Final v2v error: 2.951632261276245 mm

Highest mean error: 3.472214698791504 mm for frame 104

Lowest mean error: 2.638686418533325 mm for frame 7

Saving results

Total time: 41.17558312416077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429622
Iteration 2/25 | Loss: 0.00101074
Iteration 3/25 | Loss: 0.00088408
Iteration 4/25 | Loss: 0.00084644
Iteration 5/25 | Loss: 0.00084099
Iteration 6/25 | Loss: 0.00083875
Iteration 7/25 | Loss: 0.00083833
Iteration 8/25 | Loss: 0.00083833
Iteration 9/25 | Loss: 0.00083833
Iteration 10/25 | Loss: 0.00083833
Iteration 11/25 | Loss: 0.00083833
Iteration 12/25 | Loss: 0.00083833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008383264066651464, 0.0008383264066651464, 0.0008383264066651464, 0.0008383264066651464, 0.0008383264066651464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008383264066651464

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33484864
Iteration 2/25 | Loss: 0.00126869
Iteration 3/25 | Loss: 0.00126869
Iteration 4/25 | Loss: 0.00126869
Iteration 5/25 | Loss: 0.00126869
Iteration 6/25 | Loss: 0.00126869
Iteration 7/25 | Loss: 0.00126869
Iteration 8/25 | Loss: 0.00126869
Iteration 9/25 | Loss: 0.00126868
Iteration 10/25 | Loss: 0.00126868
Iteration 11/25 | Loss: 0.00126868
Iteration 12/25 | Loss: 0.00126868
Iteration 13/25 | Loss: 0.00126868
Iteration 14/25 | Loss: 0.00126868
Iteration 15/25 | Loss: 0.00126868
Iteration 16/25 | Loss: 0.00126868
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001268684514798224, 0.001268684514798224, 0.001268684514798224, 0.001268684514798224, 0.001268684514798224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001268684514798224

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126868
Iteration 2/1000 | Loss: 0.00004634
Iteration 3/1000 | Loss: 0.00003149
Iteration 4/1000 | Loss: 0.00002821
Iteration 5/1000 | Loss: 0.00002688
Iteration 6/1000 | Loss: 0.00002605
Iteration 7/1000 | Loss: 0.00002549
Iteration 8/1000 | Loss: 0.00002504
Iteration 9/1000 | Loss: 0.00002479
Iteration 10/1000 | Loss: 0.00002454
Iteration 11/1000 | Loss: 0.00002440
Iteration 12/1000 | Loss: 0.00002439
Iteration 13/1000 | Loss: 0.00002439
Iteration 14/1000 | Loss: 0.00002438
Iteration 15/1000 | Loss: 0.00002438
Iteration 16/1000 | Loss: 0.00002429
Iteration 17/1000 | Loss: 0.00002427
Iteration 18/1000 | Loss: 0.00002423
Iteration 19/1000 | Loss: 0.00002423
Iteration 20/1000 | Loss: 0.00002420
Iteration 21/1000 | Loss: 0.00002420
Iteration 22/1000 | Loss: 0.00002420
Iteration 23/1000 | Loss: 0.00002419
Iteration 24/1000 | Loss: 0.00002419
Iteration 25/1000 | Loss: 0.00002419
Iteration 26/1000 | Loss: 0.00002419
Iteration 27/1000 | Loss: 0.00002418
Iteration 28/1000 | Loss: 0.00002418
Iteration 29/1000 | Loss: 0.00002418
Iteration 30/1000 | Loss: 0.00002418
Iteration 31/1000 | Loss: 0.00002418
Iteration 32/1000 | Loss: 0.00002418
Iteration 33/1000 | Loss: 0.00002418
Iteration 34/1000 | Loss: 0.00002418
Iteration 35/1000 | Loss: 0.00002418
Iteration 36/1000 | Loss: 0.00002416
Iteration 37/1000 | Loss: 0.00002416
Iteration 38/1000 | Loss: 0.00002416
Iteration 39/1000 | Loss: 0.00002415
Iteration 40/1000 | Loss: 0.00002414
Iteration 41/1000 | Loss: 0.00002414
Iteration 42/1000 | Loss: 0.00002413
Iteration 43/1000 | Loss: 0.00002413
Iteration 44/1000 | Loss: 0.00002411
Iteration 45/1000 | Loss: 0.00002410
Iteration 46/1000 | Loss: 0.00002409
Iteration 47/1000 | Loss: 0.00002403
Iteration 48/1000 | Loss: 0.00002401
Iteration 49/1000 | Loss: 0.00002400
Iteration 50/1000 | Loss: 0.00002399
Iteration 51/1000 | Loss: 0.00002399
Iteration 52/1000 | Loss: 0.00002399
Iteration 53/1000 | Loss: 0.00002399
Iteration 54/1000 | Loss: 0.00002398
Iteration 55/1000 | Loss: 0.00002398
Iteration 56/1000 | Loss: 0.00002398
Iteration 57/1000 | Loss: 0.00002398
Iteration 58/1000 | Loss: 0.00002398
Iteration 59/1000 | Loss: 0.00002398
Iteration 60/1000 | Loss: 0.00002398
Iteration 61/1000 | Loss: 0.00002398
Iteration 62/1000 | Loss: 0.00002398
Iteration 63/1000 | Loss: 0.00002398
Iteration 64/1000 | Loss: 0.00002398
Iteration 65/1000 | Loss: 0.00002398
Iteration 66/1000 | Loss: 0.00002397
Iteration 67/1000 | Loss: 0.00002397
Iteration 68/1000 | Loss: 0.00002397
Iteration 69/1000 | Loss: 0.00002397
Iteration 70/1000 | Loss: 0.00002397
Iteration 71/1000 | Loss: 0.00002397
Iteration 72/1000 | Loss: 0.00002396
Iteration 73/1000 | Loss: 0.00002396
Iteration 74/1000 | Loss: 0.00002395
Iteration 75/1000 | Loss: 0.00002395
Iteration 76/1000 | Loss: 0.00002395
Iteration 77/1000 | Loss: 0.00002395
Iteration 78/1000 | Loss: 0.00002395
Iteration 79/1000 | Loss: 0.00002395
Iteration 80/1000 | Loss: 0.00002395
Iteration 81/1000 | Loss: 0.00002395
Iteration 82/1000 | Loss: 0.00002395
Iteration 83/1000 | Loss: 0.00002395
Iteration 84/1000 | Loss: 0.00002394
Iteration 85/1000 | Loss: 0.00002394
Iteration 86/1000 | Loss: 0.00002394
Iteration 87/1000 | Loss: 0.00002394
Iteration 88/1000 | Loss: 0.00002393
Iteration 89/1000 | Loss: 0.00002393
Iteration 90/1000 | Loss: 0.00002393
Iteration 91/1000 | Loss: 0.00002393
Iteration 92/1000 | Loss: 0.00002393
Iteration 93/1000 | Loss: 0.00002393
Iteration 94/1000 | Loss: 0.00002392
Iteration 95/1000 | Loss: 0.00002392
Iteration 96/1000 | Loss: 0.00002392
Iteration 97/1000 | Loss: 0.00002392
Iteration 98/1000 | Loss: 0.00002392
Iteration 99/1000 | Loss: 0.00002392
Iteration 100/1000 | Loss: 0.00002392
Iteration 101/1000 | Loss: 0.00002392
Iteration 102/1000 | Loss: 0.00002392
Iteration 103/1000 | Loss: 0.00002392
Iteration 104/1000 | Loss: 0.00002391
Iteration 105/1000 | Loss: 0.00002391
Iteration 106/1000 | Loss: 0.00002391
Iteration 107/1000 | Loss: 0.00002391
Iteration 108/1000 | Loss: 0.00002391
Iteration 109/1000 | Loss: 0.00002391
Iteration 110/1000 | Loss: 0.00002391
Iteration 111/1000 | Loss: 0.00002390
Iteration 112/1000 | Loss: 0.00002390
Iteration 113/1000 | Loss: 0.00002390
Iteration 114/1000 | Loss: 0.00002390
Iteration 115/1000 | Loss: 0.00002390
Iteration 116/1000 | Loss: 0.00002390
Iteration 117/1000 | Loss: 0.00002389
Iteration 118/1000 | Loss: 0.00002389
Iteration 119/1000 | Loss: 0.00002389
Iteration 120/1000 | Loss: 0.00002389
Iteration 121/1000 | Loss: 0.00002388
Iteration 122/1000 | Loss: 0.00002388
Iteration 123/1000 | Loss: 0.00002388
Iteration 124/1000 | Loss: 0.00002388
Iteration 125/1000 | Loss: 0.00002388
Iteration 126/1000 | Loss: 0.00002388
Iteration 127/1000 | Loss: 0.00002388
Iteration 128/1000 | Loss: 0.00002387
Iteration 129/1000 | Loss: 0.00002387
Iteration 130/1000 | Loss: 0.00002387
Iteration 131/1000 | Loss: 0.00002387
Iteration 132/1000 | Loss: 0.00002387
Iteration 133/1000 | Loss: 0.00002387
Iteration 134/1000 | Loss: 0.00002386
Iteration 135/1000 | Loss: 0.00002385
Iteration 136/1000 | Loss: 0.00002385
Iteration 137/1000 | Loss: 0.00002385
Iteration 138/1000 | Loss: 0.00002385
Iteration 139/1000 | Loss: 0.00002385
Iteration 140/1000 | Loss: 0.00002385
Iteration 141/1000 | Loss: 0.00002385
Iteration 142/1000 | Loss: 0.00002385
Iteration 143/1000 | Loss: 0.00002384
Iteration 144/1000 | Loss: 0.00002384
Iteration 145/1000 | Loss: 0.00002384
Iteration 146/1000 | Loss: 0.00002384
Iteration 147/1000 | Loss: 0.00002384
Iteration 148/1000 | Loss: 0.00002384
Iteration 149/1000 | Loss: 0.00002384
Iteration 150/1000 | Loss: 0.00002384
Iteration 151/1000 | Loss: 0.00002384
Iteration 152/1000 | Loss: 0.00002384
Iteration 153/1000 | Loss: 0.00002384
Iteration 154/1000 | Loss: 0.00002384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.3837392291170545e-05, 2.3837392291170545e-05, 2.3837392291170545e-05, 2.3837392291170545e-05, 2.3837392291170545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3837392291170545e-05

Optimization complete. Final v2v error: 4.064192771911621 mm

Highest mean error: 4.4841461181640625 mm for frame 88

Lowest mean error: 3.7013556957244873 mm for frame 171

Saving results

Total time: 40.30330467224121
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780198
Iteration 2/25 | Loss: 0.00153515
Iteration 3/25 | Loss: 0.00102406
Iteration 4/25 | Loss: 0.00092371
Iteration 5/25 | Loss: 0.00091210
Iteration 6/25 | Loss: 0.00089357
Iteration 7/25 | Loss: 0.00088955
Iteration 8/25 | Loss: 0.00088136
Iteration 9/25 | Loss: 0.00086806
Iteration 10/25 | Loss: 0.00087010
Iteration 11/25 | Loss: 0.00086190
Iteration 12/25 | Loss: 0.00085628
Iteration 13/25 | Loss: 0.00085478
Iteration 14/25 | Loss: 0.00085455
Iteration 15/25 | Loss: 0.00085454
Iteration 16/25 | Loss: 0.00085454
Iteration 17/25 | Loss: 0.00085453
Iteration 18/25 | Loss: 0.00085453
Iteration 19/25 | Loss: 0.00085453
Iteration 20/25 | Loss: 0.00085453
Iteration 21/25 | Loss: 0.00085453
Iteration 22/25 | Loss: 0.00085453
Iteration 23/25 | Loss: 0.00085453
Iteration 24/25 | Loss: 0.00085453
Iteration 25/25 | Loss: 0.00085453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62686229
Iteration 2/25 | Loss: 0.00141238
Iteration 3/25 | Loss: 0.00141233
Iteration 4/25 | Loss: 0.00141233
Iteration 5/25 | Loss: 0.00141233
Iteration 6/25 | Loss: 0.00141233
Iteration 7/25 | Loss: 0.00141233
Iteration 8/25 | Loss: 0.00141233
Iteration 9/25 | Loss: 0.00141233
Iteration 10/25 | Loss: 0.00141233
Iteration 11/25 | Loss: 0.00141233
Iteration 12/25 | Loss: 0.00141233
Iteration 13/25 | Loss: 0.00141233
Iteration 14/25 | Loss: 0.00141233
Iteration 15/25 | Loss: 0.00141233
Iteration 16/25 | Loss: 0.00141233
Iteration 17/25 | Loss: 0.00141233
Iteration 18/25 | Loss: 0.00141233
Iteration 19/25 | Loss: 0.00141233
Iteration 20/25 | Loss: 0.00141233
Iteration 21/25 | Loss: 0.00141233
Iteration 22/25 | Loss: 0.00141233
Iteration 23/25 | Loss: 0.00141233
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014123321743682027, 0.0014123321743682027, 0.0014123321743682027, 0.0014123321743682027, 0.0014123321743682027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014123321743682027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141233
Iteration 2/1000 | Loss: 0.00003957
Iteration 3/1000 | Loss: 0.00002824
Iteration 4/1000 | Loss: 0.00002509
Iteration 5/1000 | Loss: 0.00002379
Iteration 6/1000 | Loss: 0.00002286
Iteration 7/1000 | Loss: 0.00002220
Iteration 8/1000 | Loss: 0.00002161
Iteration 9/1000 | Loss: 0.00002122
Iteration 10/1000 | Loss: 0.00002089
Iteration 11/1000 | Loss: 0.00002068
Iteration 12/1000 | Loss: 0.00002048
Iteration 13/1000 | Loss: 0.00002046
Iteration 14/1000 | Loss: 0.00002039
Iteration 15/1000 | Loss: 0.00002025
Iteration 16/1000 | Loss: 0.00002019
Iteration 17/1000 | Loss: 0.00002016
Iteration 18/1000 | Loss: 0.00002015
Iteration 19/1000 | Loss: 0.00002014
Iteration 20/1000 | Loss: 0.00002014
Iteration 21/1000 | Loss: 0.00002013
Iteration 22/1000 | Loss: 0.00002011
Iteration 23/1000 | Loss: 0.00002009
Iteration 24/1000 | Loss: 0.00002009
Iteration 25/1000 | Loss: 0.00002008
Iteration 26/1000 | Loss: 0.00002008
Iteration 27/1000 | Loss: 0.00002008
Iteration 28/1000 | Loss: 0.00002007
Iteration 29/1000 | Loss: 0.00002007
Iteration 30/1000 | Loss: 0.00002007
Iteration 31/1000 | Loss: 0.00002006
Iteration 32/1000 | Loss: 0.00002006
Iteration 33/1000 | Loss: 0.00002006
Iteration 34/1000 | Loss: 0.00002005
Iteration 35/1000 | Loss: 0.00002005
Iteration 36/1000 | Loss: 0.00002005
Iteration 37/1000 | Loss: 0.00002004
Iteration 38/1000 | Loss: 0.00002003
Iteration 39/1000 | Loss: 0.00002003
Iteration 40/1000 | Loss: 0.00002002
Iteration 41/1000 | Loss: 0.00002002
Iteration 42/1000 | Loss: 0.00002002
Iteration 43/1000 | Loss: 0.00002001
Iteration 44/1000 | Loss: 0.00002001
Iteration 45/1000 | Loss: 0.00002000
Iteration 46/1000 | Loss: 0.00002000
Iteration 47/1000 | Loss: 0.00002000
Iteration 48/1000 | Loss: 0.00002000
Iteration 49/1000 | Loss: 0.00002000
Iteration 50/1000 | Loss: 0.00002000
Iteration 51/1000 | Loss: 0.00001999
Iteration 52/1000 | Loss: 0.00001999
Iteration 53/1000 | Loss: 0.00001999
Iteration 54/1000 | Loss: 0.00001999
Iteration 55/1000 | Loss: 0.00001998
Iteration 56/1000 | Loss: 0.00001998
Iteration 57/1000 | Loss: 0.00001998
Iteration 58/1000 | Loss: 0.00001998
Iteration 59/1000 | Loss: 0.00001998
Iteration 60/1000 | Loss: 0.00001998
Iteration 61/1000 | Loss: 0.00001998
Iteration 62/1000 | Loss: 0.00001998
Iteration 63/1000 | Loss: 0.00001998
Iteration 64/1000 | Loss: 0.00001998
Iteration 65/1000 | Loss: 0.00001998
Iteration 66/1000 | Loss: 0.00001998
Iteration 67/1000 | Loss: 0.00001998
Iteration 68/1000 | Loss: 0.00001997
Iteration 69/1000 | Loss: 0.00001997
Iteration 70/1000 | Loss: 0.00001997
Iteration 71/1000 | Loss: 0.00001997
Iteration 72/1000 | Loss: 0.00001997
Iteration 73/1000 | Loss: 0.00001997
Iteration 74/1000 | Loss: 0.00001997
Iteration 75/1000 | Loss: 0.00001997
Iteration 76/1000 | Loss: 0.00001997
Iteration 77/1000 | Loss: 0.00001996
Iteration 78/1000 | Loss: 0.00001996
Iteration 79/1000 | Loss: 0.00001996
Iteration 80/1000 | Loss: 0.00001996
Iteration 81/1000 | Loss: 0.00001996
Iteration 82/1000 | Loss: 0.00001996
Iteration 83/1000 | Loss: 0.00001996
Iteration 84/1000 | Loss: 0.00001995
Iteration 85/1000 | Loss: 0.00001995
Iteration 86/1000 | Loss: 0.00001995
Iteration 87/1000 | Loss: 0.00001995
Iteration 88/1000 | Loss: 0.00001995
Iteration 89/1000 | Loss: 0.00001995
Iteration 90/1000 | Loss: 0.00001995
Iteration 91/1000 | Loss: 0.00001995
Iteration 92/1000 | Loss: 0.00001995
Iteration 93/1000 | Loss: 0.00001994
Iteration 94/1000 | Loss: 0.00001994
Iteration 95/1000 | Loss: 0.00001994
Iteration 96/1000 | Loss: 0.00001994
Iteration 97/1000 | Loss: 0.00001994
Iteration 98/1000 | Loss: 0.00001994
Iteration 99/1000 | Loss: 0.00001994
Iteration 100/1000 | Loss: 0.00001994
Iteration 101/1000 | Loss: 0.00001993
Iteration 102/1000 | Loss: 0.00001993
Iteration 103/1000 | Loss: 0.00001993
Iteration 104/1000 | Loss: 0.00001993
Iteration 105/1000 | Loss: 0.00001992
Iteration 106/1000 | Loss: 0.00001992
Iteration 107/1000 | Loss: 0.00001992
Iteration 108/1000 | Loss: 0.00001992
Iteration 109/1000 | Loss: 0.00001992
Iteration 110/1000 | Loss: 0.00001992
Iteration 111/1000 | Loss: 0.00001992
Iteration 112/1000 | Loss: 0.00001992
Iteration 113/1000 | Loss: 0.00001992
Iteration 114/1000 | Loss: 0.00001991
Iteration 115/1000 | Loss: 0.00001991
Iteration 116/1000 | Loss: 0.00001991
Iteration 117/1000 | Loss: 0.00001991
Iteration 118/1000 | Loss: 0.00001991
Iteration 119/1000 | Loss: 0.00001991
Iteration 120/1000 | Loss: 0.00001991
Iteration 121/1000 | Loss: 0.00001991
Iteration 122/1000 | Loss: 0.00001991
Iteration 123/1000 | Loss: 0.00001991
Iteration 124/1000 | Loss: 0.00001991
Iteration 125/1000 | Loss: 0.00001991
Iteration 126/1000 | Loss: 0.00001991
Iteration 127/1000 | Loss: 0.00001991
Iteration 128/1000 | Loss: 0.00001991
Iteration 129/1000 | Loss: 0.00001991
Iteration 130/1000 | Loss: 0.00001991
Iteration 131/1000 | Loss: 0.00001990
Iteration 132/1000 | Loss: 0.00001990
Iteration 133/1000 | Loss: 0.00001990
Iteration 134/1000 | Loss: 0.00001990
Iteration 135/1000 | Loss: 0.00001990
Iteration 136/1000 | Loss: 0.00001990
Iteration 137/1000 | Loss: 0.00001990
Iteration 138/1000 | Loss: 0.00001990
Iteration 139/1000 | Loss: 0.00001990
Iteration 140/1000 | Loss: 0.00001990
Iteration 141/1000 | Loss: 0.00001990
Iteration 142/1000 | Loss: 0.00001990
Iteration 143/1000 | Loss: 0.00001990
Iteration 144/1000 | Loss: 0.00001990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.99041714949999e-05, 1.99041714949999e-05, 1.99041714949999e-05, 1.99041714949999e-05, 1.99041714949999e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.99041714949999e-05

Optimization complete. Final v2v error: 3.8066093921661377 mm

Highest mean error: 4.40474796295166 mm for frame 104

Lowest mean error: 3.4223270416259766 mm for frame 10

Saving results

Total time: 93.8354434967041
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874654
Iteration 2/25 | Loss: 0.00137925
Iteration 3/25 | Loss: 0.00098928
Iteration 4/25 | Loss: 0.00091610
Iteration 5/25 | Loss: 0.00088698
Iteration 6/25 | Loss: 0.00088133
Iteration 7/25 | Loss: 0.00088042
Iteration 8/25 | Loss: 0.00088042
Iteration 9/25 | Loss: 0.00088042
Iteration 10/25 | Loss: 0.00088042
Iteration 11/25 | Loss: 0.00088041
Iteration 12/25 | Loss: 0.00088041
Iteration 13/25 | Loss: 0.00088041
Iteration 14/25 | Loss: 0.00088041
Iteration 15/25 | Loss: 0.00088041
Iteration 16/25 | Loss: 0.00088041
Iteration 17/25 | Loss: 0.00088041
Iteration 18/25 | Loss: 0.00088041
Iteration 19/25 | Loss: 0.00088041
Iteration 20/25 | Loss: 0.00088041
Iteration 21/25 | Loss: 0.00088041
Iteration 22/25 | Loss: 0.00088041
Iteration 23/25 | Loss: 0.00088041
Iteration 24/25 | Loss: 0.00088041
Iteration 25/25 | Loss: 0.00088041

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.92401338
Iteration 2/25 | Loss: 0.00139340
Iteration 3/25 | Loss: 0.00139339
Iteration 4/25 | Loss: 0.00139338
Iteration 5/25 | Loss: 0.00139338
Iteration 6/25 | Loss: 0.00139338
Iteration 7/25 | Loss: 0.00139338
Iteration 8/25 | Loss: 0.00139338
Iteration 9/25 | Loss: 0.00139338
Iteration 10/25 | Loss: 0.00139338
Iteration 11/25 | Loss: 0.00139338
Iteration 12/25 | Loss: 0.00139338
Iteration 13/25 | Loss: 0.00139338
Iteration 14/25 | Loss: 0.00139338
Iteration 15/25 | Loss: 0.00139338
Iteration 16/25 | Loss: 0.00139338
Iteration 17/25 | Loss: 0.00139338
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013933824375271797, 0.0013933824375271797, 0.0013933824375271797, 0.0013933824375271797, 0.0013933824375271797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013933824375271797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139338
Iteration 2/1000 | Loss: 0.00003886
Iteration 3/1000 | Loss: 0.00002752
Iteration 4/1000 | Loss: 0.00002447
Iteration 5/1000 | Loss: 0.00002359
Iteration 6/1000 | Loss: 0.00002284
Iteration 7/1000 | Loss: 0.00002228
Iteration 8/1000 | Loss: 0.00002182
Iteration 9/1000 | Loss: 0.00002142
Iteration 10/1000 | Loss: 0.00002121
Iteration 11/1000 | Loss: 0.00002104
Iteration 12/1000 | Loss: 0.00002103
Iteration 13/1000 | Loss: 0.00002101
Iteration 14/1000 | Loss: 0.00002098
Iteration 15/1000 | Loss: 0.00002093
Iteration 16/1000 | Loss: 0.00002090
Iteration 17/1000 | Loss: 0.00002089
Iteration 18/1000 | Loss: 0.00002086
Iteration 19/1000 | Loss: 0.00002086
Iteration 20/1000 | Loss: 0.00002085
Iteration 21/1000 | Loss: 0.00002084
Iteration 22/1000 | Loss: 0.00002079
Iteration 23/1000 | Loss: 0.00002077
Iteration 24/1000 | Loss: 0.00002076
Iteration 25/1000 | Loss: 0.00002076
Iteration 26/1000 | Loss: 0.00002075
Iteration 27/1000 | Loss: 0.00002074
Iteration 28/1000 | Loss: 0.00002071
Iteration 29/1000 | Loss: 0.00002071
Iteration 30/1000 | Loss: 0.00002070
Iteration 31/1000 | Loss: 0.00002070
Iteration 32/1000 | Loss: 0.00002069
Iteration 33/1000 | Loss: 0.00002069
Iteration 34/1000 | Loss: 0.00002069
Iteration 35/1000 | Loss: 0.00002068
Iteration 36/1000 | Loss: 0.00002068
Iteration 37/1000 | Loss: 0.00002068
Iteration 38/1000 | Loss: 0.00002067
Iteration 39/1000 | Loss: 0.00002067
Iteration 40/1000 | Loss: 0.00002067
Iteration 41/1000 | Loss: 0.00002067
Iteration 42/1000 | Loss: 0.00002067
Iteration 43/1000 | Loss: 0.00002066
Iteration 44/1000 | Loss: 0.00002066
Iteration 45/1000 | Loss: 0.00002066
Iteration 46/1000 | Loss: 0.00002066
Iteration 47/1000 | Loss: 0.00002065
Iteration 48/1000 | Loss: 0.00002065
Iteration 49/1000 | Loss: 0.00002065
Iteration 50/1000 | Loss: 0.00002065
Iteration 51/1000 | Loss: 0.00002064
Iteration 52/1000 | Loss: 0.00002064
Iteration 53/1000 | Loss: 0.00002064
Iteration 54/1000 | Loss: 0.00002064
Iteration 55/1000 | Loss: 0.00002064
Iteration 56/1000 | Loss: 0.00002064
Iteration 57/1000 | Loss: 0.00002064
Iteration 58/1000 | Loss: 0.00002064
Iteration 59/1000 | Loss: 0.00002064
Iteration 60/1000 | Loss: 0.00002064
Iteration 61/1000 | Loss: 0.00002064
Iteration 62/1000 | Loss: 0.00002063
Iteration 63/1000 | Loss: 0.00002063
Iteration 64/1000 | Loss: 0.00002063
Iteration 65/1000 | Loss: 0.00002063
Iteration 66/1000 | Loss: 0.00002063
Iteration 67/1000 | Loss: 0.00002063
Iteration 68/1000 | Loss: 0.00002063
Iteration 69/1000 | Loss: 0.00002063
Iteration 70/1000 | Loss: 0.00002062
Iteration 71/1000 | Loss: 0.00002062
Iteration 72/1000 | Loss: 0.00002062
Iteration 73/1000 | Loss: 0.00002062
Iteration 74/1000 | Loss: 0.00002062
Iteration 75/1000 | Loss: 0.00002061
Iteration 76/1000 | Loss: 0.00002061
Iteration 77/1000 | Loss: 0.00002061
Iteration 78/1000 | Loss: 0.00002061
Iteration 79/1000 | Loss: 0.00002060
Iteration 80/1000 | Loss: 0.00002060
Iteration 81/1000 | Loss: 0.00002060
Iteration 82/1000 | Loss: 0.00002060
Iteration 83/1000 | Loss: 0.00002060
Iteration 84/1000 | Loss: 0.00002059
Iteration 85/1000 | Loss: 0.00002059
Iteration 86/1000 | Loss: 0.00002059
Iteration 87/1000 | Loss: 0.00002059
Iteration 88/1000 | Loss: 0.00002059
Iteration 89/1000 | Loss: 0.00002059
Iteration 90/1000 | Loss: 0.00002059
Iteration 91/1000 | Loss: 0.00002059
Iteration 92/1000 | Loss: 0.00002059
Iteration 93/1000 | Loss: 0.00002059
Iteration 94/1000 | Loss: 0.00002058
Iteration 95/1000 | Loss: 0.00002058
Iteration 96/1000 | Loss: 0.00002058
Iteration 97/1000 | Loss: 0.00002058
Iteration 98/1000 | Loss: 0.00002058
Iteration 99/1000 | Loss: 0.00002058
Iteration 100/1000 | Loss: 0.00002058
Iteration 101/1000 | Loss: 0.00002058
Iteration 102/1000 | Loss: 0.00002058
Iteration 103/1000 | Loss: 0.00002058
Iteration 104/1000 | Loss: 0.00002058
Iteration 105/1000 | Loss: 0.00002057
Iteration 106/1000 | Loss: 0.00002057
Iteration 107/1000 | Loss: 0.00002057
Iteration 108/1000 | Loss: 0.00002057
Iteration 109/1000 | Loss: 0.00002057
Iteration 110/1000 | Loss: 0.00002057
Iteration 111/1000 | Loss: 0.00002056
Iteration 112/1000 | Loss: 0.00002056
Iteration 113/1000 | Loss: 0.00002056
Iteration 114/1000 | Loss: 0.00002056
Iteration 115/1000 | Loss: 0.00002056
Iteration 116/1000 | Loss: 0.00002056
Iteration 117/1000 | Loss: 0.00002056
Iteration 118/1000 | Loss: 0.00002056
Iteration 119/1000 | Loss: 0.00002056
Iteration 120/1000 | Loss: 0.00002056
Iteration 121/1000 | Loss: 0.00002056
Iteration 122/1000 | Loss: 0.00002055
Iteration 123/1000 | Loss: 0.00002055
Iteration 124/1000 | Loss: 0.00002055
Iteration 125/1000 | Loss: 0.00002055
Iteration 126/1000 | Loss: 0.00002055
Iteration 127/1000 | Loss: 0.00002055
Iteration 128/1000 | Loss: 0.00002055
Iteration 129/1000 | Loss: 0.00002055
Iteration 130/1000 | Loss: 0.00002055
Iteration 131/1000 | Loss: 0.00002055
Iteration 132/1000 | Loss: 0.00002055
Iteration 133/1000 | Loss: 0.00002055
Iteration 134/1000 | Loss: 0.00002054
Iteration 135/1000 | Loss: 0.00002054
Iteration 136/1000 | Loss: 0.00002054
Iteration 137/1000 | Loss: 0.00002054
Iteration 138/1000 | Loss: 0.00002054
Iteration 139/1000 | Loss: 0.00002054
Iteration 140/1000 | Loss: 0.00002054
Iteration 141/1000 | Loss: 0.00002054
Iteration 142/1000 | Loss: 0.00002054
Iteration 143/1000 | Loss: 0.00002054
Iteration 144/1000 | Loss: 0.00002054
Iteration 145/1000 | Loss: 0.00002054
Iteration 146/1000 | Loss: 0.00002054
Iteration 147/1000 | Loss: 0.00002054
Iteration 148/1000 | Loss: 0.00002054
Iteration 149/1000 | Loss: 0.00002054
Iteration 150/1000 | Loss: 0.00002054
Iteration 151/1000 | Loss: 0.00002054
Iteration 152/1000 | Loss: 0.00002053
Iteration 153/1000 | Loss: 0.00002053
Iteration 154/1000 | Loss: 0.00002053
Iteration 155/1000 | Loss: 0.00002053
Iteration 156/1000 | Loss: 0.00002053
Iteration 157/1000 | Loss: 0.00002053
Iteration 158/1000 | Loss: 0.00002053
Iteration 159/1000 | Loss: 0.00002053
Iteration 160/1000 | Loss: 0.00002053
Iteration 161/1000 | Loss: 0.00002053
Iteration 162/1000 | Loss: 0.00002053
Iteration 163/1000 | Loss: 0.00002053
Iteration 164/1000 | Loss: 0.00002053
Iteration 165/1000 | Loss: 0.00002053
Iteration 166/1000 | Loss: 0.00002053
Iteration 167/1000 | Loss: 0.00002053
Iteration 168/1000 | Loss: 0.00002053
Iteration 169/1000 | Loss: 0.00002053
Iteration 170/1000 | Loss: 0.00002053
Iteration 171/1000 | Loss: 0.00002053
Iteration 172/1000 | Loss: 0.00002053
Iteration 173/1000 | Loss: 0.00002053
Iteration 174/1000 | Loss: 0.00002053
Iteration 175/1000 | Loss: 0.00002053
Iteration 176/1000 | Loss: 0.00002053
Iteration 177/1000 | Loss: 0.00002053
Iteration 178/1000 | Loss: 0.00002053
Iteration 179/1000 | Loss: 0.00002053
Iteration 180/1000 | Loss: 0.00002053
Iteration 181/1000 | Loss: 0.00002053
Iteration 182/1000 | Loss: 0.00002053
Iteration 183/1000 | Loss: 0.00002053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.0531160771497525e-05, 2.0531160771497525e-05, 2.0531160771497525e-05, 2.0531160771497525e-05, 2.0531160771497525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0531160771497525e-05

Optimization complete. Final v2v error: 3.7747366428375244 mm

Highest mean error: 4.0471320152282715 mm for frame 62

Lowest mean error: 3.5406222343444824 mm for frame 112

Saving results

Total time: 65.01350355148315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057830
Iteration 2/25 | Loss: 0.00143309
Iteration 3/25 | Loss: 0.00112914
Iteration 4/25 | Loss: 0.00105696
Iteration 5/25 | Loss: 0.00103229
Iteration 6/25 | Loss: 0.00102593
Iteration 7/25 | Loss: 0.00102395
Iteration 8/25 | Loss: 0.00102395
Iteration 9/25 | Loss: 0.00102395
Iteration 10/25 | Loss: 0.00102395
Iteration 11/25 | Loss: 0.00102395
Iteration 12/25 | Loss: 0.00102395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010239464463666081, 0.0010239464463666081, 0.0010239464463666081, 0.0010239464463666081, 0.0010239464463666081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010239464463666081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02599645
Iteration 2/25 | Loss: 0.00123622
Iteration 3/25 | Loss: 0.00123604
Iteration 4/25 | Loss: 0.00123604
Iteration 5/25 | Loss: 0.00123604
Iteration 6/25 | Loss: 0.00123604
Iteration 7/25 | Loss: 0.00123604
Iteration 8/25 | Loss: 0.00123604
Iteration 9/25 | Loss: 0.00123604
Iteration 10/25 | Loss: 0.00123604
Iteration 11/25 | Loss: 0.00123604
Iteration 12/25 | Loss: 0.00123604
Iteration 13/25 | Loss: 0.00123604
Iteration 14/25 | Loss: 0.00123604
Iteration 15/25 | Loss: 0.00123604
Iteration 16/25 | Loss: 0.00123604
Iteration 17/25 | Loss: 0.00123604
Iteration 18/25 | Loss: 0.00123604
Iteration 19/25 | Loss: 0.00123604
Iteration 20/25 | Loss: 0.00123604
Iteration 21/25 | Loss: 0.00123604
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012360364198684692, 0.0012360364198684692, 0.0012360364198684692, 0.0012360364198684692, 0.0012360364198684692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012360364198684692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123604
Iteration 2/1000 | Loss: 0.00012241
Iteration 3/1000 | Loss: 0.00008438
Iteration 4/1000 | Loss: 0.00007253
Iteration 5/1000 | Loss: 0.00006649
Iteration 6/1000 | Loss: 0.00006277
Iteration 7/1000 | Loss: 0.00006004
Iteration 8/1000 | Loss: 0.00005841
Iteration 9/1000 | Loss: 0.00005685
Iteration 10/1000 | Loss: 0.00005528
Iteration 11/1000 | Loss: 0.00005443
Iteration 12/1000 | Loss: 0.00005365
Iteration 13/1000 | Loss: 0.00005292
Iteration 14/1000 | Loss: 0.00005241
Iteration 15/1000 | Loss: 0.00005203
Iteration 16/1000 | Loss: 0.00005162
Iteration 17/1000 | Loss: 0.00005131
Iteration 18/1000 | Loss: 0.00005103
Iteration 19/1000 | Loss: 0.00005078
Iteration 20/1000 | Loss: 0.00005056
Iteration 21/1000 | Loss: 0.00005054
Iteration 22/1000 | Loss: 0.00005038
Iteration 23/1000 | Loss: 0.00005031
Iteration 24/1000 | Loss: 0.00005021
Iteration 25/1000 | Loss: 0.00005016
Iteration 26/1000 | Loss: 0.00005014
Iteration 27/1000 | Loss: 0.00005012
Iteration 28/1000 | Loss: 0.00005008
Iteration 29/1000 | Loss: 0.00005005
Iteration 30/1000 | Loss: 0.00005002
Iteration 31/1000 | Loss: 0.00004998
Iteration 32/1000 | Loss: 0.00004984
Iteration 33/1000 | Loss: 0.00004984
Iteration 34/1000 | Loss: 0.00004980
Iteration 35/1000 | Loss: 0.00004971
Iteration 36/1000 | Loss: 0.00004971
Iteration 37/1000 | Loss: 0.00004968
Iteration 38/1000 | Loss: 0.00004967
Iteration 39/1000 | Loss: 0.00004967
Iteration 40/1000 | Loss: 0.00004966
Iteration 41/1000 | Loss: 0.00004966
Iteration 42/1000 | Loss: 0.00004965
Iteration 43/1000 | Loss: 0.00004964
Iteration 44/1000 | Loss: 0.00004963
Iteration 45/1000 | Loss: 0.00004963
Iteration 46/1000 | Loss: 0.00004963
Iteration 47/1000 | Loss: 0.00004962
Iteration 48/1000 | Loss: 0.00004962
Iteration 49/1000 | Loss: 0.00004961
Iteration 50/1000 | Loss: 0.00004960
Iteration 51/1000 | Loss: 0.00004960
Iteration 52/1000 | Loss: 0.00004959
Iteration 53/1000 | Loss: 0.00004959
Iteration 54/1000 | Loss: 0.00004959
Iteration 55/1000 | Loss: 0.00004959
Iteration 56/1000 | Loss: 0.00004959
Iteration 57/1000 | Loss: 0.00004958
Iteration 58/1000 | Loss: 0.00004958
Iteration 59/1000 | Loss: 0.00004958
Iteration 60/1000 | Loss: 0.00004957
Iteration 61/1000 | Loss: 0.00004957
Iteration 62/1000 | Loss: 0.00004957
Iteration 63/1000 | Loss: 0.00004957
Iteration 64/1000 | Loss: 0.00004956
Iteration 65/1000 | Loss: 0.00004956
Iteration 66/1000 | Loss: 0.00004956
Iteration 67/1000 | Loss: 0.00004955
Iteration 68/1000 | Loss: 0.00004955
Iteration 69/1000 | Loss: 0.00004955
Iteration 70/1000 | Loss: 0.00004955
Iteration 71/1000 | Loss: 0.00004955
Iteration 72/1000 | Loss: 0.00004955
Iteration 73/1000 | Loss: 0.00004955
Iteration 74/1000 | Loss: 0.00004955
Iteration 75/1000 | Loss: 0.00004955
Iteration 76/1000 | Loss: 0.00004955
Iteration 77/1000 | Loss: 0.00004954
Iteration 78/1000 | Loss: 0.00004954
Iteration 79/1000 | Loss: 0.00004954
Iteration 80/1000 | Loss: 0.00004954
Iteration 81/1000 | Loss: 0.00004954
Iteration 82/1000 | Loss: 0.00004954
Iteration 83/1000 | Loss: 0.00004953
Iteration 84/1000 | Loss: 0.00004953
Iteration 85/1000 | Loss: 0.00004953
Iteration 86/1000 | Loss: 0.00004953
Iteration 87/1000 | Loss: 0.00004953
Iteration 88/1000 | Loss: 0.00004952
Iteration 89/1000 | Loss: 0.00004952
Iteration 90/1000 | Loss: 0.00004952
Iteration 91/1000 | Loss: 0.00004952
Iteration 92/1000 | Loss: 0.00004952
Iteration 93/1000 | Loss: 0.00004952
Iteration 94/1000 | Loss: 0.00004952
Iteration 95/1000 | Loss: 0.00004952
Iteration 96/1000 | Loss: 0.00004952
Iteration 97/1000 | Loss: 0.00004951
Iteration 98/1000 | Loss: 0.00004951
Iteration 99/1000 | Loss: 0.00004951
Iteration 100/1000 | Loss: 0.00004951
Iteration 101/1000 | Loss: 0.00004951
Iteration 102/1000 | Loss: 0.00004951
Iteration 103/1000 | Loss: 0.00004951
Iteration 104/1000 | Loss: 0.00004951
Iteration 105/1000 | Loss: 0.00004951
Iteration 106/1000 | Loss: 0.00004951
Iteration 107/1000 | Loss: 0.00004951
Iteration 108/1000 | Loss: 0.00004950
Iteration 109/1000 | Loss: 0.00004950
Iteration 110/1000 | Loss: 0.00004950
Iteration 111/1000 | Loss: 0.00004950
Iteration 112/1000 | Loss: 0.00004950
Iteration 113/1000 | Loss: 0.00004950
Iteration 114/1000 | Loss: 0.00004950
Iteration 115/1000 | Loss: 0.00004949
Iteration 116/1000 | Loss: 0.00004949
Iteration 117/1000 | Loss: 0.00004949
Iteration 118/1000 | Loss: 0.00004949
Iteration 119/1000 | Loss: 0.00004949
Iteration 120/1000 | Loss: 0.00004949
Iteration 121/1000 | Loss: 0.00004949
Iteration 122/1000 | Loss: 0.00004949
Iteration 123/1000 | Loss: 0.00004949
Iteration 124/1000 | Loss: 0.00004948
Iteration 125/1000 | Loss: 0.00004948
Iteration 126/1000 | Loss: 0.00004948
Iteration 127/1000 | Loss: 0.00004948
Iteration 128/1000 | Loss: 0.00004948
Iteration 129/1000 | Loss: 0.00004948
Iteration 130/1000 | Loss: 0.00004948
Iteration 131/1000 | Loss: 0.00004947
Iteration 132/1000 | Loss: 0.00004947
Iteration 133/1000 | Loss: 0.00004947
Iteration 134/1000 | Loss: 0.00004947
Iteration 135/1000 | Loss: 0.00004946
Iteration 136/1000 | Loss: 0.00004946
Iteration 137/1000 | Loss: 0.00004946
Iteration 138/1000 | Loss: 0.00004946
Iteration 139/1000 | Loss: 0.00004946
Iteration 140/1000 | Loss: 0.00004946
Iteration 141/1000 | Loss: 0.00004945
Iteration 142/1000 | Loss: 0.00004945
Iteration 143/1000 | Loss: 0.00004945
Iteration 144/1000 | Loss: 0.00004945
Iteration 145/1000 | Loss: 0.00004945
Iteration 146/1000 | Loss: 0.00004945
Iteration 147/1000 | Loss: 0.00004944
Iteration 148/1000 | Loss: 0.00004944
Iteration 149/1000 | Loss: 0.00004944
Iteration 150/1000 | Loss: 0.00004944
Iteration 151/1000 | Loss: 0.00004944
Iteration 152/1000 | Loss: 0.00004944
Iteration 153/1000 | Loss: 0.00004944
Iteration 154/1000 | Loss: 0.00004944
Iteration 155/1000 | Loss: 0.00004943
Iteration 156/1000 | Loss: 0.00004943
Iteration 157/1000 | Loss: 0.00004943
Iteration 158/1000 | Loss: 0.00004943
Iteration 159/1000 | Loss: 0.00004943
Iteration 160/1000 | Loss: 0.00004943
Iteration 161/1000 | Loss: 0.00004943
Iteration 162/1000 | Loss: 0.00004943
Iteration 163/1000 | Loss: 0.00004942
Iteration 164/1000 | Loss: 0.00004942
Iteration 165/1000 | Loss: 0.00004942
Iteration 166/1000 | Loss: 0.00004942
Iteration 167/1000 | Loss: 0.00004942
Iteration 168/1000 | Loss: 0.00004942
Iteration 169/1000 | Loss: 0.00004942
Iteration 170/1000 | Loss: 0.00004942
Iteration 171/1000 | Loss: 0.00004941
Iteration 172/1000 | Loss: 0.00004941
Iteration 173/1000 | Loss: 0.00004941
Iteration 174/1000 | Loss: 0.00004941
Iteration 175/1000 | Loss: 0.00004941
Iteration 176/1000 | Loss: 0.00004941
Iteration 177/1000 | Loss: 0.00004941
Iteration 178/1000 | Loss: 0.00004941
Iteration 179/1000 | Loss: 0.00004941
Iteration 180/1000 | Loss: 0.00004941
Iteration 181/1000 | Loss: 0.00004941
Iteration 182/1000 | Loss: 0.00004940
Iteration 183/1000 | Loss: 0.00004940
Iteration 184/1000 | Loss: 0.00004940
Iteration 185/1000 | Loss: 0.00004940
Iteration 186/1000 | Loss: 0.00004940
Iteration 187/1000 | Loss: 0.00004940
Iteration 188/1000 | Loss: 0.00004940
Iteration 189/1000 | Loss: 0.00004940
Iteration 190/1000 | Loss: 0.00004940
Iteration 191/1000 | Loss: 0.00004940
Iteration 192/1000 | Loss: 0.00004940
Iteration 193/1000 | Loss: 0.00004940
Iteration 194/1000 | Loss: 0.00004940
Iteration 195/1000 | Loss: 0.00004940
Iteration 196/1000 | Loss: 0.00004939
Iteration 197/1000 | Loss: 0.00004939
Iteration 198/1000 | Loss: 0.00004939
Iteration 199/1000 | Loss: 0.00004939
Iteration 200/1000 | Loss: 0.00004939
Iteration 201/1000 | Loss: 0.00004939
Iteration 202/1000 | Loss: 0.00004939
Iteration 203/1000 | Loss: 0.00004939
Iteration 204/1000 | Loss: 0.00004939
Iteration 205/1000 | Loss: 0.00004939
Iteration 206/1000 | Loss: 0.00004939
Iteration 207/1000 | Loss: 0.00004939
Iteration 208/1000 | Loss: 0.00004939
Iteration 209/1000 | Loss: 0.00004939
Iteration 210/1000 | Loss: 0.00004938
Iteration 211/1000 | Loss: 0.00004938
Iteration 212/1000 | Loss: 0.00004938
Iteration 213/1000 | Loss: 0.00004938
Iteration 214/1000 | Loss: 0.00004938
Iteration 215/1000 | Loss: 0.00004938
Iteration 216/1000 | Loss: 0.00004938
Iteration 217/1000 | Loss: 0.00004938
Iteration 218/1000 | Loss: 0.00004938
Iteration 219/1000 | Loss: 0.00004938
Iteration 220/1000 | Loss: 0.00004938
Iteration 221/1000 | Loss: 0.00004938
Iteration 222/1000 | Loss: 0.00004938
Iteration 223/1000 | Loss: 0.00004938
Iteration 224/1000 | Loss: 0.00004938
Iteration 225/1000 | Loss: 0.00004938
Iteration 226/1000 | Loss: 0.00004938
Iteration 227/1000 | Loss: 0.00004938
Iteration 228/1000 | Loss: 0.00004938
Iteration 229/1000 | Loss: 0.00004938
Iteration 230/1000 | Loss: 0.00004938
Iteration 231/1000 | Loss: 0.00004938
Iteration 232/1000 | Loss: 0.00004938
Iteration 233/1000 | Loss: 0.00004938
Iteration 234/1000 | Loss: 0.00004938
Iteration 235/1000 | Loss: 0.00004938
Iteration 236/1000 | Loss: 0.00004938
Iteration 237/1000 | Loss: 0.00004938
Iteration 238/1000 | Loss: 0.00004938
Iteration 239/1000 | Loss: 0.00004938
Iteration 240/1000 | Loss: 0.00004938
Iteration 241/1000 | Loss: 0.00004938
Iteration 242/1000 | Loss: 0.00004938
Iteration 243/1000 | Loss: 0.00004938
Iteration 244/1000 | Loss: 0.00004938
Iteration 245/1000 | Loss: 0.00004938
Iteration 246/1000 | Loss: 0.00004938
Iteration 247/1000 | Loss: 0.00004938
Iteration 248/1000 | Loss: 0.00004938
Iteration 249/1000 | Loss: 0.00004938
Iteration 250/1000 | Loss: 0.00004938
Iteration 251/1000 | Loss: 0.00004938
Iteration 252/1000 | Loss: 0.00004938
Iteration 253/1000 | Loss: 0.00004938
Iteration 254/1000 | Loss: 0.00004938
Iteration 255/1000 | Loss: 0.00004938
Iteration 256/1000 | Loss: 0.00004938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [4.938090933137573e-05, 4.938090933137573e-05, 4.938090933137573e-05, 4.938090933137573e-05, 4.938090933137573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.938090933137573e-05

Optimization complete. Final v2v error: 5.782068252563477 mm

Highest mean error: 6.346273422241211 mm for frame 181

Lowest mean error: 5.108490467071533 mm for frame 151

Saving results

Total time: 105.40793681144714
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994870
Iteration 2/25 | Loss: 0.00223323
Iteration 3/25 | Loss: 0.00157578
Iteration 4/25 | Loss: 0.00141466
Iteration 5/25 | Loss: 0.00128276
Iteration 6/25 | Loss: 0.00120080
Iteration 7/25 | Loss: 0.00115492
Iteration 8/25 | Loss: 0.00113638
Iteration 9/25 | Loss: 0.00112838
Iteration 10/25 | Loss: 0.00111393
Iteration 11/25 | Loss: 0.00110124
Iteration 12/25 | Loss: 0.00108971
Iteration 13/25 | Loss: 0.00110310
Iteration 14/25 | Loss: 0.00108733
Iteration 15/25 | Loss: 0.00106853
Iteration 16/25 | Loss: 0.00103838
Iteration 17/25 | Loss: 0.00101559
Iteration 18/25 | Loss: 0.00101216
Iteration 19/25 | Loss: 0.00101780
Iteration 20/25 | Loss: 0.00099381
Iteration 21/25 | Loss: 0.00098658
Iteration 22/25 | Loss: 0.00098491
Iteration 23/25 | Loss: 0.00097743
Iteration 24/25 | Loss: 0.00097837
Iteration 25/25 | Loss: 0.00097564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64911425
Iteration 2/25 | Loss: 0.00262989
Iteration 3/25 | Loss: 0.00251732
Iteration 4/25 | Loss: 0.00251732
Iteration 5/25 | Loss: 0.00251732
Iteration 6/25 | Loss: 0.00251732
Iteration 7/25 | Loss: 0.00251732
Iteration 8/25 | Loss: 0.00251732
Iteration 9/25 | Loss: 0.00251732
Iteration 10/25 | Loss: 0.00251732
Iteration 11/25 | Loss: 0.00251732
Iteration 12/25 | Loss: 0.00251732
Iteration 13/25 | Loss: 0.00251732
Iteration 14/25 | Loss: 0.00251732
Iteration 15/25 | Loss: 0.00251732
Iteration 16/25 | Loss: 0.00251732
Iteration 17/25 | Loss: 0.00251732
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002517321379855275, 0.002517321379855275, 0.002517321379855275, 0.002517321379855275, 0.002517321379855275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002517321379855275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251732
Iteration 2/1000 | Loss: 0.00049277
Iteration 3/1000 | Loss: 0.00017621
Iteration 4/1000 | Loss: 0.00011131
Iteration 5/1000 | Loss: 0.00012030
Iteration 6/1000 | Loss: 0.00008440
Iteration 7/1000 | Loss: 0.00037714
Iteration 8/1000 | Loss: 0.00008041
Iteration 9/1000 | Loss: 0.00033415
Iteration 10/1000 | Loss: 0.00007786
Iteration 11/1000 | Loss: 0.00006885
Iteration 12/1000 | Loss: 0.00006529
Iteration 13/1000 | Loss: 0.00006347
Iteration 14/1000 | Loss: 0.00006200
Iteration 15/1000 | Loss: 0.00006093
Iteration 16/1000 | Loss: 0.00006013
Iteration 17/1000 | Loss: 0.00005921
Iteration 18/1000 | Loss: 0.00007206
Iteration 19/1000 | Loss: 0.00006093
Iteration 20/1000 | Loss: 0.00005795
Iteration 21/1000 | Loss: 0.00005733
Iteration 22/1000 | Loss: 0.00187641
Iteration 23/1000 | Loss: 0.00111440
Iteration 24/1000 | Loss: 0.00065726
Iteration 25/1000 | Loss: 0.00151037
Iteration 26/1000 | Loss: 0.00186921
Iteration 27/1000 | Loss: 0.00158447
Iteration 28/1000 | Loss: 0.00195021
Iteration 29/1000 | Loss: 0.00156160
Iteration 30/1000 | Loss: 0.00230739
Iteration 31/1000 | Loss: 0.00360118
Iteration 32/1000 | Loss: 0.00489714
Iteration 33/1000 | Loss: 0.00320772
Iteration 34/1000 | Loss: 0.00169946
Iteration 35/1000 | Loss: 0.00078860
Iteration 36/1000 | Loss: 0.00141457
Iteration 37/1000 | Loss: 0.00097492
Iteration 38/1000 | Loss: 0.00115807
Iteration 39/1000 | Loss: 0.00043897
Iteration 40/1000 | Loss: 0.00038561
Iteration 41/1000 | Loss: 0.00006692
Iteration 42/1000 | Loss: 0.00011935
Iteration 43/1000 | Loss: 0.00124201
Iteration 44/1000 | Loss: 0.00138126
Iteration 45/1000 | Loss: 0.00029114
Iteration 46/1000 | Loss: 0.00091710
Iteration 47/1000 | Loss: 0.00107475
Iteration 48/1000 | Loss: 0.00127797
Iteration 49/1000 | Loss: 0.00063406
Iteration 50/1000 | Loss: 0.00051143
Iteration 51/1000 | Loss: 0.00006345
Iteration 52/1000 | Loss: 0.00005685
Iteration 53/1000 | Loss: 0.00039061
Iteration 54/1000 | Loss: 0.00005266
Iteration 55/1000 | Loss: 0.00004905
Iteration 56/1000 | Loss: 0.00010245
Iteration 57/1000 | Loss: 0.00004482
Iteration 58/1000 | Loss: 0.00004360
Iteration 59/1000 | Loss: 0.00004189
Iteration 60/1000 | Loss: 0.00062285
Iteration 61/1000 | Loss: 0.00049879
Iteration 62/1000 | Loss: 0.00006663
Iteration 63/1000 | Loss: 0.00006604
Iteration 64/1000 | Loss: 0.00005800
Iteration 65/1000 | Loss: 0.00004576
Iteration 66/1000 | Loss: 0.00004265
Iteration 67/1000 | Loss: 0.00004012
Iteration 68/1000 | Loss: 0.00004537
Iteration 69/1000 | Loss: 0.00003717
Iteration 70/1000 | Loss: 0.00008977
Iteration 71/1000 | Loss: 0.00003385
Iteration 72/1000 | Loss: 0.00003264
Iteration 73/1000 | Loss: 0.00003639
Iteration 74/1000 | Loss: 0.00004077
Iteration 75/1000 | Loss: 0.00003157
Iteration 76/1000 | Loss: 0.00003124
Iteration 77/1000 | Loss: 0.00003105
Iteration 78/1000 | Loss: 0.00003097
Iteration 79/1000 | Loss: 0.00003094
Iteration 80/1000 | Loss: 0.00003094
Iteration 81/1000 | Loss: 0.00003093
Iteration 82/1000 | Loss: 0.00003093
Iteration 83/1000 | Loss: 0.00003092
Iteration 84/1000 | Loss: 0.00003089
Iteration 85/1000 | Loss: 0.00003087
Iteration 86/1000 | Loss: 0.00003086
Iteration 87/1000 | Loss: 0.00003086
Iteration 88/1000 | Loss: 0.00003086
Iteration 89/1000 | Loss: 0.00003086
Iteration 90/1000 | Loss: 0.00003085
Iteration 91/1000 | Loss: 0.00003085
Iteration 92/1000 | Loss: 0.00003085
Iteration 93/1000 | Loss: 0.00003084
Iteration 94/1000 | Loss: 0.00003084
Iteration 95/1000 | Loss: 0.00003083
Iteration 96/1000 | Loss: 0.00003083
Iteration 97/1000 | Loss: 0.00003082
Iteration 98/1000 | Loss: 0.00003082
Iteration 99/1000 | Loss: 0.00003082
Iteration 100/1000 | Loss: 0.00003082
Iteration 101/1000 | Loss: 0.00003081
Iteration 102/1000 | Loss: 0.00003081
Iteration 103/1000 | Loss: 0.00003081
Iteration 104/1000 | Loss: 0.00003081
Iteration 105/1000 | Loss: 0.00003081
Iteration 106/1000 | Loss: 0.00003081
Iteration 107/1000 | Loss: 0.00003081
Iteration 108/1000 | Loss: 0.00003080
Iteration 109/1000 | Loss: 0.00003080
Iteration 110/1000 | Loss: 0.00003080
Iteration 111/1000 | Loss: 0.00003080
Iteration 112/1000 | Loss: 0.00003079
Iteration 113/1000 | Loss: 0.00003079
Iteration 114/1000 | Loss: 0.00003078
Iteration 115/1000 | Loss: 0.00003078
Iteration 116/1000 | Loss: 0.00003077
Iteration 117/1000 | Loss: 0.00003077
Iteration 118/1000 | Loss: 0.00003077
Iteration 119/1000 | Loss: 0.00003077
Iteration 120/1000 | Loss: 0.00003076
Iteration 121/1000 | Loss: 0.00003076
Iteration 122/1000 | Loss: 0.00003076
Iteration 123/1000 | Loss: 0.00003076
Iteration 124/1000 | Loss: 0.00003075
Iteration 125/1000 | Loss: 0.00003075
Iteration 126/1000 | Loss: 0.00003075
Iteration 127/1000 | Loss: 0.00003075
Iteration 128/1000 | Loss: 0.00003075
Iteration 129/1000 | Loss: 0.00003075
Iteration 130/1000 | Loss: 0.00003074
Iteration 131/1000 | Loss: 0.00003074
Iteration 132/1000 | Loss: 0.00003074
Iteration 133/1000 | Loss: 0.00003074
Iteration 134/1000 | Loss: 0.00003074
Iteration 135/1000 | Loss: 0.00003073
Iteration 136/1000 | Loss: 0.00003073
Iteration 137/1000 | Loss: 0.00003073
Iteration 138/1000 | Loss: 0.00003073
Iteration 139/1000 | Loss: 0.00003073
Iteration 140/1000 | Loss: 0.00003073
Iteration 141/1000 | Loss: 0.00003073
Iteration 142/1000 | Loss: 0.00003073
Iteration 143/1000 | Loss: 0.00003072
Iteration 144/1000 | Loss: 0.00003072
Iteration 145/1000 | Loss: 0.00003072
Iteration 146/1000 | Loss: 0.00003072
Iteration 147/1000 | Loss: 0.00003072
Iteration 148/1000 | Loss: 0.00003071
Iteration 149/1000 | Loss: 0.00003071
Iteration 150/1000 | Loss: 0.00003071
Iteration 151/1000 | Loss: 0.00003071
Iteration 152/1000 | Loss: 0.00003071
Iteration 153/1000 | Loss: 0.00003070
Iteration 154/1000 | Loss: 0.00003070
Iteration 155/1000 | Loss: 0.00003070
Iteration 156/1000 | Loss: 0.00003070
Iteration 157/1000 | Loss: 0.00003070
Iteration 158/1000 | Loss: 0.00003070
Iteration 159/1000 | Loss: 0.00003070
Iteration 160/1000 | Loss: 0.00003070
Iteration 161/1000 | Loss: 0.00003070
Iteration 162/1000 | Loss: 0.00003070
Iteration 163/1000 | Loss: 0.00003070
Iteration 164/1000 | Loss: 0.00003070
Iteration 165/1000 | Loss: 0.00003070
Iteration 166/1000 | Loss: 0.00003070
Iteration 167/1000 | Loss: 0.00003070
Iteration 168/1000 | Loss: 0.00003070
Iteration 169/1000 | Loss: 0.00003070
Iteration 170/1000 | Loss: 0.00003070
Iteration 171/1000 | Loss: 0.00003070
Iteration 172/1000 | Loss: 0.00003070
Iteration 173/1000 | Loss: 0.00003070
Iteration 174/1000 | Loss: 0.00003070
Iteration 175/1000 | Loss: 0.00003070
Iteration 176/1000 | Loss: 0.00003070
Iteration 177/1000 | Loss: 0.00003070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [3.0695886380271986e-05, 3.0695886380271986e-05, 3.0695886380271986e-05, 3.0695886380271986e-05, 3.0695886380271986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0695886380271986e-05

Optimization complete. Final v2v error: 4.308376312255859 mm

Highest mean error: 10.978898048400879 mm for frame 14

Lowest mean error: 3.2106478214263916 mm for frame 10

Saving results

Total time: 165.28699564933777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849045
Iteration 2/25 | Loss: 0.00132738
Iteration 3/25 | Loss: 0.00101091
Iteration 4/25 | Loss: 0.00092268
Iteration 5/25 | Loss: 0.00089154
Iteration 6/25 | Loss: 0.00088648
Iteration 7/25 | Loss: 0.00088543
Iteration 8/25 | Loss: 0.00088543
Iteration 9/25 | Loss: 0.00088543
Iteration 10/25 | Loss: 0.00088543
Iteration 11/25 | Loss: 0.00088543
Iteration 12/25 | Loss: 0.00088543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008854347979649901, 0.0008854347979649901, 0.0008854347979649901, 0.0008854347979649901, 0.0008854347979649901]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008854347979649901

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60125721
Iteration 2/25 | Loss: 0.00140658
Iteration 3/25 | Loss: 0.00140658
Iteration 4/25 | Loss: 0.00140658
Iteration 5/25 | Loss: 0.00140658
Iteration 6/25 | Loss: 0.00140658
Iteration 7/25 | Loss: 0.00140658
Iteration 8/25 | Loss: 0.00140658
Iteration 9/25 | Loss: 0.00140658
Iteration 10/25 | Loss: 0.00140658
Iteration 11/25 | Loss: 0.00140658
Iteration 12/25 | Loss: 0.00140658
Iteration 13/25 | Loss: 0.00140658
Iteration 14/25 | Loss: 0.00140658
Iteration 15/25 | Loss: 0.00140658
Iteration 16/25 | Loss: 0.00140658
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014065756695345044, 0.0014065756695345044, 0.0014065756695345044, 0.0014065756695345044, 0.0014065756695345044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014065756695345044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140658
Iteration 2/1000 | Loss: 0.00004217
Iteration 3/1000 | Loss: 0.00002728
Iteration 4/1000 | Loss: 0.00002505
Iteration 5/1000 | Loss: 0.00002369
Iteration 6/1000 | Loss: 0.00002301
Iteration 7/1000 | Loss: 0.00002241
Iteration 8/1000 | Loss: 0.00002206
Iteration 9/1000 | Loss: 0.00002171
Iteration 10/1000 | Loss: 0.00002149
Iteration 11/1000 | Loss: 0.00002140
Iteration 12/1000 | Loss: 0.00002140
Iteration 13/1000 | Loss: 0.00002134
Iteration 14/1000 | Loss: 0.00002131
Iteration 15/1000 | Loss: 0.00002130
Iteration 16/1000 | Loss: 0.00002130
Iteration 17/1000 | Loss: 0.00002129
Iteration 18/1000 | Loss: 0.00002124
Iteration 19/1000 | Loss: 0.00002119
Iteration 20/1000 | Loss: 0.00002119
Iteration 21/1000 | Loss: 0.00002119
Iteration 22/1000 | Loss: 0.00002119
Iteration 23/1000 | Loss: 0.00002119
Iteration 24/1000 | Loss: 0.00002118
Iteration 25/1000 | Loss: 0.00002117
Iteration 26/1000 | Loss: 0.00002117
Iteration 27/1000 | Loss: 0.00002117
Iteration 28/1000 | Loss: 0.00002117
Iteration 29/1000 | Loss: 0.00002117
Iteration 30/1000 | Loss: 0.00002117
Iteration 31/1000 | Loss: 0.00002117
Iteration 32/1000 | Loss: 0.00002117
Iteration 33/1000 | Loss: 0.00002117
Iteration 34/1000 | Loss: 0.00002116
Iteration 35/1000 | Loss: 0.00002116
Iteration 36/1000 | Loss: 0.00002116
Iteration 37/1000 | Loss: 0.00002116
Iteration 38/1000 | Loss: 0.00002116
Iteration 39/1000 | Loss: 0.00002116
Iteration 40/1000 | Loss: 0.00002116
Iteration 41/1000 | Loss: 0.00002116
Iteration 42/1000 | Loss: 0.00002116
Iteration 43/1000 | Loss: 0.00002115
Iteration 44/1000 | Loss: 0.00002115
Iteration 45/1000 | Loss: 0.00002115
Iteration 46/1000 | Loss: 0.00002115
Iteration 47/1000 | Loss: 0.00002115
Iteration 48/1000 | Loss: 0.00002115
Iteration 49/1000 | Loss: 0.00002115
Iteration 50/1000 | Loss: 0.00002115
Iteration 51/1000 | Loss: 0.00002115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 51. Stopping optimization.
Last 5 losses: [2.1154568457859568e-05, 2.1154568457859568e-05, 2.1154568457859568e-05, 2.1154568457859568e-05, 2.1154568457859568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1154568457859568e-05

Optimization complete. Final v2v error: 3.790473222732544 mm

Highest mean error: 4.154966831207275 mm for frame 210

Lowest mean error: 3.57112193107605 mm for frame 237

Saving results

Total time: 55.85474491119385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395342
Iteration 2/25 | Loss: 0.00097197
Iteration 3/25 | Loss: 0.00079228
Iteration 4/25 | Loss: 0.00076561
Iteration 5/25 | Loss: 0.00075791
Iteration 6/25 | Loss: 0.00075512
Iteration 7/25 | Loss: 0.00075427
Iteration 8/25 | Loss: 0.00075427
Iteration 9/25 | Loss: 0.00075427
Iteration 10/25 | Loss: 0.00075427
Iteration 11/25 | Loss: 0.00075427
Iteration 12/25 | Loss: 0.00075427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007542741950601339, 0.0007542741950601339, 0.0007542741950601339, 0.0007542741950601339, 0.0007542741950601339]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007542741950601339

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58442390
Iteration 2/25 | Loss: 0.00123336
Iteration 3/25 | Loss: 0.00123335
Iteration 4/25 | Loss: 0.00123335
Iteration 5/25 | Loss: 0.00123335
Iteration 6/25 | Loss: 0.00123335
Iteration 7/25 | Loss: 0.00123335
Iteration 8/25 | Loss: 0.00123335
Iteration 9/25 | Loss: 0.00123335
Iteration 10/25 | Loss: 0.00123335
Iteration 11/25 | Loss: 0.00123335
Iteration 12/25 | Loss: 0.00123335
Iteration 13/25 | Loss: 0.00123335
Iteration 14/25 | Loss: 0.00123335
Iteration 15/25 | Loss: 0.00123335
Iteration 16/25 | Loss: 0.00123335
Iteration 17/25 | Loss: 0.00123335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001233351300470531, 0.001233351300470531, 0.001233351300470531, 0.001233351300470531, 0.001233351300470531]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001233351300470531

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123335
Iteration 2/1000 | Loss: 0.00002464
Iteration 3/1000 | Loss: 0.00001835
Iteration 4/1000 | Loss: 0.00001701
Iteration 5/1000 | Loss: 0.00001589
Iteration 6/1000 | Loss: 0.00001547
Iteration 7/1000 | Loss: 0.00001514
Iteration 8/1000 | Loss: 0.00001501
Iteration 9/1000 | Loss: 0.00001485
Iteration 10/1000 | Loss: 0.00001466
Iteration 11/1000 | Loss: 0.00001453
Iteration 12/1000 | Loss: 0.00001452
Iteration 13/1000 | Loss: 0.00001451
Iteration 14/1000 | Loss: 0.00001450
Iteration 15/1000 | Loss: 0.00001449
Iteration 16/1000 | Loss: 0.00001449
Iteration 17/1000 | Loss: 0.00001442
Iteration 18/1000 | Loss: 0.00001438
Iteration 19/1000 | Loss: 0.00001438
Iteration 20/1000 | Loss: 0.00001438
Iteration 21/1000 | Loss: 0.00001437
Iteration 22/1000 | Loss: 0.00001437
Iteration 23/1000 | Loss: 0.00001437
Iteration 24/1000 | Loss: 0.00001437
Iteration 25/1000 | Loss: 0.00001437
Iteration 26/1000 | Loss: 0.00001437
Iteration 27/1000 | Loss: 0.00001436
Iteration 28/1000 | Loss: 0.00001436
Iteration 29/1000 | Loss: 0.00001436
Iteration 30/1000 | Loss: 0.00001436
Iteration 31/1000 | Loss: 0.00001436
Iteration 32/1000 | Loss: 0.00001436
Iteration 33/1000 | Loss: 0.00001436
Iteration 34/1000 | Loss: 0.00001436
Iteration 35/1000 | Loss: 0.00001436
Iteration 36/1000 | Loss: 0.00001436
Iteration 37/1000 | Loss: 0.00001436
Iteration 38/1000 | Loss: 0.00001435
Iteration 39/1000 | Loss: 0.00001435
Iteration 40/1000 | Loss: 0.00001435
Iteration 41/1000 | Loss: 0.00001435
Iteration 42/1000 | Loss: 0.00001435
Iteration 43/1000 | Loss: 0.00001435
Iteration 44/1000 | Loss: 0.00001434
Iteration 45/1000 | Loss: 0.00001434
Iteration 46/1000 | Loss: 0.00001434
Iteration 47/1000 | Loss: 0.00001434
Iteration 48/1000 | Loss: 0.00001434
Iteration 49/1000 | Loss: 0.00001433
Iteration 50/1000 | Loss: 0.00001433
Iteration 51/1000 | Loss: 0.00001433
Iteration 52/1000 | Loss: 0.00001432
Iteration 53/1000 | Loss: 0.00001432
Iteration 54/1000 | Loss: 0.00001432
Iteration 55/1000 | Loss: 0.00001432
Iteration 56/1000 | Loss: 0.00001432
Iteration 57/1000 | Loss: 0.00001432
Iteration 58/1000 | Loss: 0.00001432
Iteration 59/1000 | Loss: 0.00001432
Iteration 60/1000 | Loss: 0.00001432
Iteration 61/1000 | Loss: 0.00001432
Iteration 62/1000 | Loss: 0.00001432
Iteration 63/1000 | Loss: 0.00001431
Iteration 64/1000 | Loss: 0.00001431
Iteration 65/1000 | Loss: 0.00001431
Iteration 66/1000 | Loss: 0.00001431
Iteration 67/1000 | Loss: 0.00001430
Iteration 68/1000 | Loss: 0.00001430
Iteration 69/1000 | Loss: 0.00001430
Iteration 70/1000 | Loss: 0.00001430
Iteration 71/1000 | Loss: 0.00001430
Iteration 72/1000 | Loss: 0.00001430
Iteration 73/1000 | Loss: 0.00001429
Iteration 74/1000 | Loss: 0.00001429
Iteration 75/1000 | Loss: 0.00001428
Iteration 76/1000 | Loss: 0.00001428
Iteration 77/1000 | Loss: 0.00001428
Iteration 78/1000 | Loss: 0.00001428
Iteration 79/1000 | Loss: 0.00001428
Iteration 80/1000 | Loss: 0.00001427
Iteration 81/1000 | Loss: 0.00001427
Iteration 82/1000 | Loss: 0.00001427
Iteration 83/1000 | Loss: 0.00001427
Iteration 84/1000 | Loss: 0.00001426
Iteration 85/1000 | Loss: 0.00001426
Iteration 86/1000 | Loss: 0.00001425
Iteration 87/1000 | Loss: 0.00001425
Iteration 88/1000 | Loss: 0.00001424
Iteration 89/1000 | Loss: 0.00001424
Iteration 90/1000 | Loss: 0.00001424
Iteration 91/1000 | Loss: 0.00001424
Iteration 92/1000 | Loss: 0.00001424
Iteration 93/1000 | Loss: 0.00001423
Iteration 94/1000 | Loss: 0.00001423
Iteration 95/1000 | Loss: 0.00001422
Iteration 96/1000 | Loss: 0.00001422
Iteration 97/1000 | Loss: 0.00001422
Iteration 98/1000 | Loss: 0.00001422
Iteration 99/1000 | Loss: 0.00001422
Iteration 100/1000 | Loss: 0.00001422
Iteration 101/1000 | Loss: 0.00001422
Iteration 102/1000 | Loss: 0.00001422
Iteration 103/1000 | Loss: 0.00001421
Iteration 104/1000 | Loss: 0.00001421
Iteration 105/1000 | Loss: 0.00001421
Iteration 106/1000 | Loss: 0.00001421
Iteration 107/1000 | Loss: 0.00001421
Iteration 108/1000 | Loss: 0.00001421
Iteration 109/1000 | Loss: 0.00001421
Iteration 110/1000 | Loss: 0.00001421
Iteration 111/1000 | Loss: 0.00001420
Iteration 112/1000 | Loss: 0.00001420
Iteration 113/1000 | Loss: 0.00001420
Iteration 114/1000 | Loss: 0.00001420
Iteration 115/1000 | Loss: 0.00001420
Iteration 116/1000 | Loss: 0.00001420
Iteration 117/1000 | Loss: 0.00001420
Iteration 118/1000 | Loss: 0.00001420
Iteration 119/1000 | Loss: 0.00001420
Iteration 120/1000 | Loss: 0.00001420
Iteration 121/1000 | Loss: 0.00001419
Iteration 122/1000 | Loss: 0.00001419
Iteration 123/1000 | Loss: 0.00001419
Iteration 124/1000 | Loss: 0.00001419
Iteration 125/1000 | Loss: 0.00001419
Iteration 126/1000 | Loss: 0.00001418
Iteration 127/1000 | Loss: 0.00001418
Iteration 128/1000 | Loss: 0.00001418
Iteration 129/1000 | Loss: 0.00001418
Iteration 130/1000 | Loss: 0.00001418
Iteration 131/1000 | Loss: 0.00001418
Iteration 132/1000 | Loss: 0.00001418
Iteration 133/1000 | Loss: 0.00001418
Iteration 134/1000 | Loss: 0.00001418
Iteration 135/1000 | Loss: 0.00001417
Iteration 136/1000 | Loss: 0.00001417
Iteration 137/1000 | Loss: 0.00001417
Iteration 138/1000 | Loss: 0.00001417
Iteration 139/1000 | Loss: 0.00001417
Iteration 140/1000 | Loss: 0.00001417
Iteration 141/1000 | Loss: 0.00001417
Iteration 142/1000 | Loss: 0.00001417
Iteration 143/1000 | Loss: 0.00001417
Iteration 144/1000 | Loss: 0.00001417
Iteration 145/1000 | Loss: 0.00001417
Iteration 146/1000 | Loss: 0.00001417
Iteration 147/1000 | Loss: 0.00001417
Iteration 148/1000 | Loss: 0.00001416
Iteration 149/1000 | Loss: 0.00001416
Iteration 150/1000 | Loss: 0.00001416
Iteration 151/1000 | Loss: 0.00001416
Iteration 152/1000 | Loss: 0.00001416
Iteration 153/1000 | Loss: 0.00001416
Iteration 154/1000 | Loss: 0.00001416
Iteration 155/1000 | Loss: 0.00001416
Iteration 156/1000 | Loss: 0.00001416
Iteration 157/1000 | Loss: 0.00001416
Iteration 158/1000 | Loss: 0.00001415
Iteration 159/1000 | Loss: 0.00001415
Iteration 160/1000 | Loss: 0.00001415
Iteration 161/1000 | Loss: 0.00001415
Iteration 162/1000 | Loss: 0.00001415
Iteration 163/1000 | Loss: 0.00001415
Iteration 164/1000 | Loss: 0.00001415
Iteration 165/1000 | Loss: 0.00001415
Iteration 166/1000 | Loss: 0.00001415
Iteration 167/1000 | Loss: 0.00001415
Iteration 168/1000 | Loss: 0.00001415
Iteration 169/1000 | Loss: 0.00001415
Iteration 170/1000 | Loss: 0.00001415
Iteration 171/1000 | Loss: 0.00001415
Iteration 172/1000 | Loss: 0.00001415
Iteration 173/1000 | Loss: 0.00001415
Iteration 174/1000 | Loss: 0.00001415
Iteration 175/1000 | Loss: 0.00001415
Iteration 176/1000 | Loss: 0.00001415
Iteration 177/1000 | Loss: 0.00001415
Iteration 178/1000 | Loss: 0.00001415
Iteration 179/1000 | Loss: 0.00001415
Iteration 180/1000 | Loss: 0.00001415
Iteration 181/1000 | Loss: 0.00001415
Iteration 182/1000 | Loss: 0.00001415
Iteration 183/1000 | Loss: 0.00001415
Iteration 184/1000 | Loss: 0.00001415
Iteration 185/1000 | Loss: 0.00001415
Iteration 186/1000 | Loss: 0.00001415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.4151491086522583e-05, 1.4151491086522583e-05, 1.4151491086522583e-05, 1.4151491086522583e-05, 1.4151491086522583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4151491086522583e-05

Optimization complete. Final v2v error: 3.1705570220947266 mm

Highest mean error: 3.5161237716674805 mm for frame 105

Lowest mean error: 2.899998188018799 mm for frame 2

Saving results

Total time: 50.03592920303345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409736
Iteration 2/25 | Loss: 0.00102974
Iteration 3/25 | Loss: 0.00082544
Iteration 4/25 | Loss: 0.00081361
Iteration 5/25 | Loss: 0.00080601
Iteration 6/25 | Loss: 0.00080416
Iteration 7/25 | Loss: 0.00080412
Iteration 8/25 | Loss: 0.00080412
Iteration 9/25 | Loss: 0.00080412
Iteration 10/25 | Loss: 0.00080412
Iteration 11/25 | Loss: 0.00080412
Iteration 12/25 | Loss: 0.00080412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000804124865680933, 0.000804124865680933, 0.000804124865680933, 0.000804124865680933, 0.000804124865680933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000804124865680933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78850758
Iteration 2/25 | Loss: 0.00156876
Iteration 3/25 | Loss: 0.00156875
Iteration 4/25 | Loss: 0.00156875
Iteration 5/25 | Loss: 0.00156875
Iteration 6/25 | Loss: 0.00156875
Iteration 7/25 | Loss: 0.00156875
Iteration 8/25 | Loss: 0.00156875
Iteration 9/25 | Loss: 0.00156875
Iteration 10/25 | Loss: 0.00156875
Iteration 11/25 | Loss: 0.00156875
Iteration 12/25 | Loss: 0.00156875
Iteration 13/25 | Loss: 0.00156875
Iteration 14/25 | Loss: 0.00156875
Iteration 15/25 | Loss: 0.00156875
Iteration 16/25 | Loss: 0.00156875
Iteration 17/25 | Loss: 0.00156875
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00156875210814178, 0.00156875210814178, 0.00156875210814178, 0.00156875210814178, 0.00156875210814178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00156875210814178

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156875
Iteration 2/1000 | Loss: 0.00002895
Iteration 3/1000 | Loss: 0.00002038
Iteration 4/1000 | Loss: 0.00001684
Iteration 5/1000 | Loss: 0.00001552
Iteration 6/1000 | Loss: 0.00001482
Iteration 7/1000 | Loss: 0.00001436
Iteration 8/1000 | Loss: 0.00001416
Iteration 9/1000 | Loss: 0.00001394
Iteration 10/1000 | Loss: 0.00001367
Iteration 11/1000 | Loss: 0.00001363
Iteration 12/1000 | Loss: 0.00001344
Iteration 13/1000 | Loss: 0.00001333
Iteration 14/1000 | Loss: 0.00001323
Iteration 15/1000 | Loss: 0.00001318
Iteration 16/1000 | Loss: 0.00001318
Iteration 17/1000 | Loss: 0.00001313
Iteration 18/1000 | Loss: 0.00001313
Iteration 19/1000 | Loss: 0.00001309
Iteration 20/1000 | Loss: 0.00001307
Iteration 21/1000 | Loss: 0.00001306
Iteration 22/1000 | Loss: 0.00001305
Iteration 23/1000 | Loss: 0.00001305
Iteration 24/1000 | Loss: 0.00001305
Iteration 25/1000 | Loss: 0.00001305
Iteration 26/1000 | Loss: 0.00001304
Iteration 27/1000 | Loss: 0.00001304
Iteration 28/1000 | Loss: 0.00001304
Iteration 29/1000 | Loss: 0.00001303
Iteration 30/1000 | Loss: 0.00001303
Iteration 31/1000 | Loss: 0.00001303
Iteration 32/1000 | Loss: 0.00001303
Iteration 33/1000 | Loss: 0.00001303
Iteration 34/1000 | Loss: 0.00001302
Iteration 35/1000 | Loss: 0.00001302
Iteration 36/1000 | Loss: 0.00001302
Iteration 37/1000 | Loss: 0.00001302
Iteration 38/1000 | Loss: 0.00001302
Iteration 39/1000 | Loss: 0.00001302
Iteration 40/1000 | Loss: 0.00001302
Iteration 41/1000 | Loss: 0.00001302
Iteration 42/1000 | Loss: 0.00001302
Iteration 43/1000 | Loss: 0.00001301
Iteration 44/1000 | Loss: 0.00001301
Iteration 45/1000 | Loss: 0.00001301
Iteration 46/1000 | Loss: 0.00001301
Iteration 47/1000 | Loss: 0.00001301
Iteration 48/1000 | Loss: 0.00001301
Iteration 49/1000 | Loss: 0.00001300
Iteration 50/1000 | Loss: 0.00001300
Iteration 51/1000 | Loss: 0.00001300
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001299
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001298
Iteration 57/1000 | Loss: 0.00001298
Iteration 58/1000 | Loss: 0.00001298
Iteration 59/1000 | Loss: 0.00001298
Iteration 60/1000 | Loss: 0.00001298
Iteration 61/1000 | Loss: 0.00001297
Iteration 62/1000 | Loss: 0.00001297
Iteration 63/1000 | Loss: 0.00001297
Iteration 64/1000 | Loss: 0.00001297
Iteration 65/1000 | Loss: 0.00001297
Iteration 66/1000 | Loss: 0.00001297
Iteration 67/1000 | Loss: 0.00001297
Iteration 68/1000 | Loss: 0.00001296
Iteration 69/1000 | Loss: 0.00001296
Iteration 70/1000 | Loss: 0.00001296
Iteration 71/1000 | Loss: 0.00001296
Iteration 72/1000 | Loss: 0.00001296
Iteration 73/1000 | Loss: 0.00001296
Iteration 74/1000 | Loss: 0.00001296
Iteration 75/1000 | Loss: 0.00001296
Iteration 76/1000 | Loss: 0.00001296
Iteration 77/1000 | Loss: 0.00001296
Iteration 78/1000 | Loss: 0.00001296
Iteration 79/1000 | Loss: 0.00001296
Iteration 80/1000 | Loss: 0.00001296
Iteration 81/1000 | Loss: 0.00001295
Iteration 82/1000 | Loss: 0.00001295
Iteration 83/1000 | Loss: 0.00001295
Iteration 84/1000 | Loss: 0.00001295
Iteration 85/1000 | Loss: 0.00001294
Iteration 86/1000 | Loss: 0.00001294
Iteration 87/1000 | Loss: 0.00001294
Iteration 88/1000 | Loss: 0.00001294
Iteration 89/1000 | Loss: 0.00001294
Iteration 90/1000 | Loss: 0.00001294
Iteration 91/1000 | Loss: 0.00001294
Iteration 92/1000 | Loss: 0.00001294
Iteration 93/1000 | Loss: 0.00001294
Iteration 94/1000 | Loss: 0.00001294
Iteration 95/1000 | Loss: 0.00001294
Iteration 96/1000 | Loss: 0.00001294
Iteration 97/1000 | Loss: 0.00001294
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001294
Iteration 100/1000 | Loss: 0.00001294
Iteration 101/1000 | Loss: 0.00001294
Iteration 102/1000 | Loss: 0.00001294
Iteration 103/1000 | Loss: 0.00001294
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001294
Iteration 106/1000 | Loss: 0.00001294
Iteration 107/1000 | Loss: 0.00001294
Iteration 108/1000 | Loss: 0.00001294
Iteration 109/1000 | Loss: 0.00001294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.2942141438543331e-05, 1.2942141438543331e-05, 1.2942141438543331e-05, 1.2942141438543331e-05, 1.2942141438543331e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2942141438543331e-05

Optimization complete. Final v2v error: 3.0701141357421875 mm

Highest mean error: 3.2283833026885986 mm for frame 93

Lowest mean error: 2.9261884689331055 mm for frame 188

Saving results

Total time: 40.256508588790894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952630
Iteration 2/25 | Loss: 0.00151378
Iteration 3/25 | Loss: 0.00132490
Iteration 4/25 | Loss: 0.00127423
Iteration 5/25 | Loss: 0.00125626
Iteration 6/25 | Loss: 0.00125383
Iteration 7/25 | Loss: 0.00125026
Iteration 8/25 | Loss: 0.00124624
Iteration 9/25 | Loss: 0.00124891
Iteration 10/25 | Loss: 0.00124489
Iteration 11/25 | Loss: 0.00124236
Iteration 12/25 | Loss: 0.00124042
Iteration 13/25 | Loss: 0.00123980
Iteration 14/25 | Loss: 0.00124035
Iteration 15/25 | Loss: 0.00123921
Iteration 16/25 | Loss: 0.00123805
Iteration 17/25 | Loss: 0.00123737
Iteration 18/25 | Loss: 0.00123704
Iteration 19/25 | Loss: 0.00123694
Iteration 20/25 | Loss: 0.00123684
Iteration 21/25 | Loss: 0.00123679
Iteration 22/25 | Loss: 0.00123679
Iteration 23/25 | Loss: 0.00123679
Iteration 24/25 | Loss: 0.00123679
Iteration 25/25 | Loss: 0.00123679

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49020004
Iteration 2/25 | Loss: 0.00371541
Iteration 3/25 | Loss: 0.00371530
Iteration 4/25 | Loss: 0.00371530
Iteration 5/25 | Loss: 0.00371530
Iteration 6/25 | Loss: 0.00371530
Iteration 7/25 | Loss: 0.00371530
Iteration 8/25 | Loss: 0.00371530
Iteration 9/25 | Loss: 0.00371530
Iteration 10/25 | Loss: 0.00371530
Iteration 11/25 | Loss: 0.00371530
Iteration 12/25 | Loss: 0.00371530
Iteration 13/25 | Loss: 0.00371530
Iteration 14/25 | Loss: 0.00371530
Iteration 15/25 | Loss: 0.00371530
Iteration 16/25 | Loss: 0.00371530
Iteration 17/25 | Loss: 0.00371530
Iteration 18/25 | Loss: 0.00371530
Iteration 19/25 | Loss: 0.00371530
Iteration 20/25 | Loss: 0.00371530
Iteration 21/25 | Loss: 0.00371530
Iteration 22/25 | Loss: 0.00371530
Iteration 23/25 | Loss: 0.00371530
Iteration 24/25 | Loss: 0.00371530
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0037152976728975773, 0.0037152976728975773, 0.0037152976728975773, 0.0037152976728975773, 0.0037152976728975773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037152976728975773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00371530
Iteration 2/1000 | Loss: 0.00048407
Iteration 3/1000 | Loss: 0.00031031
Iteration 4/1000 | Loss: 0.00113313
Iteration 5/1000 | Loss: 0.00177419
Iteration 6/1000 | Loss: 0.00197219
Iteration 7/1000 | Loss: 0.00058327
Iteration 8/1000 | Loss: 0.00397165
Iteration 9/1000 | Loss: 0.00047112
Iteration 10/1000 | Loss: 0.00157066
Iteration 11/1000 | Loss: 0.00479909
Iteration 12/1000 | Loss: 0.00116399
Iteration 13/1000 | Loss: 0.00207489
Iteration 14/1000 | Loss: 0.00018316
Iteration 15/1000 | Loss: 0.00501781
Iteration 16/1000 | Loss: 0.01705162
Iteration 17/1000 | Loss: 0.00928738
Iteration 18/1000 | Loss: 0.00046343
Iteration 19/1000 | Loss: 0.00013746
Iteration 20/1000 | Loss: 0.00054186
Iteration 21/1000 | Loss: 0.00455714
Iteration 22/1000 | Loss: 0.01062123
Iteration 23/1000 | Loss: 0.00298288
Iteration 24/1000 | Loss: 0.00066759
Iteration 25/1000 | Loss: 0.00350406
Iteration 26/1000 | Loss: 0.00024750
Iteration 27/1000 | Loss: 0.00013962
Iteration 28/1000 | Loss: 0.00225821
Iteration 29/1000 | Loss: 0.00377064
Iteration 30/1000 | Loss: 0.00047569
Iteration 31/1000 | Loss: 0.00008871
Iteration 32/1000 | Loss: 0.00006595
Iteration 33/1000 | Loss: 0.00005577
Iteration 34/1000 | Loss: 0.00004770
Iteration 35/1000 | Loss: 0.00076530
Iteration 36/1000 | Loss: 0.00004673
Iteration 37/1000 | Loss: 0.00004187
Iteration 38/1000 | Loss: 0.00063459
Iteration 39/1000 | Loss: 0.00005594
Iteration 40/1000 | Loss: 0.00004067
Iteration 41/1000 | Loss: 0.00003824
Iteration 42/1000 | Loss: 0.00003704
Iteration 43/1000 | Loss: 0.00003543
Iteration 44/1000 | Loss: 0.00003414
Iteration 45/1000 | Loss: 0.00021806
Iteration 46/1000 | Loss: 0.00005298
Iteration 47/1000 | Loss: 0.00003544
Iteration 48/1000 | Loss: 0.00003353
Iteration 49/1000 | Loss: 0.00003202
Iteration 50/1000 | Loss: 0.00003118
Iteration 51/1000 | Loss: 0.00003077
Iteration 52/1000 | Loss: 0.00003051
Iteration 53/1000 | Loss: 0.00003029
Iteration 54/1000 | Loss: 0.00003017
Iteration 55/1000 | Loss: 0.00003015
Iteration 56/1000 | Loss: 0.00003014
Iteration 57/1000 | Loss: 0.00003010
Iteration 58/1000 | Loss: 0.00003009
Iteration 59/1000 | Loss: 0.00003005
Iteration 60/1000 | Loss: 0.00003004
Iteration 61/1000 | Loss: 0.00003004
Iteration 62/1000 | Loss: 0.00003003
Iteration 63/1000 | Loss: 0.00003003
Iteration 64/1000 | Loss: 0.00003003
Iteration 65/1000 | Loss: 0.00003003
Iteration 66/1000 | Loss: 0.00003002
Iteration 67/1000 | Loss: 0.00003002
Iteration 68/1000 | Loss: 0.00003002
Iteration 69/1000 | Loss: 0.00002998
Iteration 70/1000 | Loss: 0.00002997
Iteration 71/1000 | Loss: 0.00002994
Iteration 72/1000 | Loss: 0.00002993
Iteration 73/1000 | Loss: 0.00002992
Iteration 74/1000 | Loss: 0.00002992
Iteration 75/1000 | Loss: 0.00002987
Iteration 76/1000 | Loss: 0.00002983
Iteration 77/1000 | Loss: 0.00002983
Iteration 78/1000 | Loss: 0.00002982
Iteration 79/1000 | Loss: 0.00002981
Iteration 80/1000 | Loss: 0.00002975
Iteration 81/1000 | Loss: 0.00002975
Iteration 82/1000 | Loss: 0.00002973
Iteration 83/1000 | Loss: 0.00002973
Iteration 84/1000 | Loss: 0.00002968
Iteration 85/1000 | Loss: 0.00002968
Iteration 86/1000 | Loss: 0.00002966
Iteration 87/1000 | Loss: 0.00002966
Iteration 88/1000 | Loss: 0.00002965
Iteration 89/1000 | Loss: 0.00002965
Iteration 90/1000 | Loss: 0.00002965
Iteration 91/1000 | Loss: 0.00002965
Iteration 92/1000 | Loss: 0.00002964
Iteration 93/1000 | Loss: 0.00002964
Iteration 94/1000 | Loss: 0.00002964
Iteration 95/1000 | Loss: 0.00002964
Iteration 96/1000 | Loss: 0.00002963
Iteration 97/1000 | Loss: 0.00002963
Iteration 98/1000 | Loss: 0.00002963
Iteration 99/1000 | Loss: 0.00002963
Iteration 100/1000 | Loss: 0.00002963
Iteration 101/1000 | Loss: 0.00002963
Iteration 102/1000 | Loss: 0.00002963
Iteration 103/1000 | Loss: 0.00002962
Iteration 104/1000 | Loss: 0.00002962
Iteration 105/1000 | Loss: 0.00002962
Iteration 106/1000 | Loss: 0.00002962
Iteration 107/1000 | Loss: 0.00002961
Iteration 108/1000 | Loss: 0.00002961
Iteration 109/1000 | Loss: 0.00002961
Iteration 110/1000 | Loss: 0.00002960
Iteration 111/1000 | Loss: 0.00002960
Iteration 112/1000 | Loss: 0.00002960
Iteration 113/1000 | Loss: 0.00002960
Iteration 114/1000 | Loss: 0.00002959
Iteration 115/1000 | Loss: 0.00002959
Iteration 116/1000 | Loss: 0.00002959
Iteration 117/1000 | Loss: 0.00002959
Iteration 118/1000 | Loss: 0.00002958
Iteration 119/1000 | Loss: 0.00002958
Iteration 120/1000 | Loss: 0.00002957
Iteration 121/1000 | Loss: 0.00002957
Iteration 122/1000 | Loss: 0.00002957
Iteration 123/1000 | Loss: 0.00002957
Iteration 124/1000 | Loss: 0.00002957
Iteration 125/1000 | Loss: 0.00002957
Iteration 126/1000 | Loss: 0.00002956
Iteration 127/1000 | Loss: 0.00002956
Iteration 128/1000 | Loss: 0.00002956
Iteration 129/1000 | Loss: 0.00002956
Iteration 130/1000 | Loss: 0.00002956
Iteration 131/1000 | Loss: 0.00002956
Iteration 132/1000 | Loss: 0.00002955
Iteration 133/1000 | Loss: 0.00002955
Iteration 134/1000 | Loss: 0.00002955
Iteration 135/1000 | Loss: 0.00002955
Iteration 136/1000 | Loss: 0.00002955
Iteration 137/1000 | Loss: 0.00002955
Iteration 138/1000 | Loss: 0.00002955
Iteration 139/1000 | Loss: 0.00002955
Iteration 140/1000 | Loss: 0.00002955
Iteration 141/1000 | Loss: 0.00002955
Iteration 142/1000 | Loss: 0.00002955
Iteration 143/1000 | Loss: 0.00002954
Iteration 144/1000 | Loss: 0.00002954
Iteration 145/1000 | Loss: 0.00002954
Iteration 146/1000 | Loss: 0.00002954
Iteration 147/1000 | Loss: 0.00002954
Iteration 148/1000 | Loss: 0.00002954
Iteration 149/1000 | Loss: 0.00002953
Iteration 150/1000 | Loss: 0.00002953
Iteration 151/1000 | Loss: 0.00002953
Iteration 152/1000 | Loss: 0.00002953
Iteration 153/1000 | Loss: 0.00002953
Iteration 154/1000 | Loss: 0.00002953
Iteration 155/1000 | Loss: 0.00002953
Iteration 156/1000 | Loss: 0.00002953
Iteration 157/1000 | Loss: 0.00002953
Iteration 158/1000 | Loss: 0.00002953
Iteration 159/1000 | Loss: 0.00002952
Iteration 160/1000 | Loss: 0.00002952
Iteration 161/1000 | Loss: 0.00002952
Iteration 162/1000 | Loss: 0.00002952
Iteration 163/1000 | Loss: 0.00002952
Iteration 164/1000 | Loss: 0.00002952
Iteration 165/1000 | Loss: 0.00002952
Iteration 166/1000 | Loss: 0.00002951
Iteration 167/1000 | Loss: 0.00002951
Iteration 168/1000 | Loss: 0.00002951
Iteration 169/1000 | Loss: 0.00002951
Iteration 170/1000 | Loss: 0.00002951
Iteration 171/1000 | Loss: 0.00002951
Iteration 172/1000 | Loss: 0.00002951
Iteration 173/1000 | Loss: 0.00002951
Iteration 174/1000 | Loss: 0.00002951
Iteration 175/1000 | Loss: 0.00002951
Iteration 176/1000 | Loss: 0.00002950
Iteration 177/1000 | Loss: 0.00002950
Iteration 178/1000 | Loss: 0.00002950
Iteration 179/1000 | Loss: 0.00002950
Iteration 180/1000 | Loss: 0.00002950
Iteration 181/1000 | Loss: 0.00002950
Iteration 182/1000 | Loss: 0.00002949
Iteration 183/1000 | Loss: 0.00002949
Iteration 184/1000 | Loss: 0.00002949
Iteration 185/1000 | Loss: 0.00002949
Iteration 186/1000 | Loss: 0.00002948
Iteration 187/1000 | Loss: 0.00002948
Iteration 188/1000 | Loss: 0.00002948
Iteration 189/1000 | Loss: 0.00002948
Iteration 190/1000 | Loss: 0.00002948
Iteration 191/1000 | Loss: 0.00002947
Iteration 192/1000 | Loss: 0.00002947
Iteration 193/1000 | Loss: 0.00002947
Iteration 194/1000 | Loss: 0.00002947
Iteration 195/1000 | Loss: 0.00002947
Iteration 196/1000 | Loss: 0.00002947
Iteration 197/1000 | Loss: 0.00002947
Iteration 198/1000 | Loss: 0.00002947
Iteration 199/1000 | Loss: 0.00002947
Iteration 200/1000 | Loss: 0.00002947
Iteration 201/1000 | Loss: 0.00002947
Iteration 202/1000 | Loss: 0.00002947
Iteration 203/1000 | Loss: 0.00002947
Iteration 204/1000 | Loss: 0.00002947
Iteration 205/1000 | Loss: 0.00002947
Iteration 206/1000 | Loss: 0.00002947
Iteration 207/1000 | Loss: 0.00002947
Iteration 208/1000 | Loss: 0.00002947
Iteration 209/1000 | Loss: 0.00002947
Iteration 210/1000 | Loss: 0.00002947
Iteration 211/1000 | Loss: 0.00002947
Iteration 212/1000 | Loss: 0.00002947
Iteration 213/1000 | Loss: 0.00002947
Iteration 214/1000 | Loss: 0.00002947
Iteration 215/1000 | Loss: 0.00002947
Iteration 216/1000 | Loss: 0.00002947
Iteration 217/1000 | Loss: 0.00002947
Iteration 218/1000 | Loss: 0.00002947
Iteration 219/1000 | Loss: 0.00002947
Iteration 220/1000 | Loss: 0.00002947
Iteration 221/1000 | Loss: 0.00002947
Iteration 222/1000 | Loss: 0.00002947
Iteration 223/1000 | Loss: 0.00002947
Iteration 224/1000 | Loss: 0.00002947
Iteration 225/1000 | Loss: 0.00002947
Iteration 226/1000 | Loss: 0.00002947
Iteration 227/1000 | Loss: 0.00002947
Iteration 228/1000 | Loss: 0.00002947
Iteration 229/1000 | Loss: 0.00002947
Iteration 230/1000 | Loss: 0.00002947
Iteration 231/1000 | Loss: 0.00002947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [2.9469938453985378e-05, 2.9469938453985378e-05, 2.9469938453985378e-05, 2.9469938453985378e-05, 2.9469938453985378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9469938453985378e-05

Optimization complete. Final v2v error: 4.264631748199463 mm

Highest mean error: 5.70046329498291 mm for frame 175

Lowest mean error: 3.177656888961792 mm for frame 18

Saving results

Total time: 148.1291823387146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072763
Iteration 2/25 | Loss: 0.00154413
Iteration 3/25 | Loss: 0.00109280
Iteration 4/25 | Loss: 0.00104755
Iteration 5/25 | Loss: 0.00097422
Iteration 6/25 | Loss: 0.00089074
Iteration 7/25 | Loss: 0.00086651
Iteration 8/25 | Loss: 0.00084966
Iteration 9/25 | Loss: 0.00083774
Iteration 10/25 | Loss: 0.00083932
Iteration 11/25 | Loss: 0.00082270
Iteration 12/25 | Loss: 0.00081986
Iteration 13/25 | Loss: 0.00082168
Iteration 14/25 | Loss: 0.00081512
Iteration 15/25 | Loss: 0.00081273
Iteration 16/25 | Loss: 0.00080358
Iteration 17/25 | Loss: 0.00080036
Iteration 18/25 | Loss: 0.00079700
Iteration 19/25 | Loss: 0.00079298
Iteration 20/25 | Loss: 0.00078968
Iteration 21/25 | Loss: 0.00078832
Iteration 22/25 | Loss: 0.00078771
Iteration 23/25 | Loss: 0.00078765
Iteration 24/25 | Loss: 0.00078764
Iteration 25/25 | Loss: 0.00078764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66237223
Iteration 2/25 | Loss: 0.00139811
Iteration 3/25 | Loss: 0.00139811
Iteration 4/25 | Loss: 0.00139810
Iteration 5/25 | Loss: 0.00139810
Iteration 6/25 | Loss: 0.00139810
Iteration 7/25 | Loss: 0.00139810
Iteration 8/25 | Loss: 0.00139810
Iteration 9/25 | Loss: 0.00139810
Iteration 10/25 | Loss: 0.00139810
Iteration 11/25 | Loss: 0.00139810
Iteration 12/25 | Loss: 0.00139810
Iteration 13/25 | Loss: 0.00139810
Iteration 14/25 | Loss: 0.00139810
Iteration 15/25 | Loss: 0.00139810
Iteration 16/25 | Loss: 0.00139810
Iteration 17/25 | Loss: 0.00139810
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013981037773191929, 0.0013981037773191929, 0.0013981037773191929, 0.0013981037773191929, 0.0013981037773191929]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013981037773191929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139810
Iteration 2/1000 | Loss: 0.00003780
Iteration 3/1000 | Loss: 0.00002604
Iteration 4/1000 | Loss: 0.00002329
Iteration 5/1000 | Loss: 0.00002196
Iteration 6/1000 | Loss: 0.00002103
Iteration 7/1000 | Loss: 0.00002050
Iteration 8/1000 | Loss: 0.00062967
Iteration 9/1000 | Loss: 0.00027154
Iteration 10/1000 | Loss: 0.00002732
Iteration 11/1000 | Loss: 0.00002788
Iteration 12/1000 | Loss: 0.00002048
Iteration 13/1000 | Loss: 0.00033740
Iteration 14/1000 | Loss: 0.00025266
Iteration 15/1000 | Loss: 0.00013955
Iteration 16/1000 | Loss: 0.00029641
Iteration 17/1000 | Loss: 0.00002963
Iteration 18/1000 | Loss: 0.00002115
Iteration 19/1000 | Loss: 0.00002115
Iteration 20/1000 | Loss: 0.00001779
Iteration 21/1000 | Loss: 0.00002070
Iteration 22/1000 | Loss: 0.00001598
Iteration 23/1000 | Loss: 0.00001559
Iteration 24/1000 | Loss: 0.00002531
Iteration 25/1000 | Loss: 0.00001564
Iteration 26/1000 | Loss: 0.00001513
Iteration 27/1000 | Loss: 0.00001510
Iteration 28/1000 | Loss: 0.00001509
Iteration 29/1000 | Loss: 0.00001509
Iteration 30/1000 | Loss: 0.00001509
Iteration 31/1000 | Loss: 0.00001509
Iteration 32/1000 | Loss: 0.00001508
Iteration 33/1000 | Loss: 0.00001508
Iteration 34/1000 | Loss: 0.00001508
Iteration 35/1000 | Loss: 0.00001508
Iteration 36/1000 | Loss: 0.00001508
Iteration 37/1000 | Loss: 0.00001508
Iteration 38/1000 | Loss: 0.00001508
Iteration 39/1000 | Loss: 0.00001507
Iteration 40/1000 | Loss: 0.00001507
Iteration 41/1000 | Loss: 0.00001507
Iteration 42/1000 | Loss: 0.00001503
Iteration 43/1000 | Loss: 0.00001502
Iteration 44/1000 | Loss: 0.00001502
Iteration 45/1000 | Loss: 0.00001501
Iteration 46/1000 | Loss: 0.00001501
Iteration 47/1000 | Loss: 0.00001500
Iteration 48/1000 | Loss: 0.00001500
Iteration 49/1000 | Loss: 0.00001499
Iteration 50/1000 | Loss: 0.00001499
Iteration 51/1000 | Loss: 0.00001499
Iteration 52/1000 | Loss: 0.00001499
Iteration 53/1000 | Loss: 0.00001499
Iteration 54/1000 | Loss: 0.00001498
Iteration 55/1000 | Loss: 0.00001498
Iteration 56/1000 | Loss: 0.00001498
Iteration 57/1000 | Loss: 0.00001497
Iteration 58/1000 | Loss: 0.00001497
Iteration 59/1000 | Loss: 0.00001496
Iteration 60/1000 | Loss: 0.00001496
Iteration 61/1000 | Loss: 0.00001496
Iteration 62/1000 | Loss: 0.00001496
Iteration 63/1000 | Loss: 0.00001495
Iteration 64/1000 | Loss: 0.00001495
Iteration 65/1000 | Loss: 0.00001495
Iteration 66/1000 | Loss: 0.00001495
Iteration 67/1000 | Loss: 0.00001495
Iteration 68/1000 | Loss: 0.00001494
Iteration 69/1000 | Loss: 0.00001494
Iteration 70/1000 | Loss: 0.00001494
Iteration 71/1000 | Loss: 0.00001493
Iteration 72/1000 | Loss: 0.00001493
Iteration 73/1000 | Loss: 0.00001493
Iteration 74/1000 | Loss: 0.00001492
Iteration 75/1000 | Loss: 0.00001492
Iteration 76/1000 | Loss: 0.00002221
Iteration 77/1000 | Loss: 0.00001497
Iteration 78/1000 | Loss: 0.00001485
Iteration 79/1000 | Loss: 0.00001485
Iteration 80/1000 | Loss: 0.00001484
Iteration 81/1000 | Loss: 0.00001484
Iteration 82/1000 | Loss: 0.00001484
Iteration 83/1000 | Loss: 0.00001483
Iteration 84/1000 | Loss: 0.00001483
Iteration 85/1000 | Loss: 0.00001483
Iteration 86/1000 | Loss: 0.00001483
Iteration 87/1000 | Loss: 0.00001483
Iteration 88/1000 | Loss: 0.00001483
Iteration 89/1000 | Loss: 0.00001483
Iteration 90/1000 | Loss: 0.00001482
Iteration 91/1000 | Loss: 0.00001482
Iteration 92/1000 | Loss: 0.00001482
Iteration 93/1000 | Loss: 0.00001482
Iteration 94/1000 | Loss: 0.00001482
Iteration 95/1000 | Loss: 0.00001482
Iteration 96/1000 | Loss: 0.00001482
Iteration 97/1000 | Loss: 0.00001482
Iteration 98/1000 | Loss: 0.00001482
Iteration 99/1000 | Loss: 0.00001482
Iteration 100/1000 | Loss: 0.00001482
Iteration 101/1000 | Loss: 0.00001482
Iteration 102/1000 | Loss: 0.00001482
Iteration 103/1000 | Loss: 0.00001482
Iteration 104/1000 | Loss: 0.00001482
Iteration 105/1000 | Loss: 0.00001481
Iteration 106/1000 | Loss: 0.00001481
Iteration 107/1000 | Loss: 0.00001481
Iteration 108/1000 | Loss: 0.00001481
Iteration 109/1000 | Loss: 0.00001481
Iteration 110/1000 | Loss: 0.00001480
Iteration 111/1000 | Loss: 0.00001480
Iteration 112/1000 | Loss: 0.00001480
Iteration 113/1000 | Loss: 0.00001480
Iteration 114/1000 | Loss: 0.00001480
Iteration 115/1000 | Loss: 0.00001479
Iteration 116/1000 | Loss: 0.00001479
Iteration 117/1000 | Loss: 0.00001479
Iteration 118/1000 | Loss: 0.00001479
Iteration 119/1000 | Loss: 0.00001478
Iteration 120/1000 | Loss: 0.00001478
Iteration 121/1000 | Loss: 0.00001478
Iteration 122/1000 | Loss: 0.00001478
Iteration 123/1000 | Loss: 0.00001478
Iteration 124/1000 | Loss: 0.00001477
Iteration 125/1000 | Loss: 0.00001477
Iteration 126/1000 | Loss: 0.00001477
Iteration 127/1000 | Loss: 0.00001477
Iteration 128/1000 | Loss: 0.00001477
Iteration 129/1000 | Loss: 0.00001477
Iteration 130/1000 | Loss: 0.00001477
Iteration 131/1000 | Loss: 0.00001477
Iteration 132/1000 | Loss: 0.00001477
Iteration 133/1000 | Loss: 0.00001477
Iteration 134/1000 | Loss: 0.00001477
Iteration 135/1000 | Loss: 0.00001476
Iteration 136/1000 | Loss: 0.00001476
Iteration 137/1000 | Loss: 0.00001476
Iteration 138/1000 | Loss: 0.00001476
Iteration 139/1000 | Loss: 0.00001476
Iteration 140/1000 | Loss: 0.00001476
Iteration 141/1000 | Loss: 0.00001476
Iteration 142/1000 | Loss: 0.00001476
Iteration 143/1000 | Loss: 0.00001476
Iteration 144/1000 | Loss: 0.00001476
Iteration 145/1000 | Loss: 0.00001476
Iteration 146/1000 | Loss: 0.00001476
Iteration 147/1000 | Loss: 0.00001476
Iteration 148/1000 | Loss: 0.00001476
Iteration 149/1000 | Loss: 0.00001476
Iteration 150/1000 | Loss: 0.00001476
Iteration 151/1000 | Loss: 0.00001476
Iteration 152/1000 | Loss: 0.00001475
Iteration 153/1000 | Loss: 0.00001475
Iteration 154/1000 | Loss: 0.00001475
Iteration 155/1000 | Loss: 0.00001475
Iteration 156/1000 | Loss: 0.00001475
Iteration 157/1000 | Loss: 0.00001475
Iteration 158/1000 | Loss: 0.00001475
Iteration 159/1000 | Loss: 0.00001475
Iteration 160/1000 | Loss: 0.00001475
Iteration 161/1000 | Loss: 0.00001475
Iteration 162/1000 | Loss: 0.00001475
Iteration 163/1000 | Loss: 0.00001474
Iteration 164/1000 | Loss: 0.00001474
Iteration 165/1000 | Loss: 0.00001474
Iteration 166/1000 | Loss: 0.00001474
Iteration 167/1000 | Loss: 0.00001474
Iteration 168/1000 | Loss: 0.00001474
Iteration 169/1000 | Loss: 0.00001474
Iteration 170/1000 | Loss: 0.00001474
Iteration 171/1000 | Loss: 0.00001474
Iteration 172/1000 | Loss: 0.00001474
Iteration 173/1000 | Loss: 0.00001474
Iteration 174/1000 | Loss: 0.00001474
Iteration 175/1000 | Loss: 0.00001474
Iteration 176/1000 | Loss: 0.00001474
Iteration 177/1000 | Loss: 0.00001474
Iteration 178/1000 | Loss: 0.00001474
Iteration 179/1000 | Loss: 0.00001474
Iteration 180/1000 | Loss: 0.00001474
Iteration 181/1000 | Loss: 0.00001474
Iteration 182/1000 | Loss: 0.00001474
Iteration 183/1000 | Loss: 0.00001474
Iteration 184/1000 | Loss: 0.00001474
Iteration 185/1000 | Loss: 0.00001474
Iteration 186/1000 | Loss: 0.00001474
Iteration 187/1000 | Loss: 0.00001474
Iteration 188/1000 | Loss: 0.00001474
Iteration 189/1000 | Loss: 0.00001474
Iteration 190/1000 | Loss: 0.00001474
Iteration 191/1000 | Loss: 0.00001474
Iteration 192/1000 | Loss: 0.00001474
Iteration 193/1000 | Loss: 0.00001474
Iteration 194/1000 | Loss: 0.00001474
Iteration 195/1000 | Loss: 0.00001474
Iteration 196/1000 | Loss: 0.00001474
Iteration 197/1000 | Loss: 0.00001474
Iteration 198/1000 | Loss: 0.00001474
Iteration 199/1000 | Loss: 0.00001474
Iteration 200/1000 | Loss: 0.00001474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.4738764548383188e-05, 1.4738764548383188e-05, 1.4738764548383188e-05, 1.4738764548383188e-05, 1.4738764548383188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4738764548383188e-05

Optimization complete. Final v2v error: 3.23824405670166 mm

Highest mean error: 3.9857177734375 mm for frame 65

Lowest mean error: 2.9363367557525635 mm for frame 17

Saving results

Total time: 93.34372043609619
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499452
Iteration 2/25 | Loss: 0.00105010
Iteration 3/25 | Loss: 0.00089982
Iteration 4/25 | Loss: 0.00086814
Iteration 5/25 | Loss: 0.00086451
Iteration 6/25 | Loss: 0.00086331
Iteration 7/25 | Loss: 0.00086317
Iteration 8/25 | Loss: 0.00086317
Iteration 9/25 | Loss: 0.00086317
Iteration 10/25 | Loss: 0.00086317
Iteration 11/25 | Loss: 0.00086317
Iteration 12/25 | Loss: 0.00086317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008631670498289168, 0.0008631670498289168, 0.0008631670498289168, 0.0008631670498289168, 0.0008631670498289168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008631670498289168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54433763
Iteration 2/25 | Loss: 0.00116153
Iteration 3/25 | Loss: 0.00116152
Iteration 4/25 | Loss: 0.00116152
Iteration 5/25 | Loss: 0.00116152
Iteration 6/25 | Loss: 0.00116152
Iteration 7/25 | Loss: 0.00116152
Iteration 8/25 | Loss: 0.00116152
Iteration 9/25 | Loss: 0.00116152
Iteration 10/25 | Loss: 0.00116152
Iteration 11/25 | Loss: 0.00116152
Iteration 12/25 | Loss: 0.00116152
Iteration 13/25 | Loss: 0.00116152
Iteration 14/25 | Loss: 0.00116152
Iteration 15/25 | Loss: 0.00116152
Iteration 16/25 | Loss: 0.00116152
Iteration 17/25 | Loss: 0.00116152
Iteration 18/25 | Loss: 0.00116152
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011615179246291518, 0.0011615179246291518, 0.0011615179246291518, 0.0011615179246291518, 0.0011615179246291518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011615179246291518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116152
Iteration 2/1000 | Loss: 0.00007071
Iteration 3/1000 | Loss: 0.00004495
Iteration 4/1000 | Loss: 0.00003694
Iteration 5/1000 | Loss: 0.00003516
Iteration 6/1000 | Loss: 0.00003387
Iteration 7/1000 | Loss: 0.00003314
Iteration 8/1000 | Loss: 0.00003244
Iteration 9/1000 | Loss: 0.00003197
Iteration 10/1000 | Loss: 0.00003165
Iteration 11/1000 | Loss: 0.00003143
Iteration 12/1000 | Loss: 0.00003128
Iteration 13/1000 | Loss: 0.00003109
Iteration 14/1000 | Loss: 0.00003107
Iteration 15/1000 | Loss: 0.00003103
Iteration 16/1000 | Loss: 0.00003102
Iteration 17/1000 | Loss: 0.00003101
Iteration 18/1000 | Loss: 0.00003101
Iteration 19/1000 | Loss: 0.00003095
Iteration 20/1000 | Loss: 0.00003095
Iteration 21/1000 | Loss: 0.00003087
Iteration 22/1000 | Loss: 0.00003083
Iteration 23/1000 | Loss: 0.00003075
Iteration 24/1000 | Loss: 0.00003075
Iteration 25/1000 | Loss: 0.00003074
Iteration 26/1000 | Loss: 0.00003072
Iteration 27/1000 | Loss: 0.00003070
Iteration 28/1000 | Loss: 0.00003070
Iteration 29/1000 | Loss: 0.00003070
Iteration 30/1000 | Loss: 0.00003070
Iteration 31/1000 | Loss: 0.00003070
Iteration 32/1000 | Loss: 0.00003070
Iteration 33/1000 | Loss: 0.00003070
Iteration 34/1000 | Loss: 0.00003069
Iteration 35/1000 | Loss: 0.00003069
Iteration 36/1000 | Loss: 0.00003069
Iteration 37/1000 | Loss: 0.00003069
Iteration 38/1000 | Loss: 0.00003069
Iteration 39/1000 | Loss: 0.00003066
Iteration 40/1000 | Loss: 0.00003066
Iteration 41/1000 | Loss: 0.00003065
Iteration 42/1000 | Loss: 0.00003065
Iteration 43/1000 | Loss: 0.00003065
Iteration 44/1000 | Loss: 0.00003064
Iteration 45/1000 | Loss: 0.00003064
Iteration 46/1000 | Loss: 0.00003063
Iteration 47/1000 | Loss: 0.00003063
Iteration 48/1000 | Loss: 0.00003063
Iteration 49/1000 | Loss: 0.00003063
Iteration 50/1000 | Loss: 0.00003062
Iteration 51/1000 | Loss: 0.00003062
Iteration 52/1000 | Loss: 0.00003062
Iteration 53/1000 | Loss: 0.00003062
Iteration 54/1000 | Loss: 0.00003062
Iteration 55/1000 | Loss: 0.00003062
Iteration 56/1000 | Loss: 0.00003062
Iteration 57/1000 | Loss: 0.00003062
Iteration 58/1000 | Loss: 0.00003062
Iteration 59/1000 | Loss: 0.00003062
Iteration 60/1000 | Loss: 0.00003062
Iteration 61/1000 | Loss: 0.00003061
Iteration 62/1000 | Loss: 0.00003061
Iteration 63/1000 | Loss: 0.00003061
Iteration 64/1000 | Loss: 0.00003061
Iteration 65/1000 | Loss: 0.00003061
Iteration 66/1000 | Loss: 0.00003061
Iteration 67/1000 | Loss: 0.00003061
Iteration 68/1000 | Loss: 0.00003060
Iteration 69/1000 | Loss: 0.00003060
Iteration 70/1000 | Loss: 0.00003059
Iteration 71/1000 | Loss: 0.00003059
Iteration 72/1000 | Loss: 0.00003059
Iteration 73/1000 | Loss: 0.00003059
Iteration 74/1000 | Loss: 0.00003059
Iteration 75/1000 | Loss: 0.00003059
Iteration 76/1000 | Loss: 0.00003059
Iteration 77/1000 | Loss: 0.00003059
Iteration 78/1000 | Loss: 0.00003059
Iteration 79/1000 | Loss: 0.00003059
Iteration 80/1000 | Loss: 0.00003059
Iteration 81/1000 | Loss: 0.00003059
Iteration 82/1000 | Loss: 0.00003059
Iteration 83/1000 | Loss: 0.00003059
Iteration 84/1000 | Loss: 0.00003059
Iteration 85/1000 | Loss: 0.00003059
Iteration 86/1000 | Loss: 0.00003059
Iteration 87/1000 | Loss: 0.00003059
Iteration 88/1000 | Loss: 0.00003059
Iteration 89/1000 | Loss: 0.00003059
Iteration 90/1000 | Loss: 0.00003059
Iteration 91/1000 | Loss: 0.00003059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [3.058628135477193e-05, 3.058628135477193e-05, 3.058628135477193e-05, 3.058628135477193e-05, 3.058628135477193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.058628135477193e-05

Optimization complete. Final v2v error: 4.357300758361816 mm

Highest mean error: 5.496142387390137 mm for frame 77

Lowest mean error: 3.7217814922332764 mm for frame 113

Saving results

Total time: 44.40071725845337
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934182
Iteration 2/25 | Loss: 0.00110118
Iteration 3/25 | Loss: 0.00093572
Iteration 4/25 | Loss: 0.00089481
Iteration 5/25 | Loss: 0.00088336
Iteration 6/25 | Loss: 0.00088037
Iteration 7/25 | Loss: 0.00087964
Iteration 8/25 | Loss: 0.00087964
Iteration 9/25 | Loss: 0.00087964
Iteration 10/25 | Loss: 0.00087964
Iteration 11/25 | Loss: 0.00087964
Iteration 12/25 | Loss: 0.00087964
Iteration 13/25 | Loss: 0.00087964
Iteration 14/25 | Loss: 0.00087964
Iteration 15/25 | Loss: 0.00087964
Iteration 16/25 | Loss: 0.00087964
Iteration 17/25 | Loss: 0.00087964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000879643193911761, 0.000879643193911761, 0.000879643193911761, 0.000879643193911761, 0.000879643193911761]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000879643193911761

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52219880
Iteration 2/25 | Loss: 0.00103787
Iteration 3/25 | Loss: 0.00103776
Iteration 4/25 | Loss: 0.00103776
Iteration 5/25 | Loss: 0.00103776
Iteration 6/25 | Loss: 0.00103776
Iteration 7/25 | Loss: 0.00103776
Iteration 8/25 | Loss: 0.00103776
Iteration 9/25 | Loss: 0.00103776
Iteration 10/25 | Loss: 0.00103776
Iteration 11/25 | Loss: 0.00103776
Iteration 12/25 | Loss: 0.00103776
Iteration 13/25 | Loss: 0.00103776
Iteration 14/25 | Loss: 0.00103776
Iteration 15/25 | Loss: 0.00103776
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010377605212852359, 0.0010377605212852359, 0.0010377605212852359, 0.0010377605212852359, 0.0010377605212852359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010377605212852359

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103776
Iteration 2/1000 | Loss: 0.00005208
Iteration 3/1000 | Loss: 0.00003939
Iteration 4/1000 | Loss: 0.00003623
Iteration 5/1000 | Loss: 0.00003426
Iteration 6/1000 | Loss: 0.00003346
Iteration 7/1000 | Loss: 0.00003248
Iteration 8/1000 | Loss: 0.00003167
Iteration 9/1000 | Loss: 0.00003127
Iteration 10/1000 | Loss: 0.00003093
Iteration 11/1000 | Loss: 0.00003062
Iteration 12/1000 | Loss: 0.00003024
Iteration 13/1000 | Loss: 0.00002999
Iteration 14/1000 | Loss: 0.00002979
Iteration 15/1000 | Loss: 0.00002962
Iteration 16/1000 | Loss: 0.00002962
Iteration 17/1000 | Loss: 0.00002948
Iteration 18/1000 | Loss: 0.00002947
Iteration 19/1000 | Loss: 0.00002940
Iteration 20/1000 | Loss: 0.00002939
Iteration 21/1000 | Loss: 0.00002938
Iteration 22/1000 | Loss: 0.00002937
Iteration 23/1000 | Loss: 0.00002937
Iteration 24/1000 | Loss: 0.00002937
Iteration 25/1000 | Loss: 0.00002936
Iteration 26/1000 | Loss: 0.00002936
Iteration 27/1000 | Loss: 0.00002936
Iteration 28/1000 | Loss: 0.00002935
Iteration 29/1000 | Loss: 0.00002935
Iteration 30/1000 | Loss: 0.00002934
Iteration 31/1000 | Loss: 0.00002934
Iteration 32/1000 | Loss: 0.00002934
Iteration 33/1000 | Loss: 0.00002933
Iteration 34/1000 | Loss: 0.00002933
Iteration 35/1000 | Loss: 0.00002932
Iteration 36/1000 | Loss: 0.00002932
Iteration 37/1000 | Loss: 0.00002931
Iteration 38/1000 | Loss: 0.00002931
Iteration 39/1000 | Loss: 0.00002931
Iteration 40/1000 | Loss: 0.00002930
Iteration 41/1000 | Loss: 0.00002930
Iteration 42/1000 | Loss: 0.00002930
Iteration 43/1000 | Loss: 0.00002930
Iteration 44/1000 | Loss: 0.00002929
Iteration 45/1000 | Loss: 0.00002929
Iteration 46/1000 | Loss: 0.00002929
Iteration 47/1000 | Loss: 0.00002929
Iteration 48/1000 | Loss: 0.00002929
Iteration 49/1000 | Loss: 0.00002929
Iteration 50/1000 | Loss: 0.00002928
Iteration 51/1000 | Loss: 0.00002928
Iteration 52/1000 | Loss: 0.00002928
Iteration 53/1000 | Loss: 0.00002928
Iteration 54/1000 | Loss: 0.00002928
Iteration 55/1000 | Loss: 0.00002927
Iteration 56/1000 | Loss: 0.00002927
Iteration 57/1000 | Loss: 0.00002927
Iteration 58/1000 | Loss: 0.00002927
Iteration 59/1000 | Loss: 0.00002926
Iteration 60/1000 | Loss: 0.00002926
Iteration 61/1000 | Loss: 0.00002926
Iteration 62/1000 | Loss: 0.00002926
Iteration 63/1000 | Loss: 0.00002926
Iteration 64/1000 | Loss: 0.00002926
Iteration 65/1000 | Loss: 0.00002926
Iteration 66/1000 | Loss: 0.00002926
Iteration 67/1000 | Loss: 0.00002926
Iteration 68/1000 | Loss: 0.00002925
Iteration 69/1000 | Loss: 0.00002925
Iteration 70/1000 | Loss: 0.00002925
Iteration 71/1000 | Loss: 0.00002925
Iteration 72/1000 | Loss: 0.00002925
Iteration 73/1000 | Loss: 0.00002925
Iteration 74/1000 | Loss: 0.00002925
Iteration 75/1000 | Loss: 0.00002925
Iteration 76/1000 | Loss: 0.00002925
Iteration 77/1000 | Loss: 0.00002924
Iteration 78/1000 | Loss: 0.00002924
Iteration 79/1000 | Loss: 0.00002924
Iteration 80/1000 | Loss: 0.00002924
Iteration 81/1000 | Loss: 0.00002924
Iteration 82/1000 | Loss: 0.00002924
Iteration 83/1000 | Loss: 0.00002924
Iteration 84/1000 | Loss: 0.00002924
Iteration 85/1000 | Loss: 0.00002924
Iteration 86/1000 | Loss: 0.00002924
Iteration 87/1000 | Loss: 0.00002924
Iteration 88/1000 | Loss: 0.00002923
Iteration 89/1000 | Loss: 0.00002923
Iteration 90/1000 | Loss: 0.00002923
Iteration 91/1000 | Loss: 0.00002923
Iteration 92/1000 | Loss: 0.00002923
Iteration 93/1000 | Loss: 0.00002922
Iteration 94/1000 | Loss: 0.00002922
Iteration 95/1000 | Loss: 0.00002922
Iteration 96/1000 | Loss: 0.00002922
Iteration 97/1000 | Loss: 0.00002922
Iteration 98/1000 | Loss: 0.00002922
Iteration 99/1000 | Loss: 0.00002922
Iteration 100/1000 | Loss: 0.00002922
Iteration 101/1000 | Loss: 0.00002922
Iteration 102/1000 | Loss: 0.00002921
Iteration 103/1000 | Loss: 0.00002921
Iteration 104/1000 | Loss: 0.00002921
Iteration 105/1000 | Loss: 0.00002921
Iteration 106/1000 | Loss: 0.00002921
Iteration 107/1000 | Loss: 0.00002921
Iteration 108/1000 | Loss: 0.00002921
Iteration 109/1000 | Loss: 0.00002921
Iteration 110/1000 | Loss: 0.00002921
Iteration 111/1000 | Loss: 0.00002920
Iteration 112/1000 | Loss: 0.00002920
Iteration 113/1000 | Loss: 0.00002920
Iteration 114/1000 | Loss: 0.00002920
Iteration 115/1000 | Loss: 0.00002919
Iteration 116/1000 | Loss: 0.00002919
Iteration 117/1000 | Loss: 0.00002919
Iteration 118/1000 | Loss: 0.00002919
Iteration 119/1000 | Loss: 0.00002919
Iteration 120/1000 | Loss: 0.00002918
Iteration 121/1000 | Loss: 0.00002918
Iteration 122/1000 | Loss: 0.00002918
Iteration 123/1000 | Loss: 0.00002918
Iteration 124/1000 | Loss: 0.00002917
Iteration 125/1000 | Loss: 0.00002917
Iteration 126/1000 | Loss: 0.00002917
Iteration 127/1000 | Loss: 0.00002917
Iteration 128/1000 | Loss: 0.00002917
Iteration 129/1000 | Loss: 0.00002917
Iteration 130/1000 | Loss: 0.00002917
Iteration 131/1000 | Loss: 0.00002917
Iteration 132/1000 | Loss: 0.00002917
Iteration 133/1000 | Loss: 0.00002917
Iteration 134/1000 | Loss: 0.00002917
Iteration 135/1000 | Loss: 0.00002917
Iteration 136/1000 | Loss: 0.00002917
Iteration 137/1000 | Loss: 0.00002917
Iteration 138/1000 | Loss: 0.00002917
Iteration 139/1000 | Loss: 0.00002917
Iteration 140/1000 | Loss: 0.00002917
Iteration 141/1000 | Loss: 0.00002917
Iteration 142/1000 | Loss: 0.00002917
Iteration 143/1000 | Loss: 0.00002917
Iteration 144/1000 | Loss: 0.00002917
Iteration 145/1000 | Loss: 0.00002917
Iteration 146/1000 | Loss: 0.00002917
Iteration 147/1000 | Loss: 0.00002917
Iteration 148/1000 | Loss: 0.00002917
Iteration 149/1000 | Loss: 0.00002917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.9170329071348533e-05, 2.9170329071348533e-05, 2.9170329071348533e-05, 2.9170329071348533e-05, 2.9170329071348533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9170329071348533e-05

Optimization complete. Final v2v error: 4.489021301269531 mm

Highest mean error: 4.622706890106201 mm for frame 197

Lowest mean error: 4.34019660949707 mm for frame 59

Saving results

Total time: 65.68787455558777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435944
Iteration 2/25 | Loss: 0.00147347
Iteration 3/25 | Loss: 0.00138204
Iteration 4/25 | Loss: 0.00137223
Iteration 5/25 | Loss: 0.00137074
Iteration 6/25 | Loss: 0.00137074
Iteration 7/25 | Loss: 0.00137074
Iteration 8/25 | Loss: 0.00137074
Iteration 9/25 | Loss: 0.00137074
Iteration 10/25 | Loss: 0.00137074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013707432663068175, 0.0013707432663068175, 0.0013707432663068175, 0.0013707432663068175, 0.0013707432663068175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013707432663068175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17158115
Iteration 2/25 | Loss: 0.00265705
Iteration 3/25 | Loss: 0.00265704
Iteration 4/25 | Loss: 0.00265704
Iteration 5/25 | Loss: 0.00265704
Iteration 6/25 | Loss: 0.00265704
Iteration 7/25 | Loss: 0.00265704
Iteration 8/25 | Loss: 0.00265704
Iteration 9/25 | Loss: 0.00265704
Iteration 10/25 | Loss: 0.00265704
Iteration 11/25 | Loss: 0.00265704
Iteration 12/25 | Loss: 0.00265704
Iteration 13/25 | Loss: 0.00265704
Iteration 14/25 | Loss: 0.00265704
Iteration 15/25 | Loss: 0.00265704
Iteration 16/25 | Loss: 0.00265704
Iteration 17/25 | Loss: 0.00265704
Iteration 18/25 | Loss: 0.00265704
Iteration 19/25 | Loss: 0.00265704
Iteration 20/25 | Loss: 0.00265704
Iteration 21/25 | Loss: 0.00265704
Iteration 22/25 | Loss: 0.00265704
Iteration 23/25 | Loss: 0.00265704
Iteration 24/25 | Loss: 0.00265704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0026570414192974567, 0.0026570414192974567, 0.0026570414192974567, 0.0026570414192974567, 0.0026570414192974567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026570414192974567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00265704
Iteration 2/1000 | Loss: 0.00004296
Iteration 3/1000 | Loss: 0.00002982
Iteration 4/1000 | Loss: 0.00002473
Iteration 5/1000 | Loss: 0.00002185
Iteration 6/1000 | Loss: 0.00002046
Iteration 7/1000 | Loss: 0.00001976
Iteration 8/1000 | Loss: 0.00001925
Iteration 9/1000 | Loss: 0.00001890
Iteration 10/1000 | Loss: 0.00001872
Iteration 11/1000 | Loss: 0.00001862
Iteration 12/1000 | Loss: 0.00001862
Iteration 13/1000 | Loss: 0.00001847
Iteration 14/1000 | Loss: 0.00001843
Iteration 15/1000 | Loss: 0.00001842
Iteration 16/1000 | Loss: 0.00001838
Iteration 17/1000 | Loss: 0.00001837
Iteration 18/1000 | Loss: 0.00001833
Iteration 19/1000 | Loss: 0.00001827
Iteration 20/1000 | Loss: 0.00001825
Iteration 21/1000 | Loss: 0.00001824
Iteration 22/1000 | Loss: 0.00001824
Iteration 23/1000 | Loss: 0.00001824
Iteration 24/1000 | Loss: 0.00001822
Iteration 25/1000 | Loss: 0.00001822
Iteration 26/1000 | Loss: 0.00001822
Iteration 27/1000 | Loss: 0.00001821
Iteration 28/1000 | Loss: 0.00001821
Iteration 29/1000 | Loss: 0.00001821
Iteration 30/1000 | Loss: 0.00001820
Iteration 31/1000 | Loss: 0.00001820
Iteration 32/1000 | Loss: 0.00001819
Iteration 33/1000 | Loss: 0.00001819
Iteration 34/1000 | Loss: 0.00001819
Iteration 35/1000 | Loss: 0.00001818
Iteration 36/1000 | Loss: 0.00001818
Iteration 37/1000 | Loss: 0.00001818
Iteration 38/1000 | Loss: 0.00001818
Iteration 39/1000 | Loss: 0.00001817
Iteration 40/1000 | Loss: 0.00001817
Iteration 41/1000 | Loss: 0.00001817
Iteration 42/1000 | Loss: 0.00001816
Iteration 43/1000 | Loss: 0.00001816
Iteration 44/1000 | Loss: 0.00001815
Iteration 45/1000 | Loss: 0.00001815
Iteration 46/1000 | Loss: 0.00001815
Iteration 47/1000 | Loss: 0.00001814
Iteration 48/1000 | Loss: 0.00001814
Iteration 49/1000 | Loss: 0.00001814
Iteration 50/1000 | Loss: 0.00001814
Iteration 51/1000 | Loss: 0.00001814
Iteration 52/1000 | Loss: 0.00001813
Iteration 53/1000 | Loss: 0.00001813
Iteration 54/1000 | Loss: 0.00001812
Iteration 55/1000 | Loss: 0.00001812
Iteration 56/1000 | Loss: 0.00001811
Iteration 57/1000 | Loss: 0.00001811
Iteration 58/1000 | Loss: 0.00001811
Iteration 59/1000 | Loss: 0.00001811
Iteration 60/1000 | Loss: 0.00001811
Iteration 61/1000 | Loss: 0.00001811
Iteration 62/1000 | Loss: 0.00001811
Iteration 63/1000 | Loss: 0.00001810
Iteration 64/1000 | Loss: 0.00001810
Iteration 65/1000 | Loss: 0.00001810
Iteration 66/1000 | Loss: 0.00001810
Iteration 67/1000 | Loss: 0.00001810
Iteration 68/1000 | Loss: 0.00001810
Iteration 69/1000 | Loss: 0.00001810
Iteration 70/1000 | Loss: 0.00001809
Iteration 71/1000 | Loss: 0.00001809
Iteration 72/1000 | Loss: 0.00001809
Iteration 73/1000 | Loss: 0.00001809
Iteration 74/1000 | Loss: 0.00001809
Iteration 75/1000 | Loss: 0.00001809
Iteration 76/1000 | Loss: 0.00001809
Iteration 77/1000 | Loss: 0.00001809
Iteration 78/1000 | Loss: 0.00001808
Iteration 79/1000 | Loss: 0.00001808
Iteration 80/1000 | Loss: 0.00001808
Iteration 81/1000 | Loss: 0.00001808
Iteration 82/1000 | Loss: 0.00001808
Iteration 83/1000 | Loss: 0.00001808
Iteration 84/1000 | Loss: 0.00001808
Iteration 85/1000 | Loss: 0.00001808
Iteration 86/1000 | Loss: 0.00001808
Iteration 87/1000 | Loss: 0.00001808
Iteration 88/1000 | Loss: 0.00001808
Iteration 89/1000 | Loss: 0.00001808
Iteration 90/1000 | Loss: 0.00001808
Iteration 91/1000 | Loss: 0.00001808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.8082411770592444e-05, 1.8082411770592444e-05, 1.8082411770592444e-05, 1.8082411770592444e-05, 1.8082411770592444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8082411770592444e-05

Optimization complete. Final v2v error: 3.577531337738037 mm

Highest mean error: 4.027581691741943 mm for frame 14

Lowest mean error: 3.278428316116333 mm for frame 49

Saving results

Total time: 38.825809478759766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399999
Iteration 2/25 | Loss: 0.00160659
Iteration 3/25 | Loss: 0.00143691
Iteration 4/25 | Loss: 0.00141659
Iteration 5/25 | Loss: 0.00141363
Iteration 6/25 | Loss: 0.00141331
Iteration 7/25 | Loss: 0.00141331
Iteration 8/25 | Loss: 0.00141331
Iteration 9/25 | Loss: 0.00141331
Iteration 10/25 | Loss: 0.00141331
Iteration 11/25 | Loss: 0.00141331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014133065706118941, 0.0014133065706118941, 0.0014133065706118941, 0.0014133065706118941, 0.0014133065706118941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014133065706118941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12538338
Iteration 2/25 | Loss: 0.00355936
Iteration 3/25 | Loss: 0.00355936
Iteration 4/25 | Loss: 0.00355936
Iteration 5/25 | Loss: 0.00355936
Iteration 6/25 | Loss: 0.00355936
Iteration 7/25 | Loss: 0.00355936
Iteration 8/25 | Loss: 0.00355936
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.0035593630746006966, 0.0035593630746006966, 0.0035593630746006966, 0.0035593630746006966, 0.0035593630746006966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035593630746006966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00355936
Iteration 2/1000 | Loss: 0.00005009
Iteration 3/1000 | Loss: 0.00002998
Iteration 4/1000 | Loss: 0.00002458
Iteration 5/1000 | Loss: 0.00002196
Iteration 6/1000 | Loss: 0.00002018
Iteration 7/1000 | Loss: 0.00001952
Iteration 8/1000 | Loss: 0.00001884
Iteration 9/1000 | Loss: 0.00001832
Iteration 10/1000 | Loss: 0.00001774
Iteration 11/1000 | Loss: 0.00001750
Iteration 12/1000 | Loss: 0.00001729
Iteration 13/1000 | Loss: 0.00001717
Iteration 14/1000 | Loss: 0.00001713
Iteration 15/1000 | Loss: 0.00001702
Iteration 16/1000 | Loss: 0.00001691
Iteration 17/1000 | Loss: 0.00001684
Iteration 18/1000 | Loss: 0.00001684
Iteration 19/1000 | Loss: 0.00001682
Iteration 20/1000 | Loss: 0.00001680
Iteration 21/1000 | Loss: 0.00001679
Iteration 22/1000 | Loss: 0.00001678
Iteration 23/1000 | Loss: 0.00001674
Iteration 24/1000 | Loss: 0.00001674
Iteration 25/1000 | Loss: 0.00001672
Iteration 26/1000 | Loss: 0.00001672
Iteration 27/1000 | Loss: 0.00001672
Iteration 28/1000 | Loss: 0.00001672
Iteration 29/1000 | Loss: 0.00001671
Iteration 30/1000 | Loss: 0.00001671
Iteration 31/1000 | Loss: 0.00001670
Iteration 32/1000 | Loss: 0.00001670
Iteration 33/1000 | Loss: 0.00001670
Iteration 34/1000 | Loss: 0.00001670
Iteration 35/1000 | Loss: 0.00001670
Iteration 36/1000 | Loss: 0.00001670
Iteration 37/1000 | Loss: 0.00001670
Iteration 38/1000 | Loss: 0.00001670
Iteration 39/1000 | Loss: 0.00001670
Iteration 40/1000 | Loss: 0.00001670
Iteration 41/1000 | Loss: 0.00001670
Iteration 42/1000 | Loss: 0.00001670
Iteration 43/1000 | Loss: 0.00001670
Iteration 44/1000 | Loss: 0.00001670
Iteration 45/1000 | Loss: 0.00001670
Iteration 46/1000 | Loss: 0.00001670
Iteration 47/1000 | Loss: 0.00001670
Iteration 48/1000 | Loss: 0.00001670
Iteration 49/1000 | Loss: 0.00001670
Iteration 50/1000 | Loss: 0.00001670
Iteration 51/1000 | Loss: 0.00001670
Iteration 52/1000 | Loss: 0.00001670
Iteration 53/1000 | Loss: 0.00001670
Iteration 54/1000 | Loss: 0.00001670
Iteration 55/1000 | Loss: 0.00001670
Iteration 56/1000 | Loss: 0.00001670
Iteration 57/1000 | Loss: 0.00001670
Iteration 58/1000 | Loss: 0.00001670
Iteration 59/1000 | Loss: 0.00001670
Iteration 60/1000 | Loss: 0.00001670
Iteration 61/1000 | Loss: 0.00001670
Iteration 62/1000 | Loss: 0.00001670
Iteration 63/1000 | Loss: 0.00001670
Iteration 64/1000 | Loss: 0.00001670
Iteration 65/1000 | Loss: 0.00001670
Iteration 66/1000 | Loss: 0.00001670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [1.669843004492577e-05, 1.669843004492577e-05, 1.669843004492577e-05, 1.669843004492577e-05, 1.669843004492577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.669843004492577e-05

Optimization complete. Final v2v error: 3.492713212966919 mm

Highest mean error: 3.8341798782348633 mm for frame 194

Lowest mean error: 2.9808998107910156 mm for frame 1

Saving results

Total time: 45.35939311981201
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082284
Iteration 2/25 | Loss: 0.00236184
Iteration 3/25 | Loss: 0.00203460
Iteration 4/25 | Loss: 0.00173401
Iteration 5/25 | Loss: 0.00167695
Iteration 6/25 | Loss: 0.00172008
Iteration 7/25 | Loss: 0.00158759
Iteration 8/25 | Loss: 0.00149548
Iteration 9/25 | Loss: 0.00144205
Iteration 10/25 | Loss: 0.00142127
Iteration 11/25 | Loss: 0.00142641
Iteration 12/25 | Loss: 0.00141609
Iteration 13/25 | Loss: 0.00141133
Iteration 14/25 | Loss: 0.00140850
Iteration 15/25 | Loss: 0.00140042
Iteration 16/25 | Loss: 0.00139916
Iteration 17/25 | Loss: 0.00139674
Iteration 18/25 | Loss: 0.00139062
Iteration 19/25 | Loss: 0.00138527
Iteration 20/25 | Loss: 0.00138722
Iteration 21/25 | Loss: 0.00138895
Iteration 22/25 | Loss: 0.00138597
Iteration 23/25 | Loss: 0.00137796
Iteration 24/25 | Loss: 0.00137477
Iteration 25/25 | Loss: 0.00137452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23504210
Iteration 2/25 | Loss: 0.00289306
Iteration 3/25 | Loss: 0.00289306
Iteration 4/25 | Loss: 0.00289306
Iteration 5/25 | Loss: 0.00289306
Iteration 6/25 | Loss: 0.00289306
Iteration 7/25 | Loss: 0.00289306
Iteration 8/25 | Loss: 0.00289306
Iteration 9/25 | Loss: 0.00289306
Iteration 10/25 | Loss: 0.00289306
Iteration 11/25 | Loss: 0.00289306
Iteration 12/25 | Loss: 0.00289306
Iteration 13/25 | Loss: 0.00289306
Iteration 14/25 | Loss: 0.00289306
Iteration 15/25 | Loss: 0.00289306
Iteration 16/25 | Loss: 0.00289306
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002893060678616166, 0.002893060678616166, 0.002893060678616166, 0.002893060678616166, 0.002893060678616166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002893060678616166

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00289306
Iteration 2/1000 | Loss: 0.00007329
Iteration 3/1000 | Loss: 0.00004238
Iteration 4/1000 | Loss: 0.00005647
Iteration 5/1000 | Loss: 0.00003233
Iteration 6/1000 | Loss: 0.00004029
Iteration 7/1000 | Loss: 0.00004174
Iteration 8/1000 | Loss: 0.00003430
Iteration 9/1000 | Loss: 0.00003601
Iteration 10/1000 | Loss: 0.00004708
Iteration 11/1000 | Loss: 0.00005343
Iteration 12/1000 | Loss: 0.00004104
Iteration 13/1000 | Loss: 0.00004548
Iteration 14/1000 | Loss: 0.00004631
Iteration 15/1000 | Loss: 0.00004664
Iteration 16/1000 | Loss: 0.00004531
Iteration 17/1000 | Loss: 0.00003880
Iteration 18/1000 | Loss: 0.00005246
Iteration 19/1000 | Loss: 0.00004841
Iteration 20/1000 | Loss: 0.00005938
Iteration 21/1000 | Loss: 0.00003834
Iteration 22/1000 | Loss: 0.00005426
Iteration 23/1000 | Loss: 0.00003736
Iteration 24/1000 | Loss: 0.00004821
Iteration 25/1000 | Loss: 0.00002723
Iteration 26/1000 | Loss: 0.00005192
Iteration 27/1000 | Loss: 0.00004376
Iteration 28/1000 | Loss: 0.00003582
Iteration 29/1000 | Loss: 0.00005021
Iteration 30/1000 | Loss: 0.00002855
Iteration 31/1000 | Loss: 0.00004635
Iteration 32/1000 | Loss: 0.00004595
Iteration 33/1000 | Loss: 0.00005200
Iteration 34/1000 | Loss: 0.00004547
Iteration 35/1000 | Loss: 0.00005273
Iteration 36/1000 | Loss: 0.00002064
Iteration 37/1000 | Loss: 0.00004568
Iteration 38/1000 | Loss: 0.00005137
Iteration 39/1000 | Loss: 0.00002125
Iteration 40/1000 | Loss: 0.00004571
Iteration 41/1000 | Loss: 0.00006765
Iteration 42/1000 | Loss: 0.00012750
Iteration 43/1000 | Loss: 0.00006602
Iteration 44/1000 | Loss: 0.00003229
Iteration 45/1000 | Loss: 0.00002770
Iteration 46/1000 | Loss: 0.00003769
Iteration 47/1000 | Loss: 0.00003447
Iteration 48/1000 | Loss: 0.00004863
Iteration 49/1000 | Loss: 0.00003741
Iteration 50/1000 | Loss: 0.00004806
Iteration 51/1000 | Loss: 0.00004295
Iteration 52/1000 | Loss: 0.00002636
Iteration 53/1000 | Loss: 0.00005512
Iteration 54/1000 | Loss: 0.00002352
Iteration 55/1000 | Loss: 0.00001975
Iteration 56/1000 | Loss: 0.00001919
Iteration 57/1000 | Loss: 0.00001886
Iteration 58/1000 | Loss: 0.00001866
Iteration 59/1000 | Loss: 0.00001864
Iteration 60/1000 | Loss: 0.00001862
Iteration 61/1000 | Loss: 0.00001862
Iteration 62/1000 | Loss: 0.00001862
Iteration 63/1000 | Loss: 0.00001862
Iteration 64/1000 | Loss: 0.00001862
Iteration 65/1000 | Loss: 0.00001862
Iteration 66/1000 | Loss: 0.00001861
Iteration 67/1000 | Loss: 0.00001861
Iteration 68/1000 | Loss: 0.00001861
Iteration 69/1000 | Loss: 0.00001861
Iteration 70/1000 | Loss: 0.00001861
Iteration 71/1000 | Loss: 0.00001860
Iteration 72/1000 | Loss: 0.00001860
Iteration 73/1000 | Loss: 0.00001859
Iteration 74/1000 | Loss: 0.00001858
Iteration 75/1000 | Loss: 0.00001858
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001857
Iteration 79/1000 | Loss: 0.00001857
Iteration 80/1000 | Loss: 0.00001856
Iteration 81/1000 | Loss: 0.00001856
Iteration 82/1000 | Loss: 0.00001855
Iteration 83/1000 | Loss: 0.00001855
Iteration 84/1000 | Loss: 0.00001855
Iteration 85/1000 | Loss: 0.00001854
Iteration 86/1000 | Loss: 0.00001854
Iteration 87/1000 | Loss: 0.00001854
Iteration 88/1000 | Loss: 0.00001854
Iteration 89/1000 | Loss: 0.00001854
Iteration 90/1000 | Loss: 0.00001854
Iteration 91/1000 | Loss: 0.00001854
Iteration 92/1000 | Loss: 0.00001854
Iteration 93/1000 | Loss: 0.00001853
Iteration 94/1000 | Loss: 0.00001853
Iteration 95/1000 | Loss: 0.00001853
Iteration 96/1000 | Loss: 0.00001853
Iteration 97/1000 | Loss: 0.00001853
Iteration 98/1000 | Loss: 0.00001853
Iteration 99/1000 | Loss: 0.00001853
Iteration 100/1000 | Loss: 0.00001853
Iteration 101/1000 | Loss: 0.00001853
Iteration 102/1000 | Loss: 0.00001852
Iteration 103/1000 | Loss: 0.00001852
Iteration 104/1000 | Loss: 0.00001852
Iteration 105/1000 | Loss: 0.00001852
Iteration 106/1000 | Loss: 0.00001852
Iteration 107/1000 | Loss: 0.00001852
Iteration 108/1000 | Loss: 0.00001852
Iteration 109/1000 | Loss: 0.00001852
Iteration 110/1000 | Loss: 0.00001852
Iteration 111/1000 | Loss: 0.00001851
Iteration 112/1000 | Loss: 0.00001851
Iteration 113/1000 | Loss: 0.00001851
Iteration 114/1000 | Loss: 0.00001851
Iteration 115/1000 | Loss: 0.00001851
Iteration 116/1000 | Loss: 0.00001851
Iteration 117/1000 | Loss: 0.00001851
Iteration 118/1000 | Loss: 0.00001851
Iteration 119/1000 | Loss: 0.00001851
Iteration 120/1000 | Loss: 0.00001850
Iteration 121/1000 | Loss: 0.00001850
Iteration 122/1000 | Loss: 0.00001850
Iteration 123/1000 | Loss: 0.00001850
Iteration 124/1000 | Loss: 0.00001850
Iteration 125/1000 | Loss: 0.00001850
Iteration 126/1000 | Loss: 0.00001849
Iteration 127/1000 | Loss: 0.00001849
Iteration 128/1000 | Loss: 0.00001849
Iteration 129/1000 | Loss: 0.00001849
Iteration 130/1000 | Loss: 0.00001849
Iteration 131/1000 | Loss: 0.00001849
Iteration 132/1000 | Loss: 0.00001849
Iteration 133/1000 | Loss: 0.00001848
Iteration 134/1000 | Loss: 0.00001848
Iteration 135/1000 | Loss: 0.00001848
Iteration 136/1000 | Loss: 0.00001848
Iteration 137/1000 | Loss: 0.00001848
Iteration 138/1000 | Loss: 0.00001848
Iteration 139/1000 | Loss: 0.00001848
Iteration 140/1000 | Loss: 0.00001848
Iteration 141/1000 | Loss: 0.00001848
Iteration 142/1000 | Loss: 0.00001848
Iteration 143/1000 | Loss: 0.00001848
Iteration 144/1000 | Loss: 0.00001848
Iteration 145/1000 | Loss: 0.00001848
Iteration 146/1000 | Loss: 0.00001848
Iteration 147/1000 | Loss: 0.00001848
Iteration 148/1000 | Loss: 0.00001848
Iteration 149/1000 | Loss: 0.00001847
Iteration 150/1000 | Loss: 0.00001847
Iteration 151/1000 | Loss: 0.00001847
Iteration 152/1000 | Loss: 0.00001847
Iteration 153/1000 | Loss: 0.00001847
Iteration 154/1000 | Loss: 0.00001847
Iteration 155/1000 | Loss: 0.00001847
Iteration 156/1000 | Loss: 0.00001847
Iteration 157/1000 | Loss: 0.00001847
Iteration 158/1000 | Loss: 0.00001847
Iteration 159/1000 | Loss: 0.00001847
Iteration 160/1000 | Loss: 0.00001847
Iteration 161/1000 | Loss: 0.00001847
Iteration 162/1000 | Loss: 0.00001847
Iteration 163/1000 | Loss: 0.00001847
Iteration 164/1000 | Loss: 0.00001847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.8473720047040842e-05, 1.8473720047040842e-05, 1.8473720047040842e-05, 1.8473720047040842e-05, 1.8473720047040842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8473720047040842e-05

Optimization complete. Final v2v error: 3.590125322341919 mm

Highest mean error: 9.095793724060059 mm for frame 130

Lowest mean error: 3.134580135345459 mm for frame 20

Saving results

Total time: 139.15891432762146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00505986
Iteration 2/25 | Loss: 0.00152938
Iteration 3/25 | Loss: 0.00140584
Iteration 4/25 | Loss: 0.00139620
Iteration 5/25 | Loss: 0.00139417
Iteration 6/25 | Loss: 0.00139410
Iteration 7/25 | Loss: 0.00139410
Iteration 8/25 | Loss: 0.00139410
Iteration 9/25 | Loss: 0.00139410
Iteration 10/25 | Loss: 0.00139410
Iteration 11/25 | Loss: 0.00139410
Iteration 12/25 | Loss: 0.00139410
Iteration 13/25 | Loss: 0.00139410
Iteration 14/25 | Loss: 0.00139410
Iteration 15/25 | Loss: 0.00139410
Iteration 16/25 | Loss: 0.00139410
Iteration 17/25 | Loss: 0.00139410
Iteration 18/25 | Loss: 0.00139410
Iteration 19/25 | Loss: 0.00139410
Iteration 20/25 | Loss: 0.00139410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013941047945991158, 0.0013941047945991158, 0.0013941047945991158, 0.0013941047945991158, 0.0013941047945991158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013941047945991158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.70452785
Iteration 2/25 | Loss: 0.00155180
Iteration 3/25 | Loss: 0.00155179
Iteration 4/25 | Loss: 0.00155179
Iteration 5/25 | Loss: 0.00155179
Iteration 6/25 | Loss: 0.00155179
Iteration 7/25 | Loss: 0.00155179
Iteration 8/25 | Loss: 0.00155179
Iteration 9/25 | Loss: 0.00155179
Iteration 10/25 | Loss: 0.00155179
Iteration 11/25 | Loss: 0.00155179
Iteration 12/25 | Loss: 0.00155179
Iteration 13/25 | Loss: 0.00155179
Iteration 14/25 | Loss: 0.00155179
Iteration 15/25 | Loss: 0.00155179
Iteration 16/25 | Loss: 0.00155179
Iteration 17/25 | Loss: 0.00155179
Iteration 18/25 | Loss: 0.00155179
Iteration 19/25 | Loss: 0.00155179
Iteration 20/25 | Loss: 0.00155179
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015517913270741701, 0.0015517913270741701, 0.0015517913270741701, 0.0015517913270741701, 0.0015517913270741701]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015517913270741701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155179
Iteration 2/1000 | Loss: 0.00004145
Iteration 3/1000 | Loss: 0.00002845
Iteration 4/1000 | Loss: 0.00002413
Iteration 5/1000 | Loss: 0.00002208
Iteration 6/1000 | Loss: 0.00002164
Iteration 7/1000 | Loss: 0.00002126
Iteration 8/1000 | Loss: 0.00002092
Iteration 9/1000 | Loss: 0.00002068
Iteration 10/1000 | Loss: 0.00002042
Iteration 11/1000 | Loss: 0.00002035
Iteration 12/1000 | Loss: 0.00002019
Iteration 13/1000 | Loss: 0.00002005
Iteration 14/1000 | Loss: 0.00002004
Iteration 15/1000 | Loss: 0.00002004
Iteration 16/1000 | Loss: 0.00001992
Iteration 17/1000 | Loss: 0.00001992
Iteration 18/1000 | Loss: 0.00001983
Iteration 19/1000 | Loss: 0.00001982
Iteration 20/1000 | Loss: 0.00001981
Iteration 21/1000 | Loss: 0.00001969
Iteration 22/1000 | Loss: 0.00001966
Iteration 23/1000 | Loss: 0.00001960
Iteration 24/1000 | Loss: 0.00001957
Iteration 25/1000 | Loss: 0.00001949
Iteration 26/1000 | Loss: 0.00001949
Iteration 27/1000 | Loss: 0.00001949
Iteration 28/1000 | Loss: 0.00001948
Iteration 29/1000 | Loss: 0.00001948
Iteration 30/1000 | Loss: 0.00001948
Iteration 31/1000 | Loss: 0.00001948
Iteration 32/1000 | Loss: 0.00001948
Iteration 33/1000 | Loss: 0.00001948
Iteration 34/1000 | Loss: 0.00001948
Iteration 35/1000 | Loss: 0.00001948
Iteration 36/1000 | Loss: 0.00001948
Iteration 37/1000 | Loss: 0.00001948
Iteration 38/1000 | Loss: 0.00001948
Iteration 39/1000 | Loss: 0.00001948
Iteration 40/1000 | Loss: 0.00001948
Iteration 41/1000 | Loss: 0.00001948
Iteration 42/1000 | Loss: 0.00001948
Iteration 43/1000 | Loss: 0.00001948
Iteration 44/1000 | Loss: 0.00001948
Iteration 45/1000 | Loss: 0.00001948
Iteration 46/1000 | Loss: 0.00001948
Iteration 47/1000 | Loss: 0.00001943
Iteration 48/1000 | Loss: 0.00001943
Iteration 49/1000 | Loss: 0.00001943
Iteration 50/1000 | Loss: 0.00001937
Iteration 51/1000 | Loss: 0.00001937
Iteration 52/1000 | Loss: 0.00001937
Iteration 53/1000 | Loss: 0.00001937
Iteration 54/1000 | Loss: 0.00001937
Iteration 55/1000 | Loss: 0.00001937
Iteration 56/1000 | Loss: 0.00001936
Iteration 57/1000 | Loss: 0.00001936
Iteration 58/1000 | Loss: 0.00001936
Iteration 59/1000 | Loss: 0.00001936
Iteration 60/1000 | Loss: 0.00001930
Iteration 61/1000 | Loss: 0.00001930
Iteration 62/1000 | Loss: 0.00001929
Iteration 63/1000 | Loss: 0.00001929
Iteration 64/1000 | Loss: 0.00001929
Iteration 65/1000 | Loss: 0.00001929
Iteration 66/1000 | Loss: 0.00001929
Iteration 67/1000 | Loss: 0.00001929
Iteration 68/1000 | Loss: 0.00001929
Iteration 69/1000 | Loss: 0.00001929
Iteration 70/1000 | Loss: 0.00001929
Iteration 71/1000 | Loss: 0.00001928
Iteration 72/1000 | Loss: 0.00001928
Iteration 73/1000 | Loss: 0.00001928
Iteration 74/1000 | Loss: 0.00001928
Iteration 75/1000 | Loss: 0.00001927
Iteration 76/1000 | Loss: 0.00001927
Iteration 77/1000 | Loss: 0.00001926
Iteration 78/1000 | Loss: 0.00001926
Iteration 79/1000 | Loss: 0.00001926
Iteration 80/1000 | Loss: 0.00001926
Iteration 81/1000 | Loss: 0.00001925
Iteration 82/1000 | Loss: 0.00001922
Iteration 83/1000 | Loss: 0.00001922
Iteration 84/1000 | Loss: 0.00001921
Iteration 85/1000 | Loss: 0.00001921
Iteration 86/1000 | Loss: 0.00001920
Iteration 87/1000 | Loss: 0.00001920
Iteration 88/1000 | Loss: 0.00001920
Iteration 89/1000 | Loss: 0.00001920
Iteration 90/1000 | Loss: 0.00001920
Iteration 91/1000 | Loss: 0.00001920
Iteration 92/1000 | Loss: 0.00001920
Iteration 93/1000 | Loss: 0.00001920
Iteration 94/1000 | Loss: 0.00001920
Iteration 95/1000 | Loss: 0.00001920
Iteration 96/1000 | Loss: 0.00001920
Iteration 97/1000 | Loss: 0.00001920
Iteration 98/1000 | Loss: 0.00001920
Iteration 99/1000 | Loss: 0.00001920
Iteration 100/1000 | Loss: 0.00001920
Iteration 101/1000 | Loss: 0.00001920
Iteration 102/1000 | Loss: 0.00001920
Iteration 103/1000 | Loss: 0.00001920
Iteration 104/1000 | Loss: 0.00001920
Iteration 105/1000 | Loss: 0.00001920
Iteration 106/1000 | Loss: 0.00001920
Iteration 107/1000 | Loss: 0.00001920
Iteration 108/1000 | Loss: 0.00001920
Iteration 109/1000 | Loss: 0.00001920
Iteration 110/1000 | Loss: 0.00001920
Iteration 111/1000 | Loss: 0.00001920
Iteration 112/1000 | Loss: 0.00001920
Iteration 113/1000 | Loss: 0.00001920
Iteration 114/1000 | Loss: 0.00001920
Iteration 115/1000 | Loss: 0.00001920
Iteration 116/1000 | Loss: 0.00001920
Iteration 117/1000 | Loss: 0.00001920
Iteration 118/1000 | Loss: 0.00001920
Iteration 119/1000 | Loss: 0.00001920
Iteration 120/1000 | Loss: 0.00001920
Iteration 121/1000 | Loss: 0.00001920
Iteration 122/1000 | Loss: 0.00001920
Iteration 123/1000 | Loss: 0.00001920
Iteration 124/1000 | Loss: 0.00001920
Iteration 125/1000 | Loss: 0.00001920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.9198801965103485e-05, 1.9198801965103485e-05, 1.9198801965103485e-05, 1.9198801965103485e-05, 1.9198801965103485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9198801965103485e-05

Optimization complete. Final v2v error: 3.855776786804199 mm

Highest mean error: 4.108417510986328 mm for frame 39

Lowest mean error: 3.6189804077148438 mm for frame 95

Saving results

Total time: 94.65123796463013
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00943184
Iteration 2/25 | Loss: 0.00157813
Iteration 3/25 | Loss: 0.00139901
Iteration 4/25 | Loss: 0.00138779
Iteration 5/25 | Loss: 0.00138391
Iteration 6/25 | Loss: 0.00138327
Iteration 7/25 | Loss: 0.00138327
Iteration 8/25 | Loss: 0.00138327
Iteration 9/25 | Loss: 0.00138327
Iteration 10/25 | Loss: 0.00138327
Iteration 11/25 | Loss: 0.00138327
Iteration 12/25 | Loss: 0.00138327
Iteration 13/25 | Loss: 0.00138327
Iteration 14/25 | Loss: 0.00138327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001383272116072476, 0.001383272116072476, 0.001383272116072476, 0.001383272116072476, 0.001383272116072476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001383272116072476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18317771
Iteration 2/25 | Loss: 0.00263062
Iteration 3/25 | Loss: 0.00263062
Iteration 4/25 | Loss: 0.00263062
Iteration 5/25 | Loss: 0.00263062
Iteration 6/25 | Loss: 0.00263062
Iteration 7/25 | Loss: 0.00263062
Iteration 8/25 | Loss: 0.00263062
Iteration 9/25 | Loss: 0.00263062
Iteration 10/25 | Loss: 0.00263062
Iteration 11/25 | Loss: 0.00263062
Iteration 12/25 | Loss: 0.00263062
Iteration 13/25 | Loss: 0.00263062
Iteration 14/25 | Loss: 0.00263062
Iteration 15/25 | Loss: 0.00263062
Iteration 16/25 | Loss: 0.00263062
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0026306172367185354, 0.0026306172367185354, 0.0026306172367185354, 0.0026306172367185354, 0.0026306172367185354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026306172367185354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263062
Iteration 2/1000 | Loss: 0.00005141
Iteration 3/1000 | Loss: 0.00003391
Iteration 4/1000 | Loss: 0.00002865
Iteration 5/1000 | Loss: 0.00002642
Iteration 6/1000 | Loss: 0.00002539
Iteration 7/1000 | Loss: 0.00002479
Iteration 8/1000 | Loss: 0.00002432
Iteration 9/1000 | Loss: 0.00002400
Iteration 10/1000 | Loss: 0.00002380
Iteration 11/1000 | Loss: 0.00002380
Iteration 12/1000 | Loss: 0.00002373
Iteration 13/1000 | Loss: 0.00002370
Iteration 14/1000 | Loss: 0.00002364
Iteration 15/1000 | Loss: 0.00002360
Iteration 16/1000 | Loss: 0.00002359
Iteration 17/1000 | Loss: 0.00002359
Iteration 18/1000 | Loss: 0.00002357
Iteration 19/1000 | Loss: 0.00002356
Iteration 20/1000 | Loss: 0.00002354
Iteration 21/1000 | Loss: 0.00002354
Iteration 22/1000 | Loss: 0.00002352
Iteration 23/1000 | Loss: 0.00002344
Iteration 24/1000 | Loss: 0.00002343
Iteration 25/1000 | Loss: 0.00002342
Iteration 26/1000 | Loss: 0.00002342
Iteration 27/1000 | Loss: 0.00002342
Iteration 28/1000 | Loss: 0.00002339
Iteration 29/1000 | Loss: 0.00002337
Iteration 30/1000 | Loss: 0.00002336
Iteration 31/1000 | Loss: 0.00002336
Iteration 32/1000 | Loss: 0.00002335
Iteration 33/1000 | Loss: 0.00002335
Iteration 34/1000 | Loss: 0.00002334
Iteration 35/1000 | Loss: 0.00002332
Iteration 36/1000 | Loss: 0.00002332
Iteration 37/1000 | Loss: 0.00002332
Iteration 38/1000 | Loss: 0.00002332
Iteration 39/1000 | Loss: 0.00002332
Iteration 40/1000 | Loss: 0.00002332
Iteration 41/1000 | Loss: 0.00002332
Iteration 42/1000 | Loss: 0.00002332
Iteration 43/1000 | Loss: 0.00002332
Iteration 44/1000 | Loss: 0.00002332
Iteration 45/1000 | Loss: 0.00002331
Iteration 46/1000 | Loss: 0.00002330
Iteration 47/1000 | Loss: 0.00002329
Iteration 48/1000 | Loss: 0.00002328
Iteration 49/1000 | Loss: 0.00002328
Iteration 50/1000 | Loss: 0.00002328
Iteration 51/1000 | Loss: 0.00002327
Iteration 52/1000 | Loss: 0.00002327
Iteration 53/1000 | Loss: 0.00002327
Iteration 54/1000 | Loss: 0.00002326
Iteration 55/1000 | Loss: 0.00002326
Iteration 56/1000 | Loss: 0.00002326
Iteration 57/1000 | Loss: 0.00002326
Iteration 58/1000 | Loss: 0.00002326
Iteration 59/1000 | Loss: 0.00002326
Iteration 60/1000 | Loss: 0.00002326
Iteration 61/1000 | Loss: 0.00002325
Iteration 62/1000 | Loss: 0.00002325
Iteration 63/1000 | Loss: 0.00002325
Iteration 64/1000 | Loss: 0.00002325
Iteration 65/1000 | Loss: 0.00002325
Iteration 66/1000 | Loss: 0.00002325
Iteration 67/1000 | Loss: 0.00002325
Iteration 68/1000 | Loss: 0.00002325
Iteration 69/1000 | Loss: 0.00002325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [2.325340756215155e-05, 2.325340756215155e-05, 2.325340756215155e-05, 2.325340756215155e-05, 2.325340756215155e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.325340756215155e-05

Optimization complete. Final v2v error: 3.893587589263916 mm

Highest mean error: 4.6557769775390625 mm for frame 178

Lowest mean error: 3.178382158279419 mm for frame 210

Saving results

Total time: 35.70275855064392
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399297
Iteration 2/25 | Loss: 0.00141967
Iteration 3/25 | Loss: 0.00135850
Iteration 4/25 | Loss: 0.00135570
Iteration 5/25 | Loss: 0.00135499
Iteration 6/25 | Loss: 0.00135499
Iteration 7/25 | Loss: 0.00135499
Iteration 8/25 | Loss: 0.00135499
Iteration 9/25 | Loss: 0.00135499
Iteration 10/25 | Loss: 0.00135499
Iteration 11/25 | Loss: 0.00135499
Iteration 12/25 | Loss: 0.00135499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013549926225095987, 0.0013549926225095987, 0.0013549926225095987, 0.0013549926225095987, 0.0013549926225095987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013549926225095987

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49325573
Iteration 2/25 | Loss: 0.00275460
Iteration 3/25 | Loss: 0.00275460
Iteration 4/25 | Loss: 0.00275460
Iteration 5/25 | Loss: 0.00275460
Iteration 6/25 | Loss: 0.00275460
Iteration 7/25 | Loss: 0.00275460
Iteration 8/25 | Loss: 0.00275460
Iteration 9/25 | Loss: 0.00275460
Iteration 10/25 | Loss: 0.00275460
Iteration 11/25 | Loss: 0.00275460
Iteration 12/25 | Loss: 0.00275460
Iteration 13/25 | Loss: 0.00275460
Iteration 14/25 | Loss: 0.00275460
Iteration 15/25 | Loss: 0.00275460
Iteration 16/25 | Loss: 0.00275460
Iteration 17/25 | Loss: 0.00275460
Iteration 18/25 | Loss: 0.00275460
Iteration 19/25 | Loss: 0.00275460
Iteration 20/25 | Loss: 0.00275460
Iteration 21/25 | Loss: 0.00275460
Iteration 22/25 | Loss: 0.00275460
Iteration 23/25 | Loss: 0.00275460
Iteration 24/25 | Loss: 0.00275460
Iteration 25/25 | Loss: 0.00275460

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00275460
Iteration 2/1000 | Loss: 0.00003568
Iteration 3/1000 | Loss: 0.00002210
Iteration 4/1000 | Loss: 0.00001938
Iteration 5/1000 | Loss: 0.00001779
Iteration 6/1000 | Loss: 0.00001715
Iteration 7/1000 | Loss: 0.00001660
Iteration 8/1000 | Loss: 0.00001655
Iteration 9/1000 | Loss: 0.00001619
Iteration 10/1000 | Loss: 0.00001617
Iteration 11/1000 | Loss: 0.00001593
Iteration 12/1000 | Loss: 0.00001583
Iteration 13/1000 | Loss: 0.00001576
Iteration 14/1000 | Loss: 0.00001576
Iteration 15/1000 | Loss: 0.00001576
Iteration 16/1000 | Loss: 0.00001572
Iteration 17/1000 | Loss: 0.00001571
Iteration 18/1000 | Loss: 0.00001570
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001570
Iteration 21/1000 | Loss: 0.00001569
Iteration 22/1000 | Loss: 0.00001569
Iteration 23/1000 | Loss: 0.00001568
Iteration 24/1000 | Loss: 0.00001567
Iteration 25/1000 | Loss: 0.00001567
Iteration 26/1000 | Loss: 0.00001567
Iteration 27/1000 | Loss: 0.00001567
Iteration 28/1000 | Loss: 0.00001567
Iteration 29/1000 | Loss: 0.00001566
Iteration 30/1000 | Loss: 0.00001566
Iteration 31/1000 | Loss: 0.00001565
Iteration 32/1000 | Loss: 0.00001564
Iteration 33/1000 | Loss: 0.00001564
Iteration 34/1000 | Loss: 0.00001564
Iteration 35/1000 | Loss: 0.00001563
Iteration 36/1000 | Loss: 0.00001563
Iteration 37/1000 | Loss: 0.00001563
Iteration 38/1000 | Loss: 0.00001562
Iteration 39/1000 | Loss: 0.00001562
Iteration 40/1000 | Loss: 0.00001561
Iteration 41/1000 | Loss: 0.00001561
Iteration 42/1000 | Loss: 0.00001560
Iteration 43/1000 | Loss: 0.00001559
Iteration 44/1000 | Loss: 0.00001559
Iteration 45/1000 | Loss: 0.00001559
Iteration 46/1000 | Loss: 0.00001559
Iteration 47/1000 | Loss: 0.00001559
Iteration 48/1000 | Loss: 0.00001559
Iteration 49/1000 | Loss: 0.00001559
Iteration 50/1000 | Loss: 0.00001559
Iteration 51/1000 | Loss: 0.00001559
Iteration 52/1000 | Loss: 0.00001559
Iteration 53/1000 | Loss: 0.00001559
Iteration 54/1000 | Loss: 0.00001559
Iteration 55/1000 | Loss: 0.00001558
Iteration 56/1000 | Loss: 0.00001558
Iteration 57/1000 | Loss: 0.00001557
Iteration 58/1000 | Loss: 0.00001557
Iteration 59/1000 | Loss: 0.00001557
Iteration 60/1000 | Loss: 0.00001557
Iteration 61/1000 | Loss: 0.00001557
Iteration 62/1000 | Loss: 0.00001556
Iteration 63/1000 | Loss: 0.00001556
Iteration 64/1000 | Loss: 0.00001556
Iteration 65/1000 | Loss: 0.00001556
Iteration 66/1000 | Loss: 0.00001556
Iteration 67/1000 | Loss: 0.00001556
Iteration 68/1000 | Loss: 0.00001554
Iteration 69/1000 | Loss: 0.00001554
Iteration 70/1000 | Loss: 0.00001554
Iteration 71/1000 | Loss: 0.00001554
Iteration 72/1000 | Loss: 0.00001554
Iteration 73/1000 | Loss: 0.00001553
Iteration 74/1000 | Loss: 0.00001553
Iteration 75/1000 | Loss: 0.00001553
Iteration 76/1000 | Loss: 0.00001553
Iteration 77/1000 | Loss: 0.00001553
Iteration 78/1000 | Loss: 0.00001553
Iteration 79/1000 | Loss: 0.00001552
Iteration 80/1000 | Loss: 0.00001552
Iteration 81/1000 | Loss: 0.00001552
Iteration 82/1000 | Loss: 0.00001552
Iteration 83/1000 | Loss: 0.00001552
Iteration 84/1000 | Loss: 0.00001552
Iteration 85/1000 | Loss: 0.00001552
Iteration 86/1000 | Loss: 0.00001552
Iteration 87/1000 | Loss: 0.00001551
Iteration 88/1000 | Loss: 0.00001551
Iteration 89/1000 | Loss: 0.00001551
Iteration 90/1000 | Loss: 0.00001551
Iteration 91/1000 | Loss: 0.00001551
Iteration 92/1000 | Loss: 0.00001551
Iteration 93/1000 | Loss: 0.00001551
Iteration 94/1000 | Loss: 0.00001551
Iteration 95/1000 | Loss: 0.00001551
Iteration 96/1000 | Loss: 0.00001551
Iteration 97/1000 | Loss: 0.00001551
Iteration 98/1000 | Loss: 0.00001550
Iteration 99/1000 | Loss: 0.00001550
Iteration 100/1000 | Loss: 0.00001550
Iteration 101/1000 | Loss: 0.00001550
Iteration 102/1000 | Loss: 0.00001550
Iteration 103/1000 | Loss: 0.00001550
Iteration 104/1000 | Loss: 0.00001550
Iteration 105/1000 | Loss: 0.00001550
Iteration 106/1000 | Loss: 0.00001549
Iteration 107/1000 | Loss: 0.00001549
Iteration 108/1000 | Loss: 0.00001549
Iteration 109/1000 | Loss: 0.00001549
Iteration 110/1000 | Loss: 0.00001549
Iteration 111/1000 | Loss: 0.00001549
Iteration 112/1000 | Loss: 0.00001549
Iteration 113/1000 | Loss: 0.00001549
Iteration 114/1000 | Loss: 0.00001549
Iteration 115/1000 | Loss: 0.00001549
Iteration 116/1000 | Loss: 0.00001549
Iteration 117/1000 | Loss: 0.00001548
Iteration 118/1000 | Loss: 0.00001548
Iteration 119/1000 | Loss: 0.00001548
Iteration 120/1000 | Loss: 0.00001548
Iteration 121/1000 | Loss: 0.00001548
Iteration 122/1000 | Loss: 0.00001548
Iteration 123/1000 | Loss: 0.00001548
Iteration 124/1000 | Loss: 0.00001548
Iteration 125/1000 | Loss: 0.00001548
Iteration 126/1000 | Loss: 0.00001547
Iteration 127/1000 | Loss: 0.00001547
Iteration 128/1000 | Loss: 0.00001547
Iteration 129/1000 | Loss: 0.00001547
Iteration 130/1000 | Loss: 0.00001547
Iteration 131/1000 | Loss: 0.00001547
Iteration 132/1000 | Loss: 0.00001547
Iteration 133/1000 | Loss: 0.00001547
Iteration 134/1000 | Loss: 0.00001547
Iteration 135/1000 | Loss: 0.00001546
Iteration 136/1000 | Loss: 0.00001546
Iteration 137/1000 | Loss: 0.00001546
Iteration 138/1000 | Loss: 0.00001546
Iteration 139/1000 | Loss: 0.00001546
Iteration 140/1000 | Loss: 0.00001546
Iteration 141/1000 | Loss: 0.00001546
Iteration 142/1000 | Loss: 0.00001546
Iteration 143/1000 | Loss: 0.00001546
Iteration 144/1000 | Loss: 0.00001546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.5459823771379888e-05, 1.5459823771379888e-05, 1.5459823771379888e-05, 1.5459823771379888e-05, 1.5459823771379888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5459823771379888e-05

Optimization complete. Final v2v error: 3.3740532398223877 mm

Highest mean error: 3.599212169647217 mm for frame 83

Lowest mean error: 3.1877012252807617 mm for frame 72

Saving results

Total time: 33.44462275505066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925006
Iteration 2/25 | Loss: 0.00150138
Iteration 3/25 | Loss: 0.00140547
Iteration 4/25 | Loss: 0.00139342
Iteration 5/25 | Loss: 0.00138895
Iteration 6/25 | Loss: 0.00138891
Iteration 7/25 | Loss: 0.00138891
Iteration 8/25 | Loss: 0.00138891
Iteration 9/25 | Loss: 0.00138891
Iteration 10/25 | Loss: 0.00138891
Iteration 11/25 | Loss: 0.00138891
Iteration 12/25 | Loss: 0.00138891
Iteration 13/25 | Loss: 0.00138891
Iteration 14/25 | Loss: 0.00138891
Iteration 15/25 | Loss: 0.00138891
Iteration 16/25 | Loss: 0.00138891
Iteration 17/25 | Loss: 0.00138891
Iteration 18/25 | Loss: 0.00138891
Iteration 19/25 | Loss: 0.00138891
Iteration 20/25 | Loss: 0.00138891
Iteration 21/25 | Loss: 0.00138891
Iteration 22/25 | Loss: 0.00138891
Iteration 23/25 | Loss: 0.00138891
Iteration 24/25 | Loss: 0.00138891
Iteration 25/25 | Loss: 0.00138891

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.77314234
Iteration 2/25 | Loss: 0.00279677
Iteration 3/25 | Loss: 0.00279677
Iteration 4/25 | Loss: 0.00279677
Iteration 5/25 | Loss: 0.00279677
Iteration 6/25 | Loss: 0.00279677
Iteration 7/25 | Loss: 0.00279677
Iteration 8/25 | Loss: 0.00279677
Iteration 9/25 | Loss: 0.00279677
Iteration 10/25 | Loss: 0.00279677
Iteration 11/25 | Loss: 0.00279677
Iteration 12/25 | Loss: 0.00279677
Iteration 13/25 | Loss: 0.00279677
Iteration 14/25 | Loss: 0.00279677
Iteration 15/25 | Loss: 0.00279677
Iteration 16/25 | Loss: 0.00279677
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0027967679779976606, 0.0027967679779976606, 0.0027967679779976606, 0.0027967679779976606, 0.0027967679779976606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027967679779976606

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00279677
Iteration 2/1000 | Loss: 0.00003470
Iteration 3/1000 | Loss: 0.00002526
Iteration 4/1000 | Loss: 0.00002196
Iteration 5/1000 | Loss: 0.00001966
Iteration 6/1000 | Loss: 0.00001886
Iteration 7/1000 | Loss: 0.00001810
Iteration 8/1000 | Loss: 0.00001750
Iteration 9/1000 | Loss: 0.00001714
Iteration 10/1000 | Loss: 0.00001694
Iteration 11/1000 | Loss: 0.00001685
Iteration 12/1000 | Loss: 0.00001681
Iteration 13/1000 | Loss: 0.00001681
Iteration 14/1000 | Loss: 0.00001680
Iteration 15/1000 | Loss: 0.00001668
Iteration 16/1000 | Loss: 0.00001667
Iteration 17/1000 | Loss: 0.00001666
Iteration 18/1000 | Loss: 0.00001666
Iteration 19/1000 | Loss: 0.00001665
Iteration 20/1000 | Loss: 0.00001665
Iteration 21/1000 | Loss: 0.00001664
Iteration 22/1000 | Loss: 0.00001663
Iteration 23/1000 | Loss: 0.00001663
Iteration 24/1000 | Loss: 0.00001663
Iteration 25/1000 | Loss: 0.00001662
Iteration 26/1000 | Loss: 0.00001662
Iteration 27/1000 | Loss: 0.00001660
Iteration 28/1000 | Loss: 0.00001660
Iteration 29/1000 | Loss: 0.00001660
Iteration 30/1000 | Loss: 0.00001659
Iteration 31/1000 | Loss: 0.00001659
Iteration 32/1000 | Loss: 0.00001659
Iteration 33/1000 | Loss: 0.00001658
Iteration 34/1000 | Loss: 0.00001658
Iteration 35/1000 | Loss: 0.00001658
Iteration 36/1000 | Loss: 0.00001658
Iteration 37/1000 | Loss: 0.00001658
Iteration 38/1000 | Loss: 0.00001657
Iteration 39/1000 | Loss: 0.00001657
Iteration 40/1000 | Loss: 0.00001657
Iteration 41/1000 | Loss: 0.00001657
Iteration 42/1000 | Loss: 0.00001657
Iteration 43/1000 | Loss: 0.00001656
Iteration 44/1000 | Loss: 0.00001656
Iteration 45/1000 | Loss: 0.00001656
Iteration 46/1000 | Loss: 0.00001656
Iteration 47/1000 | Loss: 0.00001655
Iteration 48/1000 | Loss: 0.00001655
Iteration 49/1000 | Loss: 0.00001655
Iteration 50/1000 | Loss: 0.00001655
Iteration 51/1000 | Loss: 0.00001655
Iteration 52/1000 | Loss: 0.00001655
Iteration 53/1000 | Loss: 0.00001655
Iteration 54/1000 | Loss: 0.00001655
Iteration 55/1000 | Loss: 0.00001655
Iteration 56/1000 | Loss: 0.00001655
Iteration 57/1000 | Loss: 0.00001654
Iteration 58/1000 | Loss: 0.00001654
Iteration 59/1000 | Loss: 0.00001654
Iteration 60/1000 | Loss: 0.00001654
Iteration 61/1000 | Loss: 0.00001654
Iteration 62/1000 | Loss: 0.00001654
Iteration 63/1000 | Loss: 0.00001654
Iteration 64/1000 | Loss: 0.00001653
Iteration 65/1000 | Loss: 0.00001653
Iteration 66/1000 | Loss: 0.00001653
Iteration 67/1000 | Loss: 0.00001653
Iteration 68/1000 | Loss: 0.00001653
Iteration 69/1000 | Loss: 0.00001653
Iteration 70/1000 | Loss: 0.00001653
Iteration 71/1000 | Loss: 0.00001653
Iteration 72/1000 | Loss: 0.00001653
Iteration 73/1000 | Loss: 0.00001653
Iteration 74/1000 | Loss: 0.00001652
Iteration 75/1000 | Loss: 0.00001652
Iteration 76/1000 | Loss: 0.00001652
Iteration 77/1000 | Loss: 0.00001652
Iteration 78/1000 | Loss: 0.00001652
Iteration 79/1000 | Loss: 0.00001652
Iteration 80/1000 | Loss: 0.00001652
Iteration 81/1000 | Loss: 0.00001652
Iteration 82/1000 | Loss: 0.00001652
Iteration 83/1000 | Loss: 0.00001652
Iteration 84/1000 | Loss: 0.00001652
Iteration 85/1000 | Loss: 0.00001652
Iteration 86/1000 | Loss: 0.00001652
Iteration 87/1000 | Loss: 0.00001651
Iteration 88/1000 | Loss: 0.00001651
Iteration 89/1000 | Loss: 0.00001651
Iteration 90/1000 | Loss: 0.00001651
Iteration 91/1000 | Loss: 0.00001651
Iteration 92/1000 | Loss: 0.00001651
Iteration 93/1000 | Loss: 0.00001651
Iteration 94/1000 | Loss: 0.00001651
Iteration 95/1000 | Loss: 0.00001651
Iteration 96/1000 | Loss: 0.00001650
Iteration 97/1000 | Loss: 0.00001650
Iteration 98/1000 | Loss: 0.00001650
Iteration 99/1000 | Loss: 0.00001650
Iteration 100/1000 | Loss: 0.00001650
Iteration 101/1000 | Loss: 0.00001650
Iteration 102/1000 | Loss: 0.00001650
Iteration 103/1000 | Loss: 0.00001650
Iteration 104/1000 | Loss: 0.00001650
Iteration 105/1000 | Loss: 0.00001650
Iteration 106/1000 | Loss: 0.00001650
Iteration 107/1000 | Loss: 0.00001650
Iteration 108/1000 | Loss: 0.00001650
Iteration 109/1000 | Loss: 0.00001650
Iteration 110/1000 | Loss: 0.00001650
Iteration 111/1000 | Loss: 0.00001650
Iteration 112/1000 | Loss: 0.00001650
Iteration 113/1000 | Loss: 0.00001650
Iteration 114/1000 | Loss: 0.00001650
Iteration 115/1000 | Loss: 0.00001650
Iteration 116/1000 | Loss: 0.00001650
Iteration 117/1000 | Loss: 0.00001650
Iteration 118/1000 | Loss: 0.00001650
Iteration 119/1000 | Loss: 0.00001650
Iteration 120/1000 | Loss: 0.00001650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.6497724573127925e-05, 1.6497724573127925e-05, 1.6497724573127925e-05, 1.6497724573127925e-05, 1.6497724573127925e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6497724573127925e-05

Optimization complete. Final v2v error: 3.455137252807617 mm

Highest mean error: 3.966834306716919 mm for frame 203

Lowest mean error: 3.075449228286743 mm for frame 36

Saving results

Total time: 40.267436504364014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426982
Iteration 2/25 | Loss: 0.00151134
Iteration 3/25 | Loss: 0.00139848
Iteration 4/25 | Loss: 0.00138957
Iteration 5/25 | Loss: 0.00138753
Iteration 6/25 | Loss: 0.00138753
Iteration 7/25 | Loss: 0.00138753
Iteration 8/25 | Loss: 0.00138753
Iteration 9/25 | Loss: 0.00138753
Iteration 10/25 | Loss: 0.00138753
Iteration 11/25 | Loss: 0.00138753
Iteration 12/25 | Loss: 0.00138753
Iteration 13/25 | Loss: 0.00138753
Iteration 14/25 | Loss: 0.00138753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00138753408100456, 0.00138753408100456, 0.00138753408100456, 0.00138753408100456, 0.00138753408100456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00138753408100456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18417740
Iteration 2/25 | Loss: 0.00291894
Iteration 3/25 | Loss: 0.00291893
Iteration 4/25 | Loss: 0.00291893
Iteration 5/25 | Loss: 0.00291893
Iteration 6/25 | Loss: 0.00291893
Iteration 7/25 | Loss: 0.00291893
Iteration 8/25 | Loss: 0.00291893
Iteration 9/25 | Loss: 0.00291893
Iteration 10/25 | Loss: 0.00291893
Iteration 11/25 | Loss: 0.00291893
Iteration 12/25 | Loss: 0.00291893
Iteration 13/25 | Loss: 0.00291893
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.002918932819738984, 0.002918932819738984, 0.002918932819738984, 0.002918932819738984, 0.002918932819738984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002918932819738984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00291893
Iteration 2/1000 | Loss: 0.00002995
Iteration 3/1000 | Loss: 0.00002023
Iteration 4/1000 | Loss: 0.00001704
Iteration 5/1000 | Loss: 0.00001574
Iteration 6/1000 | Loss: 0.00001506
Iteration 7/1000 | Loss: 0.00001465
Iteration 8/1000 | Loss: 0.00001424
Iteration 9/1000 | Loss: 0.00001397
Iteration 10/1000 | Loss: 0.00001375
Iteration 11/1000 | Loss: 0.00001373
Iteration 12/1000 | Loss: 0.00001373
Iteration 13/1000 | Loss: 0.00001372
Iteration 14/1000 | Loss: 0.00001370
Iteration 15/1000 | Loss: 0.00001369
Iteration 16/1000 | Loss: 0.00001368
Iteration 17/1000 | Loss: 0.00001367
Iteration 18/1000 | Loss: 0.00001366
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001365
Iteration 21/1000 | Loss: 0.00001365
Iteration 22/1000 | Loss: 0.00001364
Iteration 23/1000 | Loss: 0.00001364
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001363
Iteration 26/1000 | Loss: 0.00001363
Iteration 27/1000 | Loss: 0.00001360
Iteration 28/1000 | Loss: 0.00001360
Iteration 29/1000 | Loss: 0.00001359
Iteration 30/1000 | Loss: 0.00001359
Iteration 31/1000 | Loss: 0.00001359
Iteration 32/1000 | Loss: 0.00001359
Iteration 33/1000 | Loss: 0.00001359
Iteration 34/1000 | Loss: 0.00001358
Iteration 35/1000 | Loss: 0.00001358
Iteration 36/1000 | Loss: 0.00001358
Iteration 37/1000 | Loss: 0.00001358
Iteration 38/1000 | Loss: 0.00001358
Iteration 39/1000 | Loss: 0.00001358
Iteration 40/1000 | Loss: 0.00001358
Iteration 41/1000 | Loss: 0.00001358
Iteration 42/1000 | Loss: 0.00001357
Iteration 43/1000 | Loss: 0.00001357
Iteration 44/1000 | Loss: 0.00001357
Iteration 45/1000 | Loss: 0.00001357
Iteration 46/1000 | Loss: 0.00001356
Iteration 47/1000 | Loss: 0.00001356
Iteration 48/1000 | Loss: 0.00001355
Iteration 49/1000 | Loss: 0.00001355
Iteration 50/1000 | Loss: 0.00001355
Iteration 51/1000 | Loss: 0.00001354
Iteration 52/1000 | Loss: 0.00001353
Iteration 53/1000 | Loss: 0.00001353
Iteration 54/1000 | Loss: 0.00001353
Iteration 55/1000 | Loss: 0.00001353
Iteration 56/1000 | Loss: 0.00001353
Iteration 57/1000 | Loss: 0.00001352
Iteration 58/1000 | Loss: 0.00001352
Iteration 59/1000 | Loss: 0.00001352
Iteration 60/1000 | Loss: 0.00001352
Iteration 61/1000 | Loss: 0.00001352
Iteration 62/1000 | Loss: 0.00001352
Iteration 63/1000 | Loss: 0.00001352
Iteration 64/1000 | Loss: 0.00001352
Iteration 65/1000 | Loss: 0.00001351
Iteration 66/1000 | Loss: 0.00001351
Iteration 67/1000 | Loss: 0.00001351
Iteration 68/1000 | Loss: 0.00001351
Iteration 69/1000 | Loss: 0.00001351
Iteration 70/1000 | Loss: 0.00001351
Iteration 71/1000 | Loss: 0.00001351
Iteration 72/1000 | Loss: 0.00001351
Iteration 73/1000 | Loss: 0.00001351
Iteration 74/1000 | Loss: 0.00001350
Iteration 75/1000 | Loss: 0.00001350
Iteration 76/1000 | Loss: 0.00001350
Iteration 77/1000 | Loss: 0.00001350
Iteration 78/1000 | Loss: 0.00001350
Iteration 79/1000 | Loss: 0.00001350
Iteration 80/1000 | Loss: 0.00001350
Iteration 81/1000 | Loss: 0.00001350
Iteration 82/1000 | Loss: 0.00001350
Iteration 83/1000 | Loss: 0.00001350
Iteration 84/1000 | Loss: 0.00001350
Iteration 85/1000 | Loss: 0.00001350
Iteration 86/1000 | Loss: 0.00001350
Iteration 87/1000 | Loss: 0.00001349
Iteration 88/1000 | Loss: 0.00001349
Iteration 89/1000 | Loss: 0.00001349
Iteration 90/1000 | Loss: 0.00001349
Iteration 91/1000 | Loss: 0.00001349
Iteration 92/1000 | Loss: 0.00001349
Iteration 93/1000 | Loss: 0.00001349
Iteration 94/1000 | Loss: 0.00001349
Iteration 95/1000 | Loss: 0.00001349
Iteration 96/1000 | Loss: 0.00001349
Iteration 97/1000 | Loss: 0.00001349
Iteration 98/1000 | Loss: 0.00001349
Iteration 99/1000 | Loss: 0.00001349
Iteration 100/1000 | Loss: 0.00001349
Iteration 101/1000 | Loss: 0.00001349
Iteration 102/1000 | Loss: 0.00001349
Iteration 103/1000 | Loss: 0.00001349
Iteration 104/1000 | Loss: 0.00001349
Iteration 105/1000 | Loss: 0.00001348
Iteration 106/1000 | Loss: 0.00001348
Iteration 107/1000 | Loss: 0.00001348
Iteration 108/1000 | Loss: 0.00001348
Iteration 109/1000 | Loss: 0.00001348
Iteration 110/1000 | Loss: 0.00001348
Iteration 111/1000 | Loss: 0.00001348
Iteration 112/1000 | Loss: 0.00001348
Iteration 113/1000 | Loss: 0.00001348
Iteration 114/1000 | Loss: 0.00001348
Iteration 115/1000 | Loss: 0.00001347
Iteration 116/1000 | Loss: 0.00001347
Iteration 117/1000 | Loss: 0.00001347
Iteration 118/1000 | Loss: 0.00001347
Iteration 119/1000 | Loss: 0.00001347
Iteration 120/1000 | Loss: 0.00001347
Iteration 121/1000 | Loss: 0.00001347
Iteration 122/1000 | Loss: 0.00001347
Iteration 123/1000 | Loss: 0.00001347
Iteration 124/1000 | Loss: 0.00001347
Iteration 125/1000 | Loss: 0.00001347
Iteration 126/1000 | Loss: 0.00001347
Iteration 127/1000 | Loss: 0.00001347
Iteration 128/1000 | Loss: 0.00001347
Iteration 129/1000 | Loss: 0.00001346
Iteration 130/1000 | Loss: 0.00001346
Iteration 131/1000 | Loss: 0.00001346
Iteration 132/1000 | Loss: 0.00001346
Iteration 133/1000 | Loss: 0.00001346
Iteration 134/1000 | Loss: 0.00001346
Iteration 135/1000 | Loss: 0.00001346
Iteration 136/1000 | Loss: 0.00001345
Iteration 137/1000 | Loss: 0.00001345
Iteration 138/1000 | Loss: 0.00001345
Iteration 139/1000 | Loss: 0.00001345
Iteration 140/1000 | Loss: 0.00001345
Iteration 141/1000 | Loss: 0.00001345
Iteration 142/1000 | Loss: 0.00001345
Iteration 143/1000 | Loss: 0.00001345
Iteration 144/1000 | Loss: 0.00001345
Iteration 145/1000 | Loss: 0.00001345
Iteration 146/1000 | Loss: 0.00001345
Iteration 147/1000 | Loss: 0.00001345
Iteration 148/1000 | Loss: 0.00001345
Iteration 149/1000 | Loss: 0.00001345
Iteration 150/1000 | Loss: 0.00001345
Iteration 151/1000 | Loss: 0.00001345
Iteration 152/1000 | Loss: 0.00001345
Iteration 153/1000 | Loss: 0.00001345
Iteration 154/1000 | Loss: 0.00001345
Iteration 155/1000 | Loss: 0.00001345
Iteration 156/1000 | Loss: 0.00001345
Iteration 157/1000 | Loss: 0.00001344
Iteration 158/1000 | Loss: 0.00001344
Iteration 159/1000 | Loss: 0.00001344
Iteration 160/1000 | Loss: 0.00001344
Iteration 161/1000 | Loss: 0.00001344
Iteration 162/1000 | Loss: 0.00001344
Iteration 163/1000 | Loss: 0.00001344
Iteration 164/1000 | Loss: 0.00001344
Iteration 165/1000 | Loss: 0.00001344
Iteration 166/1000 | Loss: 0.00001344
Iteration 167/1000 | Loss: 0.00001344
Iteration 168/1000 | Loss: 0.00001344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.344466090813512e-05, 1.344466090813512e-05, 1.344466090813512e-05, 1.344466090813512e-05, 1.344466090813512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.344466090813512e-05

Optimization complete. Final v2v error: 3.148649215698242 mm

Highest mean error: 3.5061628818511963 mm for frame 58

Lowest mean error: 2.866185188293457 mm for frame 2

Saving results

Total time: 63.53233623504639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00932133
Iteration 2/25 | Loss: 0.00195772
Iteration 3/25 | Loss: 0.00162277
Iteration 4/25 | Loss: 0.00157050
Iteration 5/25 | Loss: 0.00156615
Iteration 6/25 | Loss: 0.00151981
Iteration 7/25 | Loss: 0.00150360
Iteration 8/25 | Loss: 0.00149700
Iteration 9/25 | Loss: 0.00149525
Iteration 10/25 | Loss: 0.00148691
Iteration 11/25 | Loss: 0.00148472
Iteration 12/25 | Loss: 0.00148428
Iteration 13/25 | Loss: 0.00148410
Iteration 14/25 | Loss: 0.00148397
Iteration 15/25 | Loss: 0.00148396
Iteration 16/25 | Loss: 0.00148396
Iteration 17/25 | Loss: 0.00148396
Iteration 18/25 | Loss: 0.00148396
Iteration 19/25 | Loss: 0.00148396
Iteration 20/25 | Loss: 0.00148396
Iteration 21/25 | Loss: 0.00148396
Iteration 22/25 | Loss: 0.00148396
Iteration 23/25 | Loss: 0.00148396
Iteration 24/25 | Loss: 0.00148396
Iteration 25/25 | Loss: 0.00148396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.26500463
Iteration 2/25 | Loss: 0.00299917
Iteration 3/25 | Loss: 0.00299917
Iteration 4/25 | Loss: 0.00299917
Iteration 5/25 | Loss: 0.00299917
Iteration 6/25 | Loss: 0.00299917
Iteration 7/25 | Loss: 0.00299917
Iteration 8/25 | Loss: 0.00299917
Iteration 9/25 | Loss: 0.00299917
Iteration 10/25 | Loss: 0.00299917
Iteration 11/25 | Loss: 0.00299917
Iteration 12/25 | Loss: 0.00299917
Iteration 13/25 | Loss: 0.00299917
Iteration 14/25 | Loss: 0.00299917
Iteration 15/25 | Loss: 0.00299917
Iteration 16/25 | Loss: 0.00299917
Iteration 17/25 | Loss: 0.00299917
Iteration 18/25 | Loss: 0.00299917
Iteration 19/25 | Loss: 0.00299917
Iteration 20/25 | Loss: 0.00299917
Iteration 21/25 | Loss: 0.00299917
Iteration 22/25 | Loss: 0.00299917
Iteration 23/25 | Loss: 0.00299917
Iteration 24/25 | Loss: 0.00299917
Iteration 25/25 | Loss: 0.00299917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00299917
Iteration 2/1000 | Loss: 0.00009618
Iteration 3/1000 | Loss: 0.00005997
Iteration 4/1000 | Loss: 0.00004437
Iteration 5/1000 | Loss: 0.00003852
Iteration 6/1000 | Loss: 0.00003512
Iteration 7/1000 | Loss: 0.00003299
Iteration 8/1000 | Loss: 0.00003175
Iteration 9/1000 | Loss: 0.00010932
Iteration 10/1000 | Loss: 0.00020236
Iteration 11/1000 | Loss: 0.00003026
Iteration 12/1000 | Loss: 0.00010444
Iteration 13/1000 | Loss: 0.00002966
Iteration 14/1000 | Loss: 0.00002928
Iteration 15/1000 | Loss: 0.00022231
Iteration 16/1000 | Loss: 0.00016740
Iteration 17/1000 | Loss: 0.00009604
Iteration 18/1000 | Loss: 0.00008548
Iteration 19/1000 | Loss: 0.00023412
Iteration 20/1000 | Loss: 0.00006679
Iteration 21/1000 | Loss: 0.00003450
Iteration 22/1000 | Loss: 0.00003140
Iteration 23/1000 | Loss: 0.00002914
Iteration 24/1000 | Loss: 0.00002733
Iteration 25/1000 | Loss: 0.00002608
Iteration 26/1000 | Loss: 0.00002531
Iteration 27/1000 | Loss: 0.00002492
Iteration 28/1000 | Loss: 0.00002461
Iteration 29/1000 | Loss: 0.00002458
Iteration 30/1000 | Loss: 0.00002442
Iteration 31/1000 | Loss: 0.00002441
Iteration 32/1000 | Loss: 0.00002440
Iteration 33/1000 | Loss: 0.00002440
Iteration 34/1000 | Loss: 0.00002439
Iteration 35/1000 | Loss: 0.00002437
Iteration 36/1000 | Loss: 0.00002437
Iteration 37/1000 | Loss: 0.00002435
Iteration 38/1000 | Loss: 0.00002433
Iteration 39/1000 | Loss: 0.00002429
Iteration 40/1000 | Loss: 0.00002428
Iteration 41/1000 | Loss: 0.00002426
Iteration 42/1000 | Loss: 0.00002426
Iteration 43/1000 | Loss: 0.00002426
Iteration 44/1000 | Loss: 0.00002426
Iteration 45/1000 | Loss: 0.00002426
Iteration 46/1000 | Loss: 0.00002426
Iteration 47/1000 | Loss: 0.00002425
Iteration 48/1000 | Loss: 0.00002425
Iteration 49/1000 | Loss: 0.00002425
Iteration 50/1000 | Loss: 0.00002424
Iteration 51/1000 | Loss: 0.00002423
Iteration 52/1000 | Loss: 0.00002422
Iteration 53/1000 | Loss: 0.00002422
Iteration 54/1000 | Loss: 0.00002422
Iteration 55/1000 | Loss: 0.00002422
Iteration 56/1000 | Loss: 0.00002422
Iteration 57/1000 | Loss: 0.00002421
Iteration 58/1000 | Loss: 0.00002421
Iteration 59/1000 | Loss: 0.00002421
Iteration 60/1000 | Loss: 0.00002421
Iteration 61/1000 | Loss: 0.00002421
Iteration 62/1000 | Loss: 0.00002421
Iteration 63/1000 | Loss: 0.00002421
Iteration 64/1000 | Loss: 0.00002420
Iteration 65/1000 | Loss: 0.00002419
Iteration 66/1000 | Loss: 0.00002419
Iteration 67/1000 | Loss: 0.00002419
Iteration 68/1000 | Loss: 0.00002419
Iteration 69/1000 | Loss: 0.00002419
Iteration 70/1000 | Loss: 0.00002419
Iteration 71/1000 | Loss: 0.00002419
Iteration 72/1000 | Loss: 0.00002418
Iteration 73/1000 | Loss: 0.00002418
Iteration 74/1000 | Loss: 0.00002418
Iteration 75/1000 | Loss: 0.00002418
Iteration 76/1000 | Loss: 0.00002418
Iteration 77/1000 | Loss: 0.00002418
Iteration 78/1000 | Loss: 0.00002418
Iteration 79/1000 | Loss: 0.00002418
Iteration 80/1000 | Loss: 0.00002418
Iteration 81/1000 | Loss: 0.00002417
Iteration 82/1000 | Loss: 0.00002417
Iteration 83/1000 | Loss: 0.00002417
Iteration 84/1000 | Loss: 0.00002417
Iteration 85/1000 | Loss: 0.00002417
Iteration 86/1000 | Loss: 0.00002417
Iteration 87/1000 | Loss: 0.00002417
Iteration 88/1000 | Loss: 0.00002417
Iteration 89/1000 | Loss: 0.00002417
Iteration 90/1000 | Loss: 0.00002417
Iteration 91/1000 | Loss: 0.00002417
Iteration 92/1000 | Loss: 0.00002416
Iteration 93/1000 | Loss: 0.00002416
Iteration 94/1000 | Loss: 0.00002416
Iteration 95/1000 | Loss: 0.00002416
Iteration 96/1000 | Loss: 0.00002416
Iteration 97/1000 | Loss: 0.00002416
Iteration 98/1000 | Loss: 0.00002416
Iteration 99/1000 | Loss: 0.00002416
Iteration 100/1000 | Loss: 0.00002416
Iteration 101/1000 | Loss: 0.00002416
Iteration 102/1000 | Loss: 0.00002416
Iteration 103/1000 | Loss: 0.00002416
Iteration 104/1000 | Loss: 0.00002416
Iteration 105/1000 | Loss: 0.00002416
Iteration 106/1000 | Loss: 0.00002416
Iteration 107/1000 | Loss: 0.00002416
Iteration 108/1000 | Loss: 0.00002416
Iteration 109/1000 | Loss: 0.00002415
Iteration 110/1000 | Loss: 0.00002415
Iteration 111/1000 | Loss: 0.00002415
Iteration 112/1000 | Loss: 0.00002415
Iteration 113/1000 | Loss: 0.00002415
Iteration 114/1000 | Loss: 0.00002415
Iteration 115/1000 | Loss: 0.00002415
Iteration 116/1000 | Loss: 0.00002415
Iteration 117/1000 | Loss: 0.00002415
Iteration 118/1000 | Loss: 0.00002415
Iteration 119/1000 | Loss: 0.00002415
Iteration 120/1000 | Loss: 0.00002415
Iteration 121/1000 | Loss: 0.00002414
Iteration 122/1000 | Loss: 0.00002414
Iteration 123/1000 | Loss: 0.00002414
Iteration 124/1000 | Loss: 0.00002414
Iteration 125/1000 | Loss: 0.00002414
Iteration 126/1000 | Loss: 0.00002414
Iteration 127/1000 | Loss: 0.00002414
Iteration 128/1000 | Loss: 0.00002414
Iteration 129/1000 | Loss: 0.00002414
Iteration 130/1000 | Loss: 0.00002414
Iteration 131/1000 | Loss: 0.00002414
Iteration 132/1000 | Loss: 0.00002414
Iteration 133/1000 | Loss: 0.00002414
Iteration 134/1000 | Loss: 0.00002414
Iteration 135/1000 | Loss: 0.00002414
Iteration 136/1000 | Loss: 0.00002414
Iteration 137/1000 | Loss: 0.00002414
Iteration 138/1000 | Loss: 0.00002414
Iteration 139/1000 | Loss: 0.00002414
Iteration 140/1000 | Loss: 0.00002414
Iteration 141/1000 | Loss: 0.00002414
Iteration 142/1000 | Loss: 0.00002414
Iteration 143/1000 | Loss: 0.00002414
Iteration 144/1000 | Loss: 0.00002414
Iteration 145/1000 | Loss: 0.00002414
Iteration 146/1000 | Loss: 0.00002414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.414252594462596e-05, 2.414252594462596e-05, 2.414252594462596e-05, 2.414252594462596e-05, 2.414252594462596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.414252594462596e-05

Optimization complete. Final v2v error: 4.121326446533203 mm

Highest mean error: 14.584233283996582 mm for frame 55

Lowest mean error: 3.423741579055786 mm for frame 151

Saving results

Total time: 105.47874045372009
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487645
Iteration 2/25 | Loss: 0.00147111
Iteration 3/25 | Loss: 0.00139336
Iteration 4/25 | Loss: 0.00138606
Iteration 5/25 | Loss: 0.00138308
Iteration 6/25 | Loss: 0.00138278
Iteration 7/25 | Loss: 0.00138278
Iteration 8/25 | Loss: 0.00138278
Iteration 9/25 | Loss: 0.00138278
Iteration 10/25 | Loss: 0.00138278
Iteration 11/25 | Loss: 0.00138278
Iteration 12/25 | Loss: 0.00138278
Iteration 13/25 | Loss: 0.00138278
Iteration 14/25 | Loss: 0.00138278
Iteration 15/25 | Loss: 0.00138278
Iteration 16/25 | Loss: 0.00138278
Iteration 17/25 | Loss: 0.00138278
Iteration 18/25 | Loss: 0.00138278
Iteration 19/25 | Loss: 0.00138278
Iteration 20/25 | Loss: 0.00138278
Iteration 21/25 | Loss: 0.00138278
Iteration 22/25 | Loss: 0.00138278
Iteration 23/25 | Loss: 0.00138278
Iteration 24/25 | Loss: 0.00138278
Iteration 25/25 | Loss: 0.00138278

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29022729
Iteration 2/25 | Loss: 0.00295744
Iteration 3/25 | Loss: 0.00295744
Iteration 4/25 | Loss: 0.00295744
Iteration 5/25 | Loss: 0.00295744
Iteration 6/25 | Loss: 0.00295743
Iteration 7/25 | Loss: 0.00295743
Iteration 8/25 | Loss: 0.00295743
Iteration 9/25 | Loss: 0.00295743
Iteration 10/25 | Loss: 0.00295743
Iteration 11/25 | Loss: 0.00295743
Iteration 12/25 | Loss: 0.00295743
Iteration 13/25 | Loss: 0.00295743
Iteration 14/25 | Loss: 0.00295743
Iteration 15/25 | Loss: 0.00295743
Iteration 16/25 | Loss: 0.00295743
Iteration 17/25 | Loss: 0.00295743
Iteration 18/25 | Loss: 0.00295743
Iteration 19/25 | Loss: 0.00295743
Iteration 20/25 | Loss: 0.00295743
Iteration 21/25 | Loss: 0.00295743
Iteration 22/25 | Loss: 0.00295743
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002957430901005864, 0.002957430901005864, 0.002957430901005864, 0.002957430901005864, 0.002957430901005864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002957430901005864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00295743
Iteration 2/1000 | Loss: 0.00003331
Iteration 3/1000 | Loss: 0.00002329
Iteration 4/1000 | Loss: 0.00002053
Iteration 5/1000 | Loss: 0.00001906
Iteration 6/1000 | Loss: 0.00001846
Iteration 7/1000 | Loss: 0.00001790
Iteration 8/1000 | Loss: 0.00001754
Iteration 9/1000 | Loss: 0.00001725
Iteration 10/1000 | Loss: 0.00001717
Iteration 11/1000 | Loss: 0.00001707
Iteration 12/1000 | Loss: 0.00001703
Iteration 13/1000 | Loss: 0.00001699
Iteration 14/1000 | Loss: 0.00001699
Iteration 15/1000 | Loss: 0.00001699
Iteration 16/1000 | Loss: 0.00001699
Iteration 17/1000 | Loss: 0.00001697
Iteration 18/1000 | Loss: 0.00001696
Iteration 19/1000 | Loss: 0.00001696
Iteration 20/1000 | Loss: 0.00001691
Iteration 21/1000 | Loss: 0.00001689
Iteration 22/1000 | Loss: 0.00001689
Iteration 23/1000 | Loss: 0.00001689
Iteration 24/1000 | Loss: 0.00001688
Iteration 25/1000 | Loss: 0.00001685
Iteration 26/1000 | Loss: 0.00001684
Iteration 27/1000 | Loss: 0.00001683
Iteration 28/1000 | Loss: 0.00001683
Iteration 29/1000 | Loss: 0.00001682
Iteration 30/1000 | Loss: 0.00001682
Iteration 31/1000 | Loss: 0.00001682
Iteration 32/1000 | Loss: 0.00001682
Iteration 33/1000 | Loss: 0.00001682
Iteration 34/1000 | Loss: 0.00001681
Iteration 35/1000 | Loss: 0.00001681
Iteration 36/1000 | Loss: 0.00001681
Iteration 37/1000 | Loss: 0.00001681
Iteration 38/1000 | Loss: 0.00001680
Iteration 39/1000 | Loss: 0.00001680
Iteration 40/1000 | Loss: 0.00001680
Iteration 41/1000 | Loss: 0.00001679
Iteration 42/1000 | Loss: 0.00001679
Iteration 43/1000 | Loss: 0.00001679
Iteration 44/1000 | Loss: 0.00001679
Iteration 45/1000 | Loss: 0.00001679
Iteration 46/1000 | Loss: 0.00001679
Iteration 47/1000 | Loss: 0.00001679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 47. Stopping optimization.
Last 5 losses: [1.6791174857644364e-05, 1.6791174857644364e-05, 1.6791174857644364e-05, 1.6791174857644364e-05, 1.6791174857644364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6791174857644364e-05

Optimization complete. Final v2v error: 3.5096404552459717 mm

Highest mean error: 3.940545082092285 mm for frame 83

Lowest mean error: 3.174701452255249 mm for frame 104

Saving results

Total time: 29.26931929588318
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909452
Iteration 2/25 | Loss: 0.00183072
Iteration 3/25 | Loss: 0.00157795
Iteration 4/25 | Loss: 0.00153171
Iteration 5/25 | Loss: 0.00152333
Iteration 6/25 | Loss: 0.00151567
Iteration 7/25 | Loss: 0.00150089
Iteration 8/25 | Loss: 0.00149590
Iteration 9/25 | Loss: 0.00149285
Iteration 10/25 | Loss: 0.00148893
Iteration 11/25 | Loss: 0.00149030
Iteration 12/25 | Loss: 0.00148795
Iteration 13/25 | Loss: 0.00148812
Iteration 14/25 | Loss: 0.00148776
Iteration 15/25 | Loss: 0.00148268
Iteration 16/25 | Loss: 0.00148773
Iteration 17/25 | Loss: 0.00148890
Iteration 18/25 | Loss: 0.00148294
Iteration 19/25 | Loss: 0.00148339
Iteration 20/25 | Loss: 0.00148278
Iteration 21/25 | Loss: 0.00148563
Iteration 22/25 | Loss: 0.00147909
Iteration 23/25 | Loss: 0.00147927
Iteration 24/25 | Loss: 0.00147654
Iteration 25/25 | Loss: 0.00147529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54230011
Iteration 2/25 | Loss: 0.00325603
Iteration 3/25 | Loss: 0.00325602
Iteration 4/25 | Loss: 0.00325602
Iteration 5/25 | Loss: 0.00325602
Iteration 6/25 | Loss: 0.00325602
Iteration 7/25 | Loss: 0.00325602
Iteration 8/25 | Loss: 0.00325602
Iteration 9/25 | Loss: 0.00325602
Iteration 10/25 | Loss: 0.00325602
Iteration 11/25 | Loss: 0.00325602
Iteration 12/25 | Loss: 0.00325602
Iteration 13/25 | Loss: 0.00325602
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00325602269731462, 0.00325602269731462, 0.00325602269731462, 0.00325602269731462, 0.00325602269731462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00325602269731462

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00325602
Iteration 2/1000 | Loss: 0.00024814
Iteration 3/1000 | Loss: 0.00061443
Iteration 4/1000 | Loss: 0.00010417
Iteration 5/1000 | Loss: 0.00005393
Iteration 6/1000 | Loss: 0.00011323
Iteration 7/1000 | Loss: 0.00007310
Iteration 8/1000 | Loss: 0.00012507
Iteration 9/1000 | Loss: 0.00008638
Iteration 10/1000 | Loss: 0.00011492
Iteration 11/1000 | Loss: 0.00011759
Iteration 12/1000 | Loss: 0.00031935
Iteration 13/1000 | Loss: 0.00031203
Iteration 14/1000 | Loss: 0.00035065
Iteration 15/1000 | Loss: 0.00054869
Iteration 16/1000 | Loss: 0.00013552
Iteration 17/1000 | Loss: 0.00012492
Iteration 18/1000 | Loss: 0.00004536
Iteration 19/1000 | Loss: 0.00015851
Iteration 20/1000 | Loss: 0.00007254
Iteration 21/1000 | Loss: 0.00022027
Iteration 22/1000 | Loss: 0.00013812
Iteration 23/1000 | Loss: 0.00006490
Iteration 24/1000 | Loss: 0.00011978
Iteration 25/1000 | Loss: 0.00009249
Iteration 26/1000 | Loss: 0.00008770
Iteration 27/1000 | Loss: 0.00005453
Iteration 28/1000 | Loss: 0.00011116
Iteration 29/1000 | Loss: 0.00015107
Iteration 30/1000 | Loss: 0.00008844
Iteration 31/1000 | Loss: 0.00010041
Iteration 32/1000 | Loss: 0.00013869
Iteration 33/1000 | Loss: 0.00013186
Iteration 34/1000 | Loss: 0.00014791
Iteration 35/1000 | Loss: 0.00012422
Iteration 36/1000 | Loss: 0.00009008
Iteration 37/1000 | Loss: 0.00009147
Iteration 38/1000 | Loss: 0.00011429
Iteration 39/1000 | Loss: 0.00011589
Iteration 40/1000 | Loss: 0.00016635
Iteration 41/1000 | Loss: 0.00016061
Iteration 42/1000 | Loss: 0.00014527
Iteration 43/1000 | Loss: 0.00009008
Iteration 44/1000 | Loss: 0.00013849
Iteration 45/1000 | Loss: 0.00010245
Iteration 46/1000 | Loss: 0.00007561
Iteration 47/1000 | Loss: 0.00011113
Iteration 48/1000 | Loss: 0.00008597
Iteration 49/1000 | Loss: 0.00010457
Iteration 50/1000 | Loss: 0.00008342
Iteration 51/1000 | Loss: 0.00009823
Iteration 52/1000 | Loss: 0.00008509
Iteration 53/1000 | Loss: 0.00011189
Iteration 54/1000 | Loss: 0.00012212
Iteration 55/1000 | Loss: 0.00010196
Iteration 56/1000 | Loss: 0.00007169
Iteration 57/1000 | Loss: 0.00008347
Iteration 58/1000 | Loss: 0.00006794
Iteration 59/1000 | Loss: 0.00015256
Iteration 60/1000 | Loss: 0.00008459
Iteration 61/1000 | Loss: 0.00004822
Iteration 62/1000 | Loss: 0.00013974
Iteration 63/1000 | Loss: 0.00011078
Iteration 64/1000 | Loss: 0.00011712
Iteration 65/1000 | Loss: 0.00014948
Iteration 66/1000 | Loss: 0.00010610
Iteration 67/1000 | Loss: 0.00007846
Iteration 68/1000 | Loss: 0.00009337
Iteration 69/1000 | Loss: 0.00007355
Iteration 70/1000 | Loss: 0.00011762
Iteration 71/1000 | Loss: 0.00007357
Iteration 72/1000 | Loss: 0.00011534
Iteration 73/1000 | Loss: 0.00018508
Iteration 74/1000 | Loss: 0.00012454
Iteration 75/1000 | Loss: 0.00006680
Iteration 76/1000 | Loss: 0.00008736
Iteration 77/1000 | Loss: 0.00015014
Iteration 78/1000 | Loss: 0.00008494
Iteration 79/1000 | Loss: 0.00012372
Iteration 80/1000 | Loss: 0.00003911
Iteration 81/1000 | Loss: 0.00003901
Iteration 82/1000 | Loss: 0.00003945
Iteration 83/1000 | Loss: 0.00004292
Iteration 84/1000 | Loss: 0.00003852
Iteration 85/1000 | Loss: 0.00003536
Iteration 86/1000 | Loss: 0.00003866
Iteration 87/1000 | Loss: 0.00004082
Iteration 88/1000 | Loss: 0.00003822
Iteration 89/1000 | Loss: 0.00003204
Iteration 90/1000 | Loss: 0.00003821
Iteration 91/1000 | Loss: 0.00003954
Iteration 92/1000 | Loss: 0.00003333
Iteration 93/1000 | Loss: 0.00003458
Iteration 94/1000 | Loss: 0.00003510
Iteration 95/1000 | Loss: 0.00003418
Iteration 96/1000 | Loss: 0.00003845
Iteration 97/1000 | Loss: 0.00003785
Iteration 98/1000 | Loss: 0.00003363
Iteration 99/1000 | Loss: 0.00003892
Iteration 100/1000 | Loss: 0.00003886
Iteration 101/1000 | Loss: 0.00003814
Iteration 102/1000 | Loss: 0.00003780
Iteration 103/1000 | Loss: 0.00003752
Iteration 104/1000 | Loss: 0.00003816
Iteration 105/1000 | Loss: 0.00003693
Iteration 106/1000 | Loss: 0.00003723
Iteration 107/1000 | Loss: 0.00003478
Iteration 108/1000 | Loss: 0.00004209
Iteration 109/1000 | Loss: 0.00003687
Iteration 110/1000 | Loss: 0.00003779
Iteration 111/1000 | Loss: 0.00004082
Iteration 112/1000 | Loss: 0.00003861
Iteration 113/1000 | Loss: 0.00003657
Iteration 114/1000 | Loss: 0.00003399
Iteration 115/1000 | Loss: 0.00003503
Iteration 116/1000 | Loss: 0.00003222
Iteration 117/1000 | Loss: 0.00003281
Iteration 118/1000 | Loss: 0.00003547
Iteration 119/1000 | Loss: 0.00003096
Iteration 120/1000 | Loss: 0.00004293
Iteration 121/1000 | Loss: 0.00003576
Iteration 122/1000 | Loss: 0.00003538
Iteration 123/1000 | Loss: 0.00003865
Iteration 124/1000 | Loss: 0.00002887
Iteration 125/1000 | Loss: 0.00002704
Iteration 126/1000 | Loss: 0.00002620
Iteration 127/1000 | Loss: 0.00002574
Iteration 128/1000 | Loss: 0.00002549
Iteration 129/1000 | Loss: 0.00002528
Iteration 130/1000 | Loss: 0.00002515
Iteration 131/1000 | Loss: 0.00002513
Iteration 132/1000 | Loss: 0.00002507
Iteration 133/1000 | Loss: 0.00002506
Iteration 134/1000 | Loss: 0.00002505
Iteration 135/1000 | Loss: 0.00002504
Iteration 136/1000 | Loss: 0.00002504
Iteration 137/1000 | Loss: 0.00002503
Iteration 138/1000 | Loss: 0.00002503
Iteration 139/1000 | Loss: 0.00002502
Iteration 140/1000 | Loss: 0.00002499
Iteration 141/1000 | Loss: 0.00002496
Iteration 142/1000 | Loss: 0.00002496
Iteration 143/1000 | Loss: 0.00002495
Iteration 144/1000 | Loss: 0.00002495
Iteration 145/1000 | Loss: 0.00002494
Iteration 146/1000 | Loss: 0.00002494
Iteration 147/1000 | Loss: 0.00002494
Iteration 148/1000 | Loss: 0.00002494
Iteration 149/1000 | Loss: 0.00002493
Iteration 150/1000 | Loss: 0.00002493
Iteration 151/1000 | Loss: 0.00002493
Iteration 152/1000 | Loss: 0.00002493
Iteration 153/1000 | Loss: 0.00002493
Iteration 154/1000 | Loss: 0.00002493
Iteration 155/1000 | Loss: 0.00002493
Iteration 156/1000 | Loss: 0.00002493
Iteration 157/1000 | Loss: 0.00002493
Iteration 158/1000 | Loss: 0.00002492
Iteration 159/1000 | Loss: 0.00002492
Iteration 160/1000 | Loss: 0.00002492
Iteration 161/1000 | Loss: 0.00002492
Iteration 162/1000 | Loss: 0.00002491
Iteration 163/1000 | Loss: 0.00002491
Iteration 164/1000 | Loss: 0.00002491
Iteration 165/1000 | Loss: 0.00002491
Iteration 166/1000 | Loss: 0.00002490
Iteration 167/1000 | Loss: 0.00002490
Iteration 168/1000 | Loss: 0.00002490
Iteration 169/1000 | Loss: 0.00002490
Iteration 170/1000 | Loss: 0.00002490
Iteration 171/1000 | Loss: 0.00002490
Iteration 172/1000 | Loss: 0.00002490
Iteration 173/1000 | Loss: 0.00002490
Iteration 174/1000 | Loss: 0.00002490
Iteration 175/1000 | Loss: 0.00002489
Iteration 176/1000 | Loss: 0.00002489
Iteration 177/1000 | Loss: 0.00002489
Iteration 178/1000 | Loss: 0.00002489
Iteration 179/1000 | Loss: 0.00002489
Iteration 180/1000 | Loss: 0.00002489
Iteration 181/1000 | Loss: 0.00002489
Iteration 182/1000 | Loss: 0.00002489
Iteration 183/1000 | Loss: 0.00002489
Iteration 184/1000 | Loss: 0.00002489
Iteration 185/1000 | Loss: 0.00002489
Iteration 186/1000 | Loss: 0.00002489
Iteration 187/1000 | Loss: 0.00002488
Iteration 188/1000 | Loss: 0.00002488
Iteration 189/1000 | Loss: 0.00002488
Iteration 190/1000 | Loss: 0.00002488
Iteration 191/1000 | Loss: 0.00002487
Iteration 192/1000 | Loss: 0.00002487
Iteration 193/1000 | Loss: 0.00002486
Iteration 194/1000 | Loss: 0.00002486
Iteration 195/1000 | Loss: 0.00002486
Iteration 196/1000 | Loss: 0.00002485
Iteration 197/1000 | Loss: 0.00002485
Iteration 198/1000 | Loss: 0.00002485
Iteration 199/1000 | Loss: 0.00002485
Iteration 200/1000 | Loss: 0.00002485
Iteration 201/1000 | Loss: 0.00002484
Iteration 202/1000 | Loss: 0.00002484
Iteration 203/1000 | Loss: 0.00002484
Iteration 204/1000 | Loss: 0.00002483
Iteration 205/1000 | Loss: 0.00002483
Iteration 206/1000 | Loss: 0.00002483
Iteration 207/1000 | Loss: 0.00002482
Iteration 208/1000 | Loss: 0.00002482
Iteration 209/1000 | Loss: 0.00002482
Iteration 210/1000 | Loss: 0.00002482
Iteration 211/1000 | Loss: 0.00002482
Iteration 212/1000 | Loss: 0.00002482
Iteration 213/1000 | Loss: 0.00002482
Iteration 214/1000 | Loss: 0.00002482
Iteration 215/1000 | Loss: 0.00002482
Iteration 216/1000 | Loss: 0.00002482
Iteration 217/1000 | Loss: 0.00002481
Iteration 218/1000 | Loss: 0.00002481
Iteration 219/1000 | Loss: 0.00002481
Iteration 220/1000 | Loss: 0.00002481
Iteration 221/1000 | Loss: 0.00002481
Iteration 222/1000 | Loss: 0.00002481
Iteration 223/1000 | Loss: 0.00002481
Iteration 224/1000 | Loss: 0.00002480
Iteration 225/1000 | Loss: 0.00002480
Iteration 226/1000 | Loss: 0.00002480
Iteration 227/1000 | Loss: 0.00002480
Iteration 228/1000 | Loss: 0.00002480
Iteration 229/1000 | Loss: 0.00002480
Iteration 230/1000 | Loss: 0.00002480
Iteration 231/1000 | Loss: 0.00002480
Iteration 232/1000 | Loss: 0.00002480
Iteration 233/1000 | Loss: 0.00002480
Iteration 234/1000 | Loss: 0.00002480
Iteration 235/1000 | Loss: 0.00002480
Iteration 236/1000 | Loss: 0.00002480
Iteration 237/1000 | Loss: 0.00002480
Iteration 238/1000 | Loss: 0.00002480
Iteration 239/1000 | Loss: 0.00002480
Iteration 240/1000 | Loss: 0.00002479
Iteration 241/1000 | Loss: 0.00002479
Iteration 242/1000 | Loss: 0.00002479
Iteration 243/1000 | Loss: 0.00002479
Iteration 244/1000 | Loss: 0.00002479
Iteration 245/1000 | Loss: 0.00002479
Iteration 246/1000 | Loss: 0.00002479
Iteration 247/1000 | Loss: 0.00002479
Iteration 248/1000 | Loss: 0.00002479
Iteration 249/1000 | Loss: 0.00002479
Iteration 250/1000 | Loss: 0.00002478
Iteration 251/1000 | Loss: 0.00002478
Iteration 252/1000 | Loss: 0.00002478
Iteration 253/1000 | Loss: 0.00002478
Iteration 254/1000 | Loss: 0.00002478
Iteration 255/1000 | Loss: 0.00002477
Iteration 256/1000 | Loss: 0.00002477
Iteration 257/1000 | Loss: 0.00002477
Iteration 258/1000 | Loss: 0.00002477
Iteration 259/1000 | Loss: 0.00002477
Iteration 260/1000 | Loss: 0.00002477
Iteration 261/1000 | Loss: 0.00002477
Iteration 262/1000 | Loss: 0.00002476
Iteration 263/1000 | Loss: 0.00002476
Iteration 264/1000 | Loss: 0.00002476
Iteration 265/1000 | Loss: 0.00002476
Iteration 266/1000 | Loss: 0.00002476
Iteration 267/1000 | Loss: 0.00002476
Iteration 268/1000 | Loss: 0.00002476
Iteration 269/1000 | Loss: 0.00002476
Iteration 270/1000 | Loss: 0.00002476
Iteration 271/1000 | Loss: 0.00002475
Iteration 272/1000 | Loss: 0.00002475
Iteration 273/1000 | Loss: 0.00002474
Iteration 274/1000 | Loss: 0.00002474
Iteration 275/1000 | Loss: 0.00002474
Iteration 276/1000 | Loss: 0.00002474
Iteration 277/1000 | Loss: 0.00002474
Iteration 278/1000 | Loss: 0.00002474
Iteration 279/1000 | Loss: 0.00002474
Iteration 280/1000 | Loss: 0.00002474
Iteration 281/1000 | Loss: 0.00002473
Iteration 282/1000 | Loss: 0.00002473
Iteration 283/1000 | Loss: 0.00002473
Iteration 284/1000 | Loss: 0.00002473
Iteration 285/1000 | Loss: 0.00002473
Iteration 286/1000 | Loss: 0.00002473
Iteration 287/1000 | Loss: 0.00002472
Iteration 288/1000 | Loss: 0.00002472
Iteration 289/1000 | Loss: 0.00002472
Iteration 290/1000 | Loss: 0.00002472
Iteration 291/1000 | Loss: 0.00002472
Iteration 292/1000 | Loss: 0.00002472
Iteration 293/1000 | Loss: 0.00002472
Iteration 294/1000 | Loss: 0.00002472
Iteration 295/1000 | Loss: 0.00002472
Iteration 296/1000 | Loss: 0.00002472
Iteration 297/1000 | Loss: 0.00002472
Iteration 298/1000 | Loss: 0.00002472
Iteration 299/1000 | Loss: 0.00002472
Iteration 300/1000 | Loss: 0.00002471
Iteration 301/1000 | Loss: 0.00002471
Iteration 302/1000 | Loss: 0.00002471
Iteration 303/1000 | Loss: 0.00002471
Iteration 304/1000 | Loss: 0.00002471
Iteration 305/1000 | Loss: 0.00002471
Iteration 306/1000 | Loss: 0.00002471
Iteration 307/1000 | Loss: 0.00002471
Iteration 308/1000 | Loss: 0.00002471
Iteration 309/1000 | Loss: 0.00002471
Iteration 310/1000 | Loss: 0.00002471
Iteration 311/1000 | Loss: 0.00002471
Iteration 312/1000 | Loss: 0.00002471
Iteration 313/1000 | Loss: 0.00002470
Iteration 314/1000 | Loss: 0.00002470
Iteration 315/1000 | Loss: 0.00002470
Iteration 316/1000 | Loss: 0.00002470
Iteration 317/1000 | Loss: 0.00002470
Iteration 318/1000 | Loss: 0.00002470
Iteration 319/1000 | Loss: 0.00002470
Iteration 320/1000 | Loss: 0.00002469
Iteration 321/1000 | Loss: 0.00002469
Iteration 322/1000 | Loss: 0.00002469
Iteration 323/1000 | Loss: 0.00002469
Iteration 324/1000 | Loss: 0.00002469
Iteration 325/1000 | Loss: 0.00002469
Iteration 326/1000 | Loss: 0.00002469
Iteration 327/1000 | Loss: 0.00002469
Iteration 328/1000 | Loss: 0.00002469
Iteration 329/1000 | Loss: 0.00002468
Iteration 330/1000 | Loss: 0.00002468
Iteration 331/1000 | Loss: 0.00002468
Iteration 332/1000 | Loss: 0.00002468
Iteration 333/1000 | Loss: 0.00002468
Iteration 334/1000 | Loss: 0.00002468
Iteration 335/1000 | Loss: 0.00002468
Iteration 336/1000 | Loss: 0.00002468
Iteration 337/1000 | Loss: 0.00002468
Iteration 338/1000 | Loss: 0.00002468
Iteration 339/1000 | Loss: 0.00002468
Iteration 340/1000 | Loss: 0.00002468
Iteration 341/1000 | Loss: 0.00002467
Iteration 342/1000 | Loss: 0.00002467
Iteration 343/1000 | Loss: 0.00002467
Iteration 344/1000 | Loss: 0.00002467
Iteration 345/1000 | Loss: 0.00002467
Iteration 346/1000 | Loss: 0.00002467
Iteration 347/1000 | Loss: 0.00002467
Iteration 348/1000 | Loss: 0.00002467
Iteration 349/1000 | Loss: 0.00002467
Iteration 350/1000 | Loss: 0.00002467
Iteration 351/1000 | Loss: 0.00002467
Iteration 352/1000 | Loss: 0.00002467
Iteration 353/1000 | Loss: 0.00002466
Iteration 354/1000 | Loss: 0.00002466
Iteration 355/1000 | Loss: 0.00002466
Iteration 356/1000 | Loss: 0.00002466
Iteration 357/1000 | Loss: 0.00002466
Iteration 358/1000 | Loss: 0.00002466
Iteration 359/1000 | Loss: 0.00002466
Iteration 360/1000 | Loss: 0.00002466
Iteration 361/1000 | Loss: 0.00002466
Iteration 362/1000 | Loss: 0.00002466
Iteration 363/1000 | Loss: 0.00002466
Iteration 364/1000 | Loss: 0.00002466
Iteration 365/1000 | Loss: 0.00002466
Iteration 366/1000 | Loss: 0.00002466
Iteration 367/1000 | Loss: 0.00002466
Iteration 368/1000 | Loss: 0.00002466
Iteration 369/1000 | Loss: 0.00002466
Iteration 370/1000 | Loss: 0.00002466
Iteration 371/1000 | Loss: 0.00002466
Iteration 372/1000 | Loss: 0.00002466
Iteration 373/1000 | Loss: 0.00002466
Iteration 374/1000 | Loss: 0.00002466
Iteration 375/1000 | Loss: 0.00002466
Iteration 376/1000 | Loss: 0.00002466
Iteration 377/1000 | Loss: 0.00002466
Iteration 378/1000 | Loss: 0.00002466
Iteration 379/1000 | Loss: 0.00002466
Iteration 380/1000 | Loss: 0.00002466
Iteration 381/1000 | Loss: 0.00002466
Iteration 382/1000 | Loss: 0.00002466
Iteration 383/1000 | Loss: 0.00002466
Iteration 384/1000 | Loss: 0.00002466
Iteration 385/1000 | Loss: 0.00002466
Iteration 386/1000 | Loss: 0.00002466
Iteration 387/1000 | Loss: 0.00002466
Iteration 388/1000 | Loss: 0.00002466
Iteration 389/1000 | Loss: 0.00002466
Iteration 390/1000 | Loss: 0.00002466
Iteration 391/1000 | Loss: 0.00002466
Iteration 392/1000 | Loss: 0.00002466
Iteration 393/1000 | Loss: 0.00002466
Iteration 394/1000 | Loss: 0.00002466
Iteration 395/1000 | Loss: 0.00002466
Iteration 396/1000 | Loss: 0.00002466
Iteration 397/1000 | Loss: 0.00002466
Iteration 398/1000 | Loss: 0.00002466
Iteration 399/1000 | Loss: 0.00002466
Iteration 400/1000 | Loss: 0.00002466
Iteration 401/1000 | Loss: 0.00002466
Iteration 402/1000 | Loss: 0.00002466
Iteration 403/1000 | Loss: 0.00002466
Iteration 404/1000 | Loss: 0.00002466
Iteration 405/1000 | Loss: 0.00002466
Iteration 406/1000 | Loss: 0.00002466
Iteration 407/1000 | Loss: 0.00002466
Iteration 408/1000 | Loss: 0.00002466
Iteration 409/1000 | Loss: 0.00002466
Iteration 410/1000 | Loss: 0.00002466
Iteration 411/1000 | Loss: 0.00002466
Iteration 412/1000 | Loss: 0.00002466
Iteration 413/1000 | Loss: 0.00002466
Iteration 414/1000 | Loss: 0.00002466
Iteration 415/1000 | Loss: 0.00002466
Iteration 416/1000 | Loss: 0.00002466
Iteration 417/1000 | Loss: 0.00002466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 417. Stopping optimization.
Last 5 losses: [2.4659428163431585e-05, 2.4659428163431585e-05, 2.4659428163431585e-05, 2.4659428163431585e-05, 2.4659428163431585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4659428163431585e-05

Optimization complete. Final v2v error: 4.289185523986816 mm

Highest mean error: 5.260364532470703 mm for frame 142

Lowest mean error: 3.6672885417938232 mm for frame 0

Saving results

Total time: 284.9220974445343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861000
Iteration 2/25 | Loss: 0.00161579
Iteration 3/25 | Loss: 0.00148258
Iteration 4/25 | Loss: 0.00146563
Iteration 5/25 | Loss: 0.00146282
Iteration 6/25 | Loss: 0.00146245
Iteration 7/25 | Loss: 0.00146245
Iteration 8/25 | Loss: 0.00146245
Iteration 9/25 | Loss: 0.00146245
Iteration 10/25 | Loss: 0.00146245
Iteration 11/25 | Loss: 0.00146245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014624493196606636, 0.0014624493196606636, 0.0014624493196606636, 0.0014624493196606636, 0.0014624493196606636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014624493196606636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11855578
Iteration 2/25 | Loss: 0.00345962
Iteration 3/25 | Loss: 0.00345959
Iteration 4/25 | Loss: 0.00345959
Iteration 5/25 | Loss: 0.00345959
Iteration 6/25 | Loss: 0.00345959
Iteration 7/25 | Loss: 0.00345959
Iteration 8/25 | Loss: 0.00345959
Iteration 9/25 | Loss: 0.00345959
Iteration 10/25 | Loss: 0.00345958
Iteration 11/25 | Loss: 0.00345958
Iteration 12/25 | Loss: 0.00345958
Iteration 13/25 | Loss: 0.00345958
Iteration 14/25 | Loss: 0.00345958
Iteration 15/25 | Loss: 0.00345958
Iteration 16/25 | Loss: 0.00345958
Iteration 17/25 | Loss: 0.00345958
Iteration 18/25 | Loss: 0.00345958
Iteration 19/25 | Loss: 0.00345958
Iteration 20/25 | Loss: 0.00345958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0034595842007547617, 0.0034595842007547617, 0.0034595842007547617, 0.0034595842007547617, 0.0034595842007547617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034595842007547617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00345958
Iteration 2/1000 | Loss: 0.00005133
Iteration 3/1000 | Loss: 0.00003491
Iteration 4/1000 | Loss: 0.00002923
Iteration 5/1000 | Loss: 0.00002627
Iteration 6/1000 | Loss: 0.00002436
Iteration 7/1000 | Loss: 0.00002319
Iteration 8/1000 | Loss: 0.00002231
Iteration 9/1000 | Loss: 0.00002145
Iteration 10/1000 | Loss: 0.00002086
Iteration 11/1000 | Loss: 0.00002027
Iteration 12/1000 | Loss: 0.00001987
Iteration 13/1000 | Loss: 0.00001974
Iteration 14/1000 | Loss: 0.00001972
Iteration 15/1000 | Loss: 0.00001957
Iteration 16/1000 | Loss: 0.00001939
Iteration 17/1000 | Loss: 0.00001939
Iteration 18/1000 | Loss: 0.00001934
Iteration 19/1000 | Loss: 0.00001930
Iteration 20/1000 | Loss: 0.00001929
Iteration 21/1000 | Loss: 0.00001929
Iteration 22/1000 | Loss: 0.00001928
Iteration 23/1000 | Loss: 0.00001927
Iteration 24/1000 | Loss: 0.00001927
Iteration 25/1000 | Loss: 0.00001924
Iteration 26/1000 | Loss: 0.00001924
Iteration 27/1000 | Loss: 0.00001923
Iteration 28/1000 | Loss: 0.00001923
Iteration 29/1000 | Loss: 0.00001922
Iteration 30/1000 | Loss: 0.00001920
Iteration 31/1000 | Loss: 0.00001919
Iteration 32/1000 | Loss: 0.00001919
Iteration 33/1000 | Loss: 0.00001918
Iteration 34/1000 | Loss: 0.00001918
Iteration 35/1000 | Loss: 0.00001917
Iteration 36/1000 | Loss: 0.00001917
Iteration 37/1000 | Loss: 0.00001917
Iteration 38/1000 | Loss: 0.00001916
Iteration 39/1000 | Loss: 0.00001915
Iteration 40/1000 | Loss: 0.00001915
Iteration 41/1000 | Loss: 0.00001915
Iteration 42/1000 | Loss: 0.00001915
Iteration 43/1000 | Loss: 0.00001915
Iteration 44/1000 | Loss: 0.00001914
Iteration 45/1000 | Loss: 0.00001914
Iteration 46/1000 | Loss: 0.00001914
Iteration 47/1000 | Loss: 0.00001914
Iteration 48/1000 | Loss: 0.00001914
Iteration 49/1000 | Loss: 0.00001914
Iteration 50/1000 | Loss: 0.00001914
Iteration 51/1000 | Loss: 0.00001914
Iteration 52/1000 | Loss: 0.00001914
Iteration 53/1000 | Loss: 0.00001914
Iteration 54/1000 | Loss: 0.00001914
Iteration 55/1000 | Loss: 0.00001913
Iteration 56/1000 | Loss: 0.00001913
Iteration 57/1000 | Loss: 0.00001912
Iteration 58/1000 | Loss: 0.00001912
Iteration 59/1000 | Loss: 0.00001911
Iteration 60/1000 | Loss: 0.00001911
Iteration 61/1000 | Loss: 0.00001911
Iteration 62/1000 | Loss: 0.00001911
Iteration 63/1000 | Loss: 0.00001911
Iteration 64/1000 | Loss: 0.00001911
Iteration 65/1000 | Loss: 0.00001911
Iteration 66/1000 | Loss: 0.00001911
Iteration 67/1000 | Loss: 0.00001911
Iteration 68/1000 | Loss: 0.00001911
Iteration 69/1000 | Loss: 0.00001911
Iteration 70/1000 | Loss: 0.00001911
Iteration 71/1000 | Loss: 0.00001910
Iteration 72/1000 | Loss: 0.00001910
Iteration 73/1000 | Loss: 0.00001910
Iteration 74/1000 | Loss: 0.00001910
Iteration 75/1000 | Loss: 0.00001910
Iteration 76/1000 | Loss: 0.00001910
Iteration 77/1000 | Loss: 0.00001910
Iteration 78/1000 | Loss: 0.00001910
Iteration 79/1000 | Loss: 0.00001910
Iteration 80/1000 | Loss: 0.00001910
Iteration 81/1000 | Loss: 0.00001910
Iteration 82/1000 | Loss: 0.00001909
Iteration 83/1000 | Loss: 0.00001909
Iteration 84/1000 | Loss: 0.00001909
Iteration 85/1000 | Loss: 0.00001909
Iteration 86/1000 | Loss: 0.00001909
Iteration 87/1000 | Loss: 0.00001909
Iteration 88/1000 | Loss: 0.00001909
Iteration 89/1000 | Loss: 0.00001909
Iteration 90/1000 | Loss: 0.00001909
Iteration 91/1000 | Loss: 0.00001909
Iteration 92/1000 | Loss: 0.00001909
Iteration 93/1000 | Loss: 0.00001909
Iteration 94/1000 | Loss: 0.00001908
Iteration 95/1000 | Loss: 0.00001908
Iteration 96/1000 | Loss: 0.00001908
Iteration 97/1000 | Loss: 0.00001908
Iteration 98/1000 | Loss: 0.00001907
Iteration 99/1000 | Loss: 0.00001907
Iteration 100/1000 | Loss: 0.00001907
Iteration 101/1000 | Loss: 0.00001907
Iteration 102/1000 | Loss: 0.00001907
Iteration 103/1000 | Loss: 0.00001907
Iteration 104/1000 | Loss: 0.00001907
Iteration 105/1000 | Loss: 0.00001907
Iteration 106/1000 | Loss: 0.00001907
Iteration 107/1000 | Loss: 0.00001906
Iteration 108/1000 | Loss: 0.00001906
Iteration 109/1000 | Loss: 0.00001906
Iteration 110/1000 | Loss: 0.00001906
Iteration 111/1000 | Loss: 0.00001906
Iteration 112/1000 | Loss: 0.00001905
Iteration 113/1000 | Loss: 0.00001905
Iteration 114/1000 | Loss: 0.00001905
Iteration 115/1000 | Loss: 0.00001905
Iteration 116/1000 | Loss: 0.00001904
Iteration 117/1000 | Loss: 0.00001904
Iteration 118/1000 | Loss: 0.00001904
Iteration 119/1000 | Loss: 0.00001904
Iteration 120/1000 | Loss: 0.00001903
Iteration 121/1000 | Loss: 0.00001903
Iteration 122/1000 | Loss: 0.00001903
Iteration 123/1000 | Loss: 0.00001903
Iteration 124/1000 | Loss: 0.00001903
Iteration 125/1000 | Loss: 0.00001903
Iteration 126/1000 | Loss: 0.00001902
Iteration 127/1000 | Loss: 0.00001902
Iteration 128/1000 | Loss: 0.00001902
Iteration 129/1000 | Loss: 0.00001902
Iteration 130/1000 | Loss: 0.00001902
Iteration 131/1000 | Loss: 0.00001901
Iteration 132/1000 | Loss: 0.00001901
Iteration 133/1000 | Loss: 0.00001901
Iteration 134/1000 | Loss: 0.00001901
Iteration 135/1000 | Loss: 0.00001901
Iteration 136/1000 | Loss: 0.00001901
Iteration 137/1000 | Loss: 0.00001901
Iteration 138/1000 | Loss: 0.00001901
Iteration 139/1000 | Loss: 0.00001901
Iteration 140/1000 | Loss: 0.00001901
Iteration 141/1000 | Loss: 0.00001901
Iteration 142/1000 | Loss: 0.00001901
Iteration 143/1000 | Loss: 0.00001901
Iteration 144/1000 | Loss: 0.00001901
Iteration 145/1000 | Loss: 0.00001900
Iteration 146/1000 | Loss: 0.00001900
Iteration 147/1000 | Loss: 0.00001900
Iteration 148/1000 | Loss: 0.00001900
Iteration 149/1000 | Loss: 0.00001900
Iteration 150/1000 | Loss: 0.00001900
Iteration 151/1000 | Loss: 0.00001900
Iteration 152/1000 | Loss: 0.00001899
Iteration 153/1000 | Loss: 0.00001899
Iteration 154/1000 | Loss: 0.00001899
Iteration 155/1000 | Loss: 0.00001899
Iteration 156/1000 | Loss: 0.00001899
Iteration 157/1000 | Loss: 0.00001899
Iteration 158/1000 | Loss: 0.00001899
Iteration 159/1000 | Loss: 0.00001899
Iteration 160/1000 | Loss: 0.00001899
Iteration 161/1000 | Loss: 0.00001899
Iteration 162/1000 | Loss: 0.00001898
Iteration 163/1000 | Loss: 0.00001898
Iteration 164/1000 | Loss: 0.00001898
Iteration 165/1000 | Loss: 0.00001898
Iteration 166/1000 | Loss: 0.00001898
Iteration 167/1000 | Loss: 0.00001898
Iteration 168/1000 | Loss: 0.00001898
Iteration 169/1000 | Loss: 0.00001897
Iteration 170/1000 | Loss: 0.00001897
Iteration 171/1000 | Loss: 0.00001897
Iteration 172/1000 | Loss: 0.00001897
Iteration 173/1000 | Loss: 0.00001896
Iteration 174/1000 | Loss: 0.00001896
Iteration 175/1000 | Loss: 0.00001896
Iteration 176/1000 | Loss: 0.00001896
Iteration 177/1000 | Loss: 0.00001896
Iteration 178/1000 | Loss: 0.00001896
Iteration 179/1000 | Loss: 0.00001896
Iteration 180/1000 | Loss: 0.00001896
Iteration 181/1000 | Loss: 0.00001896
Iteration 182/1000 | Loss: 0.00001896
Iteration 183/1000 | Loss: 0.00001896
Iteration 184/1000 | Loss: 0.00001896
Iteration 185/1000 | Loss: 0.00001896
Iteration 186/1000 | Loss: 0.00001896
Iteration 187/1000 | Loss: 0.00001896
Iteration 188/1000 | Loss: 0.00001896
Iteration 189/1000 | Loss: 0.00001896
Iteration 190/1000 | Loss: 0.00001896
Iteration 191/1000 | Loss: 0.00001896
Iteration 192/1000 | Loss: 0.00001896
Iteration 193/1000 | Loss: 0.00001896
Iteration 194/1000 | Loss: 0.00001896
Iteration 195/1000 | Loss: 0.00001896
Iteration 196/1000 | Loss: 0.00001896
Iteration 197/1000 | Loss: 0.00001896
Iteration 198/1000 | Loss: 0.00001896
Iteration 199/1000 | Loss: 0.00001896
Iteration 200/1000 | Loss: 0.00001896
Iteration 201/1000 | Loss: 0.00001896
Iteration 202/1000 | Loss: 0.00001896
Iteration 203/1000 | Loss: 0.00001896
Iteration 204/1000 | Loss: 0.00001896
Iteration 205/1000 | Loss: 0.00001896
Iteration 206/1000 | Loss: 0.00001896
Iteration 207/1000 | Loss: 0.00001896
Iteration 208/1000 | Loss: 0.00001896
Iteration 209/1000 | Loss: 0.00001896
Iteration 210/1000 | Loss: 0.00001896
Iteration 211/1000 | Loss: 0.00001896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.8959302906296216e-05, 1.8959302906296216e-05, 1.8959302906296216e-05, 1.8959302906296216e-05, 1.8959302906296216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8959302906296216e-05

Optimization complete. Final v2v error: 3.686460256576538 mm

Highest mean error: 4.421462535858154 mm for frame 89

Lowest mean error: 3.1793177127838135 mm for frame 9

Saving results

Total time: 72.15260004997253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101125
Iteration 2/25 | Loss: 0.01101125
Iteration 3/25 | Loss: 0.01101125
Iteration 4/25 | Loss: 0.01101124
Iteration 5/25 | Loss: 0.00181768
Iteration 6/25 | Loss: 0.00156561
Iteration 7/25 | Loss: 0.00142764
Iteration 8/25 | Loss: 0.00145311
Iteration 9/25 | Loss: 0.00142455
Iteration 10/25 | Loss: 0.00134656
Iteration 11/25 | Loss: 0.00132392
Iteration 12/25 | Loss: 0.00131584
Iteration 13/25 | Loss: 0.00129745
Iteration 14/25 | Loss: 0.00129548
Iteration 15/25 | Loss: 0.00129682
Iteration 16/25 | Loss: 0.00129551
Iteration 17/25 | Loss: 0.00129427
Iteration 18/25 | Loss: 0.00129427
Iteration 19/25 | Loss: 0.00129426
Iteration 20/25 | Loss: 0.00129426
Iteration 21/25 | Loss: 0.00129426
Iteration 22/25 | Loss: 0.00129426
Iteration 23/25 | Loss: 0.00129426
Iteration 24/25 | Loss: 0.00129426
Iteration 25/25 | Loss: 0.00129426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13957179
Iteration 2/25 | Loss: 0.00292408
Iteration 3/25 | Loss: 0.00288402
Iteration 4/25 | Loss: 0.00288402
Iteration 5/25 | Loss: 0.00288402
Iteration 6/25 | Loss: 0.00288402
Iteration 7/25 | Loss: 0.00288402
Iteration 8/25 | Loss: 0.00288402
Iteration 9/25 | Loss: 0.00288402
Iteration 10/25 | Loss: 0.00288402
Iteration 11/25 | Loss: 0.00288402
Iteration 12/25 | Loss: 0.00288402
Iteration 13/25 | Loss: 0.00288402
Iteration 14/25 | Loss: 0.00288402
Iteration 15/25 | Loss: 0.00288402
Iteration 16/25 | Loss: 0.00288402
Iteration 17/25 | Loss: 0.00288402
Iteration 18/25 | Loss: 0.00288402
Iteration 19/25 | Loss: 0.00288402
Iteration 20/25 | Loss: 0.00288402
Iteration 21/25 | Loss: 0.00288402
Iteration 22/25 | Loss: 0.00288402
Iteration 23/25 | Loss: 0.00288402
Iteration 24/25 | Loss: 0.00288402
Iteration 25/25 | Loss: 0.00288402

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288402
Iteration 2/1000 | Loss: 0.00010402
Iteration 3/1000 | Loss: 0.00006008
Iteration 4/1000 | Loss: 0.00005429
Iteration 5/1000 | Loss: 0.00002509
Iteration 6/1000 | Loss: 0.00002123
Iteration 7/1000 | Loss: 0.00010923
Iteration 8/1000 | Loss: 0.00001845
Iteration 9/1000 | Loss: 0.00015338
Iteration 10/1000 | Loss: 0.00151301
Iteration 11/1000 | Loss: 0.00270478
Iteration 12/1000 | Loss: 0.00184114
Iteration 13/1000 | Loss: 0.00137180
Iteration 14/1000 | Loss: 0.00096480
Iteration 15/1000 | Loss: 0.00145089
Iteration 16/1000 | Loss: 0.00228397
Iteration 17/1000 | Loss: 0.00050990
Iteration 18/1000 | Loss: 0.00004543
Iteration 19/1000 | Loss: 0.00050159
Iteration 20/1000 | Loss: 0.00012874
Iteration 21/1000 | Loss: 0.00007630
Iteration 22/1000 | Loss: 0.00006730
Iteration 23/1000 | Loss: 0.00002323
Iteration 24/1000 | Loss: 0.00003209
Iteration 25/1000 | Loss: 0.00002614
Iteration 26/1000 | Loss: 0.00002796
Iteration 27/1000 | Loss: 0.00001739
Iteration 28/1000 | Loss: 0.00011055
Iteration 29/1000 | Loss: 0.00023499
Iteration 30/1000 | Loss: 0.00001889
Iteration 31/1000 | Loss: 0.00002413
Iteration 32/1000 | Loss: 0.00002221
Iteration 33/1000 | Loss: 0.00001689
Iteration 34/1000 | Loss: 0.00001689
Iteration 35/1000 | Loss: 0.00001688
Iteration 36/1000 | Loss: 0.00001688
Iteration 37/1000 | Loss: 0.00001688
Iteration 38/1000 | Loss: 0.00001688
Iteration 39/1000 | Loss: 0.00001688
Iteration 40/1000 | Loss: 0.00001688
Iteration 41/1000 | Loss: 0.00001688
Iteration 42/1000 | Loss: 0.00001688
Iteration 43/1000 | Loss: 0.00001688
Iteration 44/1000 | Loss: 0.00001688
Iteration 45/1000 | Loss: 0.00001687
Iteration 46/1000 | Loss: 0.00001687
Iteration 47/1000 | Loss: 0.00002025
Iteration 48/1000 | Loss: 0.00001685
Iteration 49/1000 | Loss: 0.00001682
Iteration 50/1000 | Loss: 0.00001682
Iteration 51/1000 | Loss: 0.00002305
Iteration 52/1000 | Loss: 0.00001679
Iteration 53/1000 | Loss: 0.00001679
Iteration 54/1000 | Loss: 0.00001679
Iteration 55/1000 | Loss: 0.00001678
Iteration 56/1000 | Loss: 0.00001677
Iteration 57/1000 | Loss: 0.00001676
Iteration 58/1000 | Loss: 0.00001676
Iteration 59/1000 | Loss: 0.00001675
Iteration 60/1000 | Loss: 0.00002049
Iteration 61/1000 | Loss: 0.00001673
Iteration 62/1000 | Loss: 0.00001673
Iteration 63/1000 | Loss: 0.00001673
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001673
Iteration 66/1000 | Loss: 0.00001673
Iteration 67/1000 | Loss: 0.00001673
Iteration 68/1000 | Loss: 0.00001672
Iteration 69/1000 | Loss: 0.00001672
Iteration 70/1000 | Loss: 0.00001672
Iteration 71/1000 | Loss: 0.00001672
Iteration 72/1000 | Loss: 0.00001672
Iteration 73/1000 | Loss: 0.00001672
Iteration 74/1000 | Loss: 0.00002175
Iteration 75/1000 | Loss: 0.00001673
Iteration 76/1000 | Loss: 0.00001673
Iteration 77/1000 | Loss: 0.00001673
Iteration 78/1000 | Loss: 0.00001673
Iteration 79/1000 | Loss: 0.00001673
Iteration 80/1000 | Loss: 0.00001672
Iteration 81/1000 | Loss: 0.00001672
Iteration 82/1000 | Loss: 0.00001672
Iteration 83/1000 | Loss: 0.00001672
Iteration 84/1000 | Loss: 0.00001672
Iteration 85/1000 | Loss: 0.00002656
Iteration 86/1000 | Loss: 0.00001672
Iteration 87/1000 | Loss: 0.00001670
Iteration 88/1000 | Loss: 0.00001670
Iteration 89/1000 | Loss: 0.00001670
Iteration 90/1000 | Loss: 0.00001670
Iteration 91/1000 | Loss: 0.00001670
Iteration 92/1000 | Loss: 0.00001670
Iteration 93/1000 | Loss: 0.00001670
Iteration 94/1000 | Loss: 0.00001670
Iteration 95/1000 | Loss: 0.00001669
Iteration 96/1000 | Loss: 0.00001668
Iteration 97/1000 | Loss: 0.00001668
Iteration 98/1000 | Loss: 0.00001667
Iteration 99/1000 | Loss: 0.00001667
Iteration 100/1000 | Loss: 0.00001667
Iteration 101/1000 | Loss: 0.00001666
Iteration 102/1000 | Loss: 0.00001666
Iteration 103/1000 | Loss: 0.00001665
Iteration 104/1000 | Loss: 0.00001665
Iteration 105/1000 | Loss: 0.00001665
Iteration 106/1000 | Loss: 0.00001665
Iteration 107/1000 | Loss: 0.00001665
Iteration 108/1000 | Loss: 0.00001665
Iteration 109/1000 | Loss: 0.00001665
Iteration 110/1000 | Loss: 0.00001665
Iteration 111/1000 | Loss: 0.00001665
Iteration 112/1000 | Loss: 0.00001665
Iteration 113/1000 | Loss: 0.00001665
Iteration 114/1000 | Loss: 0.00001665
Iteration 115/1000 | Loss: 0.00001664
Iteration 116/1000 | Loss: 0.00001664
Iteration 117/1000 | Loss: 0.00001664
Iteration 118/1000 | Loss: 0.00001664
Iteration 119/1000 | Loss: 0.00001664
Iteration 120/1000 | Loss: 0.00001664
Iteration 121/1000 | Loss: 0.00001664
Iteration 122/1000 | Loss: 0.00001664
Iteration 123/1000 | Loss: 0.00001664
Iteration 124/1000 | Loss: 0.00001664
Iteration 125/1000 | Loss: 0.00001664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.6642849004711024e-05, 1.6642849004711024e-05, 1.6642849004711024e-05, 1.6642849004711024e-05, 1.6642849004711024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6642849004711024e-05

Optimization complete. Final v2v error: 3.4327774047851562 mm

Highest mean error: 9.407857894897461 mm for frame 49

Lowest mean error: 3.1030702590942383 mm for frame 25

Saving results

Total time: 117.6767508983612
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01135019
Iteration 2/25 | Loss: 0.01135019
Iteration 3/25 | Loss: 0.00301488
Iteration 4/25 | Loss: 0.00229221
Iteration 5/25 | Loss: 0.00220833
Iteration 6/25 | Loss: 0.00217815
Iteration 7/25 | Loss: 0.00207085
Iteration 8/25 | Loss: 0.00203070
Iteration 9/25 | Loss: 0.00199781
Iteration 10/25 | Loss: 0.00212093
Iteration 11/25 | Loss: 0.00187642
Iteration 12/25 | Loss: 0.00176163
Iteration 13/25 | Loss: 0.00163794
Iteration 14/25 | Loss: 0.00158584
Iteration 15/25 | Loss: 0.00157256
Iteration 16/25 | Loss: 0.00156960
Iteration 17/25 | Loss: 0.00156840
Iteration 18/25 | Loss: 0.00156815
Iteration 19/25 | Loss: 0.00156812
Iteration 20/25 | Loss: 0.00156812
Iteration 21/25 | Loss: 0.00156812
Iteration 22/25 | Loss: 0.00156812
Iteration 23/25 | Loss: 0.00156812
Iteration 24/25 | Loss: 0.00156812
Iteration 25/25 | Loss: 0.00156812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14020264
Iteration 2/25 | Loss: 0.00395500
Iteration 3/25 | Loss: 0.00395509
Iteration 4/25 | Loss: 0.00393321
Iteration 5/25 | Loss: 0.00393321
Iteration 6/25 | Loss: 0.00393321
Iteration 7/25 | Loss: 0.00393321
Iteration 8/25 | Loss: 0.00393321
Iteration 9/25 | Loss: 0.00393321
Iteration 10/25 | Loss: 0.00393321
Iteration 11/25 | Loss: 0.00393321
Iteration 12/25 | Loss: 0.00393321
Iteration 13/25 | Loss: 0.00393321
Iteration 14/25 | Loss: 0.00393321
Iteration 15/25 | Loss: 0.00393321
Iteration 16/25 | Loss: 0.00393321
Iteration 17/25 | Loss: 0.00393321
Iteration 18/25 | Loss: 0.00393321
Iteration 19/25 | Loss: 0.00393321
Iteration 20/25 | Loss: 0.00393321
Iteration 21/25 | Loss: 0.00393321
Iteration 22/25 | Loss: 0.00393321
Iteration 23/25 | Loss: 0.00393321
Iteration 24/25 | Loss: 0.00393321
Iteration 25/25 | Loss: 0.00393321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00393321
Iteration 2/1000 | Loss: 0.00046822
Iteration 3/1000 | Loss: 0.00046220
Iteration 4/1000 | Loss: 0.00049254
Iteration 5/1000 | Loss: 0.00028453
Iteration 6/1000 | Loss: 0.00053040
Iteration 7/1000 | Loss: 0.00019251
Iteration 8/1000 | Loss: 0.00013831
Iteration 9/1000 | Loss: 0.00014883
Iteration 10/1000 | Loss: 0.00012807
Iteration 11/1000 | Loss: 0.00031882
Iteration 12/1000 | Loss: 0.00012177
Iteration 13/1000 | Loss: 0.00022508
Iteration 14/1000 | Loss: 0.00015349
Iteration 15/1000 | Loss: 0.00038182
Iteration 16/1000 | Loss: 0.00010993
Iteration 17/1000 | Loss: 0.00539922
Iteration 18/1000 | Loss: 0.00354580
Iteration 19/1000 | Loss: 0.00293215
Iteration 20/1000 | Loss: 0.00199551
Iteration 21/1000 | Loss: 0.00074159
Iteration 22/1000 | Loss: 0.00338565
Iteration 23/1000 | Loss: 0.00324685
Iteration 24/1000 | Loss: 0.00489618
Iteration 25/1000 | Loss: 0.00723264
Iteration 26/1000 | Loss: 0.00593540
Iteration 27/1000 | Loss: 0.00313901
Iteration 28/1000 | Loss: 0.00410513
Iteration 29/1000 | Loss: 0.00160298
Iteration 30/1000 | Loss: 0.00294066
Iteration 31/1000 | Loss: 0.00416315
Iteration 32/1000 | Loss: 0.00389880
Iteration 33/1000 | Loss: 0.00259039
Iteration 34/1000 | Loss: 0.00496586
Iteration 35/1000 | Loss: 0.00124109
Iteration 36/1000 | Loss: 0.00241374
Iteration 37/1000 | Loss: 0.00210158
Iteration 38/1000 | Loss: 0.00177306
Iteration 39/1000 | Loss: 0.00145479
Iteration 40/1000 | Loss: 0.00104891
Iteration 41/1000 | Loss: 0.00114405
Iteration 42/1000 | Loss: 0.00102353
Iteration 43/1000 | Loss: 0.00148764
Iteration 44/1000 | Loss: 0.00028347
Iteration 45/1000 | Loss: 0.00015017
Iteration 46/1000 | Loss: 0.00044795
Iteration 47/1000 | Loss: 0.00019157
Iteration 48/1000 | Loss: 0.00037701
Iteration 49/1000 | Loss: 0.00149058
Iteration 50/1000 | Loss: 0.00126130
Iteration 51/1000 | Loss: 0.00241935
Iteration 52/1000 | Loss: 0.00101153
Iteration 53/1000 | Loss: 0.00115251
Iteration 54/1000 | Loss: 0.00031418
Iteration 55/1000 | Loss: 0.00047495
Iteration 56/1000 | Loss: 0.00071142
Iteration 57/1000 | Loss: 0.00013925
Iteration 58/1000 | Loss: 0.00100947
Iteration 59/1000 | Loss: 0.00109329
Iteration 60/1000 | Loss: 0.00087120
Iteration 61/1000 | Loss: 0.00033849
Iteration 62/1000 | Loss: 0.00031133
Iteration 63/1000 | Loss: 0.00011118
Iteration 64/1000 | Loss: 0.00005986
Iteration 65/1000 | Loss: 0.00005451
Iteration 66/1000 | Loss: 0.00008700
Iteration 67/1000 | Loss: 0.00005085
Iteration 68/1000 | Loss: 0.00007661
Iteration 69/1000 | Loss: 0.00006201
Iteration 70/1000 | Loss: 0.00007905
Iteration 71/1000 | Loss: 0.00006773
Iteration 72/1000 | Loss: 0.00010791
Iteration 73/1000 | Loss: 0.00004031
Iteration 74/1000 | Loss: 0.00006664
Iteration 75/1000 | Loss: 0.00003438
Iteration 76/1000 | Loss: 0.00011167
Iteration 77/1000 | Loss: 0.00005830
Iteration 78/1000 | Loss: 0.00008723
Iteration 79/1000 | Loss: 0.00006912
Iteration 80/1000 | Loss: 0.00006515
Iteration 81/1000 | Loss: 0.00003105
Iteration 82/1000 | Loss: 0.00003063
Iteration 83/1000 | Loss: 0.00004883
Iteration 84/1000 | Loss: 0.00004324
Iteration 85/1000 | Loss: 0.00003002
Iteration 86/1000 | Loss: 0.00002964
Iteration 87/1000 | Loss: 0.00002933
Iteration 88/1000 | Loss: 0.00004097
Iteration 89/1000 | Loss: 0.00003388
Iteration 90/1000 | Loss: 0.00002887
Iteration 91/1000 | Loss: 0.00002886
Iteration 92/1000 | Loss: 0.00002886
Iteration 93/1000 | Loss: 0.00002886
Iteration 94/1000 | Loss: 0.00002885
Iteration 95/1000 | Loss: 0.00002885
Iteration 96/1000 | Loss: 0.00002885
Iteration 97/1000 | Loss: 0.00002879
Iteration 98/1000 | Loss: 0.00003171
Iteration 99/1000 | Loss: 0.00002870
Iteration 100/1000 | Loss: 0.00002865
Iteration 101/1000 | Loss: 0.00002865
Iteration 102/1000 | Loss: 0.00002864
Iteration 103/1000 | Loss: 0.00002861
Iteration 104/1000 | Loss: 0.00002860
Iteration 105/1000 | Loss: 0.00002859
Iteration 106/1000 | Loss: 0.00002854
Iteration 107/1000 | Loss: 0.00002854
Iteration 108/1000 | Loss: 0.00002853
Iteration 109/1000 | Loss: 0.00002853
Iteration 110/1000 | Loss: 0.00002853
Iteration 111/1000 | Loss: 0.00002853
Iteration 112/1000 | Loss: 0.00002853
Iteration 113/1000 | Loss: 0.00002853
Iteration 114/1000 | Loss: 0.00002853
Iteration 115/1000 | Loss: 0.00002853
Iteration 116/1000 | Loss: 0.00002853
Iteration 117/1000 | Loss: 0.00002853
Iteration 118/1000 | Loss: 0.00002853
Iteration 119/1000 | Loss: 0.00002853
Iteration 120/1000 | Loss: 0.00002852
Iteration 121/1000 | Loss: 0.00002852
Iteration 122/1000 | Loss: 0.00002852
Iteration 123/1000 | Loss: 0.00002852
Iteration 124/1000 | Loss: 0.00002851
Iteration 125/1000 | Loss: 0.00002851
Iteration 126/1000 | Loss: 0.00002851
Iteration 127/1000 | Loss: 0.00002850
Iteration 128/1000 | Loss: 0.00002850
Iteration 129/1000 | Loss: 0.00002850
Iteration 130/1000 | Loss: 0.00002850
Iteration 131/1000 | Loss: 0.00002850
Iteration 132/1000 | Loss: 0.00002850
Iteration 133/1000 | Loss: 0.00002850
Iteration 134/1000 | Loss: 0.00002850
Iteration 135/1000 | Loss: 0.00002850
Iteration 136/1000 | Loss: 0.00002850
Iteration 137/1000 | Loss: 0.00002849
Iteration 138/1000 | Loss: 0.00002849
Iteration 139/1000 | Loss: 0.00002849
Iteration 140/1000 | Loss: 0.00002849
Iteration 141/1000 | Loss: 0.00002849
Iteration 142/1000 | Loss: 0.00002849
Iteration 143/1000 | Loss: 0.00002848
Iteration 144/1000 | Loss: 0.00002848
Iteration 145/1000 | Loss: 0.00002848
Iteration 146/1000 | Loss: 0.00002848
Iteration 147/1000 | Loss: 0.00002848
Iteration 148/1000 | Loss: 0.00002848
Iteration 149/1000 | Loss: 0.00002848
Iteration 150/1000 | Loss: 0.00002848
Iteration 151/1000 | Loss: 0.00002848
Iteration 152/1000 | Loss: 0.00002848
Iteration 153/1000 | Loss: 0.00002847
Iteration 154/1000 | Loss: 0.00002847
Iteration 155/1000 | Loss: 0.00002847
Iteration 156/1000 | Loss: 0.00002847
Iteration 157/1000 | Loss: 0.00002847
Iteration 158/1000 | Loss: 0.00002847
Iteration 159/1000 | Loss: 0.00002847
Iteration 160/1000 | Loss: 0.00002847
Iteration 161/1000 | Loss: 0.00002847
Iteration 162/1000 | Loss: 0.00002847
Iteration 163/1000 | Loss: 0.00002847
Iteration 164/1000 | Loss: 0.00002847
Iteration 165/1000 | Loss: 0.00002847
Iteration 166/1000 | Loss: 0.00002847
Iteration 167/1000 | Loss: 0.00002847
Iteration 168/1000 | Loss: 0.00002847
Iteration 169/1000 | Loss: 0.00002847
Iteration 170/1000 | Loss: 0.00002846
Iteration 171/1000 | Loss: 0.00002846
Iteration 172/1000 | Loss: 0.00002846
Iteration 173/1000 | Loss: 0.00002846
Iteration 174/1000 | Loss: 0.00002846
Iteration 175/1000 | Loss: 0.00002846
Iteration 176/1000 | Loss: 0.00002846
Iteration 177/1000 | Loss: 0.00002846
Iteration 178/1000 | Loss: 0.00002846
Iteration 179/1000 | Loss: 0.00002846
Iteration 180/1000 | Loss: 0.00002846
Iteration 181/1000 | Loss: 0.00002846
Iteration 182/1000 | Loss: 0.00002846
Iteration 183/1000 | Loss: 0.00002846
Iteration 184/1000 | Loss: 0.00002846
Iteration 185/1000 | Loss: 0.00002846
Iteration 186/1000 | Loss: 0.00002846
Iteration 187/1000 | Loss: 0.00002846
Iteration 188/1000 | Loss: 0.00002846
Iteration 189/1000 | Loss: 0.00002846
Iteration 190/1000 | Loss: 0.00002846
Iteration 191/1000 | Loss: 0.00002846
Iteration 192/1000 | Loss: 0.00002846
Iteration 193/1000 | Loss: 0.00002846
Iteration 194/1000 | Loss: 0.00002846
Iteration 195/1000 | Loss: 0.00002846
Iteration 196/1000 | Loss: 0.00002846
Iteration 197/1000 | Loss: 0.00002846
Iteration 198/1000 | Loss: 0.00002846
Iteration 199/1000 | Loss: 0.00002846
Iteration 200/1000 | Loss: 0.00002846
Iteration 201/1000 | Loss: 0.00002846
Iteration 202/1000 | Loss: 0.00002846
Iteration 203/1000 | Loss: 0.00002846
Iteration 204/1000 | Loss: 0.00002846
Iteration 205/1000 | Loss: 0.00002846
Iteration 206/1000 | Loss: 0.00002846
Iteration 207/1000 | Loss: 0.00002846
Iteration 208/1000 | Loss: 0.00002846
Iteration 209/1000 | Loss: 0.00002846
Iteration 210/1000 | Loss: 0.00002846
Iteration 211/1000 | Loss: 0.00002846
Iteration 212/1000 | Loss: 0.00002846
Iteration 213/1000 | Loss: 0.00002846
Iteration 214/1000 | Loss: 0.00002846
Iteration 215/1000 | Loss: 0.00002846
Iteration 216/1000 | Loss: 0.00002846
Iteration 217/1000 | Loss: 0.00002846
Iteration 218/1000 | Loss: 0.00002846
Iteration 219/1000 | Loss: 0.00002846
Iteration 220/1000 | Loss: 0.00002846
Iteration 221/1000 | Loss: 0.00002846
Iteration 222/1000 | Loss: 0.00002846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [2.8460783141781576e-05, 2.8460783141781576e-05, 2.8460783141781576e-05, 2.8460783141781576e-05, 2.8460783141781576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8460783141781576e-05

Optimization complete. Final v2v error: 3.5126023292541504 mm

Highest mean error: 12.589875221252441 mm for frame 8

Lowest mean error: 3.087287425994873 mm for frame 117

Saving results

Total time: 197.12948346138
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814302
Iteration 2/25 | Loss: 0.00175715
Iteration 3/25 | Loss: 0.00159386
Iteration 4/25 | Loss: 0.00154222
Iteration 5/25 | Loss: 0.00152316
Iteration 6/25 | Loss: 0.00150381
Iteration 7/25 | Loss: 0.00150467
Iteration 8/25 | Loss: 0.00149636
Iteration 9/25 | Loss: 0.00149459
Iteration 10/25 | Loss: 0.00149412
Iteration 11/25 | Loss: 0.00149395
Iteration 12/25 | Loss: 0.00149387
Iteration 13/25 | Loss: 0.00149379
Iteration 14/25 | Loss: 0.00149377
Iteration 15/25 | Loss: 0.00149377
Iteration 16/25 | Loss: 0.00149377
Iteration 17/25 | Loss: 0.00149377
Iteration 18/25 | Loss: 0.00149377
Iteration 19/25 | Loss: 0.00149377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014937736559659243, 0.0014937736559659243, 0.0014937736559659243, 0.0014937736559659243, 0.0014937736559659243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014937736559659243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68070543
Iteration 2/25 | Loss: 0.00467364
Iteration 3/25 | Loss: 0.00463262
Iteration 4/25 | Loss: 0.00463262
Iteration 5/25 | Loss: 0.00463262
Iteration 6/25 | Loss: 0.00463262
Iteration 7/25 | Loss: 0.00463262
Iteration 8/25 | Loss: 0.00463262
Iteration 9/25 | Loss: 0.00463262
Iteration 10/25 | Loss: 0.00463262
Iteration 11/25 | Loss: 0.00463262
Iteration 12/25 | Loss: 0.00463262
Iteration 13/25 | Loss: 0.00463262
Iteration 14/25 | Loss: 0.00463262
Iteration 15/25 | Loss: 0.00463262
Iteration 16/25 | Loss: 0.00463262
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004632618278264999, 0.004632618278264999, 0.004632618278264999, 0.004632618278264999, 0.004632618278264999]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004632618278264999

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00463262
Iteration 2/1000 | Loss: 0.00028939
Iteration 3/1000 | Loss: 0.00109884
Iteration 4/1000 | Loss: 0.00017486
Iteration 5/1000 | Loss: 0.00025831
Iteration 6/1000 | Loss: 0.00006922
Iteration 7/1000 | Loss: 0.00005436
Iteration 8/1000 | Loss: 0.00003974
Iteration 9/1000 | Loss: 0.00008158
Iteration 10/1000 | Loss: 0.00003342
Iteration 11/1000 | Loss: 0.00034250
Iteration 12/1000 | Loss: 0.00004426
Iteration 13/1000 | Loss: 0.00003503
Iteration 14/1000 | Loss: 0.00003191
Iteration 15/1000 | Loss: 0.00002988
Iteration 16/1000 | Loss: 0.00002851
Iteration 17/1000 | Loss: 0.00026177
Iteration 18/1000 | Loss: 0.00023009
Iteration 19/1000 | Loss: 0.00025135
Iteration 20/1000 | Loss: 0.00023590
Iteration 21/1000 | Loss: 0.00004105
Iteration 22/1000 | Loss: 0.00003273
Iteration 23/1000 | Loss: 0.00002922
Iteration 24/1000 | Loss: 0.00002786
Iteration 25/1000 | Loss: 0.00002722
Iteration 26/1000 | Loss: 0.00018873
Iteration 27/1000 | Loss: 0.00003507
Iteration 28/1000 | Loss: 0.00002952
Iteration 29/1000 | Loss: 0.00002791
Iteration 30/1000 | Loss: 0.00042566
Iteration 31/1000 | Loss: 0.00022441
Iteration 32/1000 | Loss: 0.00022237
Iteration 33/1000 | Loss: 0.00009554
Iteration 34/1000 | Loss: 0.00006924
Iteration 35/1000 | Loss: 0.00003562
Iteration 36/1000 | Loss: 0.00003084
Iteration 37/1000 | Loss: 0.00005684
Iteration 38/1000 | Loss: 0.00003657
Iteration 39/1000 | Loss: 0.00002747
Iteration 40/1000 | Loss: 0.00002690
Iteration 41/1000 | Loss: 0.00042491
Iteration 42/1000 | Loss: 0.00006184
Iteration 43/1000 | Loss: 0.00005243
Iteration 44/1000 | Loss: 0.00002687
Iteration 45/1000 | Loss: 0.00002601
Iteration 46/1000 | Loss: 0.00002484
Iteration 47/1000 | Loss: 0.00002401
Iteration 48/1000 | Loss: 0.00002368
Iteration 49/1000 | Loss: 0.00002366
Iteration 50/1000 | Loss: 0.00034906
Iteration 51/1000 | Loss: 0.00002763
Iteration 52/1000 | Loss: 0.00002469
Iteration 53/1000 | Loss: 0.00002364
Iteration 54/1000 | Loss: 0.00002251
Iteration 55/1000 | Loss: 0.00002183
Iteration 56/1000 | Loss: 0.00002125
Iteration 57/1000 | Loss: 0.00002104
Iteration 58/1000 | Loss: 0.00002101
Iteration 59/1000 | Loss: 0.00002100
Iteration 60/1000 | Loss: 0.00002100
Iteration 61/1000 | Loss: 0.00002099
Iteration 62/1000 | Loss: 0.00002099
Iteration 63/1000 | Loss: 0.00002098
Iteration 64/1000 | Loss: 0.00002095
Iteration 65/1000 | Loss: 0.00002095
Iteration 66/1000 | Loss: 0.00002094
Iteration 67/1000 | Loss: 0.00002093
Iteration 68/1000 | Loss: 0.00002088
Iteration 69/1000 | Loss: 0.00002083
Iteration 70/1000 | Loss: 0.00002083
Iteration 71/1000 | Loss: 0.00002083
Iteration 72/1000 | Loss: 0.00002082
Iteration 73/1000 | Loss: 0.00002082
Iteration 74/1000 | Loss: 0.00002082
Iteration 75/1000 | Loss: 0.00002081
Iteration 76/1000 | Loss: 0.00002081
Iteration 77/1000 | Loss: 0.00002080
Iteration 78/1000 | Loss: 0.00002080
Iteration 79/1000 | Loss: 0.00002079
Iteration 80/1000 | Loss: 0.00002078
Iteration 81/1000 | Loss: 0.00002078
Iteration 82/1000 | Loss: 0.00002078
Iteration 83/1000 | Loss: 0.00002074
Iteration 84/1000 | Loss: 0.00002074
Iteration 85/1000 | Loss: 0.00002073
Iteration 86/1000 | Loss: 0.00002072
Iteration 87/1000 | Loss: 0.00002071
Iteration 88/1000 | Loss: 0.00002070
Iteration 89/1000 | Loss: 0.00002069
Iteration 90/1000 | Loss: 0.00002069
Iteration 91/1000 | Loss: 0.00002068
Iteration 92/1000 | Loss: 0.00002068
Iteration 93/1000 | Loss: 0.00002067
Iteration 94/1000 | Loss: 0.00002067
Iteration 95/1000 | Loss: 0.00002067
Iteration 96/1000 | Loss: 0.00002066
Iteration 97/1000 | Loss: 0.00002066
Iteration 98/1000 | Loss: 0.00002066
Iteration 99/1000 | Loss: 0.00002064
Iteration 100/1000 | Loss: 0.00002064
Iteration 101/1000 | Loss: 0.00002063
Iteration 102/1000 | Loss: 0.00002062
Iteration 103/1000 | Loss: 0.00002062
Iteration 104/1000 | Loss: 0.00002062
Iteration 105/1000 | Loss: 0.00002061
Iteration 106/1000 | Loss: 0.00002061
Iteration 107/1000 | Loss: 0.00002061
Iteration 108/1000 | Loss: 0.00002061
Iteration 109/1000 | Loss: 0.00002061
Iteration 110/1000 | Loss: 0.00002061
Iteration 111/1000 | Loss: 0.00002061
Iteration 112/1000 | Loss: 0.00002061
Iteration 113/1000 | Loss: 0.00002061
Iteration 114/1000 | Loss: 0.00002061
Iteration 115/1000 | Loss: 0.00002061
Iteration 116/1000 | Loss: 0.00002061
Iteration 117/1000 | Loss: 0.00002060
Iteration 118/1000 | Loss: 0.00002060
Iteration 119/1000 | Loss: 0.00002059
Iteration 120/1000 | Loss: 0.00002059
Iteration 121/1000 | Loss: 0.00002058
Iteration 122/1000 | Loss: 0.00002058
Iteration 123/1000 | Loss: 0.00002058
Iteration 124/1000 | Loss: 0.00002058
Iteration 125/1000 | Loss: 0.00002057
Iteration 126/1000 | Loss: 0.00002057
Iteration 127/1000 | Loss: 0.00002057
Iteration 128/1000 | Loss: 0.00002056
Iteration 129/1000 | Loss: 0.00002056
Iteration 130/1000 | Loss: 0.00002056
Iteration 131/1000 | Loss: 0.00002056
Iteration 132/1000 | Loss: 0.00002056
Iteration 133/1000 | Loss: 0.00002056
Iteration 134/1000 | Loss: 0.00002056
Iteration 135/1000 | Loss: 0.00002056
Iteration 136/1000 | Loss: 0.00002056
Iteration 137/1000 | Loss: 0.00002056
Iteration 138/1000 | Loss: 0.00002056
Iteration 139/1000 | Loss: 0.00002055
Iteration 140/1000 | Loss: 0.00002055
Iteration 141/1000 | Loss: 0.00002055
Iteration 142/1000 | Loss: 0.00002055
Iteration 143/1000 | Loss: 0.00002055
Iteration 144/1000 | Loss: 0.00002055
Iteration 145/1000 | Loss: 0.00002055
Iteration 146/1000 | Loss: 0.00002055
Iteration 147/1000 | Loss: 0.00002055
Iteration 148/1000 | Loss: 0.00002055
Iteration 149/1000 | Loss: 0.00002054
Iteration 150/1000 | Loss: 0.00002054
Iteration 151/1000 | Loss: 0.00002054
Iteration 152/1000 | Loss: 0.00002054
Iteration 153/1000 | Loss: 0.00002054
Iteration 154/1000 | Loss: 0.00002054
Iteration 155/1000 | Loss: 0.00002054
Iteration 156/1000 | Loss: 0.00002054
Iteration 157/1000 | Loss: 0.00002054
Iteration 158/1000 | Loss: 0.00002054
Iteration 159/1000 | Loss: 0.00002054
Iteration 160/1000 | Loss: 0.00002053
Iteration 161/1000 | Loss: 0.00002053
Iteration 162/1000 | Loss: 0.00002053
Iteration 163/1000 | Loss: 0.00002053
Iteration 164/1000 | Loss: 0.00002053
Iteration 165/1000 | Loss: 0.00002053
Iteration 166/1000 | Loss: 0.00002053
Iteration 167/1000 | Loss: 0.00002053
Iteration 168/1000 | Loss: 0.00002053
Iteration 169/1000 | Loss: 0.00002053
Iteration 170/1000 | Loss: 0.00002053
Iteration 171/1000 | Loss: 0.00002053
Iteration 172/1000 | Loss: 0.00002053
Iteration 173/1000 | Loss: 0.00002052
Iteration 174/1000 | Loss: 0.00002052
Iteration 175/1000 | Loss: 0.00002052
Iteration 176/1000 | Loss: 0.00002052
Iteration 177/1000 | Loss: 0.00002052
Iteration 178/1000 | Loss: 0.00002051
Iteration 179/1000 | Loss: 0.00002051
Iteration 180/1000 | Loss: 0.00002051
Iteration 181/1000 | Loss: 0.00002051
Iteration 182/1000 | Loss: 0.00002051
Iteration 183/1000 | Loss: 0.00002051
Iteration 184/1000 | Loss: 0.00002051
Iteration 185/1000 | Loss: 0.00002051
Iteration 186/1000 | Loss: 0.00002051
Iteration 187/1000 | Loss: 0.00002051
Iteration 188/1000 | Loss: 0.00002051
Iteration 189/1000 | Loss: 0.00002051
Iteration 190/1000 | Loss: 0.00002051
Iteration 191/1000 | Loss: 0.00002051
Iteration 192/1000 | Loss: 0.00002051
Iteration 193/1000 | Loss: 0.00002051
Iteration 194/1000 | Loss: 0.00002051
Iteration 195/1000 | Loss: 0.00002051
Iteration 196/1000 | Loss: 0.00002051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.050586226687301e-05, 2.050586226687301e-05, 2.050586226687301e-05, 2.050586226687301e-05, 2.050586226687301e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.050586226687301e-05

Optimization complete. Final v2v error: 3.9334049224853516 mm

Highest mean error: 4.936007976531982 mm for frame 232

Lowest mean error: 3.612089157104492 mm for frame 214

Saving results

Total time: 136.46212434768677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852670
Iteration 2/25 | Loss: 0.00147417
Iteration 3/25 | Loss: 0.00136547
Iteration 4/25 | Loss: 0.00135627
Iteration 5/25 | Loss: 0.00135521
Iteration 6/25 | Loss: 0.00135521
Iteration 7/25 | Loss: 0.00135521
Iteration 8/25 | Loss: 0.00135521
Iteration 9/25 | Loss: 0.00135521
Iteration 10/25 | Loss: 0.00135521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001355210319161415, 0.001355210319161415, 0.001355210319161415, 0.001355210319161415, 0.001355210319161415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001355210319161415

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16578436
Iteration 2/25 | Loss: 0.00288578
Iteration 3/25 | Loss: 0.00288578
Iteration 4/25 | Loss: 0.00288578
Iteration 5/25 | Loss: 0.00288578
Iteration 6/25 | Loss: 0.00288578
Iteration 7/25 | Loss: 0.00288577
Iteration 8/25 | Loss: 0.00288577
Iteration 9/25 | Loss: 0.00288577
Iteration 10/25 | Loss: 0.00288577
Iteration 11/25 | Loss: 0.00288577
Iteration 12/25 | Loss: 0.00288577
Iteration 13/25 | Loss: 0.00288577
Iteration 14/25 | Loss: 0.00288577
Iteration 15/25 | Loss: 0.00288577
Iteration 16/25 | Loss: 0.00288577
Iteration 17/25 | Loss: 0.00288577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002885773777961731, 0.002885773777961731, 0.002885773777961731, 0.002885773777961731, 0.002885773777961731]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002885773777961731

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288577
Iteration 2/1000 | Loss: 0.00003026
Iteration 3/1000 | Loss: 0.00002105
Iteration 4/1000 | Loss: 0.00001838
Iteration 5/1000 | Loss: 0.00001690
Iteration 6/1000 | Loss: 0.00001636
Iteration 7/1000 | Loss: 0.00001582
Iteration 8/1000 | Loss: 0.00001553
Iteration 9/1000 | Loss: 0.00001529
Iteration 10/1000 | Loss: 0.00001510
Iteration 11/1000 | Loss: 0.00001506
Iteration 12/1000 | Loss: 0.00001505
Iteration 13/1000 | Loss: 0.00001505
Iteration 14/1000 | Loss: 0.00001504
Iteration 15/1000 | Loss: 0.00001504
Iteration 16/1000 | Loss: 0.00001502
Iteration 17/1000 | Loss: 0.00001501
Iteration 18/1000 | Loss: 0.00001501
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001501
Iteration 21/1000 | Loss: 0.00001501
Iteration 22/1000 | Loss: 0.00001500
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001498
Iteration 25/1000 | Loss: 0.00001495
Iteration 26/1000 | Loss: 0.00001495
Iteration 27/1000 | Loss: 0.00001495
Iteration 28/1000 | Loss: 0.00001493
Iteration 29/1000 | Loss: 0.00001493
Iteration 30/1000 | Loss: 0.00001493
Iteration 31/1000 | Loss: 0.00001492
Iteration 32/1000 | Loss: 0.00001492
Iteration 33/1000 | Loss: 0.00001491
Iteration 34/1000 | Loss: 0.00001491
Iteration 35/1000 | Loss: 0.00001490
Iteration 36/1000 | Loss: 0.00001489
Iteration 37/1000 | Loss: 0.00001488
Iteration 38/1000 | Loss: 0.00001488
Iteration 39/1000 | Loss: 0.00001487
Iteration 40/1000 | Loss: 0.00001487
Iteration 41/1000 | Loss: 0.00001487
Iteration 42/1000 | Loss: 0.00001486
Iteration 43/1000 | Loss: 0.00001486
Iteration 44/1000 | Loss: 0.00001485
Iteration 45/1000 | Loss: 0.00001485
Iteration 46/1000 | Loss: 0.00001485
Iteration 47/1000 | Loss: 0.00001484
Iteration 48/1000 | Loss: 0.00001484
Iteration 49/1000 | Loss: 0.00001484
Iteration 50/1000 | Loss: 0.00001484
Iteration 51/1000 | Loss: 0.00001483
Iteration 52/1000 | Loss: 0.00001483
Iteration 53/1000 | Loss: 0.00001483
Iteration 54/1000 | Loss: 0.00001483
Iteration 55/1000 | Loss: 0.00001483
Iteration 56/1000 | Loss: 0.00001482
Iteration 57/1000 | Loss: 0.00001482
Iteration 58/1000 | Loss: 0.00001482
Iteration 59/1000 | Loss: 0.00001482
Iteration 60/1000 | Loss: 0.00001482
Iteration 61/1000 | Loss: 0.00001482
Iteration 62/1000 | Loss: 0.00001482
Iteration 63/1000 | Loss: 0.00001481
Iteration 64/1000 | Loss: 0.00001481
Iteration 65/1000 | Loss: 0.00001481
Iteration 66/1000 | Loss: 0.00001481
Iteration 67/1000 | Loss: 0.00001480
Iteration 68/1000 | Loss: 0.00001480
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001479
Iteration 72/1000 | Loss: 0.00001479
Iteration 73/1000 | Loss: 0.00001479
Iteration 74/1000 | Loss: 0.00001479
Iteration 75/1000 | Loss: 0.00001479
Iteration 76/1000 | Loss: 0.00001478
Iteration 77/1000 | Loss: 0.00001478
Iteration 78/1000 | Loss: 0.00001477
Iteration 79/1000 | Loss: 0.00001477
Iteration 80/1000 | Loss: 0.00001477
Iteration 81/1000 | Loss: 0.00001477
Iteration 82/1000 | Loss: 0.00001476
Iteration 83/1000 | Loss: 0.00001476
Iteration 84/1000 | Loss: 0.00001476
Iteration 85/1000 | Loss: 0.00001476
Iteration 86/1000 | Loss: 0.00001476
Iteration 87/1000 | Loss: 0.00001476
Iteration 88/1000 | Loss: 0.00001476
Iteration 89/1000 | Loss: 0.00001476
Iteration 90/1000 | Loss: 0.00001476
Iteration 91/1000 | Loss: 0.00001476
Iteration 92/1000 | Loss: 0.00001476
Iteration 93/1000 | Loss: 0.00001476
Iteration 94/1000 | Loss: 0.00001476
Iteration 95/1000 | Loss: 0.00001476
Iteration 96/1000 | Loss: 0.00001476
Iteration 97/1000 | Loss: 0.00001476
Iteration 98/1000 | Loss: 0.00001476
Iteration 99/1000 | Loss: 0.00001476
Iteration 100/1000 | Loss: 0.00001476
Iteration 101/1000 | Loss: 0.00001476
Iteration 102/1000 | Loss: 0.00001475
Iteration 103/1000 | Loss: 0.00001475
Iteration 104/1000 | Loss: 0.00001475
Iteration 105/1000 | Loss: 0.00001475
Iteration 106/1000 | Loss: 0.00001475
Iteration 107/1000 | Loss: 0.00001475
Iteration 108/1000 | Loss: 0.00001475
Iteration 109/1000 | Loss: 0.00001475
Iteration 110/1000 | Loss: 0.00001475
Iteration 111/1000 | Loss: 0.00001475
Iteration 112/1000 | Loss: 0.00001475
Iteration 113/1000 | Loss: 0.00001475
Iteration 114/1000 | Loss: 0.00001475
Iteration 115/1000 | Loss: 0.00001475
Iteration 116/1000 | Loss: 0.00001475
Iteration 117/1000 | Loss: 0.00001475
Iteration 118/1000 | Loss: 0.00001475
Iteration 119/1000 | Loss: 0.00001475
Iteration 120/1000 | Loss: 0.00001475
Iteration 121/1000 | Loss: 0.00001475
Iteration 122/1000 | Loss: 0.00001475
Iteration 123/1000 | Loss: 0.00001475
Iteration 124/1000 | Loss: 0.00001475
Iteration 125/1000 | Loss: 0.00001475
Iteration 126/1000 | Loss: 0.00001475
Iteration 127/1000 | Loss: 0.00001475
Iteration 128/1000 | Loss: 0.00001475
Iteration 129/1000 | Loss: 0.00001475
Iteration 130/1000 | Loss: 0.00001475
Iteration 131/1000 | Loss: 0.00001475
Iteration 132/1000 | Loss: 0.00001475
Iteration 133/1000 | Loss: 0.00001475
Iteration 134/1000 | Loss: 0.00001475
Iteration 135/1000 | Loss: 0.00001475
Iteration 136/1000 | Loss: 0.00001475
Iteration 137/1000 | Loss: 0.00001475
Iteration 138/1000 | Loss: 0.00001475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.4750004083907697e-05, 1.4750004083907697e-05, 1.4750004083907697e-05, 1.4750004083907697e-05, 1.4750004083907697e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4750004083907697e-05

Optimization complete. Final v2v error: 3.2619762420654297 mm

Highest mean error: 3.587656021118164 mm for frame 60

Lowest mean error: 2.898226499557495 mm for frame 200

Saving results

Total time: 39.07737326622009
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00542420
Iteration 2/25 | Loss: 0.00177947
Iteration 3/25 | Loss: 0.00150394
Iteration 4/25 | Loss: 0.00148338
Iteration 5/25 | Loss: 0.00147879
Iteration 6/25 | Loss: 0.00147734
Iteration 7/25 | Loss: 0.00147734
Iteration 8/25 | Loss: 0.00147734
Iteration 9/25 | Loss: 0.00147734
Iteration 10/25 | Loss: 0.00147734
Iteration 11/25 | Loss: 0.00147734
Iteration 12/25 | Loss: 0.00147734
Iteration 13/25 | Loss: 0.00147734
Iteration 14/25 | Loss: 0.00147734
Iteration 15/25 | Loss: 0.00147734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001477341982536018, 0.001477341982536018, 0.001477341982536018, 0.001477341982536018, 0.001477341982536018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001477341982536018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84514260
Iteration 2/25 | Loss: 0.00270879
Iteration 3/25 | Loss: 0.00270879
Iteration 4/25 | Loss: 0.00270879
Iteration 5/25 | Loss: 0.00270879
Iteration 6/25 | Loss: 0.00270879
Iteration 7/25 | Loss: 0.00270879
Iteration 8/25 | Loss: 0.00270878
Iteration 9/25 | Loss: 0.00270878
Iteration 10/25 | Loss: 0.00270878
Iteration 11/25 | Loss: 0.00270878
Iteration 12/25 | Loss: 0.00270878
Iteration 13/25 | Loss: 0.00270878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0027087845373898745, 0.0027087845373898745, 0.0027087845373898745, 0.0027087845373898745, 0.0027087845373898745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027087845373898745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00270878
Iteration 2/1000 | Loss: 0.00009393
Iteration 3/1000 | Loss: 0.00005350
Iteration 4/1000 | Loss: 0.00004441
Iteration 5/1000 | Loss: 0.00004131
Iteration 6/1000 | Loss: 0.00003895
Iteration 7/1000 | Loss: 0.00003782
Iteration 8/1000 | Loss: 0.00003652
Iteration 9/1000 | Loss: 0.00003560
Iteration 10/1000 | Loss: 0.00003474
Iteration 11/1000 | Loss: 0.00003422
Iteration 12/1000 | Loss: 0.00003369
Iteration 13/1000 | Loss: 0.00003334
Iteration 14/1000 | Loss: 0.00003308
Iteration 15/1000 | Loss: 0.00003286
Iteration 16/1000 | Loss: 0.00003269
Iteration 17/1000 | Loss: 0.00003249
Iteration 18/1000 | Loss: 0.00003234
Iteration 19/1000 | Loss: 0.00003219
Iteration 20/1000 | Loss: 0.00003205
Iteration 21/1000 | Loss: 0.00003188
Iteration 22/1000 | Loss: 0.00003180
Iteration 23/1000 | Loss: 0.00003180
Iteration 24/1000 | Loss: 0.00003179
Iteration 25/1000 | Loss: 0.00003175
Iteration 26/1000 | Loss: 0.00003169
Iteration 27/1000 | Loss: 0.00003165
Iteration 28/1000 | Loss: 0.00003165
Iteration 29/1000 | Loss: 0.00003164
Iteration 30/1000 | Loss: 0.00003164
Iteration 31/1000 | Loss: 0.00003164
Iteration 32/1000 | Loss: 0.00003164
Iteration 33/1000 | Loss: 0.00003164
Iteration 34/1000 | Loss: 0.00003164
Iteration 35/1000 | Loss: 0.00003161
Iteration 36/1000 | Loss: 0.00003161
Iteration 37/1000 | Loss: 0.00003161
Iteration 38/1000 | Loss: 0.00003161
Iteration 39/1000 | Loss: 0.00003161
Iteration 40/1000 | Loss: 0.00003161
Iteration 41/1000 | Loss: 0.00003160
Iteration 42/1000 | Loss: 0.00003160
Iteration 43/1000 | Loss: 0.00003160
Iteration 44/1000 | Loss: 0.00003160
Iteration 45/1000 | Loss: 0.00003159
Iteration 46/1000 | Loss: 0.00003159
Iteration 47/1000 | Loss: 0.00003159
Iteration 48/1000 | Loss: 0.00003158
Iteration 49/1000 | Loss: 0.00003158
Iteration 50/1000 | Loss: 0.00003158
Iteration 51/1000 | Loss: 0.00003158
Iteration 52/1000 | Loss: 0.00003158
Iteration 53/1000 | Loss: 0.00003158
Iteration 54/1000 | Loss: 0.00003157
Iteration 55/1000 | Loss: 0.00003157
Iteration 56/1000 | Loss: 0.00003157
Iteration 57/1000 | Loss: 0.00003156
Iteration 58/1000 | Loss: 0.00003156
Iteration 59/1000 | Loss: 0.00003155
Iteration 60/1000 | Loss: 0.00003154
Iteration 61/1000 | Loss: 0.00003154
Iteration 62/1000 | Loss: 0.00003154
Iteration 63/1000 | Loss: 0.00003154
Iteration 64/1000 | Loss: 0.00003154
Iteration 65/1000 | Loss: 0.00003154
Iteration 66/1000 | Loss: 0.00003154
Iteration 67/1000 | Loss: 0.00003154
Iteration 68/1000 | Loss: 0.00003153
Iteration 69/1000 | Loss: 0.00003153
Iteration 70/1000 | Loss: 0.00003152
Iteration 71/1000 | Loss: 0.00003151
Iteration 72/1000 | Loss: 0.00003151
Iteration 73/1000 | Loss: 0.00003150
Iteration 74/1000 | Loss: 0.00003150
Iteration 75/1000 | Loss: 0.00003150
Iteration 76/1000 | Loss: 0.00003150
Iteration 77/1000 | Loss: 0.00003150
Iteration 78/1000 | Loss: 0.00003150
Iteration 79/1000 | Loss: 0.00003149
Iteration 80/1000 | Loss: 0.00003149
Iteration 81/1000 | Loss: 0.00003149
Iteration 82/1000 | Loss: 0.00003149
Iteration 83/1000 | Loss: 0.00003149
Iteration 84/1000 | Loss: 0.00003149
Iteration 85/1000 | Loss: 0.00003149
Iteration 86/1000 | Loss: 0.00003148
Iteration 87/1000 | Loss: 0.00003148
Iteration 88/1000 | Loss: 0.00003148
Iteration 89/1000 | Loss: 0.00003148
Iteration 90/1000 | Loss: 0.00003148
Iteration 91/1000 | Loss: 0.00003147
Iteration 92/1000 | Loss: 0.00003147
Iteration 93/1000 | Loss: 0.00003147
Iteration 94/1000 | Loss: 0.00003146
Iteration 95/1000 | Loss: 0.00003146
Iteration 96/1000 | Loss: 0.00003146
Iteration 97/1000 | Loss: 0.00003145
Iteration 98/1000 | Loss: 0.00003145
Iteration 99/1000 | Loss: 0.00003145
Iteration 100/1000 | Loss: 0.00003145
Iteration 101/1000 | Loss: 0.00003144
Iteration 102/1000 | Loss: 0.00003143
Iteration 103/1000 | Loss: 0.00003143
Iteration 104/1000 | Loss: 0.00003143
Iteration 105/1000 | Loss: 0.00003143
Iteration 106/1000 | Loss: 0.00003143
Iteration 107/1000 | Loss: 0.00003143
Iteration 108/1000 | Loss: 0.00003143
Iteration 109/1000 | Loss: 0.00003143
Iteration 110/1000 | Loss: 0.00003143
Iteration 111/1000 | Loss: 0.00003143
Iteration 112/1000 | Loss: 0.00003142
Iteration 113/1000 | Loss: 0.00003142
Iteration 114/1000 | Loss: 0.00003142
Iteration 115/1000 | Loss: 0.00003142
Iteration 116/1000 | Loss: 0.00003142
Iteration 117/1000 | Loss: 0.00003142
Iteration 118/1000 | Loss: 0.00003141
Iteration 119/1000 | Loss: 0.00003141
Iteration 120/1000 | Loss: 0.00003141
Iteration 121/1000 | Loss: 0.00003141
Iteration 122/1000 | Loss: 0.00003141
Iteration 123/1000 | Loss: 0.00003141
Iteration 124/1000 | Loss: 0.00003141
Iteration 125/1000 | Loss: 0.00003140
Iteration 126/1000 | Loss: 0.00003140
Iteration 127/1000 | Loss: 0.00003140
Iteration 128/1000 | Loss: 0.00003140
Iteration 129/1000 | Loss: 0.00003140
Iteration 130/1000 | Loss: 0.00003140
Iteration 131/1000 | Loss: 0.00003140
Iteration 132/1000 | Loss: 0.00003139
Iteration 133/1000 | Loss: 0.00003139
Iteration 134/1000 | Loss: 0.00003139
Iteration 135/1000 | Loss: 0.00003139
Iteration 136/1000 | Loss: 0.00003138
Iteration 137/1000 | Loss: 0.00003138
Iteration 138/1000 | Loss: 0.00003138
Iteration 139/1000 | Loss: 0.00003138
Iteration 140/1000 | Loss: 0.00003138
Iteration 141/1000 | Loss: 0.00003138
Iteration 142/1000 | Loss: 0.00003138
Iteration 143/1000 | Loss: 0.00003138
Iteration 144/1000 | Loss: 0.00003138
Iteration 145/1000 | Loss: 0.00003138
Iteration 146/1000 | Loss: 0.00003137
Iteration 147/1000 | Loss: 0.00003137
Iteration 148/1000 | Loss: 0.00003137
Iteration 149/1000 | Loss: 0.00003137
Iteration 150/1000 | Loss: 0.00003137
Iteration 151/1000 | Loss: 0.00003137
Iteration 152/1000 | Loss: 0.00003137
Iteration 153/1000 | Loss: 0.00003137
Iteration 154/1000 | Loss: 0.00003137
Iteration 155/1000 | Loss: 0.00003137
Iteration 156/1000 | Loss: 0.00003137
Iteration 157/1000 | Loss: 0.00003137
Iteration 158/1000 | Loss: 0.00003136
Iteration 159/1000 | Loss: 0.00003136
Iteration 160/1000 | Loss: 0.00003136
Iteration 161/1000 | Loss: 0.00003136
Iteration 162/1000 | Loss: 0.00003136
Iteration 163/1000 | Loss: 0.00003136
Iteration 164/1000 | Loss: 0.00003136
Iteration 165/1000 | Loss: 0.00003136
Iteration 166/1000 | Loss: 0.00003136
Iteration 167/1000 | Loss: 0.00003136
Iteration 168/1000 | Loss: 0.00003136
Iteration 169/1000 | Loss: 0.00003135
Iteration 170/1000 | Loss: 0.00003135
Iteration 171/1000 | Loss: 0.00003135
Iteration 172/1000 | Loss: 0.00003135
Iteration 173/1000 | Loss: 0.00003135
Iteration 174/1000 | Loss: 0.00003135
Iteration 175/1000 | Loss: 0.00003135
Iteration 176/1000 | Loss: 0.00003135
Iteration 177/1000 | Loss: 0.00003135
Iteration 178/1000 | Loss: 0.00003135
Iteration 179/1000 | Loss: 0.00003134
Iteration 180/1000 | Loss: 0.00003134
Iteration 181/1000 | Loss: 0.00003134
Iteration 182/1000 | Loss: 0.00003134
Iteration 183/1000 | Loss: 0.00003134
Iteration 184/1000 | Loss: 0.00003134
Iteration 185/1000 | Loss: 0.00003134
Iteration 186/1000 | Loss: 0.00003134
Iteration 187/1000 | Loss: 0.00003134
Iteration 188/1000 | Loss: 0.00003134
Iteration 189/1000 | Loss: 0.00003134
Iteration 190/1000 | Loss: 0.00003134
Iteration 191/1000 | Loss: 0.00003134
Iteration 192/1000 | Loss: 0.00003134
Iteration 193/1000 | Loss: 0.00003134
Iteration 194/1000 | Loss: 0.00003134
Iteration 195/1000 | Loss: 0.00003134
Iteration 196/1000 | Loss: 0.00003134
Iteration 197/1000 | Loss: 0.00003133
Iteration 198/1000 | Loss: 0.00003133
Iteration 199/1000 | Loss: 0.00003133
Iteration 200/1000 | Loss: 0.00003133
Iteration 201/1000 | Loss: 0.00003133
Iteration 202/1000 | Loss: 0.00003133
Iteration 203/1000 | Loss: 0.00003133
Iteration 204/1000 | Loss: 0.00003133
Iteration 205/1000 | Loss: 0.00003133
Iteration 206/1000 | Loss: 0.00003132
Iteration 207/1000 | Loss: 0.00003132
Iteration 208/1000 | Loss: 0.00003132
Iteration 209/1000 | Loss: 0.00003132
Iteration 210/1000 | Loss: 0.00003132
Iteration 211/1000 | Loss: 0.00003132
Iteration 212/1000 | Loss: 0.00003132
Iteration 213/1000 | Loss: 0.00003132
Iteration 214/1000 | Loss: 0.00003132
Iteration 215/1000 | Loss: 0.00003132
Iteration 216/1000 | Loss: 0.00003132
Iteration 217/1000 | Loss: 0.00003132
Iteration 218/1000 | Loss: 0.00003132
Iteration 219/1000 | Loss: 0.00003132
Iteration 220/1000 | Loss: 0.00003132
Iteration 221/1000 | Loss: 0.00003132
Iteration 222/1000 | Loss: 0.00003132
Iteration 223/1000 | Loss: 0.00003132
Iteration 224/1000 | Loss: 0.00003132
Iteration 225/1000 | Loss: 0.00003131
Iteration 226/1000 | Loss: 0.00003131
Iteration 227/1000 | Loss: 0.00003131
Iteration 228/1000 | Loss: 0.00003131
Iteration 229/1000 | Loss: 0.00003131
Iteration 230/1000 | Loss: 0.00003131
Iteration 231/1000 | Loss: 0.00003131
Iteration 232/1000 | Loss: 0.00003131
Iteration 233/1000 | Loss: 0.00003131
Iteration 234/1000 | Loss: 0.00003131
Iteration 235/1000 | Loss: 0.00003131
Iteration 236/1000 | Loss: 0.00003131
Iteration 237/1000 | Loss: 0.00003131
Iteration 238/1000 | Loss: 0.00003131
Iteration 239/1000 | Loss: 0.00003131
Iteration 240/1000 | Loss: 0.00003131
Iteration 241/1000 | Loss: 0.00003131
Iteration 242/1000 | Loss: 0.00003131
Iteration 243/1000 | Loss: 0.00003131
Iteration 244/1000 | Loss: 0.00003131
Iteration 245/1000 | Loss: 0.00003131
Iteration 246/1000 | Loss: 0.00003131
Iteration 247/1000 | Loss: 0.00003131
Iteration 248/1000 | Loss: 0.00003131
Iteration 249/1000 | Loss: 0.00003131
Iteration 250/1000 | Loss: 0.00003131
Iteration 251/1000 | Loss: 0.00003131
Iteration 252/1000 | Loss: 0.00003131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [3.131396806566045e-05, 3.131396806566045e-05, 3.131396806566045e-05, 3.131396806566045e-05, 3.131396806566045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.131396806566045e-05

Optimization complete. Final v2v error: 4.388585567474365 mm

Highest mean error: 5.957179546356201 mm for frame 18

Lowest mean error: 3.318793773651123 mm for frame 0

Saving results

Total time: 64.85339903831482
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924175
Iteration 2/25 | Loss: 0.00924174
Iteration 3/25 | Loss: 0.00924174
Iteration 4/25 | Loss: 0.00924174
Iteration 5/25 | Loss: 0.00924174
Iteration 6/25 | Loss: 0.00924174
Iteration 7/25 | Loss: 0.00924173
Iteration 8/25 | Loss: 0.00924173
Iteration 9/25 | Loss: 0.00924173
Iteration 10/25 | Loss: 0.00924172
Iteration 11/25 | Loss: 0.00924172
Iteration 12/25 | Loss: 0.00924172
Iteration 13/25 | Loss: 0.00924172
Iteration 14/25 | Loss: 0.00924171
Iteration 15/25 | Loss: 0.00924171
Iteration 16/25 | Loss: 0.00924171
Iteration 17/25 | Loss: 0.00924171
Iteration 18/25 | Loss: 0.00924171
Iteration 19/25 | Loss: 0.00924171
Iteration 20/25 | Loss: 0.00924170
Iteration 21/25 | Loss: 0.00924170
Iteration 22/25 | Loss: 0.00924170
Iteration 23/25 | Loss: 0.00924170
Iteration 24/25 | Loss: 0.00924169
Iteration 25/25 | Loss: 0.00924169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30173981
Iteration 2/25 | Loss: 0.19843769
Iteration 3/25 | Loss: 0.19488008
Iteration 4/25 | Loss: 0.20147654
Iteration 5/25 | Loss: 0.19025066
Iteration 6/25 | Loss: 0.18924552
Iteration 7/25 | Loss: 0.18929781
Iteration 8/25 | Loss: 0.19051018
Iteration 9/25 | Loss: 0.19007525
Iteration 10/25 | Loss: 0.18949527
Iteration 11/25 | Loss: 0.18891108
Iteration 12/25 | Loss: 0.18891101
Iteration 13/25 | Loss: 0.18891099
Iteration 14/25 | Loss: 0.18891098
Iteration 15/25 | Loss: 0.18891098
Iteration 16/25 | Loss: 0.18891095
Iteration 17/25 | Loss: 0.18891095
Iteration 18/25 | Loss: 0.18891095
Iteration 19/25 | Loss: 0.18891095
Iteration 20/25 | Loss: 0.18891095
Iteration 21/25 | Loss: 0.18891095
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.18891094624996185, 0.18891094624996185, 0.18891094624996185, 0.18891094624996185, 0.18891094624996185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18891094624996185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18891095
Iteration 2/1000 | Loss: 0.00394717
Iteration 3/1000 | Loss: 0.00153370
Iteration 4/1000 | Loss: 0.00084655
Iteration 5/1000 | Loss: 0.00049666
Iteration 6/1000 | Loss: 0.00034409
Iteration 7/1000 | Loss: 0.00024674
Iteration 8/1000 | Loss: 0.00018148
Iteration 9/1000 | Loss: 0.00015092
Iteration 10/1000 | Loss: 0.00012479
Iteration 11/1000 | Loss: 0.00010531
Iteration 12/1000 | Loss: 0.00009125
Iteration 13/1000 | Loss: 0.00008297
Iteration 14/1000 | Loss: 0.00007471
Iteration 15/1000 | Loss: 0.00006966
Iteration 16/1000 | Loss: 0.00006388
Iteration 17/1000 | Loss: 0.00007657
Iteration 18/1000 | Loss: 0.00011827
Iteration 19/1000 | Loss: 0.00007992
Iteration 20/1000 | Loss: 0.00007377
Iteration 21/1000 | Loss: 0.00006171
Iteration 22/1000 | Loss: 0.00005594
Iteration 23/1000 | Loss: 0.00006922
Iteration 24/1000 | Loss: 0.00005739
Iteration 25/1000 | Loss: 0.00006157
Iteration 26/1000 | Loss: 0.00005499
Iteration 27/1000 | Loss: 0.00005067
Iteration 28/1000 | Loss: 0.00004864
Iteration 29/1000 | Loss: 0.00004634
Iteration 30/1000 | Loss: 0.00004470
Iteration 31/1000 | Loss: 0.00004313
Iteration 32/1000 | Loss: 0.00004220
Iteration 33/1000 | Loss: 0.00004142
Iteration 34/1000 | Loss: 0.00004087
Iteration 35/1000 | Loss: 0.00004041
Iteration 36/1000 | Loss: 0.00004000
Iteration 37/1000 | Loss: 0.00003957
Iteration 38/1000 | Loss: 0.00003901
Iteration 39/1000 | Loss: 0.00003864
Iteration 40/1000 | Loss: 0.00003813
Iteration 41/1000 | Loss: 0.00003779
Iteration 42/1000 | Loss: 0.00003754
Iteration 43/1000 | Loss: 0.00003737
Iteration 44/1000 | Loss: 0.00003728
Iteration 45/1000 | Loss: 0.00003721
Iteration 46/1000 | Loss: 0.00003720
Iteration 47/1000 | Loss: 0.00003720
Iteration 48/1000 | Loss: 0.00003720
Iteration 49/1000 | Loss: 0.00003720
Iteration 50/1000 | Loss: 0.00003719
Iteration 51/1000 | Loss: 0.00003705
Iteration 52/1000 | Loss: 0.00003701
Iteration 53/1000 | Loss: 0.00003682
Iteration 54/1000 | Loss: 0.00003682
Iteration 55/1000 | Loss: 0.00003678
Iteration 56/1000 | Loss: 0.00003676
Iteration 57/1000 | Loss: 0.00003675
Iteration 58/1000 | Loss: 0.00003665
Iteration 59/1000 | Loss: 0.00003665
Iteration 60/1000 | Loss: 0.00003664
Iteration 61/1000 | Loss: 0.00003663
Iteration 62/1000 | Loss: 0.00003663
Iteration 63/1000 | Loss: 0.00003663
Iteration 64/1000 | Loss: 0.00003663
Iteration 65/1000 | Loss: 0.00003663
Iteration 66/1000 | Loss: 0.00003663
Iteration 67/1000 | Loss: 0.00003662
Iteration 68/1000 | Loss: 0.00003662
Iteration 69/1000 | Loss: 0.00003662
Iteration 70/1000 | Loss: 0.00003662
Iteration 71/1000 | Loss: 0.00003661
Iteration 72/1000 | Loss: 0.00003660
Iteration 73/1000 | Loss: 0.00003658
Iteration 74/1000 | Loss: 0.00003657
Iteration 75/1000 | Loss: 0.00003657
Iteration 76/1000 | Loss: 0.00003657
Iteration 77/1000 | Loss: 0.00003656
Iteration 78/1000 | Loss: 0.00003656
Iteration 79/1000 | Loss: 0.00003656
Iteration 80/1000 | Loss: 0.00003656
Iteration 81/1000 | Loss: 0.00003655
Iteration 82/1000 | Loss: 0.00003655
Iteration 83/1000 | Loss: 0.00003655
Iteration 84/1000 | Loss: 0.00003655
Iteration 85/1000 | Loss: 0.00003655
Iteration 86/1000 | Loss: 0.00003655
Iteration 87/1000 | Loss: 0.00003654
Iteration 88/1000 | Loss: 0.00003654
Iteration 89/1000 | Loss: 0.00003654
Iteration 90/1000 | Loss: 0.00003654
Iteration 91/1000 | Loss: 0.00003654
Iteration 92/1000 | Loss: 0.00003654
Iteration 93/1000 | Loss: 0.00003654
Iteration 94/1000 | Loss: 0.00003654
Iteration 95/1000 | Loss: 0.00003654
Iteration 96/1000 | Loss: 0.00003654
Iteration 97/1000 | Loss: 0.00003654
Iteration 98/1000 | Loss: 0.00003654
Iteration 99/1000 | Loss: 0.00003654
Iteration 100/1000 | Loss: 0.00003653
Iteration 101/1000 | Loss: 0.00003653
Iteration 102/1000 | Loss: 0.00003653
Iteration 103/1000 | Loss: 0.00003653
Iteration 104/1000 | Loss: 0.00003653
Iteration 105/1000 | Loss: 0.00003652
Iteration 106/1000 | Loss: 0.00003652
Iteration 107/1000 | Loss: 0.00003652
Iteration 108/1000 | Loss: 0.00003652
Iteration 109/1000 | Loss: 0.00003652
Iteration 110/1000 | Loss: 0.00003651
Iteration 111/1000 | Loss: 0.00003651
Iteration 112/1000 | Loss: 0.00003651
Iteration 113/1000 | Loss: 0.00003650
Iteration 114/1000 | Loss: 0.00003650
Iteration 115/1000 | Loss: 0.00003650
Iteration 116/1000 | Loss: 0.00003650
Iteration 117/1000 | Loss: 0.00003650
Iteration 118/1000 | Loss: 0.00003649
Iteration 119/1000 | Loss: 0.00003649
Iteration 120/1000 | Loss: 0.00003649
Iteration 121/1000 | Loss: 0.00003649
Iteration 122/1000 | Loss: 0.00003649
Iteration 123/1000 | Loss: 0.00003649
Iteration 124/1000 | Loss: 0.00003649
Iteration 125/1000 | Loss: 0.00003649
Iteration 126/1000 | Loss: 0.00003648
Iteration 127/1000 | Loss: 0.00003648
Iteration 128/1000 | Loss: 0.00003648
Iteration 129/1000 | Loss: 0.00003647
Iteration 130/1000 | Loss: 0.00003647
Iteration 131/1000 | Loss: 0.00003647
Iteration 132/1000 | Loss: 0.00003647
Iteration 133/1000 | Loss: 0.00003647
Iteration 134/1000 | Loss: 0.00003647
Iteration 135/1000 | Loss: 0.00003647
Iteration 136/1000 | Loss: 0.00003647
Iteration 137/1000 | Loss: 0.00003647
Iteration 138/1000 | Loss: 0.00003647
Iteration 139/1000 | Loss: 0.00003647
Iteration 140/1000 | Loss: 0.00003647
Iteration 141/1000 | Loss: 0.00003647
Iteration 142/1000 | Loss: 0.00003647
Iteration 143/1000 | Loss: 0.00003647
Iteration 144/1000 | Loss: 0.00003647
Iteration 145/1000 | Loss: 0.00003647
Iteration 146/1000 | Loss: 0.00003646
Iteration 147/1000 | Loss: 0.00003646
Iteration 148/1000 | Loss: 0.00003646
Iteration 149/1000 | Loss: 0.00003646
Iteration 150/1000 | Loss: 0.00003646
Iteration 151/1000 | Loss: 0.00003646
Iteration 152/1000 | Loss: 0.00003646
Iteration 153/1000 | Loss: 0.00003646
Iteration 154/1000 | Loss: 0.00003646
Iteration 155/1000 | Loss: 0.00003646
Iteration 156/1000 | Loss: 0.00003646
Iteration 157/1000 | Loss: 0.00003646
Iteration 158/1000 | Loss: 0.00003646
Iteration 159/1000 | Loss: 0.00003646
Iteration 160/1000 | Loss: 0.00003645
Iteration 161/1000 | Loss: 0.00003645
Iteration 162/1000 | Loss: 0.00003645
Iteration 163/1000 | Loss: 0.00003645
Iteration 164/1000 | Loss: 0.00003645
Iteration 165/1000 | Loss: 0.00003645
Iteration 166/1000 | Loss: 0.00003644
Iteration 167/1000 | Loss: 0.00003644
Iteration 168/1000 | Loss: 0.00003644
Iteration 169/1000 | Loss: 0.00003644
Iteration 170/1000 | Loss: 0.00003644
Iteration 171/1000 | Loss: 0.00003644
Iteration 172/1000 | Loss: 0.00003644
Iteration 173/1000 | Loss: 0.00003643
Iteration 174/1000 | Loss: 0.00003643
Iteration 175/1000 | Loss: 0.00003643
Iteration 176/1000 | Loss: 0.00003643
Iteration 177/1000 | Loss: 0.00003643
Iteration 178/1000 | Loss: 0.00003643
Iteration 179/1000 | Loss: 0.00003643
Iteration 180/1000 | Loss: 0.00003643
Iteration 181/1000 | Loss: 0.00003643
Iteration 182/1000 | Loss: 0.00003643
Iteration 183/1000 | Loss: 0.00003643
Iteration 184/1000 | Loss: 0.00003643
Iteration 185/1000 | Loss: 0.00003643
Iteration 186/1000 | Loss: 0.00003643
Iteration 187/1000 | Loss: 0.00003643
Iteration 188/1000 | Loss: 0.00003643
Iteration 189/1000 | Loss: 0.00003643
Iteration 190/1000 | Loss: 0.00003643
Iteration 191/1000 | Loss: 0.00003643
Iteration 192/1000 | Loss: 0.00003643
Iteration 193/1000 | Loss: 0.00003643
Iteration 194/1000 | Loss: 0.00003643
Iteration 195/1000 | Loss: 0.00003643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [3.642539741122164e-05, 3.642539741122164e-05, 3.642539741122164e-05, 3.642539741122164e-05, 3.642539741122164e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.642539741122164e-05

Optimization complete. Final v2v error: 4.584057331085205 mm

Highest mean error: 10.223321914672852 mm for frame 83

Lowest mean error: 3.589237689971924 mm for frame 0

Saving results

Total time: 136.55316472053528
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410355
Iteration 2/25 | Loss: 0.00152961
Iteration 3/25 | Loss: 0.00141217
Iteration 4/25 | Loss: 0.00139827
Iteration 5/25 | Loss: 0.00139551
Iteration 6/25 | Loss: 0.00139485
Iteration 7/25 | Loss: 0.00139485
Iteration 8/25 | Loss: 0.00139485
Iteration 9/25 | Loss: 0.00139485
Iteration 10/25 | Loss: 0.00139485
Iteration 11/25 | Loss: 0.00139485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001394846010953188, 0.001394846010953188, 0.001394846010953188, 0.001394846010953188, 0.001394846010953188]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001394846010953188

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09833288
Iteration 2/25 | Loss: 0.00370522
Iteration 3/25 | Loss: 0.00370522
Iteration 4/25 | Loss: 0.00370521
Iteration 5/25 | Loss: 0.00370521
Iteration 6/25 | Loss: 0.00370521
Iteration 7/25 | Loss: 0.00370521
Iteration 8/25 | Loss: 0.00370521
Iteration 9/25 | Loss: 0.00370521
Iteration 10/25 | Loss: 0.00370521
Iteration 11/25 | Loss: 0.00370521
Iteration 12/25 | Loss: 0.00370521
Iteration 13/25 | Loss: 0.00370521
Iteration 14/25 | Loss: 0.00370521
Iteration 15/25 | Loss: 0.00370521
Iteration 16/25 | Loss: 0.00370521
Iteration 17/25 | Loss: 0.00370521
Iteration 18/25 | Loss: 0.00370521
Iteration 19/25 | Loss: 0.00370521
Iteration 20/25 | Loss: 0.00370521
Iteration 21/25 | Loss: 0.00370521
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0037052114494144917, 0.0037052114494144917, 0.0037052114494144917, 0.0037052114494144917, 0.0037052114494144917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037052114494144917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00370521
Iteration 2/1000 | Loss: 0.00006719
Iteration 3/1000 | Loss: 0.00003400
Iteration 4/1000 | Loss: 0.00002341
Iteration 5/1000 | Loss: 0.00001936
Iteration 6/1000 | Loss: 0.00001739
Iteration 7/1000 | Loss: 0.00001592
Iteration 8/1000 | Loss: 0.00001514
Iteration 9/1000 | Loss: 0.00001453
Iteration 10/1000 | Loss: 0.00001420
Iteration 11/1000 | Loss: 0.00001387
Iteration 12/1000 | Loss: 0.00001364
Iteration 13/1000 | Loss: 0.00001339
Iteration 14/1000 | Loss: 0.00001339
Iteration 15/1000 | Loss: 0.00001336
Iteration 16/1000 | Loss: 0.00001328
Iteration 17/1000 | Loss: 0.00001327
Iteration 18/1000 | Loss: 0.00001322
Iteration 19/1000 | Loss: 0.00001312
Iteration 20/1000 | Loss: 0.00001308
Iteration 21/1000 | Loss: 0.00001307
Iteration 22/1000 | Loss: 0.00001307
Iteration 23/1000 | Loss: 0.00001306
Iteration 24/1000 | Loss: 0.00001305
Iteration 25/1000 | Loss: 0.00001305
Iteration 26/1000 | Loss: 0.00001304
Iteration 27/1000 | Loss: 0.00001304
Iteration 28/1000 | Loss: 0.00001303
Iteration 29/1000 | Loss: 0.00001302
Iteration 30/1000 | Loss: 0.00001302
Iteration 31/1000 | Loss: 0.00001301
Iteration 32/1000 | Loss: 0.00001300
Iteration 33/1000 | Loss: 0.00001299
Iteration 34/1000 | Loss: 0.00001298
Iteration 35/1000 | Loss: 0.00001298
Iteration 36/1000 | Loss: 0.00001297
Iteration 37/1000 | Loss: 0.00001296
Iteration 38/1000 | Loss: 0.00001295
Iteration 39/1000 | Loss: 0.00001294
Iteration 40/1000 | Loss: 0.00001294
Iteration 41/1000 | Loss: 0.00001294
Iteration 42/1000 | Loss: 0.00001294
Iteration 43/1000 | Loss: 0.00001294
Iteration 44/1000 | Loss: 0.00001294
Iteration 45/1000 | Loss: 0.00001294
Iteration 46/1000 | Loss: 0.00001294
Iteration 47/1000 | Loss: 0.00001293
Iteration 48/1000 | Loss: 0.00001293
Iteration 49/1000 | Loss: 0.00001293
Iteration 50/1000 | Loss: 0.00001293
Iteration 51/1000 | Loss: 0.00001292
Iteration 52/1000 | Loss: 0.00001292
Iteration 53/1000 | Loss: 0.00001292
Iteration 54/1000 | Loss: 0.00001291
Iteration 55/1000 | Loss: 0.00001291
Iteration 56/1000 | Loss: 0.00001291
Iteration 57/1000 | Loss: 0.00001291
Iteration 58/1000 | Loss: 0.00001290
Iteration 59/1000 | Loss: 0.00001290
Iteration 60/1000 | Loss: 0.00001290
Iteration 61/1000 | Loss: 0.00001289
Iteration 62/1000 | Loss: 0.00001289
Iteration 63/1000 | Loss: 0.00001289
Iteration 64/1000 | Loss: 0.00001288
Iteration 65/1000 | Loss: 0.00001288
Iteration 66/1000 | Loss: 0.00001288
Iteration 67/1000 | Loss: 0.00001288
Iteration 68/1000 | Loss: 0.00001288
Iteration 69/1000 | Loss: 0.00001287
Iteration 70/1000 | Loss: 0.00001287
Iteration 71/1000 | Loss: 0.00001287
Iteration 72/1000 | Loss: 0.00001287
Iteration 73/1000 | Loss: 0.00001286
Iteration 74/1000 | Loss: 0.00001286
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001284
Iteration 82/1000 | Loss: 0.00001284
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001284
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001284
Iteration 87/1000 | Loss: 0.00001284
Iteration 88/1000 | Loss: 0.00001284
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001284
Iteration 92/1000 | Loss: 0.00001284
Iteration 93/1000 | Loss: 0.00001284
Iteration 94/1000 | Loss: 0.00001284
Iteration 95/1000 | Loss: 0.00001283
Iteration 96/1000 | Loss: 0.00001283
Iteration 97/1000 | Loss: 0.00001283
Iteration 98/1000 | Loss: 0.00001283
Iteration 99/1000 | Loss: 0.00001283
Iteration 100/1000 | Loss: 0.00001283
Iteration 101/1000 | Loss: 0.00001283
Iteration 102/1000 | Loss: 0.00001283
Iteration 103/1000 | Loss: 0.00001282
Iteration 104/1000 | Loss: 0.00001282
Iteration 105/1000 | Loss: 0.00001282
Iteration 106/1000 | Loss: 0.00001282
Iteration 107/1000 | Loss: 0.00001282
Iteration 108/1000 | Loss: 0.00001282
Iteration 109/1000 | Loss: 0.00001282
Iteration 110/1000 | Loss: 0.00001282
Iteration 111/1000 | Loss: 0.00001281
Iteration 112/1000 | Loss: 0.00001281
Iteration 113/1000 | Loss: 0.00001281
Iteration 114/1000 | Loss: 0.00001281
Iteration 115/1000 | Loss: 0.00001281
Iteration 116/1000 | Loss: 0.00001281
Iteration 117/1000 | Loss: 0.00001281
Iteration 118/1000 | Loss: 0.00001281
Iteration 119/1000 | Loss: 0.00001281
Iteration 120/1000 | Loss: 0.00001281
Iteration 121/1000 | Loss: 0.00001281
Iteration 122/1000 | Loss: 0.00001281
Iteration 123/1000 | Loss: 0.00001281
Iteration 124/1000 | Loss: 0.00001280
Iteration 125/1000 | Loss: 0.00001280
Iteration 126/1000 | Loss: 0.00001280
Iteration 127/1000 | Loss: 0.00001280
Iteration 128/1000 | Loss: 0.00001280
Iteration 129/1000 | Loss: 0.00001280
Iteration 130/1000 | Loss: 0.00001280
Iteration 131/1000 | Loss: 0.00001280
Iteration 132/1000 | Loss: 0.00001280
Iteration 133/1000 | Loss: 0.00001280
Iteration 134/1000 | Loss: 0.00001280
Iteration 135/1000 | Loss: 0.00001279
Iteration 136/1000 | Loss: 0.00001279
Iteration 137/1000 | Loss: 0.00001279
Iteration 138/1000 | Loss: 0.00001279
Iteration 139/1000 | Loss: 0.00001279
Iteration 140/1000 | Loss: 0.00001279
Iteration 141/1000 | Loss: 0.00001279
Iteration 142/1000 | Loss: 0.00001279
Iteration 143/1000 | Loss: 0.00001279
Iteration 144/1000 | Loss: 0.00001279
Iteration 145/1000 | Loss: 0.00001278
Iteration 146/1000 | Loss: 0.00001278
Iteration 147/1000 | Loss: 0.00001278
Iteration 148/1000 | Loss: 0.00001278
Iteration 149/1000 | Loss: 0.00001278
Iteration 150/1000 | Loss: 0.00001278
Iteration 151/1000 | Loss: 0.00001278
Iteration 152/1000 | Loss: 0.00001278
Iteration 153/1000 | Loss: 0.00001277
Iteration 154/1000 | Loss: 0.00001277
Iteration 155/1000 | Loss: 0.00001277
Iteration 156/1000 | Loss: 0.00001277
Iteration 157/1000 | Loss: 0.00001277
Iteration 158/1000 | Loss: 0.00001277
Iteration 159/1000 | Loss: 0.00001277
Iteration 160/1000 | Loss: 0.00001277
Iteration 161/1000 | Loss: 0.00001277
Iteration 162/1000 | Loss: 0.00001277
Iteration 163/1000 | Loss: 0.00001277
Iteration 164/1000 | Loss: 0.00001276
Iteration 165/1000 | Loss: 0.00001276
Iteration 166/1000 | Loss: 0.00001276
Iteration 167/1000 | Loss: 0.00001276
Iteration 168/1000 | Loss: 0.00001276
Iteration 169/1000 | Loss: 0.00001276
Iteration 170/1000 | Loss: 0.00001276
Iteration 171/1000 | Loss: 0.00001276
Iteration 172/1000 | Loss: 0.00001276
Iteration 173/1000 | Loss: 0.00001276
Iteration 174/1000 | Loss: 0.00001276
Iteration 175/1000 | Loss: 0.00001276
Iteration 176/1000 | Loss: 0.00001276
Iteration 177/1000 | Loss: 0.00001276
Iteration 178/1000 | Loss: 0.00001276
Iteration 179/1000 | Loss: 0.00001276
Iteration 180/1000 | Loss: 0.00001276
Iteration 181/1000 | Loss: 0.00001276
Iteration 182/1000 | Loss: 0.00001276
Iteration 183/1000 | Loss: 0.00001276
Iteration 184/1000 | Loss: 0.00001276
Iteration 185/1000 | Loss: 0.00001276
Iteration 186/1000 | Loss: 0.00001276
Iteration 187/1000 | Loss: 0.00001276
Iteration 188/1000 | Loss: 0.00001276
Iteration 189/1000 | Loss: 0.00001276
Iteration 190/1000 | Loss: 0.00001276
Iteration 191/1000 | Loss: 0.00001276
Iteration 192/1000 | Loss: 0.00001276
Iteration 193/1000 | Loss: 0.00001276
Iteration 194/1000 | Loss: 0.00001276
Iteration 195/1000 | Loss: 0.00001276
Iteration 196/1000 | Loss: 0.00001276
Iteration 197/1000 | Loss: 0.00001276
Iteration 198/1000 | Loss: 0.00001276
Iteration 199/1000 | Loss: 0.00001276
Iteration 200/1000 | Loss: 0.00001276
Iteration 201/1000 | Loss: 0.00001276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.2760712706949562e-05, 1.2760712706949562e-05, 1.2760712706949562e-05, 1.2760712706949562e-05, 1.2760712706949562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2760712706949562e-05

Optimization complete. Final v2v error: 3.103566884994507 mm

Highest mean error: 3.4047446250915527 mm for frame 115

Lowest mean error: 2.8841121196746826 mm for frame 37

Saving results

Total time: 69.34603762626648
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882947
Iteration 2/25 | Loss: 0.00167054
Iteration 3/25 | Loss: 0.00142365
Iteration 4/25 | Loss: 0.00140496
Iteration 5/25 | Loss: 0.00140308
Iteration 6/25 | Loss: 0.00140266
Iteration 7/25 | Loss: 0.00140266
Iteration 8/25 | Loss: 0.00140266
Iteration 9/25 | Loss: 0.00140266
Iteration 10/25 | Loss: 0.00140266
Iteration 11/25 | Loss: 0.00140266
Iteration 12/25 | Loss: 0.00140266
Iteration 13/25 | Loss: 0.00140266
Iteration 14/25 | Loss: 0.00140266
Iteration 15/25 | Loss: 0.00140266
Iteration 16/25 | Loss: 0.00140266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014026634162291884, 0.0014026634162291884, 0.0014026634162291884, 0.0014026634162291884, 0.0014026634162291884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014026634162291884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07649136
Iteration 2/25 | Loss: 0.00182145
Iteration 3/25 | Loss: 0.00182144
Iteration 4/25 | Loss: 0.00182144
Iteration 5/25 | Loss: 0.00182144
Iteration 6/25 | Loss: 0.00182144
Iteration 7/25 | Loss: 0.00182144
Iteration 8/25 | Loss: 0.00182144
Iteration 9/25 | Loss: 0.00182144
Iteration 10/25 | Loss: 0.00182144
Iteration 11/25 | Loss: 0.00182144
Iteration 12/25 | Loss: 0.00182144
Iteration 13/25 | Loss: 0.00182144
Iteration 14/25 | Loss: 0.00182144
Iteration 15/25 | Loss: 0.00182144
Iteration 16/25 | Loss: 0.00182144
Iteration 17/25 | Loss: 0.00182144
Iteration 18/25 | Loss: 0.00182144
Iteration 19/25 | Loss: 0.00182144
Iteration 20/25 | Loss: 0.00182144
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018214426236227155, 0.0018214426236227155, 0.0018214426236227155, 0.0018214426236227155, 0.0018214426236227155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018214426236227155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182144
Iteration 2/1000 | Loss: 0.00004795
Iteration 3/1000 | Loss: 0.00002859
Iteration 4/1000 | Loss: 0.00002322
Iteration 5/1000 | Loss: 0.00002042
Iteration 6/1000 | Loss: 0.00001958
Iteration 7/1000 | Loss: 0.00001919
Iteration 8/1000 | Loss: 0.00001894
Iteration 9/1000 | Loss: 0.00001872
Iteration 10/1000 | Loss: 0.00001859
Iteration 11/1000 | Loss: 0.00001858
Iteration 12/1000 | Loss: 0.00001856
Iteration 13/1000 | Loss: 0.00001853
Iteration 14/1000 | Loss: 0.00001853
Iteration 15/1000 | Loss: 0.00001833
Iteration 16/1000 | Loss: 0.00001820
Iteration 17/1000 | Loss: 0.00001819
Iteration 18/1000 | Loss: 0.00001819
Iteration 19/1000 | Loss: 0.00001819
Iteration 20/1000 | Loss: 0.00001819
Iteration 21/1000 | Loss: 0.00001816
Iteration 22/1000 | Loss: 0.00001810
Iteration 23/1000 | Loss: 0.00001808
Iteration 24/1000 | Loss: 0.00001808
Iteration 25/1000 | Loss: 0.00001808
Iteration 26/1000 | Loss: 0.00001808
Iteration 27/1000 | Loss: 0.00001808
Iteration 28/1000 | Loss: 0.00001808
Iteration 29/1000 | Loss: 0.00001808
Iteration 30/1000 | Loss: 0.00001807
Iteration 31/1000 | Loss: 0.00001807
Iteration 32/1000 | Loss: 0.00001805
Iteration 33/1000 | Loss: 0.00001804
Iteration 34/1000 | Loss: 0.00001804
Iteration 35/1000 | Loss: 0.00001804
Iteration 36/1000 | Loss: 0.00001804
Iteration 37/1000 | Loss: 0.00001804
Iteration 38/1000 | Loss: 0.00001804
Iteration 39/1000 | Loss: 0.00001803
Iteration 40/1000 | Loss: 0.00001803
Iteration 41/1000 | Loss: 0.00001803
Iteration 42/1000 | Loss: 0.00001803
Iteration 43/1000 | Loss: 0.00001803
Iteration 44/1000 | Loss: 0.00001803
Iteration 45/1000 | Loss: 0.00001802
Iteration 46/1000 | Loss: 0.00001802
Iteration 47/1000 | Loss: 0.00001802
Iteration 48/1000 | Loss: 0.00001802
Iteration 49/1000 | Loss: 0.00001802
Iteration 50/1000 | Loss: 0.00001801
Iteration 51/1000 | Loss: 0.00001801
Iteration 52/1000 | Loss: 0.00001801
Iteration 53/1000 | Loss: 0.00001800
Iteration 54/1000 | Loss: 0.00001800
Iteration 55/1000 | Loss: 0.00001800
Iteration 56/1000 | Loss: 0.00001799
Iteration 57/1000 | Loss: 0.00001799
Iteration 58/1000 | Loss: 0.00001799
Iteration 59/1000 | Loss: 0.00001799
Iteration 60/1000 | Loss: 0.00001799
Iteration 61/1000 | Loss: 0.00001799
Iteration 62/1000 | Loss: 0.00001799
Iteration 63/1000 | Loss: 0.00001799
Iteration 64/1000 | Loss: 0.00001799
Iteration 65/1000 | Loss: 0.00001799
Iteration 66/1000 | Loss: 0.00001799
Iteration 67/1000 | Loss: 0.00001799
Iteration 68/1000 | Loss: 0.00001799
Iteration 69/1000 | Loss: 0.00001799
Iteration 70/1000 | Loss: 0.00001799
Iteration 71/1000 | Loss: 0.00001799
Iteration 72/1000 | Loss: 0.00001799
Iteration 73/1000 | Loss: 0.00001799
Iteration 74/1000 | Loss: 0.00001799
Iteration 75/1000 | Loss: 0.00001799
Iteration 76/1000 | Loss: 0.00001799
Iteration 77/1000 | Loss: 0.00001799
Iteration 78/1000 | Loss: 0.00001798
Iteration 79/1000 | Loss: 0.00001798
Iteration 80/1000 | Loss: 0.00001797
Iteration 81/1000 | Loss: 0.00001797
Iteration 82/1000 | Loss: 0.00001797
Iteration 83/1000 | Loss: 0.00001797
Iteration 84/1000 | Loss: 0.00001796
Iteration 85/1000 | Loss: 0.00001796
Iteration 86/1000 | Loss: 0.00001796
Iteration 87/1000 | Loss: 0.00001796
Iteration 88/1000 | Loss: 0.00001796
Iteration 89/1000 | Loss: 0.00001795
Iteration 90/1000 | Loss: 0.00001795
Iteration 91/1000 | Loss: 0.00001795
Iteration 92/1000 | Loss: 0.00001795
Iteration 93/1000 | Loss: 0.00001795
Iteration 94/1000 | Loss: 0.00001795
Iteration 95/1000 | Loss: 0.00001795
Iteration 96/1000 | Loss: 0.00001795
Iteration 97/1000 | Loss: 0.00001795
Iteration 98/1000 | Loss: 0.00001795
Iteration 99/1000 | Loss: 0.00001795
Iteration 100/1000 | Loss: 0.00001795
Iteration 101/1000 | Loss: 0.00001795
Iteration 102/1000 | Loss: 0.00001795
Iteration 103/1000 | Loss: 0.00001795
Iteration 104/1000 | Loss: 0.00001794
Iteration 105/1000 | Loss: 0.00001794
Iteration 106/1000 | Loss: 0.00001794
Iteration 107/1000 | Loss: 0.00001794
Iteration 108/1000 | Loss: 0.00001794
Iteration 109/1000 | Loss: 0.00001794
Iteration 110/1000 | Loss: 0.00001794
Iteration 111/1000 | Loss: 0.00001794
Iteration 112/1000 | Loss: 0.00001794
Iteration 113/1000 | Loss: 0.00001794
Iteration 114/1000 | Loss: 0.00001794
Iteration 115/1000 | Loss: 0.00001794
Iteration 116/1000 | Loss: 0.00001794
Iteration 117/1000 | Loss: 0.00001794
Iteration 118/1000 | Loss: 0.00001794
Iteration 119/1000 | Loss: 0.00001794
Iteration 120/1000 | Loss: 0.00001794
Iteration 121/1000 | Loss: 0.00001794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.7944428691407666e-05, 1.7944428691407666e-05, 1.7944428691407666e-05, 1.7944428691407666e-05, 1.7944428691407666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7944428691407666e-05

Optimization complete. Final v2v error: 3.670083522796631 mm

Highest mean error: 4.010064601898193 mm for frame 58

Lowest mean error: 3.5206923484802246 mm for frame 7

Saving results

Total time: 32.638771295547485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917996
Iteration 2/25 | Loss: 0.00179888
Iteration 3/25 | Loss: 0.00151125
Iteration 4/25 | Loss: 0.00142740
Iteration 5/25 | Loss: 0.00141319
Iteration 6/25 | Loss: 0.00140754
Iteration 7/25 | Loss: 0.00140684
Iteration 8/25 | Loss: 0.00140659
Iteration 9/25 | Loss: 0.00140657
Iteration 10/25 | Loss: 0.00140657
Iteration 11/25 | Loss: 0.00140657
Iteration 12/25 | Loss: 0.00140657
Iteration 13/25 | Loss: 0.00140657
Iteration 14/25 | Loss: 0.00140657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014065733412280679, 0.0014065733412280679, 0.0014065733412280679, 0.0014065733412280679, 0.0014065733412280679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014065733412280679

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15036213
Iteration 2/25 | Loss: 0.00252469
Iteration 3/25 | Loss: 0.00252469
Iteration 4/25 | Loss: 0.00252468
Iteration 5/25 | Loss: 0.00252468
Iteration 6/25 | Loss: 0.00252468
Iteration 7/25 | Loss: 0.00252468
Iteration 8/25 | Loss: 0.00252468
Iteration 9/25 | Loss: 0.00252468
Iteration 10/25 | Loss: 0.00252468
Iteration 11/25 | Loss: 0.00252468
Iteration 12/25 | Loss: 0.00252468
Iteration 13/25 | Loss: 0.00252468
Iteration 14/25 | Loss: 0.00252468
Iteration 15/25 | Loss: 0.00252468
Iteration 16/25 | Loss: 0.00252468
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0025246827863156796, 0.0025246827863156796, 0.0025246827863156796, 0.0025246827863156796, 0.0025246827863156796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025246827863156796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00252468
Iteration 2/1000 | Loss: 0.00005412
Iteration 3/1000 | Loss: 0.00003045
Iteration 4/1000 | Loss: 0.00002366
Iteration 5/1000 | Loss: 0.00002112
Iteration 6/1000 | Loss: 0.00001964
Iteration 7/1000 | Loss: 0.00001885
Iteration 8/1000 | Loss: 0.00001827
Iteration 9/1000 | Loss: 0.00001782
Iteration 10/1000 | Loss: 0.00001749
Iteration 11/1000 | Loss: 0.00001718
Iteration 12/1000 | Loss: 0.00001694
Iteration 13/1000 | Loss: 0.00001675
Iteration 14/1000 | Loss: 0.00001671
Iteration 15/1000 | Loss: 0.00001665
Iteration 16/1000 | Loss: 0.00001664
Iteration 17/1000 | Loss: 0.00001659
Iteration 18/1000 | Loss: 0.00001659
Iteration 19/1000 | Loss: 0.00001659
Iteration 20/1000 | Loss: 0.00001658
Iteration 21/1000 | Loss: 0.00001658
Iteration 22/1000 | Loss: 0.00001654
Iteration 23/1000 | Loss: 0.00001654
Iteration 24/1000 | Loss: 0.00001653
Iteration 25/1000 | Loss: 0.00001652
Iteration 26/1000 | Loss: 0.00001652
Iteration 27/1000 | Loss: 0.00001651
Iteration 28/1000 | Loss: 0.00001651
Iteration 29/1000 | Loss: 0.00001650
Iteration 30/1000 | Loss: 0.00001650
Iteration 31/1000 | Loss: 0.00001650
Iteration 32/1000 | Loss: 0.00001649
Iteration 33/1000 | Loss: 0.00001649
Iteration 34/1000 | Loss: 0.00001649
Iteration 35/1000 | Loss: 0.00001649
Iteration 36/1000 | Loss: 0.00001649
Iteration 37/1000 | Loss: 0.00001648
Iteration 38/1000 | Loss: 0.00001648
Iteration 39/1000 | Loss: 0.00001648
Iteration 40/1000 | Loss: 0.00001648
Iteration 41/1000 | Loss: 0.00001648
Iteration 42/1000 | Loss: 0.00001648
Iteration 43/1000 | Loss: 0.00001648
Iteration 44/1000 | Loss: 0.00001647
Iteration 45/1000 | Loss: 0.00001647
Iteration 46/1000 | Loss: 0.00001646
Iteration 47/1000 | Loss: 0.00001646
Iteration 48/1000 | Loss: 0.00001645
Iteration 49/1000 | Loss: 0.00001645
Iteration 50/1000 | Loss: 0.00001645
Iteration 51/1000 | Loss: 0.00001644
Iteration 52/1000 | Loss: 0.00001644
Iteration 53/1000 | Loss: 0.00001644
Iteration 54/1000 | Loss: 0.00001643
Iteration 55/1000 | Loss: 0.00001643
Iteration 56/1000 | Loss: 0.00001643
Iteration 57/1000 | Loss: 0.00001643
Iteration 58/1000 | Loss: 0.00001642
Iteration 59/1000 | Loss: 0.00001642
Iteration 60/1000 | Loss: 0.00001641
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001641
Iteration 63/1000 | Loss: 0.00001641
Iteration 64/1000 | Loss: 0.00001641
Iteration 65/1000 | Loss: 0.00001641
Iteration 66/1000 | Loss: 0.00001641
Iteration 67/1000 | Loss: 0.00001640
Iteration 68/1000 | Loss: 0.00001640
Iteration 69/1000 | Loss: 0.00001640
Iteration 70/1000 | Loss: 0.00001639
Iteration 71/1000 | Loss: 0.00001639
Iteration 72/1000 | Loss: 0.00001639
Iteration 73/1000 | Loss: 0.00001639
Iteration 74/1000 | Loss: 0.00001638
Iteration 75/1000 | Loss: 0.00001638
Iteration 76/1000 | Loss: 0.00001638
Iteration 77/1000 | Loss: 0.00001638
Iteration 78/1000 | Loss: 0.00001637
Iteration 79/1000 | Loss: 0.00001637
Iteration 80/1000 | Loss: 0.00001637
Iteration 81/1000 | Loss: 0.00001637
Iteration 82/1000 | Loss: 0.00001637
Iteration 83/1000 | Loss: 0.00001637
Iteration 84/1000 | Loss: 0.00001637
Iteration 85/1000 | Loss: 0.00001637
Iteration 86/1000 | Loss: 0.00001637
Iteration 87/1000 | Loss: 0.00001637
Iteration 88/1000 | Loss: 0.00001637
Iteration 89/1000 | Loss: 0.00001637
Iteration 90/1000 | Loss: 0.00001637
Iteration 91/1000 | Loss: 0.00001636
Iteration 92/1000 | Loss: 0.00001636
Iteration 93/1000 | Loss: 0.00001636
Iteration 94/1000 | Loss: 0.00001636
Iteration 95/1000 | Loss: 0.00001636
Iteration 96/1000 | Loss: 0.00001636
Iteration 97/1000 | Loss: 0.00001636
Iteration 98/1000 | Loss: 0.00001636
Iteration 99/1000 | Loss: 0.00001636
Iteration 100/1000 | Loss: 0.00001636
Iteration 101/1000 | Loss: 0.00001636
Iteration 102/1000 | Loss: 0.00001636
Iteration 103/1000 | Loss: 0.00001636
Iteration 104/1000 | Loss: 0.00001636
Iteration 105/1000 | Loss: 0.00001636
Iteration 106/1000 | Loss: 0.00001636
Iteration 107/1000 | Loss: 0.00001636
Iteration 108/1000 | Loss: 0.00001636
Iteration 109/1000 | Loss: 0.00001636
Iteration 110/1000 | Loss: 0.00001636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.6364538169000298e-05, 1.6364538169000298e-05, 1.6364538169000298e-05, 1.6364538169000298e-05, 1.6364538169000298e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6364538169000298e-05

Optimization complete. Final v2v error: 3.425173044204712 mm

Highest mean error: 4.642033576965332 mm for frame 53

Lowest mean error: 2.9527926445007324 mm for frame 30

Saving results

Total time: 85.68431830406189
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846568
Iteration 2/25 | Loss: 0.00159033
Iteration 3/25 | Loss: 0.00140921
Iteration 4/25 | Loss: 0.00139906
Iteration 5/25 | Loss: 0.00139737
Iteration 6/25 | Loss: 0.00139737
Iteration 7/25 | Loss: 0.00139737
Iteration 8/25 | Loss: 0.00139737
Iteration 9/25 | Loss: 0.00139737
Iteration 10/25 | Loss: 0.00139737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001397372572682798, 0.001397372572682798, 0.001397372572682798, 0.001397372572682798, 0.001397372572682798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001397372572682798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14645278
Iteration 2/25 | Loss: 0.00194917
Iteration 3/25 | Loss: 0.00194917
Iteration 4/25 | Loss: 0.00194916
Iteration 5/25 | Loss: 0.00194916
Iteration 6/25 | Loss: 0.00194916
Iteration 7/25 | Loss: 0.00194916
Iteration 8/25 | Loss: 0.00194916
Iteration 9/25 | Loss: 0.00194916
Iteration 10/25 | Loss: 0.00194916
Iteration 11/25 | Loss: 0.00194916
Iteration 12/25 | Loss: 0.00194916
Iteration 13/25 | Loss: 0.00194916
Iteration 14/25 | Loss: 0.00194916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0019491637358441949, 0.0019491637358441949, 0.0019491637358441949, 0.0019491637358441949, 0.0019491637358441949]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019491637358441949

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194916
Iteration 2/1000 | Loss: 0.00003737
Iteration 3/1000 | Loss: 0.00002567
Iteration 4/1000 | Loss: 0.00002111
Iteration 5/1000 | Loss: 0.00002024
Iteration 6/1000 | Loss: 0.00001986
Iteration 7/1000 | Loss: 0.00001946
Iteration 8/1000 | Loss: 0.00001928
Iteration 9/1000 | Loss: 0.00001926
Iteration 10/1000 | Loss: 0.00001907
Iteration 11/1000 | Loss: 0.00001903
Iteration 12/1000 | Loss: 0.00001891
Iteration 13/1000 | Loss: 0.00001886
Iteration 14/1000 | Loss: 0.00001883
Iteration 15/1000 | Loss: 0.00001882
Iteration 16/1000 | Loss: 0.00001882
Iteration 17/1000 | Loss: 0.00001881
Iteration 18/1000 | Loss: 0.00001880
Iteration 19/1000 | Loss: 0.00001880
Iteration 20/1000 | Loss: 0.00001879
Iteration 21/1000 | Loss: 0.00001879
Iteration 22/1000 | Loss: 0.00001879
Iteration 23/1000 | Loss: 0.00001878
Iteration 24/1000 | Loss: 0.00001878
Iteration 25/1000 | Loss: 0.00001877
Iteration 26/1000 | Loss: 0.00001877
Iteration 27/1000 | Loss: 0.00001877
Iteration 28/1000 | Loss: 0.00001876
Iteration 29/1000 | Loss: 0.00001875
Iteration 30/1000 | Loss: 0.00001874
Iteration 31/1000 | Loss: 0.00001874
Iteration 32/1000 | Loss: 0.00001873
Iteration 33/1000 | Loss: 0.00001872
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001872
Iteration 36/1000 | Loss: 0.00001872
Iteration 37/1000 | Loss: 0.00001872
Iteration 38/1000 | Loss: 0.00001872
Iteration 39/1000 | Loss: 0.00001872
Iteration 40/1000 | Loss: 0.00001872
Iteration 41/1000 | Loss: 0.00001872
Iteration 42/1000 | Loss: 0.00001871
Iteration 43/1000 | Loss: 0.00001871
Iteration 44/1000 | Loss: 0.00001871
Iteration 45/1000 | Loss: 0.00001870
Iteration 46/1000 | Loss: 0.00001870
Iteration 47/1000 | Loss: 0.00001870
Iteration 48/1000 | Loss: 0.00001870
Iteration 49/1000 | Loss: 0.00001869
Iteration 50/1000 | Loss: 0.00001869
Iteration 51/1000 | Loss: 0.00001869
Iteration 52/1000 | Loss: 0.00001869
Iteration 53/1000 | Loss: 0.00001869
Iteration 54/1000 | Loss: 0.00001869
Iteration 55/1000 | Loss: 0.00001869
Iteration 56/1000 | Loss: 0.00001869
Iteration 57/1000 | Loss: 0.00001869
Iteration 58/1000 | Loss: 0.00001869
Iteration 59/1000 | Loss: 0.00001869
Iteration 60/1000 | Loss: 0.00001869
Iteration 61/1000 | Loss: 0.00001869
Iteration 62/1000 | Loss: 0.00001868
Iteration 63/1000 | Loss: 0.00001868
Iteration 64/1000 | Loss: 0.00001868
Iteration 65/1000 | Loss: 0.00001868
Iteration 66/1000 | Loss: 0.00001868
Iteration 67/1000 | Loss: 0.00001868
Iteration 68/1000 | Loss: 0.00001868
Iteration 69/1000 | Loss: 0.00001868
Iteration 70/1000 | Loss: 0.00001868
Iteration 71/1000 | Loss: 0.00001868
Iteration 72/1000 | Loss: 0.00001868
Iteration 73/1000 | Loss: 0.00001868
Iteration 74/1000 | Loss: 0.00001868
Iteration 75/1000 | Loss: 0.00001868
Iteration 76/1000 | Loss: 0.00001868
Iteration 77/1000 | Loss: 0.00001868
Iteration 78/1000 | Loss: 0.00001868
Iteration 79/1000 | Loss: 0.00001867
Iteration 80/1000 | Loss: 0.00001867
Iteration 81/1000 | Loss: 0.00001867
Iteration 82/1000 | Loss: 0.00001867
Iteration 83/1000 | Loss: 0.00001867
Iteration 84/1000 | Loss: 0.00001867
Iteration 85/1000 | Loss: 0.00001867
Iteration 86/1000 | Loss: 0.00001867
Iteration 87/1000 | Loss: 0.00001866
Iteration 88/1000 | Loss: 0.00001866
Iteration 89/1000 | Loss: 0.00001866
Iteration 90/1000 | Loss: 0.00001866
Iteration 91/1000 | Loss: 0.00001866
Iteration 92/1000 | Loss: 0.00001866
Iteration 93/1000 | Loss: 0.00001866
Iteration 94/1000 | Loss: 0.00001866
Iteration 95/1000 | Loss: 0.00001866
Iteration 96/1000 | Loss: 0.00001866
Iteration 97/1000 | Loss: 0.00001865
Iteration 98/1000 | Loss: 0.00001865
Iteration 99/1000 | Loss: 0.00001865
Iteration 100/1000 | Loss: 0.00001865
Iteration 101/1000 | Loss: 0.00001865
Iteration 102/1000 | Loss: 0.00001865
Iteration 103/1000 | Loss: 0.00001865
Iteration 104/1000 | Loss: 0.00001865
Iteration 105/1000 | Loss: 0.00001865
Iteration 106/1000 | Loss: 0.00001865
Iteration 107/1000 | Loss: 0.00001865
Iteration 108/1000 | Loss: 0.00001865
Iteration 109/1000 | Loss: 0.00001865
Iteration 110/1000 | Loss: 0.00001865
Iteration 111/1000 | Loss: 0.00001865
Iteration 112/1000 | Loss: 0.00001865
Iteration 113/1000 | Loss: 0.00001865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.8648390323505737e-05, 1.8648390323505737e-05, 1.8648390323505737e-05, 1.8648390323505737e-05, 1.8648390323505737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8648390323505737e-05

Optimization complete. Final v2v error: 3.7182912826538086 mm

Highest mean error: 3.9256865978240967 mm for frame 90

Lowest mean error: 3.583585739135742 mm for frame 110

Saving results

Total time: 32.89374542236328
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090240
Iteration 2/25 | Loss: 0.00314930
Iteration 3/25 | Loss: 0.00218198
Iteration 4/25 | Loss: 0.00171326
Iteration 5/25 | Loss: 0.00166124
Iteration 6/25 | Loss: 0.00158415
Iteration 7/25 | Loss: 0.00155452
Iteration 8/25 | Loss: 0.00150212
Iteration 9/25 | Loss: 0.00149678
Iteration 10/25 | Loss: 0.00150603
Iteration 11/25 | Loss: 0.00150024
Iteration 12/25 | Loss: 0.00148306
Iteration 13/25 | Loss: 0.00148080
Iteration 14/25 | Loss: 0.00146904
Iteration 15/25 | Loss: 0.00145909
Iteration 16/25 | Loss: 0.00145393
Iteration 17/25 | Loss: 0.00144923
Iteration 18/25 | Loss: 0.00144462
Iteration 19/25 | Loss: 0.00143808
Iteration 20/25 | Loss: 0.00143449
Iteration 21/25 | Loss: 0.00143352
Iteration 22/25 | Loss: 0.00143598
Iteration 23/25 | Loss: 0.00143193
Iteration 24/25 | Loss: 0.00143424
Iteration 25/25 | Loss: 0.00142912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13600481
Iteration 2/25 | Loss: 0.00822363
Iteration 3/25 | Loss: 0.00461283
Iteration 4/25 | Loss: 0.00461282
Iteration 5/25 | Loss: 0.00461282
Iteration 6/25 | Loss: 0.00461282
Iteration 7/25 | Loss: 0.00461282
Iteration 8/25 | Loss: 0.00461282
Iteration 9/25 | Loss: 0.00461282
Iteration 10/25 | Loss: 0.00461282
Iteration 11/25 | Loss: 0.00461282
Iteration 12/25 | Loss: 0.00461282
Iteration 13/25 | Loss: 0.00461282
Iteration 14/25 | Loss: 0.00461282
Iteration 15/25 | Loss: 0.00461282
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0046128216199576855, 0.0046128216199576855, 0.0046128216199576855, 0.0046128216199576855, 0.0046128216199576855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0046128216199576855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00461282
Iteration 2/1000 | Loss: 0.00046454
Iteration 3/1000 | Loss: 0.00054012
Iteration 4/1000 | Loss: 0.00032331
Iteration 5/1000 | Loss: 0.00071806
Iteration 6/1000 | Loss: 0.00063078
Iteration 7/1000 | Loss: 0.00031759
Iteration 8/1000 | Loss: 0.00088466
Iteration 9/1000 | Loss: 0.00043764
Iteration 10/1000 | Loss: 0.00032419
Iteration 11/1000 | Loss: 0.00066509
Iteration 12/1000 | Loss: 0.00247689
Iteration 13/1000 | Loss: 0.00024542
Iteration 14/1000 | Loss: 0.00013393
Iteration 15/1000 | Loss: 0.00019999
Iteration 16/1000 | Loss: 0.00015662
Iteration 17/1000 | Loss: 0.00022156
Iteration 18/1000 | Loss: 0.00011699
Iteration 19/1000 | Loss: 0.00011896
Iteration 20/1000 | Loss: 0.00069694
Iteration 21/1000 | Loss: 0.00043996
Iteration 22/1000 | Loss: 0.00021350
Iteration 23/1000 | Loss: 0.00020124
Iteration 24/1000 | Loss: 0.00011953
Iteration 25/1000 | Loss: 0.00011822
Iteration 26/1000 | Loss: 0.00051246
Iteration 27/1000 | Loss: 0.00118176
Iteration 28/1000 | Loss: 0.00436296
Iteration 29/1000 | Loss: 0.00152648
Iteration 30/1000 | Loss: 0.00040317
Iteration 31/1000 | Loss: 0.00129481
Iteration 32/1000 | Loss: 0.00080388
Iteration 33/1000 | Loss: 0.00118509
Iteration 34/1000 | Loss: 0.00681638
Iteration 35/1000 | Loss: 0.00236362
Iteration 36/1000 | Loss: 0.00708437
Iteration 37/1000 | Loss: 0.00283167
Iteration 38/1000 | Loss: 0.00938071
Iteration 39/1000 | Loss: 0.00833811
Iteration 40/1000 | Loss: 0.00559550
Iteration 41/1000 | Loss: 0.00443555
Iteration 42/1000 | Loss: 0.00076489
Iteration 43/1000 | Loss: 0.00290487
Iteration 44/1000 | Loss: 0.00219077
Iteration 45/1000 | Loss: 0.00189967
Iteration 46/1000 | Loss: 0.00128582
Iteration 47/1000 | Loss: 0.00247213
Iteration 48/1000 | Loss: 0.00133622
Iteration 49/1000 | Loss: 0.00532758
Iteration 50/1000 | Loss: 0.00264309
Iteration 51/1000 | Loss: 0.00148967
Iteration 52/1000 | Loss: 0.00135764
Iteration 53/1000 | Loss: 0.00180621
Iteration 54/1000 | Loss: 0.00386319
Iteration 55/1000 | Loss: 0.00347870
Iteration 56/1000 | Loss: 0.00146412
Iteration 57/1000 | Loss: 0.00165266
Iteration 58/1000 | Loss: 0.00134813
Iteration 59/1000 | Loss: 0.00108814
Iteration 60/1000 | Loss: 0.00081258
Iteration 61/1000 | Loss: 0.00077625
Iteration 62/1000 | Loss: 0.00064954
Iteration 63/1000 | Loss: 0.00096337
Iteration 64/1000 | Loss: 0.00157104
Iteration 65/1000 | Loss: 0.00071272
Iteration 66/1000 | Loss: 0.00109990
Iteration 67/1000 | Loss: 0.00051272
Iteration 68/1000 | Loss: 0.00078280
Iteration 69/1000 | Loss: 0.00081212
Iteration 70/1000 | Loss: 0.00114241
Iteration 71/1000 | Loss: 0.00082063
Iteration 72/1000 | Loss: 0.00145836
Iteration 73/1000 | Loss: 0.00144360
Iteration 74/1000 | Loss: 0.00082887
Iteration 75/1000 | Loss: 0.00115430
Iteration 76/1000 | Loss: 0.00087893
Iteration 77/1000 | Loss: 0.00059476
Iteration 78/1000 | Loss: 0.00033180
Iteration 79/1000 | Loss: 0.00068652
Iteration 80/1000 | Loss: 0.00056644
Iteration 81/1000 | Loss: 0.00062673
Iteration 82/1000 | Loss: 0.00271996
Iteration 83/1000 | Loss: 0.00014740
Iteration 84/1000 | Loss: 0.00017860
Iteration 85/1000 | Loss: 0.00021661
Iteration 86/1000 | Loss: 0.00031209
Iteration 87/1000 | Loss: 0.00039614
Iteration 88/1000 | Loss: 0.00026622
Iteration 89/1000 | Loss: 0.00026198
Iteration 90/1000 | Loss: 0.00065313
Iteration 91/1000 | Loss: 0.00057982
Iteration 92/1000 | Loss: 0.00044174
Iteration 93/1000 | Loss: 0.00378729
Iteration 94/1000 | Loss: 0.00076013
Iteration 95/1000 | Loss: 0.00133727
Iteration 96/1000 | Loss: 0.00059538
Iteration 97/1000 | Loss: 0.00049175
Iteration 98/1000 | Loss: 0.00046028
Iteration 99/1000 | Loss: 0.00049362
Iteration 100/1000 | Loss: 0.00051049
Iteration 101/1000 | Loss: 0.00049527
Iteration 102/1000 | Loss: 0.00045075
Iteration 103/1000 | Loss: 0.00060503
Iteration 104/1000 | Loss: 0.00091662
Iteration 105/1000 | Loss: 0.00073058
Iteration 106/1000 | Loss: 0.00063023
Iteration 107/1000 | Loss: 0.00033681
Iteration 108/1000 | Loss: 0.00038588
Iteration 109/1000 | Loss: 0.00029023
Iteration 110/1000 | Loss: 0.00031984
Iteration 111/1000 | Loss: 0.00044055
Iteration 112/1000 | Loss: 0.00046392
Iteration 113/1000 | Loss: 0.00033020
Iteration 114/1000 | Loss: 0.00032998
Iteration 115/1000 | Loss: 0.00030089
Iteration 116/1000 | Loss: 0.00029832
Iteration 117/1000 | Loss: 0.00016964
Iteration 118/1000 | Loss: 0.00030552
Iteration 119/1000 | Loss: 0.00018831
Iteration 120/1000 | Loss: 0.00028798
Iteration 121/1000 | Loss: 0.00005246
Iteration 122/1000 | Loss: 0.00015648
Iteration 123/1000 | Loss: 0.00037984
Iteration 124/1000 | Loss: 0.00034591
Iteration 125/1000 | Loss: 0.00032115
Iteration 126/1000 | Loss: 0.00027499
Iteration 127/1000 | Loss: 0.00033447
Iteration 128/1000 | Loss: 0.00042870
Iteration 129/1000 | Loss: 0.00054055
Iteration 130/1000 | Loss: 0.00044697
Iteration 131/1000 | Loss: 0.00005746
Iteration 132/1000 | Loss: 0.00013801
Iteration 133/1000 | Loss: 0.00017245
Iteration 134/1000 | Loss: 0.00008651
Iteration 135/1000 | Loss: 0.00005460
Iteration 136/1000 | Loss: 0.00027551
Iteration 137/1000 | Loss: 0.00025090
Iteration 138/1000 | Loss: 0.00029740
Iteration 139/1000 | Loss: 0.00029425
Iteration 140/1000 | Loss: 0.00029210
Iteration 141/1000 | Loss: 0.00034147
Iteration 142/1000 | Loss: 0.00029095
Iteration 143/1000 | Loss: 0.00034278
Iteration 144/1000 | Loss: 0.00027569
Iteration 145/1000 | Loss: 0.00059319
Iteration 146/1000 | Loss: 0.00055279
Iteration 147/1000 | Loss: 0.00043304
Iteration 148/1000 | Loss: 0.00005651
Iteration 149/1000 | Loss: 0.00014665
Iteration 150/1000 | Loss: 0.00004762
Iteration 151/1000 | Loss: 0.00004589
Iteration 152/1000 | Loss: 0.00004378
Iteration 153/1000 | Loss: 0.00026765
Iteration 154/1000 | Loss: 0.00005147
Iteration 155/1000 | Loss: 0.00004310
Iteration 156/1000 | Loss: 0.00004066
Iteration 157/1000 | Loss: 0.00003962
Iteration 158/1000 | Loss: 0.00003882
Iteration 159/1000 | Loss: 0.00003809
Iteration 160/1000 | Loss: 0.00003737
Iteration 161/1000 | Loss: 0.00003709
Iteration 162/1000 | Loss: 0.00003690
Iteration 163/1000 | Loss: 0.00003674
Iteration 164/1000 | Loss: 0.00003673
Iteration 165/1000 | Loss: 0.00003671
Iteration 166/1000 | Loss: 0.00003670
Iteration 167/1000 | Loss: 0.00003670
Iteration 168/1000 | Loss: 0.00003670
Iteration 169/1000 | Loss: 0.00003670
Iteration 170/1000 | Loss: 0.00003670
Iteration 171/1000 | Loss: 0.00003670
Iteration 172/1000 | Loss: 0.00003670
Iteration 173/1000 | Loss: 0.00003670
Iteration 174/1000 | Loss: 0.00003670
Iteration 175/1000 | Loss: 0.00003670
Iteration 176/1000 | Loss: 0.00003670
Iteration 177/1000 | Loss: 0.00003670
Iteration 178/1000 | Loss: 0.00003669
Iteration 179/1000 | Loss: 0.00003669
Iteration 180/1000 | Loss: 0.00003669
Iteration 181/1000 | Loss: 0.00003669
Iteration 182/1000 | Loss: 0.00003668
Iteration 183/1000 | Loss: 0.00003666
Iteration 184/1000 | Loss: 0.00003665
Iteration 185/1000 | Loss: 0.00003664
Iteration 186/1000 | Loss: 0.00003663
Iteration 187/1000 | Loss: 0.00003663
Iteration 188/1000 | Loss: 0.00003663
Iteration 189/1000 | Loss: 0.00003662
Iteration 190/1000 | Loss: 0.00003658
Iteration 191/1000 | Loss: 0.00003657
Iteration 192/1000 | Loss: 0.00003655
Iteration 193/1000 | Loss: 0.00003654
Iteration 194/1000 | Loss: 0.00003653
Iteration 195/1000 | Loss: 0.00003653
Iteration 196/1000 | Loss: 0.00003653
Iteration 197/1000 | Loss: 0.00003652
Iteration 198/1000 | Loss: 0.00003652
Iteration 199/1000 | Loss: 0.00003652
Iteration 200/1000 | Loss: 0.00003652
Iteration 201/1000 | Loss: 0.00003652
Iteration 202/1000 | Loss: 0.00003652
Iteration 203/1000 | Loss: 0.00003652
Iteration 204/1000 | Loss: 0.00003652
Iteration 205/1000 | Loss: 0.00003651
Iteration 206/1000 | Loss: 0.00003651
Iteration 207/1000 | Loss: 0.00003651
Iteration 208/1000 | Loss: 0.00003651
Iteration 209/1000 | Loss: 0.00003650
Iteration 210/1000 | Loss: 0.00003650
Iteration 211/1000 | Loss: 0.00003650
Iteration 212/1000 | Loss: 0.00003650
Iteration 213/1000 | Loss: 0.00003650
Iteration 214/1000 | Loss: 0.00003650
Iteration 215/1000 | Loss: 0.00003649
Iteration 216/1000 | Loss: 0.00003649
Iteration 217/1000 | Loss: 0.00003649
Iteration 218/1000 | Loss: 0.00003649
Iteration 219/1000 | Loss: 0.00003649
Iteration 220/1000 | Loss: 0.00003649
Iteration 221/1000 | Loss: 0.00003649
Iteration 222/1000 | Loss: 0.00003649
Iteration 223/1000 | Loss: 0.00003649
Iteration 224/1000 | Loss: 0.00003649
Iteration 225/1000 | Loss: 0.00003649
Iteration 226/1000 | Loss: 0.00003649
Iteration 227/1000 | Loss: 0.00003649
Iteration 228/1000 | Loss: 0.00003649
Iteration 229/1000 | Loss: 0.00003649
Iteration 230/1000 | Loss: 0.00003649
Iteration 231/1000 | Loss: 0.00003649
Iteration 232/1000 | Loss: 0.00003649
Iteration 233/1000 | Loss: 0.00003649
Iteration 234/1000 | Loss: 0.00003649
Iteration 235/1000 | Loss: 0.00003649
Iteration 236/1000 | Loss: 0.00003649
Iteration 237/1000 | Loss: 0.00003649
Iteration 238/1000 | Loss: 0.00003648
Iteration 239/1000 | Loss: 0.00003648
Iteration 240/1000 | Loss: 0.00003648
Iteration 241/1000 | Loss: 0.00003647
Iteration 242/1000 | Loss: 0.00003647
Iteration 243/1000 | Loss: 0.00003646
Iteration 244/1000 | Loss: 0.00003645
Iteration 245/1000 | Loss: 0.00003645
Iteration 246/1000 | Loss: 0.00003644
Iteration 247/1000 | Loss: 0.00003644
Iteration 248/1000 | Loss: 0.00038317
Iteration 249/1000 | Loss: 0.00003712
Iteration 250/1000 | Loss: 0.00003643
Iteration 251/1000 | Loss: 0.00003643
Iteration 252/1000 | Loss: 0.00003643
Iteration 253/1000 | Loss: 0.00003642
Iteration 254/1000 | Loss: 0.00003642
Iteration 255/1000 | Loss: 0.00003642
Iteration 256/1000 | Loss: 0.00003642
Iteration 257/1000 | Loss: 0.00003642
Iteration 258/1000 | Loss: 0.00003642
Iteration 259/1000 | Loss: 0.00003642
Iteration 260/1000 | Loss: 0.00003642
Iteration 261/1000 | Loss: 0.00003642
Iteration 262/1000 | Loss: 0.00003642
Iteration 263/1000 | Loss: 0.00003642
Iteration 264/1000 | Loss: 0.00003639
Iteration 265/1000 | Loss: 0.00003638
Iteration 266/1000 | Loss: 0.00003638
Iteration 267/1000 | Loss: 0.00003634
Iteration 268/1000 | Loss: 0.00003633
Iteration 269/1000 | Loss: 0.00003633
Iteration 270/1000 | Loss: 0.00003633
Iteration 271/1000 | Loss: 0.00003632
Iteration 272/1000 | Loss: 0.00003632
Iteration 273/1000 | Loss: 0.00003632
Iteration 274/1000 | Loss: 0.00003631
Iteration 275/1000 | Loss: 0.00003631
Iteration 276/1000 | Loss: 0.00003631
Iteration 277/1000 | Loss: 0.00003631
Iteration 278/1000 | Loss: 0.00003630
Iteration 279/1000 | Loss: 0.00003630
Iteration 280/1000 | Loss: 0.00003630
Iteration 281/1000 | Loss: 0.00003630
Iteration 282/1000 | Loss: 0.00003630
Iteration 283/1000 | Loss: 0.00003629
Iteration 284/1000 | Loss: 0.00003629
Iteration 285/1000 | Loss: 0.00003629
Iteration 286/1000 | Loss: 0.00003629
Iteration 287/1000 | Loss: 0.00003628
Iteration 288/1000 | Loss: 0.00003628
Iteration 289/1000 | Loss: 0.00003628
Iteration 290/1000 | Loss: 0.00003628
Iteration 291/1000 | Loss: 0.00003628
Iteration 292/1000 | Loss: 0.00003628
Iteration 293/1000 | Loss: 0.00003627
Iteration 294/1000 | Loss: 0.00003627
Iteration 295/1000 | Loss: 0.00003627
Iteration 296/1000 | Loss: 0.00003627
Iteration 297/1000 | Loss: 0.00003627
Iteration 298/1000 | Loss: 0.00003627
Iteration 299/1000 | Loss: 0.00003627
Iteration 300/1000 | Loss: 0.00003627
Iteration 301/1000 | Loss: 0.00003627
Iteration 302/1000 | Loss: 0.00003627
Iteration 303/1000 | Loss: 0.00003626
Iteration 304/1000 | Loss: 0.00003626
Iteration 305/1000 | Loss: 0.00003626
Iteration 306/1000 | Loss: 0.00003626
Iteration 307/1000 | Loss: 0.00003626
Iteration 308/1000 | Loss: 0.00003626
Iteration 309/1000 | Loss: 0.00003626
Iteration 310/1000 | Loss: 0.00003626
Iteration 311/1000 | Loss: 0.00003626
Iteration 312/1000 | Loss: 0.00003626
Iteration 313/1000 | Loss: 0.00003626
Iteration 314/1000 | Loss: 0.00003626
Iteration 315/1000 | Loss: 0.00003626
Iteration 316/1000 | Loss: 0.00003626
Iteration 317/1000 | Loss: 0.00003626
Iteration 318/1000 | Loss: 0.00003626
Iteration 319/1000 | Loss: 0.00003626
Iteration 320/1000 | Loss: 0.00003626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 320. Stopping optimization.
Last 5 losses: [3.625688987085596e-05, 3.625688987085596e-05, 3.625688987085596e-05, 3.625688987085596e-05, 3.625688987085596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.625688987085596e-05

Optimization complete. Final v2v error: 3.8226664066314697 mm

Highest mean error: 18.294239044189453 mm for frame 52

Lowest mean error: 2.7865982055664062 mm for frame 0

Saving results

Total time: 381.2536177635193
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01120350
Iteration 2/25 | Loss: 0.00245649
Iteration 3/25 | Loss: 0.00295941
Iteration 4/25 | Loss: 0.00174662
Iteration 5/25 | Loss: 0.00172045
Iteration 6/25 | Loss: 0.00142304
Iteration 7/25 | Loss: 0.00135873
Iteration 8/25 | Loss: 0.00132109
Iteration 9/25 | Loss: 0.00130766
Iteration 10/25 | Loss: 0.00130621
Iteration 11/25 | Loss: 0.00130374
Iteration 12/25 | Loss: 0.00130833
Iteration 13/25 | Loss: 0.00129973
Iteration 14/25 | Loss: 0.00129716
Iteration 15/25 | Loss: 0.00129656
Iteration 16/25 | Loss: 0.00129642
Iteration 17/25 | Loss: 0.00129633
Iteration 18/25 | Loss: 0.00129620
Iteration 19/25 | Loss: 0.00129618
Iteration 20/25 | Loss: 0.00129618
Iteration 21/25 | Loss: 0.00129618
Iteration 22/25 | Loss: 0.00129618
Iteration 23/25 | Loss: 0.00129618
Iteration 24/25 | Loss: 0.00129618
Iteration 25/25 | Loss: 0.00129618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.26447630
Iteration 2/25 | Loss: 0.00300533
Iteration 3/25 | Loss: 0.00289159
Iteration 4/25 | Loss: 0.00289158
Iteration 5/25 | Loss: 0.00289158
Iteration 6/25 | Loss: 0.00289158
Iteration 7/25 | Loss: 0.00289158
Iteration 8/25 | Loss: 0.00289158
Iteration 9/25 | Loss: 0.00289158
Iteration 10/25 | Loss: 0.00289158
Iteration 11/25 | Loss: 0.00289158
Iteration 12/25 | Loss: 0.00289158
Iteration 13/25 | Loss: 0.00289158
Iteration 14/25 | Loss: 0.00289158
Iteration 15/25 | Loss: 0.00289158
Iteration 16/25 | Loss: 0.00289158
Iteration 17/25 | Loss: 0.00289158
Iteration 18/25 | Loss: 0.00289158
Iteration 19/25 | Loss: 0.00289158
Iteration 20/25 | Loss: 0.00289158
Iteration 21/25 | Loss: 0.00289158
Iteration 22/25 | Loss: 0.00289158
Iteration 23/25 | Loss: 0.00289158
Iteration 24/25 | Loss: 0.00289158
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0028915810398757458, 0.0028915810398757458, 0.0028915810398757458, 0.0028915810398757458, 0.0028915810398757458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028915810398757458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00289158
Iteration 2/1000 | Loss: 0.00003995
Iteration 3/1000 | Loss: 0.00002685
Iteration 4/1000 | Loss: 0.00011188
Iteration 5/1000 | Loss: 0.00002264
Iteration 6/1000 | Loss: 0.00010518
Iteration 7/1000 | Loss: 0.00003080
Iteration 8/1000 | Loss: 0.00002360
Iteration 9/1000 | Loss: 0.00002009
Iteration 10/1000 | Loss: 0.00003190
Iteration 11/1000 | Loss: 0.00001928
Iteration 12/1000 | Loss: 0.00010543
Iteration 13/1000 | Loss: 0.00003475
Iteration 14/1000 | Loss: 0.00003618
Iteration 15/1000 | Loss: 0.00008905
Iteration 16/1000 | Loss: 0.00004939
Iteration 17/1000 | Loss: 0.00001922
Iteration 18/1000 | Loss: 0.00001892
Iteration 19/1000 | Loss: 0.00001884
Iteration 20/1000 | Loss: 0.00001881
Iteration 21/1000 | Loss: 0.00001881
Iteration 22/1000 | Loss: 0.00001881
Iteration 23/1000 | Loss: 0.00001880
Iteration 24/1000 | Loss: 0.00001864
Iteration 25/1000 | Loss: 0.00001863
Iteration 26/1000 | Loss: 0.00001861
Iteration 27/1000 | Loss: 0.00001858
Iteration 28/1000 | Loss: 0.00001856
Iteration 29/1000 | Loss: 0.00001855
Iteration 30/1000 | Loss: 0.00001852
Iteration 31/1000 | Loss: 0.00001851
Iteration 32/1000 | Loss: 0.00001850
Iteration 33/1000 | Loss: 0.00001842
Iteration 34/1000 | Loss: 0.00001842
Iteration 35/1000 | Loss: 0.00001839
Iteration 36/1000 | Loss: 0.00001838
Iteration 37/1000 | Loss: 0.00001838
Iteration 38/1000 | Loss: 0.00001838
Iteration 39/1000 | Loss: 0.00001837
Iteration 40/1000 | Loss: 0.00001837
Iteration 41/1000 | Loss: 0.00001837
Iteration 42/1000 | Loss: 0.00001836
Iteration 43/1000 | Loss: 0.00001836
Iteration 44/1000 | Loss: 0.00001836
Iteration 45/1000 | Loss: 0.00001835
Iteration 46/1000 | Loss: 0.00001834
Iteration 47/1000 | Loss: 0.00001833
Iteration 48/1000 | Loss: 0.00001833
Iteration 49/1000 | Loss: 0.00001833
Iteration 50/1000 | Loss: 0.00001833
Iteration 51/1000 | Loss: 0.00001832
Iteration 52/1000 | Loss: 0.00001832
Iteration 53/1000 | Loss: 0.00001832
Iteration 54/1000 | Loss: 0.00001832
Iteration 55/1000 | Loss: 0.00001832
Iteration 56/1000 | Loss: 0.00001832
Iteration 57/1000 | Loss: 0.00001832
Iteration 58/1000 | Loss: 0.00001831
Iteration 59/1000 | Loss: 0.00001829
Iteration 60/1000 | Loss: 0.00001829
Iteration 61/1000 | Loss: 0.00001828
Iteration 62/1000 | Loss: 0.00001828
Iteration 63/1000 | Loss: 0.00001828
Iteration 64/1000 | Loss: 0.00001828
Iteration 65/1000 | Loss: 0.00001828
Iteration 66/1000 | Loss: 0.00001828
Iteration 67/1000 | Loss: 0.00001828
Iteration 68/1000 | Loss: 0.00001827
Iteration 69/1000 | Loss: 0.00004881
Iteration 70/1000 | Loss: 0.00004138
Iteration 71/1000 | Loss: 0.00002828
Iteration 72/1000 | Loss: 0.00004432
Iteration 73/1000 | Loss: 0.00002654
Iteration 74/1000 | Loss: 0.00001942
Iteration 75/1000 | Loss: 0.00001882
Iteration 76/1000 | Loss: 0.00015753
Iteration 77/1000 | Loss: 0.00016659
Iteration 78/1000 | Loss: 0.00002166
Iteration 79/1000 | Loss: 0.00002264
Iteration 80/1000 | Loss: 0.00001845
Iteration 81/1000 | Loss: 0.00002382
Iteration 82/1000 | Loss: 0.00001846
Iteration 83/1000 | Loss: 0.00001826
Iteration 84/1000 | Loss: 0.00001821
Iteration 85/1000 | Loss: 0.00001818
Iteration 86/1000 | Loss: 0.00001808
Iteration 87/1000 | Loss: 0.00001808
Iteration 88/1000 | Loss: 0.00001808
Iteration 89/1000 | Loss: 0.00001808
Iteration 90/1000 | Loss: 0.00001808
Iteration 91/1000 | Loss: 0.00001807
Iteration 92/1000 | Loss: 0.00001807
Iteration 93/1000 | Loss: 0.00001807
Iteration 94/1000 | Loss: 0.00001806
Iteration 95/1000 | Loss: 0.00001806
Iteration 96/1000 | Loss: 0.00001806
Iteration 97/1000 | Loss: 0.00001806
Iteration 98/1000 | Loss: 0.00001805
Iteration 99/1000 | Loss: 0.00001805
Iteration 100/1000 | Loss: 0.00001805
Iteration 101/1000 | Loss: 0.00001805
Iteration 102/1000 | Loss: 0.00001805
Iteration 103/1000 | Loss: 0.00001805
Iteration 104/1000 | Loss: 0.00001805
Iteration 105/1000 | Loss: 0.00001805
Iteration 106/1000 | Loss: 0.00001805
Iteration 107/1000 | Loss: 0.00001805
Iteration 108/1000 | Loss: 0.00001804
Iteration 109/1000 | Loss: 0.00001804
Iteration 110/1000 | Loss: 0.00001804
Iteration 111/1000 | Loss: 0.00001804
Iteration 112/1000 | Loss: 0.00001804
Iteration 113/1000 | Loss: 0.00001803
Iteration 114/1000 | Loss: 0.00001803
Iteration 115/1000 | Loss: 0.00001803
Iteration 116/1000 | Loss: 0.00001803
Iteration 117/1000 | Loss: 0.00001803
Iteration 118/1000 | Loss: 0.00001803
Iteration 119/1000 | Loss: 0.00001803
Iteration 120/1000 | Loss: 0.00001803
Iteration 121/1000 | Loss: 0.00001803
Iteration 122/1000 | Loss: 0.00001803
Iteration 123/1000 | Loss: 0.00001803
Iteration 124/1000 | Loss: 0.00001803
Iteration 125/1000 | Loss: 0.00001803
Iteration 126/1000 | Loss: 0.00001803
Iteration 127/1000 | Loss: 0.00001803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.8031139916274697e-05, 1.8031139916274697e-05, 1.8031139916274697e-05, 1.8031139916274697e-05, 1.8031139916274697e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8031139916274697e-05

Optimization complete. Final v2v error: 3.5817017555236816 mm

Highest mean error: 9.906411170959473 mm for frame 34

Lowest mean error: 3.1442644596099854 mm for frame 144

Saving results

Total time: 130.56353330612183
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_0569/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_0569/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088715
Iteration 2/25 | Loss: 0.00215216
Iteration 3/25 | Loss: 0.00168542
Iteration 4/25 | Loss: 0.00157693
Iteration 5/25 | Loss: 0.00154704
Iteration 6/25 | Loss: 0.00153784
Iteration 7/25 | Loss: 0.00148816
Iteration 8/25 | Loss: 0.00147639
Iteration 9/25 | Loss: 0.00144972
Iteration 10/25 | Loss: 0.00144269
Iteration 11/25 | Loss: 0.00144442
Iteration 12/25 | Loss: 0.00144820
Iteration 13/25 | Loss: 0.00144108
Iteration 14/25 | Loss: 0.00143322
Iteration 15/25 | Loss: 0.00142655
Iteration 16/25 | Loss: 0.00142831
Iteration 17/25 | Loss: 0.00142412
Iteration 18/25 | Loss: 0.00142392
Iteration 19/25 | Loss: 0.00142053
Iteration 20/25 | Loss: 0.00141651
Iteration 21/25 | Loss: 0.00141583
Iteration 22/25 | Loss: 0.00141652
Iteration 23/25 | Loss: 0.00141849
Iteration 24/25 | Loss: 0.00141888
Iteration 25/25 | Loss: 0.00141817

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21583509
Iteration 2/25 | Loss: 0.00329369
Iteration 3/25 | Loss: 0.00329369
Iteration 4/25 | Loss: 0.00329369
Iteration 5/25 | Loss: 0.00329369
Iteration 6/25 | Loss: 0.00329369
Iteration 7/25 | Loss: 0.00329369
Iteration 8/25 | Loss: 0.00329369
Iteration 9/25 | Loss: 0.00329369
Iteration 10/25 | Loss: 0.00329369
Iteration 11/25 | Loss: 0.00329369
Iteration 12/25 | Loss: 0.00329369
Iteration 13/25 | Loss: 0.00329369
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.003293692134320736, 0.003293692134320736, 0.003293692134320736, 0.003293692134320736, 0.003293692134320736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003293692134320736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00329369
Iteration 2/1000 | Loss: 0.00015777
Iteration 3/1000 | Loss: 0.00005338
Iteration 4/1000 | Loss: 0.00005357
Iteration 5/1000 | Loss: 0.00005286
Iteration 6/1000 | Loss: 0.00013809
Iteration 7/1000 | Loss: 0.00004203
Iteration 8/1000 | Loss: 0.00003644
Iteration 9/1000 | Loss: 0.00003496
Iteration 10/1000 | Loss: 0.00005388
Iteration 11/1000 | Loss: 0.00003208
Iteration 12/1000 | Loss: 0.00003471
Iteration 13/1000 | Loss: 0.00003286
Iteration 14/1000 | Loss: 0.00004081
Iteration 15/1000 | Loss: 0.00005009
Iteration 16/1000 | Loss: 0.00002977
Iteration 17/1000 | Loss: 0.00002594
Iteration 18/1000 | Loss: 0.00002341
Iteration 19/1000 | Loss: 0.00002205
Iteration 20/1000 | Loss: 0.00002121
Iteration 21/1000 | Loss: 0.00002077
Iteration 22/1000 | Loss: 0.00002046
Iteration 23/1000 | Loss: 0.00002004
Iteration 24/1000 | Loss: 0.00018678
Iteration 25/1000 | Loss: 0.00020705
Iteration 26/1000 | Loss: 0.00027083
Iteration 27/1000 | Loss: 0.00014934
Iteration 28/1000 | Loss: 0.00002042
Iteration 29/1000 | Loss: 0.00010903
Iteration 30/1000 | Loss: 0.00039449
Iteration 31/1000 | Loss: 0.00017155
Iteration 32/1000 | Loss: 0.00056798
Iteration 33/1000 | Loss: 0.00025463
Iteration 34/1000 | Loss: 0.00002528
Iteration 35/1000 | Loss: 0.00002186
Iteration 36/1000 | Loss: 0.00001961
Iteration 37/1000 | Loss: 0.00001873
Iteration 38/1000 | Loss: 0.00001809
Iteration 39/1000 | Loss: 0.00001773
Iteration 40/1000 | Loss: 0.00001761
Iteration 41/1000 | Loss: 0.00002220
Iteration 42/1000 | Loss: 0.00001884
Iteration 43/1000 | Loss: 0.00001750
Iteration 44/1000 | Loss: 0.00001725
Iteration 45/1000 | Loss: 0.00001709
Iteration 46/1000 | Loss: 0.00001709
Iteration 47/1000 | Loss: 0.00001704
Iteration 48/1000 | Loss: 0.00001702
Iteration 49/1000 | Loss: 0.00001701
Iteration 50/1000 | Loss: 0.00001701
Iteration 51/1000 | Loss: 0.00001701
Iteration 52/1000 | Loss: 0.00001701
Iteration 53/1000 | Loss: 0.00001701
Iteration 54/1000 | Loss: 0.00001700
Iteration 55/1000 | Loss: 0.00001700
Iteration 56/1000 | Loss: 0.00001700
Iteration 57/1000 | Loss: 0.00001700
Iteration 58/1000 | Loss: 0.00001700
Iteration 59/1000 | Loss: 0.00001700
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001699
Iteration 62/1000 | Loss: 0.00001699
Iteration 63/1000 | Loss: 0.00001699
Iteration 64/1000 | Loss: 0.00001699
Iteration 65/1000 | Loss: 0.00001699
Iteration 66/1000 | Loss: 0.00001699
Iteration 67/1000 | Loss: 0.00001699
Iteration 68/1000 | Loss: 0.00001699
Iteration 69/1000 | Loss: 0.00001699
Iteration 70/1000 | Loss: 0.00001698
Iteration 71/1000 | Loss: 0.00001698
Iteration 72/1000 | Loss: 0.00001698
Iteration 73/1000 | Loss: 0.00001698
Iteration 74/1000 | Loss: 0.00001698
Iteration 75/1000 | Loss: 0.00001698
Iteration 76/1000 | Loss: 0.00001698
Iteration 77/1000 | Loss: 0.00001698
Iteration 78/1000 | Loss: 0.00001698
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001698
Iteration 82/1000 | Loss: 0.00001698
Iteration 83/1000 | Loss: 0.00001698
Iteration 84/1000 | Loss: 0.00001698
Iteration 85/1000 | Loss: 0.00001698
Iteration 86/1000 | Loss: 0.00001698
Iteration 87/1000 | Loss: 0.00001698
Iteration 88/1000 | Loss: 0.00001698
Iteration 89/1000 | Loss: 0.00001698
Iteration 90/1000 | Loss: 0.00001698
Iteration 91/1000 | Loss: 0.00001698
Iteration 92/1000 | Loss: 0.00001698
Iteration 93/1000 | Loss: 0.00001698
Iteration 94/1000 | Loss: 0.00001698
Iteration 95/1000 | Loss: 0.00001698
Iteration 96/1000 | Loss: 0.00001698
Iteration 97/1000 | Loss: 0.00001698
Iteration 98/1000 | Loss: 0.00001698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.6977079212665558e-05, 1.6977079212665558e-05, 1.6977079212665558e-05, 1.6977079212665558e-05, 1.6977079212665558e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6977079212665558e-05

Optimization complete. Final v2v error: 3.4605917930603027 mm

Highest mean error: 5.52293062210083 mm for frame 188

Lowest mean error: 3.1995177268981934 mm for frame 93

Saving results

Total time: 130.9750781059265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01181582
Iteration 2/25 | Loss: 0.00310254
Iteration 3/25 | Loss: 0.00227352
Iteration 4/25 | Loss: 0.00213118
Iteration 5/25 | Loss: 0.00205089
Iteration 6/25 | Loss: 0.00196355
Iteration 7/25 | Loss: 0.00193197
Iteration 8/25 | Loss: 0.00187331
Iteration 9/25 | Loss: 0.00183257
Iteration 10/25 | Loss: 0.00193749
Iteration 11/25 | Loss: 0.00199995
Iteration 12/25 | Loss: 0.00169243
Iteration 13/25 | Loss: 0.00159092
Iteration 14/25 | Loss: 0.00152036
Iteration 15/25 | Loss: 0.00146133
Iteration 16/25 | Loss: 0.00146509
Iteration 17/25 | Loss: 0.00144333
Iteration 18/25 | Loss: 0.00141742
Iteration 19/25 | Loss: 0.00141092
Iteration 20/25 | Loss: 0.00140119
Iteration 21/25 | Loss: 0.00140228
Iteration 22/25 | Loss: 0.00139959
Iteration 23/25 | Loss: 0.00139324
Iteration 24/25 | Loss: 0.00139153
Iteration 25/25 | Loss: 0.00139112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35739470
Iteration 2/25 | Loss: 0.00231966
Iteration 3/25 | Loss: 0.00231966
Iteration 4/25 | Loss: 0.00231966
Iteration 5/25 | Loss: 0.00231966
Iteration 6/25 | Loss: 0.00231966
Iteration 7/25 | Loss: 0.00231966
Iteration 8/25 | Loss: 0.00231966
Iteration 9/25 | Loss: 0.00231966
Iteration 10/25 | Loss: 0.00231966
Iteration 11/25 | Loss: 0.00231966
Iteration 12/25 | Loss: 0.00231966
Iteration 13/25 | Loss: 0.00231966
Iteration 14/25 | Loss: 0.00231966
Iteration 15/25 | Loss: 0.00231966
Iteration 16/25 | Loss: 0.00231966
Iteration 17/25 | Loss: 0.00231966
Iteration 18/25 | Loss: 0.00231966
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0023196577094495296, 0.0023196577094495296, 0.0023196577094495296, 0.0023196577094495296, 0.0023196577094495296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023196577094495296

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231966
Iteration 2/1000 | Loss: 0.00121888
Iteration 3/1000 | Loss: 0.00023919
Iteration 4/1000 | Loss: 0.00582769
Iteration 5/1000 | Loss: 0.00478261
Iteration 6/1000 | Loss: 0.00587010
Iteration 7/1000 | Loss: 0.00398959
Iteration 8/1000 | Loss: 0.00271480
Iteration 9/1000 | Loss: 0.00315827
Iteration 10/1000 | Loss: 0.00180843
Iteration 11/1000 | Loss: 0.00213002
Iteration 12/1000 | Loss: 0.00157707
Iteration 13/1000 | Loss: 0.00179404
Iteration 14/1000 | Loss: 0.00295680
Iteration 15/1000 | Loss: 0.00106180
Iteration 16/1000 | Loss: 0.00017028
Iteration 17/1000 | Loss: 0.00167627
Iteration 18/1000 | Loss: 0.00139914
Iteration 19/1000 | Loss: 0.00165159
Iteration 20/1000 | Loss: 0.00129117
Iteration 21/1000 | Loss: 0.00105560
Iteration 22/1000 | Loss: 0.00057353
Iteration 23/1000 | Loss: 0.00138296
Iteration 24/1000 | Loss: 0.00061991
Iteration 25/1000 | Loss: 0.00054252
Iteration 26/1000 | Loss: 0.00012856
Iteration 27/1000 | Loss: 0.00011647
Iteration 28/1000 | Loss: 0.00050412
Iteration 29/1000 | Loss: 0.00025688
Iteration 30/1000 | Loss: 0.00053887
Iteration 31/1000 | Loss: 0.00029253
Iteration 32/1000 | Loss: 0.00089629
Iteration 33/1000 | Loss: 0.00044819
Iteration 34/1000 | Loss: 0.00136164
Iteration 35/1000 | Loss: 0.00150892
Iteration 36/1000 | Loss: 0.00226633
Iteration 37/1000 | Loss: 0.00137085
Iteration 38/1000 | Loss: 0.00250337
Iteration 39/1000 | Loss: 0.00170254
Iteration 40/1000 | Loss: 0.00128853
Iteration 41/1000 | Loss: 0.00104580
Iteration 42/1000 | Loss: 0.00118153
Iteration 43/1000 | Loss: 0.00504699
Iteration 44/1000 | Loss: 0.00248945
Iteration 45/1000 | Loss: 0.00676808
Iteration 46/1000 | Loss: 0.00359644
Iteration 47/1000 | Loss: 0.00748100
Iteration 48/1000 | Loss: 0.00394656
Iteration 49/1000 | Loss: 0.00627273
Iteration 50/1000 | Loss: 0.00177315
Iteration 51/1000 | Loss: 0.00269608
Iteration 52/1000 | Loss: 0.00274944
Iteration 53/1000 | Loss: 0.00094998
Iteration 54/1000 | Loss: 0.00096255
Iteration 55/1000 | Loss: 0.00011836
Iteration 56/1000 | Loss: 0.00014790
Iteration 57/1000 | Loss: 0.00017340
Iteration 58/1000 | Loss: 0.00014781
Iteration 59/1000 | Loss: 0.00011726
Iteration 60/1000 | Loss: 0.00009030
Iteration 61/1000 | Loss: 0.00003103
Iteration 62/1000 | Loss: 0.00002949
Iteration 63/1000 | Loss: 0.00010280
Iteration 64/1000 | Loss: 0.00002728
Iteration 65/1000 | Loss: 0.00007785
Iteration 66/1000 | Loss: 0.00003200
Iteration 67/1000 | Loss: 0.00002539
Iteration 68/1000 | Loss: 0.00003447
Iteration 69/1000 | Loss: 0.00003713
Iteration 70/1000 | Loss: 0.00006417
Iteration 71/1000 | Loss: 0.00003354
Iteration 72/1000 | Loss: 0.00005537
Iteration 73/1000 | Loss: 0.00002509
Iteration 74/1000 | Loss: 0.00003582
Iteration 75/1000 | Loss: 0.00002422
Iteration 76/1000 | Loss: 0.00004512
Iteration 77/1000 | Loss: 0.00010912
Iteration 78/1000 | Loss: 0.00012358
Iteration 79/1000 | Loss: 0.00018279
Iteration 80/1000 | Loss: 0.00012741
Iteration 81/1000 | Loss: 0.00005441
Iteration 82/1000 | Loss: 0.00004975
Iteration 83/1000 | Loss: 0.00003643
Iteration 84/1000 | Loss: 0.00003026
Iteration 85/1000 | Loss: 0.00002399
Iteration 86/1000 | Loss: 0.00003156
Iteration 87/1000 | Loss: 0.00002385
Iteration 88/1000 | Loss: 0.00002385
Iteration 89/1000 | Loss: 0.00002385
Iteration 90/1000 | Loss: 0.00002385
Iteration 91/1000 | Loss: 0.00002385
Iteration 92/1000 | Loss: 0.00002385
Iteration 93/1000 | Loss: 0.00002385
Iteration 94/1000 | Loss: 0.00002385
Iteration 95/1000 | Loss: 0.00002385
Iteration 96/1000 | Loss: 0.00002385
Iteration 97/1000 | Loss: 0.00002385
Iteration 98/1000 | Loss: 0.00002384
Iteration 99/1000 | Loss: 0.00002384
Iteration 100/1000 | Loss: 0.00002384
Iteration 101/1000 | Loss: 0.00002384
Iteration 102/1000 | Loss: 0.00002383
Iteration 103/1000 | Loss: 0.00002383
Iteration 104/1000 | Loss: 0.00002383
Iteration 105/1000 | Loss: 0.00002383
Iteration 106/1000 | Loss: 0.00002382
Iteration 107/1000 | Loss: 0.00002382
Iteration 108/1000 | Loss: 0.00002382
Iteration 109/1000 | Loss: 0.00002382
Iteration 110/1000 | Loss: 0.00002382
Iteration 111/1000 | Loss: 0.00002382
Iteration 112/1000 | Loss: 0.00002382
Iteration 113/1000 | Loss: 0.00002382
Iteration 114/1000 | Loss: 0.00002381
Iteration 115/1000 | Loss: 0.00005084
Iteration 116/1000 | Loss: 0.00007769
Iteration 117/1000 | Loss: 0.00002394
Iteration 118/1000 | Loss: 0.00002384
Iteration 119/1000 | Loss: 0.00002384
Iteration 120/1000 | Loss: 0.00002384
Iteration 121/1000 | Loss: 0.00002384
Iteration 122/1000 | Loss: 0.00002383
Iteration 123/1000 | Loss: 0.00005364
Iteration 124/1000 | Loss: 0.00028259
Iteration 125/1000 | Loss: 0.00006539
Iteration 126/1000 | Loss: 0.00003188
Iteration 127/1000 | Loss: 0.00009645
Iteration 128/1000 | Loss: 0.00003977
Iteration 129/1000 | Loss: 0.00003851
Iteration 130/1000 | Loss: 0.00002385
Iteration 131/1000 | Loss: 0.00002385
Iteration 132/1000 | Loss: 0.00002384
Iteration 133/1000 | Loss: 0.00002382
Iteration 134/1000 | Loss: 0.00002381
Iteration 135/1000 | Loss: 0.00002380
Iteration 136/1000 | Loss: 0.00002380
Iteration 137/1000 | Loss: 0.00002380
Iteration 138/1000 | Loss: 0.00002380
Iteration 139/1000 | Loss: 0.00002380
Iteration 140/1000 | Loss: 0.00002379
Iteration 141/1000 | Loss: 0.00002379
Iteration 142/1000 | Loss: 0.00004487
Iteration 143/1000 | Loss: 0.00002700
Iteration 144/1000 | Loss: 0.00002379
Iteration 145/1000 | Loss: 0.00002379
Iteration 146/1000 | Loss: 0.00002379
Iteration 147/1000 | Loss: 0.00002379
Iteration 148/1000 | Loss: 0.00002379
Iteration 149/1000 | Loss: 0.00002379
Iteration 150/1000 | Loss: 0.00002379
Iteration 151/1000 | Loss: 0.00002379
Iteration 152/1000 | Loss: 0.00002379
Iteration 153/1000 | Loss: 0.00002378
Iteration 154/1000 | Loss: 0.00002378
Iteration 155/1000 | Loss: 0.00002378
Iteration 156/1000 | Loss: 0.00006417
Iteration 157/1000 | Loss: 0.00002381
Iteration 158/1000 | Loss: 0.00002377
Iteration 159/1000 | Loss: 0.00002377
Iteration 160/1000 | Loss: 0.00002377
Iteration 161/1000 | Loss: 0.00002377
Iteration 162/1000 | Loss: 0.00002376
Iteration 163/1000 | Loss: 0.00002376
Iteration 164/1000 | Loss: 0.00002376
Iteration 165/1000 | Loss: 0.00002375
Iteration 166/1000 | Loss: 0.00002375
Iteration 167/1000 | Loss: 0.00002375
Iteration 168/1000 | Loss: 0.00002375
Iteration 169/1000 | Loss: 0.00002375
Iteration 170/1000 | Loss: 0.00002375
Iteration 171/1000 | Loss: 0.00002375
Iteration 172/1000 | Loss: 0.00002375
Iteration 173/1000 | Loss: 0.00002375
Iteration 174/1000 | Loss: 0.00002375
Iteration 175/1000 | Loss: 0.00002375
Iteration 176/1000 | Loss: 0.00002375
Iteration 177/1000 | Loss: 0.00002374
Iteration 178/1000 | Loss: 0.00002374
Iteration 179/1000 | Loss: 0.00002374
Iteration 180/1000 | Loss: 0.00002374
Iteration 181/1000 | Loss: 0.00002374
Iteration 182/1000 | Loss: 0.00002374
Iteration 183/1000 | Loss: 0.00002374
Iteration 184/1000 | Loss: 0.00002374
Iteration 185/1000 | Loss: 0.00002374
Iteration 186/1000 | Loss: 0.00002374
Iteration 187/1000 | Loss: 0.00002374
Iteration 188/1000 | Loss: 0.00002374
Iteration 189/1000 | Loss: 0.00002374
Iteration 190/1000 | Loss: 0.00002374
Iteration 191/1000 | Loss: 0.00002374
Iteration 192/1000 | Loss: 0.00002374
Iteration 193/1000 | Loss: 0.00002374
Iteration 194/1000 | Loss: 0.00002374
Iteration 195/1000 | Loss: 0.00002374
Iteration 196/1000 | Loss: 0.00002374
Iteration 197/1000 | Loss: 0.00002374
Iteration 198/1000 | Loss: 0.00002374
Iteration 199/1000 | Loss: 0.00002374
Iteration 200/1000 | Loss: 0.00002374
Iteration 201/1000 | Loss: 0.00002374
Iteration 202/1000 | Loss: 0.00002373
Iteration 203/1000 | Loss: 0.00002373
Iteration 204/1000 | Loss: 0.00002373
Iteration 205/1000 | Loss: 0.00002373
Iteration 206/1000 | Loss: 0.00002373
Iteration 207/1000 | Loss: 0.00002373
Iteration 208/1000 | Loss: 0.00002373
Iteration 209/1000 | Loss: 0.00002373
Iteration 210/1000 | Loss: 0.00002373
Iteration 211/1000 | Loss: 0.00002373
Iteration 212/1000 | Loss: 0.00002373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [2.3734926799079403e-05, 2.3734926799079403e-05, 2.3734926799079403e-05, 2.3734926799079403e-05, 2.3734926799079403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3734926799079403e-05

Optimization complete. Final v2v error: 4.154478549957275 mm

Highest mean error: 4.37636137008667 mm for frame 36

Lowest mean error: 3.8740060329437256 mm for frame 137

Saving results

Total time: 202.73157453536987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425196
Iteration 2/25 | Loss: 0.00120721
Iteration 3/25 | Loss: 0.00112367
Iteration 4/25 | Loss: 0.00111490
Iteration 5/25 | Loss: 0.00111212
Iteration 6/25 | Loss: 0.00111101
Iteration 7/25 | Loss: 0.00111101
Iteration 8/25 | Loss: 0.00111101
Iteration 9/25 | Loss: 0.00111101
Iteration 10/25 | Loss: 0.00111101
Iteration 11/25 | Loss: 0.00111101
Iteration 12/25 | Loss: 0.00111101
Iteration 13/25 | Loss: 0.00111101
Iteration 14/25 | Loss: 0.00111101
Iteration 15/25 | Loss: 0.00111101
Iteration 16/25 | Loss: 0.00111101
Iteration 17/25 | Loss: 0.00111101
Iteration 18/25 | Loss: 0.00111101
Iteration 19/25 | Loss: 0.00111101
Iteration 20/25 | Loss: 0.00111101
Iteration 21/25 | Loss: 0.00111101
Iteration 22/25 | Loss: 0.00111101
Iteration 23/25 | Loss: 0.00111101
Iteration 24/25 | Loss: 0.00111101
Iteration 25/25 | Loss: 0.00111101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41618037
Iteration 2/25 | Loss: 0.00055091
Iteration 3/25 | Loss: 0.00055091
Iteration 4/25 | Loss: 0.00055091
Iteration 5/25 | Loss: 0.00055091
Iteration 6/25 | Loss: 0.00055090
Iteration 7/25 | Loss: 0.00055090
Iteration 8/25 | Loss: 0.00055090
Iteration 9/25 | Loss: 0.00055090
Iteration 10/25 | Loss: 0.00055090
Iteration 11/25 | Loss: 0.00055090
Iteration 12/25 | Loss: 0.00055090
Iteration 13/25 | Loss: 0.00055090
Iteration 14/25 | Loss: 0.00055090
Iteration 15/25 | Loss: 0.00055090
Iteration 16/25 | Loss: 0.00055090
Iteration 17/25 | Loss: 0.00055090
Iteration 18/25 | Loss: 0.00055090
Iteration 19/25 | Loss: 0.00055090
Iteration 20/25 | Loss: 0.00055090
Iteration 21/25 | Loss: 0.00055090
Iteration 22/25 | Loss: 0.00055090
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005509034963324666, 0.0005509034963324666, 0.0005509034963324666, 0.0005509034963324666, 0.0005509034963324666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005509034963324666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055090
Iteration 2/1000 | Loss: 0.00002424
Iteration 3/1000 | Loss: 0.00001896
Iteration 4/1000 | Loss: 0.00001764
Iteration 5/1000 | Loss: 0.00001697
Iteration 6/1000 | Loss: 0.00001661
Iteration 7/1000 | Loss: 0.00001629
Iteration 8/1000 | Loss: 0.00001621
Iteration 9/1000 | Loss: 0.00001620
Iteration 10/1000 | Loss: 0.00001620
Iteration 11/1000 | Loss: 0.00001619
Iteration 12/1000 | Loss: 0.00001619
Iteration 13/1000 | Loss: 0.00001619
Iteration 14/1000 | Loss: 0.00001619
Iteration 15/1000 | Loss: 0.00001617
Iteration 16/1000 | Loss: 0.00001615
Iteration 17/1000 | Loss: 0.00001614
Iteration 18/1000 | Loss: 0.00001612
Iteration 19/1000 | Loss: 0.00001612
Iteration 20/1000 | Loss: 0.00001612
Iteration 21/1000 | Loss: 0.00001611
Iteration 22/1000 | Loss: 0.00001611
Iteration 23/1000 | Loss: 0.00001610
Iteration 24/1000 | Loss: 0.00001610
Iteration 25/1000 | Loss: 0.00001610
Iteration 26/1000 | Loss: 0.00001610
Iteration 27/1000 | Loss: 0.00001609
Iteration 28/1000 | Loss: 0.00001608
Iteration 29/1000 | Loss: 0.00001607
Iteration 30/1000 | Loss: 0.00001604
Iteration 31/1000 | Loss: 0.00001604
Iteration 32/1000 | Loss: 0.00001603
Iteration 33/1000 | Loss: 0.00001602
Iteration 34/1000 | Loss: 0.00001601
Iteration 35/1000 | Loss: 0.00001597
Iteration 36/1000 | Loss: 0.00001597
Iteration 37/1000 | Loss: 0.00001593
Iteration 38/1000 | Loss: 0.00001593
Iteration 39/1000 | Loss: 0.00001590
Iteration 40/1000 | Loss: 0.00001590
Iteration 41/1000 | Loss: 0.00001585
Iteration 42/1000 | Loss: 0.00001584
Iteration 43/1000 | Loss: 0.00001584
Iteration 44/1000 | Loss: 0.00001583
Iteration 45/1000 | Loss: 0.00001583
Iteration 46/1000 | Loss: 0.00001583
Iteration 47/1000 | Loss: 0.00001582
Iteration 48/1000 | Loss: 0.00001580
Iteration 49/1000 | Loss: 0.00001580
Iteration 50/1000 | Loss: 0.00001579
Iteration 51/1000 | Loss: 0.00001579
Iteration 52/1000 | Loss: 0.00001579
Iteration 53/1000 | Loss: 0.00001578
Iteration 54/1000 | Loss: 0.00001577
Iteration 55/1000 | Loss: 0.00001577
Iteration 56/1000 | Loss: 0.00001577
Iteration 57/1000 | Loss: 0.00001577
Iteration 58/1000 | Loss: 0.00001576
Iteration 59/1000 | Loss: 0.00001576
Iteration 60/1000 | Loss: 0.00001575
Iteration 61/1000 | Loss: 0.00001575
Iteration 62/1000 | Loss: 0.00001575
Iteration 63/1000 | Loss: 0.00001574
Iteration 64/1000 | Loss: 0.00001574
Iteration 65/1000 | Loss: 0.00001574
Iteration 66/1000 | Loss: 0.00001574
Iteration 67/1000 | Loss: 0.00001574
Iteration 68/1000 | Loss: 0.00001573
Iteration 69/1000 | Loss: 0.00001573
Iteration 70/1000 | Loss: 0.00001573
Iteration 71/1000 | Loss: 0.00001573
Iteration 72/1000 | Loss: 0.00001573
Iteration 73/1000 | Loss: 0.00001573
Iteration 74/1000 | Loss: 0.00001573
Iteration 75/1000 | Loss: 0.00001573
Iteration 76/1000 | Loss: 0.00001573
Iteration 77/1000 | Loss: 0.00001572
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001572
Iteration 80/1000 | Loss: 0.00001572
Iteration 81/1000 | Loss: 0.00001572
Iteration 82/1000 | Loss: 0.00001572
Iteration 83/1000 | Loss: 0.00001572
Iteration 84/1000 | Loss: 0.00001572
Iteration 85/1000 | Loss: 0.00001572
Iteration 86/1000 | Loss: 0.00001571
Iteration 87/1000 | Loss: 0.00001571
Iteration 88/1000 | Loss: 0.00001571
Iteration 89/1000 | Loss: 0.00001571
Iteration 90/1000 | Loss: 0.00001571
Iteration 91/1000 | Loss: 0.00001571
Iteration 92/1000 | Loss: 0.00001571
Iteration 93/1000 | Loss: 0.00001571
Iteration 94/1000 | Loss: 0.00001571
Iteration 95/1000 | Loss: 0.00001570
Iteration 96/1000 | Loss: 0.00001570
Iteration 97/1000 | Loss: 0.00001570
Iteration 98/1000 | Loss: 0.00001570
Iteration 99/1000 | Loss: 0.00001570
Iteration 100/1000 | Loss: 0.00001570
Iteration 101/1000 | Loss: 0.00001570
Iteration 102/1000 | Loss: 0.00001570
Iteration 103/1000 | Loss: 0.00001570
Iteration 104/1000 | Loss: 0.00001570
Iteration 105/1000 | Loss: 0.00001570
Iteration 106/1000 | Loss: 0.00001569
Iteration 107/1000 | Loss: 0.00001569
Iteration 108/1000 | Loss: 0.00001569
Iteration 109/1000 | Loss: 0.00001569
Iteration 110/1000 | Loss: 0.00001569
Iteration 111/1000 | Loss: 0.00001569
Iteration 112/1000 | Loss: 0.00001569
Iteration 113/1000 | Loss: 0.00001569
Iteration 114/1000 | Loss: 0.00001569
Iteration 115/1000 | Loss: 0.00001569
Iteration 116/1000 | Loss: 0.00001568
Iteration 117/1000 | Loss: 0.00001568
Iteration 118/1000 | Loss: 0.00001568
Iteration 119/1000 | Loss: 0.00001568
Iteration 120/1000 | Loss: 0.00001568
Iteration 121/1000 | Loss: 0.00001568
Iteration 122/1000 | Loss: 0.00001568
Iteration 123/1000 | Loss: 0.00001568
Iteration 124/1000 | Loss: 0.00001567
Iteration 125/1000 | Loss: 0.00001567
Iteration 126/1000 | Loss: 0.00001567
Iteration 127/1000 | Loss: 0.00001567
Iteration 128/1000 | Loss: 0.00001567
Iteration 129/1000 | Loss: 0.00001567
Iteration 130/1000 | Loss: 0.00001567
Iteration 131/1000 | Loss: 0.00001566
Iteration 132/1000 | Loss: 0.00001566
Iteration 133/1000 | Loss: 0.00001566
Iteration 134/1000 | Loss: 0.00001566
Iteration 135/1000 | Loss: 0.00001566
Iteration 136/1000 | Loss: 0.00001566
Iteration 137/1000 | Loss: 0.00001566
Iteration 138/1000 | Loss: 0.00001566
Iteration 139/1000 | Loss: 0.00001566
Iteration 140/1000 | Loss: 0.00001566
Iteration 141/1000 | Loss: 0.00001566
Iteration 142/1000 | Loss: 0.00001566
Iteration 143/1000 | Loss: 0.00001566
Iteration 144/1000 | Loss: 0.00001566
Iteration 145/1000 | Loss: 0.00001566
Iteration 146/1000 | Loss: 0.00001566
Iteration 147/1000 | Loss: 0.00001566
Iteration 148/1000 | Loss: 0.00001566
Iteration 149/1000 | Loss: 0.00001566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.566273203934543e-05, 1.566273203934543e-05, 1.566273203934543e-05, 1.566273203934543e-05, 1.566273203934543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.566273203934543e-05

Optimization complete. Final v2v error: 3.4059205055236816 mm

Highest mean error: 3.6677920818328857 mm for frame 88

Lowest mean error: 3.28764271736145 mm for frame 141

Saving results

Total time: 34.298436403274536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868799
Iteration 2/25 | Loss: 0.00144712
Iteration 3/25 | Loss: 0.00120179
Iteration 4/25 | Loss: 0.00117199
Iteration 5/25 | Loss: 0.00116730
Iteration 6/25 | Loss: 0.00116586
Iteration 7/25 | Loss: 0.00116586
Iteration 8/25 | Loss: 0.00116586
Iteration 9/25 | Loss: 0.00116586
Iteration 10/25 | Loss: 0.00116586
Iteration 11/25 | Loss: 0.00116586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011658554431051016, 0.0011658554431051016, 0.0011658554431051016, 0.0011658554431051016, 0.0011658554431051016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011658554431051016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41281176
Iteration 2/25 | Loss: 0.00078431
Iteration 3/25 | Loss: 0.00078430
Iteration 4/25 | Loss: 0.00078430
Iteration 5/25 | Loss: 0.00078430
Iteration 6/25 | Loss: 0.00078430
Iteration 7/25 | Loss: 0.00078430
Iteration 8/25 | Loss: 0.00078430
Iteration 9/25 | Loss: 0.00078430
Iteration 10/25 | Loss: 0.00078430
Iteration 11/25 | Loss: 0.00078430
Iteration 12/25 | Loss: 0.00078430
Iteration 13/25 | Loss: 0.00078430
Iteration 14/25 | Loss: 0.00078430
Iteration 15/25 | Loss: 0.00078430
Iteration 16/25 | Loss: 0.00078430
Iteration 17/25 | Loss: 0.00078430
Iteration 18/25 | Loss: 0.00078430
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007842981722205877, 0.0007842981722205877, 0.0007842981722205877, 0.0007842981722205877, 0.0007842981722205877]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007842981722205877

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078430
Iteration 2/1000 | Loss: 0.00003638
Iteration 3/1000 | Loss: 0.00002856
Iteration 4/1000 | Loss: 0.00002738
Iteration 5/1000 | Loss: 0.00002627
Iteration 6/1000 | Loss: 0.00002561
Iteration 7/1000 | Loss: 0.00002518
Iteration 8/1000 | Loss: 0.00002473
Iteration 9/1000 | Loss: 0.00002445
Iteration 10/1000 | Loss: 0.00002426
Iteration 11/1000 | Loss: 0.00002413
Iteration 12/1000 | Loss: 0.00002407
Iteration 13/1000 | Loss: 0.00002403
Iteration 14/1000 | Loss: 0.00002400
Iteration 15/1000 | Loss: 0.00002399
Iteration 16/1000 | Loss: 0.00002398
Iteration 17/1000 | Loss: 0.00002396
Iteration 18/1000 | Loss: 0.00002396
Iteration 19/1000 | Loss: 0.00002395
Iteration 20/1000 | Loss: 0.00002395
Iteration 21/1000 | Loss: 0.00002394
Iteration 22/1000 | Loss: 0.00002394
Iteration 23/1000 | Loss: 0.00002394
Iteration 24/1000 | Loss: 0.00002394
Iteration 25/1000 | Loss: 0.00002394
Iteration 26/1000 | Loss: 0.00002394
Iteration 27/1000 | Loss: 0.00002394
Iteration 28/1000 | Loss: 0.00002394
Iteration 29/1000 | Loss: 0.00002394
Iteration 30/1000 | Loss: 0.00002393
Iteration 31/1000 | Loss: 0.00002393
Iteration 32/1000 | Loss: 0.00002393
Iteration 33/1000 | Loss: 0.00002392
Iteration 34/1000 | Loss: 0.00002392
Iteration 35/1000 | Loss: 0.00002392
Iteration 36/1000 | Loss: 0.00002392
Iteration 37/1000 | Loss: 0.00002392
Iteration 38/1000 | Loss: 0.00002388
Iteration 39/1000 | Loss: 0.00002383
Iteration 40/1000 | Loss: 0.00002382
Iteration 41/1000 | Loss: 0.00002382
Iteration 42/1000 | Loss: 0.00002381
Iteration 43/1000 | Loss: 0.00002381
Iteration 44/1000 | Loss: 0.00002380
Iteration 45/1000 | Loss: 0.00002379
Iteration 46/1000 | Loss: 0.00002379
Iteration 47/1000 | Loss: 0.00002379
Iteration 48/1000 | Loss: 0.00002378
Iteration 49/1000 | Loss: 0.00002378
Iteration 50/1000 | Loss: 0.00002377
Iteration 51/1000 | Loss: 0.00002377
Iteration 52/1000 | Loss: 0.00002377
Iteration 53/1000 | Loss: 0.00002377
Iteration 54/1000 | Loss: 0.00002377
Iteration 55/1000 | Loss: 0.00002377
Iteration 56/1000 | Loss: 0.00002376
Iteration 57/1000 | Loss: 0.00002376
Iteration 58/1000 | Loss: 0.00002375
Iteration 59/1000 | Loss: 0.00002375
Iteration 60/1000 | Loss: 0.00002374
Iteration 61/1000 | Loss: 0.00002374
Iteration 62/1000 | Loss: 0.00002374
Iteration 63/1000 | Loss: 0.00002374
Iteration 64/1000 | Loss: 0.00002374
Iteration 65/1000 | Loss: 0.00002374
Iteration 66/1000 | Loss: 0.00002374
Iteration 67/1000 | Loss: 0.00002374
Iteration 68/1000 | Loss: 0.00002374
Iteration 69/1000 | Loss: 0.00002374
Iteration 70/1000 | Loss: 0.00002373
Iteration 71/1000 | Loss: 0.00002373
Iteration 72/1000 | Loss: 0.00002373
Iteration 73/1000 | Loss: 0.00002373
Iteration 74/1000 | Loss: 0.00002372
Iteration 75/1000 | Loss: 0.00002372
Iteration 76/1000 | Loss: 0.00002372
Iteration 77/1000 | Loss: 0.00002372
Iteration 78/1000 | Loss: 0.00002372
Iteration 79/1000 | Loss: 0.00002372
Iteration 80/1000 | Loss: 0.00002372
Iteration 81/1000 | Loss: 0.00002372
Iteration 82/1000 | Loss: 0.00002372
Iteration 83/1000 | Loss: 0.00002372
Iteration 84/1000 | Loss: 0.00002371
Iteration 85/1000 | Loss: 0.00002371
Iteration 86/1000 | Loss: 0.00002371
Iteration 87/1000 | Loss: 0.00002371
Iteration 88/1000 | Loss: 0.00002371
Iteration 89/1000 | Loss: 0.00002371
Iteration 90/1000 | Loss: 0.00002371
Iteration 91/1000 | Loss: 0.00002371
Iteration 92/1000 | Loss: 0.00002371
Iteration 93/1000 | Loss: 0.00002371
Iteration 94/1000 | Loss: 0.00002371
Iteration 95/1000 | Loss: 0.00002371
Iteration 96/1000 | Loss: 0.00002371
Iteration 97/1000 | Loss: 0.00002371
Iteration 98/1000 | Loss: 0.00002371
Iteration 99/1000 | Loss: 0.00002371
Iteration 100/1000 | Loss: 0.00002371
Iteration 101/1000 | Loss: 0.00002371
Iteration 102/1000 | Loss: 0.00002371
Iteration 103/1000 | Loss: 0.00002371
Iteration 104/1000 | Loss: 0.00002371
Iteration 105/1000 | Loss: 0.00002371
Iteration 106/1000 | Loss: 0.00002371
Iteration 107/1000 | Loss: 0.00002371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.3709795641480014e-05, 2.3709795641480014e-05, 2.3709795641480014e-05, 2.3709795641480014e-05, 2.3709795641480014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3709795641480014e-05

Optimization complete. Final v2v error: 4.213592529296875 mm

Highest mean error: 4.489966869354248 mm for frame 162

Lowest mean error: 3.8422367572784424 mm for frame 78

Saving results

Total time: 39.650899171829224
